{"id": "2505.06356", "pdf": "https://arxiv.org/pdf/2505.06356", "abs": "https://arxiv.org/abs/2505.06356", "authors": ["Karthik Reddy Kanjula", "Surya Guthikonda", "Nahid Alam", "Shayekh Bin Islam"], "title": "Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA", "categories": ["cs.CV"], "comment": "Accepted at ReGenAI CVPR2025 Workshop as Oral", "summary": "Pretraining datasets are foundational to the development of multimodal\nmodels, yet they often have inherent biases and toxic content from the\nweb-scale corpora they are sourced from. In this paper, we investigate the\nprevalence of toxicity in LLaVA image-text pretraining dataset, examining how\nharmful content manifests in different modalities. We present a comprehensive\nanalysis of common toxicity categories and propose targeted mitigation\nstrategies, resulting in the creation of a refined toxicity-mitigated dataset.\nThis dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training\ndataset. We offer guidelines for implementing robust toxicity detection\npipelines. Our findings underscore the need to actively identify and filter\ntoxic content - such as hate speech, explicit imagery, and targeted harassment\n- to build more responsible and equitable multimodal systems. The\ntoxicity-mitigated dataset is open source and is available for further\nresearch."}
{"id": "2505.06370", "pdf": "https://arxiv.org/pdf/2505.06370", "abs": "https://arxiv.org/abs/2505.06370", "authors": ["Adhora Madhuri", "Nusaiba Sobir", "Tasnia Binte Mamun", "Taufiq Hasan"], "title": "LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 5 figures, 6 tables", "summary": "Lung cancer is the leading cause of patient mortality in the world. Early\ndiagnosis of malignant pulmonary nodules in CT images can have a significant\nimpact on reducing disease mortality and morbidity. In this work, we propose\nLMLCC-Net, a novel deep learning framework for classifying nodules from CT scan\nimages using a 3D CNN, considering Hounsfield Unit (HU)-based intensity\nfiltering. Benign and malignant nodules have significant differences in their\nintensity profile of HU, which was not exploited in the literature. Our method\nconsiders the intensity pattern as well as the texture for the prediction of\nmalignancies. LMLCC-Net extracts features from multiple branches that each use\na separate learnable HU-based intensity filtering stage. Various combinations\nof branches and learnable ranges of filters were explored to finally produce\nthe best-performing model. In addition, we propose a semi-supervised learning\nscheme for labeling ambiguous cases and also developed a lightweight model to\nclassify the nodules. The experimental evaluations are carried out on the\nLUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of\n91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of\n91.87%, showing improved performance compared to existing methods. The proposed\nmethod can have a significant impact in helping radiologists in the\nclassification of pulmonary nodules and improving patient care."}
{"id": "2505.06381", "pdf": "https://arxiv.org/pdf/2505.06381", "abs": "https://arxiv.org/abs/2505.06381", "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"], "title": "Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal", "categories": ["cs.CV"], "comment": null, "summary": "Medical disease prediction, particularly through imaging, remains a\nchallenging task due to the complexity and variability of medical data,\nincluding noise, ambiguity, and differing image quality. Recent deep learning\nmodels, including Knowledge Distillation (KD) methods, have shown promising\nresults in brain tumor image identification but still face limitations in\nhandling uncertainty and generalizing across diverse medical conditions.\nTraditional KD methods often rely on a context-unaware temperature parameter to\nsoften teacher model predictions, which does not adapt effectively to varying\nuncertainty levels present in medical images. To address this issue, we propose\na novel framework that integrates Ant Colony Optimization (ACO) for optimal\nteacher-student model selection and a novel context-aware predictor approach\nfor temperature scaling. The proposed context-aware framework adjusts the\ntemperature based on factors such as image quality, disease complexity, and\nteacher model confidence, allowing for more robust knowledge transfer.\nAdditionally, ACO efficiently selects the most appropriate teacher-student\nmodel pair from a set of pre-trained models, outperforming current optimization\nmethods by exploring a broader solution space and better handling complex,\nnon-linear relationships within the data. The proposed framework is evaluated\nusing three publicly available benchmark datasets, each corresponding to a\ndistinct medical imaging task. The results demonstrate that the proposed\nframework significantly outperforms current state-of-the-art methods, achieving\ntop accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on\nthe Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced\nperformance is further evidenced by the improved results, surpassing existing\nbenchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet)."}
{"id": "2505.06389", "pdf": "https://arxiv.org/pdf/2505.06389", "abs": "https://arxiv.org/abs/2505.06389", "authors": ["Adrien Chan-Hon-Tong", "Aurélien Plyer", "Baptiste Cadalen", "Laurent Serre"], "title": "Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms", "categories": ["cs.CV"], "comment": null, "summary": "Sensor-based guidance is required for long-range platforms. To bypass the\nstructural limitation of classical registration on reference image framework,\nwe offer in this paper to encode a stack of images of the scene into a deep\nnetwork. Relying on a stack is showed to be relevant on bimodal scene (e.g.\nwhen the scene can or can not be snowy)."}
{"id": "2505.06393", "pdf": "https://arxiv.org/pdf/2505.06393", "abs": "https://arxiv.org/abs/2505.06393", "authors": ["Valfride Nascimento", "Gabriel E. Lima", "Rafael O. Ribeiro", "William Robson Schwartz", "Rayson Laroca", "David Menotti"], "title": "Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark", "categories": ["cs.CV"], "comment": "Accepted for publication in the Journal of the Brazilian Computer\n  Society", "summary": "Recent advancements in super-resolution for License Plate Recognition (LPR)\nhave sought to address challenges posed by low-resolution (LR) and degraded\nimages in surveillance, traffic monitoring, and forensic applications. However,\nexisting studies have relied on private datasets and simplistic degradation\nmodels. To address this gap, we introduce UFPR-SR-Plates, a novel dataset\ncontaining 10,000 tracks with 100,000 paired low and high-resolution license\nplate images captured under real-world conditions. We establish a benchmark\nusing multiple sequential LR and high-resolution (HR) images per vehicle --\nfive of each -- and two state-of-the-art models for super-resolution of license\nplates. We also investigate three fusion strategies to evaluate how combining\npredictions from a leading Optical Character Recognition (OCR) model for\nmultiple super-resolved license plates enhances overall performance. Our\nfindings demonstrate that super-resolution significantly boosts LPR\nperformance, with further improvements observed when applying majority\nvote-based fusion techniques. Specifically, the Layout-Aware and\nCharacter-Driven Network (LCDNet) model combined with the Majority Vote by\nCharacter Position (MVCP) strategy led to the highest recognition rates,\nincreasing from 1.7% with low-resolution images to 31.1% with super-resolution,\nand up to 44.7% when combining OCR outputs from five super-resolved images.\nThese findings underscore the critical role of super-resolution and temporal\ninformation in enhancing LPR accuracy under real-world, adverse conditions. The\nproposed dataset is publicly available to support further research and can be\naccessed at: https://valfride.github.io/nascimento2024toward/"}
{"id": "2505.06411", "pdf": "https://arxiv.org/pdf/2505.06411", "abs": "https://arxiv.org/abs/2505.06411", "authors": ["Fangyu Du", "Yang Yang", "Xuehao Gao", "Hongye Hou"], "title": "MAGE:A Multi-stage Avatar Generator with Sparse Observations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Inferring full-body poses from Head Mounted Devices, which capture only\n3-joint observations from the head and wrists, is a challenging task with wide\nAR/VR applications. Previous attempts focus on learning one-stage motion\nmapping and thus suffer from an over-large inference space for unobserved body\njoint motions. This often leads to unsatisfactory lower-body predictions and\npoor temporal consistency, resulting in unrealistic or incoherent motion\nsequences. To address this, we propose a powerful Multi-stage Avatar GEnerator\nnamed MAGE that factorizes this one-stage direct motion mapping learning with a\nprogressive prediction strategy. Specifically, given initial 3-joint motions,\nMAGE gradually inferring multi-scale body part poses at different abstract\ngranularity levels, starting from a 6-part body representation and gradually\nrefining to 22 joints. With decreasing abstract levels step by step, MAGE\nintroduces more motion context priors from former prediction stages and thus\nimproves realistic motion completion with richer constraint conditions and less\nambiguity. Extensive experiments on large-scale datasets verify that MAGE\nsignificantly outperforms state-of-the-art methods with better accuracy and\ncontinuity."}
{"id": "2505.06413", "pdf": "https://arxiv.org/pdf/2505.06413", "abs": "https://arxiv.org/abs/2505.06413", "authors": ["Ming Liu", "Siyuan Liang", "Koushik Howlader", "Liwen Wang", "Dacheng Tao", "Wensheng Zhang"], "title": "Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have been integrated into autonomous driving\nsystems to enhance reasoning capabilities through tasks such as Visual Question\nAnswering (VQA). However, the robustness of these systems against backdoor\nattacks remains underexplored. In this paper, we propose a natural\nreflection-based backdoor attack targeting VLM systems in autonomous driving\nscenarios, aiming to induce substantial response delays when specific visual\ntriggers are present. We embed faint reflection patterns, mimicking natural\nsurfaces such as glass or water, into a subset of images in the DriveLM\ndataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories\nor system update notifications) to the corresponding textual labels. This\nstrategy trains the model to generate abnormally long responses upon\nencountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and\nLLaMA-Adapter, using parameter-efficient methods. Experimental results\ndemonstrate that while the models maintain normal performance on clean inputs,\nthey exhibit significantly increased inference latency when triggered,\npotentially leading to hazardous delays in real-world autonomous driving\ndecision-making. Further analysis examines factors such as poisoning rates,\ncamera perspectives, and cross-view transferability. Our findings uncover a new\nclass of attacks that exploit the stringent real-time requirements of\nautonomous driving, posing serious challenges to the security and reliability\nof VLM-augmented driving systems."}
{"id": "2505.06436", "pdf": "https://arxiv.org/pdf/2505.06436", "abs": "https://arxiv.org/abs/2505.06436", "authors": ["Jingrui He", "Andrew Stephen McGough"], "title": "My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to 2nd International Workshop on Synthetic Data for Face\n  and Gesture Analysis at IEEE FG 2025", "summary": "Generative Adversarial Network approaches such as StyleGAN/2 provide two key\nbenefits: the ability to generate photo-realistic face images and possessing a\nsemantically structured latent space from which these images are created. Many\napproaches have emerged for editing images derived from vectors in the latent\nspace of a pre-trained StyleGAN/2 models by identifying semantically meaningful\ndirections (e.g., gender or age) in the latent space. By moving the vector in a\nspecific direction, the ideal result would only change the target feature while\npreserving all the other features. Providing an ideal data augmentation\napproach for gesture research as it could be used to generate numerous image\nvariations whilst keeping the facial expressions intact. However, entanglement\nissues, where changing one feature inevitably affects other features, impacts\nthe ability to preserve facial expressions. To address this, we propose the use\nof an addition to the loss function of a Facial Keypoint Detection model to\nrestrict changes to the facial expressions. Building on top of an existing\nmodel, adding the proposed Human Face Landmark Detection (HFLD) loss, provided\nby a pre-trained Facial Keypoint Detection model, to the original loss\nfunction. We quantitatively and qualitatively evaluate the existing and our\nextended model, showing the effectiveness of our approach in addressing the\nentanglement issue and maintaining the facial expression. Our approach achieves\nup to 49% reduction in the change of emotion in our experiments. Moreover, we\nshow the benefit of our approach by comparing with state-of-the-art models. By\nincreasing the ability to preserve the facial gesture and expression during\nfacial transformation, we present a way to create human face images with fixed\nexpression but different appearances, making it a reliable data augmentation\napproach for Facial Gesture and Expression research."}
{"id": "2505.06467", "pdf": "https://arxiv.org/pdf/2505.06467", "abs": "https://arxiv.org/abs/2505.06467", "authors": ["Nisan Chhetri", "Arpan Sainju"], "title": "PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation", "categories": ["cs.CV", "cs.HC"], "comment": "4 pages, 2 figures", "summary": "Generating high-quality images without prompt engineering expertise remains a\nchallenge for text-to-image (T2I) models, which often misinterpret poorly\nstructured prompts, leading to distortions and misalignments. While humans\neasily recognize these flaws, metrics like CLIP fail to capture structural\ninconsistencies, exposing a key limitation in current evaluation methods. To\naddress this, we introduce PromptIQ, an automated framework that refines\nprompts and assesses image quality using our novel Component-Aware Similarity\n(CAS) metric, which detects and penalizes structural errors. Unlike\nconventional methods, PromptIQ iteratively generates and evaluates images until\nthe user is satisfied, eliminating trial-and-error prompt tuning. Our results\nshow that PromptIQ significantly improves generation quality and evaluation\naccuracy, making T2I models more accessible for users with little to no prompt\nengineering expertise."}
{"id": "2505.06512", "pdf": "https://arxiv.org/pdf/2505.06512", "abs": "https://arxiv.org/abs/2505.06512", "authors": ["Hang Wang", "Zhi-Qi Cheng", "Chenhao Lin", "Chao Shen", "Lei Zhang"], "title": "HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "Text-to-image synthesis has progressed to the point where models can generate\nvisually compelling images from natural language prompts. Yet, existing methods\noften fail to reconcile high-level semantic fidelity with explicit spatial\ncontrol, particularly in scenes involving multiple objects, nuanced relations,\nor complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal\nAlignment (HCMA) framework for grounded text-to-image generation. HCMA\nintegrates two alignment modules into each diffusion sampling step: a global\nmodule that continuously aligns latent representations with textual\ndescriptions to ensure scene-level coherence, and a local module that employs\nbounding-box layouts to anchor objects at specified locations, enabling\nfine-grained spatial control. Extensive experiments on the MS-COCO 2014\nvalidation set show that HCMA surpasses state-of-the-art baselines, achieving a\n0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP\nScore. These results demonstrate HCMA's effectiveness in faithfully capturing\nintricate textual semantics while adhering to user-defined spatial constraints,\noffering a robust solution for semantically grounded image generation.Our code\nis available at https://github.com/hwang-cs-ime/HCMA"}
{"id": "2505.06515", "pdf": "https://arxiv.org/pdf/2505.06515", "abs": "https://arxiv.org/abs/2505.06515", "authors": ["Zhiwen Zeng", "Yunfei Yin", "Zheng Yuan", "Argho Dey", "Xianjian Bao"], "title": "RESAR-BEV: An Explainable Progressive Residual Autoregressive Approach for Camera-Radar Fusion in BEV Segmentation", "categories": ["cs.CV"], "comment": "This work was submitted to IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS) on 09-May-2025", "summary": "Bird's-Eye-View (BEV) semantic segmentation provides comprehensive\nenvironmental perception for autonomous driving but suffers multi-modal\nmisalignment and sensor noise. We propose RESAR-BEV, a progressive refinement\nframework that advances beyond single-step end-to-end approaches: (1)\nprogressive refinement through residual autoregressive learning that decomposes\nBEV segmentation into interpretable coarse-to-fine stages via our\nDrive-Transformer and Modifier-Transformer residual prediction cascaded\narchitecture, (2) robust BEV representation combining ground-proximity voxels\nwith adaptive height offsets and dual-path voxel feature encoding\n(max+attention pooling) for efficient feature extraction, and (3) decoupled\nsupervision with offline Ground Truth decomposition and online joint\noptimization to prevent overfitting while ensuring structural coherence.\nExperiments on nuScenes demonstrate RESAR-BEV achieves state-of-the-art\nperformance with 54.0% mIoU across 7 essential driving-scene categories while\nmaintaining real-time capability at 14.6 FPS. The framework exhibits robustness\nin challenging scenarios of long-range perception and adverse weather\nconditions."}
{"id": "2505.06516", "pdf": "https://arxiv.org/pdf/2505.06516", "abs": "https://arxiv.org/abs/2505.06516", "authors": ["Yilin Dong", "Tianyun Zhu", "Xinde Li", "Jean Dezert", "Rigui Zhou", "Changming Zhu", "Lei Cao", "Shuzhi Sam Ge"], "title": "Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": "16 pages, 28 figures", "summary": "Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to\nderive a quantum mass function (QMF) as a fuzzy metric type from information\nobtained from various data sources. In addition, QDST uses quantum parallel\ncomputing to speed up computation. Nevertheless, the effective management of\nconflicts between multiple QMFs in QDST is a challenging question. This work\naims to address this problem by proposing a Quantum Conflict Indicator (QCI)\nthat measures the conflict between two QMFs in decision-making. Then, the\nproperties of the QCI are carefully investigated. The obtained results validate\nits compliance with desirable conflict measurement properties such as\nnon-negativity, symmetry, boundedness, extreme consistency and insensitivity to\nrefinement. We then apply the proposed QCI in conflict fusion methods and\ncompare its performance with several commonly used fusion approaches. This\ncomparison demonstrates the superiority of the QCI-based conflict fusion\nmethod. Moreover, the Class Description Domain Space (C-DDS) and its optimized\nversion, C-DDS+ by utilizing the QCI-based fusion method, are proposed to\naddress the Out-of-Distribution (OOD) detection task. The experimental results\nshow that the proposed approach gives better OOD performance with respect to\nseveral state-of-the-art baseline OOD detection methods. Specifically, it\nachieves an average increase in Area Under the Receiver Operating\nCharacteristic Curve (AUC) of 1.2% and a corresponding average decrease in\nFalse Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the\noptimal baseline method."}
{"id": "2505.06517", "pdf": "https://arxiv.org/pdf/2505.06517", "abs": "https://arxiv.org/abs/2505.06517", "authors": ["Xiaohong Huang", "Cui Yang", "Miaowen Wen"], "title": "Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation", "categories": ["cs.CV", "cs.RO"], "comment": "9 pages with 9 figures", "summary": "This paper presents a visual-inertial odometry (VIO) method using\nlong-tracked features. Long-tracked features can constrain more visual frames,\nreducing localization drift. However, they may also lead to accumulated\nmatching errors and drift in feature tracking. Current VIO methods adjust\nobservation weights based on re-projection errors, yet this approach has flaws.\nRe-projection errors depend on estimated camera poses and map points, so\nincreased errors might come from estimation inaccuracies, not actual feature\ntracking errors. This can mislead the optimization process and make\nlong-tracked features ineffective for suppressing localization drift.\nFurthermore, long-tracked features constrain a larger number of frames, which\nposes a significant challenge to real-time performance of the system. To tackle\nthese issues, we propose an active decoupling mechanism for accumulated errors\nin long-tracked feature utilization. We introduce a visual reference frame\nreset strategy to eliminate accumulated tracking errors and a depth prediction\nstrategy to leverage the long-term constraint. To ensure real time preformane,\nwe implement three strategies for efficient system state estimation: a parallel\nelimination strategy based on predefined elimination order, an inverse-depth\nelimination simplification strategy, and an elimination skipping strategy.\nExperiments on various datasets show that our method offers higher positioning\naccuracy with relatively short consumption time, making it more suitable for\nedge-enabled low-altitude IoT navigation, where high-accuracy positioning and\nreal-time operation on edge device are required. The code will be published at\ngithub."}
{"id": "2505.06524", "pdf": "https://arxiv.org/pdf/2505.06524", "abs": "https://arxiv.org/abs/2505.06524", "authors": ["Jingyao Wang", "Jianqi Zhang", "Wenwen Qiang", "Changwen Zheng"], "title": "Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Despite the strength of the Segment Anything Model (SAM), it struggles with\ngeneralization issues in open-vocabulary multi-entity segmentation (OVMS).\nThrough empirical and causal analyses, we find that (i) the prompt bias is the\nprimary cause of the generalization issues; (ii) this bias is closely tied to\nthe task-irrelevant generating factors within the prompts, which act as\nconfounders and affect generalization. To address the generalization issues, we\naim to propose a method that can calibrate prompts to eliminate confounders for\naccurate OVMS. Building upon the causal analysis, we propose that the optimal\nprompt for OVMS should contain only task-relevant causal factors. We define it\nas the causal prompt, serving as the goal of calibration. Next, our theoretical\nanalysis, grounded by causal multi-distribution consistency theory, proves that\nthis prompt can be obtained by enforcing segmentation consistency and\noptimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration\nmethod for SAM to achieve accurate OVMS. It integrates a lightweight causal\nprompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first\ngenerate multiple prompts using random annotations to simulate diverse\ndistributions and then reweight them via CaPL by enforcing causal\nmulti-distribution consistency in both task and entity levels. To ensure\nobtaining causal prompts, CaPL is optimized by minimizing the cumulative\nsegmentation loss across the reweighted prompts to achieve consistency and\noptimality. A bi-level optimization strategy alternates between optimizing CaPL\nand SAM, ensuring accurate OVMS. Extensive experiments validate its\nsuperiority."}
{"id": "2505.06527", "pdf": "https://arxiv.org/pdf/2505.06527", "abs": "https://arxiv.org/abs/2505.06527", "authors": ["Jing Hu", "Kaiwei Yu", "Hongjiang Xian", "Shu Hu", "Xin Wang"], "title": "Improving Generalization of Medical Image Registration Foundation Model", "categories": ["cs.CV", "cs.AI"], "comment": "IJCNN", "summary": "Deformable registration is a fundamental task in medical image processing,\naiming to achieve precise alignment by establishing nonlinear correspondences\nbetween images. Traditional methods offer good adaptability and\ninterpretability but are limited by computational efficiency. Although deep\nlearning approaches have significantly improved registration speed and\naccuracy, they often lack flexibility and generalizability across different\ndatasets and tasks. In recent years, foundation models have emerged as a\npromising direction, leveraging large and diverse datasets to learn universal\nfeatures and transformation patterns for image registration, thus demonstrating\nstrong cross-task transferability. However, these models still face challenges\nin generalization and robustness when encountering novel anatomical structures,\nvarying imaging conditions, or unseen modalities. To address these limitations,\nthis paper incorporates Sharpness-Aware Minimization (SAM) into foundation\nmodels to enhance their generalization and robustness in medical image\nregistration. By optimizing the flatness of the loss landscape, SAM improves\nmodel stability across diverse data distributions and strengthens its ability\nto handle complex clinical scenarios. Experimental results show that foundation\nmodels integrated with SAM achieve significant improvements in cross-dataset\nregistration performance, offering new insights for the advancement of medical\nimage registration technology. Our code is available at\nhttps://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\\_sam."}
{"id": "2505.06528", "pdf": "https://arxiv.org/pdf/2505.06528", "abs": "https://arxiv.org/abs/2505.06528", "authors": ["Mahmudul Hasan"], "title": "Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection", "categories": ["cs.CV"], "comment": null, "summary": "Deepfake videos, produced through advanced artificial intelligence methods\nnow a days, pose a new challenge to the truthfulness of the digital media. As\nDeepfake becomes more convincing day by day, detecting them requires advanced\nmethods capable of identifying subtle inconsistencies. The primary motivation\nof this paper is to recognize deepfake videos using deep learning techniques,\nspecifically by using convolutional neural networks. Deep learning excels in\npattern recognition, hence, makes it an ideal approach for detecting the\nintricate manipulations in deepfakes. In this paper, we consider using MTCNN as\na face detector and EfficientNet-B5 as encoder model to predict if a video is\ndeepfake or not. We utilize training and evaluation dataset from Kaggle DFDC.\nThe results shows that our deepfake detection model acquired 42.78% log loss,\n93.80% AUC and 86.82% F1 score on kaggle's DFDC dataset."}
{"id": "2505.06536", "pdf": "https://arxiv.org/pdf/2505.06536", "abs": "https://arxiv.org/abs/2505.06536", "authors": ["Feng Liu", "Ziwang Fu", "Yunlong Wang", "Qijian Zheng"], "title": "TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2111.02172", "summary": "The fusion technique is the key to the multimodal emotion recognition task.\nRecently, cross-modal attention-based fusion methods have demonstrated high\nperformance and strong robustness. However, cross-modal attention suffers from\nredundant features and does not capture complementary features well. We find\nthat it is not necessary to use the entire information of one modality to\nreinforce the other during cross-modal interaction, and the features that can\nreinforce a modality may contain only a part of it. To this end, we design an\ninnovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN).\nSpecifically, for the redundant features, we make one modality perform\nintra-modal feature selection through a self-attention mechanism, so that the\nselected features can adaptively and efficiently interact with another\nmodality. To better capture the complementary information between the\nmodalities, we obtain the fused weight vector by splicing and use the weight\nvector to achieve feature reinforcement of the modalities. We apply TCAFN to\nthe RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal\nrepresentations to validate the effectiveness of the proposed fusion method.\nThe experimental results show that TACFN brings a significant performance\nimprovement compared to other methods and reaches the state-of-the-art. All\ncode and models could be accessed from https://github.com/shuzihuaiyu/TACFN."}
{"id": "2505.06537", "pdf": "https://arxiv.org/pdf/2505.06537", "abs": "https://arxiv.org/abs/2505.06537", "authors": ["Xianghao Kong", "Qiaosong Qi", "Yuanbin Wang", "Anyi Rao", "Biaolong Chen", "Aixi Zhang", "Si Liu", "Hao Jiang"], "title": "ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fashion video generation aims to synthesize temporally consistent videos from\nreference images of a designated character. Despite significant progress,\nexisting diffusion-based methods only support a single reference image as\ninput, severely limiting their capability to generate view-consistent fashion\nvideos, especially when there are different patterns on the clothes from\ndifferent perspectives. Moreover, the widely adopted motion module does not\nsufficiently model human body movement, leading to sub-optimal spatiotemporal\nconsistency. To address these issues, we propose ProFashion, a fashion video\ngeneration framework leveraging multiple reference images to achieve improved\nview consistency and temporal coherency. To effectively leverage features from\nmultiple reference images while maintaining a reasonable computational cost, we\ndevise a Pose-aware Prototype Aggregator, which selects and aggregates global\nand fine-grained reference features according to pose information to form\nframe-wise prototypes, which serve as guidance in the denoising process. To\nfurther enhance motion consistency, we introduce a Flow-enhanced Prototype\nInstantiator, which exploits the human keypoint motion flow to guide an extra\nspatiotemporal attention process in the denoiser. To demonstrate the\neffectiveness of ProFashion, we extensively evaluate our method on the\nMRFashion-7K dataset we collected from the Internet. ProFashion also\noutperforms previous methods on the UBC Fashion dataset."}
{"id": "2505.06543", "pdf": "https://arxiv.org/pdf/2505.06543", "abs": "https://arxiv.org/abs/2505.06543", "authors": ["Shuhan Zhuang", "Mengqi Huang", "Fengyi Fu", "Nan Chen", "Bohan Lei", "Zhendong Mao"], "title": "HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Visual text rendering, which aims to accurately integrate specified textual\ncontent within generated images, is critical for various applications such as\ncommercial design. Despite recent advances, current methods struggle with\nlong-tail text cases, particularly when handling unseen or small-sized text. In\nthis work, we propose a novel Hierarchical Disentangled Glyph-Based framework\n(HDGlyph) that hierarchically decouples text generation from non-text visual\nsynthesis, enabling joint optimization of both common and long-tail text\nrendering. At the training stage, HDGlyph disentangles pixel-level\nrepresentations via the Multi-Linguistic GlyphNet and the Glyph-Aware\nPerceptual Loss, ensuring robust rendering even for unseen characters. At\ninference time, HDGlyph applies Noise-Disentangled Classifier-Free Guidance and\nLatent-Disentangled Two-Stage Rendering (LD-TSR) scheme, which refines both\nbackground and small-sized text. Extensive evaluations show our model\nconsistently outperforms others, with 5.08% and 11.7% accuracy gains in English\nand Chinese text rendering while maintaining high image quality. It also excels\nin long-tail scenarios with strong accuracy and visual performance."}
{"id": "2505.06557", "pdf": "https://arxiv.org/pdf/2505.06557", "abs": "https://arxiv.org/abs/2505.06557", "authors": ["Lu Dong", "Haiyu Zhang", "Hongjie Zhang", "Yifei Huang", "Zhen-Hua Ling", "Yu Qiao", "Limin Wang", "Yali Wang"], "title": "Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining", "categories": ["cs.CV"], "comment": "TCSVT 2025, doi at https://ieeexplore.ieee.org/document/10970001", "summary": "The task of weakly supervised temporal sentence grounding (WSTSG) aims to\ndetect temporal intervals corresponding to a language description from\nuntrimmed videos with only video-level video-language correspondence. For an\nanchor sample, most existing approaches generate negative samples either from\nother videos or within the same video for contrastive learning. However, some\ntraining samples are highly similar to the anchor sample, directly regarding\nthem as negative samples leads to difficulties for optimization and ignores the\ncorrelations between these similar samples and the anchor sample. To address\nthis, we propose Positive Sample Mining (PSM), a novel framework that mines\npositive samples from the training set to provide more discriminative\nsupervision. Specifically, for a given anchor sample, we partition the\nremaining training set into semantically similar and dissimilar subsets based\non the similarity of their text queries. To effectively leverage these\ncorrelations, we introduce a PSM-guided contrastive loss to ensure that the\nanchor proposal is closer to similar samples and further from dissimilar ones.\nAdditionally, we design a PSM-guided rank loss to ensure that similar samples\nare closer to the anchor proposal than to the negative intra-video proposal,\naiming to distinguish the anchor proposal and the negative intra-video\nproposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the\neffectiveness and superiority of our method."}
{"id": "2505.06566", "pdf": "https://arxiv.org/pdf/2505.06566", "abs": "https://arxiv.org/abs/2505.06566", "authors": ["Zequn Xie", "Haoming Ji", "Lingwei Meng"], "title": "Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image person search aims to identify an individual based on a text\ndescription. To reduce data collection costs, large-scale text-image datasets\nare created from co-occurrence pairs found online. However, this can introduce\nnoise, particularly mismatched pairs, which degrade retrieval performance.\nExisting methods often focus on negative samples, amplifying this noise. To\naddress these issues, we propose the Dynamic Uncertainty and Relational\nAlignment (DURA) framework, which includes the Key Feature Selector (KFS) and a\nnew loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and\nmodels noise uncertainty, improving retrieval reliability. The bidirectional\nevidence from cross-modal similarity is modeled as a Dirichlet distribution,\nenhancing adaptability to noisy data. DSH adjusts the difficulty of negative\nsamples to improve robustness in noisy environments. Our experiments on three\ndatasets show that the method offers strong noise resistance and improves\nretrieval performance in both low- and high-noise scenarios."}
{"id": "2505.06573", "pdf": "https://arxiv.org/pdf/2505.06573", "abs": "https://arxiv.org/abs/2505.06573", "authors": ["Xingchen Li", "LiDian Wang", "Yu Sheng", "ZhiPeng Tang", "Haojie Ren", "Guoliang You", "YiFan Duan", "Jianmin Ji", "Yanyong Zhang"], "title": "ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors", "categories": ["cs.CV"], "comment": null, "summary": "Protecting power transmission lines from potential hazards involves critical\ntasks, one of which is the accurate measurement of distances between power\nlines and potential threats, such as large cranes. The challenge with this task\nis that the current sensor-based methods face challenges in balancing accuracy\nand cost in distance measurement. A common practice is to install cameras on\ntransmission towers, which, however, struggle to measure true 3D distances due\nto the lack of depth information. Although 3D lasers can provide accurate depth\ndata, their high cost makes large-scale deployment impractical.\n  To address this challenge, we present ElectricSight, a system designed for 3D\ndistance measurement and monitoring of potential hazards to power transmission\nlines. This work's key innovations lie in both the overall system framework and\na monocular depth estimation method. Specifically, the system framework\ncombines real-time images with environmental point cloud priors, enabling\ncost-effective and precise 3D distance measurements. As a core component of the\nsystem, the monocular depth estimation method enhances the performance by\nintegrating 3D point cloud data into image-based estimates, improving both the\naccuracy and reliability of the system.\n  To assess ElectricSight's performance, we conducted tests with data from a\nreal-world power transmission scenario. The experimental results demonstrate\nthat ElectricSight achieves an average accuracy of 1.08 m for distance\nmeasurements and an early warning accuracy of 92%."}
{"id": "2505.06575", "pdf": "https://arxiv.org/pdf/2505.06575", "abs": "https://arxiv.org/abs/2505.06575", "authors": ["Chengfeng Wang", "Wei Zhai", "Yuhang Yang", "Yang Cao", "Zhengjun Zha"], "title": "GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images", "categories": ["cs.CV"], "comment": null, "summary": "Estimating the geometry level of human-scene contact aims to ground specific\ncontact surface points at 3D human geometries, which provides a spatial prior\nand bridges the interaction between human and scene, supporting applications\nsuch as human behavior analysis, embodied AI, and AR/VR. To complete the task,\nexisting approaches predominantly rely on parametric human models (e.g., SMPL),\nwhich establish correspondences between images and contact regions through\nfixed SMPL vertex sequences. This actually completes the mapping from image\nfeatures to an ordered sequence. However, this approach lacks consideration of\ngeometry, limiting its generalizability in distinct human geometries. In this\npaper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact\nEstimation), a new paradigm for 3D human contact estimation. GRACE incorporates\na point cloud encoder-decoder architecture along with a hierarchical feature\nextraction and fusion module, enabling the effective integration of 3D human\ngeometric structures with 2D interaction semantics derived from images. Guided\nby visual cues, GRACE establishes an implicit mapping from geometric features\nto the vertex space of the 3D human mesh, thereby achieving accurate modeling\nof contact regions. This design ensures high prediction accuracy and endows the\nframework with strong generalization capability across diverse human\ngeometries. Extensive experiments on multiple benchmark datasets demonstrate\nthat GRACE achieves state-of-the-art performance in contact estimation, with\nadditional results further validating its robust generalization to unstructured\nhuman point clouds."}
{"id": "2505.06576", "pdf": "https://arxiv.org/pdf/2505.06576", "abs": "https://arxiv.org/abs/2505.06576", "authors": ["Haorui Chen", "Zeyu Ren", "Jiaxuan Ren", "Ran Ran", "Jinliang Shao", "Jie Huang", "Liangjian Deng"], "title": "Two-Stage Random Alternation Framework for Zero-Shot Pansharpening", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, pansharpening has seen rapid advancements with deep learning\nmethods, which have demonstrated impressive fusion quality. However, the\nchallenge of acquiring real high-resolution images limits the practical\napplicability of these methods. To address this, we propose a two-stage random\nalternating framework (TRA-PAN) that effectively integrates strong supervision\nconstraints from reduced-resolution images with the physical characteristics of\nfull-resolution images. The first stage introduces a pre-training procedure,\nwhich includes Degradation-Aware Modeling (DAM) to capture spatial-spectral\ndegradation mappings, alongside a warm-up procedure designed to reduce training\ntime and mitigate the negative effects of reduced-resolution data. In the\nsecond stage, Random Alternation Optimization (RAO) is employed, where random\nalternating training leverages the strengths of both reduced- and\nfull-resolution images, further optimizing the fusion model. By primarily\nrelying on full-resolution images, our method enables zero-shot training with\njust a single image pair, obviating the need for large datasets. Experimental\nresults demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in\nboth quantitative metrics and visual quality in real-world scenarios,\nhighlighting its strong practical applicability."}
{"id": "2505.06578", "pdf": "https://arxiv.org/pdf/2505.06578", "abs": "https://arxiv.org/abs/2505.06578", "authors": ["Maxim Vashkevich", "Egor Krivalcevich"], "title": "Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform", "categories": ["cs.CV", "cs.LG", "68T07", "I.5.1"], "comment": "6 pages, 9 figures", "summary": "The paper presents a learned two-dimensional separable transform (LST) that\ncan be considered as a new type of computational layer for constructing neural\nnetwork (NN) architecture for image recognition tasks. The LST based on the\nidea of sharing the weights of one fullyconnected (FC) layer to process all\nrows of an image. After that, a second shared FC layer is used to process all\ncolumns of image representation obtained from the first layer. The use of LST\nlayers in a NN architecture significantly reduces the number of model\nparameters compared to models that use stacked FC layers. We show that a\nNN-classifier based on a single LST layer followed by an FC layer achieves\n98.02\\% accuracy on the MNIST dataset, while having only 9.5k parameters. We\nalso implemented a LST-based classifier for handwritten digit recognition on\nthe FPGA platform to demonstrate the efficiency of the suggested approach for\ndesigning a compact and high-performance implementation of NN models. Git\nrepository with supplementary materials: https://github.com/Mak-Sim/LST-2d"}
{"id": "2505.06592", "pdf": "https://arxiv.org/pdf/2505.06592", "abs": "https://arxiv.org/abs/2505.06592", "authors": ["H M Dipu Kabir", "Subrota Kumar Mondal", "Mohammad Ali Moni"], "title": "Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes batch augmentation with unimodal fine-tuning to detect\nthe fetus's organs from ultrasound images and associated clinical textual\ninformation. We also prescribe pre-training initial layers with investigated\nmedical data before the multimodal training. At first, we apply a transferred\ninitialization with the unimodal image portion of the dataset with batch\naugmentation. This step adjusts the initial layer weights for medical data.\nThen, we apply neural networks (NNs) with fine-tuned initial layers to images\nin batches with batch augmentation to obtain features. We also extract\ninformation from descriptions of images. We combine this information with\nfeatures obtained from images to train the head layer. We write a dataloader\nscript to load the multimodal data and use existing unimodal image augmentation\ntechniques with batch augmentation for the multimodal data. The dataloader\nbrings a new random augmentation for each batch to get a good generalization.\nWe investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The\nmultimodal large language model (LLM) with the proposed training provides the\nbest results among the investigated methods. We receive near state-of-the-art\n(SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the\nproposed method with traditional counterparts at the following repository:\ngithub.com/dipuk0506/multimodal"}
{"id": "2505.06603", "pdf": "https://arxiv.org/pdf/2505.06603", "abs": "https://arxiv.org/abs/2505.06603", "authors": ["Lei Hu", "Zhiyong Gan", "Ling Deng", "Jinglin Liang", "Lingyu Liang", "Shuangping Huang", "Tianshui Chen"], "title": "ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Continual Anomaly Detection (CAD) enables anomaly detection models in\nlearning new classes while preserving knowledge of historical classes. CAD\nfaces two key challenges: catastrophic forgetting and segmentation of small\nanomalous regions. Existing CAD methods store image distributions or patch\nfeatures to mitigate catastrophic forgetting, but they fail to preserve\npixel-level detailed features for accurate segmentation. To overcome this\nlimitation, we propose ReplayCAD, a novel diffusion-driven generative replay\nframework that replay high-quality historical data, thus effectively preserving\npixel-level detailed features. Specifically, we compress historical data by\nsearching for a class semantic embedding in the conditional space of the\npre-trained diffusion model, which can guide the model to replay data with\nfine-grained pixel details, thus improving the segmentation performance.\nHowever, relying solely on semantic features results in limited spatial\ndiversity. Hence, we further use spatial features to guide data compression,\nachieving precise control of sample space, thereby generating more diverse\ndata. Our method achieves state-of-the-art performance in both classification\nand segmentation, with notable improvements in segmentation: 11.5% on VisA and\n8.1% on MVTec. Our source code is available at\nhttps://github.com/HULEI7/ReplayCAD."}
{"id": "2505.06635", "pdf": "https://arxiv.org/pdf/2505.06635", "abs": "https://arxiv.org/abs/2505.06635", "authors": ["Xu Zheng", "Yuanhuiyi Lyu", "Lutao Jiang", "Danda Pani Paudel", "Luc Van Gool", "Xuming Hu"], "title": "Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization", "categories": ["cs.CV"], "comment": null, "summary": "Fusing and balancing multi-modal inputs from novel sensors for dense\nprediction tasks, particularly semantic segmentation, is critically important\nyet remains a significant challenge. One major limitation is the tendency of\nmulti-modal frameworks to over-rely on easily learnable modalities, a\nphenomenon referred to as unimodal dominance or bias. This issue becomes\nespecially problematic in real-world scenarios where the dominant modality may\nbe unavailable, resulting in severe performance degradation. To this end, we\napply a simple but effective plug-and-play regularization term based on\nfunctional entropy, which introduces no additional parameters or modules. This\nterm is designed to intuitively balance the contribution of each visual\nmodality to the segmentation results. Specifically, we leverage the log-Sobolev\ninequality to bound functional entropy using functional-Fisher-information. By\nmaximizing the information contributed by each visual modality, our approach\nmitigates unimodal dominance and establishes a more balanced and robust\nsegmentation framework. A multi-scale regularization module is proposed to\napply our proposed plug-and-play term on high-level features and also\nsegmentation predictions for more balanced multi-modal learning. Extensive\nexperiments on three datasets demonstrate that our proposed method achieves\nsuperior performance, i.e., +13.94%, +3.25%, and +3.64%, without introducing\nany additional parameters."}
{"id": "2505.06647", "pdf": "https://arxiv.org/pdf/2505.06647", "abs": "https://arxiv.org/abs/2505.06647", "authors": ["Zhe Li", "Sarah Cechnicka", "Cheng Ouyang", "Katharina Breininger", "Peter Schüffler", "Bernhard Kainz"], "title": "Dataset Distillation with Probabilistic Latent Features", "categories": ["cs.CV"], "comment": "23 pages", "summary": "As deep learning models grow in complexity and the volume of training data\nincreases, reducing storage and computational costs becomes increasingly\nimportant. Dataset distillation addresses this challenge by synthesizing a\ncompact set of synthetic data that can effectively replace the original dataset\nin downstream classification tasks. While existing methods typically rely on\nmapping data from pixel space to the latent space of a generative model, we\npropose a novel stochastic approach that models the joint distribution of\nlatent features. This allows our method to better capture spatial structures\nand produce diverse synthetic samples, which benefits model training.\nSpecifically, we introduce a low-rank multivariate normal distribution\nparameterized by a lightweight network. This design maintains low computational\ncomplexity and is compatible with various matching networks used in dataset\ndistillation. After distillation, synthetic images are generated by feeding the\nlearned latent features into a pretrained generator. These synthetic images are\nthen used to train classification models, and performance is evaluated on real\ntest set. We validate our method on several benchmarks, including ImageNet\nsubsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach\nachieves state-of-the-art cross architecture performance across a range of\nbackbone architectures, demonstrating its generality and effectiveness."}
{"id": "2505.06663", "pdf": "https://arxiv.org/pdf/2505.06663", "abs": "https://arxiv.org/abs/2505.06663", "authors": ["Yongqi Wang", "Xinxiao Wu", "Shuo Yang"], "title": "METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection", "categories": ["cs.CV"], "comment": "IJCAI2025", "summary": "Open-vocabulary video visual relationship detection aims to detect objects\nand their relationships in videos without being restricted by predefined object\nor relationship categories. Existing methods leverage the rich semantic\nknowledge of pre-trained vision-language models such as CLIP to identify novel\ncategories. They typically adopt a cascaded pipeline to first detect objects\nand then classify relationships based on the detected objects, which may lead\nto error propagation and thus suboptimal performance. In this paper, we propose\nMutual EnhancemenT of Objects and Relationships (METOR), a query-based unified\nframework to jointly model and mutually enhance object detection and\nrelationship classification in open-vocabulary scenarios. Under this framework,\nwe first design a CLIP-based contextual refinement encoding module that\nextracts visual contexts of objects and relationships to refine the encoding of\ntext features and object queries, thus improving the generalization of encoding\nto novel categories. Then we propose an iterative enhancement module to\nalternatively enhance the representations of objects and relationships by fully\nexploiting their interdependence to improve recognition performance. Extensive\nexperiments on two public datasets, VidVRD and VidOR, demonstrate that our\nframework achieves state-of-the-art performance."}
{"id": "2505.06665", "pdf": "https://arxiv.org/pdf/2505.06665", "abs": "https://arxiv.org/abs/2505.06665", "authors": ["Zixian Zhao", "Andrew Howes", "Xingchen Zhang"], "title": "MultiTaskVIF: Segmentation-oriented visible and infrared image fusion via multi-task learning", "categories": ["cs.CV"], "comment": null, "summary": "Visible and infrared image fusion (VIF) has attracted significant attention\nin recent years. Traditional VIF methods primarily focus on generating fused\nimages with high visual quality, while recent advancements increasingly\nemphasize incorporating semantic information into the fusion model during\ntraining. However, most existing segmentation-oriented VIF methods adopt a\ncascade structure comprising separate fusion and segmentation models, leading\nto increased network complexity and redundancy. This raises a critical\nquestion: can we design a more concise and efficient structure to integrate\nsemantic information directly into the fusion model during training-Inspired by\nmulti-task learning, we propose a concise and universal training framework,\nMultiTaskVIF, for segmentation-oriented VIF models. In this framework, we\nintroduce a multi-task head decoder (MTH) to simultaneously output both the\nfused image and the segmentation result during training. Unlike previous\ncascade training frameworks that necessitate joint training with a complete\nsegmentation model, MultiTaskVIF enables the fusion model to learn semantic\nfeatures by simply replacing its decoder with MTH. Extensive experimental\nevaluations validate the effectiveness of the proposed method. Our code will be\nreleased upon acceptance."}
{"id": "2505.06668", "pdf": "https://arxiv.org/pdf/2505.06668", "abs": "https://arxiv.org/abs/2505.06668", "authors": ["Ziyi Wang", "Haipeng Li", "Lin Sui", "Tianhao Zhou", "Hai Jiang", "Lang Nie", "Shuaicheng Liu"], "title": "StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "We present StableMotion, a novel framework leverages knowledge (geometry and\ncontent priors) from pretrained large-scale image diffusion models to perform\nmotion estimation, solving single-image-based image rectification tasks such as\nStitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC).\nSpecifically, StableMotion framework takes text-to-image Stable Diffusion (SD)\nmodels as backbone and repurposes it into an image-to-motion estimator. To\nmitigate inconsistent output produced by diffusion models, we propose Adaptive\nEnsemble Strategy (AES) that consolidates multiple outputs into a cohesive,\nhigh-fidelity result. Additionally, we present the concept of Sampling Steps\nDisaster (SSD), the counterintuitive scenario where increasing the number of\nsampling steps can lead to poorer outcomes, which enables our framework to\nachieve one-step inference. StableMotion is verified on two image rectification\ntasks and delivers state-of-the-art performance in both, as well as showing\nstrong generalizability. Supported by SSD, StableMotion offers a speedup of 200\ntimes compared to previous diffusion model-based methods."}
{"id": "2505.06670", "pdf": "https://arxiv.org/pdf/2505.06670", "abs": "https://arxiv.org/abs/2505.06670", "authors": ["Zhe Li", "Hadrien Reynaud", "Mischa Dombrowski", "Sarah Cechnicka", "Franciskus Xaverius Erick", "Bernhard Kainz"], "title": "Video Dataset Condensation with Diffusion Models", "categories": ["cs.CV"], "comment": "10 pages", "summary": "In recent years, the rapid expansion of dataset sizes and the increasing\ncomplexity of deep learning models have significantly escalated the demand for\ncomputational resources, both for data storage and model training. Dataset\ndistillation has emerged as a promising solution to address this challenge by\ngenerating a compact synthetic dataset that retains the essential information\nfrom a large real dataset. However, existing methods often suffer from limited\nperformance and poor data quality, particularly in the video domain. In this\npaper, we focus on video dataset distillation by employing a video diffusion\nmodel to generate high-quality synthetic videos. To enhance representativeness,\nwe introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select\na diverse and informative subset of videos that effectively captures the\ncharacteristics of the original dataset. To further optimize computational\nefficiency, we explore a training-free clustering algorithm, Temporal-Aware\nCluster-based Distillation (TAC-DT), to select representative videos without\nrequiring additional training overhead. We validate the effectiveness of our\napproach through extensive experiments on four benchmark datasets,\ndemonstrating performance improvements of up to \\(10.61\\%\\) over the\nstate-of-the-art. Our method consistently outperforms existing approaches\nacross all datasets, establishing a new benchmark for video dataset\ndistillation."}
{"id": "2505.06679", "pdf": "https://arxiv.org/pdf/2505.06679", "abs": "https://arxiv.org/abs/2505.06679", "authors": ["Jiayang Liu", "Siyuan Liang", "Shiqian Zhao", "Rongcheng Tu", "Wenbo Zhou", "Xiaochun Cao", "Dacheng Tao", "Siew Kei Lam"], "title": "Jailbreaking the Text-to-Video Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video generative models have achieved significant progress, driven by\nthe rapid advancements in diffusion models, with notable examples including\nPika, Luma, Kling, and Sora. Despite their remarkable generation ability, their\nvulnerability to jailbreak attack, i.e. to generate unsafe content, including\npornography, violence, and discrimination, raises serious safety concerns.\nExisting efforts, such as T2VSafetyBench, have provided valuable benchmarks for\nevaluating the safety of text-to-video models against unsafe prompts but lack\nsystematic studies for exploiting their vulnerabilities effectively. In this\npaper, we propose the \\textit{first} optimization-based jailbreak attack\nagainst text-to-video models, which is specifically designed. Our approach\nformulates the prompt generation task as an optimization problem with three key\nobjectives: (1) maximizing the semantic similarity between the input and\ngenerated prompts, (2) ensuring that the generated prompts can evade the safety\nfilter of the text-to-video model, and (3) maximizing the semantic similarity\nbetween the generated videos and the original input prompts. To further enhance\nthe robustness of the generated prompts, we introduce a prompt mutation\nstrategy that creates multiple prompt variants in each iteration, selecting the\nmost effective one based on the averaged score. This strategy not only improves\nthe attack success rate but also boosts the semantic relevance of the generated\nvideo. We conduct extensive experiments across multiple text-to-video models,\nincluding Open-Sora, Pika, Luma, and Kling. The results demonstrate that our\nmethod not only achieves a higher attack success rate compared to baseline\nmethods but also generates videos with greater semantic similarity to the\noriginal input prompts."}
{"id": "2505.06683", "pdf": "https://arxiv.org/pdf/2505.06683", "abs": "https://arxiv.org/abs/2505.06683", "authors": ["Chunming He", "Rihan Zhang", "Fengyang Xiao", "Chengyu Fang", "Longxiang Tang", "Yulun Zhang", "Sina Farsiu"], "title": "UnfoldIR: Rethinking Deep Unfolding Network in Illumination Degradation Image Restoration", "categories": ["cs.CV"], "comment": "16 pages, 14 tables, 11 figures", "summary": "Deep unfolding networks (DUNs) are widely employed in illumination\ndegradation image restoration (IDIR) to merge the interpretability of\nmodel-based approaches with the generalization of learning-based methods.\nHowever, the performance of DUN-based methods remains considerably inferior to\nthat of state-of-the-art IDIR solvers. Our investigation indicates that this\nlimitation does not stem from structural shortcomings of DUNs but rather from\nthe limited exploration of the unfolding structure, particularly for (1)\nconstructing task-specific restoration models, (2) integrating advanced network\narchitectures, and (3) designing DUN-specific loss functions. To address these\nissues, we propose a novel DUN-based method, UnfoldIR, for IDIR tasks. UnfoldIR\nfirst introduces a new IDIR model with dedicated regularization terms for\nsmoothing illumination and enhancing texture. We unfold the iterative optimized\nsolution of this model into a multistage network, with each stage comprising a\nreflectance-assisted illumination correction (RAIC) module and an\nillumination-guided reflectance enhancement (IGRE) module. RAIC employs a\nvisual state space (VSS) to extract non-local features, enforcing illumination\nsmoothness, while IGRE introduces a frequency-aware VSS to globally align\nsimilar textures, enabling mildly degraded regions to guide the enhancement of\ndetails in more severely degraded areas. This suppresses noise while enhancing\ndetails. Furthermore, given the multistage structure, we propose an inter-stage\ninformation consistent loss to maintain network stability in the final stages.\nThis loss contributes to structural preservation and sustains the model's\nperformance even in unsupervised settings. Experiments verify our effectiveness\nacross 5 IDIR tasks and 3 downstream problems."}
{"id": "2505.06684", "pdf": "https://arxiv.org/pdf/2505.06684", "abs": "https://arxiv.org/abs/2505.06684", "authors": ["Xuefeng Jiang", "Jia Li", "Nannan Wu", "Zhiyuan Wu", "Xujing Li", "Sheng Sun", "Gang Xu", "Yuwei Wang", "Qi Li", "Min Liu"], "title": "FNBench: Benchmarking Robust Federated Learning against Noisy Labels", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to IEEE TDSC, currently under major revision", "summary": "Robustness to label noise within data is a significant challenge in federated\nlearning (FL). From the data-centric perspective, the data quality of\ndistributed datasets can not be guaranteed since annotations of different\nclients contain complicated label noise of varying degrees, which causes the\nperformance degradation. There have been some early attempts to tackle noisy\nlabels in FL. However, there exists a lack of benchmark studies on\ncomprehensively evaluating their practical performance under unified settings.\nTo this end, we propose the first benchmark study FNBench to provide an\nexperimental investigation which considers three diverse label noise patterns\ncovering synthetic label noise, imperfect human-annotation errors and\nsystematic errors. Our evaluation incorporates eighteen state-of-the-art\nmethods over five image recognition datasets and one text classification\ndataset. Meanwhile, we provide observations to understand why noisy labels\nimpair FL, and additionally exploit a representation-aware regularization\nmethod to enhance the robustness of existing methods against noisy labels based\non our observations. Finally, we discuss the limitations of this work and\npropose three-fold future directions. To facilitate related communities, our\nsource code is open-sourced at https://github.com/Sprinter1999/FNBench."}
{"id": "2505.06694", "pdf": "https://arxiv.org/pdf/2505.06694", "abs": "https://arxiv.org/abs/2505.06694", "authors": ["XiaoTong Gu", "Shengyu Tang", "Yiming Cao", "Changdong Yu"], "title": "Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Underwater object detection using sonar imagery has become a critical and\nrapidly evolving research domain within marine technology. However, sonar\nimages are characterized by lower resolution and sparser features compared to\noptical images, which seriously degrades the performance of object detection.To\naddress these challenges, we specifically propose a Detection Transformer\n(DETR) architecture optimized with a Neural Architecture Search (NAS) approach\ncalled NAS-DETR for object detection in sonar images. First, an improved\nZero-shot Neural Architecture Search (NAS) method based on the maximum entropy\nprinciple is proposed to identify a real-time, high-representational-capacity\nCNN-Transformer backbone for sonar image detection. This method enables the\nefficient discovery of high-performance network architectures with low\ncomputational and time overhead. Subsequently, the backbone is combined with a\nFeature Pyramid Network (FPN) and a deformable attention-based Transformer\ndecoder to construct a complete network architecture. This architecture\nintegrates various advanced components and training schemes to enhance overall\nperformance. Extensive experiments demonstrate that this architecture achieves\nstate-of-the-art performance on two Representative datasets, while maintaining\nminimal overhead in real-time efficiency and computational complexity.\nFurthermore, correlation analysis between the key parameters and differential\nentropy-based fitness function is performed to enhance the interpretability of\nthe proposed framework. To the best of our knowledge, this is the first work in\nthe field of sonar object detection to integrate the DETR architecture with a\nNAS search mechanism."}
{"id": "2505.06710", "pdf": "https://arxiv.org/pdf/2505.06710", "abs": "https://arxiv.org/abs/2505.06710", "authors": ["Yicheng Song", "Tiancheng Lin", "Die Peng", "Su Yang", "Yi Xu"], "title": "SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images", "categories": ["cs.CV"], "comment": null, "summary": "Various multi-instance learning (MIL) based approaches have been developed\nand successfully applied to whole-slide pathological images (WSI). Existing MIL\nmethods emphasize the importance of feature aggregators, but largely neglect\nthe instance-level representation learning. They assume that the availability\nof a pre-trained feature extractor can be directly utilized or fine-tuned,\nwhich is not always the case. This paper proposes to pre-train feature\nextractor for MIL via a weakly-supervised scheme, i.e., propagating the weak\nbag-level labels to the corresponding instances for supervised learning. To\nlearn effective features for MIL, we further delve into several key components,\nincluding strong data augmentation, a non-linear prediction head and the robust\nloss function. We conduct experiments on common large-scale WSI datasets and\nfind it achieves better performance than other pre-training schemes (e.g.,\nImageNet pre-training and self-supervised learning) in different downstream\ntasks. We further show the compatibility and scalability of the proposed scheme\nby deploying it in fine-tuning the pathological-specific models and\npre-training on merged multiple datasets. To our knowledge, this is the first\nwork focusing on the representation learning for MIL."}
{"id": "2505.06745", "pdf": "https://arxiv.org/pdf/2505.06745", "abs": "https://arxiv.org/abs/2505.06745", "authors": ["Parth Padalkar", "Gopal Gupta"], "title": "Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent neuro-symbolic approaches have successfully extracted symbolic\nrule-sets from CNN-based models to enhance interpretability. However, applying\nsimilar techniques to Vision Transformers (ViTs) remains challenging due to\ntheir lack of modular concept detectors and reliance on global self-attention\nmechanisms. We propose a framework for symbolic rule extraction from ViTs by\nintroducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This\nlinear layer operates on attention-weighted patch representations and learns a\ndisentangled, binarized representation in which individual neurons activate for\nhigh-level visual concepts. To encourage interpretability, we apply a\ncombination of L1 sparsity, entropy minimization, and supervised contrastive\nloss. These binarized concept activations are used as input to the FOLD-SE-M\nalgorithm, which generates a rule-set in the form of logic programs. Our method\nachieves a 5.14% better classification accuracy than the standard ViT while\nenabling symbolic reasoning. Crucially, the extracted rule-set is not merely\npost-hoc but acts as a logic-based decision layer that operates directly on the\nsparse concept representations. The resulting programs are concise and\nsemantically meaningful. This work is the first to extract executable logic\nprograms from ViTs using sparse symbolic representations. It bridges the gap\nbetween transformer-based vision models and symbolic logic programming,\nproviding a step forward in interpretable and verifiable neuro-symbolic AI."}
{"id": "2505.06796", "pdf": "https://arxiv.org/pdf/2505.06796", "abs": "https://arxiv.org/abs/2505.06796", "authors": ["Ye Zhu", "Yunan Wang", "Zitong Yu"], "title": "Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Multimodal news contains a wealth of information and is easily affected by\ndeepfake modeling attacks. To combat the latest image and text generation\nmethods, we present a new Multimodal Fake News Detection dataset (MFND)\ncontaining 11 manipulated types, designed to detect and localize highly\nauthentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning\n(SDML) model for fake news, which fully uses unimodal and mutual modal features\nto mine the intrinsic semantics of news. Under shallow inference, we propose\nthe momentum distillation-based light punishment contrastive learning for\nfine-grained uniform spatial image and text semantic alignment, and an adaptive\ncross-modal fusion module to enhance mutual modal features. Under deep\ninference, we design a two-branch framework to augment the image and text\nunimodal features, respectively merging with mutual modalities features, for\nfour predictions via dedicated detection and localization projections.\nExperiments on both mainstream and our proposed datasets demonstrate the\nsuperiority of the model. Codes and dataset are released at\nhttps://github.com/yunan-wang33/sdml."}
{"id": "2505.06814", "pdf": "https://arxiv.org/pdf/2505.06814", "abs": "https://arxiv.org/abs/2505.06814", "authors": ["Bin Li", "Shenxi Liu", "Yixuan Weng", "Yue Du", "Yuhang Tian", "Shoujun Zhou"], "title": "Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "12 pages, 5 figures, 4 tables", "summary": "Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the\n2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been\nintroduced to further advance research in multi-modal, multilingual, and\nmulti-hop medical instructional question answering (M4IVQA) systems, with a\nspecific focus on medical instructional videos. The M4IVQA challenge focuses on\nevaluating models that integrate information from medical instructional videos,\nunderstand multiple languages, and answer multi-hop questions requiring\nreasoning over various modalities. This task consists of three tracks:\nmulti-modal, multilingual, and multi-hop Temporal Answer Grounding in Single\nVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus\nRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer\nGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to\ndevelop algorithms capable of processing both video and text data,\nunderstanding multilingual queries, and providing relevant answers to multi-hop\nmedical questions. We believe the newly introduced M4IVQA challenge will drive\ninnovations in multimodal reasoning systems for healthcare scenarios,\nultimately contributing to smarter emergency response systems and more\neffective medical education platforms in multilingual communities. Our official\nwebsite is https://cmivqa.github.io/"}
{"id": "2505.06825", "pdf": "https://arxiv.org/pdf/2505.06825", "abs": "https://arxiv.org/abs/2505.06825", "authors": ["Thien Nhan Vo"], "title": "Active Learning for Multi-class Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "A principle bottleneck in image classification is the large number of\ntraining examples needed to train a classifier. Using active learning, we can\nreduce the number of training examples to teach a CNN classifier by\nstrategically selecting examples. Assigning values to image examples using\ndifferent uncertainty metrics allows the model to identify and select\nhigh-value examples in a smaller training set size. We demonstrate results for\ndigit recognition and fruit classification on the MNIST and Fruits360 data\nsets. We formally compare results for four different uncertainty metrics.\nFinally, we observe active learning is also effective on simpler (binary)\nclassification tasks, but marked improvement from random sampling is more\nevident on more difficult tasks. We show active learning is a viable algorithm\nfor image classification problems."}
{"id": "2505.06831", "pdf": "https://arxiv.org/pdf/2505.06831", "abs": "https://arxiv.org/abs/2505.06831", "authors": ["Miaoyun Zhao", "Qiang Zhang", "Chenrong Li"], "title": "Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification", "categories": ["cs.CV"], "comment": null, "summary": "Achieving group-robust generalization in the presence of spurious\ncorrelations remains a significant challenge, particularly when bias\nannotations are unavailable. Recent studies on Class-Conditional Distribution\nBalancing (CCDB) reveal that spurious correlations often stem from mismatches\nbetween the class-conditional and marginal distributions of bias attributes.\nThey achieve promising results by addressing this issue through simple\ndistribution matching in a bias-agnostic manner. However, CCDB approximates\neach distribution using a single Gaussian, which is overly simplistic and\nrarely holds in real-world applications. To address this limitation, we propose\na novel method called Bias Exploration via Overfitting (BEO), which captures\neach distribution in greater detail by modeling it as a mixture of latent\ngroups. Building on these group-level descriptions, we introduce a fine-grained\nvariant of CCDB, termed FG-CCDB, which performs more precise distribution\nmatching and balancing within each group. Through group-level reweighting,\nFG-CCDB learns sample weights from a global perspective, achieving stronger\nmitigation of spurious correlations without incurring substantial storage or\ncomputational costs. Extensive experiments demonstrate that BEO serves as a\nstrong proxy for ground-truth bias annotations and can be seamlessly integrated\nwith bias-supervised methods. Moreover, when combined with FG-CCDB, our method\nperforms on par with bias-supervised approaches on binary classification tasks\nand significantly outperforms them in highly biased multi-class scenarios."}
{"id": "2505.06840", "pdf": "https://arxiv.org/pdf/2505.06840", "abs": "https://arxiv.org/abs/2505.06840", "authors": ["Yixin Chen", "Shuai Zhang", "Boran Han", "Bernie Wang"], "title": "Visual Instruction Tuning with Chain of Region-of-Interest", "categories": ["cs.CV"], "comment": "N/A", "summary": "High-resolution (HR) images are pivotal for enhancing the recognition and\nunderstanding capabilities of multimodal large language models (MLLMs).\nHowever, directly increasing image resolution can significantly escalate\ncomputational demands. In this study, we propose a method called Chain of\nRegion-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating\nthe computational burden associated with high-resolution images for MLLMs.\nDrawing inspiration from the selective nature of the human visual system, we\nrecognize that not all regions within high-resolution images carry equal\nimportance. CoRoI seeks to identify and prioritize the most informative\nregions, thereby enhancing multimodal visual comprehension and recognition\nwhile circumventing the need for processing lengthy HR image tokens. Through\nextensive experiments on 11 benchmarks, we validate the efficacy of CoRoI\nacross varying sizes, ranging from 7B to 34B in parameters. Our models\nconsistently demonstrate superior performance across diverse multimodal\nbenchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all\nbenchmarks and our finetuned 34B model surpasses proprietary methods like\nGemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB,\nSEED-I, and MME."}
{"id": "2505.06853", "pdf": "https://arxiv.org/pdf/2505.06853", "abs": "https://arxiv.org/abs/2505.06853", "authors": ["Carolina Vargas-Ecos", "Edwin Salcedo"], "title": "Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach", "categories": ["cs.CV"], "comment": "Accepted for publication at the 6th BioSMART Conference, 2025", "summary": "According to the Pan American Health Organization, the number of cancer cases\nin Latin America was estimated at 4.2 million in 2022 and is projected to rise\nto 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone\ncancers affecting young people, is difficult to detect due to its unique\ntexture and intensity. Surgical removal of osteosarcoma requires precise safety\nmargins to ensure complete resection while preserving healthy tissue.\nTherefore, this study proposes a method for estimating the confidence interval\nof surgical safety margins in osteosarcoma surgery around the knee. The\nproposed approach uses MRI and X-ray data from open-source repositories,\ndigital processing techniques, and unsupervised learning algorithms (such as\nk-means clustering) to define tumor boundaries. Experimental results highlight\nthe potential for automated, patient-specific determination of safety margins."}
{"id": "2505.06855", "pdf": "https://arxiv.org/pdf/2505.06855", "abs": "https://arxiv.org/abs/2505.06855", "authors": ["Zhengmi Tang", "Yuto Mitsui", "Tomo Miyazaki", "Shinichiro Omachi"], "title": "Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies", "categories": ["cs.CV"], "comment": null, "summary": "Most existing text recognition methods are trained on large-scale synthetic\ndatasets due to the scarcity of labeled real-world datasets. Synthetic images,\nhowever, cannot faithfully reproduce real-world scenarios, such as uneven\nillumination, irregular layout, occlusion, and degradation, resulting in\nperformance disparities when handling complex real-world images. Recent\nself-supervised learning techniques, notably contrastive learning and masked\nimage modeling (MIM), narrow this domain gap by exploiting unlabeled real text\nimages. This study first analyzes the original Masked AutoEncoder (MAE) and\nobserves that random patch masking predominantly captures low-level textural\nfeatures but misses high-level contextual representations. To fully exploit the\nhigh-level contextual representations, we introduce random blockwise and span\nmasking in the text recognition task. These strategies can mask the continuous\nimage patches and completely remove some characters, forcing the model to infer\nrelationships among characters within a word. Our Multi-Masking Strategy (MMS)\nintegrates random patch, blockwise, and span masking into the MIM frame, which\njointly learns low and high-level textual representations. After fine-tuning\nwith real data, MMS outperforms the state-of-the-art self-supervised methods in\nvarious text-related tasks, including text recognition, segmentation, and\ntext-image super-resolution."}
{"id": "2505.06881", "pdf": "https://arxiv.org/pdf/2505.06881", "abs": "https://arxiv.org/abs/2505.06881", "authors": ["Hamd Jalil", "Ahmed Qazi", "Asim Iqbal"], "title": "NeuRN: Neuro-inspired Domain Generalization for Image Classification", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "14 pages, 7 figures, 1 table", "summary": "Domain generalization in image classification is a crucial challenge, with\nmodels often failing to generalize well across unseen datasets. We address this\nissue by introducing a neuro-inspired Neural Response Normalization (NeuRN)\nlayer which draws inspiration from neurons in the mammalian visual cortex,\nwhich aims to enhance the performance of deep learning architectures on unseen\ntarget domains by training deep learning models on a source domain. The\nperformance of these models is considered as a baseline and then compared\nagainst models integrated with NeuRN on image classification tasks. We perform\nexperiments across a range of deep learning architectures, including ones\nderived from Neural Architecture Search and Vision Transformer. Additionally,\nin order to shortlist models for our experiment from amongst the vast range of\ndeep neural networks available which have shown promising results, we also\npropose a novel method that uses the Needleman-Wunsch algorithm to compute\nsimilarity between deep learning architectures. Our results demonstrate the\neffectiveness of NeuRN by showing improvement against baseline in cross-domain\nimage classification tasks. Our framework attempts to establish a foundation\nfor future neuro-inspired deep learning models."}
{"id": "2505.06886", "pdf": "https://arxiv.org/pdf/2505.06886", "abs": "https://arxiv.org/abs/2505.06886", "authors": ["Ahmed Qazi", "Hamd Jalil", "Asim Iqbal"], "title": "Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "12 pages, 8 figures, 1 table", "summary": "The mouse is one of the most studied animal models in the field of systems\nneuroscience. Understanding the generalized patterns and decoding the neural\nrepresentations that are evoked by the diverse range of natural scene stimuli\nin the mouse visual cortex is one of the key quests in computational vision. In\nrecent years, significant parallels have been drawn between the primate visual\ncortex and hierarchical deep neural networks. However, their generalized\nefficacy in understanding mouse vision has been limited. In this study, we\ninvestigate the functional alignment between the mouse visual cortex and deep\nlearning models for object classification tasks. We first introduce a\ngeneralized representational learning strategy that uncovers a striking\nresemblance between the functional mapping of the mouse visual cortex and\nhigh-performing deep learning models on both top-down (population-level) and\nbottom-up (single cell-level) scenarios. Next, this representational similarity\nacross the two systems is further enhanced by the addition of Neural Response\nNormalization (NeuRN) layer, inspired by the activation profile of excitatory\nand inhibitory neurons in the visual cortex. To test the performance effect of\nNeuRN on real-world tasks, we integrate it into deep learning models and\nobserve significant improvements in their robustness against data shifts in\ndomain generalization tasks. Our work proposes a novel framework for comparing\nthe functional architecture of the mouse visual cortex with deep learning\nmodels. Our findings carry broad implications for the development of advanced\nAI models that draw inspiration from the mouse visual cortex, suggesting that\nthese models serve as valuable tools for studying the neural representations of\nthe mouse visual cortex and, as a result, enhancing their performance on\nreal-world tasks."}
{"id": "2505.06894", "pdf": "https://arxiv.org/pdf/2505.06894", "abs": "https://arxiv.org/abs/2505.06894", "authors": ["Ahmed Qazi", "Abdul Basit", "Asim Iqbal"], "title": "NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "18 pages, 6 figures", "summary": "Neural Radiance Fields (NeRF) have significantly advanced the field of novel\nview synthesis, yet their generalization across diverse scenes and conditions\nremains challenging. Addressing this, we propose the integration of a novel\nbrain-inspired normalization technique Neural Generalization (NeuGen) into\nleading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts\nthe domain-invariant features, thereby enhancing the models' generalization\ncapabilities. It can be seamlessly integrated into NeRF architectures and\ncultivates a comprehensive feature set that significantly improves accuracy and\nrobustness in image rendering. Through this integration, NeuGen shows improved\nperformance on benchmarks on diverse datasets across state-of-the-art NeRF\narchitectures, enabling them to generalize better across varied scenes. Our\ncomprehensive evaluations, both quantitative and qualitative, confirm that our\napproach not only surpasses existing models in generalizability but also\nmarkedly improves rendering quality. Our work exemplifies the potential of\nmerging neuroscientific principles with deep learning frameworks, setting a new\nprecedent for enhanced generalizability and efficiency in novel view synthesis.\nA demo of our study is available at https://neugennerf.github.io."}
{"id": "2505.06898", "pdf": "https://arxiv.org/pdf/2505.06898", "abs": "https://arxiv.org/abs/2505.06898", "authors": ["Honglong Yang", "Shanshan Song", "Yi Qin", "Lehan Wang", "Haonan Wang", "Xinpeng Ding", "Qixiang Zhang", "Bodong Du", "Xiaomeng Li"], "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generalist Medical AI (GMAI) systems have demonstrated expert-level\nperformance in biomedical perception tasks, yet their clinical utility remains\nlimited by inadequate multi-modal explainability and suboptimal prognostic\ncapabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI\nassistant that integrates textual and visual interpretability to support\ntransparent and trustworthy medical decision-making. XMedGPT not only produces\naccurate diagnostic and descriptive outputs, but also grounds referenced\nanatomical sites within medical images, bridging critical gaps in\ninterpretability and enhancing clinician usability. To support real-world\ndeployment, we introduce a reliability indexing mechanism that quantifies\nuncertainty through consistency-based assessment via interactive\nquestion-answering. We validate XMedGPT across four pillars: multi-modal\ninterpretability, uncertainty quantification, and prognostic modeling, and\nrigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical\nregions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between\nvisual rationales and clinical outcomes. For uncertainty estimation, it attains\nan AUC of 0.862 on visual question answering and 0.764 on radiology report\ngeneration. In survival and recurrence prediction for lung and glioma cancers,\nit surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.\nRigorous benchmarking across 347 datasets covers 40 imaging modalities and\nexternal validation spans 4 anatomical systems confirming exceptional\ngeneralizability, with performance gains surpassing existing GMAI by 20.7% for\nin-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,\nXMedGPT represents a significant leap forward in clinician-centric AI\nintegration, offering trustworthy and scalable support for diverse healthcare\napplications."}
{"id": "2505.06903", "pdf": "https://arxiv.org/pdf/2505.06903", "abs": "https://arxiv.org/abs/2505.06903", "authors": ["Yuanzhuo Wang", "Junwen Duan", "Xinyu Li", "Jianxin Wang"], "title": "CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal medical image analysis is essential for clinical decision-making,\nyet existing methods either align images and text at a coarse level - causing\npotential semantic mismatches - or depend solely on visual information, lacking\nmedical semantic integration. We present CheXLearner, the first end-to-end\nframework that unifies anatomical region detection, Riemannian manifold-based\nstructure alignment, and fine-grained regional semantic guidance. Our proposed\nMed-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to\nrobustly align anatomical structures and capture pathologically meaningful\ndiscrepancies across temporal chest X-rays. By introducing regional progression\ndescriptions as supervision, CheXLearner achieves enhanced cross-modal\nrepresentation learning and supports dynamic low-level feature optimization.\nExperiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and\n80.32% (+11.05%) F1-score on anatomical region progression detection -\nsubstantially outperforming state-of-the-art baselines, especially in\nstructurally complex regions. Additionally, our model attains a 91.52% average\nAUC score in downstream disease classification, validating its superior feature\nrepresentation."}
{"id": "2505.06905", "pdf": "https://arxiv.org/pdf/2505.06905", "abs": "https://arxiv.org/abs/2505.06905", "authors": ["Jian Song", "Hongruixuan Chen", "Naoto Yokoya"], "title": "Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Monocular height estimation (MHE) from very-high-resolution (VHR) remote\nsensing imagery via deep learning is notoriously challenging due to the lack of\nsufficient structural information. Conventional digital elevation models\n(DEMs), typically derived from airborne LiDAR or multi-view stereo, remain\ncostly and geographically limited. Recently, models trained on synthetic data\nand refined through domain adaptation have shown remarkable performance in MHE,\nyet it remains unclear how these models make predictions or how reliable they\ntruly are. In this paper, we investigate a state-of-the-art MHE model trained\npurely on synthetic data to explore where the model looks when making height\npredictions. Through systematic analyses, we find that the model relies heavily\non shadow cues, a factor that can lead to overestimation or underestimation of\nheights when shadows deviate from expected norms. Furthermore, the inherent\ndifficulty of evaluating regression tasks with the human eye underscores\nadditional limitations of purely synthetic training. To address these issues,\nwe propose a novel correction pipeline that integrates sparse, imperfect global\nLiDAR measurements (ICESat-2) with deep-learning outputs to improve local\naccuracy and achieve spatially consistent corrections. Our method comprises two\nstages: pre-processing raw ICESat-2 data, followed by a random forest-based\napproach to densely refine height estimates. Experiments in three\nrepresentative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal\nsubstantial error reductions, with mean absolute error (MAE) decreased by\n22.8\\%, 6.9\\%, and 4.9\\%, respectively. These findings highlight the critical\nrole of shadow awareness in synthetic data-driven models and demonstrate how\nfusing imperfect real-world LiDAR data can bolster the robustness of MHE,\npaving the way for more reliable and scalable 3D mapping solutions."}
{"id": "2505.06912", "pdf": "https://arxiv.org/pdf/2505.06912", "abs": "https://arxiv.org/abs/2505.06912", "authors": ["Chao Ding", "Mouxiao Bian", "Pengcheng Chen", "Hongliang Zhang", "Tianbin Li", "Lihao Liu", "Jiayuan Chen", "Zhuoran Li", "Yabei Zhong", "Yongqi Liu", "Haiqing Huang", "Dongming Shan", "Junjun He", "Jie Xu"], "title": "Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI", "categories": ["cs.CV"], "comment": null, "summary": "Despite strong performance in medical question-answering, the clinical\nadoption of Large Language Models (LLMs) is critically hampered by their opaque\n'black-box' reasoning, limiting clinician trust. This challenge is compounded\nby the predominant reliance of current medical LLMs on corpora from scientific\nliterature or synthetic data, which often lack the granular expert validation\nand high clinical relevance essential for advancing their specialized medical\ncapabilities. To address these critical gaps, we introduce a highly clinically\nrelevant dataset with 31,247 medical question-answer pairs, each accompanied by\nexpert-validated chain-of-thought (CoT) explanations. This resource, spanning\nmultiple clinical domains, was curated via a scalable human-LLM hybrid\npipeline: LLM-generated rationales were iteratively reviewed, scored, and\nrefined by medical experts against a structured rubric, with substandard\noutputs revised through human effort or guided LLM regeneration until expert\nconsensus. This publicly available dataset provides a vital source for the\ndevelopment of medical LLMs that capable of transparent and verifiable\nreasoning, thereby advancing safer and more interpretable AI in medicine."}
{"id": "2505.06920", "pdf": "https://arxiv.org/pdf/2505.06920", "abs": "https://arxiv.org/abs/2505.06920", "authors": ["Timing Li", "Bing Cao", "Pengfei Zhu", "Bin Xiao", "Qinghua Hu"], "title": "Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Acquiring accurately aligned multi-modal image pairs is fundamental for\nachieving high-quality multi-modal image fusion. To address the lack of ground\ntruth in current multi-modal image registration and fusion methods, we propose\na novel self-supervised \\textbf{B}i-directional\n\\textbf{S}elf-\\textbf{R}egistration framework (\\textbf{B-SR}). Specifically,\nB-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator\n(IPDG) to achieve self-supervised global-local registration. Visible-infrared\nimage pairs with spatially misaligned differences are aligned to obtain global\ndifferences through the registration module. The same image pairs are processed\nby PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain\nlocal differences. IPDG converts the obtained local differences into\npseudo-global differences, which are used to perform global-local difference\nconsistency with the global differences. Furthermore, aiming at eliminating the\neffect of modal gaps on the registration module, we design a neighborhood\ndynamic alignment loss to achieve cross-modal image edge alignment. Extensive\nexperiments on misaligned multi-modal images demonstrate the effectiveness of\nthe proposed method in multi-modal image alignment and fusion against the\ncompeting methods. Our code will be publicly available."}
{"id": "2505.06937", "pdf": "https://arxiv.org/pdf/2505.06937", "abs": "https://arxiv.org/abs/2505.06937", "authors": ["Fei Zhou", "Yi Li", "Mingqing Zhu"], "title": "Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, the dual-optical attention fusion crowd head point counting\nmodel (TAPNet) is proposed to address the problem of the difficulty of accurate\ncounting in complex scenes such as crowd dense occlusion and low light in crowd\ncounting tasks under UAV view. The model designs a dual-optical attention\nfusion module (DAFP) by introducing complementary information from infrared\nimages to improve the accuracy and robustness of all-day crowd counting. In\norder to fully utilize different modal information and solve the problem of\ninaccurate localization caused by systematic misalignment between image pairs,\nthis paper also proposes an adaptive two-optical feature decomposition fusion\nmodule (AFDF). In addition, we optimize the training strategy to improve the\nmodel robustness through spatial random offset data augmentation. Experiments\non two challenging public datasets, DroneRGBT and GAIIC2, show that the\nproposed method outperforms existing techniques in terms of performance,\nespecially in challenging dense low-light scenes. Code is available at\nhttps://github.com/zz-zik/TAPNet"}
{"id": "2505.06948", "pdf": "https://arxiv.org/pdf/2505.06948", "abs": "https://arxiv.org/abs/2505.06948", "authors": ["Pan Du", "Wangbo Zhao", "Xinai Lu", "Nian Liu", "Zhikai Li", "Chaoyu Gong", "Suyun Zhao", "Hong Chen", "Cuiping Li", "Kai Wang", "Yang You"], "title": "Unsupervised Learning for Class Distribution Mismatch", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Class distribution mismatch (CDM) refers to the discrepancy between class\ndistributions in training data and target tasks. Previous methods address this\nby designing classifiers to categorize classes known during training, while\ngrouping unknown or new classes into an \"other\" category. However, they focus\non semi-supervised scenarios and heavily rely on labeled data, limiting their\napplicability and performance. To address this, we propose Unsupervised\nLearning for Class Distribution Mismatch (UCDM), which constructs\npositive-negative pairs from unlabeled data for classifier training. Our\napproach randomly samples images and uses a diffusion model to add or erase\nsemantic classes, synthesizing diverse training pairs. Additionally, we\nintroduce a confidence-based labeling mechanism that iteratively assigns\npseudo-labels to valuable real-world data and incorporates them into the\ntraining process. Extensive experiments on three datasets demonstrate UCDM's\nsuperiority over previous semi-supervised methods. Specifically, with a 60%\nmismatch proportion on Tiny-ImageNet dataset, our approach, without relying on\nlabeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,\nand 72.5% in classifying known, unknown, and new classes."}
{"id": "2505.06951", "pdf": "https://arxiv.org/pdf/2505.06951", "abs": "https://arxiv.org/abs/2505.06951", "authors": ["Seokjun Kwon", "Jeongmin Shin", "Namil Kim", "Soonmin Hwang", "Yukyung Choi"], "title": "Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": "7 pages, 4 figures, International Conference on Robotics and\n  Automation(ICRA) 2025", "summary": "In autonomous driving, thermal image semantic segmentation has emerged as a\ncritical research area, owing to its ability to provide robust scene\nunderstanding under adverse visual conditions. In particular, unsupervised\ndomain adaptation (UDA) for thermal image segmentation can be an efficient\nsolution to address the lack of labeled thermal datasets. Nevertheless, since\nthese methods do not effectively utilize the complementary information between\nRGB and thermal images, they significantly decrease performance during domain\nadaptation. In this paper, we present a comprehensive study on cross-spectral\nUDA for thermal image semantic segmentation. We first propose a novel masked\nmutual learning strategy that promotes complementary information exchange by\nselectively transferring results between each spectral model while masking out\nuncertain regions. Additionally, we introduce a novel prototypical\nself-supervised loss designed to enhance the performance of the thermal\nsegmentation model in nighttime scenarios. This approach addresses the\nlimitations of RGB pre-trained networks, which cannot effectively transfer\nknowledge under low illumination due to the inherent constraints of RGB\nsensors. In experiments, our method achieves higher performance over previous\nUDA methods and comparable performance to state-of-the-art supervised methods."}
{"id": "2505.06975", "pdf": "https://arxiv.org/pdf/2505.06975", "abs": "https://arxiv.org/abs/2505.06975", "authors": ["Wei Shang", "Dongwei Ren", "Wanying Zhang", "Pengfei Zhu", "Qinghua Hu", "Wangmeng Zuo"], "title": "High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution", "categories": ["cs.CV", "I.4.3"], "comment": "10 pages, 6 figures, 5 tables", "summary": "The primary challenge in accelerating image super-resolution lies in reducing\ncomputation while maintaining performance and adaptability. Motivated by the\nobservation that high-frequency regions (e.g., edges and textures) are most\ncritical for reconstruction, we propose a training-free adaptive masking module\nfor acceleration that dynamically focuses computation on these challenging\nareas. Specifically, our method first extracts high-frequency components via\nGaussian blur subtraction and adaptively generates binary masks using K-means\nclustering to identify regions requiring intensive processing. Our method can\nbe easily integrated with both CNNs and Transformers. For CNN-based\narchitectures, we replace standard $3 \\times 3$ convolutions with an unfold\noperation followed by $1 \\times 1$ convolutions, enabling pixel-wise sparse\ncomputation guided by the mask. For Transformer-based models, we partition the\nmask into non-overlapping windows and selectively process tokens based on their\naverage values. During inference, unnecessary pixels or windows are pruned,\nsignificantly reducing computation. Moreover, our method supports\ndilation-based mask adjustment to control the processing scope without\nretraining, and is robust to unseen degradations (e.g., noise, compression).\nExtensive experiments on benchmarks demonstrate that our method reduces FLOPs\nby 24--43% for state-of-the-art models (e.g., CARN, SwinIR) while achieving\ncomparable or better quantitative metrics. The source code is available at\nhttps://github.com/shangwei5/AMSR"}
{"id": "2505.06982", "pdf": "https://arxiv.org/pdf/2505.06982", "abs": "https://arxiv.org/abs/2505.06982", "authors": ["Md. Naimur Asif Borno", "Md Sakib Hossain Shovon", "MD Hanif Sikder", "Iffat Firozy Rimi", "Tahani Jaser Alahmadi", "Mohammad Ali Moni"], "title": "Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in image-based medical disease detection encounters\nchallenges such as limited annotated data sets, inadequate spatial feature\nanalysis, data security issues, and inefficient training frameworks. This study\nintroduces a data-efficient image transformer (DeIT)-based approach that\novercomes these challenges by utilizing multiscale patch embedding for better\nfeature extraction and stratified weighted random sampling to address class\nimbalance. The model also incorporates a LoRA-enhanced transformer encoder, a\ndistillation framework, and federated learning for decentralized training,\nimproving both efficiency and data security. Consequently, it achieves\nstate-of-the-art performance, with the highest AUC, F1 score, precision,\nminimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations\nimprove interpretability by highlighting critical pathological regions,\nenhancing the model's clinical relevance. These results highlight the potential\nof this approach to advance AI-powered medical imaging and disease detection."}
{"id": "2505.06985", "pdf": "https://arxiv.org/pdf/2505.06985", "abs": "https://arxiv.org/abs/2505.06985", "authors": ["Panwen Hu", "Jiehui Huang", "Qiang Sun", "Xiaodan Liang"], "title": "BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation", "categories": ["cs.CV"], "comment": null, "summary": "Both zero-shot and tuning-based customized text-to-image (CT2I) generation\nhave made significant progress for storytelling content creation. In contrast,\nresearch on customized text-to-video (CT2V) generation remains relatively\nlimited. Existing zero-shot CT2V methods suffer from poor generalization, while\nanother line of work directly combining tuning-based T2I models with temporal\nmotion modules often leads to the loss of structural and texture information.\nTo bridge this gap, we propose an autoregressive structure and texture\npropagation module (STPM), which extracts key structural and texture features\nfrom the reference subject and injects them autoregressively into each video\nframe to enhance consistency. Additionally, we introduce a test-time reward\noptimization (TTRO) method to further refine fine-grained details. Quantitative\nand qualitative experiments validate the effectiveness of STPM and TTRO,\ndemonstrating improvements of 7.8 and 13.1 in CLIP-I and DINO consistency\nmetrics over the baseline, respectively."}
{"id": "2505.06991", "pdf": "https://arxiv.org/pdf/2505.06991", "abs": "https://arxiv.org/abs/2505.06991", "authors": ["Chih-Chung Hsu", "I-Hsuan Wu", "Wen-Hai Tseng", "Ching-Heng Cheng", "Ming-Hsuan Wu", "Jin-Hui Jiang", "Yu-Jou Hsiao"], "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "This report presents our semantic segmentation framework developed by team\nACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which\nfocuses on parsing outdoor scenes into nine semantic categories under\nreal-world conditions. Our method integrates a Swin Transformer backbone\nenhanced with Rotary Position Embedding (RoPE) for improved spatial\ngeneralization, alongside a Color Shift Estimation-and-Correction module\ndesigned to compensate for illumination inconsistencies in natural\nenvironments. To further improve training stability, we adopt a quantile-based\ndenoising strategy that downweights the top 2.5\\% of highest-error pixels,\ntreating them as noise and suppressing their influence during optimization.\nEvaluated on the official GOOSE test set, our approach achieved a mean\nIntersection over Union (mIoU) of 0.848, demonstrating the effectiveness of\ncombining color correction, positional encoding, and error-aware denoising in\nrobust semantic segmentation."}
{"id": "2505.06995", "pdf": "https://arxiv.org/pdf/2505.06995", "abs": "https://arxiv.org/abs/2505.06995", "authors": ["Md. Naimur Asif Borno", "Md Sakib Hossain Shovon", "Asmaa Soliman Al-Moisheer", "Mohammad Ali Moni"], "title": "Replay-Based Continual Learning with Dual-Layered Distillation and a Streamlined U-Net for Efficient Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-to-image diffusion models are hindered by high\ncomputational demands, limiting accessibility and scalability. This paper\nintroduces KDC-Diff, a novel stable diffusion framework that enhances\nefficiency while maintaining image quality. KDC-Diff features a streamlined\nU-Net architecture with nearly half the parameters of the original U-Net\n(482M), significantly reducing model complexity. We propose a dual-layered\ndistillation strategy to ensure high-fidelity generation, transferring semantic\nand structural insights from a teacher to a compact student model while\nminimizing quality degradation. Additionally, replay-based continual learning\nis integrated to mitigate catastrophic forgetting, allowing the model to retain\nprior knowledge while adapting to new data. Despite operating under extremely\nlow computational resources, KDC-Diff achieves state-of-the-art performance on\nthe Oxford Flowers and Butterflies & Moths 100 Species datasets, demonstrating\ncompetitive metrics such as FID, CLIP, and LPIPS. Moreover, it significantly\nreduces inference time compared to existing models. These results establish\nKDC-Diff as a highly efficient and adaptable solution for text-to-image\ngeneration, particularly in computationally constrained environments."}
{"id": "2505.07001", "pdf": "https://arxiv.org/pdf/2505.07001", "abs": "https://arxiv.org/abs/2505.07001", "authors": ["Bidur Khanal", "Sandesh Pokhrel", "Sanjay Bhandari", "Ramesh Rana", "Nikesh Shrestha", "Ram Bahadur Gurung", "Cristian Linte", "Angus Watson", "Yash Raj Shrestha", "Binod Bhattarai"], "title": "Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) are becoming increasingly popular in the\nmedical domain, bridging the gap between medical images and clinical language.\nExisting VLMs demonstrate an impressive ability to comprehend medical images\nand text queries to generate detailed, descriptive diagnostic medical reports.\nHowever, hallucination--the tendency to generate descriptions that are\ninconsistent with the visual content--remains a significant issue in VLMs, with\nparticularly severe implications in the medical field. To facilitate VLM\nresearch on gastrointestinal (GI) image analysis and study hallucination, we\ncurate a multimodal image-text GI dataset: Gut-VLM. This dataset is created\nusing a two-stage pipeline: first, descriptive medical reports of Kvasir-v2\nimages are generated using ChatGPT, which introduces some hallucinated or\nincorrect texts. In the second stage, medical experts systematically review\nthese reports, and identify and correct potential inaccuracies to ensure\nhigh-quality, clinically reliable annotations. Unlike traditional datasets that\ncontain only descriptive texts, our dataset also features tags identifying\nhallucinated sentences and their corresponding corrections. A common approach\nto reducing hallucination in VLM is to finetune the model on a small-scale,\nproblem-specific dataset. However, we take a different strategy using our\ndataset. Instead of finetuning the VLM solely for generating textual reports,\nwe finetune it to detect and correct hallucinations, an approach we call\nhallucination-aware finetuning. Our results show that this approach is better\nthan simply finetuning for descriptive report generation. Additionally, we\nconduct an extensive evaluation of state-of-the-art VLMs across several\nmetrics, establishing a benchmark. GitHub Repo:\nhttps://github.com/bhattarailab/Hallucination-Aware-VLM."}
{"id": "2505.07003", "pdf": "https://arxiv.org/pdf/2505.07003", "abs": "https://arxiv.org/abs/2505.07003", "authors": ["Peng Li", "Suizhi Ma", "Jialiang Chen", "Yuan Liu", "Chongyi Zhang", "Wei Xue", "Wenhan Luo", "Alla Sheffer", "Wenping Wang", "Yike Guo"], "title": "CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation", "categories": ["cs.CV"], "comment": "Siggraph 2025", "summary": "Recently, 3D generation methods have shown their powerful ability to automate\n3D model creation. However, most 3D generation methods only rely on an input\nimage or a text prompt to generate a 3D model, which lacks the control of each\ncomponent of the generated 3D model. Any modifications of the input image lead\nto an entire regeneration of the 3D models. In this paper, we introduce a new\nmethod called CMD that generates a 3D model from an input image while enabling\nflexible local editing of each component of the 3D model. In CMD, we formulate\nthe 3D generation as a conditional multiview diffusion model, which takes the\nexisting or known parts as conditions and generates the edited or added\ncomponents. This conditional multiview diffusion model not only allows the\ngeneration of 3D models part by part but also enables local editing of 3D\nmodels according to the local revision of the input image without changing\nother 3D parts. Extensive experiments are conducted to demonstrate that CMD\ndecomposes a complex 3D generation task into multiple components, improving the\ngeneration quality. Meanwhile, CMD enables efficient and flexible local editing\nof a 3D model by just editing one rendered image."}
{"id": "2505.07007", "pdf": "https://arxiv.org/pdf/2505.07007", "abs": "https://arxiv.org/abs/2505.07007", "authors": ["Zhengye Zhang", "Sirui Zhao", "Shifeng Liu", "Shukang Yin", "Xinglong Mao", "Tong Xu", "Enhong Chen"], "title": "MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expressions (MEs) are crucial psychological responses with significant\npotential for affective computing. However, current automatic micro-expression\nrecognition (MER) research primarily focuses on discrete emotion\nclassification, neglecting a convincing analysis of the subtle dynamic\nmovements and inherent emotional cues. The rapid progress in multimodal large\nlanguage models (MLLMs), known for their strong multimodal comprehension and\nlanguage generation abilities, offers new possibilities. MLLMs have shown\nsuccess in various vision-language tasks, indicating their potential to\nunderstand MEs comprehensively, including both fine-grained motion patterns and\nunderlying emotional semantics. Nevertheless, challenges remain due to the\nsubtle intensity and short duration of MEs, as existing MLLMs are not designed\nto capture such delicate frame-level facial dynamics. In this paper, we propose\na novel Micro-Expression Large Language Model (MELLM), which incorporates a\nsubtle facial motion perception strategy with the strong inference capabilities\nof MLLMs, representing the first exploration of MLLMs in the domain of ME\nanalysis. Specifically, to explicitly guide the MLLM toward motion-sensitive\nregions, we construct an interpretable motion-enhanced color map by fusing\nonset-apex optical flow dynamics with the corresponding grayscale onset frame\nas the model input. Additionally, specialized fine-tuning strategies are\nincorporated to further enhance the model's visual perception of MEs.\nFurthermore, we construct an instruction-description dataset based on Facial\nAction Coding System (FACS) annotations and emotion labels to train our MELLM.\nComprehensive evaluations across multiple benchmark datasets demonstrate that\nour model exhibits superior robustness and generalization capabilities in ME\nunderstanding (MEU). Code is available at https://github.com/zyzhangUstc/MELLM."}
{"id": "2505.07013", "pdf": "https://arxiv.org/pdf/2505.07013", "abs": "https://arxiv.org/abs/2505.07013", "authors": ["Jitesh Joshi", "Youngjun Cho"], "title": "Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages, 6 figures", "summary": "Remote physiological sensing using camera-based technologies offers\ntransformative potential for non-invasive vital sign monitoring across\nhealthcare and human-computer interaction domains. Although deep learning\napproaches have advanced the extraction of physiological signals from video\ndata, existing methods have not been sufficiently assessed for their robustness\nto domain shifts. These shifts in remote physiological sensing include\nvariations in ambient conditions, camera specifications, head movements, facial\nposes, and physiological states which often impact real-world performance\nsignificantly. Cross-dataset evaluation provides an objective measure to assess\ngeneralization capabilities across these domain shifts. We introduce Target\nSignal Constrained Factorization module (TSFM), a novel multidimensional\nattention mechanism that explicitly incorporates physiological signal\ncharacteristics as factorization constraints, allowing more precise feature\nextraction. Building on this innovation, we present MMRPhys, an efficient\ndual-branch 3D-CNN architecture designed for simultaneous multitask estimation\nof photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal\nRGB and thermal video inputs. Through comprehensive cross-dataset evaluation on\nfive benchmark datasets, we demonstrate that MMRPhys with TSFM significantly\noutperforms state-of-the-art methods in generalization across domain shifts for\nrPPG and rRSP estimation, while maintaining a minimal inference latency\nsuitable for real-time applications. Our approach establishes new benchmarks\nfor robust multitask and multimodal physiological sensing and offers a\ncomputationally efficient framework for practical deployment in unconstrained\nenvironments. The web browser-based application featuring on-device real-time\ninference of MMRPhys model is available at\nhttps://physiologicailab.github.io/mmrphys-live"}
{"id": "2505.07019", "pdf": "https://arxiv.org/pdf/2505.07019", "abs": "https://arxiv.org/abs/2505.07019", "authors": ["Khang Nguyen Quoc", "Lan Le Thi Thu", "Luyl-Da Quach"], "title": "A Vision-Language Foundation Model for Leaf Disease Identification", "categories": ["cs.CV"], "comment": null, "summary": "Leaf disease identification plays a pivotal role in smart agriculture.\nHowever, many existing studies still struggle to integrate image and textual\nmodalities to compensate for each other's limitations. Furthermore, many of\nthese approaches rely on pretraining with constrained datasets such as\nImageNet, which lack domain-specific information. We propose SCOLD (Soft-target\nCOntrastive learning for Leaf Disease identification), a context-aware\nvision-language foundation model tailored to address these challenges for\nagricultural tasks. SCOLD is developed using a diverse corpus of plant leaf\nimages and corresponding symptom descriptions, comprising over 186,000\nimage-caption pairs aligned with 97 unique concepts. Through task-agnostic\npretraining, SCOLD leverages contextual soft targets to mitigate overconfidence\nin contrastive learning by smoothing labels, thereby improving model\ngeneralization and robustness on fine-grained classification tasks.\nExperimental results demonstrate that SCOLD outperforms existing\nvision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across\nseveral benchmarks, including zero-shot and few-shot classification, image-text\nretrieval, and image classification, while maintaining a competitive parameter\nfootprint. Ablation studies further highlight SCOLD's effectiveness in contrast\nto its counterparts. The proposed approach significantly advances the\nagricultural vision-language foundation model, offering strong performance with\nminimal or no supervised fine-tuning. This work lays a solid groundwork for\nfuture research on models trained with long-form and simplified contexts, tasks\ninvolving class ambiguity, and multi-modal systems for intelligent plant\ndisease diagnostics. The code for this study is available at\nhttps://huggingface.co/enalis/scold"}
{"id": "2505.07032", "pdf": "https://arxiv.org/pdf/2505.07032", "abs": "https://arxiv.org/abs/2505.07032", "authors": ["Fei Zhao", "Runlin Zhang", "Chengcui Zhang", "Nitesh Saxena"], "title": "MarkMatch: Same-Hand Stuffing Detection", "categories": ["cs.CV"], "comment": null, "summary": "We present MarkMatch, a retrieval system for detecting whether two paper\nballot marks were filled by the same hand. Unlike the previous SOTA method\nBubbleSig, which used binary classification on isolated mark pairs, MarkMatch\nranks stylistic similarity between a query mark and a mark in the database\nusing contrastive learning. Our model is trained with a dense batch similarity\nmatrix and a dual loss objective. Each sample is contrasted against many\nnegatives within each batch, enabling the model to learn subtle handwriting\ndifference and improve generalization under handwriting variation and visual\nnoise, while diagonal supervision reinforces high confidence on true matches.\nThe model achieves an F1 score of 0.943, surpassing BubbleSig's best\nperformance. MarkMatch also integrates Segment Anything Model for flexible mark\nextraction via box- or point-based prompts. The system offers election auditors\na practical tool for visual, non-biometric investigation of suspicious ballots."}
{"id": "2505.07040", "pdf": "https://arxiv.org/pdf/2505.07040", "abs": "https://arxiv.org/abs/2505.07040", "authors": ["Zhengyang Lu", "Bingjie Lu", "Weifan Wang", "Feng Wang"], "title": "Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection", "categories": ["cs.CV"], "comment": null, "summary": "Fabric defect detection confronts two fundamental challenges. First,\nconventional non-maximum suppression disrupts gradient flow, which hinders\ngenuine end-to-end learning. Second, acquiring pixel-level annotations at\nindustrial scale is prohibitively costly. Addressing these limitations, we\npropose a differentiable NMS framework for fabric defect detection that\nachieves superior localization precision through end-to-end optimization. We\nreformulate NMS as a differentiable bipartite matching problem solved through\nthe Sinkhorn-Knopp algorithm, maintaining uninterrupted gradient flow\nthroughout the network. This approach specifically targets the irregular\nmorphologies and ambiguous boundaries of fabric defects by integrating proposal\nquality, feature similarity, and spatial relationships. Our entropy-constrained\nmask refinement mechanism further enhances localization precision through\nprincipled uncertainty modeling. Extensive experiments on the Tianchi fabric\ndefect dataset demonstrate significant performance improvements over existing\nmethods while maintaining real-time speeds suitable for industrial deployment.\nThe framework exhibits remarkable adaptability across different architectures\nand generalizes effectively to general object detection tasks."}
{"id": "2505.07050", "pdf": "https://arxiv.org/pdf/2505.07050", "abs": "https://arxiv.org/abs/2505.07050", "authors": ["Binbin Wei", "Yuhang Zhang", "Shishun Tian", "Muxin Liao", "Wei Li", "Wenbin Zou"], "title": "Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) aims to align source and target domain\ndistributions to close the domain gap, but still struggles with obtaining the\ntarget data. Fortunately, Domain Generalization (DG) excels without the need\nfor any target data. Recent works expose that depth maps contribute to improved\ngeneralized performance in the UDA tasks, but they ignore the noise and holes\nin depth maps due to device and environmental factors, failing to sufficiently\nand effectively learn domain-invariant representation. Although\nhigh-sensitivity region suppression has shown promising results in learning\ndomain-invariant features, existing methods cannot be directly applicable to\ndepth maps due to their unique characteristics. Hence, we propose a novel\nframework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal\nstylization flow (DSSS), focusing on learning domain-invariant features from\ndepth maps for the DG semantic segmentation. Specifically, we propose the RGB-D\ninter-modal stylization flow to generate stylized depth maps for sensitivity\ndetection, cleverly utilizing RGB information as the stylization source. Then,\na class-wise soft spatial sensitivity suppression is designed to identify and\nemphasize non-sensitive depth features that contain more domain-invariant\ninformation. Furthermore, an RGB-D soft alignment loss is proposed to ensure\nthat the stylized depth maps only align part of the RGB features while still\nretaining the unique depth information. To our best knowledge, our DSSS\nframework is the first work to integrate RGB and Depth information in the\nmulti-class DG semantic segmentation task. Extensive experiments over multiple\nbackbone networks show that our framework achieves remarkable performance\nimprovement."}
{"id": "2505.07057", "pdf": "https://arxiv.org/pdf/2505.07057", "abs": "https://arxiv.org/abs/2505.07057", "authors": ["Junhao Xia", "Chaoyang Zhang", "Yecheng Zhang", "Chengyang Zhou", "Zhichang Wang", "Bochun Liu", "Dongshuo Yin"], "title": "DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video generation based on diffusion models presents a challenging multimodal\ntask, with video editing emerging as a pivotal direction in this field. Recent\nvideo editing approaches primarily fall into two categories: training-required\nand training-free methods. While training-based methods incur high\ncomputational costs, training-free alternatives often yield suboptimal\nperformance. To address these limitations, we propose DAPE, a high-quality yet\ncost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for\nvideo editing. In the first stage, we design an efficient norm-tuning method to\nenhance temporal consistency in generated videos. The second stage introduces a\nvision-friendly adapter to improve visual quality. Additionally, we identify\ncritical shortcomings in existing benchmarks, including limited category\ndiversity, imbalanced object distribution, and inconsistent frame counts. To\nmitigate these issues, we curate a large dataset benchmark comprising 232\nvideos with rich annotations and 6 editing prompts, enabling objective and\ncomprehensive evaluation of advanced methods. Extensive experiments on existing\ndatasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate\nthat DAPE significantly improves temporal coherence and text-video alignment\nwhile outperforming previous state-of-the-art approaches."}
{"id": "2505.07062", "pdf": "https://arxiv.org/pdf/2505.07062", "abs": "https://arxiv.org/abs/2505.07062", "authors": ["Dong Guo", "Faming Wu", "Feida Zhu", "Fuxing Leng", "Guang Shi", "Haobin Chen", "Haoqi Fan", "Jian Wang", "Jianyu Jiang", "Jiawei Wang", "Jingji Chen", "Jingjia Huang", "Kang Lei", "Liping Yuan", "Lishu Luo", "Pengfei Liu", "Qinghao Ye", "Rui Qian", "Shen Yan", "Shixiong Zhao", "Shuai Peng", "Shuangye Li", "Sihang Yuan", "Sijin Wu", "Tianheng Cheng", "Weiwei Liu", "Wenqian Wang", "Xianhan Zeng", "Xiao Liu", "Xiaobo Qin", "Xiaohan Ding", "Xiaojun Xiao", "Xiaoying Zhang", "Xuanwei Zhang", "Xuehan Xiong", "Yanghua Peng", "Yangrui Chen", "Yanwei Li", "Yanxu Hu", "Yi Lin", "Yiyuan Hu", "Yiyuan Zhang", "Youbin Wu", "Yu Li", "Yudong Liu", "Yue Ling", "Yujia Qin", "Zanbo Wang", "Zhiwu He", "Aoxue Zhang", "Bairen Yi", "Bencheng Liao", "Can Huang", "Can Zhang", "Chaorui Deng", "Chaoyi Deng", "Cheng Lin", "Cheng Yuan", "Chenggang Li", "Chenhui Gou", "Chenwei Lou", "Chengzhi Wei", "Chundian Liu", "Chunyuan Li", "Deyao Zhu", "Donghong Zhong", "Feng Li", "Feng Zhang", "Gang Wu", "Guodong Li", "Guohong Xiao", "Haibin Lin", "Haihua Yang", "Haoming Wang", "Heng Ji", "Hongxiang Hao", "Hui Shen", "Huixia Li", "Jiahao Li", "Jialong Wu", "Jianhua Zhu", "Jianpeng Jiao", "Jiashi Feng", "Jiaze Chen", "Jianhui Duan", "Jihao Liu", "Jin Zeng", "Jingqun Tang", "Jingyu Sun", "Joya Chen", "Jun Long", "Junda Feng", "Junfeng Zhan", "Junjie Fang", "Junting Lu", "Kai Hua", "Kai Liu", "Kai Shen", "Kaiyuan Zhang", "Ke Shen", "Ke Wang", "Keyu Pan", "Kun Zhang", "Kunchang Li", "Lanxin Li", "Lei Li", "Lei Shi", "Li Han", "Liang Xiang", "Liangqiang Chen", "Lin Chen", "Lin Li", "Lin Yan", "Liying Chi", "Longxiang Liu", "Mengfei Du", "Mingxuan Wang", "Ningxin Pan", "Peibin Chen", "Pengfei Chen", "Pengfei Wu", "Qingqing Yuan", "Qingyao Shuai", "Qiuyan Tao", "Renjie Zheng", "Renrui Zhang", "Ru Zhang", "Rui Wang", "Rui Yang", "Rui Zhao", "Shaoqiang Xu", "Shihao Liang", "Shipeng Yan", "Shu Zhong", "Shuaishuai Cao", "Shuangzhi Wu", "Shufan Liu", "Shuhan Chang", "Songhua Cai", "Tenglong Ao", "Tianhao Yang", "Tingting Zhang", "Wanjun Zhong", "Wei Jia", "Wei Weng", "Weihao Yu", "Wenhao Huang", "Wenjia Zhu", "Wenli Yang", "Wenzhi Wang", "Xiang Long", "XiangRui Yin", "Xiao Li", "Xiaolei Zhu", "Xiaoying Jia", "Xijin Zhang", "Xin Liu", "Xinchen Zhang", "Xinyu Yang", "Xiongcai Luo", "Xiuli Chen", "Xuantong Zhong", "Xuefeng Xiao", "Xujing Li", "Yan Wu", "Yawei Wen", "Yifan Du", "Yihao Zhang", "Yining Ye", "Yonghui Wu", "Yu Liu", "Yu Yue", "Yufeng Zhou", "Yufeng Yuan", "Yuhang Xu", "Yuhong Yang", "Yun Zhang", "Yunhao Fang", "Yuntao Li", "Yurui Ren", "Yuwen Xiong", "Zehua Hong", "Zehua Wang", "Zewei Sun", "Zeyu Wang", "Zhao Cai", "Zhaoyue Zha", "Zhecheng An", "Zhehui Zhao", "Zhengzhuo Xu", "Zhipeng Chen", "Zhiyong Wu", "Zhuofan Zheng", "Zihao Wang", "Zilong Huang", "Ziyu Zhu", "Zuquan Song"], "title": "Seed1.5-VL Technical Report", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)"}
{"id": "2505.07071", "pdf": "https://arxiv.org/pdf/2505.07071", "abs": "https://arxiv.org/abs/2505.07071", "authors": ["Zihang Liu", "Zhenyu Zhang", "Hao Tang"], "title": "Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based image super-resolution (SR) methods have demonstrated\nremarkable performance. Recent advancements have introduced deterministic\nsampling processes that reduce inference from 15 iterative steps to a single\nstep, thereby significantly improving the inference speed of existing diffusion\nmodels. However, their efficiency remains limited when handling complex\nsemantic regions due to the single-step inference. To address this limitation,\nwe propose SAMSR, a semantic-guided diffusion framework that incorporates\nsemantic segmentation masks into the sampling process. Specifically, we\nintroduce the SAM-Noise Module, which refines Gaussian noise using segmentation\nmasks to preserve spatial and semantic features. Furthermore, we develop a\npixel-wise sampling strategy that dynamically adjusts the residual transfer\nrate and noise strength based on pixel-level semantic weights, prioritizing\nsemantically rich regions during the diffusion process. To enhance model\ntraining, we also propose a semantic consistency loss, which aligns pixel-wise\nsemantic weights between predictions and ground truth. Extensive experiments on\nboth real-world and synthetic datasets demonstrate that SAMSR significantly\nimproves perceptual quality and detail recovery, particularly in semantically\ncomplex images. Our code is released at https://github.com/Liu-Zihang/SAMSR."}
{"id": "2505.07073", "pdf": "https://arxiv.org/pdf/2505.07073", "abs": "https://arxiv.org/abs/2505.07073", "authors": ["Payal Varshney", "Adriano Lucieri", "Christoph Balada", "Andreas Dengel", "Sheraz Ahmed"], "title": "Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Concept-based explanations have emerged as an effective approach within\nExplainable Artificial Intelligence, enabling interpretable insights by\naligning model decisions with human-understandable concepts. However, existing\nmethods rely on computationally intensive procedures and struggle to\nefficiently capture complex, semantic concepts. Recently, the Concept Discovery\nthrough Latent Diffusion-based Counterfactual Trajectories (CDCT) framework,\nintroduced by Varshney et al. (2025), attempts to identify concepts via\ndimension-wise traversal of the latent space of a Variational Autoencoder\ntrained on counterfactual trajectories. Extending the CDCT framework, this work\nintroduces Concept Directions via Latent Clustering (CDLC), which extracts\nglobal, class-specific concept directions by clustering latent difference\nvectors derived from factual and diffusion-generated counterfactual image\npairs. CDLC substantially reduces computational complexity by eliminating the\nexhaustive latent dimension traversal required in CDCT and enables the\nextraction of multidimensional semantic concepts encoded across the latent\ndimensions. This approach is validated on a real-world skin lesion dataset,\ndemonstrating that the extracted concept directions align with clinically\nrecognized dermoscopic features and, in some cases, reveal dataset-specific\nbiases or unknown biomarkers. These results highlight that CDLC is\ninterpretable, scalable, and applicable across high-stakes domains and diverse\ndata modalities."}
{"id": "2505.07119", "pdf": "https://arxiv.org/pdf/2505.07119", "abs": "https://arxiv.org/abs/2505.07119", "authors": ["Arianna Stropeni", "Francesco Borsatti", "Manuel Barusco", "Davide Dalle Pezze", "Marco Fabris", "Gian Antonio Susto"], "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing waste and operational costs is essential. Deploying deep learning\nmodels within Internet of Things (IoT) environments introduces specific\nchallenges due to the limited computational power and bandwidth of edge\ndevices. This study investigates how to perform VAD effectively under such\nconstraints by leveraging compact and efficient processing strategies. We\nevaluate several data compression techniques, examining the trade-off between\nsystem latency and detection accuracy. Experiments on the MVTec AD benchmark\ndemonstrate that significant compression can be achieved with minimal loss in\nanomaly detection performance compared to uncompressed data."}
{"id": "2505.07165", "pdf": "https://arxiv.org/pdf/2505.07165", "abs": "https://arxiv.org/abs/2505.07165", "authors": ["Jun Li", "Hongzhang Zhu", "Tao Chen", "Xiaohua Qian"], "title": "Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework", "categories": ["cs.CV"], "comment": "accept by IEEE JBHI. Due to the limitation \"The abstract field cannot\n  be longer than 1,920 characters\", the abstract here is shorter than that in\n  the PDF file", "summary": "Recently, numerous pancreas segmentation methods have achieved promising\nperformance on local single-source datasets. However, these methods don't\nadequately account for generalizability issues, and hence typically show\nlimited performance and low stability on test data from other sources.\nConsidering the limited availability of distinct data sources, we seek to\nimprove the generalization performance of a pancreas segmentation model trained\nwith a single-source dataset, i.e., the single source generalization task. In\nparticular, we propose a dual self-supervised learning model that incorporates\nboth global and local anatomical contexts. Our model aims to fully exploit the\nanatomical features of the intra-pancreatic and extra-pancreatic regions, and\nhence enhance the characterization of the high-uncertainty regions for more\nrobust generalization. Specifically, we first construct a global-feature\ncontrastive self-supervised learning module that is guided by the pancreatic\nspatial structure. This module obtains complete and consistent pancreatic\nfeatures through promoting intra-class cohesion, and also extracts more\ndiscriminative features for differentiating between pancreatic and\nnon-pancreatic tissues through maximizing inter-class separation. It mitigates\nthe influence of surrounding tissue on the segmentation outcomes in\nhigh-uncertainty regions. Subsequently, a local-image restoration\nself-supervised learning module is introduced to further enhance the\ncharacterization of the high uncertainty regions. In this module, informative\nanatomical contexts are actually learned to recover randomly corrupted\nappearance patterns in those regions."}
{"id": "2505.07172", "pdf": "https://arxiv.org/pdf/2505.07172", "abs": "https://arxiv.org/abs/2505.07172", "authors": ["Zexian Yang", "Dian Li", "Dayan Wu", "Gang Liu", "Weiping Wang"], "title": "Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant advancements in multimodal reasoning tasks, existing\nLarge Vision-Language Models (LVLMs) are prone to producing visually ungrounded\nresponses when interpreting associated images. In contrast, when humans embark\non learning new knowledge, they often rely on a set of fundamental pre-study\nprinciples: reviewing outlines to grasp core concepts, summarizing key points\nto guide their focus and enhance understanding. However, such preparatory\nactions are notably absent in the current instruction tuning processes. This\npaper presents Re-Critic, an easily scalable rationale-augmented framework\ndesigned to incorporate fundamental rules and chain-of-thought (CoT) as a\nbridge to enhance reasoning abilities. Specifically, Re-Critic develops a\nvisual rationale synthesizer that scalably augments raw instructions with\nrationale explanation. To probe more contextually grounded responses, Re-Critic\nemploys an in-context self-critic mechanism to select response pairs for\npreference tuning. Experiments demonstrate that models fine-tuned with our\nrationale-augmented dataset yield gains that extend beyond\nhallucination-specific tasks to broader multimodal reasoning tasks."}
{"id": "2505.07198", "pdf": "https://arxiv.org/pdf/2505.07198", "abs": "https://arxiv.org/abs/2505.07198", "authors": ["Xufei Wang", "Gengxuan Tian", "Junqiao Zhao", "Siyue Tao", "Qiwen Gu", "Qiankun Yu", "Tiantian Feng"], "title": "Ranking-aware Continual Learning for LiDAR Place Recognition", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Place recognition plays a significant role in SLAM, robot navigation, and\nautonomous driving applications. Benefiting from deep learning, the performance\nof LiDAR place recognition (LPR) has been greatly improved. However, many\nexisting learning-based LPR methods suffer from catastrophic forgetting, which\nseverely harms the performance of LPR on previously trained places after\ntraining on a new environment. In this paper, we introduce a continual learning\nframework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate\nforgetting. Inspired by the ranking process of place recognition retrieval, we\npresent a ranking-aware knowledge distillation loss that encourages the network\nto preserve the high-level place recognition knowledge. We also introduce a\nknowledge fusion module to integrate the knowledge of old and new models for\nLiDAR place recognition. Our extensive experiments demonstrate that KDF can be\napplied to different networks to overcome catastrophic forgetting, surpassing\nthe state-of-the-art methods in terms of mean Recall@1 and forgetting score."}
{"id": "2505.07209", "pdf": "https://arxiv.org/pdf/2505.07209", "abs": "https://arxiv.org/abs/2505.07209", "authors": ["Yan Xie", "Zequn Zeng", "Hao Zhang", "Yucheng Ding", "Yi Wang", "Zhengjue Wang", "Bo Chen", "Hongwei Liu"], "title": "Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Concept Bottleneck Models (CBMs) try to make the decision-making process\ntransparent by exploring an intermediate concept space between the input image\nand the output prediction. Existing CBMs just learn coarse-grained relations\nbetween the whole image and the concepts, less considering local image\ninformation, leading to two main drawbacks: i) they often produce spurious\nvisual-concept relations, hence decreasing model reliability; and ii) though\nCBMs could explain the importance of every concept to the final prediction, it\nis still challenging to tell which visual region produces the prediction. To\nsolve these problems, this paper proposes a Disentangled Optimal Transport CBM\n(DOT-CBM) framework to explore fine-grained visual-concept relations between\nlocal image patches and concepts. Specifically, we model the concept prediction\nprocess as a transportation problem between the patches and concepts, thereby\nachieving explicit fine-grained feature alignment. We also incorporate\northogonal projection losses within the modality to enhance local feature\ndisentanglement. To further address the shortcut issues caused by statistical\nbiases in the data, we utilize the visual saliency map and concept label\nstatistics as transportation priors. Thus, DOT-CBM can visualize inversion\nheatmaps, provide more reliable concept predictions, and produce more accurate\nclass predictions. Comprehensive experiments demonstrate that our proposed\nDOT-CBM achieves SOTA performance on several tasks, including image\nclassification, local part detection and out-of-distribution generalization."}
{"id": "2505.07219", "pdf": "https://arxiv.org/pdf/2505.07219", "abs": "https://arxiv.org/abs/2505.07219", "authors": ["Hongda Qin", "Xiao Lu", "Zhiyong Wei", "Yihong Cao", "Kailun Yang", "Ningjiang Chen"], "title": "Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "The source code and pre-trained models will be publicly available at\n  https://github.com/qinhongda8/LDDS", "summary": "Generalizing an object detector trained on a single domain to multiple unseen\ndomains is a challenging task. Existing methods typically introduce image or\nfeature augmentation to diversify the source domain to raise the robustness of\nthe detector. Vision-Language Model (VLM)-based augmentation techniques have\nbeen proven to be effective, but they require that the detector's backbone has\nthe same structure as the image encoder of VLM, limiting the detector framework\nselection. To address this problem, we propose Language-Driven Dual Style\nMixing (LDDS) for single-domain generalization, which diversifies the source\ndomain by fully utilizing the semantic information of the VLM. Specifically, we\nfirst construct prompts to transfer style semantics embedded in the VLM to an\nimage translation network. This facilitates the generation of style diversified\nimages with explicit semantic information. Then, we propose image-level style\nmixing between the diversified images and source domain images. This\neffectively mines the semantic information for image augmentation without\nrelying on specific augmentation selections. Finally, we propose feature-level\nstyle mixing in a double-pipeline manner, allowing feature augmentation to be\nmodel-agnostic and can work seamlessly with the mainstream detector frameworks,\nincluding the one-stage, two-stage, and transformer-based detectors. Extensive\nexperiments demonstrate the effectiveness of our approach across various\nbenchmark datasets, including real to cartoon and normal to adverse weather\ntasks. The source code and pre-trained models will be publicly available at\nhttps://github.com/qinhongda8/LDDS."}
{"id": "2505.07249", "pdf": "https://arxiv.org/pdf/2505.07249", "abs": "https://arxiv.org/abs/2505.07249", "authors": ["Philippe Colantoni", "Rafique Ahmed", "Prashant Ghimire", "Damien Muselet", "Alain Trémeau"], "title": "When Dance Video Archives Challenge Computer Vision", "categories": ["cs.CV"], "comment": null, "summary": "The accuracy and efficiency of human body pose estimation depend on the\nquality of the data to be processed and of the particularities of these data.\nTo demonstrate how dance videos can challenge pose estimation techniques, we\nproposed a new 3D human body pose estimation pipeline which combined up-to-date\ntechniques and methods that had not been yet used in dance analysis. Second, we\nperformed tests and extensive experimentations from dance video archives, and\nused visual analytic tools to evaluate the impact of several data parameters on\nhuman body pose. Our results are publicly available for research at\nhttps://www.couleur.org/articles/arXiv-1-2025/"}
{"id": "2505.07251", "pdf": "https://arxiv.org/pdf/2505.07251", "abs": "https://arxiv.org/abs/2505.07251", "authors": ["Wenqiang Wang", "Yangshijie Zhang"], "title": "Incomplete In-context Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision language models (LVLMs) achieve remarkable performance through\nVision In-context Learning (VICL), a process that depends significantly on\ndemonstrations retrieved from an extensive collection of annotated examples\n(retrieval database). Existing studies often assume that the retrieval database\ncontains annotated examples for all labels. However, in real-world scenarios,\ndelays in database updates or incomplete data annotation may result in the\nretrieval database containing labeled samples for only a subset of classes. We\nrefer to this phenomenon as an \\textbf{incomplete retrieval database} and\ndefine the in-context learning under this condition as \\textbf{Incomplete\nIn-context Learning (IICL)}. To address this challenge, we propose\n\\textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage\nframework designed to mitigate the limitations of IICL. The Iterative Judgments\nStage reformulates an \\(\\boldsymbol{m}\\)-class classification problem into a\nseries of \\(\\boldsymbol{m}\\) binary classification tasks, effectively\nconverting the IICL setting into a standard VICL scenario. The Integrated\nPrediction Stage further refines the classification process by leveraging both\nthe input image and the predictions from the Iterative Judgments Stage to\nenhance overall classification accuracy. IJIP demonstrates considerable\nperformance across two LVLMs and two datasets under three distinct conditions\nof label incompleteness, achieving the highest accuracy of 93.9\\%. Notably,\neven in scenarios where labels are fully available, IJIP still achieves the\nbest performance of all six baselines. Furthermore, IJIP can be directly\napplied to \\textbf{Prompt Learning} and is adaptable to the \\textbf{text\ndomain}."}
{"id": "2505.07254", "pdf": "https://arxiv.org/pdf/2505.07254", "abs": "https://arxiv.org/abs/2505.07254", "authors": ["Mohamed Nagy", "Naoufel Werghi", "Bilal Hassan", "Jorge Dias", "Majid Khonji"], "title": "Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This work addresses the critical lack of precision in state estimation in the\nKalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of\nselecting the appropriate motion model. Existing literature commonly relies on\nconstant motion models for estimating the states of objects, neglecting the\ncomplex motion dynamics unique to each object. Consequently, trajectory\ndivision and imprecise object localization arise, especially under occlusion\nconditions. The core of these challenges lies in the limitations of the current\nKalman filter formulation, which fails to account for the variability of motion\ndynamics as objects navigate their environments. This work introduces a novel\nformulation of the Kalman filter that incorporates motion dynamics, allowing\nthe motion model to adaptively adjust according to changes in the object's\nmovement. The proposed Kalman filter substantially improves state estimation,\nlocalization, and trajectory prediction compared to the traditional Kalman\nfilter. This is reflected in tracking performance that surpasses recent\nbenchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\\% and\n0.81\\% in higher order tracking accuracy (HOTA) and multi-object tracking\naccuracy (MOTA), respectively. Furthermore, the proposed Kalman filter\nconsistently outperforms the baseline across various detectors. Additionally,\nit shows an enhanced capability in managing long occlusions compared to the\nbaseline Kalman filter, achieving margins of 1.22\\% in higher order tracking\naccuracy (HOTA) and 1.55\\% in multi-object tracking accuracy (MOTA) on the\nKITTI dataset. The formulation's efficiency is evident, with an additional\nprocessing time of only approximately 0.078 ms per frame, ensuring its\napplicability in real-time applications."}
{"id": "2505.07256", "pdf": "https://arxiv.org/pdf/2505.07256", "abs": "https://arxiv.org/abs/2505.07256", "authors": ["Christoph Huber", "Ludwig Schleeh", "Dino Knoll", "Michael Guthe"], "title": "Synthetic Similarity Search in Automotive Production", "categories": ["cs.CV"], "comment": "Accepted for publication in Procedia CIRP", "summary": "Visual quality inspection in automotive production is essential for ensuring\nthe safety and reliability of vehicles. Computer vision (CV) has become a\npopular solution for these inspections due to its cost-effectiveness and\nreliability. However, CV models require large, annotated datasets, which are\ncostly and time-consuming to collect. To reduce the need for extensive training\ndata, we propose a novel image classification pipeline that combines similarity\nsearch using a vision-based foundation model with synthetic data. Our approach\nleverages a DINOv2 model to transform input images into feature vectors, which\nare then compared to pre-classified reference images using cosine distance\nmeasurements. By utilizing synthetic data instead of real images as references,\nour pipeline achieves high classification accuracy without relying on real\ndata. We evaluate this approach in eight real-world inspection scenarios and\ndemonstrate that it meets the high performance requirements of production\nenvironments."}
{"id": "2505.07263", "pdf": "https://arxiv.org/pdf/2505.07263", "abs": "https://arxiv.org/abs/2505.07263", "authors": ["Xiaokun Wang", "Chris", "Jiangbo Pei", "Wei Shen", "Yi Peng", "Yunzhuo Hao", "Weijie Qiu", "Ai Jian", "Tianyidan Xie", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We propose Skywork-VL Reward, a multimodal reward model that provides reward\nsignals for both multimodal understanding and reasoning tasks. Our technical\napproach comprises two key components: First, we construct a large-scale\nmultimodal preference dataset that covers a wide range of tasks and scenarios,\nwith responses collected from both standard vision-language models (VLMs) and\nadvanced VLM reasoners. Second, we design a reward model architecture based on\nQwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage\nfine-tuning using pairwise ranking loss on pairwise preference data.\nExperimental evaluations show that Skywork-VL Reward achieves state-of-the-art\nresults on multimodal VL-RewardBench and exhibits competitive performance on\nthe text-only RewardBench benchmark. Furthermore, preference data constructed\nbased on our Skywork-VL Reward proves highly effective for training Mixed\nPreference Optimization (MPO), leading to significant improvements in\nmultimodal reasoning capabilities. Our results underscore Skywork-VL Reward as\na significant advancement toward general-purpose, reliable reward models for\nmultimodal alignment. Our model has been publicly released to promote\ntransparency and reproducibility."}
{"id": "2505.07300", "pdf": "https://arxiv.org/pdf/2505.07300", "abs": "https://arxiv.org/abs/2505.07300", "authors": ["Sofia Casarin", "Sergio Escalera", "Oswald Lanz"], "title": "L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers", "categories": ["cs.CV"], "comment": "accepted at CVPR 2025", "summary": "Training-free Neural Architecture Search (NAS) efficiently identifies\nhigh-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot\nand one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the\nneed for model training, and (ii) interpretable, with proxy designs often\ntheoretically grounded. Despite rapid developments in the field, current SOTA\nZC proxies are typically constrained to well-established convolutional search\nspaces. With the rise of Large Language Models shaping the future of deep\nlearning, this work extends ZC proxy applicability to Vision Transformers\n(ViTs). We present a new benchmark using the Autoformer search space evaluated\non 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients\ninformation (L-SWAG), a novel, generalizable metric that characterizes both\nconvolutional and transformer architectures across 14 tasks. Additionally,\nprevious works highlighted how different proxies contain complementary\ninformation, motivating the need for a ML model to identify useful\ncombinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low\nInformation gain and Bias Re-Alignment), a method that strategically combines\nproxies to best represent a specific benchmark. Integrated into the NAS search,\nLIBRA-NAS outperforms evolution and gradient-based NAS techniques by\nidentifying an architecture with a 17.0% test error on ImageNet1k in just 0.1\nGPU days."}
{"id": "2505.07301", "pdf": "https://arxiv.org/pdf/2505.07301", "abs": "https://arxiv.org/abs/2505.07301", "authors": ["Katsuki Shimbo", "Hiromu Taketsugu", "Norimichi Ukita"], "title": "Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos", "categories": ["cs.CV"], "comment": "5 pages, 4 figures", "summary": "In 3D Human Motion Prediction (HMP), conventional methods train HMP models\nwith expensive motion capture data. However, the data collection cost of such\nmotion capture data limits the data diversity, which leads to poor\ngeneralizability to unseen motions or subjects. To address this issue, this\npaper proposes to enhance HMP with additional learning using estimated poses\nfrom easily available videos. The 2D poses estimated from the monocular videos\nare carefully transformed into motion capture-style 3D motions through our\npipeline. By additional learning with the obtained motions, the HMP model is\nadapted to the test domain. The experimental results demonstrate the\nquantitative and qualitative impact of our method."}
{"id": "2505.07306", "pdf": "https://arxiv.org/pdf/2505.07306", "abs": "https://arxiv.org/abs/2505.07306", "authors": ["Sander De Coninck", "Emilio Gamba", "Bart Van Doninck", "Abdellatif Bey-Temsamani", "Sam Leroux", "Pieter Simoens"], "title": "Enabling Privacy-Aware AI-Based Ergonomic Analysis", "categories": ["cs.CV"], "comment": "Accepted and presented at the 35th CIRP Design conference", "summary": "Musculoskeletal disorders (MSDs) are a leading cause of injury and\nproductivity loss in the manufacturing industry, incurring substantial economic\ncosts. Ergonomic assessments can mitigate these risks by identifying workplace\nadjustments that improve posture and reduce strain. Camera-based systems offer\na non-intrusive, cost-effective method for continuous ergonomic tracking, but\nthey also raise significant privacy concerns. To address this, we propose a\nprivacy-aware ergonomic assessment framework utilizing machine learning\ntechniques. Our approach employs adversarial training to develop a lightweight\nneural network that obfuscates video data, preserving only the essential\ninformation needed for human pose estimation. This obfuscation ensures\ncompatibility with standard pose estimation algorithms, maintaining high\naccuracy while protecting privacy. The obfuscated video data is transmitted to\na central server, where state-of-the-art keypoint detection algorithms extract\nbody landmarks. Using multi-view integration, 3D keypoints are reconstructed\nand evaluated with the Rapid Entire Body Assessment (REBA) method. Our system\nprovides a secure, effective solution for ergonomic monitoring in industrial\nenvironments, addressing both privacy and workplace safety concerns."}
{"id": "2505.07322", "pdf": "https://arxiv.org/pdf/2505.07322", "abs": "https://arxiv.org/abs/2505.07322", "authors": ["Gang He", "Siqi Wang", "Kepeng Xu", "Lin Zhang"], "title": "RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming\nincreasingly prevalent, intensifying the demand for converting Standard Dynamic\nRange (SDR) content to HDR. Existing methods primarily rely on fixed tone\nmapping operators, which are inadequate for handling SDR inputs with diverse\nstyles commonly found in real-world scenarios. To address this challenge, we\npropose a generalized SDR-to-HDR method that handles diverse styles in\nreal-world SDR content, termed Realistic Style Disentangled Representation\nLearning (RealRep). By disentangling luminance and chrominance, we analyze the\nintrinsic differences between contents with varying styles and propose a\ndisentangled multi-view style representation learning method. This approach\ncaptures the guidance prior of true luminance and chrominance distributions\nacross different styles, even when the SDR style distributions exhibit\nsignificant variations, thereby establishing a robust embedding space for\ninverse tone mapping. Motivated by the difficulty of directly utilizing\ndegradation representation priors, we further introduce the Degradation-Domain\nAware Controlled Mapping Network (DDACMNet), a two-stage framework that\nperforms adaptive hierarchical mapping guided by a control-aware normalization\nmechanism. DDACMNet dynamically modulates the mapping process via\ndegradation-conditioned hierarchical features, enabling robust adaptation\nacross diverse degradation domains. Extensive experiments show that RealRep\nconsistently outperforms state-of-the-art methods with superior generalization\nand perceptually faithful HDR color gamut reconstruction."}
{"id": "2505.07333", "pdf": "https://arxiv.org/pdf/2505.07333", "abs": "https://arxiv.org/abs/2505.07333", "authors": ["Matthew Marchellus", "Nadhira Noor", "In Kyu Park"], "title": "Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025", "summary": "Fast 3D clothed human reconstruction from monocular video remains a\nsignificant challenge in computer vision, particularly in balancing\ncomputational efficiency with reconstruction quality. Current approaches are\neither focused on static image reconstruction but too computationally\nintensive, or achieve high quality through per-video optimization that requires\nminutes to hours of processing, making them unsuitable for real-time\napplications. To this end, we present TemPoFast3D, a novel method that\nleverages temporal coherency of human appearance to reduce redundant\ncomputation while maintaining reconstruction quality. Our approach is a\n\"plug-and play\" solution that uniquely transforms pixel-aligned reconstruction\nnetworks to handle continuous video streams by maintaining and refining a\ncanonical appearance representation through efficient coordinate mapping.\nExtensive experiments demonstrate that TemPoFast3D matches or exceeds\nstate-of-the-art methods across standard metrics while providing high-quality\ntextured reconstruction across diverse pose and appearance, with a maximum\nspeed of 12 FPS."}
{"id": "2505.07336", "pdf": "https://arxiv.org/pdf/2505.07336", "abs": "https://arxiv.org/abs/2505.07336", "authors": ["Zhixuan Zhang", "Xiaopeng Li", "Qi Liu"], "title": "SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by Pattern Recognition", "summary": "Background subtraction (BGS) is utilized to detect moving objects in a video\nand is commonly employed at the onset of object tracking and human recognition\nprocesses. Nevertheless, existing BGS techniques utilizing deep learning still\nencounter challenges with various background noises in videos, including\nvariations in lighting, shifts in camera angles, and disturbances like air\nturbulence or swaying trees. To address this problem, we design a spiking\nautoencoder network, termed SAEN-BGS, based on noise resilience and\ntime-sequence sensitivity of spiking neural networks (SNNs) to enhance the\nseparation of foreground and background. To eliminate unnecessary background\nnoise and preserve the important foreground elements, we begin by creating the\ncontinuous spiking conv-and-dconv block, which serves as the fundamental\nbuilding block for the decoder in SAEN-BGS. Moreover, in striving for enhanced\nenergy efficiency, we introduce a novel self-distillation spiking supervised\nlearning method grounded in ANN-to-SNN frameworks, resulting in decreased power\nconsumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016\ndatasets, our approach demonstrates superior segmentation performance relative\nto other baseline methods, even when challenged by complex scenarios with\ndynamic backgrounds."}
{"id": "2505.07344", "pdf": "https://arxiv.org/pdf/2505.07344", "abs": "https://arxiv.org/abs/2505.07344", "authors": ["Yuan Zhang", "Jiacheng Jiang", "Guoqing Ma", "Zhiying Lu", "Haoyang Huang", "Jianlong Yuan", "Nan Duan"], "title": "Generative Pre-trained Autoregressive Diffusion Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space."}
{"id": "2505.07347", "pdf": "https://arxiv.org/pdf/2505.07347", "abs": "https://arxiv.org/abs/2505.07347", "authors": ["Jiewen Yang", "Taoran Huang", "Shangwei Ding", "Xiaowei Xu", "Qinhua Zhao", "Yong Jiang", "Jiarong Guo", "Bin Pu", "Jiexuan Zheng", "Caojin Zhang", "Hongwen Fei", "Xiaomeng Li"], "title": "AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography", "categories": ["cs.CV"], "comment": null, "summary": "Echocardiographers can detect pulmonary hypertension using Doppler\nechocardiography; however, accurately assessing its progression often proves\nchallenging. Right heart catheterization (RHC), the gold standard for precise\nevaluation, is invasive and unsuitable for routine use, limiting its\npracticality for timely diagnosis and monitoring of pulmonary hypertension\nprogression. Here, we propose MePH, a multi-view, multi-modal vision-language\nmodel to accurately assess pulmonary hypertension progression using\nnon-invasive echocardiography. We constructed a large dataset comprising paired\nstandardized echocardiogram videos, spectral images and RHC data, covering\n1,237 patient cases from 12 medical centers. For the first time, MePH precisely\nmodels the correlation between non-invasive multi-view, multi-modal\nechocardiography and the pressure and resistance obtained via RHC. We show that\nMePH significantly outperforms echocardiographers' assessments using\nechocardiography, reducing the mean absolute error in estimating mean pulmonary\narterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and\n43.81%, respectively. In eight independent external hospitals, MePH achieved a\nmean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an\narea under the curve of 0.921, surpassing echocardiographers (area under the\ncurve of 0.842) in accurately predicting the severity of pulmonary\nhypertension, whether mild or severe. A prospective study demonstrated that\nMePH can predict treatment efficacy for patients. Our work provides pulmonary\nhypertension patients with a non-invasive and timely method for monitoring\ndisease progression, improving the accuracy and efficiency of pulmonary\nhypertension management while enabling earlier interventions and more\npersonalized treatment decisions."}
{"id": "2505.07373", "pdf": "https://arxiv.org/pdf/2505.07373", "abs": "https://arxiv.org/abs/2505.07373", "authors": ["Lintao Xiang", "Hongpei Zheng", "Bailin Deng", "Hujun Yin"], "title": "Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Neural implicit surface reconstruction using volume rendering techniques has\nrecently achieved significant advancements in creating high-fidelity surfaces\nfrom multiple 2D images. However, current methods primarily target scenes with\nconsistent illumination and struggle to accurately reconstruct 3D geometry in\nuncontrolled environments with transient occlusions or varying appearances.\nWhile some neural radiance field (NeRF)-based variants can better manage\nphotometric variations and transient objects in complex scenes, they are\ndesigned for novel view synthesis rather than precise surface reconstruction\ndue to limited surface constraints. To overcome this limitation, we introduce a\nnovel approach that applies multiple geometric constraints to the implicit\nsurface optimization process, enabling more accurate reconstructions from\nunconstrained image collections. First, we utilize sparse 3D points from\nstructure-from-motion (SfM) to refine the signed distance function estimation\nfor the reconstructed surface, with a displacement compensation to accommodate\nnoise in the sparse points. Additionally, we employ robust normal priors\nderived from a normal predictor, enhanced by edge prior filtering and\nmulti-view consistency constraints, to improve alignment with the actual\nsurface geometry. Extensive testing on the Heritage-Recon benchmark and other\ndatasets has shown that the proposed method can accurately reconstruct surfaces\nfrom in-the-wild images, yielding geometries with superior accuracy and\ngranularity compared to existing techniques. Our approach enables high-quality\n3D reconstruction of various landmarks, making it applicable to diverse\nscenarios such as digital preservation of cultural heritage sites."}
{"id": "2505.07375", "pdf": "https://arxiv.org/pdf/2505.07375", "abs": "https://arxiv.org/abs/2505.07375", "authors": ["Yuqi Cheng", "Yunkang Cao", "Dongfang Wang", "Weiming Shen", "Wenlong Li"], "title": "Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection", "categories": ["cs.CV"], "comment": "12 pages, 12 figures", "summary": "Point cloud anomaly detection is essential for various industrial\napplications. The huge computation and storage costs caused by the increasing\nproduct classes limit the application of single-class unsupervised methods,\nnecessitating the development of multi-class unsupervised methods. However, the\nfeature similarity between normal and anomalous points from different class\ndata leads to the feature confusion problem, which greatly hinders the\nperformance of multi-class methods. Therefore, we introduce a multi-class point\ncloud anomaly detection method, named GLFM, leveraging global-local feature\nmatching to progressively separate data that are prone to confusion across\nmultiple classes. Specifically, GLFM is structured into three stages: Stage-I\nproposes an anomaly synthesis pipeline that stretches point clouds to create\nabundant anomaly data that are utilized to adapt the point cloud feature\nextractor for better feature representation. Stage-II establishes the global\nand local memory banks according to the global and local feature distributions\nof all the training data, weakening the impact of feature confusion on the\nestablishment of the memory bank. Stage-III implements anomaly detection of\ntest data leveraging its feature distance from global and local memory banks.\nExtensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts\ndataset showcase our proposed GLFM's superior point cloud anomaly detection\nperformance. The code is available at\nhttps://github.com/hustCYQ/GLFM-Multi-class-3DAD."}
{"id": "2505.07380", "pdf": "https://arxiv.org/pdf/2505.07380", "abs": "https://arxiv.org/abs/2505.07380", "authors": ["David Vázquez-Padín", "Fernando Pérez-González", "Pablo Pérez-Miguélez"], "title": "Apple's Synthetic Defocus Noise Pattern: Characterization and Forensic Applications", "categories": ["cs.CV", "cs.CR", "eess.IV"], "comment": "This paper was submitted to IEEE Transactions on Information\n  Forensics & Security on May, 2025", "summary": "iPhone portrait-mode images contain a distinctive pattern in out-of-focus\nregions simulating the bokeh effect, which we term Apple's Synthetic Defocus\nNoise Pattern (SDNP). If overlooked, this pattern can interfere with blind\nforensic analyses, especially PRNU-based camera source verification, as noted\nin earlier works. Since Apple's SDNP remains underexplored, we provide a\ndetailed characterization, proposing a method for its precise estimation,\nmodeling its dependence on scene brightness, ISO settings, and other factors.\nLeveraging this characterization, we explore forensic applications of the SDNP,\nincluding traceability of portrait-mode images across iPhone models and iOS\nversions in open-set scenarios, assessing its robustness under post-processing.\nFurthermore, we show that masking SDNP-affected regions in PRNU-based camera\nsource verification significantly reduces false positives, overcoming a\ncritical limitation in camera attribution, and improving state-of-the-art\ntechniques."}
{"id": "2505.07381", "pdf": "https://arxiv.org/pdf/2505.07381", "abs": "https://arxiv.org/abs/2505.07381", "authors": ["Baoping Cheng", "Yukun Zhang", "Liming Wang", "Xiaoyan Xie", "Tao Fu", "Dongkun Wang", "Xiaoming Tao"], "title": "Few-shot Semantic Encoding and Decoding for Video Surveillance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the continuous increase in the number and resolution of video\nsurveillance cameras, the burden of transmitting and storing surveillance video\nis growing. Traditional communication methods based on Shannon's theory are\nfacing optimization bottlenecks. Semantic communication, as an emerging\ncommunication method, is expected to break through this bottleneck and reduce\nthe storage and transmission consumption of video. Existing semantic decoding\nmethods often require many samples to train the neural network for each scene,\nwhich is time-consuming and labor-intensive. In this study, a semantic encoding\nand decoding method for surveillance video is proposed. First, the sketch was\nextracted as semantic information, and a sketch compression method was proposed\nto reduce the bit rate of semantic information. Then, an image translation\nnetwork was proposed to translate the sketch into a video frame with a\nreference frame. Finally, a few-shot sketch decoding network was proposed to\nreconstruct video from sketch. Experimental results showed that the proposed\nmethod achieved significantly better video reconstruction performance than\nbaseline methods. The sketch compression method could effectively reduce the\nstorage and transmission consumption of semantic information with little\ncompromise on video quality. The proposed method provides a novel semantic\nencoding and decoding method that only needs a few training samples for each\nsurveillance scene, thus improving the practicality of the semantic\ncommunication system."}
{"id": "2505.07387", "pdf": "https://arxiv.org/pdf/2505.07387", "abs": "https://arxiv.org/abs/2505.07387", "authors": ["Chunpeng Li", "Ya-tang Li"], "title": "Feature Visualization in 3D Convolutional Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the computations of convolutional neural networks requires\neffective visualization of their kernels. While maximal activation methods have\nproven successful in highlighting the preferred features of 2D convolutional\nkernels, directly applying these techniques to 3D convolutions often leads to\nuninterpretable results due to the higher dimensionality and complexity of 3D\nfeatures. To address this challenge, we propose a novel visualization approach\nfor 3D convolutional kernels that disentangles their texture and motion\npreferences. Our method begins with a data-driven decomposition of the optimal\ninput that maximally activates a given kernel. We then introduce a two-stage\noptimization strategy to extract distinct texture and motion components from\nthis input. Applying our approach to visualize kernels at various depths of\nseveral pre-trained models, we find that the resulting\nvisualizations--particularly those capturing motion--clearly reveal the\npreferred dynamic patterns encoded by 3D kernels. These results demonstrate the\neffectiveness of our method in providing interpretable insights into 3D\nconvolutional operations. Code is available at\nhttps://github.com/YatangLiLab/3DKernelVisualizer."}
{"id": "2505.07396", "pdf": "https://arxiv.org/pdf/2505.07396", "abs": "https://arxiv.org/abs/2505.07396", "authors": ["Olaf Wysocki", "Benedikt Schwab", "Manoj Kumar Biswanath", "Qilin Zhang", "Jingwei Zhu", "Thomas Froech", "Medhini Heeramaglore", "Ihab Hijazi", "Khaoula Kanna", "Mathias Pechinger", "Zhaiyu Chen", "Yao Sun", "Alejandro Rueda Segura", "Ziyang Xu", "Omar AbdelGafar", "Mansour Mehranfar", "Chandan Yeshwanth", "Yueh-Cheng Liu", "Hadi Yazdi", "Jiapan Wang", "Stefan Auer", "Katharina Anders", "Klaus Bogenberger", "Andre Borrmann", "Angela Dai", "Ludwig Hoegner", "Christoph Holst", "Thomas H. Kolbe", "Ferdinand Ludwig", "Matthias Nießner", "Frank Petzold", "Xiao Xiang Zhu", "Boris Jutzi"], "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing", "summary": "Urban Digital Twins (UDTs) have become essential for managing cities and\nintegrating complex, heterogeneous data from diverse sources. Creating UDTs\ninvolves challenges at multiple process stages, including acquiring accurate 3D\nsource data, reconstructing high-fidelity 3D models, maintaining models'\nupdates, and ensuring seamless interoperability to downstream tasks. Current\ndatasets are usually limited to one part of the processing chain, hampering\ncomprehensive UDTs validation. To address these challenges, we introduce the\nfirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.\nThis dataset includes georeferenced, semantically aligned 3D models and\nnetworks along with various terrestrial, mobile, aerial, and satellite\nobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently\n767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high\naccuracy, and multimodal data integration, the benchmark supports robust\nanalysis of sensors and the development of advanced reconstruction methods.\nAdditionally, we explore downstream tasks demonstrating the potential of\nTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar\npotential analysis, point cloud semantic segmentation, and LoD3 building\nreconstruction. We are convinced this contribution lays a foundation for\novercoming current limitations in UDT creation, fostering new research\ndirections and practical solutions for smarter, data-driven urban environments.\nThe project is available under: https://tum2t.win"}
{"id": "2505.07398", "pdf": "https://arxiv.org/pdf/2505.07398", "abs": "https://arxiv.org/abs/2505.07398", "authors": ["Mingqian Ji", "Jian Yang", "Shanshan Zhang"], "title": "DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "State-of-the-art LiDAR-camera 3D object detectors usually focus on feature\nfusion. However, they neglect the factor of depth while designing the fusion\nstrategy. In this work, we are the first to observe that different modalities\nplay different roles as depth varies via statistical analysis and\nvisualization. Based on this finding, we propose a Depth-Aware Hybrid Feature\nFusion (DepthFusion) strategy that guides the weights of point cloud and RGB\nimage modalities by introducing depth encoding at both global and local levels.\nSpecifically, the Depth-GFusion module adaptively adjusts the weights of image\nBird's-Eye-View (BEV) features in multi-modal global features via depth\nencoding. Furthermore, to compensate for the information lost when transferring\nraw features to the BEV space, we propose a Depth-LFusion module, which\nadaptively adjusts the weights of original voxel features and multi-view image\nfeatures in multi-modal local features via depth encoding. Extensive\nexperiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion\nmethod surpasses previous state-of-the-art methods. Moreover, our DepthFusion\nis more robust to various kinds of corruptions, outperforming previous methods\non the nuScenes-C dataset."}
{"id": "2505.07444", "pdf": "https://arxiv.org/pdf/2505.07444", "abs": "https://arxiv.org/abs/2505.07444", "authors": ["Zeynep Galymzhankyzy", "Eric Martinson"], "title": "Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture", "categories": ["cs.CV", "I.4.6"], "comment": "4 pages, 5 figures, 1 table", "summary": "Efficient crop-weed segmentation is critical for site-specific weed control\nin precision agriculture. Conventional CNN-based methods struggle to generalize\nand rely on RGB imagery, limiting performance under complex field conditions.\nTo address these challenges, we propose a lightweight transformer-CNN hybrid.\nIt processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using\nspecialized encoders and dynamic modality integration. Evaluated on the\nWeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of\n78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7\nmillion parameters, the model offers high accuracy, computational efficiency,\nand potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and\nedge devices, advancing precision weed management."}
{"id": "2505.07481", "pdf": "https://arxiv.org/pdf/2505.07481", "abs": "https://arxiv.org/abs/2505.07481", "authors": ["Erik Landolsi", "Fredrik Kahl"], "title": "Addressing degeneracies in latent interpolation for diffusion models", "categories": ["cs.CV"], "comment": "14 pages, 12 figures", "summary": "There is an increasing interest in using image-generating diffusion models\nfor deep data augmentation and image morphing. In this context, it is useful to\ninterpolate between latents produced by inverting a set of input images, in\norder to generate new images representing some mixture of the inputs. We\nobserve that such interpolation can easily lead to degenerate results when the\nnumber of inputs is large. We analyze the cause of this effect theoretically\nand experimentally, and suggest a suitable remedy. The suggested approach is a\nrelatively simple normalization scheme that is easy to use whenever\ninterpolation between latents is needed. We measure image quality using FID and\nCLIP embedding distance and show experimentally that baseline interpolation\nmethods lead to a drop in quality metrics long before the degeneration issue is\nclearly visible. In contrast, our method significantly reduces the degeneration\neffect and leads to improved quality metrics also in non-degenerate situations."}
{"id": "2505.07496", "pdf": "https://arxiv.org/pdf/2505.07496", "abs": "https://arxiv.org/abs/2505.07496", "authors": ["Mohamed Ali Souibgui", "Changkyu Choi", "Andrey Barsky", "Kangsoo Jung", "Ernest Valveny", "Dimosthenis Karatzas"], "title": "DocVXQA: Context-Aware Visual Explanations for Document Question Answering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose DocVXQA, a novel framework for visually self-explainable document\nquestion answering. The framework is designed not only to produce accurate\nanswers to questions but also to learn visual heatmaps that highlight\ncontextually critical regions, thereby offering interpretable justifications\nfor the model's decisions. To integrate explanations into the learning process,\nwe quantitatively formulate explainability principles as explicit learning\nobjectives. Unlike conventional methods that emphasize only the regions\npertinent to the answer, our framework delivers explanations that are\n\\textit{contextually sufficient} while remaining\n\\textit{representation-efficient}. This fosters user trust while achieving a\nbalance between predictive performance and interpretability in DocVQA\napplications. Extensive experiments, including human evaluation, provide strong\nevidence supporting the effectiveness of our method. The code is available at\nhttps://github.com/dali92002/DocVXQA."}
{"id": "2505.07500", "pdf": "https://arxiv.org/pdf/2505.07500", "abs": "https://arxiv.org/abs/2505.07500", "authors": ["Bahram Mohammadi", "Ehsan Abbasnejad", "Yuankai Qi", "Qi Wu", "Anton Van Den Hengel", "Javen Qinfeng Shi"], "title": "Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The remote embodied referring expression (REVERIE) task requires an agent to\nnavigate through complex indoor environments and localize a remote object\nspecified by high-level instructions, such as \"bring me a spoon\", without\npre-exploration. Hence, an efficient navigation plan is essential for the final\nsuccess. This paper proposes a novel parameter-efficient action planner using\nlarge language models (PEAP-LLM) to generate a single-step instruction at each\nlocation. The proposed model consists of two modules, LLM goal planner (LGP)\nand LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan\nfrom REVERIE instructions, including the target object and room. Then, LAP\ngenerates a single-step instruction with the goal-oriented plan, high-level\ninstruction, and current visual observation as input. PEAP-LLM enables the\nembodied agent to interact with LAP as the path planner on the fly. A simple\ndirect application of LLMs hardly achieves good performance. Also, existing\nhard-prompt-based methods are error-prone in complicated scenarios and need\nhuman intervention. To address these issues and prevent the LLM from generating\nhallucinations and biased information, we propose a novel two-stage method for\nfine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct\npreference optimization (DPO). SFT improves the quality of generated\ninstructions, while DPO utilizes environmental feedback. Experimental results\nshow the superiority of our proposed model on REVERIE compared to the previous\nstate-of-the-art."}
{"id": "2505.07511", "pdf": "https://arxiv.org/pdf/2505.07511", "abs": "https://arxiv.org/abs/2505.07511", "authors": ["Mauricio Orbes-Arteaga", "Oeslle Lucena", "Sabastien Ourselin", "M. Jorge Cardoso"], "title": "MAIS: Memory-Attention for Interactive Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Interactive medical segmentation reduces annotation effort by refining\npredictions through user feedback. Vision Transformer (ViT)-based models, such\nas the Segment Anything Model (SAM), achieve state-of-the-art performance using\nuser clicks and prior masks as prompts. However, existing methods treat\ninteractions as independent events, leading to redundant corrections and\nlimited refinement gains. We address this by introducing MAIS, a\nMemory-Attention mechanism for Interactive Segmentation that stores past user\ninputs and segmentation states, enabling temporal context integration. Our\napproach enhances ViT-based segmentation across diverse imaging modalities,\nachieving more efficient and accurate refinements."}
{"id": "2505.07530", "pdf": "https://arxiv.org/pdf/2505.07530", "abs": "https://arxiv.org/abs/2505.07530", "authors": ["Raul Ismayilov", "Luuk Spreeuwers", "Dzemila Sero"], "title": "FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic face datasets are increasingly used to overcome the limitations of\nreal-world biometric data, including privacy concerns, demographic imbalance,\nand high collection costs. However, many existing methods lack fine-grained\ncontrol over identity attributes and fail to produce paired,\nidentity-consistent images under structured capture conditions. We introduce\nFLUXSynID, a framework for generating high-resolution synthetic face datasets\nwith user-defined identity attribute distributions and paired document-style\nand trusted live capture images. The dataset generated using the FLUXSynID\nframework shows improved alignment with real-world identity distributions and\ngreater inter-set diversity compared to prior work. The FLUXSynID framework for\ngenerating custom datasets, along with a dataset of 14,889 synthetic\nidentities, is publicly released to support biometric research, including face\nrecognition and morphing attack detection."}
{"id": "2505.07533", "pdf": "https://arxiv.org/pdf/2505.07533", "abs": "https://arxiv.org/abs/2505.07533", "authors": ["Ahmad Fall", "Federica Granese", "Alex Lence", "Dominique Fourer", "Blaise Hanczar", "Joe-Elie Salem", "Jean-Daniel Zucker", "Edi Prifti"], "title": "IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monitoring and analyzing electrocardiogram (ECG) signals, even under varying\nphysiological conditions, including those influenced by physical activity,\ndrugs and stress, is crucial to accurately assess cardiac health. However,\ncurrent AI-based methods often fail to account for how these factors interact\nand alter ECG patterns, ultimately limiting their applicability in real-world\nsettings. This study introduces IKrNet, a novel neural network model, which\nidentifies drug-specific patterns in ECGs amidst certain physiological\nconditions. IKrNet's architecture incorporates spatial and temporal dynamics by\nusing a convolutional backbone with varying receptive field size to capture\nspatial features. A bi-directional Long Short-Term Memory module is also\nemployed to model temporal dependencies. By treating heart rate variability as\na surrogate for physiological fluctuations, we evaluated IKrNet's performance\nacross diverse scenarios, including conditions with physical stress, drug\nintake alone, and a baseline without drug presence. Our assessment follows a\nclinical protocol in which 990 healthy volunteers were administered 80mg of\nSotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a\nlife-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art\nmodels' accuracy and stability in varying physiological conditions,\nunderscoring its clinical viability."}
{"id": "2505.07538", "pdf": "https://arxiv.org/pdf/2505.07538", "abs": "https://arxiv.org/abs/2505.07538", "authors": ["Bohan Wang", "Zhongqi Yue", "Fengda Zhang", "Shuo Chen", "Li'an Bi", "Junzhe Zhang", "Xue Song", "Kennard Yanting Chan", "Jiachun Pan", "Weijia Wu", "Mingze Zhou", "Wang Lin", "Kaihang Pan", "Saining Zhang", "Liyu Jia", "Wentao Hu", "Wei Zhao", "Hanwang Zhang"], "title": "Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We completely discard the conventional spatial prior in image representation\nand introduce a novel discrete visual tokenizer: Self-consistency Tokenizer\n(Selftok). At its design core, we compose an autoregressive (AR) prior --\nmirroring the causal structure of language -- into visual tokens by using the\nreverse diffusion process of image generation. The AR property makes Selftok\nfundamentally distinct from traditional spatial tokens in the following two key\nways: - Selftok offers an elegant and minimalist approach to unify diffusion\nand AR for vision-language models (VLMs): By representing images with Selftok\ntokens, we can train a VLM using a purely discrete autoregressive architecture\n-- like that in LLMs -- without requiring additional modules or training\nobjectives. - We theoretically show that the AR prior satisfies the Bellman\nequation, whereas the spatial prior does not. Therefore, Selftok supports\nreinforcement learning (RL) for visual generation with effectiveness comparable\nto that achieved in LLMs. Besides the AR property, Selftok is also a SoTA\ntokenizer that achieves a favorable trade-off between high-quality\nreconstruction and compression rate. We use Selftok to build a pure AR VLM for\nboth visual comprehension and generation tasks. Impressively, without using any\ntext-image training pairs, a simple policy gradient RL working in the visual\ntokens can significantly boost the visual generation benchmark, surpassing all\nthe existing models by a large margin. Therefore, we believe that Selftok\neffectively addresses the long-standing challenge that visual tokens cannot\nsupport effective RL. When combined with the well-established strengths of RL\nin LLMs, this brings us one step closer to realizing a truly multimodal LLM.\nProject Page: https://selftok-team.github.io/report/."}
{"id": "2505.07539", "pdf": "https://arxiv.org/pdf/2505.07539", "abs": "https://arxiv.org/abs/2505.07539", "authors": ["Hao Li", "Sicheng Li", "Xiang Gao", "Abudouaihati Batuer", "Lu Yu", "Yiyi Liao"], "title": "GIFStream: 4D Gaussian-based Immersive Video with Feature Stream", "categories": ["cs.CV"], "comment": "14 pages, 10 figures", "summary": "Immersive video offers a 6-Dof-free viewing experience, potentially playing a\nkey role in future video technology. Recently, 4D Gaussian Splatting has gained\nattention as an effective approach for immersive video due to its high\nrendering efficiency and quality, though maintaining quality with manageable\nstorage remains challenging. To address this, we introduce GIFStream, a novel\n4D Gaussian representation using a canonical space and a deformation field\nenhanced with time-dependent feature streams. These feature streams enable\ncomplex motion modeling and allow efficient compression by leveraging temporal\ncorrespondence and motion-aware pruning. Additionally, we incorporate both\ntemporal and spatial compression networks for end-to-end compression.\nExperimental results show that GIFStream delivers high-quality immersive video\nat 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project\npage: https://xdimlab.github.io/GIFStream"}
{"id": "2505.07540", "pdf": "https://arxiv.org/pdf/2505.07540", "abs": "https://arxiv.org/abs/2505.07540", "authors": ["Juan E. Tapia", "Fabian Stockhardt", "Lázaro Janier González-Soler", "Christoph Busch"], "title": "SynID: Passport Synthetic Dataset for Presentation Attack Detection", "categories": ["cs.CV"], "comment": null, "summary": "The demand for Presentation Attack Detection (PAD) to identify fraudulent ID\ndocuments in remote verification systems has significantly risen in recent\nyears. This increase is driven by several factors, including the rise of remote\nwork, online purchasing, migration, and advancements in synthetic images.\nAdditionally, we have noticed a surge in the number of attacks aimed at the\nenrolment process. Training a PAD to detect fake ID documents is very\nchallenging because of the limited number of ID documents available due to\nprivacy concerns. This work proposes a new passport dataset generated from a\nhybrid method that combines synthetic data and open-access information using\nthe ICAO requirement to obtain realistic training and testing images."}
{"id": "2505.07552", "pdf": "https://arxiv.org/pdf/2505.07552", "abs": "https://arxiv.org/abs/2505.07552", "authors": ["Efe Bozkir", "Christian Kosel", "Tina Seidel", "Enkelejda Kasneci"], "title": "Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted as a long paper at the Educational Data Mining (EDM)\n  Conference 2025", "summary": "Teachers' visual attention and its distribution across the students in\nclassrooms can constitute important implications for student engagement,\nachievement, and professional teacher training. Despite that, inferring the\ninformation about where and which student teachers focus on is not trivial.\nMobile eye tracking can provide vital help to solve this issue; however, the\nuse of mobile eye tracking alone requires a significant amount of manual\nannotations. To address this limitation, we present an automated processing\npipeline concept that requires minimal manually annotated data to recognize\nwhich student the teachers focus on. To this end, we utilize state-of-the-art\nface detection models and face recognition feature embeddings to train face\nrecognition models with transfer learning in the classroom context and combine\nthese models with the teachers' gaze from mobile eye trackers. We evaluated our\napproach with data collected from four different classrooms, and our results\nshow that while it is possible to estimate the visually focused students with\nreasonable performance in all of our classroom setups, U-shaped and small\nclassrooms led to the best results with accuracies of approximately 0.7 and\n0.9, respectively. While we did not evaluate our method for teacher-student\ninteractions and focused on the validity of the technical approach, as our\nmethodology does not require a vast amount of manually annotated data and\noffers a non-intrusive way of handling teachers' visual attention, it could\nhelp improve instructional strategies, enhance classroom management, and\nprovide feedback for professional teacher development."}
{"id": "2505.07556", "pdf": "https://arxiv.org/pdf/2505.07556", "abs": "https://arxiv.org/abs/2505.07556", "authors": ["Kamil Jeziorek", "Tomasz Kryjak"], "title": "Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs", "categories": ["cs.CV"], "comment": "Presented at the Real-time Processing of Image, Depth and Video\n  Information 2025 workshop and to be considered for publication is the SPIE\n  Proceedings", "summary": "Event cameras offer significant advantages over traditional frame-based\nsensors. These include microsecond temporal resolution, robustness under\nvarying lighting conditions and low power consumption. Nevertheless, the\neffective processing of their sparse, asynchronous event streams remains\nchallenging. Existing approaches to this problem can be categorised into two\ndistinct groups. The first group involves the direct processing of event data\nwith neural models, such as Spiking Neural Networks or Graph Convolutional\nNeural Networks. However, this approach is often accompanied by a compromise in\nterms of qualitative performance. The second group involves the conversion of\nevents into dense representations with handcrafted aggregation functions, which\ncan boost accuracy at the cost of temporal fidelity. This paper introduces a\nnovel Self-Supervised Event Representation (SSER) method leveraging Gated\nRecurrent Unit (GRU) networks to achieve precise per-pixel encoding of event\ntimestamps and polarities without temporal discretisation. The recurrent layers\nare trained in a self-supervised manner to maximise the fidelity of event-time\nencoding. The inference is performed with event representations generated\nasynchronously, thus ensuring compatibility with high-throughput sensors. The\nexperimental validation demonstrates that SSER outperforms aggregation-based\nbaselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx\nobject detection datasets. Furthermore, the paper presents the first hardware\nimplementation of recurrent representation for event data on a System-on-Chip\nFPGA, achieving sub-microsecond latency and power consumption between 1-2 W,\nsuitable for real-time, power-efficient applications. Code is available at\nhttps://github.com/vision-agh/RecRepEvent."}
{"id": "2505.07573", "pdf": "https://arxiv.org/pdf/2505.07573", "abs": "https://arxiv.org/abs/2505.07573", "authors": ["Sarah de Boer", "Hartmut Häntze", "Kiran Vaidhya Venkadesh", "Myrthe A. D. Buser", "Gabriel E. Humpire Mamani", "Lina Xu", "Lisa C. Adams", "Jawed Nawabi", "Keno K. Bressem", "Bram van Ginneken", "Mathias Prokop", "Alessa Hering"], "title": "Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework", "categories": ["cs.CV", "cs.AI"], "comment": "35 pages, 11 figures", "summary": "Kidney abnormality segmentation has important potential to enhance the\nclinical workflow, especially in settings requiring quantitative assessments.\nKidney volume could serve as an important biomarker for renal diseases, with\nchanges in volume correlating directly with kidney function. Currently,\nclinical practice often relies on subjective visual assessment for evaluating\nkidney size and abnormalities, including tumors and cysts, which are typically\nstaged based on diameter, volume, and anatomical location. To support a more\nobjective and reproducible approach, this research aims to develop a robust,\nthoroughly validated kidney abnormality segmentation algorithm, made publicly\navailable for clinical and research use. We employ publicly available training\ndatasets and leverage the state-of-the-art medical image segmentation framework\nnnU-Net. Validation is conducted using both proprietary and public test\ndatasets, with segmentation performance quantified by Dice coefficient and the\n95th percentile Hausdorff distance. Furthermore, we analyze robustness across\nsubgroups based on patient sex, age, CT contrast phases, and tumor histologic\nsubtypes. Our findings demonstrate that our segmentation algorithm, trained\nexclusively on publicly available data, generalizes effectively to external\ntest sets and outperforms existing state-of-the-art models across all tested\ndatasets. Subgroup analyses reveal consistent high performance, indicating\nstrong robustness and reliability. The developed algorithm and associated code\nare publicly accessible at\nhttps://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation."}
{"id": "2505.07576", "pdf": "https://arxiv.org/pdf/2505.07576", "abs": "https://arxiv.org/abs/2505.07576", "authors": ["Manuel Barusco", "Francesco Borsatti", "Youssef Ben Khalifa", "Davide Dalle Pezze", "Gian Antonio Susto"], "title": "Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semiconductor manufacturing is a complex, multistage process. Automated\nvisual inspection of Scanning Electron Microscope (SEM) images is indispensable\nfor minimizing equipment downtime and containing costs. Most previous research\nconsiders supervised approaches, assuming a sufficient number of anomalously\nlabeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging\nresearch domain, focuses on unsupervised learning, avoiding the costly defect\ncollection phase while providing explanations of the predictions. We introduce\na benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.\nOur results demonstrate the efficacy of modern VAD approaches in this field."}
{"id": "2505.07611", "pdf": "https://arxiv.org/pdf/2505.07611", "abs": "https://arxiv.org/abs/2505.07611", "authors": ["Yi Zhang", "Wenye Zhou", "Ruonan Lin", "Xin Yang", "Hao Zheng"], "title": "Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions", "categories": ["cs.CV"], "comment": null, "summary": "Traffic accident prediction and detection are critical for enhancing road\nsafety,and vision-based traffic accident anticipation (Vision-TAA) has emerged\nas a promising approach in the era of deep learning.This paper reviews 147\nrecent studies,focusing on the application of supervised,unsupervised,and\nhybrid deep learning models for accident prediction,alongside the use of\nreal-world and synthetic datasets.Current methodologies are categorized into\nfour key approaches: image and video feature-based prediction, spatiotemporal\nfeature-based prediction, scene understanding,and multimodal data fusion.While\nthese methods demonstrate significant potential,challenges such as data\nscarcity,limited generalization to complex scenarios,and real-time performance\nconstraints remain prevalent. This review highlights opportunities for future\nresearch,including the integration of multimodal data fusion, self-supervised\nlearning,and Transformer-based architectures to enhance prediction accuracy and\nscalability.By synthesizing existing advancements and identifying critical\ngaps, this paper provides a foundational reference for developing robust and\nadaptive Vision-TAA systems,contributing to road safety and traffic management."}
{"id": "2505.07620", "pdf": "https://arxiv.org/pdf/2505.07620", "abs": "https://arxiv.org/abs/2505.07620", "authors": ["Simone Azeglio", "Victor Calbiague Garcia", "Guilhem Glaziou", "Peter Neri", "Olivier Marre", "Ulisse Ferrari"], "title": "Higher-Order Convolution Improves Neural Predictivity in the Retina", "categories": ["cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "We present a novel approach to neural response prediction that incorporates\nhigher-order operations directly within convolutional neural networks (CNNs).\nOur model extends traditional 3D CNNs by embedding higher-order operations\nwithin the convolutional operator itself, enabling direct modeling of\nmultiplicative interactions between neighboring pixels across space and time.\nOur model increases the representational power of CNNs without increasing their\ndepth, therefore addressing the architectural disparity between deep artificial\nnetworks and the relatively shallow processing hierarchy of biological visual\nsystems. We evaluate our approach on two distinct datasets: salamander retinal\nganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC\nresponses to controlled geometric transformations. Our higher-order CNN (HoCNN)\nachieves superior performance while requiring only half the training data\ncompared to standard architectures, demonstrating correlation coefficients up\nto 0.75 with neural responses (against 0.80$\\pm$0.02 retinal reliability). When\nintegrated into state-of-the-art architectures, our approach consistently\nimproves performance across different species and stimulus conditions. Analysis\nof the learned representations reveals that our network naturally encodes\nfundamental geometric transformations, particularly scaling parameters that\ncharacterize object expansion and contraction. This capability is especially\nrelevant for specific cell types, such as transient OFF-alpha and transient ON\ncells, which are known to detect looming objects and object motion\nrespectively, and where our model shows marked improvement in response\nprediction. The correlation coefficients for scaling parameters are more than\ntwice as high in HoCNN (0.72) compared to baseline models (0.32)."}
{"id": "2505.07622", "pdf": "https://arxiv.org/pdf/2505.07622", "abs": "https://arxiv.org/abs/2505.07622", "authors": ["Zhuo Song", "Ye Zhang", "Kunhong Li", "Longguang Wang", "Yulan Guo"], "title": "A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Cross-view geo-localization is a promising solution for large-scale\nlocalization problems, requiring the sequential execution of retrieval and\nmetric localization tasks to achieve fine-grained predictions. However,\nexisting methods typically focus on designing standalone models for these two\ntasks, resulting in inefficient collaboration and increased training overhead.\nIn this paper, we propose UnifyGeo, a novel unified hierarchical\ngeo-localization framework that integrates retrieval and metric localization\ntasks into a single network. Specifically, we first employ a unified learning\nstrategy with shared parameters to jointly learn multi-granularity\nrepresentation, facilitating mutual reinforcement between these two tasks.\nSubsequently, we design a re-ranking mechanism guided by a dedicated loss\nfunction, which enhances geo-localization performance by improving both\nretrieval accuracy and metric localization references. Extensive experiments\ndemonstrate that UnifyGeo significantly outperforms the state-of-the-arts in\nboth task-isolated and task-associated settings. Remarkably, on the challenging\nVIGOR benchmark, which supports fine-grained localization evaluation, the\n1-meter-level localization recall rate improves from 1.53\\% to 39.64\\% and from\n0.43\\% to 25.58\\% under same-area and cross-area evaluations, respectively.\nCode will be made publicly available."}
{"id": "2505.07652", "pdf": "https://arxiv.org/pdf/2505.07652", "abs": "https://arxiv.org/abs/2505.07652", "authors": ["Ozgur Kara", "Krishna Kumar Singh", "Feng Liu", "Duygu Ceylan", "James M. Rehg", "Tobias Hinz"], "title": "ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Current diffusion-based text-to-video methods are limited to producing short\nvideo clips of a single shot and lack the capability to generate multi-shot\nvideos with discrete transitions where the same character performs distinct\nactivities across the same or different backgrounds. To address this limitation\nwe propose a framework that includes a dataset collection pipeline and\narchitectural extensions to video diffusion models to enable text-to-multi-shot\nvideo generation. Our approach enables generation of multi-shot videos as a\nsingle video with full attention across all frames of all shots, ensuring\ncharacter and background consistency, and allows users to control the number,\nduration, and content of shots through shot-specific conditioning. This is\nachieved by incorporating a transition token into the text-to-video model to\ncontrol at which frames a new shot begins and a local attention masking\nstrategy which controls the transition token's effect and allows shot-specific\nprompting. To obtain training data we propose a novel data collection pipeline\nto construct a multi-shot video dataset from existing single-shot video\ndatasets. Extensive experiments demonstrate that fine-tuning a pre-trained\ntext-to-video model for a few thousand iterations is enough for the model to\nsubsequently be able to generate multi-shot videos with shot-specific control,\noutperforming the baselines. You can find more details in\nhttps://shotadapter.github.io/"}
{"id": "2505.07689", "pdf": "https://arxiv.org/pdf/2505.07689", "abs": "https://arxiv.org/abs/2505.07689", "authors": ["Quang Vinh Nguyen", "Minh Duc Nguyen", "Thanh Hoang Son Vo", "Hyung-Jeong Yang", "Soo-Hyung Kim"], "title": "Anatomical Attention Alignment representation for Radiology Report Generation", "categories": ["cs.CV"], "comment": null, "summary": "Automated Radiology report generation (RRG) aims at producing detailed\ndescriptions of medical images, reducing radiologists' workload and improving\naccess to high-quality diagnostic services. Existing encoder-decoder models\nonly rely on visual features extracted from raw input images, which can limit\nthe understanding of spatial structures and semantic relationships, often\nresulting in suboptimal text generation. To address this, we propose Anatomical\nAttention Alignment Network (A3Net), a framework that enhance visual-textual\nunderstanding by constructing hyper-visual representations. Our approach\nintegrates a knowledge dictionary of anatomical structures with patch-level\nvisual features, enabling the model to effectively associate image regions with\ntheir corresponding anatomical entities. This structured representation\nimproves semantic reasoning, interpretability, and cross-modal alignment,\nultimately enhancing the accuracy and clinical relevance of generated reports.\nExperimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net\nsignificantly improves both visual perception and text generation quality. Our\ncode is available at \\href{https://github.com/Vinh-AI/A3Net}{GitHub}."}
{"id": "2505.07690", "pdf": "https://arxiv.org/pdf/2505.07690", "abs": "https://arxiv.org/abs/2505.07690", "authors": ["Songlin Dong", "Chenhao Ding", "Jiangyang Li", "Jizhou Han", "Qiang Wang", "Yuhang He", "Yihong Gong"], "title": "Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter for Continual Learning of Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "This study aims to address the problem of multi-domain task incremental\nlearning~(MTIL), which requires that vision-language models~(VLMs) continuously\nacquire new knowledge while maintaining their inherent zero-shot recognition\ncapability. Existing paradigms delegate the testing of unseen-domain samples to\nthe original CLIP, which only prevents the degradation of the model's zero-shot\ncapability but fails to enhance the generalization of the VLM further. To this\nend, we propose a novel MTIL framework, named AFA, which comprises two core\nmodules: (1) an against forward-forgetting adapter that learns task-invariant\ninformation for each dataset in the incremental tasks to enhance the zero-shot\nrecognition ability of VLMs; (2) an against backward-forgetting adapter that\nstrengthens the few-shot learning capability of VLMs while supporting\nincremental learning. Extensive experiments demonstrate that the AFA method\nsignificantly outperforms existing state-of-the-art approaches, especially in\nfew-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP\nin terms of transferability. The code is provided in the Supplementary\nMaterial."}
{"id": "2505.07691", "pdf": "https://arxiv.org/pdf/2505.07691", "abs": "https://arxiv.org/abs/2505.07691", "authors": ["Negin Ghamsarian", "Sahar Nasirihaghighi", "Klaus Schoeffmann", "Raphael Sznitman"], "title": "Feedback-Driven Pseudo-Label Reliability Assessment: Redefining Thresholding for Semi-Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": "11 pages, 5 Figures", "summary": "Semi-supervised learning leverages unlabeled data to enhance model\nperformance, addressing the limitations of fully supervised approaches. Among\nits strategies, pseudo-supervision has proven highly effective, typically\nrelying on one or multiple teacher networks to refine pseudo-labels before\ntraining a student network. A common practice in pseudo-supervision is\nfiltering pseudo-labels based on pre-defined confidence thresholds or entropy.\nHowever, selecting optimal thresholds requires large labeled datasets, which\nare often scarce in real-world semi-supervised scenarios. To overcome this\nchallenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic\nfeedback-driven thresholding strategy for pseudo-label selection. Instead of\nrelying on static confidence thresholds, ENCORE estimates class-wise\ntrue-positive confidence within the unlabeled dataset and continuously adjusts\nthresholds based on the model's response to different levels of pseudo-label\nfiltering. This feedback-driven mechanism ensures the retention of informative\npseudo-labels while filtering unreliable ones, enhancing model training without\nmanual threshold tuning. Our method seamlessly integrates into existing\npseudo-supervision frameworks and significantly improves segmentation\nperformance, particularly in data-scarce conditions. Extensive experiments\ndemonstrate that integrating ENCORE with existing pseudo-supervision frameworks\nenhances performance across multiple datasets and network architectures,\nvalidating its effectiveness in semi-supervised learning."}
{"id": "2505.07704", "pdf": "https://arxiv.org/pdf/2505.07704", "abs": "https://arxiv.org/abs/2505.07704", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Anton Razzhigaev", "Alexander Panchenko", "Vasily Konovalov"], "title": "Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Measuring how real images look is a complex task in artificial intelligence\nresearch. For example, an image of a boy with a vacuum cleaner in a desert\nviolates common sense. We introduce a novel method, which we call Through the\nLooking Glass (TLG), to assess image common sense consistency using Large\nVision-Language Models (LVLMs) and Transformer-based encoder. By leveraging\nLVLMs to extract atomic facts from these images, we obtain a mix of accurate\nfacts. We proceed by fine-tuning a compact attention-pooling classifier over\nencoded atomic facts. Our TLG has achieved a new state-of-the-art performance\non the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning\ncomponent."}
{"id": "2505.07715", "pdf": "https://arxiv.org/pdf/2505.07715", "abs": "https://arxiv.org/abs/2505.07715", "authors": ["Qi Xu", "Jie Deng", "Jiangrong Shen", "Biwu Chen", "Huajin Tang", "Gang Pan"], "title": "Hybrid Spiking Vision Transformer for Object Detection with Event Cameras", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Event-based object detection has gained increasing attention due to its\nadvantages such as high temporal resolution, wide dynamic range, and\nasynchronous address-event representation. Leveraging these advantages, Spiking\nNeural Networks (SNNs) have emerged as a promising approach, offering low\nenergy consumption and rich spatiotemporal dynamics. To further enhance the\nperformance of event-based object detection, this study proposes a novel hybrid\nspike vision Transformer (HsVT) model. The HsVT model integrates a spatial\nfeature extraction module to capture local and global features, and a temporal\nfeature extraction module to model time dependencies and long-term patterns in\nevent sequences. This combination enables HsVT to capture spatiotemporal\nfeatures, improving its capability to handle complex event-based object\ndetection tasks. To support research in this area, we developed and publicly\nreleased The Fall Detection Dataset as a benchmark for event-based object\ndetection tasks. This dataset, captured using an event-based camera, ensures\nfacial privacy protection and reduces memory usage due to the event\nrepresentation format. We evaluated the HsVT model on GEN1 and Fall Detection\ndatasets across various model sizes. Experimental results demonstrate that HsVT\nachieves significant performance improvements in event detection with fewer\nparameters."}
{"id": "2505.07721", "pdf": "https://arxiv.org/pdf/2505.07721", "abs": "https://arxiv.org/abs/2505.07721", "authors": ["Vignesh Edithal", "Le Zhang", "Ilia Blank", "Imran Junejo"], "title": "Gameplay Highlights Generation", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we enable gamers to share their gaming experience on social\nmedia by automatically generating eye-catching highlight reels from their\ngameplay session Our automation will save time for gamers while increasing\naudience engagement. We approach the highlight generation problem by first\nidentifying intervals in the video where interesting events occur and then\nconcatenate them. We developed an in-house gameplay event detection dataset\ncontaining interesting events annotated by humans using VIA video annotator.\nTraditional techniques for highlight detection such as game engine integration\nrequires expensive collaboration with game developers. OCR techniques which\ndetect patches of specific images or texts require expensive per game\nengineering and may not generalize across game UI and different language. We\nfinetuned a multimodal general purpose video understanding model such as X-CLIP\nusing our dataset which generalizes across multiple games in a genre without\nper game engineering. Prompt engineering was performed to improve the\nclassification performance of this multimodal model. Our evaluation showed that\nsuch a finetuned model can detect interesting events in first person shooting\ngames from unseen gameplay footage with more than 90% accuracy. Moreover, our\nmodel performed significantly better on low resource games (small dataset) when\ntrained along with high resource games, showing signs of transfer learning. To\nmake the model production ready, we used ONNX libraries to enable cross\nplatform inference. These libraries also provide post training quantization\ntools to reduce model size and inference time for deployment. ONNX runtime\nlibraries with DirectML backend were used to perform efficient inference on\nWindows OS. We show that natural language supervision in the X-CLIP model leads\nto data efficient and highly performant video recognition models."}
{"id": "2505.07734", "pdf": "https://arxiv.org/pdf/2505.07734", "abs": "https://arxiv.org/abs/2505.07734", "authors": ["Jiangling Zhang", "Weijie Zhu", "Jirui Huang", "Yaxiong Chen"], "title": "LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention", "categories": ["cs.CV"], "comment": null, "summary": "Detecting AI-synthetic faces presents a critical challenge: it is hard to\ncapture consistent structural relationships between facial regions across\ndiverse generation techniques. Current methods, which focus on specific\nartifacts rather than fundamental inconsistencies, often fail when confronted\nwith novel generative models. To address this limitation, we introduce\nLayer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer\ndesigned for robust facial forgery detection. This model integrates distinct\nRegion-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation\n(LAMM) components within each layer. RG-MHA utilizes facial landmarks to create\nregional attention masks, guiding the model to scrutinize architectural\ninconsistencies across different facial areas. Crucially, the separate LAMM\nmodule dynamically generates layer-specific parameters, including mask weights\nand gating values, based on network context. These parameters then modulate the\nbehavior of RG-MHA, enabling adaptive adjustment of regional focus across\nnetwork depths. This architecture facilitates the capture of subtle,\nhierarchical forgery cues ubiquitous among diverse generation techniques, such\nas GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT\ndemonstrates superior performance, achieving 94.09% mean ACC (a +5.45%\nimprovement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results\ndemonstrate LAMM-ViT's exceptional ability to generalize and its potential for\nreliable deployment against evolving synthetic media threats."}
{"id": "2505.07744", "pdf": "https://arxiv.org/pdf/2505.07744", "abs": "https://arxiv.org/abs/2505.07744", "authors": ["Halid Ziya Yerebakan", "Kritika Iyer", "Xueqi Guo", "Yoshihisa Shinagawa", "Gerardo Hermosillo Valadez"], "title": "BodyGPS: Anatomical Positioning System", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce a new type of foundational model for parsing human anatomy in\nmedical images that works for different modalities. It supports supervised or\nunsupervised training and can perform matching, registration, classification,\nor segmentation with or without user interaction. We achieve this by training a\nneural network estimator that maps query locations to atlas coordinates via\nregression. Efficiency is improved by sparsely sampling the input, enabling\nresponse times of less than 1 ms without additional accelerator hardware. We\ndemonstrate the utility of the algorithm in both CT and MRI modalities."}
{"id": "2505.07747", "pdf": "https://arxiv.org/pdf/2505.07747", "abs": "https://arxiv.org/abs/2505.07747", "authors": ["Weiyu Li", "Xuanyang Zhang", "Zheng Sun", "Di Qi", "Hao Li", "Wei Cheng", "Weiwei Cai", "Shihao Wu", "Jiarui Liu", "Zihao Wang", "Xiao Chen", "Feipeng Tian", "Jianxiong Pan", "Zeming Li", "Gang Yu", "Xiangyu Zhang", "Daxin Jiang", "Ping Tan"], "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets", "categories": ["cs.CV"], "comment": "Technical report", "summary": "While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation."}
{"id": "2505.07812", "pdf": "https://arxiv.org/pdf/2505.07812", "abs": "https://arxiv.org/abs/2505.07812", "authors": ["Chenze Shao", "Fandong Meng", "Jie Zhou"], "title": "Continuous Visual Autoregressive Generation via Score Maximization", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Conventional wisdom suggests that autoregressive models are used to process\ndiscrete data. When applied to continuous modalities such as visual data,\nVisual AutoRegressive modeling (VAR) typically resorts to quantization-based\napproaches to cast the data into a discrete space, which can introduce\nsignificant information loss. To tackle this issue, we introduce a Continuous\nVAR framework that enables direct visual autoregressive generation without\nvector quantization. The underlying theoretical foundation is strictly proper\nscoring rules, which provide powerful statistical tools capable of evaluating\nhow well a generative model approximates the true distribution. Within this\nframework, all we need is to select a strictly proper score and set it as the\ntraining objective to optimize. We primarily explore a class of training\nobjectives based on the energy score, which is likelihood-free and thus\novercomes the difficulty of making probabilistic predictions in the continuous\nspace. Previous efforts on continuous autoregressive generation, such as GIVT\nand diffusion loss, can also be derived from our framework using other strictly\nproper scores. Source code: https://github.com/shaochenze/EAR."}
{"id": "2505.07818", "pdf": "https://arxiv.org/pdf/2505.07818", "abs": "https://arxiv.org/abs/2505.07818", "authors": ["Zeyue Xue", "Jie Wu", "Yu Gao", "Fangyuan Kong", "Lingting Zhu", "Mengzhao Chen", "Zhiheng Liu", "Wei Liu", "Qiushan Guo", "Weilin Huang", "Ping Luo"], "title": "DanceGRPO: Unleashing GRPO on Visual Generation", "categories": ["cs.CV"], "comment": "Project Page: https://dancegrpo.github.io/", "summary": "Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased."}
{"id": "2505.06250", "pdf": "https://arxiv.org/pdf/2505.06250", "abs": "https://arxiv.org/abs/2505.06250", "authors": ["Yizhuo Wu", "Yi Zhu", "Kun Qian", "Qinyu Chen", "Anding Zhu", "John Gajadharsing", "Leo C. N. de Vreede", "Chang Gao"], "title": "DeltaDPD: Exploiting Dynamic Temporal Sparsity in Recurrent Neural Networks for Energy-Efficient Wideband Digital Predistortion", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE Microwave and Wireless Technology Letters (MWTL)", "summary": "Digital Predistortion (DPD) is a popular technique to enhance signal quality\nin wideband RF power amplifiers (PAs). With increasing bandwidth and data\nrates, DPD faces significant energy consumption challenges during deployment,\ncontrasting with its efficiency goals. State-of-the-art DPD models rely on\nrecurrent neural networks (RNN), whose computational complexity hinders system\nefficiency. This paper introduces DeltaDPD, exploring the dynamic temporal\nsparsity of input signals and neuronal hidden states in RNNs for\nenergy-efficient DPD, reducing arithmetic operations and memory accesses while\npreserving satisfactory linearization performance. Applying a TM3.1a 200MHz-BW\n256-QAM OFDM signal to a 3.5 GHz GaN Doherty RF PA, DeltaDPD achieves -50.03\ndBc in Adjacent Channel Power Ratio (ACPR), -37.22 dB in Normalized Mean Square\nError (NMSE) and -38.52 dBc in Error Vector Magnitude (EVM) with 52% temporal\nsparsity, leading to a 1.8X reduction in estimated inference power. The\nDeltaDPD code will be released after formal publication at\nhttps://www.opendpd.com."}
{"id": "2505.06275", "pdf": "https://arxiv.org/pdf/2505.06275", "abs": "https://arxiv.org/abs/2505.06275", "authors": ["Yuzhou Zhu", "Zheng Zhang", "Ruyi Zhang", "Liang Zhou"], "title": "Attonsecond Streaking Phase Retrieval Via Deep Learning Methods", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.optics"], "comment": null, "summary": "Attosecond streaking phase retrieval is essential for resolving electron\ndynamics on sub-femtosecond time scales yet traditional algorithms rely on\niterative minimization and central momentum approximations that degrade\naccuracy for broadband pulses. In this work phase retrieval is reformulated as\na supervised computer-vision problem and four neural architectures are\nsystematically compared. A convolutional network demonstrates strong\nsensitivity to local streak edges but lacks global context; a vision\ntransformer captures long-range delay-energy correlations at the expense of\nlocal inductive bias; a hybrid CNN-ViT model unites local feature extraction\nand full-graph attention; and a capsule network further enforces spatial pose\nagreement through dynamic routing. A theoretical analysis introduces local,\nglobal and positional sensitivity measures and derives surrogate error bounds\nthat predict the strict ordering $CNN<ViT<Hybrid<Capsule$. Controlled\nexperiments on synthetic streaking spectrograms confirm this hierarchy, with\nthe capsule network achieving the highest retrieval fidelity. Looking forward,\nembedding the strong-field integral into physics-informed neural networks and\nexploring photonic hardware implementations promise pathways toward real-time\nattosecond pulse characterization under demanding experimental conditions."}
{"id": "2505.06277", "pdf": "https://arxiv.org/pdf/2505.06277", "abs": "https://arxiv.org/abs/2505.06277", "authors": ["John Song", "Lihao Zhang", "Feng Ye", "Haijian Sun"], "title": "Terahertz Spatial Wireless Channel Modeling with Radio Radiance Field", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.NI"], "comment": "submitted to IEEE conferences", "summary": "Terahertz (THz) communication is a key enabler for 6G systems, offering\nultra-wide bandwidth and unprecedented data rates. However, THz signal\npropagation differs significantly from lower-frequency bands due to severe free\nspace path loss, minimal diffraction and specular reflection, and prominent\nscattering, making conventional channel modeling and pilot-based estimation\napproaches inefficient. In this work, we investigate the feasibility of\napplying radio radiance field (RRF) framework to the THz band. This method\nreconstructs a continuous RRF using visual-based geometry and sparse THz RF\nmeasurements, enabling efficient spatial channel state information\n(Spatial-CSI) modeling without dense sampling. We first build a fine simulated\nTHz scenario, then we reconstruct the RRF and evaluate the performance in terms\nof both reconstruction quality and effectiveness in THz communication, showing\nthat the reconstructed RRF captures key propagation paths with sparse training\nsamples. Our findings demonstrate that RRF modeling remains effective in the\nTHz regime and provides a promising direction for scalable, low-cost spatial\nchannel reconstruction in future 6G networks."}
{"id": "2505.06285", "pdf": "https://arxiv.org/pdf/2505.06285", "abs": "https://arxiv.org/abs/2505.06285", "authors": ["Yuhan Yuan", "Xiaomo Jiang", "Yanfeng Han", "Ke Xiao"], "title": "FEMSN: Frequency-Enhanced Multiscale Network for fault diagnosis of rotating machinery under strong noise environments", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Rolling bearings are critical components of rotating machinery, and their\nproper functioning is essential for industrial production. Most existing\ncondition monitoring methods focus on extracting discriminative features from\ntime-domain signals to assess bearing health status. However, under complex\noperating conditions, periodic impulsive characteristics related to fault\ninformation are often obscured by noise interference. Consequently, existing\napproaches struggle to learn distinctive fault-related features in such\nscenarios. To address this issue, this paper proposes a novel CNN-based model\nnamed FEMSN. Specifically, a Fourier Adaptive Denoising Encoder Layer (FADEL)\nis introduced as an input denoising layer to enhance key features while\nfiltering out irrelevant information. Subsequently, a Multiscale Time-Frequency\nFusion (MSTFF) module is employed to extract fused time-frequency features,\nfurther improving the model robustness and nonlinear representation capability.\nAdditionally, a distillation layer is incorporated to expand the receptive\nfield. Based on these advancements, a novel deep lightweight CNN model, termed\nthe Frequency-Enhanced Multiscale Network (FEMSN), is developed. The\neffectiveness of FEMSN and FADEL in machine health monitoring and stability\nassessment is validated through two case studies."}
{"id": "2505.06483", "pdf": "https://arxiv.org/pdf/2505.06483", "abs": "https://arxiv.org/abs/2505.06483", "authors": ["Shehryar Khattak", "Timon Homberger", "Lukas Bernreiter", "Julian Nubert", "Olov Andersson", "Roland Siegwart", "Kostas Alexis", "Marco Hutter"], "title": "CompSLAM: Complementary Hierarchical Multi-Modal Localization and Mapping for Robot Autonomy in Underground Environments", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 9 figures, Code:\n  https://github.com/leggedrobotics/compslam_subt", "summary": "Robot autonomy in unknown, GPS-denied, and complex underground environments\nrequires real-time, robust, and accurate onboard pose estimation and mapping\nfor reliable operations. This becomes particularly challenging in\nperception-degraded subterranean conditions under harsh environmental factors,\nincluding darkness, dust, and geometrically self-similar structures. This paper\ndetails CompSLAM, a highly resilient and hierarchical multi-modal localization\nand mapping framework designed to address these challenges. Its flexible\narchitecture achieves resilience through redundancy by leveraging the\ncomplementary nature of pose estimates derived from diverse sensor modalities.\nDeveloped during the DARPA Subterranean Challenge, CompSLAM was successfully\ndeployed on all aerial, legged, and wheeled robots of Team Cerberus during\ntheir competition-winning final run. Furthermore, it has proven to be a\nreliable odometry and mapping solution in various subsequent projects, with\nextensions enabling multi-robot map sharing for marsupial robotic deployments\nand collaborative mapping. This paper also introduces a comprehensive dataset\nacquired by a manually teleoperated quadrupedal robot, covering a significant\nportion of the DARPA Subterranean Challenge finals course. This dataset\nevaluates CompSLAM's robustness to sensor degradations as the robot traverses\n740 meters in an environment characterized by highly variable geometries and\ndemanding lighting conditions. The CompSLAM code and the DARPA SubT Finals\ndataset are made publicly available for the benefit of the robotics community"}
{"id": "2505.06502", "pdf": "https://arxiv.org/pdf/2505.06502", "abs": "https://arxiv.org/abs/2505.06502", "authors": ["Md Rakibul Hasan", "Pouria Behnoudfar", "Dan MacKinlay", "Thomas Poulet"], "title": "PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations", "categories": ["eess.IV", "cs.CE", "cs.CV", "cs.LG"], "comment": null, "summary": "Machine Learning, particularly Generative Adversarial Networks (GANs), has\nrevolutionised Super Resolution (SR). However, generated images often lack\nphysical meaningfulness, which is essential for scientific applications. Our\napproach, PC-SRGAN, enhances image resolution while ensuring physical\nconsistency for interpretable simulations. PC-SRGAN significantly improves both\nthe Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure\ncompared to conventional methods, even with limited training data (e.g., only\n13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments\nphysically meaningful machine learning, incorporating numerically justified\ntime integrators and advanced quality metrics. These advancements promise\nreliable and causal machine-learning models in scientific domains. A\nsignificant advantage of PC-SRGAN over conventional SR techniques is its\nphysical consistency, which makes it a viable surrogate model for\ntime-dependent problems. PC-SRGAN advances scientific machine learning,\noffering improved accuracy and efficiency for image processing, enhanced\nprocess understanding, and broader applications to scientific research. The\nsource codes and data will be made publicly available at\nhttps://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper."}
{"id": "2505.06507", "pdf": "https://arxiv.org/pdf/2505.06507", "abs": "https://arxiv.org/abs/2505.06507", "authors": ["Haoyang Xie", "Feng Ju"], "title": "Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Computer-aided design (CAD) is fundamental to modern engineering and\nmanufacturing, but creating CAD models still requires expert knowledge and\nspecialized software. Recent advances in large language models (LLMs) open up\nthe possibility of generative CAD, where natural language is directly\ntranslated into parametric 3D models. However, most existing methods generate\ntask-specific command sequences that pretrained models cannot directly handle.\nThese sequences must be converted into CAD representations such as CAD vectors\nbefore a 3D model can be produced, which requires training models from scratch\nand adds unnecessary complexity. To tackle this issue, we propose generating\nCadQuery code directly from text, leveraging the strengths of pretrained LLMs\nto produce 3D models without intermediate representations, using this\nPython-based scripting language. Since LLMs already excel at Python generation\nand spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly\neffective. Given that these capabilities typically improve with scale, we\nhypothesize that larger models will perform better after fine-tuning. To enable\nthis, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We\nfine-tune six open-source LLMs of varying sizes and observe consistent\nimprovements. Our best model achieves a top-1 exact match of 69.3%, up from\n58.8%, and reduces Chamfer Distance by 48.6%. Project page:\nhttps://github.com/Text-to-CadQuery/Text-to-CadQuery."}
{"id": "2505.06594", "pdf": "https://arxiv.org/pdf/2505.06594", "abs": "https://arxiv.org/abs/2505.06594", "authors": ["Galann Pennec", "Zhengyuan Liu", "Nicholas Asher", "Philippe Muller", "Nancy F. Chen"], "title": "Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) often struggle to balance visual and textual\ninformation when summarizing complex multimodal inputs, such as entire TV show\nepisodes. In this paper, we propose a zero-shot video-to-text summarization\napproach that builds its own screenplay representation of an episode,\neffectively integrating key video moments, dialogue, and character information\ninto a unified document. Unlike previous approaches, we simultaneously generate\nscreenplays and name the characters in zero-shot, using only the audio, video,\nand transcripts as input. Additionally, we highlight that existing\nsummarization metrics can fail to assess the multimodal content in summaries.\nTo address this, we introduce MFactSum, a multimodal metric that evaluates\nsummaries with respect to both vision and text modalities. Using MFactSum, we\nevaluate our screenplay summaries on the SummScreen3D dataset, demonstrating\nsuperiority against state-of-the-art VLMs such as Gemini 1.5 by generating\nsummaries containing 20% more relevant visual information while requiring 75%\nless of the video as input."}
{"id": "2505.06595", "pdf": "https://arxiv.org/pdf/2505.06595", "abs": "https://arxiv.org/abs/2505.06595", "authors": ["Hai-Vy Nguyen", "Fabrice Gamboa", "Sixin Zhang", "Reda Chhaibi", "Serge Gratton", "Thierry Giaccone"], "title": "Feature Representation Transferring to Lightweight Models via Perception Coherence", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "math.PR"], "comment": null, "summary": "In this paper, we propose a method for transferring feature representation to\nlightweight student models from larger teacher models. We mathematically define\na new notion called \\textit{perception coherence}. Based on this notion, we\npropose a loss function, which takes into account the dissimilarities between\ndata points in feature space through their ranking. At a high level, by\nminimizing this loss function, the student model learns to mimic how the\nteacher model \\textit{perceives} inputs. More precisely, our method is\nmotivated by the fact that the representational capacity of the student model\nis weaker than the teacher model. Hence, we aim to develop a new method\nallowing for a better relaxation. This means that, the student model does not\nneed to preserve the absolute geometry of the teacher one, while preserving\nglobal coherence through dissimilarity ranking. Our theoretical insights\nprovide a probabilistic perspective on the process of feature representation\ntransfer. Our experiments results show that our method outperforms or achieves\non-par performance compared to strong baseline methods for representation\ntransferring."}
{"id": "2505.06621", "pdf": "https://arxiv.org/pdf/2505.06621", "abs": "https://arxiv.org/abs/2505.06621", "authors": ["Thamiris Coelho", "Leo S. F. Ribeiro", "João Macedo", "Jefersson A. dos Santos", "Sandra Avila"], "title": "Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models", "categories": ["cs.LG", "cs.CV"], "comment": "ACM Conference on Fairness, Accountability, and Transparency (FAccT\n  2025)", "summary": "The distribution of child sexual abuse imagery (CSAI) is an ever-growing\nconcern of our modern world; children who suffered from this heinous crime are\nrevictimized, and the growing amount of illegal imagery distributed overwhelms\nlaw enforcement agents (LEAs) with the manual labor of categorization. To ease\nthis burden researchers have explored methods for automating data triage and\ndetection of CSAI, but the sensitive nature of the data imposes restricted\naccess and minimal interaction between real data and learning algorithms,\navoiding leaks at all costs. In observing how these restrictions have shaped\nthe literature we formalize a definition of \"Proxy Tasks\", i.e., the substitute\ntasks used for training models for CSAI without making use of CSA data. Under\nthis new terminology we review current literature and present a protocol for\nmaking conscious use of Proxy Tasks together with consistent input from LEAs to\ndesign better automation in this field. Finally, we apply this protocol to\nstudy -- for the first time -- the task of Few-shot Indoor Scene Classification\non CSAI, showing a final model that achieves promising results on a real-world\nCSAI dataset whilst having no weights actually trained on sensitive data."}
{"id": "2505.06646", "pdf": "https://arxiv.org/pdf/2505.06646", "abs": "https://arxiv.org/abs/2505.06646", "authors": ["Daniel Strick", "Carlos Garcia", "Anthony Huang"], "title": "Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "12 pages, 4 figures", "summary": "Deep learning for radiologic image analysis is a rapidly growing field in\nbiomedical research and is likely to become a standard practice in modern\nmedicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray\nimages that are classified by the presence or absence of 14 different diseases,\nwe reproduced an algorithm known as CheXNet, as well as explored other\nalgorithms that outperform CheXNet's baseline metrics. Model performance was\nprimarily evaluated using the F1 score and AUC-ROC, both of which are critical\nmetrics for imbalanced, multi-label classification tasks in medical imaging.\nThe best model achieved an average AUC-ROC score of 0.85 and an average F1\nscore of 0.39 across all 14 disease classifications present in the dataset."}
{"id": "2505.06685", "pdf": "https://arxiv.org/pdf/2505.06685", "abs": "https://arxiv.org/abs/2505.06685", "authors": ["Dawei Huang", "Qing Li", "Chuan Yan", "Zebang Cheng", "Yurong Huang", "Xiang Li", "Bin Li", "Xiaohui Wang", "Zheng Lian", "Xiaojiang Peng"], "title": "Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Emotion understanding in videos aims to accurately recognize and interpret\nindividuals' emotional states by integrating contextual, visual, textual, and\nauditory cues. While Large Multimodal Models (LMMs) have demonstrated\nsignificant progress in general vision-language (VL) tasks, their performance\nin emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on\nemotion-related tasks often leads to catastrophic forgetting, hindering their\nability to generalize across diverse tasks. To address these challenges, we\npresent Emotion-Qwen, a tailored multimodal framework designed to enhance both\nemotion understanding and general VL reasoning. Emotion-Qwen incorporates a\nsophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,\nwhich dynamically routes inputs to balance emotion-specific and general-purpose\nprocessing. The model is pre-trained in a three-stage pipeline on large-scale\ngeneral and emotional image datasets to support robust multimodal\nrepresentations. Furthermore, we construct the Video Emotion Reasoning (VER)\ndataset, comprising more than 40K bilingual video clips with fine-grained\ndescriptive annotations, to further enrich Emotion-Qwen's emotional reasoning\ncapability. Experimental results demonstrate that Emotion-Qwen achieves\nstate-of-the-art performance on multiple emotion recognition benchmarks, while\nmaintaining competitive results on general VL tasks. Code and models are\navailable at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous."}
{"id": "2505.06746", "pdf": "https://arxiv.org/pdf/2505.06746", "abs": "https://arxiv.org/abs/2505.06746", "authors": ["Morui Zhu", "Yongqi Zhu", "Yihao Zhu", "Qi Chen", "Deyuan Qu", "Song Fu", "Qing Yang"], "title": "M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark", "categories": ["cs.RO", "cs.CV", "I.2.10; I.2.9"], "comment": "supplementary material included", "summary": "We introduce M$^3$CAD, a novel benchmark designed to advance research in\ngeneric cooperative autonomous driving. M$^3$CAD comprises 204 sequences with\n30k frames, spanning a diverse range of cooperative driving scenarios. Each\nsequence includes multiple vehicles and sensing modalities, e.g., LiDAR point\nclouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving\ntasks, including object detection and tracking, mapping, motion forecasting,\noccupancy prediction, and path planning. This rich multimodal setup enables\nM$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving\nresearch, significantly broadening the scope of research in the field. To our\nknowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored\nfor cooperative multi-task autonomous driving research. We evaluate the\nstate-of-the-art end-to-end solution on M$^3$CAD to establish baseline\nperformance. To foster cooperative autonomous driving research, we also propose\nE2EC, a simple yet effective framework for cooperative driving solution that\nleverages inter-vehicle shared information for improved path planning. We\nrelease M$^3$CAD, along with our baseline models and evaluation results, to\nsupport the development of robust cooperative autonomous driving systems. All\nresources will be made publicly available on https://github.com/zhumorui/M3CAD"}
{"id": "2505.06793", "pdf": "https://arxiv.org/pdf/2505.06793", "abs": "https://arxiv.org/abs/2505.06793", "authors": ["Erik Großkopf", "Valay Bundele", "Mehran Hossienzadeh", "Hendrik P. A. Lensch"], "title": "HistDiST: Histopathological Diffusion-based Stain Transfer", "categories": ["eess.IV", "cs.CV"], "comment": "8 pages, 4 figures", "summary": "Hematoxylin and Eosin (H&E) staining is the cornerstone of histopathology but\nlacks molecular specificity. While Immunohistochemistry (IHC) provides\nmolecular insights, it is costly and complex, motivating H&E-to-IHC translation\nas a cost-effective alternative. Existing translation methods are mainly\nGAN-based, often struggling with training instability and limited structural\nfidelity, while diffusion-based approaches remain underexplored. We propose\nHistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity\nH&E-to-IHC translation. HistDiST introduces a dual-conditioning strategy,\nutilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&E\nrepresentations to ensure pathology-relevant context and structural\nconsistency. To overcome brightness biases, we incorporate a rescaled noise\nschedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition\nat the final timestep. During inference, DDIM inversion preserves the\nmorphological structure, while an eta-cosine noise schedule introduces\ncontrolled stochasticity, balancing structural consistency and molecular\nfidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel\npathology-aware metric leveraging GigaPath embeddings to assess molecular\nrelevance. Extensive evaluations on MIST and BCI datasets demonstrate that\nHistDiST significantly outperforms existing methods, achieving a 28%\nimprovement in MRA on the H&E-to-Ki67 translation task, highlighting its\neffectiveness in capturing true IHC semantics."}
{"id": "2505.06803", "pdf": "https://arxiv.org/pdf/2505.06803", "abs": "https://arxiv.org/abs/2505.06803", "authors": ["Xilin Jiang", "Junkai Wu", "Vishal Choudhari", "Nima Mesgarani"], "title": "Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Audio large language models (LLMs) are considered experts at recognizing\nsound objects, yet their performance relative to LLMs in other sensory\nmodalities, such as visual or audio-visual LLMs, and to humans using their\nears, eyes, or both remains unexplored. To investigate this, we systematically\nevaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio,\nQwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of\ndifferent classes from audio-only, silent video, or sounded video inputs. We\nuncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the\nsensory discrepancy between human ears and eyes. To reduce this gap, we\nintroduce a cross-modal distillation framework, where an LLM in one modality\nserves as the teacher and another as the student, with knowledge transfer in\nsound classes predicted as more challenging to the student by a heuristic\nmodel. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice\nversa, leads to notable improvements, particularly in challenging classes. This\nwork highlights the sensory gap in LLMs from a human-aligned perspective and\nproposes a principled approach to enhancing modality-specific perception in\nmultimodal LLMs."}
{"id": "2505.06811", "pdf": "https://arxiv.org/pdf/2505.06811", "abs": "https://arxiv.org/abs/2505.06811", "authors": ["Tan-Hanh Pham", "Ovidiu C. Andronesi", "Xianqi Li", "Kim-Doang Nguyen"], "title": "Missing Data Estimation for MR Spectroscopic Imaging via Mask-Free Deep Learning Methods", "categories": ["eess.IV", "cs.CV"], "comment": "8 pages", "summary": "Magnetic Resonance Spectroscopic Imaging (MRSI) is a powerful tool for\nnon-invasive mapping of brain metabolites, providing critical insights into\nneurological conditions. However, its utility is often limited by missing or\ncorrupted data due to motion artifacts, magnetic field inhomogeneities, or\nfailed spectral fitting-especially in high resolution 3D acquisitions. To\naddress this, we propose the first deep learning-based, mask-free framework for\nestimating missing data in MRSI metabolic maps. Unlike conventional restoration\nmethods that rely on explicit masks to identify missing regions, our approach\nimplicitly detects and estimates these areas using contextual spatial features\nthrough 2D and 3D U-Net architectures. We also introduce a progressive training\nstrategy to enhance robustness under varying levels of data degradation. Our\nmethod is evaluated on both simulated and real patient datasets and\nconsistently outperforms traditional interpolation techniques such as cubic and\nlinear interpolation. The 2D model achieves an MSE of 0.002 and an SSIM of 0.97\nwith 20% missing voxels, while the 3D model reaches an MSE of 0.001 and an SSIM\nof 0.98 with 15% missing voxels. Qualitative results show improved fidelity in\nestimating missing data, particularly in metabolically heterogeneous regions\nand ventricular regions. Importantly, our model generalizes well to real-world\ndatasets without requiring retraining or mask input. These findings demonstrate\nthe effectiveness and broad applicability of mask-free deep learning for MRSI\nrestoration, with strong potential for clinical and research integration."}
{"id": "2505.06861", "pdf": "https://arxiv.org/pdf/2505.06861", "abs": "https://arxiv.org/abs/2505.06861", "authors": ["Dongxiu Liu", "Haoyi Niu", "Zhihao Wang", "Jinliang Zheng", "Yinan Zheng", "Zhonghong Ou", "Jianming Hu", "Jianxiong Li", "Xianyuan Zhan"], "title": "Efficient Robotic Policy Learning via Latent Space Backward Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Current robotic planning methods often rely on predicting multi-frame images\nwith full pixel details. While this fine-grained approach can serve as a\ngeneric world model, it introduces two significant challenges for downstream\npolicy learning: substantial computational costs that hinder real-time\ndeployment, and accumulated inaccuracies that can mislead action extraction.\nPlanning with coarse-grained subgoals partially alleviates efficiency issues.\nHowever, their forward planning schemes can still result in off-task\npredictions due to accumulation errors, leading to misalignment with long-term\ngoals. This raises a critical question: Can robotic planning be both efficient\nand accurate enough for real-time control in long-horizon, multi-stage tasks?\nTo address this, we propose a Latent Space Backward Planning scheme (LBP),\nwhich begins by grounding the task into final latent goals, followed by\nrecursively predicting intermediate subgoals closer to the current state. The\ngrounded final goal enables backward subgoal planning to always remain aware of\ntask completion, facilitating on-task prediction along the entire planning\nhorizon. The subgoal-conditioned policy incorporates a learnable token to\nsummarize the subgoal sequences and determines how each subgoal guides action\nextraction. Through extensive simulation and real-robot long-horizon\nexperiments, we show that LBP outperforms existing fine-grained and forward\nplanning methods, achieving SOTA performance. Project Page:\nhttps://lbp-authors.github.io"}
{"id": "2505.06890", "pdf": "https://arxiv.org/pdf/2505.06890", "abs": "https://arxiv.org/abs/2505.06890", "authors": ["Kosuke Ukita", "Ye Xiaolong", "Tsuyoshi Okita"], "title": "Image Classification Using a Diffusion Model as a Pre-Training Model", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": "10 pages, 9 figures", "summary": "In this paper, we propose a diffusion model that integrates a\nrepresentation-conditioning mechanism, where the representations derived from a\nVision Transformer (ViT) are used to condition the internal process of a\nTransformer-based diffusion model. This approach enables\nrepresentation-conditioned data generation, addressing the challenge of\nrequiring large-scale labeled datasets by leveraging self-supervised learning\non unlabeled data. We evaluate our method through a zero-shot classification\ntask for hematoma detection in brain imaging. Compared to the strong\ncontrastive learning baseline, DINOv2, our method achieves a notable\nimprovement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its\neffectiveness in image classification."}
{"id": "2505.06907", "pdf": "https://arxiv.org/pdf/2505.06907", "abs": "https://arxiv.org/abs/2505.06907", "authors": ["Yu Qiao", "Huy Q. Le", "Avi Deb Raha", "Phuong-Nam Tran", "Apurba Adhikary", "Mengchun Zhang", "Loc X. Nguyen", "Eui-Nam Huh", "Dusit Niyato", "Choong Seon Hong"], "title": "Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence", "categories": ["cs.AI", "cs.CV", "cs.NE"], "comment": "On going work", "summary": "The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and\nGrok-3, has reshaped the artificial intelligence landscape. As prominent\nexamples of foundational models (FMs) built on LLMs, these models exhibit\nremarkable capabilities in generating human-like content, bringing us closer to\nachieving artificial general intelligence (AGI). However, their large-scale\nnature, sensitivity to privacy concerns, and substantial computational demands\npresent significant challenges to personalized customization for end users. To\nbridge this gap, this paper presents the vision of artificial personalized\nintelligence (API), focusing on adapting these powerful models to meet the\nspecific needs and preferences of users while maintaining privacy and\nefficiency. Specifically, this paper proposes personalized federated\nintelligence (PFI), which integrates the privacy-preserving advantages of\nfederated learning (FL) with the zero-shot generalization capabilities of FMs,\nenabling personalized, efficient, and privacy-protective deployment at the\nedge. We first review recent advances in both FL and FMs, and discuss the\npotential of leveraging FMs to enhance federated systems. We then present the\nkey motivations behind realizing PFI and explore promising opportunities in\nthis space, including efficient PFI, trustworthy PFI, and PFI empowered by\nretrieval-augmented generation (RAG). Finally, we outline key challenges and\nfuture research directions for deploying FM-powered FL systems at the edge with\nimproved personalization, computational efficiency, and privacy guarantees.\nOverall, this survey aims to lay the groundwork for the development of API as a\ncomplement to AGI, with a particular focus on PFI as a key enabling technique."}
{"id": "2505.06918", "pdf": "https://arxiv.org/pdf/2505.06918", "abs": "https://arxiv.org/abs/2505.06918", "authors": ["Yanhui Hong", "Nan Wang", "Zhiyi Xia", "Haoyi Tao", "Xi Fang", "Yiming Li", "Jiankun Wang", "Peng Jin", "Xiaochen Cai", "Shengyu Li", "Ziqi Chen", "Zezhong Zhang", "Guolin Ke", "Linfeng Zhang"], "title": "Uni-AIMS: AI-Powered Microscopy Image Analysis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper presents a systematic solution for the intelligent recognition and\nautomatic analysis of microscopy images. We developed a data engine that\ngenerates high-quality annotated datasets through a combination of the\ncollection of diverse microscopy images from experiments, synthetic data\ngeneration and a human-in-the-loop annotation process. To address the unique\nchallenges of microscopy images, we propose a segmentation model capable of\nrobustly detecting both small and large objects. The model effectively\nidentifies and separates thousands of closely situated targets, even in\ncluttered visual environments. Furthermore, our solution supports the precise\nautomatic recognition of image scale bars, an essential feature in quantitative\nmicroscopic analysis. Building upon these components, we have constructed a\ncomprehensive intelligent analysis platform and validated its effectiveness and\npracticality in real-world applications. This study not only advances automatic\nrecognition in microscopy imaging but also ensures scalability and\ngeneralizability across multiple application domains, offering a powerful tool\nfor automated microscopic analysis in interdisciplinary research."}
{"id": "2505.06934", "pdf": "https://arxiv.org/pdf/2505.06934", "abs": "https://arxiv.org/abs/2505.06934", "authors": ["Roy Betser", "Meir Yossef Levi", "Guy Gilboa"], "title": "Whitened CLIP as a Likelihood Surrogate of Images and Captions", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to ICML 2025. This version matches the camera-ready version", "summary": "Likelihood approximations for images are not trivial to compute and can be\nuseful in many applications. We examine the use of Contrastive Language-Image\nPre-training (CLIP) to assess the likelihood of images and captions. We\nintroduce \\textit{Whitened CLIP}, a novel transformation of the CLIP latent\nspace via an invertible linear operation. This transformation ensures that each\nfeature in the embedding space has zero mean, unit standard deviation, and no\ncorrelation with all other features, resulting in an identity covariance\nmatrix. We show that the whitened embeddings statistics can be well\napproximated as a standard normal distribution, thus, the log-likelihood is\nestimated simply by the square Euclidean norm in the whitened embedding space.\nThe whitening procedure is completely training-free and performed using a\npre-computed whitening matrix, hence, is very fast. We present several\npreliminary experiments demonstrating the properties and applicability of these\nlikelihood scores to images and captions."}
{"id": "2505.06963", "pdf": "https://arxiv.org/pdf/2505.06963", "abs": "https://arxiv.org/abs/2505.06963", "authors": ["Tarik Houichime", "Younes EL Amrani"], "title": "Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper introduces an innovative approach for the autonomous landing of\nUnmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera,\ntherefore obviating the requirement for depth estimation cameras. Drawing on\nthe inherent human estimating process, the proposed method reframes the landing\ntask as an optimization problem. The UAV employs variations in the visual\ncharacteristics of a specially designed lenticular circle on the landing pad,\nwhere the perceived color and form provide critical information for estimating\nboth altitude and depth. Reinforcement learning algorithms are utilized to\napproximate the functions governing these estimations, enabling the UAV to\nascertain ideal landing settings via training. This method's efficacy is\nassessed by simulations and experiments, showcasing its potential for robust\nand accurate autonomous landing without dependence on complex sensor setups.\nThis research contributes to the advancement of cost-effective and efficient\nUAV landing solutions, paving the way for wider applicability across various\nfields."}
{"id": "2505.06980", "pdf": "https://arxiv.org/pdf/2505.06980", "abs": "https://arxiv.org/abs/2505.06980", "authors": ["Lei Wan", "Prabesh Gupta", "Andreas Eich", "Marcel Kettelgerdes", "Hannan Ejaz Keen", "Michael Klöppel-Gersdorf", "Alexey Vinel"], "title": "VALISENS: A Validated Innovative Multi-Sensor System for Cooperative Automated Driving", "categories": ["cs.RO", "cs.CV"], "comment": "7 pages, 11 figures, submitted to IEEE ITSC", "summary": "Perception is a core capability of automated vehicles and has been\nsignificantly advanced through modern sensor technologies and artificial\nintelligence. However, perception systems still face challenges in complex\nreal-world scenarios. To improve robustness against various external factors,\nmulti-sensor fusion techniques are essential, combining the strengths of\ndifferent sensor modalities. With recent developments in Vehicle-to-Everything\n(V2X communication, sensor fusion can now extend beyond a single vehicle to a\ncooperative multi-agent system involving Connected Automated Vehicle (CAV) and\nintelligent infrastructure. This paper presents VALISENS, an innovative\nmulti-sensor system distributed across multiple agents. It integrates onboard\nand roadside LiDARs, radars, thermal cameras, and RGB cameras to enhance\nsituational awareness and support cooperative automated driving. The thermal\ncamera adds critical redundancy for perceiving Vulnerable Road User (VRU),\nwhile fusion with roadside sensors mitigates visual occlusions and extends the\nperception range beyond the limits of individual vehicles. We introduce the\ncorresponding perception module built on this sensor system, which includes\nobject detection, tracking, motion forecasting, and high-level data fusion. The\nproposed system demonstrates the potential of cooperative perception in\nreal-world test environments and lays the groundwork for future Cooperative\nIntelligent Transport Systems (C-ITS) applications."}
{"id": "2505.06993", "pdf": "https://arxiv.org/pdf/2505.06993", "abs": "https://arxiv.org/abs/2505.06993", "authors": ["Yuxuan He", "Junpeng Zhang", "Hongyuan Zhang", "Quanshi Zhang"], "title": "Towards the Three-Phase Dynamics of Generalization Power of a DNN", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses."}
{"id": "2505.07085", "pdf": "https://arxiv.org/pdf/2505.07085", "abs": "https://arxiv.org/abs/2505.07085", "authors": ["Matt Franchi", "Hauke Sandhaus", "Madiha Zahrah Choksi", "Severin Engelmann", "Wendy Ju", "Helen Nissenbaum"], "title": "Privacy of Groups in Dense Street Imagery", "categories": ["cs.CY", "cs.CV", "cs.ET"], "comment": "To appear in ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) '25", "summary": "Spatially and temporally dense street imagery (DSI) datasets have grown\nunbounded. In 2024, individual companies possessed around 3 trillion unique\nimages of public streets. DSI data streams are only set to grow as companies\nlike Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze\ncollisions. Academic researchers leverage DSI to explore novel approaches to\nurban analysis. Despite good-faith efforts by DSI providers to protect\nindividual privacy through blurring faces and license plates, these measures\nfail to address broader privacy concerns. In this work, we find that increased\ndata density and advancements in artificial intelligence enable harmful group\nmembership inferences from supposedly anonymized data. We perform a penetration\ntest to demonstrate how easily sensitive group affiliations can be inferred\nfrom obfuscated pedestrians in 25,232,608 dashcam images taken in New York\nCity. We develop a typology of identifiable groups within DSI and analyze\nprivacy implications through the lens of contextual integrity. Finally, we\ndiscuss actionable recommendations for researchers working with data from DSI\nproviders."}
{"id": "2505.07110", "pdf": "https://arxiv.org/pdf/2505.07110", "abs": "https://arxiv.org/abs/2505.07110", "authors": ["Tong Zhang", "Fenghua Shao", "Runsheng Zhang", "Yifan Zhuang", "Liuqingqing Yang"], "title": "DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Based on the DeepSORT algorithm, this study explores the application of\nvisual tracking technology in intelligent human-computer interaction,\nespecially in the field of gesture recognition and tracking. With the rapid\ndevelopment of artificial intelligence and deep learning technology,\nvisual-based interaction has gradually replaced traditional input devices and\nbecome an important way for intelligent systems to interact with users. The\nDeepSORT algorithm can achieve accurate target tracking in dynamic environments\nby combining Kalman filters and deep learning feature extraction methods. It is\nespecially suitable for complex scenes with multi-target tracking and fast\nmovements. This study experimentally verifies the superior performance of\nDeepSORT in gesture recognition and tracking. It can accurately capture and\ntrack the user's gesture trajectory and is superior to traditional tracking\nmethods in terms of real-time and accuracy. In addition, this study also\ncombines gesture recognition experiments to evaluate the recognition ability\nand feedback response of the DeepSORT algorithm under different gestures (such\nas sliding, clicking, and zooming). The experimental results show that DeepSORT\ncan not only effectively deal with target occlusion and motion blur but also\ncan stably track in a multi-target environment, achieving a smooth user\ninteraction experience. Finally, this paper looks forward to the future\ndevelopment direction of intelligent human-computer interaction systems based\non visual tracking and proposes future research focuses such as algorithm\noptimization, data fusion, and multimodal interaction in order to promote a\nmore intelligent and personalized interactive experience. Keywords-DeepSORT,\nvisual tracking, gesture recognition, human-computer interaction"}
{"id": "2505.07159", "pdf": "https://arxiv.org/pdf/2505.07159", "abs": "https://arxiv.org/abs/2505.07159", "authors": ["Jong Sung Park", "Juhyung Ha", "Siddhesh Thakur", "Alexandra Badea", "Spyridon Bakas", "Eleftherios Garyfallidis"], "title": "Skull stripping with purely synthetic data", "categories": ["eess.IV", "cs.CV"], "comment": "Oral at ISMRM 2025", "summary": "While many skull stripping algorithms have been developed for multi-modal and\nmulti-species cases, there is still a lack of a fundamentally generalizable\napproach. We present PUMBA(PUrely synthetic Multimodal/species invariant Brain\nextrAction), a strategy to train a model for brain extraction with no real\nbrain images or labels. Our results show that even without any real images or\nanatomical priors, the model achieves comparable accuracy in multi-modal,\nmulti-species and pathological cases. This work presents a new direction of\nresearch for any generalizable medical image segmentation task."}
{"id": "2505.07175", "pdf": "https://arxiv.org/pdf/2505.07175", "abs": "https://arxiv.org/abs/2505.07175", "authors": ["Yash Deo", "Yan Jia", "Toni Lassila", "William A. P. Smith", "Tom Lawton", "Siyuan Kang", "Alejandro F. Frangi", "Ibrahim Habli"], "title": "Metrics that matter: Evaluating image quality metrics for medical image generation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Evaluating generative models for synthetic medical imaging is crucial yet\nchallenging, especially given the high standards of fidelity, anatomical\naccuracy, and safety required for clinical applications. Standard evaluation of\ngenerated images often relies on no-reference image quality metrics when ground\ntruth images are unavailable, but their reliability in this complex domain is\nnot well established. This study comprehensively assesses commonly used\nno-reference image quality metrics using brain MRI data, including tumour and\nvascular images, providing a representative exemplar for the field. We\nsystematically evaluate metric sensitivity to a range of challenges, including\nnoise, distribution shifts, and, critically, localised morphological\nalterations designed to mimic clinically relevant inaccuracies. We then compare\nthese metric scores against model performance on a relevant downstream\nsegmentation task, analysing results across both controlled image perturbations\nand outputs from different generative model architectures. Our findings reveal\nsignificant limitations: many widely-used no-reference image quality metrics\ncorrelate poorly with downstream task suitability and exhibit a profound\ninsensitivity to localised anatomical details crucial for clinical validity.\nFurthermore, these metrics can yield misleading scores regarding distribution\nshifts, e.g. data memorisation. This reveals the risk of misjudging model\nreadiness, potentially leading to the deployment of flawed tools that could\ncompromise patient safety. We conclude that ensuring generative models are\ntruly fit for clinical purpose requires a multifaceted validation framework,\nintegrating performance on relevant downstream tasks with the cautious\ninterpretation of carefully selected no-reference image quality metrics."}
{"id": "2505.07214", "pdf": "https://arxiv.org/pdf/2505.07214", "abs": "https://arxiv.org/abs/2505.07214", "authors": ["Pascal Spiegler", "Arash Harirpoush", "Yiming Xiao"], "title": "Towards user-centered interactive medical image segmentation in VR with an assistive AI agent", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "Crucial in disease analysis and surgical planning, manual segmentation of\nvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and\nchallenging to master, while fully automatic algorithms can benefit from\nuser-feedback. Therefore, with the complementary power of the latest\nradiological AI foundation models and virtual reality (VR)'s intuitive data\ninteraction, we propose SAMIRA, a novel conversational AI agent that assists\nusers with localizing, segmenting, and visualizing 3D medical concepts in VR.\nThrough speech-based interaction, the agent helps users understand radiological\nfeatures, locate clinical targets, and generate segmentation masks that can be\nrefined with just a few point prompts. The system also supports true-to-scale\n3D visualization of segmented pathology to enhance patient-specific anatomical\nunderstanding. Furthermore, to determine the optimal interaction paradigm under\nnear-far attention-switching for refining segmentation masks in an immersive,\nhuman-in-the-loop workflow, we compare VR controller pointing, head pointing,\nand eye tracking as input modes. With a user study, evaluations demonstrated a\nhigh usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as\nstrong support for the proposed VR system's guidance, training potential, and\nintegration of AI in radiological segmentation tasks."}
{"id": "2505.07349", "pdf": "https://arxiv.org/pdf/2505.07349", "abs": "https://arxiv.org/abs/2505.07349", "authors": ["Badhan Kumar Das", "Gengyan Zhao", "Boris Mailhe", "Thomas J. Re", "Dorin Comaniciu", "Eli Gibson", "Andreas Maier"], "title": "Multi-Plane Vision Transformer for Hemorrhage Classification Using Axial and Sagittal MRI Data", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages", "summary": "Identifying brain hemorrhages from magnetic resonance imaging (MRI) is a\ncritical task for healthcare professionals. The diverse nature of MRI\nacquisitions with varying contrasts and orientation introduce complexity in\nidentifying hemorrhage using neural networks. For acquisitions with varying\norientations, traditional methods often involve resampling images to a fixed\nplane, which can lead to information loss. To address this, we propose a 3D\nmulti-plane vision transformer (MP-ViT) for hemorrhage classification with\nvarying orientation data. It employs two separate transformer encoders for\naxial and sagittal contrasts, using cross-attention to integrate information\nacross orientations. MP-ViT also includes a modality indication vector to\nprovide missing contrast information to the model. The effectiveness of the\nproposed model is demonstrated with extensive experiments on real world\nclinical dataset consists of 10,084 training, 1,289 validation and 1,496 test\nsubjects. MP-ViT achieved substantial improvement in area under the curve\n(AUC), outperforming the vision transformer (ViT) by 5.5% and CNN-based\narchitectures by 1.8%. These results highlight the potential of MP-ViT in\nimproving performance for hemorrhage detection when different orientation\ncontrasts are needed."}
{"id": "2505.07411", "pdf": "https://arxiv.org/pdf/2505.07411", "abs": "https://arxiv.org/abs/2505.07411", "authors": ["Wenhao Hu", "Paul Henderson", "José Cano"], "title": "ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "Pruning is a widely used method for compressing Deep Neural Networks (DNNs),\nwhere less relevant parameters are removed from a DNN model to reduce its size.\nHowever, removing parameters reduces model accuracy, so pruning is typically\ncombined with fine-tuning, and sometimes other operations such as rewinding\nweights, to recover accuracy. A common approach is to repeatedly prune and then\nfine-tune, with increasing amounts of model parameters being removed in each\nstep. While straightforward to implement, pruning pipelines that follow this\napproach are computationally expensive due to the need for repeated\nfine-tuning.\n  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs\nthat significantly decreases the time required for pruning by reducing the\noverall cost of fine-tuning, while maintaining a similar accuracy to existing\npruning pipelines. ICE-Pruning is based on three main components: i) an\nautomatic mechanism to determine after which pruning steps fine-tuning should\nbe performed; ii) a freezing strategy for faster fine-tuning in each pruning\nstep; and iii) a custom pruning-aware learning rate scheduler to further\nimprove the accuracy of each pruning step and reduce the overall time\nconsumption. We also propose an efficient auto-tuning stage for the\nhyperparameters (e.g., freezing percentage) introduced by the three components.\nWe evaluate ICE-Pruning on several DNN models and datasets, showing that it can\naccelerate pruning by up to 9.61x. Code is available at\nhttps://github.com/gicLAB/ICE-Pruning"}
{"id": "2505.07447", "pdf": "https://arxiv.org/pdf/2505.07447", "abs": "https://arxiv.org/abs/2505.07447", "authors": ["Peng Sun", "Yi Jiang", "Tao Lin"], "title": "Unified Continuous Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "https://github.com/LINs-lab/UCGM", "summary": "Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM."}
{"id": "2505.07449", "pdf": "https://arxiv.org/pdf/2505.07449", "abs": "https://arxiv.org/abs/2505.07449", "authors": ["Wei Li", "Ming Hu", "Guoan Wang", "Lihao Liu", "Kaijin Zhou", "Junzhi Ning", "Xin Guo", "Zongyuan Ge", "Lixu Gu", "Junjun He"], "title": "Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In ophthalmic surgery, developing an AI system capable of interpreting\nsurgical videos and predicting subsequent operations requires numerous\nophthalmic surgical videos with high-quality annotations, which are difficult\nto collect due to privacy concerns and labor consumption. Text-guided video\ngeneration (T2V) emerges as a promising solution to overcome this issue by\ngenerating ophthalmic surgical videos based on surgeon instructions. In this\npaper, we present Ophora, a pioneering model that can generate ophthalmic\nsurgical videos following natural language instructions. To construct Ophora,\nwe first propose a Comprehensive Data Curation pipeline to convert narrative\nophthalmic surgical videos into a large-scale, high-quality dataset comprising\nover 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive\nVideo-Instruction Tuning scheme to transfer rich spatial-temporal knowledge\nfrom a T2V model pre-trained on natural video-text datasets for\nprivacy-preserved ophthalmic surgical video generation based on Ophora-160K.\nExperiments on video quality evaluation via quantitative analysis and\nophthalmologist feedback demonstrate that Ophora can generate realistic and\nreliable ophthalmic surgical videos based on surgeon instructions. We also\nvalidate the capability of Ophora for empowering downstream tasks of ophthalmic\nsurgical workflow understanding. Code is available at\nhttps://github.com/mar-cry/Ophora."}
{"id": "2505.07477", "pdf": "https://arxiv.org/pdf/2505.07477", "abs": "https://arxiv.org/abs/2505.07477", "authors": ["Hongkun Dou", "Zeyu Li", "Xingyu Jiang", "Hongjue Li", "Lijun Yang", "Wen Yao", "Yue Deng"], "title": "You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models (DMs) have recently demonstrated remarkable success in\nmodeling large-scale data distributions. However, many downstream tasks require\nguiding the generated content based on specific differentiable metrics,\ntypically necessitating backpropagation during the generation process. This\napproach is computationally expensive, as generating with DMs often demands\ntens to hundreds of recursive network calls, resulting in high memory usage and\nsignificant time consumption. In this paper, we propose a more efficient\nalternative that approaches the problem from the perspective of parallel\ndenoising. We show that full backpropagation throughout the entire generation\nprocess is unnecessary. The downstream metrics can be optimized by retaining\nthe computational graph of only one step during generation, thus providing a\nshortcut for gradient propagation. The resulting method, which we call Shortcut\nDiffusion Optimization (SDO), is generic, high-performance, and computationally\nlightweight, capable of optimizing all parameter types in diffusion sampling.\nWe demonstrate the effectiveness of SDO on several real-world tasks, including\ncontrolling generation by optimizing latent and aligning the DMs by fine-tuning\nnetwork parameters. Compared to full backpropagation, our approach reduces\ncomputational costs by $\\sim 90\\%$ while maintaining superior performance. Code\nis available at https://github.com/deng-ai-lab/SDO."}
{"id": "2505.07548", "pdf": "https://arxiv.org/pdf/2505.07548", "abs": "https://arxiv.org/abs/2505.07548", "authors": ["Lingkun Luo", "Shiqiang Hu", "Liming Chen"], "title": "Noise Optimized Conditional Diffusion for Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "9 pages, 4 figures This work has been accepted by the International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet\nthe scarcity of High-Confidence Pseudo-Labeled Target Domain Samples\n(\\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical\nalignment, causing DA failures. To address this challenge, we propose\n\\textbf{N}oise \\textbf{O}ptimized \\textbf{C}onditional \\textbf{D}iffusion for\n\\textbf{D}omain \\textbf{A}daptation (\\textbf{NOCDDA}), which seamlessly\nintegrates the generative capabilities of conditional diffusion models with the\ndecision-making requirements of DA to achieve task-coupled optimization for\nefficient adaptation. For robust cross-domain consistency, we modify the DA\nclassifier to align with the conditional diffusion classifier within a unified\noptimization framework, enabling forward training on noise-varying cross-domain\nsamples. Furthermore, we argue that the conventional \\( \\mathcal{N}(\\mathbf{0},\n\\mathbf{I}) \\) initialization in diffusion models often generates\nclass-confused hcpl-tds, compromising discriminative DA. To resolve this, we\nintroduce a class-aware noise optimization strategy that refines sampling\nregions for reverse class-specific hcpl-tds generation, effectively enhancing\ncross-domain alignment. Extensive experiments across 5 benchmark datasets and\n29 DA tasks demonstrate significant performance gains of \\textbf{NOCDDA} over\n31 state-of-the-art methods, validating its robustness and effectiveness."}
{"id": "2505.07600", "pdf": "https://arxiv.org/pdf/2505.07600", "abs": "https://arxiv.org/abs/2505.07600", "authors": ["Oriol Barbany", "Adrià Colomé", "Carme Torras"], "title": "Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at ICRA 2025 Workshop \"Reflections on Representations and\n  Manipulating Deformable Objects\". Project page\n  https://barbany.github.io/bifold/", "summary": "Manipulating clothes is challenging due to their complex dynamics, high\ndeformability, and frequent self-occlusions. Garments exhibit a nearly infinite\nnumber of configurations, making explicit state representations difficult to\ndefine. In this paper, we analyze BiFold, a model that predicts\nlanguage-conditioned pick-and-place actions from visual observations, while\nimplicitly encoding garment state through end-to-end learning. To address\nscenarios such as crumpled garments or recovery from failed manipulations,\nBiFold leverages temporal context to improve state estimation. We examine the\ninternal representations of the model and present evidence that its fine-tuning\nand temporal context enable effective alignment between text and image regions,\nas well as temporal consistency."}
{"id": "2505.07634", "pdf": "https://arxiv.org/pdf/2505.07634", "abs": "https://arxiv.org/abs/2505.07634", "authors": ["Jian Liu", "Xiongtao Shi", "Thai Duy Nguyen", "Haitian Zhang", "Tianxiang Zhang", "Wei Sun", "Yanjie Li", "Athanasios V. Vasilakos", "Giovanni Iacca", "Arshad Ali Khan", "Arvind Kumar", "Jae Won Cho", "Ajmal Mian", "Lihua Xie", "Erik Cambria", "Lin Wang"], "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "51 pages, 17 figures, 9 tables", "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios."}
{"id": "2505.07654", "pdf": "https://arxiv.org/pdf/2505.07654", "abs": "https://arxiv.org/abs/2505.07654", "authors": ["Pouya Afshin", "David Helminiak", "Tongtong Lu", "Tina Yen", "Julie M. Jorns", "Mollie Patton", "Bing Yu", "Dong Hye Ye"], "title": "Breast Cancer Classification in Deep Ultraviolet Fluorescence Images Using a Patch-Level Vision Transformer Framework", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Breast-conserving surgery (BCS) aims to completely remove malignant lesions\nwhile maximizing healthy tissue preservation. Intraoperative margin assessment\nis essential to achieve a balance between thorough cancer resection and tissue\nconservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM)\nenables rapid acquisition of whole surface images (WSIs) for excised tissue,\nproviding contrast between malignant and normal tissues. However, breast cancer\nclassification with DUV WSIs is challenged by high resolutions and complex\nhistopathological features. This study introduces a DUV WSI classification\nframework using a patch-level vision transformer (ViT) model, capturing local\nand global features. Grad-CAM++ saliency weighting highlights relevant spatial\nregions, enhances result interpretability, and improves diagnostic accuracy for\nbenign and malignant tissue classification. A comprehensive 5-fold\ncross-validation demonstrates the proposed approach significantly outperforms\nconventional deep learning methods, achieving a classification accuracy of\n98.33%."}
{"id": "2505.07661", "pdf": "https://arxiv.org/pdf/2505.07661", "abs": "https://arxiv.org/abs/2505.07661", "authors": ["Elad Yoshai", "Dana Yagoda-Aharoni", "Eden Dotan", "Natan T. Shaked"], "title": "Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present SparseAttnNet, a new hierarchical attention-driven framework for\nefficient image classification that adaptively selects and processes only the\nmost informative pixels from images. Traditional convolutional neural networks\ntypically process the entire images regardless of information density, leading\nto computational inefficiency and potential focus on irrelevant features. Our\napproach leverages a dynamic selection mechanism that uses coarse attention\ndistilled by fine multi-head attention from the downstream layers of the model,\nallowing the model to identify and extract the most salient k pixels, where k\nis adaptively learned during training based on loss convergence trends. Once\nthe top-k pixels are selected, the model processes only these pixels, embedding\nthem as words in a language model to capture their semantics, followed by\nmulti-head attention to incorporate global context. For biological cell images,\nwe demonstrate that SparseAttnNet can process approximately 15% of the pixels\ninstead of the full image. Applied to cell classification tasks using white\nblood cells images from the following modalities: optical path difference (OPD)\nimages from digital holography for stain-free cells, images from\nmotion-sensitive (event) camera from stain-free cells, and brightfield\nmicroscopy images of stained cells, For all three imaging modalities,\nSparseAttnNet achieves competitive accuracy while drastically reducing\ncomputational requirements in terms of both parameters and floating-point\noperations per second, compared to traditional CNNs and Vision Transformers.\nSince the model focuses on biologically relevant regions, it also offers\nimproved explainability. The adaptive and lightweight nature of SparseAttnNet\nmakes it ideal for deployment in resource-constrained and high-throughput\nsettings, including imaging flow cytometry."}
{"id": "2505.07675", "pdf": "https://arxiv.org/pdf/2505.07675", "abs": "https://arxiv.org/abs/2505.07675", "authors": ["Seongjae Kang", "Dong Bok Lee", "Hyungjoon Jang", "Sung Ju Hwang"], "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "41 pages, 19 figures, preprint", "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\n$\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead\n$\\mathbf{\\texttt{O}}$ptimization ($\\mathbf{\\texttt{DHO}}$) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that $\\texttt{DHO}$ mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that $\\texttt{DHO}$\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters."}
{"id": "2505.07687", "pdf": "https://arxiv.org/pdf/2505.07687", "abs": "https://arxiv.org/abs/2505.07687", "authors": ["Feng Yuan", "Yifan Gao", "Wenbin Wu", "Keqing Wu", "Xiaotong Guo", "Jie Jiang", "Xin Gao"], "title": "ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025(under view)", "summary": "Accurate multi-modal medical image translation requires ha-rmonizing global\nanatomical semantics and local structural fidelity, a challenge complicated by\nintermodality information loss and structural distortion. We propose ABS-Mamba,\na novel architecture integrating the Segment Anything Model 2 (SAM2) for\norgan-aware semantic representation, specialized convolutional neural networks\n(CNNs) for preserving modality-specific edge and texture details, and Mamba's\nselective state-space modeling for efficient long- and short-range feature\ndependencies. Structurally, our dual-resolution framework leverages SAM2's\nimage encoder to capture organ-scale semantics from high-resolution inputs,\nwhile a parallel CNNs branch extracts fine-grained local features. The Robust\nFeature Fusion Network (RFFN) integrates these epresentations, and the\nBidirectional Mamba Residual Network (BMRN) models spatial dependencies using\nspiral scanning and bidirectional state-space dynamics. A three-stage skip\nfusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank\nAdaptation (LoRA+) fine-tuning to enable precise domain specialization while\nmaintaining the foundational capabilities of the pre-trained components.\nExtensive experimental validation on the SynthRAD2023 and BraTS2019 datasets\ndemonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering\nhigh-fidelity cross-modal synthesis that preserves anatomical semantics and\nstructural details to enhance diagnostic accuracy in clinical applications. The\ncode is available at https://github.com/gatina-yone/ABS-Mamba"}
{"id": "2505.07754", "pdf": "https://arxiv.org/pdf/2505.07754", "abs": "https://arxiv.org/abs/2505.07754", "authors": ["Samik Banerjee", "Caleb Stam", "Daniel J. Tward", "Steven Savoia", "Yusu Wang", "Partha P. Mitra"], "title": "Skeletonization of neuronal processes using Discrete Morse techniques from computational topology", "categories": ["q-bio.NC", "cs.CV"], "comment": "Under Review in Nature", "summary": "To understand biological intelligence we need to map neuronal networks in\nvertebrate brains. Mapping mesoscale neural circuitry is done using injections\nof tracers that label groups of neurons whose axons project to different brain\nregions. Since many neurons are labeled, it is difficult to follow individual\naxons. Previous approaches have instead quantified the regional projections\nusing the total label intensity within a region. However, such a quantification\nis not biologically meaningful. We propose a new approach better connected to\nthe underlying neurons by skeletonizing labeled axon fragments and then\nestimating a volumetric length density. Our approach uses a combination of deep\nnets and the Discrete Morse (DM) technique from computational topology. This\ntechnique takes into account nonlocal connectivity information and therefore\nprovides noise-robustness. We demonstrate the utility and scalability of the\napproach on whole-brain tracer injected data. We also define and illustrate an\ninformation theoretic measure that quantifies the additional information\nobtained, compared to the skeletonized tracer injection fragments, when\nindividual axon morphologies are available. Our approach is the first\napplication of the DM technique to computational neuroanatomy. It can help\nbridge between single-axon skeletons and tracer injections, two important data\ntypes in mapping neural networks in vertebrates."}
{"id": "2505.07766", "pdf": "https://arxiv.org/pdf/2505.07766", "abs": "https://arxiv.org/abs/2505.07766", "authors": ["Xuying Huang", "Sicong Pan", "Maren Bennewitz"], "title": "Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "User privacy is a crucial concern in robotic applications, especially when\nmobile service robots are deployed in personal or sensitive environments.\nHowever, many robotic downstream tasks require the use of cameras, which may\nraise privacy risks. To better understand user perceptions of privacy in\nrelation to visual data, we conducted a user study investigating how different\nimage modalities and image resolutions affect users' privacy concerns. The\nresults show that depth images are broadly viewed as privacy-safe, and a\nsimilarly high proportion of respondents feel the same about semantic\nsegmentation images. Additionally, the majority of participants consider 32*32\nresolution RGB images to be almost sufficiently privacy-preserving, while most\nbelieve that 16*16 resolution can fully guarantee privacy protection."}
{"id": "2505.07813", "pdf": "https://arxiv.org/pdf/2505.07813", "abs": "https://arxiv.org/abs/2505.07813", "authors": ["Tony Tao", "Mohan Kumar Srirama", "Jason Jingzhou Liu", "Kenneth Shaw", "Deepak Pathak"], "title": "DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "In RSS 2025. Website at https://dexwild.github.io", "summary": "Large-scale, diverse robot datasets have emerged as a promising path toward\nenabling dexterous manipulation policies to generalize to novel environments,\nbut acquiring such datasets presents many challenges. While teleoperation\nprovides high-fidelity datasets, its high cost limits its scalability. Instead,\nwhat if people could use their own hands, just as they do in everyday life, to\ncollect data? In DexWild, a diverse team of data collectors uses their hands to\ncollect hours of interactions across a multitude of environments and objects.\nTo record this data, we create DexWild-System, a low-cost, mobile, and\neasy-to-use device. The DexWild learning framework co-trains on both human and\nrobot demonstrations, leading to improved performance compared to training on\neach dataset individually. This combination results in robust robot policies\ncapable of generalizing to novel environments, tasks, and embodiments with\nminimal additional robot-specific data. Experimental results demonstrate that\nDexWild significantly improves performance, achieving a 68.5% success rate in\nunseen environments-nearly four times higher than policies trained with robot\ndata only-and offering 5.8x better cross-embodiment generalization. Video\nresults, codebases, and instructions at https://dexwild.github.io"}
{"id": "2505.07815", "pdf": "https://arxiv.org/pdf/2505.07815", "abs": "https://arxiv.org/abs/2505.07815", "authors": ["Seungjae Lee", "Daniel Ekpo", "Haowen Liu", "Furong Huang", "Abhinav Shrivastava", "Jia-Bin Huang"], "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project webpage: https://ive-robot.github.io/", "summary": "Exploration is essential for general-purpose robotic learning, especially in\nopen-ended environments where dense rewards, explicit goals, or task-specific\nsupervision are scarce. Vision-language models (VLMs), with their semantic\nreasoning over objects, spatial relations, and potential outcomes, present a\ncompelling foundation for generating high-level exploratory behaviors. However,\ntheir outputs are often ungrounded, making it difficult to determine whether\nimagined transitions are physically feasible or informative. To bridge the gap\nbetween imagination and execution, we present IVE (Imagine, Verify, Execute),\nan agentic exploration framework inspired by human curiosity. Human exploration\nis often driven by the desire to discover novel scene configurations and to\ndeepen understanding of the environment. Similarly, IVE leverages VLMs to\nabstract RGB-D observations into semantic scene graphs, imagine novel scenes,\npredict their physical plausibility, and generate executable skill sequences\nthrough action tools. We evaluate IVE in both simulated and real-world tabletop\nenvironments. The results show that IVE enables more diverse and meaningful\nexploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the\nentropy of visited states. Moreover, the collected experience supports\ndownstream learning, producing policies that closely match or exceed the\nperformance of those trained on human-collected demonstrations."}
{"id": "2505.07817", "pdf": "https://arxiv.org/pdf/2505.07817", "abs": "https://arxiv.org/abs/2505.07817", "authors": ["Kanchana Ranasinghe", "Xiang Li", "Cristina Mata", "Jongwoo Park", "Michael S Ryoo"], "title": "Pixel Motion as Universal Representation for Robot Control", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present LangToMo, a vision-language-action framework structured as a\ndual-system architecture that uses pixel motion forecasts as intermediate\nrepresentations. Our high-level System 2, an image diffusion model, generates\ntext-conditioned pixel motion sequences from a single frame to guide robot\ncontrol. Pixel motion-a universal, interpretable, and motion-centric\nrepresentation-can be extracted from videos in a self-supervised manner,\nenabling diffusion model training on web-scale video-caption data. Treating\ngenerated pixel motion as learned universal representations, our low level\nSystem 1 module translates these into robot actions via motion-to-action\nmapping functions, which can be either hand-crafted or learned with minimal\nsupervision. System 2 operates as a high-level policy applied at sparse\ntemporal intervals, while System 1 acts as a low-level policy at dense temporal\nintervals. This hierarchical decoupling enables flexible, scalable, and\ngeneralizable robot control under both unsupervised and supervised settings,\nbridging the gap between language, motion, and action. Checkout\nhttps://kahnchana.github.io/LangToMo for visualizations."}
{"id": "2505.07819", "pdf": "https://arxiv.org/pdf/2505.07819", "abs": "https://arxiv.org/abs/2505.07819", "authors": ["Yiyang Lu", "Yufeng Tian", "Zhecheng Yuan", "Xianbang Wang", "Pu Hua", "Zhengrong Xue", "Huazhe Xu"], "title": "H$^{\\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Visuomotor policy learning has witnessed substantial progress in robotic\nmanipulation, with recent approaches predominantly relying on generative models\nto model the action distribution. However, these methods often overlook the\ncritical coupling between visual perception and action prediction. In this\nwork, we introduce $\\textbf{Triply-Hierarchical Diffusion\nPolicy}~(\\textbf{H$^{\\mathbf{3}}$DP})$, a novel visuomotor learning framework\nthat explicitly incorporates hierarchical structures to strengthen the\nintegration between visual features and action generation. H$^{3}$DP contains\n$\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes\nRGB-D observations based on depth information; (2) multi-scale visual\nrepresentations that encode semantic features at varying levels of granularity;\nand (3) a hierarchically conditioned diffusion process that aligns the\ngeneration of coarse-to-fine actions with corresponding visual features.\nExtensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5\\%}$\naverage relative improvement over baselines across $\\mathbf{44}$ simulation\ntasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual\nreal-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/."}
