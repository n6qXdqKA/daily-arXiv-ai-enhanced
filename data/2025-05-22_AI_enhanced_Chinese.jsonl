{"id": "2505.14699", "pdf": "https://arxiv.org/pdf/2505.14699", "abs": "https://arxiv.org/abs/2505.14699", "authors": ["Miguel Lopez-Duran", "Julian Fierrez", "Aythami Morales", "Ruben Tolosana", "Oscar Delgado-Mohatar", "Alvaro Ortigosa"], "title": "Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "15 pages, 2 figures, preprint presented in The Fifth ICDAR\n  International Workshop on Machine Learning", "summary": "The automatic analysis of document layouts in digital-born PDF documents\nremains a challenging problem due to the heterogeneous arrangement of textual\nand nontextual elements and the imprecision of the textual metadata in the\nPortable Document Format. In this work, we benchmark Graph Neural Network (GNN)\narchitectures for the task of fine-grained layout classification of text blocks\nfrom digital native documents. We introduce two graph construction structures:\na k-closest-neighbor graph and a fully connected graph, and generate node\nfeatures via pre-trained text and vision models, thus avoiding manual feature\nengineering. Three experimental frameworks are evaluated: single-modality (text\nor visual), concatenated multimodal, and dual-branch multimodal. We evaluated\nfour foundational GNN models and compared them with the baseline. Our\nexperiments are specifically conducted on a rich dataset of public affairs\ndocuments that includes more than 20 sources (e.g., regional and national-level\nofficial gazettes), 37K PDF documents, with 441K pages in total. Our results\ndemonstrate that GraphSAGE operating on the k-closest-neighbor graph in a\ndual-branch configuration achieves the highest per-class and overall accuracy,\noutperforming the baseline in some sources. These findings confirm the\nimportance of local layout relationships and multimodal fusion exploited\nthrough GNNs for the analysis of native digital document layouts.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5bf9\u6570\u5b57\u539f\u751fPDF\u6587\u6863\u7684\u5e03\u5c40\u8fdb\u884c\u5206\u7c7b\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u63d0\u5347\u4e86\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u7531\u4e8ePDF\u6587\u6863\u4e2d\u6587\u672c\u548c\u975e\u6587\u672c\u5143\u7d20\u7684\u5f02\u6784\u5e03\u5c40\u53ca\u6587\u672c\u5143\u6570\u636e\u7684\u4e0d\u7cbe\u786e\u6027\uff0c\u81ea\u52a8\u5206\u6790\u6587\u6863\u5e03\u5c40\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165k\u6700\u8fd1\u90bb\u56fe\u548c\u5168\u8fde\u63a5\u56fe\u4e24\u79cd\u56fe\u7ed3\u6784\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u548c\u89c6\u89c9\u6a21\u578b\u751f\u6210\u8282\u70b9\u7279\u5f81\uff0c\u907f\u514d\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\u3002\u8bc4\u4f30\u4e86\u4e09\u79cd\u5b9e\u9a8c\u6846\u67b6\uff08\u5355\u6a21\u6001\u3001\u591a\u6a21\u6001\u62fc\u63a5\u3001\u53cc\u5206\u652f\u591a\u6a21\u6001\uff09\u548c\u56db\u79cdGNN\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGraphSAGE\u6a21\u578b\u5728k\u6700\u8fd1\u90bb\u56fe\u548c\u53cc\u5206\u652f\u914d\u7f6e\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u5c40\u90e8\u5e03\u5c40\u5173\u7cfb\u548c\u591a\u6a21\u6001\u878d\u5408\u5728GNN\u5206\u6790\u6570\u5b57\u6587\u6863\u5e03\u5c40\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.14705", "pdf": "https://arxiv.org/pdf/2505.14705", "abs": "https://arxiv.org/abs/2505.14705", "authors": ["Xin Zhang", "Ziruo Zhang", "Jiawei Du", "Zuozhu Liu", "Joey Tianyi Zhou"], "title": "Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Dataset Distillation (MDD) seeks to condense large-scale\nimage-text datasets into compact surrogates while retaining their effectiveness\nfor cross-modal learning. Despite recent progress, existing MDD approaches\noften suffer from \\textit{\\textbf{Modality Collapse}}, characterized by\nover-concentrated intra-modal representations and enlarged distributional gap\nacross modalities. In this paper, at the first time, we identify this issue as\nstemming from a fundamental conflict between the over-compression behavior\ninherent in dataset distillation and the cross-modal supervision imposed by\ncontrastive objectives. To alleviate modality collapse, we introduce\n\\textbf{RepBlend}, a novel MDD framework that weakens overdominant cross-modal\nsupervision via representation blending, thereby significantly enhancing\nintra-modal diversity. Additionally, we observe that current MDD methods impose\nasymmetric supervision across modalities, resulting in biased optimization. To\naddress this, we propose symmetric projection trajectory matching, which\nsynchronizes the optimization dynamics using modality-specific projection\nheads, thereby promoting balanced supervision and enhancing cross-modal\nalignment. Experiments on Flickr-30K and MS-COCO show that RepBlend\nconsistently outperforms prior state-of-the-art MDD methods, achieving\nsignificant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10 under\nthe 100-pair setting) and offering up to 6.7$\\times$ distillation speedup.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRepBlend\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u6df7\u5408\u548c\u5bf9\u79f0\u6295\u5f71\u8f68\u8ff9\u5339\u914d\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u7684\u6a21\u6001\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u5b58\u5728\u6a21\u6001\u5d29\u6e83\u95ee\u9898\uff0c\u8868\u73b0\u4e3a\u6a21\u6001\u5185\u8868\u793a\u8fc7\u5ea6\u96c6\u4e2d\u548c\u6a21\u6001\u95f4\u5206\u5e03\u5dee\u8ddd\u6269\u5927\u3002", "method": "\u5f15\u5165RepBlend\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u6df7\u5408\u51cf\u5f31\u8de8\u6a21\u6001\u76d1\u7763\u7684\u8fc7\u5ea6\u4e3b\u5bfc\uff0c\u5e76\u63d0\u51fa\u5bf9\u79f0\u6295\u5f71\u8f68\u8ff9\u5339\u914d\u4ee5\u5e73\u8861\u4f18\u5316\u3002", "result": "\u5728Flickr-30K\u548cMS-COCO\u4e0a\uff0cRepBlend\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u68c0\u7d22\u6027\u80fd\u63d0\u5347\uff08\u5982+9.4 IR@10\uff09\uff0c\u84b8\u998f\u901f\u5ea6\u63d0\u53476.7\u500d\u3002", "conclusion": "RepBlend\u6709\u6548\u7f13\u89e3\u6a21\u6001\u5d29\u6e83\uff0c\u63d0\u5347\u591a\u6a21\u6001\u6570\u636e\u96c6\u84b8\u998f\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.14707", "pdf": "https://arxiv.org/pdf/2505.14707", "abs": "https://arxiv.org/abs/2505.14707", "authors": ["Georgiana Manolache", "Gerard Schouten", "Joaquin Vanschoren"], "title": "CrypticBio: A Large Multimodal Dataset for Visually Confusing Biodiversity", "categories": ["cs.CV", "cs.AI"], "comment": "We present CrypticBio, the largest publicly available multimodal\n  dataset of visually confusing species, specifically curated to support the\n  development of AI models for biodiversity identification using images,\n  language and spatiotemporal data", "summary": "We present CrypticBio, the largest publicly available multimodal dataset of\nvisually confusing species, specifically curated to support the development of\nAI models in the context of biodiversity applications. Visually confusing or\ncryptic species are groups of two or more taxa that are nearly\nindistinguishable based on visual characteristics alone. While much existing\nwork addresses taxonomic identification in a broad sense, datasets that\ndirectly address the morphological confusion of cryptic species are small,\nmanually curated, and target only a single taxon. Thus, the challenge of\nidentifying such subtle differences in a wide range of taxa remains\nunaddressed. Curated from real-world trends in species misidentification among\ncommunity annotators of iNaturalist, CrypticBio contains 52K unique cryptic\ngroups spanning 67K species, represented in 166 million images. Rich\nresearch-grade image annotations--including scientific, multicultural, and\nmultilingual species terminology, hierarchical taxonomy, spatiotemporal\ncontext, and associated cryptic groups--address multimodal AI in biodiversity\nresearch. For easy dataset curation, we provide an open-source pipeline\nCrypticBio-Curate. The multimodal nature of the dataset beyond vision-language\narises from the integration of geographical and temporal data as complementary\ncues to identifying cryptic species. To highlight the importance of the\ndataset, we benchmark a suite of state-of-the-art foundation models across\nCrypticBio subsets of common, unseen, endangered, and invasive species, and\ndemonstrate the substantial impact of geographical context on vision-language\nzero-shot learning for cryptic species. By introducing CrypticBio, we aim to\ncatalyze progress toward real-world-ready biodiversity AI models capable of\nhandling the nuanced challenges of species ambiguity.", "AI": {"tldr": "CrypticBio\u662f\u4e00\u4e2a\u516c\u5f00\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u89c6\u89c9\u4e0a\u96be\u4ee5\u533a\u5206\u7684\u7269\u79cd\uff0c\u652f\u6301\u751f\u7269\u591a\u6837\u6027AI\u6a21\u578b\u7684\u5f00\u53d1\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u5355\u4e00\u5206\u7c7b\u7fa4\u4e14\u89c4\u6a21\u5c0f\uff0c\u65e0\u6cd5\u89e3\u51b3\u5e7f\u6cdb\u5206\u7c7b\u7fa4\u4e2d\u7ec6\u5fae\u5dee\u5f02\u7684\u8bc6\u522b\u95ee\u9898\u3002", "method": "\u4eceiNaturalist\u793e\u533a\u6ce8\u91ca\u8005\u7684\u8bef\u8bc6\u522b\u8d8b\u52bf\u4e2d\u63d0\u53d6\u6570\u636e\uff0c\u5305\u542b52K\u4e2a\u72ec\u7279\u52a0\u5bc6\u7ec4\uff0c\u8986\u76d667K\u7269\u79cd\u548c1.66\u4ebf\u56fe\u50cf\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u591a\u6a21\u6001AI\u7814\u7a76\uff0c\u5730\u7406\u548c\u65f6\u95f4\u6570\u636e\u589e\u5f3a\u4e86\u8bc6\u522b\u80fd\u529b\uff0c\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5730\u7406\u4e0a\u4e0b\u6587\u5bf9\u96f6\u6837\u672c\u5b66\u4e60\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "CrypticBio\u65e8\u5728\u63a8\u52a8\u751f\u7269\u591a\u6837\u6027AI\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u89e3\u51b3\u7269\u79cd\u6a21\u7cca\u6027\u7684\u6311\u6218\u3002"}}
{"id": "2505.14708", "pdf": "https://arxiv.org/pdf/2505.14708", "abs": "https://arxiv.org/abs/2505.14708", "authors": ["Xuan Shen", "Chenxia Han", "Yufa Zhou", "Yanyue Xie", "Yifan Gong", "Quanyi Wang", "Yiwei Wang", "Yanzhi Wang", "Pu Zhao", "Jiuxiang Gu"], "title": "DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint Version", "summary": "Diffusion transformer-based video generation models (DiTs) have recently\nattracted widespread attention for their excellent generation quality. However,\ntheir computational cost remains a major bottleneck-attention alone accounts\nfor over 80% of total latency, and generating just 8 seconds of 720p video\ntakes tens of minutes-posing serious challenges to practical application and\nscalability. To address this, we propose the DraftAttention, a training-free\nframework for the acceleration of video diffusion transformers with dynamic\nsparse attention on GPUs. We apply down-sampling to each feature map across\nframes in the compressed latent space, enabling a higher-level receptive field\nover the latent composed of hundreds of thousands of tokens. The low-resolution\ndraft attention map, derived from draft query and key, exposes redundancy both\nspatially within each feature map and temporally across frames. We reorder the\nquery, key, and value based on the draft attention map to guide the sparse\nattention computation in full resolution, and subsequently restore their\noriginal order after the attention computation. This reordering enables\nstructured sparsity that aligns with hardware-optimized execution. Our\ntheoretical analysis demonstrates that the low-resolution draft attention\nclosely approximates the full attention, providing reliable guidance for\nconstructing accurate sparse attention. Experimental results show that our\nmethod outperforms existing sparse attention approaches in video generation\nquality and achieves up to 1.75x end-to-end speedup on GPUs. Code:\nhttps://github.com/shawnricecake/draft-attention", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDraftAttention\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901f\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u5728GPU\u4e0a\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08DiTs\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5360\u7528\u4e8680%\u4ee5\u4e0a\u7684\u5ef6\u8fdf\uff0c\u751f\u62108\u79d2720p\u89c6\u9891\u9700\u6570\u5341\u5206\u949f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u548c\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u4e0b\u91c7\u6837\u6280\u672f\u5bf9\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7279\u5f81\u56fe\u8fdb\u884c\u5904\u7406\uff0c\u751f\u6210\u4f4e\u5206\u8fa8\u7387\u8349\u7a3f\u6ce8\u610f\u529b\u56fe\uff0c\u63ed\u793a\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\uff0c\u5e76\u901a\u8fc7\u91cd\u6392\u5e8f\u5b9e\u73b0\u7ed3\u6784\u5316\u7a00\u758f\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u751f\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u5e76\u5728GPU\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad81.75\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "DraftAttention\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.14709", "pdf": "https://arxiv.org/pdf/2505.14709", "abs": "https://arxiv.org/abs/2505.14709", "authors": ["Xuan Shen", "Weize Ma", "Yufa Zhou", "Enhao Tang", "Yanyue Xie", "Zhengang Li", "Yifan Gong", "Quanyi Wang", "Henghui Ding", "Yiwei Wang", "Yanzhi Wang", "Pu Zhao", "Jun Lin", "Jiuxiang Gu"], "title": "FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint Version", "summary": "Auto-regressive (AR) models, initially successful in language generation,\nhave recently shown promise in visual generation tasks due to their superior\nsampling efficiency. Unlike image generation, video generation requires a\nsubstantially larger number of tokens to produce coherent temporal frames,\nresulting in significant overhead during the decoding phase. Our key\nobservations are: (i) MLP modules in the decode phase dominate the inference\nlatency, and (ii) there exists high temporal redundancy in MLP outputs of\nadjacent frames. In this paper, we propose the \\textbf{FastCar} framework to\naccelerate the decode phase for the AR video generation by exploring the\ntemporal redundancy. The Temporal Attention Score (TAS) is proposed to\ndetermine whether to apply the replay strategy (\\textit{i.e.}, reusing cached\nMLP outputs from the previous frame to reduce redundant computations) with\ndetailed theoretical analysis and justification. Also, we develop a hardware\naccelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to\nenable better resource utilization and faster inference. Experimental results\ndemonstrate the effectiveness of our method, which outperforms traditional\nsparse attention approaches with more than 2.1x decoding speedup and higher\nenergy efficiency on the edge. Furthermore, by combining FastCar and sparse\nattention, FastCar can boost the performance of sparse attention with\nalleviated drifting, demonstrating our unique advantages for high-resolution\nand long-duration video generation. Code:\nhttps://github.com/shawnricecake/fast-car", "AI": {"tldr": "FastCar\u6846\u67b6\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5197\u4f59\u52a0\u901f\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u7684\u89e3\u7801\u9636\u6bb5\uff0c\u63d0\u51faTAS\u548c\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u663e\u8457\u63d0\u5347\u89e3\u7801\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u9700\u8981\u5927\u91cf\u4ee4\u724c\uff0c\u5bfc\u81f4\u89e3\u7801\u9636\u6bb5\u5f00\u9500\u5927\uff0cMLP\u6a21\u5757\u662f\u5ef6\u8fdf\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u4e14\u76f8\u90bb\u5e27\u7684MLP\u8f93\u51fa\u5b58\u5728\u9ad8\u65f6\u95f4\u5197\u4f59\u3002", "method": "\u63d0\u51faFastCar\u6846\u67b6\uff0c\u5229\u7528TAS\u786e\u5b9a\u662f\u5426\u91cd\u7528\u7f13\u5b58\u7684MLP\u8f93\u51fa\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8eFPGA\u7684\u786c\u4ef6\u52a0\u901f\u5668\u3002", "result": "FastCar\u6bd4\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u5feb2.1\u500d\uff0c\u80fd\u6548\u66f4\u9ad8\uff0c\u4e14\u80fd\u63d0\u5347\u7a00\u758f\u6ce8\u610f\u529b\u7684\u6027\u80fd\u3002", "conclusion": "FastCar\u5728\u9ad8\u5206\u8fa8\u7387\u548c\u957f\u89c6\u9891\u751f\u6210\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u7ed3\u5408\u7a00\u758f\u6ce8\u610f\u529b\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2505.14714", "pdf": "https://arxiv.org/pdf/2505.14714", "abs": "https://arxiv.org/abs/2505.14714", "authors": ["Tuan-Vinh La", "Minh-Hieu Nguyen", "Minh-Son Dao"], "title": "KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fake news detection remains a challenging problem due to the complex\ninterplay between textual misinformation, manipulated images, and external\nknowledge reasoning. While existing approaches have achieved notable results in\nverifying veracity and cross-modal consistency, two key challenges persist: (1)\nExisting methods often consider only the global image context while neglecting\nlocal object-level details, and (2) they fail to incorporate external knowledge\nand entity relationships for deeper semantic understanding. To address these\nchallenges, we propose a novel multi-modal fake news detection framework that\nintegrates visual, textual, and knowledge-based representations. Our approach\nleverages bottom-up attention to capture fine-grained object details, CLIP for\nglobal image semantics, and RoBERTa for context-aware text encoding. We further\nenhance knowledge utilization by retrieving and adaptively selecting relevant\nentities from a knowledge graph. The fused multi-modal features are processed\nthrough a Transformer-based classifier to predict news veracity. Experimental\nresults demonstrate that our model outperforms recent approaches, showcasing\nthe effectiveness of neighbor selection mechanism and multi-modal fusion for\nfake news detection. Our proposal introduces a new paradigm: knowledge-grounded\nmultimodal reasoning. By integrating explicit entity-level selection and\nNLI-guided filtering, we shift fake news detection from feature fusion to\nsemantically grounded verification. For reproducibility and further research,\nthe source code is publicly at\n\\href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u3001\u6587\u672c\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u7ec6\u8282\u548c\u5916\u90e8\u77e5\u8bc6\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u5c40\u90e8\u5bf9\u8c61\u7ec6\u8282\u548c\u5916\u90e8\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5047\u65b0\u95fb\u68c0\u6d4b\u6548\u679c\u53d7\u9650\u3002", "method": "\u91c7\u7528\u81ea\u5e95\u5411\u4e0a\u6ce8\u610f\u529b\u6355\u6349\u5bf9\u8c61\u7ec6\u8282\uff0cCLIP\u5904\u7406\u5168\u5c40\u56fe\u50cf\u8bed\u4e49\uff0cRoBERTa\u7f16\u7801\u6587\u672c\uff0c\u5e76\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u5b9e\u4f53\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u90bb\u5c45\u9009\u62e9\u673a\u5236\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5c06\u5047\u65b0\u95fb\u68c0\u6d4b\u4ece\u7279\u5f81\u878d\u5408\u8f6c\u5411\u8bed\u4e49\u9a8c\u8bc1\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.14718", "pdf": "https://arxiv.org/pdf/2505.14718", "abs": "https://arxiv.org/abs/2505.14718", "authors": ["Guoxuan Mao", "Ting Cao", "Ziyang Li", "Yuan Dong"], "title": "Enhancing Shape Perception and Segmentation Consistency for Industrial Image Inspection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation stands as a pivotal research focus in computer vision.\nIn the context of industrial image inspection, conventional semantic\nsegmentation models fail to maintain the segmentation consistency of fixed\ncomponents across varying contextual environments due to a lack of perception\nof object contours. Given the real-time constraints and limited computing\ncapability of industrial image detection machines, it is also necessary to\ncreate efficient models to reduce computational complexity. In this work, a\nShape-Aware Efficient Network (SPENet) is proposed, which focuses on the shapes\nof objects to achieve excellent segmentation consistency by separately\nsupervising the extraction of boundary and body information from images. In\nSPENet, a novel method is introduced for describing fuzzy boundaries to better\nadapt to real-world scenarios named Variable Boundary Domain (VBD).\nAdditionally, a new metric, Consistency Mean Square Error(CMSE), is proposed to\nmeasure segmentation consistency for fixed components. Our approach attains the\nbest segmentation accuracy and competitive speed on our dataset, showcasing\nsignificant advantages in CMSE among numerous state-of-the-art real-time\nsegmentation networks, achieving a reduction of over 50% compared to the\npreviously top-performing models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f62\u72b6\u611f\u77e5\u9ad8\u6548\u7f51\u7edc\uff08SPENet\uff09\uff0c\u901a\u8fc7\u5206\u522b\u76d1\u7763\u8fb9\u754c\u548c\u4e3b\u4f53\u4fe1\u606f\u63d0\u53d6\uff0c\u63d0\u5347\u5de5\u4e1a\u56fe\u50cf\u68c0\u6d4b\u4e2d\u7684\u8bed\u4e49\u5206\u5272\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u5de5\u4e1a\u56fe\u50cf\u68c0\u6d4b\u4e2d\u56e0\u7f3a\u4e4f\u5bf9\u7269\u4f53\u8f6e\u5ed3\u7684\u611f\u77e5\uff0c\u96be\u4ee5\u4fdd\u6301\u56fa\u5b9a\u7ec4\u4ef6\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u5206\u5272\u4e00\u81f4\u6027\uff0c\u4e14\u9700\u6ee1\u8db3\u5b9e\u65f6\u6027\u548c\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "SPENet\u901a\u8fc7\u76d1\u7763\u8fb9\u754c\u548c\u4e3b\u4f53\u4fe1\u606f\u63d0\u53d6\uff0c\u5f15\u5165\u53ef\u53d8\u8fb9\u754c\u57df\uff08VBD\uff09\u63cf\u8ff0\u6a21\u7cca\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u4e00\u81f4\u6027\u5747\u65b9\u8bef\u5dee\uff08CMSE\uff09\u8861\u91cf\u5206\u5272\u4e00\u81f4\u6027\u3002", "result": "SPENet\u5728\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u5206\u5272\u7cbe\u5ea6\u548c\u7ade\u4e89\u6027\u901f\u5ea6\uff0cCMSE\u6307\u6807\u8f83\u4e4b\u524d\u6700\u4f18\u6a21\u578b\u964d\u4f4e50%\u4ee5\u4e0a\u3002", "conclusion": "SPENet\u5728\u5de5\u4e1a\u56fe\u50cf\u68c0\u6d4b\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u4e00\u81f4\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\u3002"}}
{"id": "2505.14719", "pdf": "https://arxiv.org/pdf/2505.14719", "abs": "https://arxiv.org/abs/2505.14719", "authors": ["Wei Hua", "Chenlin Zhou", "Jibin Wu", "Yansong Chua", "Yangyang Shu"], "title": "MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The combination of Spiking Neural Networks(SNNs) with Vision Transformer\narchitectures has attracted significant attention due to the great potential\nfor energy-efficient and high-performance computing paradigms. However, a\nsubstantial performance gap still exists between SNN-based and ANN-based\ntransformer architectures. While existing methods propose spiking\nself-attention mechanisms that are successfully combined with SNNs, the overall\narchitectures proposed by these methods suffer from a bottleneck in effectively\nextracting features from different image scales. In this paper, we address this\nissue and propose MSVIT, a novel spike-driven Transformer architecture, which\nfirstly uses multi-scale spiking attention (MSSA) to enrich the capability of\nspiking attention blocks. We validate our approach across various main data\nsets. The experimental results show that MSVIT outperforms existing SNN-based\nmodels, positioning itself as a state-of-the-art solution among SNN-transformer\narchitectures. The codes are available at\nhttps://github.com/Nanhu-AI-Lab/MSViT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u8109\u51b2\u9a71\u52a8Transformer\u67b6\u6784MSVIT\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8109\u51b2\u6ce8\u610f\u529b\uff08MSSA\uff09\u89e3\u51b3\u4e86\u73b0\u6709SNN-Transformer\u5728\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u4e0a\u7684\u74f6\u9888\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709SNN\u6a21\u578b\u3002", "motivation": "\u73b0\u6709SNN-Transformer\u67b6\u6784\u5728\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0a\u5b58\u5728\u74f6\u9888\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0eANN-Transformer\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u63d0\u51faMSVIT\u67b6\u6784\uff0c\u9996\u6b21\u5f15\u5165\u591a\u5c3a\u5ea6\u8109\u51b2\u6ce8\u610f\u529b\uff08MSSA\uff09\u6a21\u5757\uff0c\u589e\u5f3a\u8109\u51b2\u6ce8\u610f\u529b\u5757\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMSVIT\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709SNN\u6a21\u578b\uff0c\u6210\u4e3aSNN-Transformer\u67b6\u6784\u7684SOTA\u65b9\u6848\u3002", "conclusion": "MSVIT\u901a\u8fc7\u591a\u5c3a\u5ea6\u8109\u51b2\u6ce8\u610f\u529b\u663e\u8457\u63d0\u5347\u4e86SNN-Transformer\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u80fd\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.14728", "pdf": "https://arxiv.org/pdf/2505.14728", "abs": "https://arxiv.org/abs/2505.14728", "authors": ["Xiao Lin", "Zhining Liu", "Ze Yang", "Gaotang Li", "Ruizhong Qiu", "Shuke Wang", "Hui Liu", "Haotian Li", "Sumit Keswani", "Vishwa Pardeshi", "Huijun Zhao", "Wei Fan", "Hanghang Tong"], "title": "MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.MM"], "comment": "21 pages, 11 figures, 7 tables", "summary": "Warning: This paper contains examples of harmful language and images. Reader\ndiscretion is advised. Recently, vision-language models have demonstrated\nincreasing influence in morally sensitive domains such as autonomous driving\nand medical analysis, owing to their powerful multimodal reasoning\ncapabilities. As these models are deployed in high-stakes real-world\napplications, it is of paramount importance to ensure that their outputs align\nwith human moral values and remain within moral boundaries. However, existing\nwork on moral alignment either focuses solely on textual modalities or relies\nheavily on AI-generated images, leading to distributional biases and reduced\nrealism. To overcome these limitations, we introduce MORALISE, a comprehensive\nbenchmark for evaluating the moral alignment of vision-language models (VLMs)\nusing diverse, expert-verified real-world data. We begin by proposing a\ncomprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,\nspanning the personal, interpersonal, and societal moral domains encountered in\neveryday life. Built on this framework, we manually curate 2,481 high-quality\nimage-text pairs, each annotated with two fine-grained labels: (1) topic\nannotation, identifying the violated moral topic(s), and (2) modality\nannotation, indicating whether the violation arises from the image or the text.\nFor evaluation, we encompass two tasks, \\textit{moral judgment} and\n\\textit{moral norm attribution}, to assess models' awareness of moral\nviolations and their reasoning ability on morally salient content. Extensive\nexperiments on 19 popular open- and closed-source VLMs show that MORALISE poses\na significant challenge, revealing persistent moral limitations in current\nstate-of-the-art models. The full benchmark is publicly available at\nhttps://huggingface.co/datasets/Ze1025/MORALISE.", "AI": {"tldr": "MORALISE\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9053\u5fb7\u5bf9\u9f50\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\uff0c\u6db5\u76d613\u4e2a\u9053\u5fb7\u4e3b\u9898\uff0c\u5305\u542b2481\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u9053\u5fb7\u5c40\u9650\u6027\u3002", "motivation": "\u786e\u4fdd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7b26\u5408\u4eba\u7c7b\u9053\u5fb7\u4ef7\u503c\u89c2\uff0c\u514b\u670d\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u6587\u672c\u6216\u4f9d\u8d56AI\u751f\u6210\u56fe\u50cf\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eTuriel\u9886\u57df\u7406\u8bba\u768413\u4e2a\u9053\u5fb7\u4e3b\u9898\u5206\u7c7b\uff0c\u624b\u52a8\u6807\u6ce82481\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u8bbe\u8ba1\u9053\u5fb7\u5224\u65ad\u548c\u9053\u5fb7\u89c4\u8303\u5f52\u56e0\u4e24\u4e2a\u8bc4\u4f30\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMORALISE\u5bf919\u79cd\u4e3b\u6d41\u6a21\u578b\u6784\u6210\u6311\u6218\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u9053\u5fb7\u5bf9\u9f50\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MORALISE\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u5bf9\u9f50\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u9053\u5fb7\u8868\u73b0\u3002"}}
{"id": "2505.14729", "pdf": "https://arxiv.org/pdf/2505.14729", "abs": "https://arxiv.org/abs/2505.14729", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Srishti Yadav", "Alejandro Salamanca", "Desmond Elliott"], "title": "Uncovering Cultural Representation Disparities in Vision-Language Models", "categories": ["cs.CV"], "comment": "26 pages, 36 figures", "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities\nacross a range of tasks, yet concerns about their potential biases exist. This\nwork investigates the extent to which prominent VLMs exhibit cultural biases by\nevaluating their performance on an image-based country identification task at a\ncountry level. Utilizing the geographically diverse Country211 dataset, we\nprobe several large vision language models (VLMs) under various prompting\nstrategies: open-ended questions, multiple-choice questions (MCQs) including\nchallenging setups like multilingual and adversarial settings. Our analysis\naims to uncover disparities in model accuracy across different countries and\nquestion formats, providing insights into how training data distribution and\nevaluation methodologies might influence cultural biases in VLMs. The findings\nhighlight significant variations in performance, suggesting that while VLMs\npossess considerable visual understanding, they inherit biases from their\npre-training data and scale that impact their ability to generalize uniformly\nacross diverse global contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u56fd\u5bb6\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u6587\u5316\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u56e0\u56fd\u5bb6\u548c\u63d0\u95ee\u65b9\u5f0f\u4e0d\u540c\u800c\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6f5c\u5728\u7684\u6587\u5316\u504f\u89c1\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30VLMs\u5728\u56fd\u5bb6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6587\u5316\u504f\u89c1\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528Country211\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u591a\u79cdVLMs\u5728\u4e0d\u540c\u63d0\u95ee\u7b56\u7565\uff08\u5982\u5f00\u653e\u5f0f\u95ee\u9898\u3001\u591a\u9009\u9898\u3001\u591a\u8bed\u8a00\u548c\u5bf9\u6297\u6027\u8bbe\u7f6e\uff09\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u5728\u4e0d\u540c\u56fd\u5bb6\u548c\u63d0\u95ee\u65b9\u5f0f\u4e0b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660eVLMs\u7ee7\u627f\u4e86\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u89c1\u3002", "conclusion": "VLMs\u867d\u5177\u5907\u5f3a\u5927\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u9884\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u548c\u89c4\u6a21\u5f71\u54cd\uff0c\u96be\u4ee5\u5728\u5168\u7403\u8303\u56f4\u5185\u5747\u5300\u6cdb\u5316\u3002"}}
{"id": "2505.14843", "pdf": "https://arxiv.org/pdf/2505.14843", "abs": "https://arxiv.org/abs/2505.14843", "authors": ["Yunha Yeo", "Daeho Um"], "title": "Leveraging Generative AI Models to Explore Human Identity", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ISEA 2025", "summary": "This paper attempts to explore human identity by utilizing neural networks in\nan indirect manner. For this exploration, we adopt diffusion models,\nstate-of-the-art AI generative models trained to create human face images. By\nrelating the generated human face to human identity, we establish a\ncorrespondence between the face image generation process of the diffusion model\nand the process of human identity formation. Through experiments with the\ndiffusion model, we observe that changes in its external input result in\nsignificant changes in the generated face image. Based on the correspondence,\nwe indirectly confirm the dependence of human identity on external factors in\nthe process of human identity formation. Furthermore, we introduce\n\\textit{Fluidity of Human Identity}, a video artwork that expresses the fluid\nnature of human identity affected by varying external factors. The video is\navailable at\nhttps://www.behance.net/gallery/219958453/Fluidity-of-Human-Identity?.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u4eba\u8138\u56fe\u50cf\uff0c\u63a2\u7d22\u4eba\u7c7b\u8eab\u4efd\u4e0e\u5916\u90e8\u56e0\u7d20\u7684\u5173\u7cfb\uff0c\u5e76\u521b\u4f5c\u4e86\u8868\u8fbe\u8eab\u4efd\u6d41\u52a8\u6027\u7684\u89c6\u9891\u4f5c\u54c1\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u8eab\u4efd\u7684\u5f62\u6210\u8fc7\u7a0b\u53ca\u5176\u5bf9\u5916\u90e8\u56e0\u7d20\u7684\u4f9d\u8d56\u6027\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u4eba\u8138\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u89c2\u5bdf\u5916\u90e8\u8f93\u5165\u53d8\u5316\u5bf9\u751f\u6210\u56fe\u50cf\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5916\u90e8\u56e0\u7d20\u663e\u8457\u5f71\u54cd\u751f\u6210\u7684\u4eba\u8138\u56fe\u50cf\uff0c\u95f4\u63a5\u8bc1\u5b9e\u4eba\u7c7b\u8eab\u4efd\u5bf9\u5916\u90e8\u56e0\u7d20\u7684\u4f9d\u8d56\u3002", "conclusion": "\u4eba\u7c7b\u8eab\u4efd\u5177\u6709\u6d41\u52a8\u6027\uff0c\u53d7\u5916\u90e8\u56e0\u7d20\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u89c6\u9891\u4f5c\u54c1\u300aFluidity of Human Identity\u300b\u8868\u8fbe\u8fd9\u4e00\u89c2\u70b9\u3002"}}
{"id": "2505.14846", "pdf": "https://arxiv.org/pdf/2505.14846", "abs": "https://arxiv.org/abs/2505.14846", "authors": ["Daniya Najiha A. Kareem", "Jean Lahoud", "Mustansar Fiaz", "Amandeep Kumar", "Hisham Cholakkal"], "title": "Open-Set Semi-Supervised Learning for Long-Tailed Medical Datasets", "categories": ["cs.CV"], "comment": null, "summary": "Many practical medical imaging scenarios include categories that are\nunder-represented but still crucial. The relevance of image recognition models\nto real-world applications lies in their ability to generalize to these rare\nclasses as well as unseen classes. Real-world generalization requires taking\ninto account the various complexities that can be encountered in the\nreal-world. First, training data is highly imbalanced, which may lead to model\nexhibiting bias toward the more frequently represented classes. Moreover,\nreal-world data may contain unseen classes that need to be identified, and\nmodel performance is affected by the data scarcity. While medical image\nrecognition has been extensively addressed in the literature, current methods\ndo not take into account all the intricacies in the real-world scenarios. To\nthis end, we propose an open-set learning method for highly imbalanced medical\ndatasets using a semi-supervised approach. Understanding the adverse impact of\nlong-tail distribution at the inherent model characteristics, we implement a\nregularization strategy at the feature level complemented by a classifier\nnormalization technique. We conduct extensive experiments on the publicly\navailable datasets, ISIC2018, ISIC2019, and TissueMNIST with various numbers of\nlabelled samples. Our analysis shows that addressing the impact of long-tail\ndata in classification significantly improves the overall performance of the\nnetwork in terms of closed-set and open-set accuracies on all datasets. Our\ncode and trained models will be made publicly available at\nhttps://github.com/Daniyanaj/OpenLTR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u5f00\u653e\u96c6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u548c\u6b63\u5219\u5316\u7b56\u7565\u63d0\u5347\u6a21\u578b\u5bf9\u7f55\u89c1\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u6570\u636e\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u672a\u89c1\u7c7b\u522b\u7684\u5b58\u5728\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\uff0c\u9700\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u7279\u5f81\u7ea7\u6b63\u5219\u5316\u548c\u5206\u7c7b\u5668\u5f52\u4e00\u5316\u6280\u672f\uff0c\u5904\u7406\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u3002", "result": "\u5728ISIC2018\u3001ISIC2019\u548cTissueMNIST\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u6570\u636e\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u7f55\u89c1\u548c\u672a\u89c1\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.14931", "pdf": "https://arxiv.org/pdf/2505.14931", "abs": "https://arxiv.org/abs/2505.14931", "authors": ["Rama Alyoubi", "Taif Alharbi", "Albatul Alghamdi", "Yara Alshehri", "Elham Alghamdi"], "title": "Colors Matter: AI-Driven Exploration of Human Feature Colors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study presents a robust framework that leverages advanced imaging\ntechniques and machine learning for feature extraction and classification of\nkey human attributes-namely skin tone, hair color, iris color, and vein-based\nundertones. The system employs a multi-stage pipeline involving face detection,\nregion segmentation, and dominant color extraction to isolate and analyze these\nfeatures. Techniques such as X-means clustering, alongside perceptually uniform\ndistance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV\ncolor spaces to enhance the accuracy of color differentiation. For\nclassification, the dominant tones of the skin, hair, and iris are extracted\nand matched to a custom tone scale, while vein analysis from wrist images\nenables undertone classification into \"Warm\" or \"Cool\" based on LAB\ndifferences. Each module uses targeted segmentation and color space\ntransformations to ensure perceptual precision. The system achieves up to 80%\naccuracy in tone classification using the Delta E-HSV method with Gaussian\nblur, demonstrating reliable performance across varied lighting and image\nconditions. This work highlights the potential of AI-powered color analysis and\nfeature extraction for delivering inclusive, precise, and nuanced\nclassification, supporting applications in beauty technology, digital\npersonalization, and visual analytics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5148\u8fdb\u6210\u50cf\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u53d6\u548c\u5206\u7c7b\u4eba\u7c7b\u5173\u952e\u5c5e\u6027\uff08\u5982\u80a4\u8272\u3001\u53d1\u8272\u3001\u8679\u819c\u989c\u8272\u548c\u9759\u8109\u8272\u8c03\uff09\u3002\u7cfb\u7edf\u901a\u8fc7\u591a\u9636\u6bb5\u6d41\u7a0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u5e76\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u8fbe\u523080%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7AI\u9a71\u52a8\u7684\u989c\u8272\u5206\u6790\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0\u66f4\u5305\u5bb9\u3001\u7cbe\u786e\u548c\u7ec6\u81f4\u7684\u5206\u7c7b\uff0c\u652f\u6301\u7f8e\u5bb9\u6280\u672f\u3001\u6570\u5b57\u4e2a\u6027\u5316\u548c\u89c6\u89c9\u5206\u6790\u7b49\u5e94\u7528\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u5305\u62ec\u4eba\u8138\u68c0\u6d4b\u3001\u533a\u57df\u5206\u5272\u548c\u4e3b\u8272\u63d0\u53d6\uff0c\u7ed3\u5408X-means\u805a\u7c7b\u548cDelta E\uff08CIEDE2000\uff09\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5728LAB\u548cHSV\u8272\u5f69\u7a7a\u95f4\u4e2d\u8fdb\u884c\u989c\u8272\u533a\u5206\u3002", "result": "\u7cfb\u7edf\u5728Delta E-HSV\u65b9\u6cd5\u7ed3\u5408\u9ad8\u65af\u6a21\u7cca\u7684\u60c5\u51b5\u4e0b\uff0c\u8272\u8c03\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523080%\uff0c\u5728\u4e0d\u540c\u5149\u7167\u548c\u56fe\u50cf\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86AI\u5728\u989c\u8272\u5206\u6790\u548c\u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u7f8e\u5bb9\u6280\u672f\u548c\u6570\u5b57\u4e2a\u6027\u5316\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2505.14948", "pdf": "https://arxiv.org/pdf/2505.14948", "abs": "https://arxiv.org/abs/2505.14948", "authors": ["Hao Tang", "Kevin Ellis", "Suhas Lohit", "Michael J. Jones", "Moitreya Chatterjee"], "title": "Programmatic Video Prediction Using Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The task of estimating the world model describing the dynamics of a real\nworld process assumes immense importance for anticipating and preparing for\nfuture outcomes. For applications such as video surveillance, robotics\napplications, autonomous driving, etc. this objective entails synthesizing\nplausible visual futures, given a few frames of a video to set the visual\ncontext. Towards this end, we propose ProgGen, which undertakes the task of\nvideo frame prediction by representing the dynamics of the video using a set of\nneuro-symbolic, human-interpretable set of states (one per frame) by leveraging\nthe inductive biases of Large (Vision) Language Models (LLM/VLM). In\nparticular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate\nthe states of the video, given the visual context (i.e. the frames); (ii) to\npredict the states corresponding to future time steps by estimating the\ntransition dynamics; (iii) to render the predicted states as visual RGB-frames.\nEmpirical evaluations reveal that our proposed method outperforms competing\ntechniques at the task of video frame prediction in two challenging\nenvironments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits\ncounter-factual reasoning and interpretable video generation attesting to its\neffectiveness and generalizability for video generation tasks.", "AI": {"tldr": "ProgGen\u5229\u7528\u795e\u7ecf\u7b26\u53f7\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LLM/VLM\uff09\u8fdb\u884c\u89c6\u9891\u5e27\u9884\u6d4b\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u89e3\u91ca\u7684\u72b6\u6001\u548c\u52a8\u6001\u8f6c\u6362\u7a0b\u5e8f\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u89c6\u9891\u76d1\u63a7\u3001\u673a\u5668\u4eba\u5e94\u7528\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u4efb\u52a1\u63d0\u4f9b\u52a8\u6001\u9884\u6d4b\u80fd\u529b\uff0c\u901a\u8fc7\u751f\u6210\u89c6\u89c9\u672a\u6765\u5e27\u6765\u589e\u5f3a\u5bf9\u771f\u5b9e\u4e16\u754c\u8fc7\u7a0b\u7684\u7406\u89e3\u3002", "method": "ProgGen\u5229\u7528LLM/VLM\u751f\u6210\u7a0b\u5e8f\uff1a(i) \u4f30\u8ba1\u89c6\u9891\u72b6\u6001\uff1b(ii) \u9884\u6d4b\u672a\u6765\u72b6\u6001\uff1b(iii) \u5c06\u72b6\u6001\u6e32\u67d3\u4e3aRGB\u5e27\u3002", "result": "\u5728PhyWorld\u548cCart Pole\u73af\u5883\u4e2d\uff0cProgGen\u5728\u89c6\u9891\u5e27\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\uff0c\u5e76\u652f\u6301\u53cd\u4e8b\u5b9e\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u89c6\u9891\u751f\u6210\u3002", "conclusion": "ProgGen\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u4e3a\u52a8\u6001\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14951", "pdf": "https://arxiv.org/pdf/2505.14951", "abs": "https://arxiv.org/abs/2505.14951", "authors": ["Jose Sosa", "Danila Rukhovich", "Anis Kacem", "Djamila Aouada"], "title": "MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal data in Earth Observation (EO) presents a huge opportunity for\nimproving transfer learning capabilities when pre-training deep learning\nmodels. Unlike prior work that often overlooks multi-modal EO data, recent\nmethods have started to include it, resulting in more effective pre-training\nstrategies. However, existing approaches commonly face challenges in\neffectively transferring learning to downstream tasks where the structure of\navailable data differs from that used during pre-training. This paper addresses\nthis limitation by exploring a more flexible multi-modal, multi-task\npre-training strategy for EO data. Specifically, we adopt a Multi-modal\nMulti-task Masked Autoencoder (MultiMAE) that we pre-train by reconstructing\ndiverse input modalities, including spectral, elevation, and segmentation data.\nThe pre-trained model demonstrates robust transfer learning capabilities,\noutperforming state-of-the-art methods on various EO datasets for\nclassification and segmentation tasks. Our approach exhibits significant\nflexibility, handling diverse input configurations without requiring\nmodality-specific pre-trained models. Code will be available at:\nhttps://github.com/josesosajs/multimae-meets-eo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u591a\u6a21\u6001\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u7b56\u7565\uff08MultiMAE\uff09\uff0c\u7528\u4e8e\u5730\u7403\u89c2\u6d4b\u6570\u636e\uff0c\u901a\u8fc7\u91cd\u5efa\u591a\u79cd\u8f93\u5165\u6a21\u6001\u63d0\u5347\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5730\u7403\u89c2\u6d4b\u6570\u636e\u9884\u8bad\u7ec3\u4e2d\u96be\u4ee5\u6709\u6548\u8fc1\u79fb\u5230\u4e0b\u6e38\u4efb\u52a1\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528Multi-modal Multi-task Masked Autoencoder\uff08MultiMAE\uff09\uff0c\u9884\u8bad\u7ec3\u65f6\u91cd\u5efa\u5149\u8c31\u3001\u9ad8\u7a0b\u548c\u5206\u5272\u6570\u636e\u7b49\u591a\u79cd\u6a21\u6001\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u7075\u6d3b\u5904\u7406\u591a\u6837\u8f93\u5165\u914d\u7f6e\u3002", "conclusion": "MultiMAE\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u5730\u7403\u89c2\u6d4b\u6570\u636e\u3002"}}
{"id": "2505.15077", "pdf": "https://arxiv.org/pdf/2505.15077", "abs": "https://arxiv.org/abs/2505.15077", "authors": ["Alessandro dos Santos Ferreira", "Ana Paula Marques Ramos", "Jos\u00e9 Marcato Junior", "Wesley Nunes Gon\u00e7alves"], "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07 (Primary), 68U10, 68T45 (Secondary)", "I.4.8; I.2.10; I.5.4"], "comment": "18 pages, 13 figures", "summary": "Urban forests play a key role in enhancing environmental quality and\nsupporting biodiversity in cities. Mapping and monitoring these green spaces\nare crucial for urban planning and conservation, yet accurately detecting trees\nis challenging due to complex landscapes and the variability in image\nresolution caused by different satellite sensors or UAV flight altitudes. While\ndeep learning architectures have shown promise in addressing these challenges,\ntheir effectiveness remains strongly dependent on the availability of large and\nmanually labeled datasets, which are often expensive and difficult to obtain in\nsufficient quantity. In this work, we propose a novel pipeline that integrates\ndomain adaptation with GANs and Diffusion models to enhance the quality of\nlow-resolution aerial images. Our proposed pipeline enhances low-resolution\nimagery while preserving semantic content, enabling effective tree segmentation\nwithout requiring large volumes of manually annotated data. Leveraging models\nsuch as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we\ngenerate realistic and structurally consistent synthetic samples that expand\nthe training dataset and unify scale across domains. This approach not only\nimproves the robustness of segmentation models across different acquisition\nconditions but also provides a scalable and replicable solution for remote\nsensing scenarios with scarce annotation resources. Experimental results\ndemonstrated an improvement of over 50% in IoU for low-resolution images,\nhighlighting the effectiveness of our method compared to traditional pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u57df\u9002\u5e94\u3001GAN\u548c\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u822a\u62cd\u56fe\u50cf\u8d28\u91cf\uff0c\u5b9e\u73b0\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u6811\u6728\u5206\u5272\u3002", "motivation": "\u57ce\u5e02\u68ee\u6797\u5bf9\u73af\u5883\u548c\u751f\u7269\u591a\u6837\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6811\u6728\u68c0\u6d4b\u56e0\u590d\u6742\u666f\u89c2\u548c\u56fe\u50cf\u5206\u8fa8\u7387\u5dee\u5f02\u800c\u56f0\u96be\uff0c\u4e14\u6df1\u5ea6\u5b66\u4e60\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002", "method": "\u6574\u5408pix2pix\u3001Real-ESRGAN\u3001Latent Diffusion\u548cStable Diffusion\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6837\u672c\u4ee5\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684IoU\u63d0\u5347\u8d85\u8fc750%\uff0c\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9065\u611f\u573a\u666f\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u6807\u6ce8\u8d44\u6e90\u7a00\u7f3a\u65f6\u3002"}}
{"id": "2505.15111", "pdf": "https://arxiv.org/pdf/2505.15111", "abs": "https://arxiv.org/abs/2505.15111", "authors": ["Ke Guo", "Haochen Liu", "Xiaojun Wu", "Jia Pan", "Chen Lv"], "title": "iPad: Iterative Proposal-centric End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "End-to-end (E2E) autonomous driving systems offer a promising alternative to\ntraditional modular pipelines by reducing information loss and error\naccumulation, with significant potential to enhance both mobility and safety.\nHowever, most existing E2E approaches directly generate plans based on dense\nbird's-eye view (BEV) grid features, leading to inefficiency and limited\nplanning awareness. To address these limitations, we propose iterative\nProposal-centric autonomous driving (iPad), a novel framework that places\nproposals - a set of candidate future plans - at the center of feature\nextraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder\nthat iteratively refines proposals and their associated features through\nproposal-anchored attention, effectively fusing multi-view image data.\nAdditionally, we introduce two lightweight, proposal-centric auxiliary tasks -\nmapping and prediction - that improve planning quality with minimal\ncomputational overhead. Extensive experiments on the NAVSIM and CARLA\nBench2Drive benchmarks demonstrate that iPad achieves state-of-the-art\nperformance while being significantly more efficient than prior leading\nmethods.", "AI": {"tldr": "iPad\u662f\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u6848\u4e2d\u5fc3\u7684\u65b9\u6cd5\u63d0\u5347\u6548\u7387\u548c\u89c4\u5212\u610f\u8bc6\u3002", "motivation": "\u4f20\u7edf\u7aef\u5230\u7aef\u65b9\u6cd5\u57fa\u4e8e\u5bc6\u96c6BEV\u7f51\u683c\u7279\u5f81\u751f\u6210\u8ba1\u5212\uff0c\u6548\u7387\u4f4e\u4e14\u89c4\u5212\u610f\u8bc6\u6709\u9650\u3002", "method": "\u63d0\u51faiPad\u6846\u67b6\uff0c\u4f7f\u7528ProFormer\u8fed\u4ee3\u4f18\u5316\u63d0\u6848\u53ca\u5176\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u8f85\u52a9\u4efb\u52a1\uff08\u5730\u56fe\u548c\u9884\u6d4b\uff09\u3002", "result": "\u5728NAVSIM\u548cCARLA Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "iPad\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.15123", "pdf": "https://arxiv.org/pdf/2505.15123", "abs": "https://arxiv.org/abs/2505.15123", "authors": ["Ta Duc Huy", "Duy Anh Huynh", "Yutong Xie", "Yuankai Qi", "Qi Chen", "Phi Le Nguyen", "Sen Kim Tran", "Son Lam Phung", "Anton van den Hengel", "Zhibin Liao", "Minh-Son To", "Johan W. Verjans", "Vu Minh Hieu Phan"], "title": "Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": "Under Review", "summary": "Visual grounding (VG) is the capability to identify the specific regions in\nan image associated with a particular text description. In medical imaging, VG\nenhances interpretability by highlighting relevant pathological features\ncorresponding to textual descriptions, improving model transparency and\ntrustworthiness for wider adoption of deep learning models in clinical\npractice. Current models struggle to associate textual descriptions with\ndisease regions due to inefficient attention mechanisms and a lack of\nfine-grained token representations. In this paper, we empirically demonstrate\ntwo key observations. First, current VLMs assign high norms to background\ntokens, diverting the model's attention from regions of disease. Second, the\nglobal tokens used for cross-modal learning are not representative of local\ndisease tokens. This hampers identifying correlations between the text and\ndisease tokens. To address this, we introduce simple, yet effective\nDisease-Aware Prompting (DAP) process, which uses the explainability map of a\nVLM to identify the appropriate image features. This simple strategy amplifies\ndisease-relevant regions while suppressing background interference. Without any\nadditional pixel-level annotations, DAP improves visual grounding accuracy by\n20.74% compared to state-of-the-art methods across three major chest X-ray\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDisease-Aware Prompting (DAP)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u96be\u4ee5\u51c6\u786e\u5173\u8054\u6587\u672c\u63cf\u8ff0\u4e0e\u75be\u75c5\u533a\u57df\uff0c\u4e3b\u8981\u7531\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u6548\u7387\u4f4e\u4e0b\u548c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u6807\u8bb0\u8868\u793a\u3002", "method": "\u63d0\u51faDAP\u65b9\u6cd5\uff0c\u5229\u7528VLM\u7684\u53ef\u89e3\u91ca\u6027\u56fe\u8bc6\u522b\u5408\u9002\u7684\u56fe\u50cf\u7279\u5f81\uff0c\u589e\u5f3a\u75be\u75c5\u76f8\u5173\u533a\u57df\u5e76\u6291\u5236\u80cc\u666f\u5e72\u6270\u3002", "result": "\u5728\u4e0d\u589e\u52a0\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0cDAP\u5728\u4e09\u4e2a\u4e3b\u8981\u80f8\u90e8X\u5149\u6570\u636e\u96c6\u4e0a\u5c06\u89c6\u89c9\u5b9a\u4f4d\u51c6\u786e\u7387\u63d0\u9ad8\u4e8620.74%\u3002", "conclusion": "DAP\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2505.15133", "pdf": "https://arxiv.org/pdf/2505.15133", "abs": "https://arxiv.org/abs/2505.15133", "authors": ["Haiduo Huang", "Jiangcheng Song", "Yadong Zhang", "Pengju Ren"], "title": "DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in knowledge distillation have emphasized the importance of\ndecoupling different knowledge components. While existing methods utilize\nmomentum mechanisms to separate task-oriented and distillation gradients, they\noverlook the inherent conflict between target-class and non-target-class\nknowledge flows. Furthermore, low-confidence dark knowledge in non-target\nclasses introduces noisy signals that hinder effective knowledge transfer. To\naddress these limitations, we propose DeepKD, a novel training framework that\nintegrates dual-level decoupling with adaptive denoising. First, through\ntheoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics\nin task-oriented and non-task-oriented knowledge distillation, we design\nindependent momentum updaters for each component to prevent mutual\ninterference. We observe that the optimal momentum coefficients for\ntask-oriented gradient (TOG), target-class gradient (TCG), and non-target-class\ngradient (NCG) should be positively related to their GSNR. Second, we introduce\na dynamic top-k mask (DTM) mechanism that gradually increases K from a small\ninitial value to incorporate more non-target classes as training progresses,\nfollowing curriculum learning principles. The DTM jointly filters\nlow-confidence logits from both teacher and student models, effectively\npurifying dark knowledge during early training. Extensive experiments on\nCIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code\nis available at https://github.com/haiduo/DeepKD.", "AI": {"tldr": "DeepKD\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u7ea7\u89e3\u8026\u548c\u81ea\u9002\u5e94\u53bb\u566a\u89e3\u51b3\u76ee\u6807\u7c7b\u4e0e\u975e\u76ee\u6807\u7c7b\u77e5\u8bc6\u6d41\u7684\u51b2\u7a81\u548c\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u76ee\u6807\u7c7b\u4e0e\u975e\u76ee\u6807\u7c7b\u77e5\u8bc6\u6d41\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4e14\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u6697\u77e5\u8bc6\u5f15\u5165\u566a\u58f0\u4fe1\u53f7\uff0c\u5f71\u54cd\u77e5\u8bc6\u4f20\u9012\u3002", "method": "\u8bbe\u8ba1\u72ec\u7acb\u52a8\u91cf\u66f4\u65b0\u5668\u5206\u79bb\u4efb\u52a1\u5bfc\u5411\u548c\u975e\u4efb\u52a1\u5bfc\u5411\u77e5\u8bc6\uff0c\u5e76\u5f15\u5165\u52a8\u6001top-k\u63a9\u7801\u673a\u5236\u9010\u6b65\u589e\u52a0\u975e\u76ee\u6807\u7c7b\u77e5\u8bc6\u3002", "result": "\u5728CIFAR-100\u3001ImageNet\u548cMS-COCO\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DeepKD\u7684\u6709\u6548\u6027\u3002", "conclusion": "DeepKD\u901a\u8fc7\u89e3\u8026\u548c\u53bb\u566a\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\u3002"}}
{"id": "2505.15137", "pdf": "https://arxiv.org/pdf/2505.15137", "abs": "https://arxiv.org/abs/2505.15137", "authors": ["Seongmin Hwang", "Daeyoung Han", "Moongu Jeon"], "title": "Multispectral Detection Transformer with Infrared-Centric Sensor Fusion", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Multispectral object detection aims to leverage complementary information\nfrom visible (RGB) and infrared (IR) modalities to enable robust performance\nunder diverse environmental conditions. In this letter, we propose IC-Fusion, a\nmultispectral object detector that effectively fuses visible and infrared\nfeatures through a lightweight and modalityaware design. Motivated by wavelet\nanalysis and empirical observations, we find that IR images contain\nstructurally rich high-frequency information critical for object localization,\nwhile RGB images provide complementary semantic context. To exploit this, we\nadopt a compact RGB backbone and design a novel fusion module comprising a\nMulti-Scale Feature Distillation (MSFD) block to enhance RGB features and a\nthree-stage fusion block with Cross-Modal Channel Shuffle Gate (CCSG) and\nCross-Modal Large Kernel Gate (CLKG) to facilitate effective cross-modal\ninteraction. Experiments on the FLIR and LLVIP benchmarks demonstrate the\neffectiveness and efficiency of our IR-centric fusion strategy. Our code is\navailable at https://github.com/smin-hwang/IC-Fusion.", "AI": {"tldr": "IC-Fusion\u662f\u4e00\u79cd\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u548c\u6a21\u6001\u611f\u77e5\u8bbe\u8ba1\u6709\u6548\u878d\u5408\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u7279\u5f81\uff0c\u5229\u7528RGB\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u548cIR\u7684\u9ad8\u9891\u4fe1\u606f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u53ef\u89c1\u5149\uff08RGB\uff09\u548c\u7ea2\u5916\uff08IR\uff09\u6a21\u6001\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u76ee\u6807\u68c0\u6d4b\u3002", "method": "\u91c7\u7528\u7d27\u51d1\u7684RGB\u9aa8\u5e72\u7f51\u7edc\uff0c\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u7279\u5f81\u84b8\u998f\uff08MSFD\uff09\u6a21\u5757\u589e\u5f3aRGB\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4e09\u9636\u6bb5\u878d\u5408\u5757\uff08CCSG\u548cCLKG\uff09\u4fc3\u8fdb\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728FLIR\u548cLLVIP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "IC-Fusion\u901a\u8fc7IR\u4e3a\u4e2d\u5fc3\u7684\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.15139", "pdf": "https://arxiv.org/pdf/2505.15139", "abs": "https://arxiv.org/abs/2505.15139", "authors": ["Badhan Mazumder", "Lei Wu", "Vince D. Calhoun", "Dong Hye Ye"], "title": "Unified Cross-Modal Attention-Mixer Based Structural-Functional Connectomics Fusion for Neuropsychiatric Disorder Diagnosis", "categories": ["cs.CV"], "comment": "Accepted at 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBC) 2025", "summary": "Gaining insights into the structural and functional mechanisms of the brain\nhas been a longstanding focus in neuroscience research, particularly in the\ncontext of understanding and treating neuropsychiatric disorders such as\nSchizophrenia (SZ). Nevertheless, most of the traditional multimodal deep\nlearning approaches fail to fully leverage the complementary characteristics of\nstructural and functional connectomics data to enhance diagnostic performance.\nTo address this issue, we proposed ConneX, a multimodal fusion method that\nintegrates cross-attention mechanism and multilayer perceptron (MLP)-Mixer for\nrefined feature fusion. Modality-specific backbone graph neural networks (GNNs)\nwere firstly employed to obtain feature representation for each modality. A\nunified cross-modal attention network was then introduced to fuse these\nembeddings by capturing intra- and inter-modal interactions, while MLP-Mixer\nlayers refined global and local features, leveraging higher-order dependencies\nfor end-to-end classification with a multi-head joint loss. Extensive\nevaluations demonstrated improved performance on two distinct clinical\ndatasets, highlighting the robustness of our proposed framework.", "AI": {"tldr": "ConneX\u662f\u4e00\u79cd\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548cMLP-Mixer\uff0c\u7528\u4e8e\u63d0\u5347\u8111\u7ed3\u6784-\u529f\u80fd\u6570\u636e\u7684\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8111\u7ed3\u6784\u548c\u529f\u80fd\u6570\u636e\u7684\u4e92\u8865\u6027\uff0c\u5f71\u54cd\u8bca\u65ad\u6548\u679c\u3002", "method": "\u91c7\u7528\u6a21\u6001\u7279\u5b9a\u7684GNN\u63d0\u53d6\u7279\u5f81\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u7f51\u7edc\u878d\u5408\u6a21\u6001\uff0cMLP-Mixer\u4f18\u5316\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ConneX\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u8111\u75be\u75c5\u8bca\u65ad\u6027\u80fd\u3002"}}
{"id": "2505.15145", "pdf": "https://arxiv.org/pdf/2505.15145", "abs": "https://arxiv.org/abs/2505.15145", "authors": ["Xinran Wang", "Songyu Xu", "Xiangxuan Shan", "Yuxuan Zhang", "Muxi Diao", "Xueyan Duan", "Yanhua Huang", "Kongming Liang", "Zhanyu Ma"], "title": "CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation", "categories": ["cs.CV"], "comment": "Under review", "summary": "Cinematography is a cornerstone of film production and appreciation, shaping\nmood, emotion, and narrative through visual elements such as camera movement,\nshot composition, and lighting. Despite recent progress in multimodal large\nlanguage models (MLLMs) and video generation models, the capacity of current\nmodels to grasp and reproduce cinematographic techniques remains largely\nuncharted, hindered by the scarcity of expert-annotated data. To bridge this\ngap, we present CineTechBench, a pioneering benchmark founded on precise,\nmanual annotation by seasoned cinematography experts across key cinematography\ndimensions. Our benchmark covers seven essential aspects-shot scale, shot\nangle, composition, camera movement, lighting, color, and focal length-and\nincludes over 600 annotated movie images and 120 movie clips with clear\ncinematographic techniques. For the understanding task, we design question\nanswer pairs and annotated descriptions to assess MLLMs' ability to interpret\nand explain cinematographic techniques. For the generation task, we assess\nadvanced video generation models on their capacity to reconstruct\ncinema-quality camera movements given conditions such as textual prompts or\nkeyframes. We conduct a large-scale evaluation on 15+ MLLMs and 5+ video\ngeneration models. Our results offer insights into the limitations of current\nmodels and future directions for cinematography understanding and generation in\nautomatically film production and appreciation. The code and benchmark can be\naccessed at https://github.com/PRIS-CV/CineTechBench.", "AI": {"tldr": "CineTechBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e13\u5bb6\u6807\u6ce8\u7684\u7535\u5f71\u6444\u5f71\u6280\u672f\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u7535\u5f71\u6444\u5f71\u6280\u672f\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u7535\u5f71\u6444\u5f71\u6280\u672f\u65b9\u9762\u7f3a\u4e4f\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0cCineTechBench\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u6807\u6ce8600\u591a\u5f20\u7535\u5f71\u56fe\u50cf\u548c120\u4e2a\u7535\u5f71\u7247\u6bb5\uff0c\u8bbe\u8ba1\u4e86\u95ee\u7b54\u5bf9\u548c\u751f\u6210\u4efb\u52a1\uff0c\u8bc4\u4f3015+ MLLMs\u548c5+\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u81ea\u52a8\u7535\u5f71\u5236\u4f5c\u548c\u6b23\u8d4f\u7684\u65b9\u5411\u3002", "conclusion": "CineTechBench\u4e3a\u7535\u5f71\u6444\u5f71\u6280\u672f\u7684\u7406\u89e3\u548c\u751f\u6210\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.15147", "pdf": "https://arxiv.org/pdf/2505.15147", "abs": "https://arxiv.org/abs/2505.15147", "authors": ["Quanwei Liu", "Tao Huang", "Yanni Dong", "Jiaqi Yang", "Wei Xiang"], "title": "From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation", "categories": ["cs.CV"], "comment": "38 pages, 14 figures, 10 tables", "summary": "Remote sensing images (RSIs) capture both natural and human-induced changes\non the Earth's surface, serving as essential data for environmental monitoring,\nurban planning, and resource management. Semantic segmentation (SS) of RSIs\nenables the fine-grained interpretation of surface features, making it a\ncritical task in remote sensing analysis. With the increasing diversity and\nvolume of RSIs collected by sensors on various platforms, traditional\nprocessing methods struggle to maintain efficiency and accuracy. In response,\ndeep learning (DL) has emerged as a transformative approach, enabling\nsubstantial advances in remote sensing image semantic segmentation (RSISS) by\nautomating feature extraction and improving segmentation accuracy across\ndiverse modalities. This paper revisits the evolution of DL-based RSISS by\ncategorizing existing approaches into four stages: the early pixel-based\nmethods, the prevailing patch-based and tile-based techniques, and the emerging\nimage-based strategies enabled by foundation models. We analyze these\ndevelopments from the perspective of feature extraction and learning\nstrategies, revealing the field's progression from pixel-level to tile-level\nand from unimodal to multimodal segmentation. Furthermore, we conduct a\ncomprehensive evaluation of nearly 40 advanced techniques on a unified dataset\nto quantitatively characterize their performance and applicability. This review\noffers a holistic view of DL-based SS for RS, highlighting key advancements,\ncomparative insights, and open challenges to guide future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u56de\u987e\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\uff08RSISS\uff09\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5c06\u5176\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff0c\u5e76\u5206\u6790\u4e86\u7279\u5f81\u63d0\u53d6\u548c\u5b66\u4e60\u7b56\u7565\u7684\u6f14\u53d8\u3002\u540c\u65f6\uff0c\u5bf9\u8fd140\u79cd\u5148\u8fdb\u6280\u672f\u8fdb\u884c\u4e86\u7edf\u4e00\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u8fdb\u5c55\u548c\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u968f\u7740\u9065\u611f\u56fe\u50cf\uff08RSIs\uff09\u7684\u591a\u6837\u6027\u548c\u6570\u91cf\u589e\u52a0\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u4e3a\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u5e26\u6765\u4e86\u663e\u8457\u8fdb\u6b65\u3002", "method": "\u8bba\u6587\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff08\u50cf\u7d20\u57fa\u3001\u5757\u57fa\u3001\u74e6\u7247\u57fa\u548c\u56fe\u50cf\u57fa\uff09\uff0c\u5e76\u4ece\u7279\u5f81\u63d0\u53d6\u548c\u5b66\u4e60\u7b56\u7565\u7684\u89d2\u5ea6\u8fdb\u884c\u5206\u6790\uff0c\u540c\u65f6\u5bf9\u8fd140\u79cd\u6280\u672f\u8fdb\u884c\u4e86\u7edf\u4e00\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4ece\u50cf\u7d20\u7ea7\u5230\u74e6\u7247\u7ea7\u3001\u4ece\u5355\u6a21\u6001\u5230\u591a\u6a21\u6001\u5206\u5272\u7684\u6f14\u8fdb\u8d8b\u52bf\uff0c\u5e76\u5b9a\u91cf\u6bd4\u8f83\u4e86\u4e0d\u540c\u6280\u672f\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86DL-based RSISS\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u8fdb\u5c55\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.15158", "pdf": "https://arxiv.org/pdf/2505.15158", "abs": "https://arxiv.org/abs/2505.15158", "authors": ["Yunsheng Ma", "Burhaneddin Yaman", "Xin Ye", "Mahmut Yurt", "Jingru Luo", "Abhirup Mallik", "Ziran Wang", "Liu Ren"], "title": "ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages", "summary": "Recent advances have explored integrating large language models (LLMs) into\nend-to-end autonomous driving systems to enhance generalization and\ninterpretability. However, most existing approaches are limited to either\ndriving performance or vision-language reasoning, making it difficult to\nachieve both simultaneously. In this paper, we propose ALN-P3, a unified\nco-distillation framework that introduces cross-modal alignment between \"fast\"\nvision-based autonomous driving systems and \"slow\" language-driven reasoning\nmodules. ALN-P3 incorporates three novel alignment mechanisms: Perception\nAlignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A),\nwhich explicitly align visual tokens with corresponding linguistic outputs\nacross the full perception, prediction, and planning stack. All alignment\nmodules are applied only during training and incur no additional costs during\ninference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X,\nTOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both\ndriving decisions and language reasoning, achieving state-of-the-art results.", "AI": {"tldr": "ALN-P3\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u534f\u540c\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9a7e\u9a76\u51b3\u7b56\u548c\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u9a7e\u9a76\u6027\u80fd\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff0cALN-P3\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u5bf9\u9f50\u673a\u5236\uff08P1A\u3001P2A\u3001P3A\uff09\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9\u9f50\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u6001\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8fbe\u5230\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "ALN-P3\u6709\u6548\u7ed3\u5408\u9a7e\u9a76\u4e0e\u8bed\u8a00\u63a8\u7406\uff0c\u65e0\u9700\u63a8\u7406\u9636\u6bb5\u989d\u5916\u6210\u672c\u3002"}}
{"id": "2505.15160", "pdf": "https://arxiv.org/pdf/2505.15160", "abs": "https://arxiv.org/abs/2505.15160", "authors": ["Jaeyeon Lee", "Dong-Wan Choi"], "title": "Lossless Token Merging Even Without Fine-Tuning in Vision Transformers", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Although Vision Transformers (ViTs) have become the standard architecture in\ncomputer vision, their massive sizes lead to significant computational\noverhead. Token compression techniques have attracted considerable attention to\naddress this issue, but they often suffer from severe information loss,\nrequiring extensive additional training to achieve practical performance. In\nthis paper, we propose Adaptive Token Merging (ATM), a novel method that\nensures lossless token merging, eliminating the need for fine-tuning while\nmaintaining competitive performance. ATM adaptively reduces tokens across\nlayers and batches by carefully adjusting layer-specific similarity thresholds,\nthereby preventing the undesirable merging of less similar tokens with respect\nto each layer. Furthermore, ATM introduces a novel token matching technique\nthat considers not only similarity but also merging sizes, particularly for the\nfinal layers, to minimize the information loss incurred from each merging\noperation. We empirically validate our method across a wide range of pretrained\nmodels, demonstrating that ATM not only outperforms all existing training-free\nmethods but also surpasses most training-intensive approaches, even without\nadditional training. Remarkably, training-free ATM achieves over a 30%\nreduction in FLOPs for the DeiT-T and DeiT-S models without any drop in their\noriginal accuracy.", "AI": {"tldr": "ATM\u662f\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u65e0\u635f\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u4e14\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3ViTs\u56e0\u89c4\u6a21\u5927\u5bfc\u81f4\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u4ee4\u724c\u538b\u7f29\u6280\u672f\u7684\u4fe1\u606f\u4e22\u5931\u548c\u989d\u5916\u8bad\u7ec3\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u5c42\u7279\u5b9a\u76f8\u4f3c\u6027\u9608\u503c\u548c\u5f15\u5165\u8003\u8651\u76f8\u4f3c\u6027\u53ca\u5408\u5e76\u5927\u5c0f\u7684\u4ee4\u724c\u5339\u914d\u6280\u672f\uff0c\u5b9e\u73b0\u65e0\u635f\u4ee4\u724c\u5408\u5e76\u3002", "result": "\u5728\u591a\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cATM\u4f18\u4e8e\u73b0\u6709\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8d8a\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0cFLOPs\u51cf\u5c1130%\u4e14\u7cbe\u5ea6\u4e0d\u53d8\u3002", "conclusion": "ATM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347ViTs\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2505.15172", "pdf": "https://arxiv.org/pdf/2505.15172", "abs": "https://arxiv.org/abs/2505.15172", "authors": ["Xinran Wang", "Muxi Diao", "Yuanzhi Liu", "Chunyu Wang", "Kongming Liang", "Zhanyu Ma", "Jun Guo"], "title": "Harnessing Caption Detailness for Data-Efficient Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Training text-to-image (T2I) models with detailed captions can significantly\nimprove their generation quality. Existing methods often rely on simplistic\nmetrics like caption length to represent the detailness of the caption in the\nT2I training set. In this paper, we propose a new metric to estimate caption\ndetailness based on two aspects: image coverage rate (ICR), which evaluates\nwhether the caption covers all regions/objects in the image, and average object\ndetailness (AOD), which quantifies the detailness of each object's description.\nThrough experiments on the COCO dataset using ShareGPT4V captions, we\ndemonstrate that T2I models trained on high-ICR and -AOD captions achieve\nsuperior performance on DPG and other benchmarks. Notably, our metric enables\nmore effective data selection-training on only 20% of full data surpasses both\nfull-dataset training and length-based selection method, improving alignment\nand reconstruction ability. These findings highlight the critical role of\ndetail-aware metrics over length-based heuristics in caption selection for T2I\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\uff08ICR\u548cAOD\uff09\u6765\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u8bad\u7ec3\u4e2d\u6807\u9898\u7684\u8be6\u7ec6\u7a0b\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660e\u57fa\u4e8e\u8be5\u6807\u51c6\u9009\u62e9\u7684\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6807\u9898\u957f\u5ea6\u7b49\u7b80\u5355\u6307\u6807\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6807\u9898\u7684\u8be6\u7ec6\u7a0b\u5ea6\uff0c\u9650\u5236\u4e86T2I\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u6307\u6807\uff1a\u56fe\u50cf\u8986\u76d6\u7387\uff08ICR\uff09\u548c\u5e73\u5747\u5bf9\u8c61\u8be6\u7ec6\u5ea6\uff08AOD\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u6807\u9898\u7684\u8be6\u7ec6\u7a0b\u5ea6\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u4f7f\u7528\u9ad8ICR\u548cAOD\u6807\u9898\u8bad\u7ec3\u7684T2I\u6a21\u578b\u5728DPG\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u4ec5\u970020%\u6570\u636e\u5373\u53ef\u8d85\u8d8a\u5168\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "conclusion": "\u8be6\u7ec6\u611f\u77e5\u7684\u5ea6\u91cf\u6807\u51c6\u4f18\u4e8e\u57fa\u4e8e\u957f\u5ea6\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bf9T2I\u4efb\u52a1\u7684\u6570\u636e\u9009\u62e9\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.15173", "pdf": "https://arxiv.org/pdf/2505.15173", "abs": "https://arxiv.org/abs/2505.15173", "authors": ["Zhipei Xu", "Xuanyu Zhang", "Xing Zhou", "Jian Zhang"], "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC)\ntechnologies, particularly in video generation, has led to unprecedented\ncreative capabilities but also increased threats to information integrity,\nidentity security, and public trust. Existing detection methods, while\neffective in general scenarios, lack robust solutions for human-centric videos,\nwhich pose greater risks due to their realism and potential for legal and\nethical misuse. Moreover, current detection approaches often suffer from poor\ngeneralization, limited scalability, and reliance on labor-intensive supervised\nfine-tuning. To address these challenges, we propose AvatarShield, the first\ninterpretable MLLM-based framework for detecting human-centric fake videos,\nenhanced via Group Relative Policy Optimization (GRPO). Through our carefully\ndesigned accuracy detection reward and temporal compensation reward, it\neffectively avoids the use of high-cost text annotation data, enabling precise\ntemporal modeling and forgery detection. Meanwhile, we design a dual-encoder\narchitecture, combining high-level semantic reasoning and low-level artifact\namplification to guide MLLMs in effective forgery detection. We further collect\nFakeHumanVid, a large-scale human-centric video benchmark that includes\nsynthesis methods guided by pose, audio, and text inputs, enabling rigorous\nevaluation of detection methods in real-world scenes. Extensive experiments\nshow that AvatarShield significantly outperforms existing approaches in both\nin-domain and cross-domain detection, setting a new standard for human-centric\nvideo forensics.", "AI": {"tldr": "AvatarShield\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4f2a\u9020\u89c6\u9891\uff0c\u901a\u8fc7GRPO\u4f18\u5316\u548c\u53cc\u7f16\u7801\u5668\u67b6\u6784\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "AIGC\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u89c6\u9891\u751f\u6210\u7684\u521b\u9020\u529b\uff0c\u4f46\u4e5f\u5a01\u80c1\u5230\u4fe1\u606f\u5b8c\u6574\u6027\u3001\u8eab\u4efd\u5b89\u5168\u548c\u516c\u4f17\u4fe1\u4efb\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u4eba\u4e3a\u4e2d\u5fc3\u89c6\u9891\u4e2d\u6548\u679c\u4e0d\u8db3\u3002", "method": "\u63d0\u51faAvatarShield\u6846\u67b6\uff0c\u7ed3\u5408GRPO\u4f18\u5316\u548c\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u907f\u514d\u9ad8\u6210\u672c\u6587\u672c\u6807\u6ce8\u6570\u636e\uff0c\u5b9e\u73b0\u7cbe\u786e\u65f6\u95f4\u5efa\u6a21\u548c\u4f2a\u9020\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAvatarShield\u5728\u57df\u5185\u548c\u8de8\u57df\u68c0\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AvatarShield\u4e3a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u53d6\u8bc1\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2505.15176", "pdf": "https://arxiv.org/pdf/2505.15176", "abs": "https://arxiv.org/abs/2505.15176", "authors": ["Qian Zhou", "Xianda Guo", "Jilong Wang", "Chuanfu Shen", "Zhongyuan Wang", "Hua Zou", "Qin Zou", "Chao Liang", "Chen Long", "Gang Wu"], "title": "Exploring Generalized Gait Recognition: Reducing Redundancy and Noise within Indoor and Outdoor Datasets", "categories": ["cs.CV"], "comment": "10 pages, 3 figures", "summary": "Generalized gait recognition, which aims to achieve robust performance across\ndiverse domains, remains a challenging problem due to severe domain shifts in\nviewpoints, appearances, and environments. While mixed-dataset training is\nwidely used to enhance generalization, it introduces new obstacles including\ninter-dataset optimization conflicts and redundant or noisy samples, both of\nwhich hinder effective representation learning. To address these challenges, we\npropose a unified framework that systematically improves cross-domain gait\nrecognition. First, we design a disentangled triplet loss that isolates\nsupervision signals across datasets, mitigating gradient conflicts during\noptimization. Second, we introduce a targeted dataset distillation strategy\nthat filters out the least informative 20\\% of training samples based on\nfeature redundancy and prediction uncertainty, enhancing data efficiency.\nExtensive experiments on CASIA-B, OU-MVLP, Gait3D, and GREW demonstrate that\nour method significantly improves cross-dataset recognition for both GaitBase\nand DeepGaitV2 backbones, without sacrificing source-domain accuracy. Code will\nbe released at https://github.com/li1er3/Generalized_Gait.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u4e09\u5143\u7ec4\u635f\u5931\u548c\u76ee\u6807\u6570\u636e\u96c6\u84b8\u998f\u7b56\u7565\uff0c\u63d0\u5347\u8de8\u57df\u6b65\u6001\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8de8\u57df\u6b65\u6001\u8bc6\u522b\u56e0\u89c6\u89d2\u3001\u5916\u89c2\u548c\u73af\u5883\u7684\u5de8\u5927\u5dee\u5f02\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u867d\u5e38\u7528\u4f46\u5b58\u5728\u4f18\u5316\u51b2\u7a81\u548c\u566a\u58f0\u6837\u672c\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u89e3\u8026\u4e09\u5143\u7ec4\u635f\u5931\u4ee5\u51cf\u5c11\u6570\u636e\u96c6\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\uff0c\u5e76\u5f15\u5165\u76ee\u6807\u6570\u636e\u96c6\u84b8\u998f\u7b56\u7565\u8fc7\u6ee4\u5197\u4f59\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6570\u636e\u96c6\u8bc6\u522b\u6027\u80fd\uff0c\u4e14\u4e0d\u5f71\u54cd\u6e90\u57df\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8de8\u57df\u6b65\u6001\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.15184", "pdf": "https://arxiv.org/pdf/2505.15184", "abs": "https://arxiv.org/abs/2505.15184", "authors": ["Yangting Shi", "Renjie He", "Le Hui", "Xiang Li", "Jian Yang", "Ming-Ming Cheng", "Yimian Dai"], "title": "AuxDet: Auxiliary Metadata Matters for Omni-Domain Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Omni-domain infrared small target detection (IRSTD) poses formidable\nchallenges, as a single model must seamlessly adapt to diverse imaging systems,\nvarying resolutions, and multiple spectral bands simultaneously. Current\napproaches predominantly rely on visual-only modeling paradigms that not only\nstruggle with complex background interference and inherently scarce target\nfeatures, but also exhibit limited generalization capabilities across complex\nomni-scene environments where significant domain shifts and appearance\nvariations occur. In this work, we reveal a critical oversight in existing\nparadigms: the neglect of readily available auxiliary metadata describing\nimaging parameters and acquisition conditions, such as spectral bands, sensor\nplatforms, resolution, and observation perspectives. To address this\nlimitation, we propose the Auxiliary Metadata Driven Infrared Small Target\nDetector (AuxDet), a novel multi-modal framework that fundamentally reimagines\nthe IRSTD paradigm by incorporating textual metadata for scene-aware\noptimization. Through a high-dimensional fusion module based on multi-layer\nperceptrons (MLPs), AuxDet dynamically integrates metadata semantics with\nvisual features, guiding adaptive representation learning for each individual\nsample. Additionally, we design a lightweight prior-initialized enhancement\nmodule using 1D convolutional blocks to further refine fused features and\nrecover fine-grained target cues. Extensive experiments on the challenging\nWideIRSTD-Full benchmark demonstrate that AuxDet consistently outperforms\nstate-of-the-art methods, validating the critical role of auxiliary information\nin improving robustness and accuracy in omni-domain IRSTD tasks. Code is\navailable at https://github.com/GrokCV/AuxDet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAuxDet\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u5143\u6570\u636e\u6539\u8fdb\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff08IRSTD\uff09\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709IRSTD\u65b9\u6cd5\u5ffd\u89c6\u8f85\u52a9\u5143\u6570\u636e\uff08\u5982\u6210\u50cf\u53c2\u6570\uff09\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u8bba\u6587\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u5143\u6570\u636e\u4f18\u5316\u68c0\u6d4b\u3002", "method": "AuxDet\u901a\u8fc7\u591a\u5c42\u611f\u77e5\u673a\uff08MLPs\uff09\u52a8\u6001\u878d\u5408\u5143\u6570\u636e\u4e0e\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u7ea71D\u5377\u79ef\u6a21\u5757\u7ec6\u5316\u7279\u5f81\u3002", "result": "\u5728WideIRSTD-Full\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAuxDet\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8f85\u52a9\u4fe1\u606f\u5bf9\u63d0\u5347\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u7684\u4f5c\u7528\u3002", "conclusion": "\u8f85\u52a9\u5143\u6570\u636e\u5728IRSTD\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\uff0cAuxDet\u4e3a\u591a\u6a21\u6001\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.15185", "pdf": "https://arxiv.org/pdf/2505.15185", "abs": "https://arxiv.org/abs/2505.15185", "authors": ["Yifan Liu", "Keyu Fan", "Weihao Yu", "Chenxin Li", "Hao Lu", "Yixuan Yuan"], "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated\npromising results in real-time high-fidelity rendering without per-scene\noptimization, yet existing approaches still struggle to handle unfamiliar\nvisual content during inference on novel scenes due to limited\ngeneralizability. To address this challenge, we introduce MonoSplat, a novel\nframework that leverages rich visual priors from pre-trained monocular depth\nfoundation models for robust Gaussian reconstruction. Our approach consists of\ntwo key components: a Mono-Multi Feature Adapter that transforms monocular\nfeatures into multi-view representations, coupled with an Integrated Gaussian\nPrediction module that effectively fuses both feature types for precise\nGaussian generation. Through the Adapter's lightweight attention mechanism,\nfeatures are seamlessly aligned and aggregated across views while preserving\nvaluable monocular priors, enabling the Prediction module to generate Gaussian\nprimitives with accurate geometry and appearance. Through extensive experiments\non diverse real-world datasets, we convincingly demonstrate that MonoSplat\nachieves superior reconstruction quality and generalization capability compared\nto existing methods while maintaining computational efficiency with minimal\ntrainable parameters. Codes are available at\nhttps://github.com/CUHK-AIM-Group/MonoSplat.", "AI": {"tldr": "MonoSplat\u5229\u7528\u5355\u76ee\u6df1\u5ea6\u5148\u9a8c\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u7279\u5f81\u9002\u914d\u5668\u548c\u96c6\u6210\u9ad8\u65af\u9884\u6d4b\u6a21\u5757\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u964c\u751f\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0cMonoSplat\u65e8\u5728\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u5355\u76ee\u6df1\u5ea6\u6a21\u578b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5355\u76ee\u591a\u7279\u5f81\u9002\u914d\u5668\u548c\u96c6\u6210\u9ad8\u65af\u9884\u6d4b\u6a21\u5757\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u9f50\u7279\u5f81\u5e76\u751f\u6210\u7cbe\u786e\u7684\u9ad8\u65af\u57fa\u5143\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u91cd\u5efa\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "MonoSplat\u901a\u8fc7\u878d\u5408\u5355\u76ee\u5148\u9a8c\u548c\u591a\u89c6\u56fe\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u6cdb\u5316\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2505.15191", "pdf": "https://arxiv.org/pdf/2505.15191", "abs": "https://arxiv.org/abs/2505.15191", "authors": ["Hana Satou", "Alan Mitkiy", "F Monkey"], "title": "Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation", "categories": ["cs.CV"], "comment": null, "summary": "Transfer learning under domain shift remains a fundamental challenge due to\nthe divergence between source and target data manifolds. In this paper, we\npropose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel framework\nthat decomposes adversarial perturbations into on-manifold and off-manifold\ncomponents to simultaneously capture semantic variation and model brittleness.\nWe theoretically demonstrate that enforcing on-manifold consistency reduces\nhypothesis complexity and improves generalization, while off-manifold\nregularization smooths decision boundaries in low-density regions. Moreover, we\nintroduce a geometry-aware alignment loss that minimizes geodesic discrepancy\nbetween source and target manifolds. Experiments on DomainNet, VisDA, and\nOffice-Home show that MAADA consistently outperforms existing adversarial and\nadaptation methods in both unsupervised and few-shot settings, demonstrating\nsuperior structural robustness and cross-domain generalization.", "AI": {"tldr": "MAADA\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u5bf9\u6297\u6270\u52a8\u4e3a\u6d41\u5f62\u4e0a\u548c\u6d41\u5f62\u5916\u90e8\u5206\uff0c\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6e90\u57df\u548c\u76ee\u6807\u57df\u6570\u636e\u6d41\u5f62\u5dee\u5f02\u5bfc\u81f4\u7684\u8fc1\u79fb\u5b66\u4e60\u6311\u6218\u3002", "method": "\u63d0\u51faMAADA\u6846\u67b6\uff0c\u7ed3\u5408\u6d41\u5f62\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u51e0\u4f55\u5bf9\u9f50\u635f\u5931\u3002", "result": "\u5728DomainNet\u3001VisDA\u548cOffice-Home\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAADA\u5728\u7ed3\u6784\u9c81\u68d2\u6027\u548c\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2505.15192", "pdf": "https://arxiv.org/pdf/2505.15192", "abs": "https://arxiv.org/abs/2505.15192", "authors": ["Fatemeh Ziaeetabar", "Florentin W\u00f6rg\u00f6tter"], "title": "Leveraging Foundation Models for Multimodal Graph-Based Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Foundation models have ushered in a new era for multimodal video\nunderstanding by enabling the extraction of rich spatiotemporal and semantic\nrepresentations. In this work, we introduce a novel graph-based framework that\nintegrates a vision-language foundation, leveraging VideoMAE for dynamic visual\nencoding and BERT for contextual textual embedding, to address the challenge of\nrecognizing fine-grained bimanual manipulation actions. Departing from\nconventional static graph architectures, our approach constructs an adaptive\nmultimodal graph where nodes represent frames, objects, and textual\nannotations, and edges encode spatial, temporal, and semantic relationships.\nThese graph structures evolve dynamically based on learned interactions,\nallowing for flexible and context-aware reasoning. A task-specific attention\nmechanism within a Graph Attention Network further enhances this reasoning by\nmodulating edge importance based on action semantics. Through extensive\nevaluations on diverse benchmark datasets, we demonstrate that our method\nconsistently outperforms state-of-the-art baselines, underscoring the strength\nof combining foundation models with dynamic graph-based reasoning for robust\nand generalizable action recognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08VideoMAE\u548cBERT\uff09\uff0c\u7528\u4e8e\u8bc6\u522b\u7ec6\u7c92\u5ea6\u53cc\u624b\u64cd\u4f5c\u52a8\u4f5c\uff0c\u901a\u8fc7\u52a8\u6001\u591a\u6a21\u6001\u56fe\u548c\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7ec6\u7c92\u5ea6\u53cc\u624b\u64cd\u4f5c\u52a8\u4f5c\u8bc6\u522b\u7684\u6311\u6218\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u4e30\u5bcc\u7684\u65f6\u7a7a\u548c\u8bed\u4e49\u8868\u793a\u3002", "method": "\u6784\u5efa\u52a8\u6001\u591a\u6a21\u6001\u56fe\uff0c\u8282\u70b9\u4e3a\u5e27\u3001\u5bf9\u8c61\u548c\u6587\u672c\u6ce8\u91ca\uff0c\u8fb9\u7f16\u7801\u65f6\u7a7a\u548c\u8bed\u4e49\u5173\u7cfb\uff1b\u91c7\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u57fa\u7840\u6a21\u578b\u4e0e\u52a8\u6001\u56fe\u63a8\u7406\u7ed3\u5408\u7684\u4f18\u52bf\u3002", "conclusion": "\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u548c\u52a8\u6001\u56fe\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u52a8\u4f5c\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2505.15194", "pdf": "https://arxiv.org/pdf/2505.15194", "abs": "https://arxiv.org/abs/2505.15194", "authors": ["Hana Satou", "F Monkey"], "title": "GAMA: Geometry-Aware Manifold Alignment via Structured Adversarial Perturbations for Robust Domain Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Domain adaptation remains a challenge when there is significant manifold\ndiscrepancy between source and target domains. Although recent methods leverage\nmanifold-aware adversarial perturbations to perform data augmentation, they\noften neglect precise manifold alignment and systematic exploration of\nstructured perturbations. To address this, we propose GAMA (Geometry-Aware\nManifold Alignment), a structured framework that achieves explicit manifold\nalignment via adversarial perturbation guided by geometric information. GAMA\nsystematically employs tangent space exploration and manifold-constrained\nadversarial optimization, simultaneously enhancing semantic consistency,\nrobustness to off-manifold deviations, and cross-domain alignment. Theoretical\nanalysis shows that GAMA tightens the generalization bound via structured\nregularization and explicit alignment. Empirical results on DomainNet, VisDA,\nand Office-Home demonstrate that GAMA consistently outperforms existing\nadversarial and adaptation methods in both unsupervised and few-shot settings,\nexhibiting superior robustness, generalization, and manifold alignment\ncapability.", "AI": {"tldr": "GAMA\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u6d41\u5f62\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u6297\u6270\u52a8\u5b9e\u73b0\u663e\u5f0f\u6d41\u5f62\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u9002\u5e94\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6e90\u57df\u548c\u76ee\u6807\u57df\u6d41\u5f62\u5dee\u5f02\u5927\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u7cbe\u786e\u6d41\u5f62\u5bf9\u9f50\u548c\u7ed3\u6784\u5316\u6270\u52a8\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "GAMA\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u5f15\u5bfc\u5bf9\u6297\u6270\u52a8\uff0c\u901a\u8fc7\u5207\u7a7a\u95f4\u63a2\u7d22\u548c\u6d41\u5f62\u7ea6\u675f\u5bf9\u6297\u4f18\u5316\uff0c\u5b9e\u73b0\u663e\u5f0f\u6d41\u5f62\u5bf9\u9f50\u3002", "result": "\u5728DomainNet\u3001VisDA\u548cOffice-Home\u6570\u636e\u96c6\u4e0a\uff0cGAMA\u5728\u65e0\u76d1\u7763\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GAMA\u901a\u8fc7\u7ed3\u6784\u5316\u6b63\u5219\u5316\u548c\u663e\u5f0f\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u548c\u8de8\u57df\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2505.15197", "pdf": "https://arxiv.org/pdf/2505.15197", "abs": "https://arxiv.org/abs/2505.15197", "authors": ["Pinxin Liu", "Haiyang Liu", "Luchuan Song", "Chenliang Xu"], "title": "Intentional Gesture: Deliver Your Intentions with Gestures for Speech", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": null, "summary": "When humans speak, gestures help convey communicative intentions, such as\nadding emphasis or describing concepts. However, current co-speech gesture\ngeneration methods rely solely on superficial linguistic cues (\\textit{e.g.}\nspeech audio or text transcripts), neglecting to understand and leverage the\ncommunicative intention that underpins human gestures. This results in outputs\nthat are rhythmically synchronized with speech but are semantically shallow. To\naddress this gap, we introduce \\textbf{Intentional-Gesture}, a novel framework\nthat casts gesture generation as an intention-reasoning task grounded in\nhigh-level communicative functions. % First, we curate the \\textbf{InG} dataset\nby augmenting BEAT-2 with gesture-intention annotations (\\textit{i.e.}, text\nsentences summarizing intentions), which are automatically annotated using\nlarge vision-language models. Next, we introduce the \\textbf{Intentional\nGesture Motion Tokenizer} to leverage these intention annotations. It injects\nhigh-level communicative functions (\\textit{e.g.}, intentions) into tokenized\nmotion representations to enable intention-aware gesture synthesis that are\nboth temporally aligned and semantically meaningful, achieving new\nstate-of-the-art performance on the BEAT-2 benchmark. Our framework offers a\nmodular foundation for expressive gesture generation in digital humans and\nembodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u610f\u56fe\u63a8\u7406\u7684\u624b\u52bf\u751f\u6210\u6846\u67b6Intentional-Gesture\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u7ea7\u4ea4\u6d41\u529f\u80fd\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u4e14\u65f6\u95f4\u540c\u6b65\u7684\u624b\u52bf\u3002", "motivation": "\u73b0\u6709\u624b\u52bf\u751f\u6210\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6d45\u5c42\u8bed\u8a00\u7ebf\u7d22\uff08\u5982\u8bed\u97f3\u6216\u6587\u672c\uff09\uff0c\u5ffd\u7565\u4e86\u80cc\u540e\u7684\u4ea4\u6d41\u610f\u56fe\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u624b\u52bf\u8bed\u4e49\u6d45\u8584\u3002", "method": "\u63d0\u51faIntentional-Gesture\u6846\u67b6\uff0c\u5229\u7528\u610f\u56fe\u6807\u6ce8\u6570\u636e\u96c6\uff08InG\uff09\u548cIntentional Gesture Motion Tokenizer\uff0c\u5c06\u9ad8\u7ea7\u4ea4\u6d41\u529f\u80fd\u6ce8\u5165\u8fd0\u52a8\u8868\u793a\u4e2d\u3002", "result": "\u5728BEAT-2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u65b0\u6027\u80fd\uff0c\u751f\u6210\u7684\u624b\u52bf\u65e2\u65f6\u95f4\u540c\u6b65\u53c8\u8bed\u4e49\u4e30\u5bcc\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u5b57\u4eba\u548c\u5177\u8eabAI\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u7684\u624b\u52bf\u751f\u6210\u57fa\u7840\u3002"}}
{"id": "2505.15205", "pdf": "https://arxiv.org/pdf/2505.15205", "abs": "https://arxiv.org/abs/2505.15205", "authors": ["Hyogun Lee", "Haksub Kim", "Ig-Jae Kim", "Yonghun Choi"], "title": "Flashback: Memory-Driven Zero-shot, Real-time Video Anomaly Detection", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Video Anomaly Detection (VAD) automatically identifies anomalous events from\nvideo, mitigating the need for human operators in large-scale surveillance\ndeployments. However, three fundamental obstacles hinder real-world adoption:\ndomain dependency and real-time constraints -- requiring near-instantaneous\nprocessing of incoming video. To this end, we propose Flashback, a zero-shot\nand real-time video anomaly detection paradigm. Inspired by the human cognitive\nmechanism of instantly judging anomalies and reasoning in current scenes based\non past experience, Flashback operates in two stages: Recall and Respond. In\nthe offline recall stage, an off-the-shelf LLM builds a pseudo-scene memory of\nboth normal and anomalous captions without any reliance on real anomaly data.\nIn the online respond stage, incoming video segments are embedded and matched\nagainst this memory via similarity search. By eliminating all LLM calls at\ninference time, Flashback delivers real-time VAD even on a consumer-grade GPU.\nOn two large datasets from real-world surveillance scenarios, UCF-Crime and\nXD-Violence, we achieve 87.3 AUC (+7.0 pp) and 75.1 AP (+13.1 pp),\nrespectively, outperforming prior zero-shot VAD methods by large margins.", "AI": {"tldr": "Flashback\u662f\u4e00\u79cd\u96f6\u6837\u672c\u3001\u5b9e\u65f6\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u8bb0\u5fc6\u6784\u5efa\u548c\u5728\u7ebf\u54cd\u5e94\u4e24\u9636\u6bb5\u5b9e\u73b0\uff0c\u65e0\u9700\u771f\u5b9e\u5f02\u5e38\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u9886\u57df\u4f9d\u8d56\u6027\u548c\u5b9e\u65f6\u6027\u9650\u5236\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u79bb\u7ebf\u9636\u6bb5\u7528LLM\u6784\u5efa\u4f2a\u573a\u666f\u8bb0\u5fc6\uff0c\u5728\u7ebf\u9636\u6bb5\u901a\u8fc7\u76f8\u4f3c\u6027\u641c\u7d22\u5339\u914d\u89c6\u9891\u7247\u6bb5\u3002", "result": "\u5728UCF-Crime\u548cXD-Violence\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523087.3 AUC\u548c75.1 AP\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Flashback\u901a\u8fc7\u521b\u65b0\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.15208", "pdf": "https://arxiv.org/pdf/2505.15208", "abs": "https://arxiv.org/abs/2505.15208", "authors": ["Wenjie Liu", "Zhongliang Liu", "Junwei Shu", "Changbo Wang", "Yang Li"], "title": "GT^2-GS: Geometry-aware Texture Transfer for Gaussian Splatting", "categories": ["cs.CV"], "comment": "15 pages, 16 figures", "summary": "Transferring 2D textures to 3D modalities is of great significance for\nimproving the efficiency of multimedia content creation. Existing approaches\nhave rarely focused on transferring image textures onto 3D representations. 3D\nstyle transfer methods are capable of transferring abstract artistic styles to\n3D scenes. However, these methods often overlook the geometric information of\nthe scene, which makes it challenging to achieve high-quality 3D texture\ntransfer results. In this paper, we present GT^2-GS, a geometry-aware texture\ntransfer framework for gaussian splitting. From the perspective of matching\ntexture features with geometric information in rendered views, we identify the\nissue of insufficient texture features and propose a geometry-aware texture\naugmentation module to expand the texture feature set. Moreover, a\ngeometry-consistent texture loss is proposed to optimize texture features into\nthe scene representation. This loss function incorporates both camera pose and\n3D geometric information of the scene, enabling controllable texture-oriented\nappearance editing. Finally, a geometry preservation strategy is introduced. By\nalternating between the texture transfer and geometry correction stages over\nmultiple iterations, this strategy achieves a balance between learning texture\nfeatures and preserving geometric integrity. Extensive experiments demonstrate\nthe effectiveness and controllability of our method. Through geometric\nawareness, our approach achieves texture transfer results that better align\nwith human visual perception. Our homepage is available at\nhttps://vpx-ecnu.github.io/GT2-GS-website.", "AI": {"tldr": "GT^2-GS\u662f\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u7eb9\u7406\u8f6c\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u7eb9\u7406\u7279\u5f81\u4e0e\u51e0\u4f55\u4fe1\u606f\u5339\u914d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u51e0\u4f55\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u98ce\u683c\u8f6c\u79fb\u65b9\u6cd5\u5e38\u5ffd\u7565\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u7eb9\u7406\u8f6c\u79fb\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u63d0\u5347\u7eb9\u7406\u8f6c\u79fb\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u7eb9\u7406\u589e\u5f3a\u6a21\u5757\u548c\u51e0\u4f55\u4e00\u81f4\u7eb9\u7406\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u76f8\u673a\u59ff\u6001\u548c3D\u51e0\u4f55\u4fe1\u606f\u4f18\u5316\u7eb9\u7406\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u7eb9\u7406\u8f6c\u79fb\u6548\u679c\u548c\u51e0\u4f55\u5b8c\u6574\u6027\u4fdd\u6301\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u66f4\u7b26\u5408\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u3002", "conclusion": "GT^2-GS\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u7eb9\u7406\u8f6c\u79fb\uff0c\u4e3a\u591a\u5a92\u4f53\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2505.15217", "pdf": "https://arxiv.org/pdf/2505.15217", "abs": "https://arxiv.org/abs/2505.15217", "authors": ["Haotian Qin", "Dongliang Chang", "Yueying Gao", "Bingyao Yu", "Lei Chen", "Zhanyu Ma"], "title": "Multimodal Conditional Information Bottleneck for Generalizable AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "24 pages, 16 figures", "summary": "Although existing CLIP-based methods for detecting AI-generated images have\nachieved promising results, they are still limited by severe feature\nredundancy, which hinders their generalization ability. To address this issue,\nincorporating an information bottleneck network into the task presents a\nstraightforward solution. However, relying solely on image-corresponding\nprompts results in suboptimal performance due to the inherent diversity of\nprompts. In this paper, we propose a multimodal conditional bottleneck network\nto reduce feature redundancy while enhancing the discriminative power of\nfeatures extracted by CLIP, thereby improving the model's generalization\nability. We begin with a semantic analysis experiment, where we observe that\narbitrary text features exhibit lower cosine similarity with real image\nfeatures than with fake image features in the CLIP feature space, a phenomenon\nwe refer to as \"bias\". Therefore, we introduce InfoFD, a text-guided\nAI-generated image detection framework. InfoFD consists of two key components:\nthe Text-Guided Conditional Information Bottleneck (TGCIB) and Dynamic Text\nOrthogonalization (DTO). TGCIB improves the generalizability of learned\nrepresentations by conditioning on both text and class modalities. DTO\ndynamically updates weighted text features, preserving semantic information\nwhile leveraging the global \"bias\". Our model achieves exceptional\ngeneralization performance on the GenImage dataset and latest generative\nmodels. Our code is available at https://github.com/Ant0ny44/InfoFD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6761\u4ef6\u74f6\u9888\u7f51\u7edc\uff08InfoFD\uff09\uff0c\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u548c\u52a8\u6001\u6b63\u4ea4\u5316\u51cf\u5c11\u7279\u5f81\u5197\u4f59\uff0c\u63d0\u5347AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u5197\u4f59\u95ee\u9898\uff0c\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u4ec5\u4f9d\u8d56\u56fe\u50cf\u5bf9\u5e94\u63d0\u793a\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u6587\u672c\u548c\u7c7b\u522b\u6a21\u6001\u7684\u591a\u6a21\u6001\u6761\u4ef6\u74f6\u9888\u7f51\u7edc\uff08TGCIB\uff09\u548c\u52a8\u6001\u6587\u672c\u6b63\u4ea4\u5316\uff08DTO\uff09\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u589e\u5f3a\u7279\u5f81\u5224\u522b\u529b\u3002", "result": "\u5728GenImage\u6570\u636e\u96c6\u548c\u6700\u65b0\u751f\u6210\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "InfoFD\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u74f6\u9888\u548c\u52a8\u6001\u6b63\u4ea4\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.15222", "pdf": "https://arxiv.org/pdf/2505.15222", "abs": "https://arxiv.org/abs/2505.15222", "authors": ["Yisi Luo", "Xile Zhao", "Deyu Meng"], "title": "Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives", "categories": ["cs.CV"], "comment": null, "summary": "Recently, continuous representation methods emerge as novel paradigms that\ncharacterize the intrinsic structures of real-world data through function\nrepresentations that map positional coordinates to their corresponding values\nin the continuous space. As compared with the traditional discrete framework,\nthe continuous framework demonstrates inherent superiority for data\nrepresentation and reconstruction (e.g., image restoration, novel view\nsynthesis, and waveform inversion) by offering inherent advantages including\nresolution flexibility, cross-modal adaptability, inherent smoothness, and\nparameter efficiency. In this review, we systematically examine recent\nadvancements in continuous representation frameworks, focusing on three\naspects: (i) Continuous representation method designs such as basis function\nrepresentation, statistical modeling, tensor function decomposition, and\nimplicit neural representation; (ii) Theoretical foundations of continuous\nrepresentations such as approximation error analysis, convergence property, and\nimplicit regularization; (iii) Real-world applications of continuous\nrepresentations derived from computer vision, graphics, bioinformatics, and\nremote sensing. Furthermore, we outline future directions and perspectives to\ninspire exploration and deepen insights to facilitate continuous representation\nmethods, theories, and applications. All referenced works are summarized in our\nopen-source repository:\nhttps://github.com/YisiLuo/Continuous-Representation-Zoo.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8fde\u7eed\u8868\u793a\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u65b9\u6cd5\u8bbe\u8ba1\u3001\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563\u6846\u67b6\u5728\u6570\u636e\u8868\u793a\u548c\u91cd\u5efa\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fde\u7eed\u8868\u793a\u65b9\u6cd5\u56e0\u5176\u5206\u8fa8\u7387\u7075\u6d3b\u6027\u3001\u8de8\u6a21\u6001\u9002\u5e94\u6027\u548c\u53c2\u6570\u6548\u7387\u7b49\u4f18\u52bf\u6210\u4e3a\u65b0\u5174\u8303\u5f0f\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u8fde\u7eed\u8868\u793a\u65b9\u6cd5\u7684\u8bbe\u8ba1\uff08\u5982\u57fa\u51fd\u6570\u8868\u793a\u3001\u7edf\u8ba1\u5efa\u6a21\u3001\u5f20\u91cf\u51fd\u6570\u5206\u89e3\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff09\u3001\u7406\u8bba\u57fa\u7840\uff08\u5982\u8fd1\u4f3c\u8bef\u5dee\u5206\u6790\u3001\u6536\u655b\u6027\u548c\u9690\u5f0f\u6b63\u5219\u5316\uff09\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u8fde\u7eed\u8868\u793a\u65b9\u6cd5\u5728\u56fe\u50cf\u6062\u590d\u3001\u65b0\u89c6\u89d2\u5408\u6210\u548c\u6ce2\u5f62\u53cd\u6f14\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fde\u7eed\u8868\u793a\u65b9\u6cd5\u3001\u7406\u8bba\u548c\u5e94\u7528\uff0c\u4ee5\u6df1\u5316\u5176\u6f5c\u529b\u3002"}}
{"id": "2505.15232", "pdf": "https://arxiv.org/pdf/2505.15232", "abs": "https://arxiv.org/abs/2505.15232", "authors": ["Ting Huang", "Zeyu Zhang", "Ruicheng Zhang", "Yang Zhao"], "title": "DC-Scene: Data-Centric Learning for 3D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "3D scene understanding plays a fundamental role in vision applications such\nas robotics, autonomous driving, and augmented reality. However, advancing\nlearning-based 3D scene understanding remains challenging due to two key\nlimitations: (1) the large scale and complexity of 3D scenes lead to higher\ncomputational costs and slower training compared to 2D counterparts; and (2)\nhigh-quality annotated 3D datasets are significantly scarcer than those\navailable for 2D vision. These challenges underscore the need for more\nefficient learning paradigms. In this work, we propose DC-Scene, a data-centric\nframework tailored for 3D scene understanding, which emphasizes enhancing data\nquality and training efficiency. Specifically, we introduce a CLIP-driven\ndual-indicator quality (DIQ) filter, combining vision-language alignment scores\nwith caption-loss perplexity, along with a curriculum scheduler that\nprogressively expands the training pool from the top 25% to 75% of\nscene-caption pairs. This strategy filters out noisy samples and significantly\nreduces dependence on large-scale labeled 3D data. Extensive experiments on\nScanRefer and Nr3D demonstrate that DC-Scene achieves state-of-the-art\nperformance (86.1 CIDEr with the top-75% subset vs. 85.4 with the full dataset)\nwhile reducing training cost by approximately two-thirds, confirming that a\ncompact set of high-quality samples can outperform exhaustive training. Code\nwill be available at https://github.com/AIGeeksGroup/DC-Scene.", "AI": {"tldr": "DC-Scene\u662f\u4e00\u4e2a\u6570\u636e\u4e2d\u5fc3\u76843D\u573a\u666f\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7CLIP\u9a71\u52a8\u7684\u53cc\u6307\u6807\u8d28\u91cf\u8fc7\u6ee4\u5668\u548c\u8bfe\u7a0b\u8c03\u5ea6\u5668\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "3D\u573a\u666f\u7406\u89e3\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCLIP\u9a71\u52a8\u7684\u53cc\u6307\u6807\u8d28\u91cf\u8fc7\u6ee4\u5668\uff08DIQ\uff09\u548c\u8bfe\u7a0b\u8c03\u5ea6\u5668\uff0c\u9010\u6b65\u6269\u5c55\u8bad\u7ec3\u6837\u672c\u6c60\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5728ScanRefer\u548cNr3D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0886.1 CIDEr\uff09\uff0c\u540c\u65f6\u964d\u4f4e\u4e09\u5206\u4e4b\u4e8c\u7684\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u5c0f\u6837\u672c\u8bad\u7ec3\u4f18\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\uff0cDC-Scene\u4e3a3D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.15233", "pdf": "https://arxiv.org/pdf/2505.15233", "abs": "https://arxiv.org/abs/2505.15233", "authors": ["Yuxuan Du", "Zhendong Wang", "Yuhao Luo", "Caiyong Piao", "Zhiyuan Yan", "Hao Li", "Li Yuan"], "title": "CAD: A General Multimodal Framework for Video Deepfake Detection via Cross-Modal Alignment and Distillation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid emergence of multimodal deepfakes (visual and auditory content are\nmanipulated in concert) undermines the reliability of existing detectors that\nrely solely on modality-specific artifacts or cross-modal inconsistencies. In\nthis work, we first demonstrate that modality-specific forensic traces (e.g.,\nface-swap artifacts or spectral distortions) and modality-shared semantic\nmisalignments (e.g., lip-speech asynchrony) offer complementary evidence, and\nthat neglecting either aspect limits detection performance. Existing approaches\neither naively fuse modality-specific features without reconciling their\nconflicting characteristics or focus predominantly on semantic misalignment at\nthe expense of modality-specific fine-grained artifact cues. To address these\nshortcomings, we propose a general multimodal framework for video deepfake\ndetection via Cross-Modal Alignment and Distillation (CAD). CAD comprises two\ncore components: 1) Cross-modal alignment that identifies inconsistencies in\nhigh-level semantic synchronization (e.g., lip-speech mismatches); 2)\nCross-modal distillation that mitigates feature conflicts during fusion while\npreserving modality-specific forensic traces (e.g., spectral distortions in\nsynthetic audio). Extensive experiments on both multimodal and unimodal (e.g.,\nimage-only/video-only)deepfake benchmarks demonstrate that CAD significantly\noutperforms previous methods, validating the necessity of harmonious\nintegration of multimodal complementary information.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0e\u84b8\u998f\uff08CAD\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u6a21\u6001\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u6001\u7279\u5b9a\u75d5\u8ff9\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u5668\u4ec5\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u7684\u75d5\u8ff9\u6216\u8de8\u6a21\u6001\u4e0d\u4e00\u81f4\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u591a\u6a21\u6001\u6df1\u5ea6\u4f2a\u9020\u7684\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u7ed3\u5408\u6a21\u6001\u7279\u5b9a\u75d5\u8ff9\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51faCAD\u6846\u67b6\uff0c\u5305\u542b\u8de8\u6a21\u6001\u5bf9\u9f50\uff08\u68c0\u6d4b\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027\uff09\u548c\u8de8\u6a21\u6001\u84b8\u998f\uff08\u878d\u5408\u7279\u5f81\u65f6\u4fdd\u7559\u6a21\u6001\u7279\u5b9a\u75d5\u8ff9\uff09\u3002", "result": "\u5728\u591a\u79cd\u6df1\u5ea6\u4f2a\u9020\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u4fe1\u606f\u548c\u8c10\u6574\u5408\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "CAD\u901a\u8fc7\u7ed3\u5408\u6a21\u6001\u7279\u5b9a\u75d5\u8ff9\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u4e3a\u591a\u6a21\u6001\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.15241", "pdf": "https://arxiv.org/pdf/2505.15241", "abs": "https://arxiv.org/abs/2505.15241", "authors": ["Kim Yun", "Hana Satou", "F Monkey"], "title": "GAMA++: Disentangled Geometric Alignment with Adaptive Contrastive Perturbation for Reliable Domain Transfer", "categories": ["cs.CV"], "comment": null, "summary": "Despite progress in geometry-aware domain adaptation, current methods such as\nGAMA still suffer from two unresolved issues: (1) insufficient disentanglement\nof task-relevant and task-irrelevant manifold dimensions, and (2) rigid\nperturbation schemes that ignore per-class alignment asymmetries. To address\nthis, we propose GAMA++, a novel framework that introduces (i) latent space\ndisentanglement to isolate label-consistent manifold directions from nuisance\nfactors, and (ii) an adaptive contrastive perturbation strategy that tailors\nboth on- and off-manifold exploration to class-specific manifold curvature and\nalignment discrepancy. We further propose a cross-domain contrastive\nconsistency loss that encourages local semantic clusters to align while\npreserving intra-domain diversity. Our method achieves state-of-the-art results\non DomainNet, Office-Home, and VisDA benchmarks under both standard and\nfew-shot settings, with notable improvements in class-level alignment fidelity\nand boundary robustness. GAMA++ sets a new standard for semantic geometry\nalignment in transfer learning.", "AI": {"tldr": "GAMA++\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u89e3\u8026\u548c\u81ea\u9002\u5e94\u5bf9\u6bd4\u6270\u52a8\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u51e0\u4f55\u611f\u77e5\u57df\u9002\u5e94\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u51e0\u4f55\u611f\u77e5\u57df\u9002\u5e94\u65b9\u6cd5\uff08\u5982GAMA\uff09\u5b58\u5728\u4efb\u52a1\u76f8\u5173\u548c\u4efb\u52a1\u65e0\u5173\u6d41\u5f62\u7ef4\u5ea6\u89e3\u8026\u4e0d\u8db3\uff0c\u4ee5\u53ca\u6270\u52a8\u65b9\u6848\u5ffd\u7565\u7c7b\u95f4\u5bf9\u9f50\u4e0d\u5bf9\u79f0\u6027\u7684\u95ee\u9898\u3002", "method": "GAMA++\u5f15\u5165\u6f5c\u5728\u7a7a\u95f4\u89e3\u8026\u548c\u81ea\u9002\u5e94\u5bf9\u6bd4\u6270\u52a8\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u8de8\u57df\u5bf9\u6bd4\u4e00\u81f4\u6027\u635f\u5931\uff0c\u4f18\u5316\u7c7b\u95f4\u5bf9\u9f50\u548c\u8fb9\u754c\u9c81\u68d2\u6027\u3002", "result": "\u5728DomainNet\u3001Office-Home\u548cVisDA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGAMA++\u5728\u6807\u51c6\u548c\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u7ea7\u5bf9\u9f50\u4fdd\u771f\u5ea6\u548c\u8fb9\u754c\u9c81\u68d2\u6027\u3002", "conclusion": "GAMA++\u4e3a\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u8bed\u4e49\u51e0\u4f55\u5bf9\u9f50\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2505.15248", "pdf": "https://arxiv.org/pdf/2505.15248", "abs": "https://arxiv.org/abs/2505.15248", "authors": ["Andre Dourson", "Kylie Taylor", "Xiaoli Qiao", "Michael Fitzke"], "title": "VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Self-supervised learning has emerged as a powerful paradigm for training deep\nneural networks, particularly in medical imaging where labeled data is scarce.\nWhile current approaches typically rely on synthetic augmentations of single\nimages, we propose VET-DINO, a framework that leverages a unique characteristic\nof medical imaging: the availability of multiple standardized views from the\nsame study. Using a series of clinical veterinary radiographs from the same\npatient study, we enable models to learn view-invariant anatomical structures\nand develop an implied 3D understanding from 2D projections. We demonstrate our\napproach on a dataset of 5 million veterinary radiographs from 668,000 canine\nstudies. Through extensive experimentation, including view synthesis and\ndownstream task performance, we show that learning from real multi-view pairs\nleads to superior anatomical understanding compared to purely synthetic\naugmentations. VET-DINO achieves state-of-the-art performance on various\nveterinary imaging tasks. Our work establishes a new paradigm for\nself-supervised learning in medical imaging that leverages domain-specific\nproperties rather than merely adapting natural image techniques.", "AI": {"tldr": "VET-DINO\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u533b\u5b66\u5f71\u50cf\u4e2d\u591a\u89c6\u89d2\u6807\u51c6\u5316\u89c6\u56fe\u7684\u7279\u6027\uff0c\u4ece2D\u6295\u5f71\u4e2d\u5b66\u4e60\u89c6\u56fe\u4e0d\u53d8\u7684\u89e3\u5256\u7ed3\u6784\uff0c\u5e76\u5728\u517d\u533b\u5f71\u50cf\u4efb\u52a1\u4e2d\u53d6\u5f97\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u56fe\u50cf\u7684\u5408\u6210\u589e\u5f3a\uff0c\u800cVET-DINO\u5229\u7528\u591a\u89c6\u89d2\u89c6\u56fe\u7684\u7279\u6027\uff0c\u63d0\u5347\u89e3\u5256\u7ed3\u6784\u7684\u5b66\u4e60\u6548\u679c\u3002", "method": "\u901a\u8fc7\u540c\u4e00\u60a3\u8005\u7814\u7a76\u4e2d\u7684\u591a\u89c6\u89d2\u517d\u533bX\u5149\u7247\uff0c\u5b66\u4e60\u89c6\u56fe\u4e0d\u53d8\u7684\u89e3\u5256\u7ed3\u6784\uff0c\u5e76\u9690\u542b3D\u7406\u89e3\u3002\u5b9e\u9a8c\u57fa\u4e8e500\u4e07\u5f20\u517d\u533bX\u5149\u7247\u3002", "result": "VET-DINO\u5728\u591a\u89c6\u89d2\u5408\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u7eaf\u5408\u6210\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u5728\u517d\u533b\u5f71\u50cf\u4efb\u52a1\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002", "conclusion": "VET-DINO\u4e3a\u533b\u5b66\u5f71\u50cf\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5229\u7528\u9886\u57df\u7279\u6027\u800c\u975e\u5355\u7eaf\u501f\u9274\u81ea\u7136\u56fe\u50cf\u6280\u672f\u3002"}}
{"id": "2505.15256", "pdf": "https://arxiv.org/pdf/2505.15256", "abs": "https://arxiv.org/abs/2505.15256", "authors": ["Tatyana Shmykova", "Leila Khaertdinova", "Ilya Pershin"], "title": "Zero-Shot Gaze-based Volumetric Medical Image Segmentation", "categories": ["cs.CV", "cs.AI", "I.2.1"], "comment": "Accepted to MMFM-BIOMED Workshop @ CVPR 2025", "summary": "Accurate segmentation of anatomical structures in volumetric medical images\nis crucial for clinical applications, including disease monitoring and cancer\ntreatment planning. Contemporary interactive segmentation models, such as\nSegment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on\nmanually provided prompts like bounding boxes and mouse clicks. In this study,\nwe introduce eye gaze as a novel informational modality for interactive\nsegmentation, marking the application of eye-tracking for 3D medical image\nsegmentation. We evaluate the performance of using gaze-based prompts with\nSAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to\nbounding boxes, gaze-based prompts offer a time-efficient interaction approach\nwith slightly lower segmentation quality. Our findings highlight the potential\nof using gaze as a complementary input modality for interactive 3D medical\nimage segmentation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7684\u65b0\u578b\u4ea4\u4e92\u5f0f\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u4f7f\u7528\u773c\u52a8\u6570\u636e\u4f5c\u4e3a\u63d0\u793a\uff0c\u8bc4\u4f30\u4e86\u5176\u5728SAM-2\u548cMedSAM-2\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4ea4\u4e92\u5f0f\u5206\u5272\u6a21\u578b\u4f9d\u8d56\u624b\u52a8\u63d0\u4f9b\u7684\u63d0\u793a\uff08\u5982\u8fb9\u754c\u6846\u548c\u9f20\u6807\u70b9\u51fb\uff09\uff0c\u800c\u773c\u52a8\u8ffd\u8e2a\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u8f93\u5165\u6a21\u6001\uff0c\u53ef\u80fd\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u773c\u52a8\u6570\u636e\u4f5c\u4e3a\u63d0\u793a\uff0c\u5e76\u5728SAM-2\u548cMedSAM-2\u6a21\u578b\u4e0a\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u4f7f\u7528\u4e86\u5408\u6210\u548c\u771f\u5b9e\u7684\u773c\u52a8\u6570\u636e\u3002", "result": "\u4e0e\u8fb9\u754c\u6846\u76f8\u6bd4\uff0c\u57fa\u4e8e\u773c\u52a8\u7684\u63d0\u793a\u5728\u65f6\u95f4\u6548\u7387\u4e0a\u66f4\u9ad8\uff0c\u4f46\u5206\u5272\u8d28\u91cf\u7565\u4f4e\u3002", "conclusion": "\u773c\u52a8\u8ffd\u8e2a\u53ef\u4f5c\u4e3a3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u8865\u5145\u8f93\u5165\u6a21\u6001\uff0c\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.15263", "pdf": "https://arxiv.org/pdf/2505.15263", "abs": "https://arxiv.org/abs/2505.15263", "authors": ["Om Khangaonkar", "Hamed Pirsiavash"], "title": "gen2seg: Generative Models Enable Generalizable Instance Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Website: https://reachomk.github.io/gen2seg/", "summary": "By pretraining to synthesize coherent images from perturbed inputs,\ngenerative models inherently learn to understand object boundaries and scene\ncompositions. How can we repurpose these generative representations for\ngeneral-purpose perceptual organization? We finetune Stable Diffusion and MAE\n(encoder+decoder) for category-agnostic instance segmentation using our\ninstance coloring loss exclusively on a narrow set of object types (indoor\nfurnishings and cars). Surprisingly, our models exhibit strong zero-shot\ngeneralization, accurately segmenting objects of types and styles unseen in\nfinetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our\nbest-performing models closely approach the heavily supervised SAM when\nevaluated on unseen object types and styles, and outperform it when segmenting\nfine structures and ambiguous boundaries. In contrast, existing promptable\nsegmentation architectures or discriminatively pretrained models fail to\ngeneralize. This suggests that generative models learn an inherent grouping\nmechanism that transfers across categories and domains, even without\ninternet-scale pretraining. Code, pretrained models, and demos are available on\nour website.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03\u751f\u6210\u6a21\u578b\uff08\u5982Stable Diffusion\u548cMAE\uff09\u7528\u4e8e\u7c7b\u522b\u65e0\u5173\u7684\u5b9e\u4f8b\u5206\u5272\uff0c\u53d1\u73b0\u5176\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u76d1\u7763\u6a21\u578bSAM\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u8868\u5f81\u80fd\u529b\u8fdb\u884c\u901a\u7528\u611f\u77e5\u7ec4\u7ec7\uff0c\u5c24\u5176\u662f\u5bf9\u8c61\u8fb9\u754c\u548c\u573a\u666f\u7ec4\u5408\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u5b9e\u4f8b\u7740\u8272\u635f\u5931\u5fae\u8c03Stable Diffusion\u548cMAE\uff0c\u4ec5\u9488\u5bf9\u5c11\u91cf\u5bf9\u8c61\u7c7b\u578b\uff08\u5ba4\u5185\u5bb6\u5177\u548c\u6c7d\u8f66\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u5bf9\u8c61\u7c7b\u578b\u548c\u98ce\u683c\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u751a\u81f3\u5728\u7cbe\u7ec6\u7ed3\u6784\u548c\u6a21\u7cca\u8fb9\u754c\u7684\u5206\u5272\u4e0a\u4f18\u4e8eSAM\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u5b66\u4e60\u4e86\u4e00\u79cd\u8de8\u7c7b\u522b\u548c\u9886\u57df\u7684\u56fa\u6709\u5206\u7ec4\u673a\u5236\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u6cdb\u5316\u3002"}}
{"id": "2505.15265", "pdf": "https://arxiv.org/pdf/2505.15265", "abs": "https://arxiv.org/abs/2505.15265", "authors": ["Zihao Pan", "Yu Tong", "Weibin Wu", "Jingyi Wang", "Lifeng Chen", "Zhe Zhao", "Jiajia Wei", "Yitong Qiao", "Zibin Zheng"], "title": "Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408LLMs\u548cT2I\u6a21\u578b\uff0c\u63a2\u7d22\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5bf9\u7279\u5b9a\u8bed\u4e49\u6982\u5ff5\u7684\u654f\u611f\u6027\uff0c\u5e76\u91cf\u5316\u5176\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793aLVLMs\u5728\u9762\u5bf9\u7279\u5b9a\u8bed\u4e49\u6982\u5ff5\u65f6\u6613\u4ea7\u751f\u5e7b\u89c9\u548c\u9519\u8bef\u7684\u673a\u5236\uff0c\u4ee5\u9488\u5bf9\u6027\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5229\u7528LLMs\u8fdb\u884c\u8bed\u4e49\u6982\u5ff5\u7684\u4ea4\u53c9\u548c\u53d8\u5f02\u64cd\u4f5c\uff0c\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u518d\u901a\u8fc7T2I\u6a21\u578b\u8f6c\u6362\u4e3a\u89c6\u89c9\u8f93\u5165\uff0c\u4ee5LVLMs\u7684\u4efb\u52a1\u6027\u80fd\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\u8bed\u4e49\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u5728\u4e03\u79cd\u4e3b\u6d41LVLMs\u548c\u4e24\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u53d1\u73b0\u4e86LVLMs\u7684\u654f\u611f\u8bed\u4e49\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63a2\u7d22LVLMs\u7684\u654f\u611f\u8bed\u4e49\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u542f\u53d1\u3002"}}
{"id": "2505.15267", "pdf": "https://arxiv.org/pdf/2505.15267", "abs": "https://arxiv.org/abs/2505.15267", "authors": ["Wenmin Li", "Shunsuke Sakai", "Tatsuhito Hasegawa"], "title": "Contrastive Learning-Enhanced Trajectory Matching for Small-Scale Dataset Distillation", "categories": ["cs.CV"], "comment": "Under review", "summary": "Deploying machine learning models in resource-constrained environments, such\nas edge devices or rapid prototyping scenarios, increasingly demands\ndistillation of large datasets into significantly smaller yet informative\nsynthetic datasets. Current dataset distillation techniques, particularly\nTrajectory Matching methods, optimize synthetic data so that the model's\ntraining trajectory on synthetic samples mirrors that on real data. While\ndemonstrating efficacy on medium-scale synthetic datasets, these methods fail\nto adequately preserve semantic richness under extreme sample scarcity. To\naddress this limitation, we propose a novel dataset distillation method\nintegrating contrastive learning during image synthesis. By explicitly\nmaximizing instance-level feature discrimination, our approach produces more\ninformative and diverse synthetic samples, even when dataset sizes are\nsignificantly constrained. Experimental results demonstrate that incorporating\ncontrastive learning substantially enhances the performance of models trained\non very small-scale synthetic datasets. This integration not only guides more\neffective feature representation but also significantly improves the visual\nfidelity of the synthesized images. Experimental results demonstrate that our\nmethod achieves notable performance improvements over existing distillation\ntechniques, especially in scenarios with extremely limited synthetic data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5728\u6781\u5c11\u91cf\u6837\u672c\u4e0b\u8bed\u4e49\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u7684\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5c06\u5927\u6570\u636e\u96c6\u538b\u7f29\u4e3a\u5c0f\u800c\u4fe1\u606f\u4e30\u5bcc\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u6837\u672c\u7a00\u7f3a\u65f6\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u4e30\u5bcc\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u7684\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5b9e\u4f8b\u7ea7\u7279\u5f81\u533a\u5206\u5ea6\uff0c\u751f\u6210\u66f4\u5177\u4fe1\u606f\u91cf\u548c\u591a\u6837\u6027\u7684\u5408\u6210\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6781\u5c11\u91cf\u5408\u6210\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u6539\u5584\u4e86\u5408\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u5728\u6781\u7aef\u6837\u672c\u7a00\u7f3a\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2505.15269", "pdf": "https://arxiv.org/pdf/2505.15269", "abs": "https://arxiv.org/abs/2505.15269", "authors": ["Zhenyu Ning", "Guangda Liu", "Qihao Jin", "Wenchao Ding", "Minyi Guo", "Jieru Zhao"], "title": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance.", "AI": {"tldr": "LiveVLM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u4e13\u4e3a\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u548c\u4ea4\u4e92\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u5f0fKV\u7f13\u5b58\u6280\u672f\u63d0\u5347\u5904\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u79bb\u7ebf\u89c6\u9891\u95ee\u7b54\uff0c\u5ffd\u7565\u4e86\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u5185\u5b58\u548c\u54cd\u5e94\u901f\u5ea6\u9700\u6c42\u3002", "method": "LiveVLM\u91c7\u7528\u6d41\u5f0fKV\u7f13\u5b58\u6280\u672f\uff0c\u5b9e\u65f6\u5904\u7406\u89c6\u9891\u6d41\u5e76\u4fdd\u7559\u957f\u671f\u7ec6\u8282\uff0c\u540c\u65f6\u538b\u7f29\u89c6\u9891KV\u5f20\u91cf\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLiveVLM\u5728\u76f8\u540c\u8bbe\u5907\u4e0a\u53ef\u5904\u740644\u500d\u5e27\u6570\uff0c\u54cd\u5e94\u901f\u5ea6\u63d0\u53475\u500d\uff0c\u4e14\u6027\u80fd\u4e0d\u964d\u3002", "conclusion": "LiveVLM\u4e3a\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2505.15272", "pdf": "https://arxiv.org/pdf/2505.15272", "abs": "https://arxiv.org/abs/2505.15272", "authors": ["Eduarda Caldeira", "Jan Niklas Kolf", "Naser Damer", "Fadi Boutros"], "title": "DiffProb: Data Pruning for Face Recognition", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG) 2025", "summary": "Face recognition models have made substantial progress due to advances in\ndeep learning and the availability of large-scale datasets. However, reliance\non massive annotated datasets introduces challenges related to training\ncomputational cost and data storage, as well as potential privacy concerns\nregarding managing large face datasets. This paper presents DiffProb, the first\ndata pruning approach for the application of face recognition. DiffProb\nassesses the prediction probabilities of training samples within each identity\nand prunes the ones with identical or close prediction probability values, as\nthey are likely reinforcing the same decision boundaries, and thus contribute\nminimally with new information. We further enhance this process with an\nauxiliary cleaning mechanism to eliminate mislabeled and label-flipped samples,\nboosting data quality with minimal loss. Extensive experiments on CASIA-WebFace\nwith different pruning ratios and multiple benchmarks, including LFW, CFP-FP,\nand IJB-C, demonstrate that DiffProb can prune up to 50% of the dataset while\nmaintaining or even, in some settings, improving the verification accuracies.\nAdditionally, we demonstrate DiffProb's robustness across different\narchitectures and loss functions. Our method significantly reduces training\ncost and data volume, enabling efficient face recognition training and reducing\nthe reliance on massive datasets and their demanding management.", "AI": {"tldr": "DiffProb\u662f\u4e00\u79cd\u7528\u4e8e\u4eba\u8138\u8bc6\u522b\u7684\u6570\u636e\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u8bad\u7ec3\u6837\u672c\u7684\u9884\u6d4b\u6982\u7387\u5e76\u526a\u9664\u5197\u4f59\u6837\u672c\uff0c\u51cf\u5c11\u8bad\u7ec3\u6210\u672c\u548c\u6570\u636e\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u5e26\u6765\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u5b58\u50a8\u538b\u529b\u5927\u53ca\u9690\u79c1\u95ee\u9898\uff0c\u9700\u51cf\u5c11\u6570\u636e\u4f9d\u8d56\u3002", "method": "DiffProb\u901a\u8fc7\u5206\u6790\u6837\u672c\u9884\u6d4b\u6982\u7387\u526a\u9664\u5197\u4f59\u6837\u672c\uff0c\u5e76\u52a0\u5165\u8f85\u52a9\u6e05\u7406\u673a\u5236\u53bb\u9664\u9519\u8bef\u6807\u7b7e\u6837\u672c\u3002", "result": "\u5728CASIA-WebFace\u4e0a\u526a\u679d50%\u6570\u636e\u540e\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u4fdd\u6301\u6216\u63d0\u5347\uff0c\u4e14\u65b9\u6cd5\u5bf9\u4e0d\u540c\u67b6\u6784\u548c\u635f\u5931\u51fd\u6570\u9c81\u68d2\u3002", "conclusion": "DiffProb\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u548c\u6570\u636e\u91cf\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002"}}
{"id": "2505.15287", "pdf": "https://arxiv.org/pdf/2505.15287", "abs": "https://arxiv.org/abs/2505.15287", "authors": ["Yuchen Li", "Chaoran Feng", "Zhenyu Tang", "Kaiyuan Deng", "Wangbo Yu", "Yonghong Tian", "Li Yuan"], "title": "GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation", "categories": ["cs.CV"], "comment": "21 pages, 7 figures. More details at\n  http://intothemild.github.io/GS2E.github.io", "summary": "We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic\nevent dataset for high-fidelity event vision tasks, captured from real-world\nsparse multi-view RGB images. Existing event datasets are often synthesized\nfrom dense RGB videos, which typically lack viewpoint diversity and geometric\nconsistency, or depend on expensive, difficult-to-scale hardware setups. GS2E\novercomes these limitations by first reconstructing photorealistic static\nscenes using 3D Gaussian Splatting, and subsequently employing a novel,\nphysically-informed event simulation pipeline. This pipeline generally\nintegrates adaptive trajectory interpolation with physically-consistent event\ncontrast threshold modeling. Such an approach yields temporally dense and\ngeometrically consistent event streams under diverse motion and lighting\nconditions, while ensuring strong alignment with underlying scene structures.\nExperimental results on event-based 3D reconstruction demonstrate GS2E's\nsuperior generalization capabilities and its practical value as a benchmark for\nadvancing event vision research.", "AI": {"tldr": "GS2E\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6563\u5c04\u91cd\u5efa\u7684\u5927\u89c4\u6a21\u5408\u6210\u4e8b\u4ef6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u4e8b\u4ef6\u89c6\u89c9\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u89c6\u89d2\u5355\u4e00\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u6570\u636e\u96c6\u901a\u5e38\u4ece\u5bc6\u96c6RGB\u89c6\u9891\u5408\u6210\uff0c\u7f3a\u4e4f\u89c6\u89d2\u591a\u6837\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u6216\u4f9d\u8d56\u6602\u8d35\u786c\u4ef6\u3002GS2E\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u901a\u8fc73D\u9ad8\u65af\u6563\u5c04\u91cd\u5efa\u771f\u5b9e\u9759\u6001\u573a\u666f\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u8f68\u8ff9\u63d2\u503c\u548c\u7269\u7406\u4e00\u81f4\u7684\u4e8b\u4ef6\u5bf9\u6bd4\u9608\u503c\u5efa\u6a21\uff0c\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u7684\u4e8b\u4ef6\u6d41\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGS2E\u5728\u4e8b\u4ef63D\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u5408\u4f5c\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u7814\u7a76\u7684\u57fa\u51c6\u3002", "conclusion": "GS2E\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.15294", "pdf": "https://arxiv.org/pdf/2505.15294", "abs": "https://arxiv.org/abs/2505.15294", "authors": ["Xu yan", "Zhaohui Wang", "Rong Wei", "Jingbo Yu", "Dong Li", "Xiangde Liu"], "title": "R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in Unconstrained Image Collections", "categories": ["cs.CV", "cs.RO"], "comment": "7 pages, 4 figures", "summary": "We propose R3GS, a robust reconstruction and relocalization framework\ntailored for unconstrained datasets. Our method uses a hybrid representation\nduring training. Each anchor combines a global feature from a convolutional\nneural network (CNN) with a local feature encoded by the multiresolution hash\ngrids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict\nthe attributes of each Gaussians, including color, opacity, and covariance. To\nmitigate the adverse effects of transient objects on the reconstruction\nprocess, we ffne-tune a lightweight human detection network. Once ffne-tuned,\nthis network generates a visibility map that efffciently generalizes to other\ntransient objects (such as posters, banners, and cars) with minimal need for\nfurther adaptation. Additionally, to address the challenges posed by sky\nregions in outdoor scenes, we propose an effective sky-handling technique that\nincorporates a depth prior as a constraint. This allows the inffnitely distant\nsky to be represented on the surface of a large-radius sky sphere,\nsigniffcantly reducing ffoaters caused by errors in sky reconstruction.\nFurthermore, we introduce a novel relocalization method that remains robust to\nchanges in lighting conditions while estimating the camera pose of a given\nimage within the reconstructed 3DGS scene. As a result, R3GS significantly\nenhances rendering ffdelity, improves both training and rendering efffciency,\nand reduces storage requirements. Our method achieves state-of-the-art\nperformance compared to baseline methods on in-the-wild datasets. The code will\nbe made open-source following the acceptance of the paper.", "AI": {"tldr": "R3GS\u662f\u4e00\u4e2a\u9488\u5bf9\u65e0\u7ea6\u675f\u6570\u636e\u96c6\u7684\u9c81\u68d2\u91cd\u5efa\u548c\u91cd\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u4f18\u5316\u8bad\u7ec3\u548c\u6e32\u67d3\u6548\u7387\uff0c\u5e76\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ea6\u675f\u6570\u636e\u96c6\u4e2d\u77ac\u6001\u7269\u4f53\u548c\u5929\u7a7a\u533a\u57df\u5bf9\u91cd\u5efa\u8fc7\u7a0b\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4ee5\u53ca\u5149\u7167\u53d8\u5316\u5bf9\u91cd\u5b9a\u4f4d\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u8868\u793a\uff08CNN\u5168\u5c40\u7279\u5f81\u548c\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u5c40\u90e8\u7279\u5f81\uff09\uff0c\u6d45\u5c42MLPs\u9884\u6d4b\u9ad8\u65af\u5c5e\u6027\uff0c\u5fae\u8c03\u8f7b\u91cf\u7ea7\u4eba\u4f53\u68c0\u6d4b\u7f51\u7edc\u751f\u6210\u53ef\u89c1\u6027\u5730\u56fe\uff0c\u5e76\u63d0\u51fa\u5929\u7a7a\u5904\u7406\u6280\u672f\u548c\u9c81\u68d2\u91cd\u5b9a\u4f4d\u65b9\u6cd5\u3002", "result": "\u663e\u8457\u63d0\u5347\u6e32\u67d3\u4fdd\u771f\u5ea6\u3001\u8bad\u7ec3\u548c\u6e32\u67d3\u6548\u7387\uff0c\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\uff0c\u5728\u91ce\u5916\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "R3GS\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u91cd\u5efa\u548c\u91cd\u5b9a\u4f4d\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.15308", "pdf": "https://arxiv.org/pdf/2505.15308", "abs": "https://arxiv.org/abs/2505.15308", "authors": ["Ji Guo", "Xiaolei Wen", "Wenbo Jiang", "Cheng Huang", "Jinjin Li", "Hongwei Li"], "title": "BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the widespread application of super-resolution (SR) in various fields,\nresearchers have begun to investigate its security. Previous studies have\ndemonstrated that SR models can also be subjected to backdoor attacks through\ndata poisoning, affecting downstream tasks. A backdoor SR model generates an\nattacker-predefined target image when given a triggered image while producing a\nnormal high-resolution (HR) output for clean images. However, prior backdoor\nattacks on SR models have primarily focused on the stealthiness of poisoned\nlow-resolution (LR) images while ignoring the stealthiness of poisoned HR\nimages, making it easy for users to detect anomalous data. To address this\nproblem, we propose BadSR, which improves the stealthiness of poisoned HR\nimages. The key idea of BadSR is to approximate the clean HR image and the\npre-defined target image in the feature space while ensuring that modifications\nto the clean HR image remain within a constrained range. The poisoned HR images\ngenerated by BadSR can be integrated with existing triggers. To further improve\nthe effectiveness of BadSR, we design an adversarially optimized trigger and a\nbackdoor gradient-driven poisoned sample selection method based on a genetic\nalgorithm. The experimental results show that BadSR achieves a high attack\nsuccess rate in various models and data sets, significantly affecting\ndownstream tasks.", "AI": {"tldr": "BadSR\u662f\u4e00\u79cd\u9488\u5bf9\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u6a21\u578b\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u9ad8\u6bd2\u5316\u9ad8\u5206\u8fa8\u7387\uff08HR\uff09\u56fe\u50cf\u7684\u9690\u853d\u6027\uff0c\u4f7f\u5176\u66f4\u96be\u88ab\u7528\u6237\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u653b\u51fb\u4e3b\u8981\u5173\u6ce8\u6bd2\u5316\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u56fe\u50cf\u7684\u9690\u853d\u6027\uff0c\u5ffd\u7565\u4e86HR\u56fe\u50cf\u7684\u9690\u853d\u6027\uff0c\u5bb9\u6613\u88ab\u7528\u6237\u53d1\u73b0\u5f02\u5e38\u6570\u636e\u3002", "method": "BadSR\u901a\u8fc7\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u903c\u8fd1\u5e72\u51c0HR\u56fe\u50cf\u548c\u9884\u5b9a\u4e49\u76ee\u6807\u56fe\u50cf\uff0c\u5e76\u9650\u5236\u5bf9\u5e72\u51c0HR\u56fe\u50cf\u7684\u4fee\u6539\u8303\u56f4\uff0c\u751f\u6210\u9690\u853d\u7684\u6bd2\u5316HR\u56fe\u50cf\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u5bf9\u6297\u4f18\u5316\u7684\u89e6\u53d1\u5668\u548c\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u6bd2\u5316\u6837\u672c\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBadSR\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u663e\u8457\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "BadSR\u901a\u8fc7\u6539\u8fdbHR\u56fe\u50cf\u7684\u9690\u853d\u6027\uff0c\u63d0\u5347\u4e86\u540e\u95e8\u653b\u51fb\u7684\u9690\u853d\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2505.15313", "pdf": "https://arxiv.org/pdf/2505.15313", "abs": "https://arxiv.org/abs/2505.15313", "authors": ["Kazuaki Mishima", "Antoni Bigata Casademunt", "Stavros Petridis", "Maja Pantic", "Kenji Suzuki"], "title": "FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion", "categories": ["cs.CV"], "comment": "9 pages(excluding references), 3 figures, 5 tables", "summary": "Human facial images encode a rich spectrum of information, encompassing both\nstable identity-related traits and mutable attributes such as pose, expression,\nand emotion. While recent advances in image generation have enabled\nhigh-quality identity-conditional face synthesis, precise control over\nnon-identity attributes remains challenging, and disentangling identity from\nthese mutable factors is particularly difficult. To address these limitations,\nwe propose a novel identity-conditional diffusion model that introduces two\nlightweight control modules designed to independently manipulate facial pose,\nexpression, and emotion without compromising identity preservation. These\nmodules are embedded within the cross-attention layers of the base diffusion\nmodel, enabling precise attribute control with minimal parameter overhead.\nFurthermore, our tailored training strategy, which leverages cross-attention\nbetween the identity feature and each non-identity control feature, encourages\nidentity features to remain orthogonal to control signals, enhancing\ncontrollability and diversity. Quantitative and qualitative evaluations, along\nwith perceptual user studies, demonstrate that our method surpasses existing\napproaches in terms of control accuracy over pose, expression, and emotion,\nwhile also improving generative diversity under identity-only conditioning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8eab\u4efd\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a7\u5236\u6a21\u5757\u72ec\u7acb\u64cd\u7eb5\u9762\u90e8\u59ff\u6001\u3001\u8868\u60c5\u548c\u60c5\u611f\uff0c\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u4e0d\u53d8\u3002", "motivation": "\u4eba\u8138\u56fe\u50cf\u5305\u542b\u4e30\u5bcc\u7684\u8eab\u4efd\u548c\u975e\u8eab\u4efd\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u8eab\u4efd\u5c5e\u6027\u63a7\u5236\u548c\u8eab\u4efd\u5206\u79bb\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u5728\u57fa\u7840\u6269\u6563\u6a21\u578b\u4e2d\u5d4c\u5165\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u63a7\u5236\u6a21\u5757\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u72ec\u7acb\u63a7\u5236\u975e\u8eab\u4efd\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u5b9a\u5236\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u6b63\u4ea4\u6027\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a7\u5236\u7cbe\u5ea6\u548c\u751f\u6210\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u975e\u8eab\u4efd\u5c5e\u6027\u63a7\u5236\u7684\u6311\u6218\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u751f\u6210\u591a\u6837\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\u3002"}}
{"id": "2505.15322", "pdf": "https://arxiv.org/pdf/2505.15322", "abs": "https://arxiv.org/abs/2505.15322", "authors": ["Qi'ao Xu", "Yan Xing", "Jiali Hu", "Yunan Jia", "Rui Huang"], "title": "CEBSNet: Change-Excited and Background-Suppressed Network with Temporal Dependency Modeling for Bitemporal Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "Change detection, a critical task in remote sensing and computer vision, aims\nto identify pixel-level differences between image pairs captured at the same\ngeographic area but different times. It faces numerous challenges such as\nillumination variation, seasonal changes, background interference, and shooting\nangles, especially with a large time gap between images. While current methods\nhave advanced, they often overlook temporal dependencies and overemphasize\nprominent changes while ignoring subtle but equally important changes. To\naddress these limitations, we introduce \\textbf{CEBSNet}, a novel\nchange-excited and background-suppressed network with temporal dependency\nmodeling for change detection. During the feature extraction, we utilize a\nsimple Channel Swap Module (CSM) to model temporal dependency, reducing\ndifferences and noise. The Feature Excitation and Suppression Module (FESM) is\ndeveloped to capture both obvious and subtle changes, maintaining the integrity\nof change regions. Additionally, we design a Pyramid-Aware Spatial-Channel\nAttention module (PASCA) to enhance the ability to detect change regions at\ndifferent sizes and focus on critical regions. We conduct extensive experiments\non three common street view datasets and two remote sensing datasets, and our\nmethod achieves the state-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCEBSNet\u7684\u65b0\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u53d8\u5316\u68c0\u6d4b\u9762\u4e34\u5149\u7167\u3001\u5b63\u8282\u3001\u80cc\u666f\u5e72\u6270\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u7ec6\u5fae\u53d8\u5316\u3002", "method": "\u4f7f\u7528Channel Swap Module (CSM)\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u6027\uff0cFeature Excitation and Suppression Module (FESM)\u6355\u6349\u53d8\u5316\uff0cPyramid-Aware Spatial-Channel Attention (PASCA)\u589e\u5f3a\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "CEBSNet\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2505.15325", "pdf": "https://arxiv.org/pdf/2505.15325", "abs": "https://arxiv.org/abs/2505.15325", "authors": ["Mengqi Lei", "Yihong Wu", "Siqi Li", "Xinhu Zheng", "Juan Wang", "Yue Gao", "Shaoyi Du"], "title": "SoftHGNN: Soft Hypergraph Neural Networks for General Visual Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Visual recognition relies on understanding both the semantics of image tokens\nand the complex interactions among them. Mainstream self-attention methods,\nwhile effective at modeling global pair-wise relations, fail to capture\nhigh-order associations inherent in real-world scenes and often suffer from\nredundant computation. Hypergraphs extend conventional graphs by modeling\nhigh-order interactions and offer a promising framework for addressing these\nlimitations. However, existing hypergraph neural networks typically rely on\nstatic and hard hyperedge assignments, leading to excessive and redundant\nhyperedges with hard binary vertex memberships that overlook the continuity of\nvisual semantics. To overcome these issues, we present Soft Hypergraph Neural\nNetworks (SoftHGNNs), which extend the methodology of hypergraph computation,\nto make it truly efficient and versatile in visual recognition tasks. Our\nframework introduces the concept of soft hyperedges, where each vertex is\nassociated with hyperedges via continuous participation weights rather than\nhard binary assignments. This dynamic and differentiable association is\nachieved by using the learnable hyperedge prototype. Through similarity\nmeasurements between token features and the prototype, the model generates\nsemantically rich soft hyperedges. SoftHGNN then aggregates messages over soft\nhyperedges to capture high-order semantics. To further enhance efficiency when\nscaling up the number of soft hyperedges, we incorporate a sparse hyperedge\nselection mechanism that activates only the top-k important hyperedges, along\nwith a load-balancing regularizer to ensure balanced hyperedge utilization.\nExperimental results across three tasks on five datasets demonstrate that\nSoftHGNN efficiently captures high-order associations in visual scenes,\nachieving significant performance improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSoftHGNN\u7684\u8f6f\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u52a8\u6001\u548c\u53ef\u5fae\u7684\u8d85\u8fb9\u5206\u914d\u673a\u5236\u6539\u8fdb\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u9ad8\u9636\u5173\u8054\u5efa\u6a21\u3002", "motivation": "\u4e3b\u6d41\u81ea\u6ce8\u610f\u529b\u65b9\u6cd5\u5728\u5efa\u6a21\u5168\u5c40\u6210\u5bf9\u5173\u7cfb\u65f6\u6709\u6548\uff0c\u4f46\u65e0\u6cd5\u6355\u6349\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9ad8\u9636\u5173\u8054\u4e14\u8ba1\u7b97\u5197\u4f59\u3002\u73b0\u6709\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u4f9d\u8d56\u9759\u6001\u786c\u8d85\u8fb9\u5206\u914d\uff0c\u5bfc\u81f4\u5197\u4f59\u4e14\u5ffd\u7565\u89c6\u89c9\u8bed\u4e49\u8fde\u7eed\u6027\u3002", "method": "\u5f15\u5165\u8f6f\u8d85\u8fb9\u6982\u5ff5\uff0c\u901a\u8fc7\u8fde\u7eed\u53c2\u4e0e\u6743\u91cd\u5173\u8054\u9876\u70b9\u4e0e\u8d85\u8fb9\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u7684\u8d85\u8fb9\u539f\u578b\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u8f6f\u8d85\u8fb9\uff0c\u5e76\u91c7\u7528\u7a00\u758f\u8d85\u8fb9\u9009\u62e9\u673a\u5236\u548c\u8d1f\u8f7d\u5747\u8861\u6b63\u5219\u5316\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u7684\u4e09\u4e2a\u4efb\u52a1\u4e0a\uff0cSoftHGNN\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9ad8\u6548\u6355\u6349\u4e86\u89c6\u89c9\u573a\u666f\u4e2d\u7684\u9ad8\u9636\u5173\u8054\u3002", "conclusion": "SoftHGNN\u901a\u8fc7\u8f6f\u8d85\u8fb9\u548c\u52a8\u6001\u5206\u914d\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.15332", "pdf": "https://arxiv.org/pdf/2505.15332", "abs": "https://arxiv.org/abs/2505.15332", "authors": ["Ria Shekhawat", "Hailin Li", "Raghavendra Ramachandra", "Sushma Venkatesh"], "title": "Towards Zero-Shot Differential Morphing Attack Detection with Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2025)", "summary": "Leveraging the power of multimodal large language models (LLMs) offers a\npromising approach to enhancing the accuracy and interpretability of morphing\nattack detection (MAD), especially in real-world biometric applications. This\nwork introduces the use of LLMs for differential morphing attack detection\n(D-MAD). To the best of our knowledge, this is the first study to employ\nmultimodal LLMs to D-MAD using real biometric data. To effectively utilize\nthese models, we design Chain-of-Thought (CoT)-based prompts to reduce\nfailure-to-answer rates and enhance the reasoning behind decisions. Our\ncontributions include: (1) the first application of multimodal LLMs for D-MAD\nusing real data subjects, (2) CoT-based prompt engineering to improve response\nreliability and explainability, (3) comprehensive qualitative and quantitative\nbenchmarking of LLM performance using data from 54 individuals captured in\npassport enrollment scenarios, and (4) comparative analysis of two multimodal\nLLMs: ChatGPT-4o and Gemini providing insights into their morphing attack\ndetection accuracy and decision transparency. Experimental results show that\nChatGPT-4o outperforms Gemini in detection accuracy, especially against\nGAN-based morphs, though both models struggle under challenging conditions.\nWhile Gemini offers more consistent explanations, ChatGPT-4o is more resilient\nbut prone to a higher failure-to-answer rate.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u4e8e\u5dee\u5206\u53d8\u5f62\u653b\u51fb\u68c0\u6d4b\uff08D-MAD\uff09\uff0c\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8e\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u63d0\u793a\u63d0\u9ad8\u51b3\u7b56\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cChatGPT-4o\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8eGemini\uff0c\u4f46\u4e24\u8005\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5229\u7528\u591a\u6a21\u6001LLMs\u63d0\u5347\u53d8\u5f62\u653b\u51fb\u68c0\u6d4b\uff08MAD\uff09\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u662f\u5728\u73b0\u5b9e\u751f\u7269\u8bc6\u522b\u5e94\u7528\u4e2d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u63d0\u793a\u8bbe\u8ba1\uff0c\u51cf\u5c11\u65e0\u5e94\u7b54\u7387\u5e76\u589e\u5f3a\u51b3\u7b56\u63a8\u7406\u3002\u7814\u7a76\u8fd8\u5bf9\u6bd4\u4e86ChatGPT-4o\u548cGemini\u4e24\u79cd\u591a\u6a21\u6001LLMs\u7684\u6027\u80fd\u3002", "result": "ChatGPT-4o\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8eGemini\uff0c\u5c24\u5176\u5728\u5bf9\u6297GAN\u751f\u6210\u7684\u53d8\u5f62\u653b\u51fb\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4e24\u8005\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u4e0d\u4f73\u3002Gemini\u7684\u89e3\u91ca\u66f4\u4e00\u81f4\uff0c\u800cChatGPT-4o\u66f4\u7a33\u5065\u4f46\u65e0\u5e94\u7b54\u7387\u8f83\u9ad8\u3002", "conclusion": "\u591a\u6a21\u6001LLMs\u5728D-MAD\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5e94\u5bf9\u590d\u6742\u6761\u4ef6\u3002ChatGPT-4o\u548cGemini\u5404\u6709\u4f18\u52a3\uff0c\u672a\u6765\u7814\u7a76\u53ef\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002"}}
{"id": "2505.15334", "pdf": "https://arxiv.org/pdf/2505.15334", "abs": "https://arxiv.org/abs/2505.15334", "authors": ["Bernardin Ligan", "Khalide Jbilou", "Fahd Kalloubi", "Ahmed Ratnani"], "title": "Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": "33 pages, 14 figures", "summary": "Foundation models have achieved great success across diverse domains,\nincluding remote sensing (RS), thanks to their versatility and strong\ngeneralization abilities. However, most RS foundation models are designed for\nmultispectral data, while hyperspectral imagery (HSI) - with its hundreds of\nspectral bands - remains less explored. Fine-tuning such models for downstream\ntasks is also challenging, often demanding considerable memory and storage. In\nthis paper, we propose an efficient framework to fine-tune SpectralGPT, a\nmultispectral foundation model, for hyperspectral image classification (HSIC).\nWe explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including\nLow-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank\nKronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for\nlow-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce\nKronA+, which applies a similar mechanism to the Kronecker matrices. We\nevaluate our approach on five datasets from different sensors, showing\ncompetitive performance with state-of-the-art HSI models. Our full fine-tuning\n(FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral\nfoundation model on some datasets while requiring only a quarter of the\ntraining epochs. Under the same number of epochs, KronA+ reaches similar\nperformance with far fewer trainable parameters - just 0.056 percent - and adds\nonly approximately 0.2 megabytes of storage, making it the most effective PEFT\nmethod tested.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5fae\u8c03\u591a\u5149\u8c31\u57fa\u7840\u6a21\u578bSpectralGPT\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff0c\u5e76\u5f15\u5165KronA+\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u548c\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u5728\u591a\u5149\u8c31\u57fa\u7840\u6a21\u578b\u4e2d\u7814\u7a76\u8f83\u5c11\uff0c\u4e14\u5fae\u8c03\u8fc7\u7a0b\u901a\u5e38\u9700\u8981\u5927\u91cf\u5185\u5b58\u548c\u5b58\u50a8\u8d44\u6e90\u3002", "method": "\u63a2\u7d22\u4e86\u591a\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u5305\u62ecLoRA\u3001KronA\u3001LoKr\u548cLoRA+\uff0c\u5e76\u63d0\u51fa\u4e86KronA+\u65b9\u6cd5\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cKronA+\u4ec5\u97000.056%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u548c\u7ea60.2MB\u5b58\u50a8\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u5fae\u8c03\u3002", "conclusion": "KronA+\u662f\u6700\u9ad8\u6548\u7684PEFT\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2505.15336", "pdf": "https://arxiv.org/pdf/2505.15336", "abs": "https://arxiv.org/abs/2505.15336", "authors": ["Hon Ming Yam", "Zhongliang Guo", "Chun Pong Lau"], "title": "My Face Is Mine, Not Yours: Facial Protection Against Diffusion Model Face Swapping", "categories": ["cs.CV"], "comment": null, "summary": "The proliferation of diffusion-based deepfake technologies poses significant\nrisks for unauthorized and unethical facial image manipulation. While\ntraditional countermeasures have primarily focused on passive detection\nmethods, this paper introduces a novel proactive defense strategy through\nadversarial attacks that preemptively protect facial images from being\nexploited by diffusion-based deepfake systems. Existing adversarial protection\nmethods predominantly target conventional generative architectures (GANs, AEs,\nVAEs) and fail to address the unique challenges presented by diffusion models,\nwhich have become the predominant framework for high-quality facial deepfakes.\nCurrent diffusion-specific adversarial approaches are limited by their reliance\non specific model architectures and weights, rendering them ineffective against\nthe diverse landscape of diffusion-based deepfake implementations.\nAdditionally, they typically employ global perturbation strategies that\ninadequately address the region-specific nature of facial manipulation in\ndeepfakes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u4e3b\u52a8\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u5bf9\u6297\u653b\u51fb\u9884\u5148\u4fdd\u62a4\u9762\u90e8\u56fe\u50cf\uff0c\u907f\u514d\u88ab\u6269\u6563\u5f0f\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5229\u7528\u3002", "motivation": "\u6269\u6563\u5f0f\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u666e\u53ca\u5e26\u6765\u4e86\u672a\u7ecf\u6388\u6743\u548c\u4e0d\u9053\u5fb7\u7684\u9762\u90e8\u56fe\u50cf\u64cd\u7eb5\u98ce\u9669\uff0c\u4f20\u7edf\u88ab\u52a8\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u6269\u6563\u6a21\u578b\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u653b\u51fb\u7b56\u7565\uff0c\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u7279\u70b9\u8bbe\u8ba1\u533a\u57df\u7279\u5f02\u6027\u6270\u52a8\uff0c\u800c\u975e\u4f9d\u8d56\u7279\u5b9a\u6a21\u578b\u67b6\u6784\u6216\u5168\u5c40\u6270\u52a8\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u62a4\u9762\u90e8\u56fe\u50cf\uff0c\u907f\u514d\u88ab\u591a\u6837\u5316\u7684\u6269\u6563\u5f0f\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5229\u7528\u3002", "conclusion": "\u4e3b\u52a8\u9632\u5fa1\u7b56\u7565\u662f\u5e94\u5bf9\u6269\u6563\u5f0f\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u6709\u6548\u624b\u6bb5\uff0c\u533a\u57df\u7279\u5f02\u6027\u6270\u52a8\u8bbe\u8ba1\u662f\u5173\u952e\u3002"}}
{"id": "2505.15358", "pdf": "https://arxiv.org/pdf/2505.15358", "abs": "https://arxiv.org/abs/2505.15358", "authors": ["Angelique Mangubat", "Shane Gilroy"], "title": "Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Road safety is a critical challenge, particularly for cyclists, who are among\nthe most vulnerable road users. This study aims to enhance road safety by\nproposing a novel benchmark for bicycle occlusion level classification using\nadvanced computer vision techniques. Utilizing a parts-based detection model,\nimages are annotated and processed through a custom image detection pipeline. A\nnovel method of bicycle occlusion level is proposed to objectively quantify the\nvisibility and occlusion level of bicycle semantic parts. The findings indicate\nthat the model robustly quantifies the visibility and occlusion level of\nbicycles, a significant improvement over the subjective methods used by the\ncurrent state of the art. Widespread use of the proposed methodology will\nfacilitate the accurate performance reporting of cyclist detection algorithms\nfor occluded cyclists, informing the development of more robust vulnerable road\nuser detection methods for autonomous vehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u81ea\u884c\u8f66\u906e\u6321\u7b49\u7ea7\u5206\u7c7b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u884c\u8f66\u53ef\u89c1\u6027\u548c\u906e\u6321\u7684\u91cf\u5316\u51c6\u786e\u6027\u3002", "motivation": "\u63d0\u5347\u81ea\u884c\u8f66\u9a91\u884c\u8005\u7684\u9053\u8def\u5b89\u5168\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u81ea\u884c\u8f66\u906e\u6321\u60c5\u51b5\u4e3b\u89c2\u8bc4\u4f30\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u90e8\u4ef6\u7684\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u56fe\u50cf\u68c0\u6d4b\u6d41\u7a0b\u5904\u7406\u6807\u6ce8\u56fe\u50cf\uff0c\u63d0\u51fa\u81ea\u884c\u8f66\u906e\u6321\u7b49\u7ea7\u91cf\u5316\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u80fd\u7a33\u5065\u91cf\u5316\u81ea\u884c\u8f66\u7684\u53ef\u89c1\u6027\u548c\u906e\u6321\u7b49\u7ea7\uff0c\u4f18\u4e8e\u73b0\u6709\u4e3b\u89c2\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u4fc3\u8fdb\u81ea\u52a8\u9a7e\u9a76\u4e2d\u66f4\u9c81\u68d2\u7684\u5f31\u52bf\u9053\u8def\u7528\u6237\u68c0\u6d4b\u7b97\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.15367", "pdf": "https://arxiv.org/pdf/2505.15367", "abs": "https://arxiv.org/abs/2505.15367", "authors": ["Dasol Choi", "Seunghyun Lee", "Youngsook Song"], "title": "Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "13 pages", "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nunderstanding visual content, but their reliability in safety-critical contexts\nremains under-explored. We introduce VERI (Visual Emergency Recognition\nDataset), a carefully designed diagnostic benchmark of 200 images (100\ncontrastive pairs). Each emergency scene is matched with a visually similar but\nsafe counterpart through multi-stage human verification and iterative\nrefinement. Using a two-stage protocol - risk identification and emergency\nresponse - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,\naccidents, and natural disasters. Our analysis reveals a systematic\noverreaction problem: models excel at identifying real emergencies (70-100\npercent success rate) but suffer from an alarming rate of false alarms,\nmisidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios\nfailed by all models regardless of scale. This \"better-safe-than-sorry\" bias\nmanifests primarily through contextual overinterpretation (88-93 percent of\nerrors), challenging VLMs' reliability for safety applications. These findings\nhighlight persistent limitations that are not resolved by increasing model\nscale, motivating targeted approaches for improving contextual safety\nassessment in visually misleading scenarios.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u8fc7\u5ea6\u53cd\u5e94\u95ee\u9898\uff0c\u5373\u5bf9\u5b89\u5168\u573a\u666f\u8bef\u5224\u4e3a\u5371\u9669\u3002", "motivation": "\u63a2\u7d22VLMs\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u63ed\u793a\u5176\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u63d0\u4f9b\u65b9\u5411\u3002", "method": "\u901a\u8fc7VERI\u6570\u636e\u96c6\uff08200\u5f20\u56fe\u50cf\uff0c100\u5bf9\u5bf9\u6bd4\u56fe\u50cf\uff09\u548c\u4e24\u9636\u6bb5\u8bc4\u4f30\u534f\u8bae\uff08\u98ce\u9669\u8bc6\u522b\u548c\u5e94\u6025\u54cd\u5e94\uff09\uff0c\u8bc4\u4f3014\u4e2aVLMs\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u8bc6\u522b\u771f\u5b9e\u7d27\u6025\u60c5\u51b5\u65f6\u8868\u73b0\u826f\u597d\uff0870-100%\u6210\u529f\u7387\uff09\uff0c\u4f46\u5bf9\u5b89\u5168\u573a\u666f\u8bef\u5224\u7387\u9ad8\u8fbe31-96%\uff0c\u4e1410\u79cd\u573a\u666f\u6240\u6709\u6a21\u578b\u5747\u5931\u8d25\u3002", "conclusion": "\u6a21\u578b\u89c4\u6a21\u7684\u589e\u52a0\u65e0\u6cd5\u89e3\u51b3\u5176\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9700\u9488\u5bf9\u6027\u6539\u8fdb\u5bf9\u89c6\u89c9\u8bef\u5bfc\u573a\u666f\u7684\u5b89\u5168\u8bc4\u4f30\u3002"}}
{"id": "2505.15373", "pdf": "https://arxiv.org/pdf/2505.15373", "abs": "https://arxiv.org/abs/2505.15373", "authors": ["Naman Patel", "Prashanth Krishnamurthy", "Farshad Khorrami"], "title": "RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction with Spatio-Temporal Aggregation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Mapping and understanding complex 3D environments is fundamental to how\nautonomous systems perceive and interact with the physical world, requiring\nboth precise geometric reconstruction and rich semantic comprehension. While\nexisting 3D semantic mapping systems excel at reconstructing and identifying\npredefined object instances, they lack the flexibility to efficiently build\nsemantic maps with open-vocabulary during online operation. Although recent\nvision-language models have enabled open-vocabulary object recognition in 2D\nimages, they haven't yet bridged the gap to 3D spatial understanding. The\ncritical challenge lies in developing a training-free unified system that can\nsimultaneously construct accurate 3D maps while maintaining semantic\nconsistency and supporting natural language interactions in real time. In this\npaper, we develop a zero-shot framework that seamlessly integrates\nGPU-accelerated geometric reconstruction with open-vocabulary vision-language\nmodels through online instance-level semantic embedding fusion, guided by\nhierarchical object association with spatial indexing. Our training-free system\nachieves superior performance through incremental processing and unified\ngeometric-semantic updates, while robustly handling 2D segmentation\ninconsistencies. The proposed general-purpose 3D scene understanding framework\ncan be used for various tasks including zero-shot 3D instance retrieval,\nsegmentation, and object detection to reason about previously unseen objects\nand interpret natural language queries. The project page is available at\nhttps://razer-3d.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6846\u67b6\uff0c\u7ed3\u5408GPU\u52a0\u901f\u7684\u51e0\u4f55\u91cd\u5efa\u4e0e\u5f00\u653e\u8bcd\u6c47\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u5b9e\u65f63D\u8bed\u4e49\u5730\u56fe\u6784\u5efa\u4e0e\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3002", "motivation": "\u73b0\u67093D\u8bed\u4e49\u5730\u56fe\u7cfb\u7edf\u7f3a\u4e4f\u5728\u7ebf\u64cd\u4f5c\u4e2d\u6784\u5efa\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5730\u56fe\u7684\u7075\u6d3b\u6027\uff0c\u4e14\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c1a\u672a\u89e3\u51b33D\u7a7a\u95f4\u7406\u89e3\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u5d4c\u5165\u878d\u5408\u548c\u5206\u5c42\u5bf9\u8c61\u5173\u8054\uff0c\u7ed3\u5408GPU\u52a0\u901f\u51e0\u4f55\u91cd\u5efa\u4e0e\u5f00\u653e\u8bcd\u6c47\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u5728\u96f6\u6837\u672c3D\u5b9e\u4f8b\u68c0\u7d22\u3001\u5206\u5272\u548c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u5bf9\u672a\u89c1\u7269\u4f53\u7684\u63a8\u7406\u548c\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u89e3\u91ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u901a\u75283D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u8bad\u7ec3\u514d\u8d39\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2505.15379", "pdf": "https://arxiv.org/pdf/2505.15379", "abs": "https://arxiv.org/abs/2505.15379", "authors": ["Raphael Sulzer", "Liuyun Duan", "Nicolas Girard", "Florent Lafarge"], "title": "The P$^3$ dataset: Pixels, Points and Polygons for Multimodal Building Vectorization", "categories": ["cs.CV"], "comment": null, "summary": "We present the P$^3$ dataset, a large-scale multimodal benchmark for building\nvectorization, constructed from aerial LiDAR point clouds, high-resolution\naerial imagery, and vectorized 2D building outlines, collected across three\ncontinents. The dataset contains over 10 billion LiDAR points with\ndecimeter-level accuracy and RGB images at a ground sampling distance of 25\ncentimeter. While many existing datasets primarily focus on the image modality,\nP$^3$ offers a complementary perspective by also incorporating dense 3D\ninformation. We demonstrate that LiDAR point clouds serve as a robust modality\nfor predicting building polygons, both in hybrid and end-to-end learning\nframeworks. Moreover, fusing aerial LiDAR and imagery further improves accuracy\nand geometric quality of predicted polygons. The P$^3$ dataset is publicly\navailable, along with code and pretrained weights of three state-of-the-art\nmodels for building polygon prediction at\nhttps://github.com/raphaelsulzer/PixelsPointsPolygons .", "AI": {"tldr": "P$^3$\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5efa\u7b51\u7269\u77e2\u91cf\u5316\uff0c\u7ed3\u5408\u4e86LiDAR\u70b9\u4e91\u3001\u9ad8\u5206\u8fa8\u7387\u822a\u7a7a\u5f71\u50cf\u548c\u77e2\u91cf\u5316\u76842D\u5efa\u7b51\u7269\u8f6e\u5ed3\uff0c\u8986\u76d6\u4e09\u5927\u6d32\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u6a21\u6001\uff0c\u800cP$^3$\u901a\u8fc7\u5f15\u5165\u5bc6\u96c6\u76843D\u4fe1\u606f\u63d0\u4f9b\u4e92\u8865\u89c6\u89d2\uff0c\u65e8\u5728\u63d0\u5347\u5efa\u7b51\u7269\u591a\u8fb9\u5f62\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u51e0\u4f55\u8d28\u91cf\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc7100\u4ebf\u4e2aLiDAR\u70b9\u548c25\u5398\u7c73\u5206\u8fa8\u7387\u7684RGB\u56fe\u50cf\uff0c\u5c55\u793a\u4e86LiDAR\u70b9\u4e91\u5728\u6df7\u5408\u548c\u7aef\u5230\u7aef\u5b66\u4e60\u6846\u67b6\u4e2d\u9884\u6d4b\u5efa\u7b51\u7269\u591a\u8fb9\u5f62\u7684\u6709\u6548\u6027\uff0c\u5e76\u878d\u5408\u822a\u7a7aLiDAR\u548c\u5f71\u50cf\u8fdb\u4e00\u6b65\u63d0\u5347\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLiDAR\u70b9\u4e91\u662f\u9884\u6d4b\u5efa\u7b51\u7269\u591a\u8fb9\u5f62\u7684\u7a33\u5065\u6a21\u6001\uff0c\u4e14\u591a\u6a21\u6001\u878d\u5408\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u51e0\u4f55\u8d28\u91cf\u3002", "conclusion": "P$^3$\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e09\u79cd\u5148\u8fdb\u6a21\u578b\u7684\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u4e3a\u5efa\u7b51\u7269\u591a\u8fb9\u5f62\u9884\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2505.15385", "pdf": "https://arxiv.org/pdf/2505.15385", "abs": "https://arxiv.org/abs/2505.15385", "authors": ["Hendrik Junkawitsch", "Guoxing Sun", "Heming Zhu", "Christian Theobalt", "Marc Habermann"], "title": "EVA: Expressive Virtual Avatars from Multi-view Videos", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted at SIGGRAPH 2025 Conference Track, Project page:\n  https://vcai.mpi-inf.mpg.de/projects/EVA/", "summary": "With recent advancements in neural rendering and motion capture algorithms,\nremarkable progress has been made in photorealistic human avatar modeling,\nunlocking immense potential for applications in virtual reality, augmented\nreality, remote communication, and industries such as gaming, film, and\nmedicine. However, existing methods fail to provide complete, faithful, and\nexpressive control over human avatars due to their entangled representation of\nfacial expressions and body movements. In this work, we introduce Expressive\nVirtual Avatars (EVA), an actor-specific, fully controllable, and expressive\nhuman avatar framework that achieves high-fidelity, lifelike renderings in real\ntime while enabling independent control of facial expressions, body movements,\nand hand gestures. Specifically, our approach designs the human avatar as a\ntwo-layer model: an expressive template geometry layer and a 3D Gaussian\nappearance layer. First, we present an expressive template tracking algorithm\nthat leverages coarse-to-fine optimization to accurately recover body motions,\nfacial expressions, and non-rigid deformation parameters from multi-view\nvideos. Next, we propose a novel decoupled 3D Gaussian appearance model\ndesigned to effectively disentangle body and facial appearance. Unlike unified\nGaussian estimation approaches, our method employs two specialized and\nindependent modules to model the body and face separately. Experimental results\ndemonstrate that EVA surpasses state-of-the-art methods in terms of rendering\nquality and expressiveness, validating its effectiveness in creating full-body\navatars. This work represents a significant advancement towards fully drivable\ndigital human models, enabling the creation of lifelike digital avatars that\nfaithfully replicate human geometry and appearance.", "AI": {"tldr": "EVA\u6846\u67b6\u901a\u8fc7\u53cc\u5c42\u6a21\u578b\uff08\u51e0\u4f55\u5c42\u4e0e\u5916\u89c2\u5c42\uff09\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u5b9e\u65f6\u6e32\u67d3\u7684\u865a\u62df\u5316\u8eab\uff0c\u72ec\u7acb\u63a7\u5236\u9762\u90e8\u8868\u60c5\u3001\u8eab\u4f53\u52a8\u4f5c\u548c\u624b\u52bf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u72ec\u7acb\u63a7\u5236\u9762\u90e8\u8868\u60c5\u4e0e\u8eab\u4f53\u52a8\u4f5c\uff0c\u9650\u5236\u4e86\u865a\u62df\u5316\u8eab\u7684\u8868\u8fbe\u6027\u548c\u903c\u771f\u5ea6\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u6a21\u578b\uff1a\u51e0\u4f55\u5c42\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u6062\u590d\u52a8\u4f5c\u53c2\u6570\uff0c\u5916\u89c2\u5c42\u901a\u8fc7\u89e3\u8026\u76843D\u9ad8\u65af\u6a21\u578b\u5206\u522b\u5efa\u6a21\u8eab\u4f53\u548c\u9762\u90e8\u3002", "result": "EVA\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u8868\u8fbe\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "EVA\u4e3a\u53ef\u9a71\u52a8\u7684\u6570\u5b57\u4eba\u4f53\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5b9e\u73b0\u4e86\u903c\u771f\u7684\u51e0\u4f55\u4e0e\u5916\u89c2\u590d\u5236\u3002"}}
{"id": "2505.15398", "pdf": "https://arxiv.org/pdf/2505.15398", "abs": "https://arxiv.org/abs/2505.15398", "authors": ["Huilin Zhu", "Senyao Li", "Jingling Yuan", "Zhengwei Yang", "Yu Guo", "Wenxuan Liu", "Xian Zhong", "Shengfeng He"], "title": "Expanding Zero-Shot Object Counting with Rich Prompts", "categories": ["cs.CV"], "comment": null, "summary": "Expanding pre-trained zero-shot counting models to handle unseen categories\nrequires more than simply adding new prompts, as this approach does not achieve\nthe necessary alignment between text and visual features for accurate counting.\nWe introduce RichCount, the first framework to address these limitations,\nemploying a two-stage training strategy that enhances text encoding and\nstrengthens the model's association with objects in images. RichCount improves\nzero-shot counting for unseen categories through two key objectives: (1)\nenriching text features with a feed-forward network and adapter trained on\ntext-image similarity, thereby creating robust, aligned representations; and\n(2) applying this refined encoder to counting tasks, enabling effective\ngeneralization across diverse prompts and complex images. In this manner,\nRichCount goes beyond simple prompt expansion to establish meaningful feature\nalignment that supports accurate counting across novel categories. Extensive\nexperiments on three benchmark datasets demonstrate the effectiveness of\nRichCount, achieving state-of-the-art performance in zero-shot counting and\nsignificantly enhancing generalization to unseen categories in open-world\nscenarios.", "AI": {"tldr": "RichCount\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u6587\u672c\u7f16\u7801\u548c\u56fe\u50cf\u7279\u5f81\u5bf9\u9f50\uff0c\u63d0\u5347\u96f6\u6837\u672c\u8ba1\u6570\u6a21\u578b\u5bf9\u65b0\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u901a\u8fc7\u6dfb\u52a0\u65b0\u63d0\u793a\u65e0\u6cd5\u5b9e\u73b0\u6587\u672c\u4e0e\u89c6\u89c9\u7279\u5f81\u7684\u6709\u6548\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u96f6\u6837\u672c\u8ba1\u6570\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "RichCount\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u524d\u9988\u7f51\u7edc\u548c\u9002\u914d\u5668\u4e30\u5bcc\u6587\u672c\u7279\u5f81\uff1b2\uff09\u5c06\u4f18\u5316\u540e\u7684\u7f16\u7801\u5668\u5e94\u7528\u4e8e\u8ba1\u6570\u4efb\u52a1\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cRichCount\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8ba1\u6570\u7684SOTA\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b0\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RichCount\u901a\u8fc7\u7279\u5f81\u5bf9\u9f50\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u8ba1\u6570\u6a21\u578b\u5bf9\u65b0\u7c7b\u522b\u7684\u9002\u5e94\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.15401", "pdf": "https://arxiv.org/pdf/2505.15401", "abs": "https://arxiv.org/abs/2505.15401", "authors": ["Hichem Boussaid", "Lucrezia Tosato", "Flora Weissgerber", "Camille Kurtz", "Laurent Wendling", "Sylvain Lobry"], "title": "Visual Question Answering on Multiple Remote Sensing Image Modalities", "categories": ["cs.CV"], "comment": "EARTHVISION 2025 8 pages, 1 page of supplementary material, 4 figures", "summary": "The extraction of visual features is an essential step in Visual Question\nAnswering (VQA). Building a good visual representation of the analyzed scene is\nindeed one of the essential keys for the system to be able to correctly\nunderstand the latter in order to answer complex questions. In many fields such\nas remote sensing, the visual feature extraction step could benefit\nsignificantly from leveraging different image modalities carrying complementary\nspectral, spatial and contextual information. In this work, we propose to add\nmultiple image modalities to VQA in the particular context of remote sensing,\nleading to a novel task for the computer vision community. To this end, we\nintroduce a new VQA dataset, named TAMMI (Text and Multi-Modal Imagery) with\ndiverse questions on scenes described by three different modalities (very high\nresolution RGB, multi-spectral imaging data and synthetic aperture radar).\nThanks to an automated pipeline, this dataset can be easily extended according\nto experimental needs. We also propose the MM-RSVQA (Multi-modal\nMulti-resolution Remote Sensing Visual Question Answering) model, based on\nVisualBERT, a vision-language transformer, to effectively combine the multiple\nimage modalities and text through a trainable fusion process. A preliminary\nexperimental study shows promising results of our methodology on this\nchallenging dataset, with an accuracy of 65.56% on the targeted VQA task. This\npioneering work paves the way for the community to a new multi-modal\nmulti-resolution VQA task that can be applied in other imaging domains (such as\nmedical imaging) where multi-modality can enrich the visual representation of a\nscene. The dataset and code are available at https://tammi.sylvainlobry.com/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u591a\u5206\u8fa8\u7387\u9065\u611f\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6TAMMI\u548c\u6a21\u578bMM-RSVQA\uff0c\u521d\u6b65\u5b9e\u9a8c\u663e\u793a65.56%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u662fVQA\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u591a\u6a21\u6001\u56fe\u50cf\uff08\u5982RGB\u3001\u591a\u5149\u8c31\u548c\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff09\u80fd\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\uff0c\u63d0\u5347\u9065\u611f\u573a\u666f\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faTAMMI\u6570\u636e\u96c6\u548c\u57fa\u4e8eVisualBERT\u7684MM-RSVQA\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u878d\u5408\u8fc7\u7a0b\u7ed3\u5408\u591a\u6a21\u6001\u56fe\u50cf\u548c\u6587\u672c\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u5728TAMMI\u6570\u636e\u96c6\u4e0a\u8fbe\u523065.56%\u7684\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u591a\u5206\u8fa8\u7387VQA\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u591a\u6a21\u6001\u6210\u50cf\u9886\u57df\uff08\u5982\u533b\u5b66\u5f71\u50cf\uff09\u3002"}}
{"id": "2505.15408", "pdf": "https://arxiv.org/pdf/2505.15408", "abs": "https://arxiv.org/abs/2505.15408", "authors": ["Patrik Reiske", "Marcus N. Boon", "Niek Andresen", "Sole Traverso", "Katharina Hohlbaum", "Lars Lewejohann", "Christa Th\u00f6ne-Reineke", "Olaf Hellwich", "Henning Sprekeler"], "title": "Mouse Lockbox Dataset: Behavior Recognition for Mice Solving Lockboxes", "categories": ["cs.CV"], "comment": null, "summary": "Machine learning and computer vision methods have a major impact on the study\nof natural animal behavior, as they enable the (semi-)automatic analysis of\nvast amounts of video data. Mice are the standard mammalian model system in\nmost research fields, but the datasets available today to refine such methods\nfocus either on simple or social behaviors. In this work, we present a video\ndataset of individual mice solving complex mechanical puzzles, so-called\nlockboxes. The more than 110 hours of total playtime show their behavior\nrecorded from three different perspectives. As a benchmark for frame-level\naction classification methods, we provide human-annotated labels for all videos\nof two different mice, that equal 13% of our dataset. Our keypoint (pose)\ntracking-based action classification framework illustrates the challenges of\nautomated labeling of fine-grained behaviors, such as the manipulation of\nobjects. We hope that our work will help accelerate the advancement of\nautomated action and behavior classification in the computational neuroscience\ncommunity. Our dataset is publicly available at\nhttps://doi.org/10.14279/depositonce-23850", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u5c0f\u9f20\u89e3\u51b3\u590d\u6742\u673a\u68b0\u8c1c\u9898\uff08\u9501\u76d2\uff09\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u8fdb\u884c\u4e3a\u5206\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u6216\u793e\u4ea4\u884c\u4e3a\uff0c\u7f3a\u4e4f\u590d\u6742\u884c\u4e3a\u7684\u7814\u7a76\u6570\u636e\u3002", "method": "\u63d0\u4f9b\u8d85\u8fc7110\u5c0f\u65f6\u7684\u89c6\u9891\u6570\u636e\uff0c\u5305\u542b\u4e09\u79cd\u89c6\u89d2\uff0c\u5e76\u6807\u6ce8\u4e8613%\u7684\u6570\u636e\u4f5c\u4e3a\u57fa\u51c6\u3002", "result": "\u5c55\u793a\u4e86\u57fa\u4e8e\u5173\u952e\u70b9\u8ddf\u8e2a\u7684\u884c\u4e3a\u5206\u7c7b\u6846\u67b6\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7cbe\u7ec6\u884c\u4e3a\uff08\u5982\u7269\u4f53\u64cd\u4f5c\uff09\u7684\u81ea\u52a8\u6807\u6ce8\u4e0a\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u6709\u671b\u52a0\u901f\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u4e2d\u884c\u4e3a\u548c\u52a8\u4f5c\u5206\u7c7b\u7684\u81ea\u52a8\u5316\u7814\u7a76\u3002"}}
{"id": "2505.15414", "pdf": "https://arxiv.org/pdf/2505.15414", "abs": "https://arxiv.org/abs/2505.15414", "authors": ["Uranik Berisha", "Jens Mehnert", "Alexandru Paul Condurache"], "title": "Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks", "categories": ["cs.CV"], "comment": "Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025", "summary": "Vision Transformers have emerged as the state-of-the-art models in various\nComputer Vision tasks, but their high computational and resource demands pose\nsignificant challenges. While Mixture-of-Experts (MoE) can make these models\nmore efficient, they often require costly retraining or even training from\nscratch. Recent developments aim to reduce these computational costs by\nleveraging pretrained networks. These have been shown to produce sparse\nactivation patterns in the Multi-Layer Perceptrons (MLPs) of the encoder\nblocks, allowing for conditional activation of only relevant subnetworks for\neach sample. Building on this idea, we propose a new method to construct MoE\nvariants from pretrained models. Our approach extracts expert subnetworks from\nthe model's MLP layers post-training in two phases. First, we cluster output\nactivations to identify distinct activation patterns. In the second phase, we\nuse these clusters to extract the corresponding subnetworks responsible for\nproducing them. On ImageNet-1k recognition tasks, we demonstrate that these\nextracted experts can perform surprisingly well out of the box and require only\nminimal fine-tuning to regain 98% of the original performance, all while\nreducing MACs and model size, by up to 36% and 32% respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u6784\u5efaMoE\u53d8\u4f53\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u6fc0\u6d3b\u6a21\u5f0f\u63d0\u53d6\u4e13\u5bb6\u5b50\u7f51\u7edc\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u6a21\u578b\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3Vision Transformers\u9ad8\u8ba1\u7b97\u548c\u8d44\u6e90\u9700\u6c42\u7684\u95ee\u9898\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u51cf\u5c11\u91cd\u65b0\u8bad\u7ec3\u6216\u4ece\u5934\u8bad\u7ec3\u7684\u6210\u672c\u3002", "method": "\u5206\u4e24\u9636\u6bb5\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u7684MLP\u5c42\u63d0\u53d6\u4e13\u5bb6\u5b50\u7f51\u7edc\uff1a\u805a\u7c7b\u8f93\u51fa\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u518d\u63d0\u53d6\u5bf9\u5e94\u7684\u5b50\u7f51\u7edc\u3002", "result": "\u5728ImageNet-1k\u4efb\u52a1\u4e2d\uff0c\u63d0\u53d6\u7684\u4e13\u5bb6\u5b50\u7f51\u7edc\u4ec5\u9700\u5c11\u91cf\u5fae\u8c03\u5373\u53ef\u6062\u590d98%\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c1136% MACs\u548c32%\u6a21\u578b\u5927\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86Vision Transformers\u7684\u8ba1\u7b97\u548c\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2505.15425", "pdf": "https://arxiv.org/pdf/2505.15425", "abs": "https://arxiv.org/abs/2505.15425", "authors": ["Raza Imam", "Rufael Marew", "Mohammad Yaqub"], "title": "On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?", "categories": ["cs.CV"], "comment": "Dataset and Code is available at\n  https://github.com/BioMedIA-MBZUAI/RobustMedCLIP Accepted at: Medical Image\n  Understanding and Analysis (MIUA) 2025", "summary": "Medical Vision-Language Models (MVLMs) have achieved par excellence\ngeneralization in medical image analysis, yet their performance under noisy,\ncorrupted conditions remains largely untested. Clinical imaging is inherently\nsusceptible to acquisition artifacts and noise; however, existing evaluations\npredominantly assess generally clean datasets, overlooking robustness -- i.e.,\nthe model's ability to perform under real-world distortions. To address this\ngap, we first introduce MediMeta-C, a corruption benchmark that systematically\napplies several perturbations across multiple medical imaging datasets.\nCombined with MedMNIST-C, this establishes a comprehensive robustness\nevaluation framework for MVLMs. We further propose RobustMedCLIP, a visual\nencoder adaptation of a pretrained MVLM that incorporates few-shot tuning to\nenhance resilience against corruptions. Through extensive experiments, we\nbenchmark 5 major MVLMs across 5 medical imaging modalities, revealing that\nexisting models exhibit severe degradation under corruption and struggle with\ndomain-modality tradeoffs. Our findings highlight the necessity of diverse\ntraining and robust adaptation strategies, demonstrating that efficient\nlow-rank adaptation when paired with few-shot tuning, improves robustness while\npreserving generalization across modalities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MediMeta-C\u548cMedMNIST-C\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08MVLMs\uff09\u5728\u566a\u58f0\u548c\u5931\u771f\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51faRobustMedCLIP\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5e72\u51c0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u771f\u5b9e\u4e34\u5e8a\u56fe\u50cf\u4e2d\u7684\u566a\u58f0\u548c\u5931\u771f\u6761\u4ef6\u4e0b\u6027\u80fd\u672a\u5f97\u5230\u5145\u5206\u6d4b\u8bd5\uff0c\u4e9f\u9700\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u5f15\u5165MediMeta-C\u548cMedMNIST-C\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51faRobustMedCLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709MVLMs\u5728\u5931\u771f\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0cRobustMedCLIP\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u548c\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6837\u5316\u8bad\u7ec3\u548c\u9c81\u68d2\u9002\u5e94\u7b56\u7565\u7684\u5fc5\u8981\u6027\uff0cRobustMedCLIP\u5728\u4fdd\u6301\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.15435", "pdf": "https://arxiv.org/pdf/2505.15435", "abs": "https://arxiv.org/abs/2505.15435", "authors": ["Zeqing Wang", "Shiyuan Zhang", "Chengpei Tang", "Keze Wang"], "title": "TimeCausality: Evaluating the Causal Ability in Time Dimension for Vision Language Models", "categories": ["cs.CV", "I.4.9"], "comment": "17 pages, 6 figures, 3 tables", "summary": "Reasoning about temporal causality, particularly irreversible transformations\nof objects governed by real-world knowledge (e.g., fruit decay and human\naging), is a fundamental aspect of human visual understanding. Unlike temporal\nperception based on simple event sequences, this form of reasoning requires a\ndeeper comprehension of how object states change over time. Although the\ncurrent powerful Vision-Language Models (VLMs) have demonstrated impressive\nperformance on a wide range of downstream tasks, their capacity to reason about\ntemporal causality remains underexplored. To address this gap, we introduce\n\\textbf{TimeCausality}, a novel benchmark specifically designed to evaluate the\ncausal reasoning ability of VLMs in the temporal dimension. Based on our\nTimeCausality, we find that while the current SOTA open-source VLMs have\nachieved performance levels comparable to closed-source models like GPT-4o on\nvarious standard visual question answering tasks, they fall significantly\nbehind on our benchmark compared with their closed-source competitors.\nFurthermore, even GPT-4o exhibits a marked drop in performance on TimeCausality\ncompared to its results on other tasks. These findings underscore the critical\nneed to incorporate temporal causality into the evaluation and development of\nVLMs, and they highlight an important challenge for the open-source VLM\ncommunity moving forward. Code and Data are available at\n\\href{https://github.com/Zeqing-Wang/TimeCausality }{TimeCausality}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTimeCausality\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u7406\u89e3\u4e2d\u65f6\u95f4\u56e0\u679c\u63a8\u7406\uff08\u5982\u7269\u4f53\u72b6\u6001\u53d8\u5316\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u9886\u57df\uff0c\u73b0\u6709VLMs\u5728\u6b64\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86TimeCausality\u57fa\u51c6\uff0c\u7528\u4e8e\u6d4b\u8bd5VLMs\u5728\u65f6\u95f4\u56e0\u679c\u63a8\u7406\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5f53\u524d\u5f00\u6e90VLMs\u5728TimeCausality\u4e0a\u8868\u73b0\u663e\u8457\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\uff09\uff0c\u4e14GPT-4o\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e5f\u660e\u663e\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06\u65f6\u95f4\u56e0\u679c\u63a8\u7406\u7eb3\u5165VLMs\u8bc4\u4f30\u548c\u5f00\u53d1\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u6307\u51fa\u5f00\u6e90VLM\u793e\u533a\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2505.15436", "pdf": "https://arxiv.org/pdf/2505.15436", "abs": "https://arxiv.org/abs/2505.15436", "authors": ["Xintong Zhang", "Zhi Gao", "Bofei Zhang", "Pengxiang Li", "Xiaowen Zhang", "Yang Liu", "Tao Yuan", "Yuwei Wu", "Yunde Jia", "Song-Chun Zhu", "Qing Li"], "title": "Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL", "categories": ["cs.CV"], "comment": null, "summary": "Vision language models (VLMs) have achieved impressive performance across a\nvariety of computer vision tasks. However, the multimodal reasoning capability\nhas not been fully explored in existing models. In this paper, we propose a\nChain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and\nzooming in on key image regions based on obtained visual cues and the given\nquestions, achieving efficient multimodal reasoning. To enable this CoF\ncapability, we present a two-stage training pipeline, including supervised\nfine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we\nconstruct the MM-CoF dataset, comprising 3K samples derived from a visual agent\ndesigned to adaptively identify key regions to solve visual tasks with\ndifferent image resolutions and questions. We use MM-CoF to fine-tune the\nQwen2.5-VL model for cold start. In the RL stage, we leverage the outcome\naccuracies and formats as rewards to update the Qwen2.5-VL model, enabling\nfurther refining the search and reasoning strategy of models without human\npriors. Our model achieves significant improvements on multiple benchmarks. On\nthe V* benchmark that requires strong visual reasoning capability, our model\noutperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to\n4K, demonstrating the effectiveness of the proposed CoF method and facilitating\nthe more efficient deployment of VLMs in practical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Focus (CoF)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u7126\u548c\u653e\u5927\u5173\u952e\u56fe\u50cf\u533a\u57df\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u81ea\u9002\u5e94\u5730\u805a\u7126\u5173\u952e\u56fe\u50cf\u533a\u57df\u4ee5\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51faCoF\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u5b9e\u73b0\u81ea\u9002\u5e94\u805a\u7126\u3002\u6784\u5efaMM-CoF\u6570\u636e\u96c6\u7528\u4e8e\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728V*\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u57288\u79cd\u56fe\u50cf\u5206\u8fa8\u7387\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b5%\uff0c\u9a8c\u8bc1\u4e86CoF\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "CoF\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3a\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2505.15438", "pdf": "https://arxiv.org/pdf/2505.15438", "abs": "https://arxiv.org/abs/2505.15438", "authors": ["Jianyuan Guo", "Peike Li", "Trevor Cohn"], "title": "Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation", "categories": ["cs.CV"], "comment": "Technical report, 21 pages", "summary": "Sign Language Translation (SLT) aims to map sign language videos to spoken\nlanguage text. A common approach relies on gloss annotations as an intermediate\nrepresentation, decomposing SLT into two sub-tasks: video-to-gloss recognition\nand gloss-to-text translation. While effective, this paradigm depends on\nexpert-annotated gloss labels, which are costly and rarely available in\nexisting datasets, limiting its scalability. To address this challenge, we\npropose a gloss-free pseudo gloss generation framework that eliminates the need\nfor human-annotated glosses while preserving the structured intermediate\nrepresentation. Specifically, we prompt a Large Language Model (LLM) with a few\nexample text-gloss pairs using in-context learning to produce draft sign\nglosses from spoken language text. To enhance the correspondence between\nLLM-generated pseudo glosses and the sign sequences in video, we correct the\nordering in the pseudo glosses for better alignment via a weakly supervised\nlearning process. This reordering facilitates the incorporation of auxiliary\nalignment objectives, and allows for the use of efficient supervision via a\nConnectionist Temporal Classification (CTC) loss. We train our SLT mode, which\nconsists of a vision encoder and a translator, through a three-stage pipeline,\nwhich progressively narrows the modality gap between sign language and spoken\nlanguage. Despite its simplicity, our approach outperforms previous\nstate-of-the-art gloss-free frameworks on two SLT benchmarks and achieves\ncompetitive results compared to gloss-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u624b\u8bed\u6ce8\u91ca\u7684\u4f2a\u6ce8\u91ca\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f31\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u624b\u8bed\u7ffb\u8bd1\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6ce8\u91ca\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6ce8\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4f2a\u6ce8\u91ca\uff0c\u5e76\u901a\u8fc7\u5f31\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u5bf9\u9f50\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u7f29\u5c0f\u6a21\u6001\u5dee\u8ddd\u3002", "result": "\u5728\u4e24\u4e2a\u624b\u8bed\u7ffb\u8bd1\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65e0\u6ce8\u91ca\u65b9\u6cd5\uff0c\u5e76\u4e0e\u57fa\u4e8e\u6ce8\u91ca\u7684\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6ce8\u91ca\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u624b\u8bed\u7ffb\u8bd1\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.15439", "pdf": "https://arxiv.org/pdf/2505.15439", "abs": "https://arxiv.org/abs/2505.15439", "authors": ["Ge Meng", "Zhongnan Cai", "Ruizhe Chen", "Jingyan Tu", "Yingying Wang", "Yue Huang", "Xinghao Ding"], "title": "FRN: Fractal-Based Recursive Spectral Reconstruction Network", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Generating hyperspectral images (HSIs) from RGB images through spectral\nreconstruction can significantly reduce the cost of HSI acquisition. In this\npaper, we propose a Fractal-Based Recursive Spectral Reconstruction Network\n(FRN), which differs from existing paradigms that attempt to directly integrate\nthe full-spectrum information from the R, G, and B channels in a one-shot\nmanner. Instead, it treats spectral reconstruction as a progressive process,\npredicting from broad to narrow bands or employing a coarse-to-fine approach\nfor predicting the next wavelength. Inspired by fractals in mathematics, FRN\nestablishes a novel spectral reconstruction paradigm by recursively invoking an\natomic reconstruction module. In each invocation, only the spectral information\nfrom neighboring bands is used to provide clues for the generation of the image\nat the next wavelength, which follows the low-rank property of spectral data.\nMoreover, we design a band-aware state space model that employs a\npixel-differentiated scanning strategy at different stages of the generation\nprocess, further suppressing interference from low-correlation regions caused\nby reflectance differences. Through extensive experimentation across different\ndatasets, FRN achieves superior reconstruction performance compared to\nstate-of-the-art methods in both quantitative and qualitative evaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5f62\u7684\u9012\u5f52\u5149\u8c31\u91cd\u5efa\u7f51\u7edc\uff08FRN\uff09\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u65b9\u6cd5\u4eceRGB\u56fe\u50cf\u751f\u6210\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u964d\u4f4e\u9ad8\u5149\u8c31\u56fe\u50cf\u83b7\u53d6\u6210\u672c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e00\u6b21\u6027\u6574\u5408\u5168\u5149\u8c31\u4fe1\u606f\u6548\u679c\u6709\u9650\u3002", "method": "\u91c7\u7528\u9012\u5f52\u8c03\u7528\u539f\u5b50\u91cd\u5efa\u6a21\u5757\u7684\u6e10\u8fdb\u5f0f\u65b9\u6cd5\uff0c\u5229\u7528\u76f8\u90bb\u6ce2\u6bb5\u4fe1\u606f\u9884\u6d4b\u4e0b\u4e00\u6ce2\u957f\uff0c\u5e76\u7ed3\u5408\u6ce2\u6bb5\u611f\u77e5\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cFRN\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FRN\u901a\u8fc7\u6e10\u8fdb\u5f0f\u91cd\u5efa\u548c\u5206\u5f62\u542f\u53d1\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5149\u8c31\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2505.15441", "pdf": "https://arxiv.org/pdf/2505.15441", "abs": "https://arxiv.org/abs/2505.15441", "authors": ["David Nordstr\u00f6m", "Johan Edstedt", "Fredrik Kahl", "Georg B\u00f6kman"], "title": "Stronger ViTs With Octic Equivariance", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent efforts at scaling computer vision models have established Vision\nTransformers (ViTs) as the leading architecture. ViTs incorporate weight\nsharing over image patches as an important inductive bias. In this work, we\nshow that ViTs benefit from incorporating equivariance under the octic group,\ni.e., reflections and 90-degree rotations, as a further inductive bias. We\ndevelop new architectures, octic ViTs, that use octic-equivariant layers and\nput them to the test on both supervised and self-supervised learning. Through\nextensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show\nthat octic ViTs yield more computationally efficient networks while also\nimproving performance. In particular, we achieve approximately 40% reduction in\nFLOPs for ViT-H while simultaneously improving both classification and\nsegmentation results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u516b\u5ea6\u7fa4\u7b49\u53d8\u6027\u7684ViT\u67b6\u6784\uff08octic ViTs\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u53cd\u5c04\u548c90\u5ea6\u65cb\u8f6c\u7684\u7b49\u53d8\u6027\u504f\u7f6e\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684ViT\u67b6\u6784\u5728\u56fe\u50cf\u5757\u4e0a\u5171\u4eab\u6743\u91cd\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fdb\u4e00\u6b65\u5f15\u5165\u516b\u5ea6\u7fa4\uff08\u53cd\u5c04\u548c90\u5ea6\u65cb\u8f6c\uff09\u7684\u7b49\u53d8\u6027\u504f\u7f6e\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86octic ViTs\uff0c\u91c7\u7528\u516b\u5ea6\u7b49\u53d8\u5c42\uff0c\u5e76\u5728\u76d1\u7763\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u5176\u6548\u679c\u3002\u5b9e\u9a8c\u57fa\u4e8eDeiT-III\u548cDINOv2\u5728ImageNet-1K\u4e0a\u7684\u8bad\u7ec3\u3002", "result": "octic ViTs\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff08ViT-H\u7684FLOPs\u51cf\u5c11\u7ea640%\uff09\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u5f15\u5165\u516b\u5ea6\u7fa4\u7b49\u53d8\u6027\u504f\u7f6e\u7684ViT\u67b6\u6784\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u672a\u6765\u89c6\u89c9\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.15447", "pdf": "https://arxiv.org/pdf/2505.15447", "abs": "https://arxiv.org/abs/2505.15447", "authors": ["Ziqiang Xu", "Qi Dai", "Tian Xie", "Yifan Yang", "Kai Qiu", "DongDong Chen", "Zuxuan Wu", "Chong Luo"], "title": "ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video understanding is inherently intention-driven-humans naturally focus on\nrelevant frames based on their goals. Recent advancements in multimodal large\nlanguage models (MLLMs) have enabled flexible query-driven reasoning; however,\nvideo-based frameworks like Video Chain-of-Thought lack direct training signals\nto effectively identify relevant frames. Current approaches often rely on\nheuristic methods or pseudo-label supervised annotations, which are both costly\nand limited in scalability across diverse scenarios. To overcome these\nchallenges, we introduce ViaRL, the first framework to leverage rule-based\nreinforcement learning (RL) for optimizing frame selection in intention-driven\nvideo understanding. An iterated amplification strategy is adopted to perform\nalternating cyclic training in the video CoT system, where each component\nundergoes iterative cycles of refinement to improve its capabilities. ViaRL\nutilizes the answer accuracy of a downstream model as a reward signal to train\na frame selector through trial-and-error, eliminating the need for expensive\nannotations while closely aligning with human-like learning processes.\nComprehensive experiments across multiple benchmarks, including VideoMME,\nLVBench, and MLVU, demonstrate that ViaRL consistently delivers superior\ntemporal grounding performance and robust generalization across diverse video\nunderstanding tasks, highlighting its effectiveness and scalability. Notably,\nViaRL achieves a nearly 15\\% improvement on Needle QA, a subset of MLVU, which\nis required to search a specific needle within a long video and regarded as one\nof the most suitable benchmarks for evaluating temporal grounding.", "AI": {"tldr": "ViaRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e27\u9009\u62e9\uff0c\u65e0\u9700\u6602\u8d35\u6807\u6ce8\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u4f2a\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u5dee\uff0cViaRL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u4e0b\u6e38\u6a21\u578b\u7b54\u6848\u51c6\u786e\u7387\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u8bad\u7ec3\u5e27\u9009\u62e9\u5668\uff0c\u7ed3\u5408\u8fed\u4ee3\u653e\u5927\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728Needle QA\u4e0a\u63d0\u5347\u8fd115%\u3002", "conclusion": "ViaRL\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u89c6\u9891\u7406\u89e3\u7684\u5e27\u9009\u62e9\u6548\u679c\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2505.15450", "pdf": "https://arxiv.org/pdf/2505.15450", "abs": "https://arxiv.org/abs/2505.15450", "authors": ["Die Chen", "Zhiwen Li", "Cen Chen", "Yuexiang Xie", "Xiaodan Li", "Jinyan Ye", "Yingda Chen", "Yaliang Li"], "title": "Comprehensive Evaluation and Analysis for NSFW Concept Erasure in Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models have gained widespread application across\nvarious domains, demonstrating remarkable creative potential. However, the\nstrong generalization capabilities of diffusion models can inadvertently lead\nto the generation of not-safe-for-work (NSFW) content, posing significant risks\nto their safe deployment. While several concept erasure methods have been\nproposed to mitigate the issue associated with NSFW content, a comprehensive\nevaluation of their effectiveness across various scenarios remains absent. To\nbridge this gap, we introduce a full-pipeline toolkit specifically designed for\nconcept erasure and conduct the first systematic study of NSFW concept erasure\nmethods. By examining the interplay between the underlying mechanisms and\nempirical observations, we provide in-depth insights and practical guidance for\nthe effective application of concept erasure methods in various real-world\nscenarios, with the aim of advancing the understanding of content safety in\ndiffusion models and establishing a solid foundation for future research and\ndevelopment in this critical area.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2dNSFW\u5185\u5bb9\u7684\u5168\u6d41\u7a0b\u5de5\u5177\u5305\uff0c\u5e76\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u5bfc\u81f4\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u6d41\u7a0b\u5de5\u5177\u5305\uff0c\u7cfb\u7edf\u7814\u7a76NSFW\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u7ed3\u5408\u673a\u5236\u4e0e\u5b9e\u8bc1\u89c2\u5bdf\u3002", "result": "\u63d0\u4f9b\u4e86\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6df1\u5165\u89c1\u89e3\u548c\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u4e3a\u6269\u6563\u6a21\u578b\u5185\u5bb9\u5b89\u5168\u7684\u7406\u89e3\u548c\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.15476", "pdf": "https://arxiv.org/pdf/2505.15476", "abs": "https://arxiv.org/abs/2505.15476", "authors": ["Guotao Xu", "Bowen Zhao", "Yang Xiao", "Yantao Zhong", "Liang Zhai", "Qingqi Pei"], "title": "Pura: An Efficient Privacy-Preserving Solution for Face Recognition", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Face recognition is an effective technology for identifying a target person\nby facial images. However, sensitive facial images raises privacy concerns.\nAlthough privacy-preserving face recognition is one of potential solutions,\nthis solution neither fully addresses the privacy concerns nor is efficient\nenough. To this end, we propose an efficient privacy-preserving solution for\nface recognition, named Pura, which sufficiently protects facial privacy and\nsupports face recognition over encrypted data efficiently. Specifically, we\npropose a privacy-preserving and non-interactive architecture for face\nrecognition through the threshold Paillier cryptosystem. Additionally, we\ncarefully design a suite of underlying secure computing protocols to enable\nefficient operations of face recognition over encrypted data directly.\nFurthermore, we introduce a parallel computing mechanism to enhance the\nperformance of the proposed secure computing protocols. Privacy analysis\ndemonstrates that Pura fully safeguards personal facial privacy. Experimental\nevaluations demonstrate that Pura achieves recognition speeds up to 16 times\nfaster than the state-of-the-art.", "AI": {"tldr": "Pura\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u4eba\u8138\u8bc6\u522b\u65b9\u6848\uff0c\u901a\u8fc7\u9608\u503cPaillier\u5bc6\u7801\u7cfb\u7edf\u548c\u5b89\u5168\u8ba1\u7b97\u534f\u8bae\uff0c\u5b9e\u73b0\u4e86\u52a0\u5bc6\u6570\u636e\u4e0a\u7684\u5feb\u901f\u8bc6\u522b\uff0c\u901f\u5ea6\u63d0\u534716\u500d\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u6280\u672f\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u6548\u7387\u4e0d\u8db3\u4e14\u672a\u80fd\u5b8c\u5168\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9608\u503cPaillier\u5bc6\u7801\u7cfb\u7edf\u548c\u975e\u4ea4\u4e92\u5f0f\u67b6\u6784\uff0c\u8bbe\u8ba1\u5b89\u5168\u8ba1\u7b97\u534f\u8bae\uff0c\u5e76\u5f15\u5165\u5e76\u884c\u8ba1\u7b97\u673a\u5236\u3002", "result": "Pura\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u8bc6\u522b\u901f\u5ea6\u6bd4\u73b0\u6709\u6280\u672f\u5feb16\u500d\u3002", "conclusion": "Pura\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u52a0\u5bc6\u6570\u636e\u4e0a\u7684\u4eba\u8138\u8bc6\u522b\u3002"}}
{"id": "2505.15489", "pdf": "https://arxiv.org/pdf/2505.15489", "abs": "https://arxiv.org/abs/2505.15489", "authors": ["Jiaying Wu", "Fanxiao Li", "Min-Yen Kan", "Bryan Hooi"], "title": "Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "The real-world impact of misinformation stems from the underlying misleading\nnarratives that creators seek to convey. As such, interpreting misleading\ncreator intent is essential for multimodal misinformation detection (MMD)\nsystems aimed at effective information governance. In this paper, we introduce\nan automated framework that simulates real-world multimodal news creation by\nexplicitly modeling creator intent through two components: the desired\ninfluence and the execution plan. Using this framework, we construct\nDeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs\naligned with trustworthy reference articles. The dataset captures both\nmisleading and non-misleading intents and spans manipulations across visual and\ntextual modalities. We conduct a comprehensive evaluation of 14\nstate-of-the-art vision-language models (VLMs) on three intent-centric tasks:\n(1) misleading intent detection, (2) misleading source attribution, and (3)\ncreator desire inference. Despite recent advances, we observe that current VLMs\nfall short in recognizing misleading intent, often relying on spurious cues\nsuch as superficial cross-modal consistency, stylistic signals, and heuristic\nauthenticity hints. Our findings highlight the pressing need for intent-aware\nmodeling in MMD and open new directions for developing systems capable of\ndeeper reasoning about multimodal misinformation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u6a21\u62df\u591a\u6a21\u6001\u65b0\u95fb\u521b\u4f5c\u4e2d\u7684\u521b\u4f5c\u8005\u610f\u56fe\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6DeceptionDecoded\uff0c\u5e76\u8bc4\u4f30\u4e8614\u79cd\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u610f\u56fe\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u7406\u89e3\u521b\u4f5c\u8005\u7684\u8bef\u5bfc\u610f\u56fe\u5bf9\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\uff08MMD\uff09\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u652f\u6301\u6709\u6548\u7684\u4fe1\u606f\u6cbb\u7406\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u521b\u4f5c\u8005\u610f\u56fe\uff08\u671f\u671b\u5f71\u54cd\u548c\u6267\u884c\u8ba1\u5212\uff09\uff0c\u6784\u5efa\u4e86\u5305\u542b12,000\u4e2a\u56fe\u50cf-\u6807\u9898\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e8614\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u4e2a\u610f\u56fe\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u8bc6\u522b\u8bef\u5bfc\u610f\u56fe\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u5e38\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u5982\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u6216\u98ce\u683c\u4fe1\u53f7\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u610f\u56fe\u611f\u77e5\u5efa\u6a21\u5728MMD\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5f00\u53d1\u80fd\u6df1\u5165\u63a8\u7406\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.15491", "pdf": "https://arxiv.org/pdf/2505.15491", "abs": "https://arxiv.org/abs/2505.15491", "authors": ["Ce Zhang", "Zifu Wan", "Simon Stepputtis", "Katia Sycara", "Yaqi Xie"], "title": "Spectral-Aware Global Fusion for RGB-Thermal Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by ICIP 2025", "summary": "Semantic segmentation relying solely on RGB data often struggles in\nchallenging conditions such as low illumination and obscured views, limiting\nits reliability in critical applications like autonomous driving. To address\nthis, integrating additional thermal radiation data with RGB images\ndemonstrates enhanced performance and robustness. However, how to effectively\nreconcile the modality discrepancies and fuse the RGB and thermal features\nremains a well-known challenge. In this work, we address this challenge from a\nnovel spectral perspective. We observe that the multi-modal features can be\ncategorized into two spectral components: low-frequency features that provide\nbroad scene context, including color variations and smooth areas, and\nhigh-frequency features that capture modality-specific details such as edges\nand textures. Inspired by this, we propose the Spectral-aware Global Fusion\nNetwork (SGFNet) to effectively enhance and fuse the multi-modal features by\nexplicitly modeling the interactions between the high-frequency,\nmodality-specific features. Our experimental results demonstrate that SGFNet\noutperforms the state-of-the-art methods on the MFNet and PST900 datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u8c31\u89c6\u89d2\u7684\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u65b9\u6cd5SGFNet\uff0c\u901a\u8fc7\u533a\u5206\u4f4e\u9891\u548c\u9ad8\u9891\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86RGB\u4e0e\u70ed\u8f90\u5c04\u6570\u636e\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "RGB\u6570\u636e\u5728\u4f4e\u5149\u7167\u548c\u906e\u6321\u7b49\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u7ed3\u5408\u70ed\u8f90\u5c04\u6570\u636e\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faSGFNet\uff0c\u4ece\u9891\u8c31\u89c6\u89d2\u5c06\u591a\u6a21\u6001\u7279\u5f81\u5206\u4e3a\u4f4e\u9891\uff08\u573a\u666f\u4e0a\u4e0b\u6587\uff09\u548c\u9ad8\u9891\uff08\u6a21\u6001\u7ec6\u8282\uff09\uff0c\u5e76\u663e\u5f0f\u5efa\u6a21\u9ad8\u9891\u7279\u5f81\u7684\u4ea4\u4e92\u3002", "result": "\u5728MFNet\u548cPST900\u6570\u636e\u96c6\u4e0a\uff0cSGFNet\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u9891\u8c31\u89c6\u89d2\u7684\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2505.15504", "pdf": "https://arxiv.org/pdf/2505.15504", "abs": "https://arxiv.org/abs/2505.15504", "authors": ["Conghao Xiong", "Zhengrui Guo", "Zhe Xu", "Yifei Zhang", "Raymond Kai-Yu Tong", "Si Yong Yeo", "Hao Chen", "Joseph J. Y. Sung", "Irwin King"], "title": "Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning has advanced computational pathology but expert annotations\nremain scarce. Few-shot learning mitigates annotation burdens yet suffers from\noverfitting and discriminative feature mischaracterization. In addition, the\ncurrent few-shot multiple instance learning (MIL) approaches leverage\npretrained vision-language models to alleviate these issues, but at the cost of\ncomplex preprocessing and high computational cost. We propose a\nSqueeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in\nMIL models to address these challenges. The SR block comprises two core\ncomponents: a pair of low-rank trainable matrices (squeeze pathway, SP) that\nreduces parameter count and imposes a bottleneck to prevent spurious feature\nlearning, and a frozen random recalibration matrix that preserves geometric\nstructure, diversifies feature directions, and redefines the optimization\nobjective for the SP. We provide theoretical guarantees that the SR block can\napproximate any linear mapping to arbitrary precision, thereby ensuring that\nthe performance of a standard MIL model serves as a lower bound for its\nSR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL\nmodels consistently outperform prior methods while requiring significantly\nfewer parameters and no architectural changes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdSqueeze-and-Recalibrate (SR)\u5757\uff0c\u4f5c\u4e3aMIL\u6a21\u578b\u4e2d\u7ebf\u6027\u5c42\u7684\u66ff\u4ee3\uff0c\u89e3\u51b3\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8fc7\u62df\u5408\u548c\u7279\u5f81\u8bef\u5224\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u4e13\u5bb6\u6807\u6ce8\u7a00\u7f3a\uff0c\u73b0\u6709\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8fc7\u62df\u5408\u548c\u7279\u5f81\u8bef\u5224\u95ee\u9898\uff0c\u4e14\u5f53\u524d\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684MIL\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "SR\u5757\u5305\u542b\u4f4e\u79e9\u53ef\u8bad\u7ec3\u77e9\u9635\uff08\u538b\u7f29\u8def\u5f84\uff09\u548c\u51bb\u7ed3\u968f\u673a\u91cd\u6821\u51c6\u77e9\u9635\uff0c\u51cf\u5c11\u53c2\u6570\u5e76\u4fdd\u6301\u51e0\u4f55\u7ed3\u6784\u3002", "result": "SR-MIL\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53c2\u6570\u66f4\u5c11\u4e14\u65e0\u9700\u67b6\u6784\u6539\u52a8\u3002", "conclusion": "SR\u5757\u4e3aMIL\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.15506", "pdf": "https://arxiv.org/pdf/2505.15506", "abs": "https://arxiv.org/abs/2505.15506", "authors": ["Debarshi Brahma", "Anuska Roy", "Soma Biswas"], "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts", "categories": ["cs.CV", "cs.LG"], "comment": "Published in TMLR (2025)", "summary": "Recently, Vision-Language foundation models like CLIP and ALIGN, which are\npre-trained on large-scale data have shown remarkable zero-shot generalization\nto diverse datasets with different classes and even domains. In this work, we\ntake a step further and analyze whether these models can be adapted to target\ndatasets having very different distributions and classes compared to what these\nmodels have been trained on, using only a few labeled examples from the target\ndataset. In such scenarios, finetuning large pretrained models is challenging\ndue to problems of overfitting as well as loss of generalization, and has not\nbeen well explored in prior literature. Since, the pre-training data of such\nmodels are unavailable, it is difficult to comprehend the performance on\nvarious downstream datasets. First, we try to answer the question: Given a\ntarget dataset with a few labelled examples, can we estimate whether further\nfine-tuning can enhance the performance compared to zero-shot evaluation? by\nanalyzing the common vision-language embedding space. Based on the analysis, we\npropose a novel prompt-tuning method, PromptMargin for adapting such\nlarge-scale VLMs directly on the few target samples. PromptMargin effectively\ntunes the text as well as visual prompts for this task, and has two main\nmodules: 1) Firstly, we use a selective augmentation strategy to complement the\nfew training samples in each task; 2) Additionally, to ensure robust training\nin the presence of unfamiliar class names, we increase the inter-class margin\nfor improved class discrimination using a novel Multimodal Margin Regularizer.\nExtensive experiments and analysis across fifteen target benchmark datasets,\nwith varying degrees of distribution shifts from natural images, shows the\neffectiveness of the proposed framework over the existing state-of-the-art\napproaches applied to this setting. github.com/debarshigit/PromptMargin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPromptMargin\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5fae\u8c03\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u548cALIGN\uff09\u5728\u76ee\u6807\u6570\u636e\u96c6\u5206\u5e03\u548c\u7c7b\u522b\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u5dee\u5f02\u8f83\u5927\u65f6\uff0c\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u6709\u6548\u5fae\u8c03\u3002", "method": "\u63d0\u51faPromptMargin\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u589e\u5f3a\u7b56\u7565\u548c\u65b0\u578b\u591a\u6a21\u6001\u8fb9\u754c\u6b63\u5219\u5316\u5668\uff0c\u4f18\u5316\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\u3002", "result": "\u572815\u4e2a\u76ee\u6807\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "PromptMargin\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.15510", "pdf": "https://arxiv.org/pdf/2505.15510", "abs": "https://arxiv.org/abs/2505.15510", "authors": ["Zihui Cheng", "Qiguang Chen", "Xiao Xu", "Jiaqi Wang", "Weiyun Wang", "Hao Fei", "Yidong Wang", "Alex Jinpeng Wang", "Zhi Chen", "Wanxiang Che", "Libo Qin"], "title": "Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved significant success in\nmultimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing\nperformance and interpretability. Recent MCoT methods fall into two categories:\n(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual\noutput; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved\nimage-text outputs. Despite advances in both approaches, the mechanisms driving\nthese improvements are not fully understood. To fill this gap, we first reveal\nthat MCoT boosts LVLMs by incorporating visual thoughts, which convey image\ninformation to the reasoning process regardless of the MCoT format, depending\nonly on clarity and conciseness of expression. Furthermore, to explore visual\nthoughts systematically, we define four distinct forms of visual thought\nexpressions and analyze them comprehensively. Our findings demonstrate that\nthese forms differ in clarity and conciseness, yielding varying levels of MCoT\nimprovement. Additionally, we explore the internal nature of visual thoughts,\nfinding that visual thoughts serve as intermediaries between the input image\nand reasoning to deeper transformer layers, enabling more advanced visual\ninformation transmission. We hope that the visual thoughts can inspire further\nbreakthroughs for future MCoT research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u5982\u4f55\u901a\u8fc7\u89c6\u89c9\u601d\u7ef4\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5206\u6790\u4e86\u56db\u79cd\u89c6\u89c9\u601d\u7ef4\u8868\u8fbe\u5f62\u5f0f\u7684\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1MCoT\u65b9\u6cd5\u5728\u63d0\u5347LVLMs\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u80cc\u540e\u7684\u673a\u5236\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u89c6\u89c9\u601d\u7ef4\u7684\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u63ed\u793a\u4e86MCoT\u901a\u8fc7\u89c6\u89c9\u601d\u7ef4\u4f20\u9012\u56fe\u50cf\u4fe1\u606f\uff0c\u5b9a\u4e49\u4e86\u56db\u79cd\u89c6\u89c9\u601d\u7ef4\u8868\u8fbe\u5f62\u5f0f\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6e05\u6670\u5ea6\u548c\u7b80\u6d01\u6027\u5bf9MCoT\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u5f62\u5f0f\u7684\u89c6\u89c9\u601d\u7ef4\u5728\u6e05\u6670\u5ea6\u548c\u7b80\u6d01\u6027\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u4ece\u800c\u5bf9MCoT\u7684\u63d0\u5347\u6548\u679c\u4e0d\u540c\uff1b\u89c6\u89c9\u601d\u7ef4\u4f5c\u4e3a\u56fe\u50cf\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u4e2d\u4ecb\uff0c\u4fc3\u8fdb\u4e86\u66f4\u6df1\u5c42\u6b21\u7684\u89c6\u89c9\u4fe1\u606f\u4f20\u9012\u3002", "conclusion": "\u89c6\u89c9\u601d\u7ef4\u7684\u6df1\u5165\u7814\u7a76\u4e3a\u672a\u6765MCoT\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u6709\u671b\u63a8\u52a8\u8fdb\u4e00\u6b65\u7a81\u7834\u3002"}}
{"id": "2505.15518", "pdf": "https://arxiv.org/pdf/2505.15518", "abs": "https://arxiv.org/abs/2505.15518", "authors": ["Chang Liu"], "title": "Detection of Underwater Multi-Targets Based on Self-Supervised Learning and Deformable Path Aggregation Feature Pyramid Network", "categories": ["cs.CV"], "comment": "ICIC 2025 accepted", "summary": "To overcome the constraints of the underwater environment and improve the\naccuracy and robustness of underwater target detection models, this paper\ndevelops a specialized dataset for underwater target detection and proposes an\nefficient algorithm for underwater multi-target detection. A self-supervised\nlearning based on the SimSiam structure is employed for the pre-training of\nunderwater target detection network. To address the problems of low detection\naccuracy caused by low contrast, mutual occlusion and dense distribution of\nunderwater targets in underwater object detection, a detection model suitable\nfor underwater target detection is proposed by introducing deformable\nconvolution and dilated convolution. The proposed detection model can obtain\nmore effective information by increasing the receptive field. In addition, the\nregression loss function EIoU is introduced, which improves model performance\nby separately calculating the width and height losses of the predicted box.\nExperiment results show that the accuracy of the underwater target detection\nhas been improved by the proposed detector.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u6539\u8fdb\u7684\u5377\u79ef\u65b9\u6cd5\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u9650\u5236\uff08\u5982\u4f4e\u5bf9\u6bd4\u5ea6\u3001\u76ee\u6807\u906e\u6321\u548c\u5bc6\u96c6\u5206\u5e03\uff09\u5bfc\u81f4\u76ee\u6807\u68c0\u6d4b\u7cbe\u5ea6\u4f4e\uff0c\u9700\u6539\u8fdb\u6a21\u578b\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8eSimSiam\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u53ef\u53d8\u5f62\u5377\u79ef\u548c\u6269\u5f20\u5377\u79ef\u4ee5\u589e\u52a0\u611f\u53d7\u91ce\uff0c\u540c\u65f6\u4f7f\u7528EIoU\u635f\u5931\u51fd\u6570\u4f18\u5316\u9884\u6d4b\u6846\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u68c0\u6d4b\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u6539\u8fdb\u7684\u5377\u79ef\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.15528", "pdf": "https://arxiv.org/pdf/2505.15528", "abs": "https://arxiv.org/abs/2505.15528", "authors": ["Zane K J Hartley", "Lewis A G Stuart", "Andrew P French", "Michael P Pound"], "title": "PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting", "categories": ["cs.CV", "cs.GR", "I.2.10; I.3.0; I.4.5"], "comment": "13 pages, 5 figures, 4 tables", "summary": "Recent years have seen substantial improvements in the ability to generate\nsynthetic 3D objects using AI. However, generating complex 3D objects, such as\nplants, remains a considerable challenge. Current generative 3D models struggle\nwith plant generation compared to general objects, limiting their usability in\nplant analysis tools, which require fine detail and accurate geometry. We\nintroduce PlantDreamer, a novel approach to 3D synthetic plant generation,\nwhich can achieve greater levels of realism for complex plant geometry and\ntextures than available text-to-3D models. To achieve this, our new generation\npipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an\nadaptable Gaussian culling algorithm, which directly improve textural realism\nand geometric integrity of generated 3D plant models. Additionally,\nPlantDreamer enables both purely synthetic plant generation, by leveraging\nL-System-generated meshes, and the enhancement of real-world plant point clouds\nby converting them into 3D Gaussian Splats. We evaluate our approach by\ncomparing its outputs with state-of-the-art text-to-3D models, demonstrating\nthat PlantDreamer outperforms existing methods in producing high-fidelity\nsynthetic plants. Our results indicate that our approach not only advances\nsynthetic plant generation, but also facilitates the upgrading of legacy point\ncloud datasets, making it a valuable tool for 3D phenotyping applications.", "AI": {"tldr": "PlantDreamer\u662f\u4e00\u79cd\u65b0\u76843D\u5408\u6210\u690d\u7269\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6ControlNet\u3001\u5fae\u8c03\u7684\u4f4e\u79e9\u9002\u5e94\u548c\u9ad8\u65af\u5254\u9664\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u690d\u7269\u6a21\u578b\u7684\u771f\u5b9e\u611f\u548c\u51e0\u4f55\u5b8c\u6574\u6027\u3002", "motivation": "\u5f53\u524d\u751f\u62103D\u690d\u7269\u7684\u6a21\u578b\u5728\u590d\u6742\u690d\u7269\u751f\u6210\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u690d\u7269\u5206\u6790\u5de5\u5177\u7684\u5e94\u7528\u3002PlantDreamer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6ControlNet\u3001\u4f4e\u79e9\u9002\u5e94\u548c\u9ad8\u65af\u5254\u9664\u7b97\u6cd5\uff0c\u652f\u6301\u7eaf\u5408\u6210\u751f\u6210\u548c\u771f\u5b9e\u70b9\u4e91\u589e\u5f3a\u3002", "result": "PlantDreamer\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u690d\u7269\u6a21\u578b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u5347\u65e7\u70b9\u4e91\u6570\u636e\u96c6\u3002", "conclusion": "PlantDreamer\u4e0d\u4ec5\u63a8\u52a8\u4e86\u5408\u6210\u690d\u7269\u751f\u6210\u6280\u672f\uff0c\u8fd8\u4e3a3D\u8868\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2505.15529", "pdf": "https://arxiv.org/pdf/2505.15529", "abs": "https://arxiv.org/abs/2505.15529", "authors": ["Lingyu Kong", "Hongzhi Zhang", "Jingyuan Zhang", "Jianzhao Huang", "Kunze Li", "Qi Wang", "Fuzheng Zhang"], "title": "Clapper: Compact Learning and Video Representation in VLMs", "categories": ["cs.CV"], "comment": null, "summary": "Current vision-language models (VLMs) have demonstrated remarkable\ncapabilities across diverse video understanding applications. Designing VLMs\nfor video inputs requires effectively modeling the temporal dimension (i.e.\ncapturing dependencies across frames) and balancing the processing of short and\nlong videos. Specifically, short videos demand preservation of fine-grained\ndetails, whereas long videos require strategic compression of visual\ninformation to handle extensive temporal contexts efficiently. However, our\nempirical analysis reveals a critical limitation: most existing VLMs suffer\nsevere performance degradation in long video understanding tasks when\ncompressing visual tokens below a quarter of their original visual tokens. To\nenable more effective modeling of both short and long video inputs, we propose\nClapper, a method that utilizes a slow-fast strategy for video representation\nand introduces a novel module named TimePerceiver for efficient\ntemporal-spatial encoding within existing VLM backbones. By using our method,\nwe achieves 13x compression of visual tokens per frame (averaging 61\ntokens/frame) without compromising QA accuracy. In our experiments, Clapper\nachieves 62.0% on VideoMME, 69.8% on MLVU, and 67.4% on TempCompass, all with\nfewer than 6,000 visual tokens per video. The code will be publicly available\non the homepage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faClapper\u65b9\u6cd5\uff0c\u901a\u8fc7\u6162\u5feb\u7b56\u7565\u548cTimePerceiver\u6a21\u5757\u4f18\u5316\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u89c6\u89c9\u6807\u8bb0\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\uff0c\u9700\u5e73\u8861\u77ed\u89c6\u9891\u7ec6\u8282\u4fdd\u7559\u4e0e\u957f\u89c6\u9891\u4fe1\u606f\u538b\u7f29\u3002", "method": "\u91c7\u7528\u6162\u5feb\u7b56\u7565\u8868\u793a\u89c6\u9891\uff0c\u5f15\u5165TimePerceiver\u6a21\u5757\u8fdb\u884c\u65f6\u7a7a\u7f16\u7801\uff0c\u538b\u7f29\u89c6\u89c9\u6807\u8bb0\u3002", "result": "\u5b9e\u73b0\u6bcf\u5e2713\u500d\u89c6\u89c9\u6807\u8bb0\u538b\u7f29\uff08\u5e73\u574761\u6807\u8bb0/\u5e27\uff09\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Clapper\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2505.15533", "pdf": "https://arxiv.org/pdf/2505.15533", "abs": "https://arxiv.org/abs/2505.15533", "authors": ["Chang Liu"], "title": "Convolutional Long Short-Term Memory Neural Networks Based Numerical Simulation of Flow Field", "categories": ["cs.CV"], "comment": "ICIC 2025 accepted", "summary": "Computational Fluid Dynamics (CFD) is the main approach to analyzing flow\nfield. However, the convergence and accuracy depend largely on mathematical\nmodels of flow, numerical methods, and time consumption. Deep learning-based\nanalysis of flow filed provides an alternative. For the task of flow field\nprediction, an improved Convolutional Long Short-Term Memory (Con-vLSTM) Neural\nNetwork is proposed as the baseline network in consideration of the temporal\nand spatial characteristics of flow field. Combining dynamic mesh technology\nand User-Defined Function (UDF), numerical simulations of flow around a\ncircular cylinder were conducted. Flow field snapshots were used to sample data\nfrom the cylinder's wake region at different time instants, constructing a flow\nfield dataset with sufficient volume and rich flow state var-iations. Residual\nnetworks and attention mechanisms are combined with the standard ConvLSTM\nmodel. Compared with the standard ConvLSTM model, the results demonstrate that\nthe improved ConvLSTM model can extract more temporal and spatial features\nwhile having fewer parameters and shorter train-ing time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684ConvLSTM\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u6b8b\u5dee\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u6d41\u573a\u9884\u6d4b\uff0c\u76f8\u6bd4\u6807\u51c6ConvLSTM\u6a21\u578b\uff0c\u80fd\u63d0\u53d6\u66f4\u591a\u65f6\u7a7a\u7279\u5f81\u4e14\u53c2\u6570\u66f4\u5c11\u3001\u8bad\u7ec3\u66f4\u5feb\u3002", "motivation": "\u4f20\u7edfCFD\u65b9\u6cd5\u4f9d\u8d56\u6570\u5b66\u6a21\u578b\u548c\u6570\u503c\u65b9\u6cd5\uff0c\u6536\u655b\u6027\u548c\u51c6\u786e\u6027\u53d7\u9650\uff0c\u6df1\u5ea6\u5b66\u4e60\u4e3a\u6d41\u573a\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u7f51\u683c\u6280\u672f\u548cUDF\u8fdb\u884c\u6570\u503c\u6a21\u62df\uff0c\u6784\u5efa\u6d41\u573a\u6570\u636e\u96c6\uff1b\u6539\u8fdbConvLSTM\u6a21\u578b\uff0c\u5f15\u5165\u6b8b\u5dee\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u6539\u8fdb\u7684ConvLSTM\u6a21\u578b\u5728\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u53c2\u6570\u66f4\u5c11\uff0c\u8bad\u7ec3\u65f6\u95f4\u66f4\u77ed\u3002", "conclusion": "\u6539\u8fdb\u7684ConvLSTM\u6a21\u578b\u4e3a\u6d41\u573a\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.15545", "pdf": "https://arxiv.org/pdf/2505.15545", "abs": "https://arxiv.org/abs/2505.15545", "authors": ["Andrew Caunes", "Thierry Chateau", "Vincent Fremont"], "title": "seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and Adaptation in 3D Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic segmentation plays a pivotal role in autonomous driving and road\ninfrastructure analysis, yet state-of-the-art 3D models are prone to severe\ndomain shift when deployed across different datasets. We propose a novel\nmulti-view projection framework that excels in both domain generalization (DG)\nand unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans\ninto coherent 3D scenes and renders them from multiple virtual camera poses to\ncreate a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D\nsegmentation model in-domain. During inference, the model processes hundreds of\nviews per scene; the resulting logits are back-projected to 3D with an\nocclusion-aware voting scheme to generate final point-wise labels. Our\nframework is modular and enables extensive exploration of key design\nparameters, such as view generation optimization (VGO), visualization modality\noptimization (MODO), and 2D model choice. We evaluate on the nuScenes and\nSemanticKITTI datasets under both the DG and UDA settings. We achieve\nstate-of-the-art results in UDA and close to state-of-the-art in DG, with\nparticularly large gains on large, static classes. Our code and dataset\ngeneration tools will be publicly available at\nhttps://github.com/andrewcaunes/ia4markings", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u89c6\u89d2\u6295\u5f71\u6846\u67b6\uff0c\u7528\u4e8e3D\u8bed\u4e49\u5206\u5272\u7684\u9886\u57df\u6cdb\u5316\u548c\u65e0\u76d1\u7763\u9886\u57df\u9002\u5e94\uff0c\u901a\u8fc7\u751f\u6210\u5408\u62102D\u6570\u636e\u96c6\u8bad\u7ec32D\u6a21\u578b\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u6295\u7968\u65b9\u6848\u751f\u62103D\u6807\u7b7e\u3002", "motivation": "\u89e3\u51b33D\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "\u5c06Lidar\u626b\u63cf\u5bf9\u9f50\u4e3a3D\u573a\u666f\u5e76\u6e32\u67d3\u591a\u89c6\u89d22D\u6570\u636e\uff0c\u8bad\u7ec32D\u5206\u5272\u6a21\u578b\uff0c\u63a8\u7406\u65f6\u901a\u8fc7\u906e\u6321\u611f\u77e5\u6295\u7968\u65b9\u6848\u751f\u62103D\u6807\u7b7e\u3002", "result": "\u5728nuScenes\u548cSemanticKITTI\u6570\u636e\u96c6\u4e0a\uff0cUDA\u8fbe\u5230SOTA\uff0cDG\u63a5\u8fd1SOTA\uff0c\u5c24\u5176\u5728\u5927\u9759\u6001\u7c7b\u522b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u6a21\u5757\u5316\u4e14\u9ad8\u6548\uff0c\u4ee3\u7801\u548c\u5de5\u5177\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.15564", "pdf": "https://arxiv.org/pdf/2505.15564", "abs": "https://arxiv.org/abs/2505.15564", "authors": ["Hossein Hassani", "Soodeh Nikan", "Abdallah Shami"], "title": "TinyDrive: Multiscale Visual Question Answering with Selective Token Routing for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Vision Language Models (VLMs) employed for visual question-answering (VQA) in\nautonomous driving often require substantial computational resources that pose\na challenge for their deployment in resource-constrained vehicles. To address\nthis challenge, we introduce TinyDrive, a lightweight yet effective VLM for\nmulti-view VQA in driving scenarios. Our model comprises two key components\nincluding a multiscale vision encoder and a dual-level prioritization mechanism\nfor tokens and sequences. The multiscale encoder facilitates the processing of\nmulti-view images at diverse resolutions through scale injection and\ncross-scale gating to generate enhanced visual representations. At the token\nlevel, we design a token routing mechanism that dynamically selects and process\nthe most informative tokens based on learned importance scores. At the sequence\nlevel, we propose integrating normalized loss, uncertainty estimates, and a\ndiversity metric to formulate sequence scores that rank and preserve samples\nwithin a sequence priority buffer. Samples with higher scores are more\nfrequently selected for training. TinyDrive is first evaluated on our\ncustom-curated VQA dataset, and it is subsequently tested on the public DriveLM\nbenchmark, where it achieves state-of-the-art language understanding\nperformance. Notably, it achieves relative improvements of 11.1% and 35.4% in\nBLEU-4 and METEOR scores, respectively, despite having a significantly smaller\nparameter count.", "AI": {"tldr": "TinyDrive\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u89c6\u89d2\u89c6\u89c9\u95ee\u7b54\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u89c6\u89c9\u7f16\u7801\u5668\u548c\u53cc\u7ea7\u4f18\u5148\u7ea7\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u56e0\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u800c\u96be\u4ee5\u90e8\u7f72\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u89c6\u89c9\u7f16\u7801\u5668\u548c\u53cc\u7ea7\u4f18\u5148\u7ea7\u673a\u5236\uff08\u4ee4\u724c\u8def\u7531\u548c\u5e8f\u5217\u8bc4\u5206\uff09\u3002", "result": "\u5728\u81ea\u5b9a\u4e49VQA\u6570\u636e\u96c6\u548c\u516c\u5f00DriveLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cBLEU-4\u548cMETEOR\u5206\u6570\u5206\u522b\u63d0\u534711.1%\u548c35.4%\u3002", "conclusion": "TinyDrive\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u89c6\u89d2\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\u3002"}}
{"id": "2505.15576", "pdf": "https://arxiv.org/pdf/2505.15576", "abs": "https://arxiv.org/abs/2505.15576", "authors": ["Xin Huang", "Ruibin Li", "Tong Jia", "Wei Zheng", "Ya Wang"], "title": "Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at the International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "Vision-Language Models (VLMs) are essential for multimodal tasks, especially\ncompositional reasoning (CR) tasks, which require distinguishing fine-grained\nsemantic differences between visual and textual embeddings. However, existing\nmethods primarily fine-tune the model by generating text-based hard negative\nsamples, neglecting the importance of image-based negative samples, which\nresults in insufficient training of the visual encoder and ultimately impacts\nthe overall performance of the model. Moreover, negative samples are typically\ntreated uniformly, without considering their difficulty levels, and the\nalignment of positive samples is insufficient, which leads to challenges in\naligning difficult sample pairs. To address these issues, we propose Adaptive\nHard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard\nnegatives into the visual domain to generate semantically disturbed image-based\nnegatives for training the model, thereby enhancing its overall performance.\nAHNPL also introduces a contrastive learning approach using a multimodal hard\nnegative loss to improve the model's discrimination of hard negatives within\neach modality and a dynamic margin loss that adjusts the contrastive margin\naccording to sample difficulty to enhance the distinction of challenging sample\npairs. Experiments on three public datasets demonstrate that our method\neffectively boosts VLMs' performance on complex CR tasks. The source code is\navailable at https://github.com/nynu-BDAI/AHNPL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u786c\u8d1f\u6837\u672c\u6270\u52a8\u5b66\u4e60\uff08AHNPL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u89c6\u89c9\u57df\u4e2d\u7684\u8d1f\u6837\u672c\u548c\u6539\u8fdb\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u8d1f\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u8d1f\u6837\u672c\u7684\u91cd\u8981\u6027\uff0c\u4e14\u672a\u8003\u8651\u6837\u672c\u96be\u5ea6\u5dee\u5f02\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u53d7\u9650\u3002", "method": "AHNPL\u5c06\u6587\u672c\u8d1f\u6837\u672c\u8f6c\u5316\u4e3a\u89c6\u89c9\u8d1f\u6837\u672c\uff0c\u5e76\u5f15\u5165\u591a\u6a21\u6001\u786c\u8d1f\u6837\u672c\u635f\u5931\u548c\u52a8\u6001\u8fb9\u7f18\u635f\u5931\uff0c\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAHNPL\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "AHNPL\u901a\u8fc7\u6539\u8fdb\u8d1f\u6837\u672c\u751f\u6210\u548c\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.15581", "pdf": "https://arxiv.org/pdf/2505.15581", "abs": "https://arxiv.org/abs/2505.15581", "authors": ["Hua Li", "Shijie Lian", "Zhiyuan Li", "Runmin Cong", "Sam Kwong"], "title": "UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With recent breakthroughs in large-scale modeling, the Segment Anything Model\n(SAM) has demonstrated significant potential in a variety of visual\napplications. However, due to the lack of underwater domain expertise, SAM and\nits variants face performance limitations in end-to-end underwater instance\nsegmentation tasks, while their higher computational requirements further\nhinder their application in underwater scenarios. To address this challenge, we\npropose a large-scale underwater instance segmentation dataset, UIIS10K, which\nincludes 10,048 images with pixel-level annotations for 10 categories. Then, we\nintroduce UWSAM, an efficient model designed for automatic and accurate\nsegmentation of underwater instances. UWSAM efficiently distills knowledge from\nthe SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the\nMask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective\nvisual representation learning. Furthermore, we design an End-to-end Underwater\nPrompt Generator (EUPG) for UWSAM, which automatically generates underwater\nprompts instead of explicitly providing foreground points or boxes as prompts,\nthus enabling the network to locate underwater instances accurately for\nefficient segmentation. Comprehensive experimental results show that our model\nis effective, achieving significant performance improvements over\nstate-of-the-art methods on multiple underwater instance datasets. Datasets and\ncodes are available at https://github.com/LiamLian0727/UIIS10K.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u7684\u9ad8\u6548\u6a21\u578bUWSAM\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u81ea\u52a8\u63d0\u793a\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eSAM\u53ca\u5176\u53d8\u4f53\u5728\u6c34\u4e0b\u9886\u57df\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u4e14\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u7684\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86UIIS10K\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86UWSAM\u6a21\u578b\uff0c\u91c7\u7528Mask GAT-based\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u548c\u7aef\u5230\u7aef\u6c34\u4e0b\u63d0\u793a\u751f\u6210\u5668\uff08EUPG\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUWSAM\u5728\u591a\u4e2a\u6c34\u4e0b\u5b9e\u4f8b\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UWSAM\u4e3a\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.15592", "pdf": "https://arxiv.org/pdf/2505.15592", "abs": "https://arxiv.org/abs/2505.15592", "authors": ["Niccolo Avogaro", "Thomas Frick", "Yagmur G. Cinar", "Daniel Caraballo", "Cezary Skura", "Filip M. Janicki", "Piotr Kluska", "Brown Ebouky", "Nicola Farronato", "Florian Scheidegger", "Cristiano Malossi", "Konrad Schindler", "Andrea Bartezzaghi", "Roy Assaf", "Mattia Rigotti"], "title": "VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pretrained vision backbones have transformed computer vision by\nproviding powerful feature extractors that enable various downstream tasks,\nincluding training-free approaches like visual prompting for semantic\nsegmentation. Despite their success in generic scenarios, these models often\nfall short when applied to specialized technical domains where the visual\nfeatures differ significantly from their training distribution. To bridge this\ngap, we introduce VP Lab, a comprehensive iterative framework that enhances\nvisual prompting for robust segmentation model development. At the core of VP\nLab lies E-PEFT, a novel ensemble of parameter-efficient fine-tuning techniques\nspecifically designed to adapt our visual prompting pipeline to specific\ndomains in a manner that is both parameter- and data-efficient. Our approach\nnot only surpasses the state-of-the-art in parameter-efficient fine-tuning for\nthe Segment Anything Model (SAM), but also facilitates an interactive,\nnear-real-time loop, allowing users to observe progressively improving results\nas they experiment within the framework. By integrating E-PEFT with visual\nprompting, we demonstrate a remarkable 50\\% increase in semantic segmentation\nmIoU performance across various technical datasets using only 5 validated\nimages, establishing a new paradigm for fast, efficient, and interactive model\ndeployment in new, challenging domains. This work comes in the form of a\ndemonstration.", "AI": {"tldr": "VP Lab\u6846\u67b6\u901a\u8fc7E-PEFT\u6280\u672f\u63d0\u5347\u89c6\u89c9\u63d0\u793a\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faVP Lab\u6846\u67b6\u548cE-PEFT\u6280\u672f\uff0c\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u89c6\u89c9\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6280\u672f\u6570\u636e\u96c6\u4e0a\u5b9e\u73b050%\u7684mIoU\u63d0\u5347\uff0c\u4ec5\u97005\u5f20\u9a8c\u8bc1\u56fe\u50cf\u3002", "conclusion": "VP Lab\u4e3a\u5feb\u901f\u3001\u9ad8\u6548\u3001\u4ea4\u4e92\u5f0f\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.15616", "pdf": "https://arxiv.org/pdf/2505.15616", "abs": "https://arxiv.org/abs/2505.15616", "authors": ["Ruilin Yao", "Bo Zhang", "Jirui Huang", "Xinwei Long", "Yifang Zhang", "Tianyu Zou", "Yufei Wu", "Shichao Su", "Yifan Xu", "Wenxi Zeng", "Zhaoyu Yang", "Guoyou Li", "Shilan Zhang", "Zichan Li", "Yaxiong Chen", "Shengwu Xiong", "Peng Xu", "Jiajun Zhang", "Bowen Zhou", "David Clifton", "Luc Van Gool"], "title": "LENS: Multi-level Evaluation of Multimodal Reasoning with Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved significant advances\nin integrating visual and linguistic information, yet their ability to reason\nabout complex and real-world scenarios remains limited. The existing benchmarks\nare usually constructed in the task-oriented manner without guarantee that\ndifferent task samples come from the same data distribution, thus they often\nfall short in evaluating the synergistic effects of lower-level perceptual\ncapabilities on higher-order reasoning. To lift this limitation, we contribute\nLens, a multi-level benchmark with 3.4K contemporary images and 60K+\nhuman-authored questions covering eight tasks and 12 daily scenarios, forming\nthree progressive task tiers, i.e., perception, understanding, and reasoning.\nOne feature is that each image is equipped with rich annotations for all tasks.\nThus, this dataset intrinsically supports to evaluate MLLMs to handle\nimage-invariable prompts, from basic perception to compositional reasoning. In\naddition, our images are manully collected from the social media, in which 53%\nwere published later than Jan. 2025. We evaluate 15+ frontier MLLMs such as\nQwen2.5-VL-72B, InternVL3-78B, GPT-4o and two reasoning models QVQ-72B-preview\nand Kimi-VL. These models are released later than Dec. 2024, and none of them\nachieve an accuracy greater than 60% in the reasoning tasks. Project page:\nhttps://github.com/Lens4MLLMs/lens. ICCV 2025 workshop page:\nhttps://lens4mllms.github.io/mars2-workshop-iccv2025/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Lens\uff0c\u4e00\u4e2a\u591a\u5c42\u7ea7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ece\u611f\u77e5\u5230\u63a8\u7406\u7684\u80fd\u529b\uff0c\u5305\u542b3.4K\u56fe\u50cf\u548c60K+\u95ee\u9898\uff0c\u8986\u76d68\u4efb\u52a1\u548c12\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u8bc4\u4f30MLLMs\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u534f\u540c\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u4efb\u52a1\u6837\u672c\u5206\u5e03\u4e0d\u4e00\u81f4\u3002", "method": "\u6784\u5efaLens\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u4efb\u52a1\u5c42\u7ea7\uff08\u611f\u77e5\u3001\u7406\u89e3\u3001\u63a8\u7406\uff09\uff0c\u6bcf\u5f20\u56fe\u50cf\u914d\u5907\u4e30\u5bcc\u6807\u6ce8\uff0c\u652f\u6301\u56fe\u50cf\u4e0d\u53d8\u63d0\u793a\u7684\u8bc4\u4f30\u3002", "result": "\u6d4b\u8bd5\u4e8615+\u524d\u6cbfMLLMs\uff0c\u63a8\u7406\u4efb\u52a1\u51c6\u786e\u7387\u5747\u672a\u8d85\u8fc760%\u3002", "conclusion": "Lens\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u63ed\u793a\u4e86MLLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.15628", "pdf": "https://arxiv.org/pdf/2505.15628", "abs": "https://arxiv.org/abs/2505.15628", "authors": ["Iuliia Kotseruba", "John K. Tsotsos"], "title": "SNAP: A Benchmark for Testing the Effects of Capture Conditions on Fundamental Vision Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Generalization of deep-learning-based (DL) computer vision algorithms to\nvarious image perturbations is hard to establish and remains an active area of\nresearch. The majority of past analyses focused on the images already captured,\nwhereas effects of the image formation pipeline and environment are less\nstudied. In this paper, we address this issue by analyzing the impact of\ncapture conditions, such as camera parameters and lighting, on DL model\nperformance on 3 vision tasks -- image classification, object detection, and\nvisual question answering (VQA). To this end, we assess capture bias in common\nvision datasets and create a new benchmark, SNAP (for $\\textbf{S}$hutter speed,\nISO se$\\textbf{N}$sitivity, and $\\textbf{AP}$erture), consisting of images of\nobjects taken under controlled lighting conditions and with densely sampled\ncamera settings. We then evaluate a large number of DL vision models and show\nthe effects of capture conditions on each selected vision task. Lastly, we\nconduct an experiment to establish a human baseline for the VQA task. Our\nresults show that computer vision datasets are significantly biased, the models\ntrained on this data do not reach human accuracy even on the well-exposed\nimages, and are susceptible to both major exposure changes and minute\nvariations of camera settings. Code and data can be found at\nhttps://github.com/ykotseruba/SNAP", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u50cf\u6355\u6349\u6761\u4ef6\uff08\u5982\u76f8\u673a\u53c2\u6570\u548c\u5149\u7167\uff09\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6SNAP\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5df2\u6355\u83b7\u56fe\u50cf\uff0c\u800c\u5ffd\u7565\u4e86\u56fe\u50cf\u5f62\u6210\u7ba1\u9053\u548c\u73af\u5883\u7684\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5e38\u89c1\u89c6\u89c9\u6570\u636e\u96c6\u4e2d\u7684\u6355\u6349\u504f\u5dee\uff0c\u521b\u5efaSNAP\u6570\u636e\u96c6\uff0c\u5e76\u5728\u63a7\u5236\u5149\u7167\u548c\u76f8\u673a\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u591a\u4e2aDL\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u89c6\u89c9\u6570\u636e\u96c6\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u6a21\u578b\u5728\u826f\u597d\u66dd\u5149\u56fe\u50cf\u4e0a\u4e5f\u65e0\u6cd5\u8fbe\u5230\u4eba\u7c7b\u51c6\u786e\u7387\uff0c\u4e14\u5bf9\u76f8\u673a\u8bbe\u7f6e\u53d8\u5316\u654f\u611f\u3002", "conclusion": "\u6355\u6349\u6761\u4ef6\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u51cf\u5c11\u504f\u5dee\u3002"}}
{"id": "2505.15637", "pdf": "https://arxiv.org/pdf/2505.15637", "abs": "https://arxiv.org/abs/2505.15637", "authors": ["Pujun Xue", "Junyi Ge", "Xiaotong Jiang", "Siyang Song", "Zijian Wu", "Yupeng Huo", "Weicheng Xie", "Linlin Shen", "Xiaoqin Zhou", "Xiaofeng Liu", "Min Gu"], "title": "Oral Imaging for Malocclusion Issues Assessments: OMNI Dataset, Deep Learning Baselines and Benchmarking", "categories": ["cs.CV"], "comment": null, "summary": "Malocclusion is a major challenge in orthodontics, and its complex\npresentation and diverse clinical manifestations make accurate localization and\ndiagnosis particularly important. Currently, one of the major shortcomings\nfacing the field of dental image analysis is the lack of large-scale,\naccurately labeled datasets dedicated to malocclusion issues, which limits the\ndevelopment of automated diagnostics in the field of dentistry and leads to a\nlack of diagnostic accuracy and efficiency in clinical practice. Therefore, in\nthis study, we propose the Oral and Maxillofacial Natural Images (OMNI)\ndataset, a novel and comprehensive dental image dataset aimed at advancing the\nstudy of analyzing dental images for issues of malocclusion. Specifically, the\ndataset contains 4166 multi-view images with 384 participants in data\ncollection and annotated by professional dentists. In addition, we performed a\ncomprehensive validation of the created OMNI dataset, including three CNN-based\nmethods, two Transformer-based methods, and one GNN-based method, and conducted\nautomated diagnostic experiments for malocclusion issues. The experimental\nresults show that the OMNI dataset can facilitate the automated diagnosis\nresearch of malocclusion issues and provide a new benchmark for the research in\nthis field. Our OMNI dataset and baseline code are publicly available at\nhttps://github.com/RoundFaceJ/OMNI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aOMNI\u7684\u65b0\u578b\u7259\u79d1\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63a8\u52a8\u9519\u988c\u7578\u5f62\u95ee\u9898\u7684\u81ea\u52a8\u8bca\u65ad\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7259\u79d1\u56fe\u50cf\u5206\u6790\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u51c6\u786e\u6807\u6ce8\u7684\u9519\u988c\u7578\u5f62\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u8bca\u65ad\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b4166\u5f20\u591a\u89c6\u89d2\u56fe\u50cf\u7684OMNI\u6570\u636e\u96c6\uff0c\u5e76\u7531\u4e13\u4e1a\u7259\u533b\u6807\u6ce8\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOMNI\u6570\u636e\u96c6\u80fd\u6709\u6548\u4fc3\u8fdb\u9519\u988c\u7578\u5f62\u7684\u81ea\u52a8\u8bca\u65ad\u7814\u7a76\u3002", "conclusion": "OMNI\u6570\u636e\u96c6\u4e3a\u9519\u988c\u7578\u5f62\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u548c\u4ee3\u7801\u3002"}}
{"id": "2505.15644", "pdf": "https://arxiv.org/pdf/2505.15644", "abs": "https://arxiv.org/abs/2505.15644", "authors": ["Zhen Sun", "Ziyi Zhang", "Zeren Luo", "Zeyang Sha", "Tianshuo Cong", "Zheng Li", "Shiwen Cui", "Weiqiang Wang", "Jiaheng Wei", "Xinlei He", "Qi Li", "Qian Wang"], "title": "FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "14pages,15 figures", "summary": "Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.", "AI": {"tldr": "\u63d0\u51faFragFake\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7f16\u8f91\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u5c40\u90e8\u7f16\u8f91\u68c0\u6d4b\u7684\u4e09\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u96be\u4ee5\u68c0\u6d4b\u5c40\u90e8\u7f16\u8f91\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5b9a\u4f4d\u80fd\u529b\u3001\u4f9d\u8d56\u6602\u8d35\u6807\u6ce8\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\u521b\u5efaFragFake\u6570\u636e\u96c6\uff0c\u5e76\u9996\u6b21\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7f16\u8f91\u5206\u7c7b\u548c\u533a\u57df\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5fae\u8c03\u540e\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u9996\u6b21\u5c06\u5c40\u90e8\u7f16\u8f91\u68c0\u6d4b\u91cd\u6784\u4e3a\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u4e3a\u591a\u6a21\u6001\u5185\u5bb9\u771f\u5b9e\u6027\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.15649", "pdf": "https://arxiv.org/pdf/2505.15649", "abs": "https://arxiv.org/abs/2505.15649", "authors": ["Tianjiao Cao", "Jiahao Lyu", "Weichao Zeng", "Weimin Mu", "Yu Zhou"], "title": "The Devil is in Fine-tuning and Long-tailed Problems:A New Benchmark for Scene Text Detection", "categories": ["cs.CV"], "comment": "Accepted by IJCAI2025", "summary": "Scene text detection has seen the emergence of high-performing methods that\nexcel on academic benchmarks. However, these detectors often fail to replicate\nsuch success in real-world scenarios. We uncover two key factors contributing\nto this discrepancy through extensive experiments. First, a \\textit{Fine-tuning\nGap}, where models leverage \\textit{Dataset-Specific Optimization} (DSO)\nparadigm for one domain at the cost of reduced effectiveness in others, leads\nto inflated performances on academic benchmarks. Second, the suboptimal\nperformance in practical settings is primarily attributed to the long-tailed\ndistribution of texts, where detectors struggle with rare and complex\ncategories as artistic or overlapped text. Given that the DSO paradigm might\nundermine the generalization ability of models, we advocate for a\n\\textit{Joint-Dataset Learning} (JDL) protocol to alleviate the Fine-tuning\nGap. Additionally, an error analysis is conducted to identify three major\ncategories and 13 subcategories of challenges in long-tailed scene text, upon\nwhich we propose a Long-Tailed Benchmark (LTB). LTB facilitates a comprehensive\nevaluation of ability to handle a diverse range of long-tailed challenges. We\nfurther introduce MAEDet, a self-supervised learning-based method, as a strong\nbaseline for LTB. The code is available at https://github.com/pd162/LTB.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u573a\u666f\u6587\u672c\u68c0\u6d4b\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u539f\u56e0\u5305\u62ec\u5fae\u8c03\u5dee\u8ddd\u548c\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u3002\u4f5c\u8005\u63d0\u51fa\u8054\u5408\u6570\u636e\u96c6\u5b66\u4e60\u534f\u8bae\u548c\u957f\u5c3e\u57fa\u51c6\uff08LTB\uff09\uff0c\u5e76\u5f15\u5165MAEDet\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u573a\u666f\u6587\u672c\u68c0\u6d4b\u5728\u5b66\u672f\u57fa\u51c6\u4e0e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8054\u5408\u6570\u636e\u96c6\u5b66\u4e60\uff08JDL\uff09\u534f\u8bae\u7f13\u89e3\u5fae\u8c03\u5dee\u8ddd\uff0c\u5e76\u6784\u5efa\u957f\u5c3e\u57fa\u51c6\uff08LTB\uff09\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\uff0c\u540c\u65f6\u5f15\u5165MAEDet\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5fae\u8c03\u5dee\u8ddd\u548c\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86LTB\u548cMAEDet\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u4e3a\u573a\u666f\u6587\u672c\u68c0\u6d4b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u6cdb\u5316\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.15671", "pdf": "https://arxiv.org/pdf/2505.15671", "abs": "https://arxiv.org/abs/2505.15671", "authors": ["Hamzeh Asgharnezhad", "Afshar Shamsi", "Roohallah Alizadehsani", "Arash Mohammadi", "Hamid Alinejad-Rokny"], "title": "Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 5 tables, 7 figures", "summary": "Knowing the uncertainty associated with the output of a deep neural network\nis of paramount importance in making trustworthy decisions, particularly in\nhigh-stakes fields like medical diagnosis and autonomous systems. Monte Carlo\nDropout (MCD) is a widely used method for uncertainty quantification, as it can\nbe easily integrated into various deep architectures. However, conventional MCD\noften struggles with providing well-calibrated uncertainty estimates. To\naddress this, we introduce innovative frameworks that enhances MCD by\nintegrating different search solutions namely Grey Wolf Optimizer (GWO),\nBayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an\nuncertainty-aware loss function, thereby improving the reliability of\nuncertainty quantification. We conduct comprehensive experiments using\ndifferent backbones, namely DenseNet121, ResNet50, and VGG16, on various\ndatasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic\ndataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3%\non average in terms of both conventional accuracy and uncertainty accuracy\nwhile achieving significantly better calibration. These results highlight the\npotential of our approach to enhance the trustworthiness of deep learning\nmodels in safety-critical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8499\u7279\u5361\u6d1bDropout\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u4f18\u5316\u7b97\u6cd5\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u533b\u7597\u8bca\u65ad\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfMCD\u65b9\u6cd5\u5728\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u7070\u72fc\u4f18\u5316\u5668\u3001\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u53ca\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u6539\u8fdb\u4e86MCD\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0c\u65b0\u65b9\u6cd5\u6bd4\u57fa\u7ebfMCD\u5728\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u51c6\u786e\u6027\u4e0a\u5e73\u5747\u63d0\u53472-3%\uff0c\u4e14\u6821\u51c6\u6548\u679c\u663e\u8457\u66f4\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2505.15687", "pdf": "https://arxiv.org/pdf/2505.15687", "abs": "https://arxiv.org/abs/2505.15687", "authors": ["Zhe Xu", "Cheng Jin", "Yihui Wang", "Ziyi Liu", "Hao Chen"], "title": "Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u8fb9\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u548c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u75c5\u7406\u56fe\u50cf\u7406\u89e3\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u8bca\u65ad\u573a\u666f\u4e2d\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u4e14\u75c5\u7406\u56fe\u50cf\u5c3a\u5bf8\u5927\u5bfc\u81f4\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u53cc\u8fb9\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e00\u4e2a\u5206\u652f\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u5e73\u5747\u63d0\u534741.7%\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e70.3%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2505.15703", "pdf": "https://arxiv.org/pdf/2505.15703", "abs": "https://arxiv.org/abs/2505.15703", "authors": ["Xiaodong Mei", "Sheng Wang", "Jie Cheng", "Yingbing Chen", "Dan Xu"], "title": "HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": "In submission", "summary": "Motion forecasting represents a critical challenge in autonomous driving\nsystems, requiring accurate prediction of surrounding agents' future\ntrajectories. While existing approaches predict future motion states with the\nextracted scene context feature from historical agent trajectories and road\nlayouts, they suffer from the information degradation during the scene feature\nencoding. To address the limitation, we propose HAMF, a novel motion\nforecasting framework that learns future motion representations with the scene\ncontext encoding jointly, to coherently combine the scene understanding and\nfuture motion state prediction. We first embed the observed agent states and\nmap information into 1D token sequences, together with the target multi-modal\nfuture motion features as a set of learnable tokens. Then we design a unified\nAttention-based encoder, which synergistically combines self-attention and\ncross-attention mechanisms to model the scene context information and aggregate\nfuture motion features jointly. Complementing the encoder, we implement the\nMamba module in the decoding stage to further preserve the consistency and\ncorrelations among the learned future motion representations, to generate the\naccurate and diverse final trajectories. Extensive experiments on Argoverse 2\nbenchmark demonstrate that our hybrid Attention-Mamba model achieves\nstate-of-the-art motion forecasting performance with the simple and lightweight\narchitecture.", "AI": {"tldr": "HAMF\u662f\u4e00\u79cd\u65b0\u578b\u8fd0\u52a8\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u573a\u666f\u4e0a\u4e0b\u6587\u7f16\u7801\u548c\u672a\u6765\u8fd0\u52a8\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u573a\u666f\u7279\u5f81\u7f16\u7801\u4e2d\u7684\u4fe1\u606f\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\u72b6\u6001\u65f6\uff0c\u7531\u4e8e\u573a\u666f\u7279\u5f81\u7f16\u7801\u4e2d\u7684\u4fe1\u606f\u9000\u5316\u95ee\u9898\uff0c\u5bfc\u81f4\u9884\u6d4b\u51c6\u786e\u6027\u53d7\u9650\u3002", "method": "HAMF\u5c06\u89c2\u5bdf\u5230\u7684\u4ee3\u7406\u72b6\u6001\u548c\u5730\u56fe\u4fe1\u606f\u5d4c\u51651D\u6807\u8bb0\u5e8f\u5217\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8054\u5408\u5efa\u6a21\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u672a\u6765\u8fd0\u52a8\u7279\u5f81\u3002\u89e3\u7801\u9636\u6bb5\u4f7f\u7528Mamba\u6a21\u5757\u4fdd\u6301\u8fd0\u52a8\u8868\u793a\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728Argoverse 2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHAMF\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u9884\u6d4b\u6027\u80fd\uff0c\u4e14\u67b6\u6784\u7b80\u5355\u8f7b\u91cf\u3002", "conclusion": "HAMF\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u672a\u6765\u8fd0\u52a8\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2505.15737", "pdf": "https://arxiv.org/pdf/2505.15737", "abs": "https://arxiv.org/abs/2505.15737", "authors": ["Zhuodong Jiang", "Haoran Wang", "Guoxi Huang", "Brett Seymour", "Nantheera Anantrasirichai"], "title": "RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 3 figures. Submitted to BMVC 2025", "summary": "Reconstructing high-fidelity underwater scenes remains a challenging task due\nto light absorption, scattering, and limited visibility inherent in aquatic\nenvironments. This paper presents an enhanced Gaussian Splatting-based\nframework that improves both the visual quality and geometric accuracy of deep\nunderwater rendering. We propose decoupled learning for RGB channels, guided by\nthe physics of underwater attenuation, to enable more accurate colour\nrestoration. To address sparse-view limitations and improve view consistency,\nwe introduce a frame interpolation strategy with a novel adaptive weighting\nscheme. Additionally, we introduce a new loss function aimed at reducing noise\nwhile preserving edges, which is essential for deep-sea content. We also\nrelease a newly collected dataset, Submerged3D, captured specifically in\ndeep-sea environments. Experimental results demonstrate that our framework\nconsistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB,\ndelivering superior perceptual quality and robustness, and offering promising\ndirections for marine robotics and underwater visual analytics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u548c\u5e27\u63d2\u503c\u7b56\u7565\uff0c\u63d0\u5347\u6c34\u4e0b\u573a\u666f\u91cd\u5efa\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6\u3002", "motivation": "\u6c34\u4e0b\u573a\u666f\u91cd\u5efa\u56e0\u5149\u7ebf\u5438\u6536\u3001\u6563\u5c04\u548c\u80fd\u89c1\u5ea6\u4f4e\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u89e3\u8026\u5b66\u4e60RGB\u901a\u9053\u3001\u5e27\u63d2\u503c\u7b56\u7565\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u51cf\u5c11\u566a\u58f0\u5e76\u4fdd\u6301\u8fb9\u7f18\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPSNR\u63d0\u5347\u8fbe1.90dB\uff0c\u89c6\u89c9\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6c34\u4e0b\u89c6\u89c9\u5206\u6790\u548c\u6d77\u6d0b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2505.15755", "pdf": "https://arxiv.org/pdf/2505.15755", "abs": "https://arxiv.org/abs/2505.15755", "authors": ["Weihao Xia", "Cengiz Oztireli"], "title": "Exploring The Visual Feature Space for Multimodal Neural Decoding", "categories": ["cs.CV"], "comment": "Project: https://weihaox.github.io/VINDEX", "summary": "The intrication of brain signals drives research that leverages multimodal AI\nto align brain modalities with visual and textual data for explainable\ndescriptions. However, most existing studies are limited to coarse\ninterpretations, lacking essential details on object descriptions, locations,\nattributes, and their relationships. This leads to imprecise and ambiguous\nreconstructions when using such cues for visual decoding. To address this, we\nanalyze different choices of vision feature spaces from pre-trained visual\ncomponents within Multimodal Large Language Models (MLLMs) and introduce a\nzero-shot multimodal brain decoding method that interacts with these models to\ndecode across multiple levels of granularities. % To assess a model's ability\nto decode fine details from brain signals, we propose the Multi-Granularity\nBrain Detail Understanding Benchmark (MG-BrainDub). This benchmark includes two\nkey tasks: detailed descriptions and salient question-answering, with metrics\nhighlighting key visual elements like objects, attributes, and relationships.\nOur approach enhances neural decoding precision and supports more accurate\nneuro-decoding applications. Code will be available at\nhttps://github.com/weihaox/VINDEX.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u591a\u6a21\u6001\u8111\u4fe1\u53f7\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u7ec6\u8282\u7406\u89e3\u57fa\u51c6\uff08MG-BrainDub\uff09\u63d0\u5347\u795e\u7ecf\u89e3\u7801\u7684\u7cbe\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u8111\u4fe1\u53f7\u7684\u89e3\u91ca\u8f83\u4e3a\u7c97\u7cd9\uff0c\u7f3a\u4e4f\u5bf9\u7269\u4f53\u63cf\u8ff0\u3001\u4f4d\u7f6e\u3001\u5c5e\u6027\u53ca\u5176\u5173\u7cfb\u7684\u7ec6\u8282\uff0c\u5bfc\u81f4\u89c6\u89c9\u89e3\u7801\u4e0d\u7cbe\u786e\u3002", "method": "\u5206\u6790\u9884\u8bad\u7ec3\u89c6\u89c9\u7ec4\u4ef6\u7684\u7279\u5f81\u7a7a\u95f4\u9009\u62e9\uff0c\u5f15\u5165\u96f6\u6837\u672c\u591a\u6a21\u6001\u8111\u89e3\u7801\u65b9\u6cd5\uff0c\u5e76\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ea4\u4e92\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u795e\u7ecf\u89e3\u7801\u7684\u7cbe\u786e\u6027\uff0c\u652f\u6301\u66f4\u51c6\u786e\u7684\u795e\u7ecf\u89e3\u7801\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7MG-BrainDub\u57fa\u51c6\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.15765", "pdf": "https://arxiv.org/pdf/2505.15765", "abs": "https://arxiv.org/abs/2505.15765", "authors": ["Kaizhi Zheng", "Ruijian Zhang", "Jing Gu", "Jie Yang", "Xin Eric Wang"], "title": "Constructing a 3D Town from a Single Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.", "AI": {"tldr": "3DTown\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5f20\u4fef\u89c6\u56fe\u751f\u6210\u9ad8\u8d28\u91cf3D\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u5e03\u5c40\u771f\u5b9e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf3D\u573a\u666f\u83b7\u53d6\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u800c\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u6269\u5c55\u5230\u5168\u573a\u666f\u65f6\u5b58\u5728\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u4f4e\u8d28\u91cf\u7f51\u683c\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u533a\u57df\u751f\u6210\u548c\u7a7a\u95f4\u611f\u77e53D\u4fee\u590d\uff0c\u5206\u89e3\u8f93\u5165\u56fe\u50cf\u4e3a\u91cd\u53e0\u533a\u57df\uff0c\u5229\u7528\u9884\u8bad\u7ec33D\u751f\u6210\u5668\u751f\u6210\u5404\u90e8\u5206\uff0c\u518d\u901a\u8fc7\u63a9\u7801\u4fee\u6b63\u6d41\u4fee\u590d\u7f3a\u5931\u51e0\u4f55\u3002", "result": "\u5728\u591a\u6837\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c3DTown\u5728\u51e0\u4f55\u8d28\u91cf\u3001\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u7eb9\u7406\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "3DTown\u8bc1\u660e\u901a\u8fc7\u65e0\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf3D\u573a\u666f\u3002"}}
{"id": "2505.15779", "pdf": "https://arxiv.org/pdf/2505.15779", "abs": "https://arxiv.org/abs/2505.15779", "authors": ["Chuanhao Li", "Jianwen Sun", "Yukang Feng", "Mingliang Zhai", "Yifan Chang", "Kaipeng Zhang"], "title": "IA-T2I: Internet-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 7 figures, a framework that integrates reference images\n  from the Internet into T2I/TI2I models", "summary": "Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u8054\u7f51\u589e\u5f3a\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff08IA-T2I\uff09\uff0c\u901a\u8fc7\u63d0\u4f9b\u53c2\u8003\u56fe\u50cf\u89e3\u51b3\u6a21\u578b\u5bf9\u4e0d\u786e\u5b9a\u77e5\u8bc6\u7684\u751f\u6210\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u63d0\u793a\u9690\u542b\u77e5\u8bc6\u4e0d\u786e\u5b9a\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f8b\u5982\u65e0\u6cd5\u751f\u6210\u672a\u6765\u4e8b\u4ef6\u7684\u56fe\u50cf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e3b\u52a8\u68c0\u7d22\u6a21\u5757\u3001\u5206\u5c42\u56fe\u50cf\u9009\u62e9\u6a21\u5757\u548c\u81ea\u53cd\u601d\u673a\u5236\uff0c\u4ee5\u589e\u5f3aT2I\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728Img-Ref-T2I\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u6846\u67b6\u6027\u80fd\u4f18\u4e8eGPT-4o\u7ea630%\u3002", "conclusion": "IA-T2I\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86T2I\u6a21\u578b\u5bf9\u4e0d\u786e\u5b9a\u77e5\u8bc6\u7684\u751f\u6210\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2505.15791", "pdf": "https://arxiv.org/pdf/2505.15791", "abs": "https://arxiv.org/abs/2505.15791", "authors": ["Fengyuan Dai", "Zifeng Zhuang", "Yufei Huang", "Siteng Huang", "Bangyan Liao", "Donglin Wang", "Fajie Yuan"], "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL", "categories": ["cs.CV", "cs.LG"], "comment": "Under review", "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.", "AI": {"tldr": "VARD\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ef7\u503c\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u548c\u4e0d\u53ef\u5fae\u5956\u52b1\u7684\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u7279\u5b9a\u5c5e\u6027\u4e0a\u7684\u4f18\u5316\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u7a33\u5b9a\u3001\u9ad8\u6548\u7684\u5fae\u8c03\u548c\u652f\u6301\u4e0d\u53ef\u5fae\u5956\u52b1\u3002", "method": "VARD\u901a\u8fc7\u5b66\u4e60\u9884\u6d4b\u5956\u52b1\u671f\u671b\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u5e76\u7ed3\u5408KL\u6b63\u5219\u5316\uff0c\u4e3a\u751f\u6210\u8fc7\u7a0b\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVARD\u80fd\u6709\u6548\u6307\u5bfc\u8f68\u8ff9\u3001\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u6269\u5c55\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u4e0d\u53ef\u5fae\u5956\u52b1\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "VARD\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u7a33\u5b9a\u3001\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u590d\u6742\u975e\u53ef\u5fae\u5956\u52b1\u51fd\u6570\u3002"}}
{"id": "2505.15800", "pdf": "https://arxiv.org/pdf/2505.15800", "abs": "https://arxiv.org/abs/2505.15800", "authors": ["Ruizhi Shao", "Yinghao Xu", "Yujun Shen", "Ceyuan Yang", "Yang Zheng", "Changan Chen", "Yebin Liu", "Gordon Wetzstein"], "title": "Interspatial Attention for Efficient 4D Human Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://dsaurus.github.io/isa4d/", "summary": "Generating photorealistic videos of digital humans in a controllable manner\nis crucial for a plethora of applications. Existing approaches either build on\nmethods that employ template-based 3D representations or emerging video\ngeneration models but suffer from poor quality or limited consistency and\nidentity preservation when generating individual or multiple digital humans. In\nthis paper, we introduce a new interspatial attention (ISA) mechanism as a\nscalable building block for modern diffusion transformer (DiT)--based video\ngeneration models. ISA is a new type of cross attention that uses relative\npositional encodings tailored for the generation of human videos. Leveraging a\ncustom-developed video variation autoencoder, we train a latent ISA-based\ndiffusion model on a large corpus of video data. Our model achieves\nstate-of-the-art performance for 4D human video synthesis, demonstrating\nremarkable motion consistency and identity preservation while providing precise\ncontrol of the camera and body poses. Our code and model are publicly released\nat https://dsaurus.github.io/isa4d/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff08ISA\uff09\uff0c\u7528\u4e8e\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b57\u4eba\u7c7b\u89c6\u9891\u7684\u751f\u6210\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u6570\u5b57\u4eba\u7c7b\u89c6\u9891\u65f6\u5b58\u5728\u8d28\u91cf\u4f4e\u3001\u4e00\u81f4\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165ISA\u673a\u5236\uff0c\u7ed3\u5408\u5b9a\u5236\u5f00\u53d1\u7684\u89c6\u9891\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u5728\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u4e0a\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002", "result": "\u6a21\u578b\u57284D\u4eba\u7c7b\u89c6\u9891\u5408\u6210\u4e2d\u8fbe\u5230SOTA\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8fd0\u52a8\u4e00\u81f4\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\uff0c\u540c\u65f6\u652f\u6301\u7cbe\u786e\u7684\u76f8\u673a\u548c\u8eab\u4f53\u59ff\u6001\u63a7\u5236\u3002", "conclusion": "ISA\u673a\u5236\u4e3a\u6570\u5b57\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u63a7\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.15804", "pdf": "https://arxiv.org/pdf/2505.15804", "abs": "https://arxiv.org/abs/2505.15804", "authors": ["Zongzhao Li", "Zongyang Ma", "Mingze Li", "Songyou Li", "Yu Rong", "Tingyang Xu", "Ziqi Zhang", "Deli Zhao", "Wenbing Huang"], "title": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.", "AI": {"tldr": "STAR-R1\u6846\u67b6\u901a\u8fc7\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u4eba\u7c7b\uff0c\u7814\u7a76\u65e8\u5728\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faSTAR-R1\u6846\u67b6\uff0c\u7ed3\u5408\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u5236\uff0c\u4f18\u5316\u63a2\u7d22\u548c\u63a8\u7406\u6548\u7387\u3002", "result": "STAR-R1\u572811\u9879\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u8de8\u89c6\u56fe\u573a\u666f\u4e0b\u6027\u80fd\u63d0\u534723%\u3002", "conclusion": "STAR-R1\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u63a8\u7406\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2505.15809", "pdf": "https://arxiv.org/pdf/2505.15809", "abs": "https://arxiv.org/abs/2505.15809", "authors": ["Ling Yang", "Ye Tian", "Bowen Li", "Xinchen Zhang", "Ke Shen", "Yunhai Tong", "Mengdi Wang"], "title": "MMaDA: Multimodal Large Diffusion Language Models", "categories": ["cs.CV"], "comment": "Project: https://github.com/Gen-Verse/MMaDA", "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA", "AI": {"tldr": "MMaDA\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u6269\u6563\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u3001\u6df7\u5408\u957f\u94fe\u601d\u7ef4\u5fae\u8c03\u548cUniGRPO\u7b97\u6cd5\uff0c\u5728\u6587\u672c\u63a8\u7406\u3001\u591a\u6a21\u6001\u7406\u89e3\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u7684\u65e0\u7f1d\u96c6\u6210\u548c\u590d\u6742\u4efb\u52a1\u5904\u7406\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u6269\u6563\u67b6\u6784\u3001\u6df7\u5408\u957f\u94fe\u601d\u7ef4\u5fae\u8c03\u7b56\u7565\u548cUniGRPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "MMaDA-8B\u5728\u6587\u672c\u63a8\u7406\u3001\u591a\u6a21\u6001\u7406\u89e3\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8d85\u8d8a\u591a\u4e2a\u5f3a\u5927\u6a21\u578b\u3002", "conclusion": "MMaDA\u4e3a\u7edf\u4e00\u6269\u6563\u67b6\u6784\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6846\u67b6\uff0c\u672a\u6765\u7814\u7a76\u6f5c\u529b\u5de8\u5927\u3002"}}
{"id": "2505.15812", "pdf": "https://arxiv.org/pdf/2505.15812", "abs": "https://arxiv.org/abs/2505.15812", "authors": ["Satoshi Kosugi"], "title": "Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization", "categories": ["cs.CV"], "comment": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "summary": "Exemplar-based image colorization aims to colorize a grayscale image using a\nreference color image, ensuring that reference colors are applied to\ncorresponding input regions based on their semantic similarity. To achieve\naccurate semantic matching between regions, we leverage the self-attention\nmodule of a pre-trained diffusion model, which is trained on a large dataset\nand exhibits powerful attention capabilities. To harness this power, we propose\na novel, fine-tuning-free approach based on a pre-trained diffusion model,\nmaking two key contributions. First, we introduce dual attention-guided color\ntransfer. We utilize the self-attention module to compute an attention map\nbetween the input and reference images, effectively capturing semantic\ncorrespondences. The color features from the reference image is then\ntransferred to the semantically matching regions of the input image, guided by\nthis attention map, and finally, the grayscale features are replaced with the\ncorresponding color features. Notably, we utilize dual attention to calculate\nattention maps separately for the grayscale and color images, achieving more\nprecise semantic alignment. Second, we propose classifier-free colorization\nguidance, which enhances the transferred colors by combining color-transferred\nand non-color-transferred outputs. This process improves the quality of\ncolorization. Our experimental results demonstrate that our method outperforms\nexisting techniques in terms of image quality and fidelity to the reference.\nSpecifically, we use 335 input-reference pairs from previous research,\nachieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to\nthe reference). Our source code is available at\nhttps://github.com/satoshi-kosugi/powerful-attention.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7740\u8272\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u6ce8\u610f\u529b\u5f15\u5bfc\u989c\u8272\u8f6c\u79fb\u548c\u65e0\u5206\u7c7b\u5668\u7740\u8272\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7740\u8272\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u793a\u4f8b\u7684\u56fe\u50cf\u7740\u8272\u4e2d\u8bed\u4e49\u5339\u914d\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u6ce8\u610f\u529b\u80fd\u529b\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u8ba1\u7b97\u8f93\u5165\u4e0e\u53c2\u8003\u56fe\u50cf\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u901a\u8fc7\u53cc\u91cd\u6ce8\u610f\u529b\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u65e0\u5206\u7c7b\u5668\u7740\u8272\u6307\u5bfc\u63d0\u5347\u989c\u8272\u8f6c\u79fb\u8d28\u91cf\u3002", "result": "\u5728335\u5bf9\u8f93\u5165-\u53c2\u8003\u56fe\u50cf\u4e0a\u6d4b\u8bd5\uff0cFID\u4e3a95.27\uff08\u56fe\u50cf\u8d28\u91cf\uff09\uff0cSI-FID\u4e3a5.51\uff08\u53c2\u8003\u4fdd\u771f\u5ea6\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5fae\u8c03\uff0c\u901a\u8fc7\u53cc\u91cd\u6ce8\u610f\u529b\u548c\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7740\u8272\u7684\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2505.15814", "pdf": "https://arxiv.org/pdf/2505.15814", "abs": "https://arxiv.org/abs/2505.15814", "authors": ["Federica Arrigoni"], "title": "A Taxonomy of Structure from Motion Methods", "categories": ["cs.CV"], "comment": null, "summary": "Structure from Motion (SfM) refers to the problem of recovering both\nstructure (i.e., 3D coordinates of points in the scene) and motion (i.e.,\ncamera matrices) starting from point correspondences in multiple images. It has\nattracted significant attention over the years, counting practical\nreconstruction pipelines as well as theoretical results. This paper is\nconceived as a conceptual review of SfM methods, which are grouped into three\nmain categories, according to which part of the problem - between motion and\nstructure - they focus on. The proposed taxonomy brings a new perspective on\nexisting SfM approaches as well as insights into open problems and possible\nfuture research directions. Particular emphasis is given on identifying the\ntheoretical conditions that make SfM well posed, which depend on the problem\nformulation that is being considered.", "AI": {"tldr": "\u672c\u6587\u662f\u5bf9\u7ed3\u6784\u4ece\u8fd0\u52a8\uff08SfM\uff09\u65b9\u6cd5\u7684\u5206\u7c7b\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u89c6\u89d2\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f00\u653e\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "SfM\u95ee\u9898\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7684\u5206\u7c7b\u89c6\u89d2\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\u5e76\u5206\u6790\u5176\u7406\u8bba\u6761\u4ef6\u3002", "method": "\u5c06SfM\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff0c\u6839\u636e\u5176\u5173\u6ce8\u7684\u662f\u8fd0\u52a8\u3001\u7ed3\u6784\u8fd8\u662f\u4e24\u8005\u3002\u91cd\u70b9\u5206\u6790\u4e86\u4e0d\u540c\u95ee\u9898\u8868\u8ff0\u4e0b\u7684\u7406\u8bba\u6761\u4ef6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SfM\u5206\u7c7b\u89c6\u89d2\uff0c\u5e76\u660e\u786e\u4e86\u4e0d\u540c\u95ee\u9898\u8868\u8ff0\u4e0b\u7684\u7406\u8bba\u6761\u4ef6\u3002", "conclusion": "\u672c\u6587\u4e3aSfM\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u5f00\u653e\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u7406\u8bba\u6761\u4ef6\u65b9\u9762\u7684\u63a2\u7d22\u3002"}}
{"id": "2505.15816", "pdf": "https://arxiv.org/pdf/2505.15816", "abs": "https://arxiv.org/abs/2505.15816", "authors": ["Penghao Wu", "Lewei Lu", "Ziwei Liu"], "title": "Streamline Without Sacrifice -- Squeeze out Computation Redundancy in LMM", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Large multimodal models excel in multimodal tasks but face significant\ncomputational challenges due to excessive computation on visual tokens. Unlike\ntoken reduction methods that focus on token-level redundancy, we identify and\nstudy the computation-level redundancy on vision tokens to ensure no\ninformation loss. Our key insight is that vision tokens from the pretrained\nvision encoder do not necessarily require all the heavy operations (e.g.,\nself-attention, FFNs) in decoder-only LMMs and could be processed more lightly\nwith proper designs. We designed a series of experiments to discover and\nprogressively squeeze out the vision-related computation redundancy. Based on\nour findings, we propose ProxyV, a novel approach that utilizes proxy vision\ntokens to alleviate the computational burden on original vision tokens. ProxyV\nenhances efficiency without compromising performance and can even yield notable\nperformance gains in scenarios with more moderate efficiency improvements.\nFurthermore, the flexibility of ProxyV is demonstrated through its combination\nwith token reduction methods to boost efficiency further. The code will be made\npublic at this https://github.com/penghao-wu/ProxyV URL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faProxyV\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7406\u89c6\u89c9\u4ee4\u724c\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\uff0c\u63d0\u5347\u6548\u7387\u4e14\u4e0d\u635f\u5931\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u4ee4\u724c\u4e0a\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4ee4\u724c\u7ea7\u5197\u4f59\uff0c\u800c\u672c\u6587\u7814\u7a76\u8ba1\u7b97\u7ea7\u5197\u4f59\u3002", "method": "\u8bbe\u8ba1\u5b9e\u9a8c\u53d1\u73b0\u89c6\u89c9\u4ee4\u724c\u8ba1\u7b97\u5197\u4f59\uff0c\u63d0\u51faProxyV\u65b9\u6cd5\uff0c\u7528\u4ee3\u7406\u4ee4\u724c\u51cf\u8f7b\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "ProxyV\u63d0\u5347\u6548\u7387\u4e14\u4e0d\u635f\u5931\u6027\u80fd\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ProxyV\u7075\u6d3b\u9ad8\u6548\uff0c\u53ef\u4e0e\u4ee4\u724c\u7f29\u51cf\u65b9\u6cd5\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2505.15818", "pdf": "https://arxiv.org/pdf/2505.15818", "abs": "https://arxiv.org/abs/2505.15818", "authors": ["Yijie Zheng", "Weijie Wu", "Qingyun Li", "Xuehui Wang", "Xu Zhou", "Aiai Ren", "Jun Shen", "Long Zhao", "Guoqing Li", "Xue Yang"], "title": "InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Language-Guided object recognition in remote sensing imagery is crucial for\nlarge-scale mapping and automated data annotation. However, existing\nopen-vocabulary and visual grounding methods rely on explicit category cues,\nlimiting their ability to handle complex or implicit queries that require\nadvanced reasoning. To address this issue, we introduce a new suite of tasks,\nincluding Instruction-Oriented Object Counting, Detection, and Segmentation\n(InstructCDS), covering open-vocabulary, open-ended, and open-subclass\nscenarios. We further present EarthInstruct, the first InstructCDS benchmark\nfor earth observation. It is constructed from two diverse remote sensing\ndatasets with varying spatial resolutions and annotation rules across 20\ncategories, necessitating models to interpret dataset-specific instructions.\nGiven the scarcity of semantically rich labeled data in remote sensing, we\npropose InstructSAM, a training-free framework for instruction-driven object\nrecognition. InstructSAM leverages large vision-language models to interpret\nuser instructions and estimate object counts, employs SAM2 for mask proposal,\nand formulates mask-label assignment as a binary integer programming problem.\nBy integrating semantic similarity with counting constraints, InstructSAM\nefficiently assigns categories to predicted masks without relying on confidence\nthresholds. Experiments demonstrate that InstructSAM matches or surpasses\nspecialized baselines across multiple tasks while maintaining near-constant\ninference time regardless of object count, reducing output tokens by 89% and\noverall runtime by over 32% compared to direct generation approaches. We\nbelieve the contributions of the proposed tasks, benchmark, and effective\napproach will advance future research in developing versatile object\nrecognition systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faInstructCDS\u4efb\u52a1\u5957\u4ef6\u548cEarthInstruct\u57fa\u51c6\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u8bed\u8a00\u5f15\u5bfc\u5bf9\u8c61\u8bc6\u522b\uff0c\u5e76\u5f00\u53d1\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6InstructSAM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u7c7b\u522b\u63d0\u793a\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u6216\u9690\u5f0f\u67e5\u8be2\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u652f\u6301\u66f4\u7075\u6d3b\u7684\u5bf9\u8c61\u8bc6\u522b\u3002", "method": "\u63d0\u51faInstructCDS\u4efb\u52a1\u548cEarthInstruct\u57fa\u51c6\uff1b\u5f00\u53d1InstructSAM\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cSAM2\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u6574\u6570\u7f16\u7a0b\u5206\u914d\u63a9\u7801\u6807\u7b7e\u3002", "result": "InstructSAM\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a8\u7406\u65f6\u95f4\u7a33\u5b9a\uff0c\u8f93\u51fa\u6807\u8bb0\u51cf\u5c1189%\uff0c\u8fd0\u884c\u65f6\u95f4\u964d\u4f4e32%\u3002", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u3001\u57fa\u51c6\u548c\u6846\u67b6\u4e3a\u5f00\u53d1\u591a\u529f\u80fd\u5bf9\u8c61\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2505.14715", "pdf": "https://arxiv.org/pdf/2505.14715", "abs": "https://arxiv.org/abs/2505.14715", "authors": ["Muhammad Zubair", "Muzammil Hussai", "Mousa Ahmad Al-Bashrawi", "Malika Bendechache", "Muhammad Owais"], "title": "A Comprehensive Review of Techniques, Algorithms, Advancements, Challenges, and Clinical Applications of Multi-modal Medical Image Fusion for Improved Diagnosis", "categories": ["eess.IV", "cs.CV"], "comment": "computerized medical imaging and graphics Journal submission", "summary": "Multi-modal medical image fusion (MMIF) is increasingly recognized as an\nessential technique for enhancing diagnostic precision and facilitating\neffective clinical decision-making within computer-aided diagnosis systems.\nMMIF combines data from X-ray, MRI, CT, PET, SPECT, and ultrasound to create\ndetailed, clinically useful images of patient anatomy and pathology. These\nintegrated representations significantly advance diagnostic accuracy, lesion\ndetection, and segmentation. This comprehensive review meticulously surveys the\nevolution, methodologies, algorithms, current advancements, and clinical\napplications of MMIF. We present a critical comparative analysis of traditional\nfusion approaches, including pixel-, feature-, and decision-level methods, and\ndelves into recent advancements driven by deep learning, generative models, and\ntransformer-based architectures. A critical comparative analysis is presented\nbetween these conventional methods and contemporary techniques, highlighting\ndifferences in robustness, computational efficiency, and interpretability. The\narticle addresses extensive clinical applications across oncology, neurology,\nand cardiology, demonstrating MMIF's vital role in precision medicine through\nimproved patient-specific therapeutic outcomes. Moreover, the review thoroughly\ninvestigates the persistent challenges affecting MMIF's broad adoption,\nincluding issues related to data privacy, heterogeneity, computational\ncomplexity, interpretability of AI-driven algorithms, and integration within\nclinical workflows. It also identifies significant future research avenues,\nsuch as the integration of explainable AI, adoption of privacy-preserving\nfederated learning frameworks, development of real-time fusion systems, and\nstandardization efforts for regulatory compliance.", "AI": {"tldr": "\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\uff08MMIF\uff09\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6210\u50cf\u6280\u672f\u63d0\u5347\u8bca\u65ad\u7cbe\u5ea6\uff0c\u672c\u6587\u7efc\u8ff0\u4e86\u5176\u65b9\u6cd5\u3001\u8fdb\u5c55\u53ca\u4e34\u5e8a\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "MMIF\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u4e2d\u5bf9\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u9700\u603b\u7ed3\u5176\u6280\u672f\u53d1\u5c55\u4e0e\u5e94\u7528\u3002", "method": "\u7efc\u8ff0\u4e86\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\uff08\u50cf\u7d20\u3001\u7279\u5f81\u3001\u51b3\u7b56\u7ea7\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u548c\u751f\u6210\u6a21\u578b\u7b49\u73b0\u4ee3\u6280\u672f\uff0c\u5e76\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5206\u6790\u3002", "result": "MMIF\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3001\u75c5\u7076\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u80bf\u7624\u5b66\u3001\u795e\u7ecf\u5b66\u548c\u5fc3\u810f\u75c5\u5b66\u3002", "conclusion": "MMIF\u9762\u4e34\u6570\u636e\u9690\u79c1\u3001\u8ba1\u7b97\u590d\u6742\u6027\u7b49\u6311\u6218\uff0c\u672a\u6765\u9700\u5173\u6ce8\u53ef\u89e3\u91caAI\u3001\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\u548c\u5b9e\u65f6\u878d\u5408\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.14716", "pdf": "https://arxiv.org/pdf/2505.14716", "abs": "https://arxiv.org/abs/2505.14716", "authors": ["Sahil Tomar", "Rajeshwar Tripathi", "Sandeep Kumar"], "title": "A Hybrid Quantum Classical Pipeline for X Ray Based Fracture Diagnosis", "categories": ["eess.IV", "cs.CV", "cs.ET", "cs.IT", "cs.LG", "math.IT"], "comment": "8 pages", "summary": "Bone fractures are a leading cause of morbidity and disability worldwide,\nimposing significant clinical and economic burdens on healthcare systems.\nTraditional X ray interpretation is time consuming and error prone, while\nexisting machine learning and deep learning solutions often demand extensive\nfeature engineering, large, annotated datasets, and high computational\nresources. To address these challenges, a distributed hybrid quantum classical\npipeline is proposed that first applies Principal Component Analysis (PCA) for\ndimensionality reduction and then leverages a 4 qubit quantum amplitude\nencoding circuit for feature enrichment. By fusing eight PCA derived features\nwith eight quantum enhanced features into a 16 dimensional vector and then\nclassifying with different machine learning models achieving 99% accuracy using\na public multi region X ray dataset on par with state of the art transfer\nlearning models while reducing feature extraction time by 82%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u7ba1\u9053\uff0c\u7528\u4e8e\u9aa8\u9aa8\u6298X\u5c04\u7ebf\u5206\u7c7b\uff0c\u7ed3\u5408PCA\u548c\u91cf\u5b50\u632f\u5e45\u7f16\u7801\uff0c\u8fbe\u523099%\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u7279\u5f81\u63d0\u53d6\u65f6\u95f482%\u3002", "motivation": "\u4f20\u7edfX\u5c04\u7ebf\u5206\u6790\u8017\u65f6\u4e14\u6613\u9519\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u4f7f\u7528PCA\u964d\u7ef4\uff0c\u7ed3\u54084\u91cf\u5b50\u6bd4\u7279\u632f\u5e45\u7f16\u7801\u7535\u8def\u589e\u5f3a\u7279\u5f81\uff0c\u878d\u5408\u4e3a16\u7ef4\u5411\u91cf\u540e\u7528\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u7c7b\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\u51c6\u786e\u7387\uff0c\u4e0e\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\uff0c\u7279\u5f81\u63d0\u53d6\u65f6\u95f4\u51cf\u5c1182%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u4e3a\u9aa8\u9aa8\u6298\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.14717", "pdf": "https://arxiv.org/pdf/2505.14717", "abs": "https://arxiv.org/abs/2505.14717", "authors": ["Xigui Li", "Yuanye Zhou", "Feiyang Xiao", "Xin Guo", "Chen Jiang", "Tan Pan", "Xingmeng Zhang", "Cenyu Liu", "Zeyun Miao", "Jianchao Ge", "Xiansheng Wang", "Qimeng Wang", "Yichi Zhang", "Wenbo Zhang", "Fengping Zhu", "Limei Han", "Yuan Qi", "Chensen Lin", "Yuan Cheng"], "title": "Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Intracranial aneurysms (IAs) are serious cerebrovascular lesions found in\napproximately 5\\% of the general population. Their rupture may lead to high\nmortality. Current methods for assessing IA risk focus on morphological and\npatient-specific factors, but the hemodynamic influences on IA development and\nrupture remain unclear. While accurate for hemodynamic studies, conventional\ncomputational fluid dynamics (CFD) methods are computationally intensive,\nhindering their deployment in large-scale or real-time clinical applications.\nTo address this challenge, we curated a large-scale, high-fidelity aneurysm CFD\ndataset to facilitate the development of efficient machine learning algorithms\nfor such applications. Based on 427 real aneurysm geometries, we synthesized\n10,660 3D shapes via controlled deformation to simulate aneurysm evolution. The\nauthenticity of these synthetic shapes was confirmed by neurosurgeons. CFD\ncomputations were performed on each shape under eight steady-state mass flow\nconditions, generating a total of 85,280 blood flow dynamics data covering key\nparameters. Furthermore, the dataset includes segmentation masks, which can\nsupport tasks that use images, point clouds or other multimodal data as input.\nAdditionally, we introduced a benchmark for estimating flow parameters to\nassess current modeling methods. This dataset aims to advance aneurysm research\nand promote data-driven approaches in biofluids, biomedical engineering, and\nclinical risk assessment. The code and dataset are available at:\nhttps://github.com/Xigui-Li/Aneumo.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u4fdd\u771f\u7684\u9885\u5185\u52a8\u8109\u7624CFD\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u652f\u6301\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u5f00\u53d1\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfCFD\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u9885\u5185\u52a8\u8109\u7624\uff08IAs\uff09\u7834\u88c2\u53ef\u80fd\u5bfc\u81f4\u9ad8\u6b7b\u4ea1\u7387\uff0c\u4f46\u76ee\u524d\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5f62\u6001\u5b66\u548c\u60a3\u8005\u7279\u5f02\u6027\u56e0\u7d20\uff0c\u8840\u6d41\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002\u4f20\u7edfCFD\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6216\u5b9e\u65f6\u4e34\u5e8a\u573a\u666f\u3002", "method": "\u57fa\u4e8e427\u4e2a\u771f\u5b9e\u52a8\u8109\u7624\u51e0\u4f55\u5f62\u72b6\uff0c\u901a\u8fc7\u53d7\u63a7\u53d8\u5f62\u5408\u6210\u4e8610,660\u4e2a3D\u5f62\u72b6\u6a21\u62df\u52a8\u8109\u7624\u6f14\u53d8\u3002\u6bcf\u4e2a\u5f62\u72b6\u57288\u79cd\u7a33\u6001\u8d28\u91cf\u6d41\u6761\u4ef6\u4e0b\u8fdb\u884cCFD\u8ba1\u7b97\uff0c\u751f\u621085,280\u4e2a\u8840\u6d41\u52a8\u529b\u5b66\u6570\u636e\u3002\u6570\u636e\u96c6\u8fd8\u5305\u62ec\u5206\u5272\u63a9\u7801\uff0c\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\u4efb\u52a1\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b\u8840\u6d41\u52a8\u529b\u5b66\u5173\u952e\u53c2\u6570\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u5916\u79d1\u533b\u751f\u9a8c\u8bc1\u4e86\u5408\u6210\u5f62\u72b6\u7684\u771f\u5b9e\u6027\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u8bc4\u4f30\u5f53\u524d\u5efa\u6a21\u65b9\u6cd5\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u65e8\u5728\u63a8\u52a8\u52a8\u8109\u7624\u7814\u7a76\uff0c\u4fc3\u8fdb\u751f\u7269\u6d41\u4f53\u3001\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u548c\u4e34\u5e8a\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.14722", "pdf": "https://arxiv.org/pdf/2505.14722", "abs": "https://arxiv.org/abs/2505.14722", "authors": ["Pierre-Marc Jodoin", "Manon Edde", "Gabriel Girard", "F\u00e9lix Dumais", "Guillaume Theaud", "Matthieu Dumont", "Jean-Christophe Houde", "Yoan David", "Maxime Descoteaux"], "title": "ComBAT Harmonization for diffusion MRI: Challenges and Best Practices", "categories": ["stat.AP", "cs.CV", "cs.LG", "physics.med-ph"], "comment": null, "summary": "Over the years, ComBAT has become the standard method for harmonizing\nMRI-derived measurements, with its ability to compensate for site-related\nadditive and multiplicative biases while preserving biological variability.\nHowever, ComBAT relies on a set of assumptions that, when violated, can result\nin flawed harmonization. In this paper, we thoroughly review ComBAT's\nmathematical foundation, outlining these assumptions, and exploring their\nimplications for the demographic composition necessary for optimal results.\n  Through a series of experiments involving a slightly modified version of\nComBAT called Pairwise-ComBAT tailored for normative modeling applications, we\nassess the impact of various population characteristics, including population\nsize, age distribution, the absence of certain covariates, and the magnitude of\nadditive and multiplicative factors. Based on these experiments, we present\nfive essential recommendations that should be carefully considered to enhance\nconsistency and supporting reproducibility, two essential factors for open\nscience, collaborative research, and real-life clinical deployment.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86ComBAT\u7684\u6570\u5b66\u57fa\u7840\u53ca\u5176\u5047\u8bbe\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4eba\u53e3\u7279\u5f81\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u9879\u6539\u8fdb\u5efa\u8bae\u4ee5\u589e\u5f3a\u4e00\u81f4\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "ComBAT\u662fMRI\u6570\u636e\u6807\u51c6\u5316\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u4f46\u5176\u5047\u8bbe\u53ef\u80fd\u88ab\u8fdd\u53cd\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u51c6\u786e\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e9b\u5047\u8bbe\u7684\u5f71\u54cd\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u4f7f\u7528\u6539\u8fdb\u7248\u7684Pairwise-ComBAT\uff0c\u8bc4\u4f30\u4eba\u53e3\u7279\u5f81\uff08\u5982\u6837\u672c\u91cf\u3001\u5e74\u9f84\u5206\u5e03\u7b49\uff09\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u4eba\u53e3\u7279\u5f81\u5bf9ComBAT\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u9879\u5173\u952e\u5efa\u8bae\u3002", "conclusion": "\u4e94\u9879\u5efa\u8bae\u53ef\u63d0\u5347ComBAT\u7684\u4e00\u81f4\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u652f\u6301\u5f00\u653e\u79d1\u5b66\u548c\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2505.14726", "pdf": "https://arxiv.org/pdf/2505.14726", "abs": "https://arxiv.org/abs/2505.14726", "authors": ["Manshi Limbu", "Diwita Banerjee"], "title": "MedBLIP: Fine-tuning BLIP for Medical Image Captioning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image captioning is a challenging task that requires generating\nclinically accurate and semantically meaningful descriptions of radiology\nimages. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini\nand ViT-GPT2 show strong performance on natural image datasets, they often\nproduce generic or imprecise captions when applied to specialized medical\ndomains. In this project, we explore the effectiveness of fine-tuning the BLIP\nmodel on the ROCO dataset for improved radiology captioning. We compare the\nfine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and\na ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific\nfine-tuning on BLIP significantly improves performance across both quantitative\nand qualitative evaluation metrics. We also visualize decoder cross-attention\nmaps to assess interpretability and conduct an ablation study to evaluate the\ncontributions of encoder-only and decoder-only fine-tuning. Our findings\nhighlight the importance of targeted adaptation for medical applications and\nsuggest that decoder-only fine-tuning (encoder-frozen) offers a strong\nperformance baseline with 5% lower training time than full fine-tuning, while\nfull model fine-tuning still yields the best results overall.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u533b\u5b66\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u5fae\u8c03BLIP\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u751f\u6210\u7684\u63cf\u8ff0\u901a\u5e38\u4e0d\u591f\u51c6\u786e\u6216\u8fc7\u4e8e\u901a\u7528\uff0c\u9700\u8981\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "method": "\u5728ROCO\u6570\u636e\u96c6\u4e0a\u5fae\u8c03BLIP\u6a21\u578b\uff0c\u5e76\u4e0e\u96f6\u6837\u672c\u7248\u672c\u3001BLIP-2\u3001ViT-GPT2\u7b49\u57fa\u7ebf\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "\u5fae\u8c03\u540e\u7684BLIP\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u89e3\u7801\u5668\u5355\u72ec\u5fae\u8c03\uff08\u7f16\u7801\u5668\u51bb\u7ed3\uff09\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u5bf9\u533b\u5b66\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5168\u6a21\u578b\u5fae\u8c03\u6548\u679c\u6700\u4f73\uff0c\u4f46\u89e3\u7801\u5668\u5355\u72ec\u5fae\u8c03\u662f\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.14730", "pdf": "https://arxiv.org/pdf/2505.14730", "abs": "https://arxiv.org/abs/2505.14730", "authors": ["Hikmat Khan", "Ziyu Su", "Huina Zhang", "Yihong Wang", "Bohan Ning", "Shi Wei", "Hua Guo", "Zaibo Li", "Muhammad Khalid Khan Niazi"], "title": "Predicting Neo-Adjuvant Chemotherapy Response in Triple-Negative Breast Cancer Using Pre-Treatment Histopathologic Images", "categories": ["q-bio.QM", "cs.CV", "eess.IV"], "comment": null, "summary": "Triple-negative breast cancer (TNBC) is an aggressive subtype defined by the\nlack of estrogen receptor (ER), progesterone receptor (PR), and human epidermal\ngrowth factor receptor 2 (HER2) expression, resulting in limited targeted\ntreatment options. Neoadjuvant chemotherapy (NACT) is the standard treatment\nfor early-stage TNBC, with pathologic complete response (pCR) serving as a key\nprognostic marker; however, only 40-50% of patients with TNBC achieve pCR.\nAccurate prediction of NACT response is crucial to optimize therapy, avoid\nineffective treatments, and improve patient outcomes. In this study, we\ndeveloped a deep learning model to predict NACT response using pre-treatment\nhematoxylin and eosin (H&E)-stained biopsy images. Our model achieved promising\nresults in five-fold cross-validation (accuracy: 82%, AUC: 0.86, F1-score:\n0.84, sensitivity: 0.85, specificity: 0.81, precision: 0.80). Analysis of model\nattention maps in conjunction with multiplexed immunohistochemistry (mIHC) data\nrevealed that regions of high predictive importance consistently colocalized\nwith tumor areas showing elevated PD-L1 expression, CD8+ T-cell infiltration,\nand CD163+ macrophage density - all established biomarkers of treatment\nresponse. Our findings indicate that incorporating IHC-derived immune profiling\ndata could substantially improve model interpretability and predictive\nperformance. Furthermore, this approach may accelerate the discovery of novel\nhistopathological biomarkers for NACT and advance the development of\npersonalized treatment strategies for TNBC patients.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u6cbb\u7597\u524d\u7684H&E\u67d3\u8272\u6d3b\u68c0\u56fe\u50cf\u9884\u6d4bTNBC\u60a3\u8005\u5bf9\u65b0\u8f85\u52a9\u5316\u7597\uff08NACT\uff09\u7684\u53cd\u5e94\uff0c\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u514d\u75ab\u751f\u7269\u6807\u5fd7\u7269\u76f8\u5173\u7684\u9884\u6d4b\u533a\u57df\u3002", "motivation": "TNBC\u56e0\u7f3a\u4e4f\u9776\u5411\u6cbb\u7597\u9009\u9879\uff0cNACT\u53cd\u5e94\u9884\u6d4b\u5bf9\u4f18\u5316\u6cbb\u7597\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u6790H&E\u67d3\u8272\u6d3b\u68c0\u56fe\u50cf\uff0c\u7ed3\u5408mIHC\u6570\u636e\u9a8c\u8bc1\u9884\u6d4b\u533a\u57df\u3002", "result": "\u6a21\u578b\u5728\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u8868\u73b0\u4f18\u5f02\uff08\u51c6\u786e\u738782%\uff0cAUC 0.86\uff09\uff0c\u9884\u6d4b\u533a\u57df\u4e0ePD-L1\u8868\u8fbe\u3001CD8+ T\u7ec6\u80de\u6d78\u6da6\u7b49\u751f\u7269\u6807\u5fd7\u7269\u76f8\u5173\u3002", "conclusion": "\u7ed3\u5408IHC\u514d\u75ab\u5206\u6790\u6570\u636e\u53ef\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u80fd\uff0c\u4e3aTNBC\u4e2a\u6027\u5316\u6cbb\u7597\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.14747", "pdf": "https://arxiv.org/pdf/2505.14747", "abs": "https://arxiv.org/abs/2505.14747", "authors": ["Fatemeh Chajaei", "Hossein Bagheri"], "title": "LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Three-dimensional reconstruction of buildings, particularly at Level of\nDetail 1 (LOD1), plays a crucial role in various applications such as urban\nplanning, urban environmental studies, and designing optimized transportation\nnetworks. This study focuses on assessing the potential of LiDAR data for\naccurate 3D building reconstruction at LOD1 and extracting morphological\nfeatures from these models. Four deep semantic segmentation models, U-Net,\nAttention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learning\nto extract building footprints from LiDAR data. The results showed that U-Net3+\nand Attention U-Net outperformed the others, achieving IoU scores of 0.833 and\n0.814, respectively. Various statistical measures, including maximum, range,\nmode, median, and the 90th percentile, were used to estimate building heights,\nresulting in the generation of 3D models at LOD1. As the main contribution of\nthe research, the impact of segmentation accuracy on the quality of 3D building\nmodeling and the accuracy of morphological features like building area and\nexternal wall surface area was investigated. The results showed that the\naccuracy of building identification (segmentation performance) significantly\naffects the 3D model quality and the estimation of morphological features,\ndepending on the height calculation method. Overall, the UNet3+ method,\nutilizing the 90th percentile and median measures, leads to accurate height\nestimation of buildings and the extraction of morphological features.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86LiDAR\u6570\u636e\u5728LOD1\u7ea7\u522b\u5efa\u7b51\u72693D\u91cd\u5efa\u4e2d\u7684\u6f5c\u529b\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53d1\u73b0U-Net3+\u548cAttention U-Net\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63a2\u8ba8\u4e86\u5206\u5272\u7cbe\u5ea6\u5bf93D\u5efa\u6a21\u548c\u5f62\u6001\u7279\u5f81\u63d0\u53d6\u7684\u5f71\u54cd\u3002", "motivation": "\u5efa\u7b51\u72693D\u91cd\u5efa\u5728\u57ce\u5dff\u89c4\u5212\u548c\u73af\u5883\u7814\u7a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528LiDAR\u6570\u636e\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684LOD1\u7ea7\u522b\u91cd\u5efa\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08U-Net\u3001Attention U-Net\u3001U-Net3+\u3001DeepLabV3+\uff09\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u4f30\u7b97\u5efa\u7b51\u9ad8\u5ea6\u3002", "result": "U-Net3+\u548cAttention U-Net\u8868\u73b0\u6700\u4f18\uff0cIoU\u5206\u522b\u4e3a0.833\u548c0.814\uff1b\u5206\u5272\u7cbe\u5ea6\u663e\u8457\u5f71\u54cd3D\u6a21\u578b\u8d28\u91cf\u548c\u5f62\u6001\u7279\u5f81\u4f30\u8ba1\u3002", "conclusion": "U-Net3+\u7ed3\u540890\u767e\u5206\u4f4d\u6570\u548c\u4e2d\u4f4d\u6570\u65b9\u6cd5\u80fd\u51c6\u786e\u4f30\u7b97\u5efa\u7b51\u9ad8\u5ea6\u548c\u63d0\u53d6\u5f62\u6001\u7279\u5f81\uff0c\u5206\u5272\u7cbe\u5ea6\u5bf9\u5efa\u6a21\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.14753", "pdf": "https://arxiv.org/pdf/2505.14753", "abs": "https://arxiv.org/abs/2505.14753", "authors": ["Mengzhu Wang", "Jiao Li", "Shanshan Wang", "Long Lan", "Huibin Tan", "Liang Yang", "Guoli Yang"], "title": "TransMedSeg: A Transferable Semantic Framework for Semi-Supervised Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Semi-supervised learning (SSL) has achieved significant progress in medical\nimage segmentation (SSMIS) through effective utilization of limited labeled\ndata. While current SSL methods for medical images predominantly rely on\nconsistency regularization and pseudo-labeling, they often overlook\ntransferable semantic relationships across different clinical domains and\nimaging modalities. To address this, we propose TransMedSeg, a novel\ntransferable semantic framework for semi-supervised medical image segmentation.\nOur approach introduces a Transferable Semantic Augmentation (TSA) module,\nwhich implicitly enhances feature representations by aligning domain-invariant\nsemantics through cross-domain distribution matching and intra-domain\nstructural preservation. Specifically, TransMedSeg constructs a unified feature\nspace where teacher network features are adaptively augmented towards student\nnetwork semantics via a lightweight memory module, enabling implicit semantic\ntransformation without explicit data generation. Interestingly, this\naugmentation is implicitly realized through an expected transferable\ncross-entropy loss computed over the augmented teacher distribution. An upper\nbound of the expected loss is theoretically derived and minimized during\ntraining, incurring negligible computational overhead. Extensive experiments on\nmedical image datasets demonstrate that TransMedSeg outperforms existing\nsemi-supervised methods, establishing a new direction for transferable\nrepresentation learning in medical image analysis.", "AI": {"tldr": "TransMedSeg\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u9886\u57df\u8bed\u4e49\u5bf9\u9f50\u548c\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u6a21\u5757\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5ffd\u89c6\u4e86\u8de8\u4e34\u5e8a\u9886\u57df\u548c\u6210\u50cf\u6a21\u6001\u7684\u53ef\u8f6c\u79fb\u8bed\u4e49\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faTransMedSeg\u6846\u67b6\uff0c\u5305\u542b\u53ef\u8f6c\u79fb\u8bed\u4e49\u589e\u5f3a\uff08TSA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u8de8\u9886\u57df\u5206\u5e03\u5339\u914d\u548c\u9886\u57df\u5185\u7ed3\u6784\u4fdd\u6301\u5bf9\u9f50\u8bed\u4e49\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u6a21\u5757\u5b9e\u73b0\u9690\u5f0f\u8bed\u4e49\u8f6c\u6362\u3002", "result": "\u5728\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTransMedSeg\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "TransMedSeg\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u53ef\u8f6c\u79fb\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.14754", "pdf": "https://arxiv.org/pdf/2505.14754", "abs": "https://arxiv.org/abs/2505.14754", "authors": ["Andrey Alexandrov", "Giovanni Acampora", "Giovanni De Lellis", "Antonia Di Crescenzo", "Chiara Errico", "Daria Morozova", "Valeri Tioukov", "Autilia Vittiello"], "title": "Model-Independent Machine Learning Approach for Nanometric Axial Localization and Tracking", "categories": ["eess.IV", "astro-ph.IM", "cs.CV", "cs.LG", "physics.ins-det"], "comment": "11 pages, 4 figures, 1 table", "summary": "Accurately tracking particles and determining their position along the\noptical axis is a major challenge in optical microscopy, especially when\nextremely high precision is needed. In this study, we introduce a deep learning\napproach using convolutional neural networks (CNNs) that can determine axial\npositions from dual-focal plane images without relying on predefined models.\nOur method achieves an axial localization accuracy of 40 nanometers - six times\nbetter than traditional single-focal plane techniques. The model's simple\ndesign and strong performance make it suitable for a wide range of uses,\nincluding dark matter detection, proton therapy for cancer, and radiation\nprotection in space. It also shows promise in fields like biological imaging,\nmaterials science, and environmental monitoring. This work highlights how\nmachine learning can turn complex image data into reliable, precise\ninformation, offering a flexible and powerful tool for many scientific\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53cc\u7126\u5e73\u9762\u56fe\u50cf\u8f74\u5411\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7cbe\u5ea6\u8fbe40\u7eb3\u7c73\uff0c\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u5149\u5b66\u663e\u5fae\u955c\u4e2d\u9ad8\u7cbe\u5ea6\u8f74\u5411\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5bf9\u7c92\u5b50\u8ffd\u8e2a\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u4ece\u53cc\u7126\u5e73\u9762\u56fe\u50cf\u4e2d\u76f4\u63a5\u786e\u5b9a\u8f74\u5411\u4f4d\u7f6e\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u6a21\u578b\u3002", "result": "\u8f74\u5411\u5b9a\u4f4d\u7cbe\u5ea6\u4e3a40\u7eb3\u7c73\uff0c\u6bd4\u4f20\u7edf\u5355\u7126\u5e73\u9762\u6280\u672f\u9ad86\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u7b80\u5355\u3001\u6027\u80fd\u5f3a\u5927\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u79d1\u5b66\u9886\u57df\uff0c\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u590d\u6742\u56fe\u50cf\u6570\u636e\u5904\u7406\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.14866", "pdf": "https://arxiv.org/pdf/2505.14866", "abs": "https://arxiv.org/abs/2505.14866", "authors": ["Nisarga Nilavadi", "Andrey Rudenko", "Timm Linder"], "title": "UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://nisarganc.github.io/UPTor-page/", "summary": "We introduce a unified approach to forecast the dynamics of human keypoints\nalong with the motion trajectory based on a short sequence of input poses.\nWhile many studies address either full-body pose prediction or motion\ntrajectory prediction, only a few attempt to merge them. We propose a motion\ntransformation technique to simultaneously predict full-body pose and\ntrajectory key-points in a global coordinate frame. We utilize an off-the-shelf\n3D human pose estimation module, a graph attention network to encode the\nskeleton structure, and a compact, non-autoregressive transformer suitable for\nreal-time motion prediction for human-robot interaction and human-aware\nnavigation. We introduce a human navigation dataset ``DARKO'' with specific\nfocus on navigational activities that are relevant for human-aware mobile robot\nnavigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our\nDARKO dataset. In comparison to prior work, we show that our approach is\ncompact, real-time, and accurate in predicting human navigation motion across\nall datasets. Result animations, our dataset, and code will be available at\nhttps://nisarganc.github.io/UPTor-page/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u77ed\u5e8f\u5217\u8f93\u5165\u9884\u6d4b\u4eba\u4f53\u5173\u952e\u70b9\u548c\u8fd0\u52a8\u8f68\u8ff9\u7684\u52a8\u6001\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5168\u8eab\u59ff\u6001\u9884\u6d4b\u6216\u8fd0\u52a8\u8f68\u8ff9\u9884\u6d4b\uff0c\u5c11\u6709\u5c1d\u8bd5\u5c06\u4e24\u8005\u7ed3\u5408\u3002", "method": "\u91c7\u7528\u8fd0\u52a8\u53d8\u6362\u6280\u672f\uff0c\u5728\u5168\u5c40\u5750\u6807\u7cfb\u4e2d\u540c\u65f6\u9884\u6d4b\u5168\u8eab\u59ff\u6001\u548c\u8f68\u8ff9\u5173\u952e\u70b9\uff0c\u7ed3\u54083D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u975e\u81ea\u56de\u5f52Transformer\u3002", "result": "\u5728Human3.6M\u3001CMU-Mocap\u548cDARKO\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u578b\u7d27\u51d1\u3001\u5b9e\u65f6\u4e14\u51c6\u786e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4eba\u673a\u4ea4\u4e92\u548c\u4eba\u7c7b\u611f\u77e5\u5bfc\u822a\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2505.14916", "pdf": "https://arxiv.org/pdf/2505.14916", "abs": "https://arxiv.org/abs/2505.14916", "authors": ["Yaning Wang", "Jinglun Yu", "Wenhan Guo", "Yu Sun", "Jin U. Kang"], "title": "Super-Resolution Optical Coherence Tomography Using Diffusion Model-Based Plug-and-Play Priors", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We propose an OCT super-resolution framework based on a plug-and-play\ndiffusion model (PnP-DM) to reconstruct high-quality images from sparse\nmeasurements (OCT B-mode corneal images). Our method formulates reconstruction\nas an inverse problem, combining a diffusion prior with Markov chain Monte\nCarlo sampling for efficient posterior inference. We collect high-speed\nunder-sampled B-mode corneal images and apply a deep learning-based up-sampling\npipeline to build realistic training pairs. Evaluations on in vivo and ex vivo\nfish-eye corneal models show that PnP-DM outperforms conventional 2D-UNet\nbaselines, producing sharper structures and better noise suppression. This\napproach advances high-fidelity OCT imaging in high-speed acquisition for\nclinical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5373\u63d2\u5373\u7528\u6269\u6563\u6a21\u578b\uff08PnP-DM\uff09\u7684OCT\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u6d4b\u91cf\u91cd\u5efa\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u901f\u5ea6\u91c7\u96c6\u4e0bOCT\u56fe\u50cf\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e34\u5e8a\u5e94\u7528\u7684\u6210\u50cf\u4fdd\u771f\u5ea6\u3002", "method": "\u5c06\u91cd\u5efa\u95ee\u9898\u5efa\u6a21\u4e3a\u9006\u95ee\u9898\uff0c\u7ed3\u5408\u6269\u6563\u5148\u9a8c\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u91c7\u6837\u8fdb\u884c\u9ad8\u6548\u540e\u9a8c\u63a8\u65ad\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u4e0a\u91c7\u6837\u7ba1\u9053\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u6d3b\u4f53\u548c\u79bb\u4f53\u9c7c\u773c\u89d2\u819c\u6a21\u578b\u4e2d\uff0cPnP-DM\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf2D-UNet\u57fa\u7ebf\uff0c\u7ed3\u6784\u66f4\u6e05\u6670\u4e14\u566a\u58f0\u6291\u5236\u66f4\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u901f\u5ea6OCT\u6210\u50cf\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.14926", "pdf": "https://arxiv.org/pdf/2505.14926", "abs": "https://arxiv.org/abs/2505.14926", "authors": ["Mohammad R. Salmanpour", "Seyed Mohammad Piri", "Somayeh Sadat Mehrnia", "Ahmad Shariftabrizi", "Masume Allahmoradi", "Venkata SK. Manem", "Arman Rahmim", "Ilker Hacihaliloglu"], "title": "Pathobiological Dictionary Defining Pathomics and Texture Features: Addressing Understandable AI Issues in Personalized Liver Cancer; Dictionary Version LCP1.0", "categories": ["physics.comp-ph", "cs.CV", "F.2.2; I.2.7"], "comment": "29 pages, 4 figures and 1 table", "summary": "Artificial intelligence (AI) holds strong potential for medical diagnostics,\nyet its clinical adoption is limited by a lack of interpretability and\ngeneralizability. This study introduces the Pathobiological Dictionary for\nLiver Cancer (LCP1.0), a practical framework designed to translate complex\nPathomics and Radiomics Features (PF and RF) into clinically meaningful\ninsights aligned with existing diagnostic workflows. QuPath and PyRadiomics,\nstandardized according to IBSI guidelines, were used to extract 333 imaging\nfeatures from hepatocellular carcinoma (HCC) tissue samples, including 240\nPF-based-cell detection/intensity, 74 RF-based texture, and 19 RF-based\nfirst-order features. Expert-defined ROIs from the public dataset excluded\nartifact-prone areas, and features were aggregated at the case level. Their\nrelevance to the WHO grading system was assessed using multiple classifiers\nlinked with feature selectors. The resulting dictionary was validated by 8\nexperts in oncology and pathology. In collaboration with 10 domain experts, we\ndeveloped a Pathobiological dictionary of imaging features such as PFs and RF.\nIn our study, the Variable Threshold feature selection algorithm combined with\nthe SVM model achieved the highest accuracy (0.80, P-value less than 0.05),\nselecting 20 key features, primarily clinical and pathomics traits such as\nCentroid, Cell Nucleus, and Cytoplasmic characteristics. These features,\nparticularly nuclear and cytoplasmic, were strongly associated with tumor\ngrading and prognosis, reflecting atypia indicators like pleomorphism,\nhyperchromasia, and cellular orientation.The LCP1.0 provides a clinically\nvalidated bridge between AI outputs and expert interpretation, enhancing model\ntransparency and usability. Aligning AI-derived features with clinical\nsemantics supports the development of interpretable, trustworthy diagnostic\ntools for liver cancer pathology.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLCP1.0\u7684\u75c5\u7406\u751f\u7269\u5b66\u8bcd\u5178\uff0c\u5c06\u590d\u6742\u7684\u5f71\u50cf\u7279\u5f81\u8f6c\u5316\u4e3a\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u6307\u6807\uff0c\u63d0\u5347AI\u5728\u809d\u764c\u8bca\u65ad\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3AI\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u56e0\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u800c\u4e34\u5e8a\u91c7\u7528\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528QuPath\u548cPyRadiomics\u63d0\u53d6\u809d\u764c\u7ec4\u7ec7\u6837\u672c\u7684\u5f71\u50cf\u7279\u5f81\uff0c\u7ed3\u5408\u4e13\u5bb6\u5b9a\u4e49\u7684ROIs\u548c\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\uff08\u5982SVM\uff09\uff0c\u5f00\u53d1\u75c5\u7406\u751f\u7269\u5b66\u8bcd\u5178\u3002", "result": "SVM\u6a21\u578b\u7ed3\u5408\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\uff080.80\uff09\uff0c\u7b5b\u9009\u51fa20\u4e2a\u5173\u952e\u7279\u5f81\uff0c\u4e0e\u80bf\u7624\u5206\u7ea7\u548c\u9884\u540e\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "LCP1.0\u4e3aAI\u8f93\u51fa\u4e0e\u4e13\u5bb6\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e34\u5e8a\u9a8c\u8bc1\u7684\u6865\u6881\uff0c\u652f\u6301\u5f00\u53d1\u53ef\u89e3\u91ca\u3001\u53ef\u4fe1\u8d56\u7684\u809d\u764c\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2505.15057", "pdf": "https://arxiv.org/pdf/2505.15057", "abs": "https://arxiv.org/abs/2505.15057", "authors": ["Frederic Wang", "Jonathan I. Tamir"], "title": "Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine Diffusion Models", "categories": ["eess.IV", "cs.CV"], "comment": "ICIP 2025", "summary": "Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts\ndue to the extended acquisition times required for k-space sampling. These\nartifacts can compromise diagnostic utility, particularly for dynamic imaging.\nWe propose a novel alternating minimization framework that leverages a bespoke\ndiffusion model to jointly reconstruct and correct non-rigid motion-corrupted\nk-space data. The diffusion model uses a coarse-to-fine denoising strategy to\ncapture large overall motion and reconstruct the lower frequencies of the image\nfirst, providing a better inductive bias for motion estimation than that of\nstandard diffusion models. We demonstrate the performance of our approach on\nboth real-world cine cardiac MRI datasets and complex simulated rigid and\nnon-rigid deformations, even when each motion state is undersampled by a factor\nof 64x. Additionally, our method is agnostic to sampling patterns, anatomical\nvariations, and MRI scanning protocols, as long as some low frequency\ncomponents are sampled during each motion state.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4ea4\u66ff\u6700\u5c0f\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u91cd\u5efa\u548c\u6821\u6b63MRI\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u6210\u50cf\u3002", "motivation": "MRI\u56e0\u957f\u65f6\u95f4\u91c7\u96c6\u6613\u53d7\u8fd0\u52a8\u4f2a\u5f71\u5f71\u54cd\uff0c\u7279\u522b\u662f\u52a8\u6001\u6210\u50cf\uff0c\u5f71\u54cd\u8bca\u65ad\u6548\u679c\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u6700\u5c0f\u5316\u6846\u67b6\u548c\u5b9a\u5236\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u53bb\u566a\u7b56\u7565\u6355\u83b7\u5927\u8303\u56f4\u8fd0\u52a8\u5e76\u4f18\u5148\u91cd\u5efa\u4f4e\u9891\u56fe\u50cf\u3002", "result": "\u5728\u771f\u5b9e\u5fc3\u810fMRI\u6570\u636e\u96c6\u548c\u590d\u6742\u6a21\u62df\u53d8\u5f62\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u8fd0\u52a8\u72b6\u6001\u4e0b\u91c7\u6837\u7387\u4f4e\u81f364\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u91c7\u6837\u6a21\u5f0f\u3001\u89e3\u5256\u53d8\u5f02\u548c\u626b\u63cf\u534f\u8bae\u5177\u6709\u666e\u9002\u6027\uff0c\u53ea\u8981\u6bcf\u4e2a\u8fd0\u52a8\u72b6\u6001\u91c7\u6837\u5230\u4f4e\u9891\u6210\u5206\u3002"}}
{"id": "2505.15058", "pdf": "https://arxiv.org/pdf/2505.15058", "abs": "https://arxiv.org/abs/2505.15058", "authors": ["Tianbao Zhang", "Jian Zhao", "Yuer Li", "Zheng Zhu", "Ping Hu", "Zhaoxin Fan", "Wenjun Wu", "Xuelong Li"], "title": "AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS", "68T10"], "comment": "11pages, conference", "summary": "Whole-body audio-driven avatar pose and expression generation is a critical\ntask for creating lifelike digital humans and enhancing the capabilities of\ninteractive virtual agents, with wide-ranging applications in virtual reality,\ndigital entertainment, and remote communication. Existing approaches often\ngenerate audio-driven facial expressions and gestures independently, which\nintroduces a significant limitation: the lack of seamless coordination between\nfacial and gestural elements, resulting in less natural and cohesive\nanimations. To address this limitation, we propose AsynFusion, a novel\nframework that leverages diffusion transformers to achieve harmonious\nexpression and gesture synthesis. The proposed method is built upon a\ndual-branch DiT architecture, which enables the parallel generation of facial\nexpressions and gestures. Within the model, we introduce a Cooperative\nSynchronization Module to facilitate bidirectional feature interaction between\nthe two modalities, and an Asynchronous LCM Sampling strategy to reduce\ncomputational overhead while maintaining high-quality outputs. Extensive\nexperiments demonstrate that AsynFusion achieves state-of-the-art performance\nin generating real-time, synchronized whole-body animations, consistently\noutperforming existing methods in both quantitative and qualitative\nevaluations.", "AI": {"tldr": "AsynFusion\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u534f\u8c03\u7684\u9762\u90e8\u8868\u60c5\u548c\u624b\u52bf\u52a8\u753b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7f3a\u4e4f\u540c\u6b65\u7684\u95ee\u9898\u3002", "motivation": "\u5168\u8eab\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u4eba\u59ff\u6001\u548c\u8868\u60c5\u751f\u6210\u5728\u865a\u62df\u73b0\u5b9e\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u9762\u90e8\u4e0e\u624b\u52bf\u7684\u534f\u8c03\u800c\u663e\u5f97\u4e0d\u81ea\u7136\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652fDiT\u67b6\u6784\uff0c\u901a\u8fc7\u534f\u4f5c\u540c\u6b65\u6a21\u5757\u548c\u5f02\u6b65LCM\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u9762\u90e8\u8868\u60c5\u548c\u624b\u52bf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAsynFusion\u5728\u5b9e\u65f6\u540c\u6b65\u5168\u8eab\u52a8\u753b\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AsynFusion\u901a\u8fc7\u534f\u8c03\u9762\u90e8\u548c\u624b\u52bf\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u753b\u7684\u81ea\u7136\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.15075", "pdf": "https://arxiv.org/pdf/2505.15075", "abs": "https://arxiv.org/abs/2505.15075", "authors": ["Hao Wang", "Pinzhi Huang", "Jihan Yang", "Saining Xie", "Daisuke Kawahara"], "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "https://github.com/nlp-waseda/traveling-across-languages", "summary": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u65b0\u57fa\u51c6\uff08KnowRecall\u548cVisRecall\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u548c\u6587\u5316\u77e5\u8bc6\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5728\u8de8\u8bed\u8a00\u548c\u6587\u5316\u77e5\u8bc6\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u65b0\u57fa\u51c6\uff1aKnowRecall\uff08\u8bc4\u4f3015\u79cd\u8bed\u8a00\u4e2d\u7684\u4e8b\u5b9e\u77e5\u8bc6\u4e00\u81f4\u6027\uff09\u548cVisRecall\uff08\u8bc4\u4f309\u79cd\u8bed\u8a00\u4e2d\u7684\u89c6\u89c9\u8bb0\u5fc6\u4e00\u81f4\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684MLLMs\uff08\u5305\u62ec\u4e13\u6709\u6a21\u578b\uff09\u5728\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u9700\u8981\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u5f00\u53d1\u771f\u6b63\u591a\u8bed\u8a00\u4e14\u5177\u6709\u6587\u5316\u610f\u8bc6\u7684\u6a21\u578b\u3002"}}
{"id": "2505.15120", "pdf": "https://arxiv.org/pdf/2505.15120", "abs": "https://arxiv.org/abs/2505.15120", "authors": ["Muniba Noreen", "Furqan Shaukat"], "title": "Lung Nodule-SSM: Self-Supervised Lung Nodule Detection and Classification in Thoracic CT Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Lung cancer remains among the deadliest types of cancer in recent decades,\nand early lung nodule detection is crucial for improving patient outcomes. The\nlimited availability of annotated medical imaging data remains a bottleneck in\ndeveloping accurate computer-aided diagnosis (CAD) systems. Self-supervised\nlearning can help leverage large amounts of unlabeled data to develop more\nrobust CAD systems. With the recent advent of transformer-based architecture\nand their ability to generalize to unseen tasks, there has been an effort\nwithin the healthcare community to adapt them to various medical downstream\ntasks. Thus, we propose a novel \"LungNodule-SSM\" method, which utilizes\nselfsupervised learning with DINOv2 as a backbone to enhance lung nodule\ndetection and classification without annotated data. Our methodology has two\nstages: firstly, the DINOv2 model is pre-trained on unlabeled CT scans to learn\nrobust feature representations, then secondly, these features are fine-tuned\nusing transformer-based architectures for lesionlevel detection and accurate\nlung nodule diagnosis. The proposed method has been evaluated on the\nchallenging LUNA 16 dataset, consisting of 888 CT scans, and compared with SOTA\nmethods. Our experimental results show the superiority of our proposed method\nwith an accuracy of 98.37%, explaining its effectiveness in lung nodule\ndetection. The source code, datasets, and pre-processed data can be accessed\nusing the\nlink:https://github.com/EMeRALDsNRPU/Lung-Nodule-SSM-Self-Supervised-Lung-Nodule-Detection-and-Classification/tree/main", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\u201cLungNodule-SSM\u201d\uff0c\u5229\u7528DINOv2\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u63d0\u5347\u80ba\u7ed3\u8282\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u80ba\u764c\u65e9\u671f\u68c0\u6d4b\u5bf9\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6807\u6ce8\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u7684\u5f00\u53d1\u3002\u81ea\u76d1\u7763\u5b66\u4e60\u53ef\u5229\u7528\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u9996\u5148\u5728\u672a\u6807\u6ce8CT\u626b\u63cf\u4e0a\u9884\u8bad\u7ec3DINOv2\u6a21\u578b\u4ee5\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\u8868\u793a\uff0c\u968f\u540e\u57fa\u4e8eTransformer\u67b6\u6784\u5fae\u8c03\u8fd9\u4e9b\u7279\u5f81\u7528\u4e8e\u75c5\u7076\u68c0\u6d4b\u548c\u80ba\u7ed3\u8282\u8bca\u65ad\u3002", "result": "\u5728LUNA 16\u6570\u636e\u96c6\uff08888\u4e2aCT\u626b\u63cf\uff09\u4e0a\u9a8c\u8bc1\uff0c\u51c6\u786e\u7387\u8fbe98.37%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u80ba\u7ed3\u8282\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.15135", "pdf": "https://arxiv.org/pdf/2505.15135", "abs": "https://arxiv.org/abs/2505.15135", "authors": ["Badhan Mazumder", "Ayush Kanyal", "Lei Wu", "Vince D. Calhoun", "Dong Hye Ye"], "title": "Physics-Guided Multi-View Graph Neural Network for Schizophrenia Classification via Structural-Functional Coupling", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted and presented at the 7th International Workshop on\n  PRedictive Intelligence in MEdicine (Held in Conjunction with MICCAI 2024)", "summary": "Clinical studies reveal disruptions in brain structural connectivity (SC) and\nfunctional connectivity (FC) in neuropsychiatric disorders such as\nschizophrenia (SZ). Traditional approaches might rely solely on SC due to\nlimited functional data availability, hindering comprehension of cognitive and\nbehavioral impairments in individuals with SZ by neglecting the intricate SC-FC\ninterrelationship. To tackle the challenge, we propose a novel physics-guided\ndeep learning framework that leverages a neural oscillation model to describe\nthe dynamics of a collection of interconnected neural oscillators, which\noperate via nerve fibers dispersed across the brain's structure. Our proposed\nframework utilizes SC to simultaneously generate FC by learning SC-FC coupling\nfrom a system dynamics perspective. Additionally, it employs a novel multi-view\ngraph neural network (GNN) with a joint loss to perform correlation-based SC-FC\nfusion and classification of individuals with SZ. Experiments conducted on a\nclinical dataset exhibited improved performance, demonstrating the robustness\nof our proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u632f\u8361\u6a21\u578b\u548cSC-FC\u8026\u5408\uff0c\u7ed3\u5408\u591a\u89c6\u56fe\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347\u4e86\u7cbe\u795e\u5206\u88c2\u75c7\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7ed3\u6784\u8fde\u63a5\uff08SC\uff09\uff0c\u5ffd\u7565\u4e86SC\u4e0e\u529f\u80fd\u8fde\u63a5\uff08FC\uff09\u7684\u590d\u6742\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u8ba4\u77e5\u548c\u884c\u4e3a\u969c\u788d\u7684\u7406\u89e3\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u632f\u8361\u6a21\u578b\u63cf\u8ff0\u795e\u7ecf\u632f\u8361\u5668\u52a8\u6001\uff0c\u5229\u7528SC\u751f\u6210FC\uff0c\u5e76\u901a\u8fc7\u591a\u89c6\u56fe\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u8fdb\u884cSC-FC\u878d\u5408\u548c\u5206\u7c7b\u3002", "result": "\u5728\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7SC-FC\u8026\u5408\u548c\u591a\u89c6\u56feGNN\uff0c\u4e3a\u7cbe\u795e\u5206\u88c2\u75c7\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.15234", "pdf": "https://arxiv.org/pdf/2505.15234", "abs": "https://arxiv.org/abs/2505.15234", "authors": ["Saqib Qamar", "Mohd Fazil", "Parvez Ahmad", "Ghulam Muhammad"], "title": "SAMA-UNet: Enhancing Medical Image Segmentation with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image segmentation plays an important role in various clinical\napplications, but existing models often struggle with the computational\ninefficiencies and challenges posed by complex medical data. State Space\nSequence Models (SSMs) have demonstrated promise in modeling long-range\ndependencies with linear computational complexity, yet their application in\nmedical image segmentation remains hindered by incompatibilities with image\ntokens and autoregressive assumptions. Moreover, it is difficult to achieve a\nbalance in capturing both local fine-grained information and global semantic\ndependencies. To address these challenges, we introduce SAMA-UNet, a novel\narchitecture for medical image segmentation. A key innovation is the\nSelf-Adaptive Mamba-like Aggregated Attention (SAMA) block, which integrates\ncontextual self-attention with dynamic weight modulation to prioritise the most\nrelevant features based on local and global contexts. This approach reduces\ncomputational complexity and improves the representation of complex image\nfeatures across multiple scales. We also suggest the Causal-Resonance\nMulti-Scale Module (CR-MSM), which enhances the flow of information between the\nencoder and decoder by using causal resonance learning. This mechanism allows\nthe model to automatically adjust feature resolution and causal dependencies\nacross scales, leading to better semantic alignment between the low-level and\nhigh-level features in U-shaped architectures. Experiments on MRI, CT, and\nendoscopy images show that SAMA-UNet performs better in segmentation accuracy\nthan current methods using CNN, Transformer, and Mamba. The implementation is\npublicly available at GitHub.", "AI": {"tldr": "SAMA-UNet\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u67b6\u6784\uff0c\u901a\u8fc7SAMA\u5757\u548cCR-MSM\u6a21\u5757\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7279\u5f81\u5e73\u8861\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u590d\u6742\u6570\u636e\u5efa\u6a21\u4e0a\u5b58\u5728\u6311\u6218\uff0cSSMs\u867d\u6709\u6f5c\u529b\u4f46\u5e94\u7528\u53d7\u9650\u3002", "method": "\u63d0\u51faSAMA-UNet\uff0c\u7ed3\u5408SAMA\u5757\uff08\u52a8\u6001\u6743\u91cd\u8c03\u5236\uff09\u548cCR-MSM\u6a21\u5757\uff08\u56e0\u679c\u5171\u632f\u5b66\u4e60\uff09\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u4e0e\u591a\u5c3a\u5ea6\u4fe1\u606f\u6d41\u3002", "result": "\u5728MRI\u3001CT\u548c\u5185\u7aa5\u955c\u56fe\u50cf\u4e0a\uff0cSAMA-UNet\u7684\u51c6\u786e\u7387\u4f18\u4e8eCNN\u3001Transformer\u548cMamba\u65b9\u6cd5\u3002", "conclusion": "SAMA-UNet\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.15235", "pdf": "https://arxiv.org/pdf/2505.15235", "abs": "https://arxiv.org/abs/2505.15235", "authors": ["Yifan Liu", "Wuyang Li", "Weihao Yu", "Chenxin Li", "Alexandre Alahi", "Max Meng", "Yixuan Yuan"], "title": "X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to Computed Tomography", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Computed Tomography serves as an indispensable tool in clinical workflows,\nproviding non-invasive visualization of internal anatomical structures.\nExisting CT reconstruction works are limited to small-capacity model\narchitecture, inflexible volume representation, and small-scale training data.\nIn this paper, we present X-GRM (X-ray Gaussian Reconstruction Model), a large\nfeedforward model for reconstructing 3D CT from sparse-view 2D X-ray\nprojections. X-GRM employs a scalable transformer-based architecture to encode\nan arbitrary number of sparse X-ray inputs, where tokens from different views\nare integrated efficiently. Then, tokens are decoded into a new volume\nrepresentation, named Voxel-based Gaussian Splatting (VoxGS), which enables\nefficient CT volume extraction and differentiable X-ray rendering. To support\nthe training of X-GRM, we collect ReconX-15K, a large-scale CT reconstruction\ndataset containing around 15,000 CT/X-ray pairs across diverse organs,\nincluding the chest, abdomen, pelvis, and tooth etc. This combination of a\nhigh-capacity model, flexible volume representation, and large-scale training\ndata empowers our model to produce high-quality reconstructions from various\ntesting inputs, including in-domain and out-domain X-ray projections. Project\nPage: https://github.com/CUHK-AIM-Group/X-GRM.", "AI": {"tldr": "X-GRM\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5927\u89c4\u6a21\u524d\u9988\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7a00\u758f2D X\u5c04\u7ebf\u6295\u5f71\u91cd\u5efa3D CT\uff0c\u91c7\u7528Voxel-based Gaussian Splatting\u8868\u793a\uff0c\u5e76\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6ReconX-15K\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709CT\u91cd\u5efa\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u5bb9\u91cf\u6a21\u578b\u67b6\u6784\u3001\u4e0d\u7075\u6d3b\u7684\u4f53\u8868\u793a\u548c\u5c0f\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0cX-GRM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "X-GRM\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u53ef\u6269\u5c55\u67b6\u6784\u7f16\u7801\u7a00\u758fX\u5c04\u7ebf\u8f93\u5165\uff0c\u5e76\u901a\u8fc7VoxGS\u89e3\u7801\u4e3a\u9ad8\u6548CT\u4f53\u8868\u793a\u3002", "result": "\u6a21\u578b\u5728\u591a\u6837\u6d4b\u8bd5\u8f93\u5165\uff08\u5305\u62ec\u57df\u5185\u548c\u57df\u5916X\u5c04\u7ebf\u6295\u5f71\uff09\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "conclusion": "X-GRM\u901a\u8fc7\u9ad8\u5bb9\u91cf\u6a21\u578b\u3001\u7075\u6d3b\u4f53\u8868\u793a\u548c\u5927\u89c4\u6a21\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86CT\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2505.15249", "pdf": "https://arxiv.org/pdf/2505.15249", "abs": "https://arxiv.org/abs/2505.15249", "authors": ["Yerin Hwang", "Dongryeol Lee", "Kyungmin Min", "Taegwan Kang", "Yong-il Kim", "Kyomin Jung"], "title": "Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": "(21pgs, 12 Tables, 9 Figures)", "summary": "Recently, large vision-language models (LVLMs) have emerged as the preferred\ntools for judging text-image alignment, yet their robustness along the visual\nmodality remains underexplored. This work is the first study to address a key\nresearch question: Can adversarial visual manipulations systematically fool\nLVLM judges into assigning unfairly inflated scores? We define potential image\ninduced biases within the context of T2I evaluation and examine how these\nbiases affect the evaluations of LVLM judges. Moreover, we introduce a novel,\nfine-grained, multi-domain meta-evaluation benchmark named FRAME, which is\ndeliberately constructed to exhibit diverse score distributions. By introducing\nthe defined biases into the benchmark, we reveal that all tested LVLM judges\nexhibit vulnerability across all domains, consistently inflating scores for\nmanipulated images. Further analysis reveals that combining multiple biases\namplifies their effects, and pairwise evaluations are similarly susceptible.\nMoreover, we observe that visual biases persist under prompt-based mitigation\nstrategies, highlighting the vulnerability of current LVLM evaluation systems\nand underscoring the urgent need for more robust LVLM judges.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u8bc4\u4f30\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u65f6\u6613\u53d7\u89c6\u89c9\u5bf9\u6297\u6027\u64cd\u7eb5\u5f71\u54cd\uff0c\u5bfc\u81f4\u8bc4\u5206\u865a\u9ad8\u3002", "motivation": "\u63a2\u7d22LVLM\u5728\u89c6\u89c9\u6a21\u6001\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u89c6\u89c9\u64cd\u7eb5\u662f\u5426\u4f1a\u5bfc\u81f4\u8bc4\u5206\u4e0d\u516c\u3002", "method": "\u5b9a\u4e49\u56fe\u50cf\u8bf1\u5bfc\u504f\u5dee\uff0c\u6784\u5efa\u591a\u9886\u57df\u5143\u8bc4\u4f30\u57fa\u51c6FRAME\uff0c\u6d4b\u8bd5LVLM\u7684\u8106\u5f31\u6027\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684LVLM\u5747\u8868\u73b0\u51fa\u8106\u5f31\u6027\uff0c\u8bc4\u5206\u865a\u9ad8\uff1b\u591a\u504f\u5dee\u7ec4\u5408\u6548\u679c\u66f4\u663e\u8457\uff0c\u63d0\u793a\u7b56\u7565\u65e0\u6548\u3002", "conclusion": "\u5f53\u524dLVLM\u8bc4\u4f30\u7cfb\u7edf\u5b58\u5728\u6f0f\u6d1e\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2505.15270", "pdf": "https://arxiv.org/pdf/2505.15270", "abs": "https://arxiv.org/abs/2505.15270", "authors": ["Chenyu Zheng", "Xinyu Zhang", "Rongzhen Wang", "Wei Huang", "Zhi Tian", "Weilin Huang", "Jun Zhu", "Chongxuan Li"], "title": "Scaling Diffusion Transformers Efficiently via $\u03bc$P", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 10 figures, 15 tables", "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard $\\mu$P to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\n$\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing $\\mu$P methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP\ntransferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of $\\mu$P on text-to-image generation by scaling\nPixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under $\\mu$P outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of\nconsumption by human experts for MMDiT-18B. These results establish $\\mu$P as a\nprincipled and efficient framework for scaling diffusion Transformers.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06Maximal Update Parametrization\uff08\u03bcP\uff09\u65b9\u6cd5\u63a8\u5e7f\u5230\u6269\u6563Transformer\u4e2d\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u5e76\u663e\u8457\u964d\u4f4e\u8d85\u53c2\u6570\u8c03\u4f18\u6210\u672c\u3002", "motivation": "\u6269\u6563Transformer\u5728\u89c6\u89c9\u751f\u6210\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5927\u89c4\u6a21\u8d85\u53c2\u6570\u8c03\u4f18\u6210\u672c\u9ad8\u6602\u3002\u03bcP\u65b9\u6cd5\u5728\u666e\u901aTransformer\u4e2d\u5df2\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u5c1a\u672a\u5728\u6269\u6563Transformer\u4e2d\u9a8c\u8bc1\u3002", "method": "\u5c06\u6807\u51c6\u03bcP\u65b9\u6cd5\u63a8\u5e7f\u5230\u6269\u6563Transformer\uff08\u5982DiT\u3001U-ViT\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u03bcP\u65b9\u6cd5\u5728\u6269\u6563Transformer\u4e2d\u540c\u6837\u9002\u7528\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6536\u655b\u901f\u5ea6\uff08\u5982DiT-XL-2-\u03bcP\u6536\u655b\u901f\u5ea6\u63d0\u53472.9\u500d\uff09\uff0c\u5e76\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u03bcP\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u5b8c\u5907\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u6269\u6563Transformer\u7684\u6269\u5c55\uff0c\u663e\u8457\u964d\u4f4e\u8c03\u4f18\u6210\u672c\u3002"}}
{"id": "2505.15282", "pdf": "https://arxiv.org/pdf/2505.15282", "abs": "https://arxiv.org/abs/2505.15282", "authors": ["Yanzhi Tian", "Zeming Liu", "Zhengyang Liu", "Yuhang Guo"], "title": "Exploring In-Image Machine Translation with Real-World Background", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to ACL 2025 Findings. Code available at\n  https://github.com/BITHLP/DebackX", "summary": "In-Image Machine Translation (IIMT) aims to translate texts within images\nfrom one language to another. Previous research on IIMT was primarily conducted\non simplified scenarios such as images of one-line text with black font in\nwhite backgrounds, which is far from reality and impractical for applications\nin the real world. To make IIMT research practically valuable, it is essential\nto consider a complex scenario where the text backgrounds are derived from\nreal-world images. To facilitate research of complex scenario IIMT, we design\nan IIMT dataset that includes subtitle text with real-world background. However\nprevious IIMT models perform inadequately in complex scenarios. To address the\nissue, we propose the DebackX model, which separates the background and\ntext-image from the source image, performs translation on text-image directly,\nand fuses the translated text-image with the background, to generate the target\nimage. Experimental results show that our model achieves improvements in both\ntranslation quality and visual effect.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u590d\u6742\u573a\u666f\u4e0b\u56fe\u50cf\u5185\u673a\u5668\u7ffb\u8bd1\uff08IIMT\uff09\u95ee\u9898\u7684\u65b0\u6a21\u578bDebackX\uff0c\u901a\u8fc7\u5206\u79bb\u80cc\u666f\u4e0e\u6587\u672c\u56fe\u50cf\u3001\u76f4\u63a5\u7ffb\u8bd1\u6587\u672c\u56fe\u50cf\u5e76\u878d\u5408\u80cc\u666f\u751f\u6210\u76ee\u6807\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u8d28\u91cf\u548c\u89c6\u89c9\u6548\u679c\u3002", "motivation": "\u73b0\u6709IIMT\u7814\u7a76\u591a\u57fa\u4e8e\u7b80\u5316\u573a\u666f\uff08\u5982\u5355\u884c\u6587\u672c\u3001\u9ed1\u767d\u80cc\u666f\uff09\uff0c\u4e0e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u5dee\u8ddd\u8f83\u5927\u3002\u4e3a\u4e86\u63d0\u5347IIMT\u7684\u5b9e\u9645\u4ef7\u503c\uff0c\u9700\u7814\u7a76\u590d\u6742\u573a\u666f\uff08\u5982\u771f\u5b9e\u80cc\u666f\u4e0b\u7684\u6587\u672c\uff09\u3002", "method": "\u63d0\u51faDebackX\u6a21\u578b\uff0c\u5206\u79bb\u80cc\u666f\u4e0e\u6587\u672c\u56fe\u50cf\uff0c\u76f4\u63a5\u7ffb\u8bd1\u6587\u672c\u56fe\u50cf\u540e\u4e0e\u80cc\u666f\u878d\u5408\u751f\u6210\u76ee\u6807\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDebackX\u5728\u7ffb\u8bd1\u8d28\u91cf\u548c\u89c6\u89c9\u6548\u679c\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DebackX\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684IIMT\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.15284", "pdf": "https://arxiv.org/pdf/2505.15284", "abs": "https://arxiv.org/abs/2505.15284", "authors": ["Kun Fang", "Qinghua Tao", "Mingzhen He", "Kexin Lv", "Runze Yang", "Haibo Hu", "Xiaolin Huang", "Jie Yang", "Longbin Cao"], "title": "Kernel PCA for Out-of-Distribution Detection: Non-Linear Kernel Selections and Approximations", "categories": ["cs.LG", "cs.CV"], "comment": "This study is an extension of its conference version published in\n  NeurIPS'24, see\n  https://proceedings.neurips.cc/paper_files/paper/2024/hash/f2543511e5f4d4764857f9ad833a977d-Abstract-Conference.html", "summary": "Out-of-Distribution (OoD) detection is vital for the reliability of deep\nneural networks, the key of which lies in effectively characterizing the\ndisparities between OoD and In-Distribution (InD) data. In this work, such\ndisparities are exploited through a fresh perspective of non-linear feature\nsubspace. That is, a discriminative non-linear subspace is learned from InD\nfeatures to capture representative patterns of InD, while informative patterns\nof OoD features cannot be well captured in such a subspace due to their\ndifferent distribution. Grounded on this perspective, we exploit the deviations\nof InD and OoD features in such a non-linear subspace for effective OoD\ndetection. To be specific, we leverage the framework of Kernel Principal\nComponent Analysis (KPCA) to attain the discriminative non-linear subspace and\ndeploy the reconstruction error on such subspace to distinguish InD and OoD\ndata. Two challenges emerge: (i) the learning of an effective non-linear\nsubspace, i.e., the selection of kernel function in KPCA, and (ii) the\ncomputation of the kernel matrix with large-scale InD data. For the former, we\nreveal two vital non-linear patterns that closely relate to the InD-OoD\ndisparity, leading to the establishment of a Cosine-Gaussian kernel for\nconstructing the subspace. For the latter, we introduce two techniques to\napproximate the Cosine-Gaussian kernel with significantly cheap computations.\nIn particular, our approximation is further tailored by incorporating the InD\ndata confidence, which is demonstrated to promote the learning of\ndiscriminative subspaces for OoD data. Our study presents new insights into the\nnon-linear feature subspace for OoD detection and contributes practical\nexplorations on the associated kernel design and efficient computations,\nyielding a KPCA detection method with distinctively improved efficacy and\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7ebf\u6027\u7279\u5f81\u5b50\u7a7a\u95f4\u7684OoD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528KPCA\u6846\u67b6\u5b66\u4e60\u5224\u522b\u6027\u5b50\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u91cd\u6784\u8bef\u5dee\u533a\u5206InD\u548cOoD\u6570\u636e\u3002", "motivation": "OoD\u68c0\u6d4b\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5173\u952e\u5728\u4e8e\u6709\u6548\u8868\u5f81InD\u548cOoD\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u5229\u7528KPCA\u5b66\u4e60\u975e\u7ebf\u6027\u5b50\u7a7a\u95f4\uff0c\u8bbe\u8ba1Cosine-Gaussian\u6838\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u9ad8\u6548\u8ba1\u7b97\u6280\u672f\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86OoD\u68c0\u6d4b\u7684\u6548\u679c\u548c\u6548\u7387\u3002", "conclusion": "\u975e\u7ebf\u6027\u7279\u5f81\u5b50\u7a7a\u95f4\u4e3aOoD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6838\u51fd\u6570\u8bbe\u8ba1\u548c\u9ad8\u6548\u8ba1\u7b97\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2505.15285", "pdf": "https://arxiv.org/pdf/2505.15285", "abs": "https://arxiv.org/abs/2505.15285", "authors": ["Fengting Zhang", "Boxu Liang", "Qinghao Liu", "Min Liu", "Xiang Chen", "Yaonan Wang"], "title": "Reconsider the Template Mesh in Deep Learning-based Mesh Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Mesh reconstruction is a cornerstone process across various applications,\nincluding in-silico trials, digital twins, surgical planning, and navigation.\nRecent advancements in deep learning have notably enhanced mesh reconstruction\nspeeds. Yet, traditional methods predominantly rely on deforming a standardised\ntemplate mesh for individual subjects, which overlooks the unique anatomical\nvariations between them, and may compromise the fidelity of the\nreconstructions. In this paper, we propose an adaptive-template-based mesh\nreconstruction network (ATMRN), which generates adaptive templates from the\ngiven images for the subsequent deformation, moving beyond the constraints of a\nsingular, fixed template. Our approach, validated on cortical magnetic\nresonance (MR) images from the OASIS dataset, sets a new benchmark in\nvoxel-to-cortex mesh reconstruction, achieving an average symmetric surface\ndistance of 0.267mm across four cortical structures. Our proposed method is\ngeneric and can be easily transferred to other image modalities and anatomical\nstructures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u6a21\u677f\u7684\u7f51\u683c\u91cd\u5efa\u7f51\u7edc\uff08ATMRN\uff09\uff0c\u901a\u8fc7\u751f\u6210\u81ea\u9002\u5e94\u6a21\u677f\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u4f20\u7edf\u56fa\u5b9a\u6a21\u677f\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7f51\u683c\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6a21\u677f\uff0c\u5ffd\u7565\u4e2a\u4f53\u89e3\u5256\u5dee\u5f02\uff0c\u5f71\u54cd\u91cd\u5efa\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faATMRN\uff0c\u4ece\u56fe\u50cf\u751f\u6210\u81ea\u9002\u5e94\u6a21\u677f\u7528\u4e8e\u540e\u7eed\u53d8\u5f62\uff0c\u907f\u514d\u56fa\u5b9a\u6a21\u677f\u9650\u5236\u3002", "result": "\u5728OASIS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5e73\u5747\u5bf9\u79f0\u8868\u9762\u8ddd\u79bb\u4e3a0.267mm\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ATMRN\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u56fe\u50cf\u6a21\u6001\u548c\u89e3\u5256\u7ed3\u6784\u3002"}}
{"id": "2505.15298", "pdf": "https://arxiv.org/pdf/2505.15298", "abs": "https://arxiv.org/abs/2505.15298", "authors": ["Kangan Qian", "Sicong Jiang", "Yang Zhong", "Ziang Luo", "Zilin Huang", "Tianze Zhu", "Kun Jiang", "Mengmeng Yang", "Zheng Fu", "Jinyu Miao", "Yining Shi", "He Zhe Lim", "Li Liu", "Tianbao Zhou", "Hongyi Wang", "Huang Yu", "Yifei Hu", "Guang Li", "Guang Chen", "Hao Ye", "Lijun Sun", "Diange Yang"], "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "18 pages, 8 figures", "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their\nstruggle with hallucinations, inefficient reasoning, and limited real-world\nvalidation hinders accurate perception and robust step-by-step reasoning. To\novercome this, we introduce \\textbf{AgentThink}, a pioneering unified framework\nthat, for the first time, integrates Chain-of-Thought (CoT) reasoning with\ndynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's\ncore innovations include: \\textbf{(i) Structured Data Generation}, by\nestablishing an autonomous driving tool library to automatically construct\nstructured, self-verified reasoning data explicitly incorporating tool usage\nfor diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline},\nemploying Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO) to equip VLMs with the capability for autonomous tool invocation; and\n\\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel\nmulti-tool assessment protocol to rigorously evaluate the model's tool\ninvocation and utilization. Experiments on the DriveLMM-o1 benchmark\ndemonstrate AgentThink significantly boosts overall reasoning scores by\n\\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while\nmarkedly improving reasoning quality and consistency. Furthermore, ablation\nstudies and robust zero-shot/few-shot generalization experiments across various\nbenchmarks underscore its powerful capabilities. These findings highlight a\npromising trajectory for developing trustworthy and tool-aware autonomous\ndriving models.", "AI": {"tldr": "AgentThink\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u52a8\u6001\u5de5\u5177\u8c03\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u7684\u63a8\u7406\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b58\u5728\u5e7b\u89c9\u3001\u4f4e\u6548\u63a8\u7406\u548c\u7f3a\u4e4f\u5b9e\u9645\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u6570\u636e\u751f\u6210\u3001\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff08SFT\u4e0eGRPO\uff09\u548c\u4ee3\u7406\u5f0f\u5de5\u5177\u4f7f\u7528\u8bc4\u4f30\uff0c\u63d0\u5347\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\u3002", "result": "\u5728DriveLMM-o1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63a8\u7406\u5206\u6570\u63d0\u534753.91%\uff0c\u7b54\u6848\u51c6\u786e\u6027\u63d0\u9ad833.54%\uff0c\u4e14\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AgentThink\u4e3a\u5f00\u53d1\u53ef\u4fe1\u8d56\u4e14\u5de5\u5177\u611f\u77e5\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2505.15389", "pdf": "https://arxiv.org/pdf/2505.15389", "abs": "https://arxiv.org/abs/2505.15389", "authors": ["DongGeon Lee", "Joonwon Jang", "Jihae Jeong", "Hwanjo Yu"], "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study", "categories": ["cs.CL", "cs.CR", "cs.CV"], "comment": null, "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u771f\u5b9e\u8868\u60c5\u5305\u56fe\u50cf\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5176\u5bf9\u6709\u5bb3\u63d0\u793a\u7684\u8106\u5f31\u6027\u9ad8\u4e8e\u5408\u6210\u56fe\u50cf\uff0c\u591a\u8f6e\u4ea4\u4e92\u4ec5\u90e8\u5206\u7f13\u89e3\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u5176\u5b89\u5168\u6027\u98ce\u9669\u88ab\u653e\u5927\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u591a\u57fa\u4e8e\u4eba\u5de5\u56fe\u50cf\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u7528\u6237\u5206\u4eab\u7684\u8868\u60c5\u5305\u56fe\u50cf\u7684\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u5f15\u5165MemeSafetyBench\u57fa\u51c6\uff0c\u5305\u542b50,430\u4e2a\u771f\u5b9e\u8868\u60c5\u5305\u56fe\u50cf\u4e0e\u6709\u5bb3/\u65e0\u5bb3\u6307\u4ee4\u914d\u5bf9\uff0c\u901a\u8fc7\u5b89\u5168\u5206\u7c7b\u6cd5\u548cLLM\u751f\u6210\u6307\u4ee4\uff0c\u8bc4\u4f30VLMs\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "result": "VLMs\u5bf9\u8868\u60c5\u5305\u6709\u5bb3\u63d0\u793a\u7684\u8106\u5f31\u6027\u663e\u8457\u9ad8\u4e8e\u5408\u6210\u56fe\u50cf\uff0c\u8868\u60c5\u5305\u663e\u8457\u589e\u52a0\u6709\u5bb3\u54cd\u5e94\u5e76\u51cf\u5c11\u62d2\u7edd\u7387\uff0c\u591a\u8f6e\u4ea4\u4e92\u4ec5\u90e8\u5206\u7f13\u89e3\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u66f4\u751f\u6001\u6709\u6548\u7684\u8bc4\u4f30\u548c\u66f4\u5f3a\u7684\u5b89\u5168\u673a\u5236\u3002"}}
{"id": "2505.15505", "pdf": "https://arxiv.org/pdf/2505.15505", "abs": "https://arxiv.org/abs/2505.15505", "authors": ["Abdul Samad Shaik", "Shashaank Mattur Aswatha", "Rahul Jashvantbhai Pandya"], "title": "Deep Learning Enabled Segmentation, Classification and Risk Assessment of Cervical Cancer", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 10 figures", "summary": "Cervical cancer, the fourth leading cause of cancer in women globally,\nrequires early detection through Pap smear tests to identify precancerous\nchanges and prevent disease progression. In this study, we performed a focused\nanalysis by segmenting the cellular boundaries and drawing bounding boxes to\nisolate the cancer cells. A novel Deep Learning (DL) architecture, the\n``Multi-Resolution Fusion Deep Convolutional Network\", was proposed to\neffectively handle images with varying resolutions and aspect ratios, with its\nefficacy showcased using the SIPaKMeD dataset. The performance of this DL model\nwas observed to be similar to the state-of-the-art models, with accuracy\nvariations of a mere 2\\% to 3\\%, achieved using just 1.7 million learnable\nparameters, which is approximately 85 times less than the VGG-19 model.\nFurthermore, we introduced a multi-task learning technique that simultaneously\nperforms segmentation and classification tasks and begets an Intersection over\nUnion score of 0.83 and a classification accuracy of 90\\%. The final stage of\nthe workflow employs a probabilistic approach for risk assessment, extracting\nfeature vectors to predict the likelihood of normal cells progressing to\nmalignant states, which can be utilized for the prognosis of cervical cancer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7528\u4e8e\u5bab\u9888\u764c\u7ec6\u80de\u7684\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u7ed3\u5408\u591a\u5206\u8fa8\u7387\u878d\u5408\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6280\u672f\uff0c\u6027\u80fd\u63a5\u8fd1\u73b0\u6709\u6700\u4f18\u6a21\u578b\uff0c\u540c\u65f6\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u5bab\u9888\u764c\u662f\u5168\u7403\u5973\u6027\u7b2c\u56db\u5927\u764c\u75c7\uff0c\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u8fa8\u7387\u548c\u591a\u4efb\u52a1\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u5206\u8fa8\u7387\u878d\u5408\u5377\u79ef\u7f51\u7edc\u5904\u7406\u4e0d\u540c\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u540c\u65f6\u8fdb\u884c\u5206\u5272\u548c\u5206\u7c7b\uff0c\u6700\u540e\u7528\u6982\u7387\u65b9\u6cd5\u8bc4\u4f30\u98ce\u9669\u3002", "result": "\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u6a21\u578b\uff08\u51c6\u786e\u7387\u5dee\u5f022%-3%\uff09\uff0c\u53c2\u6570\u4ec5\u4e3aVGG-19\u76841/85\uff0c\u5206\u5272IoU\u4e3a0.83\uff0c\u5206\u7c7b\u51c6\u786e\u738790%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5bab\u9888\u764c\u65e9\u671f\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u6548\u7387\u9ad8\uff0c\u9002\u5408\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2505.15507", "pdf": "https://arxiv.org/pdf/2505.15507", "abs": "https://arxiv.org/abs/2505.15507", "authors": ["Mahesh Godavarti"], "title": "Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IR", "20-XX, 08A02", "F.4.1; I.2"], "comment": "11 pages submitted to NeurIPS 2025", "summary": "We introduce a new algebraic structure for multi-dimensional compositional\nembeddings, built on directional non-commutative monoidal operators. The core\ncontribution of this work is this novel framework, which exhibits appealing\ntheoretical properties (associativity along each dimension and an interchange\nlaw ensuring global consistency) while remaining compatible with modern machine\nlearning architectures. Our construction defines a distinct composition\noperator circ_i for each axis i, ensuring associative combination along each\naxis without imposing global commutativity. Importantly, all axis-specific\noperators commute with one another, enforcing a global interchange law that\nenables consistent crossaxis compositions. This is, to our knowledge, the first\napproach that provides a common foundation that generalizes classical\nsequence-modeling paradigms (e.g., structured state-space models (SSMs) and\ntransformer self-attention) to a unified multi-dimensional framework. For\nexample, specific one-dimensional instances of our framework can recover the\nfamiliar affine transformation algebra, vanilla self-attention, and the\nSSM-style recurrence. The higher-dimensional generalizations naturally support\nrecursive, structure-aware operations in embedding spaces. We outline several\npotential applications unlocked by this structure-including structured\npositional encodings in Transformers, directional image embeddings, and\nsymbolic modeling of sequences or grids-indicating that it could inform future\ndeep learning model designs. We formally establish the algebraic properties of\nour framework and discuss efficient implementations. Finally, as our focus is\ntheoretical, we include no experiments here and defer empirical validation to\nfuture work, which we plan to undertake.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7ef4\u7ec4\u5408\u5d4c\u5165\u4ee3\u6570\u7ed3\u6784\uff0c\u57fa\u4e8e\u65b9\u5411\u6027\u975e\u4ea4\u6362\u5e7a\u534a\u7fa4\u7b97\u5b50\uff0c\u5177\u6709\u7406\u8bba\u4f18\u52bf\u4e14\u517c\u5bb9\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u67b6\u6784\u3002", "motivation": "\u4e3a\u591a\u7ef4\u5ea6\u7ec4\u5408\u5d4c\u5165\u63d0\u4f9b\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u540c\u65f6\u517c\u5bb9\u73b0\u6709\u5e8f\u5217\u5efa\u6a21\u8303\u5f0f\uff08\u5982SSMs\u548cTransformer\u81ea\u6ce8\u610f\u529b\uff09\u3002", "method": "\u5b9a\u4e49\u6bcf\u4e2a\u8f74\u7684\u72ec\u7acb\u7ec4\u5408\u7b97\u5b50circ_i\uff0c\u786e\u4fdd\u8f74\u5185\u7ed3\u5408\u6027\u4e14\u8f74\u95f4\u53ef\u4ea4\u6362\uff0c\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u63a8\u5e7f\u7ecf\u5178\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff08\u5982SSMs\u548c\u81ea\u6ce8\u610f\u529b\uff09\u5230\u591a\u7ef4\u573a\u666f\uff0c\u5e76\u652f\u6301\u7ed3\u6784\u611f\u77e5\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u7ed3\u6784\u4e3a\u672a\u6765\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6f5c\u5728\u5e94\u7528\u5305\u62ec\u7ed3\u6784\u5316\u4f4d\u7f6e\u7f16\u7801\u548c\u65b9\u5411\u6027\u56fe\u50cf\u5d4c\u5165\u3002"}}
{"id": "2505.15516", "pdf": "https://arxiv.org/pdf/2505.15516", "abs": "https://arxiv.org/abs/2505.15516", "authors": ["Christiaan Meijer", "E. G. Patrick Bos"], "title": "Explainable embeddings with Distance Explainer", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68T99", "I.2.m"], "comment": "33 pages, 19 figures. Submitted to JMLR. Method implementation:\n  https://research-software-directory.org/software/distance-explainer", "summary": "While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDistance Explainer\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5d4c\u5165\u5f0f\u5411\u91cf\u7a7a\u95f4\u4e2d\u751f\u6210\u5c40\u90e8\u89e3\u91ca\uff0c\u586b\u8865\u4e86XAI\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524dXAI\u65b9\u6cd5\u5728\u5d4c\u5165\u5f0f\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7ef4\u5ea6\u8868\u793a\u590d\u6742\u62bd\u8c61\u65f6\u3002", "method": "\u901a\u8fc7\u9009\u62e9\u6027\u63a9\u7801\u548c\u8ddd\u79bb\u6392\u5e8f\u63a9\u7801\u8fc7\u6ee4\uff0c\u5c06RISE\u7684\u663e\u8457\u6027\u6280\u672f\u5e94\u7528\u4e8e\u89e3\u91ca\u5d4c\u5165\u5f0f\u7a7a\u95f4\u4e2d\u4e24\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002", "result": "\u5728ImageNet\u548cCLIP\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u76f8\u4f3c\u6027\u6216\u5dee\u5f02\u6027\u7279\u5f81\uff0c\u5e76\u4fdd\u6301\u9ad8\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "Distance Explainer\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e2d\u5d4c\u5165\u5f0f\u7a7a\u95f4\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2505.15594", "pdf": "https://arxiv.org/pdf/2505.15594", "abs": "https://arxiv.org/abs/2505.15594", "authors": ["Yury Belousov", "Brian Pulfer", "Vitaliy Kinakh", "Slava Voloshynovskiy"], "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Paper accepted at the 33rd European Signal Processing Conference\n  (EUSIPCO 2025)", "summary": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6269\u6563\u53bb\u566a\u5e73\u6ed1\u6280\u672f\u5728\u589e\u5f3a\u57fa\u7840\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6548\u679c\uff0c\u53d1\u73b0\u9ad8\u566a\u58f0\u8bbe\u7f6e\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u4f4e\u566a\u58f0\u8bbe\u7f6e\u5219\u65e0\u6cd5\u63d0\u4f9b\u5168\u9762\u4fdd\u62a4\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u867d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u5bf9\u6297\u8f93\u5165\u4ecd\u8106\u5f31\uff0c\u6269\u6563\u53bb\u566a\u5e73\u6ed1\u6280\u672f\u6f5c\u529b\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u56db\u79cd\u4e0b\u6e38\u4efb\u52a1\u548c\u4e09\u79cd\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\uff0c\u5206\u6790\u6269\u6563\u53bb\u566a\u5e73\u6ed1\u7684\u6548\u679c\u3002", "result": "\u9ad8\u566a\u58f0\u53bb\u566a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0857%\uff09\uff0c\u4f4e\u566a\u58f0\u65e0\u6cd5\u5168\u9762\u9632\u5fa1\uff0c\u4e14\u65b0\u653b\u51fb\u7b56\u7565\u53ef\u7ed5\u8fc7\u4f4e\u566a\u58f0\u9632\u5fa1\u3002", "conclusion": "\u5bf9\u6297\u9c81\u68d2\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u4ecd\u9700\u89e3\u51b3\u3002"}}
{"id": "2505.15595", "pdf": "https://arxiv.org/pdf/2505.15595", "abs": "https://arxiv.org/abs/2505.15595", "authors": ["S\u00e9bastien Pi\u00e9rard", "Adrien Deli\u00e8ge", "Ana\u00efs Halin", "Marc Van Droogenbroeck"], "title": "A Methodology to Evaluate Strategies Predicting Rankings on Unseen Domains", "categories": ["cs.PF", "cs.CV"], "comment": null, "summary": "Frequently, multiple entities (methods, algorithms, procedures, solutions,\netc.) can be developed for a common task and applied across various domains\nthat differ in the distribution of scenarios encountered. For example, in\ncomputer vision, the input data provided to image analysis methods depend on\nthe type of sensor used, its location, and the scene content. However, a\ncrucial difficulty remains: can we predict which entities will perform best in\na new domain based on assessments on known domains, without having to carry out\nnew and costly evaluations? This paper presents an original methodology to\naddress this question, in a leave-one-domain-out fashion, for various\napplication-specific preferences. We illustrate its use with 30 strategies to\npredict the rankings of 40 entities (unsupervised background subtraction\nmethods) on 53 domains (videos).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u65b0\u9886\u57df\u4e2d\u4e0d\u540c\u5b9e\u4f53\u7684\u6027\u80fd\u6392\u540d\uff0c\u65e0\u9700\u8fdb\u884c\u6602\u8d35\u7684\u65b0\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u9886\u57df\u4e2d\u9884\u6d4b\u5b9e\u4f53\u6027\u80fd\u6392\u540d\u7684\u96be\u9898\uff0c\u907f\u514d\u91cd\u590d\u8bc4\u4f30\u7684\u6210\u672c\u3002", "method": "\u91c7\u7528\u7559\u4e00\u57df\u51fa\u7684\u65b9\u5f0f\uff0c\u7ed3\u5408\u7279\u5b9a\u5e94\u7528\u504f\u597d\uff0c\u8bc4\u4f3030\u79cd\u7b56\u7565\u5bf940\u79cd\u5b9e\u4f53\u572853\u4e2a\u89c6\u9891\u9886\u57df\u7684\u6392\u540d\u9884\u6d4b\u3002", "result": "\u5c55\u793a\u4e86\u65b9\u6cd5\u5728\u80cc\u666f\u51cf\u9664\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8de8\u9886\u57df\u6027\u80fd\u9884\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.15660", "pdf": "https://arxiv.org/pdf/2505.15660", "abs": "https://arxiv.org/abs/2505.15660", "authors": ["Jiaming Zhou", "Ke Ye", "Jiayi Liu", "Teli Ma", "Zifang Wang", "Ronghe Qiu", "Kun-Yu Lin", "Zhilin Zhao", "Junwei Liang"], "title": "Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization", "categories": ["cs.RO", "cs.CV"], "comment": "Project Page: https://jiaming-zhou.github.io/AGNOSTOS", "summary": "The generalization capabilities of vision-language-action (VLA) models to\nunseen tasks are crucial to achieving general-purpose robotic manipulation in\nopen-world settings. However, the cross-task generalization capabilities of\nexisting VLA models remain significantly underexplored. To address this gap, we\nintroduce AGNOSTOS, a novel simulation benchmark designed to rigorously\nevaluate cross-task zero-shot generalization in manipulation. AGNOSTOS\ncomprises 23 unseen manipulation tasks for testing, distinct from common\ntraining task distributions, and incorporates two levels of generalization\ndifficulty to assess robustness. Our systematic evaluation reveals that current\nVLA models, despite being trained on diverse datasets, struggle to generalize\neffectively to these unseen tasks. To overcome this limitation, we propose\nCross-Task In-Context Manipulation (X-ICM), a method that conditions large\nlanguage models (LLMs) on in-context demonstrations from seen tasks to predict\naction sequences for unseen tasks. Additionally, we introduce a dynamics-guided\nsample selection strategy that identifies relevant demonstrations by capturing\ncross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task\nzero-shot generalization performance over leading VLAs. We believe AGNOSTOS and\nX-ICM will serve as valuable tools for advancing general-purpose robotic\nmanipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AGNOSTOS\u57fa\u51c6\u548cX-ICM\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u8de8\u4efb\u52a1\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAGNOSTOS\u57fa\u51c6\u6d4b\u8bd523\u4e2a\u672a\u89c1\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1X-ICM\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u52a8\u6001\u5f15\u5bfc\u6837\u672c\u9009\u62e9\u7b56\u7565\u3002", "result": "X-ICM\u663e\u8457\u63d0\u5347\u4e86\u8de8\u4efb\u52a1\u96f6\u6837\u672c\u6cdb\u5316\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "AGNOSTOS\u548cX-ICM\u4e3a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2505.15810", "pdf": "https://arxiv.org/pdf/2505.15810", "abs": "https://arxiv.org/abs/2505.15810", "authors": ["Yuqi Zhou", "Sunhao Dai", "Shuai Wang", "Kaiwen Zhou", "Qinqlin Jia", "Junxu"], "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86GUI\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff08\u8f93\u5165\u8bbe\u8ba1\u3001\u8f93\u51fa\u8bc4\u4f30\u3001\u7b56\u7565\u66f4\u65b0\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u6700\u7ec8\u5728GUI\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5728\u8bad\u7ec3\u4e2d\u5b58\u5728\u8f93\u5165\u8bbe\u8ba1\u3001\u8f93\u51fa\u8bc4\u4f30\u548c\u7b56\u7565\u66f4\u65b0\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u8981\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "method": "\u63d0\u51faFast Thinking Template\u7b80\u5316\u63a8\u7406\uff0c\u5956\u52b1\u51fd\u6570\u52a0\u5165\u6846\u5927\u5c0f\u7ea6\u675f\uff0c\u8c03\u6574RL\u76ee\u6807\u4ee5\u4f18\u5316\u96be\u6837\u672c\u3002", "result": "GUI-G1-3B\u5728ScreenSpot\u548cScreenSpot-Pro\u4e0a\u5206\u522b\u8fbe\u523090.3%\u548c37.1%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u6539\u8fdb\uff0cGUI\u4ee3\u7406\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4e3aGUI\u4efb\u52a1\u8bbe\u5b9a\u4e86\u65b0\u6807\u6746\u3002"}}
