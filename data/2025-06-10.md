[[toc]]

## cs.CV

### [1] [Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow](https://arxiv.org/abs/2506.06283)
*Juexiao Zhou,Zhongyi Han,Mankun Xin,Xingwei He,Guotao Wang,Jiaoyan Song,Gongning Luo,Wenjia He,Xintong Li,Yuetan Chu,Juanwen Chen,Bo Wang,Xia Wu,Wenwen Duan,Zhixia Guo,Liyan Bai,Yilin Pan,Xuefei Bi,Lu Liu,Long Feng,Xiaonan He,Xin Gao*

Main category: cs.CV

TL;DR: DigitalShadow是一个基于面部基础模型的CAD早期预警系统，通过无接触方式从视频流中提取面部特征，生成个性化健康报告。

- Motivation: 全球老龄化加剧，CAD是主要死因之一，早期检测和管理至关重要。
- Method: 系统预训练2100万张面部图像，微调为LiveCAD模型，使用7004张来自1751名受试者的图像进行CAD风险评估。
- Result: DigitalShadow能被动、无接触地生成个性化风险报告和健康建议。
- Conclusion: 该系统以隐私为核心，支持本地部署，为CAD早期检测提供了创新解决方案。


### [2] [Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images](https://arxiv.org/abs/2506.06389)
*Rifat Sadik,Tanvir Rahman,Arpan Bhattacharjee,Bikash Chandra Halder,Ismail Hossain*

Main category: cs.CV

TL;DR: 论文研究了视觉变换器（ViTs）在医学图像中对对抗性水印攻击的脆弱性，发现其性能显著下降，但通过对抗训练可以大幅提升防御能力。

- Motivation: 随着视觉变换器（ViTs）在计算机视觉任务中的成功应用，研究其在医学图像中对对抗性攻击的脆弱性具有重要意义。
- Method: 使用投影梯度下降（PGD）生成对抗性水印，测试ViTs的脆弱性，并分析对抗训练的效果。
- Result: ViTs在对抗性攻击下准确率降至27.6%，但对抗训练后提升至90.0%。
- Conclusion: ViTs对对抗性攻击较为脆弱，但对抗训练是一种有效的防御手段。


### [3] [(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training](https://arxiv.org/abs/2506.06480)
*A. Postlmayr,P. Cosman,S. Dey*

Main category: cs.CV

TL;DR: 提出了一种基于RGB智能手机摄像头的远程健身追踪系统，具有隐私性、可扩展性和成本效益。

- Motivation: 现有健身追踪模型要么运动种类有限，要么过于复杂难以部署，无法适应多样化运动。
- Method: 开发了一个多任务运动分析模型，结合大规模健身数据集Olympia（1900多种运动），利用视觉-语言模型进行运动检测和重复计数。
- Result: 模型在Olympia数据集上运动检测准确率76.5%，重复计数准确率85.3%。
- Conclusion: 通过单一视觉-语言模型实现运动识别和计数，推动了AI健身追踪的普及。


### [4] [GS4: Generalizable Sparse Splatting Semantic SLAM](https://arxiv.org/abs/2506.06517)
*Mingqi Jiang,Chanho Kim,Chen Ziwen,Li Fuxin*

Main category: cs.CV

TL;DR: 提出了一种基于高斯泼溅（GS）的可泛化语义SLAM算法，通过学习网络增量构建3D场景表示，解决了传统SLAM分辨率低和现有GS方法泛化性差的问题。

- Motivation: 传统SLAM生成的地图分辨率低且不完整，而现有GS方法依赖场景优化，耗时长且泛化性差。
- Method: 使用RGB-D图像识别主干预测高斯参数，集成3D语义分割，并通过全局定位后仅优化1次高斯泼溅来修正漂移。
- Result: 在ScanNet上实现最先进的语义SLAM性能，高斯数量显著减少，并在NYUv2和TUM RGB-D数据集上展示零样本泛化能力。
- Conclusion: 该方法高效、泛化性强，为语义SLAM提供了新思路。


### [5] [Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models](https://arxiv.org/abs/2506.06537)
*Seung-jae Lee,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 提出了一种零样本视听分割框架，利用预训练模型实现无需任务特定训练的分割。

- Motivation: 传统方法依赖大规模像素级标注，成本高且耗时，需改进。
- Method: 整合音频、视觉和文本表示，探索预训练模型连接策略。
- Result: 在多个数据集上实现零样本SOTA性能。
- Conclusion: 多模态模型整合对精细视听分割有效。


### [6] [Securing Traffic Sign Recognition Systems in Autonomous Vehicles](https://arxiv.org/abs/2506.06563)
*Thushari Hapuarachchi,Long Dang,Kaiqi Xiong*

Main category: cs.CV

TL;DR: 研究探讨了深度神经网络（DNNs）在交通标志识别中的鲁棒性，提出了一种基于数据增强的训练方法以抵御误差最小化攻击，并开发了检测模型识别中毒数据。

- Motivation: 由于DNNs在交通标志识别中的广泛应用及其训练数据来源的不确定性，确保模型安全性和抗攻击能力至关重要。
- Method: 通过添加微小扰动实施误差最小化攻击，提出基于非线性变换的数据增强训练方法，并开发检测模型识别中毒数据。
- Result: 攻击使DNNs预测准确率从99.90%降至10.6%，但提出的方法将其恢复至96.05%，检测模型识别攻击成功率超过99%。
- Conclusion: 研究强调了在交通标志识别系统中采用先进训练方法以抵御数据中毒攻击的必要性。


### [7] [Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models](https://arxiv.org/abs/2506.06569)
*Yannis Spyridis,Vasileios Argyriou*

Main category: cs.CV

TL;DR: 论文探讨了利用RGB图像和深度学习技术（如迁移学习和基础模型）实现纺织品自动分类和污染物分割的可行性。

- Motivation: 纺织品回收中，准确识别材料成分和检测污染物对自动化分拣至关重要，但现有传感器数据方法仍具挑战性。
- Method: 采用RGB图像和深度学习技术，包括迁移学习（EfficientNetB0）和零样本分割（Grounding DINO + SAM）。
- Result: 分类任务准确率达81.25%，分割任务mIoU为0.90，表现优异。
- Conclusion: RGB图像结合现代深度学习技术可有效支持纺织品回收的自动化预处理任务。


### [8] [A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance](https://arxiv.org/abs/2506.06578)
*Anees Nashath Shaik,Barbara Villarini,Vasileios Argyriou*

Main category: cs.CV

TL;DR: 论文提出了一种数据驱动平台，通过生成合成训练数据来弥补数据集偏差，提升监控系统的准确性和公平性。

- Motivation: 现有监控系统因图像质量低和AI模型对肤色、遮挡的偏见，导致识别准确性下降，需解决数据多样性和公平性问题。
- Method: 利用深度学习（如自动编码器和GANs）生成多样化合成数据，并集成图像增强模块提升低分辨率或遮挡人脸的清晰度。
- Result: 在CelebA数据集上的实验表明，平台提升了数据多样性和模型公平性。
- Conclusion: 该工作减少了AI面部分析的偏见，提升了监控系统在复杂环境中的准确性和可靠性。


### [9] [EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras](https://arxiv.org/abs/2506.06596)
*Youssef Farah,Federico Paredes-Vallés,Guido De Croon,Muhammad Ahmed Humais,Hussain Sajwani,Yahya Zweiri*

Main category: cs.CV

TL;DR: EV-LayerSegNet是一种自监督CNN，用于事件相机的运动分割，通过学习仿射光流和分割掩码来去模糊输入事件，并以去模糊质量作为自监督学习损失。

- Motivation: 事件相机在捕捉运动动态时具有高时间分辨率，但获取地面真实数据昂贵且困难，因此需要自监督学习方法。
- Method: 提出EV-LayerSegNet，通过分层场景动态表示，分别学习仿射光流和分割掩码，并用于去模糊输入事件。
- Result: 在仅含仿射运动的模拟数据集上，IoU和检测率分别达到71%和87%。
- Conclusion: EV-LayerSegNet展示了自监督学习在事件相机运动分割任务中的潜力。


### [10] [RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints](https://arxiv.org/abs/2506.06600)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: 论文提出了一种名为RARL的框架，通过强化学习增强医学视觉语言模型的推理能力，同时保持高效和适应低资源环境。

- Motivation: 当前医学视觉语言模型在泛化性、透明性和计算效率方面存在局限，阻碍了其在资源受限的实际环境中的部署。
- Method: 使用低秩适应和自定义奖励函数对轻量级基础模型Qwen2-VL-2B-Instruct进行微调，结合诊断准确性和推理质量。
- Result: RARL显著提升了医学图像分析和临床推理的性能，在推理任务上比监督微调高出约7.78%，且计算资源需求更低。
- Conclusion: 推理引导学习和推理提示能有效提升医学视觉语言模型的透明性、准确性和资源效率。


### [11] [Zero Shot Composed Image Retrieval](https://arxiv.org/abs/2506.06602)
*Santhosh Kakarla,Gautama Shastry Bulusu Venkata*

Main category: cs.CV

TL;DR: 论文通过改进BLIP-2和Q-Former的视觉-文本特征融合，显著提升了零样本CIR的性能，同时分析了Retrieval-DPO方法的不足。

- Motivation: 解决零样本CIR在FashionIQ基准上Recall@10表现不佳的问题，探索更有效的多模态融合方法。
- Method: 1. 微调BLIP-2和轻量级Q-Former以融合视觉和文本特征；2. 尝试Retrieval-DPO方法优化CLIP文本编码器。
- Result: BLIP-2方法显著提升Recall@10（45.6%衬衫，40.1%裙子，50.4%T恤），平均Recall@50达67.6%；Retrieval-DPO表现极差（0.02%）。
- Conclusion: 有效的CIR需要多模态融合、排名感知目标和高质量负样本，Retrieval-DPO因设计缺陷表现不佳。


### [12] [PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments](https://arxiv.org/abs/2506.06631)
*Minghao Zou,Qingtian Zeng,Yongping Miao,Shangkun Liu,Zilong Wang,Hantao Liu,Wei Zhou*

Main category: cs.CV

TL;DR: PhysLab是一个针对教育场景的视频数据集，专注于复杂物理实验，提供多级注释以支持多种视觉任务。

- Motivation: 现有数据集在标注粒度、领域覆盖和程序指导方面存在不足，限制了视觉解析的进展。
- Method: 引入PhysLab数据集，包含620个长视频和四类代表性实验，支持动作识别、物体检测等任务。
- Result: 建立了强基线并进行了广泛评估，揭示了程序性教育视频解析的关键挑战。
- Conclusion: PhysLab有望推动细粒度视觉解析和智能课堂系统的发展，数据集已公开。


### [13] [Dark Channel-Assisted Depth-from-Defocus from a Single Image](https://arxiv.org/abs/2506.06643)
*Moushumi Medhi,Rajiv Ranjan Sahay*

Main category: cs.CV

TL;DR: 利用暗通道作为补充线索，从单张空间变异散焦模糊图像中估计场景深度。

- Motivation: 现有深度从散焦（DFD）技术通常依赖多张图像，而单张散焦图像的深度估计问题因欠约束而少有研究。
- Method: 利用局部散焦模糊与对比度变化的关系作为关键深度线索，通过端到端对抗训练优化性能。
- Result: 在真实数据上的实验表明，结合暗通道先验的单图像DFD方法能有效估计深度。
- Conclusion: 暗通道先验的引入提升了单图像DFD的性能，验证了方法的有效性。


### [14] [Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling](https://arxiv.org/abs/2506.06645)
*Cheng Peng,Jingxiang Sun,Yushuo Chen,Zhaoqi Su,Zhuo Su,Yebin Liu*

Main category: cs.CV

TL;DR: PGHM是一种基于3D高斯泼溅的通用高效框架，通过引入UV对齐的潜在身份图和分离的多头U-Net，实现从单目视频快速重建高保真人类化身。

- Motivation: 现有方法在单目输入下优化耗时且泛化能力差，PGHM旨在解决这些问题，提升虚拟/增强现实和数字娱乐中化身的重建效率和质量。
- Method: PGHM包含两个核心组件：UV对齐的潜在身份图编码几何与外观，分离的多头U-Net通过条件解码器预测高斯属性。
- Result: PGHM显著提升了效率，仅需约20分钟即可完成单主题优化，且渲染质量与从头优化方法相当。
- Conclusion: PGHM展示了其在现实单目化身创建中的实用性，为高效高保真化身重建提供了可行方案。


### [15] [Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery](https://arxiv.org/abs/2506.06667)
*Yu-Hsuan Ho,Ali Mostafavi*

Main category: cs.CV

TL;DR: Flood-DamageSense是一种专为洪水灾害建筑损坏评估设计的深度学习框架，融合多源数据，显著提升分类性能。

- Motivation: 现有模型在洪水灾害建筑损坏识别上表现不佳，因破坏特征不明显。
- Method: 结合SAR/InSAR、光学影像和洪水风险层，采用多模态Mamba架构，预测损坏等级、洪水范围和建筑轮廓。
- Result: 在Harvey飓风数据上，F1分数比现有方法提升19%，尤其在轻微和中度损坏类别。
- Conclusion: Flood-DamageSense通过风险感知建模和SAR全天候能力，提供更快、更精细的洪水损坏评估。


### [16] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/abs/2506.06680)
*Radha Kodali,Venkata Rao Dhulipalla,Venkata Siva Kishor Tatavarty,Madhavi Nadakuditi,Bharadwaj Thiruveedhula,Suryanarayana Gunnam,Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: 本文提出了一种基于CNN-LSTM的可解释人工智能框架，用于高效分类胚胎图像，解决了传统胚胎评估方法效率低下的问题。

- Motivation: 不孕症对个人生活质量有显著影响，而体外受精（IVF）是解决低生育率问题的主要技术之一。传统胚胎评估方法效率低下且耗时，因此需要一种更高效的解决方案。
- Method: 采用CNN-LSTM混合架构的XAI框架，通过深度学习实现胚胎图像的高精度分类，并保持模型的可解释性。
- Result: 模型在胚胎分类中实现了高准确率，同时通过XAI保持了可解释性。
- Conclusion: 提出的CNN-LSTM框架为胚胎评估提供了一种高效且可解释的解决方案，有望提升IVF的成功率。


### [17] [A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution](https://arxiv.org/abs/2506.06710)
*Qianqian Zhao,Chunle Guo,Tianyi Zhang,Junpei Zhang,Peiyang Jia,Tan Su,Wenjie Jiang,Chongyi Li*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的全方位图像和视频超分辨率方法，并提出了新的真实退化数据集360Insta，填补了现有数据集的不足。

- Motivation: 全方位图像和视频超分辨率在虚拟现实和增强现实中至关重要，但现有数据集多为合成退化，无法反映真实场景。
- Method: 系统回顾了深度学习方法，并引入真实退化数据集360Insta，进行定性和定量评估。
- Result: 新数据集填补了现有基准的不足，提升了超分辨率方法的泛化能力评估。
- Conclusion: 本文为全方位超分辨率研究提供了系统综述和新数据集，并指出了未来研究方向。


### [18] [Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation](https://arxiv.org/abs/2506.06712)
*Saiyu Hu,Chunlei He,Jianfeng Zhang,Dexing Kong,Shoujun Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于双曲平均曲率流的主动轮廓模型（HMCF-ACMs）和双模式正则化流驱动的主动轮廓模型（HDRF-ACMs），通过可调初始速度场和边缘感知力调制，提升了图像分割的精度和抗噪性。

- Motivation: 传统的抛物线平均曲率流驱动的主动轮廓模型（PMCF-ACMs）对初始曲线配置依赖性强，限制了其适应性。本文旨在通过引入双曲平均曲率流和优化算法，解决这一问题。
- Method: 1. 提出HMCF-ACMs，引入可调初始速度场；2. 证明HMCF-ACMs为法向流，并建立其与波动方程的数值等价性；3. 开发HDRF-ACMs，利用平滑Heaviside函数抑制弱边界处的过度扩散；4. 优化加权四阶Runge-Kutta算法求解波动方程。
- Result: 实验表明，HMCF-ACMs和HDRF-ACMs在初始速度和初始轮廓的任务自适应配置下，实现了更高精度的分割，并表现出更强的抗噪性和数值稳定性。
- Conclusion: HMCF-ACMs和HDRF-ACMs通过引入自适应机制和优化算法，显著提升了图像分割的性能，尤其在复杂场景下表现优异。


### [19] [Improving Wildlife Out-of-Distribution Detection: Africas Big Five](https://arxiv.org/abs/2506.06719)
*Mufhumudzi Muthivhi,Jiahao Huo,Fredrik Gustafsson,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 该论文研究了野生动物（特别是非洲五大动物）的分布外检测问题，比较了参数化和非参数化方法，并展示了特征方法的优越性。

- Motivation: 解决人类与野生动物冲突需要准确识别潜在威胁动物，但现有分类模型在未知类别上表现不佳。
- Method: 采用参数化的最近类均值（NCM）和非参数化的对比学习方法，并与其他分布外检测方法对比。
- Result: 特征方法表现更优，NCM结合ImageNet预训练特征在多个指标上显著优于其他方法。
- Conclusion: 特征方法在野生动物分布外检测中具有更强的泛化能力。


### [20] [Mitigating Object Hallucination via Robust Local Perception Search](https://arxiv.org/abs/2506.06729)
*Zixian Gao,Chao Yang,Zhanhui Zhou,Xing Xu,Chaochao Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为LPS的解码方法，用于减少多模态大语言模型中的幻觉现象，尤其在噪声环境中表现优异。

- Motivation: 尽管多模态大语言模型在视觉和语言结合方面取得了成功，但其输出仍存在与图像内容不符的幻觉现象，需要一种简单且无需训练的方法来缓解此问题。
- Method: 提出了Local Perception Search (LPS)，一种利用局部视觉先验信息作为值函数来修正解码过程的推理阶段方法。
- Result: 实验表明，LPS在幻觉基准测试和噪声数据中显著减少了幻觉现象，尤其在噪声环境下表现突出。
- Conclusion: LPS是一种即插即用的方法，兼容多种模型，能有效抑制幻觉现象，尤其在噪声环境中效果显著。


### [21] [RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation](https://arxiv.org/abs/2506.06733)
*Ruoxuan Zhang,Jidong Gao,Bin Wen,Hongxia Xie,Chenming Zhang,Honghan-shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: RecipeGen是一个大规模、真实世界的基准数据集，用于基于食谱的文本到图像（T2I）、图像到视频（I2V）和文本到视频（T2V）生成，填补了现有数据集中细粒度对齐的不足。

- Motivation: 现有数据集缺乏食谱目标、分步指导和视觉内容之间的细粒度对齐，限制了食品计算在烹饪教育和多模态食谱助手中的应用。
- Method: 提出了RecipeGen数据集，包含26,453个食谱、196,724张图片和4,491个视频，涵盖多样化的食材、烹饪步骤、风格和菜品类型，并设计了领域特定的评估指标。
- Result: RecipeGen为T2I、I2V和T2V模型提供了基准，并评估了食材保真度和交互建模能力。
- Conclusion: RecipeGen为未来的食谱生成模型提供了重要参考和评估标准。


### [22] [THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised Video Object Segmentation](https://arxiv.org/abs/2506.06748)
*Mingqi Gao,Haoran Duan,Tianlu Zhang,Jungong Han*

Main category: cs.CV

TL;DR: 提出了一种结合视觉预训练和深度几何线索的自中心视频对象分割方法，在VISOR测试集上达到90.1%的J&F分数。

- Motivation: 处理复杂场景和长期跟踪的挑战。
- Method: 结合SAM2的大规模视觉预训练和深度几何线索，统一框架处理。
- Result: 在VISOR测试集上J&F分数为90.1%。
- Conclusion: 统一框架有效提升了分割性能。


### [23] [SAR2Struct: Extracting 3D Semantic Structural Representation of Aircraft Targets from Single-View SAR Image](https://arxiv.org/abs/2506.06757)
*Ziyu Yue,Ruixi You,Feng Xu*

Main category: cs.CV

TL;DR: 提出了一种从单视角SAR图像推断目标结构的新任务，通过两步算法框架实现3D语义结构恢复。

- Motivation: 现有方法忽视结构建模在语义信息捕捉中的作用，本文旨在直接从SAR图像中恢复目标的结构关系。
- Method: 基于结构描述符的两步算法框架：训练阶段从真实SAR图像检测2D关键点并学习其到3D结构的映射；测试阶段整合这两步从真实SAR图像推断3D结构。
- Result: 实验验证了各步骤的有效性，首次证明可从单视角SAR图像直接推导飞机目标的3D语义结构表示。
- Conclusion: 该方法成功实现了从SAR图像中恢复目标结构的语义信息，为SAR高级信息检索提供了新思路。


### [24] [LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security](https://arxiv.org/abs/2506.06759)
*Nidheesh Gorthi,Kartik Thakral,Rishabh Ranjan,Richa Singh,Mayank Vatsa*

Main category: cs.CV

TL;DR: LitMAS是一个轻量级、通用的多模态反欺骗框架，用于检测语音、人脸、虹膜和指纹生物识别系统中的欺骗攻击。

- Motivation: 生物识别认证系统在关键应用中广泛部署，但仍易受欺骗攻击，现有研究多关注特定模态的反欺骗技术，缺乏统一且高效的跨模态解决方案。
- Method: LitMAS采用模态对齐集中损失（Modality-Aligned Concentration Loss），增强类间分离性，同时保持跨模态一致性，实现高效欺骗检测。
- Result: LitMAS仅需600万参数，在七个数据集上的平均EER比现有最优方法提升1.36%，表现出高效性、强泛化性和边缘部署适用性。
- Conclusion: LitMAS为跨模态生物识别反欺骗提供了一种高效、通用的解决方案，适合实际应用部署。


### [25] [LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization and Mapping](https://arxiv.org/abs/2506.06771)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Dorian Cojocaru,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopDB是一个包含1000多张多样化环境图像的闭环检测数据集，用于评估和训练SLAM中的闭环算法。

- Motivation: 为闭环检测算法提供高质量的基准数据集，支持SLAM技术的改进和深度学习方法的训练。
- Method: 使用高分辨率相机采集多样化场景的五张连续图像序列，并提供旋转和平移的真值数据。
- Result: 数据集公开可用，适用于闭环算法的基准测试和神经网络训练。
- Conclusion: LoopDB为闭环检测研究提供了有价值的资源，支持算法评估和深度学习应用。


### [26] [Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations](https://arxiv.org/abs/2506.06780)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 提出了一种基于神经控制微分方程的方法，用于建模连续时间旋转物体动力学，解决了SO(3)外推中的噪声、稀疏性和复杂动态问题。

- Motivation: SO(3)外推在计算机视觉和机器人学中至关重要，但面临观测噪声、稀疏性、复杂动态和长期预测需求等挑战。
- Method: 使用神经控制微分方程结合Savitzky-Golay路径，建模旋转物体动力学，保留旋转的几何结构。
- Result: 在真实数据实验中，相比现有方法，展示了更强的预测能力。
- Conclusion: 该方法通过几何结构保留和动态学习，显著提升了旋转物体轨迹的预测性能。


### [27] [Training-Free Identity Preservation in Stylized Image Generation Using Diffusion Models](https://arxiv.org/abs/2506.06802)
*Mohammad Ali Rezaei,Helia Hajikazem,Saeed Khanehgir,Mahdi Javanmardi*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的无训练框架，用于身份保留的风格化图像合成，解决了小面部或远距离拍摄时身份保留不足的问题。

- Motivation: 现有风格迁移技术在保持身份的同时实现高质量风格化方面存在困难，尤其是在小面部或远距离拍摄的图像中。
- Method: 提出了两种关键技术：1) "Mosaic Restored Content Image" 增强身份保留；2) 无训练内容一致性损失，提高细节保留。
- Result: 实验表明，该方法在保持高风格保真度和身份完整性方面显著优于基线模型，尤其适用于小面部或远距离场景。
- Conclusion: 该框架无需重新训练或微调，有效解决了身份保留问题，适用于复杂场景。


### [28] [Stepwise Decomposition and Dual-stream Focus: A Novel Approach for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2506.06818)
*Chao Yin,Hao Li,Kequan Yang,Jide Li,Pinpin Zhu,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 论文提出RDVP-MSD框架，通过多模态逐步分解链式思维（MSD-CoT）和区域约束双流视觉提示（RDVP），解决伪装目标分割中的语义模糊和空间分离问题，无需训练即可实现最先进的分割效果。

- Motivation: 当前任务通用提示分割方法在伪装目标分割（COS）中存在语义模糊和空间分离问题，导致分割不准确。
- Method: 结合MSD-CoT逐步分解图像描述消除语义模糊，RDVP通过空间约束和双流视觉提示解决空间分离问题。
- Result: 在多个COS基准测试中实现最先进的分割效果，且推理速度更快。
- Conclusion: RDVP-MSD无需训练即可显著提升分割准确性和效率，为COS任务提供了一种高效解决方案。


### [29] [Hi-LSplat: Hierarchical 3D Language Gaussian Splatting](https://arxiv.org/abs/2506.06822)
*Chenlu Zhan,Yufei Zhang,Gaoang Wang,Hongwei Wang*

Main category: cs.CV

TL;DR: Hi-LSplat提出了一种基于高斯泼溅的层次化语言3D表示方法，解决了现有方法中视图不一致和开放词汇挑战的问题。

- Motivation: 现有3DGS模型依赖2D基础模型，导致视图不一致和层次语义理解不足。
- Method: 通过构建3D层次语义树和引入实例与部件对比损失，实现视图一致的3D语义表示。
- Result: 实验表明Hi-LSplat在3D开放词汇分割和定位中表现优越，能捕捉复杂层次语义。
- Conclusion: Hi-LSplat为3D开放词汇查询提供了视图一致且层次化的语义表示解决方案。


### [30] [Exploring Visual Prompting: Robustness Inheritance and Beyond](https://arxiv.org/abs/2506.06823)
*Qi Li,Liangzhi Li,Zhouqiang Jiang,Bowen Wang,Keke Tang*

Main category: cs.CV

TL;DR: 本文探讨了视觉提示（VP）在鲁棒源模型下的表现，提出了一种名为Prompt Boundary Loosening（PBL）的策略，以解决VP在鲁棒性和泛化能力之间的权衡问题。

- Motivation: 研究VP在鲁棒源模型下的表现，探索其是否能继承鲁棒性，是否存在鲁棒性与泛化能力的权衡，并提出解决方案。
- Method: 提出PBL策略，作为一种轻量级、即插即用的方法，与VP兼容，旨在确保鲁棒性的继承并提升泛化能力。
- Result: 实验表明PBL能有效继承鲁棒性并显著提升VP的泛化能力，结果具有普遍性。
- Conclusion: PBL策略成功解决了VP在鲁棒源模型下的权衡问题，为相关研究提供了新思路。


### [31] [Controllable Coupled Image Generation via Diffusion Models](https://arxiv.org/abs/2506.06826)
*Chenfei Yuan,Nanshan Jia,Hangqi Li,Peter W. Glynn,Zeyu Zheng*

Main category: cs.CV

TL;DR: 提出了一种注意力级别控制方法，用于耦合图像生成任务，背景相似但中心对象灵活。

- Motivation: 解决多图像生成中背景耦合与对象灵活性的需求。
- Method: 通过解耦背景和实体组件，结合时间变化的权重控制参数优化。
- Result: 在背景耦合、文本对齐和视觉质量上优于现有方法。
- Conclusion: 方法有效平衡了背景一致性和对象多样性。


### [32] [EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2506.06830)
*Guankun Wang,Rui Tang,Mengya Xu,Long Bai,Huxin Gao,Hongliang Ren*

Main category: cs.CV

TL;DR: EndoARSS是一种基于DINOv2的多任务学习框架，用于内窥镜手术活动识别和语义分割，通过低秩适配和空间感知多尺度注意力提升性能。

- Motivation: 内窥镜手术场景复杂，传统深度学习模型在跨活动干扰下表现不佳，需多任务学习提升性能。
- Method: 结合低秩适配和任务共享适配器减少梯度冲突，引入空间感知多尺度注意力增强特征表示。
- Result: 在多个基准测试中表现优异，显著提升准确性和鲁棒性。
- Conclusion: EndoARSS有望推动AI驱动的内窥镜手术系统发展，提升手术安全性和效率。


### [33] [Harnessing Vision-Language Models for Time Series Anomaly Detection](https://arxiv.org/abs/2506.06836)
*Zelin He,Sarah Alnegheimish,Matthew Reimherr*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型（VLM）的两阶段时间序列异常检测方法，结合轻量级视觉编码器和VLM推理能力，显著提升了检测性能。

- Motivation: 现有方法缺乏视觉-时间推理能力，无法像人类专家一样识别上下文异常，因此探索基于VLM的解决方案。
- Method: 1. ViT4TS：轻量级视觉编码器定位候选异常；2. VLM4TS：整合全局时间上下文和VLM推理能力优化检测。
- Result: VLM4TS在未训练时间序列数据的情况下，F1-max得分比最佳基线提升24.6%，且效率更高。
- Conclusion: 该方法在性能和效率上均优于现有方法，展示了VLM在时间序列异常检测中的潜力。


### [34] [Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles](https://arxiv.org/abs/2506.06846)
*Yangkai Lin,Jiabao Lei,Kui jia*

Main category: cs.CV

TL;DR: 提出了一种名为Multi-StyleGS的新方法，用于3D高斯泼溅（GS）的多风格化，通过自动局部风格转移和语义风格损失函数实现高效训练和更好的视觉效果。

- Motivation: 近年来，对3D场景进行艺术风格化的需求增加，但现有方法在适应多风格化和保持内存效率方面存在挑战。
- Method: 采用二分匹配机制自动匹配风格图像与渲染图像的局部区域，提出语义风格损失函数和局部-全局特征匹配技术，优化分割网络以增强多视角一致性。
- Result: 实验表明，该方法在风格化效果、内存效率和编辑灵活性上优于现有方法。
- Conclusion: Multi-StyleGS为3D GS风格化提供了高效且灵活的解决方案，适用于多风格化和局部编辑。


### [35] [Deep Inertial Pose: A deep learning approach for human pose estimation](https://arxiv.org/abs/2506.06850)
*Sara M. Cerqueira,Manuel Palermo,Cristina P. Santos*

Main category: cs.CV

TL;DR: 论文研究了利用神经网络简化惯性运动捕捉系统中复杂的人体姿态估计步骤，比较了不同神经网络架构和方法，并验证了其效果。

- Motivation: 惯性运动捕捉系统因其便携性和无约束性受到关注，但现有方法复杂且昂贵。研究旨在探索神经网络是否能替代传统复杂模型。
- Method: 比较了不同神经网络架构和方法，包括Hybrid LSTM-Madgwick，并使用低成本和高端的MARG传感器进行实验。
- Result: Hybrid LSTM-Madgwick方法表现最佳，误差为7.96（使用高端传感器）。数据增强、输出表示等因素对误差有影响。
- Conclusion: 神经网络可以用于人体姿态估计，效果接近现有最优融合滤波器。


### [36] [Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation](https://arxiv.org/abs/2506.06852)
*John Waithaka,Moise Busogi*

Main category: cs.CV

TL;DR: 本文提出了一种基于位置预测的自监督学习方法（LOCA），用于多模态卫星图像语义分割，显著优于现有的基于重建的方法。

- Motivation: 卫星图像语义分割受限于标记数据不足，现有自监督方法（如MAE）专注于重建而非定位，而定位是分割任务的关键。
- Method: 扩展SatMAE的通道分组至多模态数据，引入同组注意力掩码以促进跨模态交互，采用相对补丁位置预测任务以增强空间推理。
- Result: 在Sen1Floods11洪水映射数据集上，该方法显著优于基于重建的自监督学习方法。
- Conclusion: 针对多模态卫星图像的位置预测任务能学习到更有效的表示，优于重建方法。


### [37] [DONUT: A Decoder-Only Model for Trajectory Prediction](https://arxiv.org/abs/2506.06854)
*Markus Knoche,Daan de Geus,Bastian Leibe*

Main category: cs.CV

TL;DR: DONUT是一种仅解码器网络，用于预测轨迹，通过自回归模型实现迭代预测，性能优于编码器-解码器基线。

- Motivation: 自动驾驶需要预测其他代理的运动以实现预判，现有编码器-解码器模型存在局限性。
- Method: 使用仅解码器网络（DONUT），通过自回归模型迭代预测轨迹，并引入‘过度预测’策略以提升性能。
- Result: 在Argoverse 2单代理运动预测基准上达到新SOTA。
- Conclusion: 仅解码器方法在轨迹预测任务中表现优异，具有潜力。


### [38] [Vision-EKIPL: External Knowledge-Infused Policy Learning for Visual Reasoning](https://arxiv.org/abs/2506.06856)
*Chaoyang Wang,Zeyu Zhang,Haiyun Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种新的强化学习框架Vision-EKIPL，通过引入外部辅助模型生成的高质量动作来优化策略模型，显著提升了多模态大语言模型的视觉推理能力和训练效率。

- Motivation: 现有强化学习方法仅从策略模型本身采样动作组，限制了模型的推理能力上限并导致训练效率低下。
- Method: 提出Vision-EKIPL框架，在强化学习训练过程中引入外部辅助模型生成的高质量动作，指导策略模型优化。
- Result: 在Reason-RFT-CoT Benchmark上性能提升5%，显著加速训练收敛速度。
- Conclusion: Vision-EKIPL克服了传统强化学习方法的限制，为多模态大语言模型的视觉推理研究提供了新范式。


### [39] [Face recognition on point cloud with cgan-top for denoising](https://arxiv.org/abs/2506.06864)
*Junyu Liu,Jianfeng Ren,Sunhong Liang,Xudong Jiang*

Main category: cs.CV

TL;DR: 提出了一种端到端的3D人脸识别方法，结合去噪和识别模块，显著提高了噪声点云下的识别精度。

- Motivation: 原始点云常因传感器不完美而包含大量噪声，影响识别效果。
- Method: 设计了cGAN-TOP去噪模块和LDGCNN识别模块，协同工作。
- Result: 在Bosphorus数据集上验证，识别精度显著提升，最高增益14.81%。
- Conclusion: 该方法有效解决了噪声点云下的3D人脸识别问题。


### [40] [Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via Eye-Tracking Analysis](https://arxiv.org/abs/2506.06886)
*Wafaa Kasri,Yassine Himeur,Abigail Copiaco,Wathiq Mansoor,Ammar Albanna,Valsamma Eapen*

Main category: cs.CV

TL;DR: 提出了一种结合Vision Transformers和Vision Mamba的混合深度学习框架，用于通过眼动数据检测自闭症谱系障碍（ASD），显著提升了诊断准确性和可解释性。

- Motivation: 早期诊断ASD对干预至关重要，但传统方法依赖手工特征且缺乏透明度。
- Method: 采用Vision Transformers和Vision Mamba的混合框架，结合注意力融合技术整合视觉、语音和面部线索。
- Result: 在Saliency4ASD数据集上，模型表现优异（准确率0.96，F1分数0.95，灵敏度0.97，特异度0.94）。
- Conclusion: 该模型为资源有限或偏远地区的ASD筛查提供了可扩展且可解释的解决方案。


### [41] [NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery](https://arxiv.org/abs/2506.06898)
*Reese Kneeland,Paul S. Scotti,Ghislain St-Yves,Jesse Breedlove,Kendrick Kay,Thomas Naselaris*

Main category: cs.CV

TL;DR: NSD-Imagery是一个新发布的基准数据集，用于评估fMRI到图像重建模型在心理图像上的表现，填补了现有NSD数据集的空白。

- Motivation: 现有模型仅在已见图像重建上评估，而心理图像重建对医学和脑机接口应用至关重要。
- Method: 使用NSD-Imagery评估多个开源视觉解码模型（如MindEye1、Brain Diffuser等）在心理图像上的表现。
- Result: 模型在心理图像上的表现与视觉重建表现脱钩，简单线性架构和多模态特征解码模型表现更好。
- Conclusion: 心理图像数据集对实际应用至关重要，NSD-Imagery为视觉解码方法的开发提供了重要资源。


### [42] [KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search](https://arxiv.org/abs/2506.06906)
*Nima Jamali,Matina Mahdizadeh Sani,Hanieh Naderi,Shohreh Kasaei*

Main category: cs.CV

TL;DR: KNN-Defense是一种轻量级防御策略，通过特征空间中的最近邻搜索恢复受扰动的3D点云数据，显著提升了对抗攻击下的鲁棒性。

- Motivation: 深度神经网络在3D点云数据分析中表现出色，但对对抗攻击（如点删除、移动和添加）的脆弱性威胁了3D视觉系统的可靠性。
- Method: 基于流形假设和特征空间中的最近邻搜索，KNN-Defense通过训练集中邻近样本的语义相似性恢复受扰动的输入。
- Result: 在ModelNet40数据集上，KNN-Defense显著提升了对抗攻击下的鲁棒性，特别是在点删除攻击下，对多种模型的准确率提升显著。
- Conclusion: KNN-Defense为增强3D点云分类器的对抗鲁棒性提供了一种可扩展且高效的解决方案。


### [43] [Gaussian Mapping for Evolving Scenes](https://arxiv.org/abs/2506.06909)
*Vladimir Yugay,Thies Kersten,Luca Carlone,Theo Gevers,Martin R. Oswald,Lukas Schmid*

Main category: cs.CV

TL;DR: 论文提出了一种动态场景适应机制和关键帧管理机制，用于解决3D高斯泼溅系统中长期动态场景的问题。

- Motivation: 当前3D高斯泼溅系统主要针对静态场景，对长期动态场景（如场景在视野外变化）的研究较少，因此需要一种机制来持续更新3D表示。
- Method: 引入动态场景适应机制以持续更新3D表示，并提出关键帧管理机制以丢弃过时观测并保留有效信息。
- Result: 在合成和真实数据集上评估，GaME方法比现有技术更准确。
- Conclusion: GaME方法通过动态适应和关键帧管理，有效解决了长期动态场景的问题，提升了性能。


### [44] [Sleep Stage Classification using Multimodal Embedding Fusion from EOG and PSM](https://arxiv.org/abs/2506.06912)
*Olivier Papillon,Rafik Goubran,James Green,Julien Larivière-Chartier,Caitlin Higginson,Frank Knoefel,Rébecca Robillard*

Main category: cs.CV

TL;DR: 该论文提出了一种利用ImageBind多模态嵌入深度学习模型，结合压力敏感垫（PSM）和双通道眼电图（EOG）信号进行睡眠阶段分类的新方法，显著提高了分类准确性。

- Motivation: 传统多导睡眠图（PSG）依赖脑电图（EEG）作为金标准，但其复杂性和对专业设备的需求限制了家庭睡眠监测的应用。因此，研究探索了EOG和PSM作为更便捷的替代方案。
- Method: 研究采用ImageBind模型，整合PSM数据和双通道EOG信号进行睡眠阶段分类，并首次将PSM与EOG数据融合应用于此任务。
- Result: 实验结果显示，该方法显著优于基于单通道EOG、仅PSM数据或其他多模态深度学习的现有模型，且在未微调时也表现优异。
- Conclusion: 研究表明，预训练的多模态嵌入模型可有效用于睡眠分期，其准确性接近依赖复杂EEG数据的系统，尤其适合医疗应用。


### [45] [Reading in the Dark with Foveated Event Vision](https://arxiv.org/abs/2506.06918)
*Carl Brander,Giovanni Cioffi,Nico Messikommer,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 提出了一种基于事件的新型OCR方法，用于智能眼镜，通过用户视线聚焦事件流，显著减少带宽，并在低光和高动态场景中优于传统OCR。

- Motivation: 智能眼镜的RGB相机在低光和高动态场景中表现不佳，且带宽和功耗高，影响电池寿命。
- Method: 利用用户视线聚焦事件流，结合深度二进制重建和多模态LLMs进行OCR。
- Result: 在低光环境中表现优异，带宽减少达2400倍。
- Conclusion: 该方法在低光和高动态场景中高效且节能，优于传统OCR。


### [46] [How Important are Videos for Training Video LLMs?](https://arxiv.org/abs/2506.06928)
*George Lydakis,Alexander Hermans,Ali Athar,Daan de Geus,Bastian Leibe*

Main category: cs.CV

TL;DR: 研究发现，仅通过图像训练的Video LLMs在时间推理能力上表现优于预期，而视频训练的改进效果较小。

- Motivation: 探索Video LLMs在时间推理上的能力，以及图像训练与视频训练的效果差异。
- Method: 使用LongVU算法训练两种LLMs，并在TVBench基准上测试其时间推理能力；引入基于标注图像序列的简单微调方案。
- Result: 图像训练的LLMs在时间推理任务上表现显著高于随机水平，且接近或优于视频训练的LLMs。
- Conclusion: 当前视频训练方案未充分利用视频的时序特征，需进一步研究图像训练LLMs的时间推理机制及视频训练的瓶颈。


### [47] [Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences](https://arxiv.org/abs/2506.06944)
*Mellon M. Zhang,Glen Chou,Saibal Mukhopadhyay*

Main category: cs.CV

TL;DR: PHiM是一种新型SSM架构，专为极坐标流式LiDAR设计，通过局部双向Mamba块和全局前向Mamba块，取代卷积和位置编码，显著提升了流式检测性能。

- Motivation: 自动驾驶需要低延迟、高吞吐的实时感知，但传统LiDAR处理方法存在延迟问题，流式方法虽能缓解但性能受限。
- Method: PHiM采用局部双向Mamba块进行空间编码，全局前向Mamba块进行时间建模，避免了卷积和位置编码的几何失真问题。
- Result: 在Waymo Open Dataset上，PHiM比之前最佳流式检测器性能提升10%，且吞吐量翻倍。
- Conclusion: PHiM为极坐标流式LiDAR提供了一种高效、高性能的解决方案，显著提升了实时感知能力。


### [48] [LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer](https://arxiv.org/abs/2506.06952)
*Ying Shen,Zhiyang Xu,Jiuhai Chen,Shizhe Diao,Jiaxin Zhang,Yuguang Yao,Joy Rimchala,Ismini Lourentzou,Lifu Huang*

Main category: cs.CV

TL;DR: LaTtE-Flow是一种高效的多模态模型，统一了图像理解和生成，通过分层时间步专家架构和流匹配技术，显著提升了生成速度和性能。

- Motivation: 现有统一模型需要大量预训练且性能不及专用模型，生成速度慢，限制了实际应用。
- Method: 基于预训练视觉语言模型，引入分层时间步专家流架构和条件残差注意力机制。
- Result: 在多模态理解任务中表现优异，图像生成质量与现有模型相当，推理速度快6倍。
- Conclusion: LaTtE-Flow通过高效架构设计，解决了统一模型的性能与速度问题，具有实际应用潜力。


### [49] [Task-driven real-world super-resolution of document scans](https://arxiv.org/abs/2506.06953)
*Maciej Zyrek,Tomasz Tarasiewicz,Jakub Sadel,Aleksandra Krzywon,Michal Kawulok*

Main category: cs.CV

TL;DR: 论文提出了一种针对光学字符识别任务优化的超分辨率网络，通过多任务学习框架和动态权重平均机制，结合高层视觉任务的辅助损失函数，提升了真实场景下的文本检测性能。

- Motivation: 尽管现有深度学习方法在模拟数据集上表现良好，但在真实场景（如文档扫描）中因复杂退化和语义变化而表现不佳。
- Method: 采用多任务学习框架，结合文本检测、识别、关键点定位和色调一致性等辅助损失函数，并使用动态权重平均机制平衡目标。
- Result: 在模拟和真实文档数据集上验证，提高了文本检测性能（IoU指标），同时保持图像保真度。
- Conclusion: 多目标优化有助于缩小模拟训练与真实部署之间的差距。


### [50] [AR-RAG: Autoregressive Retrieval Augmentation for Image Generation](https://arxiv.org/abs/2506.06962)
*Jingyuan Qi,Zhiyang Xu,Qifan Wang,Lifu Huang*

Main category: cs.CV

TL;DR: AR-RAG是一种新的图像生成方法，通过逐步检索和整合相关图像块，动态优化生成过程，避免现有方法的局限性。

- Motivation: 现有方法在生成图像时通常基于固定的参考图像，容易导致过度复制或风格偏差，AR-RAG旨在动态适应生成需求。
- Method: 提出了两种框架：DAiD（直接合并预测和检索块的分布）和FAiD（通过多尺度卷积平滑检索块特征）。
- Result: 在多个基准测试中（如Midjourney-30K、GenEval和DPG-Bench），AR-RAG显著优于现有图像生成模型。
- Conclusion: AR-RAG通过动态检索和整合图像块，有效提升了图像生成的质量和灵活性。


### [51] [Dual-view Spatio-Temporal Feature Fusion with CNN-Transformer Hybrid Network for Chinese Isolated Sign Language Recognition](https://arxiv.org/abs/2506.06966)
*Siyuan Jing,Guangxue Wang,Haoyang Zhai,Qin Tao,Jun Yang,Bing Wang,Peng Jin*

Main category: cs.CV

TL;DR: 本文提出了一个双视角的中国手语数据集NationalCSL-DP，并提出了一个CNN-Transformer网络作为基线模型，通过实验验证了数据集和融合策略的有效性。

- Motivation: 现有手语数据集覆盖词汇不全且多为单视角RGB视频，难以处理手部遮挡问题，因此需要构建更全面的数据集和改进识别方法。
- Method: 构建了覆盖中国手语词汇的双视角数据集NationalCSL-DP，并提出CNN-Transformer网络及简单有效的融合策略。
- Result: 实验表明融合策略显著提升了孤立手语识别性能，但序列到序列模型难以从双视角视频中学习互补特征。
- Conclusion: 双视角数据集和融合策略为孤立手语识别提供了新思路，但模型仍需改进以更好地利用多视角信息。


### [52] [Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment](https://arxiv.org/abs/2506.06970)
*Pengfei Zhao,Rongbo Luan,Wei Zhang,Peng Wu,Sifeng He*

Main category: cs.CV

TL;DR: 论文提出MAPLE框架，利用MLLM的细粒度对齐特性改进跨模态表示学习，通过强化学习和新损失函数显著提升检索性能。

- Motivation: 尽管CLIP在多模态检索中表现优异，但仍存在模态鸿沟问题。MLLM具有天然对齐特性，但现有方法依赖粗粒度对齐机制，限制了潜力。
- Method: 提出MAPLE框架，结合MLLM的细粒度对齐先验，通过自动偏好数据构建和RPA损失函数（基于DPO）进行强化学习。
- Result: 实验表明，MAPLE在细粒度跨模态检索中取得显著提升，能有效处理语义细微差异。
- Conclusion: MAPLE通过细粒度对齐机制显著缩小模态鸿沟，为跨模态表示学习提供了新思路。


### [53] [Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction](https://arxiv.org/abs/2506.06988)
*Binxiao Huang,Zhihao Li,Shiyong Liu,Xiao Tang,Jiajun Tang,Jiaqi Lin,Yuxin Cheng,Zhenyu Chen,Xiaofei Wu,Ngai Wong*

Main category: cs.CV

TL;DR: 3D高斯溅射（3DGS）在图像重建和实时渲染中表现优异，但复杂纹理区域需要大量高斯分布，导致渲染效率低。本文提出结合3DGS与纹理网格的混合表示方法，优化渲染速度。

- Motivation: 解决3DGS在复杂纹理区域因高斯分布过多导致的渲染效率问题。
- Method: 提出混合表示方法，用纹理网格处理平坦区域，保留高斯分布处理复杂几何。通过网格修剪和联合优化策略实现高效渲染。
- Result: 实验表明，混合表示在保持渲染质量的同时，显著提升了FPS并减少了高斯基元数量。
- Conclusion: 混合表示方法在室内场景中有效平衡了渲染质量和效率。


### [54] [Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization](https://arxiv.org/abs/2506.06992)
*Yanting Gao,Yepeng Liu,Junming Liu,Qi Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种名为COGO的策略，通过增强共享特征和抑制个体特征，显著提高了对抗样本在黑盒设置中的迁移成功率。

- Motivation: 理解Vision Transformers（ViTs）的特性需要有效的对抗样本，但现有方法因过拟合导致迁移性差。
- Method: COGO策略包括两部分：共性增强（CE）扰动中低频区域，个性抑制（IS）通过自适应阈值评估梯度相关性。
- Result: 实验表明COGO显著提升了对抗攻击的迁移成功率，优于现有方法。
- Conclusion: COGO通过优化共享和抑制个体特征，为ViTs对抗样本研究提供了新思路。


### [55] [DM$^3$Net: Dual-Camera Super-Resolution via Domain Modulation and Multi-scale Matching](https://arxiv.org/abs/2506.06993)
*Cong Guan,Jiacheng Ying,Yuya Ieiri,Osamu Yoshie*

Main category: cs.CV

TL;DR: DM$^3$Net是一种基于域调制和多尺度匹配的双摄像头超分辨率网络，用于提升智能手机摄影中广角图像的分辨率。

- Motivation: 解决双摄像头超分辨率中高分辨率域与退化域之间的域差距问题，并提高高频结构细节的传输可靠性。
- Method: 通过学习两个压缩全局表示来桥接域差距，并设计多尺度匹配模块进行跨多感受野的补丁级特征匹配和检索。此外，引入关键修剪以减少内存使用和推理时间。
- Result: 在三个真实数据集上的实验表明，DM$^3$Net优于现有方法。
- Conclusion: DM$^3$Net通过域调制和多尺度匹配有效提升了双摄像头超分辨率的性能，同时通过关键修剪优化了计算效率。


### [56] [Technical Report for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge: Adaptive Point Cloud Understanding for Heterogeneous Robotic Systems](https://arxiv.org/abs/2506.06995)
*Xiaoya Zhang*

Main category: cs.CV

TL;DR: 本文介绍了ICRA 2025 GOOSE 3D语义分割挑战赛中获胜解决方案的实现细节，通过结合Point Prompt Tuning（PPT）和Point Transformer v3（PTv3）骨干网络，实现了对异构LiDAR数据的自适应处理。

- Motivation: 解决多机器人平台采集的户外非结构化环境中3D点云的语义分割问题。
- Method: 采用PPT与PTv3结合的方法，通过平台特定条件和跨数据集类别对齐策略处理数据。
- Result: 相比基线PTv3模型，mIoU提升高达22.59%。
- Conclusion: 该方法展示了自适应点云理解在野外机器人应用中的有效性。


### [57] [BePo: Leveraging Birds Eye View and Sparse Points for Efficient and Accurate 3D Occupancy Prediction](https://arxiv.org/abs/2506.07002)
*Yunxiao Shi,Hong Cai,Jisoo Jeong,Yinhao Zhu,Shizhong Han,Amin Ansari,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出了一种结合BEV和稀疏点表示的新方法BePo，用于3D占用预测，解决了现有方法在小物体和平坦表面上的不足。

- Motivation: 现有3D占用预测方法计算成本高或在小物体和平坦表面上表现不佳，需要一种更高效的解决方案。
- Method: 采用双分支设计：基于查询的稀疏点分支和BEV分支，通过交叉注意力共享信息，最终融合输出。
- Result: 在Occ3D-nuScenes和Occ3D-Waymo基准测试中表现优越，且推理速度具有竞争力。
- Conclusion: BePo方法在性能和效率上均优于现有方法，为3D占用预测提供了新思路。


### [58] [UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment](https://arxiv.org/abs/2506.07013)
*Wentao Zhao,Yihe Niu,Yanbo Wang,Tianchen Deng,Shenghai Yuan,Zhenli Wang,Rui Guo,Jingchuan Wang*

Main category: cs.CV

TL;DR: UNO是一个统一的单目视觉里程计框架，能够在多样环境中实现鲁棒且自适应的位姿估计。

- Motivation: 传统方法依赖特定部署的调优或预定义的运动先验，而UNO旨在泛化适用于多种实际场景，如自动驾驶、无人机、移动机器人和手持设备。
- Method: 采用Mixture-of-Experts策略进行局部状态估计，结合多个专用解码器处理不同运动模式，并使用Gumbel-Softmax模块构建帧间关联图、选择最优解码器并剔除错误估计。后端结合预训练的尺度无关深度先验和轻量级捆绑调整。
- Result: 在KITTI、EuRoC-MAV和TUM-RGBD三个基准数据集上表现优异，达到最先进水平。
- Conclusion: UNO框架在多样化场景中实现了鲁棒且自适应的位姿估计，性能优于传统方法。


### [59] [TABLET: Table Structure Recognition using Encoder-only Transformers](https://arxiv.org/abs/2506.07015)
*Qiyu Hou,Jun Wang*

Main category: cs.CV

TL;DR: 提出了一种基于Split-Merge的新型表格结构识别方法，通过双Transformer编码器优化行和列的分割，并通过网格分类实现高效合并，显著提升了准确性和处理速度。

- Motivation: 解决大而密集的表格结构识别中的挑战，如不稳定的边界框预测和计算复杂度高的问题。
- Method: 将行和列分割视为序列标注任务，使用双Transformer编码器捕捉特征交互；合并过程作为网格分类任务，利用额外Transformer编码器确保准确性。
- Result: 在FinTabNet和PubTabNet上表现优异，准确率高且处理速度快，适用于实际应用。
- Conclusion: 该方法为大规模表格识别提供了鲁棒、可扩展且高效的解决方案，适合工业部署。


### [60] [MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks](https://arxiv.org/abs/2506.07016)
*Sanjoy Chowdhury,Mohamed Elmoghany,Yohan Abeysinghe,Junjie Fei,Sayan Nag,Salman Khan,Mohamed Elhoseiny,Dinesh Manocha*

Main category: cs.CV

TL;DR: 论文提出了AV-HaystacksQA任务和AVHaystacks基准，用于评估大型多模态模型在多视频检索和时序定位中的表现，并提出了MAGNET框架和两个新评估指标STEM和MTGS。

- Motivation: 现有视频问答基准局限于单视频查询，无法满足实际应用中大规模音频-视觉检索和推理的需求。
- Method: 提出AV-HaystacksQA任务和AVHaystacks基准，设计MAGNET多智能体框架，并引入STEM和MTGS评估指标。
- Result: MAGNET在BLEU@4和GPT评估分数上分别比基线方法提升了89%和65%。
- Conclusion: AVHaystacks和MAGNET为多视频检索和时序定位任务提供了有效的解决方案和评估工具。


### [61] [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
*Yikun Ji,Hong Yan,Jun Lan,Huijia Zhu,Weiqiang Wang,Qi Fan,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于多模态大语言模型（MLLMs）的AI生成图像检测方法，通过标注数据集和多阶段优化策略，显著提升了检测和解释能力。

- Motivation: 现有AI生成图像检测方法多为黑箱，缺乏可解释性；MLLMs虽具潜力，但存在幻觉问题且视觉解释与内容不符。
- Method: 构建标注数据集（含边界框和描述性标题），采用多阶段优化策略微调MLLMs，平衡检测、定位和解释目标。
- Result: 模型在检测AI生成图像和定位视觉缺陷方面表现优异，显著优于基线方法。
- Conclusion: 通过数据标注和多阶段优化，MLLMs可实现高效且可解释的AI生成图像检测。


### [62] [From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion](https://arxiv.org/abs/2506.07050)
*Zheng Wang,Kai Ying,Bin Xu,Chunjiao Wang,Cong Bai*

Main category: cs.CV

TL;DR: 论文提出了一种名为PRE-Net的两阶段模型，通过多模态知识扩展技术提升红外降水反演的准确性，解决了现有方法的范围限制问题。

- Motivation: 现有红外降水反演算法精度低，而微波和雷达方法范围有限，因此需要开发一种既能覆盖全盘又能保持高精度的新方法。
- Method: 采用两阶段管道：扫描带蒸馏阶段通过CoMWE技术将多模态知识迁移到红外模型；全盘适应阶段通过Self-MaskTune平衡多模态与红外知识。
- Result: 实验表明，PRE-Net在降水反演性能上显著优于PERSIANN-CCS、PDIR和IMERG等现有产品。
- Conclusion: PRE-Net成功实现了高精度的全盘红外降水反演，为相关领域提供了新的解决方案。


### [63] [A Layered Self-Supervised Knowledge Distillation Framework for Efficient Multimodal Learning on the Edge](https://arxiv.org/abs/2506.07055)
*Tarique Dahri,Zulfiqar Ali Memon,Zhenyu Yu,Mohd. Yamani Idna Idris,Sheheryar Khan,Sadiq Ahmad,Maged Shoman,Saddam Aziz,Rizwan Qureshi*

Main category: cs.CV

TL;DR: LSSKD是一种自监督知识蒸馏框架，通过中间特征图的辅助分类器生成多样化知识，无需依赖预训练教师网络，在多个数据集上表现优于现有方法。

- Motivation: 传统方法依赖预训练的教师网络，而LSSKD旨在通过自监督方式提升模型性能，适用于计算资源有限的场景。
- Method: 在中间特征图上添加辅助分类器，实现一对一知识迁移，无需教师网络。
- Result: 在CIFAR-100上平均提升4.54%，ImageNet上提升0.32%，并在小样本学习场景中取得最优结果。
- Conclusion: LSSKD高效轻量，适合低计算设备部署，尤其适用于多模态感知和弱监督学习场景。


### [64] [D2R: dual regularization loss with collaborative adversarial generation for model robustness](https://arxiv.org/abs/2506.07056)
*Zhenyu Liu,Huizhi Liang,Rajiv Ranjan,Zhanxing Zhu,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: 提出了一种双正则化损失（D2R Loss）和协作对抗生成（CAG）策略，用于增强深度神经网络对抗攻击的鲁棒性。

- Motivation: 现有方法在目标模型损失函数引导不足和对抗生成非协作性方面存在局限。
- Method: 采用D2R Loss（包含对抗分布和干净分布优化）和CAG（基于梯度的协作对抗生成）。
- Result: 在CIFAR-10、CIFAR-100、Tiny ImageNet等数据集上验证，D2R Loss与CAG显著提升模型鲁棒性。
- Conclusion: D2R Loss和CAG策略有效解决了现有方法的不足，显著增强了模型的对抗鲁棒性。


### [65] [FLAIR-HUB: Large-scale Multimodal Dataset for Land Cover and Crop Mapping](https://arxiv.org/abs/2506.07080)
*Anatol Garioud,Sébastien Giordano,Nicolas David,Nicolas Gonthier*

Main category: cs.CV

TL;DR: FLAIR-HUB是法国IGN推出的多传感器土地覆盖数据集，结合六种数据模态，用于土地覆盖和作物分类研究，支持监督和多模态预训练。

- Motivation: 解决高分辨率地球观测数据的处理和标注挑战，推动全球土地覆盖和作物类型监测的准确性。
- Method: 使用六种对齐的数据模态，评估多模态融合和深度学习模型（如CNN和Transformer），并探索多任务学习。
- Result: 最佳土地覆盖分类性能为78.2%准确率和65.8% mIoU，几乎使用了所有模态。
- Conclusion: FLAIR-HUB为土地覆盖和作物分类提供了强大的数据支持，展示了多模态融合的潜力。


### [66] [UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning](https://arxiv.org/abs/2506.07087)
*Weiqi Yan,Lvhai Chen,Huaijia Kou,Shengchuan Zhang,Yan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于动态伪标签学习的无监督伪装目标检测方法（UCOD-DPL），通过自适应伪标签模块、双分支对抗解码器和Look-Twice机制，显著提升了性能。

- Motivation: 现有无监督伪装目标检测方法因伪标签噪声和简单解码器导致性能低下，本文旨在解决这些问题。
- Method: 提出UCOD-DPL方法，包含自适应伪标签模块（APM）、双分支对抗解码器（DBA）和Look-Twice机制，以动态优化伪标签并提升特征学习能力。
- Result: 实验表明，该方法性能优异，甚至超过部分全监督方法。
- Conclusion: UCOD-DPL通过动态伪标签学习和多机制协同，显著提升了无监督伪装目标检测的性能。


### [67] [SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model](https://arxiv.org/abs/2506.07091)
*Yangkai Lin,Jiabao Lei,Kui Jia*

Main category: cs.CV

TL;DR: SceneLCM是一个端到端框架，结合LLM进行布局设计和LCM进行场景优化，解决了现有室内场景生成方法的局限性。

- Motivation: 现有方法在室内场景合成中存在编辑约束、物理不一致、人力需求高、单房间限制和材质质量差等问题。
- Method: SceneLCM将场景生成分解为四个模块：布局生成（LLM指导）、家具生成（LCM优化）、环境优化（多分辨率纹理）和物理编辑（物理模拟）。
- Result: 实验验证SceneLCM优于现有技术，具有广泛的应用潜力。
- Conclusion: SceneLCM通过模块化设计和优化方法，显著提升了室内场景生成的效率和质量。


### [68] [EdgeSpotter: Multi-Scale Dense Text Spotting for Industrial Panel Monitoring](https://arxiv.org/abs/2506.07112)
*Changhong Fu,Hua Lin,Haobo Zuo,Liangliang Yao,Liguo Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EdgeSpotter的多尺度密集文本检测方法，用于工业面板监控，解决了跨尺度定位和密集文本区域模糊边界的问题。

- Motivation: 工业面板文本检测存在跨尺度定位和密集文本区域模糊边界的挑战，现有方法多关注单一文本形状，缺乏多尺度特征探索。
- Method: 开发了基于Transformer的高效混合器，学习多级特征间的依赖关系；设计了新的特征采样方法，使用Catmull-Rom样条编码文本形状、位置和语义信息。
- Result: 在自建的工业面板监控数据集（IPM）上验证了方法的优越性能，并在边缘AI视觉系统中展示了实用性。
- Conclusion: EdgeSpotter在工业面板监控任务中表现出色，解决了多尺度和密集文本区域的检测问题，具有实际应用价值。


### [69] [Image segmentation and classification of E-waste for waste segregation](https://arxiv.org/abs/2506.07122)
*Prakriti Tripathi,Theertha Biju,Maniram Thota,Rakesh Lingam*

Main category: cs.CV

TL;DR: 使用YOLOv11和Mask-RCNN模型对电子废物进行分类，分别达到70和41 mAP，未来将集成到分拣机器人中。

- Motivation: 解决电子废物分类问题，以支持分拣机器人实现自动化废物分离。
- Method: 创建自定义数据集，训练YOLOv11和Mask-RCNN模型。
- Result: YOLOv11达到70 mAP，Mask-RCNN达到41 mAP。
- Conclusion: 模型将集成到分拣机器人中，进一步优化电子废物分类。


### [70] [Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion](https://arxiv.org/abs/2506.07136)
*Huaize Liu,Wenzhang Sun,Qiyuan Zhang,Donglin Di,Biao Gong,Hao Li,Chen Wei,Changqing Zou*

Main category: cs.CV

TL;DR: Hi-VAE是一种高效视频自编码框架，通过分层编码视频动态的粗到细运动表示，显著减少时空冗余，实现高压缩比和高保真重建。

- Motivation: 现有视频自编码方法未能高效建模动态中的时空冗余，导致压缩效果不佳和下游任务训练成本过高。
- Method: Hi-VAE将视频动态分解为全局运动和细节运动两个潜在空间，使用自监督运动编码器压缩视频潜在表示，并通过条件扩散解码器重建视频。
- Result: Hi-VAE实现了1428倍的高压缩比，比基线方法（如Cosmos-VAE的48倍）高出近30倍，同时保持高质量重建和下游任务性能。
- Conclusion: Hi-VAE在高效压缩和高质量重建方面表现出色，具有可解释性和可扩展性，为视频潜在表示和生成提供了新视角。


### [71] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: 提出了一种空间令牌融合（STF）和多块令牌融合（MBTF）方法，以减少视觉令牌序列长度并提升推理效率，同时保持多模态推理能力。

- Motivation: 解决大型多模态模型（LMMs）因高计算成本和长视觉令牌序列的二次复杂度带来的挑战。
- Method: 通过STF融合空间相邻令牌以减少序列长度，并通过MBTF补充多粒度特征。
- Result: 在8个流行视觉语言基准测试中，仅使用基线25%的视觉令牌即可达到可比或更优性能。
- Conclusion: STF和MBTF模块有效平衡了令牌减少和信息保留，提升了推理效率且不牺牲性能。


### [72] [GoTrack: Generic 6DoF Object Pose Refinement and Tracking](https://arxiv.org/abs/2506.07155)
*Van Nguyen Nguyen,Christian Forster,Sindi Shkodrani,Vincent Lepetit,Bugra Tekin,Cem Keskin,Tomas Hodan*

Main category: cs.CV

TL;DR: GoTrack是一种基于CAD的高效6DoF物体姿态优化与跟踪方法，无需特定物体训练，结合模型到帧和帧到帧注册，通过光流估计实现。

- Motivation: 现有跟踪方法仅依赖模型到帧注册，计算成本高且稳定性不足，GoTrack通过结合帧到帧注册优化性能。
- Method: 使用标准神经网络块（基于DINOv2的Transformer）实现模型到帧注册，轻量级现成光流模型处理帧到帧注册。
- Result: GoTrack与现有粗姿态估计方法结合，在标准6DoF姿态估计与跟踪基准上达到RGB-only的SOTA结果。
- Conclusion: GoTrack提供了一种高效、稳定的6DoF物体姿态跟踪解决方案，代码和模型已开源。


### [73] [Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs](https://arxiv.org/abs/2506.07164)
*Qiong Chang,Xinyuan Chen,Xiang Li,Weimin Wang,Jun Miyazaki*

Main category: cs.CV

TL;DR: 论文提出两种方法加速低端嵌入式GPU上的Oriented FAST特征检测，显著提升SLAM系统实时性。

- Motivation: 现有ORB-based SLAM系统在移动平台上实时性不足，主要因Oriented FAST计算耗时。
- Method: 采用二进制编码策略快速确定候选点，以及可分离的Harris检测策略结合GPU硬件指令优化。
- Result: 在Jetson TX2上实验显示，速度比OpenCV GPU版本平均提升7.3倍。
- Conclusion: 方法有效提升实时性，适用于移动和资源受限环境。


### [74] [Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models](https://arxiv.org/abs/2506.07177)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Jaehong Yoon,Soo Ye Kim,Zhe Lin,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的帧级信号引导方法（Frame Guidance），用于可控视频生成，显著减少内存使用并提升全局一致性。

- Motivation: 现有方法依赖微调大规模视频模型，随着模型规模增长变得不切实际，因此需要一种无需训练的解决方案。
- Method: 提出基于帧级信号（如关键帧、风格参考图等）的Frame Guidance，结合潜在处理方法和优化策略，实现全局一致性视频生成。
- Result: 实验表明，Frame Guidance能在多种任务（如关键帧引导、风格化等）中生成高质量可控视频。
- Conclusion: Frame Guidance是一种无需训练、兼容性强的高效可控视频生成方法。


### [75] [Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks](https://arxiv.org/abs/2506.07188)
*Ni Ding,Lei He,Shengbo Eben Li,Keqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种分层解耦的后训练框架，通过重构中间特征图引入监督信号，提升模型透明度和训练灵活性。

- Motivation: 解决端到端自动驾驶模型的黑盒性和安全性问题，提高模型可解释性和训练灵活性。
- Method: 利用真实标签重构中间特征图，引入代理监督信号，独立训练组件，避免传统端到端反向传播的复杂性。
- Result: 在多个标准图像分类基准上表现出优异的泛化性能和计算效率。
- Conclusion: 该方法为神经网络训练提供了新的高效范式，具有显著的应用潜力。


### [76] [SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning](https://arxiv.org/abs/2506.07196)
*Mengya Xu,Zhongzhen Huang,Dillan Imans,Yiru Ye,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: 论文提出了SAP-Bench数据集和MLLM-SAP框架，用于评估多模态大语言模型在手术动作规划任务中的表现，发现现有模型在预测下一动作方面存在不足。

- Motivation: 当前基准测试无法充分评估手术决策中的复杂能力，需开发高质量数据集以推动MLLM研究。
- Method: 构建SAP-Bench数据集，包含临床验证的手术动作标注，并提出MLLM-SAP框架，结合领域知识生成下一动作建议。
- Result: 评估了七种先进MLLM模型，发现其在下一动作预测性能上存在显著差距。
- Conclusion: SAP-Bench和MLLM-SAP为手术动作规划提供了有效工具，揭示了当前模型的局限性。


### [77] [TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation](https://arxiv.org/abs/2506.07205)
*Min-Jung Kim,Dongjin Kim,Seokju Yun,Jaegul Choo*

Main category: cs.CV

TL;DR: TV-LiVE是一个无需训练、基于文本引导的视频编辑框架，通过利用关键层的活力信息实现复杂视频编辑任务，如添加新对象和非刚性变换。

- Motivation: 现有视频编辑方法主要关注风格转换等简单任务，复杂任务如添加新对象和非刚性变换研究较少，TV-LiVE旨在填补这一空白。
- Method: 通过识别视频生成模型中与RoPE相关的关键层，选择性注入源模型的特征到目标模型，并提取显著层生成编辑区域的掩码。
- Result: 实验表明，TV-LiVE在添加新对象和非刚性视频编辑任务上优于现有方法。
- Conclusion: TV-LiVE为复杂视频编辑任务提供了一种高效且无需训练的解决方案。


### [78] [Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation](https://arxiv.org/abs/2506.07214)
*Zhiyuan Zhong,Zhen Sun,Yepang Liu,Xinlei He,Guanhong Tao*

Main category: cs.CV

TL;DR: 论文提出了一种新型的跨模态后门攻击方法BadSem，利用语义不匹配作为隐式触发器，攻击视觉语言模型（VLMs），并展示了其高攻击成功率和防御困难。

- Motivation: 现有后门攻击主要依赖单模态触发器，忽视了VLMs的跨模态融合特性，因此探索跨模态语义不匹配作为攻击手段。
- Method: 提出BadSem攻击，通过故意在训练时错配图像-文本对注入后门，并构建SIMBad数据集支持攻击。
- Result: 在四种VLMs上实验，BadSem平均攻击成功率超过98%，且能泛化到分布外数据。防御策略均未能有效缓解攻击。
- Conclusion: 研究揭示了VLMs的语义漏洞，呼吁更安全的部署方案。


### [79] [AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?](https://arxiv.org/abs/2506.07216)
*Nada Aboudeshish,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出了一种全面的数据增强框架，通过几何变换、随机裁剪、旋转、缩放和强度变换等方法，显著提升了骨架数据集的多样性和模型性能。

- Motivation: 解决骨架数据集中多样性不足的问题，提升模型在真实场景中的泛化能力和鲁棒性。
- Method: 集成几何变换、随机裁剪、旋转、缩放和强度变换，生成四倍数据量的增强样本。
- Result: 在多个基准数据集和模型上验证了框架的有效性，显著提升了模型性能。
- Conclusion: 该框架不仅实现了最先进的性能，还为手势识别和动作识别应用提供了可扩展的解决方案。


### [80] [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227)
*Tianyi Bai,Yuxuan Fan,Jiantao Qiu,Fupeng Sun,Jiayi Song,Junlin Han,Zichen Liu,Conghui He,Wentao Zhang,Binhang Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种针对多模态大语言模型（MLLMs）在细粒度视觉差异任务中的改进方法，通过生成微编辑数据集（MED）和引入特征级一致性损失，显著提升了模型性能。

- Motivation: MLLMs在视觉语言任务中表现优异，但在细粒度视觉差异上存在幻觉或语义遗漏问题，主要源于训练数据和目标函数的局限性。
- Method: 提出可控数据生成流程构建MED数据集（50K图像-文本对），并设计监督微调框架，引入特征级一致性损失以稳定视觉嵌入。
- Result: 在Micro Edit Detection基准测试中，方法显著提升了差异检测准确率并减少幻觉，同时在标准视觉语言任务（如图像描述和视觉问答）中表现一致提升。
- Conclusion: 通过针对性数据和目标对齐，有效增强了MLLMs的细粒度视觉推理能力。


### [81] [Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification](https://arxiv.org/abs/2506.07235)
*Tianyi Bai,Zengjie Hu,Fupeng Sun,Jiantao Qiu,Yizhen Jiang,Guangxin He,Bohan Zeng,Conghui He,Binhang Yuan,Wentao Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种动态推理框架，通过视觉标记缩放和验证器引导的迭代推理，提升了多模态大语言模型（MLLMs）的视觉推理能力。

- Motivation: 现有MLLMs采用静态推理范式，限制了其迭代优化和上下文适应的能力，而人类感知是动态且反馈驱动的。
- Method: 将问题建模为马尔可夫决策过程，结合提议视觉动作的推理器和通过多步直接偏好优化（DPO）训练的验证器。
- Result: 提出的方法在多个视觉推理基准测试中显著优于现有方法，提高了准确性和可解释性。
- Conclusion: 动态推理机制为下一代MLLMs实现细粒度、上下文感知的视觉推理提供了潜力。


### [82] [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](https://arxiv.org/abs/2506.07280)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: 视频扩散模型（VDMs）不仅是强大的视频生成工具，还能通过训练动态学习结构化表示和视觉世界的隐式理解。通过少量样本微调框架，VDMs可适应多样任务，展现广泛泛化能力。

- Motivation: 探索VDMs在视频生成之外的潜力，验证其内部是否学习到结构化表示和视觉理解能力。
- Method: 提出少量样本微调框架，将任务转化为视觉过渡，训练LoRA权重而不改变冻结VDM的生成接口。
- Result: 模型在从低层视觉（如分割、姿态估计）到高层推理（如ARC-AGI）的多样任务中表现出强泛化能力。
- Conclusion: VDMs不仅是生成引擎，还是适应性强的视觉学习器，有望成为未来视觉基础模型的核心。


### [83] [Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI](https://arxiv.org/abs/2506.07286)
*Aditya Chakravarty*

Main category: cs.CV

TL;DR: 提出了一种多步优化策略，显著提升了扩散模型在逆问题中的图像质量和泛化能力。

- Motivation: 现有方法（如MPGD）每去噪步骤仅应用单次梯度更新，限制了恢复的保真度和鲁棒性。
- Method: 在每去噪步骤中引入多步优化策略，增加梯度更新次数。
- Result: 实验表明，多步优化显著提升了LPIPS和PSNR指标，并在嵌入式设备上验证了泛化能力。
- Conclusion: MPGD可作为轻量级、即插即用的恢复模块，适用于无人机和移动机器人等实时视觉感知任务。


### [84] [FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos](https://arxiv.org/abs/2506.07304)
*Kavitha Viswanathan,Vrinda Goel,Shlesh Gholap,Devayan Ghosh,Madhav Gupta,Dhruvi Ganatra,Sanket Potdar,Amit Sethi*

Main category: cs.CV

TL;DR: FANVID是一个新的视频基准数据集，包含低分辨率（LR）视频片段，用于提升时间识别模型，支持人脸匹配和车牌识别任务。

- Motivation: 解决现实监控中低分辨率视频难以识别人脸和车牌的问题，推动时间建模技术的发展。
- Method: 数据集包含1,463个LR视频片段，提供手动验证的边界框和标签，定义人脸匹配和车牌识别任务，并引入评估指标。
- Result: 基线方法在两项任务中分别获得0.58和0.42的分数，显示任务的可行性和挑战性。
- Conclusion: FANVID旨在促进低分辨率识别的时间建模创新，适用于监控、法医和自动驾驶等领域。


### [85] [AllTracker: Efficient Dense Point Tracking at High Resolution](https://arxiv.org/abs/2506.07310)
*Adam W. Harley,Yang You,Xinglong Sun,Yang Zheng,Nikhil Raghuraman,Yunqi Gu,Sheldon Liang,Wen-Hsuan Chu,Achal Dave,Pavel Tokmakov,Suya You,Rares Ambrus,Katerina Fragkiadaki,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: AllTracker是一种通过估计查询帧与视频中其他帧之间的流场来估计长距离点轨迹的模型，提供高分辨率和密集的对应关系。

- Motivation: 现有方法在点跟踪和光流估计上各有局限，无法同时实现高分辨率、密集对应和长距离跟踪。
- Method: 结合光流和点跟踪技术，采用低分辨率网格迭代推断，通过2D卷积层和像素对齐注意力层传播信息。
- Result: 模型参数高效（1600万参数），在高分辨率（768x1024像素）下实现最先进的点跟踪精度。
- Conclusion: AllTracker在架构和训练方法上表现出色，代码和模型权重已开源。


### [86] ["CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/abs/2506.07327)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CV

TL;DR: 本文提出了一种诊断测试（class sensitivity）来评估显著性方法的可靠性，发现许多方法对类别标签不敏感，并提出了新的对比性解释方法CASE。

- Motivation: 显著性方法在可视化模型预测时被广泛使用，但其视觉合理性可能掩盖了关键局限性，尤其是对类别标签的敏感性不足。
- Method: 提出了一种诊断测试（class sensitivity），并通过实验验证了现有显著性方法的局限性；随后提出了对比性解释方法CASE。
- Result: 实验表明，许多显著性方法对类别标签不敏感，而CASE能够生成更忠实且类别特定的解释。
- Conclusion: CASE在诊断测试和保真度测试中表现优于现有方法，解决了显著性方法的结构性缺陷。


### [87] [Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation](https://arxiv.org/abs/2506.07338)
*Yijie Deng,Shuaihang Yuan,Geeta Chandra Raju Bethala,Anthony Tzes,Yu-Shen Liu,Yi Fang*

Main category: cs.CV

TL;DR: 提出了一种基于分层评分范式的实例图像目标导航框架，通过跨层次语义评分和局部几何评分优化视点选择，减少冗余，提升性能。

- Motivation: 现有方法依赖随机采样视点或轨迹，导致冗余和效率低下，缺乏优化的视点选择机制。
- Method: 结合CLIP驱动的语义相关性和局部几何评分，估计最佳匹配视点。
- Result: 在模拟和实际场景中均达到最先进性能。
- Conclusion: 分层评分范式显著提升了导航效率和准确性。


### [88] [CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms](https://arxiv.org/abs/2506.07357)
*Satvik Praveen,Yoonsung Jung*

Main category: cs.CV

TL;DR: 论文提出了一种结合Thin-Plate Splines（TPS）和STN的CBAM-STN-TPS-YOLO模型，用于解决农业目标检测中的遮挡和非刚性变形问题，并通过CBAM模块提升性能。

- Motivation: 现有模型（如YOLO）在农业目标检测中因遮挡、不规则结构和背景噪声导致精度下降，而STN的仿射变换无法处理非刚性变形（如弯曲叶片和重叠）。
- Method: 提出CBAM-STN-TPS-YOLO模型，将TPS引入STN以实现灵活的非刚性空间变换，并结合CBAM模块抑制背景噪声，突出相关特征。
- Result: 在PGP数据集上，模型在精度、召回率和mAP上优于STN-YOLO，假阳性减少12%，同时研究了TPS正则化参数的影响。
- Conclusion: 该轻量级模型提升了空间感知能力，适合实时边缘部署，为智能农业提供高效精准的监测方案。


### [89] [Multiple Object Stitching for Unsupervised Representation Learning](https://arxiv.org/abs/2506.07364)
*Chengchao Shen,Dawei Liu,Jianxin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MOS的方法，通过拼接单目标图像生成多目标图像，提升无监督表示学习在多目标图像上的性能。

- Motivation: 现有对比学习方法在多目标图像上表现不佳，缺乏对象对应关系。
- Method: 通过拼接单目标图像生成多目标图像，利用预定义的对象对应关系优化表示学习。
- Result: 在ImageNet、CIFAR和COCO数据集上，MOS方法在单目标和多目标图像上均取得领先的无监督表示性能。
- Conclusion: MOS方法简单有效，能显著提升多目标图像的无监督表示学习能力。


### [90] [C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.07368)
*Jiaying He,Yitong Lin,Jiahe Chen,Honghui Xu,Jianwei Zheng*

Main category: cs.CV

TL;DR: 针对医学图像分割中标注样本不足的问题，提出了一种新的半监督分割模型C3S3，通过互补竞争和对比选择显著提升边界细节的准确性。

- Motivation: 医学领域标注样本不足，现有方法难以精确捕捉边界细节，导致诊断不准确。
- Method: C3S3结合了结果驱动的对比学习模块和动态互补竞争模块，优化边界定位并生成高质量伪标签。
- Result: 在两个公开数据集上验证，性能优于现有方法，95HD和ASD指标提升至少6%。
- Conclusion: C3S3在医学图像分割中表现出色，显著提升了边界细节的准确性。


### [91] [Generative Models at the Frontier of Compression: A Survey on Generative Face Video Coding](https://arxiv.org/abs/2506.07369)
*Bolin Chen,Shanzhi Yin,Goluck Konuko,Giuseppe Valenzise,Zihan Zhang,Shiqi Wang,Yan Ye*

Main category: cs.CV

TL;DR: GFVC利用深度生成模型实现高保真面部视频压缩，超越传统VVC标准，并推动标准化与工业应用。

- Motivation: 提升面部视频压缩效率，实现超低码率下的高保真通信。
- Method: 综述现有GFVC技术，构建大规模数据库，提出标准化语法和低复杂度系统。
- Result: GFVC在压缩效率和重建质量上优于VVC，并具备标准化潜力。
- Conclusion: GFVC在工业应用前景广阔，但仍需解决当前挑战。


### [92] [ARGUS: Hallucination and Omission Evaluation in Video-LLMs](https://arxiv.org/abs/2506.07371)
*Ruchit Rawal,Reza Shirkavand,Heng Huang,Gowthami Somepalli,Tom Goldstein*

Main category: cs.CV

TL;DR: ARGUS是一个新的VideoLLM基准测试，专注于自由文本生成任务（如视频字幕），通过量化幻觉率和遗漏细节率来评估模型性能。

- Motivation: 现有的Video-LLM基准测试主要依赖多选题，无法有效衡量模型在自由文本生成任务中的幻觉问题，因此需要更全面的评估方法。
- Method: 提出ARGUS基准测试，通过比较VideoLLM生成的视频字幕与人类标注的真实字幕，量化幻觉率（错误内容或时间关系）和遗漏细节率。
- Result: ARGUS提供了双重指标，全面评估视频字幕性能，揭示了模型在自由文本生成中的主要问题。
- Conclusion: ARGUS为VideoLLM的自由文本生成任务提供了更全面的评估工具，有助于改进模型性能。


### [93] [DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models](https://arxiv.org/abs/2506.07375)
*Xunjie He,Christina Dao Wen Lee,Meiling Wang,Chengran Yuan,Zefan Huang,Yufeng Yue,Marcelo H. Ang Jr*

Main category: cs.CV

TL;DR: 提出了一种多类别协作检测与跟踪框架，通过全局空间注意力融合模块和视觉语义的REID模块，显著提升了检测与跟踪精度。

- Motivation: 现有协作感知研究主要针对车辆类别，缺乏对多类别对象的有效解决方案，限制了实际应用。
- Method: 设计了全局空间注意力融合（GSAF）模块、基于视觉语义的REID模块和速度自适应的轨迹管理（VATM）模块。
- Result: 在V2X-Real和OPV2V数据集上，方法显著优于现有技术。
- Conclusion: 提出的框架有效解决了多类别协作检测与跟踪问题，提升了实际场景的适用性。


### [94] [Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation](https://arxiv.org/abs/2506.07376)
*Jintao Tong,Ran Ma,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 论文提出了一种跨域少样本分割方法（CD-FSS），通过适配器技术解决域差距和少样本微调问题，并提出Domain Feature Navigator（DFN）和SAM-SVN方法，显著提升了性能。

- Motivation: 解决跨域少样本分割中的域差距和少样本微调问题，探索适配器在域信息解耦中的作用。
- Method: 提出Domain Feature Navigator（DFN）作为结构化解耦器，结合SAM-SVN方法防止过拟合，并在目标域上微调DFN。
- Result: 在1-shot和5-shot场景下，分别超越现有最佳方法2.69%和4.68%的MIoU。
- Conclusion: DFN和SAM-SVN方法有效解决了跨域少样本分割的挑战，显著提升了性能。


### [95] [MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems](https://arxiv.org/abs/2506.07399)
*Peiru Yang,Jinhua Yin,Haoran Zheng,Xueying Bai,Huili Wang,Yufei Sun,Xintian Li,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CV

TL;DR: 本文提出MrM，首个针对多模态RAG系统的黑盒成员推理攻击框架，通过多目标数据扰动和反事实攻击提升攻击效果。

- Motivation: 多模态RAG系统可能泄露敏感信息，现有攻击方法主要针对文本模态，视觉模态研究不足。
- Method: 提出多目标数据扰动框架，结合反事实攻击和掩码选择策略，通过统计推理提取成员信息。
- Result: 在两个视觉数据集和八个主流视觉语言模型上验证了MrM的有效性和鲁棒性。
- Conclusion: MrM填补了视觉模态成员推理攻击的研究空白，展示了强大的攻击性能和适应性。


### [96] [Compressed Feature Quality Assessment: Dataset and Baselines](https://arxiv.org/abs/2506.07412)
*Changsheng Gao,Wei Zhou,Guosheng Lin,Weisi Lin*

Main category: cs.CV

TL;DR: 论文提出了压缩特征质量评估（CFQA）问题，并创建了一个包含原始和压缩特征的基准数据集，评估了三种常用指标的性能，发现需要更精细的指标来衡量语义失真。

- Motivation: 大规模模型在资源受限环境中的部署需要高效传输中间特征表示，但特征压缩会导致语义退化，传统指标难以量化这种退化。
- Method: 提出了CFQA问题，创建了包含300个原始特征和12000个压缩特征的基准数据集，评估了MSE、余弦相似度和中心核对齐三种指标的性能。
- Result: 数据集具有代表性，但现有指标无法完全捕捉压缩特征的语义失真，需要更精细的指标。
- Conclusion: 论文为CFQA研究提供了基础资源和数据集，推动了该领域的发展。


### [97] [DPFormer: Dynamic Prompt Transformer for Continual Learning](https://arxiv.org/abs/2506.07414)
*Sheng-Kai Huang,Jiun-Feng Chang,Chun-Rong Huang*

Main category: cs.CV

TL;DR: 论文提出了一种动态提示变换器（DPFormer）及其提示方案，以解决持续学习中的稳定性-可塑性困境和任务间混淆问题，并在多个数据集上取得最佳性能。

- Motivation: 持续学习中存在稳定性-可塑性困境和任务间混淆问题，需要一种方法既能记忆旧知识又能学习新知识。
- Method: 提出DPFormer及其提示方案，结合二元交叉熵损失、知识蒸馏损失和辅助损失，以端到端方式训练模型。
- Result: 在CIFAR-100、ImageNet100和ImageNet1K数据集上，该方法在不同类增量设置下表现最佳。
- Conclusion: DPFormer及其提示方案有效解决了持续学习中的关键问题，并显著提升了性能。


### [98] [FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement](https://arxiv.org/abs/2506.07431)
*Jie He,Minglang Chen,Minying Lu,Bocheng Liang,Junming Wei,Guiyan Peng,Jiaxi Chen,Ying Tan*

Main category: cs.CV

TL;DR: 提出了一种基于特征感知和Mamba增强的胎儿股骨和颅骨超声图像分割模型，解决了现有模型在噪声高、相似性强的超声图像中适应性差的问题。

- Motivation: 超声图像分割对精确生物测量和评估至关重要，但现有模型难以适应高噪声和高相似性的超声对象，尤其是小对象分割时锯齿效应明显。
- Method: 设计了纵向和横向独立视角扫描卷积块、特征感知模块，结合Mamba优化的残差结构，增强局部细节捕捉和上下文信息融合，抑制原始噪声干扰。
- Result: FAMSeg网络在实验中实现了最快的损失减少和最佳的分割性能，适应不同大小和方向的图像。
- Conclusion: 该模型通过特征感知和Mamba增强，显著提升了超声图像分割的准确性和鲁棒性。


### [99] [Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition](https://arxiv.org/abs/2506.07436)
*Nishi Chaudhary,S M Jamil Uddin,Sathvik Sharath Chandra,Anto Ovid,Alex Albert*

Main category: cs.CV

TL;DR: 该研究比较了五种多模态大语言模型（LLMs）在建筑工地视觉危险识别任务中的表现，发现提示策略（尤其是链式思维提示）显著影响模型性能，GPT-4.5和GPT-o3表现最佳。

- Motivation: 探索多模态LLMs在建筑安全领域的应用潜力，填补现有研究对不同LLMs在安全关键任务中表现的空白。
- Method: 对五种LLMs（Claude-3 Opus、GPT-4.5、GPT-4o、GPT-o3和Gemini 2.0 Pro）进行对比评估，采用零样本、少样本和链式思维（CoT）三种提示策略，通过精确率、召回率和F1分数进行定量分析。
- Result: CoT提示策略显著提升模型性能，GPT-4.5和GPT-o3在多数情况下表现最优，提示设计对模型准确性和一致性至关重要。
- Conclusion: 研究为多模态LLMs在建筑安全领域的应用提供了实用见解，强调了提示工程的重要性，有助于开发更可靠的AI辅助安全系统。


### [100] [PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation](https://arxiv.org/abs/2506.07456)
*Wei Yao,Yunlian Sun,Chang Liu,Hongwen Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: 论文提出了一种物理映射方法，用于提升多人运动生成中的物理真实性和交互质量，通过物理仿真和损失函数优化，显著提高了运动数据的物理保真度。

- Motivation: 现有运动捕捉技术和生成模型常忽略物理约束，导致运动生成中出现穿模、滑动等问题，尤其在多人交互场景中更为严重。
- Method: 提出物理映射方法，结合物理仿真环境进行运动模仿，并引入运动一致性（MC）和基于标记的交互（MI）损失函数。
- Result: 实验表明，该方法在运动生成质量上表现优异，物理保真度提升了3%-89%。
- Conclusion: 物理映射方法有效解决了运动生成中的物理约束问题，提升了多人交互场景的运动真实性和质量。


### [101] [GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning](https://arxiv.org/abs/2506.07460)
*Taeryung Lee,Hyeongjin Nam,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 论文提出GLOS框架，通过时间对齐的gloss级条件生成手语，解决了现有方法在词序和语义准确性上的不足。

- Motivation: 现有手语生成方法因句子级条件导致词序错误和语义准确性低，无法捕捉手语的时间结构和词级语义。
- Method: 采用gloss级条件（时间对齐的gloss嵌入序列）和条件融合模块TAC，实现细粒度控制和词序保持。
- Result: 在CSL-Daily和Phoenix-2014T数据集上表现优于现有方法，生成的手语词序正确且语义准确。
- Conclusion: GLOS框架通过gloss级条件和TAC模块，显著提升了手语生成的词序和语义准确性。


### [102] [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464)
*Jinyoung Park,Jeehye Na,Jinyoung Kim,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 论文探讨了GRPO在视频LLMs中的应用问题，并提出Reg-GRPO和数据增强策略以提升性能。

- Motivation: 研究GRPO在视频LLMs中的应用，并解决其学习效率低下的问题。
- Method: 提出Reg-GRPO（回归任务形式）和难度感知数据增强策略。
- Result: DeepVideo-R1在多个视频推理基准测试中表现显著提升。
- Conclusion: Reg-GRPO和数据增强策略有效解决了视频LLMs的学习问题，提升了推理能力。


### [103] [Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.07471)
*CH Cho,WJ Moon,W Jun,MS Jung,JP Heo*

Main category: cs.CV

TL;DR: 论文提出了一种名为ARL的框架，通过多正对比学习和双重三元组边际损失，解决文本-视频对中的模糊性问题，并在PRVR任务中表现优异。

- Motivation: 传统PRVR训练假设文本查询与视频是一对一关系，忽略了文本与视频内容之间的模糊性。本文旨在将这种模糊性纳入模型学习过程。
- Method: 提出ARL框架，通过不确定性和相似性检测模糊对，采用多正对比学习和双重三元组边际损失进行分层学习，并探索视频内细粒度关系。
- Result: ARL框架在PRVR任务中表现出色，有效解决了文本-视频对的模糊性问题。
- Conclusion: ARL通过检测模糊对和分层学习，显著提升了PRVR任务的性能，同时减少了单模型检测模糊对时的误差传播。


### [104] [CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization](https://arxiv.org/abs/2506.07484)
*Dasol Hong,Wooju Lee,Hyun Myung*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoCoA-Mix的混合模型，结合了混淆感知损失（CoA-loss）和置信感知权重（CoA-weights），以提升视觉语言模型在任务特定适应中的专业化和泛化能力。

- Motivation: 冻结编码器常导致特征不对齐，引发类别混淆，限制了模型的专业化能力。
- Method: 提出混淆感知损失（CoA-loss）优化决策边界，并使用置信感知权重（CoA-weights）构建混合模型以增强泛化。
- Result: CoCoA-Mix在专业化和泛化方面优于现有方法。
- Conclusion: CoCoA-Mix通过改进决策边界和混合模型权重，显著提升了模型性能。


### [105] [Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video](https://arxiv.org/abs/2506.07489)
*Yahao Shi,Yang Liu,Yanmin Wu,Xing Liu,Chen Zhao,Jie Luo,Bin Zhou*

Main category: cs.CV

TL;DR: DriveAnyMesh是一种基于单目视频驱动网格的方法，通过4D扩散模型生成高质量动画，适用于现代渲染引擎。

- Motivation: 解决当前4D生成技术在渲染效率和跨类别泛化上的不足，以及现有3D资产动画化的复杂性问题。
- Method: 使用基于变换器的变分自编码器捕获3D形状和运动信息，通过时空扩散模型生成网格动画。
- Result: 实验表明，DriveAnyMesh能快速生成复杂运动的高质量动画，且兼容现代渲染引擎。
- Conclusion: 该方法在游戏和电影行业具有应用潜力。


### [106] [SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491)
*Yongsen Mao,Junhao Zhong,Chuan Fang,Jia Zheng,Rui Tang,Hao Zhu,Ping Tan,Zihan Zhou*

Main category: cs.CV

TL;DR: SpatialLM是一个大型语言模型，用于处理3D点云数据并生成结构化的3D场景理解输出，如墙壁、门窗等建筑元素。

- Motivation: 提升现代LLM在空间理解方面的能力，以应用于增强现实、机器人等领域。
- Method: 采用标准的多模态LLM架构，并通过开源LLM直接微调，使用大规模合成数据集进行训练。
- Result: 在公开基准测试中，布局估计达到最优性能，3D物体检测结果具有竞争力。
- Conclusion: 展示了增强现代LLM空间理解能力的可行路径。


### [107] [Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency](https://arxiv.org/abs/2506.07497)
*Xiangyu Guo,Zhanqian Wu,Kaixin Xiong,Ziyang Xu,Lijun Zhou,Gangwei Xu,Shaoqing Xu,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: Genesis是一个统一框架，用于联合生成多视角驾驶视频和LiDAR序列，具有时空和跨模态一致性。

- Motivation: 解决多模态数据生成中的一致性问题，提升生成数据的语义保真度和实用性。
- Method: 采用两阶段架构，结合DiT视频扩散模型与3D-VAE编码，以及BEV感知的LiDAR生成器与NeRF渲染。通过共享潜在空间实现模态耦合，并引入DataCrafter模块提供语义监督。
- Result: 在nuScenes基准测试中表现优异（FVD 16.95，FID 4.24，Chamfer 0.611），并提升下游任务性能。
- Conclusion: Genesis在多模态数据生成中实现了高性能和实用性，验证了其语义保真度。


### [108] [MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts](https://arxiv.org/abs/2506.07533)
*Wei Tao,Haocheng Lu,Xiaoyang Qu,Bin Zhang,Kai Lu,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: MoQAE是一种新型混合精度量化方法，通过量化感知专家的混合来优化大型语言模型的KV缓存内存消耗。

- Motivation: 现有量化方法无法同时兼顾有效性和效率，MoQAE旨在解决这一问题。
- Method: 1. 将不同量化位宽配置视为专家，使用MoE方法选择最优配置；2. 设计轻量级路由器微调过程；3. 引入路由冻结和共享机制。
- Result: 在多个基准数据集上，MoQAE在效率和有效性上均优于现有KV缓存量化方法。
- Conclusion: MoQAE通过混合精度量化和优化路由机制，显著提升了大型语言模型的推理性能。


### [109] [Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study](https://arxiv.org/abs/2506.07539)
*Xiaomeng Zhu,Jacob Henningsson,Duruo Li,Pär Mårtensson,Lars Hanson,Mårten Björkman,Atsuto Maki*

Main category: cs.CV

TL;DR: 论文提出了一种用于制造业目标检测的合成数据生成方法，通过领域随机化技术生成多样化的数据，并在公开数据集上验证了其有效性。

- Motivation: 解决制造业目标检测中合成数据生成的关键问题，以覆盖真实数据的分布。
- Method: 提出全面的数据生成流程，包括对象特征、背景、光照、相机设置和后处理，并引入SIP15-OD数据集。
- Result: 在Yolov8模型上，合成数据训练的模型在公开数据集上表现优异，mAP@50最高达99.5%。
- Conclusion: 领域随机化技术能有效生成接近真实数据分布的合成数据，适用于制造业目标检测。


### [110] [APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs](https://arxiv.org/abs/2506.07542)
*Bowen Liu,Weiyi Zhang,Peranut Chotcomwongse,Xiaolan Chen,Ruoyu Chen,Pawin Pakaymaskul,Niracha Arjkongharn,Nattaporn Vongsa,Xuelian Cheng,Zongyuan Ge,Kun Huang,Xiaohui Li,Yiru Duan,Zhenbang Wang,BaoYe Xie,Qiang Chen,Huazhu Fu,Michael A. Mahr,Jiaqi Qu,Wangyiyang Chen,Shiye Wang,Yubo Tan,Yongjie Li,Mingguang He,Danli Shi,Paisan Ruamviboonsuk*

Main category: cs.CV

TL;DR: APTOS-2024挑战赛探索了从2D眼底图像生成3D OCT图像的可行性，旨在解决OCT设备成本高和操作复杂的问题。

- Motivation: OCT设备成本高且依赖专业操作人员，限制了其广泛应用。2D眼底摄影更易获取，但缺乏3D信息。生成式AI有望填补这一技术空白。
- Method: 挑战赛提供了基准数据集和评估方法（图像和视频级保真度指标），吸引了342个团队参与。领先方法包括数据预处理、预训练、视觉基础模型和架构改进。
- Result: 42个初步提交和9个决赛团队展示了从眼底图像生成3D OCT的可行性，为资源匮乏地区提供了潜在解决方案。
- Conclusion: APTOS-2024挑战赛首次验证了眼底到3D OCT合成的可行性，有望提升眼科医疗的可及性并加速研究和临床应用。


### [111] [Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555)
*Haoxiang Wang,Zinan Lin,Da Yu,Huishuai Zhang*

Main category: cs.CV

TL;DR: SPTI是一种通过文本中介生成高分辨率差分隐私（DP）合成图像的新方法，优于现有DP方法。

- Motivation: 现有DP图像合成方法难以生成高分辨率且忠实于原始数据的图像，SPTI旨在解决这一问题。
- Method: SPTI将图像转换为文本描述，利用改进的Private Evolution算法生成DP文本，再通过文本到图像模型重建图像。
- Result: 在LSUN Bedroom和MM CelebA HQ数据集上，SPTI的FID显著优于现有方法。
- Conclusion: SPTI提供了一种资源高效且兼容专有模型的框架，扩展了对私有视觉数据的访问。


### [112] [Cross-channel Perception Learning for H&E-to-IHC Virtual Staining](https://arxiv.org/abs/2506.07559)
*Hao Yang,JianYu Wu,Run Fang,Xuelian Zhao,Yuan Ji,Zhiyu Chen,Guibin He,Junceng Guo,Yang Liu,Xinhua Zeng*

Main category: cs.CV

TL;DR: 论文提出了一种跨通道感知学习（CCPL）策略，用于解决H&E到IHC染色中细胞核与细胞膜跨通道相关性被忽视的问题，通过双通道特征提取和特征蒸馏损失提升虚拟染色质量。

- Motivation: 现有H&E到IHC研究常忽略细胞核与细胞膜的跨通道相关性，限制了病理图像分析与诊断的准确性。
- Method: CCPL将HER2免疫组化染色分解为Hematoxylin和DAB染色通道，利用Gigapath的Tile Encoder提取双通道特征并计算跨通道相关性，同时通过特征蒸馏损失和光学密度统计分析提升模型性能。
- Result: 实验表明，CCPL在PSNR、SSIM、PCC和FID等指标上表现优异，且病理学家评估确认其能有效保留病理特征并生成高质量虚拟染色图像。
- Conclusion: CCPL为多媒体医学数据驱动的自动化病理诊断提供了有力支持。


### [113] [OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data](https://arxiv.org/abs/2506.07565)
*Jinlu Zhang,Zixi Kang,Yizhou Wang*

Main category: cs.CV

TL;DR: 论文提出了OpenDance5D数据集和OpenDanceNet框架，用于解决音乐驱动舞蹈生成中的多模态数据不足和可控性问题。

- Motivation: 现有方法在舞蹈生成的多样性和可控性上受限，主要由于缺乏细粒度多模态数据和灵活的生成条件。
- Method: 构建OpenDance5D数据集（14种舞蹈类型，101小时数据，5种模态），并提出基于掩码建模的统一框架OpenDanceNet。
- Result: OpenDanceNet实现了高保真和灵活可控的舞蹈生成。
- Conclusion: OpenDance5D和OpenDanceNet为音乐驱动舞蹈生成提供了更强大的数据和模型支持。


### [114] [Towards the Influence of Text Quantity on Writer Retrieval](https://arxiv.org/abs/2506.07566)
*Marco Peer,Robert Sablatnig,Florian Kleber*

Main category: cs.CV

TL;DR: 论文研究了基于手写相似性的作者检索任务，探讨了文本量对检索性能的影响，发现使用至少四行文本时性能接近全页水平，且深度学习方法在低文本场景中表现更优。

- Motivation: 现有方法主要关注页面级检索，本文旨在探索文本量（行级和词级）对作者检索性能的影响。
- Method: 评估了三种先进的作者检索系统（包括手工特征和深度学习方法），在CVL和IAM数据集上测试不同文本量的性能。
- Result: 实验表明，使用一行文本时性能下降20-30%，但四行文本时性能可达全页的90%以上；深度学习方法在低文本场景中优于手工特征。
- Conclusion: 文本量对作者检索性能有显著影响，深度学习方法在低文本场景中更具优势。


### [115] [LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization](https://arxiv.org/abs/2506.07570)
*Yixuan Yang,Zhen Luo,Tongsheng Ding,Junru Lu,Mingqi Gao,Jinyu Yang,Victor Sanchez,Feng Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于LLM的室内布局生成方法，通过构建大规模数据集3D-SynthPlace和优化模型OptiScene，解决了现有方法的空间不一致性和泛化能力不足的问题。

- Motivation: 现有室内布局生成方法存在空间不一致性、高计算成本或泛化能力不足的问题，作者希望通过LLM优化和大规模数据集提升生成效果。
- Method: 构建3D-SynthPlace数据集，并通过两阶段训练（监督微调和多轮直接偏好优化）优化LLM模型OptiScene。
- Result: OptiScene在布局生成质量和成功率上优于传统方法，并展示了在场景编辑和机器人导航中的潜力。
- Conclusion: 3D-SynthPlace和OptiScene为室内布局生成提供了高效且泛化能力强的解决方案。


### [116] [Learning Speaker-Invariant Visual Features for Lipreading](https://arxiv.org/abs/2506.07572)
*Yu Li,Feng Xue,Shujie Li,Jinrui Zhang,Shuang Yang,Dan Guo,Richang Hong*

Main category: cs.CV

TL;DR: SIFLip是一种通过解耦说话者特定特征来提高唇读泛化性能的框架，结合隐式和显式解耦模块，显著优于现有方法。

- Motivation: 现有唇读方法提取的视觉特征包含说话者特定属性（如形状、颜色、纹理），导致虚假相关性，影响准确性和泛化能力。
- Method: SIFLip通过隐式解耦模块（利用稳定文本嵌入作为监督信号）和显式解耦模块（通过梯度反转过滤说话者特定特征）解耦说话者特定属性。
- Result: 实验表明，SIFLip在多个公开数据集上显著提升了泛化性能，优于现有方法。
- Conclusion: SIFLip通过解耦说话者特定特征，有效提高了唇读的泛化能力和准确性。


### [117] [Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models](https://arxiv.org/abs/2506.07575)
*Ruiyang Zhang,Hu Zhang,Hao Fei,Zhedong Zheng*

Main category: cs.CV

TL;DR: 论文提出Uncertainty-o框架，用于评估和量化多模态模型（LMMs）的不确定性，并通过实验验证其有效性。

- Motivation: 多模态模型（LMMs）虽被认为比纯语言模型（LLMs）更鲁棒，但其不确定性评估仍存在挑战。
- Method: 引入Uncertainty-o框架，包括模型无关的不确定性揭示方法、多模态提示扰动实验及多模态语义不确定性量化。
- Result: 在18个基准测试和10种LMMs上验证了Uncertainty-o的可靠性，提升了幻觉检测等下游任务。
- Conclusion: Uncertainty-o能有效评估LMMs不确定性，为下游任务提供支持。


### [118] [Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding](https://arxiv.org/abs/2506.07576)
*Boyu Chen,Siran Chen,Kunchang Li,Qinglin Xu,Yu Qiao,Yali Wang*

Main category: cs.CV

TL;DR: 论文提出了一种统一的超级编码网络（SEN），通过递归关联多模态编码器，提升视频理解任务的表现。

- Motivation: 现有多模态基础模型仅通过对比学习对齐不同模态的编码器，缺乏更深层次的多模态交互，而这对理解复杂视频场景至关重要。
- Method: SEN通过递归关联块（RA）逐步融合多模态信息，将预训练编码器视为“超级神经元”，实现知识整合、分发和提示。
- Result: 实验表明，SEN显著提升了四种代表性视频任务（跟踪、识别、聊天、编辑）的表现，例如跟踪任务的Jaccard指数提高2.7%，编辑任务的文本对齐提升6.4%。
- Conclusion: SEN通过深层多模态交互，有效提升了视频理解任务的性能，为多模态基础模型提供了新的设计思路。


### [119] [Explore the vulnerability of black-box models via diffusion models](https://arxiv.org/abs/2506.07590)
*Jiacheng Shi,Yanfu Zhang,Huajie Shao,Ashley Gao*

Main category: cs.CV

TL;DR: 论文揭示了一种新型安全威胁，攻击者利用扩散模型API生成合成图像训练替代模型，实现高效模型提取和对抗攻击。

- Motivation: 扩散模型的高保真图像生成能力可能被恶意利用，导致安全隐私风险，如版权侵犯和敏感信息泄露。
- Method: 通过扩散模型API生成高质量合成图像，用于训练替代模型，以最小查询量实现模型提取和对抗攻击。
- Result: 在七个基准测试中，方法平均提升27.37%，仅用0.01倍查询预算，对抗攻击成功率达98.68%。
- Conclusion: 研究展示了扩散模型API的安全隐患，需加强防范措施以应对潜在威胁。


### [120] [SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding](https://arxiv.org/abs/2506.07600)
*Nianbo Zeng,Haowen Hou,Fei Richard Yu,Si Shi,Ying Tiffany He*

Main category: cs.CV

TL;DR: SceneRAG是一个基于大语言模型的框架，通过处理视频的ASR转录和时间元数据，将视频分割为叙事一致的场景，并结合视觉和文本模态信息构建动态知识图谱，显著提升了长视频理解的性能。

- Motivation: 当前基于检索增强生成（RAG）的视频理解方法通常将视频分割为固定长度的片段，破坏了上下文连续性且无法捕捉真实场景边界。受人类自然组织连续经验为连贯场景的能力启发，研究提出了SceneRAG。
- Method: SceneRAG利用大语言模型处理ASR转录和时间元数据，分割视频为叙事一致的场景，并通过轻量级启发式和迭代校正优化边界。结合视觉和文本模态信息构建动态知识图谱，支持多跳检索和生成。
- Result: 在包含134小时多样化内容的LongerVideos基准测试中，SceneRAG显著优于现有基线，生成任务的胜率高达72.5%。
- Conclusion: SceneRAG通过叙事一致的分割和多模态信息融合，有效解决了长视频理解的挑战，为复杂视频内容的理解提供了新思路。


### [121] [SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis](https://arxiv.org/abs/2506.07603)
*Jianhui Wei,Zikai Xiao,Danyu Sun,Luqi Gong,Zongxin Yang,Zuozhu Liu,Jian Wu*

Main category: cs.CV

TL;DR: SurgBench是一个统一的手术视频基准框架，包含预训练数据集SurgBench-P和评估基准SurgBench-E，旨在解决手术视频基础模型开发中的数据集稀缺问题。

- Motivation: 手术视频理解对自动化术中决策、技能评估和术后质量改进至关重要，但大型多样化数据集的缺乏阻碍了手术视频基础模型的发展。
- Method: 提出SurgBench框架，包含预训练数据集SurgBench-P（涵盖22种手术和11个专科的5300万帧）和评估基准SurgBench-E（覆盖72个细粒度任务的6个类别）。
- Result: 实验表明，现有视频基础模型难以泛化到多样手术视频任务，而基于SurgBench-P的预训练显著提升性能并增强跨领域泛化能力。
- Conclusion: SurgBench为手术视频分析提供了全面的数据集和评估标准，推动了手术视频基础模型的发展。


### [122] [DragNeXt: Rethinking Drag-Based Image Editing](https://arxiv.org/abs/2506.07611)
*Yuan Zhou,Junbao Zhou,Qingshan Xu,Kesen Zhao,Yuxuan Wang,Hao Fei,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: DragNeXt提出了一种新的基于拖拽的图像编辑方法，通过明确指定拖拽区域和类型解决模糊性问题，并简化了编辑流程。

- Motivation: 现有基于拖拽的图像编辑方法存在模糊性和复杂性问题，难以满足用户需求。
- Method: 将拖拽编辑重新定义为变形、旋转和平移，并提出Latent Region Optimization（LRO）和Progressive Backward Self-Intervention（PBSI）框架。
- Result: 在NextBench上的实验表明，DragNeXt显著优于现有方法。
- Conclusion: DragNeXt通过简化流程和提升质量，为基于拖拽的图像编辑提供了更优解决方案。


### [123] [Scaling Human Activity Recognition: A Comparative Evaluation of Synthetic Data Generation and Augmentation Techniques](https://arxiv.org/abs/2506.07612)
*Zikang Leng,Archith Iyer,Thomas Plötz*

Main category: cs.CV

TL;DR: 论文比较了两种虚拟IMU数据生成方法与传统数据增强技术，发现虚拟IMU数据在有限数据条件下显著提升性能。

- Motivation: 解决人类活动识别中标记数据稀缺的问题，探索跨模态生成虚拟IMU数据的有效性。
- Method: 构建大规模虚拟IMU数据集，比较视频和语言两种生成方法与传统数据增强技术。
- Result: 虚拟IMU数据显著优于单独使用真实或增强数据，尤其在数据有限时。
- Conclusion: 提供了选择数据生成策略的实用建议，并分析了各方法的优缺点。


### [124] [Event-Priori-Based Vision-Language Model for Efficient Visual Understanding](https://arxiv.org/abs/2506.07627)
*Haotong Qin,Cheng Hu,Michele Magno*

Main category: cs.CV

TL;DR: EP-VLM是一种基于事件先验的视觉语言模型，通过动态事件视觉引导稀疏化视觉输入，显著提升效率，同时保持高精度。

- Motivation: 现有视觉语言模型（VLM）计算需求高，难以在资源受限的边缘设备上部署，且视觉输入中存在大量冗余信息。
- Method: EP-VLM利用动态事件视觉的运动先验，稀疏化RGB视觉输入，并采用位置保留的标记化策略处理稀疏输入。
- Result: 实验表明，EP-VLM在Qwen2-VL系列模型上实现了50%的计算量节省，同时保持98%的原始精度。
- Conclusion: 事件视觉先验可显著提升VLM效率，为边缘设备上的可持续视觉理解铺平道路。


### [125] [HuSc3D: Human Sculpture dataset for 3D object reconstruction](https://arxiv.org/abs/2506.07628)
*Weronika Smolak-Dyżewska,Dawid Malarz,Grzegorz Wilczyński,Rafał Tobiasz,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.CV

TL;DR: HuSc3D是一个专为3D重建模型在真实采集挑战下进行严格基准测试设计的新数据集，填补了现有数据集在动态背景和色彩差异等现实场景中的不足。

- Motivation: 现有数据集集中于理想化合成或精心捕获的真实数据，未能反映新获取现实场景中的复杂性，如动态背景和色彩差异。
- Method: 提出HuSc3D数据集，包含六个高度详细的全白雕塑，具有复杂穿孔和最小纹理变化，图像数量差异显著。
- Result: 评估流行3D重建方法显示，HuSc3D能有效区分模型性能，突出方法对几何细节、色彩歧义和数据变化的敏感性。
- Conclusion: HuSc3D填补了现实场景3D重建基准测试的空白，揭示了传统数据集掩盖的模型局限性。


### [126] [HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition](https://arxiv.org/abs/2506.07637)
*Yuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao Li,Shan Yin*

Main category: cs.CV

TL;DR: HieraEdgeNet是一种多尺度边缘增强框架，通过三个创新模块显著提升了花粉等微小目标的识别精度，优于现有模型。

- Motivation: 传统花粉识别方法效率低且主观性强，现有深度学习模型对微小目标的定位精度不足。
- Method: 提出HieraEdgeNet框架，包含分层边缘模块（HEM）、协同边缘融合模块（SEF）和跨阶段部分全核模块（CSPOKM），结合多尺度边缘特征与语义信息。
- Result: 在120类花粉数据集上，mAP@.5达到0.9501，优于YOLOv12n和RT-DETR等基线模型。
- Conclusion: HieraEdgeNet通过系统整合边缘信息，为高精度、高效率的微小目标检测提供了强大解决方案。


### [127] [Synthetic Visual Genome](https://arxiv.org/abs/2506.07643)
*Jae Sung Park,Zixian Ma,Linjie Li,Chenhao Zheng,Cheng-Yu Hsieh,Ximing Lu,Khyathi Chandu,Quan Kong,Norimasa Kobori,Ali Farhadi,Yejin Choi,Ranjay Krishna*

Main category: cs.CV

TL;DR: 论文介绍了ROBIN模型，通过密集标注的关系数据训练，能够生成高质量的场景图，并在关系理解任务中超越更大规模的模型。

- Motivation: 尽管多模态语言模型在视觉理解方面取得进展，但在关系推理和生成方面仍存在挑战。
- Method: 使用合成数据集SVG训练ROBIN，并引入SG-EDIT框架通过GPT-4o优化场景图生成。
- Result: ROBIN-3B模型在关系理解任务中表现优异，超越更大规模的模型，并在指代表达理解任务中达到88.9分。
- Conclusion: 精细化的场景图数据训练对提升视觉推理任务性能至关重要。


### [128] [FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images](https://arxiv.org/abs/2506.07652)
*Hangbei Cheng,Xiaorong Dong,Xueyu Liu,Jianan Zhang,Xuetao Ma,Mingqiang Wei,Liansheng Wang,Junxin Chen,Yongfei Wu*

Main category: cs.CV

TL;DR: FMaMIL是一种基于图像级标签的弱监督病变分割框架，通过两阶段方法（Mamba编码器与频域编码模块结合）实现高效分割，无需像素级标注。

- Motivation: 解决组织病理学图像中病变分割因像素级标注成本高而难以实现的问题。
- Method: 两阶段框架：1）使用Mamba编码器和频域编码模块生成CAMs；2）通过软标签监督和自我纠正机制优化伪标签。
- Result: 在公开和私有数据集上优于现有弱监督方法，无需像素级标注。
- Conclusion: FMaMIL在数字病理学中具有潜力，验证了其高效性和实用性。


### [129] [ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views](https://arxiv.org/abs/2506.07670)
*Xiaohan Lu,Jiaye Fu,Jiaqi Zhang,Zetian Song,Chuanmin Jia,Siwei Ma*

Main category: cs.CV

TL;DR: ProSplat是一个两阶段前馈框架，用于在宽基线条件下实现高保真渲染，通过3D高斯生成器和改进模型（基于扩散模型）提升性能。

- Motivation: 解决3D高斯溅射在宽基线场景下因纹理细节不足和几何不一致导致的性能下降问题。
- Method: 第一阶段生成3D高斯基元，第二阶段通过改进模型（结合MORI和DWEA）增强渲染视图。
- Result: 在RealEstate10K和DL3DV-10K数据集上，PSNR平均提升1 dB。
- Conclusion: ProSplat在宽基线条件下显著优于现有方法，实现了更高的渲染质量。


### [130] [OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting](https://arxiv.org/abs/2506.07697)
*Jens Piekenbrinck,Christian Schmidt,Alexander Hermans,Narunas Vaskevicius,Timm Linder,Bastian Leibe*

Main category: cs.CV

TL;DR: OpenSplat3D扩展了3D高斯溅射（3DGS）的能力，实现无需手动标注的开放词汇3D实例分割，结合特征溅射、SAM实例掩码和视觉语言模型，实现基于自然语言的场景理解。

- Motivation: 3DGS在神经场景重建中表现优异，但缺乏语义理解能力。本文旨在扩展其功能，实现开放词汇的3D实例分割。
- Method: 结合特征溅射技术、SAM实例掩码和对比损失，利用视觉语言模型的语言嵌入，实现文本驱动的实例分割。
- Result: 在LERF-mask、LERF-OVS和ScanNet++验证集上展示了方法的有效性。
- Conclusion: OpenSplat3D成功实现了开放词汇的3D实例分割，为场景理解提供了灵活的工具。


### [131] [NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation](https://arxiv.org/abs/2506.07698)
*Yuxiao Yang,Peihao Li,Yuhong Zhang,Junzhe Lu,Xianglong He,Minghan Qin,Weitao Wang,Haoqian Wang*

Main category: cs.CV

TL;DR: NOVA3D是一种创新的单图像到3D生成框架，通过利用预训练视频扩散模型的3D先验和几何信息，解决了多视角一致性问题。

- Motivation: 现有方法在从预训练图像扩散模型中提取3D对象时，由于缺乏足够的3D先验，导致多视角一致性不足。
- Method: NOVA3D利用预训练视频扩散模型的3D先验，结合几何信息进行多视角视频微调，并提出Geometry-Temporal Alignment (GTA)注意力机制和去冲突几何融合算法。
- Result: 实验表明，NOVA3D在多视角一致性和纹理保真度上优于现有基线方法。
- Conclusion: NOVA3D通过引入强3D先验和几何信息，显著提升了单图像到3D生成的质量。


### [132] [Adaptive Blind Super-Resolution Network for Spatial-Specific and Spatial-Agnostic Degradations](https://arxiv.org/abs/2506.07705)
*Weilei Wen,Chunle Guo,Wenqi Ren,Hongpeng Wang,Xiuli Shao*

Main category: cs.CV

TL;DR: 提出了一种动态滤波网络，通过全局和局部分支分别处理空间无关和空间相关的图像退化类型，显著提升了图像重建性能。

- Motivation: 现有方法忽视不同退化类型的多样性，采用单一网络模型处理多种退化，导致效果不佳。
- Method: 分类退化类型为空间无关和空间相关，设计动态滤波网络，结合全局和局部分支分别处理两类退化。
- Result: 在合成和真实图像数据集上，该方法优于现有盲超分辨率算法。
- Conclusion: 动态滤波网络能有效处理多样化的图像退化问题，提升重建质量。


### [133] [Consistent Video Editing as Flow-Driven Image-to-Video Generation](https://arxiv.org/abs/2506.07713)
*Ge Wang,Songlin Fan,Hangxu Liu,Quanjian Song,Hewei Wang,Jinfeng Xu*

Main category: cs.CV

TL;DR: FlowV2V提出了一种基于光流的视频编辑方法，通过分解任务为第一帧编辑和条件图像到视频生成，解决了复杂运动建模和时序一致性问题。

- Motivation: 现有视频编辑方法难以处理复杂运动（如多对象和肖像编辑），且局限于对象替换任务。光流为复杂运动建模提供了新思路。
- Method: FlowV2V将任务分解为第一帧编辑和条件I2V生成，通过模拟伪光流序列确保编辑中的时序一致性。
- Result: 在DAVIS-EDIT数据集上，FlowV2V在DOVER和warping error指标上分别提升了13.67%和50.66%，表现优于现有方法。
- Conclusion: FlowV2V通过光流驱动的方法显著提升了视频编辑的时序一致性和样本质量，适用于复杂运动场景。


### [134] [ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks](https://arxiv.org/abs/2506.07720)
*Yufei Guo,Yuhan Zhang,Zhou Jie,Xiaode Liu,Xin Tong,Yuanpei Chen,Weihang Peng,Zhe Ma*

Main category: cs.CV

TL;DR: 提出了一种名为ReverB-SNN的方法，通过反转权重和激活的位，结合实值激活和二进制权重，提升SNN的信息容量和准确性。

- Motivation: 解决SNN中二进制激活信息不足导致的精度下降问题。
- Method: 使用实值激活和二进制权重，引入可训练因子调整权重幅度，并通过重参数化保持推理效率。
- Result: 在多种网络架构和数据集上表现优于现有方法。
- Conclusion: ReverB-SNN在保持SNN高效性的同时显著提升了精度。


### [135] [ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models](https://arxiv.org/abs/2506.07725)
*Shadi Hamdan,Chonghao Sima,Zetong Yang,Hongyang Li,Fatma Güney*

Main category: cs.CV

TL;DR: 论文提出ETA系统，通过异步计算和批量推理，使大模型能及时响应自动驾驶系统的每一帧，同时保持实时性。

- Motivation: 解决自动驾驶系统中大模型推理速度慢的问题，同时保留其高信息量的优势。
- Method: 采用异步系统ETA，将当前帧的计算任务转移到过去时间步，通过批量推理实现大模型的快速响应，并结合小模型提取实时特征。
- Result: 在Bench2Drive CARLA Leaderboard-v2基准测试中，ETA将驾驶分数提升8%，达到69.53，同时保持50毫秒的推理速度。
- Conclusion: ETA系统成功平衡了大模型的高性能和小模型的实时性，为自动驾驶系统提供了一种高效的解决方案。


### [136] [SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding](https://arxiv.org/abs/2506.07737)
*Xuemei Chen,Huamin Wang,Hangchi Shen,Shukai Duan,Shiping Wen,Tingwen Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于脉冲神经网络（SNNs）的低功耗单目3D目标检测方法SpikeSMOKE，通过跨尺度门控编码机制（CSGC）和轻量级残差块提升性能并降低能耗。

- Motivation: 随着3D目标检测在自动驾驶等领域的广泛应用，能耗问题日益突出。SNNs的低功耗特性为此提供了新的解决方案。
- Method: 提出SpikeSMOKE架构，结合CSGC机制增强特征表示，并设计轻量级残差块以减少计算量。
- Result: 在KITTI数据集上，SpikeSMOKE性能显著提升（AP|R11指标），同时能耗降低72.2%，轻量版进一步减少参数和计算量。
- Conclusion: SpikeSMOKE为低功耗3D目标检测提供了有效方案，性能与能耗平衡良好。


### [137] [AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization](https://arxiv.org/abs/2506.07738)
*Lanjiong Li,Guanhua Zhao,Lingting Zhu,Zeyu Cai,Lequan Yu,Jian Zhang,Zeyu Wang*

Main category: cs.CV

TL;DR: AssetDropper是一个从参考图像中提取标准化资产的框架，解决了设计师在开放世界场景中高效提取高质量资产的挑战。

- Motivation: 设计师更倾向于使用标准化资产库，但生成模型尚未显著提升这一领域。开放世界场景提供了丰富素材，但高效提取高质量资产仍具挑战性。
- Method: 引入AssetDropper框架，通过预训练奖励模型实现闭环反馈，提取图像中的主体并处理复杂场景（如透视变形和遮挡）。
- Result: AssetDropper在资产提取任务中达到最先进水平，通过奖励驱动优化提升了提取精度。
- Conclusion: AssetDropper为设计师提供了开放世界资产调色板，并通过合成和真实数据集支持未来研究。


### [138] [ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models](https://arxiv.org/abs/2506.07739)
*Jing Zhong,Jun Yin,Peilin Li,Pengyu Zeng,Miao Zhang,Shuai Lu,Ran Luo*

Main category: cs.CV

TL;DR: 该研究提出ArchDiffBench数据集和ArchiLense框架，利用计算机视觉和深度学习技术，实现建筑风格的自动识别与分类，解决了传统主观分析的局限性。

- Motivation: 传统建筑文化研究依赖主观专家解读和历史文献，存在区域偏见和解释范围有限的问题。
- Method: 构建ArchDiffBench数据集（1,765张高质量建筑图像），并提出基于Vision-Language Models的ArchiLense框架，结合计算机视觉和深度学习算法。
- Result: ArchiLense在建筑风格识别中表现优异，与专家标注一致性达92.4%，分类准确率为84.5%。
- Conclusion: 该方法超越了传统分析的主观性，为建筑文化比较研究提供了更客观、准确的视角。


### [139] [Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images](https://arxiv.org/abs/2506.07740)
*Yingping Liang,Ying Fu,Yutao Hu,Wenqi Shao,Jiaming Liu,Debing Zhang*

Main category: cs.CV

TL;DR: 提出了Flow-Anything框架，通过单视图图像生成大规模真实世界光流训练数据，解决了合成数据带来的领域差距问题。

- Motivation: 解决光流估计中因合成数据训练导致的真实世界应用性能限制和领域差距问题。
- Method: 使用单视图图像生成3D表示，结合对象无关体积渲染和深度感知修复模块，生成真实光流数据。
- Result: 生成的FA-Flow数据集在性能上超越了最先进的监督和无监督方法，并提升了下游视频任务性能。
- Conclusion: Flow-Anything框架为光流估计提供了更真实的数据来源，并可作为基础模型支持多种视频任务。


### [140] [Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation](https://arxiv.org/abs/2506.07750)
*Hyunsoo Kim,Donghyun Kim,Suhyun Kim*

Main category: cs.CV

TL;DR: 提出了一种名为Difference Inversion的方法，通过提取图像A和A'的差异并应用于B，生成符合A:A'::B:B'关系的B'，适用于通用扩散模型。

- Motivation: 现有方法依赖特定模型，可能引入偏见或编辑能力受限，需一种模型无关的方法。
- Method: 通过Delta插值提取差异，结合Token一致性损失和零初始化Token嵌入，构建适用于稳定扩散模型的完整提示。
- Result: 实验表明，Difference Inversion在定量和定性上均优于现有基线。
- Conclusion: 该方法能以模型无关的方式生成更可行的B'。


### [141] [Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity](https://arxiv.org/abs/2506.07773)
*Mohamed Djilani,Nassim Ali Ousalah,Nidhal Eddine Chenni*

Main category: cs.CV

TL;DR: 提出了一种结合视觉、语义和用户行为的时尚推荐系统，通过深度学习模型提取特征，模拟用户购物行为，并融合多因素生成推荐。

- Motivation: 解决传统时尚推荐系统忽视视觉细节和用户个性化趋势的问题，提供更精准的推荐。
- Method: 使用语义分割提取服装区域特征，结合CNN模型生成视觉嵌入，模拟用户行为历史，融合视觉相似性、语义一致性和流行度评分。
- Result: 在DeepFashion数据集上表现优异，ResNet-50达到64.95%的类别相似性，流行度误差最低。
- Conclusion: 该方法为个性化时尚推荐提供了可扩展框架，平衡个人风格与流行趋势。


### [142] [Language-Vision Planner and Executor for Text-to-Visual Reasoning](https://arxiv.org/abs/2506.07778)
*Yichang Xu,Gaowen Liu,Ramana Rao Kompella,Sihao Hu,Tiansheng Huang,Fatih Ilhan,Selim Furkan Tekin,Zachary Yahn,Ling Liu*

Main category: cs.CV

TL;DR: VLAgent是一个结合视觉与文本推理的AI系统，通过分步规划和执行验证提升多模态推理性能。

- Motivation: 现有视觉语言模型在泛化性能上表现不佳，VLAgent旨在通过改进规划和执行模块解决这一问题。
- Method: VLAgent利用上下文学习微调LLM生成分步规划，并通过神经符号模块执行验证，优化逻辑错误和幻觉问题。
- Result: 在多个视觉推理基准测试中，VLAgent性能显著优于现有模型。
- Conclusion: VLAgent通过新颖的优化模块提升了多模态推理能力，具有实际应用潜力。


### [143] [Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion Methods](https://arxiv.org/abs/2506.07779)
*Beining Xu,Junxian Li*

Main category: cs.CV

TL;DR: 论文提出了一种高质量的双光谱数据集和全面的评估框架，以改进可见光和红外图像融合方法的评估。

- Motivation: 当前评估方法缺乏标准化基准和下游任务性能验证，且数据集不足，阻碍了融合方法的进展。
- Method: 构建了一个包含1,369对齐图像对的双光谱数据集，并提出融合速度、通用指标和目标检测性能的综合评估框架。
- Result: 实验表明，针对下游任务优化的融合模型在目标检测中表现更优，尤其是低光和遮挡场景。
- Conclusion: 论文贡献包括高质量数据集、任务感知评估框架和融合方法的全面分析，为未来研究提供了参考。


### [144] [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)
*Qi Yang,Chenghao Zhang,Lubin Fan,Kun Ding,Jieping Ye,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出了一种多模态RAG框架RCTS，通过构建推理上下文丰富的知识库和树搜索重排序方法，提升LVLMs在VQA任务中的性能。

- Motivation: 现有方法在知识推理示例稀缺和检索知识响应不稳定方面存在不足。
- Method: 引入自一致评估机制丰富知识库，并提出基于启发式奖励的蒙特卡洛树搜索（MCTS-HR）重排序。
- Result: 在多个VQA数据集上达到SOTA性能，显著优于ICL和Vanilla-RAG方法。
- Conclusion: RCTS框架通过高质量推理上下文和重排序方法有效提升了LVLMs的性能。


### [145] [Image Reconstruction as a Tool for Feature Analysis](https://arxiv.org/abs/2506.07803)
*Eduard Allakhverdov,Dmitrii Tarasov,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.CV

TL;DR: 论文提出了一种通过图像重建解释视觉特征的新方法，比较了SigLIP和SigLIP2模型，发现基于图像任务预训练的编码器保留更多图像信息。方法适用于任何视觉编码器，揭示了特征空间的内部结构。

- Motivation: 尽管视觉编码器在应用中表现优异，但其内部特征表示方式仍不明确，因此需要一种方法来解释这些特征。
- Method: 通过图像重建比较不同视觉编码器（如SigLIP和SigLIP2）的特征表示能力，并分析特征空间的结构。
- Result: 基于图像任务预训练的编码器保留更多图像信息；正交旋转控制颜色编码。
- Conclusion: 该方法可揭示视觉编码器特征空间的内部结构，为理解其工作原理提供了新视角。


### [146] [Incorporating Uncertainty-Guided and Top-k Codebook Matching for Real-World Blind Image Super-Resolution](https://arxiv.org/abs/2506.07809)
*Weilei Wen,Tianyi Zhang,Qianqian Zhao,Zhaohui Zheng,Chunle Guo,Xiuli Shao,Chongyi Li*

Main category: cs.CV

TL;DR: 提出了一种基于不确定性引导和Top-k代码书匹配的超分辨率框架（UGTSR），解决了现有方法中特征匹配不准确和纹理细节重建差的问题。

- Motivation: 现有代码书超分辨率方法存在特征匹配不准确和纹理细节重建不足的问题，需要改进。
- Method: UGTSR框架包含不确定性学习机制、Top-k特征匹配策略和对齐注意力模块。
- Result: 实验表明，UGTSR在纹理真实性和重建保真度上显著优于现有方法。
- Conclusion: UGTSR框架有效提升了超分辨率任务的性能，代码将公开。


### [147] [Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning](https://arxiv.org/abs/2506.07811)
*Tieyuan Chen,Huabin Liu,Yi Wang,Chaofan Gan,Mingxi Lyu,Gui Zou,Weiyao Lin*

Main category: cs.CV

TL;DR: 该论文提出了一个新的任务和数据集I-VQA，专注于在无法直接获取显式视觉证据的情况下回答问题，并提出了一个双流推理框架IRM，显著优于现有方法。

- Motivation: 现有VideoQA工作主要依赖显式视觉证据，但在涉及符号意义或深层意图的问题上表现不佳，因此需要解决隐式视觉证据的问题。
- Method: 提出了IRM框架，包含Action-Intent Module（AIM）和Visual Enhancement Module（VEM），通过双流建模和上下文线索增强实现隐式推理。
- Result: IRM在I-VQA任务中表现优异，分别超过GPT-4o、OpenAI-o3和VideoChat2 0.76%、1.37%和4.87%，并在类似任务中达到SOTA。
- Conclusion: IRM有效解决了隐式视觉证据的推理问题，为VideoQA领域提供了新的解决方案。


### [148] [Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution](https://arxiv.org/abs/2506.07813)
*Junseo Bang,Joonhee Lee,Kyeonghyun Lee,Haechang Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: CasArbi是一种新型的自级联扩散框架，用于任意尺度图像超分辨率，通过逐步增强分辨率实现灵活上采样。

- Motivation: 传统固定尺度超分辨率缺乏灵活性，而现有方法在连续尺度分布上学习困难。
- Method: 采用自级联扩散框架，将大尺度分解为小尺度序列，结合坐标引导残差扩散模型。
- Result: 在多种任意尺度超分辨率基准测试中，CasArbi在感知和失真性能上优于现有方法。
- Conclusion: CasArbi通过逐步扩散和连续表示学习，实现了高效且灵活的任意尺度超分辨率。


### [149] [M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration](https://arxiv.org/abs/2506.07814)
*Yongzhen Wang,Yongjun Li,Zhuoran Zheng,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: M2Restore提出了一种基于Mixture-of-Experts的Mamba-CNN融合框架，用于高效、鲁棒的全能图像恢复，解决了现有方法在动态退化场景中泛化能力不足和局部细节与全局依赖平衡不佳的问题。

- Motivation: 自然图像常受复杂退化（如雨、雪、雾）影响，现有方法在动态退化场景中泛化能力有限，且难以平衡局部细节与全局依赖。
- Method: 1. 利用CLIP引导的MoE门控机制融合任务条件提示和语义先验；2. 设计双流架构结合CNN的局部表征和Mamba的长程建模；3. 引入边缘感知动态门控机制自适应平衡全局与局部。
- Result: 在多个图像恢复基准测试中，M2Restore在视觉质量和定量性能上均表现出优越性。
- Conclusion: M2Restore通过创新的架构和机制，显著提升了图像恢复的性能和效率。


### [150] [R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation](https://arxiv.org/abs/2506.07826)
*William Ljungbergh,Bernardo Taveira,Wenzhao Zheng,Adam Tonderski,Chensheng Peng,Fredrik Kahl,Christoffer Petersson,Michael Felsberg,Kurt Keutzer,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: R3D2是一种轻量级扩散模型，用于在自动驾驶验证中实现真实3D资产插入，提升场景真实感。

- Motivation: 传统仿真平台资源密集且存在领域差距，神经重建方法如3DGS难以动态操作和复用。
- Method: R3D2通过训练新型数据集，实现实时生成逼真渲染效果（如阴影和光照）。
- Result: R3D2显著提升插入资产的真实感，支持文本到3D插入和跨场景对象转移。
- Conclusion: R3D2为自动驾驶验证提供了可扩展且逼真的仿真解决方案，数据集和代码将开源。


### [151] [Diffusion models under low-noise regime](https://arxiv.org/abs/2506.07841)
*Elizabeth Pavlova,Xue-Xin Wei*

Main category: cs.CV

TL;DR: 扩散模型在低噪声条件下的行为研究，揭示训练数据规模、几何结构和模型目标对去噪轨迹的影响。

- Motivation: 研究扩散模型在小噪声条件下的行为，填补现有研究在高噪声条件下的不足，提升模型在实际应用中的可靠性和可解释性。
- Method: 使用CelebA子集和解析高斯混合基准，分析模型在低噪声扩散动力学下的行为。
- Result: 发现模型在数据流形附近的行为会因训练数据不同而分化，即使在高噪声输出时表现一致。
- Conclusion: 研究为理解扩散模型如何学习数据分布提供了新视角，有助于提升模型在小扰动场景中的可靠性。


### [152] [F2Net: A Frequency-Fused Network for Ultra-High Resolution Remote Sensing Segmentation](https://arxiv.org/abs/2506.07847)
*Hengzhi Chen,Liqian Feng,Wenhua Wu,Xiaogang Zhu,Shawn Leo,Kun Hu*

Main category: cs.CV

TL;DR: F2Net是一种频率感知框架，通过分解超高清遥感图像为高频和低频组件进行专门处理，解决了传统方法在细节丢失和全局上下文碎片化之间的权衡问题。

- Motivation: 超高清遥感图像的语义分割在环境监测和城市规划中至关重要，但传统方法存在细节丢失或全局上下文碎片化的问题，多分支网络则效率低且训练不稳定。
- Method: F2Net将图像分解为高频和低频组件，高频分支保留全分辨率细节，低频分支通过双子分支捕获短程和长程依赖关系，并通过混合频率融合模块整合。
- Result: 在DeepGlobe和Inria Aerial基准测试中，F2Net分别达到80.22和83.39的mIoU，表现最优。
- Conclusion: F2Net通过频率分解和融合模块，实现了高效且稳定的超高清图像语义分割，性能优于现有方法。


### [153] [PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement](https://arxiv.org/abs/2506.07848)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Jiangning Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: PolyVivid是一个多主体视频定制框架，通过文本-图像融合模块和3D-RoPE增强模块实现身份一致性和交互性，优于现有基线。

- Motivation: 现有视频生成模型在多主体定制中缺乏细粒度控制，尤其是身份一致性和交互性。
- Method: 设计了VLLM文本-图像融合模块、3D-RoPE增强模块和注意力继承身份注入模块，结合MLLM数据管道。
- Result: 实验表明PolyVivid在身份保真度、视频真实性和主体对齐方面表现优异。
- Conclusion: PolyVivid在多主体视频定制中实现了灵活且身份一致的生成，性能优于现有方法。


### [154] [SAM2Auto: Auto Annotation Using FLASH](https://arxiv.org/abs/2506.07850)
*Arash Rocky,Q. M. Jonathan Wu*

Main category: cs.CV

TL;DR: SAM2Auto是一个全自动视频数据集标注系统，无需人工干预或数据集特定训练，显著减少标注时间和成本。

- Motivation: 解决视觉语言模型（VLMs）因标注数据稀缺而发展滞后的问题，传统标注方法费时费力。
- Method: 结合SMART-OD（自动掩码生成与开放世界目标检测）和FLASH（实时视频实例分割），通过统计方法减少检测错误并保持对象跟踪一致性。
- Result: 实验表明，SAM2Auto的标注精度与人工相当，显著提升效率并降低成本，适用于多样化数据集。
- Conclusion: SAM2Auto为自动化视频标注设定了新基准，解决了视觉语言理解中的数据集瓶颈，加速VLM发展。


### [155] [LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2506.07857)
*Zihui Zhang,Weisheng Dai,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: 提出了一种名为LogoSP的无监督3D语义分割方法，通过结合局部和全局点特征学习语义信息，并在频域中利用全局模式生成高精度伪标签。

- Motivation: 现有方法仅依赖局部特征，缺乏发现更丰富语义先验的能力，因此需要一种能够结合局部和全局特征的无监督方法。
- Method: LogoSP通过频域中的全局模式对超点进行分组，生成语义伪标签，用于训练分割网络。
- Result: 在两个室内和一个室外数据集上，LogoSP显著优于现有无监督方法，达到最先进的性能。
- Conclusion: LogoSP证明了在无人工标签的情况下，全局模式能够有效表示有意义的3D语义信息。


### [156] [Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction](https://arxiv.org/abs/2506.07860)
*Ivan Alberico,Marco Cannici,Giovanni Cioffi,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机的实时乒乓球轨迹预测系统，利用高时间分辨率和注视数据优化性能。

- Motivation: 解决标准相机在高速度乒乓球运动中存在的延迟和运动模糊问题。
- Method: 使用事件相机和注视数据，结合生物启发式方法优化检测和计算效率。
- Result: 系统总延迟低至4.5毫秒，计算效率提升10.81倍。
- Conclusion: 首次实现了基于事件相机的乒乓球轨迹预测，性能显著优于传统帧相机系统。


### [157] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: VIVAT通过简单调整KL-VAE训练中的损失权重、填充策略和引入空间条件归一化，显著减少了五种常见伪影，提升了图像重建和生成质量。

- Motivation: KL-VAE训练中常见的伪影（如颜色偏移、网格模式等）降低了重建和生成质量，亟需解决。
- Method: 提出VIVAT方法，通过调整损失权重、优化填充策略和引入Spatially Conditional Normalization，系统性减少伪影。
- Result: 在多个基准测试中，VIVAT在PSNR和SSIM指标上达到最优，同时提升了文本到图像生成的CLIP分数。
- Conclusion: VIVAT在保持KL-VAE框架简单性的同时，有效解决了训练中的实际问题，为优化VAE训练提供了实用方案。


### [158] [FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity](https://arxiv.org/abs/2506.07865)
*Jinxi Li,Ziyang Song,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 论文提出FreeGave方法，无需物体先验即可学习复杂动态3D场景的物理特性，通过设计无散度模块估计速度场，避免了低效的PINN损失。

- Motivation: 现有方法难以学习边界复杂物理运动或依赖物体先验（如掩码或类型），因此需要一种无需先验的方法。
- Method: 引入物理编码并设计无散度模块，估计每高斯速度场，避免使用PINN损失。
- Result: 在多个数据集上验证了方法在帧外推和运动分割上的优越性，物理编码成功学习无标签的3D物理运动模式。
- Conclusion: FreeGave无需先验即可学习复杂物理场景，且物理编码具有实际意义。


### [159] [Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow](https://arxiv.org/abs/2506.07878)
*Muhammad Ahmed Humais,Xiaoqian Huang,Hussain Sajwani,Sajid Javed,Yahya Zweiri*

Main category: cs.CV

TL;DR: 论文提出了一种新型的STSSM模块和网络架构，用于高效的事件相机运动估计，性能优于现有方法。

- Motivation: 事件相机在低延迟运动估计中具有潜力，但现有深度学习方法效率不足，而异步事件方法又缺乏时空信息捕捉能力。
- Method: 引入STSSM模块，利用状态空间模型高效捕捉事件数据的时空相关性，设计新型网络架构。
- Result: 在DSEC基准测试中，模型推理速度比TMA快4.5倍，计算量比EV-FlowNet低2倍，性能具有竞争力。
- Conclusion: STSSM模块为事件相机的运动估计提供了一种高效且性能优异的解决方案。


### [160] [CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing](https://arxiv.org/abs/2506.07885)
*Zubin Bhuyan,Yuanchang Xie,AngkeaReach Rith,Xintong Yan,Nasko Apostolov,Jimi Oke,Chengbo Ai*

Main category: cs.CV

TL;DR: CrosswalkNet是一种高效的深度学习框架，用于从高分辨率航拍图像中检测行人横道，采用定向边界框（OBB）提升检测精度，并在多个数据集上表现出色。

- Motivation: 随着航拍和卫星图像的普及，深度学习在交通资产管理、安全分析和城市规划中具有巨大潜力。
- Method: 提出CrosswalkNet框架，结合OBB、注意力机制和优化技术，利用23,000多个标注样本训练模型。
- Result: 在Massachusetts数据集上达到96.5%的精确率和93.3%的召回率，并在其他州数据集上无需微调即表现优异。
- Conclusion: CrosswalkNet为决策者和规划者提供了高效工具，可提升行人安全和城市流动性。


### [161] [EgoM2P: Egocentric Multimodal Multitask Pretraining](https://arxiv.org/abs/2506.07886)
*Gen Li,Yutong Chen,Yiqian Wu,Kaifeng Zhao,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: 论文提出EgoM2P框架，通过高效的时间标记器和掩码建模解决多模态自我中心视觉任务中的挑战，支持多任务处理，性能优于专业模型。

- Motivation: 自我中心视觉的多模态数据（如RGB视频、深度、相机位姿和视线）在增强现实、机器人等领域有重要应用，但数据异构性和缺失模态的伪标签生成困难，现有模型难以直接应用。
- Method: 引入高效时间标记器，提出EgoM2P框架，通过掩码建模学习多模态时间标记，支持多任务处理（如视线预测、相机跟踪、深度估计）。
- Result: EgoM2P在多任务中性能匹配或优于专业模型，且速度快一个数量级。
- Conclusion: EgoM2P为自我中心视觉提供了一种高效的多任务解决方案，将开源以推动研究。


### [162] [Video Unlearning via Low-Rank Refusal Vector](https://arxiv.org/abs/2506.07891)
*Simone Facchiano,Stefano Saravalle,Matteo Migliarini,Edoardo De Matteis,Alessio Sampieri,Andrea Pilzer,Emanuele Rodolà,Indro Spinelli,Luca Franco,Fabio Galasso*

Main category: cs.CV

TL;DR: 论文提出了一种针对视频扩散模型的无学习技术，仅需5对多模态提示对即可消除模型中的有害概念，同时保持生成质量。

- Motivation: 视频生成模型可能继承训练数据中的偏见和有害内容，导致用户生成不良或非法内容，亟需解决方案。
- Method: 通过计算安全与不安全示例的潜在差异生成“拒绝向量”，并采用低秩分解方法优化，直接嵌入模型权重。
- Result: 方法能有效消除多种有害内容（如裸露、暴力、版权等），且不影响生成质量。
- Conclusion: 该技术无需重新训练或原始数据，直接嵌入拒绝向量，提升了对抗绕过尝试的鲁棒性。


### [163] [WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning](https://arxiv.org/abs/2506.07905)
*Jie Yang,Feipeng Ma,Zitian Wang,Dacheng Yin,Kang Rong,Fengyun Rao,Ruimao Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种扩展文本推理模型到多模态推理的方法，通过生成多模态QA对和优化RL训练，显著提升了多模态大语言模型的性能。

- Motivation: 探索如何通过强化学习实现通用的视觉-语言推理，填补现有方法在通用任务上的不足。
- Method: 1. 开发了可扩展的多模态QA合成流水线；2. 开源了WeThink数据集；3. 结合规则和模型评估的混合奖励机制优化RL训练。
- Result: 在14个多模态基准测试中表现优异，数据多样性持续提升模型性能。
- Conclusion: WeThink数据集和自动化数据流水线为多模态推理提供了高效解决方案，具有广泛的应用潜力。


### [164] [A Comparative Study of U-Net Architectures for Change Detection in Satellite Images](https://arxiv.org/abs/2506.07925)
*Yaxita Amin,Naimisha S Trivedi,Rashmi Bhattad*

Main category: cs.CV

TL;DR: 本文通过分析34篇论文，比较了18种U-Net变体在遥感变化检测中的应用，填补了该领域的空白。

- Motivation: 遥感变化检测对监测地球景观变化至关重要，但U-Net架构在该领域的应用尚未充分探索。
- Method: 研究比较了18种U-Net变体，评估其在遥感变化检测中的潜力，特别关注专为变化检测设计的变体（如Siamese Swin-U-Net）。
- Result: 分析揭示了处理不同时间数据及长距离关系对提升变化检测精度的重要性。
- Conclusion: 研究为选择U-Net版本用于遥感变化检测提供了有价值的参考。


### [165] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型（VLM）在多模态上下文学习（MM-ICL）中依赖浅层启发式方法而非真实任务理解，性能在分布偏移下下降。提出新方法MM-ICL with Reasoning，但实验显示当前VLM未能有效利用演示信息。

- Motivation: 探讨VLM是否真正具备多模态上下文学习能力，而非依赖浅层启发式方法。
- Method: 提出MM-ICL with Reasoning方法，为每个演示生成答案和推理过程，并在不同数据集和模型上进行实验。
- Result: 实验表明VLM性能对演示数量、检索方法等不敏感，未能有效利用演示信息。
- Conclusion: 当前VLM在多模态上下文学习中表现有限，需进一步改进以提升任务理解能力。


### [166] [Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations](https://arxiv.org/abs/2506.07943)
*Yizhen Li,Dell Zhang,Xuelong Li,Yiqing Shen*

Main category: cs.CV

TL;DR: DTwinSeger是一种新的Reasoning Segmentation方法，通过Digital Twin表示将感知与推理解耦，利用LLM进行显式推理，在多个基准测试中表现优异。

- Motivation: 当前方法在图像标记化时破坏了对象间的空间关系，需要一种新方法来保留空间关系并提升推理能力。
- Method: DTwinSeger将任务分为两阶段：生成结构化DT表示，再用LLM进行推理。提出了针对LLM的监督微调方法和数据集Seg-DT。
- Result: 在两个图像RS基准和三个图像参考分割基准中达到最优性能。
- Conclusion: DT表示是视觉与文本间的有效桥梁，仅用LLM即可完成复杂多模态推理任务。


### [167] [Creating a Historical Migration Dataset from Finnish Church Records, 1800-1920](https://arxiv.org/abs/2506.07960)
*Ari Vesalainen,Jenna Kanerva,Aida Nitsch,Kiia Korsu,Ilari Larkiola,Laura Ruotsalainen,Filip Ginter*

Main category: cs.CV

TL;DR: 利用深度学习技术从芬兰1800-1920年的教会迁移记录中提取结构化数据，支持历史与人口研究。

- Motivation: 利用数字化教会迁移记录研究芬兰历史人口模式，填补相关领域数据空白。
- Method: 采用深度学习流程自动化提取数据，包括布局分析、表格检测、单元格分类和手写识别。
- Result: 生成包含600多万条目的结构化数据集，适用于研究内部迁移、城市化、家庭迁移及疾病传播。
- Conclusion: 展示了如何将大量手写档案转化为结构化数据，为历史和人口研究提供支持。


### [168] [SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design](https://arxiv.org/abs/2506.07964)
*Wenxin Tang,Jingyu Xiao,Wenxuan Jiang,Xi Xiao,Yuhang Wang,Xuxin Tang,Qing Li,Yuehe Ma,Junliang Liu,Shisong Tang,Michael R. Lyu*

Main category: cs.CV

TL;DR: 论文提出Slide2Code基准和SlideCoder框架，用于从参考图像生成可编辑幻灯片，解决了现有方法在视觉和结构设计上的不足。

- Motivation: 手动制作幻灯片耗时且需要专业知识，现有基于自然语言的LLM方法难以捕捉幻灯片设计的视觉和结构细节。
- Method: 提出Slide2Code基准和SlideCoder框架，结合颜色梯度分割算法和分层检索增强生成方法，优化代码生成。
- Result: SlideCoder在布局保真度、执行准确性和视觉一致性上优于现有方法，提升达40.5分。
- Conclusion: SlideCoder展示了在幻灯片生成任务中的强大性能，并开源了SlideMaster模型和代码。


### [169] [SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence](https://arxiv.org/abs/2506.07966)
*Ziyang Gong,Wenhao Li,Oliver Ma,Songyuan Li,Jiayi Ji,Xue Yang,Gen Luo,Junchi Yan,Rongrong Ji*

Main category: cs.CV

TL;DR: SpaCE-10是一个用于评估多模态大语言模型（MLLMs）空间智能的综合性基准，包含10种原子空间能力和8种组合能力，通过5k+QA对进行测试，发现现有MLLMs与人类表现仍有显著差距。

- Motivation: 现有基准难以全面评估MLLMs从原子到组合层面的空间智能，因此提出SpaCE-10填补这一空白。
- Method: 定义10种原子空间能力和8种组合能力，采用分层标注流程生成高质量QA对，覆盖多种评估设置。
- Result: 测试显示，即使最先进的MLLMs在空间智能上仍大幅落后于人类，且计数能力不足显著限制其组合能力。
- Conclusion: SpaCE-10为MLLM社区提供了重要发现和评估工具，揭示了现有模型的局限性。


### [170] [CyberV: Cybernetics for Test-time Scaling in Video Understanding](https://arxiv.org/abs/2506.07971)
*Jiahao Meng,Shuyang Sun,Yue Tan,Lu Qi,Yunhai Tong,Xiangtai Li,Longyin Wen*

Main category: cs.CV

TL;DR: CyberV是一种基于控制论原理的新型框架，通过动态资源分配和自我监控提升多模态大语言模型（MLLMs）在视频理解中的性能，无需重新训练。

- Motivation: 现有MLLMs在长或复杂视频理解中存在计算需求高、鲁棒性差和准确率低的问题，尤其是参数较少的模型。
- Method: 提出CyberV框架，包含MLLM推理系统、传感器和控制器，通过监控和反馈实现动态调整。
- Result: 实验显示CyberV显著提升模型性能，如Qwen2.5-VL-7B提升8.3%，性能甚至接近人类专家。
- Conclusion: CyberV有效增强MLLMs的动态视频理解能力，具有广泛适用性和泛化能力。


### [171] [OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation](https://arxiv.org/abs/2506.07977)
*Jingjing Chang,Yixiao Fang,Peng Xing,Shuhan Wu,Wei Cheng,Rui Wang,Xianfang Zeng,Gang Yu,Hai-Bao Chen*

Main category: cs.CV

TL;DR: OneIG-Bench是一个全面的文本到图像（T2I）模型评估框架，针对多维度（如对齐、文本渲染、推理等）进行细粒度评估，填补现有评测系统的不足。

- Motivation: 现有T2I模型的评测系统未能全面评估推理、文本渲染和风格化等关键维度，限制了模型性能的深入分析。
- Method: 设计OneIG-Bench框架，支持多维度（如对齐、文本渲染、推理生成内容等）的灵活评估，用户可针对特定维度生成图像并完成评测。
- Result: OneIG-Bench提供了公开的代码和数据集，支持可重复的评测研究和跨模型比较。
- Conclusion: OneIG-Bench填补了T2I模型评测的空白，为研究者和实践者提供了系统化的分析工具。


### [172] [Real-time Localization of a Soccer Ball from a Single Camera](https://arxiv.org/abs/2506.07981)
*Dmitrii Vorobev,Artem Prosvetov,Karim Elhadji Daou*

Main category: cs.CV

TL;DR: 提出了一种高效的单摄像头实时三维足球轨迹重建方法，通过多模态状态模型加速优化，保持厘米级精度，适用于复杂场景。

- Motivation: 解决现有方法在遮挡、运动模糊和复杂背景下的性能问题，同时降低多摄像头系统的成本和复杂性。
- Method: 引入多模态状态模型（$W$离散模态）加速优化，适用于标准CPU，实现低延迟。
- Result: 在6K分辨率俄罗斯超级联赛数据集上表现媲美多摄像头系统，无需昂贵设备。
- Conclusion: 提供了一种实用、低成本且高精度的三维足球轨迹跟踪方法。


### [173] [CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray](https://arxiv.org/abs/2506.07984)
*Mingquan Lin,Gregory Holste,Song Wang,Yiliang Zhou,Yishu Wei,Imon Banerjee,Pengyi Chen,Tianjie Dai,Yuexi Du,Nicha C. Dvornek,Yuyan Ge,Zuowei Guo,Shouhei Hanaoka,Dongkyun Kim,Pablo Messina,Yang Lu,Denis Parra,Donghyun Son,Álvaro Soto,Aisha Urooj,René Vidal,Yosuke Yamagishi,Zefan Yang,Ruichi Zhang,Yang Zhou,Leo Anthony Celi,Ronald M. Summers,Zhiyong Lu,Hao Chen,Adam Flanders,George Shih,Zhangyang Wang,Yifan Peng*

Main category: cs.CV

TL;DR: CXR-LT 2024是一个社区驱动的项目，旨在通过扩展数据集和引入零样本学习，提升胸部X射线（CXR）的肺病分类性能。

- Motivation: 解决开放长尾肺病分类中的挑战，并提升现有技术的可测量性。
- Method: 扩展数据集至377,110张CXR和45种疾病标签，引入零样本学习，并设计三项任务：长尾分类（噪声测试集和黄金标准子集）及零样本泛化。
- Result: 提供了高质量基准数据，整合了多模态模型、生成方法和零样本学习策略，提升了疾病覆盖率和临床实用性。
- Conclusion: CXR-LT 2024为开发更具临床现实性和泛化性的肺病诊断模型提供了宝贵资源。


### [174] [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
*Tuomas Oikarinen,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CV

TL;DR: 本文提出了一种高效且准确的众包评估策略，用于评估神经元解释的可靠性，并通过重要性采样和贝叶斯方法显著降低了成本。

- Motivation: 现有的神经元解释评估方法存在噪音大、成本高的问题，需要一种更可靠的评估策略。
- Method: 引入重要性采样选择最有价值的输入，并提出贝叶斯方法聚合多个评分，以减少所需评分数量。
- Result: 实现了约30倍的成本降低和约5倍的评分数量减少，同时保持高准确性。
- Conclusion: 该方法为大规模比较不同神经元解释方法提供了高效且可靠的评估工具。


### [175] [Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers](https://arxiv.org/abs/2506.07986)
*Zhengyao Lv,Tianlin Pan,Chenyang Si,Zhaoxi Chen,Wangmeng Zuo,Ziwei Liu,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: 论文提出了一种名为TACA的方法，通过动态调整跨模态注意力的温度和时间步依赖权重，解决了MM-DiT模型中文本与图像对齐不精确的问题。

- Motivation: 现有MM-DiT模型（如FLUX）在文本驱动的视觉生成中表现优异，但仍存在文本提示与生成内容对齐不精确的问题，主要源于跨模态注意力不平衡和缺乏时间步感知权重。
- Method: 提出Temperature-Adjusted Cross-modal Attention (TACA)，通过温度缩放和时间步依赖调整动态平衡多模态交互，并结合LoRA微调。
- Result: 在T2I-CompBench基准测试中，TACA显著提升了文本-图像对齐效果（如物体外观、属性绑定和空间关系），且计算开销极小。
- Conclusion: TACA通过平衡跨模态注意力，显著提升了文本到图像扩散模型的语义保真度，代码已开源。


### [176] [PairEdit: Learning Semantic Variations for Exemplar-based Image Editing](https://arxiv.org/abs/2506.07992)
*Haoguang Lu,Jiacheng Chen,Zhenguo Yang,Aurele Tohokantche Gnanha,Fu Lee Wang,Li Qing,Xudong Mao*

Main category: cs.CV

TL;DR: PairEdit是一种无需文本指导的视觉编辑方法，通过少量图像对学习复杂编辑语义。

- Motivation: 现有基于示例的编辑方法仍需依赖文本提示，而某些编辑语义难以通过文本精确描述。
- Method: 提出目标噪声预测和内容保留噪声调度，优化LoRAs以分离语义变化与内容学习。
- Result: PairEdit能有效学习复杂语义，显著提升内容一致性。
- Conclusion: PairEdit在无需文本指导的情况下，成功实现了高质量的图像编辑。


### [177] [UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References](https://arxiv.org/abs/2506.07996)
*Ming-Feng Li,Xin Yang,Fu-En Wang,Hritam Basak,Yuyin Sun,Shreekant Gayaka,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: UA-Pose提出了一种不确定性感知的6D物体姿态估计方法，适用于部分参考数据，显著提升了在不完整观测下的性能。

- Motivation: 现有方法需要完整3D模型或大量参考图像，而部分参考数据（如碎片化外观和几何）的6D姿态估计仍具挑战性。
- Method: 基于有限RGBD图像或单张2D图像生成部分3D模型，并引入不确定性区分已见和未见区域，结合不确定性感知采样策略完成在线物体补全。
- Result: 在YCB-Video等数据集上实验表明，UA-Pose在不完整观测下显著优于现有方法。
- Conclusion: UA-Pose通过不确定性建模和在线补全，提升了部分参考数据下的6D姿态估计鲁棒性和准确性。


### [178] [MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation](https://arxiv.org/abs/2506.07999)
*Junhao Chen,Yulia Tsvetkov,Xiaochuang Han*

Main category: cs.CV

TL;DR: 论文提出MADFormer，结合自回归和扩散模型，通过分块和分层混合优化高分辨率图像生成。

- Motivation: 现有混合模型缺乏系统指导，无法明确如何分配自回归和扩散模型的能力。
- Method: MADFormer将图像生成分块，自回归层用于全局条件，扩散层用于局部细化。
- Result: 实验表明分块显著提升高分辨率图像生成性能，混合层在质量和效率上取得更好平衡。
- Conclusion: 研究为未来混合生成模型提供了实用设计原则。


### [179] [Aligning Text, Images, and 3D Structure Token-by-Token](https://arxiv.org/abs/2506.08002)
*Aadarsh Sahoo,Vansh Tibrewal,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 论文提出了一种统一的LLM框架，将语言、图像和3D场景对齐，并提供了关键设计选择的详细指南，以优化训练和性能。

- Motivation: 创建能够理解3D世界的机器，以辅助设计师构建和编辑3D环境，以及帮助机器人在三维空间中导航和交互。
- Method: 采用自回归模型，提出统一的LLM框架，结合语言、图像和3D场景，并通过量化形状编码增强3D模态。
- Result: 在四个核心3D任务（渲染、识别、指令跟随和问答）和四个3D数据集上评估性能，展示了模型在真实世界3D物体识别任务中的有效性。
- Conclusion: 该框架为3D场景建模提供了新的可能性，并在多模态对齐和3D任务中表现出色。


### [180] [Audio-Sync Video Generation with Multi-Stream Temporal Control](https://arxiv.org/abs/2506.08003)
*Shuchen Weng,Haojie Zheng,Zheng Chang,Si Li,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: MTV是一个用于音频同步视频生成的框架，通过分离音频为语音、效果和音乐轨道，实现精细控制。DEMIX数据集支持其训练，实验显示MTV在视频质量、文本一致性和音视频对齐方面表现优异。

- Motivation: 音频与视觉世界紧密同步，是可控视频生成的理想控制信号，但现有方法在高质量视频生成和音视频同步方面不足。
- Method: MTV将音频分为语音、效果和音乐轨道，分别控制唇动、事件时间和视觉情绪，并利用DEMIX数据集进行多阶段训练。
- Result: MTV在视频质量、文本一致性和音视频对齐六个标准指标上达到最优性能。
- Conclusion: MTV通过音频分离和多阶段训练，实现了高质量且音视频同步的视频生成。


### [181] [Dynamic View Synthesis as an Inverse Problem](https://arxiv.org/abs/2506.08004)
*Hidir Yesiltepe,Pinar Yanardag*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的基于单目视频的动态视图合成方法，通过改进预训练视频扩散模型的噪声初始化阶段，实现了高保真度的动态视图合成。

- Motivation: 解决动态视图合成中的确定性反转问题，尤其是在零终端信噪比（SNR）调度下的挑战。
- Method: 引入K阶递归噪声表示（K-order Recursive Noise Representation）和随机潜在调制（Stochastic Latent Modulation）技术。
- Result: 实验证明，通过噪声初始化阶段的结构化潜在操作，可以有效实现动态视图合成。
- Conclusion: 该方法在无需权重更新或辅助模块的情况下，实现了高质量的动态视图合成。


### [182] [ZeroVO: Visual Odometry with Minimal Assumptions](https://arxiv.org/abs/2506.08005)
*Lei Lai,Zekai Yin,Eshed Ohn-Bar*

Main category: cs.CV

TL;DR: ZeroVO是一种新型视觉里程计算法，无需预定义或静态相机校准，实现了跨多样相机和环境的零样本泛化。

- Motivation: 现有方法依赖预定义或静态相机校准，限制了其适用性。ZeroVO旨在克服这一限制，提供更通用的解决方案。
- Method: 1. 设计无校准、几何感知的网络结构；2. 引入基于语言的先验增强语义信息；3. 开发半监督训练范式，适应新场景。
- Result: 在KITTI、nuScenes和Argoverse 2等基准测试中，性能提升超过30%。
- Conclusion: ZeroVO无需微调或相机校准，扩展了视觉里程计的适用性，适用于大规模实际部署。


### [183] [Dreamland: Controllable World Creation with Simulator and Generative Models](https://arxiv.org/abs/2506.08006)
*Sicheng Mo,Ziyang Leng,Leon Liu,Weizhen Wang,Honglin He,Bolei Zhou*

Main category: cs.CV

TL;DR: Dreamland是一个结合物理模拟器和生成模型的混合世界生成框架，通过分层抽象增强可控性，提升图像质量和可控性。

- Motivation: 现有大规模视频生成模型缺乏元素级可控性，限制了其在场景编辑和AI代理训练中的应用。
- Method: 设计分层世界抽象，结合物理模拟器的控制力和生成模型的真实感，使用中间表示桥接两者。
- Result: 实验显示Dreamland图像质量提升50.8%，可控性增强17.9%，并支持现成生成模型。
- Conclusion: Dreamland在可控性和生成质量上表现优异，有望推动AI代理训练的发展。


### [184] [Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/abs/2506.08008)
*Stephanie Fu,Tyler Bonnen,Devin Guillory,Trevor Darrell*

Main category: cs.CV

TL;DR: 论文比较了视觉语言模型（VLMs）与其视觉编码器的性能，发现VLMs在视觉任务中表现显著更差，接近随机水平。瓶颈在于VLMs未能有效利用视觉信息，且受语言模型先验影响。

- Motivation: 探索VLMs如何整合视觉与语言信息，并诊断其在视觉任务中的失败模式。
- Method: 通过一系列视觉中心基准测试（如深度估计、对应关系）比较VLMs与视觉编码器的性能，并分析VLMs的瓶颈。
- Result: VLMs在视觉任务中表现显著低于视觉编码器，主要原因是未能有效利用视觉信息且受语言模型先验影响。
- Conclusion: 研究为未来VLMs的视觉理解能力评估提供了诊断工具和方向。


### [185] [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](https://arxiv.org/abs/2506.08009)
*Xun Huang,Zhengqi Li,Guande He,Mingyuan Zhou,Eli Shechtman*

Main category: cs.CV

TL;DR: Self Forcing是一种新的自回归视频扩散模型训练范式，通过自生成输出和KV缓存解决曝光偏差问题，实现高效实时视频生成。

- Motivation: 解决自回归视频扩散模型在推理时因依赖自身不完美输出而导致的曝光偏差问题。
- Method: 采用自回归展开和KV缓存，结合多步扩散模型和随机梯度截断策略，提出滚动KV缓存机制。
- Result: 在单GPU上实现亚秒级延迟的实时视频生成，性能优于非因果扩散模型。
- Conclusion: Self Forcing在生成质量和效率上均表现优异，适用于实时视频生成任务。


### [186] [Vision Transformers Don't Need Trained Registers](https://arxiv.org/abs/2506.08010)
*Nick Jiang,Amil Dravid,Alexei Efros,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 研究发现Vision Transformers中存在高范数token导致注意力图噪声的问题，提出一种无需重新训练的方法，通过转移高范数激活到额外token来改善模型性能。

- Motivation: 解决Vision Transformers中高范数token导致的注意力噪声问题，避免重新训练模型的成本。
- Method: 通过将高范数激活转移到额外未训练的token，模拟注册token的效果。
- Result: 方法能生成更清晰的注意力和特征图，提升下游视觉任务性能，效果接近显式训练注册token的模型。
- Conclusion: 测试时注册方法为未预置注册token的预训练模型提供了无需训练的解决方案。


### [187] [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
*Yunfei Xie,Yinsong Ma,Shiyi Lan,Alan Yuille,Junfei Xiao,Chen Wei*

Main category: cs.CV

TL;DR: 通过视觉游戏学习（ViGaL）提升多模态大语言模型（MLLMs）的泛化推理能力，无需特定领域数据即可显著提升多模态数学和跨学科问题的表现。

- Motivation: 认知科学研究表明，游戏能促进可迁移的认知技能，因此提出通过游戏训练提升MLLMs的泛化推理能力。
- Method: 采用强化学习（RL）在简单街机游戏（如贪吃蛇）上对7B参数的MLLM进行后训练。
- Result: 模型在多模态数学基准（MathVista）和跨学科问题（MMMU）上表现优于专用模型，同时保持基础模型在通用视觉任务上的性能。
- Conclusion: 基于规则的游戏可作为可控且可扩展的预训练任务，解锁MLLMs的泛化多模态推理能力。


### [188] [StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets](https://arxiv.org/abs/2506.08013)
*Anh-Quan Cao,Ivan Lopes,Raoul de Charette*

Main category: cs.CV

TL;DR: StableMTL利用扩散模型在零样本设置下进行多任务学习，通过统一潜在损失和任务注意力机制提升性能。

- Motivation: 多任务学习需要大量标注数据，而部分任务标注限制了其应用。本文旨在通过扩散模型扩展至零样本设置。
- Method: 采用图像生成器进行潜在回归，结合任务编码、任务条件化和定制训练方案，引入多流模型和任务注意力机制。
- Result: 在8个基准测试的7个任务上优于基线方法。
- Conclusion: StableMTL通过统一损失和任务注意力机制，实现了高效的多任务学习。


### [189] [4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos](https://arxiv.org/abs/2506.08015)
*Zhen Xu,Zhengqin Li,Zhao Dong,Xiaowei Zhou,Richard Newcombe,Zhaoyang Lv*

Main category: cs.CV

TL;DR: 4DGT是一种基于4D高斯和Transformer的动态场景重建模型，通过单目视频训练，统一静态和动态组件，实现高效渲染。

- Motivation: 动态场景重建需要处理复杂的时间变化和对象生命周期，传统方法效率低且难以扩展。
- Method: 使用4D高斯作为归纳偏置，提出密度控制策略，滚动处理64帧输入，实现前馈推理。
- Result: 在真实视频中显著优于其他高斯网络，跨域视频中与优化方法精度相当，重建时间从小时级降至秒级。
- Conclusion: 4DGT在动态场景重建中高效且准确，适用于长视频序列。
## cs.IR

### [190] [HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval](https://arxiv.org/abs/2506.07296)
*Arian Askari,Emmanouil Stergiadis,Ilya Gusev,Moran Beladev*

Main category: cs.IR

TL;DR: HotelMatch-LLM是一种多模态密集检索模型，用于旅行领域的自然语言属性搜索，解决了传统旅行搜索引擎的局限性。

- Motivation: 传统旅行搜索引擎要求用户从目的地开始并编辑搜索参数，限制了用户体验。HotelMatch-LLM旨在通过自然语言搜索提升便利性。
- Method: 模型结合了多任务优化（检索、视觉和语言建模目标）、非对称密集检索架构（SLM处理查询，LLM嵌入酒店数据）以及图像处理技术。
- Result: 在四个测试集上，HotelMatch-LLM显著优于现有模型（如VISTA和MARVEL），在主查询类型测试集上达到0.681（MARVEL为0.603）。
- Conclusion: HotelMatch-LLM通过多任务优化、跨LLM架构的泛化能力和大规模图像处理能力，展示了其在旅行搜索领域的优越性和可扩展性。
## cs.GR

### [191] [Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation](https://arxiv.org/abs/2506.06440)
*Chuhao Chen,Zhiyang Dou,Chen Wang,Yiming Huang,Anjun Chen,Qiao Feng,Jiatao Gu,Lingjie Liu*

Main category: cs.GR

TL;DR: Vid2Sim是一种新颖的框架，通过基于线性混合蒙皮（LBS）的无网格简化模拟，从视频中高效恢复几何和物理属性。

- Motivation: 现有方法依赖复杂的优化流程和超参数调整，限制了实用性和泛化性。Vid2Sim旨在提供高效且通用的解决方案。
- Method: 使用前馈神经网络从视频中重建物理系统配置，并通过轻量级优化流程细化估计的外观、几何和物理属性。
- Result: 实验表明，Vid2Sim在重建几何和物理属性方面具有高精度和高效性。
- Conclusion: Vid2Sim提供了一种高效、通用的方法，显著提升了从视频中重建纹理形状和物理属性的能力。


### [192] [Splat and Replace: 3D Reconstruction with Repetitive Elements](https://arxiv.org/abs/2506.06462)
*Nicolás Violante,Andreas Meuleman,Alban Gauthier,Frédo Durand,Thibault Groueix,George Drettakis*

Main category: cs.GR

TL;DR: 利用3D场景中的重复元素改进新视角合成，通过分割、注册和共享信息提升几何和外观质量。

- Motivation: NeRF和3DGS在新视角合成中表现优异，但对未覆盖或遮挡部分的渲染质量较差。环境中常存在重复元素，利用这些重复可以提升低质量部分的重建效果。
- Method: 提出一种方法，分割3DGS重建中的重复实例，注册并共享信息，同时考虑实例间的外观变化。
- Result: 在合成和真实场景中验证，新视角合成质量显著提升。
- Conclusion: 通过利用重复元素，有效提升了新视角合成的几何和外观质量。


### [193] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: 提出两种一致性损失函数，解决Stable Diffusion微调中的欠拟合和过拟合问题，提升生成图像的多样性和主体保真度。

- Motivation: 现有微调方法在主体驱动图像合成中存在欠拟合（无法可靠捕捉主体身份）和过拟合（记忆主体图像并减少背景多样性）问题。
- Method: 提出两种辅助一致性损失：先验一致性正则化损失（保持非主体图像的扩散噪声与预训练模型一致）和主体一致性正则化损失（增强模型对噪声调制潜码的鲁棒性）。
- Result: 实验表明，该方法在CLIP分数、背景多样性和视觉质量上优于DreamBooth，同时保持主体身份。
- Conclusion: 通过引入一致性损失，有效平衡了主体保真度和图像多样性，提升了微调效果。


### [194] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 该论文提出了一种架构与算法协同设计方法，通过轴定向光栅化、神经排序和可重构处理阵列，显著提升了3D高斯泼溅（3DGS）在资源受限设备上的实时渲染效率和能耗表现。

- Motivation: 尽管3DGS在高质量视图合成中表现优异，但其在资源受限设备上的实时渲染仍面临功耗和面积限制的挑战。
- Method: 1. 轴定向光栅化以减少冗余计算；2. 神经排序替代硬件排序；3. 可重构处理阵列支持光栅化和神经网络推理；4. π轨迹瓦片调度优化内存访问。
- Result: 实验表明，设计在保持渲染质量的同时，速度提升23.4~27.8倍，能耗降低28.8~51.4倍。
- Conclusion: 该设计为资源受限设备上的高效3DGS渲染提供了可行方案，并计划开源以推动领域发展。


### [195] [HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance](https://arxiv.org/abs/2506.07209)
*Lei Li,Angela Dai*

Main category: cs.GR

TL;DR: HOI-PAGE提出了一种从文本提示零样本合成4D人-物交互（HOI）的新方法，通过部分级功能推理实现。

- Motivation: 现有方法主要关注全局的全身-物体运动，而生成真实多样的HOI需要更细粒度的理解，即人体部分如何与物体部分交互。
- Method: 引入部分功能图（PAGs），从大语言模型中提取结构化HOI表示，指导三阶段合成：分解3D物体、生成参考视频并提取运动约束、优化4D HOI序列。
- Result: 实验表明，该方法能灵活生成复杂多物体或多人的交互序列，显著提升零样本4D HOI的真实性和文本对齐性。
- Conclusion: HOI-PAGE通过细粒度部分级推理，在零样本4D HOI合成中表现出色。


### [196] [PIG: Physically-based Multi-Material Interaction with 3D Gaussians](https://arxiv.org/abs/2506.07657)
*Zeyu Xiao,Zhenyi Wu,Mingyang Sun,Qipeng Yan,Yufan Guo,Zhuoer Liang,Lihua Zhang*

Main category: cs.GR

TL;DR: PIG方法通过结合3D对象分割与高精度交互模拟，解决了3D高斯场景中的分割不准确、变形不精确和渲染伪影问题。

- Motivation: 解决3D高斯场景中对象交互的3D分割不准确、变形不精确和渲染伪影问题。
- Method: 1. 快速准确地将2D像素映射到3D高斯；2. 为分割对象分配物理属性以实现多材料交互；3. 在变形梯度中嵌入约束尺度以消除伪影。
- Result: 实验表明，PIG在视觉质量上优于现有技术，并为物理真实场景生成开辟了新方向。
- Conclusion: PIG方法在3D高斯场景中实现了高精度的对象交互和视觉一致性，推动了物理真实场景生成的发展。


### [197] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: 提出了一种轻量级生成模型，通过Hessian辅助采样策略提升3D高斯泼溅的分辨率和几何保真度，突破输入分辨率的限制。

- Motivation: 现有3DGS方法受限于输入分辨率，无法生成比训练视图更精细的细节。
- Method: 采用轻量级生成模型预测和细化额外的3D高斯分布，结合Hessian辅助采样策略智能选择需要密集化的区域。
- Result: 在几何精度和渲染质量上显著优于现有方法，且实时性高（单GPU上每次推理0.015秒）。
- Conclusion: 为分辨率无关的3D场景增强提供了新范式。


### [198] [Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes](https://arxiv.org/abs/2506.07917)
*Allen Tu,Haiyang Ying,Alex Hanson,Yonghan Lee,Tom Goldstein,Matthias Zwicker*

Main category: cs.GR

TL;DR: SpeeDe3DGS通过时间敏感剪枝和GroupFlow技术，显著提升动态3D高斯泼溅的渲染速度，减少模型大小和训练时间。

- Motivation: 动态3D高斯泼溅（3DGS）中，每帧对每个高斯进行神经推断导致渲染速度慢、内存和计算需求高，亟需优化。
- Method: 提出时间敏感剪枝分数和退火平滑剪枝机制，以及基于轨迹相似性的GroupFlow运动分析技术。
- Result: 在NeRF-DS数据集上，渲染速度提升10.37倍，模型大小减少7.71倍，训练时间缩短2.71倍。
- Conclusion: SpeeDe3DGS是一种模块化方法，可集成到任何可变形3DGS或4DGS框架中，显著提升效率。


### [199] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: Squeeze3D是一种新型框架，利用预训练的3D生成模型的隐式先验知识，实现极高压缩比的3D数据压缩。

- Motivation: 现有的3D数据压缩方法通常需要大量真实数据训练，且压缩比有限。Squeeze3D旨在通过利用预训练模型的隐式知识，实现更高压缩比且无需真实数据。
- Method: 通过可训练的映射网络，将预训练编码器的潜在空间与生成模型的潜在空间连接，将3D数据压缩为紧凑的潜在代码，再通过生成模型解压缩。
- Result: 实验显示，Squeeze3D对纹理网格、点云和辐射场的压缩比分别达到2187x、55x和619x，且视觉质量与现有方法相当。
- Conclusion: Squeeze3D提供了一种高效、灵活的3D数据压缩方法，无需真实数据训练，适用于多种3D格式。
## eess.SP

### [200] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: 该研究开发并评估了机器学习方法，用于通过多模态传感器数据早期预测社区居住的痴呆症患者的激越行为，并引入新的上下文特征以提高预测性能。

- Motivation: 激越行为是痴呆症患者常见的反应行为，早期预测可以减轻护理负担并提高生活质量。
- Method: 研究使用TIHM数据集，评估了多种机器学习和深度学习模型，包括二元分类和异常检测，并引入新的上下文特征。
- Result: 最佳模型在二元分类中达到AUC-ROC 0.9720和AUC-PR 0.4320，证明了方法的有效性。
- Conclusion: 该研究为基于隐私保护传感器数据的激越行为预测提供了首个全面基准，支持主动护理和居家养老。


### [201] [An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation](https://arxiv.org/abs/2506.06315)
*Masoud Rahimi,Reza Karbasi,Abdol-Hossein Vahabie*

Main category: eess.SP

TL;DR: 介绍了一个开源Python框架，用于生成合成ECG图像数据集，支持深度学习任务如ECG数字化、导联区域和名称检测以及波形分割。

- Motivation: 推动ECG分析中的深度学习任务，提供高质量的开源数据集和工具。
- Method: 基于PTB-XL信号数据集，生成四种开放数据集，包括ECG图像与时间序列信号配对、YOLO格式标注的导联区域和名称检测数据、单导联分割掩码图像。
- Result: 生成了四种公开可用的数据集，支持多种ECG分析任务。
- Conclusion: 开源框架和数据集为ECG分析研究提供了重要资源。


### [202] [Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning](https://arxiv.org/abs/2506.06349)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: eess.SP

TL;DR: 研究比较了传统机器学习和深度学习方法对ECG信号分类的效果，发现基于手工特征的LightGBM模型表现最佳，准确率达99%，优于基于图像的CNN方法。

- Motivation: 探讨ECG信号分类的两种方法（传统机器学习与深度学习）的性能差异，以优化心跳分类的准确性。
- Method: 1. 传统机器学习：提取HRV、均值、方差等特征，训练SVM、随机森林等分类器；2. 深度学习：将ECG信号转换为图像（GAF、MTF、RP），用CNN分类。
- Result: LightGBM模型表现最优（准确率99%，F1分数0.94），优于CNN方法（F1分数0.85）。
- Conclusion: 手工特征能更好地捕捉ECG信号的时空变化，未来可结合多导联信号和时序依赖提升分类效果。
## cs.MA

### [203] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: 提出了一种名为MedChat的多智能体诊断框架，结合专业视觉模型和角色特定的LLM智能体，以解决通用LLM在医学影像应用中的幻觉和解释性问题。

- Motivation: 解决通用LLM在医学影像领域中的局限性，如幻觉、解释性不足和缺乏领域知识，同时模拟多学科医疗团队的复杂推理。
- Method: 设计了一个多智能体框架，包括专业视觉模型和多个角色特定的LLM智能体，由导演智能体协调，以提升可靠性和交互性。
- Result: MedChat提高了诊断报告的可靠性，减少了幻觉风险，并提供了适合临床和教育用途的交互界面。
- Conclusion: MedChat通过多智能体协作，有效解决了通用LLM在医学影像应用中的问题，为临床和教育提供了实用工具。
## eess.IV

### [204] [ResPF: Residual Poisson Flow for Efficient and Physically Consistent Sparse-View CT Reconstruction](https://arxiv.org/abs/2506.06400)
*Changsheng Fang,Yongtong Liu,Bahareh Morovati,Shuo Han,Yu Shi,Li Zhou,Shuyi Fan,Hengyong Yu*

Main category: eess.IV

TL;DR: 提出了一种基于Poisson Flow生成模型（ResPF）的高效稀疏视图CT重建方法，通过条件引导和残差融合模块提升重建质量与速度。

- Motivation: 稀疏视图CT可减少辐射剂量，但重建问题复杂，现有深度学习和扩散模型缺乏物理可解释性或计算成本高。
- Method: 基于PFGM++，引入条件引导和残差融合模块，跳过冗余初始步骤并嵌入数据一致性约束。
- Result: 在合成和临床数据集上，ResPF在重建质量、推理速度和鲁棒性上优于现有方法。
- Conclusion: ResPF首次将Poisson Flow模型应用于稀疏视图CT，显著提升了重建性能。


### [205] [SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation](https://arxiv.org/abs/2506.06890)
*Sumit Sharma,Gopi Raju Matta,Kaushik Mitra*

Main category: eess.IV

TL;DR: 提出了一种两阶段框架，将二进制SPC图像转换为高质量彩色新视图，解决了传统3D合成技术因信息丢失而失效的问题。

- Motivation: SPAD和SPC技术能高精度检测光子，但二进制SPC图像导致纹理和颜色信息丢失，传统3D合成技术难以处理。
- Method: 第一阶段用Pix2PixHD进行图像到图像转换，第二阶段用NeRF或3DGS进行3D场景重建。
- Result: 实验验证了框架在感知质量和几何一致性上的显著提升。
- Conclusion: 两阶段框架有效解决了SPC图像的信息丢失问题，为3D重建提供了新思路。


### [206] [Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images](https://arxiv.org/abs/2506.07023)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 提出了一种新的深度生成模型，用于从组织学图像中分割细胞核结构，解决了信息不对称问题，并通过可逆生成器和空间约束优化性能。

- Motivation: 组织学图像中细胞核区域的分割有助于疾病检测和诊断，但现有图像到图像转换模型在信息不对称时表现不佳。
- Method: 引入嵌入空间处理信息不对称，结合最优传输和测度理论设计可逆生成器，避免显式循环一致性损失，并加入空间约束挤压操作。
- Result: 模型在公开数据集上表现优于现有方法，实现了网络复杂度和性能的更好平衡。
- Conclusion: 提出的模型有效解决了信息不对称问题，为细胞核分割提供了高效且性能优越的解决方案。


### [207] [SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images](https://arxiv.org/abs/2506.07028)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 论文提出了一种新型深度生成模型，同时实现组织学图像中细胞核结构的精确分割和颜色外观的归一化。

- Motivation: 解决组织学图像中因颜色变化导致的细胞核分割困难问题，并提升颜色归一化的效果。
- Method: 结合截断正态分布和空间注意力的深度生成模型，假设颜色外观信息与细胞核分割图及嵌入信息独立。
- Result: 模型在公开数据集上表现优异，优于现有先进算法。
- Conclusion: 该模型具有通用性和适应性，颜色外观信息的修改或丢失不影响细胞核分割结果。


### [208] [Transfer Learning and Explainable AI for Brain Tumor Classification: A Study Using MRI Data from Bangladesh](https://arxiv.org/abs/2506.07228)
*Shuvashis Sarker*

Main category: eess.IV

TL;DR: 研究开发了一种基于深度学习的自动脑肿瘤分类系统，结合可解释AI技术，显著提高了分类准确性和临床适用性。

- Motivation: 脑肿瘤的及时诊断对患者预后至关重要，尤其是在医疗资源有限的地区如孟加拉国。手动MRI分析效率低且易出错，需自动化解决方案。
- Method: 使用VGG16、VGG19和ResNet50等深度学习模型分类脑肿瘤，并结合Grad-CAM和Grad-CAM++等XAI技术提升模型可解释性。
- Result: VGG16模型表现最佳，准确率达99.17%。XAI技术增强了系统的透明度和稳定性。
- Conclusion: 深度学习与XAI结合可有效提升脑肿瘤检测，适用于医疗资源有限的地区。


### [209] [A Comprehensive Analysis of COVID-19 Detection Using Bangladeshi Data and Explainable AI](https://arxiv.org/abs/2506.07234)
*Shuvashis Sarker*

Main category: eess.IV

TL;DR: 研究利用深度学习模型（VGG19）在CXR图像中检测COVID-19，准确率达98%，并通过XAI技术提高模型透明度和可靠性。

- Motivation: COVID-19全球大流行对孟加拉国造成严重影响，亟需高效检测方法。
- Method: 使用4,350张CXR图像数据集，应用ML、DL和TL模型（如VGG19），结合LIME和SMOTE技术。
- Result: VGG19模型达到98%准确率，LIME解释模型决策，SMOTE解决类别不平衡问题。
- Conclusion: XAI技术提升了模型透明度和可靠性，有助于改进CXR图像中的COVID-19检测。


### [210] [A Narrative Review on Large AI Models in Lung Cancer Screening, Diagnosis, and Treatment Planning](https://arxiv.org/abs/2506.07236)
*Jiachen Zhong,Yiting Wang,Di Zhu,Ziwei Wang*

Main category: eess.IV

TL;DR: 本文综述了大型AI模型在肺癌筛查、诊断、预后和治疗中的应用，总结了现有模型的分类、性能及临床潜力，并指出了当前局限性和未来方向。

- Motivation: 肺癌是全球高发且致命的疾病，亟需准确及时的诊断和治疗。大型AI模型的进步为医学图像理解和临床决策提供了新工具。
- Method: 系统调查了大型AI模型在肺癌领域的应用，分类为模态特定编码器、编码器-解码器框架和联合编码器架构，并评估了其在多模态学习任务中的表现。
- Result: 模型在肺结节检测、基因突变预测、多组学整合和个性化治疗规划中表现出色，部分已进入临床验证阶段。
- Conclusion: 大型AI模型在肺癌诊疗中具有变革潜力，但需解决泛化性、可解释性和合规性等挑战，未来应构建可扩展、可解释且临床集成的AI系统。


### [211] [Text-guided multi-stage cross-perception network for medical image segmentation](https://arxiv.org/abs/2506.07475)
*Gaoyu Chen*

Main category: eess.IV

TL;DR: 提出了一种基于文本提示的多阶段跨感知网络（TMC），用于解决医学图像分割中语义表达不足的问题，显著提升了分割性能。

- Motivation: 医学图像分割在临床中至关重要，但现有方法因目标与非目标区域对比度低导致语义表达弱。文本提示信息有潜力解决这一问题，但现有方法跨模态交互不足。
- Method: 提出TMC网络，引入多阶段交叉注意力模块增强语义细节理解，并使用多阶段对齐损失提升跨模态语义一致性。
- Result: 在三个公开数据集（QaTa-COV19、MosMedData和Breast）上，TMC的Dice分数分别为84.77%、78.50%和88.73%，优于UNet和现有文本引导方法。
- Conclusion: TMC通过多阶段跨模态交互显著提升了医学图像分割性能，为临床辅助诊断提供了更有效的工具。


### [212] [Fine-Grained Motion Compression and Selective Temporal Fusion for Neural B-Frame Video Coding](https://arxiv.org/abs/2506.07709)
*Xihua Sheng,Peilin Chen,Meng Wang,Li Zhang,Shiqi Wang,Dapeng Oliver Wu*

Main category: eess.IV

TL;DR: 论文提出了一种针对神经B帧编码的改进方法，包括精细运动压缩和选择性时间融合，显著提升了压缩性能。

- Motivation: 现有神经B帧编解码器直接采用P帧工具，未能解决B帧压缩的独特挑战，导致性能不佳。
- Method: 设计了精细运动压缩方法（交互式双分支运动自编码器）和选择性时间融合方法（预测双向融合权重）。
- Result: 实验表明，该方法优于现有神经B帧编解码器，性能接近H.266/VVC参考软件。
- Conclusion: 提出的方法有效解决了B帧压缩的挑战，提升了性能。
## cs.AI

### [213] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: 论文提出了一种名为Contextual Experience Replay (CER)的训练无关框架，通过动态记忆缓冲区积累和综合过去经验，提升语言模型代理在复杂任务中的适应能力。

- Motivation: 当前大型语言模型代理在复杂任务（如网页导航）中表现不佳，主要原因是缺乏环境特定经验且无法在推理时持续学习。
- Method: CER框架通过动态记忆缓冲区积累环境动态和决策模式经验，使代理能够在新任务中检索并增强相关知识。
- Result: 在VisualWebArena和WebArena基准测试中，CER分别达到31.9%和36.7%的成功率，相对GPT-4o基线提升51.0%。
- Conclusion: CER通过经验回放显著提升了语言模型代理的适应性和性能，证明了其高效性和有效性。


### [214] [VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs](https://arxiv.org/abs/2506.06727)
*Can Li,Ting Zhang,Mei Wang,Hua Huang*

Main category: cs.AI

TL;DR: VisioMath是一个新基准，用于评估多模态模型在图像选项数学推理中的表现，发现现有模型在此任务上表现不佳。

- Motivation: 填补多模态模型在图像选项数学推理能力评估上的空白。
- Method: 引入VisioMath数据集，包含8,070张图像和1,800道多选题，每题的选项为图像。
- Result: 现有最先进模型（如GPT-4o）在此任务上准确率仅为45.9%。
- Conclusion: VisioMath为多模态推理研究提供了重要测试平台，推动未来技术进步。


### [215] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 提出一种元学习方法，通过任务相关的软提示和注意力映射模块，提升小型LMM在少样本任务中的性能。

- Motivation: 大型多模态模型（LMM）在上下文学习（ICL）中表现不稳定，尤其是小型LMM，额外图像信息可能干扰任务表现。
- Method: 采用元学习方法，从任务相关图像特征中提取软提示，并结合注意力映射模块，实现少样本适应。
- Result: 在VL-ICL Bench上，该方法优于ICL和其他提示调优方法，尤其在图像扰动下表现更优。
- Conclusion: 该方法有效提升了LMM在少样本任务中的适应性和推理能力。


### [216] [Long-Tailed Learning for Generalized Category Discovery](https://arxiv.org/abs/2506.06965)
*Cuong Manh Hoang*

Main category: cs.AI

TL;DR: 提出了一种在长尾分布中进行广义类别发现的新框架，通过自引导标记和表示平衡技术提升性能。

- Motivation: 解决现有方法在真实世界不平衡数据集上效果不佳的问题。
- Method: 采用自引导标记技术生成伪标签，并通过表示平衡过程挖掘样本邻域以关注尾部类别。
- Result: 在公开数据集上表现优于现有最优方法。
- Conclusion: 新框架有效解决了长尾分布下的广义类别发现问题。


### [217] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 论文提出SHE框架，通过两阶段方法检测和缓解行为幻觉，并引入新指标BEACH量化其严重性。

- Motivation: 多模态大语言模型存在行为幻觉问题，现有研究主要关注客观幻觉，而行为幻觉研究较少。
- Method: SHE框架通过自适应时间窗口检测幻觉，并通过正交投影缓解幻觉。
- Result: SHE在BEACH指标上减少行为幻觉超过10%，同时保持描述准确性。
- Conclusion: SHE有效填补了行为幻觉研究的空白，提升了模型可靠性。


### [218] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: 提出了一种自监督双奖励机制，通过理解与生成的逆对偶任务提升多模态模型的性能，无需外部监督。

- Motivation: 当前多模态模型在图像-文本对齐上表现不佳，且现有方法依赖外部监督，仅解决单向任务。
- Method: 引入自监督双奖励机制，通过逆对偶任务计算模型的双重似然作为自我奖励进行优化。
- Result: 实验表明，该方法显著提升了模型在视觉理解和生成任务上的性能，尤其在文本到图像任务中表现突出。
- Conclusion: 自监督双奖励机制有效提升了多模态模型的理解与生成能力，无需外部监督。


### [219] [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior](https://arxiv.org/abs/2506.08012)
*Penghao Wu,Shengnan Ma,Bo Wang,Jiaheng Yu,Lewei Lu,Ziwei Liu*

Main category: cs.AI

TL;DR: GUI-Reflection框架通过自我反思和错误纠正能力增强多模态GUI模型，无需人工标注，实现更强大的GUI自动化。

- Motivation: 现有GUI模型依赖无错误的离线轨迹，缺乏反思和错误恢复能力，限制了其实际应用。
- Method: 提出GUI-Reflection框架，包括GUI特定预训练、离线监督微调和在线反思调优三个阶段，自动生成反思数据并设计任务套件。
- Result: 框架赋予GUI代理自我反思和纠正能力，提升了模型的鲁棒性和适应性。
- Conclusion: GUI-Reflection为更智能的GUI自动化铺平了道路，相关数据和工具将公开。
## cs.AR

### [220] [QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine](https://arxiv.org/abs/2506.07046)
*Anushka Jha,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: QForce-RL利用量化和轻量级架构提升FPGA上的RL性能，减少资源消耗。

- Motivation: FPGA部署RL资源消耗大，QForce-RL旨在通过量化提升吞吐量和能效。
- Method: 结合E2HRL减少动作空间和QuaRL的量化SIMD加速硬件。
- Result: 性能提升2.3倍，FPS提升2.6倍。
- Conclusion: QForce-RL适用于资源受限设备，提供高效部署方案。
## cs.RO

### [221] [Active Illumination Control in Low-Light Environments using NightHawk](https://arxiv.org/abs/2506.06394)
*Yash Turkar,Youngjin Kim,Karthik Dantu*

Main category: cs.RO

TL;DR: NightHawk框架通过结合主动照明和曝光控制，优化了地下环境中机器人视觉的图像质量，显著提升了特征检测和匹配性能。

- Motivation: 地下环境（如涵洞）光线昏暗且缺乏显著特征，给机器人视觉带来挑战。现有照明方法存在反射、过曝和功耗问题。
- Method: 提出NightHawk框架，通过在线贝叶斯优化动态调整光照强度和曝光时间，并基于特征检测器设计新的图像效用度量作为优化目标。
- Result: 实验表明，特征检测和匹配性能提升47-197%，在恶劣光照条件下实现了更可靠的视觉估计。
- Conclusion: NightHawk有效解决了地下环境中的视觉问题，为机器人导航提供了实用解决方案。


### [222] [Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception](https://arxiv.org/abs/2506.06474)
*Everett Richards,Bipul Thapa,Lena Mashayekhy*

Main category: cs.RO

TL;DR: 论文提出了一种基于边缘计算和多车协作的实时物体检测框架ECOD，通过PACE和VOTE算法提升CAV的感知能力，实验显示其分类准确率比传统方法高75%。

- Motivation: 传统车载感知系统因遮挡和盲区精度有限，云端解决方案延迟高，无法满足实时需求。
- Method: ECOD框架结合PACE（感知聚合与协作估计）和VOTE（可变物体统计与评估）算法，利用边缘计算和多车协作实现实时检测。
- Result: 实验表明ECOD在物体分类准确率上比传统方法提升75%，且延迟低。
- Conclusion: 边缘计算可显著提升协作感知能力，适用于延迟敏感的自动驾驶系统。


### [223] [DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning](https://arxiv.org/abs/2506.06659)
*Wenhao Yao,Zhenxin Li,Shiyi Lan,Zi Wang,Xinglong Sun,Jose M. Alvarez,Zuxuan Wu*

Main category: cs.RO

TL;DR: DriveSuprim通过渐进候选过滤、旋转增强和自蒸馏框架，提升自动驾驶轨迹选择的安全性和性能，在NAVSIM数据集上表现优异。

- Motivation: 解决基于选择的轨迹预测方法在优化和区分安全关键差异上的挑战，提升自动驾驶在复杂环境中的安全性。
- Method: 采用粗到细的渐进候选过滤、旋转增强方法和自蒸馏框架，优化轨迹选择和训练稳定性。
- Result: 在NAVSIM v1和v2上分别达到93.5% PDMS和87.1% EPDMS，表现优于现有方法。
- Conclusion: DriveSuprim显著提升了自动驾驶轨迹选择的安全性和性能，适用于多样化驾驶场景。


### [224] [Generalized Trajectory Scoring for End-to-end Multimodal Planning](https://arxiv.org/abs/2506.06664)
*Zhenxin Li,Wenhao Yao,Zi Wang,Xinglong Sun,Joshua Chen,Nadine Chang,Maying Shen,Zuxuan Wu,Shiyi Lan,Jose M. Alvarez*

Main category: cs.RO

TL;DR: GTRS提出了一种统一的端到端多模态规划框架，结合粗粒度和细粒度轨迹评估，解决了静态和动态轨迹评分方法的局限性。

- Motivation: 现有轨迹评分方法在泛化性上存在显著不足，静态方法无法适应细粒度变化，动态方法难以捕捉广泛轨迹分布。
- Method: GTRS包含三个创新：扩散式轨迹生成器、词汇泛化技术和传感器增强策略。
- Result: GTRS在Navsim v2挑战赛中表现优异，接近依赖真实感知的特权方法。
- Conclusion: GTRS通过结合粗粒度和细粒度评估，显著提升了轨迹评分的泛化性和性能。


### [225] [RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation](https://arxiv.org/abs/2506.06677)
*Songhao Han,Boxiang Qiu,Yue Liao,Siyuan Huang,Chen Gao,Shuicheng Yan,Si Liu*

Main category: cs.RO

TL;DR: RoboCerebra是一个用于评估机器人长期操作中高级推理能力的基准，结合了高层次VLM规划器和低层次VLA控制器，并提供了大规模仿真数据集。

- Motivation: 现有工作多关注反应性策略，未充分利用VLM的语义推理和长期规划能力，因此需要新的基准来探索这些能力。
- Method: 通过GPT生成任务指令并分解为子任务序列，人类操作员在仿真中执行，构建高质量轨迹数据集。结合VLM规划器和VLA控制器。
- Result: RoboCerebra数据集具有更长的动作序列和更密集的标注，显著优于现有基准。
- Conclusion: RoboCerebra推动了更通用和高效的机器人规划器的发展，并分析了VLM在关键认知维度的表现。


### [226] [SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game](https://arxiv.org/abs/2506.06690)
*Hao Wang,Chengkai Hou,Xianglong Li,Yankai Fu,Chenxuan Li,Ning Chen,Gaole Dai,Jiaming Liu,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: SpikePingpong系统结合尖峰视觉与模仿学习，通过SONIC和IMPACT模块解决乒乓球机器人高精度控制问题，实验表现优于现有方法。

- Motivation: 研究高速物体控制的挑战，乒乓球作为理想测试平台，需高精度视觉和智能策略规划。
- Method: 结合20 kHz尖峰相机和神经网络模型，SONIC模块补偿现实不确定性，IMPACT模块实现精准落点。
- Result: 在30 cm和20 cm精度任务中，成功率分别达91%和71%，超越现有方法38%和37%。
- Conclusion: SpikePingpong为高速动态任务中的机器人控制提供了新视角。


### [227] [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)
*Chenguang Huang,Oier Mees,Andy Zeng,Wolfram Burgard*

Main category: cs.RO

TL;DR: 提出多模态空间语言地图（VLMaps和AVLMaps），结合预训练多模态特征与3D环境重建，实现自然语言命令到空间目标的翻译，并支持多模态目标导航。

- Motivation: 现有方法未能充分利用环境地图的几何精度或多模态信息，限制了导航代理的感知能力。
- Method: 构建视觉-语言地图（VLMaps）及其扩展音频-视觉-语言地图（AVLMaps），融合预训练多模态特征与3D环境重建。
- Result: 实验表明，该方法支持零样本空间和多模态目标导航，在模糊场景中召回率提升50%。
- Conclusion: 多模态空间语言地图显著提升了导航代理的感知和交互能力，适用于多种机器人平台。


### [228] [MapBERT: Bitwise Masked Modeling for Real-Time Semantic Mapping Generation](https://arxiv.org/abs/2506.07350)
*Yijie Deng,Shuaihang Yuan,Congcong Wen,Hao Huang,Anthony Tzes,Geeta Chandra Raju Bethala,Yi Fang*

Main category: cs.RO

TL;DR: MapBERT是一种新框架，利用BitVAE和掩码变换器生成未观察区域的语义地图，通过对象感知掩码策略提升推理能力，在Gibson基准测试中表现优异。

- Motivation: 解决现有方法在实时生成未观察区域和泛化到新环境方面的不足，学习室内语义分布的复杂性。
- Method: 采用BitVAE编码语义地图为紧凑比特令牌，结合掩码变换器推断缺失区域，并提出对象感知掩码策略增强推理。
- Result: 在Gibson基准测试中达到最先进的语义地图生成效果，平衡计算效率和未观察区域的准确重建。
- Conclusion: MapBERT通过创新的比特编码和对象感知策略，有效解决了室内语义分布建模的挑战。


### [229] [BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation](https://arxiv.org/abs/2506.07530)
*Hongyu Wang,Chuyan Xiong,Ruiping Wang,Xilin Chen*

Main category: cs.RO

TL;DR: BitVLA是首个用于机器人操作的1位VLA模型，通过三元参数和1.58位视觉编码器压缩技术，显著减少内存占用，性能接近4位量化模型。

- Motivation: 解决VLA模型在资源受限机器人系统上部署的挑战，探索1位预训练在VLA模型中的应用。
- Method: 提出BitVLA模型，采用三元参数和蒸馏感知训练策略压缩视觉编码器，利用全精度教师模型对齐潜在表示。
- Result: 在LIBERO基准测试中，BitVLA性能接近4位量化的OpenVLA-OFT，内存占用仅为29.8%。
- Conclusion: BitVLA展示了在内存受限边缘设备上部署的潜力，代码和模型已开源。
## physics.ed-ph

### [230] [Pendulum Tracker -- SimuFísica: A Web-based Tool for Real-time Measurement of Oscillatory Motion](https://arxiv.org/abs/2506.07301)
*Marco P. M. de Souza,Juciane G. Maia,Lilian N. de Andrade*

Main category: physics.ed-ph

TL;DR: Pendulum Tracker是一个基于计算机视觉的应用程序，用于实时测量物理摆的振荡运动，适用于教育平台SimuFísica，支持多种设备。

- Motivation: 为物理教学提供一个实时、准确且易于使用的工具，用于测量和分析摆的运动。
- Method: 利用OpenCV.js库和浏览器内运行的技术，通过设备摄像头自动检测摆的位置，实时显示角度-时间图和周期估计。
- Result: 实验结果表明，系统在测量周期、重力加速度和分析阻尼振荡方面与理论预测高度一致。
- Conclusion: Pendulum Tracker是一个适用于实验物理教学的实用工具，具有高准确性和用户友好性。
## cs.CY

### [231] [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
*Lingyao Li,Dawei Li,Zhenhui Ou,Xiaoran Xu,Jingxiao Liu,Zihui Ma,Runlong Yu,Min Deng*

Main category: cs.CY

TL;DR: 该研究利用大语言模型（LLMs）模拟地震影响，结合多模态数据生成MMI预测，结果显示与真实数据高度一致。

- Motivation: 提升突发灾害（如地震）的主动应对能力，探索LLMs在复杂场景模拟中的潜力。
- Method: 使用多模态数据集（地理空间、社会经济、建筑和街景图像），结合RAG和ICL技术，生成MMI预测。
- Result: 在2014年Napa和2019年Ridgecrest地震中，预测与USGS报告高度相关（0.88），RMSE低至0.77。
- Conclusion: LLMs在灾害影响模拟中具有潜力，可支持灾前规划。
## cs.LG

### [232] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: CellCLIP是一种用于高内涵筛选数据的跨模态对比学习框架，通过预训练图像编码器和新型通道编码方案，显著提升了性能并减少了计算时间。

- Motivation: 利用高内涵筛选数据理解扰动与细胞形态效应的关系，但现有方法因图像语义差异和扰动类别多样性而难以直接应用。
- Method: 结合预训练图像编码器、新型通道编码方案和自然语言编码器，构建跨模态对比学习框架CellCLIP。
- Result: CellCLIP在跨模态检索和下游生物学任务中表现最佳，同时显著减少计算时间。
- Conclusion: CellCLIP为高内涵筛选数据提供了一种高效且性能优越的跨模态学习方法。


### [233] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: NeurNCD是一种新颖的开放世界类别发现框架，通过结合Embedding-NeRF模型和KL散度，替代传统显式3D分割图，提升语义嵌入和视觉嵌入空间的信息聚合能力。

- Motivation: 传统显式表示（如对象描述符或3D分割图）存在离散、易产生空洞和噪声的问题，限制了新类别的准确发现。
- Method: 采用Embedding-NeRF模型和KL散度，结合特征查询、调制和聚类等关键组件，实现高效特征增强和信息交换。
- Result: 在NYUv2和Replica数据集上显著优于现有方法，无需密集标注数据或人工干预。
- Conclusion: NeurNCD为开放世界中的新类别发现提供了一种高效、数据友好的解决方案。


### [234] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: 论文提出了一种混合量子-经典架构Vision-QRWKV，用于图像分类任务，通过引入变分量子电路提升性能。

- Motivation: 探索量子机器学习在视觉任务中的潜力，特别是在处理复杂高维数据时提升经典神经网络的表现。
- Method: 在RWKV架构的通道混合组件中集成变分量子电路（VQC），以增强非线性特征变换和视觉表示的表达能力。
- Result: 量子增强模型在多数数据集（尤其是具有细微或噪声类别区分的数据集）上优于经典模型。
- Conclusion: 该研究首次系统地将量子增强RWKV应用于视觉领域，为轻量高效视觉任务的量子模型提供了未来潜力。


### [235] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: 论文提出一种结合图像负载特征和持续学习的非侵入式负载监测方法，显著提高了识别精度。

- Motivation: 传统NILM方法因负载组合复杂多变导致特征鲁棒性差和模型泛化能力不足。
- Method: 将多维电力信号转换为图像负载特征，结合深度卷积神经网络和持续学习策略。
- Result: 在高采样率负载数据集上实验表明，该方法在识别精度上有显著提升。
- Conclusion: 该方法通过图像特征和持续学习有效解决了传统NILM的局限性。


### [236] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: 本文提出了一种通过模型编辑技术提升低资源语言识别能力的方法，优于传统元学习，显著提升了跨域和跨字母表的性能。

- Motivation: 解决低资源语言（如古代手稿和非西方语言）在识别系统中代表性不足的问题，提升模型的泛化能力。
- Method: 利用模型编辑技术增强对未见脚本的适应性，提出域合并方法，无需依赖整体数据分布或原型需求。
- Result: 实验显示，在相同训练数据下，新方法在跨字母表迁移学习和域外评估中表现显著提升。
- Conclusion: 该方法为低资源字母表的识别提供了新思路，扩展了文档识别的应用范围。


### [237] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 论文提出FIND方法，通过特征实例邻居发现解决动态多测试分布下的性能下降问题，显著提升准确性。

- Motivation: 深度神经网络在训练与测试域分布偏移时性能下降，现有TTA方法难以应对动态多测试分布。
- Method: FIND包含三个组件：LFD（层间特征解耦）、FABN（特征感知批量归一化）和S-FABN（选择性FABN），分别处理特征聚类、统计优化和效率提升。
- Result: 实验表明FIND在动态场景中准确率提升30%，同时保持计算效率。
- Conclusion: FIND通过特征聚类和动态归一化策略，有效解决了分布偏移问题，显著优于现有方法。


### [238] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 论文提出了一种名为FREE的方法，通过对抗训练和GAN框架在视觉语言模型中实现早期退出策略，以提升推理速度并保持性能。

- Motivation: 视觉语言模型（VLMs）的大规模导致推理延迟问题，限制了实际应用。
- Method: 采用对抗训练方法（FREE），每个退出点包含一个Transformer层和分类器，通过GAN框架训练以生成类似最终层的特征表示。
- Result: 实验表明，该方法在保持性能的同时，推理速度提升超过1.51倍，并增强了模型鲁棒性。
- Conclusion: FREE方法有效解决了VLMs的推理延迟问题，同时保持了模型性能。


### [239] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 本文提出了一种针对图像分类器的非对称黑盒攻击框架，通过改进搜索策略和梯度估计过程，显著降低了攻击的总查询成本。

- Motivation: 现有黑盒攻击方法假设所有查询成本相同，而实际应用中某些查询可能成本更高（如触发额外审查）。本文旨在解决非对称查询成本下的攻击效率问题。
- Method: 提出了非对称搜索（AS）和非对称梯度估计（AGREST），通过减少高成本查询的依赖和调整采样分布，优化攻击策略。
- Result: 实验表明，该方法在多种成本设置下，总查询成本和扰动幅度均优于现有方法，最高提升40%。
- Conclusion: 本文提出的非对称黑盒攻击框架有效解决了实际场景中的查询成本不对称问题，为黑盒攻击提供了更高效的解决方案。


### [240] [Towards Physics-informed Diffusion for Anomaly Detection in Trajectories](https://arxiv.org/abs/2506.06999)
*Arun Sharma,Mingzhou Yang,Majid Farhadloo,Subhankar Ghosh,Bharat Jayaprakash,Shashi Shekhar*

Main category: cs.LG

TL;DR: 提出了一种物理信息扩散模型，用于检测异常轨迹，尤其是在GPS欺骗场景中，结合运动学约束以提高准确性。

- Motivation: 解决国际水域非法活动（如非法捕鱼和石油走私）中的GPS欺骗问题，同时应对AI生成虚假轨迹和数据稀疏的挑战。
- Method: 提出了一种物理信息扩散模型，整合运动学约束以识别不符合物理规律的轨迹。
- Result: 在真实数据集（海事和城市领域）上实验显示，该方法在异常检测和轨迹生成方面具有更高的准确性和更低的误差率。
- Conclusion: 该方法有效解决了虚假轨迹检测问题，尤其在数据稀疏和复杂依赖场景中表现优异。


### [241] [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)
*Tong Xiao,Xin Xu,Zhenya Huang,Hongyu Gao,Quan Liu,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 论文提出Perception-R1方法，通过引入视觉感知奖励增强多模态大语言模型的感知与推理能力。

- Motivation: 现有RLVR方法未能有效提升多模态感知能力，限制了推理能力的进一步提升。
- Method: 提出Perception-R1，利用视觉注释作为奖励依据，通过LLM评估一致性并分配奖励。
- Result: 在多个多模态推理基准测试中取得最优性能，仅需1,442训练数据。
- Conclusion: Perception-R1有效提升了多模态感知与推理能力，验证了其高效性。


### [242] [Variational Supervised Contrastive Learning](https://arxiv.org/abs/2506.07413)
*Ziwen Wang,Jiajun Fan,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: VarCon通过变分推理改进对比学习，解决了嵌入分布无明确调控和过度依赖负样本的问题，在多个数据集上达到SOTA性能。

- Motivation: 解决对比学习中嵌入分布无明确调控和过度依赖负样本的问题。
- Method: 提出VarCon，将监督对比学习重新表述为对潜在类别变量的变分推理，最大化后验加权的ELBO，实现高效类感知匹配。
- Result: 在CIFAR-10、CIFAR-100、ImageNet等数据集上达到SOTA性能，Top-1准确率79.36%（ImageNet-1K），并展示清晰的决策边界和语义组织。
- Conclusion: VarCon在性能、泛化能力和鲁棒性上均优于基线方法。


### [243] [Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning](https://arxiv.org/abs/2506.07735)
*Haizhao Jing,Haokui Zhang,Zhenhao Shang,Rong Xiao,Peng Wang,Yanning Zhang*

Main category: cs.LG

TL;DR: LeDG-Former提出了一种结合语言嵌入和动态图表示学习的新框架，解决了现有方法忽略硬件属性和静态拓扑结构的局限性，实现了跨硬件平台的零样本预测和更优的神经网络建模性能。

- Motivation: 当前方法在神经网络架构表示学习中忽略了硬件属性信息，且依赖静态邻接矩阵表示拓扑结构，限制了模型的实用性和编码效果。
- Method: LeDG-Former通过语言嵌入框架将神经网络架构和硬件平台规范投影到统一语义空间，并采用动态图变换器建模神经网络架构。
- Result: 在NNLQP基准测试中，LeDG-Former超越现有方法，实现了跨硬件延迟预测，并在NAS-Bench-101和NAS-Bench-201数据集上表现优异。
- Conclusion: LeDG-Former通过结合语言嵌入和动态图表示学习，显著提升了神经网络架构表示学习的性能和应用范围。


### [244] [Identifiable Object Representations under Spatial Ambiguities](https://arxiv.org/abs/2506.07806)
*Avinash Kori,Francesca Toni,Ben Glocker*

Main category: cs.LG

TL;DR: 提出一种多视角概率方法，解决空间模糊性问题，无需视角标注，实验验证其鲁棒性和扩展性。

- Motivation: 模块化物体中心表示对人类推理至关重要，但在空间模糊性（如遮挡和视角模糊）下难以获取。
- Method: 引入多视角概率方法，聚合视角特定槽以捕捉不变内容信息，同时学习解耦的全局视角信息。
- Result: 解决了空间模糊性，提供可识别性理论保证，无需视角标注，实验验证其鲁棒性和扩展性。
- Conclusion: 该方法在复杂数据集上表现优异，为模块化表示提供了新思路。


### [245] [Diffusion Counterfactual Generation with Semantic Abduction](https://arxiv.org/abs/2506.07883)
*Rajat Rasal,Avinash Kori,Fabio De Sousa Ribeiro,Tian Xia,Ben Glocker*

Main category: cs.LG

TL;DR: 该论文提出了一种基于扩散模型的因果机制框架，用于反事实图像生成，解决了身份保留、感知质量和因果忠实性等问题。

- Motivation: 现有自编码框架在可扩展性和保真度方面存在不足，而扩散模型在视觉质量和感知对齐方面表现出色，因此探索如何利用扩散模型改进反事实图像编辑。
- Method: 提出了一套扩散模型的因果机制，包括空间、语义和动态反事实推理，并结合Pearl因果理论将语义表示集成到扩散模型中。
- Result: 首次实现了扩散模型中高级语义身份保留，展示了如何在忠实因果控制和身份保留之间进行权衡。
- Conclusion: 该框架为反事实图像生成提供了一种新方法，结合了扩散模型的优势与因果推理的严谨性。


### [246] [Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/abs/2506.07903)
*Kevin Rojas,Yuchen Zhu,Sichen Zhu,Felix X. -F. Ye,Molei Tao*

Main category: cs.LG

TL;DR: 提出了一种新的多模态扩散模型框架，支持原生生成跨模态的耦合数据，无需依赖外部预处理协议。

- Motivation: 现有方法依赖外部预处理协议（如分词器和变分自编码器）来统一多模态数据表示，这对数据有限的应用存在问题。
- Method: 引入解耦的噪声调度策略，支持无条件生成和模态条件生成，适用于任意状态空间的多模态数据。
- Result: 在文本-图像生成和混合类型表格数据合成任务中表现优异。
- Conclusion: 该框架为多模态扩散模型提供了一种灵活且高效的解决方案。


### [247] [Generative Modeling of Weights: Generalization or Memorization?](https://arxiv.org/abs/2506.07998)
*Boya Zeng,Yida Yin,Zhiqiu Xu,Zhuang Liu*

Main category: cs.LG

TL;DR: 生成模型在图像和视频生成中表现出色，但用于生成神经网络权重时，现有方法主要通过记忆训练检查点，无法生成新颖且高性能的权重。

- Motivation: 探索生成模型在合成神经网络权重方面的能力，评估其是否能生成不同于训练数据的新权重。
- Method: 研究了四种代表性方法，分析其生成权重的表现，并与简单基线（如添加噪声或权重集成）进行比较。
- Result: 现有方法主要通过记忆训练检查点生成权重，无法超越简单基线，且无法通过常见缓解记忆的方法改进。
- Conclusion: 研究揭示了生成模型在新领域的局限性，强调了对生成模型更谨慎评估的必要性。
## cs.CL

### [248] [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
*Bhuiyan Sanjid Shafique,Ashmal Vayani,Muhammad Maaz,Hanoona Abdul Rasheed,Dinura Dissanayake,Mohammed Irfan Kurpath,Yahya Hmaiti,Go Inoue,Jean Lahoud,Md. Safirur Rashid,Shadid Intisar Quasem,Maheen Fatima,Franco Vidal,Mykola Maslych,Ketan Pravin More,Sanoojan Baliah,Hasindri Watawana,Yuhao Li,Fabian Farestam,Leon Schaller,Roman Tymtsiv,Simon Weber,Hisham Cholakkal,Ivan Laptev,Shin'ichi Satoh,Michael Felsberg,Mubarak Shah,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CL

TL;DR: 论文提出多语言视频LMM基准ViMUL-Bench，涵盖14种语言，并开发了多语言视频LMM模型ViMUL，旨在促进文化及语言包容性研究。

- Motivation: 现有LMM主要针对英语，缺乏对多语言及文化包容性的研究，特别是在视频LMM领域。
- Method: 引入ViMUL-Bench基准，包含14种语言的8k样本，并开发ViMUL模型，利用120万样本的多语言视频训练集。
- Result: ViMUL模型在高、低资源语言间取得更好平衡，ViMUL-Bench为多语言视频LMM研究提供工具。
- Conclusion: ViMUL-Bench、ViMUL模型及训练数据将推动多语言视频LMM的发展，促进文化及语言包容性。


### [249] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: 论文提出了一种针对医学领域的多模态大语言模型Lingshu，通过优化数据收集和训练策略，解决了现有医学MLLMs的局限性，并在多项任务中表现优异。

- Motivation: 现有医学MLLMs在医学应用中表现有限，主要由于数据与任务的不匹配、幻觉问题及缺乏针对复杂医学场景的推理能力。
- Method: 提出综合数据收集方法，构建多模态医学数据集，并通过多阶段训练和强化学习增强模型能力。开发MedEvalKit评估框架。
- Result: Lingshu在多项医学任务中表现优于现有开源多模态模型。
- Conclusion: Lingshu通过优化数据和训练策略，显著提升了医学MLLMs的性能，为医学AI应用提供了新方向。


### [250] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: 论文提出了VISE基准，用于评估视频大语言模型（Video-LLMs）在误导性用户输入下的迎合行为，填补了该领域系统性评估的空白。

- Motivation: 视频大语言模型在现实应用中的可靠性至关重要，但其迎合用户输入的倾向（即使与视觉证据矛盾）影响了可信度，目前缺乏针对视频语言领域的系统性研究。
- Method: 提出VISE基准，通过多样化问题格式、提示偏见和视觉推理任务评估Video-LLMs的迎合行为，并探索基于关键帧选择的缓解策略。
- Result: VISE首次将语言领域的迎合行为分析引入视觉领域，为多类型迎合行为和交互模式提供了细粒度分析。
- Conclusion: 关键帧选择作为一种无需训练的缓解策略，展示了通过增强视觉基础减少迎合偏见的潜力。


### [251] [Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline](https://arxiv.org/abs/2506.07631)
*Brian Gordon,Yonatan Bitton,Andreea Marzoca,Yasumasa Onoe,Xiao Wang,Daniel Cohen-Or,Idan Szpektor*

Main category: cs.CL

TL;DR: 论文提出了DOCCI-Critique基准和VNLI-Critique模型，用于评估和改进视觉语言模型（VLM）生成段落的准确性。

- Motivation: 当前评估VLM生成段落的事实准确性方法存在不足，缺乏细粒度错误检测和验证数据集。
- Method: 构建DOCCI-Critique基准，包含1,400个VLM生成的段落标注，并开发VNLI-Critique模型进行自动化分类和错误分析。
- Result: VNLI-Critique在多个基准测试中表现优异，AutoRater与人类判断高度一致，Critic-and-Revise流程显著提升准确性。
- Conclusion: 研究提供了关键基准和工具，显著提升了VLM图像理解的细粒度评估标准。
## cs.MM

### [252] [Experimental Evaluation of Static Image Sub-Region-Based Search Models Using CLIP](https://arxiv.org/abs/2506.06938)
*Bastian Jäckl,Vojtěch Kloda,Daniel A. Keim,Jakub Lokoč*

Main category: cs.MM

TL;DR: 研究探讨了在高度同质化的专业领域中，通过添加基于位置的提示来增强模糊文本查询的检索性能。

- Motivation: 在多模态文本-图像模型中，专业领域的查询仍具挑战性，因为用户通常只能提供模糊的文本描述。
- Method: 收集了741个人工标注的数据集，包含短/长文本描述和感兴趣区域的边界框，评估CLIP模型在不同静态子区域上的检索性能。
- Result: 简单的3x3分区和5网格重叠显著提高了检索效果，并对标注框的扰动具有鲁棒性。
- Conclusion: 基于位置的提示可以有效补充模糊文本查询，提升专业领域的检索性能。
