{"id": "2504.15332", "pdf": "https://arxiv.org/pdf/2504.15332", "abs": "https://arxiv.org/abs/2504.15332", "authors": ["Patrick Adolf", "Martin Hirsch", "Sara Krieg", "Heinrich Päs", "Mustafa Tabet"], "title": "Addendum: Fitting the DESI BAO Data with Dark Energy Driven by the Cohen-Kaplan-Nelson Bound", "categories": ["astro-ph.CO", "gr-qc", "hep-ph", "hep-th"], "comment": "8 pages, 4 figures, 4 tables", "summary": "Motivated by the recent Year-2 data release of the DESI collaboration, we\nupdate our results on time-varying dark energy models driven by the\nCohen-Kaplan-Nelson bound. The previously found preference of time-dependent\ndark energy models compared to $\\Lambda$CDM is further strengthend by the new\ndata release. For our particular models, we find that this preference increases\nup to $\\approx 2.6\\,\\sigma$ depending on the used supernova dataset."}
{"id": "2504.15334", "pdf": "https://arxiv.org/pdf/2504.15334", "abs": "https://arxiv.org/abs/2504.15334", "authors": ["Joshua W. Foster", "Diego Blas", "Adrien Bourgoin", "Aurelien Hees", "Míriam Herrero-Valea", "Alexander C. Jenkins", "Xiao Xue"], "title": "Discovering $μ$Hz gravitational waves and ultra-light dark matter with binary resonances", "categories": ["astro-ph.CO", "gr-qc", "hep-ph"], "comment": "5+2 pages, 2 figures", "summary": "In the presence of a weak gravitational wave (GW) background, astrophysical\nbinary systems act as high-quality resonators, with efficient transfer of\nenergy and momentum between the orbit and a harmonic GW leading to potentially\ndetectable orbital perturbations. In this work, we develop and apply a novel\nmodeling and analysis framework that describes the imprints of GWs on binary\nsystems in a fully time-resolved manner to study the sensitivity of lunar laser\nranging, satellite laser ranging, and pulsar timing to both resonant and\nnonresonant GW backgrounds. We demonstrate that optimal data collection,\nmodeling, and analysis lead to projected sensitivities which are orders of\nmagnitude better than previously appreciated possible, opening up a new\npossibility for probing the physics-rich but notoriously challenging to access\n$\\mu\\mathrm{Hz}$ frequency GWs. We also discuss improved prospects for the\ndetection of the stochastic fluctuations of ultra-light dark matter, which may\nanalogously perturb the binary orbits."}
{"id": "2504.15336", "pdf": "https://arxiv.org/pdf/2504.15336", "abs": "https://arxiv.org/abs/2504.15336", "authors": ["Marina Cortês", "Andrew R Liddle"], "title": "On DESI's DR2 exclusion of $Λ$CDM", "categories": ["astro-ph.CO"], "comment": "6 pages with two figures", "summary": "The DESI collaboration, combining their Baryon Acoustic Oscillation (BAO)\ndata with cosmic microwave background (CMB) anisotropy and supernovae data,\nhave found significant indication against $\\Lambda$CDM cosmology. The\nsignificance of the exclusion of $\\Lambda$CDM can be interpreted entirely as\nsignificance of the detection of the $\\boldsymbol{w_a}$ parameter that measures\nvariation of the dark energy equation of state. We emphasize that DESI's DR2\nexclusion of $\\Lambda$CDM is quoted in the articles for a combination of BAO\nand CMB data with each of three different and overlapping supernovae datasets\n(at 2.8-sigma for Pantheon+, 3.8-sigma for Union3, and 4.2-sigma for DESY5). We\nshow that one can neither choose amongst nor average over these three different\nsignificances. We demonstrate how a principled statistical combination yields a\ncombined exclusion significance of 3.1-sigma. Further we argue that, based on\navailable knowledge, and faced with these competing significances, the most\nsecure inference from the DESI DR2 results is the 3.1-sigma level exclusion of\n$\\Lambda$CDM, obtained from combining DESI+CMB alone, while omitting\nsupernovae."}
{"id": "2504.15340", "pdf": "https://arxiv.org/pdf/2504.15340", "abs": "https://arxiv.org/abs/2504.15340", "authors": ["Shouvik Roy Choudhury"], "title": "Cosmology in Extended Parameter Space with DESI DR2 BAO: A 2$σ$+ Detection of Non-zero Neutrino Masses with an Update on Dynamical Dark Energy and Lensing Anomaly", "categories": ["astro-ph.CO", "hep-ph"], "comment": "15 pages, 6 figures", "summary": "We obtain constraints in a 12 parameter cosmological model using the recent\nDESI Data Release (DR) 2 Baryon Acoustic Oscillations (BAO) data, combined with\nCosmic Microwave Background (CMB) power spectra (Planck Public Release (PR) 4)\nand lensing (Planck PR4 + Atacama Cosmology Telescope (ACT) Data Release (DR)\n6) data, uncalibrated type Ia Supernovae (SNe) data from Pantheon+ and Dark\nEnergy Survey (DES) Year 5 (DESY5) samples, and Weak Lensing (WL: DES Year 1)\ndata. The cosmological model consists of six $\\Lambda$CDM parameters, and\nadditionally, the dynamical dark energy parameters ($w_0$, $w_a$), the sum of\nneutrino masses ($\\sum m_{\\nu}$), the effective number of non-photon radiation\nspecies ($N_{\\textrm{eff}}$), the scaling of the lensing amplitude\n($A_{\\textrm{lens}}$), and the running of the scalar spectral index\n($\\alpha_s$). Our major findings are the following: i) With CMB+BAO+DESY5+WL,\nwe obtain the first 2$\\sigma$+ detection of a non-zero $\\sum m_{\\nu} =\n0.19^{+0.15}_{-0.18}$ eV (95%). Replacing DESY5 with Pantheon+ still yields a\n$\\sim$1.9$\\sigma$ detection. ii) The cosmological constant lies at the edge of\nthe 95% contour with CMB+BAO+Pantheon+, but is excluded at 2$\\sigma$+ with\nDESY5, leaving evidence for dynamical dark energy inconclusive, contrary to\nclaims by DESI collaboration. iii) With CMB+BAO+SNe+WL, $A_{\\textrm{lens}} = 1$\nis excluded at $>2\\sigma$, while it remains consistent with unity without WL\ndata - suggesting for the first time that the existence of lensing anomaly may\ndepend on non-CMB datasets. iv) The Hubble tension persists at 3.6-4.2$\\sigma$\nwith CMB+BAO+SNe; WL data has minimal impact."}
{"id": "2504.15348", "pdf": "https://arxiv.org/pdf/2504.15348", "abs": "https://arxiv.org/abs/2504.15348", "authors": ["Sarah Libanore", "Subhajit Ghosh", "Ely D. Kovetz", "Kimberly K. Boddy", "Alvise Raccanelli"], "title": "Joint 21-cm and CMB Forecasts for Constraining Self-Interacting Massive Neutrinos", "categories": ["astro-ph.CO"], "comment": "28 pages, 13 fugures, prepared for submission", "summary": "Self-interacting neutrinos provide an intriguing extension to the Standard\nModel, motivated by both particle physics and cosmology. Recent cosmological\nanalyses suggest a bimodal posterior for the coupling strength $G_{\\rm eff}$,\nfavoring either strong or moderate interactions. These interactions modify the\nscale-dependence of the growth of cosmic structures, leaving distinct imprints\non the matter power spectrum at small scales, $k\\,>\\,0.1\\,{\\rm Mpc}^{-1}$. For\nthe first time, we explore how the 21-cm power spectrum from the cosmic dawn\nand the dark ages can constrain the properties of self-interacting, massive\nneutrinos. The effects of small-scale suppression and enhancement in the matter\npower spectrum caused by self-interacting neutrinos propagate to the halo mass\nfunction, shaping the abundance of small- and intermediate-mass halos. It is\nprecisely these halos that host the galaxies responsible for driving the\nevolution of the 21-cm signal during the cosmic dawn. We find that HERA at its\ndesign sensitivity can improve upon existing constraints on $G_{\\rm eff}$ and\nbe sensitive to small values of the coupling, beyond the reach of current and\nfuture CMB experiments. Crucially, we find that the combination of HERA and\nCMB-S4 can break parameter degeneracies, significantly improving the\nsensitivity to $G_{\\rm eff}$ over either experiment alone. Finally, we\ninvestigate the prospects of probing neutrino properties with futuristic Lunar\ninterferometers, accessing the astrophysics-free 21-cm power spectrum during\nthe dark ages. The capability of probing small scales of these instruments will\nallow us to reach a percent-level constraint on the neutrino self-coupling."}
{"id": "2504.15351", "pdf": "https://arxiv.org/pdf/2504.15351", "abs": "https://arxiv.org/abs/2504.15351", "authors": ["Fabian Schmidt"], "title": "On the Connection between Field-Level Inference and $n$-point Correlation Functions", "categories": ["astro-ph.CO"], "comment": "43 pages", "summary": "Bayesian field-level inference of galaxy clustering guarantees optimal\nextraction of all cosmological information, provided that the data are\ncorrectly described by the forward model employed. The latter is unfortunately\nnever strictly the case. A key question for field-level inference approaches\nthen is where the cosmological information is coming from, and how to ensure\nthat it is robust. In the context of perturbative approaches such as effective\nfield theory, some progress on this question can be made analytically. We\nderive the parameter posterior given the data for the field-level likelihood\ngiven in the effective field theory, marginalized over initial conditions in\nthe zero-noise limit. Particular attention is paid to cutoffs in the theory,\nthe generalization to higher orders, and the error made by an incomplete\nforward model at a given order. The main finding is that, broadly speaking, an\n$m$-th order forward model captures the information in $n$-point correlation\nfunctions with $n \\leqslant m+1$. Thus, by adding more terms to the forward\nmodel, field-level inference is made to automatically incorporate higher-order\n$n$-point functions. Also shown is how the effect of an incomplete forward\nmodel (at a given order) on the parameter inference can be estimated."}
{"id": "2504.15382", "pdf": "https://arxiv.org/pdf/2504.15382", "abs": "https://arxiv.org/abs/2504.15382", "authors": ["Michael Sekatchev", "Xunyu Liang", "Fereshteh Majidi", "Ben Scully", "Ludovic Van Waerbeke", "Ariel Zhitnitsky"], "title": "The Glow of Axion Quark Nugget Dark Matter: (III) The Mysteries of the Milky Way UV Background", "categories": ["astro-ph.CO", "hep-ph"], "comment": "25 pages, 7 figures, prepared for submission to JCAP", "summary": "Axion quark nuggets (AQNs) are hypothetical objects with nuclear density that\nwould have formed during the quark-hadron transition and could make up most of\nthe dark matter today. These objects have a mass greater than a few grams and\nare sub-micrometer in size. They would also help explain the matter-antimatter\nasymmetry and the similarity between visible and dark components of the\nuniverse, i.e. $\\Omega_{\\text{DM}} \\sim \\Omega_{\\text{visible}}$. These\ncomposite objects behave as cold dark matter, interacting with ordinary matter\nand producing pervasive electromagnetic radiation. This work aims to calculate\nthe FUV electromagnetic signature in a 1 kpc region surrounding the solar\nsystem, resulting from the interaction between antimatter AQNs and baryons. To\nthis end, we use the high-resolution hydrodynamic simulation of the Milky Way,\nFIRE-2 Latter suite, to select solar system-like regions. From the simulated\ngas and dark matter distributions in these regions, we calculate the FUV\nbackground radiation generated by the AQN model. We find that the results are\nconsistent with the FUV excess recently confirmed by the Alice spectrograph\naboard New Horizons, which corroborated the FUV excess initially discovered by\nGALEX a decade ago. We also discuss the potential cosmological implications of\nour work, which suggest the existence of a new source of FUV radiation in\ngalaxies, linked to the interaction between dark matter and baryons."}
{"id": "2504.15400", "pdf": "https://arxiv.org/pdf/2504.15400", "abs": "https://arxiv.org/abs/2504.15400", "authors": ["Subhasis Maiti", "Debaprasad Maity", "Rohan Srikanth"], "title": "Minimal Magnetogenesis: The Role of Inflationary Perturbations and ALPs, and Its Gravitational Wave Signatures", "categories": ["astro-ph.CO", "hep-ph", "hep-th"], "comment": "27 pages, 12 figures", "summary": "Any attempt to understand the ubiquitous nature of the magnetic field in the\npresent universe seems to lead us towards its primordial origin. For\nlarge-scale magnetic fields, however, their strength and length scale may not\nnecessarily originate from a singular primordial mechanism, namely inflationary\nmagnetogenesis, which has been a popular consideration in the literature. In\nthis paper, we propose a minimal scenario wherein a large-scale magnetic field\nis generated from the inflationary perturbation without any non-conformal\ncoupling. Due to their origin in the inflationary scalar spectrum, these\nprimordial fields are inherently weak, with their strength suppressed by the\nsmall amplitude of scalar fluctuations. We then consider the coupling between\nthis large-scale weak primordial magnetic field and a light axion of mass\n$<10^{-28}$ eV, which is assumed to be frozen in a misaligned state until the\nphoton decoupling. After the decoupling, when the universe enters into a dark\nage, the light axion coherently oscillates. By appropriately tuning the\naxion-photon coupling parameter $\\alpha$, we demonstrate that a large-scale\nmagnetic field of sufficient strength can indeed be generated through tachyonic\nresonance. We further show that the produced magnetic field induces a unique\nspectrum with multiple peaks of secondary gravitational waves, which the\nupcoming CMB-S4 can probe through B-mode polarization. The strength can be\nsufficient enough to violate the PLANCK bound on tensor-to-scalar ratio $r\n\\lesssim 0.036$. Such a violation leads to a constraint on $\\alpha \\lesssim\n80$. With this limiting value of the coupling, we find that present-day\nmagnetic field strength could be as high as $10^{-10}$ Gauss at $\\Mpc$ scale,\nconsistent with observation."}
{"id": "2504.15506", "pdf": "https://arxiv.org/pdf/2504.15506", "abs": "https://arxiv.org/abs/2504.15506", "authors": ["Paula S. Ferreira", "Carlos Hernández-Monteagudo", "Ribamar R. R. Reis"], "title": "Baryon Acoustic Oscillations in tomographic Angular Density and Redshift Fluctuations", "categories": ["astro-ph.CO"], "comment": "36 pages, 20 figures", "summary": "In this work we examine the baryon acoustic oscillations (BAO) in 2D angular\nand redshift space $\\{\\theta, \\Delta z\\}$, with $\\Delta z$ denoting the\nredshift difference between two given angular shells. We thus work in the\ncontext of tomographic analyses of the large scale structure (LSS) where data\nare sliced in different redshift shells and constraints on Cosmology are\nextracted from the auto and cross-angular spectra of two different probes,\nnamely the standard galaxy angular density fluctuations (ADF, or 2D\nclustering), and the galaxy angular redshift fluctuations (ARF). For these two\nobservables we study by first time how the BAO peak arises in the $\\{\\theta,\n\\Delta z\\}$ plane. Despite being a weak feature (particularly for $\\Delta z\n\\neq 0$), a Fisher forecast analysis shows that, a priori, most of the\ninformation on cosmological and galaxy bias parameters is carried by the BAO\nfeatures in shell auto- and cross-angular power spectra. The same study shows\nthat a joint probe analysis (ADF+ARF) increases the Fisher determinant\nassociated to cosmological parameters such as $H_0$ or the Dark Energy\nChevallier-Polarski-Linder (CPL) parameters $\\{w_0,w_a\\}$ by at least an order\nof magnitude. We also study how the Fisher information on cosmological and\ngalaxy bias-related parameters behaves under different redshift shell\nconfigurations: including cross-correlations to neighbour shells extending up\nto $(\\Delta z)^{\\rm tot}\\sim 0.6$ ($(\\Delta z)^{\\rm tot}\\sim 0.4$) for ADF\n(ARF) is required for Fisher information to converge. At the same time,\nconfigurations using narrow shell widths ($\\sigma_z \\leq 0.02$) preserve the\ncosmological information associated to peculiar velocities and typically yield\nFisher determinants that are about two orders of magnitudes larger than for\nwider shell ($\\sigma_z>0.02$) configurations."}
{"id": "2504.15633", "pdf": "https://arxiv.org/pdf/2504.15633", "abs": "https://arxiv.org/abs/2504.15633", "authors": ["Matthieu Schaller", "Joop Schaye"], "title": "An analytic redshift-independent formulation of baryonic effects on the matter power spectrum", "categories": ["astro-ph.CO"], "comment": "8 pages, 6 figures, submitted to MNRAS, comments welcome", "summary": "Baryonic effects created by feedback processes associated with galaxy\nformation are an important, poorly constrained systematic effect for models of\nlarge-scale structure as probed by weak gravitational lensing. Upcoming surveys\nrequire fast methods to predict and marginalize over the potential impact of\nbaryons on the total matter power spectrum. Here we use the FLAMINGO\ncosmological hydrodynamical simulations to test a recent proposal to\napproximate the matter power spectrum as the sum of the linear matter power\nspectrum and a constant multiple, $A_{\\rm mod}$, of the difference between the\nlinear and non-linear gravity-only power spectra. We show that replacing this\nconstant multiple with a one-parameter family of sigmoid functions of the\nwavenumber $k$ allows to us match the predictions of simulations with different\nfeedback strengths for $z \\leq 1, k < 3~h\\cdot{\\rm Mpc}^{-1}$, and the\ndifferent cosmological models in the FLAMINGO suite. The baryonic response\npredicted by FLAMINGO models that use jet-like AGN feedback instead of the\nfiducial thermally-driven AGN feedback can also be reproduced, but at the cost\nof increasing the number of parameters in the sigmoid function from one to\nthree. The assumption that $A_{\\rm mod}$ depends only on $k$ breaks down for\ndecaying dark matter models, highlighting the need for more advanced baryon\nresponse models when studying cosmological models that deviate strongly from\n$\\Lambda$CDM."}
{"id": "2504.15635", "pdf": "https://arxiv.org/pdf/2504.15635", "abs": "https://arxiv.org/abs/2504.15635", "authors": ["Deng Wang"], "title": "Questioning Cosmic Acceleration with DESI: The Big Stall of the Universe", "categories": ["astro-ph.CO", "astro-ph.HE", "gr-qc"], "comment": "7 pages, 3 figures", "summary": "One of the most important discoveries in modern cosmology is cosmic\nacceleration. However, we find that today's universe could decelerate in the\nstatistically preferred Chevallier-Polarski-Linder (CPL) scenario over the\n$\\Lambda$CDM model by cosmic microwave background, type Ia supernova and DESI's\nnew measurements of baryon acoustic oscillations. Using various datasets, at a\nbeyond $5\\,\\sigma$ confidence level, we demonstrate that the universe\nexperiences a triple deceleration during its evolution and finally reaches the\nstate of the ``Big Stall\", which predicts that: (i) the universe suddenly comes\nto a halt in the distant future; (ii) its eventual destiny is dominated by dark\nmatter rather than dark energy ; (iii) it ultimately retains an extremely small\nfraction of dark energy but exerts an extremely large pressure. Our findings\nprofoundly challenge the established understanding of cosmic acceleration and\nenrich our comprehension of cosmic evolution."}
{"id": "2504.15739", "pdf": "https://arxiv.org/pdf/2504.15739", "abs": "https://arxiv.org/abs/2504.15739", "authors": ["Dylan Kuhn", "Marc Betoule"], "title": "Development of an Ultra-fast, Likelihood-based, Distance Inference Framework for the Next Generation of Type Ia Supernova Surveys", "categories": ["astro-ph.CO"], "comment": "5 pages, 3 figures, contribution to the 2024 Cosmology session of the\n  58th Rencontres de Moriond", "summary": "In this work, we present EDRIS (French for Distance Estimator for Incomplete\nSupernova Surveys), a cosmological inference framework tailored to reconstruct\nunbiased cosmological distances from type Ia supernovae light-curve parameters.\nThis goal is achieved by including data truncation directly in the statistical\nmodel which takes care of the standardization of luminosity distances. It\nallows us to build a single-step distance estimate by maximizing the\ncorresponding likelihood, free from the biases the survey detection limits\nwould introduce otherwise. Moreover, we expect the current worldwide statistics\nto be multiplied by O(10) in the upcoming years. This provides a new challenge\nto handle as the cosmological analysis must stay computationally towable. We\nshow that the optimization methods used in EDRIS allow for a reasonable time\ncomplexity of O($N^2$) resulting in a very fast inference process (O(10s) for\n1500 supernovae)."}
{"id": "2504.16059", "pdf": "https://arxiv.org/pdf/2504.16059", "abs": "https://arxiv.org/abs/2504.16059", "authors": ["Debottam Nandi", "Rohan Roy", "Simran Yadav", "Arnab Sarkar"], "title": "Sub-Horizon Amplification of Curvature Perturbations: A New Route to Primordial Black Holes and Gravitational Waves", "categories": ["astro-ph.CO", "gr-qc", "hep-ph", "hep-th"], "comment": "32 pages, 4 figures", "summary": "The enhanced primordial scalar power spectrum is a widely studied mechanism\nfor generating primordial gravitational waves (PGWs), also referred to as\nscalar-induced gravitational waves (SIGWs). This process also plays a pivotal\nrole in facilitating the formation of primordial black holes (PBHs).\nTraditionally, the ultra slow-roll (USR) mechanism has been the predominant\napproach used in the early universe. In this framework, the second slow-roll\nparameter $\\epsilon_2$, is typically set to $-6$ or lower for a brief period --\nmarking a significant departure from the standard slow-roll condition where\n$\\epsilon_2 \\simeq 0$. Such conditions often emerge in models with inflection\npoints or localized features, such as bumps in the potential. In this paper, we\nchallenge the conventional assumption that $\\epsilon_2 \\lesssim -6$ is a\nprerequisite for substantial amplification of the scalar power spectrum. We\ndemonstrate that any negative value of the second slow-roll parameter can\nindeed enhance the scalar power spectrum through sub-horizon growth,\nestablishing this as a necessary and sufficient condition for amplification.\nConsequently, this mechanism facilitates the generation of both PGWs and PBHs.\nTo illustrate this, we examine a standard scenario where a brief USR phase is\nembedded between two slow-roll (SR) phases. By systematically varying\n$\\epsilon_{2}$ values from $-1$ to $-10$ in the USR region, we investigate the\namplification of the power spectrum and its implications for PGWs and PBHs\nproduction, particularly in the context of ongoing and future cosmological\nmissions."}
{"id": "2504.16069", "pdf": "https://arxiv.org/pdf/2504.16069", "abs": "https://arxiv.org/abs/2504.16069", "authors": ["Minxi He", "Muzi Hong", "Kyohei Mukaida"], "title": "Increase of $n_s$ in regularized pole inflation & Einstein-Cartan gravity", "categories": ["astro-ph.CO", "hep-ph"], "comment": null, "summary": "We show that the regularization of the second order pole in the pole\ninflation can induce the increase of $n_s$, which may be important after the\nlatest data release of cosmic microwave background (CMB) observation by Atacama\nCosmology Telescope (ACT). Pole inflation is known to provide a unified\ndescription of attractor models that they can generate a flat plateau for\ninflation given a general potential. Recent ACT observation suggests that the\nconstraint on the scalar spectral index $n_s$ at CMB scale may be shifted to a\nlarger value than the predictions in the Starobinsky model, the Higgs\ninflation, and the $\\alpha$-attractor model, which motivates us to consider the\nmodification of the pole inflation. We find that if we regularize the second\norder pole in the kinetic term such that the kinetic term becomes regular for\nall field range, we can generally increase $n_s$ because the potential in the\nlarge field regime will be lifted. We have explicitly demonstrated that this\ntype of regularized pole inflation can naturally arise from the Einstein-Cartan\nformalism, and the inflationary predictions are consistent with the latest ACT\ndata without spoiling the success of the $\\alpha$-attractor models."}
{"id": "2504.16076", "pdf": "https://arxiv.org/pdf/2504.16076", "abs": "https://arxiv.org/abs/2504.16076", "authors": ["Claire Lamman", "Jonathan Blazek", "Daniel J. Eisenstein"], "title": "Optimal intrinsic alignment estimators in the presence of redshift-space distortions", "categories": ["astro-ph.CO"], "comment": "For an accessible summary of this paper, see\n  https://cmlamman.github.io/doc/optimalRSDstats-annotation.pdf", "summary": "We present estimators for quantifying intrinsic alignments in large\nspectroscopic surveys that efficiently capture line-of-sight (LOS) information\nwhile being relatively insensitive to redshift-space distortions (RSD). We\ndemonstrate that changing the LOS integration range, {\\Pi}max, as a function of\ntransverse separation outperforms the conventional choice of a single {\\Pi}max\nvalue. This is further improved by replacing the flat {\\Pi}max cut with a LOS\nweighting based on shape projection and RSD. Although these estimators\nincorporate additional LOS information, they are projected correlations that\nexhibit signal-to-noise ratios comparable to 3D correlation functions, such as\nthe IA quadrupole. Using simulations from Abacus Summit, we evaluate these\nestimators and provide recommended {\\Pi}max values and weights for projected\nseparations of 1 - 100 Mpc/h. These will improve measurements of intrinsic\nalignments in large cosmological surveys and the constraints they provide for\nboth weak lensing and direct cosmological applications."}
{"id": "2504.07559", "pdf": "https://arxiv.org/pdf/2504.07559", "abs": "https://arxiv.org/abs/2504.07559", "authors": ["Atsushi Nishizawa", "Atsushi Taruya", "Yoshiaki Himemoto"], "title": "Axion dark matter search from terrestrial magnetic fields at extremely low frequencies", "categories": ["hep-ph", "astro-ph.CO"], "comment": "19 pages, 12 figures, minor changes", "summary": "The natural environment of the Earth can act as a sensitive detector for dark\nmatter in ultralight axions. When axions with masses between\n$1\\times10^{-15}\\,{\\rm eV}$ and $1\\times10^{-13}\\,{\\rm eV}$ pass through the\nEarth, they interact with the global geomagnetic field, generating\nelectromagnetic (EM) waves in the extremely low-frequency range\n($0.3$--$30\\,{\\rm Hz}$) through axion-photon coupling. This paper is one of a\nseries of companion papers for~\\cite{Taruya:2025zql}, focusing on the data\nanalysis method and search results for an axion signal. Utilizing the\ntheoretical predictions of axion-induced EM spectra from a companion study, we\nanalyzed long-term observational data of terrestrial magnetic fields in this\nfrequency band to search for axion-induced signals. Our analysis identified 65\npersistent signal candidates with a signal-to-noise ratio (SNR) greater than 3.\nAside from these candidates, we placed a new upper bound on the axion-photon\ncoupling parameter, significantly refining the previous constraint from CAST by\nat most two orders of magnitude down to $g_{a\\gamma} \\lesssim 4\\times10^{-13}\n\\,{\\rm GeV}^{-1}$ for the axion mass around $3 \\times 10^{-14}\\,{\\rm eV}$."}
{"id": "2504.14799", "pdf": "https://arxiv.org/pdf/2504.14799", "abs": "https://arxiv.org/abs/2504.14799", "authors": ["Anupam Ray"], "title": "Detecting Dark Matter with Neutron Stars", "categories": ["hep-ph", "astro-ph.CO"], "comment": "Contribution to the 2025 Electroweak session of the 59th Rencontres\n  de Moriond, La Thuile, Italy, March 23-30, 2025", "summary": "Neutron stars offer powerful astrophysical laboratories to probe the\nproperties of dark matter. Gradual accumulation of heavy, non-annihilating dark\nmatter in neutron stars can lead to the formation of comparable-mass black\nholes, and non-detection of gravitational waves from mergers of such low-mass\nblack holes can constrain such dark matter interactions with nucleons. These\nconstraints, though dependent on the currently uncertain binary neutron star\nmerger rate density, are significantly more stringent than those from direct\ndetection experiments and provide some of the strongest limits on heavy,\nnon-annihilating dark matter interactions. Additionally, dark matter with\nbaryon number-violating interactions can induce excess heating in cold neutron\nstars and is thus significantly constrained by thermal observations of cold\nneutron stars."}
{"id": "2504.15345", "pdf": "https://arxiv.org/pdf/2504.15345", "abs": "https://arxiv.org/abs/2504.15345", "authors": ["Tirso Marin-Gilabert", "Max Gronke", "S. Peng Oh"], "title": "The (Limited) Effect of Viscosity in Multiphase Turbulent Mixing", "categories": ["astro-ph.GA", "astro-ph.CO"], "comment": "18 pages, 20 figures, submitted to Astronomy and Astrophysics", "summary": "Multiphase gas can be found in many astrophysical environments, such as\ngalactic outflows, stellar wind bubbles, and the circumgalactic medium, where\nthe interplay between turbulence, cooling, and viscosity can significantly\ninfluence gas dynamics and star formation processes. We investigate the role of\nviscosity in modulating turbulence and radiative cooling in turbulent radiative\nmixing layers (TRMLs). In particular, we aim to determine how different amounts\nof viscosity affect the Kelvin-Helmholtz instability (KHI), turbulence\nevolution, and the efficiency of gas mixing and cooling. Using idealized 2D\nnumerical setups, we compute the critical viscosity required to suppress the\nKHI in shear flows characterized by different density contrasts and Mach\nnumbers. These results are then used in a 3D shear layer setup to explore the\nimpact of viscosity on cooling efficiency and turbulence across different\ncooling regimes. We find that the critical viscosity follows the expected\ndependence on overdensity and Mach number. Our viscous TRMLs simulations show\ndifferent behaviors in the weak and strong cooling regimes. In the weak cooling\nregime, viscosity has a strong impact, resulting in laminar flows and breaking\npreviously established inviscid relations between cooling and turbulence\n(albeit leaving the total luminosity unaffected). However, in the strong\ncooling regime, when cooling timescales are shorter than viscous timescales,\nkey scaling relations in TRMLs remain largely intact. In this regime -- which\nmust hold for gas to remain multiphase -- radiative losses dominate, and the\nsystem effectively behaves as non-viscous regardless of the actual level of\nviscosity. Our findings have direct implications for both the interpretation of\nobservational diagnostics and the development of subgrid models in large-scale\nsimulations."}
{"id": "2504.15374", "pdf": "https://arxiv.org/pdf/2504.15374", "abs": "https://arxiv.org/abs/2504.15374", "authors": ["Zafri A. Borboruah", "Lekhika Malhotra", "Frank F. Deppisch", "Anish Ghoshal"], "title": "Inflationary Gravitational Waves and Laboratory Searches as Complementary Probes of Right-handed Neutrinos", "categories": ["hep-ph", "astro-ph.CO"], "comment": "17 pages including references, 10 figures including sub-figures", "summary": "We analyze the damping of inflationary gravitational waves (GW) that re-enter\nthe Hubble horizon before or during a post-inflationary era dominated by a\nmeta-stable, right-handed neutrino (RHN), whose out-of-equilibrium decay\nreleases entropy. Within a minimal type-I seesaw extension of the Standard\nModel (SM), we explore the conditions under which the population of thermally\nproduced RHNs remain long-lived and cause a period of matter-domination. We\nfind that the suppression of the GW spectrum occurs above a characteristic\nfrequency determined by the RHN mass and active-sterile mixing. For RHN masses\nin the range $0.1$ - $10$ GeV and mixing $10^{-12} \\lesssim |V_{eN}|^2 \\lesssim\n10^{-5}$, we estimate such characteristic frequencies and the signal-to-noise\nratio to assess the detection prospects in GW observatories such as THEIA,\n$\\mu$-ARES, LISA, BBO and ET. We find complementarity between GW signals and\nlaboratory searches in SHiP, DUNE and LEGEND-1000. Notably, RHN masses of $0.2$\n- $2$ GeV and mixing $10^{-10} \\lesssim |V_{eN}|^2 \\lesssim 10^{-7}$ are\ntestable in both laboratory experiments and GW observations. Additionally, GW\nexperiments can probe the canonical seesaw regime of light neutrino mass\ngeneration, a region largely inaccessible to laboratory searches."}
{"id": "2504.15606", "pdf": "https://arxiv.org/pdf/2504.15606", "abs": "https://arxiv.org/abs/2504.15606", "authors": ["Asuka Ito", "Rudnei O. Ramos"], "title": "Warm multi natural inflation", "categories": ["hep-ph", "astro-ph.CO", "gr-qc", "hep-th"], "comment": "13 pages, 5 figures, 2 tables", "summary": "Multi natural inflation is studied in the context of warm inflation. We study\nthe warm multi natural inflation scenario with both linear and cubic\ndissipation coefficients. The model is motivated by axion-like inflation models\nwith coupling to non-Abelian gauge fields through a dimension five coupling and\ndissipation originating from sphaleron decay in a thermal bath. Both cases of\ndissipation coefficients can be compatible with current observations. In the\ncase of the cubic dissipation coefficient, we find that the curvature\nperturbation starts to grow suddenly when a transition from a weak dissipation\nto a strong dissipation regime occurs at the later stage of the inflation. We\nalso show that such rapid growth of the curvature perturbation on small scales\ngives rise to abundant scalar induced gravitational waves, which may be\ndetectable with future gravitational wave detectors such as DECIGO and ET. On\nthe other hand, there are also other parameter regions of the model, in the\nwarm inflation regime of weak to strong dissipation and with sub-Planckian\naxion decay constant, that can lead to overproduction of primordial black holes\non small scales, which are constrained by nucleosynthesis bounds, thus ruling\nout the model in this region of parameters."}
{"id": "2504.15802", "pdf": "https://arxiv.org/pdf/2504.15802", "abs": "https://arxiv.org/abs/2504.15802", "authors": ["Ellis R. Owen", "Yoshiyuki Inoue", "Tatsuki Fujiwara", "Qin Han", "Kinwah Wu"], "title": "Cosmic ray neutrons in magnetized astrophysical structures", "categories": ["astro-ph.HE", "astro-ph.CO", "astro-ph.GA"], "comment": "10 pages, 3 figures, proceedings of the 28th European Cosmic Ray\n  Symposium (ECRS2024)", "summary": "Cosmic rays are often modeled as charged particles. This allows their\nnon-ballistic propagation in magnetized structures to be captured. In certain\nsituations, a neutral cosmic ray component can arise. For example, cosmic ray\nneutrons are produced in considerable numbers through hadronic pp and p$\\gamma$\ninteractions. At ultrahigh energies, the decay timescales of these neutrons is\ndilated, allowing them to traverse distances on the scale of galactic and\ncosmological structures. Unlike charged cosmic rays, neutrons are not deflected\nby magnetic fields. They propagate ballistically at the speed of light in\nstraight lines. The presence of a neutral baryonic cosmic ray component formed\nin galaxies, clusters and cosmological filaments can facilitate the escape and\nleakage of cosmic rays from magnetic structures that would otherwise confine\nthem. We show that, by allowing confinement breaking, the formation of\ncosmic-ray neutrons by high-energy hadronic interactions in large scale\nastrophysical structures can modify the exchange of ultra high-energy particles\nacross magnetic interfaces between galaxies, clusters, cosmological filaments\nand voids."}
{"id": "2504.15821", "pdf": "https://arxiv.org/pdf/2504.15821", "abs": "https://arxiv.org/abs/2504.15821", "authors": ["Eugene Hyeonmin Lee", "Joohyun Lee", "Paul R. Shapiro", "Pierre Ocvirk", "Joseph S. W. Lewis", "Taha Dawoodbhoy", "Ilian T. Iliev", "Luke Conaboy", "Kyungjin Ahn", "Hyunbae Park", "Jenny G. Sorce", "Dominique Aubert", "Romain Teyssier", "Gustavo Yepes", "Yohan Dubois", "Stefan Gottlöber"], "title": "Line Intensity Mapping Prediction from the Cosmic Dawn (CoDa) III Simulation for H$α$ from Galaxies and the Intergalactic Medium during the Epoch of Reionization", "categories": ["astro-ph.GA", "astro-ph.CO"], "comment": "4 pages, 1 figure, to appear in RNAAS", "summary": "The evolution of large-scale structure, galaxies and the intergalactic medium\n(IGM) during the Epoch of Reionization (EoR) can be probed by upcoming Line\nIntensity Mapping (LIM) experiments, which sample in redshift and direction\nwithout needing to resolve individual galaxies. We predict the intensity and\nsources of hydrogen H$\\alpha$ emission, dominated by radiative recombination\nfollowing ionization by UV from the same massive stars that caused\nreionization, down to redshift 4.6, using the largest fully-coupled,\nradiation-hydro simulation of galaxy formation and reionization to date, Cosmic\nDawn (CoDa) III. We compute the mean intensity and Voxel Intensity Distribution\n(VID) vs. redshift, including the relative contributions of galaxies and IGM.\nThis will provide mock data to guide and interpret LIM experiments such as\nNASA's SPHEREx and proposed Cosmic Dawn Intensity Mapper (CDIM)."}
{"id": "2504.15825", "pdf": "https://arxiv.org/pdf/2504.15825", "abs": "https://arxiv.org/abs/2504.15825", "authors": ["Antonio Troisi", "Sante Carloni"], "title": "Bounce Cosmologies in Generalized Coupling Theories", "categories": ["gr-qc", "astro-ph.CO"], "comment": null, "summary": "We describe an exact solution representing a bouncing cosmology in the\nMinimal Exponential Measure (MEMe) model. Such a solution, obtained by means of\nthe linearization around small values of the characteristic energy scale q of\nthe theory, has the peculiarity of representing a complete bounce model that\ncan be used to explore quantitative processes in non-singular cosmologies."}
{"id": "2504.15902", "pdf": "https://arxiv.org/pdf/2504.15902", "abs": "https://arxiv.org/abs/2504.15902", "authors": ["Olivia Curtis", "Bryanne McDonough", "Tereasa Brainerd"], "title": "Density Profiles of TNG300 Voids across Cosmic Time", "categories": ["astro-ph.GA", "astro-ph.CO"], "comment": "Accepted for publication in ApJ. 40 pages, 11 figures, 2 tables", "summary": "We present radial density profiles, as traced by luminous galaxies and dark\nmatter particles, for voids in eleven snapshots of the \\texttt{TNG300}\nsimulation. The snapshots span 11.65~Gyr of cosmic time, corresponding to the\nredshift range $0 \\le z \\le 3$. Using the comoving galaxy fields, voids were\nidentified via a well-tested, watershed transformation-based algorithm. Voids\nwere defined to be underdense regions that are unlikely to have arisen from\nPoisson noise, resulting in the selection of $\\sim100-200$ of the largest\nunderdense regions in each snapshot. At all redshifts, the radial density\nprofiles as traced by both the galaxies and the dark matter resemble inverse\ntop-hat functions. However, details of the functions (particularly the\nunderdensities of the innermost regions and the overdensities of the ridges)\nevolve considerably more for the dark matter density profiles than for the\ngalaxy density profiles. At all redshifts, a linear relationship between the\ngalaxy and dark matter density profiles exists, and the slope of the\nrelationship is similar to the bias estimates for \\texttt{TNG300} snapshots.\nLastly, we identify distinct environments in which voids can exist, defining\n``void-in-void\" and ``void-in-cloud\" populations (i.e., voids that reside in\nlarger underdense or overdense regions, respectively) and we investigate ways\nin which the relative densities of dark matter and galaxies in the interiors\nand ridges of these structures vary as a function of void environment."}
{"id": "2504.15913", "pdf": "https://arxiv.org/pdf/2504.15913", "abs": "https://arxiv.org/abs/2504.15913", "authors": ["Roberto J. Assef", "Marko Stalevski", "Lee Armus", "Franz E. Bauer", "Andrew Blain", "Murray Brightman", "Tanio Díaz-Santos", "Peter R. M. Eisenhardt", "Román Fernández-Aranda", "Hyunsung D. Jun", "Mai Liao", "Guodong Li", "Lee R. Martin", "Elena Shablovinskaia", "Devika Shobhana", "Daniel Stern", "Chao-Wei Tsai", "Andrey Vayner", "Dominic J. Walton", "Jingwen Wu", "Dejene Zewdie"], "title": "A Massive Gas Outflow Outside the Line-of-Sight: Imaging Polarimetry of the Blue Excess Hot Dust Obscured Galaxy W0204-0506", "categories": ["astro-ph.GA", "astro-ph.CO"], "comment": "8 pages, 4 figures, 1 table. Submitted to A&A", "summary": "(Aims) Hot Dust Obscured Galaxies (Hot DOGs) are a population of\nhyper-luminous, heavily obscured quasars. Although nuclear obscurations close\nto Compton-thick are typical, a fraction show blue UV spectral energy\ndistributions consistent with unobscured quasar activity, albeit two orders of\nmagnitude fainter than expected from their mid-IR luminosity. The origin of the\nUV emission in these Blue excess Hot DOGs (BHDs) has been linked to scattered\nlight from the central engine. Here we study the properties of the UV emission\nin the BHD WISE J020446.13-050640.8 (W0204-0506). (Methods) We use imaging\npolarization observations in the $R_{\\rm Special}$ band obtained with the FORS2\ninstrument at VLT. We compare these data with radiative transfer simulations to\nconstrain the characteristics of the scattering material. (Results) We find a\nspatially integrated polarization fraction of $24.7\\pm 0.7$%, confirming the\nscattered-light nature of the UV emission of W0204-0506. The source is\nspatially resolved in the observations and we find a gradient in polarization\nfraction and angle that is aligned with the extended morphology of the source\nfound in HST/WFC3 imaging. A dusty, conical polar outflow starting at the AGN\nsublimation radius with a half-opening angle $\\lesssim 50~\\rm deg$ viewed at an\ninclination $\\gtrsim 45~\\rm deg$ can reproduce the observed polarization\nfraction if the dust is graphite-rich. We find that the gas mass and outflow\nvelocity are consistent with the range of values found for [OIII] outflows\nthrough spectroscopy in other Hot DOGs, though it is unclear whether the\noutflow is energetic enough to affect the long-term evolution of the host\ngalaxy. Our study highlights the unique potential for polarization imaging to\nstudy dusty quasar outflows, providing complementary constraints to those\nobtained through traditional spectroscopic studies."}
{"id": "2504.15332", "pdf": "https://arxiv.org/pdf/2504.15332", "abs": "https://arxiv.org/abs/2504.15332", "authors": ["Patrick Adolf", "Martin Hirsch", "Sara Krieg", "Heinrich Päs", "Mustafa Tabet"], "title": "Addendum: Fitting the DESI BAO Data with Dark Energy Driven by the Cohen-Kaplan-Nelson Bound", "categories": ["astro-ph.CO", "gr-qc", "hep-ph", "hep-th"], "comment": "8 pages, 4 figures, 4 tables", "summary": "Motivated by the recent Year-2 data release of the DESI collaboration, we\nupdate our results on time-varying dark energy models driven by the\nCohen-Kaplan-Nelson bound. The previously found preference of time-dependent\ndark energy models compared to $\\Lambda$CDM is further strengthend by the new\ndata release. For our particular models, we find that this preference increases\nup to $\\approx 2.6\\,\\sigma$ depending on the used supernova dataset."}
{"id": "2504.15334", "pdf": "https://arxiv.org/pdf/2504.15334", "abs": "https://arxiv.org/abs/2504.15334", "authors": ["Joshua W. Foster", "Diego Blas", "Adrien Bourgoin", "Aurelien Hees", "Míriam Herrero-Valea", "Alexander C. Jenkins", "Xiao Xue"], "title": "Discovering $μ$Hz gravitational waves and ultra-light dark matter with binary resonances", "categories": ["astro-ph.CO", "gr-qc", "hep-ph"], "comment": "5+2 pages, 2 figures", "summary": "In the presence of a weak gravitational wave (GW) background, astrophysical\nbinary systems act as high-quality resonators, with efficient transfer of\nenergy and momentum between the orbit and a harmonic GW leading to potentially\ndetectable orbital perturbations. In this work, we develop and apply a novel\nmodeling and analysis framework that describes the imprints of GWs on binary\nsystems in a fully time-resolved manner to study the sensitivity of lunar laser\nranging, satellite laser ranging, and pulsar timing to both resonant and\nnonresonant GW backgrounds. We demonstrate that optimal data collection,\nmodeling, and analysis lead to projected sensitivities which are orders of\nmagnitude better than previously appreciated possible, opening up a new\npossibility for probing the physics-rich but notoriously challenging to access\n$\\mu\\mathrm{Hz}$ frequency GWs. We also discuss improved prospects for the\ndetection of the stochastic fluctuations of ultra-light dark matter, which may\nanalogously perturb the binary orbits."}
{"id": "2504.15336", "pdf": "https://arxiv.org/pdf/2504.15336", "abs": "https://arxiv.org/abs/2504.15336", "authors": ["Marina Cortês", "Andrew R Liddle"], "title": "On DESI's DR2 exclusion of $Λ$CDM", "categories": ["astro-ph.CO"], "comment": "6 pages with two figures", "summary": "The DESI collaboration, combining their Baryon Acoustic Oscillation (BAO)\ndata with cosmic microwave background (CMB) anisotropy and supernovae data,\nhave found significant indication against $\\Lambda$CDM cosmology. The\nsignificance of the exclusion of $\\Lambda$CDM can be interpreted entirely as\nsignificance of the detection of the $\\boldsymbol{w_a}$ parameter that measures\nvariation of the dark energy equation of state. We emphasize that DESI's DR2\nexclusion of $\\Lambda$CDM is quoted in the articles for a combination of BAO\nand CMB data with each of three different and overlapping supernovae datasets\n(at 2.8-sigma for Pantheon+, 3.8-sigma for Union3, and 4.2-sigma for DESY5). We\nshow that one can neither choose amongst nor average over these three different\nsignificances. We demonstrate how a principled statistical combination yields a\ncombined exclusion significance of 3.1-sigma. Further we argue that, based on\navailable knowledge, and faced with these competing significances, the most\nsecure inference from the DESI DR2 results is the 3.1-sigma level exclusion of\n$\\Lambda$CDM, obtained from combining DESI+CMB alone, while omitting\nsupernovae."}
{"id": "2504.15340", "pdf": "https://arxiv.org/pdf/2504.15340", "abs": "https://arxiv.org/abs/2504.15340", "authors": ["Shouvik Roy Choudhury"], "title": "Cosmology in Extended Parameter Space with DESI DR2 BAO: A 2$σ$+ Detection of Non-zero Neutrino Masses with an Update on Dynamical Dark Energy and Lensing Anomaly", "categories": ["astro-ph.CO", "hep-ph"], "comment": "15 pages, 6 figures", "summary": "We obtain constraints in a 12 parameter cosmological model using the recent\nDESI Data Release (DR) 2 Baryon Acoustic Oscillations (BAO) data, combined with\nCosmic Microwave Background (CMB) power spectra (Planck Public Release (PR) 4)\nand lensing (Planck PR4 + Atacama Cosmology Telescope (ACT) Data Release (DR)\n6) data, uncalibrated type Ia Supernovae (SNe) data from Pantheon+ and Dark\nEnergy Survey (DES) Year 5 (DESY5) samples, and Weak Lensing (WL: DES Year 1)\ndata. The cosmological model consists of six $\\Lambda$CDM parameters, and\nadditionally, the dynamical dark energy parameters ($w_0$, $w_a$), the sum of\nneutrino masses ($\\sum m_{\\nu}$), the effective number of non-photon radiation\nspecies ($N_{\\textrm{eff}}$), the scaling of the lensing amplitude\n($A_{\\textrm{lens}}$), and the running of the scalar spectral index\n($\\alpha_s$). Our major findings are the following: i) With CMB+BAO+DESY5+WL,\nwe obtain the first 2$\\sigma$+ detection of a non-zero $\\sum m_{\\nu} =\n0.19^{+0.15}_{-0.18}$ eV (95%). Replacing DESY5 with Pantheon+ still yields a\n$\\sim$1.9$\\sigma$ detection. ii) The cosmological constant lies at the edge of\nthe 95% contour with CMB+BAO+Pantheon+, but is excluded at 2$\\sigma$+ with\nDESY5, leaving evidence for dynamical dark energy inconclusive, contrary to\nclaims by DESI collaboration. iii) With CMB+BAO+SNe+WL, $A_{\\textrm{lens}} = 1$\nis excluded at $>2\\sigma$, while it remains consistent with unity without WL\ndata - suggesting for the first time that the existence of lensing anomaly may\ndepend on non-CMB datasets. iv) The Hubble tension persists at 3.6-4.2$\\sigma$\nwith CMB+BAO+SNe; WL data has minimal impact."}
{"id": "2504.15348", "pdf": "https://arxiv.org/pdf/2504.15348", "abs": "https://arxiv.org/abs/2504.15348", "authors": ["Sarah Libanore", "Subhajit Ghosh", "Ely D. Kovetz", "Kimberly K. Boddy", "Alvise Raccanelli"], "title": "Joint 21-cm and CMB Forecasts for Constraining Self-Interacting Massive Neutrinos", "categories": ["astro-ph.CO"], "comment": "28 pages, 13 fugures, prepared for submission", "summary": "Self-interacting neutrinos provide an intriguing extension to the Standard\nModel, motivated by both particle physics and cosmology. Recent cosmological\nanalyses suggest a bimodal posterior for the coupling strength $G_{\\rm eff}$,\nfavoring either strong or moderate interactions. These interactions modify the\nscale-dependence of the growth of cosmic structures, leaving distinct imprints\non the matter power spectrum at small scales, $k\\,>\\,0.1\\,{\\rm Mpc}^{-1}$. For\nthe first time, we explore how the 21-cm power spectrum from the cosmic dawn\nand the dark ages can constrain the properties of self-interacting, massive\nneutrinos. The effects of small-scale suppression and enhancement in the matter\npower spectrum caused by self-interacting neutrinos propagate to the halo mass\nfunction, shaping the abundance of small- and intermediate-mass halos. It is\nprecisely these halos that host the galaxies responsible for driving the\nevolution of the 21-cm signal during the cosmic dawn. We find that HERA at its\ndesign sensitivity can improve upon existing constraints on $G_{\\rm eff}$ and\nbe sensitive to small values of the coupling, beyond the reach of current and\nfuture CMB experiments. Crucially, we find that the combination of HERA and\nCMB-S4 can break parameter degeneracies, significantly improving the\nsensitivity to $G_{\\rm eff}$ over either experiment alone. Finally, we\ninvestigate the prospects of probing neutrino properties with futuristic Lunar\ninterferometers, accessing the astrophysics-free 21-cm power spectrum during\nthe dark ages. The capability of probing small scales of these instruments will\nallow us to reach a percent-level constraint on the neutrino self-coupling."}
{"id": "2504.15351", "pdf": "https://arxiv.org/pdf/2504.15351", "abs": "https://arxiv.org/abs/2504.15351", "authors": ["Fabian Schmidt"], "title": "On the Connection between Field-Level Inference and $n$-point Correlation Functions", "categories": ["astro-ph.CO"], "comment": "43 pages", "summary": "Bayesian field-level inference of galaxy clustering guarantees optimal\nextraction of all cosmological information, provided that the data are\ncorrectly described by the forward model employed. The latter is unfortunately\nnever strictly the case. A key question for field-level inference approaches\nthen is where the cosmological information is coming from, and how to ensure\nthat it is robust. In the context of perturbative approaches such as effective\nfield theory, some progress on this question can be made analytically. We\nderive the parameter posterior given the data for the field-level likelihood\ngiven in the effective field theory, marginalized over initial conditions in\nthe zero-noise limit. Particular attention is paid to cutoffs in the theory,\nthe generalization to higher orders, and the error made by an incomplete\nforward model at a given order. The main finding is that, broadly speaking, an\n$m$-th order forward model captures the information in $n$-point correlation\nfunctions with $n \\leqslant m+1$. Thus, by adding more terms to the forward\nmodel, field-level inference is made to automatically incorporate higher-order\n$n$-point functions. Also shown is how the effect of an incomplete forward\nmodel (at a given order) on the parameter inference can be estimated."}
{"id": "2504.15382", "pdf": "https://arxiv.org/pdf/2504.15382", "abs": "https://arxiv.org/abs/2504.15382", "authors": ["Michael Sekatchev", "Xunyu Liang", "Fereshteh Majidi", "Ben Scully", "Ludovic Van Waerbeke", "Ariel Zhitnitsky"], "title": "The Glow of Axion Quark Nugget Dark Matter: (III) The Mysteries of the Milky Way UV Background", "categories": ["astro-ph.CO", "hep-ph"], "comment": "25 pages, 7 figures, prepared for submission to JCAP", "summary": "Axion quark nuggets (AQNs) are hypothetical objects with nuclear density that\nwould have formed during the quark-hadron transition and could make up most of\nthe dark matter today. These objects have a mass greater than a few grams and\nare sub-micrometer in size. They would also help explain the matter-antimatter\nasymmetry and the similarity between visible and dark components of the\nuniverse, i.e. $\\Omega_{\\text{DM}} \\sim \\Omega_{\\text{visible}}$. These\ncomposite objects behave as cold dark matter, interacting with ordinary matter\nand producing pervasive electromagnetic radiation. This work aims to calculate\nthe FUV electromagnetic signature in a 1 kpc region surrounding the solar\nsystem, resulting from the interaction between antimatter AQNs and baryons. To\nthis end, we use the high-resolution hydrodynamic simulation of the Milky Way,\nFIRE-2 Latter suite, to select solar system-like regions. From the simulated\ngas and dark matter distributions in these regions, we calculate the FUV\nbackground radiation generated by the AQN model. We find that the results are\nconsistent with the FUV excess recently confirmed by the Alice spectrograph\naboard New Horizons, which corroborated the FUV excess initially discovered by\nGALEX a decade ago. We also discuss the potential cosmological implications of\nour work, which suggest the existence of a new source of FUV radiation in\ngalaxies, linked to the interaction between dark matter and baryons."}
{"id": "2504.15400", "pdf": "https://arxiv.org/pdf/2504.15400", "abs": "https://arxiv.org/abs/2504.15400", "authors": ["Subhasis Maiti", "Debaprasad Maity", "Rohan Srikanth"], "title": "Minimal Magnetogenesis: The Role of Inflationary Perturbations and ALPs, and Its Gravitational Wave Signatures", "categories": ["astro-ph.CO", "hep-ph", "hep-th"], "comment": "27 pages, 12 figures", "summary": "Any attempt to understand the ubiquitous nature of the magnetic field in the\npresent universe seems to lead us towards its primordial origin. For\nlarge-scale magnetic fields, however, their strength and length scale may not\nnecessarily originate from a singular primordial mechanism, namely inflationary\nmagnetogenesis, which has been a popular consideration in the literature. In\nthis paper, we propose a minimal scenario wherein a large-scale magnetic field\nis generated from the inflationary perturbation without any non-conformal\ncoupling. Due to their origin in the inflationary scalar spectrum, these\nprimordial fields are inherently weak, with their strength suppressed by the\nsmall amplitude of scalar fluctuations. We then consider the coupling between\nthis large-scale weak primordial magnetic field and a light axion of mass\n$<10^{-28}$ eV, which is assumed to be frozen in a misaligned state until the\nphoton decoupling. After the decoupling, when the universe enters into a dark\nage, the light axion coherently oscillates. By appropriately tuning the\naxion-photon coupling parameter $\\alpha$, we demonstrate that a large-scale\nmagnetic field of sufficient strength can indeed be generated through tachyonic\nresonance. We further show that the produced magnetic field induces a unique\nspectrum with multiple peaks of secondary gravitational waves, which the\nupcoming CMB-S4 can probe through B-mode polarization. The strength can be\nsufficient enough to violate the PLANCK bound on tensor-to-scalar ratio $r\n\\lesssim 0.036$. Such a violation leads to a constraint on $\\alpha \\lesssim\n80$. With this limiting value of the coupling, we find that present-day\nmagnetic field strength could be as high as $10^{-10}$ Gauss at $\\Mpc$ scale,\nconsistent with observation."}
{"id": "2504.15506", "pdf": "https://arxiv.org/pdf/2504.15506", "abs": "https://arxiv.org/abs/2504.15506", "authors": ["Paula S. Ferreira", "Carlos Hernández-Monteagudo", "Ribamar R. R. Reis"], "title": "Baryon Acoustic Oscillations in tomographic Angular Density and Redshift Fluctuations", "categories": ["astro-ph.CO"], "comment": "36 pages, 20 figures", "summary": "In this work we examine the baryon acoustic oscillations (BAO) in 2D angular\nand redshift space $\\{\\theta, \\Delta z\\}$, with $\\Delta z$ denoting the\nredshift difference between two given angular shells. We thus work in the\ncontext of tomographic analyses of the large scale structure (LSS) where data\nare sliced in different redshift shells and constraints on Cosmology are\nextracted from the auto and cross-angular spectra of two different probes,\nnamely the standard galaxy angular density fluctuations (ADF, or 2D\nclustering), and the galaxy angular redshift fluctuations (ARF). For these two\nobservables we study by first time how the BAO peak arises in the $\\{\\theta,\n\\Delta z\\}$ plane. Despite being a weak feature (particularly for $\\Delta z\n\\neq 0$), a Fisher forecast analysis shows that, a priori, most of the\ninformation on cosmological and galaxy bias parameters is carried by the BAO\nfeatures in shell auto- and cross-angular power spectra. The same study shows\nthat a joint probe analysis (ADF+ARF) increases the Fisher determinant\nassociated to cosmological parameters such as $H_0$ or the Dark Energy\nChevallier-Polarski-Linder (CPL) parameters $\\{w_0,w_a\\}$ by at least an order\nof magnitude. We also study how the Fisher information on cosmological and\ngalaxy bias-related parameters behaves under different redshift shell\nconfigurations: including cross-correlations to neighbour shells extending up\nto $(\\Delta z)^{\\rm tot}\\sim 0.6$ ($(\\Delta z)^{\\rm tot}\\sim 0.4$) for ADF\n(ARF) is required for Fisher information to converge. At the same time,\nconfigurations using narrow shell widths ($\\sigma_z \\leq 0.02$) preserve the\ncosmological information associated to peculiar velocities and typically yield\nFisher determinants that are about two orders of magnitudes larger than for\nwider shell ($\\sigma_z>0.02$) configurations."}
{"id": "2504.15633", "pdf": "https://arxiv.org/pdf/2504.15633", "abs": "https://arxiv.org/abs/2504.15633", "authors": ["Matthieu Schaller", "Joop Schaye"], "title": "An analytic redshift-independent formulation of baryonic effects on the matter power spectrum", "categories": ["astro-ph.CO"], "comment": "8 pages, 6 figures, submitted to MNRAS, comments welcome", "summary": "Baryonic effects created by feedback processes associated with galaxy\nformation are an important, poorly constrained systematic effect for models of\nlarge-scale structure as probed by weak gravitational lensing. Upcoming surveys\nrequire fast methods to predict and marginalize over the potential impact of\nbaryons on the total matter power spectrum. Here we use the FLAMINGO\ncosmological hydrodynamical simulations to test a recent proposal to\napproximate the matter power spectrum as the sum of the linear matter power\nspectrum and a constant multiple, $A_{\\rm mod}$, of the difference between the\nlinear and non-linear gravity-only power spectra. We show that replacing this\nconstant multiple with a one-parameter family of sigmoid functions of the\nwavenumber $k$ allows to us match the predictions of simulations with different\nfeedback strengths for $z \\leq 1, k < 3~h\\cdot{\\rm Mpc}^{-1}$, and the\ndifferent cosmological models in the FLAMINGO suite. The baryonic response\npredicted by FLAMINGO models that use jet-like AGN feedback instead of the\nfiducial thermally-driven AGN feedback can also be reproduced, but at the cost\nof increasing the number of parameters in the sigmoid function from one to\nthree. The assumption that $A_{\\rm mod}$ depends only on $k$ breaks down for\ndecaying dark matter models, highlighting the need for more advanced baryon\nresponse models when studying cosmological models that deviate strongly from\n$\\Lambda$CDM."}
{"id": "2504.15635", "pdf": "https://arxiv.org/pdf/2504.15635", "abs": "https://arxiv.org/abs/2504.15635", "authors": ["Deng Wang"], "title": "Questioning Cosmic Acceleration with DESI: The Big Stall of the Universe", "categories": ["astro-ph.CO", "astro-ph.HE", "gr-qc"], "comment": "7 pages, 3 figures", "summary": "One of the most important discoveries in modern cosmology is cosmic\nacceleration. However, we find that today's universe could decelerate in the\nstatistically preferred Chevallier-Polarski-Linder (CPL) scenario over the\n$\\Lambda$CDM model by cosmic microwave background, type Ia supernova and DESI's\nnew measurements of baryon acoustic oscillations. Using various datasets, at a\nbeyond $5\\,\\sigma$ confidence level, we demonstrate that the universe\nexperiences a triple deceleration during its evolution and finally reaches the\nstate of the ``Big Stall\", which predicts that: (i) the universe suddenly comes\nto a halt in the distant future; (ii) its eventual destiny is dominated by dark\nmatter rather than dark energy ; (iii) it ultimately retains an extremely small\nfraction of dark energy but exerts an extremely large pressure. Our findings\nprofoundly challenge the established understanding of cosmic acceleration and\nenrich our comprehension of cosmic evolution."}
{"id": "2504.15739", "pdf": "https://arxiv.org/pdf/2504.15739", "abs": "https://arxiv.org/abs/2504.15739", "authors": ["Dylan Kuhn", "Marc Betoule"], "title": "Development of an Ultra-fast, Likelihood-based, Distance Inference Framework for the Next Generation of Type Ia Supernova Surveys", "categories": ["astro-ph.CO"], "comment": "5 pages, 3 figures, contribution to the 2024 Cosmology session of the\n  58th Rencontres de Moriond", "summary": "In this work, we present EDRIS (French for Distance Estimator for Incomplete\nSupernova Surveys), a cosmological inference framework tailored to reconstruct\nunbiased cosmological distances from type Ia supernovae light-curve parameters.\nThis goal is achieved by including data truncation directly in the statistical\nmodel which takes care of the standardization of luminosity distances. It\nallows us to build a single-step distance estimate by maximizing the\ncorresponding likelihood, free from the biases the survey detection limits\nwould introduce otherwise. Moreover, we expect the current worldwide statistics\nto be multiplied by O(10) in the upcoming years. This provides a new challenge\nto handle as the cosmological analysis must stay computationally towable. We\nshow that the optimization methods used in EDRIS allow for a reasonable time\ncomplexity of O($N^2$) resulting in a very fast inference process (O(10s) for\n1500 supernovae)."}
{"id": "2504.16059", "pdf": "https://arxiv.org/pdf/2504.16059", "abs": "https://arxiv.org/abs/2504.16059", "authors": ["Debottam Nandi", "Rohan Roy", "Simran Yadav", "Arnab Sarkar"], "title": "Sub-Horizon Amplification of Curvature Perturbations: A New Route to Primordial Black Holes and Gravitational Waves", "categories": ["astro-ph.CO", "gr-qc", "hep-ph", "hep-th"], "comment": "32 pages, 4 figures", "summary": "The enhanced primordial scalar power spectrum is a widely studied mechanism\nfor generating primordial gravitational waves (PGWs), also referred to as\nscalar-induced gravitational waves (SIGWs). This process also plays a pivotal\nrole in facilitating the formation of primordial black holes (PBHs).\nTraditionally, the ultra slow-roll (USR) mechanism has been the predominant\napproach used in the early universe. In this framework, the second slow-roll\nparameter $\\epsilon_2$, is typically set to $-6$ or lower for a brief period --\nmarking a significant departure from the standard slow-roll condition where\n$\\epsilon_2 \\simeq 0$. Such conditions often emerge in models with inflection\npoints or localized features, such as bumps in the potential. In this paper, we\nchallenge the conventional assumption that $\\epsilon_2 \\lesssim -6$ is a\nprerequisite for substantial amplification of the scalar power spectrum. We\ndemonstrate that any negative value of the second slow-roll parameter can\nindeed enhance the scalar power spectrum through sub-horizon growth,\nestablishing this as a necessary and sufficient condition for amplification.\nConsequently, this mechanism facilitates the generation of both PGWs and PBHs.\nTo illustrate this, we examine a standard scenario where a brief USR phase is\nembedded between two slow-roll (SR) phases. By systematically varying\n$\\epsilon_{2}$ values from $-1$ to $-10$ in the USR region, we investigate the\namplification of the power spectrum and its implications for PGWs and PBHs\nproduction, particularly in the context of ongoing and future cosmological\nmissions."}
{"id": "2504.16069", "pdf": "https://arxiv.org/pdf/2504.16069", "abs": "https://arxiv.org/abs/2504.16069", "authors": ["Minxi He", "Muzi Hong", "Kyohei Mukaida"], "title": "Increase of $n_s$ in regularized pole inflation & Einstein-Cartan gravity", "categories": ["astro-ph.CO", "hep-ph"], "comment": null, "summary": "We show that the regularization of the second order pole in the pole\ninflation can induce the increase of $n_s$, which may be important after the\nlatest data release of cosmic microwave background (CMB) observation by Atacama\nCosmology Telescope (ACT). Pole inflation is known to provide a unified\ndescription of attractor models that they can generate a flat plateau for\ninflation given a general potential. Recent ACT observation suggests that the\nconstraint on the scalar spectral index $n_s$ at CMB scale may be shifted to a\nlarger value than the predictions in the Starobinsky model, the Higgs\ninflation, and the $\\alpha$-attractor model, which motivates us to consider the\nmodification of the pole inflation. We find that if we regularize the second\norder pole in the kinetic term such that the kinetic term becomes regular for\nall field range, we can generally increase $n_s$ because the potential in the\nlarge field regime will be lifted. We have explicitly demonstrated that this\ntype of regularized pole inflation can naturally arise from the Einstein-Cartan\nformalism, and the inflationary predictions are consistent with the latest ACT\ndata without spoiling the success of the $\\alpha$-attractor models."}
{"id": "2504.16076", "pdf": "https://arxiv.org/pdf/2504.16076", "abs": "https://arxiv.org/abs/2504.16076", "authors": ["Claire Lamman", "Jonathan Blazek", "Daniel J. Eisenstein"], "title": "Optimal intrinsic alignment estimators in the presence of redshift-space distortions", "categories": ["astro-ph.CO"], "comment": "For an accessible summary of this paper, see\n  https://cmlamman.github.io/doc/optimalRSDstats-annotation.pdf", "summary": "We present estimators for quantifying intrinsic alignments in large\nspectroscopic surveys that efficiently capture line-of-sight (LOS) information\nwhile being relatively insensitive to redshift-space distortions (RSD). We\ndemonstrate that changing the LOS integration range, {\\Pi}max, as a function of\ntransverse separation outperforms the conventional choice of a single {\\Pi}max\nvalue. This is further improved by replacing the flat {\\Pi}max cut with a LOS\nweighting based on shape projection and RSD. Although these estimators\nincorporate additional LOS information, they are projected correlations that\nexhibit signal-to-noise ratios comparable to 3D correlation functions, such as\nthe IA quadrupole. Using simulations from Abacus Summit, we evaluate these\nestimators and provide recommended {\\Pi}max values and weights for projected\nseparations of 1 - 100 Mpc/h. These will improve measurements of intrinsic\nalignments in large cosmological surveys and the constraints they provide for\nboth weak lensing and direct cosmological applications."}
{"id": "2504.07559", "pdf": "https://arxiv.org/pdf/2504.07559", "abs": "https://arxiv.org/abs/2504.07559", "authors": ["Atsushi Nishizawa", "Atsushi Taruya", "Yoshiaki Himemoto"], "title": "Axion dark matter search from terrestrial magnetic fields at extremely low frequencies", "categories": ["hep-ph", "astro-ph.CO"], "comment": "19 pages, 12 figures, minor changes", "summary": "The natural environment of the Earth can act as a sensitive detector for dark\nmatter in ultralight axions. When axions with masses between\n$1\\times10^{-15}\\,{\\rm eV}$ and $1\\times10^{-13}\\,{\\rm eV}$ pass through the\nEarth, they interact with the global geomagnetic field, generating\nelectromagnetic (EM) waves in the extremely low-frequency range\n($0.3$--$30\\,{\\rm Hz}$) through axion-photon coupling. This paper is one of a\nseries of companion papers for~\\cite{Taruya:2025zql}, focusing on the data\nanalysis method and search results for an axion signal. Utilizing the\ntheoretical predictions of axion-induced EM spectra from a companion study, we\nanalyzed long-term observational data of terrestrial magnetic fields in this\nfrequency band to search for axion-induced signals. Our analysis identified 65\npersistent signal candidates with a signal-to-noise ratio (SNR) greater than 3.\nAside from these candidates, we placed a new upper bound on the axion-photon\ncoupling parameter, significantly refining the previous constraint from CAST by\nat most two orders of magnitude down to $g_{a\\gamma} \\lesssim 4\\times10^{-13}\n\\,{\\rm GeV}^{-1}$ for the axion mass around $3 \\times 10^{-14}\\,{\\rm eV}$."}
{"id": "2504.14799", "pdf": "https://arxiv.org/pdf/2504.14799", "abs": "https://arxiv.org/abs/2504.14799", "authors": ["Anupam Ray"], "title": "Detecting Dark Matter with Neutron Stars", "categories": ["hep-ph", "astro-ph.CO"], "comment": "Contribution to the 2025 Electroweak session of the 59th Rencontres\n  de Moriond, La Thuile, Italy, March 23-30, 2025", "summary": "Neutron stars offer powerful astrophysical laboratories to probe the\nproperties of dark matter. Gradual accumulation of heavy, non-annihilating dark\nmatter in neutron stars can lead to the formation of comparable-mass black\nholes, and non-detection of gravitational waves from mergers of such low-mass\nblack holes can constrain such dark matter interactions with nucleons. These\nconstraints, though dependent on the currently uncertain binary neutron star\nmerger rate density, are significantly more stringent than those from direct\ndetection experiments and provide some of the strongest limits on heavy,\nnon-annihilating dark matter interactions. Additionally, dark matter with\nbaryon number-violating interactions can induce excess heating in cold neutron\nstars and is thus significantly constrained by thermal observations of cold\nneutron stars."}
{"id": "2504.15345", "pdf": "https://arxiv.org/pdf/2504.15345", "abs": "https://arxiv.org/abs/2504.15345", "authors": ["Tirso Marin-Gilabert", "Max Gronke", "S. Peng Oh"], "title": "The (Limited) Effect of Viscosity in Multiphase Turbulent Mixing", "categories": ["astro-ph.GA", "astro-ph.CO"], "comment": "18 pages, 20 figures, submitted to Astronomy and Astrophysics", "summary": "Multiphase gas can be found in many astrophysical environments, such as\ngalactic outflows, stellar wind bubbles, and the circumgalactic medium, where\nthe interplay between turbulence, cooling, and viscosity can significantly\ninfluence gas dynamics and star formation processes. We investigate the role of\nviscosity in modulating turbulence and radiative cooling in turbulent radiative\nmixing layers (TRMLs). In particular, we aim to determine how different amounts\nof viscosity affect the Kelvin-Helmholtz instability (KHI), turbulence\nevolution, and the efficiency of gas mixing and cooling. Using idealized 2D\nnumerical setups, we compute the critical viscosity required to suppress the\nKHI in shear flows characterized by different density contrasts and Mach\nnumbers. These results are then used in a 3D shear layer setup to explore the\nimpact of viscosity on cooling efficiency and turbulence across different\ncooling regimes. We find that the critical viscosity follows the expected\ndependence on overdensity and Mach number. Our viscous TRMLs simulations show\ndifferent behaviors in the weak and strong cooling regimes. In the weak cooling\nregime, viscosity has a strong impact, resulting in laminar flows and breaking\npreviously established inviscid relations between cooling and turbulence\n(albeit leaving the total luminosity unaffected). However, in the strong\ncooling regime, when cooling timescales are shorter than viscous timescales,\nkey scaling relations in TRMLs remain largely intact. In this regime -- which\nmust hold for gas to remain multiphase -- radiative losses dominate, and the\nsystem effectively behaves as non-viscous regardless of the actual level of\nviscosity. Our findings have direct implications for both the interpretation of\nobservational diagnostics and the development of subgrid models in large-scale\nsimulations."}
{"id": "2504.15374", "pdf": "https://arxiv.org/pdf/2504.15374", "abs": "https://arxiv.org/abs/2504.15374", "authors": ["Zafri A. Borboruah", "Lekhika Malhotra", "Frank F. Deppisch", "Anish Ghoshal"], "title": "Inflationary Gravitational Waves and Laboratory Searches as Complementary Probes of Right-handed Neutrinos", "categories": ["hep-ph", "astro-ph.CO"], "comment": "17 pages including references, 10 figures including sub-figures", "summary": "We analyze the damping of inflationary gravitational waves (GW) that re-enter\nthe Hubble horizon before or during a post-inflationary era dominated by a\nmeta-stable, right-handed neutrino (RHN), whose out-of-equilibrium decay\nreleases entropy. Within a minimal type-I seesaw extension of the Standard\nModel (SM), we explore the conditions under which the population of thermally\nproduced RHNs remain long-lived and cause a period of matter-domination. We\nfind that the suppression of the GW spectrum occurs above a characteristic\nfrequency determined by the RHN mass and active-sterile mixing. For RHN masses\nin the range $0.1$ - $10$ GeV and mixing $10^{-12} \\lesssim |V_{eN}|^2 \\lesssim\n10^{-5}$, we estimate such characteristic frequencies and the signal-to-noise\nratio to assess the detection prospects in GW observatories such as THEIA,\n$\\mu$-ARES, LISA, BBO and ET. We find complementarity between GW signals and\nlaboratory searches in SHiP, DUNE and LEGEND-1000. Notably, RHN masses of $0.2$\n- $2$ GeV and mixing $10^{-10} \\lesssim |V_{eN}|^2 \\lesssim 10^{-7}$ are\ntestable in both laboratory experiments and GW observations. Additionally, GW\nexperiments can probe the canonical seesaw regime of light neutrino mass\ngeneration, a region largely inaccessible to laboratory searches."}
{"id": "2504.15606", "pdf": "https://arxiv.org/pdf/2504.15606", "abs": "https://arxiv.org/abs/2504.15606", "authors": ["Asuka Ito", "Rudnei O. Ramos"], "title": "Warm multi natural inflation", "categories": ["hep-ph", "astro-ph.CO", "gr-qc", "hep-th"], "comment": "13 pages, 5 figures, 2 tables", "summary": "Multi natural inflation is studied in the context of warm inflation. We study\nthe warm multi natural inflation scenario with both linear and cubic\ndissipation coefficients. The model is motivated by axion-like inflation models\nwith coupling to non-Abelian gauge fields through a dimension five coupling and\ndissipation originating from sphaleron decay in a thermal bath. Both cases of\ndissipation coefficients can be compatible with current observations. In the\ncase of the cubic dissipation coefficient, we find that the curvature\nperturbation starts to grow suddenly when a transition from a weak dissipation\nto a strong dissipation regime occurs at the later stage of the inflation. We\nalso show that such rapid growth of the curvature perturbation on small scales\ngives rise to abundant scalar induced gravitational waves, which may be\ndetectable with future gravitational wave detectors such as DECIGO and ET. On\nthe other hand, there are also other parameter regions of the model, in the\nwarm inflation regime of weak to strong dissipation and with sub-Planckian\naxion decay constant, that can lead to overproduction of primordial black holes\non small scales, which are constrained by nucleosynthesis bounds, thus ruling\nout the model in this region of parameters."}
{"id": "2504.15802", "pdf": "https://arxiv.org/pdf/2504.15802", "abs": "https://arxiv.org/abs/2504.15802", "authors": ["Ellis R. Owen", "Yoshiyuki Inoue", "Tatsuki Fujiwara", "Qin Han", "Kinwah Wu"], "title": "Cosmic ray neutrons in magnetized astrophysical structures", "categories": ["astro-ph.HE", "astro-ph.CO", "astro-ph.GA"], "comment": "10 pages, 3 figures, proceedings of the 28th European Cosmic Ray\n  Symposium (ECRS2024)", "summary": "Cosmic rays are often modeled as charged particles. This allows their\nnon-ballistic propagation in magnetized structures to be captured. In certain\nsituations, a neutral cosmic ray component can arise. For example, cosmic ray\nneutrons are produced in considerable numbers through hadronic pp and p$\\gamma$\ninteractions. At ultrahigh energies, the decay timescales of these neutrons is\ndilated, allowing them to traverse distances on the scale of galactic and\ncosmological structures. Unlike charged cosmic rays, neutrons are not deflected\nby magnetic fields. They propagate ballistically at the speed of light in\nstraight lines. The presence of a neutral baryonic cosmic ray component formed\nin galaxies, clusters and cosmological filaments can facilitate the escape and\nleakage of cosmic rays from magnetic structures that would otherwise confine\nthem. We show that, by allowing confinement breaking, the formation of\ncosmic-ray neutrons by high-energy hadronic interactions in large scale\nastrophysical structures can modify the exchange of ultra high-energy particles\nacross magnetic interfaces between galaxies, clusters, cosmological filaments\nand voids."}
{"id": "2504.15821", "pdf": "https://arxiv.org/pdf/2504.15821", "abs": "https://arxiv.org/abs/2504.15821", "authors": ["Eugene Hyeonmin Lee", "Joohyun Lee", "Paul R. Shapiro", "Pierre Ocvirk", "Joseph S. W. Lewis", "Taha Dawoodbhoy", "Ilian T. Iliev", "Luke Conaboy", "Kyungjin Ahn", "Hyunbae Park", "Jenny G. Sorce", "Dominique Aubert", "Romain Teyssier", "Gustavo Yepes", "Yohan Dubois", "Stefan Gottlöber"], "title": "Line Intensity Mapping Prediction from the Cosmic Dawn (CoDa) III Simulation for H$α$ from Galaxies and the Intergalactic Medium during the Epoch of Reionization", "categories": ["astro-ph.GA", "astro-ph.CO"], "comment": "4 pages, 1 figure, to appear in RNAAS", "summary": "The evolution of large-scale structure, galaxies and the intergalactic medium\n(IGM) during the Epoch of Reionization (EoR) can be probed by upcoming Line\nIntensity Mapping (LIM) experiments, which sample in redshift and direction\nwithout needing to resolve individual galaxies. We predict the intensity and\nsources of hydrogen H$\\alpha$ emission, dominated by radiative recombination\nfollowing ionization by UV from the same massive stars that caused\nreionization, down to redshift 4.6, using the largest fully-coupled,\nradiation-hydro simulation of galaxy formation and reionization to date, Cosmic\nDawn (CoDa) III. We compute the mean intensity and Voxel Intensity Distribution\n(VID) vs. redshift, including the relative contributions of galaxies and IGM.\nThis will provide mock data to guide and interpret LIM experiments such as\nNASA's SPHEREx and proposed Cosmic Dawn Intensity Mapper (CDIM)."}
{"id": "2504.15825", "pdf": "https://arxiv.org/pdf/2504.15825", "abs": "https://arxiv.org/abs/2504.15825", "authors": ["Antonio Troisi", "Sante Carloni"], "title": "Bounce Cosmologies in Generalized Coupling Theories", "categories": ["gr-qc", "astro-ph.CO"], "comment": null, "summary": "We describe an exact solution representing a bouncing cosmology in the\nMinimal Exponential Measure (MEMe) model. Such a solution, obtained by means of\nthe linearization around small values of the characteristic energy scale q of\nthe theory, has the peculiarity of representing a complete bounce model that\ncan be used to explore quantitative processes in non-singular cosmologies."}
{"id": "2504.15902", "pdf": "https://arxiv.org/pdf/2504.15902", "abs": "https://arxiv.org/abs/2504.15902", "authors": ["Olivia Curtis", "Bryanne McDonough", "Tereasa Brainerd"], "title": "Density Profiles of TNG300 Voids across Cosmic Time", "categories": ["astro-ph.GA", "astro-ph.CO"], "comment": "Accepted for publication in ApJ. 40 pages, 11 figures, 2 tables", "summary": "We present radial density profiles, as traced by luminous galaxies and dark\nmatter particles, for voids in eleven snapshots of the \\texttt{TNG300}\nsimulation. The snapshots span 11.65~Gyr of cosmic time, corresponding to the\nredshift range $0 \\le z \\le 3$. Using the comoving galaxy fields, voids were\nidentified via a well-tested, watershed transformation-based algorithm. Voids\nwere defined to be underdense regions that are unlikely to have arisen from\nPoisson noise, resulting in the selection of $\\sim100-200$ of the largest\nunderdense regions in each snapshot. At all redshifts, the radial density\nprofiles as traced by both the galaxies and the dark matter resemble inverse\ntop-hat functions. However, details of the functions (particularly the\nunderdensities of the innermost regions and the overdensities of the ridges)\nevolve considerably more for the dark matter density profiles than for the\ngalaxy density profiles. At all redshifts, a linear relationship between the\ngalaxy and dark matter density profiles exists, and the slope of the\nrelationship is similar to the bias estimates for \\texttt{TNG300} snapshots.\nLastly, we identify distinct environments in which voids can exist, defining\n``void-in-void\" and ``void-in-cloud\" populations (i.e., voids that reside in\nlarger underdense or overdense regions, respectively) and we investigate ways\nin which the relative densities of dark matter and galaxies in the interiors\nand ridges of these structures vary as a function of void environment."}
{"id": "2504.15913", "pdf": "https://arxiv.org/pdf/2504.15913", "abs": "https://arxiv.org/abs/2504.15913", "authors": ["Roberto J. Assef", "Marko Stalevski", "Lee Armus", "Franz E. Bauer", "Andrew Blain", "Murray Brightman", "Tanio Díaz-Santos", "Peter R. M. Eisenhardt", "Román Fernández-Aranda", "Hyunsung D. Jun", "Mai Liao", "Guodong Li", "Lee R. Martin", "Elena Shablovinskaia", "Devika Shobhana", "Daniel Stern", "Chao-Wei Tsai", "Andrey Vayner", "Dominic J. Walton", "Jingwen Wu", "Dejene Zewdie"], "title": "A Massive Gas Outflow Outside the Line-of-Sight: Imaging Polarimetry of the Blue Excess Hot Dust Obscured Galaxy W0204-0506", "categories": ["astro-ph.GA", "astro-ph.CO"], "comment": "8 pages, 4 figures, 1 table. Submitted to A&A", "summary": "(Aims) Hot Dust Obscured Galaxies (Hot DOGs) are a population of\nhyper-luminous, heavily obscured quasars. Although nuclear obscurations close\nto Compton-thick are typical, a fraction show blue UV spectral energy\ndistributions consistent with unobscured quasar activity, albeit two orders of\nmagnitude fainter than expected from their mid-IR luminosity. The origin of the\nUV emission in these Blue excess Hot DOGs (BHDs) has been linked to scattered\nlight from the central engine. Here we study the properties of the UV emission\nin the BHD WISE J020446.13-050640.8 (W0204-0506). (Methods) We use imaging\npolarization observations in the $R_{\\rm Special}$ band obtained with the FORS2\ninstrument at VLT. We compare these data with radiative transfer simulations to\nconstrain the characteristics of the scattering material. (Results) We find a\nspatially integrated polarization fraction of $24.7\\pm 0.7$%, confirming the\nscattered-light nature of the UV emission of W0204-0506. The source is\nspatially resolved in the observations and we find a gradient in polarization\nfraction and angle that is aligned with the extended morphology of the source\nfound in HST/WFC3 imaging. A dusty, conical polar outflow starting at the AGN\nsublimation radius with a half-opening angle $\\lesssim 50~\\rm deg$ viewed at an\ninclination $\\gtrsim 45~\\rm deg$ can reproduce the observed polarization\nfraction if the dust is graphite-rich. We find that the gas mass and outflow\nvelocity are consistent with the range of values found for [OIII] outflows\nthrough spectroscopy in other Hot DOGs, though it is unclear whether the\noutflow is energetic enough to affect the long-term evolution of the host\ngalaxy. Our study highlights the unique potential for polarization imaging to\nstudy dusty quasar outflows, providing complementary constraints to those\nobtained through traditional spectroscopic studies."}
{"id": "2504.15309", "pdf": "https://arxiv.org/pdf/2504.15309", "abs": "https://arxiv.org/abs/2504.15309", "authors": ["Anran Yu", "Wei Feng", "Yaochen Zhang", "Xiang Li", "Lei Meng", "Lei Wu", "Xiangxu Meng"], "title": "LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The personalized text-to-image generation has rapidly advanced with the\nemergence of Stable Diffusion. Existing methods, which typically fine-tune\nmodels using embedded identifiers, often struggle with insufficient stylization\nand inaccurate image content due to reduced textual controllability. In this\npaper, we propose style refinement and content preservation strategies. The\nstyle refinement strategy leverages the semantic information of visual\nreasoning prompts and reference images to optimize style embeddings, allowing a\nmore precise and consistent representation of style information. The content\npreservation strategy addresses the content bias problem by preserving the\nmodel's generalization capabilities, ensuring enhanced textual controllability\nwithout compromising stylization. Experimental results verify that our approach\nachieves superior performance in generating consistent and personalized\ntext-to-image outputs."}
{"id": "2504.15362", "pdf": "https://arxiv.org/pdf/2504.15362", "abs": "https://arxiv.org/abs/2504.15362", "authors": ["Yuan-Hong Liao", "Sven Elflein", "Liu He", "Laura Leal-Taixé", "Yejin Choi", "Sanja Fidler", "David Acuna"], "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "24 pages, 10 figures, in submission. Project page:\n  https://andrewliao11.github.io/LongPerceptualThoughts", "summary": "Recent reasoning models through test-time scaling have demonstrated that long\nchain-of-thoughts can unlock substantial performance boosts in hard reasoning\ntasks such as math and code. However, the benefit of such long thoughts for\nsystem-2 reasoning is relatively less explored in other domains such as\nperceptual tasks where shallower, system-1 reasoning seems sufficient. In this\npaper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K\nlong-thought traces for perceptual tasks. The key challenges in synthesizing\nelaborate reasoning thoughts for perceptual tasks are that off-the-shelf models\nare not yet equipped with such thinking behavior and that it is not\nstraightforward to build a reliable process verifier for perceptual tasks.\nThus, we propose a novel three-stage data synthesis framework that first\nsynthesizes verifiable multiple-choice questions from dense image descriptions,\nthen extracts simple CoTs from VLMs for those verifiable problems, and finally\nexpands those simple thoughts to elaborate long thoughts via frontier reasoning\nmodels. In controlled experiments with a strong instruction-tuned 7B model, we\ndemonstrate notable improvements over existing visual reasoning data-generation\nmethods. Our model, trained on the generated dataset, achieves an average +3.4\npoints improvement over 5 vision-centric benchmarks, including +11.8 points on\nV$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves\nperformance on the text reasoning benchmark, MMLU-Pro, by +2 points."}
{"id": "2504.15371", "pdf": "https://arxiv.org/pdf/2504.15371", "abs": "https://arxiv.org/abs/2504.15371", "authors": ["Wei Fang", "Priyadarshini Panda"], "title": "Event2Vec: Processing neuromorphic events directly by representations in vector space", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "The neuromorphic event cameras have overwhelming advantages in temporal\nresolution, power efficiency, and dynamic range compared to traditional\ncameras. However, the event cameras output asynchronous, sparse, and irregular\nevents, which are not compatible with mainstream computer vision and deep\nlearning methods. Various methods have been proposed to solve this issue but at\nthe cost of long preprocessing procedures, losing temporal resolutions, or\nbeing incompatible with massively parallel computation. Inspired by the great\nsuccess of the word to vector, we summarize the similarities between words and\nevents, then propose the first event to vector (event2vec) representation. We\nvalidate event2vec on classifying the ASL-DVS dataset, showing impressive\nparameter efficiency, accuracy, and speed than previous graph/image/voxel-based\nrepresentations. Beyond task performance, the most attractive advantage of\nevent2vec is that it aligns events to the domain of natural language\nprocessing, showing the promising prospect of integrating events into large\nlanguage and multimodal models. Our codes, models, and training logs are\navailable at https://github.com/fangwei123456/event2vec."}
{"id": "2504.15376", "pdf": "https://arxiv.org/pdf/2504.15376", "abs": "https://arxiv.org/abs/2504.15376", "authors": ["Zhiqiu Lin", "Siyuan Cen", "Daniel Jiang", "Jay Karhade", "Hewei Wang", "Chancharik Mitra", "Tiffany Ling", "Yuhan Huang", "Sifan Liu", "Mingyu Chen", "Rushikesh Zawar", "Xue Bai", "Yilun Du", "Chuang Gan", "Deva Ramanan"], "title": "Towards Understanding Camera Motions in Any Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project site: https://linzhiqiu.github.io/papers/camerabench/", "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video."}
{"id": "2504.15304", "pdf": "https://arxiv.org/pdf/2504.15304", "abs": "https://arxiv.org/abs/2504.15304", "authors": ["Kangyu Wang"], "title": "Can Machine Learning Agents Deal with Hard Choices?", "categories": ["cs.AI"], "comment": "22 pages excluding bibliography, 27 pagas including bibliography, 3\n  figures", "summary": "Machine Learning ML agents have been increasingly used in decision-making\nacross a wide range of tasks and environments. These ML agents are typically\ndesigned to balance multiple objectives when making choices. Understanding how\ntheir decision-making processes align with or diverge from human reasoning is\nessential. Human agents often encounter hard choices, that is, situations where\noptions are incommensurable; neither option is preferred, yet the agent is not\nindifferent between them. In such cases, human agents can identify hard choices\nand resolve them through deliberation. In contrast, current ML agents, due to\nfundamental limitations in Multi-Objective Optimisation or MOO methods, cannot\nidentify hard choices, let alone resolve them. Neither Scalarised Optimisation\nnor Pareto Optimisation, the two principal MOO approaches, can capture\nincommensurability. This limitation generates three distinct alignment\nproblems: the alienness of ML decision-making behaviour from a human\nperspective; the unreliability of preference-based alignment strategies for\nhard choices; and the blockage of alignment strategies pursuing multiple\nobjectives. Evaluating two potential technical solutions, I recommend an\nensemble solution that appears most promising for enabling ML agents to\nidentify hard choices and mitigate alignment problems. However, no known\ntechnique allows ML agents to resolve hard choices through deliberation, as\nthey cannot autonomously change their goals. This underscores the\ndistinctiveness of human agency and urges ML researchers to reconceptualise\nmachine autonomy and develop frameworks and methods that can better address\nthis fundamental gap."}
{"id": "2504.15378", "pdf": "https://arxiv.org/pdf/2504.15378", "abs": "https://arxiv.org/abs/2504.15378", "authors": ["Scott Sorensen", "Wayne Treible", "Robert Wagner", "Andrew D. Gilliam", "Todd Rovito", "Joseph L. Mundy"], "title": "Physics Driven Image Simulation from Commercial Satellite Imagery", "categories": ["cs.CV"], "comment": "15 pages, 9 figures", "summary": "Physics driven image simulation allows for the modeling and creation of\nrealistic imagery beyond what is afforded by typical rendering pipelines. We\naim to automatically generate a physically realistic scene for simulation of a\ngiven region using satellite imagery to model the scene geometry, drive\nmaterial estimates, and populate the scene with dynamic elements. We present\nautomated techniques to utilize satellite imagery throughout the simulated\nscene to expedite scene construction and decrease manual overhead. Our\ntechnique does not use lidar, enabling simulations that could not be\nconstructed previously. To develop a 3D scene, we model the various components\nof the real location, addressing the terrain, modelling man-made structures,\nand populating the scene with smaller elements such as vegetation and vehicles.\nTo create the scene we begin with a Digital Surface Model, which serves as the\nbasis for scene geometry, and allows us to reason about the real location in a\ncommon 3D frame of reference. These simulated scenes can provide increased\nfidelity with less manual intervention for novel locations on earth, and can\nfacilitate algorithm development, and processing pipelines for imagery ranging\nfrom UV to LWIR $(200nm-20\\mu m)$."}
{"id": "2504.15313", "pdf": "https://arxiv.org/pdf/2504.15313", "abs": "https://arxiv.org/abs/2504.15313", "authors": ["Yajie Yu", "Yue Feng"], "title": "PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agents has exhibited significant intelligence in real-word simulations\nwith Large language models (LLMs) due to the capabilities of social cognition\nand knowledge retrieval. However, existing research on agents equipped with\neffective cognition chains including reasoning, planning, decision-making and\nreflecting remains limited, especially in the dynamically interactive\nscenarios. In addition, unlike human, prompt-based responses face challenges in\npsychological state perception and empirical calibration during uncertain\ngaming process, which can inevitably lead to cognition bias. In light of above,\nwe introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework\ncharacterized by systematically acquiring intentions of others and adaptively\noptimizing irrational strategies for continual enhancement. Specifically,\nPolicyEvol-Agent first obtains reflective expertise patterns and then\nintegrates a range of cognitive operations with Theory of Mind alongside\ninternal and external perspectives. Simulation results, outperforming RL-based\nmodels and agent-based methods, demonstrate the superiority of PolicyEvol-Agent\nfor final gaming victory. Moreover, the policy evolution mechanism reveals the\neffectiveness of dynamic guideline adjustments in both automatic and human\nevaluation."}
{"id": "2504.15380", "pdf": "https://arxiv.org/pdf/2504.15380", "abs": "https://arxiv.org/abs/2504.15380", "authors": ["Huimin Zeng", "Jiacheng Li", "Zhiwei Xiong"], "title": "Plug-and-Play Versatile Compressed Video Enhancement", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "As a widely adopted technique in data transmission, video compression\neffectively reduces the size of files, making it possible for real-time cloud\ncomputing. However, it comes at the cost of visual quality, posing challenges\nto the robustness of downstream vision models. In this work, we present a\nversatile codec-aware enhancement framework that reuses codec information to\nadaptively enhance videos under different compression settings, assisting\nvarious downstream vision tasks without introducing computation bottleneck.\nSpecifically, the proposed codec-aware framework consists of a\ncompression-aware adaptation (CAA) network that employs a hierarchical\nadaptation mechanism to estimate parameters of the frame-wise enhancement\nnetwork, namely the bitstream-aware enhancement (BAE) network. The BAE network\nfurther leverages temporal and spatial priors embedded in the bitstream to\neffectively improve the quality of compressed input frames. Extensive\nexperimental results demonstrate the superior quality enhancement performance\nof our framework over existing enhancement methods, as well as its versatility\nin assisting multiple downstream tasks on compressed videos as a plug-and-play\nmodule. Code and models are available at\nhttps://huimin-zeng.github.io/PnP-VCVE/."}
{"id": "2504.15360", "pdf": "https://arxiv.org/pdf/2504.15360", "abs": "https://arxiv.org/abs/2504.15360", "authors": ["Javier Fumanal-Idocin", "Javier Andreu-Perez"], "title": "Reliable Classification with Conformal Learning and Interval-Type 2 Fuzzy Sets", "categories": ["cs.AI"], "comment": null, "summary": "Classical machine learning classifiers tend to be overconfident can be\nunreliable outside of the laboratory benchmarks. Properly assessing the\nreliability of the output of the model per sample is instrumental for real-life\nscenarios where these systems are deployed. Because of this, different\ntechniques have been employed to properly quantify the quality of prediction\nfor a given model. These are most commonly Bayesian statistics and, more\nrecently, conformal learning. Given a calibration set, conformal learning can\nproduce outputs that are guaranteed to cover the target class with a desired\nsignificance level, and are more reliable than the standard confidence\nintervals used by Bayesian methods. In this work, we propose to use conformal\nlearning with fuzzy rule-based systems in classification and show some metrics\nof their performance. Then, we discuss how the use of type 2 fuzzy sets can\nimprove the quality of the output of the system compared to both fuzzy and\ncrisp rules. Finally, we also discuss how the fine-tuning of the system can be\nadapted to improve the quality of the conformal prediction."}
{"id": "2504.15384", "pdf": "https://arxiv.org/pdf/2504.15384", "abs": "https://arxiv.org/abs/2504.15384", "authors": ["Chen Zhao", "Anjum Shaik", "Joyce H. Keyak", "Nancy E. Lane", "Jeffrey D. Deng", "Kuan-Jui Su", "Qiuying Sha", "Hui Shen", "Hong-Wen Deng", "Weihua Zhou"], "title": "ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images", "categories": ["cs.CV"], "comment": "23 pages, 4 figures", "summary": "Hip fractures represent a major health concern, particularly among the\nelderly, often leading decreased mobility and increased mortality. Early and\naccurate detection of at risk individuals is crucial for effective\nintervention. In this study, we propose Iterative Cross Graph Matching for Hip\nFracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip\nfractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX\ninvolves iteratively comparing a test (subject) graph with multiple template\ngraphs representing the characteristics of hip fracture subjects to assess the\nsimilarity and accurately to predict hip fracture risk. These graphs are\nobtained as follows. The DXA images are separated into multiple regions of\ninterest (RoIs), such as the femoral head, shaft, and lesser trochanter.\nRadiomic features are then calculated for each RoI, with the central\ncoordinates used as nodes in a graph. The connectivity between nodes is\nestablished according to the Euclidean distance between these coordinates. This\nprocess transforms each DXA image into a graph, where each node represents a\nRoI, and edges derived by the centroids of RoIs capture the spatial\nrelationships between them. If the test graph closely matches a set of template\ngraphs representing subjects with incident hip fractures, it is classified as\nindicating high hip fracture risk. We evaluated our method using 547 subjects\nfrom the UK Biobank dataset, and experimental results show that ICGM-FRAX\nachieved a sensitivity of 0.9869, demonstrating high accuracy in predicting hip\nfractures."}
{"id": "2504.15364", "pdf": "https://arxiv.org/pdf/2504.15364", "abs": "https://arxiv.org/abs/2504.15364", "authors": ["Junyoung Park", "Dalton Jones", "Matt Morse", "Raghavv Goel", "Mingu Lee", "Chris Lott"], "title": "KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments", "categories": ["cs.AI"], "comment": "8 pages, 14 figures", "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."}
{"id": "2504.15397", "pdf": "https://arxiv.org/pdf/2504.15397", "abs": "https://arxiv.org/abs/2504.15397", "authors": ["Ankit Dhiman", "Manan Shah", "R Venkatesh Babu"], "title": "MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page: https://mirror-verse.github.io/", "summary": "Diffusion models have become central to various image editing tasks, yet they\noften fail to fully adhere to physical laws, particularly with effects like\nshadows, reflections, and occlusions. In this work, we address the challenge of\ngenerating photorealistic mirror reflections using diffusion-based generative\nmodels. Despite extensive training data, existing diffusion models frequently\noverlook the nuanced details crucial to authentic mirror reflections. Recent\napproaches have attempted to resolve this by creating synhetic datasets and\nframing reflection generation as an inpainting task; however, they struggle to\ngeneralize across different object orientations and positions relative to the\nmirror. Our method overcomes these limitations by introducing key augmentations\ninto the synthetic data pipeline: (1) random object positioning, (2) randomized\nrotations, and (3) grounding of objects, significantly enhancing generalization\nacross poses and placements. To further address spatial relationships and\nocclusions in scenes with multiple objects, we implement a strategy to pair\nobjects during dataset generation, resulting in a dataset robust enough to\nhandle these complex scenarios. Achieving generalization to real-world scenes\nremains a challenge, so we introduce a three-stage training curriculum to\ndevelop the MirrorFusion 2.0 model to improve real-world performance. We\nprovide extensive qualitative and quantitative evaluations to support our\napproach. The project page is available at: https://mirror-verse.github.io/."}
{"id": "2504.15434", "pdf": "https://arxiv.org/pdf/2504.15434", "abs": "https://arxiv.org/abs/2504.15434", "authors": ["Sarath Shekkizhar", "Romain Cosentino"], "title": "AGI Is Coming... Right After AI Learns to Play Wordle", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This paper investigates multimodal agents, in particular, OpenAI's\nComputer-User Agent (CUA), trained to control and complete tasks through a\nstandard computer interface, similar to humans. We evaluated the agent's\nperformance on the New York Times Wordle game to elicit model behaviors and\nidentify shortcomings. Our findings revealed a significant discrepancy in the\nmodel's ability to recognize colors correctly depending on the context. The\nmodel had a $5.36\\%$ success rate over several hundred runs across a week of\nWordle. Despite the immense enthusiasm surrounding AI agents and their\npotential to usher in Artificial General Intelligence (AGI), our findings\nreinforce the fact that even simple tasks present substantial challenges for\ntoday's frontier AI models. We conclude with a discussion of the potential\nunderlying causes, implications for future development, and research directions\nto improve these AI systems."}
{"id": "2504.15404", "pdf": "https://arxiv.org/pdf/2504.15404", "abs": "https://arxiv.org/abs/2504.15404", "authors": ["Tajamul Ashraf", "Rajes Manna", "Partha Sarathi Purkayastha", "Tavaheed Tariq", "Janibul Bashir"], "title": "Context Aware Grounded Teacher for Source Free Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We focus on the Source Free Object Detection (SFOD) problem, when source data\nis unavailable during adaptation, and the model must adapt to the unlabeled\ntarget domain. In medical imaging, several approaches have leveraged a\nsemi-supervised student-teacher architecture to bridge domain discrepancy.\nContext imbalance in labeled training data and significant domain shifts\nbetween domains can lead to biased teacher models that produce inaccurate\npseudolabels, degrading the student model's performance and causing a mode\ncollapse. Class imbalance, particularly when one class significantly outnumbers\nanother, leads to contextual bias. To tackle the problem of context bias and\nthe significant performance drop of the student model in the SFOD setting, we\nintroduce Grounded Teacher (GT) as a standard framework. In this study, we\nmodel contextual relationships using a dedicated relational context module and\nleverage it to mitigate inherent biases in the model. This approach enables us\nto apply augmentations to closely related classes, across and within domains,\nenhancing the performance of underrepresented classes while keeping the effect\non dominant classes minimal. We further improve the quality of predictions by\nimplementing an expert foundational branch to supervise the student model. We\nvalidate the effectiveness of our approach in mitigating context bias under the\nSFOD setting through experiments on three medical datasets supported by\ncomprehensive ablation studies. All relevant resources, including preprocessed\ndata, trained model weights, and code, are publicly available at this\nhttps://github.com/Tajamul21/Grounded_Teacher."}
{"id": "2504.15457", "pdf": "https://arxiv.org/pdf/2504.15457", "abs": "https://arxiv.org/abs/2504.15457", "authors": ["Paresh Chaudhary", "Yancheng Liang", "Daphne Chen", "Simon S. Du", "Natasha Jaques"], "title": "Improving Human-AI Coordination through Adversarial Training and Generative Models", "categories": ["cs.AI"], "comment": null, "summary": "Being able to cooperate with new people is an important component of many\neconomically valuable AI tasks, from household robotics to autonomous driving.\nHowever, generalizing to novel humans requires training on data that captures\nthe diversity of human behaviors. Adversarial training is one avenue for\nsearching for such data and ensuring that agents are robust. However, it is\ndifficult to apply in the cooperative setting because adversarial policies\nintentionally learn to sabotage the task instead of simulating valid\ncooperation partners. To address this challenge, we propose a novel strategy\nfor overcoming self-sabotage that combines a pre-trained generative model to\nsimulate valid cooperative agent policies with adversarial training to maximize\nregret. We call our method GOAT: Generative Online Adversarial Training. In\nthis framework, the GOAT dynamically searches for and generates coordination\nstrategies where the learning policy -- the Cooperator agent -- underperforms.\nGOAT enables better generalization by exposing the Cooperator to various\nchallenging interaction scenarios. We maintain realistic coordination\nstrategies by updating only the generative model's embedding while keeping its\nparameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT\nwith real human partners, and the results demonstrate state-of-the-art\nperformance on the Overcooked benchmark, highlighting its effectiveness in\ngeneralizing to diverse human behaviors."}
{"id": "2504.15415", "pdf": "https://arxiv.org/pdf/2504.15415", "abs": "https://arxiv.org/abs/2504.15415", "authors": ["David Ma", "Yuanxing Zhang", "Jincheng Ren", "Jarvis Guo", "Yifan Yao", "Zhenlin Wei", "Zhenzhu Yang", "Zhongyuan Peng", "Boyu Feng", "Jun Ma", "Xiao Gu", "Zhoufutu Wen", "King Zhu", "Yancheng He", "Meng Cao", "Shiwen Ni", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Xiaojie Jin"], "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench."}
{"id": "2504.15466", "pdf": "https://arxiv.org/pdf/2504.15466", "abs": "https://arxiv.org/abs/2504.15466", "authors": ["Jiayi Pan", "Xiuyu Li", "Long Lian", "Charlie Snell", "Yifei Zhou", "Adam Yala", "Trevor Darrell", "Kurt Keutzer", "Alane Suhr"], "title": "Learning Adaptive Parallel Reasoning with Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code, model, and data are available at\n  https://github.com/Parallel-Reasoning/APR. The first three authors\n  contributed equally to this work", "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation."}
{"id": "2504.15470", "pdf": "https://arxiv.org/pdf/2504.15470", "abs": "https://arxiv.org/abs/2504.15470", "authors": ["Jonathan Brokman", "Amit Giloni", "Omer Hofman", "Roman Vainshtein", "Hisashi Kojima", "Guy Gilboa"], "title": "Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images", "categories": ["cs.CV"], "comment": "Accepted to ICLR 2025 (The International Conference on Learning\n  Representations)", "summary": "Distinguishing between real and AI-generated images, commonly referred to as\n'image detection', presents a timely and significant challenge. Despite\nextensive research in the (semi-)supervised regime, zero-shot and few-shot\nsolutions have only recently emerged as promising alternatives. Their main\nadvantage is in alleviating the ongoing data maintenance, which quickly becomes\noutdated due to advances in generative technologies. We identify two main gaps:\n(1) a lack of theoretical grounding for the methods, and (2) significant room\nfor performance improvements in zero-shot and few-shot regimes. Our approach is\nfounded on understanding and quantifying the biases inherent in generated\ncontent, where we use these quantities as criteria for characterizing generated\nimages. Specifically, we explore the biases of the implicit probability\nmanifold, captured by a pre-trained diffusion model. Through score-function\nanalysis, we approximate the curvature, gradient, and bias towards points on\nthe probability manifold, establishing criteria for detection in the zero-shot\nregime. We further extend our contribution to the few-shot setting by employing\na mixture-of-experts methodology. Empirical results across 20 generative models\ndemonstrate that our method outperforms current approaches in both zero-shot\nand few-shot settings. This work advances the theoretical understanding and\npractical usage of generated content biases through the lens of manifold\nanalysis."}
{"id": "2504.15552", "pdf": "https://arxiv.org/pdf/2504.15552", "abs": "https://arxiv.org/abs/2504.15552", "authors": ["Gengxian Cao", "Fengyuan Li", "Hong Duan", "Ye Yang", "Bofeng Wang", "Donghe Li"], "title": "A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models", "categories": ["cs.AI"], "comment": "17 pages,7 figures,1 tables", "summary": "This paper introduces a novel multi-Agent framework that automates the end to\nend production of Qinqiang opera by integrating Large Language Models , visual\ngeneration, and Text to Speech synthesis. Three specialized agents collaborate\nin sequence: Agent1 uses an LLM to craft coherent, culturally grounded\nscripts;Agent2 employs visual generation models to render contextually accurate\nstage scenes; and Agent3 leverages TTS to produce synchronized, emotionally\nexpressive vocal performances. In a case study on Dou E Yuan, the system\nachieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,\nand 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point\nimprovement over a Single Agent baseline. Ablation experiments demonstrate that\nremoving Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,\nunderscoring the value of modular collaboration. This work showcases how AI\ndriven pipelines can streamline and scale the preservation of traditional\nperforming arts, and points toward future enhancements in cross modal\nalignment, richer emotional nuance, and support for additional opera genres."}
{"id": "2504.15473", "pdf": "https://arxiv.org/pdf/2504.15473", "abs": "https://arxiv.org/abs/2504.15473", "authors": ["Berk Tinaz", "Zalan Fabian", "Mahdi Soltanolkotabi"], "title": "Emergence and Evolution of Interpretable Concepts in Diffusion Models", "categories": ["cs.CV", "cs.LG", "eess.IV", "I.2.6; I.2.10"], "comment": "32 pages, 32 figures, preliminary version", "summary": "Diffusion models have become the go-to method for text-to-image generation,\nproducing high-quality images from noise through a process called reverse\ndiffusion. Understanding the dynamics of the reverse diffusion process is\ncrucial in steering the generation and achieving high sample quality. However,\nthe inner workings of diffusion models is still largely a mystery due to their\nblack-box nature and complex, multi-step generation process. Mechanistic\nInterpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at\nuncovering the operating principles of models through granular analysis of\ntheir internal representations. These MI techniques have been successful in\nunderstanding and steering the behavior of large language models at scale.\nHowever, the great potential of SAEs has not yet been applied toward gaining\ninsight into the intricate generative process of diffusion models. In this\nwork, we leverage the SAE framework to probe the inner workings of a popular\ntext-to-image diffusion model, and uncover a variety of human-interpretable\nconcepts in its activations. Interestingly, we find that even before the first\nreverse diffusion step is completed, the final composition of the scene can be\npredicted surprisingly well by looking at the spatial distribution of activated\nconcepts. Moreover, going beyond correlational analysis, we show that the\ndiscovered concepts have a causal effect on the model output and can be\nleveraged to steer the generative process. We design intervention techniques\naimed at manipulating image composition and style, and demonstrate that (1) in\nearly stages of diffusion image composition can be effectively controlled, (2)\nin the middle stages of diffusion image composition is finalized, however\nstylistic interventions are effective, and (3) in the final stages of diffusion\nonly minor textural details are subject to change."}
{"id": "2504.15610", "pdf": "https://arxiv.org/pdf/2504.15610", "abs": "https://arxiv.org/abs/2504.15610", "authors": ["Md Millat", "Md Motiur"], "title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings", "categories": ["cs.AI", "68T05 (Learning and adaptive systems), 68T07 (Artificial\n  intelligence and education)"], "comment": "18 pages, 6 figures (3 graphs + 3 flowchart/architecture diagrams),\n  submitted as a preprint for review consideration in AI for Education or\n  Machine Learning applications in low-resource settings. Includes detailed\n  experiments with LoRA and quantization methods for efficient LLM fine-tuning", "summary": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy."}
{"id": "2504.15485", "pdf": "https://arxiv.org/pdf/2504.15485", "abs": "https://arxiv.org/abs/2504.15485", "authors": ["Atin Pothiraj", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and data: https://github.com/atinpothiraj/CAPTURe", "summary": "Recognizing and reasoning about occluded (partially or fully hidden) objects\nis vital to understanding visual scenes, as occlusions frequently occur in\nreal-world environments and act as obstacles for spatial comprehension. To test\nmodels' ability to reason about multiple occluded objects, we introduce a novel\ntask, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which\nrequires a model to count objects arranged in a pattern by inferring how the\npattern continues behind an occluder (an object which blocks parts of the\nscene). CAPTURe requires both recognizing visual patterns and reasoning, making\nit a useful testbed for evaluating vision-language models (VLMs) on whether\nthey understand occluded patterns and possess spatial understanding skills. By\nrequiring models to reason about occluded objects, CAPTURe also tests VLMs'\nability to form world models that would allow them to fill in missing\ninformation. CAPTURe consists of two parts: (1) CAPTURe-real, with manually\nfiltered images of real objects in patterns and (2) CAPTURe-synthetic, a\ncontrolled diagnostic with generated patterned images. We evaluate four strong\nVLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models\nstruggle to count on both occluded and unoccluded patterns. Crucially, we find\nthat models perform worse with occlusion, suggesting that VLMs are also\ndeficient in inferring unseen spatial relationships: even the strongest VLMs\nlike GPT-4o fail to count with occlusion. In contrast, we find that humans\nachieve very little error on CAPTURe. We also find that providing auxiliary\ninformation of occluded object locations increases performance, underscoring\nthat the model error comes both from an inability to handle occlusion as well\nas difficulty counting in images."}
{"id": "2504.15668", "pdf": "https://arxiv.org/pdf/2504.15668", "abs": "https://arxiv.org/abs/2504.15668", "authors": ["Mir Md Sajid Sarwar", "Rajarshi Ray"], "title": "Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems", "categories": ["cs.AI", "cs.FL", "I.2.0; F.4.3"], "comment": null, "summary": "Explaining unsolvability of planning problems is of significant research\ninterest in Explainable AI Planning. AI planning literature has reported\nseveral research efforts on generating explanations of solutions to planning\nproblems. However, explaining the unsolvability of planning problems remains a\nlargely open and understudied problem. A widely practiced approach to plan\ngeneration and automated problem solving, in general, is to decompose tasks\ninto sub-problems that help progressively converge towards the goal. In this\npaper, we propose to adopt the same philosophy of sub-problem identification as\na mechanism for analyzing and explaining unsolvability of planning problems in\nhybrid systems. In particular, for a given unsolvable planning problem, we\npropose to identify common waypoints, which are universal obstacles to plan\nexistence; in other words, they appear on every plan from the source to the\nplanning goal. This work envisions such waypoints as sub-problems of the\nplanning problem and the unreachability of any of these waypoints as an\nexplanation for the unsolvability of the original planning problem. We propose\na novel method of waypoint identification by casting the problem as an instance\nof the longest common subsequence problem, a widely popular problem in computer\nscience, typically considered as an illustrative example for the dynamic\nprogramming paradigm. Once the waypoints are identified, we perform symbolic\nreachability analysis on them to identify the earliest unreachable waypoint and\nreport it as the explanation of unsolvability. We present experimental results\non unsolvable planning problems in hybrid domains."}
{"id": "2504.15513", "pdf": "https://arxiv.org/pdf/2504.15513", "abs": "https://arxiv.org/abs/2504.15513", "authors": ["Yixuan Zhu", "Haolin Wang", "Ao Li", "Wenliang Zhao", "Yansong Tang", "Jingxuan Niu", "Lei Chen", "Jie Zhou", "Jiwen Lu"], "title": "InstaRevive: One-Step Image Enhancement via Dynamic Score Matching", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "Image enhancement finds wide-ranging applications in real-world scenarios due\nto complex environments and the inherent limitations of imaging devices. Recent\ndiffusion-based methods yield promising outcomes but necessitate prolonged and\ncomputationally intensive iterative sampling. In response, we propose\nInstaRevive, a straightforward yet powerful image enhancement framework that\nemploys score-based diffusion distillation to harness potent generative\ncapability and minimize the sampling steps. To fully exploit the potential of\nthe pre-trained diffusion model, we devise a practical and effective diffusion\ndistillation pipeline using dynamic control to address inaccuracies in updating\ndirection during score matching. Our control strategy enables a dynamic\ndiffusing scope, facilitating precise learning of denoising trajectories within\nthe diffusion model and ensuring accurate distribution matching gradients\nduring training. Additionally, to enrich guidance for the generative power, we\nincorporate textual prompts via image captioning as auxiliary conditions,\nfostering further exploration of the diffusion model. Extensive experiments\nsubstantiate the efficacy of our framework across a diverse array of\nchallenging tasks and datasets, unveiling the compelling efficacy and\nefficiency of InstaRevive in delivering high-quality and visually appealing\nresults. Code is available at https://github.com/EternalEvan/InstaRevive."}
{"id": "2504.15699", "pdf": "https://arxiv.org/pdf/2504.15699", "abs": "https://arxiv.org/abs/2504.15699", "authors": ["Ning Wang", "Zihan Yan", "Weiyang Li", "Chuan Ma", "He Chen", "Tao Xiang"], "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation", "categories": ["cs.AI"], "comment": "9 pages", "summary": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance."}
{"id": "2504.15599", "pdf": "https://arxiv.org/pdf/2504.15599", "abs": "https://arxiv.org/abs/2504.15599", "authors": ["Shichen Li", "Chenhui Shao"], "title": "Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 12 figures", "summary": "Food drying is essential for food production, extending shelf life, and\nreducing transportation costs. Accurate real-time forecasting of drying\nreadiness is crucial for minimizing energy consumption, improving productivity,\nand ensuring product quality. However, this remains challenging due to the\ndynamic nature of drying, limited data availability, and the lack of effective\npredictive analytical methods. To address this gap, we propose an end-to-end\nmulti-modal data fusion framework that integrates in-situ video data with\nprocess parameters for real-time food drying readiness forecasting. Our\napproach leverages a new encoder-decoder architecture with modality-specific\nencoders and a transformer-based decoder to effectively extract features while\npreserving the unique structure of each modality. We apply our approach to\nsugar cookie drying, where time-to-ready is predicted at each timestamp.\nExperimental results demonstrate that our model achieves an average prediction\nerror of only 15 seconds, outperforming state-of-the-art data fusion methods by\n65.69% and a video-only model by 11.30%. Additionally, our model balances\nprediction accuracy, model size, and computational efficiency, making it\nwell-suited for heterogenous industrial datasets. The proposed model is\nextensible to various other industrial modality fusion tasks for online\ndecision-making."}
{"id": "2504.15716", "pdf": "https://arxiv.org/pdf/2504.15716", "abs": "https://arxiv.org/abs/2504.15716", "authors": ["Jie Zhu", "Qian Chen", "Huaixia Dou", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang"], "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications."}
{"id": "2504.15609", "pdf": "https://arxiv.org/pdf/2504.15609", "abs": "https://arxiv.org/abs/2504.15609", "authors": ["Yunfeng Li", "Bo Wang", "Jiahao Wan", "Xueyi Wu", "Ye Li"], "title": "SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Underwater observation systems typically integrate optical cameras and\nimaging sonar systems. When underwater visibility is insufficient, only sonar\nsystems can provide stable data, which necessitates exploration of the\nunderwater acoustic object tracking (UAOT) task. Previous studies have explored\ntraditional methods and Siamese networks for UAOT. However, the absence of a\nunified evaluation benchmark has significantly constrained the value of these\nmethods. To alleviate this limitation, we propose the first large-scale UAOT\nbenchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and\n205K high-quality annotations. Experimental results demonstrate that SonarT165\nreveals limitations in current state-of-the-art SOT trackers. To address these\nlimitations, we propose STFTrack, an efficient framework for acoustic object\ntracking. It includes two novel modules, a multi-view template fusion module\n(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module\nintegrates multi-view feature of both the original image and the binary image\nof the dynamic template, and introduces a cross-attention-like layer to fuse\nthe spatio-temporal target representations. The OTCM module introduces the\nacoustic-response-equivalent pixel property and proposes normalized pixel\nbrightness response scores, thereby suppressing suboptimal matches caused by\ninaccurate Kalman filter prediction boxes. To further improve the model\nfeature, STFTrack introduces a acoustic image enhancement method and a\nFrequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive\nexperiments show the proposed STFTrack achieves state-of-the-art performance on\nthe proposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/SonarT165."}
{"id": "2504.15719", "pdf": "https://arxiv.org/pdf/2504.15719", "abs": "https://arxiv.org/abs/2504.15719", "authors": ["Anna Karnysheva", "Christian Drescher", "Dietrich Klakow"], "title": "Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences", "categories": ["cs.AI"], "comment": null, "summary": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain."}
{"id": "2504.15612", "pdf": "https://arxiv.org/pdf/2504.15612", "abs": "https://arxiv.org/abs/2504.15612", "authors": ["Hongxing Peng", "Kang Lin", "Huanai Liu"], "title": "HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) classification has been one of the hot topics in\nremote sensing fields. Recently, the Mamba architecture based on selective\nstate-space models (S6) has demonstrated great advantages in long sequence\nmodeling. However, the unique properties of hyperspectral data, such as high\ndimensionality and feature inlining, pose challenges to the application of\nMamba to HSI classification. To compensate for these shortcomings, we propose\nan full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts\na strategy different from pixel-patch based or whole-image based, but combines\nthe advantages of both. The patches cut from the whole image are sent to\nmulti-groups Mamba, combined with positional information to perceive local\ninline features in the spatial and spectral domains, and the whole image is\nsent to a lightweight attention module to enhance the global feature\nrepresentation ability. Specifically, HS-Mamba consists of a dual-channel\nspatial-spectral encoder (DCSS-encoder) module and a lightweight global inline\nattention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of\nMamba to decouple and model the local features of dual-channel sequences with\nnon-overlapping patches. The LGI-Att branch uses a lightweight compressed and\nextended attention module to perceive the global features of the spatial and\nspectral domains of the unsegmented whole image. By fusing local and global\nfeatures, high-precision classification of hyperspectral images is achieved.\nExtensive experiments demonstrate the superiority of the proposed HS-Mamba,\noutperforming state-of-the-art methods on four benchmark HSI datasets."}
{"id": "2504.15780", "pdf": "https://arxiv.org/pdf/2504.15780", "abs": "https://arxiv.org/abs/2504.15780", "authors": ["Daocheng Fu", "Zijun Chen", "Renqiu Xia", "Qi Liu", "Yuan Feng", "Hongbin Zhou", "Renrui Zhang", "Shiyang Feng", "Peng Gao", "Junchi Yan", "Botian Shi", "Bo Zhang", "Yu Qiao"], "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen"}
{"id": "2504.15619", "pdf": "https://arxiv.org/pdf/2504.15619", "abs": "https://arxiv.org/abs/2504.15619", "authors": ["Jinda Lu", "Jinghan Li", "Yuan Gao", "Junkang Wu", "Jiancan Wu", "Xiang Wang", "Xiangnan He"], "title": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Preference alignment through Direct Preference Optimization (DPO) has\ndemonstrated significant effectiveness in aligning multimodal large language\nmodels (MLLMs) with human preferences. However, existing methods focus\nprimarily on language preferences while neglecting the critical visual context.\nIn this paper, we propose an Adaptive Vision-enhanced Preference optimization\n(AdaViP) that addresses these limitations through two key innovations: (1)\nvision-based preference pair construction, which integrates multiple visual\nfoundation models to strategically remove key visual elements from the image,\nenhancing MLLMs' sensitivity to visual details; and (2) adaptive preference\noptimization that dynamically balances vision- and language-based preferences\nfor more accurate alignment. Extensive evaluations across different benchmarks\ndemonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%\nreductions in response-level and mentioned-level hallucination respectively on\nthe Object HalBench, significantly outperforming current state-of-the-art\nmethods."}
{"id": "2504.15785", "pdf": "https://arxiv.org/pdf/2504.15785", "abs": "https://arxiv.org/abs/2504.15785", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "categories": ["cs.AI"], "comment": "Code is available at https://github.com/elated-sawyer/WALL-E", "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations."}
{"id": "2504.15624", "pdf": "https://arxiv.org/pdf/2504.15624", "abs": "https://arxiv.org/abs/2504.15624", "authors": ["Jingzhi Li", "Changjiang Luo", "Ruoyu Chen", "Hua Zhang", "Wenqi Ren", "Jianhou Gan", "Xiaochun Cao"], "title": "FaceInsight: A Multimodal Large Language Model for Face Perception", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nstrong capabilities in understanding general visual content. However, these\ngeneral-domain MLLMs perform poorly in face perception tasks, often producing\ninaccurate or misleading responses to face-specific queries. To address this\ngap, we propose FaceInsight, the versatile face perception MLLM that provides\nfine-grained facial information. Our approach introduces visual-textual\nalignment of facial knowledge to model both uncertain dependencies and\ndeterministic relationships among facial information, mitigating the\nlimitations of language-driven reasoning. Additionally, we incorporate face\nsegmentation maps as an auxiliary perceptual modality, enriching the visual\ninput with localized structural cues to enhance semantic understanding.\nComprehensive experiments and analyses across three face perception tasks\ndemonstrate that FaceInsight consistently outperforms nine compared MLLMs under\nboth training-free and fine-tuned settings."}
{"id": "2504.15791", "pdf": "https://arxiv.org/pdf/2504.15791", "abs": "https://arxiv.org/abs/2504.15791", "authors": ["Raquel Fernandez-Peralta", "Javier Fumanal-Idocin", "Javier Andreu-Perez"], "title": "Crisp complexity of fuzzy classifiers", "categories": ["cs.AI"], "comment": null, "summary": "Rule-based systems are a very popular form of explainable AI, particularly in\nthe fuzzy community, where fuzzy rules are widely used for control and\nclassification problems. However, fuzzy rule-based classifiers struggle to\nreach bigger traction outside of fuzzy venues, because users sometimes do not\nknow about fuzzy and because fuzzy partitions are not so easy to interpret in\nsome situations. In this work, we propose a methodology to reduce fuzzy\nrule-based classifiers to crisp rule-based classifiers. We study different\npossible crisp descriptions and implement an algorithm to obtain them. Also, we\nanalyze the complexity of the resulting crisp classifiers. We believe that our\nresults can help both fuzzy and non-fuzzy practitioners understand better the\nway in which fuzzy rule bases partition the feature space and how easily one\nsystem can be translated to another and vice versa. Our complexity metric can\nalso help to choose between different fuzzy classifiers based on what the\nequivalent crisp partitions look like."}
{"id": "2504.15627", "pdf": "https://arxiv.org/pdf/2504.15627", "abs": "https://arxiv.org/abs/2504.15627", "authors": ["Doanh C. Bui", "Hoai Luan Pham", "Vu Trung Duong Le", "Tuan Hai Vu", "Van Duy Tran", "Yasuhiko Nakashima"], "title": "ZeroSlide: Is Zero-Shot Classification Adequate for Lifelong Learning in Whole-Slide Image Analysis in the Era of Pathology Vision-Language Foundation Models?", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 1 table, conference submission", "summary": "Lifelong learning for whole slide images (WSIs) poses the challenge of\ntraining a unified model to perform multiple WSI-related tasks, such as cancer\nsubtyping and tumor classification, in a distributed, continual fashion. This\nis a practical and applicable problem in clinics and hospitals, as WSIs are\nlarge, require storage, processing, and transfer time. Training new models\nwhenever new tasks are defined is time-consuming. Recent work has applied\nregularization- and rehearsal-based methods to this setting. However, the rise\nof vision-language foundation models that align diagnostic text with pathology\nimages raises the question: are these models alone sufficient for lifelong WSI\nlearning using zero-shot classification, or is further investigation into\ncontinual learning strategies needed to improve performance? To our knowledge,\nthis is the first study to compare conventional continual-learning approaches\nwith vision-language zero-shot classification for WSIs. Our source code and\nexperimental results will be available soon."}
{"id": "2504.15829", "pdf": "https://arxiv.org/pdf/2504.15829", "abs": "https://arxiv.org/abs/2504.15829", "authors": ["Modhurita Mitra", "Martine G. de Vos", "Nicola Cortinovis", "Dawa Ometto"], "title": "Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases", "categories": ["cs.AI", "68T50", "I.2.7"], "comment": "10 pages, 4 figures, 6 tables. Published in Proceedings of the 2024\n  IEEE 20th International Conference on e-Science (e-Science), Osaka, Japan", "summary": "There has been enormous interest in generative AI since ChatGPT was launched\nin 2022. However, there are concerns about the accuracy and consistency of the\noutputs of generative AI. We have carried out an exploratory study on the\napplication of this new technology in research data processing. We identified\ntasks for which rule-based or traditional machine learning approaches were\ndifficult to apply, and then performed these tasks using generative AI.\n  We demonstrate the feasibility of using the generative AI model Claude 3 Opus\nin three research projects involving complex data processing tasks:\n  1) Information extraction: We extract plant species names from historical\nseedlists (catalogues of seeds) published by botanical gardens.\n  2) Natural language understanding: We extract certain data points (name of\ndrug, name of health indication, relative effectiveness, cost-effectiveness,\netc.) from documents published by Health Technology Assessment organisations in\nthe EU.\n  3) Text classification: We assign industry codes to projects on the\ncrowdfunding website Kickstarter.\n  We share the lessons we learnt from these use cases: How to determine if\ngenerative AI is an appropriate tool for a given data processing task, and if\nso, how to maximise the accuracy and consistency of the results obtained."}
{"id": "2504.15650", "pdf": "https://arxiv.org/pdf/2504.15650", "abs": "https://arxiv.org/abs/2504.15650", "authors": ["Dengyang Jiang", "Mengmeng Wang", "Teli Ma", "Hengzhuang Li", "Yong liu", "Guang Dai", "Lei Zhang"], "title": "AffordanceSAM: Segment Anything Once More in Affordance Grounding", "categories": ["cs.CV"], "comment": "SAM Meets Affordance Grounding", "summary": "Improving the generalization ability of an affordance grounding model to\nrecognize regions for unseen objects and affordance functions is crucial for\nreal-world application. However, current models are still far away from such\nstandards. To address this problem, we introduce AffordanceSAM, an effective\napproach that extends SAM's generalization capacity to the domain of affordance\ngrounding. For the purpose of thoroughly transferring SAM's robust performance\nin segmentation to affordance, we initially propose an affordance-adaption\nmodule in order to help modify SAM's segmentation output to be adapted to the\nspecific functional regions required for affordance grounding. We concurrently\nmake a coarse-to-fine training recipe to make SAM first be aware of affordance\nobjects and actions coarsely, and then be able to generate affordance heatmaps\nfinely. Both quantitative and qualitative experiments show the strong\ngeneralization capacity of our AffordanceSAM, which not only surpasses previous\nmethods under AGD20K benchmark but also shows evidence to handle the task with\nnovel objects and affordance functions."}
{"id": "2504.15847", "pdf": "https://arxiv.org/pdf/2504.15847", "abs": "https://arxiv.org/abs/2504.15847", "authors": ["Xiang Liu", "Hau Chan", "Minming Li", "Xianlong Zeng", "Chenchen Fu", "Weiwei Wu"], "title": "CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters", "categories": ["cs.AI"], "comment": null, "summary": "Federated learning (FL) is a promising approach that allows requesters (\\eg,\nservers) to obtain local training models from workers (e.g., clients). Since\nworkers are typically unwilling to provide training services/models freely and\nvoluntarily, many incentive mechanisms in FL are designed to incentivize\nparticipation by offering monetary rewards from requesters. However, existing\nstudies neglect two crucial aspects of real-world FL scenarios. First, workers\ncan possess inherent incompatibility characteristics (e.g., communication\nchannels and data sources), which can lead to degradation of FL efficiency\n(e.g., low communication efficiency and poor model generalization). Second, the\nrequesters are budgeted, which limits the amount of workers they can hire for\ntheir tasks. In this paper, we investigate the scenario in FL where multiple\nbudgeted requesters seek training services from incompatible workers with\nprivate training costs. We consider two settings: the cooperative budget\nsetting where requesters cooperate to pool their budgets to improve their\noverall utility and the non-cooperative budget setting where each requester\noptimizes their utility within their own budgets. To address efficiency\ndegradation caused by worker incompatibility, we develop novel\ncompatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both\nsettings to elicit true private costs and determine workers to hire for\nrequesters and their rewards while satisfying requester budget constraints. Our\nmechanisms guarantee individual rationality, truthfulness, budget feasibility,\nand approximation performance. We conduct extensive experiments using\nreal-world datasets to show that the proposed mechanisms significantly\noutperform existing baselines."}
{"id": "2504.15661", "pdf": "https://arxiv.org/pdf/2504.15661", "abs": "https://arxiv.org/abs/2504.15661", "authors": ["Xian Wu", "Chang Liu"], "title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Many existing video inpainting algorithms utilize optical flows to construct\nthe corresponding maps and then propagate pixels from adjacent frames to\nmissing areas by mapping. Despite the effectiveness of the propagation\nmechanism, they might encounter blurry and inconsistencies when dealing with\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\nhas emerged as a revolutionary technique for video generation tasks. However,\npretrained DiT models for video generation all contain a large amount of\nparameters, which makes it very time consuming to apply to video inpainting\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\ntransformer network designed for video inpainting, which is trained from\nscratch instead of initializing from any large pretrained models. DiTPainter\ncan address videos with arbitrary lengths and can be applied to video\ndecaptioning and video completion tasks with an acceptable time cost.\nExperiments show that DiTPainter outperforms existing video inpainting\nalgorithms with higher quality and better spatial-temporal consistency."}
{"id": "2504.15903", "pdf": "https://arxiv.org/pdf/2504.15903", "abs": "https://arxiv.org/abs/2504.15903", "authors": ["Nikhil Khandalkar", "Pavan Yadav", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations", "categories": ["cs.AI"], "comment": "60 pages, 25 figures", "summary": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility."}
{"id": "2504.15665", "pdf": "https://arxiv.org/pdf/2504.15665", "abs": "https://arxiv.org/abs/2504.15665", "authors": ["Pei Liu", "Yisi Luo", "Wenzhen Wang", "Xiangyong Cao"], "title": "Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared dim and small target detection presents a significant challenge due\nto dynamic multi-frame scenarios and weak target signatures in the infrared\nmodality. Traditional low-rank plus sparse models often fail to capture dynamic\nbackgrounds and global spatial-temporal correlations, which results in\nbackground leakage or target loss. In this paper, we propose a novel\nmotion-enhanced nonlocal similarity implicit neural representation (INR)\nframework to address these challenges. We first integrate motion estimation via\noptical flow to capture subtle target movements, and propose multi-frame fusion\nto enhance motion saliency. Second, we leverage nonlocal similarity to\nconstruct patch tensors with strong low-rank properties, and propose an\ninnovative tensor decomposition-based INR model to represent the nonlocal patch\ntensor, effectively encoding both the nonlocal low-rankness and\nspatial-temporal correlations of background through continuous neural\nrepresentations. An alternating direction method of multipliers is developed\nfor the nonlocal INR model, which enjoys theoretical fixed-point convergence.\nExperimental results show that our approach robustly separates dim targets from\ncomplex infrared backgrounds, outperforming state-of-the-art methods in\ndetection accuracy and robustness."}
{"id": "2504.16042", "pdf": "https://arxiv.org/pdf/2504.16042", "abs": "https://arxiv.org/abs/2504.16042", "authors": ["Ismaïl Baaj"], "title": "Approximate matrices of systems of max-min fuzzy relational equations", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "In this article, we address the inconsistency of a system of max-min fuzzy\nrelational equations by minimally modifying the matrix governing the system in\norder to achieve consistency. Our method yields consistent systems that\napproximate the original inconsistent system in the following sense: the\nright-hand side vector of each consistent system is that of the inconsistent\nsystem, and the coefficients of the matrix governing each consistent system are\nobtained by modifying, exactly and minimally, the entries of the original\nmatrix that must be corrected to achieve consistency, while leaving all other\nentries unchanged.\n  To obtain a consistent system that closely approximates the considered\ninconsistent system, we study the distance (in terms of a norm among $L_1$,\n$L_2$ or $L_\\infty$) between the matrix of the inconsistent system and the set\nformed by the matrices of consistent systems that use the same right-hand side\nvector as the inconsistent system. We show that our method allows us to\ndirectly compute matrices of consistent systems that use the same right-hand\nside vector as the inconsistent system whose distance in terms of $L_\\infty$\nnorm to the matrix of the inconsistent system is minimal (the computational\ncosts are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit\nanalytical formula for computing this minimal $L_\\infty$ distance. Finally, we\ntranslate our results for systems of min-max fuzzy relational equations and\npresent some potential applications."}
{"id": "2504.15669", "pdf": "https://arxiv.org/pdf/2504.15669", "abs": "https://arxiv.org/abs/2504.15669", "authors": ["Wei Zhuo", "Zhiyue Tang", "Wufeng Xue", "Hao Ding", "Linlin Shen"], "title": "DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot semantic segmentation has gained increasing interest due to its\ngeneralization capability, i.e., segmenting pixels of novel classes requiring\nonly a few annotated images. Prior work has focused on meta-learning for\nsupport-query matching, with extensive development in both prototype-based and\naggregation-based methods. To address data scarcity, recent approaches have\nturned to foundation models to enhance representation transferability for novel\nclass segmentation. Among them, a hybrid dual-modal framework including both\nDINOv2 and SAM has garnered attention due to their complementary capabilities.\nWe wonder \"can we build a unified model with knowledge from both foundation\nmodels?\" To this end, we propose FS-DINO, with only DINOv2's encoder and a\nlightweight segmenter. The segmenter features a bottleneck adapter, a\nmeta-visual prompt generator based on dense similarities and semantic\nembeddings, and a decoder. Through coarse-to-fine cross-model distillation, we\neffectively integrate SAM's knowledge into our lightweight segmenter, which can\nbe further enhanced by 4D correlation mining on support-query pairs. Extensive\nexperiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness\nand superiority of our method."}
{"id": "2504.15286", "pdf": "https://arxiv.org/pdf/2504.15286", "abs": "https://arxiv.org/abs/2504.15286", "authors": ["Daniele Gorla", "Shivam Kumar", "Pietro Nicolaus Roselli Lorenzini", "Alireza Alipourfaz"], "title": "CUBETESTERAI: Automated JUnit Test Generation using the LLaMA Model", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted to ICST 2025 Industry Track", "summary": "This paper presents an approach to automating JUnit test generation for Java\napplications using the Spring Boot framework, leveraging the LLaMA (Large\nLanguage Model Architecture) model to enhance the efficiency and accuracy of\nthe testing process. The resulting tool, called CUBETESTERAI, includes a\nuser-friendly web interface and the integration of a CI/CD pipeline using\nGitLab and Docker. These components streamline the automated test generation\nprocess, allowing developers to generate JUnit tests directly from their code\nsnippets with minimal manual intervention. The final implementation executes\nthe LLaMA models through RunPod, an online GPU service, which also enhances the\nprivacy of our tool. Using the advanced natural language processing\ncapabilities of the LLaMA model, CUBETESTERAI is able to generate test cases\nthat provide high code coverage and accurate validation of software\nfunctionalities in Java-based Spring Boot applications. Furthermore, it\nefficiently manages resource-intensive operations and refines the generated\ntests to address common issues like missing imports and handling of private\nmethods. By comparing CUBETESTERAI with some state-of-the-art tools, we show\nthat our proposal consistently demonstrates competitive and, in many cases,\nbetter performance in terms of code coverage in different real-life Java\nprograms."}
{"id": "2504.15681", "pdf": "https://arxiv.org/pdf/2504.15681", "abs": "https://arxiv.org/abs/2504.15681", "authors": ["Vidi Team", "Celong Liu", "Chia-Wen Kuo", "Dawei Du", "Fan Chen", "Guang Chen", "Jiamin Yuan", "Lingxi Zhang", "Lu Guo", "Lusha Li", "Longyin Wen", "Qingyu Chen", "Rachel Deng", "Sijie Zhu", "Stuart Siew", "Tong Jin", "Wei Lu", "Wen Zhong", "Xiaohui Shen", "Xin Gu", "Xing Mei", "Xueqiong Qu"], "title": "Vidi: Large Multimodal Models for Video Understanding and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios."}
{"id": "2504.15296", "pdf": "https://arxiv.org/pdf/2504.15296", "abs": "https://arxiv.org/abs/2504.15296", "authors": ["Yihong Jin", "Ze Yang"], "title": "Scalability Optimization in Cloud-Based AI Inference Services: Strategies for Real-Time Load Balancing and Automated Scaling", "categories": ["cs.DC", "cs.AI", "F.2.2; I.2.8"], "comment": "Accepted to BDICN 2025", "summary": "The rapid expansion of AI inference services in the cloud necessitates a\nrobust scalability solution to manage dynamic workloads and maintain high\nperformance. This study proposes a comprehensive scalability optimization\nframework for cloud AI inference services, focusing on real-time load balancing\nand autoscaling strategies. The proposed model is a hybrid approach that\ncombines reinforcement learning for adaptive load distribution and deep neural\nnetworks for accurate demand forecasting. This multi-layered approach enables\nthe system to anticipate workload fluctuations and proactively adjust\nresources, ensuring maximum resource utilisation and minimising latency.\nFurthermore, the incorporation of a decentralised decision-making process\nwithin the model serves to enhance fault tolerance and reduce response time in\nscaling operations. Experimental results demonstrate that the proposed model\nenhances load balancing efficiency by 35\\ and reduces response delay by 28\\,\nthereby exhibiting a substantial optimization effect in comparison with\nconventional scalability solutions."}
{"id": "2504.15694", "pdf": "https://arxiv.org/pdf/2504.15694", "abs": "https://arxiv.org/abs/2504.15694", "authors": ["Jun Dong", "Wenli Wu", "Jintao Cheng", "Xiaoyu Tang"], "title": "You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable achievements in object detection, the model's accuracy\nand efficiency still require further improvement under challenging underwater\nconditions, such as low image quality and limited computational resources. To\naddress this, we propose an Ultra-Light Real-Time Underwater Object Detection\nframework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a\nMulti-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on\nthe input image, minimizing the semantic loss caused by underwater optical\ncolor distortion. Furthermore, we revisit the unique characteristics of\neven-sized and transposed convolutions, allowing the model to dynamically\nselect and enhance key information during the resampling process, thereby\nimproving its generalization ability. Finally, we eliminate model redundancy\nthrough a simple yet effective channel compression and reconstructed large\nkernel convolution (RLKC) to achieve model lightweight. As a result, forms a\nhigh-performance underwater object detector YSOOB with only 1.2 million\nparameters. Extensive experimental results demonstrate that, with the fewest\nparameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO\ndatasets, respectively, comparable to the current SOTA detectors. The inference\nspeed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge\ncomputing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by\n28.1% and 22.5%, respectively."}
{"id": "2504.15299", "pdf": "https://arxiv.org/pdf/2504.15299", "abs": "https://arxiv.org/abs/2504.15299", "authors": ["Haodong Wang", "Qihua Zhou", "Zicong Hong", "Song Guo"], "title": "D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving", "categories": ["cs.DC", "cs.AI"], "comment": "Accepted by MobiCom 2025", "summary": "The mixture of experts (MoE) model is a sparse variant of large language\nmodels (LLMs), designed to hold a better balance between intelligent capability\nand computational overhead. Despite its benefits, MoE is still too expensive to\ndeploy on resource-constrained edge devices, especially with the demands of\non-device inference services. Recent research efforts often apply model\ncompression techniques, such as quantization, pruning and merging, to restrict\nMoE complexity. Unfortunately, due to their predefined static model\noptimization strategies, they cannot always achieve the desired\nquality-overhead trade-off when handling multiple requests, finally degrading\nthe on-device quality of service. These limitations motivate us to propose the\nD$^2$MoE, an algorithm-system co-design framework that matches diverse task\nrequirements by dynamically allocating the most proper bit-width to each\nexpert. Specifically, inspired by the nested structure of matryoshka dolls, we\npropose the matryoshka weight quantization (MWQ) to progressively compress\nexpert weights in a bit-nested manner and reduce the required runtime memory.\nOn top of it, we further optimize the I/O-computation pipeline and design a\nheuristic scheduling algorithm following our hottest-expert-bit-first (HEBF)\nprinciple, which maximizes the expert parallelism between I/O and computation\nqueue under constrained memory budgets, thus significantly reducing the idle\ntemporal bubbles waiting for the experts to load. Evaluations on real edge\ndevices show that D$^2$MoE improves the overall inference throughput by up to\n1.39$\\times$ and reduces the peak memory footprint by up to 53% over the latest\non-device inference frameworks, while still preserving comparable serving\naccuracy as its INT8 counterparts."}
{"id": "2504.15707", "pdf": "https://arxiv.org/pdf/2504.15707", "abs": "https://arxiv.org/abs/2504.15707", "authors": ["Yannic Neuhaus", "Matthias Hein"], "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE ."}
{"id": "2504.15301", "pdf": "https://arxiv.org/pdf/2504.15301", "abs": "https://arxiv.org/abs/2504.15301", "authors": ["Zoi Lygizou", "Dimitris Kalles"], "title": "A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations", "categories": ["cs.MA", "cs.AI", "cs.DC"], "comment": null, "summary": "Trust management provides an alternative solution for securing open, dynamic,\nand distributed multi-agent systems, where conventional cryptographic methods\nprove to be impractical. However, existing trust models face challenges related\nto agent mobility, changing behaviors, and the cold start problem. To address\nthese issues we introduced a biologically inspired trust model in which\ntrustees assess their own capabilities and store trust data locally. This\ndesign improves mobility support, reduces communication overhead, resists\ndisinformation, and preserves privacy. Despite these advantages, prior\nevaluations revealed limitations of our model in adapting to provider\npopulation changes and continuous performance fluctuations. This study proposes\na novel algorithm, incorporating a self-classification mechanism for providers\nto detect performance drops potentially harmful for the service consumers.\nSimulation results demonstrate that the new algorithm outperforms its original\nversion and FIRE, a well-known trust and reputation model, particularly in\nhandling dynamic trustee behavior. While FIRE remains competitive under extreme\nenvironmental changes, the proposed algorithm demonstrates greater adaptability\nacross various conditions. In contrast to existing trust modeling research,\nthis study conducts a comprehensive evaluation of our model using widely\nrecognized trust model criteria, assessing its resilience against common\ntrust-related attacks while identifying strengths, weaknesses, and potential\ncountermeasures. Finally, several key directions for future research are\nproposed."}
{"id": "2504.15723", "pdf": "https://arxiv.org/pdf/2504.15723", "abs": "https://arxiv.org/abs/2504.15723", "authors": ["Dasol Jeong", "Donggoo Kang", "Jiwon Park", "Hyebean Lee", "Joonki Paik"], "title": "Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We propose a diffusion-based framework for zero-shot image editing that\nunifies text-guided and reference-guided approaches without requiring\nfine-tuning. Our method leverages diffusion inversion and timestep-specific\nnull-text embeddings to preserve the structural integrity of the source image.\nBy introducing a stage-wise latent injection strategy-shape injection in early\nsteps and attribute injection in later steps-we enable precise, fine-grained\nmodifications while maintaining global consistency. Cross-attention with\nreference latents facilitates semantic alignment between the source and\nreference. Extensive experiments across expression transfer, texture\ntransformation, and style infusion demonstrate state-of-the-art performance,\nconfirming the method's scalability and adaptability to diverse image editing\nscenarios."}
{"id": "2504.15303", "pdf": "https://arxiv.org/pdf/2504.15303", "abs": "https://arxiv.org/abs/2504.15303", "authors": ["Yi Xiong", "Jinqi Huang", "Wenjie Huang", "Xuebing Yu", "Entong Li", "Zhixiong Ning", "Jinhua Zhou", "Li Zeng", "Xin Chen"], "title": "High-Throughput LLM inference on Heterogeneous Clusters", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Nowadays, many companies possess various types of AI accelerators, forming\nheterogeneous clusters. Efficiently leveraging these clusters for\nhigh-throughput large language model (LLM) inference services can significantly\nreduce costs and expedite task processing. However, LLM inference on\nheterogeneous clusters presents two main challenges. Firstly, different\ndeployment configurations can result in vastly different performance. The\nnumber of possible configurations is large, and evaluating the effectiveness of\na specific setup is complex. Thus, finding an optimal configuration is not an\neasy task. Secondly, LLM inference instances within a heterogeneous cluster\npossess varying processing capacities, leading to different processing speeds\nfor handling inference requests. Evaluating these capacities and designing a\nrequest scheduling algorithm that fully maximizes the potential of each\ninstance is challenging. In this paper, we propose a high-throughput inference\nservice system on heterogeneous clusters. First, the deployment configuration\nis optimized by modeling the resource amount and expected throughput and using\nthe exhaustive search method. Second, a novel mechanism is proposed to schedule\nrequests among instances, which fully considers the different processing\ncapabilities of various instances. Extensive experiments show that the proposed\nscheduler improves throughput by 122.5% and 33.6% on two heterogeneous\nclusters, respectively."}
{"id": "2504.15728", "pdf": "https://arxiv.org/pdf/2504.15728", "abs": "https://arxiv.org/abs/2504.15728", "authors": ["Manjunath D", "Aniruddh Sikdar", "Prajwal Gurunath", "Sumanth Udupa", "Suresh Sundaram"], "title": "SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems", "categories": ["cs.CV"], "comment": "Accepted at CVPR-W PBVS 2025", "summary": "Domain-adaptive thermal object detection plays a key role in facilitating\nvisible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered\nimage pairs and minimizing reliance on large annotated IR datasets. However,\ninherent limitations of IR images, such as the lack of color and texture cues,\npose challenges for RGB-trained models, leading to increased false positives\nand poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray\ncolor Augmentation (SAGA), a novel strategy for mitigating color bias and\nbridging the domain gap by extracting object-level features relevant to IR\nimages. Additionally, to validate the proposed SAGA for drone imagery, we\nintroduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse\napplications. The dataset contains 5,612 images with 145,666 instances,\ncaptured from diverse angles, altitudes, backgrounds, and times of day,\noffering valuable opportunities for multimodal learning, domain adaptation for\nobject detection and segmentation, and exploration of sensor-specific strengths\nand weaknesses. IndraEye aims to enhance the development of more robust and\naccurate aerial perception systems, especially in challenging environments.\nExperimental results show that SAGA significantly improves RGB-to-IR adaptation\nfor autonomous driving and IndraEye dataset, achieving consistent performance\ngains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain\nadaptation techniques. The dataset and codes are available at\nhttps://github.com/airliisc/IndraEye."}
{"id": "2504.15310", "pdf": "https://arxiv.org/pdf/2504.15310", "abs": "https://arxiv.org/abs/2504.15310", "authors": ["Syeda Tahreem Zahra", "Syed Kashif Imdad", "Sohail Khan", "Sohail Khalid", "Nauman Anwar Baig"], "title": "Power Transformer Health Index and Life Span Assessment: A Comprehensive Review of Conventional and Machine Learning based Approaches", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Power transformers play a critical role within the electrical power system,\nmaking their health assessment and the prediction of their remaining lifespan\nparamount for the purpose of ensuring efficient operation and facilitating\neffective maintenance planning. This paper undertakes a comprehensive\nexamination of existent literature, with a primary focus on both conventional\nand cutting-edge techniques employed within this domain. The merits and\ndemerits of recent methodologies and techniques are subjected to meticulous\nscrutiny and explication. Furthermore, this paper expounds upon intelligent\nfault diagnosis methodologies and delves into the most widely utilized\nintelligent algorithms for the assessment of transformer conditions. Diverse\nArtificial Intelligence (AI) approaches, including Artificial Neural Networks\n(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),\nRandom Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization\n(PSO), are elucidated offering pragmatic solutions for enhancing the\nperformance of transformer fault diagnosis. The amalgamation of multiple AI\nmethodologies and the exploration of timeseries analysis further contribute to\nthe augmentation of diagnostic precision and the early detection of faults in\ntransformers. By furnishing a comprehensive panorama of AI applications in the\nfield of transformer fault diagnosis, this study lays the groundwork for future\nresearch endeavors and the progression of this critical area of study."}
{"id": "2504.15751", "pdf": "https://arxiv.org/pdf/2504.15751", "abs": "https://arxiv.org/abs/2504.15751", "authors": ["Menan Velayuthan", "Asiri Gawesha", "Purushoth Velayuthan", "Nuwan Kodagoda", "Dharshana Kasthurirathna", "Pradeepa Samarasinghe"], "title": "GADS: A Super Lightweight Model for Head Pose Estimation", "categories": ["cs.CV"], "comment": "16 pages, 5 tables, 10 figures, not submitted to any conference or\n  journal", "summary": "In human-computer interaction, head pose estimation profoundly influences\napplication functionality. Although utilizing facial landmarks is valuable for\nthis purpose, existing landmark-based methods prioritize precision over\nsimplicity and model size, limiting their deployment on edge devices and in\ncompute-poor environments. To bridge this gap, we propose \\textbf{Grouped\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\nframework. By grouping landmarks into regions and employing small Deep Set\nlayers, we reduce computational complexity. Our multihead attention mechanism\nextracts and combines inter-group information, resulting in a model that is\n$7.5\\times$ smaller and executes $25\\times$ faster than the current lightest\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\nbeing $4321\\times$ smaller than the best-performing model. We introduce vanilla\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\nas a robust baseline for resource-constrained head pose estimation methods."}
{"id": "2504.15311", "pdf": "https://arxiv.org/pdf/2504.15311", "abs": "https://arxiv.org/abs/2504.15311", "authors": ["Fei Shang", "Haohua Du", "Dawei Yan", "Panlong Yang", "Xiang-Yang Li"], "title": "RINN: One Sample Radio Frequency Imaging based on Physics Informed Neural Network", "categories": ["eess.IV", "cs.AI"], "comment": null, "summary": "Due to its ability to work in non-line-of-sight and low-light environments,\nradio frequency (RF) imaging technology is expected to bring new possibilities\nfor embodied intelligence and multimodal sensing. However, widely used RF\ndevices (such as Wi-Fi) often struggle to provide high-precision\nelectromagnetic measurements and large-scale datasets, hindering the\napplication of RF imaging technology. In this paper, we combine the ideas of\nPINN to design the RINN network, using physical constraints instead of true\nvalue comparison constraints and adapting it with the characteristics of\nubiquitous RF signals, allowing the RINN network to achieve RF imaging using\nonly one sample without phase and with amplitude noise. Our numerical\nevaluation results show that compared with 5 classic algorithms based on phase\ndata for imaging results, RINN's imaging results based on phaseless data are\ngood, with indicators such as RRMSE (0.11) performing similarly well. RINN\nprovides new possibilities for the universal development of radio frequency\nimaging technology."}
{"id": "2504.15756", "pdf": "https://arxiv.org/pdf/2504.15756", "abs": "https://arxiv.org/abs/2504.15756", "authors": ["Qirui Yang", "Fangpu Zhang", "Yeying Jin", "Qihua Cheng", "Pengtao Jiang", "Huanjing Yue", "Jingyu Yang"], "title": "DSDNet: Raw Domain Demoiréing via Dual Color-Space Synergy", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation, and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster\nthan the second-best method, highlighting its practical advantages. We provide\nan anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/."}
{"id": "2504.15315", "pdf": "https://arxiv.org/pdf/2504.15315", "abs": "https://arxiv.org/abs/2504.15315", "authors": ["Noa Cohen", "Rotem Dror", "Itzik Klein"], "title": "Diffusion-Driven Inertial Generated Data for Smartphone Location Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite the crucial role of inertial measurements in motion tracking and\nnavigation systems, the time-consuming and resource-intensive nature of\ncollecting extensive inertial data has hindered the development of robust\nmachine learning models in this field. In recent years, diffusion models have\nemerged as a revolutionary class of generative models, reshaping the landscape\nof artificial data generation. These models surpass generative adversarial\nnetworks and other state-of-the-art approaches to complex tasks. In this work,\nwe propose diffusion-driven specific force-generated data for smartphone\nlocation recognition. We provide a comprehensive evaluation methodology by\ncomparing synthetic and real recorded specific force data across multiple\nmetrics. Our results demonstrate that our diffusion-based generative model\nsuccessfully captures the distinctive characteristics of specific force signals\nacross different smartphone placement conditions. Thus, by creating diverse,\nrealistic synthetic data, we can reduce the burden of extensive data collection\nwhile providing high-quality training data for machine learning models."}
{"id": "2504.15770", "pdf": "https://arxiv.org/pdf/2504.15770", "abs": "https://arxiv.org/abs/2504.15770", "authors": ["Lei Xu", "Mehmet Yamac", "Mete Ahishali", "Moncef Gabbouj"], "title": "Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection has attracted considerable attention thanks to its exceptional\nability to enhance performance in downstream computer vision tasks. In recent\nyears, various deep learning methods have been explored for edge detection\ntasks resulting in a significant performance improvement compared to\nconventional computer vision algorithms. In neural networks, edge detection\ntasks require considerably large receptive fields to provide satisfactory\nperformance. In a typical convolutional operation, such a large receptive field\ncan be achieved by utilizing a significant number of consecutive layers, which\nyields deep network structures. Recently, a Multi-scale Tensorial Summation\n(MTS) factorization operator was presented, which can achieve very large\nreceptive fields even from the initial layers. In this paper, we propose a\nnovel MTS Dimensional Reduction (MTS-DR) module guided neural network,\nMTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and\ncorresponding MTS-DR blocks as a new backbone to remove redundant information\ninitially. Such a dimensional reduction module enables the neural network to\nfocus specifically on relevant information (i.e., necessary subspaces).\nFinally, a weight U-shaped refinement module follows MTS-DR blocks in the\nMTS-DR-Net. We conducted extensive experiments on two benchmark edge detection\ndatasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The\nimplementation of the proposed MTS-DR-Net can be found at\nhttps://github.com/LeiXuAI/MTS-DR-Net.git."}
{"id": "2504.15317", "pdf": "https://arxiv.org/pdf/2504.15317", "abs": "https://arxiv.org/abs/2504.15317", "authors": ["Meher Boulaabi", "Takwa Ben Aïcha Gader", "Afef Kacem Echi", "Zied Bouraoui"], "title": "Enhancing DR Classification with Swin Transformer and Shifted Window Attention", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide,\nunderscoring the importance of early detection for effective treatment.\nHowever, automated DR classification remains challenging due to variations in\nimage quality, class imbalance, and pixel-level similarities that hinder model\ntraining. To address these issues, we propose a robust preprocessing pipeline\nincorporating image cropping, Contrast-Limited Adaptive Histogram Equalization\n(CLAHE), and targeted data augmentation to improve model generalization and\nresilience. Our approach leverages the Swin Transformer, which utilizes\nhierarchical token processing and shifted window attention to efficiently\ncapture fine-grained features while maintaining linear computational\ncomplexity. We validate our method on the Aptos and IDRiD datasets for\nmulti-class DR classification, achieving accuracy rates of 89.65% and 97.40%,\nrespectively. These results demonstrate the effectiveness of our model,\nparticularly in detecting early-stage DR, highlighting its potential for\nimproving automated retinal screening in clinical settings."}
{"id": "2504.15776", "pdf": "https://arxiv.org/pdf/2504.15776", "abs": "https://arxiv.org/abs/2504.15776", "authors": ["Quentin Herau", "Nathan Piasco", "Moussab Bennehar", "Luis Rolado", "Dzmitry Tsishkou", "Bingbing Liu", "Cyrille Migniot", "Pascal Vasseur", "Cédric Demonceaux"], "title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models", "categories": ["cs.CV", "cs.RO"], "comment": "under review", "summary": "Autonomous driving systems rely on accurate perception and localization of\nthe ego car to ensure safety and reliability in challenging real-world driving\nscenarios. Public datasets play a vital role in benchmarking and guiding\nadvancement in research by providing standardized resources for model\ndevelopment and evaluation. However, potential inaccuracies in sensor\ncalibration and vehicle poses within these datasets can lead to erroneous\nevaluations of downstream tasks, adversely impacting the reliability and\nperformance of the autonomous systems. To address this challenge, we propose a\nrobust optimization method based on Neural Radiance Fields (NeRF) to refine\nsensor poses and calibration parameters, enhancing the integrity of dataset\nbenchmarks. To validate improvement in accuracy of our optimized poses without\nground truth, we present a thorough evaluation process, relying on reprojection\nmetrics, Novel View Synthesis rendering quality, and geometric alignment. We\ndemonstrate that our method achieves significant improvements in sensor pose\naccuracy. By optimizing these critical parameters, our approach not only\nimproves the utility of existing datasets but also paves the way for more\nreliable autonomous driving models. To foster continued progress in this field,\nwe make the optimized sensor poses publicly available, providing a valuable\nresource for the research community."}
{"id": "2504.15322", "pdf": "https://arxiv.org/pdf/2504.15322", "abs": "https://arxiv.org/abs/2504.15322", "authors": ["Xiao Zhou", "Yuze Sun", "Jie Wu", "Xiaomeng Huang"], "title": "How to systematically develop an effective AI-based bias correction model?", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "This study introduces ReSA-ConvLSTM, an artificial intelligence (AI)\nframework for systematic bias correction in numerical weather prediction (NWP).\nWe propose three innovations by integrating dynamic climatological\nnormalization, ConvLSTM with temporal causality constraints, and residual\nself-attention mechanisms. The model establishes a physics-aware nonlinear\nmapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years\n(1981-2021) of global atmospheric data, the framework reduces systematic biases\nin 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure\n(SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to\noperational ECMWF outputs. The lightweight architecture (10.6M parameters)\nenables efficient generalization to multiple variables and downstream\napplications, reducing retraining time by 85% for cross-variable correction\nwhile improving ocean model skill through bias-corrected boundary conditions.\nThe ablation experiments demonstrate that our innovations significantly improve\nthe model's correction performance, suggesting that incorporating variable\ncharacteristics into the model helps enhance forecasting skills."}
{"id": "2504.15782", "pdf": "https://arxiv.org/pdf/2504.15782", "abs": "https://arxiv.org/abs/2504.15782", "authors": ["Daniele Baieri", "Riccardo Cicciarella", "Michael Krützen", "Emanuele Rodolà", "Silvia Zuffi"], "title": "Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose Dolphins in Drone-Shot Videos", "categories": ["cs.CV", "cs.GR", "I.4.8; J.3"], "comment": "9 pages, 7 figures", "summary": "We address the problem of estimating the metric 3D shape and motion of wild\ndolphins from monocular video, with the aim of assessing their body condition.\nWhile considerable progress has been made in reconstructing 3D models of\nterrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty\nof observing them in their natural underwater environment. To address this, we\npropose a model-based approach that incorporates a transmission model to\naccount for water-induced occlusion. We apply our method to video captured\nunder different sea conditions. We estimate mass and volume, and compare our\nresults to a manual 2D measurements-based method."}
{"id": "2504.15323", "pdf": "https://arxiv.org/pdf/2504.15323", "abs": "https://arxiv.org/abs/2504.15323", "authors": ["Donggyun Kim", "Chanwoo Kim", "Seunghoon Hong"], "title": "HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "While test-time fine-tuning is beneficial in few-shot learning, the need for\nmultiple backpropagation steps can be prohibitively expensive in real-time or\nlow-resource scenarios. To address this limitation, we propose an approach that\nemulates gradient descent without computing gradients, enabling efficient\ntest-time adaptation. Specifically, we formulate gradient descent as an Euler\ndiscretization of an ordinary differential equation (ODE) and train an\nauxiliary network to predict the task-conditional drift using only the few-shot\nsupport set. The adaptation then reduces to a simple numerical integration\n(e.g., via the Euler method), which requires only a few forward passes of the\nauxiliary network -- no gradients or forward passes of the target model are\nneeded. In experiments on cross-domain few-shot classification using the\nMeta-Dataset and CDFSL benchmarks, our method significantly improves\nout-of-domain performance over the non-fine-tuned baseline while incurring only\n6\\% of the memory cost and 0.02\\% of the computation time of standard\nfine-tuning, thus establishing a practical middle ground between direct\ntransfer and fully fine-tuned approaches."}
{"id": "2504.15783", "pdf": "https://arxiv.org/pdf/2504.15783", "abs": "https://arxiv.org/abs/2504.15783", "authors": ["Johan Öfverstedt", "Elin Lundström", "Håkan Ahlström", "Joel Kullberg"], "title": "Towards prediction of morphological heart age from computed tomography angiography", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Age prediction from medical images or other health-related non-imaging data\nis an important approach to data-driven aging research, providing knowledge of\nhow much information a specific tissue or organ carries about the chronological\nage of the individual. In this work, we studied the prediction of age from\ncomputed tomography angiography (CTA) images, which provide detailed\nrepresentations of the heart morphology, with the goals of (i) studying the\nrelationship between morphology and aging, and (ii) developing a novel\n\\emph{morphological heart age} biomarker. We applied an image\nregistration-based method that standardizes the images from the whole cohort\ninto a single space. We then extracted supervoxels (using unsupervised\nsegmentation), and corresponding robust features of density and local volume,\nwhich provide a detailed representation of the heart morphology while being\nrobust to registration errors. Machine learning models are then trained to fit\nregression models from these features to the chronological age. We applied the\nmethod to a subset of the images from the Swedish CArdioPulomonary bioImage\nStudy (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a\nmean absolute error of $2.74$ years for females and $2.77$ years for males. The\npredictions from different sub-regions of interest were observed to be more\nhighly correlated with the predictions from the whole heart, compared to the\nchronological age, revealing a high consistency in the predictions from\nmorphology. Saliency analysis was also performed on the prediction models to\nstudy what regions are associated positively and negatively with the predicted\nage. This resulted in detailed association maps where the density and volume of\nknown, as well as some novel sub-regions of interest, are determined to be\nimportant. The saliency analysis aids in the interpretability of the models and\ntheir predictions."}
{"id": "2504.15324", "pdf": "https://arxiv.org/pdf/2504.15324", "abs": "https://arxiv.org/abs/2504.15324", "authors": ["Vuong M. Ngo", "Edward Bolger", "Stan Goodwin", "John O'Sullivan", "Dinh Viet Cuong", "Mark Roantree"], "title": "A Graph Based Raman Spectral Processing Technique for Exosome Classification", "categories": ["q-bio.QM", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": "The 23rd International Conference on Artificial Intelligence in\n  Medicine (AIME 2025), LNAI, Springer, 11 pages", "summary": "Exosomes are small vesicles crucial for cell signaling and disease\nbiomarkers. Due to their complexity, an \"omics\" approach is preferable to\nindividual biomarkers. While Raman spectroscopy is effective for exosome\nanalysis, it requires high sample concentrations and has limited sensitivity to\nlipids and proteins. Surface-enhanced Raman spectroscopy helps overcome these\nchallenges. In this study, we leverage Neo4j graph databases to organize 3,045\nRaman spectra of exosomes, enhancing data generalization. To further refine\nspectral analysis, we introduce a novel spectral filtering process that\nintegrates the PageRank Filter with optimal Dimensionality Reduction. This\nmethod improves feature selection, resulting in superior classification\nperformance. Specifically, the Extra Trees model, using our spectral processing\napproach, achieves 0.76 and 0.857 accuracy in classifying hyperglycemic,\nhypoglycemic, and normal exosome samples based on Raman spectra and surface,\nrespectively, with group 10-fold cross-validation. Our results show that\ngraph-based spectral filtering combined with optimal dimensionality reduction\nsignificantly improves classification accuracy by reducing noise while\npreserving key biomarker signals. This novel framework enhances Raman-based\nexosome analysis, expanding its potential for biomedical applications, disease\ndiagnostics, and biomarker discovery."}
{"id": "2504.15786", "pdf": "https://arxiv.org/pdf/2504.15786", "abs": "https://arxiv.org/abs/2504.15786", "authors": ["Ningli Xu", "Rongjun Qin"], "title": "Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views", "categories": ["cs.CV"], "comment": "8 figures", "summary": "Generating consistent ground-view images from satellite imagery is\nchallenging, primarily due to the large discrepancies in viewing angles and\nresolution between satellite and ground-level domains. Previous efforts mainly\nconcentrated on single-view generation, often resulting in inconsistencies\nacross neighboring ground views. In this work, we propose a novel cross-view\nsynthesis approach designed to overcome these challenges by ensuring\nconsistency across ground-view images generated from satellite views. Our\nmethod, based on a fixed latent diffusion model, introduces two conditioning\nmodules: satellite-guided denoising, which extracts high-level scene layout to\nguide the denoising process, and satellite-temporal denoising, which captures\ncamera motion to maintain consistency across multiple generated views. We\nfurther contribute a large-scale satellite-ground dataset containing over\n100,000 perspective pairs to facilitate extensive ground scene or video\ngeneration. Experimental results demonstrate that our approach outperforms\nexisting methods on perceptual and temporal metrics, achieving high\nphotorealism and consistency in multi-view outputs."}
{"id": "2504.15325", "pdf": "https://arxiv.org/pdf/2504.15325", "abs": "https://arxiv.org/abs/2504.15325", "authors": ["Alberto Casagrande", "Francesco Fabris", "Rossano Girometti", "Roberto Pagliarini"], "title": "Significativity Indices for Agreement Values", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "27 pages, 6 figures", "summary": "Agreement measures, such as Cohen's kappa or intraclass correlation, gauge\nthe matching between two or more classifiers. They are used in a wide range of\ncontexts from medicine, where they evaluate the effectiveness of medical\ntreatments and clinical trials, to artificial intelligence, where they can\nquantify the approximation due to the reduction of a classifier. The\nconsistency of different classifiers to a golden standard can be compared\nsimply by using the order induced by their agreement measure with respect to\nthe golden standard itself. Nevertheless, labelling an approach as good or bad\nexclusively by using the value of an agreement measure requires a scale or a\nsignificativity index. Some quality scales have been proposed in the literature\nfor Cohen's kappa, but they are mainly naive, and their boundaries are\narbitrary. This work proposes a general approach to evaluate the\nsignificativity of any agreement value between two classifiers and introduces\ntwo significativity indices: one dealing with finite data sets, the other one\nhandling classification probability distributions. Moreover, this manuscript\nconsiders the computational issues of evaluating such indices and identifies\nsome efficient algorithms to evaluate them."}
{"id": "2504.15792", "pdf": "https://arxiv.org/pdf/2504.15792", "abs": "https://arxiv.org/abs/2504.15792", "authors": ["Dinh Nam Pham", "Torsten Rahne"], "title": "Development and evaluation of a deep learning algorithm for German word recognition from lip movements", "categories": ["cs.CV"], "comment": "English version of journal article in HNO 2022", "summary": "When reading lips, many people benefit from additional visual information\nfrom the lip movements of the speaker, which is, however, very error prone.\nAlgorithms for lip reading with artificial intelligence based on artificial\nneural networks significantly improve word recognition but are not available\nfor the German language. A total of 1806 video clips with only one\nGerman-speaking person each were selected, split into word segments, and\nassigned to word classes using speech-recognition software. In 38,391 video\nsegments with 32 speakers, 18 polysyllabic, visually distinguishable words were\nused to train and validate a neural network. The 3D Convolutional Neural\nNetwork and Gated Recurrent Units models and a combination of both models\n(GRUConv) were compared, as were different image sections and color spaces of\nthe videos. The accuracy was determined in 5000 training epochs. Comparison of\nthe color spaces did not reveal any relevant different correct classification\nrates in the range from 69% to 72%. With a cut to the lips, a significantly\nhigher accuracy of 70% was achieved than when cut to the entire speaker's face\n(34%). With the GRUConv model, the maximum accuracies were 87% with known\nspeakers and 63% in the validation with unknown speakers. The neural network\nfor lip reading, which was first developed for the German language, shows a\nvery high level of accuracy, comparable to English-language algorithms. It\nworks with unknown speakers as well and can be generalized with more word\nclasses."}
{"id": "2504.15328", "pdf": "https://arxiv.org/pdf/2504.15328", "abs": "https://arxiv.org/abs/2504.15328", "authors": ["Usevalad Milasheuski", "Luca Barbieri", "Sanaz Kianoush", "Monica Nicoli", "Stefano Savazzi"], "title": "Bayesian Federated Learning for Continual Training", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Bayesian Federated Learning (BFL) enables uncertainty quantification and\nrobust adaptation in distributed learning. In contrast to the frequentist\napproach, it estimates the posterior distribution of a global model, offering\ninsights into model reliability. However, current BFL methods neglect continual\nlearning challenges in dynamic environments where data distributions shift over\ntime. We propose a continual BFL framework applied to human sensing with radar\ndata collected over several days. Using Stochastic Gradient Langevin Dynamics\n(SGLD), our approach sequentially updates the model, leveraging past posteriors\nto construct the prior for the new tasks. We assess the accuracy, the expected\ncalibration error (ECE) and the convergence speed of our approach against\nseveral baselines. Results highlight the effectiveness of continual Bayesian\nupdates in preserving knowledge and adapting to evolving data."}
{"id": "2504.15796", "pdf": "https://arxiv.org/pdf/2504.15796", "abs": "https://arxiv.org/abs/2504.15796", "authors": ["Jiaqi Tang", "Yinsong Xu", "Qingchao Chen"], "title": "Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object classification models utilizing point cloud data are fundamental for\n3D media understanding, yet they often struggle with unseen or\nout-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain\nadaptation (UDA) methods typically employ a multi-task learning (MTL) framework\nthat combines primary classification tasks with auxiliary self-supervision\ntasks to bridge the gap between cross-domain feature distributions. However,\nour further experiments demonstrate that not all gradients from\nself-supervision tasks are beneficial and some may negatively impact the\nclassification performance. In this paper, we propose a novel solution, termed\nSaliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient\nconflicts. Specifically, our method designs a new scoring mechanism based on\nthe skewness of 3D saliency maps to estimate gradient conflicts without\nrequiring target labels. Leveraging this, we develop a sample selection\nstrategy that dynamically filters out samples whose self-supervision gradients\nare not beneficial for the classification. Our approach is scalable,\nintroducing modest computational overhead, and can be integrated into all the\npoint cloud UDA MTL frameworks. Extensive evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches. In addition, we provide a new\nperspective on understanding the UDA problem through back-propagation analysis."}
{"id": "2504.15330", "pdf": "https://arxiv.org/pdf/2504.15330", "abs": "https://arxiv.org/abs/2504.15330", "authors": ["Mohit Gupta", "Akiko Aizawa", "Rajiv Ratn Shah"], "title": "Med-CoDE: Medical Critique based Disagreement Evaluation Framework", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "8 pages, 4 figures, NAACL SRW 2025", "summary": "The emergence of large language models (LLMs) has significantly influenced\nnumerous fields, including healthcare, by enhancing the capabilities of\nautomated systems to process and generate human-like text. However, despite\ntheir advancements, the reliability and accuracy of LLMs in medical contexts\nremain critical concerns. Current evaluation methods often lack robustness and\nfail to provide a comprehensive assessment of LLM performance, leading to\npotential risks in clinical settings. In this work, we propose Med-CoDE, a\nspecifically designed evaluation framework for medical LLMs to address these\nchallenges. The framework leverages a critique-based approach to quantitatively\nmeasure the degree of disagreement between model-generated responses and\nestablished medical ground truths. This framework captures both accuracy and\nreliability in medical settings. The proposed evaluation framework aims to fill\nthe existing gap in LLM assessment by offering a systematic method to evaluate\nthe quality and trustworthiness of medical LLMs. Through extensive experiments\nand case studies, we illustrate the practicality of our framework in providing\na comprehensive and reliable evaluation of medical LLMs."}
{"id": "2504.15823", "pdf": "https://arxiv.org/pdf/2504.15823", "abs": "https://arxiv.org/abs/2504.15823", "authors": ["Songyan Xie", "Jinghang Wen", "Encheng Su", "Qiucheng Yu"], "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/."}
{"id": "2504.15369", "pdf": "https://arxiv.org/pdf/2504.15369", "abs": "https://arxiv.org/abs/2504.15369", "authors": ["Calvin Luo", "Zilai Zeng", "Yilun Du", "Chen Sun"], "title": "Solving New Tasks by Adapting Internet Video Knowledge", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "ICLR 2025. Project Webpage:\n  https://diffusion-supervision.github.io/adapt2act/", "summary": "Video generative models demonstrate great promise in robotics by serving as\nvisual planners or as policy supervisors. When pretrained on internet-scale\ndata, such video models intimately understand alignment with natural language,\nand can thus facilitate generalization to novel downstream behavior through\ntext-conditioning. However, they may not be sensitive to the specificities of\nthe particular environment the agent inhabits. On the other hand, training\nvideo models on in-domain examples of robotic behavior naturally encodes\nenvironment-specific intricacies, but the scale of available demonstrations may\nnot be sufficient to support generalization to unseen tasks via natural\nlanguage specification. In this work, we investigate different adaptation\ntechniques that integrate in-domain information with large-scale pretrained\nvideo models, and explore the extent to which they enable novel\ntext-conditioned generalization for robotic tasks, while also considering their\nindependent data and resource considerations. We successfully demonstrate\nacross robotic environments that adapting powerful video models with small\nscales of example data can successfully facilitate generalization to novel\nbehaviors. In particular, we present a novel adaptation strategy, termed\nInverse Probabilistic Adaptation, that not only consistently achieves strong\ngeneralization performance across robotic tasks and settings, but also exhibits\nrobustness to the quality of adaptation data, successfully solving novel tasks\neven when only suboptimal in-domain demonstrations are available."}
{"id": "2504.15835", "pdf": "https://arxiv.org/pdf/2504.15835", "abs": "https://arxiv.org/abs/2504.15835", "authors": ["Yiqian Wu", "Malte Prinzler", "Xiaogang Jin", "Siyu Tang"], "title": "Text-based Animatable 3D Avatars with Morphable Model Alignment", "categories": ["cs.CV"], "comment": null, "summary": "The generation of high-quality, animatable 3D head avatars from text has\nenormous potential in content creation applications such as games, movies, and\nembodied virtual assistants. Current text-to-3D generation methods typically\ncombine parametric head models with 2D diffusion models using score\ndistillation sampling to produce 3D-consistent results. However, they struggle\nto synthesize realistic details and suffer from misalignments between the\nappearance and the driving parametric model, resulting in unnatural animation\nresults. We discovered that these limitations stem from ambiguities in the 2D\ndiffusion predictions during 3D avatar distillation, specifically: i) the\navatar's appearance and geometry is underconstrained by the text input, and ii)\nthe semantic alignment between the predictions and the parametric head model is\ninsufficient because the diffusion model alone cannot incorporate information\nfrom the parametric model. In this work, we propose a novel framework,\nAnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with\nmorphable model alignment, and introduce two key strategies to address these\nchallenges. First, we tackle appearance and geometry ambiguities by utilizing\nprior information from a pretrained text-to-3D model to initialize a 3D avatar\nwith robust appearance, geometry, and rigging relationships to the morphable\nmodel. Second, we refine the initial 3D avatar for dynamic expressions using a\nControlNet that is conditioned on semantic and normal maps of the morphable\nmodel to ensure accurate alignment. As a result, our method outperforms\nexisting approaches in terms of synthesis quality, alignment, and animation\nfidelity. Our experiments show that the proposed method advances the state of\nthe art in text-based, animatable 3D head avatar generation."}
{"id": "2504.15376", "pdf": "https://arxiv.org/pdf/2504.15376", "abs": "https://arxiv.org/abs/2504.15376", "authors": ["Zhiqiu Lin", "Siyuan Cen", "Daniel Jiang", "Jay Karhade", "Hewei Wang", "Chancharik Mitra", "Tiffany Ling", "Yuhan Huang", "Sifan Liu", "Mingyu Chen", "Rushikesh Zawar", "Xue Bai", "Yilun Du", "Chuang Gan", "Deva Ramanan"], "title": "Towards Understanding Camera Motions in Any Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project site: https://linzhiqiu.github.io/papers/camerabench/", "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video."}
{"id": "2504.15863", "pdf": "https://arxiv.org/pdf/2504.15863", "abs": "https://arxiv.org/abs/2504.15863", "authors": ["Diego de Oliveira Hitzges", "Suman Ghosh", "Guillermo Gallego"], "title": "DERD-Net: Learning Depth from Event-based Ray Densities", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.SP"], "comment": "13 pages, 3 figures, 14 tables. Project page:\n  https://github.com/tub-rip/DERD-Net", "summary": "Event cameras offer a promising avenue for multi-view stereo depth estimation\nand Simultaneous Localization And Mapping (SLAM) due to their ability to detect\nblur-free 3D edges at high-speed and over broad illumination conditions.\nHowever, traditional deep learning frameworks designed for conventional cameras\nstruggle with the asynchronous, stream-like nature of event data, as their\narchitectures are optimized for discrete, image-like inputs. We propose a\nscalable, flexible and adaptable framework for pixel-wise depth estimation with\nevent cameras in both monocular and stereo setups. The 3D scene structure is\nencoded into disparity space images (DSIs), representing spatial densities of\nrays obtained by back-projecting events into space via known camera poses. Our\nneural network processes local subregions of the DSIs combining 3D convolutions\nand a recurrent structure to recognize valuable patterns for depth prediction.\nLocal processing enables fast inference with full parallelization and ensures\nconstant ultra-low model complexity and memory costs, regardless of camera\nresolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)\ndemonstrate unprecedented effectiveness: (i) using purely monocular data, our\nmethod achieves comparable results to existing stereo methods; (ii) when\napplied to stereo data, it strongly outperforms all state-of-the-art (SOTA)\napproaches, reducing the mean absolute error by at least 42%; (iii) our method\nalso allows for increases in depth completeness by more than 3-fold while still\nyielding a reduction in median absolute error of at least 30%. Given its\nremarkable performance and effective processing of event-data, our framework\nholds strong potential to become a standard approach for using deep learning\nfor event-based depth estimation and SLAM. Project page:\nhttps://github.com/tub-rip/DERD-Net"}
{"id": "2504.15417", "pdf": "https://arxiv.org/pdf/2504.15417", "abs": "https://arxiv.org/abs/2504.15417", "authors": ["Van-Giang Trinh", "Belaid Benhamou", "Sylvain Soliman", "François Fages"], "title": "On the Boolean Network Theory of Datalog$^\\neg$", "categories": ["cs.LO", "cs.AI"], "comment": "48 pages, 7 figures", "summary": "Datalog$^\\neg$ is a central formalism used in a variety of domains ranging\nfrom deductive databases and abstract argumentation frameworks to answer set\nprogramming. Its model theory is the finite counterpart of the logical\nsemantics developed for normal logic programs, mainly based on the notions of\nClark's completion and two-valued or three-valued canonical models including\nsupported, stable, regular and well-founded models. In this paper we establish\na formal link between Datalog$^\\neg$ and Boolean network theory, which was\ninitially introduced by Stuart Kaufman and Ren\\'e Thomas to reason about gene\nregulatory networks. We use previous results from Boolean network theory to\nprove that in the absence of odd cycles in a Datalog$^\\neg$ program, the\nregular models coincide with the stable models, which entails the existence of\nstable models, and in the absence of even cycles, we show the uniqueness of\nstable partial models, which entails the uniqueness of regular models. These\nresults on regular models have been claimed by You and Yuan in 1994 for normal\nlogic programs but we show problems in their definition of well-founded\nstratification and in their proofs that we can fix for negative normal logic\nprograms only. We also give upper bounds on the numbers of stable partial,\nregular, and stable models of a Datalog$^\\neg$ program using the cardinality of\na feedback vertex set in its atom dependency graph. Interestingly, our\nconnection to Boolean network theory also points us to the notion of trap\nspaces for Datalog$^\\neg$ programs. We relate the notions of supported or\nstable trap spaces to the other semantics of Datalog$^\\neg$, and show the\nequivalence between subset-minimal stable trap spaces and regular models."}
{"id": "2504.15865", "pdf": "https://arxiv.org/pdf/2504.15865", "abs": "https://arxiv.org/abs/2504.15865", "authors": ["Lotfi Abdelkrim Mecharbat", "Ibrahim Elmakky", "Martin Takac", "Mohammed Yaqub"], "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS."}
{"id": "2504.15424", "pdf": "https://arxiv.org/pdf/2504.15424", "abs": "https://arxiv.org/abs/2504.15424", "authors": ["Nishath Rajiv Ranasinghe", "Shawn M. Jones", "Michal Kucer", "Ayan Biswas", "Daniel O'Malley", "Alexander Buschmann Most", "Selma Liliane Wanna", "Ajay Sreekumar"], "title": "LLM-Assisted Translation of Legacy FORTRAN Codes to C++: A Cross-Platform Study", "categories": ["cs.SE", "cs.AI", "I.2.2; I.2.7; D.2.3; D.2.4"], "comment": "12 pages, 7 figures, 2 tables", "summary": "Large Language Models (LLMs) are increasingly being leveraged for generating\nand translating scientific computer codes by both domain-experts and non-domain\nexperts. Fortran has served as one of the go to programming languages in legacy\nhigh-performance computing (HPC) for scientific discoveries. Despite growing\nadoption, LLM-based code translation of legacy code-bases has not been\nthoroughly assessed or quantified for its usability. Here, we studied the\napplicability of LLM-based translation of Fortran to C++ as a step towards\nbuilding an agentic-workflow using open-weight LLMs on two different\ncomputational platforms. We statistically quantified the compilation accuracy\nof the translated C++ codes, measured the similarity of the LLM translated code\nto the human translated C++ code, and statistically quantified the output\nsimilarity of the Fortran to C++ translation."}
{"id": "2504.15883", "pdf": "https://arxiv.org/pdf/2504.15883", "abs": "https://arxiv.org/abs/2504.15883", "authors": ["Farida Mohsen", "Samir Belhaouari", "Zubair Shah"], "title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis."}
{"id": "2504.15425", "pdf": "https://arxiv.org/pdf/2504.15425", "abs": "https://arxiv.org/abs/2504.15425", "authors": ["Songyuan Zhang", "Oswin So", "Mitchell Black", "Zachary Serlin", "Chuchu Fan"], "title": "Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "math.OC"], "comment": "28 pages, 16 figures; Accepted by Robotics: Science and Systems 2025", "summary": "Tasks for multi-robot systems often require the robots to collaborate and\ncomplete a team goal while maintaining safety. This problem is usually\nformalized as a constrained Markov decision process (CMDP), which targets\nminimizing a global cost and bringing the mean of constraint violation below a\nuser-defined threshold. Inspired by real-world robotic applications, we define\nsafety as zero constraint violation. While many safe multi-agent reinforcement\nlearning (MARL) algorithms have been proposed to solve CMDPs, these algorithms\nsuffer from unstable training in this setting. To tackle this, we use the\nepigraph form for constrained optimization to improve training stability and\nprove that the centralized epigraph form problem can be solved in a distributed\nfashion by each agent. This results in a novel centralized training distributed\nexecution MARL algorithm named Def-MARL. Simulation experiments on 8 different\ntasks across 2 different simulators show that Def-MARL achieves the best\noverall performance, satisfies safety constraints, and maintains stable\ntraining. Real-world hardware experiments on Crazyflie quadcopters demonstrate\nthe ability of Def-MARL to safely coordinate agents to complete complex\ncollaborative tasks compared to other methods."}
{"id": "2504.15888", "pdf": "https://arxiv.org/pdf/2504.15888", "abs": "https://arxiv.org/abs/2504.15888", "authors": ["Zhiqiang Wei", "Lianqing Zheng", "Jianan Liu", "Tao Huang", "Qing-Long Han", "Wenwen Zhang", "Fengdeng Zhang"], "title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "Accurate 3D semantic occupancy perception is essential for autonomous driving\nin complex environments with diverse and irregular objects. While\nvision-centric methods suffer from geometric inaccuracies, LiDAR-based\napproaches often lack rich semantic information. To address these limitations,\nMS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes\nmiddle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's\ngeometric fidelity with camera-based semantic richness via hierarchical\ncross-modal fusion. The framework introduces innovations at two critical\nstages: (1) In the middle-stage feature fusion, the Gaussian-Geo module\nleverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D\nimage features with dense geometric priors, and the Semantic-Aware module\nenriches LiDAR voxels with semantic context via deformable cross-attention; (2)\nIn the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically\nbalances voxel features across modalities, while the High Classification\nConfidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using\nself-attention-based refinement. Experiments on the nuScenes-OpenOccupancy\nbenchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%\nand a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU\nand +2.4% mIoU. Ablation studies further validate the contribution of each\nmodule, with substantial improvements in small-object perception, demonstrating\nthe practical value of MS-Occ for safety-critical autonomous driving scenarios."}
{"id": "2504.15431", "pdf": "https://arxiv.org/pdf/2504.15431", "abs": "https://arxiv.org/abs/2504.15431", "authors": ["Sungjun Han", "Juyoung Suk", "Suyeong An", "Hyungguk Kim", "Kyuseok Kim", "Wonsuk Yang", "Seungtaek Choi", "Jamin Shin"], "title": "Trillion 7B Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preview version", "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency."}
{"id": "2504.15918", "pdf": "https://arxiv.org/pdf/2504.15918", "abs": "https://arxiv.org/abs/2504.15918", "authors": ["Chang Zong", "Bin Li", "Shoujun Zhou", "Jian Wan", "Lei Zhang"], "title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions", "categories": ["cs.CV", "cs.AI", "cs.HC", "68T45, 68T20"], "comment": "16 pages, 8 figures", "summary": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc."}
{"id": "2504.15440", "pdf": "https://arxiv.org/pdf/2504.15440", "abs": "https://arxiv.org/abs/2504.15440", "authors": ["Andrey Fradkin"], "title": "Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming", "categories": ["cs.CY", "cs.AI", "econ.GN", "q-fin.EC", "K.4; I.2"], "comment": null, "summary": "This paper documents three stylized facts about the demand for Large Language\nModels (LLMs) using data from OpenRouter, a prominent LLM marketplace. First,\nnew models experience rapid initial adoption that stabilizes within weeks.\nSecond, model releases differ substantially in whether they primarily attract\nnew users or substitute demand from competing models. Third, multihoming, using\nmultiple models simultaneously, is common among apps. These findings suggest\nsignificant horizontal and vertical differentiation in the LLM market, implying\nopportunities for providers to maintain demand and pricing power despite rapid\ntechnological advances."}
{"id": "2504.15921", "pdf": "https://arxiv.org/pdf/2504.15921", "abs": "https://arxiv.org/abs/2504.15921", "authors": ["Jian Hu", "Dimitrios Korkinof", "Shaogang Gong", "Mariano Beguerisse-Diaz"], "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication."}
{"id": "2504.15485", "pdf": "https://arxiv.org/pdf/2504.15485", "abs": "https://arxiv.org/abs/2504.15485", "authors": ["Atin Pothiraj", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and data: https://github.com/atinpothiraj/CAPTURe", "summary": "Recognizing and reasoning about occluded (partially or fully hidden) objects\nis vital to understanding visual scenes, as occlusions frequently occur in\nreal-world environments and act as obstacles for spatial comprehension. To test\nmodels' ability to reason about multiple occluded objects, we introduce a novel\ntask, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which\nrequires a model to count objects arranged in a pattern by inferring how the\npattern continues behind an occluder (an object which blocks parts of the\nscene). CAPTURe requires both recognizing visual patterns and reasoning, making\nit a useful testbed for evaluating vision-language models (VLMs) on whether\nthey understand occluded patterns and possess spatial understanding skills. By\nrequiring models to reason about occluded objects, CAPTURe also tests VLMs'\nability to form world models that would allow them to fill in missing\ninformation. CAPTURe consists of two parts: (1) CAPTURe-real, with manually\nfiltered images of real objects in patterns and (2) CAPTURe-synthetic, a\ncontrolled diagnostic with generated patterned images. We evaluate four strong\nVLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models\nstruggle to count on both occluded and unoccluded patterns. Crucially, we find\nthat models perform worse with occlusion, suggesting that VLMs are also\ndeficient in inferring unseen spatial relationships: even the strongest VLMs\nlike GPT-4o fail to count with occlusion. In contrast, we find that humans\nachieve very little error on CAPTURe. We also find that providing auxiliary\ninformation of occluded object locations increases performance, underscoring\nthat the model error comes both from an inability to handle occlusion as well\nas difficulty counting in images."}
{"id": "2504.15928", "pdf": "https://arxiv.org/pdf/2504.15928", "abs": "https://arxiv.org/abs/2504.15928", "authors": ["Meng Wang", "Tian Lin", "Qingshan Hou", "Aidi Lin", "Jingcheng Wang", "Qingsheng Peng", "Truong X. Nguyen", "Danqi Fang", "Ke Zou", "Ting Xu", "Cancan Xue", "Ten Cheer Quek", "Qinkai Yu", "Minxin Liu", "Hui Zhou", "Zixuan Xiao", "Guiqin He", "Huiyu Liang", "Tingkun Shi", "Man Chen", "Linna Liu", "Yuanyuan Peng", "Lianyu Wang", "Qiuming Hu", "Junhong Chen", "Zhenhua Zhang", "Cheng Chen", "Yitian Zhao", "Dianbo Liu", "Jianhua Wu", "Xinjian Chen", "Changqing Zhang", "Triet Thanh Nguyen", "Yanda Meng", "Yalin Zheng", "Yih Chung Tham", "Carol Y. Cheung", "Huazhu Fu", "Haoyu Chen", "Ching-Yu Cheng"], "title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers."}
{"id": "2504.15497", "pdf": "https://arxiv.org/pdf/2504.15497", "abs": "https://arxiv.org/abs/2504.15497", "authors": ["Noah Subedar", "Taeui Kim", "Saathwick Venkataramalingam"], "title": "Scalable APT Malware Classification via Parallel Feature Extraction and GPU-Accelerated Learning", "categories": ["cs.CR", "cs.AI", "I.2.0; I.2.6; K.6.5"], "comment": "26 pages, 54 figures, 14 tables", "summary": "This paper presents an underlying framework for both automating and\naccelerating malware classification, more specifically, mapping malicious\nexecutables to known Advanced Persistent Threat (APT) groups. The main feature\nof this analysis is the assembly-level instructions present in executables\nwhich are also known as opcodes. The collection of such opcodes on many\nmalicious samples is a lengthy process; hence, open-source reverse engineering\ntools are used in tandem with scripts that leverage parallel computing to\nanalyze multiple files at once. Traditional and deep learning models are\napplied to create models capable of classifying malware samples. One-gram and\ntwo-gram datasets are constructed and used to train models such as SVM, KNN,\nand Decision Tree; however, they struggle to provide adequate results without\nrelying on metadata to support n-gram sequences. The computational limitations\nof such models are overcome with convolutional neural networks (CNNs) and\nheavily accelerated using graphical compute unit (GPU) resources."}
{"id": "2504.15929", "pdf": "https://arxiv.org/pdf/2504.15929", "abs": "https://arxiv.org/abs/2504.15929", "authors": ["Saban Ozturk", "Melih B. Yilmaz", "Muti Kara", "M. Talat Yavuz", "Aykut Koç", "Tolga Çukur"], "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods."}
{"id": "2504.15499", "pdf": "https://arxiv.org/pdf/2504.15499", "abs": "https://arxiv.org/abs/2504.15499", "authors": ["James Mickens", "Sarah Radway", "Ravi Netravali"], "title": "Guillotine: Hypervisors for Isolating Malicious AIs", "categories": ["cs.CR", "cs.AI", "cs.OS"], "comment": "To be published in the ACM SIGOPS 2025 Workshop on Hot Topics in\n  Operating Systems", "summary": "As AI models become more embedded in critical sectors like finance,\nhealthcare, and the military, their inscrutable behavior poses ever-greater\nrisks to society. To mitigate this risk, we propose Guillotine, a hypervisor\narchitecture for sandboxing powerful AI models -- models that, by accident or\nmalice, can generate existential threats to humanity. Although Guillotine\nborrows some well-known virtualization techniques, Guillotine must also\nintroduce fundamentally new isolation mechanisms to handle the unique threat\nmodel posed by existential-risk AIs. For example, a rogue AI may try to\nintrospect upon hypervisor software or the underlying hardware substrate to\nenable later subversion of that control plane; thus, a Guillotine hypervisor\nrequires careful co-design of the hypervisor software and the CPUs, RAM, NIC,\nand storage devices that support the hypervisor software, to thwart side\nchannel leakage and more generally eliminate mechanisms for AI to exploit\nreflection-based vulnerabilities. Beyond such isolation at the software,\nnetwork, and microarchitectural layers, a Guillotine hypervisor must also\nprovide physical fail-safes more commonly associated with nuclear power plants,\navionic platforms, and other types of mission critical systems. Physical\nfail-safes, e.g., involving electromechanical disconnection of network cables,\nor the flooding of a datacenter which holds a rogue AI, provide defense in\ndepth if software, network, and microarchitectural isolation is compromised and\na rogue AI must be temporarily shut down or permanently destroyed."}
{"id": "2504.15931", "pdf": "https://arxiv.org/pdf/2504.15931", "abs": "https://arxiv.org/abs/2504.15931", "authors": ["Ekaterina Kondrateva", "Sandzhi Barg", "Mikhail Vasiliev"], "title": "Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and reproducible brain morphometry from structural MRI is critical\nfor monitoring neuroanatomical changes across time and across imaging domains.\nAlthough deep learning has accelerated segmentation workflows, scanner-induced\nvariability and reproducibility limitations remain-especially in longitudinal\nand multi-site settings. In this study, we benchmark two modern segmentation\npipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the\nmost widely adopted tools in neuroimaging.\n  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and\na 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation\nvariability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),\nand Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume\nvariation in small subcortical structures such as the amygdala and ventral\ndiencephalon, even under controlled test-retest conditions. This raises a key\nquestion: is it feasible to detect subtle longitudinal changes on the order of\n5-10% in pea-sized brain regions, given the magnitude of domain-induced\nmorphometric noise?\n  We further analyze the effects of registration templates and interpolation\nmodes, and propose surface-based quality filtering to improve segmentation\nreliability. This study provides a reproducible benchmark for morphometric\nreproducibility and emphasizes the need for harmonization strategies in\nreal-world neuroimaging studies.\n  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation"}
{"id": "2504.15515", "pdf": "https://arxiv.org/pdf/2504.15515", "abs": "https://arxiv.org/abs/2504.15515", "authors": ["Wuchen Li"], "title": "Transport f divergences", "categories": ["math.ST", "cs.AI", "cs.IT", "math.IT", "stat.TH"], "comment": "Comments are welcome", "summary": "We define a class of divergences to measure differences between probability\ndensity functions in one-dimensional sample space. The construction is based on\nthe convex function with the Jacobi operator of mapping function that\npushforwards one density to the other. We call these information measures {\\em\ntransport $f$-divergences}. We present several properties of transport\n$f$-divergences, including invariances, convexities, variational formulations,\nand Taylor expansions in terms of mapping functions. Examples of transport\n$f$-divergences in generative models are provided."}
{"id": "2504.15932", "pdf": "https://arxiv.org/pdf/2504.15932", "abs": "https://arxiv.org/abs/2504.15932", "authors": ["Wang Lin", "Liyu Jia", "Wentao Hu", "Kaihang Pan", "Zhongqi Yue", "Wei Zhao", "Jingyuan Chen", "Fei Wu", "Hanwang Zhang"], "title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in video generation, producing videos that adhere to\nphysical laws remains a significant challenge. Traditional diffusion-based\nmethods struggle to extrapolate to unseen physical conditions (eg, velocity)\ndue to their reliance on data-driven approximations. To address this, we\npropose to integrate symbolic reasoning and reinforcement learning to enforce\nphysical consistency in video generation. We first introduce the Diffusion\nTimestep Tokenizer (DDT), which learns discrete, recursive visual tokens by\nrecovering visual attributes lost during the diffusion process. The recursive\nvisual tokens enable symbolic reasoning by a large language model. Based on it,\nwe propose the Phys-AR framework, which consists of two stages: The first stage\nuses supervised fine-tuning to transfer symbolic knowledge, while the second\nstage applies reinforcement learning to optimize the model's reasoning\nabilities through reward functions based on physical conditions. Our approach\nallows the model to dynamically adjust and improve the physical properties of\ngenerated videos, ensuring adherence to physical laws. Experimental results\ndemonstrate that PhysAR can generate videos that are physically consistent."}
{"id": "2504.15524", "pdf": "https://arxiv.org/pdf/2504.15524", "abs": "https://arxiv.org/abs/2504.15524", "authors": ["Qiyao Wang", "Guhong Chen", "Hongbo Wang", "Huaren Liu", "Minghui Zhu", "Zhifei Qin", "Linwei Li", "Yilin Yue", "Shiqiang Wang", "Jiayan Li", "Yihang Wu", "Ziqiang Liu", "Longze Chen", "Run Luo", "Liyang Fan", "Jiaming Li", "Lei Zhang", "Kan Xu", "Hongfei Lin", "Hamid Alinejad-Rokny", "Shiwen Ni", "Yuan Lin", "Min Yang"], "title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property", "categories": ["cs.CL", "cs.AI"], "comment": "89 pages, 75 figures, 55 tables", "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain."}
{"id": "2504.15958", "pdf": "https://arxiv.org/pdf/2504.15958", "abs": "https://arxiv.org/abs/2504.15958", "authors": ["Zebin Yao", "Lei Ren", "Huixing Jiang", "Chen Wei", "Xiaojie Wang", "Ruifan Li", "Fangxiang Feng"], "title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor."}
{"id": "2504.15546", "pdf": "https://arxiv.org/pdf/2504.15546", "abs": "https://arxiv.org/abs/2504.15546", "authors": ["Jayachandu Bandlamudi", "Ritwik Chaudhuri", "Neelamadhav Gantayat", "Kushal Mukherjee", "Prerna Agarwal", "Renuka Sindhgatta", "Sameep Mehta"], "title": "A Framework for Testing and Adapting REST APIs as LLM Tools", "categories": ["cs.SE", "cs.AI", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are enabling autonomous agents to perform\ncomplex workflows using external tools or functions, often provided via REST\nAPIs in enterprise systems. However, directly utilizing these APIs as tools\nposes challenges due to their complex input schemas, elaborate responses, and\noften ambiguous documentation. Current benchmarks for tool testing do not\nadequately address these complexities, leading to a critical gap in evaluating\nAPI readiness for agent-driven automation. In this work, we present a novel\ntesting framework aimed at evaluating and enhancing the readiness of REST APIs\nto function as tools for LLM-based agents. Our framework transforms apis as\ntools, generates comprehensive test cases for the APIs, translates tests cases\ninto natural language instructions suitable for agents, enriches tool\ndefinitions and evaluates the agent's ability t correctly invoke the API and\nprocess its inputs and responses. To provide actionable insights, we analyze\nthe outcomes of 750 test cases, presenting a detailed taxonomy of errors,\nincluding input misinterpretation, output handling inconsistencies, and schema\nmismatches. Additionally, we classify these test cases to streamline debugging\nand refinement of tool integrations. This work offers a foundational step\ntoward enabling enterprise APIs as tools, improving their usability in\nagent-based applications."}
{"id": "2504.15991", "pdf": "https://arxiv.org/pdf/2504.15991", "abs": "https://arxiv.org/abs/2504.15991", "authors": ["Leonardo Olivi", "Edoardo Santero Mormile", "Enzo Tartaglione"], "title": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field."}
{"id": "2504.15549", "pdf": "https://arxiv.org/pdf/2504.15549", "abs": "https://arxiv.org/abs/2504.15549", "authors": ["Anjali Khurana", "Xiaotian Su", "April Yi Wang", "Parmit K Chilana"], "title": "Do It For Me vs. Do It With Me: Investigating User Perceptions of Different Paradigms of Automation in Copilots for Feature-Rich Software", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted for publication in the CHI Conference on Human Factors in\n  Computing Systems (CHI 2025), April 26 - May 1, 2025, Yokohama, Japan", "summary": "Large Language Model (LLM)-based in-application assistants, or copilots, can\nautomate software tasks, but users often prefer learning by doing, raising\nquestions about the optimal level of automation for an effective user\nexperience. We investigated two automation paradigms by designing and\nimplementing a fully automated copilot (AutoCopilot) and a semi-automated\ncopilot (GuidedCopilot) that automates trivial steps while offering\nstep-by-step visual guidance. In a user study (N=20) across data analysis and\nvisual design tasks, GuidedCopilot outperformed AutoCopilot in user control,\nsoftware utility, and learnability, especially for exploratory and creative\ntasks, while AutoCopilot saved time for simpler visual tasks. A follow-up\ndesign exploration (N=10) enhanced GuidedCopilot with task-and state-aware\nfeatures, including in-context preview clips and adaptive instructions. Our\nfindings highlight the critical role of user control and tailored guidance in\ndesigning the next generation of copilots that enhance productivity, support\ndiverse skill levels, and foster deeper software engagement."}
{"id": "2504.16003", "pdf": "https://arxiv.org/pdf/2504.16003", "abs": "https://arxiv.org/abs/2504.16003", "authors": ["Yachun Mi", "Yu Li", "Weicheng Meng", "Chaofeng Chen", "Chen Hui", "Shaohui Liu"], "title": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth of long-duration, high-definition videos has made efficient\nvideo quality assessment (VQA) a critical challenge. Existing research\ntypically tackles this problem through two main strategies: reducing model\nparameters and resampling inputs. However, light-weight Convolution Neural\nNetworks (CNN) and Transformers often struggle to balance efficiency with high\nperformance due to the requirement of long-range modeling capabilities.\nRecently, the state-space model, particularly Mamba, has emerged as a promising\nalternative, offering linear complexity with respect to sequence length.\nMeanwhile, efficient VQA heavily depends on resampling long sequences to\nminimize computational costs, yet current resampling methods are often weak in\npreserving essential semantic information. In this work, we present MVQA, a\nMamba-based model designed for efficient VQA along with a novel Unified\nSemantic and Distortion Sampling (USDS) approach. USDS combines semantic patch\nsampling from low-resolution videos and distortion patch sampling from\noriginal-resolution videos. The former captures semantically dense regions,\nwhile the latter retains critical distortion details. To prevent computation\nincrease from dual inputs, we propose a fusion mechanism using pre-defined\nmasks, enabling a unified sampling strategy that captures both semantic and\nquality information without additional computational burden. Experiments show\nthat the proposed MVQA, equipped with USDS, achieve comparable performance to\nstate-of-the-art methods while being $2\\times$ as fast and requiring only $1/5$\nGPU memory."}
{"id": "2504.15564", "pdf": "https://arxiv.org/pdf/2504.15564", "abs": "https://arxiv.org/abs/2504.15564", "authors": ["Musfiqur Rahman", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "This paper was submitted to the 29th International Conference on\n  Evaluation and Assessment in Software Engineering (EASE 2025) AI models/data\n  track", "summary": "Recent advancements in large language models (LLMs) have demonstrated\npromising capabilities in code generation tasks. However, most existing\nbenchmarks focus on isolated functions and fail to capture the complexity of\nreal-world, class-level software structures. To address this gap, we introduce\na large-scale, Python class-level dataset curated from $13{,}174$ real-world\nopen-source projects. The dataset contains over 842,000 class skeletons, each\nincluding class and method signatures, along with associated docstrings when\navailable. We preserve structural and contextual dependencies critical to\nrealistic software development scenarios and enrich the dataset with static\ncode metrics to support downstream analysis. To evaluate the usefulness of this\ndataset, we use extracted class skeletons as prompts for GPT-4 to generate full\nclass implementations. Results show that the LLM-generated classes exhibit\nstrong lexical and structural similarity to human-written counterparts, with\naverage ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.\nThese findings confirm that well-structured prompts derived from real-world\nclass skeletons significantly enhance LLM performance in class-level code\ngeneration. This dataset offers a valuable resource for benchmarking, training,\nand improving LLMs in realistic software engineering contexts."}
{"id": "2504.16016", "pdf": "https://arxiv.org/pdf/2504.16016", "abs": "https://arxiv.org/abs/2504.16016", "authors": ["Xinyuan Song", "Yangfan He", "Sida Li", "Jianhui Wang", "Hongyang He", "Xinhang Yuan", "Ruoyu Wang", "Jiaqi Chen", "Keqin Li", "Kuan Lu", "Menghao Huo", "Binxu Li", "Pei Liu"], "title": "Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2501.04606", "summary": "Adapter-based methods are commonly used to enhance model performance with\nminimal additional complexity, especially in video editing tasks that require\nframe-to-frame consistency. By inserting small, learnable modules into\npretrained diffusion models, these adapters can maintain temporal coherence\nwithout extensive retraining. Approaches that incorporate prompt learning with\nboth shared and frame-specific tokens are particularly effective in preserving\ncontinuity across frames at low training cost. In this work, we want to provide\na general theoretical framework for adapters that maintain frame consistency in\nDDIM-based models under a temporal consistency loss. First, we prove that the\ntemporal consistency objective is differentiable under bounded feature norms,\nand we establish a Lipschitz bound on its gradient. Second, we show that\ngradient descent on this objective decreases the loss monotonically and\nconverges to a local minimum if the learning rate is within an appropriate\nrange. Finally, we analyze the stability of modules in the DDIM inversion\nprocedure, showing that the associated error remains controlled. These\ntheoretical findings will reinforce the reliability of diffusion-based video\nediting methods that rely on adapter strategies and provide theoretical\ninsights in video generation tasks."}
{"id": "2504.15585", "pdf": "https://arxiv.org/pdf/2504.15585", "abs": "https://arxiv.org/abs/2504.15585", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Junyuan Mao", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Xinfeng Li", "Yifan Jiang", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Tianlin Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Tianwei Zhang", "Xingjun Ma", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Yuval Elovici", "Bhavya Kailkhura", "Bo Li", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Shuicheng Yan", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field."}
{"id": "2504.16023", "pdf": "https://arxiv.org/pdf/2504.16023", "abs": "https://arxiv.org/abs/2504.16023", "authors": ["Song Wang", "Xiaolu Liu", "Lingdong Kong", "Jianyun Xu", "Chunyong Hu", "Gongfan Fang", "Wentong Li", "Jianke Zhu", "Xinchao Wang"], "title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Self-supervised representation learning for point cloud has demonstrated\neffectiveness in improving pre-trained model performance across diverse tasks.\nHowever, as pre-trained models grow in complexity, fully fine-tuning them for\ndownstream applications demands substantial computational and storage\nresources. Parameter-efficient fine-tuning (PEFT) methods offer a promising\nsolution to mitigate these resource requirements, yet most current approaches\nrely on complex adapter and prompt mechanisms that increase tunable parameters.\nIn this paper, we propose PointLoRA, a simple yet effective method that\ncombines low-rank adaptation (LoRA) with multi-scale token selection to\nefficiently fine-tune point cloud models. Our approach embeds LoRA layers\nwithin the most parameter-intensive components of point cloud transformers,\nreducing the need for tunable parameters while enhancing global feature\ncapture. Additionally, multi-scale token selection extracts critical local\ninformation to serve as prompts for downstream fine-tuning, effectively\ncomplementing the global context captured by LoRA. The experimental results\nacross various pre-trained models and three challenging public datasets\ndemonstrate that our approach achieves competitive performance with only 3.43%\nof the trainable parameters, making it highly effective for\nresource-constrained applications. Source code is available at:\nhttps://github.com/songw-zju/PointLoRA."}
{"id": "2504.15587", "pdf": "https://arxiv.org/pdf/2504.15587", "abs": "https://arxiv.org/abs/2504.15587", "authors": ["Zimo Yan", "Jie Zhang", "Zheng Xie", "Chang Liu", "Yizhen Liu", "Yiping Song"], "title": "MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Molecular generation plays an important role in drug discovery and materials\nscience, especially in data-scarce scenarios where traditional generative\nmodels often struggle to achieve satisfactory conditional generalization. To\naddress this challenge, we propose MetaMolGen, a first-order\nmeta-learning-based molecular generator designed for few-shot and\nproperty-conditioned molecular generation. MetaMolGen standardizes the\ndistribution of graph motifs by mapping them to a normalized latent space, and\nemploys a lightweight autoregressive sequence model to generate SMILES\nsequences that faithfully reflect the underlying molecular structure. In\naddition, it supports conditional generation of molecules with target\nproperties through a learnable property projector integrated into the\ngenerative process.Experimental results demonstrate that MetaMolGen\nconsistently generates valid and diverse SMILES sequences under low-data\nregimes, outperforming conventional baselines. This highlights its advantage in\nfast adaptation and efficient conditional generation for practical molecular\ndesign."}
{"id": "2504.16030", "pdf": "https://arxiv.org/pdf/2504.16030", "abs": "https://arxiv.org/abs/2504.16030", "authors": ["Joya Chen", "Ziyun Zeng", "Yiqi Lin", "Wei Li", "Zejun Ma", "Mike Zheng Shou"], "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale", "categories": ["cs.CV"], "comment": "CVPR 2025. If any references are missing, please contact\n  joyachen@u.nus.edu", "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc."}
{"id": "2504.15604", "pdf": "https://arxiv.org/pdf/2504.15604", "abs": "https://arxiv.org/abs/2504.15604", "authors": ["Pavan Yadav", "Nikhil Khandalkar", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models", "categories": ["cs.CL", "cs.AI"], "comment": "75 pages, 60 figures", "summary": "Language models have made significant progress in generating coherent text\nand predicting next tokens based on input prompts. This study compares the\nnext-token prediction performance of two well-known models: OpenAI's GPT-2 and\nMeta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their\ncapabilities, we built a dataset from 10 short stories sourced from the Explore\nToM Dataset. We enhanced these stories by programmatically inserting additional\nsentences (infills) using GPT-4, creating variations that introduce different\nlevels of contextual complexity. This setup enables analysis of how increasing\ncontext affects model performance. We tested both models under four temperature\nsettings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next\ntoken across three reasoning levels. Zero-order reasoning involves tracking the\nstate, either current (ground truth) or past (memory). First-order reasoning\nconcerns understanding another's mental state (e.g., \"Does Anne know the apple\nis salted?\"). Second-order reasoning adds recursion (e.g., \"Does Anne think\nthat Charles knows the apple is salted?\").\n  Our results show that adding more infill sentences slightly reduces\nprediction accuracy, as added context increases complexity and ambiguity.\nLlama-2 consistently outperforms GPT-2 in prediction accuracy, especially at\nlower temperatures, demonstrating greater confidence in selecting the most\nprobable token. As reasoning complexity rises, model responses diverge more.\nNotably, GPT-2 and Llama-2 display greater variability in predictions during\nfirst- and second-order reasoning tasks. These findings illustrate how model\narchitecture, temperature, and contextual complexity influence next-token\nprediction, contributing to a better understanding of the strengths and\nlimitations of current language models."}
{"id": "2504.16047", "pdf": "https://arxiv.org/pdf/2504.16047", "abs": "https://arxiv.org/abs/2504.16047", "authors": ["Frank Li", "Hari Trivedi", "Bardia Khosravi", "Theo Dapamede", "Mohammadreza Chavoshi", "Abdulhameed Dere", "Rohan Satya Isaac", "Aawez Mansuri", "Janice Newsome", "Saptarshi Purkayastha", "Judy Gichoya"], "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology."}
{"id": "2504.15634", "pdf": "https://arxiv.org/pdf/2504.15634", "abs": "https://arxiv.org/abs/2504.15634", "authors": ["Peizheng Liu", "Hitoshi Iba"], "title": "Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformer-based architectures have recently propelled advances in sequence\nmodeling across domains, but their application to the hydrophobic-hydrophilic\n(H-P) model for protein folding remains relatively unexplored. In this work, we\nadapt a Deep Q-Network (DQN) integrated with attention mechanisms\n(Transformers) to address the 3D H-P protein folding problem. Our system\nformulates folding decisions as a self-avoiding walk in a reinforced\nenvironment, and employs a specialized reward function based on favorable\nhydrophobic interactions. To improve performance, the method incorporates\nvalidity check including symmetry-breaking constraints, dueling and double\nQ-learning, and prioritized replay to focus learning on critical transitions.\nExperimental evaluations on standard benchmark sequences demonstrate that our\napproach achieves several known best solutions for shorter sequences, and\nobtains near-optimal results for longer chains. This study underscores the\npromise of attention-based reinforcement learning for protein folding, and\ncreated a prototype of Transformer-based Q-network structure for 3-dimensional\nlattice models."}
{"id": "2504.16061", "pdf": "https://arxiv.org/pdf/2504.16061", "abs": "https://arxiv.org/abs/2504.16061", "authors": ["Sangeet Khemlani", "Tyler Tran", "Nathaniel Gyory", "Anthony M. Harrison", "Wallace E. Lawson", "Ravenna Thielstrom", "Hunter Thompson", "Taaren Singh", "J. Gregory Trafton"], "title": "Vision language models are unreliable at trivial spatial cognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing."}
{"id": "2504.15637", "pdf": "https://arxiv.org/pdf/2504.15637", "abs": "https://arxiv.org/abs/2504.15637", "authors": ["Farnaz Behrang", "Zhizhou Zhang", "Georgian-Vlad Saioc", "Peng Liu", "Milind Chabbi"], "title": "DR.FIX: Automatically Fixing Data Races at Industry Scale", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "comment": "To appear in PLDI 2025", "summary": "Data races are a prevalent class of concurrency bugs in shared-memory\nparallel programs, posing significant challenges to software reliability and\nreproducibility. While there is an extensive body of research on detecting data\nraces and a wealth of practical detection tools across various programming\nlanguages, considerably less effort has been directed toward automatically\nfixing data races at an industrial scale. In large codebases, data races are\ncontinuously introduced and exhibit myriad patterns, making automated fixing\nparticularly challenging.\n  In this paper, we tackle the problem of automatically fixing data races at an\nindustrial scale. We present Dr.Fix, a tool that combines large language models\n(LLMs) with program analysis to generate fixes for data races in real-world\nsettings, effectively addressing a broad spectrum of racy patterns in complex\ncode contexts. Implemented for Go--the programming language widely used in\nmodern microservice architectures where concurrency is pervasive and data races\nare common--Dr.Fix seamlessly integrates into existing development workflows.\nWe detail the design of Dr.Fix and examine how individual design choices\ninfluence the quality of the fixes produced. Over the past 18 months, Dr.Fix\nhas been integrated into developer workflows at Uber demonstrating its\npractical utility. During this period, Dr.Fix produced patches for 224 (55%)\nfrom a corpus of 404 data races spanning various categories; 193 of these\npatches (86%) were accepted by more than a hundred developers via code reviews\nand integrated into the codebase."}
{"id": "2504.16064", "pdf": "https://arxiv.org/pdf/2504.16064", "abs": "https://arxiv.org/abs/2504.16064", "authors": ["Theodoros Kouzelis", "Efstathios Karypidis", "Ioannis Kakogeorgiou", "Spyros Gidaris", "Nikos Komodakis"], "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling."}
{"id": "2504.15640", "pdf": "https://arxiv.org/pdf/2504.15640", "abs": "https://arxiv.org/abs/2504.15640", "authors": ["Hongtao Wang", "Taiyan Zhang", "Renchi Yang", "Jianliang Xu"], "title": "Cost-Effective Text Clustering with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs."}
{"id": "2504.16072", "pdf": "https://arxiv.org/pdf/2504.16072", "abs": "https://arxiv.org/abs/2504.16072", "authors": ["Long Lian", "Yifan Ding", "Yunhao Ge", "Sifei Liu", "Hanzi Mao", "Boyi Li", "Marco Pavone", "Ming-Yu Liu", "Trevor Darrell", "Adam Yala", "Yin Cui"], "title": "Describe Anything: Detailed Localized Image and Video Captioning", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://describe-anything.github.io/", "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning."}
{"id": "2504.15654", "pdf": "https://arxiv.org/pdf/2504.15654", "abs": "https://arxiv.org/abs/2504.15654", "authors": ["Md Abdul Baset Sarker", "Art Nguyen", "Sigmond Kukla", "Kevin Fite", "Masudul H. Imtiaz"], "title": "A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces a novel AI vision-enabled pediatric prosthetic hand\ndesigned to assist children aged 10-12 with upper limb disabilities. The\nprosthesis features an anthropomorphic appearance, multi-articulating\nfunctionality, and a lightweight design that mimics a natural hand, making it\nboth accessible and affordable for low-income families. Using 3D printing\ntechnology and integrating advanced machine vision, sensing, and embedded\ncomputing, the prosthetic hand offers a low-cost, customizable solution that\naddresses the limitations of current myoelectric prostheses. A micro camera is\ninterfaced with a low-power FPGA for real-time object detection and assists\nwith precise grasping. The onboard DL-based object detection and grasp\nclassification models achieved accuracies of 96% and 100% respectively. In the\nforce prediction, the mean absolute error was found to be 0.018. The features\nof the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted\nmicro camera for artificial sensing, enabling a wide range of hand-based tasks;\nb) real-time object detection and distance estimation for precise grasping; and\nc) ultra-low-power operation that delivers high performance within constrained\npower and resource limits."}
{"id": "2504.16080", "pdf": "https://arxiv.org/pdf/2504.16080", "abs": "https://arxiv.org/abs/2504.16080", "authors": ["Le Zhuo", "Liangbing Zhao", "Sayak Paul", "Yue Liao", "Renrui Zhang", "Yi Xin", "Peng Gao", "Mohamed Elhoseiny", "Hongsheng Li"], "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning", "categories": ["cs.CV"], "comment": "All code, checkpoints, and datasets are available at\n  \\url{https://diffusion-cot.github.io/reflection2perfection}", "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks."}
{"id": "2504.15659", "pdf": "https://arxiv.org/pdf/2504.15659", "abs": "https://arxiv.org/abs/2504.15659", "authors": ["Anjiang Wei", "Huanmi Tan", "Tarun Suresh", "Daniel Mendoza", "Thiago S. F. X. Teixeira", "Ke Wang", "Caroline Trippel", "Alex Aiken"], "title": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation."}
{"id": "2504.16081", "pdf": "https://arxiv.org/pdf/2504.16081", "abs": "https://arxiv.org/abs/2504.16081", "authors": ["Yimu Wang", "Xuye Liu", "Wei Pang", "Li Ma", "Shuai Yuan", "Paul Debevec", "Ning Yu"], "title": "Survey of Video Diffusion Models: Foundations, Implementations, and Applications", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion."}
{"id": "2504.15663", "pdf": "https://arxiv.org/pdf/2504.15663", "abs": "https://arxiv.org/abs/2504.15663", "authors": ["Ju Yeon Kang", "Ji Won Yoon", "Semin Kim", "Min Hyun Han", "Nam Soo Kim"], "title": "FADEL: Uncertainty-aware Fake Audio Detection with Evidential Deep Learning", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted at ICASSP 2025", "summary": "Recently, fake audio detection has gained significant attention, as\nadvancements in speech synthesis and voice conversion have increased the\nvulnerability of automatic speaker verification (ASV) systems to spoofing\nattacks. A key challenge in this task is generalizing models to detect unseen,\nout-of-distribution (OOD) attacks. Although existing approaches have shown\npromising results, they inherently suffer from overconfidence issues due to the\nusage of softmax for classification, which can produce unreliable predictions\nwhen encountering unpredictable spoofing attempts. To deal with this\nlimitation, we propose a novel framework called fake audio detection with\nevidential learning (FADEL). By modeling class probabilities with a Dirichlet\ndistribution, FADEL incorporates model uncertainty into its predictions,\nthereby leading to more robust performance in OOD scenarios. Experimental\nresults on the ASVspoof2019 Logical Access (LA) and ASVspoof2021 LA datasets\nindicate that the proposed method significantly improves the performance of\nbaseline models. Furthermore, we demonstrate the validity of uncertainty\nestimation by analyzing a strong correlation between average uncertainty and\nequal error rate (EER) across different spoofing algorithms."}
{"id": "2504.16082", "pdf": "https://arxiv.org/pdf/2504.16082", "abs": "https://arxiv.org/abs/2504.16082", "authors": ["Ziqi Pang", "Yu-Xiong Wang"], "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding", "categories": ["cs.CV"], "comment": "Preprint", "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video"}
{"id": "2504.15707", "pdf": "https://arxiv.org/pdf/2504.15707", "abs": "https://arxiv.org/abs/2504.15707", "authors": ["Yannic Neuhaus", "Matthias Hein"], "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE ."}
{"id": "2504.16083", "pdf": "https://arxiv.org/pdf/2504.16083", "abs": "https://arxiv.org/abs/2504.16083", "authors": ["Yucheng Li", "Huiqiang Jiang", "Chengruidong Zhang", "Qianhui Wu", "Xufang Luo", "Surin Ahn", "Amir H. Abdi", "Dongsheng Li", "Jianfeng Gao", "Yuqing Yang", "Lili Qiu"], "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference."}
{"id": "2504.15724", "pdf": "https://arxiv.org/pdf/2504.15724", "abs": "https://arxiv.org/abs/2504.15724", "authors": ["Yiannis Papageorgiou", "Yannis Thomas", "Alexios Filippakopoulos", "Ramin Khalili", "Iordanis Koutsopoulos"], "title": "Collaborative Split Federated Learning with Parallel Training and Aggregation", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Federated learning (FL) operates based on model exchanges between the server\nand the clients, and it suffers from significant client-side computation and\ncommunication burden. Split federated learning (SFL) arises a promising\nsolution by splitting the model into two parts, that are trained sequentially:\nthe clients train the first part of the model (client-side model) and transmit\nit to the server that trains the second (server-side model). Existing SFL\nschemes though still exhibit long training delays and significant communication\noverhead, especially when clients of different computing capability\nparticipate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a\nnovel scheme that splits the model into three parts, namely the model parts\ntrained at the computationally weak clients, the ones trained at the\ncomputationally strong clients, and the ones at the server. Unlike existing\nworks, C-SFL enables parallel training and aggregation of model's parts at the\nclients and at the server, resulting in reduced training delays and\ncommmunication overhead while improving the model's accuracy. Experiments\nverify the multiple gains of C-SFL against the existing schemes."}
{"id": "2504.09697", "pdf": "https://arxiv.org/pdf/2504.09697", "abs": "https://arxiv.org/abs/2504.09697", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "24 pages, 21 figures. Figure 9(b) has been accepted by CVPR AI Art\n  Gallery 2025", "summary": "Recent prompt-based image editing models have demonstrated impressive\nprompt-following capability at structural editing tasks. However, existing\nmodels still fail to perform local edits, follow detailed editing prompts, or\nmaintain global image quality beyond a single editing step. To address these\nchallenges, we introduce SPICE, a training-free workflow that accepts arbitrary\nresolutions and aspect ratios, accurately follows user requirements, and\nimproves image quality consistently during more than 100 editing steps. By\nsynergizing the strengths of a base diffusion model and a Canny edge ControlNet\nmodel, SPICE robustly handles free-form editing instructions from the user.\nSPICE outperforms state-of-the-art baselines on a challenging realistic\nimage-editing dataset consisting of semantic editing (object addition, removal,\nreplacement, and background change), stylistic editing (texture changes), and\nstructural editing (action change) tasks. Not only does SPICE achieve the\nhighest quantitative performance according to standard evaluation metrics, but\nit is also consistently preferred by users over existing image-editing methods.\nWe release the workflow implementation for popular diffusion model Web UIs to\nsupport further research and artistic exploration."}
{"id": "2504.15743", "pdf": "https://arxiv.org/pdf/2504.15743", "abs": "https://arxiv.org/abs/2504.15743", "authors": ["Seung Gyu Jeong", "Sung Woo Nam", "Seong Kwan Jung", "Seong-Eun Kim"], "title": "iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Respiratory auscultation is crucial for early detection of pediatric\npneumonia, a condition that can quickly worsen without timely intervention. In\nareas with limited physician access, effective auscultation is challenging. We\npresent a smartphone-based system that leverages built-in microphones and\nadvanced deep learning algorithms to detect abnormal respiratory sounds\nindicative of pneumonia risk. Our end-to-end deep learning framework employs\ndomain generalization to integrate a large electronic stethoscope dataset with\na smaller smartphone-derived dataset, enabling robust feature learning for\naccurate respiratory assessments without expensive equipment. The accompanying\nmobile application guides caregivers in collecting high-quality lung sound\nsamples and provides immediate feedback on potential pneumonia risks. User\nstudies show strong classification performance and high acceptance,\ndemonstrating the system's ability to facilitate proactive interventions and\nreduce preventable childhood pneumonia deaths. By seamlessly integrating into\nubiquitous smartphones, this approach offers a promising avenue for more\nequitable and comprehensive remote pediatric care."}
{"id": "2504.15305", "pdf": "https://arxiv.org/pdf/2504.15305", "abs": "https://arxiv.org/abs/2504.15305", "authors": ["Abhishek Tyagi", "Charu Gaur"], "title": "SLAM-Based Navigation and Fault Resilience in a Surveillance Quadcopter with Embedded Vision Systems", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "68T40, 68U10, 70Q05", "I.2.9; I.4.8; I.2.10; C.3"], "comment": "18 pages, 21 figures, 4 tables. Onboard processing using Raspberry Pi\n  4 and Arduino Nano. Includes ORB-SLAM3-based navigation, LQR control, rotor\n  fault recovery, object detection, and PCA face recognition. Real-world and\n  simulation tests included. Designed for GPS-denied autonomous UAV\n  surveillance", "summary": "We present an autonomous aerial surveillance platform, Veg, designed as a\nfault-tolerant quadcopter system that integrates visual SLAM for\nGPS-independent navigation, advanced control architecture for dynamic\nstability, and embedded vision modules for real-time object and face\nrecognition. The platform features a cascaded control design with an LQR\ninner-loop and PD outer-loop trajectory control. It leverages ORB-SLAM3 for\n6-DoF localization and loop closure, and supports waypoint-based navigation\nthrough Dijkstra path planning over SLAM-derived maps. A real-time Failure\nDetection and Identification (FDI) system detects rotor faults and executes\nemergency landing through re-routing. The embedded vision system, based on a\nlightweight CNN and PCA, enables onboard object detection and face recognition\nwith high precision. The drone operates fully onboard using a Raspberry Pi 4\nand Arduino Nano, validated through simulations and real-world testing. This\nwork consolidates real-time localization, fault recovery, and embedded AI on a\nsingle platform suitable for constrained environments."}
{"id": "2504.15766", "pdf": "https://arxiv.org/pdf/2504.15766", "abs": "https://arxiv.org/abs/2504.15766", "authors": ["Tobias Demmler", "Lennart Hartung", "Andreas Tamke", "Thao Dang", "Alexander Hegai", "Karsten Haug", "Lars Mikelsons"], "title": "Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In autonomous driving, accurately predicting the movements of other traffic\nparticipants is crucial, as it significantly influences a vehicle's planning\nprocesses. Modern trajectory prediction models strive to interpret complex\npatterns and dependencies from agent and map data. The Motion Transformer (MTR)\narchitecture and subsequent work define the most accurate methods in common\nbenchmarks such as the Waymo Open Motion Benchmark. The MTR model employs\npre-generated static intention points as initial goal points for trajectory\nprediction. However, the static nature of these points frequently leads to\nmisalignment with map data in specific traffic scenarios, resulting in\nunfeasible or unrealistic goal points. Our research addresses this limitation\nby integrating scene-specific dynamic intention points into the MTR model. This\nadaptation of the MTR model was trained and evaluated on the Waymo Open Motion\nDataset. Our findings demonstrate that incorporating dynamic intention points\nhas a significant positive impact on trajectory prediction accuracy, especially\nfor predictions over long time horizons. Furthermore, we analyze the impact on\nground truth trajectories which are not compliant with the map data or are\nillegal maneuvers."}
{"id": "2504.15317", "pdf": "https://arxiv.org/pdf/2504.15317", "abs": "https://arxiv.org/abs/2504.15317", "authors": ["Meher Boulaabi", "Takwa Ben Aïcha Gader", "Afef Kacem Echi", "Zied Bouraoui"], "title": "Enhancing DR Classification with Swin Transformer and Shifted Window Attention", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide,\nunderscoring the importance of early detection for effective treatment.\nHowever, automated DR classification remains challenging due to variations in\nimage quality, class imbalance, and pixel-level similarities that hinder model\ntraining. To address these issues, we propose a robust preprocessing pipeline\nincorporating image cropping, Contrast-Limited Adaptive Histogram Equalization\n(CLAHE), and targeted data augmentation to improve model generalization and\nresilience. Our approach leverages the Swin Transformer, which utilizes\nhierarchical token processing and shifted window attention to efficiently\ncapture fine-grained features while maintaining linear computational\ncomplexity. We validate our method on the Aptos and IDRiD datasets for\nmulti-class DR classification, achieving accuracy rates of 89.65% and 97.40%,\nrespectively. These results demonstrate the effectiveness of our model,\nparticularly in detecting early-stage DR, highlighting its potential for\nimproving automated retinal screening in clinical settings."}
{"id": "2504.15773", "pdf": "https://arxiv.org/pdf/2504.15773", "abs": "https://arxiv.org/abs/2504.15773", "authors": ["Cong Liu", "Sharvaree Vadgama", "David Ruhe", "Erik Bekkers", "Patrick Forrè"], "title": "Clifford Group Equivariant Diffusion Models for 3D Molecular Generation", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 1 figure, 1 table", "summary": "This paper explores leveraging the Clifford algebra's expressive power for\n$\\E(n)$-equivariant diffusion models. We utilize the geometric products between\nClifford multivectors and the rich geometric information encoded in Clifford\nsubspaces in \\emph{Clifford Diffusion Models} (CDMs). We extend the diffusion\nprocess beyond just Clifford one-vectors to incorporate all higher-grade\nmultivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us\nto apply latent diffusion across complete multivectors. This enables CDMs to\ncapture the joint distribution across different subspaces of the algebra,\nincorporating richer geometric information through higher-order features. We\nprovide empirical results for unconditional molecular generation on the QM9\ndataset, showing that CDMs provide a promising avenue for generative modeling."}
{"id": "2504.15323", "pdf": "https://arxiv.org/pdf/2504.15323", "abs": "https://arxiv.org/abs/2504.15323", "authors": ["Donggyun Kim", "Chanwoo Kim", "Seunghoon Hong"], "title": "HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "While test-time fine-tuning is beneficial in few-shot learning, the need for\nmultiple backpropagation steps can be prohibitively expensive in real-time or\nlow-resource scenarios. To address this limitation, we propose an approach that\nemulates gradient descent without computing gradients, enabling efficient\ntest-time adaptation. Specifically, we formulate gradient descent as an Euler\ndiscretization of an ordinary differential equation (ODE) and train an\nauxiliary network to predict the task-conditional drift using only the few-shot\nsupport set. The adaptation then reduces to a simple numerical integration\n(e.g., via the Euler method), which requires only a few forward passes of the\nauxiliary network -- no gradients or forward passes of the target model are\nneeded. In experiments on cross-domain few-shot classification using the\nMeta-Dataset and CDFSL benchmarks, our method significantly improves\nout-of-domain performance over the non-fine-tuned baseline while incurring only\n6\\% of the memory cost and 0.02\\% of the computation time of standard\nfine-tuning, thus establishing a practical middle ground between direct\ntransfer and fully fine-tuned approaches."}
{"id": "2504.15779", "pdf": "https://arxiv.org/pdf/2504.15779", "abs": "https://arxiv.org/abs/2504.15779", "authors": ["Aaron J. Gutknecht", "Fernando E. Rosas", "David A. Ehrlich", "Abdullah Makkeh", "Pedro A. M. Mediano", "Michael Wibral"], "title": "Shannon invariants: A scalable approach to information decomposition", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT", "nlin.AO", "physics.data-an"], "comment": "16 pages, 4 Figures", "summary": "Distributed systems, such as biological and artificial neural networks,\nprocess information via complex interactions engaging multiple subsystems,\nresulting in high-order patterns with distinct properties across scales.\nInvestigating how these systems process information remains challenging due to\ndifficulties in defining appropriate multivariate metrics and ensuring their\nscalability to large systems. To address these challenges, we introduce a novel\nframework based on what we call \"Shannon invariants\" -- quantities that capture\nessential properties of high-order information processing in a way that depends\nonly on the definition of entropy and can be efficiently calculated for large\nsystems. Our theoretical results demonstrate how Shannon invariants can be used\nto resolve long-standing ambiguities regarding the interpretation of widely\nused multivariate information-theoretic measures. Moreover, our practical\nresults reveal distinctive information-processing signatures of various deep\nlearning architectures across layers, which lead to new insights into how these\nsystems process information and how this evolves during training. Overall, our\nframework resolves fundamental limitations in analyzing high-order phenomena\nand offers broad opportunities for theoretical developments and empirical\nanalyses."}
{"id": "2504.15329", "pdf": "https://arxiv.org/pdf/2504.15329", "abs": "https://arxiv.org/abs/2504.15329", "authors": ["Yike Zhang", "Eduardo Davalos", "Jack Noble"], "title": "Vision6D: 3D-to-2D Interactive Visualization and Annotation Tool for 6D Pose Estimation", "categories": ["cs.GR", "cs.CV", "cs.HC", "cs.RO"], "comment": null, "summary": "Accurate 6D pose estimation has gained more attention over the years for\nrobotics-assisted tasks that require precise interaction with physical objects.\nThis paper presents an interactive 3D-to-2D visualization and annotation tool\nto support the 6D pose estimation research community. To the best of our\nknowledge, the proposed work is the first tool that allows users to visualize\nand manipulate 3D objects interactively on a 2D real-world scene, along with a\ncomprehensive user study. This system supports robust 6D camera pose annotation\nby providing both visual cues and spatial relationships to determine object\nposition and orientation in various environments. The annotation feature in\nVision6D is particularly helpful in scenarios where the transformation matrix\nbetween the camera and world objects is unknown, as it enables accurate\nannotation of these objects' poses using only the camera intrinsic matrix. This\ncapability serves as a foundational step in developing and training advanced\npose estimation models across various domains. We evaluate Vision6D's\neffectiveness by utilizing widely-used open-source pose estimation datasets\nLinemod and HANDAL through comparisons between the default ground-truth camera\nposes with manual annotations. A user study was performed to show that Vision6D\ngenerates accurate pose annotations via visual cues in an intuitive 3D user\ninterface. This approach aims to bridge the gap between 2D scene projections\nand 3D scenes, offering an effective way for researchers and developers to\nsolve 6D pose annotation related problems. The software is open-source and\npublicly available at https://github.com/InteractiveGL/vision6D."}
{"id": "2504.15784", "pdf": "https://arxiv.org/pdf/2504.15784", "abs": "https://arxiv.org/abs/2504.15784", "authors": ["Ruizhe Li", "Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%)."}
{"id": "2504.15434", "pdf": "https://arxiv.org/pdf/2504.15434", "abs": "https://arxiv.org/abs/2504.15434", "authors": ["Sarath Shekkizhar", "Romain Cosentino"], "title": "AGI Is Coming... Right After AI Learns to Play Wordle", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This paper investigates multimodal agents, in particular, OpenAI's\nComputer-User Agent (CUA), trained to control and complete tasks through a\nstandard computer interface, similar to humans. We evaluated the agent's\nperformance on the New York Times Wordle game to elicit model behaviors and\nidentify shortcomings. Our findings revealed a significant discrepancy in the\nmodel's ability to recognize colors correctly depending on the context. The\nmodel had a $5.36\\%$ success rate over several hundred runs across a week of\nWordle. Despite the immense enthusiasm surrounding AI agents and their\npotential to usher in Artificial General Intelligence (AGI), our findings\nreinforce the fact that even simple tasks present substantial challenges for\ntoday's frontier AI models. We conclude with a discussion of the potential\nunderlying causes, implications for future development, and research directions\nto improve these AI systems."}
{"id": "2504.15801", "pdf": "https://arxiv.org/pdf/2504.15801", "abs": "https://arxiv.org/abs/2504.15801", "authors": ["Valeria Lerman", "Yaniv Dover"], "title": "A closer look at how large language models trust humans: patterns and biases", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI."}
{"id": "2504.15479", "pdf": "https://arxiv.org/pdf/2504.15479", "abs": "https://arxiv.org/abs/2504.15479", "authors": ["Jeremy Goldwasser", "Giles Hooker"], "title": "Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Counterfactuals are a popular framework for interpreting machine learning\npredictions. These what if explanations are notoriously challenging to create\nfor computer vision models: standard gradient-based methods are prone to\nproduce adversarial examples, in which imperceptible modifications to image\npixels provoke large changes in predictions. We introduce a new,\neasy-to-implement framework for counterfactual images that can flexibly adapt\nto contemporary advances in generative modeling. Our method, Counterfactual\nAttacks, resembles an adversarial attack on the representation of the image\nalong a low-dimensional manifold. In addition, given an auxiliary dataset of\nimage descriptors, we show how to accompany counterfactuals with feature\nattribution that quantify the changes between the original and counterfactual\nimages. These importance scores can be aggregated into global counterfactual\nexplanations that highlight the overall features driving model predictions.\nWhile this unification is possible for any counterfactual method, it has\nparticular computational efficiency for ours. We demonstrate the efficacy of\nour approach with the MNIST and CelebA datasets."}
{"id": "2504.15804", "pdf": "https://arxiv.org/pdf/2504.15804", "abs": "https://arxiv.org/abs/2504.15804", "authors": ["Ning Wang", "Bingkun Yao", "Jie Zhou", "Yuchen Hu", "Xi Wang", "Nan Guan", "Zhe Jiang"], "title": "Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B."}
{"id": "2504.15481", "pdf": "https://arxiv.org/pdf/2504.15481", "abs": "https://arxiv.org/abs/2504.15481", "authors": ["Michel Berthier", "Nicoletta Prencipe", "Edoardo Provenzi"], "title": "Split-quaternions for perceptual white balance", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We propose a perceptual chromatic adaptation transform for white balance that\nmakes use of split-quaternions. The novelty of the present work, which is\nmotivated by a recently developed quantum-like model of color perception,\nconsists at stressing the link between the algebraic structures appearing in\nthis model and a certain sub-algebra of the split-quaternions. We show the\npotentiality of this approach for color image processing applications by\nproposing a chromatic adaptation transform, implemented via an appropriate use\nof the split-quaternion multiplication. Moreover, quantitative comparisons with\nthe widely used state-of-the art von Kries chromatic adaptation transform are\nprovided."}
{"id": "2504.15806", "pdf": "https://arxiv.org/pdf/2504.15806", "abs": "https://arxiv.org/abs/2504.15806", "authors": ["Kai Luo", "Juan Tang", "Mingchao Cai", "Xiaoqing Zeng", "Manqi Xie", "Ming Yan"], "title": "DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\nMulti-Layer Perceptrons (MLPs) due to their superior function-fitting abilities\nin data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,\nfor solving high-index differential-algebraic equations (DAEs) by integrating\nKANs with Physics-Informed Neural Networks (PINNs). This framework not only\npreserves the ability of traditional PINNs to model complex systems governed by\nphysical laws but also enhances their performance by leveraging the\nfunction-fitting strengths of KANs. Numerical experiments demonstrate that for\nDAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute\nerrors of both differential and algebraic variables by 1 to 2 orders of\nmagnitude compared to traditional PINNs. To assess the effectiveness of this\napproach, we analyze the drift-off error and find that both PINNs and DAE-KAN\noutperform classical numerical methods in controlling this phenomenon. Our\nresults highlight the potential of neural network methods, particularly\nDAE-KAN, in solving high-index DAEs with substantial computational accuracy and\ngeneralization, offering a promising solution for challenging partial\ndifferential-algebraic equations."}
{"id": "2504.15496", "pdf": "https://arxiv.org/pdf/2504.15496", "abs": "https://arxiv.org/abs/2504.15496", "authors": ["Eammon A. Littler", "Emmanuel A. Mannoh", "Ethan P. M. LaRochelle"], "title": "Fluorescence Reference Target Quantitative Analysis Library", "categories": ["physics.med-ph", "cs.CV", "eess.IV", "q-bio.QM"], "comment": "12 pages, 1 table, 4 figures. Code available:\n  https://github.com/QUEL-Imaging/quel-qal), PyPi: quel-qal", "summary": "Standardized performance evaluation of fluorescence imaging systems remains a\ncritical unmet need in the field of fluorescence-guided surgery (FGS). While\nthe American Association of Physicists in Medicine (AAPM) TG311 report and\nrecent FDA draft guidance provide recommended metrics for system\ncharacterization, practical tools for extracting these metrics remain limited,\ninconsistent, and often inaccessible. We present QUEL-QAL, an open-source\nPython library designed to streamline and standardize the quantitative analysis\nof fluorescence images using solid reference targets. The library provides a\nmodular, reproducible workflow that includes region of interest (ROI)\ndetection, statistical analysis, and visualization capabilities. QUEL-QAL\nsupports key metrics such as response linearity, limit of detection, depth\nsensitivity, and spatial resolution, in alignment with regulatory and academic\nguidance. Built on widely adopted Python packages, the library is designed to\nbe extensible, enabling users to adapt it to novel target designs and analysis\nprotocols. By promoting transparency, reproducibility, and regulatory\nalignment, QUEL-QAL offers a foundational tool to support standardized\nbenchmarking and accelerate the development and evaluation of fluorescence\nimaging systems."}
{"id": "2504.15812", "pdf": "https://arxiv.org/pdf/2504.15812", "abs": "https://arxiv.org/abs/2504.15812", "authors": ["Xuchuang Wang", "Qirun Zeng", "Jinhang Zuo", "Xutong Liu", "Mohammad Hajiesmaili", "John C. S. Lui", "Adam Wierman"], "title": "Fusing Reward and Dueling Feedback in Stochastic Bandits", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper investigates the fusion of absolute (reward) and relative\n(dueling) feedback in stochastic bandits, where both feedback types are\ngathered in each decision round. We derive a regret lower bound, demonstrating\nthat an efficient algorithm may incur only the smaller among the reward and\ndueling-based regret for each individual arm. We propose two fusion approaches:\n(1) a simple elimination fusion algorithm that leverages both feedback types to\nexplore all arms and unifies collected information by sharing a common\ncandidate arm set, and (2) a decomposition fusion algorithm that selects the\nmore effective feedback to explore the corresponding arms and randomly assigns\none feedback type for exploration and the other for exploitation in each round.\nThe elimination fusion experiences a suboptimal multiplicative term of the\nnumber of arms in regret due to the intrinsic suboptimality of dueling\nelimination. In contrast, the decomposition fusion achieves regret matching the\nlower bound up to a constant under a common assumption. Extensive experiments\nconfirm the efficacy of our algorithms and theoretical results."}
{"id": "2504.15545", "pdf": "https://arxiv.org/pdf/2504.15545", "abs": "https://arxiv.org/abs/2504.15545", "authors": ["Zizhi Chen", "Xinyu Zhang", "Minghao Han", "Yizhou Liu", "Ziyun Qian", "Weifeng Zhang", "Xukun Zhang", "Jingwei Wei", "Lihua Zhang"], "title": "VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In histopathology, tissue sections are typically stained using common H&E\nstaining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific\ntissue structures. The rapid advancement of deep learning offers an effective\nsolution for generating virtually stained images, significantly reducing the\ntime and labor costs associated with traditional histochemical staining.\nHowever, a new challenge arises in separating the fundamental visual\ncharacteristics of tissue sections from the visual differences induced by\nstaining agents. Additionally, virtual staining often overlooks essential\npathological knowledge and the physical properties of staining, resulting in\nonly style-level transfer. To address these issues, we introduce, for the first\ntime in virtual staining tasks, a pathological vision-language large model\n(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,\nfoundational concept anchors for tissue sections, and staining-specific concept\nanchors to leverage the extensive knowledge of the pathological VLM. This\napproach is designed to describe, frame, and enhance the direction of virtual\nstaining. Furthermore, we have developed a data augmentation method based on\nthe constraints of the VLM. This method utilizes the VLM's powerful image\ninterpretation capabilities to further integrate image style and structural\ninformation, proving beneficial in high-precision pathological diagnostics.\nExtensive evaluations on publicly available multi-domain unpaired staining\ndatasets demonstrate that our method can generate highly realistic images and\nenhance the accuracy of downstream tasks, such as glomerular detection and\nsegmentation. Our code is available at:\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR"}
{"id": "2504.15823", "pdf": "https://arxiv.org/pdf/2504.15823", "abs": "https://arxiv.org/abs/2504.15823", "authors": ["Songyan Xie", "Jinghang Wen", "Encheng Su", "Qiucheng Yu"], "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/."}
{"id": "2504.15562", "pdf": "https://arxiv.org/pdf/2504.15562", "abs": "https://arxiv.org/abs/2504.15562", "authors": ["Dip Roy"], "title": "Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis", "categories": ["cs.LG", "cs.CV"], "comment": "16 pages, 6 figures", "summary": "In medical imaging, anomaly detection is a vital element of healthcare\ndiagnostics, especially for neurological conditions which can be\nlife-threatening. Conventional deterministic methods often fall short when it\ncomes to capturing the inherent uncertainty of anomaly detection tasks. This\npaper introduces a Bayesian Variational Autoencoder (VAE) equipped with\nmulti-head attention mechanisms for detecting anomalies in brain magnetic\nresonance imaging (MRI). For the purpose of improving anomaly detection\nperformance, we incorporate both epistemic and aleatoric uncertainty estimation\nthrough Bayesian inference. The model was tested on the BraTS2020 dataset, and\nthe findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper\nsuggests that modeling uncertainty is an essential component of anomaly\ndetection, enhancing both performance and interpretability and providing\nconfidence estimates, as well as anomaly predictions, for clinicians to\nleverage in making medical decisions."}
{"id": "2504.15827", "pdf": "https://arxiv.org/pdf/2504.15827", "abs": "https://arxiv.org/abs/2504.15827", "authors": ["Xuyang Zhong", "Haochen Luo", "Chen Liu"], "title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms."}
{"id": "2504.15594", "pdf": "https://arxiv.org/pdf/2504.15594", "abs": "https://arxiv.org/abs/2504.15594", "authors": ["Tatsuhito Hasegawa", "Shunsuke Sakai"], "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification", "categories": ["cs.LG", "cs.CV"], "comment": "22 pages, 11 figures, under review", "summary": "In deep learning-based classification tasks, the softmax function's\ntemperature parameter $T$ critically influences the output distribution and\noverall performance. This study presents a novel theoretical insight that the\noptimal temperature $T^*$ is uniquely determined by the dimensionality of the\nfeature representations, thereby enabling training-free determination of $T^*$.\nDespite this theoretical grounding, empirical evidence reveals that $T^*$\nfluctuates under practical conditions owing to variations in models, datasets,\nand other confounding factors. To address these influences, we propose and\noptimize a set of temperature determination coefficients that specify how $T^*$\nshould be adjusted based on the theoretical relationship to feature\ndimensionality. Additionally, we insert a batch normalization layer immediately\nbefore the output layer, effectively stabilizing the feature space. Building on\nthese coefficients and a suite of large-scale experiments, we develop an\nempirical formula to estimate $T^*$ without additional training while also\nintroducing a corrective scheme to refine $T^*$ based on the number of classes\nand task complexity. Our findings confirm that the derived temperature not only\naligns with the proposed theoretical perspective but also generalizes\neffectively across diverse tasks, consistently enhancing classification\nperformance and offering a practical, training-free solution for determining\n$T^*$."}
{"id": "2504.15865", "pdf": "https://arxiv.org/pdf/2504.15865", "abs": "https://arxiv.org/abs/2504.15865", "authors": ["Lotfi Abdelkrim Mecharbat", "Ibrahim Elmakky", "Martin Takac", "Mohammed Yaqub"], "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS."}
{"id": "2504.15616", "pdf": "https://arxiv.org/pdf/2504.15616", "abs": "https://arxiv.org/abs/2504.15616", "authors": ["Kai Chen", "Xiaodong Zhao", "Yujie Huang", "Guoyu Fang", "Xiao Song", "Ruiping Wang", "Ziyuan Wang"], "title": "SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages,6 figures", "summary": "The analysis and prediction of agent trajectories are crucial for\ndecision-making processes in intelligent systems, with precise short-term\ntrajectory forecasting being highly significant across a range of applications.\nAgents and their social interactions have been quantified and modeled by\nresearchers from various perspectives; however, substantial limitations exist\nin the current work due to the inherent high uncertainty of agent intentions\nand the complex higher-order influences among neighboring groups. SocialMOIF is\nproposed to tackle these challenges, concentrating on the higher-order\nintention interactions among neighboring groups while reinforcing the primary\nrole of first-order intention interactions between neighbors and the target\nagent. This method develops a multi-order intention fusion model to achieve a\nmore comprehensive understanding of both direct and indirect intention\ninformation. Within SocialMOIF, a trajectory distribution approximator is\ndesigned to guide the trajectories toward values that align more closely with\nthe actual data, thereby enhancing model interpretability. Furthermore, a\nglobal trajectory optimizer is introduced to enable more accurate and efficient\nparallel predictions. By incorporating a novel loss function that accounts for\ndistance and direction during training, experimental results demonstrate that\nthe model outperforms previous state-of-the-art baselines across multiple\nmetrics in both dynamic and static datasets."}
{"id": "2504.15876", "pdf": "https://arxiv.org/pdf/2504.15876", "abs": "https://arxiv.org/abs/2504.15876", "authors": ["Qizhen Wu Lei Chen", "Kexin Liu", "Jinhu Lü"], "title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In swarm robotics, confrontation scenarios, including strategic\nconfrontations, require efficient decision-making that integrates discrete\ncommands and continuous actions. Traditional task and motion planning methods\nseparate decision-making into two layers, but their unidirectional structure\nfails to capture the interdependence between these layers, limiting\nadaptability in dynamic environments. Here, we propose a novel bidirectional\napproach based on hierarchical reinforcement learning, enabling dynamic\ninteraction between the layers. This method effectively maps commands to task\nallocation and actions to path planning, while leveraging cross-training\ntechniques to enhance learning across the hierarchical framework. Furthermore,\nwe introduce a trajectory prediction model that bridges abstract task\nrepresentations with actionable planning goals. In our experiments, it achieves\nover 80\\% in confrontation win rate and under 0.01 seconds in decision time,\noutperforming existing approaches. Demonstrations through large-scale tests and\nreal-world robot experiments further emphasize the generalization capabilities\nand practical applicability of our method."}
{"id": "2504.15649", "pdf": "https://arxiv.org/pdf/2504.15649", "abs": "https://arxiv.org/abs/2504.15649", "authors": ["Biao Wu", "Diankai Zhang", "Shaoli Liu", "Si Gao", "Chengjian Zheng", "Ning Wang"], "title": "RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "Champion Solution for CVPR 2025 MAI VSR Track", "summary": "As a fundamental challenge in visual computing, video super-resolution (VSR)\nfocuses on reconstructing highdefinition video sequences from their degraded\nlowresolution counterparts. While deep convolutional neural networks have\ndemonstrated state-of-the-art performance in spatial-temporal super-resolution\ntasks, their computationally intensive nature poses significant deployment\nchallenges for resource-constrained edge devices, particularly in real-time\nmobile video processing scenarios where power efficiency and latency\nconstraints coexist. In this work, we propose a Reparameterizable Architecture\nfor High Fidelity Video Super Resolution method, named RepNet-VSR, for\nreal-time 4x video super-resolution. On the REDS validation set, the proposed\nmodel achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per\n10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an\nexcellent balance between restoration quality and deployment efficiency. The\nproposed method scores higher than the previous champion algorithm of MAI video\nsuper-resolution challenge."}
{"id": "2504.15883", "pdf": "https://arxiv.org/pdf/2504.15883", "abs": "https://arxiv.org/abs/2504.15883", "authors": ["Farida Mohsen", "Samir Belhaouari", "Zubair Shah"], "title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis."}
{"id": "2504.15654", "pdf": "https://arxiv.org/pdf/2504.15654", "abs": "https://arxiv.org/abs/2504.15654", "authors": ["Md Abdul Baset Sarker", "Art Nguyen", "Sigmond Kukla", "Kevin Fite", "Masudul H. Imtiaz"], "title": "A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces a novel AI vision-enabled pediatric prosthetic hand\ndesigned to assist children aged 10-12 with upper limb disabilities. The\nprosthesis features an anthropomorphic appearance, multi-articulating\nfunctionality, and a lightweight design that mimics a natural hand, making it\nboth accessible and affordable for low-income families. Using 3D printing\ntechnology and integrating advanced machine vision, sensing, and embedded\ncomputing, the prosthetic hand offers a low-cost, customizable solution that\naddresses the limitations of current myoelectric prostheses. A micro camera is\ninterfaced with a low-power FPGA for real-time object detection and assists\nwith precise grasping. The onboard DL-based object detection and grasp\nclassification models achieved accuracies of 96% and 100% respectively. In the\nforce prediction, the mean absolute error was found to be 0.018. The features\nof the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted\nmicro camera for artificial sensing, enabling a wide range of hand-based tasks;\nb) real-time object detection and distance estimation for precise grasping; and\nc) ultra-low-power operation that delivers high performance within constrained\npower and resource limits."}
{"id": "2504.15894", "pdf": "https://arxiv.org/pdf/2504.15894", "abs": "https://arxiv.org/abs/2504.15894", "authors": ["Chengbo Zheng", "Tim Miller", "Alina Bialkowski", "H Peter Soyer", "Monika Janda"], "title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making", "categories": ["cs.HC", "cs.AI"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "High stakes decision-making often requires a continuous interplay between\nevolving evidence and shifting hypotheses, a dynamic that is not well supported\nby current AI decision support systems. In this paper, we introduce a\nmixed-initiative framework for AI assisted decision making that is grounded in\nthe data-frame theory of sensemaking and the evaluative AI paradigm. Our\napproach enables both humans and AI to collaboratively construct, validate, and\nadapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer\ndiagnosis prototype that leverages a concept bottleneck model to facilitate\ninterpretable interactions and dynamic updates to diagnostic hypotheses."}
{"id": "2504.15664", "pdf": "https://arxiv.org/pdf/2504.15664", "abs": "https://arxiv.org/abs/2504.15664", "authors": ["Phuong Quynh Le", "Jörg Schlötterer", "Christin Seifert"], "title": "An XAI-based Analysis of Shortcut Learning in Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at The World Conference on eXplainable Artificial\n  Intelligence 2025 (XAI-2025)", "summary": "Machine learning models tend to learn spurious features - features that\nstrongly correlate with target labels but are not causal. Existing approaches\nto mitigate models' dependence on spurious features work in some cases, but\nfail in others. In this paper, we systematically analyze how and where neural\nnetworks encode spurious correlations. We introduce the neuron spurious score,\nan XAI-based diagnostic measure to quantify a neuron's dependence on spurious\nfeatures. We analyze both convolutional neural networks (CNNs) and vision\ntransformers (ViTs) using architecture-specific methods. Our results show that\nspurious features are partially disentangled, but the degree of disentanglement\nvaries across model architectures. Furthermore, we find that the assumptions\nbehind existing mitigation methods are incomplete. Our results lay the\ngroundwork for the development of novel methods to mitigate spurious\ncorrelations and make AI models safer to use in practice."}
{"id": "2504.15895", "pdf": "https://arxiv.org/pdf/2504.15895", "abs": "https://arxiv.org/abs/2504.15895", "authors": ["Chenxu Yang", "Qingyi Si", "Yongjie Duan", "Zheliang Zhu", "Chenyu Zhu", "Zheng Lin", "Li Cao", "Weiping Wang"], "title": "Dynamic Early Exit in Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 11 figures", "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%."}
{"id": "2504.15667", "pdf": "https://arxiv.org/pdf/2504.15667", "abs": "https://arxiv.org/abs/2504.15667", "authors": ["Jingchen Zou", "Jianqiang Li", "Gabriel Jimenez", "Qing Zhao", "Daniel Racoceanu", "Matias Cosarinsky", "Enzo Ferrante", "Guanghui Fu"], "title": "Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The performance of medical image segmentation models is usually evaluated\nusing metrics like the Dice score and Hausdorff distance, which compare\npredicted masks to ground truth annotations. However, when applying the model\nto unseen data, such as in clinical settings, it is often impractical to\nannotate all the data, making the model's performance uncertain. To address\nthis challenge, we propose the Segmentation Performance Evaluator (SPE), a\nframework for estimating segmentation models' performance on unlabeled data.\nThis framework is adaptable to various evaluation metrics and model\narchitectures. Experiments on six publicly available datasets across six\nevaluation metrics including pixel-based metrics such as Dice score and\ndistance-based metrics like HD95, demonstrated the versatility and\neffectiveness of our approach, achieving a high correlation (0.956$\\pm$0.046)\nand low MAE (0.025$\\pm$0.019) compare with real Dice score on the independent\ntest set. These results highlight its ability to reliably estimate model\nperformance without requiring annotations. The SPE framework integrates\nseamlessly into any model training process without adding training overhead,\nenabling performance estimation and facilitating the real-world application of\nmedical image segmentation algorithms. The source code is publicly available"}
{"id": "2504.15905", "pdf": "https://arxiv.org/pdf/2504.15905", "abs": "https://arxiv.org/abs/2504.15905", "authors": ["Wenjing Xiao", "Chenglong Shi", "Miaojiang Chen", "Zhiquan Liu", "Min Chen", "H. Herbert Song"], "title": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages,12 figures", "summary": "With the exponential growth of Internet of Things (IoT) devices, edge\ncomputing (EC) is gradually playing an important role in providing\ncost-effective services. However, existing approaches struggle to perform well\nin graph-structured scenarios where user data is correlated, such as traffic\nflow prediction and social relationship recommender systems. In particular,\ngraph neural network (GNN)-based approaches lead to expensive server\ncommunication cost. To address this problem, we propose GraphEdge, an efficient\nGNN-based EC architecture. It considers the EC system of GNN tasks, where there\nare associations between users and it needs to take into account the task data\nof its neighbors when processing the tasks of a user. Specifically, the\narchitecture first perceives the user topology and represents their data\nassociations as a graph layout at each time step. Then the graph layout is\noptimized by calling our proposed hierarchical traversal graph cut algorithm\n(HiCut), which cuts the graph layout into multiple weakly associated subgraphs\nbased on the aggregation characteristics of GNN, and the communication cost\nbetween different subgraphs during GNN inference is minimized. Finally, based\non the optimized graph layout, our proposed deep reinforcement learning (DRL)\nbased graph offloading algorithm (DRLGO) is executed to obtain the optimal\noffloading strategy for the tasks of users, the offloading strategy is\nsubgraph-based, it tries to offload user tasks in a subgraph to the same edge\nserver as possible while minimizing the task processing time and energy\nconsumption of the EC system. Experimental results show the good effectiveness\nand dynamic adaptation of our proposed architecture and it also performs well\neven in dynamic scenarios."}
{"id": "2504.15899", "pdf": "https://arxiv.org/pdf/2504.15899", "abs": "https://arxiv.org/abs/2504.15899", "authors": ["Blerim Abdullai", "Tony Wang", "Xinyuan Qiao", "Florian Shkurti", "Timothy D. Barfoot"], "title": "RaSCL: Radar to Satellite Crossview Localization", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "GNSS is unreliable, inaccurate, and insufficient in many real-time autonomous\nfield applications. In this work, we present a GNSS-free global localization\nsolution that contains a method of registering imaging radar on the ground with\noverhead RGB imagery, with joint optimization of relative poses from odometry\nand global poses from our overhead registration. Previous works have used\nvarious combinations of ground sensors and overhead imagery, and different\nfeature extraction and matching methods. These include various handcrafted and\ndeep-learning-based methods for extracting features from overhead imagery. Our\nwork presents insights on extracting essential features from RGB overhead\nimages for effective global localization against overhead imagery using only\nground radar and a single georeferenced initial guess. We motivate our method\nby evaluating it on datasets in diverse geographic conditions and robotic\nplatforms, including on an Unmanned Surface Vessel (USV) as well as urban and\nsuburban driving datasets."}
{"id": "2504.15912", "pdf": "https://arxiv.org/pdf/2504.15912", "abs": "https://arxiv.org/abs/2504.15912", "authors": ["Riley Pierson", "Armin Moin"], "title": "Automated Bug Report Prioritization in Large Open-Source Projects", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large open-source projects receive a large number of issues (known as bugs),\nincluding software defect (i.e., bug) reports and new feature requests from\ntheir user and developer communities at a fast rate. The often limited project\nresources do not allow them to deal with all issues. Instead, they have to\nprioritize them according to the project's priorities and the issues'\nseverities. In this paper, we propose a novel approach to automated bug\nprioritization based on the natural language text of the bug reports that are\nstored in the open bug repositories of the issue-tracking systems. We conduct\ntopic modeling using a variant of LDA called TopicMiner-MTM and text\nclassification with the BERT large language model to achieve a higher\nperformance level compared to the state-of-the-art. Experimental results using\nan existing reference dataset containing 85,156 bug reports of the Eclipse\nPlatform project indicate that we outperform existing approaches in terms of\nAccuracy, Precision, Recall, and F1-measure of the bug report priority\nprediction."}
{"id": "2504.15953", "pdf": "https://arxiv.org/pdf/2504.15953", "abs": "https://arxiv.org/abs/2504.15953", "authors": ["Chance J. Hamilton", "Alfredo Weitzenfeld"], "title": "Visual Place Cell Encoding: A Computational Model for Spatial Representation and Cognitive Mapping", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper presents the Visual Place Cell Encoding (VPCE) model, a\nbiologically inspired computational framework for simulating place cell-like\nactivation using visual input. Drawing on evidence that visual landmarks play a\ncentral role in spatial encoding, the proposed VPCE model activates visual\nplace cells by clustering high-dimensional appearance features extracted from\nimages captured by a robot-mounted camera. Each cluster center defines a\nreceptive field, and activation is computed based on visual similarity using a\nradial basis function. We evaluate whether the resulting activation patterns\ncorrelate with key properties of biological place cells, including spatial\nproximity, orientation alignment, and boundary differentiation. Experiments\ndemonstrate that the VPCE can distinguish between visually similar yet\nspatially distinct locations and adapt to environment changes such as the\ninsertion or removal of walls. These results suggest that structured visual\ninput, even in the absence of motion cues or reward-driven learning, is\nsufficient to generate place-cell-like spatial representations and support\nbiologically inspired cognitive mapping."}
{"id": "2504.15918", "pdf": "https://arxiv.org/pdf/2504.15918", "abs": "https://arxiv.org/abs/2504.15918", "authors": ["Chang Zong", "Bin Li", "Shoujun Zhou", "Jian Wan", "Lei Zhang"], "title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions", "categories": ["cs.CV", "cs.AI", "cs.HC", "68T45, 68T20"], "comment": "16 pages, 8 figures", "summary": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc."}
{"id": "2504.15970", "pdf": "https://arxiv.org/pdf/2504.15970", "abs": "https://arxiv.org/abs/2504.15970", "authors": ["Baichuan Zeng"], "title": "Recent Advances and Future Directions in Extended Reality (XR): Exploring AI-Powered Spatial Intelligence", "categories": ["cs.HC", "cs.CV", "cs.MA"], "comment": "7 pages,4 figures", "summary": "Extended Reality (XR), encompassing Augmented Reality (AR), Virtual Reality\n(VR) and Mixed Reality (MR), is a transformative technology bridging the\nphysical and virtual world and it has diverse potential which will be\nubiquitous in the future. This review examines XR's evolution through\nfoundational framework - hardware ranging from monitors to sensors and software\nranging from visual tasks to user interface; highlights state of the art (SOTA)\nXR products with the comparison and analysis of performance based on their\nfoundational framework; discusses how commercial XR devices can support the\ndemand of high-quality performance focusing on spatial intelligence. For future\ndirections, attention should be given to the integration of multi-modal AI and\nIoT-driven digital twins to enable adaptive XR systems. With the concept of\nspatial intelligence, future XR should establish a new digital space with\nrealistic experience that benefits humanity. This review underscores the\npivotal role of AI in unlocking XR as the next frontier in human-computer\ninteraction."}
{"id": "2504.15924", "pdf": "https://arxiv.org/pdf/2504.15924", "abs": "https://arxiv.org/abs/2504.15924", "authors": ["Alycia Carey", "Xintao Wu"], "title": "Achieving Distributive Justice in Federated Learning via Uncertainty Quantification", "categories": ["cs.LG", "cs.AI", "stat.ML", "68T01", "I.2.0"], "comment": "21 pages, 1 figure, 7 tables", "summary": "Client-level fairness metrics for federated learning are used to ensure that\nall clients in a federation either: a) have similar final performance on their\nlocal data distributions (i.e., client parity), or b) obtain final performance\non their local data distributions relative to their contribution to the\nfederated learning process (i.e., contribution fairness). While a handful of\nworks that propose either client-parity or contribution-based fairness metrics\nground their definitions and decisions in social theories of equality -- such\nas distributive justice -- most works arbitrarily choose what notion of\nfairness to align with which makes it difficult for practitioners to choose\nwhich fairness metric aligns best with their fairness ethics. In this work, we\npropose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),\na flexible federated learning framework that can achieve multiple distributive\njustice-based client-level fairness metrics. Namely, by utilizing techniques\ninspired by fair resource allocation, in conjunction with performing aleatoric\nuncertainty-based client weighing, our UDJ-FL framework is able to achieve\negalitarian, utilitarian, Rawls' difference principle, or desert-based\nclient-level fairness. We empirically show the ability of UDJ-FL to achieve all\nfour defined distributive justice-based client-level fairness metrics in\naddition to providing fairness equivalent to (or surpassing) other popular fair\nfederated learning works. Further, we provide justification for why aleatoric\nuncertainty weighing is necessary to the construction of our UDJ-FL framework\nas well as derive theoretical guarantees for the generalization bounds of\nUDJ-FL. Our code is publicly available at\nhttps://github.com/alycia-noel/UDJ-FL."}
{"id": "2504.15975", "pdf": "https://arxiv.org/pdf/2504.15975", "abs": "https://arxiv.org/abs/2504.15975", "authors": ["Peter Fletcher"], "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition", "categories": ["cs.FL", "cs.CV", "F.4.2; F.4.3"], "comment": "64 pages, 23 figures", "summary": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches."}
{"id": "2504.15927", "pdf": "https://arxiv.org/pdf/2504.15927", "abs": "https://arxiv.org/abs/2504.15927", "authors": ["Ling Cheng", "Jiashu Pu", "Ruicheng Liang", "Qian Shao", "Hezhe Qiao", "Feida Zhu"], "title": "New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics", "categories": ["cs.SI", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2203.05898 by other authors", "summary": "Semi-supervised community detection methods are widely used for identifying\nspecific communities due to the label scarcity. Existing semi-supervised\ncommunity detection methods typically involve two learning stages learning in\nboth initial identification and subsequent adjustment, which often starts from\nan unreasonable community core candidate. Moreover, these methods encounter\nscalability issues because they depend on reinforcement learning and generative\nadversarial networks, leading to higher computational costs and restricting the\nselection of candidates. To address these limitations, we draw a parallel\nbetween crystallization kinetics and community detection to integrate the\nspontaneity of the annealing process into community detection. Specifically, we\nliken community detection to identifying a crystal subgrain (core) that expands\ninto a complete grain (community) through a process similar to annealing. Based\non this finding, we propose CLique ANNealing (CLANN), which applies kinetics\nconcepts to community detection by integrating these principles into the\noptimization process to strengthen the consistency of the community core.\nSubsequently, a learning-free Transitive Annealer was employed to refine the\nfirst-stage candidates by merging neighboring cliques and repositioning the\ncommunity core, enabling a spontaneous growth process that enhances\nscalability. Extensive experiments on \\textbf{43} different network settings\ndemonstrate that CLANN outperforms state-of-the-art methods across multiple\nreal-world datasets, showcasing its exceptional efficacy and efficiency in\ncommunity detection."}
{"id": "2504.16062", "pdf": "https://arxiv.org/pdf/2504.16062", "abs": "https://arxiv.org/abs/2504.16062", "authors": ["Hardik Shah", "Jiaxu Xing", "Nico Messikommer", "Boyang Sun", "Marc Pollefeys", "Davide Scaramuzza"], "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration."}
{"id": "2504.15928", "pdf": "https://arxiv.org/pdf/2504.15928", "abs": "https://arxiv.org/abs/2504.15928", "authors": ["Meng Wang", "Tian Lin", "Qingshan Hou", "Aidi Lin", "Jingcheng Wang", "Qingsheng Peng", "Truong X. Nguyen", "Danqi Fang", "Ke Zou", "Ting Xu", "Cancan Xue", "Ten Cheer Quek", "Qinkai Yu", "Minxin Liu", "Hui Zhou", "Zixuan Xiao", "Guiqin He", "Huiyu Liang", "Tingkun Shi", "Man Chen", "Linna Liu", "Yuanyuan Peng", "Lianyu Wang", "Qiuming Hu", "Junhong Chen", "Zhenhua Zhang", "Cheng Chen", "Yitian Zhao", "Dianbo Liu", "Jianhua Wu", "Xinjian Chen", "Changqing Zhang", "Triet Thanh Nguyen", "Yanda Meng", "Yalin Zheng", "Yih Chung Tham", "Carol Y. Cheung", "Huazhu Fu", "Haoyu Chen", "Ching-Yu Cheng"], "title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers."}
{"id": "2504.15929", "pdf": "https://arxiv.org/pdf/2504.15929", "abs": "https://arxiv.org/abs/2504.15929", "authors": ["Saban Ozturk", "Melih B. Yilmaz", "Muti Kara", "M. Talat Yavuz", "Aykut Koç", "Tolga Çukur"], "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods."}
{"id": "2504.15941", "pdf": "https://arxiv.org/pdf/2504.15941", "abs": "https://arxiv.org/abs/2504.15941", "authors": ["Fanny Jourdan", "Yannick Chevalier", "Cécile Favre"], "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity", "categories": ["cs.CL", "cs.AI"], "comment": "FAccT 2025", "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub."}
{"id": "2504.15956", "pdf": "https://arxiv.org/pdf/2504.15956", "abs": "https://arxiv.org/abs/2504.15956", "authors": ["Jerry Yao-Chieh Hu", "Hude Liu", "Hong-Yu Chen", "Weimin Wu", "Han Liu"], "title": "Universal Approximation with Softmax Attention", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We prove that with linear transformations, both (i) two-layer self-attention\nand (ii) one-layer self-attention followed by a softmax function are universal\napproximators for continuous sequence-to-sequence functions on compact domains.\nOur main technique is a new interpolation-based method for analyzing\nattention's internal mechanism. This leads to our key insight: self-attention\nis able to approximate a generalized version of ReLU to arbitrary precision,\nand hence subsumes many known universal approximators. Building on these, we\nshow that two-layer multi-head attention alone suffices as a\nsequence-to-sequence universal approximator. In contrast, prior works rely on\nfeed-forward networks to establish universal approximation in Transformers.\nFurthermore, we extend our techniques to show that, (softmax-)attention-only\nlayers are capable of approximating various statistical models in-context. We\nbelieve these techniques hold independent interest."}
{"id": "2504.15972", "pdf": "https://arxiv.org/pdf/2504.15972", "abs": "https://arxiv.org/abs/2504.15972", "authors": ["Sophie C. Pope", "Andrew Barovic", "Armin Moin"], "title": "Bug Destiny Prediction in Large Open-Source Software Repositories through Sentiment Analysis and BERT Topic Modeling", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This study explores a novel approach to predicting key bug-related outcomes,\nincluding the time to resolution, time to fix, and ultimate status of a bug,\nusing data from the Bugzilla Eclipse Project. Specifically, we leverage\nfeatures available before a bug is resolved to enhance predictive accuracy. Our\nmethodology incorporates sentiment analysis to derive both an emotionality\nscore and a sentiment classification (positive or negative). Additionally, we\nintegrate the bug's priority level and its topic, extracted using a BERTopic\nmodel, as features for a Convolutional Neural Network (CNN) and a Multilayer\nPerceptron (MLP). Our findings indicate that the combination of BERTopic and\nsentiment analysis can improve certain model performance metrics. Furthermore,\nwe observe that balancing model inputs enhances practical applicability, albeit\nat the cost of a significant reduction in accuracy in most cases. To address\nour primary objectives, predicting time-to-resolution, time-to-fix, and bug\ndestiny, we employ both binary classification and exact time value predictions,\nallowing for a comparative evaluation of their predictive effectiveness.\nResults demonstrate that sentiment analysis serves as a valuable predictor of a\nbug's eventual outcome, particularly in determining whether it will be fixed.\nHowever, its utility is less pronounced when classifying bugs into more complex\nor unconventional outcome categories."}
{"id": "2504.15983", "pdf": "https://arxiv.org/pdf/2504.15983", "abs": "https://arxiv.org/abs/2504.15983", "authors": ["Shang Wang"], "title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation."}
{"id": "2504.15995", "pdf": "https://arxiv.org/pdf/2504.15995", "abs": "https://arxiv.org/abs/2504.15995", "authors": ["Sindhuja Madabushi", "Ahmad Faraz Khan", "Haider Ali", "Jin-Hee Cho"], "title": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL."}
{"id": "2504.16000", "pdf": "https://arxiv.org/pdf/2504.16000", "abs": "https://arxiv.org/abs/2504.16000", "authors": ["Soham Bonnerjee", "Zhen Wei", "Yeon", "Anna Asch", "Sagnik Nandy", "Promit Ghosal"], "title": "How Private is Your Attention? Bridging Privacy with In-Context Learning", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings."}
{"id": "2504.16005", "pdf": "https://arxiv.org/pdf/2504.16005", "abs": "https://arxiv.org/abs/2504.16005", "authors": ["Tom Zehle", "Moritz Schlager", "Timo Heiß", "Matthias Feurer"], "title": "CAPO: Cost-Aware Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "comment": "Submitted to AutoML 2025", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."}
{"id": "2504.16020", "pdf": "https://arxiv.org/pdf/2504.16020", "abs": "https://arxiv.org/abs/2504.16020", "authors": ["Soham Sane"], "title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "comment": null, "summary": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\naddressing the memory overhead and hyperparameter complexity of adaptive\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\ngradient normalization followed by a smooth hyperbolic tangent transformation,\n$g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness\nparameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\nformulation; (2) a formal non-convex convergence analysis guaranteeing\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\nperformance profile. While exhibiting instability in off-policy DQN, it\nprovides enhanced training stability with competitive results in TD3 (requiring\ncareful $\\alpha$ tuning) and achieves substantially superior performance in\non-policy PPO. These results underscore the critical importance of empirical\n$\\alpha$ selection, revealing strong interactions between the optimizer's\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\nalternative optimizer for memory-constrained scenarios and shows significant\npromise for on-policy learning regimes where its stability and efficiency\nadvantages can be particularly impactful."}
{"id": "2504.16021", "pdf": "https://arxiv.org/pdf/2504.16021", "abs": "https://arxiv.org/abs/2504.16021", "authors": ["Dinithi Dissanayake", "Suranga Nanayakkara"], "title": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support", "categories": ["cs.HC", "cs.AI"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "Flow theory describes an optimal cognitive state where individuals experience\ndeep focus and intrinsic motivation when a task's difficulty aligns with their\nskill level. In AI-augmented reasoning, interventions that disrupt the state of\ncognitive flow can hinder rather than enhance decision-making. This paper\nproposes a context-aware cognitive augmentation framework that adapts\ninterventions based on three key contextual factors: type, timing, and scale.\nBy leveraging multimodal behavioral cues (e.g., gaze behavior, typing\nhesitation, interaction speed), AI can dynamically adjust cognitive support to\nmaintain or restore flow. We introduce the concept of cognitive flow, an\nextension of flow theory in AI-augmented reasoning, where interventions are\npersonalized, adaptive, and minimally intrusive. By shifting from static\ninterventions to context-aware augmentation, our approach ensures that AI\nsystems support deep engagement in complex decision-making and reasoning\nwithout disrupting cognitive immersion."}
{"id": "2504.16026", "pdf": "https://arxiv.org/pdf/2504.16026", "abs": "https://arxiv.org/abs/2504.16026", "authors": ["Konstantin F. Pilz", "James Sanders", "Robi Rahman", "Lennart Heim"], "title": "Trends in AI Supercomputers", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Frontier AI development relies on powerful AI supercomputers, yet analysis of\nthese systems is limited. We create a dataset of 500 AI supercomputers from\n2019 to 2025 and analyze key trends in performance, power needs, hardware cost,\nownership, and global distribution. We find that the computational performance\nof AI supercomputers has doubled every nine months, while hardware acquisition\ncost and power needs both doubled every year. The leading system in March 2025,\nxAI's Colossus, used 200,000 AI chips, had a hardware cost of \\$7B, and\nrequired 300 MW of power, as much as 250,000 households. As AI supercomputers\nevolved from tools for science to industrial machines, companies rapidly\nexpanded their share of total AI supercomputer performance, while the share of\ngovernments and academia diminished. Globally, the United States accounts for\nabout 75% of total performance in our dataset, with China in second place at\n15%. If the observed trends continue, the leading AI supercomputer in 2030 will\nachieve $2\\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a\nhardware cost of \\$200 billion, and require 9 GW of power. Our analysis\nprovides visibility into the AI supercomputer landscape, allowing policymakers\nto assess key AI trends like resource needs, ownership, and national\ncompetitiveness."}
{"id": "2504.16027", "pdf": "https://arxiv.org/pdf/2504.16027", "abs": "https://arxiv.org/abs/2504.16027", "authors": ["Ahmed R. Sadik", "Siddhata Govind"], "title": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection"}
{"id": "2504.16032", "pdf": "https://arxiv.org/pdf/2504.16032", "abs": "https://arxiv.org/abs/2504.16032", "authors": ["Yazan Otoum", "Arghavan Asad", "Amiya Nayak"], "title": "LLMs meet Federated Learning for Scalable and Secure IoT Management", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": "This work has been submitted to the IEEE Global Communications\n  Conference (GLOBECOM) 2025 for possible publication", "summary": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions."}
{"id": "2504.16041", "pdf": "https://arxiv.org/pdf/2504.16041", "abs": "https://arxiv.org/abs/2504.16041", "authors": ["Amund Tveit", "Bjørn Remseth", "Arve Skogvold"], "title": "Muon Optimizer Accelerates Grokking", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": "8 pages, 4 figures", "summary": "This paper investigates the impact of different optimizers on the grokking\nphenomenon, where models exhibit delayed generalization. We conducted\nexperiments across seven numerical tasks (primarily modular arithmetic) using a\nmodern Transformer architecture. The experimental configuration systematically\nvaried the optimizer (Muon vs. AdamW) and the softmax activation function\n(standard softmax, stablemax, and sparsemax) to assess their combined effect on\nlearning dynamics. Our empirical evaluation reveals that the Muon optimizer,\ncharacterized by its use of spectral norm constraints and second-order\ninformation, significantly accelerates the onset of grokking compared to the\nwidely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch\nfrom 153.09 to 102.89 across all configurations, a statistically significant\ndifference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice\nplays a crucial role in facilitating the transition from memorization to\ngeneralization."}
{"id": "2504.16047", "pdf": "https://arxiv.org/pdf/2504.16047", "abs": "https://arxiv.org/abs/2504.16047", "authors": ["Frank Li", "Hari Trivedi", "Bardia Khosravi", "Theo Dapamede", "Mohammadreza Chavoshi", "Abdulhameed Dere", "Rohan Satya Isaac", "Aawez Mansuri", "Janice Newsome", "Saptarshi Purkayastha", "Judy Gichoya"], "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology."}
{"id": "2504.16053", "pdf": "https://arxiv.org/pdf/2504.16053", "abs": "https://arxiv.org/abs/2504.16053", "authors": ["Zhifan Ye", "Kejing Xia", "Yonggan Fu", "Xin Dong", "Jihoon Hong", "Xiangchi Yuan", "Shizhe Diao", "Jan Kautz", "Pavlo Molchanov", "Yingyan Celine Lin"], "title": "LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICLR 2025", "summary": "State space models (SSMs) have emerged as an efficient alternative to\nTransformer models for language modeling, offering linear computational\ncomplexity and constant memory usage as context length increases. However,\ndespite their efficiency in handling long contexts, recent studies have shown\nthat SSMs, such as Mamba models, generally underperform compared to\nTransformers in long-context understanding tasks. To address this significant\nshortfall and achieve both efficient and accurate long-context understanding,\nwe propose LongMamba, a training-free technique that significantly enhances the\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\nthat the hidden channels in Mamba can be categorized into local and global\nchannels based on their receptive field lengths, with global channels primarily\nresponsible for long-context capability. These global channels can become the\nkey bottleneck as the input context lengthens. Specifically, when input lengths\nlargely exceed the training sequence length, global channels exhibit\nlimitations in adaptively extend their receptive fields, leading to Mamba's\npoor long-context performance. The key idea of LongMamba is to mitigate the\nhidden state memory decay in these global channels by preventing the\naccumulation of unimportant tokens in their memory. This is achieved by first\nidentifying critical tokens in the global channels and then applying token\nfiltering to accumulate only those critical tokens. Through extensive\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\nsets a new standard for Mamba's long-context performance, significantly\nextending its operational range without requiring additional training. Our code\nis available at https://github.com/GATECH-EIC/LongMamba."}
{"id": "2504.16061", "pdf": "https://arxiv.org/pdf/2504.16061", "abs": "https://arxiv.org/abs/2504.16061", "authors": ["Sangeet Khemlani", "Tyler Tran", "Nathaniel Gyory", "Anthony M. Harrison", "Wallace E. Lawson", "Ravenna Thielstrom", "Hunter Thompson", "Taaren Singh", "J. Gregory Trafton"], "title": "Vision language models are unreliable at trivial spatial cognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing."}
{"id": "2504.16072", "pdf": "https://arxiv.org/pdf/2504.16072", "abs": "https://arxiv.org/abs/2504.16072", "authors": ["Long Lian", "Yifan Ding", "Yunhao Ge", "Sifei Liu", "Hanzi Mao", "Boyi Li", "Marco Pavone", "Ming-Yu Liu", "Trevor Darrell", "Adam Yala", "Yin Cui"], "title": "Describe Anything: Detailed Localized Image and Video Captioning", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://describe-anything.github.io/", "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning."}
{"id": "2504.16078", "pdf": "https://arxiv.org/pdf/2504.16078", "abs": "https://arxiv.org/abs/2504.16078", "authors": ["Thomas Schmied", "Jörg Bornschein", "Jordi Grau-Moya", "Markus Wulfmeier", "Razvan Pascanu"], "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making."}
