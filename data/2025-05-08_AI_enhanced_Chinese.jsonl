{"id": "2505.03821", "pdf": "https://arxiv.org/pdf/2505.03821", "abs": "https://arxiv.org/abs/2505.03821", "authors": ["Gracjan G\u00f3ral", "Alicja Ziarko", "Piotr Mi\u0142o\u015b", "Micha\u0142 Nauman", "Maciej Wo\u0142czyk", "Micha\u0142 Kosi\u0144ski"], "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Dataset:\n  https://huggingface.co/datasets/Gracjan/Isle/viewer/Isle-Brick-V2", "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u89c6\u89c9\u89c6\u89d2\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u573a\u666f\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u89c6\u89d2\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u63a2\u7d22VLMs\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u89c6\u89d2\u4efb\u52a1\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u6a21\u578b\u5728\u7a7a\u95f4\u548c\u89c6\u89d2\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1144\u4e2a\u72ec\u7279\u7684\u89c6\u89c9\u4efb\u52a1\uff0c\u7ed3\u54087\u4e2a\u8bca\u65ad\u95ee\u9898\uff0c\u8bc4\u4f30VLMs\u5728\u573a\u666f\u7406\u89e3\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u89c6\u89c9\u89c6\u89d2\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u573a\u666f\u7406\u89e3\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u89c6\u89d2\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u672a\u6765VLM\u5f00\u53d1\u9700\u6574\u5408\u663e\u5f0f\u51e0\u4f55\u8868\u793a\u548c\u9488\u5bf9\u6027\u8bad\u7ec3\u534f\u8bae\uff0c\u4ee5\u63d0\u5347\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2505.03826", "pdf": "https://arxiv.org/pdf/2505.03826", "abs": "https://arxiv.org/abs/2505.03826", "authors": ["Minji Kang", "Seongho Kim", "Eunseo Go", "Donghyeon Paek", "Geon Lim", "Muyoung Kim", "Soyeun Kim", "Sung Kyu Jang", "Min Sup Choi", "Woo Seok Kang", "Jaehyun Kim", "Jaekwang Kim", "Hyeong-U Kim"], "title": "In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "Precise monitoring of etch depth and the thickness of insulating materials,\nsuch as Silicon dioxide and silicon nitride, is critical to ensuring device\nperformance and yield in semiconductor manufacturing. While conventional\nex-situ analysis methods are accurate, they are constrained by time delays and\ncontamination risks. To address these limitations, this study proposes a\nnon-contact, in-situ etch depth prediction framework based on machine learning\n(ML) techniques. Two scenarios are explored. In the first scenario, an\nartificial neural network (ANN) is trained to predict average etch depth from\nprocess parameters, achieving a significantly lower mean squared error (MSE)\ncompared to a linear baseline model. The approach is then extended to\nincorporate variability from repeated measurements using a Bayesian Neural\nNetwork (BNN) to capture both aleatoric and epistemic uncertainty. Coverage\nanalysis confirms the BNN's capability to provide reliable uncertainty\nestimates. In the second scenario, we demonstrate the feasibility of using RGB\ndata from digital image colorimetry (DIC) as input for etch depth prediction,\nachieving strong performance even in the absence of explicit process\nparameters. These results suggest that the integration of DIC and ML offers a\nviable, cost-effective alternative for real-time, in-situ, and non-invasive\nmonitoring in plasma etching processes, contributing to enhanced process\nstability, and manufacturing efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u975e\u63a5\u89e6\u5f0f\u539f\u4f4d\u8680\u523b\u6df1\u5ea6\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u7edd\u7f18\u6750\u6599\u539a\u5ea6\u7684\u5b9e\u65f6\u76d1\u6d4b\uff0c\u901a\u8fc7\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u9884\u6d4b\u8680\u523b\u6df1\u5ea6\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u5b57\u56fe\u50cf\u6bd4\u8272\u6cd5\u6570\u636e\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5916\u90e8\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u65f6\u95f4\u5ef6\u8fdf\u548c\u6c61\u67d3\u98ce\u9669\uff0c\u65e0\u6cd5\u6ee1\u8db3\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u5bf9\u8680\u523b\u6df1\u5ea6\u548c\u7edd\u7f18\u6750\u6599\u539a\u5ea6\u5b9e\u65f6\u76d1\u6d4b\u7684\u9700\u6c42\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff08BNN\uff09\u5206\u522b\u9884\u6d4b\u8680\u523b\u6df1\u5ea6\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u6570\u5b57\u56fe\u50cf\u6bd4\u8272\u6cd5\uff08DIC\uff09\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u7684\u53ef\u884c\u6027\u3002", "result": "ANN\u5728\u9884\u6d4b\u5e73\u5747\u8680\u523b\u6df1\u5ea6\u65f6\u663e\u8457\u4f18\u4e8e\u7ebf\u6027\u57fa\u7ebf\u6a21\u578b\uff0cBNN\u80fd\u53ef\u9760\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff1bDIC\u6570\u636e\u5728\u65e0\u660e\u786e\u5de5\u827a\u53c2\u6570\u65f6\u4ecd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7ed3\u5408DIC\u548c\u673a\u5668\u5b66\u4e60\u4e3a\u7b49\u79bb\u5b50\u8680\u523b\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u65f6\u3001\u539f\u4f4d\u3001\u975e\u4fb5\u5165\u7684\u76d1\u6d4b\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u5de5\u827a\u7a33\u5b9a\u6027\u548c\u5236\u9020\u6548\u7387\u3002"}}
{"id": "2505.03829", "pdf": "https://arxiv.org/pdf/2505.03829", "abs": "https://arxiv.org/abs/2505.03829", "authors": ["Yogesh Kumar"], "title": "VideoLLM Benchmarks and Evaluation: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 2 Tables", "summary": "The rapid development of Large Language Models (LLMs) has catalyzed\nsignificant advancements in video understanding technologies. This survey\nprovides a comprehensive analysis of benchmarks and evaluation methodologies\nspecifically designed or used for Video Large Language Models (VideoLLMs). We\nexamine the current landscape of video understanding benchmarks, discussing\ntheir characteristics, evaluation protocols, and limitations. The paper\nanalyzes various evaluation methodologies, including closed-set, open-set, and\nspecialized evaluations for temporal and spatiotemporal understanding tasks. We\nhighlight the performance trends of state-of-the-art VideoLLMs across these\nbenchmarks and identify key challenges in current evaluation frameworks.\nAdditionally, we propose future research directions to enhance benchmark\ndesign, evaluation metrics, and protocols, including the need for more diverse,\nmultimodal, and interpretability-focused benchmarks. This survey aims to equip\nresearchers with a structured understanding of how to effectively evaluate\nVideoLLMs and identify promising avenues for advancing the field of video\nunderstanding with large language models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VideoLLMs\uff09\u7684\u8bc4\u6d4b\u57fa\u51c6\u4e0e\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u7684\u7279\u70b9\u3001\u8bc4\u4f30\u534f\u8bae\u53ca\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u89c6\u9891\u7406\u89e3\u6280\u672f\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5bf9VideoLLMs\u8bc4\u6d4b\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\uff08\u5982\u5c01\u95ed\u96c6\u3001\u5f00\u653e\u96c6\u53ca\u65f6\u7a7a\u4efb\u52a1\u8bc4\u4f30\uff09\uff0c\u603b\u7ed3\u8bc4\u4f30\u65b9\u6cd5\u7684\u7279\u70b9\u4e0e\u4e0d\u8db3\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u8bc4\u6d4b\u6846\u67b6\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u524d\u6cbfVideoLLMs\u7684\u6027\u80fd\u8d8b\u52bf\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u9700\u8bbe\u8ba1\u66f4\u591a\u6837\u5316\u3001\u591a\u6a21\u6001\u53ca\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u7ed3\u6784\u5316\u6307\u5bfc\u3002"}}
{"id": "2505.03832", "pdf": "https://arxiv.org/pdf/2505.03832", "abs": "https://arxiv.org/abs/2505.03832", "authors": ["Noor B. Tayfor", "Tarik A. Rashid", "Shko M. Qader", "Bryar A. Hassan", "Mohammed H. Abdalla", "Jafar Majidpour", "Aram M. Ahmed", "Hussein M. Ali", "Aso M. Aladdin", "Abdulhady A. Abdullah", "Ahmed S. Shamsaldin", "Haval M. Sidqi", "Abdulrahman Salih", "Zaher M. Yaseen", "Azad A. Ameen", "Janmenjoy Nayak", "Mahmood Yashar Hamza"], "title": "Video Forgery Detection for Surveillance Cameras: A Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The widespread availability of video recording through smartphones and\ndigital devices has made video-based evidence more accessible than ever.\nSurveillance footage plays a crucial role in security, law enforcement, and\njudicial processes. However, with the rise of advanced video editing tools,\ntampering with digital recordings has become increasingly easy, raising\nconcerns about their authenticity. Ensuring the integrity of surveillance\nvideos is essential, as manipulated footage can lead to misinformation and\nundermine judicial decisions. This paper provides a comprehensive review of\nexisting forensic techniques used to detect video forgery, focusing on their\neffectiveness in verifying the authenticity of surveillance recordings. Various\nmethods, including compression-based analysis, frame duplication detection, and\nmachine learning-based approaches, are explored. The findings highlight the\ngrowing necessity for more robust forensic techniques to counteract evolving\nforgery methods. Strengthening video forensic capabilities will ensure that\nsurveillance recordings remain credible and admissible as legal evidence.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7528\u4e8e\u68c0\u6d4b\u89c6\u9891\u4f2a\u9020\u7684\u73b0\u6709\u6cd5\u533b\u6280\u672f\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u5176\u5728\u9a8c\u8bc1\u76d1\u63a7\u5f55\u50cf\u771f\u5b9e\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5bf9\u66f4\u5f3a\u5927\u6280\u672f\u7684\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u7f16\u8f91\u5de5\u5177\u7684\u666e\u53ca\uff0c\u76d1\u63a7\u5f55\u50cf\u7684\u7be1\u6539\u53d8\u5f97\u5bb9\u6613\uff0c\u5a01\u80c1\u5176\u771f\u5b9e\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u4fe1\u606f\u548c\u53f8\u6cd5\u51b3\u7b56\u7684\u7834\u574f\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u538b\u7f29\u7684\u5206\u6790\u3001\u5e27\u91cd\u590d\u68c0\u6d4b\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u6280\u672f\u9700\u8981\u8fdb\u4e00\u6b65\u5f3a\u5316\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u4f2a\u9020\u624b\u6bb5\u3002", "conclusion": "\u52a0\u5f3a\u89c6\u9891\u6cd5\u533b\u80fd\u529b\u5c06\u786e\u4fdd\u76d1\u63a7\u5f55\u50cf\u7684\u53ef\u4fe1\u5ea6\u548c\u6cd5\u5f8b\u8bc1\u636e\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.03833", "pdf": "https://arxiv.org/pdf/2505.03833", "abs": "https://arxiv.org/abs/2505.03833", "authors": ["Xuechao Wang", "Sven Nomm", "Junqing Huang", "Kadri Medijainen", "Aaro Toomela", "Michael Ruzhansky"], "title": "PointExplainer: Towards Transparent Parkinson's Disease Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep neural networks have shown potential in analyzing digitized hand-drawn\nsignals for early diagnosis of Parkinson's disease. However, the lack of clear\ninterpretability in existing diagnostic methods presents a challenge to\nclinical trust. In this paper, we propose PointExplainer, an explainable\ndiagnostic strategy to identify hand-drawn regions that drive model diagnosis.\nSpecifically, PointExplainer assigns discrete attribution values to hand-drawn\nsegments, explicitly quantifying their relative contributions to the model's\ndecision. Its key components include: (i) a diagnosis module, which encodes\nhand-drawn signals into 3D point clouds to represent hand-drawn trajectories,\nand (ii) an explanation module, which trains an interpretable surrogate model\nto approximate the local behavior of the black-box diagnostic model. We also\nintroduce consistency measures to further address the issue of faithfulness in\nexplanations. Extensive experiments on two benchmark datasets and a newly\nconstructed dataset show that PointExplainer can provide intuitive explanations\nwith no diagnostic performance degradation. The source code is available at\nhttps://github.com/chaoxuewang/PointExplainer.", "AI": {"tldr": "PointExplainer\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u7b56\u7565\uff0c\u7528\u4e8e\u8bc6\u522b\u624b\u7ed8\u533a\u57df\u5bf9\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u8bca\u65ad\u7684\u8d21\u732e\uff0c\u901a\u8fc7\u79bb\u6563\u5f52\u56e0\u503c\u548c\u4e00\u81f4\u6027\u5ea6\u91cf\u63d0\u4f9b\u76f4\u89c2\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u5e15\u91d1\u68ee\u75c5\u8bca\u65ad\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5f71\u54cd\u4e34\u5e8a\u4fe1\u4efb\u3002", "method": "\u63d0\u51faPointExplainer\uff0c\u5305\u62ec\u8bca\u65ad\u6a21\u5757\uff08\u5c06\u624b\u7ed8\u4fe1\u53f7\u7f16\u7801\u4e3a3D\u70b9\u4e91\uff09\u548c\u89e3\u91ca\u6a21\u5757\uff08\u8bad\u7ec3\u53ef\u89e3\u91ca\u4ee3\u7406\u6a21\u578b\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPointExplainer\u63d0\u4f9b\u76f4\u89c2\u89e3\u91ca\u4e14\u4e0d\u5f71\u54cd\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "PointExplainer\u89e3\u51b3\u4e86\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2505.03837", "pdf": "https://arxiv.org/pdf/2505.03837", "abs": "https://arxiv.org/abs/2505.03837", "authors": ["Rashik Shadman", "Daqing Hou", "Faraz Hussain", "M G Sarwar Murshed"], "title": "Explainable Face Recognition via Improved Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Biometric authentication has become one of the most widely used tools in the\ncurrent technological era to authenticate users and to distinguish between\ngenuine users and imposters. Face is the most common form of biometric modality\nthat has proven effective. Deep learning-based face recognition systems are now\ncommonly used across different domains. However, these systems usually operate\nlike black-box models that do not provide necessary explanations or\njustifications for their decisions. This is a major disadvantage because users\ncannot trust such artificial intelligence-based biometric systems and may not\nfeel comfortable using them when clear explanations or justifications are not\nprovided. This paper addresses this problem by applying an efficient method for\nexplainable face recognition systems. We use a Class Activation Mapping\n(CAM)-based discriminative localization (very narrow/specific localization)\ntechnique called Scaled Directed Divergence (SDD) to visually explain the\nresults of deep learning-based face recognition systems. We perform fine\nlocalization of the face features relevant to the deep learning model for its\nprediction/decision. Our experiments show that the SDD Class Activation Map\n(CAM) highlights the relevant face features very specifically compared to the\ntraditional CAM and very accurately. The provided visual explanations with\nnarrow localization of relevant features can ensure much-needed transparency\nand trust for deep learning-based face recognition systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eScaled Directed Divergence (SDD)\u7684\u53ef\u89e3\u91ca\u6027\u4eba\u8138\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u7ec6\u5b9a\u4f4d\u76f8\u5173\u9762\u90e8\u7279\u5f81\uff0c\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u7528\u6237\u96be\u4ee5\u4fe1\u4efb\u5176\u51b3\u7b56\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u63d0\u4f9b\u6e05\u6670\u7684\u89c6\u89c9\u89e3\u91ca\u3002", "method": "\u4f7f\u7528SDD\u7c7b\u6fc0\u6d3b\u6620\u5c04\uff08CAM\uff09\u6280\u672f\uff0c\u7cbe\u7ec6\u5b9a\u4f4d\u4e0e\u6a21\u578b\u51b3\u7b56\u76f8\u5173\u7684\u9762\u90e8\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDD CAM\u6bd4\u4f20\u7edfCAM\u66f4\u7cbe\u786e\u5730\u7a81\u51fa\u76f8\u5173\u9762\u90e8\u7279\u5f81\u3002", "conclusion": "SDD CAM\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u7528\u6237\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2505.03846", "pdf": "https://arxiv.org/pdf/2505.03846", "abs": "https://arxiv.org/abs/2505.03846", "authors": ["Kangsheng Wang", "Yuhang Li", "Chengwei Ye", "Yufei Lin", "Huanzhen Zhang", "Bohan Hu", "Linuo Xu", "Shuyan Liu"], "title": "GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Apparent personality analysis from short videos poses significant chal-lenges\ndue to the complex interplay of visual, auditory, and textual cues. In this\npaper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to\nrobustly model and fuse multi-source features for automatic personality\nprediction. For the visual stream, we construct a facial graph and introduce a\ndual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks\n(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to\ncapture both structural and appearance-based facial cues. Complementing this,\nglobal context and iden-tity features are extracted using pretrained ResNet18\nand VGGFace back-bones. To capture temporal dynamics, frame-level features are\nprocessed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio\nrepresentations are derived from the VGGish network, and linguistic se-mantics\nare captured via the XLM-Roberta transformer. To achieve effective multimodal\nintegration, we propose a Channel Attention-based Fusion module, followed by a\nMulti-Layer Perceptron (MLP) regression head for predicting personality traits.\nExtensive experiments show that GAME con-sistently outperforms existing methods\nacross multiple benchmarks, vali-dating its effectiveness and generalizability.", "AI": {"tldr": "GAME\u662f\u4e00\u79cd\u56fe\u589e\u5f3a\u591a\u6a21\u6001\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u4ece\u77ed\u89c6\u9891\u4e2d\u9884\u6d4b\u4eba\u683c\u7279\u5f81\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u7279\u5f81\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u77ed\u89c6\u9891\u4e2d\u7684\u4eba\u683c\u5206\u6790\u56e0\u591a\u6e90\u7279\u5f81\u7684\u590d\u6742\u4ea4\u4e92\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u3002", "method": "GAME\u7ed3\u5408\u4e86\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCNs\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u7684\u53cc\u5206\u652fGeo Two-Stream Network\uff0c\u4ee5\u53caBiGRU\u3001VGGish\u548cXLM-Roberta\uff0c\u901a\u8fc7\u901a\u9053\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u6574\u5408\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "GAME\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GAME\u4e3a\u77ed\u89c6\u9891\u4eba\u683c\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03848", "pdf": "https://arxiv.org/pdf/2505.03848", "abs": "https://arxiv.org/abs/2505.03848", "authors": ["Janhavi Giri", "Attila Lengyel", "Don Kent", "Edward Kibardin"], "title": "Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "46 pages, 22 figures, 5 tables", "summary": "Semiconductor manufacturing generates vast amounts of image data, crucial for\ndefect identification and yield optimization, yet often exceeds manual\ninspection capabilities. Traditional clustering techniques struggle with\nhigh-dimensional, unlabeled data, limiting their effectiveness in capturing\nnuanced patterns. This paper introduces an advanced clustering framework that\nintegrates deep Topological Data Analysis (TDA) with self-supervised and\ntransfer learning techniques, offering a novel approach to unsupervised image\nclustering. TDA captures intrinsic topological features, while self-supervised\nlearning extracts meaningful representations from unlabeled data, reducing\nreliance on labeled datasets. Transfer learning enhances the framework's\nadaptability and scalability, allowing fine-tuning to new datasets without\nretraining from scratch. Validated on synthetic and open-source semiconductor\nimage datasets, the framework successfully identifies clusters aligned with\ndefect patterns and process variations. This study highlights the\ntransformative potential of combining TDA, self-supervised learning, and\ntransfer learning, providing a scalable solution for proactive process\nmonitoring and quality control in semiconductor manufacturing and other domains\nwith large-scale image datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u805a\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u7684\u65e0\u76d1\u7763\u56fe\u50cf\u805a\u7c7b\uff0c\u6709\u6548\u8bc6\u522b\u7f3a\u9677\u6a21\u5f0f\u3002", "motivation": "\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u56fe\u50cf\u6570\u636e\u91cf\u5927\u4e14\u590d\u6742\uff0c\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u6574\u5408TDA\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\uff0cTDA\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u751f\u6210\u6570\u636e\u8868\u793a\uff0c\u8fc1\u79fb\u5b66\u4e60\u589e\u5f3a\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u5f00\u6e90\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6210\u529f\u8bc6\u522b\u7f3a\u9677\u6a21\u5f0f\u548c\u5de5\u827a\u53d8\u5316\u76f8\u5173\u7684\u805a\u7c7b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u534a\u5bfc\u4f53\u5236\u9020\u7b49\u9886\u57df\u7684\u56fe\u50cf\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.03856", "pdf": "https://arxiv.org/pdf/2505.03856", "abs": "https://arxiv.org/abs/2505.03856", "authors": ["Tin Mi\u0161i\u0107", "Karlo Koledi\u0107", "Fabio Bonsignorio", "Ivan Petrovi\u0107", "Ivan Markovi\u0107"], "title": "An Active Inference Model of Covert and Overt Visual Attention", "categories": ["cs.CV", "cs.AI", "I.2.6; I.2.10"], "comment": "7 pages, 7 figures. Code available at\n  https://github.com/unizgfer-lamor/ainf-visual-attention", "summary": "The ability to selectively attend to relevant stimuli while filtering out\ndistractions is essential for agents that process complex, high-dimensional\nsensory input. This paper introduces a model of covert and overt visual\nattention through the framework of active inference, utilizing dynamic\noptimization of sensory precisions to minimize free-energy. The model\ndetermines visual sensory precisions based on both current environmental\nbeliefs and sensory input, influencing attentional allocation in both covert\nand overt modalities. To test the effectiveness of the model, we analyze its\nbehavior in the Posner cueing task and a simple target focus task using\ntwo-dimensional(2D) visual data. Reaction times are measured to investigate the\ninterplay between exogenous and endogenous attention, as well as valid and\ninvalid cueing. The results show that exogenous and valid cues generally lead\nto faster reaction times compared to endogenous and invalid cues. Furthermore,\nthe model exhibits behavior similar to inhibition of return, where previously\nattended locations become suppressed after a specific cue-target onset\nasynchrony interval. Lastly, we investigate different aspects of overt\nattention and show that involuntary, reflexive saccades occur faster than\nintentional ones, but at the expense of adaptability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u611f\u5b98\u7cbe\u5ea6\u6765\u6700\u5c0f\u5316\u81ea\u7531\u80fd\uff0c\u7814\u7a76\u4e86\u5916\u6e90\u6027\u548c\u5185\u6e90\u6027\u6ce8\u610f\u529b\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u5728Posner\u63d0\u793a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u590d\u6742\u611f\u5b98\u8f93\u5165\u4e2d\u9009\u62e9\u6027\u6ce8\u610f\u76f8\u5173\u523a\u6fc0\u5e76\u8fc7\u6ee4\u5e72\u6270\uff0c\u4e3a\u667a\u80fd\u4f53\u5904\u7406\u9ad8\u7ef4\u611f\u5b98\u6570\u636e\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u5229\u7528\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff0c\u52a8\u6001\u4f18\u5316\u611f\u5b98\u7cbe\u5ea6\uff0c\u7ed3\u5408\u73af\u5883\u4fe1\u5ff5\u548c\u611f\u5b98\u8f93\u5165\u5206\u914d\u6ce8\u610f\u529b\uff0c\u5e76\u5728Posner\u63d0\u793a\u4efb\u52a1\u548c\u7b80\u5355\u76ee\u6807\u805a\u7126\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u6a21\u578b\u3002", "result": "\u5916\u6e90\u6027\u548c\u6709\u6548\u63d0\u793a\u901a\u5e38\u5bfc\u81f4\u66f4\u5feb\u7684\u53cd\u5e94\u65f6\u95f4\uff1b\u6a21\u578b\u8868\u73b0\u51fa\u7c7b\u4f3c\u6291\u5236\u8fd4\u56de\u7684\u884c\u4e3a\uff1b\u53cd\u5c04\u6027\u773c\u52a8\u6bd4\u610f\u56fe\u6027\u773c\u52a8\u66f4\u5feb\u4f46\u9002\u5e94\u6027\u8f83\u5dee\u3002", "conclusion": "\u8be5\u6a21\u578b\u6210\u529f\u6a21\u62df\u4e86\u89c6\u89c9\u6ce8\u610f\u529b\u7684\u5173\u952e\u7279\u5f81\uff0c\u4e3a\u7406\u89e3\u6ce8\u610f\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.03896", "pdf": "https://arxiv.org/pdf/2505.03896", "abs": "https://arxiv.org/abs/2505.03896", "authors": ["Shuang Zeng", "Chee Hong Lee", "Micky C Nnamdi", "Wenqi Shi", "J Ben Tamo", "Lei Zhu", "Hangzhou He", "Xinliang Zhang", "Qian Chen", "May D. Wang", "Yanye Lu", "Qiushi Ren"], "title": "Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retinal vessel segmentation is a vital early detection method for several\nsevere ocular diseases. Despite significant progress in retinal vessel\nsegmentation with the advancement of Neural Networks, there are still\nchallenges to overcome. Specifically, retinal vessel segmentation aims to\npredict the class label for every pixel within a fundus image, with a primary\nfocus on intra-image discrimination, making it vital for models to extract more\ndiscriminative features. Nevertheless, existing methods primarily focus on\nminimizing the difference between the output from the decoder and the label,\nbut ignore fully using feature-level fine-grained representations from the\nencoder. To address these issues, we propose a novel Attention U-shaped\nKolmogorov-Arnold Network named AttUKAN along with a novel Label-guided\nPixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we\nimplement Attention Gates into Kolmogorov-Arnold Networks to enhance model\nsensitivity by suppressing irrelevant feature activations and model\ninterpretability by non-linear modeling of KAN blocks. Additionally, we also\ndesign a novel Label-guided Pixel-wise Contrastive Loss to supervise our\nproposed AttUKAN to extract more discriminative features by distinguishing\nbetween foreground vessel-pixel pairs and background pairs. Experiments are\nconducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF\nand our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,\n80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and\n66.94% in the above datasets, which are the highest compared to 11 networks for\nretinal vessel segmentation. Quantitative and qualitative results show that our\nAttUKAN achieves state-of-the-art performance and outperforms existing retinal\nvessel segmentation methods. Our code will be available at\nhttps://github.com/stevezs315/AttUKAN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAttUKAN\u7684\u65b0\u578b\u6ce8\u610f\u529bU\u5f62Kolmogorov-Arnold\u7f51\u7edc\u53ca\u6807\u7b7e\u5f15\u5bfc\u7684\u50cf\u7d20\u7ea7\u5bf9\u6bd4\u635f\u5931\uff0c\u7528\u4e8e\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89e3\u7801\u5668\u8f93\u51fa\u4e0e\u6807\u7b7e\u7684\u5dee\u5f02\uff0c\u800c\u5ffd\u7565\u4e86\u7f16\u7801\u5668\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u8868\u793a\uff0c\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u3002", "method": "\u5728Kolmogorov-Arnold\u7f51\u7edc\u4e2d\u5f15\u5165\u6ce8\u610f\u529b\u95e8\u63a7\u589e\u5f3a\u6a21\u578b\u654f\u611f\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u8bbe\u8ba1\u6807\u7b7e\u5f15\u5bfc\u7684\u50cf\u7d20\u7ea7\u5bf9\u6bd4\u635f\u5931\u4ee5\u63d0\u53d6\u66f4\u5177\u533a\u5206\u6027\u7684\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u9ad8F1\u548cMIoU\u5206\u6570\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u670911\u79cd\u7f51\u7edc\u3002", "conclusion": "AttUKAN\u5728\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.03974", "pdf": "https://arxiv.org/pdf/2505.03974", "abs": "https://arxiv.org/abs/2505.03974", "authors": ["Nikhil M. Pawar", "Jorge A. Prozzi", "Feng Hong", "Surya Sarat Chandra Congress"], "title": "Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces", "categories": ["cs.CV", "cs.AI"], "comment": "Presented :Transportation Research Board 104th Annual Meeting,\n  Washington, D.C", "summary": "Recently, there has been an impetus for the application of cutting-edge data\ncollection platforms such as drones mounted with camera sensors for\ninfrastructure asset management. However, the sensor characteristics, proximity\nto the structure, hard-to-reach access, and environmental conditions often\nlimit the resolution of the datasets. A few studies used super-resolution\ntechniques to address the problem of low-resolution images. Nevertheless, these\ntechniques were observed to increase computational cost and false alarms of\ndistress detection due to the consideration of all the infrastructure images\ni.e., positive and negative distress classes. In order to address the\npre-processing of false alarm and achieve efficient super-resolution, this\nstudy developed a framework consisting of convolutional neural network (CNN)\nand efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately\nclassified both the classes. ESPCNN, which is the lightweight super-resolution\ntechnique, generated high-resolution infrastructure image of positive distress\nobtained from CNN. The ESPCNN outperformed bicubic interpolation in all the\nevaluation metrics for super-resolution. Based on the performance metrics, the\ncombination of CNN and ESPCNN was observed to be effective in preprocessing the\ninfrastructure images with negative distress, reducing the computational cost\nand false alarms in the next step of super-resolution. The visual inspection\nshowed that EPSCNN is able to capture crack propagation, complex geometry of\neven minor cracks. The proposed framework is expected to help the highway\nagencies in accurately performing distress detection and assist in efficient\nasset management practices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548cESPCNN\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u8d85\u5206\u8fa8\u7387\u5904\u7406\u57fa\u7840\u8bbe\u65bd\u56fe\u50cf\uff0c\u51cf\u5c11\u8bef\u62a5\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u4e2d\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u8bef\u62a5\u95ee\u9898\uff0c\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528CNN\u5206\u7c7b\u6b63\u8d1f\u635f\u4f24\u56fe\u50cf\uff0c\u518d\u7528\u8f7b\u91cf\u7ea7ESPCNN\u5bf9\u6b63\u635f\u4f24\u56fe\u50cf\u8fdb\u884c\u8d85\u5206\u8fa8\u7387\u5904\u7406\u3002", "result": "ESPCNN\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u4f18\u4e8e\u53cc\u4e09\u6b21\u63d2\u503c\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u8bef\u62a5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8f85\u52a9\u9ad8\u901f\u516c\u8def\u673a\u6784\u8fdb\u884c\u635f\u4f24\u68c0\u6d4b\u548c\u8d44\u4ea7\u7ba1\u7406\u3002"}}
{"id": "2505.03991", "pdf": "https://arxiv.org/pdf/2505.03991", "abs": "https://arxiv.org/abs/2505.03991", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Well", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "title": "Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges", "categories": ["cs.CV"], "comment": "13 pages, 4 figures, 2 tables", "summary": "Video event detection has become an essential component of sports analytics,\nenabling automated identification of key moments and enhancing performance\nanalysis, viewer engagement, and broadcast efficiency. Recent advancements in\ndeep learning, particularly Convolutional Neural Networks (CNNs) and\nTransformers, have significantly improved accuracy and efficiency in Temporal\nAction Localization (TAL), Action Spotting (AS), and Precise Event Spotting\n(PES). This survey provides a comprehensive overview of these three key tasks,\nemphasizing their differences, applications, and the evolution of\nmethodological approaches. We thoroughly review and categorize existing\ndatasets and evaluation metrics specifically tailored for sports contexts,\nhighlighting the strengths and limitations of each. Furthermore, we analyze\nstate-of-the-art techniques, including multi-modal approaches that integrate\naudio and visual information, methods utilizing self-supervised learning and\nknowledge distillation, and approaches aimed at generalizing across multiple\nsports. Finally, we discuss critical open challenges and outline promising\nresearch directions toward developing more generalized, efficient, and robust\nevent detection frameworks applicable to diverse sports. This survey serves as\na foundation for future research on efficient, generalizable, and multi-modal\nsports event detection.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4f53\u80b2\u89c6\u9891\u4e8b\u4ef6\u68c0\u6d4b\u7684\u4e09\u5927\u4efb\u52a1\uff08TAL\u3001AS\u3001PES\uff09\uff0c\u603b\u7ed3\u4e86\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u53ca\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u6280\u672f\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f53\u80b2\u89c6\u9891\u4e8b\u4ef6\u68c0\u6d4b\u5bf9\u63d0\u5347\u5206\u6790\u6548\u7387\u3001\u89c2\u4f17\u53c2\u4e0e\u548c\u8f6c\u64ad\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u8fdb\u6b65\u63a8\u52a8\u4e86\u5176\u53d1\u5c55\u3002", "method": "\u7efc\u8ff0\u4e86\u57fa\u4e8eCNN\u548cTransformer\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u591a\u6a21\u6001\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5206\u6790\u4e86\u6280\u672f\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u4e3a\u9ad8\u6548\u3001\u901a\u7528\u548c\u591a\u6a21\u6001\u7684\u4f53\u80b2\u4e8b\u4ef6\u68c0\u6d4b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.04006", "pdf": "https://arxiv.org/pdf/2505.04006", "abs": "https://arxiv.org/abs/2505.04006", "authors": ["Inamullah", "Imran Razzak", "Shoaib Jameel"], "title": "The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics", "categories": ["cs.CV"], "comment": null, "summary": "The unique vascularized anatomy of the human eye, encased in the retina,\nprovides an opportunity to act as a window for human health. The retinal\nstructure assists in assessing the early detection, monitoring of disease\nprogression and intervention for both ocular and non-ocular diseases. The\nadvancement in imaging technology leveraging Artificial Intelligence has seized\nthis opportunity to bridge the gap between the eye and human health. This track\npaves the way for unveiling systemic health insight from the ocular system and\nsurrogating non-invasive markers for timely intervention and identification.\nThe new frontiers of oculomics in ophthalmology cover both ocular and systemic\ndiseases, and getting more attention to explore them. In this survey paper, we\nexplore the evolution of retinal imaging techniques, the dire need for the\nintegration of AI-driven analysis, and the shift of retinal imaging from\nclassical techniques to oculomics. We also discuss some hurdles that may be\nfaced in the progression of oculomics, highlighting the research gaps and\nfuture directions.", "AI": {"tldr": "\u89c6\u7f51\u819c\u6210\u50cf\u6280\u672f\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u5206\u6790\uff0c\u4e3a\u773c\u90e8\u53ca\u5168\u8eab\u75be\u75c5\u63d0\u4f9b\u975e\u4fb5\u5165\u6027\u65e9\u671f\u68c0\u6d4b\u548c\u5e72\u9884\u624b\u6bb5\uff0c\u63a8\u52a8\u773c\u79d1\u5b66\u65b0\u9886\u57df\uff08oculomics\uff09\u7684\u53d1\u5c55\u3002", "motivation": "\u5229\u7528\u89c6\u7f51\u819c\u8840\u7ba1\u7ed3\u6784\u7684\u72ec\u7279\u6027\uff0c\u901a\u8fc7AI\u6280\u672f\u5b9e\u73b0\u75be\u75c5\u65e9\u671f\u68c0\u6d4b\u548c\u76d1\u6d4b\uff0c\u5f25\u8865\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u7efc\u8ff0\u89c6\u7f51\u819c\u6210\u50cf\u6280\u672f\u7684\u6f14\u53d8\u53caAI\u9a71\u52a8\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4ece\u4f20\u7edf\u6280\u672f\u5411oculomics\u7684\u8f6c\u53d8\u3002", "result": "\u63ed\u793a\u4e86\u89c6\u7f51\u819c\u6210\u50cf\u5728\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u6307\u51faAI\u6574\u5408\u7684\u673a\u9047\u4e0e\u6311\u6218\u3002", "conclusion": "oculomics\u4e3a\u75be\u75c5\u7814\u7a76\u5f00\u8f9f\u65b0\u9014\u5f84\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6280\u672f\u969c\u788d\u548c\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2505.04055", "pdf": "https://arxiv.org/pdf/2505.04055", "abs": "https://arxiv.org/abs/2505.04055", "authors": ["Ervin Wang", "Yuhao Chen"], "title": "FoodTrack: Estimating Handheld Food Portions with Egocentric Video", "categories": ["cs.CV"], "comment": "Accepted as extended abstract at CVPR 2025 Metafood workshop", "summary": "Accurately tracking food consumption is crucial for nutrition and health\nmonitoring. Traditional approaches typically require specific camera angles,\nnon-occluded images, or rely on gesture recognition to estimate intake, making\nassumptions about bite size rather than directly measuring food volume. We\npropose the FoodTrack framework for tracking and measuring the volume of\nhand-held food items using egocentric video which is robust to hand occlusions\nand flexible with varying camera and object poses. FoodTrack estimates food\nvolume directly, without relying on intake gestures or fixed assumptions about\nbite size, offering a more accurate and adaptable solution for tracking food\nconsumption. We achieve absolute percentage loss of approximately 7.01% on a\nhandheld food object, improving upon a previous approach that achieved a 16.40%\nmean absolute percentage error in its best case, under less flexible\nconditions.", "AI": {"tldr": "FoodTrack\u6846\u67b6\u901a\u8fc7\u7b2c\u4e00\u89c6\u89d2\u89c6\u9891\u76f4\u63a5\u6d4b\u91cf\u624b\u6301\u98df\u7269\u7684\u4f53\u79ef\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u98df\u7269\u6444\u5165\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u98df\u7269\u6444\u5165\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u6444\u50cf\u5934\u89d2\u5ea6\u3001\u65e0\u906e\u6321\u56fe\u50cf\u6216\u624b\u52bf\u8bc6\u522b\uff0c\u4e14\u5047\u8bbe\u54ac\u5408\u5927\u5c0f\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faFoodTrack\u6846\u67b6\uff0c\u5229\u7528\u7b2c\u4e00\u89c6\u89d2\u89c6\u9891\u76f4\u63a5\u6d4b\u91cf\u98df\u7269\u4f53\u79ef\uff0c\u65e0\u9700\u4f9d\u8d56\u624b\u52bf\u6216\u56fa\u5b9a\u5047\u8bbe\uff0c\u9002\u5e94\u6027\u5f3a\u3002", "result": "\u5728\u624b\u6301\u98df\u7269\u5bf9\u8c61\u4e0a\u5b9e\u73b0\u4e86\u7ea67.01%\u7684\u7edd\u5bf9\u767e\u5206\u6bd4\u635f\u5931\uff0c\u4f18\u4e8e\u4e4b\u524d\u65b9\u6cd5\u5728\u6700\u4f73\u60c5\u51b5\u4e0b\u768416.40%\u8bef\u5dee\u3002", "conclusion": "FoodTrack\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u98df\u7269\u6444\u5165\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04058", "pdf": "https://arxiv.org/pdf/2505.04058", "abs": "https://arxiv.org/abs/2505.04058", "authors": ["Feng Xiao", "Hongbin Xu", "Guocan Zhao", "Wenxiong Kang"], "title": "AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding", "categories": ["cs.CV"], "comment": null, "summary": "3D visual grounding aims to localize the unique target described by natural\nlanguages in 3D scenes. The significant gap between 3D and language modalities\nmakes it a notable challenge to distinguish multiple similar objects through\nthe described spatial relationships. Current methods attempt to achieve\ncross-modal understanding in complex scenes via a target-centered learning\nmechanism, ignoring the perception of referred objects. We propose a novel\n2D-assisted 3D visual grounding framework that constructs semantic-spatial\nscene graphs with referred object discrimination for relationship perception.\nThe framework incorporates a dual-branch visual encoder that utilizes 2D\npre-trained attributes to guide the multi-modal object encoding. Furthermore,\nour cross-modal interaction module uses graph attention to facilitate\nrelationship-oriented information fusion. The enhanced object representation\nand iterative relational learning enable the model to establish effective\nalignment between 3D vision and referential descriptions. Experimental results\non the popular benchmarks demonstrate our superior performance compared to\nstate-of-the-art methods, especially in addressing the challenges of multiple\nsimilar distractors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd2D\u8f85\u52a9\u76843D\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49-\u7a7a\u95f4\u573a\u666f\u56fe\u548c\u53cc\u5206\u652f\u89c6\u89c9\u7f16\u7801\u5668\u63d0\u5347\u591a\u6a21\u6001\u5bf9\u8c61\u7f16\u7801\u548c\u5173\u7cfb\u611f\u77e5\u3002", "motivation": "\u89e3\u51b33D\u4e0e\u8bed\u8a00\u6a21\u6001\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u533a\u5206\u591a\u4e2a\u76f8\u4f3c\u5bf9\u8c61\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u88ab\u5f15\u7528\u5bf9\u8c61\u7684\u611f\u77e5\u3002", "method": "\u6784\u5efa\u8bed\u4e49-\u7a7a\u95f4\u573a\u666f\u56fe\uff0c\u91c7\u7528\u53cc\u5206\u652f\u89c6\u89c9\u7f16\u7801\u5668\u5229\u75282D\u9884\u8bad\u7ec3\u5c5e\u6027\u6307\u5bfc\u591a\u6a21\u6001\u5bf9\u8c61\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u8fdb\u884c\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u591a\u4e2a\u76f8\u4f3c\u5e72\u6270\u7269\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u5bf9\u8c61\u8868\u793a\u548c\u8fed\u4ee3\u5173\u7cfb\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e863D\u89c6\u89c9\u4e0e\u5f15\u7528\u63cf\u8ff0\u4e4b\u95f4\u7684\u6709\u6548\u5bf9\u9f50\u3002"}}
{"id": "2505.04087", "pdf": "https://arxiv.org/pdf/2505.04087", "abs": "https://arxiv.org/abs/2505.04087", "authors": ["Zixuan Hu", "Yichun Hu", "Ling-Yu Duan"], "title": "SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Test-Time adaptation (TTA) aims to enhance model robustness against\ndistribution shifts through rapid model adaptation during inference. While\nexisting TTA methods often rely on entropy-based unsupervised training and\nachieve promising results, the common practice of a single round of entropy\ntraining is typically unable to adequately utilize reliable samples, hindering\nadaptation efficiency. In this paper, we discover augmentation strategies can\neffectively unleash the potential of reliable samples, but the rapidly growing\ncomputational cost impedes their real-time application. To address this\nlimitation, we propose a novel TTA approach named Single-step Ensemble of\nVicinal Augmentations (SEVA), which can take advantage of data augmentations\nwithout increasing the computational burden. Specifically, instead of\nexplicitly utilizing the augmentation strategy to generate new data, SEVA\ndevelops a theoretical framework to explore the impacts of multiple\naugmentations on model adaptation and proposes to optimize an upper bound of\nthe entropy loss to integrate the effects of multiple rounds of augmentation\ntraining into a single step. Furthermore, we discover and verify that using the\nupper bound as the loss is more conducive to the selection mechanism, as it can\neffectively filter out harmful samples that confuse the model. Combining these\ntwo key advantages, the proposed efficient loss and a complementary selection\nstrategy can simultaneously boost the potential of reliable samples and meet\nthe stringent time requirements of TTA. The comprehensive experiments on\nvarious network architectures across challenging testing scenarios demonstrate\nimpressive performances and the broad adaptability of SEVA. The code will be\npublicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEVA\u7684\u65b0\u578b\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b65\u96c6\u6210\u865a\u62df\u589e\u5f3a\u4f18\u5316\u71b5\u635f\u5931\u4e0a\u754c\uff0c\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6548\u7387\u3002", "motivation": "\u73b0\u6709TTA\u65b9\u6cd5\u4f9d\u8d56\u5355\u8f6e\u71b5\u8bad\u7ec3\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u53ef\u9760\u6837\u672c\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faSEVA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u63a2\u7d22\u591a\u589e\u5f3a\u5bf9\u6a21\u578b\u9002\u5e94\u7684\u5f71\u54cd\uff0c\u4f18\u5316\u71b5\u635f\u5931\u4e0a\u754c\uff0c\u5355\u6b65\u5b8c\u6210\u591a\u8f6e\u589e\u5f3a\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u79cd\u7f51\u7edc\u67b6\u6784\u548c\u6d4b\u8bd5\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86SEVA\u7684\u9ad8\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u5e94\u6027\u3002", "conclusion": "SEVA\u901a\u8fc7\u9ad8\u6548\u635f\u5931\u548c\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u9002\u5e94\u80fd\u529b\uff0c\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002"}}
{"id": "2505.04088", "pdf": "https://arxiv.org/pdf/2505.04088", "abs": "https://arxiv.org/abs/2505.04088", "authors": ["Shang Zhang", "Huanbin Zhang", "Dali Feng", "Yujie Cui", "Ruoyan Xiong", "Cen He"], "title": "SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared (TIR) object tracking often suffers from challenges such as\ntarget occlusion, motion blur, and background clutter, which significantly\ndegrade the performance of trackers. To address these issues, this paper\npro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a\nbidirectional state-space model and a self-attention mechanism. Specifically,\nwe introduce the Motion Mamba module into the Siamese architecture to ex-tract\nmotion features and recover overlooked edge details using bidirectional\nmodeling and self-attention. We propose a Siamese parameter-sharing strate-gy\nthat allows certain convolutional layers to share weights. This approach\nreduces computational redundancy while preserving strong feature\nrepresen-tation. In addition, we design a motion edge-aware regression loss to\nimprove tracking accuracy, especially for motion-blurred targets. Extensive\nexperi-ments are conducted on four TIR tracking benchmarks, including\nLSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT\nachieves superior performance in TIR target tracking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Siamese Motion Mamba Tracker (SMMT)\uff0c\u901a\u8fc7\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3TIR\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u906e\u6321\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u80cc\u666f\u5e72\u6270\u95ee\u9898\u3002", "motivation": "TIR\u76ee\u6807\u8ddf\u8e2a\u5e38\u56e0\u76ee\u6807\u906e\u6321\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u80cc\u666f\u5e72\u6270\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165Motion Mamba\u6a21\u5757\u5230Siamese\u67b6\u6784\u4e2d\uff0c\u7ed3\u5408\u53cc\u5411\u5efa\u6a21\u548c\u81ea\u6ce8\u610f\u529b\u63d0\u53d6\u8fd0\u52a8\u7279\u5f81\uff1b\u91c7\u7528\u53c2\u6570\u5171\u4eab\u7b56\u7565\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\uff1b\u8bbe\u8ba1\u8fd0\u52a8\u8fb9\u7f18\u611f\u77e5\u56de\u5f52\u635f\u5931\u63d0\u5347\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "result": "\u5728\u56db\u4e2aTIR\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSMMT\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "SMMT\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86TIR\u76ee\u6807\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.04105", "pdf": "https://arxiv.org/pdf/2505.04105", "abs": "https://arxiv.org/abs/2505.04105", "authors": ["Andrew Zhang", "Hao Wang", "Shuchang Ye", "Michael Fulham", "Jinman Kim"], "title": "MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction", "categories": ["cs.CV"], "comment": null, "summary": "Patient motion during medical image acquisition causes blurring, ghosting,\nand distorts organs, which makes image interpretation challenging.Current\nstate-of-the-art algorithms using Generative Adversarial Network (GAN)-based\nmethods with their ability to learn the mappings between corrupted images and\ntheir ground truth via Structural Similarity Index Measure (SSIM) loss\neffectively generate motion-free images. However, we identified the following\nlimitations: (i) they mainly focus on global structural characteristics and\ntherefore overlook localized features that often carry critical pathological\ninformation, and (ii) the SSIM loss function struggles to handle images with\nvarying pixel intensities, luminance factors, and variance. In this study, we\npropose Motion-Aware Image SYnthesis (MAISY) which initially characterize\nmotion and then uses it for correction by: (a) leveraging the foundation model\nSegment Anything Model (SAM), to dynamically learn spatial patterns along\nanatomical boundaries where motion artifacts are most pronounced and, (b)\nintroducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively\nemphasizes spatial regions with high pixel variance to preserve essential\nanatomical details during artifact correction. Experiments on chest and head CT\ndatasets demonstrate that our model outperformed the state-of-the-art\ncounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by\n10%, and Dice by 16%.", "AI": {"tldr": "MAISY\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u7a7a\u95f4\u6a21\u5f0f\u548c\u5f15\u5165VS-SSIM\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u4f2a\u5f71\u6821\u6b63\u6548\u679c\u3002", "motivation": "\u73b0\u6709GAN\u65b9\u6cd5\u5728\u5168\u5c40\u7ed3\u6784\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5ffd\u7565\u4e86\u5c40\u90e8\u7279\u5f81\u548c\u50cf\u7d20\u5f3a\u5ea6\u53d8\u5316\uff0c\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u7ed3\u5408SAM\u6a21\u578b\u52a8\u6001\u5b66\u4e60\u7a7a\u95f4\u6a21\u5f0f\uff0c\u5e76\u5f15\u5165VS-SSIM\u635f\u5931\u81ea\u9002\u5e94\u5904\u7406\u9ad8\u65b9\u5dee\u533a\u57df\u3002", "result": "\u5728\u80f8\u90e8\u548c\u5934\u90e8CT\u6570\u636e\u4e0a\uff0cPSNR\u63d0\u534740%\uff0cSSIM\u63d0\u534710%\uff0cDice\u63d0\u534716%\u3002", "conclusion": "MAISY\u65b9\u6cd5\u5728\u8fd0\u52a8\u4f2a\u5f71\u6821\u6b63\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u64c5\u957f\u4fdd\u7559\u5173\u952e\u89e3\u5256\u7ec6\u8282\u3002"}}
{"id": "2505.04109", "pdf": "https://arxiv.org/pdf/2505.04109", "abs": "https://arxiv.org/abs/2505.04109", "authors": ["Mengya Liu", "Siyuan Li", "Ajad Chhatkuli", "Prune Truong", "Luc Van Gool", "Federico Tombari"], "title": "One2Any: One-Reference 6D Pose Estimation for Any Object", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "6D object pose estimation remains challenging for many applications due to\ndependencies on complete 3D models, multi-view images, or training limited to\nspecific object categories. These requirements make generalization to novel\nobjects difficult for which neither 3D models nor multi-view images may be\navailable. To address this, we propose a novel method One2Any that estimates\nthe relative 6-degrees of freedom (DOF) object pose using only a single\nreference-single query RGB-D image, without prior knowledge of its 3D model,\nmulti-view data, or category constraints. We treat object pose estimation as an\nencoding-decoding process, first, we obtain a comprehensive Reference Object\nPose Embedding (ROPE) that encodes an object shape, orientation, and texture\nfrom a single reference view. Using this embedding, a U-Net-based pose decoding\nmodule produces Reference Object Coordinate (ROC) for new views, enabling fast\nand accurate pose estimation. This simple encoding-decoding framework allows\nour model to be trained on any pair-wise pose data, enabling large-scale\ntraining and demonstrating great scalability. Experiments on multiple benchmark\ndatasets demonstrate that our model generalizes well to novel objects,\nachieving state-of-the-art accuracy and robustness even rivaling methods that\nrequire multi-view or CAD inputs, at a fraction of compute.", "AI": {"tldr": "One2Any\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4ec5\u9700\u5355\u5f20\u53c2\u8003\u548c\u67e5\u8be2RGB-D\u56fe\u50cf\u5373\u53ef\u4f30\u8ba16D\u7269\u4f53\u59ff\u6001\uff0c\u65e0\u97003D\u6a21\u578b\u6216\u591a\u89c6\u56fe\u6570\u636e\u3002", "motivation": "\u89e3\u51b36D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4f9d\u8d56\u5b8c\u65743D\u6a21\u578b\u3001\u591a\u89c6\u56fe\u56fe\u50cf\u6216\u7279\u5b9a\u7c7b\u522b\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5bf9\u65b0\u7269\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u59ff\u6001\u4f30\u8ba1\u89c6\u4e3a\u7f16\u7801-\u89e3\u7801\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5355\u53c2\u8003\u89c6\u56fe\u751f\u6210ROPE\u5d4c\u5165\uff0c\u518d\u7528U-Net\u89e3\u7801\u6a21\u5757\u751f\u6210ROC\uff0c\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u8fbe\u5230SOTA\uff0c\u751a\u81f3\u4f18\u4e8e\u4f9d\u8d56\u591a\u89c6\u56fe\u6216CAD\u7684\u65b9\u6cd5\u3002", "conclusion": "One2Any\u5c55\u793a\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u65b0\u7269\u4f53\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002"}}
{"id": "2505.04119", "pdf": "https://arxiv.org/pdf/2505.04119", "abs": "https://arxiv.org/abs/2505.04119", "authors": ["Zixiang Ai", "Zichen Liu", "Yuanhang Lei", "Zhenyu Cui", "Xu Zou", "Jiahuan Zhou"], "title": "GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Pre-trained 3D vision models have gained significant attention for their\npromising performance on point cloud data. However, fully fine-tuning these\nmodels for downstream tasks is computationally expensive and storage-intensive.\nExisting parameter-efficient fine-tuning (PEFT) approaches, which focus\nprimarily on input token prompting, struggle to achieve competitive performance\ndue to their limited ability to capture the geometric information inherent in\npoint clouds. To address this challenge, we propose a novel Geometry-Aware\nPoint Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the\nadaptability of 3D vision models. First, we introduce a Point Prompt that\nserves as an auxiliary input alongside the original point cloud, explicitly\nguiding the model to capture fine-grained geometric details. Additionally, we\npresent a Point Shift Prompter designed to extract global shape information\nfrom the point cloud, enabling instance-specific geometric adjustments at the\ninput level. Moreover, our proposed Prompt Propagation mechanism incorporates\nthe shape information into the model's feature extraction process, further\nstrengthening its ability to capture essential geometric characteristics.\nExtensive experiments demonstrate that GAPrompt significantly outperforms\nstate-of-the-art PEFT methods and achieves competitive results compared to full\nfine-tuning on various benchmarks, while utilizing only 2.19% of trainable\nparameters. Our code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-VGP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u70b9\u4e91\u63d0\u793a\u65b9\u6cd5\uff08GAPrompt\uff09\uff0c\u901a\u8fc7\u51e0\u4f55\u7ebf\u7d22\u589e\u5f3a3D\u89c6\u89c9\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u9884\u8bad\u7ec3\u76843D\u89c6\u89c9\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b8c\u5168\u5fae\u8c03\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u9ad8\u3002\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u56e0\u96be\u4ee5\u6355\u6349\u70b9\u4e91\u51e0\u4f55\u4fe1\u606f\u800c\u6027\u80fd\u6709\u9650\u3002", "method": "\u63d0\u51faGAPrompt\uff0c\u5305\u62ec\u70b9\u63d0\u793a\uff08Point Prompt\uff09\u6355\u6349\u7ec6\u8282\u3001\u70b9\u79fb\u63d0\u793a\u5668\uff08Point Shift Prompter\uff09\u63d0\u53d6\u5168\u5c40\u5f62\u72b6\u4fe1\u606f\uff0c\u4ee5\u53ca\u63d0\u793a\u4f20\u64ad\u673a\u5236\uff08Prompt Propagation\uff09\u878d\u5165\u7279\u5f81\u63d0\u53d6\u3002", "result": "GAPrompt\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ec5\u97002.19%\u53ef\u8bad\u7ec3\u53c2\u6570\u5373\u53ef\u8fbe\u5230\u4e0e\u5b8c\u5168\u5fae\u8c03\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "conclusion": "GAPrompt\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u63d0\u793a\u6709\u6548\u63d0\u5347\u4e863D\u89c6\u89c9\u6a21\u578b\u7684\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2505.04121", "pdf": "https://arxiv.org/pdf/2505.04121", "abs": "https://arxiv.org/abs/2505.04121", "authors": ["Zixiang Ai", "Zichen Liu", "Jiahuan Zhou"], "title": "Vision Graph Prompting via Semantic Low-Rank Decomposition", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Vision GNN (ViG) demonstrates superior performance by representing images as\ngraph structures, providing a more natural way to capture irregular semantic\npatterns beyond traditional grid or sequence-based representations. To\nefficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning\ntechniques like visual prompting become increasingly essential. However,\nexisting prompting methods are primarily designed for Transformer-based models,\nneglecting the rich topological relationships among nodes and edges in\ngraph-based representations, limiting their capacity to model complex\nsemantics. In this paper, we propose Vision Graph Prompting (VGP), a novel\nframework tailored for vision graph structures. Our core insight reveals that\nsemantically connected components in the graph exhibit low-rank properties.\nBuilding on this observation, we introduce a semantic low-rank prompting method\nthat decomposes low-rank semantic features and integrates them with prompts on\nvision graph topologies, capturing both global structural patterns and\nfine-grained semantic dependencies. Extensive experiments demonstrate our\nmethod significantly improves ViG's transfer performance on diverse downstream\ntasks, achieving results comparable to full fine-tuning while maintaining\nparameter efficiency. Our code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-VGP.", "AI": {"tldr": "ViG\u901a\u8fc7\u56fe\u7ed3\u6784\u8868\u793a\u56fe\u50cf\uff0c\u4f18\u4e8e\u4f20\u7edf\u7f51\u683c\u6216\u5e8f\u5217\u8868\u793a\u3002\u672c\u6587\u63d0\u51faVGP\u6846\u67b6\uff0c\u5229\u7528\u4f4e\u79e9\u8bed\u4e49\u63d0\u793a\u65b9\u6cd5\u63d0\u5347ViG\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9Transformer\u6a21\u578b\uff0c\u5ffd\u89c6\u4e86\u56fe\u7ed3\u6784\u4e2d\u8282\u70b9\u548c\u8fb9\u7684\u62d3\u6251\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u590d\u6742\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faVGP\u6846\u67b6\uff0c\u57fa\u4e8e\u4f4e\u79e9\u8bed\u4e49\u7279\u6027\uff0c\u5c06\u8bed\u4e49\u7279\u5f81\u5206\u89e3\u5e76\u4e0e\u89c6\u89c9\u56fe\u62d3\u6251\u63d0\u793a\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVGP\u663e\u8457\u63d0\u5347ViG\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u63a5\u8fd1\u5168\u5fae\u8c03\u6548\u679c\u4e14\u4fdd\u6301\u53c2\u6570\u9ad8\u6548\u3002", "conclusion": "VGP\u4e3a\u89c6\u89c9\u56fe\u7ed3\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u63d0\u793a\u65b9\u6cd5\uff0c\u517c\u987e\u5168\u5c40\u7ed3\u6784\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4f9d\u8d56\u3002"}}
{"id": "2505.04147", "pdf": "https://arxiv.org/pdf/2505.04147", "abs": "https://arxiv.org/abs/2505.04147", "authors": ["Lixing Niu", "Jiapeng Li", "Xingping Yu", "Shu Wang", "Ruining Feng", "Bo Wu", "Ping Wei", "Yisen Wang", "Lifeng Fan"], "title": "R^3-VQA: \"Read the Room\" by Video Social Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "\"Read the room\" is a significant social reasoning capability in human daily\nlife. Humans can infer others' mental states from subtle social cues. Previous\nsocial reasoning tasks and datasets lack complexity (e.g., simple scenes, basic\ninteractions, incomplete mental state variables, single-step reasoning, etc.)\nand fall far short of the challenges present in real-life social interactions.\nIn this paper, we contribute a valuable, high-quality, and comprehensive video\ndataset named R^3-VQA with precise and fine-grained annotations of social\nevents and mental states (i.e., belief, intent, desire, and emotion) as well as\ncorresponding social causal chains in complex social scenarios. Moreover, we\ninclude human-annotated and model-generated QAs. Our task R^3-VQA includes\nthree aspects: Social Event Understanding, Mental State Estimation, and Social\nCausal Reasoning. As a benchmark, we comprehensively evaluate the social\nreasoning capabilities and consistencies of current state-of-the-art large\nvision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs\nare still far from human-level consistent social reasoning in complex social\nscenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on\nsocial reasoning tasks. We provide some of our dataset and codes in\nsupplementary material and will release our full dataset and codes upon\nacceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aR^3-VQA\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u793e\u4ea4\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6d4b\u8bd5\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\u548c\u6570\u636e\u96c6\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u793e\u4ea4\u4e92\u52a8\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u4efb\u52a1\u6765\u63a8\u52a8\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u7cbe\u7ec6\u793e\u4ea4\u4e8b\u4ef6\u548c\u5fc3\u667a\u72b6\u6001\u6807\u6ce8\u7684R^3-VQA\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u4efb\u52a1\uff1a\u793e\u4ea4\u4e8b\u4ef6\u7406\u89e3\u3001\u5fc3\u667a\u72b6\u6001\u4f30\u8ba1\u548c\u793e\u4ea4\u56e0\u679c\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u793e\u4ea4\u63a8\u7406\u4e2d\u4ecd\u8fdc\u672a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u4f7f\u7528\u5fc3\u7406\u7406\u8bba\u63d0\u793a\u53ef\u4ee5\u63d0\u5347\u5176\u8868\u73b0\u3002", "conclusion": "R^3-VQA\u4e3a\u793e\u4ea4\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.04150", "pdf": "https://arxiv.org/pdf/2505.04150", "abs": "https://arxiv.org/abs/2505.04150", "authors": ["Yu Yamaoka or Weng Ian Chan", "Shigeto Seno", "Soichiro Fukada", "Hideo Matsuda"], "title": "Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI2024 workshop ADSMI in Morocco (oral) [Peer-reviewed]", "summary": "Evaluating the regeneration process of damaged muscle tissue is a fundamental\nanalysis in muscle research to measure experimental effect sizes and uncover\nmechanisms behind muscle weakness due to aging and disease. The conventional\napproach to assessing muscle tissue regeneration involves whole-slide imaging\nand expert visual inspection of the recovery stages based on the morphological\ninformation of cells and fibers. There is a need to replace these tasks with\nautomated methods incorporating machine learning techniques to ensure a\nquantitative and objective analysis. Given the limited availability of fully\nlabeled data, a possible approach is Learning from Label Proportions (LLP), a\nweakly supervised learning method using class label proportions. However,\ncurrent LLP methods have two limitations: (1) they cannot adapt the feature\nextractor for muscle tissues, and (2) they treat the classes representing\nrecovery stages and cell morphological changes as nominal, resulting in the\nloss of ordinal information. To address these issues, we propose Ordinal Scale\nLearning from Similarity Proportion (OSLSP), which uses a similarity proportion\nloss derived from two bag combinations. OSLSP can update the feature extractor\nby using class proportion attention to the ordinal scale of the class. Our\nmodel with OSLSP outperforms large-scale pre-trained and fine-tuning models in\nclassification tasks of skeletal muscle recovery stages.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOSLSP\u7684\u5f31\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u808c\u8089\u7ec4\u7ec7\u518d\u751f\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u808c\u8089\u7ec4\u7ec7\u518d\u751f\u8bc4\u4f30\u4f9d\u8d56\u4eba\u5de5\u89c6\u89c9\u68c0\u67e5\uff0c\u7f3a\u4e4f\u5ba2\u89c2\u6027\u548c\u5b9a\u91cf\u5206\u6790\u3002\u73b0\u6709\u5f31\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u808c\u8089\u7ec4\u7ec7\u7279\u5f81\u4e14\u5ffd\u7565\u7c7b\u522b\u987a\u5e8f\u4fe1\u606f\u3002", "method": "\u63d0\u51faOSLSP\u65b9\u6cd5\uff0c\u5229\u7528\u76f8\u4f3c\u6027\u6bd4\u4f8b\u635f\u5931\u548c\u7c7b\u522b\u6bd4\u4f8b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u66f4\u65b0\u7279\u5f81\u63d0\u53d6\u5668\u5e76\u4fdd\u7559\u7c7b\u522b\u987a\u5e8f\u4fe1\u606f\u3002", "result": "OSLSP\u6a21\u578b\u5728\u9aa8\u9abc\u808c\u6062\u590d\u9636\u6bb5\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "OSLSP\u4e3a\u808c\u8089\u7ec4\u7ec7\u518d\u751f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u5b9a\u91cf\u4e14\u4fdd\u7559\u987a\u5e8f\u4fe1\u606f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04175", "pdf": "https://arxiv.org/pdf/2505.04175", "abs": "https://arxiv.org/abs/2505.04175", "authors": ["Naphat Nithisopa", "Teerapong Panboonyuen"], "title": "DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text recognition in natural images remains a challenging yet essential task,\nwith broad applications spanning computer vision and natural language\nprocessing. This paper introduces a novel end-to-end framework that combines\nResNet and Vision Transformer backbones with advanced methodologies, including\nDeformable Convolutions, Retrieval-Augmented Generation, and Conditional Random\nFields (CRF). These innovations collectively enhance feature representation and\nimprove Optical Character Recognition (OCR) performance. Specifically, the\nframework substitutes standard convolution layers in the third and fourth\nblocks with Deformable Convolutions, leverages adaptive dropout for\nregularization, and incorporates CRF for more refined sequence modeling.\nExtensive experiments conducted on six benchmark datasets IC13, IC15, SVT,\nIIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving\nnotable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on\nIIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy\nof 77.77%. These results establish a new state-of-the-art for text recognition,\ndemonstrating the robustness of the approach across diverse and challenging\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ResNet\u548cVision Transformer\u7684\u65b0\u578b\u7aef\u5230\u7aef\u6587\u672c\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u53d8\u5f62\u5377\u79ef\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u6761\u4ef6\u968f\u673a\u573a\u7b49\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86OCR\u6027\u80fd\u3002", "motivation": "\u81ea\u7136\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u8bc6\u522b\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\u63d0\u5347OCR\u7684\u6027\u80fd\u3002", "method": "\u6846\u67b6\u5728\u7b2c\u4e09\u548c\u7b2c\u56db\u5757\u4e2d\u7528\u53ef\u53d8\u5f62\u5377\u79ef\u66ff\u4ee3\u6807\u51c6\u5377\u79ef\u5c42\uff0c\u91c7\u7528\u81ea\u9002\u5e94dropout\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u5e76\u5f15\u5165\u6761\u4ef6\u968f\u673a\u573a\u4f18\u5316\u5e8f\u5217\u5efa\u6a21\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523077.77%\uff0c\u90e8\u5206\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff08\u5982IC13\u8fbe\u523097.32%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u786e\u7acb\u4e86\u6587\u672c\u8bc6\u522b\u9886\u57df\u7684\u65b0\u6807\u6746\u3002"}}
{"id": "2505.04185", "pdf": "https://arxiv.org/pdf/2505.04185", "abs": "https://arxiv.org/abs/2505.04185", "authors": ["Hail Song", "Wonsik Shin", "Naeun Lee", "Soomin Chung", "Nojun Kwak", "Woontack Woo"], "title": "S3D: Sketch-Driven 3D Model Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted as a short paper to the GMCV Workshop at CVPR'25", "summary": "Generating high-quality 3D models from 2D sketches is a challenging task due\nto the inherent ambiguity and sparsity of sketch data. In this paper, we\npresent S3D, a novel framework that converts simple hand-drawn sketches into\ndetailed 3D models. Our method utilizes a U-Net-based encoder-decoder\narchitecture to convert sketches into face segmentation masks, which are then\nused to generate a 3D representation that can be rendered from novel views. To\nensure robust consistency between the sketch domain and the 3D output, we\nintroduce a novel style-alignment loss that aligns the U-Net bottleneck\nfeatures with the initial encoder outputs of the 3D generation module,\nsignificantly enhancing reconstruction fidelity. To further enhance the\nnetwork's robustness, we apply augmentation techniques to the sketch dataset.\nThis streamlined framework demonstrates the effectiveness of S3D in generating\nhigh-quality 3D models from sketch inputs. The source code for this project is\npublicly available at https://github.com/hailsong/S3D.", "AI": {"tldr": "S3D\u6846\u67b6\u901a\u8fc7U-Net\u67b6\u6784\u548c\u98ce\u683c\u5bf9\u9f50\u635f\u5931\uff0c\u5c06\u624b\u7ed8\u8349\u56fe\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf3D\u6a21\u578b\u3002", "motivation": "\u89e3\u51b32D\u8349\u56fe\u56e0\u6a21\u7cca\u6027\u548c\u7a00\u758f\u6027\u5bfc\u81f43D\u5efa\u6a21\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528U-Net\u7f16\u7801\u5668-\u89e3\u7801\u5668\u751f\u6210\u9762\u90e8\u5206\u5272\u63a9\u7801\uff0c\u7ed3\u5408\u98ce\u683c\u5bf9\u9f50\u635f\u5931\u548c\u589e\u5f3a\u6280\u672f\u3002", "result": "\u751f\u6210\u9ad8\u8d28\u91cf3D\u6a21\u578b\uff0c\u652f\u6301\u591a\u89c6\u89d2\u6e32\u67d3\u3002", "conclusion": "S3D\u6846\u67b6\u9ad8\u6548\u4e14\u5f00\u6e90\uff0c\u9002\u7528\u4e8e\u8349\u56fe\u52303D\u7684\u8f6c\u6362\u3002"}}
{"id": "2505.04192", "pdf": "https://arxiv.org/pdf/2505.04192", "abs": "https://arxiv.org/abs/2505.04192", "authors": ["Trinh T. L. Vuong", "Jin Tae Kwak"], "title": "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA.", "AI": {"tldr": "VideoPath-LLaVA\u662f\u9996\u4e2a\u6574\u5408\u5355\u5f20\u5207\u7247\u56fe\u50cf\u3001\u81ea\u52a8\u5173\u952e\u5e27\u63d0\u53d6\u7247\u6bb5\u548c\u624b\u52a8\u5206\u5272\u89c6\u9891\u75c5\u7406\u56fe\u50cf\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u7684\u8bca\u65ad\u8fc7\u7a0b\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u53d9\u4e8b\u4e0e\u8bca\u65ad\u63a8\u7406\uff0c\u63d0\u5347\u75c5\u7406\u89c6\u9891\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u5229\u7528VideoPath-Instruct\u6570\u636e\u96c6\uff084278\u4e2a\u89c6\u9891\u548c\u8bca\u65ad\u6307\u4ee4\u5bf9\uff09\uff0c\u4ece\u5355\u56fe\u50cf\u6307\u4ee4\u6570\u636e\u8fc1\u79fb\u77e5\u8bc6\uff0c\u8bad\u7ec3\u5173\u952e\u5e27\u63d0\u53d6\u7247\u6bb5\uff0c\u518d\u5fae\u8c03\u624b\u52a8\u5206\u5272\u89c6\u9891\u3002", "result": "VideoPath-LLaVA\u5728\u75c5\u7406\u89c6\u9891\u5206\u6790\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u672a\u6765AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c6\u89c9\u4e0e\u8bca\u65ad\u63a8\u7406\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.04201", "pdf": "https://arxiv.org/pdf/2505.04201", "abs": "https://arxiv.org/abs/2505.04201", "authors": ["Ning Cheng", "Jinan Xu", "Jialing Chen", "Wenjuan Han"], "title": "SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "This paper explores the challenges of integrating tactile sensing into\nintelligent systems for multimodal reasoning, particularly in enabling\ncommonsense reasoning about the open-ended physical world. We identify two key\nchallenges: modality discrepancy, where existing large touch-language models\noften treat touch as a mere sub-modality of language, and open-ended tactile\ndata scarcity, where current datasets lack the diversity, open-endness and\ncomplexity needed for reasoning. To overcome these challenges, we introduce\nSToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of\nExperts (MoE) to dynamically process, unify, and manage tactile and language\nmodalities, capturing their unique characteristics. Crucially, we also present\na comprehensive tactile commonsense reasoning dataset and benchmark featuring\nfree-form questions and responses, 8 physical properties, 4 interactive\ncharacteristics, and diverse commonsense knowledge. Experiments show SToLa\nexhibits competitive performance compared to existing models on the PhysiCLeAR\nbenchmark and self-constructed datasets, proving the effectiveness of the\nMixture of Experts architecture in multimodal management and the performance\nadvantages for open-scenario tactile commonsense reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06\u89e6\u89c9\u611f\u77e5\u878d\u5165\u667a\u80fd\u7cfb\u7edf\u8fdb\u884c\u591a\u6a21\u6001\u63a8\u7406\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86SToLa\u6846\u67b6\u548c\u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u6a21\u6001\u5dee\u5f02\u548c\u89e6\u89c9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u89e6\u89c9\u4e0e\u8bed\u8a00\u6a21\u6001\u95f4\u7684\u5dee\u5f02\u4ee5\u53ca\u89e6\u89c9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5f00\u653e\u7269\u7406\u4e16\u754c\u7684\u5e38\u8bc6\u63a8\u7406\u3002", "method": "\u5f15\u5165SToLa\u6846\u67b6\uff0c\u5229\u7528Mixture of Experts\uff08MoE\uff09\u52a8\u6001\u5904\u7406\u89e6\u89c9\u4e0e\u8bed\u8a00\u6a21\u6001\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u89e6\u89c9\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\u3002", "result": "SToLa\u5728PhysiCLeAR\u57fa\u51c6\u548c\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86MoE\u5728\u591a\u6a21\u6001\u7ba1\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "SToLa\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3a\u5f00\u653e\u573a\u666f\u89e6\u89c9\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u6027\u80fd\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218\u3002"}}
{"id": "2505.04207", "pdf": "https://arxiv.org/pdf/2505.04207", "abs": "https://arxiv.org/abs/2505.04207", "authors": ["Mustafa Yurdakul", "\u015eakir Tasdemir"], "title": "An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Potholes cause vehicle damage and traffic accidents, creating serious safety\nand economic problems. Therefore, early and accurate detection of potholes is\ncrucial. Existing detection methods are usually only based on 2D RGB images and\ncannot accurately analyze the physical characteristics of potholes. In this\npaper, a publicly available dataset of RGB-D images (PothRGBD) is created and\nan improved YOLOv8-based model is proposed for both pothole detection and\npothole physical features analysis. The Intel RealSense D415 depth camera was\nused to collect RGB and depth data from the road surfaces, resulting in a\nPothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable\nfor segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg\narchitecture, which is structurally improved with Dynamic Snake Convolution\n(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit\n(GELU). The proposed model segmented potholes with irregular edge structure\nmore accurately, and performed perimeter and depth measurements on depth maps\nwith high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,\n85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to\n93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in\nprecision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model\nperforms pothole detection as well as perimeter and depth measurement with high\naccuracy and is suitable for real-time applications due to its low model\ncomplexity. In this way, a lightweight and effective model that can be used in\ndeep learning-based intelligent transportation solutions has been acquired.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbYOLOv8\u7684\u6a21\u578b\uff0c\u7528\u4e8eRGB-D\u56fe\u50cf\u4e2d\u7684\u5751\u6d1e\u68c0\u6d4b\u548c\u7269\u7406\u7279\u5f81\u5206\u6790\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u6a21\u578b\u3002", "motivation": "\u5751\u6d1e\u5bfc\u81f4\u8f66\u8f86\u635f\u574f\u548c\u4ea4\u901a\u4e8b\u6545\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u57fa\u4e8e2D RGB\u56fe\u50cf\uff0c\u65e0\u6cd5\u51c6\u786e\u5206\u6790\u5751\u6d1e\u7269\u7406\u7279\u5f81\u3002", "method": "\u521b\u5efa\u4e86RGB-D\u6570\u636e\u96c6PothRGBD\uff0c\u63d0\u51fa\u6539\u8fdb\u7684YOLOv8\u6a21\u578b\uff0c\u7ed3\u5408DSConv\u3001SimAM\u548cGELU\u6a21\u5757\u3002", "result": "\u6539\u8fdb\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cmAP\u4e0a\u5206\u522b\u63d0\u53471.96%\u30016.13%\u548c2.07%\u3002", "conclusion": "\u6a21\u578b\u8f7b\u91cf\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u667a\u80fd\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04214", "pdf": "https://arxiv.org/pdf/2505.04214", "abs": "https://arxiv.org/abs/2505.04214", "authors": ["Fabian Wolf", "Oliver T\u00fcselmann", "Arthur Matei", "Lukas Hennies", "Christoph Rass", "Gernot A. Fink"], "title": "CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The automatic extraction of key-value information from handwritten documents\nis a key challenge in document analysis. A reliable extraction is a\nprerequisite for the mass digitization efforts of many archives. Large Vision\nLanguage Models (LVLM) are a promising technology to tackle this problem\nespecially in scenarios where little annotated training data is available. In\nthis work, we present a novel dataset specifically designed to evaluate the\nfew-shot capabilities of LVLMs. The CM1 documents are a historic collection of\nforms with handwritten entries created in Europe to administer the Care and\nMaintenance program after World War Two. The dataset establishes three\nbenchmarks on extracting name and birthdate information and, furthermore,\nconsiders different training set sizes. We provide baseline results for two\ndifferent LVLMs and compare performances to an established full-page extraction\nmodel. While the traditional full-page model achieves highly competitive\nperformances, our experiments show that when only a few training samples are\navailable the considered LVLMs benefit from their size and heavy pretraining\nand outperform the classical approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0b\u6027\u80fd\u7684\u65b0\u6570\u636e\u96c6CM1\uff0c\u5e76\u5c55\u793a\u4e86LVLM\u5728\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u65f6\u7684\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u624b\u5199\u6587\u6863\u4e2d\u5173\u952e\u503c\u4fe1\u606f\u81ea\u52a8\u63d0\u53d6\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u63a8\u52a8\u5927\u89c4\u6a21\u6570\u5b57\u5316\u5de5\u4f5c\u3002", "method": "\u521b\u5efa\u4e86CM1\u6570\u636e\u96c6\uff0c\u5305\u542b\u5386\u53f2\u8868\u5355\u4e2d\u7684\u624b\u5199\u4fe1\u606f\uff0c\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u57fa\u51c6\u4efb\u52a1\uff08\u59d3\u540d\u548c\u51fa\u751f\u65e5\u671f\u63d0\u53d6\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cdLVLM\u4e0e\u4f20\u7edf\u5168\u9875\u63d0\u53d6\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u4f20\u7edf\u5168\u9875\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u65f6\uff0cLVLM\u51ed\u501f\u5176\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u4f18\u52bf\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "LVLM\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u573a\u666f\u4e0b\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u6587\u6863\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.04229", "pdf": "https://arxiv.org/pdf/2505.04229", "abs": "https://arxiv.org/abs/2505.04229", "authors": ["Theophilus Aidoo", "Till Koebe", "Akansh Maurya", "Hewan Shrestha", "Ingmar Weber"], "title": "A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation", "categories": ["cs.CV", "cs.CY"], "comment": null, "summary": "The scarcity and high cost of labeled high-resolution imagery have long\nchallenged remote sensing applications, particularly in low-income regions\nwhere high-resolution data are scarce. In this study, we propose a weak\nsupervision framework that estimates parking lot occupancy using 3m resolution\nsatellite imagery. By leveraging coarse temporal labels -- based on the\nassumption that parking lots of major supermarkets and hardware stores in\nGermany are typically full on Saturdays and empty on Sundays -- we train a\npairwise comparison model that achieves an AUC of 0.92 on large parking lots.\nThe proposed approach minimizes the reliance on expensive high-resolution\nimages and holds promise for scalable urban mobility analysis. Moreover, the\nmethod can be adapted to assess transit patterns and resource allocation in\nvulnerable communities, providing a data-driven basis to improve the well-being\nof those most in need.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f31\u76d1\u7763\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u5229\u75283\u7c73\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4f30\u8ba1\u505c\u8f66\u573a\u5360\u7528\u7387\uff0c\u51cf\u5c11\u5bf9\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u4f9d\u8d56\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u7a00\u7f3a\u4e14\u6602\u8d35\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6536\u5165\u5730\u533a\u3002", "method": "\u5229\u7528\u7c97\u7c92\u5ea6\u65f6\u95f4\u6807\u7b7e\uff08\u5047\u8bbe\u5fb7\u56fd\u5927\u578b\u8d85\u5e02\u548c\u4e94\u91d1\u5e97\u7684\u505c\u8f66\u573a\u5728\u5468\u516d\u901a\u5e38\u6ee1\u3001\u5468\u65e5\u901a\u5e38\u7a7a\uff09\uff0c\u8bad\u7ec3\u6210\u5bf9\u6bd4\u8f83\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u5927\u578b\u505c\u8f66\u573a\u4e0a\u7684AUC\u8fbe\u52300.92\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u7528\u4e8e\u57ce\u5e02\u6d41\u52a8\u6027\u5206\u6790\uff0c\u5e76\u9002\u7528\u4e8e\u8bc4\u4f30\u5f31\u52bf\u793e\u533a\u7684\u4ea4\u901a\u6a21\u5f0f\u548c\u8d44\u6e90\u5206\u914d\u3002"}}
{"id": "2505.04262", "pdf": "https://arxiv.org/pdf/2505.04262", "abs": "https://arxiv.org/abs/2505.04262", "authors": ["Feng Yang", "Wenliang Qian", "Wangmeng Zuo", "Hui Li"], "title": "Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to\nadvance text-to-3D generation but neglects multi-view correlations, being prone\nto geometric inconsistencies and multi-face artifacts in the generated 3D\ncontent. In this work, we propose Coupled Score Distillation (CSD), a framework\nthat couples multi-view joint distribution priors to ensure geometrically\nconsistent 3D generation while enabling the stable and direct optimization of\n3D Gaussian Splatting. Specifically, by reformulating the optimization as a\nmulti-view joint optimization problem, we derive an effective optimization rule\nthat effectively couples multi-view priors to guide optimization across\ndifferent viewpoints while preserving the diversity of generated 3D assets.\nAdditionally, we propose a framework that directly optimizes 3D Gaussian\nSplatting (3D-GS) with random initialization to generate geometrically\nconsistent 3D content. We further employ a deformable tetrahedral grid,\ninitialized from 3D-GS and refined through CSD, to produce high-quality,\nrefined meshes. Quantitative and qualitative experimental results demonstrate\nthe efficiency and competitive quality of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoupled Score Distillation (CSD)\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u591a\u89c6\u89d2\u8054\u5408\u5206\u5e03\u5148\u9a8c\uff0c\u89e3\u51b3Score Distillation Sampling (SDS)\u5728\u6587\u672c\u52303D\u751f\u6210\u4e2d\u7684\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\u548c\u591a\u9762\u4f2a\u5f71\u95ee\u9898\uff0c\u5e76\u76f4\u63a5\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\u751f\u6210\u9ad8\u8d28\u91cf3D\u5185\u5bb9\u3002", "motivation": "SDS\u5728\u6587\u672c\u52303D\u751f\u6210\u4e2d\u5ffd\u89c6\u4e86\u591a\u89c6\u89d2\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u591a\u9762\u4f2a\u5f71\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u53473D\u751f\u6210\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faCSD\u6846\u67b6\uff0c\u5c06\u4f18\u5316\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u89c6\u89d2\u8054\u5408\u4f18\u5316\uff0c\u5e76\u63a8\u5bfc\u51fa\u6709\u6548\u7684\u4f18\u5316\u89c4\u5219\u3002\u540c\u65f6\uff0c\u76f4\u63a5\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\uff083D-GS\uff09\u5e76\u7ed3\u5408\u53ef\u53d8\u5f62\u56db\u9762\u4f53\u7f51\u683c\u8fdb\u884c\u7ec6\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u80fd\u591f\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u76843D\u5185\u5bb9\u3002", "conclusion": "CSD\u6846\u67b6\u901a\u8fc7\u8026\u5408\u591a\u89c6\u89d2\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u751f\u6210\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u8d28\u91cf\uff0c\u4e3a\u6587\u672c\u52303D\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2505.04270", "pdf": "https://arxiv.org/pdf/2505.04270", "abs": "https://arxiv.org/abs/2505.04270", "authors": ["Yisen Feng", "Haoyu Zhang", "Meng Liu", "Weili Guan", "Liqiang Nie"], "title": "Object-Shot Enhanced Grounding Network for Egocentric Video", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Egocentric video grounding is a crucial task for embodied intelligence\napplications, distinct from exocentric video moment localization. Existing\nmethods primarily focus on the distributional differences between egocentric\nand exocentric videos but often neglect key characteristics of egocentric\nvideos and the fine-grained information emphasized by question-type queries. To\naddress these limitations, we propose OSGNet, an Object-Shot enhanced Grounding\nNetwork for egocentric video. Specifically, we extract object information from\nvideos to enrich video representation, particularly for objects highlighted in\nthe textual query but not directly captured in the video features.\nAdditionally, we analyze the frequent shot movements inherent to egocentric\nvideos, leveraging these features to extract the wearer's attention\ninformation, which enhances the model's ability to perform modality alignment.\nExperiments conducted on three datasets demonstrate that OSGNet achieves\nstate-of-the-art performance, validating the effectiveness of our approach. Our\ncode can be found at https://github.com/Yisen-Feng/OSGNet.", "AI": {"tldr": "OSGNet\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7684\u7269\u4f53-\u955c\u5934\u589e\u5f3a\u5b9a\u4f4d\u7f51\u7edc\uff0c\u901a\u8fc7\u63d0\u53d6\u7269\u4f53\u4fe1\u606f\u548c\u955c\u5934\u8fd0\u52a8\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u81ea\u6211\u4e2d\u5fc3\u4e0e\u5916\u90e8\u4e2d\u5fc3\u89c6\u9891\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u4f46\u5ffd\u7565\u4e86\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7684\u5173\u952e\u7279\u5f81\u548c\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3002", "method": "\u63d0\u53d6\u89c6\u9891\u4e2d\u7684\u7269\u4f53\u4fe1\u606f\u4ee5\u4e30\u5bcc\u89c6\u9891\u8868\u793a\uff0c\u5e76\u5206\u6790\u955c\u5934\u8fd0\u52a8\u7279\u5f81\u4ee5\u6355\u6349\u4f69\u6234\u8005\u7684\u6ce8\u610f\u529b\u4fe1\u606f\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOSGNet\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "OSGNet\u901a\u8fc7\u7ed3\u5408\u7269\u4f53\u548c\u955c\u5934\u7279\u5f81\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.04276", "pdf": "https://arxiv.org/pdf/2505.04276", "abs": "https://arxiv.org/abs/2505.04276", "authors": ["Yajie Fu", "Chaorui Huang", "Junwei Li", "Hui Kong", "Yibin Tian", "Huakang Li", "Zhiyuan Zhang"], "title": "HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation", "categories": ["cs.CV", "cs.MM"], "comment": "8 pages, 4 figures, International Joint Conference on Neural Networks\n  (IJCNN)", "summary": "We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that\nintegrates Transformer, Graph Convolutional Network (GCN), and diffusion model\ninto a unified framework. HDiffTG leverages the strengths of these techniques\nto significantly improve pose estimation accuracy and robustness while\nmaintaining a lightweight design. The Transformer captures global\nspatiotemporal dependencies, the GCN models local skeletal structures, and the\ndiffusion model provides step-by-step optimization for fine-tuning, achieving a\ncomplementary balance between global and local features. This integration\nenhances the model's ability to handle pose estimation under occlusions and in\ncomplex scenarios. Furthermore, we introduce lightweight optimizations to the\nintegrated model and refine the objective function design to reduce\ncomputational overhead without compromising performance. Evaluation results on\nthe Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves\nstate-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling\nin both accuracy and computational efficiency. Additionally, the model exhibits\nexceptional robustness in noisy and occluded environments. Source codes and\nmodels are available at https://github.com/CirceJie/HDiffTG", "AI": {"tldr": "HDiffTG\u662f\u4e00\u79cd\u65b0\u9896\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408Transformer\u3001GCN\u548c\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b33D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u5728\u906e\u6321\u548c\u590d\u6742\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u96c6\u6210Transformer\uff08\u5168\u5c40\u65f6\u7a7a\u4f9d\u8d56\uff09\u3001GCN\uff08\u5c40\u90e8\u9aa8\u9abc\u7ed3\u6784\uff09\u548c\u6269\u6563\u6a21\u578b\uff08\u9010\u6b65\u4f18\u5316\uff09\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u5316\u8bbe\u8ba1\u3002", "result": "\u5728Human3.6M\u548cMPI-INF-3DHP\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u5bf9\u566a\u58f0\u548c\u906e\u6321\u9c81\u68d2\u3002", "conclusion": "HDiffTG\u901a\u8fc7\u591a\u6280\u672f\u878d\u5408\u548c\u8f7b\u91cf\u5316\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u76843D\u59ff\u6001\u4f30\u8ba1\u3002"}}
{"id": "2505.04281", "pdf": "https://arxiv.org/pdf/2505.04281", "abs": "https://arxiv.org/abs/2505.04281", "authors": ["Yi Li", "Zhiyuan Zhang", "Jiangnan Xia", "Jianghan Cheng", "Qilong Wu", "Junwei Li", "Yibin Tian", "Hui Kong"], "title": "TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement", "categories": ["cs.CV"], "comment": "International Joint Conference on Neural Networks (IJCNN)", "summary": "This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing\nextremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes\nnoisy images by constructing multiple virtual cameras based on a noise space.\nCamera Feature Integration (CFI) modules are then designed to enable the model\nto learn generalizable features across diverse virtual cameras. During the\naligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is\nfine-tuned using a small amount of real RAW data to adapt to the noise\ncharacteristics of specific cameras. A structural reparameterization technique\nfurther simplifies CFI$^T$ for efficient deployment. To address color shifts\nduring the diffusion process, a color corrector is introduced to ensure color\nconsistency by dynamically adjusting global color distributions. Additionally,\na novel dataset, QID, is constructed, featuring quantifiable illumination\nlevels and a wide dynamic range, providing a comprehensive benchmark for\ntraining and evaluation under extreme low-light conditions. Experimental\nresults demonstrate that TS-Diff achieves state-of-the-art performance on\nmultiple datasets, including QID, SID, and ELD, excelling in denoising,\ngeneralization, and color consistency across various cameras and illumination\nlevels. These findings highlight the robustness and versatility of TS-Diff,\nmaking it a practical solution for low-light imaging applications. Source codes\nand models are available at https://github.com/CircccleK/TS-Diff", "AI": {"tldr": "TS-Diff\u662f\u4e00\u79cd\u65b0\u578b\u7684\u4e24\u9636\u6bb5\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u589e\u5f3a\u6781\u4f4e\u5149RAW\u56fe\u50cf\u3002\u901a\u8fc7\u865a\u62df\u76f8\u673a\u548c\u566a\u58f0\u7a7a\u95f4\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u76ee\u6807\u7279\u5b9a\u8c03\u6574\u548c\u989c\u8272\u6821\u6b63\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u53bb\u566a\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6781\u4f4e\u5149\u6761\u4ef6\u4e0bRAW\u56fe\u50cf\u7684\u53bb\u566a\u548c\u989c\u8272\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u4e24\u9636\u6bb5\u6a21\u578b\uff1a\u9884\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u865a\u62df\u76f8\u673a\u751f\u6210\u566a\u58f0\u56fe\u50cf\uff0c\u5bf9\u9f50\u9636\u6bb5\u4f7f\u7528\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u3002\u5f15\u5165CFI\u6a21\u5757\u548c\u989c\u8272\u6821\u6b63\u5668\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u53bb\u566a\u3001\u6cdb\u5316\u548c\u989c\u8272\u4e00\u81f4\u6027\u65b9\u9762\u3002", "conclusion": "TS-Diff\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u901a\u7528\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04306", "pdf": "https://arxiv.org/pdf/2505.04306", "abs": "https://arxiv.org/abs/2505.04306", "authors": ["Qiannan Fan", "Zhuoyang Li", "Jitong Li", "Chenyang Cao"], "title": "MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition", "categories": ["cs.CV", "I.4.8; I.5.4; I.2.10"], "comment": "8 pages,7 figures", "summary": "With the continuous impact of epidemics, people have become accustomed to\nwearing masks. However, most current occluded face recognition (OFR) algorithms\nlack prior knowledge of occlusions, resulting in poor performance when dealing\nwith occluded faces of varying types and severity in reality. Recognizing\noccluded faces is still a significant challenge, which greatly affects the\nconvenience of people's daily lives. In this paper, we propose an\nidentity-gated mixture of diffusion experts (MoDE) for OFR. Each\ndiffusion-based generative expert estimates one possible complete image for\noccluded faces. Considering the random sampling process of the diffusion model,\nwhich introduces inevitable differences and variations between the inpainted\nfaces and the real ones. To ensemble effective information from\nmulti-reconstructed faces, we introduce an identity-gating network to evaluate\nthe contribution of each reconstructed face to the identity and adaptively\nintegrate the predictions in the decision space. Moreover, our MoDE is a\nplug-and-play module for most existing face recognition models. Extensive\nexperiments on three public face datasets and two datasets in the wild validate\nour advanced performance for various occlusions in comparison with the\ncompeting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08MoDE\uff09\u7684\u906e\u6321\u4eba\u8138\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8eab\u4efd\u95e8\u63a7\u7f51\u7edc\u81ea\u9002\u5e94\u6574\u5408\u591a\u91cd\u5efa\u4eba\u8138\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u906e\u6321\u4eba\u8138\u8bc6\u522b\u7b97\u6cd5\u7f3a\u4e4f\u5bf9\u906e\u6321\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u65e5\u5e38\u751f\u6d3b\u7684\u4fbf\u5229\u6027\u3002", "method": "\u63d0\u51fa\u8eab\u4efd\u95e8\u63a7\u6269\u6563\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08MoDE\uff09\uff0c\u6bcf\u4e2a\u6269\u6563\u751f\u6210\u4e13\u5bb6\u4f30\u8ba1\u4e00\u79cd\u53ef\u80fd\u7684\u5b8c\u6574\u4eba\u8138\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u8eab\u4efd\u95e8\u63a7\u7f51\u7edc\u8bc4\u4f30\u548c\u6574\u5408\u591a\u91cd\u5efa\u4eba\u8138\u4fe1\u606f\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u4eba\u8138\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u91ce\u5916\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u906e\u6321\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MoDE\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u906e\u6321\u4eba\u8138\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.04320", "pdf": "https://arxiv.org/pdf/2505.04320", "abs": "https://arxiv.org/abs/2505.04320", "authors": ["Zijun Zhou", "Yingying Deng", "Xiangyu He", "Weiming Dong", "Fan Tang"], "title": "Multi-turn Consistent Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Many real-world applications, such as interactive photo retouching, artistic\ncontent creation, and product design, require flexible and iterative image\nediting. However, existing image editing methods primarily focus on achieving\nthe desired modifications in a single step, which often struggles with\nambiguous user intent, complex transformations, or the need for progressive\nrefinements. As a result, these methods frequently produce inconsistent\noutcomes or fail to meet user expectations. To address these challenges, we\npropose a multi-turn image editing framework that enables users to iteratively\nrefine their edits, progressively achieving more satisfactory results. Our\napproach leverages flow matching for accurate image inversion and a\ndual-objective Linear Quadratic Regulators (LQR) for stable sampling,\neffectively mitigating error accumulation. Additionally, by analyzing the\nlayer-wise roles of transformers, we introduce a adaptive attention\nhighlighting method that enhances editability while preserving multi-turn\ncoherence. Extensive experiments demonstrate that our framework significantly\nimproves edit success rates and visual fidelity compared to existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u89e3\u51b3\u5355\u6b65\u7f16\u8f91\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u7f16\u8f91\u6548\u679c\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u591a\u4e3a\u5355\u6b65\u64cd\u4f5c\uff0c\u96be\u4ee5\u5904\u7406\u6a21\u7cca\u610f\u56fe\u3001\u590d\u6742\u53d8\u6362\u6216\u6e10\u8fdb\u4f18\u5316\u9700\u6c42\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4e00\u81f4\u6216\u4e0d\u7b26\u5408\u9884\u671f\u3002", "method": "\u91c7\u7528\u6d41\u5339\u914d\u5b9e\u73b0\u56fe\u50cf\u53cd\u6f14\uff0c\u53cc\u76ee\u6807LQR\u7a33\u5b9a\u91c7\u6837\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u589e\u5f3a\u65b9\u6cd5\u63d0\u5347\u7f16\u8f91\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u8f91\u6210\u529f\u7387\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u591a\u8f6e\u7f16\u8f91\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5355\u6b65\u7f16\u8f91\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u7f16\u8f91\u8d28\u91cf\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2505.04347", "pdf": "https://arxiv.org/pdf/2505.04347", "abs": "https://arxiv.org/abs/2505.04347", "authors": ["Yanyu Li", "Pencheng Wan", "Liang Han", "Yaowei Wang", "Liqiang Nie", "Min Zhang"], "title": "CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion", "categories": ["cs.CV"], "comment": "8 pages, 9 figures, 3 tables", "summary": "Stable Diffusion has advanced text-to-image synthesis, but training models to\ngenerate images with accurate object quantity is still difficult due to the\nhigh computational cost and the challenge of teaching models the abstract\nconcept of quantity. In this paper, we propose CountDiffusion, a training-free\nframework aiming at generating images with correct object quantity from textual\ndescriptions. CountDiffusion consists of two stages. In the first stage, an\nintermediate denoising result is generated by the diffusion model to predict\nthe final synthesized image with one-step denoising, and a counting model is\nused to count the number of objects in this image. In the second stage, a\ncorrection module is used to correct the object quantity by changing the\nattention map of the object with universal guidance. The proposed\nCountDiffusion can be plugged into any diffusion-based text-to-image (T2I)\ngeneration models without further training. Experiment results demonstrate the\nsuperiority of our proposed CountDiffusion, which improves the accurate object\nquantity generation ability of T2I models by a large margin.", "AI": {"tldr": "CountDiffusion\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u6539\u8fdb\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u80fd\u529b\uff0c\u786e\u4fdd\u751f\u6210\u56fe\u50cf\u4e2d\u7269\u4f53\u6570\u91cf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\u96be\u4ee5\u51c6\u786e\u63a7\u5236\u7269\u4f53\u6570\u91cf\uff0c\u4e3b\u8981\u7531\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u62bd\u8c61\u6570\u91cf\u6982\u5ff5\u96be\u4ee5\u5efa\u6a21\u3002", "method": "CountDiffusion\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u9996\u5148\u751f\u6210\u4e2d\u95f4\u53bb\u566a\u7ed3\u679c\u5e76\u8ba1\u6570\u7269\u4f53\u6570\u91cf\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6ce8\u610f\u529b\u56fe\u4fee\u6b63\u7269\u4f53\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCountDiffusion\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u51c6\u786e\u7269\u4f53\u6570\u91cf\u7684\u80fd\u529b\u3002", "conclusion": "CountDiffusion\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\u3002"}}
{"id": "2505.04369", "pdf": "https://arxiv.org/pdf/2505.04369", "abs": "https://arxiv.org/abs/2505.04369", "authors": ["Jie Sun", "Heng Liu", "Yongzhen Wang", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we reveal a novel haze-specific wavelet degradation prior\nobserved through wavelet transform analysis, which shows that haze-related\ninformation predominantly resides in low-frequency components. Exploiting this\ninsight, we propose a novel dehazing framework, WDMamba, which decomposes the\nimage dehazing task into two sequential stages: low-frequency restoration\nfollowed by detail enhancement. This coarse-to-fine strategy enables WDMamba to\neffectively capture features specific to each stage of the dehazing process,\nresulting in high-quality restored images. Specifically, in the low-frequency\nrestoration stage, we integrate Mamba blocks to reconstruct global structures\nwith linear complexity, efficiently removing overall haze and producing a\ncoarse restored image. Thereafter, the detail enhancement stage reinstates\nfine-grained information that may have been overlooked during the previous\nphase, culminating in the final dehazed output. Furthermore, to enhance detail\nretention and achieve more natural dehazing, we introduce a self-guided\ncontrastive regularization during network training. By utilizing the coarse\nrestored output as a hard negative example, our model learns more\ndiscriminative representations, substantially boosting the overall dehazing\nperformance. Extensive evaluations on public dehazing benchmarks demonstrate\nthat our method surpasses state-of-the-art approaches both qualitatively and\nquantitatively. Code is available at https://github.com/SunJ000/WDMamba.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u96fe\u973e\u9000\u5316\u5148\u9a8c\uff0c\u53d1\u73b0\u96fe\u973e\u4fe1\u606f\u4e3b\u8981\u5b58\u5728\u4e8e\u4f4e\u9891\u5206\u91cf\u4e2d\uff0c\u5e76\u636e\u6b64\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u53bb\u96fe\u6846\u67b6WDMamba\uff0c\u7ed3\u5408Mamba\u5757\u548c\u81ea\u5f15\u5bfc\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u96fe\u6548\u679c\u3002", "motivation": "\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u5206\u6790\u53d1\u73b0\u96fe\u973e\u4fe1\u606f\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4f4e\u9891\u5206\u91cf\uff0c\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u53bb\u96fe\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51faWDMamba\u6846\u67b6\uff0c\u5206\u4f4e\u9891\u6062\u590d\u548c\u7ec6\u8282\u589e\u5f3a\u4e24\u9636\u6bb5\u5904\u7406\uff0c\u4f7f\u7528Mamba\u5757\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u5168\u5c40\u7ed3\u6784\u91cd\u5efa\uff0c\u5e76\u5f15\u5165\u81ea\u5f15\u5bfc\u5bf9\u6bd4\u6b63\u5219\u5316\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728\u516c\u5f00\u53bb\u96fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWDMamba\u5728\u8d28\u91cf\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "WDMamba\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u548c\u81ea\u5f15\u5bfc\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u53bb\u96fe\u6548\u679c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.04375", "pdf": "https://arxiv.org/pdf/2505.04375", "abs": "https://arxiv.org/abs/2505.04375", "authors": ["Moseli Mots'oehli", "Hope Mogale", "Kyungim Baek"], "title": "Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning pre-trained convolutional neural networks on ImageNet for\ndownstream tasks is well-established. Still, the impact of model size on the\nperformance of vision transformers in similar scenarios, particularly under\nlabel noise, remains largely unexplored. Given the utility and versatility of\ntransformer architectures, this study investigates their practicality under\nlow-budget constraints and noisy labels. We explore how classification accuracy\nand calibration are affected by symmetric label noise in active learning\nsettings, evaluating four vision transformer configurations (Base and Large\nwith 16x16 and 32x32 patch sizes) and three Swin Transformer configurations\n(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label\nnoise rates. Our findings show that larger ViT models (ViTl32 in particular)\nconsistently outperform their smaller counterparts in both accuracy and\ncalibration, even under moderate to high label noise, while Swin Transformers\nexhibit weaker robustness across all noise levels. We find that smaller patch\nsizes do not always lead to better performance, as ViTl16 performs consistently\nworse than ViTl32 while incurring a higher computational cost. We also find\nthat information-based Active Learning strategies only provide meaningful\naccuracy improvements at moderate label noise rates, but they result in poorer\ncalibration compared to models trained on randomly acquired labels, especially\nat high label noise rates. We hope these insights provide actionable guidance\nfor practitioners looking to deploy vision transformers in resource-constrained\nenvironments, where balancing model complexity, label noise, and compute\nefficiency is critical in model fine-tuning or distillation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u89c4\u6a21\u7684\u89c6\u89c9Transformer\uff08ViT\uff09\u548cSwin Transformer\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u8f83\u5927\u7684ViT\u6a21\u578b\uff08\u5982ViTl32\uff09\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u800cSwin Transformer\u7684\u9c81\u68d2\u6027\u8f83\u5f31\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u89c6\u89c9Transformer\u5728\u4f4e\u9884\u7b97\u548c\u6807\u7b7e\u566a\u58f0\u573a\u666f\u4e0b\u7684\u5b9e\u7528\u6027\uff0c\u586b\u8865\u4e86\u6a21\u578b\u89c4\u6a21\u5bf9Transformer\u6027\u80fd\u5f71\u54cd\u7684\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cdViT\u914d\u7f6e\u548c\u4e09\u79cdSwin Transformer\u914d\u7f6e\u5728CIFAR10\u548cCIFAR100\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u6807\u7b7e\u566a\u58f0\u7387\u5bf9\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u7684\u5f71\u54cd\u3002", "result": "\u8f83\u5927\u7684ViT\u6a21\u578b\uff08\u5982ViTl32\uff09\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u800cSwin Transformer\u5728\u6240\u6709\u566a\u58f0\u6c34\u5e73\u4e0b\u9c81\u68d2\u6027\u8f83\u5f31\u3002\u8f83\u5c0f\u7684\u8865\u4e01\u5c3a\u5bf8\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u90e8\u7f72\u89c6\u89c9Transformer\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3001\u6807\u7b7e\u566a\u58f0\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2505.04376", "pdf": "https://arxiv.org/pdf/2505.04376", "abs": "https://arxiv.org/abs/2505.04376", "authors": ["Zili Zhang", "Ziting Wen", "Yiheng Qiang", "Hongzhou Dong", "Wenle Dong", "Xinyang Li", "Xiaofan Wang", "Xiaoqiang Ren"], "title": "Label-efficient Single Photon Images Classification via Active Learning", "categories": ["cs.CV"], "comment": null, "summary": "Single-photon LiDAR achieves high-precision 3D imaging in extreme\nenvironments through quantum-level photon detection technology. Current\nresearch primarily focuses on reconstructing 3D scenes from sparse photon\nevents, whereas the semantic interpretation of single-photon images remains\nunderexplored, due to high annotation costs and inefficient labeling\nstrategies. This paper presents the first active learning framework for\nsingle-photon image classification. The core contribution is an imaging\ncondition-aware sampling strategy that integrates synthetic augmentation to\nmodel variability across imaging conditions. By identifying samples where the\nmodel is both uncertain and sensitive to these conditions, the proposed method\nselectively annotates only the most informative examples. Experiments on both\nsynthetic and real-world datasets show that our approach outperforms all\nbaselines and achieves high classification accuracy with significantly fewer\nlabeled samples. Specifically, our approach achieves 97% accuracy on synthetic\nsingle-photon data using only 1.5% labeled samples. On real-world data, we\nmaintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher\nthan the best-performing baseline. This illustrates that active learning\nenables the same level of classification performance on single-photon images as\non classical images, opening doors to large-scale integration of single-photon\ndata in real-world applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5355\u5149\u5b50\u56fe\u50cf\u5206\u7c7b\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6210\u50cf\u6761\u4ef6\u611f\u77e5\u7684\u91c7\u6837\u7b56\u7565\u548c\u5408\u6210\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u6837\u672c\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ece\u7a00\u758f\u5149\u5b50\u4e8b\u4ef6\u91cd\u5efa3D\u573a\u666f\uff0c\u800c\u5355\u5149\u5b50\u56fe\u50cf\u7684\u8bed\u4e49\u89e3\u91ca\u56e0\u9ad8\u6807\u6ce8\u6210\u672c\u548c\u4f4e\u6548\u6807\u6ce8\u7b56\u7565\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u50cf\u6761\u4ef6\u611f\u77e5\u7684\u91c7\u6837\u7b56\u7565\uff0c\u7ed3\u5408\u5408\u6210\u589e\u5f3a\u6280\u672f\uff0c\u9009\u62e9\u6027\u5730\u6807\u6ce8\u6700\u5177\u4fe1\u606f\u91cf\u7684\u6837\u672c\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u4ec5\u97001.5%\u6807\u6ce8\u6837\u672c\u5373\u8fbe\u523097%\u51c6\u786e\u7387\uff1b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u4ec5\u97008%\u6807\u6ce8\u6837\u672c\uff0c\u51c6\u786e\u7387\u8fbe90.63%\uff0c\u4f18\u4e8e\u57fa\u7ebf4.51%\u3002", "conclusion": "\u4e3b\u52a8\u5b66\u4e60\u4f7f\u5355\u5149\u5b50\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u8fbe\u5230\u4e0e\u7ecf\u5178\u56fe\u50cf\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u4e3a\u5355\u5149\u5b50\u6570\u636e\u7684\u5927\u89c4\u6a21\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2505.04380", "pdf": "https://arxiv.org/pdf/2505.04380", "abs": "https://arxiv.org/abs/2505.04380", "authors": ["Jinhai Xiang", "Shuai Guo", "Qianru Han", "Dantong Shi", "Xinwei He", "Xiang Bai"], "title": "Tetrahedron-Net for Medical Image Registration", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Medical image registration plays a vital role in medical image processing.\nExtracting expressive representations for medical images is crucial for\nimproving the registration quality. One common practice for this end is\nconstructing a convolutional backbone to enable interactions with skip\nconnections among feature extraction layers. The de facto structure, U-Net-like\nnetworks, has attempted to design skip connections such as nested or full-scale\nones to connect one single encoder and one single decoder to improve its\nrepresentation capacity. Despite being effective, it still does not fully\nexplore interactions with a single encoder and decoder architectures. In this\npaper, we embrace this observation and introduce a simple yet effective\nalternative strategy to enhance the representations for registrations by\nappending one additional decoder. The new decoder is designed to interact with\nboth the original encoder and decoder. In this way, it not only reuses feature\npresentation from corresponding layers in the encoder but also interacts with\nthe original decoder to corporately give more accurate registration results.\nThe new architecture is concise yet generalized, with only one encoder and two\ndecoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.\nThree instantiations of Tetrahedron-Net are further constructed regarding the\ndifferent structures of the appended decoder. Our extensive experiments prove\nthat superior performance can be obtained on several representative benchmarks\nof medical image registration. Finally, such a ``Tetrahedron'' design can also\nbe easily integrated into popular U-Net-like architectures including\nVoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTetrahedron-Net\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u589e\u52a0\u4e00\u4e2a\u89e3\u7801\u5668\u6765\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u8868\u793a\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709U-Net\u7c7b\u7f51\u7edc\u5728\u5355\u7f16\u7801\u5668\u548c\u5355\u89e3\u7801\u5668\u67b6\u6784\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u8868\u793a\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u4e00\u4e2a\u7f16\u7801\u5668\u548c\u4e24\u4e2a\u89e3\u7801\u5668\u7684Tetrahedron-Net\uff0c\u65b0\u589e\u89e3\u7801\u5668\u4e0e\u539f\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4ea4\u4e92\uff0c\u63d0\u5347\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u80fd\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709U-Net\u7c7b\u67b6\u6784\u4e2d\u3002", "conclusion": "Tetrahedron-Net\u662f\u4e00\u79cd\u7b80\u6d01\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2505.04384", "pdf": "https://arxiv.org/pdf/2505.04384", "abs": "https://arxiv.org/abs/2505.04384", "authors": ["Ming-Hui Liu", "Xiao-Qian Liu", "Xin Luo", "Xin-Shun Xu"], "title": "DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution", "categories": ["cs.CV"], "comment": "Accepted by IEEE TMM on 17-Jan-2025; Submitted to IEEE TMM on\n  11-Jul-2024", "summary": "Deepfake attribution (DFA) aims to perform multiclassification on different\nfacial manipulation techniques, thereby mitigating the detrimental effects of\nforgery content on the social order and personal reputations. However, previous\nmethods focus only on method-specific clues, which easily lead to overfitting,\nwhile overlooking the crucial role of common forgery features. Additionally,\nthey struggle to distinguish between uncertain novel classes in more practical\nopen-world scenarios. To address these issues, in this paper we propose an\ninnovative multi-DisentAnglement based conTrastive leArning framework, DATA, to\nenhance the generalization ability on novel classes for the open-world\nsemi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all\ngeneration techniques can be abstracted into a similar architecture, DATA\ndefines the concept of 'Orthonormal Deepfake Basis' for the first time and\nutilizes it to disentangle method-specific features, thereby reducing the\noverfitting on forgery-irrelevant information. Furthermore, an augmented-memory\nmechanism is designed to assist in novel class discovery and contrastive\nlearning, which aims to obtain clear class boundaries for the novel classes\nthrough instance-level disentanglements. Additionally, to enhance the\nstandardization and discrimination of features, DATA uses bases contrastive\nloss and center contrastive loss as auxiliaries for the aforementioned modules.\nExtensive experimental evaluations show that DATA achieves state-of-the-art\nperformance on the OSS-DFA benchmark, e.g., there are notable accuracy\nimprovements in 2.55% / 5.7% under different settings, compared with the\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDATA\u7684\u591a\u89e3\u7f20\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u534a\u76d1\u7763\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u4efb\u52a1\uff0c\u901a\u8fc7\u6b63\u4ea4\u6df1\u5ea6\u4f2a\u9020\u57fa\u548c\u589e\u5f3a\u8bb0\u5fc6\u673a\u5236\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u7279\u5b9a\u4f2a\u9020\u7ebf\u7d22\u800c\u5ffd\u7565\u5171\u540c\u4f2a\u9020\u7279\u5f81\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u5bf9\u65b0\u7c7b\u522b\u7684\u533a\u5206\u80fd\u529b\u3002", "method": "\u5b9a\u4e49\u6b63\u4ea4\u6df1\u5ea6\u4f2a\u9020\u57fa\u4ee5\u89e3\u7f20\u7279\u5b9a\u65b9\u6cd5\u7279\u5f81\uff0c\u8bbe\u8ba1\u589e\u5f3a\u8bb0\u5fc6\u673a\u5236\u7528\u4e8e\u65b0\u7c7b\u522b\u53d1\u73b0\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u57fa\u5bf9\u6bd4\u635f\u5931\u548c\u4e2d\u5fc3\u5bf9\u6bd4\u635f\u5931\u4f18\u5316\u7279\u5f81\u3002", "result": "\u5728OSS-DFA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u63d0\u53472.55%\u548c5.7%\u3002", "conclusion": "DATA\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u7684\u6cdb\u5316\u80fd\u529b\u548c\u65b0\u7c7b\u522b\u533a\u5206\u6027\u80fd\u3002"}}
{"id": "2505.04392", "pdf": "https://arxiv.org/pdf/2505.04392", "abs": "https://arxiv.org/abs/2505.04392", "authors": ["Petr Jahoda", "Jan Cech"], "title": "Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle", "categories": ["cs.CV"], "comment": "Accepted to the IEEE Intelligent Vehicles Symposium (IV), 2025", "summary": "A novel approach to detect road surface anomalies by visual tracking of a\npreceding vehicle is proposed. The method is versatile, predicting any kind of\nroad anomalies, such as potholes, bumps, debris, etc., unlike direct\nobservation methods that rely on training visual detectors of those cases. The\nmethod operates in low visibility conditions or in dense traffic where the\nanomaly is occluded by a preceding vehicle. Anomalies are detected\npredictively, i.e., before a vehicle encounters them, which allows to\npre-configure low-level vehicle systems (such as chassis) or to plan an\navoidance maneuver in case of autonomous driving. A challenge is that the\nsignal coming from camera-based tracking of a preceding vehicle may be weak and\ndisturbed by camera ego motion due to vibrations affecting the ego vehicle.\nTherefore, we propose an efficient method to compensate camera pitch rotation\nby an iterative robust estimator. Our experiments on both controlled setup and\nnormal traffic conditions show that road anomalies can be detected reliably at\na distance even in challenging cases where the ego vehicle traverses imperfect\nroad surfaces. The method is effective and performs in real time on standard\nconsumer hardware.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u89c6\u89c9\u8ddf\u8e2a\u524d\u8f66\u6765\u68c0\u6d4b\u9053\u8def\u5f02\u5e38\u7684\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u80fd\u89c1\u5ea6\u6216\u5bc6\u96c6\u4ea4\u901a\u573a\u666f\uff0c\u5e76\u80fd\u63d0\u524d\u9884\u6d4b\u5f02\u5e38\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u76f4\u63a5\u89c2\u5bdf\u548c\u8bad\u7ec3\u89c6\u89c9\u68c0\u6d4b\u5668\uff0c\u800c\u65b0\u65b9\u6cd5\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5c24\u5176\u662f\u5728\u524d\u8f66\u906e\u6321\u6216\u4f4e\u80fd\u89c1\u5ea6\u6761\u4ef6\u4e0b\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u8ddf\u8e2a\u524d\u8f66\uff0c\u7ed3\u5408\u8fed\u4ee3\u9c81\u68d2\u4f30\u8ba1\u5668\u8865\u507f\u76f8\u673a\u4fef\u4ef0\u65cb\u8f6c\uff0c\u5b9e\u65f6\u68c0\u6d4b\u9053\u8def\u5f02\u5e38\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u590d\u6742\u8def\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e5f\u80fd\u53ef\u9760\u5730\u8fdc\u8ddd\u79bb\u68c0\u6d4b\u5f02\u5e38\uff0c\u4e14\u5b9e\u65f6\u6027\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u5b9e\u65f6\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6216\u8f66\u8f86\u5e95\u76d8\u9884\u914d\u7f6e\u7b49\u573a\u666f\u3002"}}
{"id": "2505.04394", "pdf": "https://arxiv.org/pdf/2505.04394", "abs": "https://arxiv.org/abs/2505.04394", "authors": ["Young-Hu Park", "Rae-Hong Park", "Hyung-Min Park"], "title": "SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer", "categories": ["cs.CV", "eess.AS"], "comment": null, "summary": "This paper presents an efficient visual speech encoder for lip reading. While\nmost recent lip reading studies have been based on the ResNet architecture and\nhave achieved significant success, they are not sufficiently suitable for\nefficiently capturing lip reading features due to high computational complexity\nin modeling spatio-temporal information. Additionally, using a complex visual\nmodel not only increases the complexity of lip reading models but also induces\ndelays in the overall network for multi-modal studies (e.g., audio-visual\nspeech recognition, speech enhancement, and speech separation). To overcome the\nlimitations of Convolutional Neural Network (CNN)-based models, we apply the\nhierarchical structure and window self-attention of the Swin Transformer to lip\nreading. We configure a new lightweight scale of the Swin Transformer suitable\nfor processing lip reading data and present the SwinLip visual speech encoder,\nwhich efficiently reduces computational load by integrating modified\nConvolution-augmented Transformer (Conformer) temporal embeddings with\nconventional spatial embeddings in the hierarchical structure. Through\nextensive experiments, we have validated that our SwinLip successfully improves\nthe performance and inference speed of the lip reading network when applied to\nvarious backbones for word and sentence recognition, reducing computational\nload. In particular, our SwinLip demonstrated robust performance in both\nEnglish LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art\nperformance on the Mandarin LRW-1000 dataset with less computation compared to\nthe existing state-of-the-art model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSwin Transformer\u7684\u9ad8\u6548\u89c6\u89c9\u8bed\u97f3\u7f16\u7801\u5668SwinLip\uff0c\u7528\u4e8e\u5507\u8bfb\u4efb\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eResNet\u7684\u5507\u8bfb\u6a21\u578b\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u4e0d\u9002\u5408\u9ad8\u6548\u6355\u6349\u5507\u8bfb\u7279\u5f81\uff0c\u4e14\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u5f15\u5165\u5ef6\u8fdf\u3002", "method": "\u91c7\u7528Swin Transformer\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u6539\u8fdb\u7684Conformer\u65f6\u95f4\u5d4c\u5165\u548c\u4f20\u7edf\u7a7a\u95f4\u5d4c\u5165\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7SwinLip\u7f16\u7801\u5668\u3002", "result": "\u5728\u82f1\u8bedLRW\u548c\u6c49\u8bedLRW-1000\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u91cf\u66f4\u5c11\uff0c\u4e14\u5728\u6c49\u8bedLRW-1000\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "SwinLip\u5728\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5507\u8bfb\u7f51\u7edc\u7684\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u3002"}}
{"id": "2505.04397", "pdf": "https://arxiv.org/pdf/2505.04397", "abs": "https://arxiv.org/abs/2505.04397", "authors": ["Ziyuan Li", "Uwe Jaekel", "Babette Dellen"], "title": "Deep residual learning with product units", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose a deep product-unit residual neural network (PURe) that integrates\nproduct units into residual blocks to improve the expressiveness and parameter\nefficiency of deep convolutional networks. Unlike standard summation neurons,\nproduct units enable multiplicative feature interactions, potentially offering\na more powerful representation of complex patterns. PURe replaces conventional\nconvolutional layers with 2D product units in the second layer of each residual\nblock, eliminating nonlinear activation functions to preserve structural\ninformation. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,\nPURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper\nResNet152, while converging nearly five times faster and demonstrating strong\nrobustness to Poisson noise. On ImageNet, PURe architectures outperform\nstandard ResNet models at similar depths, with PURe34 achieving a top-1\naccuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet\nvariants (ResNet50, ResNet101) while utilizing significantly fewer parameters\nand computational resources. On CIFAR-10, PURe consistently outperforms ResNet\nvariants across varying depths, with PURe272 reaching 95.01% test accuracy,\ncomparable to ResNet1001 but at less than half the model size. These results\ndemonstrate that PURe achieves a favorable balance between accuracy,\nefficiency, and robustness. Compared to traditional residual networks, PURe not\nonly achieves competitive classification performance with faster convergence\nand fewer parameters, but also demonstrates greater robustness to noise. Its\neffectiveness across diverse datasets highlights the potential of\nproduct-unit-based architectures for scalable and reliable deep learning in\ncomputer vision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u4e58\u79ef\u5355\u5143\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\uff08PURe\uff09\uff0c\u901a\u8fc7\u5c06\u4e58\u79ef\u5355\u5143\u96c6\u6210\u5230\u6b8b\u5dee\u5757\u4e2d\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53c2\u6570\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6c42\u548c\u795e\u7ecf\u5143\u65e0\u6cd5\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u7279\u5f81\u4ea4\u4e92\uff0c\u800c\u4e58\u79ef\u5355\u5143\u80fd\u591f\u5b9e\u73b0\u4e58\u6cd5\u7279\u5f81\u4ea4\u4e92\uff0c\u4ece\u800c\u66f4\u5f3a\u5927\u5730\u8868\u793a\u590d\u6742\u6a21\u5f0f\u3002", "method": "PURe\u5728\u6b8b\u5dee\u5757\u7684\u7b2c\u4e8c\u5c42\u75282D\u4e58\u79ef\u5355\u5143\u66ff\u6362\u4f20\u7edf\u5377\u79ef\u5c42\uff0c\u5e76\u79fb\u9664\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u4ee5\u4fdd\u7559\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u5728Galaxy10 DECaLS\u3001ImageNet\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cPURe\u5747\u4f18\u4e8e\u6807\u51c6ResNet\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "PURe\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5c55\u793a\u4e86\u4e58\u79ef\u5355\u5143\u67b6\u6784\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.04408", "pdf": "https://arxiv.org/pdf/2505.04408", "abs": "https://arxiv.org/abs/2505.04408", "authors": ["Chengjie Huang", "Krzysztof Czarnecki"], "title": "MFSeg: Efficient Multi-frame 3D Semantic Segmentation", "categories": ["cs.CV"], "comment": "ICRA 2025", "summary": "We propose MFSeg, an efficient multi-frame 3D semantic segmentation\nframework. By aggregating point cloud sequences at the feature level and\nregularizing the feature extraction and aggregation process, MFSeg reduces\ncomputational overhead while maintaining high accuracy. Moreover, by employing\na lightweight MLP-based point decoder, our method eliminates the need to\nupsample redundant points from past frames. Experiments on the nuScenes and\nWaymo datasets show that MFSeg outperforms existing methods, demonstrating its\neffectiveness and efficiency.", "AI": {"tldr": "MFSeg\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u5e273D\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u7ea7\u70b9\u4e91\u5e8f\u5217\u805a\u5408\u548c\u6b63\u5219\u5316\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u5197\u4f59\u70b9\u91c7\u6837\u7684\u95ee\u9898\uff0cMFSeg\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7279\u5f81\u7ea7\u70b9\u4e91\u5e8f\u5217\u805a\u5408\u548c\u6b63\u5219\u5316\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7MLP\u70b9\u89e3\u7801\u5668\uff0c\u907f\u514d\u5197\u4f59\u70b9\u4e0a\u91c7\u6837\u3002", "result": "\u5728nuScenes\u548cWaymo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "MFSeg\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u76843D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u70b9\u4e91\u5e8f\u5217\u5904\u7406\u3002"}}
{"id": "2505.04410", "pdf": "https://arxiv.org/pdf/2505.04410", "abs": "https://arxiv.org/abs/2505.04410", "authors": ["Junjie Wang", "Bin Chen", "Yulin Li", "Bin Kang", "Yichi Chen", "Zhuotao Tian"], "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception", "categories": ["cs.CV"], "comment": null, "summary": "Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at \\textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.", "AI": {"tldr": "DeCLIP\u901a\u8fc7\u89e3\u8026CLIP\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u5bc6\u96c6\u89c6\u89c9\u9884\u6d4b\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u89c6\u89c9\u9884\u6d4b\u4efb\u52a1\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u800cCLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u5c40\u90e8\u7279\u5f81\u8868\u793a\u4e0d\u8db3\u3002", "method": "DeCLIP\u89e3\u8026CLIP\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5206\u522b\u83b7\u53d6\u201c\u5185\u5bb9\u201d\u548c\u201c\u4e0a\u4e0b\u6587\u201d\u7279\u5f81\uff0c\u524d\u8005\u4e0e\u56fe\u50cf\u88c1\u526a\u8868\u793a\u5bf9\u9f50\u4ee5\u63d0\u5347\u5c40\u90e8\u533a\u5206\u6027\uff0c\u540e\u8005\u5728DINO\u7b49\u6a21\u578b\u7684\u6307\u5bfc\u4e0b\u4fdd\u7559\u7a7a\u95f4\u76f8\u5173\u6027\u3002", "result": "DeCLIP\u5728\u591a\u4e2a\u5f00\u653e\u8bcd\u6c47\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DeCLIP\u901a\u8fc7\u6539\u8fdbCLIP\u7684\u5c40\u90e8\u7279\u5f81\u8868\u793a\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04424", "pdf": "https://arxiv.org/pdf/2505.04424", "abs": "https://arxiv.org/abs/2505.04424", "authors": ["Jing Hu", "Chengming Feng", "Shu Hu", "Ming-Ching Chang", "Xin Li", "Xi Wu", "Xin Wang"], "title": "RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation", "categories": ["cs.CV"], "comment": "IJCAI2025", "summary": "Arbitrary style transfer aims to apply the style of any given artistic image\nto another content image. Still, existing deep learning-based methods often\nrequire significant computational costs to generate diverse stylized results.\nMotivated by this, we propose a novel reinforcement learning-based framework\nfor arbitrary style transfer RLMiniStyler. This framework leverages a unified\nreinforcement learning policy to iteratively guide the style transfer process\nby exploring and exploiting stylization feedback, generating smooth sequences\nof stylized results while achieving model lightweight. Furthermore, we\nintroduce an uncertainty-aware multi-task learning strategy that automatically\nadjusts loss weights to adapt to the content and style balance requirements at\ndifferent training stages, thereby accelerating model convergence. Through a\nseries of experiments across image various resolutions, we have validated the\nadvantages of RLMiniStyler over other state-of-the-art methods in generating\nhigh-quality, diverse artistic image sequences at a lower cost. Codes are\navailable at https://github.com/fengxiaoming520/RLMiniStyler.", "AI": {"tldr": "RLMiniStyler\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u4efb\u610f\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u7684\u827a\u672f\u56fe\u50cf\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u751f\u6210\u591a\u6837\u5316\u98ce\u683c\u5316\u7ed3\u679c\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8fed\u4ee3\u6307\u5bfc\u98ce\u683c\u8fc1\u79fb\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\u3002", "result": "\u5728\u591a\u79cd\u56fe\u50cf\u5206\u8fa8\u7387\u4e0b\u9a8c\u8bc1\u4e86RLMiniStyler\u5728\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u4f18\u52bf\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "RLMiniStyler\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u4efb\u610f\u98ce\u683c\u8fc1\u79fb\u3002"}}
{"id": "2505.04460", "pdf": "https://arxiv.org/pdf/2505.04460", "abs": "https://arxiv.org/abs/2505.04460", "authors": ["Ming-Hui Liu", "Harry Cheng", "Tianyi Wang", "Xin Luo", "Xin-Shun Xu"], "title": "Learning Real Facial Concepts for Independent Deepfake Detection", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Deepfake detection models often struggle with generalization to unseen\ndatasets, manifesting as misclassifying real instances as fake in target\ndomains. This is primarily due to an overreliance on forgery artifacts and a\nlimited understanding of real faces. To address this challenge, we propose a\nnovel approach RealID to enhance generalization by learning a comprehensive\nconcept of real faces while assessing the probabilities of belonging to the\nreal and fake classes independently. RealID comprises two key modules: the Real\nConcept Capture Module (RealC2) and the Independent Dual-Decision Classifier\n(IDC). With the assistance of a MultiReal Memory, RealC2 maintains various\nprototypes for real faces, allowing the model to capture a comprehensive\nconcept of real class. Meanwhile, IDC redefines the classification strategy by\nmaking independent decisions based on the concept of the real class and the\npresence of forgery artifacts. Through the combined effect of the above\nmodules, the influence of forgery-irrelevant patterns is alleviated, and\nextensive experiments on five widely used datasets demonstrate that RealID\nsignificantly outperforms existing state-of-the-art methods, achieving a 1.74%\nimprovement in average accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRealID\u65b9\u6cd5\uff0c\u901a\u8fc7\u72ec\u7acb\u8bc4\u4f30\u771f\u5b9e\u548c\u4f2a\u9020\u7c7b\u522b\u7684\u6982\u7387\uff0c\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e3b\u8981\u4f9d\u8d56\u4f2a\u9020\u75d5\u8ff9\u4e14\u5bf9\u771f\u5b9e\u4eba\u8138\u7406\u89e3\u4e0d\u8db3\u3002", "method": "RealID\u5305\u542bRealC2\u6a21\u5757\uff08\u6355\u83b7\u771f\u5b9e\u4eba\u8138\u6982\u5ff5\uff09\u548cIDC\u6a21\u5757\uff08\u72ec\u7acb\u51b3\u7b56\u5206\u7c7b\uff09\uff0c\u7ed3\u5408MultiReal Memory\u5b58\u50a8\u771f\u5b9e\u4eba\u8138\u539f\u578b\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cRealID\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53471.74%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RealID\u901a\u8fc7\u589e\u5f3a\u5bf9\u771f\u5b9e\u4eba\u8138\u7684\u7406\u89e3\u548c\u72ec\u7acb\u5206\u7c7b\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.04481", "pdf": "https://arxiv.org/pdf/2505.04481", "abs": "https://arxiv.org/abs/2505.04481", "authors": ["Jiahao Li", "Weijian Ma", "Xueyang Li", "Yunzhong Lou", "Guichun Zhou", "Xiangdong Zhou"], "title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCAD-Llama\u7684\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u53c2\u6570\u53163D CAD\u6a21\u578b\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5f00\u53d1\u5206\u5c42\u6ce8\u91ca\u7ba1\u9053\u548c\u4ee3\u7801\u5316\u683c\u5f0f\uff0c\u5c06\u53c2\u6570\u5316CAD\u547d\u4ee4\u5e8f\u5217\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u53c2\u6570\u5316CAD\u4ee3\u7801\uff08SPCC\uff09\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u901a\u7528\u6587\u672c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u53c2\u6570\u53163D CAD\u6a21\u578b\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3aLLMs\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u672a\u63a5\u89e6\u53c2\u6570\u5316\u5e8f\u5217\u4e14\u7f3a\u4e4f\u5bf93D\u7ed3\u6784\u7684\u76f4\u63a5\u8ba4\u77e5\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u5206\u5c42\u6ce8\u91ca\u7ba1\u9053\u548cSPCC\u683c\u5f0f\uff0c\u5c06CAD\u547d\u4ee4\u5e8f\u5217\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u4ee3\u7801\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3aLLMs\u7684\u7a7a\u95f4\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCAD-Llama\u6846\u67b6\u5728\u751f\u6210\u53c2\u6570\u53163D CAD\u6a21\u578b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u56de\u5f52\u65b9\u6cd5\u548cLLM\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLMs\u5728\u53c2\u6570\u53163D\u5f62\u72b6\u751f\u6210\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u521d\u6b65\u63a2\u7d22\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u7ed3\u6784\u5316\u4ee3\u7801\u548c\u9488\u5bf9\u6027\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.04485", "pdf": "https://arxiv.org/pdf/2505.04485", "abs": "https://arxiv.org/abs/2505.04485", "authors": ["Ali Alawieh", "Alexandru P. Condurache"], "title": "FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging", "categories": ["cs.CV"], "comment": "8 pages, 2 figures, accepted at IJCNN 2025", "summary": "We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural\nnetwork architecture built on top of the well-known KPConv, a widely adopted\nbackbone for 3D point cloud analysis. Even though invariance and/or\nequivariance to Euclidean transformations are required for many common tasks,\nKPConv-based networks can only approximately achieve such properties when\ntraining on large datasets or with significant data augmentations. Using Frame\nAveraging, we allow to flexibly customize point cloud neural networks built\nwith KPConv layers, by making them exactly invariant and/or equivariant to\ntranslations, rotations and/or reflections of the input point clouds. By simply\nwrapping around an existing KPConv-based network, FA-KPConv embeds geometrical\nprior knowledge into it while preserving the number of learnable parameters and\nnot compromising any input information. We showcase the benefit of such an\nintroduced bias for point cloud classification and point cloud registration,\nespecially in challenging cases such as scarce training data or randomly\nrotated test data.", "AI": {"tldr": "FA-KPConv\u662f\u4e00\u79cd\u57fa\u4e8eKPConv\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5e27\u5e73\u5747\u6280\u672f\u5b9e\u73b0\u70b9\u4e91\u7f51\u7edc\u7684\u7cbe\u786e\u4e0d\u53d8\u6027\u548c/\u6216\u7b49\u53d8\u6027\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u6216\u65cb\u8f6c\u6d4b\u8bd5\u6570\u636e\u7b49\u6311\u6218\u6027\u573a\u666f\u3002", "motivation": "KPConv\u57283D\u70b9\u4e91\u5206\u6790\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u6b27\u51e0\u91cc\u5f97\u53d8\u6362\u7684\u4e0d\u53d8\u6027\u548c\u7b49\u53d8\u6027\u4ec5\u80fd\u901a\u8fc7\u5927\u6570\u636e\u96c6\u6216\u6570\u636e\u589e\u5f3a\u8fd1\u4f3c\u5b9e\u73b0\u3002FA-KPConv\u65e8\u5728\u901a\u8fc7\u5e27\u5e73\u5747\u6280\u672f\u7cbe\u786e\u5b9e\u73b0\u8fd9\u4e9b\u6027\u8d28\u3002", "method": "FA-KPConv\u901a\u8fc7\u5e27\u5e73\u5747\u6280\u672f\u5305\u88c5\u73b0\u6709\u7684KPConv\u7f51\u7edc\uff0c\u4f7f\u5176\u5bf9\u70b9\u4e91\u7684\u5e73\u79fb\u3001\u65cb\u8f6c\u548c\u53cd\u5c04\u5177\u6709\u7cbe\u786e\u7684\u4e0d\u53d8\u6027\u548c/\u6216\u7b49\u53d8\u6027\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u53ef\u5b66\u4e60\u53c2\u6570\u6216\u635f\u5931\u8f93\u5165\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFA-KPConv\u5728\u70b9\u4e91\u5206\u7c7b\u548c\u914d\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u6216\u6d4b\u8bd5\u6570\u636e\u968f\u673a\u65cb\u8f6c\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "FA-KPConv\u901a\u8fc7\u5d4c\u5165\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86KPConv\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2505.04486", "pdf": "https://arxiv.org/pdf/2505.04486", "abs": "https://arxiv.org/abs/2505.04486", "authors": ["Anirban Samaddar", "Yixuan Sun", "Viktor Nilsson", "Sandeep Madireddy"], "title": "Efficient Flow Matching using Latent Variables", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLatent-CFM\u7684\u6d41\u5339\u914d\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u9690\u53d8\u91cf\u6a21\u578b\u7b80\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u6d41\u5339\u914d\u6a21\u578b\u5728\u4ece\u7b80\u5355\u6e90\u5206\u5e03\uff08\u5982\u6807\u51c6\u9ad8\u65af\u5206\u5e03\uff09\u5b66\u4e60\u6d41\u65f6\uff0c\u672a\u663e\u5f0f\u5efa\u6a21\u76ee\u6807\u6570\u636e\u7684\u5e95\u5c42\u7ed3\u6784/\u6d41\u5f62\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u6570\u636e\u96c6\u4e2d\u3002", "method": "\u63d0\u51faLatent-CFM\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u9690\u53d8\u91cf\u6a21\u578b\u7b80\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\uff0c\u4ee5\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u7ed3\u6784\u548c\u4f4e\u7ef4\u6d41\u5f62\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLatent-CFM\u5728\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6d41\u5339\u914d\u6a21\u578b\uff0c\u8bad\u7ec3\u6210\u672c\u51cf\u5c11\u7ea650%\uff0c\u5e76\u751f\u6210\u66f4\u51c6\u786e\u7684\u7269\u7406\u6837\u672c\u3002", "conclusion": "Latent-CFM\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6570\u636e\u6d41\u5f62\u548c\u591a\u6a21\u6001\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.04488", "pdf": "https://arxiv.org/pdf/2505.04488", "abs": "https://arxiv.org/abs/2505.04488", "authors": ["Ziyi Zhang", "Zhen Sun", "Zongmin Zhang", "Zifan Peng", "Yuemeng Zhao", "Zichun Wang", "Zeren Luo", "Ruiting Zuo", "Xinlei He"], "title": "\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "comment": "12 pages, 6 figures", "summary": "The visually impaired population, especially the severely visually impaired,\nis currently large in scale, and daily activities pose significant challenges\nfor them. Although many studies use large language and vision-language models\nto assist the blind, most focus on static content and fail to meet real-time\nperception needs in dynamic and complex environments, such as daily activities.\nTo provide them with more effective intelligent assistance, it is imperative to\nincorporate advanced visual understanding technologies. Although real-time\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\nunderstanding, no prior work has systematically evaluated their effectiveness\nin assisting visually impaired individuals. In this work, we conduct the first\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\ncovering three categories of assistive tasks for visually impaired individuals:\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\nevaluate the models in both closed-world and open-world scenarios, further\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\nOne key issue we identify is the difficulty current models face in perceiving\npotential hazards in dynamic environments. To address this, we build an\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\nthat enables the model to proactively detect environmental risks. We hope this\nwork provides valuable insights and inspiration for future research in this\nfield.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86VideoLLMs\u5728\u8f85\u52a9\u89c6\u969c\u4eba\u58eb\u4e2d\u7684\u6548\u679c\uff0c\u6784\u5efa\u4e86VisAssistDaily\u548cSafeVid\u6570\u636e\u96c6\uff0c\u53d1\u73b0GPT-4o\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63d0\u51fa\u73af\u5883\u98ce\u9669\u4e3b\u52a8\u68c0\u6d4b\u673a\u5236\u3002", "motivation": "\u89c6\u969c\u4eba\u58eb\u5728\u52a8\u6001\u590d\u6742\u73af\u5883\u4e2d\u7f3a\u4e4f\u5b9e\u65f6\u611f\u77e5\u652f\u6301\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u9759\u6001\u5185\u5bb9\uff0c\u9700\u7ed3\u5408\u5148\u8fdb\u89c6\u89c9\u7406\u89e3\u6280\u672f\u63d0\u4f9b\u66f4\u6709\u6548\u5e2e\u52a9\u3002", "method": "\u6784\u5efaVisAssistDaily\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e09\u7c7b\u8f85\u52a9\u4efb\u52a1\uff1b\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u6a21\u578b\u5728\u5c01\u95ed\u548c\u5f00\u653e\u573a\u666f\u7684\u8868\u73b0\uff1b\u63d0\u51faSafeVid\u6570\u636e\u96c6\u548c\u8f6e\u8be2\u673a\u5236\u4ee5\u68c0\u6d4b\u73af\u5883\u98ce\u9669\u3002", "result": "GPT-4o\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u8868\u73b0\u6700\u4f73\uff1b\u5f53\u524d\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u611f\u77e5\u6f5c\u5728\u5371\u9669\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u7814\u7a76\u4e3aVideoLLMs\u5728\u8f85\u52a9\u89c6\u969c\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u542f\u53d1\u3002"}}
{"id": "2505.04497", "pdf": "https://arxiv.org/pdf/2505.04497", "abs": "https://arxiv.org/abs/2505.04497", "authors": ["Aditi Ramaswamy"], "title": "Defining and Quantifying Creative Behavior in Popular Image Generators", "categories": ["cs.CV", "cs.AI", "I.4.m; I.2.m"], "comment": null, "summary": "Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5b9e\u7528\u89d2\u5ea6\u8bc4\u4f30\u751f\u6210\u5f0fAI\u6a21\u578b\u521b\u9020\u529b\u7684\u5b9a\u91cf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4e0e\u4eba\u7c7b\u76f4\u89c9\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u521b\u9020\u529b\u4e00\u76f4\u662f\u79d1\u5b66\u4e89\u8bae\u7684\u7126\u70b9\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\uff0c\u5e2e\u52a9\u7528\u6237\u6839\u636e\u4efb\u52a1\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u3002", "method": "\u5f15\u5165\u5b9a\u91cf\u6307\u6807\u8bc4\u4f30\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u521b\u9020\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u6d41\u884c\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u5b9a\u91cf\u6307\u6807\u4e0e\u4eba\u7c7b\u76f4\u89c9\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u7684\u5b9a\u91cf\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u521b\u9020\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7528\u6237\u9009\u62e9\u9002\u5408\u4efb\u52a1\u7684\u6a21\u578b\u3002"}}
{"id": "2505.04502", "pdf": "https://arxiv.org/pdf/2505.04502", "abs": "https://arxiv.org/abs/2505.04502", "authors": ["Asma Baobaid", "Mahmoud Meribout"], "title": "Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition", "categories": ["cs.CV", "cs.AR", "eess.IV"], "comment": "10 pages, 11 figures", "summary": "Video face detection and recognition in public places at the edge is required\nin several applications, such as security reinforcement and contactless access\nto authorized venues. This paper aims to maximize the simultaneous usage of\nhardware engines available in edge GPUs nowadays by leveraging the concurrency\nand pipelining of tasks required for face detection and recognition. This also\nincludes the video decoding task, which is required in most face monitoring\napplications as the video streams are usually carried via Gbps Ethernet\nnetwork. This constitutes an improvement over previous works where the tasks\nare usually allocated to a single engine due to the lack of a unified and\nautomated framework that simultaneously explores all hardware engines. In\naddition, previously, the input faces were usually embedded in still images or\nwithin raw video streams that overlook the burst delay caused by the decoding\nstage. The results on real-life video streams suggest that simultaneously using\nall the hardware engines available in the recent NVIDIA edge Orin GPU, higher\nthroughput, and a slight saving of power consumption of around 300 mW,\naccounting for around 5%, have been achieved while satisfying the real-time\nperformance constraint. The performance gets even higher by considering several\nvideo streams simultaneously. Further performance improvement could have been\nobtained if the number of shuffle layers that were created by the tensor RT\nframework for the face recognition task was lower. Thus, the paper suggests\nsome hardware improvements to the existing edge GPU processors to enhance their\nperformance even higher.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8fb9\u7f18GPU\u4e0a\u6700\u5927\u5316\u5229\u7528\u786c\u4ef6\u5f15\u64ce\u5e76\u53d1\u548c\u6d41\u6c34\u7ebf\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u89c6\u9891\u4eba\u8138\u68c0\u6d4b\u548c\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u529f\u8017\u3002", "motivation": "\u516c\u5171\u573a\u5408\u7684\u89c6\u9891\u4eba\u8138\u68c0\u6d4b\u548c\u8bc6\u522b\u5728\u5b89\u5168\u548c\u65e0\u63a5\u89e6\u8bbf\u95ee\u7b49\u5e94\u7528\u4e2d\u9700\u6c42\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u786c\u4ef6\u5f15\u64ce\u7684\u5e76\u53d1\u6027\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u81ea\u52a8\u5316\u6846\u67b6\u540c\u65f6\u5229\u7528\u8fb9\u7f18GPU\u7684\u6240\u6709\u786c\u4ef6\u5f15\u64ce\uff0c\u5305\u62ec\u89c6\u9891\u89e3\u7801\u3001\u4eba\u8138\u68c0\u6d4b\u548c\u8bc6\u522b\u4efb\u52a1\uff0c\u4f18\u5316\u4efb\u52a1\u5206\u914d\u3002", "result": "\u5728NVIDIA Orin\u8fb9\u7f18GPU\u4e0a\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3001\u5b9e\u65f6\u6027\u80fd\uff0c\u5e76\u8282\u7701\u7ea65%\u7684\u529f\u8017\uff08300 mW\uff09\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u4e86\u5e76\u53d1\u5229\u7528\u786c\u4ef6\u5f15\u64ce\u7684\u4f18\u52bf\uff0c\u5e76\u5efa\u8bae\u8fdb\u4e00\u6b65\u4f18\u5316\u786c\u4ef6\u8bbe\u8ba1\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.04512", "pdf": "https://arxiv.org/pdf/2505.04512", "abs": "https://arxiv.org/abs/2505.04512", "authors": ["Teng Hu", "Zhentao Yu", "Zhengguang Zhou", "Sen Liang", "Yuan Zhou", "Qin Lin", "Qinglin Lu"], "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.", "AI": {"tldr": "HunyuanCustom\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5b9a\u5236\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u548c\u6587\u672c\u6761\u4ef6\uff0c\u5f3a\u8c03\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u5e76\u5728ID\u4e00\u81f4\u6027\u3001\u771f\u5b9e\u6027\u548c\u6587\u672c-\u89c6\u9891\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u8f93\u5165\u6a21\u6001\u591a\u6837\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0cHunyuanCustom\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8eHunyuanVideo\uff0c\u5f15\u5165\u6587\u672c-\u56fe\u50cf\u878d\u5408\u6a21\u5757\u548c\u56fe\u50cfID\u589e\u5f3a\u6a21\u5757\uff1b\u9488\u5bf9\u97f3\u9891\u548c\u89c6\u9891\u6761\u4ef6\uff0c\u63d0\u51faAudioNet\u548c\u89c6\u9891\u9a71\u52a8\u6ce8\u5165\u6a21\u5757\u3002", "result": "\u5728\u5355\u4e3b\u4f53\u548c\u591a\u4e3b\u4f53\u573a\u666f\u4e2d\uff0cHunyuanCustom\u5728ID\u4e00\u81f4\u6027\u3001\u771f\u5b9e\u6027\u548c\u6587\u672c-\u89c6\u9891\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u6761\u4ef6\u548c\u8eab\u4efd\u4fdd\u6301\u7b56\u7565\u6709\u6548\u63a8\u52a8\u4e86\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7684\u8fdb\u5c55\u3002"}}
{"id": "2505.04522", "pdf": "https://arxiv.org/pdf/2505.04522", "abs": "https://arxiv.org/abs/2505.04522", "authors": ["Pengfei Guo", "Can Zhao", "Dong Yang", "Yufan He", "Vishwesh Nath", "Ziyue Xu", "Pedro R. A. S. Bassi", "Zongwei Zhou", "Benjamin D. Simon", "Stephanie Anne Harmon", "Baris Turkbey", "Daguang Xu"], "title": "Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Generating 3D CT volumes from descriptive free-text inputs presents a\ntransformative opportunity in diagnostics and research. In this paper, we\nintroduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual\ndescriptions using the diffusion model. Unlike previous methods that rely on\nfixed-format text input, Text2CT employs a novel prompt formulation that\nenables generation from diverse, free-text descriptions. The proposed framework\nencodes medical text into latent representations and decodes them into\nhigh-resolution 3D CT scans, effectively bridging the gap between semantic text\ninputs and detailed volumetric representations in a unified 3D framework. Our\nmethod demonstrates superior performance in preserving anatomical fidelity and\ncapturing intricate structures as described in the input text. Extensive\nevaluations show that our approach achieves state-of-the-art results, offering\npromising potential applications in diagnostics, and data augmentation.", "AI": {"tldr": "Text2CT\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u81ea\u7531\u6587\u672c\u63cf\u8ff0\u751f\u62103D CT\u4f53\u79ef\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u81ea\u7531\u6587\u672c\u751f\u62103D CT\u4f53\u79ef\uff0c\u4e3a\u8bca\u65ad\u548c\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\uff0c\u5c06\u533b\u5b66\u6587\u672c\u7f16\u7801\u4e3a\u6f5c\u5728\u8868\u793a\u5e76\u89e3\u7801\u4e3a\u9ad8\u5206\u8fa8\u73873D CT\u626b\u63cf\u3002", "result": "\u5728\u4fdd\u6301\u89e3\u5256\u5b66\u4fdd\u771f\u5ea6\u548c\u6355\u6349\u590d\u6742\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "Text2CT\u5728\u8bca\u65ad\u548c\u6570\u636e\u589e\u5f3a\u65b9\u9762\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2505.04524", "pdf": "https://arxiv.org/pdf/2505.04524", "abs": "https://arxiv.org/abs/2505.04524", "authors": ["Asma Baobaid", "Mahmoud Meribout"], "title": "Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration", "categories": ["cs.CV", "cs.AR", "cs.LG", "eess.IV"], "comment": "10 pages, 12 figures", "summary": "Cost-effective machine vision systems dedicated to real-time and accurate\nface detection and recognition in public places are crucial for many modern\napplications. However, despite their high performance, which could be reached\nusing specialized edge or cloud AI hardware accelerators, there is still room\nfor improvement in throughput and power consumption. This paper aims to suggest\na combined hardware-software approach that optimizes face detection and\nrecognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX\nOrin. First, it leverages the simultaneous usage of all its hardware engines to\nimprove processing time. This offers an improvement over previous works where\nthese tasks were mainly allocated automatically and exclusively to the CPU or,\nto a higher extent, to the GPU core. Additionally, the paper suggests\nintegrating a face tracker module to avoid redundantly running the face\nrecognition algorithm for every frame but only when a new face appears in the\nscene. The results of extended experiments suggest that simultaneous usage of\nall the hardware engines that are available in the Orin GPU and tracker\nintegration into the pipeline yield an impressive throughput of 290 FPS (frames\nper second) on 1920 x 1080 input size frames containing in average of 6\nfaces/frame. Additionally, a substantial saving of power consumption of around\n800 mW was achieved when compared to running the task on the CPU/GPU engines\nonly and without integrating a tracker into the Orin GPU\\'92s pipeline. This\nhardware-codesign approach can pave the way to design high-performance machine\nvision systems at the edge, critically needed in video monitoring in public\nplaces where several nearby cameras are usually deployed for a same scene.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u786c\u4ef6-\u8f6f\u4ef6\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u5229\u7528NVIDIA Jetson AGX Orin\u7684\u8fb9\u7f18GPU\uff0c\u901a\u8fc7\u540c\u65f6\u4f7f\u7528\u6240\u6709\u786c\u4ef6\u5f15\u64ce\u548c\u96c6\u6210\u4eba\u8138\u8ddf\u8e2a\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u8138\u68c0\u6d4b\u4e0e\u8bc6\u522b\u7684\u541e\u5410\u91cf\u548c\u80fd\u6548\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709AI\u786c\u4ef6\u52a0\u901f\u5668\u5728\u516c\u5171\u573a\u5408\u7684\u4eba\u8138\u68c0\u6d4b\u4e0e\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u541e\u5410\u91cf\u548c\u529f\u8017\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u4f18\u5316\u8fd9\u4e9b\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u540c\u65f6\u4f7f\u7528Orin GPU\u7684\u6240\u6709\u786c\u4ef6\u5f15\u64ce\uff0c\u5e76\u96c6\u6210\u4eba\u8138\u8ddf\u8e2a\u6a21\u5757\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u4f18\u5316\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u57281920x1080\u5206\u8fa8\u7387\u4e0b\u8fbe\u5230290 FPS\uff0c\u540c\u65f6\u8282\u7701\u7ea6800 mW\u529f\u8017\u3002", "conclusion": "\u8fd9\u79cd\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u4e3a\u9ad8\u6027\u80fd\u8fb9\u7f18\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u516c\u5171\u573a\u5408\u7684\u591a\u6444\u50cf\u5934\u76d1\u63a7\u573a\u666f\u3002"}}
{"id": "2505.04526", "pdf": "https://arxiv.org/pdf/2505.04526", "abs": "https://arxiv.org/abs/2505.04526", "authors": ["Qi Zhou", "Yukai Shi", "Xiaojun Yang", "Xiaoyu Xian", "Lunjia Liao", "Ruimao Zhang", "Liang Lin"], "title": "DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visible and infrared image fusion is one of the most crucial tasks in the\nfield of image fusion, aiming to generate fused images with clear structural\ninformation and high-quality texture features for high-level vision tasks.\nHowever, when faced with severe illumination degradation in visible images, the\nfusion results of existing image fusion methods often exhibit blurry and dim\nvisual effects, posing major challenges for autonomous driving. To this end, a\nDarkness-Free network is proposed to handle Visible and infrared image\ndisentanglement and fusion all at Once (DFVO), which employs a cascaded\nmulti-task approach to replace the traditional two-stage cascaded training\n(enhancement and fusion), addressing the issue of information entropy loss\ncaused by hierarchical data transmission. Specifically, we construct a\nlatent-common feature extractor (LCFE) to obtain latent features for the\ncascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised\nto acquire high-frequency semantic information. Secondly, we design a hyper\ncross-attention module (HCAM) to extract low-frequency information and preserve\ntexture features from source images. Finally, a relevant loss function is\ndesigned to guide the holistic network learning, thereby achieving better image\nfusion. Extensive experiments demonstrate that our proposed approach\noutperforms state-of-the-art alternatives in terms of qualitative and\nquantitative evaluations. Particularly, DFVO can generate clearer, more\ninformative, and more evenly illuminated fusion results in the dark\nenvironments, achieving best performance on the LLVIP dataset with 63.258 dB\nPSNR and 0.724 CC, providing more effective information for high-level vision\ntasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDFVO\u7684\u7f51\u7edc\uff0c\u7528\u4e8e\u5728\u9ed1\u6697\u73af\u5883\u4e0b\u5b9e\u73b0\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\u7684\u89e3\u8026\u4e0e\u878d\u5408\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u7ea7\u8054\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u8bad\u7ec3\u7684\u4fe1\u606f\u71b5\u635f\u5931\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u5728\u53ef\u89c1\u5149\u56fe\u50cf\u5149\u7167\u4e0d\u8db3\u65f6\uff0c\u878d\u5408\u7ed3\u679c\u6a21\u7cca\u4e14\u6697\u6de1\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u7b49\u9ad8\u5c42\u6b21\u89c6\u89c9\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u7ea7\u8054\u591a\u4efb\u52a1\u7b56\u7565\uff0c\u5305\u62ec\u6f5c\u5728\u5171\u540c\u7279\u5f81\u63d0\u53d6\u5668\uff08LCFE\uff09\u3001\u7ec6\u8282\u63d0\u53d6\u6a21\u5757\uff08DEM\uff09\u548c\u8d85\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff08HCAM\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u76f8\u5173\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728LLVIP\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff08PSNR 63.258 dB\uff0cCC 0.724\uff09\uff0c\u751f\u6210\u66f4\u6e05\u6670\u3001\u4fe1\u606f\u66f4\u4e30\u5bcc\u4e14\u5149\u7167\u5747\u5300\u7684\u878d\u5408\u56fe\u50cf\u3002", "conclusion": "DFVO\u5728\u9ed1\u6697\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u878d\u5408\u8d28\u91cf\uff0c\u4e3a\u9ad8\u5c42\u6b21\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u4fe1\u606f\u3002"}}
{"id": "2505.04529", "pdf": "https://arxiv.org/pdf/2505.04529", "abs": "https://arxiv.org/abs/2505.04529", "authors": ["Edward Humes", "Xiaomin Lin", "Uttej Kallakuri", "Tinoosh Mohsenin"], "title": "RAFT: Robust Augmentation of FeaTures for Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU.", "AI": {"tldr": "RAFT\u6846\u67b6\u901a\u8fc7\u6570\u636e\u548c\u7279\u5f81\u589e\u5f3a\u4ee5\u53ca\u4e3b\u52a8\u5b66\u4e60\uff0c\u5229\u7528\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u63d0\u5347\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff08Syn2Real\u95ee\u9898\uff09\u3002", "method": "\u63d0\u51faRAFT\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u548c\u7279\u5f81\u589e\u5f3a\u4ee5\u53ca\u4e3b\u52a8\u5b66\u4e60\uff0c\u5229\u7528\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728SYNTHIA->Cityscapes\u548cGTAV->Cityscapes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cmIoU\u5206\u522b\u63d0\u53472.1%/79.9%\u548c0.4%/78.2%\uff1b\u5728Cityscapes->ACDC\u6d4b\u8bd5\u4e2d\u63d0\u53471.3%/73.2%\u3002", "conclusion": "RAFT\u6709\u6548\u7f13\u89e3\u4e86Syn2Real\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.04540", "pdf": "https://arxiv.org/pdf/2505.04540", "abs": "https://arxiv.org/abs/2505.04540", "authors": ["Ashutosh Singandhupe", "Sanket Lokhande", "Hung Manh La"], "title": "Registration of 3D Point Sets Using Exponential-based Similarity Matrix", "categories": ["cs.CV"], "comment": null, "summary": "Point cloud registration is a fundamental problem in computer vision and\nrobotics, involving the alignment of 3D point sets captured from varying\nviewpoints using depth sensors such as LiDAR or structured light. In modern\nrobotic systems, especially those focused on mapping, it is essential to merge\nmultiple views of the same environment accurately. However, state-of-the-art\nregistration techniques often struggle when large rotational differences exist\nbetween point sets or when the data is significantly corrupted by sensor noise.\nThese challenges can lead to misalignments and, consequently, to inaccurate or\ndistorted 3D reconstructions. In this work, we address both these limitations\nby proposing a robust modification to the classic Iterative Closest Point (ICP)\nalgorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),\nintegrates a Gaussian-inspired exponential weighting scheme to construct a\nsimilarity matrix that dynamically adapts across iterations. This matrix\nfacilitates improved estimation of both rotational and translational components\nduring alignment. We demonstrate the robustness of ESM-ICP in two challenging\nscenarios: (i) large rotational discrepancies between the source and target\npoint clouds, and (ii) data corrupted by non-Gaussian noise. Our results show\nthat ESM-ICP outperforms traditional geometric registration techniques as well\nas several recent learning-based methods. To encourage reproducibility and\ncommunity engagement, our full implementation is made publicly available on\nGitHub. https://github.com/aralab-unr/ESM_ICP", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684ICP\u7b97\u6cd5\uff08ESM-ICP\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u76f8\u4f3c\u6027\u77e9\u9635\uff0c\u89e3\u51b3\u4e86\u70b9\u4e91\u914d\u51c6\u4e2d\u65cb\u8f6c\u5dee\u5f02\u5927\u548c\u566a\u58f0\u5e72\u6270\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u914d\u51c6\u6280\u672f\u5728\u5927\u65cb\u8f6c\u5dee\u5f02\u6216\u566a\u58f0\u5e72\u6270\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f43D\u91cd\u5efa\u4e0d\u51c6\u786e\u3002", "method": "\u5f15\u5165\u9ad8\u65af\u542f\u53d1\u7684\u6307\u6570\u52a0\u6743\u65b9\u6848\uff0c\u6784\u5efa\u52a8\u6001\u76f8\u4f3c\u6027\u77e9\u9635\uff0c\u6539\u8fdb\u65cb\u8f6c\u548c\u5e73\u79fb\u4f30\u8ba1\u3002", "result": "ESM-ICP\u5728\u5927\u65cb\u8f6c\u5dee\u5f02\u548c\u975e\u9ad8\u65af\u566a\u58f0\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u548c\u90e8\u5206\u5b66\u4e60\u578b\u65b9\u6cd5\u3002", "conclusion": "ESM-ICP\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u914d\u51c6\u7684\u9c81\u68d2\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.04575", "pdf": "https://arxiv.org/pdf/2505.04575", "abs": "https://arxiv.org/abs/2505.04575", "authors": ["Kunlun Xu", "Xu Zou", "Gang Hua", "Jiahuan Zhou"], "title": "Componential Prompt-Knowledge Alignment for Domain Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accpted by ICML2025", "summary": "Domain Incremental Learning (DIL) aims to learn from non-stationary data\nstreams across domains while retaining and utilizing past knowledge. Although\nprompt-based methods effectively store multi-domain knowledge in prompt\nparameters and obtain advanced performance through cross-domain prompt fusion,\nwe reveal an intrinsic limitation: component-wise misalignment between\ndomain-specific prompts leads to conflicting knowledge integration and degraded\npredictions. This arises from the random positioning of knowledge components\nwithin prompts, where irrelevant component fusion introduces interference.To\naddress this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a\nnovel prompt-based DIL method that introduces component-aware prompt-knowledge\nalignment during training, significantly improving both the learning and\ninference capacity of the model. KA-Prompt operates in two phases: (1) Initial\nComponential Structure Configuring, where a set of old prompts containing\nknowledge relevant to the new domain are mined via greedy search, which is then\nexploited to initialize new prompts to achieve reusable knowledge transfer and\nestablish intrinsic alignment between new and old prompts. (2) Online Alignment\nPreservation, which dynamically identifies the target old prompts and applies\nadaptive componential consistency constraints as new prompts evolve. Extensive\nexperiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.\nOur source code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-KA-Prompt", "AI": {"tldr": "KA-Prompt\u901a\u8fc7\u7ec4\u4ef6\u611f\u77e5\u7684\u63d0\u793a-\u77e5\u8bc6\u5bf9\u9f50\u89e3\u51b3\u4e86\u9886\u57df\u589e\u91cf\u5b66\u4e60\u4e2d\u63d0\u793a\u7ec4\u4ef6\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63ed\u793a\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u9886\u57df\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0c\u63d0\u793a\u7ec4\u4ef6\u4e0d\u5bf9\u9f50\u5bfc\u81f4\u77e5\u8bc6\u51b2\u7a81\u548c\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faKA-Prompt\uff0c\u5206\u4e24\u9636\u6bb5\uff1a1) \u521d\u59cb\u7ec4\u4ef6\u7ed3\u6784\u914d\u7f6e\uff0c\u901a\u8fc7\u8d2a\u5a6a\u641c\u7d22\u521d\u59cb\u5316\u65b0\u63d0\u793a\uff1b2) \u5728\u7ebf\u5bf9\u9f50\u4fdd\u6301\uff0c\u52a8\u6001\u7ea6\u675f\u65b0\u65e7\u63d0\u793a\u7684\u7ec4\u4ef6\u4e00\u81f4\u6027\u3002", "result": "\u5728\u9886\u57df\u589e\u91cf\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86KA-Prompt\u7684\u6709\u6548\u6027\u3002", "conclusion": "KA-Prompt\u901a\u8fc7\u7ec4\u4ef6\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b66\u4e60\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.04586", "pdf": "https://arxiv.org/pdf/2505.04586", "abs": "https://arxiv.org/abs/2505.04586", "authors": ["Yuning Du", "Jingshuai Liu", "Rohan Dharmakumar", "Sotirios A. Tsaftaris"], "title": "Active Sampling for MRI-based Sequential Decision Making", "categories": ["cs.CV", "cs.LG"], "comment": "Under Review", "summary": "Despite the superior diagnostic capability of Magnetic Resonance Imaging\n(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and\ncomplexity. To enable such a future by reducing the magnetic field strength,\none key approach will be to improve sampling strategies. Previous work has\nshown that it is possible to make diagnostic decisions directly from k-space\nwith fewer samples. Such work shows that single diagnostic decisions can be\nmade, but if we aspire to see MRI as a true PoC, multiple and sequential\ndecisions are necessary while minimizing the number of samples acquired. We\npresent a novel multi-objective reinforcement learning framework enabling\ncomprehensive, sequential, diagnostic evaluation from undersampled k-space\ndata. Our approach during inference actively adapts to sequential decisions to\noptimally sample. To achieve this, we introduce a training methodology that\nidentifies the samples that contribute the best to each diagnostic objective\nusing a step-wise weighting reward function. We evaluate our approach in two\nsequential knee pathology assessment tasks: ACL sprain detection and cartilage\nthickness loss assessment. Our framework achieves diagnostic performance\ncompetitive with various policy-based benchmarks on disease detection, severity\nquantification, and overall sequential diagnosis, while substantially saving\nk-space samples. Our approach paves the way for the future of MRI as a\ncomprehensive and affordable PoC device. Our code is publicly available at\nhttps://github.com/vios-s/MRI_Sequential_Active_Sampling", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6b20\u91c7\u6837\u7684k\u7a7a\u95f4\u6570\u636e\u4e2d\u8fdb\u884c\u5168\u9762\u3001\u8fde\u7eed\u7684\u8bca\u65ad\u8bc4\u4f30\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u91c7\u6837\u9700\u6c42\u3002", "motivation": "\u5c3d\u7ba1MRI\u5177\u6709\u5353\u8d8a\u7684\u8bca\u65ad\u80fd\u529b\uff0c\u4f46\u5176\u4f5c\u4e3a\u5373\u65f6\u8bca\u65ad\u8bbe\u5907\uff08PoC\uff09\u7684\u5e94\u7528\u53d7\u5230\u9ad8\u6210\u672c\u548c\u590d\u6742\u6027\u7684\u9650\u5236\u3002\u901a\u8fc7\u964d\u4f4e\u78c1\u573a\u5f3a\u5ea6\u5e76\u6539\u8fdb\u91c7\u6837\u7b56\u7565\uff0c\u53ef\u4ee5\u63a8\u52a8MRI\u6210\u4e3a\u771f\u6b63\u7684PoC\u8bbe\u5907\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u9002\u5e94\u8fde\u7eed\u51b3\u7b56\u4ee5\u4f18\u5316\u91c7\u6837\u3002\u8bad\u7ec3\u65b9\u6cd5\u901a\u8fc7\u9010\u6b65\u52a0\u6743\u5956\u52b1\u51fd\u6570\u8bc6\u522b\u5bf9\u6bcf\u4e2a\u8bca\u65ad\u76ee\u6807\u8d21\u732e\u6700\u5927\u7684\u6837\u672c\u3002", "result": "\u5728\u4e24\u4e2a\u819d\u5173\u8282\u75c5\u7406\u8bc4\u4f30\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u75be\u75c5\u68c0\u6d4b\u3001\u4e25\u91cd\u7a0b\u5ea6\u91cf\u5316\u548c\u6574\u4f53\u8fde\u7eed\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86k\u7a7a\u95f4\u91c7\u6837\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aMRI\u6210\u4e3a\u5168\u9762\u4e14\u7ecf\u6d4e\u7684PoC\u8bbe\u5907\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.04594", "pdf": "https://arxiv.org/pdf/2505.04594", "abs": "https://arxiv.org/abs/2505.04594", "authors": ["Zhihao Zhang", "Abhinav Kumar", "Girish Chandar Ganesan", "Xiaoming Liu"], "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets.", "AI": {"tldr": "MonoCoP\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eChain-of-Prediction\uff08CoP\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u987a\u5e8f\u548c\u6761\u4ef6\u9884\u6d4b3D\u5c5e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u7269\u4f53\u68c0\u6d4b\u7684\u6df1\u5ea6\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e863D\u5c5e\u6027\u4e4b\u95f4\u7684\u5185\u5728\u5173\u8054\uff0c\u5bfc\u81f4\u6df1\u5ea6\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u53d7\u9650\u3002", "method": "MonoCoP\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u5b9e\u73b0\uff1a1\uff09\u8f7b\u91cf\u7ea7AttributeNet\u5b66\u4e60\u5c5e\u6027\u7279\u5f81\uff1b2\uff09\u663e\u5f0f\u94fe\u5f0f\u4f20\u64ad\u7279\u5f81\uff1b3\uff09\u6b8b\u5dee\u8fde\u63a5\u786e\u4fdd\u6761\u4ef6\u9884\u6d4b\u3002", "result": "\u5728KITTI\u3001Waymo\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SoTA\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u3002", "conclusion": "MonoCoP\u901a\u8fc7\u6761\u4ef6\u5316\u9884\u6d4b3D\u5c5e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.04601", "pdf": "https://arxiv.org/pdf/2505.04601", "abs": "https://arxiv.org/abs/2505.04601", "authors": ["Xianhang Li", "Yanqing Liu", "Haoqin Tu", "Hongru Zhu", "Cihang Xie"], "title": "OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning", "categories": ["cs.CV"], "comment": null, "summary": "OpenAI's CLIP, released in early 2021, have long been the go-to choice of\nvision encoder for building multimodal foundation models. Although recent\nalternatives such as SigLIP have begun to challenge this status quo, to our\nknowledge none are fully open: their training data remains proprietary and/or\ntheir training recipes are not released. This paper fills this gap with\nOpenVision, a fully-open, cost-effective family of vision encoders that match\nor surpass the performance of OpenAI's CLIP when integrated into multimodal\nframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for\ntraining framework and Recap-DataComp-1B for training data -- while revealing\nmultiple key insights in enhancing encoder quality and showcasing practical\nbenefits in advancing multimodal models. By releasing vision encoders spanning\nfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible\ntrade-off between capacity and efficiency in building multimodal models: larger\nmodels deliver enhanced multimodal performance, while smaller versions enable\nlightweight, edge-ready multimodal deployments.", "AI": {"tldr": "OpenVision\u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u653e\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5bb6\u65cf\uff0c\u6027\u80fd\u4e0eCLIP\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5f00\u653e\u6a21\u578b\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u7f16\u7801\u5668\uff08\u5982CLIP\uff09\u7684\u8bad\u7ec3\u6570\u636e\u548c\u914d\u65b9\u672a\u5b8c\u5168\u5f00\u653e\uff0cOpenVision\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u5b8c\u5168\u5f00\u653e\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u73b0\u6709\u5de5\u4f5c\uff08\u5982CLIPS\u8bad\u7ec3\u6846\u67b6\u548cRecap-DataComp-1B\u6570\u636e\u96c6\uff09\uff0cOpenVision\u901a\u8fc7\u5173\u952e\u6539\u8fdb\u63d0\u5347\u7f16\u7801\u5668\u8d28\u91cf\u3002", "result": "OpenVision\u63d0\u4f9b\u4e86\u4ece5.9M\u5230632.1M\u53c2\u6570\u7684\u6a21\u578b\uff0c\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u7075\u6d3b\u9009\u62e9\u3002", "conclusion": "OpenVision\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u5f00\u653e\u4e14\u9ad8\u6548\u7684\u89c6\u89c9\u7f16\u7801\u5668\u9009\u62e9\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.04612", "pdf": "https://arxiv.org/pdf/2505.04612", "abs": "https://arxiv.org/abs/2505.04612", "authors": ["Jiahao Li", "Haochen Wang", "Muhammad Zubair Irshad", "Igor Vasiljevic", "Matthew R. Walter", "Vitor Campagnolo Guizilini", "Greg Shakhnarovich"], "title": "FastMap: Revisiting Dense and Scalable Structure from Motion", "categories": ["cs.CV"], "comment": "Project webpage: https://jiahao.ai/fastmap", "summary": "We propose FastMap, a new global structure from motion method focused on\nspeed and simplicity. Previous methods like COLMAP and GLOMAP are able to\nestimate high-precision camera poses, but suffer from poor scalability when the\nnumber of matched keypoint pairs becomes large. We identify two key factors\nleading to this problem: poor parallelization and computationally expensive\noptimization steps. To overcome these issues, we design an SfM framework that\nrelies entirely on GPU-friendly operations, making it easily parallelizable.\nMoreover, each optimization step runs in time linear to the number of image\npairs, independent of keypoint pairs or 3D points. Through extensive\nexperiments, we show that FastMap is one to two orders of magnitude faster than\nCOLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.", "AI": {"tldr": "FastMap\u662f\u4e00\u79cd\u65b0\u7684\u5168\u5c40\u8fd0\u52a8\u7ed3\u6784\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u901f\u5ea6\u548c\u7b80\u6d01\u6027\uff0c\u89e3\u51b3\u4e86COLMAP\u548cGLOMAP\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982COLMAP\u548cGLOMAP\uff09\u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u5728\u5339\u914d\u5173\u952e\u70b9\u5bf9\u6570\u91cf\u5927\u65f6\u6269\u5c55\u6027\u5dee\uff0c\u4e3b\u8981\u7531\u4e8e\u5e76\u884c\u5316\u4e0d\u8db3\u548c\u4f18\u5316\u6b65\u9aa4\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b8c\u5168\u57fa\u4e8eGPU\u53cb\u597d\u64cd\u4f5c\u7684SfM\u6846\u67b6\uff0c\u6613\u4e8e\u5e76\u884c\u5316\uff0c\u4e14\u6bcf\u4e2a\u4f18\u5316\u6b65\u9aa4\u7684\u8fd0\u884c\u65f6\u95f4\u4e0e\u56fe\u50cf\u5bf9\u6570\u7ebf\u6027\u76f8\u5173\uff0c\u4e0e\u5173\u952e\u70b9\u5bf9\u62163D\u70b9\u65e0\u5173\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFastMap\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u6bd4COLMAP\u548cGLOMAP\u5feb\u4e00\u5230\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u59ff\u6001\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "FastMap\u901a\u8fc7\u4f18\u5316\u5e76\u884c\u5316\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u7684\u8fd0\u52a8\u7ed3\u6784\u4f30\u8ba1\u901f\u5ea6\u3002"}}
{"id": "2505.04616", "pdf": "https://arxiv.org/pdf/2505.04616", "abs": "https://arxiv.org/abs/2505.04616", "authors": ["Feng Liu", "Nicholas Chimitt", "Lanqing Guo", "Jitesh Jain", "Aditya Kane", "Minchul Kim", "Wes Robbins", "Yiyang Su", "Dingqiang Ye", "Xingguang Zhang", "Jie Zhu", "Siddharth Satyakam", "Christopher Perry", "Stanley H. Chan", "Arun Ross", "Humphrey Shi", "Zhangyang Wang", "Anil Jain", "Xiaoming Liu"], "title": "Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait", "categories": ["cs.CV"], "comment": "18 pages, 12 figures", "summary": "We address the problem of whole-body person recognition in unconstrained\nenvironments. This problem arises in surveillance scenarios such as those in\nthe IARPA Biometric Recognition and Identification at Altitude and Range\n(BRIAR) program, where biometric data is captured at long standoff distances,\nelevated viewing angles, and under adverse atmospheric conditions (e.g.,\nturbulence and high wind velocity). To this end, we propose FarSight, a unified\nend-to-end system for person recognition that integrates complementary\nbiometric cues across face, gait, and body shape modalities. FarSight\nincorporates novel algorithms across four core modules: multi-subject detection\nand tracking, recognition-aware video restoration, modality-specific biometric\nfeature encoding, and quality-guided multi-modal fusion. These components are\ndesigned to work cohesively under degraded image conditions, large pose and\nscale variations, and cross-domain gaps. Extensive experiments on the BRIAR\ndataset, one of the most comprehensive benchmarks for long-range, multi-modal\nbiometric recognition, demonstrate the effectiveness of FarSight. Compared to\nour preliminary system, this system achieves a 34.1% absolute gain in 1:1\nverification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set\nidentification (Rank-20), and a 34.3% reduction in open-set identification\nerrors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE\nFace in Video Evaluation (FIVE), which conducts standardized face recognition\ntesting on the BRIAR dataset. These results establish FarSight as a\nstate-of-the-art solution for operational biometric recognition in challenging\nreal-world conditions.", "AI": {"tldr": "FarSight\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5168\u8eab\u4eba\u7269\u8bc6\u522b\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u9762\u90e8\u3001\u6b65\u6001\u548c\u4f53\u578b\u7b49\u591a\u6a21\u6001\u751f\u7269\u7279\u5f81\uff0c\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5728\u8fdc\u8ddd\u79bb\u3001\u9ad8\u89c6\u89d2\u548c\u6076\u52a3\u5927\u6c14\u6761\u4ef6\u4e0b\uff08\u5982\u6e4d\u6d41\u548c\u9ad8\u98ce\u901f\uff09\u7684\u5168\u8eab\u4eba\u7269\u8bc6\u522b\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u76d1\u63a7\u573a\u666f\u3002", "method": "FarSight\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u591a\u76ee\u6807\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u3001\u8bc6\u522b\u611f\u77e5\u7684\u89c6\u9891\u6062\u590d\u3001\u6a21\u6001\u7279\u5b9a\u7684\u751f\u7269\u7279\u5f81\u7f16\u7801\u548c\u8d28\u91cf\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728BRIAR\u6570\u636e\u96c6\u4e0a\uff0cFarSight\u57281:1\u9a8c\u8bc1\u3001\u95ed\u96c6\u8bc6\u522b\u548c\u5f00\u96c6\u8bc6\u522b\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u5347\u4e8634.1%\u300117.8%\u548c34.3%\u7684\u6027\u80fd\u3002", "conclusion": "FarSight\u5728\u6076\u52a3\u73b0\u5b9e\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u6210\u4e3a\u751f\u7269\u8bc6\u522b\u9886\u57df\u7684\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04620", "pdf": "https://arxiv.org/pdf/2505.04620", "abs": "https://arxiv.org/abs/2505.04620", "authors": ["Hao Fei", "Yuan Zhou", "Juncheng Li", "Xiangtai Li", "Qingshan Xu", "Bobo Li", "Shengqiong Wu", "Yaoting Wang", "Junbao Zhou", "Jiahao Meng", "Qingyu Shi", "Zhiyuan Zhou", "Liangtao Shi", "Minghe Gao", "Daoan Zhang", "Zhiqi Ge", "Weiming Wu", "Siliang Tang", "Kaihang Pan", "Yaobo Ye", "Haobo Yuan", "Tao Zhang", "Tianjie Ju", "Zixiang Meng", "Shilin Xu", "Liyu Jia", "Wentao Hu", "Meng Luo", "Jiebo Luo", "Tat-Seng Chua", "Shuicheng Yan", "Hanwang Zhang"], "title": "On Path to Multimodal Generalist: General-Level and General-Bench", "categories": ["cs.CV"], "comment": "ICML'25, 305 pages, 115 tables, 177 figures, project page:\n  https://generalist.top/", "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86General-Level\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u6027\u80fd\u548c\u901a\u7528\u6027\uff0c\u5e76\u5f15\u5165Synergy\u6982\u5ff5\u548cGeneral-Bench\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709MLLM\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5168\u9762\u8861\u91cf\u6a21\u578b\u80fd\u529b\uff0c\u65e0\u6cd5\u76f4\u63a5\u5224\u65ad\u5176\u662f\u5426\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73AI\u3002", "method": "\u63d0\u51fa5\u7ea7General-Level\u6846\u67b6\u548cSynergy\u6982\u5ff5\uff0c\u5f00\u53d1General-Bench\u57fa\u51c6\uff08\u542b700\u4efb\u52a1\u548c325,800\u5b9e\u4f8b\uff09\u3002", "result": "\u8bc4\u4f30\u4e86100\u591a\u4e2aMLLM\uff0c\u63ed\u793a\u4e86\u901a\u7528\u6a21\u578b\u7684\u6027\u80fd\u6392\u540d\u53ca\u5b9e\u73b0\u771f\u6b63AI\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7814\u7a76\u94fa\u8def\uff0c\u52a0\u901f\u5b9e\u73b0AGI\u3002"}}
{"id": "2505.03757", "pdf": "https://arxiv.org/pdf/2505.03757", "abs": "https://arxiv.org/abs/2505.03757", "authors": ["Vinicius Francisco Rofatto", "Luiz Felipe Rodrigues de Almeida", "Marcelo Tomio Matsuoka", "Ivandro Klein", "Mauricio Roberto Veronez", "Luiz Gonzaga Da Silveira Junior"], "title": "On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation", "categories": ["physics.geo-ph", "cs.CV", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Coordinate transformation models often fail to account for nonlinear and\nspatially dependent distortions, leading to significant residual errors in\ngeospatial applications. Here we propose a residual-based neural correction\nstrategy, in which a neural network learns to model only the systematic\ndistortions left by an initial geometric transformation. By focusing solely on\nresidual patterns, the proposed method reduces model complexity and improves\nperformance, particularly in scenarios with sparse or structured control point\nconfigurations. We evaluate the method using both simulated datasets with\nvarying distortion intensities and sampling strategies, as well as under the\nreal-world image georeferencing tasks. Compared with direct neural network\ncoordinate converter and classical transformation models, the residual-based\nneural correction delivers more accurate and stable results under challenging\nconditions, while maintaining comparable performance in ideal cases. These\nfindings demonstrate the effectiveness of residual modelling as a lightweight\nand robust alternative for improving coordinate transformation accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b8b\u5dee\u7684\u795e\u7ecf\u6821\u6b63\u7b56\u7565\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u521d\u59cb\u51e0\u4f55\u53d8\u6362\u540e\u7684\u7cfb\u7edf\u5931\u771f\uff0c\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5750\u6807\u53d8\u6362\u6a21\u578b\u96be\u4ee5\u5904\u7406\u975e\u7ebf\u6027\u548c\u7a7a\u95f4\u4f9d\u8d56\u7684\u5931\u771f\uff0c\u5bfc\u81f4\u5730\u7406\u7a7a\u95f4\u5e94\u7528\u4e2d\u5b58\u5728\u663e\u8457\u6b8b\u5dee\u8bef\u5dee\u3002", "method": "\u91c7\u7528\u6b8b\u5dee\u5efa\u6a21\u7b56\u7565\uff0c\u795e\u7ecf\u7f51\u7edc\u4ec5\u5b66\u4e60\u521d\u59cb\u53d8\u6362\u540e\u7684\u7cfb\u7edf\u5931\u771f\uff0c\u51cf\u5c11\u6a21\u578b\u590d\u6742\u6027\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9645\u5730\u7406\u914d\u51c6\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u76f4\u63a5\u795e\u7ecf\u7f51\u7edc\u8f6c\u6362\u548c\u7ecf\u5178\u53d8\u6362\u6a21\u578b\u66f4\u51c6\u786e\u7a33\u5b9a\u3002", "conclusion": "\u6b8b\u5dee\u5efa\u6a21\u662f\u63d0\u5347\u5750\u6807\u53d8\u6362\u7cbe\u5ea6\u7684\u8f7b\u91cf\u4e14\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788", "abs": "https://arxiv.org/abs/2505.03788", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u548c\u81ea\u4e00\u81f4\u6027\u6539\u8fdb\u6821\u51c6\u6548\u679c\u3002", "motivation": "\u73b0\u6709UQ\u65b9\u6cd5\u4f9d\u8d56LLM\u5728\u591a\u6837\u8bbe\u7f6e\u4e0b\u751f\u6210\u7684\u591a\u54cd\u5e94\u4e00\u81f4\u6027\uff0c\u4f46\u5728LLM\u6301\u7eed\u9519\u8bef\u65f6\u4ecd\u62a5\u544a\u9ad8\u7f6e\u4fe1\u5ea6\uff0c\u5bfc\u81f4\u6821\u51c6\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff08\u5c06\u6587\u672c\u54cd\u5e94\u4e0e\u89c6\u89c9\u8f93\u5165\u5173\u8054\uff09\u548c\u6e29\u5ea6\u7f29\u653e\u6280\u672f\u6821\u51c6\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728\u533b\u7597\u95ee\u7b54\uff08Slake\uff09\u548c\u89c6\u89c9\u95ee\u7b54\uff08VQAv2\uff09\u4efb\u52a1\u4e2d\uff0c\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6821\u51c6\u6548\u679c\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u548c\u6e29\u5ea6\u7f29\u653e\uff0c\u663e\u8457\u6539\u5584\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u6821\u51c6\u6027\u80fd\u3002"}}
{"id": "2505.03800", "pdf": "https://arxiv.org/pdf/2505.03800", "abs": "https://arxiv.org/abs/2505.03800", "authors": ["TianYi Yu"], "title": "Design description of Wisdom Computing Persperctive", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This course design aims to develop and research a handwriting matrix\nrecognition and step-by-step visual calculation process display system,\naddressing the issue of abstract formulas and complex calculation steps that\nstudents find difficult to understand when learning mathematics. By integrating\nartificial intelligence with visualization animation technology, the system\nenhances precise recognition of handwritten matrix content through the\nintroduction of Mamba backbone networks, completes digital extraction and\nmatrix reconstruction using the YOLO model, and simultaneously combines\nCoordAttention coordinate attention mechanisms to improve the accurate grasp of\ncharacter spatial positions. The calculation process is demonstrated frame by\nframe through the Manim animation engine, vividly showcasing each mathematical\ncalculation step, helping students intuitively understand the intrinsic logic\nof mathematical operations. Through dynamically generating animation processes\nfor different computational tasks, the system exhibits high modularity and\nflexibility, capable of generating various mathematical operation examples in\nreal-time according to student needs. By innovating human-computer interaction\nmethods, it brings mathematical calculation processes to life, helping students\nbridge the gap between knowledge and understanding on a deeper level,\nultimately achieving a learning experience where \"every step is understood.\"\nThe system's scalability and interactivity make it an intuitive, user-friendly,\nand efficient auxiliary tool in education.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408AI\u548c\u53ef\u89c6\u5316\u52a8\u753b\u7684\u624b\u5199\u77e9\u9635\u8bc6\u522b\u4e0e\u8ba1\u7b97\u8fc7\u7a0b\u5c55\u793a\u7cfb\u7edf\uff0c\u5e2e\u52a9\u5b66\u751f\u7406\u89e3\u6570\u5b66\u8ba1\u7b97\u6b65\u9aa4\u3002", "motivation": "\u89e3\u51b3\u5b66\u751f\u5728\u5b66\u4e60\u6570\u5b66\u65f6\u56e0\u62bd\u8c61\u516c\u5f0f\u548c\u590d\u6742\u8ba1\u7b97\u6b65\u9aa4\u96be\u4ee5\u7406\u89e3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528Mamba\u9aa8\u5e72\u7f51\u7edc\u548cYOLO\u6a21\u578b\u8fdb\u884c\u624b\u5199\u77e9\u9635\u8bc6\u522b\u4e0e\u91cd\u6784\uff0c\u7ed3\u5408CoordAttention\u673a\u5236\u4f18\u5316\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u5e76\u901a\u8fc7Manim\u52a8\u753b\u5f15\u64ce\u5c55\u793a\u8ba1\u7b97\u6b65\u9aa4\u3002", "result": "\u7cfb\u7edf\u80fd\u52a8\u6001\u751f\u6210\u52a8\u753b\uff0c\u6a21\u5757\u5316\u5f3a\uff0c\u7075\u6d3b\u6027\u9ad8\uff0c\u5b9e\u65f6\u54cd\u5e94\u5b66\u751f\u9700\u6c42\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u4ea4\u4e92\u65b9\u5f0f\uff0c\u751f\u52a8\u5c55\u793a\u6570\u5b66\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5e2e\u52a9\u5b66\u751f\u6df1\u5165\u7406\u89e3\uff0c\u5b9e\u73b0\u201c\u6bcf\u4e00\u6b65\u90fd\u61c2\u201d\u7684\u5b66\u4e60\u4f53\u9a8c\u3002"}}
{"id": "2505.03807", "pdf": "https://arxiv.org/pdf/2505.03807", "abs": "https://arxiv.org/abs/2505.03807", "authors": ["Yiwen Zhang", "Jianing Hao", "Zhan Wang", "Hongling Sheng", "Wei Zeng"], "title": "Facilitating Video Story Interaction with Multi-Agent Collaborative System", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "comment": "Prepared and submitted in 2024", "summary": "Video story interaction enables viewers to engage with and explore narrative\ncontent for personalized experiences. However, existing methods are limited to\nuser selection, specially designed narratives, and lack customization. To\naddress this, we propose an interactive system based on user intent. Our system\nuses a Vision Language Model (VLM) to enable machines to understand video\nstories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent\nSystem (MAS) to create evolving characters and scene experiences. It includes\nthree stages: 1) Video story processing, utilizing VLM and prior knowledge to\nsimulate human understanding of stories across three modalities. 2) Multi-space\nchat, creating growth-oriented characters through MAS interactions based on\nuser queries and story stages. 3) Scene customization, expanding and\nvisualizing various story scenes mentioned in dialogue. Applied to the Harry\nPotter series, our study shows the system effectively portrays emergent\ncharacter social behavior and growth, enhancing the interactive experience in\nthe video story world.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7528\u6237\u610f\u56fe\u7684\u4ea4\u4e92\u7cfb\u7edf\uff0c\u7ed3\u5408VLM\u3001RAG\u548cMAS\uff0c\u5b9e\u73b0\u89c6\u9891\u6545\u4e8b\u7684\u4e2a\u6027\u5316\u4e92\u52a8\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u7528\u6237\u9009\u62e9\u548c\u9884\u8bbe\u53d9\u4e8b\uff0c\u7f3a\u4e4f\u5b9a\u5236\u5316\uff0c\u9700\u63d0\u5347\u4e92\u52a8\u6027\u548c\u4e2a\u6027\u5316\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a1) \u89c6\u9891\u6545\u4e8b\u5904\u7406\uff08VLM+\u5148\u9a8c\u77e5\u8bc6\uff09\uff1b2) \u591a\u7a7a\u95f4\u804a\u5929\uff08MAS\u751f\u6210\u89d2\u8272\uff09\uff1b3) \u573a\u666f\u5b9a\u5236\uff08\u6269\u5c55\u548c\u53ef\u89c6\u5316\u6545\u4e8b\u573a\u666f\uff09\u3002", "result": "\u5e94\u7528\u4e8e\u300a\u54c8\u5229\u00b7\u6ce2\u7279\u300b\u7cfb\u5217\uff0c\u7cfb\u7edf\u6210\u529f\u5c55\u73b0\u89d2\u8272\u793e\u4ea4\u884c\u4e3a\u548c\u6210\u957f\uff0c\u63d0\u5347\u4e92\u52a8\u4f53\u9a8c\u3002", "conclusion": "\u7cfb\u7edf\u901a\u8fc7\u591a\u6a21\u6001\u7406\u89e3\u548c\u52a8\u6001\u4ea4\u4e92\uff0c\u6709\u6548\u589e\u5f3a\u89c6\u9891\u6545\u4e8b\u7684\u4e2a\u6027\u5316\u4e92\u52a8\u3002"}}
{"id": "2505.03808", "pdf": "https://arxiv.org/pdf/2505.03808", "abs": "https://arxiv.org/abs/2505.03808", "authors": ["Ioannis Nasios"], "title": "AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Harmful algal blooms are a growing threat to inland water quality and public\nhealth worldwide, creating an urgent need for efficient, accurate, and\ncost-effective detection methods. This research introduces a high-performing\nmethodology that integrates multiple open-source remote sensing data with\nadvanced artificial intelligence models. Key data sources include Copernicus\nSentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and\nNOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently\nretrieved using platforms like Google Earth Engine (GEE) and Microsoft\nPlanetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the\naltitude from the elevation model, the temperature and wind from NOAA as well\nas the longitude and latitude were the most important features. The approach\ncombines two types of machine learning models, tree-based models and a neural\nnetwork, into an ensemble for classifying algal bloom severity. While the tree\nmodels performed strongly on their own, incorporating a neural network added\nrobustness and demonstrated how deep learning models can effectively use\ndiverse remote sensing inputs. The method leverages high-resolution satellite\nimagery and AI-driven analysis to monitor algal blooms dynamically, and\nalthough initially developed for a NASA competition in the U.S., it shows\npotential for global application. The complete code is available for further\nadaptation and practical implementation, illustrating the convergence of remote\nsensing data and AI to address critical environmental challenges\n(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6e90\u9065\u611f\u6570\u636e\u548c\u4eba\u5de5\u667a\u80fd\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6709\u5bb3\u85fb\u534e\uff0c\u5177\u6709\u5168\u7403\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u6709\u5bb3\u85fb\u534e\u5bf9\u5185\u9646\u6c34\u8d28\u548c\u516c\u5171\u5065\u5eb7\u6784\u6210\u5a01\u80c1\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u7ecf\u6d4e\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u6574\u5408Sentinel-2\u5149\u5b66\u5f71\u50cf\u3001DEM\u548cNOAA\u6c14\u5019\u6570\u636e\uff0c\u7ed3\u5408\u6811\u6a21\u578b\u548c\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u85fb\u534e\u4e25\u91cd\u6027\u5206\u7c7b\u3002", "result": "\u6811\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u52a0\u5165\u795e\u7ecf\u7f51\u7edc\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u591a\u6e90\u9065\u611f\u6570\u636e\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u5f71\u50cf\u548cAI\u5206\u6790\u52a8\u6001\u76d1\u6d4b\u85fb\u534e\uff0c\u4ee3\u7801\u5f00\u6e90\uff0c\u9002\u7528\u4e8e\u5168\u7403\u73af\u5883\u6311\u6218\u3002"}}
{"id": "2505.03809", "pdf": "https://arxiv.org/pdf/2505.03809", "abs": "https://arxiv.org/abs/2505.03809", "authors": ["Suorong Yang", "Peng Ye", "Furao Shen", "Dongzhan Zhou"], "title": "When Dynamic Data Selection Meets Data Augmentation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Dynamic data selection aims to accelerate training with lossless performance.\nHowever, reducing training data inherently limits data diversity, potentially\nhindering generalization. While data augmentation is widely used to enhance\ndiversity, it is typically not optimized in conjunction with selection. As a\nresult, directly combining these techniques fails to fully exploit their\nsynergies. To tackle the challenge, we propose a novel online data training\nframework that, for the first time, unifies dynamic data selection and\naugmentation, achieving both training efficiency and enhanced performance. Our\nmethod estimates each sample's joint distribution of local density and\nmultimodal semantic consistency, allowing for the targeted selection of\naugmentation-suitable samples while suppressing the inclusion of noisy or\nambiguous data. This enables a more significant reduction in dataset size\nwithout sacrificing model generalization. Experimental results demonstrate that\nour method outperforms existing state-of-the-art approaches on various\nbenchmark datasets and architectures, e.g., reducing 50\\% training costs on\nImageNet-1k with lossless performance. Furthermore, our approach enhances noise\nresistance and improves model robustness, reinforcing its practical utility in\nreal-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u6570\u636e\u8bad\u7ec3\u6846\u67b6\uff0c\u9996\u6b21\u7edf\u4e00\u52a8\u6001\u6570\u636e\u9009\u62e9\u548c\u589e\u5f3a\uff0c\u5b9e\u73b0\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u52a8\u6001\u6570\u636e\u9009\u62e9\u548c\u589e\u5f3a\u901a\u5e38\u672a\u8054\u5408\u4f18\u5316\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5176\u534f\u540c\u6548\u5e94\u3002", "method": "\u901a\u8fc7\u4f30\u8ba1\u6837\u672c\u7684\u5c40\u90e8\u5bc6\u5ea6\u548c\u591a\u6a21\u6001\u8bed\u4e49\u4e00\u81f4\u6027\u8054\u5408\u5206\u5e03\uff0c\u6709\u9488\u5bf9\u6027\u5730\u9009\u62e9\u9002\u5408\u589e\u5f3a\u7684\u6837\u672c\uff0c\u540c\u65f6\u6291\u5236\u566a\u58f0\u6216\u6a21\u7cca\u6570\u636e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728ImageNet-1k\u4e0a\u51cf\u5c1150%\u8bad\u7ec3\u6210\u672c\u4e14\u6027\u80fd\u65e0\u635f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u8fd8\u589e\u5f3a\u566a\u58f0\u62b5\u6297\u529b\u548c\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.03836", "pdf": "https://arxiv.org/pdf/2505.03836", "abs": "https://arxiv.org/abs/2505.03836", "authors": ["Chongsheng Zhang", "Shuwen Wu", "Yingqi Chen", "Matthias A\u00dfenmacher", "Christian Heumann", "Yi Men", "Gaojuan Fan", "Jo\u00e3o Gama"], "title": "OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "This is the long version of our OBD-Finder paper for AI-enabled\n  Oracle Bone Duplicates Discovery (currently under review at the ECML PKDD\n  2025 Demo Track). The models, video illustration and demonstration of this\n  paper are available at: https://github.com/cszhangLMU/OBD-Finder/.\n  Illustration video: https://www.youtube.com/watch?v=5QT4f0YIo0Q", "summary": "Oracle Bone Inscription (OBI) is the earliest systematic writing system in\nChina, while the identification of Oracle Bone (OB) duplicates is a fundamental\nissue in OBI research. In this work, we design a progressive OB duplicate\ndiscovery framework that combines unsupervised low-level keypoints matching\nwith high-level text-centric content-based matching to refine and rank the\ncandidate OB duplicates with semantic awareness and interpretability. We\ncompare our approach with state-of-the-art content-based image retrieval and\nimage matching methods, showing that our approach yields comparable recall\nperformance and the highest simplified mean reciprocal rank scores for both\nTop-5 and Top-15 retrieval results, and with significantly accelerated\ncomputation efficiency. We have discovered over 60 pairs of new OB duplicates\nin real-world deployment, which were missed by OBI researchers for decades. The\nmodels, video illustration and demonstration of this work are available at:\nhttps://github.com/cszhangLMU/OBD-Finder/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65e0\u76d1\u7763\u4f4e\u5c42\u5173\u952e\u70b9\u5339\u914d\u4e0e\u9ad8\u5c42\u6587\u672c\u5185\u5bb9\u5339\u914d\u7684\u7532\u9aa8\u6587\u91cd\u590d\u53d1\u73b0\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7532\u9aa8\u6587\u91cd\u590d\u8bc6\u522b\u662f\u7532\u9aa8\u6587\u7814\u7a76\u7684\u57fa\u7840\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6613\u9057\u6f0f\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u6846\u67b6\uff0c\u7ed3\u5408\u4f4e\u5c42\u5173\u952e\u70b9\u5339\u914d\u548c\u9ad8\u5c42\u6587\u672c\u5185\u5bb9\u5339\u914d\uff0c\u4f18\u5316\u5019\u9009\u91cd\u590d\u7532\u9aa8\u6587\u7684\u6392\u5e8f\u3002", "result": "\u5728Top-5\u548cTop-15\u68c0\u7d22\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u5e76\u53d1\u73b060\u591a\u5bf9\u65b0\u91cd\u590d\u7532\u9aa8\u6587\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u5177\u6709\u8bed\u4e49\u89e3\u91ca\u6027\uff0c\u4e3a\u7532\u9aa8\u6587\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.03838", "pdf": "https://arxiv.org/pdf/2505.03838", "abs": "https://arxiv.org/abs/2505.03838", "authors": ["Ting Yu Tsai", "An Yu", "Meghana Spurthi Maadugundu", "Ishrat Jahan Mohima", "Umme Habiba Barsha", "Mei-Hwa F. Chen", "Balakrishnan Prabhakaran", "Ming-Ching Chang"], "title": "IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Precise and effective processing of cardiac imaging data is critical for the\nidentification and management of the cardiovascular diseases. We introduce\nIntelliCardiac, a comprehensive, web-based medical image processing platform\nfor the automatic segmentation of 4D cardiac images and disease classification,\nutilizing an AI model trained on the publicly accessible ACDC dataset. The\nsystem, intended for patients, cardiologists, and healthcare professionals,\noffers an intuitive interface and uses deep learning models to identify\nessential heart structures and categorize cardiac diseases. The system supports\nanalysis of both the right and left ventricles as well as myocardium, and then\nclassifies patient's cardiac images into five diagnostic categories: dilated\ncardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right\nventricular abnormality, and no disease. IntelliCardiac combines a deep\nlearning-based segmentation model with a two-step classification pipeline. The\nsegmentation module gains an overall accuracy of 92.6\\%. The classification\nmodule, trained on characteristics taken from segmented heart structures,\nachieves 98\\% accuracy in five categories. These results exceed the performance\nof the existing state-of-the-art methods that integrate both segmentation and\nclassification models. IntelliCardiac, which supports real-time visualization,\nworkflow integration, and AI-assisted diagnostics, has great potential as a\nscalable, accurate tool for clinical decision assistance in cardiac imaging and\ndiagnosis.", "AI": {"tldr": "IntelliCardiac\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u76844D\u5fc3\u810f\u56fe\u50cf\u81ea\u52a8\u5206\u5272\u548c\u75be\u75c5\u5206\u7c7b\u5e73\u53f0\uff0c\u51c6\u786e\u7387\u9ad8\uff0c\u652f\u6301\u5b9e\u65f6\u53ef\u89c6\u5316\u548c\u4e34\u5e8a\u51b3\u7b56\u8f85\u52a9\u3002", "motivation": "\u5fc3\u810f\u5f71\u50cf\u6570\u636e\u7684\u7cbe\u786e\u5904\u7406\u5bf9\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u8bc6\u522b\u548c\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5272\u548c\u5206\u7c7b\u96c6\u6210\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\u548c\u4e24\u6b65\u5206\u7c7b\u6d41\u7a0b\uff0c\u5229\u7528\u516c\u5f00ACDC\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u652f\u6301\u5de6\u53f3\u5fc3\u5ba4\u548c\u5fc3\u808c\u5206\u6790\uff0c\u5206\u7c7b\u4e94\u79cd\u5fc3\u810f\u75be\u75c5\u3002", "result": "\u5206\u5272\u6a21\u5757\u51c6\u786e\u738792.6%\uff0c\u5206\u7c7b\u6a21\u5757\u51c6\u786e\u738798%\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IntelliCardiac\u5177\u6709\u6f5c\u529b\u6210\u4e3a\u4e34\u5e8a\u5fc3\u810f\u5f71\u50cf\u8bca\u65ad\u7684\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2505.03842", "pdf": "https://arxiv.org/pdf/2505.03842", "abs": "https://arxiv.org/abs/2505.03842", "authors": ["Vadim Musienko", "Axel Jacquet", "Ingmar Weber", "Till Koebe"], "title": "Coverage Biases in High-Resolution Satellite Imagery", "categories": ["cs.CY", "astro-ph.EP", "cs.CV"], "comment": null, "summary": "Satellite imagery is increasingly used to complement traditional data\ncollection approaches such as surveys and censuses across scientific\ndisciplines. However, we ask: Do all places on earth benefit equally from this\nnew wealth of information? In this study, we investigate coverage bias of major\nsatellite constellations that provide optical satellite imagery with a ground\nsampling distance below 10 meters, evaluating both the future on-demand tasking\nopportunities as well as the availability of historic images across the globe.\nSpecifically, forward-looking, we estimate how often different places are\nrevisited during a window of 30 days based on the satellites' orbital paths,\nthus investigating potential coverage biases caused by physical factors. We\nfind that locations farther away from the equator are generally revisited more\nfrequently by the constellations under study. Backward-looking, we show that\nhistoric satellite image availability -- based on metadata collected from major\nsatellite imagery providers -- is influenced by socio-economic factors on the\nground: less developed, less populated places have less satellite images\navailable. Furthermore, in three small case studies on recent conflict regions\nin this world, namely Gaza, Sudan and Ukraine, we show that also geopolitical\nevents play an important role in satellite image availability, hinting at\nunderlying business model decisions. These insights lay bare that the digital\ndividend yielded by satellite imagery is not equally distributed across our\nplanet.", "AI": {"tldr": "\u536b\u661f\u56fe\u50cf\u8986\u76d6\u5b58\u5728\u5730\u7406\u548c\u793e\u4f1a\u7ecf\u6d4e\u504f\u5dee\uff0c\u8fdc\u79bb\u8d64\u9053\u5730\u533a\u66f4\u9891\u7e41\u88ab\u8bbf\u95ee\uff0c\u6b20\u53d1\u8fbe\u5730\u533a\u5386\u53f2\u56fe\u50cf\u8f83\u5c11\uff0c\u5730\u7f18\u653f\u6cbb\u4e8b\u4ef6\u4e5f\u5f71\u54cd\u56fe\u50cf\u53ef\u7528\u6027\u3002", "motivation": "\u63a2\u8ba8\u536b\u661f\u56fe\u50cf\u5728\u5168\u7403\u8303\u56f4\u5185\u7684\u8986\u76d6\u662f\u5426\u516c\u5e73\uff0c\u63ed\u793a\u7269\u7406\u3001\u793e\u4f1a\u7ecf\u6d4e\u548c\u5730\u7f18\u653f\u6cbb\u56e0\u7d20\u5bf9\u56fe\u50cf\u53ef\u7528\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u4e3b\u8981\u536b\u661f\u661f\u5ea7\u7684\u8f68\u9053\u6570\u636e\uff0c\u8bc4\u4f3030\u5929\u5185\u4e0d\u540c\u5730\u533a\u7684\u91cd\u8bbf\u9891\u7387\uff1b\u6536\u96c6\u5386\u53f2\u56fe\u50cf\u5143\u6570\u636e\uff0c\u7ed3\u5408\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u5206\u6790\uff1b\u901a\u8fc7\u52a0\u6c99\u3001\u82cf\u4e39\u548c\u4e4c\u514b\u5170\u7684\u6848\u4f8b\u7814\u7a76\u5730\u7f18\u653f\u6cbb\u5f71\u54cd\u3002", "result": "\u8fdc\u79bb\u8d64\u9053\u5730\u533a\u91cd\u8bbf\u9891\u7387\u66f4\u9ad8\uff1b\u6b20\u53d1\u8fbe\u548c\u4eba\u53e3\u7a00\u5c11\u5730\u533a\u5386\u53f2\u56fe\u50cf\u8f83\u5c11\uff1b\u5730\u7f18\u653f\u6cbb\u4e8b\u4ef6\u663e\u8457\u5f71\u54cd\u56fe\u50cf\u53ef\u7528\u6027\u3002", "conclusion": "\u536b\u661f\u56fe\u50cf\u7684\u6570\u5b57\u5316\u7ea2\u5229\u5728\u5168\u7403\u5206\u5e03\u4e0d\u5747\uff0c\u9700\u5173\u6ce8\u8986\u76d6\u504f\u5dee\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2505.03844", "pdf": "https://arxiv.org/pdf/2505.03844", "abs": "https://arxiv.org/abs/2505.03844", "authors": ["Sol\u00e8ne Debuys\u00e8re", "Nicolas Trouv\u00e9", "Nathan Letheule", "Olivier L\u00e9v\u00eaque", "Elise Colin"], "title": "From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has\nincreased considerably in recent years, with datasets commercially available.\nHowever, the acquisition of high-resolution SAR images in airborne\nconfigurations, remains costly and limited. Thus, the lack of open source,\nwell-labeled, or easily exploitable SAR text-image datasets is a barrier to the\nuse of existing foundation models in remote sensing applications. In this\ncontext, synthetic image generation is a promising solution to augment this\nscarce data, enabling a broader range of applications. Leveraging over 15 years\nof ONERA's extensive archival airborn data from acquisition campaigns, we\ncreated a comprehensive training dataset of 110 thousands SAR images to exploit\na 3.5 billion parameters pre-trained latent diffusion model. In this work, we\npresent a novel approach utilizing spatial conditioning techniques within a\nfoundation model to transform satellite SAR imagery into airborne SAR\nrepresentations. Additionally, we demonstrate that our pipeline is effective\nfor bridging the realism of simulated images generated by ONERA's physics-based\nsimulator EMPRISE. Our method explores a key application of AI in advancing SAR\nimaging technology. To the best of our knowledge, we are the first to introduce\nthis approach in the literature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u7a7a\u95f4\u6761\u4ef6\u6280\u672f\uff0c\u5c06\u536b\u661fSAR\u56fe\u50cf\u8f6c\u6362\u4e3a\u673a\u8f7dSAR\u8868\u793a\u7684\u65b0\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u9ad8\u8d28\u91cfSAR\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u7531\u4e8e\u9ad8\u5206\u8fa8\u7387\u673a\u8f7dSAR\u56fe\u50cf\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u6570\u636e\u7a00\u7f3a\uff0c\u7f3a\u4e4f\u5f00\u6e90\u3001\u6807\u6ce8\u826f\u597d\u7684SAR\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u9065\u611f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528ONERA\u768415\u5e74\u673a\u8f7dSAR\u6570\u636e\u6784\u5efa\u4e8611\u4e07\u5f20\u56fe\u50cf\u7684\u8bad\u7ec3\u96c6\uff0c\u91c7\u752835\u4ebf\u53c2\u6570\u7684\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u7a7a\u95f4\u6761\u4ef6\u6280\u672f\u5b9e\u73b0\u56fe\u50cf\u8f6c\u6362\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u5c06\u536b\u661fSAR\u56fe\u50cf\u8f6c\u6362\u4e3a\u673a\u8f7dSAR\u8868\u793a\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u62df\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aSAR\u6210\u50cf\u6280\u672f\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u586b\u8865\u4e86\u76f8\u5173\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2505.03845", "pdf": "https://arxiv.org/pdf/2505.03845", "abs": "https://arxiv.org/abs/2505.03845", "authors": ["Ioannis Kyprakis", "Vasileios Skaramagkas", "Iro Boura", "Georgios Karamanis", "Dimitrios I. Fotiadis", "Zinovia Kefalopoulou", "Cleanthe Spanaki", "Manolis Tsiknakis"], "title": "A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with\nmotor and non-motor symptoms. Depressive symptoms are prevalent in PD,\naffecting up to 45% of patients. They are often underdiagnosed due to\noverlapping motor features, such as hypomimia. This study explores deep\nlearning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention\nlayers-to assess the presence and severity of depressive symptoms, as detected\nby the Geriatric Depression Scale (GDS), in PD patients through facial video\nanalysis. The same parameters were assessed in a secondary analysis taking into\naccount whether patients were one hour after (ON-medication state) or 12 hours\nwithout (OFF-medication state) dopaminergic medication. Using a dataset of\n1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest\nperformance, with up to 94% accuracy and 93.7% F1-score in binary\nclassification (presence of absence of depressive symptoms), and 87.1% accuracy\nwith an 85.4% F1-score in multiclass tasks (absence or mild or severe\ndepressive symptoms).", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08ViViT\u3001Video Swin Tiny\u548c3D CNN-LSTM\uff09\u901a\u8fc7\u9762\u90e8\u89c6\u9891\u5206\u6790\u8bc4\u4f30\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u7684\u6291\u90c1\u75c7\u72b6\uff0cVideo Swin Tiny\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u7684\u6291\u90c1\u75c7\u72b6\u5e38\u88ab\u6f0f\u8bca\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u6790\u9762\u90e8\u89c6\u9891\uff0c\u8bc4\u4f30\u6291\u90c1\u75c7\u72b6\u7684\u5b58\u5728\u548c\u4e25\u91cd\u7a0b\u5ea6\uff0c\u5e76\u8003\u8651\u836f\u7269\u72b6\u6001\u7684\u5f71\u54cd\u3002", "result": "Video Swin Tiny\u6a21\u578b\u5728\u4e8c\u5143\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe94%\u548c87.1%\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c24\u5176\u662fVideo Swin Tiny\uff0c\u53ef\u6709\u6548\u7528\u4e8e\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u6291\u90c1\u75c7\u72b6\u7684\u8bc4\u4f30\u3002"}}
{"id": "2505.03859", "pdf": "https://arxiv.org/pdf/2505.03859", "abs": "https://arxiv.org/abs/2505.03859", "authors": ["Will Hawkins", "Chris Russell", "Brent Mittelstadt"], "title": "Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators", "categories": ["cs.CY", "cs.AI", "cs.CV", "68T01"], "comment": "13 pages", "summary": "Advances in multimodal machine learning have made text-to-image (T2I) models\nincreasingly accessible and popular. However, T2I models introduce risks such\nas the generation of non-consensual depictions of identifiable individuals,\notherwise known as deepfakes. This paper presents an empirical study exploring\nthe accessibility of deepfake model variants online. Through a metadata\nanalysis of thousands of publicly downloadable model variants on two popular\nrepositories, Hugging Face and Civitai, we demonstrate a huge rise in easily\naccessible deepfake models. Almost 35,000 examples of publicly downloadable\ndeepfake model variants are identified, primarily hosted on Civitai. These\ndeepfake models have been downloaded almost 15 million times since November\n2022, with the models targeting a range of individuals from global celebrities\nto Instagram users with under 10,000 followers. Both Stable Diffusion and Flux\nmodels are used for the creation of deepfake models, with 96% of these\ntargeting women and many signalling intent to generate non-consensual intimate\nimagery (NCII). Deepfake model variants are often created via the\nparameter-efficient fine-tuning technique known as low rank adaptation (LoRA),\nrequiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this\nprocess widely accessible via consumer-grade computers. Despite these models\nviolating the Terms of Service of hosting platforms, and regulation seeking to\nprevent dissemination, these results emphasise the pressing need for greater\naction to be taken against the creation of deepfakes and NCII.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5bfc\u81f4\u6df1\u5ea6\u4f2a\u9020\uff08deepfake\uff09\u6a21\u578b\u5728\u7ebf\u4e0a\u7684\u53ef\u8bbf\u95ee\u6027\u6025\u5267\u589e\u52a0\uff0c\u4e3b\u8981\u9488\u5bf9\u5973\u6027\uff0c\u4e14\u610f\u56fe\u751f\u6210\u975e\u81ea\u613f\u4eb2\u5bc6\u56fe\u50cf\uff08NCII\uff09\u3002", "motivation": "\u63a2\u8ba8\u6df1\u5ea6\u4f2a\u9020\u6a21\u578b\u5728\u7ebf\u4e0a\u7684\u53ef\u8bbf\u95ee\u6027\u53ca\u5176\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5bf9\u4e2a\u4eba\u9690\u79c1\u548c\u5b89\u5168\u7684\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u5206\u6790Hugging Face\u548cCivitai\u4e24\u4e2a\u5e73\u53f0\u4e0a\u7684\u6570\u5343\u4e2a\u516c\u5f00\u53ef\u4e0b\u8f7d\u6a21\u578b\u53d8\u4f53\u7684\u5143\u6570\u636e\uff0c\u7814\u7a76\u6df1\u5ea6\u4f2a\u9020\u6a21\u578b\u7684\u6d41\u884c\u7a0b\u5ea6\u548c\u6280\u672f\u7279\u70b9\u3002", "result": "\u53d1\u73b0\u8fd135,000\u4e2a\u516c\u5f00\u53ef\u4e0b\u8f7d\u7684\u6df1\u5ea6\u4f2a\u9020\u6a21\u578b\u53d8\u4f53\uff0c\u4e3b\u8981\u6258\u7ba1\u5728Civitai\u4e0a\uff0c\u4e0b\u8f7d\u91cf\u8fbe1,500\u4e07\u6b21\uff0c96%\u9488\u5bf9\u5973\u6027\uff0c\u4e14\u591a\u7528\u4e8e\u751f\u6210NCII\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u8feb\u5207\u9700\u8981\u91c7\u53d6\u66f4\u591a\u884c\u52a8\u6253\u51fb\u6df1\u5ea6\u4f2a\u9020\u548cNCII\u7684\u751f\u6210\u4e0e\u4f20\u64ad\u3002"}}
{"id": "2505.03912", "pdf": "https://arxiv.org/pdf/2505.03912", "abs": "https://arxiv.org/abs/2505.03912", "authors": ["Can Cui", "Pengxiang Ding", "Wenxuan Song", "Shuanghao Bai", "Xinyang Tong", "Zirui Ge", "Runze Suo", "Wanqi Zhou", "Yang Liu", "Bofang Jia", "Han Zhao", "Siteng Huang", "Donglin Wang"], "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.", "AI": {"tldr": "\u603b\u7ed3\u5e76\u6bd4\u8f83\u73b0\u6709\u53cc\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\uff0c\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4f4e\u6210\u672c\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u53cc\u7cfb\u7edfVLA\u67b6\u6784\u7f3a\u4e4f\u5f00\u6e90\u5de5\u4f5c\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u6027\u80fd\u5206\u6790\u4e0e\u4f18\u5316\u3002", "method": "\u603b\u7ed3\u6bd4\u8f83\u73b0\u6709\u53cc\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\uff0c\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u63d0\u4f9b\u4f4e\u6210\u672c\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u6301\u7eed\u66f4\u65b0\u5b9e\u9a8c\u7ed3\u8bba\u4e0e\u6027\u80fd\u6539\u8fdb\u6a21\u578b\u3002", "conclusion": "\u4e3a\u53cc\u7cfb\u7edfVLA\u67b6\u6784\u7814\u7a76\u63d0\u4f9b\u5f00\u6e90\u652f\u6301\uff0c\u63a8\u52a8\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2505.04003", "pdf": "https://arxiv.org/pdf/2505.04003", "abs": "https://arxiv.org/abs/2505.04003", "authors": ["Feng Gao", "Sheng Liu", "Chuanzheng Gong", "Xiaowei Zhou", "Jiayi Wang", "Junyu Dong", "Qian Du"], "title": "Prototype-Based Information Compensation Network for Multi-Source Remote Sensing Data Classification", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IEEE TGRS 2025", "summary": "Multi-source remote sensing data joint classification aims to provide\naccuracy and reliability of land cover classification by leveraging the\ncomplementary information from multiple data sources. Existing methods confront\ntwo challenges: inter-frequency multi-source feature coupling and inconsistency\nof complementary information exploration. To solve these issues, we present a\nPrototype-based Information Compensation Network (PICNet) for land cover\nclassification based on HSI and SAR/LiDAR data. Specifically, we first design a\nfrequency interaction module to enhance the inter-frequency coupling in\nmulti-source feature extraction. The multi-source features are first decoupled\ninto high- and low-frequency components. Then, these features are recoupled to\nachieve efficient inter-frequency communication. Afterward, we design a\nprototype-based information compensation module to model the global\nmulti-source complementary information. Two sets of learnable modality\nprototypes are introduced to represent the global modality information of\nmulti-source data. Subsequently, cross-modal feature integration and alignment\nare achieved through cross-attention computation between the modality-specific\nprototype vectors and the raw feature representations. Extensive experiments on\nthree public datasets demonstrate the significant superiority of our PICNet\nover state-of-the-art methods. The codes are available at\nhttps://github.com/oucailab/PICNet.", "AI": {"tldr": "PICNet\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHSI\u548cSAR/LiDAR\u6570\u636e\u7684\u591a\u6e90\u9065\u611f\u6570\u636e\u8054\u5408\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u4ea4\u4e92\u6a21\u5757\u548c\u539f\u578b\u8865\u507f\u6a21\u5757\u89e3\u51b3\u7279\u5f81\u8026\u5408\u4e0e\u4fe1\u606f\u63a2\u7d22\u95ee\u9898\u3002", "motivation": "\u591a\u6e90\u9065\u611f\u6570\u636e\u8054\u5408\u5206\u7c7b\u9762\u4e34\u7279\u5f81\u8026\u5408\u4e0e\u4e92\u8865\u4fe1\u606f\u63a2\u7d22\u4e0d\u4e00\u81f4\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u9891\u7387\u4ea4\u4e92\u6a21\u5757\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u9891\u7387\u8026\u5408\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u8865\u507f\u6a21\u5757\u5efa\u6a21\u5168\u5c40\u4e92\u8865\u4fe1\u606f\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cPICNet\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PICNet\u901a\u8fc7\u9891\u7387\u4ea4\u4e92\u548c\u539f\u578b\u8865\u507f\u6709\u6548\u63d0\u5347\u4e86\u591a\u6e90\u9065\u611f\u6570\u636e\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.04050", "pdf": "https://arxiv.org/pdf/2505.04050", "abs": "https://arxiv.org/abs/2505.04050", "authors": ["Kazuki Higo", "Toshiki Kanai", "Yuki Endo", "Yoshihiro Kanamori"], "title": "TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D terrain models are essential in fields such as video game development and\nfilm production. Since surface color often correlates with terrain geometry,\ncapturing this relationship is crucial to achieving realism. However, most\nexisting methods generate either a heightmap or a texture, without sufficiently\naccounting for the inherent correlation. In this paper, we propose a method\nthat jointly generates terrain heightmaps and textures using a latent diffusion\nmodel. First, we train the model in an unsupervised manner to randomly generate\npaired heightmaps and textures. Then, we perform supervised learning of an\nexternal adapter to enable user control via hand-drawn sketches. Experiments\nshow that our approach allows intuitive terrain generation while preserving the\ncorrelation between heightmaps and textures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8054\u5408\u751f\u6210\u5730\u5f62\u9ad8\u5ea6\u56fe\u548c\u7eb9\u7406\uff0c\u5e76\u901a\u8fc7\u5916\u90e8\u9002\u914d\u5668\u5b9e\u73b0\u7528\u6237\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u751f\u6210\u9ad8\u5ea6\u56fe\u6216\u7eb9\u7406\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349\u4e8c\u8005\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5f71\u54cd\u771f\u5b9e\u611f\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u65e0\u76d1\u7763\u751f\u6210\u914d\u5bf9\u9ad8\u5ea6\u56fe\u548c\u7eb9\u7406\uff0c\u518d\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u5916\u90e8\u9002\u914d\u5668\uff0c\u652f\u6301\u624b\u7ed8\u8349\u56fe\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u76f4\u89c2\u751f\u6210\u5730\u5f62\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5ea6\u56fe\u4e0e\u7eb9\u7406\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5730\u5f62\u751f\u6210\u4e2d\u9ad8\u5ea6\u56fe\u4e0e\u7eb9\u7406\u7684\u8054\u5408\u751f\u6210\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u771f\u5b9e\u611f\u548c\u7528\u6237\u63a7\u5236\u6027\u3002"}}
{"id": "2505.04052", "pdf": "https://arxiv.org/pdf/2505.04052", "abs": "https://arxiv.org/abs/2505.04052", "authors": ["Shun Masuda", "Yuki Endo", "Yoshihiro Kanamori"], "title": "Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Compositing human figures into scene images has broad applications in areas\nsuch as entertainment and advertising. However, existing methods often cannot\nhandle occlusion of the inserted person by foreground objects and unnaturally\nplace the person in the frontmost layer. Moreover, they offer limited control\nover the inserted person's pose. To address these challenges, we propose two\nmethods. Both allow explicit pose control via a 3D body model and leverage\nlatent diffusion models to synthesize the person at a contextually appropriate\ndepth, naturally handling occlusions without requiring occlusion masks. The\nfirst is a two-stage approach: the model first learns a depth map of the scene\nwith the person through supervised learning, and then synthesizes the person\naccordingly. The second method learns occlusion implicitly and synthesizes the\nperson directly from input data without explicit depth supervision.\nQuantitative and qualitative evaluations show that both methods outperform\nexisting approaches by better preserving scene consistency while accurately\nreflecting occlusions and user-specified poses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc73D\u4eba\u4f53\u6a21\u578b\u63a7\u5236\u59ff\u52bf\uff0c\u5e76\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5408\u6210\u4eba\u7269\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u548c\u6df1\u5ea6\u653e\u7f6e\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u4eba\u7269\u5408\u6210\u5230\u573a\u666f\u56fe\u50cf\u65f6\uff0c\u96be\u4ee5\u5904\u7406\u906e\u6321\u95ee\u9898\u4e14\u65e0\u6cd5\u81ea\u7136\u63a7\u5236\u4eba\u7269\u6df1\u5ea6\u548c\u59ff\u52bf\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5148\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u751f\u6210\u573a\u666f\u6df1\u5ea6\u56fe\uff0c\u518d\u5408\u6210\u4eba\u7269\uff1b2\uff09\u76f4\u63a5\u5b66\u4e60\u906e\u6321\u5e76\u5408\u6210\u4eba\u7269\uff0c\u65e0\u9700\u663e\u5f0f\u6df1\u5ea6\u76d1\u7763\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u5e76\u51c6\u786e\u53cd\u6620\u906e\u6321\u548c\u7528\u6237\u6307\u5b9a\u59ff\u52bf\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7269\u5408\u6210\u7684\u81ea\u7136\u6027\u548c\u53ef\u63a7\u6027\uff0c\u5c24\u5176\u5728\u906e\u6321\u548c\u6df1\u5ea6\u5904\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.04095", "pdf": "https://arxiv.org/pdf/2505.04095", "abs": "https://arxiv.org/abs/2505.04095", "authors": ["Shuo Wen", "Edwin Meriaux", "Mariana Sosa Guzm\u00e1n", "Charlotte Morissette", "Chloe Si", "Bobak Baghi", "Gregory Dudek"], "title": "Scalable Aerial GNSS Localization for Marine Robots", "categories": ["cs.RO", "cs.CV"], "comment": "International Conference on Robotics and Automation 2025 Workshop\n  Robots in the Wild", "summary": "Accurate localization is crucial for water robotics, yet traditional onboard\nGlobal Navigation Satellite System (GNSS) approaches are difficult or\nineffective due to signal reflection on the water's surface and its high cost\nof aquatic GNSS receivers. Existing approaches, such as inertial navigation,\nDoppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face\nchallenges like error accumulation and high computational complexity.\nTherefore, a more efficient and scalable solution remains necessary. This paper\nproposes an alternative approach that leverages an aerial drone equipped with\nGNSS localization to track and localize a marine robot once it is near the\nsurface of the water. Our results show that this novel adaptation enables\naccurate single and multi-robot marine robot localization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u914d\u5907GNSS\u7684\u65e0\u4eba\u673a\u5b9a\u4f4d\u6c34\u9762\u9644\u8fd1\u6d77\u6d0b\u673a\u5668\u4eba\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGNSS\u548c\u73b0\u6709\u5b9a\u4f4d\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfGNSS\u5728\u6c34\u9762\u5b9a\u4f4d\u4e2d\u56e0\u4fe1\u53f7\u53cd\u5c04\u548c\u9ad8\u6210\u672c\u6548\u679c\u4e0d\u4f73\uff0c\u73b0\u6709\u6280\u672f\u5982\u60ef\u6027\u5bfc\u822a\u3001DVL\u7b49\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u914d\u5907GNSS\u7684\u65e0\u4eba\u673a\u5bf9\u6c34\u9762\u9644\u8fd1\u7684\u6d77\u6d0b\u673a\u5668\u4eba\u8fdb\u884c\u8ddf\u8e2a\u548c\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u5355\u673a\u5668\u4eba\u53ca\u591a\u673a\u5668\u4eba\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "conclusion": "\u65e0\u4eba\u673a\u8f85\u52a9\u7684GNSS\u5b9a\u4f4d\u4e3a\u6c34\u9762\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04097", "pdf": "https://arxiv.org/pdf/2505.04097", "abs": "https://arxiv.org/abs/2505.04097", "authors": ["Thien Nhan Vo", "Bac Nam Ho", "Thanh Xuan Truong"], "title": "3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "A three-dimensional convolutional neural network was developed to classify\nT1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D\nconvolution, pooling, batch normalization, dense ReLU layers, and a sigmoid\noutput. Using stochastic noise injection and five-fold cross-validation, the\nmodel achieved test set accuracy of 0.912 and area under the ROC curve of\n0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity\nand specificity both exceeded 0.90. These results align with prior work\nreporting up to 0.10 gain via synthetic augmentation. The findings demonstrate\nthe effectiveness of simple augmentation for 3D MRI classification and motivate\nfuture exploration of advanced augmentation methods and architectures such as\n3D U-Net and vision transformers.", "AI": {"tldr": "\u4f7f\u75283D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5bf9T1\u52a0\u6743\u8111MRI\u626b\u63cf\u8fdb\u884c\u5206\u7c7b\uff0c\u533a\u5206\u5065\u5eb7\u4e0e\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\uff0c\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u7b80\u5355\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u57283D MRI\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a8\u52a8\u672a\u6765\u5bf9\u9ad8\u7ea7\u589e\u5f3a\u65b9\u6cd5\u548c\u67b6\u6784\u7684\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b3D\u5377\u79ef\u3001\u6c60\u5316\u3001\u6279\u5f52\u4e00\u5316\u3001ReLU\u5c42\u548cSigmoid\u8f93\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u566a\u58f0\u6ce8\u5165\u548c\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002", "result": "\u6d4b\u8bd5\u96c6\u51c6\u786e\u73870.912\uff0cROC\u66f2\u7ebf\u4e0b\u9762\u79ef0.961\uff0c\u7075\u654f\u5ea6\u548c\u7279\u5f02\u6027\u5747\u8d85\u8fc70.90\u3002", "conclusion": "\u7b80\u5355\u589e\u5f3a\u65b9\u6cd5\u6709\u6548\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u66f4\u5148\u8fdb\u7684\u589e\u5f3a\u65b9\u6cd5\u548c\u67b6\u6784\u59823D U-Net\u548c\u89c6\u89c9\u53d8\u6362\u5668\u3002"}}
{"id": "2505.04258", "pdf": "https://arxiv.org/pdf/2505.04258", "abs": "https://arxiv.org/abs/2505.04258", "authors": ["Pietro Bonazzi", "Christian Vogt", "Michael Jost", "Haotong Qin", "Lyes Khacef", "Federico Paredes-Valles", "Michele Magno"], "title": "RGB-Event Fusion with Self-Attention for Collision Prediction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Ensuring robust and real-time obstacle avoidance is critical for the safe\noperation of autonomous robots in dynamic, real-world environments. This paper\nproposes a neural network framework for predicting the time and collision\nposition of an unmanned aerial vehicle with a dynamic object, using RGB and\nevent-based vision sensors. The proposed architecture consists of two separate\nencoder branches, one for each modality, followed by fusion by self-attention\nto improve prediction accuracy. To facilitate benchmarking, we leverage the\nABCD [8] dataset collected that enables detailed comparisons of single-modality\nand fusion-based approaches. At the same prediction throughput of 50Hz, the\nexperimental results show that the fusion-based model offers an improvement in\nprediction accuracy over single-modality approaches of 1% on average and 10%\nfor distances beyond 0.5m, but comes at the cost of +71% in memory and + 105%\nin FLOPs. Notably, the event-based model outperforms the RGB model by 4% for\nposition and 26% for time error at a similar computational cost, making it a\ncompetitive alternative. Additionally, we evaluate quantized versions of the\nevent-based models, applying 1- to 8-bit quantization to assess the trade-offs\nbetween predictive performance and computational efficiency. These findings\nhighlight the trade-offs of multi-modal perception using RGB and event-based\ncameras in robotic applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65e0\u4eba\u673a\u52a8\u6001\u969c\u788d\u7269\u907f\u969c\u6846\u67b6\uff0c\u7ed3\u5408RGB\u548c\u4e8b\u4ef6\u89c6\u89c9\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u878d\u5408\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u786e\u4fdd\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u907f\u969c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u53cc\u7f16\u7801\u5668\u5206\u652f\u5206\u522b\u5904\u7406RGB\u548c\u4e8b\u4ef6\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u878d\u5408\uff0c\u5e76\u5728ABCD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "\u878d\u5408\u6a21\u578b\u572850Hz\u4e0b\u9884\u6d4b\u7cbe\u5ea6\u5e73\u5747\u63d0\u53471%\uff0c\u8fdc\u8ddd\u79bb\u63d0\u534710%\uff0c\u4f46\u5185\u5b58\u548c\u8ba1\u7b97\u91cf\u663e\u8457\u589e\u52a0\uff1b\u4e8b\u4ef6\u6a21\u578b\u5728\u8ba1\u7b97\u6210\u672c\u76f8\u8fd1\u65f6\u4f18\u4e8eRGB\u6a21\u578b\u3002", "conclusion": "\u591a\u6a21\u6001\u611f\u77e5\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u9700\u6743\u8861\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e8b\u4ef6\u76f8\u673a\u662fRGB\u7684\u6709\u529b\u66ff\u4ee3\u3002"}}
{"id": "2505.04387", "pdf": "https://arxiv.org/pdf/2505.04387", "abs": "https://arxiv.org/abs/2505.04387", "authors": ["Amin Fadaeinejad", "Abdallah Dib", "Luiz Gustavo Hafemann", "Emeline Got", "Trevor Anderson", "Amaury Depierre", "Nikolaus F. Troje", "Marcus A. Brubaker", "Marc-Andr\u00e9 Carbonneau"], "title": "Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 9 figures, AI for Creative Visual Content Generation\n  Editing and Understanding (CVEU), CVPRW 2025", "summary": "Creating realistic 3D head assets for virtual characters that match a precise\nartistic vision remains labor-intensive. We present a novel framework that\nstreamlines this process by providing artists with intuitive control over\ngenerated 3D heads. Our approach uses a geometry-aware texture synthesis\npipeline that learns correlations between head geometry and skin texture maps\nacross different demographics. The framework offers three levels of artistic\ncontrol: manipulation of overall head geometry, adjustment of skin tone while\npreserving facial characteristics, and fine-grained editing of details such as\nwrinkles or facial hair. Our pipeline allows artists to make edits to a single\ntexture map using familiar tools, with our system automatically propagating\nthese changes coherently across the remaining texture maps needed for realistic\nrendering. Experiments demonstrate that our method produces diverse results\nwith clean geometries. We showcase practical applications focusing on intuitive\ncontrol for artists, including skin tone adjustments and simplified editing\nworkflows for adding age-related details or removing unwanted features from\nscanned models. This integrated approach aims to streamline the artistic\nworkflow in virtual character creation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u7eb9\u7406\u5408\u6210\u6d41\u7a0b\uff0c\u4e3a\u827a\u672f\u5bb6\u63d0\u4f9b\u5bf9\u751f\u62103D\u5934\u90e8\u7684\u76f4\u89c2\u63a7\u5236\uff0c\u7b80\u5316\u865a\u62df\u89d2\u8272\u521b\u5efa\u8fc7\u7a0b\u3002", "motivation": "\u4e3a\u827a\u672f\u5bb6\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u63a7\u5236\u5de5\u5177\uff0c\u51cf\u5c11\u521b\u5efa\u7b26\u5408\u7cbe\u786e\u827a\u672f\u613f\u666f\u76843D\u5934\u90e8\u8d44\u4ea7\u7684\u52b3\u52a8\u5f3a\u5ea6\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u611f\u77e5\u7684\u7eb9\u7406\u5408\u6210\u6d41\u7a0b\uff0c\u5b66\u4e60\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u4e0b\u5934\u90e8\u51e0\u4f55\u4e0e\u76ae\u80a4\u7eb9\u7406\u7684\u5173\u8054\uff0c\u63d0\u4f9b\u4e09\u4e2a\u5c42\u7ea7\u7684\u827a\u672f\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u591a\u6837\u5316\u7684\u7ed3\u679c\uff0c\u5177\u6709\u5e72\u51c0\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u5c55\u793a\u4e86\u76ae\u80a4\u8272\u8c03\u8c03\u6574\u548c\u7ec6\u8282\u7f16\u8f91\u7b49\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u96c6\u6210\u65b9\u6cd5\u65e8\u5728\u7b80\u5316\u865a\u62df\u89d2\u8272\u521b\u4f5c\u4e2d\u7684\u827a\u672f\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u7f16\u8f91\u5de5\u5177\u3002"}}
{"id": "2505.04590", "pdf": "https://arxiv.org/pdf/2505.04590", "abs": "https://arxiv.org/abs/2505.04590", "authors": ["Alexandre Binninger", "Ruben Wiersma", "Philipp Herholz", "Olga Sorkine-Hornung"], "title": "TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization", "categories": ["cs.GR", "cs.CV", "I.3.5"], "comment": "ACM Trans. Graph. 44, 4. SIGGRAPH 2025. 19 pages, 21 figures", "summary": "We introduce TetWeave, a novel isosurface representation for gradient-based\nmesh optimization that jointly optimizes the placement of a tetrahedral grid\nused for Marching Tetrahedra and a novel directional signed distance at each\npoint. TetWeave constructs tetrahedral grids on-the-fly via Delaunay\ntriangulation, enabling increased flexibility compared to predefined grids. The\nextracted meshes are guaranteed to be watertight, two-manifold and\nintersection-free. The flexibility of TetWeave enables a resampling strategy\nthat places new points where reconstruction error is high and allows to\nencourage mesh fairness without compromising on reconstruction error. This\nleads to high-quality, adaptive meshes that require minimal memory usage and\nfew parameters to optimize. Consequently, TetWeave exhibits near-linear memory\nscaling relative to the vertex count of the output mesh - a substantial\nimprovement over predefined grids. We demonstrate the applicability of TetWeave\nto a broad range of challenging tasks in computer graphics and vision, such as\nmulti-view 3D reconstruction, mesh compression and geometric texture\ngeneration.", "AI": {"tldr": "TetWeave\u662f\u4e00\u79cd\u65b0\u578b\u7b49\u503c\u9762\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u56db\u9762\u4f53\u7f51\u683c\u548c\u4f18\u5316\u65b9\u5411\u6027\u6709\u7b26\u53f7\u8ddd\u79bb\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u81ea\u9002\u5e94\u7684\u7f51\u683c\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u9884\u5b9a\u4e49\u7f51\u683c\u5728\u7075\u6d3b\u6027\u548c\u5185\u5b58\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u4f18\u7684\u7f51\u683c\u4f18\u5316\u65b9\u6848\u3002", "method": "\u52a8\u6001\u751f\u6210\u56db\u9762\u4f53\u7f51\u683c\uff08Delaunay\u4e09\u89d2\u5256\u5206\uff09\uff0c\u7ed3\u5408\u65b9\u5411\u6027\u6709\u7b26\u53f7\u8ddd\u79bb\u4f18\u5316\uff0c\u652f\u6301\u91cd\u91c7\u6837\u7b56\u7565\u4ee5\u63d0\u5347\u7f51\u683c\u8d28\u91cf\u3002", "result": "\u751f\u6210\u7684\u7f51\u683c\u5177\u6709\u6c34\u5bc6\u6027\u3001\u4e8c\u7ef4\u6d41\u5f62\u548c\u65e0\u4ea4\u53c9\u7279\u6027\uff0c\u5185\u5b58\u5360\u7528\u8fd1\u7ebf\u6027\u589e\u957f\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u5f62\u548c\u89c6\u89c9\u4efb\u52a1\u3002", "conclusion": "TetWeave\u5728\u7075\u6d3b\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u4e09\u7ef4\u91cd\u5efa\u548c\u51e0\u4f55\u5904\u7406\u4efb\u52a1\u3002"}}
{"id": "2505.04596", "pdf": "https://arxiv.org/pdf/2505.04596", "abs": "https://arxiv.org/abs/2505.04596", "authors": ["Mohammad Merati", "David Casta\u00f1\u00f3n"], "title": "Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems", "categories": ["math.OC", "cs.CV", "cs.SY", "eess.SY"], "comment": "7 pages, 3 Figures, Accepted at AIRC 2025", "summary": "This paper presents a novel approach for optimizing the scheduling and\ncontrol of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.\nThe proposed method integrates Kalman filters for motion prediction with a\ndynamic network flow model to enhance real-time video capture efficiency. By\nassigning Kalman filters to tracked objects, the system predicts future\nlocations, enabling precise scheduling of camera tasks. This prediction-driven\napproach is formulated as a network flow optimization, ensuring scalability and\nadaptability to various surveillance scenarios. To further reduce redundant\nmonitoring, we also incorporate group-tracking nodes, allowing multiple objects\nto be captured within a single camera focus when appropriate. In addition, a\nvalue-based system is introduced to prioritize camera actions, focusing on the\ntimely capture of critical events. By adjusting the decay rates of these values\nover time, the system ensures prompt responses to tasks with imminent\ndeadlines. Extensive simulations demonstrate that this approach improves\ncoverage, reduces average wait times, and minimizes missed events compared to\ntraditional master-slave camera systems. Overall, our method significantly\nenhances the efficiency, scalability, and effectiveness of surveillance\nsystems, particularly in dynamic and crowded environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u52a8\u6001\u76d1\u63a7\u73af\u5883\u4e2dPTZ\u76f8\u673a\u8c03\u5ea6\u4e0e\u63a7\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u52a8\u6001\u7f51\u7edc\u6d41\u6a21\u578b\uff0c\u63d0\u9ad8\u5b9e\u65f6\u89c6\u9891\u6355\u6349\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u4e3b\u4ece\u76f8\u673a\u7cfb\u7edf\u5728\u52a8\u6001\u548c\u62e5\u6324\u73af\u5883\u4e2d\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u8c03\u5ea6\u548c\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u96c6\u6210\u5361\u5c14\u66fc\u6ee4\u6ce2\u9884\u6d4b\u76ee\u6807\u4f4d\u7f6e\uff0c\u7ed3\u5408\u7f51\u7edc\u6d41\u4f18\u5316\u8c03\u5ea6\uff0c\u5f15\u5165\u7fa4\u7ec4\u8ddf\u8e2a\u8282\u70b9\u548c\u4ef7\u503c\u4f18\u5148\u7cfb\u7edf\u3002", "result": "\u4eff\u771f\u663e\u793a\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8986\u76d6\u7387\uff0c\u51cf\u5c11\u4e86\u7b49\u5f85\u65f6\u95f4\u548c\u9057\u6f0f\u4e8b\u4ef6\u3002", "conclusion": "\u663e\u8457\u63d0\u5347\u4e86\u76d1\u63a7\u7cfb\u7edf\u7684\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.04619", "pdf": "https://arxiv.org/pdf/2505.04619", "abs": "https://arxiv.org/abs/2505.04619", "authors": ["Abdulaziz Almuzairee", "Rohan Patil", "Dwait Bhatt", "Henrik I. Christensen"], "title": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation", "categories": ["cs.LG", "cs.CV", "cs.RO"], "comment": "For project website and code, see https://aalmuzairee.github.io/mad", "summary": "Vision is well-known for its use in manipulation, especially using visual\nservoing. To make it robust, multiple cameras are needed to expand the field of\nview. That is computationally challenging. Merging multiple views and using\nQ-learning allows the design of more effective representations and optimization\nof sample efficiency. Such a solution might be expensive to deploy. To mitigate\nthis, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently\nmerges views to increase sample efficiency while augmenting with single-view\nfeatures to allow lightweight deployment and ensure robust policies. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAD\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5408\u5e76\u591a\u89c6\u89d2\u6570\u636e\u63d0\u5347\u6837\u672c\u6548\u7387\uff0c\u540c\u65f6\u7ed3\u5408\u5355\u89c6\u89d2\u7279\u5f81\u4ee5\u5b9e\u73b0\u8f7b\u91cf\u90e8\u7f72\u548c\u9c81\u68d2\u7b56\u7565\u3002", "motivation": "\u591a\u6444\u50cf\u5934\u89c6\u89c9\u4f3a\u670d\u5728\u64cd\u7eb5\u4efb\u52a1\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u90e8\u7f72\u6602\u8d35\u3002", "method": "\u91c7\u7528Merge And Disentanglement (MAD)\u7b97\u6cd5\uff0c\u5408\u5e76\u591a\u89c6\u89d2\u6570\u636e\u5e76\u589e\u5f3a\u5355\u89c6\u89d2\u7279\u5f81\u3002", "result": "\u5728Meta-World\u548cManiSkill3\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "MAD\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u89c6\u89c9\u4f3a\u670d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u8f7b\u91cf\u90e8\u7f72\u3002"}}
{"id": "2505.04622", "pdf": "https://arxiv.org/pdf/2505.04622", "abs": "https://arxiv.org/abs/2505.04622", "authors": ["Jingwen Ye", "Yuze He", "Yanning Zhou", "Yiqin Zhu", "Kaiwen Xiao", "Yong-Jin Liu", "Wei Yang", "Xiao Han"], "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025. 14 pages, 15 figures", "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io", "AI": {"tldr": "PrimitiveAnything\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5c06\u5f62\u72b6\u57fa\u5143\u62bd\u8c61\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u5143\u7ec4\u88c5\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u5236\u4f5c\u7684\u62bd\u8c61\u76f4\u63a5\u5b66\u4e60\u57fa\u5143\u7ec4\u88c5\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u57fa\u5143\u62bd\u8c61\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u51e0\u4f55\u4f18\u5316\uff0c\u8bed\u4e49\u7406\u89e3\u6709\u9650\uff0c\u8981\u4e48\u4ece\u5c0f\u89c4\u6a21\u3001\u7c7b\u522b\u7279\u5b9a\u7684\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u5f62\u72b6\u7c7b\u522b\u3002", "method": "\u63d0\u51fa\u5f62\u72b6\u6761\u4ef6\u57fa\u5143\u53d8\u6362\u5668\u7528\u4e8e\u81ea\u56de\u5f52\u751f\u6210\uff0c\u4ee5\u53ca\u65e0\u6b67\u4e49\u53c2\u6570\u5316\u65b9\u6848\u7edf\u4e00\u8868\u793a\u591a\u79cd\u57fa\u5143\u7c7b\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPrimitiveAnything\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u57fa\u5143\u7ec4\u88c5\uff0c\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\uff0c\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u591a\u79cd3D\u5e94\u7528\uff0c\u5e76\u6709\u671b\u652f\u6301\u6e38\u620f\u4e2d\u7684\u57fa\u5143\u7528\u6237\u751f\u6210\u5185\u5bb9\u3002"}}
{"id": "2505.04623", "pdf": "https://arxiv.org/pdf/2505.04623", "abs": "https://arxiv.org/abs/2505.04623", "authors": ["Zhenghao Xing", "Xiaowei Hu", "Chi-Wing Fu", "Wenhai Wang", "Jifeng Dai", "Pheng-Ann Heng"], "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research.", "AI": {"tldr": "EchoInk-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u53f7\u7684\u7ed3\u6784\u5316\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684MLLMs\u5728\u8de8\u6a21\u6001\u63a8\u7406\uff08\u5c24\u5176\u662f\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u53f7\u7684\u6574\u5408\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u589e\u5f3a\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u57fa\u4e8eQwen2.5-Omni-7B\u6a21\u578b\uff0c\u4f7f\u7528Group Relative Policy Optimization\uff08GRPO\uff09\u4f18\u5316\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u89e3\u51b3\u540c\u6b65\u97f3\u9891-\u56fe\u50cf\u5bf9\u7684\u591a\u9009\u9898\u95ee\u7b54\u4efb\u52a1\u3002", "result": "EchoInk-R1-7B\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523085.77%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u768480.53%\uff0c\u4e14\u4ec5\u9700562\u6b65\u5f3a\u5316\u5b66\u4e60\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347MLLMs\u7684\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0cEchoInk-R1\u662f\u9996\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7edf\u4e00\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u6846\u67b6\u3002"}}
