[[toc]]

## cs.CV

### [1] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文综述了利用大语言模型（LLMs）从视频数据中检测碰撞的方法，包括融合策略、数据集、模型架构和性能比较，并探讨了未来研究方向。

- Motivation: 智能交通系统中视频碰撞检测是关键问题，LLMs和VLMs的发展为多模态信息处理提供了新思路。
- Method: 通过分类融合策略、总结数据集、分析模型架构和比较性能指标，系统综述了LLMs在碰撞检测中的应用。
- Result: 提供了该领域的结构化分类和性能基准，为未来研究奠定了基础。
- Conclusion: 本文为视频理解和基础模型交叉领域的快速发展提供了研究基础和方向。


### [2] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

TL;DR: 论文提出了一种针对水下环境的单目深度估计方法，通过微调Depth Anything V2模型，提升了在水下场景中的性能。

- Motivation: 水下环境的光衰减、散射、颜色失真和缺乏高质量真实数据限制了单目深度估计的可靠性。
- Method: 使用基于物理的水下图像生成模型创建合成数据集，并微调Depth Anything V2模型。
- Result: 微调后的模型在所有基准测试中表现优于仅基于陆地数据训练的基线模型。
- Conclusion: 研究强调了领域适应和尺度感知监督在水下环境中实现鲁棒深度估计的重要性。


### [3] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件流的场景文本识别框架ESTR-CoT，通过链式思维推理增强识别能力，解决了现有方法在解释性和上下文逻辑推理上的不足。

- Motivation: 现有事件流场景文本识别方法在低光照、快速运动等极端场景下表现优于RGB相机，但仍面临解释性不足和上下文逻辑推理弱的挑战。
- Method: 采用EVA-CLIP视觉编码器将事件流转换为token，结合Llama tokenizer和Q-former对齐预训练大语言模型Vicuna-7B，输出答案及推理过程。通过监督微调端到端优化框架，并构建大规模CoT数据集进行训练。
- Result: 在三个事件流STR基准数据集（EventSTR、WordArt*、IC15*）上的实验验证了框架的有效性和解释性。
- Conclusion: ESTR-CoT框架显著提升了事件流场景文本识别的性能，并为后续推理大模型的发展提供了数据基础。


### [4] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

TL;DR: 提出了一种零样本多模态方法，用于复合表情识别（CER），结合六种模态并通过动态权重和概率聚合生成可解释的输出。

- Motivation: 解决传统方法依赖任务特定训练数据的问题，探索零样本学习在复杂情感识别中的应用。
- Method: 使用CLIP和Qwen-VL进行零样本标签匹配和场景理解，引入MHPF模块动态加权模态预测，并通过PPA和PFSA生成复合情感输出。
- Result: 在AffWild2、AFEW和C-EXPR-DB数据集上分别达到46.95%、49.02%和34.85%的F1分数，与监督方法相当。
- Conclusion: 该方法无需领域适应即可有效捕捉复合情感，展示了零样本学习的潜力。


### [5] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 论文介绍了SciGA-145k数据集，用于支持图形摘要（GA）的选择、推荐和自动生成研究，并提出了两种任务和新的评估指标CAR。

- Motivation: 图形摘要在科学传播中至关重要，但设计有效的GA需要高水平的可视化技能，且其潜力尚未充分挖掘。
- Method: 构建了包含145,000篇论文和1.14百万张图的SciGA-145k数据集，定义了Intra-GA和Inter-GA推荐任务，并提出了CAR评估指标。
- Result: 提供了基线模型和CAR指标，为GA设计和AI辅助科学传播奠定了基础。
- Conclusion: SciGA-145k为视觉科学传播和AI在科学中的应用提供了重要支持。


### [6] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

TL;DR: 论文研究了在少量图像数据下学习鲁棒物体检测器的挑战，比较了基于提示和基于布局的两种条件策略对合成数据质量的影响。

- Motivation: 工业视觉系统中高质量训练数据收集耗时，合成数据成为解决方案，但现有方法生成慢且仿真与真实差距大。扩散模型能快速生成高质量图像，但精确控制在低数据量下仍困难。
- Method: 研究80种视觉概念，比较基于提示和基于布局的两种条件策略对合成数据质量的影响。
- Result: 当条件线索匹配完整训练分布时，合成数据比仅用真实数据平均提升34%的mAP，最高提升177%。
- Conclusion: 布局条件策略在多样性高时优于提示条件策略，合成数据能显著提升物体检测性能。


### [7] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

TL;DR: DIDB-ViT是一种新型二进制视觉变换器，通过引入差分信息和频率分解，解决了现有二进制ViT方法性能下降和依赖全精度模块的问题，显著提升了图像分类和分割性能。

- Motivation: 解决二进制视觉变换器（ViT）在边缘设备部署中性能下降和依赖全精度模块的问题。
- Method: 设计了一个包含差分信息的注意力模块，使用离散Haar小波进行频率分解，并改进了RPReLU激活函数。
- Result: DIDB-ViT在多种ViT架构中显著优于现有网络量化方法，图像分类和分割性能优越。
- Conclusion: DIDB-ViT在保持计算效率的同时，通过信息增强和频率分解，显著提升了二进制ViT的性能。


### [8] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

TL;DR: FMOcc提出了一种基于流匹配选择性状态空间模型的TPV细化占用网络，用于少帧3D占用预测，显著提升了预测精度和效率。

- Motivation: 解决少帧图像和3D空间冗余导致的遮挡和远距离场景预测精度不足的问题。
- Method: 设计了FMSSM模块生成缺失特征，TPV SSM层和PS3M选择性过滤特征，以及MT方法增强鲁棒性。
- Result: 在Occ3D-nuScenes和OpenOcc数据集上表现优异，两帧输入下达到43.1% RayIoU和39.8% mIoU。
- Conclusion: FMOcc通过流匹配和选择性特征过滤，显著提升了3D占用预测的性能和效率。


### [9] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

TL;DR: 提出SurgVisAgent，基于多模态大语言模型，动态识别内窥镜图像失真类别与严重程度，支持多种增强任务，优于传统单任务模型。

- Motivation: 现有算法多为单任务设计，难以应对复杂手术场景，需统一解决方案。
- Method: 结合先验模型、上下文少样本学习与链式推理，实现定制化图像增强。
- Result: 在模拟真实手术失真的基准测试中，SurgVisAgent表现优于传统模型。
- Conclusion: SurgVisAgent有望成为手术辅助的统一解决方案。


### [10] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 本文提出了一种基于多标签分类的飓风后损害评估框架，结合ResNet和注意力机制，显著提升了评估准确性。

- Motivation: 传统单标签分类方法无法全面捕捉飓风后损害的复杂性，亟需更高效的评估方法以支持灾害响应。
- Method: 采用ResNet特征提取模块和类特定注意力机制，实现单张图像中多类损害类型的识别。
- Result: 在Rescuenet数据集上，平均精度达90.23%，优于现有基线方法。
- Conclusion: 该框架提升了飓风后损害评估的效率和针对性，为灾害缓解和韧性建设提供了新思路。


### [11] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: 提出了一种双向域适应（BiDA）框架，用于跨域高光谱图像分类，通过提取域不变特征和域特定信息提升分类性能。

- Motivation: 解决不同场景下相同类别光谱偏移问题，提升跨域高光谱图像分类的适应性和可分性。
- Method: 设计了三分支Transformer架构（源分支、目标分支和耦合分支），结合耦合多头交叉注意力机制和双向蒸馏损失，提出自适应强化策略。
- Result: 在跨时空/场景数据集上表现优于现有方法，跨时空树种分类任务中性能提升3%~5%。
- Conclusion: BiDA框架有效提升了跨域高光谱图像分类性能，代码已开源。


### [12] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出了一种名为MAC-Lookup的模型，通过改进颜色准确性、清晰度和对比度来增强水下图像质量。

- Motivation: 水下图像因光线变化、水体浑浊和气泡等问题导致可见性和颜色失真，传统方法效果不佳，且深度学习缺乏高质量数据集。
- Method: 模型包含Conditional 3D Lookup Table Color Correction (CLTCC) 进行初步颜色和品质校正，以及Multi-Axis Adaptive Enhancement (MAAE) 进行细节优化。
- Result: 实验表明，MAC-Lookup在恢复细节和颜色方面优于现有方法，且避免了过度增强和饱和问题。
- Conclusion: MAC-Lookup是一种有效的水下图像增强方法，显著提升了视觉质量。


### [13] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 论文提出了一种自蒸馏方法，用于改进视频到音频生成模型在电影语言场景中的表现，特别是在部分可见性情况下。

- Motivation: 当前视频到音频生成方法忽视了电影语言这一关键艺术表达元素，导致在部分可见性场景中性能下降。
- Method: 采用自蒸馏方法，通过模拟电影语言变化，使学生模型学习对齐视频特征与音频-视觉对应关系。
- Result: 方法在部分可见性场景中显著提升性能，并在大规模V2A数据集VGGSound上表现更优。
- Conclusion: 自蒸馏方法有效解决了电影语言场景中的视频到音频生成问题，提升了模型性能。


### [14] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

TL;DR: LaCo是一种新型视觉令牌压缩框架，通过在视觉编码器的中间层进行压缩，优于现有方法，并显著提升训练和推理效率。

- Motivation: 现有视觉令牌压缩方法多为后编码器模块，限制了效率提升潜力。
- Method: 提出LaCo框架，包含层间像素重组机制和残差学习架构，保留关键视觉信息。
- Result: LaCo在中间层压缩中表现最优，训练效率提升20%以上，推理吞吐量提升15%以上。
- Conclusion: LaCo在视觉令牌压缩中表现出高效性和优越性，适用于多模态大语言模型。


### [15] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉基础模型（VFM）的文本特征引导视觉提示调优框架，结合语言模型和视觉提示，解决了域泛化（DG）中跨域不变特征解耦的挑战。通过引入WERA方法，进一步提升了模型性能。

- Motivation: 解决域泛化中跨域不变特征解耦的挑战，利用VFM的语言提示灵活性提升模型泛化能力。
- Method: 提出文本特征引导的视觉提示调优框架，结合LLM解耦文本提示，并引入WERA方法通过抽象提示增强视觉表示。
- Result: 在多个DG数据集（如PACS、VLCS等）上表现优于现有方法。
- Conclusion: 结合语言和视觉提示的方法有效提升了域泛化性能，WERA进一步增强了模型的鲁棒性。


### [16] [ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation](https://arxiv.org/abs/2507.02294)
*Hanbo Bi,Yulong Xu,Ya Li,Yongqiang Mao,Boyuan Tong,Chongyang Li,Chunbo Lang,Wenhui Diao,Hongqi Wang,Yingchao Feng,Xian Sun*

Main category: cs.CV

TL;DR: ViRefSAM通过结合少量标注参考图像，解决了SAM在遥感图像分割中的提示构造和领域适应性问题，实现了自动分割未见类别。

- Motivation: 解决SAM在遥感图像分割中手动构造提示效率低和缺乏领域适应性的问题。
- Method: 引入视觉上下文提示编码器和动态目标对齐适配器，保持SAM架构不变。
- Result: 在多个基准测试中表现优于现有少样本分割方法。
- Conclusion: ViRefSAM通过少量参考图像实现了高效准确的遥感图像分割。


### [17] [DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation](https://arxiv.org/abs/2507.02299)
*Yunhan Yang,Shuo Chen,Yukun Huang,Xiaoyang Wu,Yuan-Chen Guo,Edmund Y. Lam,Hengshuang Zhao,Tong He,Xihui Liu*

Main category: cs.CV

TL;DR: DreamComposer++ 是一个改进多视角条件控制的框架，通过提取和融合多视角3D表示，提升现有扩散模型生成可控新视角的能力。

- Motivation: 现有方法在生成可控新视角时因缺乏多视角信息而受限，DreamComposer++旨在解决这一问题。
- Method: 利用视角感知3D提升模块提取多视角3D表示，并通过多视角特征融合模块将其渲染为目标视角的潜在特征，最后集成到预训练扩散模型中。
- Result: 实验表明，DreamComposer++能无缝集成前沿扩散模型，显著提升多视角条件下的可控新视角生成能力。
- Conclusion: 该框架为可控3D物体重建和广泛应用提供了技术支持。


### [18] [Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images](https://arxiv.org/abs/2507.02307)
*Haoxuan Li,Chenxu Wei,Haodong Wang,Xiaomeng Hu,Boyuan An,Lingyan Ran,Baosen Zhang,Jin Jin,Omirzhan Taukebayev,Amirkhan Temirbayev,Junrui Liu,Xiuwei Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Flow-CDNet的双分支网络，用于同时检测慢速和快速变化，结合光流分支和二值变化检测分支，并在自建数据集Flow-Change上验证了其优越性。

- Motivation: 现实场景中，慢速变化往往是重大灾害的前兆，但现有方法难以同时检测慢速和快速变化。
- Method: 设计了双分支网络（光流分支和二值变化检测分支），结合金字塔结构和ResNet，并提出了新的损失函数和评价指标FEPE。
- Result: 在Flow-Change数据集上，Flow-CDNet优于现有方法，且双分支相互促进提升检测性能。
- Conclusion: Flow-CDNet能有效检测慢速和快速变化，为灾害预警等应用提供了新思路。


### [19] [LMPNet for Weakly-supervised Keypoint Discovery](https://arxiv.org/abs/2507.02308)
*Pei Guo,Ryan Farrell*

Main category: cs.CV

TL;DR: 论文提出了一种弱监督的语义关键点发现方法LMPNet，通过改进的Leaky Max Pooling层和选择策略，自动发现与物体姿态无关的语义关键点。

- Motivation: 探索仅通过类别标签弱监督学习语义关键点的方法，避免依赖手工设计的损失项。
- Method: 使用Leaky Max Pooling层鼓励卷积层学习非重复局部模式，结合选择策略和注意力掩码，最后通过可学习的聚类层生成关键点预测。
- Result: LMPNet能够自动发现与物体姿态无关的语义关键点，预测精度接近全监督的姿态估计模型。
- Conclusion: LMPNet提供了一种高效且可解释的弱监督关键点发现方法，性能接近全监督模型。


### [20] [Perception Activator: An intuitive and portable framework for brain cognitive exploration](https://arxiv.org/abs/2507.02311)
*Le Xu,Qi Zhang,Qixian Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种利用fMRI信号通过跨注意力机制注入多尺度图像特征的实验框架，以增强下游检测和分割任务的准确性。

- Motivation: 现有的大脑视觉解码方法在语义对齐上不足，导致多语义对象重建失真，因此需要更好地理解大脑视觉感知模式和解码模型处理语义对象的方式。
- Method: 通过将fMRI表示作为干预条件，注入多尺度图像特征，并使用跨注意力机制比较下游任务性能及中间特征变化。
- Result: 实验表明，加入fMRI信号提高了下游检测和分割的准确性，证实fMRI包含丰富的多对象语义线索和粗略空间定位信息。
- Conclusion: fMRI信号具有未被充分利用的语义和空间信息，可显著提升视觉解码模型的性能。


### [21] [MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation](https://arxiv.org/abs/2507.02314)
*JaeHyuck Choi,MinJun Kim,JeHyeong Hong*

Main category: cs.CV

TL;DR: MAGIC是一种新的少样本异常生成方法，通过结合掩码引导修复和多级扰动，解决了现有方法在背景保护、掩码对齐和语义合理性上的不足。

- Motivation: 工业质量控制中异常数据稀缺，现有扩散方法无法同时满足背景保护、掩码对齐和语义合理性的需求。
- Method: MAGIC基于Stable Diffusion修复框架，结合高斯提示级扰动和掩码引导空间噪声注入，并引入上下文感知掩码对齐模块。
- Result: 在MVTec-AD数据集上，MAGIC在下游异常任务中优于现有方法。
- Conclusion: MAGIC通过多级扰动和上下文对齐，实现了高质量、多样化的异常生成，解决了现有方法的局限性。


### [22] [Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos](https://arxiv.org/abs/2507.02316)
*Zecheng Zhao,Selena Song,Tong Chen,Zhi Chen,Shazia Sadiq,Yadan Luo*

Main category: cs.CV

TL;DR: SynTVA是一个新的数据集和基准，用于评估合成视频在文本到视频检索（TVR）任务中的实用性，通过多维度语义对齐评分和自动评估工具提升下游任务性能。

- Motivation: 当前文本到视频（T2V）合成的评估指标主要关注视觉质量和时间一致性，缺乏对下游任务（如TVR）的实用性评估。
- Method: 基于800个多样化用户查询生成合成视频，标注视频-文本对的四个语义对齐维度，并开发自动评估工具（Auto-Evaluator）。
- Result: SynTVA不仅作为基准工具，还能通过选择高质量合成样本显著提升TVR性能。
- Conclusion: SynTVA为合成视频的实用性评估和数据集增强提供了有效工具，推动了T2V合成在TVR任务中的应用。


### [23] [Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback](https://arxiv.org/abs/2507.02321)
*Nina Konovalova,Maxim Nikolaev,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: InnerControl通过训练轻量级卷积探针，在扩散模型的每一步中强制空间一致性，提升了生成图像的空间控制精度。

- Motivation: 现有方法（如ControlNet++）仅关注最终去噪步骤，忽略了中间生成阶段的空间一致性，限制了控制效果。
- Method: 提出InnerControl，训练卷积探针从中间UNet特征重建输入控制信号，并在整个扩散过程中最小化预测与目标条件的差异。
- Result: InnerControl结合ControlNet++，在多种条件方法（如边缘、深度）上实现了最先进的性能。
- Conclusion: InnerControl通过全步骤空间一致性优化，显著提升了控制精度和生成质量。


### [24] [Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model](https://arxiv.org/abs/2507.02322)
*Farida Siddiqi Prity,Mirza Raquib,Saydul Akbar Murad,Md. Jubayar Alam Rafi,Md. Khairul Bashar Bhuiyan,Anupam Kumar Bairagi*

Main category: cs.CV

TL;DR: 该研究提出了一种基于人工神经网络的图像处理技术，用于水稻叶病的早期分类和识别，比较了特征分析检测模型（FADM）和直接图像中心检测模型（DICDM）的性能，发现FADM表现更优。

- Motivation: 水稻叶病严重影响产量和经济收益，早期检测对有效管理和提高产量至关重要。
- Method: 研究采用特征提取算法、降维算法、特征选择算法和极限学习机（ELM）构建FADM，并与未使用特征提取算法的DICDM进行对比，通过10折交叉验证评估性能。
- Result: 实验结果显示，FADM在分类水稻叶病方面表现最佳。
- Conclusion: FADM在水稻叶病检测中具有显著潜力，可提升作物健康、减少产量损失，并促进水稻种植的可持续性。


### [25] [Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection](https://arxiv.org/abs/2507.02349)
*Rafic Nader,Vincent L'Allinec,Romain Bourcier,Florent Autrusseau*

Main category: cs.CV

TL;DR: 论文提出了一种自动检测颅内动脉瘤（ICA）关键标志点的方法，通过两步神经网络流程，先定位感兴趣区域，再精确识别分叉点，解决了标志点相近和视觉相似的问题，并在两个数据集上验证了其高效性。

- Motivation: 颅内动脉瘤常见于Willis环（CoW）的特定分叉处，准确检测这些标志点对快速诊断至关重要。现有方法存在标志点相近和视觉相似导致的漏检问题。
- Method: 采用两步神经网络流程：1）目标检测网络定位感兴趣区域；2）改进的U-Net精确识别分叉点，结合深度监督。
- Result: 在两个脑MRA数据集上验证，方法在分叉检测任务中表现最优。
- Conclusion: 提出的两步方法能有效解决标志点检测中的问题，提升诊断效率。


### [26] [Lightweight Shrimp Disease Detection Research Based on YOLOv8n](https://arxiv.org/abs/2507.02354)
*Fei Yuhuan,Wang Gengchen,Liu Fenghao,Zang Ran,Sun Xufei,Chang Hao*

Main category: cs.CV

TL;DR: 论文提出了一种基于YOLOv8n的轻量级网络架构，用于虾病智能检测，通过改进检测头和自注意力机制，在降低计算复杂度的同时提高了检测精度。

- Motivation: 虾病是虾养殖中经济损失的主要原因之一，需要高效智能的检测方法来预防疾病传播。
- Method: 设计了RLDD检测头和C2f-EMCM模块降低计算复杂度，引入改进的SegNext_Attention自注意力机制增强特征提取能力。
- Result: 模型参数减少32.3%，mAP@0.5达92.7%（比YOLOv8n提高3%），在URPC2020数据集上泛化能力验证了其鲁棒性。
- Conclusion: 该方法在精度和效率间取得平衡，为虾养殖中的智能疾病检测提供了可靠技术支持。


### [27] [Holistic Tokenizer for Autoregressive Image Generation](https://arxiv.org/abs/2507.02358)
*Anlin Zheng,Haochen Wang,Yucheng Zhao,Weipeng Deng,Tiancai Wang,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: Hita是一种新型图像分词器，通过全局到局部的分词方案和关键策略改进自回归图像生成，显著提升训练速度和生成质量。

- Motivation: 传统自回归图像生成模型逐步生成视觉标记，难以捕捉标记序列的整体关系，且现有视觉分词器缺乏全局信息。
- Method: Hita引入全局到局部的分词方案，结合可学习的全局查询和局部补丁标记，并采用因果注意力和轻量级融合模块优化信息流。
- Result: Hita在ImageNet基准测试中达到2.59 FID和281.9 IS，并在零样本风格迁移和图像修复中表现优异。
- Conclusion: Hita通过全局表示显著提升生成质量，适用于多种图像任务。


### [28] [LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling](https://arxiv.org/abs/2507.02363)
*Jiahao Wu,Rui Peng,Jianbo Jiao,Jiayu Yang,Luyang Tang,Kaiqiang Xiong,Jie Liang,Jinbo Yan,Runling Liu,Ronggang Wang*

Main category: cs.CV

TL;DR: LocalDyGS是一种动态场景重建框架，通过分解复杂动态场景为局部空间并解耦静态与动态特征，实现了对大尺度和小尺度运动场景的建模。

- Motivation: 由于现实世界中复杂且高度动态的运动，从多视角输入合成任意视角的动态视频具有挑战性。现有方法（如神经辐射场或3D高斯泼溅）难以建模大尺度运动，限制了其应用。
- Method: 1) 将复杂动态场景分解为由种子定义的局部空间，实现全局建模；2) 解耦静态与动态特征，静态特征捕获静态信息，动态残差场提供时间特定特征，生成时间高斯模型。
- Result: 在多种小尺度数据集上表现优异，并首次尝试建模更大、更复杂的高度动态场景。
- Conclusion: LocalDyGS为动态场景重建提供了一种更现实的建模方法，扩展了现有技术的应用范围。


### [29] [UVLM: Benchmarking Video Language Model for Underwater World Understanding](https://arxiv.org/abs/2507.02373)
*Xizhe Xue,Yang Zhou,Dawei Yan,Ying Li,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 论文提出了UVLM，一个水下观测基准数据集，用于填补现有视频语言模型（VidLMs）在陆地场景外的空白，特别针对水下环境的挑战。

- Motivation: 现有研究主要关注陆地场景，忽视了水下观测的高需求应用。UVLM旨在通过结合人类专业知识和AI模型，构建一个高质量、多样化的水下观测数据集。
- Method: 通过选择典型水下挑战的视频（如光线变化、水质浑浊和多视角）构建数据集，覆盖多种帧率、分辨率、419类海洋生物及静态植物和地形。任务设计分为生物和环境两大类，每类包括内容观察和变化/动作观察，共20种任务类型。
- Result: 实验表明，在UVLM上微调的VidLMs显著提升了对水下世界的理解，同时对现有空中VidLM基准（如VideoMME和Perception text）也有轻微改进。
- Conclusion: UVLM为水下观测提供了首个综合性基准，展示了其在实际应用中的潜力，并计划公开数据集和提示工程。


### [30] [PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection](https://arxiv.org/abs/2507.02393)
*Seokyeong Lee,Sithu Aung,Junyong Choi,Seungryong Kim,Ig-Jae Kim,Junghyun Cho*

Main category: cs.CV

TL;DR: 提出了一种仅使用视频数据的伪标签框架，用于单目3D目标检测（M3OD），无需多视角设置或额外传感器。

- Motivation: 解决M3OD中数据稀缺和2D到3D模糊性问题，克服现有弱监督和伪标签方法的局限性。
- Method: 通过对象点跟踪聚合静态和动态对象的伪LiDAR数据，提取3D属性。
- Result: 实验表明该方法具有可靠精度和强扩展性。
- Conclusion: 该方法为M3OD提供了一种实用有效的解决方案。


### [31] [Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis](https://arxiv.org/abs/2507.02395)
*Byung Hyun Lee,Wongi Jeong,Woojae Han,Kyoungbun Lee,Se Young Chun*

Main category: cs.CV

TL;DR: CoMEL提出了一种持续多实例学习框架，用于解决大规模图像（如WSI）的定位和适应性问题，通过GDAT、BPPL和OWLoRA技术显著提升性能。

- Motivation: 多实例学习（MIL）在大规模图像（如WSI）中通过弱标签降低了标注成本，但其在持续任务中的适应性和遗忘问题尚未充分研究。
- Method: CoMEL框架包括GDAT（高效实例编码）、BPPL（可靠伪标签生成）和OWLoRA（减少遗忘）三个关键技术。
- Result: 在三个公共WSI数据集上，CoMEL在持续MIL设置下的袋级和定位准确率分别比现有方法提升11.00%和23.4%。
- Conclusion: CoMEL为持续多实例学习提供了一种高效且适应性强的解决方案，显著提升了定位和分类性能。


### [32] [Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection](https://arxiv.org/abs/2507.02398)
*Taehoon Kim,Jongwook Choi,Yonghyun Jeong,Haeun Noh,Jaejun Yoo,Seungryul Baek,Jongwon Choi*

Main category: cs.CV

TL;DR: 提出了一种基于像素级时间不一致性的深度伪造视频检测方法，通过1D傅里叶变换提取时间特征，结合注意力机制和Transformer模块提升检测性能。

- Motivation: 传统基于空间频率的检测器忽略了像素级的时间不一致性，导致无法有效检测时间伪影。
- Method: 对每个像素进行时间轴上的1D傅里叶变换，提取时间不一致特征；引入注意力提议模块和联合Transformer模块整合时空特征。
- Result: 方法在多样化和挑战性检测场景中表现出色，显著提升了深度伪造视频检测的鲁棒性。
- Conclusion: 该框架为深度伪造视频检测提供了重要进展，尤其在检测时间伪影方面具有显著优势。


### [33] [TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation](https://arxiv.org/abs/2507.02399)
*Peilin Zhang,Shaouxan Wua,Jun Feng,Zhuo Jin,Zhizezhang Gao,Jingkun Chen,Yaqiong Xing,Xiao Zhang*

Main category: cs.CV

TL;DR: TAB Net提出了一种基于涂鸦标注的弱监督医学图像分割框架，通过三重增强自恢复模块和边界感知伪标签监督模块，显著提升了分割性能。

- Motivation: 医学图像分割需要大量标注数据，但标注成本高。涂鸦标注是一种稀疏标注方式，但其稀疏性限制了特征学习和边界监督。
- Method: TAB Net包含三重增强自恢复模块（TAS）和边界感知伪标签监督模块（BAP）。TAS通过三种增强策略提升特征学习，BAP通过双分支预测和边界感知损失优化伪标签和边界建模。
- Result: 在ACDC和MSCMR seg数据集上，TAB Net显著优于现有弱监督方法，性能接近全监督方法。
- Conclusion: TAB Net为稀疏标注的医学图像分割提供了一种高效解决方案，性能接近全监督方法。


### [34] [Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings](https://arxiv.org/abs/2507.02403)
*Mufhumudzi Muthivhi,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 本文研究了自监督学习在野生动物重识别中的应用，通过无监督方式提取图像对训练模型，实验表明自监督模型在数据有限时更鲁棒，且在下游任务中表现优于监督模型。

- Motivation: 当前野生动物重识别依赖标注数据，自监督学习可以避免这一限制，利用无标注数据训练模型。
- Method: 利用相机陷阱数据中的时间图像对自动提取个体视图，训练自监督模型。
- Result: 自监督模型在数据有限时更鲁棒，且在下游任务中表现优于监督模型。
- Conclusion: 自监督学习为野生动物重识别提供了更高效和鲁棒的解决方案。


### [35] [PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration](https://arxiv.org/abs/2507.02405)
*Ayantika Das,Moitreya Chaudhuri,Koushik Bhat,Keerthi Ram,Mihail Bota,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 该论文提出了一种结合扩散模型与自动编码器的方法，以学习图像特定的语义表示，并用于脑图像的区域特异性模式识别、撕裂伪影修复和JPEG伪影修复。

- Motivation: 扩散模型虽然能生成高质量图像，但缺乏提取图像特定语义表示的能力，而自动编码器能提供这种能力。通过结合两者，可以更好地组织潜在空间并实现特定任务。
- Method: 1. 设计了一种机制来结构化扩散自动编码模型的潜在空间，用于识别脑图像中的区域特异性模式。2. 提出了一种基于邻域感知的无监督撕裂伪影修复技术。3. 利用扩散模型的可控噪声和去噪能力，开发了无监督JPEG伪影修复技术。
- Result: 该方法能够有效识别脑图像中的组织类型，并成功修复撕裂伪影和JPEG伪影。
- Conclusion: 通过结合扩散模型与自动编码器，论文实现了图像特定语义表示的学习，并在脑图像分析和伪影修复中展示了其有效性。


### [36] [A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern](https://arxiv.org/abs/2507.02408)
*Duong Nguyen-Ngoc Tran,Long Hoang Pham,Chi Dai Tran,Quoc Pham-Nam Ho,Huy-Hung Nguyen,Jae Wook Jeon*

Main category: cs.CV

TL;DR: 论文提出了一种针对热成像中行人跟踪的新型调优方法，通过优化两阶段框架的超级参数，提升跟踪性能。

- Motivation: 热成像在低能见度或光线不足的环境中优于RGB相机，但其低层次特征表示使得行人检测和跟踪困难。
- Method: 提出两阶段调优框架，优化超级参数以适应热成像中的复杂运动模式。
- Result: 在PBVS Thermal MOT数据集上验证了方法的有效性，适用于多种热成像条件。
- Conclusion: 该方法无需复杂重识别或运动模型，即可实现高精度实时跟踪，适用于实际监控应用。


### [37] [Privacy-preserving Preselection for Face Identification Based on Packing](https://arxiv.org/abs/2507.02414)
*Rundong Xin,Taotao Wang,Jin Wang,Chonghe Zhao,Jing Wang*

Main category: cs.CV

TL;DR: 提出了一种名为PFIP的高效密文域人脸检索方案，通过预选机制和打包模块提升效率，实验表明其检索速度和准确性显著优于现有方法。

- Motivation: 随着密文模板库规模增大，人脸检索过程耗时增加，隐私保护和效率成为关键问题。
- Method: 结合预选机制减少计算开销，引入打包模块提升生物识别系统在注册阶段的灵活性。
- Result: 在LFW和CASIA数据集上，PFIP保持原始人脸识别模型的准确性，100%命中率，检索1000个密文人脸模板仅需300毫秒，效率提升近50倍。
- Conclusion: PFIP是一种高效且隐私保护的密文域人脸检索方案，显著提升了检索速度和系统灵活性。


### [38] [Determination Of Structural Cracks Using Deep Learning Frameworks](https://arxiv.org/abs/2507.02416)
*Subhasis Dasgupta,Jaydip Sen,Tuhina Halder*

Main category: cs.CV

TL;DR: 论文提出了一种基于残差U-Net和集成学习的新方法，用于提高结构裂缝检测的准确性和效率。

- Motivation: 手动检测结构裂缝存在速度慢、不一致和易出错的问题，影响评估可靠性。
- Method: 采用残差U-Net模型并结合集成学习，通过卷积块构建元模型提升预测效率。
- Result: 集成模型在IoU和DICE系数上表现最佳，优于传统U-Net和SegNet。
- Conclusion: 该方法为结构缺陷监测提供了更可靠的自动化解决方案。


### [39] [AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars](https://arxiv.org/abs/2507.02419)
*Yiming Zhong,Xiaolin Zhang,Ligang Liu,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: AvatarMakeup是一种基于扩散模型的3D化妆方法，通过从单张参考照片转移化妆模式，解决了现有方法在动态表情和多视角下的一致性、身份保持和细节控制方面的不足。

- Motivation: 虚拟化身的个性化化妆需求尚未充分探索，现有方法无法满足动态表情和多视角下的化妆一致性、身份保持和精细控制需求。
- Method: 采用从粗到细的策略，首先通过Coherent Duplication方法粗化化妆并确保一致性，然后通过Refinement Module细化细节。扩散模型用于生成化妆图像作为监督。
- Result: 实验表明，AvatarMakeup在化妆转移质量和动画一致性方面达到最先进水平。
- Conclusion: AvatarMakeup成功解决了3D虚拟化身化妆的关键问题，实现了高质量的化妆效果和一致性。


### [40] [F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning](https://arxiv.org/abs/2507.02437)
*Wei Li,Jingyang Zhang,Lihao Liu,Guoan Wang,Junjun He,Yang Chen,Lixu Gu*

Main category: cs.CV

TL;DR: 论文提出了一种自由形式的测试时间适应（F²TTA）任务，并设计了I-DiPT框架，通过图像不变和图像特定提示解决域碎片中的不可预测偏移问题。

- Motivation: 医疗数据通常以不规则的域碎片形式到达，现有TTA方法无法有效处理这种自由形式的数据流，导致适应过程失真。
- Method: 提出I-DiPT框架，结合图像不变提示和图像特定提示，并引入不确定性导向掩码（UoM）和平行图蒸馏（PGD）方法。
- Result: 在乳腺癌和青光眼分类任务中，I-DiPT优于现有TTA方法。
- Conclusion: I-DiPT框架有效解决了自由形式测试数据中的域偏移问题，提升了模型适应能力。


### [41] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: 论文提出使用FINN架构在FPGA上部署量化ANN模型，以提升机器人检测速度，减少任务执行时间。

- Motivation: 机器人因检测算法速度慢而降低移动速度，影响任务执行效率。现有工具未充分利用FPGA的PL资源。
- Method: 使用FINN架构在FPGA的PL上部署三种量化ANN模型（MobileNet v1、CNV 2-bit和CNV 1-bit），并在RG2C数据集上训练。
- Result: MobileNet v1表现最佳，成功率达98%，推理速度达6611 FPS。
- Conclusion: FPGA可加速ANN，适用于注意力机制，提升机器人检测效率。


### [42] [IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising](https://arxiv.org/abs/2507.02445)
*Hailong Yan,Junjian Huang,Tingwen Huang*

Main category: cs.CV

TL;DR: IGDNet是一种零样本图像增强方法，无需训练数据或先验知识，通过分解和去噪模块恢复低曝光图像，显著提升视觉质量。

- Motivation: 现有方法依赖成对数据集且易导致过增强，IGDNet旨在解决这些问题，实现无需训练的单图像增强。
- Method: IGDNet包含分解模块（分离光照与反射）和去噪模块（光照引导的像素自适应校正），通过迭代优化生成最终结果。
- Result: 在四个公开数据集上，IGDNet在PSNR（20.41dB）和SSIM（0.860dB）上优于14种无监督方法。
- Conclusion: IGDNet在复杂光照下表现优异，具有强泛化能力，代码即将发布。


### [43] [Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection](https://arxiv.org/abs/2507.02454)
*Weiwei Duan,Luping Ji,Shengjia Chen,Sicheng Zhu,Jianghong Huang,Mao Ye*

Main category: cs.CV

TL;DR: 提出了一种弱监督对比学习方案（WeCoL），用于红外小目标检测，减少对大量手动标注的依赖，性能接近全监督方法。

- Motivation: 红外小目标检测因目标尺寸小、背景对比度低而面临挑战，现有方法依赖大量手动标注，耗时耗力。
- Method: 基于预训练的SAM模型，设计潜在目标挖掘策略，结合对比学习提升伪标签可靠性，并提出长短期运动感知学习方案。
- Result: 在两个公开数据集上验证，弱监督方案性能优于早期全监督方法，接近SOTA全监督方法的90%。
- Conclusion: WeCoL方案有效减少标注需求，性能接近全监督方法，为红外小目标检测提供了新思路。


### [44] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: Mesh Silksong是一种高效的网格表示方法，通过自回归方式生成多边形网格，减少冗余并提升几何完整性。

- Motivation: 现有网格标记化方法存在顶点重复问题，浪费网络能力，因此需要一种更高效的表示方法。
- Method: 通过仅访问每个顶点一次的方式标记化网格顶点，减少50%的冗余，并实现约22%的压缩率。
- Result: 实验结果表明，该方法不仅能生成复杂网格，还能显著提升几何完整性（如流形拓扑、水密检测和一致法线）。
- Conclusion: Mesh Silksong在压缩率和几何特性上均达到先进水平，适用于实际应用。


### [45] [CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios](https://arxiv.org/abs/2507.02479)
*Teng Fu,Yuwen Chen,Zhuofan Chen,Mengyang Zhao,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 论文提出了一种名为“CrowdTrack”的大规模复杂场景多行人跟踪数据集，旨在解决现有数据集场景简单和非真实性的问题。

- Motivation: 现有多行人跟踪数据集场景简单且非真实，难以支持复杂场景下的算法开发。
- Method: 通过从第一人称视角拍摄真实复杂场景视频，构建包含33个视频和5,185条轨迹的数据集，并标注完整边界框和唯一对象ID。
- Result: 数据集为复杂场景下的算法开发提供了平台，并测试了多种SOTA模型和基础模型的性能。
- Conclusion: CrowdTrack数据集填补了现有数据集的不足，有助于推动复杂场景下的多行人跟踪研究。


### [46] [MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention](https://arxiv.org/abs/2507.02488)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: MedFormer是一种高效的医学视觉变换器，通过金字塔缩放结构和双稀疏选择注意力（DSSA）解决现有方法的通用性和计算效率问题。

- Motivation: 现有医学视觉变换器方法存在任务特定性、计算成本高或性能次优的问题，需要一种更通用且高效的解决方案。
- Method: 提出MedFormer，采用金字塔缩放结构作为通用骨干网络，并引入DSSA以提升计算效率和性能。
- Result: 实验表明，MedFormer在多种医学图像识别任务中表现优异，优于现有方法。
- Conclusion: MedFormer通过其通用性和高效性，显著提升了医学图像识别的性能。


### [47] [Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy](https://arxiv.org/abs/2507.02493)
*Luca Parolari,Andrea Cherubini,Lamberto Ballan,Carlo Biffi*

Main category: cs.CV

TL;DR: 提出了一种结合时间感知的监督对比损失方法，改进了结肠镜息肉计数任务，显著降低了碎片化率。

- Motivation: 现有息肉计数方法依赖自监督学习，忽略了时间关系，导致聚类效果不佳。本文旨在通过引入时间感知的监督对比损失，提升聚类的鲁棒性。
- Method: 提出了一种监督对比损失方法，结合时间感知软目标，并引入时间邻接约束改进轨迹聚类。
- Result: 在公开数据集上验证，碎片化率比现有方法降低了2.2倍。
- Conclusion: 时间感知对息肉计数至关重要，新方法达到了最新技术水平。


### [48] [MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations](https://arxiv.org/abs/2507.02494)
*Hyunsoo Son,Jeonghyun Noh,Suemin Jeon,Chaoli Wang,Won-Ki Jeong*

Main category: cs.CV

TL;DR: MC-INR是一种新型神经网络框架，通过结合元学习和聚类，解决了现有INR方法在复杂结构和多变量数据上的局限性，并引入动态重聚类和分支层进一步优化性能。

- Motivation: 现有INR方法在复杂结构、多变量数据和非结构化网格上的表现不佳，限制了其在实际科学数据中的应用。
- Method: 提出MC-INR框架，结合元学习和聚类，引入动态重聚类机制和分支层，以灵活处理多变量数据和非结构化网格。
- Result: 实验表明，MC-INR在科学数据编码任务中优于现有方法。
- Conclusion: MC-INR通过创新设计解决了INR方法的局限性，为复杂科学数据的编码提供了高效解决方案。


### [49] [Automatic Labelling for Low-Light Pedestrian Detection](https://arxiv.org/abs/2507.02513)
*Dimitrios Bouzoulas,Eerik Alamikkotervo,Risto Ojala*

Main category: cs.CV

TL;DR: 提出了一种自动红外-RGB标注流程，用于低光条件下的RGB行人检测，并在KAIST数据集上验证其效果优于人工标注。

- Motivation: 解决低光条件下RGB行人检测缺乏公开数据集的问题。
- Method: 1) 红外检测；2) 标签从红外检测结果转移到RGB图像；3) 使用生成标签训练低光RGB行人检测模型。
- Result: 在未见过的图像序列上，生成标签训练的模型在6/9情况下优于人工标注模型。
- Conclusion: 自动标注流程有效，可用于低光条件下的行人检测任务。


### [50] [Detecting Multiple Diseases in Multiple Crops Using Deep Learning](https://arxiv.org/abs/2507.02517)
*Vivek Yadav,Anugrah Jain*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的解决方案，用于检测多种作物中的多种疾病，旨在覆盖印度多样化的农业景观。

- Motivation: 印度作为以农业为主的经济体，面临作物损失的重大挑战，早期和准确的疾病检测对提高产量和确保粮食安全至关重要。
- Method: 创建了一个包含17种作物和34种疾病图像的统一数据集，并训练了一个深度学习模型。
- Result: 模型在统一数据集上实现了99%的检测准确率，比现有技术（覆盖14种作物和26种疾病）高出7%。
- Conclusion: 该解决方案通过提高可检测的作物和疾病类型数量，旨在为印度农民提供更好的工具。


### [51] [IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning](https://arxiv.org/abs/2507.02519)
*Abiam Remache González,Meriem Chagour,Timon Bijan Rüth,Raúl Trapiella Cañedo,Marina Martínez Soler,Álvaro Lorenzo Felipe,Hyun-Suk Shin,María-Jesús Zamorano Serrano,Ricardo Torres,Juan-Antonio Castillo Parra,Eduardo Reyes Abad,Miguel-Ángel Ferrer Ballester,Juan-Manuel Afonso López,Francisco-Mario Hernández Tejera,Adrian Penate-Sanchez*

Main category: cs.CV

TL;DR: IMASHRIMP是一个自动化白虾形态分析系统，通过改进的深度学习技术优化水产养殖中的遗传选择任务。

- Motivation: 传统虾类形态分析依赖人工，易出错且效率低，IMASHRIMP旨在通过自动化技术减少人为误差并提高效率。
- Method: 系统采用改进的ResNet-50架构进行视角分类和额剑完整性检测，结合VitPose进行姿态估计，并使用SVM模型将像素测量转换为厘米单位。
- Result: 系统显著降低了人为误差，姿态估计的mAP达97.94%，像素到厘米的转换误差为0.07 cm。
- Conclusion: IMASHRIMP展示了自动化虾类形态分析的潜力，有助于提升水产养殖的效率和可持续性。


### [52] [MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details](https://arxiv.org/abs/2507.02546)
*Ruicheng Wang,Sicheng Xu,Yue Dong,Yu Deng,Jianfeng Xiang,Zelong Lv,Guangzhong Sun,Xin Tong,Jiaolong Yang*

Main category: cs.CV

TL;DR: MoGe-2是一种先进的开放域几何估计模型，可从单张图像恢复场景的度量尺度3D点图。它在MoGe的基础上扩展，实现度量几何预测，并通过数据细化方法提升细节恢复能力。

- Motivation: 现有方法（如MoGe）只能预测未知尺度的仿射不变点图，无法满足度量几何需求。同时，真实数据中的噪声和误差会削弱几何细节。
- Method: 扩展MoGe以实现度量几何预测，开发统一数据细化方法，利用合成标签过滤和补全真实数据。
- Result: 模型在混合数据集上训练，评估显示其在相对几何精度、度量尺度和细节恢复方面均优于现有方法。
- Conclusion: MoGe-2首次同时实现了高精度的相对几何、度量尺度和细节恢复，填补了现有方法的空白。


### [53] [Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning](https://arxiv.org/abs/2507.02565)
*Buzhen Huang,Chen Li,Chongyang Xu,Dongyue Lu,Jinnan Chen,Yangang Wang,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出了一种基于双分支优化框架的方法，通过人类外观、社交距离和物理约束重建准确的人体交互动作。

- Motivation: 现有方法在复杂场景下难以准确估计人体姿态，尤其是在视觉模糊和遮挡情况下。
- Method: 使用扩散模型学习社交距离和姿态先验知识，结合双分支优化框架和多种约束（3D高斯、2D关键点、网格穿透）重建动作和外观。
- Result: 在多个基准测试中表现优于现有方法，并构建了带有伪真实标注的数据集。
- Conclusion: 该方法能够从复杂环境中准确估计人体交互动作，为未来研究提供了新工具和数据支持。


### [54] [Parametric shape models for vessels learned from segmentations via differentiable voxelization](https://arxiv.org/abs/2507.02576)
*Alina F. Dima,Suprosanna Shit,Huaqi Qiu,Robbie Holland,Tamara T. Mueller,Fabio Antonio Musio,Kaiyuan Yang,Bjoern Menze,Rickmer Braren,Marcus Makowski,Daniel Rueckert*

Main category: cs.CV

TL;DR: 提出了一种将体素化、网格和参数化模型结合的框架，通过可微分变换自动提取血管参数化形状模型。

- Motivation: 研究血管的多种表示方法（体素化、网格、参数化模型）通常独立使用，缺乏统一框架。
- Method: 利用可微分体素化从分割中学习形状参数，参数化血管为中心线和半径（使用三次B样条），并提取高保真网格。
- Result: 实验证明，该方法能准确捕捉复杂血管的几何形状，适用于主动脉、动脉瘤和脑血管。
- Conclusion: 该框架成功统一了三种表示方法，实现了从分割数据中自动学习形状参数并生成高质量网格。


### [55] [Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning](https://arxiv.org/abs/2507.02581)
*Tan Pan,Zhaorui Tan,Kaiyu Guo,Dongli Xu,Weidi Xu,Chen Jiang,Xin Guo,Yuan Qi,Yuan Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为$S^2DC$的自监督学习框架，通过结构感知的语义差异和一致性学习，解决了3D医学图像中解剖结构变化的问题。

- Motivation: 现有方法使用固定大小的图像块划分，忽略了医学图像中解剖结构的位置、尺度和形态变化，难以捕捉有意义的区分特征。
- Method: $S^2DC$通过两步实现结构感知表示：1) 利用最优传输策略增强不同图像块的语义差异；2) 基于邻域相似性分布提升结构级别的语义一致性。
- Result: 在10个数据集、4个任务和3种模态上的实验表明，$S^2DC$在自监督学习中优于现有方法。
- Conclusion: $S^2DC$通过结构感知的表示学习，显著提升了3D医学图像自监督学习的性能。


### [56] [AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/abs/2507.02591)
*Weili Xu,Enxin Song,Wenhao Chai,Xuexiang Wen,Tian Ye,Gaoang Wang*

Main category: cs.CV

TL;DR: AuroraLong提出了一种基于线性RNN的模型，用于解决长视频理解中的计算和内存问题，性能接近Transformer模型。

- Motivation: 长视频理解的计算复杂性和内存成本高，传统Transformer模型难以处理长序列输入。
- Method: 用线性RNN语言模型替代LLM，结合视觉令牌合并技术，按大小升序重排令牌以提高效率。
- Result: AuroraLong在多个视频基准测试中表现接近Transformer模型，且仅需2B参数和公开数据训练。
- Conclusion: 线性RNN模型有望降低长视频理解的计算门槛，推动其普及。


### [57] [Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development](https://arxiv.org/abs/2507.02602)
*Riccardo Gallon,Fabian Schiemenz,Alessandra Menicucci,Eberhard Gill*

Main category: cs.CV

TL;DR: 该研究通过模拟框架生成故障注入图像数据集，用于训练和测试基于AI的故障检测算法，以解决视觉导航中传感器故障检测的挑战。

- Motivation: 视觉导航算法在太空任务中的重要性日益增加，但传感器故障可能导致导航算法输出不准确或完全失效，传统故障检测方法存在局限性，而AI缺乏代表性数据集。
- Method: 研究分析了相机传感器的潜在故障案例，并通过模拟框架生成合成图像以重现故障条件，创建故障注入图像数据集。
- Result: 生成了一个系统性和可控的故障注入图像数据集，为AI故障检测算法提供了训练和测试工具。
- Conclusion: 该研究为AI在视觉导航故障检测中的应用提供了数据集支持，解决了代表性数据不足的问题。


### [58] [AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models](https://arxiv.org/abs/2507.02664)
*Ziyin Zhou,Yunpeng Luo,Yuanchen Wu,Ke Sun,Jiayi Ji,Ke Yan,Shouhong Ding,Xiaoshuai Sun,Yunsheng Wu,Rongrong Ji*

Main category: cs.CV

TL;DR: 论文提出了一种名为Holmes-Set的大规模数据集和Holmes Pipeline训练框架，用于解决AI生成图像（AIGI）检测中缺乏可解释性和泛化能力的问题。

- Motivation: AI生成图像（AIGI）的滥用威胁公共信息安全，现有检测技术缺乏可解释性和对新技术的泛化能力。
- Method: 提出Holmes-Set数据集（包括Holmes-SFTSet和Holmes-DPOSet）和Holmes Pipeline三阶段训练框架（预训练、监督微调、偏好优化），结合多专家评审和协作解码策略。
- Result: 在三个基准测试中验证了AIGI-Holmes模型的有效性。
- Conclusion: Holmes-Set和Holmes Pipeline显著提升了AIGI检测的可解释性和泛化能力。


### [59] [Learning few-step posterior samplers by unfolding and distillation of diffusion models](https://arxiv.org/abs/2507.02686)
*Charlesquin Kemajou Mbakam,Jonathan Spence,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 提出了一种结合深度展开和模型蒸馏的新框架，将扩散模型转化为高效的后验采样方法。

- Motivation: 扩散模型在贝叶斯计算成像中表现出强大的潜力，但现有方法存在灵活性或效率的局限性。
- Method: 通过深度展开和模型蒸馏，将扩散模型转化为少步条件模型，并首次将深度展开应用于蒙特卡洛采样方案。
- Result: 实验表明，该方法在准确性和计算效率上优于现有技术，同时保持对前向模型变化的适应性。
- Conclusion: 该框架为扩散模型在计算成像中的应用提供了高效且灵活的解决方案。


### [60] [APT: Adaptive Personalized Training for Diffusion Models with Limited Data](https://arxiv.org/abs/2507.02687)
*JungWoo Chae,Jiyoon Kim,JaeWoong Choi,Kyungyul Kim,Sangheum Hwang*

Main category: cs.CV

TL;DR: APT框架通过自适应训练调整、表示稳定化和注意力对齐，解决了扩散模型在有限数据下个性化训练中的过拟合问题，并保持了先验知识和文本对齐。

- Motivation: 解决扩散模型在有限数据下个性化训练中的过拟合、先验知识丢失和文本对齐退化问题。
- Method: 提出APT框架，包含自适应训练调整、表示稳定化和注意力对齐三个组件。
- Result: APT有效减少过拟合，保持先验知识，并在有限数据下生成高质量多样图像。
- Conclusion: APT为扩散模型的个性化训练提供了一种高效解决方案。


### [61] [CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation](https://arxiv.org/abs/2507.02691)
*Xiangyang Luo,Ye Zhu,Yunfei Liu,Lijian Lin,Cong Wan,Zijian Cai,Shao-Lun Huang,Yu Li*

Main category: cs.CV

TL;DR: CanonSwap提出了一种新的视频人脸交换框架，通过解耦运动与外观信息，解决了现有方法在保持目标动态属性上的不足。

- Motivation: 现有方法在身份转移上表现良好，但难以保持目标脸部的动态属性（如表情、姿态等），导致结果不一致。
- Method: CanonSwap通过解耦运动与外观信息，在统一规范空间进行身份修改，并通过Partial Identity Modulation模块精确控制身份特征。
- Result: 实验表明，CanonSwap在视觉质量、时间一致性和身份保持上显著优于现有方法。
- Conclusion: CanonSwap通过解耦和精确控制，实现了高质量的视频人脸交换，同时保持了目标动态属性。


### [62] [SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment](https://arxiv.org/abs/2507.02705)
*Qi Xu,Dongxu Wei,Lingzhe Zhao,Wenpu Li,Zhangchi Huang,Shunping Ji,Peidong Liu*

Main category: cs.CV

TL;DR: SIU3R是一个无需对齐的框架，通过像素对齐的3D表示实现通用化的同时理解和3D重建，避免了2D到3D特征对齐的局限性。

- Motivation: 现有方法依赖2D到3D特征对齐，导致3D理解能力有限和语义信息丢失。
- Method: SIU3R通过像素对齐的3D表示桥接重建和理解任务，并统一多个理解任务为一组可学习的查询。设计了两个轻量模块促进任务协作。
- Result: 实验表明，SIU3R在3D重建、理解及同时任务上均达到最先进性能。
- Conclusion: SIU3R的无对齐框架和协作设计有效提升了同时理解和3D重建的能力。


### [63] [UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation](https://arxiv.org/abs/2507.02713)
*Qin Guo,Ailing Zeng,Dongxu Yue,Ceyuan Yang,Yang Cao,Hanzhong Guo,Fei Shen,Wei Liu,Xihui Liu,Dan Xu*

Main category: cs.CV

TL;DR: 论文提出UniMC框架和HAIG-2.9M数据集，解决了现有关键点引导模型在生成非刚性物体和多重叠对象时的局限性。

- Motivation: 现有关键点引导模型难以生成非刚性物体（如动物）和多重叠对象，主要受限于方法和数据集的不足。
- Method: 设计基于DiT的UniMC框架，整合实例和关键点条件为紧凑令牌；构建HAIG-2.9M数据集，包含丰富标注和严格质检。
- Result: 实验证明HAIG-2.9M的高质量和UniMC的有效性，尤其在遮挡和多类场景中表现突出。
- Conclusion: UniMC和HAIG-2.9M为关键点引导的图像生成提供了更通用的解决方案。


### [64] [FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models](https://arxiv.org/abs/2507.02714)
*Yuxuan Wang,Tianwei Cao,Huayu Zhang,Zhongjiang He,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: FairHuman提出了一种多目标微调方法，通过全局和局部学习目标提升人像生成的细节质量。

- Motivation: 现有的大规模文本到图像模型在生成人像时，局部细节（如面部和手部）质量不足，缺乏训练监督。
- Method: 构建了全局和局部（面部和手部）学习目标，基于最小潜在延迟准则优化参数更新策略。
- Result: 实验表明，该方法显著提升了局部细节生成质量，同时保持了整体生成效果。
- Conclusion: FairHuman通过多目标优化有效解决了人像生成中的局部细节问题。


### [65] [Prompt learning with bounding box constraints for medical image segmentation](https://arxiv.org/abs/2507.02743)
*Mélanie Gaillochet,Mehrdad Noori,Sahar Dastani,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

TL;DR: 提出了一种结合基础模型和弱监督分割的新框架，仅需边界框标注即可自动生成提示，优化方案整合了边界框约束和伪标签，实验表明其性能优于现有方法。

- Motivation: 医学图像像素级标注成本高，弱监督方法（如边界框标注）更实用，但现有提示学习方法依赖全标注掩码。
- Method: 利用基础模型的表示能力，仅通过边界框标注自动生成提示，优化方案整合边界框约束和伪标签。
- Result: 在多模态数据集上，平均Dice分数达84.90%，优于全监督和弱监督方法。
- Conclusion: 该框架有效结合了基础模型和弱监督分割，显著减少了标注需求，性能优越。


### [66] [DexVLG: Dexterous Vision-Language-Grasp Model at Scale](https://arxiv.org/abs/2507.02747)
*Jiawei He,Danshi Li,Xinqiang Yu,Zekun Qi,Wenyao Zhang,Jiayi Chen,Zhaoxiang Zhang,Zhizheng Zhang,Li Yi,He Wang*

Main category: cs.CV

TL;DR: DexVLG是一个基于视觉-语言-动作（VLA）的大型模型，用于预测灵巧手的抓取姿态，并通过语言指令对齐。通过生成大规模数据集DexGraspNet 3.0，模型在仿真和真实场景中表现出色。

- Motivation: 当前研究主要关注简单夹爪的控制，缺乏对灵巧手的功能性抓取研究。DexVLG旨在填补这一空白。
- Method: 生成170万灵巧抓取姿态数据集，训练VLM和基于流匹配的姿态预测模型，使用单视角RGBD输入。
- Result: 在仿真和真实实验中，DexVLG实现76%的零样本执行成功率和领先的部件抓取精度。
- Conclusion: DexVLG展示了强大的零样本泛化能力，为灵巧手的语言对齐抓取提供了有效解决方案。


### [67] [Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics](https://arxiv.org/abs/2507.02748)
*Alex Colagrande,Paul Caillon,Eva Feillet,Alexandre Allauzen*

Main category: cs.CV

TL;DR: MANO是一种基于距离的多尺度注意力机制，解决了标准Transformer在高分辨率输入下的计算和内存问题。

- Motivation: 标准Transformer在处理高分辨率输入时存在二次复杂度问题，现有方法往往牺牲细节。
- Method: 将注意力建模为网格点间的交互问题，引入多极注意力神经网络算子（MANO），实现线性复杂度。
- Result: MANO在图像分类和Darcy流任务中表现优异，显著降低了运行时和内存占用。
- Conclusion: MANO为高分辨率输入提供了一种高效且性能优越的Transformer替代方案。


### [68] [Partial Weakly-Supervised Oriented Object Detection](https://arxiv.org/abs/2507.02751)
*Mingxin Liu,Peiyuan Zhang,Yuan Liu,Wei Zhang,Yue Zhou,Ning Liao,Ziyang Gong,Junwei Luo,Zhirui Wang,Yi Yu,Xue Yang*

Main category: cs.CV

TL;DR: 提出了一种基于部分弱标注的PWOOD框架，显著降低了标注成本，并在性能上优于传统半监督算法。

- Motivation: 解决定向目标检测中高标注成本的问题，利用部分弱标注（水平框或单点）提高效率。
- Method: 提出PWOOD框架、OS-Student模型和CPF策略，利用弱标注数据学习定向和尺度信息。
- Result: 在多个数据集上表现优于传统半监督算法，性能接近或超过完全监督方法。
- Conclusion: PWOOD框架为定向目标检测提供了一种低成本、高性能的解决方案。


### [69] [From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images](https://arxiv.org/abs/2507.02781)
*Danrong Zhang,Huili Huang,N. Simrill Smith,Nimisha Roy,J. David Frost*

Main category: cs.CV

TL;DR: 该研究提出了一种基于语义分割的地震后社交媒体图像损害严重性评估方法，通过构建分段损害数据集并微调SegFormer模型，实现了更客观和全面的损害分析。

- Motivation: 传统的地震后损害评估方法依赖分类，主观性强且无法处理图像中不同程度的损害。
- Method: 构建分段损害数据集（未受损、受损、废墟），微调SegFormer模型进行语义分割，并引入新的损害评分系统。
- Result: 方法能够更客观地量化社交媒体图像中的损害严重性，为灾害侦察提供精确指导。
- Conclusion: 该研究通过语义分割和评分系统，提升了地震后损害评估的客观性和精确性，有助于更有效的灾害响应。


### [70] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态叙事理解的自动视频编辑框架（HIVE），通过角色提取、对话分析和叙事摘要提升视频编辑质量，并在新数据集DramaAD上验证了其优越性。

- Motivation: 现有自动视频编辑方法主要依赖文本线索，忽略视觉上下文，导致输出不连贯。
- Method: 结合多模态大语言模型进行角色提取、对话分析和叙事摘要，并分解编辑任务为高光检测、开头/结尾选择和内容修剪。
- Result: 在DramaAD数据集上，HIVE框架显著优于现有基线，缩小了自动与人工编辑视频的质量差距。
- Conclusion: HIVE框架通过多模态叙事理解提升了视频编辑的连贯性和质量，为未来研究提供了新方向。


### [71] [RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation](https://arxiv.org/abs/2507.02792)
*Liheng Zhang,Lexi Pang,Hang Ye,Xiaoxuan Ma,Yizhou Wang*

Main category: cs.CV

TL;DR: 本文提出了一种灵活的文本到图像扩散模型特征注入框架，解决了现有方法在结构对齐和视觉质量上的问题。

- Motivation: 现有特征注入方法在结构对齐、条件泄漏和视觉伪影方面存在问题，尤其是在条件图像与自然RGB分布差异较大时。
- Method: 提出了一种解耦注入时间步和去噪过程的框架，包括结构丰富的注入模块、外观丰富的提示和重启细化策略。
- Result: 实验表明，该方法在多样化的零样本条件场景中实现了最先进的性能。
- Conclusion: 该方法无需训练即可生成结构丰富且外观丰富的图像，显著提升了生成质量。


### [72] [No time to train! Training-Free Reference-Based Instance Segmentation](https://arxiv.org/abs/2507.02798)
*Miguel Espinosa,Chenhongyi Yang,Linus Ericsson,Steven McDonagh,Elliot J. Crowley*

Main category: cs.CV

TL;DR: 论文提出了一种基于参考图像的自动目标分割方法，利用基础模型的语义先验，减少对人工提示的依赖。

- Motivation: 解决传统图像分割模型依赖大规模标注数据的问题，以及SAM模型仍需人工提示的局限性。
- Method: 采用多阶段、无需训练的方法，包括记忆库构建、表示聚合和语义感知特征匹配。
- Result: 在多个基准测试中表现优异，如COCO FSOD（36.8% nAP）、PASCAL VOC Few-Shot（71.2% nAP50）和跨域FSOD（22.4% nAP）。
- Conclusion: 该方法通过语义先验和特征匹配，实现了高效且无需训练的自动分割，显著提升了性能。


### [73] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: HyperGaussians是一种基于3D高斯泼溅的新方法，用于高质量可动画化人脸头像，解决了现有方法在非线性变形和复杂光照下的不足。

- Motivation: 现有3D高斯泼溅方法在静态人脸表现优秀，但在可动画化头像上仍存在不足，尤其是在非线性变形和复杂光照下。
- Method: 提出HyperGaussians，将3D高斯扩展到高维多变量高斯，通过可学习的局部嵌入增强表达能力，并引入逆协方差技巧提升计算效率。
- Result: 在4个数据集上的19个受试者中，HyperGaussians在数值和视觉上均优于3DGS，尤其在高频细节如眼镜框、牙齿和复杂面部动作上表现突出。
- Conclusion: HyperGaussians通过高维表示和计算优化，显著提升了可动画化人脸头像的质量，适用于增强和虚拟现实应用。


### [74] [LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion](https://arxiv.org/abs/2507.02813)
*Fangfu Liu,Hao Li,Jiawei Chi,Hanyang Wang,Minghui Yang,Fudong Wang,Yueqi Duan*

Main category: cs.CV

TL;DR: LangScene-X是一个新颖的生成框架，通过稀疏视图生成一致的多模态信息，实现3D重建和理解。

- Motivation: 解决现有方法在稀疏视图下渲染和语义合成效果不佳的问题。
- Method: 结合TriMap视频扩散模型和语言量化压缩器（LQC），生成RGB、几何和语义信息，并重建语言表面场。
- Result: 在真实数据上表现优于现有方法，质量和泛化能力更强。
- Conclusion: LangScene-X为稀疏视图下的3D语言嵌入场景重建提供了高效解决方案。


### [75] [Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach](https://arxiv.org/abs/2507.02826)
*Panpan Ji,Junni Song,Hang Xiao,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 提出了一种名为DCDP-HAR的动态对比双路径网络框架，用于解决多模态HAR系统中的跨模态特征对齐和模态贡献不平衡问题。

- Motivation: 多模态HAR系统面临跨模态特征对齐困难和模态贡献不平衡的挑战。
- Method: 采用双路径特征提取架构（ResNet和DenseNet分支）、多阶段对比学习机制和置信度驱动的梯度调制策略。
- Result: 在四个公共基准数据集上进行了广泛实验，验证了各组成部分的有效性。
- Conclusion: DCDP-HAR框架有效解决了多模态HAR系统中的关键问题，提升了性能。


### [76] [USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network](https://arxiv.org/abs/2507.02827)
*Ying Yu,Hang Xiao,Siyao Li,Jiarui Li,Haotian Tang,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于多注意力交互机制的综合优化方法，用于解决人类活动识别（HAR）中的标签数据稀缺、特征提取不足和设备性能问题。方法包括无监督数据增强、多分支时空交互网络和自适应多损失函数融合，实验表明其显著优于现有方法。

- Motivation: 解决HAR中标签数据稀缺、高维特征提取不足以及轻量设备性能不佳的问题。
- Method: 1. 使用无监督统计引导扩散模型进行数据增强；2. 设计多分支时空交互网络，结合时空注意力机制；3. 引入自适应多损失函数融合策略。
- Result: 在WISDM、PAMAP2和OPPORTUNITY数据集上分别达到98.84%、93.81%和80.92%的准确率，优于现有方法。
- Conclusion: 提出的USAD方法有效解决了HAR中的关键问题，并在实际部署中验证了其高效性和可行性。


### [77] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

TL;DR: 该论文提出了一种新型视觉中心越狱攻击（VisCo攻击），通过视觉信息构建完整的越狱场景，显著提高了攻击效果。

- Motivation: 多模态大语言模型（MLLMs）在现实应用中存在安全漏洞，尤其是视觉模态的脆弱性。现有方法缺乏现实场景的语义基础，因此需要一种更有效的视觉中心攻击方法。
- Method: 提出VisCo攻击，通过四种视觉策略构建上下文对话，动态生成辅助图像，并结合自动毒性模糊和语义优化生成最终攻击提示。
- Result: VisCo在MM-SafetyBench上对GPT-4o的毒性得分为4.78，攻击成功率为85%，显著优于基线方法。
- Conclusion: VisCo攻击展示了视觉中心越狱场景的有效性，为MLLMs的安全性研究提供了新方向。


### [78] [AnyI2V: Animating Any Conditional Image with Motion Control](https://arxiv.org/abs/2507.02857)
*Ziye Li,Hao Luo,Xincheng Shuai,Henghui Ding*

Main category: cs.CV

TL;DR: AnyI2V是一种无需训练的框架，通过用户定义的运动轨迹为任意条件图像生成动画，支持多种模态输入，并提供灵活的视频生成和编辑功能。

- Motivation: 现有文本到视频（T2V）和图像到视频（I2V）方法在动态运动信号和空间约束整合方面存在不足，缺乏精确控制和编辑灵活性。
- Method: 提出AnyI2V框架，支持多种模态的条件图像输入（如网格和点云），无需训练即可实现运动控制和风格编辑。
- Result: 实验表明，AnyI2V在空间和运动控制的视频生成中表现优异，支持混合输入和风格转换。
- Conclusion: AnyI2V为视频生成提供了新的视角，解决了现有方法的局限性，具有更高的灵活性和性能。


### [79] [Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation](https://arxiv.org/abs/2507.02859)
*Jiaer Xia,Bingkui Tong,Yuhang Zang,Rui Shao,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 论文提出GCoT方法，通过注入边界框信息改进CoT数据，提升MLLM在数据有限情况下对专业视觉任务的适应能力。

- Motivation: MLLM在专业视觉任务（如图表理解）中表现不佳，因预训练与下游数据集不匹配，且CoT数据中存在事实错误。
- Method: 提出GCoT方法，通过注入边界框信息改进CoT数据，使其更忠实于输入图像。
- Result: 在五种专业视觉任务上，GCoT显著优于微调和蒸馏方法。
- Conclusion: GCoT能有效提升MLLM在数据有限情况下对专业视觉任务的性能。


### [80] [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](https://arxiv.org/abs/2507.02860)
*Xin Zhou,Dingkang Liang,Kaijin Chen,Tianrui Feng,Xiwu Chen,Hongkai Lin,Yikang Ding,Feiyang Tan,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: EasyCache是一种无需训练的加速框架，通过动态重用计算过的变换向量，显著提升视频扩散模型的推理速度，同时保持高视觉质量。

- Motivation: 视频生成模型的推理速度慢和计算成本高限制了其广泛应用，需要解决这一瓶颈以推动技术普及。
- Method: 提出EasyCache，采用轻量级、运行时自适应的缓存机制，避免冗余计算，无需离线分析或参数调优。
- Result: 在多个大规模视频生成模型上实现2.1-3.3倍的推理加速，PSNR提升高达36%。
- Conclusion: EasyCache是一种高效且易于使用的解决方案，适用于高质量视频生成的研究和实际应用。


### [81] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality是一种将RGB-D扫描转换为紧凑、逼真且交互式3D虚拟副本的新方法，支持高质量渲染和物理交互，适用于AR/VR、游戏和机器人等领域。

- Motivation: 将现实世界的RGB-D扫描转换为逼真且可交互的3D虚拟场景，以支持图形管线需求，如对象独立性、物理渲染和交互。
- Method: 通过场景理解生成结构化场景图，检索3D模型库中的相似模型，使用材料绘画模块增强真实感，并集成到仿真引擎中实现交互。
- Result: 在Scan2CAD基准测试中实现最先进的相似性性能，材料绘画模块能处理严重不对齐和遮挡的图像。
- Conclusion: LiteReality生成的场景紧凑、可编辑且兼容标准图形管线，适用于多种应用场景。


### [82] [RefTok: Reference-Based Tokenization for Video Generation](https://arxiv.org/abs/2507.02862)
*Xiang Fan,Xiaohang Sun,Kushan Thakkar,Zhu Liu,Vimal Bhat,Ranjay Krishna,Xiang Hao*

Main category: cs.CV

TL;DR: RefTok是一种基于参考的标记化方法，显著提升了视频模型处理时间冗余的能力，优于现有方法。

- Motivation: 现有方法未能有效捕捉视频中的时间依赖性和冗余性，限制了视频模型的性能。
- Method: 引入RefTok，通过基于未量化参考帧的编码和解码，捕捉复杂时间动态和上下文信息。
- Result: 在四个数据集上，RefTok显著优于现有方法，平均提升36.7%；在视频生成任务中，性能提升27.9%。
- Conclusion: RefTok通过有效处理时间冗余，显著提升了视频模型的性能。


### [83] [Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory](https://arxiv.org/abs/2507.02863)
*Yuqi Wu,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Point3R提出了一种在线密集流式3D重建框架，通过显式空间指针内存解决隐式内存容量限制和信息丢失问题。

- Motivation: 现有方法（如DUSt3R）使用隐式内存进行密集3D重建，但存在容量限制和早期帧信息丢失的问题。
- Method: Point3R维护显式空间指针内存，每个指针关联3D位置并聚合场景信息，结合3D分层位置嵌入和高效融合机制。
- Result: 方法在多个任务中表现优异，训练成本低。
- Conclusion: Point3R通过显式指针内存实现了高效且准确的在线3D重建。
## cs.GR

### [84] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 提出了一种高效的图像照明近似方法，用于动态材质和环境贴图的实时渲染，特别针对闪烁或闪光效果。

- Motivation: 解决在实时渲染中，材料表面离散微面导致的闪烁或闪光效果难以高效模拟的问题。
- Method: 基于实时区域光照明下的闪光渲染，采用标准环境贴图滤波技术，快速处理每帧数据，并通过分区环境贴图和正态分布函数计算微面反射概率。
- Result: 验证了该方法在多种材质和光照条件下接近真实渲染效果，性能稳定，仅需两倍于平滑材质的内存开销。
- Conclusion: 该方法为实时渲染中的闪光效果提供了一种高效且接近真实的解决方案。
## cs.CL

### [85] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX提出了一种基于LoRA模块的持续领域自适应预训练方法，解决了现有方法的高计算成本、数据顺序敏感性和单一模型问题。

- Motivation: 现有持续领域自适应预训练方法存在高计算成本、数据顺序敏感性和单一模型问题，限制了其实际应用。
- Method: 利用LoRA模块（一种参数高效微调方法）实现高效、并行且对领域顺序鲁棒的预训练，并为特定任务提供定制化模型。
- Result: DoMIX在持续领域自适应预训练中表现高效且鲁棒，并能扩展到标准LLM微调场景。
- Conclusion: DoMIX通过LoRA模块解决了持续领域自适应预训练的关键问题，具有广泛的应用潜力。
## eess.IV

### [86] [CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR](https://arxiv.org/abs/2507.02289)
*Wangbin Ding,Lei Li,Junyi Qiu,Bogen Lin,Mingjing Yang,Liqin Huang,Lianming Wu,Sihan Wang,Xiahai Zhuang*

Main category: eess.IV

TL;DR: 提出了一种名为CineMyoPS的端到端深度神经网络，仅通过cine CMR图像分割心肌病理（疤痕和水肿），结合运动和解剖特征，提高了分割准确性。

- Motivation: 心肌梗死（MI）是全球主要死因之一，现有LGE和T2加权CMR成像技术耗时且有侵入性，而cine CMR快速无对比剂，但需有效利用其信息进行病理分割。
- Method: 设计CineMyoPS网络，提取MI相关运动和解剖特征，采用一致性损失促进联合学习，并提出时间序列聚合策略整合心脏周期内的特征。
- Result: 在多中心数据集上验证，CineMyoPS在心肌病理分割、运动估计和解剖分割中表现优异。
- Conclusion: CineMyoPS为无对比剂、快速的心肌病理分割提供了有效解决方案，具有临床应用潜力。


### [87] [A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\left[^{18}\text{F}\right]$FDG PET imaging](https://arxiv.org/abs/2507.02367)
*Christian Salomonsen,Luigi Tommaso Luppino,Fredrik Aspheim,Kristoffer Wickstrøm,Elisabeth Wetzer,Michael Kampffmeyer,Rodrigo Berzaghi,Rune Sundset,Robert Jenssen,Samuel Kuttner*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的非侵入性方法（FC-DLIF），用于从小动物动态PET成像中预测动脉输入函数，避免了传统血液采样的复杂性。

- Motivation: 传统动脉血液采样在小动物研究中复杂且不可行，限制了纵向研究的开展。
- Method: 使用全卷积深度学习模型（FC-DLIF），从PET图像中提取时空特征以预测输入函数。
- Result: FC-DLIF能可靠预测输入函数，对时间偏移和扫描时长变化具有鲁棒性，但对未训练过的示踪剂表现不佳。
- Conclusion: FC-DLIF为小动物动态PET研究提供了一种非侵入性、可靠的替代方案。


### [88] [3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices](https://arxiv.org/abs/2507.02411)
*Zhurong Chen,Jinhua Chen,Wei Zhuo,Wufeng Xue,Dong Ni*

Main category: eess.IV

TL;DR: 提出了一种从2D超声切片重建个性化3D心脏解剖结构的创新框架，显著提高了左心室和右心室体积估计的准确性。

- Motivation: 传统2D超声成像难以准确估计临床参数（如左心室体积），而3D超声成像又受限于分辨率和手动标注的高要求。
- Method: 设计了一种新颖的3D重建流程，通过交替优化2D切片的3D姿态估计和基于隐式神经网络的3D切片整合，逐步将先验3D心脏形状转化为个性化模型。
- Result: 使用六个平面时，重建的3D心脏显著提高了左心室体积估计的准确性（误差1.98% vs. 20.24%），并首次实现了从2D切片估计右心室体积（误差5.75%）。
- Conclusion: 该方法为心脏超声的个性化3D结构和功能分析提供了新途径，具有重要临床潜力。


### [89] [MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak Boundary Polyp Detection](https://arxiv.org/abs/2507.02668)
*Zhe Yee Tan*

Main category: eess.IV

TL;DR: MEGANet-W是一种基于小波的边缘引导注意力网络，用于提高结肠息肉分割的准确性，无需额外可学习参数。

- Motivation: 结肠息肉分割对早期癌症检测至关重要，但现有方法在弱对比边界下表现不佳。
- Method: 提出MEGANet-W，利用Haar小波边缘图和WEGA模块融合多方向边缘信息。
- Result: 在五个公共数据集上，MEGANet-W显著优于现有方法，mIoU提升2.3%，mDice提升1.2%。
- Conclusion: MEGANet-W通过小波边缘引导有效提升了分割精度，且无需额外参数。
## cs.RO

### [90] [MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping](https://arxiv.org/abs/2507.02672)
*Qingyu Fan,Yinghao Cai,Chao Li,Chunting Jiao,Xudong Zheng,Tao Lu,Bin Liang,Shuo Wang*

Main category: cs.RO

TL;DR: MISCGrasp是一种体积抓取方法，通过多尺度特征提取和对比特征增强实现自适应抓取，优于基线方法。

- Motivation: 解决机器人抓取在适应不同形状和大小物体时的挑战。
- Method: 结合多尺度特征提取与对比特征增强，使用Insight Transformer和Empower Transformer实现特征交互与选择。
- Result: 在模拟和真实环境中，MISCGrasp在桌面整理任务中表现优于基线方法。
- Conclusion: MISCGrasp通过多尺度对比学习和特征增强，实现了高效的自适应抓取。


### [91] [MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real](https://arxiv.org/abs/2507.02864)
*Renhao Wang,Haoran Geng,Tingle Li,Feishi Wang,Gopala Anumanchipalli,Philipp Wu,Trevor Darrell,Boyi Li,Pieter Abbeel,Jitendra Malik,Alexei A. Efros*

Main category: cs.RO

TL;DR: 论文提出MultiGen框架，利用生成模型增强物理模拟器，实现多感官模拟，成功应用于机器人倒水任务，展示了零样本迁移到真实世界的潜力。

- Motivation: 解决多模态策略学习在真实世界中的挑战，尤其是难以模拟的感官模态（如声音），以及多模态模拟到现实的迁移问题。
- Method: 引入MultiGen框架，结合大规模生成模型与传统物理模拟器，生成逼真的音频以配合模拟视频，从而训练多模态策略。
- Result: 在机器人倒水任务中，无需真实机器人数据即可训练，并成功实现零样本迁移到新容器和液体的真实场景。
- Conclusion: 生成模型能够模拟难以建模的感官模态，并缩小多模态模拟到现实的差距，具有广泛应用潜力。
## q-bio.QM

### [92] [TubuleTracker: a high-fidelity shareware software to quantify angiogenesis architecture and maturity](https://arxiv.org/abs/2507.02024)
*Danish Mahmood,Stephanie Buczkowski,Sahaj Shah,Autumn Anthony,Rohini Desetty,Carlo R Bartoli*

Main category: q-bio.QM

TL;DR: tubuleTracker是一种快速、客观的软件工具，用于量化内皮细胞网络的架构和成熟度，优于手动和ImageJ分析。

- Motivation: 解决手动分析内皮细胞网络的时间消耗和主观性问题，以及现有自动化工具（如ImageJ）的速度和准确性不足。
- Method: 使用tubuleTracker和ImageJ分析54张内皮细胞网络图像，并与手动分析结果对比，评估关键指标如管状结构数量、总长度、节点数等。
- Result: tubuleTracker分析速度最快（6秒/图像），且其指标（如管状结构数量、长度等）与血管生成成熟度显著相关。
- Conclusion: tubuleTracker在速度和一致性上优于手动和ImageJ分析，尤其血管圆形度能有效反映成熟度，已作为免费软件提供。
## cs.AI

### [93] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的核心目标，强调其跨领域的重要性和结构化特性。

- Motivation: 运动在生物和人工智能系统中具有核心意义，但当前AI研究常忽视其作为独立模态的潜力。
- Method: 提出将运动视为结构化、低维的建模目标，利用其物理约束和形态共性。
- Result: 通过跨领域运动数据建模，有望提升生成模型和控制能力，并为行为理解提供统一框架。
- Conclusion: 运动不仅是行为的结果，更是智能系统与世界互动的窗口，应成为AI研究的重点。
## cs.LG

### [94] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 论文提出了一种新型的Energy-Based Transformers（EBTs），通过无监督学习实现类似人类System 2 Thinking的推理能力，并在多种任务中表现优于现有方法。

- Motivation: 现有推理时计算方法存在模态或问题特定性，且需要额外监督训练。本文旨在探索是否可以通过无监督学习实现通用的System 2 Thinking。
- Method: 训练EBTs，一种基于能量的模型，通过梯度下降能量最小化实现预测，适用于离散和连续模态。
- Result: EBTs在训练和推理中均优于Transformer++和Diffusion Transformers，数据扩展率提高35%，语言任务性能提升29%，图像去噪效果更好。
- Conclusion: EBTs是一种有前景的新范式，能够同时提升模型的学习和推理能力。


### [95] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 提出了一种高效的潜在扩散框架，结合变分自编码器和条件扩散模型，显著提升数据压缩性能。

- Motivation: 生成模型在条件设置下表现优异，但可控性和重建精度不足限制了其实际应用。
- Method: 使用变分自编码器和条件扩散模型，仅压缩少量关键帧并通过生成插值重建其余帧。
- Result: 在多个数据集上，压缩比提升10倍，性能比学习型方法高63%。
- Conclusion: 该方法在保持高重建精度的同时显著降低了存储成本。


### [96] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: 提出了一种名为AMR的轻量级方法，用于动态数据流中的持续学习，通过选择性更新记忆缓冲区来应对概念漂移，性能接近完全重新学习但成本更低。

- Motivation: 传统持续学习方法假设数据分布静态，忽视了真实数据流的动态性，尤其是概念漂移问题。
- Method: 提出了AMR方法，通过选择性移除过时样本并补充新样本，动态调整记忆缓冲区以应对概念漂移。
- Result: AMR在多个数据集上表现优异，接近完全重新学习的性能，同时显著降低了计算和标注成本。
- Conclusion: AMR是一种可扩展的解决方案，在非静态持续学习环境中平衡了稳定性和可塑性。


### [97] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: L-VAE是一种新型模型，通过学习损失函数的超参数和模型参数，动态平衡解缠和重构损失，优于现有VAE变体。

- Motivation: 解决β-VAE中超参数η需手动调整的问题，实现动态平衡解缠和重构损失。
- Method: 提出L-VAE模型，通过学习损失函数权重和模型参数，并添加正则化项防止偏置。
- Result: 实验表明L-VAE在解缠和重构性能上优于β-VAE等模型，并在多个数据集上表现最佳或次佳。
- Conclusion: L-VAE成功实现了解缠和重构的动态平衡，适用于多种数据集和任务。


### [98] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: 论文提出DAID框架，通过因果分析解决Deepfake检测中公平性与泛化性的冲突，并在实验中验证其优越性。

- Motivation: Deepfake检测模型在泛化性和公平性之间存在冲突，现有方法难以兼顾两者。
- Method: 提出DAID框架，包括数据重平衡和特征聚合两部分，通过因果分析优化公平性和泛化性。
- Result: 在三个跨域基准测试中，DAID在公平性和泛化性上均优于现有方法。
- Conclusion: DAID通过因果分析有效解决了公平性与泛化性的冲突，具有理论和实践意义。


### [99] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私生成模型的数据共享方法，通过联合训练差分隐私条件变分自编码器（DP-CVAE），解决医疗影像中数据稀缺和隐私问题，同时提升效率和灵活性。

- Motivation: 深度学习在医疗影像中的应用受限于数据稀缺和隐私法规，联邦学习虽支持去中心化训练，但存在通信成本高和任务单一的问题。
- Method: 采用基础模型提取紧凑的信息嵌入，联合训练差分隐私条件变分自编码器（DP-CVAE），建模全局隐私感知数据分布。
- Result: 方法在多个特征提取器上验证，提升了隐私性、可扩展性和效率，优于传统联邦学习分类器，同时生成更高保真度的嵌入。
- Conclusion: 提出的DP-CVAE方法在保护隐私的同时，显著提升了数据共享的效率和灵活性，适用于多样化的下游任务。
