[[toc]]

## cs.CV

### [1] [RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration](https://arxiv.org/abs/2508.04797)
*Mohab Kishawy,Ali Abdellatif Hussein,Jun Chen*

Main category: cs.CV

TL;DR: RetinexDual是一种基于Retinex理论的新型框架，用于超高清图像修复任务，通过两个互补子网络（SAMBA和FIA）克服传统方法的局限性，并在多个任务中表现优异。

- Motivation: 传统方法（如下采样或频域转换）在超高清图像修复中存在信息丢失或局部退化问题，需要更有效的解决方案。
- Method: RetinexDual结合了Scale-Attentive maMBA（SAMBA）和Frequency Illumination Adaptor（FIA）两个子网络，分别处理反射分量和频域校正。
- Result: 在去雨、去模糊、去雾和低光增强等任务中，RetinexDual在质量和数量上均优于现有方法。
- Conclusion: RetinexDual通过独特设计解决了传统方法的局限性，其组件有效性通过消融实验得到验证。


### [2] [ACM Multimedia Grand Challenge on ENT Endoscopy Analysis](https://arxiv.org/abs/2508.04801)
*Trong-Thuan Nguyen,Viet-Tham Huynh,Thao Thi Phuong Dao,Ha Nguyen Thi,Tien To Vu Thuy,Uyen Hanh Tran,Tam V. Nguyen,Thanh Dinh Le,Minh-Triet Tran*

Main category: cs.CV

TL;DR: 论文介绍了ENTRep挑战赛，旨在通过双语临床监督整合内窥镜图像的细粒度分类与检索任务。

- Motivation: 现有公共基准在内窥镜图像分析中缺乏支持分类和检索任务的能力，尤其是多语言和细粒度需求。
- Method: 提出ENTRep数据集，包含专家标注的图像和双语描述，并定义三个基准任务，采用服务器端评分评估性能。
- Result: 报告了顶级团队的表现结果，并提供了深入讨论。
- Conclusion: ENTRep为内窥镜图像分析提供了新的多语言和细粒度基准，推动了该领域的发展。


### [3] [CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework](https://arxiv.org/abs/2508.04816)
*Sriram Mandalika,Lalitha V*

Main category: cs.CV

TL;DR: CoMAD是一种轻量级、无参数的框架，通过多教师知识蒸馏将自监督视觉Transformer的知识统一到紧凑的学生网络中，提升了性能。

- Motivation: 现有的自监督学习方法通常独立训练，忽略了互补性，导致模型庞大且难以部署。CoMAD旨在解决这一问题。
- Method: CoMAD采用非对称掩码策略，学生仅看到25%的图像块，而教师接收不同的掩码。通过线性适配器和层归一化对齐教师嵌入，并使用联合共识门控融合知识。
- Result: 在ImageNet-1K上，CoMAD的ViT-Tiny达到75.4% Top-1准确率，密集预测任务中表现优异，刷新了紧凑自监督蒸馏的SOTA。
- Conclusion: CoMAD通过多教师知识蒸馏和创新的掩码策略，显著提升了紧凑模型的性能，为资源受限场景提供了高效解决方案。


### [4] [Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models](https://arxiv.org/abs/2508.04818)
*Mehrdad Moradi,Marco Grasso,Bianca Maria Colosimo,Kamran Paynabar*

Main category: cs.CV

TL;DR: RADAR是一种基于注意力机制的扩散模型，用于实时无重建异常检测，解决了传统重建方法的计算效率低和模式偏差问题，并在多个数据集上表现优于现有方法。

- Motivation: 传统基于重建的扩散模型在异常检测中存在计算成本高、模式偏差和噪声水平选择困难的问题，RADAR旨在解决这些问题。
- Method: RADAR直接通过扩散模型生成异常图，避免了重建过程，提高了效率和准确性。
- Result: 在MVTec-AD和3D打印材料数据集上，RADAR的F1分数分别提高了7%和13%，优于现有方法。
- Conclusion: RADAR通过无重建方法显著提升了异常检测的性能和效率，适用于实时应用。


### [5] [A deep learning approach to track eye movements based on events](https://arxiv.org/abs/2508.04827)
*Chirag Seth,Divya Naiken,Keyan Lin*

Main category: cs.CV

TL;DR: 研究利用事件相机和深度学习算法（CNN_LSTM）实现低成本高精度的眼球中心定位，准确率达81%，未来计划通过LRP提升模型可解释性和性能。

- Motivation: 解决高速眼球运动（300°/s）的精确追踪问题，降低对昂贵高速相机的依赖，应用于VR/AR设备以提升用户体验。
- Method: 采用CNN_LSTM深度学习模型，输入来自事件相机的数据，定位眼球中心位置（x, y）。
- Result: 模型准确率约为81%，验证了方法的有效性。
- Conclusion: 研究为低成本眼球追踪提供了可行方案，未来将通过LRP进一步优化模型。


### [6] [LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction](https://arxiv.org/abs/2508.04847)
*Md Zahidul Hasan,A. Ben Hamza,Nizar Bouguila*

Main category: cs.CV

TL;DR: LuKAN是一种基于Kolmogorov-Arnold Networks（KANs）和Lucas多项式激活的3D人体运动预测模型，通过离散小波变换和空间投影层高效捕捉时空信息，并在实验中表现出色。

- Motivation: 现有方法在预测精度和计算效率之间难以平衡，LuKAN旨在通过KANs和Lucas多项式解决这一问题。
- Method: 模型使用离散小波变换编码时间信息，空间投影层捕捉关节依赖关系，核心是采用Lucas多项式参数化的KAN层进行高效函数逼近。
- Result: 在三个基准数据集上的实验表明，LuKAN在定量和定性评估中均优于基线模型，且计算效率高。
- Conclusion: LuKAN通过其紧凑架构和Lucas多项式的线性递归，实现了预测精度和计算效率的平衡。


### [7] [VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence](https://arxiv.org/abs/2508.04852)
*Chenhui Qiang,Zhaoyang Wei,Xumeng Han Zipeng Wang,Siyao Li,Xiangyuan Lan,Jianbin Jiao,Zhenjun Han*

Main category: cs.CV

TL;DR: VER-Bench是一个评估多模态大语言模型（MLLMs）在细粒度视觉线索识别和复杂推理能力的新框架。

- Motivation: 现有基准测试在评估MLLMs的视觉能力时，要么缺乏深度推理，要么忽视细微线索，而真正的视觉理解依赖于对细微细节的解析。
- Method: VER-Bench包含374个精心设计的问题，涵盖多种推理类型，每个问题附带视觉线索和结构化证据。
- Result: VER-Bench揭示了当前模型在提取细微视觉证据和构建基于证据的推理方面的局限性。
- Conclusion: 需要提升模型在细粒度视觉证据提取、整合和推理方面的能力，以实现真正的视觉理解和类人分析。


### [8] [Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications](https://arxiv.org/abs/2508.04868)
*Noreen Anwar,Guillaume-Alexandre Bilodeau,Wassim Bouachir*

Main category: cs.CV

TL;DR: DAMM提出了一种双流注意力与多模态查询框架，解决了Transformer检测器在遮挡、细粒度定位和计算效率上的问题。

- Motivation: Transformer检测器因固定查询和密集注意力导致遮挡、细粒度定位和计算效率问题。
- Method: 引入三种查询（外观、位置、随机学习查询）和双流交叉注意力模块，分别优化语义和空间特征。
- Result: 在四个基准测试中取得最优AP和召回率。
- Conclusion: 多模态查询适应和双流注意力有效提升了检测精度和效率。


### [9] [Revealing Temporal Label Noise in Multimodal Hateful Video Classification](https://arxiv.org/abs/2508.04900)
*Shuonan Yang,Tailin Chen,Rahul Singh,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 论文研究了多模态仇恨视频中标签噪声的影响，通过精细标注的仇恨片段分析，揭示了粗粒度标注的局限性，并提出了时间感知模型的需求。

- Motivation: 在线多媒体内容的快速增长加剧了仇恨言论的传播，而现有方法依赖粗粒度的视频级标注，忽略了仇恨内容的时间粒度，导致标签噪声。
- Method: 通过标注时间戳从HateMM和MultiHateClip数据集中提取仇恨片段，分析其分布和特征，并进行控制实验。
- Result: 实验表明，时间戳噪声显著影响模型决策边界和分类置信度，揭示了仇恨言论表达的时间依赖性。
- Conclusion: 研究强调了时间感知模型和基准的重要性，以提高多模态仇恨视频检测的鲁棒性和可解释性。


### [10] [Test-Time Adaptation for Video Highlight Detection Using Meta-Auxiliary Learning and Cross-Modality Hallucinations](https://arxiv.org/abs/2508.04924)
*Zahidul Islam,Sujoy Paul,Mrigank Rochan*

Main category: cs.CV

TL;DR: Highlight-TTA提出了一种测试时自适应框架，通过动态调整模型以适应每个测试视频的独特特性，从而提升视频高光检测的泛化能力和性能。

- Motivation: 现有视频高光检测方法因使用固定模型而难以适应不同测试视频的多样性，导致性能下降。
- Method: 提出Highlight-TTA框架，结合跨模态幻觉辅助任务，通过元辅助训练方案动态优化模型。
- Result: 在三个基准数据集和三种先进模型上的实验表明，Highlight-TTA显著提升了高光检测性能。
- Conclusion: Highlight-TTA通过测试时自适应有效解决了现有方法的泛化问题，提升了高光检测效果。


### [11] [Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens](https://arxiv.org/abs/2508.04928)
*Suchisrit Gangopadhyay,Jung-Hee Kim,Xien Chen,Patrick Rim,Hyoungseob Park,Alex Wong*

Main category: cs.CV

TL;DR: 提出一种方法，将基于透视图像训练的单目深度估计器扩展到鱼眼图像，通过校准令牌调整潜在嵌入分布，无需重新训练或微调。

- Motivation: 尽管单目深度估计器在大量透视图像上训练，但相机校准参数的变化会导致深度估计错误，因此需要适应鱼眼图像。
- Method: 引入校准令牌作为轻量级适配机制，调整潜在嵌入分布，利用自监督学习，无需鱼眼图像数据。
- Result: 在室内外场景中，使用单一令牌集显著优于现有方法。
- Conclusion: 该方法有效扩展了单目深度估计器的适用性，避免了传统方法的负面影响。


### [12] [Toward Errorless Training ImageNet-1k](https://arxiv.org/abs/2508.04941)
*Bo Deng,Levi Heath*

Main category: cs.CV

TL;DR: 本文描述了一种基于ImageNet 2012数据集训练的神经网络，使用新方法达到98.3%准确率和99.69% Top-1率，模型参数为322,430,160。

- Motivation: 通过新方法提升神经网络在ImageNet数据集上的分类准确率。
- Method: 使用[5]中的新方法训练前馈神经网络。
- Result: 达到98.3%准确率，99.69% Top-1率，285.9个标签完美分类。
- Conclusion: 模型未达到100%准确率可能是由于数据集中的双标签问题。


### [13] [Accelerating Conditional Prompt Learning via Masked Image Modeling for Vision-Language Models](https://arxiv.org/abs/2508.04942)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

TL;DR: ProMIM是一个轻量级框架，通过结合掩码图像建模（MIM）增强现有视觉语言模型的提示学习，提升泛化性能而不显著增加计算成本。

- Motivation: 现有提示学习方法（如CoOp和CoCoOp）容易过拟合已知类别，限制了其在未见类别上的泛化能力。
- Method: ProMIM通过掩码可见图像块生成实例条件化提示，增强特征鲁棒性，同时保持现有方法的核心架构不变。
- Result: 实验表明，ProMIM在零样本和小样本分类任务中显著提升了泛化性能。
- Conclusion: ProMIM为视觉语言应用提供了一种高效、轻量的解决方案。


### [14] [TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring](https://arxiv.org/abs/2508.04943)
*Zhu Xu,Ting Lei,Zhimin Li,Guan Wang,Qingchao Chen,Yuxin Peng,Yang liu*

Main category: cs.CV

TL;DR: 提出了一种名为TRKT的方法，通过关系感知知识挖掘和双流融合模块，解决了弱监督动态场景图生成中外部目标检测器的局限性，显著提升了性能。

- Motivation: 现有的弱监督动态场景图生成方法依赖外部目标检测器，但静态目标检测器在动态、关系感知场景中表现不佳，导致定位不准确和低置信度提议。
- Method: TRKT方法包括关系感知知识挖掘（生成类别特定注意力图并利用光流增强）和双流融合模块（整合注意力图以优化目标定位和置信度）。
- Result: 在Action Genome数据集上，TRKT实现了最先进的性能。
- Conclusion: TRKT通过关系感知和运动感知的知识挖掘，显著提升了弱监督动态场景图生成的性能。


### [15] [AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics](https://arxiv.org/abs/2508.04955)
*Stella Su,Marc Harary,Scott J. Rodig,William Lotter*

Main category: cs.CV

TL;DR: AdvDINO是一种自监督学习框架，通过对抗性训练提升领域不变性，应用于生物医学图像分析。

- Motivation: 解决自监督学习在领域偏移（如生物医学图像中的批次效应）下的鲁棒性问题。
- Method: 在DINOv2架构中引入梯度反转层，实现对抗性自监督学习。
- Result: 在非小细胞肺癌的多通道免疫荧光图像中，AdvDINO减少了批次偏差，发现了具有预后意义的表型簇，并提升了生存预测性能。
- Conclusion: AdvDINO适用于多种图像领域，解决了领域偏移和标注数据不足的问题。


### [16] [Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework](https://arxiv.org/abs/2508.04962)
*Peng Zhang,Songru Yang,Jinsheng Sun,Weiqing Li,Zhiyong Su*

Main category: cs.CV

TL;DR: HOW-Seg是一个基于人机交互的开世界点云语义分割框架，通过稀疏标注和层次化原型去歧机制，实现了对基类和新类的高质量分割。

- Motivation: 现有方法依赖资源密集的离线增量学习或密集标注支持数据，限制了实用性。HOW-Seg旨在通过人机交互解决这些问题。
- Method: 构建查询数据的类原型，利用稀疏标注指导分割，引入层次化原型去歧机制和密集CRF优化标签分配。
- Result: HOW-Seg在稀疏标注下性能与GFS-Seg相当，使用高级骨干和密集标注时，在S3DIS和ScanNetv2上分别达到85.27%和66.37% mIoU。
- Conclusion: HOW-Seg通过人机交互动态优化预测，显著提升了开世界点云分割的实用性。


### [17] [UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS](https://arxiv.org/abs/2508.04968)
*Zhihao Guo,Peng Wang,Zidong Chen,Xiangyu Kong,Yan Lyu,Guanyu Gao,Liangxiu Han*

Main category: cs.CV

TL;DR: 论文提出了一种通过自适应高斯权重和不确定性学习改进3D高斯溅射（3DGS）的方法，显著提升了稀疏视角下的渲染质量。

- Motivation: 现有3DGS方法在渲染时对所有高斯分布赋予相同权重，容易导致过拟合，尤其在稀疏视角场景下表现不佳。
- Method: 通过学习的自适应不确定性指导高斯不透明度的可微更新，并采用软可微dropout正则化策略，优化高斯投影和混合过程。
- Result: 在多个数据集上验证，该方法在稀疏视角3D合成中表现优于现有方法，如PSNR在MipNeRF 360数据集上提升3.27%。
- Conclusion: 自适应权重和不确定性学习有效提升了3DGS在稀疏视角下的渲染质量，减少了高斯数量需求。


### [18] [CSRAP: Enhanced Canvas Attention Scheduling for Real-Time Mission Critical Perception](https://arxiv.org/abs/2508.04976)
*Md Iftekharul Islam Sakib,Yigong Hu,Tarek Abdelzaher*

Main category: cs.CV

TL;DR: 本文提出了一种改进的基于画布的注意力调度方法，通过支持可变画布尺寸和可选的画布帧率，提升了边缘平台上实时感知的性能与效率。

- Motivation: 解决边缘平台在高分辨率目标检测中面临的资源限制和延迟问题，优化感知子系统的资源需求。
- Method: 扩展了基于画布的注意力调度方法，引入可变尺寸画布和可选帧率，并在NVIDIA Jetson Orin Nano上运行YOLOv11进行验证。
- Result: 实验结果表明，该方法在Waymo Open Dataset上实现了更高的平均精度（mAP）和召回率，优于现有技术。
- Conclusion: 通过增加自由度，该方法显著提升了质量与成本的权衡，为实时感知提供了更优解决方案。


### [19] [Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression](https://arxiv.org/abs/2508.04979)
*Zheng Chen,Mingde Zhou,Jinpei Guo,Jiale Yuan,Yifei Ji,Yulun Zhang*

Main category: cs.CV

TL;DR: SODEC是一种新型的单步扩散图像压缩模型，解决了传统扩散方法的高延迟和低保真问题，通过单步解码和保真度引导模块显著提升了性能和解码速度。

- Motivation: 传统扩散图像压缩方法存在解码延迟高和保真度低的问题，需要改进。
- Method: 利用预训练的VAE模型生成信息丰富的潜在表示，采用单步解码替代多步去噪，并引入保真度引导模块和速率退火训练策略。
- Result: SODEC在速率-失真-感知性能上显著优于现有方法，解码速度提升20倍以上。
- Conclusion: SODEC通过单步解码和保真度优化，实现了高效的图像压缩，解决了传统扩散方法的局限性。


### [20] [Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion](https://arxiv.org/abs/2508.04984)
*Shenglun Chen,Xinzhu Ma,Hong Zhang,Haojie Li,Zhihui Wang*

Main category: cs.CV

TL;DR: 提出了一种利用深度基础模型增强深度补全鲁棒性的新框架，无需大规模训练。

- Motivation: 现有深度补全模型在分布外（OOD）场景中性能显著下降，而基础模型在大规模训练下表现出色，因此探索如何利用基础模型提升深度补全的鲁棒性。
- Method: 结合深度基础模型提取RGB图像的环境线索，设计无参数的双空间传播方法（3D和2D），并引入可学习的校正模块逐步优化深度预测。
- Result: 在16个数据集上的广泛评估表明，该方法在OOD场景中表现优异，优于现有最佳深度补全方法。
- Conclusion: 该框架通过利用基础模型和双空间传播，显著提升了深度补全的鲁棒性，无需依赖大规模训练。


### [21] [Unified modality separation: A vision-language framework for unsupervised domain adaptation](https://arxiv.org/abs/2508.04987)
*Xinyao Li,Jingjing Li,Zhekai Du,Lei Zhu,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出了一种统一的模态分离框架，通过分离和处理视觉语言模型中的模态特定和模态不变组件，解决了模态间隙问题，提升了无监督域适应的性能。

- Motivation: 现有方法在模态间隙存在时仅传递模态不变知识，导致目标性能不佳。
- Method: 设计模态分离框架，分离模态组件并分别处理，测试时通过模态自适应集成权重最大化组件协同。
- Result: 性能提升高达9%，计算效率提高9倍。
- Conclusion: 该框架在各种实验设置中均表现出高效性和有效性。


### [22] [Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks](https://arxiv.org/abs/2508.04988)
*Yue Li,Weifan Wang,Tai Sing Lee*

Main category: cs.CV

TL;DR: 该研究使用基于ViT的自编码器探讨了熟悉性训练如何在深度神经网络早期层引入全局上下文敏感性，并通过LoRA实现快速权重。结果表明，熟悉性训练能重塑神经流形、对齐潜在表征、扩大注意力范围，且快速权重显著增强这些效果。

- Motivation: 研究早期视觉皮层如何快速学习全局图像上下文，并探索快速权重在其中的作用。
- Method: 采用基于ViT的自编码器，结合LoRA实现快速权重，分析熟悉性训练对网络的影响。
- Result: 熟悉性训练重塑神经流形、对齐潜在表征、扩大注意力范围，快速权重显著增强效果。
- Conclusion: 熟悉性训练在分层网络中引入全局敏感性，快速权重架构为研究大脑快速学习提供可行模型。


### [23] [Attribute Guidance With Inherent Pseudo-label For Occluded Person Re-identification](https://arxiv.org/abs/2508.04998)
*Rui Zhi,Zhen Yang,Haiyang Zhang*

Main category: cs.CV

TL;DR: AG-ReID利用预训练模型提取细粒度属性，通过两阶段框架（生成属性伪标签和双引导机制）提升遮挡和细微差异下的行人重识别性能。

- Motivation: 预训练视觉语言模型在遮挡场景中因忽略细粒度属性信息而表现不佳，需改进。
- Method: 提出AG-ReID框架，分两阶段：生成属性伪标签，结合整体和细粒度信息的双引导机制。
- Result: 在多个Re-ID数据集上达到最优，显著提升遮挡和细微差异处理能力。
- Conclusion: AG-ReID有效结合整体与细粒度信息，显著提升遮挡场景下的行人重识别性能。


### [24] [CRAM: Large-scale Video Continual Learning with Bootstrapped Compression](https://arxiv.org/abs/2508.05001)
*Shivani Mall,Joao F. Henriques*

Main category: cs.CV

TL;DR: 论文提出了一种名为CRAM的视频持续学习方法，通过压缩视频代码减少存储需求，并解决视频压缩器的灾难性遗忘问题。

- Motivation: 视频持续学习面临高存储需求和灾难性遗忘的挑战，传统方法难以应对长视频和持续流数据。
- Method: 使用压缩视觉技术存储视频代码而非原始数据，并通过刷新视频代码解决压缩器的遗忘问题。
- Result: 在EpicKitchens-100和Kinetics-700等大规模数据集上，CRAM方法显著优于现有技术，且存储需求大幅降低。
- Conclusion: CRAM方法有效解决了视频持续学习中的存储和遗忘问题，为实际应用提供了可行方案。


### [25] [Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation](https://arxiv.org/abs/2508.05008)
*Xusheng Liang,Lihua Zhou,Nianxin Li,Miao Xu,Ziyang Song,Dong Yi,Jinlin Wu,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 论文提出了一种名为MCDRL的新框架，结合因果推理与视觉语言模型（VLM），用于解决医学图像分割中的领域泛化问题。

- Motivation: 医学图像数据的高变异性与复杂性导致现有视觉语言模型（如CLIP）在医学影像应用中表现不佳，尤其是面对领域偏移时。
- Method: MCDRL分两步实现：1）利用CLIP的跨模态能力识别病变区域并构建领域特异性变异的混淆字典；2）训练因果干预网络消除这些变异的影响，同时保留关键解剖结构信息。
- Result: 实验表明，MCDRL在分割任务中表现优于其他方法，具有更高的准确性和泛化能力。
- Conclusion: MCDRL通过结合因果推理与VLM，有效解决了医学图像分割中的领域泛化问题，展现了优越性能。


### [26] [AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content](https://arxiv.org/abs/2508.05016)
*Shushi Wang,Chunyi Li,Zicheng Zhang,Han Zhou,Wei Dong,Jun Chen,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 该论文提出了一个专门用于评估AI增强用户生成内容（AI-UGC）质量的基准数据集AU-IQA，并评估了现有质量评估模型在该数据集上的表现。

- Motivation: 当前缺乏针对AI-UGC的专门质量评估模型，限制了用户体验和增强方法的进步。
- Method: 构建了包含4,800张AI-UGC图像的AU-IQA数据集，并评估了多种现有质量评估模型。
- Result: 提供了对当前方法在评估AI-UGC感知质量方面的全面分析。
- Conclusion: AU-IQA数据集填补了AI-UGC质量评估的空白，为未来研究提供了重要资源。


### [27] [Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes](https://arxiv.org/abs/2508.05019)
*Sadia Kamal,Tim Oates,Joy Wan*

Main category: cs.CV

TL;DR: 皮肤-SOAP是一种弱监督多模态框架，用于从有限的输入（如病变图像和稀疏临床文本）生成结构化SOAP笔记，减轻临床医生负担。

- Motivation: 皮肤癌是全球最常见的癌症，早期诊断和治疗至关重要，但手动生成SOAP笔记耗时且易导致医生疲劳。
- Method: 提出皮肤-SOAP框架，利用弱监督学习减少对人工标注的依赖，结合图像和文本输入生成SOAP笔记。
- Result: 性能与GPT-4o、Claude和DeepSeek Janus Pro相当，并引入MedConceptEval和CCS两个新指标评估临床相关性。
- Conclusion: 皮肤-SOAP可扩展临床文档生成，减轻医生负担，减少对大量标注数据的需求。


### [28] [A Novel Image Similarity Metric for Scene Composition Structure](https://arxiv.org/abs/2508.05037)
*Md Redwanul Haque,Manzur Murshed,Manoranjan Paul,Tsz-Kwan Lee*

Main category: cs.CV

TL;DR: 提出了一种新的图像质量评估方法SCSSIM，专注于场景构图结构（SCS）的完整性，解决了传统方法的不足。

- Motivation: 生成式AI模型的快速发展需要超越人类感知的图像质量评估方法，尤其是确保场景构图结构的准确性。
- Method: SCSSIM是一种无需训练的度量方法，通过图像的立方体层次分区统计量来量化SCS的保持情况。
- Result: 实验表明SCSSIM对非构图失真具有高度不变性，而对构图失真则表现出单调递减，优于现有方法。
- Conclusion: SCSSIM是评估生成模型场景构图完整性的有力工具。


### [29] [HAMoBE: Hierarchical and Adaptive Mixture of Biometric Experts for Video-based Person ReID](https://arxiv.org/abs/2508.05038)
*Yiyang Su,Yunping Shi,Feng Liu,Xiaoming Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为HAMoBE的新框架，通过分层和自适应混合生物特征专家方法，显著提升了视频行人重识别的性能。

- Motivation: 现有视频行人重识别方法常忽略从查询-图库对中选择最具区分性特征的重要性，导致匹配效果不佳。
- Method: HAMoBE框架利用预训练大模型（如CLIP）的多层特征，分层建模外观、静态体型和动态步态等关键生物特征，并通过双输入决策门控网络动态调整专家贡献。
- Result: 在MEVID等基准测试中，HAMoBE实现了显著性能提升（如Rank-1准确率提高13.0%）。
- Conclusion: HAMoBE通过模仿人类感知机制和自适应特征集成，为视频行人重识别提供了高效解决方案。


### [30] [Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?](https://arxiv.org/abs/2508.05053)
*Parth Thakkar,Ankush Agarwal,Prasad Kasu,Pulkit Bansal,Chaitanya Devaguptapu*

Main category: cs.CV

TL;DR: 论文提出了NiM基准和Spot-IT方法，用于评估和改进多模态大语言模型（MLLMs）在复杂文档中定位和推理细粒度细节的能力。

- Motivation: 当前MLLMs在文档理解任务中表现出色，但在处理复杂文档中的细粒度细节（如菜单中的营养信息或长文中的免责声明）时能力不足，类似“在图像中找针”（NiM）。
- Method: 引入NiM基准，涵盖多种真实文档（如报纸、菜单、讲座图像），并提出Spot-IT方法，通过智能补丁选择和高斯注意力增强MLLMs能力。
- Result: 实验显示当前MLLMs在细粒度文档理解任务中的局限性，同时Spot-IT方法在复杂布局中提取精确细节方面显著优于基线方法。
- Conclusion: Spot-IT方法有效提升了MLLMs在细粒度文档理解任务中的表现，为未来研究提供了新方向。


### [31] [DualMat: PBR Material Estimation via Coherent Dual-Path Diffusion](https://arxiv.org/abs/2508.05060)
*Yifeng Huang,Zhang Chen,Yi Xu,Minh Hoai,Zhong Li*

Main category: cs.CV

TL;DR: DualMat是一种双路径扩散框架，用于在复杂光照条件下从单张图像估计基于物理的渲染（PBR）材料。

- Motivation: 解决现有方法在复杂光照条件下对PBR材料估计的不足，尤其是在高分辨率和多视角输入时的性能问题。
- Method: 采用双路径潜在空间（RGB潜在空间和紧凑材料潜在空间），结合特征蒸馏和整流流技术，支持高分辨率和多视角输入。
- Result: 在Objaverse和真实数据上达到最先进性能，反照率估计提升28%，金属-粗糙度预测误差降低39%。
- Conclusion: DualMat通过双路径设计和高效推理技术，显著提升了PBR材料估计的精度和效率。


### [32] [Decoupling Continual Semantic Segmentation](https://arxiv.org/abs/2508.05065)
*Yifu Guo,Yuquan Lu,Wentao Zhang,Zishan Xu,Dexia Chen,Siyu Zhang,Yizhe Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: DecoupleCSS是一个两阶段框架，通过解耦类别感知检测和类别无关分割，解决持续语义分割中的灾难性遗忘问题。

- Motivation: 现有CSS方法中，分割掩码和类别标签紧密耦合，导致新旧类别学习干扰和次优的保留-可塑性平衡。
- Method: 第一阶段使用预训练的文本和图像编码器（通过LoRA调整）生成位置感知提示；第二阶段利用SAM模型生成精确分割掩码。
- Result: DecoupleCSS在多种挑战性任务中实现了最先进的性能，平衡了保留和适应性。
- Conclusion: DecoupleCSS通过解耦策略有效解决了CSS中的灾难性遗忘问题，提升了性能。


### [33] [Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks](https://arxiv.org/abs/2508.05068)
*Ruiyu Li,Changyuan Qiu,Hangrui Cao,Qihan Ren,Yuqing Qiu*

Main category: cs.CV

TL;DR: 论文探讨了通过分类和对抗学习实现自动图像着色的方法，改进了传统回归任务的局限性。

- Motivation: 图像着色任务因其在颜色恢复和自动动画着色等应用中的重要性而受到广泛研究，但由于其高度不适定性，需要利用场景语义和纹理信息。
- Method: 采用分类和对抗学习方法，基于先前工作构建模型并进行改进和比较。
- Result: 未明确提及具体结果，但强调了方法的改进和比较。
- Conclusion: 通过分类和对抗学习改进图像着色任务，为后续研究提供方向。


### [34] [FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer](https://arxiv.org/abs/2508.05069)
*Jian Zhu,Shanyuan Liu,Liuzhuozheng Li,Yue Gong,He Wang,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin,Yang Xu*

Main category: cs.CV

TL;DR: FLUX-Makeup是一种无需辅助面部控制组件的高保真化妆迁移框架，通过直接利用源-参考图像对实现卓越性能。

- Motivation: 现有GAN和扩散方法依赖额外组件，易引入误差，导致迁移效果不佳。
- Method: 基于FLUX-Kontext框架，引入RefLoRAInjector轻量级特征注入器，并设计高效数据生成流程。
- Result: 实验表明FLUX-Makeup在多样场景下表现优异，超越现有方法。
- Conclusion: FLUX-Makeup实现了高保真、身份一致的化妆迁移，无需额外组件。


### [35] [AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models](https://arxiv.org/abs/2508.05084)
*Yuxiang Xiao,Yang Hu,Bin Li,Tianyang Zhang,Zexi Li,Huazhu Fu,Jens Rittscher,Kaixiang Yang*

Main category: cs.CV

TL;DR: AdaFusion是一个动态整合多个病理基础模型（PFMs）知识的提示引导推理框架，通过轻量级注意力机制自适应融合特征，提升下游任务的性能和可解释性。

- Motivation: PFMs的预训练背景多样且不透明，导致潜在偏见，影响下游应用的泛化性和透明度。
- Method: AdaFusion压缩和对齐来自不同模型的瓦片级特征，利用轻量级注意力机制根据组织表型动态融合。
- Result: 在三个真实世界基准测试中，AdaFusion在分类和回归任务上均优于单个PFM，并提供模型生物语义特化的可解释性。
- Conclusion: AdaFusion能够桥接异构PFMs，提升性能并增强模型特定归纳偏见的可解释性。


### [36] [PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation](https://arxiv.org/abs/2508.05091)
*Jingxuan He,Busheng Su,Finn Wong*

Main category: cs.CV

TL;DR: PoseGen是一种新框架，通过单张参考图像和驱动姿态序列生成任意长度的视频，解决了扩散模型在身份漂移和视频长度限制上的问题。

- Motivation: 当前扩散模型在生成长视频时存在身份漂移和短片段限制的问题，PoseGen旨在解决这些挑战。
- Method: 采用in-context LoRA微调策略，在token级别注入主题外观以保持身份，同时在通道级别条件化姿态信息以控制运动。通过分段生成和共享KV缓存机制确保视频连贯性。
- Result: 在仅33小时视频数据集上训练，PoseGen在身份保真度、姿态准确性和生成无限时长视频方面显著优于现有方法。
- Conclusion: PoseGen通过创新的微调和分段生成方法，实现了高质量、长时视频生成，具有广泛的应用潜力。


### [37] [Sculpting Margin Penalty: Intra-Task Adapter Merging and Classifier Calibration for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.05094)
*Liang Bai,Hong Song,Jinfu Li,Yucong Lin,Jingfan Fan,Tianyu Fu,Danni Ai,Deqiang Xiao,Jian Yang*

Main category: cs.CV

TL;DR: SMP方法通过整合不同阶段的边界惩罚，解决了Few-Shot Class-Incremental Learning中基类与新类的平衡问题，并优化了决策边界。

- Motivation: 现实应用中数据隐私和高成本导致增量任务训练数据不足，现有方法难以平衡基类区分性和新类泛化性。
- Method: 提出SMP方法，包括MIAM机制（基类任务学习）和MPCC策略（增量任务决策边界优化）。
- Result: 在CIFAR100、ImageNet-R和CUB200上，SMP实现了最先进的性能，平衡了基类和新类。
- Conclusion: SMP通过边界惩罚和参数高效微调，显著提升了FSCIL的性能和兼容性。


### [38] [AHDMIL: Asymmetric Hierarchical Distillation Multi-Instance Learning for Fast and Accurate Whole-Slide Image Classification](https://arxiv.org/abs/2508.05114)
*Jiuyang Dong,Jiahan Li,Junjun Jiang,Kui Jiang,Yongbing Zhang*

Main category: cs.CV

TL;DR: AHDMIL框架通过两步训练和不对称层次蒸馏，显著提升了病理图像分类的效率和准确性。

- Motivation: 解决多实例学习（MIL）在处理千兆像素全切片图像（WSI）时的高推理成本问题。
- Method: 提出AHDMIL框架，包含动态多实例网络（DMIN）和双分支轻量级实例预筛选网络（DB-LIPN），通过自蒸馏和不对称蒸馏两步训练消除无关补丁。
- Result: 在四个公共数据集上，AHDMIL在分类性能和推理速度上均优于现有方法，例如在Camelyon16数据集上准确率提升5.3%，推理速度提高1.2倍。
- Conclusion: AHDMIL通过高效补丁筛选和新型CKA分类器，显著提升了病理图像分类的性能和效率。


### [39] [Latent Expression Generation for Referring Image Segmentation and Grounding](https://arxiv.org/abs/2508.05123)
*Seonghoon Yu,Joonbeom Hong,Joonseok Lee,Jeany Son*

Main category: cs.CV

TL;DR: 论文提出了一种新的视觉定位框架，通过生成多个潜在表达来丰富文本输入，结合视觉细节，提升目标对象定位的准确性。

- Motivation: 现有方法通常依赖单一文本输入，无法充分利用视觉细节，容易导致目标误识别。
- Method: 引入主题分发器和视觉概念注入器模块，嵌入共享主题和独特属性概念，并提出正边际对比学习策略。
- Result: 在多个基准测试中优于现有RIS和REC方法，并在GRES基准上表现突出。
- Conclusion: 通过多潜在表达和视觉细节的结合，显著提升了视觉定位任务的性能。


### [40] [FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images](https://arxiv.org/abs/2508.05137)
*Sachin Dudda Nagaraju,Ashkan Moradi,Bendik Skarre Abrahamsen,Mattijs Elschot*

Main category: cs.CV

TL;DR: FedGIN是一种基于联邦学习的多模态医学图像分割框架，通过GIN模块解决数据稀缺和隐私问题，在不同数据集场景下均表现出色。

- Motivation: 解决医学图像分割中数据稀缺、模态间域偏移和隐私限制的问题，开发一种无需共享原始数据的统一模型。
- Method: 提出FedGIN框架，结合GIN模块在局部训练中协调模态强度分布，并在有限和完整数据集场景下进行评估。
- Result: 在有限数据场景下，FedGIN在MRI测试中Dice分数提升12-18%；在完整数据场景下，性能接近集中式训练，Dice分数提升30%（MRI）和10%（CT）。
- Conclusion: FedGIN在多模态医学图像分割中表现出强大的跨模态泛化能力，同时保护数据隐私。


### [41] [Deep Learning-based Animal Behavior Analysis: Insights from Mouse Chronic Pain Models](https://arxiv.org/abs/2508.05138)
*Yu-Hsi Chen,Wei-Hsin Chen,Chien-Yao Wang,Hong-Yuan Mark Liao,James C. Liao,Chien-Chang Chen*

Main category: cs.CV

TL;DR: 该研究提出了一种自动发现慢性疼痛相关特征的框架，避免了人工标注的偏差，显著提高了分类准确性，并在药物测试中验证了其临床潜力。

- Motivation: 现有方法依赖人工标注行为特征，难以准确捕捉慢性疼痛的持续行为变化，因此需要一种自动化的解决方案。
- Method: 使用通用动作空间投影器自动提取小鼠动作特征，保留原始视频中的丰富行为信息，避免人为标注的偏差。
- Result: 在15类疼痛分类任务中准确率达48.41%，显著优于人类专家（21.33%）和B-SOiD方法（30.52%）；简化至3类分类时准确率达73.1%。
- Conclusion: 该方法展示了在疼痛研究和药物开发中的临床潜力，为相关领域提供了新见解。


### [42] [Rotation Equivariant Arbitrary-scale Image Super-Resolution](https://arxiv.org/abs/2508.05160)
*Qi Xie,Jiahong Fu,Zongben Xu,Deyu Meng*

Main category: cs.CV

TL;DR: 该论文提出了一种旋转等变的任意尺度图像超分辨率（ASISR）方法，通过重新设计编码器和隐式神经表示（INR）模块，实现了从输入到输出的端到端旋转等变性。

- Motivation: 解决现有ASISR方法在处理低分辨率图像中几何模式（如重复纹理、边缘或形状）时出现的变形和伪影问题，通过嵌入旋转等变性来保持原始方向和结构完整性。
- Method: 重新设计INR和编码器模块的基本架构，嵌入旋转等变能力，并提供理论分析评估其内在等变误差。
- Result: 在模拟和真实数据集上的实验验证了方法的优越性，且该框架可以即插即用地集成到现有ASISR方法中。
- Conclusion: 提出的旋转等变ASISR方法有效提升了图像超分辨率的性能，尤其是在保持几何模式完整性方面。


### [43] [X-MoGen: Unified Motion Generation across Humans and Animals](https://arxiv.org/abs/2508.05162)
*Xuan Wang,Kai Ruan,Liyang Qian,Zhizhi Guo,Chang Su,Gaoang Wang*

Main category: cs.CV

TL;DR: X-MoGen是一个统一的跨物种文本驱动运动生成框架，通过两阶段架构和形态一致性模块解决形态差异问题，并在大规模数据集UniMo4D上验证了其优越性。

- Motivation: 现有方法通常分别建模人类和动物运动，而联合跨物种方法能提供统一表示和改进的泛化能力，但形态差异仍是挑战。
- Method: X-MoGen采用两阶段架构：1）条件图变分自编码器学习T-pose先验，自编码器编码运动到共享潜空间；2）掩码运动建模生成文本条件运动嵌入。训练中使用形态一致性模块。
- Result: 在UniMo4D数据集上，X-MoGen在已知和未知物种上均优于现有方法。
- Conclusion: X-MoGen首次实现了人类和动物的统一文本驱动运动生成，解决了形态差异问题，并通过实验验证了其有效性。


### [44] [PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems](https://arxiv.org/abs/2508.05167)
*Qi Guo,Xiaojun Jia,Shanmin Pang,Simeng Qin,Lin Wang,Ju Jia,Yang Liu,Qing Guo*

Main category: cs.CV

TL;DR: PhysPatch是一种针对多模态大语言模型（MLLM）的自动驾驶系统的物理可实现对抗性补丁框架，优化补丁位置、形状和内容以提高攻击效果和现实适用性。

- Motivation: MLLM在自动驾驶系统中易受对抗性补丁攻击，现有方法因模型复杂性和推理能力而效果不佳。
- Method: PhysPatch通过语义掩码初始化、SVD局部对齐损失和势场掩码优化，联合优化补丁参数。
- Result: 实验表明PhysPatch显著优于现有方法，并在物理可行区域放置补丁，确保现实适用性。
- Conclusion: PhysPatch为MLLM系统提供了高效且物理可行的对抗性补丁攻击方案。


### [45] [Multi-tracklet Tracking for Generic Targets with Adaptive Detection Clustering](https://arxiv.org/abs/2508.05172)
*Zewei Wu,Longhao Wang,Cui Wang,César Teixeira,Wei Ke,Zhang Xiong*

Main category: cs.CV

TL;DR: 提出了一种名为MTT的跟踪方法，通过灵活的轨迹片段生成和多片段关联框架解决未见类别目标跟踪问题。

- Motivation: 现实场景中，未见类别的目标因检测置信度低、运动与外观约束弱以及长期遮挡等问题，挑战现有跟踪方法。
- Method: MTT框架首先根据短时空相关性自适应聚类检测结果生成鲁棒轨迹片段，然后利用位置和外观等多线索估计最佳片段划分以减少长期关联中的误差传播。
- Result: 在通用多目标跟踪基准测试中，MTT表现出竞争力。
- Conclusion: MTT通过轨迹片段增强和多线索关联，有效解决了未见类别目标的跟踪问题。


### [46] [SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation](https://arxiv.org/abs/2508.05182)
*Zhiqing Xiao,Haobo Wang,Xu Lu,Wentao Ye,Gang Chen,Junbo Zhao*

Main category: cs.CV

TL;DR: SPA++是一个用于领域自适应（DA）的图谱对齐框架，通过粗粒度图对齐和细粒度邻居感知机制提升目标域的可区分性，并在实验中表现优异。

- Motivation: 解决领域自适应中传统方法忽视域内结构导致的可区分性下降问题。
- Method: 结合粗粒度图对齐、谱正则化、邻居感知传播机制以及数据增强和一致性正则化。
- Result: 在基准数据集上优于现有方法，表现出更强的鲁棒性和适应性。
- Conclusion: SPA++通过谱对齐和一致性正则化，有效提升了领域自适应的性能。


### [47] [SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images](https://arxiv.org/abs/2508.05202)
*Dongchen Si,Di Wang,Erzhong Gao,Xiaolei Qin,Liu Zhao,Jing Zhang,Minqiang Xu,Jianbo Zhan,Jianshe Wang,Lin Liu,Bo Du,Liangpei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SPEX的多模态LLM模型，用于光谱遥感图像中的地物提取，通过结合光谱先验和视觉语言指令，显著提升了性能。

- Motivation: 光谱信息在遥感观测中至关重要，但现有视觉语言模型未能充分利用光谱信息，导致多光谱场景下性能不佳。
- Method: 构建了SPIE数据集，将光谱先验编码为LLM可识别的文本属性；设计了多尺度特征聚合、令牌上下文压缩和多光谱视觉预训练等策略。
- Result: 在五个公开多光谱数据集上，SPEX在植被、建筑和水体等地物提取任务中表现优于现有方法，并能生成预测的文本解释。
- Conclusion: SPEX是首个专注于光谱遥感图像地物提取的多模态视觉语言模型，显著提升了性能和可解释性。


### [48] [EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery](https://arxiv.org/abs/2508.05205)
*Bingyu Yang,Qingyao Tian,Yimeng Geng,Huai Liao,Xinyan Huang,Jiebo Luo,Hongbin Liu*

Main category: cs.CV

TL;DR: EndoMatcher是一种通过大规模多域数据预训练实现的内窥镜图像通用匹配方法，解决了视觉条件困难和数据稀缺问题。

- Motivation: 内窥镜图像中的密集特征匹配对机器人辅助任务至关重要，但视觉条件困难（如弱纹理、大视角变化）和标注数据稀缺使其具有挑战性。
- Method: EndoMatcher采用双分支Vision Transformer提取多尺度特征，并通过双交互块增强对应学习；构建Endo-Mix6多域数据集，采用渐进多目标训练策略。
- Result: 在零样本匹配实验中，EndoMatcher在Hamlyn和Bladder数据集上的内点匹配数分别提高了140.69%和201.43%，在Gastro-Matching数据集上的MDPA提高了9.40%。
- Conclusion: EndoMatcher在挑战性内窥镜条件下实现了密集且准确的匹配，具有通用性和高性能。


### [49] [VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)
*Sihan Yang,Runsen Xu,Chenhang Cui,Tai Wang,Dahua Lin,Jiangmiao Pang*

Main category: cs.CV

TL;DR: VFlowOpt是一种视觉令牌剪枝框架，通过重要性映射和渐进式剪枝模块减少计算成本，同时保持性能。

- Motivation: 现有视觉令牌剪枝方法简单且性能下降明显，需更高效的剪枝策略。
- Method: 提出重要性映射和渐进式剪枝模块，结合视觉信息流优化剪枝策略。
- Result: 剪枝90%视觉令牌，性能相当，减少89% KV-Cache内存，推理速度提升3.8倍。
- Conclusion: VFlowOpt显著提升计算效率，适用于多种大型多模态模型。


### [50] [Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2508.05213)
*Jianming Liu,Wenlong Qiu,Haitao Wei*

Main category: cs.CV

TL;DR: 提出了一种无需源域数据的跨域少样本分割方法，通过结合文本和视觉信息提升目标域任务适应性。

- Motivation: 解决跨域少样本分割中源域数据不可用的问题，同时关注数据隐私和训练成本。
- Method: 使用任务特定注意力适配器（TSAA）调整预训练骨干网络的多级特征，并通过视觉-视觉嵌入对齐（VVEA）和文本-视觉嵌入对齐（TVEA）模块训练TSAA参数。
- Result: 在1-shot和5-shot设置下，平均分割精度分别提升2.18%和4.11%，显著优于现有方法。
- Conclusion: 该方法在无需源域数据的情况下，有效提升了跨域少样本分割的性能。


### [51] [ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking](https://arxiv.org/abs/2508.05221)
*Xiao Wang,Liye Jin,Xufeng Lou,Shiao Wang,Lan Chen,Bo Jiang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于推理的视觉语言跟踪框架ReasoningTrack，利用预训练模型Qwen2.5-VL，结合SFT和GRPO优化推理与语言生成，并构建了大规模数据集TNLLT验证其有效性。

- Motivation: 现有视觉语言跟踪方法性能有限，未能充分利用大模型优势或提供模型推理过程的可解释性。
- Method: 结合SFT和GRPO优化推理与语言生成，嵌入更新的语言描述与视觉特征输入统一跟踪主干网络，预测目标位置。
- Result: 在多个基准数据集上验证了推理生成策略的有效性，并构建了TNLLT数据集。
- Conclusion: ReasoningTrack框架显著提升了视觉语言跟踪性能，为任务提供了新的解决方案。


### [52] [Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2](https://arxiv.org/abs/2508.05227)
*Semanur Küçük,Cosimo Della Santina,Angeliki Laskari*

Main category: cs.CV

TL;DR: 利用微调的Segment Anything Model (SAM v2.1)成功解决了多相流中非球形气泡的分割问题。

- Motivation: 多相流中气泡形状复杂（如变形、合并或破裂），传统方法假设气泡为球形，效果有限。
- Method: 将任务视为迁移学习问题，使用100张标注图像微调SAM v2.1模型。
- Result: 模型能准确分割高度非凸、不规则的气泡结构。
- Conclusion: SAM v2.1在多相流气泡分割中表现出色，为工业应用提供了新解决方案。


### [53] [ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models](https://arxiv.org/abs/2508.05236)
*Yatong Lan,Jingfeng Chen,Yiru Wang,Lei He*

Main category: cs.CV

TL;DR: Arbiviewgen是一个基于扩散的框架，用于生成任意视角的可控相机图像，通过FAVS和CVC-SSL解决无监督视角生成问题。

- Motivation: 自动驾驶中任意视角图像生成缺乏真实数据支持，限制了高保真生成模型的训练。
- Method: 采用FAVS进行特征感知自适应视角拼接，结合CVC-SSL的自监督学习确保跨视角一致性。
- Result: 无需额外传感器或深度图，仅需多相机图像及其位姿即可训练，实现多车辆配置下的可控视角生成。
- Conclusion: Arbiviewgen首次实现了多车辆配置下的可控任意视角图像生成，为自动驾驶提供了新工具。


### [54] [Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models](https://arxiv.org/abs/2508.05237)
*Zane Xu,Jason Sun*

Main category: cs.CV

TL;DR: 综述了八篇关于视觉语言模型（如CLIP）零样本对抗鲁棒性的论文，探讨了提升对抗鲁棒性与保持零样本泛化能力之间的权衡。

- Motivation: 解决视觉语言模型在对抗攻击下的鲁棒性问题，同时保持其零样本泛化能力。
- Method: 分析了两种防御范式：对抗微调（AFT）和训练无关/测试时防御方法，并追踪了从对齐保持方法到嵌入空间重构的演变。
- Result: 总结了不同防御方法的优缺点，并提出了未来研究方向。
- Conclusion: 未来需要探索混合防御策略和对抗预训练等方向。


### [55] [RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding](https://arxiv.org/abs/2508.05244)
*Tianchen Fang,Guiru Liu*

Main category: cs.CV

TL;DR: RegionMed-CLIP是一种区域感知的多模态对比学习框架，通过结合局部病理信号和全局语义表示，解决了医学图像理解中数据标注不足和全局特征依赖的问题。

- Motivation: 医学图像理解面临高质量标注数据稀缺和全局特征忽略局部病理区域的挑战。
- Method: 提出RegionMed-CLIP框架，包含自适应整合局部与全局特征的ROI处理器，以及渐进式训练策略，并构建了MedRegion-500k数据集。
- Result: 在图像-文本检索、零样本分类和视觉问答任务中，RegionMed-CLIP显著优于现有视觉语言模型。
- Conclusion: 区域感知的对比预训练对医学图像理解至关重要，RegionMed-CLIP为多模态医学图像分析提供了坚实基础。


### [56] [A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis](https://arxiv.org/abs/2508.05246)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 该论文综述了性别分类的多种方法，重点关注面部和虹膜特征，分析了现有技术的优缺点，并提出了未来研究方向。

- Motivation: 性别分类在监控、企业分析和人机交互等领域有广泛应用，研究旨在总结现有方法并指出改进方向。
- Method: 论文回顾了基于面部、虹膜等生物特征的性别分类方法，并分析了不同步骤的技术。
- Result: 总结了现有性别分类方法的优缺点，提出了未来研究的挑战和改进建议。
- Conclusion: 性别分类技术仍有改进空间，未来研究可关注多模态生物特征和算法优化。


### [57] [CF3: Compact and Fast 3D Feature Fields](https://arxiv.org/abs/2508.05254)
*Hyunjoon Lee,Joonkyu Min,Jaesik Park*

Main category: cs.CV

TL;DR: 提出了一种名为CF3的3D高斯特征场构建方法，通过自上而下的流程优化计算效率，减少高斯数量。

- Motivation: 现有方法依赖自下而上的优化过程，计算成本高，且将2D特征视为绝对真理。
- Method: 采用多视角2D特征与预训练高斯的快速加权融合，训练高斯自编码器，并引入自适应稀疏化方法优化特征场。
- Result: 仅需5%的高斯数量即可实现与Feature-3DGS竞争的结果。
- Conclusion: CF3方法高效且能保留几何细节，显著降低了计算成本。


### [58] [Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging](https://arxiv.org/abs/2508.05262)
*Suresh Guttikonda,Maximilian Neidhart,Johanna Sprenger,Johannes Petersen,Christian Detter,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 提出了一种基于粒子滤波和循环一致性检查的跟踪方法，用于心脏手术中的实时荧光成像，显著提高了跟踪精度和速度。

- Motivation: 心脏运动和图像特征波动限制了传统跟踪方法的效果，需要一种更鲁棒的跟踪技术。
- Method: 使用粒子滤波和循环一致性检查来跟踪目标标志点，实现多目标实时跟踪。
- Result: 方法以25.4 fps的速度同时跟踪117个目标，跟踪误差为5.00 +/- 0.22 px，优于其他深度学习和传统跟踪器。
- Conclusion: 该方法在心脏手术中实现了高精度和实时性的跟踪，为手术质量提供了可靠支持。


### [59] [SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion](https://arxiv.org/abs/2508.05264)
*Xiaoyang Zhang,Zhen Hua,Yakun Ju,Wei Zhou,Jun Liu,Alex C. Kot*

Main category: cs.CV

TL;DR: SGDFuse是一种基于Segment Anything Model（SAM）的条件扩散模型，用于红外与可见光图像融合，通过语义掩码引导优化，实现高保真和语义感知的融合效果。

- Motivation: 现有方法因缺乏深度语义理解而无法保留关键目标，且融合过程易引入伪影和细节丢失，影响图像质量和任务性能。
- Method: 采用两阶段方法：先初步融合多模态特征，再利用SAM生成的语义掩码与初步融合图像作为条件，驱动扩散模型进行粗到细的去噪生成。
- Result: SGDFuse在主客观评估及下游任务适应性上均达到最先进性能。
- Conclusion: SGDFuse通过语义引导和扩散模型，有效解决了图像融合中的核心挑战，提供了高性能解决方案。


### [60] [B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding](https://arxiv.org/abs/2508.05269)
*Changho Choi,Youngwoo Shin,Gyojin Han,Dong-Jae Lee,Junmo Kim*

Main category: cs.CV

TL;DR: 论文提出了B4DL基准和数据处理方法，首次将4D LiDAR直接与语言理解结合，解决了动态户外环境中的时空推理问题。

- Motivation: 动态户外环境需要捕捉复杂的物体交互及其随时间的变化，但4D LiDAR在MLLMs中尚未充分探索，缺乏高质量标注和适用的模型架构。
- Method: 提出B4DL基准、可扩展的数据生成流程和直接处理原始4D LiDAR的MLLM模型。
- Result: 提供了统一的解决方案，包括渲染的4D LiDAR视频、生成的数据集和多样化场景的推理输出。
- Conclusion: B4DL和提出的模型为动态户外环境中的时空推理提供了有效工具。


### [61] [Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection](https://arxiv.org/abs/2508.05271)
*Xiaoyang Zhang,Guodong Fan,Guang-Yong Chen,Zhen Hua,Jinjiang Li,Min Gan,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 论文提出了一种基于小波变换的双频编码方法（WGDF），通过频域特征建模提升遥感图像中细微变化的检测能力，显著优于现有方法。

- Motivation: 现有深度学习方法主要依赖空间域建模，特征表示多样性有限，难以检测细微变化区域。频域特征建模（尤其是小波域）可以放大频率分量的细粒度差异，增强边缘变化的感知。
- Method: 使用离散小波变换（DWT）将图像分解为高频和低频分量，分别建模局部细节和全局结构。高频分支设计了双频特征增强模块（DFFE）和频域交互差异模块（FDID）；低频分支利用Transformer捕获全局语义关系，并采用渐进上下文差异模块（PCDM）逐步优化变化区域。最终融合高低频特征。
- Result: 在多个遥感数据集上的实验表明，WGDF显著减轻了边缘模糊问题，检测精度和鲁棒性优于现有方法。
- Conclusion: WGDF通过频域特征建模和局部-全局特征融合，有效提升了遥感图像中细微变化的检测能力。


### [62] [VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test](https://arxiv.org/abs/2508.05299)
*Meiqi Wu,Yaxuan Kang,Xuchen Li,Shiyu Hu,Xiaotang Chen,Yunfeng Kang,Weiqiang Wang,Kaiqi Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉-语义的抑郁评估方法（VS-LLM），用于自动分析PPAT草图，提升心理状态评估效率。

- Motivation: 传统PPAT草图解释依赖心理学家经验且耗时，需自动化方法支持大规模分析。
- Method: 提出VS-LLM方法，结合视觉和语义分析PPAT草图，并搭建实验环境。
- Result: 实验显示，该方法比心理学家评估准确率提升17.6%。
- Conclusion: VS-LLM方法为基于PPAT草图的心理状态评估提供了高效自动化工具。


### [63] [CoCAViT: Compact Vision Transformer with Robust Global Coordination](https://arxiv.org/abs/2508.05307)
*Xuyang Wang,Lingjuan Miao,Zhiqiang Zhou*

Main category: cs.CV

TL;DR: 论文提出CoCAViT，一种新型视觉主干网络，通过改进架构和引入CoCA机制，提升小模型在OOD数据上的泛化性能，同时保持低延迟。

- Motivation: 现有高效小模型在OOD数据上性能下降显著，表明泛化能力不足，需改进架构设计以提升鲁棒性。
- Method: 识别关键架构瓶颈，引入Coordinator-patch Cross Attention (CoCA)机制，动态全局令牌增强局部-全局特征建模。
- Result: CoCAViT-28M在ImageNet-1K上达到84.0% top-1准确率，OOD基准显著提升，COCO检测52.2 mAP，ADE20K分割51.3 mIOU。
- Conclusion: CoCAViT通过改进设计和CoCA机制，显著提升小模型的泛化能力和性能，适用于实时视觉任务。


### [64] [mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering](https://arxiv.org/abs/2508.05318)
*Xu Yuan,Liangbo Ning,Wenqi Fan,Qing Li*

Main category: cs.CV

TL;DR: 提出了一种基于多模态知识图谱的RAG框架（mKG-RAG），用于增强知识密集型VQA任务的性能。

- Motivation: 传统RAG方法依赖非结构化文档，容易引入无关或误导性内容，影响答案准确性和可靠性。
- Method: 利用MLLM提取关键词和视觉文本匹配，构建高质量多模态知识图谱，并采用双阶段检索策略。
- Result: 实验表明，该方法显著优于现有方法，在知识型VQA任务中达到新SOTA。
- Conclusion: mKG-RAG通过结构化多模态知识提升了生成效果，为知识密集型VQA任务提供了有效解决方案。


### [65] [Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting](https://arxiv.org/abs/2508.05323)
*Frank Ruis,Gertjan Burghouts,Hugo Kuijf*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本反转（TI）的方法，用于扩展视觉语言模型（VLM）的词汇表，以检测新对象或细粒度对象，同时保持模型的零样本能力。

- Motivation: 尽管大型预训练视觉语言模型在目标检测任务中表现出色，但在特定目标上的优化仍需微调，而传统微调会丧失模型的自然语言查询和零样本能力。
- Method: 采用类似文本反转（TI）的方法，通过学习新令牌或改进现有令牌，仅需少量样本即可扩展VLM的词汇表，同时保持原始权重不变。
- Result: 该方法在多种定量和定性实验中表现优异，既保留了模型的基准性能，又显著降低了计算需求。
- Conclusion: 该方法为开放词汇目标检测提供了一种高效且兼容的解决方案，同时避免了传统微调中的遗忘问题。


### [66] [3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field Rendering](https://arxiv.org/abs/2508.05343)
*Junyu Zhou,Yuyang Huang,Wenrui Dai,Junni Zou,Ziyang Zheng,Nuowen Kan,Chenglin Li,Hongkai Xiong*

Main category: cs.CV

TL;DR: 3D Gabor Splatting (3DGabSplat) 提出了一种基于3D Gabor基元的渲染方法，解决了3D高斯泼溅（3DGS）在捕捉高频细节和效率上的不足，显著提升了渲染质量和效率。

- Motivation: 3DGS因高斯函数的低通特性难以捕捉高频细节，且存在冗余基元和内存开销大的问题。
- Method: 采用多方向3D Gabor核的基元表示辐射场，结合CUDA光栅化器和频率自适应机制优化渲染。
- Result: 实验表明，3DGabSplat在PSNR上比3DGS提升1.35 dB，同时减少基元数量和内存消耗。
- Conclusion: 3DGabSplat是一种高效且高质量的渲染方法，可无缝集成到现有3DGS框架中。


### [67] [PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation](https://arxiv.org/abs/2508.05353)
*Kang Liu,Zhuoqi Ma,Zikang Fang,Yunan Li,Kun Xie,Qiguang Miao*

Main category: cs.CV

TL;DR: PriorRG是一个新型的胸部X光报告生成框架，通过两阶段训练流程模拟临床工作流，利用患者特定先验知识提升报告质量。

- Motivation: 现有方法大多忽略患者特定先验知识（如临床背景和近期影像），导致无法捕捉诊断意图或疾病进展。PriorRG旨在填补这一空白。
- Method: 两阶段训练：1）先验引导的对比预训练，利用临床背景指导特征提取；2）先验感知的粗到细解码，逐步整合先验知识与视觉编码器状态。
- Result: 在MIMIC-CXR和MIMIC-ABN数据集上，PriorRG优于现有方法，BLEU-4和F1分数分别提升3.6%和3.8%，BLEU-1提升5.9%。
- Conclusion: PriorRG通过有效利用先验知识，显著提升了胸部X光报告的临床准确性和流畅性。


### [68] [Cross-View Localization via Redundant Sliced Observations and A-Contrario Validation](https://arxiv.org/abs/2508.05369)
*Yongjun Zhang,Mingtao Xiong,Yi Wan,Gui-Song Xia*

Main category: cs.CV

TL;DR: Slice-Loc提出了一种两阶段的跨视角定位方法，通过子图像分割和几何验证提高定位可靠性，显著降低了定位误差。

- Motivation: 现有跨视角定位方法缺乏冗余观测，难以验证定位可靠性。
- Method: Slice-Loc将查询图像分割为子图像，生成冗余观测，并通过几何刚性公式过滤错误位姿，最终合并有效位姿。
- Result: Slice-Loc将定位误差从4.47米降至1.86米，方向误差从3.42°降至1.24°，错误率降至3%以下。
- Conclusion: Slice-Loc通过冗余观测和几何验证显著提升了跨视角定位的准确性和可靠性。


### [69] [CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation](https://arxiv.org/abs/2508.05375)
*Hamza Kalisch,Fabian Hörst,Jens Kleesiek,Ken Herrmann,Constantin Seibold*

Main category: cs.CV

TL;DR: CT-GRAPH是一种分层图注意力网络，通过将解剖区域结构化为图，显式建模放射学知识，显著提升了CT报告生成的准确性。

- Motivation: 自动化生成放射学报告可减轻放射科医生的工作负担，但现有方法仅依赖全局图像特征，忽略了细粒度器官关系。
- Method: 利用预训练的3D医学特征编码器获取全局和器官级特征，通过图注意力网络细化特征，并结合大型语言模型生成报告。
- Result: 在CT-RATE数据集上，F1分数比现有方法绝对提升7.9%。
- Conclusion: CT-GRAPH通过显式建模解剖关系，显著提升了报告生成的性能，代码已开源。


### [70] [Deformable Attention Graph Representation Learning for Histopathology Whole Slide Image Analysis](https://arxiv.org/abs/2508.05382)
*Mingxi Fu,Xitong Ling,Yuxuan Chen,Jiawen Li,fanglei fu,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于可变形注意力机制的新型GNN框架，用于病理图像分析，显著提升了空间依赖性的建模能力。

- Motivation: 现有方法在病理图像分类中难以捕捉组织结构的空间依赖性，且传统注意力机制缺乏特异性。
- Method: 构建动态加权有向图，结合可学习空间偏移，自适应关注形态相关区域。
- Result: 在四个基准数据集上达到最先进性能。
- Conclusion: 可变形注意力机制能有效捕捉病理图像中的复杂空间结构。


### [71] [UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation](https://arxiv.org/abs/2508.05399)
*Wonjun Kang,Byeongkeun Ahn,Minjae Lee,Kevin Galim,Seunghyuk Oh,Hyung Il Koo,Nam Ik Cho*

Main category: cs.CV

TL;DR: 论文提出了一种名为UNCAGE的无训练方法，通过利用注意力图优先解掩代表单个对象的标记，提升了文本到图像生成的组合保真度。

- Motivation: 解决文本到图像生成中属性绑定和文本-图像对齐的挑战，尤其是在掩码生成变换器中未充分探索的问题。
- Method: 提出UNCAGE方法，利用注意力图指导解掩过程，优先处理明确代表单个对象的标记。
- Result: UNCAGE在多个基准和指标上显著提升了性能，且推理开销可忽略。
- Conclusion: UNCAGE是一种高效且无需额外训练的方法，显著改善了组合保真度。


### [72] [From Detection to Correction: Backdoor-Resilient Face Recognition via Vision-Language Trigger Detection and Noise-Based Neutralization](https://arxiv.org/abs/2508.05409)
*Farah Wahida,M. A. P. Chamikara,Yashothara Shanmugarasa,Mohan Baruwal Chhetri,Thilina Ranbaduge,Ibrahim Khalil*

Main category: cs.CV

TL;DR: TrueBiometric提出了一种新方法，通过多数投票机制和校准噪声检测并修复被投毒的人脸识别训练图像，有效防御后门攻击。

- Motivation: 现有防御机制难以在不损害数据效用的情况下精确识别和修复被投毒图像，影响了系统的可靠性。
- Method: 利用多个先进的大规模视觉语言模型进行多数投票，检测被投毒图像，并通过校准噪声修复。
- Result: TrueBiometric能够100%准确检测并修复被投毒图像，且不影响干净图像的识别准确率。
- Conclusion: TrueBiometric提供了一种更实用、准确且有效的解决方案，优于现有方法。


### [73] [Physical Adversarial Camouflage through Gradient Calibration and Regularization](https://arxiv.org/abs/2508.05414)
*Jiawei Liang,Siyuan Liang,Jianjie Huang,Chenxi Si,Ming Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出了一种基于梯度优化的对抗性伪装框架，通过梯度校准和去相关方法解决了物理环境中梯度不一致和冲突的问题，显著提高了攻击成功率。

- Motivation: 物理对抗性伪装对自动驾驶等安全关键领域构成威胁，现有方法在多变环境中效果不佳，需解决梯度不一致和冲突问题。
- Method: 引入梯度校准策略确保梯度一致性，开发梯度去相关方法消除冗余或冲突更新，提升多角度优化的稳定性。
- Result: 实验表明，该方法在攻击成功率上显著优于现有技术，平均提升13.46%（距离）和11.03%（角度）。
- Conclusion: 该方法有效提升了对抗性伪装的效果，同时强调了现实场景中系统设计需更鲁棒。


### [74] [Smoothing Slot Attention Iterations and Recurrences](https://arxiv.org/abs/2508.05417)
*Rongzhen Zhao,Wenyan Yang,Juho Kannala,Joni Pajarinen*

Main category: cs.CV

TL;DR: SmoothSA改进Slot Attention（SA）及其变体，通过预热冷启动查询和区分视频帧的变换，提升对象中心学习（OCL）的性能。

- Motivation: 冷启动查询缺乏样本特定信息，影响图像或视频首帧的精确聚合；非首帧查询已具备样本信息，需不同的变换方式。
- Method: SmoothSA通过自蒸馏模块预热首帧查询，并在视频中区分首帧（全迭代）和非首帧（单迭代）的变换。
- Result: 在对象发现、识别和下游任务中表现优异，验证了方法的有效性。
- Conclusion: SmoothSA显著平滑了SA的迭代和递归过程，提升了OCL性能。


### [75] [Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions](https://arxiv.org/abs/2508.05430)
*Hubert Baniecki,Maximilian Muschalik,Fabian Fumagalli,Barbara Hammer,Eyke Hüllermeier,Przemyslaw Biecek*

Main category: cs.CV

TL;DR: FIxLIP是一种基于博弈论的语言-图像预训练模型解释方法，通过加权Banzhaf交互指数分解相似性，优于传统一阶方法。

- Motivation: 现有显著性图仅捕捉一阶归因，忽略了跨模态交互的复杂性，需要更全面的解释方法。
- Method: 利用加权Banzhaf交互指数分析模型相似性，扩展解释评估指标至二阶交互。
- Result: 在MS COCO和ImageNet-1k基准测试中，FIxLIP优于一阶归因方法，并能比较不同模型。
- Conclusion: FIxLIP提供了高质量的解释，并展示了其在模型比较中的实用性。


### [76] [How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization](https://arxiv.org/abs/2508.05461)
*Liangwei Li,Lin Liu,Juanxiu Liu,Jing Zhang,Ruqian Hao,Xiaohui Du*

Main category: cs.CV

TL;DR: 提出了一种基于Flow Matching（FM）的无监督异常检测与定位新范式，解决了传统流方法的表达能力限制。通过时间反转FM（rFM）和Worst Transport（WT）插值，构建了非概率演化路径，实现了对异常样本的有效分离。

- Motivation: 传统流方法在模型表达能力上存在局限，无法有效处理高维空间中的异常检测问题。
- Method: 提出时间反转FM（rFM）和Worst Transport（WT）插值，构建非概率演化路径，增强对样本轨迹的动态控制。
- Result: 在MVTec数据集上实现了单尺度下的最先进性能。
- Conclusion: FM为无监督异常检测提供了理论支持的计算框架，具有可扩展性和高效性。


### [77] [F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2508.05465)
*Lumin Chen,Zhiying Wu,Tianye Lei,Xuexue Bai,Ming Feng,Yuxi Wang,Gaofeng Meng,Zhen Lei,Hongbin Liu*

Main category: cs.CV

TL;DR: 提出了一种新的垂体解剖分割数据集（PAS）和F2PASeg模型，用于实时分割关键解剖结构，提高垂体手术安全性。

- Motivation: 垂体肿瘤常导致周围重要结构变形或包裹，解剖结构分割可为外科医生提供风险区域预警，但缺乏像素级标注的手术视频数据集。
- Method: 构建PAS数据集（7,845张图像），采用数据增强技术解决类别不平衡问题，提出F2PASeg模型，结合高分辨率图像特征和深度语义嵌入，增强对术中变化的鲁棒性。
- Result: F2PASeg能实时一致地分割关键解剖结构，为术中手术规划提供可靠解决方案。
- Conclusion: PAS数据集和F2PASeg模型为垂体手术解剖分割提供了有效工具，提升了手术安全性。


### [78] [Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification](https://arxiv.org/abs/2508.05489)
*Samuel Räber,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 预处理图像通过有损压缩可防御对抗扰动，但缺乏全面攻击评估。本文构建强白盒和自适应攻击，发现高真实感重建图像显著增加攻击难度。

- Motivation: 评估压缩模型对对抗攻击的鲁棒性，揭示高真实感重建图像的防御效果。
- Method: 构建白盒和自适应攻击，测试多种压缩模型，分析攻击难度与重建图像真实感的关系。
- Result: 高真实感重建的压缩模型对攻击更具抵抗力，低真实感模型易被攻破。
- Conclusion: 高真实感重建图像提供固有鲁棒性，未来对抗攻击需克服这一挑战。


### [79] [SMOL-MapSeg: Show Me One Label](https://arxiv.org/abs/2508.05501)
*Yunshuang Yuan,Frank Thiemann,Thorsten Dahms,Monika Sester*

Main category: cs.CV

TL;DR: 论文提出了一种基于声明性知识提示（OND）的方法SMOL-MapSeg，用于解决预训练基础模型在历史地图分割中的性能问题，并通过实验验证其优于UNet基线。

- Motivation: 历史地图缺乏现代图像中的一致性模式，预训练基础模型难以直接应用，需要一种新的方法来引导模型识别多变的地图概念。
- Method: 提出OND知识提示机制，替换SAM模型的提示编码器，并针对历史地图进行微调，实现用户自定义概念和模式的推理。
- Result: SMOL-MapSeg能准确分割OND知识定义的类别，并通过少量样本适应新类别，平均分割性能优于UNet。
- Conclusion: OND知识提示机制有效提升了历史地图分割的准确性和适应性，为类似领域提供了新思路。


### [80] [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
*Yufei Gao,Jiaying Fei,Nuo Chen,Ruirui Chen,Guohang Yan,Yunshi Lan,Botian Shi*

Main category: cs.CV

TL;DR: 论文提出了一种针对低资源语言的多模态大语言模型（MLLM）增强方法，强调语言能力和文化基础的双重目标，并通过MELLA数据集实现性能提升。

- Motivation: 现有方法在低资源语言中效果有限，忽视了多模态信息和文化基础的重要性。
- Method: 提出双源策略，分别收集文化相关的原生网络替代文本和语言相关的MLLM生成标题，构建MELLA数据集。
- Result: 在MELLA上微调后，模型在八种语言上表现提升，生成更丰富的描述。
- Conclusion: 文化知识和语言能力的双重增强是提升低资源语言MLLM性能的关键。


### [81] [AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated Industrial Anomaly Detection](https://arxiv.org/abs/2508.05503)
*Dongwei Ji,Bingzhang Hu,Yi Zhou*

Main category: cs.CV

TL;DR: AutoIAD是一个多智能体协作框架，用于端到端自动化开发工业视觉异常检测，显著优于现有通用协作框架和AutoML框架。

- Motivation: 工业异常检测（IAD）对制造质量控制至关重要，但传统方法需要大量手动工作。AutoIAD旨在通过自动化框架解决这一问题。
- Method: AutoIAD采用Manager-Driven中心智能体协调多个子智能体（如数据准备、模型设计等），并结合领域知识库，从原始工业图像数据开发异常检测模型。
- Result: 实验表明，AutoIAD在任务完成率和模型性能（AUROC）上显著优于现有框架，并通过迭代优化减少幻觉问题。消融研究验证了Manager智能体和知识库的关键作用。
- Conclusion: AutoIAD为工业异常检测提供了一种高效、自动化的解决方案，其设计显著提升了模型性能和鲁棒性。


### [82] [Symmetry Understanding of 3D Shapes via Chirality Disentanglement](https://arxiv.org/abs/2508.05505)
*Weikang Wang,Tobias Weißberg,Nafie El Amrani,Florian Bernard*

Main category: cs.CV

TL;DR: 论文提出了一种无监督的手性特征提取方法，用于区分形状分析中的左右对称部分，并在多个下游任务中验证了其有效性。

- Motivation: 手性信息在形状分析（如点云和网格）中尚未充分探索，现有形状描述符无法区分左右对称部分，因此开发手性特征提取器具有迫切需求。
- Method: 基于Diff3F框架，提出了一种无监督的手性特征提取流程，利用2D基础模型提取手性感知信息。
- Result: 在左-右解耦、形状匹配和部分分割等下游任务中，提取的手性特征表现出有效性和实用性。
- Conclusion: 该方法填补了形状分析中手性特征提取的空白，为相关任务提供了实用工具。


### [83] [MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips](https://arxiv.org/abs/2508.05506)
*Shibo Wang,Haonan He,Maria Parelli,Christoph Gebhardt,Zicong Fan,Jie Song*

Main category: cs.CV

TL;DR: MagicHOI利用新视角合成扩散模型作为先验，提升单目视频中手-物体重建的准确性，尤其在物体部分遮挡情况下表现优异。

- Motivation: 现有RGB手-物体重建方法依赖物体模板或假设物体完全可见，但在实际场景中，固定视角和静态抓握会导致部分物体不可见，重建结果不真实。
- Method: MagicHOI结合新视角合成扩散模型作为先验，约束未观测物体区域，并通过可见接触约束对齐手与物体。
- Result: MagicHOI显著优于现有方法，新视角合成扩散先验有效正则化未观测区域，提升重建质量。
- Conclusion: MagicHOI通过扩散模型先验和接触约束，解决了部分遮挡下的手-物体重建问题，性能优越。


### [84] [Revealing Latent Information: A Physics-inspired Self-supervised Pre-training Framework for Noisy and Sparse Events](https://arxiv.org/abs/2508.05507)
*Lin Zhu,Ruonan Liu,Xiao Wang,Lizhi Wang,Hua Huang*

Main category: cs.CV

TL;DR: 提出了一种自监督预训练框架，用于从稀疏且噪声较多的事件数据中提取边缘信息和纹理线索，提升下游任务的性能。

- Motivation: 事件相机数据稀疏且噪声多，难以有效提取特征，需要一种方法充分挖掘其潜在信息。
- Method: 框架包含三个阶段：差异引导掩码建模、主干固定的特征转换和聚焦对比学习，分别用于增强信息提取、保留表示和提升语义区分。
- Result: 实验表明，该框架在物体识别、语义分割和光流估计等任务中表现优于现有方法。
- Conclusion: 提出的框架能有效挖掘事件数据信息，提升下游任务性能。


### [85] [Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking](https://arxiv.org/abs/2508.05514)
*Zewei Wu,César Teixeira,Wei Ke,Zhang Xiong*

Main category: cs.CV

TL;DR: 论文提出了一种增强的视觉行人跟踪框架，通过结合更丰富的特征表示和鲁棒的运动模型，解决了严重遮挡场景下的跟踪问题。

- Motivation: 现实应用中，行人跟踪面临严重遮挡挑战，传统方法依赖全身边界框特征和恒定速度假设，在遮挡场景下表现不佳。
- Method: 方法结合了目标检测器的回归和分类分支特征，引入头部关键点检测以减少遮挡影响，并采用迭代卡尔曼滤波结合3D先验改进运动建模。
- Result: 提出的方法在拥挤环境中提供了更鲁棒的多目标跟踪解决方案。
- Conclusion: 通过改进外观和运动建模，该方法显著提升了遮挡场景下的跟踪性能。


### [86] [FS-IQA: Certified Feature Smoothing for Robust Image Quality Assessment](https://arxiv.org/abs/2508.05516)
*Ekaterina Shumitskaya,Dmitriy Vatolin,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: 提出了一种基于特征空间随机平滑的新型图像质量评估（IQA）模型认证防御方法，相比直接在输入空间添加噪声的方法，该方法在保持图像质量的同时提供鲁棒性保证。

- Motivation: 现有方法在输入图像中直接添加高斯噪声会降低视觉质量，而该方法旨在在保持图像保真度的同时提供鲁棒性认证。
- Method: 在特征空间而非输入空间应用噪声，通过分析主干网络Jacobian的最大奇异值，将特征空间噪声与输入空间扰动关联。支持全参考和无参考IQA模型，无需架构修改。
- Result: 在两种基准数据集上验证，涉及六种广泛使用的IQA模型，与五种先进认证防御方法相比，主观质量分数相关性提升高达30.9%。
- Conclusion: 该方法在保持图像质量的同时显著提升了IQA模型的鲁棒性和效率，计算成本低，适用于多种场景。


### [87] [Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods](https://arxiv.org/abs/2508.05519)
*Matthew Purri,Amit Patel,Erik Deurrell*

Main category: cs.CV

TL;DR: Octozi是一个结合大型语言模型和领域特定启发式方法的AI平台，显著提高了临床试验数据清理的效率和准确性。

- Motivation: 解决临床试验数据清理中手动流程效率低下的问题，应对数据量和复杂性的快速增长。
- Method: 开发Octozi平台，结合AI和领域特定启发式方法，进行对照实验研究（n=10）。
- Result: AI辅助使数据清理吞吐量提高6.03倍，错误率从54.67%降至8.48%，假阳性查询减少15.48倍。
- Conclusion: AI辅助方法可显著提升临床试验效率，加速药物开发，同时保持合规性，展示了人机协作的潜力。


### [88] [Optimal Brain Connection: Towards Efficient Structural Pruning](https://arxiv.org/abs/2508.05521)
*Shaowu Chen,Wei Ma,Binhua Huang,Qingyuan Wang,Guoxin Wang,Weize Sun,Lei Huang,Deepu John*

Main category: cs.CV

TL;DR: 本文提出了一种名为Optimal Brain Connection的结构剪枝框架，通过Jacobian Criterion评估参数重要性，并利用Equivalent Pruning机制保留剪枝连接的影响。

- Motivation: 现有结构剪枝方法常忽略参数间的相互联系，限制了剪枝效果。
- Method: 引入Jacobian Criterion评估参数重要性，并提出Equivalent Pruning机制，利用自编码器保留剪枝连接的影响。
- Result: 实验表明，Jacobian Criterion在保持模型性能上优于其他方法，Equivalent Pruning有效减少了微调后的性能下降。
- Conclusion: 提出的框架显著提升了结构剪枝的效果，同时保持了模型性能。


### [89] [When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework](https://arxiv.org/abs/2508.05526)
*Haoyu Liu,Chaoyu Gong,Mengke He,Jiate Li,Kai Han,Siqiang Luo*

Main category: cs.CV

TL;DR: SSTGNN是一种轻量级的空间-频谱-时序图神经网络框架，用于检测AI生成和操纵的视频，性能优于现有方法且参数更少。

- Motivation: 现有视频检测方法难以泛化到多种操纵类型，且通常需要大模型。
- Method: SSTGNN通过结构化图表示视频，结合空间不一致性、时序伪影和频谱失真进行联合推理，并引入可学习频谱滤波器和时序差分建模。
- Result: SSTGNN在多样基准数据集上表现优异，泛化能力强，且参数减少42.4倍。
- Conclusion: SSTGNN是一种高效、轻量且可扩展的视频检测解决方案。


### [90] [AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety](https://arxiv.org/abs/2508.05527)
*Adi Levi,Or Levi,Sardhendu Mishra,Jonathan Morra*

Main category: cs.CV

TL;DR: 论文探讨了多模态大语言模型（MLLMs）在品牌安全分类中的应用，通过引入新的多模态多语言数据集，评估了MLLMs在内容审核中的效果和成本效率。

- Motivation: 随着在线视频内容的爆炸式增长，人工审核不安全视频的需求已超出人类能力，亟需自动化解决方案。MLLMs在视频理解任务中表现优异，但在需要综合视觉和文本线索的内容审核领域尚未充分探索。
- Method: 研究引入了一个新的多模态多语言数据集，由专业审核员标注多种风险类别。通过对比分析，评估了Gemini、GPT和Llama等MLLMs在品牌安全分类中的表现。
- Result: MLLMs在品牌安全分类中表现出高效性和成本效益，但仍存在局限性。研究详细讨论了这些局限性及失败案例。
- Conclusion: 论文通过数据集和实验验证了MLLMs在内容审核中的潜力，同时指出了其不足，为未来研究提供了基础。


### [91] [Looking into the Unknown: Exploring Action Discovery for Segmentation of Known and Unknown Actions](https://arxiv.org/abs/2508.05529)
*Federico Spurio,Emad Bahrami,Olga Zatsarynna,Yazan Abu Farha,Gianpiero Francesca,Juergen Gall*

Main category: cs.CV

TL;DR: 论文提出了一种名为“Action Discovery”的新方法，用于解决部分标注数据集中模糊动作和不完整标注的问题，通过两步方法显著提升了性能。

- Motivation: 在神经科学等领域，明确的行为（如行走、进食）与模糊或低频动作共存，且数据集常因标注不完整或模糊而受限。
- Method: 提出两步法：1) Granularity-Guided Segmentation Module (GGSM) 通过模仿标注动作的粒度识别时间间隔；2) Unknown Action Segment Assignment (UASA) 基于嵌入相似性为未知动作分配语义类别。
- Result: 在三个数据集（Breakfast、50Salads、Desktop Assembly）上验证，方法显著优于现有基线。
- Conclusion: Action Discovery 方法有效解决了部分标注数据集中的动作识别问题，为模糊和未知动作提供了可行的解决方案。


### [92] [Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis](https://arxiv.org/abs/2508.05580)
*Kunyu Feng,Yue Ma,Xinhua Zhang,Boshi Liu,Yikuang Yuluo,Yinhan Zhang,Runtao Liu,Hongyu Liu,Zhiyuan Qin,Shanhui Mo,Qifeng Chen,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出了一种基于多模态大语言模型（MLLM）的框架Follow-Your-Instruction，用于自动合成高质量的2D、3D和4D数据，解决了数据收集的成本和可扩展性问题。

- Motivation: AI生成内容（AIGC）对高质量、多样化和可扩展数据的需求日益增长，但大规模真实数据收集成本高且耗时，限制了应用发展。
- Method: 通过MLLM-Collector收集多模态输入资产及其描述，利用MLLM-Generator和MLLM-Optimizer构建3D布局并进行语义优化，最后用MLLM-Planner生成时间一致的未来帧。
- Result: 实验表明，生成的合成数据显著提升了现有基线模型的性能。
- Conclusion: Follow-Your-Instruction是一个可扩展且高效的数据引擎，为生成智能提供了潜在解决方案。


### [93] [DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition](https://arxiv.org/abs/2508.05585)
*Haijing Liu,Tao Pu,Hefeng Wu,Keze Wang,Liang Lin*

Main category: cs.CV

TL;DR: DART框架通过自适应模块增强VLP模型，结合弱监督定位和LLM知识，显著提升开放词汇多标签识别性能。

- Motivation: 解决VLP模型在细粒度定位和类别依赖建模上的不足，提升开放词汇多标签识别的性能。
- Method: 提出DART框架，包含自适应细化模块（ARM）和自适应转移模块（ATM），利用弱监督和LLM知识。
- Result: 在多个基准测试中达到最先进性能。
- Conclusion: DART有效整合弱监督和外部知识，显著提升开放词汇多标签识别能力。


### [94] [WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction](https://arxiv.org/abs/2508.05599)
*Shaobin Zhuang,Yiwei Guo,Canmiao Fu,Zhipeng Huang,Zeyue Tian,Ying Zhang,Chen Li,Yali Wang*

Main category: cs.CV

TL;DR: WeTok是一种新型视觉分词器，通过分组无查找量化和生成解码技术，显著提升了压缩比和重建保真度。

- Motivation: 现有视觉分词器在压缩比和重建保真度之间存在不足，WeTok旨在填补这一空白。
- Method: 采用分组无查找量化（GQ）和生成解码（GD）两项核心技术。
- Result: 在ImageNet 50k验证集上，WeTok实现了最低的零样本rFID（0.12），并在高压缩比下表现优异。
- Conclusion: WeTok在性能和压缩效率上均超越现有方法，为视觉生成任务提供了更优的分词器。


### [95] [LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model](https://arxiv.org/abs/2508.05602)
*Tao Sun,Oliver Liu,JinJin Li,Lan Ma*

Main category: cs.CV

TL;DR: LLaVA-RE是一种基于多模态大语言模型（MLLM）的二元图像-文本相关性评估方法，通过详细任务指令和多模态上下文样本实现高效评估。

- Motivation: 评估图像与文本的相关性是衡量生成AI响应质量的关键，但现有方法难以处理多样化的文本格式和场景定义。
- Method: 采用LLaVA架构，结合任务指令和多模态上下文样本，构建二元相关性数据集。
- Result: 实验验证了LLaVA-RE框架的有效性。
- Conclusion: LLaVA-RE为二元图像-文本相关性评估提供了灵活且高效的解决方案。


### [96] [Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision](https://arxiv.org/abs/2508.05606)
*Luozheng Qin,Jia Gong,Yuqing Sun,Tianjiao Li,Mengping Yang,Xiaomeng Yang,Chao Qu,Zhiyu Tan,Hao Li*

Main category: cs.CV

TL;DR: Uni-CoT是一个统一的思维链框架，通过宏微观两级推理范式，实现了高效的多模态推理，并在多个基准测试中表现出色。

- Motivation: 解决现有方法在视觉语言推理任务中建模视觉状态转换能力不足或架构碎片化导致的推理不连贯问题。
- Method: 提出Uni-CoT框架，结合宏微观两级推理范式，并通过结构化训练范式优化计算开销。
- Result: 在WISE、RISE和KRIS基准测试中表现最优，展示了强大的泛化能力。
- Conclusion: Uni-CoT为多模态推理提供了一种高效且连贯的解决方案。


### [97] [Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity](https://arxiv.org/abs/2508.05609)
*Yuhan Zhang,Long Zhuo,Ziyang Chu,Tong Wu,Zhibing Li,Liang Pan,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: Hi3DEval是一个针对3D生成内容的分层评估框架，结合对象级和部分级评估，支持多维度分析和细粒度质量检查。

- Motivation: 现有3D内容质量评估方法主要依赖基于图像的指标，且仅关注对象级别，无法捕捉空间一致性、材料真实性和高保真局部细节。
- Method: 提出Hi3DEval框架，结合对象级和部分级评估，扩展纹理评估至材料真实性；构建Hi3DBench数据集和多代理标注流程；开发基于混合3D表示的自动评分系统。
- Result: 实验表明，Hi3DEval在建模3D特性和与人类偏好对齐方面优于现有基于图像的指标。
- Conclusion: Hi3DEval为3D生成内容提供了可扩展的评估方法，优于传统手动评估。


### [98] [Test-Time Reinforcement Learning for GUI Grounding via Region Consistency](https://arxiv.org/abs/2508.05615)
*Yong Du,Yuchen Yan,Fei Tang,Zhengxi Lu,Chang Zong,Weiming Lu,Shengpei Jiang,Yongliang Shen*

Main category: cs.CV

TL;DR: 提出GUI-RC和GUI-RCPO方法，通过测试时空间投票和强化学习提升GUI定位精度，无需额外训练数据。

- Motivation: 现有方法依赖高成本像素级标注，限制了GUI定位任务的效率和泛化能力。
- Method: GUI-RC利用多预测的空间重叠模式构建投票网格，GUI-RCPO将一致性模式转化为奖励进行测试时强化学习。
- Result: GUI-RC提升准确率2-3%，GUI-RCPO进一步优化至85.14%。
- Conclusion: 测试时缩放和强化学习为GUI定位提供了高效且数据友好的解决方案。


### [99] [MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes](https://arxiv.org/abs/2508.05630)
*Henghui Ding,Kaining Ying,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang,Philip H. S. Torr,Song Bai*

Main category: cs.CV

TL;DR: MOSEv2是一个更具挑战性的视频对象分割数据集，旨在推动真实场景下的研究，包含复杂场景和多样化挑战。

- Motivation: 现有VOS数据集主要包含显著、孤立的对象，限制了在真实场景中的泛化能力。MOSEv2旨在填补这一空白。
- Method: MOSEv2包含5,024个视频和701,976个高质量掩码，涵盖200个类别，引入了更复杂的场景和挑战。
- Result: 在MOSEv2上，代表性VOS方法性能显著下降（如SAM2从76.4%降至50.9%），表明现有方法在真实场景中仍有不足。
- Conclusion: MOSEv2为VOS研究提供了更真实的测试平台，揭示了当前方法的局限性。


### [100] [GAP: Gaussianize Any Point Clouds with Text Guidance](https://arxiv.org/abs/2508.05631)
*Weiqi Zhang,Junsheng Zhou,Haotian Geng,Wenyuan Zhang,Yu-Shen Liu*

Main category: cs.CV

TL;DR: GAP是一种将原始点云转化为高保真3D高斯分布的新方法，利用文本指导和多视角优化框架，解决了无色点云生成高斯的挑战。

- Motivation: 无色3D点云直接生成高斯分布是一个未解决的难题，而点云作为一种广泛使用的3D表示形式，填补这一空白具有重要意义。
- Method: GAP采用多视角优化框架，结合深度感知的图像扩散模型合成一致外观，并通过表面锚定机制约束高斯分布的位置。此外，还引入了基于扩散的修复策略处理难以观察的区域。
- Result: GAP在从合成点云到真实扫描和大规模场景的不同复杂度任务中表现优异。
- Conclusion: GAP成功地将无色点云转化为高质量的高斯分布，为点云与高斯之间的桥梁提供了有效解决方案。


### [101] [FaceAnonyMixer: Cancelable Faces via Identity Consistent Latent Space Mixing](https://arxiv.org/abs/2508.05636)
*Mohammed Talha Alam,Fahad Shamshad,Fakhri Karray,Karthik Nandakumar*

Main category: cs.CV

TL;DR: FaceAnonyMixer是一个可取消的人脸生成框架，通过混合真实人脸图像的潜在代码与可撤销密钥生成的合成代码，生成隐私保护的人脸图像，同时满足生物识别模板保护的要求。

- Motivation: 随着人脸识别技术的发展，隐私保护需求增加，现有方法无法满足生物识别模板保护的要求（如可撤销性、不可链接性和不可逆性）。
- Method: 利用预训练生成模型的潜在空间，混合真实人脸图像的潜在代码与可撤销密钥生成的合成代码，并通过多目标损失函数优化。
- Result: 在基准数据集上，FaceAnonyMixer在保持高识别精度的同时，提供了更强的隐私保护，商业API上性能提升超过11%。
- Conclusion: FaceAnonyMixer是一种高效且隐私保护的人脸匿名化方法，适用于现有的人脸识别系统。
## cs.AI

### [102] [QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering](https://arxiv.org/abs/2508.05197)
*Zhuohang Jiang,Pangjing Wu,Xu Yuan,Wenqi Fan,Qing Li*

Main category: cs.AI

TL;DR: QA-Dragon是一个查询感知的动态RAG系统，通过混合文本和图像检索策略，提升复杂VQA任务的表现。

- Motivation: 现有RAG方法通常仅从文本或图像中检索，无法满足需要多跳推理或最新知识的复杂查询需求。
- Method: QA-Dragon引入领域路由器和搜索路由器，动态选择检索策略，支持多模态、多轮和多跳推理。
- Result: 在KDD Cup 2025的Meta CRAG-MM挑战中，QA-Dragon显著提升基础模型的推理性能，单源、多源和多轮任务分别提升5.06%、6.35%和5.03%。
- Conclusion: QA-Dragon通过动态混合检索策略，有效解决了复杂VQA任务中的知识密集和多模态推理问题。
## cs.CL

### [103] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: 提出了一种基于视觉语言聚类的框架，用于更全面地评估视觉活动识别系统，解决了动词语义和图像解释的模糊性问题。

- Motivation: 标准精确匹配评估方法无法捕捉动词语义和图像解释的模糊性，导致模型性能评估不完整。
- Method: 采用视觉语言聚类框架构建动词意义簇，分析imSitu数据集，比较聚类评估与标准评估方法。
- Result: 每张图像平均映射到2.8个意义簇，聚类评估更符合人类判断。
- Conclusion: 聚类评估提供了更细致的模型性能评估，优于标准方法。
## eess.IV

### [104] [Advanced Multi-Architecture Deep Learning Framework for BIRADS-Based Mammographic Image Retrieval: Comprehensive Performance Analysis with Super-Ensemble Optimization](https://arxiv.org/abs/2508.04790)
*MD Shaikh Rahman,Feiroz Humayara,Syed Maudud E Rabbi,Muhammad Mahbubur Rashid*

Main category: eess.IV

TL;DR: 该论文提出了一种基于CNN架构的乳腺图像检索系统，通过高级训练策略和严格评估框架，显著提升了精确度，并提供了临床部署的指南。

- Motivation: 解决现有医学图像检索研究中样本量不足、数据分割不当和统计验证不足的问题，以支持临床诊断和质量保证。
- Method: 系统比较了DenseNet121、ResNet50和VGG16等CNN架构，结合精细调优、度量学习和超级集成优化等训练策略，采用分层数据分割和统计验证。
- Result: DenseNet121和ResNet50的精确度显著提升，超级集成优化达到36.33%的精确度，显著超过文献中的预期值。
- Conclusion: 该框架为乳腺图像检索任务设定了新的性能基准，并为临床部署提供了基于证据的架构选择指南。


### [105] [CryoGS: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction](https://arxiv.org/abs/2508.04929)
*Suyi Chen,Haibin Ling*

Main category: eess.IV

TL;DR: 论文提出了一种名为cryoGS的新方法，通过高斯混合模型（GMM）和物理启发的改进，直接从原始冷冻电镜图像中重建分子结构，无需依赖外部初始化。

- Motivation: 冷冻电镜（cryo-EM）在结构生物学中至关重要，但现有方法依赖外部共识图或原子模型初始化，限制了其独立使用。
- Method: cryoGS结合高斯喷溅和冷冻电镜成像物理，开发了正交投影感知的高斯喷溅技术，并引入归一化项和FFT对齐坐标系。
- Result: 实验证明cryoGS在真实数据集上比基线方法更有效和稳健。
- Conclusion: cryoGS为冷冻电镜重建提供了一种独立且高效的新方法，代码将在发表后公开。


### [106] [Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations](https://arxiv.org/abs/2508.05168)
*Caner Özer,Patryk Rygiel,Bram de Wilde,İlkay Öksüz,Jelmer M. Wolterink*

Main category: eess.IV

TL;DR: 提出了一种基于隐式神经表示（INRs）的医学图像质量评估方法，解决了传统方法因预处理导致信息丢失和高内存需求的问题。

- Motivation: 医学图像中的伪影影响诊断准确性，传统基于图像的方法因预处理和高内存需求限制了分类模型的可扩展性。
- Method: 利用INRs提供紧凑连续的医学图像表示，结合深度权重空间网络、图神经网络和关系注意力变换器进行质量评估。
- Result: 在ACDC数据集上验证了方法的有效性，能以更少参数实现相似性能。
- Conclusion: INRs为医学图像质量评估提供了一种高效且可扩展的解决方案。


### [107] [Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer](https://arxiv.org/abs/2508.05240)
*Junyi Wang,Xi Zhu,Yikun Guo,Zixi Wang,Haichuan Gao,Le Zhang,Fan Zhang*

Main category: eess.IV

TL;DR: 提出了一种基于3D CycleGAN的MR与US图像配准流程，通过生成合成T1图像提升配准性能。

- Motivation: 解决术前MR与术后US图像配准的挑战，提升配准一致性。
- Method: 使用3D CycleGAN进行无配对风格迁移生成合成T1图像，结合仿射和局部可变形变换实现粗到细配准。
- Result: 在多数情况下提升了MR与US图像对的配准一致性。
- Conclusion: 该方法有效改善了MR与US图像的配准效果。


### [108] [Artificial Intelligence-Based Classification of Spitz Tumors](https://arxiv.org/abs/2508.05391)
*Ruben T. Lucassen,Marjanna Romers,Chiel F. Ebbelaar,Aia N. Najem,Donal P. Hayes,Antien L. Mooyaart,Sara Roshani,Liliane C. D. Wynaendts,Nikolas Stathonikos,Gerben E. Breimer,Anne M. L. Jansen,Mitko Veta,Willeke A. M. Blokx*

Main category: eess.IV

TL;DR: AI模型在区分Spitz肿瘤与常规黑色素瘤方面表现优异（AUROC 0.95，准确率0.86），在预测基因异常和诊断类别上优于随机猜测，且表现优于经验丰富的病理学家。

- Motivation: Spitz肿瘤与常规黑色素瘤在组织学特征上存在重叠，诊断困难，因此研究AI模型是否能辅助诊断。
- Method: 使用包含393例Spitz肿瘤和379例常规黑色素瘤的数据集开发和验证AI模型，并与四位病理学家进行比较。
- Result: AI模型在区分任务中表现优异，基因异常和诊断类别预测准确率分别为0.55和0.51，优于随机猜测。模拟实验显示AI可降低成本和周转时间。
- Conclusion: AI模型在区分Spitz肿瘤与常规黑色素瘤方面具有潜力，但在更复杂的任务中仍需改进。
## cs.GR

### [109] [Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off](https://arxiv.org/abs/2508.04825)
*Seungyong Lee,Jeong-gi Kwak*

Main category: cs.GR

TL;DR: Voost是一个统一的扩散变换器框架，联合学习虚拟试穿和试脱，通过双向监督增强服装-身体关系推理，无需额外网络或标签。

- Motivation: 虚拟试穿中服装与身体的准确对应在姿态和外观变化下仍具挑战性，需更统一的解决方案。
- Method: Voost采用扩散变换器联合建模试穿和试脱任务，支持灵活生成方向与服装类别条件，并引入注意力温度缩放和自校正采样技术。
- Result: Voost在试穿和试脱基准测试中均达到最先进水平，在对齐精度、视觉保真度和泛化性上优于基线。
- Conclusion: Voost通过联合学习和推理技术，显著提升了虚拟试穿和试脱的性能和灵活性。


### [110] [Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting](https://arxiv.org/abs/2508.04965)
*Zijian Wang,Beizhen Zhao,Hao Wang*

Main category: cs.GR

TL;DR: 提出了一种新的感知-采样-压缩框架，用于优化3D高斯泼溅技术，解决了大规模场景管理和高效存储的问题。

- Motivation: 传统3DGS在大规模场景管理和存储效率上表现不佳，尤其是在复杂环境或有限计算资源下。
- Method: 提出场景感知补偿算法和金字塔采样表示，并结合广义高斯混合模型压缩算法。
- Result: 显著提高了内存效率和视觉质量，同时保持实时渲染速度。
- Conclusion: 该方法在资源优化和视觉保真度方面取得了显著改进。


### [111] [Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction](https://arxiv.org/abs/2508.04966)
*Yifan Zhou,Beizhen Zhao,Pengcheng Wu,Hao Wang*

Main category: cs.GR

TL;DR: 提出了一种结合显式-隐式函数的动态3D高斯泼溅框架，解决了现有方法在动态场景中的过平滑和特征碰撞问题。

- Motivation: 动态3D高斯泼溅在保留运动细节和变形一致性之间存在频谱冲突，导致现有方法效果不佳。
- Method: 采用频谱感知的拉普拉斯编码架构、增强的高斯动态属性以及基于KDTree的自适应高斯分割策略。
- Result: 实验表明，该方法在复杂动态场景重建中表现优异，重建保真度更高。
- Conclusion: 提出的混合框架有效解决了动态3D高斯泼溅的挑战，性能达到最先进水平。


### [112] [A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding](https://arxiv.org/abs/2508.05064)
*Mahmoud Chick Zaouali,Todd Charter,Yehor Karpichev,Brandon Haworth,Homayoun Najjjaran*

Main category: cs.GR

TL;DR: 综述探讨了语言模型与3D高斯泼溅技术的结合，分析了其理论、应用及挑战。

- Motivation: 填补语言引导与3D高斯泼溅技术结合的研究空白，推动语言引导的3D场景理解发展。
- Method: 通过结构化综述，总结理论基础、整合策略及实际用例。
- Result: 揭示了计算瓶颈、泛化能力不足和语义标注数据稀缺等关键限制。
- Conclusion: 提出了未来研究方向，以推动语言引导的高斯泼溅技术在3D场景理解中的应用。


### [113] [RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer](https://arxiv.org/abs/2508.05115)
*Fangyu Du,Taiqing Li,Ziwei Zhang,Qian Qiao,Tan Yu,Dingcheng Zhen,Xu Jia,Yang Yang,Shunshun Yin,Siyuan Liu*

Main category: cs.GR

TL;DR: RAP提出了一种实时音频驱动肖像动画框架，通过混合注意力机制和静态-动态训练推理范式，实现了高质量、低延迟的动画生成。

- Motivation: 现有方法虽然能生成高质量动画，但计算复杂度高，无法满足实时部署需求。RAP旨在在保持高质量的同时实现实时性能。
- Method: RAP采用混合注意力机制实现精细音频控制，并通过静态-动态训练推理范式避免显式运动监督。
- Result: 实验表明，RAP在实时约束下实现了最先进的性能，同时保持了高视觉保真度和音频-视觉同步。
- Conclusion: RAP为实时音频驱动肖像动画提供了一种高效且高质量的解决方案。


### [114] [Refining Gaussian Splatting: A Volumetric Densification Approach](https://arxiv.org/abs/2508.05187)
*Mohamed Abdul Gafoor,Marius Preda,Titus Zaharia*

Main category: cs.GR

TL;DR: 提出了一种基于惯性体积的新型密度控制方法，改进了3D高斯泼溅（3DGS）的点基元管理，提升了新视角合成的质量。

- Motivation: 传统3DGS的密度控制策略存在不足，影响了新视角合成的质量。
- Method: 利用高斯函数的惯性体积指导细化过程，并研究了传统SfM和DIM方法对点云初始化的影响。
- Result: 在Mip-NeRF 360数据集上的实验表明，该方法在重建质量上优于3DGS。
- Conclusion: 提出的方法显著提升了3DGS的性能，适用于多样化场景。


### [115] [Point cloud segmentation for 3D Clothed Human Layering](https://arxiv.org/abs/2508.05531)
*Davide Garavaso,Federico Masi,Pietro Musoni,Umberto Castellani*

Main category: cs.GR

TL;DR: 论文提出了一种新的3D点云分割范式，用于同时关联不同层次的衣物和身体部分，解决了传统分割方法在衣物建模中的局限性。

- Motivation: 在3D衣物建模中，高质量结果难以实现，尤其是真实褶皱的生成。现有3D扫描缺乏语义信息，而传统分割方法无法处理衣物层间的重叠问题。
- Method: 提出了一种名为“衣物人体分层”的分割范式，创建了合成数据集，并评估了不同神经网络设置来处理3D衣物分层问题。
- Result: 实验表明，引入针对衣物领域的适当分割策略在合成和真实扫描数据集中均有效。
- Conclusion: 新方法能够有效估计衣物遮挡的底层身体部分和不可见衣物区域，为3D衣物建模提供了更可靠的语义重建。


### [116] [Physically Controllable Relighting of Photographs](https://arxiv.org/abs/2508.05626)
*Chris Careaga,Yağız Aksoy*

Main category: cs.GR

TL;DR: 提出一种自监督的图像重光照方法，结合传统渲染的物理精确性与神经渲染的逼真效果，实现完全可控的光照编辑。

- Motivation: 将传统3D图形工具中的物理光照控制能力引入真实场景的图像重光照中。
- Method: 通过单目几何和固有成分估计推断彩色网格表示，结合路径追踪引擎和神经渲染器实现光照编辑。
- Result: 实现了逼真的重光照效果，并在自监督训练中验证了方法的有效性。
- Conclusion: 该方法为真实场景的光照编辑提供了物理精确且逼真的解决方案。
## cs.RO

### [117] [Learning to See and Act: Task-Aware View Planning for Robotic Manipulation](https://arxiv.org/abs/2508.05186)
*Yongjie Bai,Zhouxia Wang,Yang Liu,Weixing Chen,Ziliang Chen,Mingtong Dai,Yongsen Zheng,Lingbo Liu,Guanbin Li,Liang Lin*

Main category: cs.RO

TL;DR: 论文提出了一种任务感知视角规划（TAVP）框架，通过主动视角规划和任务特定表示学习，解决了现有视觉-语言-动作（VLA）模型在3D感知和任务干扰方面的局限性。

- Motivation: 现有VLA模型依赖静态视角和共享视觉编码器，限制了3D感知能力并导致任务干扰，影响了模型的鲁棒性和泛化能力。
- Method: TAVP框架结合了主动视角规划和任务特定表示学习，采用高效探索策略和新型伪环境加速视角获取，并引入混合专家（MoE）视觉编码器以解耦任务特征。
- Result: 在RLBench任务上的实验表明，TAVP模型显著优于现有固定视角方法，生成了更完整和区分性的视觉表示。
- Conclusion: TAVP通过任务感知的视觉表示学习，显著提升了动作预测能力，为多任务机器人操作提供了更鲁棒和泛化的解决方案。


### [118] [DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model](https://arxiv.org/abs/2508.05402)
*Rui Yu,Xianghang Zhang,Runkai Zhao,Huaicheng Yan,Meng Wang*

Main category: cs.RO

TL;DR: DistillDrive是一种基于知识蒸馏的端到端自动驾驶模型，通过多样化实例模仿增强多模式运动特征学习，显著降低碰撞率并提升闭环性能。

- Motivation: 现有自动驾驶研究过于关注自车状态，缺乏规划导向的理解，限制了决策过程的鲁棒性。
- Method: 利用基于结构化场景表示的规划模型作为教师模型，通过多样化规划实例作为多目标学习目标，结合强化学习和生成建模优化状态到决策的映射。
- Result: 在nuScenes和NAVSIM数据集上验证，碰撞率降低50%，闭环性能提升3分。
- Conclusion: DistillDrive通过知识蒸馏和多样化实例模仿，显著提升了自动驾驶的鲁棒性和性能。


### [119] [Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling](https://arxiv.org/abs/2508.05634)
*Jianpeng Yao,Xiaopan Zhang,Yu Xia,Zejin Wang,Amit K. Roy-Chowdhury,Jiachen Li*

Main category: cs.RO

TL;DR: 论文提出了一种通过考虑行人不确定性来增强移动机器人在人群导航中鲁棒性的方法，使用自适应共形推理和约束强化学习。

- Motivation: 解决强化学习训练的移动机器人在面对分布外场景时性能下降的问题。
- Method: 通过自适应共形推理生成预测不确定性估计，并利用约束强化学习指导机器人行为。
- Result: 在分布内场景中，成功率提高8.80%，碰撞减少3.72倍，侵入人类轨迹减少2.43倍；在分布外场景中表现出更强的鲁棒性。
- Conclusion: 该方法在真实机器人上验证了其安全性和鲁棒性，适用于稀疏和密集人群。


### [120] [Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation](https://arxiv.org/abs/2508.05635)
*Yue Liao,Pengfei Zhou,Siyuan Huang,Donglin Yang,Shengcong Chen,Yuxin Jiang,Yue Hu,Jingbin Cai,Si Liu,Jianlan Luo,Liliang Chen,Shuicheng Yan,Maoqing Yao,Guanghui Ren*

Main category: cs.RO

TL;DR: Genie Envisioner (GE) 是一个统一的机器人操作平台，集成了策略学习、评估和仿真，基于视频生成框架。

- Motivation: 提供一个可扩展且实用的基础平台，支持指令驱动的通用机器人智能。
- Method: GE-Base 是一个大规模的视频扩散模型，GE-Act 将潜在表示映射为可执行动作，GE-Sim 作为神经模拟器生成高保真仿真。
- Result: 平台支持跨多样化的机器人实现精确和通用的策略推断，并提供了标准化基准 EWMBench。
- Conclusion: Genie Envisioner 是一个可扩展的、实用的机器人智能基础平台，所有代码和模型将公开。
## cs.LG

### [121] [ALScope: A Unified Toolkit for Deep Active Learning](https://arxiv.org/abs/2508.04937)
*Chenkai Wu,Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Gang Liu,Wray Buntine,Lan Du*

Main category: cs.LG

TL;DR: ALScope是一个新的深度主动学习平台，整合了多种数据集和算法，用于公平评估不同条件下的性能。

- Motivation: 由于缺乏统一的评估平台，难以公平比较深度主动学习算法在复杂场景（如分布偏移和数据不平衡）下的表现。
- Method: 开发ALScope平台，整合10个CV和NLP数据集及21种DAL算法，支持灵活配置实验因素。
- Result: 实验表明，DAL算法性能因领域和任务设置而异，非标准场景下仍有改进空间，部分算法耗时较长。
- Conclusion: ALScope为深度主动学习提供了全面评估工具，揭示了算法在不同场景下的表现和改进方向。


### [122] [Learning from Oblivion: Predicting Knowledge Overflowed Weights via Retrodiction of Forgetting](https://arxiv.org/abs/2508.05059)
*Jinhyeok Jang,Jaehong Kim,Jung Uk Kim*

Main category: cs.LG

TL;DR: 提出了一种名为KNOW的新策略，通过结构化遗忘及其逆过程合成知识增强的预训练权重，提升下游任务性能。

- Motivation: 解决如何获取超越给定数据集的更优预训练权重的问题。
- Method: 利用结构化遗忘和其逆过程，通过元学习建模权重预测，构建KNOWN超模型预测增强权重。
- Result: 实验表明KNOW预测优于朴素微调和简单权重预测，提升下游任务表现。
- Conclusion: 通过重新解释遗忘动态，推动了深度学习知识转移的极限。


### [123] [Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning](https://arxiv.org/abs/2508.05224)
*Mirko Konstantin,Anirban Mukhopadhyay*

Main category: cs.LG

TL;DR: 提出了一种去中心化的联邦学习框架LIGHTYEAR，通过本地验证集计算一致性分数，选择个性化更新，提升性能和鲁棒性。

- Motivation: 传统集中式联邦学习存在单点故障、个性化不足和对分布变化的脆弱性等问题，需要更灵活和稳健的解决方案。
- Method: 采用P2P拓扑结构，通过本地验证集计算一致性分数，选择可信更新，并结合正则化项进行聚合。
- Result: 在两个数据集上的实验表明，LIGHTYEAR在客户端性能和对抗性条件下优于集中式和现有P2P方法。
- Conclusion: LIGHTYEAR框架通过去中心化和个性化更新选择，显著提升了联邦学习的性能和鲁棒性。


### [124] [Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning](https://arxiv.org/abs/2508.05316)
*Yue Duan,Taicai Chen,Lei Qi,Yinghuan Shi*

Main category: cs.LG

TL;DR: USP是一个半监督持续学习框架，通过特征空间保留、分治伪标记和类均值锚定无标签蒸馏，协同提升学习塑性、无标签学习和记忆稳定性。

- Motivation: 减少标注成本并处理持续数据流，同时解决无标签学习、记忆稳定性和学习塑性之间的平衡问题。
- Method: 1. 特征空间保留（FSR）提升学习塑性；2. 分治伪标记（DCP）处理无标签数据；3. 类均值锚定无标签蒸馏（CUD）增强记忆稳定性。
- Result: USP在最终准确率上比现有方法提升高达5.94%。
- Conclusion: USP有效解决了半监督持续学习中的关键挑战，并在性能上显著优于现有方法。


### [125] [Parameter-free entropy-regularized multi-view clustering with hierarchical feature selection](https://arxiv.org/abs/2508.05504)
*Kristina P. Sinaga,Sara Colantonio,Miin-Shen Yang*

Main category: cs.LG

TL;DR: 论文提出两种互补算法AMVFCM-U和AAMVFCM-U，用于多视图聚类，通过熵正则化和自适应特征加权实现无参数框架，显著提升计算效率和降维效果。

- Motivation: 多视图聚类面临高维特征和无关信息管理的挑战，传统方法依赖手动参数调整且缺乏跨视图集成机制。
- Method: 采用熵正则化替代模糊参数，基于信噪比的特征加权和双级熵项自动平衡视图与特征贡献，AAMVFCM-U进一步通过自适应阈值实现分层降维。
- Result: 在五个基准测试中优于15种先进方法，AAMVFCM-U计算效率提升97%，降维至原始尺寸的0.45%，并能自动识别关键视图组合。
- Conclusion: 提出的算法在多视图聚类中实现了高效、自动化的模式发现，显著优于现有方法。


### [126] [Adapting Vision-Language Models Without Labels: A Comprehensive Survey](https://arxiv.org/abs/2508.05547)
*Hao Dong,Lijun Sheng,Jian Liang,Ran He,Eleni Chatzi,Olga Fink*

Main category: cs.LG

TL;DR: 本文提出了一种无监督视觉语言模型（VLM）适应的分类法，总结了四种关键范式，并分析了相关方法和挑战。

- Motivation: 尽管VLM在许多任务中表现出色，但在特定下游任务中性能不足，且缺乏无监督适应的系统性研究。
- Method: 提出基于未标记视觉数据的分类法，分为四种范式：数据自由迁移、无监督域迁移、批量测试时适应和流式测试时适应。
- Result: 系统分析了每种范式的核心方法和策略，并总结了代表性基准和未来研究方向。
- Conclusion: 本文填补了无监督VLM适应领域的空白，为未来研究提供了框架和方向。


### [127] [X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment](https://arxiv.org/abs/2508.05568)
*Qinghua Yao,Xiangrui Xu,Zhize Li*

Main category: cs.LG

TL;DR: X-VFL是一个新的垂直联邦学习框架，解决了数据样本不对齐和缺失特征的问题，并支持本地独立推理。

- Motivation: 垂直联邦学习（VFL）面临数据样本必须对齐且不支持本地独立推理的挑战。
- Method: X-VFL设计了两个模块：XCom（完成缺失特征）和DS-Align（决策子空间对齐），并提供了收敛性理论。
- Result: 实验显示X-VFL在CIFAR-10和MIMIC-III数据集上分别提升了15%和43%的准确率。
- Conclusion: X-VFL在缺失特征和本地推理场景中表现出优越性。
