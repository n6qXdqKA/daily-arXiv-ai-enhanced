[[toc]]

## cs.CV

### [1] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 论文提出了一种基于脉冲神经网络的神经形态眼动追踪模型，通过用轻量级LIF层替换传统模型的循环和注意力模块，并利用深度可分离卷积降低复杂度，在保持精度的同时大幅提升了能效。

- Motivation: 可穿戴系统的眼动追踪需要低延迟和毫瓦级功耗，但传统基于帧的管道存在运动模糊、计算成本高和时间分辨率有限的问题。神经形态传感器和脉冲神经网络提供了有前景的替代方案，但现有SNN方法要么过于专门化，要么性能不及现代ANN架构。
- Method: 开发了高性能事件驱动眼动追踪模型的神经形态版本，用轻量级LIF（泄漏积分发放）层替换其循环和注意力模块，并利用深度可分离卷积来降低模型复杂度。
- Result: 模型获得3.7-4.1像素的平均误差，接近专用神经形态系统Retina（3.24像素）的精度，同时与所提模型最接近的ANN变体相比，模型大小减少了20倍，理论计算量减少了850倍。这些高效变体预计在1kHz频率下以3.9-4.9 mW功耗和3 ms延迟运行。
- Conclusion: 结果表明，高性能事件驱动眼动追踪架构可以重新设计为SNN，在保持实时可穿戴部署所需精度的同时，获得显著的效率提升。


### [2] [ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects](https://arxiv.org/abs/2512.10031)
*Woojin Lee,Hyugjae Chang,Jaeho Moon,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: ABBSPO框架通过自适应边界框缩放和对称先验角度损失，在弱监督定向目标检测中实现最先进性能。

- Motivation: 现有HBox监督的OOD方法直接将GT水平框与预测旋转框的最小外接矩形比较，导致尺度估计不准确，需要改进。
- Method: 提出ABBSPO框架：1) 自适应边界框缩放(ABBS)，适当缩放GT水平框以优化每个预测旋转框的尺寸；2) 对称先验角度(SPA)损失，利用航空目标的固有对称性进行自监督学习。
- Result: 广泛的实验结果表明ABBSPO实现了最先进的性能，超越了现有方法。
- Conclusion: ABBSPO有效解决了弱监督定向目标检测中的尺度估计和角度预测问题，为HBox监督的OOD提供了更准确的解决方案。


### [3] [Diffusion Is Your Friend in Show, Suggest and Tell](https://arxiv.org/abs/2512.10038)
*Jia Cheng Hu,Roberto Cavicchioli,Alessandro Capotondi*

Main category: cs.CV

TL;DR: 提出SST方法，将扩散模型作为建议模块与自回归模型结合，在图像描述任务上取得SOTA结果

- Motivation: 扩散去噪模型在连续域表现出色，但在离散域无法超越自回归模型。作者提出结合两者优势的新范式：用扩散模型为自回归生成提供建议，而不是替代它
- Method: 提出Show, Suggest and Tell (SST)方法，将扩散模型作为建议模块与自回归模型结合。扩散模型提供双向和精炼能力，自回归模型提供强语言结构
- Result: 在COCO数据集上达到125.1 CIDEr-D（无强化学习），比自回归和扩散模型的SOTA分别高出1.5和2.5分。实验显示建议质量与描述质量正相关
- Conclusion: 将扩散模型作为建议模块与自回归模型结合是当前未被充分探索但前景广阔的研究方向，SST展示了这种混合方法的有效性


### [4] [MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata](https://arxiv.org/abs/2512.10041)
*Yihao Liu,Chenyu Gao,Lianrui Zuo,Michael E. Kim,Brian D. Boyd,Lisa L. Barnes,Walter A. Kukull,Lori L. Beason-Held,Susan M. Resnick,Timothy J. Hohman,Warren D. Taylor,Bennett A. Landman*

Main category: cs.CV

TL;DR: MetaVoxel是一个生成式联合扩散建模框架，通过单一扩散过程学习医学影像和临床元数据的联合分布，统一了传统上需要单独条件模型的任务，支持无需特定任务重新训练的灵活零样本推理。

- Motivation: 当前深度学习模型通常针对特定预测方向和特定输入变量集进行训练，这限制了模型的灵活性和临床应用范围。需要一种能够统一多种医学AI任务并支持灵活推理的方法。
- Method: 提出MetaVoxel框架，通过单一扩散过程学习成像数据和临床元数据的联合分布。该方法能够捕获所有变量之间的完整关系，支持使用任意输入子集进行零样本推理。
- Result: 在超过10,000个T1加权MRI扫描和临床元数据上验证，单个MetaVoxel模型能够执行图像生成、年龄估计和性别预测，性能与专门的任务特定基线模型相当。实验还展示了其灵活推理能力。
- Conclusion: 联合多模态扩散为统一医学AI模型和扩大临床应用范围提供了有前景的方向，MetaVoxel展示了通过单一模型实现多种医学成像任务的可行性。


### [5] [Independent Density Estimation](https://arxiv.org/abs/2512.10067)
*Jiahao Liu*

Main category: cs.CV

TL;DR: 提出独立密度估计（IDE）方法，通过建立句子中单词与图像特征之间的连接来实现组合泛化，在多个数据集上优于现有模型。

- Motivation: 大规模视觉语言模型在图像描述和条件图像生成等领域取得了显著成果，但在实现类似人类的组合泛化方面仍面临困难。
- Method: 提出独立密度估计（IDE）方法，学习句子中单个单词与图像对应特征之间的连接。构建了两种模型：一种使用完全解耦的视觉表示作为输入，另一种利用变分自编码器从原始图像中获取部分解耦特征。还提出了基于熵的组合推理方法来结合句子中每个单词的预测。
- Result: 在多个数据集上评估，模型在未见过的组合上表现出优于当前模型的泛化能力。
- Conclusion: IDE方法能够有效提升视觉语言模型的组合泛化能力，为解决这一挑战提供了新的技术途径。


### [6] [TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing](https://arxiv.org/abs/2512.10095)
*Jiachen Tao,Junyi Wu,Haoxuan Wang,Zongxin Yang,Dawen Cai,Yan Yan*

Main category: cs.CV

TL;DR: TraceFlow：通过残差材料增强的2D高斯泼溅表示和动态环境高斯，结合光栅化与光线追踪的混合渲染，实现动态镜面场景的高保真渲染

- Motivation: 解决动态镜面场景渲染中的两个关键挑战：精确反射方向估计和物理准确的反射建模，以在复杂动态环境中产生更锐利、更真实的镜面反射
- Method: 1. 提出残差材料增强的2D高斯泼溅表示，建模动态几何和材料属性以准确计算反射光线；2. 引入动态环境高斯和混合渲染管线，将渲染分解为漫反射和镜面反射分量；3. 设计从粗到精的训练策略，提高优化稳定性并促进物理有意义的分解
- Result: 在动态场景基准测试中，TraceFlow在定量和定性评估上都优于现有方法，在复杂动态环境中产生更锐利、更真实的镜面反射
- Conclusion: TraceFlow通过创新的表示方法和混合渲染管线，成功解决了动态镜面场景渲染的关键挑战，实现了高质量的物理准确反射效果


### [7] [Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information](https://arxiv.org/abs/2512.10102)
*Neelima Prasad,Jarek Reynolds,Neel Karsanbhai,Tanusree Sharma,Lotus Zhang,Abigale Stangl,Yang Wang,Leah Findlater,Danna Gurari*

Main category: cs.CV

TL;DR: 提出层次实例跟踪新任务，建立首个包含2765个实体、552个视频、40个类别的基准数据集，评估7种模型变体显示任务具有挑战性

- Motivation: 现有跟踪任务主要关注单一层次的对象跟踪，缺乏对对象与部件之间层次关系的建模。需要开发能够同时跟踪对象及其部件并维护它们层次关系的新任务
- Method: 提出层次实例跟踪任务，创建包含2765个独特实体、552个视频、40个类别（对象和部件）的基准数据集。评估了四种模型的七个变体，专门针对这一新任务进行设计
- Result: 实验表明该数据集具有挑战性，现有模型的变体在层次实例跟踪任务上表现有限，验证了新任务的难度和必要性
- Conclusion: 层次实例跟踪是一个重要且有挑战性的新任务，提出的基准数据集为未来研究提供了基础，数据集已公开可用


### [8] [Topological Conditioning for Mammography Models via a Stable Wavelet-Persistence Vectorization](https://arxiv.org/abs/2512.10151)
*Charles Fanning,Mehmet Emin Aktas*

Main category: cs.CV

TL;DR: 该论文提出了一种基于小波持久同调的条件信号方法，通过拓扑数据分析改善乳腺X光片分类模型的外部性能，在两个独立数据集上显著提升了AUC。

- Motivation: 乳腺X光筛查虽然能降低死亡率，但存在较高的假阴性和假阳性问题，且模型在不同扫描仪、模态和患者群体间的性能会下降。需要提高模型的外部泛化能力。
- Method: 使用拓扑数据分析，基于小波的持久同调向量化方法，将跨强度阈值的图像结构信息转换为空间多尺度映射图，这些映射图通过输入级通道拼接集成到两阶段检测管道中。
- Result: 在INbreast数据集上，将小波持久同调通道与ConvNeXt Tiny结合后，患者级别的AUC从0.55提升到0.75（在有限训练预算下）。模型在美国CBIS DDSM数据集上训练验证，在葡萄牙INbreast和中国CMMD数据集上评估。
- Conclusion: 基于小波持久同调的拓扑特征提取方法能有效提升乳腺X光片分类模型的外部性能，增强模型在不同设备和人群间的泛化能力。


### [9] [Feature Coding for Scalable Machine Vision](https://arxiv.org/abs/2512.10209)
*Md Eimran Hossain Eimon,Juan Merlos,Ashan Perera,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: MPEG开发了面向机器视觉的中间特征压缩标准FCM，通过特征编码测试模型FCTM实现平均85.14%的比特率降低，在保持精度的同时解决边缘-云端协同推理的带宽瓶颈问题。

- Motivation: 深度神经网络在边缘设备部署面临计算资源限制，传统全模型本地运行或云端卸载方案在延迟、带宽和隐私方面存在权衡。边缘-云端协同推理虽然提供了平衡方案，但传输中间特征会带来新的带宽挑战，需要专门的压缩技术来解决。
- Method: MPEG启动了特征编码标准FCM，设计了专门的比特流语法和编解码器流水线来压缩中间特征。论文介绍了特征编码测试模型FCTM的设计，该模型针对机器视觉任务优化特征压缩。
- Result: FCTM在多个视觉任务中实现了显著的比特率降低，平均达到85.14%，同时保持了模型精度。这为带宽受限和隐私敏感的应用提供了高效的解决方案。
- Conclusion: FCM标准为智能特征的高效、可互操作部署提供了可扩展路径，特别适用于带宽受限和隐私敏感的消费级应用场景，解决了边缘-云端协同推理的关键技术瓶颈。


### [10] [Latent Chain-of-Thought World Modeling for End-to-End Driving](https://arxiv.org/abs/2512.10226)
*Shuhan Tan,Kashyap Chitta,Yuxiao Chen,Ran Tian,Yurong You,Yan Wang,Wenjie Luo,Yulong Cao,Philipp Krahenbuhl,Marco Pavone,Boris Ivanovic*

Main category: cs.CV

TL;DR: LCDrive提出了一种使用潜在语言而非自然语言进行推理的视觉-语言-动作模型，通过动作对齐的潜在空间统一推理和决策，在自动驾驶中实现了更快的推理速度和更好的轨迹质量。

- Motivation: 现有VLA模型使用自然语言进行链式思维推理，但文本可能不是最高效的推理表示形式。作者希望探索更有效的推理表示方法来提升自动驾驶性能和安全性。
- Method: 提出Latent-CoT-Drive模型，在动作对齐的潜在空间中表达推理：1) 动作提议token使用与输出动作相同的词汇表；2) 世界模型token基于学习的潜在世界模型表达动作的未来结果。通过真实未来场景的监督冷启动，然后通过闭环强化学习进行后训练。
- Result: 在大规模端到端驾驶基准测试中，LCDrive相比非推理和文本推理基线实现了更快的推理速度、更好的轨迹质量，并且从交互式强化学习中获得了更大的改进。
- Conclusion: 使用潜在语言而非自然语言进行推理可以更有效地统一推理和决策，在自动驾驶任务中实现更好的性能和效率。


### [11] [Emerging Standards for Machine-to-Machine Video Coding](https://arxiv.org/abs/2512.10230)
*Md Eimran Hossain Eimon,Velibor Adzic,Hari Kalva,Borko Furht*

Main category: cs.CV

TL;DR: FCM（特征编码）通过压缩神经网络特征而非原始像素，显著降低机器间通信的比特率，同时保持任务精度，并支持隐私保护和计算卸载。

- Motivation: 当前机器间视觉通信依赖面向人类优化的视频编解码器，导致带宽消耗大、扩展性差、隐私泄露问题。需要为机器间通信设计更高效的编码方案。
- Method: 提出FCM（特征编码）方法，压缩神经网络中间特征而非原始像素。使用H.26X系列编解码器作为内部编码器，比较H.264/AVC、H.265/HEVC和H.266/VVC在机器任务中的性能。
- Result: FCM能在保持接近边缘推理精度的同时显著降低比特率。H.265/HEVC与H.266/VVC在机器任务性能上几乎相同（平均BD-Rate增加仅1.39%），而H.264/AVC性能较差（BD-Rate增加32.28%）。在跟踪任务中，编解码器选择影响很小。
- Conclusion: 现有已部署编解码器的硬件可以支持机器间通信而不降低性能，为机器间视觉通信提供了高效、隐私保护的解决方案。


### [12] [Multi-dimensional Preference Alignment by Conditioning Reward Itself](https://arxiv.org/abs/2512.10237)
*Jiho Jang,Jinyoung Kim,Kyungjune Baek,Nojun Kwak*

Main category: cs.CV

TL;DR: MCDPO解决了DPO中多奖励维度聚合导致的奖励冲突问题，通过解耦的Bradley-Terry目标和条件训练，让模型能独立学习各奖励维度的优化方向。

- Motivation: 标准DPO方法使用Bradley-Terry模型将多个评估维度（如美学质量和语义对齐）聚合为单一标量奖励，这导致奖励冲突：模型被迫遗忘特定维度的理想特征，如果这些特征出现在全局非优样本中。
- Method: 提出Multi Reward Conditional DPO (MCDPO)，引入解耦的Bradley-Terry目标，在训练时显式注入偏好结果向量作为条件，使模型能在单一网络中独立学习各奖励维度的优化方向，并引入维度奖励dropout确保跨维度平衡优化。
- Result: 在Stable Diffusion 1.5和SDXL上的大量实验表明，MCDPO在基准测试中实现了优越性能。条件框架还支持在推理时使用Classifier Free Guidance动态控制多个奖励维度，无需额外训练或外部奖励模型。
- Conclusion: MCDPO有效解决了多奖励维度对齐中的奖励冲突问题，实现了更精细和动态的扩散模型对齐控制。


### [13] [Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective](https://arxiv.org/abs/2512.10244)
*Tian Liu,Anwesha Basu,James Caverlee,Shu Kong*

Main category: cs.CV

TL;DR: SWIFT方法通过分类器初始化和温度调优解决VLM在半监督少样本学习中的置信度问题，在五个基准上超越现有方法约5个准确率点

- Motivation: 半监督少样本学习（SSFSL）在现实应用（如自动标注）中很重要，但现有研究忽视了开源视觉语言模型（VLM）及其预训练数据的潜力。虽然相关领域（少样本学习）已利用这些资源，但SSFSL尚未有效整合。
- Method: 提出SWIFT（Stage-Wise Finetuning with Temperature Tuning）：1）使用分类器初始化增强置信度；2）温度调优改善softmax概率分布；3）分阶段微调VLM，结合有限标注数据、大量未标注数据以及从VLM预训练集中检索的相关但带噪声数据。
- Result: 在五个SSFSL基准测试中，SWIFT比最近的FSL和SSL方法高出约5个准确率点。甚至能与监督学习相媲美（后者使用真实标签微调VLM）。
- Conclusion: 通过简单的技术（分类器初始化和温度调优）解决了VLM在SSFSL中的置信度问题，使现有SSL方法能有效利用VLM资源，为现实世界的自动标注应用提供了实用解决方案。


### [14] [RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection](https://arxiv.org/abs/2512.10248)
*Zhuo Wang,Xiliang Liu,Ligang Sun*

Main category: cs.CV

TL;DR: 该论文提出了RobustSora基准，用于评估数字水印对AI生成视频检测器性能的影响，发现现有检测器部分依赖水印模式，性能变化达2-8个百分点。

- Motivation: 当前AI生成视频检测基准忽略了关键因素：许多先进生成模型在输出中嵌入了数字水印，检测器可能部分依赖这些水印模式进行判断。需要评估水印对检测器性能的真实影响。
- Method: 构建包含6,500个视频的数据集，分为四类：原始干净视频、原始但添加假水印视频、带水印AI生成视频、去水印AI生成视频。设计两个评估任务：任务I测试对去水印AI视频的检测性能，任务II评估对带假水印原始视频的误报率。测试了十种模型，包括专用AIGC检测器、Transformer架构和MLLM方法。
- Result: 实验显示水印操作导致检测器性能变化2-8个百分点。基于Transformer的模型表现出中等程度的依赖（6-8pp），而MLLM模型表现出多样化模式（2-8pp）。这表明现有检测器部分依赖水印模式进行判断。
- Conclusion: AI生成视频检测器存在对数字水印的部分依赖，需要开发水印感知的训练策略来提高鲁棒性。RobustSora基准为推进鲁棒的AIGC检测研究提供了重要工具。


### [15] [THE-Pose: Topological Prior with Hybrid Graph Fusion for Estimating Category-Level 6D Object Pose](https://arxiv.org/abs/2512.10251)
*Eunho Lee,Chaehyeon Song,Seunghoon Jeong,Ayoung Kim*

Main category: cs.CV

TL;DR: THE-Pose提出了一种新的类别级6D姿态估计框架，通过表面嵌入和混合图融合利用拓扑先验，显著提升了在复杂物体和遮挡情况下的鲁棒性。

- Motivation: 现有基于3D图卷积的方法只关注局部几何和深度信息，对复杂物体和视觉模糊性敏感。需要结合全局上下文和局部结构来应对类别内变化。
- Method: 提出THE-Pose框架：1) 从图像域提取一致且不变的拓扑特征；2) 设计混合图融合(HGF)模块，自适应地融合拓扑特征和点云特征，桥接2D图像上下文和3D几何结构。
- Result: 在REAL275数据集上，THE-Pose相比3D-GC基线(HS-Pose)提升35.8%，超越先前最佳方法7.2%，在未见或复杂物体及严重遮挡下保持稳定。
- Conclusion: 通过引入拓扑先验和混合图融合，THE-Pose有效解决了现有3D-GC方法的局限性，为类别级6D姿态估计提供了更鲁棒的解决方案。


### [16] [GDKVM: Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule](https://arxiv.org/abs/2512.10252)
*Rui Wang,Yimu Sun,Jingxing Guo,Huisi Wu,Jing Qin*

Main category: cs.CV

TL;DR: GDKVM是一种用于超声心动图视频分割的新架构，通过线性键值关联、门控增量规则和关键像素特征融合模块，在保持实时性能的同时提高了分割精度和鲁棒性。

- Motivation: 超声心动图序列中心腔的准确分割对心脏功能定量分析和临床诊断至关重要，但成像噪声、伪影以及心脏的形变和运动给分割算法带来挑战。现有方法在捕获长程时空依赖性和保持计算效率与细粒度特征表示之间存在权衡困难。
- Method: 提出GDKVM架构，包含三个核心组件：1) 线性键值关联(LKVA)建模帧间相关性；2) 门控增量规则(GDR)高效存储中间记忆状态；3) 关键像素特征融合(KPFF)模块在多个尺度上整合局部和全局特征。
- Result: 在CAMUS和EchoNet-Dynamic两个主流超声心动图视频数据集上验证，GDKVM在分割精度和鲁棒性方面优于现有最先进方法，同时确保实时性能。
- Conclusion: GDKVM通过创新的架构设计有效解决了超声心动图视频分割中长程时空依赖性与计算效率的权衡问题，为临床心脏功能分析提供了更准确、鲁棒且高效的工具。


### [17] [VLM-NCD:Novel Class Discovery with Vision-Based Large Language Models](https://arxiv.org/abs/2512.10262)
*Yuetong Su,Baoguo Wei,Xinyu Wang,Xu Li,Lixin Li*

Main category: cs.CV

TL;DR: LLM-NCD：一个融合视觉-文本语义和原型引导聚类的多模态框架，用于新类发现，在CIFAR-100数据集上相比现有方法将未知类准确率提升高达25.3%，并首次在NCD文献中展示了对长尾分布的独特鲁棒性。

- Motivation: 现有基于视觉特征的新类发现方法存在特征区分性不足和数据长尾分布的限制，需要突破这一瓶颈。
- Method: 提出LLM-NCD多模态框架，通过融合视觉-文本语义和原型引导聚类，联合优化已知类图像和文本特征来建模聚类中心和语义原型，采用双阶段发现机制通过语义亲和度阈值和自适应聚类动态分离已知或新类样本。
- Result: 在CIFAR-100数据集上，相比现有方法，该方法在未知类准确率上实现了高达25.3%的提升，并首次展示了在长尾分布下的独特鲁棒性。
- Conclusion: LLM-NCD通过多模态融合有效解决了新类发现中的特征区分性不足和长尾分布问题，为新类发现研究提供了新的方向。


### [18] [Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction](https://arxiv.org/abs/2512.10267)
*Chen Ziwen,Hao Tan,Peng Wang,Zexiang Xu,Li Fuxin*

Main category: cs.CV

TL;DR: Long-LRM++提出了一种半显式场景表示结合轻量级解码器的模型，在保持隐式表示高质量的同时实现了实时渲染（14 FPS），并扩展到64个输入视图。

- Motivation: 现有方法存在两个问题：1）可泛化高斯溅射方法直接预测数百万高斯参数容易出错，导致模糊；2）隐式表示方法（如LVSM、LaCT）渲染质量高但需要计算密集的解码过程，无法实时渲染。需要探索能否保留隐式表示优势同时实现实时性能。
- Method: 采用半显式场景表示结合轻量级解码器的设计，避免了深度顺序解码过程，同时保持了高质量渲染能力。模型可扩展到64个输入视图（950×540分辨率）。
- Result: 在DL3DV数据集上达到与LaCT相当的渲染质量，同时在A100 GPU上实现14 FPS的实时渲染。在ScanNetv2上提供优于直接高斯深度渲染的新视角深度预测。消融研究验证了各组件有效性。
- Conclusion: Long-LRM++成功解决了隐式表示方法的实时性限制，通过半显式表示和轻量解码器实现了高质量实时渲染，并展示了良好的泛化能力和扩展性。


### [19] [Sample-wise Adaptive Weighting for Transfer Consistency in Adversarial Distillation](https://arxiv.org/abs/2512.10275)
*Hongsin Lee,Hye Won Chung*

Main category: cs.CV

TL;DR: 对抗蒸馏中，更强的教师模型不一定产生更鲁棒的学生模型（鲁棒饱和现象），研究发现对抗可转移性是关键因素，提出SAAD方法通过样本级自适应加权提升鲁棒性转移效果。

- Motivation: 现有对抗蒸馏研究通常忽视使用最先进的鲁棒教师模型，且发现更强的教师模型并不总能产生更鲁棒的学生模型（鲁棒饱和现象）。传统解释将其归因于容量差距，但作者认为这种解释不完整，需要更深入理解鲁棒性转移机制。
- Method: 提出样本级自适应对抗蒸馏（SAAD），核心思想是利用对抗可转移性（学生生成的对抗样本对教师仍然有效的比例）作为关键指标。SAAD通过测量每个训练样本的对抗可转移性来重新加权训练样本，无需额外计算成本，从而优化鲁棒性转移过程。
- Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet数据集上的实验表明，SAAD方法相比先前方法在AutoAttack鲁棒性方面取得了一致的改进。
- Conclusion: 对抗可转移性是成功进行鲁棒性转移的关键因素，而非仅容量差距。提出的SAAD方法通过样本级自适应加权有效提升了对抗蒸馏的性能，为鲁棒性转移提供了新的视角和解决方案。


### [20] [MotionEdit: Benchmarking and Learning Motion-Centric Image Editing](https://arxiv.org/abs/2512.10284)
*Yixin Wan,Lei Ke,Wenhao Yu,Kai-Wei Chang,Dong Yu*

Main category: cs.CV

TL;DR: MotionEdit：首个专注于运动编辑的图像编辑数据集，包含高质量运动变换图像对，并提出了MotionNFT训练框架来提升运动编辑质量

- Motivation: 现有图像编辑数据集主要关注静态外观变化，缺乏高质量的运动编辑数据，而运动编辑在科学上具有挑战性，对视频合成和动画等应用有重要价值
- Method: 1) 构建MotionEdit数据集：从连续视频中提取和验证高质量运动变换图像对；2) 创建MotionEdit-Bench基准测试；3) 提出MotionNFT框架：通过运动对齐奖励指导模型学习准确运动变换
- Result: MotionEdit-Bench测试显示现有扩散模型在运动编辑任务上表现不佳，而MotionNFT框架在FLUX.1 Kontext和Qwen-Image-Edit模型上显著提升了编辑质量和运动保真度，且不损害一般编辑能力
- Conclusion: 运动编辑是一个具有挑战性的新任务，MotionEdit数据集和MotionNFT框架为解决这一问题提供了有效工具，推动了运动中心图像编辑技术的发展


### [21] [ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions](https://arxiv.org/abs/2512.10286)
*Xiaoxue Wu,Xinyuan Chen,Yaohui Wang,Yu Qiao*

Main category: cs.CV

TL;DR: ShotDirector是一个用于可控多镜头视频生成的框架，通过参数级相机控制和分层编辑模式感知提示，实现电影风格的镜头转场。

- Motivation: 当前多镜头视频生成主要关注镜头间的低级视觉一致性，忽略了镜头转场设计和电影语言对连贯叙事表达的作用，导致镜头变化缺乏有意的电影编辑模式。
- Method: 提出ShotDirector框架：1）采用相机控制模块注入6-DoF位姿和内在参数实现精确相机信息控制；2）使用镜头感知掩码机制引入专业编辑模式感知的分层提示，实现细粒度镜头内容控制。
- Result: 构建了ShotWeaver40K数据集捕获电影编辑模式先验，开发了可控多镜头视频生成评估指标。大量实验证明了框架的有效性。
- Conclusion: ShotDirector通过结合参数级条件和高级语义指导，实现了电影风格的可控镜头转场，解决了现有方法缺乏有意识电影编辑模式的问题。


### [22] [Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings](https://arxiv.org/abs/2512.10293)
*Karthikeya KV,Narendra Bandaru*

Main category: cs.CV

TL;DR: Disentangled360是一种创新的3D感知技术，通过方向解耦的体积渲染实现单图像360°视图合成，适用于医学影像和自然场景重建，无需场景特定微调。

- Motivation: 现有技术要么过度简化各向异性光行为，要么缺乏跨不同场景的泛化能力，无法同时满足医学成像和自然场景重建的需求，需要一种能区分各向同性和各向异性贡献的统一框架。
- Method: 在Gaussian Splatting骨干网络中区分各向同性和各向异性贡献，采用双分支条件框架：一个针对CT强度驱动的体数据散射优化，另一个通过归一化相机嵌入处理真实世界RGB场景。提出混合姿态无关锚定方法解决尺度模糊并保持结构真实感。
- Result: 在Mip-NeRF 360、RealEstate10K和DeepDRR数据集上表现出优越的SSIM和LPIPS性能，运行时评估证实适用于交互式应用，能够实现快速、逼真的视图合成。
- Conclusion: Disentangled360将术前放射学模拟和消费级360°渲染集成到单一推理流程中，支持混合现实医疗监督、机器人感知和沉浸式内容创作，无需场景特定微调或昂贵的光子模拟。


### [23] [Efficient-VLN: A Training-Efficient Vision-Language Navigation Model](https://arxiv.org/abs/2512.10310)
*Duo Zheng,Shijia Huang,Yanyang Li,Liwei Wang*

Main category: cs.CV

TL;DR: Efficient-VLN：一种训练高效的视觉语言导航模型，通过渐进式记忆和可学习递归记忆减少计算负担，通过动态混合策略平衡探索效率，在显著降低训练开销的同时达到SOTA性能。

- Motivation: 当前多模态大语言模型在视觉语言导航中面临两大训练开销问题：1）处理长历史观察时产生的二次计算负担；2）DAgger方法中探索与效率的权衡问题（更多探索带来更长轨迹）。
- Method: 提出Efficient-VLN模型：1）设计渐进式记忆机制，动态分配更多token给近期观察；2）可学习递归记忆机制，使用可学习token的键值缓存作为记忆状态；3）动态混合策略平衡探索效率。
- Result: 在R2R-CE上达到64.2%成功率，RxR-CE上达到67.0%成功率，均为SOTA性能。训练仅需282 H800 GPU小时，相比现有方法大幅降低训练开销。
- Conclusion: Efficient-VLN通过高效记忆机制和动态混合策略，在显著减少训练计算负担的同时实现了SOTA性能，为实际部署多模态大语言模型在视觉语言导航任务中提供了可行方案。


### [24] [DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation](https://arxiv.org/abs/2512.10314)
*Anh M. Vu,Khang P. Le,Trang T. K. Vo,Ha Thach,Huy Hung Nguyen,David Yang,Han H. Huynh,Quynh Nguyen,Tuan M. Pham,Tuan-Anh Le,Minh H. N. Le,Thanh-Huy Nguyen,Akash Awasthi,Chandra Mohan,Zhu Han,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 提出基于视觉-语言对齐的原型驱动框架，通过双模态原型库（文本+图像）和多尺度金字塔模块，改进病理图像弱监督语义分割，在BCSS-WSSS基准上超越现有方法。

- Motivation: 解决病理图像弱监督语义分割（WSSS）中的三大挑战：类间同质性、类内异质性以及CAM监督的区域收缩效应，旨在降低标注成本的同时提升分割性能。
- Method: 1. 采用CoOp风格的可学习提示调优生成文本原型；2. 结合可学习图像原型构建双模态原型库，捕捉语义和外观特征；3. 引入多尺度金字塔模块缓解ViT表示的过度平滑问题，提升空间定位精度。
- Result: 在BCSS-WSSS基准测试中超越了现有最先进方法，分析显示文本描述多样性、上下文长度以及文本与图像原型的互补行为对性能有显著提升。
- Conclusion: 联合利用文本语义和视觉原型学习是数字病理学中弱监督语义分割的有效方法，双模态原型库和多尺度增强模块能显著改善区域发现和定位质量。


### [25] [ConStruct: Structural Distillation of Foundation Models for Prototype-Based Weakly Supervised Histopathology Segmentation](https://arxiv.org/abs/2512.10316)
*Khang Le,Ha Thach,Anh M. Vu,Trang T. K. Vo,Han H. Huynh,David Yang,Minh H. N. Le,Thanh-Huy Nguyen,Akash Awasthi,Chandra Mohan,Zhu Han,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 提出一种用于组织病理学图像弱监督语义分割的原型学习框架，整合CONCH的形态感知表示、SegFormer的多尺度结构线索和文本引导语义对齐，无需像素级标注即可生成高质量伪掩码。

- Motivation: 组织病理学中的弱监督语义分割主要依赖分类骨干网络，但这些模型通常只定位最具判别性的区域，难以捕捉组织结构的完整空间范围。现有的视觉语言模型（如CONCH）提供丰富的语义对齐和形态感知表示，而现代分割骨干网络（如SegFormer）保留细粒度空间线索，但如何结合这些互补优势在弱监督下仍具挑战。
- Method: 提出原型学习框架：1) 文本引导原型初始化，结合病理描述生成更完整、语义准确的伪掩码；2) 结构蒸馏机制，从SegFormer转移空间知识以保留细粒度形态模式和局部组织边界；3) 整合CONCH的形态感知表示、SegFormer的多尺度结构线索和文本引导语义对齐。
- Result: 在BCSS-WSSS数据集上的实验表明，该原型学习框架优于现有的WSSS方法，通过冻结基础模型骨干和轻量可训练适配器保持计算效率，提高了定位完整性和跨组织类型的语义一致性。
- Conclusion: 该框架成功整合了视觉语言模型和分割骨干网络的互补优势，在弱监督下实现了高质量的组织病理学图像语义分割，无需像素级标注即可生成空间连贯且语义准确的伪掩码。


### [26] [Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset](https://arxiv.org/abs/2512.10321)
*Hyunsoo Lee,Daeum Jeon,Hyeokjae Oh*

Main category: cs.CV

TL;DR: 提出Point2Pose框架，通过生成式方法解决3D人体姿态估计问题，结合点云和姿态历史信息，并在新数据集MVPose3D上验证了优越性能。

- Motivation: 3D人体姿态估计面临人体复杂几何结构、关节自遮挡以及需要大规模真实世界运动数据集的挑战。现有方法在这些方面存在局限，需要更有效的解决方案。
- Method: 提出Point2Pose框架：1) 使用时序点云编码器和姿态特征编码器提取关节级特征；2) 采用基于注意力的生成式回归器；3) 构建包含IMU数据、多视角点云和RGB图像的大规模室内数据集MVPose3D。
- Result: 实验结果表明，该方法在多个数据集上优于基线模型，展示了其在3D人体姿态估计任务上的优越性能。
- Conclusion: Point2Pose框架通过生成式方法和多模态数据融合，有效解决了3D人体姿态估计的关键挑战，为相关领域提供了新的解决方案。


### [27] [EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs](https://arxiv.org/abs/2512.10324)
*Chao Gong,Depeng Wang,Zhipeng Wei,Ya Guo,Huijia Zhu,Jingjing Chen*

Main category: cs.CV

TL;DR: EchoingPixels是一个音频-视觉大语言模型(LLM)的令牌缩减框架，通过跨模态语义筛(CS2)模块实现音频和视觉令牌的联合缩减，仅需原始令牌的5-20%就能达到基线性能，同时实现2-3倍的速度提升和内存减少。

- Motivation: 音频-视觉大语言模型面临大量音频和视频令牌带来的计算开销问题。现有的单模态令牌缩减方法无法利用音频-视觉跨模态协同作用，且静态的模态预算分配无法适应音频和视频信息密度的差异和动态变化。
- Method: 提出EchoingPixels框架，核心是跨模态语义筛(CS2)模块，它共同关注联合多模态流，从整个音频-视觉令牌池中缩减令牌，而不是为每个模态使用固定预算。同时设计了同步增强RoPE(Sync-RoPE)来保持稀疏选择令牌的关键时间关系。
- Result: 实验表明，EchoingPixels仅使用原始令牌的5-20%就能达到强基线模型的性能，同时实现2-3倍的速度提升和内存减少。
- Conclusion: EchoingPixels成功解决了音频-视觉大语言模型的令牌缩减瓶颈，通过跨模态协同和动态预算分配实现了高效的多模态处理，为音频-视觉LLM的实际部署提供了可行方案。


### [28] [StainNet: A Special Staining Self-Supervised Vision Transformer for Computational Pathology](https://arxiv.org/abs/2512.10326)
*Jiawen Li,Jiali Hu,Xitong Ling,Yongqiang Lv,Yuxuan Chen,Yizhi Wang,Tian Guan,Yifei Liu,Yonghong He*

Main category: cs.CV

TL;DR: StainNet是一个专门针对特殊染色病理图像的基础模型，基于ViT架构，通过自蒸馏自监督学习在140万张特殊染色图像上训练，解决了现有病理基础模型主要针对H&E染色的局限性。

- Motivation: 现有病理基础模型主要基于H&E染色图像预训练，但临床实践中经常使用特殊染色（如免疫组化），这限制了现有模型在特殊染色应用中的效果。
- Method: 提出StainNet模型，基于ViT架构，采用自蒸馏自监督学习方法，在HISTAI数据库的20,231张公开特殊染色WSI中裁剪的140万张图像块上进行训练。
- Result: 在内部肝恶性肿瘤分类任务和两个公开ROI级数据集上展示了强大能力；进行了少样本学习和检索评估，与近期更大的PFMs比较进一步凸显其优势。
- Conclusion: StainNet填补了特殊染色病理图像基础模型的空白，为临床应用中特殊染色图像分析提供了专门化的解决方案。


### [29] [Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering](https://arxiv.org/abs/2512.10327)
*Cai Xu,Jinlong Liu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: ISMVC提出了一种基于信息量的选择性插补方法，通过评估缺失位置的信息量来决定是否插补，结合变分自编码器和混合高斯先验学习聚类友好的表示，在严重不平衡缺失场景下优于现有方法。

- Motivation: 不完全多视图数据中，不同视图存在缺失和不平衡观测，给聚类带来挑战。现有插补方法可能引入噪声和偏差，而免插补方法在严重不完全时缺乏跨视图互补性。需要一种既能利用插补优势又能避免其风险的方法。
- Method: ISMVC方法：1) 基于视图内相似性和跨视图一致性评估每个缺失位置的信息量，仅在信息充足时选择性插补；2) 结合变分自编码器和混合高斯先验学习聚类友好的潜在表示；3) 在分布层面进行插补，稳定后验分布聚合并显式建模插补不确定性。
- Result: 在多个基准数据集上，在更现实和具有挑战性的不平衡缺失场景下，ISMVC方法优于现有的基于插补和免插补方法。该方法轻量级、数据驱动且模型无关，可作为插件模块集成到现有IMC模型中。
- Conclusion: ISMVC通过选择性插补策略有效平衡了插补的优势和风险，在严重不完全多视图数据聚类中表现出色，为不完全多视图聚类提供了新的解决方案。


### [30] [A Conditional Generative Framework for Synthetic Data Augmentation in Segmenting Thin and Elongated Structures in Biological Images](https://arxiv.org/abs/2512.10334)
*Yi Liu,Yichi Zhang*

Main category: cs.CV

TL;DR: 提出基于Pix2Pix的条件生成框架，从二值掩码生成逼真的显微镜图像中的丝状结构，解决丝状结构分割中高质量标注数据稀缺的问题。

- Motivation: 丝状结构（如微管和肌动蛋白丝）在生物系统中很重要，但分割这些结构需要像素级标注数据。由于丝状结构密集分布和几何特性，手动标注极其耗时费力，高质量标注数据集获取困难。
- Method: 基于Pix2Pix架构的条件生成框架，从二值掩码生成逼真的显微镜图像中的丝状结构。提出丝状结构感知的结构损失函数，提高生成合成图像时的结构相似性。
- Result: 实验证明该方法有效，性能优于未使用合成数据训练的现有模型。
- Conclusion: 提出的条件生成框架能有效生成逼真的丝状结构图像，解决了丝状结构分割中的数据稀缺问题，为定量分析提供了更好的数据支持。


### [31] [Zero-shot Adaptation of Stable Diffusion via Plug-in Hierarchical Degradation Representation for Real-World Super-Resolution](https://arxiv.org/abs/2512.10340)
*Yi-Cheng Liao,Shyang-En Weng,Yu-Syuan Xu,Chi-Wei Hsiao,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: HD-CLIP是一种用于真实世界图像超分辨率的即插即用模块，通过层次化降解嵌入捕捉数值降解程度，结合分类器自由投影引导提升生成质量。

- Motivation: 真实世界图像超分辨率面临未知复杂降解问题，现有方法假设已知降解程度且依赖CLIP文本编码器无法捕捉数值降解程度，限制了泛化能力。
- Method: 提出HD-CLIP，将低质量图像分解为语义嵌入和有序降解嵌入，通过分类器自由投影引导集成到扩散模型中，利用语义线索引导生成，降解线索抑制伪影。
- Result: HD-CLIP作为即插即用模块可无缝集成到各种超分辨率框架中，无需训练即可显著提升细节保真度和感知真实性，在多个真实世界数据集上表现优异。
- Conclusion: HD-CLIP通过层次化降解表示和投影引导机制，有效解决了真实世界图像超分辨率中的降解建模问题，提升了生成质量和泛化能力。


### [32] [CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates](https://arxiv.org/abs/2512.10342)
*Shresth Grover,Priyank Pathak,Akash Kumar,Vibhav Vineet,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 提出CoSPlan基准评估视觉语言模型在错误检测和步骤完成方面的顺序规划能力，并开发SGI方法提升性能

- Motivation: 大规模视觉语言模型在复杂推理方面表现出色，但在视觉顺序规划（执行多步骤动作达成目标）方面研究不足。实际规划常涉及非最优步骤，需要模型检测和纠正错误。
- Method: 提出CoSPlan基准，评估4个领域的视觉顺序规划任务；开发训练免费方法SGI（场景图增量更新），在初始状态和目标状态之间引入中间推理步骤。
- Result: 当前最先进的视觉语言模型（如Intern-VLM和Qwen2）在CoSPlan上表现不佳，无法有效利用上下文线索。SGI方法平均提升5.2%性能，并能泛化到传统规划任务。
- Conclusion: CoSPlan揭示了视觉语言模型在顺序规划中的局限性，SGI方法通过增量推理显著提升性能，为可靠纠正性顺序规划提供了有效解决方案。


### [33] [Topology-Agnostic Animal Motion Generation from Text Prompt](https://arxiv.org/abs/2512.10352)
*Keyi Chen,Mingze Sun,Zhenyu Liu,Zhangquan Chen,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出了OmniZoo大规模动物运动数据集和通用自回归运动生成框架，能够为任意骨骼拓扑结构生成文本驱动的运动

- Motivation: 现有运动生成方法大多依赖固定的骨骼模板，无法泛化到不同或扰动的骨骼拓扑结构，同时缺乏大规模异构动物运动数据和统一的生成框架
- Method: 1) 构建OmniZoo数据集，涵盖140个物种和32,979个序列；2) 提出拓扑感知骨骼嵌入模块，将任意骨骼的几何和结构属性编码到共享标记空间；3) 建立通用自回归运动生成框架，结合文本语义生成运动
- Result: 方法能够为任意骨骼拓扑结构生成时间一致、物理合理且语义对齐的运动，并支持跨物种运动风格迁移
- Conclusion: OmniZoo数据集和通用运动生成框架解决了现有方法在骨骼拓扑泛化和多模态条件生成方面的局限性，为计算机动画、机器人和虚拟环境提供了更灵活的运动生成能力


### [34] [Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation](https://arxiv.org/abs/2512.10353)
*Yiheng Lyu,Lian Xu,Mohammed Bennamoun,Farid Boussaid,Coen Arrow,Girish Dwivedi*

Main category: cs.CV

TL;DR: TranSamba是一个混合Transformer-Mamba架构，用于弱监督体积医学分割，通过跨平面Mamba块实现高效3D上下文建模，在三个数据集上达到SOTA性能。

- Motivation: 现有弱监督医学分割方法通常依赖2D编码器，忽略了数据的体积特性，需要一种能有效捕获3D上下文的方法。
- Method: 提出TranSamba混合架构，在标准Vision Transformer基础上增加Cross-Plane Mamba块，利用状态空间模型的线性复杂度实现相邻切片间的高效信息交换，增强Transformer块内的自注意力机制。
- Result: 在三个数据集上的实验表明，TranSamba建立了新的最先进性能，在不同模态和病理上一致优于现有方法，同时保持线性时间复杂度和恒定内存使用。
- Conclusion: TranSamba通过结合Transformer和Mamba的优势，实现了高效的3D上下文建模，为弱监督体积医学分割提供了有效的解决方案。


### [35] [mmCounter: Static People Counting in Dense Indoor Scenarios Using mmWave Radar](https://arxiv.org/abs/2512.10357)
*Tarik Reza Toha,Shao-Jung,Lu,Shahriar Nirjon*

Main category: cs.CV

TL;DR: mmCounter：利用毫米波雷达通过提取呼吸和微动作的超低频信号，实现密集静态人群的准确计数

- Motivation: 现有毫米波雷达在检测密集静态人群时存在困难，因为空间分辨率有限且依赖运动检测。需要一种能够准确计数静态人群的方法。
- Method: 采用多阶段信号处理流程，提取超低频（<1Hz）信号（主要来自呼吸和微尺度身体运动），结合空间信息将信号源映射到个体，实现准确计数。
- Result: 在熟悉环境中平均F1分数87%，平均绝对误差0.6；在未测试环境中平均F1分数60%，平均绝对误差1.1。可在3平方米空间内计数最多7人。
- Conclusion: mmCounter能够有效解决毫米波雷达在密集静态人群计数中的挑战，通过提取超低频生理信号实现准确计数。


### [36] [Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task](https://arxiv.org/abs/2512.10359)
*Sunqi Fan,Jiashuo Cui,Meng-Hao Guo,Shuojin Yang*

Main category: cs.CV

TL;DR: 该论文提出了一个增强多模态大语言模型视频问答能力的视频工具包和时空推理框架，通过在复杂推理任务中战略性地调度时空工具来提升性能。

- Motivation: 现有MLLMs在复杂视频问答任务中难以同时建模视频帧内的空间关系和理解时间演化的因果动态，需要增强其时空推理能力。
- Method: 提出了全面的可扩展视频工具包，并设计了时空推理框架（STAR），通过战略性地调度时间和空间工具来逐步定位视频中的关键区域。
- Result: STAR框架使用轻量级工具增强了GPT-4o，在VideoMME上获得8.2%的提升，在LongVideoBench上获得4.6%的提升。
- Conclusion: 提出的视频工具包和STAR框架为构建自主智能的视频分析助手迈出了重要一步。


### [37] [Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models](https://arxiv.org/abs/2512.10362)
*Woojun Jung,Jaehoon Go,Mingyu Jeon,Sunjae Yoon,Junyeong Kim*

Main category: cs.CV

TL;DR: 提出Visual Funnel方法解决MLLMs的"上下文盲区"问题，通过上下文锚定和熵缩放组合构建分层视觉输入，显著提升细粒度视觉感知能力

- Motivation: 多模态大语言模型在细粒度视觉细节感知方面存在不足，现有裁剪方法导致"上下文盲区"问题，即高保真细节与全局上下文之间的结构性脱节
- Method: 提出无需训练的Visual Funnel方法：1) 上下文锚定：单次前向传播识别感兴趣区域；2) 熵缩放组合：基于注意力熵动态确定裁剪尺寸，构建从焦点细节到周围环境的分层上下文组合
- Result: Visual Funnel显著优于朴素单裁剪和非结构化多裁剪基线，验证了分层结构组合对解决上下文盲区的关键作用，而简单增加非结构化裁剪效果有限甚至有害
- Conclusion: 问题的核心不是信息"数量"不足，而是输入"结构多样性"缺乏，通过构建分层视觉组合能有效解决MLLMs的上下文盲区问题，提升细粒度视觉感知能力


### [38] [Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos](https://arxiv.org/abs/2512.10363)
*Mingyu Jeon,Jisoo Yang,Sungjin Han,Jinkwon Hwang,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: P2S是一个无需训练的长视频时刻检索框架，通过自适应跨度生成器和查询分解技术，解决了传统方法在搜索阶段候选爆炸和精炼阶段计算成本高的问题。

- Motivation: 现有长视频时刻检索方法存在严重限制：监督学习方法扩展性差、泛化能力弱且资源消耗大；现有零样本方法面临搜索阶段候选爆炸和精炼阶段需要高成本视觉语言模型验证的双重挑战。
- Method: 提出P2S框架，包含两个关键创新：1) 自适应跨度生成器防止搜索阶段候选爆炸；2) 查询分解技术在不依赖高成本VLM验证的情况下精炼候选片段。
- Result: P2S是首个能够在小时级长视频中进行时间定位的零样本框架，在MAD数据集上R5@0.1指标超过监督SOTA方法3.7%。
- Conclusion: P2S通过创新的训练免费框架，有效解决了长视频时刻检索中的计算效率和语义准确性挑战，在零样本设置下超越了监督学习方法。


### [39] [Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views](https://arxiv.org/abs/2512.10369)
*Zhankuo Xu,Chaoran Feng,Yingtao Li,Jianbin Zhao,Jiashu Yang,Wangbo Yu,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: CoherentGS：一种从稀疏模糊图像进行高保真3D重建的新框架，通过双先验策略结合去模糊网络和扩散模型，解决稀疏视图与运动模糊的恶性循环问题。

- Motivation: 3D高斯泼溅（3DGS）在新视角合成中表现出色，但严重依赖密集高质量输入图像。现实应用中数据通常稀疏且存在运动模糊，这两个问题形成恶性循环：稀疏视图忽略了解析运动模糊所需的多视图约束，而运动模糊又抹去了对齐有限视图所需的高频细节，导致重建失败。
- Method: 提出CoherentGS框架，采用双先验策略：结合专门去模糊网络恢复清晰细节并提供光度指导，以及扩散模型提供几何先验填充未观测区域。关键技术包括一致性引导的相机探索模块自适应指导生成过程，以及深度正则化损失确保几何合理性。
- Result: 在合成和真实场景上使用少至3、6、9个输入视图进行定量和定性评估，CoherentGS显著优于现有方法，为该挑战性任务设定了新的最先进水平。
- Conclusion: CoherentGS通过双先验策略成功解决了稀疏模糊图像3D重建中的恶性循环问题，实现了高质量重建，代码和视频演示已公开。


### [40] [RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds](https://arxiv.org/abs/2512.10376)
*Jingyun Fu,Zhiyu Xiang,Na Zhao*

Main category: cs.CV

TL;DR: 提出首个雷达-激光雷达场景流估计数据集和方法，通过动态感知双向跨模态融合模块有效结合雷达动态信息和激光雷达几何信息，显著提升场景流估计性能。

- Motivation: 现有多模态融合方法主要关注图像与激光雷达，而4D毫米波雷达与激光雷达的融合尚未探索。雷达成本更低、在各种天气条件下更鲁棒，并能检测点速度，是激光雷达的有价值补充。但雷达数据存在噪声、低分辨率和稀疏性问题，且目前缺乏专门的雷达-激光雷达场景流数据集。
- Method: 1) 基于公开真实世界汽车数据集构建雷达-激光雷达场景流数据集；2) 提出有效的雷达去噪预处理策略和场景流标签生成方法；3) 提出RaLiFlow框架，包含动态感知双向跨模态融合(DBCF)模块和精心设计的损失函数集。DBCF模块将雷达动态线索集成到局部交叉注意力机制中，实现跨模态上下文信息传播。
- Result: 在重新构建的场景流数据集上的大量实验表明，该方法显著优于现有的基于激光雷达和基于雷达的单模态方法。
- Conclusion: 首次探索了4D雷达与激光雷达的联合场景流学习，通过有效的跨模态融合框架解决了雷达数据挑战，为自动驾驶感知提供了更鲁棒的多模态解决方案。


### [41] [Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching](https://arxiv.org/abs/2512.10379)
*Alberto Rota,Elena De Momi*

Main category: cs.CV

TL;DR: 提出了一种用于内窥镜图像特征匹配的深度学习管道，通过自监督对比学习优化DINOv2骨干网络，在SCARED数据集上实现了优于现有方法的匹配精度。

- Motivation: 在微创手术中，内窥镜图像间的精确像素级对应关系对于3D重建、相机跟踪和场景理解至关重要。然而手术领域存在独特挑战：弱透视线索、非朗伯组织反射和复杂可变形解剖结构会降低传统计算机视觉技术的性能。深度学习模型虽然在自然场景中表现良好，但其特征并不天然适合手术图像的细粒度匹配，需要针对该领域进行专门适配。
- Method: 提出了一种用于内窥镜图像对特征匹配的深度学习管道和自监督优化框架。方法利用新颖视角合成管道生成真实内点对应关系，随后在对比学习范式中用于挖掘三元组。通过这种自监督方法，在DINOv2骨干网络基础上增加了一个额外的Transformer层，专门优化以产生能够通过余弦相似度阈值进行直接匹配的嵌入表示。
- Result: 实验评估表明，该管道在SCARED数据集上超越了最先进的方法，相比相关工作具有更高的匹配精度和更低的极线误差。
- Conclusion: 所提出的框架为实现手术内窥镜中更准确的高级计算机视觉应用做出了有价值的贡献。


### [42] [Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies](https://arxiv.org/abs/2512.10384)
*Cong Pang,Hongtao Yu,Zixuan Chen,Lewei Lu,Xin Lou*

Main category: cs.CV

TL;DR: 提出了FROW基准测试，用于评估大型视觉语言模型在细粒度识别任务上的性能，并通过数据构建和训练过程优化策略提升模型表现。

- Motivation: 现有基准测试主要关注推理任务，忽略了细粒度识别能力，而细粒度识别在实际应用中至关重要，因此需要专门的评估基准。
- Method: 1) 创建FROW基准测试，包含马赛克数据（组合多个简短回答）和开放世界数据（基于真实世界问答生成）；2) 从数据构建和训练过程两个角度提出优化策略。
- Result: 马赛克数据提升类别识别准确率1%；开放世界数据提升FROW基准准确率10%-20%，内容准确率6%-12%；在预训练阶段加入细粒度数据可提升类别识别准确率最高达10%。
- Conclusion: FROW基准填补了细粒度识别评估的空白，提出的优化策略能有效提升大型视觉语言模型的细粒度识别能力，对实际应用具有重要意义。


### [43] [Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method](https://arxiv.org/abs/2512.10386)
*Ge Zhang,Chunyang Wang,Bo Xiao,Xuelian Liu,Bin Liu*

Main category: cs.CV

TL;DR: 提出自适应双权重引力点云去噪方法，在保持高精度和强边缘保留的同时实现实时处理

- Motivation: 现有点云去噪方法难以同时实现高精度、强边缘保留和实时性能，要么牺牲计算效率追求精度，要么牺牲边界细节追求速度
- Method: 1) 使用八叉树进行空间分区实现并行加速；2) 在叶节点内使用自适应体素占用统计和kNN密度估计快速去除孤立和低密度噪声点；3) 构建结合密度权重和自适应距离权重的引力评分函数精细区分噪声点和目标点
- Result: 在Stanford 3D Scanning Repository、CADC数据集和实验室FMCW LiDAR点云上的实验表明，相比现有方法，在各种噪声条件下F1、PSNR和Chamfer Distance指标均有提升，同时减少了单帧处理时间
- Conclusion: 该方法在多噪声场景下实现了高精度、鲁棒性和实时性能的平衡，验证了其有效性


### [44] [MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos](https://arxiv.org/abs/2512.10408)
*Qiyue Sun,Tailin Chen,Yinghui Zhang,Yuchen Zhang,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: MultiHateLoc：首个弱监督多模态仇恨言论时序定位框架，通过模态感知编码器、动态跨模态融合和模态感知MIL目标，在仅视频级标签下实现帧级定位，在HateMM和MultiHateClip数据集上达到SOTA性能。

- Motivation: TikTok和YouTube等平台视频内容快速增长加剧了多模态仇恨言论传播，现有研究主要关注视频级分类，而实际应用中关键的时序定位任务（识别仇恨片段何时发生）在弱监督下（仅有视频级标签）面临挑战，静态融合或基于分类的架构难以捕捉跨模态和时序动态。
- Method: 提出MultiHateLoc框架：1）模态感知时序编码器建模异质序列模式，包括针对文本的预处理模块增强特征；2）动态跨模态融合自适应强调每个时刻最具信息量的模态，以及跨模态对比对齐策略增强多模态特征一致性；3）模态感知MIL目标在视频级监督下识别判别性片段。
- Result: 在HateMM和MultiHateClip数据集上的实验表明，该方法在定位任务中达到最先进的性能。尽管仅依赖粗粒度标签，MultiHateLoc能产生细粒度、可解释的帧级预测。
- Conclusion: MultiHateLoc是首个针对弱监督多模态仇恨言论时序定位的框架，通过创新的模态感知编码、动态融合和MIL目标，有效解决了现有方法在捕捉跨模态和时序动态方面的不足，为多模态仇恨言论检测提供了更精细的定位能力。


### [45] [Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction](https://arxiv.org/abs/2512.10416)
*Wenfei Guan,Jilin Mei,Tong Shen,Xumin Wu,Shuo Wang,Cheng Min,Yu Hu*

Main category: cs.CV

TL;DR: 提出WildRoad数据集和MaGRoad方法，解决越野道路提取中的数据集缺乏和拓扑错误问题

- Motivation: 现有深度学习方法在越野道路提取中存在两大问题：缺乏大规模矢量化数据集，以及现有节点中心方法在遮挡和模糊路口处容易产生拓扑错误
- Method: 1) 发布WildRoad越野道路网络数据集，使用专用交互标注工具构建；2) 提出MaGRoad（Mask-aware Geodesic Road network extractor）路径中心框架，沿候选路径聚合多尺度视觉证据来推断连接性
- Result: MaGRoad在WildRoad基准测试中达到最先进性能，同时在城市数据集上泛化良好；简化流程使推理速度提升约2.5倍
- Conclusion: 数据集和路径中心范式为野外道路映射提供了更强的基础


### [46] [TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning](https://arxiv.org/abs/2512.10419)
*Phu Pham,Damon Conover,Aniket Bera*

Main category: cs.CV

TL;DR: TransLocNet：一种通过跨模态注意力融合LiDAR几何与航拍语义的空中-地面定位框架，在CARLA和KITTI数据集上实现亚米级、亚度级精度，定位误差降低达63%。

- Motivation: 地面LiDAR与航拍图像之间存在巨大的视角和模态差异，导致空中-地面定位困难。现有方法难以有效融合这两种不同模态的数据进行精确定位。
- Method: 提出TransLocNet跨模态注意力框架：1) 将LiDAR扫描投影为鸟瞰图表示；2) 通过双向注意力机制与航拍特征对齐；3) 使用似然图解码器输出位置和方向的空间概率分布；4) 加入对比学习模块强制共享嵌入空间以改善跨模态对齐。
- Result: 在CARLA和KITTI数据集上的实验表明，TransLocNet优于现有最先进方法，定位误差降低高达63%，实现亚米级、亚度级精度，在合成和真实场景中均表现出鲁棒性和泛化能力。
- Conclusion: TransLocNet通过有效融合LiDAR几何与航拍语义上下文，解决了空中-地面定位中的视角和模态差异问题，为合成和真实世界场景提供了鲁棒且可泛化的定位解决方案。


### [47] [Neural Collapse in Test-Time Adaptation](https://arxiv.org/abs/2512.10421)
*Xiao Chen,Zhongjing Du,Jiazhen Huang,Xu Jiang,Li Lu,Jingyan Jiang,Zhi Wang*

Main category: cs.CV

TL;DR: 该论文提出NCTTA方法，通过样本级神经坍缩分析发现域偏移下性能下降源于特征-分类器错位，采用混合目标进行特征-分类器对齐以提升测试时自适应性能。

- Motivation: 现有测试时自适应(TTA)方法缺乏对域偏移下性能下降根本原因的理论分析，需要从几何角度理解深度神经网络在域偏移下的行为。
- Method: 提出NCTTA方法：1) 将神经坍缩扩展到样本级，发现样本对齐坍缩现象(NC3+)；2) 基于NC3+分析发现性能下降源于特征-分类器错位；3) 设计混合目标特征-分类器对齐方法，结合几何邻近性和预测置信度来缓解不可靠伪标签的影响。
- Result: NCTTA在ImageNet-C上比Tent方法提升14.52%，在多个数据集上验证了其有效性，显著增强了模型对域偏移的鲁棒性。
- Conclusion: 样本级神经坍缩分析为理解TTA性能下降提供了新视角，NCTTA通过特征-分类器对齐有效缓解了域偏移下的错位问题，为测试时自适应提供了理论指导和实用方法。


### [48] [An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time](https://arxiv.org/abs/2512.10437)
*Stylianos Kandylakis,Christos Orfanopoulos,Georgios Siolas,Panayiotis Tsanakas*

Main category: cs.CV

TL;DR: 提出基于移动设备的实时物理治疗动作识别、分类与评估框架，通过姿态估计、角度特征提取和动态规划序列匹配实现实时动作分析与偏差检测。

- Motivation: 远程物理治疗和移动健康应用需要实时、准确的动作识别与评估系统，但现有方法在移动设备上的实时性和准确性存在挑战，需要开发轻量级、客户端运行的解决方案。
- Method: 将运动分解为静态姿态序列，使用姿态估计神经网络提取关键点，转换为三角函数角度特征，用轻量级监督模型分类，最后通过改进的Levenshtein距离动态规划算法进行序列匹配和偏差定位。
- Result: 实验评估验证了方法的有效性，系统能够在客户端实现实时性能，适用于远程物理治疗监督和移动健康应用。
- Conclusion: 该框架为移动设备上的实时物理治疗动作分析提供了高效解决方案，具有实际应用价值，特别是在远程医疗和移动健康领域。


### [49] [Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment](https://arxiv.org/abs/2512.10450)
*Han Li,Shaohui Li,Wenrui Dai,Chenglin Li,Xinlong Pan,Haipeng Wang,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: 提出一种新的统一变换框架，通过双域渐进时间对齐和质量条件专家混合模块，实现无误差传播且质量一致的视频压缩流

- Motivation: 现有学习视频压缩框架存在时间对齐不准确和误差传播的困境：分离变换框架虽然R-D性能好但会导致明显误差传播，而统一变换框架消除了误差传播但在共享潜在域中的运动估计/补偿性能较差
- Method: 提出统一变换框架，包含双域渐进时间对齐（粗粒度像素域对齐+细粒度潜在域对齐）和质量条件专家混合模块。像素域对齐处理简单运动模式，潜在域对齐使用流引导可变形变换器实现长期运动细化处理复杂运动模式。QCMoE模块基于目标质量和内容动态分配专家调整每像素量化步长
- Result: 实验结果表明，该方法在保持竞争性R-D性能的同时，成功消除了误差传播问题
- Conclusion: 提出的框架解决了学习视频压缩中时间对齐与误差传播的困境，实现了无误差传播且质量一致的视频压缩流


### [50] [Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network](https://arxiv.org/abs/2512.10498)
*Khurram Ashfaq,Muhammad Tariq Mahmood*

Main category: cs.CV

TL;DR: 提出混合框架，结合传统DDL核计算多尺度聚焦体积与轻量级GRU深度提取模块，通过迭代优化和凸上采样实现高精度深度估计

- Motivation: 现有深度学习SFF方法通常采用两阶段流程：使用重型特征编码器提取聚焦体积，然后通过简单聚合估计深度，这会导致伪影和噪声放大问题
- Method: 1) 使用手工设计的DDL核传统计算多尺度聚焦体积，捕捉长距离和方向性聚焦变化；2) 轻量级多尺度GRU深度提取模块迭代优化低分辨率初始深度估计；3) 学习凸上采样模块重建高分辨率深度图
- Result: 在合成和真实数据集上的广泛实验表明，该方法在准确性和泛化能力方面优于最先进的深度学习和传统方法
- Conclusion: 提出的混合框架通过结合传统聚焦体积计算和轻量级深度提取网络，有效解决了现有SFF方法的伪影和噪声问题，实现了更精确的深度估计


### [51] [3D Blood Pulsation Maps](https://arxiv.org/abs/2512.10517)
*Maurice Rohr,Tobias Reinhardt,Tizian Dege,Justus Thies,Christoph Hoog Antink*

Main category: cs.CV

TL;DR: 首个用于估计3D血流搏动图的数据集，包含多视角视频、脉搏参考测量和3D面部扫描，支持远程脉搏估计方法的研究与验证。

- Motivation: 目前缺乏用于研究面部动态血流搏动的3D数据集，限制了远程光电容积脉搏波成像方法的改进和验证，需要多视角数据来减轻光照影响。
- Method: 收集15名受试者在23个视角下30Hz的RGB视频，结合脉搏参考测量，使用单目运动结构技术生成3D面部扫描，并处理成与FLAME头部模型纹理空间兼容的3D搏动图。
- Result: 创建了包含原始视频、脉搏参考、3D扫描和处理后搏动图的完整数据集，搏动图包含信噪比、局部脉搏幅度、相位等信息，并验证了数据集能捕捉面部和颈部皮肤区域的生理特征。
- Conclusion: Pulse3DFace是首个3D血流搏动数据集，为改进远程脉搏估计方法、验证合成视频数据以及研究多视角光照补偿方法提供了重要资源。


### [52] [Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA](https://arxiv.org/abs/2512.10521)
*Pasquale De Marinis,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: TaP提出了一种简单有效的少样本语义分割方法，通过LoRA微调编码器来增强对新类别的适应性，在多个基准测试中显著提升性能。

- Motivation: 少样本语义分割中，编码器提取新类别特征的能力有限是主要瓶颈，现有研究多关注解码器改进，而编码器对新类别的泛化能力不足。
- Method: 提出TaP方法，利用低秩适应（LoRA）在支持集上微调编码器，计算开销小，能快速适应新类别同时避免灾难性遗忘，且与现有FSS框架兼容。
- Result: 在COCO 20^i、Pascal 5^i等基准测试以及跨域数据集（DeepGlobe、ISIC、Chest X-ray）上，TaP显著提升多种模型和shot设置的性能，尤其在复杂多类别场景表现突出。
- Conclusion: TaP通过解决编码器对新类别泛化的关键限制，为构建更鲁棒、高效、可泛化的分割系统铺平了道路，即使低秩适应也能获得强性能。


### [53] [Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding](https://arxiv.org/abs/2512.10548)
*Yuchen Feng,Zhenyu Zhang,Naibin Gu,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CV

TL;DR: Blink是一个受人类眨眼过程启发的动态视觉token分辨率框架，通过选择性分配计算资源到显著区域来增强多模态大语言模型的视觉感知能力。

- Motivation: 人类通过动态扫描和聚焦显著区域来高效感知复杂场景，而当前多模态大语言模型(MLLMs)的视觉感知能力有限。研究发现MLLMs在不同层会自然关注不同视觉区域，选择性分配更多计算到显著token可以增强视觉感知。
- Method: 提出Blink框架，包含两个模块：显著性引导扫描和动态token分辨率。首先基于注意力图估计每层视觉token的显著性，通过即插即用的token超分辨率模块扩展重要token；在下一层，当这些扩展token失去焦点时将其丢弃。这种动态机制平衡了广泛探索和细粒度聚焦。
- Result: 大量实验验证了Blink的有效性，证明其能够增强视觉感知和多模态理解能力。
- Conclusion: Blink通过模拟人类启发的动态视觉处理过程，在单次前向传播中实现了自适应且高效的视觉感知增强，为多模态大语言模型的视觉能力提升提供了新思路。


### [54] [Grounding Everything in Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2512.10554)
*Xiangxuan Ren,Zhongdao Wang,Liping Hou,Pin Tang,Guoqing Wang,Chao Ma*

Main category: cs.CV

TL;DR: GETok是一种用于多模态大语言模型的空间表示方法，通过引入可学习的网格标记和偏移标记来提升模型在2D图像空间中的对象定位能力，无需改变自回归Transformer架构。

- Motivation: 当前多模态大语言模型使用自回归Transformer架构需要对输入图像进行标记化，这限制了模型在2D图像空间中准确定位对象的能力。需要解决如何改进序列语言标记以更好地在2D空间中对对象进行定位的问题。
- Method: GETok方法将专门的可学习标记词汇集成到MLLMs中：首先使用网格标记将图像平面划分为结构化的空间锚点，然后利用偏移标记实现精确且迭代的定位预测细化。通过将空间关系直接嵌入到标记中，GETok在保持自回归架构不变的情况下提升了2D空间推理能力。
- Result: 大量实验表明，GETok在监督微调和强化学习设置下的各种指代任务中，都显著优于现有最先进的方法，取得了卓越的性能表现。
- Conclusion: GETok通过引入空间表示方法，成功解决了MLLMs在2D图像空间中对象定位的局限性，为多模态大语言模型的空间推理能力提供了有效的解决方案，同时保持了原有的自回归架构不变。


### [55] [Data-Efficient American Sign Language Recognition via Few-Shot Prototypical Networks](https://arxiv.org/abs/2512.10562)
*Meher Md Saad*

Main category: cs.CV

TL;DR: 提出基于骨架编码器的少样本原型网络框架，用于解决孤立手语识别中的数据稀缺和长尾分布问题，通过度量学习在语义空间中分类手语，在WLASL数据集上取得显著性能提升。

- Motivation: 孤立手语识别面临数据稀缺和词汇长尾分布问题，传统分类方法在数据不足时容易过拟合高频类别而无法泛化到罕见类别，需要一种能有效处理数据稀缺情况的方法。
- Method: 提出少样本原型网络框架，结合时空图卷积网络（ST-GCN）和多尺度时间聚合（MSTA）模块，通过情景训练学习语义度量空间，基于动态类别原型进行手语分类。
- Result: 在WLASL数据集上达到43.75%的Top-1准确率和77.10%的Top-5准确率，比相同骨干架构的标准分类基线提升超过13%，在SignASL数据集上实现近30%的零样本泛化准确率。
- Conclusion: 原型训练策略在数据稀缺情况下优于标准分类方法，提供了用有限数据识别大量手语词汇的可扩展途径，具有强大的零样本泛化能力。


### [56] [Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner](https://arxiv.org/abs/2512.10571)
*Haojie Zheng,Shuchen Weng,Jingqi Liu,Siqi Yang,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: AVI-Edit是一个音频同步视频实例编辑框架，通过粒度感知掩码细化器和自反馈音频代理实现精确的实例级编辑和音频视觉同步。

- Motivation: 现有视频编辑方法大多忽视音频视觉同步，缺乏对实例级编辑所需的精细空间和时间控制能力，这限制了高质量音频同步视频内容的创建。
- Method: 提出粒度感知掩码细化器迭代优化用户提供的粗糙掩码为精确实例区域；设计自反馈音频代理生成高质量音频指导；构建大规模实例中心对应数据集。
- Result: 实验表明AVI-Edit在视觉质量、条件遵循和音频视觉同步方面优于现有最先进方法。
- Conclusion: AVI-Edit通过创新的掩码细化和音频代理设计，成功解决了视频实例编辑中的音频同步问题，为精确的音频同步视频编辑提供了有效解决方案。


### [57] [Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration](https://arxiv.org/abs/2512.10581)
*Wenlong Jiao,Heyang Lee,Ping Wang,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: 提出对称U-Net架构用于一体化图像修复，无需复杂架构或退化提示策略，通过特征对齐和跨尺度传播实现SOTA性能，并引入语义增强变体SE-SymUNet。

- Motivation: 现有的一体化图像修复方法过度依赖复杂架构（如混合专家、扩散模型）和精心设计的退化提示策略，作者发现精心设计的特征提取本身已编码退化信息，对称U-Net架构足以有效利用这些线索。
- Method: 提出对称U-Net架构，通过编码器-解码器间的特征尺度对齐和简化的跨尺度传播，在跳跃连接中使用简单的加法融合。进一步提出语义增强变体SE-SymUNet，通过交叉注意力从冻结的CLIP特征中直接注入语义信息以增强退化先验。
- Result: 在多个基准测试中验证了方法的优越性。SymUNet在基准数据集上取得了比现有方法更好的结果，同时降低了计算成本。SE-SymUNet进一步提升了性能，为一体化图像修复建立了更简单更强的基础。
- Conclusion: 精心设计的特征提取本身编码了退化信息，对称U-Net架构足以有效利用这些线索。提出的SymUNet和SE-SymUNet为一体化图像修复的未来发展提供了更简单更强的基础框架。


### [58] [Salient Object Detection in Complex Weather Conditions via Noise Indicators](https://arxiv.org/abs/2512.10592)
*Quan Chen,Xiaokai Yang,Tingyu Wang,Rongfeng Lu,Xichun Sheng,Yaoqi Sun,Chenggang Yan*

Main category: cs.CV

TL;DR: 提出一种针对多天气条件的显著目标检测框架，包含特定编码器和可替换解码器，通过噪声指示器融合模块嵌入天气感知先验，提升复杂天气下的分割精度。

- Motivation: 现有显著目标检测方法大多假设低噪声视觉条件，忽视了真实场景中天气引起的噪声对分割精度的影响，需要专门针对多天气条件的解决方案。
- Method: 1) 提出包含特定编码器和可替换解码器的SOD框架；2) 引入one-hot向量作为噪声指示器表示不同天气类型；3) 设计噪声指示器融合模块(NIFM)，将语义特征和噪声指示器作为双输入，通过自适应特征调制嵌入天气感知先验；4) 特定编码器保持与主流SOD解码器的兼容性。
- Result: 在WXSOD数据集上，使用不同训练数据规模(100%、50%、30%完整训练集)、三种编码器和七种解码器配置进行实验。结果表明，提出的SOD框架（特别是NIFM增强的特定编码器）相比普通编码器，在复杂天气条件下提高了分割精度。
- Conclusion: 该研究针对多天气条件下的显著目标检测问题，通过噪声指示器融合模块有效嵌入天气感知先验，提出的框架在复杂天气条件下表现出更好的分割性能，且保持与现有解码器的兼容性。


### [59] [Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.10596)
*J. Xiao,Y. Guo,X. Zi,K. Thiyagarajan,C. Moreira,M. Prasad*

Main category: cs.CV

TL;DR: 提出RSRT数据集和TRSLLaVA方法，通过文本到文本匹配实现无需训练的遥感图像语义检索，在多个基准测试中达到与监督模型竞争的性能。

- Motivation: 解决遥感图像检索中的"语义鸿沟"问题，现有方法依赖昂贵的领域特定训练，且缺乏评估VLM生成文本在零样本检索中实用性的基准。
- Method: 引入RSRT数据集（包含多结构化标注），提出TRSLLaVA方法，将跨模态检索重新定义为文本到文本匹配问题，利用丰富文本描述作为查询，在统一文本嵌入空间中进行匹配，完全无需模型训练或微调。
- Result: 在RSITMD和RSICD基准测试中，该方法与最先进的监督模型竞争激烈。在RSITMD上达到42.62%的平均召回率，是标准零样本CLIP基线（23.86%）的近两倍，超越多个顶级监督模型。
- Conclusion: 通过结构化文本实现高质量语义表示，为遥感图像检索提供了一个强大且成本效益高的范式，验证了训练免费方法的有效性。


### [60] [Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos](https://arxiv.org/abs/2512.10607)
*Bishoy Galoaa,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: TCAM是一个无需用户查询的自动视频理解框架，通过运动场注意力机制发现并描述视频中的多种运动模式，在遮挡、伪装等挑战性条件下表现优异。

- Motivation: 在遮挡、伪装或快速运动等挑战性视频条件下，理解视频内容往往更依赖于运动动态而非静态外观。需要一种能够自动发现和描述运动模式的方法。
- Method: TCAM通过运动场注意力机制自主观察视频，识别多种运动活动，并将每个自然语言描述空间定位到对应的轨迹上。结合对比视觉-语言表示的运动模式提供强大的语义信号，通过统一训练结合全局视频-文本对齐和细粒度空间对应。
- Result: 在MeViS基准测试中，TCAM实现了58.4%的视频到文本检索准确率，64.9的JF空间定位分数，每个视频发现4.8个相关表达，准确率达84.7%，展示了强大的跨任务泛化能力。
- Conclusion: TCAM证明了运动模式与对比视觉-语言表示对齐时，能够为识别和描述动作提供强大的语义信号，实现了无需查询的多运动表达发现。


### [61] [Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation](https://arxiv.org/abs/2512.10608)
*Mohammad Sadegh Gholizadeh,Amir Arsalan Rezapour*

Main category: cs.CV

TL;DR: 该论文提出了一种结合深度特征提取与可解释图像处理模块的深度学习架构，用于眼科疾病的自动诊断，特别关注视网膜血管分割作为辅助任务来提高分类的可解释性和临床适用性。

- Motivation: 近年来威胁视力的眼病发病率急剧上升，需要可扩展且准确的筛查解决方案。标准卷积神经网络存在"黑盒"限制，需要弥合算法输出与专家医学验证之间的差距。
- Method: 实现了一个结合深度特征提取与可解释图像处理模块的管道，特别关注高保真视网膜血管分割作为辅助任务来指导分类过程，将模型预测建立在临床相关的形态学特征基础上。
- Result: 通过将模型预测建立在临床相关形态学特征上，旨在减少假阳性并提高在临床环境中的部署可行性。
- Conclusion: 该方法通过结合深度学习和可解释的临床特征，为眼科疾病的自动诊断提供了一种更可靠、更可信的解决方案，有助于弥合算法输出与医学验证之间的差距。


### [62] [Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces](https://arxiv.org/abs/2512.10617)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: Lang2Motion是一个通过将运动流形与联合嵌入空间对齐来生成语言引导点轨迹的框架，利用真实视频中的点跟踪生成任意物体的显式轨迹，在文本到轨迹检索和运动准确性方面优于现有方法。

- Motivation: 现有研究主要关注人体运动或视频合成，缺乏为任意物体生成显式轨迹的能力。作者希望开发一个能够根据语言描述生成精确物体运动轨迹的框架，利用真实视频中的点跟踪数据。
- Method: 基于Transformer的自编码器通过双监督学习轨迹表示：文本运动描述和渲染的轨迹可视化，两者都通过冻结的CLIP编码器映射。从真实世界视频中提取点跟踪数据来学习运动流形。
- Result: 在文本到轨迹检索上达到34.2%的Recall@1，比基于视频的方法高出12.5个百分点；运动准确性比视频生成基线提高33-52%（12.4 ADE vs 18.3-25.3）；仅在不同物体运动上训练就能在人体动作识别上达到88.3%的Top-1准确率。
- Conclusion: Lang2Motion成功地将语言与物体运动轨迹对齐，支持风格迁移、语义插值和潜在空间编辑，展示了跨运动领域的有效迁移能力，为语言引导的运动生成提供了新方法。


### [63] [DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM](https://arxiv.org/abs/2512.10619)
*Qintong Zhang,Junyuan Zhang,Zhifei Ren,Linke Ouyang,Zichen Wen,Junbo Niu,Yuan Qu,Bin Wang,Ka-Ho Chow,Conghui He,Wentao Zhang*

Main category: cs.CV

TL;DR: DOCR-Inspector是一个基于视觉语言模型的文档解析质量评估系统，通过细粒度错误检测和分析来评估文档解析质量，在真实场景中优于商业模型。

- Motivation: 现有文档解析评估存在三个问题：1）标准基准测试存在数据集特定偏差，导致模型排名不一致；2）基准指标通常只提供总体分数，掩盖了具体的错误模式；3）缺乏可靠、全面的真实场景文档解析质量评估方法。
- Method: 提出DOCR-Inspector系统，将文档解析评估形式化为细粒度错误检测和分析。使用VLM-as-a-Judge方法分析文档图像及其解析输出，识别所有错误并将其分类到28种预定义类型。构建DOCRcase-200K训练数据集，并提出Chain-of-Checklist推理范式来实现分层结构的解析质量评估。
- Result: 在DOCRcaseBench（882个真实文档解析案例的手动标注基准）上，DOCR-Inspector-7B模型优于Gemini 2.5 Pro等商业模型以及领先的开源模型。实验还表明其质量评估能为解析结果优化提供有价值的指导。
- Conclusion: DOCR-Inspector不仅是一个实用的文档解析评估工具，还能推动文档解析系统的大规模发展。该方法解决了真实场景中文档解析质量评估的可靠性问题，并为系统优化提供了有效指导。


### [64] [K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices](https://arxiv.org/abs/2512.10628)
*Bishoy Galoaa,Pau Closas,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: K-Track是一个基于卡尔曼滤波的点跟踪加速框架，通过稀疏深度学习关键帧更新和轻量级卡尔曼滤波预测，实现5-10倍加速同时保持85%以上原始精度，适用于边缘设备部署。

- Motivation: 现有深度学习点跟踪器依赖逐帧GPU推理，在计算、功耗和连接性有限的边缘设备上部署困难，需要一种既能保持精度又能大幅降低计算成本的解决方案。
- Method: 提出K-Track框架，结合稀疏深度学习关键帧更新和轻量级卡尔曼滤波预测，使用贝叶斯不确定性传播保持时间一致性，实现跟踪器无关的通用加速。
- Result: 在多个先进点跟踪器上评估，实现5-10倍加速，保持超过85%的原始跟踪器精度，在NVIDIA Jetson Nano和RTX Titan等边缘平台上达到实时性能。
- Conclusion: K-Track通过显著降低计算需求同时保持精度，为在资源受限的真实世界环境中部署高质量点跟踪提供了实用路径，弥合了现代跟踪算法与可部署视觉系统之间的差距。


### [65] [TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection](https://arxiv.org/abs/2512.10652)
*Jian-Yu Jiang-Lin,Kang-Yang Huang,Ling Zou,Ling Lo,Sheng-Ping Yang,Yu-Wen Tseng,Kun-Hsiang Lin,Chia-Ling Chen,Yu-Ting Ta,Yan-Tsung Wang,Po-Ching Chen,Hongxia Xie,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: TriDF是一个用于可解释DeepFake检测的综合基准，包含图像、视频和音频三种模态的16种深度伪造类型，评估感知、检测和幻觉三个关键方面。

- Motivation: 生成模型的进步使得伪造真实人物肖像变得容易，对安全、通信和公共信任构成严重威胁。需要不仅能区分真实与伪造内容，还能提供清晰可靠推理的系统。
- Method: 构建TriDF基准，包含来自先进合成模型的高质量伪造内容，涵盖16种DeepFake类型。评估三个关键方面：感知（识别细粒度伪造痕迹）、检测（跨不同伪造家族和生成器的分类性能）、幻觉（量化模型生成解释的可靠性）。
- Result: 实验表明，准确的感知对可靠检测至关重要，但幻觉会严重干扰决策过程，揭示了这三个方面之间的相互依赖性。
- Conclusion: TriDF提供了一个统一框架来理解检测准确性、证据识别和解释可靠性之间的相互作用，为构建可信赖的系统以应对现实世界合成媒体威胁奠定了基础。


### [66] [NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation](https://arxiv.org/abs/2512.10660)
*Hanfeng Wu,Marlon Steiner,Michael Schmidt,Alvaro Marcos-Ramiro,Christoph Stiller*

Main category: cs.CV

TL;DR: NaviHydra是一个可控的导航引导端到端模型，通过从现有规则模拟器蒸馏而来，能够根据高级导航指令生成符合意图的轨迹，在NAVSIM基准测试中达到最先进水平。

- Motivation: 自动驾驶场景的复杂性需要能够解释高级导航指令并生成安全轨迹的鲁棒模型。传统基于规则的系统在动态环境中表现不佳，而端到端方法难以遵循明确的导航指令。
- Method: 提出NaviHydra框架：1）从现有规则模拟器蒸馏可控的导航引导端到端模型；2）采用鸟瞰图（BEV）轨迹收集方法增强轨迹特征提取；3）引入新的导航合规性指标评估意图路线遵循度；4）设计测试评估模型对各种导航指令的响应。
- Result: 在NAVSIM基准测试中显著优于基线模型，达到最先进的结果，证明了其在推进自动驾驶方面的有效性。
- Conclusion: NaviHydra通过结合规则模拟器的可解释性和端到端方法的适应性，成功解决了导航指令遵循问题，提高了自动驾驶系统的可控性和导航安全性。


### [67] [XDen-1K: A Density Field Dataset of Real-World Objects](https://arxiv.org/abs/2512.10668)
*Jingxuan Zhang,Tianqi Yu,Yatu Zhang,Jinze Wu,Kaixin Yao,Jingyang Liu,Yuyao Zhang,Jiayuan Gu,Jingyi Yu*

Main category: cs.CV

TL;DR: XDen-1K是首个大规模多模态数据集，专注于物体体积密度估计，包含1000个真实物体及其3D模型、部件标注和双平面X射线扫描，用于提升物理属性理解和机器人操作性能。

- Motivation: 当前AI模型主要关注物体表面几何和外观，但忽略了内部物理属性（如体积密度），而这些属性对于预测物体质心、稳定性和交互动态至关重要。主要瓶颈在于缺乏大规模真实世界数据。
- Method: 1) 构建XDen-1K数据集：包含1000个真实物体、148个类别，提供高分辨率3D几何模型、部件级标注和双平面X射线扫描；2) 提出新颖的优化框架，从稀疏X射线视图恢复高保真体积密度场；3) 将X射线图像作为条件信号集成到现有分割网络中进行体积分割。
- Result: 实验表明，利用该数据集能有效提高质心估计的准确性，并提升机器人操作的成功率。X射线条件化的分割网络实现了体积分割。
- Conclusion: XDen-1K将成为物理基础视觉推理和具身AI研究的基石资源和挑战性新基准，推动未来研究发展。


### [68] [Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching](https://arxiv.org/abs/2512.10674)
*Javier Villena Toro,Mehdi Tarkian*

Main category: cs.CV

TL;DR: Geo6DPose：一种轻量级、完全本地化、无需训练的零样本6D物体姿态估计方法，通过几何过滤策略实现快速推理，适用于计算资源有限的机器人部署场景。

- Motivation: 现有零样本6D姿态估计方法依赖大规模模型和云端推理，存在高延迟、高能耗、连接风险、成本和数据治理等问题，不适用于计算资源有限且需要本地推理的机器人实际应用场景。
- Method: 结合基础模型视觉特征与几何过滤策略：计算模板DINO描述符与场景补丁的相似性图，通过将场景补丁中心投影到3D空间和模板描述符投影到物体模型坐标系建立相互对应关系，最后通过对应关系驱动的RANSAC恢复姿态，并使用加权几何对齐度量进行排序。
- Result: 在单个商用GPU上实现亚秒级推理（1.08 FPS），平均召回率达到53.7 AR，与显著更大的零样本基线方法性能相当，无需训练、微调或网络访问。
- Conclusion: Geo6DPose通过用几何可靠性换取模型规模，实现了实用、完全本地的6D感知，适用于机器人部署，且与演进的基础模型骨干网络保持兼容。


### [69] [Optimal transport unlocks end-to-end learning for single-molecule localization](https://arxiv.org/abs/2512.10683)
*Romain Seailles,Jean-Baptiste Masson,Jean Ponce,Julien Mairal*

Main category: cs.CV

TL;DR: 提出基于最优传输损失和迭代神经网络的新SMLM方法，消除NMS需求，提升高密度发射体下的定位性能

- Motivation: 传统SMLM需要非重叠发射体导致采集时间长，现有深度学习方法依赖不可微的NMS层，可能丢弃真实信号
- Method: 将SMLM训练目标重新表述为集合匹配问题，推导最优传输损失函数；提出集成显微镜光学系统知识的迭代神经网络架构
- Result: 在合成基准测试和真实生物数据上，新损失函数和架构在中等和高发射体密度下均超越现有最佳方法
- Conclusion: 提出的方法消除了推理过程中对NMS的需求，实现了端到端训练，显著提升了SMLM在高密度发射体下的性能


### [70] [Sharp Monocular View Synthesis in Less Than a Second](https://arxiv.org/abs/2512.10685)
*Lars Mescheder,Wei Dong,Shiwei Li,Xuyang Bai,Marcel Santos,Peiyun Hu,Bruno Lecouat,Mingmin Zhen,Amaël Delaunoy,Tian Fang,Yanghai Tsin,Stephan R. Richter,Vladlen Koltun*

Main category: cs.CV

TL;DR: SHARP是一种从单张图像进行逼真视图合成的方法，通过神经网络前向传播在1秒内回归3D高斯表示参数，支持实时渲染和度量相机运动，在多个数据集上实现SOTA性能。

- Motivation: 解决从单张图像进行逼真视图合成的挑战，传统方法通常需要多视图输入或复杂的优化过程，SHARP旨在实现快速、高质量的零样本泛化。
- Method: 通过神经网络单次前向传播回归3D高斯表示参数，在标准GPU上不到1秒完成，生成的表示支持实时渲染和度量相机运动。
- Result: 在多个数据集上实现SOTA，LPIPS降低25-34%，DISTS降低21-43%，合成时间降低三个数量级，具有强大的零样本泛化能力。
- Conclusion: SHARP提供了一种高效的单图像逼真视图合成方法，在速度和质量上都显著优于先前方法，为实时应用提供了可能。


### [71] [CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images](https://arxiv.org/abs/2512.10715)
*Matias Cosarinsky,Nicolas Gaggion,Rodrigo Echeveste,Enzo Ferrante*

Main category: cs.CV

TL;DR: 提出两种不确定性估计方法（潜在不确定性和预测不确定性）用于胸部X光解剖标志点分割，并发布包含65.7万张图像不确定性标注的CheXmask-U数据集

- Motivation: 医学图像分割系统需要不确定性估计来确保临床部署的安全性，但现有研究主要关注像素级不确定性，而基于标志点的分割方法具有拓扑保证却缺乏不确定性研究
- Method: 结合标准图像卷积编码器和基于图的生成解码器的混合神经网络架构，利用其变分潜在空间推导两种不确定性度量：1）直接从学习分布参数获得的潜在不确定性；2）通过从潜在样本生成多个随机输出预测获得的预测不确定性
- Result: 两种不确定性度量都随扰动严重程度增加而增加，能够识别不可靠预测，支持分布外检测，并发布了包含657,566个胸部X光标志点分割及逐节点不确定性估计的CheXmask-U数据集
- Conclusion: 不确定性估计是增强基于标志点的解剖分割方法在胸部X光中鲁棒性和安全部署的有前景方向，发布的数据集和工具支持社区进一步研究


### [72] [SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving](https://arxiv.org/abs/2512.10719)
*Peizheng Li,Zhenghao Zhang,David Holtz,Hang Yu,Yutong Yang,Yuzhi Lai,Rui Song,Andreas Geiger,Andreas Zell*

Main category: cs.CV

TL;DR: SpaceDrive提出了一种空间感知的视觉语言模型驾驶框架，通过将空间信息作为显式位置编码而非文本数字标记来处理，提升了3D空间关系理解和轨迹规划精度。

- Motivation: 当前基于视觉语言模型（VLM）的端到端自动驾驶方法虽然具有强大的视觉理解和推理能力，但在理解细粒度的3D空间关系方面存在困难，而这是物理世界交互系统的基本要求。
- Method: 提出SpaceDrive框架，将空间信息作为显式位置编码（PEs）而非文本数字标记。使用通用位置编码器处理来自多视角深度估计、历史自车状态和文本提示的所有3D坐标。这些3D位置编码既增强对应的2D视觉标记，又作为任务无关的坐标表示，替代数字标记作为VLM的输入和输出。
- Result: 在nuScenes数据集上实现了最先进的开放环性能，在Bench2Drive闭环基准测试中获得了78.02的驾驶分数，在现有VLM方法中排名第二。
- Conclusion: SpaceDrive通过将空间信息作为显式位置编码，显著提升了VLM在自动驾驶中的3D空间关系理解和轨迹规划能力，验证了空间感知表示对物理世界交互系统的重要性。


### [73] [Video Depth Propagation](https://arxiv.org/abs/2512.10725)
*Luigi Piccinelli,Thiemo Wandel,Christos Sakaridis,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: VeloDepth：一种高效的在线视频深度估计方法，通过传播模块利用时空先验和深度特征传播，实现实时、一致且准确的深度估计。

- Motivation: 现有视频深度估计方法存在两个主要问题：1）逐帧单目模型导致时间不一致性和不准确性；2）使用计算密集的时间建模方法不适合实时应用。这些限制严重影响了实际应用中的通用性和性能。
- Method: 提出VeloDepth，一种高效鲁棒的在线视频深度估计流水线。核心是引入传播模块，通过基于光流的扭曲和学习的残差校正来细化和传播深度特征和预测。该方法结构上强制时间一致性，在连续帧间产生稳定的深度预测。
- Result: 在多个基准测试上的零样本评估表明，VeloDepth在时间一致性方面达到最先进水平，同时保持有竞争力的准确性。与现有视频深度估计器相比，推理速度显著更快。
- Conclusion: VeloDepth为实时深度估计提供了一个实用、高效且准确的解决方案，适用于多种感知任务。代码和模型已开源。


### [74] [IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation](https://arxiv.org/abs/2512.10730)
*Yuan-Ming Li,Qize Yang,Nan Lei,Shenghao Fu,Ling-An Zeng,Jian-Fang Hu,Xihan Wei,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: IRMoGen提出了一种通过评估和精炼任务桥接运动理解与生成的新范式，IRG-MotionLLM模型通过迭代式文本-运动对话实现三者交织推理，显著提升文本-运动对齐性能。

- Motivation: 现有运动感知大语言模型通常将理解与生成任务分开处理，限制了任务间交互反馈的相互增益。研究发现运动评估和精炼任务可作为理解与生成之间的关键桥梁，实现双向知识流动。
- Method: 提出IRMoGen范式，通过迭代式文本-运动对话将运动生成与评估、精炼紧密耦合。开发IRG-MotionLLM模型，采用三阶段渐进训练方案：初始化基础能力，然后增强原生IRMoGen能力。构建自动化数据引擎从现有文本-运动数据集合成交织推理标注。
- Result: 实验表明：1) 评估和精炼任务显著改善文本-运动对齐；2) 交织运动生成、评估和精炼步骤在各训练阶段均带来一致性能提升；3) IRG-MotionLLM明显优于基线模型，在标准文本到运动生成基准上达到先进性能。跨评估器测试进一步验证其有效性。
- Conclusion: IRMoGen通过评估和精炼任务桥接运动理解与生成，IRG-MotionLLM模型通过交织推理机制实现了性能显著提升，为运动生成任务提供了新的交互式范式。


### [75] [LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation](https://arxiv.org/abs/2512.10750)
*Tianyu Zhou,Junyi Tang,Zehui Li,Dahong Qian,Suncheng Xiang*

Main category: cs.CV

TL;DR: 提出LDP框架，利用多模态大语言模型生成专业息肉诊断报告，通过MMEndo数据集和参数高效微调，显著提升报告质量并降低计算成本

- Motivation: 传统自动化息肉诊断报告存在不一致性和幻觉问题，高质量多模态医学数据稀缺，需要开发更可靠、临床可行的解决方案
- Method: 构建MMEndo多模态内窥镜数据集，基于Qwen2-VL-7B模型，采用LoRA参数高效微调和DPO直接偏好优化进行临床标准对齐
- Result: LDP在自动指标和临床专家评估中优于现有基线（医师评分7.2/10），训练计算成本比全微调降低833倍，在IU-XRay数据集上验证了鲁棒性
- Conclusion: LDP为初级医疗保健提供了可扩展、临床可行的息肉诊断解决方案，显著提升报告质量同时大幅降低计算需求


### [76] [Blood Pressure Prediction for Coronary Artery Disease Diagnosis using Coronary Computed Tomography Angiography](https://arxiv.org/abs/2512.10765)
*Rene Lisasi,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 提出一个端到端管道，从CCTA自动提取冠状动脉几何结构，并利用扩散回归模型直接预测冠状动脉血压，绕过传统CFD计算，实现快速、非侵入性的CAD诊断支持。

- Motivation: 传统CFD模拟冠状动脉血流计算成本高、耗时，难以集成到大规模临床工作流中，限制了AI模型的训练数据和基于生理学的CAD评估的广泛应用。
- Method: 开发端到端管道：1) 从CCTA自动提取冠状动脉几何结构；2) 简化模拟数据生成；3) 引入扩散回归模型，直接从CCTA特征预测冠状动脉血压分布，绕过CFD计算。
- Result: 在模拟冠状动脉血流动力学数据集上，模型达到R²=64.42%，RMSE=0.0974，归一化RMSE=0.154，优于多个基线方法，表现最优。
- Conclusion: 该工作提供了一个可扩展且易于访问的框架，用于快速、非侵入性的血压预测，支持CAD诊断，解决了传统CFD工作流的局限性。


### [77] [What matters for Representation Alignment: Global Information or Spatial Structure?](https://arxiv.org/abs/2512.10794)
*Jaskirat Singh,Xingjian Leng,Zongze Wu,Liang Zheng,Richard Zhang,Eli Shechtman,Saining Xie*

Main category: cs.CV

TL;DR: 研究发现，在生成式训练中，目标表示的空间结构（而非全局语义性能）对生成质量起关键作用。通过简单修改增强空间信息传递，提出的iREPA方法能显著提升收敛速度。

- Motivation: 研究REPA（表示对齐）方法中目标表示的关键因素：是全局语义信息（如ImageNet准确率）还是空间结构（补丁标记间的余弦相似度）对生成质量更重要。传统观点认为更强的全局语义性能会带来更好的生成效果，但本文对此提出质疑。
- Method: 首先对27种不同视觉编码器进行大规模实证分析。然后提出两个简单修改：1）将标准MLP投影层替换为卷积层；2）引入外部表示的空间归一化层。这些修改专门强调空间信息的传递，形成iREPA方法。
- Result: 实证分析发现空间结构（而非全局性能）驱动生成性能。iREPA方法（仅需<4行代码）在各种视觉编码器、模型规模和训练变体（如REPA、REPA-E、Meanflow、JiT等）中都能一致提升收敛速度。
- Conclusion: 空间结构是目标表示中影响生成性能的关键因素。简单的iREPA修改能有效提升表示对齐方法的训练效率，促使重新思考表示对齐的基本工作机制及其在生成模型训练中的应用。


### [78] [Graph Laplacian Transformer with Progressive Sampling for Prostate Cancer Grading](https://arxiv.org/abs/2512.10808)
*Masum Shah Junayed,John Derek Van Vessem,Qian Wan,Gahie Nam,Sheida Nabavi*

Main category: cs.CV

TL;DR: 提出GLAT+IRM模型，通过迭代优化补丁选择和图拉普拉斯注意力机制，提升前列腺癌WSI分级的准确性和空间一致性

- Motivation: 解决前列腺癌全切片图像分级中的三大挑战：大规模图像处理、组织异质性、诊断相关区域选择困难。现有方法采用随机或静态补丁选择，导致冗余或非信息区域影响性能
- Method: 1. 迭代优化模块(IRM)：使用预训练ResNet50提取局部特征，基础模型进行重要性评分，迭代优化补丁选择；2. 图拉普拉斯注意力变换器(GLAT)：将补丁作为节点构建图，通过图拉普拉斯约束确保空间一致性，可学习滤波机制增强特征表示；3. 凸聚合机制：动态调整补丁重要性，生成鲁棒的WSI级表示
- Result: 在5个公共数据集和1个私有数据集上验证，模型性能优于现有最先进方法，实现了更高的性能和空间一致性，同时保持计算效率
- Conclusion: 提出的GLAT+IRM框架有效解决了前列腺癌WSI分级中的关键挑战，通过智能补丁选择和空间一致性建模显著提升了诊断性能


### [79] [Self-Ensemble Post Learning for Noisy Domain Generalization](https://arxiv.org/abs/2512.10818)
*Wang Lu,Jindong Wang*

Main category: cs.CV

TL;DR: 提出SEPL方法，通过特征探针训练和预测集成推理，利用模型中间特征表示来增强域泛化方法在噪声标签下的鲁棒性。

- Motivation: 当域泛化遇到噪声标签时，噪声会加剧深度层中虚假特征的放大，导致现有算法性能下降。需要探索如何使现有方法在遇到噪声时仍能有效工作。
- Method: 提出自集成后学习（SEPL）方法，包括特征探针训练和预测集成推理两部分。利用模型架构中的中间特征表示，训练多个探针分类器充分挖掘预训练模型能力，同时通过集成这些多样化分类头的输出获得最终预测。针对噪声标签问题，采用半监督算法训练探针分类器。
- Result: 广泛的实验评估表明，所提方法不仅增强了现有方法的鲁棒性，而且在现实应用中展现出显著潜力和高度灵活性。
- Conclusion: SEPL方法通过利用模型内部潜在特征的判别能力和关注不同图像区域的特点，有效缓解了噪声标签对域泛化性能的负面影响，提供了一种实用的解决方案。


### [80] [PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning](https://arxiv.org/abs/2512.10840)
*Jianqi Chen,Biao Zhang,Xiangjun Tang,Peter Wonka*

Main category: cs.CV

TL;DR: PoseGAM：一个几何感知的多视角框架，无需显式特征匹配，直接从查询图像和多个模板图像预测6D物体姿态，在未见物体上表现优异。

- Motivation: 现有6D物体姿态估计方法通常依赖显式构建查询图像与物体模型或模板图像之间的特征对应关系，这在处理未见物体时存在挑战。需要一种无需显式匹配的直接预测方法。
- Method: 基于多视角基础模型架构，通过两种互补机制整合物体几何信息：显式基于点的几何和从几何表示网络学习到的特征。构建大规模合成数据集（超过19万个物体）增强鲁棒性和泛化能力。
- Result: 在多个基准测试中达到最先进性能，平均AR提升5.1%，在个别数据集上提升高达17.6%，显示出对未见物体的强泛化能力。
- Conclusion: PoseGAM通过几何感知的多视角框架，无需显式特征匹配即可直接预测物体姿态，在未见物体姿态估计任务中表现出优异的性能和泛化能力。


### [81] [SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation](https://arxiv.org/abs/2512.10860)
*Kehong Gong,Zhengyu Wen,Mingxi Xu,Weixia He,Qi Wang,Ning Zhang,Zhengyu Li,Chenbin Li,Dongze Lian,Wei Zhao,Xiaoyu He,Mingyuan Zhang*

Main category: cs.CV

TL;DR: SWiT-4D：基于滑动窗口Transformer的视频到4D网格生成方法，利用图像到3D生成先验，仅需少量视频微调即可实现高质量时空一致的4D重建。

- Motivation: 单目视频到高质量动画3D资产（显式4D网格）的转换仍具挑战性，缺乏大规模自然捕获的4D网格数据集限制了纯数据驱动的视频到4D模型训练。同时，图像到3D生成领域已有强大先验模型可供利用。
- Method: 提出SWiT-4D（滑动窗口Transformer），无缝集成任何基于DiT的图像到3D生成器，通过时空建模跨视频帧处理，保持原始单图像前向过程，支持任意长度视频的4D网格重建。针对静态相机单目视频，引入基于优化的轨迹模块恢复全局平移。
- Result: 仅需单个短视频（<10秒）微调即可实现高保真几何和稳定时间一致性。在领域内测试集和挑战性领域外基准（C4D、Objaverse、野外视频）上均优于现有基线，在时间平滑性方面表现突出。
- Conclusion: SWiT-4D展示了强大的数据效率，在极有限的4D监督下实现高质量视频到4D转换，具有实际部署可行性，为利用图像到3D先验进行4D内容生成提供了有效解决方案。


### [82] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: MMSI-Video-Bench是一个全面评估多模态大语言模型视频空间智能的基准测试，包含1,106个问题，覆盖感知、规划、预测和跨视频推理四个层次，揭示了模型与人类在空间理解上的巨大差距。

- Motivation: 目前缺乏全面评估MLLMs在连续视觉输入中空间理解能力的基准测试，而这对MLLMs成为物理环境中的通用助手至关重要。
- Method: 构建了一个包含1,106个问题的人类标注基准，基于1,278个视频片段（来自25个数据集和内部视频），采用四层框架（感知、规划、预测、跨视频推理），每个问题由3DV专家设计和审核。
- Result: 评估了25个开源和专有MLLMs，发现显著的人机差距：许多模型接近随机猜测，最佳推理模型落后人类近60%。空间微调模型在基准上泛化能力差，存在几何推理、运动定位、长时预测和跨视频对应等系统性失败。
- Conclusion: 该基准为推进视频空间智能研究提供了坚实的测试平台，揭示了当前MLLMs在空间理解方面的严重不足，并指出了典型帧采样策略、3D空间线索和思维链提示在空间推理任务中的局限性。


### [83] [From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models](https://arxiv.org/abs/2512.10867)
*Zongzhao Li,Xiangzhe Kong,Jiahui Su,Zongyang Ma,Mingze Li,Songyou Li,Yuelin Zhang,Yu Rong,Tingyang Xu,Deli Zhao,Wenbing Huang*

Main category: cs.CV

TL;DR: 论文提出微观空间智能(MiSI)概念，并构建包含16.3万问答对和58.7万图像的MiSI-Bench基准框架，评估视觉语言模型在微观空间推理能力，发现当前SOTA模型表现远低于人类水平，但微调后7B模型在空间转换任务上可超越人类。

- Motivation: 评估视觉语言模型在微观空间智能（感知和推理不可见微观实体空间关系）方面的能力，这是科学发现的基础能力，但目前缺乏系统性的评估基准。
- Method: 提出MiSI-Bench基准框架，包含从约4000个分子结构衍生的16.3万问答对和58.7万图像，涵盖9个互补任务，评估从基本空间转换到复杂关系识别的能力。
- Result: 当前最先进的视觉语言模型在该基准上表现显著低于人类水平，但微调的7B模型显示出巨大潜力，在空间转换任务上甚至超越人类，但在氢键识别等科学基础任务上表现不佳。
- Conclusion: 微观空间智能是科学AGI的关键能力，当前VLMs在此领域仍有很大提升空间，需要整合明确的领域知识才能取得进展，特别是在科学基础任务方面。


### [84] [MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos](https://arxiv.org/abs/2512.10881)
*Kehong Gong,Zhengyu Wen,Weixia He,Mingxi Xu,Qi Wang,Ning Zhang,Zhengyu Li,Dongze Lian,Wei Zhao,Xiaoyu He,Mingyuan Zhang*

Main category: cs.CV

TL;DR: MoCapAnything：一个类别无关的运动捕捉框架，能够从单目视频和任意骨骼化3D资产中生成驱动特定资产的BVH动画

- Motivation: 现有运动捕捉流程大多局限于特定物种或模板，缺乏能够处理任意3D资产的通用解决方案
- Method: 采用参考引导的分解框架：1)参考提示编码器提取关节查询；2)视频特征提取器计算视觉描述符并重建粗粒度4D变形网格；3)统一运动解码器融合信息生成时序一致的轨迹；4)轻量级约束感知逆运动学恢复资产特定旋转
- Result: 在领域内基准测试和野外视频上均能生成高质量骨骼动画，实现跨物种的异构骨骼重定向，支持可扩展的提示驱动3D运动捕捉
- Conclusion: MoCapAnything填补了类别无关运动捕捉的空白，为任意3D资产提供可扩展的提示驱动动画生成能力


### [85] [PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction](https://arxiv.org/abs/2512.10888)
*Brandon Smock,Valerie Faucon-Morin,Max Sokolov,Libin Liang,Tayyibah Khanam,Maury Courtland*

Main category: cs.CV

TL;DR: 提出了PubTables-v2数据集，支持多页表格结构识别等挑战性任务，并基于此开发了POTATR模型用于页面级表格提取。

- Motivation: 表格提取是视觉文档理解的关键挑战，传统方法先检测表格再识别结构。最近对直接在完整页面或文档上下文中提取表格的方法（如视觉语言模型）兴趣激增，但由于缺乏标注数据，进展难以展示。
- Method: 创建了大规模数据集PubTables-v2，支持多个当前具有挑战性的表格提取任务，特别是首个大规模多页表格结构识别基准。基于此数据集开发了Page-Object Table Transformer (POTATR)，这是Table Transformer的图像到图扩展，用于全面的页面级表格提取。
- Result: 通过评估领域专用视觉语言模型在这些任务上的表现，展示了PubTables-v2的实用性，并突出了当前进展。数据、代码和训练模型将公开发布。
- Conclusion: PubTables-v2解决了表格提取领域缺乏标注数据的问题，为多页表格结构识别等挑战性任务提供了首个大规模基准，并基于此开发了有效的页面级表格提取模型POTATR。


### [86] [DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance](https://arxiv.org/abs/2512.10894)
*Peiying Zhang,Nanxuan Zhao,Matthew Fisher,Yiran Xu,Jing Liao,Difan Liu*

Main category: cs.CV

TL;DR: DuetSVG是一个统一的多模态模型，通过联合生成图像token和SVG token，并利用测试时缩放策略提升SVG生成质量，在复杂语义理解和几何一致性方面优于现有方法。

- Motivation: 现有基于视觉语言模型（VLM）的SVG生成方法仅生成文本，缺乏解码过程中的视觉信号，导致在处理复杂语义时效果不佳，难以生成视觉吸引人且几何一致的SVGs。
- Method: 提出DuetSVG统一多模态模型，以端到端方式联合生成图像token和对应的SVG token。模型在图像和SVG数据集上训练，推理时采用新颖的测试时缩放策略，利用模型自身的视觉预测作为指导来提升SVG解码质量。
- Result: 大量实验表明，该方法在广泛的应用场景中优于现有方法，能够生成视觉忠实、语义对齐且语法干净的SVGs。
- Conclusion: DuetSVG通过联合图像和SVG生成的多模态方法，结合测试时缩放策略，有效解决了现有VLM方法在SVG生成中缺乏视觉信号的问题，显著提升了生成质量。


### [87] [FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos](https://arxiv.org/abs/2512.10927)
*Yulu Gan,Ligeng Zhu,Dandan Shan,Baifeng Shi,Hongxu Yin,Boris Ivanovic,Song Han,Trevor Darrell,Jitendra Malik,Marco Pavone,Boyi Li*

Main category: cs.CV

TL;DR: FoundationMotion：一个全自动数据构建管道，通过检测视频中的物体轨迹，结合大语言模型生成细粒度运动描述和问答对，用于训练模型提升运动理解能力。

- Motivation: 当前运动理解模型在最新基准测试中表现不佳，主要原因是缺乏大规模、细粒度的运动数据集。现有数据集通常依赖昂贵的人工标注，严重限制了可扩展性。
- Method: 提出全自动数据构建管道：1）检测和跟踪视频中的物体以提取轨迹；2）利用轨迹和视频帧，结合大语言模型生成细粒度描述和多样化的运动与空间推理问答对。
- Result: 使用该管道生成的数据集微调开源模型（NVILA-Video-15B和Qwen2.5-7B），在运动理解方面取得显著提升，且不影响其他任务性能。模型在多个运动理解数据集和基准测试中超越了Gemini-2.5 Flash和Qwen2.5-VL-72B等强基线。
- Conclusion: FoundationMotion提供了一个可扩展的解决方案，用于构建细粒度运动数据集，有效微调各种模型以增强运动理解和空间推理能力。


### [88] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: BabyVLM-V2是一个基于儿童发展轨迹的视觉语言模型框架，通过纵向多模态预训练数据和认知评估工具箱，实现了高效的小模型预训练，在部分任务上超越GPT-4o。

- Motivation: 早期儿童发展轨迹为视觉基础模型的高效预训练提供了自然目标。研究者希望建立发展合理的视觉基础模型预训练框架，加速相关研究。
- Method: 1. 构建纵向、多方面的预训练集：覆盖婴儿中心视听语料，包含视频-话语、图像-话语和多轮对话数据；2. 开发DevCV工具箱：将NIH Baby Toolbox的视觉相关测量转化为10个多模态任务基准；3. 从零开始预训练紧凑模型。
- Result: 紧凑模型在DevCV工具箱上表现优异，部分任务性能超越GPT-4o。该框架为发展合理的视觉基础模型预训练提供了系统方法。
- Conclusion: BabyVLM-V2框架通过发展合理的设计，为视觉基础模型预训练提供了原则性统一方法，有望加速该领域研究。


### [89] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D是一个可扩展的多视角Transformer，用于度量尺度、密集前馈4D重建，直接生成每像素的运动和几何预测，支持多种模态输入，在精度和计算效率上显著优于现有方法。

- Motivation: 现有方法通常专注于2视角密集场景流或稀疏3D点跟踪，而最近的单目RGB视频4D重建方法无法处理其他模态和传感器数据。需要一种更灵活、更全面的4D重建框架。
- Method: 采用模块化的4D场景表示：将每视角4D预测编码为以局部相机坐标表示的自我中心因子（深度图和相机内参）和以全局世界坐标表示的异我中心因子（相机外参和场景流）。使用可扩展的多视角Transformer架构，支持RGB-D帧、IMU自我运动、雷达多普勒测量等多种模态输入。
- Result: 在多种设置下实现卓越性能：精度提高2-3倍（误差降低2-3倍），计算效率提高15倍（速度快15倍）。
- Conclusion: Any4D提供了一个灵活、高效的4D重建框架，支持多种传感器模态，在精度和速度上显著优于现有方法，为多种下游应用开辟了新途径。


### [90] [GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting](https://arxiv.org/abs/2512.10939)
*Madhav Agarwal,Mingtian Zhang,Laura Sevilla-Lara,Steven McDonagh*

Main category: cs.CV

TL;DR: 提出一种基于3D Morphable Models映射高斯泼溅的方法，通过transformer从音频预测模型参数，实现实时、时序一致的人脸头像生成

- Motivation: 现有语音驱动头像方法存在速度慢或时序不稳定的问题：扩散方法在单样本设置下效果不佳，高斯泼溅方法因面部跟踪不准确或高斯映射不一致导致输出不稳定和视频伪影，限制了实际应用
- Method: 使用3D Morphable Models映射高斯泼溅生成个性化头像，引入基于transformer的模型参数预测器直接从音频驱动时序一致性，仅需单目视频和独立音频输入
- Result: 能够生成实时的说话头像视频，在定量和定性评估中都表现出竞争力
- Conclusion: 该方法解决了现有语音驱动头像方法的时序不稳定问题，实现了高质量、实时的个性化头像生成，具有实际应用价值


### [91] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView是一个统一的4D一致性框架，通过分离空间、时间和视角条件，能够处理多种4D任务（如新视角合成、文本/图像到视频生成等），在多个基准测试中优于特定任务模型。

- Motivation: 现有的相机控制注入方法专注于特定的4D一致性任务子集（如新视角合成、文本到视频等），这些分散的方法在不相交的3D/4D数据上进行训练。需要一个统一的框架来泛化处理广泛的4D一致性任务。
- Method: OmniView分别表示空间、时间和视角条件，使这些输入能够灵活组合。该方法能够从静态、动态和多视角输入合成新视角，在时间上前向和后向外推轨迹，以及从文本或图像提示创建具有完整相机控制的视频。
- Result: 在多个基准测试中与特定任务模型竞争：在LLFF多视角新视角合成数据集上图像质量得分提升33%，在Neural 3D Video动态新视角合成基准上提升60%，在RE-10K静态相机控制上提升20%，在文本条件视频生成中相机轨迹误差减少4倍。
- Conclusion: OmniView展示了通用4D视频模型的可行性，通过一个模型实现强大的泛化能力，统一处理多种4D一致性任务。


### [92] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: Mull-Tokens是一种模态无关的潜在令牌，允许模型在图像和文本模态之间自由思考，以解决多模态推理问题，在空间推理基准上比基线方法平均提升3%。

- Motivation: 现实世界推理需要超越语言的空间、时间、可用性等多模态理解。现有多模态模型依赖专业工具、昂贵的图像生成或手工制作的数据在文本和图像思维间切换，这些方法脆弱且难以扩展。
- Method: 提出Mull-Tokens——模态无关的潜在令牌，预训练用于在图像或文本模态中保存中间信息。首先使用交错的文本-图像轨迹进行监督训练，然后仅使用最终答案进行无监督微调。
- Result: 在四个具有挑战性的空间推理基准测试中（包括解谜和视角转换等任务），Mull-Tokens优于仅使用文本推理或交错图像-文本推理的基线方法，平均提升3%，在推理密集的谜题解决任务上最高提升16%。
- Conclusion: Mull-Tokens为多模态抽象推理提供了一个简单解决方案，促进了文本和视觉推理的融合，使模型能够在多种模态中自由思考以获得正确答案。


### [93] [VL-JEPA: Joint Embedding Predictive Architecture for Vision-language](https://arxiv.org/abs/2512.10942)
*Delong Chen,Mustafa Shukor,Theo Moutakanni,Willy Chung,Jade Yu,Tejaswi Kasarla,Allen Bolourchi,Yann LeCun,Pascale Fung*

Main category: cs.CV

TL;DR: VL-JEPA是一种基于联合嵌入预测架构的视觉语言模型，通过预测目标文本的连续嵌入而非生成token，在减少50%参数的情况下实现更强性能，并支持选择性解码和多种视觉语言任务。

- Motivation: 传统视觉语言模型采用自回归token生成方式，存在计算效率低和参数冗余问题。VL-JEPA旨在通过预测连续嵌入而非token，在抽象表示空间中学习，专注于任务相关语义，同时抽象掉表面语言变异性，提高模型效率和性能。
- Method: VL-JEPA采用联合嵌入预测架构，预测目标文本的连续嵌入而非生成离散token。模型在抽象表示空间中进行学习，推理时仅在需要时调用轻量级文本解码器将预测嵌入转换为文本。该方法支持选择性解码，减少解码操作次数。
- Result: 在相同视觉编码器和训练数据条件下，VL-JEPA比标准token空间VLM训练性能更强，且参数减少50%。选择性解码减少2.85倍解码操作同时保持相似性能。在8个视频分类和8个视频检索数据集上平均性能超越CLIP、SigLIP2和Perception Encoder。在4个VQA数据集上与经典VLM（InstructBLIP、QwenVL）性能相当，仅使用1.6B参数。
- Conclusion: VL-JEPA通过预测连续嵌入而非生成token，在减少参数的同时实现了更强的性能，支持多种视觉语言任务，为高效视觉语言建模提供了新方向。


### [94] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT是一个统一框架，通过引入显式时间戳条件，实现主题驱动视频生成中的细粒度时序控制，能够精确控制多个主题在视频中的出现和消失时间。

- Motivation: 现有主题驱动视频生成方法缺乏对主题出现和消失的细粒度时序控制，这在合成视频、故事板和可控动画等应用中至关重要。需要一种能够精确控制多个主题在视频中时序行为的方法。
- Method: 提出AlcheMinT框架：1）引入新颖的位置编码机制，支持时间间隔编码并与预训练视频生成模型的位置嵌入无缝集成；2）加入主题描述性文本标记，增强视觉身份与视频描述之间的绑定；3）通过标记级连接避免额外交叉注意力模块，参数开销极小。
- Result: 建立了评估多主题身份保持、视频保真度和时序遵循的基准。实验结果表明，AlcheMinT在视觉质量上达到最先进的视频个性化方法水平，同时首次实现了视频中多主题生成的精确时序控制。
- Conclusion: AlcheMinT通过显式时间戳条件和创新的位置编码机制，成功解决了主题驱动视频生成中的时序控制问题，为合成视频、故事板和可控动画等应用提供了强大的工具。


### [95] [MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation](https://arxiv.org/abs/2512.10945)
*Henghui Ding,Chang Liu,Shuting He,Kaining Ying,Xudong Jiang,Chen Change Loy,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出MeViS数据集，专注于基于运动描述的视频对象分割与跟踪，包含大量带运动标注的复杂场景视频，并评估现有方法表现不佳，提出LMPM++方法取得SOTA结果。

- Motivation: 现有视频分割数据集多关注静态属性，忽略了运动信息在视频理解和语言描述中的重要性，需要专门针对运动表达的视频理解数据集。
- Method: 构建MeViS数据集（33,072个运动表达标注，8,171个对象，2,006个复杂场景视频），评估15种现有方法在4个任务上的表现，并提出LMPM++方法用于RVOS/AVOS/RMOT任务。
- Result: 现有方法在运动表达引导的视频理解任务上表现不佳，提出的LMPM++方法在RVOS/AVOS/RMOT任务上取得了新的最佳结果。
- Conclusion: MeViS数据集为运动表达引导的视频理解算法开发提供了重要平台，揭示了现有方法的局限性，提出的LMPM++方法显著提升了性能。


### [96] [Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving](https://arxiv.org/abs/2512.10947)
*Jiawei Yang,Ziyu Chen,Yurong You,Yan Wang,Yiming Li,Yuxiao Chen,Boyi Li,Boris Ivanovic,Marco Pavone,Yue Wang*

Main category: cs.CV

TL;DR: Flex是一种高效的多摄像头场景编码器，使用少量可学习场景token联合编码所有摄像头和时间步的图像信息，无需3D先验，显著提升推理吞吐量和驾驶性能。

- Motivation: 解决端到端自动驾驶中处理多摄像头高容量数据的计算瓶颈问题，挑战现有方法依赖3D先验（如BEV、占用网格等）的假设，寻求更可扩展的解决方案。
- Method: 使用少量可学习场景token联合编码所有摄像头和时间步的图像token，采用几何无关的设计，直接从数据中学习紧凑场景表示，无需3D归纳偏置。
- Result: 在2万小时专有数据集上，Flex实现2.2倍推理吞吐量提升，驾驶性能大幅优于SOTA方法，且场景token展现出无监督场景分解的涌现能力。
- Conclusion: 数据驱动的联合编码策略比依赖3D先验的方法更具可扩展性、高效性和有效性，为未来自动驾驶系统提供了新路径。


### [97] [ClusIR: Towards Cluster-Guided All-in-One Image Restoration](https://arxiv.org/abs/2512.10948)
*Shengkai Hu,Jiaqi Ma,Jun Wan,Wenwen Min,Yongcheng Jing,Lefei Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: ClusIR是一个基于聚类引导的图像修复框架，通过可学习聚类显式建模退化语义，在空间和频域传播聚类感知线索，实现自适应修复。

- Motivation: 现有的一体化图像修复方法通常无法显式建模退化类型，难以适应复杂或混合退化场景，需要更有效的退化感知和自适应修复机制。
- Method: 提出ClusIR框架，包含两个核心组件：概率聚类引导路由机制（PCGRM）和退化感知频率调制模块（DAFMM）。PCGRM将退化识别与专家激活解耦，实现判别性退化感知和稳定路由；DAFMM利用聚类引导先验进行自适应频率分解和针对性调制。
- Result: 在多个基准测试上的广泛实验验证了ClusIR在多种退化场景下达到有竞争力的性能，能够获得显著的修复结果。
- Conclusion: ClusIR通过聚类引导的协同作用，将语义线索与频域调制无缝连接，实现了对各种退化的自适应高质量修复。


### [98] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 首次系统研究RL在文本到3D自回归生成中的应用，提出AR3D-R1模型，通过奖励设计、算法优化和层次化训练提升3D生成质量

- Motivation: RL在2D图像生成中已证明有效，但应用于3D生成仍面临挑战，因为3D对象具有更高的空间复杂性，需要全局一致的几何结构和细粒度局部纹理，对奖励设计和RL算法更敏感
- Method: 1) 评估奖励维度和模型选择；2) 研究GRPO变体，强调token级优化的有效性；3) 引入MME-3DR基准测试；4) 提出Hi-GRPO层次化RL范式，通过专用奖励集成优化全局到局部的3D生成
- Result: 开发了AR3D-R1，这是首个RL增强的文本到3D模型，从粗糙形状到纹理细化都表现出色，证明了RL驱动推理在3D生成中的有效性
- Conclusion: 本研究为RL驱动的3D生成推理提供了重要见解，展示了RL在提升3D生成质量方面的潜力，特别是在处理复杂空间结构和纹理细节方面


### [99] [E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training](https://arxiv.org/abs/2512.10950)
*Qitao Zhao,Hao Tan,Qianqian Wang,Sai Bi,Kai Zhang,Kalyan Sunkavalli,Shubham Tulsiani,Hanwen Jiang*

Main category: cs.CV

TL;DR: E-RayZer是一个自监督的大型3D视觉模型，直接从无标签图像学习真正3D感知的表征，通过显式几何进行自监督3D重建，优于现有方法。

- Motivation: 自监督预训练在语言、2D图像和视频领域取得了革命性进展，但在从多视角图像学习3D感知表征方面仍探索不足。现有方法如RayZer通过潜在空间视角合成间接推断3D，缺乏真正的几何基础。
- Method: E-RayZer直接在3D空间中操作，通过显式几何进行自监督3D重建，消除捷径解。引入细粒度学习课程，从易到难组织训练样本，并以完全无监督方式协调异构数据源。
- Result: E-RayZer在姿态估计上显著优于RayZer，匹配或有时超越全监督重建模型如VGGT。学习到的表征在迁移到3D下游任务时优于DINOv3、CroCo v2、VideoMAE V2和RayZer等领先视觉预训练模型。
- Conclusion: E-RayZer建立了3D感知视觉预训练的新范式，通过显式几何自监督学习实现了真正3D感知的表征，为3D视觉基础模型的发展开辟了新方向。


### [100] [Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration](https://arxiv.org/abs/2512.10954)
*Sicheng Mo,Thao Nguyen,Richard Zhang,Nick Kolkin,Siddharth Srinivasan Iyer,Eli Shechtman,Krishna Kumar Singh,Yong Jae Lee,Bolei Zhou,Yuheng Li*

Main category: cs.CV

TL;DR: 提出Group Diffusion方法，让扩散模型在推理时能够协同生成多个图像，通过跨样本注意力机制提升生成质量

- Motivation: 探索扩散模型推理中未被利用的信号：传统方法独立生成图像，本文研究能否让样本在推理时协同生成
- Method: 提出Group Diffusion，解锁注意力机制使其能在图像间共享（而不仅限于单个图像内部），实现联合去噪，学习图像内和图像间的对应关系
- Result: 观察到明显的缩放效应：更大的组规模产生更强的跨样本注意力和更好的生成质量；在ImageNet-256x256上实现高达32.2%的FID改进
- Conclusion: 跨样本推理是生成建模中一个有效且先前未被探索的机制，GroupDiff揭示了这一潜力


### [101] [Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization](https://arxiv.org/abs/2512.10955)
*Tsai-Shien Chen,Aliaksandr Siarohin,Guocheng Gordon Qian,Kuan-Chieh Jackson Wang,Egor Nemchinov,Moayed Haji-Ali,Riza Alp Guler,Willi Menapace,Ivan Skorokhodov,Anil Kag,Jun-Yan Zhu,Sergey Tulyakov*

Main category: cs.CV

TL;DR: Omni-Attribute：首个开放词汇图像属性编码器，通过语义链接的图像对和双目标训练，学习高保真、属性特定的表示，解决现有方法中视觉因素纠缠和信息泄漏问题。

- Motivation: 现有视觉概念个性化方法依赖通用图像编码器的整体嵌入，这些嵌入纠缠了多个视觉因素（如身份、表情、光照、风格），难以隔离单一属性，导致信息泄漏和不连贯的合成。
- Method: 1. 数据设计：策划带有正负属性标注的语义链接图像对，明确教导编码器保留或抑制什么；2. 模型设计：采用双目标训练范式，平衡生成保真度和对比解缠。
- Result: 生成的嵌入在开放词汇属性检索、个性化和组合生成方面表现有效，在多个基准测试中达到最先进的性能。
- Conclusion: Omni-Attribute通过联合设计数据和模型，成功解决了视觉属性纠缠问题，实现了高保真、属性特定的表示学习，为视觉概念个性化提供了更精确的工具。


### [102] [Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision](https://arxiv.org/abs/2512.10956)
*Wentao Zhou,Xuweiyi Chen,Vignesh Rajagopal,Jeffrey Chen,Rohan Chandra,Zezhou Cheng*

Main category: cs.CV

TL;DR: StereoWalker通过引入立体视觉和显式中层视觉模块（深度估计、密集像素跟踪）来解决机器人导航基础模型在单目视觉下的效率问题，显著减少训练数据需求并提升导航性能。

- Motivation: 当前基于单目视觉的机器人导航基础模型（NFMs）完全忽略中层视觉模块（跟踪、深度估计等），假设视觉能力会隐式涌现，但这需要大量像素到动作的监督数据，在动态非结构化环境中尤其困难。单目视觉存在深度尺度模糊性，限制了准确的空间推理。
- Method: 提出StereoWalker，为NFMs增加立体视觉输入和显式中层视觉模块，包括深度估计和密集像素跟踪。立体视觉解决深度尺度模糊问题，现代中层视觉模型提供可靠的几何和运动结构。同时构建了大规模立体导航数据集，包含从互联网立体视频自动标注的动作信息。
- Result: 中层视觉使StereoWalker仅使用1.5%的训练数据就能达到与最先进方法相当的性能，使用完整数据时超越最先进方法。立体视觉相比单目输入产生更高的导航性能。
- Conclusion: 依赖单目视觉并忽略中层视觉先验是低效的。通过引入立体视觉和显式中层视觉模块，StereoWalker显著提高了机器人导航基础模型的效率和性能，同时构建的数据集将促进未来研究。


### [103] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: SceneMaker是一个解耦的3D场景生成框架，通过分离去遮挡模型和3D物体生成，并改进姿态估计，解决了现有方法在严重遮挡和开放集场景下难以同时生成高质量几何和准确姿态的问题。

- Motivation: 现有方法缺乏足够的开放集去遮挡和姿态估计先验知识，在严重遮挡和开放集设置下难以同时生成高质量几何和准确姿态。
- Method: 1. 将去遮挡模型与3D物体生成解耦，利用图像数据集和收集的去遮挡数据集增强开放集遮挡模式处理能力；2. 提出统一姿态估计模型，集成全局和局部机制的自注意力和交叉注意力；3. 构建开放集3D场景数据集以扩展姿态估计模型的泛化能力。
- Result: 综合实验证明该解耦框架在室内和开放集场景上的优越性，代码和数据集已开源。
- Conclusion: SceneMaker通过解耦去遮挡和3D生成，结合改进的姿态估计和开放集数据集，有效解决了遮挡场景下的3D场景生成问题。


### [104] [WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World](https://arxiv.org/abs/2512.10958)
*Ao Liang,Lingdong Kong,Tianyi Yan,Hongsi Liu,Wesley Yang,Ziqi Huang,Wei Yin,Jialong Zuo,Yixuan Hu,Dekai Zhu,Dongyue Lu,Youquan Liu,Guangfeng Jiang,Linfeng Li,Xiangtai Li,Long Zhuo,Lai Xing Ng,Benoit R. Cottereau,Changxin Gao,Liang Pan,Wei Tsang Ooi,Ziwei Liu*

Main category: cs.CV

TL;DR: WorldLens是一个评估生成世界模型的全方位基准，涵盖生成质量、重建能力、动作跟随、下游任务和人类偏好五个维度，并包含大规模人工标注数据集和评估代理，旨在标准化世界保真度的评估。

- Motivation: 当前生成世界模型能合成逼真的4D驾驶环境，但往往在物理或行为上存在缺陷，且缺乏统一的方法来评估生成世界是否保持几何一致性、遵守物理规律或支持可靠控制。
- Method: 提出WorldLens基准，涵盖五个方面：生成、重建、动作跟随、下游任务和人类偏好；构建WorldLens-26K大规模人工标注视频数据集；开发WorldLens-Agent评估模型，从人工标注中蒸馏学习以实现可扩展、可解释的评分。
- Result: 现有世界模型在五个维度上表现不均衡：纹理强的模型常违反物理规律，几何稳定的模型则缺乏行为保真度。WorldLens生态系统为衡量世界保真度提供了统一框架。
- Conclusion: WorldLens基准、数据集和评估代理构成了衡量世界保真度的统一生态系统，标准化了未来模型的评估标准，不仅关注视觉真实性，更关注行为真实性。


### [105] [StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space](https://arxiv.org/abs/2512.10959)
*Tjark Behrens,Anton Obukhov,Bingxin Ke,Fabio Tosi,Matteo Poggi,Konrad Schindler*

Main category: cs.CV

TL;DR: StereoSpace：基于扩散模型的单目到立体合成框架，仅通过视点条件建模几何，无需显式深度或变形，在端到端协议下在感知舒适度和几何一致性方面优于现有方法。

- Motivation: 现有单目到立体合成方法通常依赖显式深度估计或变形操作，这限制了方法的通用性和鲁棒性。StereoSpace旨在探索仅通过视点条件建模几何的可能性，提供一种无需深度信息的可扩展解决方案。
- Method: 基于扩散模型框架，使用规范校正空间和视点条件引导生成器端到端推断对应关系并填充遮挡区域。引入端到端评估协议，排除测试时使用真实或代理几何估计，专注于下游相关指标。
- Result: StereoSpace在感知舒适度（iSQoE）和几何一致性（MEt3R）方面优于变形填充、潜在变形和变形条件等类别的方法，在分层和非朗伯场景中表现出锐利的视差和强鲁棒性。
- Conclusion: 视点条件扩散模型是单目到立体合成的可扩展、无需深度信息的有效解决方案，为几何建模提供了新的范式。
## cs.RO

### [106] [Design of a six wheel suspension and a three-axis linear actuation mechanism for a laser weeding robot](https://arxiv.org/abs/2512.10319)
*Muhammad Usama,Muhammad Ibrahim Khan,Ahmad Hasan,Muhammad Shaaf Nadeem,Khawaja Fahad Iqbal,Jawad Aslam,Mian Ashfaq Ali,Asad Nisar Awan*

Main category: cs.RO

TL;DR: 本文提出了一种使用低能量激光束进行杂草清除的自主除草机器人，采用六轮设计和新型双四连杆悬挂系统提高稳定性，通过三维线性驱动机制引导激光瞄准杂草，在农田环境中实现了高效的杂草检测和清除。

- Motivation: 传统机械除草在大面积农田中效率低下，而除草剂会破坏土壤生态系统。激光除草作为一种可持续的精准农业替代方案，需要开发能够有效导航农田地形并精确瞄准杂草的自主机器人系统。
- Method: 开发六轮自主除草机器人，采用新型双四连杆悬挂系统提高稳定性。配备三维线性驱动机制引导低能量激光束瞄准检测到的杂草。系统结合杂草检测算法和精确的激光定位技术。
- Result: 机器人能够有效导航农田地形，克服高达15厘米的障碍物。在42.5厘米/秒的最优速度下，杂草检测率达到86.2%，每米操作时间为87秒。激光驱动机制保持1.54毫米的最小平均位置误差，命中率高达97%。
- Conclusion: 该机器人系统在速度、精度和效率方面的优异表现，展示了其在显著提升精准农业实践方面的潜力，为可持续的杂草管理提供了有效的技术解决方案。


### [107] [Evaluating Gemini Robotics Policies in a Veo World Simulator](https://arxiv.org/abs/2512.10675)
*Gemini Robotics Team,Coline Devin,Yilun Du,Debidatta Dwibedi,Ruiqi Gao,Abhishek Jindal,Thomas Kipf,Sean Kirmani,Fangchen Liu,Anirudha Majumdar,Andrew Marmon,Carolina Parada,Yulia Rubanova,Dhruv Shah,Vikas Sindhwani,Jie Tan,Fei Xia,Ted Xiao,Sherry Yang,Wenhao Yu,Allan Zhou*

Main category: cs.RO

TL;DR: 视频生成模型可用于机器人策略评估，包括名义性能、分布外泛化和安全性测试，通过编辑真实场景实现多维度泛化评估

- Motivation: 生成式世界模型在机器人学中具有模拟视觉运动策略与环境交互的潜力，但现有视频模型主要局限于分布内评估，需要扩展到更全面的策略评估场景
- Method: 基于前沿视频基础模型(Veo)构建生成式评估系统，优化支持机器人动作条件化和多视图一致性，集成生成式图像编辑和多视图补全技术，合成真实场景的多维度泛化变体
- Result: 系统保留了视频模型的基础能力，能准确模拟包含新交互对象、新视觉背景和新干扰对象的场景，可预测不同策略在名义和分布外条件下的相对性能，识别不同泛化维度对策略性能的影响，并进行红队测试暴露违反安全约束的行为
- Conclusion: 视频模型可用于机器人策略评估的完整谱系，从名义性能评估到分布外泛化和安全性测试，通过1600+次真实世界评估验证了系统的有效性
## cs.CR

### [108] [Metaphor-based Jailbreaking Attacks on Text-to-Image Models](https://arxiv.org/abs/2512.10766)
*Chenyu Zhang,Yiwen Ma,Lanjun Wang,Wenhui Li,Yi Tu,An-An Liu*

Main category: cs.CR

TL;DR: MJA是一种基于隐喻的越狱攻击方法，通过生成隐喻式对抗提示来绕过文本到图像模型的各种防御机制，无需预先知道防御类型。

- Motivation: 现有越狱攻击方法通常假设攻击者知道部署的防御类型，这限制了它们对未知或多样化防御机制的有效性。需要一种能够有效攻击各种防御机制而无需先验知识的方法。
- Method: MJA包含两个模块：基于LLM的多智能体生成模块(MLAG)和对抗提示优化模块(APO)。MLAG将隐喻式对抗提示生成分解为三个子任务：隐喻检索、上下文匹配和对抗提示生成，并协调三个LLM智能体生成多样化对抗提示。APO训练代理模型预测攻击结果，并设计获取策略自适应识别最优对抗提示。
- Result: 在具有各种外部和内部防御机制的T2I模型上进行广泛实验表明，MJA优于六种基线方法，在更少查询的情况下实现了更强的攻击性能。
- Conclusion: MJA通过隐喻式对抗提示有效攻击多样化的防御机制，无需预先知道防御类型，揭示了T2I模型安全漏洞，并提供了高效的攻击方法。
## cs.AI

### [109] [Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting](https://arxiv.org/abs/2512.09944)
*Moein Heidari,Mohammad Amin Roohi,Armin Khosravi,Ilker Hacihaliloglu*

Main category: cs.AI

TL;DR: Echo-CoPilot是一个基于大语言模型的多视图、多任务智能体，用于协调专门的超声心动图工具，实现临床一致的自动化心脏评估。

- Motivation: 超声心动图是心血管诊疗的核心，但全研究解读仍然是需要人工完成的认知密集型多视图任务。现有的基础模型虽然能在单个感知子任务上表现良好，但缺乏统一的临床连贯评估。
- Method: 引入Echo-CoPilot，这是一个多视图、多任务智能体，使用大语言模型协调一套专门的超声心动图工具。在ReAct式循环中，智能体分解临床医生查询，调用视图识别、心脏结构分割、测量和疾病预测、报告合成等工具，并将输出整合为指南感知的答案和叙述性总结。
- Result: 在公开的MIMIC-EchoQA基准测试中，Echo-CoPilot达到50.8%的准确率，优于通用和生物医学视频视觉语言模型。定性分析显示智能体能利用定量测量和生理学上下文解决临床决策阈值附近的挑战性病例。
- Conclusion: Echo-CoPilot通过大语言模型协调专门工具，实现了超声心动图的多视图、多任务自动化评估，为临床提供统一的指南感知解决方案。


### [110] [Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning](https://arxiv.org/abs/2512.10691)
*Benjamin Gundersen,Nicolas Deperrois,Samuel Ruiperez-Campillo,Thomas M. Sutter,Julia E. Vogt,Michael Moor,Farhad Nooralahzadeh,Michael Krauthammer*

Main category: cs.AI

TL;DR: 该研究探讨了在胸部X光视觉语言模型中结合强化学习和显式推理的效果，发现强化学习能提升报告生成和视觉定位性能，但显式推理未带来额外增益。

- Motivation: 当前医学视觉语言模型主要依赖监督微调，缺乏对答案质量的评估。强化学习能整合任务特定反馈，而显式推理在数学和编程任务中已证明有效，研究者希望探索这两者在医学影像分析中的效果。
- Method: 基于Qwen3-VL构建RadVLM模型，先进行大规模监督微调，然后进行冷启动监督微调以赋予基本推理能力。接着应用基于临床任务特定奖励的GRPO强化学习，在有无推理能力的模型上进行对比实验。
- Result: 强化学习在报告生成和视觉定位任务上均带来额外提升，而显式推理未进一步改善结果。强化学习优化的RadVLM模型在两项任务上均达到最先进性能。
- Conclusion: 强监督微调对基础性能至关重要，但临床对齐的强化学习可作为有效补充。显式推理在医学视觉语言模型中可能不如在数学和编程任务中有效。


### [111] [Agile Deliberation: Concept Deliberation for Subjective Visual Classification](https://arxiv.org/abs/2512.10821)
*Leijie Wang,Otilia Stretcu,Wei Qiao,Thomas Denby,Krishnamurthy Viswanathan,Enming Luo,Chun-Ta Lu,Tushar Dogra,Ranjay Krishna,Ariel Fuxman*

Main category: cs.AI

TL;DR: 提出Agile Deliberation框架，通过概念界定和迭代两个阶段，支持用户在模糊概念下与系统协作训练视觉分类器，相比基线方法提升7.5% F1分数。

- Motivation: 现有的人机协同方法假设用户已有清晰稳定的概念理解，但现实中用户常从模糊概念开始，需要通过"概念审议"过程迭代细化。内容审核专家的实践表明，概念审议是实际工作中必要的认知过程。
- Method: Agile Deliberation框架包含两个阶段：1) 概念界定 - 将初始概念分解为结构化子概念层次；2) 概念迭代 - 展示语义边界案例供用户反思和反馈，迭代对齐图像分类器与用户意图。框架基于真实内容审核员的审议策略设计。
- Result: 通过18个1.5小时用户会话评估，Agile Deliberation比自动分解基线提高7.5% F1分数，比手动审议提高3%以上。参与者报告概念理解更清晰，认知负担更低。
- Conclusion: Agile Deliberation有效支持用户从模糊概念开始迭代细化，通过结构化概念分解和边界案例反馈，显著提升分类器性能并降低用户认知负担，适用于主观和演化概念的场景。
## cs.LG

### [112] [CC-GRMAS: A Multi-Agent Graph Neural System for Spatiotemporal Landslide Risk Assessment in High Mountain Asia](https://arxiv.org/abs/2510.20875)
*Mihir Panchal,Ying-Jung Chen,Surya Parkash*

Main category: cs.LG

TL;DR: CC-GRMAS是一个利用卫星观测和环境信号提高滑坡预测准确性的多智能体框架，包含预测、规划和执行三个协同模块，为脆弱山区提供可扩展的气候韧性灾害准备方案。

- Motivation: 滑坡是日益严重的气候诱发灾害，在高亚洲山区造成严重的环境和人类后果。尽管卫星和时序数据日益丰富，但及时的滑坡检测和灾害响应仍然不完善且分散，需要更有效的解决方案。
- Method: 提出CC-GRMAS框架，利用一系列卫星观测和环境信号，构建三个相互关联的智能体：预测、规划和执行。这些智能体协同工作，实现实时态势感知、响应规划和干预，并整合当地环境因素和多智能体协调机制。
- Result: 该框架提高了滑坡预测的准确性，能够实现实时态势感知、响应规划和干预，为脆弱山区提供了可扩展的主动式气候韧性灾害准备解决方案。
- Conclusion: CC-GRMAS通过整合卫星数据、环境信号和多智能体协调，为高亚洲山区等脆弱地形提供了一个可扩展的、主动的滑坡灾害管理框架，有助于增强气候韧性灾害准备能力。


### [113] [Federated Domain Generalization with Latent Space Inversion](https://arxiv.org/abs/2512.10224)
*Ragja Palakkadavath,Hung Le,Thanh Nguyen-Tang,Svetha Venkatesh,Sunil Gupta*

Main category: cs.LG

TL;DR: 提出一种新的联邦域泛化方法，通过潜在空间反转技术保护隐私，并使用重要权重聚合策略处理非独立同分布客户端数据，在减少通信开销的同时实现更好的泛化性能。

- Motivation: 现有联邦域泛化方法在提升全局模型泛化能力时，通常需要共享客户端数据的统计信息，这会危及数据隐私。同时，当客户端数据非独立同分布时，传统的模型聚合方法可能会丢弃重要的本地适应信息。
- Method: 1. 潜在空间反转技术：在本地客户端训练中强制实现跨域不变性，增强隐私保护；2. 重要权重聚合策略：在模型聚合时优先考虑对本地模型预测有显著影响的参数，保留重要的本地适应信息。
- Result: 实验表明，该方法在减少通信开销的同时，取得了优于现有最先进方法的性能，在联邦域泛化任务中表现出色。
- Conclusion: 提出的联邦域泛化方法通过潜在空间反转和重要权重聚合，在保护隐私的同时有效处理非独立同分布客户端数据，实现了更好的泛化性能和更低的通信成本。


### [114] [Mode-Seeking for Inverse Problems with Diffusion Models](https://arxiv.org/abs/2512.10524)
*Sai Bharath Chandra Gutha,Ricardo Vinuesa,Hossein Azizpour*

Main category: cs.LG

TL;DR: 提出VML损失函数，通过最小化扩散后验与测量后验的KL散度，引导生成样本向MAP估计收敛，无需任务特定训练，在计算效率和性能上优于现有方法。

- Motivation: 现有基于预训练无条件扩散模型的后验采样和MAP估计方法存在建模近似和计算复杂度高的问题，需要更高效准确的逆问题求解方法。
- Method: 提出变分模式寻求损失(VML)，在反向扩散的每一步最小化扩散后验p(x₀|xₜ)与测量后验p(x₀|y)的KL散度，推导出VML-MAP算法，特别针对线性逆问题有解析解。
- Result: 在多个数据集上的多种图像恢复任务中，VML-MAP在性能和计算时间上均优于现有方法，验证了其有效性。
- Conclusion: VML提供了一种无需任务特定训练的高效逆问题求解框架，通过理论推导和实验验证，在准确性和计算效率方面均有显著提升。


### [115] [Interpretable and Steerable Concept Bottleneck Sparse Autoencoders](https://arxiv.org/abs/2512.10805)
*Akshay Kulkarni,Tsui-Wei Weng,Vivek Narayanaswamy,Shusen Liu,Wesam A. Sakla,Kowshik Thopalli*

Main category: cs.LG

TL;DR: 该论文提出了概念瓶颈稀疏自编码器（CB-SAE），通过修剪低效用神经元并添加轻量级概念瓶颈来提升稀疏自编码器的可解释性和可操控性。

- Motivation: 稀疏自编码器（SAEs）在大型语言模型和视觉语言模型中具有机制可解释性、概念发现和模型操控的潜力，但现有SAEs存在两个问题：1）大多数神经元要么可解释性低，要么可操控性低，或两者都低；2）由于无监督学习性质，用户期望的概念往往不在学习到的字典中，限制了实际应用。
- Method: 提出概念瓶颈稀疏自编码器（CB-SAE）框架：1）引入两个计算成本低的可解释性和可操控性度量指标；2）修剪低效用神经元；3）在潜在空间中添加与用户定义概念集对齐的轻量级概念瓶颈。
- Result: CB-SAE在视觉语言模型和图像生成任务中，可解释性提升32.1%，可操控性提升14.5%。
- Conclusion: CB-SAE通过后处理框架有效解决了SAEs在可解释性和可操控性方面的局限性，为机制可解释性和模型操控提供了更实用的解决方案。


### [116] [Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values](https://arxiv.org/abs/2512.10817)
*Brian P. Powell,Jordan A. Caraballo-Vega,Mark L. Carroll,Thomas Maxwell,Andrew Ptak,Greg Olmschenk,Jorge Martinez-Palomera*

Main category: cs.LG

TL;DR: 该论文发现二进制编码能使神经网络在训练范围外外推周期函数，提出了归一化二进制编码(NB2E)方法，使普通MLP能成功外推各种周期信号。

- Motivation: 传统神经网络在训练范围外外推周期函数存在困难，需要探索新的输入编码方法来提升神经网络的泛化能力和外推性能。
- Method: 提出了归一化二进制编码(NB2E)方法，将连续数值编码为二进制形式，使用普通多层感知机(MLP)进行训练和测试。
- Result: NB2E编码使MLP能成功外推各种周期信号，无需事先知道函数形式。内部激活分析显示NB2E诱导了位相表示，使MLP能独立于位置学习和外推信号结构。
- Conclusion: 二进制编码为神经网络外推周期函数提供了有效方法，NB2E编码通过诱导位相表示使MLP具备超越训练范围的外推能力。


### [117] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的点状函数Derf（erf函数变体），用于替代传统的归一化层，在多种任务中超越了LayerNorm、RMSNorm和DyT等现有方法。

- Motivation: 尽管归一化层长期以来被视为深度学习架构中不可或缺的组件，但最近提出的Dynamic Tanh（DyT）表明存在替代方案。DyT通过约束极端值实现稳定收敛并达到归一化级别的性能，本研究旨在寻找能够超越DyT的函数设计。
- Method: 首先研究点状函数的内在特性如何影响训练和性能，然后基于这些发现进行大规模的函数设计搜索。通过探索，引入了Derf函数：Derf(x) = erf(αx + s)，其中erf(x)是重新缩放的高斯累积分布函数，并确定其为性能最佳的设计。
- Result: Derf在多种领域超越了LayerNorm、RMSNorm和DyT，包括视觉（图像识别和生成）、语音表示和DNA序列建模。性能提升主要源于改进的泛化能力而非更强的拟合能力。
- Conclusion: Derf的简单性和更强的性能使其成为无归一化Transformer架构的实用选择，为深度学习架构设计提供了新的方向。


### [118] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: BiFlow提出了一种双向归一化流框架，通过近似逆映射替代精确解析逆，解决了因果解码瓶颈，在ImageNet上实现了生成质量提升和采样速度的显著加速。

- Motivation: 传统归一化流（NFs）需要精确的解析逆变换，这限制了模型架构的灵活性。最近基于Transformer的自回归流方法（如TARFlow）虽然复兴了NFs，但暴露了因果解码作为主要瓶颈的问题。需要一种方法既能保持NFs的理论优势，又能克服这些限制。
- Method: 提出BiFlow框架，它不需要精确的解析逆变换。BiFlow学习一个反向模型来近似底层噪声到数据的逆映射，这使得可以使用更灵活的损失函数和架构。通过双向学习过程，解决了传统NFs的因果解码瓶颈。
- Result: 在ImageNet上的实验表明，BiFlow相比因果解码对应方法，在生成质量上有所提升，同时将采样速度加速了高达两个数量级。BiFlow在基于NF的方法中达到了最先进的结果，在单次评估（"1-NFE"）方法中具有竞争力。
- Conclusion: BiFlow通过移除精确解析逆的要求，为归一化流提供了更灵活的框架，解决了因果解码瓶颈，显著提升了采样效率。这项工作有望进一步推动这一经典范式的发展。
