[[toc]]

## cs.CV

### [1] [iFlyBot-VLA Technical Report](https://arxiv.org/abs/2511.01914)
*Yuan Zhang,Chenyu Xue,Wenjie Xu,Chao Ji,Jiajia wu,Jia Pan*

Main category: cs.CV

TL;DR: iFlyBot-VLA是一个大规模视觉-语言-动作模型，采用双级动作表示框架和混合训练策略，在机器人操作任务中表现出色

- Motivation: 开发一个能够有效整合视觉、语言和动作表示的VLA模型，提升机器人在复杂操作任务中的表现
- Method: 使用潜在动作模型和结构化离散动作令牌的双级动作表示框架，结合机器人轨迹数据和通用QA数据集进行混合训练
- Result: 在LIBERO Franka基准测试中表现优异，在真实世界多样化挑战性操作任务中达到有竞争力的成功率
- Conclusion: iFlyBot-VLA通过双级动作表示和混合训练策略成功实现了视觉、语言和动作表示的协同对齐，为机器人操作任务提供了有效的解决方案


### [2] [Challenging DINOv3 Foundation Model under Low Inter-Class Variability: A Case Study on Fetal Brain Ultrasound](https://arxiv.org/abs/2511.01915)
*Edoardo Conti,Riccardo Rosati,Lorenzo Federici,Adriano Mancini,Maria Chiara Fiorentin*

Main category: cs.CV

TL;DR: 本研究首次全面评估了基础模型在胎儿超声成像中低类间变异性条件下的表现。研究发现，在胎儿大脑标准切面（TT、TV、TC）这种解剖结构高度相似的场景下，领域特定的预训练比通用基础模型表现更优。

- Motivation: 虽然近期视觉基础模型（如DINOv3）在医学领域展现出良好的迁移能力，但其在解剖结构相似情况下的区分能力尚未得到系统研究。本研究针对胎儿大脑标准切面这种解剖特征高度重叠的场景，填补了这一研究空白。
- Method: 收集并整合所有公开可用的胎儿超声数据集，构建了包含18.8万张标注图像的FetalUS-188K多中心基准数据集。采用自监督预训练学习超声感知表示，并通过线性探测和全微调两种标准化适应协议进行评估。
- Result: 在胎儿超声数据上预训练的模型始终优于基于自然图像初始化的模型，加权F1分数提升高达20%。领域自适应预训练使网络能够保留区分中间切面（如TV）所需的细微回声和结构线索。
- Conclusion: 通用基础模型在低类间变异性条件下泛化能力不足，而领域特定的预训练对于在胎儿大脑超声成像中获得稳健且临床可靠的表示至关重要。


### [3] [Assessing the value of Geo-Foundational Models for Flood Inundation Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for end-users](https://arxiv.org/abs/2511.01990)
*Saurabh Kaushik,Lalit Maurya,Elizabeth Tellman,ZhiJie Zhang*

Main category: cs.CV

TL;DR: Geo-Foundational Models (GFMs)在洪水淹没制图方面与传统U-Net模型相比，提供了小幅到中等的精度提升，同时具有更低的计算成本和标注工作量。Clay模型在多个传感器上表现最佳，特别是在小样本学习中优势明显。

- Motivation: 尽管GFMs在从卫星图像中提取时空信息方面具有潜力，但目前尚不清楚它们是否优于传统模型如U-Net，缺乏跨传感器和数据可用性场景的系统比较，这对指导最终用户模型选择至关重要。
- Method: 评估了三种GFMs（Prithvi 2.0、Clay V1.5、DOFA）和UViT，与TransNorm、U-Net和Attention U-Net进行比较，使用PlanetScope、Sentinel-1和Sentinel-2数据，采用留一区域交叉验证和少样本实验。
- Result: 所有GFMs表现竞争性，性能差异仅2-5%。Clay在PlanetScope（0.79 mIoU）和Sentinel-2（0.70）上表现最佳，Prithvi在Sentinel-1（0.57）上领先。Clay在少样本学习中表现突出，仅用5张训练图像就达到0.64 mIoU。Clay模型大小仅26M参数，比Prithvi快3倍，比DOFA快2倍。
- Conclusion: 与先前发现相反，GFMs在洪水制图精度上相比传统U-Net提供了小幅到中等的改进，同时具有更低的计算成本和标注工作量，Clay是计算效率最高的选择。


### [4] [Locally-Supervised Global Image Restoration](https://arxiv.org/abs/2511.01998)
*Benjamin Walder,Daniel Toader,Robert Nuster,Günther Paltauf,Peter Burgholzer,Gregor Langer,Lukas Krainer,Markus Haltmeier*

Main category: cs.CV

TL;DR: 提出一种基于学习框架的图像重建方法，用于处理固定确定性采样模式下的不完整测量数据重建问题，包括上采样和修复。

- Motivation: 传统监督方法需要完整采样数据，自监督方法依赖随机采样但期望覆盖整个图像。本文针对固定确定性采样模式，即使期望下也无法完全覆盖的情况，寻求解决方案。
- Method: 利用底层图像分布的多种不变性，理论上达到与完全监督方法相同的重建性能。在光分辨率图像上采样（PAM）中验证方法。
- Result: 在光声显微镜图像上采样任务中，展示了竞争性或更优的结果，同时需要显著减少的ground truth数据。
- Conclusion: 该方法能够在不完整采样模式下实现高质量图像重建，减少了对完整ground truth数据的依赖。


### [5] [Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images](https://arxiv.org/abs/2511.02014)
*Tuan Truong,Guillermo Jimenez Perez,Pedro Osorio,Matthias Lenga*

Main category: cs.CV

TL;DR: 本文系统评估了三种大型多模态模型在医疗图像中保护健康信息检测的性能，发现LMM在OCR准确性上优于传统方法，但OCR性能提升并不总能转化为更好的PHI检测效果。

- Motivation: 医疗图像中的保护健康信息检测对保护患者隐私和遵守法规至关重要，传统方法主要依赖OCR和命名实体识别，而新兴的大型多模态模型为文本提取和语义分析提供了新的机会。
- Method: 系统评估了GPT-4o、Gemini 2.5 Flash和Qwen 2.5 7B三种LMM，采用两种管道配置：纯文本分析和OCR+语义分析集成。
- Result: LMM在OCR效果上优于传统模型，但在复杂印记模式测试案例中表现最佳。当文本区域可读性好且使用强LMM进行文本分析时，不同管道配置结果相似。
- Conclusion: 提供了基于实证的LMM选择建议和可扩展模块化基础设施的部署策略，强调需要根据具体操作约束选择合适模型。


### [6] [StrengthSense: A Dataset of IMU Signals Capturing Everyday Strength-Demanding Activities](https://arxiv.org/abs/2511.02027)
*Zeyu Yang,Clayton Souza Leite,Yu Xiao*

Main category: cs.CV

TL;DR: StrengthSense是一个包含11种力量需求活动和2种非力量需求活动的开放IMU数据集，使用10个IMU传感器从29名健康受试者收集，并通过视频记录进行标注验证。

- Motivation: 目前缺乏捕捉力量需求活动的全面可穿戴传感器数据集，这限制了肌肉力量、耐力和功率监测的研究和应用发展。
- Method: 使用10个IMU传感器放置在四肢和躯干上，从29名健康受试者收集数据，通过视频记录进行标注，并比较IMU估计的关节角度与视频直接提取的角度以验证数据准确性。
- Result: 创建了一个包含11种力量需求活动（如坐起、爬楼梯、拖地等）和2种非力量需求活动的综合数据集，技术验证显示IMU数据具有准确性和可靠性。
- Conclusion: StrengthSense数据集可用于推进人类活动识别算法开发，创建健身和健康监测应用，填补了力量需求活动数据集的空白。


### [7] [Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis](https://arxiv.org/abs/2511.02046)
*Soham Joshi,Shwet Kamal Mishra,Viswanath Gopalakrishnan*

Main category: cs.CV

TL;DR: 提出一个自动化合成文本VQA数据集的端到端流水线，利用OCR、ROI检测、图像描述生成和问题生成等技术，无需人工标注即可生成约72K个QA对。

- Motivation: 传统文本VQA数据集创建需要大量人工标注，耗时且困难。随着多模态基础模型和OCR技术的成熟，需要建立自动化合成QA对的流水线来扩展数据集规模。
- Method: 整合OCR文本检测识别、感兴趣区域检测、图像描述生成和问题生成等多个模型和算法，构建端到端的自动化流水线来合成和验证QA对。
- Result: 成功构建了首个自动化合成的大规模文本VQA数据集，包含约44K张图像和72K个QA对。
- Conclusion: 该方法能够高效、可扩展地自动合成高质量的文本VQA数据集，为相关研究提供了新的数据生成途径。


### [8] [Markerless Augmented Reality Registration for Surgical Guidance: A Multi-Anatomy Clinical Accuracy Study](https://arxiv.org/abs/2511.02086)
*Yue Yang,Fabian Necker,Christoph Leuze,Michelle Chen,Andrey Finegersh,Jake Lee,Vasu Divi,Bruce Daniel,Brian Hargreaves,Jie Ying Wu,Fred M Baik*

Main category: cs.CV

TL;DR: 开发并临床评估了基于深度传感器的无标记增强现实配准系统，在头戴显示器上实现小尺寸或低曲率解剖结构的术中定位，临床中位误差约3-4毫米。

- Motivation: 解决在小型或低曲率解剖结构上实现无标记增强现实配准的挑战，提升临床手术导航的准确性和实用性。
- Method: 在HoloLens 2上，通过深度偏差校正、人工初始化引导和全局-局部配准，将手部追踪深度数据与CT皮肤网格对齐，并在术中验证。
- Result: 临床验证显示：足部中位误差3.2毫米，耳部4.3毫米，小腿5.3毫米；5毫米覆盖率达72-95%，接近中等风险手术的临床误差阈值。
- Conclusion: 基于深度传感器的无标记AR配准系统在活体手术中实现了接近临床要求的精度，人工引导初始化和全局-局部配准策略有效提升了小目标配准的准确性。


### [9] [From Instance Segmentation to 3D Growth Trajectory Reconstruction in Planktonic Foraminifera](https://arxiv.org/abs/2511.02142)
*Huahua Lin,Xiaohao Cai,Mark Nixon,James M. Mulqueeney,Thomas H. G. Ezard*

Main category: cs.CV

TL;DR: 提出首个全自动的浮游有孔虫生长分析流程，结合实例分割和腔室排序算法，从CT扫描中重建三维生长轨迹，显著减少人工工作量。

- Motivation: 浮游有孔虫是重要的环境指示生物，但其腔室生长轨迹分析依赖耗时且主观的人工分割，需要自动化解决方案。
- Method: 端到端流程：使用实例分割技术识别腔室，结合专用腔室排序算法重建三维生长轨迹；评估多种针对不同腔室空间特征的实例分割方法。
- Result: 在专家标注数据集上验证，流程大幅减少人工工作量，保持生物学意义的准确性；腔室排序算法对分割错误具有鲁棒性，即使部分分割也能一致重建发育轨迹。
- Conclusion: 建立了首个完全自动化和可重现的数字有孔虫生长分析流程，为大规模数据驱动的生态研究奠定基础。


### [10] [Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis](https://arxiv.org/abs/2511.02144)
*Zhicheng Wang,Junbiao Pang*

Main category: cs.CV

TL;DR: 提出了一种结合PCA和RPCA的级联框架，用于从数字图像中高效提取路面裂缝宽度，解决了裂缝边界复杂形态和快速测量需求的问题。

- Motivation: 准确量化路面裂缝宽度对评估结构完整性和指导维护至关重要，但传统方法在复杂裂缝边界形态和快速测量需求方面存在局限。
- Method: 三阶段级联框架：1) 使用现有检测算法进行初始裂缝分割生成二值图像；2) 通过PCA确定准平行裂缝的主方向轴；3) 使用RPCA提取不规则裂缝几何的主传播轴。
- Result: 在三个公开数据集上的综合评估表明，该方法在计算效率和测量精度方面均优于现有最先进技术。
- Conclusion: 提出的PCA-RPCA级联框架能够有效解决路面裂缝宽度测量的挑战，为路面状况评估提供了高效准确的解决方案。


### [11] [Autobiasing Event Cameras for Flickering Mitigation](https://arxiv.org/abs/2511.02180)
*Mehdi Sefidgar Dilmaghani,Waseem Shariff,Cian Ryan,Joe Lemley,Peter Corcoran*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机偏置自动调节的闪烁抑制方法，使用CNN识别闪烁并动态调整偏置，在25-500Hz频率范围内有效减少闪烁影响

- Motivation: 解决事件相机在不同环境中因光强快速变化产生的闪烁问题，提升事件相机性能
- Method: 利用事件相机固有偏置设置，通过简单CNN在空间域识别闪烁实例，并动态调整特定偏置来最小化闪烁影响
- Result: 在光照良好和低光条件下测试，YOLO人脸检测置信度提升，检测到人脸的帧比例增加，平均梯度（闪烁指示器）在光照良好和低光条件下分别降低38.2%和53.6%
- Conclusion: 该方法能显著改善事件相机在不利光照场景下的功能，无需额外硬件或软件滤波


### [12] [Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models](https://arxiv.org/abs/2511.02182)
*Jinhwan Seo,Yoonki Cho,Junhyug Noh,Sung-eui Yoon*

Main category: cs.CV

TL;DR: 提出了一个用于解决Grounded Video Question Answering任务的三阶段框架，通过引入触发时刻概念来改进目标对象的时空定位和跟踪性能。

- Motivation: GVQA任务需要强大的多模态模型来处理视频内容推理、答案视觉定位和对象时间跟踪，现有方法在这些方面存在不足。
- Method: 将GVQA任务分解为三个阶段的流水线：视频推理与问答、时空定位和跟踪，关键创新是引入基于CORTEX提示的触发时刻来定位目标对象最清晰的帧作为锚点。
- Result: 在GVQA任务上获得了0.4968的HOTA分数，相比去年获胜分数0.2704有显著提升。
- Conclusion: 提出的三阶段框架和触发时刻概念有效提升了GVQA任务的性能，证明了该方法在复杂视频推理和对象跟踪方面的有效性。


### [13] [MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation](https://arxiv.org/abs/2511.02193)
*Jiawen Liu,Yuanbo Zeng,Jiaming Liang,Yizhen Yang,Yiheng Zhang,Enhui Cai,Xiaoqi Sheng,Hongmin Cai*

Main category: cs.CV

TL;DR: 提出MM-UNet架构用于视网膜血管分割，通过形态Mamba卷积层和反向选择性状态引导模块提升对细分支结构的感知能力和边界检测精度，在两个公开数据集上取得显著性能提升。

- Motivation: 视网膜血管具有极细分支结构且形态变化大，传统分割方法在精度和鲁棒性方面面临挑战，需要专门针对血管拓扑特征设计的解决方案。
- Method: 提出MM-UNet架构，包含形态Mamba卷积层（替代点卷积增强分支拓扑感知）和反向选择性状态引导模块（结合反向引导理论和状态空间建模提升边界感知和解码效率）。
- Result: 在DRIVE和STARE两个公开数据集上，相比现有方法分别获得1.64%和1.25%的F1分数提升，证明了方法的有效性。
- Conclusion: MM-UNet通过专门针对视网膜血管特征设计的架构，在分割精度和鲁棒性方面表现出色，为眼科疾病诊断提供了更可靠的血管形态分析工具。


### [14] [Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers](https://arxiv.org/abs/2511.02206)
*Zhengjie Zhang,Xiaoxie Mao,Qihao Guo,Shaoting Zhang,Qi Huang,Mu Zhou,Fang Xie,Mianxin Liu*

Main category: cs.CV

TL;DR: 该研究开发了一种基于语言增强的生成模型，能够从血液生物标志物和MRI扫描中合成淀粉样蛋白PET图像，用于阿尔茨海默病的诊断。

- Motivation: 阿尔茨海默病诊断依赖淀粉样蛋白PET成像，但该技术成本高且可及性有限。研究旨在探索是否可以从血液生物标志物和MRI扫描中预测淀粉样蛋白PET的空间模式。
- Method: 收集了566名参与者的淀粉样蛋白PET图像、T1加权MRI扫描和血液生物标志物。开发了基于大语言模型和多模态信息融合的语言增强生成模型来合成PET图像。
- Result: 合成的PET图像在结构细节和区域模式上与真实PET扫描高度相似。基于合成PET的诊断结果与真实PET诊断具有高度一致性。合成PET模型在诊断性能上优于基于T1和血液生物标志物的模型。
- Conclusion: 语言增强生成模型能够合成逼真的PET图像，增强了MRI和血液生物标志物在淀粉样蛋白空间模式评估中的实用性，改善了阿尔茨海默病的诊断流程。


### [15] [Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping](https://arxiv.org/abs/2511.02207)
*Jiajia Li,Keyi Zhu,Qianwen Zhang,Dong Chen,Qi Sun,Zhaojian Li*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅的对象中心3D重建框架，结合SAM-2分割和背景掩码技术，实现草莓植物的精准重建和表型性状自动测量。

- Motivation: 传统植物表型分析方法耗时费力且具有破坏性，而现有3D重建方法包含背景噪声，影响下游性状分析的准确性和效率。
- Method: 使用SAM-2模型进行语义分割和alpha通道背景掩码，结合3D高斯泼溅技术进行对象中心3D重建，通过DBSCAN聚类和PCA自动测量植物高度和冠层宽度。
- Result: 实验结果表明该方法在准确性和效率上优于传统流程，实现了更精确的几何表示并显著减少计算时间。
- Conclusion: 该方法为草莓植物表型分析提供了可扩展、非破坏性的解决方案，能够自动估计重要植物性状。


### [16] [Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning](https://arxiv.org/abs/2511.02210)
*Anders Austlid Taskén,Thierry Judge,Erik Andreas Rye Berg,Jinyang Yu,Bjørnar Grenne,Frank Lindseth,Svend Aakhus,Pierre-Marc Jodoin,Nicolas Duchateau,Olivier Bernard,Gabriel Kiss*

Main category: cs.CV

TL;DR: 本研究开发了首个自动化的左心室节段纵向应变估计管道autoStrain，使用深度学习方法进行运动估计，在经食管超声心动图中实现了准确的应变测量。

- Motivation: 当前应变估计技术需要大量手动干预和专业知识，限制了效率和监测应用。需要自动化解决方案来提高心脏功能评估的精确性和效率。
- Method: 提出了两种深度学习方法：基于RAFT光流模型的TeeFlow（密集帧间预测）和基于CoTracker点轨迹模型的TeeTracker（稀疏长序列预测）。使用SIMUS模拟管道生成80例患者的合成TEE数据集进行训练和评估。
- Result: TeeTracker在运动估计精度上优于TeeFlow，在合成TEE测试数据集上的平均距离误差为0.65mm。在16例患者的临床验证中，autoStrain管道与临床参考值一致，平均差异为1.09%。
- Conclusion: 将AI驱动的运动估计与TEE结合可以显著提高临床环境中心脏功能评估的精确性和效率。


### [17] [Can Foundation Models Revolutionize Mobile AR Sparse Sensing?](https://arxiv.org/abs/2511.02215)
*Yiqin Zhao,Tian Guo*

Main category: cs.CV

TL;DR: 本文探讨了基础模型如何改变移动稀疏感知的格局，通过几何感知图像扭曲技术显著提升跨帧信息重用的准确性，并在3D场景重建中展现领先性能。

- Motivation: 移动感知系统长期面临感知质量与效率之间的权衡，传统稀疏感知方法因跨时空信息缺失导致精度下降，需要探索基础模型能否解决这一问题。
- Method: 使用真实世界移动AR数据，评估基础模型在几何感知图像扭曲方面的表现，这是实现跨帧信息准确重用的核心技术。
- Result: 基础模型在几何感知图像扭曲方面提供显著改进，基于基础模型的稀疏感知具有可扩展性，在3D场景重建中表现领先。
- Conclusion: 研究揭示了基础模型集成到移动稀疏感知系统中的前景和挑战，表明基础模型能够改变移动稀疏感知的现状。


### [18] [Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2511.02228)
*Delin Ma,Menghui Zhou,Jun Qi,Yun Yang,Po Yang*

Main category: cs.CV

TL;DR: 提出了一种用于阿尔茨海默病诊断的协作注意力和一致性引导融合框架，通过多模态MRI和PET神经影像融合来提升早期诊断性能。

- Motivation: 现有多模态融合方法主要关注跨模态互补性，但忽略了模态特定特征的重要性，且模态间的分布差异会导致有偏和噪声表示，影响分类性能。
- Method: 使用可学习参数表示块补偿缺失模态信息，结合共享编码器和模态独立编码器保留共享和特定表示，并采用一致性引导机制显式对齐跨模态潜在分布。
- Result: 在ADNI数据集上的实验结果表明，该方法相比现有融合策略实现了更优越的诊断性能。
- Conclusion: 该协作注意力和一致性引导融合框架能够有效整合MRI和PET的多模态信息，在阿尔茨海默病诊断中表现出色。


### [19] [Monocular absolute depth estimation from endoscopy via domain-invariant feature learning and latent consistency](https://arxiv.org/abs/2511.02247)
*Hao Li,Daiwei Lu,Jesse d'Almeida,Dilara Isik,Ehsan Khodapanah Aghdam,Nick DiSanto,Ayberk Acar,Susheela Sharma,Jie Ying Wu,Robert J. Webster III,Ipek Oguz*

Main category: cs.CV

TL;DR: 提出了一种潜在特征对齐方法，通过对抗学习和方向特征一致性来减少内窥镜深度估计中的域差距，从而改进绝对深度估计。

- Motivation: 内窥镜手术中获取绝对深度信息困难，现有无监督域适应方法在图像风格转换后仍存在域差距，限制了深度网络的监督学习效果。
- Method: 使用潜在特征对齐方法，通过对抗学习和方向特征一致性学习域不变特征，深度网络同时处理转换后的合成图像和真实内窥镜图像。
- Result: 在中央气道内窥镜视频上评估，相比最先进的单目深度估计方法，在绝对和相对深度指标上均取得更优性能，并在不同骨干网络和预训练权重上表现一致。
- Conclusion: 该方法能够有效减少内窥镜深度估计中的域差距，提升绝对深度估计精度，且对图像转换过程具有不可知性。


### [20] [Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework](https://arxiv.org/abs/2511.02271)
*Yucheng Song,Yifan Ge,Junhao Li,Zhining Liao,Zhifang Liao*

Main category: cs.CV

TL;DR: 提出HTSC-CIF框架，通过分层任务分解解决医学报告生成中的三个关键挑战：领域知识不足、跨模态对齐差和伪相关性问题。

- Motivation: 医学报告生成(MRG)在自动生成放射学图像报告方面面临三个主要挑战：领域知识理解不足、文本-视觉实体嵌入对齐差以及跨模态偏差导致的伪相关性。现有工作只解决单个挑战，本文旨在同时解决所有三个挑战。
- Method: HTSC-CIF框架将三个挑战分解为低、中、高三个层次任务：1)低层：对齐医学实体特征与空间位置以增强视觉编码器的领域知识；2)中层：使用前缀语言建模和掩码图像建模通过相互指导提升跨模态对齐；3)高层：通过前门干预的跨模态因果干预模块减少混杂因素并提高可解释性。
- Result: 大量实验证实HTSC-CIF的有效性，显著优于最先进的MRG方法。
- Conclusion: HTSC-CIF框架通过分层任务分解成功解决了医学报告生成中的三个关键挑战，在性能上超越了现有最佳方法。


### [21] [Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?](https://arxiv.org/abs/2511.02277)
*Giorgos Sfikas,Konstantina Nikolaidou,Foteini Papadopoulou,George Retsinas,Anastasios L. Kesidis*

Main category: cs.CV

TL;DR: 本文探讨了使用欧拉角参数化作为归一化流模型基础进行姿态估计的实用性，尽管欧拉角存在缺点，但在某些方面可能比复杂参数化更有用。

- Motivation: 姿态估计在3D计算机视觉中很重要，虽然单点估计通常足够，但概率姿态输出在姿态不明确时（由于传感器限制或物体对称性）具有优势。
- Method: 使用欧拉角参数化作为归一化流模型的基础来构建姿态估计模型。
- Result: 欧拉角参数化尽管有其缺点，但在某些方面可能产生有用的模型。
- Conclusion: 欧拉角参数化在姿态估计中可能是一个实用的选择，特别是在构建概率模型时，相比更复杂的参数化方法具有优势。


### [22] [SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning](https://arxiv.org/abs/2511.02280)
*Fangxun Shu,Yongjie Ye,Yue Liao,Zijian Kang,Weijie Yin,Jiacong Wang,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-RL是一个强化学习后训练框架，通过教导多模态大语言模型何时以及如何思考来增强其推理能力，采用双重奖励系统评估推理质量和自适应判断，显著减少幻觉并提升性能。

- Motivation: 现有方法存在两个主要限制：仅基于结果监督（只奖励正确答案而不确保推理质量）和统一的思考策略（在简单任务上过度思考，在复杂任务上思考不足）。
- Method: 提出双重奖励系统：思考奖励（通过事实基础、逻辑一致性和答案一致性评估推理质量）和判断奖励（自适应决定是深度推理还是直接回答更合适）。
- Result: 在SAIL-VL2上的实验表明，SAIL-RL在4B和8B规模上都提升了推理和多模态理解基准性能，与GPT-4o等商业闭源模型竞争，并显著减少幻觉。
- Conclusion: SAIL-RL是构建更可靠和自适应多模态大语言模型的原则性框架。


### [23] [Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions](https://arxiv.org/abs/2511.02288)
*Cuong Tuan Nguyen,Ngoc Tuan Nguyen,Triet Hoang Minh Dao,Huy Minh Nhat,Huy Truong Dinh*

Main category: cs.CV

TL;DR: 提出基于图神经网络的方法，将手写数学表达式建模为图结构，通过符号分割、识别和空间关系分类构建初始图，再使用2D-CFG解析器和GNN链接预测模型优化结构。

- Motivation: 手写数学表达式识别需要同时处理符号识别和结构分析，传统方法难以有效捕捉符号间的复杂空间关系。
- Method: 使用深度BLSTM网络进行符号分割、识别和空间关系分类构建初始图，然后通过2D-CFG解析器生成所有可能空间关系，最后用GNN链接预测模型去除不必要连接形成符号标签图。
- Result: 实验结果表明该方法在手写数学表达式结构识别方面表现出色，具有良好性能。
- Conclusion: 基于图神经网络的方法能够有效处理手写数学表达式的结构识别问题，为复杂数学表达式的自动识别提供了可行方案。


### [24] [Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization](https://arxiv.org/abs/2511.02329)
*Shaohan Li,Yunpeng Shi,Gilad Lerman*

Main category: cs.CV

TL;DR: Cycle-Sync是一个用于估计相机姿态（旋转和位置）的鲁棒全局框架，通过改进的消息传递最小二乘法强调循环一致性，无需相机间距离即可实现最低样本复杂度，并在实验中优于现有姿态估计方法。

- Motivation: 现有的相机姿态估计方法通常需要相机间距离信息或依赖束调整，缺乏对循环一致性的充分利用和鲁棒性保证，因此需要开发一个不依赖距离信息、具有强理论保证的全局框架。
- Method: 改进消息传递最小二乘法（MPLS），强调循环一致性信息，重新定义基于估计距离的循环一致性，引入Welsch型鲁棒损失函数，并集成基于鲁棒子空间恢复的离群值拒绝模块。
- Result: 建立了相机位置估计中最强的确定性精确恢复保证，仅通过循环一致性即可达到目前已知的最低样本复杂度；在合成和真实数据集上的实验表明，Cycle-Sync始终优于包括带束调整的完整SfM流程在内的领先姿态估计方法。
- Conclusion: Cycle-Sync提供了一个鲁棒的全局相机姿态估计框架，无需束调整，仅通过循环一致性就能实现优越的性能，为相机姿态估计提供了新的理论保证和实践方案。


### [25] [GAFD-CC: Global-Aware Feature Decoupling with Confidence Calibration for OOD Detection](https://arxiv.org/abs/2511.02335)
*Kun Zou,Yongheng Xu,Jianxing Yu,Yan Pan,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: 提出了一种名为GAFD-CC的OOD检测方法，通过全局感知特征解耦和置信度校准来改进决策边界，提高判别性能。

- Motivation: 现有的后处理OOD检测方法通常忽略了特征与logits之间的内在相关性，这对于有效的OOD检测至关重要。
- Method: GAFD-CC首先通过分类权重引导进行全局感知特征解耦，提取正相关和负相关特征；然后自适应地将这些解耦特征与多尺度logit置信度融合。
- Result: 在大规模基准测试上的广泛实验表明，GAFD-CC相比最先进方法具有竞争性的性能和强大的泛化能力。
- Conclusion: GAFD-CC通过利用特征与logits之间的相关性，有效改进了OOD检测性能，展示了在现实应用中的可靠性。


### [26] [M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings](https://arxiv.org/abs/2511.02349)
*Jiankai Tang,Tao Zhang,Jia Li,Yiru Zhang,Mingyu Zhang,Kegang Wang,Yuming Hao,Bolin Wang,Haiyang Li,Xingyao Wang,Yuanchun Shi,Yuntao Wang,Sichong Qian*

Main category: cs.CV

TL;DR: 提出了M3PD数据集和F3Mamba模型，通过双视角智能手机摄像头同步采集面部和指尖视频，结合Mamba时序建模融合双视角信息，显著提升心率监测精度和鲁棒性。

- Motivation: 现有便携生理监测方法设备要求高、姿势限制多，视频光电容积描记法面临运动伪影、光照变化和单视角限制等可靠性挑战，缺乏心血管患者验证和跨设备精度数据集。
- Method: 构建首个公开双视角移动光电容积描记数据集M3PD，包含60名参与者（47名心血管患者）同步面部和指尖视频；提出F3Mamba模型，通过Mamba时序建模融合双视角信息。
- Result: 相比现有单视角基线方法，心率误差降低21.9%至30.2%，在挑战性真实场景中鲁棒性显著提升。
- Conclusion: 双视角移动光电容积描记技术结合Mamba时序建模能有效提升心率监测精度和可靠性，为心血管疾病早期检测提供更便捷的解决方案。


### [27] [CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](https://arxiv.org/abs/2511.02360)
*Jizheng Ma,Xiaofei Zhou,Yanlong Song,Han Yan*

Main category: cs.CV

TL;DR: CoCoVa是一个新颖的视觉语言模型框架，通过连续跨模态推理解决传统VLMs在离散语言空间中推理的局限性。它使用迭代推理循环和潜在Q-Former作为动态推理引擎，在多个基准测试中表现出色。

- Motivation: 人类认知中存在许多难以用语言表达的思维过程，而当前的视觉语言模型被限制在离散的语言标记空间中推理，这限制了视觉感知的丰富性。需要弥合离散语言处理与连续视觉理解之间的表征差距。
- Method: 提出CoCoVa框架，核心是迭代推理循环：使用Latent Q-Former作为动态推理引擎，通过跨模态融合迭代优化潜在思维向量链；采用token选择机制动态识别显著视觉区域；通过对比学习和基于扩散的重建多任务目标训练模型。
- Result: CoCoVa在准确性和token效率上优于强基线模型。使用1.5B骨干网络时，在几乎所有基准测试中与或超过7B-9B模型；扩展到7B LLM骨干时仍与最先进模型竞争。定性分析显示学习到的潜在空间捕获了可解释的结构化推理模式。
- Conclusion: CoCoVa通过连续跨模态推理有效弥合了离散语言处理与连续视觉理解之间的表征差距，展示了在视觉语言任务中利用连续思维表示的潜力。


### [28] [RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning](https://arxiv.org/abs/2511.02384)
*Jiahe Song,Chuang Wang,Bowen Jiang,Yinfan Wang,Hao Zheng,Xingjian Wei,Chengjin Liu,Junyuan Gao,Yubin Wang,Lijun Wu,Jiang Wu,Qian Yu,Conghui He*

Main category: cs.CV

TL;DR: RxnCaption框架将化学反应图解析重新定义为图像描述问题，使用视觉提示策略和分子检测器来提升解析质量，并构建了大规模数据集。

- Motivation: 现有化学反应数据多为论文中的图像格式，无法被机器读取和用于机器学习模型训练，需要解决化学反应图的自动解析问题。
- Method: 提出RxnCaption框架，将坐标预测驱动的解析过程重新定义为图像描述问题，引入BIVP策略使用MolYOLO分子检测器预绘制分子边界框和索引，将下游解析转化为自然语言描述问题。
- Result: 构建了RxnCaption-11k数据集（比现有文献基准大一个数量级），RxnCaption-VL在多个指标上达到最先进性能，BIVP策略显著提升结构提取质量并简化模型设计。
- Conclusion: 该方法、数据集和模型将推动化学文献中结构化信息提取的发展，促进化学领域更广泛的AI应用。


### [29] [Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2511.02395)
*Leon Schwarzer,Matthias Zeller,Daniel Casado Herraez,Simon Dierl,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 提出一种用于稀疏噪声雷达点云的自监督运动目标分割方法，通过对比自监督表示学习和有限标注数据微调的两步方法，提高标签效率并提升最先进性能。

- Motivation: 解决雷达点云数据标注困难、耗时且成本高的问题，同时利用雷达直接测量多普勒速度的优势进行单次扫描的运动目标分割。
- Method: 采用两步法：对比自监督表示学习+监督微调，提出基于聚类的对比损失函数和动态点去除的聚类细化方法，预训练网络产生运动感知的雷达数据表示。
- Result: 方法提高了标签效率，通过自监督预训练有效提升了最先进性能。
- Conclusion: 自监督预训练能够显著提高雷达点云运动目标分割的性能，同时减少对大量标注数据的依赖。


### [30] [A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds](https://arxiv.org/abs/2511.02397)
*Kuo-Liang Chung,Ting-Chung Tang*

Main category: cs.CV

TL;DR: 提出了一种基于分组的混合颜色校正算法，用于彩色点云的颜色一致性校正。根据重叠率将目标点云分为2-3组，针对不同组别采用不同的颜色校正方法。

- Motivation: 彩色点云的颜色一致性校正是3D渲染和压缩应用中的重要任务，现有方法主要针对彩色图像，缺乏专门针对点云的有效校正方法。
- Method: 1. 估计源点云和目标点云的重叠率；2. 根据重叠率自适应地将目标点分为2组（Gcl和Gmod）或3组（Gcl、Gmod和Gdist）；3. 对Gcl使用K近邻双边插值法，对Gmod使用联合KBI和直方图均衡法，对Gdist使用直方图均衡法。
- Result: 在1086个测试彩色点云对上验证了算法的有效性，相比现有最优方法表现出更好的颜色一致性校正效果。
- Conclusion: 该分组混合颜色校正算法能够有效解决彩色点云的颜色一致性问题，代码已开源。


### [31] [Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs](https://arxiv.org/abs/2511.02404)
*Arya Shah,Vaibhav Tripathi*

Main category: cs.CV

TL;DR: 该研究通过统一的冻结编码器基准测试，量化了猫与人类视觉表征在野外的对齐程度，发现自监督ViT模型在跨物种视觉表征对齐方面表现最佳。

- Motivation: 猫和人类在眼部解剖结构上存在差异（如猫的垂直瞳孔），但尚不清楚这种差异如何影响下游视觉表征。研究旨在理解跨物种视觉表征的对齐机制。
- Method: 使用卷积网络、监督ViT、窗口Transformer和自监督ViT（DINO），通过层间中心核对齐和表征相似性分析等方法，量化猫与人类视觉表征的对齐程度。
- Result: DINO ViT-B/16获得最高的对齐度（平均CKA-RBF≈0.814），在早期块达到峰值，表明自监督学习能产生桥接物种特定统计的早期特征。监督ViT在CKA上表现良好但在几何对应上较弱。
- Conclusion: 自监督学习与ViT归纳偏置相结合，产生的表征几何比广泛使用的CNN和窗口Transformer更接近猫与人类视觉系统的对齐，为跨物种视觉计算收敛提供了可测试的神经科学假设。


### [32] [IllumFlow: Illumination-Adaptive Low-Light Enhancement via Conditional Rectified Flow and Retinex Decomposition](https://arxiv.org/abs/2511.02411)
*Wenyang Wei,Yang yang,Xixi Jia,Xiangchu Feng,Weiwei Wang,Renzhen Wang*

Main category: cs.CV

TL;DR: IllumFlow是一个结合条件整流流和Retinex理论的低光照图像增强框架，通过分别优化光照和反射分量来处理光照变化和噪声问题。

- Motivation: 解决低光照图像中广泛的光照动态范围和复杂噪声问题，实现精确的光照适应和色彩保真度。
- Method: 首先基于Retinex理论分解图像为反射和光照分量；使用条件整流流框架建模光照变化的连续流场；通过流增强的数据增强去噪网络去除反射噪声和色差。
- Result: 在低光照增强和曝光校正任务上展现出优于现有方法的定量和定性性能。
- Conclusion: IllumFlow能够精确适应不同光照条件，同时支持可定制的亮度增强，在低光照图像增强方面表现出色。


### [33] [ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension](https://arxiv.org/abs/2511.02415)
*Duo Xu,Hao Cheng,Xin Lin,Zhen Xie,Hao Wang*

Main category: cs.CV

TL;DR: 提出了一个自动化多阶段代码驱动管道，用于生成视觉推理数据集ChartM³，包含38K图表和142K问答对，显著提升了多模态大语言模型在复杂图表理解任务中的推理能力和跨域泛化性能。

- Motivation: 当前多模态大语言模型在复杂图表理解任务中覆盖场景有限，且缺乏计算密集型推理任务，无法满足实际应用需求。
- Method: 采用自动化多阶段代码驱动管道，集成检索增强生成(RAG)获取专业图表模板，使用思维链(CoT)策略生成模拟真实数据分布的推理代码，驱动图表渲染和统计计算。
- Result: 构建了ChartM³数据集，包含38K图表和142K问答对，监督微调和强化学习实验表明该数据集显著提升了模型的推理能力和跨域泛化性能，使小模型达到与大模型相当的复杂图表理解能力。
- Conclusion: 提出的自动化数据生成管道和ChartM³数据集有效解决了复杂图表理解任务的数据稀缺问题，显著提升了多模态大语言模型的视觉推理能力。


### [34] [Synthetic Crop-Weed Image Generation and its Impact on Model Generalization](https://arxiv.org/abs/2511.02417)
*Garen Boyadjian,Cyrille Pierre,Johann Laconte,Riccardo Bertoglio*

Main category: cs.CV

TL;DR: 提出使用Blender程序化生成合成作物-杂草图像的方法，用于训练农业除草机器人的语义分割模型，在跨域场景中表现出色。

- Motivation: 农业除草机器人需要精确的语义分割，但真实标注数据获取成本高，合成数据与真实图像之间的差距是主要挑战。
- Method: 使用Blender构建程序化生成合成作物-杂草图像的流程，生成包含不同植物生长、杂草密度、光照和相机角度的标注数据集。
- Result: 在合成图像上训练导致10%的模拟到真实差距，超越先前最先进方法。合成数据在跨域场景中表现出比真实数据集更好的泛化性能。
- Conclusion: 合成农业数据集具有巨大潜力，支持混合策略以实现更高效的模型训练。


### [35] [From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics](https://arxiv.org/abs/2511.02427)
*Nicolas Schuler,Lea Dewald,Nick Baldig,Jürgen Graf*

Main category: cs.CV

TL;DR: 该论文研究了在移动机器人边缘设备上部署小型视觉语言模型进行场景理解和动作识别的能力，重点分析了计算复杂度、准确性与推理时间的权衡。

- Motivation: 大型语言模型和视觉语言模型在视频理解、场景解释和常识推理方面取得了显著进展，但其计算复杂度限制了在边缘设备和移动机器人领域的应用，需要在准确性和推理时间之间找到平衡。
- Method: 提出一个评估管道，在包含真实世界城市景观、校园和室内场景的多样化数据集上，测试最先进的小型视觉语言模型在边缘设备上的场景解释和动作识别能力。
- Result: 实验评估讨论了这些小型模型在边缘设备上的潜力，特别关注挑战、弱点、固有模型偏差以及获得信息的应用。
- Conclusion: 研究表明小型视觉语言模型在移动机器人边缘设备上具有应用潜力，但需要解决计算复杂度、模型偏差等挑战，以实现准确性和推理时间的有效平衡。


### [36] [KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image](https://arxiv.org/abs/2511.02462)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: KAO是一个基于核自适应优化的扩散模型框架，专门用于高分辨率卫星图像修复，通过潜在空间条件化和显式传播技术实现高效准确的修复效果。

- Motivation: 解决高分辨率卫星图像修复中现有方法面临的挑战，包括预条件模型需要大量重训练或后条件模型计算开销大的问题。
- Method: 提出核自适应优化框架，采用潜在空间条件化方法优化紧凑潜在空间，并在扩散过程中引入显式传播实现前向-后向融合。
- Result: 实验结果表明KAO在高分辨率卫星图像修复方面设立了新的基准，在DeepGlobe和Massachusetts Roads Dataset上表现出色。
- Conclusion: KAO提供了一个可扩展的高性能解决方案，平衡了预条件模型的效率与后条件模型的灵活性，为卫星图像修复提供了新的有效方法。


### [37] [MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer](https://arxiv.org/abs/2511.02473)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shotaro Tora*

Main category: cs.CV

TL;DR: 提出了MVAFormer方法，用于多视角时空动作识别，通过基于transformer的协作模块有效处理多视角信息，在STAR设置下优于基线方法4.4个F值点。

- Motivation: 现有方法仅适用于从整个视频识别单个动作的任务设置，无法应用于新兴的时空动作识别(STAR)设置，后者需要按时间顺序识别每个人的动作。
- Method: 提出MVAFormer方法，引入基于transformer的视角间协作模块，使用保留空间信息的特征图而非丢失空间信息的嵌入向量，并将自注意力分为相同视角和不同视角以有效建模多视角关系。
- Result: 在新收集的数据集上的实验结果表明，MVAFormer在F值上比比较基线高出约4.4个点。
- Conclusion: MVAFormer成功解决了STAR设置下的多视角动作识别问题，通过保留空间信息的特征图和分视角自注意力机制实现了有效的多视角协作。


### [38] [OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control](https://arxiv.org/abs/2511.02483)
*Xilong Zhou,Jianchun Chen,Pramod Rao,Timo Teufel,Linjie Lyu,Tigran Minasian,Oleksandr Sotnychenko,Xiaoxiao Long,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: OLATverse是一个大规模数据集，包含765个真实世界物体的约900万张图像，在精确控制的光照条件下从多个视角拍摄，为逆向渲染和重光照研究提供高质量真实数据。

- Motivation: 现有技术主要依赖合成数据集进行训练和小规模真实数据集进行基准测试，这限制了方法的真实性和泛化能力。OLATverse旨在填补这一空白，提供大规模真实物体覆盖和精确控制光照下的高保真外观。
- Method: 使用35台DSLR相机和331个独立控制的光源对765个常见和不常见物体进行拍摄，涵盖多种材质类别。提供校准的相机参数、精确物体掩码、光度表面法线和漫反射反照率等辅助资源。
- Result: 构建了包含约900万张图像的大规模数据集，建立了首个全面的真实世界物体中心逆向渲染和法线估计基准评估集。
- Conclusion: OLATverse代表了将下一代逆向渲染和重光照方法与真实世界数据整合的关键一步，将公开发布数据集和所有后处理工作流程。


### [39] [Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization](https://arxiv.org/abs/2511.02489)
*Tao Liu,Kan Ren,Qian Chen*

Main category: cs.CV

TL;DR: 提出基于目标检测和图像神经网络的跨视角无人机定位框架，通过提取显著实例特征和节点相似性度量，有效解决GNSS拒止环境下的无人机定位问题。

- Motivation: 在GNSS拒止区域，传统卫星定位方法失效，需要开发基于视觉的无人机定位方法。现有方法存在跨域差异、内容丢失和真实感有限等问题。
- Method: 利用现代目标检测技术从无人机和卫星图像中提取显著实例，集成图神经网络推理图像间和图像内节点关系，使用细粒度图基节点相似性度量。
- Result: 在公开和真实数据集上的大量实验表明，该方法能有效处理异构外观差异，泛化能力强，适用于红外-可见光图像匹配等更大模态差距场景。
- Conclusion: 提出的跨视角无人机定位框架通过目标检测和图神经网络实现了强检索和定位性能，为GNSS拒止环境提供了有效的解决方案。


### [40] [DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding](https://arxiv.org/abs/2511.02495)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang*

Main category: cs.CV

TL;DR: 提出了DetectiumFire数据集，包含22.5k高分辨率火灾图像和2.5k真实火灾视频，涵盖多种火灾类型和环境，为火灾领域多模态研究填补数据空白。

- Motivation: 当前多模态模型在火灾领域应用受限，主要原因是缺乏高质量的公开火灾标注数据集。
- Method: 构建大规模多模态数据集，包含图像和视频数据，并标注传统计算机视觉标签和详细文本描述。
- Result: 数据集在规模、多样性和数据质量上优于现有基准，在目标检测、图像生成和视觉语言推理等任务中验证了其有效性。
- Conclusion: DetectiumFire数据集有望推动火灾相关研究，支持智能安全系统开发，并已公开发布供AI社区使用。


### [41] [Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes](https://arxiv.org/abs/2511.02503)
*Robinson Umeike,Neil Getty,Yin Xiangyu,Yi Jiang*

Main category: cs.CV

TL;DR: PtychoBench是一个用于评估LLM和VLM在显微工作流程中专业化策略的多模态基准，研究发现视觉任务中SFT和ICL互补最优，文本任务中ICL优于SFT。

- Motivation: 解决基础模型在专业科学任务中适应策略不明确的问题，为显微工作流程自动化提供优化框架。
- Method: 引入PtychoBench基准，系统比较监督微调(SFT)和上下文学习(ICL)两种专业化策略，在视觉伪影检测和文本参数推荐任务上进行评估。
- Result: 视觉任务：SFT+ICL组合达到最高性能(Micro-F1 0.728)；文本任务：ICL在大型基础模型上表现最佳(Micro-F1 0.847)，优于SFT模型。
- Conclusion: 最优专业化路径取决于任务模态，为开发更有效的科学代理系统提供了清晰框架。


### [42] [ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing](https://arxiv.org/abs/2511.02505)
*Yaosen Chen,Wei Wang,Xuming Wen,Han Yang,Yanru Zhang*

Main category: cs.CV

TL;DR: 提出基于能量优化的视频镜头组装方法，通过视觉语义匹配和参考视频风格学习，实现自动化视频编辑并保持艺术表达。

- Motivation: 解决当前智能视频编辑技术无法捕捉创作者独特艺术表达的问题，让无经验的用户也能制作高质量视频。
- Method: 1. 大语言模型生成脚本与视频库进行视觉语义匹配；2. 参考视频镜头分割标注；3. 基于能量模型学习参考风格；4. 结合语法规则优化镜头组装。
- Result: 系统能够自动按照特定逻辑、叙事需求或艺术风格安排独立镜头，学习参考视频的组装风格，创建连贯的视觉序列。
- Conclusion: 该方法不仅实现了镜头组装的自动化，还能学习并保持参考视频的艺术风格，使无经验用户也能创作视觉吸引力强的视频。


### [43] [Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems](https://arxiv.org/abs/2511.02507)
*Nicolas Schuler,Lea Dewald,Jürgen Graf*

Main category: cs.CV

TL;DR: 提出了一种基于本地模型的自动化报告生成管道，用于移动机器人系统，能够在边缘设备上运行，保护隐私且无需外部服务。

- Motivation: 深度学习使机器人系统能够在动态非结构化环境中交互，但关键任务应用需要评估大量异构数据，自动化报告生成对促进系统评估和接受度至关重要。
- Method: 开发了一个利用多模态传感器的自动化报告生成管道，完全基于可在边缘计算设备上部署的本地模型，无需外部服务。
- Result: 在包含室内、室外和城市环境的多样化数据集上评估了实现，提供了定量和定性评估结果，示例报告可通过公共仓库获取。
- Conclusion: 提出的自动化报告生成管道能够有效支持移动机器人系统的评估，同时保护隐私并减少对外部服务的依赖。


### [44] [LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization](https://arxiv.org/abs/2511.02510)
*Jee Won Lee,Jongseong Brad Choi*

Main category: cs.CV

TL;DR: LiteVoxel是一个自调节的稀疏体素光栅化训练管道，通过逆Sobel重加权和深度分位数剪枝等创新方法，解决了传统方法在低频内容拟合、内存占用和边界稳定性方面的问题，显著降低了VRAM使用并保持了感知质量。

- Motivation: 稀疏体素光栅化虽然快速且可微分，但存在低频内容拟合不足、依赖脆弱的剪枝启发式方法以及内存过度增长导致VRAM膨胀的问题。
- Method: 采用逆Sobel重加权损失函数，结合训练中期的gamma斜坡调整；使用基于深度分位数的剪枝逻辑，通过EMA滞后保护和基于射线足迹的优先级驱动细分来实现自适应优化。
- Result: 在Mip-NeRF 360和Tanks & Temples数据集上的实验表明，LiteVoxel减少了低频区域和边界不稳定性错误，同时保持PSNR/SSIM、训练时间和FPS与强基线相当，关键是将峰值VRAM降低了40%-60%。
- Conclusion: LiteVoxel实现了更可预测、内存效率更高的训练，在保持感知质量的同时保留了先前方法遗漏的低频细节。


### [45] [Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data](https://arxiv.org/abs/2511.02541)
*Jessica Plassmann,Nicolas Schuler,Georg von Freymann,Michael Schuth*

Main category: cs.CV

TL;DR: 本研究探索无监督学习方法用于剪切散斑图像的自动异常检测，评估了三种架构：全连接自编码器、卷积自编码器和师生特征匹配模型，结果显示师生方法在分类鲁棒性和缺陷定位方面表现最优。

- Motivation: 剪切散斑技术虽然具有高灵敏度和全场检测能力，但由于需要专家解释，工业应用受限。为了减少对标记数据和人工评估的依赖，本研究探索无监督学习方法实现自动化异常检测。
- Method: 开发了包含可重复缺陷模式的定制样本数据集，在理想和实际变形条件下系统采集剪切散斑测量数据。评估了三种无监督架构：全连接自编码器、卷积自编码器和师生特征匹配模型，所有模型仅使用无缺陷数据进行训练。
- Result: 师生特征匹配方法在分类鲁棒性方面表现最优，能够实现精确的缺陷定位。与自编码器模型相比，该方法显示出更好的特征表示可分性，并通过t-SNE嵌入可视化得到验证。
- Conclusion: 本研究强调了无监督深度学习在工业环境中实现可扩展、标签高效的剪切散斑检测的潜力，师生方法在自动化异常检测方面表现出优越性能。


### [46] [Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction](https://arxiv.org/abs/2511.02558)
*Ali Farki,Elaheh Moradi,Deepika Koundal,Jussi Tohka*

Main category: cs.CV

TL;DR: 该研究使用深度学习模型从基线MRI预测未来几年的脑部MRI图像，在阿尔茨海默病研究中实现了高保真度的脑部图像预测。

- Motivation: 从基线MRI预测未来脑状态是神经影像学的核心挑战，对研究阿尔茨海默病等神经退行性疾病具有重要意义。现有方法主要预测认知评分或临床结果，而本研究旨在直接预测整个脑部MRI图像。
- Method: 在ADNI和AIBL两个纵向队列上实现并评估了五种深度学习架构：UNet、U2-Net、UNETR、时间嵌入UNet和ODE-UNet，直接比较预测的随访MRI与实际扫描。
- Result: 表现最佳的模型实现了高保真度预测，所有模型在独立外部数据集上泛化良好，展示了稳健的跨队列性能。
- Conclusion: 深度学习可以在体素级别可靠地预测个体特异性脑部MRI，为个体化预后提供了新的机会。


### [47] [The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic](https://arxiv.org/abs/2511.02563)
*Akash Sharma,Chinmay Mhatre,Sankalp Gawali,Ruthvik Bokkasam,Brij Kishore,Vishwajeet Pattanaik,Tarun Rambha,Abdul R. Pinjari,Vijay Kovvali,Anirban Chakraborty,Punit Rathore,Raghu Krishnapuram,Yogesh Simmhan*

Main category: cs.CV

TL;DR: UVH-26是首个印度大规模交通摄像头数据集，包含26,646张高分辨率图像和180万个边界框标注，涵盖14个印度特有车辆类别。在印度交通场景下，基于该数据集训练的检测器相比COCO数据集有8.4-31.5%的性能提升。

- Motivation: 解决现有全球基准数据集在印度复杂交通场景下的不足，为新兴国家智能交通系统提供领域特定的训练数据基础。
- Method: 通过2800个班加罗尔安全城市CCTV摄像头采集图像，组织565名大学生进行众包标注，使用多数投票和STAPLE算法生成共识标注，训练YOLO和DETR等当代检测器。
- Result: 在UVH-26上训练的模型相比COCO基线在mAP50:95指标上提升8.4-31.5%，RT-DETR-X表现最佳达到0.67 mAP50:95，而COCO仅为0.40。
- Conclusion: UVH-26填补了现有全球基准在印度交通场景的空白，证明了领域特定训练数据对复杂交通条件国家的重要性，为智能交通系统发展提供了基础。


### [48] [Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification](https://arxiv.org/abs/2511.02564)
*Md Rashidunnabi,Kailash A. Hambarde,Vasco Lopes,Joao C. Neves,Hugo Proenca*

Main category: cs.CV

TL;DR: 提出了MTF-CVReID框架，通过7个互补模块增强跨视角视频行人重识别性能，在保持实时效率的同时实现了最先进的性能。

- Motivation: 解决跨视角（如空中-地面监控）视频行人重识别中的极端视角变化、尺度差异和时间不一致性问题。
- Method: 在ViT-B/16骨干网络上添加7个参数高效模块：跨流特征归一化、多分辨率特征协调、身份感知记忆模块、时序动态建模、跨视角特征对齐、分层时序模式学习和多视角身份一致性学习。
- Result: 在AG-VPReID基准测试中所有高度级别均达到最先进性能，在G2A-VReID和MARS数据集上表现出强大的跨数据集泛化能力，保持189 FPS的实时效率。
- Conclusion: 精心设计的基于适配器的模块可以显著增强跨视角鲁棒性和时间一致性，且不损害计算效率。


### [49] [A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding](https://arxiv.org/abs/2511.02565)
*Jingyu Lu,Haonan Wang,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: VCFlow是一个无需特定对象训练、基于人类视觉系统腹背侧架构的脑解码框架，能够从fMRI重建连续视觉体验，在牺牲7%准确率的情况下实现快速重建（每视频10秒）。

- Motivation: 解决跨对象泛化问题，开发无需特定对象训练即可从fMRI重建视觉体验的临床适用方法。
- Method: 分层解码框架，显式建模视觉系统的腹背侧架构，使用特征级对比学习提取对象不变语义表示。
- Result: 相比传统方法需要12小时数据和大量计算，VCFlow无需重新训练即可快速重建视频，平均准确率仅下降7%。
- Conclusion: VCFlow提供了快速且临床可扩展的解决方案，为无特定对象训练的脑解码开辟了新方向。


### [50] [TAUE: Training-free Noise Transplant and Cultivation Diffusion Model](https://arxiv.org/abs/2511.02580)
*Daichi Nagai,Ryugo Morita,Shunsuke Kitada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 提出了TAUE框架，一种零样本、分层图像生成的训练免费方法，通过噪声移植和培养技术实现多层图像的语义和结构一致性。

- Motivation: 现有文本到图像扩散模型只能生成单一平面图像，无法满足专业应用对分层控制的需求。现有解决方案要么需要大量数据集进行微调，要么只能生成孤立前景元素而无法产生完整连贯的场景。
- Method: 核心技术创新是噪声移植和培养(NTC)，从前景和复合生成过程中提取中间潜在表示，将其移植到后续层的初始噪声中，确保各层之间的语义和结构一致性。
- Result: 大量实验表明，这种训练免费方法达到了与微调方法相当的性能，在保持高图像质量和保真度的同时增强了分层一致性。
- Conclusion: TAUE不仅消除了昂贵的训练和数据集需求，还解锁了复杂组合编辑等新的下游应用，为更易访问和可控的生成工作流程铺平了道路。


### [51] [Zero-Shot Multi-Animal Tracking in the Wild](https://arxiv.org/abs/2511.02591)
*Jan Frederik Meier,Timo Lüddecke*

Main category: cs.CV

TL;DR: 提出了一种基于视觉基础模型的零样本多动物追踪框架，无需重新训练或超参数调整即可应用于新数据集。

- Motivation: 多动物追踪对理解动物生态和行为至关重要，但传统方法需要针对每个应用场景进行大量模型微调和启发式设计，限制了其通用性。
- Method: 结合Grounding Dino目标检测器和Segment Anything Model 2 (SAM 2)追踪器，并设计精心设计的启发式方法，构建零样本追踪框架。
- Result: 在ChimpAct、Bird Flock Tracking、AnimalTrack和GMOT-40子集上的评估显示，该框架在不同物种和环境中的表现强劲且一致。
- Conclusion: 该方法展示了视觉基础模型在多动物追踪任务中的潜力，提供了一种无需重新训练即可应用于新场景的通用解决方案。


### [52] [UniChange: Unifying Change Detection with Multimodal Large Language Model](https://arxiv.org/abs/2511.02607)
*Xu Zhang,Danyang Li,Xiaohang Dong,Tianhao Wu,Hualong Yu,Jianye Wang,Qicheng Li,Xiang Li*

Main category: cs.CV

TL;DR: UniChange是首个基于多模态大语言模型的统一变化检测框架，通过引入特殊令牌[T1]、[T2]和[CHANGE]统一了二元变化检测和语义变化检测任务，利用文本提示指导变化类别识别，在多个基准测试中达到最先进性能。

- Motivation: 当前变化检测模型通常只能从单一类型标注数据中学习有限知识，无法同时利用多样的二元变化检测和语义变化检测数据集，导致泛化能力差和功能有限。
- Method: 利用多模态大语言模型的语言先验和统一能力，开发UniChange模型，集成生成式语言能力与专门的变化检测功能，通过三个特殊令牌统一不同任务，使用文本提示指导变化类别识别。
- Result: 在WHU-CD、S2Looking、LEVIR-CD+和SECOND四个公开基准测试中分别达到90.41、53.04、78.87和57.62的IoU分数，超越了所有先前方法。
- Conclusion: UniChange成功实现了变化检测任务的统一框架，能够有效从多源数据集中获取知识，即使在类别定义冲突的情况下也能良好工作，展示了多模态大语言模型在变化检测领域的巨大潜力。


### [53] [Robust Face Liveness Detection for Biometric Authentication using Single Image](https://arxiv.org/abs/2511.02645)
*Poulami Raha,Yeongnam Chae*

Main category: cs.CV

TL;DR: 提出了一种轻量级CNN框架来检测面部识别系统中的呈现攻击（打印/显示、视频和包裹攻击），确保快速生物特征认证。

- Motivation: 面部识别系统容易受到呈现攻击，恶意行为者可以通过欺骗手段非法访问安全系统。
- Method: 使用轻量级CNN框架进行活体检测，能够识别多种呈现攻击类型。
- Result: 开发了包含60个受试者500多个视频的2D欺骗攻击数据集，认证时间仅需1-2秒（在CPU上）。
- Conclusion: 该架构提供了无缝的活体检测，有效防止面部识别系统的呈现攻击。


### [54] [Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models](https://arxiv.org/abs/2511.02650)
*Tianfan Peng,Yuntao Du,Pengzhou Ji,Shijie Dong,Kailin Jiang,Mingchuan Ma,Yijun Tian,Jinhe Bi,Qian Li,Wei Du,Feng Xiao,Lizhen Cui*

Main category: cs.CV

TL;DR: 提出了UniPruneBench，一个用于多模态大模型中视觉token剪枝的统一基准测试框架，评估了10种压缩算法在3类LMM模型上的表现，发现随机剪枝是强基线，不同方法在不同场景下表现各异，OCR任务对剪枝最敏感。

- Motivation: 多模态大模型由于图像编码器引入的大量视觉token导致推理效率低下，现有的token压缩方法评估碎片化且不一致，需要统一的基准测试。
- Method: 构建UniPruneBench基准测试，在6个能力维度和10个数据集上提供标准化协议，涵盖10种代表性压缩算法和3类LMM模型，除了任务精度外还包含运行时和预填充延迟等系统级指标。
- Result: 实验发现：(1)随机剪枝是强基线；(2)没有单一方法在所有场景下始终最优；(3)剪枝敏感性因任务而异，OCR最脆弱；(4)剪枝比例是影响性能下降的主导因素。
- Conclusion: UniPruneBench将为未来高效多模态建模研究提供可靠基础。


### [55] [Differentiable Hierarchical Visual Tokenization](https://arxiv.org/abs/2511.02652)
*Marius Aasan,Martine Hjelkrem-Tan,Nico Catalano,Changkyu Choi,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: 提出了一种端到端可微分的tokenizer，能够根据图像内容自适应地生成token，同时保持与现有架构的向后兼容性。

- Motivation: 传统的Vision Transformers使用固定的patch token，忽略了图像的空间和语义结构，需要更灵活的内容自适应tokenization方法。
- Method: 使用分层模型选择和信息准则，以像素级粒度自适应图像内容，实现端到端可微分tokenization。
- Result: 在图像级分类和密集预测任务中表现具有竞争力，支持开箱即用的栅格到矢量转换。
- Conclusion: 该方法提供了一种向后兼容的内容自适应tokenization方案，能够有效处理图像的空间和语义结构。


### [56] [Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.02685)
*Chao Yuan,Zanwu Liu,Guiwei Zhang,Haoxuan Xu,Yujian Zhao,Guanglin Niu,Bo Li*

Main category: cs.CV

TL;DR: 提出了一种基于模态转换表示学习(MTRL)的可见光-红外行人重识别框架，通过生成中间图像作为模态转换器，无需额外参数即可有效对齐跨模态特征。

- Motivation: 现有方法主要依赖中间表示来对齐跨模态特征，但存在参数多、可解释性差、未能充分利用中间特征等问题。可见光和红外模态之间存在显著差异，需要更有效的对齐方法。
- Method: 使用生成的中间图像作为可见光到红外模态的转换器，该图像与原始可见光图像完全对齐且与红外模态相似。采用模态转换对比损失和模态查询正则化损失进行训练。
- Result: 在三个典型的VI-ReID数据集上显著且持续地优于现有最先进方法，且推理速度与骨干网络相同。
- Conclusion: 提出的MTRL框架无需额外参数就能有效对齐跨模态特征，在VI-ReID任务上实现了性能提升，同时保持了高效的推理速度。


### [57] [VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models](https://arxiv.org/abs/2511.02712)
*Zhicheng Zhang,Weicheng Wang,Yongjie Zhu,Wenyu Qin,Pengfei Wan,Di Zhang,Jufeng Yang*

Main category: cs.CV

TL;DR: 提出了一种情感线索引导的推理框架VidEmo，通过两阶段调优（课程情感学习和情感树强化学习）来提升视频情感分析能力，并构建了包含210万样本的情感数据集Emo-CFG。

- Motivation: 情感具有动态性和线索依赖性，现有方法难以理解复杂且不断演化的情感状态及其合理依据，需要更有效的情感分析框架。
- Method: 采用阶段式统一框架，结合基础属性感知、表情分析和高级情感理解；开发视频情感基础模型VidEmo，通过课程情感学习和情感树强化学习进行两阶段调优；构建大规模情感数据集Emo-CFG。
- Result: 在15个面部感知任务上取得了有竞争力的性能，设立了新的里程碑。
- Conclusion: 提出的情感线索引导推理框架和VidEmo模型有效解决了视频情感分析的挑战，Emo-CFG数据集为情感理解任务提供了重要资源。


### [58] [LLEXICORP: End-user Explainability of Convolutional Neural Networks](https://arxiv.org/abs/2511.02720)
*Vojtěch Kůr,Adam Bajger,Adam Kukučka,Marek Hradil,Vít Musil,Tomáš Brázdil*

Main category: cs.CV

TL;DR: 提出了LLEXICORP方法，将概念相关性传播与多模态大语言模型结合，自动为CNN概念原型命名并生成自然语言解释，降低深度神经网络解释的门槛。

- Motivation: 现有概念相关性传播方法需要专家手动检查激活图像来命名概念并合成解释，限制了可访问性和可扩展性。
- Method: 构建模块化流水线，将CRP与多模态大语言模型耦合，通过精心设计的提示词教导语言模型CRP语义，并分离命名和解释任务。
- Result: 在ImageNet数据集上的VGG16模型上定性评估，能够自动生成针对不同受众的定制化解释文本。
- Conclusion: 将基于概念的归因方法与大型语言模型集成可以显著降低解释深度神经网络的门槛，为更透明的AI系统铺平道路。


### [59] [Dynamic Reflections: Probing Video Representations with Text Alignment](https://arxiv.org/abs/2511.02767)
*Tyler Zhu,Tengda Han,Leonidas Guibas,Viorica Pătrăucean,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: 本文首次对视频-文本表示对齐进行了全面研究，揭示了跨模态对齐高度依赖于测试时视觉和文本数据的丰富程度，并提出了参数化测试时缩放定律。研究发现强对齐可能与通用视频表示理解相关，并为视觉语言模型提供了具有挑战性的测试平台。

- Motivation: 虽然图像与文本的对齐研究已取得显著进展，但视频数据的时序特性在此背景下仍未被充分探索。本文旨在填补这一空白，研究现代视频和语言编码器的能力。
- Method: 提出了参数化测试时缩放定律来捕捉跨模态对齐行为，研究了语义对齐与下游任务性能之间的相关性，并将时序推理与跨模态对齐相关联。
- Result: 研究发现跨模态对齐高度依赖于测试时视觉和文本数据的丰富程度，强对齐可能关联通用视频表示理解，时序推理与跨模态对齐存在相关性。
- Conclusion: 视频-文本对齐是探索时空数据表示能力的信息丰富的零样本方法，为视觉语言模型提供了具有挑战性的测试平台。


### [60] [PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction & Editing](https://arxiv.org/abs/2511.02777)
*Antonio Oroz,Matthias Nießner,Tobias Kirschstein*

Main category: cs.CV

TL;DR: PercHead是一种单图像3D头部重建和语义3D编辑方法，通过双分支编码器和ViT解码器实现视图一致的3D头部重建，并支持通过分割图和文本/图像提示进行语义编辑。

- Motivation: 解决单图像3D头部重建中的严重视角遮挡、弱感知监督以及3D空间编辑模糊性等挑战性问题。
- Method: 使用双分支编码器和ViT解码器通过迭代交叉注意力将2D特征提升到3D空间，采用高斯泼溅渲染，并基于DINOv2和SAM2.1的感知监督策略。
- Result: 在新视角合成方面达到最先进性能，对极端视角具有卓越鲁棒性，并可通过轻量级交互GUI实现直观的3D编辑。
- Conclusion: PercHead提供了一个统一的3D头部重建和编辑框架，通过感知监督和模块化设计实现了高质量的重建和直观的语义编辑能力。


### [61] [VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation](https://arxiv.org/abs/2511.02778)
*Kevin Qinghong Lin,Yuhao Zheng,Hangyu Ran,Dantong Zhu,Dongxing Mao,Linjie Li,Philip Torr,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: VCode是一个将多模态理解重新定义为代码生成的基准，使用SVG作为紧凑、可解释的可执行视觉表示。VCoder框架通过迭代修订和视觉工具增强VLM，在多个基准上显著提升性能。

- Motivation: 当前AI进展主要集中在语言中心的任务上，视觉中心编码研究不足。受人类草图推理启发，提倡使用SVG代码作为紧凑、可解释的可执行视觉表示。
- Method: 引入VCode基准，将多模态理解重构为代码生成任务；提出CodeVQA评估协议；开发VCoder框架，包含"带修订的思考"（迭代分析差异并优化SVG代码）和"带视觉工具的行动"（使用检测器和解析器提供结构化线索）。
- Result: 前沿VLM在生成忠实SVG方面表现不佳，VCoder相比表现最佳的Claude-4-Opus带来12.3分的总体提升。人类研究显示人类和VLM在渲染SVG上都表现更差，但一致性揭示了符号视觉表示的前景。
- Conclusion: SVG代码作为视觉表示具有潜力，但当前VLM在视觉中心编码方面仍有局限。VCoder框架有效提升了符号保真度，为视觉推理开辟了新途径。


### [62] [When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought](https://arxiv.org/abs/2511.02779)
*Yiyang Zhou,Haoqin Tu,Zijun Wang,Zeyu Wang,Niklas Muennighoff,Fan Nie,Yejin Choi,James Zou,Chaorui Deng,Shen Yan,Haoqi Fan,Cihang Xie,Huaxiu Yao,Qinghao Ye*

Main category: cs.CV

TL;DR: MIRA是一个新的基准测试，用于评估模型在需要生成中间视觉图像来指导推理过程的场景中的表现。该基准包含546个多模态问题，要求模型生成草图、结构图等中间图像来辅助推理，模拟人类"画图思考"的过程。

- Motivation: 传统的思维链方法仅依赖文本，而许多复杂问题需要视觉信息来辅助推理。MIRA旨在评估模型在需要生成和利用中间视觉图像进行推理的场景中的能力，这更接近人类解决复杂问题的方式。
- Method: MIRA包含546个高质量的多模态问题，标注了中间视觉图像和最终答案。提出了统一的评估协议，包括三个层次的输入：仅图像和问题的直接输入、带思维提示的文本思维链输入、以及带注释图像线索和文本思维提示的视觉思维链输入。
- Result: 实验结果显示，现有的多模态大语言模型在仅依赖文本提示时表现不佳，但当提供中间视觉线索时，所有模型和任务的平均相对增益达到33.7%。扩展搜索空间和设计对齐视觉思维链的文本提示只能带来有限的改进。
- Conclusion: MIRA基准测试强调了想象视觉信息在成功推理中的关键作用，表明中间视觉图像的生成和利用对于解决复杂推理问题至关重要。


### [63] [AI-Generated Image Detection: An Empirical Study and Future Research Directions](https://arxiv.org/abs/2511.02791)
*Nusrat Tasnim,Kutub Uddin,Khalid Mahmood Malik*

Main category: cs.CV

TL;DR: 本文提出了一个统一的基准测试框架，用于在受控和可复现条件下系统评估深度伪造检测方法，揭示了现有方法在泛化性和可解释性方面的局限性。

- Motivation: AI生成的深度伪造媒体对多媒体取证、错误信息检测和生物识别系统构成严重威胁，导致公众对法律系统信任度下降、欺诈案件增加和社会工程攻击。现有取证方法存在三个关键问题：非标准化基准、不一致的训练协议和有限的评估指标。
- Method: 引入统一的基准测试框架，对10种最先进的取证方法（从头训练、冻结训练和微调）和7个公开数据集（GAN和扩散模型生成）进行系统评估，使用多种指标包括准确率、平均精度、ROC-AUC、错误率和类别敏感度，并通过置信度曲线和Grad-CAM热图分析模型可解释性。
- Result: 评估显示不同方法在泛化性方面存在显著差异，某些方法在分布内性能良好但在跨模型可迁移性方面表现下降。
- Conclusion: 本研究旨在引导研究社区更深入理解当前取证方法的优缺点，并启发开发更鲁棒、可泛化和可解释的解决方案。


### [64] [PLUTO-4: Frontier Pathology Foundation Models](https://arxiv.org/abs/2511.02826)
*Harshith Padigela,Shima Nofallah,Atchuth Naveen Chilaparasetti,Ryun Han,Andrew Walker,Judy Shen,Chintan Shah,Blake Martin,Aashish Sood,Elliot Miller,Ben Glass,Andy Beck,Harsha Pokkalla,Syed Ashar Javed*

Main category: cs.CV

TL;DR: PLUTO-4是新一代病理学基础模型，包含紧凑型PLUTO-4S和前沿规模PLUTO-4G两个版本，在55万+WSI上预训练，在多种病理任务上实现最先进性能。

- Motivation: 基于大规模病理图像训练的基础模型已展现出强大的迁移能力，需要开发更先进的模型来进一步提升病理诊断性能。
- Method: 采用两种Vision Transformer架构：PLUTO-4S使用FlexiViT设置和2D-RoPE嵌入进行多尺度部署优化；PLUTO-4G使用单一补丁尺寸最大化表示能力。基于DINOv2自监督目标在55万+WSI上预训练。
- Result: PLUTO-4在需要不同空间和生物学背景的任务上实现最先进性能，包括补丁级分类、分割和切片级诊断。PLUTO-4S提供高吞吐量，PLUTO-4G在多个基准测试中建立新性能前沿，皮肤病理诊断提升11%。
- Conclusion: PLUTO-4作为转化研究和诊断用例的骨干模型，具有改变实际应用的潜力。


### [65] [Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks](https://arxiv.org/abs/2511.02830)
*Dmitrii Pozdeev,Alexey Artemov,Ananta R. Bhattarai,Artem Sevastopolsky*

Main category: cs.CV

TL;DR: DenseMarks是一种新的人类头部学习表示方法，通过Vision Transformer网络为每个像素预测3D嵌入，在3D规范单元立方体中建立对应关系，实现高质量密集对应。

- Motivation: 为了解决人类头部图像的高质量密集对应问题，特别是在不同姿态和个体间保持一致性，同时覆盖整个头部（包括头发）的鲁棒表示。
- Method: 使用Vision Transformer网络预测像素级3D嵌入，通过对比损失训练，结合多任务学习（面部标志点和分割约束），并利用潜在立方体特征确保空间连续性。
- Result: 在几何感知点匹配和单目头部跟踪方面达到最先进水平，对姿态变化具有鲁棒性，并能覆盖整个头部区域。
- Conclusion: DenseMarks提供了一种可解释、可查询的规范空间表示，能够用于语义部分查找、面部/头部跟踪和立体重建等任务，且在不同姿态和个体间保持一致性。
## cs.HC

### [66] [HAGI++: Head-Assisted Gaze Imputation and Generation](https://arxiv.org/abs/2511.02468)
*Chuhan Jiao,Zhiming Hu,Andreas Bulling*

Main category: cs.HC

TL;DR: HAGI++是一种基于扩散模型的多模态注视数据填补方法，首次利用头部方向传感器来挖掘头眼运动之间的内在关联，在多个数据集上优于传统方法和深度学习方法。

- Motivation: 移动眼动追踪在现实世界和扩展现实环境中捕捉人类视觉注意力至关重要，但由于眨眼、瞳孔检测错误或光照变化导致的缺失值给后续注视数据分析带来了重大挑战。
- Method: HAGI++采用基于transformer的扩散模型来学习眼部和头部表征之间的跨模态依赖关系，并可轻松扩展到包含额外的身体运动信息。
- Result: 在Nymeria、Ego-Exo4D和HOT3D数据集上的广泛评估表明，HAGI++在注视填补方面始终优于传统插值方法和基于深度学习的时序填补基线方法。
- Conclusion: 该方法为现实世界环境中更完整准确的眼动记录铺平了道路，在增强基于注视的分析和交互方面具有重要潜力。


### [67] [SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration](https://arxiv.org/abs/2511.02560)
*Dan Bohus,Sean Andrist,Ann Paradiso,Nick Saw,Tim Schoonbeek,Maia Stiber*

Main category: cs.HC

TL;DR: SigmaCollab是一个用于研究物理环境中人机协作的数据集，包含85个会话，记录了未受训参与者在混合现实AI助手指导下完成物理世界程序性任务的过程。

- Motivation: 为物理环境中的人机协作研究提供真实的多模态数据，解决当前该领域缺乏现实测试环境的问题。
- Method: 收集了85个会话的丰富多模态数据流，包括音频、第一人称视角视频、深度图、头部/手部/视线追踪信息以及后处理标注。
- Result: 创建了一个相对较小但应用驱动和交互性强的数据集（约14小时），为人机协作研究提供了更真实的测试环境。
- Conclusion: SigmaCollab数据集为人机协作研究提供了宝贵资源，未来计划基于此构建混合现实任务辅助场景的基准测试集。
## eess.IV

### [68] [Opto-Electronic Convolutional Neural Network Design Via Direct Kernel Optimization](https://arxiv.org/abs/2511.02065)
*Ali Almuallem,Harshana Weligampola,Abhiram Gnanasambandam,Wei Xu,Dilshan Godaliyadda,Hamid R. Sheikh,Stanley H. Chan,Qi Guo*

Main category: eess.IV

TL;DR: 提出一种两阶段设计方法，用于优化光电卷积神经网络，通过先训练标准电子CNN，再直接优化光学前端（超表面阵列）的卷积核，显著降低计算和内存需求，并在深度估计任务中实现比端到端训练高两倍的精度。

- Motivation: 传统光电神经网络端到端优化面临计算模拟成本高和参数空间大的限制，需要更高效的优化策略来平衡光学和电子模块的设计。
- Method: 采用两阶段设计策略：第一阶段训练标准电子CNN，第二阶段通过直接核优化实现光学前端（超表面阵列）的第一卷积层。
- Result: 该方法将计算和内存需求降低数百倍，提高训练稳定性，在单目深度估计任务中，相同训练时间和资源条件下精度比端到端训练提高两倍。
- Conclusion: 两阶段设计方法为光电神经网络提供了一种计算效率高且性能优越的优化方案，解决了端到端优化的计算瓶颈问题。


### [69] [MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization](https://arxiv.org/abs/2511.02400)
*Yalda Zafari,Hongyi Pan,Gorkem Durak,Ulas Bagci,Essam A. Rashed,Mohamed Mabrok*

Main category: eess.IV

TL;DR: MammoClean是一个用于乳腺X光数据集标准化和偏差量化的公开框架，通过标准化病例选择、图像处理和统一元数据来解决数据异质性导致的模型泛化问题。

- Motivation: 乳腺X光AI系统开发面临数据质量、元数据标准和人群分布的严重异质性，这些数据集特定偏差严重损害模型泛化能力，阻碍临床部署。
- Method: MammoClean标准化病例选择、图像处理（包括侧位和强度校正），并将元数据统一为一致的多视图结构。系统识别偏差来源并量化分布偏移。
- Result: 在三个异构数据集上的应用显示乳腺密度和异常患病率存在显著分布偏移。AI模型在污染数据集上训练时性能显著下降。
- Conclusion: MammoClean为乳腺X光AI开发提供了可重复的偏差感知管道，能够构建统一的多数据集训练语料库，开发具有优越跨域泛化能力的稳健模型。


### [70] [Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback](https://arxiv.org/abs/2511.02576)
*Alix de Langlais,Benjamin Billot,Théo Aguilar Vidal,Marc-Olivier Gauci,Hervé Delingette*

Main category: eess.IV

TL;DR: SCORE是一个弱监督的医学图像分割优化框架，仅使用轻量级反馈训练，通过区域质量评分和过/欠分割错误标签来改进初始分割预测，显著减少了监督需求和标注时间。

- Motivation: 医学图像解剖区域分割是关键任务，手动分割准确但耗时且存在变异性，自动分割方法虽广泛但可能达不到临床精度标准。现有优化方法依赖大量用户交互或全监督训练，需要更高效的解决方案。
- Method: SCORE引入了一种新颖的损失函数，利用区域质量评分和过/欠分割错误标签，在训练过程中仅使用轻量级反馈，而不依赖密集的训练图像标注。
- Result: 在肱骨CT扫描上，SCORE显著改进了TotalSegmentator的初始预测，性能与现有优化方法相当，同时大大减少了监督需求和标注时间。
- Conclusion: SCORE提供了一种高效的弱监督分割优化方法，能够在减少监督需求的同时达到与现有方法相当的性能，为医学图像分割提供了实用的解决方案。
## cs.RO

### [71] [A Step Toward World Models: A Survey on Robotic Manipulation](https://arxiv.org/abs/2511.02097)
*Peng-Fei Zhang,Ying Cheng,Xiaofan Sun,Shijie Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.RO

TL;DR: 这篇论文对机器人操作中的世界模型方法进行了系统性调查，分析了世界模型在感知、预测和控制中的作用，旨在为开发通用实用的机器人世界模型提供路线图。

- Motivation: 自主代理需要在复杂动态环境中执行操作任务，这要求它们理解世界机制和动态，而不仅仅是反应式控制或状态复制。世界模型作为内部表征能够编码环境状态、捕捉动态并支持预测、规划和推理。
- Method: 通过回顾机器人操作方法，调查具有世界模型核心能力的方法，分析它们在感知、预测和控制中的角色，识别关键挑战和解决方案，提炼世界模型应具备的核心组件、能力和功能。
- Result: 论文系统性地分析了世界模型方法，识别了关键挑战和相应解决方案，提炼了真实世界模型应具备的核心组件、能力和功能。
- Conclusion: 基于分析结果，论文旨在为开发通用实用的机器人世界模型提供路线图，推动世界模型在复杂动态环境中的应用。


### [72] [TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](https://arxiv.org/abs/2511.02832)
*Yanjie Ze,Siheng Zhao,Weizhuo Wang,Angjoo Kanazawa,Rocky Duan,Pieter Abbeel,Guanya Shi,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: TWIST2是一个便携式、无需动作捕捉的人形机器人遥操作和数据收集系统，通过VR设备获取全身运动数据，使用低成本机器人颈部实现自我中心视觉，能够高效收集演示数据并训练分层视觉运动策略。

- Motivation: 当前人形机器人缺乏有效的数据收集框架，现有遥操作系统要么使用解耦控制，要么依赖昂贵的动作捕捉设备，限制了可扩展性。
- Method: 利用PICO4U VR获取实时全身人体运动数据，设计2自由度的低成本机器人颈部（约250美元）实现自我中心视觉，实现整体人形机器人控制。
- Result: 能够执行长时间范围的灵巧和移动技能，15分钟内收集100次演示且成功率接近100%。基于此构建的分层视觉运动策略能够自主控制完整人形机器人身体。
- Conclusion: TWIST2系统提供了可扩展的人形机器人数据收集和训练解决方案，整个系统完全可复现并开源，收集的数据集也已开源。
## physics.med-ph

### [73] [High-Resolution Magnetic Particle Imaging System Matrix Recovery Using a Vision Transformer with Residual Feature Network](https://arxiv.org/abs/2511.02212)
*Abuobaida M. Khair,Wenjing Jiang,Yousuf Babiker M. Osman,Wenjun Xia,Xiaopeng Ma*

Main category: physics.med-ph

TL;DR: VRF-Net是一种结合视觉变换器和残差特征网络的混合深度学习框架，用于磁粒子成像中高分辨率系统矩阵的恢复，在多个尺度下显著优于传统方法和CNN方法。

- Motivation: 磁粒子成像的分辨率常因下采样和线圈灵敏度变化而受限，需要一种能同时恢复大尺度结构和精细细节的方法。
- Method: 结合基于变换器的全局注意力和残差卷积细化，采用双阶段下采样策略模拟真实MPI条件，在Open MPI数据集和模拟数据集上进行配对图像超分辨率训练。
- Result: 在Open MPI数据集上，2倍缩放时达到nRMSE=0.403，pSNR=39.08 dB，SSIM=0.835；在模拟数据集上，2倍缩放时达到nRMSE=4.44，pSNR=28.52 dB，SSIM=0.771。相比插值和CNN方法，平均减少nRMSE 88.2%，提高pSNR 44.7%，改善SSIM 34.3%。
- Conclusion: VRF-Net能够实现更清晰、无伪影的系统矩阵恢复和鲁棒的图像重建，为未来体内应用提供了有前景的方向。
## eess.SP

### [74] [A Kullback-Leibler divergence method for input-system-state identification](https://arxiv.org/abs/2511.02426)
*Marios Impraimakis*

Main category: eess.SP

TL;DR: 提出了一种基于Kullback-Leibler散度的新方法，在卡尔曼滤波框架中选择最合理的输入-参数-状态估计结果，解决不同初始参数集导致不同结果的不确定性问题。

- Motivation: 系统识别中存在因不同初始参数集猜测导致结果不确定的问题，需要一种方法来选择最合理的估计结果。
- Method: 首先对多个不同初始参数集执行卡尔曼滤波，然后使用Kullback-Leibler散度同时比较后验分布与先验分布，最后选择Kullback-Leibler散度最小的识别结果。
- Result: 该方法在线性、非线性和有限信息应用中都能选择性能更好的识别结果。
- Conclusion: 该方法为系统监控提供了一个强大的工具，能够有效选择最合理的参数估计结果。


### [75] [An unscented Kalman filter method for real time input-parameter-state estimation](https://arxiv.org/abs/2511.02717)
*Marios Impraimakis,Andrew W. Smyth*

Main category: eess.SP

TL;DR: 提出了一种新型无迹卡尔曼滤波器，用于同时估计线性/非线性系统的输入、参数和状态，通过两阶段估计方法在输出数据下实现实时联合辨识。

- Motivation: 传统输出参数辨识方法无法同时估计系统输入、参数和状态，限制了系统理解的完整性。需要开发能够实时联合估计所有未知量的方法。
- Method: 采用两阶段无迹卡尔曼滤波器：第一阶段用预测的状态和参数估计输入，第二阶段用测量校正后的状态和参数进行最终估计。
- Result: 通过扰动分析证明，系统在至少有一个零或非零已知输入时可被唯一辨识。该方法比传统输出参数辨识策略能更好地理解系统。
- Conclusion: 该输出方法能够实时联合估计所有动态状态、参数和输入，为系统辨识提供了更全面的理解能力。
## cs.LG

### [76] [Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models](https://arxiv.org/abs/2511.01932)
*Haoming Wang,Wei Gao*

Main category: cs.LG

TL;DR: FineXL是一个为个性化图像生成模型提供细粒度自然语言解释的新技术，能够识别多个个性化方面及其不同程度的个性化水平。

- Motivation: 现有个性化模型缺乏解释性，特别是无法精确识别多个个性化方面及其不同程度的个性化水平，而自然语言解释更适合用户理解。
- Method: 提出FineXL技术，通过自然语言描述每个个性化方面，并提供量化分数来指示每个方面的个性化程度。
- Result: 实验结果显示，FineXL在不同个性化场景下应用于多种图像生成模型时，能将解释准确性提高56%。
- Conclusion: FineXL有效解决了现有方法在细粒度解释方面的局限性，为个性化图像生成模型提供了更精确的自然语言解释能力。


### [77] [OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning](https://arxiv.org/abs/2511.02205)
*Kevin Valencia,Thilina Balasooriya,Xihaier Luo,Shinjae Yoo,David Keetae Park*

Main category: cs.LG

TL;DR: OmniField是一个连续感知的多模态时空学习框架，能够处理稀疏、不规则、有噪声的测量数据，并适应训练和测试时任意可用的模态子集。

- Motivation: 解决真实世界实验数据中多模态时空学习面临的两个挑战：模态内测量稀疏、不规则、有噪声但跨模态相关；可用模态集在时空上变化，需要模型能够适应任意子集。
- Method: 提出OmniField框架，学习以可用模态为条件的连续神经场，通过多模态串扰块架构和迭代跨模态精炼在解码器前对齐信号，实现统一的重建、插值、预测和跨模态预测。
- Result: 广泛评估显示OmniField持续优于八个强大的多模态时空基线方法。在重度模拟传感器噪声下，性能仍接近干净输入水平，显示出对损坏测量的鲁棒性。
- Conclusion: OmniField提供了一个有效的多模态时空学习框架，能够处理真实世界数据中的稀疏性、噪声和模态变化问题，具有强大的鲁棒性和适应性。
## cs.DC

### [78] [3D Point Cloud Object Detection on Edge Devices for Split Computing](https://arxiv.org/abs/2511.02293)
*Taisuke Noguchi,Takuya Azumi*

Main category: cs.DC

TL;DR: 该研究利用分割计算技术优化自动驾驶中的3D目标检测，通过在点云体素化后或网络内部进行分割，显著降低了边缘设备的推理时间和功耗。

- Motivation: 解决自动驾驶中基于LiDAR点云的3D目标检测模型在边缘设备上计算复杂、处理时间长和功耗高的问题。
- Method: 采用分割计算这一分布式机器学习推理方法，在点云体素化后或深度神经网络内部进行模型分割，将部分计算任务转移到云端。
- Result: 体素化后分割使推理时间减少70.8%，边缘设备执行时间减少90.0%；网络内部分割使推理时间最多减少57.1%，边缘设备执行时间最多减少69.5%。
- Conclusion: 分割计算能有效降低自动驾驶3D目标检测在边缘设备上的计算负担，同时通过仅传输中间数据来降低数据泄露风险。
