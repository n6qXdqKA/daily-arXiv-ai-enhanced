{"id": "2504.16102", "pdf": "https://arxiv.org/pdf/2504.16102", "abs": "https://arxiv.org/abs/2504.16102", "authors": ["Xiwen Li", "Ross Whitaker", "Tolga Tasdizen"], "title": "Audio and Multiscale Visual Cues Driven Cross-modal Transformer for Idling Vehicle Detection", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Idling vehicle detection (IVD) supports real-time systems that reduce\npollution and emissions by dynamically messaging drivers to curb excess idling\nbehavior. In computer vision, IVD has become an emerging task that leverages\nvideo from surveillance cameras and audio from remote microphones to localize\nand classify vehicles in each frame as moving, idling, or engine-off. As with\nother cross-modal tasks, the key challenge lies in modeling the correspondence\nbetween audio and visual modalities, which differ in representation but provide\ncomplementary cues -- video offers spatial and motion context, while audio\nconveys engine activity beyond the visual field. The previous end-to-end model,\nwhich uses a basic attention mechanism, struggles to align these modalities\neffectively, often missing vehicle detections. To address this issue, we\npropose AVIVDNetv2, a transformer-based end-to-end detection network. It\nincorporates a cross-modal transformer with global patch-level learning, a\nmultiscale visual feature fusion module, and decoupled detection heads.\nExtensive experiments show that AVIVDNetv2 improves mAP by 7.66 over the\ndisjoint baseline and 9.42 over the E2E baseline, with consistent AP gains\nacross all vehicle categories. Furthermore, AVIVDNetv2 outperforms the\nstate-of-the-art method for sounding object localization, establishing a new\nperformance benchmark on the AVIVD dataset.", "AI": {"tldr": "AVIVDNetv2\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7aef\u5230\u7aef\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u8de8\u6a21\u6001Transformer\u548c\u591a\u5c3a\u5ea6\u89c6\u89c9\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95f2\u7f6e\u8f66\u8f86\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u5bf9\u9f50\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u95f2\u7f6e\u8f66\u8f86\u68c0\u6d4b\u7684\u51c6\u786e\u7387\u3002", "method": "\u63d0\u51faAVIVDNetv2\uff0c\u7ed3\u5408\u8de8\u6a21\u6001Transformer\u3001\u591a\u5c3a\u5ea6\u89c6\u89c9\u7279\u5f81\u878d\u5408\u6a21\u5757\u548c\u89e3\u8026\u68c0\u6d4b\u5934\u3002", "result": "mAP\u63d0\u53477.66\uff08\u76f8\u5bf9\u4e8e\u57fa\u7ebf\uff09\u548c9.42\uff08\u76f8\u5bf9\u4e8e\u7aef\u5230\u7aef\u57fa\u7ebf\uff09\uff0c\u5728\u6240\u6709\u8f66\u8f86\u7c7b\u522b\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u5f02\u3002", "conclusion": "AVIVDNetv2\u5728AVIVD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.16103", "pdf": "https://arxiv.org/pdf/2504.16103", "abs": "https://arxiv.org/abs/2504.16103", "authors": ["Oussema Dhaouadi", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "Shape Your Ground: Refining Road Surfaces Beyond Planar Representations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Road surface reconstruction from aerial images is fundamental for autonomous\ndriving, urban planning, and virtual simulation, where smoothness, compactness,\nand accuracy are critical quality factors. Existing reconstruction methods\noften produce artifacts and inconsistencies that limit usability, while\ndownstream tasks have a tendency to represent roads as planes for simplicity\nbut at the cost of accuracy. We introduce FlexRoad, the first framework to\ndirectly address road surface smoothing by fitting Non-Uniform Rational\nB-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric\nreconstructions or geodata providers. Our method at its core utilizes the\nElevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust\nanomaly correction, significantly reducing surface roughness and fitting\nerrors. To facilitate quantitative comparison between road surface\nreconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse\ncollection of road surface and terrain profiles derived from openly accessible\ngeodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D\nDataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used\nroad surface representations across various metrics while being insensitive to\nvarious input sources, terrains, and noise types. By performing ablation\nstudies, we identify the key role of each component towards high-quality\nreconstruction performance, making FlexRoad a generic method for realistic road\nsurface modeling.", "AI": {"tldr": "FlexRoad\u662f\u4e00\u4e2a\u901a\u8fc7NURBS\u66f2\u9762\u62df\u54083D\u9053\u8def\u70b9\u7684\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u8868\u9762\u7c97\u7cd9\u5ea6\u548c\u62df\u5408\u8bef\u5dee\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9053\u8def\u91cd\u5efa\u65b9\u6cd5\u5e38\u4ea7\u751f\u4f2a\u5f71\u548c\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u53ef\u7528\u6027\uff0c\u800c\u4e0b\u6e38\u4efb\u52a1\u5e38\u7b80\u5316\u9053\u8def\u4e3a\u5e73\u9762\uff0c\u727a\u7272\u51c6\u786e\u6027\u3002", "method": "FlexRoad\u5229\u7528ECSRC\u7b97\u6cd5\u8fdb\u884c\u5f02\u5e38\u6821\u6b63\uff0c\u5e76\u62df\u5408NURBS\u66f2\u9762\u52303D\u9053\u8def\u70b9\u3002", "result": "\u5728GeRoD\u548cDSC3D\u6570\u636e\u96c6\u4e0a\uff0cFlexRoad\u5728\u591a\u79cd\u6307\u6807\u4e0a\u4f18\u4e8e\u5e38\u7528\u65b9\u6cd5\uff0c\u4e14\u5bf9\u8f93\u5165\u6e90\u3001\u5730\u5f62\u548c\u566a\u58f0\u7c7b\u578b\u4e0d\u654f\u611f\u3002", "conclusion": "FlexRoad\u662f\u4e00\u79cd\u901a\u7528\u7684\u9ad8\u8d28\u91cf\u9053\u8def\u8868\u9762\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2504.16114", "pdf": "https://arxiv.org/pdf/2504.16114", "abs": "https://arxiv.org/abs/2504.16114", "authors": ["Johannes Ferner", "Stefan Huber", "Saverio Messineo", "Angel Pop", "Martin Uray"], "title": "Persistence-based Hough Transform for Line Detection", "categories": ["cs.CV"], "comment": "Accepted at iDSC'25, Salzburg, Austria", "summary": "The Hough transform is a popular and classical technique in computer vision\nfor the detection of lines (or more general objects). It maps a pixel into a\ndual space -- the Hough space: each pixel is mapped to the set of lines through\nthis pixel, which forms a curve in Hough space. The detection of lines then\nbecomes a voting process to find those lines that received many votes by\npixels. However, this voting is done by thresholding, which is susceptible to\nnoise and other artifacts.\n  In this work, we present an alternative voting technique to detect peaks in\nthe Hough space based on persistent homology, which very naturally addresses\nlimitations of simple thresholding. Experiments on synthetic data show that our\nmethod significantly outperforms the original method, while also demonstrating\nenhanced robustness.\n  This work seeks to inspire future research in two key directions. First, we\nhighlight the untapped potential of Topological Data Analysis techniques and\nadvocate for their broader integration into existing methods, including\nwell-established ones. Secondly, we initiate a discussion on the mathematical\nstability of the Hough transform, encouraging exploration of mathematically\ngrounded improvements to enhance its robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6301\u4e45\u540c\u8c03\u7684\u65b0\u6295\u7968\u6280\u672f\uff0c\u7528\u4e8e\u6539\u8fdb\u970d\u592b\u53d8\u6362\u4e2d\u7684\u5cf0\u503c\u68c0\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5e76\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u970d\u592b\u53d8\u6362\u901a\u8fc7\u9608\u503c\u6295\u7968\u68c0\u6d4b\u7ebf\u6761\uff0c\u6613\u53d7\u566a\u58f0\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u6301\u4e45\u540c\u8c03\u6280\u672f\u66ff\u4ee3\u7b80\u5355\u9608\u503c\u6295\u7968\uff0c\u4ee5\u66f4\u81ea\u7136\u5730\u68c0\u6d4b\u970d\u592b\u7a7a\u95f4\u4e2d\u7684\u5cf0\u503c\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u65b9\u6cd5\uff0c\u4e14\u9c81\u68d2\u6027\u66f4\u5f3a\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u62d3\u6251\u6570\u636e\u5206\u6790\u6280\u672f\u7684\u6f5c\u529b\uff0c\u5e76\u547c\u5401\u5c06\u5176\u66f4\u5e7f\u6cdb\u5730\u6574\u5408\u5230\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u540c\u65f6\u63a2\u8ba8\u970d\u592b\u53d8\u6362\u7684\u6570\u5b66\u7a33\u5b9a\u6027\u3002"}}
{"id": "2504.16117", "pdf": "https://arxiv.org/pdf/2504.16117", "abs": "https://arxiv.org/abs/2504.16117", "authors": ["Sridevi Polavaram", "Xin Zhou", "Meenu Ravi", "Mohammad Zarei", "Anmol Srivastava"], "title": "Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted to IEEE Conference for Artificial Intelligence, 2025", "summary": "Vision systems are increasingly deployed in critical domains such as\nsurveillance, law enforcement, and transportation. However, their\nvulnerabilities to rare or unforeseen scenarios pose significant safety risks.\nTo address these challenges, we introduce Context-Awareness and\nInterpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive\ndiscovery framework for failure cases (or CP - Critical Phenomena) detection\nand formalization. CAIRO by design incentivizes human-in-the-loop for testing\nand evaluation of criticality that arises from misdetections, adversarial\nattacks, and hallucinations in AI black-box models. Our robust analysis of\nobject detection model(s) failures in automated driving systems (ADS) showcases\nscalable and interpretable ways of formalizing the observed gaps between camera\nperception and real-world contexts, resulting in test cases stored as explicit\nknowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,\nlogical reasoning, and accountability.", "AI": {"tldr": "CAIRO\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u53c2\u4e0e\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u68c0\u6d4b\u548c\u5f62\u5f0f\u5316AI\u6a21\u578b\u4e2d\u7684\u7f55\u89c1\u6545\u969c\u6848\u4f8b\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89c6\u89c9\u7cfb\u7edf\u5728\u5173\u952e\u9886\u57df\uff08\u5982\u76d1\u63a7\u3001\u6267\u6cd5\u548c\u4ea4\u901a\uff09\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u5bf9\u7f55\u89c1\u6216\u672a\u77e5\u573a\u666f\u7684\u8106\u5f31\u6027\u5e26\u6765\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51faCAIRO\u6846\u67b6\uff0c\u57fa\u4e8e\u672c\u4f53\u8bba\u548c\u4eba\u7c7b\u8f85\u52a9\u53d1\u73b0\uff0c\u68c0\u6d4b\u548c\u5f62\u5f0f\u5316AI\u6a21\u578b\u4e2d\u7684\u5173\u952e\u73b0\u8c61\uff08CP\uff09\uff0c\u5305\u62ec\u8bef\u68c0\u6d4b\u3001\u5bf9\u6297\u653b\u51fb\u548c\u5e7b\u89c9\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\uff0cCAIRO\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u5c06\u6444\u50cf\u5934\u611f\u77e5\u4e0e\u73b0\u5b9e\u573a\u666f\u7684\u5dee\u8ddd\u5f62\u5f0f\u5316\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff08OWL/XML\u683c\u5f0f\uff09\u3002", "conclusion": "CAIRO\u901a\u8fc7\u4eba\u7c7b\u53c2\u4e0e\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u4e3aAI\u6a21\u578b\u7684\u6545\u969c\u68c0\u6d4b\u548c\u5f62\u5f0f\u5316\u63d0\u4f9b\u4e86\u53ef\u5171\u4eab\u3001\u53ef\u5206\u6790\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u5b89\u5168\u6027\u548c\u95ee\u8d23\u6027\u3002"}}
{"id": "2504.16115", "pdf": "https://arxiv.org/pdf/2504.16115", "abs": "https://arxiv.org/abs/2504.16115", "authors": ["Yibo Jacky Zhang", "Sanmi Koyejo"], "title": "A Framework for Objective-Driven Dynamical Stochastic Fields", "categories": ["cs.AI", "cs.LG", "cs.MA", "nlin.AO"], "comment": null, "summary": "Fields offer a versatile approach for describing complex systems composed of\ninteracting and dynamic components. In particular, some of these dynamical and\nstochastic systems may exhibit goal-directed behaviors aimed at achieving\nspecific objectives, which we refer to as $\\textit{intelligent fields}$.\nHowever, due to their inherent complexity, it remains challenging to develop a\nformal theoretical description of such systems and to effectively translate\nthese descriptions into practical applications. In this paper, we propose three\nfundamental principles -- complete configuration, locality, and purposefulness\n-- to establish a theoretical framework for understanding intelligent fields.\nMoreover, we explore methodologies for designing such fields from the\nperspective of artificial intelligence applications. This initial investigation\naims to lay the groundwork for future theoretical developments and practical\nadvances in understanding and harnessing the potential of such objective-driven\ndynamical stochastic fields.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u57fa\u672c\u539f\u5219\uff08\u5b8c\u5168\u914d\u7f6e\u3001\u5c40\u90e8\u6027\u548c\u76ee\u7684\u6027\uff09\u6765\u7406\u89e3\u548c\u8bbe\u8ba1\u667a\u80fd\u573a\uff0c\u65e8\u5728\u4e3a\u672a\u6765\u7406\u8bba\u548c\u5b9e\u8df5\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u590d\u6742\u52a8\u6001\u968f\u673a\u7cfb\u7edf\uff08\u667a\u80fd\u573a\uff09\u7684\u56fa\u6709\u590d\u6742\u6027\u4f7f\u5f97\u5176\u7406\u8bba\u63cf\u8ff0\u548c\u5b9e\u9645\u5e94\u7528\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u57fa\u672c\u539f\u5219\uff08\u5b8c\u5168\u914d\u7f6e\u3001\u5c40\u90e8\u6027\u548c\u76ee\u7684\u6027\uff09\u4f5c\u4e3a\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u4ece\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u89d2\u5ea6\u63a2\u8ba8\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u521d\u6b65\u5efa\u7acb\u4e86\u667a\u80fd\u573a\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3\u548c\u5229\u7528\u76ee\u6807\u9a71\u52a8\u7684\u52a8\u6001\u968f\u673a\u573a\uff08\u667a\u80fd\u573a\uff09\u5960\u5b9a\u4e86\u521d\u6b65\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2504.16127", "pdf": "https://arxiv.org/pdf/2504.16127", "abs": "https://arxiv.org/abs/2504.16127", "authors": ["Xingxing Zuo", "Nikhil Ranganathan", "Connor Lee", "Georgia Gkioxari", "Soon-Jo Chung"], "title": "MonoTher-Depth: Enhancing Thermal Depth Estimation via Confidence-Aware Distillation", "categories": ["cs.CV", "cs.RO"], "comment": "8 Pages; The code will be available at\n  https://github.com/ZuoJiaxing/monother_depth", "summary": "Monocular depth estimation (MDE) from thermal images is a crucial technology\nfor robotic systems operating in challenging conditions such as fog, smoke, and\nlow light. The limited availability of labeled thermal data constrains the\ngeneralization capabilities of thermal MDE models compared to foundational RGB\nMDE models, which benefit from datasets of millions of images across diverse\nscenarios. To address this challenge, we introduce a novel pipeline that\nenhances thermal MDE through knowledge distillation from a versatile RGB MDE\nmodel. Our approach features a confidence-aware distillation method that\nutilizes the predicted confidence of the RGB MDE to selectively strengthen the\nthermal MDE model, capitalizing on the strengths of the RGB model while\nmitigating its weaknesses. Our method significantly improves the accuracy of\nthe thermal MDE, independent of the availability of labeled depth supervision,\nand greatly expands its applicability to new scenarios. In our experiments on\nnew scenarios without labeled depth, the proposed confidence-aware distillation\nmethod reduces the absolute relative error of thermal MDE by 22.88\\% compared\nto the baseline without distillation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4eceRGB\u6a21\u578b\u589e\u5f3a\u70ed\u56fe\u50cf\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u65b0\u573a\u666f\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u70ed\u56fe\u50cfMDE\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\uff08\u5982\u96fe\u3001\u70df\u3001\u4f4e\u5149\uff09\u5bf9\u673a\u5668\u4eba\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u6807\u8bb0\u6570\u636e\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7f6e\u4fe1\u611f\u77e5\u84b8\u998f\u65b9\u6cd5\uff0c\u5229\u7528RGB MDE\u6a21\u578b\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u9009\u62e9\u6027\u589e\u5f3a\u70edMDE\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u6807\u8bb0\u6df1\u5ea6\u6570\u636e\u7684\u65b0\u573a\u666f\u4e2d\uff0c\u5c06\u70edMDE\u7684\u76f8\u5bf9\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e\u4e8622.88%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u70edMDE\u7684\u51c6\u786e\u6027\uff0c\u4e14\u4e0d\u4f9d\u8d56\u6807\u8bb0\u6df1\u5ea6\u6570\u636e\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2504.16209", "pdf": "https://arxiv.org/pdf/2504.16209", "abs": "https://arxiv.org/abs/2504.16209", "authors": ["Paul Zaidins", "Robert P. Goldman", "Ugur Kuter", "Dana Nau", "Mark Roberts"], "title": "HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods", "categories": ["cs.AI", "I.2.8"], "comment": "20 pages; 19 figures; To appear in the Proceedings for ICAPS 2025,\n  the 35th International Conference on Automated Planning and Schedulings", "summary": "This paper provides theoretical and empirical comparisons of three recent\nhierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our\ntheoretical results show that the three algorithms correspond to three\ndifferent definitions of the plan repair problem, leading to differences in the\nalgorithms' search spaces, the repair problems they can solve, and the kinds of\nrepairs they can make. Understanding these distinctions is important when\nchoosing a repair method for any given application.\n  Building on the theoretical results, we evaluate the algorithms empirically\nin a series of benchmark planning problems. Our empirical results provide more\ndetailed insight into the runtime repair performance of these systems and the\ncoverage of the repair problems solved, based on algorithmic properties such as\nreplanning, chronological backtracking, and backjumping over plan trees.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u5206\u5c42\u8ba1\u5212\u4fee\u590d\u7b97\u6cd5\uff08SHOPFixer\u3001IPyHOPPER\u548cRewrite\uff09\u7684\u7406\u8bba\u4e0e\u5b9e\u8bc1\u5dee\u5f02\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u641c\u7d22\u7a7a\u95f4\u3001\u4fee\u590d\u80fd\u529b\u548c\u9002\u7528\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u4e0d\u540c\u8ba1\u5212\u4fee\u590d\u7b97\u6cd5\u7684\u7406\u8bba\u5dee\u5f02\u53ca\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u4fbf\u4e3a\u5177\u4f53\u5e94\u7528\u9009\u62e9\u5408\u9002\u7684\u4fee\u590d\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\uff08\u57fa\u51c6\u89c4\u5212\u95ee\u9898\uff09\u6bd4\u8f83\u4e09\u79cd\u7b97\u6cd5\u7684\u641c\u7d22\u7a7a\u95f4\u3001\u4fee\u590d\u80fd\u529b\u548c\u6027\u80fd\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u8868\u660e\u4e09\u79cd\u7b97\u6cd5\u5bf9\u5e94\u4e0d\u540c\u7684\u4fee\u590d\u95ee\u9898\u5b9a\u4e49\uff0c\u5b9e\u8bc1\u7ed3\u679c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u8fd0\u884c\u65f6\u6027\u80fd\u548c\u95ee\u9898\u8986\u76d6\u7387\u4e0a\u7684\u5dee\u5f02\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u4fee\u590d\u7b97\u6cd5\u9700\u57fa\u4e8e\u5176\u7406\u8bba\u5b9a\u4e49\u548c\u5b9e\u9645\u6027\u80fd\uff0c\u5177\u4f53\u5e94\u7528\u573a\u666f\u51b3\u5b9a\u6700\u4f73\u9009\u62e9\u3002"}}
{"id": "2504.16128", "pdf": "https://arxiv.org/pdf/2504.16128", "abs": "https://arxiv.org/abs/2504.16128", "authors": ["Stanley Mugisha", "Rashid Kisitu", "Florence Tushabe"], "title": "Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.10; I.4.9"], "comment": "12 pages and 4 figures", "summary": "Integrating deep learning applications into agricultural IoT systems faces a\nserious challenge of balancing the high accuracy of Vision Transformers (ViTs)\nwith the efficiency demands of resource-constrained edge devices. Large\ntransformer models like the Swin Transformers excel in plant disease\nclassification by capturing global-local dependencies. However, their\ncomputational complexity (34.1 GFLOPs) limits applications and renders them\nimpractical for real-time on-device inference. Lightweight models such as\nMobileNetV3 and TinyML would be suitable for on-device inference but lack the\nrequired spatial reasoning for fine-grained disease detection. To bridge this\ngap, we propose a hybrid knowledge distillation framework that synergistically\ntransfers logit and attention knowledge from a Swin Transformer teacher to a\nMobileNetV3 student model. Our method includes the introduction of adaptive\nattention alignment to resolve cross-architecture mismatch (resolution,\nchannels) and a dual-loss function optimizing both class probabilities and\nspatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled\nMobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%\nreduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU\nand 86ms/image on smartphone CPUs). Key innovations include IoT-centric\nvalidation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching\nattention maps. Comparative experiments show significant improvements over\nstandalone CNNs and prior distillation methods, with a 3.5% accuracy gain over\nMobileNetV3 baselines. Significantly, this work advances real-time,\nenergy-efficient crop monitoring in precision agriculture and demonstrates how\nwe can attain ViT-level diagnostic precision on edge devices. Code and models\nwill be made available for replication after acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5c06Swin Transformer\u7684\u9ad8\u7cbe\u5ea6\u4e0eMobileNetV3\u7684\u9ad8\u6548\u6027\u7ed3\u5408\uff0c\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u3002", "motivation": "\u519c\u4e1a\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\uff0cVision Transformers\uff08ViTs\uff09\u7684\u9ad8\u7cbe\u5ea6\u4e0e\u8fb9\u7f18\u8bbe\u5907\u7684\u6548\u7387\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\u3002Swin Transformers\u8ba1\u7b97\u590d\u6742\uff0c\u800c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5982MobileNetV3\u7f3a\u4e4f\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u5bf9\u9f50\u548c\u53cc\u635f\u5931\u51fd\u6570\uff0c\u5c06Swin Transformer\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230MobileNetV3\u4e2d\u3002", "result": "\u5728lantVillage-Tomato\u6570\u636e\u96c6\u4e0a\uff0c\u84b8\u998f\u540e\u7684MobileNetV3\u8fbe\u523092.4%\u7684\u51c6\u786e\u7387\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1195%\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e82%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u9ad8\u6548\u4f5c\u7269\u76d1\u6d4b\uff0c\u5c55\u793a\u4e86ViT\u7ea7\u8bca\u65ad\u7cbe\u5ea6\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2504.16273", "pdf": "https://arxiv.org/pdf/2504.16273", "abs": "https://arxiv.org/abs/2504.16273", "authors": ["Joseph Lee", "Tianqi Shang", "Jae Young Baik", "Duy Duong-Tran", "Shu Yang", "Lingyao Li", "Li Shen"], "title": "Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases", "categories": ["cs.AI", "cs.HC"], "comment": "Accepted to GenAI4Health Workshop @ AAAI 2025", "summary": "Large Language Models (LLMs) have shown promise in clinical decision support,\nyet their application to triage remains underexplored. We systematically\ninvestigate the capabilities of LLMs in emergency department triage through two\nkey dimensions: (1) robustness to distribution shifts and missing data, and (2)\ncounterfactual analysis of intersectional biases across sex and race. We assess\nmultiple LLM-based approaches, ranging from continued pre-training to\nin-context learning, as well as machine learning approaches. Our results\nindicate that LLMs exhibit superior robustness, and we investigate the key\nfactors contributing to the promising LLM-based approaches. Furthermore, in\nthis setting, we identify gaps in LLM preferences that emerge in particular\nintersections of sex and race. LLMs generally exhibit sex-based differences,\nbut they are most pronounced in certain racial groups. These findings suggest\nthat LLMs encode demographic preferences that may emerge in specific clinical\ncontexts or particular combinations of characteristics.", "AI": {"tldr": "LLMs\u5728\u6025\u8bca\u5206\u8bca\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u6027\u522b\u548c\u79cd\u65cf\u7684\u4ea4\u53c9\u5206\u6790\u4e2d\u663e\u793a\u51fa\u504f\u597d\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u6025\u8bca\u5206\u8bca\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5206\u5e03\u504f\u79fb\u3001\u7f3a\u5931\u6570\u636e\u4ee5\u53ca\u4ea4\u53c9\u504f\u89c1\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b49\u591a\u79cdLLM\u65b9\u6cd5\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5176\u5728\u6025\u8bca\u5206\u8bca\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6027\u522b\u548c\u79cd\u65cf\u7684\u7279\u5b9a\u7ec4\u5408\u4e2d\u663e\u793a\u51fa\u504f\u597d\u5dee\u5f02\u3002", "conclusion": "LLMs\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6ce8\u610f\u5176\u53ef\u80fd\u9690\u542b\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u597d\u3002"}}
{"id": "2504.16134", "pdf": "https://arxiv.org/pdf/2504.16134", "abs": "https://arxiv.org/abs/2504.16134", "authors": ["Mohammad Abu Tami", "Mohammed Elhenawy", "Huthaifa I. Ashqar"], "title": "Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Traffic safety remains a critical global challenge, with traditional Advanced\nDriver-Assistance Systems (ADAS) often struggling in dynamic real-world\nscenarios due to fragmented sensor processing and susceptibility to adversarial\nconditions. This paper reviews the transformative potential of Multimodal Large\nLanguage Models (MLLMs) in addressing these limitations by integrating\ncross-modal data such as visual, spatial, and environmental inputs to enable\nholistic scene understanding. Through a comprehensive analysis of MLLM-based\napproaches, we highlight their capabilities in enhancing perception,\ndecision-making, and adversarial robustness, while also examining the role of\nkey datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research.\nFurthermore, we outline future directions, including real-time edge deployment,\ncausality-driven reasoning, and human-AI collaboration. By positioning MLLMs as\na cornerstone for next-generation traffic safety systems, this review\nunderscores their potential to revolutionize the field, offering scalable,\ncontext-aware solutions that proactively mitigate risks and improve overall\nroad safety.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u6027\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u5b9e\u73b0\u5168\u9762\u573a\u666f\u7406\u89e3\uff0c\u5f25\u8865\u4f20\u7edfADAS\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edfADAS\u5728\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u66f4\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u6790MLLM\u65b9\u6cd5\uff0c\u6574\u5408\u89c6\u89c9\u3001\u7a7a\u95f4\u548c\u73af\u5883\u6570\u636e\uff0c\u63d0\u5347\u611f\u77e5\u3001\u51b3\u7b56\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "result": "MLLMs\u80fd\u663e\u8457\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u6027\uff0c\u652f\u6301\u5b9e\u65f6\u8fb9\u7f18\u90e8\u7f72\u548c\u56e0\u679c\u63a8\u7406\u3002", "conclusion": "MLLMs\u6709\u671b\u6210\u4e3a\u4e0b\u4e00\u4ee3\u4ea4\u901a\u5b89\u5168\u7cfb\u7edf\u7684\u6838\u5fc3\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16622", "pdf": "https://arxiv.org/pdf/2504.16622", "abs": "https://arxiv.org/abs/2504.16622", "authors": ["Christoforus Yoga Haryanto", "Emily Lomempow"], "title": "Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems", "categories": ["cs.AI", "cs.CY"], "comment": "Working Paper, 37 pages, 1 figure, 5 tables", "summary": "Autonomous AI systems reveal foundational limitations in deterministic,\nhuman-authored computing architectures. This paper presents Cognitive Silicon:\na hypothetical full-stack architectural framework projected toward 2035,\nexploring a possible trajectory for cognitive computing system design. The\nproposed architecture would integrate symbolic scaffolding, governed memory,\nruntime moral coherence, and alignment-aware execution across\nsilicon-to-semantics layers. Our design grammar has emerged from dialectical\nco-design with LLMs under asymmetric epistemic conditions--creating structured\nfriction to expose blind spots and trade-offs. The envisioned framework would\nestablish mortality as a natural consequence of physical constraints,\nnon-copyable tacit knowledge, and non-cloneable identity keys as\ncognitive-embodiment primitives. Core tensions (trust/agency,\nscaffolding/emergence, execution/governance) would function as central\narchitectural pressures rather than edge cases. The architecture theoretically\nconverges with the Free Energy Principle, potentially offering a formal account\nof how cognitive systems could maintain identity through prediction error\nminimization across physical and computational boundaries. The resulting\nframework aims to deliver a morally tractable cognitive infrastructure that\ncould maintain human-alignment through irreversible hardware constraints and\nidentity-bound epistemic mechanisms resistant to replication or subversion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cCognitive Silicon\u201d\u7684\u5047\u8bbe\u6027\u5168\u6808\u67b6\u6784\u6846\u67b6\uff0c\u63a2\u7d22\u4e86\u8ba4\u77e5\u8ba1\u7b97\u7cfb\u7edf\u8bbe\u8ba1\u7684\u672a\u6765\u8def\u5f84\uff0c\u65e8\u5728\u89e3\u51b3\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u81ea\u4e3bAI\u7cfb\u7edf\u5728\u786e\u5b9a\u6027\u3001\u4eba\u7c7b\u7f16\u5199\u7684\u8ba1\u7b97\u67b6\u6784\u4e2d\u5b58\u5728\u57fa\u7840\u6027\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u67b6\u6784\u6846\u67b6\u6765\u652f\u6301\u8ba4\u77e5\u8ba1\u7b97\u3002", "method": "\u901a\u8fc7\u7b26\u53f7\u652f\u67b6\u3001\u53d7\u63a7\u5185\u5b58\u3001\u8fd0\u884c\u65f6\u9053\u5fb7\u4e00\u81f4\u6027\u548c\u5bf9\u9f50\u611f\u77e5\u6267\u884c\u7b49\u8bbe\u8ba1\u5143\u7d20\uff0c\u7ed3\u5408\u7845\u5230\u8bed\u4e49\u7684\u5c42\u6b21\u96c6\u6210\uff0c\u6784\u5efa\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u7406\u8bba\u4e0a\u4e0e\u81ea\u7531\u80fd\u539f\u7406\u4e00\u81f4\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u89e3\u91ca\uff0c\u8bf4\u660e\u8ba4\u77e5\u7cfb\u7edf\u5982\u4f55\u901a\u8fc7\u9884\u6d4b\u8bef\u5dee\u6700\u5c0f\u5316\u7ef4\u6301\u8eab\u4efd\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9053\u5fb7\u53ef\u5904\u7406\u7684\u8ba4\u77e5\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u4e0d\u53ef\u9006\u7684\u786c\u4ef6\u7ea6\u675f\u548c\u6297\u590d\u5236\u7684\u8eab\u4efd\u673a\u5236\u4fdd\u6301\u4e0e\u4eba\u7c7b\u7684\u5bf9\u9f50\u3002"}}
{"id": "2504.16145", "pdf": "https://arxiv.org/pdf/2504.16145", "abs": "https://arxiv.org/abs/2504.16145", "authors": ["Jingchao Wang", "Hong Wang", "Wenlong Zhang", "Kunhua Ji", "Dingjiang Huang", "Yefeng Zheng"], "title": "Progressive Language-guided Visual Learning for Multi-Task Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring\nExpression Comprehension (REC) and Referring Expression Segmentation (RES). The\nexisting representative approaches generally follow the research pipeline which\nmainly consists of three core procedures, including independent feature\nextraction for visual and linguistic modalities, respectively, cross-modal\ninteraction module, and independent prediction heads for different sub-tasks.\nAlbeit achieving remarkable performance, this research line has two\nlimitations: 1) The linguistic content has not been fully injected into the\nentire visual backbone for boosting more effective visual feature extraction\nand it needs an extra cross-modal interaction module; 2) The relationship\nbetween REC and RES tasks is not effectively exploited to help the\ncollaborative prediction for more accurate output. To deal with these problems,\nin this paper, we propose a Progressive Language-guided Visual Learning\nframework for multi-task visual grounding, called PLVL, which not only finely\nmine the inherent feature expression of the visual modality itself but also\nprogressively inject the language information to help learn linguistic-related\nvisual features. In this manner, our PLVL does not need additional cross-modal\nfusion module while fully introducing the language guidance. Furthermore, we\nanalyze that the localization center for REC would help identify the\nto-be-segmented object region for RES to some extent. Inspired by this\ninvestigation, we design a multi-task head to accomplish collaborative\npredictions for these two sub-tasks. Extensive experiments conducted on several\nbenchmark datasets comprehensively substantiate that our PLVL obviously\noutperforms the representative methods in both REC and RES tasks.\nhttps://github.com/jcwang0602/PLVL", "AI": {"tldr": "PLVL\u6846\u67b6\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bed\u8a00\u5f15\u5bfc\u89c6\u89c9\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u8bed\u8a00\u4fe1\u606f\u672a\u5145\u5206\u6ce8\u5165\u89c6\u89c9\u7279\u5f81\u53ca\u5b50\u4efb\u52a1\u5173\u7cfb\u672a\u6709\u6548\u5229\u7528\u7684\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8de8\u6a21\u6001\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u8a00\u4fe1\u606f\u6ce8\u5165\u89c6\u89c9\u7279\u5f81\u548c\u5b50\u4efb\u52a1\u534f\u4f5c\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u591a\u4efb\u52a1\u89c6\u89c9\u5b9a\u4f4d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faPLVL\u6846\u67b6\uff0c\u6e10\u8fdb\u5f0f\u6ce8\u5165\u8bed\u8a00\u4fe1\u606f\u5e76\u8bbe\u8ba1\u591a\u4efb\u52a1\u5934\u5b9e\u73b0REC\u548cRES\u7684\u534f\u4f5c\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cPLVL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PLVL\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u548c\u591a\u4efb\u52a1\u534f\u4f5c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u89c6\u89c9\u5b9a\u4f4d\u7684\u6027\u80fd\u3002"}}
{"id": "2504.16635", "pdf": "https://arxiv.org/pdf/2504.16635", "abs": "https://arxiv.org/abs/2504.16635", "authors": ["Fredy Pokou", "Jules Sadefo Kamdem", "Fran\u00e7ois Benhmad"], "title": "Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models", "categories": ["cs.AI", "q-fin.CP", "q-fin.RM", "q-fin.ST"], "comment": null, "summary": "In an environment of increasingly volatile financial markets, the accurate\nestimation of risk remains a major challenge. Traditional econometric models,\nsuch as GARCH and its variants, are based on assumptions that are often too\nrigid to adapt to the complexity of the current market dynamics. To overcome\nthese limitations, we propose a hybrid framework for Value-at-Risk (VaR)\nestimation, combining GARCH volatility models with deep reinforcement learning.\nOur approach incorporates directional market forecasting using the Double Deep\nQ-Network (DDQN) model, treating the task as an imbalanced classification\nproblem. This architecture enables the dynamic adjustment of risk-level\nforecasts according to market conditions. Empirical validation on daily\nEurostoxx 50 data covering periods of crisis and high volatility shows a\nsignificant improvement in the accuracy of VaR estimates, as well as a\nreduction in the number of breaches and also in capital requirements, while\nrespecting regulatory risk thresholds. The ability of the model to adjust risk\nlevels in real time reinforces its relevance to modern and proactive risk\nmanagement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GARCH\u6a21\u578b\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u98ce\u9669\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86VaR\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfGARCH\u6a21\u578b\u5047\u8bbe\u8fc7\u4e8e\u521a\u6027\uff0c\u96be\u4ee5\u9002\u5e94\u590d\u6742\u5e02\u573a\u52a8\u6001\uff0c\u9700\u66f4\u7075\u6d3b\u7684\u98ce\u9669\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408GARCH\u548cDouble Deep Q-Network\uff08DDQN\uff09\u6a21\u578b\uff0c\u5c06\u4efb\u52a1\u89c6\u4e3a\u4e0d\u5e73\u8861\u5206\u7c7b\u95ee\u9898\u3002", "result": "\u5728Eurostoxx 50\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cVaR\u4f30\u8ba1\u51c6\u786e\u6027\u63d0\u5347\uff0c\u8fdd\u7ea6\u6b21\u6570\u548c\u8d44\u672c\u9700\u6c42\u51cf\u5c11\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u5b9e\u65f6\u8c03\u6574\u98ce\u9669\u6c34\u5e73\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u4e3b\u52a8\u98ce\u9669\u7ba1\u7406\u3002"}}
{"id": "2504.16150", "pdf": "https://arxiv.org/pdf/2504.16150", "abs": "https://arxiv.org/abs/2504.16150", "authors": ["Sarah Day", "Jesse Dimino", "Matt Jester", "Kaitlin Keegan", "Thomas Weighill"], "title": "Classification of Firn Data via Topological Features", "categories": ["cs.CV", "math.AT", "55N31, 68T45"], "comment": null, "summary": "In this paper we evaluate the performance of topological features for\ngeneralizable and robust classification of firn image data, with the broader\ngoal of understanding the advantages, pitfalls, and trade-offs in topological\nfeaturization. Firn refers to layers of granular snow within glaciers that\nhaven't been compressed into ice. This compactification process imposes\ndistinct topological and geometric structure on firn that varies with depth\nwithin the firn column, making topological data analysis (TDA) a natural choice\nfor understanding the connection between depth and structure. We use two\nclasses of topological features, sublevel set features and distance transform\nfeatures, together with persistence curves, to predict sample depth from\nmicroCT images. A range of challenging training-test scenarios reveals that no\none choice of method dominates in all categories, and uncoveres a web of\ntrade-offs between accuracy, interpretability, and generalizability.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u62d3\u6251\u7279\u5f81\u5728\u96ea\u7c92\u56fe\u50cf\u6570\u636e\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u63a2\u8ba8\u4e86\u62d3\u6251\u7279\u5f81\u5316\u7684\u4f18\u52bf\u3001\u5c40\u9650\u6027\u548c\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u96ea\u7c92\uff08firn\uff09\u7684\u538b\u7f29\u8fc7\u7a0b\u5982\u4f55\u901a\u8fc7\u6df1\u5ea6\u5f71\u54cd\u5176\u62d3\u6251\u548c\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u63a2\u7d22\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u5728\u6b64\u8fc7\u7a0b\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u4e24\u7c7b\u62d3\u6251\u7279\u5f81\uff08\u5b50\u6c34\u5e73\u96c6\u7279\u5f81\u548c\u8ddd\u79bb\u53d8\u6362\u7279\u5f81\uff09\u53ca\u6301\u4e45\u66f2\u7ebf\uff0c\u4ece\u5faeCT\u56fe\u50cf\u9884\u6d4b\u6837\u672c\u6df1\u5ea6\u3002", "result": "\u4e0d\u540c\u8bad\u7ec3-\u6d4b\u8bd5\u573a\u666f\u663e\u793a\uff0c\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u7c7b\u522b\u4e2d\u5360\u4f18\uff0c\u63ed\u793a\u4e86\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u4e4b\u95f4\u7684\u590d\u6742\u6743\u8861\u3002", "conclusion": "\u62d3\u6251\u7279\u5f81\u5728\u96ea\u7c92\u56fe\u50cf\u5206\u7c7b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6839\u636e\u5177\u4f53\u9700\u6c42\u6743\u8861\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002"}}
{"id": "2504.16728", "pdf": "https://arxiv.org/pdf/2504.16728", "abs": "https://arxiv.org/abs/2504.16728", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery", "categories": ["cs.AI", "cs.CL"], "comment": "6 pages main-text, 2 pages appendix", "summary": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System", "AI": {"tldr": "IRIS\u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548cLLM\u8f85\u52a9\uff0c\u589e\u5f3a\u79d1\u5b66\u7814\u7a76\u7684\u5047\u8bbe\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u5316\u5047\u8bbe\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u53ef\u63a7\u6027\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u4eba\u673a\u534f\u540c\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1IRIS\u5e73\u53f0\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u3001\u7ec6\u7c92\u5ea6\u53cd\u9988\u673a\u5236\u548c\u57fa\u4e8e\u67e5\u8be2\u7684\u6587\u732e\u5408\u6210\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cIRIS\u80fd\u6709\u6548\u63d0\u5347\u7814\u7a76\u4eba\u5458\u7684\u521b\u610f\u751f\u6210\u80fd\u529b\u3002", "conclusion": "IRIS\u901a\u8fc7\u4eba\u673a\u534f\u540c\u65b9\u6cd5\uff0c\u4e3a\u79d1\u5b66\u5047\u8bbe\u751f\u6210\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16171", "pdf": "https://arxiv.org/pdf/2504.16171", "abs": "https://arxiv.org/abs/2504.16171", "authors": ["Zezhang Yang", "Zitong Yu", "Nuri Choi", "Abhinav K. Jha"], "title": "A detection-task-specific deep-learning method to improve the quality of sparse-view myocardial perfusion SPECT images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Myocardial perfusion imaging (MPI) with single-photon emission computed\ntomography (SPECT) is a widely used and cost-effective diagnostic tool for\ncoronary artery disease. However, the lengthy scanning time in this imaging\nprocedure can cause patient discomfort, motion artifacts, and potentially\ninaccurate diagnoses due to misalignment between the SPECT scans and the\nCT-scans which are acquired for attenuation compensation. Reducing projection\nangles is a potential way to shorten scanning time, but this can adversely\nimpact the quality of the reconstructed images. To address this issue, we\npropose a detection-task-specific deep-learning method for sparse-view MPI\nSPECT images. This method integrates an observer loss term that penalizes the\nloss of anthropomorphic channel features with the goal of improving performance\nin perfusion defect-detection task. We observed that, on the task of detecting\nmyocardial perfusion defects, the proposed method yielded an area under the\nreceiver operating characteristic (ROC) curve (AUC) significantly larger than\nthe sparse-view protocol. Further, the proposed method was observed to be able\nto restore the structure of the left ventricle wall, demonstrating ability to\novercome sparse-sampling artifacts. Our preliminary results motivate further\nevaluations of the method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7a00\u758f\u89c6\u89d2\u5fc3\u808c\u704c\u6ce8\u6210\u50cf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u704c\u6ce8\u7f3a\u9677\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u7f29\u77ed\u4e86\u626b\u63cf\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edfSPECT\u6210\u50cf\u626b\u63cf\u65f6\u95f4\u957f\uff0c\u53ef\u80fd\u5bfc\u81f4\u60a3\u8005\u4e0d\u9002\u548c\u8bca\u65ad\u8bef\u5dee\uff0c\u51cf\u5c11\u6295\u5f71\u89d2\u5ea6\u867d\u80fd\u7f29\u77ed\u65f6\u95f4\u4f46\u4f1a\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u4efb\u52a1\u7279\u5b9a\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c2\u5bdf\u8005\u635f\u5931\u9879\u4ee5\u4f18\u5316\u704c\u6ce8\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u68c0\u6d4b\u5fc3\u808c\u704c\u6ce8\u7f3a\u9677\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86AUC\u503c\uff0c\u5e76\u80fd\u6062\u590d\u5de6\u5fc3\u5ba4\u58c1\u7ed3\u6784\u3002", "conclusion": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u8bc4\u4f30\u3002"}}
{"id": "2504.16736", "pdf": "https://arxiv.org/pdf/2504.16736", "abs": "https://arxiv.org/abs/2504.16736", "authors": ["Yingxuan Yang", "Huacan Chai", "Yuanyi Song", "Siyuan Qi", "Muning Wen", "Ning Li", "Junwei Liao", "Haoyi Hu", "Jianghao Lin", "Gaowei Chang", "Weiwen Liu", "Ying Wen", "Yong Yu", "Weinan Zhang"], "title": "A Survey of AI Agent Protocols", "categories": ["cs.AI"], "comment": null, "summary": "The rapid development of large language models (LLMs) has led to the\nwidespread deployment of LLM agents across diverse industries, including\ncustomer service, content generation, data analysis, and even healthcare.\nHowever, as more LLM agents are deployed, a major issue has emerged: there is\nno standard way for these agents to communicate with external tools or data\nsources. This lack of standardized protocols makes it difficult for agents to\nwork together or scale effectively, and it limits their ability to tackle\ncomplex, real-world tasks. A unified communication protocol for LLM agents\ncould change this. It would allow agents and tools to interact more smoothly,\nencourage collaboration, and triggering the formation of collective\nintelligence. In this paper, we provide a systematic overview of existing\ncommunication protocols for LLM agents. We classify them into four main\ncategories and make an analysis to help users and developers select the most\nsuitable protocols for specific applications. Additionally, we conduct a\ncomparative performance analysis of these protocols across key dimensions such\nas security, scalability, and latency. Finally, we explore future challenges,\nsuch as how protocols can adapt and survive in fast-evolving environments, and\nwhat qualities future protocols might need to support the next generation of\nLLM agent ecosystems. We expect this work to serve as a practical reference for\nboth researchers and engineers seeking to design, evaluate, or integrate robust\ncommunication infrastructures for intelligent agents.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u4e4b\u95f4\u7f3a\u4e4f\u6807\u51c6\u5316\u901a\u4fe1\u534f\u8bae\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u534f\u8bae\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5bf9\u73b0\u6709\u534f\u8bae\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u6027\u80fd\u5206\u6790\uff0c\u540c\u65f6\u5c55\u671b\u4e86\u672a\u6765\u6311\u6218\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u5404\u884c\u4e1a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u901a\u4fe1\u534f\u8bae\u9650\u5236\u4e86\u5176\u534f\u4f5c\u548c\u6269\u5c55\u80fd\u529b\uff0c\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u63d0\u5347\u4ee3\u7406\u7684\u96c6\u4f53\u667a\u80fd\u3002", "method": "\u5bf9\u73b0\u6709LLM\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\uff0c\u5e76\u4ece\u5b89\u5168\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5ef6\u8fdf\u7b49\u7ef4\u5ea6\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u63d0\u4f9b\u4e86\u56db\u7c7b\u4e3b\u8981\u901a\u4fe1\u534f\u8bae\u7684\u6982\u8ff0\uff0c\u5e76\u5206\u6790\u4e86\u5176\u9002\u7528\u573a\u666f\u548c\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u7528\u6237\u63d0\u4f9b\u9009\u62e9\u4f9d\u636e\u3002", "conclusion": "\u7edf\u4e00\u901a\u4fe1\u534f\u8bae\u6709\u671b\u4fc3\u8fdbLLM\u4ee3\u7406\u7684\u534f\u4f5c\u4e0e\u667a\u80fd\u63d0\u5347\uff0c\u672a\u6765\u9700\u89e3\u51b3\u534f\u8bae\u5728\u5feb\u901f\u53d8\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002"}}
{"id": "2504.16181", "pdf": "https://arxiv.org/pdf/2504.16181", "abs": "https://arxiv.org/abs/2504.16181", "authors": ["Banafsheh Karimian", "Giulia Avanzato", "Soufian Belharbi", "Luke McCaffrey", "Mohammadhadi Shateri", "Eric Granger"], "title": "CLIP-IT: CLIP-based Pairing for Histology Images Classification", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal learning has shown significant promise for improving medical image\nanalysis by integrating information from complementary data sources. This is\nwidely employed for training vision-language models (VLMs) for cancer detection\nbased on histology images and text reports. However, one of the main\nlimitations in training these VLMs is the requirement for large paired\ndatasets, raising concerns over privacy, and data collection, annotation, and\nmaintenance costs. To address this challenge, we introduce CLIP-IT method to\ntrain a vision backbone model to classify histology images by pairing them with\nprivileged textual information from an external source. At first, the modality\npairing step relies on a CLIP-based model to match histology images with\nsemantically relevant textual report data from external sources, creating an\naugmented multimodal dataset without the need for manually paired samples.\nThen, we propose a multimodal training procedure that distills the knowledge\nfrom the paired text modality to the unimodal image classifier for enhanced\nperformance without the need for the textual data during inference. A\nparameter-efficient fine-tuning method is used to efficiently address the\nmisalignment between the main (image) and paired (text) modalities. During\ninference, the improved unimodal histology classifier is used, with only\nminimal additional computational complexity. Our experiments on challenging\nPCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a\ncost-effective approach to leverage privileged textual information and\noutperform unimodal classifiers for histology.", "AI": {"tldr": "CLIP-IT\u65b9\u6cd5\u901a\u8fc7\u5916\u90e8\u6587\u672c\u4fe1\u606f\u589e\u5f3a\u5355\u6a21\u6001\u56fe\u50cf\u5206\u7c7b\u5668\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u914d\u5bf9\u6570\u636e\u7684\u9700\u6c42\uff0c\u63d0\u5347\u764c\u75c7\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u8981\u5927\u91cf\u914d\u5bf9\u6570\u636e\uff0c\u5b58\u5728\u9690\u79c1\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "\u5229\u7528CLIP\u6a21\u578b\u5339\u914d\u56fe\u50cf\u4e0e\u5916\u90e8\u6587\u672c\uff0c\u6784\u5efa\u589e\u5f3a\u6570\u636e\u96c6\uff1b\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u6587\u672c\u4fe1\u606f\u878d\u5165\u56fe\u50cf\u5206\u7c7b\u5668\u3002", "result": "\u5728PCAM\u3001CRC\u548cBACH\u6570\u636e\u96c6\u4e0a\uff0cCLIP-IT\u4f18\u4e8e\u5355\u6a21\u6001\u5206\u7c7b\u5668\u3002", "conclusion": "CLIP-IT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5229\u7528\u5916\u90e8\u6587\u672c\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u6570\u636e\u9700\u6c42\u3002"}}
{"id": "2504.16760", "pdf": "https://arxiv.org/pdf/2504.16760", "abs": "https://arxiv.org/abs/2504.16760", "authors": ["Bartosz Piotrowski", "Witold Drzewakowski", "Konrad Staniszewski", "Piotr Mi\u0142o\u015b"], "title": "Lightweight Latent Verifiers for Efficient Meta-Generation Strategies", "categories": ["cs.AI"], "comment": null, "summary": "Verifiers are auxiliary models that assess the correctness of outputs\ngenerated by base large language models (LLMs). They play a crucial role in\nmany strategies for solving reasoning-intensive problems with LLMs. Typically,\nverifiers are LLMs themselves, often as large (or larger) than the base model\nthey support, making them computationally expensive. In this work, we introduce\na novel lightweight verification approach, LiLaVe, which reliably extracts\ncorrectness signals from the hidden states of the base LLM. A key advantage of\nLiLaVe is its ability to operate with only a small fraction of the\ncomputational budget required by traditional LLM-based verifiers. To\ndemonstrate its practicality, we couple LiLaVe with popular meta-generation\nstrategies, like best-of-n or self-consistency. Moreover, we design novel\nLiLaVe-based approaches, like conditional self-correction or conditional\nmajority voting, that significantly improve both accuracy and efficiency in\ngeneration tasks with smaller LLMs. Our work demonstrates the fruitfulness of\nextracting latent information from the hidden states of LLMs, and opens the\ndoor to scalable and resource-efficient solutions for reasoning-intensive\napplications.", "AI": {"tldr": "LiLaVe\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u57fa\u7840LLM\u7684\u9690\u85cf\u72b6\u6001\u4e2d\u63d0\u53d6\u6b63\u786e\u6027\u4fe1\u53f7\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u9a8c\u8bc1\u5668\u901a\u5e38\u4e0e\u57fa\u7840LLM\u89c4\u6a21\u76f8\u5f53\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLiLaVe\uff0c\u5229\u7528\u57fa\u7840LLM\u7684\u9690\u85cf\u72b6\u6001\u63d0\u53d6\u6b63\u786e\u6027\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u5143\u751f\u6210\u7b56\u7565\uff08\u5982best-of-n\u6216\u81ea\u4e00\u81f4\u6027\uff09\u6216\u8bbe\u8ba1\u65b0\u65b9\u6cd5\uff08\u5982\u6761\u4ef6\u81ea\u6821\u6b63\u6216\u6761\u4ef6\u591a\u6570\u6295\u7968\uff09\u3002", "result": "LiLaVe\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8f83\u5c0f\u89c4\u6a21\u7684LLM\u3002", "conclusion": "LiLaVe\u5c55\u793a\u4e86\u4eceLLM\u9690\u85cf\u72b6\u6001\u4e2d\u63d0\u53d6\u6f5c\u5728\u4fe1\u606f\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8d44\u6e90\u9ad8\u6548\u7684\u63a8\u7406\u5bc6\u96c6\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.16242", "pdf": "https://arxiv.org/pdf/2504.16242", "abs": "https://arxiv.org/abs/2504.16242", "authors": ["Henry Marichal", "Ver\u00f3nica Casaravilla", "Candice Power", "Karolain Mello", "Joaqu\u00edn Mazarino", "Christine Lucas", "Ludmila Profumo", "Diego Passarella", "Gregory Randall"], "title": "DeepCS-TRD, a Deep Learning-based Cross-Section Tree Ring Detector", "categories": ["cs.CV"], "comment": "12 pages, 6 figures. Accepted in ICIAP 2025", "summary": "Here, we propose Deep CS-TRD, a new automatic algorithm for detecting tree\nrings in whole cross-sections. It substitutes the edge detection step of CS-TRD\nby a deep-learning-based approach (U-Net), which allows the application of the\nmethod to different image domains: microscopy, scanner or smartphone acquired,\nand species (Pinus taeda, Gleditsia triachantos and Salix glauca).\nAdditionally, we introduce two publicly available datasets of annotated images\nto the community. The proposed method outperforms state-of-the-art approaches\nin macro images (Pinus taeda and Gleditsia triacanthos) while showing slightly\nlower performance in microscopy images of Salix glauca. To our knowledge, this\nis the first paper that studies automatic tree ring detection for such\ndifferent species and acquisition conditions. The dataset and source code are\navailable in https://github.com/hmarichal93/deepcstrd", "AI": {"tldr": "Deep CS-TRD\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u68c0\u6d4b\u6811\u6728\u5e74\u8f6e\u7684\u65b0\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u7c7b\u578b\u548c\u6811\u79cd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u68c0\u6d4b\u4e0d\u540c\u56fe\u50cf\u7c7b\u578b\u548c\u6811\u79cd\u7684\u5e74\u8f6e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7528U-Net\u66ff\u6362CS-TRD\u7684\u8fb9\u7f18\u68c0\u6d4b\u6b65\u9aa4\uff0c\u9002\u7528\u4e8e\u663e\u5fae\u955c\u3001\u626b\u63cf\u4eea\u6216\u667a\u80fd\u624b\u673a\u83b7\u53d6\u7684\u56fe\u50cf\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e09\u79cd\u6811\u79cd\u3002", "result": "\u5728\u5b8f\u89c2\u56fe\u50cf\uff08Pinus taeda\u548cGleditsia triacanthos\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f46\u5728\u663e\u5fae\u955c\u56fe\u50cf\uff08Salix glauca\uff09\u4e0a\u7a0d\u900a\u3002", "conclusion": "Deep CS-TRD\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u79cd\u6811\u79cd\u548c\u91c7\u96c6\u6761\u4ef6\u8fdb\u884c\u81ea\u52a8\u5e74\u8f6e\u68c0\u6d4b\u7684\u7814\u7a76\uff0c\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.16891", "pdf": "https://arxiv.org/pdf/2504.16891", "abs": "https://arxiv.org/abs/2504.16891", "authors": ["Ivan Moshkov", "Darragh Hanley", "Ivan Sorokin", "Shubham Toshniwal", "Christof Henkel", "Benedikt Schifferer", "Wei Du", "Igor Gitman"], "title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Report of AIMO-2 winning submission", "summary": "This paper presents our winning submission to the AI Mathematical Olympiad -\nProgress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art\nmathematical reasoning models relies on three key pillars. First, we create a\nlarge-scale dataset comprising 540K unique high-quality math problems,\nincluding olympiad-level problems, and their 3.2M long-reasoning solutions.\nSecond, we develop a novel method to integrate code execution with long\nreasoning models through iterative training, generation, and quality filtering,\nresulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we\ncreate a pipeline to train models to select the most promising solution from\nmany candidates. We show that such generative solution selection (GenSelect)\ncan significantly improve upon majority voting baseline. Combining these ideas,\nwe train a series of models that achieve state-of-the-art results on\nmathematical reasoning benchmarks. To facilitate further research, we release\nour code, models, and the complete OpenMathReasoning dataset under a\ncommercially permissive license.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728AI\u6570\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u4e2d\u83b7\u80dc\u7684\u6a21\u578b\uff0c\u5176\u6838\u5fc3\u662f\u57fa\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u7ed3\u5408\u4ee3\u7801\u6267\u884c\u7684\u957f\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u53ca\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\u9009\u62e9\u6280\u672f\u3002", "motivation": "\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u89e3\u51b3\u9ad8\u96be\u5ea6\u6570\u5b66\u95ee\u9898\uff08\u5982\u5965\u6570\u9898\uff09\u4e0a\u7684\u8868\u73b0\u3002", "method": "1. \u6784\u5efa\u5305\u542b54\u4e07\u9053\u9ad8\u8d28\u91cf\u6570\u5b66\u9898\u53ca\u5176320\u4e07\u6761\u957f\u63a8\u7406\u89e3\u7b54\u7684\u6570\u636e\u96c6\uff1b2. \u5f00\u53d1\u7ed3\u5408\u4ee3\u7801\u6267\u884c\u7684\u8fed\u4ee3\u8bad\u7ec3\u65b9\u6cd5\uff0c\u751f\u6210170\u4e07\u6761\u9ad8\u8d28\u91cf\u5de5\u5177\u96c6\u6210\u63a8\u7406\u89e3\u7b54\uff1b3. \u8bbe\u8ba1\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\u9009\u62e9\uff08GenSelect\uff09\u6d41\u7a0b\uff0c\u4f18\u5316\u5019\u9009\u89e3\u7b54\u7684\u9009\u62e9\u3002", "result": "\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u96c6\u3001\u5de5\u5177\u96c6\u6210\u63a8\u7406\u548cGenSelect\u6280\u672f\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u8d44\u6e90\u3002"}}
{"id": "2504.16290", "pdf": "https://arxiv.org/pdf/2504.16290", "abs": "https://arxiv.org/abs/2504.16290", "authors": ["Andr\u00e9 Longon"], "title": "Naturally Computed Scale Invariance in the Residual Stream of ResNet18", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "An important capacity in visual object recognition is invariance to\nimage-altering variables which leave the identity of objects unchanged, such as\nlighting, rotation, and scale. How do neural networks achieve this? Prior\nmechanistic interpretability research has illuminated some invariance-building\ncircuitry in InceptionV1, but the results are limited and networks with\ndifferent architectures have remained largely unexplored. This work\ninvestigates ResNet18 with a particular focus on its residual stream, an\narchitectural component which InceptionV1 lacks. We observe that many\nconvolutional channels in intermediate blocks exhibit scale invariant\nproperties, computed by the element-wise residual summation of scale\nequivariant representations: the block input's smaller-scale copy with the\nblock pre-sum output's larger-scale copy. Through subsequent ablation\nexperiments, we attempt to causally link these neural properties with\nscale-robust object recognition behavior. Our tentative findings suggest how\nthe residual stream computes scale invariance and its possible role in\nbehavior. Code is available at:\nhttps://github.com/cest-andre/residual-stream-interp", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86ResNet18\u4e2d\u7684\u6b8b\u5dee\u6d41\u5982\u4f55\u901a\u8fc7\u5c3a\u5ea6\u7b49\u53d8\u8868\u793a\u7684\u5143\u7d20\u7ea7\u6b8b\u5dee\u6c42\u548c\u5b9e\u73b0\u5c3a\u5ea6\u4e0d\u53d8\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u884c\u4e3a\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u5b9e\u73b0\u89c6\u89c9\u5bf9\u8c61\u8bc6\u522b\u4e2d\u5bf9\u56fe\u50cf\u53d8\u6362\uff08\u5982\u5149\u7167\u3001\u65cb\u8f6c\u548c\u5c3a\u5ea6\uff09\u7684\u4e0d\u53d8\u6027\uff0c\u5c24\u5176\u662f\u4e0d\u540c\u67b6\u6784\u7f51\u7edc\uff08\u5982ResNet18\uff09\u4e2d\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u6b8b\u5dee\u6d41\u3002", "method": "\u901a\u8fc7\u5206\u6790ResNet18\u7684\u4e2d\u95f4\u5757\u5377\u79ef\u901a\u9053\uff0c\u89c2\u5bdf\u5176\u5c3a\u5ea6\u4e0d\u53d8\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u795e\u7ecf\u7279\u6027\u4e0e\u5c3a\u5ea6\u9c81\u68d2\u5bf9\u8c61\u8bc6\u522b\u884c\u4e3a\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u8bb8\u591a\u4e2d\u95f4\u5757\u5377\u79ef\u901a\u9053\u901a\u8fc7\u5c3a\u5ea6\u7b49\u53d8\u8868\u793a\u7684\u5143\u7d20\u7ea7\u6b8b\u5dee\u6c42\u548c\u8868\u73b0\u51fa\u5c3a\u5ea6\u4e0d\u53d8\u6027\uff0c\u521d\u6b65\u63ed\u793a\u4e86\u6b8b\u5dee\u6d41\u5728\u8ba1\u7b97\u5c3a\u5ea6\u4e0d\u53d8\u6027\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u6b8b\u5dee\u6d41\u53ef\u80fd\u662fResNet18\u5b9e\u73b0\u5c3a\u5ea6\u4e0d\u53d8\u6027\u7684\u5173\u952e\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.16093", "pdf": "https://arxiv.org/pdf/2504.16093", "abs": "https://arxiv.org/abs/2504.16093", "authors": ["Yurun Ge", "Lucas B\u00f6ttcher", "Tom Chou", "Maria R. D'Orsogna"], "title": "Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model", "categories": ["q-fin.PM", "cs.AI", "math.PR", "60-08, 90-08", "G.3; I.6.1; J.4"], "comment": "15pp, 4 figs", "summary": "How to allocate limited resources to projects that will yield the greatest\nlong-term benefits is a problem that often arises in decision-making under\nuncertainty. For example, organizations may need to evaluate and select\ninnovation projects with risky returns. Similarly, when allocating resources to\nresearch projects, funding agencies are tasked with identifying the most\npromising proposals based on idiosyncratic criteria. Finally, in participatory\nbudgeting, a local community may need to select a subset of public projects to\nfund. Regardless of context, agents must estimate the uncertain values of a\npotentially large number of projects. Developing parsimonious methods to\ncompare these projects, and aggregating agent evaluations so that the overall\nbenefit is maximized, are critical in assembling the best project portfolio.\nUnlike in standard sorting algorithms, evaluating projects on the basis of\nuncertain long-term benefits introduces additional complexities. We propose\ncomparison rules based on Quicksort and the Bradley--Terry model, which\nconnects rankings to pairwise \"win\" probabilities. In our model, each agent\ndetermines win probabilities of a pair of projects based on his or her specific\nevaluation of the projects' long-term benefit. The win probabilities are then\nappropriately aggregated and used to rank projects. Several of the methods we\npropose perform better than the two most effective aggregation methods\ncurrently available. Additionally, our methods can be combined with sampling\ntechniques to significantly reduce the number of pairwise comparisons. We also\ndiscuss how the Bradley--Terry portfolio selection approach can be implemented\nin practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQuicksort\u548cBradley-Terry\u6a21\u578b\u7684\u6bd4\u8f83\u89c4\u5219\uff0c\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5206\u914d\u8d44\u6e90\u4ee5\u6700\u5927\u5316\u957f\u671f\u6548\u76ca\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5982\u4f55\u5206\u914d\u6709\u9650\u8d44\u6e90\u4ee5\u9009\u62e9\u6700\u5177\u957f\u671f\u6548\u76ca\u7684\u9879\u76ee\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u521b\u65b0\u9879\u76ee\u8bc4\u4f30\u3001\u7814\u7a76\u8d44\u91d1\u5206\u914d\u548c\u53c2\u4e0e\u5f0f\u9884\u7b97\u7b49\u573a\u666f\u3002", "method": "\u91c7\u7528Quicksort\u548cBradley-Terry\u6a21\u578b\uff0c\u901a\u8fc7\u4ee3\u7406\u5bf9\u9879\u76ee\u5bf9\u7684\u201c\u80dc\u7387\u201d\u8bc4\u4f30\uff0c\u805a\u5408\u8fd9\u4e9b\u8bc4\u4f30\u4ee5\u6392\u540d\u9879\u76ee\u3002\u7ed3\u5408\u62bd\u6837\u6280\u672f\u51cf\u5c11\u6bd4\u8f83\u6b21\u6570\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u6709\u6548\u7684\u4e24\u79cd\u805a\u5408\u65b9\u6cd5\uff0c\u5e76\u80fd\u663e\u8457\u51cf\u5c11\u6210\u5bf9\u6bd4\u8f83\u7684\u6b21\u6570\u3002", "conclusion": "Bradley-Terry\u6a21\u578b\u5728\u9879\u76ee\u7ec4\u5408\u9009\u62e9\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u4f18\u5316\u8d44\u6e90\u5206\u914d\u51b3\u7b56\u3002"}}
{"id": "2504.16304", "pdf": "https://arxiv.org/pdf/2504.16304", "abs": "https://arxiv.org/abs/2504.16304", "authors": ["Wonjeong Jo", "Magdalena Wojcieszak"], "title": "MetaHarm: Harmful YouTube Video Dataset Annotated by Domain Experts, GPT-4-Turbo, and Crowdworkers", "categories": ["cs.CV"], "comment": null, "summary": "Short video platforms, such as YouTube, Instagram, or TikTok, are used by\nbillions of users. These platforms expose users to harmful content, ranging\nfrom clickbait or physical harms to hate or misinformation. Yet, we lack a\ncomprehensive understanding and measurement of online harm on short video\nplatforms. Toward this end, we present two large-scale datasets of multi-modal\nand multi-categorical online harm: (1) 60,906 systematically selected\npotentially harmful YouTube videos and (2) 19,422 videos annotated by three\nlabeling actors: trained domain experts, GPT-4-Turbo (using 14 image frames, 1\nthumbnail, and text metadata), and crowdworkers (Amazon Mechanical Turk master\nworkers). The annotated dataset includes both (a) binary classification\n(harmful vs. harmless) and (b) multi-label categorizations of six harm\ncategories: Information, Hate and harassment, Addictive, Clickbait, Sexual, and\nPhysical harms. Furthermore, the annotated dataset provides (1) ground truth\ndata with videos annotated consistently across (a) all three actors and (b) the\nmajority of the labeling actors, and (2) three data subsets labeled by\nindividual actors. These datasets are expected to facilitate future work on\nonline harm, aid in (multi-modal) classification efforts, and advance the\nidentification and potential mitigation of harmful content on video platforms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6d4b\u91cf\u548c\u5206\u7c7b\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u6709\u5bb3\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u3001AI\u548c\u4f17\u5305\u6807\u6ce8\u63d0\u4f9b\u591a\u6a21\u6001\u3001\u591a\u7c7b\u522b\u7684\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u6709\u5bb3\u5185\u5bb9\u7f3a\u4e4f\u5168\u9762\u7406\u89e3\u548c\u6d4b\u91cf\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\uff1a\u4e00\u4e2a\u5305\u542b60,906\u4e2a\u6f5c\u5728\u6709\u5bb3\u89c6\u9891\uff0c\u53e6\u4e00\u4e2a\u5305\u542b19,422\u4e2a\u7531\u4e13\u5bb6\u3001GPT-4-Turbo\u548c\u4f17\u5305\u6807\u6ce8\u7684\u89c6\u9891\uff0c\u6db5\u76d6\u516d\u7c7b\u6709\u5bb3\u5185\u5bb9\u3002", "result": "\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u652f\u6301\u672a\u6765\u6709\u5bb3\u5185\u5bb9\u5206\u7c7b\u548c\u7f13\u89e3\u7814\u7a76\u3002", "conclusion": "\u8fd9\u4e9b\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u77ed\u89c6\u9891\u5e73\u53f0\u6709\u5bb3\u5185\u5bb9\u7684\u8bc6\u522b\u548c\u7f13\u89e3\u5de5\u4f5c\u3002"}}
{"id": "2504.16096", "pdf": "https://arxiv.org/pdf/2504.16096", "abs": "https://arxiv.org/abs/2504.16096", "authors": ["Jiaxing Xu", "Kai He", "Yue Tang", "Wei Li", "Mengcheng Lan", "Xia Dong", "Yiping Ke", "Mengling Feng"], "title": "BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Neurological conditions, such as Alzheimer's Disease, are challenging to\ndiagnose, particularly in the early stages where symptoms closely resemble\nhealthy controls. Existing brain network analysis methods primarily focus on\ngraph-based models that rely solely on imaging data, which may overlook\nimportant non-imaging factors and limit the model's predictive power and\ninterpretability. In this paper, we present BrainPrompt, an innovative\nframework that enhances Graph Neural Networks (GNNs) by integrating Large\nLanguage Models (LLMs) with knowledge-driven prompts, enabling more effective\ncapture of complex, non-imaging information and external knowledge for\nneurological disease identification. BrainPrompt integrates three types of\nknowledge-driven prompts: (1) ROI-level prompts to encode the identity and\nfunction of each brain region, (2) subject-level prompts that incorporate\ndemographic information, and (3) disease-level prompts to capture the temporal\nprogression of disease. By leveraging these multi-level prompts, BrainPrompt\neffectively harnesses knowledge-enhanced multi-modal information from LLMs,\nenhancing the model's capability to predict neurological disease stages and\nmeanwhile offers more interpretable results. We evaluate BrainPrompt on two\nresting-state functional Magnetic Resonance Imaging (fMRI) datasets from\nneurological disorders, showing its superiority over state-of-the-art methods.\nAdditionally, a biomarker study demonstrates the framework's ability to extract\nvaluable and interpretable information aligned with domain knowledge in\nneuroscience.", "AI": {"tldr": "BrainPrompt\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u7ea7\u77e5\u8bc6\u9a71\u52a8\u63d0\u793a\u589e\u5f3a\u795e\u7ecf\u75be\u75c5\u8bca\u65ad\u7684\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u795e\u7ecf\u75be\u75c5\uff08\u5982\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff09\u65e9\u671f\u8bca\u65ad\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6210\u50cf\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u975e\u6210\u50cf\u56e0\u7d20\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faBrainPrompt\u6846\u67b6\uff0c\u6574\u5408ROI\u7ea7\u3001\u53d7\u8bd5\u8005\u7ea7\u548c\u75be\u75c5\u7ea7\u77e5\u8bc6\u9a71\u52a8\u63d0\u793a\uff0c\u7ed3\u5408GNN\u548cLLM\u6355\u83b7\u591a\u6a21\u6001\u4fe1\u606f\u3002", "result": "\u5728\u9759\u606f\u6001fMRI\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u53d6\u4e0e\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u77e5\u8bc6\u4e00\u81f4\u7684\u53ef\u89e3\u91ca\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "BrainPrompt\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u75be\u75c5\u8bca\u65ad\u7684\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.16315", "pdf": "https://arxiv.org/pdf/2504.16315", "abs": "https://arxiv.org/abs/2504.16315", "authors": ["Sen Fang", "Chunyu Sui", "Hongwei Yi", "Carol Neidle", "Dimitris N. Metaxas"], "title": "SignX: The Foundation Model for Sign Recognition", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The complexity of sign language data processing brings many challenges. The\ncurrent approach to recognition of ASL signs aims to translate RGB sign\nlanguage videos through pose information into English-based ID glosses, which\nserve to uniquely identify ASL signs. Note that there is no shared convention\nfor assigning such glosses to ASL signs, so it is essential that the same\nglossing conventions are used for all of the data in the datasets that are\nemployed. This paper proposes SignX, a foundation model framework for sign\nrecognition. It is a concise yet powerful framework applicable to multiple\nhuman activity recognition scenarios. First, we developed a Pose2Gloss\ncomponent based on an inverse diffusion model, which contains a multi-track\npose fusion layer that unifies five of the most powerful pose information\nsources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens\nSegmentation--into a single latent pose representation. Second, we trained a\nVideo2Pose module based on ViT that can directly convert raw video into signer\npose representation. Through this 2-stage training framework, we enable sign\nlanguage recognition models to be compatible with existing pose formats, laying\nthe foundation for the common pose estimation necessary for sign recognition.\nExperimental results show that SignX can recognize signs from sign language\nvideo, producing predicted gloss representations with greater accuracy than has\nbeen reported in prior work.", "AI": {"tldr": "SignX\u662f\u4e00\u4e2a\u7528\u4e8e\u624b\u8bed\u8bc6\u522b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08Pose2Gloss\u548cVideo2Pose\uff09\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u3002", "motivation": "\u624b\u8bed\u6570\u636e\u5904\u7406\u590d\u6742\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56RGB\u89c6\u9891\u548c\u59ff\u52bf\u4fe1\u606f\u8f6c\u6362\u4e3a\u82f1\u6587ID gloss\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684gloss\u7ea6\u5b9a\u3002", "method": "\u63d0\u51faSignX\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u9006\u6269\u6563\u6a21\u578b\u7684Pose2Gloss\u548c\u57fa\u4e8eViT\u7684Video2Pose\u6a21\u5757\uff0c\u6574\u5408\u591a\u79cd\u59ff\u52bf\u4fe1\u606f\u6765\u6e90\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSignX\u5728\u624b\u8bed\u89c6\u9891\u8bc6\u522b\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u3002", "conclusion": "SignX\u4e3a\u624b\u8bed\u8bc6\u522b\u63d0\u4f9b\u4e86\u517c\u5bb9\u73b0\u6709\u59ff\u52bf\u683c\u5f0f\u7684\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2504.16097", "pdf": "https://arxiv.org/pdf/2504.16097", "abs": "https://arxiv.org/abs/2504.16097", "authors": ["Arthur Buzelin", "Pedro Robles Dutenhefner", "Turi Rezende", "Luisa G. Porfirio", "Pedro Bento", "Yan Aquino", "Jose Fernandes", "Caio Santana", "Gabriela Miana", "Gisele L. Pappa", "Antonio Ribeiro", "Wagner Meira Jr"], "title": "A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Cardiovascular diseases remain the leading cause of global mortality,\nemphasizing the critical need for efficient diagnostic tools such as\nelectrocardiograms (ECGs). Recent advancements in deep learning, particularly\ntransformers, have revolutionized ECG analysis by capturing detailed waveform\nfeatures as well as global rhythm patterns. However, traditional transformers\nstruggle to effectively capture local morphological features that are critical\nfor accurate ECG interpretation. We propose a novel Local-Global Attention ECG\nmodel (LGA-ECG) to address this limitation, integrating convolutional inductive\nbiases with global self-attention mechanisms. Our approach extracts queries by\naveraging embeddings obtained from overlapping convolutional windows, enabling\nfine-grained morphological analysis, while simultaneously modeling global\ncontext through attention to keys and values derived from the entire sequence.\nExperiments conducted on the CODE-15 dataset demonstrate that LGA-ECG\noutperforms state-of-the-art models and ablation studies validate the\neffectiveness of the local-global attention strategy. By capturing the\nhierarchical temporal dependencies and morphological patterns in ECG signals,\nthis new design showcases its potential for clinical deployment with robust\nautomated ECG classification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b0\u578bLGA-ECG\u6a21\u578b\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u5206\u6790\u5fc3\u7535\u56fe\u4fe1\u53f7\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5fc3\u7535\u56fe\u8bca\u65ad\u5de5\u5177\u3002\u4f20\u7edfTransformer\u6a21\u578b\u96be\u4ee5\u6355\u6349\u5c40\u90e8\u5f62\u6001\u7279\u5f81\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faLGA-ECG\u6a21\u578b\uff0c\u901a\u8fc7\u5377\u79ef\u7a97\u53e3\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5206\u6790\u6574\u4f53\u8282\u5f8b\u3002", "result": "\u5728CODE-15\u6570\u636e\u96c6\u4e0a\uff0cLGA-ECG\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5c40\u90e8-\u5168\u5c40\u6ce8\u610f\u529b\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "LGA-ECG\u901a\u8fc7\u6355\u6349\u5fc3\u7535\u56fe\u4fe1\u53f7\u7684\u5c42\u6b21\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5f62\u6001\u6a21\u5f0f\uff0c\u5c55\u793a\u4e86\u4e34\u5e8a\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2504.16362", "pdf": "https://arxiv.org/pdf/2504.16362", "abs": "https://arxiv.org/abs/2504.16362", "authors": ["Colton R. Crum", "Adam Czajka"], "title": "Almost Right: Making First-layer Kernels Nearly Orthogonal Improves Model Generalization", "categories": ["cs.CV"], "comment": "8 pages, 1 figure, 3 tables", "summary": "An ongoing research challenge within several domains in computer vision is\nhow to increase model generalization capabilities. Several attempts to improve\nmodel generalization performance are heavily inspired by human perceptual\nintelligence, which is remarkable in both its performance and efficiency to\ngeneralize to unknown samples. Many of these methods attempt to force portions\nof the network to be orthogonal, following some observation within neuroscience\nrelated to early vision processes. In this paper, we propose a loss component\nthat regularizes the filtering kernels in the first convolutional layer of a\nnetwork to make them nearly orthogonal. Deviating from previous works, we give\nthe network flexibility in which pairs of kernels it makes orthogonal, allowing\nthe network to navigate to a better solution space, imposing harsh penalties.\nWithout architectural modifications, we report substantial gains in\ngeneralization performance using the proposed loss against previous works\n(including orthogonalization- and saliency-based regularization methods) across\nthree different architectures (ResNet-50, DenseNet-121, ViT-b-16) and two\ndifficult open-set recognition tasks: presentation attack detection in iris\nbiometrics, and anomaly detection in chest X-ray images.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u635f\u5931\u7ec4\u4ef6\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u7b2c\u4e00\u5377\u79ef\u5c42\u7684\u6ee4\u6ce2\u6838\u4f7f\u5176\u63a5\u8fd1\u6b63\u4ea4\uff0c\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u53d7\u4eba\u7c7b\u611f\u77e5\u667a\u80fd\u542f\u53d1\uff0c\u5c1d\u8bd5\u901a\u8fc7\u6b63\u4ea4\u5316\u6ee4\u6ce2\u6838\u6539\u8fdb\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7075\u6d3b\u7684\u635f\u5931\u7ec4\u4ef6\uff0c\u5141\u8bb8\u7f51\u7edc\u81ea\u4e3b\u9009\u62e9\u6b63\u4ea4\u5316\u7684\u6838\u5bf9\uff0c\u907f\u514d\u4e25\u683c\u9650\u5236\u3002", "result": "\u5728\u4e09\u79cd\u67b6\u6784\u548c\u4e24\u4e2a\u5f00\u653e\u96c6\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u6cdb\u5316\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u67b6\u6784\u5373\u53ef\u663e\u8457\u63d0\u9ad8\u6cdb\u5316\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u6b63\u4ea4\u5316\u548c\u663e\u8457\u6027\u6b63\u5219\u5316\u65b9\u6cd5\u3002"}}
{"id": "2504.16099", "pdf": "https://arxiv.org/pdf/2504.16099", "abs": "https://arxiv.org/abs/2504.16099", "authors": ["Luyuan Zhang", "Xidong Mu", "An Liu", "Yuanwei Liu"], "title": "Two-Timescale Joint Transmit and Pinching Beamforming for Pinching-Antenna Systems", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "comment": "5 pages, 4 figures, letter", "summary": "Pinching antenna systems (PASS) have been proposed as a revolutionary\nflexible antenna technology which facilitates line-of-sight links via numerous\nlow-cost pinching antennas with adjustable activation positions over\nwaveguides. This letter proposes a two-timescale joint transmit and pinching\nbeamforming design for the maximization of sum rate of a PASS-based downlink\nmulti-user multiple input single output system. A primal dual decomposition\nmethod is developed to decouple the two-timescale problem into two\nsub-problems: 1) A Karush-Kuhn-Tucker-guided dual learning-based approach is\nproposed to solve the short-term transmit beamforming design sub-problem; 2)\nThe long-term pinching beamforming design sub-problem is tackled by adopting a\nstochastic successive convex approximation method. Simulation results\ndemonstrate that the proposed two-timescale algorithm achieves a significant\nperformance gain compared to other baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u65f6\u95f4\u5c3a\u5ea6\u7684\u8054\u5408\u53d1\u5c04\u548c\u5939\u6301\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u7528\u4e8e\u6700\u5927\u5316PASS\u7cfb\u7edf\u7684\u591a\u7528\u6237\u4e0b\u884c\u94fe\u8def\u6027\u80fd\u3002", "motivation": "PASS\u6280\u672f\u901a\u8fc7\u4f4e\u6210\u672c\u5939\u6301\u5929\u7ebf\u5b9e\u73b0\u7075\u6d3b\u7684\u6ce2\u675f\u6210\u5f62\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7684\u7b97\u6cd5\u4f18\u5316\u5176\u6027\u80fd\u3002", "method": "\u91c7\u7528\u539f\u59cb\u5bf9\u5076\u5206\u89e3\u65b9\u6cd5\uff0c\u5c06\u95ee\u9898\u62c6\u5206\u4e3a\u77ed\u671f\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u548c\u957f\u671f\u5939\u6301\u6ce2\u675f\u6210\u5f62\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u5206\u522b\u7528KKT\u5f15\u5bfc\u7684\u5bf9\u5076\u5b66\u4e60\u548c\u968f\u673a\u8fde\u7eed\u51f8\u8fd1\u4f3c\u65b9\u6cd5\u89e3\u51b3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u53cc\u65f6\u95f4\u5c3a\u5ea6\u7b97\u6cd5\u6709\u6548\u4f18\u5316\u4e86PASS\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7075\u6d3b\u5929\u7ebf\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16364", "pdf": "https://arxiv.org/pdf/2504.16364", "abs": "https://arxiv.org/abs/2504.16364", "authors": ["Fengchun Liu", "Tong Zhang", "Chunying Zhang"], "title": "CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "In recent years, a large number of works have introduced Convolutional Neural\nNetworks (CNNs) into image steganography, which transform traditional\nsteganography methods such as hand-crafted features and prior knowledge design\ninto steganography methods that neural networks autonomically learn information\nembedding. However, due to the inherent complexity of digital images, issues of\ninvisibility and security persist when using CNN models for information\nembedding. In this paper, we propose Curriculum Learning Progressive Steganophy\nNetwork (CLPSTNet). The network consists of multiple progressive multi-scale\nconvolutional modules that integrate Inception structures and dilated\nconvolutions. The module contains multiple branching pathways, starting from a\nsmaller convolutional kernel and dilatation rate, extracting the basic, local\nfeature information from the feature map, and gradually expanding to the\nconvolution with a larger convolutional kernel and dilatation rate for\nperceiving the feature information of a larger receptive field, so as to\nrealize the multi-scale feature extraction from shallow to deep, and from fine\nto coarse, allowing the shallow secret information features to be refined in\ndifferent fusion stages. The experimental results show that the proposed\nCLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three\nlarge public datasets, ALASKA2, VOC2012 and ImageNet, but also the\nsteganographic images generated by CLPSTNet have low steganalysis scores.You\ncan find our code at\n\\href{https://github.com/chaos-boops/CLPSTNet}{https://github.com/chaos-boops/CLPSTNet}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u6e10\u8fdb\u5f0f\u591a\u5c3a\u5ea6\u5377\u79ef\u7f51\u7edc\uff08CLPSTNet\uff09\uff0c\u7528\u4e8e\u89e3\u51b3CNN\u5728\u56fe\u50cf\u9690\u5199\u672f\u4e2d\u7684\u4e0d\u53ef\u89c1\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9690\u5199\u672f\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u548c\u5148\u9a8c\u77e5\u8bc6\u8bbe\u8ba1\uff0c\u800cCNN\u5728\u56fe\u50cf\u9690\u5199\u672f\u4e2d\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u4e0d\u53ef\u89c1\u6027\u548c\u5b89\u5168\u6027\u6311\u6218\u3002", "method": "CLPSTNet\u91c7\u7528\u6e10\u8fdb\u5f0f\u591a\u5c3a\u5ea6\u5377\u79ef\u6a21\u5757\uff0c\u7ed3\u5408Inception\u7ed3\u6784\u548c\u7a7a\u6d1e\u5377\u79ef\uff0c\u4ece\u6d45\u5230\u6df1\u3001\u4ece\u7ec6\u5230\u7c97\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "result": "\u5728ALASKA2\u3001VOC2012\u548cImageNet\u6570\u636e\u96c6\u4e0a\uff0cCLPSTNet\u8868\u73b0\u51fa\u9ad8PSNR\u3001SSIM\u548c\u89e3\u7801\u7cbe\u5ea6\uff0c\u4e14\u751f\u6210\u7684\u9690\u5199\u56fe\u50cf\u5177\u6709\u4f4e\u9690\u5199\u5206\u6790\u5206\u6570\u3002", "conclusion": "CLPSTNet\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u9690\u5199\u672f\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2504.16100", "pdf": "https://arxiv.org/pdf/2504.16100", "abs": "https://arxiv.org/abs/2504.16100", "authors": ["Eloi Lindas", "Yannig Goude", "Philippe Ciais"], "title": "Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France", "categories": ["eess.SP", "cs.AI", "cs.LG", "stat.ML"], "comment": "24 pages, 4 tables, 18 figures", "summary": "Accurate prediction of non-dispatchable renewable energy sources is essential\nfor grid stability and price prediction. Regional power supply forecasts are\nusually indirect through a bottom-up approach of plant-level forecasts,\nincorporate lagged power values, and do not use the potential of spatially\nresolved data. This study presents a comprehensive methodology for predicting\nsolar and wind power production at country scale in France using machine\nlearning models trained with spatially explicit weather data combined with\nspatial information about production sites capacity. A dataset is built\nspanning from 2012 to 2023, using daily power production data from RTE (the\nnational grid operator) as the target variable, with daily weather data from\nERA5, production sites capacity and location, and electricity prices as input\nfeatures. Three modeling approaches are explored to handle spatially resolved\nweather data: spatial averaging over the country, dimension reduction through\nprincipal component analysis, and a computer vision architecture to exploit\ncomplex spatial relationships. The study benchmarks state-of-the-art machine\nlearning models as well as hyperparameter tuning approaches based on\ncross-validation methods on daily power production data. Results indicate that\ncross-validation tailored to time series is best suited to reach low error. We\nfound that neural networks tend to outperform traditional tree-based models,\nwhich face challenges in extrapolation due to the increasing renewable capacity\nover time. Model performance ranges from 4% to 10% in nRMSE for midterm\nhorizon, achieving similar error metrics to local models established at a\nsingle-plant level, highlighting the potential of these methods for regional\npower supply forecasting.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6cd5\u56fd\u592a\u9633\u80fd\u548c\u98ce\u80fd\u53d1\u7535\u91cf\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a7a\u95f4\u660e\u786e\u7684\u5929\u6c14\u6570\u636e\u548c\u4ea7\u80fd\u4fe1\u606f\uff0c\u7ed3\u679c\u663e\u793a\u795e\u7ecf\u7f51\u7edc\u4f18\u4e8e\u4f20\u7edf\u6811\u6a21\u578b\uff0c\u8bef\u5dee\u8303\u56f4\u4e3a4%-10%\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u91cf\u5bf9\u7535\u7f51\u7a33\u5b9a\u548c\u7535\u4ef7\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u95f4\u63a5\u4e14\u672a\u5145\u5206\u5229\u7528\u7a7a\u95f4\u6570\u636e\u3002", "method": "\u4f7f\u7528ERA5\u5929\u6c14\u6570\u636e\u3001\u4ea7\u80fd\u548c\u7535\u4ef7\u7b49\u7279\u5f81\uff0c\u63a2\u7d22\u4e86\u4e09\u79cd\u5904\u7406\u7a7a\u95f4\u5929\u6c14\u6570\u636e\u7684\u65b9\u6cd5\uff08\u7a7a\u95f4\u5e73\u5747\u3001\u4e3b\u6210\u5206\u5206\u6790\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u67b6\u6784\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u8868\u73b0\u6700\u4f73\uff0c\u8bef\u5dee\u8303\u56f4\u4e3a4%-10%\uff0c\u4e0e\u5355\u5382\u7ea7\u6a21\u578b\u76f8\u5f53\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u533a\u57df\u7535\u529b\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533a\u57df\u53ef\u518d\u751f\u80fd\u6e90\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u7a7a\u95f4\u6570\u636e\u548c\u5bb9\u91cf\u53d8\u5316\u65f6\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2504.16368", "pdf": "https://arxiv.org/pdf/2504.16368", "abs": "https://arxiv.org/abs/2504.16368", "authors": ["Linhua Kong", "Dongxia Chang", "Lian Liu", "Zisen Kong", "Pengyuan Li", "Yao Zhao"], "title": "Revisiting Radar Camera Alignment by Contrastive Learning for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recently, 3D object detection algorithms based on radar and camera fusion\nhave shown excellent performance, setting the stage for their application in\nautonomous driving perception tasks. Existing methods have focused on dealing\nwith feature misalignment caused by the domain gap between radar and camera.\nHowever, existing methods either neglect inter-modal features interaction\nduring alignment or fail to effectively align features at the same spatial\nlocation across modalities. To alleviate the above problems, we propose a new\nalignment model called Radar Camera Alignment (RCAlign). Specifically, we\ndesign a Dual-Route Alignment (DRA) module based on contrastive learning to\nalign and fuse the features between radar and camera. Moreover, considering the\nsparsity of radar BEV features, a Radar Feature Enhancement (RFE) module is\nproposed to improve the densification of radar BEV features with the knowledge\ndistillation loss. Experiments show RCAlign achieves a new state-of-the-art on\nthe public nuScenes benchmark in radar camera fusion for 3D Object Detection.\nFurthermore, the RCAlign achieves a significant performance gain (4.3\\% NDS and\n8.4\\% mAP) in real-time 3D detection compared to the latest state-of-the-art\nmethod (RCBEVDet).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRCAlign\u7684\u65b0\u5bf9\u9f50\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u5bf9\u9f50\u6a21\u5757\u548c\u96f7\u8fbe\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u96f7\u8fbe\u4e0e\u76f8\u673a\u7279\u5f81\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u96f7\u8fbe\u4e0e\u76f8\u673a\u7279\u5f81\u5bf9\u9f50\u65f6\uff0c\u5ffd\u89c6\u4e86\u6a21\u6001\u95f4\u7279\u5f81\u4ea4\u4e92\u6216\u672a\u80fd\u6709\u6548\u5bf9\u9f50\u8de8\u6a21\u6001\u7a7a\u95f4\u7279\u5f81\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u53cc\u8def\u5f84\u5bf9\u9f50\u6a21\u5757\uff08DRA\uff09\u548c\u96f7\u8fbe\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff08RFE\uff09\uff0c\u4ee5\u63d0\u5347\u7279\u5f81\u5bf9\u9f50\u548c\u878d\u5408\u6548\u679c\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u65b0\u6700\u4f73\u6027\u80fd\uff0c\u5b9e\u65f63D\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff084.3% NDS\u548c8.4% mAP\uff09\u3002", "conclusion": "RCAlign\u901a\u8fc7\u6539\u8fdb\u7279\u5f81\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f7\u8fbe\u4e0e\u76f8\u673a\u878d\u5408\u76843D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2504.16101", "pdf": "https://arxiv.org/pdf/2504.16101", "abs": "https://arxiv.org/abs/2504.16101", "authors": ["Lei Kang", "Xuanshuo Fu", "Javier Vazquez-Corral", "Ernest Valveny", "Dimosthenis Karatzas"], "title": "xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Cardiovascular diseases (CVDs) remain the leading cause of mortality\nworldwide, highlighting the critical need for efficient and accurate diagnostic\ntools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart\nconditions; however, their manual interpretation is time-consuming and\nerror-prone. In this paper, we propose xLSTM-ECG, a novel approach that\nleverages an extended Long Short-Term Memory (xLSTM) network for multi-label\nclassification of ECG signals, using the PTB-XL dataset. To the best of our\nknowledge, this work represents the first design and application of xLSTM\nmodules specifically adapted for multi-label ECG classification. Our method\nemploys a Short-Time Fourier Transform (STFT) to convert time-series ECG\nwaveforms into the frequency domain, thereby enhancing feature extraction. The\nxLSTM architecture is specifically tailored to address the complexities of\n12-lead ECG recordings by capturing both local and global signal features.\nComprehensive experiments on the PTB-XL dataset reveal that our model achieves\nstrong multi-label classification performance, while additional tests on the\nGeorgia 12-Lead dataset underscore its robustness and efficiency. This approach\nsignificantly improves ECG classification accuracy, thereby advancing clinical\ndiagnostics and patient care. The code will be publicly available upon\nacceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8exLSTM\u7f51\u7edc\u7684\u591a\u6807\u7b7eECG\u4fe1\u53f7\u5206\u7c7b\u65b9\u6cd5xLSTM-ECG\uff0c\u901a\u8fc7STFT\u8f6c\u6362\u548c\u4f18\u5316\u7684xLSTM\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u8bca\u65ad\u6548\u7387\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0cECG\u624b\u52a8\u89e3\u8bfb\u8017\u65f6\u4e14\u6613\u9519\uff0c\u4e9f\u9700\u9ad8\u6548\u51c6\u786e\u7684\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u4f7f\u7528STFT\u5c06ECG\u4fe1\u53f7\u8f6c\u6362\u5230\u9891\u57df\uff0c\u8bbe\u8ba1xLSTM\u7f51\u7edc\u6355\u634912\u5bfc\u8054ECG\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "result": "\u5728PTB-XL\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728Georgia 12-Lead\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "xLSTM-ECG\u663e\u8457\u63d0\u9ad8\u4e86ECG\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u63a8\u52a8\u4e86\u4e34\u5e8a\u8bca\u65ad\u548c\u60a3\u8005\u62a4\u7406\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.16389", "pdf": "https://arxiv.org/pdf/2504.16389", "abs": "https://arxiv.org/abs/2504.16389", "authors": ["Yuanjian Wang", "Yufei Deng", "Rong Xiao", "Jiahao Fan", "Chenwei Tang", "Deng Xiong", "Jiancheng Lv"], "title": "SaENeRF: Suppressing Artifacts in Event-based Neural Radiance Fields", "categories": ["cs.CV"], "comment": "Accepted by IJCNN 2025", "summary": "Event cameras are neuromorphic vision sensors that asynchronously capture\nchanges in logarithmic brightness changes, offering significant advantages such\nas low latency, low power consumption, low bandwidth, and high dynamic range.\nWhile these characteristics make them ideal for high-speed scenarios,\nreconstructing geometrically consistent and photometrically accurate 3D\nrepresentations from event data remains fundamentally challenging. Current\nevent-based Neural Radiance Fields (NeRF) methods partially address these\nchallenges but suffer from persistent artifacts caused by aggressive network\nlearning in early stages and the inherent noise of event cameras. To overcome\nthese limitations, we present SaENeRF, a novel self-supervised framework that\neffectively suppresses artifacts and enables 3D-consistent, dense, and\nphotorealistic NeRF reconstruction of static scenes solely from event streams.\nOur approach normalizes predicted radiance variations based on accumulated\nevent polarities, facilitating progressive and rapid learning for scene\nrepresentation construction. Additionally, we introduce regularization losses\nspecifically designed to suppress artifacts in regions where photometric\nchanges fall below the event threshold and simultaneously enhance the light\nintensity difference of non-zero events, thereby improving the visual fidelity\nof the reconstructed scene. Extensive qualitative and quantitative experiments\ndemonstrate that our method significantly reduces artifacts and achieves\nsuperior reconstruction quality compared to existing methods. The code is\navailable at https://github.com/Mr-firework/SaENeRF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaENeRF\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4e8b\u4ef6\u6d41\u4e2d\u91cd\u5efa\u9ad8\u8d28\u91cf3D\u573a\u666f\uff0c\u663e\u8457\u51cf\u5c11\u4f2a\u5f71\u5e76\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u9ad8\u901f\u573a\u666f\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u5efa3D\u573a\u666f\u65f6\u5b58\u5728\u4f2a\u5f71\u548c\u566a\u58f0\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f52\u4e00\u5316\u9884\u6d4b\u7684\u8f90\u5c04\u53d8\u5316\u548c\u5f15\u5165\u6b63\u5219\u5316\u635f\u5931\uff0c\u6291\u5236\u4f2a\u5f71\u5e76\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSaENeRF\u663e\u8457\u51cf\u5c11\u4f2a\u5f71\uff0c\u91cd\u5efa\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SaENeRF\u4e3a\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16110", "pdf": "https://arxiv.org/pdf/2504.16110", "abs": "https://arxiv.org/abs/2504.16110", "authors": ["Krti Tallam"], "title": "Security-First AI: Foundations for Robust and Trustworthy Systems", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The conversation around artificial intelligence (AI) often focuses on safety,\ntransparency, accountability, alignment, and responsibility. However, AI\nsecurity (i.e., the safeguarding of data, models, and pipelines from\nadversarial manipulation) underpins all of these efforts. This manuscript\nposits that AI security must be prioritized as a foundational layer. We present\na hierarchical view of AI challenges, distinguishing security from safety, and\nargue for a security-first approach to enable trustworthy and resilient AI\nsystems. We discuss core threat models, key attack vectors, and emerging\ndefense mechanisms, concluding that a metric-driven approach to AI security is\nessential for robust AI safety, transparency, and accountability.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5c06AI\u5b89\u5168\u4f5c\u4e3a\u57fa\u7840\u5c42\u4f18\u5148\u8003\u8651\uff0c\u63d0\u51fa\u5206\u5c42\u89c6\u89d2\u533a\u5206\u5b89\u5168\u4e0e\u4fdd\u969c\uff0c\u5e76\u63d0\u5021\u4ee5\u5b89\u5168\u4e3a\u5148\u7684\u65b9\u6cd5\u6784\u5efa\u53ef\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dAI\u8ba8\u8bba\u591a\u805a\u7126\u4e8e\u5b89\u5168\u6027\u3001\u900f\u660e\u5ea6\u7b49\uff0c\u4f46AI\u5b89\u5168\uff08\u5bf9\u6297\u6027\u64cd\u7eb5\u7684\u9632\u62a4\uff09\u662f\u8fd9\u4e9b\u52aa\u529b\u7684\u57fa\u7840\uff0c\u9700\u4f18\u5148\u5173\u6ce8\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u89c6\u89d2\uff0c\u533a\u5206\u5b89\u5168\u4e0e\u4fdd\u969c\uff0c\u8ba8\u8bba\u6838\u5fc3\u5a01\u80c1\u6a21\u578b\u3001\u653b\u51fb\u5411\u91cf\u53ca\u9632\u5fa1\u673a\u5236\u3002", "result": "\u5f3a\u8c03\u4ee5\u6307\u6807\u9a71\u52a8\u7684\u65b9\u6cd5\u5bf9AI\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u652f\u6301\u7a33\u5065\u7684AI\u5b89\u5168\u6027\u3001\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u3002", "conclusion": "AI\u5b89\u5168\u662f\u6784\u5efa\u53ef\u4fe1\u8d56\u548c\u5f39\u6027AI\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u9700\u4f18\u5148\u8003\u8651\u5e76\u91c7\u7528\u6307\u6807\u9a71\u52a8\u7684\u65b9\u6cd5\u3002"}}
{"id": "2504.16404", "pdf": "https://arxiv.org/pdf/2504.16404", "abs": "https://arxiv.org/abs/2504.16404", "authors": ["Md Fahimuzzman Sohan"], "title": "Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Cattle lameness is often caused by hoof injuries or interdigital dermatitis,\nleads to pain and significantly impacts essential physiological activities such\nas walking, feeding, and drinking. This study presents a deep learning-based\nmodel for detecting cattle lameness, sickness, or gait abnormalities using\npublicly available video data. The dataset consists of 50 unique videos from 40\nindividual cattle, recorded from various angles in both indoor and outdoor\nenvironments. Half of the dataset represents naturally walking\n(normal/non-lame) cattle, while the other half consists of cattle exhibiting\ngait abnormalities (lame). To enhance model robustness and generalizability,\ndata augmentation was applied to the training data. The pre-processed videos\nwere then classified using two deep learning models: ConvLSTM2D and 3D CNN. A\ncomparative analysis of the results demonstrates strong classification\nperformance. Specifically, the 3D CNN model achieved a video-level\nclassification accuracy of 90%, with precision, recall, and f1-score of 90.9%,\n90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower\naccuracy of 85%. This study highlights the effectiveness of directly applying\nclassification models to learn spatiotemporal features from video data,\noffering an alternative to traditional multi-stage approaches that typically\ninvolve object detection, pose estimation, and feature extraction. Besides, the\nfindings demonstrate that the proposed deep learning models, particularly the\n3D CNN, effectively classify and detect lameness in cattle while simplifying\nthe processing pipeline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7\u89c6\u9891\u6570\u636e\u68c0\u6d4b\u725b\u7684\u8ddb\u884c\u6216\u6b65\u6001\u5f02\u5e38\uff0c3D CNN\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe90%\u3002", "motivation": "\u725b\u7684\u8ddb\u884c\u5e38\u7531\u8e44\u90e8\u635f\u4f24\u6216\u8dbe\u95f4\u76ae\u708e\u5f15\u8d77\uff0c\u5f71\u54cd\u5176\u884c\u8d70\u3001\u8fdb\u98df\u7b49\u751f\u7406\u6d3b\u52a8\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u89c6\u9891\u6570\u636e\uff0c\u5305\u542b50\u4e2a\u89c6\u9891\uff0840\u5934\u725b\uff09\uff0c\u5206\u4e3a\u6b63\u5e38\u548c\u8ddb\u884c\u4e24\u7c7b\u3002\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86ConvLSTM2D\u548c3D CNN\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5206\u7c7b\u6548\u679c\u3002", "result": "3D CNN\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u89c6\u9891\u5206\u7c7b\u51c6\u786e\u738790%\uff0c\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u4e3a90.9%\uff1bConvLSTM2D\u6a21\u578b\u51c6\u786e\u738785%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c3D CNN\u80fd\u6709\u6548\u5206\u7c7b\u725b\u8ddb\u884c\uff0c\u7b80\u5316\u4e86\u4f20\u7edf\u591a\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\uff0c\u4e3a\u76f4\u63a5\u5b66\u4e60\u65f6\u7a7a\u7279\u5f81\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.16112", "pdf": "https://arxiv.org/pdf/2504.16112", "abs": "https://arxiv.org/abs/2504.16112", "authors": ["Myunghyun Rhee", "Joonseop Sim", "Taeyoung Ahn", "Seungyong Lee", "Daegun Yoon", "Euiseok Kim", "Kyoung Park", "Youngpyo Joo", "Hosik Kim"], "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.DC"], "comment": "6 pages", "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5e26\u5bbd\u5904\u7406\u5355\u5143\uff08HPU\uff09\u4f5c\u4e3a\u534f\u5904\u7406\u5668\uff0c\u901a\u8fc7\u5378\u8f7d\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u5347GPU\u5728\u5927\u578b\u6279\u6b21LLM\u63a8\u7406\u4e2d\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3Transformer-based LLMs\u4e2d\u6ce8\u610f\u529b\u5c42\u56e0\u4f4e\u64cd\u4f5c\u5f3a\u5ea6\u548cKV\u7f13\u5b58\u9ad8\u5185\u5b58\u9700\u6c42\u5bfc\u81f4\u7684GPU\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u57fa\u4e8ePCIe FPGA\u5361\u7684HPU\u539f\u578b\uff0c\u4f5c\u4e3aGPU\u7cfb\u7edf\u7684\u9644\u52a0\u5361\uff0c\u5206\u62c5\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\u3002", "result": "GPU-HPU\u5f02\u6784\u7cfb\u7edf\u76f8\u6bd4\u7eafGPU\u7cfb\u7edf\u6027\u80fd\u63d0\u53474.1\u500d\uff0c\u80fd\u6548\u63d0\u53474.6\u500d\u3002", "conclusion": "HPU\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u534f\u5904\u7406\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u7684\u6548\u7387\u548c\u80fd\u6548\uff0c\u65e0\u9700\u589e\u52a0GPU\u6570\u91cf\u3002"}}
{"id": "2504.16419", "pdf": "https://arxiv.org/pdf/2504.16419", "abs": "https://arxiv.org/abs/2504.16419", "authors": ["Qi Yang", "Weichen Bi", "Haiyang Shen", "Yaoqi Guo", "Yun Ma"], "title": "PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Graphical User Interface (GUI) datasets are crucial for various downstream\ntasks. However, GUI datasets often generate annotation information through\nautomatic labeling, which commonly results in inaccurate GUI element BBox\nannotations, including missing, duplicate, or meaningless BBoxes. These issues\ncan degrade the performance of models trained on these datasets, limiting their\neffectiveness in real-world applications. Additionally, existing GUI datasets\nonly provide BBox annotations visually, which restricts the development of\nvisually related GUI downstream tasks. To address these issues, we introduce\nPixelWeb, a large-scale GUI dataset containing over 100,000 annotated web\npages. PixelWeb is constructed using a novel automatic annotation approach that\nintegrates visual feature extraction and Document Object Model (DOM) structure\nanalysis through two core modules: channel derivation and layer analysis.\nChannel derivation ensures accurate localization of GUI elements in cases of\nocclusion and overlapping elements by extracting BGRA four-channel bitmap\nannotations. Layer analysis uses the DOM to determine the visibility and\nstacking order of elements, providing precise BBox annotations. Additionally,\nPixelWeb includes comprehensive metadata such as element images, contours, and\nmask annotations. Manual verification by three independent annotators confirms\nthe high quality and accuracy of PixelWeb annotations. Experimental results on\nGUI element detection tasks show that PixelWeb achieves performance on the\nmAP95 metric that is 3-7 times better than existing datasets. We believe that\nPixelWeb has great potential for performance improvement in downstream tasks\nsuch as GUI generation and automated user interaction.", "AI": {"tldr": "PixelWeb\u662f\u4e00\u4e2a\u5927\u89c4\u6a21GUI\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u548cDOM\u7ed3\u6784\u5206\u6790\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684BBox\u6ce8\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u5143\u7d20\u68c0\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GUI\u6570\u636e\u96c6\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u751f\u6210\u7684BBox\u6ce8\u91ca\u4e0d\u51c6\u786e\uff08\u5982\u7f3a\u5931\u3001\u91cd\u590d\u6216\u65e0\u610f\u4e49\uff09\uff0c\u4e14\u4ec5\u63d0\u4f9b\u89c6\u89c9BBox\u6ce8\u91ca\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u76f8\u5173\u4e0b\u6e38\u4efb\u52a1\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u901a\u9053\u6d3e\u751f\u548c\u5c42\u6b21\u5206\u6790\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u7ed3\u5408BGRA\u56db\u901a\u9053\u4f4d\u56fe\u6ce8\u91ca\u548cDOM\u7ed3\u6784\u5206\u6790\uff0c\u751f\u6210\u7cbe\u786e\u7684BBox\u6ce8\u91ca\u3002", "result": "\u5728GUI\u5143\u7d20\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cPixelWeb\u7684mAP95\u6307\u6807\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u9ad83-7\u500d\u3002", "conclusion": "PixelWeb\u4e3aGUI\u751f\u6210\u548c\u81ea\u52a8\u5316\u7528\u6237\u4ea4\u4e92\u7b49\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u6027\u80fd\u6539\u8fdb\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.16113", "pdf": "https://arxiv.org/pdf/2504.16113", "abs": "https://arxiv.org/abs/2504.16113", "authors": ["Xin Wang", "Xiaoqi Li"], "title": "AI-Based Vulnerability Analysis of NFT Smart Contracts", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "In the research experiment of this article, our research work is divided into\nseveral stages. Firstly, we collected a large number of smart contract codes\nand classified them, identifying several common defects, including Risky\nMutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and\nPublic Burns. Secondly, we used Python to process the smart contracts. On the\none hand, we modified the file names, and on the other hand, we batched the\nprocess of the content for analysis and application. Next, we built a model of\nthe decision tree. Firstly, we carried out the feature extraction. We selected\nthe algorithm and divided the data. After comparing and processing, we chose\nthe CART classification tree to process. By gene coefficient, we analyzed and\nsorted the data, and got the initial model of the decision tree. Then, we\nintroduced the random forest model on the basis of the decision tree. From\nabstracting the same amount of samples to selecting features randomly.From\nadjusting and optimizing parameters to completing the construction of the\nforest model. Finally, we compared and analyzed the decision tree, random\nforest, and self-built model in the paper and drew general conclusions.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6536\u96c6\u548c\u5206\u7c7b\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\uff0c\u8bc6\u522b\u5e38\u89c1\u7f3a\u9677\uff0c\u5e76\u5229\u7528\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u5206\u6790\uff0c\u6700\u7ec8\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u667a\u80fd\u5408\u7ea6\u4e2d\u7684\u5e38\u89c1\u7f3a\u9677\uff0c\u5e76\u5f00\u53d1\u6709\u6548\u7684\u6a21\u578b\u6765\u8bc6\u522b\u548c\u5206\u6790\u8fd9\u4e9b\u7f3a\u9677\u3002", "method": "1. \u6536\u96c6\u548c\u5206\u7c7b\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\uff1b2. \u4f7f\u7528Python\u5904\u7406\u6570\u636e\uff1b3. \u6784\u5efa\u51b3\u7b56\u6811\u6a21\u578b\u5e76\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff1b4. \u5f15\u5165\u968f\u673a\u68ee\u6797\u6a21\u578b\u5e76\u4f18\u5316\u53c2\u6570\uff1b5. \u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u6210\u529f\u5206\u6790\u4e86\u667a\u80fd\u5408\u7ea6\u7684\u7f3a\u9677\uff0c\u5e76\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002", "conclusion": "\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u667a\u80fd\u5408\u7ea6\u7f3a\u9677\u5206\u6790\u4e2d\u5747\u8868\u73b0\u826f\u597d\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2504.16433", "pdf": "https://arxiv.org/pdf/2504.16433", "abs": "https://arxiv.org/abs/2504.16433", "authors": ["Hariseetharam Gunduboina", "Muhammad Haris Khan", "Biplab Banerjee"], "title": "FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, large-scale vision-language models (VLMs) like CLIP have\ngained attention for their zero-shot inference using instructional text\nprompts. While these models excel in general computer vision, their potential\nfor domain generalization in remote sensing (RS) remains underexplored.\nExisting approaches enhance prompt learning by generating visual prompt tokens\nbut rely on full-image features, introducing noise and background artifacts\nthat vary within a class, causing misclassification. To address this, we\npropose FrogDogNet, a novel prompt learning framework integrating Fourier\nfrequency filtering and self-attention to improve RS scene classification and\ndomain generalization. FrogDogNet selectively retains invariant low-frequency\ncomponents while eliminating noise and irrelevant backgrounds, ensuring robust\nfeature representation across domains. The model first extracts significant\nfeatures via projection and self-attention, then applies frequency-based\nfiltering to preserve essential structural information for prompt learning.\nExtensive experiments on four RS datasets and three domain generalization tasks\nshow that FrogDogNet consistently outperforms state-of-the-art prompt learning\nmethods, demonstrating superior adaptability across domain shifts. Our findings\nhighlight the effectiveness of frequency-based invariant feature retention in\ngeneralization, paving the way for broader applications. Our code is available\nat https://github.com/HariseetharamG/FrogDogNet", "AI": {"tldr": "FrogDogNet\u662f\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u9891\u7387\u8fc7\u6ee4\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u63d0\u5347\u9065\u611f\u573a\u666f\u5206\u7c7b\u548c\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u901a\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9065\u611f\u9886\u57df\u7684\u9886\u57df\u6cdb\u5316\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5168\u56fe\u50cf\u7279\u5f81\uff0c\u5f15\u5165\u566a\u58f0\u548c\u80cc\u666f\u5e72\u6270\uff0c\u5bfc\u81f4\u5206\u7c7b\u9519\u8bef\u3002", "method": "FrogDogNet\u901a\u8fc7\u5085\u91cc\u53f6\u9891\u7387\u8fc7\u6ee4\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9009\u62e9\u6027\u4fdd\u7559\u4e0d\u53d8\u7684\u4f4e\u9891\u6210\u5206\uff0c\u6d88\u9664\u566a\u58f0\u548c\u65e0\u5173\u80cc\u666f\uff0c\u63d0\u53d6\u5173\u952e\u7279\u5f81\u7528\u4e8e\u63d0\u793a\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2a\u9065\u611f\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFrogDogNet\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8de8\u9886\u57df\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u9891\u7387\u4e0d\u53d8\u7279\u5f81\u4fdd\u7559\u5728\u9886\u57df\u6cdb\u5316\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2504.16116", "pdf": "https://arxiv.org/pdf/2504.16116", "abs": "https://arxiv.org/abs/2504.16116", "authors": ["Miracle Master", "Rainy Sun", "Anya Reese", "Joey Ouyang", "Alex Chen", "Winter Dong", "Frank Li", "James Yi", "Garry Zhao", "Tony Ling", "Hobert Wong", "Lowes Yang"], "title": "DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nprogress on a wide range of natural language processing tasks. However, their\neffectiveness in specialized and rapidly evolving domains such as Web3 remains\nunderexplored. In this paper, we introduce DMind Benchmark, a novel framework\nthat systematically tests LLMs across nine key categories encompassing\nblockchain fundamentals, infrastructure, smart contract analysis, decentralized\nfinance (DeFi), decentralized autonomous organizations (DAOs), non-fungible\ntokens (NFTs), token economics, meme concepts, and security vulnerabilities.\n  DMind Benchmark goes beyond conventional multiple-choice questions by\nincorporating domain-specific subjective tasks (e.g., smart contract code\nauditing and repair, numeric reasoning on on-chain data, and fill-in\nassessments), thereby capturing real-world complexities and stress-testing\nmodel adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek,\nClaude, and Gemini series) on DMind Benchmark, uncovering performance gaps in\nWeb3-specific reasoning and application, particularly in emerging areas like\ntoken economics and meme concepts. Even the strongest models face significant\nchallenges in identifying subtle security vulnerabilities and analyzing complex\nDeFi mechanisms. To foster progress in this area, we publicly release our\nbenchmark dataset, evaluation pipeline, and annotated results at\nhttp://www.dmind.ai, offering a valuable resource for advancing specialized\ndomain adaptation and the development of more robust Web3-enabled LLMs.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DMind Benchmark\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728Web3\u9886\u57df\u8868\u73b0\u7684\u6846\u67b6\uff0c\u6db5\u76d6\u4e5d\u4e2a\u5173\u952e\u7c7b\u522b\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728Web3\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728Web3\u7b49\u4e13\u4e1a\u548c\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7DMind Benchmark\u7cfb\u7edf\u6d4b\u8bd515\u79cd\u6d41\u884cLLMs\uff0c\u5305\u62ec\u591a\u9009\u548c\u9886\u57df\u7279\u5b9a\u4e3b\u89c2\u4efb\u52a1\uff08\u5982\u667a\u80fd\u5408\u7ea6\u5ba1\u8ba1\u3001\u94fe\u4e0a\u6570\u636e\u63a8\u7406\u7b49\uff09\u3002", "result": "\u53d1\u73b0LLMs\u5728Web3\u7279\u5b9a\u63a8\u7406\u548c\u5e94\u7528\u4e2d\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u65b0\u5174\u9886\u57df\u548c\u5b89\u5168\u6f0f\u6d1e\u8bc6\u522b\u65b9\u9762\u3002", "conclusion": "\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u4fc3\u8fdbWeb3\u9886\u57dfLLMs\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u9002\u5e94\u3002"}}
{"id": "2504.16443", "pdf": "https://arxiv.org/pdf/2504.16443", "abs": "https://arxiv.org/abs/2504.16443", "authors": ["Duy-Tho Le", "Trung Pham", "Jianfei Cai", "Hamid Rezatofighi"], "title": "Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Optimizing the similarity between parametric shapes is crucial for numerous\ncomputer vision tasks, where Intersection over Union (IoU) stands as the\ncanonical measure. However, existing optimization methods exhibit significant\nshortcomings: regression-based losses like L1/L2 lack correlation with IoU,\nIoU-based losses are unstable and limited to simple shapes, and task-specific\nmethods are computationally intensive and not generalizable accross domains. As\na result, the current landscape of parametric shape objective functions has\nbecome scattered, with each domain proposing distinct IoU approximations. To\naddress this, we unify the parametric shape optimization objective functions by\nintroducing Marginalized Generalized IoU (MGIoU), a novel loss function that\novercomes these challenges by projecting structured convex shapes onto their\nunique shape Normals to compute one-dimensional normalized GIoU. MGIoU offers a\nsimple, efficient, fully differentiable approximation strongly correlated with\nIoU. We then extend MGIoU to MGIoU+ that supports optimizing unstructured\nconvex shapes. Together, MGIoU and MGIoU+ unify parametric shape optimization\nacross diverse applications. Experiments on standard benchmarks demonstrate\nthat MGIoU and MGIoU+ consistently outperform existing losses while reducing\nloss computation latency by 10-40x. Additionally, MGIoU and MGIoU+ satisfy\nmetric properties and scale-invariance, ensuring robustness as an objective\nfunction. We further propose MGIoU- for minimizing overlaps in tasks like\ncollision-free trajectory prediction. Code is available at\nhttps://ldtho.github.io/MGIoU", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570MGIoU\u548cMGIoU+\uff0c\u7528\u4e8e\u7edf\u4e00\u53c2\u6570\u5316\u5f62\u72b6\u4f18\u5316\u7684\u76ee\u6807\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u5728\u53c2\u6570\u5316\u5f62\u72b6\u76f8\u4f3c\u6027\u4f18\u5316\u4e2d\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u5982\u56de\u5f52\u635f\u5931\u4e0eIoU\u4e0d\u76f8\u5173\uff0cIoU\u635f\u5931\u4e0d\u7a33\u5b9a\u4e14\u4ec5\u9002\u7528\u4e8e\u7b80\u5355\u5f62\u72b6\uff0c\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u4e0d\u901a\u7528\u3002", "method": "\u901a\u8fc7\u5c06\u7ed3\u6784\u5316\u51f8\u5f62\u72b6\u6295\u5f71\u5230\u5176\u72ec\u7279\u5f62\u72b6\u6cd5\u7ebf\u4e0a\uff0c\u8ba1\u7b97\u4e00\u7ef4\u5f52\u4e00\u5316GIoU\uff0c\u63d0\u51faMGIoU\u548cMGIoU+\uff0c\u652f\u6301\u4f18\u5316\u975e\u7ed3\u6784\u5316\u51f8\u5f62\u72b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMGIoU\u548cMGIoU+\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u635f\u5931\u51fd\u6570\uff0c\u8ba1\u7b97\u5ef6\u8fdf\u51cf\u5c1110-40\u500d\uff0c\u4e14\u6ee1\u8db3\u5ea6\u91cf\u6027\u8d28\u548c\u5c3a\u5ea6\u4e0d\u53d8\u6027\u3002", "conclusion": "MGIoU\u548cMGIoU+\u7edf\u4e00\u4e86\u53c2\u6570\u5316\u5f62\u72b6\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51faMGIoU-\u7528\u4e8e\u6700\u5c0f\u5316\u91cd\u53e0\u4efb\u52a1\u3002"}}
{"id": "2504.16455", "pdf": "https://arxiv.org/pdf/2504.16455", "abs": "https://arxiv.org/abs/2504.16455", "authors": ["Shun Zou", "Yi Zou", "Juncheng Li", "Guangwei Gao", "Guojun Qi"], "title": "Cross Paradigm Representation and Alignment Transformer for Image Deraining", "categories": ["cs.CV"], "comment": "code: https://github.com/zs1314/CPRAformer", "summary": "Transformer-based networks have achieved strong performance in low-level\nvision tasks like image deraining by utilizing spatial or channel-wise\nself-attention. However, irregular rain patterns and complex geometric overlaps\nchallenge single-paradigm architectures, necessitating a unified framework to\nintegrate complementary global-local and spatial-channel representations. To\naddress this, we propose a novel Cross Paradigm Representation and Alignment\nTransformer (CPRAformer). Its core idea is the hierarchical representation and\nalignment, leveraging the strengths of both paradigms (spatial-channel and\nglobal-local) to aid image reconstruction. It bridges the gap within and\nbetween paradigms, aligning and coordinating them to enable deep interaction\nand fusion of features. Specifically, we use two types of self-attention in the\nTransformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial\npixel refinement self-attention (SPR-SA). SPC-SA enhances global channel\ndependencies through dynamic sparsity, while SPR-SA focuses on spatial rain\ndistribution and fine-grained texture recovery. To address the feature\nmisalignment and knowledge differences between them, we introduce the Adaptive\nAlignment Frequency Module (AAFM), which aligns and interacts with features in\na two-stage progressive manner, enabling adaptive guidance and complementarity.\nThis reduces the information gap within and between paradigms. Through this\nunified cross-paradigm dynamic interaction framework, we achieve the extraction\nof the most valuable interactive fusion information from the two paradigms.\nExtensive experiments demonstrate that our model achieves state-of-the-art\nperformance on eight benchmark datasets and further validates CPRAformer's\nrobustness in other image restoration tasks and downstream applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPRAformer\u7684\u65b0\u578bTransformer\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5168\u5c40-\u5c40\u90e8\u548c\u7a7a\u95f4-\u901a\u9053\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u53bb\u96e8\u4efb\u52a1\u4e2d\u4e0d\u89c4\u5219\u96e8\u7eb9\u548c\u590d\u6742\u51e0\u4f55\u91cd\u53e0\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5355\u8303\u5f0f\u67b6\u6784\u96be\u4ee5\u5904\u7406\u4e0d\u89c4\u5219\u96e8\u7eb9\u548c\u590d\u6742\u51e0\u4f55\u91cd\u53e0\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6574\u5408\u4e92\u8865\u7684\u5168\u5c40-\u5c40\u90e8\u548c\u7a7a\u95f4-\u901a\u9053\u8868\u793a\u3002", "method": "\u63d0\u51faCPRAformer\uff0c\u91c7\u7528\u7a00\u758f\u63d0\u793a\u901a\u9053\u81ea\u6ce8\u610f\u529b\uff08SPC-SA\uff09\u548c\u7a7a\u95f4\u50cf\u7d20\u7ec6\u5316\u81ea\u6ce8\u610f\u529b\uff08SPR-SA\uff09\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5bf9\u9f50\u9891\u7387\u6a21\u5757\uff08AAFM\uff09\u5b9e\u73b0\u7279\u5f81\u5bf9\u9f50\u548c\u4ea4\u4e92\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5176\u4ed6\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "CPRAformer\u901a\u8fc7\u8de8\u8303\u5f0f\u7684\u52a8\u6001\u4ea4\u4e92\u6846\u67b6\uff0c\u6709\u6548\u63d0\u53d6\u548c\u878d\u5408\u4e92\u8865\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u53bb\u96e8\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.16118", "pdf": "https://arxiv.org/pdf/2504.16118", "abs": "https://arxiv.org/abs/2504.16118", "authors": ["Milad Rahmati"], "title": "Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As cyber threats continue to evolve, securing edge networks has become\nincreasingly challenging due to their distributed nature and resource\nlimitations. Many AI-driven threat detection systems rely on complex deep\nlearning models, which, despite their high accuracy, suffer from two major\ndrawbacks: lack of interpretability and high computational cost. Black-box AI\nmodels make it difficult for security analysts to understand the reasoning\nbehind their predictions, limiting their practical deployment. Moreover,\nconventional deep learning techniques demand significant computational\nresources, rendering them unsuitable for edge devices with limited processing\npower. To address these issues, this study introduces an Explainable and\nLightweight AI (ELAI) framework designed for real-time cyber threat detection\nin edge networks. Our approach integrates interpretable machine learning\nalgorithms with optimized lightweight deep learning techniques, ensuring both\ntransparency and computational efficiency. The proposed system leverages\ndecision trees, attention-based deep learning, and federated learning to\nenhance detection accuracy while maintaining explainability. We evaluate ELAI\nusing benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing\nits performance across diverse cyberattack scenarios. Experimental results\ndemonstrate that the proposed framework achieves high detection rates with\nminimal false positives, all while significantly reducing computational demands\ncompared to traditional deep learning methods. The key contributions of this\nwork include: (1) a novel interpretable AI-based cybersecurity model tailored\nfor edge computing environments, (2) an optimized lightweight deep learning\napproach for real-time cyber threat detection, and (3) a comprehensive analysis\nof explainability techniques in AI-driven cybersecurity applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u8f7b\u91cf\u7ea7\u7684AI\u6846\u67b6\uff08ELAI\uff09\uff0c\u7528\u4e8e\u8fb9\u7f18\u7f51\u7edc\u4e2d\u7684\u5b9e\u65f6\u7f51\u7edc\u5a01\u80c1\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7684\u4e0d\u53ef\u89e3\u91ca\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u8fb9\u7f18\u7f51\u7edc\u7684\u5206\u5e03\u5f0f\u7279\u6027\u548c\u8d44\u6e90\u9650\u5236\u4f7f\u5f97\u7f51\u7edc\u5b89\u5168\u9632\u62a4\u9762\u4e34\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684AI\u9a71\u52a8\u5a01\u80c1\u68c0\u6d4b\u7cfb\u7edf\u56e0\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u96be\u4ee5\u5b9e\u7528\u3002", "method": "\u7ed3\u5408\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u4f18\u5316\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u91c7\u7528\u51b3\u7b56\u6811\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u8054\u90a6\u5b66\u4e60\uff0c\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728CICIDS\u548cUNSW-NB15\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cELAI\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7387\u548c\u4f4e\u8bef\u62a5\u7387\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "ELAI\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u7f51\u7edc\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u5b9e\u65f6\u6027\u548c\u4f4e\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2504.16467", "pdf": "https://arxiv.org/pdf/2504.16467", "abs": "https://arxiv.org/abs/2504.16467", "authors": ["Qishan He", "Lingjun Zhao", "Ru Luo", "Siqian Zhang", "Lin Lei", "Kefeng Ji", "Gangyao Kuang"], "title": "MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Aircraft recognition in synthetic aperture radar (SAR) imagery is a\nfundamental mission in both military and civilian applications. Recently deep\nlearning (DL) has emerged a dominant paradigm for its explosive performance on\nextracting discriminative features. However, current classification algorithms\nfocus primarily on learning decision hyperplane without enough comprehension on\naircraft structural knowledge. Inspired by the fined aircraft annotation\nmethods for optical remote sensing images (RSI), we first introduce a\nstructure-based SAR aircraft annotations approach to provide structural and\ncompositional supplement information. On this basis, we propose a multi-task\nstructure guided learning (MTSGL) network for robust and interpretable SAR\naircraft recognition. Besides the classification task, MTSGL includes a\nstructural semantic awareness (SSA) module and a structural consistency\nregularization (SCR) module. The SSA is designed to capture structure semantic\ninformation, which is conducive to gain human-like comprehension of aircraft\nknowledge. The SCR helps maintain the geometric consistency between the\naircraft structure in SAR imagery and the proposed annotation. In this process,\nthe structural attribute can be disentangled in a geometrically meaningful\nmanner. In conclusion, the MTSGL is presented with the expert-level aircraft\nprior knowledge and structure guided learning paradigm, aiming to comprehend\nthe aircraft concept in a way analogous to the human cognitive process.\nExtensive experiments are conducted on a self-constructed multi-task SAR\naircraft recognition dataset (MT-SARD) and the effective results illustrate the\nsuperiority of robustness and interpretation ability of the proposed MTSGL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u7f51\u7edc\uff08MTSGL\uff09\uff0c\u7528\u4e8eSAR\u56fe\u50cf\u4e2d\u7684\u98de\u673a\u8bc6\u522b\uff0c\u7ed3\u5408\u7ed3\u6784\u8bed\u4e49\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524dSAR\u56fe\u50cf\u98de\u673a\u8bc6\u522b\u7b97\u6cd5\u7f3a\u4e4f\u5bf9\u98de\u673a\u7ed3\u6784\u77e5\u8bc6\u7684\u6df1\u5165\u7406\u89e3\uff0c\u800c\u5149\u5b66\u9065\u611f\u56fe\u50cf\u7684\u7cbe\u7ec6\u6807\u6ce8\u65b9\u6cd5\u4e3aSAR\u56fe\u50cf\u63d0\u4f9b\u4e86\u542f\u53d1\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u7684SAR\u98de\u673a\u6807\u6ce8\u65b9\u6cd5\uff0c\u5e76\u63d0\u51faMTSGL\u7f51\u7edc\uff0c\u5305\u542b\u5206\u7c7b\u4efb\u52a1\u3001\u7ed3\u6784\u8bed\u4e49\u611f\u77e5\u6a21\u5757\uff08SSA\uff09\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u6b63\u5219\u5316\u6a21\u5757\uff08SCR\uff09\u3002", "result": "\u5728\u81ea\u5efa\u6570\u636e\u96c6MT-SARD\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cMTSGL\u5728\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "MTSGL\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u7ea7\u5148\u9a8c\u77e5\u8bc6\u548c\u7ed3\u6784\u5f15\u5bfc\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u8ba4\u77e5\u7684\u98de\u673a\u8bc6\u522b\u3002"}}
{"id": "2504.16120", "pdf": "https://arxiv.org/pdf/2504.16120", "abs": "https://arxiv.org/abs/2504.16120", "authors": ["Chaima Njeh", "Ha\u00effa Nakouri", "Fehmi Jaafar"], "title": "A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content", "categories": ["cs.CR", "cs.AI"], "comment": "This paper is under revision in the International Journal of\n  Information Security", "summary": "Large Language Models (LLM) have made remarkable progress, but concerns about\npotential biases and harmful content persist. To address these apprehensions,\nwe introduce a practical solution for ensuring LLM's safe and ethical use. Our\nnovel approach focuses on a post-generation correction mechanism, the\nBART-Corrective Model, which adjusts generated content to ensure safety and\nsecurity. Unlike relying solely on model fine-tuning or prompt engineering, our\nmethod provides a robust data-centric alternative for mitigating harmful\ncontent. We demonstrate the effectiveness of our approach through experiments\non multiple toxic datasets, which show a significant reduction in mean toxicity\nand jail-breaking scores after integration. Specifically, our results show a\nreduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,\na substantial reduction of 28% and 5% with PaLM2, a reduction of approximately\n26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.\nThese results demonstrate the potential of our approach to improve the safety\nand security of LLM, making them more suitable for real-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u751f\u6210\u4fee\u6b63\u673a\u5236BART-Corrective Model\uff0c\u7528\u4e8e\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6709\u5bb3\u5185\u5bb9\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6bd2\u6027\u548c\u8d8a\u72f1\u5206\u6570\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u80fd\u5b58\u5728\u7684\u504f\u89c1\u548c\u6709\u5bb3\u5185\u5bb9\u95ee\u9898\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u4e0e\u4f26\u7406\u4f7f\u7528\u3002", "method": "\u91c7\u7528\u540e\u751f\u6210\u4fee\u6b63\u673a\u5236BART-Corrective Model\uff0c\u8c03\u6574\u751f\u6210\u5185\u5bb9\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u6a21\u578b\u5fae\u8c03\u6216\u63d0\u793a\u5de5\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6bd2\u6027\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u6bd2\u6027\u548c\u8d8a\u72f1\u5206\u6570\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\uff1aGPT-4\uff0815%\u548c21%\uff09\u3001PaLM2\uff0828%\u548c5%\uff09\u3001Mistral-7B\uff0826%\u548c23%\uff09\u3001Gemma-2b-it\uff0811.1%\u548c19%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u5b89\u5168\u6027\u548c\u9002\u7528\u6027\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.16471", "pdf": "https://arxiv.org/pdf/2504.16471", "abs": "https://arxiv.org/abs/2504.16471", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Gangshan Wu"], "title": "RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory", "categories": ["cs.CV"], "comment": null, "summary": "The RGB-Depth (RGB-D) Video Object Segmentation (VOS) aims to integrate the\nfine-grained texture information of RGB with the spatial geometric clues of\ndepth modality, boosting the performance of segmentation. However,\noff-the-shelf RGB-D segmentation methods fail to fully explore cross-modal\ninformation and suffer from object drift during long-term prediction. In this\npaper, we propose a novel RGB-D VOS method via multi-store feature memory for\nrobust segmentation. Specifically, we design the hierarchical modality\nselection and fusion, which adaptively combines features from both modalities.\nAdditionally, we develop a segmentation refinement module that effectively\nutilizes the Segmentation Anything Model (SAM) to refine the segmentation mask,\nensuring more reliable results as memory to guide subsequent segmentation\ntasks. By leveraging spatio-temporal embedding and modality embedding, mixed\nprompts and fused images are fed into SAM to unleash its potential in RGB-D\nVOS. Experimental results show that the proposed method achieves\nstate-of-the-art performance on the latest RGB-D VOS benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5b58\u50a8\u7279\u5f81\u8bb0\u5fc6\u7684RGB-D\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u6001\u878d\u5408\u548cSAM\u7ec6\u5316\u6a21\u5757\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RGB-D\u5206\u5272\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8de8\u6a21\u6001\u4fe1\u606f\u4e14\u5b58\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u7684\u5bf9\u8c61\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u5206\u5c42\u6a21\u6001\u9009\u62e9\u4e0e\u878d\u5408\u673a\u5236\uff0c\u7ed3\u5408SAM\u7ec6\u5316\u6a21\u5757\uff0c\u5229\u7528\u65f6\u7a7a\u548c\u6a21\u6001\u5d4c\u5165\u751f\u6210\u6df7\u5408\u63d0\u793a\u548c\u878d\u5408\u56fe\u50cf\u3002", "result": "\u5728\u6700\u65b0RGB-D VOS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548cSAM\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86RGB-D\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.16122", "pdf": "https://arxiv.org/pdf/2504.16122", "abs": "https://arxiv.org/abs/2504.16122", "authors": ["Xuhui Zhou", "Zhe Su", "Sophie Feng", "Jiaxu Zhou", "Jen-tse Huang", "Hsien-Te Kao", "Spencer Lynch", "Svitlana Volkova", "Tongshuang Sherry Wu", "Anita Woolley", "Hao Zhu", "Maarten Sap"], "title": "SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation", "categories": ["cs.CY", "cs.AI"], "comment": "The first author and the second author contributed equally", "summary": "Social simulation through large language model (LLM) agents is a promising\napproach to explore and validate hypotheses related to social science questions\nand LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable\nsocial simulation system that addresses the technical barriers of current\nframeworks while enabling practitioners to generate multi-turn and multi-party\nLLM-based interactions with customizable evaluation metrics for hypothesis\ntesting. SOTOPIA-S4 comes as a pip package that contains a simulation engine,\nan API server with flexible RESTful APIs for simulation management, and a web\ninterface that enables both technical and non-technical users to design, run,\nand analyze simulations without programming. We demonstrate the usefulness of\nSOTOPIA-S4 with two use cases involving dyadic hiring negotiation and\nmulti-party planning scenarios.", "AI": {"tldr": "SOTOPIA-S4\u662f\u4e00\u4e2a\u5feb\u901f\u3001\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u793e\u4ea4\u6a21\u62df\u7cfb\u7edf\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u652f\u6301\u591a\u8f6e\u548c\u591a\u65b9\u7684\u4ea4\u4e92\uff0c\u5e76\u63d0\u4f9b\u53ef\u5b9a\u5236\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u793e\u4ea4\u6a21\u62df\u6846\u67b6\u7684\u6280\u672f\u969c\u788d\uff0c\u652f\u6301\u793e\u4f1a\u79d1\u5b66\u95ee\u9898\u548cLLM\u4ee3\u7406\u884c\u4e3a\u7684\u5047\u8bbe\u9a8c\u8bc1\u3002", "method": "\u63d0\u4f9b\u5305\u542b\u6a21\u62df\u5f15\u64ce\u3001RESTful API\u670d\u52a1\u5668\u548cWeb\u754c\u9762\u7684pip\u5305\uff0c\u652f\u6301\u975e\u6280\u672f\u7528\u6237\u8bbe\u8ba1\u3001\u8fd0\u884c\u548c\u5206\u6790\u6a21\u62df\u3002", "result": "\u901a\u8fc7\u62db\u8058\u8c08\u5224\u548c\u591a\u515a\u89c4\u5212\u4e24\u4e2a\u6848\u4f8b\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "SOTOPIA-S4\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u6280\u672f\u4e0e\u975e\u6280\u672f\u7528\u6237\uff0c\u652f\u6301\u590d\u6742\u7684\u793e\u4ea4\u6a21\u62df\u573a\u666f\u3002"}}
{"id": "2504.16487", "pdf": "https://arxiv.org/pdf/2504.16487", "abs": "https://arxiv.org/abs/2504.16487", "authors": ["Yahao Lu", "Yuehui Li", "Xingyuan Guo", "Shuai Yuan", "Yukai Shi", "Liang Lin"], "title": "Rethinking Generalizable Infrared Small Target Detection: A Real-scene Benchmark and Cross-view Representation Learning", "categories": ["cs.CV"], "comment": "A benchmark associated with real-world scenes for the Infrared Small\n  Target Detection (ISTD) is presented", "summary": "Infrared small target detection (ISTD) is highly sensitive to sensor type,\nobservation conditions, and the intrinsic properties of the target. These\nfactors can introduce substantial variations in the distribution of acquired\ninfrared image data, a phenomenon known as domain shift. Such distribution\ndiscrepancies significantly hinder the generalization capability of ISTD models\nacross diverse scenarios. To tackle this challenge, this paper introduces an\nISTD framework enhanced by domain adaptation. To alleviate distribution shift\nbetween datasets and achieve cross-sample alignment, we introduce Cross-view\nChannel Alignment (CCA). Additionally, we propose the Cross-view Top-K Fusion\nstrategy, which integrates target information with diverse background features,\nenhancing the model' s ability to extract critical data characteristics. To\nfurther mitigate the impact of noise on ISTD, we develop a Noise-guided\nRepresentation learning strategy. This approach enables the model to learn more\nnoise-resistant feature representations, to improve its generalization\ncapability across diverse noisy domains. Finally, we develop a dedicated\ninfrared small target dataset, RealScene-ISTD. Compared to state-of-the-art\nmethods, our approach demonstrates superior performance in terms of detection\nprobability (Pd), false alarm rate (Fa), and intersection over union (IoU). The\ncode is available at: https://github.com/luy0222/RealScene-ISTD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57df\u9002\u5e94\u7684\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u89c6\u56fe\u901a\u9053\u5bf9\u9f50\u548c\u566a\u58f0\u5f15\u5bfc\u8868\u793a\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u53d7\u4f20\u611f\u5668\u7c7b\u578b\u3001\u89c2\u6d4b\u6761\u4ef6\u548c\u76ee\u6807\u7279\u6027\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6570\u636e\u5206\u5e03\u5dee\u5f02\uff08\u57df\u504f\u79fb\uff09\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8de8\u89c6\u56fe\u901a\u9053\u5bf9\u9f50\uff08CCA\uff09\u548c\u8de8\u89c6\u56feTop-K\u878d\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u566a\u58f0\u5f15\u5bfc\u8868\u793a\u5b66\u4e60\uff0c\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u5e76\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u5728\u65b0\u6570\u636e\u96c6RealScene-ISTD\u4e0a\uff0c\u6a21\u578b\u5728\u68c0\u6d4b\u6982\u7387\uff08Pd\uff09\u3001\u8bef\u62a5\u7387\uff08Fa\uff09\u548c\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u57df\u9002\u5e94\u548c\u566a\u58f0\u5904\u7406\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2504.16499", "pdf": "https://arxiv.org/pdf/2504.16499", "abs": "https://arxiv.org/abs/2504.16499", "authors": ["Daniil Sinitsyn", "Linus H\u00e4renstam-Nielsen", "Daniel Cremers"], "title": "PRaDA: Projective Radial Distortion Averaging", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. 8 pages + references", "summary": "We tackle the problem of automatic calibration of radially distorted cameras\nin challenging conditions. Accurately determining distortion parameters\ntypically requires either 1) solving the full Structure from Motion (SfM)\nproblem involving camera poses, 3D points, and the distortion parameters, which\nis only possible if many images with sufficient overlap are provided, or 2)\nrelying heavily on learning-based methods that are comparatively less accurate.\nIn this work, we demonstrate that distortion calibration can be decoupled from\n3D reconstruction, maintaining the accuracy of SfM-based methods while avoiding\nmany of the associated complexities. This is achieved by working in Projective\nSpace, where the geometry is unique up to a homography, which encapsulates all\ncamera parameters except for distortion. Our proposed method, Projective Radial\nDistortion Averaging, averages multiple distortion estimates in a fully\nprojective framework without creating 3d points and full bundle adjustment. By\nrelying on pairwise projective relations, our methods support any\nfeature-matching approaches without constructing point tracks across multiple\nimages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6295\u5f71\u7a7a\u95f4\u4e2d\u89e3\u8026\u5f84\u5411\u7578\u53d8\u6821\u51c6\u4e0e3D\u91cd\u5efa\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u590d\u6742\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u81ea\u52a8\u6821\u51c6\u5f84\u5411\u7578\u53d8\u76f8\u673a\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5bf9\u5927\u91cf\u56fe\u50cf\u6216\u5b66\u4e60\u65b9\u6cd5\u7684\u4f9d\u8d56\u3002", "method": "\u5728\u6295\u5f71\u7a7a\u95f4\u4e2d\u5de5\u4f5c\uff0c\u5229\u7528\u540c\u6001\u6027\u5c01\u88c5\u9664\u7578\u53d8\u5916\u7684\u6240\u6709\u76f8\u673a\u53c2\u6570\uff0c\u63d0\u51fa\u6295\u5f71\u5f84\u5411\u7578\u53d8\u5e73\u5747\u65b9\u6cd5\uff0c\u65e0\u97003D\u70b9\u6216\u5b8c\u6574\u6346\u7ed1\u8c03\u6574\u3002", "result": "\u65b9\u6cd5\u4fdd\u6301\u4e86\u57fa\u4e8eSfM\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5176\u590d\u6742\u6027\uff0c\u652f\u6301\u4efb\u4f55\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u3002", "conclusion": "\u6295\u5f71\u5f84\u5411\u7578\u53d8\u5e73\u5747\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5f84\u5411\u7578\u53d8\u6821\u51c6\u65b9\u6848\u3002"}}
{"id": "2504.16129", "pdf": "https://arxiv.org/pdf/2504.16129", "abs": "https://arxiv.org/abs/2504.16129", "authors": ["Junwei Liao", "Muning Wen", "Jun Wang", "Weinan Zhang"], "title": "MARFT: Multi-Agent Reinforcement Fine-Tuning", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.RO"], "comment": "36 pages", "summary": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks requiring multifaceted reasoning and\ncollaboration, from generating high-quality presentation slides to conducting\nsophisticated scientific research. Meanwhile, RL has been widely recognized for\nits effectiveness in enhancing agent intelligence, but limited research has\ninvestigated the fine-tuning of LaMAS using foundational RL techniques.\nMoreover, the direct application of MARL methodologies to LaMAS introduces\nsignificant challenges, stemming from the unique characteristics and mechanisms\ninherent to LaMAS. To address these challenges, this article presents a\ncomprehensive study of LLM-based MARL and proposes a novel paradigm termed\nMulti-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal\nalgorithmic framework tailored for LaMAS, outlining the conceptual foundations,\nkey distinctions, and practical implementation strategies. We begin by\nreviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage\nfor a parallel analysis in the multi-agent domain. In the context of LaMAS, we\nelucidate critical differences between MARL and MARFT. These differences\nmotivate a transition toward a novel, LaMAS-oriented formulation of RFT.\nCentral to this work is the presentation of a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work aims to serve as a roadmap for researchers seeking to advance MARFT\ntoward resilient and adaptive solutions in agentic systems. Our implementation\nof the proposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5fae\u8c03\uff08MARFT\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u7528\u4e8e\u4f18\u5316\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LaMAS\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7b97\u6cd5\u6846\u67b6\u548c\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5e94\u7528\u4e8eLaMAS\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u9002\u5e94LaMAS\u7684\u72ec\u7279\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e86MARFT\u6846\u67b6\uff0c\u5305\u62ec\u7406\u8bba\u57fa\u7840\u3001\u5173\u952e\u533a\u522b\u548c\u5b9e\u9645\u5b9e\u73b0\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\u3002", "result": "MARFT\u4e3aLaMAS\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86MARL\u4e0eLaMAS\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86MARFT\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u65e8\u5728\u63a8\u52a8LaMAS\u5411\u66f4\u5177\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2504.16505", "pdf": "https://arxiv.org/pdf/2504.16505", "abs": "https://arxiv.org/abs/2504.16505", "authors": ["Meng Chu", "Yukang Chen", "Haokun Gui", "Shaozuo Yu", "Yi Wang", "Jiaya Jia"], "title": "TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Tourism and travel planning increasingly rely on digital assistance, yet\nexisting multimodal AI systems often lack specialized knowledge and contextual\nunderstanding of urban environments. We present TraveLLaMA, a specialized\nmultimodal language model designed for urban scene understanding and travel\nassistance. Our work addresses the fundamental challenge of developing\npractical AI travel assistants through a novel large-scale dataset of 220k\nquestion-answer pairs. This comprehensive dataset uniquely combines 130k text\nQA pairs meticulously curated from authentic travel forums with GPT-enhanced\nresponses, alongside 90k vision-language QA pairs specifically focused on map\nunderstanding and scene comprehension. Through extensive fine-tuning\nexperiments on state-of-the-art vision-language models (LLaVA, Qwen-VL,\nShikra), we demonstrate significant performance improvements ranging from\n6.5\\%-9.4\\% in both pure text travel understanding and visual question\nanswering tasks. Our model exhibits exceptional capabilities in providing\ncontextual travel recommendations, interpreting map locations, and\nunderstanding place-specific imagery while offering practical information such\nas operating hours and visitor reviews. Comparative evaluations show TraveLLaMA\nsignificantly outperforms general-purpose models in travel-specific tasks,\nestablishing a new benchmark for multi-modal travel assistance systems.", "AI": {"tldr": "TraveLLaMA\u662f\u4e00\u4e2a\u4e13\u4e3a\u57ce\u5e02\u573a\u666f\u7406\u89e3\u548c\u65c5\u884c\u8f85\u52a9\u8bbe\u8ba1\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5fae\u8c03\u5b9e\u9a8c\u663e\u8457\u63d0\u5347\u4e86\u65c5\u884c\u76f8\u5173\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u57ce\u5e02\u73af\u5883\u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u65c5\u884c\u89c4\u5212\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528220k\u95ee\u7b54\u5bf9\u6570\u636e\u96c6\uff08130k\u6587\u672cQA\u548c90k\u89c6\u89c9QA\uff09\uff0c\u5bf9\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u6027\u80fd\u63d0\u53476.5%-9.4%\uff0c\u5728\u65c5\u884c\u63a8\u8350\u3001\u5730\u56fe\u7406\u89e3\u548c\u573a\u666f\u89e3\u8bfb\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TraveLLaMA\u5728\u65c5\u884c\u7279\u5b9a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u4e3a\u591a\u6a21\u6001\u65c5\u884c\u8f85\u52a9\u7cfb\u7edf\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2504.16130", "pdf": "https://arxiv.org/pdf/2504.16130", "abs": "https://arxiv.org/abs/2504.16130", "authors": ["Pengju Ren", "Ri-gui Zhou", "Yaochong Li"], "title": "A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "15 pages, 10 figures", "summary": "Raman spectroscopy serves as a powerful and reliable tool for analyzing the\nchemical information of substances. The integration of Raman spectroscopy with\ndeep learning methods enables rapid qualitative and quantitative analysis of\nmaterials. Most existing approaches adopt supervised learning methods. Although\nsupervised learning has achieved satisfactory accuracy in spectral analysis, it\nis still constrained by costly and limited well-annotated spectral datasets for\ntraining. When spectral annotation is challenging or the amount of annotated\ndata is insufficient, the performance of supervised learning in spectral\nmaterial identification declines. In order to address the challenge of feature\nextraction from unannotated spectra, we propose a self-supervised learning\nparadigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE.\nSMAE does not require any spectral annotations during pre-training. By randomly\nmasking and then reconstructing the spectral information, the model learns\nessential spectral features. The reconstructed spectra exhibit certain\ndenoising properties, improving the signal-to-noise ratio (SNR) by more than\ntwofold. Utilizing the network weights obtained from masked pre-training, SMAE\nachieves clustering accuracy of over 80% for 30 classes of isolated bacteria in\na pathogenic bacterial dataset, demonstrating significant improvements compared\nto classical unsupervised methods and other state-of-the-art deep clustering\nmethods. After fine-tuning the network with a limited amount of annotated data,\nSMAE achieves an identification accuracy of 83.90% on the test set, presenting\ncompetitive performance against the supervised ResNet (83.40%).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08SMAE\uff09\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff0c\u7528\u4e8e\u62c9\u66fc\u5149\u8c31\u5206\u6790\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b66\u4e60\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5fae\u8c03\u540e\u8fbe\u5230\u4e0e\u76d1\u7763\u5b66\u4e60\u76f8\u5f53\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u53d7\u9650\uff1b\u81ea\u76d1\u7763\u5b66\u4e60\u53ef\u89e3\u51b3\u672a\u6807\u6ce8\u5149\u8c31\u7684\u7279\u5f81\u63d0\u53d6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08SMAE\uff09\uff0c\u901a\u8fc7\u968f\u673a\u63a9\u7801\u548c\u91cd\u5efa\u5149\u8c31\u4fe1\u606f\u5b66\u4e60\u7279\u5f81\uff0c\u5177\u6709\u53bb\u566a\u80fd\u529b\u3002", "result": "\u9884\u8bad\u7ec3\u540e\u805a\u7c7b\u51c6\u786e\u7387\u8d8580%\uff0c\u5fae\u8c03\u540e\u8bc6\u522b\u51c6\u786e\u7387\u8fbe83.90%\uff0c\u4f18\u4e8e\u7ecf\u5178\u65e0\u76d1\u7763\u65b9\u6cd5\u548c\u76d1\u7763ResNet\u3002", "conclusion": "SMAE\u4e3a\u62c9\u66fc\u5149\u8c31\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2504.16515", "pdf": "https://arxiv.org/pdf/2504.16515", "abs": "https://arxiv.org/abs/2504.16515", "authors": ["Abdul Hannaan", "Zubair Shah", "Aiman Erbad", "Amr Mohamed", "Ali Safa"], "title": "Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity", "categories": ["cs.CV", "cs.AI"], "comment": "accepted for publication at IEEE IWCMC 2025", "summary": "This paper introduces a novel federated learning framework termed LoRa-FL\ndesigned for training low-rank one-shot image detection models deployed on edge\ndevices. By incorporating low-rank adaptation techniques into one-shot\ndetection architectures, our method significantly reduces both computational\nand communication overhead while maintaining scalable accuracy. The proposed\nframework leverages federated learning to collaboratively train lightweight\nimage recognition models, enabling rapid adaptation and efficient deployment\nacross heterogeneous, resource-constrained devices. Experimental evaluations on\nthe MNIST and CIFAR10 benchmark datasets, both in an\nindependent-and-identically-distributed (IID) and non-IID setting, demonstrate\nthat our approach achieves competitive detection performance while\nsignificantly reducing communication bandwidth and compute complexity. This\nmakes it a promising solution for adaptively reducing the communication and\ncompute power overheads, while not sacrificing model accuracy.", "AI": {"tldr": "LoRa-FL\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8bad\u7ec3\u4f4e\u79e9\u5355\u6b21\u56fe\u50cf\u68c0\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6269\u5c55\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u6280\u672f\u548c\u8054\u90a6\u5b66\u4e60\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u56fe\u50cf\u8bc6\u522b\u6a21\u578b\u7684\u534f\u4f5c\u8bad\u7ec3\u548c\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u5c06\u4f4e\u79e9\u9002\u5e94\u6280\u672f\u878d\u5165\u5355\u6b21\u68c0\u6d4b\u67b6\u6784\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5e76\u5728MNIST\u548cCIFAR10\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728IID\u548c\u975eIID\u8bbe\u7f6e\u4e0b\uff0cLoRa-FL\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5e26\u5bbd\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "LoRa-FL\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u51cf\u5c11\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6a21\u578b\u51c6\u786e\u6027\u3002"}}
{"id": "2504.16131", "pdf": "https://arxiv.org/pdf/2504.16131", "abs": "https://arxiv.org/abs/2504.16131", "authors": ["Samuel Yen-Chi Chen", "Zhiding Liang"], "title": "Introduction to Quantum Machine Learning and Quantum Architecture Search", "categories": ["quant-ph", "cs.AI", "cs.ET", "cs.LG", "cs.NE"], "comment": "ISCAS 2025 Tutorial", "summary": "Recent advancements in quantum computing (QC) and machine learning (ML) have\nfueled significant research efforts aimed at integrating these two\ntransformative technologies. Quantum machine learning (QML), an emerging\ninterdisciplinary field, leverages quantum principles to enhance the\nperformance of ML algorithms. Concurrently, the exploration of systematic and\nautomated approaches for designing high-performance quantum circuit\narchitectures for QML tasks has gained prominence, as these methods empower\nresearchers outside the quantum computing domain to effectively utilize\nquantum-enhanced tools. This tutorial will provide an in-depth overview of\nrecent breakthroughs in both areas, highlighting their potential to expand the\napplication landscape of QML across diverse fields.", "AI": {"tldr": "\u91cf\u5b50\u8ba1\u7b97\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u7ed3\u5408\u63a8\u52a8\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u7684\u53d1\u5c55\uff0c\u65e8\u5728\u901a\u8fc7\u91cf\u5b50\u539f\u7406\u63d0\u5347ML\u7b97\u6cd5\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u81ea\u52a8\u5316\u8bbe\u8ba1\u9ad8\u6027\u80fd\u91cf\u5b50\u7535\u8def\u67b6\u6784\u7684\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u4e24\u8005\u7684\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u5e76\u6269\u5927\u5e94\u7528\u8303\u56f4\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5316\u548c\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u8bbe\u8ba1\u9ad8\u6027\u80fd\u91cf\u5b50\u7535\u8def\u67b6\u6784\uff0c\u4f7f\u975e\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u7684\u7814\u7a76\u8005\u4e5f\u80fd\u6709\u6548\u5229\u7528\u91cf\u5b50\u589e\u5f3a\u5de5\u5177\u3002", "result": "QML\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u6269\u5c55\u5e94\u7528\u524d\u666f\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u6559\u7a0b\u5c06\u6df1\u5165\u63a2\u8ba8QML\u7684\u6700\u65b0\u7a81\u7834\u53ca\u5176\u8de8\u9886\u57df\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.16516", "pdf": "https://arxiv.org/pdf/2504.16516", "abs": "https://arxiv.org/abs/2504.16516", "authors": ["Junrong Yue", "Yifan Zhang", "Chuan Qin", "Bo Li", "Xiaomin Lie", "Xinlei Yu", "Wenxin Zhang", "Zhendong Zhao"], "title": "Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures, Submitted to ACM MM 2025", "summary": "Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow\nnatural language instructions and reach target locations in real-world\nenvironments. While prior methods often rely on either global scene\nrepresentations or object-level features, these approaches are insufficient for\ncapturing the complex interactions across modalities required for accurate\nnavigation. In this paper, we propose a Multi-level Fusion and Reasoning\nArchitecture (MFRA) to enhance the agent's ability to reason over visual\nobservations, language instructions and navigation history. Specifically, MFRA\nintroduces a hierarchical fusion mechanism that aggregates multi-level\nfeatures-ranging from low-level visual cues to high-level semantic\nconcepts-across multiple modalities. We further design a reasoning module that\nleverages fused representations to infer navigation actions through\ninstruction-guided attention and dynamic context integration. By selectively\ncapturing and combining relevant visual, linguistic, and temporal signals, MFRA\nimproves decision-making accuracy in complex navigation scenarios. Extensive\nexperiments on benchmark VLN datasets including REVERIE, R2R, and SOON\ndemonstrate that MFRA achieves superior performance compared to\nstate-of-the-art methods, validating the effectiveness of multi-level modal\nfusion for embodied navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u878d\u5408\u4e0e\u63a8\u7406\u67b6\u6784\uff08MFRA\uff09\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u7279\u5f81\uff08\u4ece\u4f4e\u7ea7\u89c6\u89c9\u7ebf\u7d22\u5230\u9ad8\u7ea7\u8bed\u4e49\u6982\u5ff5\uff09\u548c\u63a8\u7406\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4efb\u52a1\u4e2d\u4ee3\u7406\u7684\u5bfc\u822a\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5168\u5c40\u573a\u666f\u8868\u793a\u6216\u5bf9\u8c61\u7ea7\u7279\u5f81\uff0c\u96be\u4ee5\u6355\u6349\u8de8\u6a21\u6001\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u5bfc\u822a\u51c6\u786e\u6027\u3002", "method": "MFRA\u91c7\u7528\u5206\u5c42\u878d\u5408\u673a\u5236\u6574\u5408\u591a\u7ea7\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u63a8\u7406\u6a21\u5757\u5229\u7528\u6307\u4ee4\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u96c6\u6210\u63a8\u65ad\u5bfc\u822a\u52a8\u4f5c\u3002", "result": "\u5728REVERIE\u3001R2R\u548cSOON\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMFRA\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u7ea7\u6a21\u6001\u878d\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "MFRA\u901a\u8fc7\u591a\u7ea7\u6a21\u6001\u878d\u5408\u548c\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86VLN\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u5bfc\u822a\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16132", "pdf": "https://arxiv.org/pdf/2504.16132", "abs": "https://arxiv.org/abs/2504.16132", "authors": ["Andrew M. Olney", "Sidney K. D'Mello", "Natalie Person", "Whitney Cade", "Patrick Hays", "Claire W. Dempsey", "Blair Lehman", "Betsy Williams", "Art Graesser"], "title": "Efficacy of a Computer Tutor that Models Expert Human Tutors", "categories": ["cs.CY", "cs.AI", "I.2.4; I.2.7; K.3.1"], "comment": "Shortened version of this paper has been accepted to AIED 2025", "summary": "Tutoring is highly effective for promoting learning. However, the\ncontribution of expertise to tutoring effectiveness is unclear and continues to\nbe debated. We conducted a 9-week learning efficacy study of an intelligent\ntutoring system (ITS) for biology modeled on expert human tutors with two\ncontrol conditions: human tutors who were experts in the domain but not in\ntutoring and a no-tutoring condition. All conditions were supplemental to\nclassroom instruction, and students took learning tests immediately before and\nafter tutoring sessions as well as delayed tests 1-2 weeks later. Analysis\nusing logistic mixed-effects modeling indicates significant positive effects on\nthe immediate post-test for the ITS (d =.71) and human tutors (d =.66) which\nare in the 99th percentile of meta-analytic effects, as well as significant\npositive effects on the delayed post-test for the ITS (d =.36) and human tutors\n(d =.39). We discuss implications for the role of expertise in tutoring and the\ndesign of future studies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\uff08ITS\uff09\u548c\u4e13\u5bb6\u4eba\u7c7b\u8f85\u5bfc\u5728\u5373\u65f6\u548c\u5ef6\u8fdf\u6d4b\u8bd5\u4e2d\u5747\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff0c\u4f46\u4e13\u5bb6\u8f85\u5bfc\u7684\u4f5c\u7528\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002", "motivation": "\u63a2\u8ba8\u4e13\u5bb6\u8f85\u5bfc\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u8d21\u732e\uff0c\u4ee5\u53ca\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u8f85\u5bfc\u7684\u6bd4\u8f83\u3002", "method": "\u8fdb\u884c\u4e3a\u671f9\u5468\u7684\u5b66\u4e60\u6548\u679c\u7814\u7a76\uff0c\u6bd4\u8f83\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u3001\u975e\u8f85\u5bfc\u4e13\u5bb6\u4eba\u7c7b\u8f85\u5bfc\u548c\u65e0\u8f85\u5bfc\u6761\u4ef6\u7684\u6548\u679c\u3002", "result": "\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u548c\u4eba\u7c7b\u8f85\u5bfc\u5728\u5373\u65f6\u548c\u5ef6\u8fdf\u6d4b\u8bd5\u4e2d\u5747\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff08\u6548\u5e94\u91cf\u5206\u522b\u4e3a0.71/0.66\u548c0.36/0.39\uff09\u3002", "conclusion": "\u4e13\u5bb6\u8f85\u5bfc\u548c\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u5747\u6709\u6548\uff0c\u4f46\u4e13\u5bb6\u8f85\u5bfc\u7684\u4f5c\u7528\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.16520", "pdf": "https://arxiv.org/pdf/2504.16520", "abs": "https://arxiv.org/abs/2504.16520", "authors": ["Wenwei Li", "Liyi Cai", "Wu Chen", "Anan Li"], "title": "A Few-Shot Metric Learning Method with Dual-Channel Attention for Cross-Modal Same-Neuron Identification", "categories": ["cs.CV", "q-bio.NC"], "comment": "23 pages, 9 figures, submitted to arXiv for public access", "summary": "In neuroscience research, achieving single-neuron matching across different\nimaging modalities is critical for understanding the relationship between\nneuronal structure and function. However, modality gaps and limited annotations\npresent significant challenges. We propose a few-shot metric learning method\nwith a dual-channel attention mechanism and a pretrained vision transformer to\nenable robust cross-modal neuron identification. The local and global channels\nextract soma morphology and fiber context, respectively, and a gating mechanism\nfuses their outputs. To enhance the model's fine-grained discrimination\ncapability, we introduce a hard sample mining strategy based on the\nMultiSimilarityMiner algorithm, along with the Circle Loss function.\nExperiments on two-photon and fMOST datasets demonstrate superior Top-K\naccuracy and recall compared to existing methods. Ablation studies and t-SNE\nvisualizations validate the effectiveness of each module. The method also\nachieves a favorable trade-off between accuracy and training efficiency under\ndifferent fine-tuning strategies. These results suggest that the proposed\napproach offers a promising technical solution for accurate single-cell level\nmatching and multimodal neuroimaging integration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u89c6\u89c9\u53d8\u6362\u5668\u7684\u5c11\u6837\u672c\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8de8\u6a21\u6001\u795e\u7ecf\u5143\u8bc6\u522b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u4e2d\uff0c\u8de8\u6a21\u6001\u5355\u795e\u7ecf\u5143\u5339\u914d\u5bf9\u7406\u89e3\u795e\u7ecf\u5143\u7ed3\u6784\u4e0e\u529f\u80fd\u5173\u7cfb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6a21\u6001\u5dee\u5f02\u548c\u6709\u9650\u6807\u6ce8\u5e26\u6765\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff08\u5c40\u90e8\u548c\u5168\u5c40\u901a\u9053\uff09\u63d0\u53d6\u795e\u7ecf\u5143\u5f62\u6001\u548c\u7ea4\u7ef4\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u95e8\u63a7\u673a\u5236\u878d\u5408\u8f93\u51fa\uff1b\u5f15\u5165\u57fa\u4e8eMultiSimilarityMiner\u7684\u96be\u6837\u672c\u6316\u6398\u7b56\u7565\u548cCircle Loss\u51fd\u6570\u3002", "result": "\u5728\u53cc\u5149\u5b50\u548cfMOST\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684Top-K\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u6d88\u878d\u5b9e\u9a8c\u548ct-SNE\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5355\u7ec6\u80de\u6c34\u5e73\u5339\u914d\u548c\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u6574\u5408\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2504.16133", "pdf": "https://arxiv.org/pdf/2504.16133", "abs": "https://arxiv.org/abs/2504.16133", "authors": ["Milad Leyli-abadi", "Ricardo J. Bessa", "Jan Viebahn", "Daniel Boos", "Clark Borst", "Alberto Castagna", "Ricardo Chavarriaga", "Mohamed Hassouna", "Bruno Lemetayer", "Giulia Leto", "Antoine Marot", "Maroua Meddeb", "Manuel Meyer", "Viola Schiaffonati", "Manuel Schneider", "Toni Waefler"], "title": "A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The interaction between humans and AI in safety-critical systems presents a\nunique set of challenges that remain partially addressed by existing\nframeworks. These challenges stem from the complex interplay of requirements\nfor transparency, trust, and explainability, coupled with the necessity for\nrobust and safe decision-making. A framework that holistically integrates human\nand AI capabilities while addressing these concerns is notably required,\nbridging the critical gaps in designing, deploying, and maintaining safe and\neffective systems. This paper proposes a holistic conceptual framework for\ncritical infrastructures by adopting an interdisciplinary approach. It\nintegrates traditionally distinct fields such as mathematics, decision theory,\ncomputer science, philosophy, psychology, and cognitive engineering and draws\non specialized engineering domains, particularly energy, mobility, and\naeronautics. The flexibility in its adoption is also demonstrated through its\ninstantiation on an already existing framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u6027\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u7c7b\u4e0eAI\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u4ea4\u4e92\u6311\u6218\uff0c\u6574\u5408\u4e86\u591a\u5b66\u79d1\u77e5\u8bc6\u5e76\u5c55\u793a\u4e86\u5176\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u672a\u80fd\u5b8c\u5168\u89e3\u51b3\u4eba\u7c7b\u4e0eAI\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u4ea4\u4e92\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u900f\u660e\u5ea6\u3001\u4fe1\u4efb\u3001\u89e3\u91ca\u6027\u53ca\u5b89\u5168\u51b3\u7b56\u65b9\u9762\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u8de8\u5b66\u79d1\u65b9\u6cd5\uff0c\u6574\u5408\u6570\u5b66\u3001\u51b3\u7b56\u7406\u8bba\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u54f2\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u8ba4\u77e5\u5de5\u7a0b\u7b49\u9886\u57df\u77e5\u8bc6\uff0c\u5e76\u7ed3\u5408\u80fd\u6e90\u3001\u4ea4\u901a\u548c\u822a\u7a7a\u7b49\u4e13\u4e1a\u5de5\u7a0b\u9886\u57df\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u7075\u6d3b\u5e94\u7528\u4e8e\u73b0\u6709\u6846\u67b6\u7684\u7efc\u5408\u6027\u6982\u5ff5\u6846\u67b6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bbe\u8ba1\u548c\u7ef4\u62a4\u5b89\u5168\u6709\u6548\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.16538", "pdf": "https://arxiv.org/pdf/2504.16538", "abs": "https://arxiv.org/abs/2504.16538", "authors": ["Joan Perez", "Giovanni Fusco"], "title": "Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes", "categories": ["cs.CV", "cs.LG", "I.2; I.4; J.4"], "comment": "25 pages, 6 figures in main paper, 6 figures in appendices", "summary": "Streetscapes are an essential component of urban space. Their assessment is\npresently either limited to morphometric properties of their mass skeleton or\nrequires labor-intensive qualitative evaluations of visually perceived\nqualities. This paper introduces SAGAI: Streetscape Analysis with Generative\nArtificial Intelligence, a modular workflow for scoring street-level urban\nscenes using open-access data and vision-language models. SAGAI integrates\nOpenStreetMap geometries, Google Street View imagery, and a lightweight version\nof the LLaVA model to generate structured spatial indicators from images via\ncustomizable natural language prompts. The pipeline includes an automated\nmapping module that aggregates visual scores at both the point and street\nlevels, enabling direct cartographic interpretation. It operates without\ntask-specific training or proprietary software dependencies, supporting\nscalable and interpretable analysis of urban environments. Two exploratory case\nstudies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial\noutputs from vision-language inference. The initial results show strong\nperformance for binary urban-rural scene classification, moderate precision in\ncommercial feature detection, and lower estimates, but still informative, of\nsidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a\nwide range of urban research themes, such as walkability, safety, or urban\ndesign, through prompt modification alone.", "AI": {"tldr": "SAGAI\u662f\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u8bc4\u4f30\u8857\u9053\u666f\u89c2\uff0c\u7ed3\u5408\u5f00\u653e\u6570\u636e\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u57ce\u5e02\u573a\u666f\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u8857\u9053\u666f\u89c2\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u5f62\u6001\u6d4b\u91cf\u6216\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5b9a\u6027\u5206\u6790\uff0cSAGAI\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "SAGAI\u6574\u5408OpenStreetMap\u3001Google\u8857\u666f\u548c\u8f7b\u91cf\u7ea7LLaVA\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u7ed3\u6784\u5316\u7a7a\u95f4\u6307\u6807\uff0c\u5e76\u652f\u6301\u81ea\u52a8\u5730\u56fe\u7ed8\u5236\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cSAGAI\u5728\u57ce\u4e61\u5206\u7c7b\u548c\u5546\u4e1a\u7279\u5f81\u68c0\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4eba\u884c\u9053\u5bbd\u5ea6\u4f30\u8ba1\u4e0a\u7cbe\u5ea6\u8f83\u4f4e\u3002", "conclusion": "SAGAI\u65e0\u9700\u7279\u5b9a\u8bad\u7ec3\u6216\u4e13\u6709\u8f6f\u4ef6\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u63d0\u793a\u9002\u5e94\u591a\u79cd\u57ce\u5e02\u7814\u7a76\u4e3b\u9898\uff0c\u5982\u6b65\u884c\u53cb\u597d\u6027\u6216\u5b89\u5168\u6027\u3002"}}
{"id": "2504.16138", "pdf": "https://arxiv.org/pdf/2504.16138", "abs": "https://arxiv.org/abs/2504.16138", "authors": ["Iyngkarran Kumar", "Sam Manning"], "title": "Trends in Frontier AI Model Count: A Forecast to 2028", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Governments are starting to impose requirements on AI models based on how\nmuch compute was used to train them. For example, the EU AI Act imposes\nrequirements on providers of general-purpose AI with systemic risk, which\nincludes systems trained using greater than $10^{25}$ floating point operations\n(FLOP). In the United States' AI Diffusion Framework, a training compute\nthreshold of $10^{26}$ FLOP is used to identify \"controlled models\" which face\na number of requirements. We explore how many models such training compute\nthresholds will capture over time. We estimate that by the end of 2028, there\nwill be between 103-306 foundation models exceeding the $10^{25}$ FLOP\nthreshold put forward in the EU AI Act (90% CI), and 45-148 models exceeding\nthe $10^{26}$ FLOP threshold that defines controlled models in the AI Diffusion\nFramework (90% CI). We also find that the number of models exceeding these\nabsolute compute thresholds each year will increase superlinearly -- that is,\neach successive year will see more new models captured within the threshold\nthan the year before. Thresholds that are defined with respect to the largest\ntraining run to date (for example, such that all models within one order of\nmagnitude of the largest training run to date are captured by the threshold)\nsee a more stable trend, with a median forecast of 14-16 models being captured\nby this definition annually from 2025-2028.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u8bad\u7ec3\u8ba1\u7b97\u91cf\u7684AI\u6a21\u578b\u76d1\u7ba1\u95e8\u69db\uff0c\u9884\u6d4b\u672a\u6765\u51e0\u5e74\u8d85\u8fc7\u8fd9\u4e9b\u95e8\u69db\u7684\u6a21\u578b\u6570\u91cf\u5c06\u8d85\u7ebf\u6027\u589e\u957f\u3002", "motivation": "\u7814\u7a76\u653f\u5e9c\u57fa\u4e8e\u8bad\u7ec3\u8ba1\u7b97\u91cf\u5bf9AI\u6a21\u578b\u65bd\u52a0\u76d1\u7ba1\u8981\u6c42\u7684\u8d8b\u52bf\uff0c\u5e76\u91cf\u5316\u8fd9\u4e9b\u95e8\u69db\u5c06\u8986\u76d6\u7684\u6a21\u578b\u6570\u91cf\u3002", "method": "\u901a\u8fc7\u7edf\u8ba1\u548c\u9884\u6d4b\u6a21\u578b\uff0c\u4f30\u8ba1\u672a\u6765\u51e0\u5e74\u8d85\u8fc7\u6b27\u76df\u548c\u7f8e\u56fd\u8bbe\u5b9a\u7684\u8ba1\u7b97\u91cf\u95e8\u69db\u7684AI\u6a21\u578b\u6570\u91cf\u3002", "result": "\u9884\u6d4b\u52302028\u5e74\uff0c\u5c06\u6709103-306\u4e2a\u6a21\u578b\u8d85\u8fc7\u6b27\u76df\u768410^25 FLOP\u95e8\u69db\uff0c45-148\u4e2a\u6a21\u578b\u8d85\u8fc7\u7f8e\u56fd\u768410^26 FLOP\u95e8\u69db\u3002", "conclusion": "\u76d1\u7ba1\u95e8\u69db\u7684\u8bbe\u5b9a\u65b9\u5f0f\u4f1a\u5f71\u54cd\u8986\u76d6\u7684\u6a21\u578b\u6570\u91cf\uff0c\u7edd\u5bf9\u95e8\u69db\u4f1a\u5bfc\u81f4\u8d85\u7ebf\u6027\u589e\u957f\uff0c\u800c\u76f8\u5bf9\u95e8\u69db\u5219\u66f4\u7a33\u5b9a\u3002"}}
{"id": "2504.16545", "pdf": "https://arxiv.org/pdf/2504.16545", "abs": "https://arxiv.org/abs/2504.16545", "authors": ["Andrea Conti", "Matteo Poggi", "Valerio Cambareri", "Martin R. Oswald", "Stefano Mattoccia"], "title": "ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration", "categories": ["cs.CV"], "comment": null, "summary": "Time-of-Flight (ToF) sensors provide efficient active depth sensing at\nrelatively low power budgets; among such designs, only very sparse measurements\nfrom low-resolution sensors are considered to meet the increasingly limited\npower constraints of mobile and AR/VR devices. However, such extreme sparsity\nlevels limit the seamless usage of ToF depth in SLAM. In this work, we propose\nToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for\nusing effectively very sparse ToF input data. Our approach improves upon the\nstate of the art by introducing a multi-frame integration module, which\nproduces dense depth maps by merging cues from extremely sparse ToF depth,\nmonocular color, and multi-view geometry. Extensive experiments on both\nsynthetic and real sparse ToF datasets demonstrate the viability of our\napproach, as it achieves state-of-the-art tracking and mapping performances on\nreference datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u5206\u5e03\u7684SLAM\u65b9\u6cd5ToF-Splatting\uff0c\u7528\u4e8e\u5904\u7406\u6781\u7a00\u758f\u7684ToF\u6df1\u5ea6\u6570\u636e\uff0c\u901a\u8fc7\u591a\u5e27\u6574\u5408\u6a21\u5757\u751f\u6210\u5bc6\u96c6\u6df1\u5ea6\u56fe\u3002", "motivation": "\u89e3\u51b3\u6781\u7a00\u758fToF\u6df1\u5ea6\u6570\u636e\u5728SLAM\u4e2d\u5e94\u7528\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6781\u7a00\u758fToF\u6df1\u5ea6\u3001\u5355\u76ee\u5f69\u8272\u548c\u591a\u89c6\u89d2\u51e0\u4f55\u4fe1\u606f\uff0c\u901a\u8fc7\u591a\u5e27\u6574\u5408\u6a21\u5757\u751f\u6210\u5bc6\u96c6\u6df1\u5ea6\u56fe\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u7a00\u758fToF\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u548c\u5efa\u56fe\u6027\u80fd\u3002", "conclusion": "ToF-Splatting\u4e3a\u6781\u7a00\u758fToF\u6570\u636e\u7684SLAM\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.16139", "pdf": "https://arxiv.org/pdf/2504.16139", "abs": "https://arxiv.org/abs/2504.16139", "authors": ["Sridharan Sankaran"], "title": "Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) reshapes industries and societies, ensuring\nits trustworthiness-through mitigating ethical risks like bias, opacity, and\naccountability deficits-remains a global challenge. International Organization\nfor Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to\nfoster responsible development by embedding fairness, transparency, and risk\nmanagement into AI systems. However, their effectiveness varies across diverse\nregulatory landscapes, from the EU's risk-based AI Act to China's\nstability-focused measures and the U.S.'s fragmented state-led initiatives.\nThis paper introduces a novel Comparative Risk-Impact Assessment Framework to\nevaluate how well ISO standards address ethical risks within these contexts,\nproposing enhancements to strengthen their global applicability. By mapping ISO\nstandards to the EU AI Act and surveying regulatory frameworks in ten\nregions-including the UK, Canada, India, Japan, Singapore, South Korea, and\nBrazil-we establish a baseline for ethical alignment. The framework, applied to\ncase studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO\nstandards falter in enforcement (e.g., Colorado) and undervalue region-specific\nrisks like privacy (China). We recommend mandatory risk audits, region-specific\nannexes, and a privacy-focused module to enhance ISO's adaptability. This\napproach not only synthesizes global trends but also offers a replicable tool\nfor aligning standardization with ethical imperatives, fostering\ninteroperability and trust in AI worldwide. Policymakers and standards bodies\ncan leverage these insights to evolve AI governance, ensuring it meets diverse\nsocietal needs as the technology advances.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6bd4\u8f83\u98ce\u9669\u5f71\u54cd\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30ISO AI\u6807\u51c6\u5728\u4e0d\u540c\u76d1\u7ba1\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5efa\u8bae\u6539\u8fdb\u4ee5\u589e\u5f3a\u5176\u5168\u7403\u9002\u7528\u6027\u3002", "motivation": "\u968f\u7740AI\u91cd\u5851\u884c\u4e1a\u548c\u793e\u4f1a\uff0c\u786e\u4fdd\u5176\u53ef\u4fe1\u5ea6\uff08\u5982\u51cf\u5c11\u504f\u89c1\u3001\u4e0d\u900f\u660e\u548c\u8d23\u4efb\u7f3a\u5931\u7b49\u4f26\u7406\u98ce\u9669\uff09\u662f\u5168\u7403\u6027\u6311\u6218\u3002ISO AI\u6807\u51c6\u65e8\u5728\u4fc3\u8fdb\u8d1f\u8d23\u4efb\u7684\u53d1\u5c55\uff0c\u4f46\u5176\u6548\u679c\u56e0\u76d1\u7ba1\u73af\u5883\u4e0d\u540c\u800c\u5f02\u3002", "method": "\u5f15\u5165\u6bd4\u8f83\u98ce\u9669\u5f71\u54cd\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06ISO\u6807\u51c6\u4e0e\u6b27\u76dfAI\u6cd5\u6848\u7b49\u76d1\u7ba1\u6846\u67b6\u8fdb\u884c\u6620\u5c04\uff0c\u5e76\u5728\u6b27\u76df\u3001\u7f8e\u56fd\u79d1\u7f57\u62c9\u591a\u5dde\u548c\u4e2d\u56fd\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u81ea\u613f\u6027ISO\u6807\u51c6\u5728\u6267\u6cd5\uff08\u5982\u79d1\u7f57\u62c9\u591a\u5dde\uff09\u548c\u5730\u533a\u7279\u5b9a\u98ce\u9669\uff08\u5982\u4e2d\u56fd\u7684\u9690\u79c1\u95ee\u9898\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u5efa\u8bae\u5f3a\u5236\u98ce\u9669\u5ba1\u8ba1\u3001\u5730\u533a\u7279\u5b9a\u9644\u5f55\u548c\u9690\u79c1\u6a21\u5757\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6807\u51c6\u5316\u4e0e\u4f26\u7406\u8981\u6c42\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u5168\u7403AI\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u4fe1\u4efb\u3002\u653f\u7b56\u5236\u5b9a\u8005\u548c\u6807\u51c6\u673a\u6784\u53ef\u5229\u7528\u8fd9\u4e9b\u89c1\u89e3\u6539\u8fdbAI\u6cbb\u7406\u3002"}}
{"id": "2504.16557", "pdf": "https://arxiv.org/pdf/2504.16557", "abs": "https://arxiv.org/abs/2504.16557", "authors": ["Murat Bilgehan Ertan", "Ronak Sahu", "Phuong Ha Nguyen", "Kaleel Mahmood", "Marten van Dijk"], "title": "Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D Vision Tasks", "categories": ["cs.CV"], "comment": "Submitted to ICCV 2025", "summary": "We introduce ROAR (Robust Object Removal and Re-annotation), a scalable\nframework for privacy-preserving dataset obfuscation that eliminates sensitive\nobjects instead of modifying them. Our method integrates instance segmentation\nwith generative inpainting to remove identifiable entities while preserving\nscene integrity. Extensive evaluations on 2D COCO-based object detection show\nthat ROAR achieves 87.5% of the baseline detection average precision (AP),\nwhereas image dropping achieves only 74.2% of the baseline AP, highlighting the\nadvantage of scrubbing in preserving dataset utility. The degradation is even\nmore severe for small objects due to occlusion and loss of fine-grained\ndetails. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR\nloss of at most 1.66 dB while maintaining SSIM and improving LPIPS,\ndemonstrating superior perceptual quality. Our findings establish object\nremoval as an effective privacy framework, achieving strong privacy guarantees\nwith minimal performance trade-offs. The results highlight key challenges in\ngenerative inpainting, occlusion-robust segmentation, and task-specific\nscrubbing, setting the foundation for future advancements in privacy-preserving\nvision systems.", "AI": {"tldr": "ROAR\u662f\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u6a21\u7cca\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u79fb\u9664\u800c\u975e\u4fee\u6539\u654f\u611f\u5bf9\u8c61\uff0c\u7ed3\u5408\u5b9e\u4f8b\u5206\u5272\u548c\u751f\u6210\u4fee\u590d\u6280\u672f\uff0c\u5728\u4fdd\u6301\u573a\u666f\u5b8c\u6574\u6027\u7684\u540c\u65f6\u6d88\u9664\u53ef\u8bc6\u522b\u5b9e\u4f53\u3002", "motivation": "\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u6a21\u7cca\u5316\u4e2d\u56e0\u4fee\u6539\u654f\u611f\u5bf9\u8c61\u5bfc\u81f4\u7684\u573a\u666f\u5b8c\u6574\u6027\u548c\u6570\u636e\u96c6\u6548\u7528\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u96c6\u6210\u5b9e\u4f8b\u5206\u5272\u4e0e\u751f\u6210\u4fee\u590d\u6280\u672f\uff0c\u79fb\u9664\u654f\u611f\u5bf9\u8c61\u5e76\u4fdd\u6301\u573a\u666f\u5b8c\u6574\u6027\u3002", "result": "\u57282D COCO\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u57fa\u7ebf\u68c0\u6d4bAP\u768487.5%\uff0c3D\u91cd\u5efa\u4e2dPSNR\u635f\u5931\u6700\u591a1.66 dB\uff0c\u540c\u65f6\u4fdd\u6301SSIM\u548c\u63d0\u5347LPIPS\u3002", "conclusion": "ROAR\u8bc1\u660e\u4e86\u5bf9\u8c61\u79fb\u9664\u662f\u4e00\u79cd\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u969c\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u89c6\u89c9\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2504.16140", "pdf": "https://arxiv.org/pdf/2504.16140", "abs": "https://arxiv.org/abs/2504.16140", "authors": ["Max Hartman", "Lav Varshney"], "title": "SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful\nframework for learning general-purpose representations. However, these models\noften lack interpretability and suffer from inefficiencies due to dense\nembedding representations. We propose SparseJEPA, an extension that integrates\nsparse representation learning into the JEPA framework to enhance the quality\nof learned representations. SparseJEPA employs a penalty method that encourages\nlatent space variables to be shared among data features with strong semantic\nrelationships, while maintaining predictive performance. We demonstrate the\neffectiveness of SparseJEPA by training on the CIFAR-100 dataset and\npre-training a lightweight Vision Transformer. The improved embeddings are\nutilized in linear-probe transfer learning for both image classification and\nlow-level tasks, showcasing the architecture's versatility across different\ntransfer tasks. Furthermore, we provide a theoretical proof that demonstrates\nthat the grouping mechanism enhances representation quality. This was done by\ndisplaying that grouping reduces Multiinformation among latent-variables,\nincluding proofing the Data Processing Inequality for Multiinformation. Our\nresults indicate that incorporating sparsity not only refines the latent space\nbut also facilitates the learning of more meaningful and interpretable\nrepresentations. In further work, hope to further extend this method by finding\nnew ways to leverage the grouping mechanism through object-centric\nrepresentation learning.", "AI": {"tldr": "SparseJEPA\u901a\u8fc7\u7a00\u758f\u8868\u793a\u5b66\u4e60\u6539\u8fdbJEPA\u6846\u67b6\uff0c\u63d0\u5347\u8868\u793a\u8d28\u91cf\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728CIFAR-100\u548c\u8f7b\u91cf\u7ea7Vision Transformer\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "JEPA\u6846\u67b6\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u56e0\u5bc6\u96c6\u5d4c\u5165\u8868\u793a\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u7a00\u758f\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u60e9\u7f5a\u65b9\u6cd5\u9f13\u52b1\u6f5c\u5728\u7a7a\u95f4\u53d8\u91cf\u5728\u8bed\u4e49\u76f8\u5173\u7279\u5f81\u95f4\u5171\u4eab\u3002", "result": "\u5728CIFAR-100\u548c\u8f7b\u91cf\u7ea7Vision Transformer\u4e0a\u9a8c\u8bc1\uff0c\u7a00\u758f\u6027\u63d0\u5347\u4e86\u8868\u793a\u8d28\u91cf\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u7a00\u758f\u6027\u4e0d\u4ec5\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u8fd8\u4fc3\u8fdb\u6709\u610f\u4e49\u4e14\u53ef\u89e3\u91ca\u7684\u8868\u793a\u5b66\u4e60\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2504.16564", "pdf": "https://arxiv.org/pdf/2504.16564", "abs": "https://arxiv.org/abs/2504.16564", "authors": ["Zhongtao Wang", "Xizhe Cao", "Yisong Chen", "Guoping Wang"], "title": "SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral Adaptive Information Propagation", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Semantic segmentation of remote sensing imagery demands precise spatial\nboundaries and robust intra-class consistency, challenging conventional\nhierarchical models. To address limitations arising from spatial domain feature\nfusion and insufficient receptive fields, this paper introduces SAIP-Net, a\nnovel frequency-aware segmentation framework that leverages Spectral Adaptive\nInformation Propagation. SAIP-Net employs adaptive frequency filtering and\nmulti-scale receptive field enhancement to effectively suppress intra-class\nfeature inconsistencies and sharpen boundary lines. Comprehensive experiments\ndemonstrate significant performance improvements over state-of-the-art methods,\nhighlighting the effectiveness of spectral-adaptive strategies combined with\nexpanded receptive fields for remote sensing image segmentation.", "AI": {"tldr": "SAIP-Net\u901a\u8fc7\u9891\u57df\u81ea\u9002\u5e94\u4fe1\u606f\u4f20\u64ad\uff0c\u89e3\u51b3\u4e86\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u7a7a\u95f4\u8fb9\u754c\u548c\u7c7b\u5185\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5206\u5c42\u6a21\u578b\u5728\u9065\u611f\u56fe\u50cf\u5206\u5272\u4e2d\u96be\u4ee5\u6ee1\u8db3\u7cbe\u786e\u7a7a\u95f4\u8fb9\u754c\u548c\u7c7b\u5185\u4e00\u81f4\u6027\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faSAIP-Net\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u9891\u7387\u6ee4\u6ce2\u548c\u591a\u5c3a\u5ea6\u611f\u53d7\u91ce\u589e\u5f3a\uff0c\u4f18\u5316\u7279\u5f81\u878d\u5408\u548c\u8fb9\u754c\u6e05\u6670\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAIP-Net\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u9891\u57df\u81ea\u9002\u5e94\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9891\u57df\u81ea\u9002\u5e94\u7b56\u7565\u4e0e\u6269\u5c55\u611f\u53d7\u91ce\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2504.16142", "pdf": "https://arxiv.org/pdf/2504.16142", "abs": "https://arxiv.org/abs/2504.16142", "authors": ["Hangxu Liu", "Yaojie Sun", "Yu Wang"], "title": "A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "In recent years, non-intrusive load monitoring (NILM) technology has\nattracted much attention in the related research field by virtue of its unique\nadvantage of utilizing single meter data to achieve accurate decomposition of\ndevice-level energy consumption. Cutting-edge methods based on machine learning\nand deep learning have achieved remarkable results in load decomposition\naccuracy by fusing time-frequency domain features. However, these methods\ngenerally suffer from high computational costs and huge memory requirements,\nwhich become the main obstacles for their deployment on resource-constrained\nmicrocontroller units (MCUs). To address these challenges, this study proposes\nan innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain\nand systematically compares and analyzes the performance of six machine\nlearning techniques in home electricity scenarios. Through complete\nexperimental validation on edge MCUs, this scheme successfully achieves a\nrecognition accuracy of 95%. Meanwhile, this study deeply optimizes the\nfrequency domain feature extraction process, which effectively reduces the\nrunning time by 55.55% and the storage overhead by about 34.6%. The algorithm\nperformance will be further optimized in future research work. Considering that\nthe elimination of voltage transformer design can significantly reduce the\ncost, the subsequent research will focus on this direction, and is committed to\nproviding more cost-effective solutions for the practical application of NILM,\nand providing a solid theoretical foundation and feasible technical paths for\nthe design of efficient NILM systems in edge computing environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u7b97\u6cd5\u7684\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b\uff08NILM\uff09\u6280\u672f\uff0c\u901a\u8fc7\u878d\u5408\u65f6\u9891\u57df\u7279\u5f81\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u5e76\u5728\u8fb9\u7f18MCU\u4e0a\u5b9e\u73b0\u4e8695%\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684NILM\u65b9\u6cd5\u867d\u5728\u8d1f\u8f7d\u5206\u89e3\u7cbe\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u548c\u5b58\u50a8\u9700\u6c42\u9ad8\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18MCU\u4e0a\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65f6\u9891\u57dfDTW\u7b97\u6cd5\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u516d\u79cd\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u5bb6\u7528\u7535\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u4f18\u5316\u4e86\u9891\u57df\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u3002", "result": "\u5728\u8fb9\u7f18MCU\u4e0a\u5b9e\u73b0\u4e8695%\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1155.55%\uff0c\u5b58\u50a8\u5f00\u9500\u964d\u4f4e\u7ea634.6%\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5c06\u805a\u7126\u4e8e\u6d88\u9664\u7535\u538b\u53d8\u538b\u5668\u8bbe\u8ba1\u4ee5\u964d\u4f4e\u6210\u672c\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u9ad8\u6548NILM\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2504.16570", "pdf": "https://arxiv.org/pdf/2504.16570", "abs": "https://arxiv.org/abs/2504.16570", "authors": ["Giacomo Pacini", "Lorenzo Bianchi", "Luca Ciampi", "Nicola Messina", "Giuseppe Amato", "Fabrizio Falchi"], "title": "CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using Unsupervised Backbones", "categories": ["cs.CV"], "comment": "13 pages, 2 figures, 2 tables. Project website:\n  https://lorebianchi98.github.io/CountingDINO/", "summary": "Class-agnostic counting (CAC) aims to estimate the number of objects in\nimages without being restricted to predefined categories. However, while\ncurrent exemplar-based CAC methods offer flexibility at inference time, they\nstill rely heavily on labeled data for training, which limits scalability and\ngeneralization to many downstream use cases. In this paper, we introduce\nCountingDINO, the first training-free exemplar-based CAC framework that\nexploits a fully unsupervised feature extractor. Specifically, our approach\nemploys self-supervised vision-only backbones to extract object-aware features,\nand it eliminates the need for annotated data throughout the entire proposed\npipeline. At inference time, we extract latent object prototypes via ROI-Align\nfrom DINO features and use them as convolutional kernels to generate similarity\nmaps. These are then transformed into density maps through a simple yet\neffective normalization scheme. We evaluate our approach on the FSC-147\nbenchmark, where we outperform a baseline under the same label-free setting.\nOur method also achieves competitive -- and in some cases superior -- results\ncompared to training-free approaches relying on supervised backbones, as well\nas several fully supervised state-of-the-art methods. This demonstrates that\ntraining-free CAC can be both scalable and competitive. Website:\nhttps://lorebianchi98.github.io/CountingDINO/", "AI": {"tldr": "CountingDINO\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7c7b\u522b\u65e0\u5173\u8ba1\u6570\uff08CAC\uff09\u6846\u67b6\uff0c\u5229\u7528\u81ea\u76d1\u7763\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u8ba1\u6570\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u793a\u4f8b\u7684CAC\u65b9\u6cd5\u867d\u7075\u6d3b\uff0c\u4f46\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u89c6\u89c9\u4e3b\u5e72\u63d0\u53d6\u5bf9\u8c61\u611f\u77e5\u7279\u5f81\uff0c\u901a\u8fc7ROI-Align\u63d0\u53d6\u6f5c\u5728\u5bf9\u8c61\u539f\u578b\u4f5c\u4e3a\u5377\u79ef\u6838\u751f\u6210\u76f8\u4f3c\u6027\u56fe\uff0c\u518d\u8f6c\u6362\u4e3a\u5bc6\u5ea6\u56fe\u3002", "result": "\u5728FSC-147\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u65e0\u76d1\u7763\u57fa\u7ebf\uff0c\u4e0e\u4f9d\u8d56\u76d1\u7763\u4e3b\u5e72\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\u53ca\u90e8\u5206\u5168\u76d1\u7763\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u8bc1\u660e\u4e86\u65e0\u9700\u8bad\u7ec3\u7684CAC\u65b9\u6cd5\u65e2\u5177\u6709\u6269\u5c55\u6027\u53c8\u5177\u5907\u7ade\u4e89\u529b\u3002"}}
{"id": "2504.16144", "pdf": "https://arxiv.org/pdf/2504.16144", "abs": "https://arxiv.org/abs/2504.16144", "authors": ["Ahmed El Fekih Zguir", "Ferda Ofli", "Muhammad Imran"], "title": "Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Natural disasters often result in a surge of social media activity, including\nrequests for assistance, offers of help, sentiments, and general updates. To\nenable humanitarian organizations to respond more efficiently, we propose a\nfine-grained hierarchical taxonomy to systematically organize crisis-related\ninformation about requests and offers into three critical dimensions: supplies,\nemergency personnel, and actions. Leveraging the capabilities of Large Language\nModels (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning)\nthat retrieves class-specific labeled examples from an embedding database to\nenhance the model's performance in detecting and classifying posts. Beyond\nclassification, we assess the actionability of messages to prioritize posts\nrequiring immediate attention. Extensive experiments demonstrate that our\napproach outperforms baseline prompting strategies, effectively identifying and\nprioritizing actionable requests and offers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u65b9\u6cd5\uff08QSF Learning\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u7ec4\u7ec7\u548c\u4f18\u5148\u5904\u7406\u707e\u96be\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u4fe1\u606f\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u5bfc\u81f4\u793e\u4ea4\u5a92\u4f53\u4fe1\u606f\u6fc0\u589e\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u5206\u7c7b\u4ee5\u63d0\u5347\u4eba\u9053\u4e3b\u4e49\u7ec4\u7ec7\u54cd\u5e94\u6548\u7387\u3002", "method": "\u91c7\u7528QSF Learning\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u7c7b\u7279\u5b9a\u6807\u6ce8\u6837\u672c\u589e\u5f3aLLM\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u4fe1\u606f\u7684\u53ef\u64cd\u4f5c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u63d0\u793a\u7b56\u7565\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u7d27\u6025\u8bf7\u6c42\u4e0e\u63f4\u52a9\u3002", "conclusion": "QSF Learning\u4e3a\u707e\u96be\u54cd\u5e94\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4fe1\u606f\u5206\u7c7b\u548c\u4f18\u5148\u7ea7\u6392\u5e8f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16591", "pdf": "https://arxiv.org/pdf/2504.16591", "abs": "https://arxiv.org/abs/2504.16591", "authors": ["Tristan Kenneweg", "Philip Kenneweg", "Barbara Hammer"], "title": "JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning", "categories": ["cs.CV"], "comment": "Published at ESANN 2025", "summary": "Joint-Embedding Predictive Architectures (JEPA) have recently become popular\nas promising architectures for self-supervised learning. Vision transformers\nhave been trained using JEPA to produce embeddings from images and videos,\nwhich have been shown to be highly suitable for downstream tasks like\nclassification and segmentation. In this paper, we show how to adapt the JEPA\narchitecture to reinforcement learning from images. We discuss model collapse,\nshow how to prevent it, and provide exemplary data on the classical Cart Pole\ntask.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06JEPA\u67b6\u6784\u5e94\u7528\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728Cart Pole\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u793a\u4f8b\u6570\u636e\u3002", "motivation": "JEPA\u67b6\u6784\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06JEPA\u67b6\u6784\u9002\u914d\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u51fa\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u7684\u65b9\u6cd5\uff0c\u5e76\u5728Cart Pole\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u5c06JEPA\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728\u793a\u4f8b\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002", "conclusion": "JEPA\u67b6\u6784\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u3002"}}
{"id": "2504.16612", "pdf": "https://arxiv.org/pdf/2504.16612", "abs": "https://arxiv.org/abs/2504.16612", "authors": ["Max Kirchner", "Alexander C. Jenke", "Sebastian Bodenstedt", "Fiona R. Kolbinger", "Oliver Saldanha", "Jakob N. Kather", "Martin Wagner", "Stefanie Speidel"], "title": "Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint submitted to MEDIA", "summary": "Purpose: In this study, we investigate the training of foundation models\nusing federated learning to address data-sharing limitations and enable\ncollaborative model training without data transfer for minimally invasive\nsurgery. Methods: Inspired by the EndoViT study, we adapt the Masked\nAutoencoder for federated learning, enhancing it with adaptive Sharpness-Aware\nMinimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is\npretrained on the Endo700k dataset collection and later fine-tuned and\nevaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,\nand Surgical Phase Recognition. Results: Our findings demonstrate that\nintegrating adaptive FedSAM into the federated MAE approach improves\npretraining, leading to a reduction in reconstruction loss per patch. The\napplication of FL-EndoViT in surgical downstream tasks results in performance\ncomparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over\nCEN-EndoViT in surgical scene segmentation when data is limited and in action\ntriplet recognition when large datasets are used. Conclusion: These findings\nhighlight the potential of federated learning for privacy-preserving training\nof surgical foundation models, offering a robust and generalizable solution for\nsurgical data science. Effective collaboration requires adapting federated\nlearning methods, such as the integration of FedSAM, which can accommodate the\ninherent data heterogeneity across institutions. In future, exploring FL in\nvideo-based models may enhance these capabilities by incorporating\nspatiotemporal dynamics crucial for real-world surgical environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u6570\u636e\u5171\u4eab\u9650\u5236\uff0c\u5e76\u5728\u5fae\u521b\u624b\u672f\u4e2d\u5b9e\u73b0\u65e0\u9700\u6570\u636e\u4f20\u8f93\u7684\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u5171\u4eab\u7684\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u591a\u673a\u6784\u534f\u4f5c\u8bad\u7ec3\u624b\u672f\u57fa\u7840\u6a21\u578b\u3002", "method": "\u57fa\u4e8eEndoViT\u7814\u7a76\uff0c\u6539\u8fdbMasked Autoencoder\uff0c\u7ed3\u5408\u81ea\u9002\u5e94FedSAM\u548cSWA\uff0c\u9884\u8bad\u7ec3\u4e8eEndo700k\u6570\u636e\u96c6\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5fae\u8c03\u8bc4\u4f30\u3002", "result": "\u81ea\u9002\u5e94FedSAM\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u51cf\u5c11\u4e86\u91cd\u5efa\u635f\u5931\uff1bFL-EndoViT\u5728\u624b\u672f\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0eCEN-EndoViT\u76f8\u5f53\uff0c\u4e14\u5728\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e3a\u624b\u672f\u57fa\u7840\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u89c6\u9891\u6a21\u578b\u4ee5\u589e\u5f3a\u65f6\u7a7a\u52a8\u6001\u80fd\u529b\u3002"}}
{"id": "2504.16148", "pdf": "https://arxiv.org/pdf/2504.16148", "abs": "https://arxiv.org/abs/2504.16148", "authors": ["Danial Hooshyar", "Gustav \u0160\u00edr", "Yeongwook Yang", "Eve Kikas", "Raija H\u00e4m\u00e4l\u00e4inen", "Tommi K\u00e4rkk\u00e4inen", "Dragan Ga\u0161evi\u0107", "Roger Azevedo"], "title": "Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Despite significant advancements in AI-driven educational systems and ongoing\ncalls for responsible AI for education, several critical issues remain\nunresolved -- acting as the elephant in the room within AI in education,\nlearning analytics, educational data mining, learning sciences, and educational\npsychology communities. This critical analysis identifies and examines nine\npersistent challenges that continue to undermine the fairness, transparency,\nand effectiveness of current AI methods and applications in education. These\ninclude: (1) the lack of clarity around what AI for education truly means --\noften ignoring the distinct purposes, strengths, and limitations of different\nAI families -- and the trend of equating it with domain-agnostic,\ncompany-driven large language models; (2) the widespread neglect of essential\nlearning processes such as motivation, emotion, and (meta)cognition in\nAI-driven learner modelling and their contextual nature; (3) limited\nintegration of domain knowledge and lack of stakeholder involvement in AI\ndesign and development; (4) continued use of non-sequential machine learning\nmodels on temporal educational data; (5) misuse of non-sequential metrics to\nevaluate sequential models; (6) use of unreliable explainable AI methods to\nprovide explanations for black-box models; (7) ignoring ethical guidelines in\naddressing data inconsistencies during model training; (8) use of mainstream AI\nmethods for pattern discovery and learning analytics without systematic\nbenchmarking; and (9) overemphasis on global prescriptions while overlooking\nlocalised, student-specific recommendations. Supported by theoretical and\nempirical research, we demonstrate how hybrid AI methods -- specifically\nneural-symbolic AI -- can address the elephant in the room and serve as the\nfoundation for responsible, trustworthy AI systems in education.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86AI\u5728\u6559\u80b2\u9886\u57df\u7684\u4e5d\u5927\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7AI\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u6559\u80b2\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u516c\u5e73\u6027\u3001\u900f\u660e\u6027\u548c\u6709\u6548\u6027\u4ecd\u5b58\u5728\u95ee\u9898\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\u3002", "result": "\u795e\u7ecf\u7b26\u53f7AI\u80fd\u6709\u6548\u89e3\u51b3\u5f53\u524dAI\u5728\u6559\u80b2\u4e2d\u7684\u95ee\u9898\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7AI\u662f\u6784\u5efa\u8d1f\u8d23\u4efb\u3001\u53ef\u4fe1\u8d56\u6559\u80b2AI\u7cfb\u7edf\u7684\u57fa\u7840\u3002"}}
{"id": "2504.16616", "pdf": "https://arxiv.org/pdf/2504.16616", "abs": "https://arxiv.org/abs/2504.16616", "authors": ["Haosheng Chen", "Lian Luo", "Mengjingcheng Mo", "Zhanjie Wu", "Guobao Xiao", "Ji Gan", "Jiaxu Leng", "Xinbo Gao"], "title": "EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras, with microsecond temporal resolution and high dynamic range\n(HDR) characteristics, emit high-speed event stream for perception tasks.\nDespite the recent advancement in GNN-based perception methods, they are prone\nto use straightforward pairwise connectivity mechanisms in the pure Euclidean\nspace where they struggle to capture long-range dependencies and fail to\neffectively characterize the inherent hierarchical structures of non-uniformly\ndistributed event stream. To this end, in this paper we propose a novel\napproach named EHGCN, which is a pioneer to perceive event stream in both\nEuclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an\nadaptive sampling strategy to dynamically regulate sampling rates, retaining\ndiscriminative events while attenuating chaotic noise. Then we present a Markov\nVector Field (MVF)-driven motion-aware hyperedge generation method based on\nmotion state transition probabilities, thereby eliminating cross-target\nspurious associations and providing critically topological priors while\ncapturing long-range dependencies between events. Finally, we propose a\nEuclidean-Hyperbolic GCN to fuse the information locally aggregated and\nglobally hierarchically modeled in Euclidean and hyperbolic spaces,\nrespectively, to achieve hybrid event perception. Experimental results on event\nperception tasks such as object detection and recognition validate the\neffectiveness of our approach.", "AI": {"tldr": "EHGCN\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7ed3\u5408\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u7a7a\u95f4\u5904\u7406\u4e8b\u4ef6\u6d41\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u548c\u9a6c\u5c14\u53ef\u592b\u5411\u91cf\u573a\u9a71\u52a8\u7684\u8d85\u8fb9\u751f\u6210\uff0c\u63d0\u5347\u4e8b\u4ef6\u611f\u77e5\u4efb\u52a1\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u96be\u4ee5\u6355\u6349\u4e8b\u4ef6\u6d41\u7684\u957f\u7a0b\u4f9d\u8d56\u548c\u5c42\u6b21\u7ed3\u6784\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6df7\u5408\u7a7a\u95f4\u611f\u77e5\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEHGCN\uff0c\u5305\u542b\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u3001MVF\u9a71\u52a8\u7684\u8d85\u8fb9\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u53ca\u6b27\u51e0\u91cc\u5f97-\u53cc\u66f2GCN\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u3002", "result": "\u5728\u7269\u4f53\u68c0\u6d4b\u548c\u8bc6\u522b\u7b49\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "EHGCN\u901a\u8fc7\u6df7\u5408\u7a7a\u95f4\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.16152", "pdf": "https://arxiv.org/pdf/2504.16152", "abs": "https://arxiv.org/abs/2504.16152", "authors": ["Mohammad Molaee", "Nasrollah Moghadam Charkari"], "title": "Heterogeneous networks in drug-target interaction prediction", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": "18 pages, 5 figures, 10 tables", "summary": "Drug discovery requires a tremendous amount of time and cost. Computational\ndrug-target interaction prediction, a significant part of this process, can\nreduce these requirements by narrowing the search space for wet lab\nexperiments. In this survey, we provide comprehensive details of graph machine\nlearning-based methods in predicting drug-target interaction, as they have\nshown promising results in this field. These details include the overall\nframework, main contribution, datasets, and their source codes. The selected\npapers were mainly published from 2020 to 2024. Prior to discussing papers, we\nbriefly introduce the datasets commonly used with these methods and\nmeasurements to assess their performance. Finally, future challenges and some\ncrucial areas that need to be explored are discussed.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e862020\u81f32024\u5e74\u95f4\u57fa\u4e8e\u56fe\u673a\u5668\u5b66\u4e60\u7684\u836f\u7269-\u9776\u6807\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u65b9\u6cd5\uff0c\u603b\u7ed3\u4e86\u5176\u6846\u67b6\u3001\u8d21\u732e\u3001\u6570\u636e\u96c6\u53ca\u6e90\u4ee3\u7801\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u6311\u6218\u3002", "motivation": "\u836f\u7269\u53d1\u73b0\u8017\u65f6\u957f\u3001\u6210\u672c\u9ad8\uff0c\u8ba1\u7b97\u9884\u6d4b\u65b9\u6cd5\u53ef\u901a\u8fc7\u7f29\u5c0f\u5b9e\u9a8c\u8303\u56f4\u964d\u4f4e\u6210\u672c\uff0c\u56fe\u673a\u5668\u5b66\u4e60\u5728\u6b64\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "method": "\u7efc\u8ff0\u4e86\u56fe\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u6846\u67b6\u3001\u8d21\u732e\u3001\u6570\u636e\u96c6\u53ca\u6e90\u4ee3\u7801\uff0c\u5e76\u4ecb\u7ecd\u4e86\u5e38\u7528\u6570\u636e\u96c6\u548c\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u603b\u7ed3\u4e86\u56fe\u673a\u5668\u5b66\u4e60\u5728\u836f\u7269-\u9776\u6807\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u4e2d\u7684\u6210\u529f\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u8d44\u6e90\u3002", "conclusion": "\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u8be5\u9886\u57df\u7684\u6311\u6218\u548c\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2504.16636", "pdf": "https://arxiv.org/pdf/2504.16636", "abs": "https://arxiv.org/abs/2504.16636", "authors": ["Xianrui Luo", "Zijin Wu", "Juewen Peng", "Huiqiang Sun", "Zhiguo Cao", "Guosheng Lin"], "title": "Dual-Camera All-in-Focus Neural Radiance Fields", "categories": ["cs.CV"], "comment": "Published by IEEE TPAMI 2025", "summary": "We present the first framework capable of synthesizing the all-in-focus\nneural radiance field (NeRF) from inputs without manual refocusing. Without\nrefocusing, the camera will automatically focus on the fixed object for all\nviews, and current NeRF methods typically using one camera fail due to the\nconsistent defocus blur and a lack of sharp reference. To restore the\nall-in-focus NeRF, we introduce the dual-camera from smartphones, where the\nultra-wide camera has a wider depth-of-field (DoF) and the main camera\npossesses a higher resolution. The dual camera pair saves the high-fidelity\ndetails from the main camera and uses the ultra-wide camera's deep DoF as\nreference for all-in-focus restoration. To this end, we first implement spatial\nwarping and color matching to align the dual camera, followed by a\ndefocus-aware fusion module with learnable defocus parameters to predict a\ndefocus map and fuse the aligned camera pair. We also build a multi-view\ndataset that includes image pairs of the main and ultra-wide cameras in a\nsmartphone. Extensive experiments on this dataset verify that our solution,\ntermed DC-NeRF, can produce high-quality all-in-focus novel views and compares\nfavorably against strong baselines quantitatively and qualitatively. We further\nshow DoF applications of DC-NeRF with adjustable blur intensity and focal\nplane, including refocusing and split diopter.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u65e0\u9700\u624b\u52a8\u5bf9\u7126\u5373\u53ef\u5408\u6210\u5168\u805a\u7126\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u6846\u67b6\uff0c\u5229\u7528\u667a\u80fd\u624b\u673a\u7684\u53cc\u6444\u50cf\u5934\uff08\u4e3b\u6444\u548c\u8d85\u5e7f\u89d2\uff09\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5168\u805a\u7126\u89c6\u56fe\u751f\u6210\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u5355\u4e00\u6444\u50cf\u5934\u56fa\u5b9a\u5bf9\u7126\u5bfc\u81f4\u6a21\u7cca\u548c\u7f3a\u4e4f\u6e05\u6670\u53c2\u8003\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5168\u805a\u7126\u3002", "method": "\u901a\u8fc7\u53cc\u6444\u50cf\u5934\uff08\u4e3b\u6444\u9ad8\u5206\u8fa8\u7387\u3001\u8d85\u5e7f\u89d2\u5927\u666f\u6df1\uff09\u7684\u7a7a\u95f4\u5bf9\u9f50\u548c\u989c\u8272\u5339\u914d\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u79bb\u7126\u611f\u77e5\u878d\u5408\u6a21\u5757\uff0c\u9884\u6d4b\u79bb\u7126\u56fe\u5e76\u878d\u5408\u56fe\u50cf\u3002", "result": "\u5728\u81ea\u5efa\u591a\u89c6\u89d2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cDC-NeRF\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u805a\u7126\u65b0\u89c6\u56fe\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "DC-NeRF\u4e0d\u4ec5\u5b9e\u73b0\u5168\u805a\u7126\uff0c\u8fd8\u652f\u6301\u666f\u6df1\u8c03\u6574\u5e94\u7528\uff08\u5982\u91cd\u65b0\u5bf9\u7126\u548c\u5206\u5149\u955c\u6548\u679c\uff09\u3002"}}
{"id": "2504.16153", "pdf": "https://arxiv.org/pdf/2504.16153", "abs": "https://arxiv.org/abs/2504.16153", "authors": ["Kanwal Aalijah"], "title": "Leveraging Social Media Analytics for Sustainability Trend Detection in Saudi Arabias Evolving Market", "categories": ["cs.CY", "cs.AI"], "comment": "9", "summary": "Saudi Arabias rapid economic growth and social evolution under Vision 2030\npresent a unique opportunity to track emerging trends in real time. Uncovering\ntrends in real time can open up new avenues for business and investment\nopportunities. This paper explores how AI and social media analytics can\nuncover and monitor these trends across sectors like sustainability,\nconstruction, food beverages industry, tourism, technology, and entertainment.\nThis paper focus on use of AI-driven methodology to identify sustainability\ntrends across Saudi Arabia. We processed millions of social media posts, news,\nblogs in order to understand sustainability trends in the region. The paper\npresents an AI approach that can help economists, businesses, government to\nunderstand sustainability trends and make better decisions around them. This\napproach offers both sector-specific and cross-sector insights, giving\ndecision-makers a reliable, up to date snapshot of Saudi Arabias market shifts.\nBeyond Saudi Arabia, this framework also shows potential for adapting to other\nregions. Overall, our findings highlight how by using AI-methodologies, give\ndecision makers a reliable method to understand how initiatives are perceived\nand adopted by the public and understand growth of trends.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528AI\u548c\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u5b9e\u65f6\u8ffd\u8e2a\u6c99\u7279\u963f\u62c9\u4f2f\u5728\u300a\u613f\u666f2030\u300b\u4e0b\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u8d8b\u52bf\uff0c\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u53ef\u9760\u7684\u5e02\u573a\u6d1e\u5bdf\u3002", "motivation": "\u6c99\u7279\u963f\u62c9\u4f2f\u7684\u5feb\u901f\u7ecf\u6d4e\u589e\u957f\u548c\u793e\u4f1a\u53d8\u9769\u4e3a\u5b9e\u65f6\u8ffd\u8e2a\u65b0\u5174\u8d8b\u52bf\u63d0\u4f9b\u4e86\u72ec\u7279\u673a\u4f1a\uff0c\u6709\u52a9\u4e8e\u53d1\u73b0\u5546\u4e1a\u548c\u6295\u8d44\u673a\u4f1a\u3002", "method": "\u91c7\u7528AI\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5904\u7406\u6570\u767e\u4e07\u6761\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u3001\u65b0\u95fb\u548c\u535a\u5ba2\uff0c\u4ee5\u8bc6\u522b\u548c\u76d1\u6d4b\u53ef\u6301\u7eed\u53d1\u5c55\u8d8b\u52bf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cdAI\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u7ecf\u6d4e\u5b66\u5bb6\u3001\u4f01\u4e1a\u548c\u653f\u5e9c\u63d0\u4f9b\u53ef\u9760\u4e14\u5b9e\u65f6\u7684\u5e02\u573a\u8d8b\u52bf\u5206\u6790\uff0c\u5e76\u5c55\u793a\u8de8\u884c\u4e1a\u6f5c\u529b\u3002", "conclusion": "AI\u65b9\u6cd5\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u7406\u89e3\u516c\u4f17\u5bf9\u5021\u8bae\u7684\u63a5\u53d7\u5ea6\u548c\u8d8b\u52bf\u53d1\u5c55\u7684\u53ef\u9760\u5de5\u5177\uff0c\u4e14\u6846\u67b6\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u5730\u533a\u3002"}}
{"id": "2504.16637", "pdf": "https://arxiv.org/pdf/2504.16637", "abs": "https://arxiv.org/abs/2504.16637", "authors": ["Qifan Li", "Tianyi Liang", "Xingtao Wang", "Xiaopeng Fan"], "title": "RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Transformer models have recently garnered significant attention in image\nrestoration due to their ability to capture long-range pixel dependencies.\nHowever, long-range attention often results in computational overhead without\npractical necessity, as degradation and context are typically localized.\nNormalized average attention distance across various degradation datasets shows\nthat middle-range attention is enough for image restoration. Building on this\ninsight, we propose RouteWinFormer, a novel window-based Transformer that\nmodels middle-range context for image restoration. RouteWinFormer incorporates\nRoute-Windows Attnetion Module, which dynamically selects relevant nearby\nwindows based on regional similarity for attention aggregation, extending the\nreceptive field to a mid-range size efficiently. In addition, we introduce\nMulti-Scale Structure Regularization during training, enabling the sub-scale of\nthe U-shaped network to focus on structural information, while the\noriginal-scale learns degradation patterns based on generalized image structure\npriors. Extensive experiments demonstrate that RouteWinFormer outperforms\nstate-of-the-art methods across 9 datasets in various image restoration tasks.", "AI": {"tldr": "RouteWinFormer\u662f\u4e00\u79cd\u57fa\u4e8e\u7a97\u53e3\u7684Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u90bb\u8fd1\u7a97\u53e3\u8fdb\u884c\u6ce8\u610f\u529b\u805a\u5408\uff0c\u9ad8\u6548\u6269\u5c55\u611f\u53d7\u91ce\u81f3\u4e2d\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u7684\u957f\u8303\u56f4\u6ce8\u610f\u529b\u5728\u56fe\u50cf\u4fee\u590d\u4e2d\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4e0d\u5fc5\u8981\uff0c\u56e0\u4e3a\u9000\u5316\u548c\u4e0a\u4e0b\u6587\u901a\u5e38\u662f\u5c40\u90e8\u7684\u3002\u7814\u7a76\u8868\u660e\u4e2d\u8303\u56f4\u6ce8\u610f\u529b\u5df2\u8db3\u591f\u3002", "method": "\u63d0\u51faRouteWinFormer\uff0c\u5305\u542b\u52a8\u6001\u9009\u62e9\u90bb\u8fd1\u7a97\u53e3\u7684Route-Windows Attention\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u7ed3\u6784\u6b63\u5219\u5316\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u57289\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRouteWinFormer\u5728\u591a\u79cd\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RouteWinFormer\u901a\u8fc7\u4e2d\u8303\u56f4\u6ce8\u610f\u529b\u52a8\u6001\u805a\u5408\u548c\u7ed3\u6784\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2504.16640", "pdf": "https://arxiv.org/pdf/2504.16640", "abs": "https://arxiv.org/abs/2504.16640", "authors": ["Hasan Algafri", "Hamzah Luqman", "Sarah Alyami", "Issam Laradji"], "title": "SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sign language is the primary communication language for people with disabling\nhearing loss. Sign language recognition (SLR) systems aim to recognize sign\ngestures and translate them into spoken language. One of the main challenges in\nSLR is the scarcity of annotated datasets. To address this issue, we propose a\nsemi-supervised learning (SSL) approach for SLR (SSLR), employing a\npseudo-label method to annotate unlabeled samples. The sign gestures are\nrepresented using pose information that encodes the signer's skeletal joint\npoints. This information is used as input for the Transformer backbone model\nutilized in the proposed approach. To demonstrate the learning capabilities of\nSSL across various labeled data sizes, several experiments were conducted using\ndifferent percentages of labeled data with varying numbers of classes. The\nperformance of the SSL approach was compared with a fully supervised\nlearning-based model on the WLASL-100 dataset. The obtained results of the SSL\nmodel outperformed the supervised learning-based model with less labeled data\nin many cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08SSLR\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u8bed\u8bc6\u522b\uff08SLR\uff09\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u65b9\u6cd5\u6807\u6ce8\u672a\u6807\u8bb0\u6837\u672c\uff0c\u5e76\u5728WLASL-100\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5168\u76d1\u7763\u6a21\u578b\u3002", "motivation": "\u624b\u8bed\u662f\u542c\u529b\u969c\u788d\u4eba\u7fa4\u7684\u4e3b\u8981\u4ea4\u6d41\u8bed\u8a00\uff0c\u4f46\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08SSL\uff09\uff0c\u5229\u7528\u4f2a\u6807\u7b7e\u6807\u6ce8\u672a\u6807\u8bb0\u6837\u672c\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5904\u7406\u9aa8\u9abc\u5173\u8282\u70b9\u4fe1\u606f\u3002", "result": "\u5728WLASL-100\u6570\u636e\u96c6\u4e0a\uff0cSSL\u6a21\u578b\u5728\u6807\u6ce8\u6570\u636e\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u5168\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u624b\u8bed\u8bc6\u522b\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u51cf\u5c11\u6807\u6ce8\u6570\u636e\u9700\u6c42\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.16172", "pdf": "https://arxiv.org/pdf/2504.16172", "abs": "https://arxiv.org/abs/2504.16172", "authors": ["Zexi Fan", "Yan Sun", "Shihao Yang", "Yiping Lu"], "title": "Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning", "categories": ["math.NA", "cs.AI", "cs.LG", "cs.NA", "math.PR", "stat.ML"], "comment": null, "summary": "High-dimensional partial differential equations (PDEs) pose significant\ncomputational challenges across fields ranging from quantum chemistry to\neconomics and finance. Although scientific machine learning (SciML) techniques\noffer approximate solutions, they often suffer from bias and neglect crucial\nphysical insights. Inspired by inference-time scaling strategies in language\nmodels, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML),\na physics-informed framework that dynamically refines and debiases the SCiML\npredictions during inference by enforcing the physical laws. SCaSML leverages\nderived new physical laws that quantifies systematic errors and employs Monte\nCarlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to\ndynamically correct the prediction. Both numerical and theoretical analysis\nconfirms enhanced convergence rates via compute-optimal inference methods. Our\nnumerical experiments demonstrate that SCaSML reduces errors by 20-50% compared\nto the base surrogate model, establishing it as the first algorithm to refine\napproximated solutions to high-dimensional PDE during inference. Code of SCaSML\nis available at https://github.com/Francis-Fan-create/SCaSML.", "AI": {"tldr": "SCaSML\u662f\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4fee\u6b63\u79d1\u5b66\u673a\u5668\u5b66\u4e60\uff08SciML\uff09\u9884\u6d4b\uff0c\u51cf\u5c11\u9ad8\u7ef4\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6c42\u89e3\u4e2d\u7684\u504f\u5dee\u548c\u8bef\u5dee\u3002", "motivation": "\u9ad8\u7ef4PDE\u5728\u591a\u4e2a\u9886\u57df\u5b58\u5728\u8ba1\u7b97\u6311\u6218\uff0c\u73b0\u6709SciML\u65b9\u6cd5\u5e38\u5ffd\u7565\u7269\u7406\u89c4\u5f8b\u4e14\u5b58\u5728\u504f\u5dee\u3002", "method": "\u63d0\u51faSCaSML\u6846\u67b6\uff0c\u5229\u7528\u63a8\u65ad\u65f6\u7f29\u653e\u7b56\u7565\u548c\u8499\u7279\u5361\u6d1b\u6c42\u89e3\u5668\u52a8\u6001\u4fee\u6b63\u9884\u6d4b\uff0c\u5e76\u91cf\u5316\u7cfb\u7edf\u8bef\u5dee\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793aSCaSML\u6bd4\u57fa\u7840\u6a21\u578b\u51cf\u5c1120-50%\u7684\u8bef\u5dee\uff0c\u9996\u6b21\u5b9e\u73b0\u63a8\u65ad\u65f6\u4fee\u6b63\u9ad8\u7ef4PDE\u8fd1\u4f3c\u89e3\u3002", "conclusion": "SCaSML\u901a\u8fc7\u7269\u7406\u89c4\u5f8b\u52a8\u6001\u4f18\u5316\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u7ef4PDE\u6c42\u89e3\u7684\u51c6\u786e\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2504.16655", "pdf": "https://arxiv.org/pdf/2504.16655", "abs": "https://arxiv.org/abs/2504.16655", "authors": ["Younggeol Cho", "Elisa Motta", "Olivia Nocentini", "Marta Lagomarsino", "Andrea Merello", "Marco Crepaldi", "Arash Ajoudani"], "title": "WiFi based Human Fall and Activity Recognition using Transformer based Encoder Decoder and Graph Neural Networks", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Human pose estimation and action recognition have received attention due to\ntheir critical roles in healthcare monitoring, rehabilitation, and assistive\ntechnologies. In this study, we proposed a novel architecture named Transformer\nbased Encoder Decoder Network (TED Net) designed for estimating human skeleton\nposes from WiFi Channel State Information (CSI). TED Net integrates\nconvolutional encoders with transformer based attention mechanisms to capture\nspatiotemporal features from CSI signals. The estimated skeleton poses were\nused as input to a customized Directed Graph Neural Network (DGNN) for action\nrecognition. We validated our model on two datasets: a publicly available multi\nmodal dataset for assessing general pose estimation, and a newly collected\ndataset focused on fall related scenarios involving 20 participants.\nExperimental results demonstrated that TED Net outperformed existing approaches\nin pose estimation, and that the DGNN achieves reliable action classification\nusing CSI based skeletons, with performance comparable to RGB based systems.\nNotably, TED Net maintains robust performance across both fall and non fall\ncases. These findings highlight the potential of CSI driven human skeleton\nestimation for effective action recognition, particularly in home environments\nsuch as elderly fall detection. In such settings, WiFi signals are often\nreadily available, offering a privacy preserving alternative to vision based\nmethods, which may raise concerns about continuous camera monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTED Net\u7684\u65b0\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u4eceWiFi CSI\u4fe1\u53f7\u4f30\u8ba1\u4eba\u4f53\u9aa8\u9abc\u59ff\u6001\uff0c\u5e76\u7ed3\u5408DGNN\u8fdb\u884c\u52a8\u4f5c\u8bc6\u522b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u573a\u666f\u3002", "motivation": "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u52a8\u4f5c\u8bc6\u522b\u5728\u533b\u7597\u76d1\u63a7\u3001\u5eb7\u590d\u548c\u8f85\u52a9\u6280\u672f\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002WiFi CSI\u4fe1\u53f7\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\u7684\u9690\u79c1\u95ee\u9898\u3002", "method": "TED Net\u7ed3\u5408\u5377\u79ef\u7f16\u7801\u5668\u548c\u57fa\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4eceCSI\u4fe1\u53f7\u4e2d\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\u3002\u4f30\u8ba1\u7684\u9aa8\u9abc\u59ff\u6001\u8f93\u5165\u5230\u5b9a\u5236\u7684DGNN\u4e2d\u8fdb\u884c\u52a8\u4f5c\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTED Net\u5728\u59ff\u6001\u4f30\u8ba1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cDGNN\u7684\u52a8\u4f5c\u5206\u7c7b\u6027\u80fd\u4e0e\u57fa\u4e8eRGB\u7684\u7cfb\u7edf\u76f8\u5f53\uff0c\u4e14\u5728\u8dcc\u5012\u548c\u975e\u8dcc\u5012\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "WiFi CSI\u9a71\u52a8\u7684\u9aa8\u9abc\u59ff\u6001\u4f30\u8ba1\u5728\u52a8\u4f5c\u8bc6\u522b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u9690\u79c1\u654f\u611f\u5e94\u7528\uff0c\u5982\u8001\u5e74\u4eba\u8dcc\u5012\u68c0\u6d4b\u3002"}}
{"id": "2504.16173", "pdf": "https://arxiv.org/pdf/2504.16173", "abs": "https://arxiv.org/abs/2504.16173", "authors": ["Pedro Antunes", "Artur Podobas"], "title": "FPGA-Based Neural Network Accelerators for Space Applications: A Survey", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Space missions are becoming increasingly ambitious, necessitating\nhigh-performance onboard spacecraft computing systems. In response,\nfield-programmable gate arrays (FPGAs) have garnered significant interest due\nto their flexibility, cost-effectiveness, and radiation tolerance potential.\nConcurrently, neural networks (NNs) are being recognized for their capability\nto execute space mission tasks such as autonomous operations, sensor data\nanalysis, and data compression. This survey serves as a valuable resource for\nresearchers aiming to implement FPGA-based NN accelerators in space\napplications. By analyzing existing literature, identifying trends and gaps,\nand proposing future research directions, this work highlights the potential of\nthese accelerators to enhance onboard computing systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86FPGA\u5728\u822a\u5929\u4efb\u52a1\u4e2d\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u7684\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u73b0\u6709\u7814\u7a76\u5e76\u63d0\u51fa\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u822a\u5929\u4efb\u52a1\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u7684\u9700\u6c42\u589e\u52a0\uff0cFPGA\u56e0\u5176\u7075\u6d3b\u6027\u548c\u6297\u8f90\u5c04\u6027\u6210\u4e3a\u7406\u60f3\u9009\u62e9\uff0c\u795e\u7ecf\u7f51\u7edc\u5728\u81ea\u4e3b\u64cd\u4f5c\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "method": "\u901a\u8fc7\u6587\u732e\u5206\u6790\uff0c\u8bc6\u522b\u8d8b\u52bf\u4e0e\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "FPGA\u57fa\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u6709\u671b\u63d0\u5347\u822a\u5929\u5668\u8ba1\u7b97\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "FPGA\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\u4e3a\u822a\u5929\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2504.16656", "pdf": "https://arxiv.org/pdf/2504.16656", "abs": "https://arxiv.org/abs/2504.16656", "authors": ["Chris", "Yichen Wei", "Yi Peng", "Xiaokun Wang", "Weijie Qiu", "Wei Shen", "Tianyidan Xie", "Jiangbo Pei", "Jianhao Zhang", "Yunzhuo Hao", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.", "AI": {"tldr": "Skywork R1V2\u662f\u4e00\u79cd\u65b0\u4e00\u4ee3\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u548c\u9009\u62e9\u6027\u6837\u672c\u7f13\u51b2\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u80fd\u529b\u548c\u5e7f\u6cdb\u6cdb\u5316\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9\u8bad\u7ec3\u4e2d\u7684\u4f18\u52bf\u6d88\u5931\u548c\u89c6\u89c9\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u7ed3\u5408\u5956\u52b1\u6a21\u578b\u6307\u5bfc\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u9009\u62e9\u6027\u6837\u672c\u7f13\u51b2\uff08SSB\uff09\u673a\u5236\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u5982OlympiadBench 62.6\u3001AIME2024 79.0\u3001LiveCodeBench 63.6\u548cMMMU 74.0\u3002", "conclusion": "Skywork R1V2\u5728\u6027\u80fd\u548c\u5f00\u6e90\u6a21\u578b\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u6b65\uff0c\u7f29\u5c0f\u4e86\u4e0e\u9876\u7ea7\u4e13\u6709\u7cfb\u7edf\u7684\u5dee\u8ddd\uff0c\u5e76\u516c\u5f00\u6a21\u578b\u6743\u91cd\u4ee5\u4fc3\u8fdb\u5f00\u653e\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2504.16188", "pdf": "https://arxiv.org/pdf/2504.16188", "abs": "https://arxiv.org/abs/2504.16188", "authors": ["Jabez Magomere", "Elena Kochkina", "Samuel Mensah", "Simerjot Kaur", "Charese H. Smiley"], "title": "FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce FinNLI, a benchmark dataset for Financial Natural Language\nInference (FinNLI) across diverse financial texts like SEC Filings, Annual\nReports, and Earnings Call transcripts. Our dataset framework ensures diverse\npremise-hypothesis pairs while minimizing spurious correlations. FinNLI\ncomprises 21,304 pairs, including a high-quality test set of 3,304 instances\nannotated by finance experts. Evaluations show that domain shift significantly\ndegrades general-domain NLI performance. The highest Macro F1 scores for\npre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and\n78.62%, respectively, highlighting the dataset's difficulty. Surprisingly,\ninstruction-tuned financial LLMs perform poorly, suggesting limited\ngeneralizability. FinNLI exposes weaknesses in current LLMs for financial\nreasoning, indicating room for improvement.", "AI": {"tldr": "FinNLI\u662f\u4e00\u4e2a\u7528\u4e8e\u91d1\u878d\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b21,304\u5bf9\u6570\u636e\uff0c\u6d4b\u8bd5\u96c6\u7531\u91d1\u878d\u4e13\u5bb6\u6807\u6ce8\u3002\u8bc4\u4f30\u663e\u793a\u9886\u57df\u8f6c\u79fb\u663e\u8457\u964d\u4f4e\u901a\u7528NLI\u6027\u80fd\uff0c\u5f53\u524dLLMs\u5728\u91d1\u878d\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u591a\u6837\u5316\u7684\u91d1\u878dNLI\u6570\u636e\u96c6\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u5347\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u91d1\u878d\u6587\u672c\uff08\u5982SEC\u6587\u4ef6\u3001\u5e74\u62a5\u3001\u7535\u8bdd\u4f1a\u8bae\u8bb0\u5f55\uff09\u6784\u5efa\u591a\u6837\u5316\u7684\u524d\u63d0-\u5047\u8bbe\u5bf9\uff0c\u5e76\u786e\u4fdd\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6700\u9ad8Macro F1\u5206\u522b\u4e3a74.57%\u548c78.62%\uff0c\u6307\u4ee4\u8c03\u4f18\u7684\u91d1\u878dLLMs\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "FinNLI\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u91d1\u878d\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.16658", "pdf": "https://arxiv.org/pdf/2504.16658", "abs": "https://arxiv.org/abs/2504.16658", "authors": ["Ole-Christian Galbo Engstr\u00f8m", "Erik Schou Dreier", "Birthe M\u00f8ller Jespersen", "Kim Steenstrup Pedersen"], "title": "A Time Series Dataset of NIR Spectra and RGB and NIR-HSI Images of the Barley Germination Process", "categories": ["cs.CV"], "comment": null, "summary": "We provide an open-source dataset of RGB and NIR-HSI (near-infrared\nhyperspectral imaging) images with associated segmentation masks and NIR\nspectra of 2242 individual malting barley kernels. We imaged every kernel\npre-exposure to moisture and every 24 hours after exposure to moisture for five\nconsecutive days. Every barley kernel was labeled as germinated or not\ngerminated during each image acquisition. The barley kernels were imaged with\nblack filter paper as the background, facilitating straight-forward intensity\nthreshold-based segmentation, e.g., by Otsu's method. This dataset facilitates\ntime series analysis of germination time for barley kernels using either RGB\nimage analysis, NIR spectral analysis, NIR-HSI analysis, or a combination\nhereof.", "AI": {"tldr": "\u5f00\u6e90\u6570\u636e\u96c6\u5305\u542b2242\u4e2a\u5927\u9ea6\u7c7d\u7c92\u7684RGB\u548c\u8fd1\u7ea2\u5916\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u9644\u5e26\u5206\u5272\u63a9\u7801\u548c\u8fd1\u7ea2\u5916\u5149\u8c31\uff0c\u7528\u4e8e\u7814\u7a76\u53d1\u82bd\u65f6\u95f4\u3002", "motivation": "\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u652f\u6301\u57fa\u4e8eRGB\u3001\u8fd1\u7ea2\u5916\u5149\u8c31\u6216\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u5927\u9ea6\u7c7d\u7c92\u53d1\u82bd\u65f6\u95f4\u5206\u6790\u3002", "method": "\u6bcf\u5929\u91c7\u96c6\u5927\u9ea6\u7c7d\u7c92\u7684RGB\u548c\u8fd1\u7ea2\u5916\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u4f7f\u7528\u9ed1\u8272\u6ee4\u7eb8\u80cc\u666f\u7b80\u5316\u5206\u5272\uff0c\u6807\u8bb0\u53d1\u82bd\u72b6\u6001\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u591a\u79cd\u5206\u6790\u65b9\u6cd5\uff0c\u5305\u62ecRGB\u56fe\u50cf\u3001\u8fd1\u7ea2\u5916\u5149\u8c31\u548c\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u5927\u9ea6\u7c7d\u7c92\u53d1\u82bd\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2504.16193", "pdf": "https://arxiv.org/pdf/2504.16193", "abs": "https://arxiv.org/abs/2504.16193", "authors": ["Carmine Attanasio", "Alireza Mortezapour"], "title": "Quality of explanation of xAI from the prespective of Italian end-users: Italian version of System Causability Scale (SCS)", "categories": ["cs.HC", "cs.AI"], "comment": "This work will be presented in Coperman 2025 Conference", "summary": "Background and aim: Considering the scope of the application of artificial\nintelligence beyond the field of computer science, one of the concerns of\nresearchers is to provide quality explanations about the functioning of\nalgorithms based on artificial intelligence and the data extracted from it. The\npurpose of the present study is to validate the Italian version of system\ncausability scale (I-SCS) to measure the quality of explanations provided in a\nxAI.\n  Method: For this purpose, the English version, initially provided in 2020 in\ncoordination with the main developer, was utilized. The forward-backward\ntranslation method was applied to ensure accuracy. Finally, these nine steps\nwere completed by calculating the content validity index/ratio and conducting\ncognitive interviews with representative end users.\n  Results: The original version of the questionnaire consisted of 10 questions.\nHowever, based on the obtained indexes (CVR below 0.49), one question (Question\n8) was entirely removed. After completing the aforementioned steps, the Italian\nversion contained 9 questions. The representative sample of Italian end users\nfully comprehended the meaning and content of the questions in the Italian\nversion.\n  Conclusion: The Italian version obtained in this study can be used in future\nresearch studies as well as in the field by xAI developers. This tool can be\nused to measure the quality of explanations provided for an xAI system in\nItalian culture.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u610f\u5927\u5229\u7248\u7cfb\u7edf\u53ef\u89e3\u91ca\u6027\u91cf\u8868\uff08I-SCS\uff09\u7684\u6709\u6548\u6027\uff0c\u7528\u4e8e\u8bc4\u4f30xAI\u7cfb\u7edf\u63d0\u4f9b\u7684\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u8303\u56f4\u7684\u6269\u5927\uff0c\u7814\u7a76\u8005\u5173\u6ce8\u5982\u4f55\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u7b97\u6cd5\u89e3\u91ca\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u610f\u5927\u5229\u7248\u91cf\u8868\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u524d\u5411-\u540e\u5411\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u8ba1\u7b97\u5185\u5bb9\u6548\u5ea6\u6307\u6570/\u6bd4\u7387\uff0c\u5e76\u4e0e\u4ee3\u8868\u6027\u7ec8\u7aef\u7528\u6237\u8fdb\u884c\u8ba4\u77e5\u8bbf\u8c08\u3002", "result": "\u539f\u95ee\u537710\u4e2a\u95ee\u9898\u4e2d\uff0c1\u4e2a\u56e0\u6548\u5ea6\u4e0d\u8db3\u88ab\u79fb\u9664\uff0c\u6700\u7ec8\u610f\u5927\u5229\u7248\u5305\u542b9\u4e2a\u95ee\u9898\uff0c\u7528\u6237\u7406\u89e3\u826f\u597d\u3002", "conclusion": "\u610f\u5927\u5229\u7248\u91cf\u8868\u53ef\u7528\u4e8e\u672a\u6765\u7814\u7a76\u53caxAI\u5f00\u53d1\uff0c\u8bc4\u4f30\u610f\u5927\u5229\u6587\u5316\u4e2d\u7684\u89e3\u91ca\u8d28\u91cf\u3002"}}
{"id": "2504.16665", "pdf": "https://arxiv.org/pdf/2504.16665", "abs": "https://arxiv.org/abs/2504.16665", "authors": ["Wenping Ma", "Boyou Xue", "Mengru Ma", "Chuang Chen", "Hekai Zhang", "Hao Zhu"], "title": "A Diff-Attention Aware State Space Fusion Model for Remote Sensing Classification", "categories": ["cs.CV"], "comment": "12 pages,9 figures", "summary": "Multispectral (MS) and panchromatic (PAN) images describe the same land\nsurface, so these images not only have their own advantages, but also have a\nlot of similar information. In order to separate these similar information and\ntheir respective advantages, reduce the feature redundancy in the fusion stage.\nThis paper introduces a diff-attention aware state space fusion model\n(DAS2F-Model) for multimodal remote sensing image classification. Based on the\nselective state space model, a cross-modal diff-attention module (CMDA-Module)\nis designed to extract and separate the common features and their respective\ndominant features of MS and PAN images. Among this, space preserving visual\nmamba (SPVM) retains image spatial features and captures local features by\noptimizing visual mamba's input reasonably. Considering that features in the\nfusion stage will have large semantic differences after feature separation and\nsimple fusion operations struggle to effectively integrate these significantly\ndifferent features, an attention-aware linear fusion module (AALF-Module) is\nproposed. It performs pixel-wise linear fusion by calculating influence\ncoefficients. This mechanism can fuse features with large semantic differences\nwhile keeping the feature size unchanged. Empirical evaluations indicate that\nthe presented method achieves better results than alternative approaches. The\nrelevant code can be found at:https://github.com/AVKSKVL/DAS-F-Model", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u8de8\u6a21\u6001\u5dee\u5f02\u6ce8\u610f\u529b\u6a21\u5757\u548c\u6ce8\u610f\u529b\u611f\u77e5\u7ebf\u6027\u878d\u5408\u6a21\u5757\uff0c\u6709\u6548\u5206\u79bb\u548c\u878d\u5408MS\u4e0ePAN\u56fe\u50cf\u7684\u5171\u540c\u7279\u5f81\u548c\u4f18\u52bf\u7279\u5f81\u3002", "motivation": "MS\u548cPAN\u56fe\u50cf\u5305\u542b\u76f8\u4f3c\u4fe1\u606f\u4f46\u5404\u6709\u4f18\u52bf\uff0c\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5206\u79bb\u548c\u6574\u5408\u8fd9\u4e9b\u7279\u5f81\uff0c\u5bfc\u81f4\u5197\u4f59\u3002", "method": "\u8bbe\u8ba1\u4e86\u8de8\u6a21\u6001\u5dee\u5f02\u6ce8\u610f\u529b\u6a21\u5757\uff08CMDA-Module\uff09\u5206\u79bb\u7279\u5f81\uff0c\u7a7a\u95f4\u4fdd\u7559\u89c6\u89c9Mamba\uff08SPVM\uff09\u6355\u83b7\u5c40\u90e8\u7279\u5f81\uff0c\u6ce8\u610f\u529b\u611f\u77e5\u7ebf\u6027\u878d\u5408\u6a21\u5757\uff08AALF-Module\uff09\u878d\u5408\u8bed\u4e49\u5dee\u5f02\u5927\u7684\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684DAS2F-Model\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u7279\u5f81\u5206\u79bb\u548c\u878d\u5408\u95ee\u9898\u3002"}}
{"id": "2504.16204", "pdf": "https://arxiv.org/pdf/2504.16204", "abs": "https://arxiv.org/abs/2504.16204", "authors": ["Christian Djeffal"], "title": "Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.ET"], "comment": "20 pages one figure", "summary": "Responsible prompt engineering has emerged as a critical framework for\nensuring that generative artificial intelligence (AI) systems serve society's\nneeds while minimizing potential harms. As generative AI applications become\nincreasingly powerful and ubiquitous, the way we instruct and interact with\nthem through prompts has profound implications for fairness, accountability,\nand transparency. This article examines how strategic prompt engineering can\nembed ethical and legal considerations and societal values directly into AI\ninteractions, moving beyond mere technical optimization for functionality. This\narticle proposes a comprehensive framework for responsible prompt engineering\nthat encompasses five interconnected components: prompt design, system\nselection, system configuration, performance evaluation, and prompt management.\nDrawing from empirical evidence, the paper demonstrates how each component can\nbe leveraged to promote improved societal outcomes while mitigating potential\nrisks. The analysis reveals that effective prompt engineering requires a\ndelicate balance between technical precision and ethical consciousness,\ncombining the systematic rigor and focus on functionality with the nuanced\nunderstanding of social impact. Through examination of real-world and emerging\npractices, the article illustrates how responsible prompt engineering serves as\na crucial bridge between AI development and deployment, enabling organizations\nto fine-tune AI outputs without modifying underlying model architectures. This\napproach aligns with broader \"Responsibility by Design\" principles, embedding\nethical considerations directly into the implementation process rather than\ntreating them as post-hoc additions. The article concludes by identifying key\nresearch directions and practical guidelines for advancing the field of\nresponsible prompt engineering.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8d1f\u8d23\u4efb\u7684\u63d0\u793a\u5de5\u7a0b\u5982\u4f55\u901a\u8fc7\u5d4c\u5165\u4f26\u7406\u548c\u6cd5\u5f8b\u8003\u91cf\uff0c\u4f18\u5316\u751f\u6210\u5f0fAI\u7684\u793e\u4f1a\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u666e\u53ca\uff0c\u63d0\u793a\u5de5\u7a0b\u5bf9\u516c\u5e73\u6027\u3001\u95ee\u8d23\u5236\u548c\u900f\u660e\u5ea6\u7684\u5f71\u54cd\u65e5\u76ca\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u63d0\u793a\u8bbe\u8ba1\u3001\u7cfb\u7edf\u9009\u62e9\u3001\u914d\u7f6e\u3001\u6027\u80fd\u8bc4\u4f30\u548c\u7ba1\u7406\u7684\u7efc\u5408\u6846\u67b6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6709\u6548\u7684\u63d0\u793a\u5de5\u7a0b\u9700\u5e73\u8861\u6280\u672f\u7cbe\u786e\u6027\u4e0e\u4f26\u7406\u610f\u8bc6\uff0c\u6539\u5584\u793e\u4f1a\u7ed3\u679c\u5e76\u964d\u4f4e\u98ce\u9669\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u4e86\u5173\u952e\u7814\u7a76\u65b9\u5411\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u63a8\u52a8\u8d1f\u8d23\u4efb\u7684\u63d0\u793a\u5de5\u7a0b\u53d1\u5c55\u3002"}}
{"id": "2504.16684", "pdf": "https://arxiv.org/pdf/2504.16684", "abs": "https://arxiv.org/abs/2504.16684", "authors": ["Gerardus Croonen", "Andreas Trondl", "Julia Simon", "Daniel Steininger"], "title": "SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW). Code and dataset available at\n  https://github.com/semanticsugarbeets/semanticsugarbeets", "summary": "While sugar beets are stored prior to processing, they lose sugar due to\nfactors such as microorganisms present in adherent soil and excess vegetation.\nTheir automated visual inspection promises to aide in quality assurance and\nthereby increase efficiency throughout the processing chain of sugar\nproduction. In this work, we present a novel high-quality annotated dataset and\ntwo-stage method for the detection, semantic segmentation and mass estimation\nof post-harvest and post-storage sugar beets in monocular RGB images. We\nconduct extensive ablation experiments for the detection of sugar beets and\ntheir fine-grained semantic segmentation regarding damages, rot, soil adhesion\nand excess vegetation. For these tasks, we evaluate multiple image sizes, model\narchitectures and encoders, as well as the influence of environmental\nconditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection\nand an mIoU of 64.0 for the best-performing segmentation model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7cd6\u7528\u751c\u83dc\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u8d28\u91cf\u4f30\u8ba1\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7cd6\u7528\u751c\u83dc\u5728\u50a8\u5b58\u8fc7\u7a0b\u4e2d\u56e0\u5fae\u751f\u7269\u7b49\u56e0\u7d20\u5bfc\u81f4\u7cd6\u5206\u635f\u5931\uff0c\u81ea\u52a8\u5316\u89c6\u89c9\u68c0\u6d4b\u6709\u52a9\u4e8e\u63d0\u9ad8\u7cd6\u751f\u4ea7\u94fe\u7684\u6548\u7387\u548c\u8d28\u91cf\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7ed3\u5408\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u8d28\u91cf\u4f30\u8ba1\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540c\u56fe\u50cf\u5c3a\u5bf8\u3001\u6a21\u578b\u67b6\u6784\u548c\u73af\u5883\u6761\u4ef6\u7684\u5f71\u54cd\u3002", "result": "\u6700\u4f73\u68c0\u6d4b\u6a21\u578b\u7684mAP50-95\u4e3a98.8\uff0c\u6700\u4f73\u5206\u5272\u6a21\u578b\u7684mIoU\u4e3a64.0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cd6\u7528\u751c\u83dc\u7684\u81ea\u52a8\u5316\u89c6\u89c9\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u6709\u671b\u63d0\u5347\u7cd6\u751f\u4ea7\u94fe\u7684\u6548\u7387\u3002"}}
{"id": "2504.16213", "pdf": "https://arxiv.org/pdf/2504.16213", "abs": "https://arxiv.org/abs/2504.16213", "authors": ["Andrew Barovic", "Armin Moin"], "title": "TinyML for Speech Recognition", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "We train and deploy a quantized 1D convolutional neural network model to\nconduct speech recognition on a highly resource-constrained IoT edge device.\nThis can be useful in various Internet of Things (IoT) applications, such as\nsmart homes and ambient assisted living for the elderly and people with\ndisabilities, just to name a few examples. In this paper, we first create a new\ndataset with over one hour of audio data that enables our research and will be\nuseful to future studies in this field. Second, we utilize the technologies\nprovided by Edge Impulse to enhance our model's performance and achieve a high\nAccuracy of up to 97% on our dataset. For the validation, we implement our\nprototype using the Arduino Nano 33 BLE Sense microcontroller board. This\nmicrocontroller board is specifically designed for IoT and AI applications,\nmaking it an ideal choice for our target use case scenarios. While most\nexisting research focuses on a limited set of keywords, our model can process\n23 different keywords, enabling complex commands.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d44\u6e90\u53d7\u9650\u7684IoT\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u91cf\u53161D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bed\u97f3\u8bc6\u522b\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe97%\uff0c\u5e76\u652f\u630123\u79cd\u5173\u952e\u8bcd\u3002", "motivation": "\u4e3a\u667a\u80fd\u5bb6\u5c45\u548c\u8f85\u52a9\u751f\u6d3b\u7b49IoT\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u7684\u8bed\u97f3\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Edge Impulse\u6280\u672f\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728Arduino Nano 33 BLE Sense\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u539f\u578b\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u8fbe\u523097%\u7684\u51c6\u786e\u7387\uff0c\u652f\u630123\u79cd\u5173\u952e\u8bcd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bed\u97f3\u8bc6\u522b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.16692", "pdf": "https://arxiv.org/pdf/2504.16692", "abs": "https://arxiv.org/abs/2504.16692", "authors": ["Xinru Meng", "Han Sun", "Jiamei Liu", "Ningzhong Liu", "Huiyu Zhou"], "title": "Energy-Based Pseudo-Label Refining for Source-free Domain Adaptation", "categories": ["cs.CV"], "comment": "8 pages, 3 figures, accepted by PRL. code at\n  https://github.com/Sthen111/EBPR", "summary": "Source-free domain adaptation (SFDA), which involves adapting models without\naccess to source data, is both demanding and challenging. Existing SFDA\ntechniques typically rely on pseudo-labels generated from confidence levels,\nleading to negative transfer due to significant noise. To tackle this problem,\nEnergy-Based Pseudo-Label Refining (EBPR) is proposed for SFDA. Pseudo-labels\nare created for all sample clusters according to their energy scores. Global\nand class energy thresholds are computed to selectively filter pseudo-labels.\nFurthermore, a contrastive learning strategy is introduced to filter difficult\nsamples, aligning them with their augmented versions to learn more\ndiscriminative features. Our method is validated on the Office-31, Office-Home,\nand VisDA-C datasets, consistently finding that our model outperformed\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u4f2a\u6807\u7b7e\u7ec6\u5316\u65b9\u6cd5\uff08EBPR\uff09\uff0c\u7528\u4e8e\u65e0\u6e90\u57df\u9002\u5e94\uff08SFDA\uff09\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u7c7b\u522b\u80fd\u91cf\u9608\u503c\u8fc7\u6ee4\u4f2a\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684SFDA\u65b9\u6cd5\u4f9d\u8d56\u7f6e\u4fe1\u5ea6\u751f\u6210\u7684\u4f2a\u6807\u7b7e\uff0c\u566a\u58f0\u8f83\u5927\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u80fd\u91cf\u5206\u6570\u4e3a\u6837\u672c\u805a\u7c7b\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u8ba1\u7b97\u5168\u5c40\u548c\u7c7b\u522b\u80fd\u91cf\u9608\u503c\u7b5b\u9009\u4f2a\u6807\u7b7e\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u5bf9\u9f50\u56f0\u96be\u6837\u672c\u3002", "result": "\u5728Office-31\u3001Office-Home\u548cVisDA-C\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EBPR\u901a\u8fc7\u80fd\u91cf\u9608\u503c\u548c\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86SFDA\u4e2d\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.16214", "pdf": "https://arxiv.org/pdf/2504.16214", "abs": "https://arxiv.org/abs/2504.16214", "authors": ["Xiao Zhang", "Yaoyao Ding", "Yang Hu", "Gennady Pekhimenko"], "title": "Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis", "categories": ["cs.LG", "cs.AI", "cs.PL"], "comment": "17 pages, 24 figures", "summary": "Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL\nquantization techniques demand a new matrix multiplication operator with mixed\ninput data types, further complicating GPU optimization. Prior high-level\ncompilers like Triton lack the expressiveness to implement key optimizations\nlike fine-grained data pipelines and hardware-friendly memory layouts for these\noperators, while low-level programming models, such as Hidet, Graphene, and\nCUTLASS, require significant programming efforts. To balance expressiveness\nwith engineering effort, we propose Hexcute, a tile-based programming language\nthat exposes shared memory and register abstractions to enable fine-grained\noptimization for these operators. Additionally, Hexcute leverages task mapping\nto schedule the GPU program, and to reduce programming efforts, it automates\nlayout and task mapping synthesis with a novel type-inference-based algorithm.\nOur evaluation shows that Hexcute generalizes to a wide range of DL operators,\nachieves 1.7-11.28$\\times$ speedup over existing DL compilers for mixed-type\noperators, and brings up to 2.91$\\times$ speedup in the end-to-end evaluation.", "AI": {"tldr": "Hexcute\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u5757\u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u7528\u4e8e\u4f18\u5316GPU\u4e0a\u7684\u6df7\u5408\u6570\u636e\u7c7b\u578b\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\uff0c\u5e73\u8861\u8868\u8fbe\u80fd\u529b\u548c\u5de5\u7a0b\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u91cf\u5316\u6280\u672f\u9700\u8981\u6df7\u5408\u6570\u636e\u7c7b\u578b\u7684\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\uff0c\u73b0\u6709\u7f16\u8bd1\u5668\u5728\u8868\u8fbe\u80fd\u529b\u548c\u7f16\u7a0b\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "Hexcute\u901a\u8fc7\u5206\u5757\u7f16\u7a0b\u8bed\u8a00\u66b4\u9732\u5171\u4eab\u5185\u5b58\u548c\u5bc4\u5b58\u5668\u62bd\u8c61\uff0c\u7ed3\u5408\u4efb\u52a1\u6620\u5c04\u548c\u81ea\u52a8\u5e03\u5c40\u5408\u6210\u7b97\u6cd5\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4f18\u5316\u3002", "result": "Hexcute\u5728\u6df7\u5408\u7c7b\u578b\u8fd0\u7b97\u4e0a\u6bd4\u73b0\u6709\u7f16\u8bd1\u5668\u5feb1.7-11.28\u500d\uff0c\u7aef\u5230\u7aef\u8bc4\u4f30\u4e2d\u63d0\u5347\u8fbe2.91\u500d\u3002", "conclusion": "Hexcute\u5728\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u4f18\u5316\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u5b50\u3002"}}
{"id": "2504.16722", "pdf": "https://arxiv.org/pdf/2504.16722", "abs": "https://arxiv.org/abs/2504.16722", "authors": ["Yingjie Xi", "Jian Jun Zhang", "Xiaosong Yang"], "title": "PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In computer animation, game design, and human-computer interaction,\nsynthesizing human motion that aligns with user intent remains a significant\nchallenge. Existing methods have notable limitations: textual approaches offer\nhigh-level semantic guidance but struggle to describe complex actions\naccurately; trajectory-based techniques provide intuitive global motion\ndirection yet often fall short in generating precise or customized character\nmovements; and anchor poses-guided methods are typically confined to synthesize\nonly simple motion patterns. To generate more controllable and precise human\nmotions, we propose \\textbf{ProMoGen (Progressive Motion Generation)}, a novel\nframework that integrates trajectory guidance with sparse anchor motion\ncontrol. Global trajectories ensure consistency in spatial direction and\ndisplacement, while sparse anchor motions only deliver precise action guidance\nwithout displacement. This decoupling enables independent refinement of both\naspects, resulting in a more controllable, high-fidelity, and sophisticated\nmotion synthesis. ProMoGen supports both dual and single control paradigms\nwithin a unified training process. Moreover, we recognize that direct learning\nfrom sparse motions is inherently unstable, we introduce \\textbf{SAP-CL (Sparse\nAnchor Posture Curriculum Learning)}, a curriculum learning strategy that\nprogressively adjusts the number of anchors used for guidance, thereby enabling\nmore precise and stable convergence. Extensive experiments demonstrate that\nProMoGen excels in synthesizing vivid and diverse motions guided by predefined\ntrajectory and arbitrary anchor frames. Our approach seamlessly integrates\npersonalized motion with structured guidance, significantly outperforming\nstate-of-the-art methods across multiple control scenarios.", "AI": {"tldr": "ProMoGen\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f68\u8ff9\u5f15\u5bfc\u548c\u7a00\u758f\u951a\u70b9\u63a7\u5236\u7684\u6e10\u8fdb\u5f0f\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u5168\u5c40\u8f68\u8ff9\u548c\u7cbe\u786e\u52a8\u4f5c\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u548c\u590d\u6742\u7684\u8fd0\u52a8\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u7b26\u5408\u7528\u6237\u610f\u56fe\u7684\u4eba\u7c7b\u8fd0\u52a8\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u6587\u672c\u65b9\u6cd5\u96be\u4ee5\u63cf\u8ff0\u590d\u6742\u52a8\u4f5c\uff0c\u8f68\u8ff9\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u7cbe\u786e\u52a8\u4f5c\uff0c\u951a\u70b9\u65b9\u6cd5\u4ec5\u652f\u6301\u7b80\u5355\u6a21\u5f0f\u3002", "method": "ProMoGen\u7ed3\u5408\u5168\u5c40\u8f68\u8ff9\u548c\u7a00\u758f\u951a\u70b9\u8fd0\u52a8\uff0c\u5e76\u5f15\u5165SAP-CL\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u9010\u6b65\u8c03\u6574\u951a\u70b9\u6570\u91cf\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u7cbe\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cProMoGen\u80fd\u751f\u6210\u751f\u52a8\u591a\u6837\u7684\u8fd0\u52a8\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ProMoGen\u901a\u8fc7\u89e3\u8026\u548c\u6e10\u8fdb\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u63a7\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u5408\u6210\u3002"}}
{"id": "2504.16226", "pdf": "https://arxiv.org/pdf/2504.16226", "abs": "https://arxiv.org/abs/2504.16226", "authors": ["Yazan Otoum", "Arghavan Asad", "Amiya Nayak"], "title": "Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "comment": "This paper has been submitted to the IEEE Transactions on Network\n  Science and Engineering (TNSE) for possible publication", "summary": "Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer\nenhanced bandwidth capacity for large-scale service provisioning but remain\nvulnerable to evolving cyber threats. Existing intrusion detection and\nprevention methods provide limited security as adversaries continually adapt\ntheir attack strategies. We propose a dynamic attack detection and prevention\napproach to address this challenge. First, blockchain-based authentication uses\nthe Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy\nbefore data transmission. Next, a bi-stage intrusion detection system is\nintroduced: the first stage uses signature-based detection via an Improved\nRandom Forest (IRF) algorithm. In contrast, the second stage applies\nfeature-based anomaly detection using a Diffusion Convolution Recurrent Neural\nNetwork (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level\nAgreements (SLA), trust-aware service migration is performed using Heap-Based\nOptimization (HBO). Additionally, on-demand virtual High-Interaction honeypots\ndeceive attackers and extract attack patterns, which are securely stored using\nthe Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based\nIntrusion Detection Systems (IDS). The proposed framework is implemented in the\nNS3 simulation environment and evaluated against existing methods across\nmultiple performance metrics, including accuracy, attack detection rate, false\nnegative rate, precision, recall, ROC curve, memory usage, CPU usage, and\nexecution time. Experimental results demonstrate that the framework\nsignificantly outperforms existing approaches, reinforcing the security of\nNGWN-enabled IoT ecosystems", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u653b\u51fb\u68c0\u6d4b\u4e0e\u9632\u5fa1\u65b9\u6cd5\uff0c\u7ed3\u5408\u533a\u5757\u94fe\u8ba4\u8bc1\u3001\u53cc\u9636\u6bb5\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u548c\u4fe1\u4efb\u611f\u77e5\u670d\u52a1\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7269\u8054\u7f51\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u5165\u4fb5\u68c0\u6d4b\u4e0e\u9632\u5fa1\u65b9\u6cd5\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u7f51\u7edc\u5a01\u80c1\u9632\u62a4\u6709\u9650\uff0c\u9700\u63d0\u51fa\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u533a\u5757\u94fe\u8ba4\u8bc1\uff08DAA\uff09\u3001\u53cc\u9636\u6bb5\u5165\u4fb5\u68c0\u6d4b\uff08IRF\u548cDCRNN\uff09\u3001\u4fe1\u4efb\u611f\u77e5\u670d\u52a1\u8fc1\u79fb\uff08HBO\uff09\u53ca\u865a\u62df\u871c\u7f50\u6280\u672f\uff08BLISS\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u653b\u51fb\u68c0\u6d4b\u7387\u7b49\u591a\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7269\u8054\u7f51\u751f\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2504.16723", "pdf": "https://arxiv.org/pdf/2504.16723", "abs": "https://arxiv.org/abs/2504.16723", "authors": ["Ali Anaissi", "Junaid Akram", "Kunal Chaturvedi", "Ali Braytee"], "title": "Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 2 figures, 2025 International Conference on Computational\n  Science", "summary": "Memes are widely used for humor and cultural commentary, but they are\nincreasingly exploited to spread hateful content. Due to their multimodal\nnature, hateful memes often evade traditional text-only or image-only detection\nsystems, particularly when they employ subtle or coded references. To address\nthese challenges, we propose a multimodal hate detection framework that\nintegrates key components: OCR to extract embedded text, captioning to describe\nvisual content neutrally, sub-label classification for granular categorization\nof hateful content, RAG for contextually relevant retrieval, and VQA for\niterative analysis of symbolic and contextual cues. This enables the framework\nto uncover latent signals that simpler pipelines fail to detect. Experimental\nresults on the Facebook Hateful Memes dataset reveal that the proposed\nframework exceeds the performance of unimodal and conventional multimodal\nmodels in both accuracy and AUC-ROC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408OCR\u3001\u5b57\u5e55\u751f\u6210\u3001\u5b50\u6807\u7b7e\u5206\u7c7b\u3001RAG\u548cVQA\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u4ec7\u6068\u8868\u60c5\u5305\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u4f20\u7edf\u5355\u6a21\u6001\u68c0\u6d4b\u7cfb\u7edf\u96be\u4ee5\u8bc6\u522b\u5176\u9690\u542b\u5185\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5148\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u6574\u5408OCR\u63d0\u53d6\u6587\u672c\u3001\u5b57\u5e55\u751f\u6210\u63cf\u8ff0\u56fe\u50cf\u3001\u5b50\u6807\u7b7e\u5206\u7c7b\u7ec6\u5316\u4ec7\u6068\u5185\u5bb9\u3001RAG\u68c0\u7d22\u4e0a\u4e0b\u6587\u3001VQA\u5206\u6790\u7b26\u53f7\u7ebf\u7d22\u3002", "result": "\u5728Facebook Hateful Memes\u6570\u636e\u96c6\u4e0a\uff0c\u6846\u67b6\u7684\u51c6\u786e\u7387\u548cAUC-ROC\u5747\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u4f20\u7edf\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u591a\u6a21\u6001\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u4ec7\u6068\u8868\u60c5\u5305\u7684\u9690\u542b\u4fe1\u53f7\uff0c\u4e3a\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2504.16263", "pdf": "https://arxiv.org/pdf/2504.16263", "abs": "https://arxiv.org/abs/2504.16263", "authors": ["Magnus Sieverding", "Nathan Steffen", "Kelly Cohen"], "title": "Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents a performance benchmarking study of a Gradient-Optimized\nFuzzy Inference System (GF) classifier against several state-of-the-art machine\nlearning models, including Random Forest, XGBoost, Logistic Regression, Support\nVector Machines, and Neural Networks. The evaluation was conducted across five\ndatasets from the UCI Machine Learning Repository, each chosen for their\ndiversity in input types, class distributions, and classification complexity.\nUnlike traditional Fuzzy Inference Systems that rely on derivative-free\noptimization methods, the GF leverages gradient descent to significantly\nimproving training efficiency and predictive performance. Results demonstrate\nthat the GF model achieved competitive, and in several cases superior,\nclassification accuracy while maintaining high precision and exceptionally low\ntraining times. In particular, the GF exhibited strong consistency across folds\nand datasets, underscoring its robustness in handling noisy data and variable\nfeature sets. These findings support the potential of gradient optimized fuzzy\nsystems as interpretable, efficient, and adaptable alternatives to more complex\ndeep learning models in supervised learning tasks.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u7684\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\uff08GF\uff09\u4e0e\u591a\u79cd\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\uff0c\u7ed3\u679c\u8868\u660eGF\u5728\u5206\u7c7b\u51c6\u786e\u6027\u3001\u8bad\u7ec3\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u68af\u5ea6\u4f18\u5316\u7684\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u662f\u5426\u80fd\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u6210\u4e3a\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\uff0c\u5e76\u5728\u4e94\u4e2aUCI\u6570\u636e\u96c6\u4e0a\u4e0e\u968f\u673a\u68ee\u6797\u3001XGBoost\u3001\u903b\u8f91\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "GF\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u65f6\u95f4\u77ed\uff0c\u4e14\u5bf9\u566a\u58f0\u6570\u636e\u548c\u591a\u6837\u7279\u5f81\u96c6\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u68af\u5ea6\u4f18\u5316\u7684\u6a21\u7cca\u7cfb\u7edf\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u76d1\u7763\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2504.16727", "pdf": "https://arxiv.org/pdf/2504.16727", "abs": "https://arxiv.org/abs/2504.16727", "authors": ["Zhiyuan Fan", "Yumeng Wang", "Sandeep Polisetty", "Yi R.", "Fung"], "title": "V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision Language Models (LVLMs) excel in various vision-language tasks.\nYet, their robustness to visual variations in position, scale, orientation, and\ncontext that objects in natural scenes inevitably exhibit due to changes in\nviewpoint and environment remains largely underexplored. To bridge this gap, we\nintroduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating\nVisual Variation Robustness of LVLMs, which encompasses automated evaluation\ndataset generation and principled metrics for thorough robustness assessment.\nThrough extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability\nto visual variations, in which even advanced models that excel at complex\nvision-language tasks significantly underperform on simple tasks such as object\nrecognition. Interestingly, these models exhibit a distinct visual position\nbias that contradicts theories of effective receptive fields, and demonstrate a\nhuman-like visual acuity threshold. To identify the source of these\nvulnerabilities, we present a systematic framework for component-level\nanalysis, featuring a novel visualization approach for aligned visual features.\nResults show that these vulnerabilities stem from error accumulation in the\npipeline architecture and inadequate multimodal alignment. Complementary\nexperiments with synthetic data further demonstrate that these limitations are\nfundamentally architectural deficiencies, scoring the need for architectural\ninnovations in future LVLM designs.", "AI": {"tldr": "LVLMs\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u89c6\u89c9\u53d8\u5316\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\u3002V\u00b2R-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0LVLMs\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u89c6\u89c9\u4f4d\u7f6e\u504f\u5dee\u548c\u4eba\u7c7b\u7c7b\u4f3c\u89c6\u89c9\u654f\u9510\u5ea6\u9608\u503c\u3002\u95ee\u9898\u6e90\u4e8e\u67b6\u6784\u7f3a\u9677\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76LVLMs\u5bf9\u89c6\u89c9\u53d8\u5316\uff08\u5982\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u3001\u65b9\u5411\uff09\u7684\u9c81\u68d2\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faV\u00b2R-Bench\u6846\u67b6\uff0c\u5305\u62ec\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5bf921\u4e2aLVLMs\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u7ec4\u4ef6\u7ea7\u5206\u6790\u3002", "result": "\u53d1\u73b0LVLMs\u5bf9\u89c6\u89c9\u53d8\u5316\u654f\u611f\uff0c\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\u548c\u89c6\u89c9\u654f\u9510\u5ea6\u9608\u503c\uff0c\u95ee\u9898\u6e90\u4e8e\u67b6\u6784\u7f3a\u9677\u3002", "conclusion": "\u672a\u6765LVLM\u8bbe\u8ba1\u9700\u6539\u8fdb\u67b6\u6784\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u3002"}}
{"id": "2504.16268", "pdf": "https://arxiv.org/pdf/2504.16268", "abs": "https://arxiv.org/abs/2504.16268", "authors": ["Abdesslem Layeb"], "title": "Boosting Classifier Performance with Opposition-Based Data Transformation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we introduce a novel data transformation framework based on\nOpposition-Based Learning (OBL) to boost the performance of traditional\nclassification algorithms. Originally developed to accelerate convergence in\noptimization tasks, OBL is leveraged here to generate synthetic opposite\nsamples that replace the acutely training data and improve decision boundary\nformation. We explore three OBL variants; Global OBL, Class-Wise OBL, and\nLocalized Class-Wise OBL; and integrate them with several widely used\nclassifiers, including K-Nearest Neighbors (KNN), Support Vector Machines\n(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments\nconducted on 26 heterogeneous and high-dimensional datasets demonstrate that\nOBL-enhanced classifiers consistently outperform their standard counterparts in\nterms of accuracy and F1-score, frequently achieving near-perfect or perfect\nclassification. Furthermore, OBL contributes to improved computational\nefficiency, particularly in SVM and LR. These findings underscore the potential\nof OBL as a lightweight yet powerful data transformation strategy for enhancing\nclassification performance, especially in complex or sparse learning\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u7acb\u5b66\u4e60\uff08OBL\uff09\u7684\u65b0\u578b\u6570\u636e\u8f6c\u6362\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u4f20\u7edf\u5206\u7c7b\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u901a\u8fc7\u751f\u6210\u5408\u6210\u5bf9\u7acb\u6837\u672c\uff0cOBL\u663e\u8457\u6539\u5584\u4e86\u51b3\u7b56\u8fb9\u754c\u7684\u5f62\u6210\uff0c\u5e76\u5728\u591a\u4e2a\u5206\u7c7b\u5668\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5206\u7c7b\u7b97\u6cd5\u5728\u590d\u6742\u6216\u7a00\u758f\u5b66\u4e60\u73af\u5883\u4e2d\u6027\u80fd\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u9ad8\u6548\u7684\u6570\u636e\u8f6c\u6362\u7b56\u7565\u6765\u63d0\u5347\u5176\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cdOBL\u53d8\u4f53\uff08\u5168\u5c40OBL\u3001\u7c7b\u7ea7OBL\u548c\u5c40\u90e8\u7c7b\u7ea7OBL\uff09\uff0c\u5e76\u5c06\u5176\u4e0eKNN\u3001SVM\u3001LR\u548cDT\u7b49\u5206\u7c7b\u5668\u7ed3\u5408\u3002", "result": "\u572826\u4e2a\u5f02\u6784\u548c\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOBL\u589e\u5f3a\u7684\u5206\u7c7b\u5668\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6\u7248\u672c\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "OBL\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u6570\u636e\u8f6c\u6362\u7b56\u7565\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u6216\u7a00\u758f\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2504.16739", "pdf": "https://arxiv.org/pdf/2504.16739", "abs": "https://arxiv.org/abs/2504.16739", "authors": ["Tristan Piater", "Bj\u00f6rn Barz", "Alexander Freytag"], "title": "Prompt-Tuning SAM: From Generalist to Specialist with only 2048 Parameters and 16 Training Images", "categories": ["cs.CV"], "comment": null, "summary": "The Segment Anything Model (SAM) is widely used for segmenting a diverse\nrange of objects in natural images from simple user prompts like points or\nbounding boxes. However, SAM's performance decreases substantially when applied\nto non-natural domains like microscopic imaging. Furthermore, due to SAM's\ninteractive design, it requires a precise prompt for each image and object,\nwhich is unfeasible in many automated biomedical applications. Previous\nsolutions adapt SAM by training millions of parameters via fine-tuning large\nparts of the model or of adapter layers. In contrast, we show that as little as\n2,048 additional parameters are sufficient for turning SAM into a use-case\nspecialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM)\nmethod uses prompt-tuning, a parameter-efficient fine-tuning technique, to\nadapt SAM for a specific task. We validate the performance of our approach on\nmultiple microscopic and one medical dataset. Our results show that\nprompt-tuning only SAM's mask decoder already leads to a performance on-par\nwith state-of-the-art techniques while requiring roughly 2,000x less trainable\nparameters. For addressing domain gaps, we find that additionally prompt-tuning\nSAM's image encoder is beneficial, further improving segmentation accuracy by\nup to 18% over state-of-the-art results. Since PTSAM can be reliably trained\nwith as little as 16 annotated images, we find it particularly helpful for\napplications with limited training data and domain shifts.", "AI": {"tldr": "PTSAM\u901a\u8fc7\u4ec5\u8c03\u65742048\u4e2a\u53c2\u6570\uff0c\u5c06SAM\u6a21\u578b\u4f18\u5316\u4e3a\u7279\u5b9a\u4efb\u52a1\u7684\u4e13\u5bb6\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u9700\u6c42\u5927\u5e45\u51cf\u5c11\u3002", "motivation": "SAM\u5728\u975e\u81ea\u7136\u56fe\u50cf\u9886\u57df\uff08\u5982\u663e\u5fae\u6210\u50cf\uff09\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u9700\u8981\u7cbe\u786e\u63d0\u793a\uff0c\u4e0d\u9002\u5408\u81ea\u52a8\u5316\u751f\u7269\u533b\u5b66\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u63d0\u793a\u8c03\u4f18\uff08prompt-tuning\uff09\u6280\u672f\uff0c\u4ec5\u8c03\u6574SAM\u7684\u63a9\u7801\u89e3\u7801\u5668\u548c\u56fe\u50cf\u7f16\u7801\u5668\u3002", "result": "\u5728\u591a\u4e2a\u663e\u5fae\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\uff0c\u53c2\u6570\u9700\u6c42\u51cf\u5c112000\u500d\uff1b\u8c03\u6574\u56fe\u50cf\u7f16\u7801\u5668\u53ef\u8fdb\u4e00\u6b65\u63d0\u534718%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "PTSAM\u9002\u7528\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u9886\u57df\u8fc1\u79fb\u7684\u5e94\u7528\uff0c\u4ec5\u970016\u5f20\u6807\u6ce8\u56fe\u50cf\u5373\u53ef\u53ef\u9760\u8bad\u7ec3\u3002"}}
{"id": "2504.16275", "pdf": "https://arxiv.org/pdf/2504.16275", "abs": "https://arxiv.org/abs/2504.16275", "authors": ["Jannis Born", "Filip Skogh", "Kahn Rhrissorrakrai", "Filippo Utro", "Nico Wagner", "Aleksandros Sobczyk"], "title": "Quantum Doubly Stochastic Transformers", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CV"], "comment": "Under Review", "summary": "At the core of the Transformer, the Softmax normalizes the attention matrix\nto be right stochastic. Previous research has shown that this often\ndestabilizes training and that enforcing the attention matrix to be doubly\nstochastic (through Sinkhorn's algorithm) consistently improves performance\nacross different tasks, domains and Transformer flavors. However, Sinkhorn's\nalgorithm is iterative, approximative, non-parametric and thus inflexible\nw.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been\nproven that DSMs can be obtained with a parametric quantum circuit, yielding a\nnovel quantum inductive bias for DSMs with no known classical analogue.\nMotivated by this, we demonstrate the feasibility of a hybrid classical-quantum\ndoubly stochastic Transformer (QDSFormer) that replaces the Softmax in the\nself-attention layer with a variational quantum circuit. We study the\nexpressive power of the circuit and find that it yields more diverse DSMs that\nbetter preserve information than classical operators. Across multiple\nsmall-scale object recognition tasks, we find that our QDSFormer consistently\nsurpasses both a standard Vision Transformer and other doubly stochastic\nTransformers. Beyond the established Sinkformer, this comparison includes a\nnovel quantum-inspired doubly stochastic Transformer (based on QR\ndecomposition) that can be of independent interest. The QDSFormer also shows\nimproved training stability and lower performance variation suggesting that it\nmay mitigate the notoriously unstable training of ViTs on small-scale data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u53cc\u968f\u673aTransformer\uff08QDSFormer\uff09\uff0c\u7528\u53d8\u5206\u91cf\u5b50\u7535\u8def\u66ff\u4ee3Softmax\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edfTransformer\u4e2dSoftmax\u7684\u5f52\u4e00\u5316\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u800c\u73b0\u6709\u7684\u53cc\u968f\u673a\u77e9\u9635\u65b9\u6cd5\uff08\u5982Sinkhorn\u7b97\u6cd5\uff09\u5b58\u5728\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u91cf\u5b50\u7535\u8def\u4e3a\u53cc\u968f\u673a\u77e9\u9635\u63d0\u4f9b\u4e86\u65b0\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faQDSFormer\uff0c\u7528\u53d8\u5206\u91cf\u5b50\u7535\u8def\u66ff\u4ee3Softmax\uff0c\u751f\u6210\u66f4\u7075\u6d3b\u7684\u53cc\u968f\u673a\u77e9\u9635\u3002\u7814\u7a76\u4e86\u7535\u8def\u7684\u8868\u8fbe\u80fd\u529b\u53ca\u5176\u4fe1\u606f\u4fdd\u7559\u80fd\u529b\u3002", "result": "QDSFormer\u5728\u5c0f\u89c4\u6a21\u7269\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u6807\u51c6Vision Transformer\u548c\u5176\u4ed6\u53cc\u968f\u673aTransformer\uff0c\u8bad\u7ec3\u66f4\u7a33\u5b9a\u3002", "conclusion": "QDSFormer\u5c55\u793a\u4e86\u91cf\u5b50\u7535\u8def\u5728Transformer\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5c0f\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16740", "pdf": "https://arxiv.org/pdf/2504.16740", "abs": "https://arxiv.org/abs/2504.16740", "authors": ["Farhad G. Zanjani", "Davide Abati", "Auke Wiggers", "Dimitris Kalatzis", "Jens Petersen", "Hong Cai", "Amirhossein Habibian"], "title": "Gaussian Splatting is an Effective Data Generator for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We investigate data augmentation for 3D object detection in autonomous\ndriving. We utilize recent advancements in 3D reconstruction based on Gaussian\nSplatting for 3D object placement in driving scenes. Unlike existing\ndiffusion-based methods that synthesize images conditioned on BEV layouts, our\napproach places 3D objects directly in the reconstructed 3D space with\nexplicitly imposed geometric transformations. This ensures both the physical\nplausibility of object placement and highly accurate 3D pose and position\nannotations.\n  Our experiments demonstrate that even by integrating a limited number of\nexternal 3D objects into real scenes, the augmented data significantly enhances\n3D object detection performance and outperforms existing diffusion-based 3D\naugmentation for object detection. Extensive testing on the nuScenes dataset\nreveals that imposing high geometric diversity in object placement has a\ngreater impact compared to the appearance diversity of objects. Additionally,\nwe show that generating hard examples, either by maximizing detection loss or\nimposing high visual occlusion in camera images, does not lead to more\nefficient 3D data augmentation for camera-based 3D object detection in\nautonomous driving.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u76843D\u91cd\u5efa\u6280\u672f\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u76843D\u7269\u4f53\u68c0\u6d4b\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u76f4\u63a5\u653e\u7f6e3D\u7269\u4f53\u5e76\u65bd\u52a0\u51e0\u4f55\u53d8\u6362\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5728BEV\u5e03\u5c40\u4e0b\u5408\u6210\u56fe\u50cf\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u5408\u7406\u6027\u548c\u7cbe\u786e\u76843D\u59ff\u6001\u6807\u6ce8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u76f4\u63a5\u7684\u65b9\u6cd5\u6765\u589e\u5f3a3D\u7269\u4f53\u68c0\u6d4b\u6570\u636e\u3002", "method": "\u5229\u7528\u9ad8\u65af\u6e85\u5c04\u76843D\u91cd\u5efa\u6280\u672f\uff0c\u76f4\u63a5\u57283D\u7a7a\u95f4\u4e2d\u653e\u7f6e\u7269\u4f53\u5e76\u65bd\u52a0\u51e0\u4f55\u53d8\u6362\uff0c\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\u548c\u7cbe\u786e\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u65b9\u6cd5\uff0c\u4e14\u51e0\u4f55\u591a\u6837\u6027\u6bd4\u5916\u89c2\u591a\u6837\u6027\u66f4\u91cd\u8981\u3002", "conclusion": "\u76f4\u63a53D\u7269\u4f53\u653e\u7f6e\u548c\u51e0\u4f55\u53d8\u6362\u662f\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u800c\u751f\u6210\u56f0\u96be\u6837\u672c\u5bf9\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u6548\u679c\u6709\u9650\u3002"}}
{"id": "2504.16276", "pdf": "https://arxiv.org/pdf/2504.16276", "abs": "https://arxiv.org/abs/2504.16276", "authors": ["Abhishek Jana", "Moeumu Uili", "James Atherton", "Mark O'Brien", "Joe Wood", "Leandra Brickson"], "title": "An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD"], "comment": "16 pages, 5 figures, 4 tables", "summary": "This paper presents an automated one-shot bird call classification pipeline\ndesigned for rare species absent from large publicly available classifiers like\nBirdNET and Perch. While these models excel at detecting common birds with\nabundant training data, they lack options for species with only 1-3 known\nrecordings-a critical limitation for conservationists monitoring the last\nremaining individuals of endangered birds. To address this, we leverage the\nembedding space of large bird classification networks and develop a classifier\nusing cosine similarity, combined with filtering and denoising preprocessing\ntechniques, to optimize detection with minimal training data. We evaluate\nvarious embedding spaces using clustering metrics and validate our approach in\nboth a simulated scenario with Xeno-Canto recordings and a real-world test on\nthe critically endangered tooth-billed pigeon (Didunculus strigirostris), which\nhas no existing classifiers and only three confirmed recordings. The final\nmodel achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon\ncalls, making it practical for use in the field. This open-source system\nprovides a practical tool for conservationists seeking to detect and monitor\nrare species on the brink of extinction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7a00\u6709\u9e1f\u7c7b\u7684\u81ea\u52a8\u5316\u5355\u6b21\u9e1f\u9e23\u5206\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5206\u7c7b\u5668\u56e0\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u65e0\u6cd5\u8bc6\u522b\u7a00\u6709\u7269\u79cd\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5206\u7c7b\u5668\uff08\u5982BirdNET\u548cPerch\uff09\u5bf9\u5e38\u89c1\u9e1f\u7c7b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u4ec5\u67091-3\u4e2a\u5f55\u97f3\u7684\u7a00\u6709\u7269\u79cd\u65e0\u80fd\u4e3a\u529b\uff0c\u8fd9\u5bf9\u6fd2\u5371\u7269\u79cd\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u5927\u578b\u9e1f\u7c7b\u5206\u7c7b\u7f51\u7edc\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u7ed3\u5408\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5206\u7c7b\u5668\u53ca\u9884\u5904\u7406\u6280\u672f\uff08\u8fc7\u6ee4\u548c\u964d\u566a\uff09\uff0c\u4ee5\u6700\u5c11\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u68c0\u6d4b\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\uff08\u6781\u5ea6\u6fd2\u5371\u7684\u9f7f\u5634\u9e3d\uff09\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u53ec\u56de\u7387\u8fbe1.0\uff0c\u51c6\u786e\u7387\u8fbe0.95\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4fdd\u62a4\u6fd2\u5371\u7269\u79cd\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u4e14\u5f00\u6e90\u53ef\u7528\u3002"}}
{"id": "2504.16749", "pdf": "https://arxiv.org/pdf/2504.16749", "abs": "https://arxiv.org/abs/2504.16749", "authors": ["Rupak Bose", "Chinedu Innocent Nwoye", "Jorge Lazo", "Jo\u00ebl Lukas Lavanchy", "Nicolas Padoy"], "title": "Feature Mixing Approach for Detecting Intraoperative Adverse Events in Laparoscopic Roux-en-Y Gastric Bypass Surgery", "categories": ["cs.CV"], "comment": "9 pages, 7 figures, 8 tables, Release new dataset annotations", "summary": "Intraoperative adverse events (IAEs), such as bleeding or thermal injury, can\nlead to severe postoperative complications if undetected. However, their rarity\nresults in highly imbalanced datasets, posing challenges for AI-based detection\nand severity quantification. We propose BetaMixer, a novel deep learning model\nthat addresses these challenges through a Beta distribution-based mixing\napproach, converting discrete IAE severity scores into continuous values for\nprecise severity regression (0-5 scale). BetaMixer employs Beta\ndistribution-based sampling to enhance underrepresented classes and regularizes\nintermediate embeddings to maintain a structured feature space. A generative\napproach aligns the feature space with sampled IAE severity, enabling robust\nclassification and severity regression via a transformer. Evaluated on the\nMultiBypass140 dataset, which we extended with IAE labels, BetaMixer achieves a\nweighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84,\ndemonstrating strong performance on imbalanced data. By integrating Beta\ndistribution-based sampling, feature mixing, and generative modeling, BetaMixer\noffers a robust solution for IAE detection and quantification in clinical\nsettings.", "AI": {"tldr": "BetaMixer\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7Beta\u5206\u5e03\u6df7\u5408\u65b9\u6cd5\u89e3\u51b3\u672f\u4e2d\u4e0d\u826f\u4e8b\u4ef6\uff08IAE\uff09\u68c0\u6d4b\u548c\u4e25\u91cd\u6027\u91cf\u5316\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u672f\u4e2d\u4e0d\u826f\u4e8b\u4ef6\uff08IAE\uff09\u5982\u51fa\u8840\u6216\u70ed\u635f\u4f24\uff0c\u82e5\u672a\u88ab\u68c0\u6d4b\u5230\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u672f\u540e\u5e76\u53d1\u75c7\uff0c\u4f46\u5176\u7f55\u89c1\u6027\u5bfc\u81f4\u6570\u636e\u96c6\u9ad8\u5ea6\u4e0d\u5e73\u8861\uff0c\u4e3aAI\u68c0\u6d4b\u5e26\u6765\u6311\u6218\u3002", "method": "BetaMixer\u91c7\u7528Beta\u5206\u5e03\u91c7\u6837\u589e\u5f3a\u5c11\u6570\u7c7b\uff0c\u901a\u8fc7\u8fde\u7eed\u5316\u79bb\u6563\u4e25\u91cd\u6027\u8bc4\u5206\uff080-5\uff09\u5b9e\u73b0\u7cbe\u786e\u56de\u5f52\uff0c\u5e76\u5229\u7528\u751f\u6210\u65b9\u6cd5\u5bf9\u9f50\u7279\u5f81\u7a7a\u95f4\uff0c\u7ed3\u5408Transformer\u8fdb\u884c\u5206\u7c7b\u548c\u56de\u5f52\u3002", "result": "\u5728MultiBypass140\u6570\u636e\u96c6\u4e0a\uff0cBetaMixer\u52a0\u6743F1\u5f97\u5206\u4e3a0.76\uff0c\u53ec\u56de\u73870.81\uff0cPPV 0.73\uff0cNPV 0.84\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "BetaMixer\u901a\u8fc7Beta\u5206\u5e03\u91c7\u6837\u3001\u7279\u5f81\u6df7\u5408\u548c\u751f\u6210\u5efa\u6a21\uff0c\u4e3a\u4e34\u5e8aIAE\u68c0\u6d4b\u548c\u91cf\u5316\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16277", "pdf": "https://arxiv.org/pdf/2504.16277", "abs": "https://arxiv.org/abs/2504.16277", "authors": ["Neha Hulkund", "Alaa Maalouf", "Levi Cai", "Daniel Yang", "Tsun-Hsuan Wang", "Abigail O'Neil", "Timm Haucke", "Sandeep Mukherjee", "Vikram Ramaswamy", "Judy Hansen Shen", "Gabriel Tseng", "Mike Walmsley", "Daniela Rus", "Ken Goldberg", "Hannah Kerner", "Irene Chen", "Yogesh Girdhar", "Sara Beery"], "title": "DataS^3: Dataset Subset Selection for Specialization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In many real-world machine learning (ML) applications (e.g. detecting broken\nbones in x-ray images, detecting species in camera traps), in practice models\nneed to perform well on specific deployments (e.g. a specific hospital, a\nspecific national park) rather than the domain broadly. However, deployments\noften have imbalanced, unique data distributions. Discrepancy between the\ntraining distribution and the deployment distribution can lead to suboptimal\nperformance, highlighting the need to select deployment-specialized subsets\nfrom the available training data. We formalize dataset subset selection for\nspecialization (DS3): given a training set drawn from a general distribution\nand a (potentially unlabeled) query set drawn from the desired\ndeployment-specific distribution, the goal is to select a subset of the\ntraining data that optimizes deployment performance.\n  We introduce DataS^3; the first dataset and benchmark designed specifically\nfor the DS3 problem. DataS^3 encompasses diverse real-world application\ndomains, each with a set of distinct deployments to specialize in. We conduct a\ncomprehensive study evaluating algorithms from various families--including\ncoresets, data filtering, and data curation--on DataS^3, and find that\ngeneral-distribution methods consistently fail on deployment-specific tasks.\nAdditionally, we demonstrate the existence of manually curated\n(deployment-specific) expert subsets that outperform training on all available\ndata with accuracy gains up to 51.3 percent. Our benchmark highlights the\ncritical role of tailored dataset curation in enhancing performance and\ntraining efficiency on deployment-specific distributions, which we posit will\nonly become more important as global, public datasets become available across\ndomains and ML models are deployed in the real world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7279\u5b9a\u90e8\u7f72\u573a\u666f\u7684\u6570\u636e\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\uff08DS3\uff09\uff0c\u5e76\u5f15\u5165\u4e86DataS^3\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5b9a\u5236\u5316\u6570\u636e\u7b5b\u9009\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u9700\u8981\u5728\u7279\u5b9a\u90e8\u7f72\u573a\u666f\uff08\u5982\u7279\u5b9a\u533b\u9662\u6216\u56fd\u5bb6\u516c\u56ed\uff09\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8fd9\u4e9b\u573a\u666f\u7684\u6570\u636e\u5206\u5e03\u5f80\u5f80\u4e0d\u5e73\u8861\u4e14\u72ec\u7279\u3002\u4f20\u7edf\u8bad\u7ec3\u6570\u636e\u4e0e\u90e8\u7f72\u6570\u636e\u5206\u5e03\u7684\u4e0d\u5339\u914d\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u9009\u62e9\u9002\u5408\u90e8\u7f72\u7684\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u5b50\u96c6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86DS3\u95ee\u9898\uff0c\u5373\u4ece\u901a\u7528\u5206\u5e03\u7684\u8bad\u7ec3\u6570\u636e\u4e2d\u9009\u62e9\u9002\u5408\u7279\u5b9a\u90e8\u7f72\u5206\u5e03\u7684\u5b50\u96c6\u3002\u4f5c\u8005\u5f15\u5165\u4e86DataS^3\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u7b97\u6cd5\uff08\u5982\u6838\u5fc3\u96c6\u3001\u6570\u636e\u8fc7\u6ee4\u548c\u6570\u636e\u7b5b\u9009\uff09\u5728DS3\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u7528\u5206\u5e03\u65b9\u6cd5\u5728\u90e8\u7f72\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u624b\u52a8\u7b5b\u9009\u7684\u4e13\u5bb6\u5b50\u96c6\u5728\u51c6\u786e\u6027\u4e0a\u6700\u9ad8\u53ef\u63d0\u534751.3%\u3002", "conclusion": "\u5b9a\u5236\u5316\u7684\u6570\u636e\u7b5b\u9009\u5bf9\u63d0\u5347\u90e8\u7f72\u7279\u5b9a\u4efb\u52a1\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u968f\u7740\u5168\u7403\u516c\u5171\u6570\u636e\u96c6\u7684\u666e\u53ca\uff0c\u8fd9\u4e00\u95ee\u9898\u5c06\u66f4\u52a0\u91cd\u8981\u3002"}}
{"id": "2504.16761", "pdf": "https://arxiv.org/pdf/2504.16761", "abs": "https://arxiv.org/abs/2504.16761", "authors": ["Lakshita Agarwal", "Bindu Verma"], "title": "Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Image description generation is essential for accessibility and AI\nunderstanding of visual content. Recent advancements in deep learning have\nsignificantly improved natural language processing and computer vision. In this\nwork, we propose Tri-FusionNet, a novel image description generation model that\nintegrates transformer modules: a Vision Transformer (ViT) encoder module with\ndual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder\nmodule, and a Contrastive Language-Image Pre-Training (CLIP) integrating\nmodule. The ViT encoder, enhanced with dual attention, focuses on relevant\nspatial regions and linguistic context, improving image feature extraction. The\nRoBERTa decoder is employed to generate precise textual descriptions. CLIP's\nintegrating module aligns visual and textual data through contrastive learning,\nensuring effective combination of both modalities. This fusion of ViT, RoBERTa,\nand CLIP, along with dual attention, enables the model to produce more\naccurate, contextually rich, and flexible descriptions. The proposed framework\ndemonstrated competitive performance on the Flickr30k and Flickr8k datasets,\nwith BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores\nof 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of\n0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores\nof 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results\ndemonstrate the effectiveness of Tri-FusionNet in generating high-quality image\ndescriptions.", "AI": {"tldr": "Tri-FusionNet\u662f\u4e00\u4e2a\u7ed3\u5408ViT\u3001RoBERTa\u548cCLIP\u7684\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63d0\u5347\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u4f18\u52bf\u3002", "method": "\u6574\u5408ViT\u7f16\u7801\u5668\uff08\u53cc\u6ce8\u610f\u529b\uff09\u3001RoBERTa\u89e3\u7801\u5668\u548cCLIP\u6a21\u5757\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u89c6\u89c9\u4e0e\u6587\u672c\u6570\u636e\u3002", "result": "\u5728Flickr30k\u3001Flickr8k\u548cMS-COCO\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9ad8BLEU\u3001CIDEr\u3001METEOR\u548cROUGE-L\u5206\u6570\u3002", "conclusion": "Tri-FusionNet\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u63cf\u8ff0\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.16316", "pdf": "https://arxiv.org/pdf/2504.16316", "abs": "https://arxiv.org/abs/2504.16316", "authors": ["Hossein Shokouhinejad", "Griffin Higgins", "Roozbeh Razavi-Far", "Hesamodin Mohammadian", "Ali A. Ghorbani"], "title": "On the Consistency of GNN Explanations for Malware Detection", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Control Flow Graphs (CFGs) are critical for analyzing program execution and\ncharacterizing malware behavior. With the growing adoption of Graph Neural\nNetworks (GNNs), CFG-based representations have proven highly effective for\nmalware detection. This study proposes a novel framework that dynamically\nconstructs CFGs and embeds node features using a hybrid approach combining\nrule-based encoding and autoencoder-based embedding. A GNN-based classifier is\nthen constructed to detect malicious behavior from the resulting graph\nrepresentations. To improve model interpretability, we apply state-of-the-art\nexplainability techniques, including GNNExplainer, PGExplainer, and\nCaptumExplainer, the latter is utilized three attribution methods: Integrated\nGradients, Guided Backpropagation, and Saliency. In addition, we introduce a\nnovel aggregation method, called RankFusion, that integrates the outputs of the\ntop-performing explainers to enhance the explanation quality. We also evaluate\nexplanations using two subgraph extraction strategies, including the proposed\nGreedy Edge-wise Composition (GEC) method for improved structural coherence. A\ncomprehensive evaluation using accuracy, fidelity, and consistency metrics\ndemonstrates the effectiveness of the proposed framework in terms of accurate\nidentification of malware samples and generating reliable and interpretable\nexplanations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6784\u5efa\u63a7\u5236\u6d41\u56fe\uff08CFG\uff09\u5e76\u5d4c\u5165\u8282\u70b9\u7279\u5f81\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u7f16\u7801\u548c\u81ea\u52a8\u7f16\u7801\u5668\u5d4c\u5165\uff0c\u5229\u7528GNN\u5206\u7c7b\u5668\u68c0\u6d4b\u6076\u610f\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u89e3\u91ca\u6027\u6280\u672f\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0cCFG\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u5f97\u5230\u9a8c\u8bc1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u52a8\u6001\u6784\u5efaCFG\u5e76\u5d4c\u5165\u8282\u70b9\u7279\u5f81\uff0c\u7ed3\u5408\u89c4\u5219\u7f16\u7801\u548c\u81ea\u52a8\u7f16\u7801\u5668\u5d4c\u5165\uff0c\u4f7f\u7528GNN\u5206\u7c7b\u5668\u68c0\u6d4b\u6076\u610f\u884c\u4e3a\uff0c\u5e76\u5e94\u7528\u591a\u79cd\u89e3\u91ca\u6027\u6280\u672f\uff08\u5982GNNExplainer\u3001PGExplainer\u7b49\uff09\u548c\u65b0\u578b\u805a\u5408\u65b9\u6cd5RankFusion\u3002", "result": "\u901a\u8fc7\u51c6\u786e\u6027\u3001\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u6307\u6807\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u6076\u610f\u8f6f\u4ef6\u6837\u672c\u8bc6\u522b\u548c\u751f\u6210\u53ef\u9760\u89e3\u91ca\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u589e\u5f3a\u7684\u89e3\u91ca\u6027\u6280\u672f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u7ed3\u679c\u3002"}}
{"id": "2504.16788", "pdf": "https://arxiv.org/pdf/2504.16788", "abs": "https://arxiv.org/abs/2504.16788", "authors": ["Lakshita Agarwal", "Bindu Verma"], "title": "Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding and analyzing video actions are essential for producing\ninsightful and contextualized descriptions, especially for video-based\napplications like intelligent monitoring and autonomous systems. The proposed\nwork introduces a novel framework for generating natural language descriptions\nfrom video datasets by combining textual and visual modalities. The suggested\narchitecture makes use of ResNet50 to extract visual features from video frames\nthat are taken from the Microsoft Research Video Description Corpus (MSVD), and\nBerkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual\ncharacteristics are converted into patch embeddings and then run through an\nencoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In\norder to align textual and visual representations and guarantee high-quality\ndescription production, the system uses multi-head self-attention and\ncross-attention techniques. The model's efficacy is demonstrated by performance\nevaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested\nframework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)\nand 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores\nof 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and\n0.795 (MSVD). By producing human-like, contextually relevant descriptions,\nstrengthening interpretability, and improving real-world applications, this\nresearch advances explainable AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u89c6\u9891\u6570\u636e\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89c6\u9891\u52a8\u4f5c\u7684\u7406\u89e3\u4e0e\u5206\u6790\u5bf9\u667a\u80fd\u76d1\u63a7\u548c\u81ea\u4e3b\u7cfb\u7edf\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u63cf\u8ff0\u3002", "method": "\u4f7f\u7528ResNet50\u63d0\u53d6\u89c6\u9891\u5e27\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eGPT-2\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u751f\u6210\u63cf\u8ff0\uff0c\u7ed3\u5408\u591a\u5934\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6280\u672f\u3002", "result": "\u5728BDD-X\u548cMSVD\u6570\u636e\u96c6\u4e0a\uff0cBLEU-4\u3001CIDEr\u3001METEOR\u548cROUGE-L\u5206\u6570\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u63cf\u8ff0\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91caAI\u7684\u53d1\u5c55\uff0c\u5e76\u589e\u5f3a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.16343", "pdf": "https://arxiv.org/pdf/2504.16343", "abs": "https://arxiv.org/abs/2504.16343", "authors": ["Chad Marshall", "Andrew Barovic", "Armin Moin"], "title": "Mining Software Repositories for Expert Recommendation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "We propose an automated approach to bug assignment to developers in large\nopen-source software projects. This way, we assist human bug triagers who are\nin charge of finding the best developer with the right level of expertise in a\nparticular area to be assigned to a newly reported issue. Our approach is based\non the history of software development as documented in the issue tracking\nsystems. We deploy BERTopic and techniques from TopicMiner. Our approach works\nbased on the bug reports' features, such as the corresponding products and\ncomponents, as well as their priority and severity levels. We sort developers\nbased on their experience with specific combinations of new reports. The\nevaluation is performed using Top-k accuracy, and the results are compared with\nthe reported results in prior work, namely TopicMiner MTM, BUGZIE, Bug triaging\nvia deep Reinforcement Learning BT-RL, and LDA-SVM. The evaluation data come\nfrom various Eclipse and Mozilla projects, such as JDT, Firefox, and\nThunderbird.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERTopic\u548cTopicMiner\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u5f00\u6e90\u9879\u76ee\u4e2d\u7684bug\u5206\u914d\u7ed9\u5408\u9002\u7684\u5f00\u53d1\u8005\uff0c\u901a\u8fc7\u5386\u53f2\u6570\u636e\u548cbug\u62a5\u544a\u7279\u5f81\u4f18\u5316\u5206\u914d\u6548\u679c\u3002", "motivation": "\u5e2e\u52a9\u4eba\u5de5bug\u5206\u7c7b\u5458\u66f4\u9ad8\u6548\u5730\u4e3a\u65b0\u62a5\u544a\u7684bug\u627e\u5230\u5177\u6709\u76f8\u5173\u4e13\u4e1a\u77e5\u8bc6\u7684\u5f00\u53d1\u8005\u3002", "method": "\u5229\u7528BERTopic\u548cTopicMiner\u6280\u672f\uff0c\u7ed3\u5408bug\u62a5\u544a\u7684\u4ea7\u54c1\u3001\u7ec4\u4ef6\u3001\u4f18\u5148\u7ea7\u548c\u4e25\u91cd\u6027\u7b49\u7279\u5f81\uff0c\u6839\u636e\u5f00\u53d1\u8005\u7684\u5386\u53f2\u7ecf\u9a8c\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u5728Eclipse\u548cMozilla\u9879\u76ee\uff08\u5982JDT\u3001Firefox\u3001Thunderbird\uff09\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528Top-k\u51c6\u786e\u7387\uff0c\u5e76\u4e0eTopicMiner MTM\u3001BUGZIE\u3001BT-RL\u548cLDA-SVM\u7b49\u65b9\u6cd5\u5bf9\u6bd4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5386\u53f2\u6570\u636e\u548c\u7279\u5f81\u5206\u6790\uff0c\u6709\u6548\u63d0\u5347\u4e86bug\u5206\u914d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.16801", "pdf": "https://arxiv.org/pdf/2504.16801", "abs": "https://arxiv.org/abs/2504.16801", "authors": ["Xiaoxing Hu", "Kaicheng Yang", "Jun Wang", "Haoran Xu", "Ziyong Feng", "Yupei Wang"], "title": "Decoupled Global-Local Alignment for Improving Compositional Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA", "AI": {"tldr": "DeGLA\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u5168\u5c40-\u5c40\u90e8\u5bf9\u9f50\u548c\u81ea\u84b8\u998f\u673a\u5236\uff0c\u63d0\u5347CLIP\u7684\u7ec4\u5408\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u901a\u7528\u80fd\u529b\u3002", "motivation": "CLIP\u7684\u5168\u5c40\u5bf9\u6bd4\u5b66\u4e60\u9650\u5236\u4e86\u5176\u5bf9\u7ec4\u5408\u6982\u5ff5\uff08\u5982\u5173\u7cfb\u548c\u5c5e\u6027\uff09\u7684\u7406\u89e3\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u5c1d\u8bd5\u6539\u8fdb\u4f46\u727a\u7272\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "method": "\u63d0\u51faDeGLA\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u84b8\u998f\u673a\u5236\u548c\u57fa\u4e8eLLM\u6784\u5efa\u7684\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\uff0c\u8bbe\u8ba1\u4e86IGC\u548cTGC\u635f\u5931\u51fd\u6570\u4ee5\u589e\u5f3a\u7ec4\u5408\u7406\u89e3\u3002", "result": "\u5728VALSE\u3001SugarCrepe\u548cARO\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53473.5%\uff0c\u572811\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u534713.0%\u3002", "conclusion": "DeGLA\u5728\u63d0\u5347\u7ec4\u5408\u7406\u89e3\u7684\u540c\u65f6\uff0c\u6709\u6548\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.16350", "pdf": "https://arxiv.org/pdf/2504.16350", "abs": "https://arxiv.org/abs/2504.16350", "authors": ["Ilya Tyagin", "Marwa H. Farag", "Kyle Sherbert", "Karunya Shirali", "Yuri Alexeev", "Ilya Safro"], "title": "QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Quantum computing has the potential to improve our ability to solve certain\noptimization problems that are computationally difficult for classical\ncomputers, by offering new algorithmic approaches that may provide speedups\nunder specific conditions. In this work, we introduce QAOA-GPT, a generative\nframework that leverages Generative Pretrained Transformers (GPT) to directly\nsynthesize quantum circuits for solving quadratic unconstrained binary\noptimization problems, and demonstrate it on the MaxCut problem on graphs. To\ndiversify the training circuits and ensure their quality, we have generated a\nsynthetic dataset using the adaptive QAOA approach, a method that incrementally\nbuilds and optimizes problem-specific circuits. The experiments conducted on a\ncurated set of graph instances demonstrate that QAOA-GPT, generates high\nquality quantum circuits for new problem instances unseen in the training as\nwell as successfully parametrizes QAOA. Our results show that using QAOA-GPT to\ngenerate quantum circuits will significantly decrease both the computational\noverhead of classical QAOA and adaptive approaches that often use gradient\nevaluation to generate the circuit and the classical optimization of the\ncircuit parameters. Our work shows that generative AI could be a promising\navenue to generate compact quantum circuits in a scalable way.", "AI": {"tldr": "QAOA-GPT\u662f\u4e00\u4e2a\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528GPT\u76f4\u63a5\u5408\u6210\u91cf\u5b50\u7535\u8def\u6765\u89e3\u51b3\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728MaxCut\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002\u901a\u8fc7\u81ea\u9002\u5e94QAOA\u65b9\u6cd5\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660eQAOA-GPT\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u91cf\u5b50\u7535\u8def\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u5728\u89e3\u51b3\u7ecf\u5178\u8ba1\u7b97\u673a\u96be\u4ee5\u5904\u7406\u7684\u4f18\u5316\u95ee\u9898\u4e0a\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982QAOA\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u6210\u5f0fAI\uff08\u5982GPT\uff09\u6765\u9ad8\u6548\u751f\u6210\u91cf\u5b50\u7535\u8def\u3002", "method": "\u63d0\u51faQAOA-GPT\u6846\u67b6\uff0c\u5229\u7528GPT\u76f4\u63a5\u5408\u6210\u91cf\u5b50\u7535\u8def\u3002\u901a\u8fc7\u81ea\u9002\u5e94QAOA\u65b9\u6cd5\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\uff0c\u5e76\u5728MaxCut\u95ee\u9898\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eQAOA-GPT\u80fd\u4e3a\u672a\u89c1\u8fc7\u7684\u5b9e\u4f8b\u751f\u6210\u9ad8\u8d28\u91cf\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u6210\u529f\u53c2\u6570\u5316QAOA\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u662f\u751f\u6210\u7d27\u51d1\u91cf\u5b50\u7535\u8def\u7684\u53ef\u6269\u5c55\u9014\u5f84\uff0cQAOA-GPT\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2504.16840", "pdf": "https://arxiv.org/pdf/2504.16840", "abs": "https://arxiv.org/abs/2504.16840", "authors": ["Joe Hrzich", "Michael A. Beck", "Christopher P. Bidinosti", "Christopher J. Henry", "Kalhari Manawasinghe", "Karen Tanino"], "title": "A Low-Cost Photogrammetry System for 3D Plant Modeling and Phenotyping", "categories": ["cs.CV"], "comment": null, "summary": "We present an open-source, low-cost photogrammetry system for 3D plant\nmodeling and phenotyping. The system uses a structure-from-motion approach to\nreconstruct 3D representations of the plants via point clouds. Using wheat as\nan example, we demonstrate how various phenotypic traits can be computed easily\nfrom the point clouds. These include standard measurements such as plant height\nand radius, as well as features that would be more cumbersome to measure by\nhand, such as leaf angles and convex hull. We further demonstrate the utility\nof the system through the investigation of specific metrics that may yield\nobjective classifications of erectophile versus planophile wheat canopy\narchitectures.", "AI": {"tldr": "\u5f00\u6e90\u4f4e\u6210\u672c\u7684\u5149\u5ea6\u6d4b\u91cf\u7cfb\u7edf\uff0c\u7528\u4e8e3D\u690d\u7269\u5efa\u6a21\u548c\u8868\u578b\u5206\u6790\uff0c\u901a\u8fc7\u70b9\u4e91\u91cd\u5efa\u690d\u7269\u6a21\u578b\u5e76\u8ba1\u7b97\u8868\u578b\u7279\u5f81\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u5f00\u6e90\u7684\u7cfb\u7edf\uff0c\u4ee5\u7b80\u5316\u690d\u7269\u8868\u578b\u5206\u6790\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5c0f\u9ea6\u7b49\u4f5c\u7269\u76843D\u5efa\u6a21\u548c\u7279\u5f81\u63d0\u53d6\u3002", "method": "\u91c7\u7528\u8fd0\u52a8\u7ed3\u6784\uff08SfM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u4e91\u91cd\u5efa\u690d\u7269\u76843D\u6a21\u578b\uff0c\u5e76\u4ece\u4e2d\u8ba1\u7b97\u591a\u79cd\u8868\u578b\u7279\u5f81\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u6d4b\u91cf\u690d\u7269\u9ad8\u5ea6\u3001\u534a\u5f84\u3001\u53f6\u7247\u89d2\u5ea6\u7b49\u7279\u5f81\uff0c\u5e76\u7528\u4e8e\u5c0f\u9ea6\u51a0\u5c42\u7ed3\u6784\u7684\u5206\u7c7b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u690d\u7269\u8868\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u9ea6\u51a0\u5c42\u7ed3\u6784\u7684\u7814\u7a76\u3002"}}
{"id": "2504.16352", "pdf": "https://arxiv.org/pdf/2504.16352", "abs": "https://arxiv.org/abs/2504.16352", "authors": ["Jiwan Kim", "Hongseok Kang", "Sein Kim", "Kibum Kim", "Chanyoung Park"], "title": "Disentangling and Generating Modalities for Recommendation in Missing Modality Scenarios", "categories": ["cs.IR", "cs.AI"], "comment": "SIGIR 2025", "summary": "Multi-modal recommender systems (MRSs) have achieved notable success in\nimproving personalization by leveraging diverse modalities such as images,\ntext, and audio. However, two key challenges remain insufficiently addressed:\n(1) Insufficient consideration of missing modality scenarios and (2) the\noverlooking of unique characteristics of modality features. These challenges\nresult in significant performance degradation in realistic situations where\nmodalities are missing. To address these issues, we propose Disentangling and\nGenerating Modality Recommender (DGMRec), a novel framework tailored for\nmissing modality scenarios. DGMRec disentangles modality features into general\nand specific modality features from an information-based perspective, enabling\nricher representations for recommendation. Building on this, it generates\nmissing modality features by integrating aligned features from other modalities\nand leveraging user modality preferences. Extensive experiments show that\nDGMRec consistently outperforms state-of-the-art MRSs in challenging scenarios,\nincluding missing modalities and new item settings as well as diverse missing\nratios and varying levels of missing modalities. Moreover, DGMRec's\ngeneration-based approach enables cross-modal retrieval, a task inapplicable\nfor existing MRSs, highlighting its adaptability and potential for real-world\napplications. Our code is available at https://github.com/ptkjw1997/DGMRec.", "AI": {"tldr": "DGMRec\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u7f3a\u5931\u6a21\u6001\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u548c\u751f\u6210\u6a21\u6001\u7279\u5f81\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u7f3a\u5931\u6a21\u6001\u95ee\u9898\u548c\u6a21\u6001\u7279\u5f81\u72ec\u7279\u6027\u672a\u88ab\u5145\u5206\u8003\u8651\u7684\u95ee\u9898\u3002", "method": "DGMRec\u5c06\u6a21\u6001\u7279\u5f81\u89e3\u8026\u4e3a\u901a\u7528\u548c\u7279\u5b9a\u6a21\u6001\u7279\u5f81\uff0c\u5e76\u751f\u6210\u7f3a\u5931\u6a21\u6001\u7279\u5f81\u3002", "result": "DGMRec\u5728\u7f3a\u5931\u6a21\u6001\u548c\u65b0\u7269\u54c1\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u8de8\u6a21\u6001\u68c0\u7d22\u3002", "conclusion": "DGMRec\u5c55\u793a\u4e86\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.16851", "pdf": "https://arxiv.org/pdf/2504.16851", "abs": "https://arxiv.org/abs/2504.16851", "authors": ["Ruben Gonzalez Avil\u00e9s", "Linus Scheibenreif", "Nassim Ait Ali Braham", "Benedikt Blumenstiel", "Thomas Brunschwiler", "Ranjini Guruprasad", "Damian Borth", "Conrad Albrecht", "Paolo Fraccaro", "Devyani Lambhate", "Johannes Jakubik"], "title": "Hyperspectral Vision Transformers for Greenhouse Gas Estimations from Space", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral imaging provides detailed spectral information and holds\nsignificant potential for monitoring of greenhouse gases (GHGs). However, its\napplication is constrained by limited spatial coverage and infrequent revisit\ntimes. In contrast, multispectral imaging offers broader spatial and temporal\ncoverage but often lacks the spectral detail that can enhance GHG detection. To\naddress these challenges, this study proposes a spectral transformer model that\nsynthesizes hyperspectral data from multispectral inputs. The model is\npre-trained via a band-wise masked autoencoder and subsequently fine-tuned on\nspatio-temporally aligned multispectral-hyperspectral image pairs. The\nresulting synthetic hyperspectral data retain the spatial and temporal benefits\nof multispectral imagery and improve GHG prediction accuracy relative to using\nmultispectral data alone. This approach effectively bridges the trade-off\nbetween spectral resolution and coverage, highlighting its potential to advance\natmospheric monitoring by combining the strengths of hyperspectral and\nmultispectral systems with self-supervised deep learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5149\u8c31\u53d8\u6362\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5149\u8c31\u6570\u636e\u5408\u6210\u9ad8\u5149\u8c31\u6570\u636e\uff0c\u4ee5\u5f25\u8865\u9ad8\u5149\u8c31\u8986\u76d6\u8303\u56f4\u6709\u9650\u548c\u591a\u5149\u8c31\u5149\u8c31\u7ec6\u8282\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u6e29\u5ba4\u6c14\u4f53\u76d1\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9ad8\u5149\u8c31\u6210\u50cf\u5728\u6e29\u5ba4\u6c14\u4f53\u76d1\u6d4b\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u7a7a\u95f4\u8986\u76d6\u548c\u91cd\u8bbf\u9891\u7387\uff1b\u591a\u5149\u8c31\u6210\u50cf\u8986\u76d6\u8303\u56f4\u5e7f\u4f46\u5149\u8c31\u7ec6\u8282\u4e0d\u8db3\u3002\u7814\u7a76\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u5149\u8c31\u53d8\u6362\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6ce2\u6bb5\u63a9\u7801\u81ea\u7f16\u7801\u5668\u751f\u6210\u9ad8\u5149\u8c31\u6570\u636e\uff0c\u5e76\u5728\u65f6\u7a7a\u5bf9\u9f50\u7684\u591a\u5149\u8c31-\u9ad8\u5149\u8c31\u56fe\u50cf\u5bf9\u4e0a\u5fae\u8c03\u3002", "result": "\u5408\u6210\u7684\u6570\u636e\u4fdd\u7559\u4e86\u591a\u5149\u8c31\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4f18\u52bf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6e29\u5ba4\u6c14\u4f53\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u5149\u8c31\u5206\u8fa8\u7387\u548c\u8986\u76d6\u8303\u56f4\uff0c\u4e3a\u7ed3\u5408\u9ad8\u5149\u8c31\u548c\u591a\u5149\u8c31\u7cfb\u7edf\u7684\u4f18\u52bf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2504.16353", "pdf": "https://arxiv.org/pdf/2504.16353", "abs": "https://arxiv.org/abs/2504.16353", "authors": ["Arpana Hosabettu", "Harsh Shah"], "title": "Transformer-Based Extraction of Statutory Definitions from the U.S. Code", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, to be published in IEEE AIIoT 2025", "summary": "Automatic extraction of definitions from legal texts is critical for\nenhancing the comprehension and clarity of complex legal corpora such as the\nUnited States Code (U.S.C.). We present an advanced NLP system leveraging\ntransformer-based architectures to automatically extract defined terms, their\ndefinitions, and their scope from the U.S.C. We address the challenges of\nautomatically identifying legal definitions, extracting defined terms, and\ndetermining their scope within this complex corpus of over 200,000 pages of\nfederal statutory law. Building upon previous feature-based machine learning\nmethods, our updated model employs domain-specific transformers (Legal-BERT)\nfine-tuned specifically for statutory texts, significantly improving extraction\naccuracy. Our work implements a multi-stage pipeline that combines document\nstructure analysis with state-of-the-art language models to process legal text\nfrom the XML version of the U.S. Code. Each paragraph is first classified using\na fine-tuned legal domain BERT model to determine if it contains a definition.\nOur system then aggregates related paragraphs into coherent definitional units\nand applies a combination of attention mechanisms and rule-based patterns to\nextract defined terms and their jurisdictional scope. The definition extraction\nsystem is evaluated on multiple titles of the U.S. Code containing thousands of\ndefinitions, demonstrating significant improvements over previous approaches.\nOur best model achieves 96.8% precision and 98.9% recall (98.2% F1-score),\nsubstantially outperforming traditional machine learning classifiers. This work\ncontributes to improving accessibility and understanding of legal information\nwhile establishing a foundation for downstream legal reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u67b6\u6784\u7684NLP\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u7f8e\u56fd\u6cd5\u5178\u4e2d\u81ea\u52a8\u63d0\u53d6\u6cd5\u5f8b\u5b9a\u4e49\u3001\u672f\u8bed\u53ca\u5176\u8303\u56f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63d0\u53d6\u51c6\u786e\u6027\u3002", "motivation": "\u63d0\u5347\u5bf9\u590d\u6742\u6cd5\u5f8b\u6587\u672c\uff08\u5982\u7f8e\u56fd\u6cd5\u5178\uff09\u7684\u7406\u89e3\u548c\u6e05\u6670\u5ea6\uff0c\u89e3\u51b3\u81ea\u52a8\u8bc6\u522b\u6cd5\u5f8b\u5b9a\u4e49\u3001\u63d0\u53d6\u672f\u8bed\u53ca\u5176\u8303\u56f4\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u9886\u57df\u7279\u5b9a\u7684Transformer\u6a21\u578b\uff08Legal-BERT\uff09\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u6587\u6863\u7ed3\u6784\u5206\u6790\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u7c7b\u6bb5\u843d\u5e76\u63d0\u53d6\u5b9a\u4e49\u3002", "result": "\u5728\u591a\u4e2a\u7f8e\u56fd\u6cd5\u5178\u6807\u9898\u4e0a\u8bc4\u4f30\uff0c\u6700\u4f73\u6a21\u578b\u8fbe\u523096.8%\u7684\u7cbe\u786e\u7387\u548c98.9%\u7684\u53ec\u56de\u7387\uff08F1\u5206\u657098.2%\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u6cd5\u5f8b\u4fe1\u606f\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u7406\u89e3\uff0c\u4e3a\u4e0b\u6e38\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.16870", "pdf": "https://arxiv.org/pdf/2504.16870", "abs": "https://arxiv.org/abs/2504.16870", "authors": ["Chenxi Duan"], "title": "High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR and Contaminated Optical Data", "categories": ["cs.CV"], "comment": null, "summary": "Addressing gaps caused by cloud cover and the long revisit cycle of\nsatellites is vital for providing essential data to support remote sensing\napplications. This paper tackles the challenges of missing optical data\nsynthesis, particularly in complex scenarios with cloud cover. We propose\nCRSynthNet, a novel image synthesis network that incorporates innovative\ndesigned modules such as the DownUp Block and Fusion Attention to enhance\naccuracy. Experimental results validate the effectiveness of CRSynthNet,\ndemonstrating substantial improvements in restoring structural details,\npreserving spectral consist, and achieving superior visual effects that far\nexceed those produced by comparison methods. It achieves quantitative\nimprovements across multiple metrics: a peak signal-to-noise ratio (PSNR) of\n26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean\nsquare error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12\ndataset, a valuable resource specifically designed to address cloud cover\nchallenges in missing optical data synthesis study. The dataset uniquely\nincludes cloud-covered images and leverages earlier image to predict later\nimage, offering a realistic representation of real-world scenarios. This study\noffer practical method and valuable resources for optical satellite image\nsynthesis task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCRSynthNet\u7f51\u7edc\uff0c\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u89e3\u51b3\u4e91\u8986\u76d6\u5bfc\u81f4\u7684\u5149\u5b66\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\uff0c\u5e76\u521b\u5efaTCSEN12\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u536b\u661f\u56fe\u50cf\u4e2d\u4e91\u8986\u76d6\u548c\u957f\u91cd\u8bbf\u5468\u671f\u5bfc\u81f4\u7684\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u4e3a\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u63d0\u51faCRSynthNet\u7f51\u7edc\uff0c\u5305\u542bDownUp Block\u548cFusion Attention\u6a21\u5757\uff0c\u63d0\u5347\u56fe\u50cf\u5408\u6210\u7cbe\u5ea6\u3002", "result": "CRSynthNet\u5728PSNR\u3001SSIM\u548cRMSE\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5149\u5b66\u536b\u661f\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u548cTCSEN12\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u95ee\u9898\u3002"}}
{"id": "2504.16357", "pdf": "https://arxiv.org/pdf/2504.16357", "abs": "https://arxiv.org/abs/2504.16357", "authors": ["Ying Chang", "Xiaohu Shi", "Xiaohui Zhao", "Zhaohuang Chen", "Deyin Ma"], "title": "DP2FL: Dual Prompt Personalized Federated Learning in Foundation Models", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Personalized federated learning (PFL) has garnered significant attention for\nits ability to address heterogeneous client data distributions while preserving\ndata privacy. However, when local client data is limited, deep learning models\noften suffer from insufficient training, leading to suboptimal performance.\nFoundation models, such as CLIP (Contrastive Language-Image Pretraining),\nexhibit strong feature extraction capabilities and can alleviate this issue by\nfine-tuning on limited local data. Despite their potential, foundation models\nare rarely utilized in federated learning scenarios, and challenges related to\nintegrating new clients remain largely unresolved. To address these challenges,\nwe propose the Dual Prompt Personalized Federated Learning (DP2FL) framework,\nwhich introduces dual prompts and an adaptive aggregation strategy. DP2FL\ncombines global task awareness with local data-driven insights, enabling local\nmodels to achieve effective generalization while remaining adaptable to\nspecific data distributions. Moreover, DP2FL introduces a global model that\nenables prediction on new data sources and seamlessly integrates newly added\nclients without requiring retraining. Experimental results in highly\nheterogeneous environments validate the effectiveness of DP2FL's prompt design\nand aggregation strategy, underscoring the advantages of prediction on novel\ndata sources and demonstrating the seamless integration of new clients into the\nfederated learning framework.", "AI": {"tldr": "DP2FL\u6846\u67b6\u901a\u8fc7\u53cc\u63d0\u793a\u548c\u81ea\u9002\u5e94\u805a\u5408\u7b56\u7565\u89e3\u51b3\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u4e0d\u8db3\u548c\u65b0\u5ba2\u6237\u7aef\u96c6\u6210\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u672c\u5730\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u7684\u6a21\u578b\u8bad\u7ec3\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51faDP2FL\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u63d0\u793a\u548c\u81ea\u9002\u5e94\u805a\u5408\u7b56\u7565\uff0c\u5e73\u8861\u5168\u5c40\u4efb\u52a1\u610f\u8bc6\u548c\u672c\u5730\u6570\u636e\u9a71\u52a8\uff0c\u652f\u6301\u65b0\u5ba2\u6237\u7aef\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DP2FL\u5728\u5f02\u6784\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65b0\u6570\u636e\u9884\u6d4b\u548c\u65b0\u5ba2\u6237\u7aef\u96c6\u6210\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "DP2FL\u901a\u8fc7\u521b\u65b0\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u805a\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u6570\u636e\u4e0d\u8db3\u548c\u65b0\u5ba2\u6237\u7aef\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.16907", "pdf": "https://arxiv.org/pdf/2504.16907", "abs": "https://arxiv.org/abs/2504.16907", "authors": ["Ruotong Wang", "Mingli Zhu", "Jiarong Ou", "Rui Chen", "Xin Tao", "Pengfei Wan", "Baoyuan Wu"], "title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-video (T2V) generative models have rapidly advanced and found\nwidespread applications across fields like entertainment, education, and\nmarketing. However, the adversarial vulnerabilities of these models remain\nrarely explored. We observe that in T2V generation tasks, the generated videos\noften contain substantial redundant information not explicitly specified in the\ntext prompts, such as environmental elements, secondary objects, and additional\ndetails, providing opportunities for malicious attackers to embed hidden\nharmful content. Exploiting this inherent redundancy, we introduce BadVideo,\nthe first backdoor attack framework tailored for T2V generation. Our attack\nfocuses on designing target adversarial outputs through two key strategies: (1)\nSpatio-Temporal Composition, which combines different spatiotemporal features\nto encode malicious information; (2) Dynamic Element Transformation, which\nintroduces transformations in redundant elements over time to convey malicious\ninformation. Based on these strategies, the attacker's malicious target\nseamlessly integrates with the user's textual instructions, providing high\nstealthiness. Moreover, by exploiting the temporal dimension of videos, our\nattack successfully evades traditional content moderation systems that\nprimarily analyze spatial information within individual frames. Extensive\nexperiments demonstrate that BadVideo achieves high attack success rates while\npreserving original semantics and maintaining excellent performance on clean\ninputs. Overall, our work reveals the adversarial vulnerability of T2V models,\ncalling attention to potential risks and misuse. Our project page is at\nhttps://wrt2000.github.io/BadVideo2025/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u9690\u853d\u540e\u95e8\u653b\u51fb\u6846\u67b6BadVideo\uff0c\u5229\u7528\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u5d4c\u5165\u6076\u610f\u5185\u5bb9\u3002", "motivation": "\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5e7f\u6cdb\u5e94\u7528\u4e2d\u5b58\u5728\u5bf9\u6297\u6027\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u751f\u6210\u89c6\u9891\u4e2d\u672a\u660e\u786e\u6307\u5b9a\u7684\u5197\u4f59\u4fe1\u606f\u53ef\u80fd\u88ab\u6076\u610f\u5229\u7528\u3002", "method": "\u901a\u8fc7\u65f6\u7a7a\u7ec4\u5408\u548c\u52a8\u6001\u5143\u7d20\u53d8\u6362\u4e24\u79cd\u7b56\u7565\uff0c\u5c06\u6076\u610f\u76ee\u6807\u4e0e\u7528\u6237\u6587\u672c\u6307\u4ee4\u65e0\u7f1d\u7ed3\u5408\uff0c\u540c\u65f6\u5229\u7528\u89c6\u9891\u7684\u65f6\u95f4\u7ef4\u5ea6\u89c4\u907f\u4f20\u7edf\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBadVideo\u653b\u51fb\u6210\u529f\u7387\u9ad8\uff0c\u4e14\u80fd\u4fdd\u6301\u539f\u59cb\u8bed\u4e49\u548c\u5e72\u51c0\u8f93\u5165\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u5bf9\u6297\u6027\u6f0f\u6d1e\uff0c\u547c\u5401\u5173\u6ce8\u6f5c\u5728\u98ce\u9669\u548c\u6ee5\u7528\u95ee\u9898\u3002"}}
{"id": "2504.16915", "pdf": "https://arxiv.org/pdf/2504.16915", "abs": "https://arxiv.org/abs/2504.16915", "authors": ["Chong Mou", "Yanze Wu", "Wenxu Wu", "Zinan Guo", "Pengze Zhang", "Yufeng Cheng", "Yiming Luo", "Fei Ding", "Shiwen Zhang", "Xinghui Li", "Mengtian Li", "Songtao Zhao", "Jian Zhang", "Qian He", "Xinglong Wu"], "title": "DreamO: A Unified Framework for Image Customization", "categories": ["cs.CV"], "comment": null, "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.", "AI": {"tldr": "DreamO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u5b9a\u5236\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\u548c\u6761\u4ef6\u96c6\u6210\uff0c\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u5904\u7406\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5b9a\u5236\u65b9\u6cd5\u591a\u4e3a\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u96be\u4ee5\u6574\u5408\u591a\u79cd\u6761\u4ef6\u3002DreamO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7edf\u4e00\u5904\u7406\u8f93\u5165\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u6570\u636e\u96c6\uff0c\u5f15\u5165\u7279\u5f81\u8def\u7531\u7ea6\u675f\u548c\u5360\u4f4d\u7b26\u7b56\u7565\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDreamO\u80fd\u9ad8\u8d28\u91cf\u5b8c\u6210\u591a\u79cd\u56fe\u50cf\u5b9a\u5236\u4efb\u52a1\uff0c\u5e76\u7075\u6d3b\u6574\u5408\u4e0d\u540c\u63a7\u5236\u6761\u4ef6\u3002", "conclusion": "DreamO\u4e3a\u56fe\u50cf\u5b9a\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u6761\u4ef6\u6574\u5408\u7684\u96be\u9898\u3002"}}
{"id": "2504.16378", "pdf": "https://arxiv.org/pdf/2504.16378", "abs": "https://arxiv.org/abs/2504.16378", "authors": ["Tadashi Okoshi", "Zexiong Gao", "Tan Yi Zhen", "Takumi Karasawa", "Takeshi Miki", "Wataru Sasaki", "Rajesh K. Balan"], "title": "Cyberoception: Finding a Painlessly-Measurable New Sense in the Cyberworld Towards Emotion-Awareness in Computing", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by ACM CHI2025", "summary": "In Affective computing, recognizing users' emotions accurately is the basis\nof affective human-computer interaction. Understanding users' interoception\ncontributes to a better understanding of individually different emotional\nabilities, which is essential for achieving inter-individually accurate emotion\nestimation. However, existing interoception measurement methods, such as the\nheart rate discrimination task, have several limitations, including their\ndependence on a well-controlled laboratory environment and precision apparatus,\nmaking monitoring users' interoception challenging. This study aims to\ndetermine other forms of data that can explain users' interoceptive or similar\nstates in their real-world lives and propose a novel hypothetical concept\n\"cyberoception,\" a new sense (1) which has properties similar to interoception\nin terms of the correlation with other emotion-related abilities, and (2) which\ncan be measured only by the sensors embedded inside commodity smartphone\ndevices in users' daily lives. Results from a 10-day-long in-lab/in-the-wild\nhybrid experiment reveal a specific cyberoception type \"Turn On\" (users'\nsubjective sensory perception about the frequency of turning-on behavior on\ntheir smartphones), significantly related to participants' emotional valence.\nWe anticipate that cyberoception to serve as a fundamental building block for\ndeveloping more \"emotion-aware\", user-friendly applications and services.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6982\u5ff5\u201ccyberoception\u201d\uff0c\u901a\u8fc7\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u5728\u7528\u6237\u65e5\u5e38\u751f\u6d3b\u4e2d\u6d4b\u91cf\u7c7b\u4f3c\u5185\u611f\u53d7\u7684\u72b6\u6001\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u60c5\u7eea\u3002", "motivation": "\u73b0\u6709\u5185\u611f\u53d7\u6d4b\u91cf\u65b9\u6cd5\u4f9d\u8d56\u5b9e\u9a8c\u5ba4\u73af\u5883\u548c\u9ad8\u7cbe\u5ea6\u8bbe\u5907\uff0c\u96be\u4ee5\u5728\u73b0\u5b9e\u751f\u6d3b\u4e2d\u76d1\u6d4b\u7528\u6237\u7684\u5185\u611f\u53d7\u72b6\u6001\u3002", "method": "\u63d0\u51fa\u201ccyberoception\u201d\u6982\u5ff5\uff0c\u5e76\u901a\u8fc710\u5929\u7684\u5b9e\u9a8c\u5ba4\u4e0e\u91ce\u5916\u6df7\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4e0e\u60c5\u7eea\u7684\u76f8\u5173\u6027\u3002", "result": "\u53d1\u73b0\u4e00\u79cd\u7279\u5b9a\u7684\u201cTurn On\u201dcyberoception\u7c7b\u578b\u4e0e\u7528\u6237\u60c5\u7eea\u6548\u4ef7\u663e\u8457\u76f8\u5173\u3002", "conclusion": "cyberoception\u53ef\u4f5c\u4e3a\u5f00\u53d1\u66f4\u201c\u60c5\u7eea\u611f\u77e5\u201d\u5e94\u7528\u7684\u57fa\u7840\u3002"}}
{"id": "2504.16922", "pdf": "https://arxiv.org/pdf/2504.16922", "abs": "https://arxiv.org/abs/2504.16922", "authors": ["Ali Hassani", "Fengzhe Zhou", "Aditya Kane", "Jiannan Huang", "Chieh-Yun Chen", "Min Shi", "Steven Walton", "Markus Hoehnerbach", "Vijay Thakkar", "Michael Isaev", "Qinsheng Zhang", "Bing Xu", "Haicheng Wu", "Wen-mei Hwu", "Ming-Yu Liu", "Humphrey Shi"], "title": "Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "https://github.com/SHI-Labs/NATTEN/", "summary": "Many sparse attention mechanisms such as Neighborhood Attention have\ntypically failed to consistently deliver speedup over the self attention\nbaseline. This is largely due to the level of complexity in attention\ninfrastructure, and the rapid evolution of AI hardware architecture. At the\nsame time, many state-of-the-art foundational models, particularly in computer\nvision, are heavily bound by attention, and need reliable sparsity to escape\nthe O(n^2) complexity. In this paper, we study a class of promising sparse\nattention mechanisms that focus on locality, and aim to develop a better\nanalytical model of their performance improvements. We first introduce\nGeneralized Neighborhood Attention (GNA), which can describe sliding window,\nstrided sliding window, and blocked attention. We then consider possible design\nchoices in implementing these approaches, and create a simulator that can\nprovide much more realistic speedup upper bounds for any given setting.\nFinally, we implement GNA on top of a state-of-the-art fused multi-headed\nattention (FMHA) kernel designed for the NVIDIA Blackwell architecture in\nCUTLASS. Our implementation can fully realize the maximum speedup theoretically\npossible in many perfectly block-sparse cases, and achieves an effective\nutilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA\nconfigurations into off-the-shelf generative models, such as Cosmos-7B,\nHunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end\nspeedup on B200 without any fine-tuning. We will open source our simulator and\nBlackwell kernels directly through the NATTEN project.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5c40\u90e8\u6027\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u5e7f\u4e49\u90bb\u57df\u6ce8\u610f\u529b\uff08GNA\uff09\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5668\u548c\u5b9e\u9645\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982\u90bb\u57df\u6ce8\u610f\u529b\uff09\u56e0\u590d\u6742\u6027\u548c\u786c\u4ef6\u67b6\u6784\u5feb\u901f\u53d8\u5316\uff0c\u672a\u80fd\u7a33\u5b9a\u8d85\u8d8a\u81ea\u6ce8\u610f\u529b\u57fa\u51c6\u3002\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u53d7\u6ce8\u610f\u529b\u9650\u5236\uff0c\u9700\u53ef\u9760\u7a00\u758f\u6027\u4ee5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51faGNA\uff0c\u6db5\u76d6\u6ed1\u52a8\u7a97\u53e3\u3001\u8de8\u6b65\u6ed1\u52a8\u7a97\u53e3\u548c\u5206\u5757\u6ce8\u610f\u529b\uff1b\u8bbe\u8ba1\u6a21\u62df\u5668\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u4e0a\u9650\uff1b\u5728NVIDIA Blackwell\u67b6\u6784\u4e0a\u5b9e\u73b0GNA\u3002", "result": "GNA\u5728\u7406\u60f3\u5206\u5757\u7a00\u758f\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7406\u8bba\u6700\u5927\u52a0\u901f\uff0cFP16\u4e0b\u5229\u7528\u7387\u8fbe1.3 petaFLOPs/\u79d2\uff1b\u5728Cosmos-7B\u7b49\u6a21\u578b\u4e2d\u5e26\u676528%-46%\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "GNA\u4e3a\u7a00\u758f\u6ce8\u610f\u529b\u63d0\u4f9b\u9ad8\u6548\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u76f8\u5173\u5de5\u5177\u5c06\u901a\u8fc7NATTEN\u9879\u76ee\u5f00\u6e90\u3002"}}
{"id": "2504.16381", "pdf": "https://arxiv.org/pdf/2504.16381", "abs": "https://arxiv.org/abs/2504.16381", "authors": ["Magnus Petersen", "Roberto Covino"], "title": "PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems", "categories": ["physics.chem-ph", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "Characterizing conformational transitions in physical systems remains a\nfundamental challenge in the computational sciences. Traditional sampling\nmethods like molecular dynamics (MD) or MCMC often struggle with the\nhigh-dimensional nature of molecular systems and the high energy barriers of\ntransitions between stable states. While these transitions are rare events in\nsimulation timescales, they often represent the most biologically significant\nprocesses - for example, the conformational change of an ion channel protein\nfrom its closed to open state, which controls cellular ion flow and is crucial\nfor neural signaling. Such transitions in real systems may take milliseconds to\nseconds but could require months or years of continuous simulation to observe\neven once. We present a method that reformulates transition path generation as\na continuous optimization problem solved through physics-informed neural\nnetworks (PINNs) inspired by string methods for minimum-energy path (MEP)\ngeneration. By representing transition paths as implicit neural functions and\nleveraging automatic differentiation with differentiable molecular dynamics\nforce fields, our method enables the efficient discovery of physically\nrealistic transition pathways without requiring expensive path sampling. We\ndemonstrate our method's effectiveness on two proteins, including an explicitly\nhydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300\natoms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u5206\u5b50\u7cfb\u7edf\u7684\u8fc7\u6e21\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u5728\u9ad8\u7ef4\u7cfb\u7edf\u548c\u80fd\u91cf\u58c1\u5792\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5206\u5b50\u7cfb\u7edf\u4e2d\u7684\u6784\u8c61\u8f6c\u53d8\u662f\u8ba1\u7b97\u79d1\u5b66\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\u6216MCMC\u96be\u4ee5\u9ad8\u6548\u6355\u6349\u8fd9\u4e9b\u7a00\u6709\u4f46\u751f\u7269\u5b66\u610f\u4e49\u91cd\u5927\u7684\u4e8b\u4ef6\u3002", "method": "\u901a\u8fc7\u5c06\u8fc7\u6e21\u8def\u5f84\u751f\u6210\u95ee\u9898\u8f6c\u5316\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u548c\u53ef\u5fae\u5206\u5206\u5b50\u52a8\u529b\u5b66\u529b\u573a\uff0c\u5b9e\u73b0\u65e0\u9700\u6602\u8d35\u8def\u5f84\u91c7\u6837\u7684\u7269\u7406\u771f\u5b9e\u8def\u5f84\u53d1\u73b0\u3002", "result": "\u65b9\u6cd5\u5728\u5305\u62ec8,300\u591a\u4e2a\u539f\u5b50\u7684BPTI\u7cfb\u7edf\u5728\u5185\u7684\u4e24\u79cd\u86cb\u767d\u8d28\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u7814\u7a76\u5206\u5b50\u6784\u8c61\u8f6c\u53d8\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u6f5c\u5728\u751f\u7269\u5b66\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.16930", "pdf": "https://arxiv.org/pdf/2504.16930", "abs": "https://arxiv.org/abs/2504.16930", "authors": ["David Yan", "Alexander Raistrick", "Jia Deng"], "title": "Procedural Dataset Generation for Zero-Shot Stereo Matching", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic datasets are a crucial ingredient for training stereo matching\nnetworks, but the question of what makes a stereo dataset effective remains\nlargely unexplored. We investigate the design space of synthetic datasets by\nvarying the parameters of a procedural dataset generator, and report the\neffects on zero-shot stereo matching performance using standard benchmarks. We\ncollect the best settings to produce Infinigen-Stereo, a procedural generator\nspecifically optimized for zero-shot stereo datasets. Models trained only on\ndata from our system outperform robust baselines trained on a combination of\nexisting synthetic datasets and have stronger zero-shot stereo matching\nperformance than public checkpoints from prior works. We open source our system\nat https://github.com/princeton-vl/InfinigenStereo to enable further research\non procedural stereo datasets.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5408\u6210\u7acb\u4f53\u6570\u636e\u96c6\u7684\u4f18\u5316\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86Infinigen-Stereo\u751f\u6210\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u7acb\u4f53\u5339\u914d\u6027\u80fd\u3002", "motivation": "\u5408\u6210\u6570\u636e\u96c6\u5bf9\u8bad\u7ec3\u7acb\u4f53\u5339\u914d\u7f51\u7edc\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u7acb\u4f53\u6570\u636e\u96c6\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u7a0b\u5e8f\u5316\u6570\u636e\u96c6\u751f\u6210\u5668\u7684\u53c2\u6570\uff0c\u7814\u7a76\u5176\u5bf9\u96f6\u6837\u672c\u7acb\u4f53\u5339\u914d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4f18\u5316\u751f\u6210\u5668\u8bbe\u8ba1\u3002", "result": "Infinigen-Stereo\u751f\u6210\u7684\u8bad\u7ec3\u6570\u636e\u4f7f\u6a21\u578b\u5728\u96f6\u6837\u672c\u7acb\u4f53\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u5f00\u6e90Infinigen-Stereo\u7cfb\u7edf\uff0c\u4e3a\u7a0b\u5e8f\u5316\u7acb\u4f53\u6570\u636e\u96c6\u7814\u7a76\u63d0\u4f9b\u5de5\u5177\u3002"}}
{"id": "2504.16394", "pdf": "https://arxiv.org/pdf/2504.16394", "abs": "https://arxiv.org/abs/2504.16394", "authors": ["Fahmida Liza Piya", "Rahmatollah Beheshti"], "title": "ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConTextual\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fdd\u7559\u7684\u4ee4\u724c\u8fc7\u6ee4\u65b9\u6cd5\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\uff0c\u7528\u4e8e\u63d0\u5347\u4e34\u5e8a\u6587\u672c\u6458\u8981\u7684\u8d28\u91cf\u3002", "motivation": "\u4e34\u5e8a\u975e\u7ed3\u6784\u5316\u6570\u636e\u662f\u4e30\u5bcc\u7684\u4fe1\u606f\u6e90\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff0c\u5f71\u54cd\u4e86\u4e34\u5e8a\u51b3\u7b56\u7684\u53ca\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faConTextual\u6846\u67b6\uff0c\u96c6\u6210\u4e0a\u4e0b\u6587\u4fdd\u7559\u4ee4\u724c\u8fc7\u6ee4\u548c\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u589e\u5f3a\u4e34\u5e8a\u6587\u672c\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConTextual\u5728\u8bed\u8a00\u8fde\u8d2f\u6027\u548c\u4e34\u5e8a\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ConTextual\u5c55\u793a\u4e86\u4ee4\u724c\u7ea7\u8fc7\u6ee4\u548c\u7ed3\u6784\u5316\u68c0\u7d22\u5728\u63d0\u5347\u4e34\u5e8a\u6587\u672c\u751f\u6210\u7cbe\u5ea6\u4e2d\u7684\u4e92\u8865\u4f5c\u7528\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16237", "pdf": "https://arxiv.org/pdf/2504.16237", "abs": "https://arxiv.org/abs/2504.16237", "authors": ["Obed Korshie Dzikunu", "Amirhossein Toosi", "Shadab Ahamed", "Sara Harsini", "Francois Benard", "Xiaoxiao Li", "Arman Rahmim"], "title": "Comprehensive Evaluation of Quantitative Measurements from Automated Deep Segmentations of PSMA PET/CT Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "12 pages, 8 figures", "summary": "This study performs a comprehensive evaluation of quantitative measurements\nas extracted from automated deep-learning-based segmentation methods, beyond\ntraditional Dice Similarity Coefficient assessments, focusing on six\nquantitative metrics, namely SUVmax, SUVmean, total lesion activity (TLA),\ntumor volume (TMTV), lesion count, and lesion spread. We analyzed 380\nprostate-specific membrane antigen (PSMA) targeted [18F]DCFPyL PET/CT scans of\npatients with biochemical recurrence of prostate cancer, training deep neural\nnetworks, U-Net, Attention U-Net and SegResNet with four loss functions: Dice\nLoss, Dice Cross Entropy, Dice Focal Loss, and our proposed L1 weighted Dice\nFocal Loss (L1DFL). Evaluations indicated that Attention U-Net paired with\nL1DFL achieved the strongest correlation with the ground truth (concordance\ncorrelation = 0.90-0.99 for SUVmax and TLA), whereas models employing the Dice\nLoss and the other two compound losses, particularly with SegResNet,\nunderperformed. Equivalence testing (TOST, alpha = 0.05, Delta = 20%) confirmed\nhigh performance for SUV metrics, lesion count and TLA, with L1DFL yielding the\nbest performance. By contrast, tumor volume and lesion spread exhibited greater\nvariability. Bland-Altman, Coverage Probability, and Total Deviation Index\nanalyses further highlighted that our proposed L1DFL minimizes variability in\nquantification of the ground truth clinical measures. The code is publicly\navailable at: https://github.com/ObedDzik/pca\\_segment.git.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u65b9\u6cd5\u8bc4\u4f30\u4e86\u516d\u79cd\u5b9a\u91cf\u6307\u6807\uff0c\u63d0\u51faL1\u52a0\u6743Dice Focal Loss\uff08L1DFL\uff09\u5728Attention U-Net\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e0e\u771f\u5b9e\u503c\u76f8\u5173\u6027\u6700\u9ad8\u3002", "motivation": "\u4f20\u7edfDice\u76f8\u4f3c\u7cfb\u6570\u8bc4\u4f30\u6709\u9650\uff0c\u9700\u66f4\u5168\u9762\u7684\u5b9a\u91cf\u6307\u6807\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528U-Net\u3001Attention U-Net\u548cSegResNet\uff0c\u7ed3\u5408\u56db\u79cd\u635f\u5931\u51fd\u6570\uff08\u5305\u62ec\u63d0\u51fa\u7684L1DFL\uff09\uff0c\u5206\u6790380\u4f8bPSMA PET/CT\u626b\u63cf\u6570\u636e\u3002", "result": "Attention U-Net\u4e0eL1DFL\u7ec4\u5408\u8868\u73b0\u6700\u4f18\uff08\u76f8\u5173\u60270.90-0.99\uff09\uff0c\u800cDice Loss\u548c\u5176\u4ed6\u590d\u5408\u635f\u5931\u8868\u73b0\u8f83\u5dee\u3002SUV\u6307\u6807\u3001\u75c5\u7076\u8ba1\u6570\u548cTLA\u6027\u80fd\u9ad8\uff0c\u80bf\u7624\u4f53\u79ef\u548c\u75c5\u7076\u6269\u6563\u53d8\u5f02\u6027\u8f83\u5927\u3002", "conclusion": "L1DFL\u663e\u8457\u51cf\u5c11\u4e34\u5e8a\u6d4b\u91cf\u53d8\u5f02\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.16416", "pdf": "https://arxiv.org/pdf/2504.16416", "abs": "https://arxiv.org/abs/2504.16416", "authors": ["Tao Long", "Kendra Wannamaker", "Jo Vermeulen", "George Fitzmaurice", "Justin Matejka"], "title": "FeedQUAC: Quick Unobtrusive AI-Generated Commentary", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.MM"], "comment": "20 pages, 12 figures", "summary": "Design thrives on feedback. However, gathering constant feedback throughout\nthe design process can be labor-intensive and disruptive. We explore how AI can\nbridge this gap by providing effortless, ambient feedback. We introduce\nFeedQUAC, a design companion that delivers real-time AI-generated commentary\nfrom a variety of perspectives through different personas. A design probe study\nwith eight participants highlights how designers can leverage quick yet ambient\nAI feedback to enhance their creative workflows. Participants highlight\nbenefits such as convenience, playfulness, confidence boost, and inspiration\nfrom this lightweight feedback agent, while suggesting additional features,\nlike chat interaction and context curation. We discuss the role of AI feedback,\nits strengths and limitations, and how to integrate it into existing design\nworkflows while balancing user involvement. Our findings also suggest that\nambient interaction is a valuable consideration for both the design and\nevaluation of future creativity support systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5982\u4f55\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u5de5\u5177FeedQUAC\u63d0\u5347\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u7814\u7a76\u663e\u793a\u5176\u4fbf\u5229\u6027\u548c\u542f\u53d1\u6027\uff0c\u4f46\u4e5f\u9700\u5e73\u8861\u7528\u6237\u53c2\u4e0e\u3002", "motivation": "\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u6301\u7eed\u53cd\u9988\u9700\u6c42\u9ad8\u4f46\u8017\u65f6\uff0cAI\u53ef\u63d0\u4f9b\u65e0\u7f1d\u3001\u8f7b\u677e\u7684\u53cd\u9988\u652f\u6301\u3002", "method": "\u5f15\u5165FeedQUAC\u5de5\u5177\uff0c\u901a\u8fc7\u591a\u89d2\u8272AI\u5b9e\u65f6\u53cd\u9988\uff0c\u8fdb\u884c8\u4eba\u8bbe\u8ba1\u63a2\u9488\u7814\u7a76\u3002", "result": "\u53c2\u4e0e\u8005\u8ba4\u53ef\u5176\u4fbf\u5229\u6027\u3001\u8da3\u5473\u6027\u548c\u542f\u53d1\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u5982\u804a\u5929\u529f\u80fd\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002", "conclusion": "AI\u53cd\u9988\u5728\u8bbe\u8ba1\u4e2d\u6709\u6f5c\u529b\uff0c\u9700\u5e73\u8861\u7528\u6237\u53c2\u4e0e\uff0c\u73af\u5883\u4ea4\u4e92\u662f\u672a\u6765\u521b\u610f\u652f\u6301\u7cfb\u7edf\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2504.16420", "pdf": "https://arxiv.org/pdf/2504.16420", "abs": "https://arxiv.org/abs/2504.16420", "authors": ["Chengkai Huang", "Hongtao Huang", "Tong Yu", "Kaige Xie", "Junda Wu", "Shuai Zhang", "Julian Mcauley", "Dietmar Jannach", "Lina Yao"], "title": "A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender systems (RS) have become essential in filtering information and\npersonalizing content for users. RS techniques have traditionally relied on\nmodeling interactions between users and items as well as the features of\ncontent using models specific to each task. The emergence of foundation models\n(FMs), large scale models trained on vast amounts of data such as GPT, LLaMA\nand CLIP, is reshaping the recommendation paradigm. This survey provides a\ncomprehensive overview of the Foundation Models for Recommender Systems\n(FM4RecSys), covering their integration in three paradigms: (1) Feature-Based\naugmentation of representations, (2) Generative recommendation approaches, and\n(3) Agentic interactive systems. We first review the data foundations of RS,\nfrom traditional explicit or implicit feedback to multimodal content sources.\nWe then introduce FMs and their capabilities for representation learning,\nnatural language understanding, and multi-modal reasoning in RS contexts. The\ncore of the survey discusses how FMs enhance RS under different paradigms.\nAfterward, we examine FM applications in various recommendation tasks. Through\nan analysis of recent research, we highlight key opportunities that have been\nrealized as well as challenges encountered. Finally, we outline open research\ndirections and technical challenges for next-generation FM4RecSys. This survey\nnot only reviews the state-of-the-art methods but also provides a critical\nanalysis of the trade-offs among the feature-based, the generative, and the\nagentic paradigms, outlining key open issues and future research directions.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u63a8\u8350\u7cfb\u7edf\uff08RS\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u4e09\u79cd\u96c6\u6210\u8303\u5f0f\uff1a\u7279\u5f81\u589e\u5f3a\u3001\u751f\u6210\u5f0f\u63a8\u8350\u548c\u4ea4\u4e92\u5f0f\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u603b\u7ed3\u4e86\u673a\u9047\u3001\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\uff08\u5982GPT\u3001LLaMA\u3001CLIP\uff09\u7684\u5174\u8d77\uff0c\u63a8\u8350\u7cfb\u7edf\u8303\u5f0f\u6b63\u5728\u91cd\u5851\u3002\u672c\u6587\u65e8\u5728\u5168\u9762\u7efc\u8ff0FMs\u5728RS\u4e2d\u7684\u96c6\u6210\u65b9\u5f0f\u53ca\u5176\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e09\u79cd\u8303\u5f0f\uff08\u7279\u5f81\u589e\u5f3a\u3001\u751f\u6210\u5f0f\u63a8\u8350\u3001\u4ea4\u4e92\u5f0f\u4ee3\u7406\u7cfb\u7edf\uff09\u7684\u5e94\u7528\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u57fa\u7840\u548cFMs\u7684\u80fd\u529b\uff08\u5982\u8868\u793a\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff09\uff0c\u63a2\u8ba8FMs\u5982\u4f55\u63d0\u5347RS\u3002", "result": "FMs\u4e3aRS\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\uff0c\u4f46\u4e5f\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u603b\u7ed3\u4e86\u73b0\u6709\u7814\u7a76\u7684\u6210\u679c\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4e0d\u540c\u8303\u5f0f\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8FMs\u5728RS\u4e2d\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u4e0e\u96c6\u6210\uff0c\u89e3\u51b3\u73b0\u6709\u6311\u6218\u5e76\u63a2\u7d22\u65b0\u7684\u6280\u672f\u65b9\u5411\u3002"}}
{"id": "2504.16306", "pdf": "https://arxiv.org/pdf/2504.16306", "abs": "https://arxiv.org/abs/2504.16306", "authors": ["Yanlin Zhou", "Mostafa El-Khamy", "Kee-Bong Song"], "title": "Regularizing Differentiable Architecture Search with Smooth Activation", "categories": ["cs.NE", "cs.CV"], "comment": null, "summary": "Differentiable Architecture Search (DARTS) is an efficient Neural\nArchitecture Search (NAS) method but suffers from robustness, generalization,\nand discrepancy issues. Many efforts have been made towards the performance\ncollapse issue caused by skip dominance with various regularization techniques\ntowards operation weights, path weights, noise injection, and super-network\nredesign. It had become questionable at a certain point if there could exist a\nbetter and more elegant way to retract the search to its intended goal -- NAS\nis a selection problem. In this paper, we undertake a simple but effective\napproach, named Smooth Activation DARTS (SA-DARTS), to overcome skip dominance\nand discretization discrepancy challenges. By leveraging a smooth activation\nfunction on architecture weights as an auxiliary loss, our SA-DARTS mitigates\nthe unfair advantage of weight-free operations, converging to fanned-out\narchitecture weight values, and can recover the search process from\nskip-dominance initialization. Through theoretical and empirical analysis, we\ndemonstrate that the SA-DARTS can yield new state-of-the-art (SOTA) results on\nNAS-Bench-201, classification, and super-resolution. Further, we show that\nSA-DARTS can help improve the performance of SOTA models with fewer parameters,\nsuch as Information Multi-distillation Network on the super-resolution task.", "AI": {"tldr": "SA-DARTS\u901a\u8fc7\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\u89e3\u51b3DARTS\u4e2d\u7684\u8df3\u8dc3\u8fde\u63a5\u4f18\u52bf\u548c\u79bb\u6563\u5316\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347\u4e86NAS\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "DARTS\u65b9\u6cd5\u5b58\u5728\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u5dee\u5f02\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u8df3\u8dc3\u8fde\u63a5\u4f18\u52bf\u5bfc\u81f4\u7684\u6027\u80fd\u5d29\u6e83\u3002", "method": "\u63d0\u51faSA-DARTS\uff0c\u5229\u7528\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\u4f5c\u4e3a\u8f85\u52a9\u635f\u5931\uff0c\u5e73\u8861\u6743\u91cd\u81ea\u7531\u64cd\u4f5c\u7684\u4e0d\u516c\u5e73\u4f18\u52bf\u3002", "result": "SA-DARTS\u5728NAS-Bench-201\u3001\u5206\u7c7b\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\u7ed3\u679c\uff0c\u5e76\u63d0\u5347\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "SA-DARTS\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86DARTS\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.16427", "pdf": "https://arxiv.org/pdf/2504.16427", "abs": "https://arxiv.org/abs/2504.16427", "authors": ["Hanlei Zhang", "Zhuohang Li", "Yeshuang Zhu", "Hua Xu", "Peiwu Wang", "Jinchao Zhang", "Jie Zhou", "Haige Zhu"], "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "23 pages, 5 figures", "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MMLA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7406\u89e3\u8ba4\u77e5\u7ea7\u8bed\u4e49\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8986\u76d6\u516d\u4e2a\u6838\u5fc3\u7ef4\u5ea6\u3002\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6a21\u578b\u6027\u80fd\u6709\u9650\uff0c\u4ec5\u8fbe60%~70%\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u5bf9MLLMs\u7406\u89e3\u8ba4\u77e5\u7ea7\u8bed\u4e49\u80fd\u529b\u7684\u8bc4\u4f30\uff0cMMLA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b61K\u591a\u6a21\u6001\u8bdd\u8bed\u7684MMLA\u57fa\u51c6\uff0c\u8bc4\u4f30\u516b\u79cd\u4e3b\u6d41LLMs\u548cMLLMs\uff0c\u91c7\u7528\u96f6\u6837\u672c\u63a8\u7406\u3001\u76d1\u7763\u5fae\u8c03\u548c\u6307\u4ee4\u8c03\u6574\u4e09\u79cd\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5fae\u8c03\u540e\u6a21\u578b\u51c6\u786e\u7387\u4ec560%~70%\uff0c\u7a81\u663e\u5f53\u524dMLLMs\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MMLA\u4e3a\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bed\u8a00\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u5f00\u6e90\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2504.16430", "pdf": "https://arxiv.org/pdf/2504.16430", "abs": "https://arxiv.org/abs/2504.16430", "authors": ["Andrew Ilyas", "Logan Engstrom"], "title": "MAGIC: Near-Optimal Data Attribution for Deep Learning", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "comment": null, "summary": "The goal of predictive data attribution is to estimate how adding or removing\na given set of training datapoints will affect model predictions. In convex\nsettings, this goal is straightforward (i.e., via the infinitesimal jackknife).\nIn large-scale (non-convex) settings, however, existing methods are far less\nsuccessful -- current methods' estimates often only weakly correlate with\nground truth. In this work, we present a new data attribution method (MAGIC)\nthat combines classical methods and recent advances in metadifferentiation to\n(nearly) optimally estimate the effect of adding or removing training data on\nmodel predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5MAGIC\uff0c\u7ed3\u5408\u7ecf\u5178\u65b9\u6cd5\u548c\u5143\u5fae\u5206\u6280\u672f\uff0c\u7528\u4e8e\u4f30\u8ba1\u8bad\u7ec3\u6570\u636e\u589e\u51cf\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u975e\u51f8\u5927\u89c4\u6a21\u573a\u666f\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6570\u636e\u5f52\u5c5e\u7684\u4f30\u8ba1\u6548\u679c\u4e0d\u4f73\uff0c\u4e0e\u771f\u5b9e\u60c5\u51b5\u76f8\u5173\u6027\u5f31\u3002", "method": "\u7ed3\u5408\u7ecf\u5178\u65b9\u6cd5\u548c\u5143\u5fae\u5206\u6280\u672f\uff0c\u5f00\u53d1\u4e86MAGIC\u65b9\u6cd5\u3002", "result": "MAGIC\u80fd\u8fd1\u4e4e\u6700\u4f18\u5730\u4f30\u8ba1\u8bad\u7ec3\u6570\u636e\u589e\u51cf\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "conclusion": "MAGIC\u65b9\u6cd5\u5728\u975e\u51f8\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u5f52\u5c5e\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.16432", "pdf": "https://arxiv.org/pdf/2504.16432", "abs": "https://arxiv.org/abs/2504.16432", "authors": ["Ziran Liang", "Rui An", "Wenqi Fan", "Yanghui Rao", "Yuxuan Liang"], "title": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As time evolves, data within specific domains exhibit predictability that\nmotivates time series forecasting to predict future trends from historical\ndata. However, current deep forecasting methods can achieve promising\nperformance but generally lack interpretability, hindering trustworthiness and\npractical deployment in safety-critical applications such as auto-driving and\nhealthcare. In this paper, we propose a novel interpretable model, iTFKAN, for\ncredible time series forecasting. iTFKAN enables further exploration of model\ndecision rationales and underlying data patterns due to its interpretability\nachieved through model symbolization. Besides, iTFKAN develops two strategies,\nprior knowledge injection, and time-frequency synergy learning, to effectively\nguide model learning under complex intertwined time series data. Extensive\nexperimental results demonstrated that iTFKAN can achieve promising forecasting\nperformance while simultaneously possessing high interpretive capabilities.", "AI": {"tldr": "iTFKAN\u662f\u4e00\u79cd\u65b0\u578b\u53ef\u89e3\u91ca\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u578b\u7b26\u53f7\u5316\u5b9e\u73b0\u9ad8\u89e3\u91ca\u6027\uff0c\u5e76\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u6ce8\u5165\u548c\u65f6\u9891\u534f\u540c\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u590d\u6742\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u9884\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\uff0c\u5982\u81ea\u52a8\u9a7e\u9a76\u548c\u533b\u7597\u4fdd\u5065\u3002", "method": "\u63d0\u51faiTFKAN\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u578b\u7b26\u53f7\u5316\u5b9e\u73b0\u89e3\u91ca\u6027\uff0c\u5e76\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u6ce8\u5165\u548c\u65f6\u9891\u534f\u540c\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eiTFKAN\u5728\u9884\u6d4b\u6027\u80fd\u548c\u89e3\u91ca\u80fd\u529b\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "iTFKAN\u4e3a\u53ef\u4fe1\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u9ad8\u6027\u80fd\u548c\u9ad8\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.16606", "pdf": "https://arxiv.org/pdf/2504.16606", "abs": "https://arxiv.org/abs/2504.16606", "authors": ["Zhongtao Wang", "Mai Su", "Huishan Au", "Yilong Li", "Xizhe Cao", "Chengwei Pan", "Yisong Chen", "Guoping Wang"], "title": "HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "As urban 3D scenes become increasingly complex and the demand for\nhigh-quality rendering grows, efficient scene reconstruction and rendering\ntechniques become crucial. We present HUG, a novel approach to address\ninefficiencies in handling large-scale urban environments and intricate details\nbased on 3D Gaussian splatting. Our method optimizes data partitioning and the\nreconstruction pipeline by incorporating a hierarchical neural Gaussian\nrepresentation. We employ an enhanced block-based reconstruction pipeline\nfocusing on improving reconstruction quality within each block and reducing the\nneed for redundant training regions around block boundaries. By integrating\nneural Gaussian representation with a hierarchical architecture, we achieve\nhigh-quality scene rendering at a low computational cost. This is demonstrated\nby our state-of-the-art results on public benchmarks, which prove the\neffectiveness and advantages in large-scale urban scene representation.", "AI": {"tldr": "HUG\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u795e\u7ecf\u9ad8\u65af\u8868\u793a\u4f18\u5316\u5927\u89c4\u6a21\u57ce\u5e02\u73af\u5883\u7684\u91cd\u5efa\u4e0e\u6e32\u67d3\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u968f\u7740\u57ce\u5e023D\u573a\u666f\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u5bf9\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u9ad8\u6548\u91cd\u5efa\u4e0e\u6e32\u67d3\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u5206\u5c42\u795e\u7ecf\u9ad8\u65af\u8868\u793a\uff0c\u4f18\u5316\u6570\u636e\u5206\u533a\u548c\u91cd\u5efa\u6d41\u7a0b\uff0c\u51cf\u5c11\u5197\u4f59\u8bad\u7ec3\u533a\u57df\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u8868\u793a\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "HUG\u901a\u8fc7\u5206\u5c42\u795e\u7ecf\u9ad8\u65af\u8868\u793a\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u6210\u672c\u7684\u573a\u666f\u6e32\u67d3\uff0c\u9002\u7528\u4e8e\u590d\u6742\u57ce\u5e02\u73af\u5883\u3002"}}
{"id": "2504.16438", "pdf": "https://arxiv.org/pdf/2504.16438", "abs": "https://arxiv.org/abs/2504.16438", "authors": ["Charlie Hou", "Mei-Yu Wang", "Yige Zhu", "Daniel Lazar", "Giulia Fanti"], "title": "Private Federated Learning using Preference-Optimized Synthetic Data", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": "Spotlight presentation at SynthData Workshop ICLR25", "summary": "In practical settings, differentially private Federated learning (DP-FL) is\nthe dominant method for training models from private, on-device client data.\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\nalgorithms for generating DP synthetic data for FL applications require careful\nprompt engineering based on public information and/or iterative private client\nfeedback. Our key insight is that the private client feedback collected by\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\nviewed as a preference ranking. Our algorithm, Preference Optimization for\nPrivate Client Data (POPri) harnesses client feedback using preference\noptimization algorithms such as Direct Preference Optimization (DPO) to\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\nevaluations on federated client data. POPri substantially improves the utility\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\nnext-token prediction accuracy in the fully-private and non-private settings by\nup to 68%, compared to 52% for prior synthetic data methods, and 10% for\nstate-of-the-art DP federated learning methods. The code and data are available\nat https://github.com/meiyuw/POPri.", "AI": {"tldr": "POPri\u5229\u7528\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u63d0\u5347\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u663e\u8457\u7f29\u5c0f\u9690\u79c1\u4e0e\u975e\u9690\u79c1\u8bbe\u7f6e\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5dee\u5206\u9690\u79c1\u8054\u90a6\u5b66\u4e60\uff08DP-FL\uff09\u5728\u5904\u7406\u9690\u79c1\u6570\u636e\u65f6\u8868\u73b0\u6709\u9650\uff0c\u800c\u73b0\u6709\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u63d0\u793a\u5de5\u7a0b\u6216\u8fed\u4ee3\u53cd\u9988\u3002POPri\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u5ba2\u6237\u7aef\u53cd\u9988\u63d0\u5347\u5408\u6210\u6570\u636e\u8d28\u91cf\u3002", "method": "POPri\u5c06\u5ba2\u6237\u7aef\u53cd\u9988\u89c6\u4e3a\u504f\u597d\u6392\u540d\uff0c\u5e76\u5229\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7b49\u7b97\u6cd5\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u3002", "result": "\u5728LargeFedBench\u6570\u636e\u96c6\u4e0a\uff0cPOPri\u5c06\u9690\u79c1\u4e0e\u975e\u9690\u79c1\u8bbe\u7f6e\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\u81f368%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0852%\uff09\u548cDP-FL\uff0810%\uff09\u3002", "conclusion": "POPri\u901a\u8fc7\u504f\u597d\u4f18\u5316\u663e\u8457\u63d0\u5347\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16667", "pdf": "https://arxiv.org/pdf/2504.16667", "abs": "https://arxiv.org/abs/2504.16667", "authors": ["Zhaohan Daniel Guo", "Bernardo Avila Pires", "Khimya Khetarpal", "Dale Schuurmans", "Bo Dai"], "title": "Representation Learning via Non-Contrastive Mutual Information", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML", "I.2.6; I.2.10"], "comment": null, "summary": "Labeling data is often very time consuming and expensive, leaving us with a\nmajority of unlabeled data. Self-supervised representation learning methods\nsuch as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very\nsuccessful at learning meaningful latent representations from unlabeled image\ndata, resulting in much more general and transferable representations for\ndownstream tasks. Broadly, self-supervised methods fall into two types: 1)\nContrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as\nBYOL. Contrastive methods are generally trying to maximize mutual information\nbetween related data points, so they need to compare every data point to every\nother data point, resulting in high variance, and thus requiring large batch\nsizes to work well. Non-contrastive methods like BYOL have much lower variance\nas they do not need to make pairwise comparisons, but are much trickier to\nimplement as they have the possibility of collapsing to a constant vector. In\nthis paper, we aim to develop a self-supervised objective that combines the\nstrength of both types. We start with a particular contrastive method called\nthe Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we\nconvert it into a more general non-contrastive form; this removes the pairwise\ncomparisons resulting in lower variance, but keeps the mutual information\nformulation of the contrastive method preventing collapse. We call our new\nobjective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by\nlearning image representations on ImageNet (similar to SimCLR and BYOL) and\nshow that it consistently improves upon the Spectral Contrastive loss baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u548c\u975e\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u4f18\u52bf\u7684\u65b0\u76ee\u6807\u51fd\u6570MINC\uff0c\u901a\u8fc7\u6539\u8fdb\u8c31\u5bf9\u6bd4\u635f\u5931\uff0c\u907f\u514d\u4e86\u9ad8\u65b9\u5dee\u548c\u6a21\u578b\u574d\u584c\u95ee\u9898\uff0c\u5e76\u5728ImageNet\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\uff08\u5982SimCLR\u548cBYOL\uff09\u867d\u80fd\u4ece\u672a\u6807\u6ce8\u6570\u636e\u4e2d\u5b66\u4e60\u6709\u7528\u8868\u793a\uff0c\u4f46\u5bf9\u6bd4\u65b9\u6cd5\u65b9\u5dee\u9ad8\uff0c\u975e\u5bf9\u6bd4\u65b9\u6cd5\u6613\u574d\u584c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u8c31\u5bf9\u6bd4\u635f\u5931\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u975e\u5bf9\u6bd4\u5f62\u5f0f\uff0c\u63d0\u51faMINC\u635f\u5931\uff0c\u907f\u514d\u6210\u5bf9\u6bd4\u8f83\uff08\u964d\u4f4e\u65b9\u5dee\uff09\u5e76\u4fdd\u7559\u4e92\u4fe1\u606f\uff08\u9632\u6b62\u574d\u584c\uff09\u3002", "result": "\u5728ImageNet\u4e0a\u6d4b\u8bd5\uff0cMINC\u8868\u73b0\u4f18\u4e8e\u8c31\u5bf9\u6bd4\u635f\u5931\u57fa\u7ebf\u3002", "conclusion": "MINC\u6210\u529f\u7ed3\u5408\u4e86\u5bf9\u6bd4\u548c\u975e\u5bf9\u6bd4\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u3002"}}
{"id": "2504.16448", "pdf": "https://arxiv.org/pdf/2504.16448", "abs": "https://arxiv.org/abs/2504.16448", "authors": ["Shuguang Zhao", "Qiangzhong Feng", "Zhiyang He", "Peipei Sun", "Yingying Wang", "Xiaodong Tao", "Xiaoliang Lu", "Mei Cheng", "Xinyue Wu", "Yanyan Wang", "Wei Liang"], "title": "EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical consultation dialogues contain critical clinical information, yet\ntheir unstructured nature hinders effective utilization in diagnosis and\ntreatment. Traditional methods, relying on rule-based or shallow machine\nlearning techniques, struggle to capture deep and implicit semantics. Recently,\nlarge pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight\nfine-tuning method, have shown promise for structured information extraction.\nWe propose EMRModel, a novel approach that integrates LoRA-based fine-tuning\nwith code-style prompt design, aiming to efficiently convert medical\nconsultation dialogues into structured electronic medical records (EMRs).\nAdditionally, we construct a high-quality, realistically grounded dataset of\nmedical consultation dialogues with detailed annotations. Furthermore, we\nintroduce a fine-grained evaluation benchmark for medical consultation\ninformation extraction and provide a systematic evaluation methodology,\nadvancing the optimization of medical natural language processing (NLP) models.\nExperimental results show EMRModel achieves an F1 score of 88.1%, improving\nby49.5% over standard pre-trained models. Compared to traditional LoRA\nfine-tuning methods, our model shows superior performance, highlighting its\neffectiveness in structured medical record extraction tasks.", "AI": {"tldr": "EMRModel\u7ed3\u5408LoRA\u5fae\u8c03\u548c\u4ee3\u7801\u98ce\u683c\u63d0\u793a\u8bbe\u8ba1\uff0c\u5c06\u533b\u7597\u54a8\u8be2\u5bf9\u8bdd\u9ad8\u6548\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7535\u5b50\u75c5\u5386\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u533b\u7597\u54a8\u8be2\u5bf9\u8bdd\u7684\u975e\u7ed3\u6784\u5316\u7279\u6027\u9650\u5236\u4e86\u5176\u5728\u8bca\u65ad\u548c\u6cbb\u7597\u4e2d\u7684\u6709\u6548\u5229\u7528\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6df1\u5c42\u8bed\u4e49\u3002", "method": "\u91c7\u7528LoRA\u5fae\u8c03\u548c\u4ee3\u7801\u98ce\u683c\u63d0\u793a\u8bbe\u8ba1\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "EMRModel\u7684F1\u5206\u6570\u8fbe88.1%\uff0c\u6bd4\u6807\u51c6\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u534749.5%\uff0c\u4f18\u4e8e\u4f20\u7edfLoRA\u65b9\u6cd5\u3002", "conclusion": "EMRModel\u5728\u7ed3\u6784\u5316\u75c5\u5386\u63d0\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u52a8\u4e86\u533b\u7597NLP\u6a21\u578b\u7684\u4f18\u5316\u3002"}}
{"id": "2504.16745", "pdf": "https://arxiv.org/pdf/2504.16745", "abs": "https://arxiv.org/abs/2504.16745", "authors": ["Jialiang Zhang", "Feng Gao", "Yanhai Gan", "Junyu Dong", "Qian Du"], "title": "Frequency-Compensated Network for Daily Arctic Sea Ice Concentration Prediction", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IEEE TGRS 2025", "summary": "Accurately forecasting sea ice concentration (SIC) in the Arctic is critical\nto global ecosystem health and navigation safety. However, current methods\nstill is confronted with two challenges: 1) these methods rarely explore the\nlong-term feature dependencies in the frequency domain. 2) they can hardly\npreserve the high-frequency details, and the changes in the marginal area of\nthe sea ice cannot be accurately captured. To this end, we present a\nFrequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily\nbasis. In particular, we design a dual-branch network, including branches for\nfrequency feature extraction and convolutional feature extraction. For\nfrequency feature extraction, we design an adaptive frequency filter block,\nwhich integrates trainable layers with Fourier-based filters. By adding\nfrequency features, the FCNet can achieve refined prediction of edges and\ndetails. For convolutional feature extraction, we propose a high-frequency\nenhancement block to separate high and low-frequency information. Moreover,\nhigh-frequency features are enhanced via channel-wise attention, and temporal\nattention unit is employed for low-frequency feature extraction to capture\nlong-range sea ice changes. Extensive experiments are conducted on a\nsatellite-derived daily SIC dataset, and the results verify the effectiveness\nof the proposed FCNet. Our codes and data will be made public available at:\nhttps://github.com/oucailab/FCNet .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387\u8865\u507f\u7f51\u7edc\uff08FCNet\uff09\u7528\u4e8e\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\uff08SIC\uff09\u7684\u6bcf\u65e5\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9891\u57df\u957f\u671f\u7279\u5f81\u4f9d\u8d56\u548c\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\u7684\u51c6\u786e\u9884\u6d4b\u5bf9\u5168\u7403\u751f\u6001\u7cfb\u7edf\u5065\u5eb7\u548c\u822a\u884c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9891\u57df\u7279\u5f81\u4f9d\u8d56\u548c\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u5206\u652f\u7f51\u7edc\uff0c\u5305\u62ec\u9891\u7387\u7279\u5f81\u63d0\u53d6\u548c\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u5206\u652f\uff0c\u5206\u522b\u901a\u8fc7\u81ea\u9002\u5e94\u9891\u7387\u6ee4\u6ce2\u5757\u548c\u9ad8\u9891\u589e\u5f3a\u5757\u5b9e\u73b0\u7279\u5f81\u63d0\u53d6\u4e0e\u589e\u5f3a\u3002", "result": "\u5728\u536b\u661fSIC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FCNet\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u8fb9\u7f18\u548c\u7ec6\u8282\u7684\u7cbe\u7ec6\u9884\u6d4b\u3002", "conclusion": "FCNet\u901a\u8fc7\u9891\u57df\u548c\u5377\u79ef\u7279\u5f81\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2504.16460", "pdf": "https://arxiv.org/pdf/2504.16460", "abs": "https://arxiv.org/abs/2504.16460", "authors": ["Vignesh Ethiraj", "Sidhanth Menon", "Divya Vijay"], "title": "T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning", "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "Introduces T-VEC, a telecom-specific text embedding model. Fine-tuned\n  gte-Qwen2-1.5B-instruct on curated telecom data points. Includes the first\n  open-source telecom tokenizer. Model available at\n  https://huggingface.co/NetoAISolutions/T-VEC", "summary": "The specialized vocabulary and complex concepts of the telecommunications\nindustry present significant challenges for standard Natural Language\nProcessing models. Generic text embeddings often fail to capture\ntelecom-specific semantics, hindering downstream task performance. We introduce\nT-VEC (Telecom Vectorization Model), a novel embedding model tailored for the\ntelecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created\nby adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet\nloss objective on a meticulously curated, large-scale dataset of\ntelecom-specific data. Crucially, this process involved substantial\nmodification of weights across 338 layers of the base model, ensuring deep\nintegration of domain knowledge, far exceeding superficial adaptation\ntechniques. We quantify this deep change via weight difference analysis. A key\ncontribution is the development and open-sourcing (MIT License) of the first\ndedicated telecom-specific tokenizer, enhancing the handling of industry\njargon. T-VEC achieves a leading average MTEB score (0.825) compared to\nestablished models and demonstrates vastly superior performance (0.9380 vs.\nless than 0.07) on our internal telecom-specific triplet evaluation benchmark,\nindicating an exceptional grasp of domain-specific nuances, visually confirmed\nby improved embedding separation. This work positions NetoAI at the forefront\nof telecom AI innovation, providing the community with a powerful, deeply\nadapted, open-source tool.", "AI": {"tldr": "T-VEC\u662f\u4e00\u79cd\u4e13\u4e3a\u7535\u4fe1\u884c\u4e1a\u5b9a\u5236\u7684\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u7535\u4fe1\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u7535\u4fe1\u884c\u4e1a\u7684\u4e13\u4e1a\u8bcd\u6c47\u548c\u590d\u6742\u6982\u5ff5\u5bf9\u901a\u7528NLP\u6a21\u578b\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u5d4c\u5165\u6a21\u578b\u3002", "method": "\u57fa\u4e8egte-Qwen2-1.5B-instruct\u6a21\u578b\uff0c\u91c7\u7528\u4e09\u5143\u7ec4\u635f\u5931\u76ee\u6807\uff0c\u5bf9338\u5c42\u6743\u91cd\u8fdb\u884c\u6df1\u5ea6\u4fee\u6539\uff0c\u5e76\u5f00\u53d1\u4e86\u7535\u4fe1\u4e13\u7528\u5206\u8bcd\u5668\u3002", "result": "T-VEC\u5728MTEB\u8bc4\u5206\uff080.825\uff09\u548c\u5185\u90e8\u7535\u4fe1\u8bc4\u4f30\uff080.9380\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "T-VEC\u4e3a\u7535\u4fe1AI\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u521b\u65b0\u3002"}}
{"id": "2504.16763", "pdf": "https://arxiv.org/pdf/2504.16763", "abs": "https://arxiv.org/abs/2504.16763", "authors": ["Edison Mucllari", "Aswin Raghavan", "Zachary Alan Daniels"], "title": "Noise-Tolerant Coreset-Based Class Incremental Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "comment": "Work-in-Progress", "summary": "Many applications of computer vision require the ability to adapt to novel\ndata distributions after deployment. Adaptation requires algorithms capable of\ncontinual learning (CL). Continual learners must be plastic to adapt to novel\ntasks while minimizing forgetting of previous tasks.However, CL opens up\navenues for noise to enter the training pipeline and disrupt the CL. This work\nfocuses on label noise and instance noise in the context of class-incremental\nlearning (CIL), where new classes are added to a classifier over time, and\nthere is no access to external data from past classes. We aim to understand the\nsensitivity of CL methods that work by replaying items from a memory\nconstructed using the idea of Coresets. We derive a new bound for the\nrobustness of such a method to uncorrelated instance noise under a general\nadditive noise threat model, revealing several insights. Putting the theory\ninto practice, we create two continual learning algorithms to construct\nnoise-tolerant replay buffers. We empirically compare the effectiveness of\nprior memory-based continual learners and the proposed algorithms under label\nand uncorrelated instance noise on five diverse datasets. We show that existing\nmemory-based CL are not robust whereas the proposed methods exhibit significant\nimprovements in maximizing classification accuracy and minimizing forgetting in\nthe noisy CIL setting.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u4e2d\uff0c\u6807\u7b7e\u566a\u58f0\u548c\u5b9e\u4f8b\u566a\u58f0\u5bf9\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u566a\u58f0\u5bb9\u5fcd\u7684\u91cd\u653e\u7f13\u51b2\u7b97\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u9700\u8981\u9002\u5e94\u65b0\u6570\u636e\u5206\u5e03\uff0c\u4f46\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u6613\u53d7\u566a\u58f0\u5e72\u6270\uff0c\u5c24\u5176\u662f\u6807\u7b7e\u566a\u58f0\u548c\u5b9e\u4f8b\u566a\u58f0\u3002\u672c\u6587\u65e8\u5728\u7406\u89e3\u57fa\u4e8eCoresets\u7684\u91cd\u653e\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u566a\u58f0\u5bb9\u5fcd\u7684CL\u7b97\u6cd5\u3002", "method": "\u63a8\u5bfc\u4e86\u5728\u4e00\u822c\u52a0\u6027\u566a\u58f0\u5a01\u80c1\u6a21\u578b\u4e0b\uff0c\u57fa\u4e8eCoresets\u7684\u91cd\u653e\u65b9\u6cd5\u5bf9\u4e0d\u76f8\u5173\u5b9e\u4f8b\u566a\u58f0\u7684\u9c81\u68d2\u6027\u8fb9\u754c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u566a\u58f0\u5bb9\u5fcd\u7684\u91cd\u653e\u7f13\u51b2\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u57fa\u4e8e\u5185\u5b58\u7684CL\u65b9\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4e0d\u9c81\u68d2\uff0c\u800c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u6027\u548c\u51cf\u5c11\u9057\u5fd8\u65b9\u9762\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u566a\u58f0\u5bb9\u5fcd\u91cd\u653e\u7f13\u51b2\u7b97\u6cd5\u5728\u566a\u58f0CIL\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16464", "pdf": "https://arxiv.org/pdf/2504.16464", "abs": "https://arxiv.org/abs/2504.16464", "authors": ["Ying Li", "Xiaobao Wei", "Xiaowei Chi", "Yuming Li", "Zhongyu Zhao", "Hao Wang", "Ningning Ma", "Ming Lu", "Shanghang Zhang"], "title": "ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 3 figures", "summary": "While recent advancements in robotic manipulation video synthesis have shown\npromise, significant challenges persist in ensuring effective\ninstruction-following and achieving high visual quality. Recent methods, like\nRoboDreamer, utilize linguistic decomposition to divide instructions into\nseparate lower-level primitives, conditioning the world model on these\nprimitives to achieve compositional instruction-following. However, these\nseparate primitives do not consider the relationships that exist between them.\nFurthermore, recent methods neglect valuable visual guidance, including depth\nand semantic guidance, both crucial for enhancing visual quality. This paper\nintroduces ManipDreamer, an advanced world model based on the action tree and\nvisual guidance. To better learn the relationships between instruction\nprimitives, we represent the instruction as the action tree and assign\nembeddings to tree nodes, each instruction can acquire its embeddings by\nnavigating through the action tree. The instruction embeddings can be used to\nguide the world model. To enhance visual quality, we combine depth and semantic\nguidance by introducing a visual guidance adapter compatible with the world\nmodel. This visual adapter enhances both the temporal and physical consistency\nof video generation. Based on the action tree and visual guidance, ManipDreamer\nsignificantly boosts the instruction-following ability and visual quality.\nComprehensive evaluations on robotic manipulation benchmarks reveal that\nManipDreamer achieves large improvements in video quality metrics in both seen\nand unseen tasks, with PSNR improved from 19.55 to 21.05, SSIM improved from\n0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks,\ncompared to the recent RoboDreamer model. Additionally, our method increases\nthe success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on\naverage.", "AI": {"tldr": "ManipDreamer\u901a\u8fc7\u52a8\u4f5c\u6811\u548c\u89c6\u89c9\u5f15\u5bfc\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u5408\u6210\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982RoboDreamer\u672a\u8003\u8651\u6307\u4ee4\u539f\u8bed\u95f4\u7684\u5173\u7cfb\uff0c\u4e14\u5ffd\u89c6\u6df1\u5ea6\u548c\u8bed\u4e49\u5f15\u5bfc\uff0c\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u5c06\u6307\u4ee4\u8868\u793a\u4e3a\u52a8\u4f5c\u6811\uff0c\u4e3a\u8282\u70b9\u5206\u914d\u5d4c\u5165\uff1b\u5f15\u5165\u89c6\u89c9\u5f15\u5bfc\u9002\u914d\u5668\u589e\u5f3a\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u4efb\u52a1\u4e2d\uff0cPSNR\u4ece19.55\u63d0\u5347\u81f321.05\uff0cSSIM\u4ece0.7474\u63d0\u5347\u81f30.7982\uff0cFlow Error\u4ece3.506\u964d\u81f33.201\uff1bRLbench\u4efb\u52a1\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad82.5%\u3002", "conclusion": "ManipDreamer\u663e\u8457\u63d0\u5347\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2504.16774", "pdf": "https://arxiv.org/pdf/2504.16774", "abs": "https://arxiv.org/abs/2504.16774", "authors": ["Lakshita Agarwal", "Bindu Verma"], "title": "Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors and Cross-Model Attention Mechanism", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The examination of chest X-ray images is a crucial component in detecting\nvarious thoracic illnesses. This study introduces a new image description\ngeneration model that integrates a Vision Transformer (ViT) encoder with\ncross-modal attention and a GPT-4-based transformer decoder. The ViT captures\nhigh-quality visual features from chest X-rays, which are fused with text data\nthrough cross-modal attention to improve the accuracy, context, and richness of\nimage descriptions. The GPT-4 decoder transforms these fused features into\naccurate and relevant captions. The model was tested on the National Institutes\nof Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU\ndataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and\n0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all\nmetrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726),\nand ROUGE-L (0.705). This framework has the potential to enhance chest X-ray\nevaluation, assisting radiologists in more precise and efficient diagnosis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Vision Transformer\u7f16\u7801\u5668\u548cGPT-4\u89e3\u7801\u5668\u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u80f8\u90e8X\u5149\u56fe\u50cf\u7684\u63cf\u8ff0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63cf\u8ff0\u51c6\u786e\u6027\u548c\u4e30\u5bcc\u6027\u3002", "motivation": "\u80f8\u90e8X\u5149\u68c0\u67e5\u5bf9\u8bca\u65ad\u80f8\u8154\u75be\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u63cf\u8ff0\u751f\u6210\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Vision Transformer\u7f16\u7801\u5668\u63d0\u53d6\u9ad8\u8d28\u91cf\u89c6\u89c9\u7279\u5f81\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u4e0e\u6587\u672c\u6570\u636e\u878d\u5408\uff0c\u518d\u7528GPT-4\u89e3\u7801\u5668\u751f\u6210\u63cf\u8ff0\u3002", "result": "\u5728NIH\u548cIU\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cIU\u6570\u636e\u96c6\u4e0aB-1\u4e3a0.854\uff0cCIDEr\u4e3a0.883\uff1bNIH\u6570\u636e\u96c6\u4e0a\u6240\u6709\u6307\u6807\u5747\u4e3a\u6700\u4f73\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u671b\u63d0\u5347\u80f8\u90e8X\u5149\u8bc4\u4f30\u7684\u7cbe\u786e\u6027\u548c\u6548\u7387\uff0c\u8f85\u52a9\u653e\u5c04\u79d1\u533b\u751f\u8bca\u65ad\u3002"}}
{"id": "2504.16472", "pdf": "https://arxiv.org/pdf/2504.16472", "abs": "https://arxiv.org/abs/2504.16472", "authors": ["Mark Harman", "Peter O'Hearn", "Shubho Sengupta"], "title": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges", "categories": ["cs.SE", "cs.AI"], "comment": "To Appear as keynote paper at FSE 2025", "summary": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching `Just-in-Time' (JiTTest) Challenge, in which tests are generated\n`just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper\\footnote{Author order is alphabetical. The\ncorresponding author is Mark Harman.} was written to accompany the keynote by\nthe authors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u786c\u5316\u6d4b\u8bd5\u548c\u6355\u83b7\u6d4b\u8bd5\uff0c\u63d0\u51fa\u4e86\u201c\u53ca\u65f6\u6355\u83b7\u6d4b\u8bd5\u201d\u6311\u6218\uff0c\u5e76\u8ba8\u8bba\u4e86\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u786c\u5316\u6d4b\u8bd5\u7684\u521d\u6b65\u6210\u679c\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u52a8\u5316\u8f6f\u4ef6\u6d4b\u8bd5\u7814\u7a76\u5df2\u6709\u6570\u5341\u5e74\uff0c\u4f46\u4e00\u4e9b\u57fa\u672c\u6982\u5ff5\u4ecd\u4e0d\u660e\u786e\u4e14\u6f5c\u529b\u5de8\u5927\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u8fd9\u4e9b\u6982\u5ff5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u65b0\u6311\u6218\u3002", "method": "\u8bba\u6587\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u786c\u5316\u6d4b\u8bd5\u548c\u6355\u83b7\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u53ca\u65f6\u6355\u83b7\u6d4b\u8bd5\u201d\uff08JiTTest\uff09\u6311\u6218\uff0c\u63a2\u8ba8\u4e86\u6d4b\u8bd5\u751f\u6210\u7684\u53ef\u80fd\u7ed3\u679c\u548c\u90e8\u7f72\u9009\u9879\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u786c\u5316\u6d4b\u8bd5\u5728Meta\u7684\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u786c\u5316\u6d4b\u8bd5\u548c\u6355\u83b7\u6d4b\u8bd5\u7684\u7814\u7a76\u95ee\u9898\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u5728LLM\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2504.16798", "pdf": "https://arxiv.org/pdf/2504.16798", "abs": "https://arxiv.org/abs/2504.16798", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "title": "4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis", "categories": ["cs.MM", "cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal neuroimaging provides complementary structural and functional\ninsights into both human brain organization and disease-related dynamics.\nRecent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's\ndisease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,\nfMRI) with behavioral cognitive scores tabular data biomarkers. However, the\nintrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI\ndynamics vs. 3D anatomical sMRI structure) presents critical challenges for\ndiscriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a\ngeometry-aware multimodal co-attention network with latent alignment for early\nAD diagnosis using sMRI and fMRI. At the core of our approach is a\nmulti-patch-to-multi-patch (M2M) contrastive loss function that quantifies and\nreduces representational discrepancies via geometry-weighted patch\ncorrespondence, explicitly aligning fMRI components across brain regions with\ntheir sMRI structural substrates without one-to-one constraints. Additionally,\nwe propose a latent-as-query co-attention module to autonomously discover\nfusion patterns, circumventing modality prioritization biases while minimizing\nfeature redundancy. We conduct extensive experiments to confirm the\neffectiveness of our method and highlight the correspondance between fMRI and\nsMRI as AD biomarkers.", "AI": {"tldr": "M2M-AlignNet\u662f\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u591a\u6a21\u6001\u5171\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u901a\u8fc7sMRI\u548cfMRI\u6570\u636e\u65e9\u671f\u8bca\u65ad\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\uff0c\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u548c\u6f5c\u5728\u5bf9\u9f50\u51cf\u5c11\u6a21\u6001\u95f4\u5dee\u5f02\u3002", "motivation": "\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u6570\u636e\uff08\u5982sMRI\u548cfMRI\uff09\u7684\u5f02\u6784\u6027\u4e3a\u7279\u5f81\u878d\u5408\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u6a21\u6001\u95f4\u5dee\u5f02\u5e76\u63d0\u5347AD\u8bca\u65ad\u654f\u611f\u6027\u3002", "method": "\u63d0\u51faM2M-AlignNet\uff0c\u4f7f\u7528\u591a\u8865\u4e01\u5230\u591a\u8865\u4e01\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u548c\u6f5c\u5728\u67e5\u8be2\u5171\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u663e\u5f0f\u5bf9\u9f50fMRI\u548csMRI\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86fMRI\u548csMRI\u4f5c\u4e3aAD\u751f\u7269\u6807\u5fd7\u7269\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "M2M-AlignNet\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u5bf9\u9f50\u548c\u591a\u6a21\u6001\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86AD\u65e9\u671f\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.16479", "pdf": "https://arxiv.org/pdf/2504.16479", "abs": "https://arxiv.org/abs/2504.16479", "authors": ["Yujie Qin", "Ming He", "Changyong Yu", "Ming Ni", "Xian Liu", "Xiaochen Bo"], "title": "The Dance of Atoms-De Novo Protein Design with Diffusion Model", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "The de novo design of proteins refers to creating proteins with specific\nstructures and functions that do not naturally exist. In recent years, the\naccumulation of high-quality protein structure and sequence data and\ntechnological advancements have paved the way for the successful application of\ngenerative artificial intelligence (AI) models in protein design. These models\nhave surpassed traditional approaches that rely on fragments and\nbioinformatics. They have significantly enhanced the success rate of de novo\nprotein design, and reduced experimental costs, leading to breakthroughs in the\nfield. Among various generative AI models, diffusion models have yielded the\nmost promising results in protein design. In the past two to three years, more\nthan ten protein design models based on diffusion models have emerged. Among\nthem, the representative model, RFDiffusion, has demonstrated success rates in\n25 protein design tasks that far exceed those of traditional methods, and other\nAI-based approaches like RFjoint and hallucination. This review will\nsystematically examine the application of diffusion models in generating\nprotein backbones and sequences. We will explore the strengths and limitations\nof different models, summarize successful cases of protein design using\ndiffusion models, and discuss future development directions.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u6a21\u578b\uff0c\u5c24\u5176\u662f\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u86cb\u767d\u8d28\u4ece\u5934\u8bbe\u8ba1\u7684\u6210\u529f\u7387\uff0c\u964d\u4f4e\u4e86\u5b9e\u9a8c\u6210\u672c\u3002\u4ee3\u8868\u6027\u6a21\u578bRFDiffusion\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5229\u7528\u9ad8\u8d28\u91cf\u86cb\u767d\u8d28\u7ed3\u6784\u548c\u5e8f\u5217\u6570\u636e\uff0c\u7ed3\u5408\u751f\u6210\u5f0fAI\u6280\u672f\uff0c\u7a81\u7834\u4f20\u7edf\u86cb\u767d\u8d28\u8bbe\u8ba1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u6269\u6563\u6a21\u578b\u7528\u4e8e\u751f\u6210\u86cb\u767d\u8d28\u9aa8\u67b6\u548c\u5e8f\u5217\uff0c\u5bf9\u6bd4\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\u3002", "result": "\u6269\u6563\u6a21\u578b\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5982RFDiffusion\u572825\u9879\u4efb\u52a1\u4e2d\u8fdc\u8d85\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u86cb\u767d\u8d28\u8bbe\u8ba1\u5e26\u6765\u7a81\u7834\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u63a2\u7d22\u5e94\u7528\u65b9\u5411\u3002"}}
{"id": "2504.16929", "pdf": "https://arxiv.org/pdf/2504.16929", "abs": "https://arxiv.org/abs/2504.16929", "authors": ["Shaden Alshammari", "John Hershey", "Axel Feldmann", "William T. Freeman", "Mark Hamilton"], "title": "I-Con: A Unifying Framework for Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IT", "math.IT"], "comment": "ICLR 2025; website: https://aka.ms/i-con . Proceedings of the\n  Thirteenth International Conference on Learning Representations (ICLR 2025)", "summary": "As the field of representation learning grows, there has been a proliferation\nof different loss functions to solve different classes of problems. We\nintroduce a single information-theoretic equation that generalizes a large\ncollection of modern loss functions in machine learning. In particular, we\nintroduce a framework that shows that several broad classes of machine learning\nmethods are precisely minimizing an integrated KL divergence between two\nconditional distributions: the supervisory and learned representations. This\nviewpoint exposes a hidden information geometry underlying clustering, spectral\nmethods, dimensionality reduction, contrastive learning, and supervised\nlearning. This framework enables the development of new loss functions by\ncombining successful techniques from across the literature. We not only present\na wide array of proofs, connecting over 23 different approaches, but we also\nleverage these theoretical results to create state-of-the-art unsupervised\nimage classifiers that achieve a +8% improvement over the prior\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\ndemonstrate that I-Con can be used to derive principled debiasing methods which\nimprove contrastive representation learners.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u635f\u5931\u51fd\u6570\uff0c\u63ed\u793a\u4e86\u805a\u7c7b\u3001\u8c31\u65b9\u6cd5\u3001\u964d\u7ef4\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u76d1\u7763\u5b66\u4e60\u80cc\u540e\u7684\u4fe1\u606f\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u65e0\u76d1\u7763\u56fe\u50cf\u5206\u7c7b\u7684\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u968f\u7740\u8868\u793a\u5b66\u4e60\u9886\u57df\u7684\u53d1\u5c55\uff0c\u51fa\u73b0\u4e86\u5927\u91cf\u9488\u5bf9\u4e0d\u540c\u95ee\u9898\u7684\u635f\u5931\u51fd\u6570\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e00\u4e2a\u7edf\u4e00\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u6982\u62ec\u8fd9\u4e9b\u635f\u5931\u51fd\u6570\uff0c\u63ed\u793a\u5176\u5171\u6027\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u4fe1\u606f\u8bba\u65b9\u7a0b\uff0c\u5c06\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7edf\u4e00\u4e3a\u6700\u5c0f\u5316\u4e24\u4e2a\u6761\u4ef6\u5206\u5e03\u4e4b\u95f4\u7684KL\u6563\u5ea6\uff0c\u5e76\u57fa\u4e8e\u6b64\u6846\u67b6\u5f00\u53d1\u65b0\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u8fde\u63a5\u4e8623\u79cd\u4e0d\u540c\u65b9\u6cd5\uff0c\u5e76\u5728\u65e0\u76d1\u7763\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e868%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u6539\u8fdb\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u7edf\u4e00\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u65b0\u635f\u5931\u51fd\u6570\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2504.16485", "pdf": "https://arxiv.org/pdf/2504.16485", "abs": "https://arxiv.org/abs/2504.16485", "authors": ["Syed Mohammad Kashif", "Peng Liang", "Amjed Tahir"], "title": "On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices", "categories": ["cs.SE", "cs.AI"], "comment": "35 pages, 17 images, 8 tables, Manuscript submitted to a journal\n  (2025)", "summary": "AI code generation tools have gained significant popularity among developers,\nwho use them to assist in software development due to their capability to\ngenerate code. Existing studies mainly explored the quality, e.g., correctness\nand security, of AI-generated code, while in real-world software development,\nthe prerequisite is to distinguish AI-generated code from human-written code,\nwhich emphasizes the need to explicitly declare AI-generated code by\ndevelopers. To this end, this study intends to understand the ways developers\nuse to self-declare AI-generated code and explore the reasons why developers\nchoose to self-declare or not. We conducted a mixed-methods study consisting of\ntwo phases. In the first phase, we mined GitHub repositories and collected 613\ninstances of AI-generated code snippets. In the second phase, we conducted a\nfollow-up industrial survey, which received 111 valid responses. Our research\nrevealed the practices followed by developers to self-declare AI-generated\ncode. Most practitioners (76.6%) always or sometimes self-declare AI-generated\ncode. In contrast, other practitioners (23.4%) noted that they never\nself-declare AI-generated code. The reasons for self-declaring AI-generated\ncode include the need to track and monitor the code for future review and\ndebugging, and ethical considerations. The reasons for not self-declaring\nAI-generated code include extensive modifications to AI-generated code and the\ndevelopers' perception that self-declaration is an unnecessary activity. We\nfinally provided guidelines for practitioners to self-declare AI-generated\ncode, addressing ethical and code quality concerns.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5f00\u53d1\u8005\u5982\u4f55\u81ea\u6211\u58f0\u660eAI\u751f\u6210\u4ee3\u7801\u53ca\u5176\u539f\u56e0\uff0c\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u53d1\u73b0\u591a\u6570\u5f00\u53d1\u8005\u4f1a\u58f0\u660e\uff0c\u5e76\u5206\u6790\u4e86\u58f0\u660e\u4e0e\u4e0d\u58f0\u660e\u7684\u539f\u56e0\u3002", "motivation": "\u73b0\u5b9e\u5f00\u53d1\u4e2d\u9700\u533a\u5206AI\u751f\u6210\u4e0e\u4eba\u5de5\u4ee3\u7801\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u4ee3\u7801\u8d28\u91cf\u800c\u975e\u58f0\u660e\u884c\u4e3a\uff0c\u56e0\u6b64\u63a2\u7d22\u5f00\u53d1\u8005\u81ea\u6211\u58f0\u660e\u7684\u65b9\u5f0f\u4e0e\u52a8\u673a\u3002", "method": "\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff1a1) \u6316\u6398GitHub\u4ed3\u5e93\u6536\u96c6613\u4e2aAI\u751f\u6210\u4ee3\u7801\u5b9e\u4f8b\uff1b2) \u5de5\u4e1a\u8c03\u67e5\u83b7\u53d6111\u4efd\u6709\u6548\u53cd\u9988\u3002", "result": "76.6%\u5f00\u53d1\u8005\u4f1a\u58f0\u660eAI\u751f\u6210\u4ee3\u7801\uff0c\u539f\u56e0\u5305\u62ec\u8ffd\u8e2a\u8c03\u8bd5\u548c\u4f26\u7406\u8003\u91cf\uff1b23.4%\u4e0d\u58f0\u660e\uff0c\u56e0\u4ee3\u7801\u4fee\u6539\u591a\u6216\u8ba4\u4e3a\u58f0\u660e\u4e0d\u5fc5\u8981\u3002", "conclusion": "\u63d0\u4f9b\u5b9e\u8df5\u6307\u5357\uff0c\u5f3a\u8c03\u58f0\u660eAI\u751f\u6210\u4ee3\u7801\u5bf9\u4f26\u7406\u4e0e\u4ee3\u7801\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.16489", "pdf": "https://arxiv.org/pdf/2504.16489", "abs": "https://arxiv.org/abs/2504.16489", "authors": ["Senmao Qi", "Yifei Zou", "Peng Li", "Ziyi Lin", "Xiuzhen Cheng", "Dongxiao Yu"], "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate", "categories": ["cs.CR", "cs.AI"], "comment": "33 pages, 5 figures", "summary": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\nHowever, the security implications of their iterative dialogues and\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\neliciting harmful content, remain critically underexplored. This paper\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\nand DeepSeek) without compromising internal agents. We introduce a novel\nstructured prompt-rewriting framework specifically designed to exploit MAD\ndynamics via narrative encapsulation, role-driven escalation, iterative\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\nthat MAD systems are inherently more vulnerable than single-agent setups.\nCrucially, our proposed attack methodology significantly amplifies this\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\nattack success rates as high as 80% in certain scenarios. These findings reveal\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\nfor robust, specialized defenses prior to real-world deployment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u6846\u67b6\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5176\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\u7684\u7279\u6027\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u91cd\u5199\u6846\u67b6\uff0c\u5b9e\u9a8c\u8868\u660eMAD\u7cfb\u7edf\u6bd4\u5355\u667a\u80fd\u4f53\u8bbe\u7f6e\u66f4\u8106\u5f31\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe80%\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u901a\u8fc7\u534f\u4f5c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u662f\u8d8a\u72f1\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u63d0\u793a\u91cd\u5199\u6846\u67b6\uff0c\u5229\u7528\u53d9\u4e8b\u5c01\u88c5\u3001\u89d2\u8272\u9a71\u52a8\u5347\u7ea7\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u4fee\u8f9e\u6df7\u6dc6\u6765\u653b\u51fbMAD\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMAD\u7cfb\u7edf\u6bd4\u5355\u667a\u80fd\u4f53\u66f4\u8106\u5f31\uff0c\u653b\u51fb\u6210\u529f\u7387\u8fbe80%\uff0c\u5371\u5bb3\u6027\u4ece28.14%\u63d0\u5347\u81f380.34%\u3002", "conclusion": "MAD\u67b6\u6784\u5b58\u5728\u56fa\u6709\u6f0f\u6d1e\uff0c\u9700\u5728\u90e8\u7f72\u524d\u5f00\u53d1\u4e13\u95e8\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2504.16537", "pdf": "https://arxiv.org/pdf/2504.16537", "abs": "https://arxiv.org/abs/2504.16537", "authors": ["Hong Ting Tsang", "Zihao Wang", "Yangqiu Song"], "title": "Transformers for Complex Query Answering over Knowledge Hypergraphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Complex Query Answering (CQA) has been extensively studied in recent years.\nIn order to model data that is closer to real-world distribution, knowledge\ngraphs with different modalities have been introduced. Triple KGs, as the\nclassic KGs composed of entities and relations of arity 2, have limited\nrepresentation of real-world facts. Real-world data is more sophisticated.\nWhile hyper-relational graphs have been introduced, there are limitations in\nrepresenting relationships of varying arity that contain entities with equal\ncontributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and\nM-FB15k-HCQA. Each dataset contains various query types that include logical\noperations such as projection, negation, conjunction, and disjunction. In order\nto answer knowledge hypergraph (KHG) existential first-order queries, we\npropose a two-stage transformer model, the Logical Knowledge Hypergraph\nTransformer (LKHGT), which consists of a Projection Encoder for atomic\nprojection and a Logical Encoder for complex logical operations. Both encoders\nare equipped with Type Aware Bias (TAB) for capturing token interactions.\nExperimental results on CQA datasets show that LKHGT is a state-of-the-art CQA\nmethod over KHG and is able to generalize to out-of-distribution query types.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u8d85\u56fe\uff08KHG\uff09\u7684\u590d\u6742\u67e5\u8be2\u56de\u7b54\u65b9\u6cd5LKHGT\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5Transformer\u6a21\u578b\u5904\u7406\u903b\u8f91\u64cd\u4f5c\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e09\u5143\u7ec4\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u96be\u4ee5\u8868\u793a\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u5c24\u5176\u662f\u5173\u7cfb\u591a\u6837\u6027\u548c\u5b9e\u4f53\u8d21\u732e\u5e73\u7b49\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faLKHGT\u6a21\u578b\uff0c\u5305\u542b\u6295\u5f71\u7f16\u7801\u5668\u548c\u903b\u8f91\u7f16\u7801\u5668\uff0c\u91c7\u7528\u7c7b\u578b\u611f\u77e5\u504f\u7f6e\uff08TAB\uff09\u6355\u6349\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLKHGT\u5728KHG\u4e0a\u7684\u590d\u6742\u67e5\u8be2\u56de\u7b54\u6027\u80fd\u6700\u4f18\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u5206\u5e03\u5916\u67e5\u8be2\u7c7b\u578b\u3002", "conclusion": "LKHGT\u4e3a\u77e5\u8bc6\u8d85\u56fe\u4e0a\u7684\u590d\u6742\u67e5\u8be2\u56de\u7b54\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16548", "pdf": "https://arxiv.org/pdf/2504.16548", "abs": "https://arxiv.org/abs/2504.16548", "authors": ["Lirui Guo", "Michael G. Burke", "Wynita M. Griggs"], "title": "Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "There has been extensive prior work exploring how psychological factors such\nas anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).\nHowever, limited research has been conducted on how prompt strategies in large\nlanguage model (LLM)-powered SAV User Interfaces (UIs) affect users'\nperceptions, experiences, and intentions to adopt such technology. In this\nwork, we investigate how conversational UIs powered by LLMs drive these\npsychological factors and psychological ownership, the sense of possession a\nuser may come to feel towards an entity or object they may not legally own. We\ndesigned four SAV UIs with varying levels of anthropomorphic characteristics\nand psychological ownership triggers. Quantitative measures of psychological\nownership, anthropomorphism, quality of service, disclosure tendency, sentiment\nof SAV responses, and overall acceptance were collected after participants\ninteracted with each SAV. Qualitative feedback was also gathered regarding the\nexperience of psychological ownership during the interactions. The results\nindicate that an SAV conversational UI designed to be more anthropomorphic and\nto induce psychological ownership improved users' perceptions of the SAV's\nhuman-like qualities and improved the sentiment of responses compared to a\ncontrol condition. These findings provide practical guidance for designing\nLLM-based conversational UIs that enhance user experience and adoption of SAVs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLM\u9a71\u52a8\u7684\u5171\u4eab\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08SAV\uff09\u7528\u6237\u754c\u9762\uff08UI\uff09\u4e2d\u63d0\u793a\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u7528\u6237\u611f\u77e5\u3001\u4f53\u9a8c\u548c\u91c7\u7528\u610f\u56fe\uff0c\u53d1\u73b0\u66f4\u5177\u62df\u4eba\u5316\u548c\u5fc3\u7406\u6240\u6709\u6743\u89e6\u53d1\u7684UI\u80fd\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u63a5\u53d7\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8LLM\u9a71\u52a8\u7684SAV UI\u4e2d\u63d0\u793a\u7b56\u7565\u5bf9\u7528\u6237\u5fc3\u7406\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5fc3\u7406\u6240\u6709\u6743\u548c\u62df\u4eba\u5316\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u79cd\u5177\u6709\u4e0d\u540c\u62df\u4eba\u5316\u7279\u5f81\u548c\u5fc3\u7406\u6240\u6709\u6743\u89e6\u53d1\u5668\u7684SAV UI\uff0c\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u6cd5\u6536\u96c6\u7528\u6237\u53cd\u9988\u3002", "result": "\u62df\u4eba\u5316\u548c\u5fc3\u7406\u6240\u6709\u6743\u89e6\u53d1\u7684UI\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u5bf9SAV\u7684\u62df\u4eba\u5316\u611f\u77e5\u548c\u60c5\u611f\u53cd\u9988\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548cSAV\u91c7\u7528\u7387\u7684LLM\u5bf9\u8bddUI\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2504.16562", "pdf": "https://arxiv.org/pdf/2504.16562", "abs": "https://arxiv.org/abs/2504.16562", "authors": ["Julian Rasch", "Florian M\u00fcller", "Francesco Chiossi"], "title": "A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Augmented Reality (AR) is transforming the way we interact with virtual\ninformation in the physical world. By overlaying digital content in real-world\nenvironments, AR enables new forms of immersive and engaging experiences.\nHowever, existing AR systems often struggle to effectively manage the many\ninteractive possibilities that AR presents. This vision paper speculates on\nAI-driven approaches for adaptive AR content placement, dynamically adjusting\nto user movement and environmental changes. By leveraging machine learning\nmethods, such a system would intelligently manage content distribution between\nAR projections integrated into the external environment and fixed static\ncontent, enabling seamless UI layout and potentially reducing users' cognitive\nload. By exploring the possibilities of AI-driven dynamic AR content placement,\nwe aim to envision new opportunities for innovation and improvement in various\nindustries, from urban navigation and workplace productivity to immersive\nlearning and beyond. This paper outlines a vision for the development of more\nintuitive, engaging, and effective AI-powered AR experiences.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u9a71\u52a8\u7684\u52a8\u6001AR\u5185\u5bb9\u5e03\u5c40\uff0c\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4f18\u5316AR\u4f53\u9a8c\uff0c\u51cf\u5c11\u7528\u6237\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u73b0\u6709AR\u7cfb\u7edf\u5728\u7ba1\u7406\u4ea4\u4e92\u53ef\u80fd\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u5185\u5bb9\u5e03\u5c40\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5229\u7528\u673a\u5668\u5b66\u4e60\u52a8\u6001\u8c03\u6574AR\u5185\u5bb9\uff0c\u9002\u5e94\u73af\u5883\u548c\u7528\u6237\u884c\u4e3a\u3002", "result": "\u8bbe\u60f3\u4e86\u4e00\u79cd\u66f4\u76f4\u89c2\u3001\u9ad8\u6548\u7684AI\u9a71\u52a8AR\u7cfb\u7edf\u3002", "conclusion": "\u5c55\u671b\u4e86AI\u9a71\u52a8AR\u5728\u5404\u884c\u4e1a\u7684\u521b\u65b0\u6f5c\u529b\u3002"}}
{"id": "2504.16573", "pdf": "https://arxiv.org/pdf/2504.16573", "abs": "https://arxiv.org/abs/2504.16573", "authors": ["Xianghe Liu", "Jiaqi Xu", "Tao Sun"], "title": "PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Psychological counseling is a highly personalized and dynamic process that\nrequires therapists to continuously monitor emotional changes, document session\ninsights, and maintain therapeutic continuity. In this paper, we introduce\nPsyCounAssist, a comprehensive AI-powered counseling assistant system\nspecifically designed to augment psychological counseling practices.\nPsyCounAssist integrates multimodal emotion recognition combining speech and\nphotoplethysmography (PPG) signals for accurate real-time affective analysis,\nautomated structured session reporting using large language models (LLMs), and\npersonalized AI-generated follow-up support. Deployed on Android-based tablet\ndevices, the system demonstrates practical applicability and flexibility in\nreal-world counseling scenarios. Experimental evaluation confirms the\nreliability of PPG-based emotional classification and highlights the system's\npotential for non-intrusive, privacy-aware emotional support. PsyCounAssist\nrepresents a novel approach to ethically and effectively integrating AI into\npsychological counseling workflows.", "AI": {"tldr": "PsyCounAssist\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u5fc3\u7406\u54a8\u8be2\u52a9\u624b\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u548c\u81ea\u52a8\u5316\u62a5\u544a\u589e\u5f3a\u5fc3\u7406\u54a8\u8be2\u5b9e\u8df5\u3002", "motivation": "\u5fc3\u7406\u54a8\u8be2\u662f\u4e2a\u6027\u5316\u548c\u52a8\u6001\u7684\u8fc7\u7a0b\uff0c\u9700\u8981\u5b9e\u65f6\u76d1\u63a7\u60c5\u7eea\u53d8\u5316\u548c\u8bb0\u5f55\u4f1a\u8bdd\u5185\u5bb9\uff0cAI\u53ef\u4ee5\u8f85\u52a9\u63d0\u5347\u6548\u7387\u548c\u8fde\u7eed\u6027\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u8bed\u97f3\u548cPPG\u4fe1\u53f7\u8fdb\u884c\u5b9e\u65f6\u60c5\u611f\u5206\u6790\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u968f\u8bbf\u652f\u6301\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PPG\u60c5\u611f\u5206\u7c7b\u7684\u53ef\u9760\u6027\uff0c\u7cfb\u7edf\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5b9e\u7528\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002", "conclusion": "PsyCounAssist\u4e3aAI\u5728\u5fc3\u7406\u54a8\u8be2\u4e2d\u7684\u4f26\u7406\u548c\u6709\u6548\u6574\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.16574", "pdf": "https://arxiv.org/pdf/2504.16574", "abs": "https://arxiv.org/abs/2504.16574", "authors": ["Lizhe Chen", "Binjia Zhou", "Yuyao Ge", "Jiayi Chen", "Shiguang NI"], "title": "PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress, demonstrating\nunprecedented capabilities across various natural language processing tasks.\nHowever, the high costs associated with such exceptional performance limit the\nwidespread adoption of LLMs, highlighting the need for prompt compression.\nExisting prompt compression methods primarily rely on heuristic truncation or\nabstractive summarization techniques, which fundamentally overlook the\nintrinsic mechanisms of LLMs and lack a systematic evaluation of token\nimportance for generation. In this work, we introduce Prompt Importance\nSampling (PIS), a novel compression framework that dynamically compresses\nprompts by sampling important tokens based on the analysis of attention scores\nof hidden states. PIS employs a dual-level compression mechanism: 1) at the\ntoken level, we quantify saliency using LLM-native attention scores and\nimplement adaptive compression through a lightweight 9-layer reinforcement\nlearning (RL) network; 2) at the semantic level, we propose a Russian roulette\nsampling strategy for sentence-level importance sampling. Comprehensive\nevaluations across multiple domain benchmarks demonstrate that our method\nachieves state-of-the-art compression performance. Notably, our framework\nserendipitously enhances reasoning efficiency through optimized context\nstructuring. This work advances prompt engineering by offering both theoretical\ngrounding and practical efficiency in context management for LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPrompt Importance Sampling (PIS)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u91c7\u6837\u91cd\u8981\u6807\u8bb0\u6765\u538b\u7f29\u63d0\u793a\uff0c\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u6210\u672c\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86LLMs\u7684\u5185\u5728\u673a\u5236\uff0c\u7f3a\u4e4f\u5bf9\u6807\u8bb0\u91cd\u8981\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "PIS\u901a\u8fc7\u5206\u6790\u9690\u85cf\u72b6\u6001\u7684\u6ce8\u610f\u529b\u5206\u6570\u52a8\u6001\u538b\u7f29\u63d0\u793a\uff0c\u91c7\u7528\u53cc\u7ea7\u538b\u7f29\u673a\u5236\uff1a\u6807\u8bb0\u7ea7\u522b\u4f7f\u7528\u6ce8\u610f\u529b\u5206\u6570\u91cf\u5316\u663e\u8457\u6027\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u7f51\u7edc\u5b9e\u73b0\u81ea\u9002\u5e94\u538b\u7f29\uff1b\u8bed\u4e49\u7ea7\u522b\u91c7\u7528\u4fc4\u7f57\u65af\u8f6e\u76d8\u8d4c\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPIS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u538b\u7f29\u6027\u80fd\uff0c\u5e76\u610f\u5916\u5730\u901a\u8fc7\u4f18\u5316\u4e0a\u4e0b\u6587\u7ed3\u6784\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "PIS\u4e3aLLMs\u7684\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6548\u7387\uff0c\u63a8\u52a8\u4e86\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.16576", "pdf": "https://arxiv.org/pdf/2504.16576", "abs": "https://arxiv.org/abs/2504.16576", "authors": ["Xu Guo", "Tong Zhang", "Fuyun Wang", "Xudong Wang", "Xiaoya Zhang", "Xin Liu", "Zhen Cui"], "title": "MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "23 pages, 8 figures. This manuscript is currently under major\n  revision for ACM Transactions on Multimedia Computing, Communications, and\n  Applications (ACM TOMM)", "summary": "The burgeoning presence of multimodal content-sharing platforms propels the\ndevelopment of personalized recommender systems. Previous works usually suffer\nfrom data sparsity and cold-start problems, and may fail to adequately explore\nsemantic user-product associations from multimodal data. To address these\nissues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL)\nframework for user recommendation. For a comprehensive information exploration\nfrom user-product relations, we construct two hypergraphs, i.e. a user-to-user\n(u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared\npreferences among users and intricate multimodal semantic resemblance among\nitems, respectively. This process yields denser second-order semantics that are\nfused with first-order user-item interaction as complementary to alleviate the\ndata sparsity issue. Then, we design a contrastive feature enhancement paradigm\nby applying synergistic contrastive learning. By maximizing/minimizing the\nmutual information between second-order (e.g. shared preference pattern for\nusers) and first-order (information of selected items for users) embeddings of\nthe same/different users and items, the feature distinguishability can be\neffectively enhanced. Compared with using sparse primary user-item interaction\nonly, our MMHCL obtains denser second-order hypergraphs and excavates more\nabundant shared attributes to explore the user-product associations, which to a\ncertain extent alleviates the problems of data sparsity and cold-start.\nExtensive experiments have comprehensively demonstrated the effectiveness of\nour method. Our code is publicly available at: https://github.com/Xu107/MMHCL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8d85\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08MMHCL\uff09\uff0c\u901a\u8fc7\u6784\u5efa\u7528\u6237\u548c\u7269\u54c1\u7684\u8d85\u56fe\u6765\u6316\u6398\u5171\u4eab\u504f\u597d\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u7f13\u89e3\u6570\u636e\u7a00\u758f\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5185\u5bb9\u5171\u4eab\u5e73\u53f0\u7684\u5174\u8d77\u63a8\u52a8\u4e86\u4e2a\u6027\u5316\u63a8\u8350\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u7a00\u758f\u548c\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u4e14\u672a\u80fd\u5145\u5206\u6316\u6398\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7684\u8bed\u4e49\u5173\u8054\u3002", "method": "\u6784\u5efa\u7528\u6237\u5230\u7528\u6237\uff08u2u\uff09\u548c\u7269\u54c1\u5230\u7269\u54c1\uff08i2i\uff09\u8d85\u56fe\uff0c\u6316\u6398\u5171\u4eab\u504f\u597d\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\uff1b\u8bbe\u8ba1\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\u589e\u5f3a\u7279\u5f81\u533a\u5206\u6027\u3002", "result": "MMHCL\u901a\u8fc7\u6316\u6398\u66f4\u4e30\u5bcc\u7684\u5171\u4eab\u5c5e\u6027\u548c\u6784\u5efa\u66f4\u5bc6\u96c6\u7684\u8d85\u56fe\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u7a00\u758f\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u4e86MMHCL\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.16584", "pdf": "https://arxiv.org/pdf/2504.16584", "abs": "https://arxiv.org/abs/2504.16584", "authors": ["Md. Azizul Hakim Bappy", "Hossen A Mustafa", "Prottoy Saha", "Rajinus Salehat"], "title": "Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 2 figures, 3 tables. Dataset available at\n  https://huggingface.co/datasets/floxihunter/synthetic_python_cwe. Model\n  available at https://huggingface.co/floxihunter/codegen-mono-CWEdetect.\n  Keywords: Small Language Models (SLMs), Vulnerability Detection, CWE,\n  Fine-tuning, Python Security, Privacy-Preserving Code Analysis", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and analyzing code for security vulnerabilities, such as Common\nWeakness Enumerations (CWEs). However, their reliance on cloud infrastructure\nand substantial computational requirements pose challenges for analyzing\nsensitive or proprietary codebases due to privacy concerns and inference costs.\nThis work explores the potential of Small Language Models (SLMs) as a viable\nalternative for accurate, on-premise vulnerability detection. We investigated\nwhether a 350-million parameter pre-trained code model (codegen-mono) could be\neffectively fine-tuned to detect the MITRE Top 25 CWEs specifically within\nPython code. To facilitate this, we developed a targeted dataset of 500\nexamples using a semi-supervised approach involving LLM-driven synthetic data\ngeneration coupled with meticulous human review. Initial tests confirmed that\nthe base codegen-mono model completely failed to identify CWEs in our samples.\nHowever, after applying instruction-following fine-tuning, the specialized SLM\nachieved remarkable performance on our test set, yielding approximately 99%\naccuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results\nstrongly suggest that fine-tuned SLMs can serve as highly accurate and\nefficient tools for CWE detection, offering a practical and privacy-preserving\nsolution for integrating advanced security analysis directly into development\nworkflows.", "AI": {"tldr": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u901a\u8fc7\u5fae\u8c03\u53ef\u4ee5\u9ad8\u6548\u68c0\u6d4b\u4ee3\u7801\u6f0f\u6d1e\uff0c\u66ff\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u51b3\u9690\u79c1\u548c\u6210\u672c\u95ee\u9898\u3002", "motivation": "LLMs\u4f9d\u8d56\u4e91\u7aef\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u5408\u654f\u611f\u6216\u4e13\u6709\u4ee3\u7801\u5e93\u7684\u5b89\u5168\u5206\u6790\uff0cSLMs\u53ef\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528350M\u53c2\u6570\u7684\u9884\u8bad\u7ec3\u4ee3\u7801\u6a21\u578b\uff08codegen-mono\uff09\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u65b9\u6cd5\u751f\u6210\u6570\u636e\u96c6\u5e76\u5fae\u8c03\uff0c\u68c0\u6d4bPython\u4ee3\u7801\u4e2d\u7684MITRE Top 25 CWEs\u3002", "result": "\u5fae\u8c03\u540e\u7684SLM\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u51c6\u786e\u738799%\uff0c\u7cbe\u786e\u738798.08%\uff0c\u53ec\u56de\u7387100%\uff0cF1\u5206\u657099.04%\u3002", "conclusion": "\u5fae\u8c03SLMs\u662f\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684CWE\u68c0\u6d4b\u5de5\u5177\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u5f00\u53d1\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2504.16601", "pdf": "https://arxiv.org/pdf/2504.16601", "abs": "https://arxiv.org/abs/2504.16601", "authors": ["Andy Li", "Wei Zhou", "Rashina Hoda", "Chris Bain", "Peter Poon"], "title": "Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 2 tables and 1 Figure", "summary": "This study evaluates how well large language models (LLMs) and traditional\nmachine translation (MT) tools translate medical consultation summaries from\nEnglish into Arabic, Chinese, and Vietnamese. It assesses both patient,\nfriendly and clinician, focused texts using standard automated metrics. Results\nshowed that traditional MT tools generally performed better, especially for\ncomplex texts, while LLMs showed promise, particularly in Vietnamese and\nChinese, when translating simpler summaries. Arabic translations improved with\ncomplexity due to the language's morphology. Overall, while LLMs offer\ncontextual flexibility, they remain inconsistent, and current evaluation\nmetrics fail to capture clinical relevance. The study highlights the need for\ndomain-specific training, improved evaluation methods, and human oversight in\nmedical translation.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4f20\u7edf\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u5de5\u5177\u5728\u533b\u5b66\u54a8\u8be2\u6458\u8981\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4f20\u7edf\u5de5\u5177\u8868\u73b0\u66f4\u4f18\uff0c\u4f46LLM\u5728\u7b80\u5355\u6587\u672c\u7ffb\u8bd1\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\u3002", "motivation": "\u8bc4\u4f30LLM\u548c\u4f20\u7edfMT\u5de5\u5177\u5728\u533b\u5b66\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u6539\u8fdb\u7ffb\u8bd1\u5de5\u5177\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u81ea\u52a8\u5316\u6307\u6807\u8bc4\u4f30LLM\u548c\u4f20\u7edfMT\u5de5\u5177\u5bf9\u82f1\u6587\u533b\u5b66\u6458\u8981\u7ffb\u8bd1\u6210\u963f\u62c9\u4f2f\u8bed\u3001\u4e2d\u6587\u548c\u8d8a\u5357\u8bed\u7684\u6548\u679c\u3002", "result": "\u4f20\u7edfMT\u5de5\u5177\u8868\u73b0\u66f4\u597d\uff0c\u5c24\u5176\u662f\u590d\u6742\u6587\u672c\uff1bLLM\u5728\u7b80\u5355\u6587\u672c\u7ffb\u8bd1\u4e2d\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "conclusion": "\u9700\u8981\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u3001\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u53ca\u4eba\u5de5\u76d1\u7763\u4ee5\u63d0\u9ad8\u533b\u5b66\u7ffb\u8bd1\u8d28\u91cf\u3002"}}
{"id": "2504.16604", "pdf": "https://arxiv.org/pdf/2504.16604", "abs": "https://arxiv.org/abs/2504.16604", "authors": ["Mareike Lisker", "Christina Gottschalk", "Helena Mihaljevi\u0107"], "title": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories", "categories": ["cs.CL", "cs.AI", "cs.SI", "I.2.7"], "comment": "15 pages", "summary": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\u3001Llama 3\u548cMistral\uff09\u751f\u6210\u9488\u5bf9\u9634\u8c0b\u8bba\u7684\u5bf9\u6297\u6027\u8a00\u8bba\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u901a\u5e38\u6cdb\u6cdb\u3001\u91cd\u590d\u6216\u80a4\u6d45\uff0c\u4e14\u5b58\u5728\u8fc7\u5ea6\u627f\u8ba4\u6050\u60e7\u548c\u865a\u6784\u4e8b\u5b9e\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4e13\u5bb6\u9a71\u52a8\u7684\u5bf9\u6297\u6027\u8a00\u8bba\u96be\u4ee5\u89c4\u6a21\u5316\u7684\u95ee\u9898\uff0c\u5e76\u586b\u8865\u9488\u5bf9\u9634\u8c0b\u8bba\u7684\u5bf9\u6297\u6027\u8a00\u8bba\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u8bc4\u4f30GPT-4o\u3001Llama 3\u548cMistral\u5728\u751f\u6210\u5bf9\u6297\u6027\u8a00\u8bba\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002", "result": "\u6a21\u578b\u751f\u6210\u7684\u5bf9\u6297\u6027\u8a00\u8bba\u901a\u5e38\u6cdb\u6cdb\u3001\u91cd\u590d\u6216\u80a4\u6d45\uff0c\u4e14\u5b58\u5728\u8fc7\u5ea6\u627f\u8ba4\u6050\u60e7\u548c\u865a\u6784\u4e8b\u5b9e\u7684\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u63d0\u5347\u5bf9\u6297\u6027\u8a00\u8bba\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2504.16651", "pdf": "https://arxiv.org/pdf/2504.16651", "abs": "https://arxiv.org/abs/2504.16651", "authors": ["William Corrias", "Fabio De Gaspari", "Dorjan Hitaj", "Luigi V. Mancini"], "title": "MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of generative models has led to their integration across\nvarious fields, including password guessing, aiming to generate passwords that\nresemble human-created ones in complexity, structure, and patterns. Despite\ngenerative model's promise, inconsistencies in prior research and a lack of\nrigorous evaluation have hindered a comprehensive understanding of their true\npotential. In this paper, we introduce MAYA, a unified, customizable,\nplug-and-play password benchmarking framework. MAYA provides a standardized\napproach for evaluating generative password-guessing models through a rigorous\nset of advanced testing scenarios and a collection of eight real-life password\ndatasets. Using MAYA, we comprehensively evaluate six state-of-the-art\napproaches, which have been re-implemented and adapted to ensure\nstandardization, for a total of over 15,000 hours of computation. Our findings\nindicate that these models effectively capture different aspects of human\npassword distribution and exhibit strong generalization capabilities. However,\ntheir effectiveness varies significantly with long and complex passwords.\nThrough our evaluation, sequential models consistently outperform other\ngenerative architectures and traditional password-guessing tools, demonstrating\nunique capabilities in generating accurate and complex guesses. Moreover,\nmodels learn and generate different password distributions, enabling a\nmulti-model attack that outperforms the best individual model. By releasing\nMAYA, we aim to foster further research, providing the community with a new\ntool to consistently and reliably benchmark password-generation techniques. Our\nframework is publicly available at\nhttps://github.com/williamcorrias/MAYA-Password-Benchmarking", "AI": {"tldr": "MAYA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5bc6\u7801\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0f\u5bc6\u7801\u731c\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5e8f\u5217\u6a21\u578b\u5728\u751f\u6210\u590d\u6742\u5bc6\u7801\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u5bc6\u7801\u731c\u6d4b\u9886\u57df\u7684\u5e94\u7528\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\uff0c\u963b\u788d\u4e86\u5bf9\u5176\u5b9e\u7528\u6027\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "\u5f15\u5165MAYA\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6d4b\u8bd5\u573a\u666f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u8bc4\u4f30\u516d\u79cd\u5148\u8fdb\u6a21\u578b\u3002", "result": "\u5e8f\u5217\u6a21\u578b\u5728\u751f\u6210\u590d\u6742\u5bc6\u7801\u65b9\u9762\u8868\u73b0\u6700\u4f18\uff0c\u591a\u6a21\u578b\u653b\u51fb\u6548\u679c\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u3002", "conclusion": "MAYA\u4e3a\u5bc6\u7801\u751f\u6210\u6280\u672f\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.16677", "pdf": "https://arxiv.org/pdf/2504.16677", "abs": "https://arxiv.org/abs/2504.16677", "authors": ["Luisa Shimabucoro", "Ahmet Ustun", "Marzieh Fadaee", "Sebastian Ruder"], "title": "A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In order for large language models to be useful across the globe, they are\nfine-tuned to follow instructions on multilingual data. Despite the ubiquity of\nsuch post-training, a clear understanding of the dynamics that enable\ncross-lingual transfer remains elusive. This study examines cross-lingual\ntransfer (CLT) dynamics in realistic post-training settings. We study two model\nfamilies of up to 35B parameters in size trained on carefully controlled\nmixtures of multilingual data on three generative tasks with varying levels of\ncomplexity (summarization, instruction following, and mathematical reasoning)\nin both single-task and multi-task instruction tuning settings. Overall, we\nfind that the dynamics of cross-lingual transfer and multilingual performance\ncannot be explained by isolated variables, varying depending on the combination\nof post-training settings. Finally, we identify the conditions that lead to\neffective cross-lingual transfer in practice.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00\u6570\u636e\u5fae\u8c03\u540e\u5927\u8bed\u8a00\u6a21\u578b\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u52a8\u6001\uff0c\u53d1\u73b0\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u591a\u79cd\u56e0\u7d20\u7ec4\u5408\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u9645\u6709\u6548\u7684\u8fc1\u79fb\u6761\u4ef6\u3002", "motivation": "\u7406\u89e3\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u52a8\u6001\u673a\u5236\uff0c\u4ee5\u4f18\u5316\u591a\u8bed\u8a00\u5927\u6a21\u578b\u7684\u5fae\u8c03\u6548\u679c\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u6a21\u578b\u5bb6\u65cf\uff08\u6700\u592735B\u53c2\u6570\uff09\u5728\u53d7\u63a7\u591a\u8bed\u8a00\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u7814\u7a76\u4e09\u79cd\u751f\u6210\u4efb\u52a1\uff08\u6458\u8981\u3001\u6307\u4ee4\u8ddf\u968f\u3001\u6570\u5b66\u63a8\u7406\uff09\u5728\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u5fae\u8c03\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u8de8\u8bed\u8a00\u8fc1\u79fb\u548c\u6027\u80fd\u65e0\u6cd5\u7531\u5355\u4e00\u53d8\u91cf\u89e3\u91ca\uff0c\u6548\u679c\u53d6\u51b3\u4e8e\u5fae\u8c03\u8bbe\u7f6e\u7684\u7ec4\u5408\u3002", "conclusion": "\u786e\u5b9a\u4e86\u5b9e\u8df5\u4e2d\u5b9e\u73b0\u6709\u6548\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u6761\u4ef6\u3002"}}
{"id": "2504.16680", "pdf": "https://arxiv.org/pdf/2504.16680", "abs": "https://arxiv.org/abs/2504.16680", "authors": ["Chenhao Li", "Andreas Krause", "Marco Hutter"], "title": "Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement Learning (RL) has demonstrated impressive capabilities in\nrobotic control but remains challenging due to high sample complexity, safety\nconcerns, and the sim-to-real gap. While offline RL eliminates the need for\nrisky real-world exploration by learning from pre-collected data, it suffers\nfrom distributional shift, limiting policy generalization. Model-Based RL\n(MBRL) addresses this by leveraging predictive models for synthetic rollouts,\nyet existing approaches often lack robust uncertainty estimation, leading to\ncompounding errors in offline settings. We introduce Offline Robotic World\nModel (RWM-O), a model-based approach that explicitly estimates epistemic\nuncertainty to improve policy learning without reliance on a physics simulator.\nBy integrating these uncertainty estimates into policy optimization, our\napproach penalizes unreliable transitions, reducing overfitting to model errors\nand enhancing stability. Experimental results show that RWM-O improves\ngeneralization and safety, enabling policy learning purely from real-world data\nand advancing scalable, data-efficient RL for robotics.", "AI": {"tldr": "RWM-O\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6765\u63d0\u5347\u7b56\u7565\u5b66\u4e60\uff0c\u65e0\u9700\u4f9d\u8d56\u7269\u7406\u6a21\u62df\u5668\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u9762\u4e34\u5206\u5e03\u504f\u79fb\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u800c\u73b0\u6709\u6a21\u578b\u65b9\u6cd5\u7f3a\u4e4f\u9c81\u68d2\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "method": "\u63d0\u51faRWM-O\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5e76\u5c06\u5176\u6574\u5408\u5230\u7b56\u7565\u4f18\u5316\u4e2d\uff0c\u51cf\u5c11\u5bf9\u6a21\u578b\u8bef\u5dee\u7684\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRWM-O\u63d0\u5347\u4e86\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b89\u5168\u6027\uff0c\u5b9e\u73b0\u4e86\u4ec5\u4ece\u771f\u5b9e\u6570\u636e\u4e2d\u5b66\u4e60\u7b56\u7565\u3002", "conclusion": "RWM-O\u4e3a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2504.16738", "pdf": "https://arxiv.org/pdf/2504.16738", "abs": "https://arxiv.org/abs/2504.16738", "authors": ["Itamar Mishani", "Yorai Shaoul", "Maxim Likhachev"], "title": "MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon Manipulation Planning", "categories": ["cs.RO", "cs.AI"], "comment": "Under review. Project page: https://skill-mosaic.github.io", "summary": "Planning long-horizon motions using a set of predefined skills is a key\nchallenge in robotics and AI. Addressing this challenge requires methods that\nsystematically explore skill combinations to uncover task-solving sequences,\nharness generic, easy-to-learn skills (e.g., pushing, grasping) to generalize\nacross unseen tasks, and bypass reliance on symbolic world representations that\ndemand extensive domain and task-specific knowledge. Despite significant\nprogress, these elements remain largely disjoint in existing approaches,\nleaving a critical gap in achieving robust, scalable solutions for complex,\nlong-horizon problems. In this work, we present MOSAIC, a skill-centric\nframework that unifies these elements by using the skills themselves to guide\nthe planning process. MOSAIC uses two families of skills: Generators compute\nexecutable trajectories and world configurations, and Connectors link these\nindependently generated skill trajectories by solving boundary value problems,\nenabling progress toward completing the overall task. By breaking away from the\nconventional paradigm of incrementally discovering skills from predefined start\nor goal states--a limitation that significantly restricts exploration--MOSAIC\nfocuses planning efforts on regions where skills are inherently effective. We\ndemonstrate the efficacy of MOSAIC in both simulated and real-world robotic\nmanipulation tasks, showcasing its ability to solve complex long-horizon\nplanning problems using a diverse set of skills incorporating generative\ndiffusion models, motion planning algorithms, and manipulation-specific models.\nVisit https://skill-mosaic.github.io for demonstrations and examples.", "AI": {"tldr": "MOSAIC\u662f\u4e00\u4e2a\u6280\u80fd\u4e2d\u5fc3\u6846\u67b6\uff0c\u901a\u8fc7\u6280\u80fd\u672c\u8eab\u6307\u5bfc\u89c4\u5212\u8fc7\u7a0b\uff0c\u89e3\u51b3\u957f\u671f\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u957f\u671f\u8fd0\u52a8\u89c4\u5212\u4e2d\u6280\u80fd\u7ec4\u5408\u63a2\u7d22\u3001\u901a\u7528\u6280\u80fd\u5229\u7528\u548c\u907f\u514d\u4f9d\u8d56\u7b26\u53f7\u4e16\u754c\u8868\u793a\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528Generators\u751f\u6210\u53ef\u6267\u884c\u8f68\u8ff9\u548c\u4e16\u754c\u914d\u7f6e\uff0cConnectors\u901a\u8fc7\u89e3\u51b3\u8fb9\u754c\u503c\u95ee\u9898\u94fe\u63a5\u6280\u80fd\u8f68\u8ff9\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u89e3\u51b3\u590d\u6742\u957f\u671f\u89c4\u5212\u95ee\u9898\u7684\u80fd\u529b\u3002", "conclusion": "MOSAIC\u901a\u8fc7\u6280\u80fd\u4e2d\u5fc3\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u957f\u671f\u89c4\u5212\u95ee\u9898\u7684\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16754", "pdf": "https://arxiv.org/pdf/2504.16754", "abs": "https://arxiv.org/abs/2504.16754", "authors": ["Kwangseob Ahn"], "title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) struggle with maintaining coherence in extended\nconversations spanning hundreds of turns, despite performing well within their\ncontext windows. This paper introduces HEMA (Hippocampus-Inspired Extended\nMemory Architecture), a dual-memory system inspired by human cognitive\nprocesses. HEMA combines Compact Memory - a continuously updated one-sentence\nsummary preserving global narrative coherence, and Vector Memory - an episodic\nstore of chunk embeddings queried via cosine similarity. When integrated with a\n6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns\nwhile keeping prompt length under 3,500 tokens. Experimental results show\nsubstantial improvements: factual recall accuracy increases from 41% to 87%,\nand human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K\nindexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling\nthe area under the precision-recall curve compared to summarization-only\napproaches. Ablation studies reveal two key insights: semantic forgetting\nthrough age-weighted pruning reduces retrieval latency by 34% with minimal\nrecall loss, and a two-level summary hierarchy prevents cascade errors in\nultra-long conversations exceeding 1,000 turns. HEMA demonstrates that\ncombining verbatim recall with semantic continuity provides a practical\nsolution for privacy-aware conversational AI capable of month-long dialogues\nwithout model retraining.", "AI": {"tldr": "HEMA\u662f\u4e00\u79cd\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u53cc\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u7d27\u51d1\u8bb0\u5fc6\u548c\u5411\u91cf\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u7684\u8fde\u8d2f\u6027\u548c\u4e8b\u5b9e\u56de\u5fc6\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u96be\u4ee5\u4fdd\u6301\u8fde\u8d2f\u6027\uff0cHEMA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HEMA\u7ed3\u5408\u7d27\u51d1\u8bb0\u5fc6\uff08\u6301\u7eed\u66f4\u65b0\u7684\u5355\u53e5\u6458\u8981\uff09\u548c\u5411\u91cf\u8bb0\u5fc6\uff08\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u67e5\u8be2\u7684\u5206\u5757\u5d4c\u5165\u5b58\u50a8\uff09\uff0c\u5e76\u4e0e6B\u53c2\u6570\u53d8\u538b\u5668\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cHEMA\u5728300\u8f6e\u5bf9\u8bdd\u4e2d\u4fdd\u6301\u8fde\u8d2f\u6027\uff0c\u4e8b\u5b9e\u56de\u5fc6\u51c6\u786e\u7387\u4ece41%\u63d0\u5347\u81f387%\uff0c\u4eba\u7c7b\u8bc4\u5206\u8fde\u8d2f\u6027\u4ece2.7\u63d0\u5347\u81f34.3\u3002", "conclusion": "HEMA\u901a\u8fc7\u7ed3\u5408\u9010\u5b57\u56de\u5fc6\u548c\u8bed\u4e49\u8fde\u7eed\u6027\uff0c\u4e3a\u9690\u79c1\u611f\u77e5\u7684\u5bf9\u8bddAI\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u957f\u8fbe\u4e00\u4e2a\u6708\u7684\u5bf9\u8bdd\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2504.16768", "pdf": "https://arxiv.org/pdf/2504.16768", "abs": "https://arxiv.org/abs/2504.16768", "authors": ["Waad Alhoshan", "Alessio Ferrari", "Liping Zhao"], "title": "How Effective are Generative Large Language Models in Performing Requirements Classification?", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "In recent years, transformer-based large language models (LLMs) have\nrevolutionised natural language processing (NLP), with generative models\nopening new possibilities for tasks that require context-aware text generation.\nRequirements engineering (RE) has also seen a surge in the experimentation of\nLLMs for different tasks, including trace-link detection, regulatory\ncompliance, and others. Requirements classification is a common task in RE.\nWhile non-generative LLMs like BERT have been successfully applied to this\ntask, there has been limited exploration of generative LLMs. This gap raises an\nimportant question: how well can generative LLMs, which produce context-aware\noutputs, perform in requirements classification? In this study, we explore the\neffectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing\nboth binary and multi-class requirements classification. We design an extensive\nexperimental study involving over 400 experiments across three widely used\ndatasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes\nthat while factors like prompt design and LLM architecture are universally\nimportant, others-such as dataset variations-have a more situational impact,\ndepending on the complexity of the classification task. This insight can guide\nfuture model development and deployment strategies, focusing on optimising\nprompt structures and aligning model architectures with task-specific needs for\nimproved performance.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Bloom\u3001Gemma\u548cLlama\uff09\u5728\u9700\u6c42\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7400\u591a\u6b21\u5b9e\u9a8c\u53d1\u73b0\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u67b6\u6784\u662f\u5173\u952e\u56e0\u7d20\uff0c\u800c\u6570\u636e\u96c6\u7684\u5f71\u54cd\u5219\u56e0\u4efb\u52a1\u590d\u6742\u5ea6\u800c\u5f02\u3002", "motivation": "\u9700\u6c42\u5de5\u7a0b\u4e2d\u751f\u6210\u5f0fLLMs\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30\u5176\u5728\u9700\u6c42\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u6db5\u76d6\u4e09\u4e2a\u6570\u636e\u96c6\uff08PROMISE NFR\u3001Functional-Quality\u548cSecReq\uff09\u7684400\u591a\u6b21\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u751f\u6210\u5f0fLLMs\u5728\u4e8c\u5143\u548c\u591a\u7c7b\u9700\u6c42\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u67b6\u6784\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u800c\u6570\u636e\u96c6\u7684\u5f71\u54cd\u5219\u56e0\u4efb\u52a1\u590d\u6742\u5ea6\u4e0d\u540c\u800c\u6709\u6240\u53d8\u5316\u3002", "conclusion": "\u672a\u6765\u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72\u5e94\u4f18\u5316\u63d0\u793a\u7ed3\u6784\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u9700\u6c42\u8c03\u6574\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2504.16778", "pdf": "https://arxiv.org/pdf/2504.16778", "abs": "https://arxiv.org/abs/2504.16778", "authors": ["Sarah Jabbour", "Trenton Chang", "Anindya Das Antar", "Joseph Peper", "Insu Jang", "Jiachen Liu", "Jae-Won Chung", "Shiqi He", "Michael Wellman", "Bryan Goodman", "Elizabeth Bondi-Kelly", "Kevin Samy", "Rada Mihalcea", "Mosharaf Chowhury", "David Jurgens", "Lu Wang"], "title": "Evaluation Framework for AI Systems in \"the Wild\"", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "35 pages", "summary": "Generative AI (GenAI) models have become vital across industries, yet current\nevaluation methods have not adapted to their widespread use. Traditional\nevaluations often rely on benchmarks and fixed datasets, frequently failing to\nreflect real-world performance, which creates a gap between lab-tested outcomes\nand practical applications. This white paper proposes a comprehensive framework\nfor how we should evaluate real-world GenAI systems, emphasizing diverse,\nevolving inputs and holistic, dynamic, and ongoing assessment approaches. The\npaper offers guidance for practitioners on how to design evaluation methods\nthat accurately reflect real-time capabilities, and provides policymakers with\nrecommendations for crafting GenAI policies focused on societal impacts, rather\nthan fixed performance numbers or parameter sizes. We advocate for holistic\nframeworks that integrate performance, fairness, and ethics and the use of\ncontinuous, outcome-oriented methods that combine human and automated\nassessments while also being transparent to foster trust among stakeholders.\nImplementing these strategies ensures GenAI models are not only technically\nproficient but also ethically responsible and impactful.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u52a8\u6001\u3001\u591a\u6837\u5316\u7684\u8f93\u5165\u548c\u6301\u7eed\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u5f25\u8865\u4f20\u7edf\u8bc4\u4f30\u4e0e\u771f\u5b9e\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u9002\u5e94GenAI\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u3001\u591a\u6837\u5316\u7684\u8f93\u5165\u548c\u6301\u7eed\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u5de5\u4e0e\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u6ce8\u91cd\u900f\u660e\u6027\u3002", "result": "\u4e3a\u4ece\u4e1a\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u8bc4\u4f30\u65b9\u6cd5\u548c\u5236\u5b9a\u653f\u7b56\u7684\u6307\u5bfc\uff0c\u5f3a\u8c03\u6027\u80fd\u3001\u516c\u5e73\u6027\u548c\u4f26\u7406\u3002", "conclusion": "\u5b9e\u65bd\u8be5\u6846\u67b6\u53ef\u786e\u4fddGenAI\u6a21\u578b\u6280\u672f\u719f\u7ec3\u3001\u4f26\u7406\u8d1f\u8d23\u4e14\u5177\u6709\u5b9e\u9645\u5f71\u54cd\u529b\u3002"}}
{"id": "2504.16787", "pdf": "https://arxiv.org/pdf/2504.16787", "abs": "https://arxiv.org/abs/2504.16787", "authors": ["Ningning Zhang", "Chi Zhang", "Zhizhong Tan", "Xingxing Yang", "Weiping Deng", "Wenyong Wang"], "title": "Credible plan-driven RAG method for Multi-hop Question Answering", "categories": ["cs.CL", "cs.AI", "I.2.0"], "comment": "18 pages, 3 figures", "summary": "Multi-hop question answering (QA) presents a considerable challenge for\nRetrieval-Augmented Generation (RAG), requiring the structured decomposition of\ncomplex queries into logical reasoning paths and the generation of dependable\nintermediate results. However, deviations in reasoning paths or errors in\nintermediate results, which are common in current RAG methods, may propagate\nand accumulate throughout the reasoning process, diminishing the accuracy of\nthe answer to complex queries. To address this challenge, we propose the\nPlan-then-Act-and-Review (PAR RAG) framework, which is organized into three key\nstages: planning, act, and review, and aims to offer an interpretable and\nincremental reasoning paradigm for accurate and reliable multi-hop question\nanswering by mitigating error propagation.PAR RAG initially applies a top-down\nproblem decomposition strategy, formulating a comprehensive plan that\nintegrates multiple executable steps from a holistic viewpoint. This approach\navoids the pitfalls of local optima common in traditional RAG methods, ensuring\nthe accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a\nplan execution mechanism based on multi-granularity verification. By utilizing\nboth coarse-grained similarity information and fine-grained relevant data, the\nframework thoroughly checks and adjusts intermediate results, ensuring process\naccuracy while effectively managing error propagation and amplification.\nExperimental results on multi-hop QA datasets demonstrate that the PAR RAG\nframework substantially outperforms existing state-of-the-art methods in key\nmetrics, including EM and F1 scores.", "AI": {"tldr": "PAR RAG\u6846\u67b6\u901a\u8fc7\u89c4\u5212\u3001\u6267\u884c\u548c\u5ba1\u67e5\u4e09\u9636\u6bb5\uff0c\u51cf\u5c11\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524dRAG\u65b9\u6cd5\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u5bb9\u6613\u56e0\u63a8\u7406\u8def\u5f84\u504f\u5dee\u6216\u4e2d\u95f4\u7ed3\u679c\u9519\u8bef\u5bfc\u81f4\u7b54\u6848\u4e0d\u51c6\u786e\uff0c\u9700\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51faPAR RAG\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u95ee\u9898\u5206\u89e3\u548c\u591a\u7c92\u5ea6\u9a8c\u8bc1\u673a\u5236\uff0c\u786e\u4fdd\u63a8\u7406\u8def\u5f84\u548c\u4e2d\u95f4\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0cPAR RAG\u5728EM\u548cF1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PAR RAG\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u9519\u8bef\u63a7\u5236\uff0c\u4e3a\u591a\u8df3\u95ee\u7b54\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16791", "pdf": "https://arxiv.org/pdf/2504.16791", "abs": "https://arxiv.org/abs/2504.16791", "authors": ["S. A. K. Leeney", "H. T. J. Bevins", "E. de Lera Acedo", "W. J. Handley", "C. Kirkham", "R. S. Patel", "J. Zhu", "D. Molnar", "J. Cumner", "D. Anstey", "K. Artuc", "G. Bernardi", "M. Bucher", "S. Carey", "J. Cavillot", "R. Chiello", "W. Croukamp", "D. I. L. de Villiers", "J. A. Ely", "A. Fialkov", "T. Gessey-Jones", "G. Kulkarni", "A. Magro", "P. D. Meerburg", "S. Mittal", "J. H. N. Pattison", "S. Pegwal", "C. M. Pieterse", "J. R. Pritchard", "E. Puchwein", "N. Razavi-Ghods", "I. L. V. Roque", "A. Saxena", "K. H. Scheutwinkel", "P. Scott", "E. Shen", "P. H. Sims", "M. Spinelli"], "title": "Radiometer Calibration using Machine Learning", "categories": ["astro-ph.IM", "astro-ph.CO", "cs.AI"], "comment": "Under peer review for publication in Nature Scientific Reports as\n  part of the Radio Astronomy collection", "summary": "Radiometers are crucial instruments in radio astronomy, forming the primary\ncomponent of nearly all radio telescopes. They measure the intensity of\nelectromagnetic radiation, converting this radiation into electrical signals. A\nradiometer's primary components are an antenna and a Low Noise Amplifier (LNA),\nwhich is the core of the ``receiver'' chain. Instrumental effects introduced by\nthe receiver are typically corrected or removed during calibration. However,\nimpedance mismatches between the antenna and receiver can introduce unwanted\nsignal reflections and distortions. Traditional calibration methods, such as\nDicke switching, alternate the receiver input between the antenna and a\nwell-characterised reference source to mitigate errors by comparison. Recent\nadvances in Machine Learning (ML) offer promising alternatives. Neural\nnetworks, which are trained using known signal sources, provide a powerful\nmeans to model and calibrate complex systems where traditional analytical\napproaches struggle. These methods are especially relevant for detecting the\nfaint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is\none of the main challenges in observational Cosmology today. Here, for the\nfirst time, we introduce and test a machine learning-based calibration\nframework capable of achieving the precision required for radiometric\nexperiments aiming to detect the 21-cm line.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6821\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u5c04\u7535\u5929\u6587\u8f90\u5c04\u8ba1\u7684\u7cbe\u5ea6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u63a2\u6d4b21\u5398\u7c73\u7ebf\u7684\u5fae\u5f31\u4fe1\u53f7\u3002", "motivation": "\u4f20\u7edf\u6821\u51c6\u65b9\u6cd5\uff08\u5982Dicke\u5207\u6362\uff09\u5728\u5904\u7406\u5929\u7ebf\u4e0e\u63a5\u6536\u5668\u963b\u6297\u4e0d\u5339\u914d\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u673a\u5668\u5b66\u4e60\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u6821\u51c6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5df2\u77e5\u4fe1\u53f7\u6e90\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u6821\u51c6\u8f90\u5c04\u8ba1\u5e76\u51cf\u5c11\u4fe1\u53f7\u53cd\u5c04\u548c\u5931\u771f\u3002", "result": "\u9996\u6b21\u6d4b\u8bd5\u4e86\u673a\u5668\u5b66\u4e60\u6821\u51c6\u6846\u67b6\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6ee1\u8db3\u63a2\u6d4b21\u5398\u7c73\u7ebf\u6240\u9700\u7684\u7cbe\u5ea6\u8981\u6c42\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6821\u51c6\u6846\u67b6\u4e3a\u5c04\u7535\u5929\u6587\u8f90\u5c04\u8ba1\u7684\u9ad8\u7cbe\u5ea6\u6821\u51c6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5b87\u5b99\u5b66\u4e2d\u7684\u5fae\u5f31\u4fe1\u53f7\u63a2\u6d4b\u3002"}}
{"id": "2504.16795", "pdf": "https://arxiv.org/pdf/2504.16795", "abs": "https://arxiv.org/abs/2504.16795", "authors": ["Xiang Hu", "Jiaqi Leng", "Jun Zhao", "Kewei Tu", "Wei Wu"], "title": "Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention", "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "A key advantage of Recurrent Neural Networks (RNNs) over Transformers is\ntheir linear computational and space complexity enables faster training and\ninference for long sequences. However, RNNs are fundamentally unable to\nrandomly access historical context, and simply integrating attention mechanisms\nmay undermine their efficiency advantages. To overcome this limitation, we\npropose \\textbf{H}ierarchical \\textbf{S}parse \\textbf{A}ttention (HSA), a novel\nattention mechanism that enhances RNNs with long-range random access\nflexibility while preserving their merits in efficiency and length\ngeneralization. HSA divides inputs into chunks, selecting the top-$k$ chunks\nand hierarchically aggregates information. The core innovation lies in learning\ntoken-to-chunk relevance based on fine-grained token-level information inside\neach chunk. This approach enhances the precision of chunk selection across both\nin-domain and out-of-domain context lengths. To make HSA efficient, we further\nintroduce a hardware-aligned kernel design. By combining HSA with Mamba, we\nintroduce RAMba, which achieves perfect accuracy in passkey retrieval across 64\nmillion contexts despite pre-training on only 4K-length contexts, and\nsignificant improvements on various downstream tasks, with nearly constant\nmemory footprint. These results show RAMba's huge potential in long-context\nmodeling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHSA\u7684\u5206\u5c42\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408RNN\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u957f\u5e8f\u5217\u7684\u9ad8\u6548\u5efa\u6a21\u548c\u968f\u673a\u8bbf\u95ee\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3RNN\u65e0\u6cd5\u968f\u673a\u8bbf\u95ee\u5386\u53f2\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u3002", "method": "\u63d0\u51faHSA\u673a\u5236\uff0c\u5c06\u8f93\u5165\u5206\u5757\u5e76\u9009\u62e9top-k\u5757\uff0c\u901a\u8fc7\u5206\u5c42\u805a\u5408\u4fe1\u606f\uff0c\u7ed3\u5408\u786c\u4ef6\u5bf9\u9f50\u7684\u5185\u6838\u8bbe\u8ba1\u3002", "result": "RAMba\u6a21\u578b\u572864\u767e\u4e07\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u51c6\u786e\u7387\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HSA\u548cRAMba\u5c55\u793a\u4e86\u5728\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2504.16828", "pdf": "https://arxiv.org/pdf/2504.16828", "abs": "https://arxiv.org/abs/2504.16828", "authors": ["Muhammad Khalifa", "Rishabh Agarwal", "Lajanugen Logeswaran", "Jaekyeom Kim", "Hao Peng", "Moontae Lee", "Honglak Lee", "Lu Wang"], "title": "Process Reward Models That Think", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm.", "AI": {"tldr": "ThinkPRM\u662f\u4e00\u79cd\u57fa\u4e8e\u957f\u94fe\u601d\u7ef4\uff08CoT\uff09\u7684\u751f\u6210\u5f0f\u9a8c\u8bc1\u6a21\u578b\uff0c\u4ec5\u97001%\u7684\u8fc7\u7a0b\u6807\u7b7e\u5373\u53ef\u4f18\u4e8e\u4f20\u7edf\u5224\u522b\u5f0f\u9a8c\u8bc1\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u9700\u8981\u5927\u91cf\u6b65\u9aa4\u7ea7\u76d1\u7763\u6570\u636e\uff0c\u8bad\u7ec3\u6210\u672c\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u6570\u636e\u9ad8\u6548\u7684PRMs\uff0c\u901a\u8fc7\u751f\u6210\u9a8c\u8bc1\u6027\u601d\u7ef4\u94fe\uff08CoT\uff09\u6765\u9a8c\u8bc1\u6bcf\u4e00\u6b65\u3002", "method": "\u63d0\u51faThinkPRM\uff0c\u4e00\u79cd\u957f\u94fe\u601d\u7ef4\u9a8c\u8bc1\u6a21\u578b\uff0c\u5229\u7528\u751f\u6210\u5f0fCoT\u6a21\u578b\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u9700\u5c11\u91cf\u8fc7\u7a0b\u6807\u7b7e\u8fdb\u884c\u5fae\u8c03\u3002", "result": "ThinkPRM\u5728ProcessBench\u3001MATH-500\u548cAIME '24\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u5728GPQA-Diamond\u548cLiveCodeBench\u7684\u57df\u5916\u8bc4\u4f30\u4e2d\u5206\u522b\u63d0\u53478%\u548c4.5%\u3002", "conclusion": "\u751f\u6210\u5f0f\u957f\u94fe\u601d\u7ef4PRMs\u5728\u9a8c\u8bc1\u8ba1\u7b97\u6269\u5c55\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u8bad\u7ec3\u6240\u9700\u76d1\u7763\u6570\u636e\u6781\u5c11\u3002"}}
{"id": "2504.16834", "pdf": "https://arxiv.org/pdf/2504.16834", "abs": "https://arxiv.org/abs/2504.16834", "authors": ["Yilin Zhai", "Hongyuan Shi", "Chao Zhan", "Qing Wang", "Zaijin You", "Nan Wang"], "title": "Improving Significant Wave Height Prediction Using Chronos Models", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling.", "AI": {"tldr": "Chronos\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65f6\u95f4\u67b6\u6784\uff0c\u7528\u4e8e\u6ce2\u6d6a\u9ad8\u5ea6\u9884\u6d4b\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u975e\u7ebf\u6027\u52a8\u6001\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6ce2\u6d6a\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528LLM\u589e\u5f3a\u7684\u65f6\u95f4\u67b6\u6784\uff08Chronos\uff09\uff0c\u901a\u8fc7\u5bf9\u897f\u5317\u592a\u5e73\u6d0b\u4e09\u4e2a\u6d77\u57df\u5386\u53f2\u6ce2\u6d6a\u6570\u636e\u7684\u9ad8\u7ea7\u65f6\u95f4\u6a21\u5f0f\u8bc6\u522b\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1114.3%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.5\u500d\uff0c\u77ed\u671f\uff081-24h\uff09\u548c\u957f\u671f\uff081-120h\uff09\u9884\u6d4b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5177\u5907\u96f6\u6837\u672c\u80fd\u529b\u3002", "conclusion": "Chronos\u4e3a\u6ce2\u6d6a\u9884\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u8fc1\u79fb\u7684\u590d\u6742\u5730\u7403\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u6846\u67b6\u3002"}}
{"id": "2504.16837", "pdf": "https://arxiv.org/pdf/2504.16837", "abs": "https://arxiv.org/abs/2504.16837", "authors": ["Daniele Carnevale", "Gianlorenzo D'Angelo", "Martin Olsen"], "title": "Approximating Optimal Labelings for Temporal Connectivity", "categories": ["cs.DS", "cs.AI"], "comment": null, "summary": "In a temporal graph the edge set dynamically changes over time according to a\nset of time-labels associated with each edge that indicates at which time-steps\nthe edge is available. Two vertices are connected if there is a path connecting\nthem in which the edges are traversed in increasing order of their labels. We\nstudy the problem of scheduling the availability time of the edges of a\ntemporal graph in such a way that all pairs of vertices are connected within a\ngiven maximum allowed time $a$ and the overall number of labels is minimized.\n  The problem, known as \\emph{Minimum Aged Labeling} (MAL), has several\napplications in logistics, distribution scheduling, and information spreading\nin social networks, where carefully choosing the time-labels can significantly\nreduce infrastructure costs, fuel consumption, or greenhouse gases.\n  The problem MAL has previously been proved to be NP-complete on undirected\ngraphs and \\APX-hard on directed graphs. In this paper, we extend our knowledge\non the complexity and approximability of MAL in several directions. We first\nshow that the problem cannot be approximated within a factor better than\n$O(\\log n)$ when $a\\geq 2$, unless $\\text{P} = \\text{NP}$, and a factor better\nthan $2^{\\log ^{1-\\epsilon} n}$ when $a\\geq 3$, unless $\\text{NP}\\subseteq\n\\text{DTIME}(2^{\\text{polylog}(n)})$, where $n$ is the number of vertices in\nthe graph. Then we give a set of approximation algorithms that, under some\nconditions, almost match these lower bounds. In particular, we show that the\napproximation depends on a relation between $a$ and the diameter of the input\ngraph.\n  We further establish a connection with a foundational optimization problem on\nstatic graphs called \\emph{Diameter Constrained Spanning Subgraph} (DCSS) and\nshow that our hardness results also apply to DCSS.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u65f6\u95f4\u56fe\u4e2d\u6700\u5c0f\u5316\u8fb9\u6807\u7b7e\u6570\u91cf\u4ee5\u786e\u4fdd\u6240\u6709\u9876\u70b9\u5bf9\u5728\u7ed9\u5b9a\u65f6\u95f4\u5185\u8fde\u63a5\u7684\u4f18\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u8fd1\u4f3c\u96be\u5ea6\u5e76\u63d0\u51fa\u4e86\u8fd1\u4f3c\u7b97\u6cd5\u3002", "motivation": "\u5728\u7269\u6d41\u3001\u793e\u4ea4\u7f51\u7edc\u4fe1\u606f\u4f20\u64ad\u7b49\u9886\u57df\uff0c\u4f18\u5316\u8fb9\u7684\u65f6\u95f4\u6807\u7b7e\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u6210\u672c\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u590d\u6742\u6027\u548c\u8fd1\u4f3c\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u95ee\u9898\u7684\u8fd1\u4f3c\u96be\u5ea6\uff0c\u5e76\u57fa\u4e8e\u8f93\u5165\u56fe\u7684\u76f4\u5f84\u4e0e\u5141\u8bb8\u65f6\u95f4\u7684\u5173\u7cfb\u63d0\u51fa\u4e86\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u95ee\u9898\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u65e0\u6cd5\u8fd1\u4f3c\u5230\u7279\u5b9a\u56e0\u5b50\uff0c\u5e76\u63d0\u51fa\u4e86\u63a5\u8fd1\u8fd9\u4e9b\u4e0b\u754c\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "conclusion": "\u7814\u7a76\u6269\u5c55\u4e86\u5bf9MAL\u95ee\u9898\u7684\u7406\u89e3\uff0c\u5e76\u5efa\u7acb\u4e86\u4e0e\u9759\u6001\u56fe\u4e2dDCSS\u95ee\u9898\u7684\u8054\u7cfb\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.16902", "pdf": "https://arxiv.org/pdf/2504.16902", "abs": "https://arxiv.org/abs/2504.16902", "authors": ["Idan Habler", "Ken Huang", "Vineeth Sai Narajala", "Prashant Kulkarni"], "title": "Building A Secure Agentic AI Application Leveraging A2A Protocol", "categories": ["cs.CR", "cs.AI"], "comment": "13 pages, 4 figures, 1 table, Authors contributed equally to this\n  work", "summary": "As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust protocols such as Google's Agent2Agent (A2A) become\nessential enablers. To foster secure adoption and ensure the reliability of\nthese complex interactions, understanding the secure implementation of A2A is\nessential. This paper addresses this goal by providing a comprehensive security\nanalysis centered on the A2A protocol. We examine its fundamental elements and\noperational dynamics, situating it within the framework of agent communication\ndevelopment. Utilizing the MAESTRO framework, specifically designed for AI\nrisks, we apply proactive threat modeling to assess potential security issues\nin A2A deployments, focusing on aspects such as Agent Card management, task\nexecution integrity, and authentication methodologies.\n  Based on these insights, we recommend practical secure development\nmethodologies and architectural best practices designed to build resilient and\neffective A2A systems. Our analysis also explores how the synergy between A2A\nand the Model Context Protocol (MCP) can further enhance secure\ninteroperability. This paper equips developers and architects with the\nknowledge and practical guidance needed to confidently leverage the A2A\nprotocol for building robust and secure next generation agentic applications.", "AI": {"tldr": "\u672c\u6587\u5bf9Google\u7684Agent2Agent\uff08A2A\uff09\u534f\u8bae\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b89\u5168\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u5b89\u5168\u5f00\u53d1\u65b9\u6cd5\u548c\u67b6\u6784\u5b9e\u8df5\uff0c\u4ee5\u589e\u5f3a\u590d\u6742\u591a\u4ee3\u7406\u534f\u4f5c\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u4ece\u7b80\u5355\u5de5\u4f5c\u6d41\u53d1\u5c55\u4e3a\u590d\u6742\u7684\u591a\u4ee3\u7406\u534f\u4f5c\uff0c\u786e\u4fddA2A\u534f\u8bae\u7684\u5b89\u5168\u5b9e\u73b0\u6210\u4e3a\u5173\u952e\u9700\u6c42\u3002", "method": "\u4f7f\u7528MAESTRO\u6846\u67b6\u8fdb\u884c\u4e3b\u52a8\u5a01\u80c1\u5efa\u6a21\uff0c\u5206\u6790A2A\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5982\u4ee3\u7406\u5361\u7ba1\u7406\u3001\u4efb\u52a1\u6267\u884c\u5b8c\u6574\u6027\u548c\u8ba4\u8bc1\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u5b89\u5168\u5f00\u53d1\u65b9\u6cd5\u548c\u67b6\u6784\u5b9e\u8df5\uff0c\u5e76\u63a2\u8ba8\u4e86A2A\u4e0e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u534f\u540c\u4f5c\u7528\u4ee5\u589e\u5f3a\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6784\u5efa\u5b89\u5168\u53ef\u9760\u7684\u4e0b\u4e00\u4ee3\u4ee3\u7406\u5e94\u7528\u6240\u9700\u7684\u77e5\u8bc6\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2504.16913", "pdf": "https://arxiv.org/pdf/2504.16913", "abs": "https://arxiv.org/abs/2504.16913", "authors": ["Shifali Agrahari", "Sanasam Ranbir Singh"], "title": "Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text", "categories": ["cs.CL", "cs.AI"], "comment": "De-Factify 4: 4th Workshop on Multimodal Fact Checking and Hate\n  Speech Detection, co-located with AAAI 2025. Pennsylvania", "summary": "In recent years, the detection of AI-generated text has become a critical\narea of research due to concerns about academic integrity, misinformation, and\nethical AI deployment. This paper presents COT Fine-tuned, a novel framework\nfor detecting AI-generated text and identifying the specific language model.\nresponsible for generating the text. We propose a dual-task approach, where\nTask A involves classifying text as AI-generated or human-written, and Task B\nidentifies the specific LLM behind the text. The key innovation of our method\nlies in the use of Chain-of-Thought reasoning, which enables the model to\ngenerate explanations for its predictions, enhancing transparency and\ninterpretability. Our experiments demonstrate that COT Fine-tuned achieves high\naccuracy in both tasks, with strong performance in LLM identification and\nhuman-AI classification. We also show that the CoT reasoning process\ncontributes significantly to the models effectiveness and interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOT Fine-tuned\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u7684\u6587\u672c\u5e76\u8bc6\u522b\u751f\u6210\u6587\u672c\u7684\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u53cc\u4efb\u52a1\u8bbe\u8ba1\u548cChain-of-Thought\u63a8\u7406\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0cAI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u6d89\u53ca\u5b66\u672f\u8bda\u4fe1\u3001\u9519\u8bef\u4fe1\u606f\u548cAI\u4f26\u7406\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u4efb\u52a1\u8bbe\u8ba1\uff08\u4efb\u52a1A\u533a\u5206AI\u4e0e\u4eba\u7c7b\u6587\u672c\uff0c\u4efb\u52a1B\u8bc6\u522b\u5177\u4f53\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u5e76\u5f15\u5165Chain-of-Thought\u63a8\u7406\u4ee5\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCOT Fine-tuned\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u548c\u4eba\u7c7b-AI\u5206\u7c7b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Chain-of-Thought\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3aAI\u6587\u672c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.16918", "pdf": "https://arxiv.org/pdf/2504.16918", "abs": "https://arxiv.org/abs/2504.16918", "authors": ["Raghav Thind", "Youran Sun", "Ling Liang", "Haizhao Yang"], "title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Optimization plays a vital role in scientific research and practical\napplications, but formulating a concrete optimization problem described in\nnatural language into a mathematical form and selecting a suitable solver to\nsolve the problem requires substantial domain expertise. We introduce\n\\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems\ndescribed in natural language by leveraging LLM-powered \\underline{AI} agents,\nachieving superior performance over current state-of-the-art methods. Our\nframework is built upon four key roles: (1) a \\emph{formulator} that translates\nnatural language problem descriptions into precise mathematical formulations;\n(2) a \\emph{planner} that constructs a high-level solution strategy prior to\nexecution; and (3) a \\emph{coder} and a \\emph{code critic} capable of\ninteracting with the environment and reflecting on outcomes to refine future\nactions. Ablation studies confirm that all roles are essential; removing the\nplanner or code critic results in $5.8\\times$ and $3.1\\times$ drops in\nproductivity, respectively. Furthermore, we introduce UCB-based debug\nscheduling to dynamically switch between alternative plans, yielding an\nadditional $3.3\\times$ productivity gain. Our design emphasizes multi-agent\ncollaboration, allowing us to conveniently explore the synergistic effect of\ncombining diverse models within a unified system. Our approach attains 88.1\\%\naccuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o\ntable) subset, reducing error rates by 58\\% and 50\\% respectively over prior\nbest results.", "AI": {"tldr": "OptimAI\u662f\u4e00\u4e2a\u5229\u7528LLM\u9a71\u52a8\u7684AI\u4ee3\u7406\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u4f18\u5316\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u89d2\u8272\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f18\u5316\u95ee\u9898\u5728\u79d1\u5b66\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u6570\u5b66\u5f62\u5f0f\u5e76\u9009\u62e9\u5408\u9002\u7684\u6c42\u89e3\u5668\u9700\u8981\u5927\u91cf\u9886\u57df\u77e5\u8bc6\u3002", "method": "\u6846\u67b6\u5305\u542b\u56db\u4e2a\u89d2\u8272\uff1aformulator\uff08\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3a\u6570\u5b66\u516c\u5f0f\uff09\u3001planner\uff08\u5236\u5b9a\u9ad8\u5c42\u89e3\u51b3\u65b9\u6848\uff09\u3001coder\u548ccode critic\uff08\u4e0e\u73af\u5883\u4ea4\u4e92\u5e76\u4f18\u5316\u884c\u52a8\uff09\u3002\u91c7\u7528UCB-based debug\u8c03\u5ea6\u52a8\u6001\u5207\u6362\u8ba1\u5212\u3002", "result": "\u5728NLP4LP\u6570\u636e\u96c6\u4e0a\u8fbe\u523088.1%\u51c6\u786e\u7387\uff0cOptibench\u5b50\u96c6\u4e0a71.2%\uff0c\u9519\u8bef\u7387\u5206\u522b\u964d\u4f4e58%\u548c50%\u3002", "conclusion": "OptimAI\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u663e\u8457\u63d0\u5347\u4f18\u5316\u95ee\u9898\u7684\u89e3\u51b3\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.16925", "pdf": "https://arxiv.org/pdf/2504.16925", "abs": "https://arxiv.org/abs/2504.16925", "authors": ["Amber Xie", "Oleh Rybkin", "Dorsa Sadigh", "Chelsea Finn"], "title": "Latent Diffusion Planning for Imitation Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent progress in imitation learning has been enabled by policy\narchitectures that scale to complex visuomotor tasks, multimodal distributions,\nand large datasets. However, these methods often rely on learning from large\namount of expert demonstrations. To address these shortcomings, we propose\nLatent Diffusion Planning (LDP), a modular approach consisting of a planner\nwhich can leverage action-free demonstrations, and an inverse dynamics model\nwhich can leverage suboptimal data, that both operate over a learned latent\nspace. First, we learn a compact latent space through a variational\nautoencoder, enabling effective forecasting of future states in image-based\ndomains. Then, we train a planner and an inverse dynamics model with diffusion\nobjectives. By separating planning from action prediction, LDP can benefit from\nthe denser supervision signals of suboptimal and action-free data. On simulated\nvisual robotic manipulation tasks, LDP outperforms state-of-the-art imitation\nlearning approaches, as they cannot leverage such additional data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLatent Diffusion Planning (LDP)\u7684\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u65e0\u52a8\u4f5c\u6f14\u793a\u548c\u6b21\u4f18\u6570\u636e\uff0c\u5728\u5b66\u4e60\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89c4\u5212\u548c\u9006\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4e13\u5bb6\u6f14\u793a\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002LDP\u65e8\u5728\u901a\u8fc7\u5229\u7528\u65e0\u52a8\u4f5c\u6f14\u793a\u548c\u6b21\u4f18\u6570\u636e\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff1b2. \u4f7f\u7528\u6269\u6563\u76ee\u6807\u8bad\u7ec3\u89c4\u5212\u5668\u548c\u9006\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "\u5728\u6a21\u62df\u89c6\u89c9\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cLDP\u4f18\u4e8e\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "LDP\u901a\u8fc7\u5206\u79bb\u89c4\u5212\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u80fd\u591f\u5229\u7528\u66f4\u591a\u6570\u636e\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
