[[toc]]

## cs.CV

### [1] [ReaMOT: A Benchmark and Framework for Reasoning-based Multi-Object Tracking](https://arxiv.org/abs/2505.20381)
*Sijia Chen,Yanqiu Yu,En Yu,Wenbing Tao*

Main category: cs.CV

TL;DR: 论文提出了一种新的任务ReaMOT，旨在解决复杂语言指令下的多目标跟踪问题，并构建了ReaMOT Challenge基准和ReaTrack框架。

- Motivation: 现有的RMOT任务在复杂语言指令下表现不佳，需要更强大的推理能力。
- Method: 提出了ReaMOT任务，构建了包含1,156条推理语言指令的基准，并提出了基于LVLM和SAM2的无训练框架ReaTrack。
- Result: ReaTrack框架在ReaMOT Challenge基准上表现出色。
- Conclusion: ReaMOT任务和ReaTrack框架为复杂语言指令下的多目标跟踪提供了新思路。


### [2] [What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models](https://arxiv.org/abs/2505.20405)
*Lorenzo Baraldi,Davide Bucciarelli,Federico Betti,Marcella Cornia,Lorenzo Baraldi,Nicu Sebe,Rita Cucchiara*

Main category: cs.CV

TL;DR: DICE是一个用于评估指令驱动图像编辑模型的新方法，通过检测局部差异和评估编辑相关性，显著提升了与人类判断的一致性。

- Motivation: 现有评估指标在人类判断一致性和可解释性方面表现不足，需要更有效的评估方法。
- Method: DICE由差异检测器和一致性评估器组成，基于自回归多模态大语言模型（MLLM），结合自监督、蒸馏和全监督训练策略。
- Result: 实验表明DICE能有效识别一致编辑，并与人类判断高度相关。
- Conclusion: DICE为图像编辑评估提供了更可靠的解决方案，并公开了代码、模型和数据。


### [3] [RetroMotion: Retrocausal Motion Forecasting Models are Instructable](https://arxiv.org/abs/2505.20414)
*Royden Wagner,Omer Sahin Tas,Felix Hauser,Marlon Steiner,Dominik Strutz,Abhishek Vivekanandan,Carlos Fernandez,Christoph Stiller*

Main category: cs.CV

TL;DR: 提出了一种多任务学习方法，用于运动预测，结合了逆向因果信息流，实现了对交互行为的建模，并在多个数据集中取得了先进的结果。

- Motivation: 解决道路用户运动预测中因场景约束和交互行为导致的复杂性差异问题。
- Method: 使用Transformer模型，通过重新编码边际分布并进行成对建模，生成联合分布，同时采用压缩指数幂分布建模位置不确定性。
- Result: 在Waymo Interaction Prediction和Argoverse 2数据集中取得了先进的结果，并能通过轨迹修改实现指令响应。
- Conclusion: 该方法不仅能有效预测运动，还能适应场景上下文并响应目标指令。


### [4] [MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness](https://arxiv.org/abs/2505.20426)
*Yunlong Tang,Pinxin Liu,Mingqian Feng,Zhangyun Tan,Rui Mao,Chao Huang,Jing Bi,Yunzhong Xiao,Susan Liang,Hang Hua,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Chenliang Xu*

Main category: cs.CV

TL;DR: MMPerspective是一个专门评估多模态大语言模型（MLLMs）对透视几何理解的基准测试，包含10个任务和2,711个图像实例。研究发现模型在表面感知任务上表现良好，但在组合推理和空间一致性上存在局限。

- Motivation: 研究动机是探究MLLMs是否真正理解透视几何，填补现有评估工具的空白。
- Method: 方法是通过MMPerspective基准测试，包含10个任务和5,083个问答对，评估43个先进MLLMs的能力。
- Result: 结果显示模型在感知任务上表现良好，但在组合推理和扰动下的空间一致性上表现不佳。
- Conclusion: 结论是MMPerspective为诊断和提升视觉语言系统的空间理解能力提供了重要工具。


### [5] [DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data](https://arxiv.org/abs/2505.20460)
*Ruqi Wu,Xinjie Wang,Liu Liu,Chunle Guo,Jiaxiong Qiu,Chongyi Li,Lichao Huang,Zhizhong Su,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: DIPO是一个新颖的框架，通过一对图像（静止状态和关节状态）可控生成3D关节物体，优于现有基线。

- Motivation: 解决单图像输入缺乏运动信息的问题，双图像输入提供更可靠的关节关系预测。
- Method: 提出双图像扩散模型和基于Chain-of-Thought的图推理器，并开发自动化数据集扩展管道LEGO-Art。
- Result: DIPO在静止和关节状态下显著优于基线，PM-X数据集增强了泛化能力。
- Conclusion: DIPO和PM-X数据集为复杂关节物体的生成提供了有效解决方案，代码和数据集将开源。


### [6] [CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting](https://arxiv.org/abs/2505.20469)
*Lei Tian,Xiaomin Li,Liqian Ma,Hefei Huang,Zirui Zheng,Hao Yin,Taiqing Li,Huchuan Lu,Xu Jia*

Main category: cs.CV

TL;DR: 论文提出CCL-LGS框架，通过多视角语义线索解决3D语义理解中的跨视角不一致性问题，提升3D高斯语义场质量。

- Motivation: 现有依赖2D先验的方法因遮挡、图像模糊和视角变化导致跨视角语义不一致，影响3D语义场质量和渲染输出。
- Method: 结合零样本跟踪器对齐SAM生成的2D掩码，利用CLIP提取跨视角语义编码，并通过对比代码本学习（CCL）模块优化特征。
- Result: CCL-LGS在实验中表现优于现有方法，有效解决语义冲突并保持类别区分性。
- Conclusion: CCL-LGS通过多视角一致性监督显著提升3D语义理解质量，为相关应用提供新思路。


### [7] [WeatherEdit: Controllable Weather Editing with 4D Gaussian Field](https://arxiv.org/abs/2505.20471)
*Chenghao Qian,Wenjing Li,Yuhu Guo,Gustav Markkula*

Main category: cs.CV

TL;DR: WeatherEdit是一个用于在3D场景中生成可控类型和强度的逼真天气效果的管道，包括天气背景编辑和天气粒子构建两部分。

- Motivation: 为自动驾驶模拟提供多样且可控的恶劣天气效果。
- Method: 1. 使用多天气风格适配器编辑2D背景；2. 设计TV注意力机制确保多帧和多视图一致性；3. 通过4D高斯场生成动态天气粒子。
- Result: 在多个驾驶数据集上验证了WeatherEdit能生成多样且可控的天气效果。
- Conclusion: WeatherEdit在自动驾驶模拟中具有潜力，能生成逼真且可控的恶劣天气效果。


### [8] [ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image](https://arxiv.org/abs/2505.20498)
*Dongyu Luo,Kelin Yu,Amir-Hossein Shahidzadeh,Cornelia Fermüller,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: ControlTac提出了一种可控框架，通过单张触觉参考图像、接触力和位置生成逼真的触觉图像，用于数据增强。

- Motivation: 解决触觉数据收集成本高、现有方法生成不真实且难以迁移的问题。
- Method: 两阶段可控框架，基于物理先验生成触觉图像。
- Result: 实验证明ControlTac能有效增强触觉数据集并提升下游任务性能。
- Conclusion: ControlTac具有实际应用价值，能生成多样且物理合理的触觉图像。


### [9] [Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset](https://arxiv.org/abs/2505.20507)
*Elias Arbash,Ahmed Jamal Afifi,Ymane Belahsen,Margret Fuchs,Pedram Ghamisi,Paul Scheunders,Richard Gloaguen*

Main category: cs.CV

TL;DR: 论文提出了一种名为Electrolyzers-HSI的多模态基准数据集，用于电解器材料的准确分类，以支持可持续回收。

- Motivation: 解决全球可持续回收挑战，需要自动化、快速且准确的SOTA材料检测系统，以推动循环经济和绿色协议。
- Method: 数据集包含55个高分辨率RGB图像和HSI数据立方体，覆盖400-2500 nm光谱范围。评估了多种ML和DL方法，包括Vision Transformer和Multimodal Fusion Transformer。
- Result: 数据集支持非侵入式光谱分析，实现了电解器材料的定量和定性分类。通过零样本检测和多数投票提高了分类鲁棒性。
- Conclusion: Electrolyzers-HSI数据集和代码公开可用，支持可重复研究，促进智能可持续电子废物回收的广泛应用。


### [10] [CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic](https://arxiv.org/abs/2505.20510)
*Yuxuan Sun,Yixuan Si,Chenglu Zhu,Kai Zhang,Zhongyi Shui,Bowen Ding,Tao Lin,Lin Yang*

Main category: cs.CV

TL;DR: CPathAgent是一种基于代理的模型，通过模拟病理学家的诊断逻辑（如放大/缩小和导航操作）生成更详细和可解释的诊断报告。

- Motivation: 现有方法未能模拟病理学家的诊断逻辑，无法系统性地从低倍镜到高倍镜逐步检查图像。
- Method: 采用多阶段训练策略，统一了补丁级、区域级和全玻片级能力，模拟病理学家的跨尺度推理。
- Result: CPathAgent在三个尺度的基准测试中均优于现有方法，并构建了首个用于大区域分析的PathMMU-HR²基准。
- Conclusion: CPathAgent展示了基于代理的诊断方法的有效性，为计算病理学的未来发展提供了新方向。


### [11] [A Feature-level Bias Evaluation Framework for Facial Expression Recognition Models](https://arxiv.org/abs/2505.20512)
*Tangzheng Lian,Oya Celiktutan*

Main category: cs.CV

TL;DR: 本文提出了一种在测试集缺乏人口统计标签的情况下，通过特征级偏置评估框架评估FER模型人口统计偏置的方法，并引入统计模块确保结果的显著性。

- Motivation: 现有FER模型存在对某些视觉感知人口群体的偏置，但公开数据集缺乏人口统计标签限制了偏置分析的范围，伪标签可能扭曲结果。
- Method: 提出特征级偏置评估框架，无需测试集人口统计标签，并引入统计模块验证结果显著性。
- Result: 实验表明，该方法比依赖伪标签的现有方法更有效，且揭示了FER模型在年龄、性别和种族上的显著偏置。
- Conclusion: 该方法为FER模型的公平性评估提供了新思路，并帮助选择更公平的网络架构。


### [12] [MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning](https://arxiv.org/abs/2505.20513)
*Wenhao Gu,Li Gu,Ching Yee Suen,Yang Wang*

Main category: cs.CV

TL;DR: 提出了一种高效的手写文本识别框架，通过提示调优和自监督学习实现个性化，显著减少参数更新和标注需求。

- Motivation: 传统方法在多样书写风格下表现不佳，且缺乏个性化调整能力，现有方法计算和内存开销大。
- Method: 将个性化建模为提示调优，结合自监督损失和元学习优化提示初始化，仅更新少量参数。
- Result: 在RIMES和IAM基准测试中优于现有方法，参数减少20倍。
- Conclusion: 该方法为资源受限场景下的个性化手写识别提供了可靠解决方案。


### [13] [MultLFG: Training-free Multi-LoRA composition using Frequency-domain Guidance](https://arxiv.org/abs/2505.20525)
*Aniket Roy,Maitreya Suin,Ketul Shah,Rama Chellappa*

Main category: cs.CV

TL;DR: MultLFG是一种无需训练的多LoRA组合框架，通过频域指导实现自适应融合，显著提升多概念生成任务的准确性和一致性。

- Motivation: 当前方法在无需训练的情况下难以有效合并多个LoRA适配器，尤其是在涉及多样化视觉元素的复杂组合中。
- Method: MultLFG采用时间和频率子带自适应融合策略，根据内容相关性在特定时间步和频带选择性激活相关LoRA。
- Result: 在ComposLoRA基准测试中，MultLFG显著提升了组合保真度和图像质量，优于现有方法。
- Conclusion: MultLFG通过频域指导实现了更精确和一致的多LoRA组合，为多概念生成任务提供了更优解决方案。


### [14] [Causality and "In-the-Wild" Video-Based Person Re-ID: A Survey](https://arxiv.org/abs/2505.20540)
*Md Rashidunnabi,Kailash Hambarde,Hugo Proença*

Main category: cs.CV

TL;DR: 这篇综述探讨了因果推理在视频行人重识别（Re-ID）中的作用，分析了基于结构因果模型、干预和反事实推理的方法，并提出了新的分类法和评估指标。

- Motivation: 现有模型依赖表面相关性（如服装、背景或光照），难以泛化到不同领域、视角和时间变化，因果推理为解决这一问题提供了新思路。
- Method: 综述围绕生成解耦、域不变建模和因果Transformer等方法展开，提出了新的分类法，并评估了实际挑战（如可扩展性、公平性、隐私）。
- Result: 综述总结了因果Re-ID方法的进展，并提出了因果特定的鲁棒性评估指标。
- Conclusion: 该研究为因果视频行人重识别奠定了基础，并指出了未来研究方向，如结合高效架构和自监督学习。


### [15] [Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.20569)
*Jihoon Lee,Min Song*

Main category: cs.CV

TL;DR: RVCD通过利用正负图像在logit级别抑制目标幻觉，显著优于现有解码方法。

- Motivation: 尽管大型视觉语言模型取得进展，目标幻觉问题仍存在。
- Method: RVCD结合正负图像，在logit级别显式参考AI生成图像以抑制幻觉。
- Result: 方法显著优于现有解码方法。
- Conclusion: RVCD为抑制目标幻觉提供了有效解决方案。


### [16] [Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting](https://arxiv.org/abs/2505.20582)
*Yizhou Zhao,Chunjiang Liu,Haoyu Chen,Bhiksha Raj,Min Xu,Tadas Baltrusaitis,Mitch Rundle,HsiangTao Wu,Kamran Ghasedi*

Main category: cs.CV

TL;DR: Total-Editing是一个统一的肖像编辑框架，结合了面部重演和肖像重光照，通过神经辐射场和变形场实现高质量编辑。

- Motivation: 现有方法独立处理面部重演和肖像重光照，缺乏协同效应，限制了编辑的灵活性和真实性。
- Method: 设计了具有本征分解能力的神经辐射场解码器，并结合基于移动最小二乘的变形场，增强时空一致性。
- Result: 显著提升了肖像编辑的质量和真实感，支持多源应用如光照转移和肖像动画。
- Conclusion: Total-Editing通过统一框架实现了更灵活、高质量的肖像编辑。


### [17] [OmniIndoor3D: Comprehensive Indoor 3D Reconstruction](https://arxiv.org/abs/2505.20610)
*Xiaobao Wei,Xiaoan Zhang,Hao Wang,Qingpo Wuwu,Ming Lu,Wenzhao Zheng,Shanghang Zhang*

Main category: cs.CV

TL;DR: OmniIndoor3D是一个基于高斯表示的室内3D重建框架，通过RGB-D相机实现外观、几何和全景重建。

- Motivation: 现有3DGS方法主要针对渲染优化，缺乏高质量全景重建所需的精确几何信息。
- Method: 结合RGB-D图像生成粗略3D重建，初始化3D高斯并引入轻量级MLP优化几何属性，辅以全景先验的密集化策略。
- Result: 在多数据集上实现外观、几何和全景重建的领先性能。
- Conclusion: OmniIndoor3D填补了室内3D重建的关键空白，支持机器人导航。


### [18] [Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation](https://arxiv.org/abs/2505.20611)
*Zenghao Zheng,Lianping Yang,Jinshan Pan,Hegui Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的拓扑融合框架，用于3D人体姿态估计，通过骨骼感知模块和图卷积增强局部依赖，显著降低计算成本并提高精度。

- Motivation: Transformer在3D姿态估计中因自注意力复杂度高而面临计算挑战，而Mamba模型虽能处理长序列，但对拓扑结构和局部关系捕捉不足。
- Method: 设计了骨骼感知模块推断骨骼向量方向与长度，增强Mamba的图卷积结构，并引入时空细化模块建模时空关系。
- Result: 在Human3.6M和MPI-INF-3DHP数据集上验证，方法显著降低计算成本且精度更高。
- Conclusion: 通过融合骨骼拓扑结构，有效解决了Mamba在捕捉人体结构关系上的局限性，各模块均被证明有效。


### [19] [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
*Peter Robicheaux,Matvei Popov,Anish Madan,Isaac Robinson,Joseph Nelson,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: 论文提出了一种通过多模态标注指令（少量视觉示例和丰富文本描述）对齐视觉语言模型（VLMs）到新概念的方法，并发布了Roboflow100-VL数据集以评估模型性能。

- Motivation: 现有视觉语言模型在常见对象上表现优异，但在分布外类别、任务和成像模态上泛化能力不足，需要改进对齐方法。
- Method: 引入Roboflow100-VL数据集，包含100个多模态目标检测数据集，用于评估零样本、少样本、半监督和全监督设置下的模型性能。
- Result: 实验显示，GroundingDINO和Qwen2.5-VL等模型在挑战性医疗影像数据集上的零样本准确率不足2%，表明少样本概念对齐的必要性。
- Conclusion: 通过多模态标注指令对齐VLMs到新概念是提升模型泛化能力的有效方法，Roboflow100-VL为研究提供了重要基准。


### [20] [Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea](https://arxiv.org/abs/2505.20615)
*Omid Halimi Milani,Ahmet Enis Cetin,Bharati Prasad*

Main category: cs.CV

TL;DR: 该研究提出了一种结合离散余弦变换（DCT）和迁移学习的深度学习方法，用于预测阻塞性睡眠呼吸暂停（OSA）患者在五年内发展为高血压的风险。

- Motivation: OSA是高血压的重要风险因素，但目前预测其发展为高血压的准确性有限。研究旨在通过整合多导睡眠图信号和频率域特征提取，提高预测性能。
- Method: 研究提取多导睡眠图信号的特征，并将其转换为2D表示，利用预训练的2D神经网络（如MobileNet、EfficientNet和ResNet）。通过引入DCT层，将特征转换为频率域表示，并结合迁移学习优化模型。
- Result: 模型在EfficientNet中最佳截断深度处放置DCT层，取得了72.88%的AUC值，表明频率域特征提取和迁移学习的有效性。
- Conclusion: 该方法通过频率域特征提取和迁移学习，显著提高了OSA患者五年内高血压风险的预测准确性。


### [21] [OccLE: Label-Efficient 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2505.20617)
*Naiyu Fang,Zheyuan Zhou,Fayao Liu,Xulei Yang,Jiacheng Wei,Lemiao Qiu,Guosheng Lin*

Main category: cs.CV

TL;DR: OccLE是一种标签高效的3D语义占用预测方法，通过解耦语义和几何学习任务，并结合双Mamba融合特征，仅需10%的体素标注即可实现高性能。

- Motivation: 现有方法依赖昂贵的全监督或性能有限的自监督，OccLE旨在通过标签高效的方式解决这一问题。
- Method: 解耦语义和几何学习任务，利用2D基础模型生成伪标签，结合图像和LiDAR输入进行半监督几何学习，并通过双Mamba融合特征。
- Result: 在SemanticKITTI验证集上达到16.59%的mIoU，仅需10%的体素标注。
- Conclusion: OccLE在减少标注成本的同时保持了高性能，为3D语义占用预测提供了实用解决方案。


### [22] [ConsiStyle: Style Diversity in Training-Free Consistent T2I Generation](https://arxiv.org/abs/2505.20626)
*Yohai Mazuz,Janna Bruner,Lior Wolf*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过操纵注意力矩阵实现风格对齐和主题一致性。

- Motivation: 现有方法在保持主题一致性和风格对齐之间存在困难，且依赖大规模微调或优化，泛化能力不足。
- Method: 通过从锚定图像获取Queries和Keys，从非锚定图像获取Values，并扩展Key和Value矩阵以增强跨图像注意力。
- Result: 实验表明，该方法能有效解耦风格与主题外观，生成风格多样且主题一致的图像。
- Conclusion: 该方法无需训练即可实现风格对齐和主题一致性，具有广泛适用性。


### [23] [Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training](https://arxiv.org/abs/2505.20629)
*Bolin Lai,Sangmin Lee,Xu Cao,Xiang Li,James M. Rehg*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的FlexTI2V方法，用于灵活地将任意数量的图像条件融入文本到视频生成模型中，通过随机块交换策略和动态控制机制实现高保真度和创造性。

- Motivation: 现有方法通常通过微调将视觉条件融入文本到视频基础模型，成本高且仅限于预定义条件设置。本文旨在解决这一问题，提出更灵活的方法。
- Method: 采用无需训练的方法，将条件图像反转为潜在空间的噪声表示，并在去噪过程中通过随机块交换策略融入视觉特征，动态调整条件强度。
- Result: 实验证明，该方法显著优于现有无需训练的图像条件方法，并通过详细消融研究展示了其优势。
- Conclusion: FlexTI2V提供了一种高效、灵活且无需训练的方法，为可控视频生成开辟了新途径。


### [24] [TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone](https://arxiv.org/abs/2505.20637)
*Ana M. Cabanas,Alma Pedro,Domingo Mery*

Main category: cs.CV

TL;DR: 研究比较了两种肤色分类方法（ITA和基于$L^*$-$H^*$的方法），发现肤色较暗的群体在FAA系统中代表性不足且公平性存在差异。$L^*$-$H^*$方法更一致，ITA因对光照敏感而受限。

- Motivation: 探讨FAA系统在不同肤色群体中的表现差异，强调肤色分类方法对公平性评估的重要性。
- Method: 比较ITA和$L^*$-$H^*$两种肤色分类方法，使用AffectNet数据集和MobileNet模型评估公平性。
- Result: 肤色较暗群体代表性不足（约2%），公平性指标（F1-score和TPR）存在显著差异。$L^*$-$H^*$方法更稳定。
- Conclusion: 肤色分类方法的选择影响公平性评估，ITA可能掩盖肤色较暗群体的差异。建议采用$L^*$-$H^*$方法并整合公平性评估流程。


### [25] [Open-Det: An Efficient Learning Framework for Open-Ended Detection](https://arxiv.org/abs/2505.20639)
*Guiping Cao,Tao Wang,Wenjian Huang,Xiangyuan Lan,Jianguo Zhang,Dongmei Jiang*

Main category: cs.CV

TL;DR: Open-Det是一个高效的开集目标检测框架，通过重构检测器和名称生成器、引入视觉-语言对齐机制，显著减少了训练数据和资源需求，同时提升了性能。

- Motivation: 解决现有开集目标检测模型（如GenerateU）需要大规模数据集、收敛慢且性能有限的问题。
- Method: 提出Open-Det框架，包括重构检测器和名称生成器、视觉-语言对齐机制（V-to-L和L-to-V）、提示蒸馏器和联合损失函数。
- Result: 仅用1.5%训练数据、20.8%训练轮次和更少GPU资源，性能提升1.0%（APr）。
- Conclusion: Open-Det在效率和性能上均优于现有方法，为开集目标检测提供了更实用的解决方案。


### [26] [IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios](https://arxiv.org/abs/2505.20640)
*Yifan Li,Yuhang Chen,Anh Dao,Lichi Li,Zhongyi Cai,Zhen Tan,Tianlong Chen,Yu Kong*

Main category: cs.CV

TL;DR: IndustryEQA是首个专注于评估工业仓库场景中安全关键问题的EQA基准，弥补了现有基准的不足。

- Motivation: 现有EQA基准主要关注家庭环境，忽略了工业场景中的安全关键问题和推理过程，限制了代理在工业应用中的评估。
- Method: 基于NVIDIA Isaac Sim平台，IndustryEQA提供高保真视频和丰富标注，涵盖六类安全与推理任务，并包含1,344个问题-答案对。
- Result: 提出了一个全面的评估框架，包括多种基线模型，用于测试代理在工业环境中的感知和推理能力。
- Conclusion: IndustryEQA旨在推动EQA研究，开发更鲁棒、安全且实用的工业环境代理。


### [27] [See through the Dark: Learning Illumination-affined Representations for Nighttime Occupancy Prediction](https://arxiv.org/abs/2505.20641)
*Yuan Wu,Zhiqiang Yan,Yigong Zhang,Xiang Li,ian Yang*

Main category: cs.CV

TL;DR: LIAR是一种新框架，通过学习光照相关表示来解决夜间场景下的占用预测问题，通过选择性低光增强和光照感知组件提升性能。

- Motivation: 现有视觉方法在白天表现良好，但在夜间因能见度低和光照条件差而表现不佳，LIAR旨在解决这一问题。
- Method: LIAR引入选择性低光增强（SLLIE）和两个光照感知组件（2D-IGS和3D-IDP），分别处理局部曝光不足和过度曝光。
- Result: 在真实和合成数据集上的实验表明，LIAR在夜间场景下表现优异。
- Conclusion: LIAR通过光照相关表示和针对性增强，显著提升了夜间占用预测的性能。


### [28] [HCQA-1.5 @ Ego4D EgoSchema Challenge 2025](https://arxiv.org/abs/2505.20644)
*Haoyu Zhang,Yisen Feng,Qiaohui Chu,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出了一种改进的HCQA框架，通过多源聚合策略和置信度过滤机制，提升了自我中心视频问答的准确性，最终在CVPR 2025 Ego4D EgoSchema挑战赛中取得第三名。

- Motivation: 为了提高自我中心视频问答中答案预测的可靠性，作者对现有的HCQA框架进行了扩展。
- Method: 方法包括多源聚合策略生成多样化预测，置信度过滤机制选择高置信度答案，以及针对低置信度情况的细粒度推理模块。
- Result: 在EgoSchema盲测集上，该方法在5000多个人工标注的多选题中达到了77%的准确率，优于去年的获胜方案和大多数参赛团队。
- Conclusion: 该方法显著提升了自我中心视频问答的性能，代码已开源。


### [29] [Scan-and-Print: Patch-level Data Summarization and Augmentation for Content-aware Layout Generation in Poster Design](https://arxiv.org/abs/2505.20649)
*HsiaoYuan Hsu,Yuxin Peng*

Main category: cs.CV

TL;DR: 论文提出了一种名为Scan-and-Print的补丁级数据总结与增强方法，用于AI海报设计中的内容感知布局生成，显著降低了计算瓶颈并提升了生成质量。

- Motivation: 现有方法在感知背景图像时需要大量参数，导致实时性能和泛化能力受限。
- Method: Scan-and-Print方法通过扫描（选择适合放置元素顶点的补丁）和打印（混合补丁和顶点以合成新样本）实现高效感知与数据增强。
- Result: 实验表明，该方法能生成视觉吸引力的布局，计算瓶颈减少95.2%，达到最先进质量。
- Conclusion: Scan-and-Print是一种高效且高质量的内容感知布局生成方法。


### [30] [RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment](https://arxiv.org/abs/2505.20653)
*Lingyu Qiu,Ke Jiang,Xiaoyang Tan*

Main category: cs.CV

TL;DR: 提出一种新的学习目标，通过梯度对齐增强深度伪造检测模型的泛化能力，避免额外正则化。

- Motivation: 现有方法通过额外模块防止过拟合，但可能阻碍经验风险最小化（ERM）目标的优化，影响性能。
- Method: 通过扰动模型参数对齐跨域梯度更新，保留域不变特征并管理域特定特征。
- Result: 在多个深度伪造检测数据集上表现优于现有域泛化技术。
- Conclusion: 梯度对齐策略有效提升模型对域偏移的鲁棒性，代码已开源。


### [31] [Photography Perspective Composition: Towards Aesthetic Perspective Recommendation](https://arxiv.org/abs/2505.20655)
*Lujian Yao,Siming Zheng,Xinbin Yuan,Zhuoxuan Cai,Pu Wu,Jinwei Chen,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D透视调整的摄影构图方法（PPC），解决了传统2D裁剪方法的局限性，并通过自动化数据集构建、视频生成和透视质量评估模型（PQA）实现了优化。

- Motivation: 传统2D裁剪方法在场景中主体排列不佳时效果有限，而专业摄影师常通过3D透视调整优化构图。论文旨在通过PPC方法弥补这一不足。
- Method: 提出了PPC方法，包括自动化构建数据集、生成透视调整视频和基于人类表现的PQA模型。
- Result: 实现了无需额外提示或相机轨迹的简洁方法，帮助普通用户提升构图技能。
- Conclusion: PPC方法扩展了传统构图技术，通过3D透视调整提升了摄影构图的质量和灵活性。


### [32] [DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving](https://arxiv.org/abs/2505.20665)
*Muxi Diao,Lele Yang,Hongbo Yin,Zhexu Wang,Yejie Wang,Daxin Tian,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: AutoDriveRL框架将自动驾驶任务建模为四个核心任务的视觉语言问答问题，通过任务特定奖励模型优化，训练出DriveRX模型，在复杂场景中表现优于GPT-4o。

- Motivation: 传统端到端模型在复杂场景中泛化能力不足，现有视觉语言模型缺乏多阶段决策支持。
- Method: 将自动驾驶任务分解为四个视觉语言问答问题，使用任务特定奖励模型优化，训练DriveRX模型。
- Result: DriveRX在公共基准测试中表现优异，行为推理优于GPT-4o，且在复杂或损坏条件下具有鲁棒性。
- Conclusion: AutoDriveRL框架和DriveRX模型为自动驾驶研究提供了新的结构化推理方法，未来将开源支持进一步研究。


### [33] [Contrastive Desensitization Learning for Cross Domain Face Forgery Detection](https://arxiv.org/abs/2505.20675)
*Lingyu Qiu,Ke Jiang,Xiaoyang Tan*

Main category: cs.CV

TL;DR: 提出了一种新的跨域人脸伪造检测方法CDN，通过对比去敏网络降低误报率并提升检测精度。

- Motivation: 现有方法在多域适用性上存在高误报率问题，影响系统可用性。
- Method: 基于对比去敏网络（CDN），通过学习真实人脸图像的域变换特征来捕获本质域特性。
- Result: 在大规模基准数据集上实验表明，CDN显著降低了误报率并提高了检测精度。
- Conclusion: CDN方法在跨域人脸伪造检测中表现出色，具有理论鲁棒性和实际应用价值。


### [34] [Supervised Contrastive Learning for Ordinal Engagement Measurement](https://arxiv.org/abs/2505.20676)
*Sadaf Safa,Ali Abedi,Shehroz S. Khan*

Main category: cs.CV

TL;DR: 论文提出了一种基于视频的学生参与度测量方法，利用监督对比学习进行有序分类，解决了类别不平衡和顺序性问题。

- Motivation: 学生参与度对教育项目成功至关重要，自动化测量可帮助教师监控学生参与情况并调整教学策略。
- Method: 提取视频样本中的情感和行为特征，结合监督对比学习框架训练有序分类器，并应用时间序列数据增强技术。
- Result: 在公开数据集DAiSEE上验证了方法的有效性，展示了强大的参与度分类能力。
- Conclusion: 该方法为虚拟学习环境中学生参与度的理解和提升提供了重要贡献。


### [35] [Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors](https://arxiv.org/abs/2505.20680)
*Haodong Lu,Xinyu Zhang,Kristen Moore,Jason Xue,Lina Yao,Anton van den Hengel,Dong Gong*

Main category: cs.CV

TL;DR: 提出了一种基于增量提示调优的简洁持续学习方法TPPT，充分利用CLIP的多模态结构和文本表示的稳定性，通过双向监督策略减少遗忘并提高学习效果。

- Motivation: 现有方法依赖复杂设计，可能引入不必要的复杂性，未能充分利用CLIP的内在能力。
- Method: TPPT方法通过文本原型引导视觉提示的学习，并联合优化视觉和文本提示，同时引入关系多样性正则化。
- Result: 实验表明，该方法能有效学习新知识并减少遗忘，优于现有方法。
- Conclusion: TPPT展示了利用CLIP内在指导进行持续适应的优势。


### [36] [VisAlgae 2023: A Dataset and Challenge for Algae Detection in Microscopy Images](https://arxiv.org/abs/2505.20687)
*Mingxuan Sun,Juntao Jiang,Zhiqiang Yang,Shenao Kong,Jiamin Qi,Jianru Shang,Shuangling Luo,Wanfa Sun,Tianyi Wang,Yanqi Wang,Qixuan Wang,Tingjian Dai,Tianxiang Chen,Jinming Zhang,Xuerui Zhang,Yuepeng He,Pengcheng Fu,Qiu Guan,Shizheng Zhou,Yanbo Yu,Qigui Jiang,Teng Zhou,Liuyong Shi,Hong Yan*

Main category: cs.CV

TL;DR: VisAlgae 2023挑战赛旨在提升微藻细胞的高通量检测技术，吸引了369支团队参与，提供了1000张图像数据集，总结了前10名方法。

- Motivation: 微藻因其多样性和复杂环境条件难以检测，需结合计算机视觉技术提升检测效率。
- Method: 挑战赛提供六类微藻图像数据集，任务包括小目标检测、运动模糊处理和复杂背景应对。
- Result: 前10名方法展示了克服检测挑战的有效策略，提升了检测精度。
- Conclusion: 微藻研究与计算机视觉的结合为生态理解和科技进步提供了新方向。


### [37] [Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets](https://arxiv.org/abs/2505.20694)
*Xulin Gu,Xinhao Zhong,Zhixing Wei,Yimin Zhou,Shuoyang Sun,Bin Chen,Hongpeng Wang,Yuan Luo*

Main category: cs.CV

TL;DR: 提出了一种新的单级视频数据集蒸馏框架，通过优化合成视频以减少计算成本并保留时间动态。

- Motivation: 视频数据集蒸馏因高维度和时间复杂性而具有挑战性，现有方法计算成本高且难以保留时间动态。
- Method: 引入时间显著性引导的过滤机制，利用帧间差异指导蒸馏过程，保留信息时间线索并减少冗余。
- Result: 在标准视频基准测试中表现优异，缩小了真实与蒸馏视频数据之间的差距。
- Conclusion: 该方法为视频数据集压缩提供了可扩展的解决方案，性能达到最先进水平。


### [38] [Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation](https://arxiv.org/abs/2505.20704)
*Zixuan Hu,Yichun Hu,Xiaotong Li,Shixiang Tang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 论文提出了一种名为ReCAP的新方法，通过区域置信度优化解决WTTA中的噪声优化问题，显著提升了适应效率。

- Motivation: 现有方法主要关注样本选择策略，忽视了底层优化问题，尤其是熵最小化框架在噪声优化中的局限性。
- Method: 提出ReCAP方法，包括概率区域建模方案和有限到无限的渐进逼近，将难以处理的区域置信度转化为可操作的代理。
- Result: 实验表明，ReCAP在多种数据集和场景中均优于现有方法。
- Conclusion: ReCAP通过区域集成方法高效解决了WTTA中的优化问题，展现了显著优势。


### [39] [Hierarchical Instruction-aware Embodied Visual Tracking](https://arxiv.org/abs/2505.20710)
*Kui Wu,Hao Chen,Churan Wang,Fakhri Karray,Zhoujun Li,Yizhou Wang,Fangwei Zhong*

Main category: cs.CV

TL;DR: HIEVT提出了一种分层指令感知的视觉跟踪方法，通过空间目标桥接指令理解和动作生成，解决了UC-EVT任务中语言模型在推理速度和泛化性上的限制。

- Motivation: UC-EVT任务中，高级用户指令与低级代理动作之间存在巨大鸿沟，现有语言模型在推理速度或泛化性上存在不足。
- Method: HIEVT采用LLM将指令转化为空间目标，再通过离线强化学习策略实现目标对齐的视觉跟踪。
- Result: 实验和实际部署表明，HIEVT在多样环境、动态目标和复杂指令下具有鲁棒性和泛化性。
- Conclusion: HIEVT通过分层方法有效解决了UC-EVT任务中的挑战，展现了广泛适用性。


### [40] [MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding](https://arxiv.org/abs/2505.20715)
*Fuwen Luo,Shengfeng Lou,Chi Chen,Ziyue Wang,Chenliang Li,Weizhou Shen,Jiyue Guo,Peng Li,Ming Yan,Ji Zhang,Fei Huang,Yang Liu*

Main category: cs.CV

TL;DR: MUSEG是一种基于强化学习的方法，通过引入时间戳感知的多段定位，提升多模态大语言模型在视频时间理解上的能力。

- Motivation: 当前多模态大语言模型在细粒度时间推理上表现不佳，现有强化学习方法效果有限。
- Method: 提出MUSEG方法，结合时间戳感知的多段定位和分阶段奖励的强化学习训练方案。
- Result: 在时间定位和时间敏感视频QA任务上显著优于现有方法，且泛化能力强。
- Conclusion: MUSEG有效提升了视频时间理解能力，为多模态大语言模型的细粒度时间推理提供了新思路。


### [41] [VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models](https://arxiv.org/abs/2505.20718)
*Kui Wu,Shuhang Xu,Hao Chen,Churan Wang,Zhoujun Li,Yizhou Wang,Fangwei Zhong*

Main category: cs.CV

TL;DR: 提出了一种结合视觉语言模型（VLM）的自改进框架，用于增强具身视觉跟踪（EVT）系统在跟踪失败时的恢复能力。

- Motivation: 当前主动视觉跟踪系统在跟踪失败时恢复能力有限，需要一种更智能的解决方案。
- Method: 结合现成的主动跟踪方法和VLM的推理能力，快速视觉策略用于正常跟踪，失败时激活VLM推理，并通过记忆增强的自反思机制逐步改进。
- Result: 实验显示性能显著提升，成功率比基于RL的方法提高72%，比基于PID的方法提高220%。
- Conclusion: 首次将VLM推理集成到EVT中，为动态非结构化环境中的机器人应用提供了重要进展。


### [42] [LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation](https://arxiv.org/abs/2505.20723)
*Pascal Zwick,Nils Friederich,Maximilian Beichter,Lennart Hilbert,Ralf Mikut,Oliver Bringmann*

Main category: cs.CV

TL;DR: LeDiFlow提出了一种基于学习分布引导的流匹配方法，通过改进先验分布来优化图像生成效率和质量。

- Motivation: 扩散模型（DMs）的迭代过程导致高质量图像生成效率低下，流匹配（FM）虽提供了一种替代方案，但其基于高斯先验的路径对ODE求解器提出了挑战。
- Method: LeDiFlow通过学习一个更适合的先验分布，初始化ODE求解器，从而减少推理步骤。该方法结合了SOTA变压器架构和潜在空间采样。
- Result: LeDiFlow在像素空间推理速度提升3.75倍，潜在空间模型的图像质量提升1.32倍（CMMD指标）。
- Conclusion: LeDiFlow显著提升了FM模型的效率和图像质量，适用于消费级工作站。


### [43] [Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2505.20729)
*Xiangyu Sun,Runnan Chen,Mingming Gong,Dong Xu,Tongliang Liu*

Main category: cs.CV

TL;DR: 论文提出Intern-GS方法，利用视觉基础模型增强稀疏视图高斯泼溅，实现高质量场景重建。

- Motivation: 稀疏视图场景重建因数据有限导致信息不完整，现有方法效果不佳。
- Method: Intern-GS利用视觉基础模型指导3D高斯泼溅的初始化和优化，DUSt3R生成密集高斯点云，模型预测深度和外观以补全缺失信息。
- Result: 实验表明，Intern-GS在多个数据集（如LLFF、DTU）上达到最先进的渲染质量。
- Conclusion: Intern-GS通过结合视觉基础模型，显著提升了稀疏视图场景重建的效果。


### [44] [MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition](https://arxiv.org/abs/2505.20744)
*Hao Zhang,Zhan Zhuang,Xuehao Wang,Xiaodong Yang,Yu Zhang*

Main category: cs.CV

TL;DR: MoPFormer是一种自监督框架，通过将传感器信号转化为语义化的运动基元并利用Transformer架构提升可解释性和跨数据集泛化能力。

- Motivation: 解决可穿戴传感器在人类活动识别（HAR）中可解释性不足和跨数据集泛化能力差的问题。
- Method: 两阶段方法：1）将传感器信号分割并量化为运动基元；2）通过上下文感知嵌入模块和Transformer编码器处理。
- Result: 在六个HAR基准测试中表现优于现有方法，并显著提升跨数据集性能。
- Conclusion: MoPFormer通过捕捉基础运动模式，提升了可解释性和泛化能力。


### [45] [Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models](https://arxiv.org/abs/2505.20753)
*Yufei Zhan,Hongyin Zhao,Yousong Zhu,Shurong Zheng,Fan Yang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 论文提出了一种统一的视觉推理机制，提升大型多模态模型（LMMs）在复杂组合问题上的表现，无需多次推理或外部工具。

- Motivation: 当前LMMs在组合推理任务中表现不足，缺乏任务特定能力，阻碍其成为真正通用的视觉模型。
- Method: 引入类似人类的‘理解-思考-回答’过程，利用模型内在能力（如视觉理解和接地能力），在单次前向传播中完成推理。
- Result: 模型Griffon-R在VSR、CLEVR等复杂视觉推理基准上表现优异，同时在MMBench和ScienceQA等多模态任务中提升能力。
- Conclusion: 该机制填补了基础视觉能力与通用问答之间的鸿沟，使LMMs能生成可信且可追溯的复杂视觉推理答案。


### [46] [PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding](https://arxiv.org/abs/2505.20759)
*Ansel Blume,Jeonghwan Kim,Hyeonjeong Ha,Elen Chatikyan,Xiaomeng Jin,Khanh Duy Nguyen,Nanyun Peng,Kai-Wei Chang,Derek Hoiem,Heng Ji*

Main category: cs.CV

TL;DR: PARTONOMY是一个用于像素级部分定位的LMM基准测试，揭示了现有LMM在部分定位能力上的不足，并提出了改进模型PLUM。

- Motivation: 现实世界中的物体由独特的、特定于对象的部分组成，识别这些部分对细粒度和组合推理至关重要，但现有LMM在此任务上表现不佳。
- Method: 通过构建PARTONOMY基准测试（包含862个部分标签和534个对象标签），并设计PLUM模型（采用span tagging和反馈循环机制）来解决现有LMM的不足。
- Result: 实验显示现有LMM（如LISA-13B）在部分定位任务上表现较差（gIoU仅5.9%），而PLUM在多个任务上优于现有模型。
- Conclusion: PLUM为LMM的细粒度视觉理解提供了新方向，解决了部分定位的关键问题。


### [47] [ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval](https://arxiv.org/abs/2505.20764)
*Eric Xing,Pranavi Kolouju,Robert Pless,Abby Stylianou,Nathan Jacobs*

Main category: cs.CV

TL;DR: 论文提出了一种名为ConText-CIR的框架，通过Text Concept-Consistency损失函数提升图像和文本修改的表示准确性，并结合合成数据生成方法，在多个基准数据集上实现了最先进的性能。

- Motivation: 现有方法在组合图像检索（CIR）任务中难以准确表示图像和文本修改，导致性能不佳。
- Method: 提出ConText-CIR框架，使用Text Concept-Consistency损失函数，并设计合成数据生成流程。
- Result: 在CIRR和CIRCO等多个基准数据集上实现了监督和零样本设置下的最先进性能。
- Conclusion: ConText-CIR框架通过改进表示和合成数据方法，显著提升了CIR任务的性能。


### [48] [MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning](https://arxiv.org/abs/2505.20772)
*Hongjia Liu,Rongzhen Zhao,Haohan Chen,Joni Pajarinen*

Main category: cs.CV

TL;DR: MetaSlot是一种改进的Slot Attention变体，通过动态调整对象数量、去除重复槽位和注入噪声优化对象中心学习，显著提升了性能。

- Motivation: 现有对象中心学习方法因固定槽位数导致对象表示不准确，MetaSlot旨在解决这一问题。
- Method: MetaSlot通过维护对象原型代码本、去除重复槽位和注入噪声优化Slot Attention。
- Result: 在多个数据集和任务中，MetaSlot显著提升了性能并生成更可解释的槽位表示。
- Conclusion: MetaSlot是一种通用且高效的Slot Attention改进方法，适用于现有对象中心学习架构。


### [49] [TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs](https://arxiv.org/abs/2505.20777)
*Zhehan Kan,Yanlin Liu,Kun Yin,Xinghua Jiang,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Qingmin Liao,Wenming Yang*

Main category: cs.CV

TL;DR: TACO是一种新型强化学习算法，通过Think-Answer Consistency和Rollback Resample Strategy解决多模态推理中的问题，提升模型稳定性和数据效率。

- Motivation: 现有方法在多模态推理中存在推理与答案不一致、模型不稳定和低数据效率等问题，需要改进。
- Method: 提出TACO算法，结合GRPO框架，引入Think-Answer Consistency、Rollback Resample Strategy和自适应学习计划。
- Result: 在REC和VQA任务中，TACO显著提升了性能。
- Conclusion: TACO在多模态推理中表现出色，解决了现有方法的局限性。


### [50] [Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks](https://arxiv.org/abs/2505.20782)
*Taïga Gonçalves,Tomo Miyazaki,Shinichiro Omachi*

Main category: cs.CV

TL;DR: CD-MTA是一种生成对抗样本的方法，能够误导图像分类器至任何目标类别，包括未见过的类别，解决了传统攻击方法的局限性。

- Motivation: 传统目标攻击方法每模型仅支持一个类别，需昂贵重训练；现有多目标攻击方法依赖训练数据，导致数据泄漏。CD-MTA旨在解决这些问题。
- Method: CD-MTA采用基于图像的条件输入和类别无关损失，消除对类别语义的依赖，实现跨数据集泛化。
- Result: 在ImageNet等数据集上，CD-MTA在标准和跨域场景中均优于现有方法，且无需访问黑盒模型的训练数据。
- Conclusion: CD-MTA通过创新设计实现了对未见类别的泛化，提升了对抗攻击的实用性。


### [51] [Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models](https://arxiv.org/abs/2505.20789)
*Yang Zheng,Wen Li,Zhaoqiang Liu*

Main category: cs.CV

TL;DR: 本文提出了两种新方法DMILO和DMILO-PGD，以解决基于扩散模型的逆问题求解中的计算负担和收敛性问题。

- Motivation: 传统方法依赖手工先验，无法捕捉真实数据的复杂性，而现有扩散模型方法存在计算量大和收敛性差的问题。
- Method: DMILO通过中间层优化减轻内存负担，并引入稀疏偏差扩展模型范围；DMILO-PGD结合中间层优化和投影梯度下降以改善收敛性。
- Result: 在多种图像数据集上的实验表明，DMILO和DMILO-PGD显著优于现有方法。
- Conclusion: DMILO和DMILO-PGD有效解决了扩散模型在逆问题求解中的常见挑战，性能显著提升。


### [52] [Rendering-Aware Reinforcement Learning for Vector Graphics Generation](https://arxiv.org/abs/2505.20793)
*Juan A. Rodriguez,Haotian Zhang,Abhay Puri,Aarash Feizi,Rishav Pramanik,Pascal Wichmann,Arnab Mondal,Mohammad Reza Samsami,Rabiul Awal,Perouz Taslakian,Spandana Gella,Sai Rajeswar,David Vazquez,Christopher Pal,Marco Pedersoli*

Main category: cs.CV

TL;DR: 论文提出RLRF方法，通过渲染反馈增强自回归视觉语言模型（VLM）的SVG生成能力，显著提升生成SVG的准确性和效率。

- Motivation: 现有VLM方法在生成SVG时难以保证忠实性和效率，因为它们未在训练中观察渲染图像。
- Method: 引入RLRF，利用渲染后的SVG输出与原图比较的视觉保真度反馈，通过强化学习优化SVG生成。
- Result: RLRF显著优于监督微调，能生成更准确、高效且语义连贯的SVG。
- Conclusion: RLRF通过渲染反馈解决了现有方法的局限性，实现了高质量SVG生成。


### [53] [Not All Thats Rare Is Lost: Causal Paths to Rare Concept Synthesis](https://arxiv.org/abs/2505.20808)
*Bo-Kai Ruan,Zi-Xiang Ni,Bo-Lun Huang,Teng-Fang Hsiao,Hong-Han Shuai*

Main category: cs.CV

TL;DR: RAP框架通过将罕见概念生成视为潜在因果路径，利用频繁提示近似罕见提示，并通过动态切换和二阶去噪机制提升生成效果。

- Motivation: 扩散模型在生成高频概念时表现良好，但在罕见概念生成上效果不佳，需要一种更系统的方法来解决这一问题。
- Method: 提出RAP框架，将罕见概念生成视为潜在因果路径，利用频繁提示近似罕见提示，并通过动态切换和二阶去噪机制优化生成过程。
- Result: RAP在多种扩散模型上表现优异，显著提升了罕见概念的生成质量，优于现有基线方法。
- Conclusion: RAP通过因果路径和动态提示切换，有效解决了罕见概念生成问题，为扩散模型提供了更强大的生成能力。


### [54] [Frame-Level Captions for Long Video Generation with Complex Multi Scenes](https://arxiv.org/abs/2505.20827)
*Guangcong Zheng,Jianlong Yuan,Bo Wang,Haoyang Huang,Guoqing Ma,Nan Duan*

Main category: cs.CV

TL;DR: 提出一种新方法，通过帧级标注和注意力机制解决长视频生成中的误差累积和多场景问题，并在复杂场景中表现优异。

- Motivation: 当前自回归扩散模型在生成长视频时存在误差累积问题，且现有方法难以处理多场景复杂故事。
- Method: 提出帧级标注数据集和帧级注意力机制，结合Diffusion Forcing训练模型，实现精确文本-视频匹配。
- Result: 在VBench 2.0基准测试中表现优异，能生成高质量多场景长视频。
- Conclusion: 新方法有效解决了长视频生成中的问题，计划公开数据集和模型。


### [55] [Causality-Driven Infrared and Visible Image Fusion](https://arxiv.org/abs/2505.20830)
*Linli Ma,Suzhen Lin,Jianchao Zeng,Zanxia Jin,Yanbo Wang,Fengyuan Li,Yubing Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于因果关系的图像融合方法，通过构建因果图消除数据集场景偏差的影响，并设计BAFFM模块提升融合性能。

- Motivation: 现有方法忽略数据集场景偏差对模型训练的影响，导致模型学习到虚假相关性，限制了融合性能。
- Method: 从因果视角重新审视图像融合任务，构建因果图消除偏差影响，并提出BAFFM模块消除混杂因素干扰。
- Result: 在三个标准数据集上的实验表明，该方法显著优于现有红外与可见光图像融合方法。
- Conclusion: 通过因果分析和BAFFM模块，有效提升了图像融合性能。


### [56] [Fully Spiking Neural Networks for Unified Frame-Event Object Tracking](https://arxiv.org/abs/2505.20834)
*Jingjun Yang,Liangwei Fan,Jinpu Zhang,Xiangkai Lian,Hui Shen,Dewen Hu*

Main category: cs.CV

TL;DR: 提出了一种名为SpikeFET的全脉冲框架事件跟踪框架，通过卷积局部特征提取和基于Transformer的全局建模，高效融合图像和事件流数据，同时降低功耗。

- Motivation: 当前融合方法在复杂环境中实现高性能时计算开销大，且难以高效提取事件流中的稀疏异步信息，未能充分利用事件驱动脉冲范式的能效优势。
- Method: 提出SpikeFET框架，结合卷积局部特征提取和Transformer全局建模；引入随机拼图模块（RPM）消除位置偏差；提出时空正则化（STR）策略增强特征一致性。
- Result: 在多个基准测试中表现优于现有方法，显著降低功耗，实现性能与效率的最佳平衡。
- Conclusion: SpikeFET框架在视觉目标跟踪中实现了高性能与低功耗的协同优化，代码将公开。


### [57] [ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient](https://arxiv.org/abs/2505.20858)
*Jason Chui,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出了一种新的概率化Bundle Adjustment方法（ProBA），无需相机位姿或焦距的先验知识，通过建模和传播2D观测与3D场景结构的不确定性，实现更鲁棒的优化。

- Motivation: 传统BA方法依赖准确的初始估计和已知相机内参，限制了其在不确定或未知内参情况下的适用性。
- Method: 使用3D高斯模型代替点状地标，引入不确定性感知的重投影损失，并通过Bhattacharyya系数强制几何一致性。
- Result: ProBA在挑战性真实场景中优于传统方法，减少了对强初始化和已知内参的需求。
- Conclusion: ProBA提升了SLAM系统在非结构化环境中的实用性。


### [58] [Exploring Timeline Control for Facial Motion Generation](https://arxiv.org/abs/2505.20861)
*Yifeng Ma,Jinwei Qi,Chaonan Ji,Peng Zhang,Bang Zhang,Zhidong Deng,Liefeng Bo*

Main category: cs.CV

TL;DR: 本文提出了一种新的面部动作生成控制信号：时间线控制，相比音频和文本信号，时间线能提供更精细的控制。

- Motivation: 现有方法（如音频和文本信号）对面部动作生成的控制不够精细，无法精确指定动作的时序。
- Method: 1. 使用Toeplitz逆协方差聚类标注面部动作的时间区间；2. 提出基于扩散的生成模型，支持时间线输入；3. 结合ChatGPT将文本转换为时间线。
- Result: 实验表明，方法能准确标注面部动作区间，并生成与时间线精确对齐的自然面部动作。
- Conclusion: 时间线控制为面部动作生成提供了更精细的时序控制能力，结合文本引导进一步扩展了应用场景。


### [59] [AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding](https://arxiv.org/abs/2505.20862)
*Chaeyoung Jung,Youngjoon Jang,Joon Son Chung*

Main category: cs.CV

TL;DR: 本文提出了一种新型的解码框架AVCD，用于抑制多模态大语言模型中的幻觉问题，通过动态识别主导模态和注意力掩码来优化解码效果。

- Motivation: 多模态大语言模型（AV-LLMs）中的幻觉问题复杂，涉及音频、视频和语言的跨模态组合，需要一种更自适应的解码策略。
- Method: 提出Audio-Visual Contrastive Decoding（AVCD），利用注意力分布动态识别主导模态，并通过注意力掩码生成扰动输出对数。同时引入熵引导的自适应解码以提高效率。
- Result: 在AVHBench数据集上，AVCD显著提升了模型性能，VideoLLaMA2和video-SALMONN的准确率分别提高了6%和11%。
- Conclusion: AVCD是一种高效且通用的解码框架，能有效抑制多模态模型中的幻觉问题。


### [60] [In Context Learning with Vision Transformers: Case Study](https://arxiv.org/abs/2505.20872)
*Antony Zhao,Alex Proshkin,Fergal Hennessy,Francesco Crivelli*

Main category: cs.CV

TL;DR: 论文探讨了大型Transformer模型在图像空间中通过上下文学习复杂函数（如卷积神经网络）的能力。

- Motivation: 研究旨在扩展Transformer模型在随机数据上学习简单函数的能力，探索其在图像空间中学习更复杂函数的潜力。
- Method: 利用上下文学习（如少样本、单样本或零样本学习）方法，通过提示中的示例和查询，分析模型在图像空间的表现。
- Result: 模型能够学习线性函数和小型两层神经网络，研究进一步探索其在图像空间中的能力。
- Conclusion: 研究为Transformer模型在图像空间中学习复杂函数提供了新的方向。


### [61] [Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models](https://arxiv.org/abs/2505.20873)
*Chaeyoung Jung,Youngjoon Jang,Jongmin Choi,Joon Son Chung*

Main category: cs.CV

TL;DR: 提出Fork-Merge Decoding (FMD)方法，通过推理阶段的分支-合并策略解决音频-视觉大语言模型中的模态偏差问题，无需额外训练。

- Motivation: 当前音频-视觉大语言模型（AV-LLMs）在处理多模态特征时存在模态偏差问题，模型可能过度依赖某一模态。
- Method: 提出FMD方法，在推理阶段先分别处理音频和视频输入（分支阶段），再合并隐藏状态进行联合推理（合并阶段）。
- Result: 在VideoLLaMA2和video-SALMONN模型上验证，FMD在音频、视频及联合任务上均表现提升。
- Conclusion: FMD通过推理阶段的干预有效提升多模态理解的鲁棒性。


### [62] [Stereo Radargrammetry Using Deep Learning from Airborne SAR Images](https://arxiv.org/abs/2505.20876)
*Tatsuya Sasayama,Shintaro Ito,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的机载SAR图像立体雷达测量方法，通过创建数据集和微调深度学习模型，解决了传统方法受几何图像调制影响的问题。

- Motivation: 当前缺乏用于训练深度学习方法的公开SAR图像数据集，且传统方法易受几何图像调制影响。
- Method: 创建SAR图像数据集，微调深度学习图像对应方法，避免地面投影，分块处理图像。
- Result: 实验表明，该方法比传统方法具有更广范围和更高精度的高程测量能力。
- Conclusion: 提出的方法有效提升了SAR图像立体测量的精度和范围。


### [63] [YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation](https://arxiv.org/abs/2505.20884)
*Weichao Pan,Bohan Xu,Xu Wang,Chengze Lv,Shuoyang Wang,Zhenke Duan*

Main category: cs.CV

TL;DR: 提出了一种基于YOLO的火灾检测模型YOLO-FireAD，通过注意力引导的倒置残差块和双池化下采样融合块提升特征提取能力，减少信息丢失，实现了高效且准确的火灾检测。

- Motivation: 解决动态环境中火灾检测面临的干扰问题（如光照变化、误检和漏检），并克服现有YOLO模型在特征提取和信息保留上的局限性。
- Method: 1. 注意力引导的倒置残差块（AIR）结合混合通道-空间注意力，增强火灾特征并抑制环境噪声；2. 双池化下采样融合块（DPDF）通过可学习的最大-平均池化融合保留多尺度火灾模式。
- Result: 在公开数据集上表现优异，参数量（1.45M）和计算量（4.6G）显著低于YOLOv8n，mAP75比主流实时目标检测模型高1.3-5.5%。
- Conclusion: YOLO-FireAD在效率和准确性上均优于现有模型，适用于动态环境中的火灾检测。


### [64] [Frequency Composition for Compressed and Domain-Adaptive Neural Networks](https://arxiv.org/abs/2505.20890)
*Yoojin Kwon,Hongjun Suh,Wooseok Lee,Taesik Gong,Songyi Han,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: CoDA框架通过频率组合统一模型压缩和域适应，结合量化感知训练和测试时适应，显著提升压缩模型在域偏移下的性能。

- Motivation: 现代设备上的神经网络应用需在资源受限且域偏移不可预测的条件下运行，但现有方法分别处理压缩和域适应，未解决两者的结合问题。
- Method: CoDA在训练时使用量化感知训练（QAT）和低频成分学习通用特征，测试时利用全频信息进行源无关的域适应，高频成分作为域特定线索。
- Result: 在CIFAR10-C和ImageNet-C等域偏移基准测试中，CoDA显著提升了压缩模型的准确率，分别比全精度TTA基线提高了7.96%和5.37%。
- Conclusion: CoDA框架有效统一了模型压缩和域适应，为资源受限环境下的神经网络应用提供了实用解决方案。


### [65] [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/abs/2505.20897)
*Pingrui Zhang,Yifei Su,Pengyuan Wu,Dong An,Li Zhang,Zhigang Wang,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种基于语言的自适应想象方法（ATD），通过双分支架构结合逻辑推理与想象力，优化视觉与语言导航任务。

- Motivation: 解决视觉与语言导航中感知与语言对齐困难的问题，避免传统视觉合成方法的高计算成本和冗余细节。
- Method: 设计双分支自引导想象策略（ATD），利用LLM的Q-former微调激活领域知识，动态更新逻辑推理与想象，并通过交叉交互机制注入导航专家模块。
- Result: 在R2R基准测试中达到最优性能，且参数更少。
- Conclusion: ATD通过语言形式的自适应想象，实现了更可靠高效的导航策略。


### [66] [HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion](https://arxiv.org/abs/2505.20904)
*Guanghu Xie,Yonglong Zhang,Zhiduo Jiang,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.CV

TL;DR: HTMNet是一种结合Transformer、CNN和Mamba架构的混合模型，用于解决透明和反射物体深度信息不完整的问题，取得了SOTA性能。

- Motivation: 透明和反射物体导致深度传感器信息不完整，影响机器人感知和操作任务。
- Method: 采用双分支Transformer-CNN编码器，结合Transformer-Mamba的多尺度融合模块，并设计基于自注意力机制和状态空间模型的多模态融合模块。
- Result: 在多个公开数据集上验证，模型性能达到SOTA。
- Conclusion: HTMNet在透明物体深度补全领域首次应用Mamba架构，展示了其潜力，并通过多尺度融合模块有效整合特征。


### [67] [Create Anything Anywhere: Layout-Controllable Personalized Diffusion Model for Multiple Subjects](https://arxiv.org/abs/2505.20909)
*Wei Li,Hebei Li,Yansong Peng,Siying Wu,Yueyi Zhang,Xiaoyan Sun*

Main category: cs.CV

TL;DR: LCP-Diffusion模型通过动态-静态互补视觉精炼模块和双重布局控制机制，实现了无调优的个性化图像生成，同时提升了身份保留和布局可控性。

- Motivation: 现有方法在文本到图像生成中缺乏精确的布局控制和动态特征利用，影响了生成图像的保真度。
- Method: 提出LCP-Diffusion模型，结合动态-静态互补视觉精炼模块和双重布局控制机制，实现无调优的个性化生成。
- Result: 实验验证LCP-Diffusion在身份保留和布局可控性方面表现优异。
- Conclusion: LCP-Diffusion是首个支持用户“在任何地方创建任何内容”的框架。


### [68] [Geometry-Editable and Appearance-Preserving Object Compositon](https://arxiv.org/abs/2505.20914)
*Jianman Lin,Haojie Li,Chunmei Qing,Zhijing Yang,Liang Lin,Tianshui Chen*

Main category: cs.CV

TL;DR: DGAD模型通过解耦几何编辑和外观保留，结合语义嵌入和交叉注意力机制，实现精确的几何编辑和外观一致性。

- Motivation: 现有方法仅依赖高层次的语义嵌入，丢失了细粒度的外观细节，DGAD旨在解决这一问题。
- Method: 利用CLIP/DINO提取语义嵌入和外观特征，通过预训练扩散模型捕获几何信息，并设计交叉注意力机制对齐外观。
- Result: 在公开基准测试中，DGAD表现出色，实现了灵活的几何编辑和外观一致性。
- Conclusion: DGAD通过解耦几何和外观处理，显著提升了对象合成的效果。


### [69] [HuMoCon: Concept Discovery for Human Motion Understanding](https://arxiv.org/abs/2505.20920)
*Qihang Fang,Chengcheng Tang,Bugra Tekin,Shugao Ma,Yanchao Yang*

Main category: cs.CV

TL;DR: HuMoCon是一种新颖的运动视频理解框架，用于高级人类行为分析，通过多模态编码器提取语义特征，解决了特征对齐和高频信息丢失问题。

- Motivation: 解决运动概念发现中的多模态特征对齐不足和高频信息丢失问题。
- Method: 结合视频上下文理解和运动交互建模，引入速度重建机制以减少时间平滑。
- Result: 在标准基准测试中显著优于现有方法。
- Conclusion: HuMoCon有效实现了运动概念发现，并将开源代码。


### [70] [Good Enough: Is it Worth Improving your Label Quality?](https://arxiv.org/abs/2505.20928)
*Alexander Jaus,Zdravko Marinov,Constantin Seibold,Simon Reiß,Jens Kleesiek,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 评估医学图像分割中标签质量的影响，发现高质量标签对域内性能有提升，但对预训练影响较小。

- Motivation: 研究标签质量对医学图像分割性能的实际影响，以指导资源分配。
- Method: 使用nnU-Net、TotalSegmentator和MedSAM生成多版本伪标签CT数据集，系统评估标签质量的影响。
- Result: 高质量标签提升域内性能，但对预训练效果影响有限。
- Conclusion: 标签质量提升的效益有限，需权衡资源投入。


### [71] [QwT-v2: Practical, Effective and Efficient Post-Training Quantization](https://arxiv.org/abs/2505.20932)
*Ningyuan Tang,Minghao Fu,Hao Yu,Jianxin Wu*

Main category: cs.CV

TL;DR: QwT-v2改进QwT方法，通过轻量级通道仿射补偿模块减少额外参数和计算，提升兼容性和性能。

- Motivation: 解决QwT方法的额外参数、延迟和硬件兼容性问题。
- Method: 采用轻量级通道仿射补偿（CWAC）模块。
- Result: QwT-v2减少额外开销，性能优于QwT，兼容多数硬件平台。
- Conclusion: QwT-v2是高效且兼容性强的网络量化改进方法。


### [72] [ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation](https://arxiv.org/abs/2505.20935)
*Sanghyun Jo,Wooyeol Lee,Ziseok Lee,Kyungsu Kim*

Main category: cs.CV

TL;DR: ISAC是一种无需训练的方法，通过实例优先建模解决多实例场景中对象合并或遗漏的问题，显著提升多类和多实例准确性。

- Motivation: 现有文本到图像扩散模型在多实例场景中表现不佳，常合并或遗漏对象，需解决实例个体化和语义纠缠问题。
- Method: 提出Instance-to-Semantic Attention Control (ISAC)，采用实例优先建模和分层树状提示机制，分离多对象实例并与其语义标签对齐。
- Result: ISAC在不使用外部模型的情况下，多类准确率平均提升52%，多实例准确率平均提升83%。
- Conclusion: ISAC通过实例优先建模有效解决了多实例场景中的问题，性能显著优于现有方法。


### [73] [PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter](https://arxiv.org/abs/2505.20941)
*Yaohua Zha,Yanzi Wang,Hang Guo,Jinpeng Wang,Tao Dai,Bin Chen,Zhihao Ouyang,Xue Yuerong,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 论文提出Point Mamba Adapter (PMA)，利用预训练模型所有层的特征序列，结合Mamba融合多层级信息，提升点云理解能力。

- Motivation: 现有方法仅利用预训练模型的最终输出，忽略了中间层的丰富互补信息，未能充分发挥预训练模型的潜力。
- Method: 提出PMA，构建预训练模型所有层的特征序列，利用Mamba融合多层级语义；设计G2PG生成器，动态优化空间顺序。
- Result: 在多个点云数据集上的实验表明，PMA通过融合中间特征显著提升了点云理解能力。
- Conclusion: PMA通过多层级特征融合，为点云理解提供了更全面的解决方案。


### [74] [DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2505.20951)
*Naiyu Fang,Zheyuan Zhou,Kang Wang,Ruibo Li,Lemiao Qiu,Shuyou Zhang,Zhe Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: DSOcc通过深度感知和语义辅助提升基于相机的3D语义占用预测，联合推断占用状态和类别，实验表明其在SemanticKITTI数据集上表现最佳。

- Motivation: 现有方法依赖显式占用状态推断导致特征分配错误，且样本不足限制占用类别推断学习。
- Method: 通过非学习方法计算软占用置信度并与图像特征结合，实现深度感知的自适应隐式占用状态推断；利用训练好的图像语义分割和多帧融合辅助占用类别推断。
- Result: 在SemanticKITTI数据集上，DSOcc在基于相机的方法中达到最优性能。
- Conclusion: DSOcc通过深度和语义辅助显著提升了3D语义占用预测的准确性和鲁棒性。


### [75] [OrienText: Surface Oriented Textual Image Generation](https://arxiv.org/abs/2505.20958)
*Shubham Singh Paliwal,Arushi Jain,Monika Sharma,Vikram Jamwal,Lovekesh Vig*

Main category: cs.CV

TL;DR: OrienText方法通过利用区域特定表面法线作为条件输入，改进了文本到图像生成模型在复杂表面上准确嵌入文本的能力。

- Motivation: 电商等领域需要图像中准确嵌入文本，但现有T2I模型在复杂表面上表现不佳。
- Method: 提出OrienText方法，利用表面法线作为条件输入，优化文本在图像中的渲染和方向。
- Result: 在自建数据集上验证了OrienText的有效性，优于现有方法。
- Conclusion: OrienText显著提升了复杂表面上文本生成的准确性。


### [76] [RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes](https://arxiv.org/abs/2505.20967)
*Jiarui Zhang,Zhihao Li,Chong Wang,Bihan Wen*

Main category: cs.CV

TL;DR: RF4D是一个基于雷达的神经场框架，专为动态户外场景的新视角合成设计，通过显式整合时空信息提升性能。

- Motivation: 现有神经场方法在恶劣天气下表现脆弱，而毫米波雷达对环境变化具有鲁棒性，但其与神经场的结合尚未充分探索。动态场景的时空建模需求也未被满足。
- Method: RF4D引入时间信息表示和特征级流模块，预测帧间潜在时间偏移，增强动态场景建模的时序一致性。还提出雷达功率渲染公式，提升合成精度。
- Result: 在公开雷达数据集上的实验显示，RF4D在雷达测量合成质量和占用估计准确性上表现优异，尤其在动态户外场景中改进显著。
- Conclusion: RF4D通过雷达和神经场的结合，显著提升了动态户外场景的新视角合成性能，填补了现有方法的不足。


### [77] [DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization](https://arxiv.org/abs/2505.20975)
*Shamil Ayupov,Maksim Nakhodnov,Anastasia Yaschenko,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的个性化扩散模型方法，通过生成合成配对数据集优化概念保真度和上下文对齐。

- Motivation: 解决文本到图像生成中概念保真度与上下文对齐的平衡问题。
- Method: 利用外部质量指标生成合成配对数据集，进行类似DPO的训练，支持灵活调整图像保真度与文本对齐的权衡。
- Result: 在多步训练中表现优于基线方法，收敛速度和输出质量均有提升。
- Conclusion: 该方法在各种架构和微调技术中均表现出有效性，代码已开源。


### [78] [RefAV: Towards Planning-Centric Scenario Mining](https://arxiv.org/abs/2505.20981)
*Cainan Davidson,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉语言模型（VLMs）的空间-时间场景挖掘方法RefAV，用于从自动驾驶车辆日志中检测和定位复杂场景，并发布了大规模数据集。

- Motivation: 传统场景挖掘方法依赖手工查询，效率低且易出错，而自动驾驶车辆日志中的关键场景识别仍具挑战性。
- Method: 利用视觉语言模型（VLMs）检测和定位日志中描述的场景，并引入RefAV数据集（10,000个自然语言查询）。
- Result: 实验表明，直接使用现成VLMs效果不佳，场景挖掘存在独特挑战。
- Conclusion: RefAV为场景挖掘提供了新方法，但需进一步优化VLMs以适应这一任务。


### [79] [Assessing the Use of Face Swapping Methods as Face Anonymizers in Videos](https://arxiv.org/abs/2505.20985)
*Mustafa İzzet Muştu,Hazım Kemal Ekenel*

Main category: cs.CV

TL;DR: 本文探讨了人脸交换技术在视频隐私保护中的潜力，通过评估时间一致性、匿名强度和视觉保真度，发现该方法能有效隐藏身份且保持数据质量。

- Motivation: 大规模视觉数据需求与隐私法规的严格性促使研究匿名化方法，以在不显著降低数据质量的情况下隐藏个人身份。
- Method: 采用人脸交换技术，重点评估其在视频数据中的时间一致性、匿名强度和视觉保真度。
- Result: 人脸交换技术能产生一致的面部过渡并有效隐藏身份，适合隐私保护视频应用。
- Conclusion: 人脸交换技术为隐私保护视频应用提供了可行方案，并为未来匿名化研究奠定了基础。


### [80] [Facial Attribute Based Text Guided Face Anonymization](https://arxiv.org/abs/2505.21002)
*Mustafa İzzet Muştu,Hazım Kemal Ekenel*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的面部匿名化流程，利用扩散模型生成自然且不可识别的面部图像，以解决隐私合规数据集的需求。

- Motivation: 计算机视觉应用中处理大量包含个人信息的视觉数据时，隐私保护成为关键问题。现有方法难以在不侵犯隐私的情况下收集高质量数据集。
- Method: 采用三阶段流程：RetinaNet进行面部检测，VGG-Face提取特征，BrushNet扩散模型生成匿名面部图像。
- Result: 生成的图像自然且不可识别，满足隐私合规要求，适用于计算机视觉研究。
- Conclusion: 该方法无需训练GAN，通过扩散模型实现了高效且隐私保护的面部匿名化。


### [81] [Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains](https://arxiv.org/abs/2505.21010)
*Sabbir Ahmed,Mamshad Nayeem Rizve,Abdullah Al Arafat,Jacqueline Liu,Rahim Hossain,Mohaiminul Al Nahian,Adnan Siraj Rakin*

Main category: cs.CV

TL;DR: 论文提出了一种名为统一对齐协议（UAP）的新框架，用于解决半监督联邦学习（SSFL）中的领域泛化问题，通过两阶段训练过程提升模型在新领域的泛化能力。

- Motivation: 由于现实应用中客户端数据标签有限且领域偏移频繁发生，传统SSFL假设训练和测试数据分布相同的问题限制了其实际应用。因此，需要一种方法提升模型在未见领域的泛化能力。
- Method: UAP框架包含两阶段训练：1）服务器模型学习特征并与参数分布对齐；2）客户端利用服务器特征分布对齐自身特征。
- Result: 在多个标准领域泛化基准数据集上的实验表明，UAP在SSFL设置中实现了最先进的泛化性能。
- Conclusion: UAP通过创新的两阶段训练方法，有效解决了SSFL中的领域泛化问题，提升了模型的实用性。


### [82] [FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models](https://arxiv.org/abs/2505.21032)
*Nils Neukirch,Johanna Vielhaben,Nils Strodthoff*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的方法，用于高保真地从特征空间映射到输入空间，以改进深度神经网络内部表示的可解释性。

- Motivation: 深度神经网络的内部表示难以解释，现有方法通常依赖粗略近似。
- Method: 使用预训练的条件扩散模型，以概率方式学习从特征空间到输入空间的映射。
- Result: 在多种预训练图像分类器（从CNN到ViT）上展示了优秀的重建能力，并通过定性比较和鲁棒性分析验证了方法。
- Conclusion: 该方法在计算机视觉模型中具有广泛潜力，可用于特征空间的可视化和理解。


### [83] [RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy](https://arxiv.org/abs/2505.21036)
*Aiyue Chen,Bin Dong,Jingru Li,Jing Lin,Yiwu Yao,Gongyi Wang*

Main category: cs.CV

TL;DR: RainFusion是一种无需训练的新型稀疏注意力方法，通过利用视觉数据中的固有稀疏性加速注意力计算，同时保持视频质量。

- Motivation: 扩散模型在视频生成中计算成本高，3D注意力占用了大量资源。
- Method: 提出RainFusion，利用空间、时间和纹理三种稀疏模式，通过自适应识别模块（ARM）在线确定稀疏模式。
- Result: 在多个领先模型上验证，RainFusion实现2倍以上的注意力计算加速，视频质量几乎不受影响（VBench分数仅下降0.2%）。
- Conclusion: RainFusion是一种即插即用的高效方法，适用于现有3D注意力视频生成模型。


### [84] [Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing](https://arxiv.org/abs/2505.21049)
*Dehao Wang,Haohang Zhu,Yiwen Xu,Kaiqi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种结合目标检测和单目深度估计的鲁棒性道路坑洼面积估计框架，通过改进的YOLOv8模型和深度估计技术，显著提升了检测和估计的准确性。

- Motivation: 道路坑洼对驾驶安全和舒适性构成严重威胁，现有视觉方法易受相机角度和路面平坦假设影响，导致误差较大。
- Method: 提出ACSH-YOLOv8模型增强特征提取和小目标检测，结合BoT-SORT跟踪和DepthAnything V2深度估计，采用MBTP方法估计坑洼面积，并通过CDKF优化结果一致性。
- Result: ACSH-YOLOv8的AP(50)达到76.6%，比YOLOv8提升7.6%；CDKF优化使预测更鲁棒。
- Conclusion: 该方法显著提升了坑洼检测和面积估计的准确性，增强了实际应用价值。


### [85] [Advancing high-fidelity 3D and Texture Generation with 2.5D latents](https://arxiv.org/abs/2505.21050)
*Xin Yang,Jiantao Lin,Yingjie Xu,Haodong Li,Yingcong Chen*

Main category: cs.CV

TL;DR: 提出了一种联合生成3D几何和纹理的新框架，通过2.5D潜在表示和轻量级解码器，显著提升了生成质量。

- Motivation: 现有方法中3D几何和纹理生成分离导致不连贯，且数据质量不均影响性能。
- Method: 整合多视角RGB、法线和坐标图像为2.5D潜在表示，利用预训练2D模型生成2.5D，再通过轻量级解码器转为3D。
- Result: 模型在生成高质量3D对象及纹理方面优于现有方法。
- Conclusion: 该框架有效解决了3D生成中的几何与纹理一致性问题，性能显著提升。


### [86] [Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles](https://arxiv.org/abs/2505.21060)
*Peng Wang,Xiang Liu,Peidong Liu*

Main category: cs.CV

TL;DR: 提出了一种快速3D场景风格化方法，通过分支架构分离结构建模和外观着色，实现了多视角一致性和高效性。

- Motivation: 解决现有3D风格化方法计算密集、依赖密集输入图像的问题，实现快速且高质量的3D场景风格化。
- Method: 采用分支架构分离结构和外观，结合身份损失进行预训练，支持稀疏视图输入和任意风格图像。
- Result: 在多种数据集上验证了方法的高质量风格化效果，优于现有方法的多视角一致性和效率。
- Conclusion: 该方法在保持3D结构的同时实现了快速风格化，为3D内容创作提供了高效工具。


### [87] [LPOI: Listwise Preference Optimization for Vision Language Models](https://arxiv.org/abs/2505.21061)
*Fatemeh Pesaran Zadeh,Yoojin Oh,Gunhee Kim*

Main category: cs.CV

TL;DR: LPOI是一种针对视觉语言模型（VLM）的列表式偏好优化方法，通过掩蔽关键对象并插值生成渐进图像序列，减少幻觉并保持视觉保真度。

- Motivation: 现有方法（如RLHF和DPO）容易过拟合文本信息或加剧幻觉，而负样本增强效果有限。LPOI旨在填补列表式优化在VLM中的空白。
- Method: LPOI通过掩蔽关键对象并在正负样本间插值生成渐进图像序列，训练模型按对象可见性排序，无需额外标注。
- Result: 在MMHalBench、AMBER和Object HalBench上，LPOI在减少幻觉和提升VLM性能方面优于现有方法。
- Conclusion: LPOI为VLM的偏好优化提供了一种高效且无需额外标注的解决方案，显著减少幻觉并提升性能。


### [88] [Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals](https://arxiv.org/abs/2505.21062)
*Davide Lobba,Fulvio Sanguigni,Bin Ren,Marcella Cornia,Rita Cucchiara,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一种名为TEMU-VTOFF的新架构，用于解决虚拟试脱（VTOFF）任务，通过多模态输入和双DiT主干网络提升服装特征提取能力，显著提高了生成图像的视觉质量和保真度。

- Motivation: 虚拟试脱（VTOFF）任务旨在从穿着服装的个体照片中生成标准化的服装产品图像，但现有方法在处理遮挡、复杂姿势和单类别限制时存在不足。本文旨在解决这些问题。
- Method: 提出TEMU-VTOFF架构，采用双DiT主干网络和改进的多模态注意力机制，结合图像、文本和掩码等多模态输入，并引入对齐模块优化生成细节。
- Result: 在VITON-HD和Dress Code数据集上的实验表明，TEMU-VTOFF在VTOFF任务中达到了新的最佳性能，显著提升了视觉质量和服装保真度。
- Conclusion: TEMU-VTOFF通过多模态输入和优化架构，有效解决了VTOFF任务中的关键挑战，为数据生成和数据集增强提供了有力工具。


### [89] [Minute-Long Videos with Dual Parallelisms](https://arxiv.org/abs/2505.21070)
*Zeqing Wang,Bowen Zheng,Xingyi Yang,Yuecong Xu,Xinchao Wang*

Main category: cs.CV

TL;DR: 提出DualParal分布式推理策略，通过并行化时间帧和模型层，结合块式去噪和特征缓存，显著降低长视频生成的延迟和内存成本。

- Motivation: 解决基于DiT的视频扩散模型在处理长视频时的高延迟和高内存成本问题。
- Method: 采用DualParal策略，并行化时间帧和模型层，结合块式去噪、特征缓存和协调噪声初始化。
- Result: 在8块RTX 4090 GPU上，生成1,025帧视频的延迟降低6.54倍，内存成本降低1.48倍。
- Conclusion: DualParal策略实现了高效、无伪影的无限长视频生成。


### [90] [DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding](https://arxiv.org/abs/2505.21076)
*Weihao Xuan,Junjue Wang,Heli Qi,Zihang Chen,Zhuo Zheng,Yanfei Zhong,Junshi Xia,Naoto Yokoya*

Main category: cs.CV

TL;DR: DVL-Suite是一个用于分析长期城市动态的遥感图像框架，包含DVL-Bench和DVL-Instruct两部分，旨在解决多模态大语言模型在长期地球观测中的局限性。

- Motivation: 现有模型在长期地球观测分析中表现有限，主要集中于单时相或双时相图像，因此需要开发更全面的框架。
- Method: DVL-Suite包含15,063张高分辨率多时相图像，覆盖42个美国大城市，分为DVL-Bench（7个城市理解任务）和DVL-Instruct（指令调优数据集）。
- Result: 评估了17个先进模型，发现其在长期时间理解和定量分析方面存在局限性。基于DVL-Instruct开发了DVLChat模型，支持图像级问答和像素级分割。
- Conclusion: DVL-Suite填补了长期地球观测分析的空白，并通过DVLChat模型提升了多模态语言模型的能力。


### [91] [Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts](https://arxiv.org/abs/2505.21079)
*Yue Zhang,Yingzhao Jian,Hehe Fan,Yi Yang,Roger Zimmermann*

Main category: cs.CV

TL;DR: Uni3D-MoE是一种基于稀疏混合专家（MoE）的多模态大语言模型，旨在通过动态选择专家实现自适应3D多模态融合，提升3D场景理解的全面性和准确性。

- Motivation: 现有方法通常仅使用部分3D模态，导致场景表示不完整且解释准确性降低；同时，不同查询依赖不同模态，统一处理可能无法有效捕捉查询特定上下文。
- Method: Uni3D-MoE整合了多种3D模态（如RGB、深度图像、BEV地图、点云和体素表示），并采用稀疏MoE架构中的可学习路由机制，动态选择专家处理多模态令牌。
- Result: 在标准3D场景理解基准和专用数据集上的广泛评估证明了Uni3D-MoE的有效性。
- Conclusion: Uni3D-MoE通过自适应多模态融合，显著提升了3D场景理解的全面性和准确性。


### [92] [DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response](https://arxiv.org/abs/2505.21089)
*Junjue Wang,Weihao Xuan,Heli Qi,Zhihao Liu,Kunyi Liu,Yuhan Wu,Hongruixuan Chen,Jian Song,Junshi Xia,Zhuo Zheng,Naoto Yokoya*

Main category: cs.CV

TL;DR: 论文提出了一个全球尺度的遥感视觉语言数据集DisasterM3，用于灾害评估与响应，并通过微调现有模型提升了其在多灾害、多传感器和多任务场景下的性能。

- Motivation: 现有的大规模视觉语言模型（VLMs）在复杂灾害场景中面临挑战，如灾害类型多样、地理区域广泛和传感器差异。为了解决这些问题，作者构建了一个新的数据集。
- Method: 作者构建了DisasterM3数据集，包含26,988对双时相卫星图像和123k指令对，覆盖5大洲的36次灾害事件。数据集支持多灾害、多传感器和多任务评估。随后，作者对14个通用和遥感VLMs进行了评估，并针对灾害任务微调了4个模型。
- Result: 实验表明，现有模型在灾害任务上表现不佳，主要由于缺乏灾害相关语料、跨传感器差异和损伤对象计数不敏感。微调后的模型在所有任务上均取得稳定提升，并展现出跨传感器和跨灾害的泛化能力。
- Conclusion: DisasterM3填补了灾害评估领域的空白，通过微调提升了VLMs在复杂灾害场景中的应用能力，为未来研究提供了重要基准。


### [93] [Instance Data Condensation for Image Super-Resolution](https://arxiv.org/abs/2505.21099)
*Tianhao Peng,Ho Man Kwan,Yuxuan Jiang,Ge Gao,Fan Zhang,Xiaozhong Xu,Shan Liu,David Bull*

Main category: cs.CV

TL;DR: 提出了一种针对图像超分辨率（ISR）的实例数据压缩（IDC）框架，通过随机局部傅里叶特征提取和多级特征分布匹配，实现了10%压缩率的DIV2K数据集，性能接近或优于原始数据集。

- Motivation: 深度学习图像超分辨率依赖大数据集训练，计算和存储成本高；数据压缩在高层次视觉任务中有效，但在ISR中尚未充分探索。
- Method: 采用随机局部傅里叶特征提取和多级特征分布匹配，优化全局和局部特征分布，生成高质量合成训练数据。
- Result: 压缩后的数据集（10%体积）在训练多种ISR模型时性能接近或优于原始数据集，且训练稳定性高。
- Conclusion: 首次证明10%数据量的压缩数据集在ISR任务中表现优异，代码和数据集已开源。


### [94] [Differentiable Solver Search for Fast Diffusion Sampling](https://arxiv.org/abs/2505.21114)
*Shuai Wang,Zexian Li,Qipeng zhang,Tianhui Song,Xubin Li,Tiezheng Ge,Bo Zheng,Limin Wang*

Main category: cs.CV

TL;DR: 提出了一种新的可微分求解器搜索算法，显著提升了扩散模型的生成效率，仅需10步即可达到高质量生成效果。

- Motivation: 传统ODE求解器依赖时间相关的拉格朗日插值，效率不足，需优化求解器以提升扩散模型的生成效率。
- Method: 通过分析时间步长和求解器系数构建紧凑搜索空间，提出可微分求解器搜索算法，寻找更优求解器。
- Result: 在ImageNet256上，SiT-XL/2和FlowDCN-XL/2分别达到FID 2.40和2.35，DiT-XL/2达到2.33，均仅需10步。
- Conclusion: 搜索到的求解器显著优于传统方法，且适用于多种模型架构、分辨率和规模。


### [95] [ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction](https://arxiv.org/abs/2505.21117)
*Adeela Islam,Stefano Fiorini,Stuart James,Pietro Morerio,Alessio Del Bue*

Main category: cs.CV

TL;DR: ReassembleNet通过轮廓关键点和图神经网络降低复杂性，提升多模态数据整合能力，在旋转和平移误差上显著优于现有方法。

- Motivation: 解决深度学习在重组任务中的可扩展性、多模态和实际应用问题。
- Method: 使用轮廓关键点和图神经网络选择信息点，结合扩散式姿态估计恢复原始结构。
- Result: 旋转和平移误差分别提升55%和86%。
- Conclusion: ReassembleNet在复杂重组任务中表现优异，具有实际应用潜力。


### [96] [FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention](https://arxiv.org/abs/2505.21144)
*Sergey Karpukhin,Vadim Titov,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出FastFace框架，解决预训练ID适配器在蒸馏加速扩散模型中的训练自由适应问题，通过改进分类器自由引导和注意力机制提升身份相似性和保真度。

- Motivation: 现有身份保留适配器多与基础扩散模型联合训练，推理速度慢，需解决训练自由适应问题。
- Method: 重新设计分类器自由引导和注意力机制，提出FastFace框架。
- Result: 提升身份相似性和保真度，支持少步风格生成。
- Conclusion: FastFace框架有效解决了训练自由适应问题，并开发了公开评估协议。


### [97] [RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images](https://arxiv.org/abs/2505.21152)
*Xurui Li,Zhonesheng Jiang,Tingxuan Ai,Yu Zhou*

Main category: cs.CV

TL;DR: RoBiS框架通过Swin-Cropping、数据增强和自适应二值化策略，显著提升了MVTec AD 2上的异常检测性能。

- Motivation: 解决现有方法在复杂真实场景（如MVTec AD 2）中性能下降的问题。
- Method: 1. Swin-Cropping保留小异常信息；2. 数据增强提升鲁棒性；3. 结合传统统计和MEBin的自适应二值化策略。
- Result: SegF1在Test_private和Test_private_mixed上分别提升29.2%和29.82%。
- Conclusion: RoBiS在复杂真实场景中表现优异，代码已开源。


### [98] [Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model](https://arxiv.org/abs/2505.21179)
*Dar-Yen Chen,Hmrishav Bandyopadhyay,Kai Zou,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 论文提出了一种名为NAG的高效、免训练方法，用于解决扩散模型中负向引导的挑战，适用于多种架构和模态。

- Motivation: 负向引导在扩散模型中是一个基本挑战，尤其在少步采样情况下，现有方法如CFG表现不佳。
- Method: NAG通过在注意力空间中使用L1归一化和细化进行外推，实现有效的负向引导。
- Result: 实验表明，NAG在文本对齐、保真度和人类感知质量上均有提升，且适用于多种架构和模态。
- Conclusion: NAG作为一种模型无关的推理时方法，无需重新训练，为现代扩散框架提供了高效的负向引导。


### [99] [Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion](https://arxiv.org/abs/2505.21181)
*Yayin Zheng,Chen Wan,Zihong Guo,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的对抗攻击框架FSA，通过结合频域和空域变换，显著提升了对抗样本在黑盒防御模型中的攻击成功率。

- Motivation: 对抗攻击对机器学习模型的安全性构成重大挑战，现有方法主要集中在空域，而频域的作用尚未充分探索。
- Method: FSA框架结合了高频增强（通过傅里叶变换和频率选择性放大）和分层梯度融合（多尺度梯度分解与融合）。
- Result: 实验表明，FSA在多种黑盒模型上优于现有方法，攻击成功率平均提升23.6%。
- Conclusion: FSA通过频域和空域的结合，显著提升了对抗攻击的效果，为黑盒防御提供了新的挑战。


### [100] [Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling](https://arxiv.org/abs/2505.21187)
*Hesam Araghi,Jan van Gemert,Nergis Tomen*

Main category: cs.CV

TL;DR: 本文系统评估了六种硬件友好的子采样方法对事件视频分类任务的影响，提出了一种基于密度的子采样方法，提高了稀疏场景下的分类准确性。

- Motivation: 事件相机的高事件率对数据传输和处理提出了挑战，而子采样方法的效果尚未充分研究。
- Method: 使用卷积神经网络评估六种子采样方法，并提出一种基于密度的子采样方法。
- Result: 基于密度的子采样方法在稀疏场景下提高了分类准确性，同时分析了影响子采样性能的关键因素。
- Conclusion: 研究为硬件高效的子采样策略提供了见解，平衡了数据效率和任务准确性。


### [101] [Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models](https://arxiv.org/abs/2505.21200)
*Xudong Tan,Yaoxin Yang,Peng Ye,Jialin Zheng,Bizhe Bai,Xinyi Wang,Jia Hao,Tao Chen*

Main category: cs.CV

TL;DR: FlashVLA是一种无需训练、即插即用的加速框架，通过动作重用和视觉令牌选择策略，显著降低VLA模型的推理成本，同时保持任务成功率。

- Motivation: VLA模型在实时部署和边缘应用中面临高推理成本的挑战，主要源于大规模令牌计算和自回归解码。FlashVLA通过识别动作步骤和视觉令牌的冗余性，提出了一种高效的解决方案。
- Method: FlashVLA采用令牌感知的动作重用机制避免稳定动作步骤的冗余解码，并通过信息引导的视觉令牌选择策略修剪低贡献令牌。
- Result: 在LIBERO基准测试中，FlashVLA减少了55.7%的FLOPs和36.0%的延迟，任务成功率仅下降0.7%。
- Conclusion: FlashVLA有效实现了轻量级、低延迟的VLA推理，无需重新训练。


### [102] [Sci-Fi: Symmetric Constraint for Frame Inbetweening](https://arxiv.org/abs/2505.21205)
*Liuhan Chen,Xiaodong Cun,Xiaoyu Li,Xianyi He,Shenghai Yuan,Jie Chen,Ying Shan,Li Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Sci-Fi的新框架，通过改进的机制对称约束起始帧和结束帧，解决了现有方法中不对称控制导致的问题。

- Motivation: 现有方法在引入结束帧约束时，通常沿用起始帧的机制，导致约束强度不对称，可能引发运动不一致或外观崩溃。
- Method: 提出Sci-Fi框架，通过轻量级模块EF-Net编码结束帧并扩展为时序自适应特征，对称增强结束帧约束。
- Result: 实验证明Sci-Fi能生成更和谐的过渡效果，优于其他基线方法。
- Conclusion: Sci-Fi通过对称约束机制，有效提升了帧间合成的质量。


### [103] [Is Hyperbolic Space All You Need for Medical Anomaly Detection?](https://arxiv.org/abs/2505.21228)
*Alvaro Gonzalez-Jimenez,Simone Lionetti,Ludovic Amruthalingam,Philippe Gottfrois,Fabian Gröger,Marc Pouly,Alexander A. Navarini*

Main category: cs.CV

TL;DR: 该论文提出了一种将特征投影到双曲空间的新方法，用于医学异常检测，相比传统欧几里得空间方法表现更优。

- Motivation: 医学异常检测面临数据可用性和标注限制的挑战，传统欧几里得空间方法无法有效捕捉特征的层次关系，导致性能不佳。
- Method: 将特征表示投影到双曲空间，基于置信度聚合特征，并分类为健康或异常样本。
- Result: 实验表明，双曲空间方法在多个医学基准数据集上均优于欧几里得方法，AUROC分数更高，且对参数变化具有鲁棒性，在少样本场景下表现优异。
- Conclusion: 双曲空间是医学异常检测的强大替代方案，尤其在数据稀缺情况下表现突出。


### [104] [Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning](https://arxiv.org/abs/2505.21231)
*Lintao Xu,Yinghao Wang,Chaohui Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MoDOT的网络，联合估计深度和遮挡边界（OB），通过CASM模块和OBDCL损失函数提升深度预测精度，实验证明其性能优于现有方法。

- Motivation: 遮挡边界（OB）和单目深度估计（MDE）之间存在相互促进的关系，OB为深度估计提供几何线索，而深度先验可以优化遮挡推理。
- Method: 提出MoDOT网络，结合CASM模块（跨注意力多尺度条带卷积）和OBDCL损失函数，联合估计深度和OB。
- Result: 在合成数据集和NYUD-v2数据集上达到SOTA性能，深度转移结果与竞争对手相当，同时保持几何保真度。
- Conclusion: 联合估计深度和OB具有显著优势，MoDOT的设计有效，未来将公开代码和数据集。


### [105] [CROP: Contextual Region-Oriented Visual Token Pruning](https://arxiv.org/abs/2505.21233)
*Jiawei Guo,Feifei Zhai,Pu Jian,Qianrun Wei,Yu Zhou*

Main category: cs.CV

TL;DR: CROP框架通过定位和剪枝两步压缩视觉令牌，减少冗余信息，提升VQA任务性能。

- Motivation: 当前VLM方法处理整张图像导致过多冗余视觉令牌，增加计算负担。
- Method: CROP采用定位和剪枝两步：定位相关区域，通过PLC和ILP策略剪枝。
- Result: CROP在多种VQA任务中表现优异，超越现有方法。
- Conclusion: CROP有效压缩视觉令牌，提升性能，代码和数据集将公开。


### [106] [3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin](https://arxiv.org/abs/2505.21238)
*Jieyu Yuan,Yujun Li,Yuanlin Zhang,Chunle Guo,Xiongxin Tang,Ruixing Wang,Chongyi Li*

Main category: cs.CV

TL;DR: 提出了一种基于物理的框架，用于水下场景重建中的新视角合成，通过高斯建模分离物体外观和水介质效应，并结合距离引导优化策略，显著提升了渲染质量和恢复精度。

- Motivation: 水下场景重建因复杂的光介质相互作用而面临独特挑战，传统体积渲染假设的均匀传播介质不适用，3D高斯泼溅（3DGS）在水下非均匀环境中表现不佳。
- Method: 通过定制高斯建模分离物体外观和水介质效应，引入外观嵌入表示背向散射和衰减，并提出距离引导优化策略，结合伪深度图监督和深度正则化。
- Result: 实验表明，该方法在渲染质量和恢复精度上显著优于现有方法。
- Conclusion: 该框架成功实现了高质量的新视角合成和物理准确的场景恢复，为水下场景重建提供了有效解决方案。


### [107] [Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation](https://arxiv.org/abs/2505.21258)
*Changguanng Wu,Jiangxin Dong,Chengjian Li,Jinhui Tang*

Main category: cs.CV

TL;DR: Plenodium是一种高效的3D表示框架，可同时建模物体和参与介质，通过球谐编码结合方向和位置信息，显著提升水下场景重建精度。

- Motivation: 现有介质表示仅依赖视图依赖建模，无法准确处理水下环境。Plenodium旨在解决这一问题。
- Method: 提出伪深度高斯补充增强COLMAP点云，并开发深度排序正则化损失优化场景几何。
- Result: 在真实水下数据集上表现优异，模拟数据集验证了其恢复能力。
- Conclusion: Plenodium在水下3D重建中效果显著，代码和数据集已开源。


### [108] [DiMoSR: Feature Modulation via Multi-Branch Dilated Convolutions for Efficient Image Super-Resolution](https://arxiv.org/abs/2505.21262)
*M. Akin Yilmaz,Ahmet Bilican,A. Murat Tekalp*

Main category: cs.CV

TL;DR: DiMoSR是一种轻量级单图像超分辨率（SISR）架构，通过调制增强特征表示，结合多分支扩张卷积，在保持计算效率的同时提升性能。

- Motivation: 解决轻量级SISR中重建质量与模型效率的平衡问题，探索注意力机制之外的架构范式。
- Method: 提出DiMoSR架构，利用多分支扩张卷积捕获更广感受野的上下文信息，结合调制增强特征表示。
- Result: 在多个基准数据集上优于现有轻量级方法，PSNR和SSIM指标更优，计算复杂度相当或更低。
- Conclusion: DiMoSR验证了调制与注意力机制的互补性，为高效网络设计提供了新思路。


### [109] [Supervised and self-supervised land-cover segmentation & classification of the Biesbosch wetlands](https://arxiv.org/abs/2505.21269)
*Eva Gmelich Meijling,Roberto Del Prete,Arnoud Visser*

Main category: cs.CV

TL;DR: 本文提出了一种结合监督学习和自监督学习的方法，用于湿地土地覆盖分类，解决了高分辨率卫星图像标注数据稀缺的问题，并通过实验验证了其有效性。

- Motivation: 湿地土地覆盖分类对环境监测和生态系统管理至关重要，但高分辨率卫星图像的标注数据稀缺，限制了监督学习的效果。
- Method: 采用U-Net模型，结合监督学习和自监督学习（SSL），利用Sentinel-2图像在荷兰六个湿地区域进行训练。
- Result: 基线模型准确率为85.26%，通过SSL预训练提升至88.23%。高分辨率图像提供了更清晰的边界和细节。
- Conclusion: 该方法有效解决了标注数据稀缺问题，并公开了一个适用于湿地分类的Sentinel-2数据集。


### [110] [Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2505.21309)
*Zenghao Zheng,Lianping Yang,Hegui Zhu,Mingrui Ye*

Main category: cs.CV

TL;DR: 提出了一种基于谱压缩变换器（SCT）和线姿态图（LPG）的双流网络架构，用于高效3D人体姿态估计，减少冗余并提升计算效率。

- Motivation: 现有基于Transformer的3D人体姿态估计方法因自注意力机制的二次复杂度导致计算成本高，且未能有效消除序列冗余。
- Method: 引入SCT通过离散余弦变换压缩序列长度，减少冗余；提出LPG补充输入信息；设计双流网络建模空间关节关系和压缩运动轨迹。
- Result: 在Human3.6M和MPI-INF-3DHP数据集上实现最优性能（如Human3.6M上MPJPE为37.7mm），计算效率高。
- Conclusion: SCT和LPG有效减少冗余并提升性能，双流网络设计进一步优化模型表现，代码将开源。


### [111] [Efficient Leaf Disease Classification and Segmentation using Midpoint Normalization Technique and Attention Mechanism](https://arxiv.org/abs/2505.21316)
*Enam Ahmed Taufik,Antara Firoz Parsa,Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: 论文提出了一种结合Mid Point Normalization（MPN）和注意力机制的两阶段方法，显著提升了植物病害检测的准确性和效率。

- Motivation: 植物病害检测面临标记数据稀缺和复杂背景因素的挑战，需要一种高效且准确的方法。
- Method: 采用MPN进行图像预处理，并结合Squeeze-and-Excitation（SE）注意力机制动态调整特征表示。分类任务中结合MPN与SE块，分割任务中在U-Net架构中集成注意力模块。
- Result: 分类任务达到93%准确率，目标类F1分数完美；分割任务Dice分数72.44%，IoU 58.54%，显著优于基线。
- Conclusion: 该方法不仅精度高，还实现了轻量化架构，适合实际计算机视觉应用。


### [112] [MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on](https://arxiv.org/abs/2505.21325)
*Guangyuan Li,Siming Zheng,Hao Zhang,Jinwei Chen,Junsheng Luan,Binkai Ou,Lei Zhao,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: MagicTryOn提出了一种基于视频扩散Transformer的视频虚拟试穿框架，解决了现有方法在时空一致性和服装内容保留上的不足。

- Motivation: 现有视频虚拟试穿方法在时空一致性、服装细节重建和动态一致性方面存在局限性，影响了合成结果的真实性和稳定性。
- Method: 采用扩散Transformer替代U-Net，结合全自注意力建模时空一致性，并设计了从粗到细的服装保留策略和掩码感知损失。
- Result: 在图像和视频试穿数据集上的实验表明，该方法在综合评估中优于现有SOTA方法，并能泛化到真实场景。
- Conclusion: MagicTryOn通过改进架构和策略，显著提升了视频虚拟试穿的性能和效果。


### [113] [MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios](https://arxiv.org/abs/2505.21333)
*Yang Shi,Huanqian Wang,Wulin Xie,Huanyao Zhang,Lijie Zhao,Yi-Fan Zhang,Xinfeng Li,Chaoyou Fu,Zhuoer Wen,Wenting Liu,Zhuoran Zhang,Xinlong Chen,Bohan Zeng,Sihan Yang,Yuanxing Zhang,Pengfei Wan,Haotian Wang,Wenjing Yang*

Main category: cs.CV

TL;DR: 论文提出了MME-VideoOCR基准，用于评估多模态大语言模型（MLLMs）在视频OCR任务中的表现，发现现有模型在需要时空推理和跨帧信息整合的任务上表现有限。

- Motivation: 现有MLLMs在静态图像OCR中表现良好，但在视频OCR中因运动模糊、时间变化和视觉效应等因素表现不佳，需要更全面的评估基准。
- Method: 引入MME-VideoOCR基准，包含10个任务类别、25个任务和44种场景，涵盖1,464个视频和2,000个标注问答对，评估了18种先进MLLMs。
- Result: 最佳模型（Gemini-2.5 Pro）准确率仅73.7%，在需要跨帧理解和时空推理的任务上表现较差。
- Conclusion: 视频OCR需要高分辨率视觉输入和充分的时间覆盖，现有MLLMs在复杂视频理解任务上仍有改进空间。


### [114] [HoliTom: Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/abs/2505.21334)
*Kele Shao,Keda Tao,Can Qin,Haoxuan You,Yang Sui,Huan Wang*

Main category: cs.CV

TL;DR: HoliTom是一种训练无关的整体令牌合并框架，通过内外LLM令牌修剪策略结合，显著减少视频LLM的计算负担。

- Motivation: 现有令牌修剪方法在视频LLM中存在计算效率低的问题，内外修剪策略的协同潜力未被探索。
- Method: HoliTom采用外LLM修剪（全局冗余感知时间分割）和内LLM令牌相似性合并，减少90%以上视觉令牌。
- Result: 在LLaVA-OneVision-7B上，计算成本降至6.9% FLOPs，保持99.1%性能，TTFT减少2.28倍，解码吞吐量提升1.32倍。
- Conclusion: HoliTom展示了内外修剪策略结合的高效性，为视频LLM推理提供了实用优化方案。


### [115] [Beyond Accuracy: Uncovering the Role of Similarity Perception and its Alignment with Semantics in Supervised Learning](https://arxiv.org/abs/2505.21338)
*Katarzyna Filus,Mateusz Żarski*

Main category: cs.CV

TL;DR: 论文提出Deep Similarity Inspector（DSI）框架，研究深度视觉网络如何形成相似性感知及其与语义相似性的对齐。实验发现CNN和ViT在训练中经历三个阶段（初始相似性激增、细化、稳定），且两者有明显差异。

- Motivation: 研究深度视觉网络中相似性感知的形成机制及其与语义相似性的关系，填补该领域的研究空白。
- Method: 引入DSI框架，通过实验分析CNN和ViT在训练过程中的相似性感知发展。
- Result: CNN和ViT在训练中经历三个阶段，且存在明显差异；观察到错误消除和错误细化现象。
- Conclusion: DSI为理解深度视觉网络的相似性感知提供了系统方法，揭示了CNN和ViT的不同行为模式。


### [116] [AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping](https://arxiv.org/abs/2505.21357)
*Wenyuan Li,Shunlin Liang,Keyan Chen,Yongzhe Chen,Han Ma,Jianglei Xu,Yichuan Ma,Shikang Guan,Husheng Fang,Zhenwei Shi*

Main category: cs.CV

TL;DR: AgriFM是一种基于Transformer的多源遥感基础模型，专为农业作物测绘设计，通过同步时空特征提取和动态融合，显著优于现有方法。

- Motivation: 现有遥感基础模型在作物测绘中表现不佳，主要因固定时空窗口或忽略时间信息，无法捕捉作物系统的多尺度时空特征。
- Method: 采用改进的Video Swin Transformer架构，同步时空下采样，结合MODIS、Landsat-8/9和Sentinel-2数据，预训练全球数据集。
- Result: AgriFM在多项下游任务中表现优于传统深度学习方法和通用遥感基础模型。
- Conclusion: AgriFM通过统一时空处理和动态特征融合，为农业作物测绘提供了高效解决方案。


### [117] [YOLO-SPCI: Enhancing Remote Sensing Object Detection via Selective-Perspective-Class Integration](https://arxiv.org/abs/2505.21370)
*Xinyuan Wang,Lian Peng,Xiangcheng Li,Yilin He,KinTak U*

Main category: cs.CV

TL;DR: YOLO-SPCI是一种基于注意力机制的改进检测框架，通过引入SPCI模块提升遥感图像中的目标检测性能。

- Motivation: 遥感图像中的目标检测面临尺度变化大、物体分布密集和背景复杂等问题，现有检测器（如YOLOv8）缺乏多尺度特征细化机制。
- Method: 提出SPCI模块，包含选择性流门（SSG）、视角融合模块（PFM）和类别判别模块（CDM），并将其嵌入YOLOv8的P3和P5阶段。
- Result: 在NWPU VHR-10数据集上，YOLO-SPCI性能优于现有先进检测器。
- Conclusion: YOLO-SPCI通过SPCI模块有效提升了遥感图像目标检测的性能，同时保持与原始框架的兼容性。


### [118] [Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?](https://arxiv.org/abs/2505.21374)
*Junhao Cheng,Yuying Ge,Teng Wang,Yixiao Ge,Jing Liao,Ying Shan*

Main category: cs.CV

TL;DR: 论文提出了Video-Holmes基准，用于评估多模态语言模型（MLLMs）在复杂视频推理中的表现，发现现有模型在信息整合和线索捕捉方面存在显著不足。

- Motivation: 现有视频基准主要评估视觉感知和基础能力，无法反映真实世界中人类推理的复杂性，因此需要一种更全面的评估方法。
- Method: 通过手动标注270部悬疑短片，设计1,837个问题，涵盖7项任务，要求模型主动定位并整合分散的视觉线索。
- Result: 评估显示，即使表现最佳的Gemini-2.5-Pro模型准确率仅为45%，多数模型低于40%，表明模型在复杂推理中存在困难。
- Conclusion: Video-Holmes可作为多模态推理的“Holmes测试”，推动模型更接近人类推理能力，并突显该领域的挑战。


### [119] [GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution](https://arxiv.org/abs/2505.21375)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Di Wang,Haotian Wang,Zonghao Guo,Zefan Wang,Boqi Shan,Long Lan,Yulin Wang,Hongzhen Wang,Wenjing Yang,Bo Du,Jing Zhang*

Main category: cs.CV

TL;DR: 论文提出GeoLLaVA-8K模型，通过背景令牌修剪和锚定令牌选择解决超高分辨率遥感图像的多模态模型挑战，并在XLRS-Bench上取得最佳性能。

- Motivation: 超高分辨率遥感图像数据稀缺且存在令牌爆炸问题，限制了多模态基础模型的应用。
- Method: 引入SuperRS-VQA和HighRS-VQA数据集，提出背景令牌修剪和锚定令牌选择策略，构建GeoLLaVA-8K模型。
- Result: GeoLLaVA-8K在XLRS-Bench上达到新最优性能。
- Conclusion: 提出的方法有效解决了超高分辨率遥感图像的多模态建模问题，并展示了实际应用潜力。


### [120] [Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility](https://arxiv.org/abs/2505.21377)
*Yidi Li,Jun Xiao,Zhengda Lu,Yiqun Wang,Haiyong Jiang*

Main category: cs.CV

TL;DR: Dream3DVG提出了一种新颖的文本到矢量图形生成方法，支持任意视角查看、渐进细节优化和视角相关遮挡感知。

- Motivation: 解决文本提示与矢量图形之间的领域差距，并通过渐进细节控制和遮挡感知提升生成效果。
- Method: 采用双分支优化框架，包括辅助的3D高斯泼溅优化分支和3D矢量图形优化分支，结合可见性感知渲染模块。
- Result: 在3D草图和3D图标上展示了方法在细节抽象、跨视角一致性和遮挡感知笔画剔除方面的优越性。
- Conclusion: Dream3DVG通过双分支框架和渐进优化策略，显著提升了文本到矢量图形的生成质量。


### [121] [ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding](https://arxiv.org/abs/2505.21381)
*Linshuang Diao,Dayong Ren,Sensen Song,Yurong Qian*

Main category: cs.CV

TL;DR: ZigzagPointMamba通过简单的zigzag扫描路径和语义相似掩码策略（SMS），解决了PointMamba在点云自监督学习中的空间连续性和局部语义相关性破坏问题，显著提升了下游任务性能。

- Motivation: 现有PointMamba方法依赖复杂的令牌排序和随机掩码，破坏了空间连续性和局部语义相关性，影响了自监督学习效果。
- Method: 提出ZigzagPointMamba，采用zigzag扫描路径全局排序点云令牌，增强空间连续性；引入SMS策略，掩码语义相似令牌以整合局部特征。
- Result: 在ShapeNetPart部件分割任务中mIoU提升1.59%，ModelNet40分类任务准确率提升0.4%，ScanObjectNN子集分类任务准确率分别提升0.19%、1.22%和0.72%。
- Conclusion: ZigzagPointMamba通过改进令牌排序和掩码策略，显著提升了点云自监督学习的性能和下游任务表现。


### [122] [Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios](https://arxiv.org/abs/2505.21387)
*Xihong Yang,Siwei Wang,Fangdi Wang,Jiaqi Jin,Suyuan Liu,Yue Liu,En Zhu,Xinwang Liu,Yueming Jin*

Main category: cs.CV

TL;DR: AIRMVC是一种新型多视图聚类框架，通过高斯混合模型识别噪声数据并设计混合校正策略，结合噪声鲁棒对比机制提升性能。

- Motivation: 现实场景中多视图数据常含噪声，现有方法假设视图干净，导致性能下降。
- Method: 使用GMM识别噪声数据，设计混合校正策略和噪声鲁棒对比机制。
- Result: 在六个基准数据集上表现优于现有方法，尤其在噪声环境下。
- Conclusion: AIRMVC能有效识别和校正噪声，提升多视图聚类性能。


### [123] [Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning](https://arxiv.org/abs/2505.21420)
*Jinbao Wang,Hanzhe Liang,Can Gao,Chenxi Hu,Jie Zhou,Yunkang Cao,Linlin Shen,Weiming Shen*

Main category: cs.CV

TL;DR: 论文提出了一种名为Mentor3AD的新方法，通过多模态导师学习提升3D异常检测性能。

- Motivation: 利用多模态互补信息改进3D异常检测，通过融合中间特征进一步区分正常与异常特征差异。
- Method: 提出Mentor3AD方法，包含融合模块（MFM）、指导模块（MGM）和投票模块（VM），用于特征提取与重建。
- Result: 在MVTec 3D-AD和Eyecandies数据集上的实验验证了方法的有效性。
- Conclusion: Mentor3AD通过多模态导师学习显著提升了3D异常检测性能。


### [124] [OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers](https://arxiv.org/abs/2505.21448)
*Ziqiao Peng,Jiwen Liu,Haoxian Zhang,Xiaoqiang Liu,Songlin Tang,Pengfei Wan,Di Zhang,Hongyan Liu,Jun He*

Main category: cs.CV

TL;DR: OmniSync是一个通用的唇同步框架，通过无掩码训练和动态时空分类器自由引导机制，显著提升了唇同步的视觉质量和准确性。

- Motivation: 现有唇同步方法依赖参考帧和掩码修复，对身份一致性、姿态变化和面部遮挡的鲁棒性不足，且音频信号条件较弱。
- Method: 采用Diffusion Transformer模型进行无掩码直接帧编辑，提出流匹配渐进噪声初始化和动态时空分类器自由引导机制。
- Result: 在真实和AI生成视频中，OmniSync在视觉质量和唇同步准确性上显著优于现有方法。
- Conclusion: OmniSync为多样视觉场景提供了一种高效、通用的唇同步解决方案。


### [125] [Visual Product Graph: Bridging Visual Products And Composite Images For End-to-End Style Recommendations](https://arxiv.org/abs/2505.21454)
*Yue Li Du,Ben Alexander,Mikhail Antonenka,Rohan Mahadev,Hao-yu Wu,Dmitry Kislyuk*

Main category: cs.CV

TL;DR: VPG（视觉产品图）是一个实时检索系统，通过高性能基础设施和先进计算机视觉模型，实现语义相似但视觉不同的内容检索，并提供上下文风格和互补产品推荐。

- Motivation: 解决视觉搜索系统中检索语义相似但视觉不同内容的问题，提升用户体验和产品推荐效果。
- Method: 构建VPG系统，结合高性能存储和先进计算机视觉模型（如目标检测、视觉嵌入等），实现实时检索和推荐功能。
- Result: 系统在端到端人类相关性评估中达到78.8%的极相似@1，模块参与率为6%，并已在Pinterest生产环境中部署。
- Conclusion: VPG系统通过技术改进和实际应用验证，成功解决了语义相似但视觉不同内容的检索问题，并提供了有效的产品推荐。


### [126] [Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO](https://arxiv.org/abs/2505.21457)
*Muzhi Zhu,Hao Zhong,Canyu Zhao,Zongze Du,Zheng Huang,Mingyu Liu,Hao Chen,Cheng Zou,Jingdong Chen,Ming Yang,Chunhua Shen*

Main category: cs.CV

TL;DR: 论文提出ACTIVE-O3框架，通过强化学习赋予多模态大语言模型（MLLMs）主动感知能力，并建立评测基准。

- Motivation: 尽管MLLMs在机器人系统中广泛应用，但其主动感知能力尚未被充分探索。
- Method: 提出基于GRPO的强化学习框架ACTIVE-O3，并建立评测基准。
- Result: ACTIVE-O3在开放世界任务和特定领域场景中表现优异，并具备零样本推理能力。
- Conclusion: ACTIVE-O3为MLLMs的主动感知研究提供了简单代码库和评测协议。


### [127] [ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models](https://arxiv.org/abs/2505.21465)
*Bozhou Li,Wentao Zhang*

Main category: cs.CV

TL;DR: ID-Align通过重新排序位置ID，解决了高分辨率图像和缩略图之间以及文本与图像交互受限的问题，显著提升了性能。

- Motivation: 现有方法同时编码高分辨率和缩略图图像会产生大量图像标记，结合RoPE的长程衰减特性，限制了高分辨率标记与缩略图标记及文本与图像的交互。
- Method: 提出ID-Align方法，通过重新排序位置ID，使高分辨率标记继承对应缩略图标记的ID，并限制位置索引的过度扩展。
- Result: 在LLaVA-Next框架中，ID-Align显著提升了性能，如在MMBench关系推理任务中提升了6.09%，并在多个基准测试中表现出色。
- Conclusion: ID-Align有效解决了高分辨率与缩略图标记交互受限的问题，提升了视觉语言模型的性能。


### [128] [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/abs/2505.21472)
*Mehrdad Fazli,Bowen Wei,Ziwei Zhu*

Main category: cs.CV

TL;DR: CAAC框架通过视觉标记校准和自适应注意力重缩放，减少大型视觉语言模型中的幻觉问题。

- Motivation: 大型视觉语言模型在多模态任务中表现优异，但存在幻觉问题，即在图像中不存在的对象或属性上过度自信。现有方法在开放性和长文本生成场景中难以保持准确性。
- Method: CAAC框架通过视觉标记校准（VTC）平衡视觉标记的注意力，并通过自适应注意力重缩放（AAR）根据模型置信度强化视觉基础。
- Result: 在CHAIR、AMBER和POPE基准测试中，CAAC优于基线方法，尤其在长文本生成中有效减少幻觉。
- Conclusion: CAAC通过针对空间感知偏差和模态偏差的校准，显著提升了模型的视觉对齐能力，减少了幻觉现象。


### [129] [DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction](https://arxiv.org/abs/2505.21473)
*Yiheng Liu,Liao Qu,Huichao Zhang,Xu Wang,Yi Jiang,Yiming Gao,Hu Ye,Xian Li,Shuai Wang,Daniel K. Du,Shu Cheng,Zehuan Yuan,Xinglong Wu*

Main category: cs.CV

TL;DR: DetailFlow是一种从粗到细的1D自回归图像生成方法，通过新颖的细节预测策略生成高质量图像，显著减少令牌数量并提升生成速度。

- Motivation: 现有自回归模型在生成复杂视觉内容时效率低下且令牌数量庞大，DetailFlow旨在通过分辨率感知的令牌序列和并行推理机制解决这些问题。
- Method: 采用分辨率感知的令牌序列和逐步降级图像监督，结合并行推理与自校正机制，实现从全局结构到细节的渐进生成。
- Result: 在ImageNet 256x256基准测试中，DetailFlow以128令牌达到2.96 gFID，优于VAR和FlexVAR，且生成速度快2倍。
- Conclusion: DetailFlow在生成质量和效率上均优于现有方法，为自回归图像生成提供了更高效的解决方案。


### [130] [Policy Optimized Text-to-Image Pipeline Design](https://arxiv.org/abs/2505.21478)
*Uri Gadot,Rinon Gal,Yftah Ziser,Gal Chechik,Shie Mannor*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的框架，通过奖励模型预测图像质量，减少训练时的计算需求，并采用两阶段训练策略优化工作流，提升图像生成质量。

- Motivation: 现有文本到图像生成的多组件流水线设计复杂且依赖专家知识，自动化方法计算成本高且泛化能力差。
- Method: 训练奖励模型预测图像质量，采用两阶段训练（初始词汇训练和GRPO优化），并结合无分类器引导增强技术。
- Result: 新框架能生成更多样化的工作流，图像质量优于现有基线。
- Conclusion: 该框架有效解决了计算效率和泛化问题，提升了文本到图像生成的性能。


### [131] [MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation](https://arxiv.org/abs/2505.21483)
*Kerui Ren,Jiayang Bai,Linning Xu,Lihan Jiang,Jiangmiao Pang,Mulin Yu,Bo Dai*

Main category: cs.CV

TL;DR: MV-CoLight是一个两阶段框架，用于在2D图像和3D场景中实现光照一致的对象合成，解决了多视角一致性、复杂场景和多样化光照条件的挑战。

- Motivation: 现有方法主要关注单图像场景或固有分解技术，难以处理多视角一致性、复杂场景和多样化光照条件。
- Method: 采用新颖的前馈架构直接建模光照和阴影，避免基于扩散方法的迭代偏差，并使用Hilbert曲线映射将2D图像输入与3D高斯场景表示对齐。
- Result: 实验表明，MV-CoLight在标准基准测试和真实场景中均实现了最先进的和谐效果，具有鲁棒性和广泛泛化能力。
- Conclusion: MV-CoLight通过两阶段框架和新型架构，显著提升了光照一致对象合成的效果和适用性。


### [132] [Be Decisive: Noise-Induced Layouts for Multi-Subject Generation](https://arxiv.org/abs/2505.21488)
*Omer Dahary,Yehonathan Cohen,Or Patashnik,Kfir Aberman,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: 提出了一种基于初始噪声预测空间布局的新方法，避免外部布局控制与模型先验的冲突，提升多主体生成的准确性和稳定性。

- Motivation: 现有文本到图像扩散模型在多主体生成时易出现主体泄漏问题，外部布局控制常与模型先验冲突。
- Method: 通过小神经网络预测并优化初始噪声诱导的空间布局，确保主体边界清晰且一致性。
- Result: 实验表明该方法在文本-图像对齐和多主体生成稳定性上优于现有布局引导技术，同时保留模型原始多样性。
- Conclusion: 噪声对齐策略有效解决了多主体生成中的布局冲突问题，提升了生成质量。


### [133] [Frame In-N-Out: Unbounded Controllable Image-to-Video Generation](https://arxiv.org/abs/2505.21491)
*Boyang Wang,Xuweiyi Chen,Matheus Gadelha,Zezhou Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于图像到视频生成的方法，支持用户控制物体进出场景，并引入新数据集和评估协议。

- Motivation: 解决视频生成中的可控性、时间一致性和细节合成问题。
- Method: 采用身份保持的运动可控视频扩散Transformer架构。
- Result: 显著优于现有基线方法。
- Conclusion: 提出的方法在控制物体进出场景方面表现优异。


### [134] [Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment](https://arxiv.org/abs/2505.21494)
*Xiaojun Jia,Sensen Gao,Simeng Qin,Tianyu Pang,Chao Du,Yihao Huang,Xinfeng Li,Yiming Li,Bo Li,Yang Liu*

Main category: cs.CV

TL;DR: 提出了一种基于特征最优对齐的目标可转移对抗攻击方法（FOA-Attack），通过全局和局部特征对齐提升对抗样本的迁移能力。

- Motivation: 现有方法通常通过全局特征对齐实现目标攻击，但忽略了局部信息，导致迁移能力有限，尤其是对闭源模型。
- Method: 在全局层面引入余弦相似度损失对齐粗粒度特征；在局部层面利用聚类提取紧凑局部模式，并通过最优运输问题对齐细粒度特征。
- Result: 实验表明，该方法在多种模型上表现优异，尤其在闭源MLLMs上迁移效果显著优于现有方法。
- Conclusion: FOA-Attack通过全局和局部特征对齐显著提升了对抗样本的迁移能力，尤其在闭源模型中表现突出。


### [135] [Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers](https://arxiv.org/abs/2505.21497)
*Wei Pang,Kevin Qinghong Lin,Xiangru Jian,Xi He,Philip Torr*

Main category: cs.CV

TL;DR: 论文提出首个学术海报生成基准和评估指标，并介绍了一种多代理流程PosterAgent，其在多个指标上优于现有方法，且成本低。

- Motivation: 解决学术海报生成中长文本压缩和视觉连贯性的挑战。
- Method: 提出PosterAgent，包含Parser、Planner和Painter-Commenter循环，结合视觉语言模型反馈优化海报生成。
- Result: PosterAgent在视觉质量、文本连贯性和内容传达上优于GPT-4o，且成本更低。
- Conclusion: 研究为下一代全自动海报生成模型指明了方向，代码和数据集已开源。


### [136] [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500)
*Dingming Li,Hongxing Li,Zixuan Wang,Yuchen Yan,Hang Zhang,Siqi Chen,Guiyang Hou,Shengpei Jiang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Yueting Zhuang*

Main category: cs.CV

TL;DR: 论文提出了ViewSpatial-Bench，首个专注于多视角空间定位识别的基准测试，揭示了当前视觉语言模型在跨视角空间推理中的局限性，并通过微调显著提升了性能。

- Motivation: 当前视觉语言模型在跨视角空间推理中存在局限性，尤其是从非相机视角（如人类视角）进行推理时表现不佳。
- Method: 引入ViewSpatial-Bench基准测试，包含五种任务类型，并通过自动化3D标注生成精确方向标签。对多种视觉语言模型进行综合评估，并通过微调多视角空间数据集提升性能。
- Result: 模型在相机视角任务中表现尚可，但在人类视角任务中准确性下降。微调后，整体性能提升了46.24%。
- Conclusion: ViewSpatial-Bench为具身AI系统的空间智能提供了重要基准，证明建模3D空间关系能增强视觉语言模型的空间理解能力。


### [137] [Vision Transformers with Self-Distilled Registers](https://arxiv.org/abs/2505.21501)
*Yinjie Chen,Zipeng Yan,Chong Zhou,Bo Dai,Andrew F. Luo*

Main category: cs.CV

TL;DR: 论文提出了一种无需重新训练的方法PH-Reg，通过自蒸馏技术为预训练的ViT模型添加寄存器令牌，以减少异常令牌并提升性能。

- Motivation: ViT模型在处理视觉任务时会出现与局部语义不符的异常令牌，影响细粒度定位或结构一致性任务的性能。
- Method: 提出PH-Reg方法，通过自蒸馏技术将寄存器令牌集成到预训练ViT中，无需额外标注数据或完整重新训练。
- Result: PH-Reg有效减少了异常令牌数量，提升了分割和深度预测任务的性能。
- Conclusion: PH-Reg是一种高效且实用的方法，能够改进预训练ViT模型的性能，无需重新训练。


### [138] [Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis](https://arxiv.org/abs/2505.21502)
*Yipengjing Sun,Chenyang Wang,Shunyuan Zheng,Zonglin Li,Shengping Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: GRGS是一个可泛化且可重光照的3D高斯框架，用于在多样光照条件下实现高保真人体新视角合成。

- Motivation: 现有方法依赖逐角色优化或忽略物理约束，GRGS通过前馈全监督策略，从多视角2D观测中提取几何、材质和光照信息，构建3D高斯表示。
- Method: 引入光照感知几何细化模块（LGR）重建光照无关几何，基于高质量几何设计物理基础神经渲染模块（PGNR），结合神经预测与物理着色，支持可编辑重光照。
- Result: GRGS在视觉质量、几何一致性和跨角色及光照条件的泛化性上表现优异。
- Conclusion: GRGS通过创新的模块和训练方案，实现了高效且高质量的3D人体新视角合成与重光照。
## cs.AI

### [139] [MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs](https://arxiv.org/abs/2505.21327)
*Jiakang Yuan,Tianshuo Peng,Yilei Jiang,Yiting Lu,Renrui Zhang,Kaituo Feng,Chaoyou Fu,Tao Chen,Lei Bai,Bo Zhang,Xiangyu Yue*

Main category: cs.AI

TL;DR: MME-Reasoning是一个新的多模态大型语言模型（MLLMs）逻辑推理能力评估基准，覆盖归纳、演绎和溯因推理，揭示当前MLLMs在综合逻辑推理中的局限性。

- Motivation: 现有基准未能全面评估MLLMs的逻辑推理能力，缺乏明确的推理类型分类和对推理能力的清晰理解。
- Method: 设计MME-Reasoning基准，涵盖三种推理类型，精心设计问题以评估推理能力而非感知或知识广度，并扩展评估协议。
- Result: 当前最先进的MLLMs在综合逻辑推理中表现有限，且在不同推理类型间存在显著性能不平衡。
- Conclusion: 研究揭示了MLLMs在逻辑推理中的关键局限性，为理解和评估推理能力提供了系统性见解。
## cs.CL

### [140] [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)
*Mengru Wang,Ziwen Xu,Shengyu Mao,Shumin Deng,Zhaopeng Tu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为Steering Target Atoms (STA)的新方法，通过分离和操作知识组件来提高语言模型的安全性和可靠性。

- Motivation: 语言模型生成的控制对安全性和可靠性至关重要，但现有方法（如提示工程和引导）因参数众多且内部表示高度交织，导致控制精度受限和副作用。
- Method: 使用稀疏自编码器（SAE）分离高维空间中的知识，提出STA方法定位和操作原子知识组件。
- Result: 实验证明STA方法有效，尤其在对抗场景中表现出更强的鲁棒性和灵活性，并成功应用于大型推理模型。
- Conclusion: STA方法为语言模型的精确控制提供了新思路，尤其在安全和推理任务中具有潜力。


### [141] [PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy](https://arxiv.org/abs/2505.20429)
*Shuhao Guan,Moule Lin,Cheng Xu,Xinyi Liu,Jinman Zhao,Jiexin Fan,Qi Xu,Derek Greene*

Main category: cs.CL

TL;DR: PreP-OCR是一个两阶段流程，结合图像修复和语义感知OCR后校正，显著提升历史文档文本提取效果。

- Motivation: 解决历史文档因退化导致的OCR提取错误问题，提升文本提取准确率。
- Method: 1. 生成合成图像对训练图像修复模型；2. 使用ByT5后校正器处理剩余OCR错误。
- Result: 在13,831页历史文档上测试，字符错误率降低63.9-70.3%。
- Conclusion: PreP-OCR展示了图像修复与语言校正结合在历史档案数字化中的潜力。


### [142] [Predicting Implicit Arguments in Procedural Video Instructions](https://arxiv.org/abs/2505.21068)
*Anil Batra,Laura Sevilla-Lara,Marcus Rohrbach,Frank Keller*

Main category: cs.CL

TL;DR: 论文提出Implicit-VidSRL数据集，用于解决语义角色标注（SRL）中隐含参数缺失的问题，并通过多模态烹饪流程提升上下文推理能力。

- Motivation: 现有SRL基准常忽略隐含参数，导致对步骤理解不完整，需通过多模态数据改进。
- Method: 引入Implicit-VidSRL数据集，结合视觉变化追踪实体，并评估多模态模型在隐含参数推理上的表现。
- Result: 多模态LLMs在预测隐含参数（如what和where/with）上表现不佳，而提出的iSRL-Qwen2-VL模型在F1分数上相对GPT-4o提升17%（what）和14.7%（where/with）。
- Conclusion: Implicit-VidSRL数据集和多模态方法显著提升了隐含参数的识别能力，为SRL任务提供了新方向。


### [143] [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/abs/2505.21496)
*Han Xiao,Guozhi Wang,Yuxiang Chai,Zimu Lu,Weifeng Lin,Hao He,Lue Fan,Liuyang Bian,Rui Hu,Liang Liu,Shuai Ren,Yafei Wen,Xiaoxin Chen,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

TL;DR: UI-Genie是一个自改进框架，通过奖励模型和数据生成策略解决GUI代理中的轨迹验证和高质量训练数据问题。

- Motivation: 解决GUI代理中轨迹验证困难和高质训练数据难以扩展的问题。
- Method: 采用图像-文本交错架构的奖励模型（UI-Genie-RM）和自改进流程，结合数据生成策略（如规则验证和硬负样本挖掘）。
- Result: UI-Genie在多个GUI代理基准测试中达到最先进性能，并通过三阶段数据-模型自改进验证效果。
- Conclusion: UI-Genie框架和数据集开源，为GUI代理研究提供了新工具。
## cs.LG

### [144] [FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation](https://arxiv.org/abs/2505.20353)
*Dong Liu,Jiayi Zhang,Yifan Li,Yanxuan Yu,Ben Lengerich,Ying Nian Wu*

Main category: cs.LG

TL;DR: FastCache通过隐藏状态级缓存和压缩框架加速DiT推理，减少计算冗余，同时保持生成质量。

- Motivation: DiT模型计算密集，迭代结构和深度Transformer堆栈导致效率低下。
- Method: 提出FastCache框架，包括空间感知令牌选择机制和Transformer级缓存，减少冗余计算。
- Result: 实验表明FastCache显著降低延迟和内存使用，生成质量优于其他缓存方法。
- Conclusion: FastCache在保持生成质量的同时，有效提升了DiT的推理效率。


### [145] [HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models](https://arxiv.org/abs/2505.20444)
*Haoran Li,Yingjie Qin,Baoyuan Ou,Lai Xu,Ruiwen Xu*

Main category: cs.LG

TL;DR: 论文提出HoPE，一种混合位置嵌入方法，旨在提升视觉语言模型在长上下文场景（如长视频）中的性能。

- Motivation: 现有方法在长视频任务中表现不佳，且缺乏理论支持，因此需要改进。
- Method: HoPE结合混合频率分配策略和动态时间缩放机制，优化长上下文语义建模。
- Result: 在四个视频基准测试中，HoPE表现优于现有方法。
- Conclusion: HoPE有效提升了视觉语言模型的长上下文处理能力。


### [146] [Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data](https://arxiv.org/abs/2505.20485)
*Abhijit Chunduru,Majid Morafah,Mahdi Morafah,Vishnu Pandi Chellapandi,Ang Li*

Main category: cs.LG

TL;DR: 论文分析了联邦学习中数据异构性对全局决策边界的影响，提出了FedProj框架以避免局部训练中的遗忘问题，并通过实验验证其优越性。

- Motivation: 数据异构性对联邦学习的全局决策边界影响未被深入理解，现有方法存在遗忘问题。
- Method: 提出FedProj框架，包括服务器端集成知识转移损失和利用公共未标记数据集的记忆机制。
- Result: FedProj在实验中显著优于现有方法。
- Conclusion: FedProj有效解决了全局决策边界的遗忘问题，提升了联邦学习性能。


### [147] [Bi-Level Unsupervised Feature Selection](https://arxiv.org/abs/2505.20563)
*Jingjing Liu,Xiansen Ju,Xianchao Xiu,Wanquan Liu*

Main category: cs.LG

TL;DR: 提出了一种新的双层无监督特征选择方法（BLUFS），结合聚类和特征级别，利用谱聚类和ℓ2,0-范数约束，提升了特征选择性能。

- Motivation: 现有无监督特征选择方法多从单一视角建模，难以同时评估特征重要性和保留数据结构。
- Method: BLUFS包含聚类层和特征层：聚类层用谱聚类生成伪标签，特征层用ℓ2,0-范数约束投影矩阵。通过PAM算法求解模型。
- Result: 在合成和真实数据集上的实验表明，BLUFS在聚类和分类任务中表现优越。
- Conclusion: BLUFS首次结合双层框架与ℓ2,0-范数，显著提升了无监督特征选择的效果。


### [148] [Detecting Informative Channels: ActionFormer](https://arxiv.org/abs/2505.20739)
*Kunpeng Zhao,Asahi Miyazaki,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 论文提出改进的ActionFormer模型，用于传感器信号的人类活动识别（HAR），通过Sequence-and-Excitation策略和swish激活函数优化性能，实验显示平均mAP提升16.01%。

- Motivation: 传统Transformer模型在HAR中难以捕捉细微变化和时空特征的相互依赖，尤其在传感器信号输入时表现受限。
- Method: 改进ActionFormer，采用Sequence-and-Excitation策略减少额外参数，使用swish激活函数保留负范围信息。
- Result: 在WEAR数据集上，改进模型对惯性数据的平均mAP提升16.01%。
- Conclusion: 改进的ActionFormer有效解决了传感器信号输入时的性能限制，显著提升了HAR的准确性。


### [149] [Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction](https://arxiv.org/abs/2505.20755)
*Yifei Wang,Weimin Bai,Colin Zhang,Debing Zhang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: Uni-Instruct统一了10多种一步扩散蒸馏方法，提出基于f-散度家族的理论框架，实现了高效的一步扩散模型训练，并在多个基准测试中取得最优性能。

- Motivation: 通过扩散扩展理论解决现有一步扩散蒸馏方法的分散性问题，提出统一的理论框架Uni-Instruct。
- Method: 基于f-散度家族的扩散扩展理论，提出等效且可处理的损失函数，用于训练一步扩散模型。
- Result: 在CIFAR10和ImageNet-64×64基准测试中取得最优FID值（1.46和1.02），并在文本到3D生成任务中略优于现有方法。
- Conclusion: Uni-Instruct为一步扩散蒸馏和知识迁移提供了理论和实践基础，具有广泛的应用潜力。


### [150] [Leaner Transformers: More Heads, Less Depth](https://arxiv.org/abs/2505.20802)
*Hemanth Saratchandran,Damien Teney,Simon Lucey*

Main category: cs.LG

TL;DR: 论文挑战了Transformer模型‘越大越好’的观念，提出通过增加注意力头数而非深度来优化模型，减少参数30-50%的同时保持性能。

- Motivation: 现有Transformer模型可能过度庞大，作者希望通过理论分析重新定义多头注意力的作用，优化模型设计。
- Method: 提出理论原则，强调多头注意力对注意力块条件化的作用，并基于此重新设计架构，增加头数、减少深度。
- Result: 在多种任务（如ImageNet-1k、GLUE等）上验证，参数减少30-50%时性能不变。
- Conclusion: 通过优化多头注意力设计，可实现更高效的Transformer模型，打破‘越大越好’的固有观念。


### [151] [Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling](https://arxiv.org/abs/2505.21074)
*Yichuan Cao,Yibo Miao,Xiao-Shan Gao,Yinpeng Dong*

Main category: cs.LG

TL;DR: 提出了一种基于规则偏好建模的红队测试方法（RPG-RT），通过迭代优化提示词来评估文本到图像模型的安全性，适应未知防御机制。

- Motivation: 解决现有红队测试方法在评估文本到图像模型安全性时的局限性，特别是对未知防御机制的适应问题。
- Method: 结合LLM迭代修改提示词，利用T2I系统的反馈进行微调，并通过规则偏好建模细化控制。
- Result: 在19种T2I系统、3种商业API和T2V模型上验证了方法的优越性和实用性。
- Conclusion: RPG-RT能够有效适应未知防御机制，提升红队测试的实用性和效果。


### [152] [Learning Single Index Models with Diffusion Priors](https://arxiv.org/abs/2505.21135)
*Anqi Tang,Youming Chen,Shuchen Xue,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于扩散模型（DMs）的信号恢复方法，适用于半参数单指标模型，解决了非线性测量模型中不连续或未知链接函数的问题，并通过理论分析和实验验证了其高效性和准确性。

- Motivation: 扩散模型在图像生成和信号恢复中表现出色，但现有研究无法处理非线性测量模型中的不连续或未知链接函数。本文旨在填补这一空白，提出一种更通用的方法。
- Method: 提出了一种高效的信号恢复方法，仅需一轮无条件采样和部分扩散模型的反演，适用于半参数单指标模型。
- Result: 实验表明，该方法在图像数据集上相比其他方法能更准确地重建信号，同时显著减少神经函数评估次数。
- Conclusion: 该方法为非线性测量模型下的信号恢复提供了一种高效且准确的解决方案，扩展了扩散模型的应用范围。


### [153] [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/abs/2505.21136)
*Jintao Zhang,Xiaoming Xu,Jia Wei,Haofeng Huang,Pengle Zhang,Chendong Xiang,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: SageAttention2++通过量化加速注意力计算，利用FP8指令进一步提速，比FlashAttention快3.9倍，同时保持准确性。

- Motivation: 注意力计算的时间复杂度随序列长度二次增长，效率至关重要。
- Method: 采用量化加速矩阵乘法，并利用FP8指令（比FP8快2倍）进一步优化。
- Result: SageAttention2++比FlashAttention快3.9倍，且准确性无损失。
- Conclusion: SageAttention2++高效加速多种模型，代码已开源。


### [154] [Topological Deep Learning for Speech Data](https://arxiv.org/abs/2505.21173)
*Zhiwang Yu*

Main category: cs.LG

TL;DR: 论文提出了一种基于拓扑数据分析（TDA）的拓扑感知卷积核，显著提升了语音识别网络的性能。

- Motivation: 受Carlsson等人启发，探索TDA在深度学习中的应用潜力，尤其是通过数学工具优化神经网络。
- Method: 通过研究正交群作用在核上的性质，建立了矩阵空间的纤维丛分解，提出新的滤波器生成方法，并设计了Orthogonal Feature（OF）层。
- Result: OF层在音素识别任务中表现优异，尤其在低噪声环境下，并展示了跨领域的适应能力。
- Conclusion: 该研究揭示了TDA在神经网络优化中的潜力，为数学与深度学习的跨学科研究开辟了新方向。
## cs.DB

### [155] [LazyVLM: Neuro-Symbolic Approach to Video Analytics](https://arxiv.org/abs/2505.21459)
*Xiangru Jian,Wei Pang,Zhengyuan Dong,Chao Zhang,M. Tamer Özsu*

Main category: cs.DB

TL;DR: LazyVLM是一种神经符号视频分析系统，结合了用户友好的查询接口和高效处理能力，解决了现有视频分析方法的灵活性与效率问题。

- Motivation: 现有视频分析方法在灵活性和效率之间存在权衡，视觉语言模型（VLMs）处理长上下文时效率低，而神经符号方法依赖手动标注和固定规则。
- Method: LazyVLM通过半结构化文本接口支持复杂多帧视频查询，将查询分解为细粒度操作，利用关系查询执行和向量相似性搜索提高效率。
- Result: LazyVLM为开放域视频数据提供了高效、鲁棒且用户友好的查询解决方案。
- Conclusion: LazyVLM成功解决了VLMs的可扩展性问题，同时保持了用户友好性。
## cs.SD

### [156] [Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs](https://arxiv.org/abs/2505.20638)
*Wenhao You,Xingjian Diao,Chunhui Zhang,Keyi Kong,Weiyi Wu,Zhongyu Ouyang,Chiyu Ma,Tingxuan Wu,Noah Wei,Zong Ke,Ming Cheng,Soroush Vosoughi,Jiang Gui*

Main category: cs.SD

TL;DR: 论文指出，针对音乐领域的多模态任务（如音乐视听问答），需要专门的处理方法，包括输入处理、时空架构设计和音乐特定建模策略。

- Motivation: 通用多模态大语言模型在音乐领域表现不足，音乐视听问答因其复杂的时空动态和领域知识需求，需要专门的研究。
- Method: 系统分析了音乐视听问答数据集和方法，提出专门输入处理、时空架构设计和音乐特定建模策略。
- Result: 研究提供了有效的设计模式，并提出了结合音乐先验的未来方向，为多模态音乐理解奠定了基础。
- Conclusion: 论文旨在激发更多关注和研究，推动音乐多模态领域的发展，并提供了相关论文的GitHub资源。


### [157] [VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin](https://arxiv.org/abs/2505.21445)
*Zhiqi Ai,Meixuan Bao,Zhiyong Chen,Zhi Yang,Xinnuo Li,Shugong Xu*

Main category: cs.SD

TL;DR: VoxAging数据集解决了说话人验证系统中因年龄变化导致性能下降的问题，提供了293名说话人的长期纵向数据。

- Motivation: 由于缺乏长期、大规模的纵向数据，说话人年龄变化的研究一直受限。
- Method: 收集了293名说话人（226名英语和67名普通话）的长期数据，最长跨度达17年，每周记录一次。
- Result: 研究了说话人年龄变化现象及其对验证系统的影响，分析了年龄组和性别等因素的影响。
- Conclusion: VoxAging数据集为说话人年龄变化研究提供了重要资源。
## eess.IV

### [158] [Unpaired Image-to-Image Translation for Segmentation and Signal Unmixing](https://arxiv.org/abs/2505.20746)
*Nikola Andrejic,Milica Spasic,Igor Mihajlovic,Petra Milosavljevic,Djordje Pavlovic,Filip Milisavljevic,Uros Milivojevic,Danilo Delibasic,Ivana Mikic,Sinisa Todorovic*

Main category: eess.IV

TL;DR: Ui2i是一种新型的无配对图像到图像翻译模型，通过改进CycleGAN，更好地解耦内容和风格特征，并保留内容完整性。

- Motivation: 解决无配对数据集中风格转换时内容保留的挑战，特别是在需要高结构保真度的生物医学任务中。
- Method: 采用U-Net生成器、近似双向谱归一化和注意力机制，结合图像尺度增强训练。
- Result: 在生物医学任务中（如IHC图像核分割和IF图像信号分离），Ui2i表现出优异的内容保真度。
- Conclusion: Ui2i是首个能够使用真实无配对数据分离IF图像叠加信号的方法，具有广泛的应用潜力。


### [159] [The Role of AI in Early Detection of Life-Threatening Diseases: A Retinal Imaging Perspective](https://arxiv.org/abs/2505.20810)
*Tariq M Khan,Toufique Ahmed Soomro,Imran Razzak*

Main category: eess.IV

TL;DR: 视网膜成像技术结合AI和移动健康技术，为系统性疾病的早期检测提供了新方法，但临床转化仍面临挑战。

- Motivation: 整合分散的视网膜成像技术，利用AI和移动健康技术提高系统性疾病的早期检测能力。
- Method: 综述了OCT/OCTA、AO技术、AI/ML算法及移动健康平台的最新进展。
- Result: AI模型在糖尿病视网膜病变和心血管风险预测中表现出高敏感性和特异性。
- Conclusion: 提出标准化协议和多中心验证的路线图，以推动视网膜筛查在临床中的广泛应用。


### [160] [Multitemporal Latent Dynamical Framework for Hyperspectral Images Unmixing](https://arxiv.org/abs/2505.20902)
*Ruiying Li,Bin Pan,Lan Ma,Xia Xu,Zhenwei Shi*

Main category: eess.IV

TL;DR: 论文提出了一种多时相高光谱解混框架MiLD，通过神经ODE建模丰度动态变化，解决了现有方法忽略丰度动态性的问题，并提供了理论支持。

- Motivation: 现有方法多关注端元变化而忽略丰度动态性，作者希望通过神经ODE建模丰度的时间演化，但面临问题定义复杂和缺乏理论支持的挑战。
- Method: 提出MiLD框架，包括问题定义（ODE和潜变量）、数学建模（动态离散化方法）、求解算法（神经网络近似丰度演化）和理论验证（一致性、收敛性和稳定性定理）。
- Result: 在合成和真实数据集上的实验验证了MiLD的有效性。
- Conclusion: MiLD通过ODE定义问题、动态离散化建模、算法求解和理论支持，成功解决了多时相高光谱解混中的丰度动态性问题。


### [161] [Generative Image Compression by Estimating Gradients of the Rate-variable Feature Distribution](https://arxiv.org/abs/2505.20984)
*Minghao Han,Weiyi You,Jinhua Zhang,Leheng Zhang,Ce Zhu,Shuhang Gu*

Main category: eess.IV

TL;DR: 本文提出了一种基于扩散模型的生成式图像压缩方法，通过将压缩过程重新解释为随机微分方程（SDEs）控制的前向扩散路径，实现了高质量图像重建。

- Motivation: 传统学习图像压缩（LIC）关注高效数据传输，而生成式图像压缩（GIC）通过集成生成模型进一步生成逼真重建图像。本文旨在改进现有方法。
- Method: 提出了一种新的扩散生成模型框架，将压缩过程视为前向扩散路径，并通过反向神经网络直接重建图像，无需高斯噪声初始化。
- Result: 实验表明，该方法在感知失真、统计保真度和无参考质量评估等多项指标上优于现有生成式图像压缩方法。
- Conclusion: 该方法通过少量采样步骤实现了平滑速率调整和逼真重建，为生成式图像压缩提供了新思路。


### [162] [Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods](https://arxiv.org/abs/2505.21355)
*Muhammad Imran,Wayne G. Brisbane,Li-Ming Su,Jason P. Joseph,Wei Shao*

Main category: eess.IV

TL;DR: AI分析微超声图像在检测前列腺癌方面优于传统PSA和DRE筛查方法，提高了特异性且保持高敏感性。

- Motivation: 探索AI在微超声图像分析中的应用，以改进前列腺癌的筛查方法，减少不必要的活检。
- Method: 使用自监督卷积自编码器提取图像特征，随机森林分类器预测癌症，并与传统临床筛查方法对比。
- Result: AI模型的AUROC为0.871，敏感性92.5%，特异性68.1%，优于临床模型的AUROC 0.753。
- Conclusion: AI辅助的微超声分析可作为低成本替代方案，减少不必要的活检，提高筛查效率。
## cs.CY

### [163] [Cultural Awareness in Vision-Language Models: A Cross-Country Exploration](https://arxiv.org/abs/2505.20326)
*Avinash Madasu,Vasudev Lal,Phillip Howard*

Main category: cs.CY

TL;DR: 提出了一种新框架，用于评估视觉语言模型（VLMs）在种族、性别和身体特征方面的文化偏见，揭示了模型中的刻板印象问题。

- Motivation: 理解VLMs在不同文化背景中的内部偏见，填补现有研究的空白。
- Method: 设计了三个检索任务：种族与国家关联、个人特质与国家关联、身体特征与国家关联。
- Result: 发现VLMs中存在持续的偏见，视觉表征可能无意中强化社会刻板印象。
- Conclusion: 强调需要改进VLMs以减少文化偏见和刻板印象。
## cs.RO

### [164] [Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments](https://arxiv.org/abs/2505.20423)
*Julio de la Torre-Vanegas,Miguel Soriano-Garcia,Israel Becerra,Diego Mercado-Ravell*

Main category: cs.RO

TL;DR: 提出了一种基于语义分割的风险感知方法，用于无人机在复杂城市环境中安全着陆，成功率达90%以上。

- Motivation: 解决无人机在紧急情况下于拥挤城市环境中安全着陆的挑战。
- Method: 使用深度神经网络进行像素级风险评估，结合风险地图算法动态识别安全着陆区，并通过控制系统引导无人机。
- Result: 在多样化城市环境中验证，着陆成功率超过90%，风险指标显著改善。
- Conclusion: 风险导向的视觉方法能有效降低紧急着陆事故风险，提升无人机在复杂城市环境中的操作能力。


### [165] [Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review](https://arxiv.org/abs/2505.20503)
*Matthew Lisondra,Beno Benhabib,Goldie Nejat*

Main category: cs.RO

TL;DR: 本文系统综述了基础模型在移动服务机器人中的集成，探讨了其在实时传感器融合、语言条件控制和自适应任务执行中的作用，并提出了未来研究方向。

- Motivation: 基础模型的快速发展为移动服务机器人中的具身AI提供了新机遇，但多模态传感器融合、实时决策等挑战仍需解决。
- Method: 通过系统综述，分析基础模型如何应对具身AI的关键挑战，并探讨其在实际应用中的潜力。
- Result: 基础模型在家庭辅助、医疗和服务自动化等领域展示了变革性影响，但仍需进一步研究以实现规模化部署。
- Conclusion: 未来研究应关注预测性扩展法则、自主长期适应和跨具身泛化，以实现基础模型在机器人系统中的高效稳健部署。


### [166] [Spatial RoboGrasp: Generalized Robotic Grasping Control Policy](https://arxiv.org/abs/2505.20814)
*Yiqi Huang,Travis Davies,Jiahuan Yan,Jiankai Sun,Xiang Chen,Luhui Hu*

Main category: cs.RO

TL;DR: 论文提出了一种结合多模态感知和扩散策略的统一框架，显著提升了机器人抓取的成功率和任务完成率。

- Motivation: 解决机器人操作在多样化环境中泛化性和精确性不足的问题，尤其是空间感知的局限性。
- Method: 融合领域随机增强、单目深度估计和深度感知的6自由度抓取提示，构建统一的空间表示，并基于扩散策略生成动作序列。
- Result: 抓取成功率提升40%，任务成功率提升45%，在环境变化下表现稳健。
- Conclusion: 空间感知与扩散模仿学习的结合为通用机器人抓取提供了可扩展且鲁棒的解决方案。


### [167] [Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning](https://arxiv.org/abs/2505.20962)
*Nikos Giannakakis,Argyris Manetas,Panagiotis P. Filntisis,Petros Maragos,George Retsinas*

Main category: cs.RO

TL;DR: 提出了一种对象为中心的编码器，结合语义分割和视觉表示生成，利用Slot Attention和SOLV模型，通过模拟机器人任务验证其有效性。

- Motivation: 受人类认知和感知启发，探索从观察动作中学习视觉表示以提升机器人视觉运动策略生成。
- Method: 使用Slot Attention机制和预训练的SOLV模型，在人类动作视频数据上进行微调，实现语义分割与视觉表示的耦合生成。
- Result: 视觉表示增强了强化学习和模仿学习的训练效果，预训练模型在非领域数据集上的微调显著提升性能。
- Conclusion: 该方法减少了对标注或机器人特定数据集的依赖，利用现有视觉编码器加速训练并提高泛化能力。
## cs.HC

### [168] [Learning Annotation Consensus for Continuous Emotion Recognition](https://arxiv.org/abs/2505.21196)
*Ibrahim Shoer,Engin Erzin*

Main category: cs.HC

TL;DR: 提出一种多标注者训练方法，用于连续情感识别，通过共识网络整合标注，优于传统单标签方法。

- Motivation: 情感计算数据集中多标注者标注常不一致，传统方法合并为单一标签会丢失有价值的信息。
- Method: 使用共识网络整合多标注者标注，指导主预测器更好地反映集体输入。
- Result: 在RECOLA和COGNIMUSE数据集上表现优于传统单标签方法。
- Conclusion: 多标注者数据在情感识别中具有优势，适用于标注丰富但不一致的领域。
## stat.ML

### [169] [A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data](https://arxiv.org/abs/2505.20688)
*Taehyo Kim,Qiran Jia,Mony J. de Leon,Hai Shu*

Main category: stat.ML

TL;DR: 提出了一种名为fcHMRF-LIS的空间FDR控制方法，用于解决神经影像数据中多重测试的三大挑战：复杂空间依赖性、FDP和FNP的低变异性以及计算可扩展性。

- Motivation: 经典FDR控制方法假设测试独立，导致高FNR，且现有空间FDR方法未能同时解决神经影像应用中的三大挑战。
- Method: 结合LIS测试程序和新型fcHMRF模型，利用EM算法、CRF-RNN技术和permutohedral lattice过滤，降低计算复杂度。
- Result: fcHMRF-LIS在模拟中表现出准确的FDR控制、更低的FNR、更稳定的FDP/FNP变异性及更高的真阳性率。
- Conclusion: fcHMRF-LIS在阿尔茨海默病FDG-PET数据中识别出相关脑区，并显著提升计算效率。
## cs.GR

### [170] [ART-DECO: Arbitrary Text Guidance for 3D Detailizer Construction](https://arxiv.org/abs/2505.20431)
*Qimin Chen,Yuezhi Yang,Yifang Wang,Vladimir G. Kim,Siddhartha Chaudhuri,Hao Zhang,Zhiqin Chen*

Main category: cs.GR

TL;DR: 提出了一种3D细节化模型，能够快速将粗糙的3D形状转换为高质量资产，支持文本引导的细节生成。

- Motivation: 解决现有文本到3D生成模型在细节控制和交互性上的不足，提供更灵活的结构控制和快速生成能力。
- Method: 使用预训练的多视角图像扩散模型，通过Score Distillation Sampling (SDS)进行知识蒸馏，分两阶段训练以处理复杂结构。
- Result: 生成质量优于现有文本到3D模型，支持交互式编辑，且能处理超出分布的结构和风格。
- Conclusion: 该方法在3D建模中实现了高效、高质量的细节生成，具有广泛的应用潜力。


### [171] [Stochastic Preconditioning for Neural Field Optimization](https://arxiv.org/abs/2505.20473)
*Selena Ling,Merlin Nimier-David,Alec Jacobson,Nicholas Sharp*

Main category: cs.GR

TL;DR: 通过引入空间随机性训练神经场，显著提升拟合效果，替代或超越定制层次结构。

- Motivation: 神经场在视觉计算中表现优异，但训练时加入空间随机性可进一步提升效果。
- Method: 提出一种基于高斯分布偏移采样的模糊场查询方法，优化收敛性和鲁棒性。
- Result: 实验证明该方法在多种任务中表现优异，甚至超越定制层次结构。
- Conclusion: 随机预处理方法简单高效，为神经场训练提供了统一且强大的工具。


### [172] [CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians](https://arxiv.org/abs/2505.21041)
*Weihang Liu,Yuhui Zhong,Yuke Li,Xi Chen,Jiadi Cui,Honglong Zhang,Lan Xu,Xin Lou,Yujiao Shi,Jingyi Yu,Yingliang Zhang*

Main category: cs.GR

TL;DR: CityGo是一个结合纹理代理几何与3D高斯残差和周围高斯的混合框架，用于轻量级、逼真的城市场景渲染。

- Motivation: 大规模城市场景的精确高效建模对AR导航、无人机巡检和智慧城市数字孪生至关重要，但现有方法存在遮挡、几何不完整和高内存需求等问题。
- Method: CityGo首先从MVS点云提取建筑代理网格，利用零阶SH高斯生成无遮挡纹理，并通过残差高斯和周围高斯捕捉高频细节和上下文。
- Result: 实验表明，CityGo显著减少训练时间（平均1.4倍加速），并在移动GPU上实现实时渲染，内存和能耗大幅降低。
- Conclusion: CityGo通过混合表示实现了高效、轻量级的城市场景渲染，适用于移动设备。


### [173] [IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model](https://arxiv.org/abs/2505.21146)
*Yang Zhao,Yan Zhang,Xubo Yang*

Main category: cs.GR

TL;DR: IKMo是一种基于扩散模型的运动生成方法，通过解耦轨迹和姿态输入，采用两阶段条件框架生成高保真运动。实验证明其在轨迹关键帧约束下优于现有方法，并通过MLLM代理优化输入提升用户满意度。

- Motivation: 现有方法对轨迹和姿态进行全局处理导致输出不理想，需改进以提高运动生成的保真度和可控性。
- Method: 提出IKMo方法，采用两阶段框架：优化模块细化输入，轨迹和姿态编码器并行处理，再通过ControlNet融合生成运动。
- Result: 在HumanML3D和KIT-ML数据集上表现优于现有方法，用户研究表明MLLM代理优化输入更符合用户期望。
- Conclusion: IKMo通过解耦和优化输入提升了扩散模型运动生成的保真度和可控性。


### [174] [efunc: An Efficient Function Representation without Neural Networks](https://arxiv.org/abs/2505.21319)
*Biao Zhang,Peter Wonka*

Main category: cs.GR

TL;DR: 论文提出了一种参数高效、不依赖神经网络的函数拟合方法，基于多项式插值和径向基函数，显著减少了计算时间和内存消耗。

- Motivation: 解决传统神经网络方法参数过多、实用性受限的问题，追求高效且高质量的函数逼近。
- Method: 提出连续函数建模框架，采用多项式插值和径向基函数的紧凑表示，开发了CUDA优化的高效算法。
- Result: 在3D符号距离函数实验中，性能优于或媲美现有技术，且参数更少。
- Conclusion: 该方法提供了一种高效、参数少的函数逼近方案，适用于计算机图形学等工程应用。


### [175] [Structure from Collision](https://arxiv.org/abs/2505.21335)
*Takuhiro Kaneko*

Main category: cs.GR

TL;DR: 论文提出了一种名为SfC-NeRF的新模型，通过碰撞过程中的外观变化估计物体的内部结构，解决了现有3D表示方法无法捕捉内部结构的问题。

- Motivation: 现有的神经3D表示方法（如NeRF和3DGS）只能估计可见的外部结构，无法捕捉内部结构。本文旨在解决这一局限性。
- Method: 提出SfC-NeRF模型，通过物理约束、外观保持和关键帧约束优化视频序列中的内部结构，并引入体积退火方法避免局部最优。
- Result: 在115个不同结构和材料属性的物体上进行了实验，验证了SfC-NeRF的有效性。
- Conclusion: SfC-NeRF成功解决了从碰撞中估计内部结构的任务，为3D表示提供了新的可能性。


### [176] [CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects](https://arxiv.org/abs/2505.21437)
*Huaijin Pi,Zhi Cen,Zhiyang Dou,Taku Komura*

Main category: cs.GR

TL;DR: 提出了一种协调扩散噪声优化框架，用于合成全身操纵关节物体的动作，包括身体、手部和物体运动，解决了协调性和精确性问题。

- Motivation: 全身操纵关节物体在虚拟人类和机器人领域有广泛应用，但协调身体与手部运动以及高自由度精确操作是两大挑战。
- Method: 采用三个专用扩散模型（身体、左手、右手）进行噪声空间优化，通过梯度流实现协调，并使用BPS统一表示增强手-物体交互精度。
- Result: 实验表明，该方法在运动质量和物理合理性上优于现有方法，支持多种功能如物体姿态控制、行走与操纵同步等。
- Conclusion: 该方法有效解决了全身操纵的协调与精确性问题，具有广泛的应用潜力。
