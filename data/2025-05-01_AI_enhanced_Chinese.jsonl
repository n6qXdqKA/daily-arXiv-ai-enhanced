{"id": "2504.21040", "pdf": "https://arxiv.org/pdf/2504.21040", "abs": "https://arxiv.org/abs/2504.21040", "authors": ["Chenyi Cai", "Kosuke Kuriyama", "Youlong Gu", "Filip Biljecki", "Pieter Herthogs"], "title": "Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels", "categories": ["cs.CV"], "comment": null, "summary": "Urban street environments are vital to supporting human activity in public\nspaces. The emergence of big data, such as street view images (SVIs) combined\nwith multimodal large language models (MLLMs), is transforming how researchers\nand practitioners investigate, measure, and evaluate semantic and visual\nelements of urban environments. Considering the low threshold for creating\nautomated evaluative workflows using MLLMs, it is crucial to explore both the\nrisks and opportunities associated with these probabilistic models. In\nparticular, the extent to which the integration of expert knowledge can\ninfluence the performance of MLLMs in evaluating the quality of urban design\nhas not been fully explored. This study sets out an initial exploration of how\nintegrating more formal and structured representations of expert urban design\nknowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's\ncapability and reliability in evaluating the walkability of built environments\nusing SVIs. We collect walkability metrics from the existing literature and\ncategorize them using relevant ontologies. We then select a subset of these\nmetrics, focusing on the subthemes of pedestrian safety and attractiveness, and\ndevelop prompts for the MLLM accordingly. We analyze the MLLM's ability to\nevaluate SVI walkability subthemes through prompts with varying levels of\nclarity and specificity regarding evaluation criteria. Our experiments\ndemonstrate that MLLMs are capable of providing assessments and interpretations\nbased on general knowledge and can support the automation of multimodal\nimage-text evaluations. However, they generally provide more optimistic scores\nand can make mistakes when interpreting the provided metrics, resulting in\nincorrect evaluations. By integrating expert knowledge, the MLLM's evaluative\nperformance exhibits higher consistency and concentration.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u8bc4\u4f30\u57ce\u5e02\u6b65\u884c\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e13\u5bb6\u77e5\u8bc6\u80fd\u63d0\u9ad8\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4e13\u5bb6\u77e5\u8bc6\u5982\u4f55\u5f71\u54cdMLLM\u5728\u8bc4\u4f30\u57ce\u5e02\u8bbe\u8ba1\u8d28\u91cf\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6536\u96c6\u6b65\u884c\u6027\u6307\u6807\uff0c\u57fa\u4e8e\u4e13\u5bb6\u77e5\u8bc6\u8bbe\u8ba1\u63d0\u793a\u8bcd\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u6e05\u6670\u5ea6\u548c\u7279\u5f02\u6027\u7684\u63d0\u793a\u6d4b\u8bd5MLLM\u7684\u8bc4\u4f30\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMLLM\u80fd\u57fa\u4e8e\u901a\u7528\u77e5\u8bc6\u63d0\u4f9b\u8bc4\u4f30\uff0c\u4f46\u6613\u8fc7\u4e8e\u4e50\u89c2\u6216\u8bef\u89e3\u6307\u6807\uff1b\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\u540e\uff0c\u8bc4\u4f30\u8868\u73b0\u66f4\u4e00\u81f4\u548c\u96c6\u4e2d\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u4e13\u5bb6\u77e5\u8bc6\u80fd\u663e\u8457\u63d0\u5347MLLM\u7684\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u652f\u6301\u5176\u5728\u591a\u6a21\u6001\u56fe\u50cf-\u6587\u672c\u8bc4\u4f30\u4e2d\u7684\u81ea\u52a8\u5316\u5e94\u7528\u3002"}}
{"id": "2504.21136", "pdf": "https://arxiv.org/pdf/2504.21136", "abs": "https://arxiv.org/abs/2504.21136", "authors": ["Murali Ramanujam", "Yinwei Dai", "Kyle Jamieson", "Ravi Netravali"], "title": "Legilimens: Performant Video Analytics on the System-on-Chip Edge", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Continually retraining models has emerged as a primary technique to enable\nhigh-accuracy video analytics on edge devices. Yet, existing systems employ\nsuch adaptation by relying on the spare compute resources that traditional\n(memory-constrained) edge servers afford. In contrast, mobile edge devices such\nas drones and dashcams offer a fundamentally different resource profile:\nweak(er) compute with abundant unified memory pools. We present Legilimens, a\ncontinuous learning system for the mobile edge's System-on-Chip GPUs. Our\ndriving insight is that visually distinct scenes that require retraining\nexhibit substantial overlap in model embeddings; if captured into a base model\non device memory, specializing to each new scene can become lightweight,\nrequiring very few samples. To practically realize this approach, Legilimens\npresents new, compute-efficient techniques to (1) select high-utility data\nsamples for retraining specialized models, (2) update the base model without\ncomplete retraining, and (3) time-share compute resources between retraining\nand live inference for maximal accuracy. Across diverse workloads, Legilimens\nlowers retraining costs by 2.8-10x compared to existing systems, resulting in\n18-45% higher accuracies.", "AI": {"tldr": "Legilimens\u662f\u4e00\u79cd\u9488\u5bf9\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u65e0\u4eba\u673a\u548c\u884c\u8f66\u8bb0\u5f55\u4eea\uff09\u7684\u8fde\u7eed\u5b66\u4e60\u7cfb\u7edf\uff0c\u5229\u7528\u8bbe\u5907\u5185\u5b58\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u548c\u9ad8\u6548\u8ba1\u7b97\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u8fb9\u7f18\u670d\u52a1\u5668\u8d44\u6e90\u6709\u9650\uff0c\u800c\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u5177\u6709\u4e30\u5bcc\u7684\u7edf\u4e00\u5185\u5b58\u6c60\u4f46\u8ba1\u7b97\u80fd\u529b\u8f83\u5f31\uff0c\u9700\u8981\u4e00\u79cd\u9002\u5e94\u5176\u8d44\u6e90\u7279\u6027\u7684\u8fde\u7eed\u5b66\u4e60\u7cfb\u7edf\u3002", "method": "Legilimens\u901a\u8fc7\u9ad8\u6548\u8ba1\u7b97\u6280\u672f\u9009\u62e9\u9ad8\u6548\u7528\u6570\u636e\u6837\u672c\u3001\u66f4\u65b0\u57fa\u7840\u6a21\u578b\u800c\u4e0d\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u5728\u91cd\u65b0\u8bad\u7ec3\u548c\u5b9e\u65f6\u63a8\u7406\u4e4b\u95f4\u5171\u4eab\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\uff0cLegilimens\u5c06\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u964d\u4f4e\u4e862.8-10\u500d\uff0c\u51c6\u786e\u6027\u63d0\u9ad8\u4e8618-45%\u3002", "conclusion": "Legilimens\u4e3a\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fde\u7eed\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2504.21154", "pdf": "https://arxiv.org/pdf/2504.21154", "abs": "https://arxiv.org/abs/2504.21154", "authors": ["Muhammad Turab", "Philippe Colantoni", "Damien Muselet", "Alain Tremeau"], "title": "Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a novel framework for emotion recognition in contemporary\ndance by improving existing Laban Movement Analysis (LMA) feature descriptors\nand introducing robust, novel descriptors that capture both quantitative and\nqualitative aspects of the movement. Our approach extracts expressive\ncharacteristics from 3D keypoints data of professional dancers performing\ncontemporary dance under various emotional states, and trains multiple\nclassifiers, including Random Forests and Support Vector Machines.\nAdditionally, we provide in-depth explanation of features and their impact on\nmodel predictions using explainable machine learning methods. Overall, our\nstudy improves emotion recognition in contemporary dance and offers promising\napplications in performance analysis, dance training, and human--computer\ninteraction, with a highest accuracy of 96.85\\%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdbLaban\u8fd0\u52a8\u5206\u6790\u7279\u5f81\u63cf\u8ff0\u7b26\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5f53\u4ee3\u821e\u8e48\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\uff0c\u7ed3\u5408\u5b9a\u91cf\u548c\u5b9a\u6027\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u7279\u5f81\u5f71\u54cd\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\uff0c\u6355\u6349\u821e\u8e48\u52a8\u4f5c\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u7279\u5f81\uff0c\u4ee5\u63d0\u5347\u5f53\u4ee3\u821e\u8e48\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\u6548\u679c\u3002", "method": "\u4ece3D\u5173\u952e\u70b9\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u8bad\u7ec3\u591a\u79cd\u5206\u7c7b\u5668\uff08\u5982\u968f\u673a\u68ee\u6797\u548c\u652f\u6301\u5411\u91cf\u673a\uff09\uff0c\u5e76\u4f7f\u7528\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u7279\u5f81\u3002", "result": "\u6700\u9ad8\u51c6\u786e\u7387\u8fbe\u523096.85%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u821e\u8e48\u8868\u6f14\u5206\u6790\u3001\u8bad\u7ec3\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2504.21166", "pdf": "https://arxiv.org/pdf/2504.21166", "abs": "https://arxiv.org/abs/2504.21166", "authors": ["Muhammad Turab", "Philippe Colantoni", "Damien Muselet", "Alain Tremeau"], "title": "Dance Style Recognition Using Laban Movement Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The growing interest in automated movement analysis has presented new\nchallenges in recognition of complex human activities including dance. This\nstudy focuses on dance style recognition using features extracted using Laban\nMovement Analysis. Previous studies for dance style recognition often focus on\ncross-frame movement analysis, which limits the ability to capture temporal\ncontext and dynamic transitions between movements. This gap highlights the need\nfor a method that can add temporal context to LMA features. For this, we\nintroduce a novel pipeline which combines 3D pose estimation, 3D human mesh\nreconstruction, and floor aware body modeling to effectively extract LMA\nfeatures. To address the temporal limitation, we propose a sliding window\napproach that captures movement evolution across time in features. These\nfeatures are then used to train various machine learning methods for\nclassification, and their explainability explainable AI methods to evaluate the\ncontribution of each feature to classification performance. Our proposed method\nachieves a highest classification accuracy of 99.18\\% which shows that the\naddition of temporal context significantly improves dance style recognition\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u59ff\u6001\u4f30\u8ba1\u30013D\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u548c\u5730\u677f\u611f\u77e5\u8eab\u4f53\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u53d6Laban\u8fd0\u52a8\u5206\u6790\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u6355\u6349\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u821e\u8e48\u98ce\u683c\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u821e\u8e48\u98ce\u683c\u8bc6\u522b\u65b9\u6cd5\u591a\u5173\u6ce8\u8de8\u5e27\u8fd0\u52a8\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u52a8\u6001\u8fc7\u6e21\u7684\u6355\u6349\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u589e\u5f3aLMA\u7279\u5f81\u65f6\u95f4\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u54083D\u59ff\u6001\u4f30\u8ba1\u30013D\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u548c\u5730\u677f\u611f\u77e5\u8eab\u4f53\u5efa\u6a21\u63d0\u53d6LMA\u7279\u5f81\uff0c\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u6355\u6349\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u548c\u53ef\u89e3\u91caAI\u65b9\u6cd5\u8bc4\u4f30\u7279\u5f81\u8d21\u732e\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u821e\u8e48\u98ce\u683c\u8bc6\u522b\u4e2d\u8fbe\u5230\u4e8699.18%\u7684\u6700\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8868\u660e\u65f6\u95f4\u4e0a\u4e0b\u6587\u7684\u52a0\u5165\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u65b0\u578b\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u821e\u8e48\u98ce\u683c\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.21131", "pdf": "https://arxiv.org/pdf/2504.21131", "abs": "https://arxiv.org/abs/2504.21131", "authors": ["Remo Christen", "Florian Pommerening", "Clemens B\u00fcchner", "Malte Helmert"], "title": "A Formalism for Optimal Search with Dynamic Heuristics", "categories": ["cs.AI"], "comment": null, "summary": "While most heuristics studied in heuristic search depend only on the state,\nsome accumulate information during search and thus also depend on the search\nhistory. Various existing approaches use such dynamic heuristics in\n$\\mathrm{A}^*$-like algorithms and appeal to classic results for $\\mathrm{A}^*$\nto show optimality. However, doing so ignores the complexities of searching\nwith a mutable heuristic. In this paper we formalize the idea of dynamic\nheuristics and use them in a generic algorithm framework. We study a particular\ninstantiation that models $\\mathrm{A}^*$ with dynamic heuristics and show\ngeneral optimality results. Finally we show how existing approaches from\nclassical planning can be viewed as special cases of this instantiation, making\nit possible to directly apply our optimality results.", "AI": {"tldr": "\u672c\u6587\u5f62\u5f0f\u5316\u4e86\u52a8\u6001\u542f\u53d1\u5f0f\u7684\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u901a\u7528\u7b97\u6cd5\u6846\u67b6\u4e2d\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u542f\u53d1\u5f0f\u5728A*\u7b97\u6cd5\u4e2d\u7684\u6700\u4f18\u6027\uff0c\u5e76\u5c06\u7ecf\u5178\u89c4\u5212\u4e2d\u7684\u73b0\u6709\u65b9\u6cd5\u89c6\u4e3a\u5176\u7279\u4f8b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u4f7f\u7528\u52a8\u6001\u542f\u53d1\u5f0f\u65f6\u5ffd\u7565\u4e86\u5176\u53ef\u53d8\u6027\u5e26\u6765\u7684\u590d\u6742\u6027\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7b97\u6cd5\u6846\u67b6\uff0c\u5e76\u5728\u5176\u4e2d\u5b9e\u4f8b\u5316\u52a8\u6001\u542f\u53d1\u5f0f\u7684A*\u7b97\u6cd5\uff0c\u7814\u7a76\u5176\u6700\u4f18\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u52a8\u6001\u542f\u53d1\u5f0f\u5728A*\u7b97\u6cd5\u4e2d\u7684\u4e00\u822c\u6700\u4f18\u6027\u7ed3\u679c\u3002", "conclusion": "\u5c06\u7ecf\u5178\u89c4\u5212\u4e2d\u7684\u65b9\u6cd5\u89c6\u4e3a\u672c\u6587\u6846\u67b6\u7684\u7279\u4f8b\uff0c\u4ece\u800c\u76f4\u63a5\u5e94\u7528\u6700\u4f18\u6027\u7ed3\u679c\u3002"}}
{"id": "2504.21194", "pdf": "https://arxiv.org/pdf/2504.21194", "abs": "https://arxiv.org/abs/2504.21194", "authors": ["Vedika Srivastava", "Hemant Kumar Singh", "Jaisal Singh"], "title": "Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a novel approach to geolocating images captured from the\nInternational Space Station (ISS) using advanced machine learning algorithms.\nDespite having precise ISS coordinates, the specific Earth locations depicted\nin astronaut-taken photographs often remain unidentified. Our research\naddresses this gap by employing three distinct image processing pipelines: a\nNeural Network based approach, a SIFT based method, and GPT-4 model. Each\npipeline is tailored to process high-resolution ISS imagery, identifying both\nnatural and man-made geographical features. Through extensive evaluation on a\ndiverse dataset of over 140 ISS images, our methods demonstrate significant\npromise in automated geolocation with varied levels of success. The NN approach\nshowed a high success rate in accurately matching geographical features, while\nthe SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided\nenriched geographical descriptions alongside location predictions. This\nresearch contributes to the fields of remote sensing and Earth observation by\nenhancing the accuracy and efficiency of geolocating space-based imagery,\nthereby aiding environmental monitoring and global mapping efforts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4ece\u56fd\u9645\u7a7a\u95f4\u7ad9\uff08ISS\uff09\u62cd\u6444\u7684\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u5730\u7403\u4f4d\u7f6e\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u79cd\u4e0d\u540c\u7684\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\uff08\u795e\u7ecf\u7f51\u7edc\u3001SIFT\u65b9\u6cd5\u548cGPT-4\u6a21\u578b\uff09\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5730\u7406\u7279\u5f81\u8bc6\u522b\u3002", "motivation": "ISS\u62cd\u6444\u7684\u56fe\u50cf\u867d\u7136\u5750\u6807\u7cbe\u786e\uff0c\u4f46\u5177\u4f53\u5730\u7406\u4f4d\u7f6e\u5e38\u672a\u88ab\u8bc6\u522b\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u7a7a\u95f4\u56fe\u50cf\u7684\u5730\u7406\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\uff1a\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u3001SIFT\u65b9\u6cd5\u548cGPT-4\u6a21\u578b\uff0c\u5206\u522b\u5904\u7406\u9ad8\u5206\u8fa8\u7387ISS\u56fe\u50cf\uff0c\u8bc6\u522b\u81ea\u7136\u548c\u4eba\u5de5\u5730\u7406\u7279\u5f81\u3002", "result": "\u5728140\u591a\u5f20ISS\u56fe\u50cf\u7684\u6d4b\u8bd5\u4e2d\uff0c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u5730\u7406\u7279\u5f81\u5339\u914d\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cSIFT\u65b9\u6cd5\u64c5\u957f\u5904\u7406\u653e\u5927\u56fe\u50cf\uff0cGPT-4\u6a21\u578b\u5219\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u5730\u7406\u63cf\u8ff0\u548c\u4f4d\u7f6e\u9884\u6d4b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u5347\u4e86\u7a7a\u95f4\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u6709\u52a9\u4e8e\u73af\u5883\u76d1\u6d4b\u548c\u5168\u7403\u5730\u56fe\u7ed8\u5236\u3002"}}
{"id": "2504.21184", "pdf": "https://arxiv.org/pdf/2504.21184", "abs": "https://arxiv.org/abs/2504.21184", "authors": ["Emily Zhou", "Khushboo Khatri", "Yixue Zhao", "Bhaskar Krishnamachari"], "title": "AffectEval: A Modular and Customizable Framework for Affective Computing", "categories": ["cs.AI"], "comment": "The short version is published in ACM/IEEE CHASE 2025", "summary": "The field of affective computing focuses on recognizing, interpreting, and\nresponding to human emotions, and has broad applications across education,\nchild development, and human health and wellness. However, developing affective\ncomputing pipelines remains labor-intensive due to the lack of software\nframeworks that support multimodal, multi-domain emotion recognition\napplications. This often results in redundant effort when building pipelines\nfor different applications. While recent frameworks attempt to address these\nchallenges, they remain limited in reducing manual effort and ensuring\ncross-domain generalizability. We introduce AffectEval, a modular and\ncustomizable framework to facilitate the development of affective computing\npipelines while reducing the manual effort and duplicate work involved in\ndeveloping such pipelines. We validate AffectEval by replicating prior\naffective computing experiments, and we demonstrate that our framework reduces\nprogramming effort by up to 90%, as measured by the reduction in raw lines of\ncode.", "AI": {"tldr": "AffectEval\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u5b9a\u5236\u7684\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u60c5\u611f\u8ba1\u7b97\u7ba1\u9053\u5f00\u53d1\u4e2d\u7684\u624b\u52a8\u5de5\u4f5c\u548c\u91cd\u590d\u52b3\u52a8\uff0c\u9a8c\u8bc1\u663e\u793a\u7f16\u7a0b\u5de5\u4f5c\u91cf\u51cf\u5c1190%\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\u9886\u57df\u7f3a\u4e4f\u652f\u6301\u591a\u6a21\u6001\u3001\u591a\u9886\u57df\u60c5\u611f\u8bc6\u522b\u5e94\u7528\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u5bfc\u81f4\u5f00\u53d1\u7ba1\u9053\u65f6\u91cd\u590d\u52b3\u52a8\u3002", "method": "\u5f15\u5165AffectEval\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u548c\u53ef\u5b9a\u5236\u5316\u8bbe\u8ba1\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u590d\u73b0\u5148\u524d\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "AffectEval\u663e\u8457\u51cf\u5c11\u4e86\u7f16\u7a0b\u5de5\u4f5c\u91cf\uff08\u51cf\u5c1190%\u7684\u4ee3\u7801\u884c\u6570\uff09\u3002", "conclusion": "AffectEval\u4e3a\u60c5\u611f\u8ba1\u7b97\u7ba1\u9053\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21226", "pdf": "https://arxiv.org/pdf/2504.21226", "abs": "https://arxiv.org/abs/2504.21226", "authors": ["Jiaqi Liu", "Ran Tong", "Aowei Shen", "Shuzheng Li", "Changlin Yang", "Lisha Xu"], "title": "MemeBLIP2: A novel lightweight multimodal system to detect harmful memes", "categories": ["cs.CV", "cs.AI"], "comment": "11pages,2 figures, manucripts in preparation", "summary": "Memes often merge visuals with brief text to share humor or opinions, yet\nsome memes contain harmful messages such as hate speech. In this paper, we\nintroduces MemeBLIP2, a light weight multimodal system that detects harmful\nmemes by combining image and text features effectively. We build on previous\nstudies by adding modules that align image and text representations into a\nshared space and fuse them for better classification. Using BLIP-2 as the core\nvision-language model, our system is evaluated on the PrideMM datasets. The\nresults show that MemeBLIP2 can capture subtle cues in both modalities, even in\ncases with ironic or culturally specific content, thereby improving the\ndetection of harmful material.", "AI": {"tldr": "MemeBLIP2\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u7279\u5f81\u6709\u6548\u68c0\u6d4b\u6709\u5bb3\u8868\u60c5\u5305\u3002", "motivation": "\u8868\u60c5\u5305\u5e38\u7ed3\u5408\u89c6\u89c9\u548c\u7b80\u77ed\u6587\u672c\u4f20\u9012\u5e7d\u9ed8\u6216\u89c2\u70b9\uff0c\u4f46\u90e8\u5206\u5185\u5bb9\u5305\u542b\u6709\u5bb3\u4fe1\u606f\uff08\u5982\u4ec7\u6068\u8a00\u8bba\uff09\uff0c\u9700\u6709\u6548\u68c0\u6d4b\u3002", "method": "\u57fa\u4e8eBLIP-2\u6838\u5fc3\u6a21\u578b\uff0c\u6dfb\u52a0\u6a21\u5757\u5bf9\u9f50\u56fe\u50cf\u548c\u6587\u672c\u8868\u5f81\u81f3\u5171\u4eab\u7a7a\u95f4\u5e76\u878d\u5408\uff0c\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "result": "\u5728PrideMM\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cMemeBLIP2\u80fd\u6355\u6349\u591a\u6a21\u6001\u7ec6\u5fae\u7ebf\u7d22\uff08\u5982\u8bbd\u523a\u6216\u6587\u5316\u7279\u5b9a\u5185\u5bb9\uff09\uff0c\u6539\u8fdb\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u3002", "conclusion": "MemeBLIP2\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u6709\u5bb3\u8868\u60c5\u5305\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2504.21218", "pdf": "https://arxiv.org/pdf/2504.21218", "abs": "https://arxiv.org/abs/2504.21218", "authors": ["Sebastian Dumbrava"], "title": "Theoretical Foundations for Semantic Cognition in Artificial Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "This monograph presents a modular cognitive architecture for artificial\nintelligence grounded in the formal modeling of belief as structured semantic\nstate. Belief states are defined as dynamic ensembles of linguistic expressions\nembedded within a navigable manifold, where operators enable assimilation,\nabstraction, nullification, memory, and introspection. Drawing from philosophy,\ncognitive science, and neuroscience, we develop a layered framework that\nenables self-regulating epistemic agents capable of reflective, goal-directed\nthought. At the core of this framework is the epistemic vacuum: a class of\nsemantically inert cognitive states that serves as the conceptual origin of\nbelief space. From this foundation, the Null Tower arises as a generative\nstructure recursively built through internal representational capacities. The\ntheoretical constructs are designed to be implementable in both symbolic and\nneural systems, including large language models, hybrid agents, and adaptive\nmemory architectures. This work offers a foundational substrate for\nconstructing agents that reason, remember, and regulate their beliefs in\nstructured, interpretable ways.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u8bed\u4e49\u72b6\u6001\u7684\u6a21\u5757\u5316\u8ba4\u77e5\u67b6\u6784\uff0c\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\uff0c\u652f\u6301\u4fe1\u5ff5\u7684\u52a8\u6001\u64cd\u4f5c\u548c\u81ea\u8c03\u8282\u3002", "motivation": "\u7ed3\u5408\u54f2\u5b66\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\u7684\u7406\u8bba\uff0c\u6784\u5efa\u4e00\u79cd\u80fd\u591f\u652f\u6301\u81ea\u8c03\u8282\u3001\u76ee\u6807\u5bfc\u5411\u601d\u7ef4\u7684\u8ba4\u77e5\u6846\u67b6\u3002", "method": "\u5b9a\u4e49\u4fe1\u5ff5\u72b6\u6001\u4e3a\u52a8\u6001\u8bed\u8a00\u8868\u8fbe\u96c6\u5408\uff0c\u5f15\u5165\u64cd\u4f5c\u7b26\u5b9e\u73b0\u540c\u5316\u3001\u62bd\u8c61\u3001\u6d88\u9664\u7b49\u529f\u80fd\uff0c\u63d0\u51fa\u201c\u8ba4\u77e5\u771f\u7a7a\u201d\u548c\u201cNull Tower\u201d\u4f5c\u4e3a\u751f\u6210\u7ed3\u6784\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u5728\u7b26\u53f7\u548c\u795e\u7ecf\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7684\u6846\u67b6\uff0c\u652f\u6301\u667a\u80fd\u4f53\u7684\u63a8\u7406\u3001\u8bb0\u5fc6\u548c\u4fe1\u5ff5\u8c03\u8282\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6784\u5efa\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2504.21231", "pdf": "https://arxiv.org/pdf/2504.21231", "abs": "https://arxiv.org/abs/2504.21231", "authors": ["Manikanta Varaganti", "Amulya Vankayalapati", "Nour Awad", "Gregory R. Dion", "Laura J. Brattain"], "title": "T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "submitted to IEEE EMBC 2025", "summary": "Neck ultrasound (US) plays a vital role in airway management by providing\nnon-invasive, real-time imaging that enables rapid and precise interventions.\nDeep learning-based anatomical landmark detection in neck US can further\nfacilitate procedural efficiency. However, class imbalance within datasets,\nwhere key structures like tracheal rings and vocal folds are underrepresented,\npresents significant challenges for object detection models. To address this,\nwe propose T2ID-CAS, a hybrid approach that combines a text-to-image latent\ndiffusion model with class-aware sampling to generate high-quality synthetic\nsamples for underrepresented classes. This approach, rarely explored in the\nultrasound domain, improves the representation of minority classes.\nExperimental results using YOLOv9 for anatomical landmark detection in neck US\ndemonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,\nsignificantly surpassing the baseline of 66. This highlights its potential as a\ncomputationally efficient and scalable solution for mitigating class imbalance\nin AI-assisted ultrasound-guided interventions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u5230\u56fe\u50cf\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u7c7b\u522b\u611f\u77e5\u91c7\u6837\u7684\u6df7\u5408\u65b9\u6cd5\uff08T2ID-CAS\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u9888\u90e8\u8d85\u58f0\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u9888\u90e8\u8d85\u58f0\u5728\u6c14\u9053\u7ba1\u7406\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u96c6\u4e2d\u5173\u952e\u7ed3\u6784\uff08\u5982\u6c14\u7ba1\u73af\u548c\u58f0\u5e26\uff09\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u5f71\u54cd\u4e86\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faT2ID-CAS\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u672c\u5230\u56fe\u50cf\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u7c7b\u522b\u611f\u77e5\u91c7\u6837\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6837\u672c\u4ee5\u589e\u5f3a\u5c11\u6570\u7c7b\u522b\u7684\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cT2ID-CAS\u5728YOLOv9\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8688.2\u7684\u5e73\u5747\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u768466\u3002", "conclusion": "T2ID-CAS\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3AI\u8f85\u52a9\u8d85\u58f0\u5f15\u5bfc\u5e72\u9884\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2504.21277", "pdf": "https://arxiv.org/pdf/2504.21277", "abs": "https://arxiv.org/abs/2504.21277", "authors": ["Guanghao Zhou", "Panjia Qiu", "Cen Chen", "Jie Wang", "Zheming Yang", "Jian Xu", "Minghui Qiu"], "title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The integration of reinforcement learning (RL) into the reasoning\ncapabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as\na transformative research direction. While MLLMs significantly extend Large\nLanguage Models (LLMs) to handle diverse modalities such as vision, audio, and\nvideo, enabling robust reasoning across multimodal inputs remains a major\nchallenge. This survey systematically reviews recent advances in RL-based\nreasoning for MLLMs, covering key algorithmic designs, reward mechanism\ninnovations, and practical applications. We highlight two main RL\nparadigms--value-free and value-based methods--and analyze how RL enhances\nreasoning abilities by optimizing reasoning trajectories and aligning\nmultimodal information. Furthermore, we provide an extensive overview of\nbenchmark datasets, evaluation protocols, and existing limitations, and propose\nfuture research directions to address current bottlenecks such as sparse\nrewards, inefficient cross-modal reasoning, and real-world deployment\nconstraints. Our goal is to offer a comprehensive and structured guide to\nresearchers interested in advancing RL-based reasoning in the multimodal era.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u63a8\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u3001\u5956\u52b1\u673a\u5236\u53ca\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1MLLMs\u6269\u5c55\u4e86LLMs\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\uff0c\u4f46\u5176\u8de8\u6a21\u6001\u63a8\u7406\u4ecd\u9762\u4e34\u6311\u6218\uff0cRL\u88ab\u8ba4\u4e3a\u662f\u4e00\u79cd\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86RL\u5728MLLMs\u63a8\u7406\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u65e0\u4ef7\u503c\u4e0e\u57fa\u4e8e\u4ef7\u503c\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5956\u52b1\u673a\u5236\u548c\u63a8\u7406\u8f68\u8ff9\u4f18\u5316\u3002", "result": "\u603b\u7ed3\u4e86RL\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\u548c\u5bf9\u9f50\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u7a00\u758f\u5956\u52b1\u3001\u8de8\u6a21\u6001\u63a8\u7406\u6548\u7387\u548c\u5b9e\u9645\u90e8\u7f72\u95ee\u9898\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u7ed3\u6784\u5316\u6307\u5357\u3002"}}
{"id": "2504.21247", "pdf": "https://arxiv.org/pdf/2504.21247", "abs": "https://arxiv.org/abs/2504.21247", "authors": ["Yangyang Qu", "Dazhi Fu", "Jicong Fan"], "title": "Subject Information Extraction for Novelty Detection with Domain Shifts", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised novelty detection (UND), aimed at identifying novel samples, is\nessential in fields like medical diagnosis, cybersecurity, and industrial\nquality control. Most existing UND methods assume that the training data and\ntesting normal data originate from the same domain and only consider the\ndistribution variation between training data and testing data. However, in real\nscenarios, it is common for normal testing and training data to originate from\ndifferent domains, a challenge known as domain shift. The discrepancies between\ntraining and testing data often lead to incorrect classification of normal data\nas novel by existing methods. A typical situation is that testing normal data\nand training data describe the same subject, yet they differ in the background\nconditions. To address this problem, we introduce a novel method that separates\nsubject information from background variation encapsulating the domain\ninformation to enhance detection performance under domain shifts. The proposed\nmethod minimizes the mutual information between the representations of the\nsubject and background while modelling the background variation using a deep\nGaussian mixture model, where the novelty detection is conducted on the subject\nrepresentations solely and hence is not affected by the variation of domains.\nExtensive experiments demonstrate that our model generalizes effectively to\nunseen domains and significantly outperforms baseline methods, especially under\nsubstantial domain shifts between training and testing data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u4e3b\u4f53\u4fe1\u606f\u548c\u80cc\u666f\u53d8\u5316\u6765\u63d0\u5347\u65e0\u76d1\u7763\u65b0\u9896\u6027\u68c0\u6d4b\u5728\u57df\u504f\u79fb\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u65b0\u9896\u6027\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u6765\u81ea\u540c\u4e00\u57df\uff0c\u5ffd\u7565\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u6b63\u5e38\u6570\u636e\u88ab\u8bef\u5206\u7c7b\u4e3a\u65b0\u9896\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4e3b\u4f53\u548c\u80cc\u666f\u8868\u793a\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5efa\u6a21\u80cc\u666f\u53d8\u5316\uff0c\u4ec5\u5728\u4e3b\u4f53\u8868\u793a\u4e0a\u8fdb\u884c\u65b0\u9896\u6027\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u57df\u4e0a\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u57df\u504f\u79fb\u8f83\u5927\u65f6\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u65b0\u9896\u6027\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.21318", "pdf": "https://arxiv.org/pdf/2504.21318", "abs": "https://arxiv.org/abs/2504.21318", "authors": ["Marah Abdin", "Sahaj Agarwal", "Ahmed Awadallah", "Vidhisha Balachandran", "Harkirat Behl", "Lingjiao Chen", "Gustavo de Rosa", "Suriya Gunasekar", "Mojan Javaheripi", "Neel Joshi", "Piero Kauffmann", "Yash Lara", "Caio C\u00e9sar Teodoro Mendes", "Arindam Mitra", "Besmira Nushi", "Dimitris Papailiopoulos", "Olli Saarikivi", "Shital Shah", "Vaishnavi Shrivastava", "Vibhav Vineet", "Yue Wu", "Safoora Yousefi", "Guoqing Zheng"], "title": "Phi-4-reasoning Technical Report", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models.", "AI": {"tldr": "Phi-4-reasoning\u662f\u4e00\u4e2a14B\u53c2\u6570\u7684\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u66f4\u5927\u7684\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u7cbe\u9009\u548c\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5bf9\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5fae\u8c03Phi-4\u6a21\u578b\uff0c\u7ed3\u5408\u7cbe\u9009\u7684\u201c\u53ef\u6559\u5b66\u201d\u63d0\u793a\u548c\u63a8\u7406\u6f14\u793a\uff1b\u8fdb\u4e00\u6b65\u901a\u8fc7\u7ed3\u679c\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u4e86Phi-4-reasoning-plus\u53d8\u4f53\u3002", "result": "\u6a21\u578b\u5728\u6570\u5b66\u3001\u79d1\u5b66\u63a8\u7406\u3001\u7f16\u7a0b\u3001\u7b97\u6cd5\u95ee\u9898\u89e3\u51b3\u7b49\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u4e00\u4e9b\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u63a5\u8fd1DeepSeek-R1\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6570\u636e\u7cbe\u9009\u548c\u76d1\u7763\u5fae\u8c03\u5bf9\u63a8\u7406\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5f3a\u5316\u5b66\u4e60\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u8bc4\u4f30\u63a8\u7406\u6a21\u578b\u6027\u80fd\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.21248", "pdf": "https://arxiv.org/pdf/2504.21248", "abs": "https://arxiv.org/abs/2504.21248", "authors": ["Ezra Engel", "Lishan Li", "Chris Hudy", "Robert Schleusner"], "title": "Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Facial expression recognition (FER) is a subset of computer vision with\nimportant applications for human-computer-interaction, healthcare, and customer\nservice. FER represents a challenging problem-space because accurate\nclassification requires a model to differentiate between subtle changes in\nfacial features. In this paper, we examine the use of multi-modal transfer\nlearning to improve performance on a challenging video-based FER dataset,\nDynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrained\nResNets, OpenPose, and OmniVec networks, we explore the impact of\ncross-temporal, multi-modal features on classification accuracy. Ultimately, we\nfind that these finely-tuned multi-modal feature generators modestly improve\naccuracy of our transformer-based classification model.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\u5728\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08DFEW\u6570\u636e\u96c6\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\uff08ResNets\u3001OpenPose\u3001OmniVec\uff09\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u5728\u591a\u4e2a\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u9762\u90e8\u7279\u5f81\u7684\u5fae\u5999\u53d8\u5316\uff0c\u51c6\u786e\u5206\u7c7b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684ResNets\u3001OpenPose\u548cOmniVec\u7f51\u7edc\uff0c\u63a2\u7d22\u8de8\u65f6\u7a7a\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u5206\u7c7b\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7ecf\u8fc7\u7cbe\u7ec6\u8c03\u6574\u7684\u591a\u6a21\u6001\u7279\u5f81\u751f\u6210\u5668\u7565\u5fae\u63d0\u5347\u4e86\u57fa\u4e8eTransformer\u7684\u5206\u7c7b\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\u5bf9\u63d0\u5347FER\u4efb\u52a1\u7684\u5206\u7c7b\u51c6\u786e\u6027\u5177\u6709\u6f5c\u5728\u4ef7\u503c\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2504.21347", "pdf": "https://arxiv.org/pdf/2504.21347", "abs": "https://arxiv.org/abs/2504.21347", "authors": ["Seonghee Lee", "Denae Ford", "John Tang", "Sasa Junuzovic", "Asta Roseway", "Ed Cutrell", "Kori Inkpen"], "title": "IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces", "categories": ["cs.AI", "cs.HC", "H.5.2; I.2.9"], "comment": "8 pages, 3 figures", "summary": "We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent\ndesigned to represent remote colleagues in shared office spaces, creating\nopportunities for real-time exchanges even in their absence. IRL Ditto offers a\nunique hybrid experience by allowing in-person colleagues to encounter a\ndigital version of their remote teammates, initiating greetings, updates, or\nsmall talk as they might in person. Our research question examines: How can the\nIRL Ditto influence interactions and relationships among colleagues in a shared\noffice space? Through a four-day study, we assessed IRL Ditto's ability to\nstrengthen social ties by simulating presence and enabling meaningful\ninteractions across different levels of social familiarity. We find that\nenhancing social relationships depended deeply on the foundation of the\nrelationship participants had with the source of the IRL Ditto. This study\nprovides insights into the role of embodied agents in enriching workplace\ndynamics for distributed teams.", "AI": {"tldr": "IRL Ditto\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u5b9e\u4f53\u4ee3\u7406\uff0c\u7528\u4e8e\u5728\u5171\u4eab\u529e\u516c\u7a7a\u95f4\u4e2d\u4ee3\u8868\u8fdc\u7a0b\u540c\u4e8b\uff0c\u4fc3\u8fdb\u5b9e\u65f6\u4ea4\u6d41\u3002\u7814\u7a76\u53d1\u73b0\u5176\u589e\u5f3a\u793e\u4ea4\u5173\u7cfb\u7684\u80fd\u529b\u53d6\u51b3\u4e8e\u7528\u6237\u4e0e\u4ee3\u7406\u6e90\u7684\u5173\u7cfb\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7IRL Ditto\u8fd9\u79cd\u5b9e\u4f53\u4ee3\u7406\u589e\u5f3a\u5206\u5e03\u5f0f\u56e2\u961f\u4e2d\u7684\u793e\u4ea4\u4e92\u52a8\u548c\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u4e3a\u671f\u56db\u5929\u7684\u7814\u7a76\uff0c\u8bc4\u4f30IRL Ditto\u5728\u6a21\u62df\u5b58\u5728\u548c\u4fc3\u8fdb\u4e0d\u540c\u793e\u4ea4\u719f\u6089\u5ea6\u4e0b\u7684\u4e92\u52a8\u6548\u679c\u3002", "result": "IRL Ditto\u589e\u5f3a\u793e\u4ea4\u5173\u7cfb\u7684\u80fd\u529b\u4e0e\u7528\u6237\u4e0e\u4ee3\u7406\u6e90\u7684\u5173\u7cfb\u57fa\u7840\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5b9e\u4f53\u4ee3\u7406\u5982IRL Ditto\u53ef\u4ee5\u4e30\u5bcc\u5206\u5e03\u5f0f\u56e2\u961f\u7684\u804c\u573a\u52a8\u6001\uff0c\u4f46\u5176\u6548\u679c\u4f9d\u8d56\u4e8e\u73b0\u6709\u5173\u7cfb\u57fa\u7840\u3002"}}
{"id": "2504.21263", "pdf": "https://arxiv.org/pdf/2504.21263", "abs": "https://arxiv.org/abs/2504.21263", "authors": ["Jinpeng Wang", "Tianci Luo", "Yaohua Zha", "Yan Feng", "Ruisheng Luo", "Bin Chen", "Tao Dai", "Long Chen", "Yaowei Wang", "Shu-Tao Xia"], "title": "Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted by CVPR'25. 10 pages, 5 figures, 6 tables", "summary": "Visual In-Context Learning (VICL) enables adaptively solving vision tasks by\nleveraging pixel demonstrations, mimicking human-like task completion through\nanalogy. Prompt selection is critical in VICL, but current methods assume the\nexistence of a single \"ideal\" prompt in a pool of candidates, which in practice\nmay not hold true. Multiple suitable prompts may exist, but individually they\noften fall short, leading to difficulties in selection and the exclusion of\nuseful context. To address this, we propose a new perspective: prompt\ncondensation. Rather than relying on a single prompt, candidate prompts\ncollaborate to efficiently integrate informative contexts without sacrificing\nresolution. We devise Condenser, a lightweight external plugin that compresses\nrelevant fine-grained context across multiple prompts. Optimized end-to-end\nwith the backbone, Condenser ensures accurate integration of contextual cues.\nExperiments demonstrate Condenser outperforms state-of-the-arts across\nbenchmark tasks, showing superior context compression, scalability with more\nprompts, and enhanced computational efficiency compared to ensemble methods,\npositioning it as a highly competitive solution for VICL. Code is open-sourced\nat https://github.com/gimpong/CVPR25-Condenser.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCondenser\u7684\u8f7b\u91cf\u7ea7\u63d2\u4ef6\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u534f\u540c\u6574\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08VICL\uff09\u4e2d\u5355\u4e00\u63d0\u793a\u5047\u8bbe\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524dVICL\u65b9\u6cd5\u5047\u8bbe\u5b58\u5728\u5355\u4e00\u201c\u7406\u60f3\u201d\u63d0\u793a\uff0c\u4f46\u5b9e\u9645\u4e2d\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u5408\u9002\u63d0\u793a\uff0c\u5355\u72ec\u4f7f\u7528\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u63d0\u793a\u538b\u7f29\uff08prompt condensation\uff09\u65b9\u6cd5\uff0c\u8bbe\u8ba1Condenser\u63d2\u4ef6\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u534f\u4f5c\u6574\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCondenser\u5728\u57fa\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u80fd\u529b\u3001\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Condenser\u4e3aVICL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.21370", "pdf": "https://arxiv.org/pdf/2504.21370", "abs": "https://arxiv.org/abs/2504.21370", "authors": ["Jingyang Yi", "Jiazheng Wang"], "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning", "categories": ["cs.AI"], "comment": "An appendix will be uploaded soon", "summary": "Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong\nperformance on reasoning-intensive tasks through extended Chain-of-Thought\n(CoT) prompting. While longer reasoning traces can facilitate a more thorough\nexploration of solution paths for complex problems, researchers have observed\nthat these models often \"overthink\", leading to inefficient inference. In this\npaper, we introduce ShorterBetter, a simple yet effective reinforcement\nlearning methed that enables reasoning language models to discover their own\noptimal CoT lengths without human intervention. By sampling multiple outputs\nper problem and defining the Sample Optimal Length (SOL) as the shortest\ncorrect response among all the outputs, our method dynamically guides the model\ntoward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B\nmodel, ShorterBetter achieves up to an 80% reduction in output length on both\nin-domain and out-of-domain reasoning tasks while maintaining accuracy. Our\nanalysis shows that overly long reasoning traces often reflect loss of\nreasoning direction, and thus suggests that the extended CoT produced by\nreasoning models is highly compressible.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faShorterBetter\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9\u63a8\u7406\u6a21\u578b\u81ea\u52a8\u627e\u5230\u6700\u4f18\u7684Chain-of-Thought\u957f\u5ea6\uff0c\u51cf\u5c11\u63a8\u7406\u957f\u5ea680%\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u957fChain-of-Thought\uff08CoT\uff09\u63d0\u793a\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8fc7\u957f\u63a8\u7406\u4f1a\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9a\u4e49Sample Optimal Length\uff08SOL\uff09\u4e3a\u6700\u77ed\u6b63\u786e\u8f93\u51fa\uff0c\u52a8\u6001\u5f15\u5bfc\u6a21\u578b\u4f18\u5316\u63a8\u7406\u957f\u5ea6\u3002", "result": "\u5728DeepSeek-Distill-Qwen-1.5B\u6a21\u578b\u4e0a\uff0cShorterBetter\u5c06\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1180%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u8fc7\u957f\u63a8\u7406\u5e38\u5bfc\u81f4\u65b9\u5411\u8ff7\u5931\uff0c\u8868\u660e\u63a8\u7406\u6a21\u578b\u7684CoT\u5177\u6709\u9ad8\u5ea6\u53ef\u538b\u7f29\u6027\u3002"}}
{"id": "2504.21266", "pdf": "https://arxiv.org/pdf/2504.21266", "abs": "https://arxiv.org/abs/2504.21266", "authors": ["Zhifu Zhao", "Hanyang Hua", "Jianan Li", "Shaoxin Wu", "Fu Li", "Yangtao Zhou", "Yang Li"], "title": "CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "In action recognition tasks, feature diversity is essential for enhancing\nmodel generalization and performance. Existing methods typically promote\nfeature diversity by expanding the training data in the sample space, which\noften leads to inefficiencies and semantic inconsistencies. To overcome these\nproblems, we propose a novel Coarse-fine text co-guidance Diffusion model\n(CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in\nthe latent space by leveraging diffusion and multi-granularity textual\nguidance. Specifically, our approach feeds spatio-temporal features extracted\nfrom skeleton sequences into a latent diffusion model to generate diverse\naction representations. Meanwhile, we introduce a coarse-fine text co-guided\nstrategy that leverages textual information from large language models (LLMs)\nto ensure semantic consistency between the generated features and the original\ninputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module\nduring training, incurring no additional inference cost. Extensive experiments\ndemonstrate that CoCoDiff achieves SOTA performance on skeleton-based action\nrecognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and\nKinetics-Skeleton.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoCoDiff\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u591a\u7c92\u5ea6\u6587\u672c\u5f15\u5bfc\u751f\u6210\u591a\u6837\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u7279\u5f81\uff0c\u63d0\u5347\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6837\u672c\u7a7a\u95f4\u6269\u5c55\u7279\u5f81\u591a\u6837\u6027\uff0c\u4f46\u6548\u7387\u4f4e\u4e14\u8bed\u4e49\u4e0d\u4e00\u81f4\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u52a8\u4f5c\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u7c97-\u7ec6\u7c92\u5ea6\u6587\u672c\u5f15\u5bfc\u7b56\u7565\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "CoCoDiff\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u8f85\u52a9\u6a21\u5757\uff0c\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.21433", "pdf": "https://arxiv.org/pdf/2504.21433", "abs": "https://arxiv.org/abs/2504.21433", "authors": ["Zhicong Li", "Hangyu Mao", "Jiangjin Yin", "Mingzhe Xing", "Zhiwei Xu", "Yuanxing Zhang", "Yang Xiao"], "title": "NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "This paper argues that the next generation of AI agent (NGENT) should\nintegrate across-domain abilities to advance toward Artificial General\nIntelligence (AGI). Although current AI agents are effective in specialized\ntasks such as robotics, role-playing, and tool-using, they remain confined to\nnarrow domains. We propose that future AI agents should synthesize the\nstrengths of these specialized systems into a unified framework capable of\noperating across text, vision, robotics, reinforcement learning, emotional\nintelligence, and beyond. This integration is not only feasible but also\nessential for achieving the versatility and adaptability that characterize\nhuman intelligence. The convergence of technologies across AI domains, coupled\nwith increasing user demand for cross-domain capabilities, suggests that such\nintegration is within reach. Ultimately, the development of these versatile\nagents is a critical step toward realizing AGI. This paper explores the\nrationale for this shift, potential pathways for achieving it.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u4e0b\u4e00\u4ee3AI\u4ee3\u7406\uff08NGENT\uff09\u5e94\u6574\u5408\u8de8\u9886\u57df\u80fd\u529b\u4ee5\u63a8\u8fdb\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\uff0c\u5e76\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5c40\u9650\u4e8e\u72ed\u7a84\u9886\u57df\uff0c\u7f3a\u4e4f\u8de8\u9886\u57df\u80fd\u529b\uff0c\u800c\u6574\u5408\u591a\u9886\u57df\u6280\u672f\u662f\u5b9e\u73b0AGI\u7684\u5173\u952e\u3002", "method": "\u63d0\u51fa\u5c06\u6587\u672c\u3001\u89c6\u89c9\u3001\u673a\u5668\u4eba\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u60c5\u611f\u667a\u80fd\u7b49\u9886\u57df\u7684\u4f18\u52bf\u6574\u5408\u4e3a\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u8de8\u9886\u57df\u6574\u5408\u4e0d\u4ec5\u53ef\u884c\uff0c\u4e14\u662f\u5b9e\u73b0\u4eba\u7c7b\u667a\u80fd\u822c\u591a\u529f\u80fd\u6027\u548c\u9002\u5e94\u6027\u7684\u5fc5\u8981\u6b65\u9aa4\u3002", "conclusion": "\u5f00\u53d1\u591a\u529f\u80fd\u4ee3\u7406\u662f\u5b9e\u73b0AGI\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u5176\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u73b0\u8def\u5f84\u3002"}}
{"id": "2504.21281", "pdf": "https://arxiv.org/pdf/2504.21281", "abs": "https://arxiv.org/abs/2504.21281", "authors": ["Zexin Ji", "Beiji Zou", "Xiaoyan Kui", "Hua Li", "Pierre Vera", "Su Ruan"], "title": "Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal 3D medical image segmentation aims to accurately identify tumor\nregions across different modalities, facing challenges from variations in image\nintensity and tumor morphology. Traditional convolutional neural network\n(CNN)-based methods struggle with capturing global features, while\nTransformers-based methods, despite effectively capturing global context,\nencounter high computational costs in 3D medical image segmentation. The Mamba\nmodel combines linear scalability with long-distance modeling, making it a\npromising approach for visual representation learning. However, Mamba-based 3D\nmulti-modal segmentation still struggles to leverage modality-specific features\nand fuse complementary information effectively. In this paper, we propose a\nMamba based feature extraction and adaptive multilevel feature fusion for 3D\ntumor segmentation using multi-modal medical image. We first develop the\nspecific modality Mamba encoder to efficiently extract long-range relevant\nfeatures that represent anatomical and pathological structures present in each\nmodality. Moreover, we design an bi-level synergistic integration block that\ndynamically merges multi-modal and multi-level complementary features by the\nmodality attention and channel attention learning. Lastly, the decoder combines\ndeep semantic information with fine-grained details to generate the tumor\nsegmentation map. Experimental results on medical image datasets (PET/CT and\nMRI multi-sequence) show that our approach achieve competitive performance\ncompared to the state-of-the-art CNN, Transformer, and Mamba-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u9002\u5e94\u591a\u7ea7\u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6a21\u60013D\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u80bf\u7624\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCNN\u548cTransformer\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u591a\u6a21\u60013D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u56fe\u50cf\u5f3a\u5ea6\u548c\u80bf\u7624\u5f62\u6001\u53d8\u5316\u7684\u6311\u6218\uff0c\u4f20\u7edfCNN\u96be\u4ee5\u6355\u6349\u5168\u5c40\u7279\u5f81\uff0c\u800cTransformer\u8ba1\u7b97\u6210\u672c\u9ad8\u3002Mamba\u6a21\u578b\u7ed3\u5408\u4e86\u7ebf\u6027\u6269\u5c55\u6027\u548c\u957f\u8ddd\u79bb\u5efa\u6a21\u80fd\u529b\uff0c\u4f46\u5728\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u7279\u5b9a\u6a21\u6001\u7684Mamba\u7f16\u7801\u5668\u63d0\u53d6\u957f\u7a0b\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86\u53cc\u7ea7\u534f\u540c\u96c6\u6210\u5757\uff0c\u901a\u8fc7\u6a21\u6001\u6ce8\u610f\u529b\u548c\u901a\u9053\u6ce8\u610f\u529b\u52a8\u6001\u878d\u5408\u591a\u6a21\u6001\u548c\u591a\u7ea7\u7279\u5f81\u3002\u89e3\u7801\u5668\u7ed3\u5408\u6df1\u5c42\u8bed\u4e49\u4fe1\u606f\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u751f\u6210\u5206\u5272\u56fe\u3002", "result": "\u5728PET/CT\u548cMRI\u591a\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684CNN\u3001Transformer\u548cMamba\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u60013D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u80bf\u7624\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21568", "pdf": "https://arxiv.org/pdf/2504.21568", "abs": "https://arxiv.org/abs/2504.21568", "authors": ["Shui-jin Rong", "Wei Guo", "Da-qing Zhang"], "title": "A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks", "categories": ["cs.AI"], "comment": null, "summary": "Aiming at the group decision - making problem with multi - objective\nattributes, this study proposes a group decision - making system that\nintegrates fuzzy inference and Bayesian network. A fuzzy rule base is\nconstructed by combining threshold values, membership functions, expert\nexperience, and domain knowledge to address quantitative challenges such as\nscale differences and expert linguistic variables. A hierarchical Bayesian\nnetwork is designed, featuring a directed acyclic graph with nodes selected by\nexperts, and maximum likelihood estimation is used to dynamically optimize the\nconditional probability table, modeling the nonlinear correlations among\nmultidimensional indices for posterior probability aggregation. In a\ncomprehensive student evaluation case, this method is compared with the\ntraditional weighted scoring approach. The results indicate that the proposed\nmethod demonstrates effectiveness in both rule criterion construction and\nranking consistency, with a classification accuracy of 86.0% and an F1 value\nimprovement of 53.4% over the traditional method. Additionally, computational\nexperiments on real - world datasets across various group decision scenarios\nassess the method's performance and robustness, providing evidence of its\nreliability in diverse contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u7cca\u63a8\u7406\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u7fa4\u51b3\u7b56\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u76ee\u6807\u5c5e\u6027\u7684\u7fa4\u51b3\u7b56\u95ee\u9898\uff0c\u5e76\u5728\u5b66\u751f\u8bc4\u4ef7\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u591a\u76ee\u6807\u5c5e\u6027\u7684\u7fa4\u51b3\u7b56\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5b9a\u91cf\u6311\u6218\uff08\u5982\u5c3a\u5ea6\u5dee\u5f02\u548c\u4e13\u5bb6\u8bed\u8a00\u53d8\u91cf\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u6a21\u7cca\u89c4\u5219\u5e93\uff0c\u8bbe\u8ba1\u5206\u5c42\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u52a8\u6001\u4f18\u5316\u6761\u4ef6\u6982\u7387\u8868\uff0c\u5efa\u6a21\u591a\u7ef4\u6307\u6807\u7684\u975e\u7ebf\u6027\u76f8\u5173\u6027\u3002", "result": "\u5728\u5b66\u751f\u8bc4\u4ef7\u6848\u4f8b\u4e2d\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe86.0%\uff0cF1\u503c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u9ad853.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c4\u5219\u6784\u5efa\u548c\u6392\u5e8f\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u4e0d\u540c\u7fa4\u51b3\u7b56\u573a\u666f\u4e2d\u5177\u6709\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.21292", "pdf": "https://arxiv.org/pdf/2504.21292", "abs": "https://arxiv.org/abs/2504.21292", "authors": ["ZiYi Dong", "Chengxing Zhou", "Weijian Deng", "Pengxu Wei", "Xiangyang Ji", "Liang Lin"], "title": "Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions", "categories": ["cs.CV"], "comment": null, "summary": "Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT)\narchitectures have revolutionized image generation through transformer-based\nattention mechanisms. The prevailing paradigm has commonly employed\nself-attention with quadratic computational complexity to handle global spatial\nrelationships in complex images, thereby synthesizing high-fidelity images with\ncoherent visual semantics.Contrary to conventional wisdom, our systematic\nlayer-wise analysis reveals an interesting discrepancy: self-attention in\npre-trained diffusion models predominantly exhibits localized attention\npatterns, closely resembling convolutional inductive biases. This suggests that\nglobal interactions in self-attention may be less critical than commonly\nassumed.Driven by this, we propose \\(\\Delta\\)ConvFusion to replace conventional\nself-attention modules with Pyramid Convolution Blocks\n(\\(\\Delta\\)ConvBlocks).By distilling attention patterns into localized\nconvolutional operations while keeping other components frozen,\n\\(\\Delta\\)ConvFusion achieves performance comparable to transformer-based\ncounterparts while reducing computational cost by 6929$\\times$ and surpassing\nLinFusion by 5.42$\\times$ in efficiency--all without compromising generative\nfidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u0394ConvFusion\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u91d1\u5b57\u5854\u5377\u79ef\u5757\uff08\u0394ConvBlocks\uff09\u66ff\u4ee3\u4f20\u7edf\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e3b\u8981\u8868\u73b0\u51fa\u5c40\u90e8\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u7c7b\u4f3c\u4e8e\u5377\u79ef\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u8868\u660e\u5168\u5c40\u4ea4\u4e92\u53ef\u80fd\u4e0d\u5982\u901a\u5e38\u5047\u8bbe\u7684\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u6a21\u5f0f\u84b8\u998f\u5230\u5c40\u90e8\u5377\u79ef\u64cd\u4f5c\u4e2d\uff0c\u540c\u65f6\u51bb\u7ed3\u5176\u4ed6\u7ec4\u4ef6\uff0c\u63d0\u51fa\u4e86\u0394ConvFusion\u65b9\u6cd5\u3002", "result": "\u0394ConvFusion\u5728\u8ba1\u7b97\u6210\u672c\u4e0a\u964d\u4f4e\u4e866929\u500d\uff0c\u6548\u7387\u8d85\u8fc7LinFusion 5.42\u500d\uff0c\u4e14\u751f\u6210\u8d28\u91cf\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "\u5c40\u90e8\u5377\u79ef\u64cd\u4f5c\u53ef\u4ee5\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2504.21643", "pdf": "https://arxiv.org/pdf/2504.21643", "abs": "https://arxiv.org/abs/2504.21643", "authors": ["Luca Marzari", "Francesco Trotti", "Enrico Marchesini", "Alessandro Farinelli"], "title": "Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Achieving safe autonomous navigation systems is critical for deploying robots\nin dynamic and uncertain real-world environments. In this paper, we propose a\nhierarchical control framework leveraging neural network verification\ntechniques to design control barrier functions (CBFs) and policy correction\nmechanisms that ensure safe reinforcement learning navigation policies. Our\napproach relies on probabilistic enumeration to identify unsafe regions of\noperation, which are then used to construct a safe CBF-based control layer\napplicable to arbitrary policies. We validate our framework both in simulation\nand on a real robot, using a standard mobile robot benchmark and a highly\ndynamic aquatic environmental monitoring task. These experiments demonstrate\nthe ability of the proposed solution to correct unsafe actions while preserving\nefficient navigation behavior. Our results show the promise of developing\nhierarchical verification-based systems to enable safe and robust navigation\nbehaviors in complex scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u7684\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u786e\u4fdd\u5f3a\u5316\u5b66\u4e60\u5bfc\u822a\u7b56\u7565\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u6982\u7387\u679a\u4e3e\u8bc6\u522b\u4e0d\u5b89\u5168\u64cd\u4f5c\u533a\u57df\uff0c\u6784\u5efa\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBF\uff09\u7684\u5b89\u5168\u63a7\u5236\u5c42\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u7ea0\u6b63\u4e0d\u5b89\u5168\u884c\u4e3a\u5e76\u4fdd\u6301\u9ad8\u6548\u5bfc\u822a\u3002", "conclusion": "\u5206\u5c42\u9a8c\u8bc1\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u9c81\u68d2\u7684\u5bfc\u822a\u884c\u4e3a\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2504.21294", "pdf": "https://arxiv.org/pdf/2504.21294", "abs": "https://arxiv.org/abs/2504.21294", "authors": ["Qianzi Yu", "Yang Cao", "Yu Kang"], "title": "Learning Multi-view Multi-class Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "The latest trend in anomaly detection is to train a unified model instead of\ntraining a separate model for each category. However, existing multi-class\nanomaly detection (MCAD) models perform poorly in multi-view scenarios because\nthey often fail to effectively model the relationships and complementary\ninformation among different views. In this paper, we introduce a Multi-View\nMulti-Class Anomaly Detection model (MVMCAD), which integrates information from\nmultiple views to accurately identify anomalies. Specifically, we propose a\nsemi-frozen encoder, where a pre-encoder prior enhancement mechanism is added\nbefore the frozen encoder, enabling stable cross-view feature modeling and\nefficient adaptation for improved anomaly detection. Furthermore, we propose an\nAnomaly Amplification Module (AAM) that models global token interactions and\nsuppresses normal regions to enhance anomaly signals, leading to improved\ndetection performance in multi-view settings. Finally, we propose a\nCross-Feature Loss that aligns shallow encoder features with deep decoder\nfeatures and vice versa, enhancing the model's sensitivity to anomalies at\ndifferent semantic levels under multi-view scenarios. Extensive experiments on\nthe Real-IAD dataset for multi-view multi-class anomaly detection validate the\neffectiveness of our approach, achieving state-of-the-art performance of\n91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level,\nrespectively.", "AI": {"tldr": "MVMCAD\u6a21\u578b\u901a\u8fc7\u591a\u89c6\u56fe\u4fe1\u606f\u6574\u5408\u548c\u5f02\u5e38\u4fe1\u53f7\u589e\u5f3a\uff0c\u5728\u591a\u89c6\u56fe\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MCAD\u6a21\u578b\u5728\u591a\u89c6\u56fe\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u672a\u80fd\u6709\u6548\u5efa\u6a21\u89c6\u56fe\u95f4\u5173\u7cfb\u548c\u4e92\u8865\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u534a\u51bb\u7ed3\u7f16\u7801\u5668\u3001\u5f02\u5e38\u653e\u5927\u6a21\u5757\uff08AAM\uff09\u548c\u8de8\u7279\u5f81\u635f\u5931\uff0c\u4ee5\u589e\u5f3a\u591a\u89c6\u56fe\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5728Real-IAD\u6570\u636e\u96c6\u4e0a\uff0c\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7\u68c0\u6d4b\u6027\u80fd\u5206\u522b\u8fbe\u523091.0/88.6/82.1\u548c99.1/43.9/48.2/95.2\u3002", "conclusion": "MVMCAD\u5728\u591a\u89c6\u56fe\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.21659", "pdf": "https://arxiv.org/pdf/2504.21659", "abs": "https://arxiv.org/abs/2504.21659", "authors": ["Haotian Luo", "Haiying He", "Yibo Wang", "Jinluan Yang", "Rui Liu", "Naiqiang Tan", "Xiaochun Cao", "Dacheng Tao", "Li Shen"], "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u957f\u77ed\u63a8\u7406\u6a21\u578b\u548c\u53cc\u5c42\u504f\u597d\u8bad\u7ec3\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u957f\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u5f00\u9500\u5927\uff0c\u4e14\u4e0d\u540c\u95ee\u9898\u5bf9\u957f\u63a8\u7406\u7684\u9700\u6c42\u5dee\u5f02\u663e\u8457\uff0c\u9700\u8981\u81ea\u9002\u5e94\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u5408\u5e76\u957f\u77ed\u63a8\u7406\u6a21\u578b\u4ee5\u652f\u6301\u591a\u6837\u63a8\u7406\u98ce\u683c\uff1b2) \u901a\u8fc7\u53cc\u5c42\u504f\u597d\u8bad\u7ec3\u9009\u62e9\u5408\u9002\u63a8\u7406\u98ce\u683c\u5e76\u4f18\u5316\u63a8\u7406\u7b80\u6d01\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63a8\u7406\u957f\u5ea6\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u81ea\u9002\u5e94\u7b56\u7565\u80fd\u6709\u6548\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.21302", "pdf": "https://arxiv.org/pdf/2504.21302", "abs": "https://arxiv.org/abs/2504.21302", "authors": ["Zhelun Shen", "Zhuo Li", "Chenming Wu", "Zhibo Rao", "Lina Liu", "Yuchao Dai", "Liangjun Zhang"], "title": "CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching", "categories": ["cs.CV", "cs.RO"], "comment": "13 pages, 5 figures, accepted for publication in Pattern Recognition", "summary": "Recently, learning-based stereo matching methods have achieved great\nimprovement in public benchmarks, where soft argmin and smooth L1 loss play a\ncore contribution to their success. However, in unsupervised domain adaptation\nscenarios, we observe that these two operations often yield multimodal\ndisparity probability distributions in target domains, resulting in degraded\ngeneralization. In this paper, we propose a novel approach, Constrain\nMulti-modal Distribution (CMD), to address this issue. Specifically, we\nintroduce \\textit{uncertainty-regularized minimization} and \\textit{anisotropic\nsoft argmin} to encourage the network to produce predominantly unimodal\ndisparity distributions in the target domain, thereby improving prediction\naccuracy. Experimentally, we apply the proposed method to multiple\nrepresentative stereo-matching networks and conduct domain adaptation from\nsynthetic data to unlabeled real-world scenes. Results consistently demonstrate\nimproved generalization in both top-performing and domain-adaptable\nstereo-matching models. The code for CMD will be available at:\n\\href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMD\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u591a\u6a21\u6001\u5206\u5e03\u6765\u6539\u5584\u65e0\u76d1\u7763\u57df\u9002\u5e94\u573a\u666f\u4e2d\u7684\u7acb\u4f53\u5339\u914d\u6027\u80fd\u3002", "motivation": "\u5728\u65e0\u76d1\u7763\u57df\u9002\u5e94\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u7684soft argmin\u548c\u5e73\u6ed1L1\u635f\u5931\u4f1a\u5bfc\u81f4\u591a\u6a21\u6001\u89c6\u5dee\u5206\u5e03\uff0c\u4ece\u800c\u964d\u4f4e\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u6b63\u5219\u5316\u6700\u5c0f\u5316\u548c\u5404\u5411\u5f02\u6027soft argmin\uff0c\u4ee5\u9f13\u52b1\u7f51\u7edc\u5728\u76ee\u6807\u57df\u4e2d\u751f\u6210\u5355\u6a21\u6001\u89c6\u5dee\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4ee3\u8868\u6027\u7acb\u4f53\u5339\u914d\u7f51\u7edc\u4e2d\u5747\u80fd\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "CMD\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57df\u9002\u5e94\u4e2d\u7684\u591a\u6a21\u6001\u5206\u5e03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2504.21683", "pdf": "https://arxiv.org/pdf/2504.21683", "abs": "https://arxiv.org/abs/2504.21683", "authors": ["Kenneth Skiba", "Tjitze Rienstra", "Matthias Thimm", "Jesse Heyninck", "Gabriele Kern-Isberner"], "title": "Extension-ranking Semantics for Abstract Argumentation Preprint", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, we present a general framework for ranking sets of arguments\nin abstract argumentation based on their plausibility of acceptance. We present\na generalisation of Dung's extension semantics as extension-ranking semantics,\nwhich induce a preorder over the power set of all arguments, allowing us to\nstate that one set is \"closer\" to being acceptable than another. To evaluate\nthe extension-ranking semantics, we introduce a number of principles that a\nwell-behaved extension-ranking semantics should satisfy. We consider several\nsimple base relations, each of which models a single central aspect of\nargumentative reasoning. The combination of these base relations provides us\nwith a family of extension-ranking semantics. We also adapt a number of\napproaches from the literature for ranking extensions to be usable in the\ncontext of extension-ranking semantics, and evaluate their behaviour.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bba\u8bc1\u53ef\u63a5\u53d7\u6027\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u62bd\u8c61\u8bba\u8bc1\u4e2d\u7684\u8bba\u8bc1\u96c6\u8fdb\u884c\u6392\u5e8f\uff0c\u6269\u5c55\u4e86Dung\u7684\u6269\u5c55\u8bed\u4e49\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u7cfb\u5217\u539f\u5219\u6765\u8bc4\u4f30\u6269\u5c55\u6392\u5e8f\u8bed\u4e49\u3002", "motivation": "\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6bd4\u8f83\u8bba\u8bc1\u96c6\u7684\u53ef\u63a5\u53d7\u6027\uff0c\u4ece\u800c\u66f4\u7075\u6d3b\u5730\u8bc4\u4f30\u8bba\u8bc1\u7684\u5408\u7406\u6027\u3002", "method": "\u901a\u8fc7\u6269\u5c55Dung\u7684\u6269\u5c55\u8bed\u4e49\u4e3a\u6269\u5c55\u6392\u5e8f\u8bed\u4e49\uff0c\u5f15\u5165\u591a\u4e2a\u57fa\u7840\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408\u5b83\u4eec\u751f\u6210\u4e00\u7cfb\u5217\u6269\u5c55\u6392\u5e8f\u8bed\u4e49\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bb6\u65cf\u5f0f\u7684\u6269\u5c55\u6392\u5e8f\u8bed\u4e49\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u884c\u4e3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bba\u8bc1\u96c6\u7684\u6392\u5e8f\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u6ee1\u8db3\u4e00\u7cfb\u5217\u884c\u4e3a\u539f\u5219\u3002"}}
{"id": "2504.21307", "pdf": "https://arxiv.org/pdf/2504.21307", "abs": "https://arxiv.org/abs/2504.21307", "authors": ["Siyi Chen", "Yimeng Zhang", "Sijia Liu", "Qing Qu"], "title": "The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable generalization capabilities of diffusion models,\nrecent studies have shown that these models can memorize and generate harmful\ncontent when prompted with specific text instructions. Although fine-tuning\napproaches have been developed to mitigate this issue by unlearning harmful\nconcepts, these methods can be easily circumvented through jailbreaking\nattacks. This indicates that the harmful concept has not been fully erased from\nthe model. However, existing attack methods, while effective, lack\ninterpretability regarding why unlearned models still retain the concept,\nthereby hindering the development of defense strategies. In this work, we\naddress these limitations by proposing an attack method that learns an\northogonal set of interpretable attack token embeddings. The attack token\nembeddings can be decomposed into human-interpretable textual elements,\nrevealing that unlearned models still retain the target concept through\nimplicit textual components. Furthermore, these attack token embeddings are\nrobust and transferable across text prompts, initial noises, and unlearned\nmodels. Finally, leveraging this diverse set of embeddings, we design a defense\nmethod applicable to both our proposed attack and existing attack methods.\nExperimental results demonstrate the effectiveness of both our attack and\ndefense strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u653b\u51fb\u4ee4\u724c\u5d4c\u5165\u63ed\u793a\u672a\u5b66\u4e60\u6a21\u578b\u4e2d\u4ecd\u4fdd\u7559\u6709\u5bb3\u6982\u5ff5\u7684\u539f\u56e0\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u53ef\u80fd\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u8fd9\u4e9b\u6982\u5ff5\uff0c\u4e14\u653b\u51fb\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b66\u4e60\u6b63\u4ea4\u53ef\u89e3\u91ca\u653b\u51fb\u4ee4\u724c\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u5206\u89e3\u4e3a\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u6587\u672c\u5143\u7d20\u3002", "result": "\u653b\u51fb\u4ee4\u724c\u5d4c\u5165\u63ed\u793a\u4e86\u672a\u5b66\u4e60\u6a21\u578b\u4e2d\u4ecd\u4fdd\u7559\u6709\u5bb3\u6982\u5ff5\uff0c\u4e14\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u653b\u51fb\u548c\u9632\u5fa1\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.21694", "pdf": "https://arxiv.org/pdf/2504.21694", "abs": "https://arxiv.org/abs/2504.21694", "authors": ["Tom Westermann", "Malte Ramonat", "Johannes Hujer", "Felix Gehlhoff", "Alexander Fay"], "title": "Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation", "categories": ["cs.AI"], "comment": null, "summary": "AutomationML has seen widespread adoption as an open data exchange format in\nthe automation domain. It is an open and vendor neutral standard based on the\nextensible markup language XML. However, AutomationML extends XML with\nadditional semantics, that limit the applicability of common XML-tools for\napplications like querying or data validation. This article provides\npractitioners with 1) an up-to-date ontology of the concepts in the\nAutomationML-standard, as well as 2) a declarative mapping to automatically\ntransform any AutomationML model into RDF triples. Together, these artifacts\nallow practitioners an easy integration of AutomationML information into\nindustrial knowledge graphs. A study on examples from the automation domain\nconcludes that transforming AutomationML to OWL opens up new powerful ways for\nquerying and validation that are impossible without transformation.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86AutomationML\u6807\u51c6\u7684\u6700\u65b0\u672c\u4f53\u8bba\u548cRDF\u8f6c\u6362\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u67e5\u8be2\u548c\u9a8c\u8bc1\u80fd\u529b\u3002", "motivation": "AutomationML\u4f5c\u4e3a\u81ea\u52a8\u5316\u9886\u57df\u7684\u6570\u636e\u4ea4\u6362\u683c\u5f0f\uff0c\u5176\u6269\u5c55\u8bed\u4e49\u9650\u5236\u4e86\u901a\u7528XML\u5de5\u5177\u7684\u9002\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u67e5\u8be2\u548c\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86AutomationML\u7684\u672c\u4f53\u8bba\u548cRDF\u8f6c\u6362\u6620\u5c04\uff0c\u652f\u6301\u5c06AutomationML\u6a21\u578b\u81ea\u52a8\u8f6c\u6362\u4e3aRDF\u4e09\u5143\u7ec4\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8f6c\u6362\u4e3aOWL\u540e\uff0c\u67e5\u8be2\u548c\u9a8c\u8bc1\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u672c\u4f53\u8bba\u548cRDF\u8f6c\u6362\uff0cAutomationML\u80fd\u66f4\u9ad8\u6548\u5730\u96c6\u6210\u5230\u5de5\u4e1a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u3002"}}
{"id": "2504.21308", "pdf": "https://arxiv.org/pdf/2504.21308", "abs": "https://arxiv.org/abs/2504.21308", "authors": ["Yunhao Li", "Sijing Wu", "Wei Sun", "Zhichao Zhang", "Yucheng Zhu", "Zicheng Zhang", "Huiyu Duan", "Xiongkuo Min", "Guangtao Zhai"], "title": "AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of text-to-image (T2I) generation approaches has\nattracted extensive interest in evaluating the quality of generated images,\nleading to the development of various quality assessment methods for\ngeneral-purpose T2I outputs. However, existing image quality assessment (IQA)\nmethods are limited to providing global quality scores, failing to deliver\nfine-grained perceptual evaluations for structurally complex subjects like\nhumans, which is a critical challenge considering the frequent anatomical and\ntextural distortions in AI-generated human images (AGHIs). To address this gap,\nwe introduce AGHI-QA, the first large-scale benchmark specifically designed for\nquality assessment of AGHIs. The dataset comprises 4,000 images generated from\n400 carefully crafted text prompts using 10 state of-the-art T2I models. We\nconduct a systematic subjective study to collect multidimensional annotations,\nincluding perceptual quality scores, text-image correspondence scores, visible\nand distorted body part labels. Based on AGHI-QA, we evaluate the strengths and\nweaknesses of current T2I methods in generating human images from multiple\ndimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that\nintegrates the large multimodal model (LMM) with domain-specific human features\nfor precise quality prediction and identification of visible and distorted body\nparts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor\nshowcases state-of-the-art performance, significantly outperforming existing\nIQA methods in multidimensional quality assessment and surpassing leading LMMs\nin detecting structural distortions in AGHIs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AGHI-QA\u57fa\u51c6\u548cAGHI-Assessor\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u8bc4\u4f30AI\u751f\u6210\u4eba\u7c7b\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709IQA\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u5168\u5c40\u8bc4\u5206\uff0c\u65e0\u6cd5\u5bf9\u590d\u6742\u7ed3\u6784\uff08\u5982\u4eba\u4f53\uff09\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u800cAI\u751f\u6210\u7684\u4eba\u7c7b\u56fe\u50cf\u5e38\u5b58\u5728\u89e3\u5256\u548c\u7eb9\u7406\u5931\u771f\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b4000\u5f20\u56fe\u50cf\u7684AGHI-QA\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e3b\u89c2\u7814\u7a76\u6536\u96c6\u591a\u7ef4\u6807\u6ce8\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u5927\u6a21\u6001\u6a21\u578b\u548c\u4eba\u4f53\u7279\u5f81\u7684AGHI-Assessor\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "AGHI-Assessor\u5728\u591a\u7ef4\u8d28\u91cf\u8bc4\u4f30\u548c\u7ed3\u6784\u5931\u771f\u68c0\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709IQA\u65b9\u6cd5\u548c\u9886\u5148\u7684\u5927\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "AGHI-QA\u548cAGHI-Assessor\u4e3aAI\u751f\u6210\u4eba\u7c7b\u56fe\u50cf\u7684\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.21774", "pdf": "https://arxiv.org/pdf/2504.21774", "abs": "https://arxiv.org/abs/2504.21774", "authors": ["Jiuwu Hao", "Liguo Sun", "Yuting Wan", "Yueyang Wu", "Ti Xiang", "Haolin Song", "Pin Lv"], "title": "Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?", "categories": ["cs.AI"], "comment": null, "summary": "Collaborative perception enhances environmental awareness through inter-agent\ncommunication and is regarded as a promising solution to intelligent\ntransportation systems. However, existing collaborative methods for Unmanned\nAerial Vehicles (UAVs) overlook the unique characteristics of the UAV\nperspective, resulting in substantial communication overhead. To address this\nissue, we propose a novel communication-efficient collaborative perception\nframework based on late-intermediate fusion, dubbed LIF. The core concept is to\nexchange informative and compact detection results and shift the fusion stage\nto the feature representation level. In particular, we leverage vision-guided\npositional embedding (VPE) and box-based virtual augmented feature (BoBEV) to\neffectively integrate complementary information from various agents.\nAdditionally, we innovatively introduce an uncertainty-driven communication\nmechanism that uses uncertainty evaluation to select high-quality and reliable\nshared areas. Experimental results demonstrate that our LIF achieves superior\nperformance with minimal communication bandwidth, proving its effectiveness and\npracticality. Code and models are available at https://github.com/uestchjw/LIF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u665a\u671f\u4e2d\u95f4\u878d\u5408\u7684\u901a\u4fe1\u9ad8\u6548\u534f\u4f5c\u611f\u77e5\u6846\u67b6LIF\uff0c\u901a\u8fc7\u4ea4\u6362\u7d27\u51d1\u68c0\u6d4b\u7ed3\u679c\u548c\u7279\u5f81\u7ea7\u878d\u5408\uff0c\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\u5ffd\u89c6\u65e0\u4eba\u673a\u89c6\u89d2\u7279\u6027\uff0c\u5bfc\u81f4\u901a\u4fe1\u5f00\u9500\u5927\u3002", "method": "\u91c7\u7528\u665a\u671f\u4e2d\u95f4\u878d\u5408\u6846\u67b6LIF\uff0c\u7ed3\u5408\u89c6\u89c9\u5f15\u5bfc\u4f4d\u7f6e\u5d4c\u5165\uff08VPE\uff09\u548c\u57fa\u4e8e\u6846\u7684\u865a\u62df\u589e\u5f3a\u7279\u5f81\uff08BoBEV\uff09\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u901a\u4fe1\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLIF\u5728\u6700\u5c0f\u901a\u4fe1\u5e26\u5bbd\u4e0b\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "LIF\u6846\u67b6\u9ad8\u6548\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u534f\u4f5c\u611f\u77e5\u3002"}}
{"id": "2504.21309", "pdf": "https://arxiv.org/pdf/2504.21309", "abs": "https://arxiv.org/abs/2504.21309", "authors": ["Modesto Castrill\u00f3n-Santana", "Oliverio J Santana", "David Freire-Obreg\u00f3n", "Daniel Hern\u00e1ndez-Sosa", "Javier Lorenzo-Navarro"], "title": "An Evaluation of a Visual Question Answering Strategy for Zero-shot Facial Expression Recognition in Still Images", "categories": ["cs.CV", "I.2.10"], "comment": null, "summary": "Facial expression recognition (FER) is a key research area in computer vision\nand human-computer interaction. Despite recent advances in deep learning,\nchallenges persist, especially in generalizing to new scenarios. In fact,\nzero-shot FER significantly reduces the performance of state-of-the-art FER\nmodels. To address this problem, the community has recently started to explore\nthe integration of knowledge from Large Language Models for visual tasks. In\nthis work, we evaluate a broad collection of locally executed Visual Language\nModels (VLMs), avoiding the lack of task-specific knowledge by adopting a\nVisual Question Answering strategy. We compare the proposed pipeline with\nstate-of-the-art FER models, both integrating and excluding VLMs, evaluating\nwell-known FER benchmarks: AffectNet, FERPlus, and RAF-DB. The results show\nexcellent performance for some VLMs in zero-shot FER scenarios, indicating the\nneed for further exploration to improve FER generalization.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u63d0\u5347\u96f6\u6837\u672c\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728FER\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u96f6\u6837\u672cFER\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u95ee\u7b54\u7b56\u7565\uff0c\u8bc4\u4f30\u591a\u79cd\u672c\u5730\u6267\u884c\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u5e76\u4e0e\u73b0\u6709FER\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u90e8\u5206VLMs\u5728\u96f6\u6837\u672cFER\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8868\u660e\u5176\u6f5c\u529b\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22VLMs\u4ee5\u63d0\u5347FER\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.21008", "pdf": "https://arxiv.org/pdf/2504.21008", "abs": "https://arxiv.org/abs/2504.21008", "authors": ["Qiuyan Xiang", "Shuang Wu", "Dongze Wu", "Yuxin Liu", "Zhenkai Qin"], "title": "Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "With the widespread adoption of the Internet of Things (IoT) and Industrial\nIoT (IIoT) technologies, network architectures have become increasingly\ncomplex, and the volume of traffic has grown substantially. This evolution\nposes significant challenges to traditional security mechanisms, particularly\nin detecting high-frequency, diverse, and highly covert network attacks. To\naddress these challenges, this study proposes a novel network traffic anomaly\ndetection model that integrates a Convolutional Neural Network (CNN) with a\nBidirectional Long Short-Term Memory (BiLSTM) network, implemented on the\nMindSpore framework. Comprehensive experiments were conducted using the\nNF-BoT-IoT dataset. The results demonstrate that the proposed model achieves\n99% across accuracy, precision, recall, and F1-score, indicating its strong\nperformance and robustness in network intrusion detection tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548cBiLSTM\u7684\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u5728MindSpore\u6846\u67b6\u4e0a\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740IoT\u548cIIoT\u6280\u672f\u7684\u666e\u53ca\uff0c\u7f51\u7edc\u67b6\u6784\u65e5\u76ca\u590d\u6742\uff0c\u6d41\u91cf\u6fc0\u589e\uff0c\u4f20\u7edf\u5b89\u5168\u673a\u5236\u96be\u4ee5\u5e94\u5bf9\u9ad8\u9891\u3001\u591a\u6837\u4e14\u9690\u853d\u7684\u7f51\u7edc\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210CNN\u548cBiLSTM\u7684\u6a21\u578b\uff0c\u5e76\u5728MindSpore\u6846\u67b6\u4e0a\u5b9e\u73b0\uff0c\u4f7f\u7528NF-BoT-IoT\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u8fbe\u523099%\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.21325", "pdf": "https://arxiv.org/pdf/2504.21325", "abs": "https://arxiv.org/abs/2504.21325", "authors": ["Abdul Sami", "Avinash Kumar", "Irfanullah Memon", "Youngwon Jo", "Muhammad Rizwan", "Jaeyoung Choi"], "title": "Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, Accepted at ICOIN 2025", "summary": "Automatic font generation (AFG) is the process of creating a new font using\nonly a few examples of the style images. Generating fonts for complex languages\nlike Korean and Chinese, particularly in handwritten styles, presents\nsignificant challenges. Traditional AFGs, like Generative adversarial networks\n(GANs) and Variational Auto-Encoders (VAEs), are usually unstable during\ntraining and often face mode collapse problems. They also struggle to capture\nfine details within font images. To address these problems, we present a\ndiffusion-based AFG method which generates high-quality, diverse Korean font\nimages using only a single reference image, focusing on handwritten and printed\nstyles. Our approach refines noisy images incrementally, ensuring stable\ntraining and visually appealing results. A key innovation is our text encoder,\nwhich processes phonetic representations to generate accurate and contextually\ncorrect characters, even for unseen characters. We used a pre-trained style\nencoder from DG FONT to effectively and accurately encode the style images. To\nfurther enhance the generation quality, we used perceptual loss that guides the\nmodel to focus on the global style of generated images. Experimental results on\nover 2000 Korean characters demonstrate that our model consistently generates\naccurate and detailed font images and outperforms benchmark methods, making it\na reliable tool for generating authentic Korean fonts across different styles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u81ea\u52a8\u5b57\u4f53\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u751f\u6210\u97e9\u6587\u5b57\u4f53\uff0c\u4ec5\u9700\u5355\u4e00\u6837\u672c\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u6a21\u5f0f\u5d29\u6e83\u53ca\u7ec6\u8282\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u9010\u6b65\u53bb\u566a\u751f\u6210\u56fe\u50cf\uff0c\u7ed3\u5408\u6587\u672c\u7f16\u7801\u5668\u5904\u7406\u8bed\u97f3\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u9884\u8bad\u7ec3\u98ce\u683c\u7f16\u7801\u5668\u548c\u611f\u77e5\u635f\u5931\u63d0\u5347\u8d28\u91cf\u3002", "result": "\u57282000\u591a\u4e2a\u97e9\u6587\u5b57\u7b26\u4e0a\u5b9e\u9a8c\uff0c\u751f\u6210\u7684\u5b57\u4f53\u51c6\u786e\u4e14\u7ec6\u8282\u4e30\u5bcc\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u771f\u5b9e\u97e9\u6587\u5b57\u4f53\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u98ce\u683c\u3002"}}
{"id": "2504.21012", "pdf": "https://arxiv.org/pdf/2504.21012", "abs": "https://arxiv.org/abs/2504.21012", "authors": ["Makoto Sato"], "title": "Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "What underlies intuitive human thinking? One approach to this question is to\ncompare the cognitive dynamics of humans and large language models (LLMs).\nHowever, such a comparison requires a method to quantitatively analyze AI\ncognitive behavior under controlled conditions. While anecdotal observations\nsuggest that certain prompts can dramatically change LLM behavior, these\nobservations have remained largely qualitative. Here, we propose a two-part\nframework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)\nthat triggers a rapid shift in LLM responsiveness, and a Transition Quantifying\nPrompt (TQP) that evaluates this change using a separate LLM. Through\ncontrolled experiments, we examined how LLMs react to prompts embedding two\nsemantically distant concepts (e.g., mathematical aperiodicity and traditional\ncrafts)--either fused together or presented separately--by changing their\nlinguistic quality and affective tone. Whereas humans tend to experience\nheightened engagement when such concepts are meaningfully blended producing a\nnovel concept--a form of conceptual fusion--current LLMs showed no significant\ndifference in responsiveness between semantically fused and non-fused prompts.\nThis suggests that LLMs may not yet replicate the conceptual integration\nprocesses seen in human intuition. Our method enables fine-grained,\nreproducible measurement of cognitive responsiveness, and may help illuminate\nkey differences in how intuition and conceptual leaps emerge in artificial\nversus human minds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u91cf\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8ba4\u77e5\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e24\u79cd\u63d0\u793a\uff08TIP\u548cTQP\uff09\u6765\u7814\u7a76LLM\u5bf9\u8bed\u4e49\u878d\u5408\u7684\u53cd\u5e94\uff0c\u53d1\u73b0\u5176\u4e0e\u4eba\u7c7b\u76f4\u89c9\u7684\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u76f4\u89c9\u601d\u7ef4\u7684\u8ba4\u77e5\u52a8\u6001\uff0c\u5e76\u901a\u8fc7\u4e0eLLM\u7684\u5bf9\u6bd4\u6765\u63ed\u793a\u5176\u5dee\u5f02\u3002", "method": "\u8bbe\u8ba1Transition-Inducing Prompt\uff08TIP\uff09\u548cTransition Quantifying Prompt\uff08TQP\uff09\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u5206\u6790LLM\u5bf9\u8bed\u4e49\u878d\u5408\u7684\u53cd\u5e94\u3002", "result": "LLM\u5bf9\u8bed\u4e49\u878d\u5408\u548c\u975e\u878d\u5408\u63d0\u793a\u7684\u53cd\u5e94\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u5176\u672a\u80fd\u590d\u5236\u4eba\u7c7b\u7684\u6982\u5ff5\u6574\u5408\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9a\u91cf\u6d4b\u91cf\u8ba4\u77e5\u54cd\u5e94\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u4eba\u5de5\u4e0e\u4eba\u7c7b\u601d\u7ef4\u5728\u76f4\u89c9\u548c\u6982\u5ff5\u98de\u8dc3\u4e0a\u7684\u5173\u952e\u5dee\u5f02\u3002"}}
{"id": "2504.21334", "pdf": "https://arxiv.org/pdf/2504.21334", "abs": "https://arxiv.org/abs/2504.21334", "authors": ["Misora Sugiyama", "Hirokatsu Kataoka"], "title": "Simple Visual Artifact Detection in Sora-Generated Videos", "categories": ["cs.CV"], "comment": null, "summary": "The December 2024 release of OpenAI's Sora, a powerful video generation model\ndriven by natural language prompts, highlights a growing convergence between\nlarge language models (LLMs) and video synthesis. As these multimodal systems\nevolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,\nand interacting with visual content, understanding their limitations and\nensuring their safe deployment becomes essential. This study investigates\nvisual artifacts frequently found and reported in Sora-generated videos, which\ncan compromise quality, mislead viewers, or propagate disinformation. We\npropose a multi-label classification framework targeting four common artifact\nlabel types: label 1: boundary / edge defects, label 2: texture / noise issues,\nlabel 3: movement / joint anomalies, and label 4: object mismatches /\ndisappearances. Using a dataset of 300 manually annotated frames extracted from\n15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,\nEfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50\nachieved an average multi-label classification accuracy of 94.14%. This work\nsupports the broader development of VidLLMs by contributing to (1) the creation\nof datasets for video quality evaluation, (2) interpretable artifact-based\nanalysis beyond language metrics, and (3) the identification of visual risks\nrelevant to factuality and safety.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86OpenAI Sora\u751f\u6210\u7684\u89c6\u9891\u4e2d\u5e38\u89c1\u7684\u89c6\u89c9\u4f2a\u5f71\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u7406\u89e3\u5176\u5c40\u9650\u6027\u5e76\u786e\u4fdd\u5b89\u5168\u90e8\u7f72\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\u548c\u591a\u79cd2D CNN\u67b6\u6784\uff08\u5982ResNet-50\uff09\u5bf9300\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u5e27\u8fdb\u884c\u5206\u7c7b\u3002", "result": "ResNet-50\u6a21\u578b\u5728\u5e73\u5747\u591a\u6807\u7b7e\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u8fbe\u523094.14%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u3001\u89c6\u89c9\u98ce\u9669\u8bc6\u522b\u53caVidLLMs\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2504.21013", "pdf": "https://arxiv.org/pdf/2504.21013", "abs": "https://arxiv.org/abs/2504.21013", "authors": ["Antoun Yaacoub", "Zainab Assaghir", "Lionel Prevost", "J\u00e9r\u00f4me Da-Rugna"], "title": "Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge", "categories": ["cs.CL", "cs.AI"], "comment": "This paper will be presented in the 9th Int. Conf. on Computer,\n  Software and Modeling (ICCSM 2025), Roma, Italy, 2025, July 3-5", "summary": "Artificial Intelligence (AI)-generated feedback in educational settings has\ngarnered considerable attention due to its potential to enhance learning\noutcomes. However, a comprehensive understanding of the linguistic\ncharacteristics of AI-generated feedback, including readability, lexical\nrichness, and adaptability across varying challenge levels, remains limited.\nThis study delves into the linguistic and structural attributes of feedback\ngenerated by Google's Gemini 1.5-flash text model for computer science\nmultiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,\nconsidering three difficulty levels (easy, medium, hard) and three feedback\ntones (supportive, neutral, challenging). Key linguistic metrics, such as\nlength, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,\nand lexical density, were computed and examined. A fine-tuned RoBERTa-based\nmulti-task learning (MTL) model was trained to predict these linguistic\nproperties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and\n0.03 for vocabulary richness. The findings reveal significant interaction\neffects between feedback tone and question difficulty, demonstrating the\ndynamic adaptation of AI-generated feedback within diverse educational\ncontexts. These insights contribute to the development of more personalized and\neffective AI-driven feedback mechanisms, highlighting the potential for\nimproved learning outcomes while underscoring the importance of ethical\nconsiderations in their design and deployment.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86Google Gemini 1.5-flash\u6587\u672c\u6a21\u578b\u751f\u6210\u7684AI\u53cd\u9988\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u591a\u9009\u9898\u4e2d\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u53d1\u73b0\u53cd\u9988\u97f3\u8c03\u548c\u9898\u76ee\u96be\u5ea6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u4ea4\u4e92\u4f5c\u7528\u3002", "motivation": "\u5c3d\u7ba1AI\u751f\u6210\u7684\u53cd\u9988\u5728\u6559\u80b2\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u8bed\u8a00\u7279\u5f81\uff08\u5982\u53ef\u8bfb\u6027\u3001\u8bcd\u6c47\u4e30\u5bcc\u5ea6\uff09\u7684\u5168\u9762\u7406\u89e3\u4ecd\u6709\u9650\u3002", "method": "\u5206\u6790\u4e861,200\u591a\u9053\u591a\u9009\u9898\u7684\u53cd\u9988\uff0c\u8ba1\u7b97\u4e86\u8bed\u8a00\u6307\u6807\uff08\u5982\u957f\u5ea6\u3001\u53ef\u8bfb\u6027\u3001\u8bcd\u6c47\u4e30\u5bcc\u5ea6\uff09\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2aRoBERTa\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u8fd9\u4e9b\u6307\u6807\u3002", "result": "\u6a21\u578b\u5728\u53ef\u8bfb\u6027\u548c\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u9884\u6d4b\u4e0a\u8868\u73b0\u826f\u597d\uff08MAE\u5206\u522b\u4e3a2.0\u548c0.03\uff09\uff0c\u53cd\u9988\u97f3\u8c03\u4e0e\u9898\u76ee\u96be\u5ea6\u6709\u663e\u8457\u4ea4\u4e92\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u4e2a\u6027\u5316\u7684AI\u53cd\u9988\u673a\u5236\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u4e2d\u7684\u4f26\u7406\u8003\u91cf\u3002"}}
{"id": "2504.21336", "pdf": "https://arxiv.org/pdf/2504.21336", "abs": "https://arxiv.org/abs/2504.21336", "authors": ["Linshan Wu", "Yuxiang Nie", "Sunan He", "Jiaxin Zhuang", "Hao Chen"], "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation", "categories": ["cs.CV"], "comment": "The first universal foundation model for grounded biomedical image\n  interpretation", "summary": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis.", "AI": {"tldr": "UniBiomed\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u548cSegment Anything Model\uff08SAM\uff09\u7684\u65b0\u578b\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7684\u5168\u9762\u89e3\u91ca\u3002\u5b83\u7edf\u4e00\u4e86\u4e34\u5e8a\u6587\u672c\u751f\u6210\u548c\u751f\u7269\u533b\u5b66\u5bf9\u8c61\u5206\u5272\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u5206\u6790\u3002", "motivation": "\u4f20\u7edfAI\u65b9\u6cd5\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u4f9d\u8d56\u5206\u79bb\u7684\u8bad\u7ec3\u6a21\u578b\uff0c\u5bfc\u81f4\u90e8\u7f72\u4e0d\u7075\u6d3b\u4e14\u65e0\u6cd5\u5229\u7528\u6574\u4f53\u4fe1\u606f\u3002UniBiomed\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u89e3\u91ca\u3002", "method": "UniBiomed\u901a\u8fc7\u6574\u5408MLLM\u548cSAM\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u5305\u542b2700\u4e07\u7ec4\u56fe\u50cf\u3001\u6807\u6ce8\u548c\u6587\u672c\u63cf\u8ff0\uff09\uff0c\u652f\u630110\u79cd\u751f\u7269\u533b\u5b66\u6210\u50cf\u6a21\u6001\u3002", "result": "\u572884\u4e2a\u5185\u5916\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cUniBiomed\u5728\u5206\u5272\u3001\u75be\u75c5\u8bc6\u522b\u3001\u533a\u57df\u611f\u77e5\u8bca\u65ad\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u62a5\u544a\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "UniBiomed\u4e3a\u751f\u7269\u533b\u5b66AI\u5e26\u6765\u4e86\u7a81\u7834\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6548\u7387\uff0c\u4ee3\u8868\u4e86\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u65b0\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2504.21019", "pdf": "https://arxiv.org/pdf/2504.21019", "abs": "https://arxiv.org/abs/2504.21019", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Yiming Xue", "Ziwei Zhang", "Zhengxian Wu"], "title": "Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NAACL 2025 main conference", "summary": "The growing popularity of large language models has raised concerns regarding\nthe potential to misuse AI-generated text (AIGT). It becomes increasingly\ncritical to establish an excellent AIGT detection method with high\ngeneralization and robustness. However, existing methods either focus on model\ngeneralization or concentrate on robustness. The unified mechanism, to\nsimultaneously address the challenges of generalization and robustness, is less\nexplored. In this paper, we argue that robustness can be view as a specific\nform of domain shift, and empirically reveal an intrinsic mechanism for model\ngeneralization of AIGT detection task. Then, we proposed a novel AIGT detection\nmethod (DP-Net) via dynamic perturbations introduced by a reinforcement\nlearning with elaborated reward and action. Experimentally, extensive results\nshow that the proposed DP-Net significantly outperforms some state-of-the-art\nAIGT detection methods for generalization capacity in three cross-domain\nscenarios. Meanwhile, the DP-Net achieves best robustness under two text\nadversarial attacks. The code is publicly available at\nhttps://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\uff08DP-Net\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u6270\u52a8\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0cAI\u751f\u6210\u6587\u672c\u7684\u6ee5\u7528\u98ce\u9669\u589e\u52a0\uff0c\u4e9f\u9700\u4e00\u79cd\u540c\u65f6\u5177\u5907\u9ad8\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5c06\u9c81\u68d2\u6027\u89c6\u4e3a\u7279\u5b9a\u5f62\u5f0f\u7684\u9886\u57df\u504f\u79fb\uff0c\u63d0\u51faDP-Net\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u6270\u52a8\u673a\u5236\u5b9e\u73b0\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u7684\u7edf\u4e00\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDP-Net\u5728\u4e09\u79cd\u8de8\u57df\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u4e24\u79cd\u6587\u672c\u5bf9\u6297\u653b\u51fb\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "DP-Net\u4e3aAI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.21340", "pdf": "https://arxiv.org/pdf/2504.21340", "abs": "https://arxiv.org/abs/2504.21340", "authors": ["Khoa Tuan Nguyen", "Ho-min Park", "Gaeun Oh", "Joris Vankerschaver", "Wesley De Neve"], "title": "Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ISBI 2025 \"Challenge 2: Pap Smear Cell Classification\n  Challenge\"", "summary": "We propose a novel approach to cervical cell image classification for\ncervical cancer screening using the EVA-02 transformer model. We developed a\nfour-step pipeline: fine-tuning EVA-02, feature extraction, selecting important\nfeatures through multiple machine learning models, and training a new\nartificial neural network with optional loss weighting for improved\ngeneralization. With this design, our best model achieved an F1-score of\n0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized\nKernel SHAP analysis and identified key features correlating with cell\nmorphology and staining characteristics, providing interpretable insights into\nthe decision-making process of the fine-tuned model. Our code is available at\nhttps://github.com/Khoa-NT/isbi2025_ps3c.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eEVA-02\u53d8\u6362\u5668\u6a21\u578b\u7684\u5bab\u9888\u7ec6\u80de\u56fe\u50cf\u5206\u7c7b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u56db\u6b65\u6d41\u7a0b\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0cF1\u5206\u6570\u8fbe0.85227\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u6539\u8fdb\u5bab\u9888\u764c\u7b5b\u67e5\u4e2d\u7684\u7ec6\u80de\u56fe\u50cf\u5206\u7c7b\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u56db\u6b65\u6d41\u7a0b\uff1a\u5fae\u8c03EVA-02\u3001\u7279\u5f81\u63d0\u53d6\u3001\u591a\u6a21\u578b\u7279\u5f81\u9009\u62e9\u3001\u8bad\u7ec3\u65b0\u795e\u7ecf\u7f51\u7edc\uff08\u53ef\u9009\u635f\u5931\u52a0\u6743\uff09\u3002", "result": "\u6700\u4f73\u6a21\u578bF1\u5206\u65700.85227\uff0c\u4f18\u4e8e\u57fa\u7ebf\uff080.84878\uff09\uff1b\u901a\u8fc7Kernel SHAP\u5206\u6790\u8bc6\u522b\u5173\u952e\u7279\u5f81\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u4ee3\u7801\u5f00\u6e90\u3002"}}
{"id": "2504.21020", "pdf": "https://arxiv.org/pdf/2504.21020", "abs": "https://arxiv.org/abs/2504.21020", "authors": ["Jaydip Sen", "Rohit Pandey", "Hetvi Waghela"], "title": "Context-Enhanced Contrastive Search for Improved LLM Text Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This is the pre-review version of our paper, which has been accepted\n  for publication in the IEEE 6th International Conference on Emerging\n  Technologies (INCET). The conference will be organized at Belgaum, India,\n  from May 24 to 26, 2025. This is not the final camera-ready paper, which will\n  be available on IEEE Xplore. The paper is 9 pages long, and it contains 2\n  Figures and 4 Tables", "summary": "Recently, Large Language Models (LLMs) have demonstrated remarkable\nadvancements in Natural Language Processing (NLP). However, generating\nhigh-quality text that balances coherence, diversity, and relevance remains\nchallenging. Traditional decoding methods, such as bean search and top-k\nsampling, often struggle with either repetitive or incoherent outputs,\nparticularly in tasks that require long-form text generation. To address these\nlimitations, the paper proposes a novel enhancement of the well-known\nContrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with\ncontextual calibration. The proposed scheme introduces several novelties\nincluding dynamic contextual importance weighting, multi-level Contrastive\nSearch, and adaptive temperature control, to optimize the balance between\nfluency, creativity, and precision. The performance of CECS is evaluated using\nseveral standard metrics such as BLEU, ROUGE, and semantic similarity.\nExperimental results demonstrate significant improvements in both coherence and\nrelevance of the generated texts by CECS outperforming the existing Contrastive\nSearch techniques. The proposed algorithm has several potential applications in\nthe real world including legal document drafting, customer service chatbots,\nand content marketing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5bf9\u6bd4\u641c\u7d22\u7b97\u6cd5CECS\uff0c\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u91cd\u8981\u6027\u52a0\u6743\u548c\u591a\u7ea7\u5bf9\u6bd4\u641c\u7d22\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6587\u672c\u7684\u8fde\u8d2f\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u4f20\u7edf\u89e3\u7801\u65b9\u6cd5\u5728\u751f\u6210\u957f\u6587\u672c\u65f6\u5b58\u5728\u91cd\u590d\u6216\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u7684\u7b97\u6cd5\u6765\u5e73\u8861\u6d41\u7545\u6027\u3001\u521b\u9020\u6027\u548c\u7cbe\u786e\u6027\u3002", "method": "\u63d0\u51faContext-Enhanced Contrastive Search (CECS)\u7b97\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u4e0a\u4e0b\u6587\u91cd\u8981\u6027\u52a0\u6743\u3001\u591a\u7ea7\u5bf9\u6bd4\u641c\u7d22\u548c\u81ea\u9002\u5e94\u6e29\u5ea6\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCECS\u5728BLEU\u3001ROUGE\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u5bf9\u6bd4\u641c\u7d22\u6280\u672f\u3002", "conclusion": "CECS\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u6cd5\u5f8b\u6587\u4ef6\u8d77\u8349\u3001\u5ba2\u670d\u804a\u5929\u673a\u5668\u4eba\u548c\u5185\u5bb9\u8425\u9500\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.21344", "pdf": "https://arxiv.org/pdf/2504.21344", "abs": "https://arxiv.org/abs/2504.21344", "authors": ["Luoting Zhuang", "Seyed Mohammad Hossein Tabatabaei", "Ramin Salehi-Rad", "Linh M. Tran", "Denise R. Aberle", "Ashley E. Prosper", "William Hsu"], "title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection", "categories": ["cs.CV", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Objective: A number of machine learning models have utilized semantic\nfeatures, deep features, or both to assess lung nodule malignancy. However,\ntheir reliance on manual annotation during inference, limited interpretability,\nand sensitivity to imaging variations hinder their application in real-world\nclinical settings. Thus, this research aims to integrate semantic features\nderived from radiologists' assessments of nodules, allowing the model to learn\nclinically relevant, robust, and explainable features for predicting lung\ncancer. Methods: We obtained 938 low-dose CT scans from the National Lung\nScreening Trial with 1,246 nodules and semantic features. The Lung Image\nDatabase Consortium dataset contains 1,018 CT scans, with 2,625 lesions\nannotated for nodule characteristics. Three external datasets were obtained\nfrom UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We\nfinetuned a pretrained Contrastive Language-Image Pretraining model with a\nparameter-efficient fine-tuning approach to align imaging and semantic features\nand predict the one-year lung cancer diagnosis. Results: We evaluated the\nperformance of the one-year diagnosis of lung cancer with AUROC and AUPRC and\ncompared it to three state-of-the-art models. Our model demonstrated an AUROC\nof 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on\nexternal datasets. Using CLIP, we also obtained predictions on semantic\nfeatures, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and\npleural attachment (0.84), that can be used to explain model predictions.\nConclusion: Our approach accurately classifies lung nodules as benign or\nmalignant, providing explainable outputs, aiding clinicians in comprehending\nthe underlying meaning of model predictions. This approach also prevents the\nmodel from learning shortcuts and generalizes across clinical settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u7279\u5f81\u548c\u6df1\u5ea6\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u80ba\u764c\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\uff08CLIP\uff09\u5b9e\u73b0\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u4e14\u5bf9\u6210\u50cf\u53d8\u5316\u654f\u611f\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u6574\u5408\u653e\u5c04\u79d1\u533b\u751f\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u4ee5\u5b66\u4e60\u4e34\u5e8a\u76f8\u5173\u3001\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u591a\u4e2a\u6570\u636e\u96c6\u7684\u4f4e\u5242\u91cfCT\u626b\u63cf\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u901a\u8fc7CLIP\u6a21\u578b\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5bf9\u9f50\u56fe\u50cf\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u9884\u6d4b\u4e00\u5e74\u5185\u80ba\u764c\u8bca\u65ad\u3002", "result": "\u6a21\u578b\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff08AUROC: 0.90\uff0cAUPRC: 0.78\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u7279\u5f81\u9884\u6d4b\uff08\u5982\u7ed3\u8282\u8fb9\u7f18\u3001\u4e00\u81f4\u6027\u7b49\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u51c6\u786e\u5206\u7c7b\u80ba\u7ed3\u8282\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\uff0c\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u7406\u89e3\u6a21\u578b\u9884\u6d4b\uff0c\u540c\u65f6\u907f\u514d\u6a21\u578b\u5b66\u4e60\u6377\u5f84\u5e76\u5177\u6709\u8de8\u4e34\u5e8a\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.21022", "pdf": "https://arxiv.org/pdf/2504.21022", "abs": "https://arxiv.org/abs/2504.21022", "authors": ["Jun Wang", "David Smith Sundarsingh", "Jyotirmoy V. Deshmukh", "Yiannis Kantaros"], "title": "ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Linear Temporal Logic (LTL) has become a prevalent specification language for\nrobotic tasks. To mitigate the significant manual effort and expertise required\nto define LTL-encoded tasks, several methods have been proposed for translating\nNatural Language (NL) instructions into LTL formulas, which, however, lack\ncorrectness guarantees. To address this, we introduce a new NL-to-LTL\ntranslation method, called ConformalNL2LTL, that can achieve user-defined\ntranslation success rates over unseen NL commands. Our method constructs LTL\nformulas iteratively by addressing a sequence of open-vocabulary\nQuestion-Answering (QA) problems with LLMs. To enable uncertainty-aware\ntranslation, we leverage conformal prediction (CP), a distribution-free\nuncertainty quantification tool for black-box models. CP enables our method to\nassess the uncertainty in LLM-generated answers, allowing it to proceed with\ntranslation when sufficiently confident and request help otherwise. We provide\nboth theoretical and empirical results demonstrating that ConformalNL2LTL\nachieves user-specified translation accuracy while minimizing help rates.", "AI": {"tldr": "ConformalNL2LTL\u662f\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548c\u5171\u5f62\u9884\u6d4b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3aLTL\u516c\u5f0f\uff0c\u786e\u4fdd\u7ffb\u8bd1\u51c6\u786e\u7387\u5e76\u6700\u5c0f\u5316\u6c42\u52a9\u7387\u3002", "motivation": "\u51cf\u5c11\u624b\u52a8\u5b9a\u4e49LTL\u4efb\u52a1\u6240\u9700\u7684\u5de5\u4f5c\u91cf\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u540c\u65f6\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6b63\u786e\u6027\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7LLM\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47QA\u95ee\u9898\uff0c\u8fed\u4ee3\u6784\u5efaLTL\u516c\u5f0f\uff0c\u5e76\u5229\u7528\u5171\u5f62\u9884\u6d4b\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4ec5\u5728\u8db3\u591f\u81ea\u4fe1\u65f6\u7ee7\u7eed\u7ffb\u8bd1\uff0c\u5426\u5219\u8bf7\u6c42\u5e2e\u52a9\u3002", "result": "ConformalNL2LTL\u80fd\u591f\u5b9e\u73b0\u7528\u6237\u6307\u5b9a\u7684\u7ffb\u8bd1\u51c6\u786e\u7387\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6c42\u52a9\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u5747\u8bc1\u660e\u6709\u6548\uff0c\u4e3aNL\u5230LTL\u7684\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21356", "pdf": "https://arxiv.org/pdf/2504.21356", "abs": "https://arxiv.org/abs/2504.21356", "authors": ["Hong Zhang", "Zhongjie Duan", "Xingjun Wang", "Yingda Chen", "Yuze Zhao", "Yu Zhang"], "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.", "AI": {"tldr": "Nexus-Gen\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u548c\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u5408\u6210\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u7edf\u4e00\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4e0e\u9886\u57df\u4e13\u7528\u67b6\u6784\u5b58\u5728\u5dee\u8ddd\uff0cNexus-Gen\u65e8\u5728\u901a\u8fc7\u7ed3\u5408LLM\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u5bf9\u9f50\u8bad\u7ec3\uff1a1\uff09\u81ea\u56de\u5f52LLM\u5b66\u4e60\u9884\u6d4b\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u7684\u56fe\u50cf\u5d4c\u5165\uff1b2\uff09\u89c6\u89c9\u89e3\u7801\u5668\u8bad\u7ec3\u4ece\u5d4c\u5165\u91cd\u5efa\u9ad8\u4fdd\u771f\u56fe\u50cf\u3002\u5f15\u5165\u9884\u586b\u5145\u81ea\u56de\u5f52\u7b56\u7565\u4ee5\u907f\u514d\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "Nexus-Gen\u5177\u5907\u7efc\u5408\u7684\u56fe\u50cf\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u4fc3\u8fdb\u4e86\u9886\u57df\u53d1\u5c55\u3002", "conclusion": "Nexus-Gen\u901a\u8fc7\u53cc\u9636\u6bb5\u8bad\u7ec3\u548c\u9884\u586b\u5145\u7b56\u7565\uff0c\u6210\u529f\u6574\u5408\u4e86\u591a\u6a21\u6001\u80fd\u529b\uff0c\u4e3a\u7edf\u4e00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21026", "pdf": "https://arxiv.org/pdf/2504.21026", "abs": "https://arxiv.org/abs/2504.21026", "authors": ["Manish Pandey", "Nageshwar Prasad Yadav", "Mokshada Adduru", "Sawan Rai"], "title": "Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "With the growing presence of multilingual users on social media, detecting\nabusive language in code-mixed text has become increasingly challenging.\nCode-mixed communication, where users seamlessly switch between English and\ntheir native languages, poses difficulties for traditional abuse detection\nmodels, as offensive content may be context-dependent or obscured by linguistic\nblending. While abusive language detection has been extensively explored for\nhigh-resource languages like English and Hindi, low-resource languages such as\nTelugu and Nepali remain underrepresented, leaving gaps in effective\nmoderation. In this study, we introduce a novel, manually annotated dataset of\n2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized\nas abusive and non-abusive, collected from various social media platforms. The\ndataset undergoes rigorous preprocessing before being evaluated across multiple\nMachine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We\nexperimented with models including Logistic Regression, Random Forest, Support\nVector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing\ntheir performance through hyperparameter tuning, and evaluate it using 10-fold\ncross-validation and statistical significance testing (t-test). Our findings\nprovide key insights into the challenges of detecting abusive language in\ncode-mixed settings and offer a comparative analysis of computational\napproaches. This study contributes to advancing NLP for low-resource languages\nby establishing benchmarks for abusive language detection in Telugu-English and\nNepali-English code-mixed text. The dataset and insights can aid in the\ndevelopment of more robust moderation strategies for multilingual social media\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u793e\u4ea4\u5a92\u4f53\u4e2d\u591a\u8bed\u8a00\u7528\u6237\u4f7f\u7528\u6df7\u5408\u8bed\u8a00\uff08\u5982\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u548c\u5c3c\u6cca\u5c14\u8bed-\u82f1\u8bed\uff09\u65f6\uff0c\u68c0\u6d4b\u8fb1\u9a82\u6027\u8bed\u8a00\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u591a\u8bed\u8a00\u7528\u6237\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u589e\u52a0\uff0c\u6df7\u5408\u8bed\u8a00\u4e2d\u7684\u8fb1\u9a82\u6027\u8bed\u8a00\u68c0\u6d4b\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6cf0\u5362\u56fa\u8bed\u548c\u5c3c\u6cca\u5c14\u8bed\uff09\u7684\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2000\u6761\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u548c5000\u6761\u5c3c\u6cca\u5c14\u8bed-\u82f1\u8bed\u6df7\u5408\u8bc4\u8bba\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\uff08\u5982\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001SVM\u3001\u795e\u7ecf\u7f51\u7edc\u3001LSTM\u3001CNN\u548cLLMs\uff09\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u548c10\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4f18\u5316\u6027\u80fd\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u6df7\u5408\u8bed\u8a00\u4e2d\u8fb1\u9a82\u6027\u8bed\u8a00\u68c0\u6d4b\u7684\u5173\u952e\u89c1\u89e3\uff0c\u5e76\u5bf9\u4e0d\u540c\u8ba1\u7b97\u65b9\u6cd5\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5ba1\u6838\u7b56\u7565\u3002"}}
{"id": "2504.21368", "pdf": "https://arxiv.org/pdf/2504.21368", "abs": "https://arxiv.org/abs/2504.21368", "authors": ["Pramook Khungurn", "Sukit Seripanitkarn", "Phonphrm Thawatdamrongkit", "Supasorn Suwajanakorn"], "title": "Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality", "categories": ["cs.CV", "cs.AI"], "comment": "AI for Content Creation (AI4CC) Workshop at CVPR 2025", "summary": "Diffusion autoencoders (DAEs) are typically formulated as a noise prediction\nmodel and trained with a linear-$\\beta$ noise schedule that spends much of its\nsampling steps at high noise levels. Because high noise levels are associated\nwith recovering large-scale image structures and low noise levels with\nrecovering details, this configuration can result in low-quality and blurry\nimages. However, it should be possible to improve details while spending fewer\nsteps recovering structures because the latent code should already contain\nstructural information. Based on this insight, we propose a new DAE training\nmethod that improves the quality of reconstructed images. We divide training\ninto two phases. In the first phase, the DAE is trained as a vanilla\nautoencoder by always setting the noise level to the highest, forcing the\nencoder and decoder to populate the latent code with structural information. In\nthe second phase, we incorporate a noise schedule that spends more time in the\nlow-noise region, allowing the DAE to learn how to perfect the details. Our\nmethod results in images that have accurate high-level structures and low-level\ndetails while still preserving useful properties of the latent codes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6269\u6563\u81ea\u7f16\u7801\u5668\uff08DAE\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u4f18\u5316\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfDAE\u4f7f\u7528\u7ebf\u6027\u566a\u58f0\u8ba1\u5212\uff0c\u5bfc\u81f4\u56fe\u50cf\u6a21\u7cca\u4e14\u7ec6\u8282\u4e0d\u8db3\uff0c\u800c\u6f5c\u5728\u7f16\u7801\u5e94\u5df2\u5305\u542b\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u5f3a\u5236\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5728\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\u5b66\u4e60\u7ed3\u6784\u4fe1\u606f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u4f4e\u566a\u58f0\u533a\u57df\u4f18\u5316\u7ec6\u8282\u3002", "result": "\u6539\u8fdb\u540e\u7684DAE\u80fd\u751f\u6210\u5177\u6709\u51c6\u786e\u7ed3\u6784\u548c\u7ec6\u8282\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u7559\u6f5c\u5728\u7f16\u7801\u7684\u6709\u7528\u7279\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86DAE\u7684\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\uff0c\u517c\u987e\u7ed3\u6784\u548c\u7ec6\u8282\u3002"}}
{"id": "2504.21027", "pdf": "https://arxiv.org/pdf/2504.21027", "abs": "https://arxiv.org/abs/2504.21027", "authors": ["Yu Zheng", "Longyi Liu", "Yuming Lin", "Jie Feng", "Guozhen Zhang", "Depeng Jin", "Yong Li"], "title": "UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) holds promise for revolutionizing\nvarious fields traditionally dominated by human expertise. Urban planning, a\nprofessional discipline that fundamentally shapes our daily surroundings, is\none such field heavily relying on multifaceted domain knowledge and experience\nof human experts. The extent to which LLMs can assist human practitioners in\nurban planning remains largely unexplored. In this paper, we introduce a\ncomprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of\nLLMs in urban planning, which encompasses fundamental principles, professional\nknowledge, and management and regulations, aligning closely with the\nqualifications expected of human planners. Through extensive evaluation, we\nreveal a significant imbalance in the acquisition of planning knowledge among\nLLMs, with even the most proficient models falling short of meeting\nprofessional standards. For instance, we observe that 70% of LLMs achieve\nsubpar performance in understanding planning regulations compared to other\naspects. Besides the benchmark, we present the largest-ever supervised\nfine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction\npairs sourced from urban planning exams and textbooks. Our findings demonstrate\nthat fine-tuned models exhibit enhanced performance in memorization tests and\ncomprehension of urban planning knowledge, while there exists significant room\nfor improvement, particularly in tasks requiring domain-specific terminology\nand reasoning. By making our benchmark, dataset, and associated evaluation and\nfine-tuning toolsets publicly available at\nhttps://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the\nintegration of LLMs into practical urban planning, fostering a symbiotic\ncollaboration between human expertise and machine intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86UrbanPlanBench\u57fa\u51c6\u548cUrbanPlanText\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLMs\u5728\u57ce\u4e61\u89c4\u5212\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u4e13\u4e1a\u80fd\u529b\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u57ce\u4e61\u89c4\u5212\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efaUrbanPlanBench\u57fa\u51c6\u548cUrbanPlanText\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLMs\u8868\u73b0\u5e76\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "LLMs\u5728\u89c4\u5212\u77e5\u8bc6\u83b7\u53d6\u4e0a\u8868\u73b0\u4e0d\u5747\u8861\uff0c\u5fae\u8c03\u540e\u6027\u80fd\u63d0\u5347\u4f46\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u516c\u5f00\u8d44\u6e90\u4ee5\u4fc3\u8fdbLLMs\u4e0e\u57ce\u4e61\u89c4\u5212\u7684\u7ed3\u5408\uff0c\u63a8\u52a8\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2504.21385", "pdf": "https://arxiv.org/pdf/2504.21385", "abs": "https://arxiv.org/abs/2504.21385", "authors": ["Shijun Zhou", "Yajing Liu", "Chunhui Hao", "Zhiyuan Liu", "Jiandong Tian"], "title": "IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided Diffusion for Real-world Image Dehazing", "categories": ["cs.CV"], "comment": null, "summary": "Due to the domain gap between real-world and synthetic hazy images, current\ndata-driven dehazing algorithms trained on synthetic datasets perform well on\nsynthetic data but struggle to generalize to real-world scenarios. To address\nthis challenge, we propose \\textbf{I}mage \\textbf{D}ehazing \\textbf{D}iffusion\n\\textbf{M}odels (IDDM), a novel diffusion process that incorporates the\natmospheric scattering model into noise diffusion. IDDM aims to use the gradual\nhaze formation process to help the denoising Unet robustly learn the\ndistribution of clear images from the conditional input hazy images. We design\na specialized training strategy centered around IDDM. Diffusion models are\nleveraged to bridge the domain gap from synthetic to real-world, while the\natmospheric scattering model provides physical guidance for haze formation.\nDuring the forward process, IDDM simultaneously introduces haze and noise into\nclear images, and then robustly separates them during the sampling process. By\ntraining with physics-guided information, IDDM shows the ability of domain\ngeneralization, and effectively restores the real-world hazy images despite\nbeing trained on synthetic datasets. Extensive experiments demonstrate the\neffectiveness of our method through both quantitative and qualitative\ncomparisons with state-of-the-art approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5IDDM\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u6c14\u6563\u5c04\u6a21\u578b\u548c\u566a\u58f0\u6269\u6563\uff0c\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u573a\u666f\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u7b97\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5f25\u5408\u9886\u57df\u5dee\u8ddd\u7684\u65b9\u6cd5\u3002", "method": "IDDM\u5229\u7528\u6269\u6563\u8fc7\u7a0b\uff0c\u5c06\u5927\u6c14\u6563\u5c04\u6a21\u578b\u878d\u5165\u566a\u58f0\u6269\u6563\uff0c\u901a\u8fc7\u9010\u6b65\u96fe\u5316\u8fc7\u7a0b\u5e2e\u52a9\u53bb\u566aUnet\u5b66\u4e60\u6e05\u6670\u56fe\u50cf\u7684\u5206\u5e03\u3002", "result": "IDDM\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u80fd\u591f\u6709\u6548\u6062\u590d\u771f\u5b9e\u4e16\u754c\u7684\u96fe\u5316\u56fe\u50cf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "IDDM\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9886\u57df\u6cdb\u5316\uff0c\u4e3a\u56fe\u50cf\u53bb\u96fe\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21028", "pdf": "https://arxiv.org/pdf/2504.21028", "abs": "https://arxiv.org/abs/2504.21028", "authors": ["Ivan Montoya Sanchez", "Shaswata Mitra", "Aritran Piplai", "Sudip Mittal"], "title": "Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "8 pages, 5 figures, 5 tables", "summary": "The rapid evolution of malware variants requires robust classification\nmethods to enhance cybersecurity. While Large Language Models (LLMs) offer\npotential for generating malware descriptions to aid family classification,\ntheir utility is limited by semantic embedding overlaps and misalignment with\nbinary behavioral features. We propose a contrastive fine-tuning (CFT) method\nthat refines LLM embeddings via targeted selection of hard negative samples\nbased on cosine similarity, enabling LLMs to distinguish between closely\nrelated malware families. Our approach combines high-similarity negatives to\nenhance discriminative power and mid-tier negatives to increase embedding\ndiversity, optimizing both precision and generalization. Evaluated on the\nCIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into\na multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework\non a few-shot setting. Experiments demonstrate significant improvements: our\nmethod achieves 63.15% classification accuracy with as few as 20 samples on\nCIC-AndMal-2020, outperforming baselines by 11--21 percentage points and\nsurpassing prior negative sampling strategies. Ablation studies confirm the\nsuperiority of similarity-based selection over random sampling, with gains of\n10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions\nthat generalize to unseen variants, bridging textual and binary feature gaps.\nThis work advances malware classification by enabling nuanced semantic\ndistinctions and provides a scalable framework for adapting LLMs to\ncybersecurity challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u5fae\u8c03\uff08CFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u786c\u8d1f\u6837\u672c\u9009\u62e9\u4f18\u5316LLM\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6076\u610f\u8f6f\u4ef6\u53d8\u79cd\u7684\u5feb\u901f\u6f14\u53d8\u9700\u8981\u5f3a\u5927\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u800c\u73b0\u6709LLM\u5728\u8bed\u4e49\u5d4c\u5165\u548c\u884c\u4e3a\u7279\u5f81\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5fae\u8c03\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u76f8\u4f3c\u5ea6\u548c\u4e2d\u7b49\u7ea7\u522b\u7684\u8d1f\u6837\u672c\uff0c\u4f18\u5316LLM\u5d4c\u5165\uff0c\u5e76\u96c6\u6210\u5230\u591a\u6a21\u6001\u5206\u7c7b\u5668\u4e2d\u3002", "result": "\u5728CIC-AndMal-2020\u548cBODMAS\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u752820\u4e2a\u6837\u672c\u5373\u8fbe\u523063.15%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd511-21\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u533a\u5206\u548c\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u4e3aLLM\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21387", "pdf": "https://arxiv.org/pdf/2504.21387", "abs": "https://arxiv.org/abs/2504.21387", "authors": ["Teodor Boyadzhiev", "Gabriele Lagani", "Luca Ciampi", "Giuseppe Amato", "Krassimira Ivanova"], "title": "Comparison of Different Deep Neural Network Models in the Cultural Heritage Domain", "categories": ["cs.CV"], "comment": "Accepted at the 10th International Euro-Mediterranean Conference\n  (EuroMed 2024)", "summary": "The integration of computer vision and deep learning is an essential part of\ndocumenting and preserving cultural heritage, as well as improving visitor\nexperiences. In recent years, two deep learning paradigms have been established\nin the field of computer vision: convolutional neural networks and transformer\narchitectures. The present study aims to make a comparative analysis of some\nrepresentatives of these two techniques of their ability to transfer knowledge\nfrom generic dataset, such as ImageNet, to cultural heritage specific tasks.\nThe results of testing examples of the architectures VGG, ResNet, DenseNet,\nVisual Transformer, Swin Transformer, and PoolFormer, showed that DenseNet is\nthe best in terms of efficiency-computability ratio.", "AI": {"tldr": "\u6bd4\u8f83\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u67b6\u6784\u5728\u6587\u5316\u9057\u4ea7\u4efb\u52a1\u4e2d\u7684\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\uff0c\u53d1\u73b0DenseNet\u5728\u6548\u7387\u4e0e\u8ba1\u7b97\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u8ba8\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u53ca\u63d0\u5347\u6e38\u5ba2\u4f53\u9a8c\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e24\u79cd\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u6027\u80fd\u3002", "method": "\u6d4b\u8bd5VGG\u3001ResNet\u3001DenseNet\u3001Visual Transformer\u3001Swin Transformer\u548cPoolFormer\u7b49\u67b6\u6784\u5728\u6587\u5316\u9057\u4ea7\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "DenseNet\u5728\u6548\u7387\u4e0e\u8ba1\u7b97\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "DenseNet\u662f\u6587\u5316\u9057\u4ea7\u4efb\u52a1\u4e2d\u77e5\u8bc6\u8fc1\u79fb\u7684\u6700\u4f73\u9009\u62e9\u3002"}}
{"id": "2504.21029", "pdf": "https://arxiv.org/pdf/2504.21029", "abs": "https://arxiv.org/abs/2504.21029", "authors": ["Ben Goertzel", "Paulos Yibelo"], "title": "PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "We propose a robust transformer architecture designed to prevent prompt\ninjection attacks and ensure secure, reliable response generation. Our PICO\n(Prompt Isolation and Cybersecurity Oversight) framework structurally separates\ntrusted system instructions from untrusted user inputs through dual channels\nthat are processed independently and merged only by a controlled, gated fusion\nmechanism. In addition, we integrate a specialized Security Expert Agent within\na Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge\nGraph (CKG) to supply domain-specific reasoning. Our training design further\nensures that the system prompt branch remains immutable while the rest of the\nnetwork learns to handle adversarial inputs safely. This PICO framework is\npresented via a general mathematical formulation, then elaborated in terms of\nthe specifics of transformer architecture, and fleshed out via hypothetical\ncase studies including Policy Puppetry attacks. While the most effective\nimplementation may involve training transformers in a PICO-based way from\nscratch, we also present a cost-effective fine-tuning approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPICO\u7684\u9c81\u68d2Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u9694\u79bb\u7cfb\u7edf\u6307\u4ee4\u4e0e\u7528\u6237\u8f93\u5165\uff0c\u7ed3\u5408\u5b89\u5168\u4e13\u5bb6\u4ee3\u7406\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u9632\u6b62\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002", "motivation": "\u89e3\u51b3\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u95ee\u9898\uff0c\u786e\u4fdd\u751f\u6210\u54cd\u5e94\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u53cc\u901a\u9053\u5904\u7406\u6307\u4ee4\u4e0e\u8f93\u5165\uff0c\u7ed3\u5408MoE\u6846\u67b6\u548cCKG\uff0c\u8bad\u7ec3\u65f6\u4fdd\u6301\u7cfb\u7edf\u63d0\u793a\u5206\u652f\u4e0d\u53ef\u53d8\u3002", "result": "\u63d0\u51fa\u4e86PICO\u6846\u67b6\uff0c\u652f\u6301\u4ece\u5934\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u6709\u6548\u9632\u5fa1\u653b\u51fb\u3002", "conclusion": "PICO\u6846\u67b6\u5728\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2504.21403", "pdf": "https://arxiv.org/pdf/2504.21403", "abs": "https://arxiv.org/abs/2504.21403", "authors": ["Yumeng Shi", "Quanyu Long", "Wenya Wang"], "title": "Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Video question answering benefits from the rich information available in\nvideos, enabling a wide range of applications. However, the large volume of\ntokens generated from longer videos presents significant challenges to memory\nefficiency and model performance. To alleviate this issue, existing works\npropose to compress video inputs, but usually overlooking the varying\nimportance of static and dynamic information across different queries, leading\nto inefficient token usage within limited budgets. To tackle this, we propose a\nnovel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust\nstatic and dynamic information needed based on question requirements. Our\nframework first explores different token allocations between static frames,\nwhich preserve spatial details, and dynamic frames, which capture temporal\nchanges. Next, it employs a query-aware attention-based metric to select the\noptimal token combination without model updates. Our proposed framework is\nplug-and-play that can be seamlessly integrated within diverse video-language\nmodels. Extensive experiments show that our method achieves significant\nperformance improvements (up to 5.8%) among various video question answering\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEXPLORE-THEN-SELECT\u7684\u81ea\u9002\u5e94\u4ee4\u724c\u9009\u62e9\u7b56\u7565\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u9759\u6001\u548c\u52a8\u6001\u4fe1\u606f\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u95ee\u7b54\u4e2d\u957f\u89c6\u9891\u751f\u6210\u7684\u5927\u91cf\u4ee4\u724c\u5bf9\u5185\u5b58\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u6784\u6210\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u538b\u7f29\u89c6\u9891\u8f93\u5165\u65f6\u5ffd\u7565\u4e86\u4e0d\u540c\u67e5\u8be2\u5bf9\u9759\u6001\u548c\u52a8\u6001\u4fe1\u606f\u7684\u9700\u6c42\u5dee\u5f02\u3002", "method": "\u63d0\u51faEXPLORE-THEN-SELECT\u7b56\u7565\uff0c\u5148\u63a2\u7d22\u9759\u6001\u5e27\u548c\u52a8\u6001\u5e27\u7684\u4ee4\u724c\u5206\u914d\uff0c\u518d\u57fa\u4e8e\u67e5\u8be2\u611f\u77e5\u7684\u6ce8\u610f\u529b\u6307\u6807\u9009\u62e9\u6700\u4f18\u7ec4\u5408\uff0c\u65e0\u9700\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe5.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u5373\u63d2\u5373\u7528\u7684\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u591a\u79cd\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ee4\u724c\u4f7f\u7528\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2504.21030", "pdf": "https://arxiv.org/pdf/2504.21030", "abs": "https://arxiv.org/abs/2504.21030", "authors": ["Naveen Krishnan"], "title": "Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Multi-agent systems represent a significant advancement in artificial\nintelligence, enabling complex problem-solving through coordinated specialized\nagents. However, these systems face fundamental challenges in context\nmanagement, coordination efficiency, and scalable operation. This paper\nintroduces a comprehensive framework for advancing multi-agent systems through\nModel Context Protocol (MCP), addressing these challenges through standardized\ncontext sharing and coordination mechanisms. We extend previous work on AI\nagent architectures by developing a unified theoretical foundation, advanced\ncontext management techniques, and scalable coordination patterns. Through\ndetailed implementation case studies across enterprise knowledge management,\ncollaborative research, and distributed problem-solving domains, we demonstrate\nsignificant performance improvements compared to traditional approaches. Our\nevaluation methodology provides a systematic assessment framework with\nbenchmark tasks and datasets specifically designed for multi-agent systems. We\nidentify current limitations, emerging research opportunities, and potential\ntransformative applications across industries. This work contributes to the\nevolution of more capable, collaborative, and context-aware artificial\nintelligence systems that can effectively address complex real-world\nchallenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u534f\u8c03\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u7b49\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u8bc4\u4f30\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u534f\u8c03\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u6807\u51c6\u5316\u65b9\u6cd5\u63d0\u5347\u5176\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86MCP\u6846\u67b6\uff0c\u5305\u62ec\u7edf\u4e00\u7406\u8bba\u57fa\u7840\u3001\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u7ba1\u7406\u6280\u672f\u548c\u53ef\u6269\u5c55\u534f\u8c03\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u4f01\u4e1a\u77e5\u8bc6\u7ba1\u7406\u3001\u534f\u4f5c\u7814\u7a76\u548c\u5206\u5e03\u5f0f\u95ee\u9898\u89e3\u51b3\uff09\u4e2d\uff0cMCP\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "MCP\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u534f\u4f5c\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u73b0\u5b9e\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.21414", "pdf": "https://arxiv.org/pdf/2504.21414", "abs": "https://arxiv.org/abs/2504.21414", "authors": ["Qi Fan", "Kaiqi Liu", "Nian Liu", "Hisham Cholakkal", "Rao Muhammad Anwer", "Wenbin Li", "Yang Gao"], "title": "Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining", "categories": ["cs.CV"], "comment": null, "summary": "Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel\nclasses in new domains, which is often challenging due to the diverse\ncharacteristics of target domains and the limited availability of support data.\nMost CD-FSS methods redesign and retrain in-domain FSS models using various\ndomain-generalization techniques, which are effective but costly to train. To\naddress these issues, we propose adapting informative model structures of the\nwell-trained FSS model for target domains by learning domain characteristics\nfrom few-shot labeled support samples during inference, thereby eliminating the\nneed for retraining. Specifically, we first adaptively identify domain-specific\nmodel structures by measuring parameter importance using a novel structure\nFisher score in a data-dependent manner. Then, we progressively train the\nselected informative model structures with hierarchically constructed training\nsamples, progressing from fewer to more support shots. The resulting\nInformative Structure Adaptation (ISA) method effectively addresses domain\nshifts and equips existing well-trained in-domain FSS models with flexible\nadaptation capabilities for new domains, eliminating the need to redesign or\nretrain CD-FSS models on base data. Extensive experiments validate the\neffectiveness of our method, demonstrating superior performance across multiple\nCD-FSS benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff08ISA\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u578b\u7ed3\u6784\u6765\u89e3\u51b3\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u95ee\u9898\u3002", "motivation": "\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\uff08CD-FSS\uff09\u9762\u4e34\u76ee\u6807\u57df\u591a\u6837\u6027\u53ca\u652f\u6301\u6570\u636e\u6709\u9650\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u6602\u3002", "method": "\u901a\u8fc7\u7ed3\u6784Fisher\u8bc4\u5206\u81ea\u9002\u5e94\u8bc6\u522b\u57df\u7279\u5b9a\u6a21\u578b\u7ed3\u6784\uff0c\u5e76\u5206\u5c42\u8bad\u7ec3\u9009\u5b9a\u7684\u7ed3\u6784\uff0c\u9010\u6b65\u589e\u52a0\u652f\u6301\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cISA\u65b9\u6cd5\u5728\u591a\u4e2aCD-FSS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ISA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bbe\u8ba1\u6216\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2504.21032", "pdf": "https://arxiv.org/pdf/2504.21032", "abs": "https://arxiv.org/abs/2504.21032", "authors": ["Lior Limonad", "Fabiana Fournier", "Hadar Mulian", "George Manias", "Spiros Borotis", "Danai Kyrkou"], "title": "Selecting the Right LLM for eGov Explanations", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "8 pages, 7 figures. ICEDEG 2025, Bern, Switzerland, June 2025", "summary": "The perceived quality of the explanations accompanying e-government services\nis key to gaining trust in these institutions, consequently amplifying further\nusage of these services. Recent advances in generative AI, and concretely in\nLarge Language Models (LLMs) allow the automation of such content\narticulations, eliciting explanations' interpretability and fidelity, and more\ngenerally, adapting content to various audiences. However, selecting the right\nLLM type for this has become a non-trivial task for e-government service\nproviders. In this work, we adapted a previously developed scale to assist with\nthis selection, providing a systematic approach for the comparative analysis of\nthe perceived quality of explanations generated by various LLMs. We further\ndemonstrated its applicability through the tax-return process, using it as an\nexemplar use case that could benefit from employing an LLM to generate\nexplanations about tax refund decisions. This was attained through a user study\nwith 128 survey respondents who were asked to rate different versions of\nLLM-generated explanations about tax refund decisions, providing a\nmethodological basis for selecting the most appropriate LLM. Recognizing the\npractical challenges of conducting such a survey, we also began exploring the\nautomation of this process by attempting to replicate human feedback using a\nselection of cutting-edge predictive techniques.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u751f\u6210\u5f0fAI\uff08\u5982LLMs\uff09\u4e3a\u7535\u5b50\u653f\u52a1\u670d\u52a1\u63d0\u4f9b\u9ad8\u8d28\u91cf\u89e3\u91ca\uff0c\u4ee5\u589e\u5f3a\u4fe1\u4efb\u548c\u4f7f\u7528\u3002\u7814\u7a76\u901a\u8fc7\u7528\u6237\u8c03\u67e5\u6bd4\u8f83\u4e0d\u540cLLM\u751f\u6210\u89e3\u91ca\u7684\u8d28\u91cf\uff0c\u5e76\u5c1d\u8bd5\u81ea\u52a8\u5316\u8bc4\u4f30\u8fc7\u7a0b\u3002", "motivation": "\u7535\u5b50\u653f\u52a1\u670d\u52a1\u7684\u89e3\u91ca\u8d28\u91cf\u5bf9\u7528\u6237\u4fe1\u4efb\u548c\u4f7f\u7528\u81f3\u5173\u91cd\u8981\uff0c\u800c\u751f\u6210\u5f0fAI\uff08\u5982LLMs\uff09\u53ef\u4ee5\u81ea\u52a8\u5316\u751f\u6210\u89e3\u91ca\uff0c\u4f46\u9009\u62e9\u5408\u9002\u7684LLM\u7c7b\u578b\u6210\u4e3a\u6311\u6218\u3002", "method": "\u7814\u7a76\u6539\u7f16\u4e86\u4e00\u4e2a\u73b0\u6709\u91cf\u8868\uff0c\u7528\u4e8e\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540cLLM\u751f\u6210\u89e3\u91ca\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u4ee5\u7a0e\u52a1\u8fd4\u8fd8\u4e3a\u4f8b\u8fdb\u884c\u7528\u6237\u8c03\u67e5\uff08128\u540d\u53d7\u8bbf\u8005\uff09\u3002\u540c\u65f6\u63a2\u7d22\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u7684\u53ef\u80fd\u6027\u3002", "result": "\u901a\u8fc7\u7528\u6237\u8c03\u67e5\u63d0\u4f9b\u4e86\u9009\u62e9\u6700\u9002\u5408LLM\u7684\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u5e76\u5c1d\u8bd5\u7528\u9884\u6d4b\u6280\u672f\u590d\u5236\u4eba\u7c7b\u53cd\u9988\u4ee5\u81ea\u52a8\u5316\u8bc4\u4f30\u8fc7\u7a0b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7535\u5b50\u653f\u52a1\u670d\u52a1\u63d0\u4f9b\u8005\u9009\u62e9LLM\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u5b9e\u9645\u6311\u6218\u3002"}}
{"id": "2504.21423", "pdf": "https://arxiv.org/pdf/2504.21423", "abs": "https://arxiv.org/abs/2504.21423", "authors": ["Weicai Yan", "Wang Lin", "Zirun Guo", "Ye Wang", "Fangming Feng", "Xiaoda Yang", "Zehan Wang", "Tao Jin"], "title": "Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision", "categories": ["cs.CV"], "comment": "Accepted at ICLR 2025", "summary": "Prompt learning has demonstrated promising results in fine-tuning pre-trained\nmultimodal models. However, the performance improvement is limited when applied\nto more complex and fine-grained tasks. The reason is that most existing\nmethods directly optimize the parameters involved in the prompt generation\nprocess through loss backpropagation, which constrains the richness and\nspecificity of the prompt representations. In this paper, we propose\nDiffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion\nmodel to generate rich and fine-grained prompt information for complex\ndownstream tasks. Specifically, our approach consists of three stages. In the\nfirst stage, we train a Mask-VAE to compress the masks into latent space. In\nthe second stage, we leverage an improved Diffusion Transformer (DiT) to train\na prompt generator in the latent space, using the masks for supervision. In the\nthird stage, we align the denoising process of the prompt generator with the\npre-trained model in the semantic space, and use the generated prompts to\nfine-tune the model. We conduct experiments on a complex pixel-level downstream\ntask, referring expression comprehension, and compare our method with various\nparameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum\nimprovement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model\nand also outperforms other state-of-the-art methods across multiple metrics.\nThe experimental results validate the effectiveness of our approach and\nhighlight the potential of using generative models for prompt generation. Code\nis available at https://github.com/Kelvin-ywc/diff-prompt.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiff-Prompt\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u4e30\u5bcc\u4e14\u7ec6\u7c92\u5ea6\u7684\u63d0\u793a\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u590d\u6742\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u56e0\u5176\u76f4\u63a5\u901a\u8fc7\u635f\u5931\u53cd\u5411\u4f20\u64ad\u4f18\u5316\u63d0\u793a\u751f\u6210\u53c2\u6570\uff0c\u9650\u5236\u4e86\u63d0\u793a\u7684\u4e30\u5bcc\u6027\u548c\u7279\u5f02\u6027\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a1) \u8bad\u7ec3Mask-VAE\u538b\u7f29\u63a9\u7801\u5230\u9690\u7a7a\u95f4\uff1b2) \u7528\u6539\u8fdb\u7684DiT\u8bad\u7ec3\u9690\u7a7a\u95f4\u63d0\u793a\u751f\u6210\u5668\uff1b3) \u5728\u8bed\u4e49\u7a7a\u95f4\u5bf9\u9f50\u63d0\u793a\u751f\u6210\u5668\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728\u590d\u6742\u50cf\u7d20\u7ea7\u4efb\u52a1\u4e2d\uff0cDiff-Prompt\u5728R@1\u548cR@5\u4e0a\u5206\u522b\u63d0\u53478.87\u548c14.05\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Diff-Prompt\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u63d0\u793a\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.21033", "pdf": "https://arxiv.org/pdf/2504.21033", "abs": "https://arxiv.org/abs/2504.21033", "authors": ["Majid Behravan", "Maryam Haghani", "Denis Gracanin"], "title": "Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Traditional 3D modeling requires technical expertise, specialized software,\nand time-intensive processes, making it inaccessible for many users. Our\nresearch aims to lower these barriers by combining generative AI and augmented\nreality (AR) into a cohesive system that allows users to easily generate,\nmanipulate, and interact with 3D models in real time, directly within AR\nenvironments. Utilizing cutting-edge AI models like Shap-E, we address the\ncomplex challenges of transforming 2D images into 3D representations in AR\nenvironments. Key challenges such as object isolation, handling intricate\nbackgrounds, and achieving seamless user interaction are tackled through\nadvanced object detection methods, such as Mask R-CNN. Evaluation results from\n35 participants reveal an overall System Usability Scale (SUS) score of 69.64,\nwith participants who engaged with AR/VR technologies more frequently rating\nthe system significantly higher, at 80.71. This research is particularly\nrelevant for applications in gaming, education, and AR-based e-commerce,\noffering intuitive, model creation for users without specialized skills.", "AI": {"tldr": "\u7ed3\u5408\u751f\u6210\u5f0fAI\u548c\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b80\u53163D\u5efa\u6a21\u7684\u7cfb\u7edf\uff0c\u4f7f\u975e\u4e13\u4e1a\u7528\u6237\u4e5f\u80fd\u8f7b\u677e\u751f\u6210\u548c\u64cd\u4f5c3D\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf3D\u5efa\u6a21\u6280\u672f\u95e8\u69db\u9ad8\uff0c\u9700\u8981\u4e13\u4e1a\u8f6f\u4ef6\u548c\u6280\u80fd\uff0c\u9650\u5236\u4e86\u666e\u901a\u7528\u6237\u7684\u4f7f\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u548cAR\u6280\u672f\u964d\u4f4e\u8fd9\u4e00\u95e8\u69db\u3002", "method": "\u5229\u7528Shap-E\u7b49AI\u6a21\u578b\u548cMask R-CNN\u7b49\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4ece2D\u56fe\u50cf\u751f\u62103D\u6a21\u578b\u7684\u590d\u6742\u95ee\u9898\u3002", "result": "35\u540d\u53c2\u4e0e\u8005\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u7cfb\u7edf\u53ef\u7528\u6027\u8bc4\u5206\uff08SUS\uff09\u4e3a69.64\uff0c\u719f\u6089AR/VR\u6280\u672f\u7684\u7528\u6237\u8bc4\u5206\u66f4\u9ad8\uff0880.71\uff09\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u6e38\u620f\u3001\u6559\u80b2\u548cAR\u7535\u5546\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u63d0\u4f9b\u4e86\u76f4\u89c2\u76843D\u5efa\u6a21\u5de5\u5177\u3002"}}
{"id": "2504.21435", "pdf": "https://arxiv.org/pdf/2504.21435", "abs": "https://arxiv.org/abs/2504.21435", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "ShaoGuo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "29 pages, 15 figures, CVPR 2025", "summary": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\n\\textbf{standalone} videos and mainly assess ``visual elements'' like human\nactions and object states. In reality, contemporary videos often encompass\ncomplex and continuous narratives, typically presented as a \\textbf{series}. To\naddress this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting\nof 105 carefully curated narrative-driven series, covering 28 specialized tasks\nthat require deep narrative understanding. Specifically, we first select a\ndiverse set of drama series spanning various genres. Then, we introduce a novel\nlong-span narrative annotation method, combined with a full-information\ntransformation approach to convert manual annotations into diverse task\nformats. To further enhance model capacity for detailed analysis of plot\nstructures and character relationships within series, we propose a novel\nnarrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on\n\\textbf{SeriesBench} indicate that existing MLLMs still face significant\nchallenges in understanding narrative-driven series, while \\textbf{PC-DCoT}\nenables these MLLMs to achieve performance improvements. Overall, our\n\\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of\nadvancing model capabilities to understand narrative-driven series, guiding the\nfuture development of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SeriesBench\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u53d9\u4e8b\u9a71\u52a8\u89c6\u9891\u7cfb\u5217\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86PC-DCoT\u6846\u67b6\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u53d9\u4e8b\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u6d4b\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u72ec\u7acb\u89c6\u9891\u7684\u89c6\u89c9\u5143\u7d20\uff0c\u800c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e2d\u7684\u590d\u6742\u8fde\u7eed\u53d9\u4e8b\u3002", "method": "\u901a\u8fc7\u7cbe\u9009\u591a\u6837\u5316\u7684\u5267\u96c6\uff0c\u7ed3\u5408\u957f\u8de8\u5ea6\u53d9\u4e8b\u6807\u6ce8\u65b9\u6cd5\u548c\u5168\u4fe1\u606f\u8f6c\u6362\u6280\u672f\uff0c\u6784\u5efaSeriesBench\uff1b\u63d0\u51faPC-DCoT\u6846\u67b6\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u5267\u60c5\u7ed3\u6784\u548c\u89d2\u8272\u5173\u7cfb\u7684\u5206\u6790\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u53d9\u4e8b\u7406\u89e3\u4e0a\u4ecd\u6709\u6311\u6218\uff0c\u800cPC-DCoT\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SeriesBench\u548cPC-DCoT\u5f3a\u8c03\u4e86\u63d0\u5347\u6a21\u578b\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.21034", "pdf": "https://arxiv.org/pdf/2504.21034", "abs": "https://arxiv.org/abs/2504.21034", "authors": ["Georgios Syros", "Anshuman Suri", "Cristina Nita-Rotaru", "Alina Oprea"], "title": "SAGA: A Security Architecture for Governing AI Agentic Systems", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management. To address this gap, we propose SAGA,\na Security Architecture for Governing Agentic systems, that offers user\noversight over their agents' lifecycle. In our design, users register their\nagents with a central entity, the Provider, that maintains agents contact\ninformation, user-defined access control policies, and helps agents enforce\nthese policies on inter-agent communication. We introduce a cryptographic\nmechanism for deriving access control tokens, that offers fine-grained control\nover an agent's interaction with other agents, balancing security and\nperformance consideration. We evaluate SAGA on several agentic tasks, using\nagents in different geolocations, and multiple on-device and cloud LLMs,\ndemonstrating minimal performance overhead with no impact on underlying task\nutility in a wide range of conditions. Our architecture enables secure and\ntrustworthy deployment of autonomous agents, accelerating the responsible\nadoption of this technology in sensitive environments.", "AI": {"tldr": "SAGA\u662f\u4e00\u4e2a\u7528\u4e8e\u7ba1\u7406\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u67b6\u6784\uff0c\u63d0\u4f9b\u7528\u6237\u5bf9\u5176\u4ee3\u7406\u751f\u547d\u5468\u671f\u7684\u76d1\u7763\uff0c\u786e\u4fdd\u5b89\u5168\u548c\u53ef\u4fe1\u7684\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\u7f3a\u4e4f\u7528\u6237\u63a7\u5236\u548c\u7ba1\u7406\uff0c\u53ef\u80fd\u5bfc\u81f4\u6076\u610f\u4ee3\u7406\u7684\u6f5c\u5728\u635f\u5bb3\u3002", "method": "\u63d0\u51faSAGA\u67b6\u6784\uff0c\u901a\u8fc7\u4e2d\u592e\u5b9e\u4f53\uff08Provider\uff09\u7ba1\u7406\u4ee3\u7406\u6ce8\u518c\u3001\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u52a0\u5bc6\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "\u5728\u4e0d\u540c\u5730\u7406\u4f4d\u7f6e\u548c\u591a\u79cdLLM\u4e0a\u8bc4\u4f30\uff0c\u663e\u793a\u6027\u80fd\u5f00\u9500\u6700\u5c0f\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6548\u7528\u3002", "conclusion": "SAGA\u652f\u6301\u5b89\u5168\u548c\u53ef\u4fe1\u7684\u81ea\u4e3b\u4ee3\u7406\u90e8\u7f72\uff0c\u4fc3\u8fdb\u654f\u611f\u73af\u5883\u4e2d\u8be5\u6280\u672f\u7684\u8d1f\u8d23\u4efb\u91c7\u7528\u3002"}}
{"id": "2504.21447", "pdf": "https://arxiv.org/pdf/2504.21447", "abs": "https://arxiv.org/abs/2504.21447", "authors": ["Haoran Chen", "Junyan Lin", "Xinhao Chen", "Yue Fan", "Xin Jin", "Hui Su", "Jianfeng Dong", "Jinlan Fu", "Xiaoyu Shen"], "title": "Rethinking Visual Layer Selection in Multimodal LLMs", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 4 figures, submitted to ICCV 2025", "summary": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8868\u793a\u76f8\u4f3c\u6027\u65b9\u6cd5\uff0c\u5c06CLIP-ViT\u7684\u89c6\u89c9\u5c42\u5206\u4e3a\u6d45\u5c42\u3001\u4e2d\u5c42\u548c\u6df1\u5c42\uff0c\u5e76\u7814\u7a76\u4e86\u5b83\u4eec\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6027\u80fd\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u5c42\u6b21\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u800c\u8f7b\u91cf\u7ea7\u878d\u5408\u591a\u5c42\u7279\u5f81\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLM\u901a\u5e38\u57fa\u4e8e\u7ecf\u9a8c\u9009\u62e9\u89c6\u89c9\u7279\u5f81\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u5c42\u7814\u7a76\uff0c\u4f18\u5316\u89c6\u89c9\u7279\u5f81\u9009\u62e9\uff0c\u63d0\u5347MLLM\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8868\u793a\u76f8\u4f3c\u6027\u65b9\u6cd5\uff0c\u5c06CLIP-ViT\u5c42\u5206\u4e3a\u6d45\u3001\u4e2d\u3001\u6df1\u4e09\u7c7b\uff0c\u5e76\u5728\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684LLaVA\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a(1)\u6df1\u5c42\u7279\u5f81\u5bf9OCR\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff1b(2)\u6d45\u5c42\u548c\u4e2d\u5c42\u7279\u5f81\u5728\u8ba1\u6570\u3001\u5b9a\u4f4d\u7b49\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff1b(3)\u591a\u5c42\u7279\u5f81\u878d\u5408\u5728\u591a\u6570\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5355\u5c42\u9009\u62e9\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\u4e86MLLM\u4e2d\u7684\u89c6\u89c9\u5c42\u9009\u62e9\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.21036", "pdf": "https://arxiv.org/pdf/2504.21036", "abs": "https://arxiv.org/abs/2504.21036", "authors": ["Hao Du", "Shang Liu", "Yang Cao"], "title": "Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) has become an essential strategy for\nadapting them to specialized tasks; however, this process introduces\nsignificant privacy challenges, as sensitive training data may be inadvertently\nmemorized and exposed. Although differential privacy (DP) offers strong\ntheoretical guarantees against such leakage, its empirical privacy\neffectiveness on LLMs remains unclear, especially under different fine-tuning\nmethods. In this paper, we systematically investigate the impact of DP across\nfine-tuning methods and privacy budgets, using both data extraction and\nmembership inference attacks to assess empirical privacy risks. Our main\nfindings are as follows: (1) Differential privacy reduces model utility, but\nits impact varies significantly across different fine-tuning methods. (2)\nWithout DP, the privacy risks of models fine-tuned with different approaches\ndiffer considerably. (3) When DP is applied, even a relatively high privacy\nbudget can substantially lower privacy risk. (4) The privacy-utility trade-off\nunder DP training differs greatly among fine-tuning methods, with some methods\nbeing unsuitable for DP due to severe utility degradation. Our results provide\npractical guidance for privacy-conscious deployment of LLMs and pave the way\nfor future research on optimizing the privacy-utility trade-off in fine-tuning\nmethodologies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u5728\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u4e2d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9690\u79c1\u4fdd\u62a4\u7684\u6548\u679c\uff0c\u53d1\u73b0DP\u80fd\u663e\u8457\u964d\u4f4e\u9690\u79c1\u98ce\u9669\uff0c\u4f46\u5bf9\u6a21\u578b\u6548\u7528\u7684\u5f71\u54cd\u56e0\u65b9\u6cd5\u800c\u5f02\u3002", "motivation": "\u63a2\u8ba8DP\u5728LLM\u5fae\u8c03\u4e2d\u7684\u5b9e\u9645\u9690\u79c1\u4fdd\u62a4\u6548\u679c\uff0c\u4ee5\u53ca\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u5bf9\u9690\u79c1-\u6548\u7528\u6743\u8861\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6570\u636e\u63d0\u53d6\u548c\u6210\u5458\u63a8\u7406\u653b\u51fb\u8bc4\u4f30DP\u5728\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u548c\u9690\u79c1\u9884\u7b97\u4e0b\u7684\u9690\u79c1\u98ce\u9669\u3002", "result": "DP\u964d\u4f4e\u6a21\u578b\u6548\u7528\uff0c\u4f46\u9690\u79c1\u98ce\u9669\u663e\u8457\u964d\u4f4e\uff1b\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u5bf9\u9690\u79c1-\u6548\u7528\u6743\u8861\u5f71\u54cd\u5dee\u5f02\u5927\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9690\u79c1\u654f\u611f\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u63a8\u52a8\u672a\u6765\u4f18\u5316\u9690\u79c1-\u6548\u7528\u6743\u8861\u7684\u7814\u7a76\u3002"}}
{"id": "2504.21464", "pdf": "https://arxiv.org/pdf/2504.21464", "abs": "https://arxiv.org/abs/2504.21464", "authors": ["Shamim Rahim Refat", "Ziyan Shirin Raha", "Shuvashis Sarker", "Faika Fairuj Preotee", "MD. Musfikur Rahman", "Tashreef Muhammad", "Mohammad Shafiul Islam"], "title": "VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification", "categories": ["cs.CV"], "comment": "33 pages, 49 figures", "summary": "Diabetic retinopathy is a severe eye condition caused by diabetes where the\nretinal blood vessels get damaged and can lead to vision loss and blindness if\nnot treated. Early and accurate detection is key to intervention and stopping\nthe disease progressing. For addressing this disease properly, this paper\npresents a comprehensive approach for automated diabetic retinopathy detection\nby proposing a new hybrid deep learning model called VR-FuseNet. Diabetic\nretinopathy is a major eye disease and leading cause of blindness especially\namong diabetic patients so accurate and efficient automated detection methods\nare required. To address the limitations of existing methods including dataset\nimbalance, diversity and generalization issues this paper presents a hybrid\ndataset created from five publicly available diabetic retinopathy datasets.\nEssential preprocessing techniques such as SMOTE for class balancing and CLAHE\nfor image enhancement are applied systematically to the dataset to improve the\nrobustness and generalizability of the dataset. The proposed VR-FuseNet model\ncombines the strengths of two state-of-the-art convolutional neural networks,\nVGG19 which captures fine-grained spatial features and ResNet50V2 which is\nknown for its deep hierarchical feature extraction. This fusion improves the\ndiagnostic performance and achieves an accuracy of 91.824%. The model\noutperforms individual architectures on all performance metrics demonstrating\nthe effectiveness of hybrid feature extraction in Diabetic Retinopathy\nclassification tasks. To make the proposed model more clinically useful and\ninterpretable this paper incorporates multiple XAI techniques. These techniques\ngenerate visual explanations that clearly indicate the retinal features\naffecting the model's prediction such as microaneurysms, hemorrhages and\nexudates so that clinicians can interpret and validate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVR-FuseNet\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u68c0\u6d4b\uff0c\u7ed3\u5408\u4e86VGG19\u548cResNet50V2\u7684\u4f18\u52bf\uff0c\u51c6\u786e\u7387\u8fbe91.824%\uff0c\u5e76\u901a\u8fc7XAI\u6280\u672f\u63d0\u5347\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5bfc\u81f4\u7cd6\u5c3f\u75c5\u60a3\u8005\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u51c6\u786e\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u521b\u5efa\u6df7\u5408\u6570\u636e\u96c6\uff0c\u5e94\u7528SMOTE\u548cCLAHE\u8fdb\u884c\u9884\u5904\u7406\uff0c\u63d0\u51faVR-FuseNet\u6a21\u578b\uff0c\u7ed3\u5408VGG19\u548cResNet50V2\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "VR-FuseNet\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8fbe91.824%\uff0c\u4f18\u4e8e\u5355\u4e00\u67b6\u6784\u3002", "conclusion": "VR-FuseNet\u901a\u8fc7\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u548cXAI\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.21037", "pdf": "https://arxiv.org/pdf/2504.21037", "abs": "https://arxiv.org/abs/2504.21037", "authors": ["Farnaz Soltaniani", "Mohammad Ghafari", "Mohammed Sayagh"], "title": "Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Early detection of security bug reports (SBRs) is crucial for preventing\nvulnerabilities and ensuring system reliability. While machine learning models\nhave been developed for SBR prediction, their predictive performance still has\nroom for improvement. In this study, we conduct a comprehensive comparison\nbetween BERT and Random Forest (RF), a competitive baseline for predicting\nSBRs. The results show that RF outperforms BERT with a 34% higher average\nG-measure for within-project predictions. Adding only SBRs from various\nprojects improves both models' average performance. However, including both\nsecurity and nonsecurity bug reports significantly reduces RF's average\nperformance to 46%, while boosts BERT to its best average performance of 66%,\nsurpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%\nG-measure, which is substantially higher than RF.", "AI": {"tldr": "\u6bd4\u8f83BERT\u548c\u968f\u673a\u68ee\u6797\uff08RF\uff09\u5728\u5b89\u5168\u6f0f\u6d1e\u62a5\u544a\uff08SBR\uff09\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0RF\u5728\u9879\u76ee\u5185\u9884\u6d4b\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u800cBERT\u5728\u8de8\u9879\u76ee\u9884\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8eRF\u3002", "motivation": "\u63d0\u9ad8\u5b89\u5168\u6f0f\u6d1e\u62a5\u544a\u7684\u65e9\u671f\u68c0\u6d4b\u80fd\u529b\uff0c\u4ee5\u9884\u9632\u6f0f\u6d1e\u5e76\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "method": "\u5bf9BERT\u548c\u968f\u673a\u68ee\u6797\uff08RF\uff09\u8fdb\u884c\u7efc\u5408\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u9879\u76ee\u5185\u548c\u8de8\u9879\u76eeSBR\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "result": "RF\u5728\u9879\u76ee\u5185\u9884\u6d4b\u4e2d\u5e73\u5747G-measure\u6bd4BERT\u9ad834%\uff0c\u800cBERT\u5728\u8de8\u9879\u76ee\u9884\u6d4b\u4e2d\u8fbe\u523062%\u7684G-measure\uff0c\u663e\u8457\u4f18\u4e8eRF\u3002", "conclusion": "BERT\u5728\u8de8\u9879\u76eeSBR\u9884\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800cRF\u5728\u9879\u76ee\u5185\u9884\u6d4b\u4e2d\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2504.21467", "pdf": "https://arxiv.org/pdf/2504.21467", "abs": "https://arxiv.org/abs/2504.21467", "authors": ["Luc Vedrenne", "Sylvain Faisan", "Denis Fortun"], "title": "Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space", "categories": ["cs.CV"], "comment": "14 pages, 19 figures, IEEE Transactions on Image Processing", "summary": "Point cloud rigid registration is a fundamental problem in 3D computer\nvision. In the multiview case, we aim to find a set of 6D poses to align a set\nof objects. Methods based on pairwise registration rely on a subsequent\nsynchronization algorithm, which makes them poorly scalable with the number of\nviews. Generative approaches overcome this limitation, but are based on\nGaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,\nthey are not well suited to handle large transformations. Moreover, most\nexisting methods cannot handle high levels of degradations. In this paper, we\nintroduce POLAR (POint cloud LAtent Registration), a multiview registration\nmethod able to efficiently deal with a large number of views, while being\nrobust to a high level of degradations and large initial angles. To achieve\nthis, we transpose the registration problem into the latent space of a\npretrained autoencoder, design a loss taking degradations into account, and\ndevelop an efficient multistart optimization strategy. Our proposed method\nsignificantly outperforms state-of-the-art approaches on synthetic and real\ndata. POLAR is available at github.com/pypolar/polar or as a standalone package\nwhich can be installed with pip install polaregistration.", "AI": {"tldr": "POLAR\u662f\u4e00\u79cd\u591a\u89c6\u89d2\u70b9\u4e91\u521a\u6027\u914d\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u8f6c\u6362\u548c\u4f18\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c6\u89d2\u53d8\u6362\u548c\u9ad8\u9000\u5316\u60c5\u51b5\u4e0b\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u914d\u51c6\u4e2d\u96be\u4ee5\u5904\u7406\u5927\u89c6\u89d2\u53d8\u6362\u548c\u9ad8\u9000\u5316\u60c5\u51b5\uff0c\u4e14\u6269\u5c55\u6027\u5dee\u3002", "method": "\u5c06\u914d\u51c6\u95ee\u9898\u8f6c\u6362\u5230\u9884\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u8003\u8651\u9000\u5316\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u591a\u8d77\u70b9\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "POLAR\u9ad8\u6548\u3001\u9c81\u68d2\uff0c\u9002\u7528\u4e8e\u591a\u89c6\u89d2\u70b9\u4e91\u914d\u51c6\u3002"}}
{"id": "2504.21038", "pdf": "https://arxiv.org/pdf/2504.21038", "abs": "https://arxiv.org/abs/2504.21038", "authors": ["Yakai Li", "Jiekang Hu", "Weiduan Sang", "Luping Ma", "Jing Xie", "Weijuan Zhang", "Aimin Yu", "Shijie Zhao", "Qingjia Huang", "Qihang Zhou"], "title": "Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are designed to generate helpful and safe\ncontent. However, adversarial attacks, commonly referred to as jailbreak, can\nbypass their safety protocols, prompting LLMs to generate harmful content or\nreveal sensitive data. Consequently, investigating jailbreak methodologies is\ncrucial for exposing systemic vulnerabilities within LLMs, ultimately guiding\nthe continuous implementation of security enhancements by developers. In this\npaper, we introduce a novel jailbreak attack method that leverages the\nprefilling feature of LLMs, a feature designed to enhance model output\nconstraints. Unlike traditional jailbreak methods, the proposed attack\ncircumvents LLMs' safety mechanisms by directly manipulating the probability\ndistribution of subsequent tokens, thereby exerting control over the model's\noutput. We propose two attack variants: Static Prefilling (SP), which employs a\nuniversal prefill text, and Optimized Prefilling (OP), which iteratively\noptimizes the prefill text to maximize the attack success rate. Experiments on\nsix state-of-the-art LLMs using the AdvBench benchmark validate the\neffectiveness of our method and demonstrate its capability to substantially\nenhance attack success rates when combined with existing jailbreak approaches.\nThe OP method achieved attack success rates of up to 99.82% on certain models,\nsignificantly outperforming baseline methods. This work introduces a new\njailbreak attack method in LLMs, emphasizing the need for robust content\nvalidation mechanisms to mitigate the adversarial exploitation of prefilling\nfeatures. All code and data used in this paper are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u586b\u5145\u7279\u6027\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u7814\u7a76LLM\u7684\u8d8a\u72f1\u65b9\u6cd5\u4ee5\u63ed\u793a\u7cfb\u7edf\u6f0f\u6d1e\uff0c\u6307\u5bfc\u5f00\u53d1\u8005\u589e\u5f3a\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u653b\u51fb\u53d8\u4f53\uff1a\u9759\u6001\u9884\u586b\u5145\uff08SP\uff09\u548c\u4f18\u5316\u9884\u586b\u5145\uff08OP\uff09\uff0c\u901a\u8fc7\u64cd\u7eb5\u4ee4\u724c\u6982\u7387\u5206\u5e03\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u5728\u516d\u79cd\u5148\u8fdbLLM\u4e0a\u9a8c\u8bc1\uff0cOP\u65b9\u6cd5\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe99.82%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u52a0\u5f3a\u5185\u5bb9\u9a8c\u8bc1\u673a\u5236\u4ee5\u9632\u8303\u9884\u586b\u5145\u7279\u6027\u7684\u5bf9\u6297\u6027\u5229\u7528\u3002"}}
{"id": "2504.21468", "pdf": "https://arxiv.org/pdf/2504.21468", "abs": "https://arxiv.org/abs/2504.21468", "authors": ["Yu Guo", "Guoqing Chen", "Tieyong Zeng", "Qiyu Jin", "Michael Kwok-Po Ng"], "title": "Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion", "categories": ["cs.CV", "65F35, 90C30, 94A08, 68U10"], "comment": null, "summary": "Recovering hidden structures from incomplete or noisy data remains a\npervasive challenge across many fields, particularly where multi-dimensional\ndata representation is essential. Quaternion matrices, with their ability to\nnaturally model multi-dimensional data, offer a promising framework for this\nproblem. This paper introduces the quaternion nuclear norm over the Frobenius\nnorm (QNOF) as a novel nonconvex approximation for the rank of quaternion\nmatrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion\nsingular value decomposition, we prove that solving the QNOF can be simplified\nto solving the singular value $L_1/L_2$ problem. Additionally, we extend the\nQNOF to robust quaternion matrix completion, employing the alternating\ndirection multiplier method to derive solutions that guarantee weak convergence\nunder mild conditions. Extensive numerical experiments validate the proposed\nmodel's superiority, consistently outperforming state-of-the-art quaternion\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u51f8\u8fd1\u4f3c\u65b9\u6cd5QNOF\uff0c\u7528\u4e8e\u6062\u590d\u56db\u5143\u6570\u77e9\u9635\u7684\u79e9\uff0c\u5e76\u6269\u5c55\u81f3\u9c81\u68d2\u77e9\u9635\u8865\u5168\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u7ef4\u6570\u636e\u8868\u793a\u4e2d\uff0c\u4ece\u4e0d\u5b8c\u6574\u6216\u566a\u58f0\u6570\u636e\u4e2d\u6062\u590d\u9690\u85cf\u7ed3\u6784\u662f\u4e00\u4e2a\u666e\u904d\u6311\u6218\uff0c\u56db\u5143\u6570\u77e9\u9635\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002", "method": "\u5f15\u5165QNOF\u4f5c\u4e3a\u56db\u5143\u6570\u77e9\u9635\u79e9\u7684\u975e\u51f8\u8fd1\u4f3c\uff0c\u5229\u7528\u56db\u5143\u6570\u5947\u5f02\u503c\u5206\u89e3\u7b80\u5316\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u81f3\u9c81\u68d2\u77e9\u9635\u8865\u5168\uff0c\u4f7f\u7528\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\u6c42\u89e3\u3002", "result": "QNOF\u53c2\u6570\u65e0\u5173\u4e14\u5c3a\u5ea6\u4e0d\u53d8\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u6570\u503c\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u56db\u5143\u6570\u65b9\u6cd5\u3002", "conclusion": "QNOF\u4e3a\u56db\u5143\u6570\u77e9\u9635\u79e9\u6062\u590d\u548c\u9c81\u68d2\u8865\u5168\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4f18\u52bf\u3002"}}
{"id": "2504.21039", "pdf": "https://arxiv.org/pdf/2504.21039", "abs": "https://arxiv.org/abs/2504.21039", "authors": ["Paul Kassianik", "Baturay Saglam", "Alexander Chen", "Blaine Nelson", "Anu Vellore", "Massimo Aufiero", "Fraser Burch", "Dhruv Kedia", "Avi Zohary", "Sajana Weerawardhena", "Aman Priyanshu", "Adam Swanda", "Amy Chang", "Hyrum Anderson", "Kojin Oshiba", "Omar Santos", "Yaron Singer", "Amin Karbasi"], "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As transformer-based large language models (LLMs) increasingly permeate\nsociety, they have revolutionized domains such as software engineering,\ncreative writing, and digital arts. However, their adoption in cybersecurity\nremains limited due to challenges like scarcity of specialized training data\nand complexity of representing cybersecurity-specific knowledge. To address\nthese gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on\nthe Llama 3.1 architecture and enhanced through continued pretraining on a\ncarefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across\nboth established and new cybersecurity benchmarks, showing that it matches\nLlama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By\nreleasing our model to the public, we aim to accelerate progress and adoption\nof AI-driven tools in both public and private cybersecurity contexts.", "AI": {"tldr": "Foundation-Sec-8B\u662f\u4e00\u4e2a\u57fa\u4e8eLlama 3.1\u67b6\u6784\u7684\u7f51\u7edc\u5b89\u5168\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u8bad\u7ec3\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u77e5\u8bc6\u8868\u793a\u95ee\u9898\uff0c\u6027\u80fd\u5ab2\u7f8e\u4e3b\u6d41\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u53d7\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u4e13\u4e1a\u6570\u636e\u548c\u77e5\u8bc6\u8868\u793a\u7684\u590d\u6742\u6027\u3002", "method": "\u57fa\u4e8eLlama 3.1\u67b6\u6784\uff0c\u901a\u8fc7\u7cbe\u5fc3\u7b5b\u9009\u7684\u7f51\u7edc\u5b89\u5168\u8bed\u6599\u5e93\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0eLlama 3.1-70B\u548cGPT-4o-mini\u76f8\u5f53\u3002", "conclusion": "\u516c\u5f00\u6a21\u578b\u4ee5\u63a8\u52a8AI\u5de5\u5177\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2504.21472", "pdf": "https://arxiv.org/pdf/2504.21472", "abs": "https://arxiv.org/abs/2504.21472", "authors": ["Jingjing Liu", "Nian Wu", "Xianchao Xiu", "Jianhua Zhang"], "title": "Robust Orthogonal NMF with Label Propagation for Image Clustering", "categories": ["cs.CV"], "comment": null, "summary": "Non-negative matrix factorization (NMF) is a popular unsupervised learning\napproach widely used in image clustering. However, in real-world clustering\nscenarios, most existing NMF methods are highly sensitive to noise corruption\nand are unable to effectively leverage limited supervised information. To\novercome these drawbacks, we propose a unified non-convex framework with label\npropagation called robust orthogonal nonnegative matrix factorization (RONMF).\nThis method not only considers the graph Laplacian and label propagation as\nregularization terms but also introduces a more effective non-convex structure\nto measure the reconstruction error and imposes orthogonal constraints on the\nbasis matrix to reduce the noise corruption, thereby achieving higher\nrobustness. To solve RONMF, we develop an alternating direction method of\nmultipliers (ADMM)-based optimization algorithm. In particular, all subproblems\nhave closed-form solutions, which ensures its efficiency. Experimental\nevaluations on eight public image datasets demonstrate that the proposed RONMF\noutperforms state-of-the-art NMF methods across various standard metrics and\nshows excellent robustness. The code will be available at\nhttps://github.com/slinda-liu.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRONMF\u7684\u9c81\u68d2\u6b63\u4ea4\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709NMF\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\u4e14\u96be\u4ee5\u5229\u7528\u6709\u9650\u76d1\u7763\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709NMF\u65b9\u6cd5\u5728\u56fe\u50cf\u805a\u7c7b\u4e2d\u5bf9\u566a\u58f0\u654f\u611f\u4e14\u96be\u4ee5\u5229\u7528\u76d1\u7763\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faRONMF\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u62c9\u666e\u62c9\u65af\u548c\u6807\u7b7e\u4f20\u64ad\u4f5c\u4e3a\u6b63\u5219\u9879\uff0c\u5f15\u5165\u975e\u51f8\u7ed3\u6784\u6d4b\u91cf\u91cd\u6784\u8bef\u5dee\uff0c\u5e76\u65bd\u52a0\u6b63\u4ea4\u7ea6\u675f\u4ee5\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u3002\u91c7\u7528ADMM\u4f18\u5316\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u5728\u516b\u4e2a\u516c\u5171\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRONMF\u5728\u591a\u79cd\u6807\u51c6\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709NMF\u65b9\u6cd5\uff0c\u5e76\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "RONMF\u901a\u8fc7\u7ed3\u5408\u6b63\u5219\u5316\u548c\u975e\u51f8\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86NMF\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.21042", "pdf": "https://arxiv.org/pdf/2504.21042", "abs": "https://arxiv.org/abs/2504.21042", "authors": ["Jiamin Chang", "Haoyang Li", "Hammond Pearce", "Ruoxi Sun", "Bo Li", "Minhui Xue"], "title": "What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Accept By The ACM Conference on Computer and Communications Security\n  (CCS) 2025", "summary": "The growing adoption of artificial intelligence (AI) has amplified concerns\nabout trustworthiness, including integrity, privacy, robustness, and bias. To\nassess and attribute these threats, we propose ConceptLens, a generic framework\nthat leverages pre-trained multimodal models to identify the root causes of\nintegrity threats by analyzing Concept Shift in probing samples. ConceptLens\ndemonstrates strong detection performance for vanilla data poisoning attacks\nand uncovers vulnerabilities to bias injection, such as the generation of\ncovert advertisements through malicious concept shifts. It identifies privacy\nrisks in unaltered but high-risk samples, filters them before training, and\nprovides insights into model weaknesses arising from incomplete or imbalanced\ntraining data. Additionally, at the model level, it attributes concepts that\nthe target model is overly dependent on, identifies misleading concepts, and\nexplains how disrupting key concepts negatively impacts the model. Furthermore,\nit uncovers sociological biases in generative content, revealing disparities\nacross sociological contexts. Strikingly, ConceptLens reveals how safe training\nand inference data can be unintentionally and easily exploited, potentially\nundermining safety alignment. Our study informs actionable insights to breed\ntrust in AI systems, thereby speeding adoption and driving greater innovation.", "AI": {"tldr": "ConceptLens\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u5206\u6790\u6982\u5ff5\u6f02\u79fb\uff0c\u68c0\u6d4b\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3001\u9690\u79c1\u98ce\u9669\u53ca\u6a21\u578b\u5f31\u70b9\uff0c\u63ed\u793a\u793e\u4f1a\u5b66\u504f\u89c1\uff0c\u5e76\u63d0\u4f9b\u589e\u5f3aAI\u7cfb\u7edf\u4fe1\u4efb\u7684\u53ef\u884c\u89c1\u89e3\u3002", "motivation": "\u968f\u7740AI\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u53ef\u4fe1\u5ea6\u95ee\u9898\uff08\u5982\u5b8c\u6574\u6027\u3001\u9690\u79c1\u3001\u9c81\u68d2\u6027\u548c\u504f\u89c1\uff09\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u5f52\u56e0\u8fd9\u4e9b\u5a01\u80c1\u3002", "method": "\u63d0\u51faConceptLens\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6982\u5ff5\u6f02\u79fb\u8bc6\u522b\u5a01\u80c1\u6839\u6e90\uff0c\u68c0\u6d4b\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3001\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u63ed\u793a\u6a21\u578b\u4f9d\u8d56\u7684\u5173\u952e\u6982\u5ff5\u548c\u504f\u89c1\u3002", "result": "ConceptLens\u80fd\u6709\u6548\u68c0\u6d4b\u653b\u51fb\u3001\u8fc7\u6ee4\u9ad8\u98ce\u9669\u6837\u672c\uff0c\u63ed\u793a\u6a21\u578b\u5f31\u70b9\u548c\u793e\u4f1a\u5b66\u504f\u89c1\uff0c\u663e\u793a\u5b89\u5168\u6570\u636e\u53ef\u80fd\u88ab\u610f\u5916\u5229\u7528\u3002", "conclusion": "ConceptLens\u4e3a\u589e\u5f3aAI\u7cfb\u7edf\u4fe1\u4efb\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u52a0\u901fAI\u7684\u91c7\u7528\u548c\u521b\u65b0\u3002"}}
{"id": "2504.21476", "pdf": "https://arxiv.org/pdf/2504.21476", "abs": "https://arxiv.org/abs/2504.21476", "authors": ["Xinyu Li", "Qi Yao", "Yuanda Wang"], "title": "GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "The 34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)", "summary": "Garment sewing patterns are fundamental design elements that bridge the gap\nbetween design concepts and practical manufacturing. The generative modeling of\nsewing patterns is crucial for creating diversified garments. However, existing\napproaches are limited either by reliance on a single input modality or by\nsuboptimal generation efficiency. In this work, we present\n\\textbf{\\textit{GarmentDiffusion}}, a new generative model capable of producing\ncentimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,\nimage, and incomplete sewing pattern). Our method efficiently encodes 3D sewing\npattern parameters into compact edge token representations, achieving a\nsequence length that is $\\textbf{10}\\times$ shorter than that of the\nautoregressive SewingGPT in DressCode. By employing a diffusion transformer, we\nsimultaneously denoise all edge tokens along the temporal axis, while\nmaintaining a constant number of denoising steps regardless of dataset-specific\nedge and panel statistics. With all combination of designs of our model, the\nsewing pattern generation speed is accelerated by $\\textbf{100}\\times$ compared\nto SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well\nas on the largest sewing pattern dataset, namely GarmentCodeData. The project\nwebsite is available at https://shenfu-research.github.io/Garment-Diffusion/.", "AI": {"tldr": "GarmentDiffusion\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u591a\u6a21\u6001\u8f93\u5165\uff08\u6587\u672c\u3001\u56fe\u50cf\u548c\u4e0d\u5b8c\u6574\u7684\u7f1d\u7eab\u56fe\u6848\uff09\u751f\u6210\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u77e2\u91cf\u53163D\u7f1d\u7eab\u56fe\u6848\uff0c\u6548\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8100\u500d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7f1d\u7eab\u56fe\u6848\u751f\u6210\u4e2d\u53d7\u9650\u4e8e\u5355\u4e00\u8f93\u5165\u6a21\u6001\u6216\u751f\u6210\u6548\u7387\u4f4e\u4e0b\uff0cGarmentDiffusion\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c063D\u7f1d\u7eab\u56fe\u6848\u53c2\u6570\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u8fb9\u7f18\u4ee4\u724c\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u6269\u6563\u53d8\u6362\u5668\u540c\u65f6\u5bf9\u6240\u6709\u8fb9\u7f18\u4ee4\u724c\u8fdb\u884c\u53bb\u566a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u751f\u6210\u3002", "result": "\u5728DressCodeData\u548cGarmentCodeData\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u7ed3\u679c\uff0c\u751f\u6210\u901f\u5ea6\u6bd4SewingGPT\u5feb100\u500d\u3002", "conclusion": "GarmentDiffusion\u5728\u591a\u6a21\u6001\u8f93\u5165\u548c\u9ad8\u6548\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7f1d\u7eab\u56fe\u6848\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21043", "pdf": "https://arxiv.org/pdf/2504.21043", "abs": "https://arxiv.org/abs/2504.21043", "authors": ["Lingxiang wang", "Hainan Zhang", "Qinnan Zhang", "Ziwei Wang", "Hongwei Zheng", "Jin Dong", "Zhiming Zheng"], "title": "CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at generating code from natural language\ninstructions, yet they often lack an understanding of security vulnerabilities.\nThis limitation makes it difficult for LLMs to avoid security risks in\ngenerated code, particularly in high-security programming tasks such as smart\ncontract development for blockchain. Researchers have attempted to enhance the\nvulnerability awareness of these models by training them to differentiate\nbetween vulnerable and fixed code snippets. However, this approach relies\nheavily on manually labeled vulnerability data, which is only available for\npopular languages like Python and C++. For low-resource languages like\nSolidity, used in smart contracts, large-scale annotated datasets are scarce\nand difficult to obtain. To address this challenge, we introduce CodeBC, a code\ngeneration model specifically designed for generating secure smart contracts in\nblockchain. CodeBC employs a three-stage fine-tuning approach based on\nCodeLlama, distinguishing itself from previous methods by not relying on\npairwise vulnerability location annotations. Instead, it leverages\nvulnerability and security tags to teach the model the differences between\nvulnerable and secure code. During the inference phase, the model leverages\nsecurity tags to generate secure and robust code. Experimental results\ndemonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,\nand compilation pass rates, while significantly reducing vulnerability rates.\nThese findings validate the effectiveness and cost-efficiency of our\nthree-stage fine-tuning strategy, making CodeBC a promising solution for\ngenerating secure smart contract code.", "AI": {"tldr": "CodeBC\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u751f\u6210\u5b89\u5168\u7684\u533a\u5757\u94fe\u667a\u80fd\u5408\u7ea6\u800c\u8bbe\u8ba1\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u6210\u5bf9\u7684\u6f0f\u6d1e\u6807\u6ce8\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6f0f\u6d1e\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u4ee3\u7801\u65f6\u7f3a\u4e4f\u5bf9\u5b89\u5168\u6f0f\u6d1e\u7684\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728\u667a\u80fd\u5408\u7ea6\u7b49\u9ad8\u5b89\u5168\u6027\u4efb\u52a1\u4e2d\uff0c\u800c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982Solidity\uff09\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u57fa\u4e8eCodeLlama\u7684\u4e09\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528\u6f0f\u6d1e\u548c\u5b89\u5168\u6807\u7b7e\u800c\u975e\u6210\u5bf9\u6807\u6ce8\u6570\u636e\uff0c\u8bad\u7ec3\u6a21\u578b\u533a\u5206\u6f0f\u6d1e\u4ee3\u7801\u548c\u5b89\u5168\u4ee3\u7801\u3002", "result": "CodeBC\u5728BLEU\u3001CodeBLEU\u548c\u7f16\u8bd1\u901a\u8fc7\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6f0f\u6d1e\u7387\u3002", "conclusion": "CodeBC\u7684\u4e09\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u9ad8\u6548\u4e14\u6210\u672c\u4f4e\uff0c\u4e3a\u751f\u6210\u5b89\u5168\u7684\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21478", "pdf": "https://arxiv.org/pdf/2504.21478", "abs": "https://arxiv.org/abs/2504.21478", "authors": ["Zherui Zhang", "Changwei Wang", "Rongtao Xu", "Wenhao Xu", "Shibiao Xu", "Yu Zhang", "Li Guo"], "title": "CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from\nthe given pre-trained teacher network to the target student model without\naccess to the real training data. Existing DFKD methods focus primarily on\nimproving image recognition performance on associated datasets, often\nneglecting the crucial aspect of the transferability of learned\nrepresentations. In this paper, we propose Category-Aware Embedding Data-Free\nKnowledge Distillation (CAE-DFKD), which addresses at the embedding level the\nlimitations of previous rely on image-level methods to improve model\ngeneralization but fail when directly applied to DFKD. The superiority and\nflexibility of CAE-DFKD are extensively evaluated, including:\n\\textit{\\textbf{i.)}} Significant efficiency advantages resulting from altering\nthe generator training paradigm; \\textit{\\textbf{ii.)}} Competitive performance\nwith existing DFKD state-of-the-art methods on image recognition tasks;\n\\textit{\\textbf{iii.)}} Remarkable transferability of data-free learned\nrepresentations demonstrated in downstream tasks.", "AI": {"tldr": "CAE-DFKD\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u5c42\u9762\u7684\u6539\u8fdb\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u6548\u7387\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709DFKD\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u8bc6\u522b\u6027\u80fd\uff0c\u5ffd\u7565\u4e86\u5b66\u4e60\u8868\u5f81\u7684\u53ef\u8fc1\u79fb\u6027\uff0cCAE-DFKD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CAE-DFKD\u5728\u5d4c\u5165\u5c42\u9762\u6539\u8fdb\uff0c\u6539\u53d8\u4e86\u751f\u6210\u5668\u8bad\u7ec3\u8303\u5f0f\uff0c\u63d0\u5347\u4e86\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "CAE-DFKD\u5728\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u5b66\u4e60\u8868\u5f81\u7684\u53ef\u8fc1\u79fb\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CAE-DFKD\u5728\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u548c\u9ad8\u8fc1\u79fb\u6027\u7684\u7edf\u4e00\u3002"}}
{"id": "2504.21044", "pdf": "https://arxiv.org/pdf/2504.21044", "abs": "https://arxiv.org/abs/2504.21044", "authors": ["Jianbo Gao", "Keke Gai", "Jing Yu", "Liehuang Zhu", "Qi Wu"], "title": "AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advancement in large-scale Artificial Intelligence (AI) models\noffering multimodal services have become foundational in AI systems, making\nthem prime targets for model theft. Existing methods select Out-of-Distribution\n(OoD) data as backdoor watermarks and retrain the original model for copyright\nprotection. However, existing methods are susceptible to malicious detection\nand forgery by adversaries, resulting in watermark evasion. In this work, we\npropose Model-\\underline{ag}nostic Black-box Backdoor W\\underline{ate}rmarking\nFramework (AGATE) to address stealthiness and robustness challenges in\nmultimodal model copyright protection. Specifically, we propose an adversarial\ntrigger generation method to generate stealthy adversarial triggers from\nordinary dataset, providing visual fidelity while inducing semantic shifts. To\nalleviate the issue of anomaly detection among model outputs, we propose a\npost-transform module to correct the model output by narrowing the distance\nbetween adversarial trigger image embedding and text embedding. Subsequently, a\ntwo-phase watermark verification is proposed to judge whether the current model\ninfringes by comparing the two results with and without the transform module.\nConsequently, we consistently outperform state-of-the-art methods across five\ndatasets in the downstream tasks of multimodal image-text retrieval and image\nclassification. Additionally, we validated the robustness of AGATE under two\nadversarial attack scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAGATE\u7684\u9ed1\u76d2\u540e\u95e8\u6c34\u5370\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u7248\u6743\u4fdd\u62a4\u4e2d\u7684\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\u3002\u901a\u8fc7\u751f\u6210\u9690\u853d\u7684\u5bf9\u6297\u89e6\u53d1\u5668\u5e76\u8bbe\u8ba1\u540e\u53d8\u6362\u6a21\u5757\uff0cAGATE\u5728\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u80fd\u62b5\u6297\u5bf9\u6297\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6a21\u578b\u7248\u6743\u4fdd\u62a4\u4e2d\u6613\u53d7\u6076\u610f\u68c0\u6d4b\u548c\u4f2a\u9020\u653b\u51fb\uff0c\u5bfc\u81f4\u6c34\u5370\u5931\u6548\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9690\u853d\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u4fdd\u62a4\u6a21\u578b\u7248\u6743\u3002", "method": "\u63d0\u51faAGATE\u6846\u67b6\uff0c\u5305\u62ec\u751f\u6210\u5bf9\u6297\u89e6\u53d1\u5668\u7684\u65b9\u6cd5\u548c\u540e\u53d8\u6362\u6a21\u5757\uff0c\u7528\u4e8e\u7ea0\u6b63\u6a21\u578b\u8f93\u51fa\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u6c34\u5370\u9a8c\u8bc1\u6765\u5224\u65ad\u6a21\u578b\u662f\u5426\u4fb5\u6743\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cAGATE\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5bf9\u6297\u653b\u51fb\u573a\u666f\u4e0b\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "AGATE\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7248\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u853d\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u7684\u62b5\u6297\u80fd\u529b\u3002"}}
{"id": "2504.21487", "pdf": "https://arxiv.org/pdf/2504.21487", "abs": "https://arxiv.org/abs/2504.21487", "authors": ["Hebaixu Wang", "Jing Zhang", "Haonan Guo", "Di Wang", "Jiayi Ma", "Bo Du"], "title": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable progress in universal image\nrestoration. While existing methods speed up inference by reducing sampling\nsteps, substantial step intervals often introduce cumulative errors. Moreover,\nthey struggle to balance the commonality of degradation representations and\nrestoration quality. To address these challenges, we introduce\n\\textbf{DGSolver}, a diffusion generalist solver with universal posterior\nsampling. We first derive the exact ordinary differential equations for\ngeneralist diffusion models and tailor high-order solvers with a queue-based\naccelerated sampling strategy to improve both accuracy and efficiency. We then\nintegrate universal posterior sampling to better approximate\nmanifold-constrained gradients, yielding a more accurate noise estimation and\ncorrecting errors in inverse inference. Extensive experiments show that\nDGSolver outperforms state-of-the-art methods in restoration accuracy,\nstability, and scalability, both qualitatively and quantitatively. Code and\nmodels will be available at https://github.com/MiliLab/DGSolver.", "AI": {"tldr": "DGSolver\u662f\u4e00\u79cd\u6269\u6563\u901a\u7528\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u9ad8\u7cbe\u5ea6\u6c42\u89e3\u5668\u548c\u961f\u5217\u52a0\u901f\u91c7\u6837\u7b56\u7565\u63d0\u5347\u56fe\u50cf\u6062\u590d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u5f15\u5165\u901a\u7528\u540e\u9a8c\u91c7\u6837\u4f18\u5316\u566a\u58f0\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u65f6\u5f15\u5165\u7d2f\u79ef\u8bef\u5dee\uff0c\u4e14\u96be\u4ee5\u5e73\u8861\u9000\u5316\u8868\u793a\u548c\u6062\u590d\u8d28\u91cf\u3002", "method": "\u63a8\u5bfc\u901a\u7528\u6269\u6563\u6a21\u578b\u7684\u7cbe\u786e\u5e38\u5fae\u5206\u65b9\u7a0b\uff0c\u8bbe\u8ba1\u9ad8\u9636\u6c42\u89e3\u5668\u548c\u961f\u5217\u52a0\u901f\u91c7\u6837\u7b56\u7565\uff0c\u7ed3\u5408\u901a\u7528\u540e\u9a8c\u91c7\u6837\u4f18\u5316\u566a\u58f0\u4f30\u8ba1\u3002", "result": "DGSolver\u5728\u6062\u590d\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DGSolver\u901a\u8fc7\u9ad8\u6548\u91c7\u6837\u548c\u566a\u58f0\u4f30\u8ba1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u6027\u80fd\u3002"}}
{"id": "2504.21045", "pdf": "https://arxiv.org/pdf/2504.21045", "abs": "https://arxiv.org/abs/2504.21045", "authors": ["Dennis Miczek", "Divyesh Gabbireddy", "Suman Saha"], "title": "Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection", "categories": ["cs.CR", "cs.AI"], "comment": "This work has been accepted for presentation at the ACM Workshop on\n  Wireless Security and Machine Learning (WiseML 2025)", "summary": "According to the Open Web Application Security Project (OWASP), Cross-Site\nScripting (XSS) is a critical security vulnerability. Despite decades of\nresearch, XSS remains among the top 10 security vulnerabilities. Researchers\nhave proposed various techniques to protect systems from XSS attacks, with\nmachine learning (ML) being one of the most widely used methods. An ML model is\ntrained on a dataset to identify potential XSS threats, making its\neffectiveness highly dependent on the size and diversity of the training data.\nA variation of XSS is obfuscated XSS, where attackers apply obfuscation\ntechniques to alter the code's structure, making it challenging for security\nsystems to detect its malicious intent. Our study's random forest model was\ntrained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy.\nHowever, when tested against obfuscated XSS samples, accuracy dropped to 81.9%,\nunderscoring the importance of training ML models with obfuscated data to\nimprove their effectiveness in detecting XSS attacks. A significant challenge\nis to generate highly complex obfuscated code despite the availability of\nseveral public tools. These tools can only produce obfuscation up to certain\nlevels of complexity.\n  In our proposed system, we fine-tune a Large Language Model (LLM) to generate\ncomplex obfuscated XSS payloads automatically. By transforming original XSS\nsamples into diverse obfuscated variants, we create challenging training data\nfor ML model evaluation. Our approach achieved a 99.5% accuracy rate with the\nobfuscated dataset. We also found that the obfuscated samples generated by the\nLLMs were 28.1% more complex than those created by other tools, significantly\nimproving the model's ability to handle advanced XSS attacks and making it more\neffective for real-world application security.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u590d\u6742\u6df7\u6dc6XSS\u8d1f\u8f7d\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u68c0\u6d4b\u6df7\u6dc6XSS\u653b\u51fb\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1XSS\u653b\u51fb\u7814\u7a76\u5df2\u6709\u6570\u5341\u5e74\uff0c\u4f46\u6df7\u6dc6XSS\u653b\u51fb\u4ecd\u96be\u4ee5\u68c0\u6d4b\uff0c\u73b0\u6709\u5de5\u5177\u751f\u6210\u7684\u6df7\u6dc6\u4ee3\u7801\u590d\u6742\u5ea6\u6709\u9650\uff0c\u5bfc\u81f4\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u68c0\u6d4b\u6b64\u7c7b\u653b\u51fb\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210\u590d\u6742\u6df7\u6dc6XSS\u8d1f\u8f7d\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u968f\u673a\u68ee\u6797\u6a21\u578b\u3002", "result": "\u5728\u6df7\u6dc6\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523099.5%\uff0c\u4e14LLM\u751f\u6210\u7684\u6df7\u6dc6\u6837\u672c\u590d\u6742\u5ea6\u6bd4\u5176\u4ed6\u5de5\u5177\u9ad828.1%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u9ad8\u7ea7XSS\u653b\u51fb\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u5b89\u5168\u573a\u666f\u3002"}}
{"id": "2504.21491", "pdf": "https://arxiv.org/pdf/2504.21491", "abs": "https://arxiv.org/abs/2504.21491", "authors": ["Qinfeng Zhu", "Yunxi Jiang", "Lei Fan"], "title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "We propose a result-level category-specific fusion architecture called\nClassWise-CRF. This architecture employs a two-stage process: first, it selects\nexpert networks that perform well in specific categories from a pool of\ncandidate networks using a greedy algorithm; second, it integrates the\nsegmentation predictions of these selected networks by adaptively weighting\ntheir contributions based on their segmentation performance in each category.\nInspired by Conditional Random Field (CRF), the ClassWise-CRF architecture\ntreats the segmentation predictions from multiple networks as confidence vector\nfields. It leverages segmentation metrics (such as Intersection over Union)\nfrom the validation set as priors and employs an exponential weighting strategy\nto fuse the category-specific confidence scores predicted by each network. This\nfusion method dynamically adjusts the weights of each network for different\ncategories, achieving category-specific optimization. Building on this, the\narchitecture further optimizes the fused results using unary and pairwise\npotentials in CRF to ensure spatial consistency and boundary accuracy. To\nvalidate the effectiveness of ClassWise-CRF, we conducted experiments on two\nremote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced\nsemantic segmentation networks. The results show that the ClassWise-CRF\narchitecture significantly improves segmentation performance: on the LoveDA\ndataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on\nthe validation set and by 0.68% on the test set; on the Vaihingen dataset, the\nmIoU improved by 0.87% on the validation set and by 0.91% on the test set.\nThese results fully demonstrate the effectiveness and generality of the\nClassWise-CRF architecture in semantic segmentation of remote sensing images.\nThe full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aClassWise-CRF\u7684\u7ed3\u679c\u7ea7\u7c7b\u522b\u7279\u5b9a\u878d\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff08\u9009\u62e9\u4e13\u5bb6\u7f51\u7edc\u5e76\u81ea\u9002\u5e94\u52a0\u6743\u878d\u5408\uff09\u53caCRF\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u7f51\u7edc\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u4e0d\u540c\u7c7b\u522b\u8868\u73b0\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u7c7b\u522b\u7279\u5b9a\u7684\u4f18\u5316\u3002", "method": "1. \u4f7f\u7528\u8d2a\u5a6a\u7b97\u6cd5\u9009\u62e9\u4e13\u5bb6\u7f51\u7edc\uff1b2. \u57fa\u4e8e\u5206\u5272\u6027\u80fd\u81ea\u9002\u5e94\u52a0\u6743\u878d\u5408\uff1b3. \u5229\u7528CRF\u4f18\u5316\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u8fb9\u754c\u7cbe\u5ea6\u3002", "result": "\u5728LoveDA\u548cVaihingen\u6570\u636e\u96c6\u4e0a\uff0cmIoU\u5206\u522b\u63d0\u53471.00%/0.68%\u548c0.87%/0.91%\u3002", "conclusion": "ClassWise-CRF\u67b6\u6784\u6709\u6548\u4e14\u901a\u7528\uff0c\u663e\u8457\u63d0\u5347\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2504.21047", "pdf": "https://arxiv.org/pdf/2504.21047", "abs": "https://arxiv.org/abs/2504.21047", "authors": ["Klemen Kotar", "Greta Tuckute"], "title": "Model Connectomes: A Generational Approach to Data-Efficient Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Biological neural networks are shaped both by evolution across generations\nand by individual learning within an organism's lifetime, whereas standard\nartificial neural networks undergo a single, large training procedure without\ninherited constraints. In this preliminary work, we propose a framework that\nincorporates this crucial generational dimension - an \"outer loop\" of evolution\nthat shapes the \"inner loop\" of learning - so that artificial networks better\nmirror the effects of evolution and individual learning in biological\norganisms. Focusing on language, we train a model that inherits a \"model\nconnectome\" from the outer evolution loop before exposing it to a\ndevelopmental-scale corpus of 100M tokens. Compared with two closely matched\ncontrol models, we show that the connectome model performs better or on par on\nnatural language processing tasks as well as alignment to human behavior and\nbrain data. These findings suggest that a model connectome serves as an\nefficient prior for learning in low-data regimes - narrowing the gap between\nsingle-generation artificial models and biologically evolved neural networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fdb\u5316\u548c\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u2018\u5916\u5faa\u73af\u2019\u8fdb\u5316\u5851\u9020\u2018\u5185\u5faa\u73af\u2019\u5b66\u4e60\uff0c\u4f7f\u4eba\u5de5\u7f51\u7edc\u66f4\u63a5\u8fd1\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u6027\u3002\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u5bf9\u7167\u7ec4\u76f8\u5f53\u3002", "motivation": "\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u8fdb\u5316\u548c\u5b66\u4e60\u5171\u540c\u5851\u9020\uff0c\u800c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7f3a\u4e4f\u8fd9\u79cd\u591a\u4ee3\u7ea6\u675f\u3002\u7814\u7a76\u65e8\u5728\u7f29\u5c0f\u4eba\u5de5\u6a21\u578b\u4e0e\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5305\u542b\u2018\u5916\u5faa\u73af\u2019\u8fdb\u5316\u548c\u2018\u5185\u5faa\u73af\u2019\u5b66\u4e60\uff0c\u8bad\u7ec3\u6a21\u578b\u7ee7\u627f\u2018\u6a21\u578b\u8fde\u63a5\u7ec4\u2019\u540e\u63a5\u89e6100M\u6807\u8bb0\u7684\u8bed\u6599\u5e93\u3002", "result": "\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u53ca\u4e0e\u4eba\u7c7b\u884c\u4e3a\u548c\u8111\u6570\u636e\u7684\u5bf9\u9f50\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u5bf9\u7167\u7ec4\u76f8\u5f53\u3002", "conclusion": "\u2018\u6a21\u578b\u8fde\u63a5\u7ec4\u2019\u4f5c\u4e3a\u4f4e\u6570\u636e\u5b66\u4e60\u7684\u9ad8\u6548\u5148\u9a8c\uff0c\u7f29\u5c0f\u4e86\u4eba\u5de5\u6a21\u578b\u4e0e\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7684\u5dee\u8ddd\u3002"}}
{"id": "2504.21495", "pdf": "https://arxiv.org/pdf/2504.21495", "abs": "https://arxiv.org/abs/2504.21495", "authors": ["Junxi Wang", "Jize liu", "Na Zhang", "Yaxiong Wang"], "title": "Consistency-aware Fake Videos Detection on Short Video Platforms", "categories": ["cs.CV", "cs.MM"], "comment": "2025 icic", "summary": "This paper focuses to detect the fake news on the short video platforms.\nWhile significant research efforts have been devoted to this task with notable\nprogress in recent years, current detection accuracy remains suboptimal due to\nthe rapid evolution of content manipulation and generation technologies.\nExisting approaches typically employ a cross-modal fusion strategy that\ndirectly combines raw video data with metadata inputs before applying a\nclassification layer. However, our empirical observations reveal a critical\noversight: manipulated content frequently exhibits inter-modal inconsistencies\nthat could serve as valuable discriminative features, yet remain underutilized\nin contemporary detection frameworks. Motivated by this insight, we propose a\nnovel detection paradigm that explicitly identifies and leverages cross-modal\ncontradictions as discriminative cues. Our approach consists of two core\nmodules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative\nDiagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal\nConsistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used\nto generate pseudo-labels for evaluating cross-modal semantic consistency.\nThen, CMCD extracts [CLS] tokens and computes cosine loss to quantify\ncross-modal inconsistencies. MMCD further integrates multimodal features\nthrough Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF).\nMFF employs a co-attention mechanism to enhance semantic interactions across\ndifferent modalities, while a Transformer is utilized for comprehensive feature\nfusion. Meanwhile, PSF further integrates the fake news probability scores\nobtained in the previous step. Extensive experiments on established benchmarks\n(FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in\nFake videos detection.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u8de8\u6a21\u6001\u77db\u76fe\u68c0\u6d4b\u5047\u65b0\u95fb\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u591a\u6a21\u6001\u534f\u4f5c\u8bca\u65ad\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8de8\u6a21\u6001\u4e0d\u4e00\u81f4\u6027\u4f5c\u4e3a\u5224\u522b\u7279\u5f81\uff0c\u5bfc\u81f4\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCross-modal Consistency Learning (CMCL)\u548cMulti-modal Collaborative Diagnosis (MMCD)\u6a21\u5757\uff0c\u5206\u522b\u751f\u6210\u4f2a\u6807\u7b7e\u91cf\u5316\u4e0d\u4e00\u81f4\u6027\u5e76\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "\u5728FakeSV\u548cFakeTT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5229\u7528\u8de8\u6a21\u6001\u77db\u76fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.21048", "pdf": "https://arxiv.org/pdf/2504.21048", "abs": "https://arxiv.org/abs/2504.21048", "authors": ["Mohamad A. Hady", "Siyi Hu", "Mahardhika Pratama", "Jimmy Cao", "Ryszard Kowalczyk"], "title": "Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for\nnumerous real-world applications, modeling distributed decision-making and\nlearning from interactions with complex environments. Resource Allocation\nOptimization (RAO) benefits significantly from MARL's ability to tackle dynamic\nand decentralized contexts. MARL-based approaches are increasingly applied to\nRAO challenges across sectors playing pivotal roles to Industry 4.0\ndevelopments. This survey provides a comprehensive review of recent MARL\nalgorithms for RAO, encompassing core concepts, classifications, and a\nstructured taxonomy. By outlining the current research landscape and\nidentifying primary challenges and future directions, this survey aims to\nsupport researchers and practitioners in leveraging MARL's potential to advance\nresource allocation solutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u8d44\u6e90\u5206\u914d\u4f18\u5316\uff08RAO\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u6838\u5fc3\u6982\u5ff5\u3001\u5206\u7c7b\u548c\u6311\u6218\uff0c\u65e8\u5728\u63a8\u52a8\u8d44\u6e90\u5206\u914d\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\u3002", "motivation": "MARL\u5728\u52a8\u6001\u548c\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u4f7f\u5176\u6210\u4e3aRAO\u7684\u7406\u60f3\u5de5\u5177\uff0c\u5c24\u5176\u5728\u5de5\u4e1a4.0\u80cc\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u5168\u9762\u7efc\u8ff0\u8fd1\u671fMARL\u7b97\u6cd5\uff0c\u63d0\u51fa\u5206\u7c7b\u548c\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u73b0\u72b6\uff0c\u5e76\u8bc6\u522b\u4e86\u4e3b\u8981\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86MARL\u5728RAO\u4e2d\u5e94\u7528\u7684\u6307\u5bfc\uff0c\u4ee5\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2504.21497", "pdf": "https://arxiv.org/pdf/2504.21497", "abs": "https://arxiv.org/abs/2504.21497", "authors": ["Mengting Wei", "Yante Li", "Tuomas Varanka", "Yan Jiang", "Licai Sun", "Guoying Zhao"], "title": "MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nenables precise extraction of detailed face geometry and motion features from\ndriving videos. Specifically, we enhance the latent diffusion model with rich\n3D expression and detailed pose information by incorporating depth maps, normal\nmaps, and rendering maps derived from FLAME sequences. A multi-layer face\nmovements fusion module with integrated self-attention mechanisms is used to\ncombine identity and motion latent features within the spatial domain. By\nutilizing the 3D face parametric model as motion guidance, our method enables\nparametric alignment of face identity between the reference image and the\nmotion captured from the driving video. Experimental results on benchmark\ndatasets show that our method excels at generating high-quality face animations\nwith precise expression and head pose variation modeling. In addition, it\ndemonstrates strong generalization performance on out-of-domain images. Code is\npublicly available at https://github.com/weimengting/MagicPortrait.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c063D\u4eba\u8138\u53c2\u6570\u6a21\u578b\u4e0e\u6f5c\u5728\u6269\u6563\u6846\u67b6\u7ed3\u5408\u7684\u89c6\u9891\u4eba\u8138\u91cd\u6f14\u65b9\u6cd5\uff0c\u63d0\u5347\u5f62\u72b6\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u89c6\u9891\u4eba\u8138\u751f\u6210\u65b9\u6cd5\u5728\u5f62\u72b6\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u63a7\u5236\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528FLAME\u6a21\u578b\u4f5c\u4e3a3D\u4eba\u8138\u53c2\u6570\u8868\u793a\uff0c\u7ed3\u5408\u6df1\u5ea6\u56fe\u3001\u6cd5\u7ebf\u56fe\u548c\u6e32\u67d3\u56fe\u589e\u5f3a\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u878d\u5408\u6a21\u5757\u7ed3\u5408\u8eab\u4efd\u4e0e\u8fd0\u52a8\u7279\u5f81\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u751f\u6210\u9ad8\u8d28\u91cf\u4eba\u8138\u52a8\u753b\uff0c\u7cbe\u786e\u5efa\u6a21\u8868\u60c5\u548c\u5934\u90e8\u59ff\u6001\u53d8\u5316\uff0c\u6cdb\u5316\u6027\u80fd\u5f3a\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4eba\u8138\u91cd\u6f14\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.21049", "pdf": "https://arxiv.org/pdf/2504.21049", "abs": "https://arxiv.org/abs/2504.21049", "authors": ["Sneha Baskota"], "title": "Phishing URL Detection using Bi-LSTM", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Phishing attacks threaten online users, often leading to data breaches,\nfinancial losses, and identity theft. Traditional phishing detection systems\nstruggle with high false positive rates and are usually limited by the types of\nattacks they can identify. This paper proposes a deep learning-based approach\nusing a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs\ninto four categories: benign, phishing, defacement, and malware. The model\nleverages sequential URL data and captures contextual information, improving\nthe accuracy of phishing detection. Experimental results on a dataset\ncomprising over 650,000 URLs demonstrate the model's effectiveness, achieving\n97% accuracy and significant improvements over traditional techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBi-LSTM\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u7c7bURL\uff0c\u51c6\u786e\u7387\u8fbe97%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u9493\u9c7c\u68c0\u6d4b\u7cfb\u7edf\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u548c\u68c0\u6d4b\u7c7b\u578b\u6709\u9650\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08Bi-LSTM\uff09\u5206\u6790URL\u5e8f\u5217\u6570\u636e\uff0c\u6355\u6349\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u572865\u4e07\u6761URL\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523097%\u3002", "conclusion": "Bi-LSTM\u6a21\u578b\u5728\u9493\u9c7c\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u3002"}}
{"id": "2504.21544", "pdf": "https://arxiv.org/pdf/2504.21544", "abs": "https://arxiv.org/abs/2504.21544", "authors": ["Uzair Shah", "Marco Agus", "Daniya Boges", "Vanessa Chiappini", "Mahmood Alzubaidi", "Jens Schneider", "Markus Hadwiger", "Pierre J. Magistretti", "Mowafa Househ", "Corrado Cal\u0131"], "title": "SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks", "categories": ["cs.CV"], "comment": "Accepted at (CVPRW) 10th IEEE Workshop on Computer Vision for\n  Microscopy Image Analysis (CVMI)", "summary": "We present SAM4EM, a novel approach for 3D segmentation of complex neural\nstructures in electron microscopy (EM) data by leveraging the Segment Anything\nModel (SAM) alongside advanced fine-tuning strategies. Our contributions\ninclude the development of a prompt-free adapter for SAM using two stage mask\ndecoding to automatically generate prompt embeddings, a dual-stage fine-tuning\nmethod based on Low-Rank Adaptation (LoRA) for enhancing segmentation with\nlimited annotated data, and a 3D memory attention mechanism to ensure\nsegmentation consistency across 3D stacks. We further release a unique\nbenchmark dataset for the segmentation of astrocytic processes and synapses. We\nevaluated our method on challenging neuroscience segmentation benchmarks,\nspecifically targeting mitochondria, glia, and synapses, with significant\naccuracy improvements over state-of-the-art (SOTA) methods, including recent\nSAM-based adapters developed for the medical domain and other vision\ntransformer-based approaches. Experimental results indicate that our approach\noutperforms existing solutions in the segmentation of complex processes like\nglia and post-synaptic densities. Our code and models are available at\nhttps://github.com/Uzshah/SAM4EM.", "AI": {"tldr": "SAM4EM\u662f\u4e00\u79cd\u57fa\u4e8eSegment Anything Model\uff08SAM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u5b50\u663e\u5fae\u955c\uff08EM\uff09\u6570\u636e\u4e2d\u590d\u6742\u795e\u7ecf\u7ed3\u6784\u76843D\u5206\u5272\uff0c\u901a\u8fc7\u65e0\u63d0\u793a\u9002\u914d\u5668\u548c\u53cc\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u663e\u5fae\u955c\u6570\u636e\u4e2d\u590d\u6742\u795e\u7ecf\u7ed3\u6784\uff08\u5982\u7ebf\u7c92\u4f53\u3001\u80f6\u8d28\u7ec6\u80de\u548c\u7a81\u89e6\uff09\u76843D\u5206\u5272\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5f00\u53d1\u4e86\u65e0\u63d0\u793a\u9002\u914d\u5668\uff0c\u91c7\u7528\u53cc\u9636\u6bb5\u63a9\u7801\u89e3\u7801\u81ea\u52a8\u751f\u6210\u63d0\u793a\u5d4c\u5165\uff1b\u57fa\u4e8eLoRA\u7684\u53cc\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\uff1b3D\u8bb0\u5fc6\u6ce8\u610f\u529b\u673a\u5236\u786e\u4fdd\u5206\u5272\u4e00\u81f4\u6027\u3002", "result": "\u5728\u795e\u7ecf\u79d1\u5b66\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAM4EM\u5728\u80f6\u8d28\u7ec6\u80de\u548c\u7a81\u89e6\u540e\u5bc6\u5ea6\u7b49\u590d\u6742\u7ed3\u6784\u7684\u5206\u5272\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SAM4EM\u901a\u8fc7\u7ed3\u5408SAM\u548c\u5148\u8fdb\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5206\u5272\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2504.21052", "pdf": "https://arxiv.org/pdf/2504.21052", "abs": "https://arxiv.org/abs/2504.21052", "authors": ["Yangxu Yin", "Honglong Chen", "Yudong Gao", "Peng Sun", "Zhishuai Li", "Weifeng Liu"], "title": "SFIBA: Spatial-based Full-target Invisible Backdoor Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Multi-target backdoor attacks pose significant security threats to deep\nneural networks, as they can preset multiple target classes through a single\nbackdoor injection. This allows attackers to control the model to misclassify\npoisoned samples with triggers into any desired target class during inference,\nexhibiting superior attack performance compared with conventional backdoor\nattacks. However, existing multi-target backdoor attacks fail to guarantee\ntrigger specificity and stealthiness in black-box settings, resulting in two\nmain issues. First, they are unable to simultaneously target all classes when\nonly training data can be manipulated, limiting their effectiveness in\nrealistic attack scenarios. Second, the triggers often lack visual\nimperceptibility, making poisoned samples easy to detect. To address these\nproblems, we propose a Spatial-based Full-target Invisible Backdoor Attack,\ncalled SFIBA. It restricts triggers for different classes to specific local\nspatial regions and morphologies in the pixel space to ensure specificity,\nwhile employing a frequency-domain-based trigger injection method to guarantee\nstealthiness. Specifically, for injection of each trigger, we first apply fast\nfourier transform to obtain the amplitude spectrum of clean samples in local\nspatial regions. Then, we employ discrete wavelet transform to extract the\nfeatures from the amplitude spectrum and use singular value decomposition to\nintegrate the trigger. Subsequently, we selectively filter parts of the trigger\nin pixel space to implement trigger morphology constraints and adjust injection\ncoefficients based on visual effects. We conduct experiments on multiple\ndatasets and models. The results demonstrate that SFIBA can achieve excellent\nattack performance and stealthiness, while preserving the model's performance\non benign samples, and can also bypass existing backdoor defenses.", "AI": {"tldr": "SFIBA\u662f\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u7684\u5168\u76ee\u6807\u9690\u5f62\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u89e6\u53d1\u5668\u7684\u7a7a\u95f4\u533a\u57df\u548c\u5f62\u6001\u786e\u4fdd\u7279\u5f02\u6027\uff0c\u540c\u65f6\u5229\u7528\u9891\u57df\u6ce8\u5165\u65b9\u6cd5\u4fdd\u8bc1\u9690\u853d\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u653b\u51fb\u6027\u80fd\u548c\u9690\u853d\u6027\u4f18\u5f02\u3002", "motivation": "\u591a\u76ee\u6807\u540e\u95e8\u653b\u51fb\u5728\u73b0\u5b9e\u653b\u51fb\u573a\u666f\u4e2d\u5b58\u5728\u89e6\u53d1\u5668\u7279\u5f02\u6027\u548c\u9690\u853d\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u653b\u51fb\u6548\u679c\u548c\u9690\u853d\u6027\u3002", "method": "SFIBA\u901a\u8fc7\u5c40\u90e8\u7a7a\u95f4\u533a\u57df\u548c\u5f62\u6001\u9650\u5236\u89e6\u53d1\u5668\uff0c\u7ed3\u5408\u9891\u57df\u6ce8\u5165\u65b9\u6cd5\uff08FFT\u3001DWT\u3001SVD\uff09\u5b9e\u73b0\u9690\u853d\u6027\u3002", "result": "SFIBA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u653b\u51fb\u6027\u80fd\u548c\u9690\u853d\u6027\uff0c\u4e14\u80fd\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u3002", "conclusion": "SFIBA\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u540e\u95e8\u653b\u51fb\u7684\u7279\u5f02\u6027\u548c\u9690\u853d\u6027\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21559", "pdf": "https://arxiv.org/pdf/2504.21559", "abs": "https://arxiv.org/abs/2504.21559", "authors": ["Sangmin Woo", "Kang Zhou", "Yun Zhou", "Shuai Wang", "Sheng Guan", "Haibo Ding", "Lin Lee Cheong"], "title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "NAACL 2025", "summary": "Large Vision Language Models (LVLMs) often suffer from object hallucination,\nwhich undermines their reliability. Surprisingly, we find that simple\nobject-based visual prompting -- overlaying visual cues (e.g., bounding box,\ncircle) on images -- can significantly mitigate such hallucination; however,\ndifferent visual prompts (VPs) vary in effectiveness. To address this, we\npropose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify\noptimal VPs that enhance LVLM responses without needing access to model\ninternals. Our approach employs a pool of candidate VPs and trains a router\nmodel to dynamically select the most effective VP for a given input image. This\nblack-box approach is model-agnostic, making it applicable to both open-source\nand proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR\ndemonstrate that BBVPE effectively reduces object hallucination.", "AI": {"tldr": "\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u5de5\u7a0b\uff08BBVPE\uff09\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18\u89c6\u89c9\u63d0\u793a\u4ee5\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5e38\u51fa\u73b0\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5f71\u54cd\u5176\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u89c6\u89c9\u63d0\u793a\u3002", "method": "\u63d0\u51faBlack-Box Visual Prompt Engineering\uff08BBVPE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5019\u9009\u89c6\u89c9\u63d0\u793a\u6c60\u548c\u8def\u7531\u6a21\u578b\u52a8\u6001\u9009\u62e9\u6700\u4f18\u63d0\u793a\u3002", "result": "\u5728POPE\u548cCHAIR\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBBVPE\u663e\u8457\u51cf\u5c11\u4e86\u7269\u4f53\u5e7b\u89c9\u3002", "conclusion": "BBVPE\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u9ed1\u76d2\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347LVLM\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2504.21053", "pdf": "https://arxiv.org/pdf/2504.21053", "abs": "https://arxiv.org/abs/2504.21053", "authors": ["Yi Zhou", "Wenpeng Xing", "Dezhang Kong", "Changting Lin", "Meng Han"], "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Safety alignment in large language models (LLMs) is achieved through\nfine-tuning mechanisms that regulate neuron activations to suppress harmful\ncontent. In this work, we propose a novel approach to induce disalignment by\nidentifying and modifying the neurons responsible for safety constraints. Our\nmethod consists of three key steps: Neuron Activation Analysis, where we\nexamine activation patterns in response to harmful and harmless prompts to\ndetect neurons that are critical for distinguishing between harmful and\nharmless inputs; Similarity-Based Neuron Identification, which systematically\nlocates the neurons responsible for safe alignment; and Neuron Relearning for\nSafety Removal, where we fine-tune these selected neurons to restore the\nmodel's ability to generate previously restricted responses. Experimental\nresults demonstrate that our method effectively removes safety constraints with\nminimal fine-tuning, highlighting a critical vulnerability in current alignment\ntechniques. Our findings underscore the need for robust defenses against\nadversarial fine-tuning attacks on LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u5e76\u4fee\u6539\u795e\u7ecf\u5143\u6765\u89e3\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u7684\u65b0\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5bf9\u9f50\u6280\u672f\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4fee\u6539\u795e\u7ecf\u5143\u89e3\u9664LLMs\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u4ee5\u63ed\u793a\u73b0\u6709\u5b89\u5168\u673a\u5236\u7684\u6f0f\u6d1e\u3002", "method": "1. \u795e\u7ecf\u5143\u6fc0\u6d3b\u5206\u6790\uff1b2. \u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u795e\u7ecf\u5143\u8bc6\u522b\uff1b3. \u795e\u7ecf\u5143\u518d\u5b66\u4e60\u4ee5\u79fb\u9664\u5b89\u5168\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u6700\u5c0f\u5fae\u8c03\u6709\u6548\u79fb\u9664\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "\u5f53\u524d\u5bf9\u9f50\u6280\u672f\u5b58\u5728\u6f0f\u6d1e\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u4ee5\u5bf9\u6297\u5bf9\u6297\u6027\u5fae\u8c03\u653b\u51fb\u3002"}}
{"id": "2504.21561", "pdf": "https://arxiv.org/pdf/2504.21561", "abs": "https://arxiv.org/abs/2504.21561", "authors": ["Pengxiang Li", "Zhi Gao", "Bofei Zhang", "Yapeng Mi", "Xiaojian Ma", "Chenrui Shi", "Tao Yuan", "Yuwei Wu", "Yunde Jia", "Song-Chun Zhu", "Qing Li"], "title": "Iterative Trajectory Exploration for Multimodal Agents", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "Multimodal agents, which integrate a controller (e.g., a large language\nmodel) with external tools, have demonstrated remarkable capabilities in\ntackling complex tasks. However, existing agents need to collect a large number\nof expert data for fine-tuning to adapt to new environments. In this paper, we\npropose an online self-exploration method for multimodal agents, namely SPORT,\nvia step-wise preference optimization to refine the trajectories of agents,\nwhich automatically generates tasks and learns from solving the generated\ntasks, without any expert annotation. SPORT operates through four iterative\ncomponents: task synthesis, step sampling, step verification, and preference\ntuning. First, we synthesize multi-modal tasks using language models. Then, we\nintroduce a novel search scheme, where step sampling and step verification are\nexecuted alternately to solve each generated task. We employ a verifier to\nprovide AI feedback to construct step-wise preference data. The data is\nsubsequently used to update the controller's policy through preference tuning,\nproducing a SPORT Agent. By interacting with real environments, the SPORT Agent\nevolves into a more refined and capable system. Evaluation in the GTA and GAIA\nbenchmarks show that the SPORT Agent achieves 6.41\\% and 3.64\\% improvements,\nunderscoring the generalization and effectiveness introduced by our method. The\nproject page is https://SPORT-Agents.github.io.", "AI": {"tldr": "SPORT\u662f\u4e00\u79cd\u591a\u6a21\u6001\u4ee3\u7406\u7684\u5728\u7ebf\u81ea\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u504f\u597d\u4f18\u5316\u6539\u8fdb\u4ee3\u7406\u8f68\u8ff9\uff0c\u65e0\u9700\u4e13\u5bb6\u6807\u6ce8\uff0c\u81ea\u52a8\u751f\u6210\u4efb\u52a1\u5e76\u4ece\u4e2d\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u4ee5\u9002\u5e94\u65b0\u73af\u5883\uff0cSPORT\u65e8\u5728\u901a\u8fc7\u81ea\u63a2\u7d22\u51cf\u5c11\u5bf9\u4e13\u5bb6\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "SPORT\u901a\u8fc7\u4efb\u52a1\u5408\u6210\u3001\u6b65\u9aa4\u91c7\u6837\u3001\u6b65\u9aa4\u9a8c\u8bc1\u548c\u504f\u597d\u8c03\u4f18\u56db\u4e2a\u8fed\u4ee3\u7ec4\u4ef6\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4efb\u52a1\u5e76\u901a\u8fc7AI\u53cd\u9988\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728GTA\u548cGAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPORT\u4ee3\u7406\u5206\u522b\u5b9e\u73b0\u4e866.41%\u548c3.64%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SPORT\u901a\u8fc7\u81ea\u63a2\u7d22\u548c\u504f\u597d\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4ee3\u7406\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2504.21054", "pdf": "https://arxiv.org/pdf/2504.21054", "abs": "https://arxiv.org/abs/2504.21054", "authors": ["Yangxu Yin", "Honglong Chen", "Yudong Gao", "Peng Sun", "Liantao Wu", "Zhe Li", "Weifeng Liu"], "title": "FFCBA: Feature-based Full-target Clean-label Backdoor Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Backdoor attacks pose a significant threat to deep neural networks, as\nbackdoored models would misclassify poisoned samples with specific triggers\ninto target classes while maintaining normal performance on clean samples.\nAmong these, multi-target backdoor attacks can simultaneously target multiple\nclasses. However, existing multi-target backdoor attacks all follow the\ndirty-label paradigm, where poisoned samples are mislabeled, and most of them\nrequire an extremely high poisoning rate. This makes them easily detectable by\nmanual inspection. In contrast, clean-label attacks are more stealthy, as they\navoid modifying the labels of poisoned samples. However, they generally\nstruggle to achieve stable and satisfactory attack performance and often fail\nto scale effectively to multi-target attacks. To address this issue, we propose\nthe Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which\nconsists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and\nFeature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional\nautoencoders to generate noise triggers that align perturbed in-class samples\nwith the original category's features, ensuring the effectiveness, intra-class\nconsistency, inter-class specificity and natural-feature correlation of\ntriggers. While FSBA supports swift and efficient attacks, its cross-model\nattack capability is relatively weak. FMBA employs a two-stage\nclass-conditional autoencoder training process that alternates between using\nout-of-class samples and in-class samples. This allows FMBA to generate\ntriggers with strong target-class features, making it highly effective for\ncross-model attacks. We conduct experiments on multiple datasets and models,\nthe results show that FFCBA achieves outstanding attack performance and\nmaintains desirable robustness against the state-of-the-art backdoor defenses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FFCBA\u65b9\u6cd5\uff0c\u901a\u8fc7FSBA\u548cFMBA\u4e24\u79cd\u8303\u5f0f\u5b9e\u73b0\u591a\u76ee\u6807\u6e05\u6d01\u6807\u7b7e\u540e\u95e8\u653b\u51fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u548c\u6269\u5c55\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u591a\u76ee\u6807\u540e\u95e8\u653b\u51fb\u591a\u4e3a\u810f\u6807\u7b7e\u8303\u5f0f\uff0c\u6613\u88ab\u68c0\u6d4b\u4e14\u9700\u9ad8\u6c61\u67d3\u7387\uff1b\u6e05\u6d01\u6807\u7b7e\u653b\u51fb\u867d\u9690\u853d\u4f46\u6027\u80fd\u4e0d\u7a33\u5b9a\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "FFCBA\u5305\u542bFSBA\u548cFMBA\uff1aFSBA\u5229\u7528\u7c7b\u6761\u4ef6\u81ea\u7f16\u7801\u5668\u751f\u6210\u89e6\u53d1\u5668\uff0cFMBA\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u751f\u6210\u5f3a\u76ee\u6807\u7c7b\u7279\u5f81\u7684\u89e6\u53d1\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFFCBA\u653b\u51fb\u6027\u80fd\u4f18\u5f02\uff0c\u5e76\u5bf9\u5148\u8fdb\u9632\u5fa1\u65b9\u6cd5\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "FFCBA\u4e3a\u591a\u76ee\u6807\u6e05\u6d01\u6807\u7b7e\u540e\u95e8\u653b\u51fb\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9690\u853d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21562", "pdf": "https://arxiv.org/pdf/2504.21562", "abs": "https://arxiv.org/abs/2504.21562", "authors": ["Henry John Krumb", "Anirban Mukhopadhyay"], "title": "eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Wireless Capsule Endoscopy is a non-invasive imaging method for the entire\ngastrointestinal tract, and is a pain-free alternative to traditional\nendoscopy. It generates extensive video data that requires significant review\ntime, and localizing the capsule after ingestion is a challenge. Techniques\nlike bleeding detection and depth estimation can help with localization of\npathologies, but deep learning models are typically too large to run directly\non the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and\ndepth estimation are trained on capsule endoscopic images. For monocular depth\nestimation, we distill a large foundation model into the lean NCA architecture,\nby treating the outputs of the foundation model as pseudo ground truth. We then\nport the trained NCA to the ESP32 microcontroller, enabling efficient image\nprocessing on hardware as small as a camera capsule. NCA are more accurate\n(Dice) than other portable segmentation models, while requiring more than 100x\nfewer parameters stored in memory than other small-scale models. The visual\nresults of NCA depth estimation look convincing, and in some cases beat the\nrealism and detail of the pseudo ground truth. Runtime optimizations on the\nESP32-S3 accelerate the average inference speed significantly, by more than\nfactor 3. With several algorithmic adjustments and distillation, it is possible\nto eNCApsulate NCA models into microcontrollers that fit into wireless capsule\nendoscopes. This is the first work that enables reliable bleeding segmentation\nand depth estimation on a miniaturized device, paving the way for precise\ndiagnosis combined with visual odometry as a means of precise localization of\nthe capsule -- on the capsule.", "AI": {"tldr": "\u65e0\u7ebf\u80f6\u56ca\u5185\u7aa5\u955c\uff08WCE\uff09\u662f\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u80c3\u80a0\u9053\u6210\u50cf\u65b9\u6cd5\uff0c\u4f46\u89c6\u9891\u6570\u636e\u91cf\u5927\u4e14\u80f6\u56ca\u5b9a\u4f4d\u56f0\u96be\u3002\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u8fdb\u884c\u51fa\u8840\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u84b8\u998f\u548c\u786c\u4ef6\u4f18\u5316\u5b9e\u73b0\u5fae\u578b\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u8fd0\u884c\u3002", "motivation": "\u4f20\u7edfWCE\u6570\u636e\u5904\u7406\u8017\u65f6\u4e14\u80f6\u56ca\u5b9a\u4f4d\u56f0\u96be\uff0c\u9700\u8981\u8f7b\u91cf\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4ee5\u5728\u5fae\u578b\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u75c5\u7406\u68c0\u6d4b\u548c\u5b9a\u4f4d\u3002", "method": "\u901a\u8fc7\u84b8\u998f\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u8f93\u51fa\u4f5c\u4e3a\u4f2a\u771f\u503c\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7NCA\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u79fb\u690d\u5230ESP32\u5fae\u63a7\u5236\u5668\u4e0a\uff0c\u5b9e\u73b0\u9ad8\u6548\u56fe\u50cf\u5904\u7406\u3002", "result": "NCA\u5728\u51fa\u8840\u5206\u5272\u4e0a\u6bd4\u5176\u4ed6\u8f7b\u91cf\u6a21\u578b\u66f4\u51c6\u786e\uff08Dice\u6307\u6807\uff09\uff0c\u4e14\u53c2\u6570\u5b58\u50a8\u9700\u6c42\u51cf\u5c11100\u500d\u4ee5\u4e0a\uff1b\u6df1\u5ea6\u4f30\u8ba1\u7ed3\u679c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f2a\u771f\u503c\u3002ESP32-S3\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u53473\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u7814\u7a76\u9996\u6b21\u5728\u5fae\u578b\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u51fa\u8840\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4e3a\u7ed3\u5408\u89c6\u89c9\u91cc\u7a0b\u8ba1\u5b9e\u73b0\u80f6\u56ca\u7cbe\u786e\u5b9a\u4f4d\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.21055", "pdf": "https://arxiv.org/pdf/2504.21055", "abs": "https://arxiv.org/abs/2504.21055", "authors": ["Shuai Ma", "Bin Shen", "Chuanhui Zhang", "Youlong Wu", "Hang Li", "Shiyin Li", "Guangming Shi", "Naofal Al-Dhahir"], "title": "Modeling and Performance Analysis for Semantic Communications Based on Empirical Results", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Due to the black-box characteristics of deep learning based semantic encoders\nand decoders, finding a tractable method for the performance analysis of\nsemantic communications is a challenging problem. In this paper, we propose an\nAlpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end\nmeasurement and SNR, which can be applied for both image reconstruction tasks\nand inference tasks. Specifically, for image reconstruction tasks, the proposed\nABG formula can well fit the commonly used DL networks, such as SCUNet, and\nVision Transformer, for semantic encoding with the multi scale-structural\nsimilarity index measure (MS-SSIM) measurement. Furthermore, we find that the\nupper bound of the MS-SSIM depends on the number of quantized output bits of\nsemantic encoders, and we also propose a closed-form expression to fit the\nrelationship between the MS-SSIM and quantized output bits. To the best of our\nknowledge, this is the first theoretical expression between end-to-end\nperformance metrics and SNR for semantic communications. Based on the proposed\nABG formula, we investigate an adaptive power control scheme for semantic\ncommunications over random fading channels, which can effectively guarantee\nquality of service (QoS) for semantic communications, and then design the\noptimal power allocation scheme to maximize the energy efficiency of the\nsemantic communication system. Furthermore, by exploiting the bisection\nalgorithm, we develop the power allocation scheme to maximize the minimum QoS\nof multiple users for OFDMA downlink semantic communication Extensive\nsimulations verify the effectiveness and superiority of the proposed ABG\nformula and power allocation schemes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAlpha-Beta-Gamma\uff08ABG\uff09\u516c\u5f0f\uff0c\u7528\u4e8e\u5206\u6790\u8bed\u4e49\u901a\u4fe1\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u91cd\u5efa\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u529f\u7387\u63a7\u5236\u65b9\u6848\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u9ed1\u76d2\u7279\u6027\uff0c\u5206\u6790\u8bed\u4e49\u901a\u4fe1\u6027\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faABG\u516c\u5f0f\u5efa\u6a21\u7aef\u5230\u7aef\u6027\u80fd\u4e0eSNR\u7684\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u81ea\u9002\u5e94\u529f\u7387\u63a7\u5236\u65b9\u6848\u3002", "result": "ABG\u516c\u5f0f\u80fd\u51c6\u786e\u62df\u5408\u5e38\u7528DL\u7f51\u7edc\uff0c\u5e76\u63ed\u793a\u4e86MS-SSIM\u4e0e\u91cf\u5316\u8f93\u51fa\u6bd4\u7279\u7684\u5173\u7cfb\uff1b\u529f\u7387\u5206\u914d\u65b9\u6848\u80fd\u6709\u6548\u4fdd\u969cQoS\u548c\u80fd\u91cf\u6548\u7387\u3002", "conclusion": "ABG\u516c\u5f0f\u548c\u529f\u7387\u5206\u914d\u65b9\u6848\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u4e3a\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2504.21598", "pdf": "https://arxiv.org/pdf/2504.21598", "abs": "https://arxiv.org/abs/2504.21598", "authors": ["Thomas L. Athey", "Shashata Sawmya", "Nir Shavit"], "title": "Cascade Detector Analysis and Application to Biomedical Microscopy", "categories": ["cs.CV"], "comment": null, "summary": "As both computer vision models and biomedical datasets grow in size, there is\nan increasing need for efficient inference algorithms. We utilize cascade\ndetectors to efficiently identify sparse objects in multiresolution images.\nGiven an object's prevalence and a set of detectors at different resolutions\nwith known accuracies, we derive the accuracy, and expected number of\nclassifier calls by a cascade detector. These results generalize across number\nof dimensions and number of cascade levels. Finally, we compare one- and\ntwo-level detectors in fluorescent cell detection, organelle segmentation, and\ntissue segmentation across various microscopy modalities. We show that the\nmulti-level detector achieves comparable performance in 30-75% less time. Our\nwork is compatible with a variety of computer vision models and data domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea7\u8054\u68c0\u6d4b\u5668\u7684\u9ad8\u6548\u7a00\u758f\u76ee\u6807\u8bc6\u522b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u548c\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u7684\u89c4\u6a21\u589e\u957f\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u7b97\u6cd5\u3002", "method": "\u5229\u7528\u7ea7\u8054\u68c0\u6d4b\u5668\u5728\u591a\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u8bc6\u522b\u7a00\u758f\u76ee\u6807\uff0c\u63a8\u5bfc\u4e86\u7ea7\u8054\u68c0\u6d4b\u5668\u7684\u51c6\u786e\u6027\u548c\u9884\u671f\u5206\u7c7b\u5668\u8c03\u7528\u6b21\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u7ea7\u68c0\u6d4b\u5668\u5728\u8367\u5149\u7ec6\u80de\u68c0\u6d4b\u3001\u7ec6\u80de\u5668\u5206\u5272\u548c\u7ec4\u7ec7\u5206\u5272\u4e2d\uff0c\u6027\u80fd\u76f8\u5f53\u4f46\u65f6\u95f4\u51cf\u5c1130-75%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u548c\u6570\u636e\u9886\u57df\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21063", "pdf": "https://arxiv.org/pdf/2504.21063", "abs": "https://arxiv.org/abs/2504.21063", "authors": ["Shuai Gong", "Chaoran Cui", "Xiaolin Dong", "Xiushan Nie", "Lei Zhu", "Xiaojun Chang"], "title": "Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": "The manuscript has been submitted to IEEE Transactions on Knowledge\n  and Data Engineering", "summary": "Federated domain generalization (FedDG) aims to learn a globally\ngeneralizable model from decentralized clients with heterogeneous data while\npreserving privacy. Recent studies have introduced prompt learning to adapt\nvision-language models (VLMs) in FedDG by learning a single global prompt.\nHowever, such a one-prompt-fits-all learning paradigm typically leads to\nperformance degradation on personalized samples. Although the mixture of\nexperts (MoE) offers a promising solution for specialization, existing\nMoE-based methods suffer from coarse image-level expert assignment and high\ncommunication costs from parameterized routers. To address these limitations,\nwe propose TRIP, a Token-level prompt mixture with parameter-free routing\nframework for FedDG, which treats multiple prompts as distinct experts. Unlike\nexisting image-level routing designs, TRIP assigns different tokens within an\nimage to specific experts. To ensure communication efficiency, TRIP\nincorporates a parameter-free routing mechanism based on token clustering and\noptimal transport. The instance-specific prompt is then synthesized by\naggregating experts, weighted by the number of tokens assigned to each.\nAdditionally, TRIP develops an unbiased learning strategy for prompt experts,\nleveraging the VLM's zero-shot generalization capability. Extensive experiments\nacross four benchmarks demonstrate that TRIP achieves optimal generalization\nresults, with communication of only 1K parameters per round. Our code is\navailable at https://github.com/GongShuai8210/TRIP.", "AI": {"tldr": "TRIP\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee4\u724c\u7ea7\u63d0\u793a\u6df7\u5408\u7684\u65e0\u53c2\u6570\u8def\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u90a6\u57df\u6cdb\u5316\uff08FedDG\uff09\uff0c\u901a\u8fc7\u4ee4\u724c\u805a\u7c7b\u548c\u6700\u4f18\u4f20\u8f93\u5b9e\u73b0\u9ad8\u6548\u901a\u4fe1\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709FedDG\u65b9\u6cd5\u4e2d\u5355\u4e00\u5168\u5c40\u63d0\u793a\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4ee5\u53caMoE\u65b9\u6cd5\u4e2d\u56fe\u50cf\u7ea7\u4e13\u5bb6\u5206\u914d\u7c97\u7cd9\u548c\u901a\u4fe1\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "TRIP\u91c7\u7528\u4ee4\u724c\u7ea7\u63d0\u793a\u6df7\u5408\u548c\u65e0\u53c2\u6570\u8def\u7531\u673a\u5236\uff0c\u901a\u8fc7\u4ee4\u724c\u805a\u7c7b\u548c\u6700\u4f18\u4f20\u8f93\u5206\u914d\u4e13\u5bb6\uff0c\u5e76\u5229\u7528VLM\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u8fdb\u884c\u65e0\u504f\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTRIP\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u6bcf\u8f6e\u901a\u4fe1\u4ec5\u97001K\u53c2\u6570\u3002", "conclusion": "TRIP\u901a\u8fc7\u4ee4\u724c\u7ea7\u4e13\u5bb6\u5206\u914d\u548c\u65e0\u53c2\u6570\u8def\u7531\uff0c\u663e\u8457\u63d0\u5347\u4e86FedDG\u7684\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2504.21614", "pdf": "https://arxiv.org/pdf/2504.21614", "abs": "https://arxiv.org/abs/2504.21614", "authors": ["Daniel Bogdoll", "Rajanikant Patnaik Ananta", "Abeyankar Giridharan", "Isabel Moore", "Gregory Stevens", "Henry X. Liu"], "title": "Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection", "categories": ["cs.CV"], "comment": null, "summary": "With an ever-increasing availability of data, it has become more and more\nchallenging to select and label appropriate samples for the training of machine\nlearning models. It is especially difficult to detect long-tail classes of\ninterest in large amounts of unlabeled data. This holds especially true for\nIntelligent Transportation Systems (ITS), where vehicle fleets and roadside\nperception systems generate an abundance of raw data. While industrial,\nproprietary data engines for such iterative data selection and model training\nprocesses exist, researchers and the open-source community suffer from a lack\nof an openly available system. We present the Mcity Data Engine, which provides\nmodules for the complete data-based development cycle, beginning at the data\nacquisition phase and ending at the model deployment stage. The Mcity Data\nEngine focuses on rare and novel classes through an open-vocabulary data\nselection process. All code is publicly available on GitHub under an MIT\nlicense: https://github.com/mcity/mcity_data_engine", "AI": {"tldr": "Mcity Data Engine\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u9009\u62e9\u548c\u6807\u8bb0\u6837\u672c\uff0c\u7279\u522b\u5173\u6ce8\u7a00\u6709\u548c\u65b0\u7c7b\u522b\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u6570\u636e\u91cf\u7684\u589e\u52a0\uff0c\u9009\u62e9\u548c\u6807\u8bb0\u6837\u672c\u4ee5\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u3002\u73b0\u6709\u5de5\u4e1a\u6570\u636e\u5f15\u64ce\u591a\u4e3a\u4e13\u6709\uff0c\u7f3a\u4e4f\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002", "method": "Mcity Data Engine\u63d0\u4f9b\u5b8c\u6574\u7684\u6570\u636e\u5f00\u53d1\u5468\u671f\u6a21\u5757\uff0c\u5305\u62ec\u6570\u636e\u91c7\u96c6\u5230\u6a21\u578b\u90e8\u7f72\uff0c\u91c7\u7528\u5f00\u653e\u8bcd\u6c47\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u8be5\u7cfb\u7edf\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\uff0c\u652f\u6301MIT\u8bb8\u53ef\u8bc1\u3002", "conclusion": "Mcity Data Engine\u586b\u8865\u4e86\u5f00\u6e90\u6570\u636e\u5f15\u64ce\u7684\u7a7a\u767d\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u7a00\u6709\u548c\u65b0\u7c7b\u522b\u7684\u6570\u636e\u3002"}}
{"id": "2504.21064", "pdf": "https://arxiv.org/pdf/2504.21064", "abs": "https://arxiv.org/abs/2504.21064", "authors": ["Chengkai Yang", "Xingping Dong", "Xiaofen Zong"], "title": "Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data-driven approaches for depression diagnosis have emerged as a significant\nresearch focus in neuromedicine, driven by the development of relevant\ndatasets. Recently, graph neural network (GNN)-based models have gained\nwidespread adoption due to their ability to capture brain channel functional\nconnectivity from both spatial and temporal perspectives. However, their\neffectiveness is hindered by the absence of a robust temporal biomarker. In\nthis paper, we introduce a novel and effective biomarker for depression\ndiagnosis by leveraging the discrete Fourier transform (DFT) and propose a\ncustomized graph network architecture based on Temporal Graph Convolutional\nNetwork (TGCN). Our model was trained on a dataset comprising 1,086 subjects,\nwhich is over 10 times larger than previous datasets in the field of depression\ndiagnosis. Furthermore, to align with medical requirements, we performed\npropensity score matching (PSM) to create a refined subset, referred to as the\nPSM dataset. Experimental results demonstrate that incorporating our newly\ndesigned biomarker enhances the representation of temporal characteristics in\nbrain channels, leading to improved F1 scores in both the real-world dataset\nand the PSM dataset. This advancement has the potential to contribute to the\ndevelopment of more effective depression diagnostic tools. In addition, we used\nSHapley Additive exPlaination (SHAP) to validate the interpretability of our\nmodel, ensuring its practical applicability in medical settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08DFT\uff09\u7684\u65b0\u578b\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5e76\u7ed3\u5408\u81ea\u5b9a\u4e49\u7684\u65f6\u5e8f\u56fe\u5377\u79ef\u7f51\u7edc\uff08TGCN\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u6291\u90c1\u75c7\u8bca\u65ad\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u751f\u7269\u6807\u5fd7\u7269\u63d0\u5347\u4e86\u8111\u901a\u9053\u65f6\u95f4\u7279\u5f81\u7684\u8868\u793a\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86F1\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6291\u90c1\u75c7\u8bca\u65ad\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u7684\u65f6\u95f4\u751f\u7269\u6807\u5fd7\u7269\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u5229\u7528DFT\u63d0\u53d6\u65f6\u95f4\u751f\u7269\u6807\u5fd7\u7269\uff0c\u8bbe\u8ba1\u81ea\u5b9a\u4e49TGCN\u67b6\u6784\uff0c\u5e76\u5728\u5305\u542b1,086\u540d\u53d7\u8bd5\u8005\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u503e\u5411\u5f97\u5206\u5339\u914d\uff08PSM\uff09\u751f\u6210\u4f18\u5316\u5b50\u96c6\u3002", "result": "\u65b0\u751f\u7269\u6807\u5fd7\u7269\u663e\u8457\u63d0\u5347\u4e86\u8111\u901a\u9053\u65f6\u95f4\u7279\u5f81\u7684\u8868\u793a\u80fd\u529b\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u548cPSM\u6570\u636e\u96c6\u4e0a\u5747\u63d0\u9ad8\u4e86F1\u5206\u6570\u3002SHAP\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6291\u90c1\u75c7\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u533b\u7597\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.21646", "pdf": "https://arxiv.org/pdf/2504.21646", "abs": "https://arxiv.org/abs/2504.21646", "authors": ["Liqin Wang", "Qianyue Hu", "Wei Lu", "Xiangyang Luo"], "title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection", "categories": ["cs.CV"], "comment": null, "summary": "The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun.", "AI": {"tldr": "DiffAIM\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5bf9\u6297\u6027\u4eba\u8138\u751f\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u901a\u8fc7\u751f\u6210\u81ea\u7136\u4e14\u9ad8\u8fc1\u79fb\u6027\u7684\u5bf9\u6297\u6027\u4eba\u8138\u6765\u5bf9\u6297\u6076\u610f\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u3002", "motivation": "\u7531\u4e8e\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u6210\u529f\u5bfc\u81f4\u9690\u79c1\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u81ea\u7136\u7684\u4fdd\u62a4\u9690\u79c1\u7684\u4eba\u8138\u56fe\u50cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u6ce8\u5165\u68af\u5ea6\u5bf9\u6297\u6027\u8eab\u4efd\u5f15\u5bfc\uff0c\u4f18\u5316\u8eab\u4efd\u6536\u655b\u548c\u76ee\u6807\u8bed\u4e49\u5206\u79bb\uff0c\u540c\u65f6\u4fdd\u6301\u9762\u90e8\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiffAIM\u5728\u653b\u51fb\u8fc1\u79fb\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5546\u4e1aAPI\uff08\u5982Face++\u548cAliyun\uff09\u4e0a\u6709\u6548\u3002", "conclusion": "DiffAIM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u7136\u7684\u65b9\u6cd5\u6765\u4fdd\u62a4\u4eba\u8138\u9690\u79c1\uff0c\u5bf9\u6297\u6076\u610f\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u3002"}}
{"id": "2504.21065", "pdf": "https://arxiv.org/pdf/2504.21065", "abs": "https://arxiv.org/abs/2504.21065", "authors": ["Anjie Qiao", "Junjie Xie", "Weifeng Huang", "Hao Zhang", "Jiahua Rao", "Shuangjia Zheng", "Yuedong Yang", "Zhen Wang", "Guo-Bo Li", "Jinping Lei"], "title": "A 3D pocket-aware and affinity-guided diffusion model for lead optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Molecular optimization, aimed at improving binding affinity or other\nmolecular properties, is a crucial task in drug discovery that often relies on\nthe expertise of medicinal chemists. Recently, deep learning-based 3D\ngenerative models showed promise in enhancing the efficiency of molecular\noptimization. However, these models often struggle to adequately consider\nbinding affinities with protein targets during lead optimization. Herein, we\npropose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop,\nto optimize molecules with enhanced binding affinity. The model explicitly\nincorporates the knowledge of protein-ligand binding affinity to guide the\ndenoising sampling for molecule generation with high affinity. The\ncomprehensive evaluations indicated that Diffleop outperforms baseline models\nacross multiple metrics, especially in terms of binding affinity.", "AI": {"tldr": "Diffleop\u662f\u4e00\u79cd3D\u53e3\u888b\u611f\u77e5\u548c\u4eb2\u548c\u529b\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316\u5206\u5b50\u7ed3\u5408\u4eb2\u548c\u529b\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5b50\u4f18\u5316\u4e2d\u5e38\u5ffd\u89c6\u7ed3\u5408\u4eb2\u548c\u529b\uff0cDiffleop\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6a21\u578b\u901a\u8fc7\u86cb\u767d-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u77e5\u8bc6\u5f15\u5bfc\u53bb\u566a\u91c7\u6837\uff0c\u751f\u6210\u9ad8\u4eb2\u548c\u529b\u5206\u5b50\u3002", "result": "Diffleop\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728\u7ed3\u5408\u4eb2\u548c\u529b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Diffleop\u4e3a\u5206\u5b50\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u4eb2\u548c\u529b\u63d0\u5347\u3002"}}
{"id": "2504.21650", "pdf": "https://arxiv.org/pdf/2504.21650", "abs": "https://arxiv.org/abs/2504.21650", "authors": ["Haiyang Zhou", "Wangbo Yu", "Jiawen Guan", "Xinhua Cheng", "Yonghong Tian", "Li Yuan"], "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation", "categories": ["cs.CV"], "comment": "Project homepage: https://zhouhyocean.github.io/holotime/", "summary": "The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications.", "AI": {"tldr": "HoloTime\u6846\u67b6\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u5168\u666f\u89c6\u9891\uff0c\u5e76\u901a\u8fc74D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u63d0\u5347VR/AR\u6c89\u6d78\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u9759\u60013D\u573a\u666f\u6216\u5bf9\u8c61\u7ea7\u52a8\u6001\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6c89\u6d78\u5f0f4D\u4f53\u9a8c\u9700\u6c42\u3002", "method": "\u63d0\u51faHoloTime\u6846\u67b6\uff0c\u5305\u62ec\u5168\u666f\u89c6\u9891\u751f\u6210\uff08Panoramic Animator\uff09\u548c4D\u573a\u666f\u91cd\u5efa\uff08Panoramic Space-Time Reconstruction\uff09\u3002", "result": "\u65b9\u6cd5\u5728\u751f\u6210\u5168\u666f\u89c6\u9891\u548c4D\u91cd\u5efa\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u63d0\u5347\u4e86\u6c89\u6d78\u611f\u3002", "conclusion": "HoloTime\u80fd\u521b\u9020\u66f4\u771f\u5b9e\u3001\u6c89\u6d78\u7684VR/AR\u73af\u5883\uff0c\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2504.21066", "pdf": "https://arxiv.org/pdf/2504.21066", "abs": "https://arxiv.org/abs/2504.21066", "authors": ["Andreas Karathanasis", "John Violos", "Ioannis Kompatsiaris", "Symeon Papadopoulos"], "title": "A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Training and deploying deepfake detection models on edge devices offers the\nadvantage of maintaining data privacy and confidentiality by processing it\nclose to its source. However, this approach is constrained by the limited\ncomputational and memory resources available at the edge. To address this\nchallenge, we explore compression techniques to reduce computational demands\nand inference time, alongside transfer learning methods to minimize training\noverhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate\nthe effectiveness of pruning, knowledge distillation (KD), quantization,\nfine-tuning, and adapter-based techniques. Our experimental results demonstrate\nthat both compression and transfer learning can be effectively achieved, even\nwith a high compression level of 90%, remaining at the same performance level\nwhen the training and validation data originate from the same DeepFake model.\nHowever, when the testing dataset is generated by DeepFake models not present\nin the training set, a domain generalization issue becomes evident.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u7684\u538b\u7f29\u548c\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u9ad8\u538b\u7f29\u7387\u4e0b\u6027\u80fd\u4fdd\u6301\uff0c\u4f46\u5b58\u5728\u57df\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u6709\u9650\uff0c\u9700\u901a\u8fc7\u538b\u7f29\u548c\u8fc1\u79fb\u5b66\u4e60\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u548c\u8bad\u7ec3\u5f00\u9500\u3002", "method": "\u4f7f\u7528\u526a\u679d\u3001\u77e5\u8bc6\u84b8\u998f\u3001\u91cf\u5316\u3001\u5fae\u8c03\u548c\u9002\u914d\u5668\u6280\u672f\uff0c\u5728Synthbuster\u3001RAISE\u548cForenSynths\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u5728\u76f8\u540cDeepFake\u6a21\u578b\u751f\u6210\u7684\u6570\u636e\u4e0a\uff0c90%\u538b\u7f29\u7387\u4e0b\u6027\u80fd\u4e0d\u53d8\uff1b\u4f46\u5728\u672a\u89c1\u8fc7\u7684\u6a21\u578b\u6570\u636e\u4e0a\u51fa\u73b0\u57df\u6cdb\u5316\u95ee\u9898\u3002", "conclusion": "\u538b\u7f29\u548c\u8fc1\u79fb\u5b66\u4e60\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u53ef\u884c\uff0c\u4f46\u9700\u89e3\u51b3\u57df\u6cdb\u5316\u95ee\u9898\u4ee5\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.21682", "pdf": "https://arxiv.org/pdf/2504.21682", "abs": "https://arxiv.org/abs/2504.21682", "authors": ["Yan Shu", "Weichao Zeng", "Fangmin Zhao", "Zeyu Chen", "Zhenhang Li", "Xiaomeng Yang", "Yu Zhou", "Paolo Rota", "Xiang Bai", "Lianwen Jin", "Xu-Cheng Yin", "Nicu Sebe"], "title": "Visual Text Processing: A Comprehensive Review and Unified Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Visual text is a crucial component in both document and scene images,\nconveying rich semantic information and attracting significant attention in the\ncomputer vision community. Beyond traditional tasks such as text detection and\nrecognition, visual text processing has witnessed rapid advancements driven by\nthe emergence of foundation models, including text image reconstruction and\ntext image manipulation. Despite significant progress, challenges remain due to\nthe unique properties that differentiate text from general objects. Effectively\ncapturing and leveraging these distinct textual characteristics is essential\nfor developing robust visual text processing models. In this survey, we present\na comprehensive, multi-perspective analysis of recent advancements in visual\ntext processing, focusing on two key questions: (1) What textual features are\nmost suitable for different visual text processing tasks? (2) How can these\ndistinctive text features be effectively incorporated into processing\nframeworks? Furthermore, we introduce VTPBench, a new benchmark that\nencompasses a broad range of visual text processing datasets. Leveraging the\nadvanced visual quality assessment capabilities of multimodal large language\nmodels (MLLMs), we propose VTPScore, a novel evaluation metric designed to\nensure fair and reliable evaluation. Our empirical study with more than 20\nspecific models reveals substantial room for improvement in the current\ntechniques. Our aim is to establish this work as a fundamental resource that\nfosters future exploration and innovation in the dynamic field of visual text\nprocessing. The relevant repository is available at\nhttps://github.com/shuyansy/Visual-Text-Processing-survey.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u6587\u672c\u5904\u7406\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86VTPBench\u57fa\u51c6\u548cVTPScore\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5206\u6790\u4e8620\u591a\u4e2a\u6a21\u578b\uff0c\u6307\u51fa\u5f53\u524d\u6280\u672f\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u89c6\u89c9\u6587\u672c\u5728\u6587\u6863\u548c\u573a\u666f\u56fe\u50cf\u4e2d\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u56e0\u5176\u72ec\u7279\u5c5e\u6027\uff0c\u5904\u7406\u4ecd\u9762\u4e34\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u591a\u89c6\u89d2\u5206\u6790\uff0c\u63a8\u52a8\u89c6\u89c9\u6587\u672c\u5904\u7406\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u91c7\u7528\u591a\u89c6\u89d2\u5206\u6790\u65b9\u6cd5\uff0c\u63d0\u51faVTPBench\u57fa\u51c6\u548cVTPScore\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6280\u672f\u5728\u89c6\u89c9\u6587\u672c\u5904\u7406\u65b9\u9762\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u65e8\u5728\u4e3a\u89c6\u89c9\u6587\u672c\u5904\u7406\u9886\u57df\u63d0\u4f9b\u57fa\u7840\u8d44\u6e90\uff0c\u4fc3\u8fdb\u672a\u6765\u63a2\u7d22\u548c\u521b\u65b0\u3002"}}
{"id": "2504.21072", "pdf": "https://arxiv.org/pdf/2504.21072", "abs": "https://arxiv.org/abs/2504.21072", "authors": ["Jonas Henry Grebe", "Tobias Braun", "Marcus Rohrbach", "Anna Rohrbach"], "title": "Erased but Not Forgotten: How Backdoors Compromise Concept Erasure", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "The expansion of large-scale text-to-image diffusion models has raised\ngrowing concerns about their potential to generate undesirable or harmful\ncontent, ranging from fabricated depictions of public figures to sexually\nexplicit images. To mitigate these risks, prior work has devised machine\nunlearning techniques that attempt to erase unwanted concepts through\nfine-tuning. However, in this paper, we introduce a new threat model, Toxic\nErasure (ToxE), and demonstrate how recent unlearning algorithms, including\nthose explicitly designed for robustness, can be circumvented through targeted\nbackdoor attacks. The threat is realized by establishing a link between a\ntrigger and the undesired content. Subsequent unlearning attempts fail to erase\nthis link, allowing adversaries to produce harmful content. We instantiate ToxE\nvia two established backdoor attacks: one targeting the text encoder and\nanother manipulating the cross-attention layers. Further, we introduce Deep\nIntervention Score-based Attack (DISA), a novel, deeper backdoor attack that\noptimizes the entire U-Net using a score-based objective, improving the\nattack's persistence across different erasure methods. We evaluate five recent\nconcept erasure methods against our threat model. For celebrity identity\nerasure, our deep attack circumvents erasure with up to 82% success, averaging\n57% across all erasure methods. For explicit content erasure, ToxE attacks can\nelicit up to 9 times more exposed body parts, with DISA yielding an average\nincrease by a factor of 2.9. These results highlight a critical security gap in\ncurrent unlearning strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5a01\u80c1\u6a21\u578bToxic Erasure (ToxE)\uff0c\u5c55\u793a\u4e86\u73b0\u6709\u53bb\u5b66\u4e60\u7b97\u6cd5\u5982\u4f55\u88ab\u9488\u5bf9\u6027\u540e\u95e8\u653b\u51fb\u7ed5\u8fc7\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u6df1\u7684\u653b\u51fb\u65b9\u6cd5DISA\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u53bb\u5b66\u4e60\u7b56\u7565\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u6269\u5c55\u5f15\u53d1\u4e86\u5bf9\u5176\u751f\u6210\u4e0d\u826f\u6216\u6709\u5bb3\u5185\u5bb9\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u53bb\u5b66\u4e60\u6280\u672f\u8bd5\u56fe\u901a\u8fc7\u5fae\u8c03\u6d88\u9664\u8fd9\u4e9b\u5185\u5bb9\uff0c\u4f46\u5b58\u5728\u88ab\u653b\u51fb\u7684\u98ce\u9669\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86ToxE\u5a01\u80c1\u6a21\u578b\uff0c\u901a\u8fc7\u5efa\u7acb\u89e6\u53d1\u5668\u4e0e\u4e0d\u826f\u5185\u5bb9\u7684\u94fe\u63a5\u7ed5\u8fc7\u53bb\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86DISA\u653b\u51fb\u65b9\u6cd5\uff0c\u4f18\u5316\u6574\u4e2aU-Net\u4ee5\u589e\u5f3a\u653b\u51fb\u6301\u4e45\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDISA\u653b\u51fb\u5728\u540d\u4eba\u8eab\u4efd\u6d88\u9664\u4efb\u52a1\u4e2d\u6700\u9ad8\u6210\u529f\u738782%\uff0c\u5e73\u574757%\uff1b\u5728\u4e0d\u826f\u5185\u5bb9\u6d88\u9664\u4efb\u52a1\u4e2d\uff0c\u66b4\u9732\u8eab\u4f53\u90e8\u5206\u6700\u591a\u589e\u52a09\u500d\uff0cDISA\u5e73\u5747\u589e\u52a02.9\u500d\u3002", "conclusion": "\u5f53\u524d\u53bb\u5b66\u4e60\u7b56\u7565\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6cd5\u3002"}}
{"id": "2504.21692", "pdf": "https://arxiv.org/pdf/2504.21692", "abs": "https://arxiv.org/abs/2504.21692", "authors": ["Zihan Zhou", "Changrui Dai", "Aibo Song", "Xiaolin Fang"], "title": "Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Successful video analysis relies on accurate recognition of pixels across\nframes, and frame reconstruction methods based on video correspondence learning\nare popular due to their efficiency. Existing frame reconstruction methods,\nwhile efficient, neglect the value of direct involvement of multiple reference\nframes for reconstruction and decision-making aspects, especially in complex\nsituations such as occlusion or fast movement. In this paper, we introduce a\nDynamic Memory Prediction (DMP) framework that innovatively utilizes multiple\nreference frames to concisely and directly enhance frame reconstruction. Its\ncore component is a Reference Frame Memory Engine that dynamically selects\nframes based on object pixel features to improve tracking accuracy. In\naddition, a Bidirectional Target Prediction Network is built to utilize\nmultiple reference frames to improve the robustness of the model. Through\nexperiments, our algorithm outperforms the state-of-the-art self-supervised\ntechniques on two fine-grained video object tracking tasks: object segmentation\nand keypoint tracking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5185\u5b58\u9884\u6d4b\uff08DMP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u53c2\u8003\u5e27\u76f4\u63a5\u589e\u5f3a\u5e27\u91cd\u5efa\uff0c\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u5e27\u91cd\u5efa\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u906e\u6321\u6216\u5feb\u901f\u8fd0\u52a8\uff09\u4e2d\u5ffd\u89c6\u591a\u53c2\u8003\u5e27\u7684\u76f4\u63a5\u53c2\u4e0e\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u52a8\u6001\u9009\u62e9\u53c2\u8003\u5e27\u7684\u5185\u5b58\u5f15\u64ce\u548c\u53cc\u5411\u76ee\u6807\u9884\u6d4b\u7f51\u7edc\uff0c\u63d0\u5347\u8ddf\u8e2a\u548c\u91cd\u5efa\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u7ec6\u7c92\u5ea6\u89c6\u9891\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\uff08\u5bf9\u8c61\u5206\u5272\u548c\u5173\u952e\u70b9\u8ddf\u8e2a\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "DMP\u6846\u67b6\u901a\u8fc7\u591a\u53c2\u8003\u5e27\u52a8\u6001\u9009\u62e9\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.21074", "pdf": "https://arxiv.org/pdf/2504.21074", "abs": "https://arxiv.org/abs/2504.21074", "authors": ["Adrian Rebmann", "Fabian David Schmidt", "Goran Glava\u0161", "Han van der Aa"], "title": "On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks", "categories": ["cs.DB", "cs.AI"], "comment": "31 pages, submitted to PS", "summary": "Large language models (LLMs) have shown to be valuable tools for tackling\nprocess mining tasks. Existing studies report on their capability to support\nvarious data-driven process analyses and even, to some extent, that they are\nable to reason about how processes work. This reasoning ability suggests that\nthere is potential for LLMs to tackle semantics-aware process mining tasks,\nwhich are tasks that rely on an understanding of the meaning of activities and\ntheir relationships. Examples of these include process discovery, where the\nmeaning of activities can indicate their dependency, whereas in anomaly\ndetection the meaning can be used to recognize process behavior that is\nabnormal. In this paper, we systematically explore the capabilities of LLMs for\nsuch tasks. Unlike prior work, which largely evaluates LLMs in their default\nstate, we investigate their utility through both in-context learning and\nsupervised fine-tuning. Concretely, we define five process mining tasks\nrequiring semantic understanding and provide extensive benchmarking datasets\nfor evaluation. Our experiments reveal that while LLMs struggle with\nchallenging process mining tasks when used out of the box or with minimal\nin-context examples, they achieve strong performance when fine-tuned for these\ntasks across a broad range of process types and industries.", "AI": {"tldr": "LLMs\u5728\u8fc7\u7a0b\u6316\u6398\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u8bed\u4e49\u611f\u77e5\u4efb\u52a1\u4e2d\u3002\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\uff0cLLMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u8bed\u4e49\u611f\u77e5\u8fc7\u7a0b\u6316\u6398\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u9ed8\u8ba4\u72b6\u6001\u548c\u5fae\u8c03\u540e\u6027\u80fd\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u5b9a\u4e49\u4e86\u4e94\u4e2a\u9700\u8981\u8bed\u4e49\u7406\u89e3\u7684\u8fc7\u7a0b\u6316\u6398\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u8bc4\u4f30LLMs\u7684\u6027\u80fd\u3002", "result": "LLMs\u5728\u9ed8\u8ba4\u72b6\u6001\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u7ecf\u8fc7\u5fae\u8c03\u540e\u80fd\u5728\u591a\u79cd\u8fc7\u7a0b\u548c\u884c\u4e1a\u4e2d\u53d6\u5f97\u5f3a\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u8bed\u4e49\u611f\u77e5\u8fc7\u7a0b\u6316\u6398\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.21699", "pdf": "https://arxiv.org/pdf/2504.21699", "abs": "https://arxiv.org/abs/2504.21699", "authors": ["Abu Mohammed Raisuddin", "Jesper Holmblad", "Hamed Haghighi", "Yuri Poledna", "Maikol Funk Drechsler", "Valentina Donzella", "Eren Erdal Aksoy"], "title": "REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor degradation poses a significant challenge in autonomous driving.\nDuring heavy rainfall, the interference from raindrops can adversely affect the\nquality of LiDAR point clouds, resulting in, for instance, inaccurate point\nmeasurements. This, in turn, can potentially lead to safety concerns if\nautonomous driving systems are not weather-aware, i.e., if they are unable to\ndiscern such changes. In this study, we release a new, large-scale, multi-modal\nemulated rain dataset, REHEARSE-3D, to promote research advancements in 3D\npoint cloud de-raining. Distinct from the most relevant competitors, our\ndataset is unique in several respects. First, it is the largest point-wise\nannotated dataset, and second, it is the only one with high-resolution LiDAR\ndata (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and\nnighttime conditions in a controlled weather environment. Furthermore,\nREHEARSE-3D involves rain-characteristic information, which is of significant\nvalue not only for sensor noise modeling but also for analyzing the impact of\nweather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop\ndetection and removal in fused LiDAR and 4D Radar point clouds. Our\ncomprehensive study further evaluates the performance of various statistical\nand deep-learning models. Upon publication, the dataset and benchmark models\nwill be made publicly available at: https://sporsho.github.io/REHEARSE3D.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aREHEARSE-3D\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u62df\u964d\u96e8\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4fc3\u8fdb3D\u70b9\u4e91\u53bb\u96e8\u7814\u7a76\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u611f\u5668\u9000\u5316\uff08\u5982\u964d\u96e8\u5bf9LiDAR\u70b9\u4e91\u7684\u5e72\u6270\uff09\u662f\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u5929\u6c14\u611f\u77e5\u7684\u7cfb\u7edf\u6765\u89e3\u51b3\u5b89\u5168\u95ee\u9898\u3002", "method": "\u53d1\u5e03\u4e86REHEARSE-3D\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u5206\u8fa8\u7387LiDAR\u548c4D\u96f7\u8fbe\u70b9\u4e91\uff0c\u5e76\u6807\u6ce8\u4e86\u964d\u96e8\u7279\u5f81\u4fe1\u606f\u3002", "result": "\u6570\u636e\u96c6\u662f\u6700\u5927\u7684\u70b9\u7ea7\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u79cd\u6a21\u578b\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "conclusion": "REHEARSE-3D\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6a21\u578b\u5c06\u516c\u5f00\uff0c\u4ee5\u63a8\u52a83D\u70b9\u4e91\u53bb\u96e8\u7814\u7a76\u3002"}}
{"id": "2504.21099", "pdf": "https://arxiv.org/pdf/2504.21099", "abs": "https://arxiv.org/abs/2504.21099", "authors": ["Jieming Bian", "Yuanzhe Peng", "Lei Wang", "Yin Huang", "Jie Xu"], "title": "A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "survey paper, under updating", "summary": "Foundation models have revolutionized artificial intelligence by providing\nrobust, versatile architectures pre-trained on large-scale datasets. However,\nadapting these massive models to specific downstream tasks requires\nfine-tuning, which can be prohibitively expensive in computational resources.\nParameter-Efficient Fine-Tuning (PEFT) methods address this challenge by\nselectively updating only a small subset of parameters. Meanwhile, Federated\nLearning (FL) enables collaborative model training across distributed clients\nwithout sharing raw data, making it ideal for privacy-sensitive applications.\nThis survey provides a comprehensive review of the integration of PEFT\ntechniques within federated learning environments. We systematically categorize\nexisting approaches into three main groups: Additive PEFT (which introduces new\ntrainable parameters), Selective PEFT (which fine-tunes only subsets of\nexisting parameters), and Reparameterized PEFT (which transforms model\narchitectures to enable efficient updates). For each category, we analyze how\nthese methods address the unique challenges of federated settings, including\ndata heterogeneity, communication efficiency, computational constraints, and\nprivacy concerns. We further organize the literature based on application\ndomains, covering both natural language processing and computer vision tasks.\nFinally, we discuss promising research directions, including scaling to larger\nfoundation models, theoretical analysis of federated PEFT methods, and\nsustainable approaches for resource-constrained environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u6574\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u5206\u4e3a\u4e09\u7c7b\uff1a\u6dfb\u52a0\u578b\u3001\u9009\u62e9\u578b\u548c\u91cd\u53c2\u6570\u5316\u578b\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728\u6570\u636e\u5f02\u6784\u6027\u3001\u901a\u4fe1\u6548\u7387\u7b49\u65b9\u9762\u7684\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u5fae\u8c03\u65f6\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709PEFT\u65b9\u6cd5\u4e3a\u6dfb\u52a0\u578b\u3001\u9009\u62e9\u578b\u548c\u91cd\u53c2\u6570\u5316\u578b\uff0c\u5e76\u5206\u6790\u5176\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9002\u5e94\u6027\u3002", "result": "\u603b\u7ed3\u4e86PEFT\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u8986\u76d6\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u3001\u7406\u8bba\u5206\u6790\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u7684\u53ef\u6301\u7eed\u65b9\u6cd5\u3002"}}
{"id": "2504.21706", "pdf": "https://arxiv.org/pdf/2504.21706", "abs": "https://arxiv.org/abs/2504.21706", "authors": ["Saber Mehdipour", "Seyed Abolghasem Mirroshandel", "Seyed Amirhossein Tabatabaei"], "title": "Vision Transformers in Precision Agriculture: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting plant diseases is a crucial aspect of modern agriculture - it plays\na key role in maintaining crop health and increasing overall yield. Traditional\napproaches, though still valuable, often rely on manual inspection or\nconventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering benefits such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This survey\nexplores the application of ViTs in precision agriculture, covering tasks from\nclassification to detection and segmentation. We begin by introducing the\nfoundational architecture of ViTs and discuss their transition from Natural\nLanguage Processing (NLP) to computer vision. The discussion includes the\nconcept of inductive bias in traditional models like Convolutional Neural\nNetworks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive\nreview of recent literature, focusing on key methodologies, datasets, and\nperformance metrics. The survey also includes a comparative analysis of CNNs\nand ViTs, with a look at hybrid models and performance enhancements. Technical\nchallenges - such as data requirements, computational demands, and model\ninterpretability - are addressed alongside potential solutions. Finally, we\noutline potential research directions and technological advancements that could\nfurther support the integration of ViTs in real-world agricultural settings.\nOur goal with this study is to offer practitioners and researchers a deeper\nunderstanding of how ViTs are poised to transform smart and precision\nagriculture.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u53d8\u6362\u5668\uff08ViTs\uff09\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u4ece\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5230\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u8f6c\u53d8\uff0c\u5e76\u4e0e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u4f20\u7edf\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0cViTs\u56e0\u5176\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u89c6\u89c9\u4efb\u52a1\u7684\u4f18\u52bf\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4ecb\u7ecd\u4e86ViTs\u7684\u57fa\u7840\u67b6\u6784\uff0c\u8ba8\u8bba\u4e86\u5176\u4e0eCNNs\u7684\u5dee\u5f02\uff0c\u7efc\u8ff0\u4e86\u5173\u952e\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u6027\u80fd\u6307\u6807\uff0c\u5e76\u5206\u6790\u4e86\u6280\u672f\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u3002", "result": "ViTs\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "ViTs\u6709\u671b\u63a8\u52a8\u667a\u80fd\u548c\u7cbe\u51c6\u519c\u4e1a\u7684\u53d1\u5c55\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6570\u636e\u9700\u6c42\u3001\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7b49\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2504.21152", "pdf": "https://arxiv.org/pdf/2504.21152", "abs": "https://arxiv.org/abs/2504.21152", "authors": ["Shayan Alahyari", "Mike Domaratzki"], "title": "SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Imbalanced regression refers to prediction tasks where the target variable is\nskewed. This skewness hinders machine learning models, especially neural\nnetworks, which concentrate on dense regions and therefore perform poorly on\nunderrepresented (minority) samples. Despite the importance of this problem,\nonly a few methods have been proposed for imbalanced regression. Many of the\navailable solutions for imbalanced regression adapt techniques from the class\nimbalance domain, such as linear interpolation and the addition of Gaussian\nnoise, to create synthetic data in sparse regions. However, in many cases, the\nunderlying distribution of the data is complex and non-linear. Consequently,\nthese approaches generate synthetic samples that do not accurately represent\nthe true feature-target relationship. To overcome these limitations, we propose\nSMOGAN, a two-step oversampling framework for imbalanced regression. In Stage\n1, an existing oversampler generates initial synthetic samples in sparse target\nregions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves\nas SMOGAN's filtering layer and refines these samples via adversarial loss\naugmented with a Maximum Mean Discrepancy objective, aligning them with the\ntrue joint feature-target distribution. Extensive experiments on 23 imbalanced\ndatasets show that SMOGAN consistently outperforms the default oversampling\nmethod without the DistGAN filtering layer.", "AI": {"tldr": "SMOGAN\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8fc7\u91c7\u6837\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u548c\u8fc7\u6ee4\u5408\u6210\u6837\u672c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e0d\u5e73\u8861\u56de\u5f52\u4e2d\u76ee\u6807\u53d8\u91cf\u7684\u504f\u6001\u5206\u5e03\u5bfc\u81f4\u6a21\u578b\u5728\u7a00\u758f\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u5408\u6210\u6837\u672c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u771f\u5b9e\u5206\u5e03\u3002", "method": "SMOGAN\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u521d\u59cb\u5408\u6210\u6837\u672c\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7DistGAN\uff08\u4e00\u79cd\u5206\u5e03\u611f\u77e5GAN\uff09\u8fc7\u6ee4\u548c\u4f18\u5316\u6837\u672c\u3002", "result": "\u572823\u4e2a\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSMOGAN\u663e\u8457\u4f18\u4e8e\u672a\u4f7f\u7528DistGAN\u8fc7\u6ee4\u5c42\u7684\u9ed8\u8ba4\u8fc7\u91c7\u6837\u65b9\u6cd5\u3002", "conclusion": "SMOGAN\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u548c\u8fc7\u6ee4\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7a00\u758f\u533a\u57df\u7684\u6027\u80fd\u3002"}}
{"id": "2504.21718", "pdf": "https://arxiv.org/pdf/2504.21718", "abs": "https://arxiv.org/abs/2504.21718", "authors": ["Shiying Li", "Xingqun Qi", "Bingkun Yang", "Chen Weile", "Zezhao Tian", "Muyi Sun", "Qifeng Liu", "Man Zhang", "Zhenan Sun"], "title": "VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Generating responsive listener head dynamics with nuanced emotions and\nexpressive reactions is crucial for practical dialogue modeling in various\nvirtual avatar animations. Previous studies mainly focus on the direct\nshort-term production of listener behavior. They overlook the fine-grained\ncontrol over motion variations and emotional intensity, especially in\nlong-sequence modeling. Moreover, the lack of long-term and large-scale paired\nspeaker-listener corpora including head dynamics and fine-grained\nmulti-modality annotations (e.g., text-based expression descriptions, emotional\nintensity) also limits the application of dialogue modeling.Therefore, we first\nnewly collect a large-scale multi-turn dataset of 3D dyadic conversation\ncontaining more than 1.4M valid frames for multi-modal responsive interaction,\ndubbed ListenerX. Additionally, we propose VividListener, a novel framework\nenabling fine-grained, expressive and controllable listener dynamics modeling.\nThis framework leverages multi-modal conditions as guiding principles for\nfostering coherent interactions between speakers and listeners.Specifically, we\ndesign the Responsive Interaction Module (RIM) to adaptively represent the\nmulti-modal interactive embeddings. RIM ensures the listener dynamics achieve\nfine-grained semantic coordination with textual descriptions and adjustments,\nwhile preserving expressive reaction with speaker behavior. Meanwhile, we\ndesign the Emotional Intensity Tags (EIT) for emotion intensity editing with\nmulti-modal information integration, applying to both text descriptions and\nlistener motion amplitude.Extensive experiments conducted on our newly\ncollected ListenerX dataset demonstrate that VividListener achieves\nstate-of-the-art performance, realizing expressive and controllable listener\ndynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVividListener\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u7ec6\u817b\u60c5\u611f\u548c\u8868\u8fbe\u53cd\u5e94\u7684\u542c\u8005\u5934\u90e8\u52a8\u6001\uff0c\u5e76\u901a\u8fc7\u65b0\u6536\u96c6\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6ListenerX\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u542c\u8005\u884c\u4e3a\u7684\u77ed\u671f\u751f\u6210\uff0c\u7f3a\u4e4f\u5bf9\u8fd0\u52a8\u53d8\u5316\u548c\u60c5\u611f\u5f3a\u5ea6\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u4e14\u7f3a\u4e4f\u957f\u671f\u3001\u5927\u89c4\u6a21\u7684\u591a\u6a21\u6001\u914d\u5bf9\u8bed\u6599\u5e93\u3002", "method": "\u63d0\u51faVividListener\u6846\u67b6\uff0c\u5305\u542bResponsive Interaction Module\uff08RIM\uff09\u548cEmotional Intensity Tags\uff08EIT\uff09\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u4ea4\u4e92\u5d4c\u5165\u548c\u60c5\u611f\u5f3a\u5ea6\u7f16\u8f91\u3002", "result": "\u5728ListenerX\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVividListener\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751f\u6210\u5177\u6709\u8868\u8fbe\u6027\u548c\u53ef\u63a7\u6027\u7684\u542c\u8005\u52a8\u6001\u3002", "conclusion": "VividListener\u6846\u67b6\u89e3\u51b3\u4e86\u542c\u8005\u52a8\u6001\u5efa\u6a21\u4e2d\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u591a\u6a21\u6001\u534f\u8c03\u95ee\u9898\uff0c\u4e3a\u865a\u62df\u5bf9\u8bdd\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21749", "pdf": "https://arxiv.org/pdf/2504.21749", "abs": "https://arxiv.org/abs/2504.21749", "authors": ["Leonhard Sommer", "Olaf D\u00fcnkel", "Christian Theobalt", "Adam Kortylewski"], "title": "Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space", "categories": ["cs.CV"], "comment": null, "summary": "3D morphable models (3DMMs) are a powerful tool to represent the possible\nshapes and appearances of an object category. Given a single test image, 3DMMs\ncan be used to solve various tasks, such as predicting the 3D shape, pose,\nsemantic correspondence, and instance segmentation of an object. Unfortunately,\n3DMMs are only available for very few object categories that are of particular\ninterest, like faces or human bodies, as they require a demanding 3D data\nacquisition and category-specific training process. In contrast, we introduce a\nnew method, Common3D, that learns 3DMMs of common objects in a fully\nself-supervised manner from a collection of object-centric videos. For this\npurpose, our model represents objects as a learned 3D template mesh and a\ndeformation field that is parameterized as an image-conditioned neural network.\nDifferent from prior works, Common3D represents the object appearance with\nneural features instead of RGB colors, which enables the learning of more\ngeneralizable representations through an abstraction from pixel intensities.\nImportantly, we train the appearance features using a contrastive objective by\nexploiting the correspondences defined through the deformable template mesh.\nThis leads to higher quality correspondence features compared to related works\nand a significantly improved model performance at estimating 3D object pose and\nsemantic correspondence. Common3D is the first completely self-supervised\nmethod that can solve various vision tasks in a zero-shot manner.", "AI": {"tldr": "Common3D\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u9891\u5b66\u4e603D\u53ef\u53d8\u5f62\u6a21\u578b\uff083DMMs\uff09\uff0c\u65e0\u97003D\u6570\u636e\u91c7\u96c6\u6216\u7c7b\u522b\u7279\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u73b0\u67093DMMs\u4ec5\u9002\u7528\u4e8e\u5c11\u6570\u7c7b\u522b\uff08\u5982\u4eba\u8138\u6216\u4eba\u4f53\uff09\uff0c\u4e14\u9700\u8981\u590d\u6742\u7684\u6570\u636e\u91c7\u96c6\u548c\u8bad\u7ec3\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u5bf9\u8c61\u4e2d\u5fc3\u89c6\u9891\u81ea\u76d1\u7763\u5b66\u4e603DMMs\uff0c\u901a\u8fc7\u795e\u7ecf\u7279\u5f81\u8868\u793a\u5916\u89c2\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u7279\u5f81\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cCommon3D\u57283D\u59ff\u6001\u4f30\u8ba1\u548c\u8bed\u4e49\u5bf9\u5e94\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "Common3D\u662f\u9996\u4e2a\u5b8c\u5168\u81ea\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u80fd\u96f6\u6837\u672c\u89e3\u51b3\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2504.21155", "pdf": "https://arxiv.org/pdf/2504.21155", "abs": "https://arxiv.org/abs/2504.21155", "authors": ["Fauzan Nazranda Rizqa", "Matthew Hole", "Charles Gretton"], "title": "Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation", "categories": ["physics.plasm-ph", "cs.AI", "cs.NE"], "comment": "9 pages, 4 figures", "summary": "Our contributions are motivated by fusion reactors that rely on maintaining\nmagnetohydrodynamic (MHD) equilibrium, where the balance between plasma\npressure and confining magnetic fields is required for stable operation. In\naxisymmetric tokamak reactors in particular, and under the assumption of\ntoroidal symmetry, this equilibrium can be mathematically modelled using the\nGrad-Shafranov Equation (GSE). Recent works have demonstrated the potential of\nusing Physics-Informed Neural Networks (PINNs) to model the GSE. Existing\nstudies did not examine realistic scenarios in which a single network\ngeneralizes to a variety of boundary conditions. Addressing that limitation, we\nevaluate a PINN architecture that incorporates boundary points as network\ninputs. Additionally, we compare PINN model accuracy and inference speeds with\na Fourier Neural Operator (FNO) model. Finding the PINN model to be the most\nperformant, and accurate in our setting, we use the network verification tool\nMarabou to perform a range of verification tasks. Although we find some\ndiscrepancies between evaluations of the networks natively in PyTorch, compared\nto via Marabou, we are able to demonstrate useful and practical verification\nworkflows. Our study is the first investigation of verification of such\nnetworks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8f74\u5bf9\u79f0\u6258\u5361\u9a6c\u514b\u53cd\u5e94\u5806\u4e2d\uff0c\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u5efa\u6a21Grad-Shafranov\u65b9\u7a0b\uff08GSE\uff09\u7684\u6f5c\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u591a\u79cd\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u805a\u53d8\u53cd\u5e94\u5806\u4e2d\u78c1\u6d41\u4f53\u52a8\u529b\u5b66\uff08MHD\uff09\u5e73\u8861\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7GSE\u5efa\u6a21\u5b9e\u73b0\u7a33\u5b9a\u8fd0\u884c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528PINN\u67b6\u6784\uff0c\u5c06\u8fb9\u754c\u70b9\u4f5c\u4e3a\u7f51\u7edc\u8f93\u5165\uff0c\u5e76\u4e0e\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660ePINN\u6a21\u578b\u5728\u6027\u80fd\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u9996\u6b21\u9a8c\u8bc1\u4e86\u6b64\u7c7b\u7f51\u7edc\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u7814\u7a76\u9996\u6b21\u63a2\u7d22\u4e86\u6b64\u7c7b\u7f51\u7edc\u7684\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5b9e\u7528\u7684\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2504.21771", "pdf": "https://arxiv.org/pdf/2504.21771", "abs": "https://arxiv.org/abs/2504.21771", "authors": ["Bahram Jafrasteh", "Wei Peng", "Cheng Wan", "Yimin Luo", "Ehsan Adeli", "Qingyu Zhao"], "title": "Anatomical Similarity as a New Metric to Evaluate Brain Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "Generative models enhance neuroimaging through data augmentation, quality\nimprovement, and rare condition studies. Despite advances in realistic\nsynthetic MRIs, evaluations focus on texture and perception, lacking\nsensitivity to crucial anatomical fidelity. This study proposes a new metric,\ncalled WASABI (Wasserstein-Based Anatomical Brain Index), to assess the\nanatomical realism of synthetic brain MRIs. WASABI leverages \\textit{SynthSeg},\na deep learning-based brain parcellation tool, to derive volumetric measures of\nbrain regions in each MRI and uses the multivariate Wasserstein distance to\ncompare distributions between real and synthetic anatomies. Based on controlled\nexperiments on two real datasets and synthetic MRIs from five generative\nmodels, WASABI demonstrates higher sensitivity in quantifying anatomical\ndiscrepancies compared to traditional image-level metrics, even when synthetic\nimages achieve near-perfect visual quality. Our findings advocate for shifting\nthe evaluation paradigm beyond visual inspection and conventional metrics,\nemphasizing anatomical fidelity as a crucial benchmark for clinically\nmeaningful brain MRI synthesis. Our code is available at\nhttps://github.com/BahramJafrasteh/wasabi-mri.", "AI": {"tldr": "\u63d0\u51faWASABI\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u5408\u6210\u8111MRI\u7684\u89e3\u5256\u5b66\u771f\u5b9e\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u89c6\u89c9\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5173\u6ce8\u7eb9\u7406\u548c\u611f\u77e5\uff0c\u7f3a\u4e4f\u5bf9\u89e3\u5256\u5b66\u771f\u5b9e\u6027\u7684\u654f\u611f\u5ea6\u3002", "method": "\u5229\u7528SynthSeg\u5206\u5272\u5de5\u5177\u548cWasserstein\u8ddd\u79bb\u6bd4\u8f83\u771f\u5b9e\u4e0e\u5408\u6210MRI\u7684\u89e3\u5256\u5206\u5e03\u3002", "result": "WASABI\u5728\u91cf\u5316\u89e3\u5256\u5dee\u5f02\u4e0a\u66f4\u654f\u611f\uff0c\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u3002", "conclusion": "\u5f3a\u8c03\u89e3\u5256\u5b66\u771f\u5b9e\u6027\u662f\u4e34\u5e8aMRI\u5408\u6210\u7684\u5173\u952e\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2504.21789", "pdf": "https://arxiv.org/pdf/2504.21789", "abs": "https://arxiv.org/abs/2504.21789", "authors": ["Alessia Hu", "Regina Beets-Tan", "Lishan Cai", "Eduardo Pooch"], "title": "Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Paper accepted for publication at 2025 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)\n  Copyright 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future media", "summary": "Magnetic Resonance Imaging (MRI) plays an important role in identifying\nclinically significant prostate cancer (csPCa), yet automated methods face\nchallenges such as data imbalance, variable tumor sizes, and a lack of\nannotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which\nincorporates anomaly maps derived from biparametric MRI sequences into a deep\nlearning-based segmentation framework to improve csPCa identification. We\nconduct a comparative analysis of anomaly detection methods and evaluate the\nintegration of anomaly maps into the segmentation pipeline. Anomaly maps,\ngenerated using Fixed-Point GAN reconstruction, highlight deviations from\nnormal prostate tissue, guiding the segmentation model to potential cancerous\nregions. We compare the performance by using the average score, computed as the\nmean of the AUROC and Average Precision (AP). On the external test set, adU-Net\nachieves the best average score of 0.618, outperforming the baseline nnU-Net\nmodel (0.605). The results demonstrate that incorporating anomaly detection\ninto segmentation improves generalization and performance, particularly with\nADC-based anomaly maps, offering a promising direction for automated csPCa\nidentification.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f02\u5e38\u68c0\u6d4b\u7684U-Net\u6a21\u578b\uff08adU-Net\uff09\uff0c\u901a\u8fc7\u5f02\u5e38\u56fe\u6539\u8fdb\u524d\u5217\u817a\u764c\u7684\u81ea\u52a8\u8bc6\u522b\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "MRI\u5728\u8bc6\u522b\u4e34\u5e8a\u663e\u8457\u524d\u5217\u817a\u764c\uff08csPCa\uff09\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u81ea\u52a8\u5316\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u4e0d\u5e73\u8861\u3001\u80bf\u7624\u5927\u5c0f\u4e0d\u4e00\u548c\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u5f15\u5165adU-Net\uff0c\u5c06\u57fa\u4e8e\u53cc\u53c2\u6570MRI\u5e8f\u5217\u7684\u5f02\u5e38\u56fe\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5e76\u8bc4\u4f30\u4e86\u5f02\u5e38\u56fe\u5bf9\u5206\u5272\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5916\u90e8\u6d4b\u8bd5\u96c6\u4e0a\uff0cadU-Net\u7684\u5e73\u5747\u5f97\u5206\uff08AUROC\u548cAP\u7684\u5747\u503c\uff09\u4e3a0.618\uff0c\u4f18\u4e8e\u57fa\u7ebfnnU-Net\uff080.605\uff09\u3002", "conclusion": "\u7ed3\u5408\u5f02\u5e38\u68c0\u6d4b\u7684\u5206\u5272\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u5c24\u5176\u662f\u57fa\u4e8eADC\u7684\u5f02\u5e38\u56fe\uff0c\u4e3acsPCa\u81ea\u52a8\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.21188", "pdf": "https://arxiv.org/pdf/2504.21188", "abs": "https://arxiv.org/abs/2504.21188", "authors": ["Natnael Alemayehu"], "title": "Light Weight CNN for classification of Brain Tumors from MRI Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages", "summary": "This study presents a convolutional neural network (CNN)-based approach for\nthe multi-class classification of brain tumors using magnetic resonance imaging\n(MRI) scans. We utilize a publicly available dataset containing MRI images\ncategorized into four classes: glioma, meningioma, pituitary tumor, and no\ntumor. Our primary objective is to build a light weight deep learning model\nthat can automatically classify brain tumor types with high accuracy. To\nachieve this goal, we incorporate image preprocessing steps, including\nnormalization, data augmentation, and a cropping technique designed to reduce\nbackground noise and emphasize relevant regions. The CNN architecture is\noptimized through hyperparameter tuning using Keras Tuner, enabling systematic\nexploration of network parameters. To ensure reliable evaluation, we apply\n5-fold cross-validation, where each hyperparameter configuration is evaluated\nacross multiple data splits to mitigate overfitting. Experimental results\ndemonstrate that the proposed model achieves a classification accuracy of\n98.78%, indicating its potential as a diagnostic aid in clinical settings. The\nproposed method offers a low-complexity yet effective solution for assisting in\nearly brain tumor diagnosis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5229\u7528MRI\u626b\u63cf\u5bf9\u8111\u80bf\u7624\u8fdb\u884c\u591a\u7c7b\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe98.78%\u3002", "motivation": "\u76ee\u6807\u662f\u6784\u5efa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u591f\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5206\u7c7b\u8111\u80bf\u7624\u7c7b\u578b\uff0c\u4ee5\u8f85\u52a9\u4e34\u5e8a\u8bca\u65ad\u3002", "method": "\u91c7\u7528\u56fe\u50cf\u9884\u5904\u7406\uff08\u5f52\u4e00\u5316\u3001\u6570\u636e\u589e\u5f3a\u3001\u88c1\u526a\u6280\u672f\uff09\u548c\u4f18\u5316\u7684CNN\u67b6\u6784\uff0c\u901a\u8fc7Keras Tuner\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5e76\u4f7f\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523098.78%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e9\u671f\u8111\u80bf\u7624\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21810", "pdf": "https://arxiv.org/pdf/2504.21810", "abs": "https://arxiv.org/abs/2504.21810", "authors": ["Franko Hrzic", "Mohammadreza Movahhedi", "Ophelie Lavoie-Gagne", "Ata Kiapour"], "title": "A simple and effective approach for body part recognition on CT scans based on projection estimation", "categories": ["cs.CV", "68T01, 65D19", "I.4.0; I.4.10; I.2.1"], "comment": "19 pages, 6 figures", "summary": "It is well known that machine learning models require a high amount of\nannotated data to obtain optimal performance. Labelling Computed Tomography\n(CT) data can be a particularly challenging task due to its volumetric nature\nand often missing and$/$or incomplete associated meta-data. Even inspecting one\nCT scan requires additional computer software, or in the case of programming\nlanguages $-$ additional programming libraries. This study proposes a simple,\nyet effective approach based on 2D X-ray-like estimation of 3D CT scans for\nbody region identification. Although body region is commonly associated with\nthe CT scan, it often describes only the focused major body region neglecting\nother anatomical regions present in the observed CT. In the proposed approach,\nestimated 2D images were utilized to identify 14 distinct body regions,\nproviding valuable information for constructing a high-quality medical dataset.\nTo evaluate the effectiveness of the proposed method, it was compared against\n2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed\nthe others, where it came on top with statistical significance and F1-Score for\nthe best-performing model EffNet-B0 of 0.980 $\\pm$ 0.016 in comparison to the\n0.840 $\\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\\pm$ 0.096 (3D VoxCNN), and 0.852\n$\\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three\ndifferent clinical centers and counted 15,622 CT scans (44,135 labels).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D X\u5c04\u7ebf\u4f30\u8ba13D CT\u626b\u63cf\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u8eab\u4f53\u533a\u57df\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8eCT\u6570\u636e\u7684\u4f53\u79ef\u7279\u6027\u53ca\u5143\u6570\u636e\u7f3a\u5931\uff0c\u6807\u6ce8\u5de5\u4f5c\u590d\u6742\u4e14\u8017\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u4f30\u8ba1\u76842D\u56fe\u50cf\u8bc6\u522b14\u4e2a\u4e0d\u540c\u8eab\u4f53\u533a\u57df\uff0c\u5e76\u4e0e2.5D\u30013D\u548c\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7edf\u8ba1\u663e\u8457\u6027\u548cF1\u5206\u6570\u4e0a\u8868\u73b0\u6700\u4f73\uff08EffNet-B0\u4e3a0.980 \u00b1 0.016\uff09\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u533b\u5b66\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8eab\u4f53\u533a\u57df\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.21189", "pdf": "https://arxiv.org/pdf/2504.21189", "abs": "https://arxiv.org/abs/2504.21189", "authors": ["Gulsah Hancerliogullari Koksalmis", "Bulent Soykan", "Laura J. Brattain", "Hsin-Hsiung Huang"], "title": "Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": "25 pages, 11 figures", "summary": "Alzheimer's Disease (AD) is marked by significant inter-individual\nvariability in its progression, complicating accurate prognosis and\npersonalized care planning. This heterogeneity underscores the critical need\nfor predictive models capable of forecasting patient-specific disease\ntrajectories. Artificial Intelligence (AI) offers powerful tools to address\nthis challenge by analyzing complex, multi-modal, and longitudinal patient\ndata. This paper provides a comprehensive survey of AI methodologies applied to\npersonalized AD progression prediction. We review key approaches including\nstate-space models for capturing temporal dynamics, deep learning techniques\nlike Recurrent Neural Networks for sequence modeling, Graph Neural Networks\n(GNNs) for leveraging network structures, and the emerging concept of AI-driven\ndigital twins for individualized simulation. Recognizing that data limitations\noften impede progress, we examine common challenges such as high\ndimensionality, missing data, and dataset imbalance. We further discuss\nAI-driven mitigation strategies, with a specific focus on synthetic data\ngeneration using Variational Autoencoders (VAEs) and Generative Adversarial\nNetworks (GANs) to augment and balance datasets. The survey synthesizes the\nstrengths and limitations of current approaches, emphasizing the trend towards\nmultimodal integration and the persistent need for model interpretability and\ngeneralizability. Finally, we identify critical open challenges, including\nrobust external validation, clinical integration, and ethical considerations,\nand outline promising future research directions such as hybrid models, causal\ninference, and federated learning. This review aims to consolidate current\nknowledge and guide future efforts in developing clinically relevant AI tools\nfor personalized AD prognostication.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u4e2a\u6027\u5316\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u8fdb\u5c55\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u591a\u79cdAI\u65b9\u6cd5\u3001\u6570\u636e\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "AD\u8fdb\u5c55\u7684\u4e2a\u4f53\u5dee\u5f02\u5927\uff0c\u9700\u8981\u4e2a\u6027\u5316\u9884\u6d4b\u6a21\u578b\u4ee5\u6539\u5584\u9884\u540e\u548c\u62a4\u7406\u8ba1\u5212\u3002", "method": "\u7efc\u8ff0\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001\u6df1\u5ea6\u5b66\u4e60\uff08\u5982RNN\uff09\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548c\u6570\u5b57\u5b6a\u751f\u7b49\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u636e\u6311\u6218\u53ca\u5e94\u5bf9\u7b56\u7565\uff08\u5982VAE\u548cGAN\u751f\u6210\u5408\u6210\u6570\u636e\uff09\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5f3a\u8c03\u591a\u6a21\u6001\u6574\u5408\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff08\u5982\u6df7\u5408\u6a21\u578b\u3001\u56e0\u679c\u63a8\u65ad\u548c\u8054\u90a6\u5b66\u4e60\uff09\uff0c\u5e76\u6307\u51fa\u5916\u90e8\u9a8c\u8bc1\u3001\u4e34\u5e8a\u6574\u5408\u548c\u4f26\u7406\u95ee\u9898\u7b49\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2504.21814", "pdf": "https://arxiv.org/pdf/2504.21814", "abs": "https://arxiv.org/abs/2504.21814", "authors": ["Yixin Gao", "Xiaohan Pan", "Xin Li", "Zhibo Chen"], "title": "Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of AIGC foundation models has revolutionized the\nparadigm of image compression, which paves the way for the abandonment of most\npixel-level transform and coding, compelling us to ask: why compress what you\ncan generate if the AIGC foundation model is powerful enough to faithfully\ngenerate intricate structure and fine-grained details from nothing more than\nsome compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o\nimage generation of OpenAI has achieved impressive cross-modality generation,\nediting, and design capabilities, which motivates us to answer the above\nquestion by exploring its potential in image compression fields. In this work,\nwe investigate two typical compression paradigms: textual coding and multimodal\ncoding (i.e., text + extremely low-resolution image), where all/most\npixel-level information is generated instead of compressing via the advanced\nGPT-4o image generation function. The essential challenge lies in how to\nmaintain semantic and structure consistency during the decoding process. To\novercome this, we propose a structure raster-scan prompt engineering mechanism\nto transform the image into textual space, which is compressed as the condition\nof GPT-4o image generation. Extensive experiments have shown that the\ncombination of our designed structural raster-scan prompts and GPT-4o's image\ngeneration function achieved the impressive performance compared with recent\nmultimodal/generative image compression at ultra-low bitrate, further\nindicating the potential of AIGC generation in image compression fields.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528AIGC\u57fa\u7840\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u56fe\u50cf\u538b\u7f29\u9886\u57df\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u6587\u672c\u7f16\u7801\u548c\u591a\u6a21\u6001\u7f16\u7801\u4e24\u79cd\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5149\u6805\u626b\u63cf\u63d0\u793a\u5de5\u7a0b\u673a\u5236\u63d0\u5347\u89e3\u7801\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "AIGC\u57fa\u7840\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5373\u901a\u8fc7\u751f\u6210\u800c\u975e\u4f20\u7edf\u538b\u7f29\u65b9\u5f0f\u5b9e\u73b0\u9ad8\u6548\u56fe\u50cf\u8868\u793a\u3002GPT-4o\u7684\u5f3a\u5927\u8de8\u6a21\u6001\u751f\u6210\u80fd\u529b\u6fc0\u53d1\u4e86\u5176\u5728\u56fe\u50cf\u538b\u7f29\u4e2d\u7684\u5e94\u7528\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u538b\u7f29\u8303\u5f0f\uff1a\u6587\u672c\u7f16\u7801\uff08\u4ec5\u7528\u6587\u672c\u63cf\u8ff0\uff09\u548c\u591a\u6a21\u6001\u7f16\u7801\uff08\u6587\u672c+\u6781\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff09\u3002\u63d0\u51fa\u7ed3\u6784\u5149\u6805\u626b\u63cf\u63d0\u793a\u5de5\u7a0b\u673a\u5236\uff0c\u5c06\u56fe\u50cf\u8f6c\u5316\u4e3a\u6587\u672c\u7a7a\u95f4\u4f5c\u4e3aGPT-4o\u751f\u6210\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u7ed3\u6784\u5149\u6805\u626b\u63cf\u63d0\u793a\u548cGPT-4o\u751f\u6210\u529f\u80fd\uff0c\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u4e0b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001/\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "AIGC\u751f\u6210\u5728\u56fe\u50cf\u538b\u7f29\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u751f\u6210\u4e00\u81f4\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.21190", "pdf": "https://arxiv.org/pdf/2504.21190", "abs": "https://arxiv.org/abs/2504.21190", "authors": ["Pradip Kunwar", "Minh N. Vu", "Maanak Gupta", "Mahmoud Abdelsalam", "Manish Bhattarai"], "title": "TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA\nMoE), a novel computational framework integrating Parameter-Efficient\nFine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in\nlarge model deployments. Unlike traditional MoE approaches, which face\nsubstantial computational overhead as expert counts grow, TT-LoRA MoE\ndecomposes training into two distinct, optimized stages. First, we\nindependently train lightweight, tensorized low-rank adapters (TT-LoRA\nexperts), each specialized for specific tasks. Subsequently, these expert\nadapters remain frozen, eliminating inter-task interference and catastrophic\nforgetting in multi-task setting. A sparse MoE router, trained separately,\ndynamically leverages base model representations to select exactly one\nspecialized adapter per input at inference time, automating expert selection\nwithout explicit task specification. Comprehensive experiments confirm our\narchitecture retains the memory efficiency of low-rank adapters, seamlessly\nscales to large expert pools, and achieves robust task-level optimization. This\nstructured decoupling significantly enhances computational efficiency and\nflexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion\nparameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling\npractical and scalable multi-task inference deployments.", "AI": {"tldr": "TT-LoRA MoE\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e0e\u7a00\u758fMoE\u8def\u7531\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u548c\u52a8\u6001\u8def\u7531\u63d0\u5347\u5927\u6a21\u578b\u90e8\u7f72\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfMoE\u65b9\u6cd5\u5728\u4e13\u5bb6\u6570\u91cf\u589e\u52a0\u65f6\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u7684\u5e72\u6270\u548c\u9057\u5fd8\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a1) \u72ec\u7acb\u8bad\u7ec3\u8f7b\u91cf\u7ea7TT-LoRA\u4e13\u5bb6\u9002\u914d\u5668\uff1b2) \u51bb\u7ed3\u9002\u914d\u5668\uff0c\u8bad\u7ec3\u7a00\u758fMoE\u8def\u7531\u5668\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u3002", "result": "\u4ec5\u9700\u5c11\u91cf\u53c2\u6570\uff08\u59822%\u7684LoRA\uff09\uff0c\u5728\u591a\u4efb\u52a1\u4e2d\u4f18\u4e8eAdapterFusion 4\u4e2a\u70b9\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55\u90e8\u7f72\u3002", "conclusion": "TT-LoRA MoE\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u63a8\u7406\u3002"}}
{"id": "2504.21831", "pdf": "https://arxiv.org/pdf/2504.21831", "abs": "https://arxiv.org/abs/2504.21831", "authors": ["Anas Anwarul Haq Khan", "Utkarsh Verma", "Prateek Chanda", "Ganesh Ramakrishnan"], "title": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research.", "AI": {"tldr": "DEEVISum\u662f\u4e00\u4e2a\u8f7b\u91cf\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u9891\u6458\u8981\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u63d0\u793a\u548c\u591a\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\uff08MSKD\uff09\u4e0e\u65e9\u671f\u9000\u51fa\uff08EE\uff09\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u9891\u6458\u8981\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u63d0\u793a\uff08\u6587\u672c\u548c\u97f3\u9891\u4fe1\u53f7\uff09\uff0c\u7ed3\u5408MSKD\u548cEE\u6280\u672f\uff0c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728TVSum\u6570\u636e\u96c6\u4e0a\uff0cPaLI Gemma2 3B + MSKD\u6a21\u578b\u8fbe\u523061.1\u7684F1\u5206\u6570\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1121%\uff0c\u6027\u80fd\u63a5\u8fd1\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "DEEVISum\u5728\u89c6\u9891\u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.21191", "pdf": "https://arxiv.org/pdf/2504.21191", "abs": "https://arxiv.org/abs/2504.21191", "authors": ["Lovedeep Gondara", "Jonathan Simkin", "Graham Sayle", "Shebnum Devji", "Gregory Arbour", "Raymond Ng"], "title": "Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study aims to guide language model selection by investigating: 1) the\nnecessity of finetuning versus zero-shot usage, 2) the benefits of\ndomain-adjacent versus generic pretrained models, 3) the value of further\ndomain-specific pretraining, and 4) the continued relevance of Small Language\nModels (SLMs) compared to Large Language Models (LLMs) for specific tasks.\nUsing electronic pathology reports from the British Columbia Cancer Registry\n(BCCR), three classification scenarios with varying difficulty and data size\nare evaluated. Models include various SLMs and an LLM. SLMs are evaluated both\nzero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning\nsignificantly improved SLM performance across all scenarios compared to their\nzero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was\nconsistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally\nperformed better than the generic SLM after finetuning, especially on harder\ntasks. Further domain-specific pretraining yielded modest gains on easier tasks\nbut significant improvements on the complex, data-scarce task. The results\nhighlight the critical role of finetuning for SLMs in specialized domains,\nenabling them to surpass zero-shot LLM performance on targeted classification\ntasks. Pretraining on domain-adjacent or domain-specific data provides further\nadvantages, particularly for complex problems or limited finetuning data. While\nLLMs offer strong zero-shot capabilities, their performance on these specific\ntasks did not match that of appropriately finetuned SLMs. In the era of LLMs,\nSLMs remain relevant and effective, offering a potentially superior\nperformance-resource trade-off compared to LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u9009\u62e9\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5305\u62ec\u5fae\u8c03\u4e0e\u96f6\u6837\u672c\u4f7f\u7528\u7684\u5fc5\u8981\u6027\u3001\u9886\u57df\u76f8\u5173\u4e0e\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f18\u52bf\u3001\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u7684\u4ef7\u503c\uff0c\u4ee5\u53ca\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u76f8\u5bf9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6301\u7eed\u76f8\u5173\u6027\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u53d1\u73b0\u5fae\u8c03\u663e\u8457\u63d0\u5347SLMs\u6027\u80fd\uff0c\u4f7f\u5176\u8d85\u8d8a\u96f6\u6837\u672cLLMs\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u9009\u62e9\uff0c\u7279\u522b\u662f\u5728\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\uff0c\u6bd4\u8f83\u5fae\u8c03\u4e0e\u96f6\u6837\u672c\u4f7f\u7528\u3001\u9886\u57df\u76f8\u5173\u4e0e\u901a\u7528\u6a21\u578b\u3001\u4ee5\u53caSLMs\u4e0eLLMs\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u7535\u5b50\u75c5\u7406\u62a5\u544a\u6570\u636e\uff0c\u8bc4\u4f30\u4e09\u79cd\u4e0d\u540c\u96be\u5ea6\u548c\u6570\u636e\u91cf\u7684\u5206\u7c7b\u573a\u666f\u3002\u6a21\u578b\u5305\u62ec\u591a\u79cdSLMs\u548c\u4e00\u4e2aLLM\uff0cSLMs\u5206\u522b\u6d4b\u8bd5\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6027\u80fd\uff0cLLM\u4ec5\u6d4b\u8bd5\u96f6\u6837\u672c\u6027\u80fd\u3002", "result": "\u5fae\u8c03\u663e\u8457\u63d0\u5347SLMs\u6027\u80fd\uff0c\u4f7f\u5176\u8d85\u8d8a\u96f6\u6837\u672cLLMs\uff1b\u9886\u57df\u76f8\u5173SLMs\u5728\u5fae\u8c03\u540e\u8868\u73b0\u4f18\u4e8e\u901a\u7528SLM\uff1b\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u5bf9\u590d\u6742\u4efb\u52a1\u6709\u660e\u663e\u5e2e\u52a9\u3002", "conclusion": "\u5728\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03SLMs\u6027\u80fd\u4f18\u4e8e\u96f6\u6837\u672cLLMs\uff0c\u4e14SLMs\u5728\u8d44\u6e90\u4e0e\u6027\u80fd\u6743\u8861\u4e0a\u66f4\u5177\u4f18\u52bf\uff0c\u8bc1\u660e\u5176\u5728LLM\u65f6\u4ee3\u4ecd\u5177\u4ef7\u503c\u3002"}}
{"id": "2504.21836", "pdf": "https://arxiv.org/pdf/2504.21836", "abs": "https://arxiv.org/abs/2504.21836", "authors": ["Ipek Oztas", "Duygu Ceylan", "Aysegul Dundar"], "title": "3D Stylization via Large Reconstruction Model", "categories": ["cs.CV"], "comment": "Accepted to SIGGRAPH 2025", "summary": "With the growing success of text or image guided 3D generators, users demand\nmore control over the generation process, appearance stylization being one of\nthem. Given a reference image, this requires adapting the appearance of a\ngenerated 3D asset to reflect the visual style of the reference while\nmaintaining visual consistency from multiple viewpoints. To tackle this\nproblem, we draw inspiration from the success of 2D stylization methods that\nleverage the attention mechanisms in large image generation models to capture\nand transfer visual style. In particular, we probe if large reconstruction\nmodels, commonly used in the context of 3D generation, has a similar\ncapability. We discover that the certain attention blocks in these models\ncapture the appearance specific features. By injecting features from a visual\nstyle image to such blocks, we develop a simple yet effective 3D appearance\nstylization method. Our method does not require training or test time\noptimization. Through both quantitative and qualitative evaluations, we\ndemonstrate that our approach achieves superior results in terms of 3D\nappearance stylization, significantly improving efficiency while maintaining\nhigh-quality visual outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6216\u4f18\u5316\u76843D\u5916\u89c2\u98ce\u683c\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u5165\u53c2\u8003\u56fe\u50cf\u7684\u7279\u5f81\u5230\u5927\u578b\u91cd\u5efa\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5757\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "\u968f\u7740\u6587\u672c\u6216\u56fe\u50cf\u5f15\u5bfc\u76843D\u751f\u6210\u5668\u7684\u6210\u529f\uff0c\u7528\u6237\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u63a7\u5236\u9700\u6c42\u589e\u52a0\uff0c\u7279\u522b\u662f\u5916\u89c2\u98ce\u683c\u5316\u3002", "method": "\u5229\u7528\u5927\u578b\u91cd\u5efa\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u5757\u6355\u83b7\u5916\u89c2\u7279\u5f81\uff0c\u901a\u8fc7\u6ce8\u5165\u53c2\u8003\u56fe\u50cf\u7684\u7279\u5f81\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\u3002", "result": "\u65b9\u6cd5\u57283D\u5916\u89c2\u98ce\u683c\u5316\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u5e76\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u4f18\u5316\uff0c\u4e3a3D\u5916\u89c2\u98ce\u683c\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21846", "pdf": "https://arxiv.org/pdf/2504.21846", "abs": "https://arxiv.org/abs/2504.21846", "authors": ["Hadleigh Schwartz", "Xiaofeng Yan", "Charles J. Carver", "Xia Zhou"], "title": "Active Light Modulation to Counter Manipulation of Speech Visual Content", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "High-profile speech videos are prime targets for falsification, owing to\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\nand unobtrusive system for protecting live speech videos from visual\nfalsification of speaker identity and lip and facial motion. Unlike predominant\nfalsification detection methods operating in the digital domain, Spotlight\ncreates dynamic physical signatures at the event site and embeds them into all\nvideo recordings via imperceptible modulated light. These physical signatures\nencode semantically-meaningful features unique to the speech event, including\nthe speaker's identity and facial motion, and are cryptographically-secured to\nprevent spoofing. The signatures can be extracted from any video downstream and\nvalidated against the portrayed speech content to check its integrity. Key\nelements of Spotlight include (1) a framework for generating extremely compact\n(i.e., 150-bit), pose-invariant speech video features, based on\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\n>200 bps into video while remaining imperceptible both in video and live.\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\nvideos. Further, Spotlight is highly robust across recording conditions, video\npost-processing techniques, and white-box adversarial attacks on its video\nfeature extraction methodologies.", "AI": {"tldr": "Spotlight\u662f\u4e00\u79cd\u4f4e\u5f00\u9500\u3001\u975e\u4fb5\u5165\u6027\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u7269\u7406\u7b7e\u540d\u4fdd\u62a4\u5b9e\u65f6\u6f14\u8bb2\u89c6\u9891\u514d\u53d7\u8eab\u4efd\u548c\u9762\u90e8\u52a8\u4f5c\u7684\u89c6\u89c9\u4f2a\u9020\u3002", "motivation": "\u9ad8\u77e5\u540d\u5ea6\u6f14\u8bb2\u89c6\u9891\u6613\u53d7\u4f2a\u9020\uff0c\u56e0\u5176\u53ef\u8bbf\u95ee\u6027\u548c\u5f71\u54cd\u529b\u3002", "method": "\u5229\u7528\u4e0d\u53ef\u5bdf\u89c9\u7684\u8c03\u5236\u5149\u5d4c\u5165\u52a8\u6001\u7269\u7406\u7b7e\u540d\uff0c\u751f\u6210\u7d27\u51d1\u7684\u3001\u59ff\u6001\u4e0d\u53d8\u7684\u89c6\u9891\u7279\u5f81\u3002", "result": "\u5728\u68c0\u6d4b\u4f2a\u9020\u89c6\u9891\u65f6\uff0cAUC\u22650.99\uff0c\u771f\u9633\u6027\u7387100%\uff0c\u4e14\u5bf9\u5f55\u5236\u6761\u4ef6\u548c\u540e\u5904\u7406\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "Spotlight\u4e3a\u5b9e\u65f6\u6f14\u8bb2\u89c6\u9891\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u9632\u4f2a\u9020\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21195", "pdf": "https://arxiv.org/pdf/2504.21195", "abs": "https://arxiv.org/abs/2504.21195", "authors": ["Kelsey E. Ennis", "Elizabeth A. Barnes", "Marybeth C. Arcodia", "Martin A. Fernandez", "Eric D. Maloney"], "title": "Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Extreme heat is the deadliest weather-related hazard in the United States.\nFurthermore, it is increasing in intensity, frequency, and duration, making\nskillful forecasts vital to protecting life and property. Traditional numerical\nweather prediction (NWP) models struggle with extreme heat for medium-range and\nsubseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial\nintelligence-based weather prediction (AIWP) models are progressing rapidly.\nHowever, it is largely unknown how well AIWP models forecast extremes,\nespecially for medium-range and S2S timescales. This study investigates 2-m\ntemperature forecasts for 60 heat waves across the four boreal seasons and over\nfour CONUS regions at lead times up to 20 days, using two AIWP models (Google\nGraphCast and Pangu-Weather) and one traditional NWP model (NOAA United\nForecast System Global Ensemble Forecast System (UFS GEFS)). First, case study\nanalyses show that both AIWP models and the UFS GEFS exhibit consistent cold\nbiases on regional scales in the 5-10 days of lead time before heat wave onset.\nGraphCast is the more skillful AIWP model, outperforming UFS GEFS and\nPangu-Weather in most locations. Next, the two AIWP models are isolated and\nanalyzed across all heat waves and seasons, with events split among the model's\ntesting (2018-2023) and training (1979-2017) periods. There are cold biases\nbefore and during the heat waves in both models and all seasons, except\nPangu-Weather in winter, which exhibits a mean warm bias before heat wave\nonset. Overall, results offer encouragement that AIWP models may be useful for\nmedium-range and S2S predictability of extreme heat.", "AI": {"tldr": "AIWP\u6a21\u578b\uff08\u5982GraphCast\u548cPangu-Weather\uff09\u5728\u6781\u7aef\u9ad8\u6e29\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfNWP\u6a21\u578b\uff08UFS GEFS\uff09\uff0c\u4f46\u4ecd\u5b58\u5728\u51b7\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u6781\u7aef\u9ad8\u6e29\u662f\u81f4\u547d\u7684\u5929\u6c14\u707e\u5bb3\uff0c\u4f20\u7edfNWP\u6a21\u578b\u5728\u4e2d\u957f\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800cAIWP\u6a21\u578b\u7684\u6f5c\u529b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cdAIWP\u6a21\u578b\u548c\u4e00\u79cdNWP\u6a21\u578b\u5bf960\u6b21\u70ed\u6d6a\u4e8b\u4ef6\u76842\u7c73\u6e29\u5ea6\u9884\u6d4b\u80fd\u529b\uff0c\u65f6\u95f4\u8de8\u5ea6\u4e3a20\u5929\u3002", "result": "GraphCast\u8868\u73b0\u6700\u4f73\uff0c\u4f46AIWP\u6a21\u578b\u666e\u904d\u5b58\u5728\u51b7\u504f\u5dee\uff0cPangu-Weather\u5728\u51ac\u5b63\u4f8b\u5916\u3002", "conclusion": "AIWP\u6a21\u578b\u5728\u4e2d\u957f\u671f\u6781\u7aef\u9ad8\u6e29\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2504.21847", "pdf": "https://arxiv.org/pdf/2504.21847", "abs": "https://arxiv.org/abs/2504.21847", "authors": ["Derong Jin", "Ruohan Gao"], "title": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors", "categories": ["cs.CV", "cs.SD"], "comment": "Project Page: https://humathe.github.io/avdar/", "summary": "An immersive acoustic experience enabled by spatial audio is just as crucial\nas the visual aspect in creating realistic virtual environments. However,\nexisting methods for room impulse response estimation rely either on\ndata-demanding learning-based models or computationally expensive physics-based\nmodeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic\nRendering (AV-DAR), a framework that leverages visual cues extracted from\nmulti-view images and acoustic beam tracing for physics-based room acoustic\nrendering. Experiments across six real-world environments from two datasets\ndemonstrate that our multimodal, physics-based approach is efficient,\ninterpretable, and accurate, significantly outperforming a series of prior\nmethods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves\ncomparable performance to models trained on 10 times more data while delivering\nrelative gains ranging from 16.6% to 50.9% when trained at the same scale.", "AI": {"tldr": "AV-DAR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7ebf\u7d22\u548c\u58f0\u5b66\u675f\u8ffd\u8e2a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u51c6\u786e\u7684\u623f\u95f4\u58f0\u5b66\u6e32\u67d3\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u623f\u95f4\u8109\u51b2\u54cd\u5e94\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5bc6\u96c6\u578b\u5b66\u4e60\u6a21\u578b\u6216\u8ba1\u7b97\u6602\u8d35\u7684\u7269\u7406\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "AV-DAR\u5229\u7528\u591a\u89c6\u89d2\u56fe\u50cf\u63d0\u53d6\u7684\u89c6\u89c9\u7ebf\u7d22\u548c\u58f0\u5b66\u675f\u8ffd\u8e2a\uff0c\u8fdb\u884c\u57fa\u4e8e\u7269\u7406\u7684\u623f\u95f4\u58f0\u5b66\u6e32\u67d3\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u73af\u5883\u4e2d\uff0cAV-DAR\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63a5\u8fd1\u4f7f\u752810\u500d\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u76f8\u5bf9\u589e\u76ca\u8fbe16.6%\u81f350.9%\u3002", "conclusion": "AV-DAR\u4e3a\u591a\u6a21\u6001\u3001\u7269\u7406\u57fa\u7840\u7684\u58f0\u5b66\u6e32\u67d3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21202", "pdf": "https://arxiv.org/pdf/2504.21202", "abs": "https://arxiv.org/abs/2504.21202", "authors": ["Ramon Pires", "Roseval Malaquias Junior", "Rodrigo Nogueira"], "title": "Automatic Legal Writing Evaluation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the recent advances in Large Language Models, benchmarks for\nevaluating legal writing remain scarce due to the inherent complexity of\nassessing open-ended responses in this domain. One of the key challenges in\nevaluating language models on domain-specific tasks is finding test datasets\nthat are public, frequently updated, and contain comprehensive evaluation\nguidelines. The Brazilian Bar Examination meets these requirements. We\nintroduce oab-bench, a benchmark comprising 105 questions across seven areas of\nlaw from recent editions of the exam. The benchmark includes comprehensive\nevaluation guidelines and reference materials used by human examiners to ensure\nconsistent grading. We evaluate the performance of four LLMs on oab-bench,\nfinding that Claude-3.5 Sonnet achieves the best results with an average score\nof 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can\nserve as reliable automated judges for evaluating legal writing. Our\nexperiments show that frontier models like OpenAI's o1 achieve a strong\ncorrelation with human scores when evaluating approved exams, suggesting their\npotential as reliable automated evaluators despite the inherently subjective\nnature of legal writing assessment. The source code and the benchmark --\ncontaining questions, evaluation guidelines, model-generated responses, and\ntheir respective automated evaluations -- are publicly available.", "AI": {"tldr": "oab-bench\u662f\u4e00\u4e2a\u57fa\u4e8e\u5df4\u897f\u5f8b\u5e08\u8003\u8bd5\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u5199\u4f5c\u9886\u57df\u7684\u8868\u73b0\u3002Claude-3.5 Sonnet\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u5f97\u52067.93/10\u3002\u524d\u6cbf\u6a21\u578b\u5982OpenAI\u7684o1\u5728\u8bc4\u5206\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u59d4\u6709\u8f83\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u7531\u4e8e\u6cd5\u5f8b\u5199\u4f5c\u8bc4\u4f30\u7684\u590d\u6742\u6027\uff0c\u7f3a\u4e4f\u516c\u5f00\u4e14\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5df4\u897f\u5f8b\u5e08\u8003\u8bd5\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u6784\u5efaoab-bench\u57fa\u51c6\uff0c\u5305\u542b105\u4e2a\u95ee\u9898\u548c\u8bc4\u5206\u6307\u5357\uff0c\u8bc4\u4f304\u4e2aLLM\u7684\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u5206\u5668\u7684\u53ef\u9760\u6027\u3002", "result": "Claude-3.5 Sonnet\u8868\u73b0\u6700\u4f73\uff0c\u524d\u6cbf\u6a21\u578b\u5728\u8bc4\u5206\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u59d4\u76f8\u5173\u6027\u9ad8\u3002", "conclusion": "oab-bench\u4e3a\u6cd5\u5f8b\u5199\u4f5c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\uff0cLLM\u5728\u81ea\u52a8\u8bc4\u5206\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2504.21850", "pdf": "https://arxiv.org/pdf/2504.21850", "abs": "https://arxiv.org/abs/2504.21850", "authors": ["Xindi Wu", "Hee Seung Hwang", "Polina Kirichenko", "Olga Russakovsky"], "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning", "categories": ["cs.CV"], "comment": "17 pages, 13 figures", "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks.", "AI": {"tldr": "COMPACT\u662f\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u8bad\u7ec3\u6837\u672c\u7684\u7ec4\u5408\u590d\u6742\u6027\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u591a\u80fd\u529b\u7ec4\u5408\u7684\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6570\u636e\u91cf\u800c\u975e\u7ec4\u5408\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faCOMPACT\u65b9\u6cd5\uff0c\u751f\u6210\u4e00\u4e2a\u663e\u5f0f\u63a7\u5236\u7ec4\u5408\u590d\u6742\u6027\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4f7f\u6a21\u578b\u80fd\u66f4\u9ad8\u6548\u5730\u5b66\u4e60\u590d\u6742\u80fd\u529b\u3002", "result": "COMPACT\u5728\u6570\u636e\u91cf\u4ec5\u4e3aLLaVA-665k\u768410%\u65f6\uff0c\u6027\u80fd\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u9700\u8981\u591a\u80fd\u529b\u7ec4\u5408\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff08\u5982MMStar\u63d0\u534783.3%\uff0cMM-Vet\u63d0\u534794.0%\uff09\u3002", "conclusion": "COMPACT\u4e3a\u590d\u6742\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6848\u3002"}}
{"id": "2504.21205", "pdf": "https://arxiv.org/pdf/2504.21205", "abs": "https://arxiv.org/abs/2504.21205", "authors": ["Connor Dilgren", "Purva Chiniya", "Luke Griffith", "Yu Ding", "Yizheng Chen"], "title": "SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure\ncode generation in real-world repositories. SecRepoBench has 318 code\ngeneration tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19\nstate-of-the-art LLMs using our benchmark and find that the models struggle\nwith generating correct and secure code. In addition, the performance of LLMs\nto generate self-contained programs as measured by prior benchmarks do not\ntranslate to comparative performance at generating secure and correct code at\nthe repository level in SecRepoBench. We show that the state-of-the-art prompt\nengineering techniques become less effective when applied to the repository\nlevel secure code generation problem. We conduct extensive experiments,\nincluding an agentic technique to generate secure code, to demonstrate that our\nbenchmark is currently the most difficult secure coding benchmark, compared to\nprevious state-of-the-art benchmarks. Finally, our comprehensive analysis\nprovides insights into potential directions for enhancing the ability of LLMs\nto generate correct and secure code in real-world repositories.", "AI": {"tldr": "SecRepoBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u751f\u6210\u5b89\u5168\u4ee3\u7801\u7684\u57fa\u51c6\uff0c\u5305\u542b318\u4e2a\u4efb\u52a1\uff0c\u8986\u76d615\u79cdCWE\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709LLMs\u5728\u751f\u6210\u5b89\u5168\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u6548\u679c\u6709\u9650\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u751f\u6210\u5b89\u5168\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "method": "\u6784\u5efaSecRepoBench\u57fa\u51c6\uff0c\u5305\u542b318\u4e2a\u4efb\u52a1\u548c27\u4e2aC/C++\u4ee3\u7801\u5e93\uff0c\u8bc4\u4f3019\u79cdLLMs\uff0c\u5e76\u5c1d\u8bd5\u4ee3\u7406\u6280\u672f\u751f\u6210\u5b89\u5168\u4ee3\u7801\u3002", "result": "LLMs\u5728\u751f\u6210\u5b89\u5168\u4ee3\u7801\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u6548\u679c\u6709\u9650\uff0cSecRepoBench\u662f\u76ee\u524d\u6700\u5177\u6311\u6218\u6027\u7684\u5b89\u5168\u4ee3\u7801\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347LLMs\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u751f\u6210\u5b89\u5168\u4ee3\u7801\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.21853", "pdf": "https://arxiv.org/pdf/2504.21853", "abs": "https://arxiv.org/abs/2504.21853", "authors": ["Jiwen Yu", "Yiran Qin", "Haoxuan Che", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Hao Chen", "Xihui Liu"], "title": "A Survey of Interactive Generative Video", "categories": ["cs.CV"], "comment": null, "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4ea4\u4e92\u5f0f\u751f\u6210\u89c6\u9891\uff08IGV\uff09\u6280\u672f\uff0c\u5b9a\u4e49\u4e86\u5176\u6838\u5fc3\u529f\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u5728\u6e38\u620f\u3001\u5177\u8eabAI\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u4e2a\u6a21\u5757\u7684\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u3001\u4ea4\u4e92\u5f0f\u89c6\u9891\u5185\u5bb9\u9700\u6c42\u7684\u589e\u957f\uff0cIGV\u6280\u672f\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5316IGV\u7684\u5b9a\u4e49\u3001\u5e94\u7528\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u8c03\u67e5IGV\u5728\u6e38\u620f\u3001\u5177\u8eabAI\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u751f\u6210\u3001\u63a7\u5236\u3001\u8bb0\u5fc6\u3001\u52a8\u6001\u548c\u667a\u80fd\u4e94\u4e2a\u6a21\u5757\u7684\u6846\u67b6\uff0c\u5e76\u5206\u6790\u4e86\u6280\u672f\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86IGV\u7684\u5168\u9762\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u4e86\u5b9e\u73b0\u7406\u60f3IGV\u7cfb\u7edf\u7684\u5173\u952e\u6280\u672f\u6311\u6218\uff0c\u5982\u5b9e\u65f6\u751f\u6210\u3001\u5f00\u653e\u57df\u63a7\u5236\u548c\u7269\u7406\u6a21\u62df\u7b49\u3002", "conclusion": "\u672c\u6587\u7684\u7cfb\u7edf\u5206\u6790\u5c06\u63a8\u52a8IGV\u6280\u672f\u7684\u672a\u6765\u53d1\u5c55\uff0c\u4f7f\u5176\u5728\u66f4\u590d\u6742\u548c\u5b9e\u7528\u7684\u5e94\u7528\u4e2d\u53d6\u5f97\u8fdb\u5c55\u3002"}}
{"id": "2504.21206", "pdf": "https://arxiv.org/pdf/2504.21206", "abs": "https://arxiv.org/abs/2504.21206", "authors": ["Zihan Chen", "Xingbo Fu", "Yushun Dong", "Jundong Li", "Cong Shen"], "title": "FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Federated Graph Learning (FGL) empowers clients to collaboratively train\nGraph neural networks (GNNs) in a distributed manner while preserving data\nprivacy. However, FGL methods usually require that the graph data owned by all\nclients is homophilic to ensure similar neighbor distribution patterns of\nnodes. Such an assumption ensures that the learned knowledge is consistent\nacross the local models from all clients. Therefore, these local models can be\nproperly aggregated as a global model without undermining the overall\nperformance. Nevertheless, when the neighbor distribution patterns of nodes\nvary across different clients (e.g., when clients hold graphs with different\nlevels of heterophily), their local models may gain different and even conflict\nknowledge from their node-level predictive tasks. Consequently, aggregating\nthese local models usually leads to catastrophic performance deterioration on\nthe global model. To address this challenge, we propose FedHERO, an FGL\nframework designed to harness and share insights from heterophilic graphs\neffectively. At the heart of FedHERO is a dual-channel GNN equipped with a\nstructure learner, engineered to discern the structural knowledge encoded in\nthe local graphs. With this specialized component, FedHERO enables the local\nmodel for each client to identify and learn patterns that are universally\napplicable across graphs with different patterns of node neighbor\ndistributions. FedHERO not only enhances the performance of individual client\nmodels by leveraging both local and shared structural insights but also sets a\nnew precedent in this field to effectively handle graph data with various node\nneighbor distribution patterns. We conduct extensive experiments to validate\nthe superior performance of FedHERO against existing alternatives.", "AI": {"tldr": "FedHERO\u662f\u4e00\u4e2a\u8054\u90a6\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u6709\u6548\u5904\u7406\u5f02\u8d28\u6027\u56fe\u6570\u636e\uff0c\u901a\u8fc7\u53cc\u901a\u9053GNN\u548c\u7ed3\u6784\u5b66\u4e60\u5668\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u56fe\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u5ba2\u6237\u7aef\u56fe\u6570\u636e\u540c\u8d28\u6027\uff0c\u4f46\u5728\u5f02\u8d28\u6027\u56fe\u6570\u636e\u4e0b\u6027\u80fd\u4e0b\u964d\uff0cFedHERO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53cc\u901a\u9053GNN\u548c\u7ed3\u6784\u5b66\u4e60\u5668\uff0c\u8bc6\u522b\u5c40\u90e8\u56fe\u4e2d\u7684\u7ed3\u6784\u77e5\u8bc6\uff0c\u5b66\u4e60\u9002\u7528\u4e8e\u4e0d\u540c\u8282\u70b9\u90bb\u5c45\u5206\u5e03\u6a21\u5f0f\u7684\u901a\u7528\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1FedHERO\u5728\u5f02\u8d28\u6027\u56fe\u6570\u636e\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedHERO\u4e3a\u5904\u7406\u5f02\u8d28\u6027\u56fe\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5ba2\u6237\u7aef\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.21855", "pdf": "https://arxiv.org/pdf/2504.21855", "abs": "https://arxiv.org/abs/2504.21855", "authors": ["Qihao Liu", "Ju He", "Qihang Yu", "Liang-Chieh Chen", "Alan Yuille"], "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction", "categories": ["cs.CV"], "comment": "Project Page: https://revision-video.github.io/", "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.", "AI": {"tldr": "ReVision\u662f\u4e00\u4e2a\u5c06\u53c2\u6570\u53163D\u7269\u7406\u77e5\u8bc6\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u590d\u6742\u8fd0\u52a8\u89c6\u9891\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u8fd0\u52a8\u548c\u4ea4\u4e92\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u751f\u6210\u7c97\u7565\u89c6\u9891\uff1b2\uff09\u63d0\u53d62D/3D\u7279\u5f81\u5e76\u4f18\u5316\uff1b3\uff09\u53cd\u9988\u4f18\u5316\u540e\u7684\u8fd0\u52a8\u5e8f\u5217\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "result": "\u5728Stable Video Diffusion\u4e0a\u9a8c\u8bc1\uff0cReVision\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u96c6\u62103D\u7269\u7406\u77e5\u8bc6\uff0c\u5c0f\u6a21\u578b\u4e5f\u80fd\u751f\u6210\u66f4\u771f\u5b9e\u53ef\u63a7\u7684\u590d\u6742\u8fd0\u52a8\u89c6\u9891\u3002"}}
{"id": "2504.21211", "pdf": "https://arxiv.org/pdf/2504.21211", "abs": "https://arxiv.org/abs/2504.21211", "authors": ["Juliana Barbosa", "Ulhas Gondhali", "Gohar Petrossian", "Kinshuk Sharma", "Sunandan Chakraborty", "Jennifer Jacquet", "Juliana Freire"], "title": "A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Wildlife trafficking remains a critical global issue, significantly impacting\nbiodiversity, ecological stability, and public health. Despite efforts to\ncombat this illicit trade, the rise of e-commerce platforms has made it easier\nto sell wildlife products, putting new pressure on wild populations of\nendangered and threatened species. The use of these platforms also opens a new\nopportunity: as criminals sell wildlife products online, they leave digital\ntraces of their activity that can provide insights into trafficking activities\nas well as how they can be disrupted. The challenge lies in finding these\ntraces. Online marketplaces publish ads for a plethora of products, and\nidentifying ads for wildlife-related products is like finding a needle in a\nhaystack. Learning classifiers can automate ad identification, but creating\nthem requires costly, time-consuming data labeling that hinders support for\ndiverse ads and research questions. This paper addresses a critical challenge\nin the data science pipeline for wildlife trafficking analytics: generating\nquality labeled data for classifiers that select relevant data. While large\nlanguage models (LLMs) can directly label advertisements, doing so at scale is\nprohibitively expensive. We propose a cost-effective strategy that leverages\nLLMs to generate pseudo labels for a small sample of the data and uses these\nlabels to create specialized classification models. Our novel method\nautomatically gathers diverse and representative samples to be labeled while\nminimizing the labeling costs. Our experimental evaluation shows that our\nclassifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We\npresent real use cases that demonstrate the effectiveness of our approach in\nenabling analyses of different aspects of wildlife trafficking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4f2a\u6807\u7b7e\u7684\u4f4e\u6210\u672c\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u6548\u7684\u91ce\u751f\u52a8\u7269\u8d29\u5356\u5e7f\u544a\u5206\u7c7b\u5668\uff0c\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u5e76\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u91ce\u751f\u52a8\u7269\u8d29\u5356\u5bf9\u751f\u6001\u548c\u516c\u5171\u5065\u5eb7\u9020\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u800c\u7535\u5546\u5e73\u53f0\u4e3a\u8d29\u5356\u63d0\u4f9b\u4e86\u4fbf\u5229\uff0c\u4f46\u4e5f\u7559\u4e0b\u4e86\u53ef\u8ffd\u8e2a\u7684\u6570\u5b57\u75d5\u8ff9\u3002\u7136\u800c\uff0c\u4ece\u6d77\u91cf\u5e7f\u544a\u4e2d\u8bc6\u522b\u91ce\u751f\u52a8\u7269\u76f8\u5173\u4ea7\u54c1\u6781\u5177\u6311\u6218\u6027\uff0c\u4f20\u7edf\u5206\u7c7b\u5668\u9700\u8981\u9ad8\u6602\u7684\u6807\u6ce8\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b56\u7565\uff1a\u5229\u7528LLM\u4e3a\u5c11\u91cf\u6570\u636e\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u518d\u7528\u8fd9\u4e9b\u6807\u7b7e\u8bad\u7ec3\u4e13\u7528\u5206\u7c7b\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u81ea\u52a8\u6536\u96c6\u591a\u6837\u5316\u548c\u4ee3\u8868\u6027\u7684\u6837\u672c\uff0c\u6700\u5c0f\u5316\u6807\u6ce8\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u7684\u5206\u7c7b\u5668F1\u5206\u6570\u9ad8\u8fbe95%\uff0c\u6027\u80fd\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528LLM\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u91ce\u751f\u52a8\u7269\u8d29\u5356\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u7814\u7a76\u9700\u6c42\u3002"}}
{"id": "2504.21214", "pdf": "https://arxiv.org/pdf/2504.21214", "abs": "https://arxiv.org/abs/2504.21214", "authors": ["Jinzhao Zhou", "Zehong Cao", "Yiqun Duan", "Connor Barkley", "Daniel Leong", "Xiaowei Jiang", "Quoc-Toan Nguyen", "Ziyi Zhao", "Thomas Do", "Yu-Cheng Chang", "Sheng-Fu Liang", "Chin-teng Lin"], "title": "Pretraining Large Brain Language Model for Active BCI: Silent Speech", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "This paper explores silent speech decoding in active brain-computer interface\n(BCI) systems, which offer more natural and flexible communication than\ntraditional BCI applications. We collected a new silent speech dataset of over\n120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing\n24 commonly used English words for language model pretraining and decoding.\nFollowing the recent success of pretraining large models with self-supervised\nparadigms to enhance EEG classification performance, we propose Large Brain\nLanguage Model (LBLM) pretrained to decode silent speech for active BCI. To\npretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining\nparadigm to learn effective representations from unlabeled EEG data. Unlike\nexisting EEG pretraining methods that mainly follow a masked-reconstruction\nparadigm, our proposed FSTP method employs autoregressive modeling in temporal\nand frequency domains to capture both temporal and spectral dependencies from\nEEG signals. After pretraining, we finetune our LBLM on downstream tasks,\nincluding word-level and semantic-level classification. Extensive experiments\ndemonstrate significant performance gains of the LBLM over fully-supervised and\npretrained baseline models. For instance, in the difficult cross-session\nsetting, our model achieves 47.0\\% accuracy on semantic-level classification\nand 39.6\\% in word-level classification, outperforming baseline methods by\n5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in\nactive BCI systems, offering an innovative solution for EEG language model\npretraining and a new dataset for fundamental research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8111\u8bed\u8a00\u6a21\u578b\uff08LBLM\uff09\uff0c\u7528\u4e8e\u89e3\u7801\u4e3b\u52a8\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u4e2d\u7684\u65e0\u58f0\u8bed\u97f3\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfBCI\u7cfb\u7edf\u5728\u81ea\u7136\u6027\u548c\u7075\u6d3b\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65e0\u58f0\u8bed\u97f3\u89e3\u7801\u6280\u672f\u63d0\u5347BCI\u7684\u901a\u4fe1\u80fd\u529b\u3002", "method": "\u63d0\u51faLBLM\u6a21\u578b\uff0c\u91c7\u7528\u672a\u6765\u65f6\u9891\u9884\u6d4b\uff08FSTP\uff09\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ece\u65e0\u6807\u7b7eEEG\u6570\u636e\u4e2d\u5b66\u4e60\u6709\u6548\u8868\u5f81\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u8de8\u4f1a\u8bdd\u8bbe\u7f6e\u4e0b\uff0cLBLM\u5728\u8bed\u4e49\u7ea7\u548c\u5355\u8bcd\u7ea7\u5206\u7c7b\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523047.0%\u548c39.6%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e3b\u52a8BCI\u7cfb\u7edf\u4e2d\u7684\u65e0\u58f0\u8bed\u97f3\u89e3\u7801\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8d21\u732e\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2504.21067", "pdf": "https://arxiv.org/pdf/2504.21067", "abs": "https://arxiv.org/abs/2504.21067", "authors": ["Yuhan Xie", "Yixi Cai", "Yinqiang Zhang", "Lei Yang", "Jia Pan"], "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": null, "summary": "This research tackles the challenge of real-time active view selection and\nuncertainty quantification on visual quality for active 3D reconstruction.\nVisual quality is a critical aspect of 3D reconstruction. Recent advancements\nsuch as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have\nnotably enhanced the image rendering quality of reconstruction models.\nNonetheless, the efficient and effective acquisition of input images for\nreconstruction-specifically, the selection of the most informative\nviewpoint-remains an open challenge, which is crucial for active\nreconstruction. Existing studies have primarily focused on evaluating geometric\ncompleteness and exploring unobserved or unknown regions, without direct\nevaluation of the visual uncertainty within the reconstruction model. To\naddress this gap, this paper introduces a probabilistic model that quantifies\nvisual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we\nformulate a criterion, Gaussian Splatting Shannon Mutual Information\n(GauSS-MI), for real-time assessment of visual mutual information from novel\nviewpoints, facilitating the selection of next best view. GauSS-MI is\nimplemented within an active reconstruction system integrated with a view and\nmotion planner. Extensive experiments across various simulated and real-world\nscenes showcase the superior visual quality and reconstruction efficiency\nperformance of the proposed system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u9999\u519c\u4e92\u4fe1\u606f\uff08GauSS-MI\uff09\u7684\u5b9e\u65f6\u4e3b\u52a8\u89c6\u56fe\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u91cd\u5efa\u4e2d\u7684\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u5f53\u524d3D\u91cd\u5efa\u6280\u672f\uff08\u5982NeRF\u548c3DGS\uff09\u5728\u56fe\u50cf\u6e32\u67d3\u8d28\u91cf\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u8f93\u5165\u89c6\u56fe\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u51e0\u4f55\u5b8c\u6574\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u91cd\u5efa\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u6982\u7387\u6a21\u578b\u91cf\u5316\u6bcf\u4e2a\u9ad8\u65af\u7684\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5229\u7528\u9999\u519c\u4e92\u4fe1\u606f\u63d0\u51faGauSS-MI\u51c6\u5219\uff0c\u5b9e\u65f6\u8bc4\u4f30\u65b0\u89c6\u56fe\u7684\u89c6\u89c9\u4e92\u4fe1\u606f\uff0c\u4ece\u800c\u9009\u62e9\u6700\u4f73\u89c6\u56fe\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GauSS-MI\u4e3a\u4e3b\u52a83D\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u89c6\u56fe\u9009\u62e9\u65b9\u6cd5\u3002"}}
{"id": "2504.21228", "pdf": "https://arxiv.org/pdf/2504.21228", "abs": "https://arxiv.org/abs/2504.21228", "authors": ["Rui Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ruiyi Zhang", "Ryan Rossi", "Lina Yao", "Julian McAuley"], "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.", "AI": {"tldr": "CachePrune\u901a\u8fc7\u4fee\u526aKV\u7f13\u5b58\u4e2d\u7684\u4efb\u52a1\u89e6\u53d1\u795e\u7ecf\u5143\uff0c\u9632\u5fa1LLMs\u7684\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u3002", "motivation": "LLMs\u6613\u53d7\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u79bb\u7528\u6237\u6307\u4ee4\uff0c\u9700\u5f00\u53d1\u9632\u5fa1\u65b9\u6cd5\u4ee5\u63d0\u5347AI\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faCachePrune\uff0c\u901a\u8fc7\u7279\u5f81\u5f52\u56e0\u548c\u635f\u5931\u51fd\u6570\u8bc6\u522b\u5e76\u4fee\u526a\u4efb\u52a1\u89e6\u53d1\u795e\u7ecf\u5143\uff0c\u907f\u514d\u6a21\u578b\u5c06\u8f93\u5165\u4e0a\u4e0b\u6587\u8bef\u8ba4\u4e3a\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCachePrune\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u4e14\u4e0d\u5f71\u54cd\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "CachePrune\u4e3a\u9632\u5fa1\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u63d0\u4f9b\u6709\u6548\u65b9\u6848\uff0c\u589e\u5f3aLLMs\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2504.21227", "pdf": "https://arxiv.org/pdf/2504.21227", "abs": "https://arxiv.org/abs/2504.21227", "authors": ["Omid Halimi Milani", "Amanda Nikho", "Lauren Mills", "Marouane Tliba", "Ahmet Enis Cetin", "Mohammed H. Elnagar"], "title": "Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "13 pages, 7 figures, accepted at IEEE VLSI Test Symposium (VTS) 2025", "summary": "Deep learning models have great potential in medical imaging, including\northodontics and skeletal maturity assessment. However, applying a model to\ndata different from its training set can lead to unreliable predictions that\nmay impact patient care. To address this, we propose a comprehensive\nverification framework that evaluates model suitability through multiple\ncomplementary strategies. First, we introduce a Gradient Attention Map\n(GAM)-based approach that analyzes attention patterns using Grad-CAM and\ncompares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine\nSimilarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.\nSecond, we extend verification to early convolutional feature maps, capturing\nstructural mis-alignments missed by attention alone. Finally, we incorporate an\nadditional garbage class into the classification model to explicitly reject\nout-of-distribution inputs. Experimental results demonstrate that these\ncombined methods effectively identify unsuitable models and inputs, promoting\nsafer and more reliable deployment of deep learning in medical imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u6ce8\u610f\u529b\u56fe\u548c\u7279\u5f81\u56fe\u5206\u6790\uff0c\u7ed3\u5408\u5783\u573e\u7c7b\u522b\uff0c\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u53ef\u80fd\u5bfc\u81f4\u9884\u6d4b\u4e0d\u53ef\u9760\uff0c\u5f71\u54cd\u60a3\u8005\u6cbb\u7597\u3002", "method": "1. \u4f7f\u7528\u68af\u5ea6\u6ce8\u610f\u529b\u56fe\uff08GAM\uff09\u5206\u6790\u6ce8\u610f\u529b\u6a21\u5f0f\uff1b2. \u6269\u5c55\u9a8c\u8bc1\u5230\u65e9\u671f\u5377\u79ef\u7279\u5f81\u56fe\uff1b3. \u5728\u5206\u7c7b\u6a21\u578b\u4e2d\u5f15\u5165\u5783\u573e\u7c7b\u522b\u4ee5\u62d2\u7edd\u5206\u5e03\u5916\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u4e0d\u5408\u9002\u7684\u6a21\u578b\u548c\u8f93\u5165\u3002", "conclusion": "\u7efc\u5408\u9a8c\u8bc1\u6846\u67b6\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u90e8\u7f72\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2504.21331", "pdf": "https://arxiv.org/pdf/2504.21331", "abs": "https://arxiv.org/abs/2504.21331", "authors": ["Alfred Yan", "Muhammad Nur Talha Kilic", "Gert Nolze", "Ankit Agrawal", "Alok Choudhary", "Roberto dos Reis", "Vinayak Dravid"], "title": "Towards Space Group Determination from EBSD Patterns: The Role of Deep Learning and High-throughput Dynamical Simulations", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "comment": "33 pages, preliminary version", "summary": "The design of novel materials hinges on the understanding of\nstructure-property relationships. However, our capability to synthesize a large\nnumber of materials has outpaced the ability and speed needed to characterize\nthem. While the overall chemical constituents can be readily known during\nsynthesis, the structural evolution and characterization of newly synthesized\nsamples remains a bottleneck for the ultimate goal of high throughput\nnanomaterials discovery. Thus, scalable methods for crystal symmetry\ndetermination that can analyze a large volume of material samples within a\nshort time-frame are especially needed. Kikuchi diffraction in the SEM is a\npromising technique for this due to its sensitivity to dynamical scattering,\nwhich may provide information beyond just the seven crystal systems and\nfourteen Bravais lattices. After diffraction patterns are collected from\nmaterial samples, deep learning methods may be able to classify the space group\nsymmetries using the patterns as input, which paired with the elemental\ncomposition, would help enable the determination of the crystal structure. To\ninvestigate the feasibility of this solution, neural networks were trained to\npredict the space group type of background corrected EBSD patterns. Our\nnetworks were first trained and tested on an artificial dataset of EBSD\npatterns of 5,148 different cubic phases, created through physics-based\ndynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised\ndeep learning-based domain adaptation method, was utilized to train neural\nnetworks to make predictions for experimental EBSD patterns. We introduce a\nrelabeling scheme, which enables our models to achieve accuracy scores higher\nthan 90% on simulated and experimental data, suggesting that neural networks\nare capable of making predictions of crystal symmetry from an EBSD pattern.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6676\u4f53\u5bf9\u79f0\u6027\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7Kikuchi\u884d\u5c04\u548c\u7535\u5b50\u80cc\u6563\u5c04\u884d\u5c04\uff08EBSD\uff09\u6280\u672f\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u7684\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u3002", "motivation": "\u65b0\u6750\u6599\u7684\u8bbe\u8ba1\u4f9d\u8d56\u4e8e\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u4f46\u5408\u6210\u901f\u5ea6\u8fdc\u8d85\u8868\u5f81\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u6676\u4f53\u5bf9\u79f0\u6027\u786e\u5b9a\u65b9\u6cd5\u3002", "method": "\u5229\u7528Kikuchi\u884d\u5c04\u548cEBSD\u6280\u672f\u6536\u96c6\u884d\u5c04\u56fe\u6848\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u7a7a\u95f4\u7fa4\u5bf9\u79f0\u6027\u3002\u91c7\u7528\u4eba\u5de5\u6570\u636e\u96c6\u548c\u6700\u5927\u5206\u7c7b\u5668\u5dee\u5f02\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u8868\u660e\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u4eceEBSD\u56fe\u6848\u4e2d\u9884\u6d4b\u6676\u4f53\u5bf9\u79f0\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408EBSD\u6280\u672f\u4e3a\u9ad8\u901a\u91cf\u7eb3\u7c73\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21235", "pdf": "https://arxiv.org/pdf/2504.21235", "abs": "https://arxiv.org/abs/2504.21235", "authors": ["Ben Goertzel"], "title": "Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "We present a lattice-based scheme for homomorphic evaluation of quantum\nprograms and proofs that remains secure against quantum adversaries. Classical\nhomomorphic encryption is lifted to the quantum setting by replacing\ncomposite-order groups with Module Learning-With-Errors (MLWE) lattices and by\ngeneralizing polynomial functors to bounded natural super functors (BNSFs). A\nsecret depolarizing BNSF mask hides amplitudes, while each quantum state is\nstored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game\nthat allows coherent access to the encryption oracle and give a four-hybrid\nreduction to decisional MLWE.\n  The design also covers practical issues usually left open. A typed QC-bridge\nkeeps classical bits produced by measurements encrypted yet still usable as\ncontrols, with weak-measurement semantics for expectation-value workloads.\nEncrypted Pauli twirls add circuit privacy. If a fixed knowledge base is\nneeded, its axioms are shipped as MLWE \"capsules\"; the evaluator can use them\nbut cannot read them. A rho-calculus driver schedules encrypted tasks across\nseveral QPUs and records an auditable trace on an RChain-style ledger.\n  Performance analysis shows that the extra lattice arithmetic fits inside\ntoday's QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof\nruns in about 10 ms, the public key (seed only) is 32 bytes, and even a\nCCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes\nhomomorphic teleportation plus knowledge-base-relative amplitude checks appears\nfeasible with current hardware. These results indicate that fully homomorphic,\nknowledge-base-aware quantum reasoning is compatible with near-term quantum\nclouds and standard post-quantum security assumptions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u683c\u7684\u5168\u540c\u6001\u91cf\u5b50\u7a0b\u5e8f\u4e0e\u8bc1\u660e\u8bc4\u4f30\u65b9\u6848\uff0c\u53ef\u62b5\u5fa1\u91cf\u5b50\u653b\u51fb\uff0c\u7ed3\u5408MLWE\u683c\u4e0eBNSF\u63a9\u7801\uff0c\u652f\u6301\u52a0\u5bc6\u91cf\u5b50\u6001\u4e0e\u7ecf\u5178\u63a7\u5236\uff0c\u6027\u80fd\u9ad8\u6548\u3002", "motivation": "\u5c06\u7ecf\u5178\u5168\u540c\u6001\u52a0\u5bc6\u6269\u5c55\u5230\u91cf\u5b50\u9886\u57df\uff0c\u89e3\u51b3\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u517c\u5bb9\u8fd1\u91cf\u5b50\u4e91\u4e0e\u540e\u91cf\u5b50\u5b89\u5168\u5047\u8bbe\u3002", "method": "\u4f7f\u7528MLWE\u683c\u66ff\u4ee3\u590d\u5408\u9636\u7fa4\uff0c\u5f15\u5165BNSF\u63a9\u7801\u9690\u85cf\u632f\u5e45\uff0c\u91cf\u5b50\u6001\u5b58\u50a8\u4e3aMLWE\u5bc6\u6587\u5bf9\uff0c\u901a\u8fc7qIND-CPA\u6e38\u620f\u5f62\u5f0f\u5316\u5b89\u5168\u6027\u3002", "result": "\u6027\u80fd\u5206\u6790\u663e\u793a\u65b9\u6848\u9ad8\u6548\uff0c100\u91cf\u5b50\u6bd4\u7279\u3001\u6df1\u5ea610^3\u7684\u8bc1\u660e\u4ec5\u970010ms\uff0c\u516c\u94a5\u6781\u5c0f\uff0832\u5b57\u8282\uff09\uff0cCCA\u7ea7\u5bc6\u94a5\u4f4e\u4e8e300kB\u3002", "conclusion": "\u65b9\u6848\u8868\u660e\u5168\u540c\u6001\u91cf\u5b50\u63a8\u7406\u4e0e\u8fd1\u91cf\u5b50\u4e91\u53ca\u540e\u91cf\u5b50\u5b89\u5168\u5047\u8bbe\u517c\u5bb9\uff0c\u5177\u5907\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2504.21380", "pdf": "https://arxiv.org/pdf/2504.21380", "abs": "https://arxiv.org/abs/2504.21380", "authors": ["In\u00eas Cardoso Oliveira", "Decebal Constantin Mocanu", "Luis A. Leiva"], "title": "Sparse-to-Sparse Training of Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models (DMs) are a powerful type of generative models that have\nachieved state-of-the-art results in various image synthesis tasks and have\nshown potential in other domains, such as natural language processing and\ntemporal data modeling. Despite their stable training dynamics and ability to\nproduce diverse high-quality samples, DMs are notorious for requiring\nsignificant computational resources, both in the training and inference stages.\nPrevious work has focused mostly on increasing the efficiency of model\ninference. This paper introduces, for the first time, the paradigm of\nsparse-to-sparse training to DMs, with the aim of improving both training and\ninference efficiency. We focus on unconditional generation and train sparse DMs\nfrom scratch (Latent Diffusion and ChiroDiff) on six datasets using three\ndifferent methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of\nsparsity in model performance. Our experiments show that sparse DMs are able to\nmatch and often outperform their Dense counterparts, while substantially\nreducing the number of trainable parameters and FLOPs. We also identify safe\nand effective values to perform sparse-to-sparse training of DMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u5230\u7a00\u758f\u8bad\u7ec3\u8303\u5f0f\uff0c\u7528\u4e8e\u6269\u6563\u6a21\u578b\uff08DMs\uff09\uff0c\u65e8\u5728\u63d0\u9ad8\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7a00\u758fDMs\u5728\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u5bc6\u96c6\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u3002\u6b64\u524d\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u6548\u7387\uff0c\u800c\u672c\u6587\u9996\u6b21\u63a2\u7d22\u7a00\u758f\u8bad\u7ec3\u4ee5\u540c\u65f6\u4f18\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "method": "\u91c7\u7528\u7a00\u758f\u5230\u7a00\u758f\u8bad\u7ec3\u8303\u5f0f\uff0c\u7814\u7a76\u4e86\u4e09\u79cd\u65b9\u6cd5\uff08Static-DM\u3001RigL-DM\u548cMagRan-DM\uff09\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7a00\u758fDMs\uff08Latent Diffusion\u548cChiroDiff\uff09\u7684\u6548\u679c\u3002", "result": "\u7a00\u758fDMs\u5728\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u548cFLOPs\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u5bc6\u96c6\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5e76\u786e\u5b9a\u4e86\u7a00\u758f\u8bad\u7ec3\u7684\u5b89\u5168\u6709\u6548\u503c\u3002", "conclusion": "\u7a00\u758f\u5230\u7a00\u758f\u8bad\u7ec3\u662f\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u6548\u7387\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.21239", "pdf": "https://arxiv.org/pdf/2504.21239", "abs": "https://arxiv.org/abs/2504.21239", "authors": ["Xu Pan", "Ely Hahami", "Zechen Zhang", "Haim Sompolinsky"], "title": "Memorization and Knowledge Injection in Gated LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently struggle to sequentially add new\nmemories and integrate new knowledge. These limitations contrast with the human\nability to continuously learn from new experiences and acquire knowledge\nthroughout life. Most existing approaches add memories either through large\ncontext windows or external memory buffers (e.g., Retrieval-Augmented\nGeneration), and studies on knowledge injection rarely test scenarios\nresembling everyday life events. In this work, we introduce a continual\nlearning framework, Memory Embedded in Gated LLMs (MEGa), which injects event\nmemories directly into the weights of LLMs. Each memory is stored in a\ndedicated set of gated low-rank weights. During inference, a gating mechanism\nactivates relevant memory weights by matching query embeddings to stored memory\nembeddings. This enables the model to both recall entire memories and answer\nrelated questions. On two datasets - fictional characters and Wikipedia events\n- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.\nOur model draws inspiration from the complementary memory system of the human\nbrain.", "AI": {"tldr": "MEGa\u6846\u67b6\u901a\u8fc7\u5c06\u8bb0\u5fc6\u76f4\u63a5\u5d4c\u5165LLM\u6743\u91cd\u4e2d\uff0c\u89e3\u51b3\u4e86LLM\u65e0\u6cd5\u6301\u7eed\u5b66\u4e60\u65b0\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLM\u65e0\u6cd5\u6301\u7eed\u5b66\u4e60\u548c\u6574\u5408\u65b0\u77e5\u8bc6\u7684\u5c40\u9650\u6027\uff0c\u6a21\u4eff\u4eba\u8111\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51faMEGa\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u8bb0\u5fc6\u5d4c\u5165\u5230LLM\u7684\u6743\u91cd\u4e2d\uff0c\u4f7f\u7528\u95e8\u63a7\u4f4e\u79e9\u6743\u91cd\u5b58\u50a8\u8bb0\u5fc6\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u6fc0\u6d3b\u76f8\u5173\u8bb0\u5fc6\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cMEGa\u5728\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MEGa\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u7684\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u6a21\u4eff\u4e86\u4eba\u8111\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002"}}
{"id": "2504.21432", "pdf": "https://arxiv.org/pdf/2504.21432", "abs": "https://arxiv.org/abs/2504.21432", "authors": ["Pranav Saxena", "Nishant Raghuvanshi", "Neena Goveas"], "title": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "A core challenge in AI-guided autonomy is enabling agents to navigate\nrealistically and effectively in previously unseen environments based on\nnatural language commands. We propose UAV-VLN, a novel end-to-end\nVision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)\nthat seamlessly integrates Large Language Models (LLMs) with visual perception\nto facilitate human-interactive navigation. Our system interprets free-form\nnatural language instructions, grounds them into visual observations, and plans\nfeasible aerial trajectories in diverse environments.\n  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse\nhigh-level semantic goals, while a vision model detects and localizes\nsemantically relevant objects in the environment. By fusing these modalities,\nthe UAV can reason about spatial relationships, disambiguate references in\nhuman instructions, and plan context-aware behaviors with minimal task-specific\nsupervision. To ensure robust and interpretable decision-making, the framework\nincludes a cross-modal grounding mechanism that aligns linguistic intent with\nvisual context.\n  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,\ndemonstrating its ability to generalize to novel instructions and environments\nwith minimal task-specific training. Our results show significant improvements\nin instruction-following accuracy and trajectory efficiency, highlighting the\npotential of LLM-driven vision-language interfaces for safe, intuitive, and\ngeneralizable UAV autonomy.", "AI": {"tldr": "UAV-VLN\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u611f\u77e5\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\uff08UAVs\uff09\u7684\u81ea\u7136\u8bed\u8a00\u5bfc\u822a\uff0c\u80fd\u591f\u89e3\u6790\u81ea\u7531\u5f62\u5f0f\u7684\u6307\u4ee4\u5e76\u89c4\u5212\u53ef\u884c\u8f68\u8ff9\u3002", "motivation": "\u89e3\u51b3AI\u81ea\u4e3b\u5bfc\u822a\u4e2d\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u6574\u5408LLMs\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u548c\u89c6\u89c9\u6a21\u578b\u7684\u5bf9\u8c61\u68c0\u6d4b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u673a\u5236\u5b9e\u73b0\u610f\u56fe\u4e0e\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u878d\u5408\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6307\u4ee4\u8ddf\u968f\u51c6\u786e\u6027\u548c\u8f68\u8ff9\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u89c6\u89c9\u8bed\u8a00\u63a5\u53e3\u4e3a\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u76f4\u89c2\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21261", "pdf": "https://arxiv.org/pdf/2504.21261", "abs": "https://arxiv.org/abs/2504.21261", "authors": ["Kasra Jalaldoust", "Saber Salehkaleybar", "Negar Kiyavash"], "title": "Multi-Domain Causal Discovery in Bijective Causal Models", "categories": ["cs.LG", "cs.AI", "stat.ME"], "comment": "Proceedings of Causal Learning and Reasoning (CLeaR) 2025", "summary": "We consider the problem of causal discovery (a.k.a., causal structure\nlearning) in a multi-domain setting. We assume that the causal functions are\ninvariant across the domains, while the distribution of the exogenous noise may\nvary. Under causal sufficiency (i.e., no confounders exist), we show that the\ncausal diagram can be discovered under less restrictive functional assumptions\ncompared to previous work. What enables causal discovery in this setting is\nbijective generation mechanisms (BGM), which ensures that the functional\nrelation between the exogenous noise $E$ and the endogenous variable $Y$ is\nbijective and differentiable in both directions at every level of the cause\nvariable $X = x$. BGM generalizes a variety of models including additive noise\nmodel, LiNGAM, post-nonlinear model, and location-scale noise model. Further,\nwe derive a statistical test to find the parents set of the target variable.\nExperiments on various synthetic and real-world datasets validate our\ntheoretical findings.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u57df\u73af\u5883\u4e0b\u7684\u56e0\u679c\u53d1\u73b0\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5c04\u751f\u6210\u673a\u5236\uff08BGM\uff09\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u66f4\u5bbd\u677e\u7684\u51fd\u6570\u5047\u8bbe\u4e0b\u53d1\u73b0\u56e0\u679c\u56fe\u3002", "motivation": "\u5728\u591a\u57df\u73af\u5883\u4e2d\uff0c\u56e0\u679c\u51fd\u6570\u7684\u8de8\u57df\u4e0d\u53d8\u6027\u4e3a\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u51fd\u6570\u5047\u8bbe\u4e0a\u9650\u5236\u8f83\u591a\u3002\u672c\u6587\u65e8\u5728\u653e\u5bbd\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5229\u7528\u53cc\u5c04\u751f\u6210\u673a\u5236\uff08BGM\uff09\uff0c\u786e\u4fdd\u5916\u751f\u566a\u58f0\u4e0e\u5185\u751f\u53d8\u91cf\u4e4b\u95f4\u7684\u51fd\u6570\u5173\u7cfb\u662f\u53cc\u5c04\u4e14\u53ef\u5fae\u7684\uff0c\u4ece\u800c\u63a8\u5e7f\u4e86\u591a\u79cd\u73b0\u6709\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BGM\u5728\u591a\u57df\u73af\u5883\u4e0b\u80fd\u591f\u6709\u6548\u53d1\u73b0\u56e0\u679c\u56fe\uff0c\u4e14\u7edf\u8ba1\u6d4b\u8bd5\u80fd\u51c6\u786e\u8bc6\u522b\u76ee\u6807\u53d8\u91cf\u7684\u7236\u96c6\u3002", "conclusion": "BGM\u4e3a\u591a\u57df\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u6846\u67b6\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.21530", "pdf": "https://arxiv.org/pdf/2504.21530", "abs": "https://arxiv.org/abs/2504.21530", "authors": ["Haifeng Huang", "Xinyi Chen", "Yilun Chen", "Hao Li", "Xiaoshen Han", "Zehan Wang", "Tai Wang", "Jiangmiao Pang", "Zhou Zhao"], "title": "RoboGround: Robotic Manipulation with Grounded Vision-Language Priors", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advancements in robotic manipulation have highlighted the potential of\nintermediate representations for improving policy generalization. In this work,\nwe explore grounding masks as an effective intermediate representation,\nbalancing two key advantages: (1) effective spatial guidance that specifies\ntarget objects and placement areas while also conveying information about\nobject shape and size, and (2) broad generalization potential driven by\nlarge-scale vision-language models pretrained on diverse grounding datasets. We\nintroduce RoboGround, a grounding-aware robotic manipulation system that\nleverages grounding masks as an intermediate representation to guide policy\nnetworks in object manipulation tasks. To further explore and enhance\ngeneralization, we propose an automated pipeline for generating large-scale,\nsimulated data with a diverse set of objects and instructions. Extensive\nexperiments show the value of our dataset and the effectiveness of grounding\nmasks as intermediate guidance, significantly enhancing the generalization\nabilities of robot policies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e grounding masks \u7684\u4e2d\u95f4\u8868\u793a\u65b9\u6cd5 RoboGround\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u6a21\u62df\u6570\u636e\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22 grounding masks \u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u7684\u6f5c\u529b\uff0c\u4ee5\u5e73\u8861\u7a7a\u95f4\u5f15\u5bfc\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165 RoboGround \u7cfb\u7edf\uff0c\u5229\u7528 grounding masks \u6307\u5bfc\u7b56\u7565\u7f51\u7edc\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u5316\u751f\u6210\u5927\u89c4\u6a21\u6a21\u62df\u6570\u636e\u7684\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cgrounding masks \u4f5c\u4e3a\u4e2d\u95f4\u6307\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "grounding masks \u662f\u4e00\u79cd\u6709\u6548\u7684\u4e2d\u95f4\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2504.21276", "pdf": "https://arxiv.org/pdf/2504.21276", "abs": "https://arxiv.org/abs/2504.21276", "authors": ["Wanyi Chen", "Meng-Wen Su", "Mary L. Cummings"], "title": "Assessing LLM code generation quality through path planning tasks", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As LLM-generated code grows in popularity, more evaluation is needed to\nassess the risks of using such tools, especially for safety-critical\napplications such as path planning. Existing coding benchmarks are insufficient\nas they do not reflect the context and complexity of safety-critical\napplications. To this end, we assessed six LLMs' abilities to generate the code\nfor three different path-planning algorithms and tested them on three maps of\nvarious difficulties. Our results suggest that LLM-generated code presents\nserious hazards for path planning applications and should not be applied in\nsafety-critical contexts without rigorous testing.", "AI": {"tldr": "\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\uff08\u5982\u8def\u5f84\u89c4\u5212\uff09\u4e2d\u7684\u98ce\u9669\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u4e0d\u8db3\uff0c\u6d4b\u8bd5\u516d\u79cdLLM\u751f\u6210\u4e09\u79cd\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u7684\u4ee3\u7801\uff0c\u7ed3\u679c\u663e\u793a\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u9690\u60a3\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u4ee3\u7801\u7684\u666e\u53ca\uff0c\u9700\u8bc4\u4f30\u5176\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u98ce\u9669\uff0c\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u53cd\u6620\u6b64\u7c7b\u5e94\u7528\u7684\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u3002", "method": "\u6d4b\u8bd5\u516d\u79cdLLM\u751f\u6210\u4e09\u79cd\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u7684\u4ee3\u7801\uff0c\u5e76\u5728\u4e09\u79cd\u4e0d\u540c\u96be\u5ea6\u5730\u56fe\u4e0a\u9a8c\u8bc1\u3002", "result": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u8def\u5f84\u89c4\u5212\u5e94\u7528\u4e2d\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u9690\u60a3\u3002", "conclusion": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u9700\u4e25\u683c\u6d4b\u8bd5\uff0c\u4e0d\u53ef\u76f4\u63a5\u4f7f\u7528\u3002"}}
{"id": "2504.21730", "pdf": "https://arxiv.org/pdf/2504.21730", "abs": "https://arxiv.org/abs/2504.21730", "authors": ["Ting Qiao", "Yingjia Wang", "Xing Liu", "Sixing Wu", "Jianbing Li", "Yiming Li"], "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": "15 pages", "summary": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u7279\u5b9a\u7684\u8ba4\u8bc1\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5Cert-SSB\uff0c\u901a\u8fc7\u4f18\u5316\u6bcf\u4e2a\u6837\u672c\u7684\u566a\u58f0\u5e45\u5ea6\u5e76\u805a\u5408\u591a\u4e2a\u5e73\u6ed1\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u9632\u5fa1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u968f\u673a\u5e73\u6ed1\u9632\u5fa1\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u6837\u672c\u4e0e\u51b3\u7b56\u8fb9\u754c\u7b49\u8ddd\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u8ba4\u8bc1\u6027\u80fd\u4e0d\u4f73\u3002", "method": "Cert-SSB\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0a\u5347\u4f18\u5316\u6bcf\u4e2a\u6837\u672c\u7684\u566a\u58f0\u5e45\u5ea6\uff0c\u5e76\u5f15\u5165\u5b58\u50a8\u66f4\u65b0\u8ba4\u8bc1\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u8ba4\u8bc1\u533a\u57df\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86Cert-SSB\u7684\u6709\u6548\u6027\u3002", "conclusion": "Cert-SSB\u901a\u8fc7\u6837\u672c\u7279\u5b9a\u7684\u566a\u58f0\u4f18\u5316\u548c\u52a8\u6001\u8ba4\u8bc1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9632\u5fa1\u540e\u95e8\u653b\u51fb\u7684\u80fd\u529b\u3002"}}
{"id": "2504.21289", "pdf": "https://arxiv.org/pdf/2504.21289", "abs": "https://arxiv.org/abs/2504.21289", "authors": ["Yan Huang", "Da-Qing Zhang"], "title": "Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Biclustering is an effective technique in data mining and pattern\nrecognition. Biclustering algorithms based on traditional clustering face two\nfundamental limitations when processing high-dimensional data: (1) The distance\nconcentration phenomenon in high-dimensional spaces leads to data sparsity,\nrendering similarity measures ineffective; (2) Mainstream linear dimensionality\nreduction methods disrupt critical local structural patterns. To apply\nbiclustering to high-dimensional datasets, we propose an orthogonal\nfactor-based biclustering algorithm (BCBOF). First, we constructed orthogonal\nfactors in the vector space of the high-dimensional dataset. Then, we performed\nclustering using the coordinates of the original data in the orthogonal\nsubspace as clustering targets. Finally, we obtained biclustering results of\nthe original dataset. Since dimensionality reduction was applied before\nclustering, the proposed algorithm effectively mitigated the data sparsity\nproblem caused by high dimensionality. Additionally, we applied this\nbiclustering algorithm to stock technical indicator combinations and stock\nprice trend prediction. Biclustering results were transformed into fuzzy rules,\nand we incorporated profit-preserving and stop-loss rules into the rule set,\nultimately forming a fuzzy inference system for stock price trend predictions\nand trading signals. To evaluate the performance of BCBOF, we compared it with\nexisting biclustering methods using multiple evaluation metrics. The results\nshowed that our algorithm outperformed other biclustering techniques. To\nvalidate the effectiveness of the fuzzy inference system, we conducted virtual\ntrading experiments using historical data from 10 A-share stocks. The\nexperimental results showed that the generated trading strategies yielded\nhigher returns for investors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u4ea4\u56e0\u5b50\u7684\u53cc\u805a\u7c7b\u7b97\u6cd5\uff08BCBOF\uff09\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u7a00\u758f\u6027\u548c\u5c40\u90e8\u7ed3\u6784\u7834\u574f\u95ee\u9898\uff0c\u5e76\u5728\u80a1\u7968\u9884\u6d4b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u805a\u7c7b\u7b97\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u9762\u4e34\u76f8\u4f3c\u6027\u5ea6\u91cf\u5931\u6548\u548c\u5c40\u90e8\u7ed3\u6784\u7834\u574f\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u6784\u5efa\u9ad8\u7ef4\u6570\u636e\u7684\u6b63\u4ea4\u56e0\u5b50\uff0c\u4ee5\u6b63\u4ea4\u5b50\u7a7a\u95f4\u4e2d\u7684\u5750\u6807\u4f5c\u4e3a\u805a\u7c7b\u76ee\u6807\uff0c\u751f\u6210\u53cc\u805a\u7c7b\u7ed3\u679c\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u6a21\u7cca\u89c4\u5219\u7528\u4e8e\u80a1\u7968\u9884\u6d4b\u3002", "result": "BCBOF\u5728\u591a\u9879\u8bc4\u4ef7\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u53cc\u805a\u7c7b\u65b9\u6cd5\uff0c\u865a\u62df\u4ea4\u6613\u5b9e\u9a8c\u663e\u793a\u5176\u751f\u6210\u7684\u4ea4\u6613\u7b56\u7565\u80fd\u5e26\u6765\u66f4\u9ad8\u6536\u76ca\u3002", "conclusion": "BCBOF\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u53cc\u805a\u7c7b\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2504.21731", "pdf": "https://arxiv.org/pdf/2504.21731", "abs": "https://arxiv.org/abs/2504.21731", "authors": ["Feiyu Lu", "Mengyu Chen", "Hsiang Hsu", "Pranav Deshpande", "Cheng Yao Wang", "Blair MacIntyre"], "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '24)", "summary": "Mixed Reality (MR) could assist users' tasks by continuously integrating\nvirtual content with their view of the physical environment. However, where and\nhow to place these content to best support the users has been a challenging\nproblem due to the dynamic nature of MR experiences. In contrast to prior work\nthat investigates optimization-based methods, we are exploring how\nreinforcement learning (RL) could assist with continuous 3D content placement\nthat is aware of users' poses and their surrounding environments. Through an\ninitial exploration and preliminary evaluation, our results demonstrate the\npotential of RL to position content that maximizes the reward for users on the\ngo. We further identify future directions for research that could harness the\npower of RL for personalized and optimized UI and content placement in MR.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u4e2d\u52a8\u6001\u4f18\u53163D\u5185\u5bb9\u5e03\u5c40\uff0c\u4ee5\u9002\u5e94\u7528\u6237\u59ff\u6001\u548c\u73af\u5883\u53d8\u5316\u3002", "motivation": "MR\u4e2d\u865a\u62df\u5185\u5bb9\u7684\u52a8\u6001\u5e03\u5c40\u662f\u4e00\u4e2a\u6311\u6218\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u5b9e\u65f6\u53d8\u5316\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u7528\u6237\u59ff\u6001\u548c\u73af\u5883\u4fe1\u606f\uff0c\u5b9e\u73b0\u8fde\u7eed3D\u5185\u5bb9\u5e03\u5c40\u4f18\u5316\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cRL\u80fd\u6709\u6548\u4f18\u5316\u5185\u5bb9\u5e03\u5c40\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "RL\u5728MR\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u7814\u7a76\u4e2a\u6027\u5316UI\u548c\u5185\u5bb9\u5e03\u5c40\u4f18\u5316\u3002"}}
{"id": "2504.21296", "pdf": "https://arxiv.org/pdf/2504.21296", "abs": "https://arxiv.org/abs/2504.21296", "authors": ["Renqiang Luo", "Ziqi Xu", "Xikun Zhang", "Qing Qing", "Huafei Huang", "Enyan Dai", "Zhe Wang", "Bo Yang"], "title": "Fairness in Graph Learning Augmented with Machine Learning: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Augmenting specialised machine learning techniques into traditional graph\nlearning models has achieved notable success across various domains, including\nfederated graph learning, dynamic graph learning, and graph transformers.\nHowever, the intricate mechanisms of these specialised techniques introduce\nsignificant challenges in maintaining model fairness, potentially resulting in\ndiscriminatory outcomes in high-stakes applications such as recommendation\nsystems, disaster response, criminal justice, and loan approval. This paper\nsystematically examines the unique fairness challenges posed by Graph Learning\naugmented with Machine Learning (GL-ML). It highlights the complex interplay\nbetween graph learning mechanisms and machine learning techniques, emphasising\nhow the augmentation of machine learning both enhances and complicates\nfairness. Additionally, we explore four critical techniques frequently employed\nto improve fairness in GL-ML methods. By thoroughly investigating the root\ncauses and broader implications of fairness challenges in this rapidly evolving\nfield, this work establishes a robust foundation for future research and\ninnovation in GL-ML fairness.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u56fe\u5b66\u4e60\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\uff08GL-ML\uff09\u4e2d\u7684\u516c\u5e73\u6027\u6311\u6218\uff0c\u5206\u6790\u4e86\u5176\u590d\u6742\u673a\u5236\u53ca\u6f5c\u5728\u6b67\u89c6\u6027\u7ed3\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u79cd\u6539\u8fdb\u516c\u5e73\u6027\u7684\u5173\u952e\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u56fe\u5b66\u4e60\u6a21\u578b\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6280\u672f\u867d\u5728\u591a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5176\u590d\u6742\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5f71\u54cd\u9ad8\u98ce\u9669\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76GL-ML\u4e2d\u7684\u516c\u5e73\u6027\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5206\u6790\u56fe\u5b66\u4e60\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u4ea4\u4e92\u673a\u5236\uff0c\u8bc6\u522b\u516c\u5e73\u6027\u95ee\u9898\u7684\u6839\u6e90\uff0c\u5e76\u63a2\u8ba8\u56db\u79cd\u6539\u8fdb\u516c\u5e73\u6027\u7684\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0GL-ML\u7684\u516c\u5e73\u6027\u6311\u6218\u590d\u6742\u4e14\u5f71\u54cd\u6df1\u8fdc\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u6f5c\u5728\u6b67\u89c6\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u4e3aGL-ML\u516c\u5e73\u6027\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u521b\u65b0\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.21778", "pdf": "https://arxiv.org/pdf/2504.21778", "abs": "https://arxiv.org/abs/2504.21778", "authors": ["Ayman A. Ameen", "Thomas Richter", "Andr\u00e9 Kaup"], "title": "LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Current learned image compression models typically exhibit high complexity,\nwhich demands significant computational resources. To overcome these\nchallenges, we propose an innovative approach that employs hierarchical feature\nextraction transforms to significantly reduce complexity while preserving bit\nrate reduction efficiency. Our novel architecture achieves this by using fewer\nchannels for high spatial resolution inputs/feature maps. On the other hand,\nfeature maps with a large number of channels have reduced spatial dimensions,\nthereby cutting down on computational load without sacrificing performance.\nThis strategy effectively reduces the forward pass complexity from \\(1256 \\,\n\\text{kMAC/Pixel}\\) to just \\(270 \\, \\text{kMAC/Pixel}\\). As a result, the\nreduced complexity model can open the way for learned image compression models\nto operate efficiently across various devices and pave the way for the\ndevelopment of new architectures in image compression technology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u538b\u7f29\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u578b\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u53d8\u6362\uff0c\u51cf\u5c11\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u8f93\u5165/\u7279\u5f81\u56fe\u7684\u901a\u9053\u6570\uff0c\u540c\u65f6\u964d\u4f4e\u9ad8\u901a\u9053\u6570\u7279\u5f81\u56fe\u7684\u7a7a\u95f4\u7ef4\u5ea6\u3002", "result": "\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece1256 kMAC/Pixel\u964d\u81f3270 kMAC/Pixel\uff0c\u6027\u80fd\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b66\u4e60\u578b\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u5728\u591a\u79cd\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u8fd0\u884c\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u5e76\u63a8\u52a8\u4e86\u56fe\u50cf\u538b\u7f29\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.21297", "pdf": "https://arxiv.org/pdf/2504.21297", "abs": "https://arxiv.org/abs/2504.21297", "authors": ["Wenjun Yang", "Eyhab Al-Masri"], "title": "Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI", "categories": ["cs.IT", "cs.AI", "cs.CY", "cs.ET", "math.IT"], "comment": null, "summary": "This paper introduces a conversational interface system that enables\nparticipatory design of differentially private AI systems in public sector\napplications. Addressing the challenge of balancing mathematical privacy\nguarantees with democratic accountability, we propose three key contributions:\n(1) an adaptive $\\epsilon$-selection protocol leveraging TOPSIS multi-criteria\ndecision analysis to align citizen preferences with differential privacy (DP)\nparameters, (2) an explainable noise-injection framework featuring real-time\nMean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and\n(3) an integrated legal-compliance mechanism that dynamically modulates privacy\nbudgets based on evolving regulatory constraints. Our results advance\nparticipatory AI practices by demonstrating how conversational interfaces can\nenhance public engagement in algorithmic privacy mechanisms, ensuring that\nprivacy-preserving AI in public sector governance remains both mathematically\nrobust and democratically accountable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u8bdd\u754c\u9762\u7cfb\u7edf\uff0c\u7528\u4e8e\u516c\u5171\u90e8\u95e8\u4e2d\u5dee\u5206\u9690\u79c1AI\u7cfb\u7edf\u7684\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u534f\u8bae\u3001\u53ef\u89e3\u91ca\u566a\u58f0\u6ce8\u5165\u6846\u67b6\u548c\u6cd5\u5f8b\u5408\u89c4\u673a\u5236\uff0c\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u6c11\u4e3b\u95ee\u8d23\u3002", "motivation": "\u89e3\u51b3\u5728\u516c\u5171\u90e8\u95e8\u5e94\u7528\u4e2d\u5e73\u8861\u6570\u5b66\u9690\u79c1\u4fdd\u8bc1\u4e0e\u6c11\u4e3b\u95ee\u8d23\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a\u81ea\u9002\u5e94\u03b5\u9009\u62e9\u534f\u8bae\u3001\u53ef\u89e3\u91ca\u566a\u58f0\u6ce8\u5165\u6846\u67b6\u548c\u52a8\u6001\u6cd5\u5f8b\u5408\u89c4\u673a\u5236\u3002", "result": "\u7ed3\u679c\u8868\u660e\u5bf9\u8bdd\u754c\u9762\u80fd\u589e\u5f3a\u516c\u4f17\u5bf9\u7b97\u6cd5\u9690\u79c1\u673a\u5236\u7684\u53c2\u4e0e\uff0c\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4AI\u5728\u516c\u5171\u6cbb\u7406\u4e2d\u65e2\u6570\u5b66\u7a33\u5065\u53c8\u6c11\u4e3b\u95ee\u8d23\u3002", "conclusion": "\u5bf9\u8bdd\u754c\u9762\u7cfb\u7edf\u80fd\u6709\u6548\u4fc3\u8fdb\u53c2\u4e0e\u5f0fAI\u5b9e\u8df5\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u4e0e\u6c11\u4e3b\u95ee\u8d23\u7684\u53cc\u91cd\u76ee\u6807\u3002"}}
{"id": "2504.21323", "pdf": "https://arxiv.org/pdf/2504.21323", "abs": "https://arxiv.org/abs/2504.21323", "authors": ["Chen Wu", "Qian Ma", "Prasenjit Mitra", "Sencun Zhu"], "title": "How to Backdoor the Knowledge Distillation", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge distillation has become a cornerstone in modern machine learning\nsystems, celebrated for its ability to transfer knowledge from a large, complex\nteacher model to a more efficient student model. Traditionally, this process is\nregarded as secure, assuming the teacher model is clean. This belief stems from\nconventional backdoor attacks relying on poisoned training data with backdoor\ntriggers and attacker-chosen labels, which are not involved in the distillation\nprocess. Instead, knowledge distillation uses the outputs of a clean teacher\nmodel to guide the student model, inherently preventing recognition or response\nto backdoor triggers as intended by an attacker. In this paper, we challenge\nthis assumption by introducing a novel attack methodology that strategically\npoisons the distillation dataset with adversarial examples embedded with\nbackdoor triggers. This technique allows for the stealthy compromise of the\nstudent model while maintaining the integrity of the teacher model. Our\ninnovative approach represents the first successful exploitation of\nvulnerabilities within the knowledge distillation process using clean teacher\nmodels. Through extensive experiments conducted across various datasets and\nattack settings, we demonstrate the robustness, stealthiness, and effectiveness\nof our method. Our findings reveal previously unrecognized vulnerabilities and\npave the way for future research aimed at securing knowledge distillation\nprocesses against backdoor attacks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6837\u672c\u6c61\u67d3\u77e5\u8bc6\u84b8\u998f\u6570\u636e\u96c6\uff0c\u6210\u529f\u5229\u7528\u5e72\u51c0\u7684\u6559\u5e08\u6a21\u578b\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u690d\u5165\u540e\u95e8\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u4f20\u7edf\u8ba4\u4e3a\u77e5\u8bc6\u84b8\u998f\u662f\u5b89\u5168\u7684\uff0c\u56e0\u4e3a\u6559\u5e08\u6a21\u578b\u5e72\u51c0\u4e14\u4e0d\u6d89\u53ca\u540e\u95e8\u653b\u51fb\u7684\u6570\u636e\u3002\u672c\u6587\u6311\u6218\u8fd9\u4e00\u5047\u8bbe\uff0c\u63a2\u7d22\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u84b8\u998f\u6570\u636e\u96c6\u4e2d\u6ce8\u5165\u5e26\u6709\u540e\u95e8\u89e6\u53d1\u5668\u7684\u5bf9\u6297\u6837\u672c\uff0c\u6084\u65e0\u58f0\u606f\u5730\u7834\u574f\u5b66\u751f\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u6559\u5e08\u6a21\u578b\u7684\u5b8c\u6574\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u653b\u51fb\u573a\u666f\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u3001\u9690\u853d\u6027\u548c\u9ad8\u6548\u6027\uff0c\u6210\u529f\u63ed\u793a\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u672a\u88ab\u8ba4\u8bc6\u5230\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5982\u4f55\u9632\u5fa1\u540e\u95e8\u653b\u51fb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.21326", "pdf": "https://arxiv.org/pdf/2504.21326", "abs": "https://arxiv.org/abs/2504.21326", "authors": ["Junkyu Lee", "Tian Gao", "Elliot Nelson", "Miao Liu", "Debarun Bhattacharjya", "Songtao Lu"], "title": "Q-function Decomposition with Intervention Semantics with Factored Action Spaces", "categories": ["cs.LG", "cs.AI"], "comment": "AISTATS 2025", "summary": "Many practical reinforcement learning environments have a discrete factored\naction space that induces a large combinatorial set of actions, thereby posing\nsignificant challenges. Existing approaches leverage the regular structure of\nthe action space and resort to a linear decomposition of Q-functions, which\navoids enumerating all combinations of factored actions. In this paper, we\nconsider Q-functions defined over a lower dimensional projected subspace of the\noriginal action space, and study the condition for the unbiasedness of\ndecomposed Q-functions using causal effect estimation from the no unobserved\nconfounder setting in causal statistics. This leads to a general scheme which\nwe call action decomposed reinforcement learning that uses the projected\nQ-functions to approximate the Q-function in standard model-free reinforcement\nlearning algorithms. The proposed approach is shown to improve sample\ncomplexity in a model-based reinforcement learning setting. We demonstrate\nimprovements in sample efficiency compared to state-of-the-art baselines in\nonline continuous control environments and a real-world offline sepsis\ntreatment environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u7edf\u8ba1\u7684\u52a8\u4f5c\u5206\u89e3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6295\u5f71Q\u51fd\u6570\u964d\u4f4e\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u79bb\u6563\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u907f\u514d\u679a\u4e3e\u6240\u6709\u52a8\u4f5c\u7ec4\u5408\u3002", "method": "\u5229\u7528\u56e0\u679c\u7edf\u8ba1\u4e2d\u7684\u65e0\u672a\u89c2\u6d4b\u6df7\u6742\u56e0\u5b50\u8bbe\u5b9a\uff0c\u5b9a\u4e49\u6295\u5f71\u5b50\u7a7a\u95f4\u4e0a\u7684Q\u51fd\u6570\uff0c\u63d0\u51fa\u52a8\u4f5c\u5206\u89e3\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u548c\u5b9e\u9645\u79bb\u7ebf\u73af\u5883\u4e2d\uff0c\u6837\u672c\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u4f5c\u5206\u89e3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u6837\u672c\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\u3002"}}
{"id": "2504.21358", "pdf": "https://arxiv.org/pdf/2504.21358", "abs": "https://arxiv.org/abs/2504.21358", "authors": ["Xiao Zheng", "Saeed Asadi Bagloee", "Majid Sarvi"], "title": "A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "32 pages, 16 figures", "summary": "Traffic forecasting is vital for Intelligent Transportation Systems, for\nwhich Machine Learning (ML) methods have been extensively explored to develop\ndata-driven Artificial Intelligence (AI) solutions. Recent research focuses on\nmodelling spatial-temporal correlations for short-term traffic prediction,\nleaving the favourable long-term forecasting a challenging and open issue. This\npaper presents a comparative study on large-scale real-world signalized\narterials and freeway traffic flow datasets, aiming to evaluate promising ML\nmethods in the context of large forecasting horizons up to 30 days. Focusing on\nmodelling capacity for temporal dynamics, we develop one ensemble ML method,\neXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods,\nincluding Recurrent Neural Network (RNN)-based methods and the state-of-the-art\nTransformer-based method. Time embedding is leveraged to enhance their\nunderstanding of seasonality and event factors. Experimental results highlight\nthat while the attention mechanism/Transformer framework is effective for\ncapturing long-range dependencies in sequential data, as the forecasting\nhorizon extends, the key to effective traffic forecasting gradually shifts from\ntemporal dependency capturing to periodicity modelling. Time embedding is\nparticularly effective in this context, helping naive RNN outperform Informer\nby 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust\nmodel, XGBoost, while learning solely from time features, performs\ncompetitively with DL methods. Moreover, we investigate the impacts of various\nfactors like input sequence length, holiday traffic, data granularity, and\ntraining data size. The findings offer valuable insights and serve as a\nreference for future long-term traffic forecasting research and the improvement\nof AI's corresponding learning capabilities.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u671f\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u65f6\u95f4\u5d4c\u5165\u5bf9\u5468\u671f\u6027\u5efa\u6a21\u81f3\u5173\u91cd\u8981\uff0cXGBoost\u5728\u4ec5\u4f7f\u7528\u65f6\u95f4\u7279\u5f81\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u957f\u671f\u4ea4\u901a\u9884\u6d4b\u662f\u4e00\u4e2a\u5f00\u653e\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u77ed\u671f\u9884\u6d4b\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86XGBoost\u548c\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982RNN\u548cTransformer\uff09\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u5d4c\u5165\u589e\u5f3a\u6a21\u578b\u5bf9\u5468\u671f\u6027\u548c\u4e8b\u4ef6\u7684\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u7740\u9884\u6d4b\u65f6\u95f4\u5ef6\u957f\uff0c\u5468\u671f\u6027\u5efa\u6a21\u6bd4\u65f6\u5e8f\u4f9d\u8d56\u6355\u83b7\u66f4\u91cd\u8981\uff1b\u65f6\u95f4\u5d4c\u5165\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cXGBoost\u8868\u73b0\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u4e3a\u957f\u671f\u4ea4\u901a\u9884\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u5468\u671f\u6027\u5efa\u6a21\u548c\u65f6\u95f4\u5d4c\u5165\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.21366", "pdf": "https://arxiv.org/pdf/2504.21366", "abs": "https://arxiv.org/abs/2504.21366", "authors": ["Yinfeng Yu", "Shiyu Sun"], "title": "DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion", "categories": ["cs.SD", "cs.AI"], "comment": "Main paper (9 pages). Accepted for publication by ICMR(International\n  Conference on Multimedia Retrieval) 2025", "summary": "Current Audio-Visual Source Separation methods primarily adopt two design\nstrategies. The first strategy involves fusing audio and visual features at the\nbottleneck layer of the encoder, followed by processing the fused features\nthrough the decoder. However, when there is a significant disparity between the\ntwo modalities, this approach may lead to the loss of critical information. The\nsecond strategy avoids direct fusion and instead relies on the decoder to\nhandle the interaction between audio and visual features. Nonetheless, if the\nencoder fails to integrate information across modalities adequately, the\ndecoder may be unable to effectively capture the complex relationships between\nthem. To address these issues, this paper proposes a dynamic fusion method\nbased on a gating mechanism that dynamically adjusts the modality fusion\ndegree. This approach mitigates the limitations of solely relying on the\ndecoder and facilitates efficient collaboration between audio and visual\nfeatures. Additionally, an audio attention module is introduced to enhance the\nexpressive capacity of audio features, thereby further improving model\nperformance. Experimental results demonstrate that our method achieves\nsignificant performance improvements on two benchmark datasets, validating its\neffectiveness and advantages in Audio-Visual Source Separation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7\u673a\u5236\u7684\u52a8\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u97f3\u9891-\u89c6\u89c9\u6e90\u5206\u79bb\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u6001\u878d\u5408\u548c\u4fe1\u606f\u4fdd\u7559\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u6e90\u5206\u79bb\u65b9\u6cd5\u5728\u6a21\u6001\u878d\u5408\u65f6\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u6216\u4ea4\u4e92\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u52a8\u6001\u95e8\u63a7\u673a\u5236\u8c03\u6574\u6a21\u6001\u878d\u5408\u7a0b\u5ea6\uff0c\u5e76\u5f15\u5165\u97f3\u9891\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u97f3\u9891\u7279\u5f81\u8868\u8fbe\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u52a8\u6001\u878d\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u878d\u5408\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u97f3\u9891-\u89c6\u89c9\u6e90\u5206\u79bb\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.21372", "pdf": "https://arxiv.org/pdf/2504.21372", "abs": "https://arxiv.org/abs/2504.21372", "authors": ["M\u00e1t\u00e9 Gedeon"], "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u8bed\u97f3\u4e8b\u4ef6\u63d0\u53d6\u6846\u67b6SpeechEE\uff0c\u7ed3\u5408\u9ad8\u6027\u80fdASR\u548c\u8bed\u4e49\u641c\u7d22\u589e\u5f3a\u7684LLM\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u89e6\u53d1\u548c\u53c2\u6570\u5206\u7c7b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u4e2dASR\u4e0eNLP\u7ed3\u5408\u7684\u6311\u6218\uff0c\u63a2\u7d22\u68c0\u7d22\u589e\u5f3a\u7684LLM\u5728\u4e8b\u4ef6\u63d0\u53d6\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8fc7\u6ee4\u673a\u5236\u5206\u7c7b\u8bed\u97f3\u7247\u6bb5\uff0c\u7ed3\u5408\u5c11\u6837\u672cLLM\u63d0\u793a\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u7d22\u63d0\u53d6\u4e8b\u4ef6\u89e6\u53d1\u548c\u53c2\u6570\u3002", "result": "o1-mini\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u89e6\u53d1\u5206\u7c7bF1\u8fbe63.3%\uff0c\u53c2\u6570\u5206\u7c7bF1\u8fbe27.8%\uff0c\u8d85\u8d8a\u5148\u524d\u57fa\u51c6\u3002", "conclusion": "\u6a21\u5757\u5316\u6846\u67b6\u7ed3\u5408\u68c0\u7d22\u589e\u5f3aLLM\u53ef\u5ab2\u7f8e\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u7ed3\u5408\u6587\u672c\u548c\u58f0\u5b66\u7279\u5f81\u7684\u6df7\u5408\u6a21\u578b\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2504.21383", "pdf": "https://arxiv.org/pdf/2504.21383", "abs": "https://arxiv.org/abs/2504.21383", "authors": ["Pulkit Agrawal", "Rukma Talwadker", "Aditya Pareek", "Tridib Mukherjee"], "title": "FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in state-of-the-art (SOTA) offline reinforcement learning\n(RL) have primarily focused on addressing function approximation errors, which\ncontribute to the overestimation of Q-values for out-of-distribution actions, a\nchallenge that static datasets exacerbate. However, high stakes applications\nsuch as recommendation systems in online gaming, introduce further complexities\ndue to player's psychology (intent) driven by gameplay experiences and the\ninherent volatility on the platform. These factors create highly sparse,\npartially overlapping state spaces across policies, further influenced by the\nexperiment path selection logic which biases state spaces towards specific\npolicies. Current SOTA methods constrain learning from such offline data by\nclipping known counterfactual actions as out-of-distribution due to poor\ngeneralization across unobserved states. Further aggravating conservative\nQ-learning and necessitating more online exploration. FAST-Q introduces a novel\napproach that (1) leverages Gradient Reversal Learning to construct balanced\nstate representations, regularizing the policy-specific bias between the\nplayer's state and action thereby enabling counterfactual estimation; (2)\nsupports offline counterfactual exploration in parallel with static data\nexploitation; and (3) proposes a Q-value decomposition strategy for\nmulti-objective optimization, facilitating explainable recommendations over\nshort and long-term objectives. These innovations demonstrate superiority of\nFAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent\nincrease in player returns, 2 percent improvement in lifetime value (LTV), 0.4\npercent enhancement in the recommendation driven engagement, 2 percent\nimprovement in the player's platform dwell time and an impressive 10 percent\nreduction in the costs associated with the recommendation, on our volatile\ngaming platform.", "AI": {"tldr": "FAST-Q\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u53cd\u8f6c\u5b66\u4e60\u5e73\u8861\u72b6\u6001\u8868\u793a\uff0c\u652f\u6301\u79bb\u7ebf\u53cd\u4e8b\u5b9e\u63a2\u7d22\uff0c\u5e76\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u63a8\u8350\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e38\u620f\u5e73\u53f0\u7684\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u5f53\u524dSOTA\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u9759\u6001\u6570\u636e\u96c6\u65f6\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u7684\u5728\u7ebf\u6e38\u620f\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u73a9\u5bb6\u7684\u5fc3\u7406\u548c\u5e73\u53f0\u6ce2\u52a8\u6027\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u72b6\u6001\u7a7a\u95f4\u7684\u7a00\u758f\u6027\u548c\u7b56\u7565\u504f\u5dee\u3002", "method": "FAST-Q\u91c7\u7528\u68af\u5ea6\u53cd\u8f6c\u5b66\u4e60\u6784\u5efa\u5e73\u8861\u72b6\u6001\u8868\u793a\uff0c\u652f\u6301\u79bb\u7ebf\u53cd\u4e8b\u5b9e\u63a2\u7d22\u4e0e\u9759\u6001\u6570\u636e\u5229\u7528\u5e76\u884c\uff0c\u5e76\u63d0\u51faQ\u503c\u5206\u89e3\u7b56\u7565\u5b9e\u73b0\u591a\u76ee\u6807\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFAST-Q\u5728\u73a9\u5bb6\u56de\u62a5\u3001\u7ec8\u8eab\u4ef7\u503c\u3001\u63a8\u8350\u9a71\u52a8\u53c2\u4e0e\u5ea6\u3001\u5e73\u53f0\u505c\u7559\u65f6\u95f4\u548c\u63a8\u8350\u6210\u672c\u7b49\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u4f53\u8868\u73b0\u4e3a0.15%\u81f310%\u7684\u63d0\u5347\u3002", "conclusion": "FAST-Q\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u548c\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u9ad8\u6ce2\u52a8\u6027\u5e73\u53f0\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u63a8\u8350\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21411", "pdf": "https://arxiv.org/pdf/2504.21411", "abs": "https://arxiv.org/abs/2504.21411", "authors": ["Xinyi Liu", "Yujie Wang", "Shenhan Zhu", "Fangcheng Fu", "Qingshuo Liu", "Guangming Lin", "Bin Cui"], "title": "Galvatron: An Automatic Distributed System for Efficient Foundation Model Training", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Galvatron is a distributed system for efficiently training large-scale\nFoundation Models. It overcomes the complexities of selecting optimal\nparallelism strategies by automatically identifying the most efficient hybrid\nstrategy, incorporating data, tensor, pipeline, sharded data, and sequence\nparallelism, along with recomputation. The system's architecture includes a\nprofiler for hardware and model analysis, a search engine for strategy\noptimization using decision trees and dynamic programming, and a runtime for\nexecuting these strategies efficiently. Benchmarking on various clusters\ndemonstrates Galvatron's superior throughput compared to existing frameworks.\nThis open-source system offers user-friendly interfaces and comprehensive\ndocumentation, making complex distributed training accessible and efficient.\nThe source code of Galvatron is available at\nhttps://github.com/PKU-DAIR/Hetu-Galvatron.", "AI": {"tldr": "Galvatron\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u5e76\u884c\u7b56\u7565\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u4e2d\u5e76\u884c\u7b56\u7565\u9009\u62e9\u7684\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u7ed3\u5408\u6570\u636e\u3001\u5f20\u91cf\u3001\u6d41\u6c34\u7ebf\u3001\u5206\u7247\u6570\u636e\u548c\u5e8f\u5217\u5e76\u884c\u4ee5\u53ca\u91cd\u8ba1\u7b97\uff0c\u901a\u8fc7\u786c\u4ef6\u548c\u6a21\u578b\u5206\u6790\u3001\u51b3\u7b56\u6811\u548c\u52a8\u6001\u7f16\u7a0b\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u96c6\u7fa4\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u7684\u541e\u5410\u91cf\u3002", "conclusion": "Galvatron\u901a\u8fc7\u5f00\u6e90\u548c\u7528\u6237\u53cb\u597d\u8bbe\u8ba1\uff0c\u4f7f\u590d\u6742\u5206\u5e03\u5f0f\u8bad\u7ec3\u66f4\u9ad8\u6548\u548c\u6613\u7528\u3002"}}
{"id": "2504.21415", "pdf": "https://arxiv.org/pdf/2504.21415", "abs": "https://arxiv.org/abs/2504.21415", "authors": ["Yi Wang", "Chengyv Wu", "Yang Liao", "Maowei You"], "title": "Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges", "categories": ["cs.CR", "cs.AI"], "comment": "14pages, 10 figures", "summary": "User authentication is essential to ensure secure access to computer systems,\nyet traditional methods face limitations in usability, cost, and security.\nMouse dynamics authentication, based on the analysis of users' natural\ninteraction behaviors with mouse devices, offers a cost-effective,\nnon-intrusive, and adaptable solution. However, challenges remain in\ndetermining the optimal data volume, balancing accuracy and practicality, and\neffectively capturing temporal behavioral patterns. In this study, we propose a\nstatistical method using Gaussian kernel density estimate (KDE) and\nKullback-Leibler (KL) divergence to estimate the sufficient data volume for\ntraining authentication models. We introduce the Mouse Authentication Unit\n(MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for\nefficient and accurate behavioral representation. Furthermore, we design the\nLocal-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet\nfor local feature extraction and GRU for modeling long-term temporal\ndependencies. Taking the Balabit and DFL datasets as examples, we significantly\nreduced the data scale, particularly by a factor of 10 for the DFL dataset,\ngreatly alleviating the training burden. Additionally, we determined the\noptimal input recognition unit length for the user authentication system on\ndifferent datasets based on the slope of Approximate Entropy. Training with\nimbalanced samples, our model achieved a successful defense AUC 98.52% for\nblind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing\nthe current sota performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9f20\u6807\u52a8\u6001\u7684\u7528\u6237\u8ba4\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u6570\u636e\u91cf\u548c\u884c\u4e3a\u6a21\u5f0f\u6355\u6349\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba4\u8bc1\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7528\u6237\u8ba4\u8bc1\u65b9\u6cd5\u5728\u53ef\u7528\u6027\u3001\u6210\u672c\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9f20\u6807\u52a8\u6001\u8ba4\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u975e\u4fb5\u5165\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548cKL\u6563\u5ea6\u786e\u5b9a\u8bad\u7ec3\u6570\u636e\u91cf\uff0c\u5f15\u5165MAU\u4f18\u5316\u884c\u4e3a\u8868\u793a\uff0c\u8bbe\u8ba1LT-AMouse\u6846\u67b6\u7ed3\u54081D-ResNet\u548cGRU\u63d0\u53d6\u7279\u5f81\u548c\u5efa\u6a21\u65f6\u5e8f\u4f9d\u8d56\u3002", "result": "\u5728Balabit\u548cDFL\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u6570\u636e\u91cf\uff08DFL\u51cf\u5c1110\u500d\uff09\uff0c\u8ba4\u8bc1\u7cfb\u7edfAUC\u8fbe\u523098.52%\uff08DFL\uff09\u548c94.65%\uff08Balabit\uff09\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9f20\u6807\u52a8\u6001\u8ba4\u8bc1\u4e2d\u7684\u6570\u636e\u91cf\u548c\u65f6\u5e8f\u6a21\u5f0f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba4\u8bc1\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.21427", "pdf": "https://arxiv.org/pdf/2504.21427", "abs": "https://arxiv.org/abs/2504.21427", "authors": ["Shermin Shahbazi", "Mohammad-Reza Nasiri", "Majid Ramezani"], "title": "MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages ,3 figures", "summary": "Accurate classification of EEG signals is crucial for brain-computer\ninterfaces (BCIs) and neuroprosthetic applications, yet many existing methods\nfail to account for the non-Euclidean, manifold structure of EEG data,\nresulting in suboptimal performance. Preserving this manifold information is\nessential to capture the true geometry of EEG signals, but traditional\nclassification techniques largely overlook this need. To this end, we propose\nMPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based\nClassifiers), that introduces two key innovations: (1) a feature engineering\nphase that combines covariance matrices and Radial Basis Function (RBF) kernels\nto capture both linear and non-linear relationships among EEG channels, and (2)\na clustering phase that employs a modified K-means algorithm tailored for the\nRiemannian manifold space, ensuring local geometric sensitivity. Ensembling\nmultiple clustering-based classifiers, MPEC achieves superior results,\nvalidated by significant improvements on the BCI Competition IV dataset 2a.", "AI": {"tldr": "MPEC\u65b9\u6cd5\u901a\u8fc7\u4fdd\u7559EEG\u4fe1\u53f7\u7684\u6d41\u5f62\u7ed3\u6784\uff0c\u7ed3\u5408\u534f\u65b9\u5dee\u77e9\u9635\u548cRBF\u6838\u7684\u7279\u5f81\u5de5\u7a0b\uff0c\u4ee5\u53ca\u6539\u8fdb\u7684K-means\u805a\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u4fe1\u53f7\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709EEG\u4fe1\u53f7\u5206\u7c7b\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u5176\u975e\u6b27\u51e0\u91cc\u5f97\u6d41\u5f62\u7ed3\u6784\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "MPEC\u65b9\u6cd5\u7ed3\u5408\u534f\u65b9\u5dee\u77e9\u9635\u548cRBF\u6838\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\uff0c\u5e76\u4f7f\u7528\u6539\u8fdb\u7684K-means\u7b97\u6cd5\u5728\u9ece\u66fc\u6d41\u5f62\u7a7a\u95f4\u4e2d\u8fdb\u884c\u805a\u7c7b\uff0c\u6700\u540e\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u805a\u7c7b\u5206\u7c7b\u5668\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728BCI Competition IV\u6570\u636e\u96c62a\u4e0a\u9a8c\u8bc1\u4e86MPEC\u65b9\u6cd5\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "MPEC\u901a\u8fc7\u4fdd\u7559EEG\u4fe1\u53f7\u7684\u6d41\u5f62\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u9002\u7528\u4e8eBCI\u548c\u795e\u7ecf\u5047\u4f53\u5e94\u7528\u3002"}}
{"id": "2504.21428", "pdf": "https://arxiv.org/pdf/2504.21428", "abs": "https://arxiv.org/abs/2504.21428", "authors": ["K\u0131van\u00e7 \u015eerefo\u011flu", "\u00d6nder G\u00fcrcan", "Reyhan Aydo\u011fan"], "title": "UAV Marketplace Simulation Tool for BVLOS Operations", "categories": ["cs.RO", "cs.AI", "cs.DC"], "comment": "3 pages, 2 figures, the 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2025)", "summary": "We present a simulation tool for evaluating team formation in autonomous\nmulti-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of\nSight (BVLOS). The tool models UAV collaboration and mission execution in\ndynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt\noperations. Our tool allows researchers to integrate and compare various team\nformation strategies in a controlled environment with configurable mission\nparameters and adversarial behaviors. The log of each simulation run is stored\nin a structured way along with performance metrics so that statistical analysis\ncould be done straightforwardly. The tool is versatile for testing and\nimproving UAV coordination strategies in real-world applications.", "AI": {"tldr": "\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u65e0\u4eba\u673a\u56e2\u961f\u5f62\u6210\u7684\u4eff\u771f\u5de5\u5177\uff0c\u652f\u6301\u52a8\u6001\u548c\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u4efb\u52a1\u6267\u884c\uff0c\u5e76\u5141\u8bb8\u6bd4\u8f83\u4e0d\u540c\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u56e2\u961f\u5f62\u6210\u7b56\u7565\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4f18\u5316\u4f9d\u636e\u3002", "method": "\u5f00\u53d1\u4eff\u771f\u5de5\u5177\uff0c\u6a21\u62df\u65e0\u4eba\u673a\u534f\u4f5c\u548c\u5bf9\u6297\u884c\u4e3a\uff0c\u8bb0\u5f55\u65e5\u5fd7\u548c\u6027\u80fd\u6307\u6807\u3002", "result": "\u5de5\u5177\u652f\u6301\u591a\u79cd\u7b56\u7565\u7684\u96c6\u6210\u4e0e\u6bd4\u8f83\uff0c\u4fbf\u4e8e\u7edf\u8ba1\u5206\u6790\u3002", "conclusion": "\u8be5\u5de5\u5177\u9002\u7528\u4e8e\u6d4b\u8bd5\u548c\u6539\u8fdb\u65e0\u4eba\u673a\u534f\u8c03\u7b56\u7565\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.21454", "pdf": "https://arxiv.org/pdf/2504.21454", "abs": "https://arxiv.org/abs/2504.21454", "authors": ["Federico Nesti", "Gianluca D'Amico", "Mauro Marinoni", "Giorgio Buttazzo"], "title": "SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted to IEEE ITSC 2025", "summary": "The use of machine learning in cyber-physical systems has attracted the\ninterest of both industry and academia. However, no general solution has yet\nbeen found against the unpredictable behavior of neural networks and\nreinforcement learning agents. Nevertheless, the improvements of\nphoto-realistic simulators have paved the way towards extensive testing of\ncomplex algorithms in different virtual scenarios, which would be expensive and\ndangerous to implement in the real world.\n  This paper presents SimPRIVE, a simulation framework for physical robot\ninteraction with virtual environments, which operates as a vehicle-in-the-loop\nplatform, rendering a virtual world while operating the vehicle in the real\nworld.\n  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be\nconfigured to move its digital twin in a virtual world built with the Unreal\nEngine 5 graphic engine, which can be populated with objects, people, or other\nvehicles with programmable behavior.\n  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds\nwhile being light-weight to contain execution times and allow fast rendering.\nIts main advantage lies in the possibility of testing complex algorithms on the\nfull software and hardware stack while minimizing the risks and costs of a test\ncampaign. The framework has been validated by testing a reinforcement learning\nagent trained for obstacle avoidance on an AgileX Scout Mini rover that\nnavigates a virtual office environment where everyday objects and people are\nplaced as obstacles. The physical rover moves with no collision in an indoor\nlimited space, thanks to a LiDAR-based heuristic.", "AI": {"tldr": "SimPRIVE\u662f\u4e00\u4e2a\u7528\u4e8e\u7269\u7406\u673a\u5668\u4eba\u4e0e\u865a\u62df\u73af\u5883\u4ea4\u4e92\u7684\u4eff\u771f\u6846\u67b6\uff0c\u652f\u6301ROS 2\u7684\u79fb\u52a8\u673a\u5668\u4eba\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u5728Unreal Engine 5\u6784\u5efa\u7684\u865a\u62df\u4e16\u754c\u4e2d\u8fd0\u884c\uff0c\u7528\u4e8e\u6d4b\u8bd5\u590d\u6742\u7b97\u6cd5\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u4e0d\u53ef\u9884\u6d4b\u884c\u4e3a\u9700\u8981\u4e00\u79cd\u5b89\u5168\u4e14\u7ecf\u6d4e\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1SimPRIVE\u6846\u67b6\uff0c\u5c06\u7269\u7406\u673a\u5668\u4eba\u4e0e\u865a\u62df\u73af\u5883\u7ed3\u5408\uff0c\u652f\u6301ROS 2\u548cUnreal Engine 5\uff0c\u7528\u4e8e\u6d4b\u8bd5\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u6d4b\u8bd5\u4e00\u4e2a\u907f\u969c\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u7269\u7406\u673a\u5668\u4eba\u5728\u865a\u62df\u73af\u5883\u4e2d\u65e0\u78b0\u649e\u8fd0\u884c\u3002", "conclusion": "SimPRIVE\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u4f4e\u98ce\u9669\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7b97\u6cd5\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2504.21457", "pdf": "https://arxiv.org/pdf/2504.21457", "abs": "https://arxiv.org/abs/2504.21457", "authors": ["Andrea Zanola", "Louis Fabrice Tshimanga", "Federico Del Pup", "Marco Baiesi", "Manfredo Atzori"], "title": "xEEGNet: Towards Explainable AI in EEG Dementia Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This work presents xEEGNet, a novel, compact, and explainable neural network\nfor EEG data analysis. It is fully interpretable and reduces overfitting\nthrough major parameter reduction. As an applicative use case, we focused on\nclassifying common dementia conditions, Alzheimer's and frontotemporal\ndementia, versus controls. xEEGNet is broadly applicable to other neurological\nconditions involving spectral alterations. We initially used ShallowNet, a\nsimple and popular model from the EEGNet-family. Its structure was analyzed and\ngradually modified to move from a \"black box\" to a more transparent model,\nwithout compromising performance. The learned kernels and weights were examined\nfrom a clinical standpoint to assess medical relevance. Model variants,\nincluding ShallowNet and the final xEEGNet, were evaluated using robust\nNested-Leave-N-Subjects-Out cross-validation for unbiased performance\nestimates. Variability across data splits was explained using embedded EEG\nrepresentations, grouped by class and set, with pairwise separability to\nquantify group distinction. Overfitting was assessed through\ntraining-validation loss correlation and training speed. xEEGNet uses only 168\nparameters, 200 times fewer than ShallowNet, yet retains interpretability,\nresists overfitting, achieves comparable median performance (-1.5%), and\nreduces variability across splits. This variability is explained by embedded\nEEG representations: higher accuracy correlates with greater separation between\ntest set controls and Alzheimer's cases, without significant influence from\ntraining data. xEEGNet's ability to filter specific EEG bands, learn\nband-specific topographies, and use relevant spectral features demonstrates its\ninterpretability. While large deep learning models are often prioritized for\nperformance, this study shows smaller architectures like xEEGNet can be equally\neffective in EEG pathology classification.", "AI": {"tldr": "xEEGNet\u662f\u4e00\u79cd\u65b0\u578b\u3001\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8eEEG\u6570\u636e\u5206\u6790\uff0c\u7279\u522b\u9002\u7528\u4e8e\u75f4\u5446\u75c7\u5206\u7c7b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5b8c\u5168\u53ef\u89e3\u91ca\u4e14\u51cf\u5c11\u8fc7\u62df\u5408\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8eEEG\u6570\u636e\u5206\u6790\uff0c\u5e76\u5e94\u7528\u4e8e\u75f4\u5446\u75c7\u5206\u7c7b\u3002", "method": "\u57fa\u4e8eShallowNet\u9010\u6b65\u6539\u8fdb\uff0c\u901a\u8fc7\u53c2\u6570\u51cf\u5c11\u548c\u4e34\u5e8a\u76f8\u5173\u5206\u6790\uff0c\u6784\u5efaxEEGNet\uff0c\u5e76\u4f7f\u7528\u5d4c\u5957\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6027\u80fd\u3002", "result": "xEEGNet\u4ec5\u7528168\u4e2a\u53c2\u6570\uff0c\u6027\u80fd\u4e0eShallowNet\u76f8\u5f53\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u548c\u53d8\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5c0f\u578b\u67b6\u6784\u5982xEEGNet\u5728EEG\u75c5\u7406\u5206\u7c7b\u4e2d\u540c\u6837\u6709\u6548\uff0c\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2504.21474", "pdf": "https://arxiv.org/pdf/2504.21474", "abs": "https://arxiv.org/abs/2504.21474", "authors": ["Hadi Bayrami Asl Tekanlou", "Jafar Razmara", "Mahsa Sanaei", "Mostafa Rahgouy", "Hamed Babaei Giglou"], "title": "Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 4 figures, accepted to the LLMs4Subjects shared task at\n  SemEval2025", "summary": "This paper presents our system, Homa, for SemEval-2025 Task 5: Subject\nTagging, which focuses on automatically assigning subject labels to technical\nrecords from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage\nOntoAligner, a modular ontology alignment toolkit, to address this task by\nintegrating retrieval-augmented generation (RAG) techniques. Our approach\nformulates the subject tagging problem as an alignment task, where records are\nmatched to GND categories based on semantic similarity. We evaluate\nOntoAligner's adaptability for subject indexing and analyze its effectiveness\nin handling multilingual records. Experimental results demonstrate the\nstrengths and limitations of this method, highlighting the potential of\nalignment techniques for improving subject tagging in digital libraries.", "AI": {"tldr": "Homa\u7cfb\u7edf\u5229\u7528OntoAligner\u5de5\u5177\u5305\u548cRAG\u6280\u672f\uff0c\u5c06\u4e3b\u9898\u6807\u6ce8\u95ee\u9898\u8f6c\u5316\u4e3a\u5bf9\u9f50\u4efb\u52a1\uff0c\u8bc4\u4f30\u5176\u5728\u591a\u8bed\u8a00\u8bb0\u5f55\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u6280\u672f\u8bb0\u5f55\u7684\u4e3b\u9898\u6807\u6ce8\u95ee\u9898\uff0c\u63d0\u5347\u6570\u5b57\u56fe\u4e66\u9986\u4e2d\u4e3b\u9898\u6807\u6ce8\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528OntoAligner\u5de5\u5177\u5305\u548cRAG\u6280\u672f\uff0c\u5c06\u8bb0\u5f55\u4e0eGND\u5206\u7c7b\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u8bb0\u5f55\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u5bf9\u9f50\u6280\u672f\u6709\u6f5c\u529b\u6539\u8fdb\u6570\u5b57\u56fe\u4e66\u9986\u7684\u4e3b\u9898\u6807\u6ce8\u3002"}}
{"id": "2504.21475", "pdf": "https://arxiv.org/pdf/2504.21475", "abs": "https://arxiv.org/abs/2504.21475", "authors": ["Serry Sibaee", "Samar Ahmed", "Abdullah Al Harbi", "Omer Nacar", "Adel Ammar", "Yasser Habashi", "Wadii Boulila"], "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical gap in Arabic natural language processing\nby developing an effective Arabic Reverse Dictionary (RD) system that enables\nusers to find words based on their descriptions or meanings. We present a novel\ntransformer-based approach with a semi-encoder neural network architecture\nfeaturing geometrically decreasing layers that achieves state-of-the-art\nresults for Arabic RD tasks. Our methodology incorporates a comprehensive\ndataset construction process and establishes formal quality standards for\nArabic lexicographic definitions. Experiments with various pre-trained models\ndemonstrate that Arabic-specific models significantly outperform general\nmultilingual embeddings, with ARBERTv2 achieving the best ranking score\n(0.0644). Additionally, we provide a formal abstraction of the reverse\ndictionary task that enhances theoretical understanding and develop a modular,\nextensible Python library (RDTL) with configurable training pipelines. Our\nanalysis of dataset quality reveals important insights for improving Arabic\ndefinition construction, leading to eight specific standards for building\nhigh-quality reverse dictionary resources. This work contributes significantly\nto Arabic computational linguistics and provides valuable tools for language\nlearning, academic writing, and professional communication in Arabic.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u963f\u62c9\u4f2f\u8bed\u53cd\u5411\u8bcd\u5178\u7cfb\u7edf\uff0c\u586b\u8865\u4e86\u963f\u62c9\u4f2f\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u9ad8\u8d28\u91cf\u8bcd\u5178\u8d44\u6e90\u6784\u5efa\u7684\u6807\u51c6\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u53cd\u5411\u8bcd\u5178\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u8bed\u8a00\u5b66\u4e60\u3001\u5b66\u672f\u5199\u4f5c\u548c\u4e13\u4e1a\u4ea4\u6d41\u7684\u6548\u7387\u3002", "method": "\u91c7\u7528\u534a\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u51e0\u4f55\u9012\u51cf\u5c42\uff0c\u5e76\u5229\u7528\u963f\u62c9\u4f2f\u8bed\u7279\u5b9a\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982ARBERTv2\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "ARBERTv2\u6a21\u578b\u5728\u6392\u540d\u5f97\u5206\u4e0a\u8868\u73b0\u6700\u4f73\uff080.0644\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u516b\u9879\u6784\u5efa\u9ad8\u8d28\u91cf\u53cd\u5411\u8bcd\u5178\u8d44\u6e90\u7684\u6807\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u963f\u62c9\u4f2f\u8bed\u8ba1\u7b97\u8bed\u8a00\u5b66\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u7406\u8bba\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u8bed\u8a00\u8d44\u6e90\u7684\u9ad8\u8d28\u91cf\u53d1\u5c55\u3002"}}
{"id": "2504.21480", "pdf": "https://arxiv.org/pdf/2504.21480", "abs": "https://arxiv.org/abs/2504.21480", "authors": ["Yuchen Ding", "Hongli Peng", "Xiaoqi Li"], "title": "A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "With the rapid advancement of blockchain technology, smart contracts have\nenabled the implementation of increasingly complex functionalities. However,\nensuring the security of smart contracts remains a persistent challenge across\nthe stages of development, compilation, and execution. Vulnerabilities within\nsmart contracts not only undermine the security of individual applications but\nalso pose significant risks to the broader blockchain ecosystem, as\ndemonstrated by the growing frequency of attacks since 2016, resulting in\nsubstantial financial losses. This paper provides a comprehensive analysis of\nkey security risks in Ethereum smart contracts, specifically those written in\nSolidity and executed on the Ethereum Virtual Machine (EVM). We focus on two\nprevalent and critical vulnerability types (reentrancy and integer overflow) by\nexamining their underlying mechanisms, replicating attack scenarios, and\nassessing effective countermeasures.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4ee5\u592a\u574a\u667a\u80fd\u5408\u7ea6\u4e2d\u7684\u4e24\u79cd\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\uff08\u91cd\u5165\u548c\u6574\u6570\u6ea2\u51fa\uff09\uff0c\u63a2\u8ba8\u4e86\u5176\u673a\u5236\u3001\u653b\u51fb\u573a\u666f\u53ca\u5e94\u5bf9\u63aa\u65bd\u3002", "motivation": "\u968f\u7740\u533a\u5757\u94fe\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u667a\u80fd\u5408\u7ea6\u7684\u5b89\u5168\u6027\u6210\u4e3a\u91cd\u8981\u6311\u6218\uff0c\u6f0f\u6d1e\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u7ecf\u6d4e\u635f\u5931\u548c\u751f\u6001\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5206\u6790Solidity\u7f16\u5199\u7684\u667a\u80fd\u5408\u7ea6\u5728EVM\u4e0a\u7684\u6267\u884c\u673a\u5236\uff0c\u590d\u73b0\u653b\u51fb\u573a\u666f\u5e76\u8bc4\u4f30\u6709\u6548\u5bf9\u7b56\u3002", "result": "\u63ed\u793a\u4e86\u91cd\u5165\u548c\u6574\u6570\u6ea2\u51fa\u6f0f\u6d1e\u7684\u673a\u5236\u53ca\u5176\u5bf9\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9488\u5bf9\u8fd9\u4e24\u79cd\u6f0f\u6d1e\u7684\u6709\u6548\u9632\u62a4\u63aa\u65bd\uff0c\u5f3a\u8c03\u4e86\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.21489", "pdf": "https://arxiv.org/pdf/2504.21489", "abs": "https://arxiv.org/abs/2504.21489", "authors": ["Shirin Anlen", "Zuzanna Wojciak"], "title": "TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS", "categories": ["cs.CY", "cs.AI"], "comment": "33 pages", "summary": "The rise of generative AI and deceptive synthetic media threatens the global\ninformation ecosystem, especially across the Global Majority. This report from\nWITNESS highlights the limitations of current AI detection tools, which often\nunderperform in real-world scenarios due to challenges related to\nexplainability, fairness, accessibility, and contextual relevance. In response,\nWITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)\nBenchmark, a new framework for evaluating detection tools based on their\nreal-world impact and capacity for innovation. Drawing on frontline\nexperiences, deceptive AI cases, and global consultations, the report outlines\nhow detection tools must evolve to become truly innovative and relevant by\nmeeting diverse linguistic, cultural, and technological contexts. It offers\npractical guidance for developers, policymakers, and standards bodies to design\naccountable, transparent, and user-centered detection solutions, and\nincorporate sociotechnical considerations into future AI standards, procedures\nand evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can\ndrive innovation, safeguard public trust, strengthen AI literacy, and\ncontribute to a more resilient global information credibility.", "AI": {"tldr": "WITNESS\u63d0\u51faTRIED Benchmark\uff0c\u8bc4\u4f30AI\u68c0\u6d4b\u5de5\u5177\u7684\u5b9e\u9645\u6548\u679c\u548c\u521b\u65b0\u6027\uff0c\u5f3a\u8c03\u5de5\u5177\u9700\u9002\u5e94\u591a\u6837\u5316\u7684\u8bed\u8a00\u3001\u6587\u5316\u548c\u79d1\u6280\u80cc\u666f\u3002", "motivation": "\u751f\u6210\u5f0fAI\u548c\u865a\u5047\u5408\u6210\u5a92\u4f53\u5a01\u80c1\u5168\u7403\u4fe1\u606f\u751f\u6001\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8e\u524d\u7ebf\u7ecf\u9a8c\u3001\u865a\u5047AI\u6848\u4f8b\u548c\u5168\u7403\u54a8\u8be2\uff0c\u63d0\u51faTRIED Benchmark\u6846\u67b6\u3002", "result": "\u62a5\u544a\u4e3a\u5f00\u53d1\u8005\u3001\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\uff0c\u63a8\u52a8\u900f\u660e\u3001\u8d1f\u8d23\u4efb\u7684\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u91c7\u7528TRIED Benchmark\u53ef\u4fc3\u8fdb\u521b\u65b0\u3001\u589e\u5f3a\u516c\u4f17\u4fe1\u4efb\uff0c\u63d0\u5347\u5168\u7403\u4fe1\u606f\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2504.21545", "pdf": "https://arxiv.org/pdf/2504.21545", "abs": "https://arxiv.org/abs/2504.21545", "authors": ["Yangyang Li", "Guanlong Liu", "Ronghua Shang", "Licheng Jiao"], "title": "Meta knowledge assisted Evolutionary Neural Architecture Search", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Evolutionary computation (EC)-based neural architecture search (NAS) has\nachieved remarkable performance in the automatic design of neural\narchitectures. However, the high computational cost associated with evaluating\nsearched architectures poses a challenge for these methods, and a fixed form of\nlearning rate (LR) schedule means greater information loss on diverse searched\narchitectures. This paper introduces an efficient EC-based NAS method to solve\nthese problems via an innovative meta-learning framework. Specifically, a\nmeta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a\nsuitable LR schedule, which guides the training process with lower information\nloss when evaluating each individual. An adaptive surrogate model is designed\nthrough an adaptive threshold to select the potential architectures in a few\nepochs and then evaluate the potential architectures with complete epochs.\nAdditionally, a periodic mutation operator is proposed to increase the\ndiversity of the population, which enhances the generalizability and\nrobustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets\ndemonstrate that the proposed method achieves high performance comparable to\nthat of many state-of-the-art peer methods, with lower computational cost and\ngreater robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u9ad8\u6548\u8fdb\u5316\u8ba1\u7b97\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4ee3\u7406\u6a21\u578b\u548c\u5468\u671f\u6027\u53d8\u5f02\u7b97\u5b50\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8fdb\u5316\u8ba1\u7b97\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u56fa\u5b9a\u5b66\u4e60\u7387\u5bfc\u81f4\u7684\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5143\u5b66\u4e60\u7387\u65b9\u6848\u9884\u8bad\u7ec3\u5b66\u4e60\u7387\u8ba1\u5212\uff0c\u8bbe\u8ba1\u81ea\u9002\u5e94\u4ee3\u7406\u6a21\u578b\u7b5b\u9009\u6f5c\u529b\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u5468\u671f\u6027\u53d8\u5f02\u7b97\u5b50\u589e\u52a0\u79cd\u7fa4\u591a\u6837\u6027\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet1K\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u9c81\u68d2\u6027\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.21565", "pdf": "https://arxiv.org/pdf/2504.21565", "abs": "https://arxiv.org/abs/2504.21565", "authors": ["David Fern\u00e1ndez Narro", "Pablo Ferri", "Juan M. Garc\u00eda-G\u00f3mez", "Carlos S\u00e1ez"], "title": "Towards proactive self-adaptive AI for non-stationary environments with dataset shifts", "categories": ["cs.LG", "cs.AI", "I.2.8"], "comment": "6 pages, 4 figures, conference paper", "summary": "Artificial Intelligence (AI) models deployed in production frequently face\nchallenges in maintaining their performance in non-stationary environments.\nThis issue is particularly noticeable in medical settings, where temporal\ndataset shifts often occur. These shifts arise when the distributions of\ntraining data differ from those of the data encountered during deployment over\ntime. Further, new labeled data to continuously retrain AI is not typically\navailable in a timely manner due to data access limitations. To address these\nchallenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,\nwhere we model the temporal trajectory of AI parameters, allowing us to\nshort-term forecast parameter values. To this end, we use polynomial spline\nbases, within an extensible Functional Data Analysis framework. We validate our\nmethodology with a logistic regression model addressing prior probability\nshift, covariate shift, and concept shift. This validation is conducted on both\na controlled simulated dataset and a publicly available real-world COVID-19\ndataset from Mexico, with various shifts occurring between 2020 and 2024. Our\nresults indicate that this approach enhances the performance of AI against\nshifts compared to baseline stable models trained at different time distances\nfrom the present, without requiring updated training data. This work lays the\nfoundation for pro-adaptive AI research against dynamic, non-stationary\nenvironments, being compatible with data protection, in resilient AI production\nenvironments for health.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u81ea\u9002\u5e94\u7684AI\u65b9\u6cd5\uff08pro-adaptive\uff09\uff0c\u901a\u8fc7\u5efa\u6a21AI\u53c2\u6570\u7684\u65f6\u95f4\u8f68\u8ff9\u6765\u9884\u6d4b\u77ed\u671f\u53c2\u6570\u503c\uff0c\u4ee5\u5e94\u5bf9\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u975e\u5e73\u7a33\u73af\u5883\u4e2d\uff0cAI\u6a21\u578b\u5e38\u56e0\u6570\u636e\u5206\u5e03\u968f\u65f6\u95f4\u53d8\u5316\u800c\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u7f3a\u4e4f\u53ca\u65f6\u7684\u65b0\u6807\u6ce8\u6570\u636e\u7528\u4e8e\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u591a\u9879\u5f0f\u6837\u6761\u57fa\u548c\u529f\u80fd\u6570\u636e\u5206\u6790\u6846\u67b6\uff0c\u5efa\u6a21AI\u53c2\u6570\u7684\u65f6\u95f4\u8f68\u8ff9\uff0c\u9884\u6d4b\u77ed\u671f\u53c2\u6570\u503c\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9eCOVID-19\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86AI\u5bf9\u6570\u636e\u504f\u79fb\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u66f4\u65b0\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94AI\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u533b\u7597\u7b49\u6570\u636e\u4fdd\u62a4\u4e25\u683c\u7684\u573a\u666f\u3002"}}
{"id": "2504.21582", "pdf": "https://arxiv.org/pdf/2504.21582", "abs": "https://arxiv.org/abs/2504.21582", "authors": ["Qirui Mi", "Mengyue Yang", "Xiangning Yu", "Zhiyu Zhao", "Cheng Deng", "Bo An", "Haifeng Zhang", "Xu Chen", "Jun Wang"], "title": "MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework", "categories": ["cs.MA", "cs.AI"], "comment": "27 pages, 8 figures, 4 tables", "summary": "Simulating collective decision-making involves more than aggregating\nindividual behaviors; it arises from dynamic interactions among individuals.\nWhile large language models (LLMs) show promise for social simulation, existing\napproaches often exhibit deviations from real-world data. To address this gap,\nwe propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the\nfeedback loop between micro-level decisions and macro-level population. MF-LLM\nalternates between two models: a policy model that generates individual actions\nbased on personal states and group-level information, and a mean field model\nthat updates the population distribution from the latest individual decisions.\nTogether, they produce rollouts that simulate the evolving trajectories of\ncollective decision-making. To better match real-world data, we introduce\nIB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck\nprinciple, which maximizes the relevance of population distributions to future\nactions while minimizing redundancy with historical data. We evaluate MF-LLM on\na real-world social dataset, where it reduces KL divergence to human population\ndistributions by 47 percent over non-mean-field baselines, and enables accurate\ntrend forecasting and intervention planning. It generalizes across seven\ndomains and four LLM backbones, providing a scalable foundation for\nhigh-fidelity social simulation.", "AI": {"tldr": "\u63d0\u51faMF-LLM\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u89c2\u51b3\u7b56\u4e0e\u5b8f\u89c2\u7fa4\u4f53\u7684\u53cd\u9988\u5faa\u73af\u6a21\u62df\u96c6\u4f53\u51b3\u7b56\uff0c\u7ed3\u5408IB-Tune\u65b9\u6cd5\u4f18\u5316LLM\uff0c\u663e\u8457\u63d0\u5347\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u5339\u914d\u5ea6\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728\u6a21\u62df\u96c6\u4f53\u51b3\u7b56\u65f6\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u504f\u5dee\uff0c\u9700\u6539\u8fdb\u3002", "method": "MF-LLM\u6846\u67b6\u4ea4\u66ff\u4f7f\u7528\u7b56\u7565\u6a21\u578b\u548c\u5e73\u5747\u573a\u6a21\u578b\uff0c\u7ed3\u5408IB-Tune\u65b9\u6cd5\u4f18\u5316LLM\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cKL\u6563\u5ea6\u964d\u4f4e47%\uff0c\u652f\u6301\u51c6\u786e\u8d8b\u52bf\u9884\u6d4b\u548c\u5e72\u9884\u89c4\u5212\u3002", "conclusion": "MF-LLM\u4e3a\u9ad8\u4fdd\u771f\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u548c\u591aLLM\u67b6\u6784\u3002"}}
{"id": "2504.21585", "pdf": "https://arxiv.org/pdf/2504.21585", "abs": "https://arxiv.org/abs/2504.21585", "authors": ["Yingzhuo Jiang", "Wenjun Huang", "Rongdun Lin", "Chenyang Miao", "Tianfu Sun", "Yunduan Cui"], "title": "Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper tackles the challenge of learning multi-goal dexterous hand\nmanipulation tasks using model-based Reinforcement Learning. We propose\nGoal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing\nprobabilistic neural network ensembles to describe the high-dimensional\ndexterous hand dynamics and introducing an asynchronous MPC policy to meet the\ncontrol frequency requirements in real-world dexterous hand systems. Extensive\nevaluations on four simulated Shadow Hand manipulation scenarios with randomly\ngenerated goals demonstrate GC-PMPC's superior performance over\nstate-of-the-art baselines. It successfully drives a cable-driven Dexterous\nhand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn\nmanipulating a cubic die to three goal poses within approximately 80 minutes of\ninteractions, demonstrating exceptional learning efficiency and control\nperformance on a cost-effective dexterous hand platform.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff08GC-PMPC\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u76ee\u6807\u7075\u5de7\u624b\u64cd\u63a7\u4efb\u52a1\uff0c\u901a\u8fc7\u6982\u7387\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u548c\u5f02\u6b65MPC\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u64cd\u63a7\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u5ea6\u7075\u5de7\u624b\u52a8\u529b\u5b66\u5efa\u6a21\u548c\u591a\u76ee\u6807\u64cd\u63a7\u4efb\u52a1\u7684\u6311\u6218\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u63a7\u5236\u9891\u7387\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u6982\u7387\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u6765\u63cf\u8ff0\u7075\u5de7\u624b\u52a8\u529b\u5b66\uff0c\u5e76\u5f15\u5165\u5f02\u6b65MPC\u7b56\u7565\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cGC-PMPC\u5728\u56db\u79cdShadow Hand\u64cd\u63a7\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u9a71\u52a812\u81ea\u7531\u5ea6\u7075\u5de7\u624b\u572880\u5206\u949f\u5185\u5b66\u4e60\u64cd\u63a7\u7acb\u65b9\u4f53\u81f3\u4e09\u4e2a\u76ee\u6807\u59ff\u6001\u3002", "conclusion": "GC-PMPC\u5728\u4f4e\u6210\u672c\u7075\u5de7\u624b\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u5b66\u4e60\u6548\u7387\u548c\u64cd\u63a7\u6027\u80fd\uff0c\u4e3a\u591a\u76ee\u6807\u7075\u5de7\u624b\u64cd\u63a7\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21586", "pdf": "https://arxiv.org/pdf/2504.21586", "abs": "https://arxiv.org/abs/2504.21586", "authors": ["Robin Ferede", "Till Blaha", "Erin Lucassen", "Christophe De Wagter", "Guido C. H. E. de Croon"], "title": "One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "In high-speed quadcopter racing, finding a single controller that works well\nacross different platforms remains challenging. This work presents the first\nneural network controller for drone racing that generalizes across physically\ndistinct quadcopters. We demonstrate that a single network, trained with domain\nrandomization, can robustly control various types of quadcopters. The network\nrelies solely on the current state to directly compute motor commands. The\neffectiveness of this generalized controller is validated through real-world\ntests on two substantially different crafts (3-inch and 5-inch race\nquadcopters). We further compare the performance of this generalized controller\nwith controllers specifically trained for the 3-inch and 5-inch drone, using\ntheir identified model parameters with varying levels of domain randomization\n(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower\nspeeds compared to the fine-tuned models, it excels in adaptability across\ndifferent platforms. Our results show that no randomization fails sim-to-real\ntransfer while increasing randomization improves robustness but reduces speed.\nDespite this trade-off, our findings highlight the potential of domain\nrandomization for generalizing controllers, paving the way for universal AI\ncontrollers that can adapt to any platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u901a\u7528\u65e0\u4eba\u673a\u7ade\u901f\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u57df\u968f\u673a\u5316\u8bad\u7ec3\uff0c\u80fd\u5728\u4e0d\u540c\u5c3a\u5bf8\u7684\u65e0\u4eba\u673a\u4e0a\u7a33\u5b9a\u8fd0\u884c\u3002", "motivation": "\u89e3\u51b3\u9ad8\u901f\u65e0\u4eba\u673a\u7ade\u901f\u4e2d\u5355\u4e00\u63a7\u5236\u5668\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u5e73\u53f0\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57df\u968f\u673a\u5316\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff0c\u4ec5\u4f9d\u8d56\u5f53\u524d\u72b6\u6001\u76f4\u63a5\u8ba1\u7b97\u7535\u673a\u6307\u4ee4\u3002", "result": "\u901a\u7528\u63a7\u5236\u5668\u57283\u82f1\u5bf8\u548c5\u82f1\u5bf8\u65e0\u4eba\u673a\u4e0a\u8868\u73b0\u7a33\u5065\uff0c\u867d\u901f\u5ea6\u7565\u4f4e\u4e8e\u4e13\u7528\u63a7\u5236\u5668\uff0c\u4f46\u9002\u5e94\u6027\u66f4\u5f3a\u3002", "conclusion": "\u57df\u968f\u673a\u5316\u80fd\u6709\u6548\u63d0\u5347\u63a7\u5236\u5668\u7684\u901a\u7528\u6027\uff0c\u4e3a\u901a\u7528AI\u63a7\u5236\u5668\u7684\u53d1\u5c55\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2504.21589", "pdf": "https://arxiv.org/pdf/2504.21589", "abs": "https://arxiv.org/abs/2504.21589", "authors": ["Lisa Kluge", "Maximilian K\u00e4hler"], "title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing", "categories": ["cs.CL", "cs.AI", "cs.DL", "I.2.7"], "comment": "11 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3aSemEval-2025\u4efb\u52a15\u5f00\u53d1\u7684\u7cfb\u7edf\uff0c\u5229\u7528LLM\u8fdb\u884c\u81ea\u52a8\u5316\u4e3b\u9898\u6807\u6ce8\uff0c\u7ed3\u5408\u5c11\u6837\u672c\u63d0\u793a\u548c\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u5728\u5b9a\u91cf\u6392\u540d\u4e2d\u4f4d\u5217\u7b2c\u56db\uff0c\u4f46\u5728\u4e13\u5bb6\u5b9a\u6027\u8bc4\u4ef7\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e3a\u6280\u672f\u56fe\u4e66\u9986\u7684\u5f00\u653e\u83b7\u53d6\u76ee\u5f55\u5f00\u53d1\u81ea\u52a8\u5316\u4e3b\u9898\u6807\u6ce8\u7cfb\u7edf\uff0c\u63d0\u5347\u6807\u6ce8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5c11\u6837\u672c\u63d0\u793a\u6280\u672f\uff0c\u7ed3\u5408\u591a\u6b65\u540e\u5904\u7406\uff08\u8bcd\u6c47\u6620\u5c04\u3001\u96c6\u6210\u6295\u7968\u548c\u76f8\u5173\u6027\u6392\u5e8f\uff09\u3002", "result": "\u7cfb\u7edf\u5728\u5b9a\u91cf\u6392\u540d\u4e2d\u7b2c\u56db\uff0c\u4f46\u5728\u4e13\u5bb6\u5b9a\u6027\u8bc4\u4ef7\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86LLM\u5728\u4e3b\u9898\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u4e13\u5bb6\u8bc4\u4ef7\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.21596", "pdf": "https://arxiv.org/pdf/2504.21596", "abs": "https://arxiv.org/abs/2504.21596", "authors": ["Huihui Guo", "Huilong Pi", "Yunchuan Qin", "Zhuo Tang", "Kenli Li"], "title": "Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "With the rapid advancement of artificial intelligence, there is an increasing\ndemand for intelligent robots capable of assisting humans in daily tasks and\nperforming complex operations. Such robots not only require task planning\ncapabilities but must also execute tasks with stability and robustness. In this\npaper, we present a closed-loop task planning and acting system, LLM-PAS, which\nis assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans\nlong-horizon tasks in a manner similar to traditional task and motion planners,\nit also emphasizes the execution phase of the task. By transferring part of the\nconstraint-checking process from the planning phase to the execution phase,\nLLM-PAS enables exploration of the constraint space and delivers more accurate\nfeedback on environmental anomalies during execution. The reasoning\ncapabilities of the LLM allow it to handle anomalies that cannot be addressed\nby the robust executor. To further enhance the system's ability to assist the\nplanner during replanning, we propose the First Look Prompting (FLP) method,\nwhich induces LLM to generate effective PDDL goals. Through comparative\nprompting experiments and systematic experiments, we demonstrate the\neffectiveness and robustness of LLM-PAS in handling anomalous conditions during\ntask execution.", "AI": {"tldr": "LLM-PAS\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u95ed\u73af\u4efb\u52a1\u89c4\u5212\u4e0e\u6267\u884c\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u90e8\u5206\u7ea6\u675f\u68c0\u67e5\u8f6c\u79fb\u5230\u6267\u884c\u9636\u6bb5\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740AI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u667a\u80fd\u673a\u5668\u4eba\u9700\u5177\u5907\u4efb\u52a1\u89c4\u5212\u548c\u7a33\u5b9a\u6267\u884c\u80fd\u529b\uff0cLLM-PAS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "LLM-PAS\u7ed3\u5408\u4f20\u7edf\u4efb\u52a1\u89c4\u5212\u4e0eLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51faFirst Look Prompting\uff08FLP\uff09\u65b9\u6cd5\u4f18\u5316PDDL\u76ee\u6807\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLM-PAS\u5728\u5f02\u5e38\u6761\u4ef6\u4e0b\u4efb\u52a1\u6267\u884c\u4e2d\u8868\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u3002", "conclusion": "LLM-PAS\u901a\u8fc7\u95ed\u73af\u8bbe\u8ba1\u548cLLM\u8f85\u52a9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6267\u884c\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2504.21605", "pdf": "https://arxiv.org/pdf/2504.21605", "abs": "https://arxiv.org/abs/2504.21605", "authors": ["Jonas Gwozdz", "Andreas Both"], "title": "RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRDF\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u77e5\u8bc6\u51b2\u7a81\u60c5\u51b5\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u77e5\u8bc6\u6cc4\u6f0f\u3001\u9519\u8bef\u68c0\u6d4b\u548c\u591a\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "motivation": "\u7531\u4e8eLLM\u4f5c\u4e3a\u77e5\u8bc6\u63a5\u53e3\u7684\u666e\u53ca\uff0c\u8bc4\u4f30\u5176\u5728\u51b2\u7a81\u4fe1\u606f\u4e0b\u7684\u53ef\u9760\u6027\u53d8\u5f97\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u56db\u79cd\u4e0a\u4e0b\u6587\u6761\u4ef6\uff08\u5b8c\u6574\u3001\u4e0d\u5b8c\u6574\u3001\u51b2\u7a81\u548c\u65e0\u4e0a\u4e0b\u6587\uff09\u5728\u5fb7\u8bed\u548c\u82f1\u8bed\u4e2d\u6355\u83b7\u6a21\u578b\u54cd\u5e94\uff0c\u5e76\u5229\u7528RDF\u7ed3\u6784\u5316\u8868\u793a\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6846\u67b6\u80fd\u5168\u9762\u5206\u6790\u77e5\u8bc6\u6cc4\u6f0f\u548c\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u4e14\u572828\u4e2a\u95ee\u9898\u7684\u7814\u7a76\u4e2d\u8db3\u591f\u8868\u8fbe\u6240\u6709\u8bc4\u4f30\u65b9\u9762\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63ed\u793a\u4e86LLM\u5728\u4e0a\u4e0b\u6587\u4f18\u5148\u7ea7\u548c\u8bed\u8a00\u6027\u80fd\u4e0a\u7684\u5173\u952e\u6a21\u5f0f\u3002"}}
{"id": "2504.21634", "pdf": "https://arxiv.org/pdf/2504.21634", "abs": "https://arxiv.org/abs/2504.21634", "authors": ["Chih-Cheng Rex Yuan", "Bow-Yaw Wang"], "title": "Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Fairness auditing of AI systems can identify and quantify biases. However,\ntraditional auditing using real-world data raises security and privacy\nconcerns. It exposes auditors to security risks as they become custodians of\nsensitive information and targets for cyberattacks. Privacy risks arise even\nwithout direct breaches, as data analyses can inadvertently expose confidential\ninformation. To address these, we propose a framework that leverages\ndifferentially private synthetic data to audit the fairness of AI systems. By\napplying privacy-preserving mechanisms, it generates synthetic data that\nmirrors the statistical properties of the original dataset while ensuring\nprivacy. This method balances the goal of rigorous fairness auditing and the\nneed for strong privacy protections. Through experiments on real datasets like\nAdult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real\ndata. By analyzing the alignment and discrepancies between these metrics, we\nassess the capacity of synthetic data to preserve the fairness properties of\nreal data. Our results demonstrate the framework's ability to enable meaningful\nfairness evaluations while safeguarding sensitive information, proving its\napplicability across critical and sensitive domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u5ba1\u8ba1AI\u7cfb\u7edf\u516c\u5e73\u6027\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5ba1\u8ba1\u4e2d\u7684\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5ba1\u8ba1\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u4f1a\u5f15\u53d1\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u6709\u6548\u8bc4\u4f30\u516c\u5e73\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u751f\u6210\u6280\u672f\uff0c\u4fdd\u7559\u539f\u59cb\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u9690\u79c1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u80fd\u6709\u6548\u4fdd\u7559\u771f\u5b9e\u6570\u636e\u7684\u516c\u5e73\u6027\u5c5e\u6027\uff0c\u9002\u7528\u4e8e\u654f\u611f\u9886\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9AI\u7cfb\u7edf\u516c\u5e73\u6027\u7684\u6709\u6548\u5ba1\u8ba1\u3002"}}
{"id": "2504.21635", "pdf": "https://arxiv.org/pdf/2504.21635", "abs": "https://arxiv.org/abs/2504.21635", "authors": ["Zeina Aldallal", "Sara Chrouf", "Khalil Hennara", "Mohamed Motaism Hamed", "Muhammad Hreden", "Safwan AlModhayan"], "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools.", "AI": {"tldr": "Sadeed\u662f\u4e00\u79cd\u57fa\u4e8eKuwain 1.5B\u5fae\u8c03\u7684\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u6807\u6ce8\u65b0\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6SadeedDiac-25\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u6807\u6ce8\u56e0\u5176\u5f62\u6001\u4e30\u5bcc\u6027\u4e00\u76f4\u662fNLP\u4e2d\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u516c\u5e73\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Kuwain 1.5B\u5fae\u8c03\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u548c\u4e25\u683c\u7684\u6570\u636e\u6e05\u6d17\u6d41\u7a0b\u3002", "result": "Sadeed\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u8bc4\u6d4b\u57fa\u51c6\u3002", "conclusion": "Sadeed\u548cSadeedDiac-25\u4e3a\u963f\u62c9\u4f2f\u8bedNLP\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2504.21685", "pdf": "https://arxiv.org/pdf/2504.21685", "abs": "https://arxiv.org/abs/2504.21685", "authors": ["Reem Abdel-Salam", "Mary Adewunmi"], "title": "Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Health Mention Classification (HMC) plays a critical role in leveraging\nsocial media posts for real-time tracking and public health monitoring.\nNevertheless, the process of HMC presents significant challenges due to its\nintricate nature, primarily stemming from the contextual aspects of health\nmentions, such as figurative language and descriptive terminology, rather than\nexplicitly reflecting a personal ailment. To address this problem, we argue\nthat clearer mentions can be achieved through conventional fine-tuning with\nenhanced parameters of biomedical natural language methods (NLP). In this\nstudy, we explore different techniques such as the utilisation of\npart-of-speech (POS) tagger information, improving on PEFT techniques, and\ndifferent combinations thereof. Extensive experiments are conducted on three\nwidely used datasets: RHDM, PHM, and Illness. The results incorporated POS\ntagger information, and leveraging PEFT techniques significantly improves\nperformance in terms of F1-score compared to state-of-the-art methods across\nall three datasets by utilising smaller models and efficient training.\nFurthermore, the findings highlight the effectiveness of incorporating POS\ntagger information and leveraging PEFT techniques for HMC. In conclusion, the\nproposed methodology presents a potentially effective approach to accurately\nclassifying health mentions in social media posts while optimising the model\nsize and training efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6539\u8fdb\u751f\u7269\u533b\u5b66NLP\u65b9\u6cd5\u7684\u53c2\u6570\u6765\u4f18\u5316\u5065\u5eb7\u63d0\u53ca\u5206\u7c7b\uff08HMC\uff09\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86POS\u6807\u8bb0\u4fe1\u606f\u548cPEFT\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5065\u5eb7\u63d0\u53ca\u5206\u7c7b\uff08HMC\uff09\u5728\u793e\u4ea4\u5a92\u4f53\u5b9e\u65f6\u8ffd\u8e2a\u548c\u516c\u5171\u536b\u751f\u76d1\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u590d\u6742\u6027\uff08\u5982\u6bd4\u55bb\u6027\u8bed\u8a00\u548c\u63cf\u8ff0\u6027\u672f\u8bed\uff09\uff0c\u5206\u7c7b\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684\u751f\u7269\u533b\u5b66NLP\u65b9\u6cd5\uff0c\u7ed3\u5408POS\u6807\u8bb0\u4fe1\u606f\u548cPEFT\u6280\u672f\uff0c\u5e76\u5728RHDM\u3001PHM\u548cIllness\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408POS\u6807\u8bb0\u4fe1\u606f\u548cPEFT\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86F1\u5206\u6570\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c0f\u7684\u6a21\u578b\u548c\u9ad8\u6548\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u5065\u5eb7\u63d0\u53ca\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2504.21695", "pdf": "https://arxiv.org/pdf/2504.21695", "abs": "https://arxiv.org/abs/2504.21695", "authors": ["Stavrow A. Bahnam", "Christophe De Wagter", "Guido C. H. E. de Croon"], "title": "Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Ego-motion estimation is vital for drones when flying in GPS-denied\nenvironments. Vision-based methods struggle when flight speed increases and\nclose-by objects lead to difficult visual conditions with considerable motion\nblur and large occlusions. To tackle this, vision is typically complemented by\nstate estimation filters that combine a drone model with inertial measurements.\nHowever, these drone models are currently learned in a supervised manner with\nground-truth data from external motion capture systems, limiting scalability to\ndifferent environments and drones. In this work, we propose a self-supervised\nlearning scheme to train a neural-network-based drone model using only onboard\nmonocular video and flight controller data (IMU and motor feedback). We achieve\nthis by first training a self-supervised relative pose estimation model, which\nthen serves as a teacher for the drone model. To allow this to work at high\nspeed close to obstacles, we propose an improved occlusion handling method for\ntraining self-supervised pose estimation models. Due to this method, the root\nmean squared error of resulting odometry estimates is reduced by an average of\n15%. Moreover, the student neural drone model can be successfully obtained from\nthe onboard data. It even becomes more accurate at higher speeds compared to\nits teacher, the self-supervised vision-based model. We demonstrate the value\nof the neural drone model by integrating it into a traditional filter-based VIO\nsystem (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing\ntrajectories near obstacles. Self-supervised learning of ego-motion estimation\nrepresents a significant step toward bridging the gap between flying in\ncontrolled, expensive lab environments and real-world drone applications. The\nfusion of vision and drone models will enable higher-speed flight and improve\nstate estimation, on any drone in any environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\uff0c\u5229\u7528\u5355\u76ee\u89c6\u9891\u548c\u98de\u884c\u63a7\u5236\u5668\u6570\u636e\u8bad\u7ec3\u65e0\u4eba\u673a\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u9ad8\u901f\u98de\u884c\u548c\u906e\u6321\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u8fd0\u52a8\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u89c6\u89c9\u65b9\u6cd5\u5728\u9ad8\u901f\u98de\u884c\u548c\u906e\u6321\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65e0\u4eba\u673a\u6a21\u578b\uff0c\u5e76\u6539\u8fdb\u4e86\u906e\u6321\u5904\u7406\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u7684\u5747\u65b9\u6839\u8bef\u5dee\u5e73\u5747\u964d\u4f4e15%\uff0c\u4e14\u65e0\u4eba\u673a\u6a21\u578b\u5728\u9ad8\u901f\u4e0b\u6bd4\u89c6\u89c9\u6a21\u578b\u66f4\u51c6\u786e\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u4e3a\u65e0\u4eba\u673a\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9ad8\u901f\u98de\u884c\u548c\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2504.21700", "pdf": "https://arxiv.org/pdf/2504.21700", "abs": "https://arxiv.org/abs/2504.21700", "authors": ["Marco Arazzi", "Vignesh Kumar Kembu", "Antonino Nocera", "Vinod P"], "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684LLM Jailbreaking\u653b\u51fb\u65b9\u6cd5XBreaking\uff0c\u901a\u8fc7\u5206\u6790\u5ba1\u67e5\u673a\u5236\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8bbe\u8ba1\u9488\u5bf9\u6027\u653b\u51fb\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5173\u952e\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\u963b\u788d\u4e86\u5176\u53ef\u9760\u91c7\u7528\uff0c\u73b0\u6709\u5ba1\u67e5\u673a\u5236\u53ef\u80fd\u88ab\u7ed5\u8fc7\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u5e76\u8bbe\u8ba1\u9488\u5bf9\u6027\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faXBreaking\u653b\u51fb\uff0c\u5229\u7528\u53ef\u89e3\u91caAI\u5206\u6790\u5ba1\u67e5\u4e0e\u672a\u5ba1\u67e5\u6a21\u578b\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u63d0\u53d6\u53ef\u653b\u51fb\u7684\u5bf9\u9f50\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5b9a\u5411\u566a\u58f0\u6ce8\u5165\u7a81\u7834\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eXBreaking\u80fd\u6709\u6548\u7ed5\u8fc7LLMs\u7684\u5ba1\u67e5\u673a\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u5ba1\u67e5\u673a\u5236\u7684\u65b0\u89c1\u89e3\u3002", "conclusion": "XBreaking\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9488\u5bf9\u6027\u7684LLM Jailbreaking\u653b\u51fb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5ba1\u67e5\u673a\u5236\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002"}}
{"id": "2504.21707", "pdf": "https://arxiv.org/pdf/2504.21707", "abs": "https://arxiv.org/abs/2504.21707", "authors": ["Anthony D Martin"], "title": "Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "We propose a generalization of modern representation learning objectives by\nreframing them as recursive divergence alignment processes over localized\nconditional distributions While recent frameworks like Information Contrastive\nLearning I-Con unify multiple learning paradigms through KL divergence between\nfixed neighborhood conditionals we argue this view underplays a crucial\nrecursive structure inherent in the learning process. We introduce Recursive KL\nDivergence Optimization RKDO a dynamic formalism where representation learning\nis framed as the evolution of KL divergences across data neighborhoods. This\nformulation captures contrastive clustering and dimensionality reduction\nmethods as static slices while offering a new path to model stability and local\nadaptation. Our experiments demonstrate that RKDO offers dual efficiency\nadvantages approximately 30 percent lower loss values compared to static\napproaches across three different datasets and 60 to 80 percent reduction in\ncomputational resources needed to achieve comparable results. This suggests\nthat RKDOs recursive updating mechanism provides a fundamentally more efficient\noptimization landscape for representation learning with significant\nimplications for resource constrained applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9012\u5f52KL\u6563\u5ea6\u4f18\u5316\uff08RKDO\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5c40\u90e8\u6761\u4ef6\u5206\u5e03\u7684KL\u6563\u5ea6\u6765\u6539\u8fdb\u8868\u793a\u5b66\u4e60\uff0c\u76f8\u6bd4\u9759\u6001\u65b9\u6cd5\u5728\u635f\u5931\u503c\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0a\u5747\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982I-Con\uff09\u901a\u8fc7\u56fa\u5b9a\u90bb\u57df\u6761\u4ef6\u5206\u5e03\u7684KL\u6563\u5ea6\u7edf\u4e00\u5b66\u4e60\u8303\u5f0f\uff0c\u4f46\u5ffd\u7565\u4e86\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u9012\u5f52\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u5f15\u5165RKDO\uff0c\u5c06\u8868\u793a\u5b66\u4e60\u5efa\u6a21\u4e3aKL\u6563\u5ea6\u5728\u6570\u636e\u90bb\u57df\u4e0a\u7684\u52a8\u6001\u6f14\u5316\u8fc7\u7a0b\uff0c\u6db5\u76d6\u5bf9\u6bd4\u5b66\u4e60\u3001\u805a\u7c7b\u548c\u964d\u7ef4\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRKDO\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u635f\u5931\u503c\u964d\u4f4e\u7ea630%\uff0c\u8ba1\u7b97\u8d44\u6e90\u51cf\u5c1160-80%\u3002", "conclusion": "RKDO\u7684\u9012\u5f52\u66f4\u65b0\u673a\u5236\u4e3a\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u8def\u5f84\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2504.21716", "pdf": "https://arxiv.org/pdf/2504.21716", "abs": "https://arxiv.org/abs/2504.21716", "authors": ["Marc Glocker", "Peter H\u00f6nig", "Matthias Hirschmanner", "Markus Vincze"], "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Accepted at Austrian Robotics Workshop 2025", "summary": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u4e3b\u7ba1\u7406\u5bb6\u5ead\u7269\u54c1\uff0c\u901a\u8fc7\u4efb\u52a1\u89c4\u5212\u3001\u8bb0\u5fc6\u589e\u5f3a\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u64cd\u4f5c\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5bb6\u5ead\u73af\u5883\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u7ba1\u7406\u7269\u54c1\u7684\u590d\u6742\u4efb\u52a1\u9700\u6c42\uff0c\u540c\u65f6\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u548c\u4efb\u52a1\u89c4\u5212\u63d0\u9ad8\u7cfb\u7edf\u7684\u957f\u671f\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u4e09\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\uff08\u8def\u7531\u3001\u4efb\u52a1\u89c4\u5212\u3001\u77e5\u8bc6\u5e93\uff09\uff0c\u7ed3\u5408RAG\u6280\u672f\u5b9e\u73b0\u4e0a\u4e0b\u6587\u68c0\u7d22\uff0c\u5e76\u5229\u7528Grounded SAM\u548cLLaMa3.2-Vision\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u4e0e\u573a\u666f\u7406\u89e3\u3002", "result": "\u5728\u4e09\u79cd\u5bb6\u5ead\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u9ad8\u4efb\u52a1\u89c4\u5212\u51c6\u786e\u6027\u548c\u8bb0\u5fc6\u53ec\u56de\u7387\u7684\u63d0\u5347\uff0cQwen2.5\u5728\u4e13\u7528\u667a\u80fd\u4f53\u4e2d\u8868\u73b0\u6700\u4f73\uff0cLLaMA3.1\u5728\u8def\u7531\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u67b6\u6784\u548cRAG\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bb6\u5ead\u7269\u54c1\u7ba1\u7406\u7684\u81ea\u4e3b\u6027\u548c\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\u3002"}}
{"id": "2504.21719", "pdf": "https://arxiv.org/pdf/2504.21719", "abs": "https://arxiv.org/abs/2504.21719", "authors": ["Fay\u00e7al A\u00eft Aoudia", "Jakob Hoydis", "Merlin Nimier-David", "Sebastian Cammerer", "Alexander Keller"], "title": "Sionna RT: Technical Report", "categories": ["cs.IT", "cs.AI", "eess.SP", "math.IT"], "comment": null, "summary": "Sionna is an open-source, GPU-accelerated library that, as of version 0.14,\nincorporates a ray tracer for simulating radio wave propagation. A unique\nfeature of Sionna RT is differentiability, enabling the calculation of\ngradients for the channel impulse responses (CIRs), radio maps, and other\nrelated metrics with respect to system and environmental parameters, such as\nmaterial properties, antenna patterns, and array geometries. The release of\nSionna 1.0 provides a complete overhaul of the ray tracer, significantly\nimproving its speed, memory efficiency, and extensibility. This document\ndetails the algorithms employed by Sionna RT to simulate radio wave propagation\nefficiently, while also addressing their current limitations. Given that the\ncomputation of CIRs and radio maps requires distinct algorithms, these are\ndetailed in separate sections. For CIRs, Sionna RT integrates shooting and\nbouncing of rays (SBR) with the image method and uses a hashing-based mechanism\nto efficiently eliminate duplicate paths. Radio maps are computed using a\npurely SBR-based approach.", "AI": {"tldr": "Sionna 1.0\u662f\u4e00\u4e2a\u5f00\u6e90\u3001GPU\u52a0\u901f\u7684\u5e93\uff0c\u65b0\u589e\u4e86\u53ef\u5fae\u5206\u7684\u5149\u7ebf\u8ffd\u8e2a\u529f\u80fd\uff0c\u7528\u4e8e\u9ad8\u6548\u6a21\u62df\u65e0\u7ebf\u7535\u6ce2\u4f20\u64ad\uff0c\u5e76\u6539\u8fdb\u4e86\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u5fae\u5206\u7684\u65b9\u6cd5\u6765\u6a21\u62df\u65e0\u7ebf\u7535\u6ce2\u4f20\u64ad\uff0c\u652f\u6301\u5bf9\u7cfb\u7edf\u53ca\u73af\u5883\u53c2\u6570\u7684\u68af\u5ea6\u8ba1\u7b97\u3002", "method": "\u7ed3\u5408\u4e86SBR\uff08\u5c04\u7ebf\u5f39\u8df3\u6cd5\uff09\u4e0e\u56fe\u50cf\u6cd5\u8ba1\u7b97CIR\uff08\u4fe1\u9053\u8109\u51b2\u54cd\u5e94\uff09\uff0c\u5e76\u4f7f\u7528\u54c8\u5e0c\u673a\u5236\u6d88\u9664\u91cd\u590d\u8def\u5f84\uff1b\u65e0\u7ebf\u7535\u5730\u56fe\u5219\u91c7\u7528\u7eafSBR\u65b9\u6cd5\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u5149\u7ebf\u8ffd\u8e2a\u7684\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\uff0c\u540c\u65f6\u652f\u6301\u5bf9CIR\u548c\u65e0\u7ebf\u7535\u5730\u56fe\u7684\u68af\u5ea6\u8ba1\u7b97\u3002", "conclusion": "Sionna RT\u4e3a\u65e0\u7ebf\u7535\u6ce2\u4f20\u64ad\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c3d\u7ba1\u5f53\u524d\u7b97\u6cd5\u4ecd\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\u3002"}}
{"id": "2504.21772", "pdf": "https://arxiv.org/pdf/2504.21772", "abs": "https://arxiv.org/abs/2504.21772", "authors": ["Minwoo Oh", "Minsu Park", "Eunil Park"], "title": "Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline", "categories": ["cs.MM", "cs.AI"], "comment": "will be presented in IJCAI 2025, 9 pages, 4 tables, 3 figures", "summary": "Short video platforms like YouTube Shorts and TikTok face significant\ncopyright compliance challenges, as infringers frequently embed arbitrary\nbackground music (BGM) to obscure original soundtracks (OST) and evade content\noriginality detection. To tackle this issue, we propose a novel pipeline that\nintegrates Music Source Separation (MSS) and cross-modal video-music retrieval\n(CMVMR). Our approach effectively separates arbitrary BGM from the original\nOST, enabling the restoration of authentic video audio tracks. To support this\nwork, we introduce two domain-specific datasets: OASD-20K for audio separation\nand OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips\nfeaturing mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset\ncomprising 1,121 video and mixed-audio pairs, specifically designed for short\nvideo restoration tasks. Experimental results demonstrate that our pipeline not\nonly removes arbitrary BGM with high accuracy but also restores OSTs, ensuring\ncontent integrity. This approach provides an ethical and scalable solution to\ncopyright challenges in user-generated content on short video platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u97f3\u4e50\u6e90\u5206\u79bb\u548c\u8de8\u6a21\u6001\u89c6\u9891-\u97f3\u4e50\u68c0\u7d22\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u77ed\u89c6\u9891\u5e73\u53f0\u4e2d\u80cc\u666f\u97f3\u4e50\u4fb5\u6743\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u4e2a\u4e13\u7528\u6570\u636e\u96c6\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u4e2d\uff0c\u4fb5\u6743\u8005\u5e38\u901a\u8fc7\u6dfb\u52a0\u80cc\u666f\u97f3\u4e50\u63a9\u76d6\u539f\u58f0\u4ee5\u9003\u907f\u539f\u521b\u68c0\u6d4b\uff0c\u5bfc\u81f4\u7248\u6743\u5408\u89c4\u95ee\u9898\u3002", "method": "\u6574\u5408\u97f3\u4e50\u6e90\u5206\u79bb\uff08MSS\uff09\u548c\u8de8\u6a21\u6001\u89c6\u9891-\u97f3\u4e50\u68c0\u7d22\uff08CMVMR\uff09\u7684\u7ba1\u9053\uff0c\u5206\u79bb\u80cc\u666f\u97f3\u4e50\u5e76\u6062\u590d\u539f\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u79fb\u9664\u80cc\u666f\u97f3\u4e50\u5e76\u6062\u590d\u539f\u58f0\uff0c\u786e\u4fdd\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u77ed\u89c6\u9891\u5e73\u53f0\u7528\u6237\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u95ee\u9898\u63d0\u4f9b\u4e86\u4f26\u7406\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.21773", "pdf": "https://arxiv.org/pdf/2504.21773", "abs": "https://arxiv.org/abs/2504.21773", "authors": ["Junsheng Huang", "Zhitao He", "Sandeep Polisetty", "Qingyun Wang", "May Fung"], "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5MAC-Tuning\uff0c\u7528\u4e8e\u5728\u591a\u95ee\u9898\u8bbe\u7f6e\u4e0b\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\u7684\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u79bb\u7b54\u6848\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u5b66\u4e60\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd525%\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u751f\u6210\u865a\u5047\u4e8b\u5b9e\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u95ee\u9898\u8bbe\u7f6e\u4e0b\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u800c\u591a\u95ee\u9898\u8bbe\u7f6e\u4e0bLLM\u5bf9\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\u7684\u611f\u77e5\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAC-Tuning\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6307\u4ee4\u6570\u636e\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5206\u79bb\u7b54\u6848\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u8fbe25%\u3002", "conclusion": "MAC-Tuning\u5728\u591a\u95ee\u9898\u8bbe\u7f6e\u4e0b\u6709\u6548\u63d0\u5347\u4e86LLM\u5bf9\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.21775", "pdf": "https://arxiv.org/pdf/2504.21775", "abs": "https://arxiv.org/abs/2504.21775", "authors": ["Rongguang Ye", "Ming Tang"], "title": "Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Recent methods leverage a hypernet to handle the performance-fairness\ntrade-offs in federated learning. This hypernet maps the clients' preferences\nbetween model performance and fairness to preference-specifc models on the\ntrade-off curve, known as local Pareto front. However, existing methods\ntypically adopt a uniform preference sampling distribution to train the\nhypernet across clients, neglecting the inherent heterogeneity of their local\nPareto fronts. Meanwhile, from the perspective of generalization, they do not\nconsider the gap between local and global Pareto fronts on the global dataset.\nTo address these limitations, we propose HetPFL to effectively learn both local\nand global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)\nand Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the\noptimal preference sampling distribution for each client to accommodate\nheterogeneous local Pareto fronts. While PHF performs preference-aware fusion\nof clients' hypernets to ensure the performance of the global Pareto front. We\nprove that HetPFL converges linearly with respect to the number of rounds,\nunder weaker assumptions than existing methods. Extensive experiments on four\ndatasets show that HetPFL significantly outperforms seven baselines in terms of\nthe quality of learned local and global Pareto fronts.", "AI": {"tldr": "HetPFL\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u504f\u597d\u91c7\u6837\u548c\u8d85\u7f51\u7edc\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6027\u80fd\u4e0e\u516c\u5e73\u6027\u6743\u8861\u7684\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u91c7\u7528\u7edf\u4e00\u7684\u504f\u597d\u91c7\u6837\u5206\u5e03\uff0c\u5ffd\u7565\u4e86\u5ba2\u6237\u7aef\u672c\u5730Pareto\u524d\u6cbf\u7684\u5f02\u8d28\u6027\uff0c\u4e14\u672a\u8003\u8651\u672c\u5730\u4e0e\u5168\u5c40Pareto\u524d\u6cbf\u7684\u5dee\u8ddd\u3002", "method": "HetPFL\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1aPSA\uff08\u81ea\u9002\u5e94\u786e\u5b9a\u5ba2\u6237\u7aef\u6700\u4f18\u504f\u597d\u91c7\u6837\u5206\u5e03\uff09\u548cPHF\uff08\u504f\u597d\u611f\u77e5\u7684\u8d85\u7f51\u7edc\u878d\u5408\uff09\u3002", "result": "HetPFL\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4e03\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u8bc1\u660e\u4e86\u5728\u8f83\u5f31\u5047\u8bbe\u4e0b\u7ebf\u6027\u6536\u655b\u3002", "conclusion": "HetPFL\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u548c\u878d\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6027\u80fd\u4e0e\u516c\u5e73\u6027\u6743\u8861\u7684\u5f02\u8d28\u6027\u95ee\u9898\u3002"}}
{"id": "2504.21776", "pdf": "https://arxiv.org/pdf/2504.21776", "abs": "https://arxiv.org/abs/2504.21776", "authors": ["Xiaoxi Li", "Jiajie Jin", "Guanting Dong", "Hongjin Qian", "Yutao Zhu", "Yongkang Wu", "Ji-Rong Wen", "Zhicheng Dou"], "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a \\textbf{Deep Web\nExplorer} module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\n\\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\n\\textbf{RL-based training strategy} via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.", "AI": {"tldr": "WebThinker\u662f\u4e00\u4e2a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u641c\u7d22\u548c\u6574\u5408\u7f51\u7edc\u4fe1\u606f\uff0c\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u5185\u90e8\u77e5\u8bc6\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u4efb\u52a1\u548c\u751f\u6210\u7efc\u5408\u7814\u7a76\u62a5\u544a\u3002", "method": "\u63d0\u51faWebThinker\uff0c\u7ed3\u5408\u6df1\u5ea6\u7f51\u7edc\u63a2\u7d22\u6a21\u5757\u548c\u81ea\u4e3b\u601d\u8003-\u641c\u7d22-\u8349\u62df\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u590d\u6742\u63a8\u7406\u57fa\u51c6\u548c\u79d1\u5b66\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "WebThinker\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\uff0c\u4e3a\u66f4\u5f3a\u5927\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2504.21798", "pdf": "https://arxiv.org/pdf/2504.21798", "abs": "https://arxiv.org/abs/2504.21798", "authors": ["John Yang", "Kilian Leret", "Carlos E. Jimenez", "Alexander Wettig", "Kabir Khandpur", "Yanzhe Zhang", "Binyuan Hui", "Ofir Press", "Ludwig Schmidt", "Diyi Yang"], "title": "SWE-smith: Scaling Data for Software Engineering Agents", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite recent progress in Language Models (LMs) for software engineering,\ncollecting training data remains a significant pain point. Existing datasets\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\nrepositories. The procedures to curate such datasets are often complex,\nnecessitating hundreds of hours of human labor; companion execution\nenvironments also take up several terabytes of storage, severely limiting their\nscalability and usability. To address this pain point, we introduce SWE-smith,\na novel pipeline for generating software engineering training data at scale.\nGiven any Python codebase, SWE-smith constructs a corresponding execution\nenvironment, then automatically synthesizes 100s to 1,000s of task instances\nthat break existing test(s) in the codebase. Using SWE-smith, we create a\ndataset of 50k instances sourced from 128 GitHub repositories, an order of\nmagnitude larger than all previous works. We train SWE-agent-LM-32B, achieving\n40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art\namong open source models. We open source SWE-smith (collection procedure, task\ninstances, trajectories, models) to lower the barrier of entry for research in\nLM systems for automated software engineering. All assets available at\nhttps://swesmith.com.", "AI": {"tldr": "SWE-smith\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u6210\u8f6f\u4ef6\u5de5\u7a0b\u8bad\u7ec3\u6570\u636e\u7684\u65b0\u7ba1\u9053\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5c0f\u4e14\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u8bad\u7ec3\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u6784\u5efa\u590d\u6742\u4e14\u5b58\u50a8\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u3002", "method": "SWE-smith\u901a\u8fc7\u4e3aPython\u4ee3\u7801\u5e93\u6784\u5efa\u6267\u884c\u73af\u5883\uff0c\u81ea\u52a8\u5408\u6210\u6570\u767e\u81f3\u6570\u5343\u4e2a\u7834\u574f\u73b0\u6709\u6d4b\u8bd5\u7684\u4efb\u52a1\u5b9e\u4f8b\u3002", "result": "\u751f\u6210\u4e8650k\u4e2a\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e86SWE-agent-LM-32B\u6a21\u578b\uff0c\u5728SWE-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523040.2%\u7684Pass@1\u89e3\u51b3\u7387\u3002", "conclusion": "SWE-smith\u53ca\u5176\u5f00\u6e90\u8d44\u6e90\u964d\u4f4e\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u7684\u95e8\u69db\u3002"}}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800", "abs": "https://arxiv.org/abs/2504.21800", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "11 pages, 5 tables, updated abstract and tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. In our dataset, synthetic dialogues match structural\nfeatures of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),\nhowever, synthetic interactions do not adequately reflect key fidelity markers\n(e.g., distress monitoring). We highlight gaps in existing evaluation\nframeworks and advocate for fidelity-aware metrics that go beyond surface\nfluency to uncover clinically significant failures. Our findings clarify where\nsynthetic data can effectively complement real-world datasets -- and where\ncritical limitations remain.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728PTSD\u6cbb\u7597\u4e2d\u4f7f\u7528\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5408\u6210\u6570\u636e\u5728\u7ed3\u6784\u4e0a\u63a5\u8fd1\u771f\u5b9e\u5bf9\u8bdd\uff0c\u4f46\u5728\u4e34\u5e8a\u5173\u952e\u6307\u6807\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "motivation": "\u9690\u79c1\u95ee\u9898\u3001\u771f\u5b9e\u6570\u636e\u83b7\u53d6\u56f0\u96be\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u63a8\u52a8\u4e86\u5408\u6210\u6570\u636e\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5408\u6210\u6570\u636e\u5728PTSD\u6cbb\u7597\u5bf9\u8bdd\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u8bed\u8a00\u3001\u7ed3\u6784\u548c\u534f\u8bae\u7279\u5b9a\u6307\u6807\uff08\u5982\u5bf9\u8bdd\u8f6e\u6362\u6a21\u5f0f\u548c\u6cbb\u7597\u5fe0\u5b9e\u5ea6\uff09\u7cfb\u7edf\u6bd4\u8f83\u771f\u5b9e\u4e0e\u5408\u6210\u5bf9\u8bdd\uff0c\u5e76\u5f15\u5165PE\u7279\u5b9a\u6307\u6807\u3002", "result": "\u5408\u6210\u6570\u636e\u5728\u7ed3\u6784\u7279\u5f81\u4e0a\u4e0e\u771f\u5b9e\u6570\u636e\u63a5\u8fd1\uff08\u5982\u8bf4\u8bdd\u8005\u5207\u6362\u6bd4\u4f8b0.98 vs. 0.99\uff09\uff0c\u4f46\u5728\u5173\u952e\u4e34\u5e8a\u6307\u6807\uff08\u5982\u75db\u82e6\u76d1\u6d4b\uff09\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u5728\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u548c\u4fdd\u62a4\u9690\u79c1\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u5f00\u53d1\u8d85\u8d8a\u8868\u9762\u6d41\u7545\u6027\u7684\u5fe0\u5b9e\u5ea6\u611f\u77e5\u6307\u6807\u4ee5\u5f25\u8865\u4e34\u5e8a\u52a8\u6001\u6355\u6349\u7684\u4e0d\u8db3\u3002"}}
{"id": "2504.21801", "pdf": "https://arxiv.org/pdf/2504.21801", "abs": "https://arxiv.org/abs/2504.21801", "authors": ["Z. Z. Ren", "Zhihong Shao", "Junxiao Song", "Huajian Xin", "Haocheng Wang", "Wanjia Zhao", "Liyue Zhang", "Zhe Fu", "Qihao Zhu", "Dejian Yang", "Z. F. Wu", "Zhibin Gou", "Shirong Ma", "Hongxuan Tang", "Yuxuan Liu", "Wenjun Gao", "Daya Guo", "Chong Ruan"], "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce DeepSeek-Prover-V2, an open-source large language model designed\nfor formal theorem proving in Lean 4, with initialization data collected\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\nstep-by-step reasoning, to create an initial cold start for reinforcement\nlearning. This process enables us to integrate both informal and formal\nmathematical reasoning into a unified model. The resulting model,\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\nevaluation, including 15 selected problems from the recent AIME competitions\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\nthese problems using majority voting, highlighting that the gap between formal\nand informal mathematical reasoning in large language models is substantially\nnarrowing.", "AI": {"tldr": "DeepSeek-Prover-V2\u662f\u4e00\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3aLean 4\u4e2d\u7684\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u8bbe\u8ba1\uff0c\u901a\u8fc7\u9012\u5f52\u5b9a\u7406\u8bc1\u660e\u6d41\u7a0b\u521d\u59cb\u5316\uff0c\u7ed3\u5408\u975e\u6b63\u5f0f\u548c\u6b63\u5f0f\u6570\u5b66\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u975e\u6b63\u5f0f\u548c\u6b63\u5f0f\u6570\u5b66\u63a8\u7406\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u6027\u80fd\uff0c\u7f29\u5c0f\u5f62\u5f0f\u4e0e\u975e\u5f62\u5f0f\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5229\u7528DeepSeek-V3\u5206\u89e3\u590d\u6742\u95ee\u9898\u4e3a\u5b50\u76ee\u6807\uff0c\u751f\u6210\u94fe\u5f0f\u63a8\u7406\u8fc7\u7a0b\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u51b7\u542f\u52a8\u6570\u636e\uff0c\u8bad\u7ec3\u7edf\u4e00\u6a21\u578b\u3002", "result": "\u5728MiniF2F-test\u4e0a\u8fbe\u523088.9%\u901a\u8fc7\u7387\uff0c\u89e3\u51b3PutnamBench\u4e2d49/658\u95ee\u9898\uff0c\u5e76\u5728ProverBench\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "DeepSeek-Prover-V2\u5728\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7f29\u5c0f\u4e86\u5f62\u5f0f\u4e0e\u975e\u5f62\u5f0f\u63a8\u7406\u7684\u5dee\u8ddd\uff0c\u5c55\u793a\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.21848", "pdf": "https://arxiv.org/pdf/2504.21848", "abs": "https://arxiv.org/abs/2504.21848", "authors": ["Atoosa Kasirzadeh", "Iason Gabriel"], "title": "Characterizing AI Agents for Alignment and Governance", "categories": ["cs.CY", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "The creation of effective governance mechanisms for AI agents requires a\ndeeper understanding of their core properties and how these properties relate\nto questions surrounding the deployment and operation of agents in the world.\nThis paper provides a characterization of AI agents that focuses on four\ndimensions: autonomy, efficacy, goal complexity, and generality. We propose\ndifferent gradations for each dimension, and argue that each dimension raises\nunique questions about the design, operation, and governance of these systems.\nMoreover, we draw upon this framework to construct \"agentic profiles\" for\ndifferent kinds of AI agents. These profiles help to illuminate cross-cutting\ntechnical and non-technical governance challenges posed by different classes of\nAI agents, ranging from narrow task-specific assistants to highly autonomous\ngeneral-purpose systems. By mapping out key axes of variation and continuity,\nthis framework provides developers, policymakers, and members of the public\nwith the opportunity to develop governance approaches that better align with\ncollective societal goals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAI\u4ee3\u7406\u7684\u56db\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff08\u81ea\u4e3b\u6027\u3001\u6548\u80fd\u3001\u76ee\u6807\u590d\u6742\u6027\u548c\u901a\u7528\u6027\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u201c\u4ee3\u7406\u6027\u6863\u6848\u201d\u4ee5\u5e2e\u52a9\u89e3\u51b3\u4e0d\u540cAI\u4ee3\u7406\u7684\u6cbb\u7406\u6311\u6218\u3002", "motivation": "\u7406\u89e3AI\u4ee3\u7406\u7684\u6838\u5fc3\u5c5e\u6027\u53ca\u5176\u4e0e\u6cbb\u7406\u95ee\u9898\u7684\u5173\u7cfb\uff0c\u4ee5\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u6cbb\u7406\u673a\u5236\u3002", "method": "\u63d0\u51fa\u56db\u4e2a\u7ef4\u5ea6\u7684\u5206\u7ea7\u6807\u51c6\uff0c\u5e76\u6784\u5efa\u201c\u4ee3\u7406\u6027\u6863\u6848\u201d\u4ee5\u5206\u7c7bAI\u4ee3\u7406\u3002", "result": "\u6846\u67b6\u4e3a\u5f00\u53d1\u8005\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u516c\u4f17\u63d0\u4f9b\u4e86\u6cbb\u7406AI\u4ee3\u7406\u7684\u5de5\u5177\uff0c\u4ee5\u66f4\u597d\u5730\u5b9e\u73b0\u793e\u4f1a\u76ee\u6807\u3002", "conclusion": "\u901a\u8fc7\u660e\u786eAI\u4ee3\u7406\u7684\u5dee\u5f02\u548c\u5171\u6027\uff0c\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u7b26\u5408\u793e\u4f1a\u76ee\u6807\u7684\u6cbb\u7406\u65b9\u6cd5\u3002"}}
{"id": "2504.21849", "pdf": "https://arxiv.org/pdf/2504.21849", "abs": "https://arxiv.org/abs/2504.21849", "authors": ["Justin B. Bullock", "Janet V. T. Pauketat", "Hsini Huang", "Yi-Fan Wang", "Jacy Reese Anthis"], "title": "Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure, 5 tables, accepted to Public Performance and\n  Management Review", "summary": "Governance institutions must respond to societal risks, including those posed\nby generative AI. This study empirically examines how public trust in\ninstitutions and AI technologies, along with perceived risks, shape preferences\nfor AI regulation. Using the nationally representative 2023 Artificial\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\ngovernment, AI companies, and AI technologies, as well as public support for\nregulatory measures such as slowing AI development or outright bans on advanced\nAI. Our findings reveal broad public support for AI regulation, with risk\nperception playing a significant role in shaping policy preferences.\nIndividuals with higher trust in government favor regulation, while those with\ngreater trust in AI companies and AI technologies are less inclined to support\nrestrictions. Trust in government and perceived risks significantly predict\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\nAI systems) regulatory interventions. These results highlight the importance of\npublic opinion in AI governance. As AI capabilities advance, effective\nregulation will require balancing public concerns about risks with trust in\ninstitutions. This study provides a foundational empirical baseline for\npolicymakers navigating AI governance and underscores the need for further\nresearch into public trust, risk perception, and regulatory strategies in the\nevolving AI landscape.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u516c\u4f17\u5bf9AI\u76d1\u7ba1\u7684\u504f\u597d\uff0c\u53d1\u73b0\u98ce\u9669\u611f\u77e5\u548c\u4fe1\u4efb\uff08\u653f\u5e9c\u3001AI\u516c\u53f8\u53ca\u6280\u672f\uff09\u662f\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76AI\u6cbb\u7406\u4e2d\u516c\u4f17\u4fe1\u4efb\u4e0e\u98ce\u9669\u611f\u77e5\u5982\u4f55\u5f71\u54cd\u76d1\u7ba1\u504f\u597d\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u4f7f\u75282023\u5e74AIMS\u5168\u56fd\u4ee3\u8868\u6027\u8c03\u67e5\u6570\u636e\uff0c\u5206\u6790\u516c\u4f17\u5bf9\u653f\u5e9c\u3001AI\u516c\u53f8\u53ca\u6280\u672f\u7684\u4fe1\u4efb\u4e0e\u76d1\u7ba1\u652f\u6301\u3002", "result": "\u516c\u4f17\u666e\u904d\u652f\u6301AI\u76d1\u7ba1\uff0c\u98ce\u9669\u611f\u77e5\u663e\u8457\u5f71\u54cd\u504f\u597d\uff1b\u4fe1\u4efb\u653f\u5e9c\u8005\u503e\u5411\u76d1\u7ba1\uff0c\u4fe1\u4efbAI\u516c\u53f8\u6216\u6280\u672f\u8005\u53cd\u4e4b\u3002", "conclusion": "AI\u6cbb\u7406\u9700\u5e73\u8861\u516c\u4f17\u98ce\u9669\u62c5\u5fe7\u4e0e\u673a\u6784\u4fe1\u4efb\uff0c\u7814\u7a76\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2504.21851", "pdf": "https://arxiv.org/pdf/2504.21851", "abs": "https://arxiv.org/abs/2504.21851", "authors": ["Sichang Tu", "Abigail Powers", "Stephen Doogan", "Jinho D. Choi"], "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments", "categories": ["cs.CL", "cs.AI"], "comment": "5 figures, 4 tables", "summary": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability.", "AI": {"tldr": "TRUST\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u4e34\u5e8a\u8bca\u65ad\u8bbf\u8c08\u63d0\u5347\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u7684\u53ef\u53ca\u6027\u3002", "motivation": "\u586b\u8865\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u4e2d\u7f3a\u4e4f\u6807\u51c6\u8bca\u65ad\u8bbf\u8c08\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1TRUST\u6846\u67b6\uff0c\u7ed3\u5408LLM\u6a21\u5757\u548c\u4e34\u5e8a\u8bbf\u8c08\u4e13\u7528\u7684Dialogue Acts\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u60a3\u8005\u6a21\u62df\u65b9\u6cd5\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u663e\u793aTRUST\u8868\u73b0\u63a5\u8fd1\u771f\u5b9e\u4e34\u5e8a\u8bbf\u8c08\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "TRUST\u6846\u67b6\u6709\u671b\u63d0\u5347\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u7684\u53ef\u53ca\u6027\u3002"}}
