[[toc]]

## cs.CV

### [1] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 综述了利用大语言模型（LLMs）从视频数据中检测碰撞的最新方法，包括融合策略分类、数据集总结、模型架构分析和性能比较。

- Motivation: 智能交通系统中视频碰撞检测是关键问题，LLMs和视觉语言模型（VLMs）的发展为多模态信息处理提供了新思路。
- Method: 通过结构化分类法总结融合策略，分析模型架构，并比较性能基准。
- Result: 提供了视频理解与基础模型交叉领域的研究基础。
- Conclusion: 为未来研究提供了方向，并讨论了当前挑战与机遇。


### [2] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

TL;DR: 论文评估了单目深度估计模型在水下环境中的表现，发现现有模型因域偏移表现不佳，并通过微调改进性能。

- Motivation: 水下环境中的光衰减、散射和缺乏高质量数据限制了单目深度估计的可靠性，需研究其适应性和改进方法。
- Method: 在真实水下数据集（如FLSea和SQUID）上评估零样本和微调模型，并基于合成水下数据集微调Depth Anything V2模型。
- Result: 微调后的模型在所有基准测试中表现优于仅基于陆地数据训练的基线模型。
- Conclusion: 研究强调了域适应和尺度感知监督对水下环境中鲁棒深度估计的重要性，为未来研究提供了参考。


### [3] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出了一种基于事件流的场景文本识别框架ESTR-CoT，通过链式思维推理增强可解释性和上下文逻辑推理能力。

- Motivation: 现有方法在低光照和快速运动等极端场景下表现有限，且缺乏可解释性和逻辑推理能力。
- Method: 采用EVA-CLIP视觉编码器和Llama分词器处理事件流，结合Vicuna-7B大语言模型输出答案和推理过程，并通过三阶段处理生成CoT数据集。
- Result: 在三个事件流STR基准数据集上验证了框架的有效性和可解释性。
- Conclusion: ESTR-CoT在极端场景下表现优异，且提供了可解释的推理过程，为后续推理模型发展奠定基础。


### [4] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

TL;DR: 提出了一种零样本多模态方法用于复合表情识别（CER），结合六种模态并通过动态加权和概率聚合生成可解释的输出，性能接近监督方法。

- Motivation: 解决传统CER方法依赖任务特定训练数据的问题，探索零样本多模态方法以捕捉复杂情感状态。
- Method: 结合六种模态（静态/动态面部表情、场景/标签匹配、场景上下文、音频、文本），使用CLIP和Qwen-VL进行零样本处理，引入MHPF模块动态加权，并通过PPA和PFSA生成输出。
- Result: 在AffWild2、AFEW和C-EXPR-DB上的F1分数分别为46.95%、49.02%和34.85%，性能接近监督方法。
- Conclusion: 提出的零样本方法无需领域适应即可有效捕捉复合情感，代码已公开。


### [5] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 论文介绍了SciGA-145k数据集，支持图形摘要（GA）的选择和推荐，并提出了两个任务和新的评估指标CAR。

- Motivation: 图形摘要在科学传播中的潜力未被充分探索，且设计GA需要高级可视化技能，阻碍了其广泛应用。
- Method: 引入SciGA-145k数据集，定义两个任务（Intra-GA和Inter-GA推荐），并提出新评估指标CAR。
- Result: 提供了基线模型，并通过CAR改进了传统排名指标。
- Conclusion: SciGA-145k为视觉科学传播和AI研究奠定了基础。


### [6] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

TL;DR: 论文研究了在少量图像数据下学习鲁棒物体检测器的挑战，比较了两种合成数据生成策略（基于提示和基于布局），发现布局条件在多样性高时更优，显著提升检测性能。

- Motivation: 工业视觉系统中高质量训练数据收集耗时，合成数据成为关键解决方案，但现有方法生成速度慢且仿真与真实差距大。扩散模型虽高效，但精确控制困难。
- Method: 研究80种视觉概念，比较基于提示和基于布局的两种条件策略，分析其对合成数据质量的影响。
- Result: 布局条件在多样性高时表现更优，匹配训练分布时，合成数据平均提升34%的mAP，最高提升177%。
- Conclusion: 布局条件策略在多样性高时能显著提升合成数据质量，为数据高效视觉检测提供新方向。


### [7] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

TL;DR: DIDB-ViT是一种新型二值化视觉Transformer，通过引入差分信息和频率分解减少信息损失，提升性能。

- Motivation: 解决现有二值化ViT方法性能下降严重或依赖全精度模块的问题。
- Method: 设计含差分信息的注意力模块、频率分解（离散Haar小波）和改进的RPReLU激活函数。
- Result: 在多种ViT架构中显著优于现有网络量化方法，图像分类和分割性能优越。
- Conclusion: DIDB-ViT在保持计算效率的同时，显著提升了二值化ViT的性能。


### [8] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

TL;DR: FMOcc是一种基于流匹配选择性状态空间模型的TPV细化占用网络，用于少帧3D占用预测，显著提升了预测精度和效率。

- Motivation: 解决少帧图像和3D空间冗余导致的遮挡和远距离场景预测精度问题，同时避免传统方法需要额外数据和计算资源的缺点。
- Method: 设计流匹配SSM模块（FMSSM）生成缺失特征；引入TPV SSM层和平面选择性SSM（PS3M）选择性过滤TPV特征；采用掩码训练（MT）增强鲁棒性。
- Result: 在Occ3D-nuScenes和OpenOcc数据集上表现优异，两帧输入下分别达到43.1% RayIoU和39.8% mIoU，以及42.6% RayIoU。
- Conclusion: FMOcc通过流匹配和选择性特征过滤，显著提升了少帧3D占用预测的性能和效率。


### [9] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

TL;DR: SurgVisAgent是一种基于多模态大语言模型（MLLMs）的智能手术视觉代理，能够动态识别内窥镜图像的失真类别和严重程度，执行多种增强任务，优于传统单任务模型。

- Motivation: 现有手术增强算法通常针对单一任务设计，无法满足复杂实际需求。
- Method: 提出SurgVisAgent，结合领域先验模型、上下文少样本学习和链式推理，实现定制化图像增强。
- Result: 在模拟真实手术失真的基准测试中，SurgVisAgent表现优于传统单任务模型。
- Conclusion: SurgVisAgent有望成为手术辅助的统一解决方案。


### [10] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 本文提出了一种基于多标签分类的飓风损害评估框架，结合ResNet和类特定注意力机制，显著提升了评估精度。

- Motivation: 传统单标签分类方法无法全面捕捉飓风后的复杂损害情况，亟需更高效的评估方法以支持灾害响应。
- Method: 采用ResNet特征提取模块和类特定注意力机制，实现单张图像中多类型损害的识别。
- Result: 在Rescuenet数据集上，平均精度达到90.23%，优于现有基线方法。
- Conclusion: 该框架提升了飓风损害评估的效率和准确性，为灾害响应和减灾策略提供了支持。


### [11] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: 提出了一种双向域适应（BiDA）框架，用于跨域高光谱图像分类，通过提取域不变特征和域特定信息提升分类性能。

- Motivation: 解决不同场景下相同类别的光谱偏移问题，提升跨域高光谱图像分类的适应性和可分离性。
- Method: 设计了三分支变压器架构（源分支、目标分支和耦合分支），结合耦合多头交叉注意力机制和双向蒸馏损失，以及自适应强化策略。
- Result: 在跨时空数据集上表现优于现有方法，树种类分类任务中性能提升3%~5%。
- Conclusion: BiDA框架有效提升了跨域高光谱图像分类的性能，具有实际应用潜力。


### [12] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出了一种名为MAC-Lookup的模型，通过改进颜色准确性、清晰度和对比度来增强水下图像质量。

- Motivation: 水下图像因光线变化、水体浑浊和气泡等问题导致可见性和颜色失真，传统方法和深度学习均存在不足。
- Method: 结合了条件3D查找表颜色校正（CLTCC）和多轴自适应增强（MAAE）技术，分别用于初步颜色校正和细节优化。
- Result: 实验表明，MAC-Lookup在恢复细节和颜色方面优于现有方法。
- Conclusion: MAC-Lookup能有效提升水下图像质量，避免过增强和饱和问题。


### [13] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 论文提出了一种自蒸馏方法，用于改进视频到音频（V2A）生成模型在电影语言场景中的表现，尤其是在部分可见目标的情况下。

- Motivation: 当前V2A方法忽视了电影语言对艺术表达的重要性，导致在部分可见目标场景中性能下降。
- Method: 通过自蒸馏方法，模拟电影语言变化，使学生模型学习对齐视频特征与音频-视觉对应关系。
- Result: 方法在部分可见性场景中显著提升性能，并在VGGSound数据集上表现更优。
- Conclusion: 自蒸馏方法有效解决了V2A模型在电影语言场景中的局限性，提升了性能。


### [14] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

TL;DR: LaCo是一种新型视觉令牌压缩框架，通过在视觉编码器的中间层进行压缩，显著提升了效率和性能。

- Motivation: 现有视觉令牌压缩方法多为后编码器模块，限制了效率提升潜力。
- Method: LaCo采用层间像素重组机制和残差学习架构，实现高效令牌压缩。
- Result: LaCo在中间层压缩中优于现有方法，训练效率提升20%，推理吞吐量提升15%。
- Conclusion: LaCo在保持性能的同时显著提升了效率，为MLLMs提供了一种更优的视觉令牌压缩方案。


### [15] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出了一种基于文本特征引导的视觉提示调优框架，结合语言模型和视觉提示，提升域泛化能力。

- Motivation: 解决现有视觉基础模型（VFMs）在域泛化（DG）中难以解耦跨域不变特征的问题。
- Method: 利用语言提示解耦文本特征，并通过文本特征引导视觉表示学习；引入WERA方法增强源域多样性。
- Result: 在多个DG数据集上表现优于现有方法。
- Conclusion: 结合语言和视觉提示的方法有效提升了域泛化性能。


### [16] [ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation](https://arxiv.org/abs/2507.02294)
*Hanbo Bi,Yulong Xu,Ya Li,Yongqiang Mao,Boyuan Tong,Chongyang Li,Chunbo Lang,Wenhui Diao,Hongqi Wang,Yingchao Feng,Xian Sun*

Main category: cs.CV

TL;DR: ViRefSAM通过少量标注参考图像自动生成提示，解决了SAM在遥感图像分割中的手动提示和领域适应性问题。

- Motivation: SAM在遥感图像分割中面临手动提示效率低和领域适应性差的问题。
- Method: ViRefSAM引入视觉上下文提示编码器和动态目标对齐适配器，无需手动提示，自动生成类一致分割。
- Result: 在三个少样本分割基准测试中，ViRefSAM表现优于现有方法，实现高精度自动分割。
- Conclusion: ViRefSAM通过结合参考图像的语义信息，显著提升了SAM在遥感图像分割中的性能。


### [17] [DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation](https://arxiv.org/abs/2507.02299)
*Yunhan Yang,Shuo Chen,Yukun Huang,Xiaoyang Wu,Yuan-Chen Guo,Edmund Y. Lam,Hengshuang Zhao,Tong He,Xihui Liu*

Main category: cs.CV

TL;DR: DreamComposer++ 是一个改进多视角条件控制的框架，通过提取和融合多视角3D表示，提升现有视图感知扩散模型的可控性。

- Motivation: 现有方法在生成可控新视角时因缺乏多视角信息而受限，DreamComposer++ 旨在通过多视角条件提升模型性能。
- Method: 利用视图感知3D提升模块提取多视角3D表示，通过多视角特征融合模块聚合并渲染到目标视角的潜在特征中，再集成到预训练扩散模型中进行新视角合成。
- Result: 实验表明，DreamComposer++ 能无缝集成现有模型，显著提升多视角条件下的可控新视角生成能力。
- Conclusion: DreamComposer++ 为可控3D对象重建提供了新方法，拓展了应用范围。


### [18] [Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images](https://arxiv.org/abs/2507.02307)
*Haoxuan Li,Chenxu Wei,Haodong Wang,Xiaomeng Hu,Boyuan An,Lingyan Ran,Baosen Zhang,Jin Jin,Omirzhan Taukebayev,Amirkhan Temirbayev,Junrui Liu,Xiuwei Zhang*

Main category: cs.CV

TL;DR: 提出Flow-CDNet网络，结合光流和变化检测分支，同时检测慢速和快速变化，并在自建数据集上验证其优越性。

- Motivation: 现实场景中，慢速变化常是重大灾害的前兆，需同时检测慢速和快速变化。
- Method: 设计双分支网络（光流分支和变化检测分支），结合金字塔结构和ResNet，并引入新损失函数和评估指标FEPE。
- Result: 在自建数据集Flow-Change上表现优于现有方法，分支间相互促进提升性能。
- Conclusion: Flow-CDNet有效解决了同时检测慢速和快速变化的挑战，并通过实验验证了其优越性。


### [19] [LMPNet for Weakly-supervised Keypoint Discovery](https://arxiv.org/abs/2507.02308)
*Pei Guo,Ryan Farrell*

Main category: cs.CV

TL;DR: 论文提出了一种弱监督的语义关键点发现方法LMPNet，通过改进的LMP层和选择策略，自动发现鲁棒的关键点。

- Motivation: 探索仅通过类别标签弱监督的语义关键点发现任务，避免依赖手工设计的损失项。
- Method: 使用LMP层鼓励卷积层学习非重复局部模式，结合选择策略和掩码注意力机制，最后通过可学习聚类层预测关键点。
- Result: LMPNet能自动发现鲁棒的关键点，预测精度接近有监督的姿态估计模型。
- Conclusion: LMPNet是一种高效且可解释的弱监督关键点发现方法。


### [20] [Perception Activator: An intuitive and portable framework for brain cognitive exploration](https://arxiv.org/abs/2507.02311)
*Le Xu,Qi Zhang,Qixian Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种利用fMRI信号增强视觉解码的方法，通过跨注意力机制将fMRI信息注入多尺度图像特征，提高了下游任务的性能。

- Motivation: 现有方法在解码大脑信号时过于依赖像素对齐，缺乏细粒度的语义对齐，导致重建失真。研究旨在探索大脑视觉感知模式及解码模型对语义对象的处理方式。
- Method: 开发了一个实验框架，将fMRI表征作为干预条件，通过跨注意力机制将其注入多尺度图像特征，对比有无fMRI信息的下游任务表现。
- Result: 实验表明，引入fMRI信号提升了检测和分割的准确性，证实fMRI包含丰富的多对象语义线索和粗略空间定位信息。
- Conclusion: fMRI信号具有未被充分利用的语义和空间信息，未来解码模型可进一步整合这些信息以提升性能。


### [21] [MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation](https://arxiv.org/abs/2507.02314)
*JaeHyuck Choi,MinJun Kim,JeHyeong Hong*

Main category: cs.CV

TL;DR: MAGIC是一种基于扩散模型的少样本异常生成方法，通过多级扰动和上下文感知对齐解决现有方法的不足，满足工业质量控制中对异常数据生成的需求。

- Motivation: 工业质量控制中异常数据稀缺，现有扩散模型方法无法同时满足保持背景完整、精确填充异常区域和生成语义合理且多样化的异常。
- Method: MAGIC基于Stable Diffusion修复模型，通过高斯提示级扰动和掩码引导的空间噪声注入增强多样性，并引入上下文感知掩码对齐模块确保语义合理性。
- Result: 在MVTec-AD数据集上，MAGIC在下游异常任务中优于现有方法。
- Conclusion: MAGIC通过多级扰动和上下文感知对齐，解决了少样本异常生成的核心问题，为工业应用提供了高效解决方案。


### [22] [Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos](https://arxiv.org/abs/2507.02316)
*Zecheng Zhao,Selena Song,Tong Chen,Zhi Chen,Shazia Sadiq,Yadan Luo*

Main category: cs.CV

TL;DR: SynTVA是一个新的数据集和基准，用于评估合成视频在文本到视频检索任务中的实用性，通过四个语义对齐维度标注视频-文本对，并探索了自动评估方法。

- Motivation: 当前文本到视频合成的评估指标主要关注视觉质量和时间一致性，缺乏对下游任务（如文本到视频检索）性能的评估，因此需要新的评估方法。
- Method: 基于800个用户查询生成合成视频，标注视频-文本对的四个语义对齐维度，并开发自动评估器预测对齐质量。
- Result: SynTVA不仅作为基准，还能用于数据集增强，显著提升文本到视频检索性能。
- Conclusion: SynTVA为合成视频的实用性和数据集增强提供了有价值的工具。


### [23] [Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback](https://arxiv.org/abs/2507.02321)
*Nina Konovalova,Maxim Nikolaev,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: InnerControl提出了一种新的训练策略，通过在扩散模型的所有步骤中强制空间一致性，提升了生成图像的空间控制精度。

- Motivation: 现有方法（如ControlNet++）仅关注最终去噪步骤的对齐，忽略了中间生成阶段，限制了效果。
- Method: 通过训练轻量级卷积探针，从中间UNet特征重建输入控制信号（如边缘、深度），并在整个扩散过程中最小化预测与目标条件的差异。
- Result: InnerControl结合ControlNet++等技术，在多种条件方法（如边缘、深度）上实现了最先进的性能。
- Conclusion: InnerControl通过全步骤的空间一致性优化，显著提升了控制忠实度和生成质量。


### [24] [Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model](https://arxiv.org/abs/2507.02322)
*Farida Siddiqi Prity,Mirza Raquib,Saydul Akbar Murad,Md. Jubayar Alam Rafi,Md. Khairul Bashar Bhuiyan,Anupam Kumar Bairagi*

Main category: cs.CV

TL;DR: 该研究提出了一种基于人工神经网络的图像处理技术，用于水稻叶病的早期分类和识别，比较了特征分析检测模型（FADM）和直接图像中心检测模型（DICDM）的性能，发现FADM表现更优。

- Motivation: 水稻叶病严重影响产量和经济收益，早期检测对有效管理和提高产量至关重要。
- Method: 研究采用特征提取算法、降维算法、特征选择算法和极限学习机（ELM）构建FADM，并与未使用特征提取算法的DICDM进行对比，使用10折交叉验证进行评估。
- Result: 实验结果表明，FADM在分类水稻叶病方面表现最佳。
- Conclusion: FADM在水稻叶病检测中具有显著潜力，可改善作物健康、减少产量损失并提升水稻种植的可持续性。


### [25] [Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection](https://arxiv.org/abs/2507.02349)
*Rafic Nader,Vincent L'Allinec,Romain Bourcier,Florent Autrusseau*

Main category: cs.CV

TL;DR: 论文提出了一种自动检测颅内动脉瘤关键标志点的方法，通过两步神经网络流程提高检测准确性。

- Motivation: 颅内动脉瘤常见于Willis环特定分叉处，准确检测这些标志点对快速诊断至关重要。
- Method: 采用两步神经网络：首先用目标检测网络定位感兴趣区域，再用改进的U-Net精确定位分叉点。
- Result: 实验表明，该方法在分叉检测任务中表现最佳。
- Conclusion: 两步方法有效解决了标志点邻近和视觉相似性问题，适应了Willis环的解剖变异性。


### [26] [Lightweight Shrimp Disease Detection Research Based on YOLOv8n](https://arxiv.org/abs/2507.02354)
*Fei Yuhuan,Wang Gengchen,Liu Fenghao,Zang Ran,Sun Xufei,Chang Hao*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8n的轻量级网络架构，用于虾类疾病检测，通过优化检测头和引入自注意力机制，显著提升了检测效率和准确性。

- Motivation: 虾类疾病是水产养殖中经济损失的主要原因之一，亟需高效的智能检测方法以减少疾病传播。
- Method: 设计了RLDD检测头和C2f-EMCM模块以降低计算复杂度，并引入改进的SegNext_Attention自注意力机制增强特征提取能力。
- Result: 模型参数减少32.3%，mAP@0.5达到92.7%（比YOLOv8n提高3%），并在URPC2020数据集上验证了其鲁棒性。
- Conclusion: 该方法在准确性和效率之间取得了最佳平衡，为虾类养殖中的智能疾病检测提供了可靠技术支持。


### [27] [Holistic Tokenizer for Autoregressive Image Generation](https://arxiv.org/abs/2507.02358)
*Anlin Zheng,Haochen Wang,Yucheng Zhao,Weipeng Deng,Tiancai Wang,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: Hita是一种新型图像分词器，通过整体到局部的分词方案和关键策略改进自回归图像生成，显著提升了生成质量和速度。

- Motivation: 传统自回归图像生成模型在逐步生成视觉标记时难以捕捉整体关系，且现有分词器缺乏全局信息。
- Method: Hita采用整体到局部的分词方案，结合可学习的整体查询和局部补丁标记，并引入序列结构和轻量级融合模块优化信息流。
- Result: 在ImageNet基准测试中，Hita实现了2.59 FID和281.9 IS，显著优于传统分词器，并在零样本风格迁移和图像修复中表现优异。
- Conclusion: Hita通过整体表示捕捉全局图像属性，显著提升自回归图像生成的效率和质量，具有广泛的应用潜力。


### [28] [LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling](https://arxiv.org/abs/2507.02363)
*Jiahao Wu,Rui Peng,Jianbo Jiao,Jiayu Yang,Luyang Tang,Kaiqiang Xiong,Jie Liang,Jinbo Yan,Runling Liu,Ronggang Wang*

Main category: cs.CV

TL;DR: LocalDyGS提出了一种新的动态场景重建框架，通过分解复杂动态场景为局部空间并解耦静态与动态特征，实现了对大规模和精细动态场景的建模。

- Motivation: 现实世界中复杂且高度动态的运动使得从多视角输入合成任意视角的动态视频具有挑战性，现有方法难以同时处理大规模和精细动态场景。
- Method: 1) 将复杂动态场景分解为由种子定义的局部空间，实现全局建模；2) 解耦静态与动态特征，静态特征捕获静态信息，动态残差场提供时间特定特征，结合生成时间高斯模型。
- Result: 在精细动态数据集上表现优于现有方法，并首次成功建模更大更复杂的高度动态场景。
- Conclusion: LocalDyGS为高度动态场景的重建提供了更现实的解决方案，扩展了动态场景建模的应用范围。


### [29] [UVLM: Benchmarking Video Language Model for Underwater World Understanding](https://arxiv.org/abs/2507.02373)
*Xizhe Xue,Yang Zhou,Dawei Yan,Ying Li,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 论文提出了UVLM，一个针对水下观察任务的基准数据集，填补了现有视频语言模型（VidLMs）在陆地场景外的空白。通过结合人类专业知识和AI模型，构建了高质量、多样化的数据集，并设计了多种任务类型和评估指标。实验表明，在UVLM上微调的VidLMs显著提升了水下场景的理解能力。

- Motivation: 现有视频语言模型主要关注陆地场景，忽视了水下观察的高需求应用。为填补这一空白，作者提出了UVLM基准数据集。
- Method: 通过结合人类专业知识和AI模型构建数据集，涵盖水下环境的典型挑战（如光线变化、水质浑浊等），并设计了多样化的任务类型和评估指标。
- Result: 实验表明，在UVLM上微调的VidLMs显著提升了水下场景的理解能力，同时对现有陆地基准（如VideoMME和Perception text）也有轻微改进。
- Conclusion: UVLM为水下观察任务提供了有效的基准，展示了VidLMs在水下场景的潜力，数据集和提示工程将公开。


### [30] [PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection](https://arxiv.org/abs/2507.02393)
*Seokyeong Lee,Sithu Aung,Junyong Choi,Seungryong Kim,Ig-Jae Kim,Junghyun Cho*

Main category: cs.CV

TL;DR: 提出了一种仅需视频数据的伪标签框架，用于单目3D目标检测，解决了数据稀缺和2D-3D模糊性问题。

- Motivation: 解决单目3D目标检测中数据稀缺和2D-3D模糊性问题，避免依赖多视角设置、额外传感器或领域特定训练。
- Method: 通过对象点跟踪聚合静态和动态对象的伪LiDAR数据，提取3D属性。
- Result: 实验表明，该方法具有可靠精度和强扩展性。
- Conclusion: 该方法为单目3D目标检测提供了实用且有效的解决方案。


### [31] [Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis](https://arxiv.org/abs/2507.02395)
*Byung Hyun Lee,Wongi Jeong,Woojae Han,Kyoungbun Lee,Se Young Chun*

Main category: cs.CV

TL;DR: CoMEL提出了一种持续多实例学习框架，通过高效实例编码、可靠伪标记和遗忘缓解技术，显著提升了WSI数据集的分类和定位性能。

- Motivation: 探索多实例学习（MIL）在持续任务中的适应性，解决大规模图像（如WSI）中实例分类和定位的遗忘问题。
- Method: 结合GDAT高效实例编码、BPPL伪标记和OWLoRA遗忘缓解技术。
- Result: 在三个公开WSI数据集上，CoMEL在持续MIL设置下，分类和定位准确率分别提升11.00%和23.4%。
- Conclusion: CoMEL为持续多实例学习提供了有效解决方案，显著提升了性能并减少了遗忘。


### [32] [Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection](https://arxiv.org/abs/2507.02398)
*Taehoon Kim,Jongwook Choi,Yonghyun Jeong,Haeun Noh,Jaejun Yoo,Seungryul Baek,Jongwon Choi*

Main category: cs.CV

TL;DR: 提出一种基于像素级时间不一致性的深度伪造视频检测方法，优于传统空间频率检测器。

- Motivation: 传统检测器仅通过堆叠空间频率谱来表征时间信息，无法检测像素平面的时间伪影。
- Method: 对每个像素进行时间轴1D傅里叶变换提取特征，结合注意力模块和联合变换器模块整合时空上下文特征。
- Result: 显著提升深度伪造视频检测性能，适用于多样化挑战性场景。
- Conclusion: 该方法在检测深度伪造视频方面取得了重要进展，具有鲁棒性。


### [33] [TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation](https://arxiv.org/abs/2507.02399)
*Peilin Zhang,Shaouxan Wua,Jun Feng,Zhuo Jin,Zhizezhang Gao,Jingkun Chen,Yaqiong Xing,Xiao Zhang*

Main category: cs.CV

TL;DR: TAB Net提出了一种基于涂鸦注释的弱监督医学图像分割框架，通过三重增强自恢复模块和边界感知伪标签监督模块，显著提升了分割性能。

- Motivation: 医学图像分割需要大量标注数据，但标注成本高。涂鸦注释是一种稀疏标注方式，但其稀疏性限制了特征学习和边界监督。
- Method: TAB Net包含三重增强自恢复模块（TAS）和边界感知伪标签监督模块（BAP）。TAS通过三种增强策略提升特征学习，BAP通过融合双分支预测和边界感知损失优化伪标签和边界建模。
- Result: 在ACDC和MSCMR seg数据集上，TAB Net显著优于现有弱监督方法，性能接近全监督方法。
- Conclusion: TAB Net为稀疏标注下的医学图像分割提供了一种高效解决方案，性能接近全监督方法。


### [34] [Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings](https://arxiv.org/abs/2507.02403)
*Mufhumudzi Muthivhi,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 研究探讨了自监督学习在野生动物再识别中的应用，通过无监督方式提取图像对训练模型，结果表明自监督模型在数据有限时更稳健，且在下游任务中表现优于监督学习。

- Motivation: 当前野生动物再识别依赖标注数据，自监督学习可减少对标注的依赖，利用无监督数据训练模型。
- Method: 利用相机陷阱数据中的时间图像对自动提取个体视图，训练自监督模型。
- Result: 自监督模型在数据有限时表现更稳健，且在下游任务中优于监督学习。
- Conclusion: 自监督学习为野生动物再识别提供了更高效和稳健的解决方案。


### [35] [PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration](https://arxiv.org/abs/2507.02405)
*Ayantika Das,Moitreya Chaudhuri,Koushik Bhat,Keerthi Ram,Mihail Bota,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 论文提出了一种结合扩散模型与自编码器的方法，通过学习图像特定表示并组织潜在空间，用于脑图像分析和伪影修复。

- Motivation: 扩散模型虽能生成高质量图像，但缺乏提取图像特定语义表示的能力，而自编码器能提供这种能力。通过结合两者，可以更好地组织潜在空间并实现特定任务。
- Method: 1. 设计扩散自编码模型，通过回归高分辨率图像块的位置信息来结构化潜在空间；2. 基于邻域感知的无监督撕裂伪影修复技术；3. 利用扩散模型的可控噪声和去噪能力实现无监督JPEG伪影修复。
- Result: 该方法成功区分了脑组织的不同类型，并实现了无监督的撕裂伪影和JPEG伪影修复。
- Conclusion: 结合扩散模型与自编码器的方法在图像表示学习和伪影修复任务中表现出色，为医学图像分析提供了新工具。


### [36] [A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern](https://arxiv.org/abs/2507.02408)
*Duong Nguyen-Ngoc Tran,Long Hoang Pham,Chi Dai Tran,Quoc Pham-Nam Ho,Huy-Hung Nguyen,Jae Wook Jeon*

Main category: cs.CV

TL;DR: 论文提出了一种针对热成像中行人跟踪的新方法，通过优化两阶段超参数，提高了跟踪性能。

- Motivation: 热成像在低能见度环境中优于RGB相机，但其低层次特征表示导致行人检测和跟踪困难。
- Method: 引入了一种新颖的调优方法，优化两阶段超参数，无需复杂重识别或运动模型。
- Result: 在PBVS Thermal MOT数据集上表现优异，适用于多种热成像条件。
- Conclusion: 该方法为实际监控应用提供了高效的解决方案。


### [37] [Privacy-preserving Preselection for Face Identification Based on Packing](https://arxiv.org/abs/2507.02414)
*Rundong Xin,Taotao Wang,Jin Wang,Chonghe Zhao,Jing Wang*

Main category: cs.CV

TL;DR: 提出了一种名为PFIP的高效密文域人脸检索方案，通过预选机制和打包模块提升效率，实验显示其检索速度提升近50倍。

- Motivation: 随着密文模板库规模增大，人脸检索过程耗时增加，隐私保护和效率成为关键问题。
- Method: PFIP结合创新的预选机制减少计算开销，打包模块增强生物识别系统的灵活性。
- Result: 在LFW和CASIA数据集上，PFIP保持原始人脸识别模型的准确性，检索1000个密文人脸模板仅需300毫秒，命中率100%。
- Conclusion: PFIP显著提升了密文域人脸检索的效率，同时保持了高准确性。


### [38] [Determination Of Structural Cracks Using Deep Learning Frameworks](https://arxiv.org/abs/2507.02416)
*Subhasis Dasgupta,Jaydip Sen,Tuhina Halder*

Main category: cs.CV

TL;DR: 论文提出了一种基于残差U-Net和集成学习的新方法，用于提高结构裂缝检测的准确性和效率。

- Motivation: 手动检测结构裂缝存在速度慢、不一致和易出错的问题，影响评估可靠性。
- Method: 研究采用了残差U-Net模型的多种配置，并将其集成到一个包含卷积块的元模型中。
- Result: 集成模型在IoU和DICE系数上表现最佳，优于SegNet和传统U-Net。
- Conclusion: 该方法为结构缺陷监测任务提供了更可靠的自动化解决方案。


### [39] [AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars](https://arxiv.org/abs/2507.02419)
*Yiming Zhong,Xiaolin Zhang,Ligang Liu,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出了一种名为AvatarMakeup的3D化妆方法，通过预训练的扩散模型从单张参考照片中转移化妆模式，解决了现有方法在动态表情和多视角下化妆效果不一致的问题。

- Motivation: 当前3D高斯编辑方法无法满足真实化妆效果的基本要求，如动态表情下的外观一致性、化妆过程中的身份保持以及对细节的精确控制。
- Method: 采用从粗到细的策略，首先通过Coherent Duplication方法确保化妆的一致性和身份保持，然后通过Refinement Module提升化妆质量。扩散模型用于生成化妆图像作为监督。
- Result: 实验表明，AvatarMakeup在化妆转移质量和动画一致性方面达到了最先进的水平。
- Conclusion: AvatarMakeup方法有效解决了3D虚拟角色化妆中的一致性和细节控制问题，为个性化定制提供了新思路。


### [40] [F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning](https://arxiv.org/abs/2507.02437)
*Wei Li,Jingyang Zhang,Lihao Liu,Guoan Wang,Junjun He,Yang Chen,Lixu Gu*

Main category: cs.CV

TL;DR: 本文提出了一种自由形式测试时间适应（F²TTA）任务，并设计了I-DiPT框架，通过图像不变和图像特定提示解决域片段随机到达的问题。

- Motivation: 医疗数据通常以随机顺序和任意长度的域片段到达，现有TTA方法无法处理这种自由形式的数据流。
- Method: 提出I-DiPT框架，结合不确定性导向掩码（UoM）和平行图蒸馏（PGD）方法，优化提示学习。
- Result: 在乳腺癌和青光眼分类任务中，I-DiPT优于现有TTA方法。
- Conclusion: I-DiPT有效解决了自由形式测试时间适应问题，提升了模型在随机域片段中的适应性。


### [41] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: 论文提出使用FPGA加速ANN，提升机器人检测速度和任务执行效率。

- Motivation: 机器人因检测算法速度慢而降低移动速度，影响任务执行效率。
- Method: 采用FINN架构部署三种量化ANN模型（MobileNet v1、CNV）至FPGA的PL部分，并在RG2C数据集上训练。
- Result: MobileNet v1表现最佳，成功率达98%，推理速度为6611 FPS。
- Conclusion: FPGA可有效加速ANN，适用于注意力机制，提升机器人性能。


### [42] [IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising](https://arxiv.org/abs/2507.02445)
*Hailong Yan,Junjian Huang,Tingwen Huang*

Main category: cs.CV

TL;DR: IGDNet是一种零样本增强方法，无需训练数据或先验知识，仅通过单张测试图像恢复光照并抑制噪声。

- Motivation: 现有方法依赖成对数据集且可能导致过增强，IGDNet旨在解决这些问题。
- Method: 通过分解模块和去噪模块分离图像为光照与反射分量，并迭代优化噪声对。
- Result: 在四个公开数据集上表现优异，PSNR和SSIM指标优于14种无监督方法。
- Conclusion: IGDNet在复杂光照条件下显著提升视觉质量，具有强泛化能力。


### [43] [Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection](https://arxiv.org/abs/2507.02454)
*Weiwei Duan,Luping Ji,Shengjia Chen,Sicheng Zhu,Jianghong Huang,Mao Ye*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督对比学习方案（WeCoL），用于红外小目标检测，减少对大量手动标注的依赖。

- Motivation: 红外小目标检测因目标尺寸小和背景对比度低而具有挑战性，现有方法依赖大量手动标注，耗时且昂贵。
- Method: 基于预训练的SAM模型，设计潜在目标挖掘策略，结合对比学习和长短期运动感知学习方案。
- Result: 在两个公开数据集上的实验表明，该弱监督方案性能优于早期全监督方法，甚至接近SOTA全监督方法的90%。
- Conclusion: WeCoL方案为红外小目标检测提供了一种高效且低标注成本的解决方案。


### [44] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: Mesh Silksong是一种高效的网格表示方法，通过自回归方式生成多边形网格，减少冗余并提升几何完整性。

- Motivation: 现有网格标记化方法存在顶点标记重复的问题，浪费网络能力，因此需要一种更高效的表示方法。
- Method: 通过仅访问每个顶点一次的方式标记化网格顶点，减少50%的冗余，并实现约22%的压缩率。
- Result: 生成的网格具有优异的几何特性（如流形拓扑、水密检测和一致的法线），实验证明其有效性和几何完整性。
- Conclusion: Mesh Silksong在网格生成和几何特性方面表现优异，适用于实际应用。


### [45] [CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios](https://arxiv.org/abs/2507.02479)
*Teng Fu,Yuwen Chen,Zhuofan Chen,Mengyang Zhao,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 提出一个名为CrowdTrack的大规模多行人跟踪数据集，解决现有数据集场景简单和非真实的问题，包含33个视频和5,185条轨迹，适用于复杂场景下的算法开发。

- Motivation: 现有多目标跟踪数据集场景简单且非真实，难以满足复杂场景下的研究需求。
- Method: 通过第一人称视角拍摄真实复杂场景，构建包含33个视频和5,185条轨迹的数据集，并标注完整边界框和唯一对象ID。
- Result: 数据集为复杂场景下的算法开发提供了平台，并测试了多个SOTA模型和基础模型的性能。
- Conclusion: CrowdTrack数据集填补了现有数据集的不足，为复杂场景下的多行人跟踪研究提供了有力支持。


### [46] [MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention](https://arxiv.org/abs/2507.02488)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: MedFormer是一种高效的医学视觉Transformer，通过金字塔缩放结构和新型Dual Sparse Selection Attention（DSSA）解决现有方法的通用性和计算效率问题，显著提升医学图像识别任务的性能。

- Motivation: 现有医学视觉Transformer方法存在通用性差和计算成本高的问题，限制了其在多样化医学图像识别任务中的应用。
- Method: 提出MedFormer，采用金字塔缩放结构作为通用骨干网络，并引入内容感知的DSSA机制，以高效建模长程依赖关系。
- Result: 在多种医学图像数据集上的实验表明，MedFormer在分类、语义分割和病变检测任务中均表现出色。
- Conclusion: MedFormer通过创新结构和注意力机制，显著提升了医学图像识别的通用性和效率，具有广泛的应用潜力。


### [47] [Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy](https://arxiv.org/abs/2507.02493)
*Luca Parolari,Andrea Cherubini,Lamberto Ballan,Carlo Biffi*

Main category: cs.CV

TL;DR: 提出了一种结合时间感知的监督对比损失方法，用于结肠镜息肉计数，显著降低了碎片化率。

- Motivation: 提高结肠镜筛查的成本效益，通过自动息肉计数优化程序报告和质量控制。
- Method: 引入时间感知的监督对比损失，结合时间邻接约束改进轨迹聚类。
- Result: 碎片化率比现有方法降低了2.2倍，性能显著提升。
- Conclusion: 时间感知在息肉计数中至关重要，新方法达到了最新技术水平。


### [48] [MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations](https://arxiv.org/abs/2507.02494)
*Hyunsoo Son,Jeonghyun Noh,Suemin Jeon,Chaoli Wang,Won-Ki Jeong*

Main category: cs.CV

TL;DR: MC-INR是一种新的神经网络框架，用于处理非结构化网格上的多变量数据，通过元学习和聚类改进复杂结构的编码，并引入动态重新聚类机制和分支层提升性能。

- Motivation: 现有隐式神经表示（INR）方法在复杂结构表示、多变量数据处理和非结构化网格适应性方面存在局限，限制了其在真实数据集上的表现。
- Method: 结合元学习和聚类，提出动态重新聚类机制和分支层，以灵活编码复杂结构并处理多变量数据。
- Result: 实验表明，MC-INR在科学数据编码任务中优于现有方法。
- Conclusion: MC-INR通过创新设计解决了现有INR方法的局限性，提升了复杂数据处理的性能。


### [49] [Automatic Labelling for Low-Light Pedestrian Detection](https://arxiv.org/abs/2507.02513)
*Dimitrios Bouzoulas,Eerik Alamikkotervo,Risto Ojala*

Main category: cs.CV

TL;DR: 提出了一种自动化的红外-RGB标注流程，用于低光条件下的RGB行人检测，并在KAIST数据集上验证其效果优于传统标注方法。

- Motivation: 解决低光条件下RGB行人检测缺乏公开数据集的问题。
- Method: 1) 红外检测；2) 标签从红外传递到RGB；3) 使用生成标签训练检测模型。
- Result: 在未见过的图像序列上，生成标签训练的模型在6/9情况下优于传统标注模型。
- Conclusion: 自动化标注流程有效提升了低光条件下RGB行人检测的性能。


### [50] [Detecting Multiple Diseases in Multiple Crops Using Deep Learning](https://arxiv.org/abs/2507.02517)
*Vivek Yadav,Anugrah Jain*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的解决方案，用于检测多种作物中的多种疾病，旨在覆盖印度多样化的农业景观。通过构建包含17种作物和34种疾病的统一数据集，模型在准确性和覆盖范围上优于现有技术。

- Motivation: 印度作为农业经济为主的国家，面临作物疾病、害虫和环境压力导致的严重损失，早期检测和准确识别对提高产量和确保粮食安全至关重要。
- Method: 构建包含17种作物和34种疾病的统一数据集，并训练深度学习模型。
- Result: 模型在统一数据集上达到99%的检测准确率，比现有技术（覆盖14种作物和26种疾病）高出7%。
- Conclusion: 通过扩大可检测的作物和疾病范围，该解决方案旨在为印度农民提供更有效的工具。


### [51] [IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning](https://arxiv.org/abs/2507.02519)
*Abiam Remache González,Meriem Chagour,Timon Bijan Rüth,Raúl Trapiella Cañedo,Marina Martínez Soler,Álvaro Lorenzo Felipe,Hyun-Suk Shin,María-Jesús Zamorano Serrano,Ricardo Torres,Juan-Antonio Castillo Parra,Eduardo Reyes Abad,Miguel-Ángel Ferrer Ballester,Juan-Manuel Afonso López,Francisco-Mario Hernández Tejera,Adrian Penate-Sanchez*

Main category: cs.CV

TL;DR: IMASHRIMP是一个自动化白虾形态分析系统，通过改进的深度学习和计算机视觉技术优化水产养殖中的遗传选择任务。

- Motivation: 解决虾类形态分析中的特定挑战，减少人工误差并提高遗传选择效率。
- Method: 采用改进的ResNet-50架构进行分类和检测，结合VitPose进行姿态估计，并使用SVM模型进行形态回归。
- Result: 系统显著减少人工误差，姿态估计mAP达97.94%，像素到厘米转换误差为0.07 cm。
- Conclusion: IMASHRIMP能有效自动化和加速虾类形态分析，提升水产养殖的可持续性。


### [52] [MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details](https://arxiv.org/abs/2507.02546)
*Ruicheng Wang,Sicheng Xu,Yue Dong,Yu Deng,Jianfeng Xiang,Zelong Lv,Guangzhong Sun,Xin Tong,Jiaolong Yang*

Main category: cs.CV

TL;DR: MoGe-2是一种先进的开放域几何估计模型，通过单张图像恢复场景的度量尺度3D点图。它在MoGe的基础上扩展，实现度量几何预测，并通过数据细化方法提升细节恢复能力。

- Motivation: 现有方法（如MoGe）只能预测未知尺度的仿射不变点图，无法同时实现精确的相对几何、度量尺度和细节恢复。MoGe-2旨在解决这一问题。
- Method: 在MoGe的基础上扩展，结合度量几何预测策略，并开发统一的数据细化方法，利用合成标签过滤和补全真实数据。
- Result: 在混合数据集上训练和评估，MoGe-2在相对几何精度、度量尺度和细节恢复方面均优于现有方法。
- Conclusion: MoGe-2首次同时实现了精确的相对几何、度量尺度和细节恢复，为单图像几何估计提供了更全面的解决方案。


### [53] [Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning](https://arxiv.org/abs/2507.02565)
*Buzhen Huang,Chen Li,Chongyang Xu,Dongyue Lu,Jinnan Chen,Yangang Wang,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出了一种基于双分支优化的框架，通过结合人类外观、社交距离和物理约束，从复杂视频中重建准确的人体交互动作。

- Motivation: 现有方法在视觉模糊和遮挡情况下难以准确估计人体姿态，即使是大型基础模型也无法区分复杂场景中的人类语义。
- Method: 使用扩散模型学习社交距离和姿态先验知识，结合双分支优化框架和多种约束（3D高斯、2D关键点、网格穿透）重建动作和外观。
- Result: 在多个基准测试中表现优于现有方法，并构建了带有伪真实标注的数据集。
- Conclusion: 该方法能够从复杂环境中准确估计交互动作，为未来姿态估计和行为理解研究提供了支持。


### [54] [Parametric shape models for vessels learned from segmentations via differentiable voxelization](https://arxiv.org/abs/2507.02576)
*Alina F. Dima,Suprosanna Shit,Huaqi Qiu,Robbie Holland,Tamara T. Mueller,Fabio Antonio Musio,Kaiyuan Yang,Bjoern Menze,Rickmer Braren,Marcus Makowski,Daniel Rueckert*

Main category: cs.CV

TL;DR: 提出了一种将体素化、网格和参数模型通过可微分变换结合的框架，用于血管建模。

- Motivation: 血管的多种表示方法（如体素化、网格和参数模型）通常独立使用，缺乏统一性。本文旨在通过可微分变换将它们结合起来，提升建模的灵活性和准确性。
- Method: 利用可微分体素化技术，通过形状到分割的拟合自动提取血管的参数化形状模型。血管被参数化为中心线和半径，使用三次B样条确保平滑性和连续性。网格可从学习到的形状参数中可微分地提取。
- Result: 实验证明，该方法能准确捕捉复杂血管的几何形状，如主动脉、动脉瘤和脑血管。
- Conclusion: 该框架成功地将三种表示方法统一起来，实现了高保真度的血管建模，并支持后续的网格操作。


### [55] [Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning](https://arxiv.org/abs/2507.02581)
*Tan Pan,Zhaorui Tan,Kaiyu Guo,Dongli Xu,Weidi Xu,Chen Jiang,Xin Guo,Yuan Qi,Yuan Cheng*

Main category: cs.CV

TL;DR: 提出了一种名为$S^2DC$的自监督学习框架，通过结构感知的语义差异和一致性学习3D医学图像表示，优于现有方法。

- Motivation: 现有方法使用固定大小的图像块划分，忽略了医学图像中解剖结构的位置、尺度和形态变化，导致无法捕捉有意义的差异。
- Method: $S^2DC$通过最优传输策略增强不同块间的语义差异，并通过邻域相似性分布提升结构级别的语义一致性。
- Result: 在10个数据集、4个任务和3种模态上的实验表明，$S^2DC$优于现有方法。
- Conclusion: $S^2DC$通过结构感知的表示学习，显著提升了3D医学图像自监督学习的性能。


### [56] [AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/abs/2507.02591)
*Weili Xu,Enxin Song,Wenhao Chai,Xuexiang Wen,Tian Ye,Gaoang Wang*

Main category: cs.CV

TL;DR: AuroraLong使用线性RNN替代LLM，解决了长视频理解的高计算和内存成本问题，性能接近Transformer模型。

- Motivation: 长视频理解因计算和内存成本高而具有挑战性，传统Transformer模型难以处理长序列。
- Method: 用线性RNN语言模型替代LLM，结合视觉令牌合并和升序重排，提升效率。
- Result: AuroraLong在多个视频基准测试中表现接近Transformer模型，仅需2B参数和公开数据。
- Conclusion: 线性RNN有望降低长视频理解的计算门槛，推动技术普及。


### [57] [Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development](https://arxiv.org/abs/2507.02602)
*Riccardo Gallon,Fabian Schiemenz,Alessandra Menicucci,Eberhard Gill*

Main category: cs.CV

TL;DR: 论文探讨了视觉导航（VBN）算法在太空任务中的可靠性挑战，提出利用AI检测传感器故障，并通过仿真框架生成故障图像数据集以支持AI训练。

- Motivation: 解决视觉导航算法因传感器故障导致的可靠性问题，并克服传统故障检测方法的局限性。
- Method: 分析相机传感器故障案例，建立仿真框架生成故障图像数据集。
- Result: 生成了可用于训练AI的故障图像数据集，为故障检测算法提供了支持。
- Conclusion: 通过仿真生成的故障数据集为AI在视觉导航故障检测中的应用提供了重要工具。


### [58] [AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models](https://arxiv.org/abs/2507.02664)
*Ziyin Zhou,Yunpeng Luo,Yuanchen Wu,Ke Sun,Jiayi Ji,Ke Yan,Shouhong Ding,Xiaoshuai Sun,Yunsheng Wu,Rongrong Ji*

Main category: cs.CV

TL;DR: 论文提出Holmes-Set数据集和Holmes Pipeline框架，用于解决AI生成图像检测中缺乏可解释性和泛化能力的问题，最终开发出AIGI-Holmes模型。

- Motivation: AI生成图像的滥用威胁公共信息安全，现有检测技术缺乏可解释性和对新技术的泛化能力。
- Method: 提出Holmes-Set数据集（包括Holmes-SFTSet和Holmes-DPOSet）和Holmes Pipeline三阶段训练框架（预训练、监督微调、偏好优化），结合多专家评审和协作解码策略。
- Result: 在三个基准测试中验证了AIGI-Holmes的有效性。
- Conclusion: Holmes-Set和Holmes Pipeline显著提升了AI生成图像检测的可解释性和泛化能力。


### [59] [Learning few-step posterior samplers by unfolding and distillation of diffusion models](https://arxiv.org/abs/2507.02686)
*Charlesquin Kemajou Mbakam,Jonathan Spence,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过深度展开和模型蒸馏将扩散模型转化为高效的条件模型，用于后验采样。

- Motivation: 结合扩散模型的灵活性和条件模型的准确性，解决计算成像中的后验采样问题。
- Method: 采用深度展开技术，将LATINO Langevin采样器展开为高效模型，并结合模型蒸馏优化。
- Result: 实验表明，该方法在准确性和计算效率上优于现有技术，同时保持灵活性。
- Conclusion: 新框架成功整合了扩散模型的优势，为计算成像提供了高效且灵活的后验采样解决方案。


### [60] [APT: Adaptive Personalized Training for Diffusion Models with Limited Data](https://arxiv.org/abs/2507.02687)
*JungWoo Chae,Jiyoon Kim,JaeWoong Choi,Kyungyul Kim,Sangheum Hwang*

Main category: cs.CV

TL;DR: APT框架通过自适应训练策略和特征正则化，解决了扩散模型在有限数据下微调时的过拟合问题，同时保持了先验知识和文本对齐。

- Motivation: 解决扩散模型在有限数据下微调时的过拟合、先验知识丢失和文本对齐退化问题。
- Method: APT框架包含自适应训练调整、表示稳定化和注意力对齐三个组件，分别通过过拟合检测、特征正则化和注意力图对齐来优化模型。
- Result: 实验表明，APT有效减少过拟合，保持先验知识，并在有限数据下生成高质量、多样化的图像。
- Conclusion: APT为扩散模型的个性化微调提供了一种高效且稳定的解决方案。


### [61] [CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation](https://arxiv.org/abs/2507.02691)
*Xiangyang Luo,Ye Zhu,Yunfei Liu,Lijian Lin,Cong Wan,Zijian Cai,Shao-Lun Huang,Yu Li*

Main category: cs.CV

TL;DR: CanonSwap是一种新的视频人脸交换框架，通过解耦面部外观和运动信息，解决了现有方法在保持目标面部动态属性上的不足。

- Motivation: 现有方法在视频人脸交换中难以同时实现高质量的身份转移和目标面部动态属性的准确保持，CanonSwap旨在解决这一问题。
- Method: CanonSwap通过解耦运动信息与外观信息，在统一规范空间内进行身份修改，并通过Partial Identity Modulation模块和细粒度同步指标优化身份转移。
- Result: 实验表明，CanonSwap在视觉质量、时间一致性和身份保持方面显著优于现有方法。
- Conclusion: CanonSwap通过解耦和优化设计，成功解决了视频人脸交换中的关键挑战，实现了更高质量的结果。


### [62] [SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment](https://arxiv.org/abs/2507.02705)
*Qi Xu,Dongxu Wei,Lingzhe Zhao,Wenpu Li,Zhangchi Huang,Shunping Ji,Peidong Liu*

Main category: cs.CV

TL;DR: SIU3R提出了一种无需对齐的框架，通过像素对齐的3D表示实现3D重建与理解的统一，避免了2D-3D特征对齐的局限性。

- Motivation: 现有方法依赖2D-3D特征对齐，导致3D理解能力有限和语义信息丢失，因此需要一种无需对齐的框架。
- Method: SIU3R通过像素对齐的3D表示和统一的可学习查询，实现重建与理解的统一，并设计了轻量模块促进任务协作。
- Result: 实验表明SIU3R在3D重建、理解及两者的联合任务上均达到最优性能。
- Conclusion: SIU3R的无对齐框架和任务协作设计有效提升了性能，为端到端智能系统提供了新思路。


### [63] [UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation](https://arxiv.org/abs/2507.02713)
*Qin Guo,Ailing Zeng,Dongxu Yue,Ceyuan Yang,Yang Cao,Hanzhong Guo,Fei Shen,Wei Liu,Xihui Liu,Dan Xu*

Main category: cs.CV

TL;DR: 论文提出UniMC框架和HAIG-2.9M数据集，解决现有关键点引导模型在生成非刚性物体和多重叠对象时的局限性。

- Motivation: 现有关键点引导模型难以控制非刚性物体（如动物）和多重叠对象的生成，且缺乏合适的数据集。
- Method: 设计基于DiT的UniMC框架，整合实例和关键点条件为紧凑令牌；构建HAIG-2.9M数据集，包含高质量标注。
- Result: 实验证明HAIG-2.9M数据集质量高，UniMC在遮挡和多类场景中表现优异。
- Conclusion: UniMC和HAIG-2.9M显著提升了关键点引导生成模型的性能，尤其在复杂场景中。


### [64] [FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models](https://arxiv.org/abs/2507.02714)
*Yuxuan Wang,Tianwei Cao,Huayu Zhang,Zhongjiang He,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: FairHuman提出了一种多目标微调方法，通过全局和局部目标优化，提升生成图像中细节（如人脸和手）的质量。

- Motivation: 当前文本到图像模型在生成人类图像时，局部细节（如人脸和手）质量不足，缺乏训练监督。
- Method: 构建全局和局部目标（基于预标注位置先验），采用最小潜在延迟准则优化参数更新策略。
- Result: 显著提升了局部细节生成质量，同时保持整体图像质量。
- Conclusion: FairHuman在多场景下有效提升了人类图像生成的性能。


### [65] [Prompt learning with bounding box constraints for medical image segmentation](https://arxiv.org/abs/2507.02743)
*Mélanie Gaillochet,Mehrdad Noori,Sahar Dastani,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

TL;DR: 提出了一种结合基础模型和弱监督分割的新框架，仅需边界框标注即可自动生成提示，优化性能优于全监督和弱监督方法。

- Motivation: 解决医学图像像素级标注成本高的问题，利用边界框标注的弱监督方法减少人工干预。
- Method: 结合基础模型的表示能力和弱监督分割的标注效率，通过边界框标注自动生成提示，并整合伪标签优化模型。
- Result: 在多模态数据集上平均Dice分数达84.90%，优于现有全监督和弱监督方法。
- Conclusion: 该框架显著降低了标注成本，同时保持了高性能，为医学图像分割提供了实用解决方案。


### [66] [DexVLG: Dexterous Vision-Language-Grasp Model at Scale](https://arxiv.org/abs/2507.02747)
*Jiawei He,Danshi Li,Xinqiang Yu,Zekun Qi,Wenyao Zhang,Jiayi Chen,Zhaoxiang Zhang,Zhizheng Zhang,Li Yi,He Wang*

Main category: cs.CV

TL;DR: DexVLG是一个基于视觉-语言-动作的大模型，用于预测灵巧手的抓取姿势，并通过语言指令对齐。

- Motivation: 当前研究主要集中于简单夹爪控制，缺乏针对灵巧手的功能性抓取研究，数据收集难度限制了进展。
- Method: 生成170万灵巧抓取姿势数据集DexGraspNet 3.0，训练VLM和流匹配模型，实现语言指令对齐的抓取预测。
- Result: 在仿真和真实实验中，DexVLG表现出强大的零样本泛化能力，抓取成功率达76%，并实现零件对齐抓取。
- Conclusion: DexVLG为灵巧手的功能性抓取提供了有效解决方案，展示了语言指令对齐的潜力。


### [67] [Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics](https://arxiv.org/abs/2507.02748)
*Alex Colagrande,Paul Caillon,Eva Feillet,Alexandre Allauzen*

Main category: cs.CV

TL;DR: MANO是一种新型注意力机制，通过多尺度距离计算实现线性复杂度，性能媲美ViT和Swin Transformer，同时大幅降低计算资源消耗。

- Motivation: 标准Transformer的二次复杂度限制了其在高分辨率输入上的应用，现有方法常以牺牲细节为代价。
- Method: 受n体数值模拟启发，将注意力建模为网格点间的交互问题，提出多极注意力神经算子（MANO），实现线性复杂度。
- Result: 在图像分类和Darcy流任务上，MANO性能与ViT和Swin Transformer相当，但计算时间和内存消耗显著降低。
- Conclusion: MANO为高效处理高分辨率输入提供了新思路，同时保持全局感受野和细节捕捉能力。


### [68] [Partial Weakly-Supervised Oriented Object Detection](https://arxiv.org/abs/2507.02751)
*Mingxin Liu,Peiyuan Zhang,Yuan Liu,Wei Zhang,Yue Zhou,Ning Liao,Ziyang Gong,Junwei Luo,Zhirui Wang,Yi Yu,Xue Yang*

Main category: cs.CV

TL;DR: 提出了一种基于部分弱标注的PWOOD框架，显著降低标注成本，性能优于传统弱监督算法。

- Motivation: 解决定向目标检测中高标注成本问题。
- Method: 提出PWOOD框架、OS-Student模型和CPF策略。
- Result: 在多个数据集上表现优于传统半监督算法。
- Conclusion: PWOOD框架为定向目标检测提供了一种低成本高性能的解决方案。


### [69] [From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images](https://arxiv.org/abs/2507.02781)
*Danrong Zhang,Huili Huang,N. Simrill Smith,Nimisha Roy,J. David Frost*

Main category: cs.CV

TL;DR: 该研究提出了一种基于语义分割的地震后社交媒体图像损害严重性评估方法，通过构建分段损害数据集并微调SegFormer模型，实现了更客观和全面的损害量化。

- Motivation: 传统的地震后损害评估方法依赖主观分类，无法准确反映图像中不同程度的损害。本研究旨在通过语义分割提供更客观的损害分析。
- Method: 构建包含三种损害程度的分段数据集（未损坏、损坏、废墟），微调SegFormer模型进行语义分割，并引入新的损害评分系统，结合深度估计调整。
- Result: 该方法能够更客观地量化社交媒体图像中的损害严重性，为灾害勘察团队提供更精确的指导。
- Conclusion: 通过语义分割和新的评分系统，研究提升了地震后损害评估的客观性和全面性，有助于更有效的灾害响应。


### [70] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态叙事理解的自动视频编辑框架（HIVE），通过角色提取、对话分析和叙事摘要，结合场景级分割，显著提升了视频编辑的连贯性和质量。

- Motivation: 在线视频内容快速增长，现有自动编辑方法主要依赖文本线索，忽略了丰富的视觉上下文，导致输出不连贯。
- Method: 结合多模态大语言模型进行角色提取、对话分析和叙事摘要，并将编辑过程分解为高光检测、开头/结尾选择和无关内容修剪三个子任务。
- Result: 实验结果表明，HIVE在通用和广告导向的编辑任务中均优于现有基线，显著缩小了自动与人工编辑视频的质量差距。
- Conclusion: HIVE框架通过多模态叙事理解和场景级分割，有效提升了自动视频编辑的连贯性和质量。


### [71] [RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation](https://arxiv.org/abs/2507.02792)
*Liheng Zhang,Lexi Pang,Hang Ye,Xiaoxuan Ma,Yizhou Wang*

Main category: cs.CV

TL;DR: 本文提出了一种灵活的文本到图像扩散模型特征注入框架，解决了现有方法在结构对齐和视觉质量上的问题，并通过实验验证了其优越性。

- Motivation: 现有特征注入方法在结构对齐和视觉质量上存在不足，尤其在条件图像与自然RGB分布差异较大时表现不佳。
- Method: 提出了一种解耦注入时间步和去噪过程的框架，包含结构丰富的注入模块、外观丰富的提示和重启细化策略。
- Result: 实验表明，该方法在多样化的零样本条件场景中实现了最先进的性能。
- Conclusion: 该框架在无需训练的情况下实现了结构丰富和外观丰富的生成效果。


### [72] [No time to train! Training-Free Reference-Based Instance Segmentation](https://arxiv.org/abs/2507.02798)
*Miguel Espinosa,Chenhongyi Yang,Linus Ericsson,Steven McDonagh,Elliot J. Crowley*

Main category: cs.CV

TL;DR: 论文提出了一种基于参考图像的自动对象分割方法，利用基础模型的语义先验减少对人工提示的依赖，并在多个基准测试中取得显著性能提升。

- Motivation: 解决现有分割模型（如SAM）仍需人工视觉提示或复杂规则的问题，探索仅通过少量参考图像实现对象分割的可能性。
- Method: 提出一种多阶段、无需训练的方法，包括内存库构建、表示聚合和语义感知特征匹配。
- Result: 在COCO FSOD（36.8% nAP）、PASCAL VOC Few-Shot（71.2% nAP50）和Cross-Domain FSOD（22.4% nAP）上取得最优性能。
- Conclusion: 通过利用语义先验和参考图像，实现了高效且自动化的对象分割，为下游任务提供了新思路。


### [73] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: HyperGaussians是一种基于3D高斯泼溅的新方法，用于高质量可动画化面部虚拟形象，解决了现有技术在非线性变形和复杂光照效果上的不足。

- Motivation: 现有3D高斯泼溅技术在静态面部表现优秀，但在可动画化虚拟形象中仍存在非线性变形和细节缺失问题，需要更高效的表示方法。
- Method: 提出HyperGaussians，通过高维多变量高斯分布增强表达能力，并引入逆协方差技巧提升计算效率。
- Result: 在19个受试者的4个面部数据集上，HyperGaussians在数值和视觉上均优于3DGS，尤其在细节表现上。
- Conclusion: HyperGaussians为高质量可动画化面部虚拟形象提供了一种高效且表现力强的解决方案。


### [74] [LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion](https://arxiv.org/abs/2507.02813)
*Fangfu Liu,Hao Li,Jiawei Chi,Hanyang Wang,Minghui Yang,Fudong Wang,Yueqi Duan*

Main category: cs.CV

TL;DR: LangScene-X是一个生成框架，通过稀疏视图生成3D一致的多模态信息，用于重建和理解场景，解决了传统方法在有限视图下的渲染和语义合成问题。

- Motivation: 传统方法依赖密集视图重建，导致在有限视图下出现渲染伪影和语义合成不准确的问题。LangScene-X旨在通过生成一致的新观测来解决这些问题。
- Method: 1. 训练TriMap视频扩散模型，从稀疏输入生成RGB、几何和语义信息；2. 提出语言量化压缩器（LQC）编码语言嵌入；3. 通过语言表面场对齐语言信息到3D场景。
- Result: LangScene-X在真实数据实验中表现出优于现有方法的生成质量和泛化能力。
- Conclusion: LangScene-X通过生成一致的多模态信息，实现了从稀疏视图的高质量3D重建和开放词汇场景理解。


### [75] [Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach](https://arxiv.org/abs/2507.02826)
*Panpan Ji,Junni Song,Hang Xiao,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 提出了一种名为DCDP-HAR的动态对比双路径网络框架，用于解决多模态HAR系统中的跨模态特征对齐和模态贡献不平衡问题。

- Motivation: 多模态HAR系统面临跨模态特征对齐困难和模态贡献不平衡的挑战，需要一种更有效的解决方案。
- Method: 框架包含双路径特征提取（ResNet和DenseNet）、多阶段对比学习机制和置信度驱动的梯度调制策略。
- Result: 通过消融实验和四个公共基准数据集上的对比实验验证了各组成部分的有效性。
- Conclusion: DCDP-HAR框架在多模态HAR任务中表现出色，能够有效解决现有挑战。


### [76] [USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network](https://arxiv.org/abs/2507.02827)
*Ying Yu,Hang Xiao,Siyao Li,Jiarui Li,Haotian Tang,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于多注意力交互机制的综合优化方法（USAD），用于解决人类活动识别（HAR）中的标签数据稀缺、特征提取不足和设备性能问题。通过无监督数据增强、多分支时空交互网络和自适应多损失函数融合，显著提升了模型性能。

- Motivation: HAR面临标签数据稀缺、高级特征提取不足和轻量设备性能不佳等问题，亟需一种综合优化方法。
- Method: 采用无监督扩散模型进行数据增强，设计多分支时空交互网络（含时空注意力机制），并引入自适应多损失函数融合策略。
- Result: 在WISDM、PAMAP2和OPPORTUNITY数据集上分别达到98.84%、93.81%和80.92%的准确率，优于现有方法。
- Conclusion: USAD方法有效解决了HAR的关键挑战，并在嵌入式设备上验证了其高效性和可行性。


### [77] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为VisCo的新型视觉中心越狱攻击方法，通过动态生成辅助图像和优化攻击提示，显著提高了对多模态大语言模型的攻击成功率。

- Motivation: 多模态大语言模型（MLLMs）在现实应用中存在视觉模态的安全漏洞，现有攻击方法主要依赖文本语义，缺乏现实场景的语义基础。
- Method: 提出VisCo攻击方法，通过四种视觉策略构建上下文对话，动态生成辅助图像，并结合自动毒性模糊和语义优化生成攻击提示。
- Result: VisCo在MM-SafetyBench上对GPT-4o的毒性评分为4.78，攻击成功率为85%，显著优于基线方法。
- Conclusion: VisCo攻击方法有效揭示了MLLMs在视觉模态下的安全风险，为未来防御研究提供了重要参考。


### [78] [AnyI2V: Animating Any Conditional Image with Motion Control](https://arxiv.org/abs/2507.02857)
*Ziye Li,Hao Luo,Xincheng Shuai,Henghui Ding*

Main category: cs.CV

TL;DR: AnyI2V是一个无需训练的框架，通过用户定义的运动轨迹为任意条件图像生成动画，支持多种模态输入，提供灵活的视频生成和编辑能力。

- Motivation: 现有文本到视频（T2V）和图像到视频（I2V）方法在动态运动和空间约束控制上存在不足，T2V依赖文本提示缺乏空间布局精确控制，I2V依赖真实图像限制了编辑性。
- Method: 提出AnyI2V框架，无需训练，支持多种模态输入（如网格和点云），通过用户定义的运动轨迹生成动画，并支持混合条件输入和风格编辑。
- Result: 实验表明AnyI2V在空间和运动控制视频生成中表现优异，提供了更灵活和多样化的视频生成能力。
- Conclusion: AnyI2V为视频生成提供了新的视角，解决了现有方法的局限性，支持更广泛的应用场景。


### [79] [Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation](https://arxiv.org/abs/2507.02859)
*Jiaer Xia,Bingkui Tong,Yuhang Zang,Rui Shao,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种名为Grounded Chain-of-Thought (GCoT)的方法，通过注入边界框信息改进多模态大语言模型（MLLMs）在专业视觉任务中的适应能力，解决了传统CoT数据中的事实错误问题。

- Motivation: 现有MLLMs在专业视觉任务（如图表理解）中表现不佳，主要原因是预训练数据与下游任务不匹配，且传统CoT数据存在事实错误。
- Method: 提出GCoT方法，通过引入边界框信息增强CoT数据的可靠性，提升模型在数据有限情况下的适应能力。
- Result: 在五种专业视觉任务上的实验表明，GCoT显著优于微调和蒸馏方法。
- Conclusion: GCoT通过改进CoT数据的忠实性，有效提升了MLLMs在专业视觉任务中的表现。


### [80] [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](https://arxiv.org/abs/2507.02860)
*Xin Zhou,Dingkang Liang,Kaijin Chen,Tianrui Feng,Xiwu Chen,Hongkai Lin,Yikang Ding,Feiyang Tan,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: EasyCache是一种无需训练的视频扩散模型加速框架，通过动态重用计算过的变换向量减少冗余计算，显著提升推理速度。

- Motivation: 视频生成模型因去噪过程的迭代性导致推理速度慢和计算成本高，限制了其广泛应用。
- Method: 提出轻量级、运行时自适应的缓存机制，动态重用变换向量，无需离线分析或参数调优。
- Result: 在多个大规模视频生成模型上实现2.1-3.3倍的推理加速，PSNR提升高达36%。
- Conclusion: EasyCache是一种高效且易于使用的高质量视频生成解决方案，适用于研究和实际应用。


### [81] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality是一种将RGB-D扫描转换为紧凑、逼真且交互式3D虚拟副本的新方法，支持高质量渲染和物理交互。

- Motivation: 旨在从室内环境的RGB-D扫描中创建逼真且功能完整的3D虚拟副本，适用于AR/VR、游戏和机器人等领域。
- Method: 通过场景理解、3D模型检索、材质增强和物理引擎集成，构建可编辑且兼容标准图形管线的场景。
- Result: 在Scan2CAD基准测试中达到最先进的相似性性能，并能处理严重不对齐、遮挡和光照不足的情况。
- Conclusion: LiteReality提供了一种高效且逼真的3D场景重建方法，适用于多种应用场景。


### [82] [RefTok: Reference-Based Tokenization for Video Generation](https://arxiv.org/abs/2507.02862)
*Xiang Fan,Xiaohang Sun,Kushan Thakkar,Zhu Liu,Vimal Bhat,Ranjay Krishna,Xiang Hao*

Main category: cs.CV

TL;DR: RefTok是一种基于参考的标记化方法，能有效捕捉视频中的时间依赖性和冗余性，显著优于现有方法。

- Motivation: 现有方法独立处理帧集，未能有效捕捉视频中的时间依赖性和冗余性。
- Method: 提出RefTok，通过未量化的参考帧编码和解码帧集，保留运动和外观连续性。
- Result: 在多个数据集上，RefTok显著优于现有方法，平均提升36.7%的指标，并在视频生成任务中表现更优。
- Conclusion: RefTok在视频建模中表现出色，为时间冗余问题提供了有效解决方案。


### [83] [Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory](https://arxiv.org/abs/2507.02863)
*Yuqi Wu,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Point3R提出了一种在线框架，通过显式空间指针内存实现密集流式3D重建，解决了隐式内存容量有限和信息丢失的问题。

- Motivation: 现有方法依赖隐式内存进行密集3D重建，但容量有限且可能导致早期帧信息丢失。
- Method: Point3R维护显式空间指针内存，每个指针关联特定3D位置并聚合场景信息，通过3D分层位置嵌入和融合机制实现高效交互。
- Result: 方法在多个任务中表现优异，训练成本低。
- Conclusion: Point3R通过显式内存设计，实现了高效且均匀的密集3D重建。
## cs.RO

### [84] [MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping](https://arxiv.org/abs/2507.02672)
*Qingyu Fan,Yinghao Cai,Chao Li,Chunting Jiao,Xudong Zheng,Tao Lu,Bin Liang,Shuo Wang*

Main category: cs.RO

TL;DR: MISCGrasp是一种体积抓取方法，通过多尺度特征提取和对比特征增强实现自适应抓取，优于基线方法。

- Motivation: 解决机器人抓取在适应不同形状和大小物体时的挑战。
- Method: 结合多尺度特征提取和对比特征增强，使用Insight Transformer和Empower Transformer进行特征交互和选择。
- Result: 在模拟和真实环境中，MISCGrasp在桌面清理任务中表现优于基线方法。
- Conclusion: MISCGrasp通过多尺度对比学习和特征交互，实现了自适应抓取，性能优越。


### [85] [MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real](https://arxiv.org/abs/2507.02864)
*Renhao Wang,Haoran Geng,Tingle Li,Feishi Wang,Gopala Anumanchipalli,Philipp Wu,Trevor Darrell,Boyi Li,Pieter Abbeel,Jitendra Malik,Alexei A. Efros*

Main category: cs.RO

TL;DR: 论文提出MultiGen框架，通过将生成模型与传统物理模拟器结合，实现多感官模拟，解决了多模态模拟与真实世界转移的难题。

- Motivation: 现实世界中机器人需整合多感官信息，但多模态策略的大规模学习仍具挑战性，尤其是声音等难以模拟的模态。
- Method: 引入MultiGen框架，利用生成模型合成逼真音频，结合模拟视频，实现无需真实数据的多模态训练。
- Result: 在机器人倒水任务中，展示了零样本转移到真实世界的能力，适用于新容器和液体。
- Conclusion: 生成模型能够模拟难以建模的模态，并缩小多模态模拟与真实世界的差距。
## cs.AI

### [86] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的核心目标，强调其跨领域的共享结构和物理基础，并提出运动建模对理解智能系统行为的重要性。

- Motivation: 运动是生物系统的核心特征，但在AI研究中常被忽视。论文旨在推动运动作为独立且丰富的模态进行建模，以促进跨领域的理解和应用。
- Method: 提出将运动视为结构化、低维的建模目标，利用其物理约束和形态共享特性，开发能泛化于多样运动数据的模型。
- Result: 运动建模不仅能提升生成模型和控制能力，还能为生物与人工系统的行为理解提供统一基础。
- Conclusion: 运动是智能系统与世界互动的窗口，应作为AI研究的核心方向。
## eess.IV

### [87] [CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR](https://arxiv.org/abs/2507.02289)
*Wangbin Ding,Lei Li,Junyi Qiu,Bogen Lin,Mingjing Yang,Liqin Huang,Lianming Wu,Sihan Wang,Xiahai Zhuang*

Main category: eess.IV

TL;DR: CineMyoPS是一种端到端深度神经网络，仅通过电影CMR图像分割心肌病理（瘢痕和水肿），结合运动和解剖特征，提高了分割准确性。

- Motivation: 心肌梗死（MI）是全球主要死因之一，传统多序列CMR成像耗时且有侵入性，电影CMR快速无对比剂，但需新方法提取病理信息。
- Method: 设计CineMyoPS网络，提取MI相关的运动和解剖特征，采用一致性损失联合学习，并通过时间序列聚合策略整合心脏周期特征。
- Result: 在多中心数据集上，CineMyoPS在心肌病理分割、运动估计和解剖分割中表现优异。
- Conclusion: CineMyoPS为无创、快速的心肌病理评估提供了有效工具，具有临床应用潜力。


### [88] [A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\left[^{18}\text{F}\right]$FDG PET imaging](https://arxiv.org/abs/2507.02367)
*Christian Salomonsen,Luigi Tommaso Luppino,Fredrik Aspheim,Kristoffer Wickstrøm,Elisabeth Wetzer,Michael Kampffmeyer,Rodrigo Berzaghi,Rune Sundset,Robert Jenssen,Samuel Kuttner*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的非侵入性方法（FC-DLIF），用于从小动物动态PET成像中预测输入函数，避免了传统动脉采血的复杂性和局限性。

- Motivation: 传统动脉采血在小动物研究中复杂、耗时且不可重复，限制了纵向研究。
- Method: 使用全卷积深度学习模型（FC-DLIF），从PET图像中提取时空特征预测输入函数。
- Result: FC-DLIF能可靠预测输入函数，对时间偏移和扫描时长变化具有鲁棒性，但对未训练的示踪剂效果不佳。
- Conclusion: FC-DLIF为非侵入性输入函数预测提供了可靠替代方案，适用于动态小动物PET研究。


### [89] [3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices](https://arxiv.org/abs/2507.02411)
*Zhurong Chen,Jinhua Chen,Wei Zhuo,Wufeng Xue,Dong Ni*

Main category: eess.IV

TL;DR: 提出一种从2D超声切片重建个性化3D心脏解剖的创新框架，显著提升左心室和右心室体积估计的准确性。

- Motivation: 2D超声成像在心脏疾病临床实践中存在局限性，如难以准确估计心室体积，而3D超声成像又受限于分辨率和手动标注的高要求。
- Method: 设计了一种新颖的3D重建流程，通过交替优化2D切片的3D姿态估计和基于隐式神经网络的3D切片整合，逐步将先验3D心脏形状转化为个性化模型。
- Result: 使用六个平面时，3D重建心脏显著改善了左心室体积估计（误差1.98% vs. 20.24%），并首次实现了从2D切片估计右心室体积（误差5.75%）。
- Conclusion: 该研究为心脏超声的个性化3D结构和功能分析提供了新方法，具有重要的临床应用潜力。


### [90] [MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak Boundary Polyp Detection](https://arxiv.org/abs/2507.02668)
*Zhe Yee Tan*

Main category: eess.IV

TL;DR: MEGANet-W提出了一种基于小波的边缘引导注意力网络，用于结直肠息肉分割，显著提升了分割精度。

- Motivation: 结直肠息肉分割对早期癌症检测至关重要，但现有方法在弱边界和低对比度条件下表现不佳。
- Method: MEGANet-W通过无参数的Haar小波边缘图引导注意力，结合两阶段小波头和多方向边缘提取，优化语义特征。
- Result: 在五个公开数据集上，MEGANet-W的mIoU和mDice分别提升2.3%和1.2%，且无需额外参数。
- Conclusion: MEGANet-W通过小波引导的注意力机制，显著提升了息肉分割的精度和鲁棒性。
## cs.GR

### [91] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 提出了一种高效的图像照明近似方法，用于动态材质和环境贴图下的闪烁效果实时渲染。

- Motivation: 解决实时渲染中离散微面引起的闪烁或闪光效果的挑战。
- Method: 基于区域光照明下的实时闪烁渲染，结合环境贴图过滤技术，使用正态分布函数和双门高斯近似。
- Result: 验证了方法在多种材质和光照条件下接近真实渲染，性能稳定且开销低。
- Conclusion: 方法在实时渲染中高效且接近真实，仅需两倍内存存储预过滤环境贴图。
## q-bio.QM

### [92] [TubuleTracker: a high-fidelity shareware software to quantify angiogenesis architecture and maturity](https://arxiv.org/abs/2507.02024)
*Danish Mahmood,Stephanie Buczkowski,Sahaj Shah,Autumn Anthony,Rohini Desetty,Carlo R Bartoli*

Main category: q-bio.QM

TL;DR: 该论文介绍了tubuleTracker，一种快速、客观量化内皮细胞网络结构和成熟度的软件工具，解决了传统手动和ImageJ分析方法的耗时和主观性问题。

- Motivation: 传统的内皮细胞网络分析方法（如手动或ImageJ）耗时、主观且不准确，尤其是在网络复杂度增加时。因此，需要一种更高效、客观的工具来量化网络结构和成熟度。
- Method: 使用人脐静脉内皮细胞培养，通过相衬显微镜获取图像，分别由人工、ImageJ和tubuleTracker分析，比较分析时间和关键指标（如管状结构数量、长度、节点数等）。
- Result: tubuleTracker分析速度显著快于人工和ImageJ（6秒vs. 8分钟和58秒），且其指标（如管状结构数量、长度等）与血管生成成熟度评分显著相关。
- Conclusion: tubuleTracker比传统方法更快、更一致，尤其是血管圆形度能有效反映成熟度。该工具已作为免费共享软件提供给生物医学研究社区。
## cs.CL

### [93] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX提出了一种基于LoRA模块的新方法，解决了持续DAP中的计算成本高、数据顺序敏感和模型通用性问题。

- Motivation: 现有持续DAP方法存在计算成本高、对数据顺序敏感且无法为特定任务提供定制模型的问题。
- Method: 利用LoRA模块实现高效、并行且对域顺序鲁棒的域自适应预训练。
- Result: DoMIX能够为特定任务提供定制化预训练模型，并适用于标准LLM微调场景。
- Conclusion: DoMIX是一种高效且灵活的持续DAP方法，具有广泛的应用潜力。
## cs.LG

### [94] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 论文提出了一种基于能量最小化的新型模型EBTs，通过无监督学习实现通用推理能力，并在多模态任务中表现优于现有方法。

- Motivation: 现有推理方法存在模态或问题特定性，且需额外监督训练。本文探索是否可通过无监督学习实现通用推理能力。
- Method: 训练基于能量的Transformer（EBTs），通过能量最小化优化输入与候选预测的兼容性。
- Result: EBTs在训练和推理中表现优于Transformer++和Diffusion Transformers，泛化能力更强。
- Conclusion: EBTs为模型的学习和推理能力提供了新的扩展范式。


### [95] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 提出了一种高效的潜在扩散框架，结合变分自编码器和条件扩散模型，显著提升数据压缩性能。

- Motivation: 生成模型在条件设置下表现优异，但可控性和重建精度限制了其在实际数据压缩中的应用。
- Method: 通过变分自编码器和条件扩散模型结合，仅压缩少量关键帧作为条件输入，生成其余帧。
- Result: 实验显示，压缩比提升10倍，性能比学习型方法高63%。
- Conclusion: 该方法在减少存储成本的同时，实现了高精度的时空重建。


### [96] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMR的轻量级方法，用于动态数据流中的持续学习，通过选择性更新记忆缓冲区来应对概念漂移，性能接近完全重新学习但资源消耗更低。

- Motivation: 传统持续学习方法假设数据分布静态，忽视了现实数据流的动态性（如概念漂移），需要同时保持稳定性和快速适应性。
- Method: 提出Adaptive Memory Realignment (AMR)，通过选择性移除过时样本并补充新样本，调整记忆缓冲区以匹配新分布。
- Result: AMR在多个概念漂移数据集上表现优异，性能接近完全重新学习，但显著减少了标注和计算开销。
- Conclusion: AMR是一种可扩展的解决方案，能在非静态持续学习环境中平衡稳定性和可塑性。


### [97] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: 提出了一种名为L-VAE的新模型，通过学习损失函数的超参数和表示解耦，改进了β-VAE的动态权衡问题。

- Motivation: 解决β-VAE中超参数η需经验调整的问题，动态平衡解耦和重建损失。
- Method: L-VAE同时学习损失项权重和模型参数，并添加正则化项防止偏置。
- Result: 实验表明L-VAE在重建和解耦间取得平衡，性能优于多种VAE变体。
- Conclusion: L-VAE在解耦表示学习中表现优异，适用于多种数据集。


### [98] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: 论文提出了一种新的框架DAID，通过因果关系的视角解决深度伪造检测中公平性与泛化性的冲突，并在实验中验证了其优越性。

- Motivation: 深度伪造检测模型面临泛化性和公平性的冲突，现有方法难以同时优化这两个目标。论文首次揭示了公平性与泛化性之间的因果关系，并基于此提出解决方案。
- Method: 提出了DAID框架，包括两部分：1）人口统计属性感知的数据再平衡（逆倾向加权和子群特征归一化）；2）人口统计无关的特征聚合（使用对齐损失抑制敏感属性信号）。
- Result: 在三个跨域基准测试中，DAID在公平性和泛化性上均优于现有方法。
- Conclusion: DAID通过控制混杂因素和公平性干预，成功解决了公平性与泛化性的冲突，验证了其理论和实践的有效性。


### [99] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: 提出一种基于差分隐私生成模型的数据共享方法，通过基础模型提取紧凑嵌入，降低计算开销，支持多样化下游任务。

- Motivation: 解决深度学习在医疗影像中数据稀缺和隐私限制的问题，同时克服联邦学习的高通信成本和任务单一性。
- Method: 采用差分隐私条件变分自编码器（DP-CVAE）建模全局隐私感知数据分布，通过基础模型提取嵌入。
- Result: 方法在隐私性、可扩展性和效率上优于传统联邦学习分类器，且DP-CVAE比DP-CGAN生成更高保真度的嵌入，参数需求减少5倍。
- Conclusion: 该方法为医疗影像中的隐私保护和多任务学习提供了高效解决方案。
