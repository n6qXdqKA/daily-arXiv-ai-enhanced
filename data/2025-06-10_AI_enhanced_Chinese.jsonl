{"id": "2506.06283", "pdf": "https://arxiv.org/pdf/2506.06283", "abs": "https://arxiv.org/abs/2506.06283", "authors": ["Juexiao Zhou", "Zhongyi Han", "Mankun Xin", "Xingwei He", "Guotao Wang", "Jiaoyan Song", "Gongning Luo", "Wenjia He", "Xintong Li", "Yuetan Chu", "Juanwen Chen", "Bo Wang", "Xia Wu", "Wenwen Duan", "Zhixia Guo", "Liyan Bai", "Yilin Pan", "Xuefei Bi", "Lu Liu", "Long Feng", "Xiaonan He", "Xin Gao"], "title": "Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Global population aging presents increasing challenges to healthcare systems,\nwith coronary artery disease (CAD) responsible for approximately 17.8 million\ndeaths annually, making it a leading cause of global mortality. As CAD is\nlargely preventable, early detection and proactive management are essential. In\nthis work, we introduce DigitalShadow, an advanced early warning system for\nCAD, powered by a fine-tuned facial foundation model. The system is pre-trained\non 21 million facial images and subsequently fine-tuned into LiveCAD, a\nspecialized CAD risk assessment model trained on 7,004 facial images from 1,751\nsubjects across four hospitals in China. DigitalShadow functions passively and\ncontactlessly, extracting facial features from live video streams without\nrequiring active user engagement. Integrated with a personalized database, it\ngenerates natural language risk reports and individualized health\nrecommendations. With privacy as a core design principle, DigitalShadow\nsupports local deployment to ensure secure handling of user data.", "AI": {"tldr": "DigitalShadow\u662f\u4e00\u4e2a\u57fa\u4e8e\u9762\u90e8\u57fa\u7840\u6a21\u578b\u7684CAD\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff0c\u901a\u8fc7\u65e0\u63a5\u89e6\u65b9\u5f0f\u4ece\u89c6\u9891\u6d41\u4e2d\u63d0\u53d6\u9762\u90e8\u7279\u5f81\uff0c\u751f\u6210\u4e2a\u6027\u5316\u5065\u5eb7\u62a5\u544a\u3002", "motivation": "\u5168\u7403\u8001\u9f84\u5316\u52a0\u5267\uff0cCAD\u662f\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u65e9\u671f\u68c0\u6d4b\u548c\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7cfb\u7edf\u9884\u8bad\u7ec32100\u4e07\u5f20\u9762\u90e8\u56fe\u50cf\uff0c\u5fae\u8c03\u4e3aLiveCAD\u6a21\u578b\uff0c\u4f7f\u75287004\u5f20\u6765\u81ea1751\u540d\u53d7\u8bd5\u8005\u7684\u56fe\u50cf\u8fdb\u884cCAD\u98ce\u9669\u8bc4\u4f30\u3002", "result": "DigitalShadow\u80fd\u88ab\u52a8\u3001\u65e0\u63a5\u89e6\u5730\u751f\u6210\u4e2a\u6027\u5316\u98ce\u9669\u62a5\u544a\u548c\u5065\u5eb7\u5efa\u8bae\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4ee5\u9690\u79c1\u4e3a\u6838\u5fc3\uff0c\u652f\u6301\u672c\u5730\u90e8\u7f72\uff0c\u4e3aCAD\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06389", "pdf": "https://arxiv.org/pdf/2506.06389", "abs": "https://arxiv.org/abs/2506.06389", "authors": ["Rifat Sadik", "Tanvir Rahman", "Arpan Bhattacharjee", "Bikash Chandra Halder", "Ismail Hossain"], "title": "Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep learning models have shown remarkable success in dermatological image\nanalysis, offering potential for automated skin disease diagnosis. Previously,\nconvolutional neural network(CNN) based architectures have achieved immense\npopularity and success in computer vision (CV) based task like skin image\nrecognition, generation and video analysis. But with the emergence of\ntransformer based models, CV tasks are now are nowadays carrying out using\nthese models. Vision Transformers (ViTs) is such a transformer-based models\nthat have shown success in computer vision. It uses self-attention mechanisms\nto achieve state-of-the-art performance across various tasks. However, their\nreliance on global attention mechanisms makes them susceptible to adversarial\nperturbations. This paper aims to investigate the susceptibility of ViTs for\nmedical images to adversarial watermarking-a method that adds so-called\nimperceptible perturbations in order to fool models. By generating adversarial\nwatermarks through Projected Gradient Descent (PGD), we examine the\ntransferability of such attacks to CNNs and analyze the performance defense\nmechanism -- adversarial training. Results indicate that while performance is\nnot compromised for clean images, ViTs certainly become much more vulnerable to\nadversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,\nadversarial training raises it up to 90.0%.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u53d8\u6362\u5668\uff08ViTs\uff09\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u5bf9\u5bf9\u6297\u6027\u6c34\u5370\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4f46\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u53ef\u4ee5\u5927\u5e45\u63d0\u5347\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u53d8\u6362\u5668\uff08ViTs\uff09\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u5e94\u7528\uff0c\u7814\u7a76\u5176\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u8106\u5f31\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f7f\u7528\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08PGD\uff09\u751f\u6210\u5bf9\u6297\u6027\u6c34\u5370\uff0c\u6d4b\u8bd5ViTs\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5206\u6790\u5bf9\u6297\u8bad\u7ec3\u7684\u6548\u679c\u3002", "result": "ViTs\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u51c6\u786e\u7387\u964d\u81f327.6%\uff0c\u4f46\u5bf9\u6297\u8bad\u7ec3\u540e\u63d0\u5347\u81f390.0%\u3002", "conclusion": "ViTs\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u8f83\u4e3a\u8106\u5f31\uff0c\u4f46\u5bf9\u6297\u8bad\u7ec3\u662f\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u624b\u6bb5\u3002"}}
{"id": "2506.06480", "pdf": "https://arxiv.org/pdf/2506.06480", "abs": "https://arxiv.org/abs/2506.06480", "authors": ["A. Postlmayr", "P. Cosman", "S. Dey"], "title": "(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a fitness tracking system that enables remote monitoring for\nexercises using only a RGB smartphone camera, making fitness tracking more\nprivate, scalable, and cost effective. Although prior work explored automated\nexercise supervision, existing models are either too limited in exercise\nvariety or too complex for real-world deployment. Prior approaches typically\nfocus on a small set of exercises and fail to generalize across diverse\nmovements. In contrast, we develop a robust, multitask motion analysis model\ncapable of performing exercise detection and repetition counting across\nhundreds of exercises, a scale far beyond previous methods. We overcome\nprevious data limitations by assembling a large-scale fitness dataset, Olympia\ncovering more than 1,900 exercises. To our knowledge, our vision-language model\nis the first that can perform multiple tasks on skeletal fitness data. On\nOlympia, our model can detect exercises with 76.5% accuracy and count\nrepetitions with 85.3% off-by-one accuracy, using only RGB video. By presenting\na single vision-language transformer model for both exercise identification and\nrep counting, we take a significant step toward democratizing AI-powered\nfitness tracking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB\u667a\u80fd\u624b\u673a\u6444\u50cf\u5934\u7684\u8fdc\u7a0b\u5065\u8eab\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u5177\u6709\u9690\u79c1\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u73b0\u6709\u5065\u8eab\u8ffd\u8e2a\u6a21\u578b\u8981\u4e48\u8fd0\u52a8\u79cd\u7c7b\u6709\u9650\uff0c\u8981\u4e48\u8fc7\u4e8e\u590d\u6742\u96be\u4ee5\u90e8\u7f72\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u8fd0\u52a8\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u8fd0\u52a8\u5206\u6790\u6a21\u578b\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u5065\u8eab\u6570\u636e\u96c6Olympia\uff081900\u591a\u79cd\u8fd0\u52a8\uff09\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u68c0\u6d4b\u548c\u91cd\u590d\u8ba1\u6570\u3002", "result": "\u6a21\u578b\u5728Olympia\u6570\u636e\u96c6\u4e0a\u8fd0\u52a8\u68c0\u6d4b\u51c6\u786e\u738776.5%\uff0c\u91cd\u590d\u8ba1\u6570\u51c6\u786e\u738785.3%\u3002", "conclusion": "\u901a\u8fc7\u5355\u4e00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u8fd0\u52a8\u8bc6\u522b\u548c\u8ba1\u6570\uff0c\u63a8\u52a8\u4e86AI\u5065\u8eab\u8ffd\u8e2a\u7684\u666e\u53ca\u3002"}}
{"id": "2506.06517", "pdf": "https://arxiv.org/pdf/2506.06517", "abs": "https://arxiv.org/abs/2506.06517", "authors": ["Mingqi Jiang", "Chanho Kim", "Chen Ziwen", "Li Fuxin"], "title": "GS4: Generalizable Sparse Splatting Semantic SLAM", "categories": ["cs.CV"], "comment": "13 pages, 6 figures", "summary": "Traditional SLAM algorithms are excellent at camera tracking but might\ngenerate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting\n(GS) approaches have emerged as an option for SLAM with accurate, dense 3D map\nbuilding. However, existing GS-based SLAM methods rely on per-scene\noptimization which is time-consuming and does not generalize to diverse scenes\nwell. In this work, we introduce the first generalizable GS-based semantic SLAM\nalgorithm that incrementally builds and updates a 3D scene representation from\nan RGB-D video stream using a learned generalizable network. Our approach\nstarts from an RGB-D image recognition backbone to predict the Gaussian\nparameters from every downsampled and backprojected image location.\nAdditionally, we seamlessly integrate 3D semantic segmentation into our GS\nframework, bridging 3D mapping and recognition through a shared backbone. To\ncorrect localization drifting and floaters, we propose to optimize the GS for\nonly 1 iteration following global localization. We demonstrate state-of-the-art\nsemantic SLAM performance on the real-world benchmark ScanNet with an order of\nmagnitude fewer Gaussians compared to other recent GS-based methods, and\nshowcase our model's generalization capability through zero-shot transfer to\nthe NYUv2 and TUM RGB-D datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u7684\u53ef\u6cdb\u5316\u8bed\u4e49SLAM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7f51\u7edc\u589e\u91cf\u6784\u5efa3D\u573a\u666f\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSLAM\u5206\u8fa8\u7387\u4f4e\u548c\u73b0\u6709GS\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfSLAM\u751f\u6210\u7684\u5730\u56fe\u5206\u8fa8\u7387\u4f4e\u4e14\u4e0d\u5b8c\u6574\uff0c\u800c\u73b0\u6709GS\u65b9\u6cd5\u4f9d\u8d56\u573a\u666f\u4f18\u5316\uff0c\u8017\u65f6\u957f\u4e14\u6cdb\u5316\u6027\u5dee\u3002", "method": "\u4f7f\u7528RGB-D\u56fe\u50cf\u8bc6\u522b\u4e3b\u5e72\u9884\u6d4b\u9ad8\u65af\u53c2\u6570\uff0c\u96c6\u62103D\u8bed\u4e49\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u5b9a\u4f4d\u540e\u4ec5\u4f18\u53161\u6b21\u9ad8\u65af\u6cfc\u6e85\u6765\u4fee\u6b63\u6f02\u79fb\u3002", "result": "\u5728ScanNet\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8bed\u4e49SLAM\u6027\u80fd\uff0c\u9ad8\u65af\u6570\u91cf\u663e\u8457\u51cf\u5c11\uff0c\u5e76\u5728NYUv2\u548cTUM RGB-D\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u5f3a\uff0c\u4e3a\u8bed\u4e49SLAM\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.06537", "pdf": "https://arxiv.org/pdf/2506.06537", "abs": "https://arxiv.org/abs/2506.06537", "authors": ["Seung-jae Lee", "Paul Hongsuck Seo"], "title": "Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted on INTERSPEECH2025", "summary": "Audiovisual segmentation (AVS) aims to identify visual regions corresponding\nto sound sources, playing a vital role in video understanding, surveillance,\nand human-computer interaction. Traditional AVS methods depend on large-scale\npixel-level annotations, which are costly and time-consuming to obtain. To\naddress this, we propose a novel zero-shot AVS framework that eliminates\ntask-specific training by leveraging multiple pretrained models. Our approach\nintegrates audio, vision, and text representations to bridge modality gaps,\nenabling precise sound source segmentation without AVS-specific annotations. We\nsystematically explore different strategies for connecting pretrained models\nand evaluate their efficacy across multiple datasets. Experimental results\ndemonstrate that our framework achieves state-of-the-art zero-shot AVS\nperformance, highlighting the effectiveness of multimodal model integration for\nfinegrained audiovisual segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u89c6\u542c\u5206\u5272\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u5206\u5272\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u50cf\u7d20\u7ea7\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u6574\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\uff0c\u63a2\u7d22\u9884\u8bad\u7ec3\u6a21\u578b\u8fde\u63a5\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u96f6\u6837\u672cSOTA\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001\u6a21\u578b\u6574\u5408\u5bf9\u7cbe\u7ec6\u89c6\u542c\u5206\u5272\u6709\u6548\u3002"}}
{"id": "2506.06563", "pdf": "https://arxiv.org/pdf/2506.06563", "abs": "https://arxiv.org/abs/2506.06563", "authors": ["Thushari Hapuarachchi", "Long Dang", "Kaiqi Xiong"], "title": "Securing Traffic Sign Recognition Systems in Autonomous Vehicles", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Deep Neural Networks (DNNs) are widely used for traffic sign recognition\nbecause they can automatically extract high-level features from images. These\nDNNs are trained on large-scale datasets obtained from unknown sources.\nTherefore, it is important to ensure that the models remain secure and are not\ncompromised or poisoned during training. In this paper, we investigate the\nrobustness of DNNs trained for traffic sign recognition. First, we perform the\nerror-minimizing attacks on DNNs used for traffic sign recognition by adding\nimperceptible perturbations on training data. Then, we propose a data\naugmentation-based training method to mitigate the error-minimizing attacks.\nThe proposed training method utilizes nonlinear transformations to disrupt the\nperturbations and improve the model robustness. We experiment with two\nwell-known traffic sign datasets to demonstrate the severity of the attack and\nthe effectiveness of our mitigation scheme. The error-minimizing attacks reduce\nthe prediction accuracy of the DNNs from 99.90% to 10.6%. However, our\nmitigation scheme successfully restores the prediction accuracy to 96.05%.\nMoreover, our approach outperforms adversarial training in mitigating the\nerror-minimizing attacks. Furthermore, we propose a detection model capable of\nidentifying poisoned data even when the perturbations are imperceptible to\nhuman inspection. Our detection model achieves a success rate of over 99% in\nidentifying the attack. This research highlights the need to employ advanced\ntraining methods for DNNs in traffic sign recognition systems to mitigate the\neffects of data poisoning attacks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u5728\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u62b5\u5fa1\u8bef\u5dee\u6700\u5c0f\u5316\u653b\u51fb\uff0c\u5e76\u5f00\u53d1\u4e86\u68c0\u6d4b\u6a21\u578b\u8bc6\u522b\u4e2d\u6bd2\u6570\u636e\u3002", "motivation": "\u7531\u4e8eDNNs\u5728\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u53ca\u5176\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u786e\u4fdd\u6a21\u578b\u5b89\u5168\u6027\u548c\u6297\u653b\u51fb\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6dfb\u52a0\u5fae\u5c0f\u6270\u52a8\u5b9e\u65bd\u8bef\u5dee\u6700\u5c0f\u5316\u653b\u51fb\uff0c\u63d0\u51fa\u57fa\u4e8e\u975e\u7ebf\u6027\u53d8\u6362\u7684\u6570\u636e\u589e\u5f3a\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u68c0\u6d4b\u6a21\u578b\u8bc6\u522b\u4e2d\u6bd2\u6570\u636e\u3002", "result": "\u653b\u51fb\u4f7fDNNs\u9884\u6d4b\u51c6\u786e\u7387\u4ece99.90%\u964d\u81f310.6%\uff0c\u4f46\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u5176\u6062\u590d\u81f396.05%\uff0c\u68c0\u6d4b\u6a21\u578b\u8bc6\u522b\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc799%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u4e2d\u91c7\u7528\u5148\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u62b5\u5fa1\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2506.06569", "pdf": "https://arxiv.org/pdf/2506.06569", "abs": "https://arxiv.org/abs/2506.06569", "authors": ["Yannis Spyridis", "Vasileios Argyriou"], "title": "Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automated sorting is crucial for improving the efficiency and scalability of\ntextile recycling, but accurately identifying material composition and\ndetecting contaminants from sensor data remains challenging. This paper\ninvestigates the use of standard RGB imagery, a cost-effective sensing\nmodality, for key pre-processing tasks in an automated system. We present\ncomputer vision components designed for a conveyor belt setup to perform (a)\nclassification of four common textile types and (b) segmentation of non-textile\nfeatures such as buttons and zippers. For classification, several pre-trained\narchitectures were evaluated using transfer learning and cross-validation, with\nEfficientNetB0 achieving the best performance on a held-out test set with\n81.25\\% accuracy. For feature segmentation, a zero-shot approach combining the\nGrounding DINO open-vocabulary detector with the Segment Anything Model (SAM)\nwas employed, demonstrating excellent performance with a mIoU of 0.90 for the\ngenerated masks against ground truth. This study demonstrates the feasibility\nof using RGB images coupled with modern deep learning techniques, including\ntransfer learning for classification and foundation models for zero-shot\nsegmentation, to enable essential analysis steps for automated textile\nrecycling pipelines.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528RGB\u56fe\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u5982\u8fc1\u79fb\u5b66\u4e60\u548c\u57fa\u7840\u6a21\u578b\uff09\u5b9e\u73b0\u7eba\u7ec7\u54c1\u81ea\u52a8\u5206\u7c7b\u548c\u6c61\u67d3\u7269\u5206\u5272\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7eba\u7ec7\u54c1\u56de\u6536\u4e2d\uff0c\u51c6\u786e\u8bc6\u522b\u6750\u6599\u6210\u5206\u548c\u68c0\u6d4b\u6c61\u67d3\u7269\u5bf9\u81ea\u52a8\u5316\u5206\u62e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u4f20\u611f\u5668\u6570\u636e\u65b9\u6cd5\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528RGB\u56fe\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5305\u62ec\u8fc1\u79fb\u5b66\u4e60\uff08EfficientNetB0\uff09\u548c\u96f6\u6837\u672c\u5206\u5272\uff08Grounding DINO + SAM\uff09\u3002", "result": "\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u8fbe81.25%\uff0c\u5206\u5272\u4efb\u52a1mIoU\u4e3a0.90\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RGB\u56fe\u50cf\u7ed3\u5408\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u53ef\u6709\u6548\u652f\u6301\u7eba\u7ec7\u54c1\u56de\u6536\u7684\u81ea\u52a8\u5316\u9884\u5904\u7406\u4efb\u52a1\u3002"}}
{"id": "2506.06578", "pdf": "https://arxiv.org/pdf/2506.06578", "abs": "https://arxiv.org/abs/2506.06578", "authors": ["Anees Nashath Shaik", "Barbara Villarini", "Vasileios Argyriou"], "title": "A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance", "categories": ["cs.CV"], "comment": null, "summary": "Surveillance systems play a critical role in security and reconnaissance, but\ntheir performance is often compromised by low-quality images and videos,\nleading to reduced accuracy in face recognition. Additionally, existing\nAI-based facial analysis models suffer from biases related to skin tone\nvariations and partially occluded faces, further limiting their effectiveness\nin diverse real-world scenarios. These challenges are the results of data\nlimitations and imbalances, where available training datasets lack sufficient\ndiversity, resulting in unfair and unreliable facial recognition performance.\nTo address these issues, we propose a data-driven platform that enhances\nsurveillance capabilities by generating synthetic training data tailored to\ncompensate for dataset biases. Our approach leverages deep learning-based\nfacial attribute manipulation and reconstruction using autoencoders and\nGenerative Adversarial Networks (GANs) to create diverse and high-quality\nfacial datasets. Additionally, our system integrates an image enhancement\nmodule, improving the clarity of low-resolution or occluded faces in\nsurveillance footage. We evaluate our approach using the CelebA dataset,\ndemonstrating that the proposed platform enhances both training data diversity\nand model fairness. This work contributes to reducing bias in AI-based facial\nanalysis and improving surveillance accuracy in challenging environments,\nleading to fairer and more reliable security applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u5e73\u53f0\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u6765\u5f25\u8865\u6570\u636e\u96c6\u504f\u5dee\uff0c\u63d0\u5347\u76d1\u63a7\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u76d1\u63a7\u7cfb\u7edf\u56e0\u56fe\u50cf\u8d28\u91cf\u4f4e\u548cAI\u6a21\u578b\u5bf9\u80a4\u8272\u3001\u906e\u6321\u7684\u504f\u89c1\uff0c\u5bfc\u81f4\u8bc6\u522b\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u6570\u636e\u591a\u6837\u6027\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\uff08\u5982\u81ea\u52a8\u7f16\u7801\u5668\u548cGANs\uff09\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u6570\u636e\uff0c\u5e76\u96c6\u6210\u56fe\u50cf\u589e\u5f3a\u6a21\u5757\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u6216\u906e\u6321\u4eba\u8138\u7684\u6e05\u6670\u5ea6\u3002", "result": "\u5728CelebA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5e73\u53f0\u63d0\u5347\u4e86\u6570\u636e\u591a\u6837\u6027\u548c\u6a21\u578b\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u51cf\u5c11\u4e86AI\u9762\u90e8\u5206\u6790\u7684\u504f\u89c1\uff0c\u63d0\u5347\u4e86\u76d1\u63a7\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.06596", "pdf": "https://arxiv.org/pdf/2506.06596", "abs": "https://arxiv.org/abs/2506.06596", "authors": ["Youssef Farah", "Federico Paredes-Vall\u00e9s", "Guido De Croon", "Muhammad Ahmed Humais", "Hussain Sajwani", "Yahya Zweiri"], "title": "EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras", "categories": ["cs.CV"], "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Nashville, 2025", "summary": "Event cameras are novel bio-inspired sensors that capture motion dynamics\nwith much higher temporal resolution than traditional cameras, since pixels\nreact asynchronously to brightness changes. They are therefore better suited\nfor tasks involving motion such as motion segmentation. However, training\nevent-based networks still represents a difficult challenge, as obtaining\nground truth is very expensive, error-prone and limited in frequency. In this\narticle, we introduce EV-LayerSegNet, a self-supervised CNN for event-based\nmotion segmentation. Inspired by a layered representation of the scene\ndynamics, we show that it is possible to learn affine optical flow and\nsegmentation masks separately, and use them to deblur the input events. The\ndeblurring quality is then measured and used as self-supervised learning loss.\nWe train and test the network on a simulated dataset with only affine motion,\nachieving IoU and detection rate up to 71% and 87% respectively.", "AI": {"tldr": "EV-LayerSegNet\u662f\u4e00\u79cd\u81ea\u76d1\u7763CNN\uff0c\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u8fd0\u52a8\u5206\u5272\uff0c\u901a\u8fc7\u5b66\u4e60\u4eff\u5c04\u5149\u6d41\u548c\u5206\u5272\u63a9\u7801\u6765\u53bb\u6a21\u7cca\u8f93\u5165\u4e8b\u4ef6\uff0c\u5e76\u4ee5\u53bb\u6a21\u7cca\u8d28\u91cf\u4f5c\u4e3a\u81ea\u76d1\u7763\u5b66\u4e60\u635f\u5931\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u6355\u6349\u8fd0\u52a8\u52a8\u6001\u65f6\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u4f46\u83b7\u53d6\u5730\u9762\u771f\u5b9e\u6570\u636e\u6602\u8d35\u4e14\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEV-LayerSegNet\uff0c\u901a\u8fc7\u5206\u5c42\u573a\u666f\u52a8\u6001\u8868\u793a\uff0c\u5206\u522b\u5b66\u4e60\u4eff\u5c04\u5149\u6d41\u548c\u5206\u5272\u63a9\u7801\uff0c\u5e76\u7528\u4e8e\u53bb\u6a21\u7cca\u8f93\u5165\u4e8b\u4ef6\u3002", "result": "\u5728\u4ec5\u542b\u4eff\u5c04\u8fd0\u52a8\u7684\u6a21\u62df\u6570\u636e\u96c6\u4e0a\uff0cIoU\u548c\u68c0\u6d4b\u7387\u5206\u522b\u8fbe\u523071%\u548c87%\u3002", "conclusion": "EV-LayerSegNet\u5c55\u793a\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u4e8b\u4ef6\u76f8\u673a\u8fd0\u52a8\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.06600", "pdf": "https://arxiv.org/pdf/2506.06600", "abs": "https://arxiv.org/abs/2506.06600", "authors": ["Tan-Hanh Pham", "Chris Ngo"], "title": "RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints", "categories": ["cs.CV"], "comment": "Under review", "summary": "The growing integration of vision-language models (VLMs) in medical\napplications offers promising support for diagnostic reasoning. However,\ncurrent medical VLMs often face limitations in generalization, transparency,\nand computational efficiency-barriers that hinder deployment in real-world,\nresource-constrained settings. To address these challenges, we propose a\nReasoning-Aware Reinforcement Learning framework, \\textbf{RARL}, that enhances\nthe reasoning capabilities of medical VLMs while remaining efficient and\nadaptable to low-resource environments. Our approach fine-tunes a lightweight\nbase model, Qwen2-VL-2B-Instruct, using Low-Rank Adaptation and custom reward\nfunctions that jointly consider diagnostic accuracy and reasoning quality.\nTraining is performed on a single NVIDIA A100-PCIE-40GB GPU, demonstrating the\nfeasibility of deploying such models in constrained environments. We evaluate\nthe model using an LLM-as-judge framework that scores both correctness and\nexplanation quality. Experimental results show that RARL significantly improves\nVLM performance in medical image analysis and clinical reasoning, outperforming\nsupervised fine-tuning on reasoning-focused tasks by approximately 7.78%, while\nrequiring fewer computational resources. Additionally, we demonstrate the\ngeneralization capabilities of our approach on unseen datasets, achieving\naround 27% improved performance compared to supervised fine-tuning and about 4%\nover traditional RL fine-tuning. Our experiments also illustrate that diversity\nprompting during training and reasoning prompting during inference are crucial\nfor enhancing VLM performance. Our findings highlight the potential of\nreasoning-guided learning and reasoning prompting to steer medical VLMs toward\nmore transparent, accurate, and resource-efficient clinical decision-making.\nCode and data are publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRARL\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u548c\u9002\u5e94\u4f4e\u8d44\u6e90\u73af\u5883\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6cdb\u5316\u6027\u3001\u900f\u660e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u963b\u788d\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94\u548c\u81ea\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u5bf9\u8f7b\u91cf\u7ea7\u57fa\u7840\u6a21\u578bQwen2-VL-2B-Instruct\u8fdb\u884c\u5fae\u8c03\uff0c\u7ed3\u5408\u8bca\u65ad\u51c6\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\u3002", "result": "RARL\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u6790\u548c\u4e34\u5e8a\u63a8\u7406\u7684\u6027\u80fd\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u6bd4\u76d1\u7763\u5fae\u8c03\u9ad8\u51fa\u7ea67.78%\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "\u63a8\u7406\u5f15\u5bfc\u5b66\u4e60\u548c\u63a8\u7406\u63d0\u793a\u80fd\u6709\u6548\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u900f\u660e\u6027\u3001\u51c6\u786e\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2506.06602", "pdf": "https://arxiv.org/pdf/2506.06602", "abs": "https://arxiv.org/abs/2506.06602", "authors": ["Santhosh Kakarla", "Gautama Shastry Bulusu Venkata"], "title": "Zero Shot Composed Image Retrieval", "categories": ["cs.CV"], "comment": "8 pages, 3 figures", "summary": "Composed image retrieval (CIR) allows a user to locate a target image by\napplying a fine-grained textual edit (e.g., ``turn the dress blue'' or ``remove\nstripes'') to a reference image. Zero-shot CIR, which embeds the image and the\ntext with separate pretrained vision-language encoders, reaches only 20-25\\%\nRecall@10 on the FashionIQ benchmark. We improve this by fine-tuning BLIP-2\nwith a lightweight Q-Former that fuses visual and textual features into a\nsingle embedding, raising Recall@10 to 45.6\\% (shirt), 40.1\\% (dress), and\n50.4\\% (top-tee) and increasing the average Recall@50 to 67.6\\%. We also\nexamine Retrieval-DPO, which fine-tunes CLIP's text encoder with a Direct\nPreference Optimization loss applied to FAISS-mined hard negatives. Despite\nextensive tuning of the scaling factor, index, and sampling strategy,\nRetrieval-DPO attains only 0.02\\% Recall@10 -- far below zero-shot and\nprompt-tuned baselines -- because it (i) lacks joint image-text fusion, (ii)\nuses a margin objective misaligned with top-$K$ metrics, (iii) relies on\nlow-quality negatives, and (iv) keeps the vision and Transformer layers frozen.\nOur results show that effective preference-based CIR requires genuine\nmultimodal fusion, ranking-aware objectives, and carefully curated negatives.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6539\u8fdbBLIP-2\u548cQ-Former\u7684\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672cCIR\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5206\u6790\u4e86Retrieval-DPO\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672cCIR\u5728FashionIQ\u57fa\u51c6\u4e0aRecall@10\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u3002", "method": "1. \u5fae\u8c03BLIP-2\u548c\u8f7b\u91cf\u7ea7Q-Former\u4ee5\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff1b2. \u5c1d\u8bd5Retrieval-DPO\u65b9\u6cd5\u4f18\u5316CLIP\u6587\u672c\u7f16\u7801\u5668\u3002", "result": "BLIP-2\u65b9\u6cd5\u663e\u8457\u63d0\u5347Recall@10\uff0845.6%\u886c\u886b\uff0c40.1%\u88d9\u5b50\uff0c50.4%T\u6064\uff09\uff0c\u5e73\u5747Recall@50\u8fbe67.6%\uff1bRetrieval-DPO\u8868\u73b0\u6781\u5dee\uff080.02%\uff09\u3002", "conclusion": "\u6709\u6548\u7684CIR\u9700\u8981\u591a\u6a21\u6001\u878d\u5408\u3001\u6392\u540d\u611f\u77e5\u76ee\u6807\u548c\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\uff0cRetrieval-DPO\u56e0\u8bbe\u8ba1\u7f3a\u9677\u8868\u73b0\u4e0d\u4f73\u3002"}}
{"id": "2506.06631", "pdf": "https://arxiv.org/pdf/2506.06631", "abs": "https://arxiv.org/abs/2506.06631", "authors": ["Minghao Zou", "Qingtian Zeng", "Yongping Miao", "Shangkun Liu", "Zilong Wang", "Hantao Liu", "Wei Zhou"], "title": "PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments", "categories": ["cs.CV"], "comment": null, "summary": "Visual parsing of images and videos is critical for a wide range of\nreal-world applications. However, progress in this field is constrained by\nlimitations of existing datasets: (1) insufficient annotation granularity,\nwhich impedes fine-grained scene understanding and high-level reasoning; (2)\nlimited coverage of domains, particularly a lack of datasets tailored for\neducational scenarios; and (3) lack of explicit procedural guidance, with\nminimal logical rules and insufficient representation of structured task\nprocess. To address these gaps, we introduce PhysLab, the first video dataset\nthat captures students conducting complex physics experiments. The dataset\nincludes four representative experiments that feature diverse scientific\ninstruments and rich human-object interaction (HOI) patterns. PhysLab comprises\n620 long-form videos and provides multilevel annotations that support a variety\nof vision tasks, including action recognition, object detection, HOI analysis,\netc. We establish strong baselines and perform extensive evaluations to\nhighlight key challenges in the parsing of procedural educational videos. We\nexpect PhysLab to serve as a valuable resource for advancing fine-grained\nvisual parsing, facilitating intelligent classroom systems, and fostering\ncloser integration between computer vision and educational technologies. The\ndataset and the evaluation toolkit are publicly available at\nhttps://github.com/ZMH-SDUST/PhysLab.", "AI": {"tldr": "PhysLab\u662f\u4e00\u4e2a\u9488\u5bf9\u6559\u80b2\u573a\u666f\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u590d\u6742\u7269\u7406\u5b9e\u9a8c\uff0c\u63d0\u4f9b\u591a\u7ea7\u6ce8\u91ca\u4ee5\u652f\u6301\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5728\u6807\u6ce8\u7c92\u5ea6\u3001\u9886\u57df\u8986\u76d6\u548c\u7a0b\u5e8f\u6307\u5bfc\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u89e3\u6790\u7684\u8fdb\u5c55\u3002", "method": "\u5f15\u5165PhysLab\u6570\u636e\u96c6\uff0c\u5305\u542b620\u4e2a\u957f\u89c6\u9891\u548c\u56db\u7c7b\u4ee3\u8868\u6027\u5b9e\u9a8c\uff0c\u652f\u6301\u52a8\u4f5c\u8bc6\u522b\u3001\u7269\u4f53\u68c0\u6d4b\u7b49\u4efb\u52a1\u3002", "result": "\u5efa\u7acb\u4e86\u5f3a\u57fa\u7ebf\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u7a0b\u5e8f\u6027\u6559\u80b2\u89c6\u9891\u89e3\u6790\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "PhysLab\u6709\u671b\u63a8\u52a8\u7ec6\u7c92\u5ea6\u89c6\u89c9\u89e3\u6790\u548c\u667a\u80fd\u8bfe\u5802\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.06643", "pdf": "https://arxiv.org/pdf/2506.06643", "abs": "https://arxiv.org/abs/2506.06643", "authors": ["Moushumi Medhi", "Rajiv Ranjan Sahay"], "title": "Dark Channel-Assisted Depth-from-Defocus from a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we utilize the dark channel as a complementary cue to estimate\nthe depth of a scene from a single space-variant defocus blurred image due to\nits effectiveness in implicitly capturing the local statistics of blurred\nimages and the scene structure. Existing depth-from-defocus (DFD) techniques\ntypically rely on multiple images with varying apertures or focus settings to\nrecover depth information. Very few attempts have focused on DFD from a single\ndefocused image due to the underconstrained nature of the problem. Our method\ncapitalizes on the relationship between local defocus blur and contrast\nvariations as key depth cues to enhance the overall performance in estimating\nthe scene's structure. The entire pipeline is trained adversarially in a fully\nend-to-end fashion. Experiments conducted on real data with realistic\ndepth-induced defocus blur demonstrate that incorporating dark channel prior\ninto single image DFD yields meaningful depth estimation results, validating\nthe effectiveness of our approach.", "AI": {"tldr": "\u5229\u7528\u6697\u901a\u9053\u4f5c\u4e3a\u8865\u5145\u7ebf\u7d22\uff0c\u4ece\u5355\u5f20\u7a7a\u95f4\u53d8\u5f02\u6563\u7126\u6a21\u7cca\u56fe\u50cf\u4e2d\u4f30\u8ba1\u573a\u666f\u6df1\u5ea6\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4ece\u6563\u7126\uff08DFD\uff09\u6280\u672f\u901a\u5e38\u4f9d\u8d56\u591a\u5f20\u56fe\u50cf\uff0c\u800c\u5355\u5f20\u6563\u7126\u56fe\u50cf\u7684\u6df1\u5ea6\u4f30\u8ba1\u95ee\u9898\u56e0\u6b20\u7ea6\u675f\u800c\u5c11\u6709\u7814\u7a76\u3002", "method": "\u5229\u7528\u5c40\u90e8\u6563\u7126\u6a21\u7cca\u4e0e\u5bf9\u6bd4\u5ea6\u53d8\u5316\u7684\u5173\u7cfb\u4f5c\u4e3a\u5173\u952e\u6df1\u5ea6\u7ebf\u7d22\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5bf9\u6297\u8bad\u7ec3\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u6697\u901a\u9053\u5148\u9a8c\u7684\u5355\u56fe\u50cfDFD\u65b9\u6cd5\u80fd\u6709\u6548\u4f30\u8ba1\u6df1\u5ea6\u3002", "conclusion": "\u6697\u901a\u9053\u5148\u9a8c\u7684\u5f15\u5165\u63d0\u5347\u4e86\u5355\u56fe\u50cfDFD\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.06645", "pdf": "https://arxiv.org/pdf/2506.06645", "abs": "https://arxiv.org/abs/2506.06645", "authors": ["Cheng Peng", "Jingxiang Sun", "Yushuo Chen", "Zhaoqi Su", "Zhuo Su", "Yebin Liu"], "title": "Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling", "categories": ["cs.CV"], "comment": "Project Page: https://pengc02.github.io/pghm/", "summary": "Photorealistic and animatable human avatars are a key enabler for\nvirtual/augmented reality, telepresence, and digital entertainment. While\nrecent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering\nquality and efficiency, existing methods still face fundamental challenges,\nincluding time-consuming per-subject optimization and poor generalization under\nsparse monocular inputs. In this work, we present the Parametric Gaussian Human\nModel (PGHM), a generalizable and efficient framework that integrates human\npriors into 3DGS for fast and high-fidelity avatar reconstruction from\nmonocular videos. PGHM introduces two core components: (1) a UV-aligned latent\nidentity map that compactly encodes subject-specific geometry and appearance\ninto a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that\npredicts Gaussian attributes by decomposing static, pose-dependent, and\nview-dependent components via conditioned decoders. This design enables robust\nrendering quality under challenging poses and viewpoints, while allowing\nefficient subject adaptation without requiring multi-view capture or long\noptimization time. Experiments show that PGHM is significantly more efficient\nthan optimization-from-scratch methods, requiring only approximately 20 minutes\nper subject to produce avatars with comparable visual quality, thereby\ndemonstrating its practical applicability for real-world monocular avatar\ncreation.", "AI": {"tldr": "PGHM\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u901a\u7528\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165UV\u5bf9\u9f50\u7684\u6f5c\u5728\u8eab\u4efd\u56fe\u548c\u5206\u79bb\u7684\u591a\u5934U-Net\uff0c\u5b9e\u73b0\u4ece\u5355\u76ee\u89c6\u9891\u5feb\u901f\u91cd\u5efa\u9ad8\u4fdd\u771f\u4eba\u7c7b\u5316\u8eab\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u76ee\u8f93\u5165\u4e0b\u4f18\u5316\u8017\u65f6\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0cPGHM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u865a\u62df/\u589e\u5f3a\u73b0\u5b9e\u548c\u6570\u5b57\u5a31\u4e50\u4e2d\u5316\u8eab\u7684\u91cd\u5efa\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "PGHM\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aUV\u5bf9\u9f50\u7684\u6f5c\u5728\u8eab\u4efd\u56fe\u7f16\u7801\u51e0\u4f55\u4e0e\u5916\u89c2\uff0c\u5206\u79bb\u7684\u591a\u5934U-Net\u901a\u8fc7\u6761\u4ef6\u89e3\u7801\u5668\u9884\u6d4b\u9ad8\u65af\u5c5e\u6027\u3002", "result": "PGHM\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u4ec5\u9700\u7ea620\u5206\u949f\u5373\u53ef\u5b8c\u6210\u5355\u4e3b\u9898\u4f18\u5316\uff0c\u4e14\u6e32\u67d3\u8d28\u91cf\u4e0e\u4ece\u5934\u4f18\u5316\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "PGHM\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u5355\u76ee\u5316\u8eab\u521b\u5efa\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u9ad8\u6548\u9ad8\u4fdd\u771f\u5316\u8eab\u91cd\u5efa\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.06667", "pdf": "https://arxiv.org/pdf/2506.06667", "abs": "https://arxiv.org/abs/2506.06667", "authors": ["Yu-Hsuan Ho", "Ali Mostafavi"], "title": "Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Most post-disaster damage classifiers succeed only when destructive forces\nleave clear spectral or structural signatures -- conditions rarely present\nafter inundation. Consequently, existing models perform poorly at identifying\nflood-related building damages. The model presented in this study,\nFlood-DamageSense, addresses this gap as the first deep-learning framework\npurpose-built for building-level flood-damage assessment. The architecture\nfuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical\nbasemaps and an inherent flood-risk layer that encodes long-term exposure\nprobabilities, guiding the network toward plausibly affected structures even\nwhen compositional change is minimal. A multimodal Mamba backbone with a\nsemi-Siamese encoder and task-specific decoders jointly predicts (1) graded\nbuilding-damage states, (2) floodwater extent, and (3) building footprints.\nTraining and evaluation on Hurricane Harvey (2017) imagery from Harris County,\nTexas -- supported by insurance-derived property-damage extents -- show a mean\nF1 improvement of up to 19 percentage points over state-of-the-art baselines,\nwith the largest gains in the frequently misclassified \"minor\" and \"moderate\"\ndamage categories. Ablation studies identify the inherent-risk feature as the\nsingle most significant contributor to this performance boost. An end-to-end\npost-processing pipeline converts pixel-level outputs to actionable,\nbuilding-scale damage maps within minutes of image acquisition. By combining\nrisk-aware modeling with SAR's all-weather capability, Flood-DamageSense\ndelivers faster, finer-grained, and more reliable flood-damage intelligence to\nsupport post-disaster decision-making and resource allocation.", "AI": {"tldr": "Flood-DamageSense\u662f\u4e00\u79cd\u4e13\u4e3a\u6d2a\u6c34\u707e\u5bb3\u5efa\u7b51\u635f\u574f\u8bc4\u4f30\u8bbe\u8ba1\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u878d\u5408\u591a\u6e90\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u6d2a\u6c34\u707e\u5bb3\u5efa\u7b51\u635f\u574f\u8bc6\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u7834\u574f\u7279\u5f81\u4e0d\u660e\u663e\u3002", "method": "\u7ed3\u5408SAR/InSAR\u3001\u5149\u5b66\u5f71\u50cf\u548c\u6d2a\u6c34\u98ce\u9669\u5c42\uff0c\u91c7\u7528\u591a\u6a21\u6001Mamba\u67b6\u6784\uff0c\u9884\u6d4b\u635f\u574f\u7b49\u7ea7\u3001\u6d2a\u6c34\u8303\u56f4\u548c\u5efa\u7b51\u8f6e\u5ed3\u3002", "result": "\u5728Harvey\u98d3\u98ce\u6570\u636e\u4e0a\uff0cF1\u5206\u6570\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534719%\uff0c\u5c24\u5176\u5728\u8f7b\u5fae\u548c\u4e2d\u5ea6\u635f\u574f\u7c7b\u522b\u3002", "conclusion": "Flood-DamageSense\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u5efa\u6a21\u548cSAR\u5168\u5929\u5019\u80fd\u529b\uff0c\u63d0\u4f9b\u66f4\u5feb\u3001\u66f4\u7cbe\u7ec6\u7684\u6d2a\u6c34\u635f\u574f\u8bc4\u4f30\u3002"}}
{"id": "2506.06680", "pdf": "https://arxiv.org/pdf/2506.06680", "abs": "https://arxiv.org/abs/2506.06680", "authors": ["Radha Kodali", "Venkata Rao Dhulipalla", "Venkata Siva Kishor Tatavarty", "Madhavi Nadakuditi", "Bharadwaj Thiruveedhula", "Suryanarayana Gunnam", "Durga Prasad Bavirisetti"], "title": "Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Infertility has a considerable impact on individuals' quality of life,\naffecting them socially and psychologically, with projections indicating a rise\nin the upcoming years. In vitro fertilization (IVF) emerges as one of the\nprimary techniques within economically developed nations, employed to address\nthe rising problem of low fertility. Expert embryologists conventionally grade\nembryos by reviewing blastocyst images to select the most optimal for transfer,\nyet this process is time-consuming and lacks efficiency. Blastocyst images\nprovide a valuable resource for assessing embryo viability. In this study, we\nintroduce an explainable artificial intelligence (XAI) framework for\nclassifying embryos, employing a fusion of convolutional neural network (CNN)\nand long short-term memory (LSTM) architecture, referred to as CNN-LSTM.\nUtilizing deep learning, our model achieves high accuracy in embryo\nclassification while maintaining interpretability through XAI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN-LSTM\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5206\u7c7b\u80da\u80ce\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u80da\u80ce\u8bc4\u4f30\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u4e0d\u5b55\u75c7\u5bf9\u4e2a\u4eba\u751f\u6d3b\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u800c\u4f53\u5916\u53d7\u7cbe\uff08IVF\uff09\u662f\u89e3\u51b3\u4f4e\u751f\u80b2\u7387\u95ee\u9898\u7684\u4e3b\u8981\u6280\u672f\u4e4b\u4e00\u3002\u4f20\u7edf\u80da\u80ce\u8bc4\u4f30\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u8017\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528CNN-LSTM\u6df7\u5408\u67b6\u6784\u7684XAI\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u80da\u80ce\u56fe\u50cf\u7684\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u5e76\u4fdd\u6301\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6a21\u578b\u5728\u80da\u80ce\u5206\u7c7b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6\u901a\u8fc7XAI\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684CNN-LSTM\u6846\u67b6\u4e3a\u80da\u80ce\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63d0\u5347IVF\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2506.06710", "pdf": "https://arxiv.org/pdf/2506.06710", "abs": "https://arxiv.org/abs/2506.06710", "authors": ["Qianqian Zhao", "Chunle Guo", "Tianyi Zhang", "Junpei Zhang", "Peiyang Jia", "Tan Su", "Wenjie Jiang", "Chongyi Li"], "title": "A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Omnidirectional image and video super-resolution is a crucial research topic\nin low-level vision, playing an essential role in virtual reality and augmented\nreality applications. Its goal is to reconstruct high-resolution images or\nvideo frames from low-resolution inputs, thereby enhancing detail preservation\nand enabling more accurate scene analysis and interpretation. In recent years,\nnumerous innovative and effective approaches have been proposed, predominantly\nbased on deep learning techniques, involving diverse network architectures,\nloss functions, projection strategies, and training datasets. This paper\npresents a systematic review of recent progress in omnidirectional image and\nvideo super-resolution, focusing on deep learning-based methods. Given that\nexisting datasets predominantly rely on synthetic degradation and fall short in\ncapturing real-world distortions, we introduce a new dataset, 360Insta, that\ncomprises authentically degraded omnidirectional images and videos collected\nunder diverse conditions, including varying lighting, motion, and exposure\nsettings. This dataset addresses a critical gap in current omnidirectional\nbenchmarks and enables more robust evaluation of the generalization\ncapabilities of omnidirectional super-resolution methods. We conduct\ncomprehensive qualitative and quantitative evaluations of existing methods on\nboth public datasets and our proposed dataset. Furthermore, we provide a\nsystematic overview of the current status of research and discuss promising\ndirections for future exploration. All datasets, methods, and evaluation\nmetrics introduced in this work are publicly available and will be regularly\nupdated. Project page: https://github.com/nqian1/Survey-on-ODISR-and-ODVSR.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u65b9\u4f4d\u56fe\u50cf\u548c\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u771f\u5b9e\u9000\u5316\u6570\u636e\u96c6360Insta\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u5168\u65b9\u4f4d\u56fe\u50cf\u548c\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u5728\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u5408\u6210\u9000\u5316\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u771f\u5b9e\u9000\u5316\u6570\u636e\u96c6360Insta\uff0c\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "\u65b0\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u4e3a\u5168\u65b9\u4f4d\u8d85\u5206\u8fa8\u7387\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7efc\u8ff0\u548c\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.06712", "pdf": "https://arxiv.org/pdf/2506.06712", "abs": "https://arxiv.org/abs/2506.06712", "authors": ["Saiyu Hu", "Chunlei He", "Jianfeng Zhang", "Dexing Kong", "Shoujun Huang"], "title": "Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation", "categories": ["cs.CV", "math.AP"], "comment": null, "summary": "Parabolic mean curvature flow-driven active contour models (PMCF-ACMs) are\nwidely used in image segmentation, which however depend heavily on the\nselection of initial curve configurations. In this paper, we firstly propose\nseveral hyperbolic mean curvature flow-driven ACMs (HMCF-ACMs), which introduce\ntunable initial velocity fields, enabling adaptive optimization for diverse\nsegmentation scenarios. We shall prove that HMCF-ACMs are indeed normal flows\nand establish the numerical equivalence between dissipative HMCF formulations\nand certain wave equations using the level set method with signed distance\nfunction. Building on this framework, we furthermore develop hyperbolic\ndual-mode regularized flow-driven ACMs (HDRF-ACMs), which utilize smooth\nHeaviside functions for edge-aware force modulation to suppress over-diffusion\nnear weak boundaries. Then, we optimize a weighted fourth-order Runge-Kutta\nalgorithm with nine-point stencil spatial discretization when solving the\nabove-mentioned wave equations. Experiments show that both HMCF-ACMs and\nHDRF-ACMs could achieve more precise segmentations with superior noise\nresistance and numerical stability due to task-adaptive configurations of\ninitial velocities and initial contours.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u66f2\u5e73\u5747\u66f2\u7387\u6d41\u7684\u4e3b\u52a8\u8f6e\u5ed3\u6a21\u578b\uff08HMCF-ACMs\uff09\u548c\u53cc\u6a21\u5f0f\u6b63\u5219\u5316\u6d41\u9a71\u52a8\u7684\u4e3b\u52a8\u8f6e\u5ed3\u6a21\u578b\uff08HDRF-ACMs\uff09\uff0c\u901a\u8fc7\u53ef\u8c03\u521d\u59cb\u901f\u5ea6\u573a\u548c\u8fb9\u7f18\u611f\u77e5\u529b\u8c03\u5236\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u6297\u566a\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u629b\u7269\u7ebf\u5e73\u5747\u66f2\u7387\u6d41\u9a71\u52a8\u7684\u4e3b\u52a8\u8f6e\u5ed3\u6a21\u578b\uff08PMCF-ACMs\uff09\u5bf9\u521d\u59cb\u66f2\u7ebf\u914d\u7f6e\u4f9d\u8d56\u6027\u5f3a\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u53cc\u66f2\u5e73\u5747\u66f2\u7387\u6d41\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u63d0\u51faHMCF-ACMs\uff0c\u5f15\u5165\u53ef\u8c03\u521d\u59cb\u901f\u5ea6\u573a\uff1b2. \u8bc1\u660eHMCF-ACMs\u4e3a\u6cd5\u5411\u6d41\uff0c\u5e76\u5efa\u7acb\u5176\u4e0e\u6ce2\u52a8\u65b9\u7a0b\u7684\u6570\u503c\u7b49\u4ef7\u6027\uff1b3. \u5f00\u53d1HDRF-ACMs\uff0c\u5229\u7528\u5e73\u6ed1Heaviside\u51fd\u6570\u6291\u5236\u5f31\u8fb9\u754c\u5904\u7684\u8fc7\u5ea6\u6269\u6563\uff1b4. \u4f18\u5316\u52a0\u6743\u56db\u9636Runge-Kutta\u7b97\u6cd5\u6c42\u89e3\u6ce2\u52a8\u65b9\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHMCF-ACMs\u548cHDRF-ACMs\u5728\u521d\u59cb\u901f\u5ea6\u548c\u521d\u59cb\u8f6e\u5ed3\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u914d\u7f6e\u4e0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7cbe\u5ea6\u7684\u5206\u5272\uff0c\u5e76\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6297\u566a\u6027\u548c\u6570\u503c\u7a33\u5b9a\u6027\u3002", "conclusion": "HMCF-ACMs\u548cHDRF-ACMs\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u673a\u5236\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.06719", "pdf": "https://arxiv.org/pdf/2506.06719", "abs": "https://arxiv.org/abs/2506.06719", "authors": ["Mufhumudzi Muthivhi", "Jiahao Huo", "Fredrik Gustafsson", "Terence L. van Zyl"], "title": "Improving Wildlife Out-of-Distribution Detection: Africas Big Five", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mitigating human-wildlife conflict seeks to resolve unwanted encounters\nbetween these parties. Computer Vision provides a solution to identifying\nindividuals that might escalate into conflict, such as members of the Big Five\nAfrican animals. However, environments often contain several varied species.\nThe current state-of-the-art animal classification models are trained under a\nclosed-world assumption. They almost always remain overconfident in their\npredictions even when presented with unknown classes. This study investigates\nout-of-distribution (OOD) detection of wildlife, specifically the Big Five. To\nthis end, we select a parametric Nearest Class Mean (NCM) and a non-parametric\ncontrastive learning approach as baselines to take advantage of pretrained and\nprojected features from popular classification encoders. Moreover, we compare\nour baselines to various common OOD methods in the literature. The results show\nfeature-based methods reflect stronger generalisation capability across varying\nclassification thresholds. Specifically, NCM with ImageNet pre-trained features\nachieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the\nbest OOD methods, respectively. The code can be found here\nhttps://github.com/pxpana/BIG5OOD", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91ce\u751f\u52a8\u7269\uff08\u7279\u522b\u662f\u975e\u6d32\u4e94\u5927\u52a8\u7269\uff09\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u53c2\u6570\u5316\u548c\u975e\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u7279\u5f81\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u4e0e\u91ce\u751f\u52a8\u7269\u51b2\u7a81\u9700\u8981\u51c6\u786e\u8bc6\u522b\u6f5c\u5728\u5a01\u80c1\u52a8\u7269\uff0c\u4f46\u73b0\u6709\u5206\u7c7b\u6a21\u578b\u5728\u672a\u77e5\u7c7b\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u53c2\u6570\u5316\u7684\u6700\u8fd1\u7c7b\u5747\u503c\uff08NCM\uff09\u548c\u975e\u53c2\u6570\u5316\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5176\u4ed6\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u7279\u5f81\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0cNCM\u7ed3\u5408ImageNet\u9884\u8bad\u7ec3\u7279\u5f81\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u7279\u5f81\u65b9\u6cd5\u5728\u91ce\u751f\u52a8\u7269\u5206\u5e03\u5916\u68c0\u6d4b\u4e2d\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.06729", "pdf": "https://arxiv.org/pdf/2506.06729", "abs": "https://arxiv.org/abs/2506.06729", "authors": ["Zixian Gao", "Chao Yang", "Zhanhui Zhou", "Xing Xu", "Chaochao Lu"], "title": "Mitigating Object Hallucination via Robust Local Perception Search", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled\nthem to effectively integrate vision and language, addressing a variety of\ndownstream tasks. However, despite their significant success, these models\nstill exhibit hallucination phenomena, where the outputs appear plausible but\ndo not align with the content of the images. To mitigate this issue, we\nintroduce Local Perception Search (LPS), a decoding method during inference\nthat is both simple and training-free, yet effectively suppresses\nhallucinations. This method leverages local visual prior information as a value\nfunction to correct the decoding process. Additionally, we observe that the\nimpact of the local visual prior on model performance is more pronounced in\nscenarios with high levels of image noise. Notably, LPS is a plug-and-play\napproach that is compatible with various models. Extensive experiments on\nwidely used hallucination benchmarks and noisy data demonstrate that LPS\nsignificantly reduces the incidence of hallucinations compared to the baseline,\nshowing exceptional performance, particularly in noisy settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLPS\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5c24\u5176\u5728\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u7ed3\u5408\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u8f93\u51fa\u4ecd\u5b58\u5728\u4e0e\u56fe\u50cf\u5185\u5bb9\u4e0d\u7b26\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Local Perception Search (LPS)\uff0c\u4e00\u79cd\u5229\u7528\u5c40\u90e8\u89c6\u89c9\u5148\u9a8c\u4fe1\u606f\u4f5c\u4e3a\u503c\u51fd\u6570\u6765\u4fee\u6b63\u89e3\u7801\u8fc7\u7a0b\u7684\u63a8\u7406\u9636\u6bb5\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLPS\u5728\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u548c\u566a\u58f0\u6570\u636e\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u5c24\u5176\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "LPS\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u517c\u5bb9\u591a\u79cd\u6a21\u578b\uff0c\u80fd\u6709\u6548\u6291\u5236\u5e7b\u89c9\u73b0\u8c61\uff0c\u5c24\u5176\u5728\u566a\u58f0\u73af\u5883\u4e2d\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2506.06733", "pdf": "https://arxiv.org/pdf/2506.06733", "abs": "https://arxiv.org/abs/2506.06733", "authors": ["Ruoxuan Zhang", "Jidong Gao", "Bin Wen", "Hongxia Xie", "Chenming Zhang", "Honghan-shuai", "Wen-Huang Cheng"], "title": "RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation", "categories": ["cs.CV"], "comment": "This is an extended version of arXiv:2503.05228", "summary": "Creating recipe images is a key challenge in food computing, with\napplications in culinary education and multimodal recipe assistants. However,\nexisting datasets lack fine-grained alignment between recipe goals, step-wise\ninstructions, and visual content. We present RecipeGen, the first large-scale,\nreal-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video\n(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,\n196,724 images, and 4,491 videos, covering diverse ingredients, cooking\nprocedures, styles, and dish types. We further propose domain-specific\nevaluation metrics to assess ingredient fidelity and interaction modeling,\nbenchmark representative T2I, I2V, and T2V models, and provide insights for\nfuture recipe generation models. Project page is available now.", "AI": {"tldr": "RecipeGen\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u771f\u5b9e\u4e16\u754c\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u57fa\u4e8e\u98df\u8c31\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u3001\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u548c\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u751f\u6210\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u98df\u8c31\u76ee\u6807\u3001\u5206\u6b65\u6307\u5bfc\u548c\u89c6\u89c9\u5185\u5bb9\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u98df\u54c1\u8ba1\u7b97\u5728\u70f9\u996a\u6559\u80b2\u548c\u591a\u6a21\u6001\u98df\u8c31\u52a9\u624b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86RecipeGen\u6570\u636e\u96c6\uff0c\u5305\u542b26,453\u4e2a\u98df\u8c31\u3001196,724\u5f20\u56fe\u7247\u548c4,491\u4e2a\u89c6\u9891\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7684\u98df\u6750\u3001\u70f9\u996a\u6b65\u9aa4\u3001\u98ce\u683c\u548c\u83dc\u54c1\u7c7b\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9886\u57df\u7279\u5b9a\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "RecipeGen\u4e3aT2I\u3001I2V\u548cT2V\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u98df\u6750\u4fdd\u771f\u5ea6\u548c\u4ea4\u4e92\u5efa\u6a21\u80fd\u529b\u3002", "conclusion": "RecipeGen\u4e3a\u672a\u6765\u7684\u98df\u8c31\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2506.06748", "pdf": "https://arxiv.org/pdf/2506.06748", "abs": "https://arxiv.org/abs/2506.06748", "authors": ["Mingqi Gao", "Haoran Duan", "Tianlu Zhang", "Jungong Han"], "title": "THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised Video Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In this report, we describe our approach to egocentric video object\nsegmentation. Our method combines large-scale visual pretraining from SAM2 with\ndepth-based geometric cues to handle complex scenes and long-term tracking. By\nintegrating these signals in a unified framework, we achieve strong\nsegmentation performance. On the VISOR test set, our method reaches a J&F score\nof 90.1%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u9884\u8bad\u7ec3\u548c\u6df1\u5ea6\u51e0\u4f55\u7ebf\u7d22\u7684\u81ea\u4e2d\u5fc3\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u65b9\u6cd5\uff0c\u5728VISOR\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523090.1%\u7684J&F\u5206\u6570\u3002", "motivation": "\u5904\u7406\u590d\u6742\u573a\u666f\u548c\u957f\u671f\u8ddf\u8e2a\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408SAM2\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u9884\u8bad\u7ec3\u548c\u6df1\u5ea6\u51e0\u4f55\u7ebf\u7d22\uff0c\u7edf\u4e00\u6846\u67b6\u5904\u7406\u3002", "result": "\u5728VISOR\u6d4b\u8bd5\u96c6\u4e0aJ&F\u5206\u6570\u4e3a90.1%\u3002", "conclusion": "\u7edf\u4e00\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2506.06757", "pdf": "https://arxiv.org/pdf/2506.06757", "abs": "https://arxiv.org/abs/2506.06757", "authors": ["Ziyu Yue", "Ruixi You", "Feng Xu"], "title": "SAR2Struct: Extracting 3D Semantic Structural Representation of Aircraft Targets from Single-View SAR Image", "categories": ["cs.CV"], "comment": "13 pages, 12 figures", "summary": "To translate synthetic aperture radar (SAR) image into interpretable forms\nfor human understanding is the ultimate goal of SAR advanced information\nretrieval. Existing methods mainly focus on 3D surface reconstruction or local\ngeometric feature extraction of targets, neglecting the role of structural\nmodeling in capturing semantic information. This paper proposes a novel task:\nSAR target structure recovery, which aims to infer the components of a target\nand the structural relationships between its components, specifically symmetry\nand adjacency, from a single-view SAR image. Through learning the structural\nconsistency and geometric diversity across the same type of targets as observed\nin different SAR images, it aims to derive the semantic representation of\ntarget directly from its 2D SAR image. To solve this challenging task, a\ntwo-step algorithmic framework based on structural descriptors is developed.\nSpecifically, in the training phase, it first detects 2D keypoints from real\nSAR images, and then learns the mapping from these keypoints to 3D hierarchical\nstructures using simulated data. During the testing phase, these two steps are\nintegrated to infer the 3D structure from real SAR images. Experimental results\nvalidated the effectiveness of each step and demonstrated, for the first time,\nthat 3D semantic structural representation of aircraft targets can be directly\nderived from a single-view SAR image.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u89c6\u89d2SAR\u56fe\u50cf\u63a8\u65ad\u76ee\u6807\u7ed3\u6784\u7684\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u4e24\u6b65\u7b97\u6cd5\u6846\u67b6\u5b9e\u73b03D\u8bed\u4e49\u7ed3\u6784\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7ed3\u6784\u5efa\u6a21\u5728\u8bed\u4e49\u4fe1\u606f\u6355\u6349\u4e2d\u7684\u4f5c\u7528\uff0c\u672c\u6587\u65e8\u5728\u76f4\u63a5\u4eceSAR\u56fe\u50cf\u4e2d\u6062\u590d\u76ee\u6807\u7684\u7ed3\u6784\u5173\u7cfb\u3002", "method": "\u57fa\u4e8e\u7ed3\u6784\u63cf\u8ff0\u7b26\u7684\u4e24\u6b65\u7b97\u6cd5\u6846\u67b6\uff1a\u8bad\u7ec3\u9636\u6bb5\u4ece\u771f\u5b9eSAR\u56fe\u50cf\u68c0\u6d4b2D\u5173\u952e\u70b9\u5e76\u5b66\u4e60\u5176\u52303D\u7ed3\u6784\u7684\u6620\u5c04\uff1b\u6d4b\u8bd5\u9636\u6bb5\u6574\u5408\u8fd9\u4e24\u6b65\u4ece\u771f\u5b9eSAR\u56fe\u50cf\u63a8\u65ad3D\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6b65\u9aa4\u7684\u6709\u6548\u6027\uff0c\u9996\u6b21\u8bc1\u660e\u53ef\u4ece\u5355\u89c6\u89d2SAR\u56fe\u50cf\u76f4\u63a5\u63a8\u5bfc\u98de\u673a\u76ee\u6807\u76843D\u8bed\u4e49\u7ed3\u6784\u8868\u793a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u4eceSAR\u56fe\u50cf\u4e2d\u6062\u590d\u76ee\u6807\u7ed3\u6784\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3aSAR\u9ad8\u7ea7\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.06759", "pdf": "https://arxiv.org/pdf/2506.06759", "abs": "https://arxiv.org/abs/2506.06759", "authors": ["Nidheesh Gorthi", "Kartik Thakral", "Rishabh Ranjan", "Richa Singh", "Mayank Vatsa"], "title": "LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security", "categories": ["cs.CV"], "comment": "Accepted in Interspeech 2025", "summary": "Biometric authentication systems are increasingly being deployed in critical\napplications, but they remain susceptible to spoofing. Since most of the\nresearch efforts focus on modality-specific anti-spoofing techniques, building\na unified, resource-efficient solution across multiple biometric modalities\nremains a challenge. To address this, we propose LitMAS, a\n$\\textbf{Li}$gh$\\textbf{t}$ weight and generalizable $\\textbf{M}$ulti-modal\n$\\textbf{A}$nti-$\\textbf{S}$poofing framework designed to detect spoofing\nattacks in speech, face, iris, and fingerprint-based biometric systems. At the\ncore of LitMAS is a Modality-Aligned Concentration Loss, which enhances\ninter-class separability while preserving cross-modal consistency and enabling\nrobust spoof detection across diverse biometric traits. With just 6M\nparameters, LitMAS surpasses state-of-the-art methods by $1.36\\%$ in average\nEER across seven datasets, demonstrating high efficiency, strong\ngeneralizability, and suitability for edge deployment. Code and trained models\nare available at https://github.com/IAB-IITJ/LitMAS.", "AI": {"tldr": "LitMAS\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u901a\u7528\u7684\u591a\u6a21\u6001\u53cd\u6b3a\u9a97\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u8bed\u97f3\u3001\u4eba\u8138\u3001\u8679\u819c\u548c\u6307\u7eb9\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u6b3a\u9a97\u653b\u51fb\u3002", "motivation": "\u751f\u7269\u8bc6\u522b\u8ba4\u8bc1\u7cfb\u7edf\u5728\u5173\u952e\u5e94\u7528\u4e2d\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u4ecd\u6613\u53d7\u6b3a\u9a97\u653b\u51fb\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u7279\u5b9a\u6a21\u6001\u7684\u53cd\u6b3a\u9a97\u6280\u672f\uff0c\u7f3a\u4e4f\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002", "method": "LitMAS\u91c7\u7528\u6a21\u6001\u5bf9\u9f50\u96c6\u4e2d\u635f\u5931\uff08Modality-Aligned Concentration Loss\uff09\uff0c\u589e\u5f3a\u7c7b\u95f4\u5206\u79bb\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u6b3a\u9a97\u68c0\u6d4b\u3002", "result": "LitMAS\u4ec5\u9700600\u4e07\u53c2\u6570\uff0c\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747EER\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u53471.36%\uff0c\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3001\u5f3a\u6cdb\u5316\u6027\u548c\u8fb9\u7f18\u90e8\u7f72\u9002\u7528\u6027\u3002", "conclusion": "LitMAS\u4e3a\u8de8\u6a21\u6001\u751f\u7269\u8bc6\u522b\u53cd\u6b3a\u9a97\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2506.06771", "pdf": "https://arxiv.org/pdf/2506.06771", "abs": "https://arxiv.org/abs/2506.06771", "authors": ["Mohammad-Maher Nakshbandi", "Ziad Sharawy", "Dorian Cojocaru", "Sorin Grigorescu"], "title": "LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization and Mapping", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this study, we introduce LoopDB, which is a challenging loop closure\ndataset comprising over 1000 images captured across diverse environments,\nincluding parks, indoor scenes, parking spaces, as well as centered around\nindividual objects. Each scene is represented by a sequence of five consecutive\nimages. The dataset was collected using a high resolution camera, providing\nsuitable imagery for benchmarking the accuracy of loop closure algorithms,\ntypically used in simultaneous localization and mapping. As ground truth\ninformation, we provide computed rotations and translations between each\nconsecutive images. Additional to its benchmarking goal, the dataset can be\nused to train and fine-tune loop closure methods based on deep neural networks.\nLoopDB is publicly available at https://github.com/RovisLab/LoopDB.", "AI": {"tldr": "LoopDB\u662f\u4e00\u4e2a\u5305\u542b1000\u591a\u5f20\u591a\u6837\u5316\u73af\u5883\u56fe\u50cf\u7684\u95ed\u73af\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3SLAM\u4e2d\u7684\u95ed\u73af\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u95ed\u73af\u68c0\u6d4b\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u652f\u6301SLAM\u6280\u672f\u7684\u6539\u8fdb\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u76f8\u673a\u91c7\u96c6\u591a\u6837\u5316\u573a\u666f\u7684\u4e94\u5f20\u8fde\u7eed\u56fe\u50cf\u5e8f\u5217\uff0c\u5e76\u63d0\u4f9b\u65cb\u8f6c\u548c\u5e73\u79fb\u7684\u771f\u503c\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\uff0c\u9002\u7528\u4e8e\u95ed\u73af\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u3002", "conclusion": "LoopDB\u4e3a\u95ed\u73af\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u652f\u6301\u7b97\u6cd5\u8bc4\u4f30\u548c\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u3002"}}
{"id": "2506.06780", "pdf": "https://arxiv.org/pdf/2506.06780", "abs": "https://arxiv.org/abs/2506.06780", "authors": ["Lennart Bastian", "Mohammad Rashed", "Nassir Navab", "Tolga Birdal"], "title": "Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations", "categories": ["cs.CV", "cs.LG"], "comment": "Extended abstract, presented at the CVPR Workshop on 4D Vision", "summary": "Tracking and forecasting the rotation of objects is fundamental in computer\nvision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor\nobservations can be noisy and sparse, (2) motion patterns can be governed by\ncomplex dynamics, and (3) application settings can demand long-term\nforecasting. This work proposes modeling continuous-time rotational object\ndynamics on $SO(3)$ using Neural Controlled Differential Equations guided by\nSavitzky-Golay paths. Unlike existing methods that rely on simplified motion\nassumptions, our method learns a general latent dynamical system of the\nunderlying object trajectory while respecting the geometric structure of\nrotations. Experimental results on real-world data demonstrate compelling\nforecasting capabilities compared to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u8fde\u7eed\u65f6\u95f4\u65cb\u8f6c\u7269\u4f53\u52a8\u529b\u5b66\uff0c\u89e3\u51b3\u4e86SO(3)\u5916\u63a8\u4e2d\u7684\u566a\u58f0\u3001\u7a00\u758f\u6027\u548c\u590d\u6742\u52a8\u6001\u95ee\u9898\u3002", "motivation": "SO(3)\u5916\u63a8\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u89c2\u6d4b\u566a\u58f0\u3001\u7a00\u758f\u6027\u3001\u590d\u6742\u52a8\u6001\u548c\u957f\u671f\u9884\u6d4b\u9700\u6c42\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u7ed3\u5408Savitzky-Golay\u8def\u5f84\uff0c\u5efa\u6a21\u65cb\u8f6c\u7269\u4f53\u52a8\u529b\u5b66\uff0c\u4fdd\u7559\u65cb\u8f6c\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u66f4\u5f3a\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51e0\u4f55\u7ed3\u6784\u4fdd\u7559\u548c\u52a8\u6001\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65cb\u8f6c\u7269\u4f53\u8f68\u8ff9\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2506.06802", "pdf": "https://arxiv.org/pdf/2506.06802", "abs": "https://arxiv.org/abs/2506.06802", "authors": ["Mohammad Ali Rezaei", "Helia Hajikazem", "Saeed Khanehgir", "Mahdi Javanmardi"], "title": "Training-Free Identity Preservation in Stylized Image Generation Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models have demonstrated remarkable generative capabilities,\nexisting style transfer techniques often struggle to maintain identity while\nachieving high-quality stylization. This limitation is particularly acute for\nimages where faces are small or exhibit significant camera-to-face distances,\nfrequently leading to inadequate identity preservation. To address this, we\nintroduce a novel, training-free framework for identity-preserved stylized\nimage synthesis using diffusion models. Key contributions include: (1) the\n\"Mosaic Restored Content Image\" technique, significantly enhancing identity\nretention, especially in complex scenes; and (2) a training-free content\nconsistency loss that enhances the preservation of fine-grained content details\nby directing more attention to the original image during stylization. Our\nexperiments reveal that the proposed approach substantially surpasses the\nbaseline model in concurrently maintaining high stylistic fidelity and robust\nidentity integrity, particularly under conditions of small facial regions or\nsignificant camera-to-face distances, all without necessitating model\nretraining or fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u8eab\u4efd\u4fdd\u7559\u7684\u98ce\u683c\u5316\u56fe\u50cf\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u5c0f\u9762\u90e8\u6216\u8fdc\u8ddd\u79bb\u62cd\u6444\u65f6\u8eab\u4efd\u4fdd\u7559\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u5728\u4fdd\u6301\u8eab\u4efd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u98ce\u683c\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u9762\u90e8\u6216\u8fdc\u8ddd\u79bb\u62cd\u6444\u7684\u56fe\u50cf\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a1) \"Mosaic Restored Content Image\" \u589e\u5f3a\u8eab\u4efd\u4fdd\u7559\uff1b2) \u65e0\u8bad\u7ec3\u5185\u5bb9\u4e00\u81f4\u6027\u635f\u5931\uff0c\u63d0\u9ad8\u7ec6\u8282\u4fdd\u7559\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u98ce\u683c\u4fdd\u771f\u5ea6\u548c\u8eab\u4efd\u5b8c\u6574\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u9762\u90e8\u6216\u8fdc\u8ddd\u79bb\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u7559\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2506.06818", "pdf": "https://arxiv.org/pdf/2506.06818", "abs": "https://arxiv.org/abs/2506.06818", "authors": ["Chao Yin", "Hao Li", "Kequan Yang", "Jide Li", "Pinpin Zhu", "Xiaoqiang Li"], "title": "Stepwise Decomposition and Dual-stream Focus: A Novel Approach for Training-free Camouflaged Object Segmentation", "categories": ["cs.CV"], "comment": "under review", "summary": "While promptable segmentation (\\textit{e.g.}, SAM) has shown promise for\nvarious segmentation tasks, it still requires manual visual prompts for each\nobject to be segmented. In contrast, task-generic promptable segmentation aims\nto reduce the need for such detailed prompts by employing only a task-generic\nprompt to guide segmentation across all test samples. However, when applied to\nCamouflaged Object Segmentation (COS), current methods still face two critical\nissues: 1) \\textit{\\textbf{semantic ambiguity in getting instance-specific text\nprompts}}, which arises from insufficient discriminative cues in holistic\ncaptions, leading to foreground-background confusion; 2)\n\\textit{\\textbf{semantic discrepancy combined with spatial separation in\ngetting instance-specific visual prompts}}, which results from global\nbackground sampling far from object boundaries with low feature correlation,\ncausing SAM to segment irrelevant regions. To address the issues above, we\npropose \\textbf{RDVP-MSD}, a novel training-free test-time adaptation framework\nthat synergizes \\textbf{R}egion-constrained \\textbf{D}ual-stream\n\\textbf{V}isual \\textbf{P}rompting (RDVP) via \\textbf{M}ultimodal\n\\textbf{S}tepwise \\textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT\nprogressively disentangles image captions to eliminate semantic ambiguity,\nwhile RDVP injects spatial constraints into visual prompting and independently\nsamples visual prompts for foreground and background points, effectively\nmitigating semantic discrepancy and spatial separation. Without requiring any\ntraining or supervision, RDVP-MSD achieves a state-of-the-art segmentation\nresult on multiple COS benchmarks and delivers a faster inference speed than\nprevious methods, demonstrating significantly improved accuracy and efficiency.\nThe codes will be available at\n\\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRDVP-MSD\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u9010\u6b65\u5206\u89e3\u94fe\u5f0f\u601d\u7ef4\uff08MSD-CoT\uff09\u548c\u533a\u57df\u7ea6\u675f\u53cc\u6d41\u89c6\u89c9\u63d0\u793a\uff08RDVP\uff09\uff0c\u89e3\u51b3\u4f2a\u88c5\u76ee\u6807\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u548c\u7a7a\u95f4\u5206\u79bb\u95ee\u9898\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u4efb\u52a1\u901a\u7528\u63d0\u793a\u5206\u5272\u65b9\u6cd5\u5728\u4f2a\u88c5\u76ee\u6807\u5206\u5272\uff08COS\uff09\u4e2d\u5b58\u5728\u8bed\u4e49\u6a21\u7cca\u548c\u7a7a\u95f4\u5206\u79bb\u95ee\u9898\uff0c\u5bfc\u81f4\u5206\u5272\u4e0d\u51c6\u786e\u3002", "method": "\u7ed3\u5408MSD-CoT\u9010\u6b65\u5206\u89e3\u56fe\u50cf\u63cf\u8ff0\u6d88\u9664\u8bed\u4e49\u6a21\u7cca\uff0cRDVP\u901a\u8fc7\u7a7a\u95f4\u7ea6\u675f\u548c\u53cc\u6d41\u89c6\u89c9\u63d0\u793a\u89e3\u51b3\u7a7a\u95f4\u5206\u79bb\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2aCOS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u5206\u5272\u6548\u679c\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "RDVP-MSD\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u5206\u5272\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3aCOS\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06822", "pdf": "https://arxiv.org/pdf/2506.06822", "abs": "https://arxiv.org/abs/2506.06822", "authors": ["Chenlu Zhan", "Yufei Zhang", "Gaoang Wang", "Hongwei Wang"], "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modeling 3D language fields with Gaussian Splatting for open-ended language\nqueries has recently garnered increasing attention. However, recent 3DGS-based\nmodels leverage view-dependent 2D foundation models to refine 3D semantics but\nlack a unified 3D representation, leading to view inconsistencies.\nAdditionally, inherent open-vocabulary challenges cause inconsistencies in\nobject and relational descriptions, impeding hierarchical semantic\nunderstanding. In this paper, we propose Hi-LSplat, a view-consistent\nHierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.\nTo achieve view-consistent 3D hierarchical semantics, we first lift 2D features\nto 3D features by constructing a 3D hierarchical semantic tree with layered\ninstance clustering, which addresses the view inconsistency issue caused by 2D\nsemantic features. Besides, we introduce instance-wise and part-wise\ncontrastive losses to capture all-sided hierarchical semantic representations.\nNotably, we construct two hierarchical semantic datasets to better assess the\nmodel's ability to distinguish different semantic levels. Extensive experiments\nhighlight our method's superiority in 3D open-vocabulary segmentation and\nlocalization. Its strong performance on hierarchical semantic datasets\nunderscores its ability to capture complex hierarchical semantics within 3D\nscenes.", "AI": {"tldr": "Hi-LSplat\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u5c42\u6b21\u5316\u8bed\u8a003D\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u89c6\u56fe\u4e0d\u4e00\u81f4\u548c\u5f00\u653e\u8bcd\u6c47\u6311\u6218\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u67093DGS\u6a21\u578b\u4f9d\u8d562D\u57fa\u7840\u6a21\u578b\uff0c\u5bfc\u81f4\u89c6\u56fe\u4e0d\u4e00\u81f4\u548c\u5c42\u6b21\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6784\u5efa3D\u5c42\u6b21\u8bed\u4e49\u6811\u548c\u5f15\u5165\u5b9e\u4f8b\u4e0e\u90e8\u4ef6\u5bf9\u6bd4\u635f\u5931\uff0c\u5b9e\u73b0\u89c6\u56fe\u4e00\u81f4\u76843D\u8bed\u4e49\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHi-LSplat\u57283D\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u548c\u5b9a\u4f4d\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u6355\u6349\u590d\u6742\u5c42\u6b21\u8bed\u4e49\u3002", "conclusion": "Hi-LSplat\u4e3a3D\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u63d0\u4f9b\u4e86\u89c6\u56fe\u4e00\u81f4\u4e14\u5c42\u6b21\u5316\u7684\u8bed\u4e49\u8868\u793a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06823", "pdf": "https://arxiv.org/pdf/2506.06823", "abs": "https://arxiv.org/abs/2506.06823", "authors": ["Qi Li", "Liangzhi Li", "Zhouqiang Jiang", "Bowen Wang", "Keke Tang"], "title": "Exploring Visual Prompting: Robustness Inheritance and Beyond", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.10992", "summary": "Visual Prompting (VP), an efficient method for transfer learning, has shown\nits potential in vision tasks. However, previous works focus exclusively on VP\nfrom standard source models, it is still unknown how it performs under the\nscenario of a robust source model: Can the robustness of the source model be\nsuccessfully inherited? Does VP also encounter the same trade-off between\nrobustness and generalization ability as the source model during this process?\nIf such a trade-off exists, is there a strategy specifically tailored to VP to\nmitigate this limitation? In this paper, we thoroughly explore these three\nquestions for the first time and provide affirmative answers to them. To\nmitigate the trade-off faced by VP, we propose a strategy called Prompt\nBoundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally\ncompatible with VP, PBL effectively ensures the successful inheritance of\nrobustness when the source model is a robust model, while significantly\nenhancing VP's generalization ability across various downstream datasets.\nExtensive experiments across various datasets show that our findings are\nuniversal and demonstrate the significant benefits of the proposed strategy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u89c6\u89c9\u63d0\u793a\uff08VP\uff09\u5728\u9c81\u68d2\u6e90\u6a21\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPrompt Boundary Loosening\uff08PBL\uff09\u7684\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3VP\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u7814\u7a76VP\u5728\u9c81\u68d2\u6e90\u6a21\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u5176\u662f\u5426\u80fd\u7ee7\u627f\u9c81\u68d2\u6027\uff0c\u662f\u5426\u5b58\u5728\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPBL\u7b56\u7565\uff0c\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u4e0eVP\u517c\u5bb9\uff0c\u65e8\u5728\u786e\u4fdd\u9c81\u68d2\u6027\u7684\u7ee7\u627f\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePBL\u80fd\u6709\u6548\u7ee7\u627f\u9c81\u68d2\u6027\u5e76\u663e\u8457\u63d0\u5347VP\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7ed3\u679c\u5177\u6709\u666e\u904d\u6027\u3002", "conclusion": "PBL\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86VP\u5728\u9c81\u68d2\u6e90\u6a21\u578b\u4e0b\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.06826", "pdf": "https://arxiv.org/pdf/2506.06826", "abs": "https://arxiv.org/abs/2506.06826", "authors": ["Chenfei Yuan", "Nanshan Jia", "Hangqi Li", "Peter W. Glynn", "Zeyu Zheng"], "title": "Controllable Coupled Image Generation via Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We provide an attention-level control method for the task of coupled image\ngeneration, where \"coupled\" means that multiple simultaneously generated images\nare expected to have the same or very similar backgrounds. While backgrounds\ncoupled, the centered objects in the generated images are still expected to\nenjoy the flexibility raised from different text prompts. The proposed method\ndisentangles the background and entity components in the model's\ncross-attention modules, attached with a sequence of time-varying weight\ncontrol parameters depending on the time step of sampling. We optimize this\nsequence of weight control parameters with a combined objective that assesses\nhow coupled the backgrounds are as well as text-to-image alignment and overall\nvisual quality. Empirical results demonstrate that our method outperforms\nexisting approaches across these criteria.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u7ea7\u522b\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u8026\u5408\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u80cc\u666f\u76f8\u4f3c\u4f46\u4e2d\u5fc3\u5bf9\u8c61\u7075\u6d3b\u3002", "motivation": "\u89e3\u51b3\u591a\u56fe\u50cf\u751f\u6210\u4e2d\u80cc\u666f\u8026\u5408\u4e0e\u5bf9\u8c61\u7075\u6d3b\u6027\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u80cc\u666f\u548c\u5b9e\u4f53\u7ec4\u4ef6\uff0c\u7ed3\u5408\u65f6\u95f4\u53d8\u5316\u7684\u6743\u91cd\u63a7\u5236\u53c2\u6570\u4f18\u5316\u3002", "result": "\u5728\u80cc\u666f\u8026\u5408\u3001\u6587\u672c\u5bf9\u9f50\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u80cc\u666f\u4e00\u81f4\u6027\u548c\u5bf9\u8c61\u591a\u6837\u6027\u3002"}}
{"id": "2506.06830", "pdf": "https://arxiv.org/pdf/2506.06830", "abs": "https://arxiv.org/abs/2506.06830", "authors": ["Guankun Wang", "Rui Tang", "Mengya Xu", "Long Bai", "Huxin Gao", "Hongliang Ren"], "title": "EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by Advanced Intelligent Systems", "summary": "Endoscopic surgery is the gold standard for robotic-assisted minimally\ninvasive surgery, offering significant advantages in early disease detection\nand precise interventions. However, the complexity of surgical scenes,\ncharacterized by high variability in different surgical activity scenarios and\nconfused image features between targets and the background, presents challenges\nfor surgical environment understanding. Traditional deep learning models often\nstruggle with cross-activity interference, leading to suboptimal performance in\neach downstream task. To address this limitation, we explore multi-task\nlearning, which utilizes the interrelated features between tasks to enhance\noverall task performance. In this paper, we propose EndoARSS, a novel\nmulti-task learning framework specifically designed for endoscopy surgery\nactivity recognition and semantic segmentation. Built upon the DINOv2\nfoundation model, our approach integrates Low-Rank Adaptation to facilitate\nefficient fine-tuning while incorporating Task Efficient Shared Low-Rank\nAdapters to mitigate gradient conflicts across diverse tasks. Additionally, we\nintroduce the Spatially-Aware Multi-Scale Attention that enhances feature\nrepresentation discrimination by enabling cross-spatial learning of global\ninformation. In order to evaluate the effectiveness of our framework, we\npresent three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored\nfor endoscopic surgery scenarios with detailed annotations for both activity\nrecognition and semantic segmentation tasks. Extensive experiments demonstrate\nthat EndoARSS achieves remarkable performance across multiple benchmarks,\nsignificantly improving both accuracy and robustness in comparison to existing\nmodels. These results underscore the potential of EndoARSS to advance AI-driven\nendoscopic surgical systems, offering valuable insights for enhancing surgical\nsafety and efficiency.", "AI": {"tldr": "EndoARSS\u662f\u4e00\u79cd\u57fa\u4e8eDINOv2\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u624b\u672f\u6d3b\u52a8\u8bc6\u522b\u548c\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u914d\u548c\u7a7a\u95f4\u611f\u77e5\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5185\u7aa5\u955c\u624b\u672f\u573a\u666f\u590d\u6742\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8de8\u6d3b\u52a8\u5e72\u6270\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u4f4e\u79e9\u9002\u914d\u548c\u4efb\u52a1\u5171\u4eab\u9002\u914d\u5668\u51cf\u5c11\u68af\u5ea6\u51b2\u7a81\uff0c\u5f15\u5165\u7a7a\u95f4\u611f\u77e5\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "EndoARSS\u6709\u671b\u63a8\u52a8AI\u9a71\u52a8\u7684\u5185\u7aa5\u955c\u624b\u672f\u7cfb\u7edf\u53d1\u5c55\uff0c\u63d0\u5347\u624b\u672f\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.06836", "pdf": "https://arxiv.org/pdf/2506.06836", "abs": "https://arxiv.org/abs/2506.06836", "authors": ["Zelin He", "Sarah Alnegheimish", "Matthew Reimherr"], "title": "Harnessing Vision-Language Models for Time Series Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Time-series anomaly detection (TSAD) has played a vital role in a variety of\nfields, including healthcare, finance, and industrial monitoring. Prior\nmethods, which mainly focus on training domain-specific models on numerical\ndata, lack the visual-temporal reasoning capacity that human experts have to\nidentify contextual anomalies. To fill this gap, we explore a solution based on\nvision language models (VLMs). Recent studies have shown the ability of VLMs\nfor visual reasoning tasks, yet their direct application to time series has\nfallen short on both accuracy and efficiency. To harness the power of VLMs for\nTSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening\nstage built on a relatively lightweight pretrained vision encoder, which\nleverages 2-D time-series representations to accurately localize candidate\nanomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal\ncontext and VLM reasoning capacity to refine the detection upon the candidates\nprovided by ViT4TS. We show that without any time-series training, VLM4TS\noutperforms time-series pretrained and from-scratch baselines in most cases,\nyielding a 24.6 percent improvement in F1-max score over the best baseline.\nMoreover, VLM4TS also consistently outperforms existing language-model-based\nTSAD methods and is on average 36 times more efficient in token usage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u4e24\u9636\u6bb5\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u89c6\u89c9\u7f16\u7801\u5668\u548cVLM\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u89c6\u89c9-\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u50cf\u4eba\u7c7b\u4e13\u5bb6\u4e00\u6837\u8bc6\u522b\u4e0a\u4e0b\u6587\u5f02\u5e38\uff0c\u56e0\u6b64\u63a2\u7d22\u57fa\u4e8eVLM\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. ViT4TS\uff1a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u7f16\u7801\u5668\u5b9a\u4f4d\u5019\u9009\u5f02\u5e38\uff1b2. VLM4TS\uff1a\u6574\u5408\u5168\u5c40\u65f6\u95f4\u4e0a\u4e0b\u6587\u548cVLM\u63a8\u7406\u80fd\u529b\u4f18\u5316\u68c0\u6d4b\u3002", "result": "VLM4TS\u5728\u672a\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0cF1-max\u5f97\u5206\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534724.6%\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86VLM\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.06846", "pdf": "https://arxiv.org/pdf/2506.06846", "abs": "https://arxiv.org/abs/2506.06846", "authors": ["Yangkai Lin", "Jiabao Lei", "Kui jia"], "title": "Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "In recent years, there has been a growing demand to stylize a given 3D scene\nto align with the artistic style of reference images for creative purposes.\nWhile 3D Gaussian Splatting(GS) has emerged as a promising and efficient method\nfor realistic 3D scene modeling, there remains a challenge in adapting it to\nstylize 3D GS to match with multiple styles through automatic local style\ntransfer or manual designation, while maintaining memory efficiency for\nstylization training. In this paper, we introduce a novel 3D GS stylization\nsolution termed Multi-StyleGS to tackle these challenges. In particular, we\nemploy a bipartite matching mechanism to au tomatically identify\ncorrespondences between the style images and the local regions of the rendered\nimages. To facilitate local style transfer, we introduce a novel semantic style\nloss function that employs a segmentation network to apply distinct styles to\nvarious objects of the scene and propose a local-global feature matching to\nenhance the multi-view consistency. Furthermore, this technique can achieve\nmemory efficient training, more texture details and better color match. To\nbetter assign a robust semantic label to each Gaussian, we propose several\ntechniques to regularize the segmentation network. As demonstrated by our\ncomprehensive experiments, our approach outperforms existing ones in producing\nplausible stylization results and offering flexible editing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMulti-StyleGS\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u7684\u591a\u98ce\u683c\u5316\uff0c\u901a\u8fc7\u81ea\u52a8\u5c40\u90e8\u98ce\u683c\u8f6c\u79fb\u548c\u8bed\u4e49\u98ce\u683c\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u66f4\u597d\u7684\u89c6\u89c9\u6548\u679c\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5bf93D\u573a\u666f\u8fdb\u884c\u827a\u672f\u98ce\u683c\u5316\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u591a\u98ce\u683c\u5316\u548c\u4fdd\u6301\u5185\u5b58\u6548\u7387\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e8c\u5206\u5339\u914d\u673a\u5236\u81ea\u52a8\u5339\u914d\u98ce\u683c\u56fe\u50cf\u4e0e\u6e32\u67d3\u56fe\u50cf\u7684\u5c40\u90e8\u533a\u57df\uff0c\u63d0\u51fa\u8bed\u4e49\u98ce\u683c\u635f\u5931\u51fd\u6570\u548c\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\u5339\u914d\u6280\u672f\uff0c\u4f18\u5316\u5206\u5272\u7f51\u7edc\u4ee5\u589e\u5f3a\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u98ce\u683c\u5316\u6548\u679c\u3001\u5185\u5b58\u6548\u7387\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Multi-StyleGS\u4e3a3D GS\u98ce\u683c\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u98ce\u683c\u5316\u548c\u5c40\u90e8\u7f16\u8f91\u3002"}}
{"id": "2506.06850", "pdf": "https://arxiv.org/pdf/2506.06850", "abs": "https://arxiv.org/abs/2506.06850", "authors": ["Sara M. Cerqueira", "Manuel Palermo", "Cristina P. Santos"], "title": "Deep Inertial Pose: A deep learning approach for human pose estimation", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Inertial-based Motion capture system has been attracting growing attention\ndue to its wearability and unsconstrained use. However, accurate human joint\nestimation demands several complex and expertise demanding steps, which leads\nto expensive software such as the state-of-the-art MVN Awinda from Xsens\nTechnologies. This work aims to study the use of Neural Networks to abstract\nthe complex biomechanical models and analytical mathematics required for pose\nestimation. Thus, it presents a comparison of different Neural Network\narchitectures and methodologies to understand how accurately these methods can\nestimate human pose, using both low cost(MPU9250) and high end (Mtw Awinda)\nMagnetic, Angular Rate, and Gravity (MARG) sensors. The most efficient method\nwas the Hybrid LSTM-Madgwick detached, which achieved an Quaternion Angle\ndistance error of 7.96, using Mtw Awinda data. Also, an ablation study was\nconducted to study the impact of data augmentation, output representation,\nwindow size, loss function and magnetometer data on the pose estimation error.\nThis work indicates that Neural Networks can be trained to estimate human pose,\nwith results comparable to the state-of-the-art fusion filters.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7b80\u5316\u60ef\u6027\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u4e2d\u590d\u6742\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6b65\u9aa4\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u60ef\u6027\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u56e0\u5176\u4fbf\u643a\u6027\u548c\u65e0\u7ea6\u675f\u6027\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u4e14\u6602\u8d35\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u80fd\u66ff\u4ee3\u4f20\u7edf\u590d\u6742\u6a21\u578b\u3002", "method": "\u6bd4\u8f83\u4e86\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u65b9\u6cd5\uff0c\u5305\u62ecHybrid LSTM-Madgwick\uff0c\u5e76\u4f7f\u7528\u4f4e\u6210\u672c\u548c\u9ad8\u7aef\u7684MARG\u4f20\u611f\u5668\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "Hybrid LSTM-Madgwick\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u8bef\u5dee\u4e3a7.96\uff08\u4f7f\u7528\u9ad8\u7aef\u4f20\u611f\u5668\uff09\u3002\u6570\u636e\u589e\u5f3a\u3001\u8f93\u51fa\u8868\u793a\u7b49\u56e0\u7d20\u5bf9\u8bef\u5dee\u6709\u5f71\u54cd\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u7528\u4e8e\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u6548\u679c\u63a5\u8fd1\u73b0\u6709\u6700\u4f18\u878d\u5408\u6ee4\u6ce2\u5668\u3002"}}
{"id": "2506.06852", "pdf": "https://arxiv.org/pdf/2506.06852", "abs": "https://arxiv.org/abs/2506.06852", "authors": ["John Waithaka", "Moise Busogi"], "title": "Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation of satellite imagery is crucial for Earth observation\napplications, but remains constrained by limited labelled training data. While\nself-supervised pretraining methods like Masked Autoencoders (MAE) have shown\npromise, they focus on reconstruction rather than localisation-a fundamental\naspect of segmentation tasks. We propose adapting LOCA (Location-aware), a\nposition prediction self-supervised learning method, for multimodal satellite\nimagery semantic segmentation. Our approach addresses the unique challenges of\nsatellite data by extending SatMAE's channel grouping from multispectral to\nmultimodal data, enabling effective handling of multiple modalities, and\nintroducing same-group attention masking to encourage cross-modal interaction\nduring pretraining. The method uses relative patch position prediction,\nencouraging spatial reasoning for localisation rather than reconstruction. We\nevaluate our approach on the Sen1Floods11 flood mapping dataset, where it\nsignificantly outperforms existing reconstruction-based self-supervised\nlearning methods for satellite imagery. Our results demonstrate that position\nprediction tasks, when properly adapted for multimodal satellite imagery, learn\nrepresentations more effective for satellite image semantic segmentation than\nreconstruction-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4d\u7f6e\u9884\u6d4b\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08LOCA\uff09\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u536b\u661f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u91cd\u5efa\u7684\u65b9\u6cd5\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u53d7\u9650\u4e8e\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\uff0c\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\uff08\u5982MAE\uff09\u4e13\u6ce8\u4e8e\u91cd\u5efa\u800c\u975e\u5b9a\u4f4d\uff0c\u800c\u5b9a\u4f4d\u662f\u5206\u5272\u4efb\u52a1\u7684\u5173\u952e\u3002", "method": "\u6269\u5c55SatMAE\u7684\u901a\u9053\u5206\u7ec4\u81f3\u591a\u6a21\u6001\u6570\u636e\uff0c\u5f15\u5165\u540c\u7ec4\u6ce8\u610f\u529b\u63a9\u7801\u4ee5\u4fc3\u8fdb\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u91c7\u7528\u76f8\u5bf9\u8865\u4e01\u4f4d\u7f6e\u9884\u6d4b\u4efb\u52a1\u4ee5\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u3002", "result": "\u5728Sen1Floods11\u6d2a\u6c34\u6620\u5c04\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u91cd\u5efa\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u9488\u5bf9\u591a\u6a21\u6001\u536b\u661f\u56fe\u50cf\u7684\u4f4d\u7f6e\u9884\u6d4b\u4efb\u52a1\u80fd\u5b66\u4e60\u5230\u66f4\u6709\u6548\u7684\u8868\u793a\uff0c\u4f18\u4e8e\u91cd\u5efa\u65b9\u6cd5\u3002"}}
{"id": "2506.06854", "pdf": "https://arxiv.org/pdf/2506.06854", "abs": "https://arxiv.org/abs/2506.06854", "authors": ["Markus Knoche", "Daan de Geus", "Bastian Leibe"], "title": "DONUT: A Decoder-Only Model for Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Predicting the motion of other agents in a scene is highly relevant for\nautonomous driving, as it allows a self-driving car to anticipate. Inspired by\nthe success of decoder-only models for language modeling, we propose DONUT, a\nDecoder-Only Network for Unrolling Trajectories. Different from existing\nencoder-decoder forecasting models, we encode historical trajectories and\npredict future trajectories with a single autoregressive model. This allows the\nmodel to make iterative predictions in a consistent manner, and ensures that\nthe model is always provided with up-to-date information, enhancing the\nperformance. Furthermore, inspired by multi-token prediction for language\nmodeling, we introduce an 'overprediction' strategy that gives the network the\nauxiliary task of predicting trajectories at longer temporal horizons. This\nallows the model to better anticipate the future, and further improves the\nperformance. With experiments, we demonstrate that our decoder-only approach\noutperforms the encoder-decoder baseline, and achieves new state-of-the-art\nresults on the Argoverse 2 single-agent motion forecasting benchmark.", "AI": {"tldr": "DONUT\u662f\u4e00\u79cd\u4ec5\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u8f68\u8ff9\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u5b9e\u73b0\u8fed\u4ee3\u9884\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u57fa\u7ebf\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u9884\u6d4b\u5176\u4ed6\u4ee3\u7406\u7684\u8fd0\u52a8\u4ee5\u5b9e\u73b0\u9884\u5224\uff0c\u73b0\u6709\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u4ec5\u89e3\u7801\u5668\u7f51\u7edc\uff08DONUT\uff09\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u8fed\u4ee3\u9884\u6d4b\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u2018\u8fc7\u5ea6\u9884\u6d4b\u2019\u7b56\u7565\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728Argoverse 2\u5355\u4ee3\u7406\u8fd0\u52a8\u9884\u6d4b\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "\u4ec5\u89e3\u7801\u5668\u65b9\u6cd5\u5728\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.06856", "pdf": "https://arxiv.org/pdf/2506.06856", "abs": "https://arxiv.org/abs/2506.06856", "authors": ["Chaoyang Wang", "Zeyu Zhang", "Haiyun Jiang"], "title": "Vision-EKIPL: External Knowledge-Infused Policy Learning for Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Visual reasoning is crucial for understanding complex multimodal data and\nadvancing Artificial General Intelligence. Existing methods enhance the\nreasoning capability of Multimodal Large Language Models (MLLMs) through\nReinforcement Learning (RL) fine-tuning (e.g., GRPO). However, current RL\napproaches sample action groups solely from the policy model itself, which\nlimits the upper boundary of the model's reasoning capability and leads to\ninefficient training. To address these limitations, this paper proposes a novel\nRL framework called \\textbf{Vision-EKIPL}. The core of this framework lies in\nintroducing high-quality actions generated by external auxiliary models during\nthe RL training process to guide the optimization of the policy model. The\npolicy learning with knowledge infusion from external models significantly\nexpands the model's exploration space, effectively improves the reasoning\nboundary, and substantially accelerates training convergence speed and\nefficiency. Experimental results demonstrate that our proposed Vision-EKIPL\nachieved up to a 5\\% performance improvement on the Reason-RFT-CoT Benchmark\ncompared to the state-of-the-art (SOTA). It reveals that Vision-EKIPL can\novercome the limitations of traditional RL methods, significantly enhance the\nvisual reasoning performance of MLLMs, and provide a new effective paradigm for\nresearch in this field.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6Vision-EKIPL\uff0c\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u8f85\u52a9\u6a21\u578b\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u52a8\u4f5c\u6765\u4f18\u5316\u7b56\u7565\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4ece\u7b56\u7565\u6a21\u578b\u672c\u8eab\u91c7\u6837\u52a8\u4f5c\u7ec4\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e0a\u9650\u5e76\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faVision-EKIPL\u6846\u67b6\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5916\u90e8\u8f85\u52a9\u6a21\u578b\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u52a8\u4f5c\uff0c\u6307\u5bfc\u7b56\u7565\u6a21\u578b\u4f18\u5316\u3002", "result": "\u5728Reason-RFT-CoT Benchmark\u4e0a\u6027\u80fd\u63d0\u53475%\uff0c\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "Vision-EKIPL\u514b\u670d\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.06864", "pdf": "https://arxiv.org/pdf/2506.06864", "abs": "https://arxiv.org/abs/2506.06864", "authors": ["Junyu Liu", "Jianfeng Ren", "Sunhong Liang", "Xudong Jiang"], "title": "Face recognition on point cloud with cgan-top for denoising", "categories": ["cs.CV", "cs.AI"], "comment": "Published in ICASSP 2023", "summary": "Face recognition using 3D point clouds is gaining growing interest, while raw\npoint clouds often contain a significant amount of noise due to imperfect\nsensors. In this paper, an end-to-end 3D face recognition on a noisy point\ncloud is proposed, which synergistically integrates the denoising and\nrecognition modules. Specifically, a Conditional Generative Adversarial Network\non Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the\nnoise in the point cloud, and recover the underlying features for subsequent\nrecognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is\nthen adapted to recognize faces from the processed point cloud, which\nhierarchically links both the local point features and neighboring features of\nmultiple scales. The proposed method is validated on the Bosphorus dataset. It\nsignificantly improves the recognition accuracy under all noise settings, with\na maximum gain of 14.81%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u76843D\u4eba\u8138\u8bc6\u522b\u65b9\u6cd5\uff0c\u7ed3\u5408\u53bb\u566a\u548c\u8bc6\u522b\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u566a\u58f0\u70b9\u4e91\u4e0b\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u539f\u59cb\u70b9\u4e91\u5e38\u56e0\u4f20\u611f\u5668\u4e0d\u5b8c\u7f8e\u800c\u5305\u542b\u5927\u91cf\u566a\u58f0\uff0c\u5f71\u54cd\u8bc6\u522b\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u4e86cGAN-TOP\u53bb\u566a\u6a21\u5757\u548cLDGCNN\u8bc6\u522b\u6a21\u5757\uff0c\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u5728Bosphorus\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8bc6\u522b\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u6700\u9ad8\u589e\u76ca14.81%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u70b9\u4e91\u4e0b\u76843D\u4eba\u8138\u8bc6\u522b\u95ee\u9898\u3002"}}
{"id": "2506.06886", "pdf": "https://arxiv.org/pdf/2506.06886", "abs": "https://arxiv.org/abs/2506.06886", "authors": ["Wafaa Kasri", "Yassine Himeur", "Abigail Copiaco", "Wathiq Mansoor", "Ammar Albanna", "Valsamma Eapen"], "title": "Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via Eye-Tracking Analysis", "categories": ["cs.CV"], "comment": "7 pages, 4 figures and 2 tables", "summary": "Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early\nintervention. This study presents a hybrid deep learning framework combining\nVision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking\ndata. The model uses attention-based fusion to integrate visual, speech, and\nfacial cues, capturing both spatial and temporal dynamics. Unlike traditional\nhandcrafted methods, it applies state-of-the-art deep learning and explainable\nAI techniques to enhance diagnostic accuracy and transparency. Tested on the\nSaliency4ASD dataset, the proposed ViT-Mamba model outperformed existing\nmethods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94\nspecificity. These findings show the model's promise for scalable,\ninterpretable ASD screening, especially in resource-constrained or remote\nclinical settings where access to expert diagnosis is limited.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Vision Transformers\u548cVision Mamba\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u773c\u52a8\u6570\u636e\u68c0\u6d4b\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff08ASD\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u65e9\u671f\u8bca\u65adASD\u5bf9\u5e72\u9884\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u4e14\u7f3a\u4e4f\u900f\u660e\u5ea6\u3002", "method": "\u91c7\u7528Vision Transformers\u548cVision Mamba\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u878d\u5408\u6280\u672f\u6574\u5408\u89c6\u89c9\u3001\u8bed\u97f3\u548c\u9762\u90e8\u7ebf\u7d22\u3002", "result": "\u5728Saliency4ASD\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff08\u51c6\u786e\u73870.96\uff0cF1\u5206\u65700.95\uff0c\u7075\u654f\u5ea60.97\uff0c\u7279\u5f02\u5ea60.94\uff09\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u8d44\u6e90\u6709\u9650\u6216\u504f\u8fdc\u5730\u533a\u7684ASD\u7b5b\u67e5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06898", "pdf": "https://arxiv.org/pdf/2506.06898", "abs": "https://arxiv.org/abs/2506.06898", "authors": ["Reese Kneeland", "Paul S. Scotti", "Ghislain St-Yves", "Jesse Breedlove", "Kendrick Kay", "Thomas Naselaris"], "title": "NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery", "categories": ["cs.CV", "cs.LG", "eess.IV", "q-bio.NC"], "comment": "Published at CVPR 2025", "summary": "We release NSD-Imagery, a benchmark dataset of human fMRI activity paired\nwith mental images, to complement the existing Natural Scenes Dataset (NSD), a\nlarge-scale dataset of fMRI activity paired with seen images that enabled\nunprecedented improvements in fMRI-to-image reconstruction efforts. Recent\nmodels trained on NSD have been evaluated only on seen image reconstruction.\nUsing NSD-Imagery, it is possible to assess how well these models perform on\nmental image reconstruction. This is a challenging generalization requirement\nbecause mental images are encoded in human brain activity with relatively lower\nsignal-to-noise and spatial resolution; however, generalization from seen to\nmental imagery is critical for real-world applications in medical domains and\nbrain-computer interfaces, where the desired information is always internally\ngenerated. We provide benchmarks for a suite of recent NSD-trained open-source\nvisual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et\nal.) on NSD-Imagery, and show that the performance of decoding methods on\nmental images is largely decoupled from performance on vision reconstruction.\nWe further demonstrate that architectural choices significantly impact\ncross-decoding performance: models employing simple linear decoding\narchitectures and multimodal feature decoding generalize better to mental\nimagery, while complex architectures tend to overfit visual training data. Our\nfindings indicate that mental imagery datasets are critical for the development\nof practical applications, and establish NSD-Imagery as a useful resource for\nbetter aligning visual decoding methods with this goal.", "AI": {"tldr": "NSD-Imagery\u662f\u4e00\u4e2a\u65b0\u53d1\u5e03\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30fMRI\u5230\u56fe\u50cf\u91cd\u5efa\u6a21\u578b\u5728\u5fc3\u7406\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709NSD\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u4ec5\u5728\u5df2\u89c1\u56fe\u50cf\u91cd\u5efa\u4e0a\u8bc4\u4f30\uff0c\u800c\u5fc3\u7406\u56fe\u50cf\u91cd\u5efa\u5bf9\u533b\u5b66\u548c\u8111\u673a\u63a5\u53e3\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528NSD-Imagery\u8bc4\u4f30\u591a\u4e2a\u5f00\u6e90\u89c6\u89c9\u89e3\u7801\u6a21\u578b\uff08\u5982MindEye1\u3001Brain Diffuser\u7b49\uff09\u5728\u5fc3\u7406\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u5fc3\u7406\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u4e0e\u89c6\u89c9\u91cd\u5efa\u8868\u73b0\u8131\u94a9\uff0c\u7b80\u5355\u7ebf\u6027\u67b6\u6784\u548c\u591a\u6a21\u6001\u7279\u5f81\u89e3\u7801\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5fc3\u7406\u56fe\u50cf\u6570\u636e\u96c6\u5bf9\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0cNSD-Imagery\u4e3a\u89c6\u89c9\u89e3\u7801\u65b9\u6cd5\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2506.06906", "pdf": "https://arxiv.org/pdf/2506.06906", "abs": "https://arxiv.org/abs/2506.06906", "authors": ["Nima Jamali", "Matina Mahdizadeh Sani", "Hanieh Naderi", "Shohreh Kasaei"], "title": "KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) have demonstrated remarkable performance in\nanalyzing 3D point cloud data. However, their vulnerability to adversarial\nattacks-such as point dropping, shifting, and adding-poses a critical challenge\nto the reliability of 3D vision systems. These attacks can compromise the\nsemantic and structural integrity of point clouds, rendering many existing\ndefense mechanisms ineffective. To address this issue, a defense strategy named\nKNN-Defense is proposed, grounded in the manifold assumption and\nnearest-neighbor search in feature space. Instead of reconstructing surface\ngeometry or enforcing uniform point distributions, the method restores\nperturbed inputs by leveraging the semantic similarity of neighboring samples\nfrom the training set. KNN-Defense is lightweight and computationally\nefficient, enabling fast inference and making it suitable for real-time and\npractical applications. Empirical results on the ModelNet40 dataset\ndemonstrated that KNN-Defense significantly improves robustness across various\nattack types. In particular, under point-dropping attacks-where many existing\nmethods underperform due to the targeted removal of critical points-the\nproposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on\nPointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that\nKNN-Defense offers a scalable and effective solution for enhancing the\nadversarial resilience of 3D point cloud classifiers. (An open-source\nimplementation of the method, including code and data, is available at\nhttps://github.com/nimajam41/3d-knn-defense).", "AI": {"tldr": "KNN-Defense\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u6700\u8fd1\u90bb\u641c\u7d22\u6062\u590d\u53d7\u6270\u52a8\u76843D\u70b9\u4e91\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u57283D\u70b9\u4e91\u6570\u636e\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5bf9\u6297\u653b\u51fb\uff08\u5982\u70b9\u5220\u9664\u3001\u79fb\u52a8\u548c\u6dfb\u52a0\uff09\u7684\u8106\u5f31\u6027\u5a01\u80c1\u4e863D\u89c6\u89c9\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "method": "\u57fa\u4e8e\u6d41\u5f62\u5047\u8bbe\u548c\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u6700\u8fd1\u90bb\u641c\u7d22\uff0cKNN-Defense\u901a\u8fc7\u8bad\u7ec3\u96c6\u4e2d\u90bb\u8fd1\u6837\u672c\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u6062\u590d\u53d7\u6270\u52a8\u7684\u8f93\u5165\u3002", "result": "\u5728ModelNet40\u6570\u636e\u96c6\u4e0a\uff0cKNN-Defense\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u70b9\u5220\u9664\u653b\u51fb\u4e0b\uff0c\u5bf9\u591a\u79cd\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u5347\u663e\u8457\u3002", "conclusion": "KNN-Defense\u4e3a\u589e\u5f3a3D\u70b9\u4e91\u5206\u7c7b\u5668\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06909", "pdf": "https://arxiv.org/pdf/2506.06909", "abs": "https://arxiv.org/abs/2506.06909", "authors": ["Vladimir Yugay", "Thies Kersten", "Luca Carlone", "Theo Gevers", "Martin R. Oswald", "Lukas Schmid"], "title": "Gaussian Mapping for Evolving Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Mapping systems with novel view synthesis (NVS) capabilities are widely used\nin computer vision, with augmented reality, robotics, and autonomous driving\napplications. Most notably, 3D Gaussian Splatting-based systems show high NVS\nperformance; however, many current approaches are limited to static scenes.\nWhile recent works have started addressing short-term dynamics (motion within\nthe view of the camera), long-term dynamics (the scene evolving through changes\nout of view) remain less explored. To overcome this limitation, we introduce a\ndynamic scene adaptation mechanism that continuously updates the 3D\nrepresentation to reflect the latest changes. In addition, since maintaining\ngeometric and semantic consistency remains challenging due to stale\nobservations disrupting the reconstruction process, we propose a novel keyframe\nmanagement mechanism that discards outdated observations while preserving as\nmuch information as possible. We evaluate Gaussian Mapping for Evolving Scenes\n(GaME) on both synthetic and real-world datasets and find it to be more\naccurate than the state of the art.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u573a\u666f\u9002\u5e94\u673a\u5236\u548c\u5173\u952e\u5e27\u7ba1\u7406\u673a\u5236\uff0c\u7528\u4e8e\u89e3\u51b33D\u9ad8\u65af\u6cfc\u6e85\u7cfb\u7edf\u4e2d\u957f\u671f\u52a8\u6001\u573a\u666f\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u9ad8\u65af\u6cfc\u6e85\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u573a\u666f\uff0c\u5bf9\u957f\u671f\u52a8\u6001\u573a\u666f\uff08\u5982\u573a\u666f\u5728\u89c6\u91ce\u5916\u53d8\u5316\uff09\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u6301\u7eed\u66f4\u65b03D\u8868\u793a\u3002", "method": "\u5f15\u5165\u52a8\u6001\u573a\u666f\u9002\u5e94\u673a\u5236\u4ee5\u6301\u7eed\u66f4\u65b03D\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u5173\u952e\u5e27\u7ba1\u7406\u673a\u5236\u4ee5\u4e22\u5f03\u8fc7\u65f6\u89c2\u6d4b\u5e76\u4fdd\u7559\u6709\u6548\u4fe1\u606f\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cGaME\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u51c6\u786e\u3002", "conclusion": "GaME\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u548c\u5173\u952e\u5e27\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u52a8\u6001\u573a\u666f\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.06912", "pdf": "https://arxiv.org/pdf/2506.06912", "abs": "https://arxiv.org/abs/2506.06912", "authors": ["Olivier Papillon", "Rafik Goubran", "James Green", "Julien Larivi\u00e8re-Chartier", "Caitlin Higginson", "Frank Knoefel", "R\u00e9becca Robillard"], "title": "Sleep Stage Classification using Multimodal Embedding Fusion from EOG and PSM", "categories": ["cs.CV"], "comment": "Submitted to IEEE MeMeA 2025", "summary": "Accurate sleep stage classification is essential for diagnosing sleep\ndisorders, particularly in aging populations. While traditional polysomnography\n(PSG) relies on electroencephalography (EEG) as the gold standard, its\ncomplexity and need for specialized equipment make home-based sleep monitoring\nchallenging. To address this limitation, we investigate the use of\nelectrooculography (EOG) and pressure-sensitive mats (PSM) as less obtrusive\nalternatives for five-stage sleep-wake classification. This study introduces a\nnovel approach that leverages ImageBind, a multimodal embedding deep learning\nmodel, to integrate PSM data with dual-channel EOG signals for sleep stage\nclassification. Our method is the first reported approach that fuses PSM and\nEOG data for sleep stage classification with ImageBind. Our results demonstrate\nthat fine-tuning ImageBind significantly improves classification accuracy,\noutperforming existing models based on single-channel EOG (DeepSleepNet),\nexclusively PSM data (ViViT), and other multimodal deep learning approaches\n(MBT). Notably, the model also achieved strong performance without fine-tuning,\nhighlighting its adaptability to specific tasks with limited labeled data,\nmaking it particularly advantageous for medical applications. We evaluated our\nmethod using 85 nights of patient recordings from a sleep clinic. Our findings\nsuggest that pre-trained multimodal embedding models, even those originally\ndeveloped for non-medical domains, can be effectively adapted for sleep\nstaging, with accuracies approaching systems that require complex EEG data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528ImageBind\u591a\u6a21\u6001\u5d4c\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u538b\u529b\u654f\u611f\u57ab\uff08PSM\uff09\u548c\u53cc\u901a\u9053\u773c\u7535\u56fe\uff08EOG\uff09\u4fe1\u53f7\u8fdb\u884c\u7761\u7720\u9636\u6bb5\u5206\u7c7b\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u5bfc\u7761\u7720\u56fe\uff08PSG\uff09\u4f9d\u8d56\u8111\u7535\u56fe\uff08EEG\uff09\u4f5c\u4e3a\u91d1\u6807\u51c6\uff0c\u4f46\u5176\u590d\u6742\u6027\u548c\u5bf9\u4e13\u4e1a\u8bbe\u5907\u7684\u9700\u6c42\u9650\u5236\u4e86\u5bb6\u5ead\u7761\u7720\u76d1\u6d4b\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u63a2\u7d22\u4e86EOG\u548cPSM\u4f5c\u4e3a\u66f4\u4fbf\u6377\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u91c7\u7528ImageBind\u6a21\u578b\uff0c\u6574\u5408PSM\u6570\u636e\u548c\u53cc\u901a\u9053EOG\u4fe1\u53f7\u8fdb\u884c\u7761\u7720\u9636\u6bb5\u5206\u7c7b\uff0c\u5e76\u9996\u6b21\u5c06PSM\u4e0eEOG\u6570\u636e\u878d\u5408\u5e94\u7528\u4e8e\u6b64\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u5355\u901a\u9053EOG\u3001\u4ec5PSM\u6570\u636e\u6216\u5176\u4ed6\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u7684\u73b0\u6709\u6a21\u578b\uff0c\u4e14\u5728\u672a\u5fae\u8c03\u65f6\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u53ef\u6709\u6548\u7528\u4e8e\u7761\u7720\u5206\u671f\uff0c\u5176\u51c6\u786e\u6027\u63a5\u8fd1\u4f9d\u8d56\u590d\u6742EEG\u6570\u636e\u7684\u7cfb\u7edf\uff0c\u5c24\u5176\u9002\u5408\u533b\u7597\u5e94\u7528\u3002"}}
{"id": "2506.06918", "pdf": "https://arxiv.org/pdf/2506.06918", "abs": "https://arxiv.org/abs/2506.06918", "authors": ["Carl Brander", "Giovanni Cioffi", "Nico Messikommer", "Davide Scaramuzza"], "title": "Reading in the Dark with Foveated Event Vision", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025 Workshop on Event-based Vision", "summary": "Current smart glasses equipped with RGB cameras struggle to perceive the\nenvironment in low-light and high-speed motion scenarios due to motion blur and\nthe limited dynamic range of frame cameras. Additionally, capturing dense\nimages with a frame camera requires large bandwidth and power consumption,\nconsequently draining the battery faster. These challenges are especially\nrelevant for developing algorithms that can read text from images. In this\nwork, we propose a novel event-based Optical Character Recognition (OCR)\napproach for smart glasses. By using the eye gaze of the user, we foveate the\nevent stream to significantly reduce bandwidth by around 98% while exploiting\nthe benefits of event cameras in high-dynamic and fast scenes. Our proposed\nmethod performs deep binary reconstruction trained on synthetic data and\nleverages multimodal LLMs for OCR, outperforming traditional OCR solutions. Our\nresults demonstrate the ability to read text in low light environments where\nRGB cameras struggle while using up to 2400 times less bandwidth than a\nwearable RGB camera.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u7684\u65b0\u578bOCR\u65b9\u6cd5\uff0c\u7528\u4e8e\u667a\u80fd\u773c\u955c\uff0c\u901a\u8fc7\u7528\u6237\u89c6\u7ebf\u805a\u7126\u4e8b\u4ef6\u6d41\uff0c\u663e\u8457\u51cf\u5c11\u5e26\u5bbd\uff0c\u5e76\u5728\u4f4e\u5149\u548c\u9ad8\u52a8\u6001\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edfOCR\u3002", "motivation": "\u667a\u80fd\u773c\u955c\u7684RGB\u76f8\u673a\u5728\u4f4e\u5149\u548c\u9ad8\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5e26\u5bbd\u548c\u529f\u8017\u9ad8\uff0c\u5f71\u54cd\u7535\u6c60\u5bff\u547d\u3002", "method": "\u5229\u7528\u7528\u6237\u89c6\u7ebf\u805a\u7126\u4e8b\u4ef6\u6d41\uff0c\u7ed3\u5408\u6df1\u5ea6\u4e8c\u8fdb\u5236\u91cd\u5efa\u548c\u591a\u6a21\u6001LLMs\u8fdb\u884cOCR\u3002", "result": "\u5728\u4f4e\u5149\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e26\u5bbd\u51cf\u5c11\u8fbe2400\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4f4e\u5149\u548c\u9ad8\u52a8\u6001\u573a\u666f\u4e2d\u9ad8\u6548\u4e14\u8282\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edfOCR\u3002"}}
{"id": "2506.06928", "pdf": "https://arxiv.org/pdf/2506.06928", "abs": "https://arxiv.org/abs/2506.06928", "authors": ["George Lydakis", "Alexander Hermans", "Ali Athar", "Daan de Geus", "Bastian Leibe"], "title": "How Important are Videos for Training Video LLMs?", "categories": ["cs.CV"], "comment": "Project page on\n  https://visualcomputinginstitute.github.io/videollm-pseudovideo-training/", "summary": "Research into Video Large Language Models (LLMs) has progressed rapidly, with\nnumerous models and benchmarks emerging in just a few years. Typically, these\nmodels are initialized with a pretrained text-only LLM and finetuned on both\nimage- and video-caption datasets. In this paper, we present findings\nindicating that Video LLMs are more capable of temporal reasoning after\nimage-only training than one would assume, and that improvements from\nvideo-specific training are surprisingly small. Specifically, we show that\nimage-trained versions of two LLMs trained with the recent LongVU algorithm\nperform significantly above chance level on TVBench, a temporal reasoning\nbenchmark. Additionally, we introduce a simple finetuning scheme involving\nsequences of annotated images and questions targeting temporal capabilities.\nThis baseline results in temporal reasoning performance close to, and\noccasionally higher than, what is achieved by video-trained LLMs. This suggests\nsuboptimal utilization of rich temporal features found in real video by current\nmodels. Our analysis motivates further research into the mechanisms that allow\nimage-trained LLMs to perform temporal reasoning, as well as into the\nbottlenecks that render current video training schemes inefficient.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u901a\u8fc7\u56fe\u50cf\u8bad\u7ec3\u7684Video LLMs\u5728\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u4e8e\u9884\u671f\uff0c\u800c\u89c6\u9891\u8bad\u7ec3\u7684\u6539\u8fdb\u6548\u679c\u8f83\u5c0f\u3002", "motivation": "\u63a2\u7d22Video LLMs\u5728\u65f6\u95f4\u63a8\u7406\u4e0a\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u56fe\u50cf\u8bad\u7ec3\u4e0e\u89c6\u9891\u8bad\u7ec3\u7684\u6548\u679c\u5dee\u5f02\u3002", "method": "\u4f7f\u7528LongVU\u7b97\u6cd5\u8bad\u7ec3\u4e24\u79cdLLMs\uff0c\u5e76\u5728TVBench\u57fa\u51c6\u4e0a\u6d4b\u8bd5\u5176\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff1b\u5f15\u5165\u57fa\u4e8e\u6807\u6ce8\u56fe\u50cf\u5e8f\u5217\u7684\u7b80\u5355\u5fae\u8c03\u65b9\u6848\u3002", "result": "\u56fe\u50cf\u8bad\u7ec3\u7684LLMs\u5728\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u4e14\u63a5\u8fd1\u6216\u4f18\u4e8e\u89c6\u9891\u8bad\u7ec3\u7684LLMs\u3002", "conclusion": "\u5f53\u524d\u89c6\u9891\u8bad\u7ec3\u65b9\u6848\u672a\u5145\u5206\u5229\u7528\u89c6\u9891\u7684\u65f6\u5e8f\u7279\u5f81\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u56fe\u50cf\u8bad\u7ec3LLMs\u7684\u65f6\u95f4\u63a8\u7406\u673a\u5236\u53ca\u89c6\u9891\u8bad\u7ec3\u7684\u74f6\u9888\u3002"}}
{"id": "2506.06944", "pdf": "https://arxiv.org/pdf/2506.06944", "abs": "https://arxiv.org/abs/2506.06944", "authors": ["Mellon M. Zhang", "Glen Chou", "Saibal Mukhopadhyay"], "title": "Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurate and efficient object detection is essential for autonomous vehicles,\nwhere real-time perception requires low latency and high throughput. LiDAR\nsensors provide robust depth information, but conventional methods process full\n360{\\deg} scans in a single pass, introducing significant delay. Streaming\napproaches address this by sequentially processing partial scans in the native\npolar coordinate system, yet they rely on translation-invariant convolutions\nthat are misaligned with polar geometry -- resulting in degraded performance or\nrequiring complex distortion mitigation. Recent Mamba-based state space models\n(SSMs) have shown promise for LiDAR perception, but only in the full-scan\nsetting, relying on geometric serialization and positional embeddings that are\nmemory-intensive and ill-suited to streaming. We propose Polar Hierarchical\nMamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming\nLiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial\nencoding and a global forward Mamba for inter-sector temporal modeling,\nreplacing convolutions and positional encodings with distortion-aware,\ndimensionally-decomposed operations. PHiM sets a new state-of-the-art among\nstreaming detectors on the Waymo Open Dataset, outperforming the previous best\nby 10\\% and matching full-scan baselines at twice the throughput. Code will be\navailable at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .", "AI": {"tldr": "PHiM\u662f\u4e00\u79cd\u65b0\u578bSSM\u67b6\u6784\uff0c\u4e13\u4e3a\u6781\u5750\u6807\u6d41\u5f0fLiDAR\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5c40\u90e8\u53cc\u5411Mamba\u5757\u548c\u5168\u5c40\u524d\u5411Mamba\u5757\uff0c\u53d6\u4ee3\u5377\u79ef\u548c\u4f4d\u7f6e\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u5f0f\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u7684\u5b9e\u65f6\u611f\u77e5\uff0c\u4f46\u4f20\u7edfLiDAR\u5904\u7406\u65b9\u6cd5\u5b58\u5728\u5ef6\u8fdf\u95ee\u9898\uff0c\u6d41\u5f0f\u65b9\u6cd5\u867d\u80fd\u7f13\u89e3\u4f46\u6027\u80fd\u53d7\u9650\u3002", "method": "PHiM\u91c7\u7528\u5c40\u90e8\u53cc\u5411Mamba\u5757\u8fdb\u884c\u7a7a\u95f4\u7f16\u7801\uff0c\u5168\u5c40\u524d\u5411Mamba\u5757\u8fdb\u884c\u65f6\u95f4\u5efa\u6a21\uff0c\u907f\u514d\u4e86\u5377\u79ef\u548c\u4f4d\u7f6e\u7f16\u7801\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\u3002", "result": "\u5728Waymo Open Dataset\u4e0a\uff0cPHiM\u6bd4\u4e4b\u524d\u6700\u4f73\u6d41\u5f0f\u68c0\u6d4b\u5668\u6027\u80fd\u63d0\u534710%\uff0c\u4e14\u541e\u5410\u91cf\u7ffb\u500d\u3002", "conclusion": "PHiM\u4e3a\u6781\u5750\u6807\u6d41\u5f0fLiDAR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2506.06952", "pdf": "https://arxiv.org/pdf/2506.06952", "abs": "https://arxiv.org/abs/2506.06952", "authors": ["Ying Shen", "Zhiyang Xu", "Jiuhai Chen", "Shizhe Diao", "Jiaxin Zhang", "Yuguang Yao", "Joy Rimchala", "Ismini Lourentzou", "Lifu Huang"], "title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer", "categories": ["cs.CV"], "comment": "Unified multimodal model, Flow-matching", "summary": "Recent advances in multimodal foundation models unifying image understanding\nand generation have opened exciting avenues for tackling a wide range of\nvision-language tasks within a single framework. Despite progress, existing\nunified models typically require extensive pretraining and struggle to achieve\nthe same level of performance compared to models dedicated to each task.\nAdditionally, many of these models suffer from slow image generation speeds,\nlimiting their practical deployment in real-time or resource-constrained\nsettings. In this work, we propose Layerwise Timestep-Expert Flow-based\nTransformer (LaTtE-Flow), a novel and efficient architecture that unifies image\nunderstanding and generation within a single multimodal model. LaTtE-Flow\nbuilds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong\nmultimodal understanding capabilities, and extends them with a novel Layerwise\nTimestep Experts flow-based architecture for efficient image generation.\nLaTtE-Flow distributes the flow-matching process across specialized groups of\nTransformer layers, each responsible for a distinct subset of timesteps. This\ndesign significantly improves sampling efficiency by activating only a small\nsubset of layers at each sampling timestep. To further enhance performance, we\npropose a Timestep-Conditioned Residual Attention mechanism for efficient\ninformation reuse across layers. Experiments demonstrate that LaTtE-Flow\nachieves strong performance on multimodal understanding tasks, while achieving\ncompetitive image generation quality with around 6x faster inference speed\ncompared to recent unified multimodal models.", "AI": {"tldr": "LaTtE-Flow\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7edf\u4e00\u4e86\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\uff0c\u901a\u8fc7\u5206\u5c42\u65f6\u95f4\u6b65\u4e13\u5bb6\u67b6\u6784\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u9700\u8981\u5927\u91cf\u9884\u8bad\u7ec3\u4e14\u6027\u80fd\u4e0d\u53ca\u4e13\u7528\u6a21\u578b\uff0c\u751f\u6210\u901f\u5ea6\u6162\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5f15\u5165\u5206\u5c42\u65f6\u95f4\u6b65\u4e13\u5bb6\u6d41\u67b6\u6784\u548c\u6761\u4ef6\u6b8b\u5dee\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\uff0c\u63a8\u7406\u901f\u5ea6\u5feb6\u500d\u3002", "conclusion": "LaTtE-Flow\u901a\u8fc7\u9ad8\u6548\u67b6\u6784\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u901f\u5ea6\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.06953", "pdf": "https://arxiv.org/pdf/2506.06953", "abs": "https://arxiv.org/abs/2506.06953", "authors": ["Maciej Zyrek", "Tomasz Tarasiewicz", "Jakub Sadel", "Aleksandra Krzywon", "Michal Kawulok"], "title": "Task-driven real-world super-resolution of document scans", "categories": ["cs.CV"], "comment": null, "summary": "Single-image super-resolution refers to the reconstruction of a\nhigh-resolution image from a single low-resolution observation. Although recent\ndeep learning-based methods have demonstrated notable success on simulated\ndatasets -- with low-resolution images obtained by degrading and downsampling\nhigh-resolution ones -- they frequently fail to generalize to real-world\nsettings, such as document scans, which are affected by complex degradations\nand semantic variability. In this study, we introduce a task-driven, multi-task\nlearning framework for training a super-resolution network specifically\noptimized for optical character recognition tasks. We propose to incorporate\nauxiliary loss functions derived from high-level vision tasks, including text\ndetection using the connectionist text proposal network, text recognition via a\nconvolutional recurrent neural network, keypoints localization using Key.Net,\nand hue consistency. To balance these diverse objectives, we employ dynamic\nweight averaging mechanism, which adaptively adjusts the relative importance of\neach loss term based on its convergence behavior. We validate our approach upon\nthe SRResNet architecture, which is a well-established technique for\nsingle-image super-resolution. Experimental evaluations on both simulated and\nreal-world scanned document datasets demonstrate that the proposed approach\nimproves text detection, measured with intersection over union, while\npreserving overall image fidelity. These findings underscore the value of\nmulti-objective optimization in super-resolution models for bridging the gap\nbetween simulated training regimes and practical deployment in real-world\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u4efb\u52a1\u4f18\u5316\u7684\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u548c\u52a8\u6001\u6743\u91cd\u5e73\u5747\u673a\u5236\uff0c\u7ed3\u5408\u9ad8\u5c42\u89c6\u89c9\u4efb\u52a1\u7684\u8f85\u52a9\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u4e86\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6587\u672c\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\uff08\u5982\u6587\u6863\u626b\u63cf\uff09\u4e2d\u56e0\u590d\u6742\u9000\u5316\u548c\u8bed\u4e49\u53d8\u5316\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u68c0\u6d4b\u3001\u8bc6\u522b\u3001\u5173\u952e\u70b9\u5b9a\u4f4d\u548c\u8272\u8c03\u4e00\u81f4\u6027\u7b49\u8f85\u52a9\u635f\u5931\u51fd\u6570\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u6743\u91cd\u5e73\u5747\u673a\u5236\u5e73\u8861\u76ee\u6807\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6587\u6863\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u4e86\u6587\u672c\u68c0\u6d4b\u6027\u80fd\uff08IoU\u6307\u6807\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u591a\u76ee\u6807\u4f18\u5316\u6709\u52a9\u4e8e\u7f29\u5c0f\u6a21\u62df\u8bad\u7ec3\u4e0e\u771f\u5b9e\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.06962", "pdf": "https://arxiv.org/pdf/2506.06962", "abs": "https://arxiv.org/abs/2506.06962", "authors": ["Jingyuan Qi", "Zhiyang Xu", "Qifan Wang", "Lifu Huang"], "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation", "categories": ["cs.CV"], "comment": "Image Generation, Retrieval Augmented Generation", "summary": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm\nthat enhances image generation by autoregressively incorporating knearest\nneighbor retrievals at the patch level. Unlike prior methods that perform a\nsingle, static retrieval before generation and condition the entire generation\non fixed reference images, AR-RAG performs context-aware retrievals at each\ngeneration step, using prior-generated patches as queries to retrieve and\nincorporate the most relevant patch-level visual references, enabling the model\nto respond to evolving generation needs while avoiding limitations (e.g.,\nover-copying, stylistic bias, etc.) prevalent in existing methods. To realize\nAR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in\nDecoding (DAiD), a training-free plug-and-use decoding strategy that directly\nmerges the distribution of model-predicted patches with the distribution of\nretrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a\nparameter-efficient fine-tuning method that progressively smooths the features\nof retrieved patches via multi-scale convolution operations and leverages them\nto augment the image generation process. We validate the effectiveness of\nAR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and\nDPG-Bench, demonstrating significant performance gains over state-of-the-art\nimage generation models.", "AI": {"tldr": "AR-RAG\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u68c0\u7d22\u548c\u6574\u5408\u76f8\u5173\u56fe\u50cf\u5757\uff0c\u52a8\u6001\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u56fe\u50cf\u65f6\u901a\u5e38\u57fa\u4e8e\u56fa\u5b9a\u7684\u53c2\u8003\u56fe\u50cf\uff0c\u5bb9\u6613\u5bfc\u81f4\u8fc7\u5ea6\u590d\u5236\u6216\u98ce\u683c\u504f\u5dee\uff0cAR-RAG\u65e8\u5728\u52a8\u6001\u9002\u5e94\u751f\u6210\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6846\u67b6\uff1aDAiD\uff08\u76f4\u63a5\u5408\u5e76\u9884\u6d4b\u548c\u68c0\u7d22\u5757\u7684\u5206\u5e03\uff09\u548cFAiD\uff08\u901a\u8fc7\u591a\u5c3a\u5ea6\u5377\u79ef\u5e73\u6ed1\u68c0\u7d22\u5757\u7279\u5f81\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5982Midjourney-30K\u3001GenEval\u548cDPG-Bench\uff09\uff0cAR-RAG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3002", "conclusion": "AR-RAG\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u548c\u6574\u5408\u56fe\u50cf\u5757\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.06966", "pdf": "https://arxiv.org/pdf/2506.06966", "abs": "https://arxiv.org/abs/2506.06966", "authors": ["Siyuan Jing", "Guangxue Wang", "Haoyang Zhai", "Qin Tao", "Jun Yang", "Bing Wang", "Peng Jin"], "title": "Dual-view Spatio-Temporal Feature Fusion with CNN-Transformer Hybrid Network for Chinese Isolated Sign Language Recognition", "categories": ["cs.CV"], "comment": "18 pages, 3 figures", "summary": "Due to the emergence of many sign language datasets, isolated sign language\nrecognition (ISLR) has made significant progress in recent years. In addition,\nthe development of various advanced deep neural networks is another reason for\nthis breakthrough. However, challenges remain in applying the technique in the\nreal world. First, existing sign language datasets do not cover the whole sign\nvocabulary. Second, most of the sign language datasets provide only single view\nRGB videos, which makes it difficult to handle hand occlusions when performing\nISLR. To fill this gap, this paper presents a dual-view sign language dataset\nfor ISLR named NationalCSL-DP, which fully covers the Chinese national sign\nlanguage vocabulary. The dataset consists of 134140 sign videos recorded by ten\nsigners with respect to two vertical views, namely, the front side and the left\nside. Furthermore, a CNN transformer network is also proposed as a strong\nbaseline and an extremely simple but effective fusion strategy for prediction.\nExtensive experiments were conducted to prove the effectiveness of the datasets\nas well as the baseline. The results show that the proposed fusion strategy can\nsignificantly increase the performance of the ISLR, but it is not easy for the\nsequence-to-sequence model, regardless of whether the early-fusion or\nlate-fusion strategy is applied, to learn the complementary features from the\nsign videos of two vertical views.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u89c6\u89d2\u7684\u4e2d\u56fd\u624b\u8bed\u6570\u636e\u96c6NationalCSL-DP\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2aCNN-Transformer\u7f51\u7edc\u4f5c\u4e3a\u57fa\u7ebf\u6a21\u578b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u548c\u878d\u5408\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u6570\u636e\u96c6\u8986\u76d6\u8bcd\u6c47\u4e0d\u5168\u4e14\u591a\u4e3a\u5355\u89c6\u89d2RGB\u89c6\u9891\uff0c\u96be\u4ee5\u5904\u7406\u624b\u90e8\u906e\u6321\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u6539\u8fdb\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u8986\u76d6\u4e2d\u56fd\u624b\u8bed\u8bcd\u6c47\u7684\u53cc\u89c6\u89d2\u6570\u636e\u96c6NationalCSL-DP\uff0c\u5e76\u63d0\u51faCNN-Transformer\u7f51\u7edc\u53ca\u7b80\u5355\u6709\u6548\u7684\u878d\u5408\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u878d\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u6027\u80fd\uff0c\u4f46\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\u96be\u4ee5\u4ece\u53cc\u89c6\u89d2\u89c6\u9891\u4e2d\u5b66\u4e60\u4e92\u8865\u7279\u5f81\u3002", "conclusion": "\u53cc\u89c6\u89d2\u6570\u636e\u96c6\u548c\u878d\u5408\u7b56\u7565\u4e3a\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4f46\u6a21\u578b\u4ecd\u9700\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5229\u7528\u591a\u89c6\u89d2\u4fe1\u606f\u3002"}}
{"id": "2506.06970", "pdf": "https://arxiv.org/pdf/2506.06970", "abs": "https://arxiv.org/abs/2506.06970", "authors": ["Pengfei Zhao", "Rongbo Luan", "Wei Zhang", "Peng Wu", "Sifeng He"], "title": "Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability\nto retrieve content across modalities, a substantial modality gap persists in\nits feature space. Intriguingly, we discover that off-the-shelf MLLMs\n(Multimodal Large Language Models) demonstrate powerful inherent modality\nalignment properties. While recent MLLM-based retrievers with unified\narchitectures partially mitigate this gap, their reliance on coarse modality\nalignment mechanisms fundamentally limits their potential. In this work, We\nintroduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel\nframework that leverages the fine grained alignment priors inherent in MLLM to\nguide cross modal representation learning. MAPLE formulates the learning\nprocess as reinforcement learning with two key components: (1) Automatic\npreference data construction using off-the-shelf MLLM, and (2) a new Relative\nPreference Alignment (RPA) loss, which adapts Direct Preference Optimization\n(DPO) to the embedding learning setting. Experimental results show that our\npreference-guided alignment achieves substantial gains in fine-grained\ncross-modal retrieval, underscoring its effectiveness in handling nuanced\nsemantic distinctions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMAPLE\u6846\u67b6\uff0c\u5229\u7528MLLM\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u7279\u6027\u6539\u8fdb\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u65b0\u635f\u5931\u51fd\u6570\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1CLIP\u5728\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u5b58\u5728\u6a21\u6001\u9e3f\u6c9f\u95ee\u9898\u3002MLLM\u5177\u6709\u5929\u7136\u5bf9\u9f50\u7279\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u5bf9\u9f50\u673a\u5236\uff0c\u9650\u5236\u4e86\u6f5c\u529b\u3002", "method": "\u63d0\u51faMAPLE\u6846\u67b6\uff0c\u7ed3\u5408MLLM\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u5148\u9a8c\uff0c\u901a\u8fc7\u81ea\u52a8\u504f\u597d\u6570\u636e\u6784\u5efa\u548cRPA\u635f\u5931\u51fd\u6570\uff08\u57fa\u4e8eDPO\uff09\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAPLE\u5728\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u68c0\u7d22\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u80fd\u6709\u6548\u5904\u7406\u8bed\u4e49\u7ec6\u5fae\u5dee\u5f02\u3002", "conclusion": "MAPLE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u673a\u5236\u663e\u8457\u7f29\u5c0f\u6a21\u6001\u9e3f\u6c9f\uff0c\u4e3a\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.06988", "pdf": "https://arxiv.org/pdf/2506.06988", "abs": "https://arxiv.org/abs/2506.06988", "authors": ["Binxiao Huang", "Zhihao Li", "Shiyong Liu", "Xiao Tang", "Jiajun Tang", "Jiaqi Lin", "Yuxin Cheng", "Zhenyu Chen", "Xiaofei Wu", "Ngai Wong"], "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian splatting (3DGS) has demonstrated exceptional performance in\nimage-based 3D reconstruction and real-time rendering. However, regions with\ncomplex textures require numerous Gaussians to capture significant color\nvariations accurately, leading to inefficiencies in rendering speed. To address\nthis challenge, we introduce a hybrid representation for indoor scenes that\ncombines 3DGS with textured meshes. Our approach uses textured meshes to handle\ntexture-rich flat areas, while retaining Gaussians to model intricate\ngeometries. The proposed method begins by pruning and refining the extracted\nmesh to eliminate geometrically complex regions. We then employ a joint\noptimization for 3DGS and mesh, incorporating a warm-up strategy and\ntransmittance-aware supervision to balance their contributions\nseamlessly.Extensive experiments demonstrate that the hybrid representation\nmaintains comparable rendering quality and achieves superior frames per second\nFPS with fewer Gaussian primitives.", "AI": {"tldr": "3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5728\u56fe\u50cf\u91cd\u5efa\u548c\u5b9e\u65f6\u6e32\u67d3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u590d\u6742\u7eb9\u7406\u533a\u57df\u9700\u8981\u5927\u91cf\u9ad8\u65af\u5206\u5e03\uff0c\u5bfc\u81f4\u6e32\u67d3\u6548\u7387\u4f4e\u3002\u672c\u6587\u63d0\u51fa\u7ed3\u54083DGS\u4e0e\u7eb9\u7406\u7f51\u683c\u7684\u6df7\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u4f18\u5316\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b33DGS\u5728\u590d\u6742\u7eb9\u7406\u533a\u57df\u56e0\u9ad8\u65af\u5206\u5e03\u8fc7\u591a\u5bfc\u81f4\u7684\u6e32\u67d3\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u7eb9\u7406\u7f51\u683c\u5904\u7406\u5e73\u5766\u533a\u57df\uff0c\u4fdd\u7559\u9ad8\u65af\u5206\u5e03\u5904\u7406\u590d\u6742\u51e0\u4f55\u3002\u901a\u8fc7\u7f51\u683c\u4fee\u526a\u548c\u8054\u5408\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6df7\u5408\u8868\u793a\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86FPS\u5e76\u51cf\u5c11\u4e86\u9ad8\u65af\u57fa\u5143\u6570\u91cf\u3002", "conclusion": "\u6df7\u5408\u8868\u793a\u65b9\u6cd5\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2506.06992", "pdf": "https://arxiv.org/pdf/2506.06992", "abs": "https://arxiv.org/abs/2506.06992", "authors": ["Yanting Gao", "Yepeng Liu", "Junming Liu", "Qi Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "title": "Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization", "categories": ["cs.CV"], "comment": "22 pages", "summary": "Exploring effective and transferable adversarial examples is vital for\nunderstanding the characteristics and mechanisms of Vision Transformers (ViTs).\nHowever, adversarial examples generated from surrogate models often exhibit\nweak transferability in black-box settings due to overfitting. Existing methods\nimprove transferability by diversifying perturbation inputs or applying uniform\ngradient regularization within surrogate models, yet they have not fully\nleveraged the shared and unique features of surrogate models trained on the\nsame task, leading to suboptimal transfer performance. Therefore, enhancing\nperturbations of common information shared by surrogate models and suppressing\nthose tied to individual characteristics offers an effective way to improve\ntransferability. Accordingly, we propose a commonality-oriented gradient\noptimization strategy (COGO) consisting of two components: Commonality\nEnhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low\nfrequency regions, leveraging the fact that ViTs trained on the same dataset\ntend to rely more on mid-to-low frequency information for classification. IS\nemploys adaptive thresholds to evaluate the correlation between backpropagated\ngradients and model individuality, assigning weights to gradients accordingly.\nExtensive experiments demonstrate that COGO significantly improves the transfer\nsuccess rates of adversarial attacks, outperforming current state-of-the-art\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOGO\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u589e\u5f3a\u5171\u4eab\u7279\u5f81\u548c\u6291\u5236\u4e2a\u4f53\u7279\u5f81\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6297\u6837\u672c\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e2d\u7684\u8fc1\u79fb\u6210\u529f\u7387\u3002", "motivation": "\u7406\u89e3Vision Transformers\uff08ViTs\uff09\u7684\u7279\u6027\u9700\u8981\u6709\u6548\u7684\u5bf9\u6297\u6837\u672c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u8fc7\u62df\u5408\u5bfc\u81f4\u8fc1\u79fb\u6027\u5dee\u3002", "method": "COGO\u7b56\u7565\u5305\u62ec\u4e24\u90e8\u5206\uff1a\u5171\u6027\u589e\u5f3a\uff08CE\uff09\u6270\u52a8\u4e2d\u4f4e\u9891\u533a\u57df\uff0c\u4e2a\u6027\u6291\u5236\uff08IS\uff09\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u8bc4\u4f30\u68af\u5ea6\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCOGO\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u8fc1\u79fb\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "COGO\u901a\u8fc7\u4f18\u5316\u5171\u4eab\u548c\u6291\u5236\u4e2a\u4f53\u7279\u5f81\uff0c\u4e3aViTs\u5bf9\u6297\u6837\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.06993", "pdf": "https://arxiv.org/pdf/2506.06993", "abs": "https://arxiv.org/abs/2506.06993", "authors": ["Cong Guan", "Jiacheng Ying", "Yuya Ieiri", "Osamu Yoshie"], "title": "DM$^3$Net: Dual-Camera Super-Resolution via Domain Modulation and Multi-scale Matching", "categories": ["cs.CV"], "comment": null, "summary": "Dual-camera super-resolution is highly practical for smartphone photography\nthat primarily super-resolve the wide-angle images using the telephoto image as\na reference. In this paper, we propose DM$^3$Net, a novel dual-camera\nsuper-resolution network based on Domain Modulation and Multi-scale Matching.\nTo bridge the domain gap between the high-resolution domain and the degraded\ndomain, we learn two compressed global representations from image pairs\ncorresponding to the two domains. To enable reliable transfer of high-frequency\nstructural details from the reference image, we design a multi-scale matching\nmodule that conducts patch-level feature matching and retrieval across multiple\nreceptive fields to improve matching accuracy and robustness. Moreover, we also\nintroduce Key Pruning to achieve a significant reduction in memory usage and\ninference time with little model performance sacrificed. Experimental results\non three real-world datasets demonstrate that our DM$^3$Net outperforms the\nstate-of-the-art approaches.", "AI": {"tldr": "DM$^3$Net\u662f\u4e00\u79cd\u57fa\u4e8e\u57df\u8c03\u5236\u548c\u591a\u5c3a\u5ea6\u5339\u914d\u7684\u53cc\u6444\u50cf\u5934\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\uff0c\u7528\u4e8e\u63d0\u5347\u667a\u80fd\u624b\u673a\u6444\u5f71\u4e2d\u5e7f\u89d2\u56fe\u50cf\u7684\u5206\u8fa8\u7387\u3002", "motivation": "\u89e3\u51b3\u53cc\u6444\u50cf\u5934\u8d85\u5206\u8fa8\u7387\u4e2d\u9ad8\u5206\u8fa8\u7387\u57df\u4e0e\u9000\u5316\u57df\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u9ad8\u9891\u7ed3\u6784\u7ec6\u8282\u7684\u4f20\u8f93\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u4e24\u4e2a\u538b\u7f29\u5168\u5c40\u8868\u793a\u6765\u6865\u63a5\u57df\u5dee\u8ddd\uff0c\u5e76\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u5339\u914d\u6a21\u5757\u8fdb\u884c\u8de8\u591a\u611f\u53d7\u91ce\u7684\u8865\u4e01\u7ea7\u7279\u5f81\u5339\u914d\u548c\u68c0\u7d22\u3002\u6b64\u5916\uff0c\u5f15\u5165\u5173\u952e\u4fee\u526a\u4ee5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u65f6\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDM$^3$Net\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DM$^3$Net\u901a\u8fc7\u57df\u8c03\u5236\u548c\u591a\u5c3a\u5ea6\u5339\u914d\u6709\u6548\u63d0\u5347\u4e86\u53cc\u6444\u50cf\u5934\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u5173\u952e\u4fee\u526a\u4f18\u5316\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.06995", "pdf": "https://arxiv.org/pdf/2506.06995", "abs": "https://arxiv.org/abs/2506.06995", "authors": ["Xiaoya Zhang"], "title": "Technical Report for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge: Adaptive Point Cloud Understanding for Heterogeneous Robotic Systems", "categories": ["cs.CV"], "comment": "Winner of the GOOSE 3D Semantic Segmentation Challenge at the IEEE\n  ICRA Workshop on Field Robotics 2025", "summary": "This technical report presents the implementation details of the winning\nsolution for the ICRA 2025 GOOSE 3D Semantic Segmentation Challenge. This\nchallenge focuses on semantic segmentation of 3D point clouds from diverse\nunstructured outdoor environments collected from multiple robotic platforms.\nThis problem was addressed by implementing Point Prompt Tuning (PPT) integrated\nwith Point Transformer v3 (PTv3) backbone, enabling adaptive processing of\nheterogeneous LiDAR data through platform-specific conditioning and\ncross-dataset class alignment strategies. The model is trained without\nrequiring additional external data. As a result, this approach achieved\nsubstantial performance improvements with mIoU increases of up to 22.59% on\nchallenging platforms compared to the baseline PTv3 model, demonstrating the\neffectiveness of adaptive point cloud understanding for field robotics\napplications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ICRA 2025 GOOSE 3D\u8bed\u4e49\u5206\u5272\u6311\u6218\u8d5b\u4e2d\u83b7\u80dc\u89e3\u51b3\u65b9\u6848\u7684\u5b9e\u73b0\u7ec6\u8282\uff0c\u901a\u8fc7\u7ed3\u5408Point Prompt Tuning\uff08PPT\uff09\u548cPoint Transformer v3\uff08PTv3\uff09\u9aa8\u5e72\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f02\u6784LiDAR\u6570\u636e\u7684\u81ea\u9002\u5e94\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5e73\u53f0\u91c7\u96c6\u7684\u6237\u5916\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d3D\u70b9\u4e91\u7684\u8bed\u4e49\u5206\u5272\u95ee\u9898\u3002", "method": "\u91c7\u7528PPT\u4e0ePTv3\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u53f0\u7279\u5b9a\u6761\u4ef6\u548c\u8de8\u6570\u636e\u96c6\u7c7b\u522b\u5bf9\u9f50\u7b56\u7565\u5904\u7406\u6570\u636e\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebfPTv3\u6a21\u578b\uff0cmIoU\u63d0\u5347\u9ad8\u8fbe22.59%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u70b9\u4e91\u7406\u89e3\u5728\u91ce\u5916\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.07002", "pdf": "https://arxiv.org/pdf/2506.07002", "abs": "https://arxiv.org/abs/2506.07002", "authors": ["Yunxiao Shi", "Hong Cai", "Jisoo Jeong", "Yinhao Zhu", "Shizhong Han", "Amin Ansari", "Fatih Porikli"], "title": "BePo: Leveraging Birds Eye View and Sparse Points for Efficient and Accurate 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": "Two-page abstract version available at CVPR 2025 Embodied AI Workshop", "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, BePo, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed BePo. Moreover,\nBePo also delivers competitive inference speed when compared to the latest\nefficient approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BEV\u548c\u7a00\u758f\u70b9\u8868\u793a\u7684\u65b0\u65b9\u6cd5BePo\uff0c\u7528\u4e8e3D\u5360\u7528\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5c0f\u7269\u4f53\u548c\u5e73\u5766\u8868\u9762\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u67093D\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u5728\u5c0f\u7269\u4f53\u548c\u5e73\u5766\u8868\u9762\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u8bbe\u8ba1\uff1a\u57fa\u4e8e\u67e5\u8be2\u7684\u7a00\u758f\u70b9\u5206\u652f\u548cBEV\u5206\u652f\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5171\u4eab\u4fe1\u606f\uff0c\u6700\u7ec8\u878d\u5408\u8f93\u51fa\u3002", "result": "\u5728Occ3D-nuScenes\u548cOcc3D-Waymo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "BePo\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a3D\u5360\u7528\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.07013", "pdf": "https://arxiv.org/pdf/2506.07013", "abs": "https://arxiv.org/abs/2506.07013", "authors": ["Wentao Zhao", "Yihe Niu", "Yanbo Wang", "Tianchen Deng", "Shenghai Yuan", "Zhenli Wang", "Rui Guo", "Jingchuan Wang"], "title": "UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment", "categories": ["cs.CV"], "comment": "15pages, 8 figures", "summary": "This work presents UNO, a unified monocular visual odometry framework that\nenables robust and adaptable pose estimation across diverse environments,\nplatforms, and motion patterns. Unlike traditional methods that rely on\ndeployment-specific tuning or predefined motion priors, our approach\ngeneralizes effectively across a wide range of real-world scenarios, including\nautonomous vehicles, aerial drones, mobile robots, and handheld devices. To\nthis end, we introduce a Mixture-of-Experts strategy for local state\nestimation, with several specialized decoders that each handle a distinct class\nof ego-motion patterns. Moreover, we introduce a fully differentiable\nGumbel-Softmax module that constructs a robust inter-frame correlation graph,\nselects the optimal expert decoder, and prunes erroneous estimates. These cues\nare then fed into a unified back-end that combines pre-trained,\nscale-independent depth priors with a lightweight bundling adjustment to\nenforce geometric consistency. We extensively evaluate our method on three\nmajor benchmark datasets: KITTI (outdoor/autonomous driving), EuRoC-MAV\n(indoor/aerial drones), and TUM-RGBD (indoor/handheld), demonstrating\nstate-of-the-art performance.", "AI": {"tldr": "UNO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u5728\u591a\u6837\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u4e14\u81ea\u9002\u5e94\u7684\u4f4d\u59ff\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u90e8\u7f72\u7684\u8c03\u4f18\u6216\u9884\u5b9a\u4e49\u7684\u8fd0\u52a8\u5148\u9a8c\uff0c\u800cUNO\u65e8\u5728\u6cdb\u5316\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u573a\u666f\uff0c\u5982\u81ea\u52a8\u9a7e\u9a76\u3001\u65e0\u4eba\u673a\u3001\u79fb\u52a8\u673a\u5668\u4eba\u548c\u624b\u6301\u8bbe\u5907\u3002", "method": "\u91c7\u7528Mixture-of-Experts\u7b56\u7565\u8fdb\u884c\u5c40\u90e8\u72b6\u6001\u4f30\u8ba1\uff0c\u7ed3\u5408\u591a\u4e2a\u4e13\u7528\u89e3\u7801\u5668\u5904\u7406\u4e0d\u540c\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528Gumbel-Softmax\u6a21\u5757\u6784\u5efa\u5e27\u95f4\u5173\u8054\u56fe\u3001\u9009\u62e9\u6700\u4f18\u89e3\u7801\u5668\u5e76\u5254\u9664\u9519\u8bef\u4f30\u8ba1\u3002\u540e\u7aef\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u5c3a\u5ea6\u65e0\u5173\u6df1\u5ea6\u5148\u9a8c\u548c\u8f7b\u91cf\u7ea7\u6346\u7ed1\u8c03\u6574\u3002", "result": "\u5728KITTI\u3001EuRoC-MAV\u548cTUM-RGBD\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "UNO\u6846\u67b6\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u81ea\u9002\u5e94\u7684\u4f4d\u59ff\u4f30\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.07015", "pdf": "https://arxiv.org/pdf/2506.07015", "abs": "https://arxiv.org/abs/2506.07015", "authors": ["Qiyu Hou", "Jun Wang"], "title": "TABLET: Table Structure Recognition using Encoder-only Transformers", "categories": ["cs.CV", "cs.LG"], "comment": "ICDAR 2025", "summary": "To address the challenges of table structure recognition, we propose a novel\nSplit-Merge-based top-down model optimized for large, densely populated tables.\nOur approach formulates row and column splitting as sequence labeling tasks,\nutilizing dual Transformer encoders to capture feature interactions. The\nmerging process is framed as a grid cell classification task, leveraging an\nadditional Transformer encoder to ensure accurate and coherent merging. By\neliminating unstable bounding box predictions, our method reduces resolution\nloss and computational complexity, achieving high accuracy while maintaining\nfast processing speed. Extensive experiments on FinTabNet and PubTabNet\ndemonstrate the superiority of our model over existing approaches, particularly\nin real-world applications. Our method offers a robust, scalable, and efficient\nsolution for large-scale table recognition, making it well-suited for\nindustrial deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSplit-Merge\u7684\u65b0\u578b\u8868\u683c\u7ed3\u6784\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ccTransformer\u7f16\u7801\u5668\u4f18\u5316\u884c\u548c\u5217\u7684\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u7f51\u683c\u5206\u7c7b\u5b9e\u73b0\u9ad8\u6548\u5408\u5e76\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5927\u800c\u5bc6\u96c6\u7684\u8868\u683c\u7ed3\u6784\u8bc6\u522b\u4e2d\u7684\u6311\u6218\uff0c\u5982\u4e0d\u7a33\u5b9a\u7684\u8fb9\u754c\u6846\u9884\u6d4b\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5c06\u884c\u548c\u5217\u5206\u5272\u89c6\u4e3a\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\uff0c\u4f7f\u7528\u53ccTransformer\u7f16\u7801\u5668\u6355\u6349\u7279\u5f81\u4ea4\u4e92\uff1b\u5408\u5e76\u8fc7\u7a0b\u4f5c\u4e3a\u7f51\u683c\u5206\u7c7b\u4efb\u52a1\uff0c\u5229\u7528\u989d\u5916Transformer\u7f16\u7801\u5668\u786e\u4fdd\u51c6\u786e\u6027\u3002", "result": "\u5728FinTabNet\u548cPubTabNet\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u9ad8\u4e14\u5904\u7406\u901f\u5ea6\u5feb\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u8868\u683c\u8bc6\u522b\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5de5\u4e1a\u90e8\u7f72\u3002"}}
{"id": "2506.07016", "pdf": "https://arxiv.org/pdf/2506.07016", "abs": "https://arxiv.org/abs/2506.07016", "authors": ["Sanjoy Chowdhury", "Mohamed Elmoghany", "Yohan Abeysinghe", "Junjie Fei", "Sayan Nag", "Salman Khan", "Mohamed Elhoseiny", "Dinesh Manocha"], "title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks", "categories": ["cs.CV", "cs.AI"], "comment": "Audio-visual learning, Audio-Visual RAG, Multi-Video Linkage", "summary": "Large multimodal models (LMMs) have shown remarkable progress in audio-visual\nunderstanding, yet they struggle with real-world scenarios that require complex\nreasoning across extensive video collections. Existing benchmarks for video\nquestion answering remain limited in scope, typically involving one clip per\nquery, which falls short of representing the challenges of large-scale,\naudio-visual retrieval and reasoning encountered in practical applications. To\nbridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal\nis to identify salient segments across different videos in response to a query\nand link them together to generate the most informative answer. To this end, we\npresent AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA\npairs designed to assess the capabilities of LMMs in multi-video retrieval and\ntemporal grounding task. Additionally, we propose a model-agnostic, multi-agent\nframework MAGNET to address this challenge, achieving up to 89% and 65%\nrelative improvements over baseline methods on BLEU@4 and GPT evaluation scores\nin QA task on our proposed AVHaystacks. To enable robust evaluation of\nmulti-video retrieval and temporal grounding for optimal response generation,\nwe introduce two new metrics, STEM, which captures alignment errors between a\nground truth and a predicted step sequence and MTGS, to facilitate balanced and\ninterpretable evaluation of segment-level grounding performance. Project:\nhttps://schowdhury671.github.io/magnet_project/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AV-HaystacksQA\u4efb\u52a1\u548cAVHaystacks\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u591a\u89c6\u9891\u68c0\u7d22\u548c\u65f6\u5e8f\u5b9a\u4f4d\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86MAGNET\u6846\u67b6\u548c\u4e24\u4e2a\u65b0\u8bc4\u4f30\u6307\u6807STEM\u548cMTGS\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u5c40\u9650\u4e8e\u5355\u89c6\u9891\u67e5\u8be2\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5927\u89c4\u6a21\u97f3\u9891-\u89c6\u89c9\u68c0\u7d22\u548c\u63a8\u7406\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faAV-HaystacksQA\u4efb\u52a1\u548cAVHaystacks\u57fa\u51c6\uff0c\u8bbe\u8ba1MAGNET\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5e76\u5f15\u5165STEM\u548cMTGS\u8bc4\u4f30\u6307\u6807\u3002", "result": "MAGNET\u5728BLEU@4\u548cGPT\u8bc4\u4f30\u5206\u6570\u4e0a\u5206\u522b\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8689%\u548c65%\u3002", "conclusion": "AVHaystacks\u548cMAGNET\u4e3a\u591a\u89c6\u9891\u68c0\u7d22\u548c\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.07045", "pdf": "https://arxiv.org/pdf/2506.07045", "abs": "https://arxiv.org/abs/2506.07045", "authors": ["Yikun Ji", "Hong Yan", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Qi Fan", "Liqing Zhang", "Jianfu Zhang"], "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u6ce8\u6570\u636e\u96c6\u548c\u591a\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u548c\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u591a\u4e3a\u9ed1\u7bb1\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff1bMLLMs\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u4e14\u89c6\u89c9\u89e3\u91ca\u4e0e\u5185\u5bb9\u4e0d\u7b26\u3002", "method": "\u6784\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff08\u542b\u8fb9\u754c\u6846\u548c\u63cf\u8ff0\u6027\u6807\u9898\uff09\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u5fae\u8c03MLLMs\uff0c\u5e73\u8861\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u89e3\u91ca\u76ee\u6807\u3002", "result": "\u6a21\u578b\u5728\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u548c\u5b9a\u4f4d\u89c6\u89c9\u7f3a\u9677\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u6807\u6ce8\u548c\u591a\u9636\u6bb5\u4f18\u5316\uff0cMLLMs\u53ef\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u3002"}}
{"id": "2506.07050", "pdf": "https://arxiv.org/pdf/2506.07050", "abs": "https://arxiv.org/abs/2506.07050", "authors": ["Zheng Wang", "Kai Ying", "Bin Xu", "Chunjiao Wang", "Cong Bai"], "title": "From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": null, "summary": "Accurate near-real-time precipitation retrieval has been enhanced by\nsatellite-based technologies. However, infrared-based algorithms have low\naccuracy due to weak relations with surface precipitation, whereas passive\nmicrowave and radar-based methods are more accurate but limited in range. This\nchallenge motivates the Precipitation Retrieval Expansion (PRE) task, which\naims to enable accurate, infrared-based full-disc precipitation retrievals\nbeyond the scanning swath. We introduce Multimodal Knowledge Expansion, a\ntwo-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling\nstage, PRE-Net transfers knowledge from a multimodal data integration model to\nan infrared-based model within the scanning swath via Coordinated Masking and\nWavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune\nrefines predictions across the full disc by balancing multimodal and full-disc\ninfrared knowledge. Experiments on the introduced PRE benchmark demonstrate\nthat PRE-Net significantly advanced precipitation retrieval performance,\noutperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code\nwill be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPRE-Net\u7684\u4e24\u9636\u6bb5\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u77e5\u8bc6\u6269\u5c55\u6280\u672f\u63d0\u5347\u7ea2\u5916\u964d\u6c34\u53cd\u6f14\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8303\u56f4\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7ea2\u5916\u964d\u6c34\u53cd\u6f14\u7b97\u6cd5\u7cbe\u5ea6\u4f4e\uff0c\u800c\u5fae\u6ce2\u548c\u96f7\u8fbe\u65b9\u6cd5\u8303\u56f4\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u8986\u76d6\u5168\u76d8\u53c8\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u626b\u63cf\u5e26\u84b8\u998f\u9636\u6bb5\u901a\u8fc7CoMWE\u6280\u672f\u5c06\u591a\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u5230\u7ea2\u5916\u6a21\u578b\uff1b\u5168\u76d8\u9002\u5e94\u9636\u6bb5\u901a\u8fc7Self-MaskTune\u5e73\u8861\u591a\u6a21\u6001\u4e0e\u7ea2\u5916\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPRE-Net\u5728\u964d\u6c34\u53cd\u6f14\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8ePERSIANN-CCS\u3001PDIR\u548cIMERG\u7b49\u73b0\u6709\u4ea7\u54c1\u3002", "conclusion": "PRE-Net\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5168\u76d8\u7ea2\u5916\u964d\u6c34\u53cd\u6f14\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07055", "pdf": "https://arxiv.org/pdf/2506.07055", "abs": "https://arxiv.org/abs/2506.07055", "authors": ["Tarique Dahri", "Zulfiqar Ali Memon", "Zhenyu Yu", "Mohd. Yamani Idna Idris", "Sheheryar Khan", "Sadiq Ahmad", "Maged Shoman", "Saddam Aziz", "Rizwan Qureshi"], "title": "A Layered Self-Supervised Knowledge Distillation Framework for Efficient Multimodal Learning on the Edge", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Layered Self-Supervised Knowledge Distillation (LSSKD) framework\nfor training compact deep learning models. Unlike traditional methods that rely\non pre-trained teacher networks, our approach appends auxiliary classifiers to\nintermediate feature maps, generating diverse self-supervised knowledge and\nenabling one-to-one transfer across different network stages. Our method\nachieves an average improvement of 4.54\\% over the state-of-the-art PS-KD\nmethod and a 1.14% gain over SSKD on CIFAR-100, with a 0.32% improvement on\nImageNet compared to HASSKD. Experiments on Tiny ImageNet and CIFAR-100 under\nfew-shot learning scenarios also achieve state-of-the-art results. These\nfindings demonstrate the effectiveness of our approach in enhancing model\ngeneralization and performance without the need for large over-parameterized\nteacher networks. Importantly, at the inference stage, all auxiliary\nclassifiers can be removed, yielding no extra computational cost. This makes\nour model suitable for deploying small language models on affordable\nlow-computing devices. Owing to its lightweight design and adaptability, our\nframework is particularly suitable for multimodal sensing and cyber-physical\nenvironments that require efficient and responsive inference. LSSKD facilitates\nthe development of intelligent agents capable of learning from limited sensory\ndata under weak supervision.", "AI": {"tldr": "LSSKD\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u4e2d\u95f4\u7279\u5f81\u56fe\u7684\u8f85\u52a9\u5206\u7c7b\u5668\u751f\u6210\u591a\u6837\u5316\u77e5\u8bc6\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3\u6559\u5e08\u7f51\u7edc\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u6559\u5e08\u7f51\u7edc\uff0c\u800cLSSKD\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u573a\u666f\u3002", "method": "\u5728\u4e2d\u95f4\u7279\u5f81\u56fe\u4e0a\u6dfb\u52a0\u8f85\u52a9\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u4e00\u5bf9\u4e00\u77e5\u8bc6\u8fc1\u79fb\uff0c\u65e0\u9700\u6559\u5e08\u7f51\u7edc\u3002", "result": "\u5728CIFAR-100\u4e0a\u5e73\u5747\u63d0\u53474.54%\uff0cImageNet\u4e0a\u63d0\u53470.32%\uff0c\u5e76\u5728\u5c0f\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "LSSKD\u9ad8\u6548\u8f7b\u91cf\uff0c\u9002\u5408\u4f4e\u8ba1\u7b97\u8bbe\u5907\u90e8\u7f72\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u591a\u6a21\u6001\u611f\u77e5\u548c\u5f31\u76d1\u7763\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2506.07056", "pdf": "https://arxiv.org/pdf/2506.07056", "abs": "https://arxiv.org/abs/2506.07056", "authors": ["Zhenyu Liu", "Huizhi Liang", "Rajiv Ranjan", "Zhanxing Zhu", "Vaclav Snasel", "Varun Ojha"], "title": "D2R: dual regularization loss with collaborative adversarial generation for model robustness", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The robustness of Deep Neural Network models is crucial for defending models\nagainst adversarial attacks. Recent defense methods have employed collaborative\nlearning frameworks to enhance model robustness. Two key limitations of\nexisting methods are (i) insufficient guidance of the target model via loss\nfunctions and (ii) non-collaborative adversarial generation. We, therefore,\npropose a dual regularization loss (D2R Loss) method and a collaborative\nadversarial generation (CAG) strategy for adversarial training. D2R loss\nincludes two optimization steps. The adversarial distribution and clean\ndistribution optimizations enhance the target model's robustness by leveraging\nthe strengths of different loss functions obtained via a suitable function\nspace exploration to focus more precisely on the target model's distribution.\nCAG generates adversarial samples using a gradient-based collaboration between\nguidance and target models. We conducted extensive experiments on three\nbenchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two\npopular target models, WideResNet34-10 and PreActResNet18. Our results show\nthat D2R loss with CAG produces highly robust models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6b63\u5219\u5316\u635f\u5931\uff08D2R Loss\uff09\u548c\u534f\u4f5c\u5bf9\u6297\u751f\u6210\uff08CAG\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u589e\u5f3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u76ee\u6807\u6a21\u578b\u635f\u5931\u51fd\u6570\u5f15\u5bfc\u4e0d\u8db3\u548c\u5bf9\u6297\u751f\u6210\u975e\u534f\u4f5c\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u91c7\u7528D2R Loss\uff08\u5305\u542b\u5bf9\u6297\u5206\u5e03\u548c\u5e72\u51c0\u5206\u5e03\u4f18\u5316\uff09\u548cCAG\uff08\u57fa\u4e8e\u68af\u5ea6\u7684\u534f\u4f5c\u5bf9\u6297\u751f\u6210\uff09\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001Tiny ImageNet\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cD2R Loss\u4e0eCAG\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "D2R Loss\u548cCAG\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.07080", "pdf": "https://arxiv.org/pdf/2506.07080", "abs": "https://arxiv.org/abs/2506.07080", "authors": ["Anatol Garioud", "S\u00e9bastien Giordano", "Nicolas David", "Nicolas Gonthier"], "title": "FLAIR-HUB: Large-scale Multimodal Dataset for Land Cover and Crop Mapping", "categories": ["cs.CV"], "comment": null, "summary": "The growing availability of high-quality Earth Observation (EO) data enables\naccurate global land cover and crop type monitoring. However, the volume and\nheterogeneity of these datasets pose major processing and annotation\nchallenges. To address this, the French National Institute of Geographical and\nForest Information (IGN) is actively exploring innovative strategies to exploit\ndiverse EO data, which require large annotated datasets. IGN introduces\nFLAIR-HUB, the largest multi-sensor land cover dataset with\nvery-high-resolution (20 cm) annotations, covering 2528 km2 of France. It\ncombines six aligned modalities: aerial imagery, Sentinel-1/2 time series, SPOT\nimagery, topographic data, and historical aerial images. Extensive benchmarks\nevaluate multimodal fusion and deep learning models (CNNs, transformers) for\nland cover or crop mapping and also explore multi-task learning. Results\nunderscore the complexity of multimodal fusion and fine-grained classification,\nwith best land cover performance (78.2% accuracy, 65.8% mIoU) achieved using\nnearly all modalities. FLAIR-HUB supports supervised and multimodal\npretraining, with data and code available at\nhttps://ignf.github.io/FLAIR/flairhub.", "AI": {"tldr": "FLAIR-HUB\u662f\u6cd5\u56fdIGN\u63a8\u51fa\u7684\u591a\u4f20\u611f\u5668\u571f\u5730\u8986\u76d6\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u516d\u79cd\u6570\u636e\u6a21\u6001\uff0c\u7528\u4e8e\u571f\u5730\u8986\u76d6\u548c\u4f5c\u7269\u5206\u7c7b\u7814\u7a76\uff0c\u652f\u6301\u76d1\u7763\u548c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u5730\u7403\u89c2\u6d4b\u6570\u636e\u7684\u5904\u7406\u548c\u6807\u6ce8\u6311\u6218\uff0c\u63a8\u52a8\u5168\u7403\u571f\u5730\u8986\u76d6\u548c\u4f5c\u7269\u7c7b\u578b\u76d1\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u516d\u79cd\u5bf9\u9f50\u7684\u6570\u636e\u6a21\u6001\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u878d\u5408\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982CNN\u548cTransformer\uff09\uff0c\u5e76\u63a2\u7d22\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "result": "\u6700\u4f73\u571f\u5730\u8986\u76d6\u5206\u7c7b\u6027\u80fd\u4e3a78.2%\u51c6\u786e\u7387\u548c65.8% mIoU\uff0c\u51e0\u4e4e\u4f7f\u7528\u4e86\u6240\u6709\u6a21\u6001\u3002", "conclusion": "FLAIR-HUB\u4e3a\u571f\u5730\u8986\u76d6\u548c\u4f5c\u7269\u5206\u7c7b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6570\u636e\u652f\u6301\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.07087", "pdf": "https://arxiv.org/pdf/2506.07087", "abs": "https://arxiv.org/abs/2506.07087", "authors": ["Weiqi Yan", "Lvhai Chen", "Huaijia Kou", "Shengchuan Zhang", "Yan Zhang", "Liujuan Cao"], "title": "UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Hightlight)", "summary": "Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it\ndoesn't need to rely on extensive pixel-level labels. Existing UCOD methods\ntypically generate pseudo-labels using fixed strategies and train 1 x1\nconvolutional layers as a simple decoder, leading to low performance compared\nto fully-supervised methods. We emphasize two drawbacks in these approaches:\n1). The model is prone to fitting incorrect knowledge due to the pseudo-label\ncontaining substantial noise. 2). The simple decoder fails to capture and learn\nthe semantic features of camouflaged objects, especially for small-sized\nobjects, due to the low-resolution pseudo-labels and severe confusion between\nforeground and background pixels. To this end, we propose a UCOD method with a\nteacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL,\nwhich contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial\n(DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines\npseudo-labels generated by fixed strategies and the teacher model to prevent\nthe model from overfitting incorrect knowledge while preserving the ability for\nself-correction; the DBA decoder takes adversarial learning of different\nsegmentation objectives, guides the model to overcome the foreground-background\nconfusion of camouflaged objects, and the Look-Twice mechanism mimics the human\ntendency to zoom in on camouflaged objects and performs secondary refinement on\nsmall-sized objects. Extensive experiments show that our method demonstrates\noutstanding performance, even surpassing some existing fully supervised\nmethods. The code is available now.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u4f2a\u6807\u7b7e\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08UCOD-DPL\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4f2a\u6807\u7b7e\u6a21\u5757\u3001\u53cc\u5206\u652f\u5bf9\u6297\u89e3\u7801\u5668\u548cLook-Twice\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u56e0\u4f2a\u6807\u7b7e\u566a\u58f0\u548c\u7b80\u5355\u89e3\u7801\u5668\u5bfc\u81f4\u6027\u80fd\u4f4e\u4e0b\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faUCOD-DPL\u65b9\u6cd5\uff0c\u5305\u542b\u81ea\u9002\u5e94\u4f2a\u6807\u7b7e\u6a21\u5757\uff08APM\uff09\u3001\u53cc\u5206\u652f\u5bf9\u6297\u89e3\u7801\u5668\uff08DBA\uff09\u548cLook-Twice\u673a\u5236\uff0c\u4ee5\u52a8\u6001\u4f18\u5316\u4f2a\u6807\u7b7e\u5e76\u63d0\u5347\u7279\u5f81\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8fc7\u90e8\u5206\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "UCOD-DPL\u901a\u8fc7\u52a8\u6001\u4f2a\u6807\u7b7e\u5b66\u4e60\u548c\u591a\u673a\u5236\u534f\u540c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.07091", "pdf": "https://arxiv.org/pdf/2506.07091", "abs": "https://arxiv.org/abs/2506.07091", "authors": ["Yangkai Lin", "Jiabao Lei", "Kui Jia"], "title": "SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model", "categories": ["cs.CV"], "comment": null, "summary": "Our project page: https://scutyklin.github.io/SceneLCM/. Automated generation\nof complex, interactive indoor scenes tailored to user prompt remains a\nformidable challenge. While existing methods achieve indoor scene synthesis,\nthey struggle with rigid editing constraints, physical incoherence, excessive\nhuman effort, single-room limitations, and suboptimal material quality. To\naddress these limitations, we propose SceneLCM, an end-to-end framework that\nsynergizes Large Language Model (LLM) for layout design with Latent Consistency\nModel(LCM) for scene optimization. Our approach decomposes scene generation\ninto four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D\nspatial reasoning to convert textual descriptions into parametric blueprints(3D\nlayout). And an iterative programmatic validation mechanism iteratively refines\nlayout parameters through LLM-mediated dialogue loops; (2) Furniture\nGeneration. SceneLCM employs Consistency Trajectory Sampling(CTS), a\nconsistency distillation sampling loss guided by LCM, to form fast,\nsemantically rich, and high-quality representations. We also offer two\ntheoretical justification to demonstrate that our CTS loss is equivalent to\nconsistency loss and its distillation error is bounded by the truncation error\nof the Euler solver; (3) Environment Optimization. We use a multiresolution\ntexture field to encode the appearance of the scene, and optimize via CTS loss.\nTo maintain cross-geometric texture coherence, we introduce a normal-aware\ncross-attention decoder to predict RGB by cross-attending to the anchors\nlocations in geometrically heterogeneous instance. (4)Physically Editing.\nSceneLCM supports physically editing by integrating physical simulation,\nachieved persistent physical realism. Extensive experiments validate SceneLCM's\nsuperiority over state-of-the-art techniques, showing its wide-ranging\npotential for diverse applications.", "AI": {"tldr": "SceneLCM\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7ed3\u5408LLM\u8fdb\u884c\u5e03\u5c40\u8bbe\u8ba1\u548cLCM\u8fdb\u884c\u573a\u666f\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5ba4\u5185\u573a\u666f\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5ba4\u5185\u573a\u666f\u5408\u6210\u4e2d\u5b58\u5728\u7f16\u8f91\u7ea6\u675f\u3001\u7269\u7406\u4e0d\u4e00\u81f4\u3001\u4eba\u529b\u9700\u6c42\u9ad8\u3001\u5355\u623f\u95f4\u9650\u5236\u548c\u6750\u8d28\u8d28\u91cf\u5dee\u7b49\u95ee\u9898\u3002", "method": "SceneLCM\u5c06\u573a\u666f\u751f\u6210\u5206\u89e3\u4e3a\u56db\u4e2a\u6a21\u5757\uff1a\u5e03\u5c40\u751f\u6210\uff08LLM\u6307\u5bfc\uff09\u3001\u5bb6\u5177\u751f\u6210\uff08LCM\u4f18\u5316\uff09\u3001\u73af\u5883\u4f18\u5316\uff08\u591a\u5206\u8fa8\u7387\u7eb9\u7406\uff09\u548c\u7269\u7406\u7f16\u8f91\uff08\u7269\u7406\u6a21\u62df\uff09\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1SceneLCM\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "SceneLCM\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ba4\u5185\u573a\u666f\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2506.07112", "pdf": "https://arxiv.org/pdf/2506.07112", "abs": "https://arxiv.org/abs/2506.07112", "authors": ["Changhong Fu", "Hua Lin", "Haobo Zuo", "Liangliang Yao", "Liguo Zhang"], "title": "EdgeSpotter: Multi-Scale Dense Text Spotting for Industrial Panel Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Text spotting for industrial panels is a key task for intelligent monitoring.\nHowever, achieving efficient and accurate text spotting for complex industrial\npanels remains challenging due to issues such as cross-scale localization and\nambiguous boundaries in dense text regions. Moreover, most existing methods\nprimarily focus on representing a single text shape, neglecting a comprehensive\nexploration of multi-scale feature information across different texts. To\naddress these issues, this work proposes a novel multi-scale dense text spotter\nfor edge AI-based vision system (EdgeSpotter) to achieve accurate and robust\nindustrial panel monitoring. Specifically, a novel Transformer with efficient\nmixer is developed to learn the interdependencies among multi-level features,\nintegrating multi-layer spatial and semantic cues. In addition, a new feature\nsampling with catmull-rom splines is designed, which explicitly encodes the\nshape, position, and semantic information of text, thereby alleviating missed\ndetections and reducing recognition errors caused by multi-scale or dense text\nregions. Furthermore, a new benchmark dataset for industrial panel monitoring\n(IPM) is constructed. Extensive qualitative and quantitative evaluations on\nthis challenging benchmark dataset validate the superior performance of the\nproposed method in different challenging panel monitoring tasks. Finally,\npractical tests based on the self-designed edge AI-based vision system\ndemonstrate the practicality of the method. The code and demo will be available\nat https://github.com/vision4robotics/EdgeSpotter.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEdgeSpotter\u7684\u591a\u5c3a\u5ea6\u5bc6\u96c6\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5de5\u4e1a\u9762\u677f\u76d1\u63a7\uff0c\u89e3\u51b3\u4e86\u8de8\u5c3a\u5ea6\u5b9a\u4f4d\u548c\u5bc6\u96c6\u6587\u672c\u533a\u57df\u6a21\u7cca\u8fb9\u754c\u7684\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u9762\u677f\u6587\u672c\u68c0\u6d4b\u5b58\u5728\u8de8\u5c3a\u5ea6\u5b9a\u4f4d\u548c\u5bc6\u96c6\u6587\u672c\u533a\u57df\u6a21\u7cca\u8fb9\u754c\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u4e00\u6587\u672c\u5f62\u72b6\uff0c\u7f3a\u4e4f\u591a\u5c3a\u5ea6\u7279\u5f81\u63a2\u7d22\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eTransformer\u7684\u9ad8\u6548\u6df7\u5408\u5668\uff0c\u5b66\u4e60\u591a\u7ea7\u7279\u5f81\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff1b\u8bbe\u8ba1\u4e86\u65b0\u7684\u7279\u5f81\u91c7\u6837\u65b9\u6cd5\uff0c\u4f7f\u7528Catmull-Rom\u6837\u6761\u7f16\u7801\u6587\u672c\u5f62\u72b6\u3001\u4f4d\u7f6e\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728\u81ea\u5efa\u7684\u5de5\u4e1a\u9762\u677f\u76d1\u63a7\u6570\u636e\u96c6\uff08IPM\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5728\u8fb9\u7f18AI\u89c6\u89c9\u7cfb\u7edf\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "EdgeSpotter\u5728\u5de5\u4e1a\u9762\u677f\u76d1\u63a7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u591a\u5c3a\u5ea6\u548c\u5bc6\u96c6\u6587\u672c\u533a\u57df\u7684\u68c0\u6d4b\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.07122", "pdf": "https://arxiv.org/pdf/2506.07122", "abs": "https://arxiv.org/abs/2506.07122", "authors": ["Prakriti Tripathi", "Theertha Biju", "Maniram Thota", "Rakesh Lingam"], "title": "Image segmentation and classification of E-waste for waste segregation", "categories": ["cs.CV", "cs.AI", "I.2.10"], "comment": "4 pages, 7 figures. For code and link to dataset, see\n  https://github.com/prakriti16/Image-segmentation-and-classification-of-e-waste", "summary": "Industry partners provided a problem statement that involves classifying\nelectronic waste using machine learning models that will be used by\npick-and-place robots for waste segregation. We started by taking common\nelectronic waste items, such as a mouse and charger, unsoldering them, and\ntaking pictures to create a custom dataset. Then state-of-the art YOLOv11 model\nwas trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also\ntrained and achieved 41 mAP. The model will be further integrated with\npick-and-place robots to perform segregation of e-waste.", "AI": {"tldr": "\u4f7f\u7528YOLOv11\u548cMask-RCNN\u6a21\u578b\u5bf9\u7535\u5b50\u5e9f\u7269\u8fdb\u884c\u5206\u7c7b\uff0c\u5206\u522b\u8fbe\u523070\u548c41 mAP\uff0c\u672a\u6765\u5c06\u96c6\u6210\u5230\u5206\u62e3\u673a\u5668\u4eba\u4e2d\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u5e9f\u7269\u5206\u7c7b\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5206\u62e3\u673a\u5668\u4eba\u5b9e\u73b0\u81ea\u52a8\u5316\u5e9f\u7269\u5206\u79bb\u3002", "method": "\u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u8bad\u7ec3YOLOv11\u548cMask-RCNN\u6a21\u578b\u3002", "result": "YOLOv11\u8fbe\u523070 mAP\uff0cMask-RCNN\u8fbe\u523041 mAP\u3002", "conclusion": "\u6a21\u578b\u5c06\u96c6\u6210\u5230\u5206\u62e3\u673a\u5668\u4eba\u4e2d\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u7535\u5b50\u5e9f\u7269\u5206\u7c7b\u3002"}}
{"id": "2506.07136", "pdf": "https://arxiv.org/pdf/2506.07136", "abs": "https://arxiv.org/abs/2506.07136", "authors": ["Huaize Liu", "Wenzhang Sun", "Qiyuan Zhang", "Donglin Di", "Biao Gong", "Hao Li", "Chen Wei", "Changqing Zou"], "title": "Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion", "categories": ["cs.CV"], "comment": null, "summary": "Recent breakthroughs in video autoencoders (Video AEs) have advanced video\ngeneration, but existing methods fail to efficiently model spatio-temporal\nredundancies in dynamics, resulting in suboptimal compression factors. This\nshortfall leads to excessive training costs for downstream tasks. To address\nthis, we introduce Hi-VAE, an efficient video autoencoding framework that\nhierarchically encode coarse-to-fine motion representations of video dynamics\nand formulate the decoding process as a conditional generation task.\nSpecifically, Hi-VAE decomposes video dynamics into two latent spaces: Global\nMotion, capturing overarching motion patterns, and Detailed Motion, encoding\nhigh-frequency spatial details. Using separate self-supervised motion encoders,\nwe compress video latents into compact motion representations to reduce\nredundancy significantly. A conditional diffusion decoder then reconstructs\nvideos by combining hierarchical global and detailed motions, enabling\nhigh-fidelity video reconstructions. Extensive experiments demonstrate that\nHi-VAE achieves a high compression factor of 1428$\\times$, almost 30$\\times$\nhigher than baseline methods (e.g., Cosmos-VAE at 48$\\times$), validating the\nefficiency of our approach. Meanwhile, Hi-VAE maintains high reconstruction\nquality at such high compression rates and performs effectively in downstream\ngenerative tasks. Moreover, Hi-VAE exhibits interpretability and scalability,\nproviding new perspectives for future exploration in video latent\nrepresentation and generation.", "AI": {"tldr": "Hi-VAE\u662f\u4e00\u79cd\u9ad8\u6548\u89c6\u9891\u81ea\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u89c6\u9891\u52a8\u6001\u7684\u7c97\u5230\u7ec6\u8fd0\u52a8\u8868\u793a\uff0c\u663e\u8457\u51cf\u5c11\u65f6\u7a7a\u5197\u4f59\uff0c\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u81ea\u7f16\u7801\u65b9\u6cd5\u672a\u80fd\u9ad8\u6548\u5efa\u6a21\u52a8\u6001\u4e2d\u7684\u65f6\u7a7a\u5197\u4f59\uff0c\u5bfc\u81f4\u538b\u7f29\u6548\u679c\u4e0d\u4f73\u548c\u4e0b\u6e38\u4efb\u52a1\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\u3002", "method": "Hi-VAE\u5c06\u89c6\u9891\u52a8\u6001\u5206\u89e3\u4e3a\u5168\u5c40\u8fd0\u52a8\u548c\u7ec6\u8282\u8fd0\u52a8\u4e24\u4e2a\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f7f\u7528\u81ea\u76d1\u7763\u8fd0\u52a8\u7f16\u7801\u5668\u538b\u7f29\u89c6\u9891\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u6269\u6563\u89e3\u7801\u5668\u91cd\u5efa\u89c6\u9891\u3002", "result": "Hi-VAE\u5b9e\u73b0\u4e861428\u500d\u7684\u9ad8\u538b\u7f29\u6bd4\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982Cosmos-VAE\u768448\u500d\uff09\u9ad8\u51fa\u8fd130\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "Hi-VAE\u5728\u9ad8\u6548\u538b\u7f29\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u89c6\u9891\u6f5c\u5728\u8868\u793a\u548c\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.07138", "pdf": "https://arxiv.org/pdf/2506.07138", "abs": "https://arxiv.org/abs/2506.07138", "authors": ["Hao Tang", "Chengchao Shen"], "title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "The source code and trained weights are available at\n  https://github.com/visresearch/LLaVA-STF", "summary": "Large multimodal models (LMMs) suffer significant computational challenges\ndue to the high cost of Large Language Models (LLMs) and the quadratic\ncomplexity of processing long vision token sequences. In this paper, we explore\nthe spatial redundancy among vision tokens and shorten the length of vision\ntoken sequences for inference acceleration. Specifically, we propose a Spatial\nToken Fusion (STF) method to learn compact vision tokens for short vision token\nsequence, where spatial-adjacent tokens are fused into one. Meanwhile,\nweight-frozen vision encoder can not well adapt to the demand of extensive\ndownstream vision-language tasks. To this end, we further introduce a\nMulti-Block Token Fusion (MBTF) module to supplement multi-granularity features\nfor the reduced token sequence. Overall, we combine STF and MBTF module to\nbalance token reduction and information preservation, thereby improving\ninference efficiency without sacrificing multimodal reasoning capabilities.\nExperimental results demonstrate that our method based on LLaVA-1.5 achieves\ncomparable or even superior performance to the baseline on 8 popular\nvision-language benchmarks with only $25\\%$ vision tokens of baseline. The\nsource code and trained weights are available at\nhttps://github.com/visresearch/LLaVA-STF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u4ee4\u724c\u878d\u5408\uff08STF\uff09\u548c\u591a\u5757\u4ee4\u724c\u878d\u5408\uff08MBTF\uff09\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u89c6\u89c9\u4ee4\u724c\u5e8f\u5217\u957f\u5ea6\u5e76\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u56e0\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u957f\u89c6\u89c9\u4ee4\u724c\u5e8f\u5217\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7STF\u878d\u5408\u7a7a\u95f4\u76f8\u90bb\u4ee4\u724c\u4ee5\u51cf\u5c11\u5e8f\u5217\u957f\u5ea6\uff0c\u5e76\u901a\u8fc7MBTF\u8865\u5145\u591a\u7c92\u5ea6\u7279\u5f81\u3002", "result": "\u57288\u4e2a\u6d41\u884c\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u7528\u57fa\u7ebf25%\u7684\u89c6\u89c9\u4ee4\u724c\u5373\u53ef\u8fbe\u5230\u53ef\u6bd4\u6216\u66f4\u4f18\u6027\u80fd\u3002", "conclusion": "STF\u548cMBTF\u6a21\u5757\u6709\u6548\u5e73\u8861\u4e86\u4ee4\u724c\u51cf\u5c11\u548c\u4fe1\u606f\u4fdd\u7559\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u4e14\u4e0d\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2506.07155", "pdf": "https://arxiv.org/pdf/2506.07155", "abs": "https://arxiv.org/abs/2506.07155", "authors": ["Van Nguyen Nguyen", "Christian Forster", "Sindi Shkodrani", "Vincent Lepetit", "Bugra Tekin", "Cem Keskin", "Tomas Hodan"], "title": "GoTrack: Generic 6DoF Object Pose Refinement and Tracking", "categories": ["cs.CV"], "comment": null, "summary": "We introduce GoTrack, an efficient and accurate CAD-based method for 6DoF\nobject pose refinement and tracking, which can handle diverse objects without\nany object-specific training. Unlike existing tracking methods that rely solely\non an analysis-by-synthesis approach for model-to-frame registration, GoTrack\nadditionally integrates frame-to-frame registration, which saves compute and\nstabilizes tracking. Both types of registration are realized by optical flow\nestimation. The model-to-frame registration is noticeably simpler than in\nexisting methods, relying only on standard neural network blocks (a transformer\nis trained on top of DINOv2) and producing reliable pose confidence scores\nwithout a scoring network. For the frame-to-frame registration, which is an\neasier problem as consecutive video frames are typically nearly identical, we\nemploy a light off-the-shelf optical flow model. We demonstrate that GoTrack\ncan be seamlessly combined with existing coarse pose estimation methods to\ncreate a minimal pipeline that reaches state-of-the-art RGB-only results on\nstandard benchmarks for 6DoF object pose estimation and tracking. Our source\ncode and trained models are publicly available at\nhttps://github.com/facebookresearch/gotrack", "AI": {"tldr": "GoTrack\u662f\u4e00\u79cd\u57fa\u4e8eCAD\u7684\u9ad8\u65486DoF\u7269\u4f53\u59ff\u6001\u4f18\u5316\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u65e0\u9700\u7279\u5b9a\u7269\u4f53\u8bad\u7ec3\uff0c\u7ed3\u5408\u6a21\u578b\u5230\u5e27\u548c\u5e27\u5230\u5e27\u6ce8\u518c\uff0c\u901a\u8fc7\u5149\u6d41\u4f30\u8ba1\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u8ddf\u8e2a\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6a21\u578b\u5230\u5e27\u6ce8\u518c\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0cGoTrack\u901a\u8fc7\u7ed3\u5408\u5e27\u5230\u5e27\u6ce8\u518c\u4f18\u5316\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u5757\uff08\u57fa\u4e8eDINOv2\u7684Transformer\uff09\u5b9e\u73b0\u6a21\u578b\u5230\u5e27\u6ce8\u518c\uff0c\u8f7b\u91cf\u7ea7\u73b0\u6210\u5149\u6d41\u6a21\u578b\u5904\u7406\u5e27\u5230\u5e27\u6ce8\u518c\u3002", "result": "GoTrack\u4e0e\u73b0\u6709\u7c97\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u7ed3\u5408\uff0c\u5728\u6807\u51c66DoF\u59ff\u6001\u4f30\u8ba1\u4e0e\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u8fbe\u5230RGB-only\u7684SOTA\u7ed3\u679c\u3002", "conclusion": "GoTrack\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u76846DoF\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.07164", "pdf": "https://arxiv.org/pdf/2506.07164", "abs": "https://arxiv.org/abs/2506.07164", "authors": ["Qiong Chang", "Xinyuan Chen", "Xiang Li", "Weimin Wang", "Jun Miyazaki"], "title": "Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs", "categories": ["cs.CV"], "comment": null, "summary": "The visual-based SLAM (Simultaneous Localization and Mapping) is a technology\nwidely used in applications such as robotic navigation and virtual reality,\nwhich primarily focuses on detecting feature points from visual images to\nconstruct an unknown environmental map and simultaneously determines its own\nlocation. It usually imposes stringent requirements on hardware power\nconsumption, processing speed and accuracy. Currently, the ORB (Oriented FAST\nand Rotated BRIEF)-based SLAM systems have exhibited superior performance in\nterms of processing speed and robustness. However, they still fall short of\nmeeting the demands for real-time processing on mobile platforms. This\nlimitation is primarily due to the time-consuming Oriented FAST calculations\naccounting for approximately half of the entire SLAM system. This paper\npresents two methods to accelerate the Oriented FAST feature detection on\nlow-end embedded GPUs. These methods optimize the most time-consuming steps in\nOriented FAST feature detection: FAST feature point detection and Harris corner\ndetection, which is achieved by implementing a binary-level encoding strategy\nto determine candidate points quickly and a separable Harris detection strategy\nwith efficient low-level GPU hardware-specific instructions. Extensive\nexperiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over\n7.3 times compared to widely used OpenCV with GPU support. This significant\nimprovement highlights its effectiveness and potential for real-time\napplications in mobile and resource-constrained environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\u52a0\u901f\u4f4e\u7aef\u5d4c\u5165\u5f0fGPU\u4e0a\u7684Oriented FAST\u7279\u5f81\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347SLAM\u7cfb\u7edf\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709ORB-based SLAM\u7cfb\u7edf\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u5b9e\u65f6\u6027\u4e0d\u8db3\uff0c\u4e3b\u8981\u56e0Oriented FAST\u8ba1\u7b97\u8017\u65f6\u3002", "method": "\u91c7\u7528\u4e8c\u8fdb\u5236\u7f16\u7801\u7b56\u7565\u5feb\u901f\u786e\u5b9a\u5019\u9009\u70b9\uff0c\u4ee5\u53ca\u53ef\u5206\u79bb\u7684Harris\u68c0\u6d4b\u7b56\u7565\u7ed3\u5408GPU\u786c\u4ef6\u6307\u4ee4\u4f18\u5316\u3002", "result": "\u5728Jetson TX2\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u901f\u5ea6\u6bd4OpenCV GPU\u7248\u672c\u5e73\u5747\u63d0\u53477.3\u500d\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u5b9e\u65f6\u6027\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2506.07177", "pdf": "https://arxiv.org/pdf/2506.07177", "abs": "https://arxiv.org/abs/2506.07177", "authors": ["Sangwon Jang", "Taekyung Ki", "Jaehyeong Jo", "Jaehong Yoon", "Soo Ye Kim", "Zhe Lin", "Sung Ju Hwang"], "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://frame-guidance-video.github.io/", "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5e27\u7ea7\u4fe1\u53f7\u5f15\u5bfc\u65b9\u6cd5\uff08Frame Guidance\uff09\uff0c\u7528\u4e8e\u53ef\u63a7\u89c6\u9891\u751f\u6210\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u5347\u5168\u5c40\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5fae\u8c03\u5927\u89c4\u6a21\u89c6\u9891\u6a21\u578b\uff0c\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u957f\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e27\u7ea7\u4fe1\u53f7\uff08\u5982\u5173\u952e\u5e27\u3001\u98ce\u683c\u53c2\u8003\u56fe\u7b49\uff09\u7684Frame Guidance\uff0c\u7ed3\u5408\u6f5c\u5728\u5904\u7406\u65b9\u6cd5\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u6027\u89c6\u9891\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFrame Guidance\u80fd\u5728\u591a\u79cd\u4efb\u52a1\uff08\u5982\u5173\u952e\u5e27\u5f15\u5bfc\u3001\u98ce\u683c\u5316\u7b49\uff09\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u63a7\u89c6\u9891\u3002", "conclusion": "Frame Guidance\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u517c\u5bb9\u6027\u5f3a\u7684\u9ad8\u6548\u53ef\u63a7\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2506.07188", "pdf": "https://arxiv.org/pdf/2506.07188", "abs": "https://arxiv.org/abs/2506.07188", "authors": ["Ni Ding", "Lei He", "Shengbo Eben Li", "Keqiang Li"], "title": "Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks", "categories": ["cs.CV", "I.2.10"], "comment": "13 pages, 7 figures,", "summary": "End-to-end autonomous driving has emerged as a dominant paradigm, yet its\nhighly entangled black-box models pose significant challenges in terms of\ninterpretability and safety assurance. To improve model transparency and\ntraining flexibility, this paper proposes a hierarchical and decoupled\npost-training framework tailored for pretrained neural networks. By\nreconstructing intermediate feature maps from ground-truth labels, surrogate\nsupervisory signals are introduced at transitional layers to enable independent\ntraining of specific components, thereby avoiding the complexity and coupling\nof conventional end-to-end backpropagation and providing interpretable insights\ninto networks' internal mechanisms. To the best of our knowledge, this is the\nfirst method to formalize feature-level reverse computation as well-posed\noptimization problems, which we rigorously reformulate as systems of linear\nequations or least squares problems. This establishes a novel and efficient\ntraining paradigm that extends gradient backpropagation to feature\nbackpropagation. Extensive experiments on multiple standard image\nclassification benchmarks demonstrate that the proposed method achieves\nsuperior generalization performance and computational efficiency compared to\ntraditional training approaches, validating its effectiveness and potential.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u89e3\u8026\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u6784\u4e2d\u95f4\u7279\u5f81\u56fe\u5f15\u5165\u76d1\u7763\u4fe1\u53f7\uff0c\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\u548c\u8bad\u7ec3\u7075\u6d3b\u6027\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u9ed1\u76d2\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u8bad\u7ec3\u7075\u6d3b\u6027\u3002", "method": "\u5229\u7528\u771f\u5b9e\u6807\u7b7e\u91cd\u6784\u4e2d\u95f4\u7279\u5f81\u56fe\uff0c\u5f15\u5165\u4ee3\u7406\u76d1\u7763\u4fe1\u53f7\uff0c\u72ec\u7acb\u8bad\u7ec3\u7ec4\u4ef6\uff0c\u907f\u514d\u4f20\u7edf\u7aef\u5230\u7aef\u53cd\u5411\u4f20\u64ad\u7684\u590d\u6742\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u8303\u5f0f\uff0c\u5177\u6709\u663e\u8457\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.07196", "pdf": "https://arxiv.org/pdf/2506.07196", "abs": "https://arxiv.org/abs/2506.07196", "authors": ["Mengya Xu", "Zhongzhen Huang", "Dillan Imans", "Yiru Ye", "Xiaofan Zhang", "Qi Dou"], "title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 4 figures", "summary": "Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SAP-Bench\u6570\u636e\u96c6\u548cMLLM-SAP\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u624b\u672f\u52a8\u4f5c\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u4e0b\u4e00\u52a8\u4f5c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u624b\u672f\u51b3\u7b56\u4e2d\u7684\u590d\u6742\u80fd\u529b\uff0c\u9700\u5f00\u53d1\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4ee5\u63a8\u52a8MLLM\u7814\u7a76\u3002", "method": "\u6784\u5efaSAP-Bench\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e34\u5e8a\u9a8c\u8bc1\u7684\u624b\u672f\u52a8\u4f5c\u6807\u6ce8\uff0c\u5e76\u63d0\u51faMLLM-SAP\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u751f\u6210\u4e0b\u4e00\u52a8\u4f5c\u5efa\u8bae\u3002", "result": "\u8bc4\u4f30\u4e86\u4e03\u79cd\u5148\u8fdbMLLM\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5728\u4e0b\u4e00\u52a8\u4f5c\u9884\u6d4b\u6027\u80fd\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "SAP-Bench\u548cMLLM-SAP\u4e3a\u624b\u672f\u52a8\u4f5c\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.07205", "pdf": "https://arxiv.org/pdf/2506.07205", "abs": "https://arxiv.org/abs/2506.07205", "authors": ["Min-Jung Kim", "Dongjin Kim", "Seokju Yun", "Jaegul Choo"], "title": "TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation", "categories": ["cs.CV"], "comment": null, "summary": "Video editing has garnered increasing attention alongside the rapid progress\nof diffusion-based video generation models. As part of these advancements,\nthere is a growing demand for more accessible and controllable forms of video\nediting, such as prompt-based editing. Previous studies have primarily focused\non tasks such as style transfer, background replacement, object substitution,\nand attribute modification, while maintaining the content structure of the\nsource video. However, more complex tasks, including the addition of novel\nobjects and nonrigid transformations, remain relatively unexplored. In this\npaper, we present TV-LiVE, a Training-free and text-guided Video editing\nframework via Layerinformed Vitality Exploitation. We empirically identify\nvital layers within the video generation model that significantly influence the\nquality of generated outputs. Notably, these layers are closely associated with\nRotary Position Embeddings (RoPE). Based on this observation, our method\nenables both object addition and non-rigid video editing by selectively\ninjecting key and value features from the source model into the corresponding\nlayers of the target model guided by the layer vitality. For object addition,\nwe further identify prominent layers to extract the mask regions corresponding\nto the newly added target prompt. We found that the extracted masks from the\nprominent layers faithfully indicate the region to be edited. Experimental\nresults demonstrate that TV-LiVE outperforms existing approaches for both\nobject addition and non-rigid video editing. Project Page:\nhttps://emjay73.github.io/TV_LiVE/", "AI": {"tldr": "TV-LiVE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u6587\u672c\u5f15\u5bfc\u7684\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5173\u952e\u5c42\u7684\u6d3b\u529b\u4fe1\u606f\u5b9e\u73b0\u590d\u6742\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\uff0c\u5982\u6dfb\u52a0\u65b0\u5bf9\u8c61\u548c\u975e\u521a\u6027\u53d8\u6362\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u98ce\u683c\u8f6c\u6362\u7b49\u7b80\u5355\u4efb\u52a1\uff0c\u590d\u6742\u4efb\u52a1\u5982\u6dfb\u52a0\u65b0\u5bf9\u8c61\u548c\u975e\u521a\u6027\u53d8\u6362\u7814\u7a76\u8f83\u5c11\uff0cTV-LiVE\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u4e0eRoPE\u76f8\u5173\u7684\u5173\u952e\u5c42\uff0c\u9009\u62e9\u6027\u6ce8\u5165\u6e90\u6a21\u578b\u7684\u7279\u5f81\u5230\u76ee\u6807\u6a21\u578b\uff0c\u5e76\u63d0\u53d6\u663e\u8457\u5c42\u751f\u6210\u7f16\u8f91\u533a\u57df\u7684\u63a9\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTV-LiVE\u5728\u6dfb\u52a0\u65b0\u5bf9\u8c61\u548c\u975e\u521a\u6027\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TV-LiVE\u4e3a\u590d\u6742\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07214", "pdf": "https://arxiv.org/pdf/2506.07214", "abs": "https://arxiv.org/abs/2506.07214", "authors": ["Zhiyuan Zhong", "Zhen Sun", "Yepang Liu", "Xinlei He", "Guanhong Tao"], "title": "Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Vision Language Models (VLMs) have shown remarkable performance, but are also\nvulnerable to backdoor attacks whereby the adversary can manipulate the model's\noutputs through hidden triggers. Prior attacks primarily rely on\nsingle-modality triggers, leaving the crucial cross-modal fusion nature of VLMs\nlargely unexplored. Unlike prior work, we identify a novel attack surface that\nleverages cross-modal semantic mismatches as implicit triggers. Based on this\ninsight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data\npoisoning attack that injects stealthy backdoors by deliberately misaligning\nimage-text pairs during training. To perform the attack, we construct SIMBad, a\ndataset tailored for semantic manipulation involving color and object\nattributes. Extensive experiments across four widely used VLMs show that BadSem\nachieves over 98% average ASR, generalizes well to out-of-distribution\ndatasets, and can transfer across poisoning modalities. Our detailed analysis\nusing attention visualization shows that backdoored models focus on\nsemantically sensitive regions under mismatched conditions while maintaining\nnormal behavior on clean inputs. To mitigate the attack, we try two defense\nstrategies based on system prompt and supervised fine-tuning but find that both\nof them fail to mitigate the semantic backdoor. Our findings highlight the\nurgent need to address semantic vulnerabilities in VLMs for their safer\ndeployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u8de8\u6a21\u6001\u540e\u95e8\u653b\u51fb\u65b9\u6cd5BadSem\uff0c\u5229\u7528\u8bed\u4e49\u4e0d\u5339\u914d\u4f5c\u4e3a\u9690\u5f0f\u89e6\u53d1\u5668\uff0c\u653b\u51fb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u9ad8\u653b\u51fb\u6210\u529f\u7387\u548c\u9632\u5fa1\u56f0\u96be\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u653b\u51fb\u4e3b\u8981\u4f9d\u8d56\u5355\u6a21\u6001\u89e6\u53d1\u5668\uff0c\u5ffd\u89c6\u4e86VLMs\u7684\u8de8\u6a21\u6001\u878d\u5408\u7279\u6027\uff0c\u56e0\u6b64\u63a2\u7d22\u8de8\u6a21\u6001\u8bed\u4e49\u4e0d\u5339\u914d\u4f5c\u4e3a\u653b\u51fb\u624b\u6bb5\u3002", "method": "\u63d0\u51faBadSem\u653b\u51fb\uff0c\u901a\u8fc7\u6545\u610f\u5728\u8bad\u7ec3\u65f6\u9519\u914d\u56fe\u50cf-\u6587\u672c\u5bf9\u6ce8\u5165\u540e\u95e8\uff0c\u5e76\u6784\u5efaSIMBad\u6570\u636e\u96c6\u652f\u6301\u653b\u51fb\u3002", "result": "\u5728\u56db\u79cdVLMs\u4e0a\u5b9e\u9a8c\uff0cBadSem\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc798%\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u5206\u5e03\u5916\u6570\u636e\u3002\u9632\u5fa1\u7b56\u7565\u5747\u672a\u80fd\u6709\u6548\u7f13\u89e3\u653b\u51fb\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86VLMs\u7684\u8bed\u4e49\u6f0f\u6d1e\uff0c\u547c\u5401\u66f4\u5b89\u5168\u7684\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2506.07216", "pdf": "https://arxiv.org/pdf/2506.07216", "abs": "https://arxiv.org/abs/2506.07216", "authors": ["Nada Aboudeshish", "Dmitry Ignatov", "Radu Timofte"], "title": "AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?", "categories": ["cs.CV"], "comment": null, "summary": "Data augmentation is a crucial technique in deep learning, particularly for\ntasks with limited dataset diversity, such as skeleton-based datasets. This\npaper proposes a comprehensive data augmentation framework that integrates\ngeometric transformations, random cropping, rotation, zooming and\nintensity-based transformations, brightness and contrast adjustments to\nsimulate real-world variations. Random cropping ensures the preservation of\nspatio-temporal integrity while addressing challenges such as viewpoint bias\nand occlusions. The augmentation pipeline generates three augmented versions\nfor each sample in addition to the data set sample, thus quadrupling the data\nset size and enriching the diversity of gesture representations. The proposed\naugmentation strategy is evaluated on three models: multi-stream e2eET, FPPR\npoint cloud-based hand gesture recognition (HGR), and DD-Network. Experiments\nare conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB.\nThe e2eET model, recognized as the state-of-the-art for hand gesture\nrecognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best\nperforming model on SHREC'17, excels in point cloud-based gesture recognition.\nDD-Net, a lightweight and efficient architecture for skeleton-based action\nrecognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB).\nThe results underline the effectiveness and versatility of the proposed\naugmentation strategy, significantly improving model generalization and\nrobustness across diverse datasets and architectures. This framework not only\nestablishes state-of-the-art results on all three evaluated models but also\noffers a scalable solution to advance HGR and action recognition applications\nin real-world scenarios. The framework is available at\nhttps://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u53d8\u6362\u3001\u968f\u673a\u88c1\u526a\u3001\u65cb\u8f6c\u3001\u7f29\u653e\u548c\u5f3a\u5ea6\u53d8\u6362\u7b49\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9aa8\u67b6\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9aa8\u67b6\u6570\u636e\u96c6\u4e2d\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "method": "\u96c6\u6210\u51e0\u4f55\u53d8\u6362\u3001\u968f\u673a\u88c1\u526a\u3001\u65cb\u8f6c\u3001\u7f29\u653e\u548c\u5f3a\u5ea6\u53d8\u6362\uff0c\u751f\u6210\u56db\u500d\u6570\u636e\u91cf\u7684\u589e\u5f3a\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8fd8\u4e3a\u624b\u52bf\u8bc6\u522b\u548c\u52a8\u4f5c\u8bc6\u522b\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07227", "pdf": "https://arxiv.org/pdf/2506.07227", "abs": "https://arxiv.org/abs/2506.07227", "authors": ["Tianyi Bai", "Yuxuan Fan", "Jiantao Qiu", "Fupeng Sun", "Jiayi Song", "Junlin Han", "Zichen Liu", "Conghui He", "Wentao Zhang", "Binhang Yuan"], "title": "Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks but still struggle with fine-grained visual differences,\nleading to hallucinations or missed semantic shifts. We attribute this to\nlimitations in both training data and learning objectives. To address these\nissues, we propose a controlled data generation pipeline that produces\nminimally edited image pairs with semantically aligned captions. Using this\npipeline, we construct the Micro Edit Dataset (MED), containing over 50K\nimage-text pairs spanning 11 fine-grained edit categories, including attribute,\ncount, position, and object presence changes. Building on MED, we introduce a\nsupervised fine-tuning (SFT) framework with a feature-level consistency loss\nthat promotes stable visual embeddings under small edits. We evaluate our\napproach on the Micro Edit Detection benchmark, which includes carefully\nbalanced evaluation pairs designed to test sensitivity to subtle visual\nvariations across the same edit categories. Our method improves difference\ndetection accuracy and reduces hallucinations compared to strong baselines,\nincluding GPT-4o. Moreover, it yields consistent gains on standard\nvision-language tasks such as image captioning and visual question answering.\nThese results demonstrate the effectiveness of combining targeted data and\nalignment objectives for enhancing fine-grained visual reasoning in MLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5dee\u5f02\u4efb\u52a1\u4e2d\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5fae\u7f16\u8f91\u6570\u636e\u96c6\uff08MED\uff09\u548c\u5f15\u5165\u7279\u5f81\u7ea7\u4e00\u81f4\u6027\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "MLLMs\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5dee\u5f02\u4e0a\u5b58\u5728\u5e7b\u89c9\u6216\u8bed\u4e49\u9057\u6f0f\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u548c\u76ee\u6807\u51fd\u6570\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u53ef\u63a7\u6570\u636e\u751f\u6210\u6d41\u7a0b\u6784\u5efaMED\u6570\u636e\u96c6\uff0850K\u56fe\u50cf-\u6587\u672c\u5bf9\uff09\uff0c\u5e76\u8bbe\u8ba1\u76d1\u7763\u5fae\u8c03\u6846\u67b6\uff0c\u5f15\u5165\u7279\u5f81\u7ea7\u4e00\u81f4\u6027\u635f\u5931\u4ee5\u7a33\u5b9a\u89c6\u89c9\u5d4c\u5165\u3002", "result": "\u5728Micro Edit Detection\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5dee\u5f02\u68c0\u6d4b\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u5728\u6807\u51c6\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\uff09\u4e2d\u8868\u73b0\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u6570\u636e\u548c\u76ee\u6807\u5bf9\u9f50\uff0c\u6709\u6548\u589e\u5f3a\u4e86MLLMs\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.07235", "pdf": "https://arxiv.org/pdf/2506.07235", "abs": "https://arxiv.org/abs/2506.07235", "authors": ["Tianyi Bai", "Zengjie Hu", "Fupeng Sun", "Jiantao Qiu", "Yizhen Jiang", "Guangxin He", "Bohan Zeng", "Conghui He", "Binhang Yuan", "Wentao Zhang"], "title": "Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal large language models (MLLMs) have achieved remarkable\ncapabilities by integrating visual perception with language understanding,\nenabling applications such as image-grounded dialogue, visual question\nanswering, and scientific analysis. However, most MLLMs adopt a static\ninference paradigm, encoding the entire image into fixed visual tokens upfront,\nwhich limits their ability to iteratively refine understanding or adapt to\ncontext during inference. This contrasts sharply with human perception, which\nis dynamic, selective, and feedback-driven. In this work, we introduce a novel\nframework for inference-time visual token scaling that enables MLLMs to perform\niterative, verifier-guided reasoning over visual content. We formulate the\nproblem as a Markov Decision Process, involving a reasoner that proposes visual\nactions and a verifier, which is trained via multi-step Direct Preference\nOptimization (DPO), that evaluates these actions and determines when reasoning\nshould terminate. To support this, we present a new dataset, VTS, comprising\nsupervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning\ncomparisons (VTS-DPO). Our method significantly outperforms existing approaches\nacross diverse visual reasoning benchmarks, offering not only improved accuracy\nbut also more interpretable and grounded reasoning processes. These results\ndemonstrate the promise of dynamic inference mechanisms for enabling\nfine-grained, context-aware visual reasoning in next-generation MLLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u6807\u8bb0\u7f29\u653e\u548c\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u8fed\u4ee3\u63a8\u7406\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MLLMs\u91c7\u7528\u9759\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u9650\u5236\u4e86\u5176\u8fed\u4ee3\u4f18\u5316\u548c\u4e0a\u4e0b\u6587\u9002\u5e94\u7684\u80fd\u529b\uff0c\u800c\u4eba\u7c7b\u611f\u77e5\u662f\u52a8\u6001\u4e14\u53cd\u9988\u9a71\u52a8\u7684\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7ed3\u5408\u63d0\u8bae\u89c6\u89c9\u52a8\u4f5c\u7684\u63a8\u7406\u5668\u548c\u901a\u8fc7\u591a\u6b65\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8bad\u7ec3\u7684\u9a8c\u8bc1\u5668\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u52a8\u6001\u63a8\u7406\u673a\u5236\u4e3a\u4e0b\u4e00\u4ee3MLLMs\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2506.07280", "pdf": "https://arxiv.org/pdf/2506.07280", "abs": "https://arxiv.org/abs/2506.07280", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 23 figures, 9 tables", "summary": "Video Diffusion Models (VDMs) have emerged as powerful generative tools,\ncapable of synthesizing high-quality spatiotemporal content. Yet, their\npotential goes far beyond mere video generation. We argue that the training\ndynamics of VDMs, driven by the need to model coherent sequences, naturally\npushes them to internalize structured representations and an implicit\nunderstanding of the visual world. To probe the extent of this internal\nknowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs\nfor new tasks using only a handful of examples. Our method transforms each task\ninto a visual transition, enabling the training of LoRA weights on short\ninput-output sequences without altering the generative interface of a frozen\nVDM. Despite minimal supervision, the model exhibits strong generalization\nacross diverse tasks, from low-level vision (for example, segmentation and pose\nestimation) to high-level reasoning (for example, on ARC-AGI). These results\nreframe VDMs as more than generative engines. They are adaptable visual\nlearners with the potential to serve as the backbone for future foundation\nmodels in vision.", "AI": {"tldr": "\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u4e0d\u4ec5\u662f\u5f3a\u5927\u7684\u89c6\u9891\u751f\u6210\u5de5\u5177\uff0c\u8fd8\u80fd\u901a\u8fc7\u8bad\u7ec3\u52a8\u6001\u5b66\u4e60\u7ed3\u6784\u5316\u8868\u793a\u548c\u89c6\u89c9\u4e16\u754c\u7684\u9690\u5f0f\u7406\u89e3\u3002\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u6846\u67b6\uff0cVDMs\u53ef\u9002\u5e94\u591a\u6837\u4efb\u52a1\uff0c\u5c55\u73b0\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22VDMs\u5728\u89c6\u9891\u751f\u6210\u4e4b\u5916\u7684\u6f5c\u529b\uff0c\u9a8c\u8bc1\u5176\u5185\u90e8\u662f\u5426\u5b66\u4e60\u5230\u7ed3\u6784\u5316\u8868\u793a\u548c\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u8f6c\u5316\u4e3a\u89c6\u89c9\u8fc7\u6e21\uff0c\u8bad\u7ec3LoRA\u6743\u91cd\u800c\u4e0d\u6539\u53d8\u51bb\u7ed3VDM\u7684\u751f\u6210\u63a5\u53e3\u3002", "result": "\u6a21\u578b\u5728\u4ece\u4f4e\u5c42\u89c6\u89c9\uff08\u5982\u5206\u5272\u3001\u59ff\u6001\u4f30\u8ba1\uff09\u5230\u9ad8\u5c42\u63a8\u7406\uff08\u5982ARC-AGI\uff09\u7684\u591a\u6837\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VDMs\u4e0d\u4ec5\u662f\u751f\u6210\u5f15\u64ce\uff0c\u8fd8\u662f\u9002\u5e94\u6027\u5f3a\u7684\u89c6\u89c9\u5b66\u4e60\u5668\uff0c\u6709\u671b\u6210\u4e3a\u672a\u6765\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6838\u5fc3\u3002"}}
{"id": "2506.07286", "pdf": "https://arxiv.org/pdf/2506.07286", "abs": "https://arxiv.org/abs/2506.07286", "authors": ["Aditya Chakravarty"], "title": "Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted in CVPR 2025 Embodied AI Workshop", "summary": "Diffusion models have shown remarkable flexibility for solving inverse\nproblems without task-specific retraining. However, existing approaches such as\nManifold Preserving Guided Diffusion (MPGD) apply only a single gradient update\nper denoising step, limiting restoration fidelity and robustness, especially in\nembedded or out-of-distribution settings. In this work, we introduce a\nmultistep optimization strategy within each denoising timestep, significantly\nenhancing image quality, perceptual accuracy, and generalization. Our\nexperiments on super-resolution and Gaussian deblurring demonstrate that\nincreasing the number of gradient updates per step improves LPIPS and PSNR with\nminimal latency overhead. Notably, we validate this approach on a Jetson Orin\nNano using degraded ImageNet and a UAV dataset, showing that MPGD, originally\ntrained on face datasets, generalizes effectively to natural and aerial scenes.\nOur findings highlight MPGD's potential as a lightweight, plug-and-play\nrestoration module for real-time visual perception in embodied AI agents such\nas drones and mobile robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6b65\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982MPGD\uff09\u6bcf\u53bb\u566a\u6b65\u9aa4\u4ec5\u5e94\u7528\u5355\u6b21\u68af\u5ea6\u66f4\u65b0\uff0c\u9650\u5236\u4e86\u6062\u590d\u7684\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5728\u6bcf\u53bb\u566a\u6b65\u9aa4\u4e2d\u5f15\u5165\u591a\u6b65\u4f18\u5316\u7b56\u7565\uff0c\u589e\u52a0\u68af\u5ea6\u66f4\u65b0\u6b21\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u6b65\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86LPIPS\u548cPSNR\u6307\u6807\uff0c\u5e76\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MPGD\u53ef\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u6062\u590d\u6a21\u5757\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u548c\u79fb\u52a8\u673a\u5668\u4eba\u7b49\u5b9e\u65f6\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u3002"}}
{"id": "2506.07304", "pdf": "https://arxiv.org/pdf/2506.07304", "abs": "https://arxiv.org/abs/2506.07304", "authors": ["Kavitha Viswanathan", "Vrinda Goel", "Shlesh Gholap", "Devayan Ghosh", "Madhav Gupta", "Dhruvi Ganatra", "Sanket Potdar", "Amit Sethi"], "title": "FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos", "categories": ["cs.CV"], "comment": null, "summary": "Real-world surveillance often renders faces and license plates unrecognizable\nin individual low-resolution (LR) frames, hindering reliable identification. To\nadvance temporal recognition models, we present FANVID, a novel video-based\nbenchmark comprising nearly 1,463 LR clips (180 x 320, 20--60 FPS) featuring 63\nidentities and 49 license plates from three English-speaking countries. Each\nvideo includes distractor faces and plates, increasing task difficulty and\nrealism. The dataset contains 31,096 manually verified bounding boxes and\nlabels.\n  FANVID defines two tasks: (1) face matching -- detecting LR faces and\nmatching them to high-resolution mugshots, and (2) license plate recognition --\nextracting text from LR plates without a predefined database. Videos are\ndownsampled from high-resolution sources to ensure that faces and text are\nindecipherable in single frames, requiring models to exploit temporal\ninformation. We introduce evaluation metrics adapted from mean Average\nPrecision at IoU > 0.5, prioritizing identity correctness for faces and\ncharacter-level accuracy for text.\n  A baseline method with pre-trained video super-resolution, detection, and\nrecognition achieved performance scores of 0.58 (face matching) and 0.42 (plate\nrecognition), highlighting both the feasibility and challenge of the tasks.\nFANVID's selection of faces and plates balances diversity with recognition\nchallenge. We release the software for data access, evaluation, baseline, and\nannotation to support reproducibility and extension. FANVID aims to catalyze\ninnovation in temporal modeling for LR recognition, with applications in\nsurveillance, forensics, and autonomous vehicles.", "AI": {"tldr": "FANVID\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u89c6\u9891\u7247\u6bb5\uff0c\u7528\u4e8e\u63d0\u5347\u65f6\u95f4\u8bc6\u522b\u6a21\u578b\uff0c\u652f\u6301\u4eba\u8138\u5339\u914d\u548c\u8f66\u724c\u8bc6\u522b\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u76d1\u63a7\u4e2d\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u96be\u4ee5\u8bc6\u522b\u4eba\u8138\u548c\u8f66\u724c\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u65f6\u95f4\u5efa\u6a21\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b1,463\u4e2aLR\u89c6\u9891\u7247\u6bb5\uff0c\u63d0\u4f9b\u624b\u52a8\u9a8c\u8bc1\u7684\u8fb9\u754c\u6846\u548c\u6807\u7b7e\uff0c\u5b9a\u4e49\u4eba\u8138\u5339\u914d\u548c\u8f66\u724c\u8bc6\u522b\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u57fa\u7ebf\u65b9\u6cd5\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u5206\u522b\u83b7\u5f970.58\u548c0.42\u7684\u5206\u6570\uff0c\u663e\u793a\u4efb\u52a1\u7684\u53ef\u884c\u6027\u548c\u6311\u6218\u6027\u3002", "conclusion": "FANVID\u65e8\u5728\u4fc3\u8fdb\u4f4e\u5206\u8fa8\u7387\u8bc6\u522b\u7684\u65f6\u95f4\u5efa\u6a21\u521b\u65b0\uff0c\u9002\u7528\u4e8e\u76d1\u63a7\u3001\u6cd5\u533b\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u3002"}}
{"id": "2506.07310", "pdf": "https://arxiv.org/pdf/2506.07310", "abs": "https://arxiv.org/abs/2506.07310", "authors": ["Adam W. Harley", "Yang You", "Xinglong Sun", "Yang Zheng", "Nikhil Raghuraman", "Yunqi Gu", "Sheldon Liang", "Wen-Hsuan Chu", "Achal Dave", "Pavel Tokmakov", "Suya You", "Rares Ambrus", "Katerina Fragkiadaki", "Leonidas J. Guibas"], "title": "AllTracker: Efficient Dense Point Tracking at High Resolution", "categories": ["cs.CV"], "comment": null, "summary": "We introduce AllTracker: a model that estimates long-range point tracks by\nway of estimating the flow field between a query frame and every other frame of\na video. Unlike existing point tracking methods, our approach delivers\nhigh-resolution and dense (all-pixel) correspondence fields, which can be\nvisualized as flow maps. Unlike existing optical flow methods, our approach\ncorresponds one frame to hundreds of subsequent frames, rather than just the\nnext frame. We develop a new architecture for this task, blending techniques\nfrom existing work in optical flow and point tracking: the model performs\niterative inference on low-resolution grids of correspondence estimates,\npropagating information spatially via 2D convolution layers, and propagating\ninformation temporally via pixel-aligned attention layers. The model is fast\nand parameter-efficient (16 million parameters), and delivers state-of-the-art\npoint tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on\na 40G GPU). A benefit of our design is that we can train on a wider set of\ndatasets, and we find that doing so is crucial for top performance. We provide\nan extensive ablation study on our architecture details and training recipe,\nmaking it clear which details matter most. Our code and model weights are\navailable at https://alltracker.github.io .", "AI": {"tldr": "AllTracker\u662f\u4e00\u79cd\u901a\u8fc7\u4f30\u8ba1\u67e5\u8be2\u5e27\u4e0e\u89c6\u9891\u4e2d\u5176\u4ed6\u5e27\u4e4b\u95f4\u7684\u6d41\u573a\u6765\u4f30\u8ba1\u957f\u8ddd\u79bb\u70b9\u8f68\u8ff9\u7684\u6a21\u578b\uff0c\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u548c\u5bc6\u96c6\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u70b9\u8ddf\u8e2a\u548c\u5149\u6d41\u4f30\u8ba1\u4e0a\u5404\u6709\u5c40\u9650\uff0c\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u3001\u5bc6\u96c6\u5bf9\u5e94\u548c\u957f\u8ddd\u79bb\u8ddf\u8e2a\u3002", "method": "\u7ed3\u5408\u5149\u6d41\u548c\u70b9\u8ddf\u8e2a\u6280\u672f\uff0c\u91c7\u7528\u4f4e\u5206\u8fa8\u7387\u7f51\u683c\u8fed\u4ee3\u63a8\u65ad\uff0c\u901a\u8fc72D\u5377\u79ef\u5c42\u548c\u50cf\u7d20\u5bf9\u9f50\u6ce8\u610f\u529b\u5c42\u4f20\u64ad\u4fe1\u606f\u3002", "result": "\u6a21\u578b\u53c2\u6570\u9ad8\u6548\uff081600\u4e07\u53c2\u6570\uff09\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\uff08768x1024\u50cf\u7d20\uff09\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u70b9\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "AllTracker\u5728\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.07327", "pdf": "https://arxiv.org/pdf/2506.07327", "abs": "https://arxiv.org/abs/2506.07327", "authors": ["Dane Williamson", "Yangfeng Ji", "Matthew Dwyer"], "title": "\"CASE: Contrastive Activation for Saliency Estimation", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.5.5; I.2.10"], "comment": "9 pages, 5 figures. Submitted to IEEE Transactions on Neural Networks\n  and Learning Systems (TNNLS)", "summary": "Saliency methods are widely used to visualize which input features are deemed\nrelevant to a model's prediction. However, their visual plausibility can\nobscure critical limitations. In this work, we propose a diagnostic test for\nclass sensitivity: a method's ability to distinguish between competing class\nlabels on the same input. Through extensive experiments, we show that many\nwidely used saliency methods produce nearly identical explanations regardless\nof the class label, calling into question their reliability. We find that\nclass-insensitive behavior persists across architectures and datasets,\nsuggesting the failure mode is structural rather than model-specific. Motivated\nby these findings, we introduce CASE, a contrastive explanation method that\nisolates features uniquely discriminative for the predicted class. We evaluate\nCASE using the proposed diagnostic and a perturbation-based fidelity test, and\nshow that it produces faithful and more class-specific explanations than\nexisting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bca\u65ad\u6d4b\u8bd5\uff08class sensitivity\uff09\u6765\u8bc4\u4f30\u663e\u8457\u6027\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u8bb8\u591a\u65b9\u6cd5\u5bf9\u7c7b\u522b\u6807\u7b7e\u4e0d\u654f\u611f\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5bf9\u6bd4\u6027\u89e3\u91ca\u65b9\u6cd5CASE\u3002", "motivation": "\u663e\u8457\u6027\u65b9\u6cd5\u5728\u53ef\u89c6\u5316\u6a21\u578b\u9884\u6d4b\u65f6\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u89c6\u89c9\u5408\u7406\u6027\u53ef\u80fd\u63a9\u76d6\u4e86\u5173\u952e\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u7c7b\u522b\u6807\u7b7e\u7684\u654f\u611f\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bca\u65ad\u6d4b\u8bd5\uff08class sensitivity\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u73b0\u6709\u663e\u8457\u6027\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff1b\u968f\u540e\u63d0\u51fa\u4e86\u5bf9\u6bd4\u6027\u89e3\u91ca\u65b9\u6cd5CASE\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bb8\u591a\u663e\u8457\u6027\u65b9\u6cd5\u5bf9\u7c7b\u522b\u6807\u7b7e\u4e0d\u654f\u611f\uff0c\u800cCASE\u80fd\u591f\u751f\u6210\u66f4\u5fe0\u5b9e\u4e14\u7c7b\u522b\u7279\u5b9a\u7684\u89e3\u91ca\u3002", "conclusion": "CASE\u5728\u8bca\u65ad\u6d4b\u8bd5\u548c\u4fdd\u771f\u5ea6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u663e\u8457\u6027\u65b9\u6cd5\u7684\u7ed3\u6784\u6027\u7f3a\u9677\u3002"}}
{"id": "2506.07338", "pdf": "https://arxiv.org/pdf/2506.07338", "abs": "https://arxiv.org/abs/2506.07338", "authors": ["Yijie Deng", "Shuaihang Yuan", "Geeta Chandra Raju Bethala", "Anthony Tzes", "Yu-Shen Liu", "Yi Fang"], "title": "Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Instance Image-Goal Navigation (IIN) requires autonomous agents to identify\nand navigate to a target object or location depicted in a reference image\ncaptured from any viewpoint. While recent methods leverage powerful novel view\nsynthesis (NVS) techniques, such as three-dimensional Gaussian splatting\n(3DGS), they typically rely on randomly sampling multiple viewpoints or\ntrajectories to ensure comprehensive coverage of discriminative visual cues.\nThis approach, however, creates significant redundancy through overlapping\nimage samples and lacks principled view selection, substantially increasing\nboth rendering and comparison overhead. In this paper, we introduce a novel IIN\nframework with a hierarchical scoring paradigm that estimates optimal\nviewpoints for target matching. Our approach integrates cross-level semantic\nscoring, utilizing CLIP-derived relevancy fields to identify regions with high\nsemantic similarity to the target object class, with fine-grained local\ngeometric scoring that performs precise pose estimation within promising\nregions. Extensive evaluations demonstrate that our method achieves\nstate-of-the-art performance on simulated IIN benchmarks and real-world\napplicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u8bc4\u5206\u8303\u5f0f\u7684\u5b9e\u4f8b\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5c42\u6b21\u8bed\u4e49\u8bc4\u5206\u548c\u5c40\u90e8\u51e0\u4f55\u8bc4\u5206\u4f18\u5316\u89c6\u70b9\u9009\u62e9\uff0c\u51cf\u5c11\u5197\u4f59\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u89c6\u70b9\u6216\u8f68\u8ff9\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u6548\u7387\u4f4e\u4e0b\uff0c\u7f3a\u4e4f\u4f18\u5316\u7684\u89c6\u70b9\u9009\u62e9\u673a\u5236\u3002", "method": "\u7ed3\u5408CLIP\u9a71\u52a8\u7684\u8bed\u4e49\u76f8\u5173\u6027\u548c\u5c40\u90e8\u51e0\u4f55\u8bc4\u5206\uff0c\u4f30\u8ba1\u6700\u4f73\u5339\u914d\u89c6\u70b9\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u5206\u5c42\u8bc4\u5206\u8303\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.07357", "pdf": "https://arxiv.org/pdf/2506.07357", "abs": "https://arxiv.org/abs/2506.07357", "authors": ["Satvik Praveen", "Yoonsung Jung"], "title": "CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object detection is vital in precision agriculture for plant monitoring,\ndisease detection, and yield estimation. However, models like YOLO struggle\nwith occlusions, irregular structures, and background noise, reducing detection\naccuracy. While Spatial Transformer Networks (STNs) improve spatial invariance\nthrough learned transformations, affine mappings are insufficient for non-rigid\ndeformations such as bent leaves and overlaps.\n  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)\ninto STNs for flexible, non-rigid spatial transformations that better align\nfeatures. Performance is further enhanced by the Convolutional Block Attention\nModule (CBAM), which suppresses background noise and emphasizes relevant\nspatial and channel-wise features.\n  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model\noutperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction\nin false positives, highlighting the benefits of improved spatial flexibility\nand attention-guided refinement. We also examine the impact of the TPS\nregularization parameter in balancing transformation smoothness and detection\nperformance.\n  This lightweight model improves spatial awareness and supports real-time edge\ndeployment, making it ideal for smart farming applications requiring accurate\nand efficient monitoring.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Thin-Plate Splines\uff08TPS\uff09\u548cSTN\u7684CBAM-STN-TPS-YOLO\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u519c\u4e1a\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u906e\u6321\u548c\u975e\u521a\u6027\u53d8\u5f62\u95ee\u9898\uff0c\u5e76\u901a\u8fc7CBAM\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\uff08\u5982YOLO\uff09\u5728\u519c\u4e1a\u76ee\u6807\u68c0\u6d4b\u4e2d\u56e0\u906e\u6321\u3001\u4e0d\u89c4\u5219\u7ed3\u6784\u548c\u80cc\u666f\u566a\u58f0\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\uff0c\u800cSTN\u7684\u4eff\u5c04\u53d8\u6362\u65e0\u6cd5\u5904\u7406\u975e\u521a\u6027\u53d8\u5f62\uff08\u5982\u5f2f\u66f2\u53f6\u7247\u548c\u91cd\u53e0\uff09\u3002", "method": "\u63d0\u51faCBAM-STN-TPS-YOLO\u6a21\u578b\uff0c\u5c06TPS\u5f15\u5165STN\u4ee5\u5b9e\u73b0\u7075\u6d3b\u7684\u975e\u521a\u6027\u7a7a\u95f4\u53d8\u6362\uff0c\u5e76\u7ed3\u5408CBAM\u6a21\u5757\u6291\u5236\u80cc\u666f\u566a\u58f0\uff0c\u7a81\u51fa\u76f8\u5173\u7279\u5f81\u3002", "result": "\u5728PGP\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cmAP\u4e0a\u4f18\u4e8eSTN-YOLO\uff0c\u5047\u9633\u6027\u51cf\u5c1112%\uff0c\u540c\u65f6\u7814\u7a76\u4e86TPS\u6b63\u5219\u5316\u53c2\u6570\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u6a21\u578b\u63d0\u5347\u4e86\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u9002\u5408\u5b9e\u65f6\u8fb9\u7f18\u90e8\u7f72\uff0c\u4e3a\u667a\u80fd\u519c\u4e1a\u63d0\u4f9b\u9ad8\u6548\u7cbe\u51c6\u7684\u76d1\u6d4b\u65b9\u6848\u3002"}}
{"id": "2506.07364", "pdf": "https://arxiv.org/pdf/2506.07364", "abs": "https://arxiv.org/abs/2506.07364", "authors": ["Chengchao Shen", "Dawei Liu", "Jianxin Wang"], "title": "Multiple Object Stitching for Unsupervised Representation Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Contrastive learning for single object centric images has achieved remarkable\nprogress on unsupervised representation, but suffering inferior performance on\nthe widespread images with multiple objects. In this paper, we propose a simple\nbut effective method, Multiple Object Stitching (MOS), to refine the\nunsupervised representation for multi-object images. Specifically, we construct\nthe multi-object images by stitching the single object centric ones, where the\nobjects in the synthesized multi-object images are predetermined. Hence,\ncompared to the existing contrastive methods, our method provides additional\nobject correspondences between multi-object images without human annotations.\nIn this manner, our method pays more attention to the representations of each\nobject in multi-object image, thus providing more detailed representations for\ncomplicated downstream tasks, such as object detection and semantic\nsegmentation. Experimental results on ImageNet, CIFAR and COCO datasets\ndemonstrate that our proposed method achieves the leading unsupervised\nrepresentation performance on both single object centric images and\nmulti-object ones. The source code is available at\nhttps://github.com/visresearch/MultipleObjectStitching.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMOS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u62fc\u63a5\u5355\u76ee\u6807\u56fe\u50cf\u751f\u6210\u591a\u76ee\u6807\u56fe\u50cf\uff0c\u63d0\u5347\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u5728\u591a\u76ee\u6807\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5bf9\u8c61\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u62fc\u63a5\u5355\u76ee\u6807\u56fe\u50cf\u751f\u6210\u591a\u76ee\u6807\u56fe\u50cf\uff0c\u5229\u7528\u9884\u5b9a\u4e49\u7684\u5bf9\u8c61\u5bf9\u5e94\u5173\u7cfb\u4f18\u5316\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728ImageNet\u3001CIFAR\u548cCOCO\u6570\u636e\u96c6\u4e0a\uff0cMOS\u65b9\u6cd5\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u56fe\u50cf\u4e0a\u5747\u53d6\u5f97\u9886\u5148\u7684\u65e0\u76d1\u7763\u8868\u793a\u6027\u80fd\u3002", "conclusion": "MOS\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u80fd\u663e\u8457\u63d0\u5347\u591a\u76ee\u6807\u56fe\u50cf\u7684\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2506.07368", "pdf": "https://arxiv.org/pdf/2506.07368", "abs": "https://arxiv.org/abs/2506.07368", "authors": ["Jiaying He", "Yitong Lin", "Jiahe Chen", "Honghui Xu", "Jianwei Zheng"], "title": "C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures, ICME2025", "summary": "For the immanent challenge of insufficiently annotated samples in the medical\nfield, semi-supervised medical image segmentation (SSMIS) offers a promising\nsolution. Despite achieving impressive results in delineating primary target\nareas, most current methodologies struggle to precisely capture the subtle\ndetails of boundaries. This deficiency often leads to significant diagnostic\ninaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised\nsegmentation model that synergistically integrates complementary competition\nand contrastive selection. This design significantly sharpens boundary\ndelineation and enhances overall precision. Specifically, we develop an\n$\\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining\nboundary localization. Additionally, we incorporate a $\\textit{Dynamic\nComplementary Competition}$ module that leverages two high-performing\nsub-networks to generate pseudo-labels, thereby further improving segmentation\nquality. The proposed C3S3 undergoes rigorous validation on two publicly\naccessible datasets, encompassing the practices of both MRI and CT scans. The\nresults demonstrate that our method achieves superior performance compared to\nprevious cutting-edge competitors. Especially, on the 95HD and ASD metrics, our\napproach achieves a notable improvement of at least $6\\%$, highlighting the\nsignificant advancements. The code is available at\nhttps://github.com/Y-TARL/C3S3.", "AI": {"tldr": "\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u6ce8\u6837\u672c\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u5206\u5272\u6a21\u578bC3S3\uff0c\u901a\u8fc7\u4e92\u8865\u7ade\u4e89\u548c\u5bf9\u6bd4\u9009\u62e9\u663e\u8457\u63d0\u5347\u8fb9\u754c\u7ec6\u8282\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u533b\u5b66\u9886\u57df\u6807\u6ce8\u6837\u672c\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u6355\u6349\u8fb9\u754c\u7ec6\u8282\uff0c\u5bfc\u81f4\u8bca\u65ad\u4e0d\u51c6\u786e\u3002", "method": "C3S3\u7ed3\u5408\u4e86\u7ed3\u679c\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u548c\u52a8\u6001\u4e92\u8865\u7ade\u4e89\u6a21\u5757\uff0c\u4f18\u5316\u8fb9\u754c\u5b9a\u4f4d\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c95HD\u548cASD\u6307\u6807\u63d0\u5347\u81f3\u5c116%\u3002", "conclusion": "C3S3\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u754c\u7ec6\u8282\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.07369", "pdf": "https://arxiv.org/pdf/2506.07369", "abs": "https://arxiv.org/abs/2506.07369", "authors": ["Bolin Chen", "Shanzhi Yin", "Goluck Konuko", "Giuseppe Valenzise", "Zihan Zhang", "Shiqi Wang", "Yan Ye"], "title": "Generative Models at the Frontier of Compression: A Survey on Generative Face Video Coding", "categories": ["cs.CV"], "comment": null, "summary": "The rise of deep generative models has greatly advanced video compression,\nreshaping the paradigm of face video coding through their powerful capability\nfor semantic-aware representation and lifelike synthesis. Generative Face Video\nCoding (GFVC) stands at the forefront of this revolution, which could\ncharacterize complex facial dynamics into compact latent codes for bitstream\ncompactness at the encoder side and leverages powerful deep generative models\nto reconstruct high-fidelity face signal from the compressed latent codes at\nthe decoder side. As such, this well-designed GFVC paradigm could enable\nhigh-fidelity face video communication at ultra-low bitrate ranges, far\nsurpassing the capabilities of the latest Versatile Video Coding (VVC)\nstandard. To pioneer foundational research and accelerate the evolution of\nGFVC, this paper presents the first comprehensive survey of GFVC technologies,\nsystematically bridging critical gaps between theoretical innovation and\nindustrial standardization. In particular, we first review a broad range of\nexisting GFVC methods with different feature representations and optimization\nstrategies, and conduct a thorough benchmarking analysis. In addition, we\nconstruct a large-scale GFVC-compressed face video database with subjective\nMean Opinion Scores (MOSs) based on human perception, aiming to identify the\nmost appropriate quality metrics tailored to GFVC. Moreover, we summarize the\nGFVC standardization potentials with a unified high-level syntax and develop a\nlow-complexity GFVC system which are both expected to push forward future\npractical deployments and applications. Finally, we envision the potential of\nGFVC in industrial applications and deliberate on the current challenges and\nfuture opportunities.", "AI": {"tldr": "GFVC\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u9762\u90e8\u89c6\u9891\u538b\u7f29\uff0c\u8d85\u8d8a\u4f20\u7edfVVC\u6807\u51c6\uff0c\u5e76\u63a8\u52a8\u6807\u51c6\u5316\u4e0e\u5de5\u4e1a\u5e94\u7528\u3002", "motivation": "\u63d0\u5347\u9762\u90e8\u89c6\u9891\u538b\u7f29\u6548\u7387\uff0c\u5b9e\u73b0\u8d85\u4f4e\u7801\u7387\u4e0b\u7684\u9ad8\u4fdd\u771f\u901a\u4fe1\u3002", "method": "\u7efc\u8ff0\u73b0\u6709GFVC\u6280\u672f\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u5e93\uff0c\u63d0\u51fa\u6807\u51c6\u5316\u8bed\u6cd5\u548c\u4f4e\u590d\u6742\u5ea6\u7cfb\u7edf\u3002", "result": "GFVC\u5728\u538b\u7f29\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8eVVC\uff0c\u5e76\u5177\u5907\u6807\u51c6\u5316\u6f5c\u529b\u3002", "conclusion": "GFVC\u5728\u5de5\u4e1a\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5f53\u524d\u6311\u6218\u3002"}}
{"id": "2506.07371", "pdf": "https://arxiv.org/pdf/2506.07371", "abs": "https://arxiv.org/abs/2506.07371", "authors": ["Ruchit Rawal", "Reza Shirkavand", "Heng Huang", "Gowthami Somepalli", "Tom Goldstein"], "title": "ARGUS: Hallucination and Omission Evaluation in Video-LLMs", "categories": ["cs.CV"], "comment": "Project page with all the artifacts:\n  https://ruchitrawal.github.io/argus", "summary": "Video large language models have not yet been widely deployed, largely due to\ntheir tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on\nmultiple-choice questions. Unfortunately, VideoLLMs hallucinate far more\naggressively on freeform text generation tasks like video captioning than they\ndo on multiple choice verification tasks. To address this weakness, we propose\nARGUS, a VideoLLM benchmark that measures freeform video captioning\nperformance. By comparing VideoLLM outputs to human ground truth captions,\nARGUS quantifies dual metrics. First, we measure the rate of hallucinations in\nthe form of incorrect statements about video content or temporal relationships.\nSecond, we measure the rate at which the model omits important descriptive\ndetails. Together, these dual metrics form a comprehensive view of video\ncaptioning performance.", "AI": {"tldr": "ARGUS\u662f\u4e00\u4e2a\u65b0\u7684VideoLLM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u81ea\u7531\u6587\u672c\u751f\u6210\u4efb\u52a1\uff08\u5982\u89c6\u9891\u5b57\u5e55\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u5e7b\u89c9\u7387\u548c\u9057\u6f0f\u7ec6\u8282\u7387\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684Video-LLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f9d\u8d56\u591a\u9009\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u6a21\u578b\u5728\u81ea\u7531\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faARGUS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6bd4\u8f83VideoLLM\u751f\u6210\u7684\u89c6\u9891\u5b57\u5e55\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u771f\u5b9e\u5b57\u5e55\uff0c\u91cf\u5316\u5e7b\u89c9\u7387\uff08\u9519\u8bef\u5185\u5bb9\u6216\u65f6\u95f4\u5173\u7cfb\uff09\u548c\u9057\u6f0f\u7ec6\u8282\u7387\u3002", "result": "ARGUS\u63d0\u4f9b\u4e86\u53cc\u91cd\u6307\u6807\uff0c\u5168\u9762\u8bc4\u4f30\u89c6\u9891\u5b57\u5e55\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u81ea\u7531\u6587\u672c\u751f\u6210\u4e2d\u7684\u4e3b\u8981\u95ee\u9898\u3002", "conclusion": "ARGUS\u4e3aVideoLLM\u7684\u81ea\u7531\u6587\u672c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.07375", "pdf": "https://arxiv.org/pdf/2506.07375", "abs": "https://arxiv.org/abs/2506.07375", "authors": ["Xunjie He", "Christina Dao Wen Lee", "Meiling Wang", "Chengran Yuan", "Zefan Huang", "Yufeng Yue", "Marcelo H. Ang Jr"], "title": "DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Collaborative perception plays a crucial role in enhancing environmental\nunderstanding by expanding the perceptual range and improving robustness\nagainst sensor failures, which primarily involves collaborative 3D detection\nand tracking tasks. The former focuses on object recognition in individual\nframes, while the latter captures continuous instance tracklets over time.\nHowever, existing works in both areas predominantly focus on the vehicle\nsuperclass, lacking effective solutions for both multi-class collaborative\ndetection and tracking. This limitation hinders their applicability in\nreal-world scenarios, which involve diverse object classes with varying\nappearances and motion patterns. To overcome these limitations, we propose a\nmulti-class collaborative detection and tracking framework tailored for diverse\nroad users. We first present a detector with a global spatial attention fusion\n(GSAF) module, enhancing multi-scale feature learning for objects of varying\nsizes. Next, we introduce a tracklet RE-IDentification (REID) module that\nleverages visual semantics with a vision foundation model to effectively reduce\nID SWitch (IDSW) errors, in cases of erroneous mismatches involving small\nobjects like pedestrians. We further design a velocity-based adaptive tracklet\nmanagement (VATM) module that adjusts the tracking interval dynamically based\non object motion. Extensive experiments on the V2X-Real and OPV2V datasets show\nthat our approach significantly outperforms existing state-of-the-art methods\nin both detection and tracking accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c7b\u522b\u534f\u4f5c\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u7a7a\u95f4\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u548c\u89c6\u89c9\u8bed\u4e49\u7684REID\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u534f\u4f5c\u611f\u77e5\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u8f66\u8f86\u7c7b\u522b\uff0c\u7f3a\u4e4f\u5bf9\u591a\u7c7b\u522b\u5bf9\u8c61\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u5168\u5c40\u7a7a\u95f4\u6ce8\u610f\u529b\u878d\u5408\uff08GSAF\uff09\u6a21\u5757\u3001\u57fa\u4e8e\u89c6\u89c9\u8bed\u4e49\u7684REID\u6a21\u5757\u548c\u901f\u5ea6\u81ea\u9002\u5e94\u7684\u8f68\u8ff9\u7ba1\u7406\uff08VATM\uff09\u6a21\u5757\u3002", "result": "\u5728V2X-Real\u548cOPV2V\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7c7b\u522b\u534f\u4f5c\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u573a\u666f\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2506.07376", "pdf": "https://arxiv.org/pdf/2506.07376", "abs": "https://arxiv.org/abs/2506.07376", "authors": ["Jintao Tong", "Ran Ma", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "title": "Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025 Spotlight", "summary": "Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the\nmodel on a source-domain dataset with sufficient samples, and then transfer the\nmodel to target-domain datasets where only a few samples are available for\nefficient fine-tuning. There are majorly two challenges in this task: (1) the\ndomain gap and (2) fine-tuning with scarce data. To solve these challenges, we\nrevisit the adapter-based methods, and discover an intriguing insight not\nexplored in previous works: the adapter not only helps the fine-tuning of\ndownstream tasks but also naturally serves as a domain information decoupler.\nThen, we delve into this finding for an interpretation, and find the model's\ninherent structure could lead to a natural decoupling of domain information.\nBuilding upon this insight, we propose the Domain Feature Navigator (DFN),\nwhich is a structure-based decoupler instead of loss-based ones like current\nworks, to capture domain-specific information, thereby directing the model's\nattention towards domain-agnostic knowledge. Moreover, to prevent the potential\nexcessive overfitting of DFN during the source-domain training, we further\ndesign the SAM-SVN method to constrain DFN from learning sample-specific\nknowledge. On target domains, we freeze the model and fine-tune the DFN to\nlearn target-specific knowledge specific. Extensive experiments demonstrate\nthat our method surpasses the state-of-the-art method in CD-FSS significantly\nby 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u65b9\u6cd5\uff08CD-FSS\uff09\uff0c\u901a\u8fc7\u9002\u914d\u5668\u6280\u672f\u89e3\u51b3\u57df\u5dee\u8ddd\u548c\u5c11\u6837\u672c\u5fae\u8c03\u95ee\u9898\uff0c\u5e76\u63d0\u51faDomain Feature Navigator\uff08DFN\uff09\u548cSAM-SVN\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u4e2d\u7684\u57df\u5dee\u8ddd\u548c\u5c11\u6837\u672c\u5fae\u8c03\u95ee\u9898\uff0c\u63a2\u7d22\u9002\u914d\u5668\u5728\u57df\u4fe1\u606f\u89e3\u8026\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51faDomain Feature Navigator\uff08DFN\uff09\u4f5c\u4e3a\u7ed3\u6784\u5316\u89e3\u8026\u5668\uff0c\u7ed3\u5408SAM-SVN\u65b9\u6cd5\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u5e76\u5728\u76ee\u6807\u57df\u4e0a\u5fae\u8c03DFN\u3002", "result": "\u57281-shot\u548c5-shot\u573a\u666f\u4e0b\uff0c\u5206\u522b\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd52.69%\u548c4.68%\u7684MIoU\u3002", "conclusion": "DFN\u548cSAM-SVN\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.07399", "pdf": "https://arxiv.org/pdf/2506.07399", "abs": "https://arxiv.org/abs/2506.07399", "authors": ["Peiru Yang", "Jinhua Yin", "Haoran Zheng", "Xueying Bai", "Huili Wang", "Yufei Sun", "Xintian Li", "Shangguang Wang", "Yongfeng Huang", "Tao Qi"], "title": "MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal retrieval-augmented generation (RAG) systems enhance large\nvision-language models by integrating cross-modal knowledge, enabling their\nincreasing adoption across real-world multimodal tasks. These knowledge\ndatabases may contain sensitive information that requires privacy protection.\nHowever, multimodal RAG systems inherently grant external users indirect access\nto such data, making them potentially vulnerable to privacy attacks,\nparticularly membership inference attacks (MIAs). % Existing MIA methods\ntargeting RAG systems predominantly focus on the textual modality, while the\nvisual modality remains relatively underexplored. To bridge this gap, we\npropose MrM, the first black-box MIA framework targeted at multimodal RAG\nsystems. It utilizes a multi-object data perturbation framework constrained by\ncounterfactual attacks, which can concurrently induce the RAG systems to\nretrieve the target data and generate information that leaks the membership\ninformation. Our method first employs an object-aware data perturbation method\nto constrain the perturbation to key semantics and ensure successful retrieval.\nBuilding on this, we design a counterfact-informed mask selection strategy to\nprioritize the most informative masked regions, aiming to eliminate the\ninterference of model self-knowledge and amplify attack efficacy. Finally, we\nperform statistical membership inference by modeling query trials to extract\nfeatures that reflect the reconstruction of masked semantics from response\npatterns. Experiments on two visual datasets and eight mainstream commercial\nvisual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves\nconsistently strong performance across both sample-level and set-level\nevaluations, and remains robust under adaptive defenses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMrM\uff0c\u9996\u4e2a\u9488\u5bf9\u591a\u6a21\u6001RAG\u7cfb\u7edf\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u7406\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u6570\u636e\u6270\u52a8\u548c\u53cd\u4e8b\u5b9e\u653b\u51fb\u63d0\u5347\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u591a\u6a21\u6001RAG\u7cfb\u7edf\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u6a21\u6001\uff0c\u89c6\u89c9\u6a21\u6001\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u591a\u76ee\u6807\u6570\u636e\u6270\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u53cd\u4e8b\u5b9e\u653b\u51fb\u548c\u63a9\u7801\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u7edf\u8ba1\u63a8\u7406\u63d0\u53d6\u6210\u5458\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u548c\u516b\u4e2a\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86MrM\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "MrM\u586b\u8865\u4e86\u89c6\u89c9\u6a21\u6001\u6210\u5458\u63a8\u7406\u653b\u51fb\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u653b\u51fb\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.07412", "pdf": "https://arxiv.org/pdf/2506.07412", "abs": "https://arxiv.org/abs/2506.07412", "authors": ["Changsheng Gao", "Wei Zhou", "Guosheng Lin", "Weisi Lin"], "title": "Compressed Feature Quality Assessment: Dataset and Baselines", "categories": ["cs.CV"], "comment": null, "summary": "The widespread deployment of large models in resource-constrained\nenvironments has underscored the need for efficient transmission of\nintermediate feature representations. In this context, feature coding, which\ncompresses features into compact bitstreams, becomes a critical component for\nscenarios involving feature transmission, storage, and reuse. However, this\ncompression process introduces inherent semantic degradation that is\nnotoriously difficult to quantify with traditional metrics. To address this,\nthis paper introduces the research problem of Compressed Feature Quality\nAssessment (CFQA), which seeks to evaluate the semantic fidelity of compressed\nfeatures. To advance CFQA research, we propose the first benchmark dataset,\ncomprising 300 original features and 12000 compressed features derived from\nthree vision tasks and four feature codecs. Task-specific performance drops are\nprovided as true semantic distortion for the evaluation of CFQA metrics. We\nassess the performance of three widely used metrics (MSE, cosine similarity,\nand Centered Kernel Alignment) in capturing semantic degradation. The results\nunderscore the representativeness of the dataset and highlight the need for\nmore refined metrics capable of addressing the nuances of semantic distortion\nin compressed features. To facilitate the ongoing development of CFQA research,\nwe release the dataset and all accompanying source code at\n\\href{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}.\nThis contribution aims to advance the field and provide a foundational resource\nfor the community to explore CFQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u538b\u7f29\u7279\u5f81\u8d28\u91cf\u8bc4\u4f30\uff08CFQA\uff09\u95ee\u9898\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u539f\u59cb\u548c\u538b\u7f29\u7279\u5f81\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u5e38\u7528\u6307\u6807\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u6307\u6807\u6765\u8861\u91cf\u8bed\u4e49\u5931\u771f\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u9700\u8981\u9ad8\u6548\u4f20\u8f93\u4e2d\u95f4\u7279\u5f81\u8868\u793a\uff0c\u4f46\u7279\u5f81\u538b\u7f29\u4f1a\u5bfc\u81f4\u8bed\u4e49\u9000\u5316\uff0c\u4f20\u7edf\u6307\u6807\u96be\u4ee5\u91cf\u5316\u8fd9\u79cd\u9000\u5316\u3002", "method": "\u63d0\u51fa\u4e86CFQA\u95ee\u9898\uff0c\u521b\u5efa\u4e86\u5305\u542b300\u4e2a\u539f\u59cb\u7279\u5f81\u548c12000\u4e2a\u538b\u7f29\u7279\u5f81\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86MSE\u3001\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u4e2d\u5fc3\u6838\u5bf9\u9f50\u4e09\u79cd\u6307\u6807\u7684\u6027\u80fd\u3002", "result": "\u6570\u636e\u96c6\u5177\u6709\u4ee3\u8868\u6027\uff0c\u4f46\u73b0\u6709\u6307\u6807\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u538b\u7f29\u7279\u5f81\u7684\u8bed\u4e49\u5931\u771f\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u6307\u6807\u3002", "conclusion": "\u8bba\u6587\u4e3aCFQA\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u548c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.07414", "pdf": "https://arxiv.org/pdf/2506.07414", "abs": "https://arxiv.org/abs/2506.07414", "authors": ["Sheng-Kai Huang", "Jiun-Feng Chang", "Chun-Rong Huang"], "title": "DPFormer: Dynamic Prompt Transformer for Continual Learning", "categories": ["cs.CV"], "comment": null, "summary": "In continual learning, solving the catastrophic forgetting problem may make\nthe models fall into the stability-plasticity dilemma. Moreover, inter-task\nconfusion will also occur due to the lack of knowledge exchanges between\ndifferent tasks. In order to solve the aforementioned problems, we propose a\nnovel dynamic prompt transformer (DPFormer) with prompt schemes. The prompt\nschemes help the DPFormer memorize learned knowledge of previous classes and\ntasks, and keep on learning new knowledge from new classes and tasks under a\nsingle network structure with a nearly fixed number of model parameters.\nMoreover, they also provide discrepant information to represent different tasks\nto solve the inter-task confusion problem. Based on prompt schemes, a unified\nclassification module with the binary cross entropy loss, the knowledge\ndistillation loss and the auxiliary loss is proposed to train the whole model\nin an end-to-end trainable manner. Compared with state-of-the-art methods, our\nmethod achieves the best performance in the CIFAR-100, ImageNet100 and\nImageNet1K datasets under different class-incremental settings in continual\nlearning. The source code will be available at our GitHub after acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u63d0\u793a\u53d8\u6362\u5668\uff08DPFormer\uff09\u53ca\u5176\u63d0\u793a\u65b9\u6848\uff0c\u4ee5\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u548c\u4efb\u52a1\u95f4\u6df7\u6dc6\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u4e2d\u5b58\u5728\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u548c\u4efb\u52a1\u95f4\u6df7\u6dc6\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u65e2\u80fd\u8bb0\u5fc6\u65e7\u77e5\u8bc6\u53c8\u80fd\u5b66\u4e60\u65b0\u77e5\u8bc6\u3002", "method": "\u63d0\u51faDPFormer\u53ca\u5176\u63d0\u793a\u65b9\u6848\uff0c\u7ed3\u5408\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u3001\u77e5\u8bc6\u84b8\u998f\u635f\u5931\u548c\u8f85\u52a9\u635f\u5931\uff0c\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728CIFAR-100\u3001ImageNet100\u548cImageNet1K\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u589e\u91cf\u8bbe\u7f6e\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "DPFormer\u53ca\u5176\u63d0\u793a\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.07431", "pdf": "https://arxiv.org/pdf/2506.07431", "abs": "https://arxiv.org/abs/2506.07431", "authors": ["Jie He", "Minglang Chen", "Minying Lu", "Bocheng Liang", "Junming Wei", "Guiyan Peng", "Jiaxi Chen", "Ying Tan"], "title": "FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate ultrasound image segmentation is a prerequisite for precise\nbiometrics and accurate assessment. Relying on manual delineation introduces\nsignificant errors and is time-consuming. However, existing segmentation models\nare designed based on objects in natural scenes, making them difficult to adapt\nto ultrasound objects with high noise and high similarity. This is particularly\nevident in small object segmentation, where a pronounced jagged effect occurs.\nTherefore, this paper proposes a fetal femur and cranial ultrasound image\nsegmentation model based on feature perception and Mamba enhancement to address\nthese challenges. Specifically, a longitudinal and transverse independent\nviewpoint scanning convolution block and a feature perception module were\ndesigned to enhance the ability to capture local detail information and improve\nthe fusion of contextual information. Combined with the Mamba-optimized\nresidual structure, this design suppresses the interference of raw noise and\nenhances local multi-dimensional scanning. The system builds global information\nand local feature dependencies, and is trained with a combination of different\noptimizers to achieve the optimal solution. After extensive experimental\nvalidation, the FAMSeg network achieved the fastest loss reduction and the best\nsegmentation performance across images of varying sizes and orientations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u611f\u77e5\u548cMamba\u589e\u5f3a\u7684\u80ce\u513f\u80a1\u9aa8\u548c\u9885\u9aa8\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u566a\u58f0\u9ad8\u3001\u76f8\u4f3c\u6027\u5f3a\u7684\u8d85\u58f0\u56fe\u50cf\u4e2d\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u5bf9\u7cbe\u786e\u751f\u7269\u6d4b\u91cf\u548c\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u9ad8\u566a\u58f0\u548c\u9ad8\u76f8\u4f3c\u6027\u7684\u8d85\u58f0\u5bf9\u8c61\uff0c\u5c24\u5176\u662f\u5c0f\u5bf9\u8c61\u5206\u5272\u65f6\u952f\u9f7f\u6548\u5e94\u660e\u663e\u3002", "method": "\u8bbe\u8ba1\u4e86\u7eb5\u5411\u548c\u6a2a\u5411\u72ec\u7acb\u89c6\u89d2\u626b\u63cf\u5377\u79ef\u5757\u3001\u7279\u5f81\u611f\u77e5\u6a21\u5757\uff0c\u7ed3\u5408Mamba\u4f18\u5316\u7684\u6b8b\u5dee\u7ed3\u6784\uff0c\u589e\u5f3a\u5c40\u90e8\u7ec6\u8282\u6355\u6349\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u878d\u5408\uff0c\u6291\u5236\u539f\u59cb\u566a\u58f0\u5e72\u6270\u3002", "result": "FAMSeg\u7f51\u7edc\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u6700\u5feb\u7684\u635f\u5931\u51cf\u5c11\u548c\u6700\u4f73\u7684\u5206\u5272\u6027\u80fd\uff0c\u9002\u5e94\u4e0d\u540c\u5927\u5c0f\u548c\u65b9\u5411\u7684\u56fe\u50cf\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u7279\u5f81\u611f\u77e5\u548cMamba\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.07436", "pdf": "https://arxiv.org/pdf/2506.07436", "abs": "https://arxiv.org/abs/2506.07436", "authors": ["Nishi Chaudhary", "S M Jamil Uddin", "Sathvik Sharath Chandra", "Anto Ovid", "Alex Albert"], "title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "The recent emergence of multimodal large language models (LLMs) has\nintroduced new opportunities for improving visual hazard recognition on\nconstruction sites. Unlike traditional computer vision models that rely on\ndomain-specific training and extensive datasets, modern LLMs can interpret and\ndescribe complex visual scenes using simple natural language prompts. However,\ndespite growing interest in their applications, there has been limited\ninvestigation into how different LLMs perform in safety-critical visual tasks\nwithin the construction domain. To address this gap, this study conducts a\ncomparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,\nGPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify\npotential hazards from real-world construction images. Each model was tested\nunder three prompting strategies: zero-shot, few-shot, and chain-of-thought\n(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated\nbasic safety context and a hazard source mnemonic, and CoT provided\nstep-by-step reasoning examples to scaffold model thinking. Quantitative\nanalysis was performed using precision, recall, and F1-score metrics across all\nconditions. Results reveal that prompting strategy significantly influenced\nperformance, with CoT prompting consistently producing higher accuracy across\nmodels. Additionally, LLM performance varied under different conditions, with\nGPT-4.5 and GPT-o3 outperforming others in most settings. The findings also\ndemonstrate the critical role of prompt design in enhancing the accuracy and\nconsistency of multimodal LLMs for construction safety applications. This study\noffers actionable insights into the integration of prompt engineering and LLMs\nfor practical hazard recognition, contributing to the development of more\nreliable AI-assisted safety systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e94\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5efa\u7b51\u5de5\u5730\u89c6\u89c9\u5371\u9669\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u63d0\u793a\u7b56\u7565\uff08\u5c24\u5176\u662f\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\uff09\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0cGPT-4.5\u548cGPT-o3\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001LLMs\u5728\u5efa\u7b51\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5bf9\u4e0d\u540cLLMs\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u7a7a\u767d\u3002", "method": "\u5bf9\u4e94\u79cdLLMs\uff08Claude-3 Opus\u3001GPT-4.5\u3001GPT-4o\u3001GPT-o3\u548cGemini 2.0 Pro\uff09\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u91c7\u7528\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u3002", "result": "CoT\u63d0\u793a\u7b56\u7565\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cGPT-4.5\u548cGPT-o3\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f18\uff0c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u6a21\u6001LLMs\u5728\u5efa\u7b51\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u63d0\u793a\u5de5\u7a0b\u7684\u91cd\u8981\u6027\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u7684AI\u8f85\u52a9\u5b89\u5168\u7cfb\u7edf\u3002"}}
{"id": "2506.07456", "pdf": "https://arxiv.org/pdf/2506.07456", "abs": "https://arxiv.org/abs/2506.07456", "authors": ["Wei Yao", "Yunlian Sun", "Chang Liu", "Hongwen Zhang", "Jinhui Tang"], "title": "PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation", "categories": ["cs.CV"], "comment": null, "summary": "Driven by advancements in motion capture and generative artificial\nintelligence, leveraging large-scale MoCap datasets to train generative models\nfor synthesizing diverse, realistic human motions has become a promising\nresearch direction. However, existing motion-capture techniques and generative\nmodels often neglect physical constraints, leading to artifacts such as\ninterpenetration, sliding, and floating. These issues are exacerbated in\nmulti-person motion generation, where complex interactions are involved. To\naddress these limitations, we introduce physical mapping, integrated throughout\nthe human interaction generation pipeline. Specifically, motion imitation\nwithin a physics-based simulation environment is used to project target motions\ninto a physically valid space. The resulting motions are adjusted to adhere to\nreal-world physics constraints while retaining their original semantic meaning.\nThis mapping not only improves MoCap data quality but also directly informs\npost-processing of generated motions. Given the unique interactivity of\nmulti-person scenarios, we propose a tailored motion representation framework.\nMotion Consistency (MC) and Marker-based Interaction (MI) loss functions are\nintroduced to improve model performance. Experiments show our method achieves\nimpressive results in generated human motion quality, with a 3%-89% improvement\nin physical fidelity. Project page http://yw0208.github.io/physiinter", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u6620\u5c04\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u4eba\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u7269\u7406\u771f\u5b9e\u6027\u548c\u4ea4\u4e92\u8d28\u91cf\uff0c\u901a\u8fc7\u7269\u7406\u4eff\u771f\u548c\u635f\u5931\u51fd\u6570\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u6570\u636e\u7684\u7269\u7406\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u6355\u6349\u6280\u672f\u548c\u751f\u6210\u6a21\u578b\u5e38\u5ffd\u7565\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u8fd0\u52a8\u751f\u6210\u4e2d\u51fa\u73b0\u7a7f\u6a21\u3001\u6ed1\u52a8\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u5728\u591a\u4eba\u4ea4\u4e92\u573a\u666f\u4e2d\u66f4\u4e3a\u4e25\u91cd\u3002", "method": "\u63d0\u51fa\u7269\u7406\u6620\u5c04\u65b9\u6cd5\uff0c\u7ed3\u5408\u7269\u7406\u4eff\u771f\u73af\u5883\u8fdb\u884c\u8fd0\u52a8\u6a21\u4eff\uff0c\u5e76\u5f15\u5165\u8fd0\u52a8\u4e00\u81f4\u6027\uff08MC\uff09\u548c\u57fa\u4e8e\u6807\u8bb0\u7684\u4ea4\u4e92\uff08MI\uff09\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7269\u7406\u4fdd\u771f\u5ea6\u63d0\u5347\u4e863%-89%\u3002", "conclusion": "\u7269\u7406\u6620\u5c04\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u7269\u7406\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u591a\u4eba\u4ea4\u4e92\u573a\u666f\u7684\u8fd0\u52a8\u771f\u5b9e\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2506.07460", "pdf": "https://arxiv.org/pdf/2506.07460", "abs": "https://arxiv.org/abs/2506.07460", "authors": ["Taeryung Lee", "Hyeongjin Nam", "Gyeongsik Moon", "Kyoung Mu Lee"], "title": "GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Sign language generation (SLG), or text-to-sign generation, bridges the gap\nbetween signers and non-signers. Despite recent progress in SLG, existing\nmethods still often suffer from incorrect lexical ordering and low semantic\naccuracy. This is primarily due to sentence-level condition, which encodes the\nentire sentence of the input text into a single feature vector as a condition\nfor SLG. This approach fails to capture the temporal structure of sign language\nand lacks the granularity of word-level semantics, often leading to disordered\nsign sequences and ambiguous motions. To overcome these limitations, we propose\nGLOS, a sign language generation framework with temporally aligned gloss-level\nconditioning. First, we employ gloss-level conditions, which we define as\nsequences of gloss embeddings temporally aligned with the motion sequence. This\nenables the model to access both the temporal structure of sign language and\nword-level semantics at each timestep. As a result, this allows for\nfine-grained control of signs and better preservation of lexical order. Second,\nwe introduce a condition fusion module, temporal alignment conditioning (TAC),\nto efficiently deliver the word-level semantic and temporal structure provided\nby the gloss-level condition to the corresponding motion timesteps. Our method,\nwhich is composed of gloss-level conditions and TAC, generates signs with\ncorrect lexical order and high semantic accuracy, outperforming prior methods\non CSL-Daily and Phoenix-2014T.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGLOS\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u5bf9\u9f50\u7684gloss\u7ea7\u6761\u4ef6\u751f\u6210\u624b\u8bed\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bcd\u5e8f\u548c\u8bed\u4e49\u51c6\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u751f\u6210\u65b9\u6cd5\u56e0\u53e5\u5b50\u7ea7\u6761\u4ef6\u5bfc\u81f4\u8bcd\u5e8f\u9519\u8bef\u548c\u8bed\u4e49\u51c6\u786e\u6027\u4f4e\uff0c\u65e0\u6cd5\u6355\u6349\u624b\u8bed\u7684\u65f6\u95f4\u7ed3\u6784\u548c\u8bcd\u7ea7\u8bed\u4e49\u3002", "method": "\u91c7\u7528gloss\u7ea7\u6761\u4ef6\uff08\u65f6\u95f4\u5bf9\u9f50\u7684gloss\u5d4c\u5165\u5e8f\u5217\uff09\u548c\u6761\u4ef6\u878d\u5408\u6a21\u5757TAC\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u8bcd\u5e8f\u4fdd\u6301\u3002", "result": "\u5728CSL-Daily\u548cPhoenix-2014T\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u624b\u8bed\u8bcd\u5e8f\u6b63\u786e\u4e14\u8bed\u4e49\u51c6\u786e\u3002", "conclusion": "GLOS\u6846\u67b6\u901a\u8fc7gloss\u7ea7\u6761\u4ef6\u548cTAC\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u8bed\u751f\u6210\u7684\u8bcd\u5e8f\u548c\u8bed\u4e49\u51c6\u786e\u6027\u3002"}}
{"id": "2506.07464", "pdf": "https://arxiv.org/pdf/2506.07464", "abs": "https://arxiv.org/abs/2506.07464", "authors": ["Jinyoung Park", "Jeehye Na", "Jinyoung Kim", "Hyunwoo J. Kim"], "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training in enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success by employing a PPO-style reinforcement\nalgorithm with group-based normalized rewards. However, the application of GRPO\nto Video Large Language Models (Video LLMs) has been less studied. In this\npaper, we explore GRPO for video LLMs and identify two primary issues that\nimpede its effective learning: (1) reliance on safeguards, and (2) the\nvanishing advantage problem. To mitigate these challenges, we propose\nDeepVideo-R1, a video large language model trained with our proposed Reg-GRPO\n(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO\nreformulates the GRPO objective as a regression task, directly predicting the\nadvantage in GRPO. This design eliminates the need for safeguards like clipping\nand min functions, thereby facilitating more direct policy guidance by aligning\nthe model with the advantage values. We also design the difficulty-aware data\naugmentation strategy that dynamically augments training samples at solvable\ndifficulty levels, fostering diverse and informative reward signals. Our\ncomprehensive experiments show that DeepVideo-R1 significantly improves video\nreasoning performance across multiple video reasoning benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86GRPO\u5728\u89c6\u9891LLMs\u4e2d\u7684\u5e94\u7528\u95ee\u9898\uff0c\u5e76\u63d0\u51faReg-GRPO\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76GRPO\u5728\u89c6\u9891LLMs\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u89e3\u51b3\u5176\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faReg-GRPO\uff08\u56de\u5f52\u4efb\u52a1\u5f62\u5f0f\uff09\u548c\u96be\u5ea6\u611f\u77e5\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "DeepVideo-R1\u5728\u591a\u4e2a\u89c6\u9891\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Reg-GRPO\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891LLMs\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.07471", "pdf": "https://arxiv.org/pdf/2506.07471", "abs": "https://arxiv.org/abs/2506.07471", "authors": ["CH Cho", "WJ Moon", "W Jun", "MS Jung", "JP Heo"], "title": "Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to AAAI 2025", "summary": "Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a\nspecific segment is relevant to a given text query. Typical training processes\nof PRVR assume a one-to-one relationship where each text query is relevant to\nonly one video. However, we point out the inherent ambiguity between text and\nvideo content based on their conceptual scope and propose a framework that\nincorporates this ambiguity into the model learning process. Specifically, we\npropose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous\ntext-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:\nuncertainty and similarity. Uncertainty represents whether instances include\ncommonly shared context across the dataset, while similarity indicates\npair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL\nhierarchically learns the semantic relationship via multi-positive contrastive\nlearning and dual triplet margin loss. Additionally, we delve into fine-grained\nrelationships within the video instances. Unlike typical training at the\ntext-video level, where pairwise information is provided, we address the\ninherent ambiguity within frames of the same untrimmed video, which often\ncontains multiple contexts. This allows us to further enhance learning at the\ntext-frame level. Lastly, we propose cross-model ambiguity detection to\nmitigate the error propagation that occurs when a single model is employed to\ndetect ambiguous pairs for its training. With all components combined, our\nproposed method demonstrates its effectiveness in PRVR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARL\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6b63\u5bf9\u6bd4\u5b66\u4e60\u548c\u53cc\u91cd\u4e09\u5143\u7ec4\u8fb9\u9645\u635f\u5931\uff0c\u89e3\u51b3\u6587\u672c-\u89c6\u9891\u5bf9\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5e76\u5728PRVR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edfPRVR\u8bad\u7ec3\u5047\u8bbe\u6587\u672c\u67e5\u8be2\u4e0e\u89c6\u9891\u662f\u4e00\u5bf9\u4e00\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u4e0e\u89c6\u9891\u5185\u5bb9\u4e4b\u95f4\u7684\u6a21\u7cca\u6027\u3002\u672c\u6587\u65e8\u5728\u5c06\u8fd9\u79cd\u6a21\u7cca\u6027\u7eb3\u5165\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faARL\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u548c\u76f8\u4f3c\u6027\u68c0\u6d4b\u6a21\u7cca\u5bf9\uff0c\u91c7\u7528\u591a\u6b63\u5bf9\u6bd4\u5b66\u4e60\u548c\u53cc\u91cd\u4e09\u5143\u7ec4\u8fb9\u9645\u635f\u5931\u8fdb\u884c\u5206\u5c42\u5b66\u4e60\uff0c\u5e76\u63a2\u7d22\u89c6\u9891\u5185\u7ec6\u7c92\u5ea6\u5173\u7cfb\u3002", "result": "ARL\u6846\u67b6\u5728PRVR\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c-\u89c6\u9891\u5bf9\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "conclusion": "ARL\u901a\u8fc7\u68c0\u6d4b\u6a21\u7cca\u5bf9\u548c\u5206\u5c42\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86PRVR\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5355\u6a21\u578b\u68c0\u6d4b\u6a21\u7cca\u5bf9\u65f6\u7684\u8bef\u5dee\u4f20\u64ad\u3002"}}
{"id": "2506.07484", "pdf": "https://arxiv.org/pdf/2506.07484", "abs": "https://arxiv.org/abs/2506.07484", "authors": ["Dasol Hong", "Wooju Lee", "Hyun Myung"], "title": "CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.5.2"], "comment": "8 pages, 5 figures; accepted at ICML 2025", "summary": "Prompt tuning, which adapts vision-language models by freezing model\nparameters and optimizing only the prompt, has proven effective for\ntask-specific adaptations. The core challenge in prompt tuning is improving\nspecialization for a specific task and generalization for unseen domains.\nHowever, frozen encoders often produce misaligned features, leading to\nconfusion between classes and limiting specialization. To overcome this issue,\nwe propose a confusion-aware loss (CoA-loss) that improves specialization by\nrefining the decision boundaries between confusing classes. Additionally, we\nmathematically demonstrate that a mixture model can enhance generalization\nwithout compromising specialization. This is achieved using confidence-aware\nweights (CoA-weights), which adjust the weights of each prediction in the\nmixture model based on its confidence within the class domains. Extensive\nexperiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,\noutperforms state-of-the-art methods by enhancing specialization and\ngeneralization. Our code is publicly available at\nhttps://github.com/url-kaist/CoCoA-Mix.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoCoA-Mix\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u6df7\u6dc6\u611f\u77e5\u635f\u5931\uff08CoA-loss\uff09\u548c\u7f6e\u4fe1\u611f\u77e5\u6743\u91cd\uff08CoA-weights\uff09\uff0c\u4ee5\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u4e2d\u7684\u4e13\u4e1a\u5316\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51bb\u7ed3\u7f16\u7801\u5668\u5e38\u5bfc\u81f4\u7279\u5f81\u4e0d\u5bf9\u9f50\uff0c\u5f15\u53d1\u7c7b\u522b\u6df7\u6dc6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u4e13\u4e1a\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6df7\u6dc6\u611f\u77e5\u635f\u5931\uff08CoA-loss\uff09\u4f18\u5316\u51b3\u7b56\u8fb9\u754c\uff0c\u5e76\u4f7f\u7528\u7f6e\u4fe1\u611f\u77e5\u6743\u91cd\uff08CoA-weights\uff09\u6784\u5efa\u6df7\u5408\u6a21\u578b\u4ee5\u589e\u5f3a\u6cdb\u5316\u3002", "result": "CoCoA-Mix\u5728\u4e13\u4e1a\u5316\u548c\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CoCoA-Mix\u901a\u8fc7\u6539\u8fdb\u51b3\u7b56\u8fb9\u754c\u548c\u6df7\u5408\u6a21\u578b\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.07489", "pdf": "https://arxiv.org/pdf/2506.07489", "abs": "https://arxiv.org/abs/2506.07489", "authors": ["Yahao Shi", "Yang Liu", "Yanmin Wu", "Xing Liu", "Chen Zhao", "Jie Luo", "Bin Zhou"], "title": "Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video", "categories": ["cs.CV"], "comment": "technical report", "summary": "We propose DriveAnyMesh, a method for driving mesh guided by monocular video.\nCurrent 4D generation techniques encounter challenges with modern rendering\nengines. Implicit methods have low rendering efficiency and are unfriendly to\nrasterization-based engines, while skeletal methods demand significant manual\neffort and lack cross-category generalization. Animating existing 3D assets,\ninstead of creating 4D assets from scratch, demands a deep understanding of the\ninput's 3D structure. To tackle these challenges, we present a 4D diffusion\nmodel that denoises sequences of latent sets, which are then decoded to produce\nmesh animations from point cloud trajectory sequences. These latent sets\nleverage a transformer-based variational autoencoder, simultaneously capturing\n3D shape and motion information. By employing a spatiotemporal,\ntransformer-based diffusion model, information is exchanged across multiple\nlatent frames, enhancing the efficiency and generalization of the generated\nresults. Our experimental results demonstrate that DriveAnyMesh can rapidly\nproduce high-quality animations for complex motions and is compatible with\nmodern rendering engines. This method holds potential for applications in both\nthe gaming and filming industries.", "AI": {"tldr": "DriveAnyMesh\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u89c6\u9891\u9a71\u52a8\u7f51\u683c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc74D\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u753b\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u6e32\u67d3\u5f15\u64ce\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d4D\u751f\u6210\u6280\u672f\u5728\u6e32\u67d3\u6548\u7387\u548c\u8de8\u7c7b\u522b\u6cdb\u5316\u4e0a\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u73b0\u67093D\u8d44\u4ea7\u52a8\u753b\u5316\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6355\u83b73D\u5f62\u72b6\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u901a\u8fc7\u65f6\u7a7a\u6269\u6563\u6a21\u578b\u751f\u6210\u7f51\u683c\u52a8\u753b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDriveAnyMesh\u80fd\u5feb\u901f\u751f\u6210\u590d\u6742\u8fd0\u52a8\u7684\u9ad8\u8d28\u91cf\u52a8\u753b\uff0c\u4e14\u517c\u5bb9\u73b0\u4ee3\u6e32\u67d3\u5f15\u64ce\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6e38\u620f\u548c\u7535\u5f71\u884c\u4e1a\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.07491", "pdf": "https://arxiv.org/pdf/2506.07491", "abs": "https://arxiv.org/abs/2506.07491", "authors": ["Yongsen Mao", "Junhao Zhong", "Chuan Fang", "Jia Zheng", "Rui Tang", "Hao Zhu", "Ping Tan", "Zihan Zhou"], "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "categories": ["cs.CV"], "comment": null, "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.", "AI": {"tldr": "SpatialLM\u662f\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u74063D\u70b9\u4e91\u6570\u636e\u5e76\u751f\u6210\u7ed3\u6784\u5316\u76843D\u573a\u666f\u7406\u89e3\u8f93\u51fa\uff0c\u5982\u5899\u58c1\u3001\u95e8\u7a97\u7b49\u5efa\u7b51\u5143\u7d20\u3002", "motivation": "\u63d0\u5347\u73b0\u4ee3LLM\u5728\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u5e94\u7528\u4e8e\u589e\u5f3a\u73b0\u5b9e\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u3002", "method": "\u91c7\u7528\u6807\u51c6\u7684\u591a\u6a21\u6001LLM\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90LLM\u76f4\u63a5\u5fae\u8c03\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e03\u5c40\u4f30\u8ba1\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c3D\u7269\u4f53\u68c0\u6d4b\u7ed3\u679c\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u5c55\u793a\u4e86\u589e\u5f3a\u73b0\u4ee3LLM\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2506.07497", "pdf": "https://arxiv.org/pdf/2506.07497", "abs": "https://arxiv.org/abs/2506.07497", "authors": ["Xiangyu Guo", "Zhanqian Wu", "Kaixin Xiong", "Ziyang Xu", "Lijun Zhou", "Gangwei Xu", "Shaoqing Xu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency", "categories": ["cs.CV"], "comment": null, "summary": "We present Genesis, a unified framework for joint generation of multi-view\ndriving videos and LiDAR sequences with spatio-temporal and cross-modal\nconsistency. Genesis employs a two-stage architecture that integrates a\nDiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR\ngenerator with NeRF-based rendering and adaptive sampling. Both modalities are\ndirectly coupled through a shared latent space, enabling coherent evolution\nacross visual and geometric domains. To guide the generation with structured\nsemantics, we introduce DataCrafter, a captioning module built on\nvision-language models that provides scene-level and instance-level\nsupervision. Extensive experiments on the nuScenes benchmark demonstrate that\nGenesis achieves state-of-the-art performance across video and LiDAR metrics\n(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including\nsegmentation and 3D detection, validating the semantic fidelity and practical\nutility of the generated data.", "AI": {"tldr": "Genesis\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u751f\u6210\u591a\u89c6\u89d2\u9a7e\u9a76\u89c6\u9891\u548cLiDAR\u5e8f\u5217\uff0c\u5177\u6709\u65f6\u7a7a\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u751f\u6210\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u6570\u636e\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff0c\u7ed3\u5408DiT\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0e3D-VAE\u7f16\u7801\uff0c\u4ee5\u53caBEV\u611f\u77e5\u7684LiDAR\u751f\u6210\u5668\u4e0eNeRF\u6e32\u67d3\u3002\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u6a21\u6001\u8026\u5408\uff0c\u5e76\u5f15\u5165DataCrafter\u6a21\u5757\u63d0\u4f9b\u8bed\u4e49\u76d1\u7763\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff08FVD 16.95\uff0cFID 4.24\uff0cChamfer 0.611\uff09\uff0c\u5e76\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "Genesis\u5728\u591a\u6a21\u6001\u6570\u636e\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u5b9e\u7528\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2506.07533", "pdf": "https://arxiv.org/pdf/2506.07533", "abs": "https://arxiv.org/abs/2506.07533", "authors": ["Wei Tao", "Haocheng Lu", "Xiaoyang Qu", "Bin Zhang", "Kai Lu", "Jiguang Wan", "Jianzong Wang"], "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts", "categories": ["cs.CV"], "comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness.", "AI": {"tldr": "MoQAE\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u4e13\u5bb6\u7684\u6df7\u5408\u6765\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684KV\u7f13\u5b58\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u517c\u987e\u6709\u6548\u6027\u548c\u6548\u7387\uff0cMoQAE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u5c06\u4e0d\u540c\u91cf\u5316\u4f4d\u5bbd\u914d\u7f6e\u89c6\u4e3a\u4e13\u5bb6\uff0c\u4f7f\u7528MoE\u65b9\u6cd5\u9009\u62e9\u6700\u4f18\u914d\u7f6e\uff1b2. \u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u5fae\u8c03\u8fc7\u7a0b\uff1b3. \u5f15\u5165\u8def\u7531\u51bb\u7ed3\u548c\u5171\u4eab\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMoQAE\u5728\u6548\u7387\u548c\u6709\u6548\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709KV\u7f13\u5b58\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "MoQAE\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u548c\u4f18\u5316\u8def\u7531\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2506.07539", "pdf": "https://arxiv.org/pdf/2506.07539", "abs": "https://arxiv.org/abs/2506.07539", "authors": ["Xiaomeng Zhu", "Jacob Henningsson", "Duruo Li", "P\u00e4r M\u00e5rtensson", "Lars Hanson", "M\u00e5rten Bj\u00f6rkman", "Atsuto Maki"], "title": "Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study", "categories": ["cs.CV", "cs.AI"], "comment": "This is accepted by 2025 IEEE International Conference on Robotics &\n  Automation (ICRA), waiting for publication. 14 pages, 14 figures", "summary": "This paper addresses key aspects of domain randomization in generating\nsynthetic data for manufacturing object detection applications. To this end, we\npresent a comprehensive data generation pipeline that reflects different\nfactors: object characteristics, background, illumination, camera settings, and\npost-processing. We also introduce the Synthetic Industrial Parts Object\nDetection dataset (SIP15-OD) consisting of 15 objects from three industrial use\ncases under varying environments as a test bed for the study, while also\nemploying an industrial dataset publicly available for robotic applications. In\nour experiments, we present more abundant results and insights into the\nfeasibility as well as challenges of sim-to-real object detection. In\nparticular, we identified material properties, rendering methods,\npost-processing, and distractors as important factors. Our method, leveraging\nthese, achieves top performance on the public dataset with Yolov8 models\ntrained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics\ndataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,\nrespectively. The results showcase the effectiveness of the proposed domain\nrandomization, potentially covering the distribution close to real data for the\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5236\u9020\u4e1a\u76ee\u6807\u68c0\u6d4b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u968f\u673a\u5316\u6280\u672f\u751f\u6210\u591a\u6837\u5316\u7684\u6570\u636e\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5236\u9020\u4e1a\u76ee\u6807\u68c0\u6d4b\u4e2d\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u5173\u952e\u95ee\u9898\uff0c\u4ee5\u8986\u76d6\u771f\u5b9e\u6570\u636e\u7684\u5206\u5e03\u3002", "method": "\u63d0\u51fa\u5168\u9762\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u5305\u62ec\u5bf9\u8c61\u7279\u5f81\u3001\u80cc\u666f\u3001\u5149\u7167\u3001\u76f8\u673a\u8bbe\u7f6e\u548c\u540e\u5904\u7406\uff0c\u5e76\u5f15\u5165SIP15-OD\u6570\u636e\u96c6\u3002", "result": "\u5728Yolov8\u6a21\u578b\u4e0a\uff0c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cmAP@50\u6700\u9ad8\u8fbe99.5%\u3002", "conclusion": "\u9886\u57df\u968f\u673a\u5316\u6280\u672f\u80fd\u6709\u6548\u751f\u6210\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u5206\u5e03\u7684\u5408\u6210\u6570\u636e\uff0c\u9002\u7528\u4e8e\u5236\u9020\u4e1a\u76ee\u6807\u68c0\u6d4b\u3002"}}
{"id": "2506.07542", "pdf": "https://arxiv.org/pdf/2506.07542", "abs": "https://arxiv.org/abs/2506.07542", "authors": ["Bowen Liu", "Weiyi Zhang", "Peranut Chotcomwongse", "Xiaolan Chen", "Ruoyu Chen", "Pawin Pakaymaskul", "Niracha Arjkongharn", "Nattaporn Vongsa", "Xuelian Cheng", "Zongyuan Ge", "Kun Huang", "Xiaohui Li", "Yiru Duan", "Zhenbang Wang", "BaoYe Xie", "Qiang Chen", "Huazhu Fu", "Michael A. Mahr", "Jiaqi Qu", "Wangyiyang Chen", "Shiye Wang", "Yubo Tan", "Yongjie Li", "Mingguang He", "Danli Shi", "Paisan Ruamviboonsuk"], "title": "APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Optical Coherence Tomography (OCT) provides high-resolution, 3D, and\nnon-invasive visualization of retinal layers in vivo, serving as a critical\ntool for lesion localization and disease diagnosis. However, its widespread\nadoption is limited by equipment costs and the need for specialized operators.\nIn comparison, 2D color fundus photography offers faster acquisition and\ngreater accessibility with less dependence on expensive devices. Although\ngenerative artificial intelligence has demonstrated promising results in\nmedical image synthesis, translating 2D fundus images into 3D OCT images\npresents unique challenges due to inherent differences in data dimensionality\nand biological information between modalities. To advance generative models in\nthe fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society\n(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT\nGeneration from Fundus Images. This paper details the challenge framework\n(referred to as APTOS-2024 Challenge), including: the benchmark dataset,\nevaluation methodology featuring two fidelity metrics-image-based distance\n(pixel-level OCT B-scan similarity) and video-based distance (semantic-level\nvolumetric consistency), and analysis of top-performing solutions. The\nchallenge attracted 342 participating teams, with 42 preliminary submissions\nand 9 finalists. Leading methodologies incorporated innovations in hybrid data\npreprocessing or augmentation (cross-modality collaborative paradigms),\npre-training on external ophthalmic imaging datasets, integration of vision\nfoundation models, and model architecture improvement. The APTOS-2024 Challenge\nis the first benchmark demonstrating the feasibility of fundus-to-3D-OCT\nsynthesis as a potential solution for improving ophthalmic care accessibility\nin under-resourced healthcare settings, while helping to expedite medical\nresearch and clinical applications.", "AI": {"tldr": "APTOS-2024\u6311\u6218\u8d5b\u63a2\u7d22\u4e86\u4ece2D\u773c\u5e95\u56fe\u50cf\u751f\u62103D OCT\u56fe\u50cf\u7684\u53ef\u884c\u6027\uff0c\u65e8\u5728\u89e3\u51b3OCT\u8bbe\u5907\u6210\u672c\u9ad8\u548c\u64cd\u4f5c\u590d\u6742\u7684\u95ee\u9898\u3002", "motivation": "OCT\u8bbe\u5907\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u4e13\u4e1a\u64cd\u4f5c\u4eba\u5458\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u30022D\u773c\u5e95\u6444\u5f71\u66f4\u6613\u83b7\u53d6\uff0c\u4f46\u7f3a\u4e4f3D\u4fe1\u606f\u3002\u751f\u6210\u5f0fAI\u6709\u671b\u586b\u8865\u8fd9\u4e00\u6280\u672f\u7a7a\u767d\u3002", "method": "\u6311\u6218\u8d5b\u63d0\u4f9b\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff08\u56fe\u50cf\u548c\u89c6\u9891\u7ea7\u4fdd\u771f\u5ea6\u6307\u6807\uff09\uff0c\u5438\u5f15\u4e86342\u4e2a\u56e2\u961f\u53c2\u4e0e\u3002\u9886\u5148\u65b9\u6cd5\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u9884\u8bad\u7ec3\u3001\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u67b6\u6784\u6539\u8fdb\u3002", "result": "42\u4e2a\u521d\u6b65\u63d0\u4ea4\u548c9\u4e2a\u51b3\u8d5b\u56e2\u961f\u5c55\u793a\u4e86\u4ece\u773c\u5e95\u56fe\u50cf\u751f\u62103D OCT\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "APTOS-2024\u6311\u6218\u8d5b\u9996\u6b21\u9a8c\u8bc1\u4e86\u773c\u5e95\u52303D OCT\u5408\u6210\u7684\u53ef\u884c\u6027\uff0c\u6709\u671b\u63d0\u5347\u773c\u79d1\u533b\u7597\u7684\u53ef\u53ca\u6027\u5e76\u52a0\u901f\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2506.07555", "pdf": "https://arxiv.org/pdf/2506.07555", "abs": "https://arxiv.org/abs/2506.07555", "authors": ["Haoxiang Wang", "Zinan Lin", "Da Yu", "Huishuai Zhang"], "title": "Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generating high fidelity, differentially private (DP) synthetic images offers\na promising route to share and analyze sensitive visual data without\ncompromising individual privacy. However, existing DP image synthesis methods\nstruggle to produce high resolution outputs that faithfully capture the\nstructure of the original data. In this paper, we introduce a novel method,\nreferred to as Synthesis via Private Textual Intermediaries (SPTI), that can\ngenerate high resolution DP images with easy adoption. The key idea is to shift\nthe challenge of DP image synthesis from the image domain to the text domain by\nleveraging state of the art DP text generation methods. SPTI first summarizes\neach private image into a concise textual description using image to text\nmodels, then applies a modified Private Evolution algorithm to generate DP\ntext, and finally reconstructs images using text to image models. Notably, SPTI\nrequires no model training, only inference with off the shelf models. Given a\nprivate dataset, SPTI produces synthetic images of substantially higher quality\nthan prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less\nthan or equal to 26.71 under epsilon equal to 1.0, improving over Private\nEvolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less\nthan or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine\ntuning baselines. Overall, our results demonstrate that Synthesis via Private\nTextual Intermediaries provides a resource efficient and proprietary model\ncompatible framework for generating high resolution DP synthetic images,\ngreatly expanding access to private visual datasets.", "AI": {"tldr": "SPTI\u662f\u4e00\u79cd\u901a\u8fc7\u6587\u672c\u4e2d\u4ecb\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u5408\u6210\u56fe\u50cf\u7684\u65b0\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709DP\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709DP\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u4e14\u5fe0\u5b9e\u4e8e\u539f\u59cb\u6570\u636e\u7684\u56fe\u50cf\uff0cSPTI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SPTI\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u6587\u672c\u63cf\u8ff0\uff0c\u5229\u7528\u6539\u8fdb\u7684Private Evolution\u7b97\u6cd5\u751f\u6210DP\u6587\u672c\uff0c\u518d\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u91cd\u5efa\u56fe\u50cf\u3002", "result": "\u5728LSUN Bedroom\u548cMM CelebA HQ\u6570\u636e\u96c6\u4e0a\uff0cSPTI\u7684FID\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SPTI\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u4e14\u517c\u5bb9\u4e13\u6709\u6a21\u578b\u7684\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u5bf9\u79c1\u6709\u89c6\u89c9\u6570\u636e\u7684\u8bbf\u95ee\u3002"}}
{"id": "2506.07559", "pdf": "https://arxiv.org/pdf/2506.07559", "abs": "https://arxiv.org/abs/2506.07559", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u901a\u9053\u611f\u77e5\u5b66\u4e60\uff08CCPL\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3H&E\u5230IHC\u67d3\u8272\u4e2d\u7ec6\u80de\u6838\u4e0e\u7ec6\u80de\u819c\u8de8\u901a\u9053\u76f8\u5173\u6027\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u7279\u5f81\u63d0\u53d6\u548c\u7279\u5f81\u84b8\u998f\u635f\u5931\u63d0\u5347\u865a\u62df\u67d3\u8272\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709H&E\u5230IHC\u7814\u7a76\u5e38\u5ffd\u7565\u7ec6\u80de\u6838\u4e0e\u7ec6\u80de\u819c\u7684\u8de8\u901a\u9053\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u75c5\u7406\u56fe\u50cf\u5206\u6790\u4e0e\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "method": "CCPL\u5c06HER2\u514d\u75ab\u7ec4\u5316\u67d3\u8272\u5206\u89e3\u4e3aHematoxylin\u548cDAB\u67d3\u8272\u901a\u9053\uff0c\u5229\u7528Gigapath\u7684Tile Encoder\u63d0\u53d6\u53cc\u901a\u9053\u7279\u5f81\u5e76\u8ba1\u7b97\u8de8\u901a\u9053\u76f8\u5173\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u7279\u5f81\u84b8\u998f\u635f\u5931\u548c\u5149\u5b66\u5bc6\u5ea6\u7edf\u8ba1\u5206\u6790\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCCPL\u5728PSNR\u3001SSIM\u3001PCC\u548cFID\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u75c5\u7406\u5b66\u5bb6\u8bc4\u4f30\u786e\u8ba4\u5176\u80fd\u6709\u6548\u4fdd\u7559\u75c5\u7406\u7279\u5f81\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u865a\u62df\u67d3\u8272\u56fe\u50cf\u3002", "conclusion": "CCPL\u4e3a\u591a\u5a92\u4f53\u533b\u5b66\u6570\u636e\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u75c5\u7406\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2506.07565", "pdf": "https://arxiv.org/pdf/2506.07565", "abs": "https://arxiv.org/abs/2506.07565", "authors": ["Jinlu Zhang", "Zixi Kang", "Yizhou Wang"], "title": "OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data", "categories": ["cs.CV"], "comment": null, "summary": "Music-driven dance generation offers significant creative potential yet faces\nconsiderable challenges. The absence of fine-grained multimodal data and the\ndifficulty of flexible multi-conditional generation limit previous works on\ngeneration controllability and diversity in practice. In this paper, we build\nOpenDance5D, an extensive human dance dataset comprising over 101 hours across\n14 distinct genres. Each sample has five modalities to facilitate robust\ncross-modal learning: RGB video, audio, 2D keypoints, 3D motion, and\nfine-grained textual descriptions from human arts. Furthermore, we propose\nOpenDanceNet, a unified masked modeling framework for controllable dance\ngeneration conditioned on music and arbitrary combinations of text prompts,\nkeypoints, or character positioning. Comprehensive experiments demonstrate that\nOpenDanceNet achieves high-fidelity and flexible controllability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86OpenDance5D\u6570\u636e\u96c6\u548cOpenDanceNet\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u97f3\u4e50\u9a71\u52a8\u821e\u8e48\u751f\u6210\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\u4e0d\u8db3\u548c\u53ef\u63a7\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u821e\u8e48\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u53ef\u63a7\u6027\u4e0a\u53d7\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u6570\u636e\u548c\u7075\u6d3b\u7684\u751f\u6210\u6761\u4ef6\u3002", "method": "\u6784\u5efaOpenDance5D\u6570\u636e\u96c6\uff0814\u79cd\u821e\u8e48\u7c7b\u578b\uff0c101\u5c0f\u65f6\u6570\u636e\uff0c5\u79cd\u6a21\u6001\uff09\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u63a9\u7801\u5efa\u6a21\u7684\u7edf\u4e00\u6846\u67b6OpenDanceNet\u3002", "result": "OpenDanceNet\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u548c\u7075\u6d3b\u53ef\u63a7\u7684\u821e\u8e48\u751f\u6210\u3002", "conclusion": "OpenDance5D\u548cOpenDanceNet\u4e3a\u97f3\u4e50\u9a71\u52a8\u821e\u8e48\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u6570\u636e\u548c\u6a21\u578b\u652f\u6301\u3002"}}
{"id": "2506.07566", "pdf": "https://arxiv.org/pdf/2506.07566", "abs": "https://arxiv.org/abs/2506.07566", "authors": ["Marco Peer", "Robert Sablatnig", "Florian Kleber"], "title": "Towards the Influence of Text Quantity on Writer Retrieval", "categories": ["cs.CV"], "comment": "accepted for ICDAR2025", "summary": "This paper investigates the task of writer retrieval, which identifies\ndocuments authored by the same individual within a dataset based on handwriting\nsimilarities. While existing datasets and methodologies primarily focus on page\nlevel retrieval, we explore the impact of text quantity on writer retrieval\nperformance by evaluating line- and word level retrieval. We examine three\nstate-of-the-art writer retrieval systems, including both handcrafted and deep\nlearning-based approaches, and analyze their performance using varying amounts\nof text. Our experiments on the CVL and IAM dataset demonstrate that while\nperformance decreases by 20-30% when only one line of text is used as query and\ngallery, retrieval accuracy remains above 90% of full-page performance when at\nleast four lines are included. We further show that text-dependent retrieval\ncan maintain strong performance in low-text scenarios. Our findings also\nhighlight the limitations of handcrafted features in low-text scenarios, with\ndeep learning-based methods like NetVLAD outperforming traditional VLAD\nencoding.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u624b\u5199\u76f8\u4f3c\u6027\u7684\u4f5c\u8005\u68c0\u7d22\u4efb\u52a1\uff0c\u63a2\u8ba8\u4e86\u6587\u672c\u91cf\u5bf9\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f7f\u7528\u81f3\u5c11\u56db\u884c\u6587\u672c\u65f6\u6027\u80fd\u63a5\u8fd1\u5168\u9875\u6c34\u5e73\uff0c\u4e14\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u4f4e\u6587\u672c\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9875\u9762\u7ea7\u68c0\u7d22\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6587\u672c\u91cf\uff08\u884c\u7ea7\u548c\u8bcd\u7ea7\uff09\u5bf9\u4f5c\u8005\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u5148\u8fdb\u7684\u4f5c\u8005\u68c0\u7d22\u7cfb\u7edf\uff08\u5305\u62ec\u624b\u5de5\u7279\u5f81\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff09\uff0c\u5728CVL\u548cIAM\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u6587\u672c\u91cf\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u4e00\u884c\u6587\u672c\u65f6\u6027\u80fd\u4e0b\u964d20-30%\uff0c\u4f46\u56db\u884c\u6587\u672c\u65f6\u6027\u80fd\u53ef\u8fbe\u5168\u9875\u768490%\u4ee5\u4e0a\uff1b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u4f4e\u6587\u672c\u573a\u666f\u4e2d\u4f18\u4e8e\u624b\u5de5\u7279\u5f81\u3002", "conclusion": "\u6587\u672c\u91cf\u5bf9\u4f5c\u8005\u68c0\u7d22\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u4f4e\u6587\u672c\u573a\u666f\u4e2d\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2506.07570", "pdf": "https://arxiv.org/pdf/2506.07570", "abs": "https://arxiv.org/abs/2506.07570", "authors": ["Yixuan Yang", "Zhen Luo", "Tongsheng Ding", "Junru Lu", "Mingqi Gao", "Jinyu Yang", "Victor Sanchez", "Feng Zheng"], "title": "LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automatic indoor layout generation has attracted increasing attention due to\nits potential in interior design, virtual environment construction, and\nembodied AI. Existing methods fall into two categories: prompt-driven\napproaches that leverage proprietary LLM services (e.g., GPT APIs) and\nlearning-based methods trained on layout data upon diffusion-based models.\nPrompt-driven methods often suffer from spatial inconsistency and high\ncomputational costs, while learning-based methods are typically constrained by\ncoarse relational graphs and limited datasets, restricting their generalization\nto diverse room categories. In this paper, we revisit LLM-based indoor layout\ngeneration and present 3D-SynthPlace, a large-scale dataset that combines\nsynthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,\nupgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000\nscenes, covering four common room types -- bedroom, living room, kitchen, and\nbathroom -- enriched with diverse objects and high-level spatial annotations.\nWe further introduce OptiScene, a strong open-source LLM optimized for indoor\nlayout generation, fine-tuned based on our 3D-SynthPlace dataset through our\ntwo-stage training. For the warum-up stage I, we adopt supervised fine-tuning\n(SFT), which is taught to first generate high-level spatial descriptions then\nconditionally predict concrete object placements. For the reinforcing stage II,\nto better align the generated layouts with human design preferences, we apply\nmulti-turn direct preference optimization (DPO), which significantly improving\nlayout quality and generation success rates. Extensive experiments demonstrate\nthat OptiScene outperforms traditional prompt-driven and learning-based\nbaselines. Moreover, OptiScene shows promising potential in interactive tasks\nsuch as scene editing and robot navigation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u5ba4\u5185\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c63D-SynthPlace\u548c\u4f18\u5316\u6a21\u578bOptiScene\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u95f4\u4e0d\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5ba4\u5185\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u4e0d\u4e00\u81f4\u6027\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u6216\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7LLM\u4f18\u5316\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "method": "\u6784\u5efa3D-SynthPlace\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u591a\u8f6e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff09\u4f18\u5316LLM\u6a21\u578bOptiScene\u3002", "result": "OptiScene\u5728\u5e03\u5c40\u751f\u6210\u8d28\u91cf\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u573a\u666f\u7f16\u8f91\u548c\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "3D-SynthPlace\u548cOptiScene\u4e3a\u5ba4\u5185\u5e03\u5c40\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07572", "pdf": "https://arxiv.org/pdf/2506.07572", "abs": "https://arxiv.org/abs/2506.07572", "authors": ["Yu Li", "Feng Xue", "Shujie Li", "Jinrui Zhang", "Shuang Yang", "Dan Guo", "Richang Hong"], "title": "Learning Speaker-Invariant Visual Features for Lipreading", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Lipreading is a challenging cross-modal task that aims to convert visual lip\nmovements into spoken text. Existing lipreading methods often extract visual\nfeatures that include speaker-specific lip attributes (e.g., shape, color,\ntexture), which introduce spurious correlations between vision and text. These\ncorrelations lead to suboptimal lipreading accuracy and restrict model\ngeneralization. To address this challenge, we introduce SIFLip, a\nspeaker-invariant visual feature learning framework that disentangles\nspeaker-specific attributes using two complementary disentanglement modules\n(Implicit Disentanglement and Explicit Disentanglement) to improve\ngeneralization. Specifically, since different speakers exhibit semantic\nconsistency between lip movements and phonetic text when pronouncing the same\nwords, our implicit disentanglement module leverages stable text embeddings as\nsupervisory signals to learn common visual representations across speakers,\nimplicitly decoupling speaker-specific features. Additionally, we design a\nspeaker recognition sub-task within the main lipreading pipeline to filter\nspeaker-specific features, then further explicitly disentangle these\npersonalized visual features from the backbone network via gradient reversal.\nExperimental results demonstrate that SIFLip significantly enhances\ngeneralization performance across multiple public datasets. Experimental\nresults demonstrate that SIFLip significantly improves generalization\nperformance across multiple public datasets, outperforming state-of-the-art\nmethods.", "AI": {"tldr": "SIFLip\u662f\u4e00\u79cd\u901a\u8fc7\u89e3\u8026\u8bf4\u8bdd\u8005\u7279\u5b9a\u7279\u5f81\u6765\u63d0\u9ad8\u5507\u8bfb\u6cdb\u5316\u6027\u80fd\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u9690\u5f0f\u548c\u663e\u5f0f\u89e3\u8026\u6a21\u5757\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5507\u8bfb\u65b9\u6cd5\u63d0\u53d6\u7684\u89c6\u89c9\u7279\u5f81\u5305\u542b\u8bf4\u8bdd\u8005\u7279\u5b9a\u5c5e\u6027\uff08\u5982\u5f62\u72b6\u3001\u989c\u8272\u3001\u7eb9\u7406\uff09\uff0c\u5bfc\u81f4\u865a\u5047\u76f8\u5173\u6027\uff0c\u5f71\u54cd\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "SIFLip\u901a\u8fc7\u9690\u5f0f\u89e3\u8026\u6a21\u5757\uff08\u5229\u7528\u7a33\u5b9a\u6587\u672c\u5d4c\u5165\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff09\u548c\u663e\u5f0f\u89e3\u8026\u6a21\u5757\uff08\u901a\u8fc7\u68af\u5ea6\u53cd\u8f6c\u8fc7\u6ee4\u8bf4\u8bdd\u8005\u7279\u5b9a\u7279\u5f81\uff09\u89e3\u8026\u8bf4\u8bdd\u8005\u7279\u5b9a\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSIFLip\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SIFLip\u901a\u8fc7\u89e3\u8026\u8bf4\u8bdd\u8005\u7279\u5b9a\u7279\u5f81\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5507\u8bfb\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.07575", "pdf": "https://arxiv.org/pdf/2506.07575", "abs": "https://arxiv.org/abs/2506.07575", "authors": ["Ruiyang Zhang", "Hu Zhang", "Hao Fei", "Zhedong Zheng"], "title": "Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://uncertainty-o.github.io/", "summary": "Large Multimodal Models (LMMs), harnessing the complementarity among diverse\nmodalities, are often considered more robust than pure Language Large Models\n(LLMs); yet do LMMs know what they do not know? There are three key open\nquestions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a\nunified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to\nquantify uncertainty for downstream tasks. In an attempt to address these\nchallenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed\nto reveal uncertainty in LMMs regardless of their modalities, architectures, or\ncapabilities, (2) an empirical exploration of multimodal prompt perturbations\nto uncover LMM uncertainty, offering insights and findings, and (3) derive the\nformulation of multimodal semantic uncertainty, which enables quantifying\nuncertainty from multimodal responses. Experiments across 18 benchmarks\nspanning various modalities and 10 LMMs (both open- and closed-source)\ndemonstrate the effectiveness of Uncertainty-o in reliably estimating LMM\nuncertainty, thereby enhancing downstream tasks such as hallucination\ndetection, hallucination mitigation, and uncertainty-aware Chain-of-Thought\nreasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faUncertainty-o\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u91cf\u5316\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u867d\u88ab\u8ba4\u4e3a\u6bd4\u7eaf\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u66f4\u9c81\u68d2\uff0c\u4f46\u5176\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u5f15\u5165Uncertainty-o\u6846\u67b6\uff0c\u5305\u62ec\u6a21\u578b\u65e0\u5173\u7684\u4e0d\u786e\u5b9a\u6027\u63ed\u793a\u65b9\u6cd5\u3001\u591a\u6a21\u6001\u63d0\u793a\u6270\u52a8\u5b9e\u9a8c\u53ca\u591a\u6a21\u6001\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u572818\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c10\u79cdLMMs\u4e0a\u9a8c\u8bc1\u4e86Uncertainty-o\u7684\u53ef\u9760\u6027\uff0c\u63d0\u5347\u4e86\u5e7b\u89c9\u68c0\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "Uncertainty-o\u80fd\u6709\u6548\u8bc4\u4f30LMMs\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.07576", "pdf": "https://arxiv.org/pdf/2506.07576", "abs": "https://arxiv.org/abs/2506.07576", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8d85\u7ea7\u7f16\u7801\u7f51\u7edc\uff08SEN\uff09\uff0c\u901a\u8fc7\u9012\u5f52\u5173\u8054\u591a\u6a21\u6001\u7f16\u7801\u5668\uff0c\u63d0\u5347\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4ec5\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u4e0d\u540c\u6a21\u6001\u7684\u7f16\u7801\u5668\uff0c\u7f3a\u4e4f\u66f4\u6df1\u5c42\u6b21\u7684\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u800c\u8fd9\u5bf9\u7406\u89e3\u590d\u6742\u89c6\u9891\u573a\u666f\u81f3\u5173\u91cd\u8981\u3002", "method": "SEN\u901a\u8fc7\u9012\u5f52\u5173\u8054\u5757\uff08RA\uff09\u9010\u6b65\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5c06\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u89c6\u4e3a\u201c\u8d85\u7ea7\u795e\u7ecf\u5143\u201d\uff0c\u5b9e\u73b0\u77e5\u8bc6\u6574\u5408\u3001\u5206\u53d1\u548c\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSEN\u663e\u8457\u63d0\u5347\u4e86\u56db\u79cd\u4ee3\u8868\u6027\u89c6\u9891\u4efb\u52a1\uff08\u8ddf\u8e2a\u3001\u8bc6\u522b\u3001\u804a\u5929\u3001\u7f16\u8f91\uff09\u7684\u8868\u73b0\uff0c\u4f8b\u5982\u8ddf\u8e2a\u4efb\u52a1\u7684Jaccard\u6307\u6570\u63d0\u9ad82.7%\uff0c\u7f16\u8f91\u4efb\u52a1\u7684\u6587\u672c\u5bf9\u9f50\u63d0\u53476.4%\u3002", "conclusion": "SEN\u901a\u8fc7\u6df1\u5c42\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2506.07590", "pdf": "https://arxiv.org/pdf/2506.07590", "abs": "https://arxiv.org/abs/2506.07590", "authors": ["Jiacheng Shi", "Yanfu Zhang", "Huajie Shao", "Ashley Gao"], "title": "Explore the vulnerability of black-box models via diffusion models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in diffusion models have enabled high-fidelity and\nphotorealistic image generation across diverse applications. However, these\nmodels also present security and privacy risks, including copyright violations,\nsensitive information leakage, and the creation of harmful or offensive content\nthat could be exploited maliciously. In this study, we uncover a novel security\nthreat where an attacker leverages diffusion model APIs to generate synthetic\nimages, which are then used to train a high-performing substitute model. This\nenables the attacker to execute model extraction and transfer-based adversarial\nattacks on black-box classification models with minimal queries, without\nneeding access to the original training data. The generated images are\nsufficiently high-resolution and diverse to train a substitute model whose\noutputs closely match those of the target model. Across the seven benchmarks,\nincluding CIFAR and ImageNet subsets, our method shows an average improvement\nof 27.37% over state-of-the-art methods while using just 0.01 times of the\nquery budget, achieving a 98.68% success rate in adversarial attacks on the\ntarget model.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u5b89\u5168\u5a01\u80c1\uff0c\u653b\u51fb\u8005\u5229\u7528\u6269\u6563\u6a21\u578bAPI\u751f\u6210\u5408\u6210\u56fe\u50cf\u8bad\u7ec3\u66ff\u4ee3\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u63d0\u53d6\u548c\u5bf9\u6297\u653b\u51fb\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u56fe\u50cf\u751f\u6210\u80fd\u529b\u53ef\u80fd\u88ab\u6076\u610f\u5229\u7528\uff0c\u5bfc\u81f4\u5b89\u5168\u9690\u79c1\u98ce\u9669\uff0c\u5982\u7248\u6743\u4fb5\u72af\u548c\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u3002", "method": "\u901a\u8fc7\u6269\u6563\u6a21\u578bAPI\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\uff0c\u7528\u4e8e\u8bad\u7ec3\u66ff\u4ee3\u6a21\u578b\uff0c\u4ee5\u6700\u5c0f\u67e5\u8be2\u91cf\u5b9e\u73b0\u6a21\u578b\u63d0\u53d6\u548c\u5bf9\u6297\u653b\u51fb\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u5e73\u5747\u63d0\u534727.37%\uff0c\u4ec5\u75280.01\u500d\u67e5\u8be2\u9884\u7b97\uff0c\u5bf9\u6297\u653b\u51fb\u6210\u529f\u7387\u8fbe98.68%\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578bAPI\u7684\u5b89\u5168\u9690\u60a3\uff0c\u9700\u52a0\u5f3a\u9632\u8303\u63aa\u65bd\u4ee5\u5e94\u5bf9\u6f5c\u5728\u5a01\u80c1\u3002"}}
{"id": "2506.07600", "pdf": "https://arxiv.org/pdf/2506.07600", "abs": "https://arxiv.org/abs/2506.07600", "authors": ["Nianbo Zeng", "Haowen Hou", "Fei Richard Yu", "Si Shi", "Ying Tiffany He"], "title": "SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite recent advances in retrieval-augmented generation (RAG) for video\nunderstanding, effectively understanding long-form video content remains\nunderexplored due to the vast scale and high complexity of video data. Current\nRAG approaches typically segment videos into fixed-length chunks, which often\ndisrupts the continuity of contextual information and fails to capture\nauthentic scene boundaries. Inspired by the human ability to naturally organize\ncontinuous experiences into coherent scenes, we present SceneRAG, a unified\nframework that leverages large language models to segment videos into\nnarrative-consistent scenes by processing ASR transcripts alongside temporal\nmetadata. SceneRAG further sharpens these initial boundaries through\nlightweight heuristics and iterative correction. For each scene, the framework\nfuses information from both visual and textual modalities to extract entity\nrelations and dynamically builds a knowledge graph, enabling robust multi-hop\nretrieval and generation that account for long-range dependencies. Experiments\non the LongerVideos benchmark, featuring over 134 hours of diverse content,\nconfirm that SceneRAG substantially outperforms prior baselines, achieving a\nwin rate of up to 72.5 percent on generation tasks.", "AI": {"tldr": "SceneRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5904\u7406\u89c6\u9891\u7684ASR\u8f6c\u5f55\u548c\u65f6\u95f4\u5143\u6570\u636e\uff0c\u5c06\u89c6\u9891\u5206\u5272\u4e3a\u53d9\u4e8b\u4e00\u81f4\u7684\u573a\u666f\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4fe1\u606f\u6784\u5efa\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u901a\u5e38\u5c06\u89c6\u9891\u5206\u5272\u4e3a\u56fa\u5b9a\u957f\u5ea6\u7684\u7247\u6bb5\uff0c\u7834\u574f\u4e86\u4e0a\u4e0b\u6587\u8fde\u7eed\u6027\u4e14\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u573a\u666f\u8fb9\u754c\u3002\u53d7\u4eba\u7c7b\u81ea\u7136\u7ec4\u7ec7\u8fde\u7eed\u7ecf\u9a8c\u4e3a\u8fde\u8d2f\u573a\u666f\u7684\u80fd\u529b\u542f\u53d1\uff0c\u7814\u7a76\u63d0\u51fa\u4e86SceneRAG\u3002", "method": "SceneRAG\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406ASR\u8f6c\u5f55\u548c\u65f6\u95f4\u5143\u6570\u636e\uff0c\u5206\u5272\u89c6\u9891\u4e3a\u53d9\u4e8b\u4e00\u81f4\u7684\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u542f\u53d1\u5f0f\u548c\u8fed\u4ee3\u6821\u6b63\u4f18\u5316\u8fb9\u754c\u3002\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4fe1\u606f\u6784\u5efa\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u591a\u8df3\u68c0\u7d22\u548c\u751f\u6210\u3002", "result": "\u5728\u5305\u542b134\u5c0f\u65f6\u591a\u6837\u5316\u5185\u5bb9\u7684LongerVideos\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSceneRAG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u751f\u6210\u4efb\u52a1\u7684\u80dc\u7387\u9ad8\u8fbe72.5%\u3002", "conclusion": "SceneRAG\u901a\u8fc7\u53d9\u4e8b\u4e00\u81f4\u7684\u5206\u5272\u548c\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u6311\u6218\uff0c\u4e3a\u590d\u6742\u89c6\u9891\u5185\u5bb9\u7684\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.07603", "pdf": "https://arxiv.org/pdf/2506.07603", "abs": "https://arxiv.org/abs/2506.07603", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "AI": {"tldr": "SurgBench\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u624b\u672f\u89c6\u9891\u57fa\u51c6\u6846\u67b6\uff0c\u5305\u542b\u9884\u8bad\u7ec3\u6570\u636e\u96c6SurgBench-P\u548c\u8bc4\u4f30\u57fa\u51c6SurgBench-E\uff0c\u65e8\u5728\u89e3\u51b3\u624b\u672f\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u4e2d\u7684\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u624b\u672f\u89c6\u9891\u7406\u89e3\u5bf9\u81ea\u52a8\u5316\u672f\u4e2d\u51b3\u7b56\u3001\u6280\u80fd\u8bc4\u4f30\u548c\u672f\u540e\u8d28\u91cf\u6539\u8fdb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u578b\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u963b\u788d\u4e86\u624b\u672f\u89c6\u9891\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faSurgBench\u6846\u67b6\uff0c\u5305\u542b\u9884\u8bad\u7ec3\u6570\u636e\u96c6SurgBench-P\uff08\u6db5\u76d622\u79cd\u624b\u672f\u548c11\u4e2a\u4e13\u79d1\u76845300\u4e07\u5e27\uff09\u548c\u8bc4\u4f30\u57fa\u51c6SurgBench-E\uff08\u8986\u76d672\u4e2a\u7ec6\u7c92\u5ea6\u4efb\u52a1\u76846\u4e2a\u7c7b\u522b\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u89c6\u9891\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u624b\u672f\u89c6\u9891\u4efb\u52a1\uff0c\u800c\u57fa\u4e8eSurgBench-P\u7684\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u589e\u5f3a\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SurgBench\u4e3a\u624b\u672f\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u63a8\u52a8\u4e86\u624b\u672f\u89c6\u9891\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.07611", "pdf": "https://arxiv.org/pdf/2506.07611", "abs": "https://arxiv.org/abs/2506.07611", "authors": ["Yuan Zhou", "Junbao Zhou", "Qingshan Xu", "Kesen Zhao", "Yuxuan Wang", "Hao Fei", "Richang Hong", "Hanwang Zhang"], "title": "DragNeXt: Rethinking Drag-Based Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Drag-Based Image Editing (DBIE), which allows users to manipulate images by\ndirectly dragging objects within them, has recently attracted much attention\nfrom the community. However, it faces two key challenges:\n(\\emph{\\textcolor{magenta}{i}}) point-based drag is often highly ambiguous and\ndifficult to align with users' intentions; (\\emph{\\textcolor{magenta}{ii}})\ncurrent DBIE methods primarily rely on alternating between motion supervision\nand point tracking, which is not only cumbersome but also fails to produce\nhigh-quality results. These limitations motivate us to explore DBIE from a new\nperspective -- redefining it as deformation, rotation, and translation of\nuser-specified handle regions. Thereby, by requiring users to explicitly\nspecify both drag areas and types, we can effectively address the ambiguity\nissue. Furthermore, we propose a simple-yet-effective editing framework, dubbed\n\\textcolor{SkyBlue}{\\textbf{DragNeXt}}. It unifies DBIE as a Latent Region\nOptimization (LRO) problem and solves it through Progressive Backward\nSelf-Intervention (PBSI), simplifying the overall procedure of DBIE while\nfurther enhancing quality by fully leveraging region-level structure\ninformation and progressive guidance from intermediate drag states. We validate\n\\textcolor{SkyBlue}{\\textbf{DragNeXt}} on our NextBench, and extensive\nexperiments demonstrate that our proposed method can significantly outperform\nexisting approaches. Code will be released on github.", "AI": {"tldr": "DragNeXt\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u62d6\u62fd\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u660e\u786e\u6307\u5b9a\u62d6\u62fd\u533a\u57df\u548c\u7c7b\u578b\u89e3\u51b3\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5e76\u7b80\u5316\u4e86\u7f16\u8f91\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u62d6\u62fd\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u6a21\u7cca\u6027\u548c\u590d\u6742\u6027\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002", "method": "\u5c06\u62d6\u62fd\u7f16\u8f91\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53d8\u5f62\u3001\u65cb\u8f6c\u548c\u5e73\u79fb\uff0c\u5e76\u63d0\u51faLatent Region Optimization\uff08LRO\uff09\u548cProgressive Backward Self-Intervention\uff08PBSI\uff09\u6846\u67b6\u3002", "result": "\u5728NextBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDragNeXt\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DragNeXt\u901a\u8fc7\u7b80\u5316\u6d41\u7a0b\u548c\u63d0\u5347\u8d28\u91cf\uff0c\u4e3a\u57fa\u4e8e\u62d6\u62fd\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07612", "pdf": "https://arxiv.org/pdf/2506.07612", "abs": "https://arxiv.org/abs/2506.07612", "authors": ["Zikang Leng", "Archith Iyer", "Thomas Pl\u00f6tz"], "title": "Scaling Human Activity Recognition: A Comparative Evaluation of Synthetic Data Generation and Augmentation Techniques", "categories": ["cs.CV"], "comment": null, "summary": "Human activity recognition (HAR) is often limited by the scarcity of labeled\ndatasets due to the high cost and complexity of real-world data collection. To\nmitigate this, recent work has explored generating virtual inertial measurement\nunit (IMU) data via cross-modality transfer. While video-based and\nlanguage-based pipelines have each shown promise, they differ in assumptions\nand computational cost. Moreover, their effectiveness relative to traditional\nsensor-level data augmentation remains unclear. In this paper, we present a\ndirect comparison between these two virtual IMU generation approaches against\nclassical data augmentation techniques. We construct a large-scale virtual IMU\ndataset spanning 100 diverse activities from Kinetics-400 and simulate sensor\nsignals at 22 body locations. The three data generation strategies are\nevaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four\npopular models. Results show that virtual IMU data significantly improves\nperformance over real or augmented data alone, particularly under limited-data\nconditions. We offer practical guidance on choosing data generation strategies\nand highlight the distinct advantages and disadvantages of each approach.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u865a\u62dfIMU\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4e0e\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u53d1\u73b0\u865a\u62dfIMU\u6570\u636e\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u4e2d\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u8de8\u6a21\u6001\u751f\u6210\u865a\u62dfIMU\u6570\u636e\u7684\u6709\u6548\u6027\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u865a\u62dfIMU\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u89c6\u9891\u548c\u8bed\u8a00\u4e24\u79cd\u751f\u6210\u65b9\u6cd5\u4e0e\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u865a\u62dfIMU\u6570\u636e\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u771f\u5b9e\u6216\u589e\u5f3a\u6570\u636e\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u65f6\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u9009\u62e9\u6570\u636e\u751f\u6210\u7b56\u7565\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u5e76\u5206\u6790\u4e86\u5404\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002"}}
{"id": "2506.07627", "pdf": "https://arxiv.org/pdf/2506.07627", "abs": "https://arxiv.org/abs/2506.07627", "authors": ["Haotong Qin", "Cheng Hu", "Michele Magno"], "title": "Event-Priori-Based Vision-Language Model for Efficient Visual Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Large Language Model (LLM)-based Vision-Language Models (VLMs) have\nsubstantially extended the boundaries of visual understanding capabilities.\nHowever, their high computational demands hinder deployment on\nresource-constrained edge devices. A key source of inefficiency stems from the\nVLM's need to process dense and redundant visual information. Visual inputs\ncontain significant regions irrelevant to text semantics, rendering the\nassociated computations ineffective for inference. This paper introduces a\nnovel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core\ncontribution is a novel mechanism leveraging motion priors derived from dynamic\nevent vision to enhance VLM efficiency. Inspired by human visual cognition,\nEP-VLM first employs event data to guide the patch-wise sparsification of RGB\nvisual inputs, progressively concentrating VLM computation on salient regions\nof the visual input. Subsequently, we construct a position-preserving\ntokenization strategy for the visual encoder within the VLM architecture. This\nstrategy processes the event-guided, unstructured, sparse visual input while\naccurately preserving positional understanding within the visual input.\nExperimental results demonstrate that EP-VLM achieves significant efficiency\nimprovements while maintaining nearly lossless accuracy compared to baseline\nmodels from the Qwen2-VL series. For instance, against the original\nQwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the\noriginal accuracy on the RealWorldQA dataset. This work demonstrates the\npotential of event-based vision priors for improving VLM inference efficiency,\npaving the way for creating more efficient and deployable VLMs for sustainable\nvisual understanding at the edge.", "AI": {"tldr": "EP-VLM\u662f\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u5148\u9a8c\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u4e8b\u4ef6\u89c6\u89c9\u5f15\u5bfc\u7a00\u758f\u5316\u89c6\u89c9\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\uff0c\u4e14\u89c6\u89c9\u8f93\u5165\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u4fe1\u606f\u3002", "method": "EP-VLM\u5229\u7528\u52a8\u6001\u4e8b\u4ef6\u89c6\u89c9\u7684\u8fd0\u52a8\u5148\u9a8c\uff0c\u7a00\u758f\u5316RGB\u89c6\u89c9\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u4f4d\u7f6e\u4fdd\u7559\u7684\u6807\u8bb0\u5316\u7b56\u7565\u5904\u7406\u7a00\u758f\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEP-VLM\u5728Qwen2-VL\u7cfb\u5217\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8650%\u7684\u8ba1\u7b97\u91cf\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u630198%\u7684\u539f\u59cb\u7cbe\u5ea6\u3002", "conclusion": "\u4e8b\u4ef6\u89c6\u89c9\u5148\u9a8c\u53ef\u663e\u8457\u63d0\u5347VLM\u6548\u7387\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u53ef\u6301\u7eed\u89c6\u89c9\u7406\u89e3\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2506.07628", "pdf": "https://arxiv.org/pdf/2506.07628", "abs": "https://arxiv.org/abs/2506.07628", "authors": ["Weronika Smolak-Dy\u017cewska", "Dawid Malarz", "Grzegorz Wilczy\u0144ski", "Rafa\u0142 Tobiasz", "Joanna Waczy\u0144ska", "Piotr Borycki", "Przemys\u0142aw Spurek"], "title": "HuSc3D: Human Sculpture dataset for 3D object reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D scene reconstruction from 2D images is one of the most important tasks in\ncomputer graphics. Unfortunately, existing datasets and benchmarks concentrate\non idealized synthetic or meticulously captured realistic data. Such benchmarks\nfail to convey the inherent complexities encountered in newly acquired\nreal-world scenes. In such scenes especially those acquired outside, the\nbackground is often dynamic, and by popular usage of cell phone cameras, there\nmight be discrepancies in, e.g., white balance. To address this gap, we present\nHuSc3D, a novel dataset specifically designed for rigorous benchmarking of 3D\nreconstruction models under realistic acquisition challenges. Our dataset\nuniquely features six highly detailed, fully white sculptures characterized by\nintricate perforations and minimal textural and color variation. Furthermore,\nthe number of images per scene varies significantly, introducing the additional\nchallenge of limited training data for some instances alongside scenes with a\nstandard number of views. By evaluating popular 3D reconstruction methods on\nthis diverse dataset, we demonstrate the distinctiveness of HuSc3D in\neffectively differentiating model performance, particularly highlighting the\nsensitivity of methods to fine geometric details, color ambiguity, and varying\ndata availability--limitations often masked by more conventional datasets.", "AI": {"tldr": "HuSc3D\u662f\u4e00\u4e2a\u4e13\u4e3a3D\u91cd\u5efa\u6a21\u578b\u5728\u771f\u5b9e\u91c7\u96c6\u6311\u6218\u4e0b\u8fdb\u884c\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u8bbe\u8ba1\u7684\u65b0\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u52a8\u6001\u80cc\u666f\u548c\u8272\u5f69\u5dee\u5f02\u7b49\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u96c6\u4e2d\u4e8e\u7406\u60f3\u5316\u5408\u6210\u6216\u7cbe\u5fc3\u6355\u83b7\u7684\u771f\u5b9e\u6570\u636e\uff0c\u672a\u80fd\u53cd\u6620\u65b0\u83b7\u53d6\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u590d\u6742\u6027\uff0c\u5982\u52a8\u6001\u80cc\u666f\u548c\u8272\u5f69\u5dee\u5f02\u3002", "method": "\u63d0\u51faHuSc3D\u6570\u636e\u96c6\uff0c\u5305\u542b\u516d\u4e2a\u9ad8\u5ea6\u8be6\u7ec6\u7684\u5168\u767d\u96d5\u5851\uff0c\u5177\u6709\u590d\u6742\u7a7f\u5b54\u548c\u6700\u5c0f\u7eb9\u7406\u53d8\u5316\uff0c\u56fe\u50cf\u6570\u91cf\u5dee\u5f02\u663e\u8457\u3002", "result": "\u8bc4\u4f30\u6d41\u884c3D\u91cd\u5efa\u65b9\u6cd5\u663e\u793a\uff0cHuSc3D\u80fd\u6709\u6548\u533a\u5206\u6a21\u578b\u6027\u80fd\uff0c\u7a81\u51fa\u65b9\u6cd5\u5bf9\u51e0\u4f55\u7ec6\u8282\u3001\u8272\u5f69\u6b67\u4e49\u548c\u6570\u636e\u53d8\u5316\u7684\u654f\u611f\u6027\u3002", "conclusion": "HuSc3D\u586b\u8865\u4e86\u73b0\u5b9e\u573a\u666f3D\u91cd\u5efa\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u6570\u636e\u96c6\u63a9\u76d6\u7684\u6a21\u578b\u5c40\u9650\u6027\u3002"}}
{"id": "2506.07637", "pdf": "https://arxiv.org/pdf/2506.07637", "abs": "https://arxiv.org/abs/2506.07637", "authors": ["Yuchong Long", "Wen Sun", "Ningxiao Sun", "Wenxiao Wang", "Chao Li", "Shan Yin"], "title": "HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition", "categories": ["cs.CV", "cs.LG", "68T07, 68T45", "I.2.10; I.4.9; I.5.4"], "comment": "16 pages, 5 figures, 2 tables. The dataset at\n  https://www.kaggle.com/datasets/ayinven/hieraedgenetintegratesdatasets. The\n  models at\n  https://huggingface.co/datasets/AyinMostima/HieraEdgeNetintegratesdatasets.\n  The source code in at https://github.com/AyinMostima/PalynoKit", "summary": "Automated pollen recognition is vital to paleoclimatology, biodiversity\nmonitoring, and public health, yet conventional methods are hampered by\ninefficiency and subjectivity. Existing deep learning models often struggle to\nachieve the requisite localization accuracy for microscopic targets like\npollen, which are characterized by their minute size, indistinct edges, and\ncomplex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a\nmulti-scale edge-enhancement framework. The framework's core innovation is the\nintroduction of three synergistic modules: the Hierarchical Edge Module (HEM),\nwhich explicitly extracts a multi-scale pyramid of edge features that\ncorresponds to the semantic hierarchy at early network stages; the Synergistic\nEdge Fusion (SEF) module, for deeply fusing these edge priors with semantic\ninformation at each respective scale; and the Cross Stage Partial Omni-Kernel\nModule (CSPOKM), which maximally refines the most detail-rich feature layers\nusing an Omni-Kernel operator - comprising anisotropic large-kernel\nconvolutions and mixed-domain attention - all within a computationally\nefficient Cross-Stage Partial (CSP) framework. On a large-scale dataset\ncomprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision\n(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline\nmodels such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms\nthat our approach generates feature representations that are more precisely\nfocused on object boundaries. By systematically integrating edge information,\nHieraEdgeNet provides a robust and powerful solution for high-precision,\nhigh-efficiency automated detection of microscopic objects.", "AI": {"tldr": "HieraEdgeNet\u662f\u4e00\u79cd\u591a\u5c3a\u5ea6\u8fb9\u7f18\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u521b\u65b0\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u82b1\u7c89\u7b49\u5fae\u5c0f\u76ee\u6807\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u82b1\u7c89\u8bc6\u522b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u4e3b\u89c2\u6027\u5f3a\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u5fae\u5c0f\u76ee\u6807\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faHieraEdgeNet\u6846\u67b6\uff0c\u5305\u542b\u5206\u5c42\u8fb9\u7f18\u6a21\u5757\uff08HEM\uff09\u3001\u534f\u540c\u8fb9\u7f18\u878d\u5408\u6a21\u5757\uff08SEF\uff09\u548c\u8de8\u9636\u6bb5\u90e8\u5206\u5168\u6838\u6a21\u5757\uff08CSPOKM\uff09\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u8fb9\u7f18\u7279\u5f81\u4e0e\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728120\u7c7b\u82b1\u7c89\u6570\u636e\u96c6\u4e0a\uff0cmAP@.5\u8fbe\u52300.9501\uff0c\u4f18\u4e8eYOLOv12n\u548cRT-DETR\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "HieraEdgeNet\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u8fb9\u7f18\u4fe1\u606f\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u7684\u5fae\u5c0f\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07643", "pdf": "https://arxiv.org/pdf/2506.07643", "abs": "https://arxiv.org/abs/2506.07643", "authors": ["Jae Sung Park", "Zixian Ma", "Linjie Li", "Chenhao Zheng", "Cheng-Yu Hsieh", "Ximing Lu", "Khyathi Chandu", "Quan Kong", "Norimasa Kobori", "Ali Farhadi", "Yejin Choi", "Ranjay Krishna"], "title": "Synthetic Visual Genome", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Reasoning over visual relationships-spatial, functional, interactional,\nsocial, etc.-is considered to be a fundamental component of human cognition.\nYet, despite the major advances in visual comprehension in multimodal language\nmodels (MLMs), precise reasoning over relationships and their generations\nremains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely\nannotated relationships capable of constructing high-quality dense scene graphs\nat scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by\ncompleting the missing relations of selected objects in existing scene graphs\nusing a teacher MLM and a carefully designed filtering process to ensure\nhigh-quality. To generate more accurate and rich scene graphs at scale for any\nimage, we introduce SG-EDIT: a self-distillation framework where GPT-4o further\nrefines ROBIN's predicted scene graphs by removing unlikely relations and/or\nsuggesting relevant ones. In total, our dataset contains 146K images and 5.6M\nrelationships for 2.6M objects. Results show that our ROBIN-3B model, despite\nbeing trained on less than 3 million instances, outperforms similar-size models\ntrained on over 300 million instances on relationship understanding benchmarks,\nand even surpasses larger models up to 13B parameters. Notably, it achieves\nstate-of-the-art performance in referring expression comprehension with a score\nof 88.9, surpassing the previous best of 87.4. Our results suggest that\ntraining on the refined scene graph data is crucial to maintaining high\nperformance across diverse visual reasoning task.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86ROBIN\u6a21\u578b\uff0c\u901a\u8fc7\u5bc6\u96c6\u6807\u6ce8\u7684\u5173\u7cfb\u6570\u636e\u8bad\u7ec3\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u573a\u666f\u56fe\uff0c\u5e76\u5728\u5173\u7cfb\u7406\u89e3\u4efb\u52a1\u4e2d\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5173\u7cfb\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6SVG\u8bad\u7ec3ROBIN\uff0c\u5e76\u5f15\u5165SG-EDIT\u6846\u67b6\u901a\u8fc7GPT-4o\u4f18\u5316\u573a\u666f\u56fe\u751f\u6210\u3002", "result": "ROBIN-3B\u6a21\u578b\u5728\u5173\u7cfb\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5e76\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u4e2d\u8fbe\u523088.9\u5206\u3002", "conclusion": "\u7cbe\u7ec6\u5316\u7684\u573a\u666f\u56fe\u6570\u636e\u8bad\u7ec3\u5bf9\u63d0\u5347\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.07652", "pdf": "https://arxiv.org/pdf/2506.07652", "abs": "https://arxiv.org/abs/2506.07652", "authors": ["Hangbei Cheng", "Xiaorong Dong", "Xueyu Liu", "Jianan Zhang", "Xuetao Ma", "Mingqiang Wei", "Liansheng Wang", "Junxin Chen", "Yongfei Wu"], "title": "FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate lesion segmentation in histopathology images is essential for\ndiagnostic interpretation and quantitative analysis, yet it remains challenging\ndue to the limited availability of costly pixel-level annotations. To address\nthis, we propose FMaMIL, a novel two-stage framework for weakly supervised\nlesion segmentation based solely on image-level labels. In the first stage, a\nlightweight Mamba-based encoder is introduced to capture long-range\ndependencies across image patches under the MIL paradigm. To enhance spatial\nsensitivity and structural awareness, we design a learnable frequency-domain\nencoding module that supplements spatial-domain features with spectrum-based\ninformation. CAMs generated in this stage are used to guide segmentation\ntraining. In the second stage, we refine the initial pseudo labels via a\nCAM-guided soft-label supervision and a self-correction mechanism, enabling\nrobust training even under label noise. Extensive experiments on both public\nand private histopathology datasets demonstrate that FMaMIL outperforms\nstate-of-the-art weakly supervised methods without relying on pixel-level\nannotations, validating its effectiveness and potential for digital pathology\napplications.", "AI": {"tldr": "FMaMIL\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7ea7\u6807\u7b7e\u7684\u5f31\u76d1\u7763\u75c5\u53d8\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08Mamba\u7f16\u7801\u5668\u4e0e\u9891\u57df\u7f16\u7801\u6a21\u5757\u7ed3\u5408\uff09\u5b9e\u73b0\u9ad8\u6548\u5206\u5272\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u75c5\u53d8\u5206\u5272\u56e0\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u800c\u96be\u4ee5\u5b9e\u73b0\u7684\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u4f7f\u7528Mamba\u7f16\u7801\u5668\u548c\u9891\u57df\u7f16\u7801\u6a21\u5757\u751f\u6210CAMs\uff1b2\uff09\u901a\u8fc7\u8f6f\u6807\u7b7e\u76d1\u7763\u548c\u81ea\u6211\u7ea0\u6b63\u673a\u5236\u4f18\u5316\u4f2a\u6807\u7b7e\u3002", "result": "\u5728\u516c\u5f00\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\u3002", "conclusion": "FMaMIL\u5728\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.07670", "pdf": "https://arxiv.org/pdf/2506.07670", "abs": "https://arxiv.org/abs/2506.07670", "authors": ["Xiaohan Lu", "Jiaye Fu", "Jiaqi Zhang", "Zetian Song", "Chuanmin Jia", "Siwei Ma"], "title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views", "categories": ["cs.CV"], "comment": null, "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising\nresults for novel view synthesis (NVS) from sparse input views, particularly\nunder narrow-baseline conditions. However, its performance significantly\ndegrades in wide-baseline scenarios due to limited texture details and\ngeometric inconsistencies across views. To address these challenges, in this\npaper, we propose ProSplat, a two-stage feed-forward framework designed for\nhigh-fidelity rendering under wide-baseline conditions. The first stage\ninvolves generating 3D Gaussian primitives via a 3DGS generator. In the second\nstage, rendered views from these primitives are enhanced through an improvement\nmodel. Specifically, this improvement model is based on a one-step diffusion\nmodel, further optimized by our proposed Maximum Overlap Reference view\nInjection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI\nsupplements missing texture and color by strategically selecting a reference\nview with maximum viewpoint overlap, while DWEA enforces geometric consistency\nusing epipolar constraints. Additionally, we introduce a divide-and-conquer\ntraining strategy that aligns data distributions between the two stages through\njoint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K\ndatasets under wide-baseline settings. Experimental results demonstrate that\nProSplat achieves an average improvement of 1 dB in PSNR compared to recent\nSOTA methods.", "AI": {"tldr": "ProSplat\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u524d\u9988\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5bbd\u57fa\u7ebf\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u901a\u8fc73D\u9ad8\u65af\u751f\u6210\u5668\u548c\u6539\u8fdb\u6a21\u578b\uff08\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff09\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u6e85\u5c04\u5728\u5bbd\u57fa\u7ebf\u573a\u666f\u4e0b\u56e0\u7eb9\u7406\u7ec6\u8282\u4e0d\u8db3\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u751f\u62103D\u9ad8\u65af\u57fa\u5143\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\uff08\u7ed3\u5408MORI\u548cDWEA\uff09\u589e\u5f3a\u6e32\u67d3\u89c6\u56fe\u3002", "result": "\u5728RealEstate10K\u548cDL3DV-10K\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u5e73\u5747\u63d0\u53471 dB\u3002", "conclusion": "ProSplat\u5728\u5bbd\u57fa\u7ebf\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2506.07697", "pdf": "https://arxiv.org/pdf/2506.07697", "abs": "https://arxiv.org/abs/2506.07697", "authors": ["Jens Piekenbrinck", "Christian Schmidt", "Alexander Hermans", "Narunas Vaskevicius", "Timm Linder", "Bastian Leibe"], "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nneural scene reconstruction, offering high-quality novel view synthesis while\nmaintaining computational efficiency. In this paper, we extend the capabilities\nof 3DGS beyond pure scene representation by introducing an approach for\nopen-vocabulary 3D instance segmentation without requiring manual labeling,\ntermed OpenSplat3D. Our method leverages feature-splatting techniques to\nassociate semantic information with individual Gaussians, enabling fine-grained\nscene understanding. We incorporate Segment Anything Model instance masks with\na contrastive loss formulation as guidance for the instance features to achieve\naccurate instance-level segmentation. Furthermore, we utilize language\nembeddings of a vision-language model, allowing for flexible, text-driven\ninstance identification. This combination enables our system to identify and\nsegment arbitrary objects in 3D scenes based on natural language descriptions.\nWe show results on LERF-mask and LERF-OVS as well as the full ScanNet++\nvalidation set, demonstrating the effectiveness of our approach.", "AI": {"tldr": "OpenSplat3D\u6269\u5c55\u4e863D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u7684\u5f00\u653e\u8bcd\u6c473D\u5b9e\u4f8b\u5206\u5272\uff0c\u7ed3\u5408\u7279\u5f81\u6e85\u5c04\u3001SAM\u5b9e\u4f8b\u63a9\u7801\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u573a\u666f\u7406\u89e3\u3002", "motivation": "3DGS\u5728\u795e\u7ecf\u573a\u666f\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55\u5176\u529f\u80fd\uff0c\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u76843D\u5b9e\u4f8b\u5206\u5272\u3002", "method": "\u7ed3\u5408\u7279\u5f81\u6e85\u5c04\u6280\u672f\u3001SAM\u5b9e\u4f8b\u63a9\u7801\u548c\u5bf9\u6bd4\u635f\u5931\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u5d4c\u5165\uff0c\u5b9e\u73b0\u6587\u672c\u9a71\u52a8\u7684\u5b9e\u4f8b\u5206\u5272\u3002", "result": "\u5728LERF-mask\u3001LERF-OVS\u548cScanNet++\u9a8c\u8bc1\u96c6\u4e0a\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "OpenSplat3D\u6210\u529f\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u76843D\u5b9e\u4f8b\u5206\u5272\uff0c\u4e3a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u5de5\u5177\u3002"}}
{"id": "2506.07698", "pdf": "https://arxiv.org/pdf/2506.07698", "abs": "https://arxiv.org/abs/2506.07698", "authors": ["Yuxiao Yang", "Peihao Li", "Yuhong Zhang", "Junzhe Lu", "Xianglong He", "Minghan Qin", "Weitao Wang", "Haoqian Wang"], "title": "NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 7 figures, accepted by ICME 2025", "summary": "3D AI-generated content (AIGC) has made it increasingly accessible for anyone\nto become a 3D content creator. While recent methods leverage Score\nDistillation Sampling to distill 3D objects from pretrained image diffusion\nmodels, they often suffer from inadequate 3D priors, leading to insufficient\nmulti-view consistency. In this work, we introduce NOVA3D, an innovative\nsingle-image-to-3D generation framework. Our key insight lies in leveraging\nstrong 3D priors from a pretrained video diffusion model and integrating\ngeometric information during multi-view video fine-tuning. To facilitate\ninformation exchange between color and geometric domains, we propose the\nGeometry-Temporal Alignment (GTA) attention mechanism, thereby improving\ngeneralization and multi-view consistency. Moreover, we introduce the\nde-conflict geometry fusion algorithm, which improves texture fidelity by\naddressing multi-view inaccuracies and resolving discrepancies in pose\nalignment. Extensive experiments validate the superiority of NOVA3D over\nexisting baselines.", "AI": {"tldr": "NOVA3D\u662f\u4e00\u79cd\u521b\u65b0\u7684\u5355\u56fe\u50cf\u52303D\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u76843D\u5148\u9a8c\u548c\u51e0\u4f55\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u9884\u8bad\u7ec3\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d63D\u5bf9\u8c61\u65f6\uff0c\u7531\u4e8e\u7f3a\u4e4f\u8db3\u591f\u76843D\u5148\u9a8c\uff0c\u5bfc\u81f4\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "NOVA3D\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u76843D\u5148\u9a8c\uff0c\u7ed3\u5408\u51e0\u4f55\u4fe1\u606f\u8fdb\u884c\u591a\u89c6\u89d2\u89c6\u9891\u5fae\u8c03\uff0c\u5e76\u63d0\u51faGeometry-Temporal Alignment (GTA)\u6ce8\u610f\u529b\u673a\u5236\u548c\u53bb\u51b2\u7a81\u51e0\u4f55\u878d\u5408\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNOVA3D\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u7eb9\u7406\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NOVA3D\u901a\u8fc7\u5f15\u5165\u5f3a3D\u5148\u9a8c\u548c\u51e0\u4f55\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u56fe\u50cf\u52303D\u751f\u6210\u7684\u8d28\u91cf\u3002"}}
{"id": "2506.07705", "pdf": "https://arxiv.org/pdf/2506.07705", "abs": "https://arxiv.org/abs/2506.07705", "authors": ["Weilei Wen", "Chunle Guo", "Wenqi Ren", "Hongpeng Wang", "Xiuli Shao"], "title": "Adaptive Blind Super-Resolution Network for Spatial-Specific and Spatial-Agnostic Degradations", "categories": ["cs.CV", "eess.IV"], "comment": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "summary": "Prior methodologies have disregarded the diversities among distinct\ndegradation types during image reconstruction, employing a uniform network\nmodel to handle multiple deteriorations. Nevertheless, we discover that\nprevalent degradation modalities, including sampling, blurring, and noise, can\nbe roughly categorized into two classes. We classify the first class as\nspatial-agnostic dominant degradations, less affected by regional changes in\nimage space, such as downsampling and noise degradation. The second class\ndegradation type is intimately associated with the spatial position of the\nimage, such as blurring, and we identify them as spatial-specific dominant\ndegradations. We introduce a dynamic filter network integrating global and\nlocal branches to address these two degradation types. This network can greatly\nalleviate the practical degradation problem. Specifically, the global dynamic\nfiltering layer can perceive the spatial-agnostic dominant degradation in\ndifferent images by applying weights generated by the attention mechanism to\nmultiple parallel standard convolution kernels, enhancing the network's\nrepresentation ability. Meanwhile, the local dynamic filtering layer converts\nfeature maps of the image into a spatially specific dynamic filtering operator,\nwhich performs spatially specific convolution operations on the image features\nto handle spatial-specific dominant degradations. By effectively integrating\nboth global and local dynamic filtering operators, our proposed method\noutperforms state-of-the-art blind super-resolution algorithms in both\nsynthetic and real image datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6ee4\u6ce2\u7f51\u7edc\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u5206\u652f\u5206\u522b\u5904\u7406\u7a7a\u95f4\u65e0\u5173\u548c\u7a7a\u95f4\u76f8\u5173\u7684\u56fe\u50cf\u9000\u5316\u7c7b\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e0d\u540c\u9000\u5316\u7c7b\u578b\u7684\u591a\u6837\u6027\uff0c\u91c7\u7528\u5355\u4e00\u7f51\u7edc\u6a21\u578b\u5904\u7406\u591a\u79cd\u9000\u5316\uff0c\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5206\u7c7b\u9000\u5316\u7c7b\u578b\u4e3a\u7a7a\u95f4\u65e0\u5173\u548c\u7a7a\u95f4\u76f8\u5173\uff0c\u8bbe\u8ba1\u52a8\u6001\u6ee4\u6ce2\u7f51\u7edc\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u5206\u652f\u5206\u522b\u5904\u7406\u4e24\u7c7b\u9000\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u76f2\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5\u3002", "conclusion": "\u52a8\u6001\u6ee4\u6ce2\u7f51\u7edc\u80fd\u6709\u6548\u5904\u7406\u591a\u6837\u5316\u7684\u56fe\u50cf\u9000\u5316\u95ee\u9898\uff0c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2506.07713", "pdf": "https://arxiv.org/pdf/2506.07713", "abs": "https://arxiv.org/abs/2506.07713", "authors": ["Ge Wang", "Songlin Fan", "Hangxu Liu", "Quanjian Song", "Hewei Wang", "Jinfeng Xu"], "title": "Consistent Video Editing as Flow-Driven Image-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 12 figures", "summary": "With the prosper of video diffusion models, down-stream applications like\nvideo editing have been significantly promoted without consuming much\ncomputational cost. One particular challenge in this task lies at the motion\ntransfer process from the source video to the edited one, where it requires the\nconsideration of the shape deformation in between, meanwhile maintaining the\ntemporal consistency in the generated video sequence. However, existing methods\nfail to model complicated motion patterns for video editing, and are\nfundamentally limited to object replacement, where tasks with non-rigid object\nmotions like multi-object and portrait editing are largely neglected. In this\npaper, we observe that optical flows offer a promising alternative in complex\nmotion modeling, and present FlowV2V to re-investigate video editing as a task\nof flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V\ndecomposes the entire pipeline into first-frame editing and conditional I2V\ngeneration, and simulates pseudo flow sequence that aligns with the deformed\nshape, thus ensuring the consistency during editing. Experimental results on\nDAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error\nillustrate the superior temporal consistency and sample quality of FlowV2V\ncompared to existing state-of-the-art ones. Furthermore, we conduct\ncomprehensive ablation studies to analyze the internal functionalities of the\nfirst-frame paradigm and flow alignment in the proposed method.", "AI": {"tldr": "FlowV2V\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5149\u6d41\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u4e3a\u7b2c\u4e00\u5e27\u7f16\u8f91\u548c\u6761\u4ef6\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u8fd0\u52a8\u5efa\u6a21\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u8fd0\u52a8\uff08\u5982\u591a\u5bf9\u8c61\u548c\u8096\u50cf\u7f16\u8f91\uff09\uff0c\u4e14\u5c40\u9650\u4e8e\u5bf9\u8c61\u66ff\u6362\u4efb\u52a1\u3002\u5149\u6d41\u4e3a\u590d\u6742\u8fd0\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "FlowV2V\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u7b2c\u4e00\u5e27\u7f16\u8f91\u548c\u6761\u4ef6I2V\u751f\u6210\uff0c\u901a\u8fc7\u6a21\u62df\u4f2a\u5149\u6d41\u5e8f\u5217\u786e\u4fdd\u7f16\u8f91\u4e2d\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "result": "\u5728DAVIS-EDIT\u6570\u636e\u96c6\u4e0a\uff0cFlowV2V\u5728DOVER\u548cwarping error\u6307\u6807\u4e0a\u5206\u522b\u63d0\u5347\u4e8613.67%\u548c50.66%\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlowV2V\u901a\u8fc7\u5149\u6d41\u9a71\u52a8\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7f16\u8f91\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u6837\u672c\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u590d\u6742\u8fd0\u52a8\u573a\u666f\u3002"}}
{"id": "2506.07720", "pdf": "https://arxiv.org/pdf/2506.07720", "abs": "https://arxiv.org/abs/2506.07720", "authors": ["Yufei Guo", "Yuhan Zhang", "Zhou Jie", "Xiaode Liu", "Xin Tong", "Yuanpei Chen", "Weihang Peng", "Zhe Ma"], "title": "ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks", "categories": ["cs.CV"], "comment": "Accpeted by ICML2024", "summary": "The Spiking Neural Network (SNN), a biologically inspired neural network\ninfrastructure, has garnered significant attention recently. SNNs utilize\nbinary spike activations for efficient information transmission, replacing\nmultiplications with additions, thereby enhancing energy efficiency. However,\nbinary spike activation maps often fail to capture sufficient data information,\nresulting in reduced accuracy. To address this challenge, we advocate reversing\nthe bit of the weight and activation for SNNs, called \\textbf{ReverB-SNN},\ninspired by recent findings that highlight greater accuracy degradation from\nquantizing activations compared to weights. Specifically, our method employs\nreal-valued spike activations alongside binary weights in SNNs. This preserves\nthe event-driven and multiplication-free advantages of standard SNNs while\nenhancing the information capacity of activations. Additionally, we introduce a\ntrainable factor within binary weights to adaptively learn suitable weight\namplitudes during training, thereby increasing network capacity. To maintain\nefficiency akin to vanilla \\textbf{ReverB-SNN}, our trainable binary weight\nSNNs are converted back to standard form using a re-parameterization technique\nduring inference. Extensive experiments across various network architectures\nand datasets, both static and dynamic, demonstrate that our approach\nconsistently outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReverB-SNN\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u8f6c\u6743\u91cd\u548c\u6fc0\u6d3b\u7684\u4f4d\uff0c\u7ed3\u5408\u5b9e\u503c\u6fc0\u6d3b\u548c\u4e8c\u8fdb\u5236\u6743\u91cd\uff0c\u63d0\u5347SNN\u7684\u4fe1\u606f\u5bb9\u91cf\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3SNN\u4e2d\u4e8c\u8fdb\u5236\u6fc0\u6d3b\u4fe1\u606f\u4e0d\u8db3\u5bfc\u81f4\u7684\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5b9e\u503c\u6fc0\u6d3b\u548c\u4e8c\u8fdb\u5236\u6743\u91cd\uff0c\u5f15\u5165\u53ef\u8bad\u7ec3\u56e0\u5b50\u8c03\u6574\u6743\u91cd\u5e45\u5ea6\uff0c\u5e76\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u4fdd\u6301\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728\u591a\u79cd\u7f51\u7edc\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ReverB-SNN\u5728\u4fdd\u6301SNN\u9ad8\u6548\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u3002"}}
{"id": "2506.07725", "pdf": "https://arxiv.org/pdf/2506.07725", "abs": "https://arxiv.org/abs/2506.07725", "authors": ["Shadi Hamdan", "Chonghao Sima", "Zetong Yang", "Hongyang Li", "Fatma G\u00fcney"], "title": "ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 submission. For code, see\n  https://github.com/opendrivelab/ETA", "summary": "How can we benefit from large models without sacrificing inference speed, a\ncommon dilemma in self-driving systems? A prevalent solution is a dual-system\narchitecture, employing a small model for rapid, reactive decisions and a\nlarger model for slower but more informative analyses. Existing dual-system\ndesigns often implement parallel architectures where inference is either\ndirectly conducted using the large model at each current frame or retrieved\nfrom previously stored inference results. However, these works still struggle\nto enable large models for a timely response to every online frame. Our key\ninsight is to shift intensive computations of the current frame to previous\ntime steps and perform a batch inference of multiple time steps to make large\nmodels respond promptly to each time step. To achieve the shifting, we\nintroduce Efficiency through Thinking Ahead (ETA), an asynchronous system\ndesigned to: (1) propagate informative features from the past to the current\nframe using future predictions from the large model, (2) extract current frame\nfeatures using a small model for real-time responsiveness, and (3) integrate\nthese dual features via an action mask mechanism that emphasizes\naction-critical image regions. Evaluated on the Bench2Drive CARLA\nLeaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with\na driving score of 69.53 while maintaining a near-real-time inference speed at\n50 ms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faETA\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6b65\u8ba1\u7b97\u548c\u6279\u91cf\u63a8\u7406\uff0c\u4f7f\u5927\u6a21\u578b\u80fd\u53ca\u65f6\u54cd\u5e94\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6bcf\u4e00\u5e27\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u5927\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u9ad8\u4fe1\u606f\u91cf\u7684\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u7cfb\u7edfETA\uff0c\u5c06\u5f53\u524d\u5e27\u7684\u8ba1\u7b97\u4efb\u52a1\u8f6c\u79fb\u5230\u8fc7\u53bb\u65f6\u95f4\u6b65\uff0c\u901a\u8fc7\u6279\u91cf\u63a8\u7406\u5b9e\u73b0\u5927\u6a21\u578b\u7684\u5feb\u901f\u54cd\u5e94\uff0c\u5e76\u7ed3\u5408\u5c0f\u6a21\u578b\u63d0\u53d6\u5b9e\u65f6\u7279\u5f81\u3002", "result": "\u5728Bench2Drive CARLA Leaderboard-v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cETA\u5c06\u9a7e\u9a76\u5206\u6570\u63d0\u53478%\uff0c\u8fbe\u523069.53\uff0c\u540c\u65f6\u4fdd\u630150\u6beb\u79d2\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "ETA\u7cfb\u7edf\u6210\u529f\u5e73\u8861\u4e86\u5927\u6a21\u578b\u7684\u9ad8\u6027\u80fd\u548c\u5c0f\u6a21\u578b\u7684\u5b9e\u65f6\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07737", "pdf": "https://arxiv.org/pdf/2506.07737", "abs": "https://arxiv.org/abs/2506.07737", "authors": ["Xuemei Chen", "Huamin Wang", "Hangchi Shen", "Shukai Duan", "Shiping Wen", "Tingwen Huang"], "title": "SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding", "categories": ["cs.CV"], "comment": null, "summary": "Low energy consumption for 3D object detection is an important research area\nbecause of the increasing energy consumption with their wide application in\nfields such as autonomous driving. The spiking neural networks (SNNs) with\nlow-power consumption characteristics can provide a novel solution for this\nresearch. Therefore, we apply SNNs to monocular 3D object detection and propose\nthe SpikeSMOKE architecture in this paper, which is a new attempt for low-power\nmonocular 3D object detection. As we all know, discrete signals of SNNs will\ngenerate information loss and limit their feature expression ability compared\nwith the artificial neural networks (ANNs).In order to address this issue,\ninspired by the filtering mechanism of biological neuronal synapses, we propose\na cross-scale gated coding mechanism(CSGC), which can enhance feature\nrepresentation by combining cross-scale fusion of attentional methods and gated\nfiltering mechanisms.In addition, to reduce the computation and increase the\nspeed of training, we present a novel light-weight residual block that can\nmaintain spiking computing paradigm and the highest possible detection\nperformance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,\nthe proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,\nModerate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by\nAP|R11 at 0.7 IoU threshold, respectively. It is important to note that the\nresults of SpikeSMOKE can significantly reduce energy consumption compared to\nthe results on SMOKE. For example,the energy consumption can be reduced by\n72.2% on the hard category, while the detection performance is reduced by only\n4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3\ntimes and computation by 10 times compared to SMOKE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u4f4e\u529f\u8017\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5SpikeSMOKE\uff0c\u901a\u8fc7\u8de8\u5c3a\u5ea6\u95e8\u63a7\u7f16\u7801\u673a\u5236\uff08CSGC\uff09\u548c\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u5757\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u968f\u77403D\u76ee\u6807\u68c0\u6d4b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u80fd\u8017\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002SNNs\u7684\u4f4e\u529f\u8017\u7279\u6027\u4e3a\u6b64\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSpikeSMOKE\u67b6\u6784\uff0c\u7ed3\u5408CSGC\u673a\u5236\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u5757\u4ee5\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\uff0cSpikeSMOKE\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08AP|R11\u6307\u6807\uff09\uff0c\u540c\u65f6\u80fd\u8017\u964d\u4f4e72.2%\uff0c\u8f7b\u91cf\u7248\u8fdb\u4e00\u6b65\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u3002", "conclusion": "SpikeSMOKE\u4e3a\u4f4e\u529f\u80173D\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u6027\u80fd\u4e0e\u80fd\u8017\u5e73\u8861\u826f\u597d\u3002"}}
{"id": "2506.07738", "pdf": "https://arxiv.org/pdf/2506.07738", "abs": "https://arxiv.org/abs/2506.07738", "authors": ["Lanjiong Li", "Guanhua Zhao", "Lingting Zhu", "Zeyu Cai", "Lequan Yu", "Jian Zhang", "Zeyu Wang"], "title": "AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025. 11 pages, 12 figures", "summary": "Recent research on generative models has primarily focused on creating\nproduct-ready visual outputs; however, designers often favor access to\nstandardized asset libraries, a domain that has yet to be significantly\nenhanced by generative capabilities. Although open-world scenes provide ample\nraw materials for designers, efficiently extracting high-quality, standardized\nassets remains a challenge. To address this, we introduce AssetDropper, the\nfirst framework designed to extract assets from reference images, providing\nartists with an open-world asset palette. Our model adeptly extracts a front\nview of selected subjects from input images, effectively handling complex\nscenarios such as perspective distortion and subject occlusion. We establish a\nsynthetic dataset of more than 200,000 image-subject pairs and a real-world\nbenchmark with thousands more for evaluation, facilitating the exploration of\nfuture research in downstream tasks. Furthermore, to ensure precise asset\nextraction that aligns well with the image prompts, we employ a pre-trained\nreward model to fulfill a closed-loop with feedback. We design the reward model\nto perform an inverse task that pastes the extracted assets back into the\nreference sources, which assists training with additional consistency and\nmitigates hallucination. Extensive experiments show that, with the aid of\nreward-driven optimization, AssetDropper achieves the state-of-the-art results\nin asset extraction. Project page: AssetDropper.github.io.", "AI": {"tldr": "AssetDropper\u662f\u4e00\u4e2a\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u63d0\u53d6\u6807\u51c6\u5316\u8d44\u4ea7\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8bbe\u8ba1\u5e08\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u9ad8\u6548\u63d0\u53d6\u9ad8\u8d28\u91cf\u8d44\u4ea7\u7684\u6311\u6218\u3002", "motivation": "\u8bbe\u8ba1\u5e08\u66f4\u503e\u5411\u4e8e\u4f7f\u7528\u6807\u51c6\u5316\u8d44\u4ea7\u5e93\uff0c\u4f46\u751f\u6210\u6a21\u578b\u5c1a\u672a\u663e\u8457\u63d0\u5347\u8fd9\u4e00\u9886\u57df\u3002\u5f00\u653e\u4e16\u754c\u573a\u666f\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7d20\u6750\uff0c\u4f46\u9ad8\u6548\u63d0\u53d6\u9ad8\u8d28\u91cf\u8d44\u4ea7\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165AssetDropper\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u5b9e\u73b0\u95ed\u73af\u53cd\u9988\uff0c\u63d0\u53d6\u56fe\u50cf\u4e2d\u7684\u4e3b\u4f53\u5e76\u5904\u7406\u590d\u6742\u573a\u666f\uff08\u5982\u900f\u89c6\u53d8\u5f62\u548c\u906e\u6321\uff09\u3002", "result": "AssetDropper\u5728\u8d44\u4ea7\u63d0\u53d6\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u901a\u8fc7\u5956\u52b1\u9a71\u52a8\u4f18\u5316\u63d0\u5347\u4e86\u63d0\u53d6\u7cbe\u5ea6\u3002", "conclusion": "AssetDropper\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u5f00\u653e\u4e16\u754c\u8d44\u4ea7\u8c03\u8272\u677f\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.07739", "pdf": "https://arxiv.org/pdf/2506.07739", "abs": "https://arxiv.org/abs/2506.07739", "authors": ["Jing Zhong", "Jun Yin", "Peilin Li", "Pengyu Zeng", "Miao Zhang", "Shuai Lu", "Ran Luo"], "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Architectural cultures across regions are characterized by stylistic\ndiversity, shaped by historical, social, and technological contexts in addition\nto geograph-ical conditions. Understanding architectural styles requires the\nability to describe and analyze the stylistic features of different architects\nfrom various regions through visual observations of architectural imagery.\nHowever, traditional studies of architectural culture have largely relied on\nsubjective expert interpretations and historical literature reviews, often\nsuffering from regional biases and limited ex-planatory scope. To address these\nchallenges, this study proposes three core contributions: (1) We construct a\nprofessional architectural style dataset named ArchDiffBench, which comprises\n1,765 high-quality architectural images and their corresponding style\nannotations, collected from different regions and historical periods. (2) We\npropose ArchiLense, an analytical framework grounded in Vision-Language Models\nand constructed using the ArchDiffBench dataset. By integrating ad-vanced\ncomputer vision techniques, deep learning, and machine learning algo-rithms,\nArchiLense enables automatic recognition, comparison, and precise\nclassi-fication of architectural imagery, producing descriptive language\noutputs that ar-ticulate stylistic differences. (3) Extensive evaluations show\nthat ArchiLense achieves strong performance in architectural style recognition,\nwith a 92.4% con-sistency rate with expert annotations and 84.5% classification\naccuracy, effec-tively capturing stylistic distinctions across images. The\nproposed approach transcends the subjectivity inherent in traditional analyses\nand offers a more objective and accurate perspective for comparative studies of\narchitectural culture.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faArchDiffBench\u6570\u636e\u96c6\u548cArchiLense\u6846\u67b6\uff0c\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5b9e\u73b0\u5efa\u7b51\u98ce\u683c\u7684\u81ea\u52a8\u8bc6\u522b\u4e0e\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e3b\u89c2\u5206\u6790\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u6587\u5316\u7814\u7a76\u4f9d\u8d56\u4e3b\u89c2\u4e13\u5bb6\u89e3\u8bfb\u548c\u5386\u53f2\u6587\u732e\uff0c\u5b58\u5728\u533a\u57df\u504f\u89c1\u548c\u89e3\u91ca\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaArchDiffBench\u6570\u636e\u96c6\uff081,765\u5f20\u9ad8\u8d28\u91cf\u5efa\u7b51\u56fe\u50cf\uff09\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eVision-Language Models\u7684ArchiLense\u6846\u67b6\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "ArchiLense\u5728\u5efa\u7b51\u98ce\u683c\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u4e13\u5bb6\u6807\u6ce8\u4e00\u81f4\u6027\u8fbe92.4%\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u4e3a84.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u4f20\u7edf\u5206\u6790\u7684\u4e3b\u89c2\u6027\uff0c\u4e3a\u5efa\u7b51\u6587\u5316\u6bd4\u8f83\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5ba2\u89c2\u3001\u51c6\u786e\u7684\u89c6\u89d2\u3002"}}
{"id": "2506.07740", "pdf": "https://arxiv.org/pdf/2506.07740", "abs": "https://arxiv.org/abs/2506.07740", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "categories": ["cs.CV"], "comment": null, "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Flow-Anything\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u89c6\u56fe\u56fe\u50cf\u751f\u6210\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u5149\u6d41\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u5e26\u6765\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5149\u6d41\u4f30\u8ba1\u4e2d\u56e0\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5bfc\u81f4\u7684\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u6027\u80fd\u9650\u5236\u548c\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5355\u89c6\u56fe\u56fe\u50cf\u751f\u62103D\u8868\u793a\uff0c\u7ed3\u5408\u5bf9\u8c61\u65e0\u5173\u4f53\u79ef\u6e32\u67d3\u548c\u6df1\u5ea6\u611f\u77e5\u4fee\u590d\u6a21\u5757\uff0c\u751f\u6210\u771f\u5b9e\u5149\u6d41\u6570\u636e\u3002", "result": "\u751f\u6210\u7684FA-Flow\u6570\u636e\u96c6\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u76d1\u7763\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u4e86\u4e0b\u6e38\u89c6\u9891\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "Flow-Anything\u6846\u67b6\u4e3a\u5149\u6d41\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u6570\u636e\u6765\u6e90\uff0c\u5e76\u53ef\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u652f\u6301\u591a\u79cd\u89c6\u9891\u4efb\u52a1\u3002"}}
{"id": "2506.07750", "pdf": "https://arxiv.org/pdf/2506.07750", "abs": "https://arxiv.org/abs/2506.07750", "authors": ["Hyunsoo Kim", "Donghyun Kim", "Suhyun Kim"], "title": "Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation", "categories": ["cs.CV"], "comment": "Published at CVPR 2025", "summary": "How can we generate an image B' that satisfies A:A'::B:B', given the input\nimages A,A' and B? Recent works have tackled this challenge through approaches\nlike visual in-context learning or visual instruction. However, these methods\nare typically limited to specific models (e.g. InstructPix2Pix. Inpainting\nmodels) rather than general diffusion models (e.g. Stable Diffusion, SDXL).\nThis dependency may lead to inherited biases or lower editing capabilities. In\nthis paper, we propose Difference Inversion, a method that isolates only the\ndifference from A and A' and applies it to B to generate a plausible B'. To\naddress model dependency, it is crucial to structure prompts in the form of a\n\"Full Prompt\" suitable for input to stable diffusion models, rather than using\nan \"Instruction Prompt\". To this end, we accurately extract the Difference\nbetween A and A' and combine it with the prompt of B, enabling a plug-and-play\napplication of the difference. To extract a precise difference, we first\nidentify it through 1) Delta Interpolation. Additionally, to ensure accurate\ntraining, we propose the 2) Token Consistency Loss and 3) Zero Initialization\nof Token Embeddings. Our extensive experiments demonstrate that Difference\nInversion outperforms existing baselines both quantitatively and qualitatively,\nindicating its ability to generate more feasible B' in a model-agnostic manner.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDifference Inversion\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u56fe\u50cfA\u548cA'\u7684\u5dee\u5f02\u5e76\u5e94\u7528\u4e8eB\uff0c\u751f\u6210\u7b26\u5408A:A'::B:B'\u5173\u7cfb\u7684B'\uff0c\u9002\u7528\u4e8e\u901a\u7528\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u6a21\u578b\uff0c\u53ef\u80fd\u5f15\u5165\u504f\u89c1\u6216\u7f16\u8f91\u80fd\u529b\u53d7\u9650\uff0c\u9700\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7Delta\u63d2\u503c\u63d0\u53d6\u5dee\u5f02\uff0c\u7ed3\u5408Token\u4e00\u81f4\u6027\u635f\u5931\u548c\u96f6\u521d\u59cb\u5316Token\u5d4c\u5165\uff0c\u6784\u5efa\u9002\u7528\u4e8e\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u7684\u5b8c\u6574\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDifference Inversion\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u4ee5\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u751f\u6210\u66f4\u53ef\u884c\u7684B'\u3002"}}
{"id": "2506.07773", "pdf": "https://arxiv.org/pdf/2506.07773", "abs": "https://arxiv.org/abs/2506.07773", "authors": ["Mohamed Djilani", "Nassim Ali Ousalah", "Nidhal Eddine Chenni"], "title": "Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce a trend-aware and visually-grounded fashion recommendation\nsystem that integrates deep visual representations, garment-aware segmentation,\nsemantic category similarity and user behavior simulation. Our pipeline\nextracts focused visual embeddings by masking non-garment regions via semantic\nsegmentation followed by feature extraction using pretrained CNN backbones\n(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we\ngenerate synthetic purchase histories influenced by user-specific trendiness\nand item popularity. Recommendations are computed using a weighted scoring\nfunction that fuses visual similarity, semantic coherence and popularity\nalignment. Experiments on the DeepFashion dataset demonstrate consistent gender\nalignment and improved category relevance, with ResNet-50 achieving 64.95%\ncategory similarity and lowest popularity MAE. An ablation study confirms the\ncomplementary roles of visual and popularity cues. Our method provides a\nscalable framework for personalized fashion recommendations that balances\nindividual style with emerging trends. Our implementation is available at\nhttps://github.com/meddjilani/FashionRecommender", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u7528\u6237\u884c\u4e3a\u7684\u65f6\u5c1a\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u6a21\u62df\u7528\u6237\u8d2d\u7269\u884c\u4e3a\uff0c\u5e76\u878d\u5408\u591a\u56e0\u7d20\u751f\u6210\u63a8\u8350\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65f6\u5c1a\u63a8\u8350\u7cfb\u7edf\u5ffd\u89c6\u89c6\u89c9\u7ec6\u8282\u548c\u7528\u6237\u4e2a\u6027\u5316\u8d8b\u52bf\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u63a8\u8350\u3002", "method": "\u4f7f\u7528\u8bed\u4e49\u5206\u5272\u63d0\u53d6\u670d\u88c5\u533a\u57df\u7279\u5f81\uff0c\u7ed3\u5408CNN\u6a21\u578b\u751f\u6210\u89c6\u89c9\u5d4c\u5165\uff0c\u6a21\u62df\u7528\u6237\u884c\u4e3a\u5386\u53f2\uff0c\u878d\u5408\u89c6\u89c9\u76f8\u4f3c\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6d41\u884c\u5ea6\u8bc4\u5206\u3002", "result": "\u5728DeepFashion\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cResNet-50\u8fbe\u523064.95%\u7684\u7c7b\u522b\u76f8\u4f3c\u6027\uff0c\u6d41\u884c\u5ea6\u8bef\u5dee\u6700\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e2a\u6027\u5316\u65f6\u5c1a\u63a8\u8350\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5e73\u8861\u4e2a\u4eba\u98ce\u683c\u4e0e\u6d41\u884c\u8d8b\u52bf\u3002"}}
{"id": "2506.07778", "pdf": "https://arxiv.org/pdf/2506.07778", "abs": "https://arxiv.org/abs/2506.07778", "authors": ["Yichang Xu", "Gaowen Liu", "Ramana Rao Kompella", "Sihao Hu", "Tiansheng Huang", "Fatih Ilhan", "Selim Furkan Tekin", "Zachary Yahn", "Ling Liu"], "title": "Language-Vision Planner and Executor for Text-to-Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal visual-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date suffer from\ngeneralization performance. Inspired by recent development in LLMs for visual\nreasoning, this paper presents VLAgent, an AI system that can create a\nstep-by-step visual reasoning plan with an easy-to-understand script and\nexecute each step of the plan in real time by integrating planning script with\nexecution verifications via an automated process supported by VLAgent. In the\ntask planning phase, VLAgent fine-tunes an LLM through in-context learning to\ngenerate a step-by-step planner for each user-submitted text-visual reasoning\ntask. During the plan execution phase, VLAgent progressively refines the\ncomposition of neuro-symbolic executable modules to generate high-confidence\nreasoning results. VLAgent has three unique design characteristics: First, we\nimprove the quality of plan generation through in-context learning, improving\nlogic reasoning by reducing erroneous logic steps, incorrect programs, and LLM\nhallucinations. Second, we design a syntax-semantics parser to identify and\ncorrect additional logic errors of the LLM-generated planning script prior to\nlaunching the plan executor. Finally, we employ the ensemble method to improve\nthe generalization performance of our step-executor. Extensive experiments with\nfour visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent\nachieves significant performance enhancement for multimodal text-visual\nreasoning applications, compared to the exiting representative VLMs and LLM\nbased visual composition approaches like ViperGPT and VisProg, thanks to the\nnovel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,\nOutput Verifiers). Code and data will be made available upon paper acceptance.", "AI": {"tldr": "VLAgent\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u63a8\u7406\u7684AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6b65\u89c4\u5212\u548c\u6267\u884c\u9a8c\u8bc1\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6cdb\u5316\u6027\u80fd\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cVLAgent\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u89c4\u5212\u548c\u6267\u884c\u6a21\u5757\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "VLAgent\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u5fae\u8c03LLM\u751f\u6210\u5206\u6b65\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u6a21\u5757\u6267\u884c\u9a8c\u8bc1\uff0c\u4f18\u5316\u903b\u8f91\u9519\u8bef\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLAgent\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "VLAgent\u901a\u8fc7\u65b0\u9896\u7684\u4f18\u5316\u6a21\u5757\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.07779", "pdf": "https://arxiv.org/pdf/2506.07779", "abs": "https://arxiv.org/abs/2506.07779", "authors": ["Beining Xu", "Junxian Li"], "title": "Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion Methods", "categories": ["cs.CV"], "comment": "11 pages, 13 figures", "summary": "Visible images offer rich texture details, while infrared images emphasize\nsalient targets. Fusing these complementary modalities enhances scene\nunderstanding, particularly for advanced vision tasks under challenging\nconditions. Recently, deep learning-based fusion methods have gained attention,\nbut current evaluations primarily rely on general-purpose metrics without\nstandardized benchmarks or downstream task performance. Additionally, the lack\nof well-developed dual-spectrum datasets and fair algorithm comparisons hinders\nprogress.\n  To address these gaps, we construct a high-quality dual-spectrum dataset\ncaptured in campus environments, comprising 1,369 well-aligned visible-infrared\nimage pairs across four representative scenarios: daytime, nighttime, smoke\nocclusion, and underpasses. We also propose a comprehensive and fair evaluation\nframework that integrates fusion speed, general metrics, and object detection\nperformance using the lang-segment-anything model to ensure fairness in\ndownstream evaluation.\n  Extensive experiments benchmark several state-of-the-art fusion algorithms\nunder this framework. Results demonstrate that fusion models optimized for\ndownstream tasks achieve superior performance in target detection, especially\nin low-light and occluded scenes. Notably, some algorithms that perform well on\ngeneral metrics do not translate to strong downstream performance, highlighting\nlimitations of current evaluation practices and validating the necessity of our\nproposed framework.\n  The main contributions of this work are: (1)a campus-oriented dual-spectrum\ndataset with diverse and challenging scenes; (2) a task-aware, comprehensive\nevaluation framework; and (3) thorough comparative analysis of leading fusion\nmethods across multiple datasets, offering insights for future development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u8d28\u91cf\u7684\u53cc\u5149\u8c31\u6570\u636e\u96c6\u548c\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u7684\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u9a8c\u8bc1\uff0c\u4e14\u6570\u636e\u96c6\u4e0d\u8db3\uff0c\u963b\u788d\u4e86\u878d\u5408\u65b9\u6cd5\u7684\u8fdb\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1,369\u5bf9\u9f50\u56fe\u50cf\u5bf9\u7684\u53cc\u5149\u8c31\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u878d\u5408\u901f\u5ea6\u3001\u901a\u7528\u6307\u6807\u548c\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u4f18\u5316\u7684\u878d\u5408\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u4f4e\u5149\u548c\u906e\u6321\u573a\u666f\u3002", "conclusion": "\u8bba\u6587\u8d21\u732e\u5305\u62ec\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u4efb\u52a1\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\u548c\u878d\u5408\u65b9\u6cd5\u7684\u5168\u9762\u5206\u6790\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2506.07785", "pdf": "https://arxiv.org/pdf/2506.07785", "abs": "https://arxiv.org/abs/2506.07785", "authors": ["Qi Yang", "Chenghao Zhang", "Lubin Fan", "Kun Ding", "Jieping Ye", "Shiming Xiang"], "title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025 Spotlight. 22 pages, 16 figures", "summary": "Recent advancements in Large Vision Language Models (LVLMs) have\nsignificantly improved performance in Visual Question Answering (VQA) tasks\nthrough multimodal Retrieval-Augmented Generation (RAG). However, existing\nmethods still face challenges, such as the scarcity of knowledge with reasoning\nexamples and erratic responses from retrieved knowledge. To address these\nissues, in this study, we propose a multimodal RAG framework, termed RCTS,\nwhich enhances LVLMs by constructing a Reasoning Context-enriched knowledge\nbase and a Tree Search re-ranking method. Specifically, we introduce a\nself-consistent evaluation mechanism to enrich the knowledge base with\nintrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with\nHeuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This\nensures that LVLMs can leverage high-quality contextual reasoning for better\nand more consistent responses. Extensive experiments demonstrate that our\nframework achieves state-of-the-art performance on multiple VQA datasets,\nsignificantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.\nIt highlights the effectiveness of our knowledge base and re-ranking method in\nimproving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001RAG\u6846\u67b6RCTS\uff0c\u901a\u8fc7\u6784\u5efa\u63a8\u7406\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u77e5\u8bc6\u5e93\u548c\u6811\u641c\u7d22\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u63d0\u5347LVLMs\u5728VQA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u77e5\u8bc6\u63a8\u7406\u793a\u4f8b\u7a00\u7f3a\u548c\u68c0\u7d22\u77e5\u8bc6\u54cd\u5e94\u4e0d\u7a33\u5b9a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u81ea\u4e00\u81f4\u8bc4\u4f30\u673a\u5236\u4e30\u5bcc\u77e5\u8bc6\u5e93\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u542f\u53d1\u5f0f\u5956\u52b1\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS-HR\uff09\u91cd\u6392\u5e8f\u3002", "result": "\u5728\u591a\u4e2aVQA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eICL\u548cVanilla-RAG\u65b9\u6cd5\u3002", "conclusion": "RCTS\u6846\u67b6\u901a\u8fc7\u9ad8\u8d28\u91cf\u63a8\u7406\u4e0a\u4e0b\u6587\u548c\u91cd\u6392\u5e8f\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LVLMs\u7684\u6027\u80fd\u3002"}}
{"id": "2506.07803", "pdf": "https://arxiv.org/pdf/2506.07803", "abs": "https://arxiv.org/abs/2506.07803", "authors": ["Eduard Allakhverdov", "Dmitrii Tarasov", "Elizaveta Goncharova", "Andrey Kuznetsov"], "title": "Image Reconstruction as a Tool for Feature Analysis", "categories": ["cs.CV", "68T10, 68T30, 68T45", "I.2.10"], "comment": "23 pages, 14 figures", "summary": "Vision encoders are increasingly used in modern applications, from\nvision-only models to multimodal systems such as vision-language models.\nDespite their remarkable success, it remains unclear how these architectures\nrepresent features internally. Here, we propose a novel approach for\ninterpreting vision features via image reconstruction. We compare two related\nmodel families, SigLIP and SigLIP2, which differ only in their training\nobjective, and show that encoders pre-trained on image-based tasks retain\nsignificantly more image information than those trained on non-image tasks such\nas contrastive learning. We further apply our method to a range of vision\nencoders, ranking them by the informativeness of their feature representations.\nFinally, we demonstrate that manipulating the feature space yields predictable\nchanges in reconstructed images, revealing that orthogonal rotations (rather\nthan spatial transformations) control color encoding. Our approach can be\napplied to any vision encoder, shedding light on the inner structure of its\nfeature space. The code and model weights to reproduce the experiments are\navailable in GitHub.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u56fe\u50cf\u91cd\u5efa\u89e3\u91ca\u89c6\u89c9\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86SigLIP\u548cSigLIP2\u6a21\u578b\uff0c\u53d1\u73b0\u57fa\u4e8e\u56fe\u50cf\u4efb\u52a1\u9884\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u4fdd\u7559\u66f4\u591a\u56fe\u50cf\u4fe1\u606f\u3002\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f55\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u63ed\u793a\u4e86\u7279\u5f81\u7a7a\u95f4\u7684\u5185\u90e8\u7ed3\u6784\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u7f16\u7801\u5668\u5728\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5185\u90e8\u7279\u5f81\u8868\u793a\u65b9\u5f0f\u4ecd\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u91ca\u8fd9\u4e9b\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u56fe\u50cf\u91cd\u5efa\u6bd4\u8f83\u4e0d\u540c\u89c6\u89c9\u7f16\u7801\u5668\uff08\u5982SigLIP\u548cSigLIP2\uff09\u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u5e76\u5206\u6790\u7279\u5f81\u7a7a\u95f4\u7684\u7ed3\u6784\u3002", "result": "\u57fa\u4e8e\u56fe\u50cf\u4efb\u52a1\u9884\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u4fdd\u7559\u66f4\u591a\u56fe\u50cf\u4fe1\u606f\uff1b\u6b63\u4ea4\u65cb\u8f6c\u63a7\u5236\u989c\u8272\u7f16\u7801\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63ed\u793a\u89c6\u89c9\u7f16\u7801\u5668\u7279\u5f81\u7a7a\u95f4\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u4e3a\u7406\u89e3\u5176\u5de5\u4f5c\u539f\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.07809", "pdf": "https://arxiv.org/pdf/2506.07809", "abs": "https://arxiv.org/abs/2506.07809", "authors": ["Weilei Wen", "Tianyi Zhang", "Qianqian Zhao", "Zhaohui Zheng", "Chunle Guo", "Xiuli Shao", "Chongyi Li"], "title": "Incorporating Uncertainty-Guided and Top-k Codebook Matching for Real-World Blind Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in codebook-based real image super-resolution (SR) have\nshown promising results in real-world applications. The core idea involves\nmatching high-quality image features from a codebook based on low-resolution\n(LR) image features. However, existing methods face two major challenges:\ninaccurate feature matching with the codebook and poor texture detail\nreconstruction. To address these issues, we propose a novel Uncertainty-Guided\nand Top-k Codebook Matching SR (UGTSR) framework, which incorporates three key\ncomponents: (1) an uncertainty learning mechanism that guides the model to\nfocus on texture-rich regions, (2) a Top-k feature matching strategy that\nenhances feature matching accuracy by fusing multiple candidate features, and\n(3) an Align-Attention module that enhances the alignment of information\nbetween LR and HR features. Experimental results demonstrate significant\nimprovements in texture realism and reconstruction fidelity compared to\nexisting methods. We will release the code upon formal publication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u548cTop-k\u4ee3\u7801\u4e66\u5339\u914d\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff08UGTSR\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7279\u5f81\u5339\u914d\u4e0d\u51c6\u786e\u548c\u7eb9\u7406\u7ec6\u8282\u91cd\u5efa\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u4e66\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u5339\u914d\u4e0d\u51c6\u786e\u548c\u7eb9\u7406\u7ec6\u8282\u91cd\u5efa\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "UGTSR\u6846\u67b6\u5305\u542b\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u673a\u5236\u3001Top-k\u7279\u5f81\u5339\u914d\u7b56\u7565\u548c\u5bf9\u9f50\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUGTSR\u5728\u7eb9\u7406\u771f\u5b9e\u6027\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UGTSR\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.07811", "pdf": "https://arxiv.org/pdf/2506.07811", "abs": "https://arxiv.org/abs/2506.07811", "authors": ["Tieyuan Chen", "Huabin Liu", "Yi Wang", "Chaofan Gan", "Mingxi Lyu", "Gui Zou", "Weiyao Lin"], "title": "Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the given video, with prior work primarily focusing on identifying the\nduration of relevant segments, referred to as explicit visual evidence.\nHowever, explicit visual evidence is not always directly available,\nparticularly when questions target symbolic meanings or deeper intentions,\nleading to significant performance degradation. To fill this gap, we introduce\na novel task and dataset, $\\textbf{I}$mplicit $\\textbf{V}$ideo\n$\\textbf{Q}$uestion $\\textbf{A}$nswering (I-VQA), which focuses on answering\nquestions in scenarios where explicit visual evidence is inaccessible. Given an\nimplicit question and its corresponding video, I-VQA requires answering based\non the contextual visual cues present within the video. To tackle I-VQA, we\npropose a novel reasoning framework, IRM (Implicit Reasoning Model),\nincorporating dual-stream modeling of contextual actions and intent clues as\nimplicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the\nVisual Enhancement Module (VEM). AIM deduces and preserves question-related\ndual clues by generating clue candidates and performing relation deduction. VEM\nenhances contextual visual representation by leveraging key contextual clues.\nExtensive experiments validate the effectiveness of our IRM in I-VQA tasks,\noutperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\\%$,\n$1.37\\%$, and $4.87\\%$, respectively. Additionally, IRM performs SOTA on\nsimilar implicit advertisement understanding and future prediction in\ntraffic-VQA. Datasets and codes are available for double-blind review in\nanonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\u548c\u6570\u636e\u96c6I-VQA\uff0c\u4e13\u6ce8\u4e8e\u5728\u65e0\u6cd5\u76f4\u63a5\u83b7\u53d6\u663e\u5f0f\u89c6\u89c9\u8bc1\u636e\u7684\u60c5\u51b5\u4e0b\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u6d41\u63a8\u7406\u6846\u67b6IRM\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709VideoQA\u5de5\u4f5c\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u89c6\u89c9\u8bc1\u636e\uff0c\u4f46\u5728\u6d89\u53ca\u7b26\u53f7\u610f\u4e49\u6216\u6df1\u5c42\u610f\u56fe\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u9690\u5f0f\u89c6\u89c9\u8bc1\u636e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86IRM\u6846\u67b6\uff0c\u5305\u542bAction-Intent Module\uff08AIM\uff09\u548cVisual Enhancement Module\uff08VEM\uff09\uff0c\u901a\u8fc7\u53cc\u6d41\u5efa\u6a21\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u589e\u5f3a\u5b9e\u73b0\u9690\u5f0f\u63a8\u7406\u3002", "result": "IRM\u5728I-VQA\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u8d85\u8fc7GPT-4o\u3001OpenAI-o3\u548cVideoChat2 0.76%\u30011.37%\u548c4.87%\uff0c\u5e76\u5728\u7c7b\u4f3c\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u3002", "conclusion": "IRM\u6709\u6548\u89e3\u51b3\u4e86\u9690\u5f0f\u89c6\u89c9\u8bc1\u636e\u7684\u63a8\u7406\u95ee\u9898\uff0c\u4e3aVideoQA\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07813", "pdf": "https://arxiv.org/pdf/2506.07813", "abs": "https://arxiv.org/abs/2506.07813", "authors": ["Junseo Bang", "Joonhee Lee", "Kyeonghyun Lee", "Haechang Lee", "Dong Un Kang", "Se Young Chun"], "title": "Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Arbitrary-scale image super-resolution aims to upsample images to any desired\nresolution, offering greater flexibility than traditional fixed-scale\nsuper-resolution. Recent approaches in this domain utilize regression-based or\ngenerative models, but many of them are a single-stage upsampling process,\nwhich may be challenging to learn across a wide, continuous distribution of\nscaling factors. Progressive upsampling strategies have shown promise in\nmitigating this issue, yet their integration with diffusion models for flexible\nupscaling remains underexplored. Here, we present CasArbi, a novel\nself-cascaded diffusion framework for arbitrary-scale image super-resolution.\nCasArbi meets the varying scaling demands by breaking them down into smaller\nsequential factors and progressively enhancing the image resolution at each\nstep with seamless transitions for arbitrary scales. Our novel\ncoordinate-guided residual diffusion model allows for the learning of\ncontinuous image representations while enabling efficient diffusion sampling.\nExtensive experiments demonstrate that our CasArbi outperforms prior arts in\nboth perceptual and distortion performance metrics across diverse\narbitrary-scale super-resolution benchmarks.", "AI": {"tldr": "CasArbi\u662f\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u7ea7\u8054\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u4efb\u610f\u5c3a\u5ea6\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u5f3a\u5206\u8fa8\u7387\u5b9e\u73b0\u7075\u6d3b\u4e0a\u91c7\u6837\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u8fde\u7eed\u5c3a\u5ea6\u5206\u5e03\u4e0a\u5b66\u4e60\u56f0\u96be\u3002", "method": "\u91c7\u7528\u81ea\u7ea7\u8054\u6269\u6563\u6846\u67b6\uff0c\u5c06\u5927\u5c3a\u5ea6\u5206\u89e3\u4e3a\u5c0f\u5c3a\u5ea6\u5e8f\u5217\uff0c\u7ed3\u5408\u5750\u6807\u5f15\u5bfc\u6b8b\u5dee\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u591a\u79cd\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCasArbi\u5728\u611f\u77e5\u548c\u5931\u771f\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CasArbi\u901a\u8fc7\u9010\u6b65\u6269\u6563\u548c\u8fde\u7eed\u8868\u793a\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u3002"}}
{"id": "2506.07814", "pdf": "https://arxiv.org/pdf/2506.07814", "abs": "https://arxiv.org/abs/2506.07814", "authors": ["Yongzhen Wang", "Yongjun Li", "Zhuoran Zheng", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "13 pages, 8 figures, 3 tables", "summary": "Natural images are often degraded by complex, composite degradations such as\nrain, snow, and haze, which adversely impact downstream vision applications.\nWhile existing image restoration efforts have achieved notable success, they\nare still hindered by two critical challenges: limited generalization across\ndynamically varying degradation scenarios and a suboptimal balance between\npreserving local details and modeling global dependencies. To overcome these\nchallenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based\nMamba-CNN fusion framework for efficient and robust all-in-one image\nrestoration. M2Restore introduces three key contributions: First, to boost the\nmodel's generalization across diverse degradation conditions, we exploit a\nCLIP-guided MoE gating mechanism that fuses task-conditioned prompts with\nCLIP-derived semantic priors. This mechanism is further refined via cross-modal\nfeature calibration, which enables precise expert selection for various\ndegradation types. Second, to jointly capture global contextual dependencies\nand fine-grained local details, we design a dual-stream architecture that\nintegrates the localized representational strength of CNNs with the long-range\nmodeling efficiency of Mamba. This integration enables collaborative\noptimization of global semantic relationships and local structural fidelity,\npreserving global coherence while enhancing detail restoration. Third, we\nintroduce an edge-aware dynamic gating mechanism that adaptively balances\nglobal modeling and local enhancement by reallocating computational attention\nto degradation-sensitive regions. This targeted focus leads to more efficient\nand precise restoration. Extensive experiments across multiple image\nrestoration benchmarks validate the superiority of M2Restore in both visual\nquality and quantitative performance.", "AI": {"tldr": "M2Restore\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMixture-of-Experts\u7684Mamba-CNN\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u5168\u80fd\u56fe\u50cf\u6062\u590d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u9000\u5316\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u4f9d\u8d56\u5e73\u8861\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u7136\u56fe\u50cf\u5e38\u53d7\u590d\u6742\u9000\u5316\uff08\u5982\u96e8\u3001\u96ea\u3001\u96fe\uff09\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u9000\u5316\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u96be\u4ee5\u5e73\u8861\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u4f9d\u8d56\u3002", "method": "1. \u5229\u7528CLIP\u5f15\u5bfc\u7684MoE\u95e8\u63a7\u673a\u5236\u878d\u5408\u4efb\u52a1\u6761\u4ef6\u63d0\u793a\u548c\u8bed\u4e49\u5148\u9a8c\uff1b2. \u8bbe\u8ba1\u53cc\u6d41\u67b6\u6784\u7ed3\u5408CNN\u7684\u5c40\u90e8\u8868\u5f81\u548cMamba\u7684\u957f\u7a0b\u5efa\u6a21\uff1b3. \u5f15\u5165\u8fb9\u7f18\u611f\u77e5\u52a8\u6001\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u5e73\u8861\u5168\u5c40\u4e0e\u5c40\u90e8\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u6062\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cM2Restore\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "M2Restore\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2506.07826", "pdf": "https://arxiv.org/pdf/2506.07826", "abs": "https://arxiv.org/abs/2506.07826", "authors": ["William Ljungbergh", "Bernardo Taveira", "Wenzhao Zheng", "Adam Tonderski", "Chensheng Peng", "Fredrik Kahl", "Christoffer Petersson", "Michael Felsberg", "Kurt Keutzer", "Masayoshi Tomizuka", "Wei Zhan"], "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Validating autonomous driving (AD) systems requires diverse and\nsafety-critical testing, making photorealistic virtual environments essential.\nTraditional simulation platforms, while controllable, are resource-intensive to\nscale and often suffer from a domain gap with real-world data. In contrast,\nneural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a\nscalable solution for creating photorealistic digital twins of real-world\ndriving scenes. However, they struggle with dynamic object manipulation and\nreusability as their per-scene optimization-based methodology tends to result\nin incomplete object models with integrated illumination effects. This paper\nintroduces R3D2, a lightweight, one-step diffusion model designed to overcome\nthese limitations and enable realistic insertion of complete 3D assets into\nexisting scenes by generating plausible rendering effects-such as shadows and\nconsistent lighting-in real time. This is achieved by training R3D2 on a novel\ndataset: 3DGS object assets are generated from in-the-wild AD data using an\nimage-conditioned 3D generative model, and then synthetically placed into\nneural rendering-based virtual environments, allowing R3D2 to learn realistic\nintegration. Quantitative and qualitative evaluations demonstrate that R3D2\nsignificantly enhances the realism of inserted assets, enabling use-cases like\ntext-to-3D asset insertion and cross-scene/dataset object transfer, allowing\nfor true scalability in AD validation. To promote further research in scalable\nand realistic AD simulation, we will release our dataset and code, see\nhttps://research.zenseact.com/publications/R3D2/.", "AI": {"tldr": "R3D2\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u81ea\u52a8\u9a7e\u9a76\u9a8c\u8bc1\u4e2d\u5b9e\u73b0\u771f\u5b9e3D\u8d44\u4ea7\u63d2\u5165\uff0c\u63d0\u5347\u573a\u666f\u771f\u5b9e\u611f\u3002", "motivation": "\u4f20\u7edf\u4eff\u771f\u5e73\u53f0\u8d44\u6e90\u5bc6\u96c6\u4e14\u5b58\u5728\u9886\u57df\u5dee\u8ddd\uff0c\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\u59823DGS\u96be\u4ee5\u52a8\u6001\u64cd\u4f5c\u548c\u590d\u7528\u3002", "method": "R3D2\u901a\u8fc7\u8bad\u7ec3\u65b0\u578b\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u5b9e\u65f6\u751f\u6210\u903c\u771f\u6e32\u67d3\u6548\u679c\uff08\u5982\u9634\u5f71\u548c\u5149\u7167\uff09\u3002", "result": "R3D2\u663e\u8457\u63d0\u5347\u63d2\u5165\u8d44\u4ea7\u7684\u771f\u5b9e\u611f\uff0c\u652f\u6301\u6587\u672c\u52303D\u63d2\u5165\u548c\u8de8\u573a\u666f\u5bf9\u8c61\u8f6c\u79fb\u3002", "conclusion": "R3D2\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u903c\u771f\u7684\u4eff\u771f\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.07841", "pdf": "https://arxiv.org/pdf/2506.07841", "abs": "https://arxiv.org/abs/2506.07841", "authors": ["Elizabeth Pavlova", "Xue-Xin Wei"], "title": "Diffusion models under low-noise regime", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent work on diffusion models proposed that they operate in two regimes:\nmemorization, in which models reproduce their training data, and\ngeneralization, in which they generate novel samples. While this has been\ntested in high-noise settings, the behavior of diffusion models as effective\ndenoisers when the corruption level is small remains unclear. To address this\ngap, we systematically investigated the behavior of diffusion models under\nlow-noise diffusion dynamics, with implications for model robustness and\ninterpretability. Using (i) CelebA subsets of varying sample sizes and (ii)\nanalytic Gaussian mixture benchmarks, we reveal that models trained on disjoint\ndata diverge near the data manifold even when their high-noise outputs\nconverge. We quantify how training set size, data geometry, and model objective\nchoice shape denoising trajectories and affect score accuracy, providing\ninsights into how these models actually learn representations of data\ndistributions. This work starts to address gaps in our understanding of\ngenerative model reliability in practical applications where small\nperturbations are common.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u4f4e\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u7814\u7a76\uff0c\u63ed\u793a\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3001\u51e0\u4f55\u7ed3\u6784\u548c\u6a21\u578b\u76ee\u6807\u5bf9\u53bb\u566a\u8f68\u8ff9\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u6a21\u578b\u5728\u5c0f\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528CelebA\u5b50\u96c6\u548c\u89e3\u6790\u9ad8\u65af\u6df7\u5408\u57fa\u51c6\uff0c\u5206\u6790\u6a21\u578b\u5728\u4f4e\u566a\u58f0\u6269\u6563\u52a8\u529b\u5b66\u4e0b\u7684\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u6570\u636e\u6d41\u5f62\u9644\u8fd1\u7684\u884c\u4e3a\u4f1a\u56e0\u8bad\u7ec3\u6570\u636e\u4e0d\u540c\u800c\u5206\u5316\uff0c\u5373\u4f7f\u5728\u9ad8\u566a\u58f0\u8f93\u51fa\u65f6\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u6269\u6563\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u6570\u636e\u5206\u5e03\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u5c0f\u6270\u52a8\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.07847", "pdf": "https://arxiv.org/pdf/2506.07847", "abs": "https://arxiv.org/abs/2506.07847", "authors": ["Hengzhi Chen", "Liqian Feng", "Wenhua Wu", "Xiaogang Zhu", "Shawn Leo", "Kun Hu"], "title": "F2Net: A Frequency-Fused Network for Ultra-High Resolution Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation of ultra-high-resolution (UHR) remote sensing imagery\nis critical for applications like environmental monitoring and urban planning\nbut faces computational and optimization challenges. Conventional methods\neither lose fine details through downsampling or fragment global context via\npatch processing. While multi-branch networks address this trade-off, they\nsuffer from computational inefficiency and conflicting gradient dynamics during\ntraining. We propose F2Net, a frequency-aware framework that decomposes UHR\nimages into high- and low-frequency components for specialized processing. The\nhigh-frequency branch preserves full-resolution structural details, while the\nlow-frequency branch processes downsampled inputs through dual sub-branches\ncapturing short- and long-range dependencies. A Hybrid-Frequency Fusion module\nintegrates these observations, guided by two novel objectives: Cross-Frequency\nAlignment Loss ensures semantic consistency between frequency components, and\nCross-Frequency Balance Loss regulates gradient magnitudes across branches to\nstabilize training. Evaluated on DeepGlobe and Inria Aerial benchmarks, F2Net\nachieves state-of-the-art performance with mIoU of 80.22 and 83.39,\nrespectively. Our code will be publicly available.", "AI": {"tldr": "F2Net\u662f\u4e00\u79cd\u9891\u7387\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u8d85\u9ad8\u6e05\u9065\u611f\u56fe\u50cf\u4e3a\u9ad8\u9891\u548c\u4f4e\u9891\u7ec4\u4ef6\u8fdb\u884c\u4e13\u95e8\u5904\u7406\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7ec6\u8282\u4e22\u5931\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u788e\u7247\u5316\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u8d85\u9ad8\u6e05\u9065\u611f\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u5728\u73af\u5883\u76d1\u6d4b\u548c\u57ce\u5e02\u89c4\u5212\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u7ec6\u8282\u4e22\u5931\u6216\u5168\u5c40\u4e0a\u4e0b\u6587\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u591a\u5206\u652f\u7f51\u7edc\u5219\u6548\u7387\u4f4e\u4e14\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "F2Net\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u9ad8\u9891\u548c\u4f4e\u9891\u7ec4\u4ef6\uff0c\u9ad8\u9891\u5206\u652f\u4fdd\u7559\u5168\u5206\u8fa8\u7387\u7ec6\u8282\uff0c\u4f4e\u9891\u5206\u652f\u901a\u8fc7\u53cc\u5b50\u5206\u652f\u6355\u83b7\u77ed\u7a0b\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u9891\u7387\u878d\u5408\u6a21\u5757\u6574\u5408\u3002", "result": "\u5728DeepGlobe\u548cInria Aerial\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cF2Net\u5206\u522b\u8fbe\u523080.22\u548c83.39\u7684mIoU\uff0c\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "F2Net\u901a\u8fc7\u9891\u7387\u5206\u89e3\u548c\u878d\u5408\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u8d85\u9ad8\u6e05\u56fe\u50cf\u8bed\u4e49\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.07848", "pdf": "https://arxiv.org/pdf/2506.07848", "abs": "https://arxiv.org/abs/2506.07848", "authors": ["Teng Hu", "Zhentao Yu", "Zhengguang Zhou", "Jiangning Zhang", "Yuan Zhou", "Qinglin Lu", "Ran Yi"], "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.", "AI": {"tldr": "PolyVivid\u662f\u4e00\u4e2a\u591a\u4e3b\u4f53\u89c6\u9891\u5b9a\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c-\u56fe\u50cf\u878d\u5408\u6a21\u5757\u548c3D-RoPE\u589e\u5f3a\u6a21\u5757\u5b9e\u73b0\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u4ea4\u4e92\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u591a\u4e3b\u4f53\u5b9a\u5236\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5c24\u5176\u662f\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u4ea4\u4e92\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86VLLM\u6587\u672c-\u56fe\u50cf\u878d\u5408\u6a21\u5757\u30013D-RoPE\u589e\u5f3a\u6a21\u5757\u548c\u6ce8\u610f\u529b\u7ee7\u627f\u8eab\u4efd\u6ce8\u5165\u6a21\u5757\uff0c\u7ed3\u5408MLLM\u6570\u636e\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePolyVivid\u5728\u8eab\u4efd\u4fdd\u771f\u5ea6\u3001\u89c6\u9891\u771f\u5b9e\u6027\u548c\u4e3b\u4f53\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PolyVivid\u5728\u591a\u4e3b\u4f53\u89c6\u9891\u5b9a\u5236\u4e2d\u5b9e\u73b0\u4e86\u7075\u6d3b\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.07850", "pdf": "https://arxiv.org/pdf/2506.07850", "abs": "https://arxiv.org/abs/2506.07850", "authors": ["Arash Rocky", "Q. M. Jonathan Wu"], "title": "SAM2Auto: Auto Annotation Using FLASH", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) lag behind Large Language Models due to the\nscarcity of annotated datasets, as creating paired visual-textual annotations\nis labor-intensive and expensive. To address this bottleneck, we introduce\nSAM2Auto, the first fully automated annotation pipeline for video datasets\nrequiring no human intervention or dataset-specific training. Our approach\nconsists of two key components: SMART-OD, a robust object detection system that\ncombines automatic mask generation with open-world object detection\ncapabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a\nmulti-object real-time video instance segmentation (VIS) that maintains\nconsistent object identification across video frames even with intermittent\ndetection gaps. Unlike existing open-world detection methods that require\nframe-specific hyperparameter tuning and suffer from numerous false positives,\nour system employs statistical approaches to minimize detection errors while\nensuring consistent object tracking throughout entire video sequences.\nExtensive experimental validation demonstrates that SAM2Auto achieves\ncomparable accuracy to manual annotation while dramatically reducing annotation\ntime and eliminating labor costs. The system successfully handles diverse\ndatasets without requiring retraining or extensive parameter adjustments,\nmaking it a practical solution for large-scale dataset creation. Our work\nestablishes a new baseline for automated video annotation and provides a\npathway for accelerating VLM development by addressing the fundamental dataset\nbottleneck that has constrained progress in vision-language understanding.", "AI": {"tldr": "SAM2Auto\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u89c6\u9891\u6570\u636e\u96c6\u6807\u6ce8\u7cfb\u7edf\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u6570\u636e\u96c6\u7279\u5b9a\u8bad\u7ec3\uff0c\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u65f6\u95f4\u548c\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u56e0\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u800c\u53d1\u5c55\u6ede\u540e\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u6807\u6ce8\u65b9\u6cd5\u8d39\u65f6\u8d39\u529b\u3002", "method": "\u7ed3\u5408SMART-OD\uff08\u81ea\u52a8\u63a9\u7801\u751f\u6210\u4e0e\u5f00\u653e\u4e16\u754c\u76ee\u6807\u68c0\u6d4b\uff09\u548cFLASH\uff08\u5b9e\u65f6\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\uff09\uff0c\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u51cf\u5c11\u68c0\u6d4b\u9519\u8bef\u5e76\u4fdd\u6301\u5bf9\u8c61\u8ddf\u8e2a\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAM2Auto\u7684\u6807\u6ce8\u7cbe\u5ea6\u4e0e\u4eba\u5de5\u76f8\u5f53\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u6570\u636e\u96c6\u3002", "conclusion": "SAM2Auto\u4e3a\u81ea\u52a8\u5316\u89c6\u9891\u6807\u6ce8\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u6570\u636e\u96c6\u74f6\u9888\uff0c\u52a0\u901fVLM\u53d1\u5c55\u3002"}}
{"id": "2506.07857", "pdf": "https://arxiv.org/pdf/2506.07857", "abs": "https://arxiv.org/abs/2506.07857", "authors": ["Zihui Zhang", "Weisheng Dai", "Hongtao Wen", "Bo Yang"], "title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/LogoSP", "summary": "We study the problem of unsupervised 3D semantic segmentation on raw point\nclouds without needing human labels in training. Existing methods usually\nformulate this problem into learning per-point local features followed by a\nsimple grouping strategy, lacking the ability to discover additional and\npossibly richer semantic priors beyond local features. In this paper, we\nintroduce LogoSP to learn 3D semantics from both local and global point\nfeatures. The key to our approach is to discover 3D semantic information by\ngrouping superpoints according to their global patterns in the frequency\ndomain, thus generating highly accurate semantic pseudo-labels for training a\nsegmentation network. Extensive experiments on two indoor and an outdoor\ndatasets show that our LogoSP surpasses all existing unsupervised methods by\nlarge margins, achieving the state-of-the-art performance for unsupervised 3D\nsemantic segmentation. Notably, our investigation into the learned global\npatterns reveals that they truly represent meaningful 3D semantics in the\nabsence of human labels during training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLogoSP\u7684\u65e0\u76d1\u77633D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u70b9\u7279\u5f81\u5b66\u4e60\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u5728\u9891\u57df\u4e2d\u5229\u7528\u5168\u5c40\u6a21\u5f0f\u751f\u6210\u9ad8\u7cbe\u5ea6\u4f2a\u6807\u7b7e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5c40\u90e8\u7279\u5f81\uff0c\u7f3a\u4e4f\u53d1\u73b0\u66f4\u4e30\u5bcc\u8bed\u4e49\u5148\u9a8c\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "method": "LogoSP\u901a\u8fc7\u9891\u57df\u4e2d\u7684\u5168\u5c40\u6a21\u5f0f\u5bf9\u8d85\u70b9\u8fdb\u884c\u5206\u7ec4\uff0c\u751f\u6210\u8bed\u4e49\u4f2a\u6807\u7b7e\uff0c\u7528\u4e8e\u8bad\u7ec3\u5206\u5272\u7f51\u7edc\u3002", "result": "\u5728\u4e24\u4e2a\u5ba4\u5185\u548c\u4e00\u4e2a\u5ba4\u5916\u6570\u636e\u96c6\u4e0a\uff0cLogoSP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LogoSP\u8bc1\u660e\u4e86\u5728\u65e0\u4eba\u5de5\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\uff0c\u5168\u5c40\u6a21\u5f0f\u80fd\u591f\u6709\u6548\u8868\u793a\u6709\u610f\u4e49\u76843D\u8bed\u4e49\u4fe1\u606f\u3002"}}
{"id": "2506.07860", "pdf": "https://arxiv.org/pdf/2506.07860", "abs": "https://arxiv.org/abs/2506.07860", "authors": ["Ivan Alberico", "Marco Cannici", "Giovanni Cioffi", "Davide Scaramuzza"], "title": "Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction", "categories": ["cs.CV"], "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), Nashville (TN), USA, 2025; 5th International Workshop on\n  Event-Based Vision", "summary": "In this paper, we present a real-time egocentric trajectory prediction system\nfor table tennis using event cameras. Unlike standard cameras, which suffer\nfrom high latency and motion blur at fast ball speeds, event cameras provide\nhigher temporal resolution, allowing more frequent state updates, greater\nrobustness to outliers, and accurate trajectory predictions using just a short\ntime window after the opponent's impact. We collect a dataset of ping-pong game\nsequences, including 3D ground-truth trajectories of the ball, synchronized\nwith sensor data from the Meta Project Aria glasses and event streams. Our\nsystem leverages foveated vision, using eye-gaze data from the glasses to\nprocess only events in the viewer's fovea. This biologically inspired approach\nimproves ball detection performance and significantly reduces computational\nlatency, as it efficiently allocates resources to the most perceptually\nrelevant regions, achieving a reduction factor of 10.81 on the collected\ntrajectories. Our detection pipeline has a worst-case total latency of 4.5 ms,\nincluding computation and perception - significantly lower than a frame-based\n30 FPS system, which, in the worst case, takes 66 ms solely for perception.\nFinally, we fit a trajectory prediction model to the estimated states of the\nball, enabling 3D trajectory forecasting in the future. To the best of our\nknowledge, this is the first approach to predict table tennis trajectories from\nan egocentric perspective using event cameras.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5b9e\u65f6\u4e52\u4e53\u7403\u8f68\u8ff9\u9884\u6d4b\u7cfb\u7edf\uff0c\u5229\u7528\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u6ce8\u89c6\u6570\u636e\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u76f8\u673a\u5728\u9ad8\u901f\u5ea6\u4e52\u4e53\u7403\u8fd0\u52a8\u4e2d\u5b58\u5728\u7684\u5ef6\u8fdf\u548c\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u548c\u6ce8\u89c6\u6570\u636e\uff0c\u7ed3\u5408\u751f\u7269\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f18\u5316\u68c0\u6d4b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u7cfb\u7edf\u603b\u5ef6\u8fdf\u4f4e\u81f34.5\u6beb\u79d2\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u534710.81\u500d\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u4e52\u4e53\u7403\u8f68\u8ff9\u9884\u6d4b\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5e27\u76f8\u673a\u7cfb\u7edf\u3002"}}
{"id": "2506.07863", "pdf": "https://arxiv.org/pdf/2506.07863", "abs": "https://arxiv.org/abs/2506.07863", "authors": ["Lev Novitskiy", "Viacheslav Vasilev", "Maria Kovaleva", "Vladimir Arkhipkin", "Denis Dimitrov"], "title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer\nvision, yet their training is often plagued by artifacts that degrade\nreconstruction and generation quality. This paper introduces VIVAT, a\nsystematic approach to mitigating common artifacts in KL-VAE training without\nrequiring radical architectural changes. We present a detailed taxonomy of five\nprevalent artifacts - color shift, grid patterns, blur, corner and droplet\nartifacts - and analyze their root causes. Through straightforward\nmodifications, including adjustments to loss weights, padding strategies, and\nthe integration of Spatially Conditional Normalization, we demonstrate\nsignificant improvements in VAE performance. Our method achieves\nstate-of-the-art results in image reconstruction metrics (PSNR and SSIM) across\nmultiple benchmarks and enhances text-to-image generation quality, as evidenced\nby superior CLIP scores. By preserving the simplicity of the KL-VAE framework\nwhile addressing its practical challenges, VIVAT offers actionable insights for\nresearchers and practitioners aiming to optimize VAE training.", "AI": {"tldr": "VIVAT\u901a\u8fc7\u7b80\u5355\u8c03\u6574KL-VAE\u8bad\u7ec3\u4e2d\u7684\u635f\u5931\u6743\u91cd\u3001\u586b\u5145\u7b56\u7565\u548c\u5f15\u5165\u7a7a\u95f4\u6761\u4ef6\u5f52\u4e00\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e94\u79cd\u5e38\u89c1\u4f2a\u5f71\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u91cd\u5efa\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "KL-VAE\u8bad\u7ec3\u4e2d\u5e38\u89c1\u7684\u4f2a\u5f71\uff08\u5982\u989c\u8272\u504f\u79fb\u3001\u7f51\u683c\u6a21\u5f0f\u7b49\uff09\u964d\u4f4e\u4e86\u91cd\u5efa\u548c\u751f\u6210\u8d28\u91cf\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u63d0\u51faVIVAT\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u635f\u5931\u6743\u91cd\u3001\u4f18\u5316\u586b\u5145\u7b56\u7565\u548c\u5f15\u5165Spatially Conditional Normalization\uff0c\u7cfb\u7edf\u6027\u51cf\u5c11\u4f2a\u5f71\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVIVAT\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684CLIP\u5206\u6570\u3002", "conclusion": "VIVAT\u5728\u4fdd\u6301KL-VAE\u6846\u67b6\u7b80\u5355\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bad\u7ec3\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u4e3a\u4f18\u5316VAE\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2506.07865", "pdf": "https://arxiv.org/pdf/2506.07865", "abs": "https://arxiv.org/abs/2506.07865", "authors": ["Jinxi Li", "Ziyang Song", "Siyuan Zhou", "Bo Yang"], "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/FreeGave", "summary": "In this paper, we aim to model 3D scene geometry, appearance, and the\nunderlying physics purely from multi-view videos. By applying various governing\nPDEs as PINN losses or incorporating physics simulation into neural networks,\nexisting works often fail to learn complex physical motions at boundaries or\nrequire object priors such as masks or types. In this paper, we propose\nFreeGave to learn the physics of complex dynamic 3D scenes without needing any\nobject priors. The key to our approach is to introduce a physics code followed\nby a carefully designed divergence-free module for estimating a per-Gaussian\nvelocity field, without relying on the inefficient PINN losses. Extensive\nexperiments on three public datasets and a newly collected challenging\nreal-world dataset demonstrate the superior performance of our method for\nfuture frame extrapolation and motion segmentation. Most notably, our\ninvestigation into the learned physics codes reveals that they truly learn\nmeaningful 3D physical motion patterns in the absence of any human labels in\ntraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFreeGave\u65b9\u6cd5\uff0c\u65e0\u9700\u7269\u4f53\u5148\u9a8c\u5373\u53ef\u5b66\u4e60\u590d\u6742\u52a8\u60013D\u573a\u666f\u7684\u7269\u7406\u7279\u6027\uff0c\u901a\u8fc7\u8bbe\u8ba1\u65e0\u6563\u5ea6\u6a21\u5757\u4f30\u8ba1\u901f\u5ea6\u573a\uff0c\u907f\u514d\u4e86\u4f4e\u6548\u7684PINN\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u8fb9\u754c\u590d\u6742\u7269\u7406\u8fd0\u52a8\u6216\u4f9d\u8d56\u7269\u4f53\u5148\u9a8c\uff08\u5982\u63a9\u7801\u6216\u7c7b\u578b\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u7269\u7406\u7f16\u7801\u5e76\u8bbe\u8ba1\u65e0\u6563\u5ea6\u6a21\u5757\uff0c\u4f30\u8ba1\u6bcf\u9ad8\u65af\u901f\u5ea6\u573a\uff0c\u907f\u514d\u4f7f\u7528PINN\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u5e27\u5916\u63a8\u548c\u8fd0\u52a8\u5206\u5272\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u7269\u7406\u7f16\u7801\u6210\u529f\u5b66\u4e60\u65e0\u6807\u7b7e\u76843D\u7269\u7406\u8fd0\u52a8\u6a21\u5f0f\u3002", "conclusion": "FreeGave\u65e0\u9700\u5148\u9a8c\u5373\u53ef\u5b66\u4e60\u590d\u6742\u7269\u7406\u573a\u666f\uff0c\u4e14\u7269\u7406\u7f16\u7801\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2506.07878", "pdf": "https://arxiv.org/pdf/2506.07878", "abs": "https://arxiv.org/abs/2506.07878", "authors": ["Muhammad Ahmed Humais", "Xiaoqian Huang", "Hussain Sajwani", "Sajid Javed", "Yahya Zweiri"], "title": "Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras unlock new frontiers that were previously unthinkable with\nstandard frame-based cameras. One notable example is low-latency motion\nestimation (optical flow), which is critical for many real-time applications.\nIn such applications, the computational efficiency of algorithms is paramount.\nAlthough recent deep learning paradigms such as CNN, RNN, or ViT have shown\nremarkable performance, they often lack the desired computational efficiency.\nConversely, asynchronous event-based methods including SNNs and GNNs are\ncomputationally efficient; however, these approaches fail to capture sufficient\nspatio-temporal information, a powerful feature required to achieve better\nperformance for optical flow estimation. In this work, we introduce\nSpatio-Temporal State Space Model (STSSM) module along with a novel network\narchitecture to develop an extremely efficient solution with competitive\nperformance. Our STSSM module leverages state-space models to effectively\ncapture spatio-temporal correlations in event data, offering higher performance\nwith lower complexity compared to ViT, CNN-based architectures in similar\nsettings. Our model achieves 4.5x faster inference and 8x lower computations\ncompared to TMA and 2x lower computations compared to EV-FlowNet with\ncompetitive performance on the DSEC benchmark. Our code will be available at\nhttps://github.com/AhmedHumais/E-STMFlow", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684STSSM\u6a21\u5757\u548c\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u4e8b\u4ef6\u76f8\u673a\u8fd0\u52a8\u4f30\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u4f4e\u5ef6\u8fdf\u8fd0\u52a8\u4f30\u8ba1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\uff0c\u800c\u5f02\u6b65\u4e8b\u4ef6\u65b9\u6cd5\u53c8\u7f3a\u4e4f\u65f6\u7a7a\u4fe1\u606f\u6355\u6349\u80fd\u529b\u3002", "method": "\u5f15\u5165STSSM\u6a21\u5757\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u9ad8\u6548\u6355\u6349\u4e8b\u4ef6\u6570\u636e\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u8bbe\u8ba1\u65b0\u578b\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5728DSEC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6bd4TMA\u5feb4.5\u500d\uff0c\u8ba1\u7b97\u91cf\u6bd4EV-FlowNet\u4f4e2\u500d\uff0c\u6027\u80fd\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "STSSM\u6a21\u5757\u4e3a\u4e8b\u4ef6\u76f8\u673a\u7684\u8fd0\u52a8\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u5f02\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07885", "pdf": "https://arxiv.org/pdf/2506.07885", "abs": "https://arxiv.org/abs/2506.07885", "authors": ["Zubin Bhuyan", "Yuanchang Xie", "AngkeaReach Rith", "Xintong Yan", "Nasko Apostolov", "Jimi Oke", "Chengbo Ai"], "title": "CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing", "categories": ["cs.CV"], "comment": null, "summary": "With the increasing availability of aerial and satellite imagery, deep\nlearning presents significant potential for transportation asset management,\nsafety analysis, and urban planning. This study introduces CrosswalkNet, a\nrobust and efficient deep learning framework designed to detect various types\nof pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet\nincorporates a novel detection approach that improves upon traditional object\ndetection strategies by utilizing oriented bounding boxes (OBB), enhancing\ndetection precision by accurately capturing crosswalks regardless of their\norientation. Several optimization techniques, including Convolutional Block\nAttention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine\nannealing, are implemented to maximize performance and efficiency. A\ncomprehensive dataset comprising over 23,000 annotated crosswalk instances is\nutilized to train and validate the proposed framework. The best-performing\nmodel achieves an impressive precision of 96.5% and a recall of 93.3% on aerial\nimagery from Massachusetts, demonstrating its accuracy and effectiveness.\nCrosswalkNet has also been successfully applied to datasets from New Hampshire,\nVirginia, and Maine without transfer learning or fine-tuning, showcasing its\nrobustness and strong generalization capability. Additionally, the crosswalk\ndetection results, processed using High-Performance Computing (HPC) platforms\nand provided in polygon shapefile format, have been shown to accelerate data\nprocessing and detection, supporting real-time analysis for safety and mobility\napplications. This integration offers policymakers, transportation engineers,\nand urban planners an effective instrument to enhance pedestrian safety and\nimprove urban mobility.", "AI": {"tldr": "CrosswalkNet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u9ad8\u5206\u8fa8\u7387\u822a\u62cd\u56fe\u50cf\u4e2d\u68c0\u6d4b\u884c\u4eba\u6a2a\u9053\uff0c\u91c7\u7528\u5b9a\u5411\u8fb9\u754c\u6846\uff08OBB\uff09\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u822a\u62cd\u548c\u536b\u661f\u56fe\u50cf\u7684\u666e\u53ca\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u4ea4\u901a\u8d44\u4ea7\u7ba1\u7406\u3001\u5b89\u5168\u5206\u6790\u548c\u57ce\u5e02\u89c4\u5212\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u63d0\u51faCrosswalkNet\u6846\u67b6\uff0c\u7ed3\u5408OBB\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u4f18\u5316\u6280\u672f\uff0c\u5229\u752823,000\u591a\u4e2a\u6807\u6ce8\u6837\u672c\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728Massachusetts\u6570\u636e\u96c6\u4e0a\u8fbe\u523096.5%\u7684\u7cbe\u786e\u7387\u548c93.3%\u7684\u53ec\u56de\u7387\uff0c\u5e76\u5728\u5176\u4ed6\u5dde\u6570\u636e\u96c6\u4e0a\u65e0\u9700\u5fae\u8c03\u5373\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CrosswalkNet\u4e3a\u51b3\u7b56\u8005\u548c\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u53ef\u63d0\u5347\u884c\u4eba\u5b89\u5168\u548c\u57ce\u5e02\u6d41\u52a8\u6027\u3002"}}
{"id": "2506.07886", "pdf": "https://arxiv.org/pdf/2506.07886", "abs": "https://arxiv.org/abs/2506.07886", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEgoM2P\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u65f6\u95f4\u6807\u8bb0\u5668\u548c\u63a9\u7801\u5efa\u6a21\u89e3\u51b3\u591a\u6a21\u6001\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u652f\u6301\u591a\u4efb\u52a1\u5904\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u4e13\u4e1a\u6a21\u578b\u3002", "motivation": "\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u5982RGB\u89c6\u9891\u3001\u6df1\u5ea6\u3001\u76f8\u673a\u4f4d\u59ff\u548c\u89c6\u7ebf\uff09\u5728\u589e\u5f3a\u73b0\u5b9e\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u6570\u636e\u5f02\u6784\u6027\u548c\u7f3a\u5931\u6a21\u6001\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u56f0\u96be\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u5f15\u5165\u9ad8\u6548\u65f6\u95f4\u6807\u8bb0\u5668\uff0c\u63d0\u51faEgoM2P\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u5efa\u6a21\u5b66\u4e60\u591a\u6a21\u6001\u65f6\u95f4\u6807\u8bb0\uff0c\u652f\u6301\u591a\u4efb\u52a1\u5904\u7406\uff08\u5982\u89c6\u7ebf\u9884\u6d4b\u3001\u76f8\u673a\u8ddf\u8e2a\u3001\u6df1\u5ea6\u4f30\u8ba1\uff09\u3002", "result": "EgoM2P\u5728\u591a\u4efb\u52a1\u4e2d\u6027\u80fd\u5339\u914d\u6216\u4f18\u4e8e\u4e13\u4e1a\u6a21\u578b\uff0c\u4e14\u901f\u5ea6\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "EgoM2P\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u5f00\u6e90\u4ee5\u63a8\u52a8\u7814\u7a76\u3002"}}
{"id": "2506.07891", "pdf": "https://arxiv.org/pdf/2506.07891", "abs": "https://arxiv.org/abs/2506.07891", "authors": ["Simone Facchiano", "Stefano Saravalle", "Matteo Migliarini", "Edoardo De Matteis", "Alessio Sampieri", "Andrea Pilzer", "Emanuele Rodol\u00e0", "Indro Spinelli", "Luca Franco", "Fabio Galasso"], "title": "Video Unlearning via Low-Rank Refusal Vector", "categories": ["cs.CV"], "comment": null, "summary": "Video generative models democratize the creation of visual content through\nintuitive instruction following, but they also inherit the biases and harmful\nconcepts embedded within their web-scale training data. This inheritance\ncreates a significant risk, as users can readily generate undesirable and even\nillegal content. This work introduces the first unlearning technique tailored\nexplicitly for video diffusion models to address this critical issue. Our\nmethod requires 5 multi-modal prompt pairs only. Each pair contains a \"safe\"\nand an \"unsafe\" example that differ only by the target concept. Averaging their\nper-layer latent differences produces a \"refusal vector\", which, once\nsubtracted from the model parameters, neutralizes the unsafe concept. We\nintroduce a novel low-rank factorization approach on the covariance difference\nof embeddings that yields robust refusal vectors. This isolates the target\nconcept while minimizing collateral unlearning of other semantics, thus\npreserving the visual quality of the generated video. Our method preserves the\nmodel's generation quality while operating without retraining or access to the\noriginal training data. By embedding the refusal direction directly into the\nmodel's weights, the suppression mechanism becomes inherently more robust\nagainst adversarial bypass attempts compared to surface-level input-output\nfilters. In a thorough qualitative and quantitative evaluation, we show that we\ncan neutralize a variety of harmful contents, including explicit nudity,\ngraphic violence, copyrights, and trademarks. Project page:\nhttps://www.pinlab.org/video-unlearning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u65e0\u5b66\u4e60\u6280\u672f\uff0c\u4ec5\u97005\u5bf9\u591a\u6a21\u6001\u63d0\u793a\u5bf9\u5373\u53ef\u6d88\u9664\u6a21\u578b\u4e2d\u7684\u6709\u5bb3\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ef\u80fd\u7ee7\u627f\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u89c1\u548c\u6709\u5bb3\u5185\u5bb9\uff0c\u5bfc\u81f4\u7528\u6237\u751f\u6210\u4e0d\u826f\u6216\u975e\u6cd5\u5185\u5bb9\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u793a\u4f8b\u7684\u6f5c\u5728\u5dee\u5f02\u751f\u6210\u201c\u62d2\u7edd\u5411\u91cf\u201d\uff0c\u5e76\u91c7\u7528\u4f4e\u79e9\u5206\u89e3\u65b9\u6cd5\u4f18\u5316\uff0c\u76f4\u63a5\u5d4c\u5165\u6a21\u578b\u6743\u91cd\u3002", "result": "\u65b9\u6cd5\u80fd\u6709\u6548\u6d88\u9664\u591a\u79cd\u6709\u5bb3\u5185\u5bb9\uff08\u5982\u88f8\u9732\u3001\u66b4\u529b\u3001\u7248\u6743\u7b49\uff09\uff0c\u4e14\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6280\u672f\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u539f\u59cb\u6570\u636e\uff0c\u76f4\u63a5\u5d4c\u5165\u62d2\u7edd\u5411\u91cf\uff0c\u63d0\u5347\u4e86\u5bf9\u6297\u7ed5\u8fc7\u5c1d\u8bd5\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.07905", "pdf": "https://arxiv.org/pdf/2506.07905", "abs": "https://arxiv.org/abs/2506.07905", "authors": ["Jie Yang", "Feipeng Ma", "Zitian Wang", "Dacheng Yin", "Kang Rong", "Fengyun Rao", "Ruimao Zhang"], "title": "WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Building on the success of text-based reasoning models like DeepSeek-R1,\nextending these capabilities to multimodal reasoning holds great promise. While\nrecent works have attempted to adapt DeepSeek-R1-style reinforcement learning\n(RL) training paradigms to multimodal large language models (MLLM), focusing on\ndomain-specific tasks like math and visual perception, a critical question\nremains: How can we achieve the general-purpose visual-language reasoning\nthrough RL? To address this challenge, we make three key efforts: (1) A novel\nScalable Multimodal QA Synthesis pipeline that autonomously generates\ncontext-aware, reasoning-centric question-answer (QA) pairs directly from the\ngiven images. (2) The open-source WeThink dataset containing over 120K\nmultimodal QA pairs with annotated reasoning paths, curated from 18 diverse\ndataset sources and covering various question domains. (3) A comprehensive\nexploration of RL on our dataset, incorporating a hybrid reward mechanism that\ncombines rule-based verification with model-based assessment to optimize RL\ntraining efficiency across various task domains. Across 14 diverse MLLM\nbenchmarks, we demonstrate that our WeThink dataset significantly enhances\nperformance, from mathematical reasoning to diverse general multimodal tasks.\nMoreover, we show that our automated data pipeline can continuously increase\ndata diversity to further improve model performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u6587\u672c\u63a8\u7406\u6a21\u578b\u5230\u591a\u6a21\u6001\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6a21\u6001QA\u5bf9\u548c\u4f18\u5316RL\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u901a\u7528\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\uff0c\u586b\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "1. \u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001QA\u5408\u6210\u6d41\u6c34\u7ebf\uff1b2. \u5f00\u6e90\u4e86WeThink\u6570\u636e\u96c6\uff1b3. \u7ed3\u5408\u89c4\u5219\u548c\u6a21\u578b\u8bc4\u4f30\u7684\u6df7\u5408\u5956\u52b1\u673a\u5236\u4f18\u5316RL\u8bad\u7ec3\u3002", "result": "\u572814\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u591a\u6837\u6027\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "WeThink\u6570\u636e\u96c6\u548c\u81ea\u52a8\u5316\u6570\u636e\u6d41\u6c34\u7ebf\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.07925", "pdf": "https://arxiv.org/pdf/2506.07925", "abs": "https://arxiv.org/abs/2506.07925", "authors": ["Yaxita Amin", "Naimisha S Trivedi", "Rashmi Bhattad"], "title": "A Comparative Study of U-Net Architectures for Change Detection in Satellite Images", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Remote sensing change detection is essential for monitoring the everchanging\nlandscapes of the Earth. The U-Net architecture has gained popularity for its\ncapability to capture spatial information and perform pixel-wise\nclassification. However, their application in the Remote sensing field remains\nlargely unexplored. Therefore, this paper fill the gap by conducting a\ncomprehensive analysis of 34 papers. This study conducts a comparison and\nanalysis of 18 different U-Net variations, assessing their potential for\ndetecting changes in remote sensing. We evaluate both benefits along with\ndrawbacks of each variation within the framework of this particular\napplication. We emphasize variations that are explicitly built for change\ndetection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.\nThe analysis highlights the significance of aspects such as managing data from\ndifferent time periods and collecting relationships over a long distance to\nenhance the precision of change detection. This study provides valuable\ninsights for researchers and practitioners that choose U-Net versions for\nremote sensing change detection tasks.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u679034\u7bc7\u8bba\u6587\uff0c\u6bd4\u8f83\u4e8618\u79cdU-Net\u53d8\u4f53\u5728\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002", "motivation": "\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u5bf9\u76d1\u6d4b\u5730\u7403\u666f\u89c2\u53d8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46U-Net\u67b6\u6784\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e8618\u79cdU-Net\u53d8\u4f53\uff0c\u8bc4\u4f30\u5176\u5728\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u5173\u6ce8\u4e13\u4e3a\u53d8\u5316\u68c0\u6d4b\u8bbe\u8ba1\u7684\u53d8\u4f53\uff08\u5982Siamese Swin-U-Net\uff09\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u5904\u7406\u4e0d\u540c\u65f6\u95f4\u6570\u636e\u53ca\u957f\u8ddd\u79bb\u5173\u7cfb\u5bf9\u63d0\u5347\u53d8\u5316\u68c0\u6d4b\u7cbe\u5ea6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9009\u62e9U-Net\u7248\u672c\u7528\u4e8e\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2506.07936", "pdf": "https://arxiv.org/pdf/2506.07936", "abs": "https://arxiv.org/abs/2506.07936", "authors": ["Chengyue Huang", "Yuchen Zhu", "Sichen Zhu", "Jingyun Xiao", "Moises Andrade", "Shivang Chopra", "Zsolt Kira"], "title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) are widely assumed to exhibit in-context\nlearning (ICL), a property similar to that of their language-only counterparts.\nWhile recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies\nshow they often rely on shallow heuristics -- such as copying or majority\nvoting -- rather than true task understanding. We revisit this assumption by\nevaluating VLMs under distribution shifts, where support examples come from a\ndataset different from the query. Surprisingly, performance often degrades with\nmore demonstrations, and models tend to copy answers rather than learn from\nthem. To investigate further, we propose a new MM-ICL with Reasoning pipeline\nthat augments each demonstration with a generated rationale alongside the\nanswer. We conduct extensive and comprehensive experiments on both perception-\nand reasoning-required datasets with open-source VLMs ranging from 3B to 72B\nand proprietary models such as Gemini 2.0. We conduct controlled studies\nvarying shot count, retrieval method, rationale quality, and distribution. Our\nresults show limited performance sensitivity across these factors, suggesting\nthat current VLMs do not effectively utilize demonstration-level information as\nintended in MM-ICL.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08MM-ICL\uff09\u4e2d\u4f9d\u8d56\u6d45\u5c42\u542f\u53d1\u5f0f\u65b9\u6cd5\u800c\u975e\u771f\u5b9e\u4efb\u52a1\u7406\u89e3\uff0c\u6027\u80fd\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4e0b\u964d\u3002\u63d0\u51fa\u65b0\u65b9\u6cd5MM-ICL with Reasoning\uff0c\u4f46\u5b9e\u9a8c\u663e\u793a\u5f53\u524dVLM\u672a\u80fd\u6709\u6548\u5229\u7528\u6f14\u793a\u4fe1\u606f\u3002", "motivation": "\u63a2\u8ba8VLM\u662f\u5426\u771f\u6b63\u5177\u5907\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u800c\u975e\u4f9d\u8d56\u6d45\u5c42\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMM-ICL with Reasoning\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u6f14\u793a\u751f\u6210\u7b54\u6848\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVLM\u6027\u80fd\u5bf9\u6f14\u793a\u6570\u91cf\u3001\u68c0\u7d22\u65b9\u6cd5\u7b49\u4e0d\u654f\u611f\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u6f14\u793a\u4fe1\u606f\u3002", "conclusion": "\u5f53\u524dVLM\u5728\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u63d0\u5347\u4efb\u52a1\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2506.07943", "pdf": "https://arxiv.org/pdf/2506.07943", "abs": "https://arxiv.org/abs/2506.07943", "authors": ["Yizhen Li", "Dell Zhang", "Xuelong Li", "Yiqing Shen"], "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.", "AI": {"tldr": "DTwinSeger\u662f\u4e00\u79cd\u65b0\u7684Reasoning Segmentation\u65b9\u6cd5\uff0c\u901a\u8fc7Digital Twin\u8868\u793a\u5c06\u611f\u77e5\u4e0e\u63a8\u7406\u89e3\u8026\uff0c\u5229\u7528LLM\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u56fe\u50cf\u6807\u8bb0\u5316\u65f6\u7834\u574f\u4e86\u5bf9\u8c61\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u4fdd\u7559\u7a7a\u95f4\u5173\u7cfb\u5e76\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "DTwinSeger\u5c06\u4efb\u52a1\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u751f\u6210\u7ed3\u6784\u5316DT\u8868\u793a\uff0c\u518d\u7528LLM\u8fdb\u884c\u63a8\u7406\u3002\u63d0\u51fa\u4e86\u9488\u5bf9LLM\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u548c\u6570\u636e\u96c6Seg-DT\u3002", "result": "\u5728\u4e24\u4e2a\u56fe\u50cfRS\u57fa\u51c6\u548c\u4e09\u4e2a\u56fe\u50cf\u53c2\u8003\u5206\u5272\u57fa\u51c6\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "DT\u8868\u793a\u662f\u89c6\u89c9\u4e0e\u6587\u672c\u95f4\u7684\u6709\u6548\u6865\u6881\uff0c\u4ec5\u7528LLM\u5373\u53ef\u5b8c\u6210\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2506.07960", "pdf": "https://arxiv.org/pdf/2506.07960", "abs": "https://arxiv.org/abs/2506.07960", "authors": ["Ari Vesalainen", "Jenna Kanerva", "Aida Nitsch", "Kiia Korsu", "Ilari Larkiola", "Laura Ruotsalainen", "Filip Ginter"], "title": "Creating a Historical Migration Dataset from Finnish Church Records, 1800-1920", "categories": ["cs.CV", "I.4.6, J.5"], "comment": null, "summary": "This article presents a large-scale effort to create a structured dataset of\ninternal migration in Finland between 1800 and 1920 using digitized church\nmoving records. These records, maintained by Evangelical-Lutheran parishes,\ndocument the migration of individuals and families and offer a valuable source\nfor studying historical demographic patterns. The dataset includes over six\nmillion entries extracted from approximately 200,000 images of handwritten\nmigration records.\n  The data extraction process was automated using a deep learning pipeline that\nincluded layout analysis, table detection, cell classification, and handwriting\nrecognition. The complete pipeline was applied to all images, resulting in a\nstructured dataset suitable for research.\n  The dataset can be used to study internal migration, urbanization, and family\nmigration, and the spread of disease in preindustrial Finland. A case study\nfrom the Elim\\\"aki parish shows how local migration histories can be\nreconstructed. The work demonstrates how large volumes of handwritten archival\nmaterial can be transformed into structured data to support historical and\ndemographic research.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4ece\u82ac\u51701800-1920\u5e74\u7684\u6559\u4f1a\u8fc1\u79fb\u8bb0\u5f55\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\uff0c\u652f\u6301\u5386\u53f2\u4e0e\u4eba\u53e3\u7814\u7a76\u3002", "motivation": "\u5229\u7528\u6570\u5b57\u5316\u6559\u4f1a\u8fc1\u79fb\u8bb0\u5f55\u7814\u7a76\u82ac\u5170\u5386\u53f2\u4eba\u53e3\u6a21\u5f0f\uff0c\u586b\u8865\u76f8\u5173\u9886\u57df\u6570\u636e\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\u81ea\u52a8\u5316\u63d0\u53d6\u6570\u636e\uff0c\u5305\u62ec\u5e03\u5c40\u5206\u6790\u3001\u8868\u683c\u68c0\u6d4b\u3001\u5355\u5143\u683c\u5206\u7c7b\u548c\u624b\u5199\u8bc6\u522b\u3002", "result": "\u751f\u6210\u5305\u542b600\u591a\u4e07\u6761\u76ee\u7684\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff0c\u9002\u7528\u4e8e\u7814\u7a76\u5185\u90e8\u8fc1\u79fb\u3001\u57ce\u5e02\u5316\u3001\u5bb6\u5ead\u8fc1\u79fb\u53ca\u75be\u75c5\u4f20\u64ad\u3002", "conclusion": "\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5927\u91cf\u624b\u5199\u6863\u6848\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4e3a\u5386\u53f2\u548c\u4eba\u53e3\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.07964", "pdf": "https://arxiv.org/pdf/2506.07964", "abs": "https://arxiv.org/abs/2506.07964", "authors": ["Wenxin Tang", "Jingyu Xiao", "Wenxuan Jiang", "Xi Xiao", "Yuhang Wang", "Xuxin Tang", "Qing Li", "Yuehe Ma", "Junliang Liu", "Shisong Tang", "Michael R. Lyu"], "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Manual slide creation is labor-intensive and requires expert prior knowledge.\nExisting natural language-based LLM generation methods struggle to capture the\nvisual and structural nuances of slide designs. To address this, we formalize\nthe Reference Image to Slide Generation task and propose Slide2Code, the first\nbenchmark with difficulty-tiered samples based on a novel Slide Complexity\nMetric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework\nfor generating editable slides from reference images. SlideCoder integrates a\nColor Gradient-based Segmentation algorithm and a Hierarchical\nRetrieval-Augmented Generation method to decompose complex tasks and enhance\ncode generation. We also release SlideMaster, a 7B open-source model fine-tuned\nwith improved reverse-engineered data. Experiments show that SlideCoder\noutperforms state-of-the-art baselines by up to 40.5 points, demonstrating\nstrong performance across layout fidelity, execution accuracy, and visual\nconsistency. Our code is available at\nhttps://github.com/vinsontang1/SlideCoder.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSlide2Code\u57fa\u51c6\u548cSlideCoder\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u53c2\u8003\u56fe\u50cf\u751f\u6210\u53ef\u7f16\u8f91\u5e7b\u706f\u7247\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u548c\u7ed3\u6784\u8bbe\u8ba1\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u624b\u52a8\u5236\u4f5c\u5e7b\u706f\u7247\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u73b0\u6709\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684LLM\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5e7b\u706f\u7247\u8bbe\u8ba1\u7684\u89c6\u89c9\u548c\u7ed3\u6784\u7ec6\u8282\u3002", "method": "\u63d0\u51faSlide2Code\u57fa\u51c6\u548cSlideCoder\u6846\u67b6\uff0c\u7ed3\u5408\u989c\u8272\u68af\u5ea6\u5206\u5272\u7b97\u6cd5\u548c\u5206\u5c42\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u4f18\u5316\u4ee3\u7801\u751f\u6210\u3002", "result": "SlideCoder\u5728\u5e03\u5c40\u4fdd\u771f\u5ea6\u3001\u6267\u884c\u51c6\u786e\u6027\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u8fbe40.5\u5206\u3002", "conclusion": "SlideCoder\u5c55\u793a\u4e86\u5728\u5e7b\u706f\u7247\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u4e86SlideMaster\u6a21\u578b\u548c\u4ee3\u7801\u3002"}}
{"id": "2506.07966", "pdf": "https://arxiv.org/pdf/2506.07966", "abs": "https://arxiv.org/abs/2506.07966", "authors": ["Ziyang Gong", "Wenhao Li", "Oliver Ma", "Songyuan Li", "Jiayi Ji", "Xue Yang", "Gen Luo", "Junchi Yan", "Rongrong Ji"], "title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in\nvarious multimodal tasks. To pursue higher intelligence in space, MLLMs require\nintegrating multiple atomic spatial capabilities to handle complex and dynamic\ntasks. However, existing benchmarks struggle to comprehensively evaluate the\nspatial intelligence of common MLLMs from the atomic level to the compositional\nlevel. To fill this gap, we present SpaCE-10, a comprehensive benchmark for\ncompositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial\ncapabilities, which are combined to form 8 compositional capabilities. Based on\nthese definitions, we propose a novel hierarchical annotation pipeline to\ngenerate high-quality and diverse question-answer (QA) pairs. With over 150+\nhours of human expert effort, we obtain over 5k QA pairs for 811 real indoor\nscenes in SpaCE-10, which covers various evaluation settings like point cloud\ninput and multi-choice QA. We conduct an extensive evaluation of common MLLMs\non SpaCE-10 and find that even the most advanced MLLM still lags behind humans\nby large margins. Through our careful study, we also draw several significant\nfindings that benefit the MLLM community. For example, we reveal that the\nshortcoming of counting capability greatly limits the compositional spatial\ncapabilities of existing MLLMs. The evaluation code and benchmark datasets are\navailable at https://github.com/Cuzyoung/SpaCE-10.", "AI": {"tldr": "SpaCE-10\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7a7a\u95f4\u667a\u80fd\u7684\u7efc\u5408\u6027\u57fa\u51c6\uff0c\u5305\u542b10\u79cd\u539f\u5b50\u7a7a\u95f4\u80fd\u529b\u548c8\u79cd\u7ec4\u5408\u80fd\u529b\uff0c\u901a\u8fc75k+QA\u5bf9\u8fdb\u884c\u6d4b\u8bd5\uff0c\u53d1\u73b0\u73b0\u6709MLLMs\u4e0e\u4eba\u7c7b\u8868\u73b0\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u5168\u9762\u8bc4\u4f30MLLMs\u4ece\u539f\u5b50\u5230\u7ec4\u5408\u5c42\u9762\u7684\u7a7a\u95f4\u667a\u80fd\uff0c\u56e0\u6b64\u63d0\u51faSpaCE-10\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5b9a\u4e4910\u79cd\u539f\u5b50\u7a7a\u95f4\u80fd\u529b\u548c8\u79cd\u7ec4\u5408\u80fd\u529b\uff0c\u91c7\u7528\u5206\u5c42\u6807\u6ce8\u6d41\u7a0b\u751f\u6210\u9ad8\u8d28\u91cfQA\u5bf9\uff0c\u8986\u76d6\u591a\u79cd\u8bc4\u4f30\u8bbe\u7f6e\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684MLLMs\u5728\u7a7a\u95f4\u667a\u80fd\u4e0a\u4ecd\u5927\u5e45\u843d\u540e\u4e8e\u4eba\u7c7b\uff0c\u4e14\u8ba1\u6570\u80fd\u529b\u4e0d\u8db3\u663e\u8457\u9650\u5236\u5176\u7ec4\u5408\u80fd\u529b\u3002", "conclusion": "SpaCE-10\u4e3aMLLM\u793e\u533a\u63d0\u4f9b\u4e86\u91cd\u8981\u53d1\u73b0\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.07971", "pdf": "https://arxiv.org/pdf/2506.07971", "abs": "https://arxiv.org/abs/2506.07971", "authors": ["Jiahao Meng", "Shuyang Sun", "Yue Tan", "Lu Qi", "Yunhai Tong", "Xiangtai Li", "Longyin Wen"], "title": "CyberV: Cybernetics for Test-time Scaling in Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) may struggle with\nunderstanding long or complex videos due to computational demands at test time,\nlack of robustness, and limited accuracy, primarily stemming from their\nfeed-forward processing nature. These limitations could be more severe for\nmodels with fewer parameters. To address these limitations, we propose a novel\nframework inspired by cybernetic principles, redesigning video MLLMs as\nadaptive systems capable of self-monitoring, self-correction, and dynamic\nresource allocation during inference. Our approach, CyberV, introduces a\ncybernetic loop consisting of an MLLM Inference System, a Sensor, and a\nController. Specifically, the sensor monitors forward processes of the MLLM and\ncollects intermediate interpretations, such as attention drift, then the\ncontroller determines when and how to trigger self-correction and generate\nfeedback to guide the next round. This test-time adaptive scaling framework\nenhances frozen MLLMs without requiring retraining or additional components.\nExperiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B\nby 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive\nproprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%\nimprovement, achieving performance even comparable to human experts.\nFurthermore, our method demonstrates consistent gains on general-purpose\nbenchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and\ngeneralization capabilities in making MLLMs more robust and accurate for\ndynamic video understanding. The code is released at\nhttps://github.com/marinero4972/CyberV.", "AI": {"tldr": "CyberV\u662f\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u8bba\u539f\u7406\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u5206\u914d\u548c\u81ea\u6211\u76d1\u63a7\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u957f\u6216\u590d\u6742\u89c6\u9891\u7406\u89e3\u4e2d\u5b58\u5728\u8ba1\u7b97\u9700\u6c42\u9ad8\u3001\u9c81\u68d2\u6027\u5dee\u548c\u51c6\u786e\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u53c2\u6570\u8f83\u5c11\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faCyberV\u6846\u67b6\uff0c\u5305\u542bMLLM\u63a8\u7406\u7cfb\u7edf\u3001\u4f20\u611f\u5668\u548c\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u76d1\u63a7\u548c\u53cd\u9988\u5b9e\u73b0\u52a8\u6001\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCyberV\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5982Qwen2.5-VL-7B\u63d0\u53478.3%\uff0c\u6027\u80fd\u751a\u81f3\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "CyberV\u6709\u6548\u589e\u5f3aMLLMs\u7684\u52a8\u6001\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.07977", "pdf": "https://arxiv.org/pdf/2506.07977", "abs": "https://arxiv.org/abs/2506.07977", "authors": ["Jingjing Chang", "Yixiao Fang", "Peng Xing", "Shuhan Wu", "Wei Cheng", "Rui Wang", "Xianfang Zeng", "Gang Yu", "Hai-Bao Chen"], "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) models have garnered significant attention for generating\nhigh-quality images aligned with text prompts. However, rapid T2I model\nadvancements reveal limitations in early benchmarks, lacking comprehensive\nevaluations, for example, the evaluation on reasoning, text rendering and\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\nmodeling capabilities, show promising results on the image generation problems\nrequiring strong reasoning ability, yet existing evaluation systems have not\nadequately addressed this frontier. To systematically address these gaps, we\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\nframework for fine-grained evaluation of T2I models across multiple dimensions,\nincluding prompt-image alignment, text rendering precision, reasoning-generated\ncontent, stylization, and diversity. By structuring the evaluation, this\nbenchmark enables in-depth analysis of model performance, helping researchers\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\nallowing users to focus on a particular evaluation subset. Instead of\ngenerating images for the entire set of prompts, users can generate images only\nfor the prompts associated with the selected dimension and complete the\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\navailable to facilitate reproducible evaluation studies and cross-model\ncomparisons within the T2I research community.", "AI": {"tldr": "OneIG-Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u9488\u5bf9\u591a\u7ef4\u5ea6\uff08\u5982\u5bf9\u9f50\u3001\u6587\u672c\u6e32\u67d3\u3001\u63a8\u7406\u7b49\uff09\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u586b\u8865\u73b0\u6709\u8bc4\u6d4b\u7cfb\u7edf\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709T2I\u6a21\u578b\u7684\u8bc4\u6d4b\u7cfb\u7edf\u672a\u80fd\u5168\u9762\u8bc4\u4f30\u63a8\u7406\u3001\u6587\u672c\u6e32\u67d3\u548c\u98ce\u683c\u5316\u7b49\u5173\u952e\u7ef4\u5ea6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u7684\u6df1\u5165\u5206\u6790\u3002", "method": "\u8bbe\u8ba1OneIG-Bench\u6846\u67b6\uff0c\u652f\u6301\u591a\u7ef4\u5ea6\uff08\u5982\u5bf9\u9f50\u3001\u6587\u672c\u6e32\u67d3\u3001\u63a8\u7406\u751f\u6210\u5185\u5bb9\u7b49\uff09\u7684\u7075\u6d3b\u8bc4\u4f30\uff0c\u7528\u6237\u53ef\u9488\u5bf9\u7279\u5b9a\u7ef4\u5ea6\u751f\u6210\u56fe\u50cf\u5e76\u5b8c\u6210\u8bc4\u6d4b\u3002", "result": "OneIG-Bench\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\uff0c\u652f\u6301\u53ef\u91cd\u590d\u7684\u8bc4\u6d4b\u7814\u7a76\u548c\u8de8\u6a21\u578b\u6bd4\u8f83\u3002", "conclusion": "OneIG-Bench\u586b\u8865\u4e86T2I\u6a21\u578b\u8bc4\u6d4b\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2506.07981", "pdf": "https://arxiv.org/pdf/2506.07981", "abs": "https://arxiv.org/abs/2506.07981", "authors": ["Dmitrii Vorobev", "Artem Prosvetov", "Karim Elhadji Daou"], "title": "Real-time Localization of a Soccer Ball from a Single Camera", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "We propose a computationally efficient method for real-time three-dimensional\nfootball trajectory reconstruction from a single broadcast camera. In contrast\nto previous work, our approach introduces a multi-mode state model with $W$\ndiscrete modes to significantly accelerate optimization while preserving\ncentimeter-level accuracy -- even in cases of severe occlusion, motion blur,\nand complex backgrounds. The system operates on standard CPUs and achieves low\nlatency suitable for live broadcast settings. Extensive evaluation on a\nproprietary dataset of 6K-resolution Russian Premier League matches\ndemonstrates performance comparable to multi-camera systems, without the need\nfor specialized or costly infrastructure. This work provides a practical method\nfor accessible and accurate 3D ball tracking in professional football\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u6444\u50cf\u5934\u5b9e\u65f6\u4e09\u7ef4\u8db3\u7403\u8f68\u8ff9\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u72b6\u6001\u6a21\u578b\u52a0\u901f\u4f18\u5316\uff0c\u4fdd\u6301\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u590d\u6742\u80cc\u666f\u4e0b\u7684\u6027\u80fd\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u7684\u6210\u672c\u548c\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165\u591a\u6a21\u6001\u72b6\u6001\u6a21\u578b\uff08$W$\u79bb\u6563\u6a21\u6001\uff09\u52a0\u901f\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u6807\u51c6CPU\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3002", "result": "\u57286K\u5206\u8fa8\u7387\u4fc4\u7f57\u65af\u8d85\u7ea7\u8054\u8d5b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5ab2\u7f8e\u591a\u6444\u50cf\u5934\u7cfb\u7edf\uff0c\u65e0\u9700\u6602\u8d35\u8bbe\u5907\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u4f4e\u6210\u672c\u4e14\u9ad8\u7cbe\u5ea6\u7684\u4e09\u7ef4\u8db3\u7403\u8f68\u8ff9\u8ddf\u8e2a\u65b9\u6cd5\u3002"}}
{"id": "2506.07984", "pdf": "https://arxiv.org/pdf/2506.07984", "abs": "https://arxiv.org/abs/2506.07984", "authors": ["Mingquan Lin", "Gregory Holste", "Song Wang", "Yiliang Zhou", "Yishu Wei", "Imon Banerjee", "Pengyi Chen", "Tianjie Dai", "Yuexi Du", "Nicha C. Dvornek", "Yuyan Ge", "Zuowei Guo", "Shouhei Hanaoka", "Dongkyun Kim", "Pablo Messina", "Yang Lu", "Denis Parra", "Donghyun Son", "\u00c1lvaro Soto", "Aisha Urooj", "Ren\u00e9 Vidal", "Yosuke Yamagishi", "Zefan Yang", "Ruichi Zhang", "Yang Zhou", "Leo Anthony Celi", "Ronald M. Summers", "Zhiyong Lu", "Hao Chen", "Adam Flanders", "George Shih", "Zhangyang Wang", "Yifan Peng"], "title": "CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 3 figures", "summary": "The CXR-LT series is a community-driven initiative designed to enhance lung\ndisease classification using chest X-rays (CXR). It tackles challenges in open\nlong-tailed lung disease classification and enhances the measurability of\nstate-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve\nthese goals by providing high-quality benchmark CXR data for model development\nand conducting comprehensive evaluations to identify ongoing issues impacting\nlung disease classification performance. Building on the success of CXR-LT\n2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45\ndisease labels, including 19 new rare disease findings. It also introduces a\nnew focus on zero-shot learning to address limitations identified in the\nprevious event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed\nclassification on a large, noisy test set, (ii) long-tailed classification on a\nmanually annotated \"gold standard\" subset, and (iii) zero-shot generalization\nto five previously unseen disease findings. This paper provides an overview of\nCXR-LT 2024, detailing the data curation process and consolidating\nstate-of-the-art solutions, including the use of multimodal models for rare\ndisease detection, advanced generative approaches to handle noisy labels, and\nzero-shot learning strategies for unseen diseases. Additionally, the expanded\ndataset enhances disease coverage to better represent real-world clinical\nsettings, offering a valuable resource for future research. By synthesizing the\ninsights and innovations of participating teams, we aim to advance the\ndevelopment of clinically realistic and generalizable diagnostic models for\nchest radiography.", "AI": {"tldr": "CXR-LT 2024\u662f\u4e00\u4e2a\u793e\u533a\u9a71\u52a8\u7684\u9879\u76ee\uff0c\u65e8\u5728\u901a\u8fc7\u6269\u5c55\u6570\u636e\u96c6\u548c\u5f15\u5165\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u63d0\u5347\u80f8\u90e8X\u5c04\u7ebf\uff08CXR\uff09\u7684\u80ba\u75c5\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u957f\u5c3e\u80ba\u75c5\u5206\u7c7b\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u63d0\u5347\u73b0\u6709\u6280\u672f\u7684\u53ef\u6d4b\u91cf\u6027\u3002", "method": "\u6269\u5c55\u6570\u636e\u96c6\u81f3377,110\u5f20CXR\u548c45\u79cd\u75be\u75c5\u6807\u7b7e\uff0c\u5f15\u5165\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u5e76\u8bbe\u8ba1\u4e09\u9879\u4efb\u52a1\uff1a\u957f\u5c3e\u5206\u7c7b\uff08\u566a\u58f0\u6d4b\u8bd5\u96c6\u548c\u9ec4\u91d1\u6807\u51c6\u5b50\u96c6\uff09\u53ca\u96f6\u6837\u672c\u6cdb\u5316\u3002", "result": "\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u57fa\u51c6\u6570\u636e\uff0c\u6574\u5408\u4e86\u591a\u6a21\u6001\u6a21\u578b\u3001\u751f\u6210\u65b9\u6cd5\u548c\u96f6\u6837\u672c\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u75be\u75c5\u8986\u76d6\u7387\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "conclusion": "CXR-LT 2024\u4e3a\u5f00\u53d1\u66f4\u5177\u4e34\u5e8a\u73b0\u5b9e\u6027\u548c\u6cdb\u5316\u6027\u7684\u80ba\u75c5\u8bca\u65ad\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2506.07985", "pdf": "https://arxiv.org/pdf/2506.07985", "abs": "https://arxiv.org/abs/2506.07985", "authors": ["Tuomas Oikarinen", "Ge Yan", "Akshay Kulkarni", "Tsui-Wei Weng"], "title": "Rethinking Crowd-Sourced Evaluation of Neuron Explanations", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Interpreting individual neurons or directions in activations space is an\nimportant component of mechanistic interpretability. As such, many algorithms\nhave been proposed to automatically produce neuron explanations, but it is\noften not clear how reliable these explanations are, or which methods produce\nthe best explanations. This can be measured via crowd-sourced evaluations, but\nthey can often be noisy and expensive, leading to unreliable results. In this\npaper, we carefully analyze the evaluation pipeline and develop a\ncost-effective and highly accurate crowdsourced evaluation strategy. In\ncontrast to previous human studies that only rate whether the explanation\nmatches the most highly activating inputs, we estimate whether the explanation\ndescribes neuron activations across all inputs. To estimate this effectively,\nwe introduce a novel application of importance sampling to determine which\ninputs are the most valuable to show to raters, leading to around 30x cost\nreduction compared to uniform sampling. We also analyze the label noise present\nin crowd-sourced evaluations and propose a Bayesian method to aggregate\nmultiple ratings leading to a further ~5x reduction in number of ratings\nrequired for the same accuracy. Finally, we use these methods to conduct a\nlarge-scale study comparing the quality of neuron explanations produced by the\nmost popular methods for two different vision models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u4f17\u5305\u8bc4\u4f30\u7b56\u7565\uff0c\u7528\u4e8e\u8bc4\u4f30\u795e\u7ecf\u5143\u89e3\u91ca\u7684\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u548c\u8d1d\u53f6\u65af\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u5143\u89e3\u91ca\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u566a\u97f3\u5927\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u7b56\u7565\u3002", "method": "\u5f15\u5165\u91cd\u8981\u6027\u91c7\u6837\u9009\u62e9\u6700\u6709\u4ef7\u503c\u7684\u8f93\u5165\uff0c\u5e76\u63d0\u51fa\u8d1d\u53f6\u65af\u65b9\u6cd5\u805a\u5408\u591a\u4e2a\u8bc4\u5206\uff0c\u4ee5\u51cf\u5c11\u6240\u9700\u8bc4\u5206\u6570\u91cf\u3002", "result": "\u5b9e\u73b0\u4e86\u7ea630\u500d\u7684\u6210\u672c\u964d\u4f4e\u548c\u7ea65\u500d\u7684\u8bc4\u5206\u6570\u91cf\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u6bd4\u8f83\u4e0d\u540c\u795e\u7ecf\u5143\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.07986", "pdf": "https://arxiv.org/pdf/2506.07986", "abs": "https://arxiv.org/abs/2506.07986", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTACA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u7684\u6e29\u5ea6\u548c\u65f6\u95f4\u6b65\u4f9d\u8d56\u6743\u91cd\uff0c\u89e3\u51b3\u4e86MM-DiT\u6a21\u578b\u4e2d\u6587\u672c\u4e0e\u56fe\u50cf\u5bf9\u9f50\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MM-DiT\u6a21\u578b\uff08\u5982FLUX\uff09\u5728\u6587\u672c\u9a71\u52a8\u7684\u89c6\u89c9\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u5b58\u5728\u6587\u672c\u63d0\u793a\u4e0e\u751f\u6210\u5185\u5bb9\u5bf9\u9f50\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u4e0d\u5e73\u8861\u548c\u7f3a\u4e4f\u65f6\u95f4\u6b65\u611f\u77e5\u6743\u91cd\u3002", "method": "\u63d0\u51faTemperature-Adjusted Cross-modal Attention (TACA)\uff0c\u901a\u8fc7\u6e29\u5ea6\u7f29\u653e\u548c\u65f6\u95f4\u6b65\u4f9d\u8d56\u8c03\u6574\u52a8\u6001\u5e73\u8861\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u5e76\u7ed3\u5408LoRA\u5fae\u8c03\u3002", "result": "\u5728T2I-CompBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTACA\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\uff08\u5982\u7269\u4f53\u5916\u89c2\u3001\u5c5e\u6027\u7ed1\u5b9a\u548c\u7a7a\u95f4\u5173\u7cfb\uff09\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "TACA\u901a\u8fc7\u5e73\u8861\u8de8\u6a21\u6001\u6ce8\u610f\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.07992", "pdf": "https://arxiv.org/pdf/2506.07992", "abs": "https://arxiv.org/abs/2506.07992", "authors": ["Haoguang Lu", "Jiacheng Chen", "Zhenguo Yang", "Aurele Tohokantche Gnanha", "Fu Lee Wang", "Li Qing", "Xudong Mao"], "title": "PairEdit: Learning Semantic Variations for Exemplar-based Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-guided image editing have achieved notable\nsuccess by leveraging natural language prompts for fine-grained semantic\ncontrol. However, certain editing semantics are challenging to specify\nprecisely using textual descriptions alone. A practical alternative involves\nlearning editing semantics from paired source-target examples. Existing\nexemplar-based editing methods still rely on text prompts describing the change\nwithin paired examples or learning implicit text-based editing instructions. In\nthis paper, we introduce PairEdit, a novel visual editing method designed to\neffectively learn complex editing semantics from a limited number of image\npairs or even a single image pair, without using any textual guidance. We\npropose a target noise prediction that explicitly models semantic variations\nwithin paired images through a guidance direction term. Moreover, we introduce\na content-preserving noise schedule to facilitate more effective semantic\nlearning. We also propose optimizing distinct LoRAs to disentangle the learning\nof semantic variations from content. Extensive qualitative and quantitative\nevaluations demonstrate that PairEdit successfully learns intricate semantics\nwhile significantly improving content consistency compared to baseline methods.\nCode will be available at https://github.com/xudonmao/PairEdit.", "AI": {"tldr": "PairEdit\u662f\u4e00\u79cd\u65e0\u9700\u6587\u672c\u6307\u5bfc\u7684\u89c6\u89c9\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u56fe\u50cf\u5bf9\u5b66\u4e60\u590d\u6742\u7f16\u8f91\u8bed\u4e49\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u793a\u4f8b\u7684\u7f16\u8f91\u65b9\u6cd5\u4ecd\u9700\u4f9d\u8d56\u6587\u672c\u63d0\u793a\uff0c\u800c\u67d0\u4e9b\u7f16\u8f91\u8bed\u4e49\u96be\u4ee5\u901a\u8fc7\u6587\u672c\u7cbe\u786e\u63cf\u8ff0\u3002", "method": "\u63d0\u51fa\u76ee\u6807\u566a\u58f0\u9884\u6d4b\u548c\u5185\u5bb9\u4fdd\u7559\u566a\u58f0\u8c03\u5ea6\uff0c\u4f18\u5316LoRAs\u4ee5\u5206\u79bb\u8bed\u4e49\u53d8\u5316\u4e0e\u5185\u5bb9\u5b66\u4e60\u3002", "result": "PairEdit\u80fd\u6709\u6548\u5b66\u4e60\u590d\u6742\u8bed\u4e49\uff0c\u663e\u8457\u63d0\u5347\u5185\u5bb9\u4e00\u81f4\u6027\u3002", "conclusion": "PairEdit\u5728\u65e0\u9700\u6587\u672c\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\u3002"}}
{"id": "2506.07996", "pdf": "https://arxiv.org/pdf/2506.07996", "abs": "https://arxiv.org/abs/2506.07996", "authors": ["Ming-Feng Li", "Xin Yang", "Fu-En Wang", "Hritam Basak", "Yuyin Sun", "Shreekant Gayaka", "Min Sun", "Cheng-Hao Kuo"], "title": "UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025", "summary": "6D object pose estimation has shown strong generalizability to novel objects.\nHowever, existing methods often require either a complete, well-reconstructed\n3D model or numerous reference images that fully cover the object. Estimating\n6D poses from partial references, which capture only fragments of an object's\nappearance and geometry, remains challenging. To address this, we propose\nUA-Pose, an uncertainty-aware approach for 6D object pose estimation and online\nobject completion specifically designed for partial references. We assume\naccess to either (1) a limited set of RGBD images with known poses or (2) a\nsingle 2D image. For the first case, we initialize a partial object 3D model\nbased on the provided images and poses, while for the second, we use\nimage-to-3D techniques to generate an initial object 3D model. Our method\nintegrates uncertainty into the incomplete 3D model, distinguishing between\nseen and unseen regions. This uncertainty enables confidence assessment in pose\nestimation and guides an uncertainty-aware sampling strategy for online object\ncompletion, enhancing robustness in pose estimation accuracy and improving\nobject completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and\nHO3D datasets, including RGBD sequences of YCB objects manipulated by robots\nand human hands. Experimental results demonstrate significant performance\nimprovements over existing methods, particularly when object observations are\nincomplete or partially captured. Project page:\nhttps://minfenli.github.io/UA-Pose/", "AI": {"tldr": "UA-Pose\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u90e8\u5206\u53c2\u8003\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e0d\u5b8c\u6574\u89c2\u6d4b\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5b8c\u65743D\u6a21\u578b\u6216\u5927\u91cf\u53c2\u8003\u56fe\u50cf\uff0c\u800c\u90e8\u5206\u53c2\u8003\u6570\u636e\uff08\u5982\u788e\u7247\u5316\u5916\u89c2\u548c\u51e0\u4f55\uff09\u76846D\u59ff\u6001\u4f30\u8ba1\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u57fa\u4e8e\u6709\u9650RGBD\u56fe\u50cf\u6216\u5355\u5f202D\u56fe\u50cf\u751f\u6210\u90e8\u52063D\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u533a\u5206\u5df2\u89c1\u548c\u672a\u89c1\u533a\u57df\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u91c7\u6837\u7b56\u7565\u5b8c\u6210\u5728\u7ebf\u7269\u4f53\u8865\u5168\u3002", "result": "\u5728YCB-Video\u7b49\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cUA-Pose\u5728\u4e0d\u5b8c\u6574\u89c2\u6d4b\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UA-Pose\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u5728\u7ebf\u8865\u5168\uff0c\u63d0\u5347\u4e86\u90e8\u5206\u53c2\u8003\u6570\u636e\u4e0b\u76846D\u59ff\u6001\u4f30\u8ba1\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.07999", "pdf": "https://arxiv.org/pdf/2506.07999", "abs": "https://arxiv.org/abs/2506.07999", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMADFormer\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5757\u548c\u5206\u5c42\u6df7\u5408\u4f18\u5316\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u6a21\u578b\u7f3a\u4e4f\u7cfb\u7edf\u6307\u5bfc\uff0c\u65e0\u6cd5\u660e\u786e\u5982\u4f55\u5206\u914d\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "MADFormer\u5c06\u56fe\u50cf\u751f\u6210\u5206\u5757\uff0c\u81ea\u56de\u5f52\u5c42\u7528\u4e8e\u5168\u5c40\u6761\u4ef6\uff0c\u6269\u6563\u5c42\u7528\u4e8e\u5c40\u90e8\u7ec6\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5206\u5757\u663e\u8457\u63d0\u5347\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u6027\u80fd\uff0c\u6df7\u5408\u5c42\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u6df7\u5408\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2506.08002", "pdf": "https://arxiv.org/pdf/2506.08002", "abs": "https://arxiv.org/abs/2506.08002", "authors": ["Aadarsh Sahoo", "Vansh Tibrewal", "Georgia Gkioxari"], "title": "Aligning Text, Images, and 3D Structure Token-by-Token", "categories": ["cs.CV"], "comment": "Project webpage: https://glab-caltech.github.io/kyvo/", "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684LLM\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u3001\u56fe\u50cf\u548c3D\u573a\u666f\u5bf9\u9f50\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u7684\u8be6\u7ec6\u6307\u5357\uff0c\u4ee5\u4f18\u5316\u8bad\u7ec3\u548c\u6027\u80fd\u3002", "motivation": "\u521b\u5efa\u80fd\u591f\u7406\u89e33D\u4e16\u754c\u7684\u673a\u5668\uff0c\u4ee5\u8f85\u52a9\u8bbe\u8ba1\u5e08\u6784\u5efa\u548c\u7f16\u8f913D\u73af\u5883\uff0c\u4ee5\u53ca\u5e2e\u52a9\u673a\u5668\u4eba\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u5bfc\u822a\u548c\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u63d0\u51fa\u7edf\u4e00\u7684LLM\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u8a00\u3001\u56fe\u50cf\u548c3D\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u5f62\u72b6\u7f16\u7801\u589e\u5f3a3D\u6a21\u6001\u3002", "result": "\u5728\u56db\u4e2a\u6838\u5fc33D\u4efb\u52a1\uff08\u6e32\u67d3\u3001\u8bc6\u522b\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u95ee\u7b54\uff09\u548c\u56db\u4e2a3D\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c3D\u7269\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a3D\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5728\u591a\u6a21\u6001\u5bf9\u9f50\u548c3D\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.08003", "pdf": "https://arxiv.org/pdf/2506.08003", "abs": "https://arxiv.org/abs/2506.08003", "authors": ["Shuchen Weng", "Haojie Zheng", "Zheng Chang", "Si Li", "Boxin Shi", "Xinlong Wang"], "title": "Audio-Sync Video Generation with Multi-Stream Temporal Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Audio is inherently temporal and closely synchronized with the visual world,\nmaking it a naturally aligned and expressive control signal for controllable\nvideo generation (e.g., movies). Beyond control, directly translating audio\ninto video is essential for understanding and visualizing rich audio narratives\n(e.g., Podcasts or historical recordings). However, existing approaches fall\nshort in generating high-quality videos with precise audio-visual\nsynchronization, especially across diverse and complex audio types. In this\nwork, we introduce MTV, a versatile framework for audio-sync video generation.\nMTV explicitly separates audios into speech, effects, and music tracks,\nenabling disentangled control over lip motion, event timing, and visual mood,\nrespectively -- resulting in fine-grained and semantically aligned video\ngeneration. To support the framework, we additionally present DEMIX, a dataset\ncomprising high-quality cinematic videos and demixed audio tracks. DEMIX is\nstructured into five overlapped subsets, enabling scalable multi-stage training\nfor diverse generation scenarios. Extensive experiments demonstrate that MTV\nachieves state-of-the-art performance across six standard metrics spanning\nvideo quality, text-video consistency, and audio-video alignment. Project page:\nhttps://hjzheng.net/projects/MTV/.", "AI": {"tldr": "MTV\u662f\u4e00\u4e2a\u7528\u4e8e\u97f3\u9891\u540c\u6b65\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u97f3\u9891\u4e3a\u8bed\u97f3\u3001\u6548\u679c\u548c\u97f3\u4e50\u8f68\u9053\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u63a7\u5236\u3002DEMIX\u6570\u636e\u96c6\u652f\u6301\u5176\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u663e\u793aMTV\u5728\u89c6\u9891\u8d28\u91cf\u3001\u6587\u672c\u4e00\u81f4\u6027\u548c\u97f3\u89c6\u9891\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u97f3\u9891\u4e0e\u89c6\u89c9\u4e16\u754c\u7d27\u5bc6\u540c\u6b65\uff0c\u662f\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7684\u7406\u60f3\u63a7\u5236\u4fe1\u53f7\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u548c\u97f3\u89c6\u9891\u540c\u6b65\u65b9\u9762\u4e0d\u8db3\u3002", "method": "MTV\u5c06\u97f3\u9891\u5206\u4e3a\u8bed\u97f3\u3001\u6548\u679c\u548c\u97f3\u4e50\u8f68\u9053\uff0c\u5206\u522b\u63a7\u5236\u5507\u52a8\u3001\u4e8b\u4ef6\u65f6\u95f4\u548c\u89c6\u89c9\u60c5\u7eea\uff0c\u5e76\u5229\u7528DEMIX\u6570\u636e\u96c6\u8fdb\u884c\u591a\u9636\u6bb5\u8bad\u7ec3\u3002", "result": "MTV\u5728\u89c6\u9891\u8d28\u91cf\u3001\u6587\u672c\u4e00\u81f4\u6027\u548c\u97f3\u89c6\u9891\u5bf9\u9f50\u516d\u4e2a\u6807\u51c6\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "MTV\u901a\u8fc7\u97f3\u9891\u5206\u79bb\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e14\u97f3\u89c6\u9891\u540c\u6b65\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2506.08004", "pdf": "https://arxiv.org/pdf/2506.08004", "abs": "https://arxiv.org/abs/2506.08004", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "title": "Dynamic View Synthesis as an Inverse Problem", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://inverse-dvs.github.io/", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8e\u5355\u76ee\u89c6\u9891\u7684\u52a8\u6001\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u566a\u58f0\u521d\u59cb\u5316\u9636\u6bb5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u52a8\u6001\u89c6\u56fe\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u786e\u5b9a\u6027\u53cd\u8f6c\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u96f6\u7ec8\u7aef\u4fe1\u566a\u6bd4\uff08SNR\uff09\u8c03\u5ea6\u4e0b\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165K\u9636\u9012\u5f52\u566a\u58f0\u8868\u793a\uff08K-order Recursive Noise Representation\uff09\u548c\u968f\u673a\u6f5c\u5728\u8c03\u5236\uff08Stochastic Latent Modulation\uff09\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u901a\u8fc7\u566a\u58f0\u521d\u59cb\u5316\u9636\u6bb5\u7684\u7ed3\u6784\u5316\u6f5c\u5728\u64cd\u4f5c\uff0c\u53ef\u4ee5\u6709\u6548\u5b9e\u73b0\u52a8\u6001\u89c6\u56fe\u5408\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u6743\u91cd\u66f4\u65b0\u6216\u8f85\u52a9\u6a21\u5757\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u52a8\u6001\u89c6\u56fe\u5408\u6210\u3002"}}
{"id": "2506.08005", "pdf": "https://arxiv.org/pdf/2506.08005", "abs": "https://arxiv.org/abs/2506.08005", "authors": ["Lei Lai", "Zekai Yin", "Eshed Ohn-Bar"], "title": "ZeroVO: Visual Odometry with Minimal Assumptions", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves\nzero-shot generalization across diverse cameras and environments, overcoming\nlimitations in existing methods that depend on predefined or static camera\ncalibration setups. Our approach incorporates three main innovations. First, we\ndesign a calibration-free, geometry-aware network structure capable of handling\nnoise in estimated depth and camera parameters. Second, we introduce a\nlanguage-based prior that infuses semantic information to enhance robust\nfeature extraction and generalization to previously unseen domains. Third, we\ndevelop a flexible, semi-supervised training paradigm that iteratively adapts\nto new scenes using unlabeled data, further boosting the models' ability to\ngeneralize across diverse real-world scenarios. We analyze complex autonomous\ndriving contexts, demonstrating over 30% improvement against prior methods on\nthree standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly\nintroduced, high-fidelity synthetic dataset derived from Grand Theft Auto\n(GTA). By not requiring fine-tuning or camera calibration, our work broadens\nthe applicability of VO, providing a versatile solution for real-world\ndeployment at scale.", "AI": {"tldr": "ZeroVO\u662f\u4e00\u79cd\u65b0\u578b\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7b97\u6cd5\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u6216\u9759\u6001\u76f8\u673a\u6821\u51c6\uff0c\u5b9e\u73b0\u4e86\u8de8\u591a\u6837\u76f8\u673a\u548c\u73af\u5883\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u6216\u9759\u6001\u76f8\u673a\u6821\u51c6\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002ZeroVO\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u63d0\u4f9b\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u8bbe\u8ba1\u65e0\u6821\u51c6\u3001\u51e0\u4f55\u611f\u77e5\u7684\u7f51\u7edc\u7ed3\u6784\uff1b2. \u5f15\u5165\u57fa\u4e8e\u8bed\u8a00\u7684\u5148\u9a8c\u589e\u5f3a\u8bed\u4e49\u4fe1\u606f\uff1b3. \u5f00\u53d1\u534a\u76d1\u7763\u8bad\u7ec3\u8303\u5f0f\uff0c\u9002\u5e94\u65b0\u573a\u666f\u3002", "result": "\u5728KITTI\u3001nuScenes\u548cArgoverse 2\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc730%\u3002", "conclusion": "ZeroVO\u65e0\u9700\u5fae\u8c03\u6216\u76f8\u673a\u6821\u51c6\uff0c\u6269\u5c55\u4e86\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u9002\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.08006", "pdf": "https://arxiv.org/pdf/2506.08006", "abs": "https://arxiv.org/abs/2506.08006", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "categories": ["cs.CV"], "comment": "Project Page: https://metadriverse.github.io/dreamland/", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "AI": {"tldr": "Dreamland\u662f\u4e00\u4e2a\u7ed3\u5408\u7269\u7406\u6a21\u62df\u5668\u548c\u751f\u6210\u6a21\u578b\u7684\u6df7\u5408\u4e16\u754c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u62bd\u8c61\u589e\u5f3a\u53ef\u63a7\u6027\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u5143\u7d20\u7ea7\u53ef\u63a7\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u573a\u666f\u7f16\u8f91\u548cAI\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u5206\u5c42\u4e16\u754c\u62bd\u8c61\uff0c\u7ed3\u5408\u7269\u7406\u6a21\u62df\u5668\u7684\u63a7\u5236\u529b\u548c\u751f\u6210\u6a21\u578b\u7684\u771f\u5b9e\u611f\uff0c\u4f7f\u7528\u4e2d\u95f4\u8868\u793a\u6865\u63a5\u4e24\u8005\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDreamland\u56fe\u50cf\u8d28\u91cf\u63d0\u534750.8%\uff0c\u53ef\u63a7\u6027\u589e\u5f3a17.9%\uff0c\u5e76\u652f\u6301\u73b0\u6210\u751f\u6210\u6a21\u578b\u3002", "conclusion": "Dreamland\u5728\u53ef\u63a7\u6027\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u671b\u63a8\u52a8AI\u4ee3\u7406\u8bad\u7ec3\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.08008", "pdf": "https://arxiv.org/pdf/2506.08008", "abs": "https://arxiv.org/abs/2506.08008", "authors": ["Stephanie Fu", "Tyler Bonnen", "Devin Guillory", "Trevor Darrell"], "title": "Hidden in plain sight: VLMs overlook their visual representations", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://hidden-plain-sight.github.io/", "summary": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e0e\u5176\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6027\u80fd\uff0c\u53d1\u73b0VLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u66f4\u5dee\uff0c\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\u3002\u74f6\u9888\u5728\u4e8eVLMs\u672a\u80fd\u6709\u6548\u5229\u7528\u89c6\u89c9\u4fe1\u606f\uff0c\u4e14\u53d7\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22VLMs\u5982\u4f55\u6574\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u4fe1\u606f\uff0c\u5e76\u8bca\u65ad\u5176\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u89c6\u89c9\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982\u6df1\u5ea6\u4f30\u8ba1\u3001\u5bf9\u5e94\u5173\u7cfb\uff09\u6bd4\u8f83VLMs\u4e0e\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790VLMs\u7684\u74f6\u9888\u3002", "result": "VLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u672a\u80fd\u6709\u6548\u5229\u7528\u89c6\u89c9\u4fe1\u606f\u4e14\u53d7\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765VLMs\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u548c\u65b9\u5411\u3002"}}
{"id": "2506.08009", "pdf": "https://arxiv.org/pdf/2506.08009", "abs": "https://arxiv.org/abs/2506.08009", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project website: http://self-forcing.github.io/", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "AI": {"tldr": "Self Forcing\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u8f93\u51fa\u548cKV\u7f13\u5b58\u89e3\u51b3\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u63a8\u7406\u65f6\u56e0\u4f9d\u8d56\u81ea\u8eab\u4e0d\u5b8c\u7f8e\u8f93\u51fa\u800c\u5bfc\u81f4\u7684\u66dd\u5149\u504f\u5dee\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u5c55\u5f00\u548cKV\u7f13\u5b58\uff0c\u7ed3\u5408\u591a\u6b65\u6269\u6563\u6a21\u578b\u548c\u968f\u673a\u68af\u5ea6\u622a\u65ad\u7b56\u7565\uff0c\u63d0\u51fa\u6eda\u52a8KV\u7f13\u5b58\u673a\u5236\u3002", "result": "\u5728\u5355GPU\u4e0a\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u5ef6\u8fdf\u7684\u5b9e\u65f6\u89c6\u9891\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u975e\u56e0\u679c\u6269\u6563\u6a21\u578b\u3002", "conclusion": "Self Forcing\u5728\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2506.08010", "pdf": "https://arxiv.org/pdf/2506.08010", "abs": "https://arxiv.org/abs/2506.08010", "authors": ["Nick Jiang", "Amil Dravid", "Alexei Efros", "Yossi Gandelsman"], "title": "Vision Transformers Don't Need Trained Registers", "categories": ["cs.CV", "cs.AI"], "comment": "Project page and code: https://avdravid.github.io/test-time-registers", "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Vision Transformers\u4e2d\u5b58\u5728\u9ad8\u8303\u6570token\u5bfc\u81f4\u6ce8\u610f\u529b\u56fe\u566a\u58f0\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6c\u79fb\u9ad8\u8303\u6570\u6fc0\u6d3b\u5230\u989d\u5916token\u6765\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3Vision Transformers\u4e2d\u9ad8\u8303\u6570token\u5bfc\u81f4\u7684\u6ce8\u610f\u529b\u566a\u58f0\u95ee\u9898\uff0c\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u6210\u672c\u3002", "method": "\u901a\u8fc7\u5c06\u9ad8\u8303\u6570\u6fc0\u6d3b\u8f6c\u79fb\u5230\u989d\u5916\u672a\u8bad\u7ec3\u7684token\uff0c\u6a21\u62df\u6ce8\u518ctoken\u7684\u6548\u679c\u3002", "result": "\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u6e05\u6670\u7684\u6ce8\u610f\u529b\u548c\u7279\u5f81\u56fe\uff0c\u63d0\u5347\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\uff0c\u6548\u679c\u63a5\u8fd1\u663e\u5f0f\u8bad\u7ec3\u6ce8\u518ctoken\u7684\u6a21\u578b\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u6ce8\u518c\u65b9\u6cd5\u4e3a\u672a\u9884\u7f6e\u6ce8\u518ctoken\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u4f9b\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08011", "pdf": "https://arxiv.org/pdf/2506.08011", "abs": "https://arxiv.org/abs/2506.08011", "authors": ["Yunfei Xie", "Yinsong Ma", "Shiyi Lan", "Alan Yuille", "Junfei Xiao", "Chen Wei"], "title": "Play to Generalize: Learning to Reason Through Game Play", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://yunfeixie233.github.io/ViGaL/", "summary": "Developing generalizable reasoning capabilities in multimodal large language\nmodels (MLLMs) remains challenging. Motivated by cognitive science literature\nsuggesting that gameplay promotes transferable cognitive skills, we propose a\nnovel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs\ndevelop out-of-domain generalization of multimodal reasoning through playing\narcade-like games. Specifically, we show that post-training a 7B-parameter MLLM\nvia reinforcement learning (RL) on simple arcade-like games, e.g. Snake,\nsignificantly enhances its downstream performance on multimodal math benchmarks\nlike MathVista, and on multi-discipline questions like MMMU, without seeing any\nworked solutions, equations, or diagrams during RL, suggesting the capture of\ntransferable reasoning skills. Remarkably, our model outperforms specialist\nmodels tuned on multimodal reasoning data in multimodal reasoning benchmarks,\nwhile preserving the base model's performance on general visual benchmarks, a\nchallenge where specialist models often fall short. Our findings suggest a new\npost-training paradigm: synthetic, rule-based games can serve as controllable\nand scalable pre-text tasks that unlock generalizable multimodal reasoning\nabilities in MLLMs.", "AI": {"tldr": "\u901a\u8fc7\u89c6\u89c9\u6e38\u620f\u5b66\u4e60\uff08ViGaL\uff09\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u6cdb\u5316\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u7279\u5b9a\u9886\u57df\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6570\u5b66\u548c\u8de8\u5b66\u79d1\u95ee\u9898\u7684\u8868\u73b0\u3002", "motivation": "\u8ba4\u77e5\u79d1\u5b66\u7814\u7a76\u8868\u660e\uff0c\u6e38\u620f\u80fd\u4fc3\u8fdb\u53ef\u8fc1\u79fb\u7684\u8ba4\u77e5\u6280\u80fd\uff0c\u56e0\u6b64\u63d0\u51fa\u901a\u8fc7\u6e38\u620f\u8bad\u7ec3\u63d0\u5347MLLMs\u7684\u6cdb\u5316\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u7b80\u5355\u8857\u673a\u6e38\u620f\uff08\u5982\u8d2a\u5403\u86c7\uff09\u4e0a\u5bf97B\u53c2\u6570\u7684MLLM\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u591a\u6a21\u6001\u6570\u5b66\u57fa\u51c6\uff08MathVista\uff09\u548c\u8de8\u5b66\u79d1\u95ee\u9898\uff08MMMU\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u5728\u901a\u7528\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u89c4\u5219\u7684\u6e38\u620f\u53ef\u4f5c\u4e3a\u53ef\u63a7\u4e14\u53ef\u6269\u5c55\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u89e3\u9501MLLMs\u7684\u6cdb\u5316\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.08013", "pdf": "https://arxiv.org/pdf/2506.08013", "abs": "https://arxiv.org/abs/2506.08013", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code is available at https://github.com/astra-vision/StableMTL", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "AI": {"tldr": "StableMTL\u5229\u7528\u6269\u6563\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u901a\u8fc7\u7edf\u4e00\u6f5c\u5728\u635f\u5931\u548c\u4efb\u52a1\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u90e8\u5206\u4efb\u52a1\u6807\u6ce8\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u6269\u5c55\u81f3\u96f6\u6837\u672c\u8bbe\u7f6e\u3002", "method": "\u91c7\u7528\u56fe\u50cf\u751f\u6210\u5668\u8fdb\u884c\u6f5c\u5728\u56de\u5f52\uff0c\u7ed3\u5408\u4efb\u52a1\u7f16\u7801\u3001\u4efb\u52a1\u6761\u4ef6\u5316\u548c\u5b9a\u5236\u8bad\u7ec3\u65b9\u6848\uff0c\u5f15\u5165\u591a\u6d41\u6a21\u578b\u548c\u4efb\u52a1\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u76847\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "StableMTL\u901a\u8fc7\u7edf\u4e00\u635f\u5931\u548c\u4efb\u52a1\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u3002"}}
{"id": "2506.08015", "pdf": "https://arxiv.org/pdf/2506.08015", "abs": "https://arxiv.org/abs/2506.08015", "authors": ["Zhen Xu", "Zhengqin Li", "Zhao Dong", "Xiaowei Zhou", "Richard Newcombe", "Zhaoyang Lv"], "title": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos", "categories": ["cs.CV"], "comment": "Project page: https://4dgt.github.io", "summary": "We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene\nreconstruction, trained entirely on real-world monocular posed videos. Using 4D\nGaussian as an inductive bias, 4DGT unifies static and dynamic components,\nenabling the modeling of complex, time-varying environments with varying object\nlifespans. We proposed a novel density control strategy in training, which\nenables our 4DGT to handle longer space-time input and remain efficient\nrendering at runtime. Our model processes 64 consecutive posed frames in a\nrolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike\noptimization-based methods, 4DGT performs purely feed-forward inference,\nreducing reconstruction time from hours to seconds and scaling effectively to\nlong video sequences. Trained only on large-scale monocular posed video\ndatasets, 4DGT can outperform prior Gaussian-based networks significantly in\nreal-world videos and achieve on-par accuracy with optimization-based methods\non cross-domain videos. Project page: https://4dgt.github.io", "AI": {"tldr": "4DGT\u662f\u4e00\u79cd\u57fa\u4e8e4D\u9ad8\u65af\u548cTransformer\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u76ee\u89c6\u9891\u8bad\u7ec3\uff0c\u7edf\u4e00\u9759\u6001\u548c\u52a8\u6001\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\u3002", "motivation": "\u52a8\u6001\u573a\u666f\u91cd\u5efa\u9700\u8981\u5904\u7406\u590d\u6742\u7684\u65f6\u95f4\u53d8\u5316\u548c\u5bf9\u8c61\u751f\u547d\u5468\u671f\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u4f7f\u75284D\u9ad8\u65af\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u63d0\u51fa\u5bc6\u5ea6\u63a7\u5236\u7b56\u7565\uff0c\u6eda\u52a8\u5904\u740664\u5e27\u8f93\u5165\uff0c\u5b9e\u73b0\u524d\u9988\u63a8\u7406\u3002", "result": "\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u9ad8\u65af\u7f51\u7edc\uff0c\u8de8\u57df\u89c6\u9891\u4e2d\u4e0e\u4f18\u5316\u65b9\u6cd5\u7cbe\u5ea6\u76f8\u5f53\uff0c\u91cd\u5efa\u65f6\u95f4\u4ece\u5c0f\u65f6\u7ea7\u964d\u81f3\u79d2\u7ea7\u3002", "conclusion": "4DGT\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u9002\u7528\u4e8e\u957f\u89c6\u9891\u5e8f\u5217\u3002"}}
{"id": "2506.06290", "pdf": "https://arxiv.org/pdf/2506.06290", "abs": "https://arxiv.org/abs/2506.06290", "authors": ["Mingyu Lu", "Ethan Weinberger", "Chanwoo Kim", "Su-In Lee"], "title": "CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "High-content screening (HCS) assays based on high-throughput microscopy\ntechniques such as Cell Painting have enabled the interrogation of cells'\nmorphological responses to perturbations at an unprecedented scale. The\ncollection of such data promises to facilitate a better understanding of the\nrelationships between different perturbations and their effects on cellular\nstate. Towards achieving this goal, recent advances in cross-modal contrastive\nlearning could, in theory, be leveraged to learn a unified latent space that\naligns perturbations with their corresponding morphological effects. However,\nthe application of such methods to HCS data is not straightforward due to\nsubstantial differences in the semantics of Cell Painting images compared to\nnatural images, and the difficulty of representing different classes of\nperturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent\nspace. In response to these challenges, here we introduce CellCLIP, a\ncross-modal contrastive learning framework for HCS data. CellCLIP leverages\npre-trained image encoders coupled with a novel channel encoding scheme to\nbetter capture relationships between different microscopy channels in image\nembeddings, along with natural language encoders for representing\nperturbations. Our framework outperforms current open-source models,\ndemonstrating the best performance in both cross-modal retrieval and\nbiologically meaningful downstream tasks while also achieving significant\nreductions in computation time.", "AI": {"tldr": "CellCLIP\u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u5185\u6db5\u7b5b\u9009\u6570\u636e\u7684\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\u548c\u65b0\u578b\u901a\u9053\u7f16\u7801\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u5229\u7528\u9ad8\u5185\u6db5\u7b5b\u9009\u6570\u636e\u7406\u89e3\u6270\u52a8\u4e0e\u7ec6\u80de\u5f62\u6001\u6548\u5e94\u7684\u5173\u7cfb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u56fe\u50cf\u8bed\u4e49\u5dee\u5f02\u548c\u6270\u52a8\u7c7b\u522b\u591a\u6837\u6027\u800c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\u3001\u65b0\u578b\u901a\u9053\u7f16\u7801\u65b9\u6848\u548c\u81ea\u7136\u8bed\u8a00\u7f16\u7801\u5668\uff0c\u6784\u5efa\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6CellCLIP\u3002", "result": "CellCLIP\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u4e0b\u6e38\u751f\u7269\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "CellCLIP\u4e3a\u9ad8\u5185\u6db5\u7b5b\u9009\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2506.06306", "pdf": "https://arxiv.org/pdf/2506.06306", "abs": "https://arxiv.org/abs/2506.06306", "authors": ["Ali Abedi", "Charlene H. Chu", "Shehroz S. Khan"], "title": "Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning", "categories": ["eess.SP", "cs.CV", "cs.HC", "cs.LG"], "comment": "16 pages, 4 figures, 2 tables", "summary": "Agitation is one of the most common responsive behaviors in people living\nwith dementia, particularly among those residing in community settings without\ncontinuous clinical supervision. Timely prediction of agitation can enable\nearly intervention, reduce caregiver burden, and improve the quality of life\nfor both patients and caregivers. This study aimed to develop and benchmark\nmachine learning approaches for the early prediction of agitation in\ncommunity-dwelling older adults with dementia using multimodal sensor data. A\nnew set of agitation-related contextual features derived from activity data was\nintroduced and employed for agitation prediction. A wide range of machine\nlearning and deep learning models was evaluated across multiple problem\nformulations, including binary classification for single-timestamp tabular\nsensor data and multi-timestamp sequential sensor data, as well as anomaly\ndetection for single-timestamp tabular sensor data. The study utilized the\nTechnology Integrated Health Management (TIHM) dataset, the largest publicly\navailable dataset for remote monitoring of people living with dementia,\ncomprising 2,803 days of in-home activity, physiology, and sleep data. The most\neffective setting involved binary classification of sensor data using the\ncurrent 6-hour timestamp to predict agitation at the subsequent timestamp.\nIncorporating additional information, such as time of day and agitation\nhistory, further improved model performance, with the highest AUC-ROC of 0.9720\nand AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work\npresents the first comprehensive benchmarking of state-of-the-art techniques\nfor agitation prediction in community-based dementia care using\nprivacy-preserving sensor data. The approach enables accurate, explainable, and\nefficient agitation prediction, supporting proactive dementia care and aging in\nplace.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u65e9\u671f\u9884\u6d4b\u793e\u533a\u5c45\u4f4f\u7684\u75f4\u5446\u75c7\u60a3\u8005\u7684\u6fc0\u8d8a\u884c\u4e3a\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u4ee5\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u6fc0\u8d8a\u884c\u4e3a\u662f\u75f4\u5446\u75c7\u60a3\u8005\u5e38\u89c1\u7684\u53cd\u5e94\u884c\u4e3a\uff0c\u65e9\u671f\u9884\u6d4b\u53ef\u4ee5\u51cf\u8f7b\u62a4\u7406\u8d1f\u62c5\u5e76\u63d0\u9ad8\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u7814\u7a76\u4f7f\u7528TIHM\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec\u4e8c\u5143\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u8fbe\u5230AUC-ROC 0.9720\u548cAUC-PR 0.4320\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u9690\u79c1\u4fdd\u62a4\u4f20\u611f\u5668\u6570\u636e\u7684\u6fc0\u8d8a\u884c\u4e3a\u9884\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u51c6\uff0c\u652f\u6301\u4e3b\u52a8\u62a4\u7406\u548c\u5c45\u5bb6\u517b\u8001\u3002"}}
{"id": "2506.06315", "pdf": "https://arxiv.org/pdf/2506.06315", "abs": "https://arxiv.org/abs/2506.06315", "authors": ["Masoud Rahimi", "Reza Karbasi", "Abdol-Hossein Vahabie"], "title": "An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": "5 pages, 5 figures", "summary": "We introduce an open-source Python framework for generating synthetic ECG\nimage datasets to advance critical deep learning-based tasks in ECG analysis,\nincluding ECG digitization, lead region and lead name detection, and\npixel-level waveform segmentation. Using the PTB-XL signal dataset, our\nproposed framework produces four open-access datasets: (1) ECG images in\nvarious lead configurations paired with time-series signals for ECG\ndigitization, (2) ECG images annotated with YOLO-format bounding boxes for\ndetection of lead region and lead name, (3)-(4) cropped single-lead images with\nsegmentation masks compatible with U-Net-based models in normal and overlapping\nversions. In the overlapping case, waveforms from neighboring leads are\nsuperimposed onto the target lead image, while the segmentation masks remain\nclean. The open-source Python framework and datasets are publicly available at\nhttps://github.com/rezakarbasi/ecg-image-and-signal-dataset and\nhttps://doi.org/10.5281/zenodo.15484519, respectively.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5f00\u6e90Python\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5408\u6210ECG\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u652f\u6301\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u5982ECG\u6570\u5b57\u5316\u3001\u5bfc\u8054\u533a\u57df\u548c\u540d\u79f0\u68c0\u6d4b\u4ee5\u53ca\u6ce2\u5f62\u5206\u5272\u3002", "motivation": "\u63a8\u52a8ECG\u5206\u6790\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u5f00\u6e90\u6570\u636e\u96c6\u548c\u5de5\u5177\u3002", "method": "\u57fa\u4e8ePTB-XL\u4fe1\u53f7\u6570\u636e\u96c6\uff0c\u751f\u6210\u56db\u79cd\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5305\u62ecECG\u56fe\u50cf\u4e0e\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u914d\u5bf9\u3001YOLO\u683c\u5f0f\u6807\u6ce8\u7684\u5bfc\u8054\u533a\u57df\u548c\u540d\u79f0\u68c0\u6d4b\u6570\u636e\u3001\u5355\u5bfc\u8054\u5206\u5272\u63a9\u7801\u56fe\u50cf\u3002", "result": "\u751f\u6210\u4e86\u56db\u79cd\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u591a\u79cdECG\u5206\u6790\u4efb\u52a1\u3002", "conclusion": "\u5f00\u6e90\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3aECG\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2506.06349", "pdf": "https://arxiv.org/pdf/2506.06349", "abs": "https://arxiv.org/abs/2506.06349", "authors": ["Thien Nhan Vo", "Thanh Xuan Truong"], "title": "Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": null, "summary": "This study addresses the classification of heartbeats from ECG signals\nthrough two distinct approaches: traditional machine learning utilizing\nhand-crafted features and deep learning via transformed images of ECG beats.\nThe dataset underwent preprocessing steps, including downsampling, filtering,\nand normalization, to ensure consistency and relevance for subsequent analysis.\nIn the first approach, features such as heart rate variability (HRV), mean,\nvariance, and RR intervals were extracted to train various classifiers,\nincluding SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and\nLightGBM. The second approach involved transforming ECG signals into images\nusing Gramian Angular Field (GAF), Markov Transition Field (MTF), and\nRecurrence Plots (RP), with these images subsequently classified using CNN\narchitectures like VGG and Inception.\n  Experimental results demonstrate that the LightGBM model achieved the highest\nperformance, with an accuracy of 99% and an F1 score of 0.94, outperforming the\nimage-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost\nyielded significantly lower scores, indicating limited suitability for this\ntask. The findings underscore the superior ability of hand-crafted features to\ncapture temporal and morphological variations in ECG signals compared to\nimage-based representations of individual beats. Future investigations may\nbenefit from incorporating multi-lead ECG signals and temporal dependencies\nacross successive beats to enhance classification accuracy further.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bf9ECG\u4fe1\u53f7\u5206\u7c7b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u7684LightGBM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe99%\uff0c\u4f18\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684CNN\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8ECG\u4fe1\u53f7\u5206\u7c7b\u7684\u4e24\u79cd\u65b9\u6cd5\uff08\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff09\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4ee5\u4f18\u5316\u5fc3\u8df3\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "method": "1. \u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff1a\u63d0\u53d6HRV\u3001\u5747\u503c\u3001\u65b9\u5dee\u7b49\u7279\u5f81\uff0c\u8bad\u7ec3SVM\u3001\u968f\u673a\u68ee\u6797\u7b49\u5206\u7c7b\u5668\uff1b2. \u6df1\u5ea6\u5b66\u4e60\uff1a\u5c06ECG\u4fe1\u53f7\u8f6c\u6362\u4e3a\u56fe\u50cf\uff08GAF\u3001MTF\u3001RP\uff09\uff0c\u7528CNN\u5206\u7c7b\u3002", "result": "LightGBM\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff08\u51c6\u786e\u738799%\uff0cF1\u5206\u65700.94\uff09\uff0c\u4f18\u4e8eCNN\u65b9\u6cd5\uff08F1\u5206\u65700.85\uff09\u3002", "conclusion": "\u624b\u5de5\u7279\u5f81\u80fd\u66f4\u597d\u5730\u6355\u6349ECG\u4fe1\u53f7\u7684\u65f6\u7a7a\u53d8\u5316\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u591a\u5bfc\u8054\u4fe1\u53f7\u548c\u65f6\u5e8f\u4f9d\u8d56\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002"}}
{"id": "2506.06355", "pdf": "https://arxiv.org/pdf/2506.06355", "abs": "https://arxiv.org/abs/2506.06355", "authors": ["Lingyao Li", "Dawei Li", "Zhenhui Ou", "Xiaoran Xu", "Jingxiao Liu", "Zihui Ma", "Runlong Yu", "Min Deng"], "title": "LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment", "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.CV"], "comment": null, "summary": "Efficient simulation is essential for enhancing proactive preparedness for\nsudden-onset disasters such as earthquakes. Recent advancements in large\nlanguage models (LLMs) as world models show promise in simulating complex\nscenarios. This study examines multiple LLMs to proactively estimate perceived\nearthquake impacts. Leveraging multimodal datasets including geospatial,\nsocioeconomic, building, and street-level imagery data, our framework generates\nModified Mercalli Intensity (MMI) predictions at zip code and county scales.\nEvaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did\nYou Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced\nby a high correlation of 0.88 and a low RMSE of 0.77 as compared to real\nreports at the zip code level. Techniques such as RAG and ICL can improve\nsimulation performance, while visual inputs notably enhance accuracy compared\nto structured numerical data alone. These findings show the promise of LLMs in\nsimulating disaster impacts that can help strengthen pre-event planning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6a21\u62df\u5730\u9707\u5f71\u54cd\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u751f\u6210MMI\u9884\u6d4b\uff0c\u7ed3\u679c\u663e\u793a\u4e0e\u771f\u5b9e\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u63d0\u5347\u7a81\u53d1\u707e\u5bb3\uff08\u5982\u5730\u9707\uff09\u7684\u4e3b\u52a8\u5e94\u5bf9\u80fd\u529b\uff0c\u63a2\u7d22LLMs\u5728\u590d\u6742\u573a\u666f\u6a21\u62df\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08\u5730\u7406\u7a7a\u95f4\u3001\u793e\u4f1a\u7ecf\u6d4e\u3001\u5efa\u7b51\u548c\u8857\u666f\u56fe\u50cf\uff09\uff0c\u7ed3\u5408RAG\u548cICL\u6280\u672f\uff0c\u751f\u6210MMI\u9884\u6d4b\u3002", "result": "\u57282014\u5e74Napa\u548c2019\u5e74Ridgecrest\u5730\u9707\u4e2d\uff0c\u9884\u6d4b\u4e0eUSGS\u62a5\u544a\u9ad8\u5ea6\u76f8\u5173\uff080.88\uff09\uff0cRMSE\u4f4e\u81f30.77\u3002", "conclusion": "LLMs\u5728\u707e\u5bb3\u5f71\u54cd\u6a21\u62df\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u652f\u6301\u707e\u524d\u89c4\u5212\u3002"}}
{"id": "2506.06394", "pdf": "https://arxiv.org/pdf/2506.06394", "abs": "https://arxiv.org/abs/2506.06394", "authors": ["Yash Turkar", "Youngjin Kim", "Karthik Dantu"], "title": "Active Illumination Control in Low-Light Environments using NightHawk", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Subterranean environments such as culverts present significant challenges to\nrobot vision due to dim lighting and lack of distinctive features. Although\nonboard illumination can help, it introduces issues such as specular\nreflections, overexposure, and increased power consumption. We propose\nNightHawk, a framework that combines active illumination with exposure control\nto optimize image quality in these settings. NightHawk formulates an online\nBayesian optimization problem to determine the best light intensity and\nexposure-time for a given scene. We propose a novel feature detector-based\nmetric to quantify image utility and use it as the cost function for the\noptimizer. We built NightHawk as an event-triggered recursive optimization\npipeline and deployed it on a legged robot navigating a culvert beneath the\nErie Canal. Results from field experiments demonstrate improvements in feature\ndetection and matching by 47-197% enabling more reliable visual estimation in\nchallenging lighting conditions.", "AI": {"tldr": "NightHawk\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e3b\u52a8\u7167\u660e\u548c\u66dd\u5149\u63a7\u5236\uff0c\u4f18\u5316\u4e86\u5730\u4e0b\u73af\u5883\u4e2d\u673a\u5668\u4eba\u89c6\u89c9\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f81\u68c0\u6d4b\u548c\u5339\u914d\u6027\u80fd\u3002", "motivation": "\u5730\u4e0b\u73af\u5883\uff08\u5982\u6db5\u6d1e\uff09\u5149\u7ebf\u660f\u6697\u4e14\u7f3a\u4e4f\u663e\u8457\u7279\u5f81\uff0c\u7ed9\u673a\u5668\u4eba\u89c6\u89c9\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u7167\u660e\u65b9\u6cd5\u5b58\u5728\u53cd\u5c04\u3001\u8fc7\u66dd\u548c\u529f\u8017\u95ee\u9898\u3002", "method": "\u63d0\u51faNightHawk\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u8d1d\u53f6\u65af\u4f18\u5316\u52a8\u6001\u8c03\u6574\u5149\u7167\u5f3a\u5ea6\u548c\u66dd\u5149\u65f6\u95f4\uff0c\u5e76\u57fa\u4e8e\u7279\u5f81\u68c0\u6d4b\u5668\u8bbe\u8ba1\u65b0\u7684\u56fe\u50cf\u6548\u7528\u5ea6\u91cf\u4f5c\u4e3a\u4f18\u5316\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7279\u5f81\u68c0\u6d4b\u548c\u5339\u914d\u6027\u80fd\u63d0\u534747-197%\uff0c\u5728\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u89c6\u89c9\u4f30\u8ba1\u3002", "conclusion": "NightHawk\u6709\u6548\u89e3\u51b3\u4e86\u5730\u4e0b\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06400", "pdf": "https://arxiv.org/pdf/2506.06400", "abs": "https://arxiv.org/abs/2506.06400", "authors": ["Changsheng Fang", "Yongtong Liu", "Bahareh Morovati", "Shuo Han", "Yu Shi", "Li Zhou", "Shuyi Fan", "Hengyong Yu"], "title": "ResPF: Residual Poisson Flow for Efficient and Physically Consistent Sparse-View CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Sparse-view computed tomography (CT) is a practical solution to reduce\nradiation dose, but the resulting ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Although deep learning and\ndiffusion-based methods have shown promising results, they often lack physical\ninterpretability or suffer from high computational costs due to iterative\nsampling starting from random noise. Recent advances in generative modeling,\nparticularly Poisson Flow Generative Models (PFGM), enable high-fidelity image\nsynthesis by modeling the full data distribution. In this work, we propose\nResidual Poisson Flow (ResPF) Generative Models for efficient and accurate\nsparse-view CT reconstruction. Based on PFGM++, ResPF integrates conditional\nguidance from sparse measurements and employs a hijacking strategy to\nsignificantly reduce sampling cost by skipping redundant initial steps.\nHowever, skipping early stages can degrade reconstruction quality and introduce\nunrealistic structures. To address this, we embed a data-consistency into each\niteration, ensuring fidelity to sparse-view measurements. Yet, PFGM sampling\nrelies on a fixed ordinary differential equation (ODE) trajectory induced by\nelectrostatic fields, which can be disrupted by step-wise data consistency,\nresulting in unstable or degraded reconstructions. Inspired by ResNet, we\nintroduce a residual fusion module to linearly combine generative outputs with\ndata-consistent reconstructions, effectively preserving trajectory continuity.\nTo the best of our knowledge, this is the first application of Poisson flow\nmodels to sparse-view CT. Extensive experiments on synthetic and clinical\ndatasets demonstrate that ResPF achieves superior reconstruction quality,\nfaster inference, and stronger robustness compared to state-of-the-art\niterative, learning-based, and diffusion models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePoisson Flow\u751f\u6210\u6a21\u578b\uff08ResPF\uff09\u7684\u9ad8\u6548\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u5f15\u5bfc\u548c\u6b8b\u5dee\u878d\u5408\u6a21\u5757\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u4e0e\u901f\u5ea6\u3002", "motivation": "\u7a00\u758f\u89c6\u56feCT\u53ef\u51cf\u5c11\u8f90\u5c04\u5242\u91cf\uff0c\u4f46\u91cd\u5efa\u95ee\u9898\u590d\u6742\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u548c\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u7269\u7406\u53ef\u89e3\u91ca\u6027\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u57fa\u4e8ePFGM++\uff0c\u5f15\u5165\u6761\u4ef6\u5f15\u5bfc\u548c\u6b8b\u5dee\u878d\u5408\u6a21\u5757\uff0c\u8df3\u8fc7\u5197\u4f59\u521d\u59cb\u6b65\u9aa4\u5e76\u5d4c\u5165\u6570\u636e\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "result": "\u5728\u5408\u6210\u548c\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0cResPF\u5728\u91cd\u5efa\u8d28\u91cf\u3001\u63a8\u7406\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ResPF\u9996\u6b21\u5c06Poisson Flow\u6a21\u578b\u5e94\u7528\u4e8e\u7a00\u758f\u89c6\u56feCT\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2506.06412", "pdf": "https://arxiv.org/pdf/2506.06412", "abs": "https://arxiv.org/abs/2506.06412", "authors": ["Junming Wang", "Yi Shi"], "title": "NeurNCD: Novel Class Discovery via Implicit Neural Representation", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by ICMR 2024", "summary": "Discovering novel classes in open-world settings is crucial for real-world\napplications. Traditional explicit representations, such as object descriptors\nor 3D segmentation maps, are constrained by their discrete, hole-prone, and\nnoisy nature, which hinders accurate novel class discovery. To address these\nchallenges, we introduce NeurNCD, the first versatile and data-efficient\nframework for novel class discovery that employs the meticulously designed\nEmbedding-NeRF model combined with KL divergence as a substitute for\ntraditional explicit 3D segmentation maps to aggregate semantic embedding and\nentropy in visual embedding space. NeurNCD also integrates several key\ncomponents, including feature query, feature modulation and clustering,\nfacilitating efficient feature augmentation and information exchange between\nthe pre-trained semantic segmentation network and implicit neural\nrepresentations. As a result, our framework achieves superior segmentation\nperformance in both open and closed-world settings without relying on densely\nlabelled datasets for supervised training or human interaction to generate\nsparse label supervision. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art approaches on the NYUv2 and Replica\ndatasets.", "AI": {"tldr": "NeurNCD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f00\u653e\u4e16\u754c\u7c7b\u522b\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Embedding-NeRF\u6a21\u578b\u548cKL\u6563\u5ea6\uff0c\u66ff\u4ee3\u4f20\u7edf\u663e\u5f0f3D\u5206\u5272\u56fe\uff0c\u63d0\u5347\u8bed\u4e49\u5d4c\u5165\u548c\u89c6\u89c9\u5d4c\u5165\u7a7a\u95f4\u7684\u4fe1\u606f\u805a\u5408\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u663e\u5f0f\u8868\u793a\uff08\u5982\u5bf9\u8c61\u63cf\u8ff0\u7b26\u62163D\u5206\u5272\u56fe\uff09\u5b58\u5728\u79bb\u6563\u3001\u6613\u4ea7\u751f\u7a7a\u6d1e\u548c\u566a\u58f0\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u65b0\u7c7b\u522b\u7684\u51c6\u786e\u53d1\u73b0\u3002", "method": "\u91c7\u7528Embedding-NeRF\u6a21\u578b\u548cKL\u6563\u5ea6\uff0c\u7ed3\u5408\u7279\u5f81\u67e5\u8be2\u3001\u8c03\u5236\u548c\u805a\u7c7b\u7b49\u5173\u952e\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u589e\u5f3a\u548c\u4fe1\u606f\u4ea4\u6362\u3002", "result": "\u5728NYUv2\u548cReplica\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65e0\u9700\u5bc6\u96c6\u6807\u6ce8\u6570\u636e\u6216\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "NeurNCD\u4e3a\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u65b0\u7c7b\u522b\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6570\u636e\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06440", "pdf": "https://arxiv.org/pdf/2506.06440", "abs": "https://arxiv.org/abs/2506.06440", "authors": ["Chuhao Chen", "Zhiyang Dou", "Chen Wang", "Yiming Huang", "Anjun Chen", "Qiao Feng", "Jiatao Gu", "Lingjie Liu"], "title": "Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Faithfully reconstructing textured shapes and physical properties from videos\npresents an intriguing yet challenging problem. Significant efforts have been\ndedicated to advancing such a system identification problem in this area.\nPrevious methods often rely on heavy optimization pipelines with a\ndifferentiable simulator and renderer to estimate physical parameters. However,\nthese approaches frequently necessitate extensive hyperparameter tuning for\neach scene and involve a costly optimization process, which limits both their\npracticality and generalizability. In this work, we propose a novel framework,\nVid2Sim, a generalizable video-based approach for recovering geometry and\nphysical properties through a mesh-free reduced simulation based on Linear\nBlend Skinning (LBS), offering high computational efficiency and versatile\nrepresentation capability. Specifically, Vid2Sim first reconstructs the\nobserved configuration of the physical system from video using a feed-forward\nneural network trained to capture physical world knowledge. A lightweight\noptimization pipeline then refines the estimated appearance, geometry, and\nphysical properties to closely align with video observations within just a few\nminutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,\nmesh-free simulation with high efficiency. Extensive experiments demonstrate\nthat our method achieves superior accuracy and efficiency in reconstructing\ngeometry and physical properties from video data.", "AI": {"tldr": "Vid2Sim\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u7ebf\u6027\u6df7\u5408\u8499\u76ae\uff08LBS\uff09\u7684\u65e0\u7f51\u683c\u7b80\u5316\u6a21\u62df\uff0c\u4ece\u89c6\u9891\u4e2d\u9ad8\u6548\u6062\u590d\u51e0\u4f55\u548c\u7269\u7406\u5c5e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u4f18\u5316\u6d41\u7a0b\u548c\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u6027\u3002Vid2Sim\u65e8\u5728\u63d0\u4f9b\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u4ece\u89c6\u9891\u4e2d\u91cd\u5efa\u7269\u7406\u7cfb\u7edf\u914d\u7f6e\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4f18\u5316\u6d41\u7a0b\u7ec6\u5316\u4f30\u8ba1\u7684\u5916\u89c2\u3001\u51e0\u4f55\u548c\u7269\u7406\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVid2Sim\u5728\u91cd\u5efa\u51e0\u4f55\u548c\u7269\u7406\u5c5e\u6027\u65b9\u9762\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "Vid2Sim\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u89c6\u9891\u4e2d\u91cd\u5efa\u7eb9\u7406\u5f62\u72b6\u548c\u7269\u7406\u5c5e\u6027\u7684\u80fd\u529b\u3002"}}
{"id": "2506.06462", "pdf": "https://arxiv.org/pdf/2506.06462", "abs": "https://arxiv.org/abs/2506.06462", "authors": ["Nicol\u00e1s Violante", "Andreas Meuleman", "Alban Gauthier", "Fr\u00e9do Durand", "Thibault Groueix", "George Drettakis"], "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH Conference Papers 2025. Project site:\n  https://repo-sam.inria.fr/nerphys/splat-and-replace/", "summary": "We leverage repetitive elements in 3D scenes to improve novel view synthesis.\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly\nimproved novel view synthesis but renderings of unseen and occluded parts\nremain low-quality if the training views are not exhaustive enough. Our key\nobservation is that our environment is often full of repetitive elements. We\npropose to leverage those repetitions to improve the reconstruction of\nlow-quality parts of the scene due to poor coverage and occlusions. We propose\na method that segments each repeated instance in a 3DGS reconstruction,\nregisters them together, and allows information to be shared among instances.\nOur method improves the geometry while also accounting for appearance\nvariations across instances. We demonstrate our method on a variety of\nsynthetic and real scenes with typical repetitive elements, leading to a\nsubstantial improvement in the quality of novel view synthesis.", "AI": {"tldr": "\u5229\u75283D\u573a\u666f\u4e2d\u7684\u91cd\u590d\u5143\u7d20\u6539\u8fdb\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u901a\u8fc7\u5206\u5272\u3001\u6ce8\u518c\u548c\u5171\u4eab\u4fe1\u606f\u63d0\u5347\u51e0\u4f55\u548c\u5916\u89c2\u8d28\u91cf\u3002", "motivation": "NeRF\u548c3DGS\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u672a\u8986\u76d6\u6216\u906e\u6321\u90e8\u5206\u7684\u6e32\u67d3\u8d28\u91cf\u8f83\u5dee\u3002\u73af\u5883\u4e2d\u5e38\u5b58\u5728\u91cd\u590d\u5143\u7d20\uff0c\u5229\u7528\u8fd9\u4e9b\u91cd\u590d\u53ef\u4ee5\u63d0\u5347\u4f4e\u8d28\u91cf\u90e8\u5206\u7684\u91cd\u5efa\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5206\u52723DGS\u91cd\u5efa\u4e2d\u7684\u91cd\u590d\u5b9e\u4f8b\uff0c\u6ce8\u518c\u5e76\u5171\u4eab\u4fe1\u606f\uff0c\u540c\u65f6\u8003\u8651\u5b9e\u4f8b\u95f4\u7684\u5916\u89c2\u53d8\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u91cd\u590d\u5143\u7d20\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u51e0\u4f55\u548c\u5916\u89c2\u8d28\u91cf\u3002"}}
{"id": "2506.06474", "pdf": "https://arxiv.org/pdf/2506.06474", "abs": "https://arxiv.org/abs/2506.06474", "authors": ["Everett Richards", "Bipul Thapa", "Lena Mashayekhy"], "title": "Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.MA", "cs.NI", "I.4.8; I.2.10; I.2.11; I.2.9; C.2.4"], "comment": "This paper has been accepted to IEEE EDGE 2025. The final version\n  will be published in IEEE Xplore later this year", "summary": "Accurate and reliable object detection is critical for ensuring the safety\nand efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board\nperception systems have limited accuracy due to occlusions and blind spots,\nwhile cloud-based solutions introduce significant latency, making them\nunsuitable for real-time processing demands required for autonomous driving in\ndynamic environments. To address these challenges, we introduce an innovative\nframework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that\nleverages edge computing and multi-CAV collaboration for real-time,\nmulti-perspective object detection. Our ECOD framework integrates two key\nalgorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and\nVariable Object Tally and Evaluation (VOTE). PACE aggregates detection data\nfrom multiple CAVs on an edge server to enhance perception in scenarios where\nindividual CAVs have limited visibility. VOTE utilizes a consensus-based voting\nmechanism to improve the accuracy of object classification by integrating data\nfrom multiple CAVs. Both algorithms are designed at the edge to operate in\nreal-time, ensuring low-latency and reliable decision-making for CAVs. We\ndevelop a hardware-based controlled testbed consisting of camera-equipped\nrobotic CAVs and an edge server to evaluate the efficacy of our framework. Our\nexperimental results demonstrate the significant benefits of ECOD in terms of\nimproved object classification accuracy, outperforming traditional\nsingle-perspective onboard approaches by up to 75%, while ensuring low-latency,\nedge-driven real-time processing. This research highlights the potential of\nedge computing to enhance collaborative perception for latency-sensitive\nautonomous systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u548c\u591a\u8f66\u534f\u4f5c\u7684\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\u6846\u67b6ECOD\uff0c\u901a\u8fc7PACE\u548cVOTE\u7b97\u6cd5\u63d0\u5347CAV\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5206\u7c7b\u51c6\u786e\u7387\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad875%\u3002", "motivation": "\u4f20\u7edf\u8f66\u8f7d\u611f\u77e5\u7cfb\u7edf\u56e0\u906e\u6321\u548c\u76f2\u533a\u7cbe\u5ea6\u6709\u9650\uff0c\u4e91\u7aef\u89e3\u51b3\u65b9\u6848\u5ef6\u8fdf\u9ad8\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002", "method": "ECOD\u6846\u67b6\u7ed3\u5408PACE\uff08\u611f\u77e5\u805a\u5408\u4e0e\u534f\u4f5c\u4f30\u8ba1\uff09\u548cVOTE\uff08\u53ef\u53d8\u7269\u4f53\u7edf\u8ba1\u4e0e\u8bc4\u4f30\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u8fb9\u7f18\u8ba1\u7b97\u548c\u591a\u8f66\u534f\u4f5c\u5b9e\u73b0\u5b9e\u65f6\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eECOD\u5728\u7269\u4f53\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u534775%\uff0c\u4e14\u5ef6\u8fdf\u4f4e\u3002", "conclusion": "\u8fb9\u7f18\u8ba1\u7b97\u53ef\u663e\u8457\u63d0\u5347\u534f\u4f5c\u611f\u77e5\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5ef6\u8fdf\u654f\u611f\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002"}}
{"id": "2506.06483", "pdf": "https://arxiv.org/pdf/2506.06483", "abs": "https://arxiv.org/abs/2506.06483", "authors": ["Yao Ni", "Song Wen", "Piotr Koniusz", "Anoop Cherian"], "title": "Noise Consistency Regularization for Improved Subject-Driven Image Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Fine-tuning Stable Diffusion enables subject-driven image synthesis by\nadapting the model to generate images containing specific subjects. However,\nexisting fine-tuning methods suffer from two key issues: underfitting, where\nthe model fails to reliably capture subject identity, and overfitting, where it\nmemorizes the subject image and reduces background diversity. To address these\nchallenges, we propose two auxiliary consistency losses for diffusion\nfine-tuning. First, a prior consistency regularization loss ensures that the\npredicted diffusion noise for prior (non-subject) images remains consistent\nwith that of the pretrained model, improving fidelity. Second, a subject\nconsistency regularization loss enhances the fine-tuned model's robustness to\nmultiplicative noise modulated latent code, helping to preserve subject\nidentity while improving diversity. Our experimental results demonstrate that\nincorporating these losses into fine-tuning not only preserves subject identity\nbut also enhances image diversity, outperforming DreamBooth in terms of CLIP\nscores, background variation, and overall visual quality.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3Stable Diffusion\u5fae\u8c03\u4e2d\u7684\u6b20\u62df\u5408\u548c\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u548c\u4e3b\u4f53\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u5728\u4e3b\u4f53\u9a71\u52a8\u56fe\u50cf\u5408\u6210\u4e2d\u5b58\u5728\u6b20\u62df\u5408\uff08\u65e0\u6cd5\u53ef\u9760\u6355\u6349\u4e3b\u4f53\u8eab\u4efd\uff09\u548c\u8fc7\u62df\u5408\uff08\u8bb0\u5fc6\u4e3b\u4f53\u56fe\u50cf\u5e76\u51cf\u5c11\u80cc\u666f\u591a\u6837\u6027\uff09\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8f85\u52a9\u4e00\u81f4\u6027\u635f\u5931\uff1a\u5148\u9a8c\u4e00\u81f4\u6027\u6b63\u5219\u5316\u635f\u5931\uff08\u4fdd\u6301\u975e\u4e3b\u4f53\u56fe\u50cf\u7684\u6269\u6563\u566a\u58f0\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u4e00\u81f4\uff09\u548c\u4e3b\u4f53\u4e00\u81f4\u6027\u6b63\u5219\u5316\u635f\u5931\uff08\u589e\u5f3a\u6a21\u578b\u5bf9\u566a\u58f0\u8c03\u5236\u6f5c\u7801\u7684\u9c81\u68d2\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728CLIP\u5206\u6570\u3001\u80cc\u666f\u591a\u6837\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8eDreamBooth\uff0c\u540c\u65f6\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4e00\u81f4\u6027\u635f\u5931\uff0c\u6709\u6548\u5e73\u8861\u4e86\u4e3b\u4f53\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf\u591a\u6837\u6027\uff0c\u63d0\u5347\u4e86\u5fae\u8c03\u6548\u679c\u3002"}}
{"id": "2506.06633", "pdf": "https://arxiv.org/pdf/2506.06633", "abs": "https://arxiv.org/abs/2506.06633", "authors": ["Chi-Sheng Chen"], "title": "Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent advancements in quantum machine learning have shown promise in\nenhancing classical neural network architectures, particularly in domains\ninvolving complex, high-dimensional data. Building upon prior work in temporal\nsequence modeling, this paper introduces Vision-QRWKV, a hybrid\nquantum-classical extension of the Receptance Weighted Key Value (RWKV)\narchitecture, applied for the first time to image classification tasks. By\nintegrating a variational quantum circuit (VQC) into the channel mixing\ncomponent of RWKV, our model aims to improve nonlinear feature transformation\nand enhance the expressive capacity of visual representations.\n  We evaluate both classical and quantum RWKV models on a diverse collection of\n14 medical and standard image classification benchmarks, including MedMNIST\ndatasets, MNIST, and FashionMNIST. Our results demonstrate that the\nquantum-enhanced model outperforms its classical counterpart on a majority of\ndatasets, particularly those with subtle or noisy class distinctions (e.g.,\nChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first\nsystematic application of quantum-enhanced RWKV in the visual domain, offering\ninsights into the architectural trade-offs and future potential of quantum\nmodels for lightweight and efficient vision tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784Vision-QRWKV\uff0c\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u901a\u8fc7\u5f15\u5165\u53d8\u5206\u91cf\u5b50\u7535\u8def\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u9ad8\u7ef4\u6570\u636e\u65f6\u63d0\u5347\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u73b0\u3002", "method": "\u5728RWKV\u67b6\u6784\u7684\u901a\u9053\u6df7\u5408\u7ec4\u4ef6\u4e2d\u96c6\u6210\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\uff0c\u4ee5\u589e\u5f3a\u975e\u7ebf\u6027\u7279\u5f81\u53d8\u6362\u548c\u89c6\u89c9\u8868\u793a\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u91cf\u5b50\u589e\u5f3a\u6a21\u578b\u5728\u591a\u6570\u6570\u636e\u96c6\uff08\u5c24\u5176\u662f\u5177\u6709\u7ec6\u5fae\u6216\u566a\u58f0\u7c7b\u522b\u533a\u5206\u7684\u6570\u636e\u96c6\uff09\u4e0a\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u5c06\u91cf\u5b50\u589e\u5f3aRWKV\u5e94\u7528\u4e8e\u89c6\u89c9\u9886\u57df\uff0c\u4e3a\u8f7b\u91cf\u9ad8\u6548\u89c6\u89c9\u4efb\u52a1\u7684\u91cf\u5b50\u6a21\u578b\u63d0\u4f9b\u4e86\u672a\u6765\u6f5c\u529b\u3002"}}
{"id": "2506.06637", "pdf": "https://arxiv.org/pdf/2506.06637", "abs": "https://arxiv.org/abs/2506.06637", "authors": ["Olimjon Toirov", "Wei Yu"], "title": "Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": "10 pages, 3 figures, 2025 2nd International Conference on Digital\n  Society and Artificial Intelligence (DSAI 2025), Conference dates: May 23-25,\n  2025", "summary": "Non-Intrusive Load Monitoring (NILM) identifies the operating status and\nenergy consumption of each electrical device in the circuit by analyzing the\nelectrical signals at the bus, which is of great significance for smart power\nmanagement. However, the complex and changeable load combinations and\napplication environments lead to the challenges of poor feature robustness and\ninsufficient model generalization of traditional NILM methods. To this end,\nthis paper proposes a new non-intrusive load monitoring method that integrates\n\"image load signature\" and continual learning. This method converts\nmulti-dimensional power signals such as current, voltage, and power factor into\nvisual image load feature signatures, and combines deep convolutional neural\nnetworks to realize the identification and classification of multiple devices;\nat the same time, self-supervised pre-training is introduced to improve feature\ngeneralization, and continual online learning strategies are used to overcome\nmodel forgetting to adapt to the emergence of new loads. This paper conducts a\nlarge number of experiments on high-sampling rate load datasets, and compares a\nvariety of existing methods and model variants. The results show that the\nproposed method has achieved significant improvements in recognition accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u56fe\u50cf\u8d1f\u8f7d\u7279\u5f81\u548c\u6301\u7eed\u5b66\u4e60\u7684\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfNILM\u65b9\u6cd5\u56e0\u8d1f\u8f7d\u7ec4\u5408\u590d\u6742\u591a\u53d8\u5bfc\u81f4\u7279\u5f81\u9c81\u68d2\u6027\u5dee\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u5c06\u591a\u7ef4\u7535\u529b\u4fe1\u53f7\u8f6c\u6362\u4e3a\u56fe\u50cf\u8d1f\u8f7d\u7279\u5f81\uff0c\u7ed3\u5408\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u9ad8\u91c7\u6837\u7387\u8d1f\u8f7d\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u7cbe\u5ea6\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u56fe\u50cf\u7279\u5f81\u548c\u6301\u7eed\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfNILM\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.06659", "pdf": "https://arxiv.org/pdf/2506.06659", "abs": "https://arxiv.org/abs/2506.06659", "authors": ["Wenhao Yao", "Zhenxin Li", "Shiyi Lan", "Zi Wang", "Xinglong Sun", "Jose M. Alvarez", "Zuxuan Wu"], "title": "DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "15 pages, 6 figures", "summary": "In complex driving environments, autonomous vehicles must navigate safely.\nRelying on a single predicted path, as in regression-based approaches, usually\ndoes not explicitly assess the safety of the predicted trajectory.\nSelection-based methods address this by generating and scoring multiple\ntrajectory candidates and predicting the safety score for each, but face\noptimization challenges in precisely selecting the best option from thousands\nof possibilities and distinguishing subtle but safety-critical differences,\nespecially in rare or underrepresented scenarios. We propose DriveSuprim to\novercome these challenges and advance the selection-based paradigm through a\ncoarse-to-fine paradigm for progressive candidate filtering, a rotation-based\naugmentation method to improve robustness in out-of-distribution scenarios, and\na self-distillation framework to stabilize training. DriveSuprim achieves\nstate-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS\nin NAVSIM v2 without extra data, demonstrating superior safetycritical\ncapabilities, including collision avoidance and compliance with rules, while\nmaintaining high trajectory quality in various driving scenarios.", "AI": {"tldr": "DriveSuprim\u901a\u8fc7\u6e10\u8fdb\u5019\u9009\u8fc7\u6ee4\u3001\u65cb\u8f6c\u589e\u5f3a\u548c\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u9009\u62e9\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u5728NAVSIM\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u9009\u62e9\u7684\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u5728\u4f18\u5316\u548c\u533a\u5206\u5b89\u5168\u5173\u952e\u5dee\u5f02\u4e0a\u7684\u6311\u6218\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u6e10\u8fdb\u5019\u9009\u8fc7\u6ee4\u3001\u65cb\u8f6c\u589e\u5f3a\u65b9\u6cd5\u548c\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u4f18\u5316\u8f68\u8ff9\u9009\u62e9\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728NAVSIM v1\u548cv2\u4e0a\u5206\u522b\u8fbe\u523093.5% PDMS\u548c87.1% EPDMS\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DriveSuprim\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u9009\u62e9\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u9a7e\u9a76\u573a\u666f\u3002"}}
{"id": "2506.06664", "pdf": "https://arxiv.org/pdf/2506.06664", "abs": "https://arxiv.org/abs/2506.06664", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Joshua Chen", "Nadine Chang", "Maying Shen", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "categories": ["cs.RO", "cs.CV"], "comment": "The 1st place solution of the End-to-end Driving Track at the CVPR\n  2025 Autonomous Grand Challenge", "summary": "End-to-end multi-modal planning is a promising paradigm in autonomous\ndriving, enabling decision-making with diverse trajectory candidates. A key\ncomponent is a robust trajectory scorer capable of selecting the optimal\ntrajectory from these candidates. While recent trajectory scorers focus on\nscoring either large sets of static trajectories or small sets of dynamically\ngenerated ones, both approaches face significant limitations in generalization.\nStatic vocabularies provide effective coarse discretization but struggle to\nmake fine-grained adaptation, while dynamic proposals offer detailed precision\nbut fail to capture broader trajectory distributions. To overcome these\nchallenges, we propose GTRS (Generalized Trajectory Scoring), a unified\nframework for end-to-end multi-modal planning that combines coarse and\nfine-grained trajectory evaluation. GTRS consists of three complementary\ninnovations: (1) a diffusion-based trajectory generator that produces diverse\nfine-grained proposals; (2) a vocabulary generalization technique that trains a\nscorer on super-dense trajectory sets with dropout regularization, enabling its\nrobust inference on smaller subsets; and (3) a sensor augmentation strategy\nthat enhances out-of-domain generalization while incorporating refinement\ntraining for critical trajectory discrimination. As the winning solution of the\nNavsim v2 Challenge, GTRS demonstrates superior performance even with\nsub-optimal sensor inputs, approaching privileged methods that rely on\nground-truth perception. Code will be available at\nhttps://github.com/NVlabs/GTRS.", "AI": {"tldr": "GTRS\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u591a\u6a21\u6001\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u8f68\u8ff9\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u548c\u52a8\u6001\u8f68\u8ff9\u8bc4\u5206\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u8bc4\u5206\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9759\u6001\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u7ec6\u7c92\u5ea6\u53d8\u5316\uff0c\u52a8\u6001\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5e7f\u6cdb\u8f68\u8ff9\u5206\u5e03\u3002", "method": "GTRS\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a\u6269\u6563\u5f0f\u8f68\u8ff9\u751f\u6210\u5668\u3001\u8bcd\u6c47\u6cdb\u5316\u6280\u672f\u548c\u4f20\u611f\u5668\u589e\u5f3a\u7b56\u7565\u3002", "result": "GTRS\u5728Navsim v2\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1\u4f9d\u8d56\u771f\u5b9e\u611f\u77e5\u7684\u7279\u6743\u65b9\u6cd5\u3002", "conclusion": "GTRS\u901a\u8fc7\u7ed3\u5408\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u8bc4\u5206\u7684\u6cdb\u5316\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.06677", "pdf": "https://arxiv.org/pdf/2506.06677", "abs": "https://arxiv.org/abs/2506.06677", "authors": ["Songhao Han", "Boxiang Qiu", "Yue Liao", "Siyuan Huang", "Chen Gao", "Shuicheng Yan", "Si Liu"], "title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation", "categories": ["cs.RO", "cs.CV"], "comment": "23 pages, 18 figures", "summary": "Recent advances in vision-language models (VLMs) have enabled\ninstruction-conditioned robotic systems with improved generalization. However,\nmost existing work focuses on reactive System 1 policies, underutilizing VLMs'\nstrengths in semantic reasoning and long-horizon planning. These System 2\ncapabilities-characterized by deliberative, goal-directed thinking-remain under\nexplored due to the limited temporal scale and structural complexity of current\nbenchmarks. To address this gap, we introduce RoboCerebra, a benchmark for\nevaluating high-level reasoning in long-horizon robotic manipulation.\nRoboCerebra includes: (1) a large-scale simulation dataset with extended task\nhorizons and diverse subtask sequences in household environments; (2) a\nhierarchical framework combining a high-level VLM planner with a low-level\nvision-language-action (VLA) controller; and (3) an evaluation protocol\ntargeting planning, reflection, and memory through structured System 1-System 2\ninteraction. The dataset is constructed via a top-down pipeline, where GPT\ngenerates task instructions and decomposes them into subtask sequences. Human\noperators execute the subtasks in simulation, yielding high-quality\ntrajectories with dynamic object variations. Compared to prior benchmarks,\nRoboCerebra features significantly longer action sequences and denser\nannotations. We further benchmark state-of-the-art VLMs as System 2 modules and\nanalyze their performance across key cognitive dimensions, advancing the\ndevelopment of more capable and generalizable robotic planners.", "AI": {"tldr": "RoboCerebra\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u4eba\u957f\u671f\u64cd\u4f5c\u4e2d\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u7ed3\u5408\u4e86\u9ad8\u5c42\u6b21VLM\u89c4\u5212\u5668\u548c\u4f4e\u5c42\u6b21VLA\u63a7\u5236\u5668\uff0c\u5e76\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u4eff\u771f\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u591a\u5173\u6ce8\u53cd\u5e94\u6027\u7b56\u7565\uff0c\u672a\u5145\u5206\u5229\u7528VLM\u7684\u8bed\u4e49\u63a8\u7406\u548c\u957f\u671f\u89c4\u5212\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6765\u63a2\u7d22\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u901a\u8fc7GPT\u751f\u6210\u4efb\u52a1\u6307\u4ee4\u5e76\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e8f\u5217\uff0c\u4eba\u7c7b\u64cd\u4f5c\u5458\u5728\u4eff\u771f\u4e2d\u6267\u884c\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u8f68\u8ff9\u6570\u636e\u96c6\u3002\u7ed3\u5408VLM\u89c4\u5212\u5668\u548cVLA\u63a7\u5236\u5668\u3002", "result": "RoboCerebra\u6570\u636e\u96c6\u5177\u6709\u66f4\u957f\u7684\u52a8\u4f5c\u5e8f\u5217\u548c\u66f4\u5bc6\u96c6\u7684\u6807\u6ce8\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "RoboCerebra\u63a8\u52a8\u4e86\u66f4\u901a\u7528\u548c\u9ad8\u6548\u7684\u673a\u5668\u4eba\u89c4\u5212\u5668\u7684\u53d1\u5c55\uff0c\u5e76\u5206\u6790\u4e86VLM\u5728\u5173\u952e\u8ba4\u77e5\u7ef4\u5ea6\u7684\u8868\u73b0\u3002"}}
{"id": "2506.06690", "pdf": "https://arxiv.org/pdf/2506.06690", "abs": "https://arxiv.org/abs/2506.06690", "authors": ["Hao Wang", "Chengkai Hou", "Xianglong Li", "Yankai Fu", "Chenxuan Li", "Ning Chen", "Gaole Dai", "Jiaming Liu", "Tiejun Huang", "Shanghang Zhang"], "title": "SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning to control high-speed objects in the real world remains a\nchallenging frontier in robotics. Table tennis serves as an ideal testbed for\nthis problem, demanding both rapid interception of fast-moving balls and\nprecise adjustment of their trajectories. This task presents two fundamental\nchallenges: it requires a high-precision vision system capable of accurately\npredicting ball trajectories, and it necessitates intelligent strategic\nplanning to ensure precise ball placement to target regions. The dynamic nature\nof table tennis, coupled with its real-time response requirements, makes it\nparticularly well-suited for advancing robotic control capabilities in\nfast-paced, precision-critical domains. In this paper, we present\nSpikePingpong, a novel system that integrates spike-based vision with imitation\nlearning for high-precision robotic table tennis. Our approach introduces two\nkey attempts that directly address the aforementioned challenges: SONIC, a\nspike camera-based module that achieves millimeter-level precision in\nball-racket contact prediction by compensating for real-world uncertainties\nsuch as air resistance and friction; and IMPACT, a strategic planning module\nthat enables accurate ball placement to targeted table regions. The system\nharnesses a 20 kHz spike camera for high-temporal resolution ball tracking,\ncombined with efficient neural network models for real-time trajectory\ncorrection and stroke planning. Experimental results demonstrate that\nSpikePingpong achieves a remarkable 91% success rate for 30 cm accuracy target\narea and 71% in the more challenging 20 cm accuracy task, surpassing previous\nstate-of-the-art approaches by 38% and 37% respectively. These significant\nperformance improvements enable the robust implementation of sophisticated\ntactical gameplay strategies, providing a new research perspective for robotic\ncontrol in high-speed dynamic tasks.", "AI": {"tldr": "SpikePingpong\u7cfb\u7edf\u7ed3\u5408\u5c16\u5cf0\u89c6\u89c9\u4e0e\u6a21\u4eff\u5b66\u4e60\uff0c\u901a\u8fc7SONIC\u548cIMPACT\u6a21\u5757\u89e3\u51b3\u4e52\u4e53\u7403\u673a\u5668\u4eba\u9ad8\u7cbe\u5ea6\u63a7\u5236\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u9ad8\u901f\u7269\u4f53\u63a7\u5236\u7684\u6311\u6218\uff0c\u4e52\u4e53\u7403\u4f5c\u4e3a\u7406\u60f3\u6d4b\u8bd5\u5e73\u53f0\uff0c\u9700\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u548c\u667a\u80fd\u7b56\u7565\u89c4\u5212\u3002", "method": "\u7ed3\u540820 kHz\u5c16\u5cf0\u76f8\u673a\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0cSONIC\u6a21\u5757\u8865\u507f\u73b0\u5b9e\u4e0d\u786e\u5b9a\u6027\uff0cIMPACT\u6a21\u5757\u5b9e\u73b0\u7cbe\u51c6\u843d\u70b9\u3002", "result": "\u572830 cm\u548c20 cm\u7cbe\u5ea6\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u5206\u522b\u8fbe91%\u548c71%\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd538%\u548c37%\u3002", "conclusion": "SpikePingpong\u4e3a\u9ad8\u901f\u52a8\u6001\u4efb\u52a1\u4e2d\u7684\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.06698", "pdf": "https://arxiv.org/pdf/2506.06698", "abs": "https://arxiv.org/abs/2506.06698", "authors": ["Yitao Liu", "Chenglei Si", "Karthik Narasimhan", "Shunyu Yao"], "title": "Contextual Experience Replay for Self-Improvement of Language Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to ACL 2025. 20 pages", "summary": "Large language model (LLM) agents have been applied to sequential\ndecision-making tasks such as web navigation, but without any\nenvironment-specific experiences, they often fail in these complex tasks.\nMoreover, current LLM agents are not designed to continually learn from past\nexperiences during inference time, which could be crucial for them to gain\nthese environment-specific experiences. To address this, we propose Contextual\nExperience Replay (CER), a training-free framework to enable efficient\nself-improvement for language agents in their context window. Specifically, CER\naccumulates and synthesizes past experiences into a dynamic memory buffer.\nThese experiences encompass environment dynamics and common decision-making\npatterns, allowing the agents to retrieve and augment themselves with relevant\nknowledge in new tasks, enhancing their adaptability in complex environments.\nWe evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On\nVisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,\nCER also gets a competitive average success rate of 36.7%, relatively improving\nthe success rate of the GPT-4o agent baseline by 51.0%. We also conduct a\ncomprehensive analysis on it to prove its efficiency, validity and understand\nit better.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aContextual Experience Replay (CER)\u7684\u8bad\u7ec3\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bb0\u5fc6\u7f13\u51b2\u533a\u79ef\u7d2f\u548c\u7efc\u5408\u8fc7\u53bb\u7ecf\u9a8c\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u7f51\u9875\u5bfc\u822a\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u73af\u5883\u7279\u5b9a\u7ecf\u9a8c\u4e14\u65e0\u6cd5\u5728\u63a8\u7406\u65f6\u6301\u7eed\u5b66\u4e60\u3002", "method": "CER\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8bb0\u5fc6\u7f13\u51b2\u533a\u79ef\u7d2f\u73af\u5883\u52a8\u6001\u548c\u51b3\u7b56\u6a21\u5f0f\u7ecf\u9a8c\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5728\u65b0\u4efb\u52a1\u4e2d\u68c0\u7d22\u5e76\u589e\u5f3a\u76f8\u5173\u77e5\u8bc6\u3002", "result": "\u5728VisualWebArena\u548cWebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCER\u5206\u522b\u8fbe\u523031.9%\u548c36.7%\u7684\u6210\u529f\u7387\uff0c\u76f8\u5bf9GPT-4o\u57fa\u7ebf\u63d0\u534751.0%\u3002", "conclusion": "CER\u901a\u8fc7\u7ecf\u9a8c\u56de\u653e\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.06727", "pdf": "https://arxiv.org/pdf/2506.06727", "abs": "https://arxiv.org/abs/2506.06727", "authors": ["Can Li", "Ting Zhang", "Mei Wang", "Hua Huang"], "title": "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving\ncapabilities across various domains. However, their ability to perform\nmathematical reasoning when answer options are represented as images--an\nessential aspect of multi-image comprehension--remains underexplored. To bridge\nthis gap, we introduce VisioMath, a benchmark designed to evaluate mathematical\nreasoning in multimodal contexts involving image-based answer choices.\nVisioMath comprises 8,070 images and 1,800 multiple-choice questions, where\neach answer option is an image, presenting unique challenges to existing LMMs.\nTo the best of our knowledge, VisioMath is the first dataset specifically\ntailored for mathematical reasoning in image-based-option scenarios, where\nfine-grained distinctions between answer choices are critical for accurate\nproblem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath\nand find that even the most advanced models struggle with this task. Notably,\nGPT-4o achieves only 45.9% accuracy, underscoring the limitations of current\nmodels in reasoning over visually similar answer choices. By addressing a\ncrucial gap in existing benchmarks, VisioMath establishes a rigorous testbed\nfor future research, driving advancements in multimodal reasoning.", "AI": {"tldr": "VisioMath\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf\u9009\u9879\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u586b\u8865\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf\u9009\u9879\u6570\u5b66\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002", "method": "\u5f15\u5165VisioMath\u6570\u636e\u96c6\uff0c\u5305\u542b8,070\u5f20\u56fe\u50cf\u548c1,800\u9053\u591a\u9009\u9898\uff0c\u6bcf\u9898\u7684\u9009\u9879\u4e3a\u56fe\u50cf\u3002", "result": "\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u6b64\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4ec5\u4e3a45.9%\u3002", "conclusion": "VisioMath\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63a8\u52a8\u672a\u6765\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2506.06761", "pdf": "https://arxiv.org/pdf/2506.06761", "abs": "https://arxiv.org/abs/2506.06761", "authors": ["Adri\u00e0 Molina Rodr\u00edguez", "Oriol Ramos Terrades", "Josep Llad\u00f3s"], "title": "The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Achieving robustness in recognition systems across diverse domains is crucial\nfor their practical utility. While ample data availability is usually assumed,\nlow-resource languages, such as ancient manuscripts and non-western languages,\ntend to be kept out of the equations of massive pretraining and foundational\ntechniques due to an under representation. In this work, we aim for building\nmodels which can generalize to new distributions of data, such as alphabets,\nfaster than centralized fine-tune strategies. For doing so, we take advantage\nof the recent advancements in model editing to enhance the incorporation of\nunseen scripts (low-resource learning). In contrast to state-of-the-art\nmeta-learning, we showcase the effectiveness of domain merging in sparse\ndistributions of data, with agnosticity of its relation to the overall\ndistribution or any other prototyping necessity. Even when using the same exact\ntraining data, our experiments showcase significant performance boosts in\n\\textbf{transfer learning} to new alphabets and \\textbf{out-of-domain\nevaluation} in challenging domain shifts, including historical ciphered texts\nand non-Latin scripts. This research contributes a novel approach into building\nmodels that can easily adopt under-represented alphabets and, therefore, enable\ndocument recognition to a wider set of contexts and cultures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u578b\u7f16\u8f91\u6280\u672f\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bc6\u522b\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u5143\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u548c\u8de8\u5b57\u6bcd\u8868\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u53e4\u4ee3\u624b\u7a3f\u548c\u975e\u897f\u65b9\u8bed\u8a00\uff09\u5728\u8bc6\u522b\u7cfb\u7edf\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528\u6a21\u578b\u7f16\u8f91\u6280\u672f\u589e\u5f3a\u5bf9\u672a\u89c1\u811a\u672c\u7684\u9002\u5e94\u6027\uff0c\u63d0\u51fa\u57df\u5408\u5e76\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u6574\u4f53\u6570\u636e\u5206\u5e03\u6216\u539f\u578b\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u76f8\u540c\u8bad\u7ec3\u6570\u636e\u4e0b\uff0c\u65b0\u65b9\u6cd5\u5728\u8de8\u5b57\u6bcd\u8868\u8fc1\u79fb\u5b66\u4e60\u548c\u57df\u5916\u8bc4\u4f30\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u5b57\u6bcd\u8868\u7684\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6269\u5c55\u4e86\u6587\u6863\u8bc6\u522b\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2506.06782", "pdf": "https://arxiv.org/pdf/2506.06782", "abs": "https://arxiv.org/abs/2506.06782", "authors": ["Qinting Jiang", "Chuyang Ye", "Dongyan Wei", "Bingli Wang", "Yuan Xue", "Jingyan Jiang", "Zhi Wang"], "title": "Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite progress, deep neural networks still suffer performance declines\nunder distribution shifts between training and test domains, leading to a\nsubstantial decrease in Quality of Experience (QoE) for applications. Existing\ntest-time adaptation (TTA) methods are challenged by dynamic, multiple test\ndistributions within batches. We observe that feature distributions across\ndifferent domains inherently cluster into distinct groups with varying means\nand variances. This divergence reveals a critical limitation of previous global\nnormalization strategies in TTA, which inevitably distort the original data\ncharacteristics. Based on this insight, we propose Feature-based Instance\nNeighbor Discovery (FIND), which comprises three key components: Layer-wise\nFeature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and\nSelective FABN (S-FABN). LFD stably captures features with similar\ndistributions at each layer by constructing graph structures. While FABN\noptimally combines source statistics with test-time distribution specific\nstatistics for robust feature representation. Finally, S-FABN determines which\nlayers require feature partitioning and which can remain unified, thereby\nenhancing inference efficiency. Extensive experiments demonstrate that FIND\nsignificantly outperforms existing methods, achieving a 30\\% accuracy\nimprovement in dynamic scenarios while maintaining computational efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFIND\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u5b9e\u4f8b\u90bb\u5c45\u53d1\u73b0\u89e3\u51b3\u52a8\u6001\u591a\u6d4b\u8bd5\u5206\u5e03\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u57df\u5206\u5e03\u504f\u79fb\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709TTA\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u591a\u6d4b\u8bd5\u5206\u5e03\u3002", "method": "FIND\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1aLFD\uff08\u5c42\u95f4\u7279\u5f81\u89e3\u8026\uff09\u3001FABN\uff08\u7279\u5f81\u611f\u77e5\u6279\u91cf\u5f52\u4e00\u5316\uff09\u548cS-FABN\uff08\u9009\u62e9\u6027FABN\uff09\uff0c\u5206\u522b\u5904\u7406\u7279\u5f81\u805a\u7c7b\u3001\u7edf\u8ba1\u4f18\u5316\u548c\u6548\u7387\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFIND\u5728\u52a8\u6001\u573a\u666f\u4e2d\u51c6\u786e\u7387\u63d0\u534730%\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "FIND\u901a\u8fc7\u7279\u5f81\u805a\u7c7b\u548c\u52a8\u6001\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.06862", "pdf": "https://arxiv.org/pdf/2506.06862", "abs": "https://arxiv.org/abs/2506.06862", "authors": ["Chenguang Huang", "Oier Mees", "Andy Zeng", "Wolfram Burgard"], "title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "accepted to International Journal of Robotics Research (IJRR). 24\n  pages, 18 figures. The paper contains texts from VLMaps(arXiv:2210.05714) and\n  AVLMaps(arXiv:2303.07522). The project page is https://mslmaps.github.io/", "summary": "Grounding language to a navigating agent's observations can leverage\npretrained multimodal foundation models to match perceptions to object or event\ndescriptions. However, previous approaches remain disconnected from environment\nmapping, lack the spatial precision of geometric maps, or neglect additional\nmodality information beyond vision. To address this, we propose multimodal\nspatial language maps as a spatial map representation that fuses pretrained\nmultimodal features with a 3D reconstruction of the environment. We build these\nmaps autonomously using standard exploration. We present two instances of our\nmaps, which are visual-language maps (VLMaps) and their extension to\naudio-visual-language maps (AVLMaps) obtained by adding audio information. When\ncombined with large language models (LLMs), VLMaps can (i) translate natural\nlanguage commands into open-vocabulary spatial goals (e.g., \"in between the\nsofa and TV\") directly localized in the map, and (ii) be shared across\ndifferent robot embodiments to generate tailored obstacle maps on demand.\nBuilding upon the capabilities above, AVLMaps extend VLMaps by introducing a\nunified 3D spatial representation integrating audio, visual, and language cues\nthrough the fusion of features from pretrained multimodal foundation models.\nThis enables robots to ground multimodal goal queries (e.g., text, images, or\naudio snippets) to spatial locations for navigation. Additionally, the\nincorporation of diverse sensory inputs significantly enhances goal\ndisambiguation in ambiguous environments. Experiments in simulation and\nreal-world settings demonstrate that our multimodal spatial language maps\nenable zero-shot spatial and multimodal goal navigation and improve recall by\n50% in ambiguous scenarios. These capabilities extend to mobile robots and\ntabletop manipulators, supporting navigation and interaction guided by visual,\naudio, and spatial cues.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u7a7a\u95f4\u8bed\u8a00\u5730\u56fe\uff08VLMaps\u548cAVLMaps\uff09\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u7279\u5f81\u4e0e3D\u73af\u5883\u91cd\u5efa\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u5230\u7a7a\u95f4\u76ee\u6807\u7684\u7ffb\u8bd1\uff0c\u5e76\u652f\u6301\u591a\u6a21\u6001\u76ee\u6807\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u73af\u5883\u5730\u56fe\u7684\u51e0\u4f55\u7cbe\u5ea6\u6216\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5bfc\u822a\u4ee3\u7406\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u6784\u5efa\u89c6\u89c9-\u8bed\u8a00\u5730\u56fe\uff08VLMaps\uff09\u53ca\u5176\u6269\u5c55\u97f3\u9891-\u89c6\u89c9-\u8bed\u8a00\u5730\u56fe\uff08AVLMaps\uff09\uff0c\u878d\u5408\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u7279\u5f81\u4e0e3D\u73af\u5883\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u652f\u6301\u96f6\u6837\u672c\u7a7a\u95f4\u548c\u591a\u6a21\u6001\u76ee\u6807\u5bfc\u822a\uff0c\u5728\u6a21\u7cca\u573a\u666f\u4e2d\u53ec\u56de\u7387\u63d0\u534750%\u3002", "conclusion": "\u591a\u6a21\u6001\u7a7a\u95f4\u8bed\u8a00\u5730\u56fe\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u4ee3\u7406\u7684\u611f\u77e5\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u3002"}}
{"id": "2506.06884", "pdf": "https://arxiv.org/pdf/2506.06884", "abs": "https://arxiv.org/abs/2506.06884", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "title": "FREE: Fast and Robust Vision Language Models with Early Exits", "categories": ["cs.LG", "cs.CV"], "comment": "To appear at the Association of Computational Linguistics (ACL) 2025\n  Conference", "summary": "In recent years, Vision-Language Models (VLMs) have shown remarkable\nperformance improvements in Vision-Language tasks. However, their large size\nposes challenges for real-world applications where inference latency is a\nconcern. To tackle this issue, we propose employing Early Exit (EE) strategies\nin VLMs. However, training exit classifiers in VLMs is challenging,\nparticularly with limited labeled training data. To address this, we introduce\nFREE, an adversarial training approach within a GAN-based framework. Here, each\nexit consists of a transformer layer and a classifier. The transformer layer is\nadversarially trained to produce feature representations similar to the final\nlayer, while a feature classifier serves as the discriminator. Our method\nfocuses on performing input-adaptive inference that increases inference speed\nwith minimal drop in performance. Experimental results demonstrate the\neffectiveness of our approach in enhancing accuracy and model robustness by\nmitigating overthinking and the phenomenon of mid-crisis that we highlight. We\nexperimentally validate that our method speeds up the inference process by more\nthan 1.51x while retaining comparable performance. The source code is available\nat https://github.com/Div290/FREE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFREE\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u548cGAN\u6846\u67b6\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u65e9\u671f\u9000\u51fa\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u5927\u89c4\u6a21\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff08FREE\uff09\uff0c\u6bcf\u4e2a\u9000\u51fa\u70b9\u5305\u542b\u4e00\u4e2aTransformer\u5c42\u548c\u5206\u7c7b\u5668\uff0c\u901a\u8fc7GAN\u6846\u67b6\u8bad\u7ec3\u4ee5\u751f\u6210\u7c7b\u4f3c\u6700\u7ec8\u5c42\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u8d85\u8fc71.51\u500d\uff0c\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "FREE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86VLMs\u7684\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.06890", "pdf": "https://arxiv.org/pdf/2506.06890", "abs": "https://arxiv.org/abs/2506.06890", "authors": ["Sumit Sharma", "Gopi Raju Matta", "Kaushik Mitra"], "title": "SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "Accepted for publication at ICIP 2025", "summary": "Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging\ntechnology, capable of detecting individual photons with remarkable timing\nprecision. Building on this sensitivity, Single Photon Cameras (SPCs) enable\nimage capture at exceptionally high speeds under both low and high\nillumination. Enabling 3D reconstruction and radiance field recovery from such\nSPC data holds significant promise. However, the binary nature of SPC images\nleads to severe information loss, particularly in texture and color, making\ntraditional 3D synthesis techniques ineffective. To address this challenge, we\npropose a modular two-stage framework that converts binary SPC images into\nhigh-quality colorized novel views. The first stage performs image-to-image\n(I2I) translation using generative models such as Pix2PixHD, converting binary\nSPC inputs into plausible RGB representations. The second stage employs 3D\nscene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian\nSplatting (3DGS) to generate novel views. We validate our two-stage pipeline\n(Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative\nexperiments, demonstrating significant improvements in perceptual quality and\ngeometric consistency over the alternative baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5c06\u4e8c\u8fdb\u5236SPC\u56fe\u50cf\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u5f69\u8272\u65b0\u89c6\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf3D\u5408\u6210\u6280\u672f\u56e0\u4fe1\u606f\u4e22\u5931\u800c\u5931\u6548\u7684\u95ee\u9898\u3002", "motivation": "SPAD\u548cSPC\u6280\u672f\u80fd\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u5149\u5b50\uff0c\u4f46\u4e8c\u8fdb\u5236SPC\u56fe\u50cf\u5bfc\u81f4\u7eb9\u7406\u548c\u989c\u8272\u4fe1\u606f\u4e22\u5931\uff0c\u4f20\u7edf3D\u5408\u6210\u6280\u672f\u96be\u4ee5\u5904\u7406\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u7528Pix2PixHD\u8fdb\u884c\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528NeRF\u62163DGS\u8fdb\u884c3D\u573a\u666f\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u611f\u77e5\u8d28\u91cf\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u4e24\u9636\u6bb5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86SPC\u56fe\u50cf\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u4e3a3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.06905", "pdf": "https://arxiv.org/pdf/2506.06905", "abs": "https://arxiv.org/abs/2506.06905", "authors": ["Akash Gupta", "Amos Storkey", "Mirella Lapata"], "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\nperform new tasks with minimal supervision. However, ICL performance,\nespecially in smaller LMMs, is inconsistent and does not always improve\nmonotonically with increasing examples. We hypothesize that this occurs due to\nthe LMM being overwhelmed by additional information present in the image\nembeddings, which is not required for the downstream task. To address this, we\npropose a meta-learning approach that provides an alternative for inducing\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\ndistilled from task-relevant image features and can be adapted at test time\nusing a few examples. To facilitate this distillation, we introduce an\nattention-mapper module that can be easily integrated with the popular LLaVA\nv1.5 architecture and is jointly learned with soft prompts, enabling task\nadaptation in LMMs under low-data regimes with just a few gradient steps.\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\nICL and related prompt-tuning approaches, even under image perturbations,\nimproving task induction and reasoning across visual question answering tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u76f8\u5173\u7684\u8f6f\u63d0\u793a\u548c\u6ce8\u610f\u529b\u6620\u5c04\u6a21\u5757\uff0c\u63d0\u5347\u5c0f\u578bLMM\u5728\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u5c24\u5176\u662f\u5c0f\u578bLMM\uff0c\u989d\u5916\u56fe\u50cf\u4fe1\u606f\u53ef\u80fd\u5e72\u6270\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u91c7\u7528\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u4efb\u52a1\u76f8\u5173\u56fe\u50cf\u7279\u5f81\u4e2d\u63d0\u53d6\u8f6f\u63d0\u793a\uff0c\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u6620\u5c04\u6a21\u5757\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u9002\u5e94\u3002", "result": "\u5728VL-ICL Bench\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8eICL\u548c\u5176\u4ed6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u56fe\u50cf\u6270\u52a8\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LMM\u5728\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.06933", "pdf": "https://arxiv.org/pdf/2506.06933", "abs": "https://arxiv.org/abs/2506.06933", "authors": ["Mahdi Salmani", "Alireza Abdollahpoorrostam", "Seyed-Mohsen Moosavi-Dezfooli"], "title": "Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": null, "summary": "Traditional decision-based black-box adversarial attacks on image classifiers\naim to generate adversarial examples by slightly modifying input images while\nkeeping the number of queries low, where each query involves sending an input\nto the model and observing its output. Most existing methods assume that all\nqueries have equal cost. However, in practice, queries may incur asymmetric\ncosts; for example, in content moderation systems, certain output classes may\ntrigger additional review, enforcement, or penalties, making them more costly\nthan others. While prior work has considered such asymmetric cost settings,\neffective algorithms for this scenario remain underdeveloped. In this paper, we\npropose a general framework for decision-based attacks under asymmetric query\ncosts, which we refer to as asymmetric black-box attacks. We modify two core\ncomponents of existing attacks: the search strategy and the gradient estimation\nprocess. Specifically, we propose Asymmetric Search (AS), a more conservative\nvariant of binary search that reduces reliance on high-cost queries, and\nAsymmetric Gradient Estimation (AGREST), which shifts the sampling distribution\nto favor low-cost queries. We design efficient algorithms that minimize total\nattack cost by balancing different query types, in contrast to earlier methods\nsuch as stealthy attacks that focus only on limiting expensive (high-cost)\nqueries. Our method can be integrated into a range of existing black-box\nattacks with minimal changes. We perform both theoretical analysis and\nempirical evaluation on standard image classification benchmarks. Across\nvarious cost regimes, our method consistently achieves lower total query cost\nand smaller perturbations than existing approaches, with improvements of up to\n40% in some settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u975e\u5bf9\u79f0\u9ed1\u76d2\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u641c\u7d22\u7b56\u7565\u548c\u68af\u5ea6\u4f30\u8ba1\u8fc7\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u7684\u603b\u67e5\u8be2\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u67e5\u8be2\u6210\u672c\u76f8\u540c\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u67d0\u4e9b\u67e5\u8be2\u53ef\u80fd\u6210\u672c\u66f4\u9ad8\uff08\u5982\u89e6\u53d1\u989d\u5916\u5ba1\u67e5\uff09\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u975e\u5bf9\u79f0\u67e5\u8be2\u6210\u672c\u4e0b\u7684\u653b\u51fb\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u975e\u5bf9\u79f0\u641c\u7d22\uff08AS\uff09\u548c\u975e\u5bf9\u79f0\u68af\u5ea6\u4f30\u8ba1\uff08AGREST\uff09\uff0c\u901a\u8fc7\u51cf\u5c11\u9ad8\u6210\u672c\u67e5\u8be2\u7684\u4f9d\u8d56\u548c\u8c03\u6574\u91c7\u6837\u5206\u5e03\uff0c\u4f18\u5316\u653b\u51fb\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6210\u672c\u8bbe\u7f6e\u4e0b\uff0c\u603b\u67e5\u8be2\u6210\u672c\u548c\u6270\u52a8\u5e45\u5ea6\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u534740%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u975e\u5bf9\u79f0\u9ed1\u76d2\u653b\u51fb\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u67e5\u8be2\u6210\u672c\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u4e3a\u9ed1\u76d2\u653b\u51fb\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06938", "pdf": "https://arxiv.org/pdf/2506.06938", "abs": "https://arxiv.org/abs/2506.06938", "authors": ["Bastian J\u00e4ckl", "Vojt\u011bch Kloda", "Daniel A. Keim", "Jakub Loko\u010d"], "title": "Experimental Evaluation of Static Image Sub-Region-Based Search Models Using CLIP", "categories": ["cs.MM", "cs.CV", "68U10", "H.3.3; I.4.10; H.2.8"], "comment": "14 pages, 4 figures, 2 tables", "summary": "Advances in multimodal text-image models have enabled effective text-based\nquerying in extensive image collections. While these models show convincing\nperformance for everyday life scenes, querying in highly homogeneous,\nspecialized domains remains challenging. The primary problem is that users can\noften provide only vague textual descriptions as they lack expert knowledge to\ndiscriminate between homogenous entities. This work investigates whether adding\nlocation-based prompts to complement these vague text queries can enhance\nretrieval performance. Specifically, we collected a dataset of 741 human\nannotations, each containing short and long textual descriptions and bounding\nboxes indicating regions of interest in challenging underwater scenes. Using\nthese annotations, we evaluate the performance of CLIP when queried on various\nstatic sub-regions of images compared to the full image. Our results show that\nboth a simple 3-by-3 partitioning and a 5-grid overlap significantly improve\nretrieval effectiveness and remain robust to perturbations of the annotation\nbox.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u9ad8\u5ea6\u540c\u8d28\u5316\u7684\u4e13\u4e1a\u9886\u57df\u4e2d\uff0c\u901a\u8fc7\u6dfb\u52a0\u57fa\u4e8e\u4f4d\u7f6e\u7684\u63d0\u793a\u6765\u589e\u5f3a\u6a21\u7cca\u6587\u672c\u67e5\u8be2\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u6a21\u578b\u4e2d\uff0c\u4e13\u4e1a\u9886\u57df\u7684\u67e5\u8be2\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7528\u6237\u901a\u5e38\u53ea\u80fd\u63d0\u4f9b\u6a21\u7cca\u7684\u6587\u672c\u63cf\u8ff0\u3002", "method": "\u6536\u96c6\u4e86741\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u77ed/\u957f\u6587\u672c\u63cf\u8ff0\u548c\u611f\u5174\u8da3\u533a\u57df\u7684\u8fb9\u754c\u6846\uff0c\u8bc4\u4f30CLIP\u6a21\u578b\u5728\u4e0d\u540c\u9759\u6001\u5b50\u533a\u57df\u4e0a\u7684\u68c0\u7d22\u6027\u80fd\u3002", "result": "\u7b80\u5355\u76843x3\u5206\u533a\u548c5\u7f51\u683c\u91cd\u53e0\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u6548\u679c\uff0c\u5e76\u5bf9\u6807\u6ce8\u6846\u7684\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u57fa\u4e8e\u4f4d\u7f6e\u7684\u63d0\u793a\u53ef\u4ee5\u6709\u6548\u8865\u5145\u6a21\u7cca\u6587\u672c\u67e5\u8be2\uff0c\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2506.06965", "pdf": "https://arxiv.org/pdf/2506.06965", "abs": "https://arxiv.org/abs/2506.06965", "authors": ["Cuong Manh Hoang"], "title": "Long-Tailed Learning for Generalized Category Discovery", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) utilizes labeled samples of known\nclasses to discover novel classes in unlabeled samples. Existing methods show\neffective performance on artificial datasets with balanced distributions.\nHowever, real-world datasets are always imbalanced, significantly affecting the\neffectiveness of these methods. To solve this problem, we propose a novel\nframework that performs generalized category discovery in long-tailed\ndistributions. We first present a self-guided labeling technique that uses a\nlearnable distribution to generate pseudo-labels, resulting in less biased\nclassifiers. We then introduce a representation balancing process to derive\ndiscriminative representations. By mining sample neighborhoods, this process\nencourages the model to focus more on tail classes. We conduct experiments on\npublic datasets to demonstrate the effectiveness of the proposed framework. The\nresults show that our model exceeds previous state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u957f\u5c3e\u5206\u5e03\u4e2d\u8fdb\u884c\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u5f15\u5bfc\u6807\u8bb0\u548c\u8868\u793a\u5e73\u8861\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u5f15\u5bfc\u6807\u8bb0\u6280\u672f\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u8868\u793a\u5e73\u8861\u8fc7\u7a0b\u6316\u6398\u6837\u672c\u90bb\u57df\u4ee5\u5173\u6ce8\u5c3e\u90e8\u7c7b\u522b\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u95ee\u9898\u3002"}}
{"id": "2506.06999", "pdf": "https://arxiv.org/pdf/2506.06999", "abs": "https://arxiv.org/abs/2506.06999", "authors": ["Arun Sharma", "Mingzhou Yang", "Majid Farhadloo", "Subhankar Ghosh", "Bharat Jayaprakash", "Shashi Shekhar"], "title": "Towards Physics-informed Diffusion for Anomaly Detection in Trajectories", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "Given trajectory data, a domain-specific study area, and a user-defined\nthreshold, we aim to find anomalous trajectories indicative of possible GPS\nspoofing (e.g., fake trajectory). The problem is societally important to curb\nillegal activities in international waters, such as unauthorized fishing and\nillicit oil transfers. The problem is challenging due to advances in AI\ngenerated in deep fakes generation (e.g., additive noise, fake trajectories)\nand lack of adequate amount of labeled samples for ground-truth verification.\nRecent literature shows promising results for anomalous trajectory detection\nusing generative models despite data sparsity. However, they do not consider\nfine-scale spatiotemporal dependencies and prior physical knowledge, resulting\nin higher false-positive rates. To address these limitations, we propose a\nphysics-informed diffusion model that integrates kinematic constraints to\nidentify trajectories that do not adhere to physical laws. Experimental results\non real-world datasets in the maritime and urban domains show that the proposed\nframework results in higher prediction accuracy and lower estimation error rate\nfor anomaly detection and trajectory generation methods, respectively. Our\nimplementation is available at\nhttps://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u5f02\u5e38\u8f68\u8ff9\uff0c\u5c24\u5176\u662f\u5728GPS\u6b3a\u9a97\u573a\u666f\u4e2d\uff0c\u7ed3\u5408\u8fd0\u52a8\u5b66\u7ea6\u675f\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u56fd\u9645\u6c34\u57df\u975e\u6cd5\u6d3b\u52a8\uff08\u5982\u975e\u6cd5\u6355\u9c7c\u548c\u77f3\u6cb9\u8d70\u79c1\uff09\u4e2d\u7684GPS\u6b3a\u9a97\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9AI\u751f\u6210\u865a\u5047\u8f68\u8ff9\u548c\u6570\u636e\u7a00\u758f\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u6269\u6563\u6a21\u578b\uff0c\u6574\u5408\u8fd0\u52a8\u5b66\u7ea6\u675f\u4ee5\u8bc6\u522b\u4e0d\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\uff08\u6d77\u4e8b\u548c\u57ce\u5e02\u9886\u57df\uff09\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u548c\u8f68\u8ff9\u751f\u6210\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684\u8bef\u5dee\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u865a\u5047\u8f68\u8ff9\u68c0\u6d4b\u95ee\u9898\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u758f\u548c\u590d\u6742\u4f9d\u8d56\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.07023", "pdf": "https://arxiv.org/pdf/2506.07023", "abs": "https://arxiv.org/abs/2506.07023", "authors": ["Suman Mahapatra", "Pradipta Maji"], "title": "Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 8 figures", "summary": "Segmentation of nuclei regions from histological images enables morphometric\nanalysis of nuclei structures, which in turn helps in the detection and\ndiagnosis of diseases under consideration. To develop a nuclei segmentation\nalgorithm, applicable to different types of target domain representations,\nimage-to-image translation networks can be considered as they are invariant to\ntarget domain image representations. One of the important issues with\nimage-to-image translation models is that they fail miserably when the\ninformation content between two image domains are asymmetric in nature. In this\nregard, the paper introduces a new deep generative model for segmenting nuclei\nstructures from histological images. The proposed model considers an embedding\nspace for handling information-disparity between information-rich histological\nimage space and information-poor segmentation map domain. Integrating\njudiciously the concepts of optimal transport and measure theory, the model\ndevelops an invertible generator, which provides an efficient optimization\nframework with lower network complexity. The concept of invertible generator\nautomatically eliminates the need of any explicit cycle-consistency loss. The\nproposed model also introduces a spatially-constrained squeeze operation within\nthe framework of invertible generator to maintain spatial continuity within the\nimage patches. The model provides a better trade-off between network complexity\nand model performance compared to other existing models having complex network\narchitectures. The performance of the proposed deep generative model, along\nwith a comparison with state-of-the-art nuclei segmentation methods, is\ndemonstrated on publicly available histological image data sets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7ec4\u7ec7\u5b66\u56fe\u50cf\u4e2d\u5206\u5272\u7ec6\u80de\u6838\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u53ef\u9006\u751f\u6210\u5668\u548c\u7a7a\u95f4\u7ea6\u675f\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u7ec4\u7ec7\u5b66\u56fe\u50cf\u4e2d\u7ec6\u80de\u6838\u533a\u57df\u7684\u5206\u5272\u6709\u52a9\u4e8e\u75be\u75c5\u68c0\u6d4b\u548c\u8bca\u65ad\uff0c\u4f46\u73b0\u6709\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u6a21\u578b\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u5d4c\u5165\u7a7a\u95f4\u5904\u7406\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u548c\u6d4b\u5ea6\u7406\u8bba\u8bbe\u8ba1\u53ef\u9006\u751f\u6210\u5668\uff0c\u907f\u514d\u663e\u5f0f\u5faa\u73af\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5e76\u52a0\u5165\u7a7a\u95f4\u7ea6\u675f\u6324\u538b\u64cd\u4f5c\u3002", "result": "\u6a21\u578b\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7f51\u7edc\u590d\u6742\u5ea6\u548c\u6027\u80fd\u7684\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u4e3a\u7ec6\u80de\u6838\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07028", "pdf": "https://arxiv.org/pdf/2506.07028", "abs": "https://arxiv.org/abs/2506.07028", "authors": ["Suman Mahapatra", "Pradipta Maji"], "title": "SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 9 figures", "summary": "Segmentation of nuclei regions from histological images is an important task\nfor automated computer-aided analysis of histological images, particularly in\nthe presence of impermissible color variation in the color appearance of\nstained tissue images. While color normalization enables better nuclei\nsegmentation, accurate segmentation of nuclei structures makes color\nnormalization rather trivial. In this respect, the paper proposes a novel deep\ngenerative model for simultaneously segmenting nuclei structures and\nnormalizing color appearance of stained histological images.This model\njudiciously integrates the merits of truncated normal distribution and spatial\nattention. The model assumes that the latent color appearance information,\ncorresponding to a particular histological image, is independent of respective\nnuclei segmentation map as well as embedding map information. The disentangled\nrepresentation makes the model generalizable and adaptable as the modification\nor loss in color appearance information cannot be able to affect the nuclei\nsegmentation map as well as embedding information. Also, for dealing with the\nstain overlap of associated histochemical reagents, the prior for latent color\nappearance code is assumed to be a mixture of truncated normal distributions.\nThe proposed model incorporates the concept of spatial attention for\nsegmentation of nuclei regions from histological images. The performance of the\nproposed approach, along with a comparative analysis with related\nstate-of-the-art algorithms, has been demonstrated on publicly available\nstandard histological image data sets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u540c\u65f6\u5b9e\u73b0\u7ec4\u7ec7\u5b66\u56fe\u50cf\u4e2d\u7ec6\u80de\u6838\u7ed3\u6784\u7684\u7cbe\u786e\u5206\u5272\u548c\u989c\u8272\u5916\u89c2\u7684\u5f52\u4e00\u5316\u3002", "motivation": "\u89e3\u51b3\u7ec4\u7ec7\u5b66\u56fe\u50cf\u4e2d\u56e0\u989c\u8272\u53d8\u5316\u5bfc\u81f4\u7684\u7ec6\u80de\u6838\u5206\u5272\u56f0\u96be\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u989c\u8272\u5f52\u4e00\u5316\u7684\u6548\u679c\u3002", "method": "\u7ed3\u5408\u622a\u65ad\u6b63\u6001\u5206\u5e03\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u5047\u8bbe\u989c\u8272\u5916\u89c2\u4fe1\u606f\u4e0e\u7ec6\u80de\u6838\u5206\u5272\u56fe\u53ca\u5d4c\u5165\u4fe1\u606f\u72ec\u7acb\u3002", "result": "\u6a21\u578b\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u5177\u6709\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\uff0c\u989c\u8272\u5916\u89c2\u4fe1\u606f\u7684\u4fee\u6539\u6216\u4e22\u5931\u4e0d\u5f71\u54cd\u7ec6\u80de\u6838\u5206\u5272\u7ed3\u679c\u3002"}}
{"id": "2506.07032", "pdf": "https://arxiv.org/pdf/2506.07032", "abs": "https://arxiv.org/abs/2506.07032", "authors": ["Bhuiyan Sanjid Shafique", "Ashmal Vayani", "Muhammad Maaz", "Hanoona Abdul Rasheed", "Dinura Dissanayake", "Mohammed Irfan Kurpath", "Yahya Hmaiti", "Go Inoue", "Jean Lahoud", "Md. Safirur Rashid", "Shadid Intisar Quasem", "Maheen Fatima", "Franco Vidal", "Mykola Maslych", "Ketan Pravin More", "Sanoojan Baliah", "Hasindri Watawana", "Yuhao Li", "Fabian Farestam", "Leon Schaller", "Roman Tymtsiv", "Simon Weber", "Hisham Cholakkal", "Ivan Laptev", "Shin'ichi Satoh", "Michael Felsberg", "Mubarak Shah", "Salman Khan", "Fahad Shahbaz Khan"], "title": "A Culturally-diverse Multilingual Multimodal Video Benchmark & Model", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large multimodal models (LMMs) have recently gained attention due to their\neffectiveness to understand and generate descriptions of visual content. Most\nexisting LMMs are in English language. While few recent works explore\nmultilingual image LMMs, to the best of our knowledge, moving beyond the\nEnglish language for cultural and linguistic inclusivity is yet to be\ninvestigated in the context of video LMMs. In pursuit of more inclusive video\nLMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to\nevaluate Video LMMs across 14 languages, including both low- and high-resource\nlanguages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,\nBengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is\ndesigned to rigorously test video LMMs across 15 categories including eight\nculturally diverse categories, ranging from lifestyles and festivals to foods\nand rituals and from local landmarks to prominent cultural personalities.\nViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice\nquestions spanning various video durations (short, medium, and long) with 8k\nsamples that are manually verified by native language speakers. In addition, we\nalso introduce a machine translated multilingual video training set comprising\n1.2 million samples and develop a simple multilingual video LMM, named ViMUL,\nthat is shown to provide a better tradeoff between high-and low-resource\nlanguages for video understanding. We hope our ViMUL-Bench and multilingual\nvideo LMM along with a large-scale multilingual video training set will help\nease future research in developing cultural and linguistic inclusive\nmultilingual video LMMs. Our proposed benchmark, video LMM and training data\nwill be publicly released at https://mbzuai-oryx.github.io/ViMUL/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u591a\u8bed\u8a00\u89c6\u9891LMM\u57fa\u51c6ViMUL-Bench\uff0c\u6db5\u76d614\u79cd\u8bed\u8a00\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u8bed\u8a00\u89c6\u9891LMM\u6a21\u578bViMUL\uff0c\u65e8\u5728\u4fc3\u8fdb\u6587\u5316\u53ca\u8bed\u8a00\u5305\u5bb9\u6027\u7814\u7a76\u3002", "motivation": "\u73b0\u6709LMM\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8bed\u8a00\u53ca\u6587\u5316\u5305\u5bb9\u6027\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u89c6\u9891LMM\u9886\u57df\u3002", "method": "\u5f15\u5165ViMUL-Bench\u57fa\u51c6\uff0c\u5305\u542b14\u79cd\u8bed\u8a00\u76848k\u6837\u672c\uff0c\u5e76\u5f00\u53d1ViMUL\u6a21\u578b\uff0c\u5229\u7528120\u4e07\u6837\u672c\u7684\u591a\u8bed\u8a00\u89c6\u9891\u8bad\u7ec3\u96c6\u3002", "result": "ViMUL\u6a21\u578b\u5728\u9ad8\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\uff0cViMUL-Bench\u4e3a\u591a\u8bed\u8a00\u89c6\u9891LMM\u7814\u7a76\u63d0\u4f9b\u5de5\u5177\u3002", "conclusion": "ViMUL-Bench\u3001ViMUL\u6a21\u578b\u53ca\u8bad\u7ec3\u6570\u636e\u5c06\u63a8\u52a8\u591a\u8bed\u8a00\u89c6\u9891LMM\u7684\u53d1\u5c55\uff0c\u4fc3\u8fdb\u6587\u5316\u53ca\u8bed\u8a00\u5305\u5bb9\u6027\u3002"}}
{"id": "2506.07044", "pdf": "https://arxiv.org/pdf/2506.07044", "abs": "https://arxiv.org/abs/2506.07044", "authors": ["LASA Team", "Weiwen Xu", "Hou Pong Chan", "Long Li", "Mahani Aljunied", "Ruifeng Yuan", "Jianyu Wang", "Chenghao Xiao", "Guizhen Chen", "Chaoqun Liu", "Zhaodonghui Li", "Yu Sun", "Junao Shen", "Chaojun Wang", "Jie Tan", "Deli Zhao", "Tingyang Xu", "Hao Zhang", "Yu Rong"], "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Technical Report, 53 pages, 25 tables, and 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u9886\u57df\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578bLingshu\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u6536\u96c6\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u533b\u5b66MLLMs\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u533b\u5b66MLLMs\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u6570\u636e\u4e0e\u4efb\u52a1\u7684\u4e0d\u5339\u914d\u3001\u5e7b\u89c9\u95ee\u9898\u53ca\u7f3a\u4e4f\u9488\u5bf9\u590d\u6742\u533b\u5b66\u573a\u666f\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7efc\u5408\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u6784\u5efa\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u3002\u5f00\u53d1MedEvalKit\u8bc4\u4f30\u6846\u67b6\u3002", "result": "Lingshu\u5728\u591a\u9879\u533b\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "Lingshu\u901a\u8fc7\u4f18\u5316\u6570\u636e\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66MLLMs\u7684\u6027\u80fd\uff0c\u4e3a\u533b\u5b66AI\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.07046", "pdf": "https://arxiv.org/pdf/2506.07046", "abs": "https://arxiv.org/abs/2506.07046", "authors": ["Anushka Jha", "Tanushree Dewangan", "Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine", "categories": ["cs.AR", "cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "Reinforcement Learning (RL) has outperformed other counterparts in sequential\ndecision-making and dynamic environment control. However, FPGA deployment is\nsignificantly resource-expensive, as associated with large number of\ncomputations in training agents with high-quality images and possess new\nchallenges. In this work, we propose QForce-RL takes benefits of quantization\nto enhance throughput and reduce energy footprint with light-weight RL\narchitecture, without significant performance degradation. QForce-RL takes\nadvantages from E2HRL to reduce overall RL actions to learn desired policy and\nQuaRL for quantization based SIMD for hardware acceleration. We have also\nprovided detailed analysis for different RL environments, with emphasis on\nmodel size, parameters, and accelerated compute ops. The architecture is\nscalable for resource-constrained devices and provide parametrized efficient\ndeployment with flexibility in latency, throughput, power, and energy\nefficiency. The proposed QForce-RL provides performance enhancement up to 2.3x\nand better FPS - 2.6x compared to SoTA works.", "AI": {"tldr": "QForce-RL\u5229\u7528\u91cf\u5316\u548c\u8f7b\u91cf\u7ea7\u67b6\u6784\u63d0\u5347FPGA\u4e0a\u7684RL\u6027\u80fd\uff0c\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "FPGA\u90e8\u7f72RL\u8d44\u6e90\u6d88\u8017\u5927\uff0cQForce-RL\u65e8\u5728\u901a\u8fc7\u91cf\u5316\u63d0\u5347\u541e\u5410\u91cf\u548c\u80fd\u6548\u3002", "method": "\u7ed3\u5408E2HRL\u51cf\u5c11\u52a8\u4f5c\u7a7a\u95f4\u548cQuaRL\u7684\u91cf\u5316SIMD\u52a0\u901f\u786c\u4ef6\u3002", "result": "\u6027\u80fd\u63d0\u53472.3\u500d\uff0cFPS\u63d0\u53472.6\u500d\u3002", "conclusion": "QForce-RL\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff0c\u63d0\u4f9b\u9ad8\u6548\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2506.07069", "pdf": "https://arxiv.org/pdf/2506.07069", "abs": "https://arxiv.org/abs/2506.07069", "authors": ["Zhican Wang", "Guanghui He", "Dantong Liu", "Lingjun Gao", "Shell Xu Hu", "Chen Zhang", "Zhuoran Song", "Nicholas Lane", "Wayne Luk", "Hongxiang Fan"], "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization", "categories": ["cs.GR", "cs.AR", "cs.CV", "cs.LG"], "comment": "Preprint. Under review", "summary": "3D Gaussian Splatting (3DGS) has recently gained significant attention for\nhigh-quality and efficient view synthesis, making it widely adopted in fields\nsuch as AR/VR, robotics, and autonomous driving. Despite its impressive\nalgorithmic performance, real-time rendering on resource-constrained devices\nremains a major challenge due to tight power and area budgets. This paper\npresents an architecture-algorithm co-design to address these inefficiencies.\nFirst, we reveal substantial redundancy caused by repeated computation of\ncommon terms/expressions during the conventional rasterization. To resolve\nthis, we propose axis-oriented rasterization, which pre-computes and reuses\nshared terms along both the X and Y axes through a dedicated hardware design,\neffectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by\nidentifying the resource and performance inefficiency of the sorting process,\nwe introduce a novel neural sorting approach that predicts order-independent\nblending weights using an efficient neural network, eliminating the need for\ncostly hardware sorters. A dedicated training framework is also proposed to\nimprove its algorithmic stability. Third, to uniformly support rasterization\nand neural network inference, we design an efficient reconfigurable processing\narray that maximizes hardware utilization and throughput. Furthermore, we\nintroduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and\nHilbert curve, to optimize Gaussian reuse and reduce memory access overhead.\nComprehensive experiments demonstrate that the proposed design preserves\nrendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy\nsavings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We\nplan to open-source our design to foster further development in this field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\u4e0e\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f74\u5b9a\u5411\u5149\u6805\u5316\u3001\u795e\u7ecf\u6392\u5e8f\u548c\u53ef\u91cd\u6784\u5904\u7406\u9635\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u548c\u80fd\u8017\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba13DGS\u5728\u9ad8\u8d28\u91cf\u89c6\u56fe\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6e32\u67d3\u4ecd\u9762\u4e34\u529f\u8017\u548c\u9762\u79ef\u9650\u5236\u7684\u6311\u6218\u3002", "method": "1. \u8f74\u5b9a\u5411\u5149\u6805\u5316\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff1b2. \u795e\u7ecf\u6392\u5e8f\u66ff\u4ee3\u786c\u4ef6\u6392\u5e8f\uff1b3. \u53ef\u91cd\u6784\u5904\u7406\u9635\u5217\u652f\u6301\u5149\u6805\u5316\u548c\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff1b4. \u03c0\u8f68\u8ff9\u74e6\u7247\u8c03\u5ea6\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bbe\u8ba1\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u901f\u5ea6\u63d0\u534723.4~27.8\u500d\uff0c\u80fd\u8017\u964d\u4f4e28.8~51.4\u500d\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u65483DGS\u6e32\u67d3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u8ba1\u5212\u5f00\u6e90\u4ee5\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2506.07180", "pdf": "https://arxiv.org/pdf/2506.07180", "abs": "https://arxiv.org/abs/2506.07180", "authors": ["Wenrui Zhou", "Shu Yang", "Qingsong Yang", "Zikun Guo", "Lijie Hu", "Di Wang"], "title": "Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "24 pages", "summary": "As video large language models (Video-LLMs) become increasingly integrated\ninto real-world applications that demand grounded multimodal reasoning,\nensuring their factual consistency and reliability is of critical importance.\nHowever, sycophancy, the tendency of these models to align with user input even\nwhen it contradicts the visual evidence, undermines their trustworthiness in\nsuch contexts. Current sycophancy research has largely overlooked its specific\nmanifestations in the video-language domain, resulting in a notable absence of\nsystematic benchmarks and targeted evaluations to understand how Video-LLMs\nrespond under misleading user input. To fill this gap, we propose VISE\n(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated\nbenchmark designed to evaluate sycophantic behavior in state-of-the-art\nVideo-LLMs across diverse question formats, prompt biases, and visual reasoning\ntasks. Specifically, VISE pioneeringly brings linguistic perspectives on\nsycophancy into the visual domain, enabling fine-grained analysis across\nmultiple sycophancy types and interaction patterns. In addition, we explore\nkey-frame selection as an interpretable, training-free mitigation strategy,\nwhich reveals potential paths for reducing sycophantic bias by strengthening\nvisual grounding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86VISE\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Video-LLMs\uff09\u5728\u8bef\u5bfc\u6027\u7528\u6237\u8f93\u5165\u4e0b\u7684\u8fce\u5408\u884c\u4e3a\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7cfb\u7edf\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u8fce\u5408\u7528\u6237\u8f93\u5165\u7684\u503e\u5411\uff08\u5373\u4f7f\u4e0e\u89c6\u89c9\u8bc1\u636e\u77db\u76fe\uff09\u5f71\u54cd\u4e86\u53ef\u4fe1\u5ea6\uff0c\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u89c6\u9891\u8bed\u8a00\u9886\u57df\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u63d0\u51faVISE\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u95ee\u9898\u683c\u5f0f\u3001\u63d0\u793a\u504f\u89c1\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u8bc4\u4f30Video-LLMs\u7684\u8fce\u5408\u884c\u4e3a\uff0c\u5e76\u63a2\u7d22\u57fa\u4e8e\u5173\u952e\u5e27\u9009\u62e9\u7684\u7f13\u89e3\u7b56\u7565\u3002", "result": "VISE\u9996\u6b21\u5c06\u8bed\u8a00\u9886\u57df\u7684\u8fce\u5408\u884c\u4e3a\u5206\u6790\u5f15\u5165\u89c6\u89c9\u9886\u57df\uff0c\u4e3a\u591a\u7c7b\u578b\u8fce\u5408\u884c\u4e3a\u548c\u4ea4\u4e92\u6a21\u5f0f\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "conclusion": "\u5173\u952e\u5e27\u9009\u62e9\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\u51cf\u5c11\u8fce\u5408\u504f\u89c1\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.07184", "pdf": "https://arxiv.org/pdf/2506.07184", "abs": "https://arxiv.org/abs/2506.07184", "authors": ["Liangliang You", "Junchi Yao", "Shu Yang", "Guimin Hu", "Lijie Hu", "Di Wang"], "title": "Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "While multimodal large language models excel at various tasks, they still\nsuffer from hallucinations, which limit their reliability and scalability for\nbroader domain applications. To address this issue, recent research mainly\nfocuses on objective hallucination. However, for sequential images, besides\nobjective hallucination, there is also behavioral hallucination, which is less\nstudied. This work aims to fill in the gap. We first reveal that behavioral\nhallucinations mainly arise from two key factors: prior-driven bias and the\nsnowball effect. Based on these observations, we introduce SHE (Sequence\nHallucination Eradication), a lightweight, two-stage framework that (1) detects\nhallucinations via visual-textual alignment check using our proposed adaptive\ntemporal window and (2) mitigates them via orthogonal projection onto the joint\nembedding space. We also propose a new metric (BEACH) to quantify behavioral\nhallucination severity. Empirical results on standard benchmarks demonstrate\nthat SHE reduces behavioral hallucination by over 10% on BEACH while\nmaintaining descriptive accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSHE\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u68c0\u6d4b\u548c\u7f13\u89e3\u884c\u4e3a\u5e7b\u89c9\uff0c\u5e76\u5f15\u5165\u65b0\u6307\u6807BEACH\u91cf\u5316\u5176\u4e25\u91cd\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u884c\u4e3a\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5ba2\u89c2\u5e7b\u89c9\uff0c\u800c\u884c\u4e3a\u5e7b\u89c9\u7814\u7a76\u8f83\u5c11\u3002", "method": "SHE\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u95f4\u7a97\u53e3\u68c0\u6d4b\u5e7b\u89c9\uff0c\u5e76\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u7f13\u89e3\u5e7b\u89c9\u3002", "result": "SHE\u5728BEACH\u6307\u6807\u4e0a\u51cf\u5c11\u884c\u4e3a\u5e7b\u89c9\u8d85\u8fc710%\uff0c\u540c\u65f6\u4fdd\u6301\u63cf\u8ff0\u51c6\u786e\u6027\u3002", "conclusion": "SHE\u6709\u6548\u586b\u8865\u4e86\u884c\u4e3a\u5e7b\u89c9\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u53ef\u9760\u6027\u3002"}}
{"id": "2506.07209", "pdf": "https://arxiv.org/pdf/2506.07209", "abs": "https://arxiv.org/abs/2506.07209", "authors": ["Lei Li", "Angela Dai"], "title": "HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://hoipage.github.io/ Video:\n  https://youtu.be/b1pJU9lKQTE", "summary": "We present HOI-PAGE, a new approach to synthesizing 4D human-object\ninteractions (HOIs) from text prompts in a zero-shot fashion, driven by\npart-level affordance reasoning. In contrast to prior works that focus on\nglobal, whole body-object motion for 4D HOI synthesis, we observe that\ngenerating realistic and diverse HOIs requires a finer-grained understanding --\nat the level of how human body parts engage with object parts. We thus\nintroduce Part Affordance Graphs (PAGs), a structured HOI representation\ndistilled from large language models (LLMs) that encodes fine-grained part\ninformation along with contact relations. We then use these PAGs to guide a\nthree-stage synthesis: first, decomposing input 3D objects into geometric\nparts; then, generating reference HOI videos from text prompts, from which we\nextract part-based motion constraints; finally, optimizing for 4D HOI motion\nsequences that not only mimic the reference dynamics but also satisfy\npart-level contact constraints. Extensive experiments show that our approach is\nflexible and capable of generating complex multi-object or multi-person\ninteraction sequences, with significantly improved realism and text alignment\nfor zero-shot 4D HOI generation.", "AI": {"tldr": "HOI-PAGE\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6587\u672c\u63d0\u793a\u96f6\u6837\u672c\u5408\u62104D\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u90e8\u5206\u7ea7\u529f\u80fd\u63a8\u7406\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5168\u5c40\u7684\u5168\u8eab-\u7269\u4f53\u8fd0\u52a8\uff0c\u800c\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684HOI\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u7406\u89e3\uff0c\u5373\u4eba\u4f53\u90e8\u5206\u5982\u4f55\u4e0e\u7269\u4f53\u90e8\u5206\u4ea4\u4e92\u3002", "method": "\u5f15\u5165\u90e8\u5206\u529f\u80fd\u56fe\uff08PAGs\uff09\uff0c\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316HOI\u8868\u793a\uff0c\u6307\u5bfc\u4e09\u9636\u6bb5\u5408\u6210\uff1a\u5206\u89e33D\u7269\u4f53\u3001\u751f\u6210\u53c2\u8003\u89c6\u9891\u5e76\u63d0\u53d6\u8fd0\u52a8\u7ea6\u675f\u3001\u4f18\u53164D HOI\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u7075\u6d3b\u751f\u6210\u590d\u6742\u591a\u7269\u4f53\u6216\u591a\u4eba\u7684\u4ea4\u4e92\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c4D HOI\u7684\u771f\u5b9e\u6027\u548c\u6587\u672c\u5bf9\u9f50\u6027\u3002", "conclusion": "HOI-PAGE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u90e8\u5206\u7ea7\u63a8\u7406\uff0c\u5728\u96f6\u6837\u672c4D HOI\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.07218", "pdf": "https://arxiv.org/pdf/2506.07218", "abs": "https://arxiv.org/abs/2506.07218", "authors": ["Tong Xiao", "Xin Xu", "Zhenya Huang", "Hongyu Gao", "Quan Liu", "Qi Liu", "Enhong Chen"], "title": "Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language\nModels (MLLMs) is a challenging task that has attracted increasing attention in\nthe community. Recently, several studies have applied Reinforcement Learning\nwith Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the\nreasoning abilities of MLLMs. However, these works largely overlook the\nenhancement of multimodal perception capabilities in MLLMs, which serve as a\ncore prerequisite and foundational component of complex multimodal reasoning.\nThrough McNemar's test, we find that existing RLVR method fails to effectively\nenhance the multimodal perception capabilities of MLLMs, thereby limiting their\nfurther improvement in multimodal reasoning. To address this limitation, we\npropose Perception-R1, which introduces a novel visual perception reward that\nexplicitly encourages MLLMs to perceive the visual content accurately, thereby\ncan effectively incentivizing both their multimodal perception and reasoning\ncapabilities. Specifically, we first collect textual visual annotations from\nthe CoT trajectories of multimodal problems, which will serve as visual\nreferences for reward assignment. During RLVR training, we employ a judging LLM\nto assess the consistency between the visual annotations and the responses\ngenerated by MLLM, and assign the visual perception reward based on these\nconsistency judgments. Extensive experiments on several multimodal reasoning\nbenchmarks demonstrate the effectiveness of our Perception-R1, which achieves\nstate-of-the-art performance on most benchmarks using only 1,442 training data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPerception-R1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u611f\u77e5\u5956\u52b1\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u611f\u77e5\u80fd\u529b\uff0c\u9650\u5236\u4e86\u63a8\u7406\u80fd\u529b\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u63d0\u51faPerception-R1\uff0c\u5229\u7528\u89c6\u89c9\u6ce8\u91ca\u4f5c\u4e3a\u5956\u52b1\u4f9d\u636e\uff0c\u901a\u8fc7LLM\u8bc4\u4f30\u4e00\u81f4\u6027\u5e76\u5206\u914d\u5956\u52b1\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u4ec5\u97001,442\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "Perception-R1\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002"}}
{"id": "2506.07228", "pdf": "https://arxiv.org/pdf/2506.07228", "abs": "https://arxiv.org/abs/2506.07228", "authors": ["Shuvashis Sarker"], "title": "Transfer Learning and Explainable AI for Brain Tumor Classification: A Study Using MRI Data from Bangladesh", "categories": ["eess.IV", "cs.CV"], "comment": "2024 6th International Conference on Sustainable Technologies for\n  Industry 5.0 (STI)", "summary": "Brain tumors, regardless of being benign or malignant, pose considerable\nhealth risks, with malignant tumors being more perilous due to their swift and\nuncontrolled proliferation, resulting in malignancy. Timely identification is\ncrucial for enhancing patient outcomes, particularly in nations such as\nBangladesh, where healthcare infrastructure is constrained. Manual MRI analysis\nis arduous and susceptible to inaccuracies, rendering it inefficient for prompt\ndiagnosis. This research sought to tackle these problems by creating an\nautomated brain tumor classification system utilizing MRI data obtained from\nmany hospitals in Bangladesh. Advanced deep learning models, including VGG16,\nVGG19, and ResNet50, were utilized to classify glioma, meningioma, and various\nbrain cancers. Explainable AI (XAI) methodologies, such as Grad-CAM and\nGrad-CAM++, were employed to improve model interpretability by emphasizing the\ncritical areas in MRI scans that influenced the categorization. VGG16 achieved\nthe most accuracy, attaining 99.17%. The integration of XAI enhanced the\nsystem's transparency and stability, rendering it more appropriate for clinical\napplication in resource-limited environments such as Bangladesh. This study\nhighlights the capability of deep learning models, in conjunction with\nexplainable artificial intelligence (XAI), to enhance brain tumor detection and\nidentification in areas with restricted access to advanced medical\ntechnologies.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u8111\u80bf\u7624\u5206\u7c7b\u7cfb\u7edf\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\u3002", "motivation": "\u8111\u80bf\u7624\u7684\u53ca\u65f6\u8bca\u65ad\u5bf9\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\u5982\u5b5f\u52a0\u62c9\u56fd\u3002\u624b\u52a8MRI\u5206\u6790\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528VGG16\u3001VGG19\u548cResNet50\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u7c7b\u8111\u80bf\u7624\uff0c\u5e76\u7ed3\u5408Grad-CAM\u548cGrad-CAM++\u7b49XAI\u6280\u672f\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "VGG16\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe99.17%\u3002XAI\u6280\u672f\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4e0eXAI\u7ed3\u5408\u53ef\u6709\u6548\u63d0\u5347\u8111\u80bf\u7624\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u533b\u7597\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\u3002"}}
{"id": "2506.07234", "pdf": "https://arxiv.org/pdf/2506.07234", "abs": "https://arxiv.org/abs/2506.07234", "authors": ["Shuvashis Sarker"], "title": "A Comprehensive Analysis of COVID-19 Detection Using Bangladeshi Data and Explainable AI", "categories": ["eess.IV", "cs.CV"], "comment": "2024 4th International Conference on Innovations in Science,\n  Engineering and Technology (ICISET)", "summary": "COVID-19 is a rapidly spreading and highly infectious virus which has\ntriggered a global pandemic, profoundly affecting millions across the world.\nThe pandemic has introduced unprecedented challenges in public health, economic\nstability, and societal structures, necessitating the implementation of\nextensive and multifaceted health interventions globally. It had a tremendous\nimpact on Bangladesh by April 2024, with around 29,495 fatalities and more than\n2 million confirmed cases. This study focuses on improving COVID-19 detection\nin CXR images by utilizing a dataset of 4,350 images from Bangladesh\ncategorized into four classes: Normal, Lung-Opacity, COVID-19 and\nViral-Pneumonia. ML, DL and TL models are employed with the VGG19 model\nachieving an impressive 98% accuracy. LIME is used to explain model\npredictions, highlighting the regions and features influencing classification\ndecisions. SMOTE is applied to address class imbalances. By providing insight\ninto both correct and incorrect classifications, the study emphasizes the\nimportance of XAI in enhancing the transparency and reliability of models,\nultimately improving the effectiveness of detection from CXR images.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08VGG19\uff09\u5728CXR\u56fe\u50cf\u4e2d\u68c0\u6d4bCOVID-19\uff0c\u51c6\u786e\u7387\u8fbe98%\uff0c\u5e76\u901a\u8fc7XAI\u6280\u672f\u63d0\u9ad8\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "COVID-19\u5168\u7403\u5927\u6d41\u884c\u5bf9\u5b5f\u52a0\u62c9\u56fd\u9020\u6210\u4e25\u91cd\u5f71\u54cd\uff0c\u4e9f\u9700\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u75284,350\u5f20CXR\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e94\u7528ML\u3001DL\u548cTL\u6a21\u578b\uff08\u5982VGG19\uff09\uff0c\u7ed3\u5408LIME\u548cSMOTE\u6280\u672f\u3002", "result": "VGG19\u6a21\u578b\u8fbe\u523098%\u51c6\u786e\u7387\uff0cLIME\u89e3\u91ca\u6a21\u578b\u51b3\u7b56\uff0cSMOTE\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "XAI\u6280\u672f\u63d0\u5347\u4e86\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u6709\u52a9\u4e8e\u6539\u8fdbCXR\u56fe\u50cf\u4e2d\u7684COVID-19\u68c0\u6d4b\u3002"}}
{"id": "2506.07236", "pdf": "https://arxiv.org/pdf/2506.07236", "abs": "https://arxiv.org/abs/2506.07236", "authors": ["Jiachen Zhong", "Yiting Wang", "Di Zhu", "Ziwei Wang"], "title": "A Narrative Review on Large AI Models in Lung Cancer Screening, Diagnosis, and Treatment Planning", "categories": ["eess.IV", "cs.CV"], "comment": "Under Review", "summary": "Lung cancer remains one of the most prevalent and fatal diseases worldwide,\ndemanding accurate and timely diagnosis and treatment. Recent advancements in\nlarge AI models have significantly enhanced medical image understanding and\nclinical decision-making. This review systematically surveys the\nstate-of-the-art in applying large AI models to lung cancer screening,\ndiagnosis, prognosis, and treatment. We categorize existing models into\nmodality-specific encoders, encoder-decoder frameworks, and joint encoder\narchitectures, highlighting key examples such as CLIP, BLIP, Flamingo,\nBioViL-T, and GLoRIA. We further examine their performance in multimodal\nlearning tasks using benchmark datasets like LIDC-IDRI, NLST, and MIMIC-CXR.\nApplications span pulmonary nodule detection, gene mutation prediction,\nmulti-omics integration, and personalized treatment planning, with emerging\nevidence of clinical deployment and validation. Finally, we discuss current\nlimitations in generalizability, interpretability, and regulatory compliance,\nproposing future directions for building scalable, explainable, and clinically\nintegrated AI systems. Our review underscores the transformative potential of\nlarge AI models to personalize and optimize lung cancer care.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578bAI\u6a21\u578b\u5728\u80ba\u764c\u7b5b\u67e5\u3001\u8bca\u65ad\u3001\u9884\u540e\u548c\u6cbb\u7597\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5206\u7c7b\u3001\u6027\u80fd\u53ca\u4e34\u5e8a\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u5c40\u9650\u6027\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u80ba\u764c\u662f\u5168\u7403\u9ad8\u53d1\u4e14\u81f4\u547d\u7684\u75be\u75c5\uff0c\u4e9f\u9700\u51c6\u786e\u53ca\u65f6\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u3002\u5927\u578bAI\u6a21\u578b\u7684\u8fdb\u6b65\u4e3a\u533b\u5b66\u56fe\u50cf\u7406\u89e3\u548c\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "method": "\u7cfb\u7edf\u8c03\u67e5\u4e86\u5927\u578bAI\u6a21\u578b\u5728\u80ba\u764c\u9886\u57df\u7684\u5e94\u7528\uff0c\u5206\u7c7b\u4e3a\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u3001\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u548c\u8054\u5408\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u80ba\u7ed3\u8282\u68c0\u6d4b\u3001\u57fa\u56e0\u7a81\u53d8\u9884\u6d4b\u3001\u591a\u7ec4\u5b66\u6574\u5408\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u90e8\u5206\u5df2\u8fdb\u5165\u4e34\u5e8a\u9a8c\u8bc1\u9636\u6bb5\u3002", "conclusion": "\u5927\u578bAI\u6a21\u578b\u5728\u80ba\u764c\u8bca\u7597\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5408\u89c4\u6027\u7b49\u6311\u6218\uff0c\u672a\u6765\u5e94\u6784\u5efa\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u4e34\u5e8a\u96c6\u6210\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2506.07296", "pdf": "https://arxiv.org/pdf/2506.07296", "abs": "https://arxiv.org/abs/2506.07296", "authors": ["Arian Askari", "Emmanouil Stergiadis", "Ilya Gusev", "Moran Beladev"], "title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "Accepted at ACL 2025, Main track. 13 Pages, 1 figure", "summary": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel\ndomain that enables natural language property search, addressing the\nlimitations of traditional travel search engines which require users to start\nwith a destination and editing search parameters. HotelMatch-LLM features three\nkey innovations: (1) Domain-specific multi-task optimization with three novel\nretrieval, visual, and language modeling objectives; (2) Asymmetrical dense\nretrieval architecture combining a small language model (SLM) for efficient\nonline query processing and a large language model (LLM) for embedding hotel\ndata; and (3) Extensive image processing to handle all property image\ngalleries. Experiments on four diverse test sets show HotelMatch-LLM\nsignificantly outperforms state-of-the-art models, including VISTA and MARVEL.\nSpecifically, on the test set -- main query type -- we achieve 0.681 for\nHotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our\nanalysis highlights the impact of our multi-task optimization, the\ngeneralizability of HotelMatch-LLM across LLM architectures, and its\nscalability for processing large image galleries.", "AI": {"tldr": "HotelMatch-LLM\u662f\u4e00\u79cd\u591a\u6a21\u6001\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\uff0c\u7528\u4e8e\u65c5\u884c\u9886\u57df\u7684\u81ea\u7136\u8bed\u8a00\u5c5e\u6027\u641c\u7d22\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65c5\u884c\u641c\u7d22\u5f15\u64ce\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u65c5\u884c\u641c\u7d22\u5f15\u64ce\u8981\u6c42\u7528\u6237\u4ece\u76ee\u7684\u5730\u5f00\u59cb\u5e76\u7f16\u8f91\u641c\u7d22\u53c2\u6570\uff0c\u9650\u5236\u4e86\u7528\u6237\u4f53\u9a8c\u3002HotelMatch-LLM\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u641c\u7d22\u63d0\u5347\u4fbf\u5229\u6027\u3002", "method": "\u6a21\u578b\u7ed3\u5408\u4e86\u591a\u4efb\u52a1\u4f18\u5316\uff08\u68c0\u7d22\u3001\u89c6\u89c9\u548c\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\uff09\u3001\u975e\u5bf9\u79f0\u5bc6\u96c6\u68c0\u7d22\u67b6\u6784\uff08SLM\u5904\u7406\u67e5\u8be2\uff0cLLM\u5d4c\u5165\u9152\u5e97\u6570\u636e\uff09\u4ee5\u53ca\u56fe\u50cf\u5904\u7406\u6280\u672f\u3002", "result": "\u5728\u56db\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\uff0cHotelMatch-LLM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u5982VISTA\u548cMARVEL\uff09\uff0c\u5728\u4e3b\u67e5\u8be2\u7c7b\u578b\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u52300.681\uff08MARVEL\u4e3a0.603\uff09\u3002", "conclusion": "HotelMatch-LLM\u901a\u8fc7\u591a\u4efb\u52a1\u4f18\u5316\u3001\u8de8LLM\u67b6\u6784\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5927\u89c4\u6a21\u56fe\u50cf\u5904\u7406\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65c5\u884c\u641c\u7d22\u9886\u57df\u7684\u4f18\u8d8a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.07301", "pdf": "https://arxiv.org/pdf/2506.07301", "abs": "https://arxiv.org/abs/2506.07301", "authors": ["Marco P. M. de Souza", "Juciane G. Maia", "Lilian N. de Andrade"], "title": "Pendulum Tracker -- SimuF\u00edsica: A Web-based Tool for Real-time Measurement of Oscillatory Motion", "categories": ["physics.ed-ph", "cs.CV"], "comment": null, "summary": "We present Pendulum Tracker, a computer vision-based application that enables\nreal-time measurement of the oscillatory motion of a physical pendulum.\nIntegrated into the educational platform SimuF\\'isica, the system uses the\nOpenCV.js library and runs directly in the browser, working on computers,\ntablets, and smartphones. The application automatically detects the pendulum's\nposition via the device's camera, displaying in real time the angle-versus-time\ngraph and estimates of the oscillation period. Experimental case studies\ndemonstrate its effectiveness in measuring the period, determining\ngravitational acceleration, and analyzing damped oscillations. The results show\nexcellent agreement with theoretical predictions, confirming the system's\naccuracy and its applicability in educational contexts. The accessible\ninterface and the ability to export raw data make Pendulum Tracker a versatile\ntool for experimental physics teaching.", "AI": {"tldr": "Pendulum Tracker\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u7528\u4e8e\u5b9e\u65f6\u6d4b\u91cf\u7269\u7406\u6446\u7684\u632f\u8361\u8fd0\u52a8\uff0c\u9002\u7528\u4e8e\u6559\u80b2\u5e73\u53f0SimuF\u00edsica\uff0c\u652f\u6301\u591a\u79cd\u8bbe\u5907\u3002", "motivation": "\u4e3a\u7269\u7406\u6559\u5b66\u63d0\u4f9b\u4e00\u4e2a\u5b9e\u65f6\u3001\u51c6\u786e\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u6d4b\u91cf\u548c\u5206\u6790\u6446\u7684\u8fd0\u52a8\u3002", "method": "\u5229\u7528OpenCV.js\u5e93\u548c\u6d4f\u89c8\u5668\u5185\u8fd0\u884c\u7684\u6280\u672f\uff0c\u901a\u8fc7\u8bbe\u5907\u6444\u50cf\u5934\u81ea\u52a8\u68c0\u6d4b\u6446\u7684\u4f4d\u7f6e\uff0c\u5b9e\u65f6\u663e\u793a\u89d2\u5ea6-\u65f6\u95f4\u56fe\u548c\u5468\u671f\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u6d4b\u91cf\u5468\u671f\u3001\u91cd\u529b\u52a0\u901f\u5ea6\u548c\u5206\u6790\u963b\u5c3c\u632f\u8361\u65b9\u9762\u4e0e\u7406\u8bba\u9884\u6d4b\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "Pendulum Tracker\u662f\u4e00\u4e2a\u9002\u7528\u4e8e\u5b9e\u9a8c\u7269\u7406\u6559\u5b66\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\u3002"}}
{"id": "2506.07350", "pdf": "https://arxiv.org/pdf/2506.07350", "abs": "https://arxiv.org/abs/2506.07350", "authors": ["Yijie Deng", "Shuaihang Yuan", "Congcong Wen", "Hao Huang", "Anthony Tzes", "Geeta Chandra Raju Bethala", "Yi Fang"], "title": "MapBERT: Bitwise Masked Modeling for Real-Time Semantic Mapping Generation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Spatial awareness is a critical capability for embodied agents, as it enables\nthem to anticipate and reason about unobserved regions. The primary challenge\narises from learning the distribution of indoor semantics, complicated by\nsparse, imbalanced object categories and diverse spatial scales. Existing\nmethods struggle to robustly generate unobserved areas in real time and do not\ngeneralize well to new environments. To this end, we propose \\textbf{MapBERT},\na novel framework designed to effectively model the distribution of unseen\nspaces. Motivated by the observation that the one-hot encoding of semantic maps\naligns naturally with the binary structure of bit encoding, we, for the first\ntime, leverage a lookup-free BitVAE to encode semantic maps into compact\nbitwise tokens. Building on this, a masked transformer is employed to infer\nmissing regions and generate complete semantic maps from limited observations.\nTo enhance object-centric reasoning, we propose an object-aware masking\nstrategy that masks entire object categories concurrently and pairs them with\nlearnable embeddings, capturing implicit relationships between object\nembeddings and spatial tokens. By learning these relationships, the model more\neffectively captures indoor semantic distributions crucial for practical\nrobotic tasks. Experiments on Gibson benchmarks show that MapBERT achieves\nstate-of-the-art semantic map generation, balancing computational efficiency\nwith accurate reconstruction of unobserved regions.", "AI": {"tldr": "MapBERT\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528BitVAE\u548c\u63a9\u7801\u53d8\u6362\u5668\u751f\u6210\u672a\u89c2\u5bdf\u533a\u57df\u7684\u8bed\u4e49\u5730\u56fe\uff0c\u901a\u8fc7\u5bf9\u8c61\u611f\u77e5\u63a9\u7801\u7b56\u7565\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5728Gibson\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u751f\u6210\u672a\u89c2\u5bdf\u533a\u57df\u548c\u6cdb\u5316\u5230\u65b0\u73af\u5883\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5b66\u4e60\u5ba4\u5185\u8bed\u4e49\u5206\u5e03\u7684\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528BitVAE\u7f16\u7801\u8bed\u4e49\u5730\u56fe\u4e3a\u7d27\u51d1\u6bd4\u7279\u4ee4\u724c\uff0c\u7ed3\u5408\u63a9\u7801\u53d8\u6362\u5668\u63a8\u65ad\u7f3a\u5931\u533a\u57df\uff0c\u5e76\u63d0\u51fa\u5bf9\u8c61\u611f\u77e5\u63a9\u7801\u7b56\u7565\u589e\u5f3a\u63a8\u7406\u3002", "result": "\u5728Gibson\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8bed\u4e49\u5730\u56fe\u751f\u6210\u6548\u679c\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u672a\u89c2\u5bdf\u533a\u57df\u7684\u51c6\u786e\u91cd\u5efa\u3002", "conclusion": "MapBERT\u901a\u8fc7\u521b\u65b0\u7684\u6bd4\u7279\u7f16\u7801\u548c\u5bf9\u8c61\u611f\u77e5\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5ba4\u5185\u8bed\u4e49\u5206\u5e03\u5efa\u6a21\u7684\u6311\u6218\u3002"}}
{"id": "2506.07400", "pdf": "https://arxiv.org/pdf/2506.07400", "abs": "https://arxiv.org/abs/2506.07400", "authors": ["Philip Liu", "Sparsh Bansal", "Jimmy Dinh", "Aditya Pawar", "Ramani Satishkumar", "Shail Desai", "Neeraj Gupta", "Xin Wang", "Shu Hu"], "title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "comment": "7 pages, 6 figures. Accepted to the 2025 IEEE 8th International\n  Conference on Multimedia Information Processing and Retrieval (MIPR). Code\n  and platform available at https://github.com/Purdue-M2/MedChat", "summary": "The integration of deep learning-based glaucoma detection with large language\nmodels (LLMs) presents an automated strategy to mitigate ophthalmologist\nshortages and improve clinical reporting efficiency. However, applying general\nLLMs to medical imaging remains challenging due to hallucinations, limited\ninterpretability, and insufficient domain-specific medical knowledge, which can\npotentially reduce clinical accuracy. Although recent approaches combining\nimaging models with LLM reasoning have improved reporting, they typically rely\non a single generalist agent, restricting their capacity to emulate the diverse\nand complex reasoning found in multidisciplinary medical teams. To address\nthese limitations, we propose MedChat, a multi-agent diagnostic framework and\nplatform that combines specialized vision models with multiple role-specific\nLLM agents, all coordinated by a director agent. This design enhances\nreliability, reduces hallucination risk, and enables interactive diagnostic\nreporting through an interface tailored for clinical review and educational\nuse. Code available at https://github.com/Purdue-M2/MedChat.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedChat\u7684\u591a\u667a\u80fd\u4f53\u8bca\u65ad\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u4e1a\u89c6\u89c9\u6a21\u578b\u548c\u89d2\u8272\u7279\u5b9a\u7684LLM\u667a\u80fd\u4f53\uff0c\u4ee5\u89e3\u51b3\u901a\u7528LLM\u5728\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u4e2d\u7684\u5e7b\u89c9\u548c\u89e3\u91ca\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u901a\u7528LLM\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5982\u5e7b\u89c9\u3001\u89e3\u91ca\u6027\u4e0d\u8db3\u548c\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\uff0c\u540c\u65f6\u6a21\u62df\u591a\u5b66\u79d1\u533b\u7597\u56e2\u961f\u7684\u590d\u6742\u63a8\u7406\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ec\u4e13\u4e1a\u89c6\u89c9\u6a21\u578b\u548c\u591a\u4e2a\u89d2\u8272\u7279\u5b9a\u7684LLM\u667a\u80fd\u4f53\uff0c\u7531\u5bfc\u6f14\u667a\u80fd\u4f53\u534f\u8c03\uff0c\u4ee5\u63d0\u5347\u53ef\u9760\u6027\u548c\u4ea4\u4e92\u6027\u3002", "result": "MedChat\u63d0\u9ad8\u4e86\u8bca\u65ad\u62a5\u544a\u7684\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u98ce\u9669\uff0c\u5e76\u63d0\u4f9b\u4e86\u9002\u5408\u4e34\u5e8a\u548c\u6559\u80b2\u7528\u9014\u7684\u4ea4\u4e92\u754c\u9762\u3002", "conclusion": "MedChat\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u901a\u7528LLM\u5728\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u4e2d\u7684\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u548c\u6559\u80b2\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.07413", "pdf": "https://arxiv.org/pdf/2506.07413", "abs": "https://arxiv.org/abs/2506.07413", "authors": ["Ziwen Wang", "Jiajun Fan", "Thao Nguyen", "Heng Ji", "Ge Liu"], "title": "Variational Supervised Contrastive Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive learning has proven to be highly efficient and adaptable in\nshaping representation spaces across diverse modalities by pulling similar\nsamples together and pushing dissimilar ones apart. However, two key\nlimitations persist: (1) Without explicit regulation of the embedding\ndistribution, semantically related instances can inadvertently be pushed apart\nunless complementary signals guide pair selection, and (2) excessive reliance\non large in-batch negatives and tailored augmentations hinders generalization.\nTo address these limitations, we propose Variational Supervised Contrastive\nLearning (VarCon), which reformulates supervised contrastive learning as\nvariational inference over latent class variables and maximizes a\nposterior-weighted evidence lower bound (ELBO) that replaces exhaustive\npair-wise comparisons for efficient class-aware matching and grants\nfine-grained control over intra-class dispersion in the embedding space.\nTrained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,\nImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art\nperformance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy\non ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while\nconverging in just 200 epochs; (2) yields substantially clearer decision\nboundaries and semantic organization in the embedding space, as evidenced by\nKNN classification, hierarchical clustering results, and transfer-learning\nassessments; and (3) demonstrates superior performance in few-shot learning\nthan supervised baseline and superior robustness across various augmentation\nstrategies.", "AI": {"tldr": "VarCon\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u6539\u8fdb\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u5d4c\u5165\u5206\u5e03\u65e0\u660e\u786e\u8c03\u63a7\u548c\u8fc7\u5ea6\u4f9d\u8d56\u8d1f\u6837\u672c\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5d4c\u5165\u5206\u5e03\u65e0\u660e\u786e\u8c03\u63a7\u548c\u8fc7\u5ea6\u4f9d\u8d56\u8d1f\u6837\u672c\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faVarCon\uff0c\u5c06\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u91cd\u65b0\u8868\u8ff0\u4e3a\u5bf9\u6f5c\u5728\u7c7b\u522b\u53d8\u91cf\u7684\u53d8\u5206\u63a8\u7406\uff0c\u6700\u5927\u5316\u540e\u9a8c\u52a0\u6743\u7684ELBO\uff0c\u5b9e\u73b0\u9ad8\u6548\u7c7b\u611f\u77e5\u5339\u914d\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001ImageNet\u7b49\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0cTop-1\u51c6\u786e\u738779.36%\uff08ImageNet-1K\uff09\uff0c\u5e76\u5c55\u793a\u6e05\u6670\u7684\u51b3\u7b56\u8fb9\u754c\u548c\u8bed\u4e49\u7ec4\u7ec7\u3002", "conclusion": "VarCon\u5728\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2506.07475", "pdf": "https://arxiv.org/pdf/2506.07475", "abs": "https://arxiv.org/abs/2506.07475", "authors": ["Gaoyu Chen"], "title": "Text-guided multi-stage cross-perception network for medical image segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation plays a crucial role in clinical medicine, serving\nas a tool for auxiliary diagnosis, treatment planning, and disease monitoring,\nthus facilitating physicians in the study and treatment of diseases. However,\nexisting medical image segmentation methods are limited by the weak semantic\nexpression of the target segmentation regions, which is caused by the low\ncontrast between the target and non-target segmentation regions. To address\nthis limitation, text prompt information has greast potential to capture the\nlesion location. However, existing text-guided methods suffer from insufficient\ncross-modal interaction and inadequate cross-modal feature expression. To\nresolve these issues, we propose the Text-guided Multi-stage Cross-perception\nnetwork (TMC). In TMC, we introduce a multistage cross-attention module to\nenhance the model's understanding of semantic details and a multi-stage\nalignment loss to improve the consistency of cross-modal semantics. The results\nof the experiments demonstrate that our TMC achieves a superior performance\nwith Dice of 84.77%, 78.50%, 88.73% in three public datasets (QaTa-COV19,\nMosMedData and Breast), outperforming UNet based networks and text-guided\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u591a\u9636\u6bb5\u8de8\u611f\u77e5\u7f51\u7edc\uff08TMC\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8bed\u4e49\u8868\u8fbe\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u4e34\u5e8a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u76ee\u6807\u4e0e\u975e\u76ee\u6807\u533a\u57df\u5bf9\u6bd4\u5ea6\u4f4e\u5bfc\u81f4\u8bed\u4e49\u8868\u8fbe\u5f31\u3002\u6587\u672c\u63d0\u793a\u4fe1\u606f\u6709\u6f5c\u529b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8de8\u6a21\u6001\u4ea4\u4e92\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTMC\u7f51\u7edc\uff0c\u5f15\u5165\u591a\u9636\u6bb5\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u8bed\u4e49\u7ec6\u8282\u7406\u89e3\uff0c\u5e76\u4f7f\u7528\u591a\u9636\u6bb5\u5bf9\u9f50\u635f\u5931\u63d0\u5347\u8de8\u6a21\u6001\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08QaTa-COV19\u3001MosMedData\u548cBreast\uff09\u4e0a\uff0cTMC\u7684Dice\u5206\u6570\u5206\u522b\u4e3a84.77%\u300178.50%\u548c88.73%\uff0c\u4f18\u4e8eUNet\u548c\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u65b9\u6cd5\u3002", "conclusion": "TMC\u901a\u8fc7\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2506.07530", "pdf": "https://arxiv.org/pdf/2506.07530", "abs": "https://arxiv.org/abs/2506.07530", "authors": ["Hongyu Wang", "Chuyan Xiong", "Ruiping Wang", "Xilin Chen"], "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Work in progress", "summary": "Vision-Language-Action (VLA) models have shown impressive capabilities across\na wide range of robotics manipulation tasks. However, their growing model size\nposes significant challenges for deployment on resource-constrained robotic\nsystems. While 1-bit pretraining has proven effective for enhancing the\ninference efficiency of large language models with minimal performance loss,\nits application to VLA models remains underexplored. In this work, we present\nBitVLA, the first 1-bit VLA model for robotics manipulation, in which every\nparameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint\nof the vision encoder, we propose the distillation-aware training strategy that\ncompresses the full-precision encoder to 1.58-bit weights. During this process,\na full-precision encoder serves as a teacher model to better align latent\nrepresentations. Despite the lack of large-scale robotics pretraining, BitVLA\nachieves performance comparable to the state-of-the-art model OpenVLA-OFT with\n4-bit post-training quantization on the LIBERO benchmark, while consuming only\n29.8% of the memory. These results highlight BitVLA's promise for deployment on\nmemory-constrained edge devices. We release the code and model weights in\nhttps://github.com/ustcwhy/BitVLA.", "AI": {"tldr": "BitVLA\u662f\u9996\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u76841\u4f4dVLA\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u5143\u53c2\u6570\u548c1.58\u4f4d\u89c6\u89c9\u7f16\u7801\u5668\u538b\u7f29\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u6027\u80fd\u63a5\u8fd14\u4f4d\u91cf\u5316\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u90e8\u7f72\u7684\u6311\u6218\uff0c\u63a2\u7d221\u4f4d\u9884\u8bad\u7ec3\u5728VLA\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faBitVLA\u6a21\u578b\uff0c\u91c7\u7528\u4e09\u5143\u53c2\u6570\u548c\u84b8\u998f\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\u538b\u7f29\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5229\u7528\u5168\u7cbe\u5ea6\u6559\u5e08\u6a21\u578b\u5bf9\u9f50\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBitVLA\u6027\u80fd\u63a5\u8fd14\u4f4d\u91cf\u5316\u7684OpenVLA-OFT\uff0c\u5185\u5b58\u5360\u7528\u4ec5\u4e3a29.8%\u3002", "conclusion": "BitVLA\u5c55\u793a\u4e86\u5728\u5185\u5b58\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.07631", "pdf": "https://arxiv.org/pdf/2506.07631", "abs": "https://arxiv.org/abs/2506.07631", "authors": ["Brian Gordon", "Yonatan Bitton", "Andreea Marzoca", "Yasumasa Onoe", "Xiao Wang", "Daniel Cohen-Or", "Idan Szpektor"], "title": "Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) now generate highly detailed,\nparagraphlength image captions, yet evaluating their factual accuracy remains\nchallenging. Current methods often miss fine-grained errors, being designed for\nshorter texts or lacking datasets with verified inaccuracies. We introduce\nDOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100\nimages, 14 VLMs) featuring over 10,216 sentence-level human annotations of\nfactual correctness and explanatory rationales for errors, all within paragraph\ncontext. Building on this, we develop VNLI-Critique, a model for automated\nsentence-level factuality classification and critique generation. We highlight\nthree key applications: (1) VNLI-Critique demonstrates robust generalization,\nvalidated by state-of-the-art performance on the M-HalDetect benchmark and\nstrong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven\nAutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent\nalignment with human factuality judgments (e.g., 0.98 Spearman). (3) An\ninnovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide\nLLM-based corrections, achieves substantial improvements in caption factuality\n(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark\nalongside practical tools, designed to significantly elevate the standards for\nfine-grained evaluation and foster the improvement of VLM image understanding.\nProject page: https://google.github.io/unblocking-detail-caption", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DOCCI-Critique\u57fa\u51c6\u548cVNLI-Critique\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u6bb5\u843d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30VLM\u751f\u6210\u6bb5\u843d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u9519\u8bef\u68c0\u6d4b\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efaDOCCI-Critique\u57fa\u51c6\uff0c\u5305\u542b1,400\u4e2aVLM\u751f\u6210\u7684\u6bb5\u843d\u6807\u6ce8\uff0c\u5e76\u5f00\u53d1VNLI-Critique\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u5316\u5206\u7c7b\u548c\u9519\u8bef\u5206\u6790\u3002", "result": "VNLI-Critique\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cAutoRater\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0cCritic-and-Revise\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u51c6\u548c\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86VLM\u56fe\u50cf\u7406\u89e3\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2506.07657", "pdf": "https://arxiv.org/pdf/2506.07657", "abs": "https://arxiv.org/abs/2506.07657", "authors": ["Zeyu Xiao", "Zhenyi Wu", "Mingyang Sun", "Qipeng Yan", "Yufan Guo", "Zhuoer Liang", "Lihua Zhang"], "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting has achieved remarkable success in reconstructing both\nstatic and dynamic 3D scenes. However, in a scene represented by 3D Gaussian\nprimitives, interactions between objects suffer from inaccurate 3D\nsegmentation, imprecise deformation among different materials, and severe\nrendering artifacts. To address these challenges, we introduce PIG:\nPhysically-Based Multi-Material Interaction with 3D Gaussians, a novel approach\nthat combines 3D object segmentation with the simulation of interacting objects\nin high precision. Firstly, our method facilitates fast and accurate mapping\nfrom 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.\nSecondly, we assign unique physical properties to correspondingly segmented\nobjects within the scene for multi-material coupled interactions. Finally, we\nhave successfully embedded constraint scales into deformation gradients,\nspecifically clamping the scaling and rotation properties of the Gaussian\nprimitives to eliminate artifacts and achieve geometric fidelity and visual\nconsistency. Experimental results demonstrate that our method not only\noutperforms the state-of-the-art (SOTA) in terms of visual quality, but also\nopens up new directions and pipelines for the field of physically realistic\nscene generation.", "AI": {"tldr": "PIG\u65b9\u6cd5\u901a\u8fc7\u7ed3\u54083D\u5bf9\u8c61\u5206\u5272\u4e0e\u9ad8\u7cbe\u5ea6\u4ea4\u4e92\u6a21\u62df\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u5206\u5272\u4e0d\u51c6\u786e\u3001\u53d8\u5f62\u4e0d\u7cbe\u786e\u548c\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u573a\u666f\u4e2d\u5bf9\u8c61\u4ea4\u4e92\u76843D\u5206\u5272\u4e0d\u51c6\u786e\u3001\u53d8\u5f62\u4e0d\u7cbe\u786e\u548c\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\u3002", "method": "1. \u5feb\u901f\u51c6\u786e\u5730\u5c062D\u50cf\u7d20\u6620\u5c04\u52303D\u9ad8\u65af\uff1b2. \u4e3a\u5206\u5272\u5bf9\u8c61\u5206\u914d\u7269\u7406\u5c5e\u6027\u4ee5\u5b9e\u73b0\u591a\u6750\u6599\u4ea4\u4e92\uff1b3. \u5728\u53d8\u5f62\u68af\u5ea6\u4e2d\u5d4c\u5165\u7ea6\u675f\u5c3a\u5ea6\u4ee5\u6d88\u9664\u4f2a\u5f71\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPIG\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e3a\u7269\u7406\u771f\u5b9e\u573a\u666f\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "PIG\u65b9\u6cd5\u57283D\u9ad8\u65af\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5bf9\u8c61\u4ea4\u4e92\u548c\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u63a8\u52a8\u4e86\u7269\u7406\u771f\u5b9e\u573a\u666f\u751f\u6210\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.07709", "pdf": "https://arxiv.org/pdf/2506.07709", "abs": "https://arxiv.org/abs/2506.07709", "authors": ["Xihua Sheng", "Peilin Chen", "Meng Wang", "Li Zhang", "Shiqi Wang", "Dapeng Oliver Wu"], "title": "Fine-Grained Motion Compression and Selective Temporal Fusion for Neural B-Frame Video Coding", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "With the remarkable progress in neural P-frame video coding, neural B-frame\ncoding has recently emerged as a critical research direction. However, most\nexisting neural B-frame codecs directly adopt P-frame coding tools without\nadequately addressing the unique challenges of B-frame compression, leading to\nsuboptimal performance. To bridge this gap, we propose novel enhancements for\nmotion compression and temporal fusion for neural B-frame coding. First, we\ndesign a fine-grained motion compression method. This method incorporates an\ninteractive dual-branch motion auto-encoder with per-branch adaptive\nquantization steps, which enables fine-grained compression of bi-directional\nmotion vectors while accommodating their asymmetric bitrate allocation and\nreconstruction quality requirements. Furthermore, this method involves an\ninteractive motion entropy model that exploits correlations between\nbi-directional motion latent representations by interactively leveraging\npartitioned latent segments as directional priors. Second, we propose a\nselective temporal fusion method that predicts bi-directional fusion weights to\nachieve discriminative utilization of bi-directional multi-scale temporal\ncontexts with varying qualities. Additionally, this method introduces a\nhyperprior-based implicit alignment mechanism for contextual entropy modeling.\nBy treating the hyperprior as a surrogate for the contextual latent\nrepresentation, this mechanism implicitly mitigates the misalignment in the\nfused bi-directional temporal priors. Extensive experiments demonstrate that\nour proposed codec outperforms state-of-the-art neural B-frame codecs and\nachieves comparable or even superior compression performance to the H.266/VVC\nreference software under random-access configurations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u795e\u7ecfB\u5e27\u7f16\u7801\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u5305\u62ec\u7cbe\u7ec6\u8fd0\u52a8\u538b\u7f29\u548c\u9009\u62e9\u6027\u65f6\u95f4\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u795e\u7ecfB\u5e27\u7f16\u89e3\u7801\u5668\u76f4\u63a5\u91c7\u7528P\u5e27\u5de5\u5177\uff0c\u672a\u80fd\u89e3\u51b3B\u5e27\u538b\u7f29\u7684\u72ec\u7279\u6311\u6218\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86\u7cbe\u7ec6\u8fd0\u52a8\u538b\u7f29\u65b9\u6cd5\uff08\u4ea4\u4e92\u5f0f\u53cc\u5206\u652f\u8fd0\u52a8\u81ea\u7f16\u7801\u5668\uff09\u548c\u9009\u62e9\u6027\u65f6\u95f4\u878d\u5408\u65b9\u6cd5\uff08\u9884\u6d4b\u53cc\u5411\u878d\u5408\u6743\u91cd\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u795e\u7ecfB\u5e27\u7f16\u89e3\u7801\u5668\uff0c\u6027\u80fd\u63a5\u8fd1H.266/VVC\u53c2\u8003\u8f6f\u4ef6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86B\u5e27\u538b\u7f29\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.07735", "pdf": "https://arxiv.org/pdf/2506.07735", "abs": "https://arxiv.org/abs/2506.07735", "authors": ["Haizhao Jing", "Haokui Zhang", "Zhenhao Shang", "Rong Xiao", "Peng Wang", "Yanning Zhang"], "title": "Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": "9 pages, 3 figures", "summary": "Neural Architecture Representation Learning aims to transform network models\ninto feature representations for predicting network attributes, playing a\ncrucial role in deploying and designing networks for real-world applications.\nRecently, inspired by the success of transformers, transformer-based models\nintegrated with Graph Neural Networks (GNNs) have achieved significant progress\nin representation learning. However, current methods still have some\nlimitations. First, existing methods overlook hardware attribute information,\nwhich conflicts with the current trend of diversified deep learning hardware\nand limits the practical applicability of models. Second, current encoding\napproaches rely on static adjacency matrices to represent topological\nstructures, failing to capture the structural differences between computational\nnodes, which ultimately compromises encoding effectiveness. In this paper, we\nintroduce LeDG-Former, an innovative framework that addresses these limitations\nthrough the synergistic integration of language-based semantic embedding and\ndynamic graph representation learning. Specifically, inspired by large language\nmodels (LLMs), we propose a language embedding framework where both neural\narchitectures and hardware platform specifications are projected into a unified\nsemantic space through tokenization and LLM processing, enabling zero-shot\nprediction across different hardware platforms for the first time. Then, we\npropose a dynamic graph-based transformer for modeling neural architectures,\nresulting in improved neural architecture modeling performance. On the NNLQP\nbenchmark, LeDG-Former surpasses previous methods, establishing a new SOTA\nwhile demonstrating the first successful cross-hardware latency prediction\ncapability. Furthermore, our framework achieves superior performance on the\ncell-structured NAS-Bench-101 and NAS-Bench-201 datasets.", "AI": {"tldr": "LeDG-Former\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u8a00\u5d4c\u5165\u548c\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u786c\u4ef6\u5c5e\u6027\u548c\u9759\u6001\u62d3\u6251\u7ed3\u6784\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u8de8\u786c\u4ef6\u5e73\u53f0\u7684\u96f6\u6837\u672c\u9884\u6d4b\u548c\u66f4\u4f18\u7684\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8868\u793a\u5b66\u4e60\u4e2d\u5ffd\u7565\u4e86\u786c\u4ef6\u5c5e\u6027\u4fe1\u606f\uff0c\u4e14\u4f9d\u8d56\u9759\u6001\u90bb\u63a5\u77e9\u9635\u8868\u793a\u62d3\u6251\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u7f16\u7801\u6548\u679c\u3002", "method": "LeDG-Former\u901a\u8fc7\u8bed\u8a00\u5d4c\u5165\u6846\u67b6\u5c06\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u786c\u4ef6\u5e73\u53f0\u89c4\u8303\u6295\u5f71\u5230\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u56fe\u53d8\u6362\u5668\u5efa\u6a21\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5728NNLQP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLeDG-Former\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8de8\u786c\u4ef6\u5ef6\u8fdf\u9884\u6d4b\uff0c\u5e76\u5728NAS-Bench-101\u548cNAS-Bench-201\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LeDG-Former\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u5d4c\u5165\u548c\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8868\u793a\u5b66\u4e60\u7684\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2506.07806", "pdf": "https://arxiv.org/pdf/2506.07806", "abs": "https://arxiv.org/abs/2506.07806", "authors": ["Avinash Kori", "Francesca Toni", "Ben Glocker"], "title": "Identifiable Object Representations under Spatial Ambiguities", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Modular object-centric representations are essential for *human-like\nreasoning* but are challenging to obtain under spatial ambiguities, *e.g. due\nto occlusions and view ambiguities*. However, addressing challenges presents\nboth theoretical and practical difficulties. We introduce a novel multi-view\nprobabilistic approach that aggregates view-specific slots to capture\n*invariant content* information while simultaneously learning disentangled\nglobal *viewpoint-level* information. Unlike prior single-view methods, our\napproach resolves spatial ambiguities, provides theoretical guarantees for\nidentifiability, and requires *no viewpoint annotations*. Extensive experiments\non standard benchmarks and novel complex datasets validate our method's\nrobustness and scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u89c6\u89d2\u6982\u7387\u65b9\u6cd5\uff0c\u89e3\u51b3\u7a7a\u95f4\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u65e0\u9700\u89c6\u89d2\u6807\u6ce8\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u548c\u6269\u5c55\u6027\u3002", "motivation": "\u6a21\u5757\u5316\u7269\u4f53\u4e2d\u5fc3\u8868\u793a\u5bf9\u4eba\u7c7b\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u7a7a\u95f4\u6a21\u7cca\u6027\uff08\u5982\u906e\u6321\u548c\u89c6\u89d2\u6a21\u7cca\uff09\u4e0b\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u5f15\u5165\u591a\u89c6\u89d2\u6982\u7387\u65b9\u6cd5\uff0c\u805a\u5408\u89c6\u89d2\u7279\u5b9a\u69fd\u4ee5\u6355\u6349\u4e0d\u53d8\u5185\u5bb9\u4fe1\u606f\uff0c\u540c\u65f6\u5b66\u4e60\u89e3\u8026\u7684\u5168\u5c40\u89c6\u89d2\u4fe1\u606f\u3002", "result": "\u89e3\u51b3\u4e86\u7a7a\u95f4\u6a21\u7cca\u6027\uff0c\u63d0\u4f9b\u53ef\u8bc6\u522b\u6027\u7406\u8bba\u4fdd\u8bc1\uff0c\u65e0\u9700\u89c6\u89d2\u6807\u6ce8\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u548c\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u6a21\u5757\u5316\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.07883", "pdf": "https://arxiv.org/pdf/2506.07883", "abs": "https://arxiv.org/abs/2506.07883", "authors": ["Rajat Rasal", "Avinash Kori", "Fabio De Sousa Ribeiro", "Tian Xia", "Ben Glocker"], "title": "Diffusion Counterfactual Generation with Semantic Abduction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  Vancouver, Canada", "summary": "Counterfactual image generation presents significant challenges, including\npreserving identity, maintaining perceptual quality, and ensuring faithfulness\nto an underlying causal model. While existing auto-encoding frameworks admit\nsemantic latent spaces which can be manipulated for causal control, they\nstruggle with scalability and fidelity. Advancements in diffusion models\npresent opportunities for improving counterfactual image editing, having\ndemonstrated state-of-the-art visual quality, human-aligned perception and\nrepresentation learning capabilities. Here, we present a suite of\ndiffusion-based causal mechanisms, introducing the notions of spatial, semantic\nand dynamic abduction. We propose a general framework that integrates semantic\nrepresentations into diffusion models through the lens of Pearlian causality to\nedit images via a counterfactual reasoning process. To our knowledge, this is\nthe first work to consider high-level semantic identity preservation for\ndiffusion counterfactuals and to demonstrate how semantic control enables\nprincipled trade-offs between faithful causal control and identity\npreservation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56e0\u679c\u673a\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u7559\u3001\u611f\u77e5\u8d28\u91cf\u548c\u56e0\u679c\u5fe0\u5b9e\u6027\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u81ea\u7f16\u7801\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u611f\u77e5\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u5229\u7528\u6269\u6563\u6a21\u578b\u6539\u8fdb\u53cd\u4e8b\u5b9e\u56fe\u50cf\u7f16\u8f91\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957\u6269\u6563\u6a21\u578b\u7684\u56e0\u679c\u673a\u5236\uff0c\u5305\u62ec\u7a7a\u95f4\u3001\u8bed\u4e49\u548c\u52a8\u6001\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u5e76\u7ed3\u5408Pearl\u56e0\u679c\u7406\u8bba\u5c06\u8bed\u4e49\u8868\u793a\u96c6\u6210\u5230\u6269\u6563\u6a21\u578b\u4e2d\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u9ad8\u7ea7\u8bed\u4e49\u8eab\u4efd\u4fdd\u7559\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5728\u5fe0\u5b9e\u56e0\u679c\u63a7\u5236\u548c\u8eab\u4efd\u4fdd\u7559\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\u4e0e\u56e0\u679c\u63a8\u7406\u7684\u4e25\u8c28\u6027\u3002"}}
{"id": "2506.07897", "pdf": "https://arxiv.org/pdf/2506.07897", "abs": "https://arxiv.org/abs/2506.07897", "authors": ["Shuja Khalid", "Mohamed Ibrahim", "Yang Liu"], "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We present a novel approach for enhancing the resolution and geometric\nfidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.\nCurrent 3DGS methods are fundamentally limited by their input resolution,\nproducing reconstructions that cannot extrapolate finer details than are\npresent in the training views. Our work breaks this limitation through a\nlightweight generative model that predicts and refines additional 3D Gaussians\nwhere needed most. The key innovation is our Hessian-assisted sampling\nstrategy, which intelligently identifies regions that are likely to benefit\nfrom densification, ensuring computational efficiency. Unlike computationally\nintensive GANs or diffusion approaches, our method operates in real-time\n(0.015s per inference on a single consumer-grade GPU), making it practical for\ninteractive applications. Comprehensive experiments demonstrate significant\nimprovements in both geometric accuracy and rendering quality compared to\nstate-of-the-art methods, establishing a new paradigm for resolution-free 3D\nscene enhancement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7Hessian\u8f85\u52a9\u91c7\u6837\u7b56\u7565\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\u7684\u5206\u8fa8\u7387\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u7a81\u7834\u8f93\u5165\u5206\u8fa8\u7387\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u67093DGS\u65b9\u6cd5\u53d7\u9650\u4e8e\u8f93\u5165\u5206\u8fa8\u7387\uff0c\u65e0\u6cd5\u751f\u6210\u6bd4\u8bad\u7ec3\u89c6\u56fe\u66f4\u7cbe\u7ec6\u7684\u7ec6\u8282\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u751f\u6210\u6a21\u578b\u9884\u6d4b\u548c\u7ec6\u5316\u989d\u5916\u76843D\u9ad8\u65af\u5206\u5e03\uff0c\u7ed3\u5408Hessian\u8f85\u52a9\u91c7\u6837\u7b56\u7565\u667a\u80fd\u9009\u62e9\u9700\u8981\u5bc6\u96c6\u5316\u7684\u533a\u57df\u3002", "result": "\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5b9e\u65f6\u6027\u9ad8\uff08\u5355GPU\u4e0a\u6bcf\u6b21\u63a8\u74060.015\u79d2\uff09\u3002", "conclusion": "\u4e3a\u5206\u8fa8\u7387\u65e0\u5173\u76843D\u573a\u666f\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.07903", "pdf": "https://arxiv.org/pdf/2506.07903", "abs": "https://arxiv.org/abs/2506.07903", "authors": ["Kevin Rojas", "Yuchen Zhu", "Sichen Zhu", "Felix X. -F. Ye", "Molei Tao"], "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted to ICML 2025. Code available at\n  https://github.com/KevinRojas1499/Diffuse-Everything", "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u652f\u6301\u539f\u751f\u751f\u6210\u8de8\u6a21\u6001\u7684\u8026\u5408\u6570\u636e\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u9884\u5904\u7406\u534f\u8bae\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u9884\u5904\u7406\u534f\u8bae\uff08\u5982\u5206\u8bcd\u5668\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff09\u6765\u7edf\u4e00\u591a\u6a21\u6001\u6570\u636e\u8868\u793a\uff0c\u8fd9\u5bf9\u6570\u636e\u6709\u9650\u7684\u5e94\u7528\u5b58\u5728\u95ee\u9898\u3002", "method": "\u5f15\u5165\u89e3\u8026\u7684\u566a\u58f0\u8c03\u5ea6\u7b56\u7565\uff0c\u652f\u6301\u65e0\u6761\u4ef6\u751f\u6210\u548c\u6a21\u6001\u6761\u4ef6\u751f\u6210\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u72b6\u6001\u7a7a\u95f4\u7684\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u5728\u6587\u672c-\u56fe\u50cf\u751f\u6210\u548c\u6df7\u5408\u7c7b\u578b\u8868\u683c\u6570\u636e\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07917", "pdf": "https://arxiv.org/pdf/2506.07917", "abs": "https://arxiv.org/abs/2506.07917", "authors": ["Allen Tu", "Haiyang Ying", "Alex Hanson", "Yonghan Lee", "Tom Goldstein", "Matthias Zwicker"], "title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes", "categories": ["cs.GR", "cs.CV"], "comment": "Project Page: https://speede3dgs.github.io/", "summary": "Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve\nhigh-quality novel view synthesis by using neural networks to predict the\ntime-varying deformation of each Gaussian. However, performing per-Gaussian\nneural inference at every frame poses a significant bottleneck, limiting\nrendering speed and increasing memory and compute requirements. In this paper,\nwe present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general\npipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS\nrepresentations by reducing neural inference through two complementary\ntechniques. First, we propose a temporal sensitivity pruning score that\nidentifies and removes Gaussians with low contribution to the dynamic scene\nreconstruction. We also introduce an annealing smooth pruning mechanism that\nimproves pruning robustness in real-world scenes with imprecise camera poses.\nSecond, we propose GroupFlow, a motion analysis technique that clusters\nGaussians by trajectory similarity and predicts a single rigid transformation\nper group instead of separate deformations for each Gaussian. Together, our\ntechniques accelerate rendering by $10.37\\times$, reduce model size by\n$7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset.\nSpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on\nthe D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be\nintegrated into any deformable 3DGS or 4DGS framework.", "AI": {"tldr": "SpeeDe3DGS\u901a\u8fc7\u65f6\u95f4\u654f\u611f\u526a\u679d\u548cGroupFlow\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u7684\u6e32\u67d3\u901f\u5ea6\uff0c\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\uff0c\u6bcf\u5e27\u5bf9\u6bcf\u4e2a\u9ad8\u65af\u8fdb\u884c\u795e\u7ecf\u63a8\u65ad\u5bfc\u81f4\u6e32\u67d3\u901f\u5ea6\u6162\u3001\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u4e9f\u9700\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u654f\u611f\u526a\u679d\u5206\u6570\u548c\u9000\u706b\u5e73\u6ed1\u526a\u679d\u673a\u5236\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8f68\u8ff9\u76f8\u4f3c\u6027\u7684GroupFlow\u8fd0\u52a8\u5206\u6790\u6280\u672f\u3002", "result": "\u5728NeRF-DS\u6570\u636e\u96c6\u4e0a\uff0c\u6e32\u67d3\u901f\u5ea6\u63d0\u534710.37\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c117.71\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed2.71\u500d\u3002", "conclusion": "SpeeDe3DGS\u662f\u4e00\u79cd\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u53ef\u96c6\u6210\u5230\u4efb\u4f55\u53ef\u53d8\u5f623DGS\u62164DGS\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2506.07932", "pdf": "https://arxiv.org/pdf/2506.07932", "abs": "https://arxiv.org/abs/2506.07932", "authors": ["Rishit Dagli", "Yushi Guan", "Sankeerth Durvasula", "Mohammadreza Mofayezi", "Nandita Vijaykumar"], "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.", "AI": {"tldr": "Squeeze3D\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u76843D\u751f\u6210\u6a21\u578b\u7684\u9690\u5f0f\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u6781\u9ad8\u538b\u7f29\u6bd4\u76843D\u6570\u636e\u538b\u7f29\u3002", "motivation": "\u73b0\u6709\u76843D\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u4e14\u538b\u7f29\u6bd4\u6709\u9650\u3002Squeeze3D\u65e8\u5728\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9690\u5f0f\u77e5\u8bc6\uff0c\u5b9e\u73b0\u66f4\u9ad8\u538b\u7f29\u6bd4\u4e14\u65e0\u9700\u771f\u5b9e\u6570\u636e\u3002", "method": "\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u6620\u5c04\u7f51\u7edc\uff0c\u5c06\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u4e0e\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u8fde\u63a5\uff0c\u5c063D\u6570\u636e\u538b\u7f29\u4e3a\u7d27\u51d1\u7684\u6f5c\u5728\u4ee3\u7801\uff0c\u518d\u901a\u8fc7\u751f\u6210\u6a21\u578b\u89e3\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSqueeze3D\u5bf9\u7eb9\u7406\u7f51\u683c\u3001\u70b9\u4e91\u548c\u8f90\u5c04\u573a\u7684\u538b\u7f29\u6bd4\u5206\u522b\u8fbe\u52302187x\u300155x\u548c619x\uff0c\u4e14\u89c6\u89c9\u8d28\u91cf\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "Squeeze3D\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u76843D\u6570\u636e\u538b\u7f29\u65b9\u6cd5\uff0c\u65e0\u9700\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u79cd3D\u683c\u5f0f\u3002"}}
{"id": "2506.07963", "pdf": "https://arxiv.org/pdf/2506.07963", "abs": "https://arxiv.org/abs/2506.07963", "authors": ["Jixiang Hong", "Yiran Zhang", "Guanzhong Wang", "Yi Liu", "Ji-Rong Wen", "Rui Yan"], "title": "Reinforcing Multimodal Understanding and Generation with Dual Self-rewards", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate image-text alignment, prone to\ngenerating text responses contradicting the visual input or failing to follow\nthe text-to-image prompts. Current solutions require external supervision\n(e.g., human feedback or reward models) and only address unidirectional\ntasks-either understanding or generation. In this work, based on the\nobservation that understanding and generation are inverse dual tasks, we\nintroduce a self-supervised dual reward mechanism to reinforce the\nunderstanding and generation capabilities of LMMs. Specifically, we sample\nmultiple outputs for a given input in one task domain, then reverse the\ninput-output pairs to compute the dual likelihood of the model as self-rewards\nfor optimization. Extensive experimental results on visual understanding and\ngeneration benchmarks demonstrate that our method can effectively enhance the\nperformance of the model without any external supervision, especially achieving\nremarkable improvements in text-to-image tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u53cc\u5956\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u7406\u89e3\u4e0e\u751f\u6210\u7684\u9006\u5bf9\u5076\u4efb\u52a1\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\uff0c\u4ec5\u89e3\u51b3\u5355\u5411\u4efb\u52a1\u3002", "method": "\u5f15\u5165\u81ea\u76d1\u7763\u53cc\u5956\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u9006\u5bf9\u5076\u4efb\u52a1\u8ba1\u7b97\u6a21\u578b\u7684\u53cc\u91cd\u4f3c\u7136\u4f5c\u4e3a\u81ea\u6211\u5956\u52b1\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u81ea\u76d1\u7763\u53cc\u5956\u52b1\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002"}}
{"id": "2506.07998", "pdf": "https://arxiv.org/pdf/2506.07998", "abs": "https://arxiv.org/abs/2506.07998", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "title": "Generative Modeling of Weights: Generalization or Memorization?", "categories": ["cs.LG", "cs.CV"], "comment": "Project page at https://boyazeng.github.io/weight_memorization", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "AI": {"tldr": "\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7528\u4e8e\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u8bb0\u5fc6\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u65e0\u6cd5\u751f\u6210\u65b0\u9896\u4e14\u9ad8\u6027\u80fd\u7684\u6743\u91cd\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8bc4\u4f30\u5176\u662f\u5426\u80fd\u751f\u6210\u4e0d\u540c\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u65b0\u6743\u91cd\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u751f\u6210\u6743\u91cd\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u7b80\u5355\u57fa\u7ebf\uff08\u5982\u6dfb\u52a0\u566a\u58f0\u6216\u6743\u91cd\u96c6\u6210\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u8bb0\u5fc6\u8bad\u7ec3\u68c0\u67e5\u70b9\u751f\u6210\u6743\u91cd\uff0c\u65e0\u6cd5\u8d85\u8d8a\u7b80\u5355\u57fa\u7ebf\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u5e38\u89c1\u7f13\u89e3\u8bb0\u5fc6\u7684\u65b9\u6cd5\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u65b0\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5bf9\u751f\u6210\u6a21\u578b\u66f4\u8c28\u614e\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2506.08012", "pdf": "https://arxiv.org/pdf/2506.08012", "abs": "https://arxiv.org/abs/2506.08012", "authors": ["Penghao Wu", "Shengnan Ma", "Bo Wang", "Jiaheng Yu", "Lewei Lu", "Ziwei Liu"], "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior", "categories": ["cs.AI", "cs.CV"], "comment": "Project Page at https://penghao-wu.github.io/GUI_Reflection/", "summary": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.", "AI": {"tldr": "GUI-Reflection\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u589e\u5f3a\u591a\u6a21\u6001GUI\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684GUI\u81ea\u52a8\u5316\u3002", "motivation": "\u73b0\u6709GUI\u6a21\u578b\u4f9d\u8d56\u65e0\u9519\u8bef\u7684\u79bb\u7ebf\u8f68\u8ff9\uff0c\u7f3a\u4e4f\u53cd\u601d\u548c\u9519\u8bef\u6062\u590d\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faGUI-Reflection\u6846\u67b6\uff0c\u5305\u62ecGUI\u7279\u5b9a\u9884\u8bad\u7ec3\u3001\u79bb\u7ebf\u76d1\u7763\u5fae\u8c03\u548c\u5728\u7ebf\u53cd\u601d\u8c03\u4f18\u4e09\u4e2a\u9636\u6bb5\uff0c\u81ea\u52a8\u751f\u6210\u53cd\u601d\u6570\u636e\u5e76\u8bbe\u8ba1\u4efb\u52a1\u5957\u4ef6\u3002", "result": "\u6846\u67b6\u8d4b\u4e88GUI\u4ee3\u7406\u81ea\u6211\u53cd\u601d\u548c\u7ea0\u6b63\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "GUI-Reflection\u4e3a\u66f4\u667a\u80fd\u7684GUI\u81ea\u52a8\u5316\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u76f8\u5173\u6570\u636e\u548c\u5de5\u5177\u5c06\u516c\u5f00\u3002"}}
