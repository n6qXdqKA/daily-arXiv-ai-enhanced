{"id": "2602.07006", "pdf": "https://arxiv.org/pdf/2602.07006", "abs": "https://arxiv.org/abs/2602.07006", "authors": ["Alokesh Manna", "Neil Spencer", "Dipak K. Dey"], "title": "Scalable spatial point process models for forensic footwear analysis", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u6765\u91cf\u5316\u978b\u5370\u4e2d\"\u610f\u5916\u7279\u5f81\"\u7684\u7a00\u6709\u6027\uff0c\u901a\u8fc7\u6f5c\u5728\u9ad8\u65af\u6a21\u578b\u548c\u7a7a\u95f4\u53d8\u5316\u7cfb\u6570\u6539\u8fdb\u6cd5\u533b\u978b\u5370\u5206\u6790\u3002", "motivation": "\u5728\u6cd5\u533b\u8c03\u67e5\u4e2d\uff0c\u4ec5\u5339\u914d\u978b\u5b50\u7684\u54c1\u724c\u548c\u578b\u53f7\u4e0d\u8db3\u4ee5\u786e\u5b9a\u5acc\u7591\u4eba\u7684\u978b\u5b50\uff0c\u56e0\u4e3a\u540c\u4e00\u578b\u53f7\u6709\u6570\u5343\u53cc\u3002\u9700\u8981\u5206\u6790\u978b\u5e95\u4e0a\u7684\"\u610f\u5916\u7279\u5f81\"\uff08\u5982\u5212\u75d5\u3001\u78e8\u635f\uff09\u7684\u7a00\u6709\u6027\u6765\u91cf\u5316\u8bc1\u636e\u5f3a\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u91c7\u7528\u6f5c\u5728\u9ad8\u65af\u6a21\u578b\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u901a\u8fc7\u96c6\u6210\u5d4c\u5957\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6807\u6ce8\u978b\u5370\u6570\u636e\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u53d8\u5316\u7cfb\u6570\u6765\u5efa\u6a21\u978b\u5e95\u82b1\u7eb9\u4e0e\u610f\u5916\u7279\u5f81\u4f4d\u7f6e\u7684\u5173\u7cfb\u3002", "result": "\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u6cd5\u533b\u978b\u5370\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cf\u5316\u610f\u5916\u7279\u5f81\u6a21\u5f0f\u7684\u7a00\u6709\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6cd5\u533b\u978b\u5370\u8bc1\u636e\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u4e3a\u8c03\u67e5\u4eba\u5458\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2602.07008", "pdf": "https://arxiv.org/pdf/2602.07008", "abs": "https://arxiv.org/abs/2602.07008", "authors": ["Ruoyu Chen", "Shangquan Sun", "Xiaoqing Guo", "Sanyi Zhang", "Kangwei Liu", "Shiming Liu", "Zhangcheng Wang", "Qunli Zhang", "Hua Zhang", "Xiaochun Cao"], "title": "Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f52\u56e0\u7684\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u6a21\u578b\u5bf9\u975e\u5148\u9a8c\u8bc1\u636e\u7684\u4f9d\u8d56\uff0c\u5f15\u5bfc\u6a21\u578b\u51b3\u7b56\u4f9d\u636e\u4e0e\u4eba\u7c7b\u671f\u671b\u7684\u8bc1\u636e\u533a\u57df\u5bf9\u9f50\uff0c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u548c\u51b3\u7b56\u5408\u7406\u6027\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u4ec5\u63d0\u4f9b\u7c7b\u522b\u6807\u7b7e\uff0c\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u6377\u5f84\u76f8\u5173\u6027\u800c\u975e\u9884\u671f\u8bc1\u636e\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u3002\u4eba\u7c7b\u5148\u9a8c\u53ef\u4ee5\u5e2e\u52a9\u7ea6\u675f\u8fd9\u79cd\u884c\u4e3a\uff0c\u4f46\u6a21\u578b\u5b66\u4e60\u5230\u7684\u8868\u793a\u5e38\u4e0e\u4eba\u7c7b\u611f\u77e5\u5b58\u5728\u5dee\u5f02\uff0c\u5bf9\u9f50\u6a21\u578b\u4e0e\u4eba\u7c7b\u5148\u9a8c\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5c06\u4eba\u7c7b\u5148\u9a8c\u7f16\u7801\u4e3a\u6a21\u578b\u5e94\u4f9d\u8d56\u7684\u8f93\u5165\u533a\u57df\uff08\u5982\u8fb9\u754c\u6846\uff09\uff0c\u5229\u7528\u9ad8\u4fdd\u771f\u5ea6\u7684\u57fa\u4e8e\u5b50\u96c6\u9009\u62e9\u7684\u5f52\u56e0\u65b9\u6cd5\u5728\u8bad\u7ec3\u4e2d\u66b4\u9732\u6a21\u578b\u51b3\u7b56\u8bc1\u636e\u3002\u5f53\u5f52\u56e0\u533a\u57df\u663e\u8457\u504f\u79bb\u5148\u9a8c\u533a\u57df\u65f6\uff0c\u60e9\u7f5a\u5bf9\u975e\u5148\u9a8c\u8bc1\u636e\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u8bad\u7ec3\u76ee\u6807\u65bd\u52a0\u7531\u4eba\u7c7b\u5148\u9a8c\u8bf1\u5bfc\u7684\u5f52\u56e0\u7ea6\u675f\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u548cMLLM-based GUI\u4ee3\u7406\u6a21\u578b\u7684\u70b9\u51fb\u51b3\u7b56\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002\u5728\u4f20\u7edf\u5206\u7c7b\u548c\u81ea\u56de\u5f52\u751f\u6210\u8bbe\u7f6e\u4e2d\uff0c\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u4e00\u81f4\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u51b3\u7b56\u7684\u5408\u7406\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5f52\u56e0\u7684\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\u80fd\u6709\u6548\u5f15\u5bfc\u6a21\u578b\u51b3\u7b56\u4f9d\u636e\u4e0e\u4eba\u7c7b\u671f\u671b\u7684\u8bc1\u636e\u533a\u57df\u5bf9\u9f50\uff0c\u4e0d\u4ec5\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u8fd8\u589e\u5f3a\u51b3\u7b56\u5408\u7406\u6027\uff0c\u4e3a\u89e3\u51b3\u6a21\u578b\u4f9d\u8d56\u6377\u5f84\u76f8\u5173\u6027\u800c\u975e\u9884\u671f\u8bc1\u636e\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.07011", "pdf": "https://arxiv.org/pdf/2602.07011", "abs": "https://arxiv.org/abs/2602.07011", "authors": ["Zhuonan Wang", "Zhenxuan Fan", "Siwen Tan", "Yu Zhong", "Yuqian Yuan", "Haoyuan Li", "Hao Jiang", "Wenqiao Zhang", "Feifei Shao", "Hongwei Wang", "Jun Xiao"], "title": "MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "9 pages, 5 figures", "summary": "As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.", "AI": {"tldr": "\u63d0\u51fa\u4e86MAU-Set\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\u6570\u636e\u96c6\u548cMAU-GPT\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u901a\u8fc7AMoE-LoRA\u673a\u5236\u7edf\u4e00\u5f02\u5e38\u611f\u77e5\u548c\u901a\u7528\u4e13\u5bb6\u9002\u5e94\uff0c\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5de5\u4e1a\u5236\u9020\u89c4\u6a21\u5316\u9700\u8981\u81ea\u52a8\u5316\u7ec6\u7c92\u5ea6\u4ea7\u54c1\u56fe\u50cf\u5206\u6790\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u8986\u76d6\u4e0d\u8db3\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u96be\u4ee5\u5904\u7406\u591a\u6837\u590d\u6742\u7684\u5f02\u5e38\u6a21\u5f0f\u3002", "method": "1) \u6784\u5efaMAU-Set\u591a\u7c7b\u578b\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u4e2a\u5de5\u4e1a\u9886\u57df\u548c\u5206\u5c42\u4efb\u52a1\u7ed3\u6784\uff1b2) \u63d0\u51faMAU-GPT\u9886\u57df\u9002\u5e94\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u91c7\u7528AMoE-LoRA\u673a\u5236\u7edf\u4e00\u5f02\u5e38\u611f\u77e5\u548c\u901a\u7528\u4e13\u5bb6\u9002\u5e94\uff1b3) \u5efa\u7acb\u4e25\u683c\u7684\u8bc4\u4f30\u534f\u8bae\u3002", "result": "MAU-GPT\u5728\u6240\u6709\u9886\u57df\u90fd\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u53ef\u6269\u5c55\u81ea\u52a8\u5316\u5de5\u4e1a\u68c0\u6d4b\u65b9\u9762\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "MAU-Set\u6570\u636e\u96c6\u548cMAU-GPT\u6a21\u578b\u4e3a\u89e3\u51b3\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\u4e2d\u7684\u6570\u636e\u96c6\u8986\u76d6\u548c\u6a21\u578b\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u5de5\u4e1a\u8d28\u91cf\u63a7\u5236\u7684\u8fdb\u6b65\u3002"}}
{"id": "2602.07012", "pdf": "https://arxiv.org/pdf/2602.07012", "abs": "https://arxiv.org/abs/2602.07012", "authors": ["Zhonghua Wang", "Lie Ju", "Sijia Li", "Wei Feng", "Sijin Zhou", "Ming Hu", "Jianhao Xiong", "Xiaoying Tang", "Yifan Peng", "Mingquan Lin", "Yaodong Ding", "Yong Zeng", "Wenbin Wei", "Li Dong", "Zongyuan Ge"], "title": "A General Model for Retinal Segmentation and Quantification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.", "AI": {"tldr": "RetSAM\u662f\u4e00\u4e2a\u901a\u7528\u7684\u89c6\u7f51\u819c\u5206\u5272\u548c\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u572820\u4e07\u5f20\u773c\u5e95\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u80fd\u591f\u5206\u5272\u591a\u79cd\u89e3\u5256\u7ed3\u6784\u548c\u75c5\u53d8\uff0c\u5e76\u63d0\u53d630\u591a\u79cd\u6807\u51c6\u5316\u751f\u7269\u6807\u5fd7\u7269\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u5e76\u652f\u6301\u5927\u89c4\u6a21\u773c\u79d1\u7814\u7a76\u3002", "motivation": "\u89c6\u7f51\u819c\u6210\u50cf\u5feb\u901f\u3001\u65e0\u521b\u4e14\u5e7f\u6cdb\u53ef\u7528\uff0c\u4e3a\u773c\u79d1\u548c\u5168\u8eab\u5065\u5eb7\u8bc4\u4f30\u63d0\u4f9b\u53ef\u91cf\u5316\u7684\u7ed3\u6784\u548c\u8840\u7ba1\u4fe1\u53f7\u3002\u7136\u800c\uff0c\u7531\u4e8e\u516c\u5171\u591a\u6807\u7b7e\u6570\u636e\u96c6\u6709\u9650\u4e14\u7f3a\u4e4f\u7edf\u4e00\u7684\u5206\u5272\u5230\u91cf\u5316\u6d41\u7a0b\uff0c\u5927\u89c4\u6a21\u5206\u6790\u4ecd\u7136\u56f0\u96be\u3002", "method": "\u63d0\u51faRetSAM\u6846\u67b6\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u79c1\u6709\u548c\u516c\u5171\u773c\u5e95\u6570\u636e\uff0c\u652f\u6301\u4e09\u7c7b\u4efb\u52a1\uff1a\u5206\u5272\u4e94\u79cd\u89e3\u5256\u7ed3\u6784\u3001\u56db\u79cd\u89c6\u7f51\u819c\u8868\u578b\u6a21\u5f0f\u548c20\u591a\u79cd\u4e0d\u540c\u75c5\u53d8\u7c7b\u578b\uff0c\u5e76\u5c06\u5206\u5272\u7ed3\u679c\u8f6c\u5316\u4e3a30\u591a\u79cd\u6807\u51c6\u5316\u751f\u7269\u6807\u5fd7\u7269\u3002", "result": "\u572817\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5353\u8d8a\u5206\u5272\u6027\u80fd\uff0c\u5e73\u5747DSC\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad83.9\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u4efb\u52a1\u57fa\u51c6\u4e0a\u63d0\u5347\u9ad8\u8fbe15\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u4e0d\u540c\u4eba\u7fa4\u3001\u6210\u50cf\u8bbe\u5907\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RetSAM\u5c06\u773c\u5e95\u56fe\u50cf\u8f6c\u5316\u4e3a\u6807\u51c6\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5b9a\u91cf\u8868\u578b\uff0c\u652f\u6301\u8de8\u4e3b\u8981\u773c\u79d1\u75be\u75c5\u7684\u7cfb\u7edf\u6027\u76f8\u5173\u6027\u5206\u6790\uff0c\u5305\u62ec\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u3001\u5e74\u9f84\u76f8\u5173\u6027\u9ec4\u6591\u53d8\u6027\u3001\u9752\u5149\u773c\u548c\u75c5\u7406\u6027\u8fd1\u89c6\uff0c\u4ece\u800c\u63a8\u52a8\u5927\u89c4\u6a21\u773c\u79d1\u7814\u7a76\u548c\u8f6c\u5316\u5e94\u7528\u3002"}}
{"id": "2602.07013", "pdf": "https://arxiv.org/pdf/2602.07013", "abs": "https://arxiv.org/abs/2602.07013", "authors": ["Jiaxi Yang", "Shicheng Liu", "Yuchen Yang", "Dongwon Lee"], "title": "Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \\textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \\textbf{C}onfigurable \\textbf{R}efusal in \\textbf{VLM}s (\\textbf{CR-VLM}), a robust and efficient approach for {\\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.", "AI": {"tldr": "CR-VLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08\u5f3a\u5236\u673a\u5236\u63d0\u53d6\u53ef\u914d\u7f6e\u62d2\u7edd\u5411\u91cf\u3001\u95e8\u63a7\u673a\u5236\u9632\u6b62\u8fc7\u5ea6\u62d2\u7edd\u3001\u4ee5\u53ca\u53cd\u4e8b\u5b9e\u89c6\u89c9\u589e\u5f3a\u6a21\u5757\uff0c\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7528\u6237\u81ea\u9002\u5e94\u5b89\u5168\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u62d2\u7edd\u673a\u5236\u5927\u591a\u662f\"\u4e00\u5200\u5207\"\u7684\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7528\u6237\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\u7ea6\u675f\uff0c\u5bfc\u81f4\u8981\u4e48\u62d2\u7edd\u4e0d\u8db3\u8981\u4e48\u8fc7\u5ea6\u62d2\u7edd\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u53ef\u914d\u7f6e\u7684\u62d2\u7edd\u7b56\u7565\u3002", "method": "CR-VLM\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1\uff09\u901a\u8fc7\u6559\u5e08\u5f3a\u5236\u673a\u5236\u63d0\u53d6\u53ef\u914d\u7f6e\u62d2\u7edd\u5411\u91cf\u6765\u589e\u5f3a\u62d2\u7edd\u4fe1\u53f7\uff1b2\uff09\u5f15\u5165\u95e8\u63a7\u673a\u5236\u4fdd\u62a4\u8303\u56f4\u5185\u67e5\u8be2\u7684\u63a5\u53d7\u6027\uff0c\u9632\u6b62\u8fc7\u5ea6\u62d2\u7edd\uff1b3\uff09\u8bbe\u8ba1\u53cd\u4e8b\u5b9e\u89c6\u89c9\u589e\u5f3a\u6a21\u5757\uff0c\u4f7f\u89c6\u89c9\u8868\u793a\u4e0e\u62d2\u7edd\u8981\u6c42\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cCR-VLM\u5b9e\u73b0\u4e86\u6709\u6548\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7528\u6237\u81ea\u9002\u5e94\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002", "conclusion": "CR-VLM\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u62d2\u7edd\u673a\u5236\u7f3a\u4e4f\u9002\u5e94\u6027\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u7528\u6237\u81ea\u9002\u5e94\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07014", "pdf": "https://arxiv.org/pdf/2602.07014", "abs": "https://arxiv.org/abs/2602.07014", "authors": ["Qingyu Wu", "Yuxuan Han", "Haijun Li", "Zhao Xu", "Jianshan Zhao", "Xu Jin", "Longyue Wang", "Weihua Luo"], "title": "Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.", "AI": {"tldr": "Vectra\u662f\u9996\u4e2a\u57fa\u4e8eMLLM\u7684\u3001\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u7684\u7535\u5546\u56fe\u6587\u7ffb\u8bd1\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u591a\u7ef4\u8bc4\u5206\u7cfb\u7edf\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c4B\u53c2\u6570\u6a21\u578b\uff0c\u5728\u4eba\u7c7b\u6392\u540d\u76f8\u5173\u6027\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u7535\u5546\u56fe\u6587\u7ffb\u8bd1\u7814\u7a76\u4e2d\uff0c\u89c6\u89c9\u6e32\u67d3\u8d28\u91cf\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u57fa\u4e8e\u53c2\u8003\u7684\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u65b9\u6cd5\u7f3a\u4e4f\u9886\u57df\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\u3002", "method": "\u63d0\u51faVectra\u6846\u67b6\uff1a1) Vectra Score\u5c06\u89c6\u89c9\u8d28\u91cf\u5206\u89e3\u4e3a14\u4e2a\u53ef\u89e3\u91ca\u7ef4\u5ea6\uff0c\u5f15\u5165\u7a7a\u95f4\u611f\u77e5\u7684\u7f3a\u9677\u9762\u79ef\u6bd4\u91cf\u5316\uff1b2) Vectra Dataset\u4ece110\u4e07\u771f\u5b9e\u4ea7\u54c1\u56fe\u50cf\u6784\u5efa\uff0c\u5305\u542b2K\u57fa\u51c6\u96c6\u300130K\u63a8\u7406\u6807\u6ce8\u548c3.5K\u4e13\u5bb6\u504f\u597d\u6807\u6ce8\uff1b3) Vectra Model\u662f4B\u53c2\u6570\u7684MLLM\uff0c\u80fd\u751f\u6210\u91cf\u5316\u5206\u6570\u548c\u8bca\u65ad\u63a8\u7406\u3002", "result": "Vectra\u5728\u4eba\u7c7b\u6392\u540d\u76f8\u5173\u6027\u4e0a\u8fbe\u5230SOTA\uff0c\u5176\u6a21\u578b\u5728\u8bc4\u5206\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86GPT-5\u548cGemini-3\u7b49\u9886\u5148MLLM\u3002", "conclusion": "Vectra\u586b\u8865\u4e86\u7535\u5546\u56fe\u6587\u7ffb\u8bd1\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u9996\u4e2a\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u3001\u57fa\u4e8eMLLM\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2602.07015", "pdf": "https://arxiv.org/pdf/2602.07015", "abs": "https://arxiv.org/abs/2602.07015", "authors": ["Subreena", "Mohammad Amzad Hossain", "Mirza Raquib", "Saydul Akbar Murad", "Farida Siddiqi Prity", "Muhammad Hanif", "Nick Rahimi"], "title": "Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408CNN\u67b6\u6784\u7528\u4e8e\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u8bc6\u522b\uff0c\u7ed3\u5408MobileNetV3-Large\u548cEfficientNetB0\u7279\u5f81\u63d0\u53d6\u4e0eMLP\u5206\u7c7b\u5668\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bc6\u522b\uff0c\u5e76\u5f15\u5165\u53ef\u89e3\u91caAI\u65b9\u6cd5\u589e\u5f3a\u900f\u660e\u5ea6\u3002", "motivation": "\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u51c6\u786e\u7684\u7eb8\u5e01\u8bc6\u522b\u6280\u672f\uff0c\u51cf\u5c11\u4ed6\u4eec\u5bf9\u4ed6\u4eba\u4f9d\u8d56\u5e26\u6765\u7684\u6b3a\u8bc8\u98ce\u9669\u3002\u5f53\u524d\u8bc6\u522b\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u4e14\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u6784\u5efa\u65b0\u7684\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u6570\u636e\u96c6\uff08\u5305\u542b\u63a7\u5236\u73af\u5883\u548c\u771f\u5b9e\u573a\u666f\uff09\uff1b2) \u6574\u5408\u56db\u4e2a\u989d\u5916\u6570\u636e\u96c6\u589e\u5f3a\u9c81\u68d2\u6027\uff1b3) \u63d0\u51fa\u6df7\u5408CNN\u67b6\u6784\uff08MobileNetV3-Large + EfficientNetB0\uff09\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff1b4) \u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\uff1b5) \u91c7\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u4e03\u79cd\u8bc4\u4f30\u6307\u6807\uff1b6) \u96c6\u6210LIME\u548cSHAP\u7b49\u53ef\u89e3\u91caAI\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u5728\u63a7\u5236\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.95%\u51c6\u786e\u7387\uff0c\u590d\u6742\u80cc\u666f\u4e0a92.84%\uff0c\u6240\u6709\u6570\u636e\u96c6\u7ec4\u5408\u4e0a94.98%\u3002\u901a\u8fc7\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u9a8c\u8bc1\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408CNN\u67b6\u6784\u5728\u7eb8\u5e01\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u987e\u9ad8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002\u6570\u636e\u96c6\u6784\u5efa\u548c\u53ef\u89e3\u91caAI\u65b9\u6cd5\u7684\u96c6\u6210\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u89c6\u969c\u4eba\u58eb\u8f85\u52a9\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07016", "pdf": "https://arxiv.org/pdf/2602.07016", "abs": "https://arxiv.org/abs/2602.07016", "authors": ["Mohsen Mostafa"], "title": "Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, https://www.kaggle.com/code/babydriver1233/optimized-pipeline-for-the-image-matching-challeng, https://www.kaggle.com/code/babydriver1233/integrating-lejepa-for-enhanced-image-matching", "summary": "Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u9ad8\u65af\u7ea6\u675f\u8868\u793a\uff08\u53d7LeJEPA\u542f\u53d1\uff09\u6765\u89e3\u51b3\u65e0\u76d1\u77633D\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u573a\u666f\u3001\u89c6\u89c9\u6a21\u7cca\u7684\u56fe\u50cf\u96c6\u5408\u4e2d\u3002\u901a\u8fc7\u4e09\u4e2a\u9010\u6b65\u6539\u8fdb\u7684\u6d41\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u65af\u7ea6\u675f\u5d4c\u5165\u80fd\u63d0\u5347\u573a\u666f\u5206\u79bb\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4ece\u975e\u7ed3\u6784\u5316\u56fe\u50cf\u96c6\u5408\u4e2d\u8fdb\u884c\u65e0\u76d1\u77633D\u573a\u666f\u91cd\u5efa\u662f\u4e00\u4e2a\u57fa\u7840\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u56fe\u50cf\u6765\u81ea\u591a\u4e2a\u4e0d\u76f8\u5173\u573a\u666f\u4e14\u5305\u542b\u663e\u8457\u89c6\u89c9\u6a21\u7cca\u65f6\u3002IMC2025\u6311\u6218\u8d5b\u51f8\u663e\u4e86\u8fd9\u4e9b\u56f0\u96be\uff0c\u9700\u8981\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\uff08\u5305\u62ec\u5f02\u5e38\u503c\u548c\u6df7\u5408\u5185\u5bb9\uff09\u540c\u65f6\u8fdb\u884c\u573a\u666f\u53d1\u73b0\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u9010\u6b65\u6539\u8fdb\u7684\u6d41\u7a0b\uff0c\u6700\u7ec8\u91c7\u7528\u53d7LeJEPA\u542f\u53d1\u7684\u65b9\u6848\uff0c\u5bf9\u5b66\u4e60\u5230\u7684\u56fe\u50cf\u5d4c\u5165\u65bd\u52a0\u5404\u5411\u540c\u6027\u9ad8\u65af\u7ea6\u675f\u3002\u8be5\u65b9\u6cd5\u4e0d\u662f\u5f15\u5165\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u800c\u662f\u5b9e\u8bc1\u8bc4\u4f30\u8fd9\u4e9b\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u805a\u7c7b\u4e00\u81f4\u6027\u548c\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728IMC2025\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u542f\u53d1\u5f0f\u57fa\u7ebf\u76f8\u6bd4\uff0c\u9ad8\u65af\u7ea6\u675f\u5d4c\u5165\u80fd\u591f\u6539\u5584\u573a\u666f\u5206\u79bb\u548c\u59ff\u6001\u5408\u7406\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u6a21\u7cca\u7684\u8bbe\u7f6e\u4e2d\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u7406\u8bba\u9a71\u52a8\u7684\u8868\u793a\u7ea6\u675f\u4e3a\u6865\u63a5\u81ea\u76d1\u7763\u5b66\u4e60\u539f\u7406\u548c\u5b9e\u9645\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\u6d41\u7a0b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.07017", "pdf": "https://arxiv.org/pdf/2602.07017", "abs": "https://arxiv.org/abs/2602.07017", "authors": ["Thuraya Alzubaidi", "Sana Ammar", "Maryam Alsharqi", "Islem Rekik", "Muzammil Behzad"], "title": "XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\\% reduction in runtime, a 44.6\\% improvement in dice score, and a 96.7\\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.", "AI": {"tldr": "\u63d0\u51faXAI-CLIP\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5b9a\u4f4d\u4e34\u5e8a\u76f8\u5173\u89e3\u5256\u533a\u57df\uff0c\u6307\u5bfc\u53ef\u89e3\u91caAI\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u89e3\u91ca\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5176\u6709\u9650\u7684\u53ef\u89e3\u91ca\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u4fe1\u4efb\u548c\u90e8\u7f72\u3002\u73b0\u6709XAI\u6280\u672f\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9700\u8981\u591a\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u4e14\u5e38\u4ea7\u751f\u566a\u58f0\u6216\u89e3\u5256\u5b66\u65e0\u5173\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51faXAI-CLIP\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u5b9a\u4f4d\u4e34\u5e8a\u76f8\u5173\u89e3\u5256\u533a\u57df\uff0c\u7ed3\u5408\u8bed\u8a00\u5f15\u5bfc\u7684\u533a\u57df\u5b9a\u4f4d\u4e0e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u5e94\u7528\u6709\u9488\u5bf9\u6027\u7684\u533a\u57df\u611f\u77e5\u6270\u52a8\uff0c\u751f\u6210\u8fb9\u754c\u6e05\u6670\u7684\u663e\u8457\u6027\u56fe\u3002", "result": "\u5728FLARE22\u548cCHAOS\u6570\u636e\u96c6\u4e0a\uff0cXAI-CLIP\u76f8\u6bd4\u4f20\u7edf\u6270\u52a8\u65b9\u6cd5\uff1a\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1160%\uff0cDice\u5206\u6570\u63d0\u534744.6%\uff0c\u57fa\u4e8e\u906e\u6321\u7684\u89e3\u91ca\u7684IoU\u63d0\u9ad896.7%\u3002\u5b9a\u6027\u7ed3\u679c\u663e\u793a\u66f4\u6e05\u6670\u3001\u89e3\u5256\u5b66\u4e00\u81f4\u7684\u5f52\u56e0\u56fe\u3002", "conclusion": "\u5c06\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u6574\u5408\u5230\u57fa\u4e8e\u6270\u52a8\u7684XAI\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5b9e\u73b0\u900f\u660e\u4e14\u53ef\u4e34\u5e8a\u90e8\u7f72\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9014\u5f84\u3002"}}
{"id": "2602.07019", "pdf": "https://arxiv.org/pdf/2602.07019", "abs": "https://arxiv.org/abs/2602.07019", "authors": ["Elaheh Sabziyan Varnousfaderani", "Syed A. M. Shihab", "Jonathan King"], "title": "Deep Learning Based Multi-Level Classification for Aviation Safety", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u50cf\u9e1f\u7c7b\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u9e1f\u79cd\u3001\u7fa4\u4f53\u5f62\u6001\u548c\u89c4\u6a21\uff0c\u4ee5\u6539\u8fdb\u822a\u7a7a\u9e1f\u51fb\u9884\u9632\u7cfb\u7edf", "motivation": "\u9e1f\u51fb\u5bf9\u822a\u7a7a\u5b89\u5168\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u73b0\u6709\u9e1f\u51fb\u9884\u9632\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u9e1f\u7c7b\u96f7\u8fbe\uff0c\u4f46\u65e0\u6cd5\u8bc6\u522b\u9e1f\u79cd\uff0c\u800c\u4e0d\u540c\u9e1f\u79cd\u7684\u98de\u884c\u884c\u4e3a\u548c\u9ad8\u5ea6\u504f\u597d\u4e0d\u540c\uff0c\u8fd9\u5bf9\u51c6\u786e\u9884\u6d4b\u98de\u884c\u8def\u5f84\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51fa\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u56fe\u50cf\u9e1f\u7c7b\u5206\u7c7b\u6846\u67b6\uff0c\u4e0e\u76f8\u673a\u7cfb\u7edf\u914d\u5408\u5b9e\u73b0\u81ea\u4e3b\u89c6\u89c9\u68c0\u6d4b\u3002CNN\u7528\u4e8e\u8bc6\u522b\u9e1f\u79cd\uff0c\u5e76\u4e3a\u7279\u5b9a\u7269\u79cd\u7684\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u5173\u952e\u8f93\u5165\u3002\u6b64\u5916\u8fd8\u5b9e\u73b0\u4e86\u4e13\u95e8\u7684CNN\u5206\u7c7b\u5668\u6765\u4f30\u8ba1\u7fa4\u4f53\u5f62\u6001\u7c7b\u578b\u548c\u7fa4\u4f53\u89c4\u6a21", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u9e1f\u79cd\u3001\u7fa4\u4f53\u5f62\u6001\u548c\u89c4\u6a21\uff0c\u8fd9\u4e9b\u7279\u5f81\u4e3a\u822a\u7a7a\u5b89\u5168\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8865\u5145\u4fe1\u606f\u3002\u7fa4\u4f53\u7c7b\u578b\u548c\u89c4\u6a21\u6709\u52a9\u4e8e\u7406\u89e3\u96c6\u4f53\u98de\u884c\u884c\u4e3a\u548c\u8f68\u8ff9\u5206\u6563\uff0c\u7fa4\u4f53\u89c4\u6a21\u76f4\u63a5\u5173\u7cfb\u5230\u6f5c\u5728\u649e\u51fb\u4e25\u91cd\u7a0b\u5ea6", "conclusion": "\u901a\u8fc7\u56fe\u50cf\u8bc6\u522b\u6280\u672f\u589e\u5f3a\u73b0\u6709\u9e1f\u51fb\u9884\u9632\u7cfb\u7edf\uff0c\u63d0\u4f9b\u7269\u79cd\u7279\u5f02\u6027\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u98de\u884c\u8def\u5f84\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u6539\u5584\u822a\u7a7a\u5b89\u5168"}}
{"id": "2602.07025", "pdf": "https://arxiv.org/pdf/2602.07025", "abs": "https://arxiv.org/abs/2602.07025", "authors": ["Daniele Savietto", "Declan Campbell", "Andr\u00e9 Panisson", "Marco Nurisso", "Giovanni Petri", "Jonathan D. Cohen", "Alan Perotti"], "title": "The Geometry of Representational Failures in Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the \"Binding Problem\", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill \"concept vectors\" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u7269\u4f53\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u673a\u5236\uff0c\u63d0\u51fa\u901a\u8fc7\u6982\u5ff5\u5411\u91cf\u7684\u51e0\u4f55\u91cd\u53e0\u6765\u89e3\u91ca\u6a21\u578b\u9519\u8bef\u6a21\u5f0f", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u7269\u4f53\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4ee4\u4eba\u56f0\u60d1\u7684\u5931\u8d25\uff0c\u5982\u5e7b\u89c9\u4e0d\u5b58\u5728\u7684\u5143\u7d20\u6216\u65e0\u6cd5\u5728\u5e72\u6270\u7269\u4e2d\u8bc6\u522b\u6700\u76f8\u4f3c\u7684\u5bf9\u8c61\u3002\u8fd9\u4e9b\u9519\u8bef\u53cd\u6620\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u8ba4\u77e5\u7ea6\u675f\u7684\"\u7ed1\u5b9a\u95ee\u9898\"\uff0c\u4f46\u4eba\u5de5\u7cfb\u7edf\u4e2d\u7684\u5185\u90e8\u673a\u5236\u4ecd\u4e0d\u6e05\u695a", "method": "\u901a\u8fc7\u5206\u6790\u5f00\u653e\u6743\u91cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Qwen\u3001InternVL\u3001Gemma\uff09\u7684\u8868\u5f81\u51e0\u4f55\uff0c\u6bd4\u8f83\u63d0\u53d6\"\u6982\u5ff5\u5411\u91cf\"\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5f15\u5bfc\u5e72\u9884\u9a8c\u8bc1\u8fd9\u4e9b\u5411\u91cf\uff0c\u5728\u7b80\u5316\u548c\u81ea\u7136\u89c6\u89c9\u4efb\u52a1\u4e2d\u53ef\u9760\u5730\u64cd\u7eb5\u6a21\u578b\u884c\u4e3a", "result": "\u89c2\u5bdf\u5230\u8fd9\u4e9b\u6982\u5ff5\u5411\u91cf\u4e4b\u95f4\u7684\u51e0\u4f55\u91cd\u53e0\u4e0e\u7279\u5b9a\u9519\u8bef\u6a21\u5f0f\u5f3a\u76f8\u5173\uff0c\u4e3a\u7406\u89e3\u5185\u90e8\u8868\u5f81\u5982\u4f55\u5851\u9020\u6a21\u578b\u884c\u4e3a\u548c\u9a71\u52a8\u89c6\u89c9\u5931\u8d25\u63d0\u4f9b\u4e86\u57fa\u4e8e\u91cf\u5316\u7684\u6846\u67b6", "conclusion": "\u6982\u5ff5\u5411\u91cf\u7684\u51e0\u4f55\u5206\u6790\u4e3a\u7406\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u7269\u4f53\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u673a\u5236\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89c1\u89e3\uff0c\u5efa\u7acb\u4e86\u5185\u90e8\u8868\u5f81\u4e0e\u884c\u4e3a\u9519\u8bef\u4e4b\u95f4\u7684\u5b9a\u91cf\u8054\u7cfb"}}
{"id": "2602.07026", "pdf": "https://arxiv.org/pdf/2602.07026", "abs": "https://arxiv.org/abs/2602.07026", "authors": ["Xiaomin Yu", "Yi Xin", "Wenjie Zhang", "Chonghan Liu", "Hanzhen Zhao", "Xiaoxing Hu", "Xinlei Yu", "Ziyue Qiao", "Hao Tang", "Xue Yang", "Xiaobin Hu", "Chengwei Qin", "Hui Xiong", "Yu Qiao", "Shuicheng Yan"], "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.", "AI": {"tldr": "\u63d0\u51faReVision\u6846\u67b6\uff0c\u901a\u8fc7ReAlign\u8bad\u7ec3\u514d\u8d39\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\uff0c\u5229\u7528\u672a\u914d\u5bf9\u6570\u636e\u89e3\u51b3\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff0c\u4e3aMLLMs\u63d0\u4f9b\u9ad8\u6548\u6269\u5c55\u8def\u5f84", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u8868\u793a\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5b58\u5728\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff1a\u8868\u8fbe\u76f8\u540c\u8bed\u4e49\u7684\u4e0d\u540c\u6a21\u6001\u5d4c\u5165\u5360\u636e\u7cfb\u7edf\u504f\u79fb\u533a\u57df\u3002\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u8fc7\u5ea6\u7b80\u5316\u7684\u5404\u5411\u540c\u6027\u5047\u8bbe\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u3002", "method": "1. \u63d0\u51fa\u56fa\u5b9a\u6846\u67b6\u6a21\u6001\u95f4\u9699\u7406\u8bba\uff0c\u5c06\u6a21\u6001\u95f4\u9699\u5206\u89e3\u4e3a\u7a33\u5b9a\u504f\u5dee\u548c\u5404\u5411\u5f02\u6027\u6b8b\u5dee\uff1b2. \u63d0\u51faReAlign\u8bad\u7ec3\u514d\u8d39\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7Anchor\u3001Trace\u548cCentroid Alignment\u4e09\u6b65\u5bf9\u9f50\u6587\u672c\u8868\u793a\u5230\u56fe\u50cf\u8868\u793a\u5206\u5e03\uff1b3. \u57fa\u4e8eReAlign\u63d0\u51faReVision\u53ef\u6269\u5c55\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u96c6\u6210ReAlign\uff0c\u8ba9\u6a21\u578b\u4ece\u672a\u914d\u5bf9\u6587\u672c\u5b66\u4e60\u89c6\u89c9\u8868\u793a\u5206\u5e03\u3002", "result": "\u6846\u67b6\u8bc1\u660e\u7edf\u8ba1\u5bf9\u9f50\u7684\u672a\u914d\u5bf9\u6570\u636e\u53ef\u4ee5\u6709\u6548\u66ff\u4ee3\u6602\u8d35\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u4e3aMLLMs\u7684\u9ad8\u6548\u6269\u5c55\u63d0\u4f9b\u7a33\u5065\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u786e\u5efa\u6a21\u6a21\u6001\u95f4\u9699\u51e0\u4f55\u5f62\u72b6\u5e76\u5229\u7528\u672a\u914d\u5bf9\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6a21\u578b\u6269\u5c55\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u7684\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2602.07027", "pdf": "https://arxiv.org/pdf/2602.07027", "abs": "https://arxiv.org/abs/2602.07027", "authors": ["Sanggeon Yun", "Ryozo Masukawa", "SungHeon Jeong", "Wenjun Huang", "Hanning Chen", "Mohsen Imani"], "title": "Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.", "AI": {"tldr": "FCL\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u516c\u5e73\u6027\u7ea6\u675f\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u907f\u514d\u4f7f\u7528\u71b5\u6700\u5c0f\u5316\uff0c\u901a\u8fc7\u89e3\u8026\u589e\u5f3a\u63a2\u7d22\u548c\u516c\u5e73\u6821\u51c6\u6765\u7f13\u89e3\u5171\u4eab\u8bc1\u636e\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u71b5\u6700\u5c0f\u5316\uff0c\u4f46\u5728\u7c7b\u522b\u5171\u4eab\u89c6\u89c9\u7279\u5f81\u65f6\u53ef\u80fd\u653e\u5927\u865a\u5047\u76f8\u5173\u6027\u5e76\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u3002\u9700\u8981\u4e00\u79cd\u907f\u514d\u71b5\u6700\u5c0f\u5316\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "FCL\u91c7\u7528\u60c5\u8282\u5f0f\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u57fa\u4e8e\u52a0\u6027\u8bc1\u636e\u5206\u89e3\u5047\u8bbe\uff0c\u5c06\u9002\u5e94\u8fc7\u7a0b\u89e3\u8026\u4e3a\uff1a(1)\u57fa\u4e8e\u589e\u5f3a\u7684\u63a2\u7d22\u6765\u8bc6\u522b\u53ef\u80fd\u7684\u7c7b\u522b\u5019\u9009\uff1b(2)\u516c\u5e73\u9a71\u52a8\u7684\u6821\u51c6\uff0c\u901a\u8fc7\u8c03\u6574\u6587\u672c\u4e0a\u4e0b\u6587\u6765\u5e73\u7b49\u5316\u5bf9\u5e38\u89c1\u89c6\u89c9\u8bc1\u636e\u7684\u654f\u611f\u6027\u3002", "result": "FCL\u5728\u591a\u79cd\u9886\u57df\u504f\u79fb\u548c\u7ec6\u7c92\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u90e8\u5206\u7279\u5f81\u75f4\u8ff7\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u907f\u514d\u71b5\u6700\u5c0f\u5316\u5e76\u5f15\u5165\u516c\u5e73\u6027\u7ea6\u675f\uff0cFCL\u80fd\u591f\u6709\u6548\u6821\u51c6\u6587\u672c\u5d4c\u5165\uff0c\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.07028", "pdf": "https://arxiv.org/pdf/2602.07028", "abs": "https://arxiv.org/abs/2602.07028", "authors": ["Kaaustaaub Shankar", "Bharadwaj Dogga", "Kelly Cohen"], "title": "A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to NAFIPS 2026", "summary": "Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.", "AI": {"tldr": "ANFIS\u589e\u5f3a\u7684CNN\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u8868\u73b0\u4e0d\u4e00\u81f4\uff1aResNet18-ANFIS\u6709\u6240\u6539\u5584\uff0c\u800cVGG-ANFIS\u901a\u5e38\u8868\u73b0\u66f4\u5dee\uff0c\u8868\u660e\u795e\u7ecf\u6a21\u7cca\u589e\u5f3a\u5e76\u975e\u666e\u904d\u6709\u76ca\u3002", "motivation": "\u4f20\u7edfCNN\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u795e\u7ecf\u6a21\u7cca\u6df7\u5408\u6a21\u578b\uff08\u5982DCNFIS\uff09\u7528ANFIS\u66ff\u6362\u5168\u8fde\u63a5\u5c42\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u6bd4\u8f83\u6807\u51c6CNN\uff08ConvNet\u3001VGG\u3001ResNet18\uff09\u4e0e\u5176ANFIS\u589e\u5f3a\u7248\u672c\u5728MNIST\u3001Fashion-MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684PGD\u653b\u51fb\u548c\u65e0\u68af\u5ea6\u7684Square\u653b\u51fb\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "ANFIS\u96c6\u6210\u5e76\u672a\u4e00\u81f4\u63d0\u9ad8\u5e72\u51c0\u51c6\u786e\u7387\uff0c\u4e14\u5bf9\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u5177\u6709\u67b6\u6784\u4f9d\u8d56\u6027\uff1aResNet18-ANFIS\u8868\u73b0\u51fa\u6539\u5584\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u800cVGG-ANFIS\u901a\u5e38\u8868\u73b0\u4e0d\u5982\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u795e\u7ecf\u6a21\u7cca\u589e\u5f3a\u53ef\u4ee5\u5728\u7279\u5b9a\u67b6\u6784\u4e2d\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u5e76\u975e\u666e\u904d\u6709\u76ca\uff1b\u67b6\u6784\u9009\u62e9\u5bf9ANFIS\u589e\u5f3a\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.07038", "pdf": "https://arxiv.org/pdf/2602.07038", "abs": "https://arxiv.org/abs/2602.07038", "authors": ["Yifan Ji", "Zhipeng Xu", "Zhenghao Liu", "Zulong Chen", "Qian Zhang", "Zhibo Yang", "Junyang Lin", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.", "AI": {"tldr": "UNIKIE-BENCH\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u6587\u6863\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u80fd\u529b\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u5305\u542b\u7ea6\u675f\u7c7b\u522b\u548c\u5f00\u653e\u7c7b\u522b\u4e24\u4e2a\u8d5b\u9053\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u591a\u6837\u5316\u6a21\u5f0f\u5b9a\u4e49\u3001\u957f\u5c3e\u5173\u952e\u5b57\u6bb5\u548c\u590d\u6742\u5e03\u5c40\u4e0b\u7684\u6027\u80fd\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6587\u6863\u7684\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u9762\u4e34\u5e03\u5c40\u7ed3\u6784\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u4efb\u52a1\u7279\u5b9a\u9700\u6c42\u7684\u5de8\u5927\u5dee\u5f02\uff0c\u800c\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u7f3a\u4e4f\u5168\u9762\u7cfb\u7edf\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faUNIKIE-BENCH\u7edf\u4e00\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u8d5b\u9053\uff1a1) \u7ea6\u675f\u7c7b\u522bKIE\u8d5b\u9053\uff0c\u57fa\u4e8e\u573a\u666f\u9884\u5b9a\u4e49\u6a21\u5f0f\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u9700\u6c42\uff1b2) \u5f00\u653e\u7c7b\u522bKIE\u8d5b\u9053\uff0c\u63d0\u53d6\u6587\u6863\u4e2d\u660e\u786e\u5b58\u5728\u7684\u4efb\u4f55\u5173\u952e\u4fe1\u606f\u3002", "result": "\u5bf915\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u5728\u591a\u6837\u5316\u6a21\u5f0f\u5b9a\u4e49\u3001\u957f\u5c3e\u5173\u952e\u5b57\u6bb5\u548c\u590d\u6742\u5e03\u5c40\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e0d\u540c\u6587\u6863\u7c7b\u578b\u548c\u573a\u666f\u95f4\u5b58\u5728\u660e\u663e\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u5728\u57fa\u7840\u51c6\u786e\u6027\u548c\u5e03\u5c40\u611f\u77e5\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u6301\u7eed\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2602.07041", "pdf": "https://arxiv.org/pdf/2602.07041", "abs": "https://arxiv.org/abs/2602.07041", "authors": ["Leeje Jang", "Yao-Yi Chiang", "Angela M. Hastings", "Patimaporn Pungchanchaikul", "Martha B. Lucas", "Emily C. Schultz", "Jeffrey P. Louie", "Mohamed Estai", "Wen-Chen Wang", "Ryan H. L. Ip", "Boyen Huang"], "title": "OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.", "AI": {"tldr": "OMNI-Dent\u662f\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u7259\u79d1\u8bca\u65ad\u6846\u67b6\uff0c\u5c06\u4e34\u5e8a\u63a8\u7406\u539f\u5219\u878d\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u667a\u80fd\u624b\u673a\u591a\u89c6\u89d2\u7167\u7247\u8fdb\u884c\u7259\u9f7f\u7ea7\u8bc4\u4f30\uff0c\u65e0\u9700\u7259\u79d1\u7279\u5b9a\u5fae\u8c03\u3002", "motivation": "\u5f53\u524dAI\u7259\u79d1\u8bca\u65ad\u65b9\u6cd5\u4e3b\u8981\u5c06\u8bca\u65ad\u89c6\u4e3a\u89c6\u89c9\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\uff0c\u672a\u80fd\u53cd\u6620\u7259\u79d1\u4e13\u4e1a\u4eba\u5458\u7684\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\u3002\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6210\u50cf\u6761\u4ef6\u3002\u8bb8\u591a\u4eba\u7f3a\u4e4f\u53ca\u65f6\u7684\u4e13\u4e1a\u8bc4\u4f30\u673a\u4f1a\u3002", "method": "OMNI-Dent\u6846\u67b6\u6574\u5408\u4e86\u7259\u79d1\u4e13\u5bb6\u7684\u8bca\u65ad\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5f15\u5bfc\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u667a\u80fd\u624b\u673a\u591a\u89c6\u89d2\u7167\u7247\u4e0a\u8fdb\u884c\u7259\u9f7f\u7ea7\u8bc4\u4f30\uff0c\u65e0\u9700\u5bf9VLM\u8fdb\u884c\u7259\u79d1\u7279\u5b9a\u5fae\u8c03\u3002\u5229\u7528VLM\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u80fd\u529b\uff0c\u5728\u7f3a\u4e4f\u4e34\u5e8a\u5f71\u50cf\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u8bca\u65ad\u8bc4\u4f30\u3002", "result": "\u8be5\u6846\u67b6\u4f5c\u4e3a\u65e9\u671f\u8f85\u52a9\u5de5\u5177\uff0c\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u6f5c\u5728\u5f02\u5e38\u5e76\u786e\u5b9a\u4f55\u65f6\u9700\u8981\u4e13\u4e1a\u8bc4\u4f30\uff0c\u4e3a\u7f3a\u4e4f\u73b0\u573a\u62a4\u7406\u673a\u4f1a\u7684\u4e2a\u4eba\u63d0\u4f9b\u5b9e\u7528\u9009\u62e9\u3002", "conclusion": "OMNI-Dent\u901a\u8fc7\u5c06\u4e34\u5e8a\u63a8\u7406\u539f\u5219\u878d\u5165VLM\u7ba1\u9053\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u7259\u79d1\u8bca\u65ad\u65b9\u6cd5\uff0c\u80fd\u591f\u9002\u5e94\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6210\u50cf\u6761\u4ef6\uff0c\u89e3\u51b3\u4f20\u7edfAI\u65b9\u6cd5\u5728\u7259\u79d1\u8bca\u65ad\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.07042", "pdf": "https://arxiv.org/pdf/2602.07042", "abs": "https://arxiv.org/abs/2602.07042", "authors": ["Magesh Rajasekaran", "Md Saiful Islam Sajol", "Frej Berglind", "Supratik Mukhopadhyay", "Kamalika Das"], "title": "COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification", "categories": ["cs.CV"], "comment": "Copyright by SIAM. Unauthorized reproduction of this article is prohibited First Published in Proceedings of the 2024 SIAM International Conference on Data Mining (SDM24), published by the Society for Industrial and Applied Mathematics (SIAM)", "summary": "Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.", "AI": {"tldr": "COMBOOD\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u65e0\u76d1\u7763\u534a\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u56fe\u50cf\u8bc6\u522b\u4e2d\u7684OOD\u68c0\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u6700\u8fd1\u90bb\u548c\u9a6c\u6c0f\u8ddd\u79bb\u4e24\u79cd\u4fe1\u53f7\uff0c\u5728\u8fd1OOD\u548c\u8fdcOOD\u573a\u666f\u4e0b\u90fd\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002", "motivation": "\u5728\u63a8\u7406\u65f6\u8bc6\u522bOOD\u6570\u636e\u5bf9\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u81ea\u52a8\u5316\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd1OOD\u573a\u666f\uff08\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u8fd1OOD\u548c\u8fdcOOD\u573a\u666f\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "COMBOOD\u6846\u67b6\u7ed3\u5408\u4e86\u4e24\u79cd\u8ddd\u79bb\u5ea6\u91cf\u4fe1\u53f7\uff1a\u6700\u8fd1\u90bb\uff08\u975e\u53c2\u6570\u65b9\u6cd5\uff09\u548c\u9a6c\u6c0f\u8ddd\u79bb\uff08\u53c2\u6570\u65b9\u6cd5\uff09\u3002\u6700\u8fd1\u90bb\u65b9\u6cd5\u63d0\u4f9b\u975e\u53c2\u6570\u7684OOD\u68c0\u6d4b\uff0c\u9a6c\u6c0f\u8ddd\u79bb\u5728\u8fdcOOD\u573a\u666f\u4e2d\u7279\u522b\u6709\u6548\u3002\u8be5\u6846\u67b6\u5728\u534a\u53c2\u6570\u8bbe\u7f6e\u4e2d\u878d\u5408\u8fd9\u4e24\u79cd\u4fe1\u53f7\uff0c\u4e3aOOD\u68c0\u6d4b\u751f\u6210\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002", "result": "COMBOOD\u5728OpenOOD\u57fa\u51c6\u6570\u636e\u96c6\uff08v1\u548cv1.5\uff09\u4ee5\u53ca\u6587\u6863\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u8fdcOOD\u548c\u8fd1OOD\u573a\u666f\u4e0b\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u3002\u5728\u5927\u591a\u6570\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCOMBOOD\u5e26\u6765\u7684\u51c6\u786e\u7387\u63d0\u5347\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u3002\u6846\u67b6\u7684\u590d\u6742\u5ea6\u4e0e\u5d4c\u5165\u7a7a\u95f4\u5927\u5c0f\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "conclusion": "COMBOOD\u662f\u4e00\u4e2a\u6709\u6548\u7684\u534a\u53c2\u6570\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6700\u8fd1\u90bb\u548c\u9a6c\u6c0f\u8ddd\u79bb\u7684\u4f18\u52bf\uff0c\u5728\u8fd1OOD\u548c\u8fdcOOD\u573a\u666f\u4e0b\u90fd\u80fd\u63d0\u4f9b\u51c6\u786e\u7684OOD\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.07044", "pdf": "https://arxiv.org/pdf/2602.07044", "abs": "https://arxiv.org/abs/2602.07044", "authors": ["Tianyi Qu", "Songxiao Yang", "Haolin Wang", "Huadong Song", "Xiaoting Guo", "Wenguang Hu", "Guanlin Liu", "Honghe Chen", "Yafei Ou"], "title": "PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "A dataset contains 240,320 pipeline MFL pseudo-color images and 191,530 bounding-box annotations, collected from 11 pipelines spanning approximately 1,480 km", "summary": "Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \\textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \\textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \\textbf{240,320} images and \\textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \\textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.", "AI": {"tldr": "PipeMFL-240K\uff1a\u9996\u4e2a\u5927\u89c4\u6a21\u516c\u5f00\u7684\u7ba1\u9053\u6f0f\u78c1\u68c0\u6d4b\u6570\u636e\u96c6\u4e0e\u57fa\u51c6\uff0c\u5305\u542b24\u4e07\u5f20\u56fe\u50cf\u548c19\u4e07\u6807\u6ce8\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u516c\u5e73\u6bd4\u8f83\u548c\u53ef\u590d\u73b0\u8bc4\u4f30\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u81ea\u52a8\u5316\u6f0f\u78c1\u68c0\u6d4b\u89e3\u91ca\u65b9\u9762\u6709\u524d\u666f\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u53ef\u9760\u6a21\u578b\u7684\u8fdb\u5c55\u53d7\u5230\u9650\u5236\uff0c\u96be\u4ee5\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u548c\u53ef\u590d\u73b0\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86PipeMFL-240K\u6570\u636e\u96c6\uff0c\u5305\u542b240,320\u5f20\u56fe\u50cf\u548c191,530\u4e2a\u9ad8\u8d28\u91cf\u8fb9\u754c\u6846\u6807\u6ce8\uff0c\u6765\u81ea11\u6761\u603b\u957f\u7ea61,480\u516c\u91cc\u7684\u7ba1\u9053\u3002\u6570\u636e\u96c6\u5177\u6709\u4e09\u4e2a\u72ec\u7279\u6311\u6218\uff1a12\u4e2a\u7c7b\u522b\u7684\u6781\u7aef\u957f\u5c3e\u5206\u5e03\u3001\u5927\u91cf\u5fae\u5c0f\u76ee\u6807\uff08\u4ec5\u51e0\u4e2a\u50cf\u7d20\uff09\u3001\u663e\u8457\u7684\u7c7b\u5185\u53d8\u5f02\u6027\u3002", "result": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u5efa\u7acb\u57fa\u7ebf\u3002\u7ed3\u679c\u663e\u793a\u73b0\u4ee3\u68c0\u6d4b\u5668\u5728\u6f0f\u78c1\u6570\u636e\u7684\u56fa\u6709\u7279\u6027\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u8868\u660e\u6709\u76f8\u5f53\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "PipeMFL-240K\u4e3a\u7ba1\u9053\u6f0f\u78c1\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c06\u52a0\u901f\u7b97\u6cd5\u521b\u65b0\u548c\u53ef\u590d\u73b0\u7814\u7a76\uff0c\u4e3a\u9ad8\u6548\u7ba1\u9053\u8bca\u65ad\u548c\u7ef4\u62a4\u89c4\u5212\u5960\u5b9a\u5173\u952e\u57fa\u7840\u3002"}}
{"id": "2602.07045", "pdf": "https://arxiv.org/pdf/2602.07045", "abs": "https://arxiv.org/abs/2602.07045", "authors": ["Zhiming Luo", "Di Wang", "Haonan Guo", "Jing Zhang", "Bo Du"], "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.", "AI": {"tldr": "VLRS-Bench\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u9065\u611f\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2000\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u8ba4\u77e5\u3001\u51b3\u7b56\u548c\u9884\u6d4b\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u65e8\u5728\u63a8\u52a8\u9065\u611f\u9886\u57df\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u504f\u5411\u611f\u77e5\u4efb\u52a1\uff08\u5982\u76ee\u6807\u8bc6\u522b\u548c\u573a\u666f\u5206\u7c7b\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u8981\u6c42\u9ad8\u7684\u9065\u611f\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002\u9700\u8981\u4e13\u95e8\u7684\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u4e86VLRS-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2000\u4e2a\u95ee\u7b54\u5bf9\uff0c\u5e73\u5747\u957f\u5ea671\u8bcd\uff0c\u6db5\u76d614\u4e2a\u4efb\u52a1\u548c\u6700\u591a8\u4e2a\u65f6\u95f4\u9636\u6bb5\u3002\u901a\u8fc7\u6574\u5408\u9065\u611f\u5148\u9a8c\u77e5\u8bc6\u548c\u4e13\u5bb6\u77e5\u8bc6\u7684\u4e13\u95e8\u6d41\u7a0b\u6784\u5efa\uff0c\u786e\u4fdd\u5730\u7406\u7a7a\u95f4\u771f\u5b9e\u6027\u548c\u63a8\u7406\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u5b58\u5728\u663e\u8457\u74f6\u9888\uff0c\u4e3a\u9065\u611f\u793e\u533a\u63a8\u8fdb\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "VLRS-Bench\u586b\u8865\u4e86\u9065\u611f\u9886\u57df\u590d\u6742\u63a8\u7406\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u9065\u611f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2602.07047", "pdf": "https://arxiv.org/pdf/2602.07047", "abs": "https://arxiv.org/abs/2602.07047", "authors": ["Muhammad Rashid", "Elvio G. Amparore", "Enrico Ferrari", "Damiano Verda"], "title": "ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees", "categories": ["cs.CV", "cs.LG"], "comment": "AAAI-2026", "summary": "Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.", "AI": {"tldr": "\u63d0\u51faShapBPT\u65b9\u6cd5\uff0c\u5c06\u5206\u5c42Shapley\u503c\u5e94\u7528\u4e8e\u56fe\u50cf\u6570\u636e\u7684\u591a\u5c3a\u5ea6\u5c42\u6b21\u7ed3\u6784\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u8bed\u4e49\u66f4\u4e30\u5bcc\u7684\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5206\u5c42Shapley\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u56fe\u50cf\u6570\u636e\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\uff0c\u5bfc\u81f4\u6536\u655b\u6162\u4e14\u4e0e\u771f\u5b9e\u5f62\u6001\u7279\u5f81\u5bf9\u9f50\u5f31\uff1b\u7f3a\u4e4f\u9488\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u6570\u636e\u611f\u77e5\u5c42\u6b21\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u7ed3\u6784\u5316\u89c6\u89c9\u6570\u636e\u7684\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faShapBPT\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5206\u5c42Shapley\u516c\u5f0f\uff0c\u5c06Shapley\u7cfb\u6570\u5206\u914d\u7ed9\u4e3a\u56fe\u50cf\u5b9a\u5236\u7684\u591a\u5c3a\u5ea6\u5c42\u6b21\u7ed3\u6784\u2014\u2014\u4e8c\u53c9\u5206\u5272\u6811(BPT)\uff0c\u901a\u8fc7\u6570\u636e\u611f\u77e5\u7684\u5c42\u6b21\u5206\u5272\u786e\u4fdd\u7279\u5f81\u5f52\u56e0\u4e0e\u5185\u5728\u56fe\u50cf\u5f62\u6001\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eShapBPT\u6709\u6548\u6027\uff1a\u4e0e\u56fe\u50cf\u7ed3\u6784\u5bf9\u9f50\u66f4\u4f18\u3001\u6548\u7387\u9ad8\u4e8e\u73b0\u6709XCV\u65b9\u6cd5\uff1b20\u4eba\u7528\u6237\u7814\u7a76\u786e\u8ba4\u4eba\u7c7b\u66f4\u504f\u597dShapBPT\u7684\u89e3\u91ca\u3002", "conclusion": "ShapBPT\u5c06\u5206\u5c42Shapley\u65b9\u6cd5\u4e0e\u56fe\u50cf\u6570\u636e\u8fde\u63a5\uff0c\u4e3a\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u8bed\u4e49\u66f4\u4e30\u5bcc\u7684\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u7ed3\u6784\u5316\u89c6\u89c9\u6570\u636e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.07049", "pdf": "https://arxiv.org/pdf/2602.07049", "abs": "https://arxiv.org/abs/2602.07049", "authors": ["Jindong Li", "Dario Zanca", "Vincent Christlein", "Tim Hamann", "Jens Barth", "Peter K\u00e4mpf", "Bj\u00f6rn Eskofier"], "title": "Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead", "categories": ["cs.CV"], "comment": null, "summary": "Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.", "AI": {"tldr": "\u63d0\u51faECHWR\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e34\u65f6\u8f85\u52a9\u5206\u652f\u548c\u53cc\u91cd\u5bf9\u6bd4\u76ee\u6807\u63d0\u5347IMU\u624b\u5199\u8bc6\u522b\u6027\u80fd\uff0c\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\uff0c\u5728OnHW-Words500\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u9519\u8bef\u7387\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5728\u7ebf\u624b\u5199\u8bc6\u522b\u9762\u4e34\u5185\u5b58\u9650\u5236\uff0c\u9700\u8981\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u8bc6\u522b\u7cbe\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u8868\u793a\u548c\u8bc6\u522b\u51c6\u786e\u6027\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u63d0\u51faECHWR\u8bad\u7ec3\u6846\u67b6\uff1a1) \u4f7f\u7528\u4e34\u65f6\u8f85\u52a9\u5206\u652f\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9\u9f50\u4f20\u611f\u5668\u4fe1\u53f7\u4e0e\u6587\u672c\u8bed\u4e49\u5d4c\u5165\uff1b2) \u91c7\u7528\u53cc\u91cd\u5bf9\u6bd4\u76ee\u6807\uff1a\u6279\u5185\u5bf9\u6bd4\u635f\u5931\u7528\u4e8e\u6a21\u6001\u5bf9\u9f50\uff0c\u65b0\u9896\u7684\u9519\u8bef\u5bf9\u6bd4\u635f\u5931\u533a\u5206\u6b63\u786e\u4fe1\u53f7\u4e0e\u5408\u6210\u786c\u8d1f\u6837\u672c\uff1b3) \u8bad\u7ec3\u540e\u4e22\u5f03\u8f85\u52a9\u5206\u652f\uff0c\u4fdd\u6301\u539f\u59cb\u9ad8\u6548\u67b6\u6784\u3002", "result": "\u5728OnHW-Words500\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1a\u4f5c\u8005\u72ec\u7acb\u5206\u5272\u5b57\u7b26\u9519\u8bef\u7387\u964d\u4f4e7.4%\uff0c\u4f5c\u8005\u4f9d\u8d56\u5206\u5272\u964d\u4f4e10.4%\u3002\u9519\u8bef\u5bf9\u6bd4\u635f\u5931\u5728\u5904\u7406\u672a\u89c1\u4e66\u5199\u98ce\u683c\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "ECHWR\u6846\u67b6\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u4e34\u65f6\u8f85\u52a9\u5206\u652f\u548c\u53cc\u91cd\u5bf9\u6bd4\u76ee\u6807\u6709\u6548\u63d0\u5347\u4e86IMU\u624b\u5199\u8bc6\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u9519\u8bef\u5bf9\u6bd4\u635f\u5931\u5bf9\u5904\u7406\u65b0\u4e66\u5199\u98ce\u683c\u6709\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2602.07050", "pdf": "https://arxiv.org/pdf/2602.07050", "abs": "https://arxiv.org/abs/2602.07050", "authors": ["Sonia Joseph", "Quentin Garrido", "Randall Balestriero", "Matthew Kowal", "Thomas Fel", "Shahab Bakhtiari", "Blake Richards", "Mike Rabbat"], "title": "Interpreting Physics in Video World Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.\n  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.", "AI": {"tldr": "\u89c6\u9891\u7f16\u7801\u5668\u5185\u90e8\u5b58\u5728\"\u7269\u7406\u6d8c\u73b0\u533a\"\uff0c\u7269\u7406\u53d8\u91cf\u4ee5\u5206\u5e03\u5f0f\u800c\u975e\u5206\u89e3\u5f0f\u8868\u5f81\uff0c\u8fd0\u52a8\u65b9\u5411\u901a\u8fc7\u9ad8\u7ef4\u73af\u5f62\u51e0\u4f55\u7ed3\u6784\u7f16\u7801\u3002", "motivation": "\u63a2\u7a76\u89c6\u9891\u6a21\u578b\u662f\u5426\u4f9d\u8d56\u7269\u7406\u53d8\u91cf\u7684\u5206\u89e3\u5f0f\u8868\u5f81\uff0c\u8fd8\u662f\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u5206\u5e03\u5f0f\u65b9\u5f0f\u9690\u5f0f\u8868\u793a\u8fd9\u4e9b\u53d8\u91cf\uff0c\u4ee5\u7406\u89e3\u73b0\u4ee3\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u63a2\u6d4b\u3001\u5b50\u7a7a\u95f4\u51e0\u4f55\u5206\u6790\u3001\u8865\u4e01\u7ea7\u89e3\u7801\u548c\u5b9a\u5411\u6ce8\u610f\u529b\u6d88\u878d\u7b49\u65b9\u6cd5\uff0c\u5728\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u89c6\u9891Transformer\u4e2d\u8868\u5f81\u7269\u7406\u4fe1\u606f\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u7ec4\u7ec7\u65b9\u5f0f\u3002", "result": "\u53d1\u73b0\u8de8\u67b6\u6784\u7684\u4e2d\u95f4\u6df1\u5ea6\"\u7269\u7406\u6d8c\u73b0\u533a\"\uff0c\u7269\u7406\u53d8\u91cf\u5728\u6b64\u53d8\u5f97\u53ef\u8bbf\u95ee\uff1b\u6807\u91cf\u7269\u7406\u91cf\u65e9\u671f\u5373\u53ef\u83b7\u5f97\uff0c\u800c\u8fd0\u52a8\u65b9\u5411\u4ec5\u5728\u6d8c\u73b0\u533a\u53d8\u5f97\u53ef\u8bbf\u95ee\uff0c\u4e14\u901a\u8fc7\u5177\u6709\u73af\u5f62\u51e0\u4f55\u7ed3\u6784\u7684\u9ad8\u7ef4\u7fa4\u4f53\u7f16\u7801\u3002", "conclusion": "\u73b0\u4ee3\u89c6\u9891\u6a21\u578b\u4e0d\u4f7f\u7528\u7ecf\u5178\u7269\u7406\u5f15\u64ce\u90a3\u6837\u7684\u5206\u89e3\u5f0f\u7269\u7406\u53d8\u91cf\u8868\u5f81\uff0c\u800c\u662f\u4f7f\u7528\u5206\u5e03\u5f0f\u8868\u5f81\uff0c\u8fd9\u79cd\u8868\u5f81\u8db3\u4ee5\u8fdb\u884c\u7269\u7406\u9884\u6d4b\u3002"}}
{"id": "2602.07051", "pdf": "https://arxiv.org/pdf/2602.07051", "abs": "https://arxiv.org/abs/2602.07051", "authors": ["Karthik Sivakoti"], "title": "Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.", "AI": {"tldr": "Neural Sentinel\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b8c\u6210\u8f66\u724c\u8bc6\u522b\u3001\u72b6\u6001\u5206\u7c7b\u548c\u8f66\u8f86\u5c5e\u6027\u63d0\u53d6\uff0c\u51c6\u786e\u738792.3%\uff0c\u6bd4\u4f20\u7edfOCR\u65b9\u6cd5\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u4f20\u7edfALPR\u7cfb\u7edf\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff08\u76ee\u6807\u68c0\u6d4b+OCR\u6a21\u5757\uff09\uff0c\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u3001\u5ef6\u8fdf\u9ad8\u3001\u67b6\u6784\u590d\u6742\u7684\u95ee\u9898\u3002\u9700\u8981\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u80fd\u5904\u7406\u591a\u4efb\u52a1\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528PaliGemma 3B\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u56de\u7b54\u591a\u4e2a\u5173\u4e8e\u8f66\u8f86\u56fe\u50cf\u7684\u89c6\u89c9\u95ee\u9898\u3002\u5f15\u5165\u4eba\u673a\u534f\u540c\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ecf\u9a8c\u56de\u653e\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u8f66\u724c\u8bc6\u522b\u51c6\u786e\u738792.3%\uff0c\u6bd4EasyOCR\u63d0\u534714.1%\uff0c\u6bd4PaddleOCR\u63d0\u53479.9%\u3002\u5e73\u5747\u63a8\u7406\u5ef6\u8fdf152ms\uff0c\u6821\u51c6\u8bef\u5dee0.048\u3002\u96f6\u6837\u672c\u6cdb\u5316\u5230\u8f66\u8f86\u989c\u8272\u68c0\u6d4b(89%)\u3001\u5b89\u5168\u5e26\u68c0\u6d4b(82%)\u3001\u4e58\u5458\u8ba1\u6570(78%)\u7b49\u4efb\u52a1\u3002", "conclusion": "\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u65b9\u6cd5\u4ee3\u8868\u4e86ALPR\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u63d0\u4f9b\u66f4\u4f18\u7684\u51c6\u786e\u6027\u3001\u964d\u4f4e\u7684\u67b6\u6784\u590d\u6742\u6027\u4ee5\u53ca\u4f20\u7edf\u6d41\u6c34\u7ebf\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u7684\u6d8c\u73b0\u591a\u4efb\u52a1\u80fd\u529b\u3002"}}
{"id": "2602.07052", "pdf": "https://arxiv.org/pdf/2602.07052", "abs": "https://arxiv.org/abs/2602.07052", "authors": ["Ziye Xie", "Oded Schlesinger", "Raj Kundu", "Jessica Y. Choi", "Pablo Iturralde", "Dennis A. Turner", "Stefan M. Goetz", "Guillermo Sapiro", "Angel V. Peterchev", "J. Matias Di Martino"], "title": "Toward Accurate and Accessible Markerless Neuronavigation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01\u00b0$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u65e0\u6807\u8bb0\u795e\u7ecf\u5bfc\u822a\u65b9\u6cd5\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u5149\u76f8\u673a\u7ed3\u5408\u9762\u90e8\u51e0\u4f55\u5efa\u6a21\uff0c\u66ff\u4ee3\u4f20\u7edf\u57fa\u4e8e\u6807\u8bb0\u7684\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u663e\u793a\u7cbe\u5ea6\u8db3\u591f\u7528\u4e8e\u7ecf\u9885\u78c1\u523a\u6fc0\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u4e3b\u4f53\u5b89\u88c5\u7684\u6807\u8bb0\uff0c\u9700\u8981\u624b\u52a8\u914d\u51c6\uff0c\u53ef\u80fd\u5728\u624b\u672f\u8fc7\u7a0b\u4e2d\u79fb\u4f4d\uff0c\u5e76\u5f15\u8d77\u4e0d\u9002\u3002\u8fd9\u4e9b\u7cfb\u7edf\u6602\u8d35\u4e14\u590d\u6742\uff0c\u9650\u5236\u4e86\u795e\u7ecf\u5bfc\u822a\u5728\u4e34\u5e8a\u548c\u7814\u7a76\u73af\u5883\u4e2d\u7684\u666e\u53ca\u3002", "method": "\u5f15\u5165\u65e0\u6807\u8bb0\u65b9\u6cd5\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u5149\u76f8\u673a\uff08\u7ed3\u5408\u7acb\u4f53\u89c6\u89c9\u548c\u6df1\u5ea6\u611f\u77e5\uff09\u7ed3\u5408\u9762\u90e8\u51e0\u4f55\u7684\u7b97\u6cd5\u5efa\u6a21\uff0c\u66ff\u4ee3\u6602\u8d35\u7684\u786c\u4ef6\u548c\u7269\u7406\u6807\u8bb0\u3002", "result": "\u572850\u540d\u4eba\u7c7b\u53d7\u8bd5\u8005\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0c\u6700\u4f73\u65e0\u6807\u8bb0\u7b97\u6cd5\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u6807\u8bb0\u7cfb\u7edf\u7684\u4e2d\u4f4d\u8ddf\u8e2a\u8bef\u5dee\u4ec5\u4e3a2.32\u6beb\u7c73\u548c2.01\u00b0\uff0c\u7cbe\u5ea6\u8db3\u4ee5\u7528\u4e8e\u7ecf\u9885\u78c1\u523a\u6fc0\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u65e0\u6807\u8bb0\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u6807\u8bb0\u795e\u7ecf\u5bfc\u822a\u65b9\u6cd5\u53ef\u4ee5\u964d\u4f4e\u8bbe\u7f6e\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u60a3\u8005\u8212\u9002\u5ea6\uff0c\u5e76\u6269\u5927\u795e\u7ecf\u5bfc\u822a\u5728\u4e34\u5e8a\u548c\u7814\u7a76\u73af\u5883\u4e2d\u7684\u53ef\u53ca\u6027\u3002\u4e0d\u540c\u76f8\u673a\u4f20\u611f\u5668\u6570\u636e\u7684\u878d\u5408\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6574\u4f53\u7cbe\u5ea6\u3002"}}
{"id": "2602.07057", "pdf": "https://arxiv.org/pdf/2602.07057", "abs": "https://arxiv.org/abs/2602.07057", "authors": ["Di Mo", "Mingyang Sun", "Chengxiu Yin", "Runjia Tian", "Yanhong Wu", "Liyan Xu"], "title": "RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything", "categories": ["cs.CV"], "comment": null, "summary": "Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.", "AI": {"tldr": "RECITYGEN\u662f\u4e00\u4e2a\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u4ea4\u4e92\u5f0f\u8bed\u4e49\u5206\u5272\u7684\u5de5\u5177\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6587\u672c\u63d0\u793a\u4ea4\u4e92\u5f0f\u751f\u6210\u57ce\u5e02\u8857\u666f\u7684\u53d8\u4f53\u56fe\u50cf\uff0c\u7528\u4e8e\u53c2\u4e0e\u5f0f\u57ce\u5e02\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u81ea\u4e0a\u800c\u4e0b\u7684\u57ce\u5e02\u8bbe\u8ba1\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u516c\u4f17\u610f\u89c1\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u613f\u666f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002\u6570\u5b57\u5de5\u5177\u7684\u53d1\u5c55\u4e3a\u66f4\u591a\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u57ce\u5e02\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u9700\u8981\u66f4\u6613\u7528\u7684\u5de5\u5177\u6765\u964d\u4f4e\u53c2\u4e0e\u95e8\u69db\u3002", "method": "\u7ed3\u5408\u6700\u5148\u8fdb\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u4ea4\u4e92\u5f0f\u8bed\u4e49\u5206\u5272\u6280\u672f\uff0c\u5f00\u53d1\u4e86RECITYGEN\u5de5\u5177\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u4ea4\u4e92\u5f0f\u521b\u5efa\u57ce\u5e02\u73af\u5883\u7684\u53d8\u4f53\u8857\u666f\u56fe\u50cf\u3002", "result": "\u5728\u5317\u4eac\u7684\u8bd5\u70b9\u9879\u76ee\u4e2d\uff0c\u7528\u6237\u4f7f\u7528RECITYGEN\u4e3a\u6b63\u5728\u8fdb\u884c\u7684\u57ce\u5e02\u66f4\u65b0\u9879\u76ee\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002\u5c3d\u7ba1\u5b58\u5728\u4e00\u4e9b\u9650\u5236\uff0c\u4f46\u8be5\u5de5\u5177\u5728\u7b26\u5408\u516c\u4f17\u504f\u597d\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u6f5c\u529b\u3002", "conclusion": "RECITYGEN\u4ee3\u8868\u4e86\u5411\u66f4\u52a8\u6001\u548c\u5305\u5bb9\u7684\u57ce\u5e02\u89c4\u5212\u65b9\u6cd5\u7684\u8f6c\u53d8\uff0c\u901a\u8fc7\u964d\u4f4e\u8bbe\u8ba1\u751f\u6210\u95e8\u69db\uff0c\u4f7f\u516c\u4f17\u80fd\u591f\u66f4\u6709\u6548\u5730\u53c2\u4e0e\u57ce\u5e02\u8bbe\u8ba1\u8fc7\u7a0b\u3002"}}
{"id": "2602.07058", "pdf": "https://arxiv.org/pdf/2602.07058", "abs": "https://arxiv.org/abs/2602.07058", "authors": ["Carolina R. Kelsch", "Leonardo S. B. Pereira", "Natnael Mola", "Luis H. Arribas", "Juan C. S. M. Avedillo"], "title": "FADE: Selective Forgetting via Sparse LoRA and Self-Distillation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.", "AI": {"tldr": "FADE\u662f\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u4e24\u9636\u6bb5\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5b9a\u4f4d\u548c\u81ea\u84b8\u998f\u5b9e\u73b0\u9ad8\u6548\u6982\u5ff5\u64e6\u9664\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6570\u636e\u4fdd\u62a4\u6cd5\u89c4\u548c\u8d1f\u8d23\u4efbAI\u5b9e\u8df5\u7684\u8981\u6c42\uff0c\u9700\u8981\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u6570\u636e\u6216\u6982\u5ff5\u7684\u5f71\u54cd\u3002\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u9057\u5fd8\u65b9\u6cd5\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u96be\u4ee5\u5e73\u8861\u6709\u6548\u9057\u5fd8\u4e0e\u4fdd\u7559\u65e0\u5173\u6982\u5ff5\u7684\u6311\u6218\u3002", "method": "FADE\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u663e\u8457\u6027\u8bc6\u522b\u4e0e\u9057\u5fd8\u96c6\u6700\u76f8\u5173\u7684\u53c2\u6570\uff0c\u901a\u8fc7\u7a00\u758fLoRA\u9002\u914d\u5668\u8fdb\u884c\u7ea6\u675f\u66f4\u65b0\uff1b2) \u5e94\u7528\u81ea\u84b8\u998f\u76ee\u6807\uff0c\u7528\u7528\u6237\u5b9a\u4e49\u7684\u66ff\u4ee3\u6982\u5ff5\u8986\u76d6\u88ab\u9057\u5fd8\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u7559\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u7684\u884c\u4e3a\u3002", "result": "\u5728UnlearnCanvas\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0cFADE\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u6027\u80fd\uff0c\u5728\u9057\u5fd8-\u4fdd\u7559\u6743\u8861\u65b9\u9762\u5177\u6709\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u9002\u914d\u5668\u5185\u5b58\u6548\u7387\u9ad8\u3001\u53ef\u9006\uff0c\u53ef\u5728\u8fd0\u884c\u65f6\u5408\u5e76\u6216\u79fb\u9664\u3002", "conclusion": "FADE\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6982\u5ff5\u64e6\u9664\u548c\u9ad8\u4fdd\u7559\u6027\uff0c\u662f\u6269\u6563\u57fa\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u9009\u62e9\u6027\u9057\u5fd8\u7684\u5408\u9002\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u914d\u5668\u8f7b\u91cf\u4e14\u90e8\u7f72\u7075\u6d3b\u3002"}}
{"id": "2602.07062", "pdf": "https://arxiv.org/pdf/2602.07062", "abs": "https://arxiv.org/abs/2602.07062", "authors": ["Daniil Storonkin", "Ilia Dziub", "Maksim Golyadkin", "Ilya Makarov"], "title": "From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal", "categories": ["cs.CV"], "comment": "AAAI 2026 Workshop on Addressing Challenges and Opportunities in Human-Centric Manufacturing", "summary": "Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.", "AI": {"tldr": "\u5f00\u53d1\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u5b9e\u4f8b\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u4ece\u56fe\u50cf\u4e2d\u81ea\u52a8\u8bc4\u4f30\u5e9f\u94a2\u6c61\u67d3\u7a0b\u5ea6\u548c\u5206\u7c7b\u5e9f\u94a2\u7c7b\u578b\uff0c\u51cf\u5c11\u4e3b\u89c2\u5224\u65ad\u5e76\u63d0\u9ad8\u5b89\u5168\u6027", "motivation": "\u76ee\u524d\u5e9f\u94a2\u8d28\u91cf\u4e3b\u8981\u901a\u8fc7\u4eba\u5de5\u76ee\u89c6\u8bc4\u4f30\u975e\u91d1\u5c5e\u5939\u6742\u7269\u542b\u91cf\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3b\u89c2\u6027\u5f3a\u4e14\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff08\u7c89\u5c18\u548c\u79fb\u52a8\u673a\u68b0\uff09\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u5c11\u4e3b\u89c2\u5dee\u5f02\u5e76\u63d0\u9ad8\u5b89\u5168\u6027", "method": "\u5c06\u6c61\u67d3\u8bc4\u4f30\u6784\u5efa\u4e3a\u8f66\u53a2\u7ea7\u522b\u7684\u56de\u5f52\u4efb\u52a1\uff0c\u91c7\u7528\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u5904\u7406\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u540c\u65f6\u8fdb\u884c\u6c61\u67d3\u8bc4\u4f30\u548c\u5e9f\u94a2\u5206\u7c7b\uff1b\u7cfb\u7edf\u5305\u62ec\u78c1\u94c1/\u8f66\u53a2\u68c0\u6d4b\u3001\u7248\u672c\u5316\u63a8\u7406\u670d\u52a1\u3001\u64cd\u4f5c\u5458\u5ba1\u67e5\u548c\u4e3b\u52a8\u5b66\u4e60\u5faa\u73af", "result": "\u6700\u4f73\u7ed3\u679c\uff1aMIL\u65b9\u6cd5\u8fbe\u5230MAE 0.27\u548cR\u00b2 0.83\uff1bMTL\u8bbe\u7f6e\u8fbe\u5230MAE 0.36\uff0c\u5e9f\u94a2\u5206\u7c7bF1\u5206\u65700.79\uff1b\u7cfb\u7edf\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u5904\u7406\uff0c\u51cf\u5c11\u4e3b\u89c2\u5dee\u5f02\u5e76\u63d0\u9ad8\u5b89\u5168\u6027", "conclusion": "\u63d0\u51fa\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7ba1\u9053\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u5e9f\u94a2\u6c61\u67d3\u7a0b\u5ea6\u548c\u5206\u7c7b\u5e9f\u94a2\u7c7b\u578b\uff0c\u51cf\u5c11\u4e3b\u89c2\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u4eba\u5458\u5b89\u5168\u6027\uff0c\u5e76\u80fd\u591f\u96c6\u6210\u5230\u9a8c\u6536\u548c\u7194\u70bc\u8ba1\u5212\u5de5\u4f5c\u6d41\u7a0b\u4e2d"}}
{"id": "2602.07064", "pdf": "https://arxiv.org/pdf/2602.07064", "abs": "https://arxiv.org/abs/2602.07064", "authors": ["Minghao Han", "Dingkang Yang", "Yue Jiang", "Yizhou Liu", "Lihua Zhang"], "title": "Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine", "categories": ["cs.CV"], "comment": null, "summary": "Physical understanding remains brittle in omni-modal models because key physical attributes are visually ambiguous and sparsely represented in web-scale data. We present OmniFysics, a compact omni-modal model that unifies understanding across images, audio, video, and text, with integrated speech and image generation. To inject explicit physical knowledge, we build a physical data engine with two components. FysicsAny produces physics-grounded instruction--image supervision by mapping salient objects to verified physical attributes through hierarchical retrieval over a curated prototype database, followed by physics-law--constrained verification and caption rewriting. FysicsOmniCap distills web videos via audio--visual consistency filtering to generate high-fidelity video--instruction pairs emphasizing cross-modal physical cues. We train OmniFysics with staged multimodal alignment and instruction tuning, adopt latent-space flow matching for text-to-image generation, and use an intent router to activate generation only when needed. Experiments show competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations.", "AI": {"tldr": "OmniFysics\uff1a\u4e00\u4e2a\u7d27\u51d1\u7684\u5168\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u6570\u636e\u5f15\u64ce\u6ce8\u5165\u663e\u5f0f\u7269\u7406\u77e5\u8bc6\uff0c\u7edf\u4e00\u7406\u89e3\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u548c\u6587\u672c\uff0c\u5e76\u96c6\u6210\u8bed\u97f3\u548c\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u5168\u6a21\u6001\u6a21\u578b\u5728\u7269\u7406\u7406\u89e3\u65b9\u9762\u5b58\u5728\u8106\u5f31\u6027\uff0c\u56e0\u4e3a\u5173\u952e\u7269\u7406\u5c5e\u6027\u5728\u89c6\u89c9\u4e0a\u662f\u6a21\u7cca\u7684\uff0c\u4e14\u5728\u7f51\u7edc\u89c4\u6a21\u6570\u636e\u4e2d\u7a00\u758f\u8868\u793a\u3002\u9700\u8981\u6ce8\u5165\u663e\u5f0f\u7269\u7406\u77e5\u8bc6\u6765\u589e\u5f3a\u6a21\u578b\u7684\u7269\u7406\u7406\u89e3\u80fd\u529b\u3002", "method": "1. \u6784\u5efa\u7269\u7406\u6570\u636e\u5f15\u64ce\uff1aFysicsAny\u901a\u8fc7\u5c42\u6b21\u68c0\u7d22\u5c06\u663e\u8457\u5bf9\u8c61\u6620\u5c04\u5230\u5df2\u9a8c\u8bc1\u7684\u7269\u7406\u5c5e\u6027\uff0c\u751f\u6210\u7269\u7406\u57fa\u7840\u7684\u6307\u4ee4-\u56fe\u50cf\u76d1\u7763\uff1bFysicsOmniCap\u901a\u8fc7\u97f3\u9891-\u89c6\u89c9\u4e00\u81f4\u6027\u8fc7\u6ee4\u84b8\u998f\u7f51\u7edc\u89c6\u9891\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891-\u6307\u4ee4\u5bf9\u30022. \u91c7\u7528\u5206\u9636\u6bb5\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u6307\u4ee4\u8c03\u4f18\u8bad\u7ec3OmniFysics\u6a21\u578b\u30023. \u4f7f\u7528\u6f5c\u5728\u7a7a\u95f4\u6d41\u5339\u914d\u8fdb\u884c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u30024. \u4f7f\u7528\u610f\u56fe\u8def\u7531\u5668\u4ec5\u5728\u9700\u8981\u65f6\u6fc0\u6d3b\u751f\u6210\u529f\u80fd\u3002", "result": "\u5728\u6807\u51c6\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff0c\u5728\u7269\u7406\u5bfc\u5411\u8bc4\u4f30\u4e2d\u7ed3\u679c\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "OmniFysics\u901a\u8fc7\u6ce8\u5165\u663e\u5f0f\u7269\u7406\u77e5\u8bc6\uff0c\u589e\u5f3a\u4e86\u5168\u6a21\u6001\u6a21\u578b\u7684\u7269\u7406\u7406\u89e3\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u7d27\u51d1\u6a21\u578b\u89c4\u6a21\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2602.07065", "pdf": "https://arxiv.org/pdf/2602.07065", "abs": "https://arxiv.org/abs/2602.07065", "authors": ["A. N. Maria Antony", "T. Richter", "E. Gladilin"], "title": "Contactless estimation of continuum displacement and mechanical compressibility from image series using a deep learning based framework", "categories": ["cs.CV"], "comment": "14 Pages, 8 Figures Note: Supplentary information (ancillary file) attached as .pdf", "summary": "Contactless and non-invasive estimation of mechanical properties of physical media from optical observations is of interest for manifold engineering and biomedical applications, where direct physical measurements are not possible. Conventional approaches to the assessment of image displacement and non-contact material probing typically rely on time-consuming iterative algorithms for non-rigid image registration and constitutive modelling using discretization and iterative numerical solving techniques, such as Finite Element Method (FEM) and Finite Difference Method (FDM), which are not suitable for high-throughput data processing. Here, we present an efficient deep learning based end-to-end approach for the estimation of continuum displacement and material compressibility directly from the image series. Based on two deep neural networks for image registration and material compressibility estimation, this framework outperforms conventional approaches in terms of efficiency and accuracy. In particular, our experimental results show that the deep learning model trained on a set of reference data can accurately determine the material compressibility even in the presence of substantial local deviations of the mapping predicted by image registration from the reference displacement field. Our findings suggest that the remarkable accuracy of the deep learning end-to-end model originates from its ability to assess higher-order cognitive features, such as the vorticity of the vector field, rather than conventional local features of the image displacement.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u56fe\u50cf\u5e8f\u5217\u4f30\u8ba1\u8fde\u7eed\u4f4d\u79fb\u548c\u6750\u6599\u538b\u7f29\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u90fd\u6709\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u975e\u63a5\u89e6\u5f0f\u6750\u6599\u529b\u5b66\u6027\u80fd\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u8017\u65f6\u7684\u8fed\u4ee3\u7b97\u6cd5\u548c\u975e\u521a\u6027\u56fe\u50cf\u914d\u51c6\uff0c\u4e0d\u9002\u5408\u9ad8\u901a\u91cf\u6570\u636e\u5904\u7406\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u8fdb\u884c\u56fe\u50cf\u914d\u51c6\u548c\u6750\u6599\u538b\u7f29\u6027\u4f30\u8ba1\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u6846\u67b6\u76f4\u63a5\u4ece\u56fe\u50cf\u5e8f\u5217\u9884\u6d4b\u4f4d\u79fb\u573a\u548c\u6750\u6599\u538b\u7f29\u6027\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u53c2\u8003\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u5373\u4f7f\u5728\u56fe\u50cf\u914d\u51c6\u9884\u6d4b\u7684\u6620\u5c04\u4e0e\u53c2\u8003\u4f4d\u79fb\u573a\u5b58\u5728\u663e\u8457\u5c40\u90e8\u504f\u5dee\u65f6\uff0c\u4e5f\u80fd\u51c6\u786e\u786e\u5b9a\u6750\u6599\u538b\u7f29\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u7aef\u5230\u7aef\u6a21\u578b\u7684\u5353\u8d8a\u7cbe\u5ea6\u6e90\u4e8e\u5176\u8bc4\u4f30\u9ad8\u9636\u8ba4\u77e5\u7279\u5f81\uff08\u5982\u77e2\u91cf\u573a\u7684\u6da1\u5ea6\uff09\u7684\u80fd\u529b\uff0c\u800c\u975e\u4f20\u7edf\u7684\u56fe\u50cf\u4f4d\u79fb\u5c40\u90e8\u7279\u5f81\u3002"}}
{"id": "2602.07069", "pdf": "https://arxiv.org/pdf/2602.07069", "abs": "https://arxiv.org/abs/2602.07069", "authors": ["Zihao Fan", "Xin Lu", "Yidi Liu", "Jie Huang", "Dong Li", "Xueyang Fu", "Zheng-Jun Zha"], "title": "Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.", "AI": {"tldr": "Bird-SR\uff1a\u57fa\u4e8e\u53cc\u5411\u5956\u52b1\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u53cd\u9988\u5b66\u4e60\u5c06\u8d85\u5206\u8fa8\u7387\u5efa\u6a21\u4e3a\u8f68\u8ff9\u7ea7\u504f\u597d\u4f18\u5316\uff0c\u8054\u5408\u5229\u7528\u5408\u6210LR-HR\u5bf9\u548c\u771f\u5b9e\u4e16\u754cLR\u56fe\u50cf\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u5347\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u80fd\u5408\u6210\u4e30\u5bcc\u7ec6\u8282\uff0c\u4f46\u5728\u5408\u6210\u914d\u5bf9\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5f80\u5f80\u56e0\u5206\u5e03\u504f\u79fb\u800c\u5728\u771f\u5b9e\u4e16\u754c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u5931\u6548\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBird-SR\u6846\u67b6\uff1a1\uff09\u65e9\u671f\u6269\u6563\u6b65\u5728\u5408\u6210\u5bf9\u4e0a\u76f4\u63a5\u4f18\u5316\u4ee5\u4fdd\u8bc1\u7ed3\u6784\u4fdd\u771f\u5ea6\uff1b2\uff09\u540e\u671f\u91c7\u6837\u6b65\u5bf9\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u5e94\u7528\u8d28\u91cf\u5f15\u5bfc\u5956\u52b1\uff1b3\uff09\u901a\u8fc7\u76f8\u5bf9\u4f18\u52bf\u7a7a\u95f4\u548c\u8bed\u4e49\u5bf9\u9f50\u7ea6\u675f\u9632\u6b62\u5956\u52b1\u653b\u51fb\uff1b4\uff09\u91c7\u7528\u52a8\u6001\u4fdd\u771f\u5ea6-\u611f\u77e5\u6743\u91cd\u7b56\u7565\u5e73\u8861\u7ed3\u6784\u4fdd\u6301\u548c\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBird-SR\u5728\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "conclusion": "Bird-SR\u901a\u8fc7\u5956\u52b1\u53cd\u9988\u5b66\u4e60\u548c\u52a8\u6001\u5e73\u8861\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u4e2d\u7ed3\u6784\u4fdd\u771f\u4e0e\u611f\u77e5\u8d28\u91cf\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07082", "pdf": "https://arxiv.org/pdf/2602.07082", "abs": "https://arxiv.org/abs/2602.07082", "authors": ["Haoming Wang", "Qiyao Xue", "Weichen Liu", "Wei Gao"], "title": "MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \\emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.", "AI": {"tldr": "MosaicThinker\uff1a\u4e00\u79cd\u7528\u4e8e\u8bbe\u5907\u7aef\u5177\u8eabAI\u7684\u63a8\u7406\u65f6\u8ba1\u7b97\u6280\u672f\uff0c\u901a\u8fc7\u6574\u5408\u591a\u5e27\u7a7a\u95f4\u4fe1\u606f\u5230\u7edf\u4e00\u8bed\u4e49\u5730\u56fe\u6765\u589e\u5f3a\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u80fd\u529b\u8f83\u5f31\uff0c\u5c24\u5176\u662f\u5904\u7406\u6d89\u53ca\u591a\u5e27\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u7684\u4efb\u52a1\u65f6\uff0c\u7f3a\u4e4f3D\u7a7a\u95f4\u4fe1\u606f\u77e5\u8bc6\uff0c\u800c\u8bbe\u5907\u7aef\u5177\u8eabAI\u9700\u8981\u8fd9\u79cd\u80fd\u529b\u6765\u6307\u5bfc\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u52a8\u4f5c\u89c4\u5212", "method": "\u63d0\u51faMosaicThinker\u6280\u672f\uff0c\u5c06\u591a\u5e27\u788e\u7247\u5316\u7684\u7a7a\u95f4\u4fe1\u606f\u6574\u5408\u5230\u7edf\u4e00\u7684\u5168\u5c40\u8bed\u4e49\u5730\u56fe\u8868\u793a\u4e2d\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u5f15\u5bfc\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u5730\u56fe\u4e0a\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8de8\u5e27\u7a7a\u95f4\u63a8\u7406\u7684\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7c7b\u578b\u548c\u590d\u6742\u5ea6\u7684\u63a8\u7406\u4efb\u52a1", "conclusion": "MosaicThinker\u901a\u8fc7\u6574\u5408\u591a\u5e27\u7a7a\u95f4\u4fe1\u606f\u5230\u7edf\u4e00\u8bed\u4e49\u5730\u56fe\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u8bbe\u5907\u7aef\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u5e27\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5177\u8eabAI\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.07095", "pdf": "https://arxiv.org/pdf/2602.07095", "abs": "https://arxiv.org/abs/2602.07095", "authors": ["Wang Lin", "Feng Wang", "Majun Zhang", "Wentao Hu", "Tao Jin", "Zhou Zhao", "Fei Wu", "Jingyuan Chen", "Alan Yuille", "Sucheng Ren"], "title": "WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \\textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \\textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.", "AI": {"tldr": "WorldEdit\u662f\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u4e16\u754c\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u63d0\u5347\u6a21\u578b\u5904\u7406\u9690\u5f0f\u7f16\u8f91\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0eGPT-4o\u7b49\u5148\u8fdb\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5904\u7406\u663e\u5f0f\u6307\u4ee4\uff08\u5982\u5c5e\u6027\u64cd\u4f5c\u3001\u98ce\u683c\u8f6c\u6362\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u9690\u5f0f\u7f16\u8f91\u6307\u4ee4\uff08\u63cf\u8ff0\u89c6\u89c9\u53d8\u5316\u7684\u539f\u56e0\u800c\u975e\u5177\u4f53\u7ed3\u679c\uff09\u65f6\u9762\u4e34\u6311\u6218\u3002\u8fd9\u4e9b\u6a21\u578b\u4f9d\u8d56\u7edf\u4e00\u7684\u7f16\u8f91\u7b56\u7565\uff0c\u7f3a\u4e4f\u5904\u7406\u590d\u6742\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "1) \u5f15\u5165WorldEdit\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u7f16\u8f91\u6837\u672c\uff0c\u6307\u4ee4\u7ecf\u8fc7\u6539\u5199\u4ee5\u7b26\u5408\u73b0\u5b9e\u4e16\u754c\u56e0\u679c\u903b\u8f91\uff1b2) \u63d0\u4f9bWorldEdit-Test\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u56e0\u679c\u7f16\u8f91\u573a\u666f\u7684\u6027\u80fd\uff1b3) \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u5fae\u8c03Bagel\u7b49\u6a21\u578b\uff0c\u5e76\u6574\u5408\u56e0\u679c\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\u3002", "result": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86\u4e0eGPT-4o\u548cNano-Banana\u7684\u5dee\u8ddd\uff0c\u5728\u6307\u4ee4\u9075\u5faa\u548c\u77e5\u8bc6\u5408\u7406\u6027\u65b9\u9762\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u800c\u8fd9\u4e24\u4e2a\u65b9\u9762\u901a\u5e38\u662f\u5f00\u6e90\u7cfb\u7edf\u7684\u8584\u5f31\u73af\u8282\u3002", "conclusion": "WorldEdit\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5904\u7406\u9690\u5f0f\u6307\u4ee4\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6574\u5408\u4e16\u754c\u77e5\u8bc6\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.07100", "pdf": "https://arxiv.org/pdf/2602.07100", "abs": "https://arxiv.org/abs/2602.07100", "authors": ["Biao Xiong", "Zhen Peng", "Ping Wang", "Qiegen Liu", "Xian Zhong"], "title": "TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation", "categories": ["cs.CV"], "comment": null, "summary": "Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.", "AI": {"tldr": "TLC-Plan\u662f\u4e00\u4e2a\u76f4\u63a5\u4ece\u8fb9\u754c\u8f93\u5165\u751f\u6210\u77e2\u91cf\u5e73\u9762\u56fe\u7684\u5c42\u6b21\u5316\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u7ea7VQ-VAE\u548c\u81ea\u56de\u5f52transformer\uff0c\u5728RPLAN\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff08FID=1.84\uff0cMSE=2.06\uff09\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6805\u683c\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u5e76\u4f9d\u8d56\u540e\u5904\u7406\u77e2\u91cf\u5316\uff0c\u5bfc\u81f4\u7ed3\u6784\u4e0d\u4e00\u81f4\u5e76\u963b\u788d\u7aef\u5230\u7aef\u5b66\u4e60\u3002\u53d7\u7ec4\u5408\u7a7a\u95f4\u63a8\u7406\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4e0e\u4eba\u7c7b\u5efa\u7b51\u5de5\u4f5c\u6d41\u7a0b\uff08\u57fa\u4e8e\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u6a21\u5f0f\uff09\u5bf9\u9f50\u7684\u76f4\u63a5\u77e2\u91cf\u5e73\u9762\u56fe\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTLC-Plan\uff1a1\uff09\u4f7f\u7528\u4e24\u7ea7VQ-VAE\u7f16\u7801\u5168\u5c40\u5e03\u5c40\uff08\u8bed\u4e49\u6807\u8bb0\u7684\u623f\u95f4\u8fb9\u754c\u6846\uff09\u548c\u7ec6\u5316\u5c40\u90e8\u51e0\u4f55\uff08\u591a\u8fb9\u5f62\u7ea7\u7f16\u7801\uff09\uff1b2\uff09\u7edf\u4e00\u4e3aCodeTree\u8868\u793a\uff1b3\uff09\u4f7f\u7528\u81ea\u56de\u5f52transformer\u5728\u8fb9\u754c\u6761\u4ef6\u4e0b\u91c7\u6837\u7f16\u7801\uff0c\u751f\u6210\u591a\u6837\u4e14\u62d3\u6251\u6709\u6548\u7684\u8bbe\u8ba1\uff0c\u65e0\u9700\u663e\u5f0f\u623f\u95f4\u62d3\u6251\u6216\u7ef4\u5ea6\u5148\u9a8c\u3002", "result": "\u5728RPLAN\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff08FID=1.84\uff0cMSE=2.06\uff09\uff0c\u5728LIFULL\u6570\u636e\u96c6\u4e0a\u4e5f\u53d6\u5f97\u9886\u5148\u7ed3\u679c\u3002\u6846\u67b6\u63a8\u8fdb\u4e86\u7ea6\u675f\u611f\u77e5\u548c\u53ef\u6269\u5c55\u7684\u77e2\u91cf\u5e73\u9762\u56fe\u751f\u6210\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5efa\u7b51\u5e94\u7528\u3002", "conclusion": "TLC-Plan\u901a\u8fc7\u5c42\u6b21\u5316\u751f\u6210\u6a21\u578b\u76f4\u63a5\u5408\u6210\u77e2\u91cf\u5e73\u9762\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u5efa\u7b51\u5de5\u4f5c\u6d41\u7a0b\u5bf9\u9f50\u7684\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u4e3a\u5b9e\u9645\u5efa\u7b51\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07101", "pdf": "https://arxiv.org/pdf/2602.07101", "abs": "https://arxiv.org/abs/2602.07101", "authors": ["Zinan Lv", "Yeqian Qian", "Chen Sang", "Hao Liu", "Danping Zou", "Ming Yang"], "title": "Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting", "categories": ["cs.CV", "cs.RO"], "comment": "12 pages, 8 figures", "summary": "UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u91cd\u5149\u71673D\u9ad8\u65af\u6cfc\u6e85\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u4e2d\u7684\u96f6\u6837\u672c\u5bfc\u822a\uff0c\u514b\u670d\u5149\u7167\u53d8\u5316\u6311\u6218", "motivation": "\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u4e2d\u4f7f\u7528\u5355\u76ee\u89c6\u89c9\u5bfc\u822a\u9762\u4e34\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u89c6\u89c9\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5c06\u9759\u6001\u5149\u7167\u4e0e\u51e0\u4f55\u8026\u5408\uff0c\u9650\u5236\u4e86\u7b56\u7565\u5728\u52a8\u6001\u771f\u5b9e\u5149\u7167\u4e0b\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51fa\u53ef\u91cd\u5149\u71673D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5206\u89e3\u573a\u666f\u7ec4\u4ef6\u5b9e\u73b0\u7269\u7406\u57fa\u7840\u7684\u5149\u7167\u7f16\u8f91\uff1b\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u4e2d\u8bad\u7ec3\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u5408\u6210\u5149\u7167\u6761\u4ef6\u589e\u5f3a\u8bad\u7ec3\uff0c\u5b66\u4e60\u5149\u7167\u4e0d\u53d8\u7684\u89c6\u89c9\u7279\u5f81", "result": "\u8f7b\u91cf\u7ea7\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u590d\u6742\u68ee\u6797\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u8fbe10\u7c73/\u79d2\u7684\u9c81\u68d2\u3001\u65e0\u78b0\u649e\u5bfc\u822a\uff0c\u5bf9\u5267\u70c8\u5149\u7167\u53d8\u5316\u8868\u73b0\u51fa\u663e\u8457\u97e7\u6027\uff0c\u65e0\u9700\u5fae\u8c03", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u89c6\u89c9\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u91cd\u5149\u7167\u795e\u7ecf\u8868\u793a\u548c\u5149\u7167\u589e\u5f3a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u5728\u52a8\u6001\u771f\u5b9e\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u96f6\u6837\u672c\u5bfc\u822a\u80fd\u529b"}}
{"id": "2602.07104", "pdf": "https://arxiv.org/pdf/2602.07104", "abs": "https://arxiv.org/abs/2602.07104", "authors": ["Zhuoheng Li", "Ying Chen"], "title": "Extended to Reality: Prompt Injection in 3D Environments", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.", "AI": {"tldr": "PI3D\u662f\u4e00\u79cd\u9488\u5bf93D\u73af\u5883\u4e2d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u901a\u8fc7\u653e\u7f6e\u5e26\u6709\u6587\u672c\u7684\u7269\u7406\u7269\u4f53\u6765\u8986\u76d6\u6a21\u578b\u7684\u539f\u59cb\u4efb\u52a1", "motivation": "\u968f\u7740MLLMs\u57283D\u73af\u5883\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff08\u5982\u673a\u5668\u4eba\u3001\u5bf9\u8bdd\u4ee3\u7406\uff09\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u653e\u7f6e\u7269\u7406\u7269\u4f53\u6765\u64cd\u7eb5\u6a21\u578b\u884c\u4e3a\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u57df\u548c2D\u6570\u5b57\u56fe\u50cf\u653b\u51fb\uff0c\u7f3a\u4e4f\u5bf93D\u7269\u7406\u73af\u5883\u653b\u51fb\u7684\u7814\u7a76", "method": "\u63d0\u51faPI3D\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u53163D\u7269\u4f53\u59ff\u6001\uff08\u4f4d\u7f6e\u548c\u65b9\u5411\uff09\u6765\u653e\u7f6e\u5e26\u6709\u6ce8\u5165\u6587\u672c\u7684\u7269\u7406\u7269\u4f53\uff0c\u4f7f\u653b\u51fb\u65e2\u6709\u6548\u53c8\u7269\u7406\u53ef\u884c", "result": "PI3D\u5bf9\u591a\u79cdMLLMs\u5728\u4e0d\u540c\u76f8\u673a\u8f68\u8ff9\u4e0b\u90fd\u6709\u6548\uff0c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u65e0\u6cd5\u6709\u6548\u62b5\u5fa1\u8fd9\u79cd\u653b\u51fb", "conclusion": "3D\u7269\u7406\u73af\u5883\u4e2d\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u662f\u4e00\u4e2a\u65b0\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u9632\u5fa1\u673a\u5236\u6765\u4fdd\u62a4MLLMs\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u5e94\u7528"}}
{"id": "2602.07106", "pdf": "https://arxiv.org/pdf/2602.07106", "abs": "https://arxiv.org/abs/2602.07106", "authors": ["Haoyu Zhang", "Zhipeng Li", "Yiwen Guo", "Tianshu Yu"], "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.", "AI": {"tldr": "Ex-Omni\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5168\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u751f\u6210\uff0c\u5229\u7528\u8bed\u97f3\u5355\u5143\u4f5c\u4e3a\u65f6\u95f4\u652f\u67b6\u548c\u7edf\u4e00\u7684token-as-query\u95e8\u63a7\u878d\u5408\u673a\u5236\uff0c\u4e3a\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6dfb\u52a0\u8bed\u97f3\u4f34\u968f\u76843D\u9762\u90e8\u52a8\u753b\u529f\u80fd\u3002", "motivation": "\u5f53\u524d\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u4f46\u8bed\u97f3\u4e0e3D\u9762\u90e8\u52a8\u753b\u7684\u7ed3\u5408\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u800c\u8fd9\u5bf9\u4e8e\u81ea\u7136\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8eLLMs\u7684\u79bb\u6563token\u7ea7\u8bed\u4e49\u63a8\u7406\u4e0e3D\u9762\u90e8\u8fd0\u52a8\u6240\u9700\u7684\u5bc6\u96c6\u7ec6\u7c92\u5ea6\u65f6\u95f4\u52a8\u6001\u4e4b\u95f4\u5b58\u5728\u8868\u793a\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faEx-Omni\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u751f\u6210\u6765\u964d\u4f4e\u5b66\u4e60\u96be\u5ea6\uff1a1) \u4f7f\u7528\u8bed\u97f3\u5355\u5143\u4f5c\u4e3a\u65f6\u95f4\u652f\u67b6\uff1b2) \u91c7\u7528\u7edf\u4e00\u7684token-as-query\u95e8\u63a7\u878d\u5408\u673a\u5236\u8fdb\u884c\u53d7\u63a7\u8bed\u4e49\u6ce8\u5165\uff1b3) \u5f15\u5165InstructEx\u6570\u636e\u96c6\u6765\u652f\u6301\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEx-Omni\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u5f00\u6e90\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u591f\u7a33\u5b9a\u751f\u6210\u5bf9\u9f50\u7684\u8bed\u97f3\u548c\u9762\u90e8\u52a8\u753b\u3002", "conclusion": "Ex-Omni\u6210\u529f\u89e3\u51b3\u4e86\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8bed\u97f3\u4e0e3D\u9762\u90e8\u52a8\u753b\u878d\u5408\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u89e3\u8026\u67b6\u6784\u548c\u878d\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2602.07149", "pdf": "https://arxiv.org/pdf/2602.07149", "abs": "https://arxiv.org/abs/2602.07149", "authors": ["Rawisara Lohanimit", "Yankun Wu", "Amelia Katirai", "Yuta Nakashima", "Noa Garcia"], "title": "Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds", "categories": ["cs.CV"], "comment": null, "summary": "The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LAION-400M\u6570\u636e\u96c6\u4e2d\u5305\u542b\u5927\u91cf\u654f\u611f\u6000\u5b55\u8d85\u58f0\u56fe\u50cf\u548c\u4e2a\u4eba\u9690\u79c1\u4fe1\u606f\uff0c\u5b58\u5728\u9ad8\u98ce\u9669\u6570\u636e\u6cc4\u9732\u95ee\u9898", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u4f7f\u7528\u5927\u89c4\u6a21\u7f51\u7edc\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u6570\u636e\u7b5b\u9009\u5bfc\u81f4\u654f\u611f\u9690\u79c1\u4fe1\u606f\u88ab\u7eb3\u5165\uff0c\u7279\u522b\u662f\u5305\u542b\u4e2a\u4eba\u654f\u611f\u4fe1\u606f\u7684\u6000\u5b55\u8d85\u58f0\u56fe\u50cf", "method": "\u4f7f\u7528CLIP\u5d4c\u5165\u76f8\u4f3c\u6027\u5bf9LAION-400M\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u68c0\u67e5\uff0c\u68c0\u7d22\u6000\u5b55\u8d85\u58f0\u56fe\u50cf\u5e76\u68c0\u6d4b\u59d3\u540d\u3001\u4f4d\u7f6e\u7b49\u9690\u79c1\u4fe1\u606f", "result": "\u53d1\u73b0\u6570\u5343\u6761\u5305\u542b\u59d3\u540d\u3001\u4f4d\u7f6e\u7b49\u9690\u79c1\u4fe1\u606f\u7684\u5b9e\u4f53\uff0c\u591a\u4e2a\u56fe\u50cf\u5305\u542b\u9ad8\u98ce\u9669\u4fe1\u606f\uff0c\u53ef\u80fd\u5bfc\u81f4\u91cd\u65b0\u8bc6\u522b\u6216\u8eab\u4efd\u5192\u5145", "conclusion": "\u5efa\u8bae\u6539\u8fdb\u6570\u636e\u96c6\u7b5b\u9009\u3001\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u548c\u516c\u5171\u56fe\u50cf\u6570\u636e\u96c6\u7684\u4f26\u7406\u4f7f\u7528\u5b9e\u8df5"}}
{"id": "2602.07174", "pdf": "https://arxiv.org/pdf/2602.07174", "abs": "https://arxiv.org/abs/2602.07174", "authors": ["Yongheng Sun", "Jun Shu", "Jianhua Ma", "Fan Wang"], "title": "DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \\emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.", "AI": {"tldr": "DuMeta++\uff1a\u65e0\u9700\u914d\u5bf9\u7eb5\u5411\u6570\u636e\u7684\u53cc\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u5e74\u9f84\u8111\u7ec4\u7ec7MRI\u5206\u5272\uff0c\u901a\u8fc7\u5143\u7279\u5f81\u5b66\u4e60\u548c\u5143\u521d\u59cb\u5316\u5b66\u4e60\u5b9e\u73b0\u8de8\u5e74\u9f84\u6cdb\u5316", "motivation": "\u8111\u7ec4\u7ec7MRI\u5206\u5272\u5728\u795e\u7ecf\u79d1\u5b66\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5927\u8111\u5916\u89c2\u548c\u5f62\u6001\u968f\u5e74\u9f84\u52a8\u6001\u53d8\u5316\uff0c\u5b9e\u73b0\u8de8\u4eba\u7c7b\u751f\u547d\u5468\u671f\u7684\u7a33\u5b9a\u6027\u80fd\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u914d\u5bf9\u7eb5\u5411\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u6b63\u5219\u5316\uff0c\u4f46\u8fd9\u7c7b\u6570\u636e\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u63d0\u51faDuMeta++\u53cc\u5143\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u5143\u7279\u5f81\u5b66\u4e60\u63d0\u53d6\u5e74\u9f84\u65e0\u5173\u7684\u65f6\u7a7a\u6f14\u5316\u8111\u7ed3\u6784\u8bed\u4e49\u8868\u793a\uff1b2\uff09\u5143\u521d\u59cb\u5316\u5b66\u4e60\u5b9e\u73b0\u5206\u5272\u6a21\u578b\u7684\u6570\u636e\u9ad8\u6548\u9002\u5e94\uff1b3\uff09\u57fa\u4e8e\u8bb0\u5fc6\u5e93\u7684\u7c7b\u611f\u77e5\u6b63\u5219\u5316\u7b56\u7565\uff0c\u65e0\u9700\u663e\u5f0f\u7eb5\u5411\u76d1\u7763\u5373\u53ef\u5f3a\u5236\u7eb5\u5411\u4e00\u81f4\u6027\u3002\u7406\u8bba\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "result": "\u5728iSeg-2019\u3001IBIS\u3001OASIS\u3001ADNI\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5c11\u6837\u672c\u8bbe\u7f6e\u5b9e\u9a8c\u4e2d\uff0cDuMeta++\u5728\u8de8\u5e74\u9f84\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DuMeta++\u65e0\u9700\u914d\u5bf9\u7eb5\u5411\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u8de8\u5e74\u9f84\u8111\u7ec4\u7ec7\u5206\u5272\uff0c\u901a\u8fc7\u53cc\u5143\u5b66\u4e60\u548c\u8bb0\u5fc6\u5e93\u6b63\u5219\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5e74\u9f84\u76f8\u5173\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u8de8\u5e74\u9f84\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.07198", "pdf": "https://arxiv.org/pdf/2602.07198", "abs": "https://arxiv.org/abs/2602.07198", "authors": ["Heyuan Li", "Huimin Zhang", "Yuda Qiu", "Zhengwentai Sun", "Keru Zheng", "Lingteng Qiu", "Peihao Li", "Qi Zuo", "Ce Chen", "Yujian Zheng", "Yuming Gu", "Zilong Dong", "Xiaoguang Han"], "title": "Condition Matters in Full-head 3D GANs", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted by ICLR 2026. Project page: https://lhyfst.github.io/balancehead/", "summary": "Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u89c6\u89d2\u4e0d\u53d8\u8bed\u4e49\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u89e3\u51b3\u4f20\u7edf3D\u5934\u90e8GAN\u4e2d\u89c6\u89d2\u6761\u4ef6\u5bfc\u81f4\u7684\u751f\u6210\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u53473D\u5934\u90e8\u751f\u6210\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf3D\u5934\u90e8GAN\u4f7f\u7528\u89c6\u89d2\u89d2\u5ea6\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u4f1a\u5bfc\u81f4\u5b66\u4e60\u5230\u76843D\u5934\u90e8\u7a7a\u95f4\u5b58\u5728\u89c6\u89d2\u65b9\u5411\u504f\u5dee\uff0c\u9020\u6210\u6761\u4ef6\u89c6\u89d2\u4e0e\u975e\u6761\u4ef6\u89c6\u89d2\u4e4b\u95f4\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u4e0d\u540c\u5934\u90e8\u533a\u57df\u7684\u5168\u5c40\u4e0d\u4e00\u81f4\u6027\u3002", "method": "1) \u4f7f\u7528\u89c6\u89d2\u4e0d\u53d8\u8bed\u4e49\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u5c063D\u5934\u90e8\u7684\u751f\u6210\u80fd\u529b\u4e0e\u89c6\u89d2\u65b9\u5411\u89e3\u8026\uff1b2) \u521b\u5efa\u5408\u6210\u5934\u90e8\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5229\u7528FLUX.1 Kontext\u5c06\u73b0\u6709\u9ad8\u8d28\u91cf\u6b63\u9762\u4eba\u8138\u6570\u636e\u96c6\u6269\u5c55\u5230\u591a\u89c6\u89d2\uff1b3) \u4f7f\u7528\u6b63\u9762\u89c6\u56fe\u63d0\u53d6\u7684\u56fe\u50cfclip\u7279\u5f81\u4f5c\u4e3a\u6240\u6709\u6269\u5c55\u56fe\u50cf\u89c6\u56fe\u7684\u5171\u4eab\u8bed\u4e49\u6761\u4ef6\uff0c\u786e\u4fdd\u8bed\u4e49\u5bf9\u9f50\u5e76\u6d88\u9664\u65b9\u5411\u504f\u5dee\u3002", "result": "\u5728\u5b8c\u6574\u5934\u90e8\u5408\u6210\u548c\u5355\u89c6\u56feGAN\u53cd\u8f6c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8bed\u4e49\u6761\u4ef6\u76d1\u7763\u52a0\u901f\u4e86\u8bad\u7ec3\uff0c\u589e\u5f3a\u4e86\u751f\u62103D\u5934\u90e8\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u89c6\u89d2\u4e0d\u53d8\u8bed\u4e49\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf3D\u5934\u90e8GAN\u4e2d\u7684\u89c6\u89d2\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u4e00\u81f4\u4e14\u66f4\u591a\u6837\u5316\u76843D\u5934\u90e8\u751f\u6210\uff0c\u540c\u65f6\u8bed\u4e49\u6761\u4ef6\u4fc3\u8fdb\u4e86\u751f\u6210\u5668\u9075\u5faa\u771f\u5b9e\u8bed\u4e49\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u6301\u7eed\u5b66\u4e60\u548c\u591a\u6837\u5316\u751f\u6210\u3002"}}
{"id": "2602.07212", "pdf": "https://arxiv.org/pdf/2602.07212", "abs": "https://arxiv.org/abs/2602.07212", "authors": ["Xinyu Liu", "Darryl C. Jacob", "Yuxin Liu", "Xinsong Du", "Muchao Ye", "Bolei Zhou", "Pan He"], "title": "Understanding Real-World Traffic Safety through RoadSafe365 Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.", "AI": {"tldr": "RoadSafe365\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\uff0c\u5305\u542b36,196\u4e2a\u6807\u6ce8\u89c6\u9891\u7247\u6bb5\u548c864K\u5019\u9009\u9009\u9879\uff0c\u57fa\u4e8e\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u6784\u5efa", "motivation": "\u73b0\u6709\u4ea4\u901a\u57fa\u51c6\u7f3a\u4e4f\u4e0e\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u7684\u7cfb\u7edf\u6027\u5bf9\u9f50\u8bc4\u4f30\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4ee5\u8fde\u63a5\u5b98\u65b9\u4ea4\u901a\u5b89\u5168\u6807\u51c6\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u7406\u89e3\u7cfb\u7edf", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\uff0c\u91c7\u7528\u5206\u5c42\u5206\u7c7b\u6cd5\u7cbe\u70bc\u548c\u6269\u5c55\u4e8b\u6545\u3001\u4e8b\u4ef6\u548c\u8fdd\u89c4\u7684\u57fa\u7840\u5b9a\u4e49\uff0c\u4ece\u884c\u8f66\u8bb0\u5f55\u4eea\u548c\u76d1\u63a7\u6444\u50cf\u5934\u6536\u96c6\u6570\u636e\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u5c5e\u6027\u6807\u6ce8\u548c\u591a\u9009\u9898\u96c6", "result": "\u5efa\u7acb\u4e8636,196\u4e2a\u6807\u6ce8\u7247\u6bb5\uff0c\u5305\u542b864K\u5019\u9009\u9009\u9879\u30018.4K\u72ec\u7279\u7b54\u6848\u548c36K\u8be6\u7ec6\u573a\u666f\u63cf\u8ff0\uff0c\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5fae\u8c03\u540e\u6027\u80fd\u6709\u6301\u7eed\u63d0\u5347\uff0c\u8de8\u57df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "conclusion": "RoadSafe365\u4e3a\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u53ef\u63a8\u8fdb\u73b0\u5b9e\u4e16\u754c\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\u7684\u53ef\u91cd\u590d\u7814\u7a76"}}
{"id": "2602.07251", "pdf": "https://arxiv.org/pdf/2602.07251", "abs": "https://arxiv.org/abs/2602.07251", "authors": ["Haley Duba-Sullivan", "Steven R. Young", "Emma J. Reid"], "title": "The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.", "AI": {"tldr": "AdvSR\u662f\u4e00\u79cd\u5c06\u5bf9\u6297\u884c\u4e3a\u5d4c\u5165\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u6743\u91cd\u7684\u653b\u51fb\u6846\u67b6\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u8bbf\u95ee\u8f93\u5165\uff0c\u53ef\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u8bf1\u5bfc\u4e0b\u6e38\u5206\u7c7b\u5668\u8bef\u5224", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5e38\u4f5c\u4e3a\u6210\u50cf\u7ba1\u9053\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u4f46\u8fd9\u7c7b\u6a21\u578b\u5f15\u5165\u4e86\u4e00\u4e2a\u672a\u88ab\u63a2\u7d22\u7684\u653b\u51fb\u9762\u3002\u4f20\u7edf\u5bf9\u6297\u653b\u51fb\u9700\u8981\u6270\u52a8\u8f93\u5165\u6216\u4f9d\u8d56\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u800c\u672c\u6587\u7814\u7a76\u662f\u5426\u80fd\u5728\u6a21\u578b\u5c42\u9762\u76f4\u63a5\u5d4c\u5165\u5bf9\u6297\u884c\u4e3a", "method": "\u63d0\u51faAdvSR\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u8054\u5408\u4f18\u5316\u91cd\u5efa\u8d28\u91cf\u548c\u76ee\u6807\u5bf9\u6297\u6548\u679c\uff0c\u5c06\u5bf9\u6297\u884c\u4e3a\u76f4\u63a5\u5d4c\u5165SR\u6a21\u578b\u6743\u91cd\u4e2d\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u63a8\u7406\u65f6\u8f93\u5165\u8bbf\u95ee\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cdSR\u67b6\u6784\uff08SRCNN\u3001EDSR\u3001SwinIR\uff09\u4e0eYOLOv11\u5206\u7c7b\u5668\u7684\u7ec4\u5408", "result": "AdvSR\u6a21\u578b\u5728\u6807\u51c6\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0b\u8868\u73b0\u6b63\u5e38\uff0c\u4f46\u80fd\u6709\u6548\u8bf1\u5bfc\u4e0b\u6e38\u5206\u7c7b\u5668\u8bef\u5224\uff0c\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u4e14\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u6700\u5c0f\u3002\u8fd9\u63ed\u793a\u4e86\u6210\u50cf\u7ba1\u9053\u4e2d\u65b0\u7684\u6a21\u578b\u7ea7\u5a01\u80c1", "conclusion": "AdvSR\u5c55\u793a\u4e86\u6a21\u578b\u7ea7\u5bf9\u6297\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u6a21\u578b\u6765\u6e90\u548c\u9a8c\u8bc1\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u6210\u50cf\u7ba1\u9053\u4e2d\u9700\u8981\u66f4\u4e25\u683c\u7684\u6a21\u578b\u5b89\u5168\u8bc4\u4f30"}}
{"id": "2602.07260", "pdf": "https://arxiv.org/pdf/2602.07260", "abs": "https://arxiv.org/abs/2602.07260", "authors": ["Hongyu Kan", "Kristofor Pas", "Ivan Medri", "Naqib Sad Pathan", "Natasha Ironside", "Shinjini Kundu", "Jingjia He", "Gustavo Kunde Rohde"], "title": "3D Transport-based Morphometry (3D-TBM) for medical image analysis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.", "AI": {"tldr": "3D-TBM\u662f\u4e00\u4e2a\u7528\u4e8e3D\u533b\u5b66\u56fe\u50cf\u5f62\u6001\u5b66\u5206\u6790\u7684\u5de5\u5177\uff0c\u57fa\u4e8e\u4f20\u8f93\u5f62\u6001\u6d4b\u91cf\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9006\u53d8\u6362\u5c06\u56fe\u50cf\u5d4c\u5165\u4f20\u8f93\u57df\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u53ef\u89c6\u5316\u5de5\u5177\u548c\u5b8c\u6574\u6587\u6863\u652f\u6301\u3002", "motivation": "\u4fc3\u8fdb\u4f20\u8f93\u5f62\u6001\u6d4b\u91cf\u5b66\uff08TBM\uff09\u5728\u4e34\u5e8a\u5f71\u50cf\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e00\u4e2a\u5b8c\u6574\u76843D\u533b\u5b66\u56fe\u50cf\u5f62\u6001\u5b66\u5206\u6790\u5de5\u5177\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u7a7a\u95f4\u89e3\u91ca\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "method": "\u5f00\u53d13D-TBM\u5de5\u5177\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u6700\u4f18\u4f20\u8f93\u5d4c\u5165\u8ba1\u7b97\u3001\u4e3b\u8981\u4f20\u8f93\u65b9\u5411\u53ef\u89c6\u5316\u3001\u533a\u5206\u65b9\u5411\u8bc6\u522b\u7b49\u5206\u6790\u65b9\u6cd5\uff0c\u57fa\u4e8ePyTransKit\u5f00\u6e90\u5b9e\u73b0\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u76843D-TBM\u6846\u67b6\uff0c\u5305\u542b\u9884\u5904\u7406\u3001\u4f20\u8f93\u57df\u5d4c\u5165\u3001\u5206\u6790\u65b9\u6cd5\u548c\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u63d0\u4f9b\u5168\u9762\u6587\u6863\u548c\u5b9e\u7528\u6559\u7a0b\uff0c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "3D-TBM\u5de5\u5177\u4e3a\u533b\u5b66\u5f71\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u4f20\u8f93\u5f62\u6001\u6d4b\u91cf\u5b66\u7684\u5f3a\u5927\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9006\u6620\u5c04\u5b9e\u73b0\u7ed3\u679c\u7684\u7a7a\u95f4\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u671b\u4fc3\u8fdbTBM\u5728\u4e34\u5e8a\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2602.07262", "pdf": "https://arxiv.org/pdf/2602.07262", "abs": "https://arxiv.org/abs/2602.07262", "authors": ["Junbo Jacob Lian", "Feng Xiong", "Yujun Sun", "Kaichen Ouyang", "Mingyang Yu", "Shengwei Fu", "Zhong Rui", "Zhang Yujun", "Huiling Chen"], "title": "TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/junbolian/TwistNet-2D", "summary": "Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \\emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \\TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.", "AI": {"tldr": "TwistNet-2D\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u901a\u8fc7\u5c40\u90e8\u6210\u5bf9\u901a\u9053\u4e58\u79ef\u548c\u65b9\u5411\u6027\u7a7a\u95f4\u4f4d\u79fb\u6765\u6355\u6349\u7eb9\u7406\u7279\u5f81\uff0c\u5728\u5c11\u91cf\u53c2\u6570\u589e\u52a0\u4e0b\u663e\u8457\u63d0\u5347\u7eb9\u7406\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u77db\u76fe\uff1a\u53cc\u7ebf\u6027\u6c60\u5316\u548cGram\u77e9\u9635\u6355\u6349\u5168\u5c40\u901a\u9053\u76f8\u5173\u6027\u4f46\u7834\u574f\u7a7a\u95f4\u7ed3\u6784\uff0c\u800c\u81ea\u6ce8\u610f\u529b\u901a\u8fc7\u52a0\u6743\u805a\u5408\u800c\u975e\u663e\u5f0f\u6210\u5bf9\u7279\u5f81\u4ea4\u4e92\u6765\u5efa\u6a21\u7a7a\u95f4\u4e0a\u4e0b\u6587\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u7f16\u7801\u7279\u5f81\u5171\u73b0\u4f4d\u7f6e\u548c\u4ea4\u4e92\u65b9\u5f0f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTwistNet-2D\u6a21\u5757\uff0c\u6838\u5fc3\u662f\u87ba\u65cb\u626d\u66f2\u901a\u9053\u4ea4\u4e92\uff08STCI\uff09\uff1a\u5c06\u7279\u5f81\u56fe\u6cbf\u6307\u5b9a\u65b9\u5411\u4f4d\u79fb\u540e\u8fdb\u884c\u9010\u5143\u7d20\u901a\u9053\u4e58\u6cd5\uff0c\u6355\u6349\u7ed3\u6784\u5316\u5468\u671f\u6027\u7eb9\u7406\u7684\u8de8\u4f4d\u7f6e\u5171\u73b0\u6a21\u5f0f\u3002\u4f7f\u7528\u56db\u4e2a\u65b9\u5411\u5934\u805a\u5408\uff0c\u901a\u8fc7\u5b66\u4e60\u901a\u9053\u91cd\u52a0\u6743\u548csigmoid\u95e8\u63a7\u6b8b\u5dee\u8def\u5f84\u6ce8\u5165\u3002", "result": "\u5728ResNet-18\u57fa\u7840\u4e0a\u4ec5\u589e\u52a03.5%\u53c2\u6570\u548c2%FLOPs\uff0c\u4f46\u5728\u56db\u4e2a\u7eb9\u7406\u548c\u7ec6\u7c92\u5ea6\u8bc6\u522b\u57fa\u51c6\u4e0a\u6301\u7eed\u8d85\u8d8a\u53c2\u6570\u5339\u914d\u548c\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5305\u62ecConvNeXt\u3001Swin Transformer\u548c\u6df7\u5408CNN-Transformer\u67b6\u6784\u3002", "conclusion": "TwistNet-2D\u901a\u8fc7\u5c40\u90e8\u6210\u5bf9\u901a\u9053\u4ea4\u4e92\u548c\u65b9\u5411\u6027\u7a7a\u95f4\u4f4d\u79fb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7eb9\u7406\u8bc6\u522b\u4e2d\u5168\u5c40\u901a\u9053\u76f8\u5173\u6027\u4e0e\u7a7a\u95f4\u7ed3\u6784\u4fdd\u6301\u7684\u77db\u76fe\uff0c\u4ee5\u6781\u4f4e\u8ba1\u7b97\u6210\u672c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.07272", "pdf": "https://arxiv.org/pdf/2602.07272", "abs": "https://arxiv.org/abs/2602.07272", "authors": ["Bowen Xue", "Saeed Hadadan", "Zheng Zeng", "Fabrice Rousselle", "Zahra Montazeri", "Milos Hasan"], "title": "VideoNeuMat: Neural Material Extraction from Generative Video Models", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a \"virtual gonioreflectometer\" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.", "AI": {"tldr": "VideoNeuMat\uff1a\u4ece\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u795e\u7ecf\u6750\u8d28\u8d44\u4ea7\u7684\u4e24\u9636\u6bb5\u7ba1\u9053\uff0c\u5c06\u89c6\u9891\u6a21\u578b\u7684\u6750\u8d28\u77e5\u8bc6\u8f6c\u5316\u4e3a\u72ec\u7acb3D\u8d44\u4ea7", "motivation": "\u521b\u5efa\u903c\u771f\u76843D\u6e32\u67d3\u6750\u8d28\u9700\u8981\u9ad8\u8d85\u7684\u827a\u672f\u6280\u80fd\uff0c\u800c\u751f\u6210\u6a21\u578b\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u53d7\u9650\u3002\u89c6\u9891\u751f\u6210\u6a21\u578b\u80fd\u4ea7\u751f\u903c\u771f\u6750\u8d28\u5916\u89c2\uff0c\u4f46\u8fd9\u4e9b\u77e5\u8bc6\u4e0e\u51e0\u4f55\u548c\u5149\u7167\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u65e0\u6cd5\u76f4\u63a5\u91cd\u7528\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u5fae\u8c03\u5927\u578b\u89c6\u9891\u6a21\u578b\uff08Wan 2.1 14B\uff09\u5728\u53d7\u63a7\u76f8\u673a\u548c\u5149\u7167\u8f68\u8ff9\u4e0b\u751f\u6210\u6750\u8d28\u6837\u672c\u89c6\u9891\uff0c\u521b\u5efa\"\u865a\u62df\u6d4b\u89d2\u53cd\u5c04\u8ba1\"\uff1b2\uff09\u901a\u8fc7\u4ece\u8f83\u5c0fWan 1.3B\u89c6\u9891\u4e3b\u5e72\u5fae\u8c03\u7684\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\uff0c\u4ece17\u4e2a\u751f\u6210\u89c6\u9891\u5e27\u4e2d\u91cd\u5efa\u7d27\u51d1\u795e\u7ecf\u6750\u8d28\u53c2\u6570\u3002", "result": "\u751f\u6210\u7684\u6750\u8d28\u5728\u771f\u5b9e\u611f\u548c\u591a\u6837\u6027\u4e0a\u8fdc\u8d85\u6709\u9650\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u7684\u89c2\u5bdf\u548c\u5149\u7167\u6761\u4ef6\uff0c\u6210\u529f\u5c06\u4e92\u8054\u7f51\u89c4\u6a21\u89c6\u9891\u6a21\u578b\u7684\u6750\u8d28\u77e5\u8bc6\u8f6c\u5316\u4e3a\u72ec\u7acb\u53ef\u91cd\u7528\u7684\u795e\u7ecf3D\u8d44\u4ea7\u3002", "conclusion": "VideoNeuMat\u8bc1\u660e\u53ef\u4ee5\u4ece\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u6210\u529f\u63d0\u53d6\u6750\u8d28\u77e5\u8bc6\uff0c\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u7684\u795e\u7ecf\u6750\u8d28\u8d44\u4ea7\uff0c\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u6750\u8d28\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3a3D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.07277", "pdf": "https://arxiv.org/pdf/2602.07277", "abs": "https://arxiv.org/abs/2602.07277", "authors": ["Rishabh Sharma", "Gijs Hogervorst", "Wayne E. Mackey", "David J. Heeger", "Stefano Martiniani"], "title": "Cross-View World Models", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, 7 figures", "summary": "World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.", "AI": {"tldr": "XVWM\u901a\u8fc7\u8de8\u89c6\u89d2\u9884\u6d4b\u76ee\u6807\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\uff0c\u8ba9\u667a\u80fd\u4f53\u80fd\u591f\u4ece\u4e0d\u540c\u89c6\u89d2\uff08\u5982\u9e1f\u77b0\u56fe\uff09\u8fdb\u884c\u89c4\u5212\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u6267\u884c\uff0c\u5229\u7528\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4f5c\u4e3a\u51e0\u4f55\u6b63\u5219\u5316\u5b66\u4e603D\u73af\u5883\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u901a\u5e38\u53ea\u4ece\u5355\u4e00\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u64cd\u4f5c\uff0c\u4f46\u8bb8\u591a\u4efb\u52a1\uff08\u5982\u5bfc\u822a\uff09\u4ece\u5176\u4ed6\u89c6\u89d2\uff08\u5982\u9e1f\u77b0\u56fe\uff09\u89c4\u5212\u4f1a\u66f4\u6709\u6548\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u89c6\u89d2\u8fdb\u884c\u9884\u6d4b\u548c\u89c4\u5212\u7684\u4e16\u754c\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u8de8\u89c6\u89d2\u4e16\u754c\u6a21\u578b\uff08XVWM\uff09\uff0c\u901a\u8fc7\u8de8\u89c6\u89d2\u9884\u6d4b\u76ee\u6807\u8bad\u7ec3\uff1a\u7ed9\u5b9a\u4e00\u4e2a\u89c6\u89d2\u7684\u5e27\u5e8f\u5217\uff0c\u9884\u6d4b\u6267\u884c\u52a8\u4f5c\u540e\u76f8\u540c\u6216\u4e0d\u540c\u89c6\u89d2\u7684\u672a\u6765\u72b6\u6001\u3002\u4f7f\u7528Aimlabs\u5e73\u53f0\u63d0\u4f9b\u7684\u540c\u6b65\u591a\u89c6\u89d2\u6e38\u620f\u6570\u636e\u8bad\u7ec3\uff0c\u8fd9\u4e9b\u6570\u636e\u5305\u542b\u7cbe\u786e\u5bf9\u9f50\u7684\u591a\u76f8\u673a\u8bb0\u5f55\u548c\u9ad8\u9891\u52a8\u4f5c\u6807\u7b7e\u3002", "result": "\u6a21\u578b\u80fd\u591f\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u8de8\u89c6\u89d2\u7684\u5e76\u884c\u60f3\u8c61\u6d41\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u6700\u9002\u5408\u4efb\u52a1\u7684\u53c2\u8003\u7cfb\u4e2d\u8fdb\u884c\u89c4\u5212\uff0c\u540c\u65f6\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u6267\u884c\u3002\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e3a\u7a7a\u95f4\u57fa\u7840\u8868\u793a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "conclusion": "\u8de8\u89c6\u89d2\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u51e0\u4f55\u6b63\u5219\u5316\u5b66\u4e60\u73af\u58833D\u7ed3\u6784\uff0c\u5b9e\u73b0\u591a\u89c6\u89d2\u89c4\u5212\u80fd\u529b\u3002\u4ece\u4ed6\u4eba\u89c6\u89d2\u9884\u6d4b\u81ea\u8eab\u884c\u52a8\u540e\u679c\u53ef\u80fd\u4e3a\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u89c6\u89d2\u91c7\u62e9\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.07301", "pdf": "https://arxiv.org/pdf/2602.07301", "abs": "https://arxiv.org/abs/2602.07301", "authors": ["Aruna Jithesh", "Chinmayi Karumuri", "Venkata Kiran Reddy Kotha", "Meghana Doddapuneni", "Taehee Jeong"], "title": "Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms", "categories": ["cs.CV"], "comment": null, "summary": "Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684DeepLab-V3+\u6a21\u578b\uff0c\u7528\u4e8e\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u56db\u79cd\u75c5\u7076\u7684\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u5728DDR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5fae\u52a8\u8109\u7624\u68c0\u6d4b\u6709\u4e34\u5e8a\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5bfc\u81f4\u89c6\u529b\u4e27\u5931\u548c\u5931\u660e\u7684\u773c\u75c5\uff0c\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u5728\u75c5\u7076\u5206\u5272\u65b9\u9762\u7684\u4e34\u5e8a\u5e94\u7528\u6709\u9650\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u50cf\u7d20\u7ea7\u6ce8\u91ca\u6765\u652f\u6301\u773c\u79d1\u533b\u751f\u7b5b\u67e5\u3002", "method": "\u5728DeepLab-V3+\u6a21\u578b\u4e2d\u96c6\u6210\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bf9DDR\u6570\u636e\u96c6\u7684757\u5f20\u56fe\u50cf\u8fdb\u884c\u56db\u79cdDR\u76f8\u5173\u75c5\u7076\u7684\u5206\u5272\uff1a\u5fae\u52a8\u8109\u7624\u3001\u8f6f\u6027\u6e17\u51fa\u7269\u3001\u786c\u6027\u6e17\u51fa\u7269\u548c\u51fa\u8840\u3002", "result": "\u6ce8\u610f\u529b-DeepLab\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u5e73\u5747\u7cbe\u5ea6(mAP)\u4ece0.3010\u63d0\u5347\u52300.3326\uff0c\u5e73\u5747\u4ea4\u5e76\u6bd4(mIoU)\u4ece0.1791\u63d0\u5347\u52300.1928\u3002\u5fae\u52a8\u8109\u7624\u68c0\u6d4b\u4ece0.0205\u63d0\u5347\u52300.0763\uff0c\u8fd9\u662f\u4e34\u5e8a\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u96c6\u6210\u6ce8\u610f\u529b\u673a\u5236\u7684DeepLab-V3+\u6a21\u578b\u5728DR\u75c5\u7076\u5206\u5272\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u7279\u522b\u662f\u5bf9\u5fae\u52a8\u8109\u7624\u7684\u68c0\u6d4b\u6709\u91cd\u8981\u4e34\u5e8a\u610f\u4e49\uff0c\u56e0\u4e3a\u5fae\u52a8\u8109\u7624\u662fDR\u6700\u65e9\u53ef\u89c1\u7684\u75c7\u72b6\u3002"}}
{"id": "2602.07310", "pdf": "https://arxiv.org/pdf/2602.07310", "abs": "https://arxiv.org/abs/2602.07310", "authors": ["Kyle Williams", "Andrew Seltzman"], "title": "Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing", "categories": ["cs.CV", "cs.LG"], "comment": "39 pages, 12 figures, 1 table", "summary": "Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u7ebf\u6027\u9057\u4f20\u7f16\u7a0b\u7684\u56fe\u50cf\u8fc7\u6ee4\u5206\u5272\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u589e\u6750\u5236\u9020\u94cc\u57fa\u94dc\u5408\u91d1\u663e\u5fae\u56fe\u4e2d\u7684\u6790\u51fa\u7269\uff0c\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\uff0c\u52a0\u901f\u5408\u91d1\u5f00\u53d1\u8fed\u4ee3", "motivation": "\u5f53\u524d\u589e\u6750\u5236\u9020\u94cc\u57fa\u94dc\u5408\u91d1\u5206\u6790\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u663e\u5fae\u56fe\uff0c\u7531\u4e8e\u5bf9\u6bd4\u5ea6\u53d8\u5316\u3001\u566a\u58f0\u548c\u56fe\u50cf\u4f2a\u5f71\u7b49\u95ee\u9898\uff0c\u4e25\u91cd\u62d6\u6162\u4e86\u5408\u91d1\u5f00\u53d1\u8fed\u4ee3\u901f\u5ea6", "method": "\u4f7f\u7528\u7ebf\u6027\u9057\u4f20\u7f16\u7a0b\u4f18\u5316\u56fe\u50cf\u8fc7\u6ee4\u548c\u5206\u5272\u7b97\u6cd5\uff0c\u91c7\u7528\u7279\u5b9a\u9886\u57df\u8bed\u8a00\u6784\u5efa\u56fe\u50cf\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u8fed\u4ee3\u4f18\u5316\u53ef\u8c03\u53c2\u6570\u7684\u56fe\u50cf\u8fc7\u6ee4\u5757\u5e8f\u5217", "result": "\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\uff08\u79cd\u7fa4\u5927\u5c0f60\uff0c\u6700\u5927\u7a0b\u5e8f\u957f\u5ea65\u5757\uff09\uff0c\u7cfb\u7edf\u627e\u5230\u63a5\u8fd1\u4eba\u5de5\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u5747\u8bc4\u4f30\u8bef\u5dee1.8%\uff0c\u5904\u7406360\u4e07\u50cf\u7d20\u56fe\u50cf\u7ea6\u97002\u79d2", "conclusion": "\u81ea\u52a8\u5316\u5de5\u4f5c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8fed\u4ee3\u5468\u671f\uff0c\u4fc3\u8fdb\u4e86\u6750\u6599\u6210\u5206\u548c\u52a0\u5de5\u7a7a\u95f4\u7684\u63a2\u7d22\uff0c\u6700\u7ec8\u6709\u52a9\u4e8e\u5f00\u53d1\u7528\u4e8e\u589e\u6750\u5236\u9020\u805a\u53d8\u53cd\u5e94\u5806\u90e8\u4ef6\u7684\u5f3a\u97e7\u3001\u4f4e\u6d3b\u5316\u3001\u6c89\u6dc0\u786c\u5316\u94dc\u5408\u91d1"}}
{"id": "2602.07311", "pdf": "https://arxiv.org/pdf/2602.07311", "abs": "https://arxiv.org/abs/2602.07311", "authors": ["Difei Gu", "Yunhe Gao", "Gerasimos Chatzoudis", "Zihan Dong", "Guoning Zhang", "Bangwei Guo", "Yang Zhou", "Mu Zhou", "Dimitris Metaxas"], "title": "LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.", "AI": {"tldr": "LUCID\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u5b66\u4e60\u56fe\u50cf\u5757\u548c\u6587\u672c\u6807\u8bb0\u7684\u5171\u4eab\u6f5c\u5728\u5b57\u5178\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5339\u914d\u5b9e\u73b0\u7279\u5f81\u5bf9\u9f50\uff0c\u65e0\u9700\u6807\u6ce8\u5373\u53ef\u83b7\u5f97\u53ef\u89e3\u91ca\u7684\u8de8\u6a21\u6001\u5171\u4eab\u7279\u5f81\u3002", "motivation": "\u5f53\u524d\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6309\u6a21\u6001\u5355\u72ec\u8bad\u7ec3\uff0c\u4ea7\u751f\u7684\u7279\u5f81\u5b57\u5178\u4e0d\u53ef\u76f4\u63a5\u7406\u89e3\uff0c\u4e14\u89e3\u91ca\u65e0\u6cd5\u8de8\u57df\u8fc1\u79fb\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u8de8\u6a21\u6001\u5171\u4eab\u8868\u793a\u3002", "method": "LUCID\u91c7\u7528\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00\u7a00\u758f\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5b66\u4e60\u5171\u4eab\u6f5c\u5728\u5b57\u5178\u7528\u4e8e\u56fe\u50cf\u5757\u548c\u6587\u672c\u6807\u8bb0\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u7559\u79c1\u6709\u5bb9\u91cf\u5904\u7406\u6a21\u6001\u7279\u5b9a\u7ec6\u8282\u3002\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5339\u914d\u76ee\u6807\u5b9e\u73b0\u7279\u5f81\u5bf9\u9f50\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3002", "result": "LUCID\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u5171\u4eab\u7279\u5f81\uff0c\u652f\u6301\u8865\u4e01\u7ea7\u5b9a\u4f4d\u3001\u5efa\u7acb\u8de8\u6a21\u6001\u795e\u7ecf\u5143\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u589e\u5f3a\u5bf9\u76f8\u4f3c\u6027\u8bc4\u4f30\u4e2d\u6982\u5ff5\u805a\u7c7b\u95ee\u9898\u7684\u9c81\u68d2\u6027\u3002\u5171\u4eab\u7279\u5f81\u6355\u83b7\u4e86\u8d85\u8d8a\u5bf9\u8c61\u7684\u591a\u6837\u5316\u8bed\u4e49\u7c7b\u522b\uff0c\u5305\u62ec\u52a8\u4f5c\u3001\u5c5e\u6027\u548c\u62bd\u8c61\u6982\u5ff5\u3002", "conclusion": "LUCID\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u9762\u7684\u53ef\u89e3\u91ca\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u7a00\u758f\u7f16\u7801\u5b9e\u73b0\u8de8\u6a21\u6001\u6982\u5ff5\u53d1\u73b0\uff0c\u4e3a\u81ea\u52a8\u5316\u7684\u5b57\u5178\u89e3\u91ca\u548c\u8de8\u57df\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.07343", "pdf": "https://arxiv.org/pdf/2602.07343", "abs": "https://arxiv.org/abs/2602.07343", "authors": ["Ruturaj Reddy", "Hrishav Bakul Barua", "Junn Yong Loo", "Thanh Thi Nguyen", "Ganesh Krishnasamy"], "title": "Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.", "AI": {"tldr": "CLARITY\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001RGB-\u70ed\u6210\u50cf\u878d\u5408\u7b56\u7565\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u6001\u8d21\u732e\uff0c\u5728\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u9053\u8def\u573a\u666f\u8bed\u4e49\u5206\u5272\u3002", "motivation": "\u73b0\u6709RGB-\u70ed\u6210\u50cf\u878d\u5408\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u878d\u5408\u7b56\u7565\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u5149\u7167\u6761\u4ef6\uff0c\u5bfc\u81f4\u6a21\u6001\u7279\u5b9a\u566a\u58f0\u5728\u7f51\u7edc\u4e2d\u4f20\u64ad\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u5728\u6076\u52a3\u5149\u7167\u3001\u9634\u5f71\u6761\u4ef6\u4e0b\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "method": "1) \u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u52a8\u6001\u8c03\u6574\u878d\u5408\u7b56\u7565\uff0c\u6839\u636e\u68c0\u6d4b\u5230\u7684\u573a\u666f\u6761\u4ef6\u8c03\u5236\u5404\u6a21\u6001\u8d21\u732e\uff1b2) \u4fdd\u7559\u6709\u6548\u6697\u7269\u4f53\u8bed\u4e49\uff0c\u907f\u514d\u5148\u524d\u566a\u58f0\u6291\u5236\u65b9\u6cd5\u9519\u8bef\u4e22\u5f03\uff1b3) \u5f15\u5165\u5206\u5c42\u89e3\u7801\u5668\uff0c\u8de8\u5c3a\u5ea6\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u4e00\u81f4\u6027\u4ee5\u9510\u5316\u7ec6\u957f\u7269\u4f53\u8fb9\u754c\u3002", "result": "\u5728MFNet\u6570\u636e\u96c6\u4e0a\u8fbe\u523062.3% mIoU\u548c77.5% mAcc\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "CLARITY\u901a\u8fc7\u52a8\u6001\u878d\u5408\u7b56\u7565\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u9053\u8def\u573a\u666f\u8bed\u4e49\u5206\u5272\u9c81\u68d2\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07345", "pdf": "https://arxiv.org/pdf/2602.07345", "abs": "https://arxiv.org/abs/2602.07345", "authors": ["Lichen Bai", "Zikai Zhou", "Shitong Shao", "Wenliang Zhong", "Shuo Yang", "Shuo Chen", "Bojun Chen", "Zeke Xie"], "title": "Optimizing Few-Step Generation with Adaptive Matching Distillation", "categories": ["cs.CV", "cs.LG"], "comment": "25 pages, 15 figures, 11 tables", "summary": "Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.", "AI": {"tldr": "AMD\u63d0\u51fa\u81ea\u9002\u5e94\u5339\u914d\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u4ee3\u7406\u68c0\u6d4b\u5e76\u9003\u79bb\"\u7981\u533a\"\uff0c\u63d0\u5347\u5206\u5e03\u5339\u914d\u84b8\u998f\u7684\u7a33\u5b9a\u6027\u548c\u751f\u6210\u8d28\u91cf", "motivation": "\u4f20\u7edf\u5206\u5e03\u5339\u914d\u84b8\u998f(DMD)\u5728\"\u7981\u533a\"\u533a\u57df\u4e0d\u7a33\u5b9a\uff0c\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u4e0d\u53ef\u9760\u6307\u5bfc\u800c\u4f2a\u6559\u5e08\u6392\u65a5\u529b\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u5339\u914d\u84b8\u998f(AMD)\uff1a1) \u4f7f\u7528\u5956\u52b1\u4ee3\u7406\u663e\u5f0f\u68c0\u6d4b\u7981\u533a\uff1b2) \u901a\u8fc7\u7ed3\u6784\u4fe1\u53f7\u5206\u89e3\u52a8\u6001\u4f18\u5148\u6821\u6b63\u68af\u5ea6\uff1b3) \u5f15\u5165\u6392\u65a5\u666f\u89c2\u9510\u5316\u589e\u5f3a\u80fd\u91cf\u58c1\u5792\u9632\u6b62\u5931\u8d25\u6a21\u5f0f\u574d\u7f29", "result": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1(SDXL, Wan2.1)\u4e0a\u663e\u8457\u63d0\u5347\u6837\u672c\u4fdd\u771f\u5ea6\u548c\u8bad\u7ec3\u9c81\u68d2\u6027\uff0cSDXL\u7684HPSv2\u5206\u6570\u4ece30.64\u63d0\u5347\u81f331.25\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u663e\u5f0f\u4fee\u6b63\u7981\u533a\u5185\u7684\u4f18\u5316\u8f68\u8ff9\u5bf9\u4e8e\u63d0\u5347\u5c11\u6b65\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u4e0a\u9650\u81f3\u5173\u91cd\u8981\uff0cAMD\u4e3a\u5206\u5e03\u5339\u914d\u84b8\u998f\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u6846\u67b6"}}
{"id": "2602.07428", "pdf": "https://arxiv.org/pdf/2602.07428", "abs": "https://arxiv.org/abs/2602.07428", "authors": ["Chengqi Dong", "Zhiyuan Cao", "Tuoshi Qi", "Kexin Wu", "Yixing Gao", "Fan Tang"], "title": "Row-Column Separated Attention Based Low-Light Image/Video Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.", "AI": {"tldr": "\u63d0\u51faRCSA\u6a21\u5757\u6539\u8fdbU-Net\uff0c\u7528\u4e8e\u4f4e\u5149\u7167\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\uff0c\u901a\u8fc7\u884c\u5217\u5206\u79bb\u6ce8\u610f\u529b\u673a\u5236\u5229\u7528\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\u5c40\u90e8\u4fe1\u606f\uff0c\u51cf\u5c11\u53c2\u6570\u8ba1\u7b97\u91cf", "motivation": "\u4f20\u7edfU-Net\u5728\u4f4e\u5149\u7167\u589e\u5f3a\u4e2d\u7f3a\u4e4f\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\uff0c\u5bfc\u81f4\u5c40\u90e8\u566a\u58f0\u5927\u3001\u7ec6\u8282\u4e22\u5931\uff1b\u6ce8\u610f\u529b\u673a\u5236\u80fd\u66f4\u597d\u5229\u7528\u5168\u5c40\u4fe1\u606f\u4f46\u53c2\u6570\u8ba1\u7b97\u91cf\u5927", "method": "\u63d0\u51fa\u884c\u5217\u5206\u79bb\u6ce8\u610f\u529b\u6a21\u5757(RCSA)\uff0c\u8f93\u5165\u7279\u5f81\u56fe\u884c\u5217\u7684\u5747\u503c\u548c\u6700\u5927\u503c\uff0c\u5229\u7528\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\u5c40\u90e8\u4fe1\u606f\uff1b\u6539\u8fdbU-Net\u7ed3\u6784\uff1b\u63d0\u51fa\u4e24\u79cd\u65f6\u95f4\u635f\u5931\u51fd\u6570\u7528\u4e8e\u89c6\u9891\u589e\u5f3a\u4fdd\u6301\u65f6\u5e8f\u4e00\u81f4\u6027", "result": "\u5728LOL\u3001MIT Adobe FiveK\u56fe\u50cf\u6570\u636e\u96c6\u548cSDSD\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684URCSA\u65b9\u6cd5\u901a\u8fc7RCSA\u6a21\u5757\u6709\u6548\u5229\u7528\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\u4f4e\u5149\u7167\u589e\u5f3a\uff0c\u51cf\u5c11\u53c2\u6570\u8ba1\u7b97\uff0c\u5728\u56fe\u50cf\u548c\u89c6\u9891\u589e\u5f3a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02"}}
{"id": "2602.07444", "pdf": "https://arxiv.org/pdf/2602.07444", "abs": "https://arxiv.org/abs/2602.07444", "authors": ["Ondrej Hlinka", "Georg Kaniak", "Christian Kapeller"], "title": "Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction", "categories": ["cs.CV", "eess.SP"], "comment": "submitted to IET Electronics Letters", "summary": "We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u900f\u89c6\u611f\u77e5\u7684\u5bf9\u6570\u6df1\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u4ece\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u91cd\u5efa3D\u8868\u9762\uff0c\u5904\u7406\u900f\u89c6\u6295\u5f71\u5e76\u586b\u8865\u6df1\u5ea6\u7f3a\u5931", "motivation": "\u89e3\u51b3\u4ece\u5355\u89c6\u89d2\u76f8\u673a\u83b7\u53d6\u7684\u6df1\u5ea6\u548c\u8868\u9762\u6cd5\u7ebf\u56fe\u91cd\u5efa3D\u8868\u9762\u7684\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u6b63\u4ea4\u68af\u5ea6\u7684\u6df1\u5ea6-\u6cd5\u7ebf\u878d\u5408\u65b9\u6cd5\u672a\u8003\u8651\u900f\u89c6\u6295\u5f71\uff0c\u5bfc\u81f4\u5ea6\u91cf\u7cbe\u5ea6\u4e0d\u8db3", "method": "\u63d0\u51fa\u900f\u89c6\u611f\u77e5\u7684\u5bf9\u6570\u6df1\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u6269\u5c55\u73b0\u6709\u6b63\u4ea4\u68af\u5ea6\u65b9\u6cd5\uff0c\u663e\u5f0f\u5904\u7406\u900f\u89c6\u6295\u5f71\uff0c\u540c\u65f6\u5229\u7528\u8868\u9762\u6cd5\u7ebf\u4fe1\u606f\u586b\u8865\u6df1\u5ea6\u6d4b\u91cf\u7f3a\u5931\u533a\u57df", "result": "\u5728DiLiGenT-MV\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u900f\u89c6\u611f\u77e5\u6df1\u5ea6-\u6cd5\u7ebf\u878d\u5408\u7684\u91cd\u8981\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u5b9e\u73b0\u5ea6\u91cf\u51c6\u786e\u76843D\u91cd\u5efa\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5355\u89c6\u89d2\u76f8\u673a\u7cfb\u7edf\uff0c\u901a\u8fc7\u900f\u89c6\u611f\u77e5\u878d\u5408\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6"}}
{"id": "2602.07446", "pdf": "https://arxiv.org/pdf/2602.07446", "abs": "https://arxiv.org/abs/2602.07446", "authors": ["Naqcho Ali Mehdi"], "title": "PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization", "categories": ["cs.CV"], "comment": "8 pages, 4 figures, dataset paper", "summary": "Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86PTB-XL-Image-17K\u6570\u636e\u96c6\uff0c\u5305\u542b17,271\u4e2a\u5408\u621012\u5bfc\u8054\u5fc3\u7535\u56fe\u56fe\u50cf\uff0c\u63d0\u4f9b\u56fe\u50cf\u3001\u5206\u5272\u63a9\u7801\u3001\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u3001\u8fb9\u754c\u6846\u6807\u6ce8\u548c\u5143\u6570\u636e\uff0c\u652f\u6301\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u7814\u7a76\u3002", "motivation": "\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u5bf9\u4e8e\u5229\u7528\u5386\u53f2\u4e34\u5e8a\u6570\u636e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u540c\u65f6\u5305\u542b\u5fc3\u7535\u56fe\u56fe\u50cf\u548c\u5bf9\u5e94\u771f\u5b9e\u4fe1\u53f7\u7684\u6570\u636e\u96c6\u963b\u788d\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u57fa\u4e8ePTB-XL\u4fe1\u53f7\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u5f00\u6e90Python\u6846\u67b6\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u5fc3\u7535\u56fe\u56fe\u50cf\uff0c\u53ef\u63a7\u5236\u7eb8\u5f20\u901f\u5ea6\u3001\u7535\u538b\u6bd4\u4f8b\u3001\u91c7\u6837\u7387\u3001\u7f51\u683c\u5916\u89c2\u7b49\u53c2\u6570\u3002", "result": "\u6210\u529f\u751f\u621017,271\u4e2a\u6837\u672c\uff0c100%\u751f\u6210\u6210\u529f\u7387\uff0c\u5e73\u5747\u5904\u7406\u65f6\u95f41.35\u79d2/\u6837\u672c\uff0c\u63d0\u4f9b\u4e94\u79cd\u4e92\u8865\u6570\u636e\u7c7b\u578b\uff0c\u586b\u8865\u4e86\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u7814\u7a76\u7684\u5173\u952e\u7a7a\u767d\u3002", "conclusion": "PTB-XL-Image-17K\u662f\u9996\u4e2a\u652f\u6301\u5b8c\u6574\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u6d41\u7a0b\u7684\u5927\u89c4\u6a21\u8d44\u6e90\uff0c\u5305\u62ec\u5bfc\u8054\u68c0\u6d4b\u3001\u6ce2\u5f62\u5206\u5272\u548c\u4fe1\u53f7\u63d0\u53d6\uff0c\u4e3a\u4e25\u683c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b8c\u6574\u771f\u5b9e\u6807\u6ce8\u3002"}}
{"id": "2602.07449", "pdf": "https://arxiv.org/pdf/2602.07449", "abs": "https://arxiv.org/abs/2602.07449", "authors": ["Tan Yu", "Qian Qiao", "Le Shen", "Ke Zhou", "Jincheng Hu", "Dian Sheng", "Bo Hu", "Haoming Qin", "Jun Gao", "Changhai Zhou", "Shunshun Yin", "Siyuan Liu"], "title": "SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads", "categories": ["cs.CV"], "comment": "11 pages, 3 figures", "summary": "Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.", "AI": {"tldr": "SoulX-FlashHead\u662f\u4e00\u4e2a1.3B\u53c2\u6570\u7684\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f0f\u611f\u77e5\u65f6\u7a7a\u9884\u8bad\u7ec3\u548cOracle\u5f15\u5bfc\u53cc\u5411\u84b8\u998f\u6280\u672f\uff0c\u5728\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b096 FPS\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u751f\u6210\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1a\u5927\u89c4\u6a21\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5219\u727a\u7272\u9762\u90e8\u8868\u793a\u5b8c\u6574\u6027\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u3002\u9700\u8981\u5728\u9ad8\u8d28\u91cf\u89c6\u89c9\u8f93\u51fa\u548c\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u5904\u7406\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "1) \u63d0\u51fa1.3B\u53c2\u6570\u7edf\u4e00\u6846\u67b6\uff1b2) \u5f15\u5165\u6d41\u5f0f\u611f\u77e5\u65f6\u7a7a\u9884\u8bad\u7ec3\uff0c\u914d\u5907\u65f6\u5e8f\u97f3\u9891\u4e0a\u4e0b\u6587\u7f13\u5b58\u673a\u5236\uff1b3) \u63d0\u51faOracle\u5f15\u5bfc\u53cc\u5411\u84b8\u998f\uff0c\u5229\u7528\u771f\u5b9e\u8fd0\u52a8\u5148\u9a8c\u63d0\u4f9b\u7269\u7406\u6307\u5bfc\uff1b4) \u6784\u5efaVividHead\u6570\u636e\u96c6\uff08782\u5c0f\u65f6\u4e25\u683c\u5bf9\u9f50\u89c6\u9891\uff09\u3002", "result": "\u5728HDTF\u548cVFHQ\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cLite\u53d8\u4f53\u5728\u5355\u5f20RTX 4090\u4e0a\u5b9e\u73b096 FPS\u63a8\u7406\u901f\u5ea6\uff0c\u652f\u6301\u8d85\u5feb\u901f\u4ea4\u4e92\u800c\u4e0d\u727a\u7272\u89c6\u89c9\u8fde\u8d2f\u6027\u3002", "conclusion": "SoulX-FlashHead\u6210\u529f\u89e3\u51b3\u4e86\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u751f\u6210\u4e2d\u9ad8\u4fdd\u771f\u89c6\u89c9\u8d28\u91cf\u4e0e\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u5904\u7406\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u3001\u65e0\u9650\u957f\u5ea6\u3001\u9ad8\u8d28\u91cf\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07458", "pdf": "https://arxiv.org/pdf/2602.07458", "abs": "https://arxiv.org/abs/2602.07458", "authors": ["Yancheng Long", "Yankai Yang", "Hongyang Wei", "Wei Chen", "Tianke Zhang", "Haonan fan", "Changyi Liu", "Kaiyu Jiang", "Jiankang Chen", "Kaiyu Tang", "Bin Wen", "Fan Yang", "Tingting Gao", "Han Li", "Shuo Yang"], "title": "SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term \"Attention Collapse,\" where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.", "AI": {"tldr": "SpatialReward\u662f\u4e00\u4e2a\u901a\u8fc7\u7a7a\u95f4\u63a8\u7406\u589e\u5f3a\u5956\u52b1\u4fe1\u53f7\u7684\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u56fe\u50cf\u7f16\u8f91\u4e2d\u5956\u52b1\u4fe1\u53f7\u7a00\u7f3a\u548c\u73b0\u6709\u8bc4\u4f30\u5668\"\u6ce8\u610f\u529b\u5d29\u6e83\"\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u56fe\u50cf\u7f16\u8f91\u4e2d\u9762\u4e34\u53ef\u9760\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u73b0\u6709\u8bc4\u4f30\u5668\u5b58\u5728\"\u6ce8\u610f\u529b\u5d29\u6e83\"\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5ffd\u89c6\u8de8\u56fe\u50cf\u6bd4\u8f83\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u611f\u77e5\u4e0d\u51c6\u786e\u548c\u5206\u6570\u6821\u51c6\u9519\u8bef\u3002", "method": "\u63d0\u51faSpatialReward\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u63a8\u7406\u8fdb\u884c\u7cbe\u786e\u9a8c\u8bc1\u3002\u5c06\u63a8\u7406\u951a\u5b9a\u5728\u9884\u6d4b\u7684\u7f16\u8f91\u533a\u57df\uff0c\u4f7f\u8bed\u4e49\u5224\u65ad\u57fa\u4e8e\u50cf\u7d20\u7ea7\u8bc1\u636e\u3002\u5728\u7cbe\u5fc3\u7b56\u5212\u7684260k\u7a7a\u95f4\u611f\u77e5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728MMRB2\u548cEditReward-Bench\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u63d0\u51fa\u7684MultiEditReward-Bench\u4e0a\u8d85\u8d8a\u4e13\u6709\u8bc4\u4f30\u5668\u3002\u4f5c\u4e3a\u5728\u7ebfRL\u7684\u9c81\u68d2\u4fe1\u53f7\uff0c\u5c06OmniGen2\u5728GEdit-Bench\u4e0a\u63d0\u5347+0.90\uff0c\u8d85\u8d8a\u9886\u5148\u5224\u522b\u6a21\u578b\uff0c\u662fGPT-4.1\u589e\u76ca\u7684\u4e24\u500d\u3002", "conclusion": "\u7a7a\u95f4\u63a8\u7406\u5bf9\u4e8e\u5b9e\u73b0\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u6709\u6548\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002SpatialReward\u901a\u8fc7\u50cf\u7d20\u7ea7\u8bc1\u636e\u548c\u7a7a\u95f4\u63a8\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5956\u52b1\u4fe1\u53f7\u3002"}}
{"id": "2602.07463", "pdf": "https://arxiv.org/pdf/2602.07463", "abs": "https://arxiv.org/abs/2602.07463", "authors": ["Misbah Ijaz", "Saif Ur Rehman Khan", "Abd Ur Rehman", "Tayyaba Asif", "Sebastian Vollmer", "Andreas Dengel", "Muhammad Nabeel Asim"], "title": "GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86GlobalWasteData\uff08GWD\uff09\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u591a\u4e2a\u516c\u5f00\u7684\u5783\u573e\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5305\u542b89,807\u5f20\u56fe\u50cf\u300114\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c68\u4e2a\u5b50\u7c7b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u788e\u7247\u5316\u3001\u4e0d\u4e00\u81f4\u548c\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5783\u573e\u5206\u7c7b\u6570\u636e\u96c6\u5b58\u5728\u788e\u7247\u5316\u3001\u4e0d\u4e00\u81f4\u548c\u7279\u5b9a\u73af\u5883\u504f\u5dee\u95ee\u9898\uff0c\u7c7b\u522b\u540d\u79f0\u3001\u6807\u6ce8\u683c\u5f0f\u3001\u56fe\u50cf\u6761\u4ef6\u548c\u7c7b\u522b\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u96be\u4ee5\u6574\u5408\u6570\u636e\u96c6\u6216\u8bad\u7ec3\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5408\u5e76\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u521b\u5efa\u7edf\u4e00\u7684GWD\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u8d28\u91cf\u8fc7\u6ee4\u3001\u91cd\u590d\u53bb\u9664\u548c\u5143\u6570\u636e\u751f\u6210\u7b49\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u786e\u4fdd\u4e00\u81f4\u7684\u6807\u6ce8\u3001\u6539\u8fdb\u7684\u9886\u57df\u591a\u6837\u6027\u548c\u66f4\u5e73\u8861\u7684\u7c7b\u522b\u8868\u793a\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b89,807\u5f20\u56fe\u50cf\u300114\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c68\u4e2a\u5b50\u7c7b\u7684GWD\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e00\u81f4\u7684\u6807\u6ce8\u3001\u6539\u8fdb\u7684\u9886\u57df\u591a\u6837\u6027\u548c\u66f4\u5e73\u8861\u7684\u7c7b\u522b\u8868\u793a\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u5783\u573e\u8bc6\u522b\u6a21\u578b\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "GWD\u6570\u636e\u96c6\u4e3a\u73af\u5883\u76d1\u6d4b\u3001\u56de\u6536\u81ea\u52a8\u5316\u548c\u5783\u573e\u8bc6\u522b\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u516c\u5f00\u53ef\u7528\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2602.07493", "pdf": "https://arxiv.org/pdf/2602.07493", "abs": "https://arxiv.org/abs/2602.07493", "authors": ["Tianhao Zhou", "Yujia Chen", "Zhihao Zhan", "Yuhang Ming", "Jianzhu Huai"], "title": "Thermal odometry and dense mapping using learned ddometry and Gaussian splatting", "categories": ["cs.CV"], "comment": "11 pages, 2 figures, 5 tables", "summary": "Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.", "AI": {"tldr": "TOM-GS\uff1a\u9996\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u70ed\u6210\u50cf\u76f8\u673aSLAM\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b66\u4e60\u91cc\u7a0b\u8ba1\u4e0e\u7a20\u5bc6\u5efa\u56fe\uff0c\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\u4f30\u8ba1\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa", "motivation": "\u70ed\u6210\u50cf\u4f20\u611f\u5668\u5728\u9ed1\u6697\u3001\u7070\u5c18\u548c\u70df\u96fe\u4e2d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u73b0\u6709\u70ed\u6210\u50cf\u91cc\u7a0b\u8ba1\u548c\u5efa\u56fe\u65b9\u6cd5\u4e3b\u8981\u662f\u51e0\u4f55\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u4e14\u65e0\u6cd5\u751f\u6210\u7a20\u5bc6\u5730\u56fe\u3002\u53d7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u9ad8\u6548\u9ad8\u8d28\u91cf\u91cd\u5efa\u80fd\u529b\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u70ed\u6210\u50cfSLAM\u7cfb\u7edf\u3002", "method": "\u63d0\u51faTOM-GS\u65b9\u6cd5\uff0c\u6574\u5408\u5b66\u4e60\u578b\u91cc\u7a0b\u8ba1\u4e0e\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u7a20\u5bc6\u5efa\u56fe\u3002\u7cfb\u7edf\u5305\u542b\u4e13\u95e8\u7684\u70ed\u56fe\u50cf\u589e\u5f3a\u6a21\u5757\u548c\u5355\u76ee\u6df1\u5ea6\u96c6\u6210\uff0c\u662f\u9996\u4e2a\u4e3a\u70ed\u6210\u50cf\u76f8\u673a\u5b9a\u5236\u7684\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\u3002", "result": "\u5728\u8fd0\u52a8\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cTOM-GS\u4f18\u4e8e\u73b0\u6709\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5b66\u4e60\u578b\u6d41\u7a0b\u5728\u9c81\u68d2\u70ed\u6210\u50cf\u91cc\u7a0b\u8ba1\u548c\u7a20\u5bc6\u91cd\u5efa\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "TOM-GS\u6210\u529f\u5c06\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5e94\u7528\u4e8e\u70ed\u6210\u50cfSLAM\uff0c\u901a\u8fc7\u7ed3\u5408\u5b66\u4e60\u578b\u91cc\u7a0b\u8ba1\u548c\u7a20\u5bc6\u5efa\u56fe\uff0c\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u8fd0\u52a8\u4f30\u8ba1\u548c\u9ad8\u8d28\u91cf\u73af\u5883\u611f\u77e5\u3002"}}
{"id": "2602.07495", "pdf": "https://arxiv.org/pdf/2602.07495", "abs": "https://arxiv.org/abs/2602.07495", "authors": ["Jiawen Zheng", "Haonan Jia", "Ming Li", "Yuhui Zheng", "Yufeng Zeng", "Yang Gao", "Chen Liang"], "title": "Learning Brain Representation with Hierarchical Visual Embeddings", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u591a\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u8fdb\u884c\u8111\u4fe1\u53f7-\u56fe\u50cf\u5bf9\u9f50\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u878d\u5408\u5148\u9a8c\u63d0\u5347\u89c6\u89c9\u89e3\u7801\u7684\u51c6\u786e\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u8111\u4fe1\u53f7\u89c6\u89c9\u89e3\u7801\u65b9\u6cd5\u5927\u591a\u5173\u6ce8\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\u800c\u5ffd\u7565\u50cf\u7d20\u7ea7\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u7406\u89e3\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8111-\u56fe\u50cf\u5bf9\u9f50\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u5f52\u7eb3\u504f\u7f6e\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u6355\u83b7\u5c42\u6b21\u5316\u548c\u591a\u5c3a\u5ea6\u89c6\u89c9\u8868\u793a\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u8111\u4fe1\u53f7\u4e0e\u89c6\u89c9\u5d4c\u5165\u7684\u6709\u6548\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u878d\u5408\u5148\u9a8c\u589e\u5f3a\u8de8\u6a21\u6001\u5206\u5e03\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u7f16\u7801\u5668\u5bf9\u9f50\u7b56\u7565\u548c\u878d\u5408\u5148\u9a8c\u65b9\u6cd5\u80fd\u591f\u66f4\u5168\u9762\u5730\u89e3\u7801\u8111\u4fe1\u53f7\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.07498", "pdf": "https://arxiv.org/pdf/2602.07498", "abs": "https://arxiv.org/abs/2602.07498", "authors": ["Zhufeng Xu", "Xuan Gao", "Feng-Lin Liu", "Haoxian Zhang", "Zhixue Fang", "Yu-Kun Lai", "Xiaoqiang Liu", "Pengfei Wan", "Lin Gao"], "title": "IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.", "AI": {"tldr": "IM-Animation\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6bcf\u5e27\u8fd0\u52a8\u538b\u7f29\u4e3a\u7d27\u51d1\u76841D\u8fd0\u52a8token\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u3001\u8eab\u4efd\u6cc4\u6f0f\u548c\u8fd0\u52a8-\u5916\u89c2\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89d2\u8272\u52a8\u753b\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89d2\u8272\u52a8\u753b\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff1a\u663e\u5f0f\u65b9\u6cd5\uff08\u5982\u9aa8\u67b6\u3001DWPose\uff09\u96be\u4ee5\u5904\u7406\u7a7a\u95f4\u4e0d\u5339\u914d\u548c\u8eab\u4f53\u6bd4\u4f8b\u53d8\u5316\uff1b\u9690\u5f0f\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u9ad8\u7ea7\u8fd0\u52a8\u8bed\u4e49\uff0c\u4f46\u5b58\u5728\u8eab\u4efd\u4fe1\u606f\u6cc4\u6f0f\u548c\u8fd0\u52a8-\u5916\u89c2\u7ea0\u7f20\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7d27\u51d1\u76841D\u8fd0\u52a8token\u8868\u793a\uff0c\u653e\u677e2D\u8868\u793a\u7684\u7a7a\u95f4\u7ea6\u675f\u5e76\u9632\u6b62\u8eab\u4efd\u6cc4\u6f0f\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u65f6\u95f4\u4e00\u81f4\u63a9\u7801token\u7684\u91cd\u5b9a\u5411\u6a21\u5757\uff0c\u901a\u8fc7\u65f6\u95f4\u8bad\u7ec3\u74f6\u9888\u51cf\u5c11\u6e90\u56fe\u50cf\u8fd0\u52a8\u5e72\u6270\uff1b\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u63d0\u9ad8\u6548\u7387\u5e76\u786e\u4fdd\u9ad8\u4fdd\u771f\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cIM-Animation\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u548c\u751f\u6210\u80fd\u529b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8eab\u4efd\u6cc4\u6f0f\u548c\u8fd0\u52a8-\u5916\u89c2\u7ea0\u7f20\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u548cIM-Animation\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u89d2\u8272\u52a8\u753b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc71D\u8fd0\u52a8token\u548c\u63a9\u7801token\u91cd\u5b9a\u5411\u6a21\u5757\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4e00\u81f4\u7684\u89d2\u8272\u52a8\u753b\u751f\u6210\u3002"}}
{"id": "2602.07512", "pdf": "https://arxiv.org/pdf/2602.07512", "abs": "https://arxiv.org/abs/2602.07512", "authors": ["Tao Wang", "Chenyu Lin", "Chenwei Tang", "Jizhe Zhou", "Deng Xiong", "Jianan Li", "Jian Zhao", "Jiancheng Lv"], "title": "Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection", "categories": ["cs.CV"], "comment": "paper accepted by ISPRS Journal of Photogrammetry and Remote Sensing ( IF=12.2)", "summary": "Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \\textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.", "AI": {"tldr": "ZoomDet\uff1a\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u7684\u81ea\u9002\u5e94\u653e\u5927\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5747\u5300\u653e\u5927\u5bf9\u8c61\u533a\u57df\u6765\u6539\u5584\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd", "motivation": "\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u901a\u5e38\u6bd4\u666e\u901a\u573a\u666f\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u66f4\u5c0f\u3001\u66f4\u7a00\u758f\uff0c\u8fd9\u963b\u788d\u4e86\u6709\u6548\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u4f18\u5316\u3002\u9700\u8981\u81ea\u9002\u5e94\u653e\u5927\u5bf9\u8c61\u4ee5\u66f4\u597d\u5730\u6355\u83b7\u76ee\u6807\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u504f\u79fb\u9884\u6d4b\u65b9\u6848\u548c\u57fa\u4e8e\u6846\u7684\u653e\u5927\u76ee\u6807\u6765\u5b66\u4e60\u8f93\u5165\u56fe\u50cf\u7684\u975e\u5747\u5300\u653e\u5927\u3002\u57fa\u4e8e\u5b66\u4e60\u7684\u653e\u5927\u53d8\u6362\uff0c\u63d0\u51fa\u89d2\u5bf9\u9f50\u8fb9\u754c\u6846\u53d8\u6362\u65b9\u6cd5\uff0c\u5c06\u771f\u5b9e\u6846\u53d8\u6362\u5230\u653e\u5927\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u9884\u6d4b\u6846\u53d8\u6362\u56de\u539f\u59cb\u7a7a\u95f4\u3002", "result": "\u5728VisDrone\u3001UAVDT\u548cSeaDronesSee\u4e09\u4e2a\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u3002\u5728SeaDronesSee\u6570\u636e\u96c6\u4e0a\uff0cZoomDet\u4f7f\u7528Faster R-CNN\u6a21\u578b\u5e26\u6765\u4e86\u8d85\u8fc78.4\u4e2a\u7edd\u5bf9mAP\u589e\u76ca\uff0c\u4ec5\u589e\u52a0\u7ea63ms\u5ef6\u8fdf\u3002", "conclusion": "ZoomDet\u662f\u4e00\u79cd\u67b6\u6784\u65e0\u5173\u7684\u81ea\u9002\u5e94\u653e\u5927\u6846\u67b6\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u610f\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.07523", "pdf": "https://arxiv.org/pdf/2602.07523", "abs": "https://arxiv.org/abs/2602.07523", "authors": ["Zhen Zhang", "Qing Zhao", "Xiuhe Li", "Cheng Wang", "Guoqiang Zhu", "Yu Zhang", "Yining Huo", "Hongyi Yu", "Yi Zhang"], "title": "CA-YOLO: Cross Attention Empowered YOLO for Biomimetic Localization", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication.Please note that once the article has been published by IEEE, preprints on locations not specified above should be removed if possible", "summary": "In modern complex environments, achieving accurate and efficient target localization is essential in numerous fields. However, existing systems often face limitations in both accuracy and the ability to recognize small targets. In this study, we propose a bionic stabilized localization system based on CA-YOLO, designed to enhance both target localization accuracy and small target recognition capabilities. Acting as the \"brain\" of the system, the target detection algorithm emulates the visual focusing mechanism of animals by integrating bionic modules into the YOLO backbone network. These modules include the introduction of a small target detection head and the development of a Characteristic Fusion Attention Mechanism (CFAM). Furthermore, drawing inspiration from the human Vestibulo-Ocular Reflex (VOR), a bionic pan-tilt tracking control strategy is developed, which incorporates central positioning, stability optimization, adaptive control coefficient adjustment, and an intelligent recapture function. The experimental results show that CA-YOLO outperforms the original model on standard datasets (COCO and VisDrone), with average accuracy metrics improved by 3.94%and 4.90%, respectively.Further time-sensitive target localization experiments validate the effectiveness and practicality of this bionic stabilized localization system.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCA-YOLO\u7684\u4eff\u751f\u7a33\u5b9a\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u5c0f\u76ee\u6807\u68c0\u6d4b\u5934\u548c\u7279\u5f81\u878d\u5408\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u7ed3\u5408\u4eff\u751f\u4e91\u53f0\u8ddf\u8e2a\u63a7\u5236\u7b56\u7565\uff0c\u5728COCO\u548cVisDrone\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u53473.94%\u548c4.90%\u7684\u5e73\u5747\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u590d\u6742\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u76ee\u6807\u5b9a\u4f4d\u7cfb\u7edf\u5728\u7cbe\u5ea6\u548c\u5c0f\u76ee\u6807\u8bc6\u522b\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u63d0\u5347\u76ee\u6807\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u5bf9\u5c0f\u76ee\u6807\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "1. \u57fa\u4e8eCA-YOLO\u7684\u4eff\u751f\u7a33\u5b9a\u5b9a\u4f4d\u7cfb\u7edf\uff1b2. \u5728YOLO\u9aa8\u5e72\u7f51\u7edc\u4e2d\u96c6\u6210\u4eff\u751f\u6a21\u5757\uff0c\u5305\u62ec\u5c0f\u76ee\u6807\u68c0\u6d4b\u5934\u548c\u7279\u5f81\u878d\u5408\u6ce8\u610f\u529b\u673a\u5236(CFAM)\uff1b3. \u501f\u9274\u4eba\u7c7b\u524d\u5ead\u773c\u53cd\u5c04(VOR)\u5f00\u53d1\u4eff\u751f\u4e91\u53f0\u8ddf\u8e2a\u63a7\u5236\u7b56\u7565\uff0c\u5305\u542b\u4e2d\u5fc3\u5b9a\u4f4d\u3001\u7a33\u5b9a\u6027\u4f18\u5316\u3001\u81ea\u9002\u5e94\u63a7\u5236\u7cfb\u6570\u8c03\u6574\u548c\u667a\u80fd\u91cd\u6355\u83b7\u529f\u80fd\u3002", "result": "CA-YOLO\u5728\u6807\u51c6\u6570\u636e\u96c6(COCO\u548cVisDrone)\u4e0a\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\uff0c\u5e73\u5747\u7cbe\u5ea6\u5206\u522b\u63d0\u53473.94%\u548c4.90%\uff1b\u65f6\u95f4\u654f\u611f\u76ee\u6807\u5b9a\u4f4d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u4eff\u751f\u7a33\u5b9a\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4eff\u751f\u7a33\u5b9a\u5b9a\u4f4d\u7cfb\u7edf\u901a\u8fc7\u4eff\u751f\u89c6\u89c9\u805a\u7126\u673a\u5236\u548c\u524d\u5ead\u773c\u53cd\u5c04\u539f\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u76ee\u6807\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u5c0f\u76ee\u6807\u8bc6\u522b\u80fd\u529b\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.07532", "pdf": "https://arxiv.org/pdf/2602.07532", "abs": "https://arxiv.org/abs/2602.07532", "authors": ["Krishnakant Singh", "Simone Schaub-Meyer", "Stefan Roth"], "title": "Evaluating Object-Centric Models beyond Object Discovery", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://guided-sa.github.io/eval-ocl/", "summary": "Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5e76\u5f15\u5165\u7edf\u4e00\u7684\u4efb\u52a1\u548c\u6307\u6807\u6765\u8054\u5408\u8bc4\u4f30\u5b9a\u4f4d\u80fd\u529b\u548c\u8868\u793a\u6709\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u6a21\u578b\u7684\u8bc4\u4f30\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6a21\u578b\u8868\u793a\u6709\u7528\u6027\u7684\u6d1e\u5bdf\u6709\u9650\uff1b2\uff09\u5b9a\u4f4d\u80fd\u529b\u548c\u8868\u793a\u6709\u7528\u6027\u4f7f\u7528\u5206\u79bb\u7684\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u4e00\u81f4\u3002", "method": "1\uff09\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5b9e\u73b0\u8de8\u591a\u6837\u5316\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u7684\u53ef\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\uff1b2\uff09\u5f15\u5165\u7edf\u4e00\u8bc4\u4f30\u4efb\u52a1\u548c\u6307\u6807\uff0c\u8054\u5408\u8bc4\u4f30\u5b9a\u4f4d\uff08where\uff09\u548c\u8868\u793a\u6709\u7528\u6027\uff08what\uff09\uff1b3\uff09\u5305\u542b\u7b80\u5355\u7684\u591a\u7279\u5f81\u91cd\u5efa\u57fa\u7ebf\u4f5c\u4e3a\u53c2\u8003\u70b9\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8861\u91cf\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e2d\u5b9a\u4f4d\u548c\u8868\u793a\u8bc4\u4f30\u5206\u79bb\u7684\u95ee\u9898\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u66f4\u4e00\u81f4\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u8bc4\u4f30\u5b9a\u4f4d\u548c\u8868\u793a\u6709\u7528\u6027\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8861\u91cf\u6a21\u578b\u5728\u652f\u6301\u7ec4\u5408\u6cdb\u5316\u548cOOD\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5b9e\u9645\u6548\u679c\u3002"}}
{"id": "2602.07534", "pdf": "https://arxiv.org/pdf/2602.07534", "abs": "https://arxiv.org/abs/2602.07534", "authors": ["Mowmita Parvin Hera", "Md. Shahriar Mahmud Kallol", "Shohanur Rahman Nirob", "Md. Badsha Bulbul", "Jubayer Ahmed", "M. Zhourul Islam", "Hazrat Ali", "Mohammmad Farhad Bulbul"], "title": "Fine-Grained Cat Breed Recognition with Global Context Vision Transformer", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "4 pages, accepted at International Conference on Computer and Information Technology (ICCIT) 2025", "summary": "Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.", "AI": {"tldr": "\u4f7f\u7528GCViT-Tiny\u67b6\u6784\u5728Oxford-IIIT Pet Dataset\u5b50\u96c6\u4e0a\u5b9e\u73b0\u732b\u54c1\u79cd\u5206\u7c7b\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u8fbe\u523092.00%\u6d4b\u8bd5\u51c6\u786e\u7387", "motivation": "\u732b\u54c1\u79cd\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e0d\u540c\u54c1\u79cd\u5728\u6bdb\u76ae\u56fe\u6848\u3001\u9762\u90e8\u7ed3\u6784\u548c\u989c\u8272\u4e0a\u5dee\u5f02\u7ec6\u5fae\u3002\u9700\u8981\u5f00\u53d1\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u652f\u6301\u517d\u533b\u8bca\u65ad\u3001\u52a8\u7269\u6536\u5bb9\u6240\u7ba1\u7406\u548c\u79fb\u52a8\u7aef\u8bc6\u522b\u7cfb\u7edf\u7b49\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u89c6\u89c9\u53d8\u6362\u5668\uff08GCViT-Tiny\uff09\u67b6\u6784\u8fdb\u884c\u732b\u54c1\u79cd\u8bc6\u522b\uff0c\u4f7f\u7528Oxford-IIIT Pet Dataset\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5b50\u96c6\uff0c\u901a\u8fc7\u65cb\u8f6c\u3001\u6c34\u5e73\u7ffb\u8f6c\u548c\u4eae\u5ea6\u8c03\u6574\u7b49\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "GCViT-Tiny\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523092.00%\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523094.54%\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u67b6\u6784\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u53d8\u6362\u5668\u67b6\u6784\u5728\u732b\u54c1\u79cd\u5206\u7c7b\u7b49\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5305\u62ec\u517d\u533b\u8bca\u65ad\u3001\u52a8\u7269\u6536\u5bb9\u6240\u7ba1\u7406\u548c\u79fb\u52a8\u7aef\u8bc6\u522b\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u4e86Hugging Face\u6f14\u793a\u3002"}}
{"id": "2602.07535", "pdf": "https://arxiv.org/pdf/2602.07535", "abs": "https://arxiv.org/abs/2602.07535", "authors": ["Md Sazidur Rahman", "Kjersti Engan", "Kathinka D\u00e6hli Kurz", "Mahdieh Khanmohammadi"], "title": "Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u65f6\u95f4\u70b9\u5206\u6790\u6846\u67b6\uff0c\u5229\u7528\u7edf\u8ba1\u63cf\u8ff0\u7b26\u3001\u653e\u5c04\u7ec4\u5b66\u7eb9\u7406\u7279\u5f81\u548c\u6df1\u5ea6\u7279\u5f81\u5d4c\u5165\uff0c\u5206\u6790\u8111\u5352\u4e2d\u7f3a\u8840\u7ec4\u7ec7\u7684\u65f6\u7a7a\u6f14\u5316\uff0c\u533a\u5206\u53ef\u633d\u6551\u4e0e\u4e0d\u53ef\u633d\u6551\u7ec4\u7ec7\u3002", "motivation": "\u4f20\u7edf\u5355\u65f6\u95f4\u70b9\u5206\u5272\u65e0\u6cd5\u6355\u6349\u8111\u5352\u4e2d\u751f\u7269\u5b66\u5f02\u8d28\u6027\u548c\u65f6\u95f4\u6f14\u5316\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u8868\u5f81\u7ec4\u7ec7\u72b6\u6001\u8f6c\u53d8\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u53cc\u65f6\u95f4\u70b9\uff08\u5165\u9662T1\u548c\u968f\u8bbfT2\uff09CT\u704c\u6ce8\u548cDWI\u6570\u636e\uff0c\u63d0\u53d6\u7edf\u8ba1\u3001\u653e\u5c04\u7ec4\u5b66\u7eb9\u7406\u548c\u6df1\u5ea6\u7279\u5f81\uff08mJ-Net\u548cnnU-Net\uff09\uff0c\u6784\u5efa6\u4e2aROI\u533a\u57df\uff0c\u5728\u7279\u5f81\u7a7a\u95f4\u5206\u6790\u7ec4\u7ec7\u6f14\u5316\u3002", "result": "\u572818\u540d\u6210\u529f\u518d\u704c\u6ce8\u60a3\u8005\u4e2d\uff0c\u533a\u57df\u7ea7\u8868\u5f81\u5448\u73b0\u6709\u610f\u4e49\u7684\u805a\u7c7b\uff1a\u53ef\u6062\u590d\u7684\u7f3a\u8840\u534a\u6697\u5e26\u4e0e\u5065\u5eb7\u7ec4\u7ec7\u7279\u5f81\u76f8\u4f3c\uff0c\u6897\u6b7b\u533a\u57df\u5f62\u6210\u72ec\u7acb\u5206\u7ec4\u3002\u6df1\u5ea6\u7279\u5f81\u7a7a\u95f4\uff08\u7279\u522b\u662fmJ-Net\uff09\u80fd\u663e\u8457\u533a\u5206\u53ef\u633d\u6551\u4e0e\u4e0d\u53ef\u633d\u6551\u7ec4\u7ec7\u3002", "conclusion": "\u7f16\u7801\u5668\u884d\u751f\u7684\u7279\u5f81\u6d41\u5f62\u53cd\u6620\u4e86\u5e95\u5c42\u7ec4\u7ec7\u8868\u578b\u548c\u72b6\u6001\u8f6c\u53d8\uff0c\u4e3a\u57fa\u4e8e\u5f71\u50cf\u7684\u8111\u5352\u4e2d\u6f14\u5316\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2602.07540", "pdf": "https://arxiv.org/pdf/2602.07540", "abs": "https://arxiv.org/abs/2602.07540", "authors": ["Huimin Yan", "Liang Bai", "Xian Yang", "Long Chen"], "title": "LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.", "AI": {"tldr": "\u63d0\u51faLGDEA\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u63d0\u53d6\u8bca\u65ad\u8bc1\u636e\u6784\u5efa\u5171\u4eab\u8bc1\u636e\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8bc1\u636e\u7ea7\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u51cf\u5c11\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u5168\u5c40\u5bf9\u9f50\u5bb9\u6613\u88ab\u975e\u8bca\u65ad\u4fe1\u606f\u4e3b\u5bfc\uff0c\u5c40\u90e8\u5bf9\u9f50\u65e0\u6cd5\u6574\u5408\u5173\u952e\u8bca\u65ad\u8bc1\u636e\uff0c\u5bfc\u81f4\u96be\u4ee5\u5b66\u4e60\u53ef\u9760\u7684\u8bca\u65ad\u8868\u793a\uff0c\u9650\u5236\u4e86\u5728\u914d\u5bf9\u6570\u636e\u6709\u9650\u573a\u666f\u7684\u5e94\u7528", "method": "LGDEA\u65b9\u6cd5\uff1a\u5229\u7528LLM\u4ece\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u63d0\u53d6\u5173\u952e\u8bca\u65ad\u8bc1\u636e\uff0c\u6784\u5efa\u5171\u4eab\u8bca\u65ad\u8bc1\u636e\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8bc1\u636e\u611f\u77e5\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u5927\u91cf\u672a\u914d\u5bf9\u7684\u533b\u5b66\u56fe\u50cf\u548c\u62a5\u544a", "result": "\u5728\u77ed\u8bed\u5b9a\u4f4d\u3001\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u548c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u751a\u81f3\u80fd\u4e0e\u4f9d\u8d56\u5927\u91cf\u914d\u5bf9\u6570\u636e\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5ab2\u7f8e", "conclusion": "\u901a\u8fc7\u8bc1\u636e\u7ea7\u5bf9\u9f50\u65b9\u6cd5\uff0cLGDEA\u80fd\u591f\u66f4\u7b26\u5408\u533b\u5b66\u8bca\u65ad\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5728\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2602.07544", "pdf": "https://arxiv.org/pdf/2602.07544", "abs": "https://arxiv.org/abs/2602.07544", "authors": ["Sebastian Bock", "Leonie Sch\u00fc\u00dfler", "Krishnakant Singh", "Simone Schaub-Meyer", "Stefan Roth"], "title": "MUFASA: A Multi-Layer Framework for Slot Attention", "categories": ["cs.CV"], "comment": "Authors Sebastian Bock and Leonie Sch\u00fc\u00dfler contributed equally. Project page: https://leonieschuessler.github.io/mufasa/", "summary": "Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.", "AI": {"tldr": "MUFASA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u5c42ViT\u7279\u5f81\u4e0a\u6267\u884cslot attention\u6765\u63d0\u5347\u65e0\u76d1\u7763\u7269\u4f53\u5206\u5272\u6027\u80fd\uff0c\u5229\u7528\u5404\u5c42\u7684\u8bed\u4e49\u4fe1\u606f\u6539\u5584\u5206\u5272\u7ed3\u679c\u548c\u8bad\u7ec3\u6536\u655b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eslot attention\u7684\u65e0\u76d1\u7763\u7269\u4f53\u4e2d\u5fc3\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f7f\u7528\u9884\u8bad\u7ec3ViT\u7684\u6700\u540e\u4e00\u5c42\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u5c42\u5305\u542b\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\u3002\u4e3a\u4e86\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u6f5c\u5728\u8bed\u4e49\u4fe1\u606f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6574\u5408\u591a\u5c42\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMUFASA\u6846\u67b6\uff0c\u5728ViT\u7f16\u7801\u5668\u7684\u591a\u4e2a\u7279\u5f81\u5c42\u4e0a\u8ba1\u7b97slot attention\uff0c\u5145\u5206\u5229\u7528\u5404\u5c42\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u3002\u8bbe\u8ba1\u878d\u5408\u7b56\u7565\u5c06\u591a\u5c42\u83b7\u5f97\u7684slot\u805a\u5408\u6210\u7edf\u4e00\u7684\u7269\u4f53\u4e2d\u5fc3\u8868\u793a\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709OCL\u65b9\u6cd5\u4e2d\u3002", "result": "\u5c06MUFASA\u96c6\u6210\u5230\u73b0\u6709OCL\u65b9\u6cd5\u4e2d\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6539\u5584\u4e86\u5206\u5272\u7ed3\u679c\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u4ec5\u589e\u52a0\u5c11\u91cf\u63a8\u7406\u5f00\u9500\u5c31\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "MUFASA\u901a\u8fc7\u5229\u7528ViT\u591a\u5c42\u7279\u5f81\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8eslot attention\u7684\u65e0\u76d1\u7763\u7269\u4f53\u5206\u5272\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6574\u5408\u591a\u5c42\u7279\u5f81\u5bf9\u7269\u4f53\u4e2d\u5fc3\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.07550", "pdf": "https://arxiv.org/pdf/2602.07550", "abs": "https://arxiv.org/abs/2602.07550", "authors": ["Hussni Mohd Zakir", "Eric Tatt Wei Ho"], "title": "Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 3 figures, 7 tables", "summary": "Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a \"Safest vs. Optimal\" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a \"Semantic Selection Gap\" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the \"Last-Layer\" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFSSDINO\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5229\u7528DINOv3\u7684\u51bb\u7ed3\u7279\u5f81\u3001\u7c7b\u522b\u539f\u578b\u548cGram\u77e9\u9635\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u7814\u7a76\u53d1\u73b0\"\u6700\u540e\u5c42\"\u7279\u5f81\u662f\u4e00\u4e2a\u6b3a\u9a97\u6027\u5f3a\u7684\u57fa\u7ebf\uff0c\u800c\u4e2d\u95f4\u5c42\u5b58\u5728\u66f4\u9ad8\u6027\u80fd\u6f5c\u529b\uff0c\u4f46\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u96be\u4ee5\u53ef\u9760\u8bc6\u522b\u8fd9\u4e9b\u9ad8\u8d28\u91cf\u7279\u5f81\uff0c\u63ed\u793a\u4e86\"\u8bed\u4e49\u9009\u62e9\u9e3f\u6c9f\"\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u81ea\u76d1\u7763Vision Transformers\uff08\u5982DINOv3\uff09\u5728\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u5185\u5728\u80fd\u529b\u3002\u867d\u7136DINOv3\u7b49\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u51bb\u7ed3\u7279\u5f81\u8fdb\u884c\u5c11\u6837\u672c\u5206\u5272\uff0c\u4ee5\u53ca\u4e0d\u540c\u5c42\u7279\u5f81\u7684\u8d28\u91cf\u5dee\u5f02\uff0c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faFSSDINO\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u51bb\u7ed3\u7684DINOv3\u7279\u5f81\uff1b2\uff09\u57fa\u4e8e\u7c7b\u522b\u7279\u5b9a\u539f\u578b\u8fdb\u884c\u5206\u5272\uff1b3\uff09\u901a\u8fc7Gram\u77e9\u9635\u4f18\u5316\u7279\u5f81\u8868\u793a\u3002\u8fd9\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u4e3b\u5e72\u7f51\u7edc\u7684\u6700\u7ec8\u5c42\u7279\u5f81\u3002", "result": "\u5728\u4e8c\u5143\u3001\u591a\u7c7b\u522b\u548c\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFSSDINO\u4e0e\u9700\u8981\u590d\u6742\u89e3\u7801\u5668\u6216\u6d4b\u8bd5\u65f6\u9002\u5e94\u7684\u4e13\u95e8\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u3002\u901a\u8fc7Oracle\u5f15\u5bfc\u7684\u5c42\u5206\u6790\u53d1\u73b0\uff0c\u6807\u51c6\u6700\u540e\u5c42\u7279\u5f81\u4e0e\u5168\u5c40\u6700\u4f18\u4e2d\u95f4\u8868\u793a\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\"\u6700\u5b89\u5168vs\u6700\u4f18\"\u7684\u56f0\u5883\u3002", "conclusion": "\u7814\u7a76\u786e\u7acb\u4e86\"\u6700\u540e\u5c42\"\u4f5c\u4e3a\u6b3a\u9a97\u6027\u5f3a\u7684\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86DINOv3\u4e2d\u6f5c\u5728\u7684\u8bed\u4e49\u80fd\u529b\u3002\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u9ad8\u8d28\u91cf\u7279\u5f81\uff0c\u5b58\u5728\"\u8bed\u4e49\u9009\u62e9\u9e3f\u6c9f\"\u3002\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8bca\u65ad\uff0c\u8868\u660e\u57fa\u7840\u6a21\u578b\u4e2d\u5b58\u5728\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u8bed\u4e49\u6f5c\u529b\u3002"}}
{"id": "2602.07554", "pdf": "https://arxiv.org/pdf/2602.07554", "abs": "https://arxiv.org/abs/2602.07554", "authors": ["Guandong Li", "Yijun Ding"], "title": "FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.", "AI": {"tldr": "FlexID\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u4e2a\u4eba\u5316\u6587\u751f\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u8c03\u5236\u6b63\u4ea4\u89e3\u8026\u8eab\u4efd\u7279\u5f81\uff0c\u5728\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u6587\u672c\u9002\u914d\u6027\u4e4b\u95f4\u5b9e\u73b0\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u521a\u6027\u7684\u89c6\u89c9\u7279\u5f81\u6ce8\u5165\uff0c\u5bfc\u81f4\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u9002\u914d\u6027\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\u3002\u9700\u8981\u4e00\u79cd\u80fd\u52a8\u6001\u5e73\u8861\u8fd9\u4e24\u8005\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFlexID\u6846\u67b6\uff1a1) \u8bed\u4e49\u8eab\u4efd\u6295\u5f71\u5668(SIP)\u5728\u8bed\u8a00\u7a7a\u95f4\u6ce8\u5165\u9ad8\u5c42\u5148\u9a8c\uff1b2) \u89c6\u89c9\u7279\u5f81\u951a\u70b9(VFA)\u5728\u6f5c\u5728\u7a7a\u95f4\u786e\u4fdd\u7ed3\u6784\u4fdd\u771f\u5ea6\uff1b3) \u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u9002\u5e94\u95e8\u63a7(CAG)\u673a\u5236\u6839\u636e\u7f16\u8f91\u610f\u56fe\u548c\u6269\u6563\u65f6\u95f4\u6b65\u52a8\u6001\u8c03\u5236\u4e24\u4e2a\u6d41\u6743\u91cd\u3002", "result": "\u5728IBench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFlexID\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u6587\u672c\u9075\u5faa\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e73\u8861\uff0c\u4e3a\u590d\u6742\u53d9\u4e8b\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "FlexID\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u8c03\u5236\u6b63\u4ea4\u89e3\u8026\u8eab\u4efd\u7279\u5f81\uff0c\u80fd\u81ea\u52a8\u653e\u677e\u521a\u6027\u89c6\u89c9\u7ea6\u675f\uff0c\u5b9e\u73b0\u8eab\u4efd\u4fdd\u6301\u548c\u8bed\u4e49\u53d8\u5316\u7684\u534f\u540c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.07555", "pdf": "https://arxiv.org/pdf/2602.07555", "abs": "https://arxiv.org/abs/2602.07555", "authors": ["Francesco Taioli", "Shiping Yang", "Sonia Raychaudhuri", "Marco Cristani", "Unnat Jain", "Angel X Chang"], "title": "VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer \"Is this the target object?\" and \"Why should I take this action?\" The reasoning process unfolds in three stages: \"think\", \"think summary\", and \"action\", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a3B\u53c2\u6570\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u663e\u5f0f\u56fe\u50cf\u63a8\u7406\u76f4\u63a5\u56de\u7b54\u76ee\u6807\u8bc6\u522b\u548c\u52a8\u4f5c\u9009\u62e9\u95ee\u9898\uff0c\u66ff\u4ee3\u4f20\u7edf\u591a\u6a21\u578b\u7ba1\u9053\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a(1)\u7aef\u5230\u7aef\u8bad\u7ec3\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u4e14\u7f3a\u4e4f\u52a8\u4f5c\u7ea7\u53ef\u89e3\u91ca\u6027\uff1b(2)\u6a21\u5757\u5316\u96f6\u6837\u672c\u7ba1\u9053\u5b58\u5728\u9519\u8bef\u4f20\u64ad\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u5c06\u63a8\u7406\u6574\u5408\u5230\u5bfc\u822a\u7b56\u7565\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa3B\u53c2\u6570\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u663e\u5f0f\u56fe\u50cf\u63a8\u7406\u800c\u975e\u539f\u59cb\u5d4c\u5165\u5339\u914d\u3002\u63a8\u7406\u8fc7\u7a0b\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\"\u601d\u8003\"\u3001\"\u601d\u8003\u603b\u7ed3\"\u548c\"\u52a8\u4f5c\"\uff0c\u76f4\u63a5\u56de\u7b54\"\u8fd9\u662f\u76ee\u6807\u5bf9\u8c61\u5417\uff1f\"\u548c\"\u4e3a\u4ec0\u4e48\u91c7\u53d6\u8fd9\u4e2a\u52a8\u4f5c\uff1f\"", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6539\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u3001\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u9ad8\u6548\u7684\u5bfc\u822a\uff0c\u6d88\u9664\u4e86\u591a\u6a21\u578b\u7ba1\u9053\u62fc\u63a5\u7684\u9700\u6c42\u3002", "conclusion": "\u901a\u8fc7\u7d27\u51d1\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u667a\u80fd\u4f53\u5b9e\u73b0\u4eba\u7c7b\u822c\u7684\u5177\u8eab\u63a8\u7406\uff0c\u5728\u76ee\u6807\u8bc6\u522b\u548c\u52a8\u4f5c\u9009\u62e9\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.07564", "pdf": "https://arxiv.org/pdf/2602.07564", "abs": "https://arxiv.org/abs/2602.07564", "authors": ["Xiaoyan Zhang", "Zechen Bai", "Haofan Wang", "Yiren Song"], "title": "SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens", "categories": ["cs.CV"], "comment": null, "summary": "Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.", "AI": {"tldr": "SIGMA\u662f\u4e00\u4e2a\u57fa\u4e8eBagel\u7edf\u4e00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u9009\u62e9\u6027\u591a\u5c5e\u6027\u4ee4\u724c\uff0c\u652f\u6301\u5728\u6269\u6563\u53d8\u6362\u5668\u4e2d\u5b9e\u73b0\u4ea4\u9519\u591a\u6761\u4ef6\u751f\u6210\uff0c\u63d0\u5347\u7ec4\u5408\u7f16\u8f91\u3001\u5c5e\u6027\u8fc1\u79fb\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u5982Bagel\u867d\u7136\u80fd\u5c06\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u5bf9\u9f50\u5230\u5355\u4e2a\u6269\u6563\u53d8\u6362\u5668\u4e2d\uff0c\u4f46\u4ec5\u9650\u4e8e\u5355\u6761\u4ef6\u8f93\u5165\uff0c\u7f3a\u4e4f\u4ece\u591a\u4e2a\u5f02\u6784\u6e90\u5408\u6210\u7ed3\u679c\u7684\u7075\u6d3b\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4ea4\u9519\u591a\u6761\u4ef6\u751f\u6210\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSIGMA\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u5f15\u5165\u9009\u62e9\u6027\u591a\u5c5e\u6027\u4ee4\u724c\uff08\u98ce\u683c\u3001\u5185\u5bb9\u3001\u4e3b\u9898\u3001\u8eab\u4efd\u4ee4\u724c\uff09\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u89e3\u91ca\u548c\u7ec4\u5408\u4ea4\u9519\u6587\u672c-\u56fe\u50cf\u5e8f\u5217\u4e2d\u7684\u591a\u4e2a\u89c6\u89c9\u6761\u4ef6\u3002\u5728Bagel\u7edf\u4e00\u9aa8\u5e72\u4e0a\u4f7f\u752870\u4e07\u4e2a\u4ea4\u9519\u793a\u4f8b\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "SIGMA\u5728\u7ec4\u5408\u7f16\u8f91\u3001\u9009\u62e9\u6027\u5c5e\u6027\u8fc1\u79fb\u548c\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4Bagel\u5728\u7ec4\u5408\u4efb\u52a1\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u6539\u5584\u4e86\u53ef\u63a7\u6027\u3001\u8de8\u6761\u4ef6\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "SIGMA\u901a\u8fc7\u5f15\u5165\u591a\u5c5e\u6027\u4ee4\u724c\u548c\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u6210\u529f\u6269\u5c55\u4e86\u7edf\u4e00\u6269\u6563\u53d8\u6362\u5668\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u4ea4\u9519\u591a\u6761\u4ef6\u751f\u6210\uff0c\u4e3a\u7ec4\u5408\u89c6\u89c9\u7f16\u8f91\u548c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07565", "pdf": "https://arxiv.org/pdf/2602.07565", "abs": "https://arxiv.org/abs/2602.07565", "authors": ["Jingzhe Ma", "Meng Zhang", "Jianlong Yu", "Kun Liu", "Zunxiao Xu", "Xue Cheng", "Junjie Zhou", "Yanfei Wang", "Jiahang Li", "Zepeng Wang", "Kazuki Osamura", "Rujie Liu", "Narishige Abe", "Jingjie Wang", "Shunli Zhang", "Haojun Xie", "Jiajun Wu", "Weiming Wu", "Wenxiong Kang", "Qingshuo Gao", "Jiaming Xiong", "Xianye Ben", "Lei Chen", "Lichen Song", "Junjian Cui", "Haijun Xiong", "Junhao Lu", "Bin Feng", "Mengyuan Liu", "Ji Zhou", "Baoquan Zhao", "Ke Xu", "Yongzhen Huang", "Liang Wang", "Manuel J Marin-Jimenez", "Md Atiqur Rahman Ahad", "Shiqi Yu"], "title": "Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025", "categories": ["cs.CV"], "comment": "Accepted by IJCB 2025(https://ijcb2025.ieee-biometrics.org/competitions/)", "summary": "Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86HID\u7ade\u8d5b\u4f7f\u7528SUSTech-Competition\u6570\u636e\u96c6\u8fdb\u884c\u6b65\u6001\u8bc6\u522b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c2025\u5e74\u6700\u4f73\u65b9\u6cd5\u8fbe\u523094.2%\u51c6\u786e\u7387\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "motivation": "\u8fdc\u8ddd\u79bb\u4eba\u4f53\u8bc6\u522b(HID)\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4f20\u7edf\u751f\u7269\u7279\u5f81\u5982\u4eba\u8138\u548c\u6307\u7eb9\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002\u6b65\u6001\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u5728\u8fdc\u8ddd\u79bb\u53ef\u9760\u6355\u83b7\u3002HID\u7ade\u8d5b\u65e8\u5728\u4fc3\u8fdb\u6b65\u6001\u8bc6\u522b\u8fdb\u5c55\u5e76\u63d0\u4f9b\u516c\u5e73\u8bc4\u4f30\u5e73\u53f0\u3002", "method": "\u81ea2023\u5e74\u8d77\uff0c\u7ade\u8d5b\u91c7\u7528\u5177\u6709\u6311\u6218\u6027\u7684SUSTech-Competition\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u670d\u88c5\u3001\u643a\u5e26\u7269\u54c1\u548c\u89c6\u89d2\u7684\u663e\u8457\u53d8\u5316\u3002\u4e0d\u63d0\u4f9b\u4e13\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u53c2\u8d5b\u8005\u9700\u4f7f\u7528\u5916\u90e8\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002\u6bcf\u5e74\u4f7f\u7528\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u751f\u6210\u4e0d\u540c\u8bc4\u4f30\u5206\u5272\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u98ce\u9669\u5e76\u652f\u6301\u8de8\u57df\u6cdb\u5316\u7684\u516c\u5e73\u8bc4\u4f30\u3002", "result": "HID 2025\u660e\u786e\u68c0\u9a8c\u7b97\u6cd5\u8fdb\u5c55\u662f\u5426\u80fd\u8d85\u8d8a\u5148\u524d\u89c2\u5bdf\u5230\u7684\u51c6\u786e\u7387\u6781\u9650\u3002\u5c3d\u7ba1\u96be\u5ea6\u589e\u52a0\uff0c\u53c2\u8d5b\u8005\u4ecd\u53d6\u5f97\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u6700\u4f73\u65b9\u6cd5\u8fbe\u523094.2%\u51c6\u786e\u7387\uff0c\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u521b\u4e0b\u65b0\u57fa\u51c6\u3002", "conclusion": "\u8bba\u6587\u5206\u6790\u4e86\u5173\u952e\u6280\u672f\u8d8b\u52bf\uff0c\u5e76\u6982\u8ff0\u4e86\u6b65\u6001\u8bc6\u522b\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002HID\u7ade\u8d5b\u901a\u8fc7\u516c\u5e73\u8bc4\u4f30\u5e73\u53f0\u6301\u7eed\u63a8\u52a8\u6b65\u6001\u8bc6\u522b\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07566", "pdf": "https://arxiv.org/pdf/2602.07566", "abs": "https://arxiv.org/abs/2602.07566", "authors": ["Runcheng Wang", "Yaru Chen", "Guiguo Zhang", "Honghua Jiang", "Yongliang Qiao"], "title": "Cross-Camera Cow Identification via Disentangled Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89e3\u8026\u8868\u5f81\u5b66\u4e60\u7684\u8de8\u6444\u50cf\u5934\u5976\u725b\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u5b50\u7a7a\u95f4\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1\u7406\u8bba\uff0c\u901a\u8fc7\u7279\u5f81\u89e3\u8026\u6a21\u5757\u5206\u79bb\u8eab\u4efd\u76f8\u5173\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6444\u50cf\u5934\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u52a8\u7269\u8bc6\u522b\u65b9\u6cd5\u5728\u53d7\u63a7\u5355\u6444\u50cf\u5934\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8de8\u6444\u50cf\u5934\u573a\u666f\u4e2d\u9762\u4e34\u4e25\u91cd\u6cdb\u5316\u6311\u6218\u3002\u5f53\u6a21\u578b\u4ece\u6e90\u6444\u50cf\u5934\u90e8\u7f72\u5230\u5177\u6709\u4e0d\u540c\u5149\u7167\u3001\u80cc\u666f\u3001\u89c6\u89d2\u548c\u6210\u50cf\u7279\u6027\u7684\u65b0\u76d1\u63a7\u8282\u70b9\u65f6\uff0c\u8bc6\u522b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u975e\u63a5\u89e6\u6280\u672f\u5728\u52a8\u6001\u771f\u5b9e\u519c\u573a\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89e3\u8026\u8868\u5f81\u5b66\u4e60\u7684\u8de8\u6444\u50cf\u5934\u5976\u725b\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u5b50\u7a7a\u95f4\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1\u7406\u8bba\uff0c\u8bbe\u8ba1\u539f\u5219\u9a71\u52a8\u7684\u7279\u5f81\u89e3\u8026\u6a21\u5757\uff0c\u5c06\u89c2\u6d4b\u56fe\u50cf\u5206\u89e3\u4e3a\u591a\u4e2a\u6b63\u4ea4\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u6709\u6548\u5206\u79bb\u8de8\u6444\u50cf\u5934\u4e0d\u53d8\u7684\u7a33\u5b9a\u8eab\u4efd\u76f8\u5173\u751f\u7269\u7279\u5f81\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u4e94\u4e2a\u4e0d\u540c\u6444\u50cf\u5934\u8282\u70b9\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5f02\u6784\u91c7\u96c6\u8bbe\u5907\u548c\u590d\u6742\u7684\u5149\u7167\u89d2\u5ea6\u53d8\u5316\u3002\u5728\u4e03\u4e2a\u8de8\u6444\u50cf\u5934\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523086.0%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u6e90\u6444\u50cf\u5934\u57fa\u7ebf\uff0851.9%\uff09\u548c\u6700\u5f3a\u7684\u8de8\u6444\u50cf\u5934\u57fa\u7ebf\u65b9\u6cd5\uff0879.8%\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u4e8e\u5b50\u7a7a\u95f4\u7406\u8bba\u7684\u7279\u5f81\u89e3\u8026\u6846\u67b6\uff0c\u7528\u4e8e\u534f\u540c\u8de8\u6444\u50cf\u5934\u5976\u725b\u8bc6\u522b\uff0c\u4e3a\u4e0d\u53d7\u63a7\u667a\u80fd\u519c\u573a\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.07568", "pdf": "https://arxiv.org/pdf/2602.07568", "abs": "https://arxiv.org/abs/2602.07568", "authors": ["Hui Ye", "Shilong Yang", "Yexuan Xing", "Juan Yu", "Yaoqin Xie", "Wei Zhang", "Chulong Zhang"], "title": "Visualizing the Invisible: Enhancing Radiologist Performance in Breast Mammography via Task-Driven Chromatic Encoding", "categories": ["cs.CV"], "comment": null, "summary": "Purpose:Mammography screening is less sensitive in dense breasts, where tissue overlap and subtle findings increase perceptual difficulty. We present MammoColor, an end-to-end framework with a Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views for visual augmentation. Materials and Methods:MammoColor couples a lightweight TDCE module with a BI-RADS triage classifier and was trained end-to-end on VinDr-Mammo. Performance was evaluated on an internal test set, two public datasets (CBIS-DDSM and INBreast), and three external clinical cohorts. We also conducted a multi-reader, multi-case (MRMC) observer study with a washout period, comparing (1) grayscale-only, (2) TDCE-only, and (3) side-by-side grayscale+TDCE. Results:On VinDr-Mammo, MammoColor improved AUC from 0.7669 to 0.8461 (P=0.004). Gains were larger in dense breasts (AUC 0.749 to 0.835). In the MRMC study, TDCE-encoded images improved specificity (0.90 to 0.96; P=0.052) with comparable sensitivity. Conclusion:TDCE provides a task-optimized chromatic representation that may improve perceptual salience and reduce false-positive recalls in mammography triage.", "AI": {"tldr": "MammoColor\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u7684\u8272\u5f69\u7f16\u7801\u5c06\u5355\u901a\u9053\u4e73\u817aX\u5149\u7247\u8f6c\u6362\u4e3a\u5f69\u8272\u589e\u5f3a\u89c6\u56fe\uff0c\u63d0\u9ad8\u81f4\u5bc6\u4e73\u817a\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e73\u817aX\u5149\u7b5b\u67e5\u5728\u81f4\u5bc6\u4e73\u817a\u4e2d\u654f\u611f\u6027\u8f83\u4f4e\uff0c\u7ec4\u7ec7\u91cd\u53e0\u548c\u7ec6\u5fae\u53d1\u73b0\u589e\u52a0\u4e86\u611f\u77e5\u96be\u5ea6\uff0c\u9700\u8981\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86MammoColor\u6846\u67b6\uff0c\u5305\u542b\u4efb\u52a1\u9a71\u52a8\u8272\u5f69\u7f16\u7801\u6a21\u5757\uff0c\u4e0eBI-RADS\u5206\u7c7b\u5668\u8026\u5408\uff0c\u5728VinDr-Mammo\u6570\u636e\u96c6\u4e0a\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4e34\u5e8a\u961f\u5217\u4e2d\u8bc4\u4f30\u3002", "result": "\u5728VinDr-Mammo\u4e0aAUC\u4ece0.7669\u63d0\u5347\u52300.8461\uff0c\u81f4\u5bc6\u4e73\u817a\u63d0\u5347\u66f4\u5927\uff080.749\u52300.835\uff09\u3002\u591a\u8bfb\u8005\u7814\u7a76\u4e2d\u7279\u5f02\u6027\u4ece0.90\u63d0\u9ad8\u52300.96\uff0c\u654f\u611f\u6027\u76f8\u5f53\u3002", "conclusion": "\u4efb\u52a1\u9a71\u52a8\u8272\u5f69\u7f16\u7801\u63d0\u4f9b\u4e86\u4f18\u5316\u7684\u8272\u5f69\u8868\u793a\uff0c\u53ef\u80fd\u63d0\u9ad8\u611f\u77e5\u663e\u8457\u6027\u5e76\u51cf\u5c11\u4e73\u817aX\u5149\u5206\u8bca\u4e2d\u7684\u5047\u9633\u6027\u53ec\u56de\u3002"}}
{"id": "2602.07574", "pdf": "https://arxiv.org/pdf/2602.07574", "abs": "https://arxiv.org/abs/2602.07574", "authors": ["Wenjie Liu", "Hao Wu", "Xin Qiu", "Yingqi Fan", "Yihan Zhang", "Anhao Zhao", "Yunpu Ma", "Xiaoyu Shen"], "title": "ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.", "AI": {"tldr": "ViCA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u7a00\u758f\u8de8\u6ce8\u610f\u529b\u673a\u5236\u5927\u5e45\u51cf\u5c11\u89c6\u89c9\u5904\u7406\u8ba1\u7b97\u91cf\uff0c\u4ec5\u4fdd\u75594%\u7684\u89c6\u89c9\u4fa7\u8ba1\u7b97\u5c31\u80fd\u8fbe\u523098%\u7684\u57fa\u7ebf\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709MLLM\u91c7\u7528\u7edf\u4e00\u7684\u6ce8\u610f\u529b\u8bbe\u8ba1\uff0c\u5728\u6bcf\u4e2aTransformer\u5c42\u90fd\u5904\u7406\u89c6\u89c9\u548c\u6587\u672ctoken\uff0c\u5bfc\u81f4\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u7814\u7a76\u53d1\u73b0\u6295\u5f71\u540e\u7684\u89c6\u89c9\u5d4c\u5165\u5df2\u4e0e\u8bed\u8a00\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4e14\u6709\u6548\u7684\u89c6\u89c9-\u8bed\u8a00\u4ea4\u4e92\u4ec5\u53d1\u751f\u5728\u5c11\u6570\u5c42\u4e2d\u3002", "method": "\u63d0\u51faViCA\u67b6\u6784\uff0c\u89c6\u89c9token\u7ed5\u8fc7\u6240\u6709\u81ea\u6ce8\u610f\u529b\u548c\u524d\u9988\u5c42\uff0c\u4ec5\u901a\u8fc7\u9009\u5b9a\u7684\u7a00\u758f\u8de8\u6ce8\u610f\u529b\u5c42\u4e0e\u6587\u672c\u4ea4\u4e92\uff0c\u5f62\u6210\u786c\u4ef6\u53cb\u597d\u7684\u63a8\u7406\u6d41\u7a0b\u3002", "result": "\u57283\u4e2aMLLM\u9aa8\u5e72\u30019\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u548c26\u4e2a\u526a\u679d\u57fa\u7ebf\u6d4b\u8bd5\u4e2d\uff0cViCA\u4fdd\u630198%\u57fa\u7ebf\u51c6\u786e\u7387\uff0c\u89c6\u89c9\u4fa7\u8ba1\u7b97\u964d\u81f34%\uff0c\u5355\u6279\u6b21\u63a8\u7406\u52a0\u901f3.5\u500d\uff0c\u591a\u6279\u6b21\u52a0\u901f10\u500d\u4ee5\u4e0a\uff0c\u89c6\u89c9\u5904\u7406\u5f00\u9500\u63a5\u8fd1\u7eaf\u6587\u672cLLM\u3002", "conclusion": "ViCA\u901a\u8fc7\u7a00\u758f\u8de8\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\uff0c\u63d0\u4f9b\u4e86\u786c\u4ef6\u53cb\u597d\u7684\u63a8\u7406\u6d41\u7a0b\uff0c\u4e14\u4e0etoken\u526a\u679d\u65b9\u6cd5\u6b63\u4ea4\uff0c\u53ef\u8fdb\u4e00\u6b65\u7ec4\u5408\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2602.07590", "pdf": "https://arxiv.org/pdf/2602.07590", "abs": "https://arxiv.org/abs/2602.07590", "authors": ["Jessica Ka Yi Chiu", "Tom Frode Hansen", "Eivind Magnus Paulsen", "Ole Jakob Mengshoel"], "title": "Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "35 pages, 12 figures, 2 appendices", "summary": "This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5730\u8d28\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u56fe\u50cf\u5206\u5272\u5b9e\u73b0\u5ca9\u77f3\u8282\u7406\u8ff9\u7ebf\u81ea\u52a8\u6620\u5c04\uff0c\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5ca9\u77f3\u8282\u7406\u8ff9\u7ebf\u81ea\u52a8\u6620\u5c04\u4e2d\u771f\u5b9e\u6570\u636e\u6709\u9650\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u4ee5\u53ca\u5730\u8d28\u7279\u5f81\u4fdd\u6301\u7684\u6311\u6218\uff0c\u4e3a\u5730\u8d28\u5de5\u7a0b\u63d0\u4f9b\u53ef\u9760\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u7ed3\u5408\u5730\u8d28\u5efa\u6a21\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u76d1\u7763\u56fe\u50cf\u5206\u5272\uff1a1) \u4f7f\u7528\u79bb\u6563\u65ad\u88c2\u7f51\u7edc\u6a21\u578b\u751f\u6210\u5177\u6709\u5730\u8d28\u4ee3\u8868\u6027\u7684\u5408\u6210\u56fe\u50cf\uff1b2) \u91c7\u7528\u6df7\u5408\u8bad\u7ec3\u548c\u9884\u8bad\u7ec3\u540e\u5fae\u8c03\u7684\u7b56\u7565\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u3002", "result": "\u5408\u6210\u6570\u636e\u80fd\u6709\u6548\u652f\u6301\u76d1\u7763\u8282\u7406\u68c0\u6d4b\uff1b\u6df7\u5408\u8bad\u7ec3\u5728\u6807\u7b7e\u4e00\u81f4\u65f6\u8868\u73b0\u826f\u597d\uff0c\u5fae\u8c03\u5bf9\u566a\u58f0\u6807\u7b7e\u66f4\u7a33\u5065\uff1b\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u53ef\u5b9e\u73b0\u6709\u7528\u6cdb\u5316\uff1b\u5b9a\u6027\u5206\u6790\u663e\u793a\u6bd4\u5b9a\u91cf\u6307\u6807\u66f4\u6e05\u6670\u7684\u5730\u8d28\u610f\u4e49\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u9760\u8282\u7406\u6620\u5c04\u63d0\u4f9b\u57fa\u7840\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u9886\u57df\u9002\u5e94\u548c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5730\u8d28\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u5728\u5730\u8d28\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.07595", "pdf": "https://arxiv.org/pdf/2602.07595", "abs": "https://arxiv.org/abs/2602.07595", "authors": ["Yuanzhi Liang", "Xuan'er Wu", "Yirui Liu", "Yijie Fang", "Yizhen Fan", "Ke Hao", "Rui Li", "Ruiying Liu", "Ziqi Ni", "Peng Yu", "Yanbo Wang", "Haibin Huang", "Qizhen Weng", "Chi Zhang", "Xuelong Li"], "title": "TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u89c6\u9891\u751f\u6210\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u76d1\u7763\u7b56\u7565\u5851\u9020\u3001\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u504f\u597d\u7684\u7cbe\u70bc\u6574\u5408\u5230\u5355\u4e00\u7a33\u5b9a\u6027\u7ea6\u675f\u4f18\u5316\u5806\u6808\u4e2d\uff0c\u4ee5\u63d0\u5347\u611f\u77e5\u4fdd\u771f\u5ea6\u3001\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u63d0\u793a\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u540e\u8bad\u7ec3\u662f\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u8f6c\u5316\u4e3a\u751f\u4ea7\u5bfc\u5411\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u9700\u8981\u89e3\u51b3\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u65f6\u95f4\u7d2f\u79ef\u5931\u8d25\u6a21\u5f0f\u4ee5\u53ca\u53cd\u9988\u5f02\u8d28\u3001\u4e0d\u786e\u5b9a\u4e14\u5f31\u533a\u5206\u6027\u7b49\u5b9e\u9645\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u5206\u9636\u6bb5\u3001\u8bca\u65ad\u9a71\u52a8\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u76d1\u7763\u7b56\u7565\u5851\u9020\u3001\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u504f\u597d\u7684\u7cbe\u70bc\u6574\u5408\u5230\u7a33\u5b9a\u6027\u7ea6\u675f\u4f18\u5316\u5806\u6808\u4e2d\uff0c\u56f4\u7ed5\u5b9e\u9645\u89c6\u9891\u751f\u6210\u7ea6\u675f\u8bbe\u8ba1\u6846\u67b6\u3002", "result": "\u6846\u67b6\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u84dd\u56fe\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u6269\u5c55\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5728\u4fdd\u6301\u521d\u59cb\u5316\u53ef\u63a7\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u611f\u77e5\u4fdd\u771f\u5ea6\u3001\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u63d0\u793a\u9075\u5faa\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6027\u540e\u8bad\u7ec3\u6846\u67b6\u4e3a\u6784\u5efa\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u4e14\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u6709\u6548\u7684\u89c6\u9891\u751f\u6210\u540e\u8bad\u7ec3\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07605", "pdf": "https://arxiv.org/pdf/2602.07605", "abs": "https://arxiv.org/abs/2602.07605", "authors": ["Hulingxiao He", "Zijun Geng", "Yuxin Peng"], "title": "Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "Published as a conference paper at ICLR 2026. The models are available at https://huggingface.co/collections/StevenHH2000/fine-r1", "summary": "Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of \"visual analysis, candidate sub-categories, comparison, and prediction\", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.", "AI": {"tldr": "Fine-R1\uff1a\u9488\u5bf9\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u7684MLLM\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u76d1\u7763\u5fae\u8c03\u548c\u4e09\u5143\u7ec4\u589e\u5f3a\u7b56\u7565\u4f18\u5316\uff0c\u4ec5\u97004-shot\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7c97\u7c92\u5ea6\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u5df2\u89c1\u5b50\u7c7b\u522b\uff0c\u5bf9\u65b0\u5b50\u7c7b\u522b\u6cdb\u5316\u80fd\u529b\u5dee", "method": "\u91c7\u7528R1\u98ce\u683c\u8bad\u7ec3\u6846\u67b6\uff1a1) \u601d\u7ef4\u94fe\u76d1\u7763\u5fae\u8c03\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff1b2) \u4e09\u5143\u7ec4\u589e\u5f3a\u7b56\u7565\u4f18\u5316\uff0c\u5305\u62ec\u7c7b\u5185\u589e\u5f3a\u548c\u7c7b\u95f4\u589e\u5f3a", "result": "\u4ec5\u75284-shot\u8bad\u7ec3\uff0cFine-R1\u5728\u8bc6\u522b\u5df2\u89c1\u548c\u672a\u89c1\u5b50\u7c7b\u522b\u4e0a\u90fd\u8d85\u8d8a\u4e86\u73b0\u6709\u901a\u7528MLLM\u3001\u63a8\u7406MLLM\u751a\u81f3\u5bf9\u6bd4\u5b66\u4e60CLIP\u6a21\u578b", "conclusion": "Fine-R1\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u9002\u7528\u4e8e\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\uff0c\u5176\u4e2d\u83b7\u53d6\u6240\u6709\u5b50\u7c7b\u522b\u7684\u4e13\u5bb6\u6807\u6ce8\u5f88\u56f0\u96be"}}
{"id": "2602.07608", "pdf": "https://arxiv.org/pdf/2602.07608", "abs": "https://arxiv.org/abs/2602.07608", "authors": ["Yixin Chen", "Ziyu Su", "Lingbin Meng", "Elshad Hasanov", "Wei Chen", "Anil Parwani", "M. Khalid Khan Niazi"], "title": "HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology", "categories": ["cs.CV"], "comment": null, "summary": "Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.", "AI": {"tldr": "\u63d0\u51faHistoMet\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u51b3\u7b56\u6d41\u7a0b\u4ece\u539f\u53d1\u80bf\u7624\u75c5\u7406\u56fe\u50cf\u9884\u6d4b\u8f6c\u79fb\u98ce\u9669\u548c\u8f6c\u79fb\u90e8\u4f4d\uff0c\u6574\u5408\u8bed\u8a00\u5b9a\u4e49\u6982\u5ff5\u63d0\u5347\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027", "motivation": "\u8f6c\u79fb\u662f\u764c\u75c7\u6b7b\u4ea1\u4e3b\u8981\u539f\u56e0\uff0c\u4f46\u76f4\u63a5\u4ece\u75c5\u7406\u56fe\u50cf\u9884\u6d4b\u8f6c\u79fb\u98ce\u9669\u548c\u8f6c\u79fb\u90e8\u4f4d\u4ecd\u662f\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5b64\u7acb\u5904\u7406\u8fd9\u4e24\u4e2a\u4efb\u52a1\uff0c\u672a\u5efa\u6a21\u4e34\u5e8a\u51b3\u7b56\u6d41\u7a0b", "method": "\u63d0\u51fa\u51b3\u7b56\u611f\u77e5\u3001\u6982\u5ff5\u5bf9\u9f50\u7684MIL\u6846\u67b6HistoMet\uff0c\u91c7\u7528\u4e24\u6a21\u5757\u9884\u6d4b\u6d41\u7a0b\uff1a\u5148\u8bc4\u4f30\u8f6c\u79fb\u98ce\u9669\uff0c\u518d\u5bf9\u9ad8\u98ce\u9669\u75c5\u4f8b\u9884\u6d4b\u8f6c\u79fb\u90e8\u4f4d\uff1b\u6574\u5408\u8bed\u8a00\u5b9a\u4e49\u548c\u6570\u636e\u81ea\u9002\u5e94\u8f6c\u79fb\u6982\u5ff5\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u75c5\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b", "result": "\u57286504\u540d\u60a3\u8005\u7684\u591a\u673a\u6784\u6cdb\u764c\u961f\u5217\u4e2d\uff0c\u572895%\u654f\u611f\u6027\u8bbe\u7f6e\u4e0b\u663e\u8457\u51cf\u5c11\u4e0b\u6e38\u5de5\u4f5c\u91cf\uff1b\u5bf9\u8f6c\u79fb\u75c5\u4f8b\u5b9e\u73b0\u5b8f\u89c2F1 74.6\u00b11.3\uff0c\u5b8f\u89c2\u4e00\u5bf9\u591aAUC 92.1", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u4e34\u5e8a\u51b3\u7b56\u7ed3\u6784\u80fd\u591f\u76f4\u63a5\u4ece\u539f\u53d1\u80bf\u7624\u75c5\u7406\u5b66\u5b9e\u73b0\u7a33\u5065\u53ef\u90e8\u7f72\u7684\u8f6c\u79fb\u8fdb\u5c55\u548c\u90e8\u4f4d\u503e\u5411\u6027\u9884\u540e\u9884\u6d4b"}}
{"id": "2602.07625", "pdf": "https://arxiv.org/pdf/2602.07625", "abs": "https://arxiv.org/abs/2602.07625", "authors": ["Binxiao Xu", "Junyu Feng", "Xiaopeng Lin", "Haodong Li", "Zhiyuan Feng", "Bohan Zeng", "Shaolin Lu", "Ming Lu", "Qi She", "Wentao Zhang"], "title": "AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.", "AI": {"tldr": "AD-MIR\u662f\u4e00\u4e2a\u7528\u4e8e\u5e7f\u544a\u89c6\u9891\u7406\u89e3\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u6784\u5efa\u548c\u7ed3\u6784\u5316\u63a8\u7406\u4ee3\u7406\uff0c\u5c06\u50cf\u7d20\u7ea7\u611f\u77e5\u4e0e\u9ad8\u5c42\u6b21\u8425\u9500\u903b\u8f91\u8fde\u63a5\u8d77\u6765\uff0c\u5728AdsQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u5728\u5e7f\u544a\u89c6\u9891\u7406\u89e3\u65b9\u9762\u5b58\u5728\u8ba4\u77e5\u9e3f\u6c9f\uff0c\u96be\u4ee5\u5c06\u50cf\u7d20\u7ea7\u611f\u77e5\u4e0e\u9ad8\u5c42\u6b21\u8425\u9500\u7b56\u7565\u8fde\u63a5\u8d77\u6765\uff0c\u9700\u8981\u4e13\u95e8\u6846\u67b6\u6765\u89e3\u7801\u5e7f\u544a\u610f\u56fe\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a1) \u7ed3\u6784\u611f\u77e5\u8bb0\u5fc6\u6784\u5efa\u9636\u6bb5\uff0c\u5c06\u539f\u59cb\u89c6\u9891\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u5e93\uff0c\u6574\u5408\u8bed\u4e49\u68c0\u7d22\u548c\u7cbe\u786e\u5173\u952e\u8bcd\u5339\u914d\uff0c\u4f18\u5148\u5904\u7406\u7ec6\u7c92\u5ea6\u54c1\u724c\u7ec6\u8282\uff1b2) \u7ed3\u6784\u5316\u63a8\u7406\u4ee3\u7406\uff0c\u6a21\u62df\u8425\u9500\u4e13\u5bb6\u901a\u8fc7\u8fed\u4ee3\u8be2\u95ee\u5faa\u73af\u5206\u89e3\u53d9\u4e8b\uff0c\u63a8\u65ad\u9690\u542b\u8bf4\u670d\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u8bc1\u636e\u7684\u81ea\u6211\u7ea0\u6b63\u673a\u5236\u9a8c\u8bc1\u6d1e\u5bdf\u3002", "result": "\u5728AdsQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAD-MIR\u5728\u4e25\u683c\u51c6\u786e\u7387\u4e0a\u6bd4\u6700\u5f3a\u901a\u7528\u667a\u80fd\u4f53DVD\u63d0\u53471.8%\uff0c\u5728\u5bbd\u677e\u51c6\u786e\u7387\u4e0a\u63d0\u53479.5%\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u6709\u6548\u7684\u5e7f\u544a\u7406\u89e3\u9700\u8981\u5c06\u62bd\u8c61\u8425\u9500\u7b56\u7565\u660e\u786e\u5730\u57fa\u4e8e\u50cf\u7d20\u7ea7\u8bc1\u636e\uff0cAD-MIR\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u548c\u63a8\u7406\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002"}}
{"id": "2602.07643", "pdf": "https://arxiv.org/pdf/2602.07643", "abs": "https://arxiv.org/abs/2602.07643", "authors": ["Yichi Zhang", "Feiyang Xiao", "Le Xue", "Wenbo Zhang", "Gang Feng", "Chenguang Zheng", "Yuan Qi", "Yuan Cheng", "Zixin Hu"], "title": "Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\\sim$675k 2D images, $\\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u5efaUMD\u6570\u636e\u96c6\uff08\u5305\u542b490\u4e2aPET/CT\u548c464\u4e2aPET/MRI\u5168\u8eab\u626b\u63cf\uff09\u8bc4\u4f303D\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u53d1\u73b0\u4ece\u7ed3\u6784\u6210\u50cf\u8f6c\u5411\u529f\u80fd\u6210\u50cf\u65f6\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u8fdc\u672a\u8fbe\u5230\u771f\u6b63\u901a\u7528\u72b6\u6001\u3002", "motivation": "\u5f53\u524d3D\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u9a8c\u8bc1\u4e3b\u8981\u5c40\u9650\u4e8e\u533a\u57df\u6027\u548c\u7ed3\u6784\u6027\u6210\u50cf\uff0c\u5b58\u5728\u663e\u8457\u7684\u6a21\u6001\u5dee\u5f02\u672a\u88ab\u63a2\u7d22\u3002\u9700\u8981\u63d0\u4f9b\u4e25\u8c28\u5ba2\u89c2\u7684\u8bc4\u4f30\uff0c\u4ee5\u4e86\u89e3\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u521b\u5efaUMD\u6570\u636e\u96c6\uff08\u5305\u542b\u7ea6675k 2D\u56fe\u50cf\u548c12k 3D\u5668\u5b98\u6807\u6ce8\uff09\uff0c\u901a\u8fc7\u53d7\u8bd5\u8005\u5185\u914d\u5bf9\u626b\u63cf\u7684\u5bf9\u7167\u6bd4\u8f83\uff0c\u5c06\u6210\u50cf\u6a21\u6001\u4f5c\u4e3a\u4e3b\u8981\u81ea\u53d8\u91cf\uff0c\u5168\u9762\u8bc4\u4f30\u4ee3\u8868\u60273D\u5206\u5272\u57fa\u7840\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u6587\u732e\u62a5\u9053\u7684\u57fa\u51c6\u4e0e\u771f\u5b9e\u4e16\u754c\u6548\u80fd\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u4ece\u7ed3\u6784\u57df\u8f6c\u5411\u529f\u80fd\u57df\u65f6\u3002\u8fd9\u79cd\u7cfb\u7edf\u6027\u5931\u8d25\u8868\u660e\u5f53\u524d3D\u57fa\u7840\u6a21\u578b\u8fdc\u672a\u8fbe\u5230\u771f\u6b63\u901a\u7528\u72b6\u6001\u3002", "conclusion": "\u9700\u8981\u5411\u591a\u6a21\u6001\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ee5\u5f25\u5408\u7406\u60f3\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5168\u9762\u4e34\u5e8a\u6548\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8be5\u6570\u636e\u96c6\u548c\u5206\u6790\u4e3a\u5f00\u53d1\u771f\u6b63\u6a21\u6001\u65e0\u5173\u7684\u533b\u5b66\u57fa\u7840\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u77f3\u3002"}}
{"id": "2602.07645", "pdf": "https://arxiv.org/pdf/2602.07645", "abs": "https://arxiv.org/abs/2602.07645", "authors": ["Leonardo Gonzalez"], "title": "From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication in the Companion Proceedings of the ACM Web Conference 2026 (WWW Companion '26), April 13-17, 2026, Dubai, United Arab Emirates", "summary": "Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \\textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \\textsc{Images2Slides} achieves an overall element recovery rate of $0.989\\pm0.057$ (text: $0.985\\pm0.083$, images: $1.000\\pm0.000$), with mean text transcription error $\\mathrm{CER}=0.033\\pm0.149$ and mean layout fidelity $\\mathrm{IoU}=0.364\\pm0.161$ for text regions and $0.644\\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.", "AI": {"tldr": "Images2Slides\uff1a\u57fa\u4e8eAPI\u7684\u7ba1\u9053\uff0c\u5c06\u9759\u6001\u4fe1\u606f\u56fe\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u7684Google Slides\u5e7b\u706f\u7247\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u533a\u57df\u7ea7\u89c4\u8303\u5e76\u91cd\u5efa\u5143\u7d20", "motivation": "\u4fe1\u606f\u56fe\u901a\u5e38\u5bfc\u51fa\u4e3a\u56fe\u50cf\u540e\u5185\u5bb9\u88ab\u9501\u5b9a\u5728\u50cf\u7d20\u4e2d\uff0c\u5bfc\u81f4\u66f4\u65b0\u3001\u672c\u5730\u5316\u548c\u91cd\u7528\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u9759\u6001\u4fe1\u606f\u56fe\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u683c\u5f0f", "method": "\u4f7f\u7528\u57fa\u4e8eAPI\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u533a\u57df\u7ea7\u89c4\u8303\uff0c\u5c06\u50cf\u7d20\u51e0\u4f55\u6620\u5c04\u5230\u5e7b\u706f\u7247\u5750\u6807\uff0c\u5e76\u4f7f\u7528Google Slides\u6279\u91cf\u66f4\u65b0API\u91cd\u65b0\u521b\u5efa\u5143\u7d20", "result": "\u572829\u4e2a\u7a0b\u5e8f\u751f\u6210\u7684\u4fe1\u606f\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5143\u7d20\u6062\u590d\u7387\u8fbe\u52300.989\u00b10.057\uff0c\u6587\u672c\u8f6c\u5f55\u9519\u8befCER=0.033\u00b10.149\uff0c\u6587\u672c\u533a\u57df\u5e03\u5c40\u4fdd\u771f\u5ea6IoU=0.364\u00b10.161\uff0c\u56fe\u50cf\u533a\u57df\u4e3a0.644\u00b10.131", "conclusion": "Images2Slides\u6210\u529f\u5c06\u9759\u6001\u4fe1\u606f\u56fe\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u5e7b\u706f\u7247\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5927\u5c0f\u6821\u51c6\u548c\u975e\u5747\u5300\u80cc\u666f\u7b49\u5de5\u7a0b\u6311\u6218\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u6307\u5bfc"}}
{"id": "2602.07658", "pdf": "https://arxiv.org/pdf/2602.07658", "abs": "https://arxiv.org/abs/2602.07658", "authors": ["Avinash Kumar K M", "Samarth S. Raut"], "title": "Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation", "categories": ["cs.CV"], "comment": "22 pages, 13 figures", "summary": "The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u533b\u5b66\u5f71\u50cf3D\u91cd\u5efa\u6d41\u7a0b\u4e2d\u7684\u8bef\u5dee\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u5206\u5272\u7b97\u6cd5\u548c\u51e0\u4f55\u7c7b\u578b\u7684\u4f53\u7d20\u4e0e\u8868\u9762\u7cbe\u5ea6\u6307\u6807\uff0c\u53d1\u73b0Otsu\u65b9\u6cd5\u6700\u9002\u7528\u4e8e\u5404\u79cd\u51e0\u4f55\u5f62\u72b6\uff0cJaccard\u6307\u6570\u6bd4Dice\u66f4\u9002\u5408\u8584\u58c1\u7ed3\u6784\u8bc4\u4f30\u3002", "motivation": "\u533b\u5b66\u626b\u63cf\u521b\u5efa3D\u6a21\u578b\u7684\u7cbe\u5ea6\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u4f46\u51e0\u4f55\u7c7b\u578b\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u4f53\u7d20\u548c\u70b9\u4e91\u5bf9\u9f50\u5bf9\u7cbe\u5ea6\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u5168\u9762\u8bc4\u4f30\u91cd\u5efa\u6d41\u7a0b\u4e2d\u7684\u8bef\u5dee\uff0c\u5e76\u63a2\u7d22\u4e0d\u540c\u5206\u5272\u7b97\u6cd5\u548c\u51e0\u4f55\u7c7b\u578b\u7684\u4f53\u7d20\u4e0e\u8868\u9762\u7cbe\u5ea6\u6307\u6807\u3002", "method": "\u4f7f\u7528SLA\u6280\u672f\u6253\u5370\u7403\u4f53\u3001\u9762\u7f69\u548cAAA\u6a21\u578b\uff0c\u901a\u8fc7\u5faeCT\u626b\u63cf\u3002\u91c7\u7528GMM\u3001Otsu\u548cRG\u65b9\u6cd5\u8fdb\u884c\u5206\u5272\u3002\u4f7f\u7528KU\u7b97\u6cd5\u5bf9\u9f50\u5206\u5272\u6a21\u578b\u548c\u53c2\u8003\u6a21\u578b\uff0c\u8bc4\u4f30Dice\u3001Jaccard\u5206\u6570\u548c\u7cbe\u5ea6\u7b49\u6307\u6807\u3002\u901a\u8fc7ICP\u5bf9\u9f50\u8fc7\u7a0b\u914d\u51c6\u8868\u9762\u7f51\u683c\uff0c\u8bc4\u4f30Chamfer\u8ddd\u79bb\u548c\u5e73\u5747Hausdorff\u8ddd\u79bb\u3002", "result": "Otsu\u65b9\u6cd5\u5bf9\u6240\u6709\u51e0\u4f55\u5f62\u72b6\u6700\u5408\u9002\u3002AAA\u7531\u4e8e\u58c1\u8584\u548c\u5bf9\u9f50\u95ee\u9898\u5bfc\u81f4\u91cd\u53e0\u5206\u6570\u4f4e\u3002\u7c7b\u522b\u4e0d\u5e73\u8861\u5bf9AAA\u7684\u7279\u5f02\u6027\u5f71\u54cd\u6700\u5927\u3002\u8868\u9762\u7cbe\u5ea6\u6307\u6807\u4e0e\u4f53\u7d20\u6307\u6807\u8d8b\u52bf\u4e0d\u540c\u3002RG\u65b9\u6cd5\u5bf9\u7403\u4f53\u8868\u73b0\u6700\u597d\uff0cGMM\u548cOtsu\u5bf9AAA\u66f4\u597d\u3002\u9762\u7f69\u8868\u9762\u8bef\u5dee\u6700\u5927\uff0c\u53ef\u80fd\u7531\u4e8eICP\u5bf9\u9f50\u95ee\u9898\u3002", "conclusion": "\u5206\u5272\u7cbe\u5ea6\u662f\u91cd\u5efa\u8fc7\u7a0b\u5404\u9636\u6bb5\u8bef\u5dee\u7684\u7d2f\u79ef\u603b\u548c\u3002\u9ad8\u4f53\u7d20\u7cbe\u5ea6\u6307\u6807\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u5bf9\u9f50\u654f\u611f\u60c5\u51b5\u4e0b\u53ef\u80fd\u8bef\u5bfc\u3002Jaccard\u6307\u6570\u6bd4Dice\u66f4\u4e25\u683c\uff0c\u66f4\u9002\u5408\u8584\u58c1\u7ed3\u6784\u7cbe\u5ea6\u8bc4\u4f30\u3002\u4f53\u7d20\u548c\u70b9\u4e91\u5bf9\u9f50\u5fc5\u987b\u786e\u4fdd\u624d\u80fd\u53ef\u9760\u8bc4\u4f30\u91cd\u5efa\u6d41\u7a0b\u3002"}}
{"id": "2602.07668", "pdf": "https://arxiv.org/pdf/2602.07668", "abs": "https://arxiv.org/abs/2602.07668", "authors": ["Ross Greer", "Laura Fleig", "Maitrayee Keskar", "Erika Maquiling", "Giovanni Tapia Lopez", "Angel Martinez-Sanchez", "Parthib Roy", "Jake Rattigan", "Mira Sur", "Alejandra Vidrio", "Thomas Marcotte", "Mohan Trivedi"], "title": "Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., \"turn after that red building\") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.", "AI": {"tldr": "\u63d0\u51faL-LIO\u6846\u67b6\uff0c\u5728\u73b0\u6709LILO\u89c6\u89c9\u6846\u67b6\u57fa\u7840\u4e0a\u52a0\u5165\u97f3\u9891\u6a21\u6001\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u589e\u5f3a\u8f66\u8f86\u5bf9\u9a7e\u9a76\u5458\u72b6\u6001\u548c\u5916\u90e8\u73af\u5883\u7684\u7406\u89e3\uff0c\u63d0\u5347\u8f66\u8f86\u5b89\u5168\u5e94\u7528\u3002", "motivation": "\u73b0\u6709LILO\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u4fe1\u606f\u7406\u89e3\u8f66\u5185\u9a7e\u9a76\u5458\u72b6\u6001\u548c\u8f66\u5916\u573a\u666f\uff0c\u4f46\u97f3\u9891\u6a21\u6001\uff08\u5982\u9a7e\u9a76\u5458\u8bed\u97f3\u3001\u4e58\u5ba2\u6307\u4ee4\u3001\u5916\u90e8\u58f0\u97f3\uff09\u80fd\u63d0\u4f9b\u91cd\u8981\u8865\u5145\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4fe1\u53f7\u4e0d\u8db3\u6216\u9700\u8981\u8bed\u5883\u7406\u89e3\u7684\u590d\u6742\u573a\u666f\u4e2d\u3002", "method": "\u6269\u5c55LILO\u4e3aL-LIO\u6846\u67b6\uff0c\u6574\u5408\u97f3\u9891\u4fe1\u53f7\u8fdb\u884c\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u3002\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u8bc4\u4f30\u97f3\u9891\u589e\u5f3a\uff1a1) \u9a7e\u9a76\u5458\u8bed\u97f3\u5206\u7c7b\u6f5c\u5728\u635f\u4f24\u72b6\u6001\uff1b2) \u4e58\u5ba2\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5206\u6790\uff1b3) \u97f3\u9891\u8f85\u52a9\u89c6\u89c9\u7cfb\u7edf\u89e3\u6790\u5916\u90e8\u4ee3\u7406\u7684\u6307\u5bfc\u548c\u624b\u52bf\u3002", "result": "\u521d\u6b65\u7814\u7a76\u663e\u793a\u97f3\u9891\u5728\u5b89\u5168\u76f8\u5173\u573a\u666f\u4e2d\u63d0\u4f9b\u91cd\u8981\u6d1e\u5bdf\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7ec6\u5fae\u8bed\u5883\u7406\u89e3\u6216\u89c6\u89c9\u4fe1\u53f7\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u3002\u6536\u96c6\u4e86\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8f66\u5185\u548c\u5916\u90e8\u97f3\u9891\u6570\u636e\u96c6\u3002", "conclusion": "L-LIO\u6846\u67b6\u901a\u8fc7\u97f3\u9891\u548c\u89c6\u89c9\u7684\u591a\u6a21\u6001\u878d\u5408\u589e\u5f3a\u4e86\u9a7e\u9a76\u5458\u548c\u573a\u666f\u7406\u89e3\uff0c\u4e3a\u5b89\u5168\u5e72\u9884\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u9762\u4e34\u7684\u6311\u6218\u5305\u62ec\u73af\u5883\u566a\u58f0\u5e72\u6270\u3001\u9690\u79c1\u95ee\u9898\u548c\u8de8\u4e3b\u4f53\u9c81\u68d2\u6027\uff0c\u9700\u8981\u5728\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u8fdb\u4e00\u6b65\u7814\u7a76\u53ef\u9760\u6027\u3002"}}
{"id": "2602.07680", "pdf": "https://arxiv.org/pdf/2602.07680", "abs": "https://arxiv.org/abs/2602.07680", "authors": ["Ross Greer", "Maitrayee Keskar", "Angel Martinez-Sanchez", "Parthib Roy", "Shashank Shriram", "Mohan Trivedi"], "title": "Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u8bc4\u4f30\u548c\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u7d22\u4e86\u4e09\u79cd\u7cfb\u7edf\u7ea7\u7528\u4f8b\uff1a\u57fa\u4e8eCLIP\u7684\u8bed\u4e49\u5371\u9669\u7b5b\u67e5\u3001\u573a\u666f\u7ea7\u5d4c\u5165\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u96c6\u6210\uff0c\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u884c\u4e3a\u7ea6\u675f\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5c06\u89c6\u89c9\u89c2\u5bdf\u4e0e\u81ea\u7136\u8bed\u8a00\u6982\u5ff5\u5bf9\u9f50\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8bed\u4e49\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u5982\u4f55\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u6765\u652f\u6301\u9a7e\u9a76\u573a\u666f\u5b89\u5168\u8bc4\u4f30\u548c\u51b3\u7b56\u5236\u5b9a\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u4e92\u8865\u7684\u7cfb\u7edf\u7ea7\u7528\u4f8b\uff1a1\uff09\u57fa\u4e8eCLIP\u56fe\u50cf-\u6587\u672c\u76f8\u4f3c\u6027\u7684\u8f7b\u91cf\u7ea7\u3001\u7c7b\u522b\u65e0\u5173\u7684\u5371\u9669\u7b5b\u67e5\u65b9\u6cd5\uff1b2\uff09\u5c06\u573a\u666f\u7ea7\u89c6\u89c9\u8bed\u8a00\u5d4c\u5165\u96c6\u6210\u5230\u57fa\u4e8eTransformer\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\u4e2d\uff1b3\uff09\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u8fd0\u52a8\u89c4\u5212\u7684\u663e\u5f0f\u884c\u4e3a\u7ea6\u675f\u3002", "result": "1\uff09\u5371\u9669\u7b5b\u67e5\u65b9\u6cd5\u80fd\u591f\u7a33\u5065\u68c0\u6d4b\u591a\u6837\u5316\u548c\u5206\u5e03\u5916\u7684\u9053\u8def\u5371\u9669\uff1b2\uff09\u76f4\u63a5\u5c06\u5168\u5c40\u5d4c\u5165\u6761\u4ef6\u5316\u5230\u89c4\u5212\u5668\u4e2d\u4e0d\u4f1a\u63d0\u9ad8\u8f68\u8ff9\u7cbe\u5ea6\uff1b3\uff09\u57fa\u4e8e\u89c6\u89c9\u573a\u666f\u5143\u7d20\u7684\u4e58\u5ba2\u5f0f\u6307\u4ee4\u80fd\u591f\u6291\u5236\u7f55\u89c1\u4f46\u4e25\u91cd\u7684\u89c4\u5212\u5931\u8d25\uff0c\u5728\u6a21\u7cca\u573a\u666f\u4e2d\u6539\u5584\u5b89\u5168\u5bf9\u9f50\u884c\u4e3a\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u5728\u8868\u8fbe\u8bed\u4e49\u98ce\u9669\u3001\u610f\u56fe\u548c\u884c\u4e3a\u7ea6\u675f\u65b9\u9762\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4f46\u5b9e\u73b0\u8fd9\u4e00\u6f5c\u529b\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5de5\u7a0b\u95ee\u9898\uff0c\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7cfb\u7edf\u67b6\u6784\u548c\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u7279\u5f81\u6ce8\u5165\u3002"}}
{"id": "2602.07689", "pdf": "https://arxiv.org/pdf/2602.07689", "abs": "https://arxiv.org/abs/2602.07689", "authors": ["Jusheng Zhang", "Kaitong Cai", "Jian Wang", "Yongsen Zheng", "Kwok-Yan Lam", "Keze Wang"], "title": "Process-of-Thought Reasoning for Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.", "AI": {"tldr": "PoT Reasoning for Videos\uff1a\u901a\u8fc7\u5c06\u89c6\u9891\u63a8\u7406\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u6b65\u9aa4\u5e8f\u5217\uff0c\u4f7f\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u5316\uff0c\u63d0\u9ad8\u4e8b\u5b9e\u6b63\u786e\u6027\u548c\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u89c6\u9891\u7406\u89e3\u4e0d\u4ec5\u9700\u8981\u8bc6\u522b\u89c6\u89c9\u5185\u5bb9\uff0c\u8fd8\u9700\u8981\u5728\u957f\u4e14\u5608\u6742\u7684\u89c2\u5bdf\u4e2d\u8fdb\u884c\u65f6\u95f4\u5b9a\u4f4d\u7684\u591a\u6b65\u63a8\u7406\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u7f3a\u4e4f\u663e\u5f0f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u63a8\u7406\u4e0d\u900f\u660e\u4e14\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u89e3\u91ca\u3002", "method": "\u63d0\u51faProcess-of-Thought (PoT) Reasoning\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u63a8\u7406\u7ed3\u6784\u5316\u4e3a\u4e09\u4e2a\u4ea4\u9519\u7684\u8f7b\u91cf\u7ea7\u53ef\u9a8c\u8bc1\u6b65\u9aa4\uff1a(1)\u65f6\u95f4\u8bc1\u636e\u9009\u62e9\uff0c(2)\u9010\u6b65\u72b6\u6001\u66f4\u65b0\uff0c(3)\u7ea6\u675f\u7b54\u6848\u5408\u6210\u3002\u8be5\u6846\u67b6\u4e0e\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u63d2\u5165\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\uff0c\u652f\u6301\u95ed\u5377\u63a8\u7406\u548c\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u3002", "result": "\u5728\u6807\u51c6\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPoT\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u6b63\u786e\u6027\u548c\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u540c\u65f6\u4e3a\u8bca\u65ad\u548c\u4e0b\u6e38\u4f7f\u7528\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "PoT\u6846\u67b6\u901a\u8fc7\u4f7f\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u63a8\u7406\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u51cf\u5c11\u5e7b\u89c9\u89e3\u91ca\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ed3\u6784\u5316\u63a8\u7406\u65b9\u6cd5\u3002"}}
{"id": "2602.07694", "pdf": "https://arxiv.org/pdf/2602.07694", "abs": "https://arxiv.org/abs/2602.07694", "authors": ["Wenping Jin", "Yuyang Tang", "Li Zhu"], "title": "Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Reliable foreign-object anomaly detection and pixel-level localization in conveyor-belt coal scenes are essential for safe and intelligent mining operations. This task is particularly challenging due to the highly unstructured environment: coal and gangue are randomly piled, backgrounds are complex and variable, and foreign objects often exhibit low contrast, deformation, occlusion, resulting in coupling with their surroundings. These characteristics weaken the stability and regularity assumptions that many anomaly detection methods rely on in structured industrial settings, leading to notable performance degradation. To support evaluation and comparison in this setting, we construct \\textbf{CoalAD}, a benchmark for unsupervised foreign-object anomaly detection with pixel-level localization in coal-stream scenes. We further propose a complementary-cue collaborative perception framework that extracts and fuses complementary anomaly evidence from three perspectives: object-level semantic composition modeling, semantic-attribution-based global deviation analysis, and fine-grained texture matching. The fused outputs provide robust image-level anomaly scoring and accurate pixel-level localization. Experiments on CoalAD demonstrate that our method outperforms widely used baselines across the evaluated image-level and pixel-level metrics, and ablation studies validate the contribution of each component. The code is available at https://github.com/xjpp2016/USAD.", "AI": {"tldr": "\u63d0\u51faCoalAD\u57fa\u51c6\u548c\u4e92\u8865\u7ebf\u7d22\u534f\u540c\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u7164\u77ff\u4f20\u9001\u5e26\u573a\u666f\u4e2d\u7684\u65e0\u76d1\u7763\u5f02\u7269\u5f02\u5e38\u68c0\u6d4b\u4e0e\u50cf\u7d20\u7ea7\u5b9a\u4f4d", "motivation": "\u7164\u77ff\u4f20\u9001\u5e26\u573a\u666f\u4e2d\u7684\u5f02\u7269\u5f02\u5e38\u68c0\u6d4b\u5bf9\u5b89\u5168\u751f\u4ea7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u73af\u5883\u9ad8\u5ea6\u975e\u7ed3\u6784\u5316\uff08\u7164\u77f8\u77f3\u968f\u673a\u5806\u79ef\u3001\u80cc\u666f\u590d\u6742\u591a\u53d8\u3001\u5f02\u7269\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u53d8\u5f62\u906e\u6321\u7b49\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7684\u7a33\u5b9a\u6027\u5047\u8bbe\u5931\u6548\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d", "method": "\u63d0\u51fa\u4e92\u8865\u7ebf\u7d22\u534f\u540c\u611f\u77e5\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u89d2\u5ea6\u63d0\u53d6\u548c\u878d\u5408\u5f02\u5e38\u8bc1\u636e\uff1a1\uff09\u7269\u4f53\u7ea7\u8bed\u4e49\u7ec4\u5408\u5efa\u6a21\uff1b2\uff09\u57fa\u4e8e\u8bed\u4e49\u5c5e\u6027\u7684\u5168\u5c40\u504f\u5dee\u5206\u6790\uff1b3\uff09\u7ec6\u7c92\u5ea6\u7eb9\u7406\u5339\u914d", "result": "\u5728CoalAD\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u8d21\u732e", "conclusion": "\u6784\u5efa\u4e86\u7164\u77ff\u573a\u666f\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6CoalAD\uff0c\u63d0\u51fa\u7684\u4e92\u8865\u7ebf\u7d22\u534f\u540c\u611f\u77e5\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9\u975e\u7ed3\u6784\u5316\u73af\u5883\u6311\u6218\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u7cbe\u786e\u5b9a\u4f4d"}}
{"id": "2602.07702", "pdf": "https://arxiv.org/pdf/2602.07702", "abs": "https://arxiv.org/abs/2602.07702", "authors": ["Deep Bhattacharyya", "Ali Ayub", "A. Ben Hamza"], "title": "A hybrid Kolmogorov-Arnold network for medical image segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.", "AI": {"tldr": "U-KABS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408Kolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u548cU\u578b\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684Bernstein\u591a\u9879\u5f0f\u548cB\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u6765\u6355\u6349\u590d\u6742\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u533b\u5b66\u56fe\u50cf\u7684\u56fa\u6709\u590d\u6742\u6027\u548c\u53d8\u5f02\u6027\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u6570\u636e\u4e2d\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u65b9\u9762\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6355\u6349\u8fd9\u4e9b\u590d\u6742\u5173\u7cfb\u4ee5\u51c6\u786e\u5206\u5272\u89e3\u5256\u7ed3\u6784\u3002", "method": "\u63d0\u51faU-KABS\u6df7\u5408\u6846\u67b6\uff0c\u6574\u5408KANs\u7684\u8868\u8fbe\u80fd\u529b\u548cU\u578b\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3002\u6a21\u578b\u5305\u542b\u5377\u79ef\u548c\u6324\u538b-\u6fc0\u52b1\u9636\u6bb5\uff08\u589e\u5f3a\u901a\u9053\u7279\u5f81\u8868\u793a\uff09\u4ee5\u53caKAN Bernstein Spline\uff08KABS\uff09\u9636\u6bb5\uff08\u4f7f\u7528\u57fa\u4e8eBernstein\u591a\u9879\u5f0f\u548cB\u6837\u6761\u7684\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\uff09\u3002\u8fd9\u79cd\u6df7\u5408\u8bbe\u8ba1\u5229\u7528\u4e86Bernstein\u591a\u9879\u5f0f\u7684\u5168\u5c40\u5e73\u6ed1\u6027\u548cB\u6837\u6761\u7684\u5c40\u90e8\u9002\u5e94\u6027\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u8d8b\u52bf\u548c\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u3002\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5c42\u4e4b\u95f4\u7684\u8df3\u8dc3\u8fde\u63a5\u652f\u6301\u6709\u6548\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u5e76\u4fdd\u7559\u7a7a\u95f4\u7ec6\u8282\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u6210\u50cf\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cU-KABS\u8868\u73b0\u51fa\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5206\u5272\u590d\u6742\u89e3\u5256\u7ed3\u6784\u65b9\u9762\u3002", "conclusion": "U-KABS\u901a\u8fc7\u6574\u5408KANs\u548cU\u578b\u67b6\u6784\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6355\u6349\u590d\u6742\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u5728\u5206\u5272\u590d\u6742\u89e3\u5256\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.07717", "pdf": "https://arxiv.org/pdf/2602.07717", "abs": "https://arxiv.org/abs/2602.07717", "authors": ["Yingjie Li", "Daniel Robinson", "Cunxi Yu"], "title": "All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving", "categories": ["cs.CV", "cs.ET"], "comment": null, "summary": "Semantic segmentation and lane detection are crucial tasks in autonomous driving systems. Conventional approaches predominantly rely on deep neural networks (DNNs), which incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations required for low-latency, real-time responses. Diffractive optical neural networks (DONNs) have shown promising advantages over conventional DNNs on digital or optoelectronic computing platforms in energy efficiency. By performing all-optical image processing via light diffraction at the speed of light, DONNs save computation energy costs while reducing the overhead associated with analog-to-digital conversions by all-optical encoding and computing. In this work, we propose a novel all-optical computing framework for RGB image segmentation and lane detection in autonomous driving applications. Our experimental results demonstrate the effectiveness of the DONN system for image segmentation on the CityScapes dataset. Additionally, we conduct case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA, where we further evaluate the model's generalizability under diverse environmental conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884d\u5c04\u5149\u5b66\u795e\u7ecf\u7f51\u7edc\u7684\u5168\u5149\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684RGB\u56fe\u50cf\u5206\u5272\u548c\u8f66\u9053\u7ebf\u68c0\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edfDNN\u5177\u6709\u66f4\u9ad8\u7684\u80fd\u6548\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u81ea\u52a8\u9a7e\u9a76\u7684\u8bed\u4e49\u5206\u5272\u548c\u8f66\u9053\u68c0\u6d4b\u4efb\u52a1\u4e2d\u80fd\u8017\u9ad8\uff0c\u4e3b\u8981\u56e0\u4e3a\u5927\u91cf\u7684\u6a21\u6570\u8f6c\u6362\u548c\u5927\u89c4\u6a21\u56fe\u50cf\u8ba1\u7b97\u3002\u9700\u8981\u66f4\u8282\u80fd\u3001\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u884d\u5c04\u5149\u5b66\u795e\u7ecf\u7f51\u7edc\uff08DONNs\uff09\u6784\u5efa\u5168\u5149\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u5149\u884d\u5c04\u8fdb\u884c\u5168\u5149\u56fe\u50cf\u5904\u7406\uff0c\u907f\u514d\u4e86\u4f20\u7edfDNN\u7684\u6a21\u6570\u8f6c\u6362\u5f00\u9500\uff0c\u5728\u5149\u901f\u4e0b\u5b8c\u6210\u8ba1\u7b97\u3002", "result": "\u5728CityScapes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u56fe\u50cf\u5206\u5272\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5b9a\u5236\u5ba4\u5185\u8f68\u9053\u6570\u636e\u96c6\u548cCARLA\u6a21\u62df\u9a7e\u9a76\u573a\u666f\u4e2d\u8fdb\u884c\u8f66\u9053\u68c0\u6d4b\u6848\u4f8b\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DONN\u7cfb\u7edf\u4e3a\u81ea\u52a8\u9a7e\u9a76\u56fe\u50cf\u5206\u5272\u548c\u8f66\u9053\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u8282\u80fd\u3001\u4f4e\u5ef6\u8fdf\u7684\u5168\u5149\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u80fd\u6548\u548c\u5b9e\u65f6\u54cd\u5e94\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6570\u5b57\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2602.07768", "pdf": "https://arxiv.org/pdf/2602.07768", "abs": "https://arxiv.org/abs/2602.07768", "authors": ["Qiuming Luo", "Yuebing Li", "Feng Li", "Chang Kong"], "title": "PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "6pages, 3 figures, conference", "summary": "Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two-stage framework that decouples semantic calibration from structural transfer. First, we incorporate Prompt-Aware Semantic Calibration to generate adaptive semantic anchors. Second, we introduce a neighborhood-aware structural distillation strategy to constrain the student's local decision structure. PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks. Notably, our ResNet-18 student achieves 76.09% accuracy on CUB-200, surpassing the strong baseline VL2Lite by 3.4%. Code is available at https://github.com/LLLVTA/PAND.", "AI": {"tldr": "PAND\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u611f\u77e5\u8bed\u4e49\u6821\u51c6\u548c\u90bb\u57df\u611f\u77e5\u7ed3\u6784\u84b8\u998f\uff0c\u5c06\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u7f51\u7edc\uff0c\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u4e2d\uff0c\u4ece\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u77e5\u8bc6\u5230\u8f7b\u91cf\u7ea7\u7f51\u7edc\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u56fa\u5b9a\u63d0\u793a\u548c\u5168\u5c40\u5bf9\u9f50\u7684\u9650\u5236\u3002\u9700\u8981\u89e3\u51b3\u8bed\u4e49\u6821\u51c6\u548c\u7ed3\u6784\u8f6c\u79fb\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPAND\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u63d0\u793a\u611f\u77e5\u8bed\u4e49\u6821\u51c6\u751f\u6210\u81ea\u9002\u5e94\u8bed\u4e49\u951a\u70b9\uff1b2) \u90bb\u57df\u611f\u77e5\u7ed3\u6784\u84b8\u998f\u7b56\u7565\u7ea6\u675f\u5b66\u751f\u7f51\u7edc\u7684\u5c40\u90e8\u51b3\u7b56\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2a\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002ResNet-18\u5b66\u751f\u5728CUB-200\u4e0a\u8fbe\u523076.09%\u51c6\u786e\u7387\uff0c\u6bd4VL2Lite\u57fa\u7ebf\u63d0\u53473.4%\u3002", "conclusion": "PAND\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u6821\u51c6\u548c\u7ed3\u6784\u8f6c\u79fb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u4e2d\u77e5\u8bc6\u84b8\u998f\u7684\u6311\u6218\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u7f51\u7edc\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.07775", "pdf": "https://arxiv.org/pdf/2602.07775", "abs": "https://arxiv.org/abs/2602.07775", "authors": ["Haodong Li", "Shaoteng Liu", "Zhe Lin", "Manmohan Chandraker"], "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion", "categories": ["cs.CV"], "comment": "Figure PDFs were compressed to 150 dpi to comply with arXiv's submission size limit. Project page: https://rolling-sink.github.io/", "summary": "Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/", "AI": {"tldr": "\u63d0\u51faRolling Sink\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u8d85\u51fa\u8bad\u7ec3\u65f6\u957f\u65f6\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u8d85\u957f\u89c6\u9891\u5408\u6210", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u957f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u6d4b\u8bd5\u65f6\u8d85\u51fa\u8bad\u7ec3\u65f6\u957f\u4f1a\u51fa\u73b0\u89c6\u89c9\u8d28\u91cf\u5feb\u901f\u9000\u5316\u7684\u95ee\u9898\uff0c\u800c\u957f\u89c6\u9891\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u65e0\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848", "method": "\u57fa\u4e8eSelf Forcing\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u81ea\u56de\u5f52\u7f13\u5b58\u7ef4\u62a4\u673a\u5236\uff0c\u63d0\u51faRolling Sink\u65b9\u6cd5\uff0c\u5728\u6d4b\u8bd5\u65f6\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5c06\u89c6\u9891\u5408\u6210\u6269\u5c55\u5230\u8d85\u957f\u65f6\u957f", "result": "Rolling Sink\u5728\u4ec5\u75285\u79d2\u7247\u6bb5\u8bad\u7ec3\u7684\u57fa\u7840\u4e0a\uff0c\u80fd\u5728\u6d4b\u8bd5\u65f6\u5408\u62105-30\u5206\u949f\u7684\u8d85\u957f\u89c6\u9891\uff0c\u4fdd\u6301\u4e3b\u4f53\u4e00\u81f4\u3001\u989c\u8272\u7a33\u5b9a\u3001\u7ed3\u6784\u8fde\u8d2f\u3001\u8fd0\u52a8\u5e73\u6ed1\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "Rolling Sink\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u5f25\u5408\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u6709\u9650\u8bad\u7ec3\u65f6\u957f\u4e0e\u65e0\u9650\u6d4b\u8bd5\u65f6\u957f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u8d85\u957f\u89c6\u9891\u5408\u6210"}}
{"id": "2602.07784", "pdf": "https://arxiv.org/pdf/2602.07784", "abs": "https://arxiv.org/abs/2602.07784", "authors": ["Jayawant Bodagala", "Balaji Bodagala"], "title": "Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing", "categories": ["cs.CV"], "comment": "Total pages: 9", "summary": "Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.", "AI": {"tldr": "UCATSC\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u968f\u673a\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u8def\u53e3\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff0c\u8003\u8651\u89c6\u89c9\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u786c\u7ea6\u675f\u786e\u4fdd\u5b89\u5168\u548c\u9632\u6b62\u9965\u997f\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u90e8\u7f72\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u57fa\u4e8e\u89c6\u89c9\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u9690\u542b\u5b89\u5168\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u4e3b\u8981\u5728\u6a21\u62df\u4e2d\u5b66\u4e60\u548c\u9a8c\u8bc1\u7684\u4e0d\u53ef\u89e3\u91ca\u63a7\u5236\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u5e26\u7ea6\u675f\u7684\u968f\u673a\u51b3\u7b56\u8fc7\u7a0b\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u5efa\u6a21\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff0c\u8003\u8651\u89c6\u89c9\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u3002\u5728\u4fe1\u5ff5\u7a7a\u95f4\u4e2d\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u6f14\u65f6\u9884\u6d4b\u5e76\u5f3a\u5236\u6267\u884c\u4e0e\u5b89\u5168\u548c\u9632\u6b62\u9965\u997f\u76f8\u5173\u7684\u786c\u7ea6\u675f\uff0c\u800c\u975e\u901a\u8fc7\u5956\u52b1\u5851\u9020\u5b66\u4e60\u5b89\u5168\u6027\u3002", "result": "\u7cfb\u7edf\u8bbe\u8ba1\u65e8\u5728\u6539\u5584\u4ea4\u901a\u5ef6\u8fdf\u548c\u6392\u653e\uff0c\u540c\u65f6\u9632\u6b62\u5b89\u5168\u5173\u952e\u9519\u8bef\uff0c\u5e76\u57fa\u4e8e\u663e\u5f0f\u6a21\u578b\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u7b56\u7565\u8f93\u51fa\u3002", "conclusion": "UCATSC\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u3001\u5b89\u5168\u4fdd\u8bc1\u548c\u7b56\u7565\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.07801", "pdf": "https://arxiv.org/pdf/2602.07801", "abs": "https://arxiv.org/abs/2602.07801", "authors": ["Wenqi Liu", "Yunxiao Wang", "Shijie Ma", "Meng Liu", "Qile Su", "Tianke Zhang", "Haonan Fan", "Changyi Liu", "Kaiyu Jiang", "Jiankang Chen", "Kaiyu Tang", "Bin Wen", "Fan Yang", "Tingting Gao", "Han Li", "Yinwei Wei", "Xuemeng Song"], "title": "VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.", "AI": {"tldr": "VideoTemp-o3\uff1a\u7edf\u4e00\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u89c6\u9891\u5b9a\u4f4d\u548c\u95ee\u7b54\uff0c\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5747\u5300\u91c7\u6837\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u548c\u5e7b\u89c9\u95ee\u9898", "motivation": "\u4f20\u7edf\u5747\u5300\u5e27\u91c7\u6837\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u96be\u4ee5\u6355\u6349\u5173\u952e\u89c6\u89c9\u8bc1\u636e\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u5e7b\u89c9\u589e\u52a0\u3002\u73b0\u6709\u4ee3\u7406\u5f0f\u89c6\u9891\u601d\u8003\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u3001\u5b9a\u4f4d\u80fd\u529b\u5f31\u3001\u6d41\u7a0b\u50f5\u5316\u7b49\u95ee\u9898", "method": "\u63d0\u51faVideoTemp-o3\u7edf\u4e00\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u89c6\u9891\u5b9a\u4f4d\u548c\u95ee\u7b54\u3002\u5728\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u8bbe\u8ba1\u7edf\u4e00\u63a9\u7801\u673a\u5236\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5f15\u5165\u4e13\u7528\u5956\u52b1\u9632\u6b62\u5956\u52b1\u653b\u51fb\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u957f\u89c6\u9891\u5b9a\u4f4dQA\u6570\u636e\u548c\u76f8\u5e94\u57fa\u51c6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u7406\u89e3\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347", "conclusion": "VideoTemp-o3\u901a\u8fc7\u7edf\u4e00\u7684\u4ee3\u7406\u5f0f\u89c6\u9891\u601d\u8003\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5b9a\u4f4d\u80fd\u529b\u3001\u6309\u9700\u88c1\u526a\u548c\u5b9a\u4f4d\u4fee\u6b63\u529f\u80fd"}}
{"id": "2602.07814", "pdf": "https://arxiv.org/pdf/2602.07814", "abs": "https://arxiv.org/abs/2602.07814", "authors": ["Simiao Ren", "Yuchen Zhou", "Xingyu Shen", "Kidus Zewde", "Tommy Duong", "George Huang", "Hatsanai", "Tiangratanakul", "Tsang", "Ng", "En Wei", "Jiayu Xue"], "title": "How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$\u03c1$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\\% mean accuracy) from the worst (37.5\\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $\u03c7^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.", "AI": {"tldr": "\u9996\u6b21\u5bf916\u79cd\u6700\u5148\u8fdb\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u6db5\u76d623\u4e2a\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\u53d8\u4f53\u548c12\u4e2a\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u6ca1\u6709\u901a\u7528\u6700\u4f73\u68c0\u6d4b\u5668\uff0c\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u73b0\u4ee3\u5546\u4e1a\u751f\u6210\u5668\u80fd\u51fb\u8d25\u5927\u591a\u6570\u68c0\u6d4b\u5668\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u56fe\u50cf\u5728\u6570\u5b57\u5e73\u53f0\u6cdb\u6ee5\uff0c\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u4e8e\u6253\u51fb\u865a\u5047\u4fe1\u606f\u548c\u7ef4\u62a4\u5185\u5bb9\u771f\u5b9e\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u5fae\u8c03\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u6700\u5e38\u89c1\u7684\u96f6\u6837\u672c\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u5bf916\u79cd\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0823\u4e2a\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\u53d8\u4f53\uff09\u572812\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9996\u6b21\u5168\u9762\u7684\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u6db5\u76d6291\u4e2a\u751f\u6210\u5668\u548c260\u4e07\u56fe\u50cf\u6837\u672c\uff0c\u5305\u62ec\u73b0\u4ee3\u6269\u6563\u6a21\u578b\u3002", "result": "1) \u6ca1\u6709\u901a\u7528\u6700\u4f73\u68c0\u6d4b\u5668\uff0c\u6392\u540d\u6781\u4e0d\u7a33\u5b9a\uff1b2) \u6700\u4f73\u4e0e\u6700\u5dee\u68c0\u6d4b\u5668\u6027\u80fd\u5dee\u8ddd\u8fbe37\u4e2a\u767e\u5206\u70b9\uff1b3) \u8bad\u7ec3\u6570\u636e\u5bf9\u9f50\u5bf9\u6cdb\u5316\u5f71\u54cd\u663e\u8457\uff1b4) \u73b0\u4ee3\u5546\u4e1a\u751f\u6210\u5668\u80fd\u51fb\u8d25\u5927\u591a\u6570\u68c0\u6d4b\u5668\uff1b5) \u8bc6\u522b\u51fa\u4e09\u79cd\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\"\u4e00\u5200\u5207\"\u7684\u68c0\u6d4b\u5668\u8303\u5f0f\uff0c\u8868\u660e\u4ece\u4e1a\u8005\u5fc5\u987b\u6839\u636e\u5177\u4f53\u5a01\u80c1\u73af\u5883\u4ed4\u7ec6\u9009\u62e9\u68c0\u6d4b\u5668\uff0c\u800c\u4e0d\u80fd\u4f9d\u8d56\u5df2\u53d1\u5e03\u7684\u57fa\u51c6\u6027\u80fd\u3002"}}
{"id": "2602.07815", "pdf": "https://arxiv.org/pdf/2602.07815", "abs": "https://arxiv.org/abs/2602.07815", "authors": ["Simiao Ren"], "title": "Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures", "categories": ["cs.CV"], "comment": null, "summary": "Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \\textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \\textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \\emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\\% false adult rates on minors while VLMs achieve 13--25\\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.", "AI": {"tldr": "VLMs\u5728\u5e74\u9f84\u4f30\u8ba1\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\uff0c\u6700\u4f73VLM\u6bd4\u6700\u4f73\u4e13\u7528\u6a21\u578bMAE\u4f4e15%\uff0c\u6311\u6218\u4e86\u4efb\u52a1\u4e13\u7528\u67b6\u6784\u7684\u5fc5\u8981\u6027\u5047\u8bbe\u3002", "motivation": "\u9762\u90e8\u5e74\u9f84\u4f30\u8ba1\u5bf9\u5185\u5bb9\u5ba1\u6838\u3001\u5e74\u9f84\u9a8c\u8bc1\u548c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4e13\u7528\u5e74\u9f84\u4f30\u8ba1\u67b6\u6784\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u521b\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u8de8\u8303\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f3034\u4e2a\u6a21\u578b\uff0822\u4e2a\u4e13\u7528\u67b6\u6784\u548c12\u4e2a\u901a\u7528VLMs\uff09\uff0c\u57288\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u603b\u8ba11,100\u5f20\u6d4b\u8bd5\u56fe\u50cf\uff0c\u4f7f\u7528MAE\u7b49\u6307\u6807\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\u3002", "result": "\u96f6\u6837\u672cVLMs\u663e\u8457\u4f18\u4e8e\u5927\u591a\u6570\u4e13\u7528\u6a21\u578b\uff08\u5e73\u5747MAE 5.65\u5e74 vs 9.88\u5e74\uff09\uff0c\u6700\u4f73VLM\uff08Gemini 3 Flash Preview\uff0cMAE 4.32\uff09\u6bd4\u6700\u4f73\u975eLLM\u6a21\u578b\uff08MiVOLO\uff0cMAE 5.10\uff09\u4f1815%\u3002VLMs\u572818\u5c81\u9608\u503c\u5e74\u9f84\u9a8c\u8bc1\u4e2d\u8868\u73b0\u66f4\u597d\uff0813-25%\u5047\u6210\u4eba\u7387 vs 60-100%\uff09\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u4efb\u52a1\u4e13\u7528\u67b6\u6784\u5bf9\u5e74\u9f84\u4f30\u8ba1\u5fc5\u8981\u7684\u5047\u8bbe\uff0c\u5efa\u8bae\u9886\u57df\u5e94\u8f6c\u5411\u5c06VLM\u80fd\u529b\u84b8\u998f\u5230\u9ad8\u6548\u7684\u4e13\u7528\u6a21\u578b\u4e2d\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u5e74\u9f84\u7ec4\uff08<5\u5c81\u548c65+\u5c81\uff09\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2602.07820", "pdf": "https://arxiv.org/pdf/2602.07820", "abs": "https://arxiv.org/abs/2602.07820", "authors": ["Zhibo Chen", "Yu Guan", "Yajuan Huang", "Chaoqi Chen", "XiangJi", "Qiuyun Fan", "Dong Liang", "Qiegen Liu"], "title": "Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7b97\u5b50\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u6d41\u4ea4\u4e92\u7f51\u7edc\uff08OCDI-Net\uff09\u663e\u5f0f\u5206\u79bb\u76ee\u6807\u5207\u7247\u5185\u5bb9\u548c\u5207\u7247\u95f4\u5e72\u6270\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u94fe\u5f0f\u63a8\u7406\u5b9e\u73b0\u540c\u65f6\u591a\u5207\u7247\uff08SMS\uff09MRI\u91cd\u5efa", "motivation": "\u540c\u65f6\u591a\u5207\u7247\uff08SMS\uff09\u6210\u50cf\u7ed3\u5408\u5e73\u9762\u5185\u6b20\u91c7\u6837\u53ef\u5b9e\u73b0\u9ad8\u5ea6\u52a0\u901f\u7684MRI\uff0c\u4f46\u4f1a\u4ea7\u751f\u5f3a\u8026\u5408\u7684\u9006\u95ee\u9898\uff0c\u5b58\u5728\u786e\u5b9a\u6027\u5207\u7247\u95f4\u5e72\u6270\u548c\u7f3a\u5931k\u7a7a\u95f4\u6570\u636e\u3002\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u56f4\u7ed5\u9ad8\u65af\u566a\u58f0\u8bbe\u8ba1\uff0c\u9700\u8981\u989d\u5916\u4e00\u81f4\u6027\u6b65\u9aa4\u6765\u878d\u5165SMS\u7269\u7406\uff0c\u8fd9\u53ef\u80fd\u4e0eSMS\u91c7\u96c6\u4e2d\u7684\u7b97\u5b50\u63a7\u5236\u9000\u5316\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51fa\u7b97\u5b50\u5f15\u5bfc\u6846\u67b6\uff0c\u4f7f\u7528\u5df2\u77e5\u91c7\u96c6\u7b97\u5b50\u5efa\u6a21\u9000\u5316\u8f68\u8ff9\u5e76\u901a\u8fc7\u786e\u5b9a\u6027\u66f4\u65b0\u53cd\u8f6c\u8be5\u8fc7\u7a0b\u3002\u5f15\u5165\u7b97\u5b50\u6761\u4ef6\u53cc\u6d41\u4ea4\u4e92\u7f51\u7edc\uff08OCDI-Net\uff09\uff0c\u663e\u5f0f\u5206\u79bb\u76ee\u6807\u5207\u7247\u5185\u5bb9\u4e0e\u5207\u7247\u95f4\u5e72\u6270\uff0c\u9884\u6d4b\u7ed3\u6784\u5316\u9000\u5316\u4ee5\u8fdb\u884c\u7b97\u5b50\u5bf9\u9f50\u7684\u53cd\u8f6c\u3002\u5c06\u91cd\u5efa\u5b9e\u4f8b\u5316\u4e3a\u4e24\u9636\u6bb5\u94fe\u5f0f\u63a8\u7406\uff1a\u5148\u8fdb\u884cSMS\u5207\u7247\u5206\u79bb\uff0c\u518d\u8fdb\u884c\u5e73\u9762\u5185\u8865\u5168\u3002", "result": "\u5728fastMRI\u8111\u6570\u636e\u548c\u524d\u77bb\u6027\u91c7\u96c6\u7684\u4f53\u5185\u6269\u6563MRI\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684SMS\u91cd\u5efa\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4fdd\u771f\u5ea6\u5e76\u51cf\u5c11\u4e86\u5207\u7247\u6cc4\u6f0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u5b50\u5f15\u5bfc\u6846\u67b6\u548cOCDI-Net\u80fd\u591f\u6709\u6548\u5904\u7406SMS\u6210\u50cf\u4e2d\u7684\u8026\u5408\u9006\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5207\u7247\u95f4\u5e72\u6270\u548c\u7ed3\u6784\u5316\u9000\u5316\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u91cd\u5efa\uff0c\u51cf\u5c11\u4e86\u5207\u7247\u6cc4\u6f0f\u95ee\u9898\u3002"}}
{"id": "2602.07827", "pdf": "https://arxiv.org/pdf/2602.07827", "abs": "https://arxiv.org/abs/2602.07827", "authors": ["Guoting Wei", "Xia Yuan", "Yang Zhou", "Haizhao Jing", "Yu Liu", "Xianbiao Qi", "Chunxia Zhao", "Haokui Zhang", "Rong Xiao"], "title": "Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection", "categories": ["cs.CV"], "comment": null, "summary": "Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.", "AI": {"tldr": "OTA-Det\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u5f00\u653e\u8bcd\u6c47\u822a\u7a7a\u68c0\u6d4b\uff08OVAD\uff09\u548c\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d\uff08RSVG\uff09\u4e24\u4e2a\u8303\u5f0f\u7ed3\u5408\uff0c\u652f\u6301\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u7684\u540c\u65f6\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "OVAD\u548cRSVG\u662f\u822a\u7a7a\u573a\u666f\u7406\u89e3\u7684\u4e24\u4e2a\u5173\u952e\u8303\u5f0f\uff0c\u4f46\u5404\u81ea\u5b58\u5728\u5c40\u9650\u6027\uff1aOVAD\u4ec5\u9650\u4e8e\u7c97\u7c92\u5ea6\u7684\u7c7b\u522b\u7ea7\u8bed\u4e49\uff0c\u800cRSVG\u5728\u7ed3\u6784\u4e0a\u4ec5\u9650\u4e8e\u5355\u76ee\u6807\u5b9a\u4f4d\u3002\u8fd9\u4e9b\u9650\u5236\u4f7f\u5f97\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\u3002", "method": "1. \u4efb\u52a1\u91cd\u6784\u7b56\u7565\uff1a\u7edf\u4e00\u4efb\u52a1\u76ee\u6807\u548c\u76d1\u7763\u673a\u5236\uff0c\u652f\u6301\u8de8\u8303\u5f0f\u6570\u636e\u96c6\u7684\u8054\u5408\u8bad\u7ec3\uff1b2. \u5bc6\u96c6\u8bed\u4e49\u5bf9\u9f50\u7b56\u7565\uff1a\u5efa\u7acb\u4ece\u6574\u4f53\u8868\u8fbe\u5230\u4e2a\u4f53\u5c5e\u6027\u7684\u591a\u7c92\u5ea6\u663e\u5f0f\u5bf9\u5e94\u5173\u7cfb\uff1b3. \u57fa\u4e8eRT-DETR\u67b6\u6784\u6269\u5c55\uff0c\u5f15\u5165\u9ad8\u6548\u6a21\u5757\uff0c\u4ece\u95ed\u96c6\u68c0\u6d4b\u6269\u5c55\u5230\u5f00\u653e\u6587\u672c\u68c0\u6d4b\u3002", "result": "\u5728OVAD\u548cRSVG\u4efb\u52a1\u7684\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u630134 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "OTA-Det\u6210\u529f\u7edf\u4e00\u4e86OVAD\u548cRSVG\u4e24\u4e2a\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u5404\u81ea\u5b64\u7acb\u64cd\u4f5c\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6548\u7387\u7684\u540c\u65f6\u53d6\u5f97\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.07833", "pdf": "https://arxiv.org/pdf/2602.07833", "abs": "https://arxiv.org/abs/2602.07833", "authors": ["Weijiang Lv", "Yaoxuan Feng", "Xiaobo Xia", "Jiayu Wang", "Yan Jing", "Wenchao Chen", "Bo Chen"], "title": "SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "53 pages, 42 figures, 14 tables", "summary": "Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SPD-Faith Bench\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u4e86\u611f\u77e5\u76f2\u533a\u548c\u611f\u77e5-\u63a8\u7406\u5206\u79bb\u4e24\u79cd\u7cfb\u7edf\u6545\u969c\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684SAGE\u6846\u67b6\u6765\u6539\u5584\u89c6\u89c9\u8bc1\u636e\u6821\u51c6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u7684\u5fe0\u5b9e\u6027\u4ecd\u4e0d\u6e05\u695a\u3002\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u5e7b\u89c9\uff0c\u800c\u63a8\u7406\u5c42\u9762\u7684\u4e0d\u5fe0\u5b9e\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165SPD-Faith Bench\u8bca\u65ad\u57fa\u51c6\uff0c\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5dee\u5f02\u63a8\u7406\u6765\u5f3a\u5236\u663e\u5f0f\u89c6\u89c9\u6bd4\u8f83\uff0c\u4ee5\u9694\u79bb\u8bed\u8a00\u5148\u9a8c\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5206\u6790\u53d1\u73b0\u89c6\u89c9\u6ce8\u610f\u529b\u8870\u51cf\u548c\u6b8b\u5dee\u6d41\u4e2d\u7684\u8868\u793a\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u4e86SAGE\u6846\u67b6\u6765\u6539\u5584\u89c6\u89c9\u8def\u7531\u5e76\u4f7f\u63a8\u7406\u4e0e\u611f\u77e5\u5bf9\u9f50\u3002", "result": "\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63ed\u793a\u4e86\u4e24\u79cd\u7cfb\u7edf\u6545\u969c\u6a21\u5f0f\uff1a\u611f\u77e5\u76f2\u533a\u548c\u611f\u77e5-\u63a8\u7406\u5206\u79bb\u3002\u8fd9\u4e9b\u6545\u969c\u6e90\u4e8e\u89c6\u89c9\u6ce8\u610f\u529b\u8870\u51cf\u548c\u6b8b\u5dee\u6d41\u4e2d\u7684\u8868\u793a\u504f\u79fb\u3002SAGE\u6846\u67b6\u80fd\u591f\u6539\u5584\u89c6\u89c9\u8def\u7531\u5e76\u63d0\u5347\u63a8\u7406\u5fe0\u5b9e\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8d85\u8d8a\u54cd\u5e94\u6b63\u786e\u6027\u6765\u663e\u5f0f\u8bc4\u4f30\u5fe0\u5b9e\u6027\u7684\u91cd\u8981\u6027\u3002\u63d0\u51fa\u7684\u57fa\u51c6\u548cSAGE\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u6539\u5584\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5fe0\u5b9e\u6027\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2602.07835", "pdf": "https://arxiv.org/pdf/2602.07835", "abs": "https://arxiv.org/abs/2602.07835", "authors": ["Sanoojan Baliah", "Yohan Abeysinghe", "Rusiru Thushara", "Khan Muhammad", "Abhinav Dhall", "Karthik Nandakumar", "Muhammad Haris Khan"], "title": "VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping", "categories": ["cs.CV"], "comment": null, "summary": "We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.", "AI": {"tldr": "VFace\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\uff0c\u53ef\u4e0e\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u901a\u8fc7\u9891\u7387\u8c31\u6ce8\u610f\u529b\u63d2\u503c\u3001\u76ee\u6807\u7ed3\u6784\u5f15\u5bfc\u548c\u6d41\u5f15\u5bfc\u6ce8\u610f\u529b\u65f6\u5e8f\u5e73\u6ed1\u4e09\u5927\u6280\u672f\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u5728\u89c6\u9891\u5e94\u7528\u4e2d\u5b58\u5728\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u89c6\u9891\u7279\u5b9a\u5fae\u8c03\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "method": "1. \u9891\u7387\u8c31\u6ce8\u610f\u529b\u63d2\u503c\uff1a\u4fc3\u8fdb\u751f\u6210\u5e76\u4fdd\u6301\u5173\u952e\u8eab\u4efd\u7279\u5f81\uff1b2. \u76ee\u6807\u7ed3\u6784\u5f15\u5bfc\uff1a\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7684\u6ce8\u610f\u529b\u6ce8\u5165\uff0c\u5c06\u76ee\u6807\u5e27\u7684\u7ed3\u6784\u7279\u5f81\u4e0e\u751f\u6210\u5bf9\u9f50\uff1b3. \u6d41\u5f15\u5bfc\u6ce8\u610f\u529b\u65f6\u5e8f\u5e73\u6ed1\uff1a\u5728\u4e0d\u4fee\u6539\u5e95\u5c42\u6269\u6563\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5f3a\u5236\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u9010\u5e27\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u4e3a\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6a21\u5757\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "VFace\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\uff0c\u53ef\u4e0e\u73b0\u6709\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u5e94\u7528\u4e2d\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u6a21\u5757\u5316\u4f18\u52bf\u3002"}}
{"id": "2602.07854", "pdf": "https://arxiv.org/pdf/2602.07854", "abs": "https://arxiv.org/abs/2602.07854", "authors": ["Chendong Xiang", "Jiajun Liu", "Jintao Zhang", "Xiao Yang", "Zhengwei Fang", "Shizun Wang", "Zijun Wang", "Yingtian Zou", "Hang Su", "Jun Zhu"], "title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model", "categories": ["cs.CV"], "comment": null, "summary": "Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \\textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \\textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \\textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.", "AI": {"tldr": "ViewRope\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u4e2d\u7a7a\u95f4\u6301\u4e45\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u4e00\u81f4\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u5f53\u524d\u9884\u6d4b\u6027\u4e16\u754c\u6a21\u578b\u7f3a\u4e4f\u7a7a\u95f4\u6301\u4e45\u6027\uff0c\u5728\u957f\u8f68\u8ff9\u4e2d\u65e0\u6cd5\u4fdd\u6301\u7a33\u5b9a\u7684\u573a\u666f\u7ed3\u6784\uff0c\u5f53\u76f8\u673a\u91cd\u65b0\u8bbf\u95ee\u5148\u524d\u89c2\u5bdf\u4f4d\u7f6e\u65f6\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\u7ec6\u8282\u3002\u8fd9\u79cd\u51e0\u4f55\u6f02\u79fb\u6e90\u4e8e\u5bf9\u5c4f\u5e55\u7a7a\u95f4\u4f4d\u7f6e\u5d4c\u5165\u7684\u4f9d\u8d56\uff0c\u8fd9\u4e0e3D\u4e00\u81f4\u6027\u6240\u9700\u7684\u6295\u5f71\u51e0\u4f55\u76f8\u51b2\u7a81\u3002", "method": "\u63d0\u51faViewRope\u51e0\u4f55\u611f\u77e5\u7f16\u7801\uff0c\u5c06\u76f8\u673a\u5c04\u7ebf\u65b9\u5411\u76f4\u63a5\u6ce8\u5165\u89c6\u9891transformer\u81ea\u6ce8\u610f\u529b\u5c42\uff1b\u901a\u8fc7\u76f8\u5bf9\u5c04\u7ebf\u51e0\u4f55\u800c\u975e\u50cf\u7d20\u5c40\u90e8\u6027\u53c2\u6570\u5316\u6ce8\u610f\u529b\uff1b\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u5e27\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u5229\u7528\u51e0\u4f55\u7ebf\u7d22\u9009\u62e9\u6027\u5173\u6ce8\u76f8\u5173\u5386\u53f2\u5e27\uff1b\u5efa\u7acbViewBench\u8bca\u65ad\u5957\u4ef6\u6d4b\u91cf\u95ed\u73af\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u6f02\u79fb\u3002", "result": "ViewRope\u663e\u8457\u6539\u5584\u4e86\u957f\u671f\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u89e3\u51b3\u9884\u6d4b\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u7a7a\u95f4\u6301\u4e45\u6027\u95ee\u9898\uff0cViewRope\u4e3a3D\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u6a21\u578b\u539f\u751f\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u662f\u63d0\u5347\u957f\u671f\u573a\u666f\u4e00\u81f4\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.07860", "pdf": "https://arxiv.org/pdf/2602.07860", "abs": "https://arxiv.org/abs/2602.07860", "authors": ["Fei Yu", "Shudan Guo", "Shiqing Xin", "Beibei Wang", "Haisen Zhao", "Wenzheng Chen"], "title": "Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted by 3DV 2026. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/", "summary": "We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.\n  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.\n  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u8d85\u9ad8\u901f\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u6062\u590d3D\u5f62\u72b6\u7684\u9006\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5feb\u901f\u91cd\u5fc3\u5750\u6807\u6c42\u89e3\u5668\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u73b0\u9ad8\u6548\u903c\u771f\u7684\u9ad8\u901f\u8fd0\u52a8\u6a21\u62df\u548c3D\u91cd\u5efa\u3002", "motivation": "\u5728\u81ea\u7136\u548c\u5de5\u4e1a\u573a\u666f\u4e2d\uff08\u5982\u8fd0\u52a8\u4e2d\u7684\u7403\u4f53\u6216\u65cb\u8f6c\u673a\u68b0\uff09\uff0c\u7269\u4f53\u9ad8\u901f\u8fd0\u52a8\u5bfc\u81f4\u56fe\u50cf\u4e25\u91cd\u6a21\u7cca\uff0c\u4f20\u7edf\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\u7b493D\u91cd\u5efa\u6280\u672f\u5931\u6548\uff0c\u9700\u8981\u89e3\u51b3\u4ece\u6781\u7aef\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u6062\u590d\u51e0\u4f55\u5f62\u72b6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u9006\u6e32\u67d3\u65b9\u6cd5\uff0c\u5305\u542b\u5feb\u901f\u91cd\u5fc3\u5750\u6807\u6c42\u89e3\u5668\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff08\u901f\u5ea6\u63d0\u5347\u8fbe4.57\u500d\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u903c\u771f\u7684\u9ad8\u901f\u8fd0\u52a8\u6a21\u62df\u3002\u65b9\u6cd5\u5b8c\u5168\u53ef\u5fae\u5206\uff0c\u652f\u6301\u4ece\u6e32\u67d3\u56fe\u50cf\u52303D\u5f62\u72b6\u7684\u68af\u5ea6\u4f20\u64ad\u3002", "result": "\u5728\u5feb\u901f\u5e73\u79fb\u548c\u65cb\u8f6c\u4e24\u79cd\u5178\u578b\u8fd0\u52a8\u7c7b\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u80fd\u9ad8\u6548\u903c\u771f\u5730\u6a21\u62df\u8d85\u9ad8\u901f\u8fd0\u52a8\u7269\u4f53\uff0c\u5e76\u6210\u529f\u4ece\u6781\u7aef\u5e73\u79fb\u548c\u65cb\u8f6c\u8fd0\u52a8\u76842D\u56fe\u50cf\u4e2d\u6062\u590d3D\u5f62\u72b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u57fa\u4e8e\u89c6\u89c9\u76843D\u91cd\u5efa\u8fb9\u754c\uff0c\u5b9e\u73b0\u4e86\u4ece\u8d85\u9ad8\u901f\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u4e2d\u6062\u590d3D\u5f62\u72b6\uff0c\u4e3a\u5904\u7406\u6781\u7aef\u8fd0\u52a8\u573a\u666f\u7684\u51e0\u4f55\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07864", "pdf": "https://arxiv.org/pdf/2602.07864", "abs": "https://arxiv.org/abs/2602.07864", "authors": ["Chen Yang", "Guanxin Lin", "Youquan He", "Peiyao Chen", "Guanghe Liu", "Yufan Mo", "Zhouyuan Xu", "Linhao Wang", "Guohui Zhang", "Zihang Zhang", "Shenxiang Zeng", "Chen Wang", "Jiansheng Fan"], "title": "Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds", "categories": ["cs.CV"], "comment": null, "summary": "Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.", "AI": {"tldr": "SSI-Bench\u662f\u4e00\u4e2a\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u53d7\u7ea6\u675f\u6d41\u5f62\u4e0a\u7684\u7a7a\u95f4\u63a8\u7406\uff0c\u5305\u542b1000\u4e2a\u6392\u5e8f\u95ee\u9898\uff0c\u8bc4\u4f30\u51e0\u4f55\u548c\u62d3\u6251\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u8bc4\u4f30\u65e0\u7ea6\u675f\u573a\u666f\uff0c\u6a21\u578b\u53ef\u4ee5\u5229\u75282D\u6377\u5f84\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u7269\u7406\u4e16\u754c\u4e2d\u53d7\u51e0\u4f55\u3001\u62d3\u6251\u548c\u7269\u7406\u7ea6\u675f\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5b8c\u5168\u4eba\u5de5\u4e2d\u5fc3\u7684\u6d41\u7a0b\u6784\u5efa\uff1a10\u540d\u7814\u7a76\u4eba\u5458\u82b1\u8d39400\u591a\u5c0f\u65f6\u7cbe\u5fc3\u6311\u9009\u56fe\u50cf\u3001\u6807\u6ce8\u7ed3\u6784\u7ec4\u4ef6\u3001\u8bbe\u8ba1\u95ee\u9898\u4ee5\u6700\u5c0f\u5316\u50cf\u7d20\u7ea7\u7ebf\u7d22\uff0c\u5305\u542b\u51e0\u4f55\u548c\u62d3\u6251\u63a8\u7406\u7684\u6392\u5e8f\u95ee\u9898\u3002", "result": "\u8bc4\u4f3031\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684VLM\u663e\u793a\u4e0e\u4eba\u7c7b\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\uff1a\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u51c6\u786e\u738722.2%\uff0c\u6700\u5f3a\u95ed\u6e90\u6a21\u578b33.6%\uff0c\u800c\u4eba\u7c7b\u8fbe\u523091.6%\u3002\u9f13\u52b1\u6a21\u578b\u601d\u8003\u4ec5\u5e26\u6765\u8fb9\u9645\u6536\u76ca\uff0c\u9519\u8bef\u5206\u6790\u663e\u793a\u7ed3\u6784\u57fa\u7840\u548c\u7ea6\u675f\u4e00\u81f4\u76843D\u63a8\u7406\u5931\u8d25\u3002", "conclusion": "SSI-Bench\u63ed\u793a\u4e86\u5f53\u524dVLM\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u4e25\u91cd\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u57fa\u7840\u548c\u7ea6\u675f\u4e00\u81f4\u76843D\u63a8\u7406\u65b9\u9762\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2602.07872", "pdf": "https://arxiv.org/pdf/2602.07872", "abs": "https://arxiv.org/abs/2602.07872", "authors": ["Mert Sonmezer", "Serge Vasylechko", "Duygu Atasoy", "Seyda Ertekin", "Sila Kurugol"], "title": "WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning", "categories": ["cs.CV"], "comment": null, "summary": "Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.", "AI": {"tldr": "WristMIR\uff1a\u57fa\u4e8e\u533a\u57df\u611f\u77e5\u7684\u513f\u79d1\u8155\u90e8X\u5149\u7247\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6bd4\u7f16\u7801\u5668\u5b9e\u73b0\u4e24\u9636\u6bb5\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u9aa8\u6298\u6a21\u5f0f\u68c0\u7d22\u548c\u8bca\u65ad\u6027\u80fd", "motivation": "\u8155\u90e8X\u5149\u7247\u4e2d\u9aa8\u6298\u6a21\u5f0f\u7684\u68c0\u7d22\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e34\u5e8a\u91cd\u8981\u7ebf\u7d22\u7ec6\u5fae\u3001\u9ad8\u5ea6\u5c40\u90e8\u5316\uff0c\u4e14\u5e38\u88ab\u91cd\u53e0\u89e3\u5256\u7ed3\u6784\u6216\u4e0d\u540c\u6210\u50cf\u89c6\u89d2\u6240\u63a9\u76d6\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u5927\u578b\u3001\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\u6570\u636e\u96c6\u4e5f\u9650\u5236\u4e86\u8fdb\u5c55\u3002", "method": "WristMIR\u5229\u7528\u5bc6\u96c6\u653e\u5c04\u5b66\u62a5\u544a\u548c\u9aa8\u7279\u5f02\u6027\u5b9a\u4f4d\uff0c\u901a\u8fc7MedGemma\u7ed3\u6784\u5316\u62a5\u544a\u6316\u6398\u751f\u6210\u5168\u5c40\u548c\u533a\u57df\u7ea7\u63cf\u8ff0\uff0c\u7ed3\u5408\u9884\u5904\u7406\u7684\u8155\u90e8\u56fe\u50cf\u548c\u7279\u5b9a\u9aa8\u9abc\u88c1\u526a\uff08\u8fdc\u7aef\u6861\u9aa8\u3001\u8fdc\u7aef\u5c3a\u9aa8\u3001\u5c3a\u9aa8\u830e\u7a81\uff09\uff0c\u8054\u5408\u8bad\u7ec3\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6bd4\u7f16\u7801\u5668\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\uff1a\u7c97\u7c92\u5ea6\u5168\u5c40\u5339\u914d\u8bc6\u522b\u5019\u9009\u68c0\u67e5\uff0c\u7136\u540e\u533a\u57df\u6761\u4ef6\u91cd\u6392\u5e8f\u5bf9\u9f50\u9884\u5b9a\u4e49\u89e3\u5256\u9aa8\u9abc\u533a\u57df\u3002", "result": "WristMIR\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u56fe\u50cf\u5230\u6587\u672cRecall@5\u4ece0.82%\u63d0\u9ad8\u52309.35%\uff1b\u5d4c\u5165\u8868\u793a\u5728\u9aa8\u6298\u5206\u7c7b\u4e0a\u8868\u73b0\u66f4\u5f3a\uff08AUROC 0.949\uff0cAUPRC 0.953\uff09\uff1b\u5728\u533a\u57df\u611f\u77e5\u8bc4\u4f30\u4e2d\uff0c\u4e24\u9636\u6bb5\u8bbe\u8ba1\u663e\u8457\u6539\u5584\u57fa\u4e8e\u68c0\u7d22\u7684\u9aa8\u6298\u8bca\u65ad\uff0c\u5e73\u5747F1\u4ece0.568\u63d0\u5347\u52300.753\uff1b\u653e\u5c04\u79d1\u533b\u751f\u8bc4\u4ef7\u5176\u68c0\u7d22\u75c5\u4f8b\u4e34\u5e8a\u76f8\u5173\u6027\u66f4\u9ad8\uff0c\u5e73\u5747\u8bc4\u5206\u4ece3.36\u63d0\u9ad8\u52304.35\u3002", "conclusion": "WristMIR\u5c55\u793a\u4e86\u89e3\u5256\u5f15\u5bfc\u68c0\u7d22\u5728\u589e\u5f3a\u513f\u79d1\u808c\u8089\u9aa8\u9abc\u6210\u50cf\u8bca\u65ad\u63a8\u7406\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u533a\u57df\u611f\u77e5\u6846\u67b6\u3002"}}
{"id": "2602.07891", "pdf": "https://arxiv.org/pdf/2602.07891", "abs": "https://arxiv.org/abs/2602.07891", "authors": ["Zihui Gao", "Ke Liu", "Donny Y. Chen", "Duochao Shi", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u4ece\u539f\u59cb\u89c6\u9891\u6d41\u4e2d\u81ea\u9002\u5e94\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6316\u6398\u7ba1\u9053\u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u8bad\u7ec3\u8f68\u8ff9\uff0c\u7ed3\u5408\u7a00\u758f\u51e0\u4f55\u951a\u70b9\u548c\u5bc6\u96c6\u53ef\u5fae\u4e00\u81f4\u6027\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u57283D\u91cd\u5efa\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u5230\u5927\u89c4\u6a213D\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002\u4e92\u8054\u7f51\u89c6\u9891\u63d0\u4f9b\u4e86\u51e0\u4e4e\u65e0\u9650\u7684\u539f\u59cb\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u51e0\u4f55\u6807\u6ce8\u548c\u5b58\u5728\u89c2\u6d4b\u566a\u58f0\uff0c\u5c06\u5176\u7528\u4f5c\u51e0\u4f55\u5b66\u4e60\u7684\u6269\u5c55\u6e90\u5177\u6709\u6311\u6218\u6027\u3002", "method": "SAGE\u91c7\u7528\u5206\u5c42\u6316\u6398\u7ba1\u9053\uff1a1) \u4fe1\u606f\u6027\u8bad\u7ec3\u8f68\u8ff9\u9009\u62e9\uff1b2) \u901a\u8fc7SfM\u70b9\u4e91\u8fdb\u884c\u7a00\u758f\u51e0\u4f55\u951a\u70b9\uff0c\u63d0\u4f9b\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\uff1b3) \u901a\u8fc73D\u9ad8\u65af\u6e32\u67d3\u5b9e\u73b0\u5bc6\u96c6\u53ef\u5fae\u4e00\u81f4\u6027\uff0c\u63d0\u4f9b\u591a\u89c6\u89d2\u7ea6\u675f\u3002\u4e3a\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u951a\u70b9\u6570\u636e\u7684\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u57fa\u51c6\u6d4b\u8bd5\uff087Scenes\u3001TUM-RGBD\u3001Matterport3D\uff09\u4e0a\uff0cSAGE\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5c06Chamfer\u8ddd\u79bb\u964d\u4f4e\u4e8620-42%\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SAGE\u5f00\u521b\u4e86\u901a\u8fc7\u4e92\u8054\u7f51\u89c6\u9891\u81ea\u9002\u5e94\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u5148\u6cb3\uff0c\u4e3a\u901a\u75283D\u5b66\u4e60\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u8303\u5f0f\u3002"}}
{"id": "2602.07899", "pdf": "https://arxiv.org/pdf/2602.07899", "abs": "https://arxiv.org/abs/2602.07899", "authors": ["Zhenhao Shang", "Haizhao Jing", "Guoting Wei", "Haokui Zhang", "Rong Xiao", "Jianqing Gao", "Peng Wang"], "title": "Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.", "AI": {"tldr": "TLQ\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684token\u7ea7\u91cd\u8981\u6027\u611f\u77e5\u5c42\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u5f15\u5bfc\u7684token\u91cd\u8981\u6027\u6574\u5408\u673a\u5236\u548c\u591aGPU\u5c42\u6821\u51c6\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9token\u548c\u6587\u672ctoken\u5728\u6fc0\u6d3b\u5206\u5e03\u548c\u91cf\u5316\u8bef\u5dee\u654f\u611f\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7ed9PTQ\u6821\u51c6\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003VLM\u4e2d\u7684PTQ\u6821\u51c6\u7b56\u7565\u3002", "method": "1) \u57fa\u4e8e\u68af\u5ea6\u4fe1\u606f\u8bbe\u8ba1token\u7ea7\u91cd\u8981\u6027\u6574\u5408\u673a\u5236\u7528\u4e8e\u91cf\u5316\u8bef\u5dee\uff1b2) \u6784\u5efatoken\u7ea7\u6821\u51c6\u96c6\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6821\u51c6\uff1b3) \u5f15\u5165\u591aGPU\u3001\u91cf\u5316\u66b4\u9732\u7684\u5c42\u6821\u51c6\u65b9\u6848\uff0c\u4fdd\u6301\u6821\u51c6\u4e0e\u771f\u5b9e\u91cf\u5316\u63a8\u7406\u8def\u5f84\u4e00\u81f4\u3002", "result": "\u5728\u4e24\u4e2a\u6a21\u578b\u3001\u4e09\u79cd\u6a21\u578b\u89c4\u6a21\u548c\u4e24\u79cd\u91cf\u5316\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\uff0cTLQ\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u5747\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u91cf\u5316\u7a33\u5b9a\u6027\u3002", "conclusion": "TLQ\u6846\u67b6\u901a\u8fc7token\u7ea7\u91cd\u8981\u6027\u611f\u77e5\u548c\u5c42\u6821\u51c6\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLM\u91cf\u5316\u4e2d\u7684\u6821\u51c6\u6311\u6218\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u91cf\u5316\u65b9\u6848\u3002"}}
{"id": "2602.07931", "pdf": "https://arxiv.org/pdf/2602.07931", "abs": "https://arxiv.org/abs/2602.07931", "authors": ["Olena Hrynenko", "Darya Baranouskaya", "Alina Elena Baia", "Andrea Cavallaro"], "title": "Which private attributes do VLMs agree on and predict well?", "categories": ["cs.CV"], "comment": "This work has been accepted to the ICASSP 2026", "summary": "Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9690\u79c1\u76f8\u5173\u5c5e\u6027\u8bc6\u522b\u4e2d\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u663e\u793a\uff0cVLM\u503e\u5411\u4e8e\u6bd4\u4eba\u7c7b\u6807\u6ce8\u8005\u66f4\u9891\u7e41\u5730\u9884\u6d4b\u9690\u79c1\u5c5e\u6027\u7684\u5b58\u5728\uff0c\u4f46\u5728VLM\u95f4\u9ad8\u4e00\u81f4\u6027\u60c5\u51b5\u4e0b\u53ef\u4ee5\u8865\u5145\u4eba\u7c7b\u6807\u6ce8", "motivation": "\u8bc4\u4f30\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9690\u79c1\u76f8\u5173\u5c5e\u6027\u8bc6\u522b\u4e2d\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u63a2\u7d22VLM\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ee5\u53caVLM\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u96c6\u9690\u79c1\u6807\u6ce8\u4e2d\u7684\u6f5c\u529b", "method": "\u5bf9\u5f00\u6e90VLM\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u8bc6\u522bVLM\u8868\u73b0\u51fa\u5f3a\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u7684\u5c5e\u6027\uff0c\u5206\u6790VLM\u4e0e\u4eba\u7c7b\u6807\u6ce8\u4e4b\u95f4\u7684\u5206\u6b67\u6848\u4f8b", "result": "VLM\u503e\u5411\u4e8e\u6bd4\u4eba\u7c7b\u6807\u6ce8\u8005\u66f4\u9891\u7e41\u5730\u9884\u6d4b\u9690\u79c1\u5c5e\u6027\u7684\u5b58\u5728\uff1b\u5728VLM\u95f4\u9ad8\u4e00\u81f4\u6027\u60c5\u51b5\u4e0b\uff0cVLM\u53ef\u4ee5\u8bc6\u522b\u4eba\u7c7b\u6807\u6ce8\u8005\u5ffd\u7565\u7684\u5c5e\u6027\uff0c\u8865\u5145\u4eba\u7c7b\u6807\u6ce8", "conclusion": "VLM\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u96c6\u7684\u9690\u79c1\u6807\u6ce8\u4e2d\u5177\u6709\u652f\u6301\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728VLM\u95f4\u9ad8\u4e00\u81f4\u6027\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u4f5c\u4e3a\u4eba\u7c7b\u6807\u6ce8\u7684\u8865\u5145"}}
{"id": "2602.07938", "pdf": "https://arxiv.org/pdf/2602.07938", "abs": "https://arxiv.org/abs/2602.07938", "authors": ["Rabbia Asghar", "Lukas Rummelhard", "Wenqian Liu", "Anne Spalanzani", "Christian Laugier"], "title": "Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps", "categories": ["cs.CV", "cs.RO"], "comment": "Updated version with major revisions; currently under the second round of review at IEEE Transactions on Intelligent Vehicles", "summary": "Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5360\u636e\u7f51\u683c\u5730\u56fe\u540c\u65f6\u9884\u6d4b\u672a\u6765\u5360\u636e\u72b6\u6001\u3001\u8f66\u8f86\u548c\u573a\u666f\u6d41\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u884c\u4e3a\u590d\u6742\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u7684\u5c40\u9650", "motivation": "\u73b0\u6709\u9a7e\u9a76\u573a\u666f\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u5360\u636e\u7f51\u683c\u7684agent-agnostic\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u52a8\u6001\u53c2\u4e0e\u8005\u7684\u590d\u6742\u884c\u4e3a\uff0c\u800cagent-specific\u65b9\u6cd5\u5bf9\u611f\u77e5\u4e0d\u4f73\u6216\u672a\u8bc6\u522bagent\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u7ed3\u5408\u4e24\u8005\u53ef\u5b9e\u73b0\u66f4\u9c81\u68d2\u5b89\u5168\u7684\u8fd0\u52a8\u9884\u6d4b", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u5229\u7528\u52a8\u6001\u5360\u636e\u7f51\u683c\u5730\u56fe\u5728\u6d41\u7ebf\u5316\u65f6\u95f4\u89e3\u7801\u7ba1\u9053\u4e2d\u540c\u65f6\u9884\u6d4b\u672a\u6765\u5360\u636e\u72b6\u6001\u7f51\u683c\u3001\u8f66\u8f86\u7f51\u683c\u548c\u573a\u666f\u6d41\u7f51\u683c\u3002\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u65f6\u7a7a\u9aa8\u5e72\u7f51\u7edc\uff0c\u91c7\u7528\u5b9a\u5236\u5316\u7684\u76f8\u4e92\u4f9d\u8d56\u635f\u5931\u51fd\u6570\u6355\u6349\u7f51\u683c\u95f4\u4f9d\u8d56\u5173\u7cfb\u5e76\u5b9e\u73b0\u591a\u6837\u5316\u672a\u6765\u9884\u6d4b", "result": "\u5728\u771f\u5b9e\u4e16\u754cnuScenes\u548cWoven Planet\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u52a8\u6001\u8f66\u8f86\u548c\u901a\u7528\u52a8\u6001\u573a\u666f\u5143\u7d20\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u540c\u65f6\u5904\u7406agent-agnostic\u548cagent-specific\u9884\u6d4b\uff0c\u80fd\u591f\u9884\u6d4b\u8f66\u8f86\u7684\u5177\u4f53\u884c\u4e3a\u5e76\u8bc6\u522b\u5176\u4ed6\u52a8\u6001\u5b9e\u4f53\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u9c81\u68d2\u548c\u5b89\u5168\u7684\u8fd0\u52a8\u9884\u6d4b"}}
{"id": "2602.07955", "pdf": "https://arxiv.org/pdf/2602.07955", "abs": "https://arxiv.org/abs/2602.07955", "authors": ["Jiwei Chen", "Qi Wang", "Junyu Gao", "Jing Zhang", "Dingyi Li", "Jing-Jia Luo"], "title": "One-Shot Crowd Counting With Density Guidance For Scene Adaptaion", "categories": ["cs.CV"], "comment": null, "summary": "Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c11\u6837\u672c\u5b66\u4e60\u7684\u8de8\u573a\u666f\u4eba\u7fa4\u8ba1\u6570\u65b9\u6cd5\uff0c\u5229\u7528\u5c40\u90e8\u548c\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\u6307\u5bfc\u6a21\u578b\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u76d1\u63a7\u573a\u666f", "motivation": "\u73b0\u6709\u7684\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u5728\u4e0d\u540c\u76d1\u63a7\u573a\u666f\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u76d1\u63a7\u573a\u666f\u7684\u65b9\u6cd5", "method": "\u63d0\u51fa\u591a\u5c40\u90e8\u5bc6\u5ea6\u5b66\u4e60\u5668\u5b66\u4e60\u652f\u6301\u573a\u666f\u4e2d\u4e0d\u540c\u5bc6\u5ea6\u5206\u5e03\u7684\u539f\u578b\uff0c\u7ed3\u5408\u5c40\u90e8\u5bc6\u5ea6\u76f8\u4f3c\u6027\u77e9\u9635\u548c\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\u8fdb\u884c\u53cc\u91cd\u6307\u5bfc", "result": "\u5728\u4e09\u4e2a\u76d1\u63a7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u76d1\u63a7\u573a\u666f\uff0c\u5e76\u5728\u5c11\u6837\u672c\u4eba\u7fa4\u8ba1\u6570\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\u7684\u7ed3\u5408\uff0c\u63d0\u51fa\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u5728\u65b0\u76d1\u63a7\u573a\u666f\u4e2d\u7684\u9002\u5e94\u80fd\u529b"}}
{"id": "2602.07960", "pdf": "https://arxiv.org/pdf/2602.07960", "abs": "https://arxiv.org/abs/2602.07960", "authors": ["Changli Tang", "Tianyi Wang", "Fengyun Rao", "Jing Lyu", "Chao Zhang"], "title": "D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning", "categories": ["cs.CV"], "comment": null, "summary": "Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \\textbf{d}ialogue-centric \\textbf{o}mni-modal large language model optimized for \\textbf{r}obust audio-visual \\textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \\href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \\href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.", "AI": {"tldr": "D-ORCA\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u5bf9\u8bdd\u7684\u8de8\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u9c81\u68d2\u7684\u89c6\u542c\u5b57\u5e55\u751f\u6210\uff0c\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u65f6\u95f4\u5b9a\u4f4d\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u89c6\u9891\u4e2d\u7684\u5bf9\u8bdd\u662f\u4e3b\u8981\u4fe1\u606f\u6765\u6e90\uff0c\u51c6\u786e\u8bc6\u522b\u8c01\u5728\u4f55\u65f6\u8bf4\u4e86\u4ec0\u4e48\u5bf9\u4e8e\u6df1\u5ea6\u89c6\u9891\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u5f00\u6e90\u751f\u6001\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u591a\u65b9\u5bf9\u8bdd\u89c6\u9891\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86D-ORCA\u5bf9\u8bdd\u4e2d\u5fc3\u7684\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86DVD\u53cc\u8bed\u6570\u636e\u96c6\uff08\u8fd14\u4e07\u8bad\u7ec3\u89c6\u9891+2000\u8bc4\u4f30\u89c6\u9891\uff09\u3002\u91c7\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u5305\u542b\u4e09\u4e2a\u65b0\u9896\u7684\u5956\u52b1\u51fd\u6570\uff1a\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u51c6\u786e\u6027\u3001\u5168\u5c40\u8bed\u97f3\u5185\u5bb9\u51c6\u786e\u6027\u548c\u53e5\u5b50\u7ea7\u65f6\u95f4\u8fb9\u754c\u5bf9\u9f50\u3002", "result": "D-ORCA\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u65f6\u95f4\u5b9a\u4f4d\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002\u5c3d\u7ba1\u53ea\u670980\u4ebf\u53c2\u6570\uff0c\u4f46\u5728\u591a\u4e2a\u901a\u7528\u89c6\u542c\u7406\u89e3\u57fa\u51c6\u4e0a\u4e0eQwen3-Omni\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "D-ORCA\u4e3a\u5bf9\u8bdd\u4e2d\u5fc3\u7684\u89c6\u542c\u5b57\u5e55\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u5f00\u6e90\u751f\u6001\u7684\u7a7a\u767d\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2602.07967", "pdf": "https://arxiv.org/pdf/2602.07967", "abs": "https://arxiv.org/abs/2602.07967", "authors": ["Xiaofeng Tan", "Wanjiang Weng", "Haodong Lei", "Hongsong Wang"], "title": "EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.", "AI": {"tldr": "EasyTune\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6b65\u5fae\u8c03\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5185\u5b58\u6d88\u8017\u5927\u548c\u4f18\u5316\u7c97\u7cd9\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u81ea\u7cbe\u70bc\u504f\u597d\u5b66\u4e60\u673a\u5236\u6765\u5904\u7406\u504f\u597d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u5fae\u5206\u5956\u52b1\u76f4\u63a5\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u504f\u597d\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4f18\u5316\u6548\u7387\u4f4e\u4e14\u7c92\u5ea6\u7c97\u7cd9\uff1b2) \u5185\u5b58\u6d88\u8017\u9ad8\u3002\u8fd9\u4e9b\u9650\u5236\u6e90\u4e8e\u53bb\u566a\u8f68\u8ff9\u4e2d\u4e0d\u540c\u6b65\u9aa4\u4e4b\u95f4\u7684\u9012\u5f52\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faEasyTune\u65b9\u6cd5\uff1a1) \u5728\u53bb\u566a\u8fc7\u7a0b\u7684\u6bcf\u4e2a\u6b65\u9aa4\u5206\u522b\u5fae\u8c03\u6269\u6563\u6a21\u578b\uff0c\u89e3\u8026\u9012\u5f52\u4f9d\u8d56\uff1b2) \u5f15\u5165\u81ea\u7cbe\u70bc\u504f\u597d\u5b66\u4e60(SPL)\u673a\u5236\uff0c\u52a8\u6001\u8bc6\u522b\u504f\u597d\u5bf9\u5e76\u8fdb\u884c\u504f\u597d\u5b66\u4e60\uff0c\u89e3\u51b3\u504f\u597d\u8fd0\u52a8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEasyTune\u5728MM-Dist\u5bf9\u9f50\u6307\u6807\u4e0a\u6bd4DRaFT-50\u63d0\u53478.2%\uff0c\u540c\u65f6\u4ec5\u9700\u517631.16%\u7684\u989d\u5916\u5185\u5b58\u5f00\u9500\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53477.3\u500d\u3002", "conclusion": "EasyTune\u901a\u8fc7\u5206\u6b65\u5fae\u8c03\u548c\u81ea\u7cbe\u70bc\u504f\u597d\u5b66\u4e60\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u3001\u7cbe\u7ec6\u5bf9\u9f50\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2602.07979", "pdf": "https://arxiv.org/pdf/2602.07979", "abs": "https://arxiv.org/abs/2602.07979", "authors": ["Peng Peng", "Xinrui Zhang", "Junlin Wang", "Lei Li", "Shaoyu Wang", "Qiegen Liu"], "title": "FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.", "AI": {"tldr": "FSP-Diff\uff1a\u4e00\u79cd\u7528\u4e8e\u8d85\u4f4e\u5242\u91cf\u80fd\u8c31CT\u91cd\u5efa\u7684\u5168\u80fd\u8c31\u5148\u9a8c\u589e\u5f3a\u53cc\u57df\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u4e92\u8865\u7279\u5f81\u6784\u5efa\u3001\u5168\u80fd\u8c31\u5148\u9a8c\u96c6\u6210\u548c\u9ad8\u6548\u6f5c\u5728\u6269\u6563\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u8d85\u4f4e\u5242\u91cf\u6761\u4ef6\u4e0b\uff0c\u80fd\u8c31CT\u7684\u80fd\u91cf\u7279\u5b9a\u6295\u5f71\u4fe1\u566a\u6bd4\u6025\u5267\u4e0b\u964d\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u51fa\u73b0\u4e25\u91cd\u4f2a\u5f71\u548c\u7ed3\u6784\u7ec6\u8282\u4e22\u5931\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u91cd\u5efa\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faFSP-Diff\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7b56\u7565\uff1a1\uff09\u4e92\u8865\u7279\u5f81\u6784\u5efa\uff1a\u6574\u5408\u76f4\u63a5\u56fe\u50cf\u91cd\u5efa\u4e0e\u6295\u5f71\u57df\u53bb\u566a\u7ed3\u679c\uff1b2\uff09\u5168\u80fd\u8c31\u5148\u9a8c\u96c6\u6210\uff1a\u878d\u5408\u591a\u80fd\u91cf\u6295\u5f71\u4e3a\u9ad8\u4fe1\u566a\u6bd4\u5168\u80fd\u8c31\u56fe\u50cf\u4f5c\u4e3a\u7edf\u4e00\u7ed3\u6784\u53c2\u8003\uff1b3\uff09\u9ad8\u6548\u6f5c\u5728\u6269\u6563\u5408\u6210\uff1a\u5c06\u591a\u8def\u5f84\u7279\u5f81\u5d4c\u5165\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u4f4e\u7ef4\u6d41\u5f62\u4e2d\u5b9e\u73b0\u4ea4\u4e92\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFSP-Diff\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FSP-Diff\u6846\u67b6\u4e3a\u4e34\u5e8a\u53ef\u884c\u7684\u8d85\u4f4e\u5242\u91cf\u80fd\u8c31CT\u6210\u50cf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u53cc\u57df\u7279\u5f81\u6574\u5408\u548c\u6f5c\u5728\u6269\u6563\u5408\u6210\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002"}}
{"id": "2602.07980", "pdf": "https://arxiv.org/pdf/2602.07980", "abs": "https://arxiv.org/abs/2602.07980", "authors": ["Junlin Wang", "Jiancheng Fang", "Peng Peng", "Shaoyu Wang", "Qiegen Liu"], "title": "Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51faCSDN\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u5148\u9a8c\u548c\u534f\u540c\u6269\u6563\u7b56\u7565\u89e3\u51b3\u8d85\u7a00\u758f\u89d2\u91c7\u6837CBCT\u91cd\u5efa\u4e2d\u7684\u4f2a\u5f71\u548c\u5207\u7247\u4e0d\u4e00\u81f4\u95ee\u9898", "motivation": "CBCT\u4e34\u5e8a\u5e94\u7528\u53d7\u9650\u4e8e\u8f90\u5c04\u5242\u91cf\u4e0e\u56fe\u50cf\u8d28\u91cf\u7684\u6743\u8861\u3002\u8d85\u7a00\u758f\u89d2\u91c7\u6837\u867d\u964d\u4f4e\u5242\u91cf\uff0c\u4f46\u5bfc\u81f4\u4e25\u91cd\u6b20\u91c7\u6837\u4f2a\u5f71\u548c\u5207\u7247\u95f4\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u89d2\u5ea6\u8fde\u7eed\u6027\u4e0e\u7a7a\u95f4\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faCSDN\u65b9\u6cd5\uff1a1) \u5f15\u5165\u795e\u7ecf\u5148\u9a8c\u4f5c\u4e3a\u7ed3\u6784\u57fa\u7840\uff0c\u7f16\u7801\u8fde\u7eed3D\u8870\u51cf\u8868\u793a\uff0c\u4ece\u8d85\u7a00\u758f\u6d4b\u91cf\u5408\u6210\u7269\u7406\u4e00\u81f4\u5bc6\u96c6\u6295\u5f71\uff1b2) \u57fa\u4e8e\u795e\u7ecf\u5148\u9a8c\u521d\u59cb\u5316\uff0c\u5f00\u53d1\u534f\u540c\u6269\u6563\u7b56\u7565\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u7cbe\u70bc\u8def\u5f84\uff1a\u6b63\u5f26\u56fe\u7cbe\u70bc\u6269\u6563(Sino-RD)\u6062\u590d\u89d2\u5ea6\u8fde\u7eed\u6027\uff0c\u6570\u5b57\u653e\u5c04\u6444\u5f71\u7cbe\u70bc\u6269\u6563(DR-RD)\u4ece\u6295\u5f71\u56fe\u50cf\u89d2\u5ea6\u5f3a\u5236\u5207\u7247\u4e00\u81f4\u6027\uff1b3) \u901a\u8fc7\u53cc\u6295\u5f71\u91cd\u5efa\u878d\u5408(DPRF)\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u4e24\u4e2a\u6269\u6563\u8def\u5f84\u8f93\u51fa\uff0c\u5b9e\u73b0\u8fde\u8d2f\u4f53\u79ef\u91cd\u5efa\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCSDN\u5728\u8d85\u7a00\u758f\u89d2\u6761\u4ef6\u4e0b\u6709\u6548\u6291\u5236\u4f2a\u5f71\u5e76\u6062\u590d\u7cbe\u7ec6\u7eb9\u7406\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u3002", "conclusion": "CSDN\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u5148\u9a8c\u548c\u534f\u540c\u6269\u6563\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8d85\u7a00\u758f\u89d2\u91c7\u6837CBCT\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u8fde\u8d2f\u7684\u4f53\u79ef\u91cd\u5efa\u3002"}}
{"id": "2602.07986", "pdf": "https://arxiv.org/pdf/2602.07986", "abs": "https://arxiv.org/abs/2602.07986", "authors": ["Md. Tarek Hasan", "Sanjay Saha", "Shaojing Fan", "Swakkhar Shatabda", "Terence Sim"], "title": "Deepfake Synthesis vs. Detection: An Uneven Contest", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.", "AI": {"tldr": "\u6700\u65b0\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u73b0\u4ee3\u5408\u6210\u6280\u672f\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u65f6\u8868\u73b0\u663e\u8457\u4e0d\u4f73\uff0c\u751a\u81f3\u4eba\u7c7b\u4e5f\u96be\u4ee5\u5206\u8fa8\u6700\u9ad8\u8d28\u91cf\u7684\u4f2a\u9020\u89c6\u9891\uff0c\u51f8\u663e\u68c0\u6d4b\u6280\u672f\u843d\u540e\u4e8e\u751f\u6210\u6280\u672f\u53d1\u5c55\u7684\u4e25\u5cfb\u73b0\u5b9e\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u3001NeRF\u548c\u589e\u5f3aGAN\u7b49\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5408\u6210\u5a92\u4f53\u7684\u771f\u5b9e\u6027\u548c\u53ef\u53ca\u6027\u5927\u5e45\u63d0\u5347\uff0c\u800c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u80fd\u5426\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u5148\u8fdb\u751f\u6210\u6280\u672f\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5bf9\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\u8fdb\u884c\u5168\u9762\u5b9e\u8bc1\u5206\u6790\uff0c\u5305\u62ec\u4f7f\u7528\u73b0\u4ee3\u5408\u6210\u65b9\u6cd5\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u8fdb\u884c\u4eba\u7c7b\u8bc4\u4f30\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u6bd4\u8f83\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u73b0\u4ee3\u5408\u6210\u6280\u672f\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u65f6\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u4eba\u7c7b\u53c2\u4e0e\u8005\u5728\u9762\u5bf9\u6700\u9ad8\u8d28\u91cf\u6df1\u5ea6\u4f2a\u9020\u65f6\u4e5f\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\uff0c\u68c0\u6d4b\u4e0e\u751f\u6210\u6280\u672f\u4e4b\u95f4\u5b58\u5728\u4e25\u91cd\u5dee\u8ddd\u3002", "conclusion": "\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u5df2\u843d\u540e\u4e8e\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u8feb\u5207\u9700\u8981\u6301\u7eed\u6539\u8fdb\u68c0\u6d4b\u6a21\u578b\u4ee5\u8ddf\u4e0a\u5feb\u901f\u6f14\u8fdb\u7684\u751f\u6210\u6280\u672f\uff0c\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u9700\u8981\u52a0\u5f3a\u6295\u5165\u3002"}}
{"id": "2602.07993", "pdf": "https://arxiv.org/pdf/2602.07993", "abs": "https://arxiv.org/abs/2602.07993", "authors": ["Xuehai Bai", "Xiaoling Gu", "Akide Liu", "Hangjie Yuan", "YiFan Zhang", "Jack Ma"], "title": "MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI2026", "summary": "Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.", "AI": {"tldr": "\u63d0\u51faMCIE-E1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u80cc\u666f\u4e00\u81f4\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u89e3\u51b3\u590d\u6742\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u4e0d\u8db3\u548c\u80cc\u666f\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5728CIE-Bench\u57fa\u51c6\u4e0a\u76f8\u6bd4SOTA\u65b9\u6cd5\u63d0\u534723.96%\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5c40\u9650\u4e8e\u7b80\u5355\u64cd\u4f5c\uff0c\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u5e94\u7528\u4e2d\u9700\u8981\u7684\u590d\u6742\u7ec4\u5408\u6307\u4ee4\u3002\u4e3b\u8981\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6307\u4ee4\u9075\u5faa\u4e0d\u8db3\u548c\u80cc\u666f\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faMCIE-E1\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u7a7a\u95f4\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u7a7a\u95f4\u5f15\u5bfc\u663e\u5f0f\u5bf9\u9f50\u8bed\u4e49\u6307\u4ee4\u4e0e\u7a7a\u95f4\u533a\u57df\uff1b2\uff09\u80cc\u666f\u4e00\u81f4\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4fdd\u6301\u672a\u7f16\u8f91\u533a\u57df\u7279\u5f81\u4ee5\u7ef4\u6301\u80cc\u666f\u4e00\u81f4\u6027\u3002\u6784\u5efa\u4e13\u95e8\u7684\u6570\u636e\u7ba1\u9053\uff0c\u7ed3\u5408\u5f3a\u5927\u7684MLLM\u7ec6\u7c92\u5ea6\u81ea\u52a8\u8fc7\u6ee4\u548c\u4e25\u683c\u4eba\u5de5\u9a8c\u8bc1\u6765\u89e3\u51b3\u590d\u6742\u6307\u4ee4\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u5728\u63d0\u51fa\u7684CIE-Bench\u57fa\u51c6\u4e0a\uff0cMCIE-E1\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u63d0\u534723.96%\u3002", "conclusion": "MCIE-E1\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u3001\u6570\u636e\u7ba1\u9053\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2602.08006", "pdf": "https://arxiv.org/pdf/2602.08006", "abs": "https://arxiv.org/abs/2602.08006", "authors": ["Riya Mohan", "Juana Valeria Hurtado", "Rohit Mohan", "Abhinav Valada"], "title": "ForecastOcc: Vision-based Semantic Occupancy Forecasting", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.", "AI": {"tldr": "ForecastOcc\uff1a\u9996\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u76f8\u673a\u56fe\u50cf\u8054\u5408\u9884\u6d4b\u672a\u6765\u5360\u636e\u72b6\u6001\u548c\u8bed\u4e49\u7c7b\u522b\uff0c\u65e0\u9700\u5916\u90e8\u5730\u56fe\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u548c\u52a8\u6001\u7269\u4f53\u7684\u8fd0\u52a8\u7c7b\u522b\uff0c\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\uff1b\u800c\u6700\u8fd1\u7684\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u72ec\u7f51\u7edc\u83b7\u53d6\u7684\u8fc7\u53bb\u5360\u636e\u9884\u6d4b\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u4e14\u65e0\u6cd5\u76f4\u63a5\u4ece\u56fe\u50cf\u5b66\u4e60\u65f6\u7a7a\u7279\u5f81\u3002", "method": "\u63d0\u51faForecastOcc\u6846\u67b6\uff0c\u5305\u542b\u65f6\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u9884\u6d4b\u6a21\u5757\u30012D\u52303D\u89c6\u56fe\u53d8\u6362\u5668\u30013D\u7f16\u7801\u5668\u7528\u4e8e\u5360\u636e\u9884\u6d4b\uff0c\u4ee5\u53ca\u8bed\u4e49\u5360\u636e\u5934\u7528\u4e8e\u8de8\u591a\u4e2a\u65f6\u95f4\u6b65\u7684\u4f53\u7d20\u7ea7\u9884\u6d4b\u3002", "result": "\u5728Occ3D-nuScenes\u591a\u89c6\u56fe\u9884\u6d4b\u548cSemanticKITTI\u5355\u76ee\u9884\u6d4b\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u5355\u76ee\u9884\u6d4b\u57fa\u51c6\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u4e30\u5bcc\u3001\u672a\u6765\u611f\u77e5\u7684\u9884\u6d4b\u3002", "conclusion": "ForecastOcc\u662f\u9996\u4e2a\u76f4\u63a5\u4ece\u56fe\u50cf\u8054\u5408\u9884\u6d4b\u672a\u6765\u5360\u636e\u72b6\u6001\u548c\u8bed\u4e49\u7c7b\u522b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6355\u6349\u5bf9\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u7684\u573a\u666f\u52a8\u6001\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.08020", "pdf": "https://arxiv.org/pdf/2602.08020", "abs": "https://arxiv.org/abs/2602.08020", "authors": ["Minghai Chen", "Mingyuan Liu", "Yuxiang Huan"], "title": "PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based garment draping has emerged as a promising alternative to traditional Physics-Based Simulation (PBS), yet robust collision handling remains a critical bottleneck. Most existing methods enforce physical validity through soft penalties, creating an intrinsic trade-off between geometric feasibility and physical plausibility: penalizing collisions often distorts mesh structure, while preserving shape leads to interpenetration. To resolve this conflict, we present PhysDrape, a hybrid neural-physical solver for physically realistic garment draping driven by explicit forces and constraints. Unlike soft-constrained frameworks, PhysDrape integrates neural inference with explicit geometric solvers in a fully differentiable pipeline. Specifically, we propose a Physics-Informed Graph Neural Network conditioned on a physics-enriched graph -- encoding material parameters and body proximity -- to predict residual displacements. Crucially, we integrate a differentiable two-stage solver: first, a learnable Force Solver iteratively resolves unbalanced forces derived from the Saint Venant-Kirchhoff (StVK) model to ensure quasi-static equilibrium; second, a Differentiable Projection strictly enforces collision constraints against the body surface. This differentiable design guarantees physical validity through explicit constraints, while enabling end-to-end learning to optimize the network for physically consistent predictions. Extensive experiments demonstrate that PhysDrape achieves state-of-the-art performance, ensuring negligible interpenetration with significantly lower strain energy compared to existing baselines, achieving superior physical fidelity and robustness in real-time.", "AI": {"tldr": "PhysDrape\uff1a\u4e00\u79cd\u6df7\u5408\u795e\u7ecf\u7269\u7406\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u663e\u5f0f\u529b\u548c\u7ea6\u675f\u5b9e\u73b0\u7269\u7406\u771f\u5b9e\u7684\u670d\u88c5\u60ac\u5782\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u53ef\u884c\u6027\u4e0e\u7269\u7406\u5408\u7406\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u670d\u88c5\u60ac\u5782\u65b9\u6cd5\u5df2\u6210\u4e3a\u4f20\u7edf\u7269\u7406\u6a21\u62df\u7684\u6709\u524d\u666f\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7a33\u5065\u7684\u78b0\u649e\u5904\u7406\u4ecd\u662f\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u8f6f\u60e9\u7f5a\u6765\u4fdd\u8bc1\u7269\u7406\u6709\u6548\u6027\uff0c\u5bfc\u81f4\u51e0\u4f55\u53ef\u884c\u6027\u4e0e\u7269\u7406\u5408\u7406\u6027\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\uff1a\u60e9\u7f5a\u78b0\u649e\u4f1a\u626d\u66f2\u7f51\u683c\u7ed3\u6784\uff0c\u800c\u4fdd\u6301\u5f62\u72b6\u5219\u4f1a\u5bfc\u81f4\u7a7f\u900f\u3002", "method": "\u63d0\u51faPhysDrape\u6df7\u5408\u795e\u7ecf\u7269\u7406\u6c42\u89e3\u5668\uff0c\u5c06\u795e\u7ecf\u63a8\u7406\u4e0e\u663e\u5f0f\u51e0\u4f55\u6c42\u89e3\u5668\u96c6\u6210\u5728\u5b8c\u5168\u53ef\u5fae\u7684\u6d41\u7a0b\u4e2d\u3002\u5305\u62ec\uff1a1\uff09\u57fa\u4e8e\u7269\u7406\u589e\u5f3a\u56fe\u7684\u6761\u4ef6\u7269\u7406\u4fe1\u606f\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6b8b\u5dee\u4f4d\u79fb\uff1b2\uff09\u53ef\u5b66\u4e60\u529b\u6c42\u89e3\u5668\u8fed\u4ee3\u89e3\u51b3\u6765\u81eaStVK\u6a21\u578b\u7684\u4e0d\u5e73\u8861\u529b\uff1b3\uff09\u53ef\u5fae\u6295\u5f71\u4e25\u683c\u5f3a\u5236\u6267\u884c\u4e0e\u8eab\u4f53\u8868\u9762\u7684\u78b0\u649e\u7ea6\u675f\u3002", "result": "PhysDrape\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u786e\u4fdd\u53ef\u5ffd\u7565\u7684\u7a7f\u900f\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\u5177\u6709\u663e\u8457\u66f4\u4f4e\u7684\u5e94\u53d8\u80fd\uff0c\u5728\u5b9e\u65f6\u6027\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u7269\u7406\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PhysDrape\u901a\u8fc7\u53ef\u5fae\u7684\u663e\u5f0f\u7ea6\u675f\u8bbe\u8ba1\u89e3\u51b3\u4e86\u670d\u88c5\u60ac\u5782\u4e2d\u51e0\u4f55\u53ef\u884c\u6027\u4e0e\u7269\u7406\u5408\u7406\u6027\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u4e00\u81f4\u7684\u9884\u6d4b\uff0c\u4e3a\u7269\u7406\u771f\u5b9e\u7684\u670d\u88c5\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08024", "pdf": "https://arxiv.org/pdf/2602.08024", "abs": "https://arxiv.org/abs/2602.08024", "authors": ["Ziyang Fan", "Keyu Chen", "Ruilong Xing", "Yulin Li", "Li Jiang", "Zhuotao Tian"], "title": "FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ICLR 2026 (Oral)", "summary": "Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.", "AI": {"tldr": "FlashVID\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u4e0e\u591a\u6837\u6027\u4ee4\u724c\u9009\u62e9\u548c\u6811\u72b6\u65f6\u7a7a\u4ee4\u724c\u5408\u5e76\u6280\u672f\uff0c\u4ec5\u4fdd\u755910%\u89c6\u89c9\u4ee4\u724c\u5373\u53ef\u4fdd\u630199.1%\u6027\u80fd\uff0c\u5b9e\u73b010\u500d\u89c6\u9891\u5e27\u6269\u5c55\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5904\u7406\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u52a0\u901f\u6846\u67b6\u72ec\u7acb\u538b\u7f29\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\uff0c\u5ffd\u7565\u4e86\u65f6\u7a7a\u5173\u7cfb\uff0c\u5bfc\u81f4\u6b21\u4f18\u538b\u7f29\u6548\u679c\u3002\u89c6\u9891\u7684\u52a8\u6001\u7279\u6027\u4f7f\u5f97\u89c6\u89c9\u7279\u5f81\u5728\u65f6\u7a7a\u7ef4\u5ea6\u9ad8\u5ea6\u76f8\u5173\u4e14\u4e0d\u65ad\u53d8\u5316\u3002", "method": "FlashVID\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u6ce8\u610f\u529b\u4e0e\u591a\u6837\u6027\u4ee4\u724c\u9009\u62e9(ADTS)\uff1a\u9009\u62e9\u6700\u5177\u4ee3\u8868\u6027\u7684\u4ee4\u724c\u8fdb\u884c\u57fa\u7840\u89c6\u9891\u8868\u793a\uff1b2) \u6811\u72b6\u65f6\u7a7a\u4ee4\u724c\u5408\u5e76(TSTM)\uff1a\u8fdb\u884c\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u5197\u4f59\u6d88\u9664\u3002\u8be5\u6846\u67b6\u65e0\u9700\u8bad\u7ec3\uff0c\u5373\u63d2\u5373\u7528\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027VLLM\u548c\u4e94\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002\u4ec5\u4fdd\u755910%\u89c6\u89c9\u4ee4\u724c\u5373\u53ef\u4fdd\u6301LLaVA-OneVision 99.1%\u7684\u6027\u80fd\u3002\u4f7fQwen2.5-VL\u7684\u89c6\u9891\u5e27\u8f93\u5165\u589e\u52a010\u500d\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u76f8\u5bf9\u63d0\u53478.6%\u3002", "conclusion": "FlashVID\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528\u65f6\u7a7a\u5173\u7cfb\u8fdb\u884c\u4ee4\u724c\u538b\u7f29\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u957f\u89c6\u9891\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08025", "pdf": "https://arxiv.org/pdf/2602.08025", "abs": "https://arxiv.org/abs/2602.08025", "authors": ["Yixuan Ye", "Xuanyu Lu", "Yuxin Jiang", "Yuchao Gu", "Rui Zhao", "Qiwei Liang", "Jiachun Pan", "Fengda Zhang", "Weijia Wu", "Alex Jinpeng Wang"], "title": "MIND: Benchmarking Memory Consistency and Action Control in World Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/", "AI": {"tldr": "MIND\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u80fd\u529b\u7684\u5f00\u653e\u57df\u95ed\u73af\u91cd\u8bbf\u57fa\u51c6\uff0c\u5305\u542b250\u4e2a\u9ad8\u8d28\u91cf\u89c6\u9891\u548c\u591a\u6837\u5316\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86MIND-World\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u5728\u52a8\u6001\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u57fa\u672c\u80fd\u529b\uff0c\u7279\u522b\u662f\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u65b9\u9762\u3002", "method": "\u6784\u5efa\u5305\u542b250\u4e2a1080p\u9ad8\u6e05\u89c6\u9891\u7684MIND\u57fa\u51c6\uff0c\u6db5\u76d6\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\uff0c\u8bbe\u8ba1\u591a\u6837\u5316\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u5f00\u53d1\u9ad8\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u91cf\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86MIND\u57fa\u51c6\u7684\u5b8c\u6574\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u4e16\u754c\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\uff1a\u96be\u4ee5\u4fdd\u6301\u957f\u671f\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u8de8\u52a8\u4f5c\u7a7a\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MIND\u4e3a\u4e16\u754c\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u5f00\u653e\u57df\u95ed\u73af\u91cd\u8bbf\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86MIND-World\u4f5c\u4e3a\u6027\u80fd\u57fa\u51c6\u3002"}}
{"id": "2602.08046", "pdf": "https://arxiv.org/pdf/2602.08046", "abs": "https://arxiv.org/abs/2602.08046", "authors": ["Yahia Hamdi", "Nicolas Andrialovanirina", "K\u00e9lig Mah\u00e9", "Emilie Poisson Caillault"], "title": "Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects", "categories": ["cs.CV"], "comment": "11", "summary": "The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.", "AI": {"tldr": "\u63d0\u51faMoE-DCGAN\u67b6\u6784\uff0c\u5c06\u6df1\u5ea63D\u5377\u79efGAN\u4e0e\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\u7ed3\u5408\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf3D\u6a21\u578b\u751f\u6210\u548c\u6b8b\u7f3a\u7269\u4f53\u91cd\u5efa\uff0c\u901a\u8fc7\u52a8\u6001\u5bb9\u91cf\u7ea6\u675f\u673a\u5236\u5e73\u8861\u4e13\u4e1a\u5316\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edfGAN\u57283D\u7269\u4f53\u751f\u6210\u548c\u8865\u5168\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u96be\u4ee5\u6355\u6349\u590d\u6742\u591a\u6837\u7684\u6570\u636e\u5206\u5e03\uff0c\u7279\u522b\u662f\u5728\u8f93\u5165\u4e0d\u5b8c\u6574\u6216\u7f3a\u5931\u533a\u57df\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\u3002\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u5f02\u8d28\u7ed3\u6784\u5316\u6570\u636e\u5efa\u6a21\u56f0\u96be\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faMoE-DCGAN\u67b6\u6784\uff1a1) \u96c6\u6210\u6df1\u5ea63D\u5377\u79efGAN\u4e0e\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff1b2) \u4f7f\u7528\u591a\u4e2a\u751f\u6210\u5668\uff0c\u6bcf\u4e2a\u4e13\u95e8\u6355\u6349\u6570\u636e\u96c6\u4e2d\u7684\u4e0d\u540c\u6a21\u6001\uff1b3) \u5f15\u5165\u65e0\u8f85\u52a9\u635f\u5931\u7684\u52a8\u6001\u5bb9\u91cf\u7ea6\u675f\u673a\u5236\uff0c\u6307\u5bfc\u5206\u7c7b\u751f\u6210\u5668\u9009\u62e9\uff0c\u5e73\u8861\u4e13\u4e1a\u5316\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u5927\u5c0f\u7f3a\u5931\u533a\u57df\u7684\u5f62\u72b6\u751f\u6210\u548c\u8865\u5168\u80fd\u529b\uff0c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u6bd4\u8f83\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u5747\u8bc1\u5b9eMoE-DCGAN\u5728\u5904\u7406\u590d\u67423D\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "MoE-DCGAN\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e863D\u7269\u4f53\u751f\u6210\u548c\u8865\u5168\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548c\u52a8\u6001\u5bb9\u91cf\u7ea6\u675f\u673a\u5236\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u4e3a\u590d\u67423D\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08047", "pdf": "https://arxiv.org/pdf/2602.08047", "abs": "https://arxiv.org/abs/2602.08047", "authors": ["Jiahong Fu", "Qi Xie", "Deyu Meng", "Zongben Xu"], "title": "Vanilla Group Equivariant Vision Transformer: Simple and Effective", "categories": ["cs.CV"], "comment": null, "summary": "Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7cfb\u7edf\u5316\u6784\u5efa\u7b49\u53d8Vision Transformer\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f7fViT\u5173\u952e\u7ec4\u4ef6\uff08\u5305\u62ecpatch embedding\u3001self-attention\u3001\u4f4d\u7f6e\u7f16\u7801\u548c\u91c7\u6837\u6a21\u5757\uff09\u7b49\u53d8\uff0c\u5b9e\u73b0\u7406\u8bba\u4fdd\u8bc1\u7684\u7b49\u53d8\u6027\uff0c\u53ef\u65e0\u7f1d\u66ff\u6362\u73b0\u6709ViT\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u7b49\u53d8ViT\u96be\u4ee5\u5e73\u8861\u6027\u80fd\u4e0e\u7b49\u53d8\u6027\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u96be\u4ee5\u5728ViT\u7684\u591a\u6837\u5316\u6a21\u5757\u4e2d\u5b9e\u73b0\u6574\u4f53\u7b49\u53d8\u4fee\u6539\uff0c\u7279\u522b\u662f\u534f\u8c03\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0epatch embedding\u7684\u7b49\u53d8\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u4f7fViT\u5173\u952e\u7ec4\u4ef6\u7b49\u53d8\uff1a\u5305\u62ecpatch embedding\u3001self-attention\u3001\u4f4d\u7f6e\u7f16\u7801\u548cDown/Up-Sampling\u6a21\u5757\uff0c\u6784\u5efa\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7b49\u53d8\u6027\u7684ViT\u67b6\u6784\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u66ff\u6362\u65b9\u6848\uff0c\u751a\u81f3\u53ef\u6269\u5c55\u5230Swin Transformers\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b49\u53d8ViT\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u624e\u5b9e\u4e14\u5b9e\u9645\u901a\u7528\u7684\u7b49\u53d8ViT\u6784\u5efa\u65b9\u6cd5\uff0c\u80fd\u591f\u65e0\u7f1d\u66ff\u6362\u73b0\u6709\u67b6\u6784\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u7684\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2602.08057", "pdf": "https://arxiv.org/pdf/2602.08057", "abs": "https://arxiv.org/abs/2602.08057", "authors": ["Yufei Wang", "Haixu Liu", "Tianxiang Xu", "Chuancheng Shi", "Hongsheng Xing"], "title": "Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "To tackle the automatic recognition of \"concealed emotions\" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an \"MLP-ified\" key-point backbone can match - or even surpass - GCN-based counterparts in this task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u591a\u6a21\u6001\u5f31\u76d1\u7763\u6846\u67b6\u8bc6\u522b\u89c6\u9891\u4e2d\u7684\"\u9690\u85cf\u60c5\u7eea\"\uff0c\u5728iMiGUE\u7f51\u7403\u91c7\u8bbf\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\uff0c\u51c6\u786e\u7387\u4ece0.6\u63d0\u5347\u81f30.69+\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7b80\u5316MLP\u9aa8\u5e72\u7f51\u7edc\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u4e2d\"\u9690\u85cf\u60c5\u7eea\"\u7684\u81ea\u52a8\u8bc6\u522b\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728iMiGUE\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u4e0d\u8db30.6\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u548c\u5f31\u76d1\u7763\u7b56\u7565\u3002", "method": "1) YOLO 11x\u68c0\u6d4b\u88c1\u526a\u4eba\u50cf\uff0cDINOv2-Base\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff1b2) Gemini 2.5 Pro\u901a\u8fc7CoT+Reflection\u63d0\u793a\u751f\u6210\u4f2a\u6807\u7b7e\u548c\u63a8\u7406\u6587\u672c\u4f5c\u4e3a\u5f31\u76d1\u7763\uff1b3) OpenPose\u63d0\u53d6137\u7ef4\u5173\u952e\u70b9\u5e8f\u5217\uff0c\u7528MLP\u66ff\u4ee3GCN\u5efa\u6a21\u65f6\u7a7a\u5173\u7cfb\uff1b4) \u8d85\u957f\u5e8f\u5217Transformer\u7f16\u7801\u56fe\u50cf\u548c\u5173\u952e\u70b9\u5e8f\u5217\uff0c\u4e0eBERT\u7f16\u7801\u7684\u6587\u672c\u7279\u5f81\u62fc\u63a5\uff1b5) \u5355\u6a21\u6001\u9884\u8bad\u7ec3\u540e\u8054\u5408\u5fae\u8c03\uff0c\u4f2a\u6807\u7b7e\u6837\u672c\u878d\u5165\u8bad\u7ec3\u96c6\u3002", "result": "\u5728\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u4ece\u5148\u524d\u5de5\u4f5c\u7684\u4e0d\u8db30.6\u63d0\u5347\u81f3\u8d85\u8fc70.69\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u516c\u5f00\u57fa\u51c6\uff1b\u9a8c\u8bc1\u4e86\"MLP\u5316\"\u5173\u952e\u70b9\u9aa8\u5e72\u7f51\u7edc\u53ef\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u57fa\u4e8eGCN\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u5f31\u76d1\u7763\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u9690\u85cf\u60c5\u7eea\u8bc6\u522b\u6027\u80fd\uff0c\u7b80\u5316MLP\u9aa8\u5e72\u7f51\u7edc\u5728\u8be5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u57fa\u51c6\u3002"}}
{"id": "2602.08058", "pdf": "https://arxiv.org/pdf/2602.08058", "abs": "https://arxiv.org/abs/2602.08058", "authors": ["Xihang Yu", "Rajat Talak", "Lorenzo Shaikewitz", "Luca Carlone"], "title": "Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling", "categories": ["cs.CV", "cs.AI", "cs.RO", "eess.SY"], "comment": "15 pages", "summary": "In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.", "AI": {"tldr": "Picasso\u662f\u4e00\u4e2a\u7269\u7406\u7ea6\u675f\u7684\u591a\u7269\u4f53\u573a\u666f\u91cd\u5efa\u7cfb\u7edf\uff0c\u901a\u8fc7\u8003\u8651\u51e0\u4f55\u3001\u975e\u7a7f\u900f\u548c\u7269\u7406\u7ea6\u675f\u6765\u6784\u5efa\u7269\u7406\u4e0a\u5408\u7406\u7684\u573a\u666f\u91cd\u5efa\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u542b\u63a5\u89e6\u4e30\u5bcc\u573a\u666f\u7684\u6570\u636e\u96c6\u548c\u7269\u7406\u5408\u7406\u6027\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5728\u906e\u6321\u548c\u6d4b\u91cf\u566a\u58f0\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u51e0\u4f55\u51c6\u786e\u4f46\u7269\u7406\u4e0d\u5408\u7406\u7684\u573a\u666f\u91cd\u5efa\u4f1a\u5bfc\u81f4\u7269\u4f53\u7a7f\u900f\u6216\u4e0d\u7a33\u5b9a\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u9884\u6d4b\u573a\u666f\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5f71\u54cd\u57fa\u4e8e\u4eff\u771f\u7684\u63a5\u89e6\u4e30\u5bcc\u884c\u4e3a\u7684\u89c4\u5212\u548c\u63a7\u5236\u3002", "method": "\u63d0\u51faPicasso\u7269\u7406\u7ea6\u675f\u91cd\u5efa\u7ba1\u9053\uff0c\u4f7f\u7528\u5feb\u901f\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5\u8003\u8651\u591a\u7269\u4f53\u4ea4\u4e92\uff0c\u5229\u7528\u63a8\u65ad\u7684\u7269\u4f53\u63a5\u89e6\u56fe\u6307\u5bfc\u91c7\u6837\uff0c\u786e\u4fdd\u51e0\u4f55\u3001\u975e\u7a7f\u900f\u548c\u7269\u7406\u7ea6\u675f\u7684\u6ee1\u8db3\u3002", "result": "\u5728\u65b0\u5efa\u7684Picasso\u6570\u636e\u96c6\u548cYCB-V\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aPicasso\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u63d0\u4f9b\u65e2\u7269\u7406\u5408\u7406\u53c8\u66f4\u7b26\u5408\u4eba\u7c7b\u76f4\u89c9\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "\u7269\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u4f30\u8ba1\u9700\u8981\u5bf9\u573a\u666f\u8fdb\u884c\u6574\u4f53\u63a8\u7406\uff08\u800c\u975e\u5b64\u7acb\u8003\u8651\u6bcf\u4e2a\u7269\u4f53\uff09\uff0c\u8003\u8651\u7269\u4f53\u4ea4\u4e92\u548c\u7269\u7406\u5408\u7406\u6027\uff0cPicasso\u7cfb\u7edf\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u91cd\u5efa\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002"}}
{"id": "2602.08059", "pdf": "https://arxiv.org/pdf/2602.08059", "abs": "https://arxiv.org/abs/2602.08059", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu"], "title": "DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.", "AI": {"tldr": "DICE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u827a\u672f\u5bb6\u98ce\u683c\u64e6\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b50\u7a7a\u95f4\u5206\u89e3\u5b9e\u73b0\u98ce\u683c\u4e0e\u5185\u5bb9\u7684\u89e3\u8026\uff0c\u6709\u6548\u9632\u6b62\u6269\u6563\u6a21\u578b\u4e2d\u7684\u98ce\u683c\u6a21\u4eff\u4fb5\u6743\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u666e\u53ca\u4f7f\u5f97\u98ce\u683c\u6a21\u4eff\u53d8\u5f97\u5bb9\u6613\uff0c\u5f15\u53d1\u4e86\u7248\u6743\u548c\u77e5\u8bc6\u4ea7\u6743\u98ce\u9669\u3002\u73b0\u6709\u5bf9\u7b56\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u6743\u91cd\u7f16\u8f91\uff0c\u8981\u4e48\u4f9d\u8d56\u660e\u786e\u6307\u5b9a\u7684\u7f16\u8f91\u98ce\u683c\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faDICE\u6846\u67b6\uff1a1) \u6784\u5efa\u5bf9\u6bd4\u4e09\u5143\u7ec4\uff0c\u8ba9\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u533a\u5206\u98ce\u683c\u4e0e\u975e\u98ce\u683c\u7279\u5f81\uff1b2) \u5c06\u89e3\u8026\u8fc7\u7a0b\u5f62\u5f0f\u5316\u4e3a\u53ef\u89e3\u7684\u5e7f\u4e49\u7279\u5f81\u503c\u95ee\u9898\uff0c\u7cbe\u786e\u8bc6\u522b\u98ce\u683c\u5b50\u7a7a\u95f4\uff1b3) \u5f15\u5165\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u89e3\u8026\u7f16\u8f91\u7b56\u7565\uff0c\u52a8\u6001\u8bc4\u4f30\u6bcf\u4e2atoken\u7684\u98ce\u683c\u6d53\u5ea6\uff0c\u5bf9QKV\u5411\u91cf\u8fdb\u884c\u5dee\u5f02\u5316\u6291\u5236\u548c\u5185\u5bb9\u589e\u5f3a\u3002", "result": "DICE\u5728\u98ce\u683c\u64e6\u9664\u7684\u5f7b\u5e95\u6027\u548c\u5185\u5bb9\u5b8c\u6574\u6027\u4fdd\u62a4\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\uff0c\u4ec5\u589e\u52a03\u79d2\u7684\u89e3\u8026\u5f00\u9500\uff0c\u4e3a\u904f\u5236\u98ce\u683c\u6a21\u4eff\u63d0\u4f9b\u4e86\u5b9e\u7528\u9ad8\u6548\u7684\u6280\u672f\u3002", "conclusion": "DICE\u901a\u8fc7\u8bad\u7ec3\u81ea\u7531\u7684\u98ce\u683c\u51c0\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u827a\u672f\u5bb6\u98ce\u683c\u6a21\u4eff\u7684\u7248\u6743\u4fdd\u62a4\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.08068", "pdf": "https://arxiv.org/pdf/2602.08068", "abs": "https://arxiv.org/abs/2602.08068", "authors": ["Chunyang Li", "Yuanbo Yang", "Jiahao Shao", "Hongyu Zhou", "Katja Schwarz", "Yiyi Liao"], "title": "ReRoPE: Repurposing RoPE for Relative Camera Control", "categories": ["cs.CV"], "comment": null, "summary": "Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/", "AI": {"tldr": "ReRoPE\uff1a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4fe1\u606f\u6ce8\u5165\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684RoPE\u4f4e\u9891\u5e26\uff0c\u5b9e\u73b0\u7cbe\u786e\u76f8\u673a\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f7f\u7528\u76f8\u5bf9\u4e8e\u56fa\u5b9a\u53c2\u8003\u5e27\u7684\u76f8\u673a\u59ff\u6001\u7f16\u7801\uff0c\u7f3a\u4e4f\u5e73\u79fb\u4e0d\u53d8\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u7d2f\u79ef\u6f02\u79fb\u3002\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u5d4c\u5165\u66f4\u9c81\u68d2\uff0c\u4f46\u96be\u4ee5\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u6216\u6539\u53d8\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u3002", "method": "\u63d0\u51faReRoPE\u6846\u67b6\uff0c\u5229\u7528\u73b0\u6709\u6a21\u578b\u4e2dRotary Positional Embeddings (RoPE)\u672a\u5145\u5206\u5229\u7528\u5176\u5168\u9891\u8c31\u5e26\u5bbd\uff08\u7279\u522b\u662f\u4f4e\u9891\u5206\u91cf\uff09\u7684\u6d1e\u5bdf\uff0c\u5c06\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4fe1\u606f\u65e0\u7f1d\u6ce8\u5165\u8fd9\u4e9b\u672a\u5145\u5206\u5229\u7528\u7684\u9891\u5e26\u3002", "result": "\u5728\u56fe\u50cf\u5230\u89c6\u9891(I2V)\u548c\u89c6\u9891\u5230\u89c6\u9891(V2V)\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cReRoPE\u5728\u76f8\u673a\u63a7\u5236\u7cbe\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bad\u7ec3\u9ad8\u6548\u7684\u7cbe\u786e\u53ef\u63a7\u89c6\u9891\u751f\u6210\u8def\u5f84\u3002", "conclusion": "ReRoPE\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u635f\u5bb3\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u80fd\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u76f8\u5bf9\u76f8\u673a\u4fe1\u606f\u96c6\u6210\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u53ef\u63a7\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2602.08071", "pdf": "https://arxiv.org/pdf/2602.08071", "abs": "https://arxiv.org/abs/2602.08071", "authors": ["Feng Wang", "Sucheng Ren", "Tiezheng Zhang", "Predrag Neskovic", "Anand Bhattad", "Cihang Xie", "Alan Yuille"], "title": "ViT-5: Vision Transformers for The Mid-2020s", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/wangf3014/ViT-5", "summary": "This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.", "AI": {"tldr": "ViT-5\uff1a\u901a\u8fc7\u6574\u5408\u8fc7\u53bb\u4e94\u5e74\u67b6\u6784\u8fdb\u5c55\u5bf9Vision Transformer\u8fdb\u884c\u73b0\u4ee3\u5316\u6539\u9020\uff0c\u5728\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709ViT\u53d8\u4f53", "motivation": "\u867d\u7136Vision Transformer\u5df2\u6210\u4e3a\u89c6\u89c9\u4efb\u52a1\u7684\u4e3b\u6d41\u67b6\u6784\uff0c\u4f46\u8fc7\u53bb\u4e94\u5e74\u7684\u67b6\u6784\u8fdb\u5c55\uff08\u5982\u5f52\u4e00\u5316\u3001\u6fc0\u6d3b\u51fd\u6570\u3001\u4f4d\u7f6e\u7f16\u7801\u7b49\uff09\u5c1a\u672a\u88ab\u7cfb\u7edf\u6574\u5408\u5230\u6807\u51c6ViT\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ec4\u4ef6\u7ea7\u4f18\u5316\u6765\u73b0\u4ee3\u5316ViT\u9aa8\u5e72\u7f51\u7edc\u3002", "method": "\u5728\u4fdd\u6301Attention-FFN\u6838\u5fc3\u7ed3\u6784\u7684\u524d\u63d0\u4e0b\uff0c\u5bf9\u5f52\u4e00\u5316\u3001\u6fc0\u6d3b\u51fd\u6570\u3001\u4f4d\u7f6e\u7f16\u7801\u3001\u95e8\u63a7\u673a\u5236\u548c\u53ef\u5b66\u4e60token\u7b49\u7ec4\u4ef6\u8fdb\u884c\u7cfb\u7edf\u4f18\u5316\uff0c\u5f62\u6210\u65b0\u4e00\u4ee3ViT\u67b6\u6784ViT-5\u3002", "result": "ViT-5\u5728ImageNet-1k\u5206\u7c7b\u4e0a\u8fbe\u523084.2% top-1\u51c6\u786e\u7387\uff08\u8d85\u8d8aDeiT-III\u768483.8%\uff09\uff1b\u5728SiT\u6269\u6563\u6846\u67b6\u4e2d\u5b9e\u73b01.84 FID\uff08\u4f18\u4e8e\u539f\u59cbViT\u76842.06 FID\uff09\uff0c\u540c\u65f6\u5728\u8868\u793a\u5b66\u4e60\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "ViT-5\u901a\u8fc7\u6574\u5408\u73b0\u4ee3\u67b6\u6784\u8bbe\u8ba1\u5b9e\u8df5\uff0c\u4e3a2020\u5e74\u4ee3\u4e2d\u671f\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u5373\u63d2\u5373\u7528\u7684\u5347\u7ea7\u65b9\u6848\uff0c\u5728\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.08099", "pdf": "https://arxiv.org/pdf/2602.08099", "abs": "https://arxiv.org/abs/2602.08099", "authors": ["Issar Tzachor", "Dvir Samuel", "Rami Ben-Ari"], "title": "VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://iyttor.github.io/VidVec/", "summary": "Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c6\u9891\u6587\u672c\u5d4c\u5165\u548c\u68c0\u7d22\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e2d\u95f4\u5c42\u5206\u6790\u548c\u6587\u672c\u5bf9\u9f50\u7b56\u7565\uff0c\u5728\u65e0\u9700\u89c6\u89c9\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u68c0\u7d22\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u5982\u4e13\u95e8\u7684\u89c6\u9891\u57fa\u7840\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u89c6\u9891\u6587\u672c\u5d4c\u5165\u548c\u68c0\u7d22\u65b9\u9762\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u5982\u4f55\u66f4\u597d\u5730\u5229\u7528MLLM\u8fdb\u884c\u89c6\u9891\u6587\u672c\u8868\u793a\u5b66\u4e60\u3002", "method": "1. \u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u5c42\u95f4\u5206\u6790\uff0c\u53d1\u73b0MLLM\u4e2d\u95f4\u5c42\u5df2\u7f16\u7801\u5927\u91cf\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff1b2. \u7ed3\u5408\u4e2d\u95f4\u5c42\u5d4c\u5165\u548c\u6821\u51c6\u7684MLLM\u5934\u90e8\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u7d22\uff1b3. \u63d0\u51fa\u8f7b\u91cf\u7ea7\u6587\u672c\u5bf9\u9f50\u7b56\u7565\uff0c\u5c06\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u6620\u5c04\u5230\u7b80\u77ed\u6458\u8981\uff0c\u5b9e\u73b0\u65e0\u9700\u89c6\u89c9\u76d1\u7763\u7684\u89c6\u9891\u6587\u672c\u5d4c\u5165\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u89c6\u89c9\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5e38\u89c1\u89c6\u9891\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e14\u4f18\u52bf\u663e\u8457\u3002", "conclusion": "MLLM\u7684\u4e2d\u95f4\u5c42\u5305\u542b\u4e30\u5bcc\u7684\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u5c42\u9009\u62e9\u548c\u6587\u672c\u5bf9\u9f50\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u65e0\u9700\u89c6\u89c9\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5f3a\u5927\u7684\u89c6\u9891\u6587\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u4e3aMLLM\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.08112", "pdf": "https://arxiv.org/pdf/2602.08112", "abs": "https://arxiv.org/abs/2602.08112", "authors": ["Sidike Paheding", "Abel Reyes-Angulo", "Leo Thomas Ramos", "Angel D. Sappa", "Rajaneesh A.", "Hiral P. B.", "Sajin Kumar K. S.", "Thomas Oommen"], "title": "MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2", "AI": {"tldr": "MMLSv2\u662f\u4e00\u4e2a\u7528\u4e8e\u706b\u661f\u8868\u9762\u6ed1\u5761\u5206\u5272\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b7\u4e2a\u6ce2\u6bb5\u56fe\u50cf\uff0c\u5206\u4e3a\u8bad\u7ec3/\u9a8c\u8bc1/\u6d4b\u8bd5\u96c6\u548c\u5730\u7406\u9694\u79bb\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u706b\u661f\u6ed1\u5761\u5206\u5272\u7814\u7a76\u7f3a\u4e4f\u516c\u5f00\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u8bc4\u4f30\u6a21\u578b\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u7684\u5730\u7406\u9694\u79bb\u6d4b\u8bd5\u96c6\uff0c\u9650\u5236\u4e86\u6ed1\u5761\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b664\u5f20\u56fe\u50cf\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542bRGB\u3001\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u3001\u5761\u5ea6\u3001\u70ed\u60ef\u6027\u548c\u7070\u5ea6\u901a\u9053\u3002\u989d\u5916\u63d0\u4f9b276\u5f20\u6765\u81ea\u5730\u7406\u9694\u79bb\u533a\u57df\u7684\u6d4b\u8bd5\u96c6\u3002\u4f7f\u7528\u591a\u79cd\u5206\u5272\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u7a33\u5b9a\u8bad\u7ec3\u5e76\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u4f46\u5728\u788e\u7247\u5316\u3001\u7ec6\u957f\u548c\u5c0f\u89c4\u6a21\u6ed1\u5761\u533a\u57df\u4ecd\u5b58\u5728\u6311\u6218\u3002\u5730\u7406\u9694\u79bb\u6d4b\u8bd5\u96c6\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u5176\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MMLSv2\u4e3a\u706b\u661f\u6ed1\u5761\u5206\u5272\u63d0\u4f9b\u4e86\u9996\u4e2a\u516c\u5f00\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5176\u5730\u7406\u9694\u79bb\u6d4b\u8bd5\u96c6\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u6ed1\u5761\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u7814\u7a76\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.08117", "pdf": "https://arxiv.org/pdf/2602.08117", "abs": "https://arxiv.org/abs/2602.08117", "authors": ["Smriti Siva", "Jan Cross-Zamirski"], "title": "Building Damage Detection using Satellite Images and Patch-Based Transformer Methods", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.\n  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86Vision Transformer\u6a21\u578b\u5728xBD\u6570\u636e\u96c6\u4e0a\u7684\u5efa\u7b51\u635f\u4f24\u5206\u7c7b\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6027\u7684\u57fa\u4e8epatch\u7684\u9884\u5904\u7406\u6d41\u7a0b\u548c\u51bb\u7ed3\u5934\u5fae\u8c03\u7b56\u7565\uff0c\u5728\u566a\u58f0\u548c\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u4e0eCNN\u57fa\u7ebf\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "motivation": "\u5feb\u901f\u5efa\u7b51\u635f\u4f24\u8bc4\u4f30\u5bf9\u707e\u540e\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u536b\u661f\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002xBD\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u8de8\u5730\u7406\u533a\u57df\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u9700\u8981\u7814\u7a76ViT\u6a21\u578b\u5728\u566a\u58f0\u548c\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u533a\u5206\u7ed3\u6784\u635f\u4f24\u7c7b\u578b\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528DINOv2-small\u548cDeiT\u8fdb\u884c\u591a\u7c7b\u635f\u4f24\u5206\u7c7b\uff0c\u63d0\u51fa\u9488\u5bf9\u6027\u7684\u57fa\u4e8epatch\u7684\u9884\u5904\u7406\u6d41\u7a0b\u6765\u9694\u79bb\u7ed3\u6784\u7279\u5f81\u5e76\u51cf\u5c11\u80cc\u666f\u566a\u58f0\uff0c\u91c7\u7528\u51bb\u7ed3\u5934\u5fae\u8c03\u7b56\u7565\u4ee5\u63a7\u5236\u8ba1\u7b97\u9700\u6c42\uff0c\u901a\u8fc7\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u5b8f\u5e73\u5747F1\u5206\u6570\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5c0f\u578bViT\u67b6\u6784\u914d\u5408\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u707e\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u5148\u524dCNN\u57fa\u7ebf\u7ade\u4e89\u6027\u7684\u5b8f\u5e73\u5747F1\u5206\u6570\u3002", "conclusion": "ViT\u6a21\u578b\u5728\u5efa\u7b51\u635f\u4f24\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u9884\u5904\u7406\u548c\u5fae\u8c03\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u5904\u7406\u566a\u58f0\u548c\u4e0d\u5e73\u8861\u6570\u636e\uff0c\u4e3a\u536b\u661f\u56fe\u50cf\u635f\u4f24\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08126", "pdf": "https://arxiv.org/pdf/2602.08126", "abs": "https://arxiv.org/abs/2602.08126", "authors": ["Venkatraman Narayanan", "Bala Sai", "Rahul Ahuja", "Pratik Likhar", "Varun Ravi Kumar", "Senthil Yogamani"], "title": "MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.", "AI": {"tldr": "MambaFusion\uff1a\u57fa\u4e8e\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u548c\u7a97\u53e3Transformer\u7684\u591a\u6a21\u60013D\u68c0\u6d4b\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u3001\u7269\u7406\u57fa\u7840\u7684\u611f\u77e5\uff0c\u5728nuScenes\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u53ef\u9760\u76843D\u7269\u4f53\u68c0\u6d4b\uff0c\u4f46\u73b0\u6709\u57fa\u4e8eBEV\u7684\u878d\u5408\u6846\u67b6\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u3001\u7a7a\u95f4\u4e0d\u53d8\u878d\u5408\u548c\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u56f0\u96be\u7b49\u95ee\u9898\u3002\u76f8\u673a\u63d0\u4f9b\u5bc6\u96c6\u89c6\u89c9\u7ebf\u7d22\u4f46\u6df1\u5ea6\u4fe1\u606f\u4e0d\u51c6\u786e\uff0cLiDAR\u63d0\u4f9b\u7cbe\u786e3D\u7ed3\u6784\u4f46\u8986\u76d6\u7a00\u758f\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u878d\u5408\u65b9\u6cd5", "method": "1. \u4ea4\u66ff\u4f7f\u7528\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u548c\u7a97\u53e3Transformer\uff0c\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u4f20\u64ad\u5168\u5c40\u4e0a\u4e0b\u6587\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u51e0\u4f55\u4fdd\u771f\u5ea6\uff1b2. \u591a\u6a21\u6001\u4ee4\u724c\u5bf9\u9f50\u6a21\u5757\u548c\u53ef\u9760\u6027\u611f\u77e5\u878d\u5408\u95e8\uff0c\u57fa\u4e8e\u7a7a\u95f4\u7f6e\u4fe1\u5ea6\u548c\u6807\u5b9a\u4e00\u81f4\u6027\u52a8\u6001\u91cd\u52a0\u6743\u76f8\u673a-LiDAR\u7279\u5f81\uff1b3. \u7ed3\u6784\u6761\u4ef6\u6269\u6563\u5934\uff0c\u96c6\u6210\u57fa\u4e8e\u56fe\u7684\u63a8\u7406\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u53bb\u566a\uff0c\u589e\u5f3a\u7269\u7406\u5408\u7406\u6027\u548c\u6821\u51c6\u7f6e\u4fe1\u5ea6", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u6846\u67b6\u5c55\u793a\u4e86SSM\u6548\u7387\u4e0e\u53ef\u9760\u6027\u9a71\u52a8\u878d\u5408\u7684\u7ed3\u5408\u80fd\u591f\u4e3a\u771f\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u9c81\u68d2\u3001\u65f6\u95f4\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u76843D\u611f\u77e5", "conclusion": "MambaFusion\u901a\u8fc7\u5c06SSM\u6548\u7387\u4e0e\u53ef\u9760\u6027\u9a71\u52a8\u878d\u5408\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u3001\u7269\u7406\u57fa\u7840\u76843D\u611f\u77e5\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u8fbe\u5230SOTA\u6027\u80fd"}}
{"id": "2602.08131", "pdf": "https://arxiv.org/pdf/2602.08131", "abs": "https://arxiv.org/abs/2602.08131", "authors": ["Isaac Corley", "Hannah Kerner", "Caleb Robinson", "Jennifer Marcus"], "title": "Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries", "categories": ["cs.CV"], "comment": null, "summary": "Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).", "AI": {"tldr": "FTW\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u5168\u7403\u519c\u7530\u8fb9\u754c\u6570\u636e\u96c6\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5de5\u5177\uff0c\u652f\u6301\u4ece\u5c40\u90e8\u5230\u56fd\u5bb6\u5c3a\u5ea6\u7684\u519c\u7530\u8fb9\u754c\u63d0\u53d6\u3001\u4f5c\u7269\u5206\u7c7b\u548c\u68ee\u6797\u635f\u5931\u5206\u6790\u3002", "motivation": "\u519c\u7530\u8fb9\u754c\u56fe\u662f\u519c\u4e1a\u6570\u636e\u4ea7\u54c1\u7684\u57fa\u7840\uff0c\u5bf9\u4f5c\u7269\u76d1\u6d4b\u3001\u4ea7\u91cf\u4f30\u7b97\u548c\u75be\u75c5\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8986\u76d6\u8303\u56f4\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u6784\u5efaFields of The World\u751f\u6001\u7cfb\u7edf\uff1a\u5305\u542b24\u4e2a\u56fd\u5bb6160\u4e07\u4e2a\u519c\u7530\u591a\u8fb9\u5f62\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3001\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u548c\u547d\u4ee4\u884c\u63a8\u7406\u5de5\u5177\u3002\u4f7f\u7528MOSAIKS\u968f\u673a\u5377\u79ef\u7279\u5f81\u548cFTW\u519c\u7530\u8fb9\u754c\u8fdb\u884c\u4f5c\u7269\u7c7b\u578b\u5206\u7c7b\u3002", "result": "\u4f5c\u7269\u7c7b\u578b\u5206\u7c7b\u7684\u5b8f\u89c2F1\u5206\u6570\u8fbe\u52300.65-0.75\uff08\u4f7f\u7528\u6709\u9650\u6807\u7b7e\uff09\u3002\u5728\u4e94\u4e2a\u56fd\u5bb6\uff08476\u4e07\u5e73\u65b9\u516c\u91cc\uff09\u5c55\u793a\u4e86\u9884\u6d4b\u7ed3\u679c\uff0c\u9884\u6d4b\u519c\u7530\u4e2d\u4f4d\u6570\u9762\u79ef\u4ece0.06\u516c\u9877\uff08\u5362\u65fa\u8fbe\uff09\u52300.28\u516c\u9877\uff08\u745e\u58eb\uff09\u3002", "conclusion": "FTW\u751f\u6001\u7cfb\u7edf\u4e3a\u519c\u7530\u8fb9\u754c\u63d0\u53d6\u548c\u4f5c\u7269\u5206\u7c7b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4ece\u5c40\u90e8\u5230\u56fd\u5bb6\u5c3a\u5ea6\u7684\u519c\u4e1a\u76d1\u6d4b\u5e94\u7528\u3002"}}
{"id": "2602.08136", "pdf": "https://arxiv.org/pdf/2602.08136", "abs": "https://arxiv.org/abs/2602.08136", "authors": ["Md Rafi Ur Rashid", "MD Sadik Hossain Shanto", "Vishnu Asutosh Dasu", "Shagufta Mehnaz"], "title": "Robustness of Vision Language Models Against Split-Image Harmful Input Attacks", "categories": ["cs.CV", "cs.AI"], "comment": "22 Pages, long conference paper", "summary": "Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u89c6\u89c9\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5SIVA\uff0c\u5229\u7528\u6a21\u578b\u5728\u5206\u5272\u56fe\u50cf\u8f93\u5165\u4e0a\u7684\u5b89\u5168\u5bf9\u9f50\u6f0f\u6d1e\uff0c\u901a\u8fc7\u5bf9\u6297\u77e5\u8bc6\u84b8\u998f\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u6a21\u578b\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b8c\u6574\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b89\u5168\u5bf9\u9f50\uff08\u5982RLHF\uff09\uff0c\u4f46\u5bf9\u5206\u5272\u56fe\u50cf\u8f93\u5165\u7684\u5b89\u5168\u5bf9\u9f50\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0\u6709\u5bb3\u8bed\u4e49\u5206\u5e03\u5728\u591a\u4e2a\u56fe\u50cf\u7247\u6bb5\u4e2d\u65f6\uff0c\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u68c0\u6d4b\u548c\u62d2\u7edd\uff0c\u8fd9\u6784\u6210\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faSIVA\u5206\u5272\u56fe\u50cf\u89c6\u89c9\u8d8a\u72f1\u653b\u51fb\uff0c\u5305\u62ec\u6e10\u8fdb\u5f0f\u653b\u51fb\u7b56\u7565\uff1a\u4ece\u7b80\u5355\u5206\u5272\u5230\u81ea\u9002\u5e94\u767d\u76d2\u653b\u51fb\uff0c\u6700\u7ec8\u53d1\u5c55\u4e3a\u9ed1\u76d2\u8fc1\u79fb\u653b\u51fb\u3002\u6700\u5f3a\u7b56\u7565\u91c7\u7528\u65b0\u9896\u7684\u5bf9\u6297\u77e5\u8bc6\u84b8\u998f\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6a21\u578b\u8fc1\u79fb\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u73b0\u4ee3VLM\u548c\u4e09\u4e2a\u8d8a\u72f1\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6700\u5f3a\u653b\u51fb\u6bd4\u73b0\u6709\u57fa\u7ebf\u9ad8\u51fa60%\u7684\u8fc1\u79fb\u6210\u529f\u7387\u3002", "conclusion": "\u63ed\u793a\u4e86\u5f53\u524dVLM\u5b89\u5168\u5bf9\u9f50\u5728\u5206\u5272\u56fe\u50cf\u8f93\u5165\u4e0a\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2602.08168", "pdf": "https://arxiv.org/pdf/2602.08168", "abs": "https://arxiv.org/abs/2602.08168", "authors": ["Mei Ling Chee", "Thangarajah Akilan", "Aparna Ravindra Phalke", "Kanchan Keisham"], "title": "DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation", "categories": ["cs.CV"], "comment": "13 pages", "summary": "Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.", "AI": {"tldr": "DAS-SK\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5206\u5272\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u9009\u62e9\u6027\u6838\u5377\u79ef\u96c6\u6210\u5230\u53cc\u7a7a\u6d1e\u53ef\u5206\u79bb\u5377\u79ef\u6a21\u5757\u4e2d\uff0c\u5728\u519c\u4e1a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5206\u5272\u4e2d\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u519c\u4e1a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u9700\u8981\u5e73\u8861\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6a21\u578b\uff0c\u4ee5\u4fbf\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u90e8\u7f72\u3002\u73b0\u6709\u6a21\u578b\u901a\u5e38\u9762\u4e34\u5927\u6570\u636e\u96c6\u9700\u6c42\u3001\u6709\u9650\u7684\u5149\u8c31\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u65e0\u4eba\u673a\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faDAS-SK\u67b6\u6784\uff0c\u5c06\u9009\u62e9\u6027\u6838\u5377\u79ef\uff08SK-Conv\uff09\u96c6\u6210\u5230\u53cc\u7a7a\u6d1e\u53ef\u5206\u79bb\u5377\u79ef\uff08DAS-Conv\uff09\u6a21\u5757\u4e2d\uff0c\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u5b66\u4e60\u3002\u6539\u8fdb\u7a7a\u6d1e\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\uff08ASPP\uff09\u6a21\u5757\uff0c\u540c\u65f6\u6355\u83b7\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7ed3\u6784\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u57fa\u4e8e\u6539\u8fdb\u7684DeepLabV3\u6846\u67b6\uff0c\u4f7f\u7528MobileNetV3-Large\u548cEfficientNet-B3\u4e24\u4e2a\u4e92\u8865\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u5728LandCover.ai\u3001VDD\u548cPhenoBench\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDAS-SK\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6bd4CNN\u3001Transformer\u548c\u6df7\u5408\u6a21\u578b\u66f4\u9ad8\u6548\u3002\u4e0e\u9876\u7ea7Transformer\u6a21\u578b\u76f8\u6bd4\uff0cDAS-SK\u9700\u8981\u6700\u591a21\u500d\u66f4\u5c11\u7684\u53c2\u6570\u548c19\u500d\u66f4\u5c11\u7684GFLOPs\u3002", "conclusion": "DAS-SK\u4e3a\u5b9e\u65f6\u519c\u4e1a\u673a\u5668\u4eba\u548c\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5176\u4ed6\u89c6\u89c9\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2602.08198", "pdf": "https://arxiv.org/pdf/2602.08198", "abs": "https://arxiv.org/abs/2602.08198", "authors": ["Jingyu Hu", "Bin Hu", "Ka-Hei Hui", "Haipeng Li", "Zhengzhe Liu", "Daniel Cohen-Or", "Chi-Wing Fu"], "title": "PEGAsus: 3D Personalization of Geometry and Appearance", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.", "AI": {"tldr": "PEGAsus\u662f\u4e00\u4e2a\u751f\u6210\u4e2a\u6027\u53163D\u5f62\u72b6\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u51e0\u4f55\u548c\u5916\u89c2\u5c42\u9762\u7684\u5f62\u72b6\u6982\u5ff5\uff0c\u5b9e\u73b0\u4ece\u53c2\u8003\u5f62\u72b6\u63d0\u53d6\u53ef\u91cd\u7528\u5c5e\u6027\u5e76\u4e0e\u6587\u672c\u7ed3\u5408\u751f\u6210\u65b0\u5f62\u72b6\u3002", "motivation": "\u73b0\u67093D\u5f62\u72b6\u751f\u6210\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u80fd\u591f\u4ece\u53c2\u8003\u5f62\u72b6\u63d0\u53d6\u53ef\u91cd\u7528\u6982\u5ff5\u5e76\u4e0e\u6587\u672c\u63cf\u8ff0\u7075\u6d3b\u7ec4\u5408\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u8de8\u7c7b\u522b\u751f\u6210\u3002", "method": "1. \u5c063D\u5f62\u72b6\u4e2a\u6027\u5316\u5b9a\u4e49\u4e3a\u63d0\u53d6\u7c7b\u522b\u65e0\u5173\u7684\u51e0\u4f55\u548c\u5916\u89c2\u5c5e\u6027\uff1b2. \u8bbe\u8ba1\u6e10\u8fdb\u4f18\u5316\u7b56\u7565\u5206\u79bb\u5b66\u4e60\u51e0\u4f55\u548c\u5916\u89c2\u6982\u5ff5\uff1b3. \u6269\u5c55\u5230\u533a\u57df\u7ea7\u6982\u5ff5\u5b66\u4e60\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u65e0\u4e0a\u4e0b\u6587\u635f\u5931\u3002", "result": "PEGAsus\u80fd\u591f\u4ece\u5e7f\u6cdb\u53c2\u8003\u5f62\u72b6\u4e2d\u6709\u6548\u63d0\u53d6\u5c5e\u6027\uff0c\u5e76\u4e0e\u6587\u672c\u7075\u6d3b\u7ec4\u5408\u751f\u6210\u65b0\u5f62\u72b6\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5728\u8de8\u7c7b\u522b\u573a\u666f\u4e2d\u4ea7\u751f\u591a\u6837\u5316\u4e2a\u6027\u5316\u7ed3\u679c\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PEGAsus\u901a\u8fc7\u51e0\u4f55\u548c\u5916\u89c2\u5c42\u9762\u7684\u6982\u5ff5\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u76843D\u5f62\u72b6\u4e2a\u6027\u5316\u751f\u6210\uff0c\u5728\u6982\u5ff5\u63d0\u53d6\u3001\u7ec4\u5408\u63a7\u5236\u548c\u8de8\u7c7b\u522b\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e2a\u6027\u53163D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2602.08202", "pdf": "https://arxiv.org/pdf/2602.08202", "abs": "https://arxiv.org/abs/2602.08202", "authors": ["Jinrong Lv", "Xun Gong", "Zhaohuan Li", "Weili Jiang"], "title": "Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video", "categories": ["cs.CV"], "comment": "11 pages, 5 tables, 10 figures. Under peer review", "summary": "Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4ece\u8d85\u58f0\u5fc3\u52a8\u56fe\u4f30\u8ba1\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570(LVEF)\u7684\u751f\u6210\u5f0f\u56de\u5f52\u65b9\u6cd5\uff0c\u91c7\u7528\u591a\u6a21\u6001\u6761\u4ef6\u8bc4\u5206\u6269\u6563\u6a21\u578b\u6765\u5efa\u6a21\u8fde\u7eed\u540e\u9a8c\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u5206\u5e03\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4ece\u8d85\u58f0\u5fc3\u52a8\u56fe\u4f30\u8ba1LVEF\u662f\u4e00\u4e2a\u75c5\u6001\u9006\u95ee\u9898\uff0c\u5b58\u5728\u566a\u58f0\u3001\u4f2a\u5f71\u548c\u6709\u9650\u89c6\u89d2\u5bfc\u81f4\u7684\u6a21\u7cca\u6027\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u91c7\u7528MSE\u56de\u5f52\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u6761\u4ef6\u671f\u671b\uff0c\u5f53\u540e\u9a8c\u5206\u5e03\u662f\u591a\u6a21\u6001\u6216\u91cd\u5c3e\u5206\u5e03\u65f6\u4f1a\u4ea7\u751f\u8bef\u5bfc\u6027\u9884\u6d4b\uff0c\u8fd9\u5728\u75c5\u7406\u573a\u666f\u4e2d\u5f88\u5e38\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u6761\u4ef6\u8bc4\u5206\u6269\u6563\u56de\u5f52\u6a21\u578b(MCSDR)\uff0c\u8fd9\u662f\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u4ee5\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u548c\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u5148\u9a8c\u4e3a\u6761\u4ef6\u7684LVEF\u8fde\u7eed\u540e\u9a8c\u5206\u5e03\u3002\u8be5\u65b9\u6cd5\u4ece\u786e\u5b9a\u6027\u56de\u5f52\u8f6c\u5411\u751f\u6210\u5f0f\u56de\u5f52\u3002", "result": "\u5728EchoNet-Dynamic\u3001EchoNet-Pediatric\u548cCAMUS\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMCSDR\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u5728\u9ad8\u566a\u58f0\u6216\u663e\u8457\u751f\u7406\u53d8\u5f02\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u7684\u751f\u6210\u8f68\u8ff9\u8868\u73b0\u51fa\u72ec\u7279\u884c\u4e3a\uff0c\u4e3aAI\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u5c42\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4ece\u786e\u5b9a\u6027\u56de\u5f52\u5411\u751f\u6210\u5f0f\u56de\u5f52\u7684\u8303\u5f0f\u8f6c\u53d8\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u7684MCSDR\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86LVEF\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u751f\u6210\u8f68\u8ff9\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u89c1\u89e3\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u75c5\u7406\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2602.08206", "pdf": "https://arxiv.org/pdf/2602.08206", "abs": "https://arxiv.org/abs/2602.08206", "authors": ["Chufeng Zhou", "Jian Wang", "Xinyuan Liu", "Xiaokang Zhang"], "title": "Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation", "categories": ["cs.CV"], "comment": "5 pages, 3 figures", "summary": "Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based\" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.", "AI": {"tldr": "\u63d0\u51faGR-CoT\u6846\u67b6\uff0c\u901a\u8fc7\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u94fe\u589e\u5f3aMLLMs\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u89e3\u51b3\u9065\u611f\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u76f8\u4f3c\u5149\u8c31\u7279\u5f81\u4f46\u4e0d\u540c\u8bed\u4e49\u7c7b\u522b\u7684\u6b67\u4e49\u95ee\u9898", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\u548c\u6587\u672c\u5d4c\u5165\u7684\u88ab\u52a8\u6620\u5c04\uff0c\u7f3a\u4e4f\u5730\u7406\u7a7a\u95f4\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5bfc\u81f4\u9047\u5230\u5149\u8c31\u7279\u5f81\u76f8\u4f3c\u4f46\u8bed\u4e49\u5c5e\u6027\u4e0d\u540c\u7684\u5730\u7269\u7c7b\u522b\u65f6\u4ea7\u751f\u4e25\u91cd\u8bed\u4e49\u6b67\u4e49\u548c\u8bef\u5206\u7c7b", "method": "\u63d0\u51faGeospatial Reasoning Chain-of-Thought (GR-CoT)\u6846\u67b6\uff0c\u5305\u542b\u79bb\u7ebf\u77e5\u8bc6\u84b8\u998f\u6d41\u548c\u5728\u7ebf\u5b9e\u4f8b\u63a8\u7406\u6d41\u3002\u79bb\u7ebf\u6d41\u5efa\u7acb\u7ec6\u7c92\u5ea6\u7c7b\u522b\u89e3\u91ca\u6807\u51c6\uff0c\u5728\u7ebf\u6d41\u6267\u884c\u5b8f\u89c2\u573a\u666f\u951a\u5b9a\u3001\u89c6\u89c9\u7279\u5f81\u89e3\u8026\u548c\u77e5\u8bc6\u9a71\u52a8\u51b3\u7b56\u5408\u6210\u7684\u987a\u5e8f\u63a8\u7406\u8fc7\u7a0b", "result": "\u5728LoveDA\u548cGID5\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027", "conclusion": "GR-CoT\u6846\u67b6\u901a\u8fc7\u589e\u5f3aMLLMs\u7684\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u6b67\u4e49\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u5730\u7406\u8bed\u4e49\u5bf9\u9f50"}}
{"id": "2602.08211", "pdf": "https://arxiv.org/pdf/2602.08211", "abs": "https://arxiv.org/abs/2602.08211", "authors": ["Yik Lung Pang", "Changjae Oh"], "title": "Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension", "categories": ["cs.CV"], "comment": "4 pages, 5 figures, 2 tables", "summary": "Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.", "AI": {"tldr": "\u63d0\u51faChain-of-Caption\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u89c6\u89c9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u5728REC\u4efb\u52a1\u4e0a\u5b9e\u73b05%-30%\u7684\u6027\u80fd\u63d0\u5347", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u4e0a\u5df2\u53d6\u5f97\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u989d\u5916\u89c6\u89c9\u6216\u6587\u672c\u4e0a\u4e0b\u6587\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u4e0d\u540c\u4e0a\u4e0b\u6587\u63d0\u4f9b\u6280\u672f\u5bf9REC\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faChain-of-Caption\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u5206\u6790\u5404\u79cd\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u989d\u5916\u89c6\u89c9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u6280\u672f\uff0c\u5e76\u5728RefCOCO/RefCOCOg/RefCOCO+\u548cRef-L4\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u72ec\u7684\u6587\u672c\u6216\u89c6\u89c9\u4e0a\u4e0b\u6587\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347REC\u6027\u80fd\u3002\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\uff0c\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u5728\u591a\u79cdIoU\u9608\u503c\u4e0b\u7684\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u53475%-30%\u3002", "conclusion": "Chain-of-Caption\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86MLLMs\u5728REC\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u7ed3\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u4ef7\u503c\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2602.08224", "pdf": "https://arxiv.org/pdf/2602.08224", "abs": "https://arxiv.org/abs/2602.08224", "authors": ["Jing Zhang", "Zhikai Li", "Xuewen Liu", "Qingyi Gu"], "title": "Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval", "categories": ["cs.CV"], "comment": "ICLR 2026,Code is available at: https://github.com/jingjing0419/Efficient-SAM2", "summary": "Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.", "AI": {"tldr": "\u63d0\u51faEfficient-SAM2\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u8c61\u611f\u77e5\u7684\u7a00\u758f\u7a97\u53e3\u8def\u7531\u548c\u7a00\u758f\u5185\u5b58\u68c0\u7d22\u673a\u5236\uff0c\u5728\u4fdd\u6301SAM2\u89c6\u9891\u5206\u5272\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b01.68\u500d\u52a0\u901f", "motivation": "SAM2\u5728\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u65f6\u89c6\u9891\u5904\u7406\u3002\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u91cd\u65b0\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\uff0c\u5bf9\u8bad\u7ec3\u540e\u52a0\u901f\u7684\u63a2\u7d22\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0SAM2\u5177\u6709\u7c7b\u4f3c\u751f\u7269\u89c6\u89c9\u7684\u7a00\u758f\u611f\u77e5\u6a21\u5f0f\uff0c\u5b58\u5728\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\u7684\u673a\u4f1a", "method": "\u63d0\u51faEfficient-SAM2\u6846\u67b6\uff1a1) \u5bf9\u8c61\u611f\u77e5\u7a00\u758f\u7a97\u53e3\u8def\u7531(SWR)\uff1a\u5229\u7528\u524d\u4e00\u5e27\u89e3\u7801\u5668\u7684\u8fde\u7eed\u6027\u548c\u663e\u8457\u6027\u7ebf\u7d22\uff0c\u5c06\u80cc\u666f\u533a\u57df\u8def\u7531\u5230\u8f7b\u91cf\u7ea7\u5feb\u6377\u5206\u652f\uff1b2) \u5bf9\u8c61\u611f\u77e5\u7a00\u758f\u5185\u5b58\u68c0\u7d22(SMR)\uff1a\u4ec5\u8ba9\u6bcf\u5e27\u4e2d\u7684\u663e\u8457\u5185\u5b58\u6807\u8bb0\u53c2\u4e0e\u8ba1\u7b97\uff0c\u5e76\u91cd\u7528\u9996\u6b21\u56de\u5fc6\u65f6\u7684\u663e\u8457\u6027\u6a21\u5f0f", "result": "\u5728SAM2.1-L\u6a21\u578b\u4e0a\u5b9e\u73b01.68\u500d\u52a0\u901f\uff0c\u5728SA-V\u6d4b\u8bd5\u96c6\u4e0a\u4ec5\u4ea7\u751f1.0%\u7684\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u989d\u5916\u53c2\u6570\u548c\u8bad\u7ec3\u5f00\u9500\u6781\u5c0f", "conclusion": "Efficient-SAM2\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u7126\u5bf9\u8c61\u533a\u57df\u5e76\u6d88\u9664\u4efb\u52a1\u65e0\u5173\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u9ad8\u4e86SAM2\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08230", "pdf": "https://arxiv.org/pdf/2602.08230", "abs": "https://arxiv.org/abs/2602.08230", "authors": ["Hongwei Ren", "Youxin Jiang", "Qifei Gu", "Xiangqian Wu"], "title": "Generating Adversarial Events: A Motion-Aware Point Cloud Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \\textbf{M}otion-\\textbf{A}ware \\textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.", "AI": {"tldr": "MA-ADV\u662f\u9996\u4e2a\u5229\u7528\u70b9\u4e91\u8868\u793a\u751f\u6210\u5bf9\u6297\u6027\u4e8b\u4ef6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u5e73\u6ed1\u6270\u52a8\u5e76\u5229\u7528\u65f6\u7a7a\u5173\u7cfb\uff0c\u5b9e\u73b0100%\u653b\u51fb\u6210\u529f\u7387\u4e14\u6270\u52a8\u6210\u672c\u6700\u5c0f\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u5bf9\u6297\u6837\u672c\u7684\u8106\u5f31\u6027\u5a01\u80c1\u4e8b\u4ef6\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002\u7531\u4e8e\u4e3b\u6d41\u4e8b\u4ef6\u8868\u793a\u7684\u975e\u53ef\u5fae\u6027\uff0c\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\uff0c\u5bfc\u81f4\u4e8b\u4ef6\u5bf9\u6297\u653b\u51fb\u7814\u7a76\u7a00\u7f3a\u3002", "method": "\u63d0\u51faMA-ADV\u6846\u67b6\uff1a1) \u5229\u7528\u70b9\u4e91\u8868\u793a\u751f\u6210\u5bf9\u6297\u4e8b\u4ef6\uff1b2) \u8003\u8651\u4e8b\u4ef6\u4e2d\u7684\u9ad8\u9891\u566a\u58f0\uff0c\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5e73\u6ed1\u6270\u52a8\uff1b3) \u5145\u5206\u5229\u7528\u4e8b\u4ef6\u95f4\u7684\u65f6\u7a7a\u5173\u7cfb\uff1b4) \u7ed3\u5408\u6837\u672c\u7ea7Adam\u4f18\u5316\u3001\u8fed\u4ee3\u7cbe\u70bc\u548c\u4e8c\u5206\u641c\u7d22\u5bfb\u627e\u6700\u5c0f\u6210\u672c\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1MA-ADV\u80fd\u786e\u4fdd100%\u653b\u51fb\u6210\u529f\u7387\u4e14\u6270\u52a8\u6210\u672c\u6700\u5c0f\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5bf9\u9632\u5fa1\u65b9\u6cd5\u7684\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u51f8\u663e\u4e86\u672a\u6765\u4e8b\u4ef6\u611f\u77e5\u7cfb\u7edf\u9762\u4e34\u7684\u5173\u952e\u5b89\u5168\u6311\u6218\u3002", "conclusion": "MA-ADV\u662f\u9996\u4e2a\u57fa\u4e8e\u70b9\u4e91\u8868\u793a\u7684\u4e8b\u4ef6\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e8b\u4ef6\u8868\u793a\u975e\u53ef\u5fae\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u63ed\u793a\u4e86\u4e8b\u4ef6\u611f\u77e5\u7cfb\u7edf\u5b58\u5728\u7684\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u5bf9\u672a\u6765\u5b89\u5168\u5173\u952e\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.08236", "pdf": "https://arxiv.org/pdf/2602.08236", "abs": "https://arxiv.org/abs/2602.08236", "authors": ["Shoubin Yu", "Yue Zhang", "Zun Wang", "Jaehong Yoon", "Huaxiu Yao", "Mingyu Ding", "Mohit Bansal"], "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "the first two authors are equally contributed. Project page: https://adaptive-visual-tts.github.io/", "summary": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.", "AI": {"tldr": "AVIC\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u63a7\u5236\u89c6\u89c9\u60f3\u8c61\uff0c\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u63a8\u7406\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u4e16\u754c\u6a21\u578b\u8c03\u7528", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4ece\u672a\u89c1\u89c6\u89d2\u63a8\u7406\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u60f3\u8c61\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4f55\u65f6\u9700\u8981\u60f3\u8c61\u3001\u9700\u8981\u591a\u5c11\u60f3\u8c61\u4ee5\u53ca\u4f55\u65f6\u60f3\u8c61\u6709\u5bb3\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u63d0\u51faAVIC\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u6846\u67b6\uff0c\u5305\u542b\u4e16\u754c\u6a21\u578b\uff0c\u5728\u8c03\u7528\u89c6\u89c9\u60f3\u8c61\u524d\u663e\u5f0f\u63a8\u7406\u5f53\u524d\u89c6\u89c9\u8bc1\u636e\u7684\u5145\u5206\u6027\uff0c\u7136\u540e\u9009\u62e9\u6027\u8c03\u7528\u548c\u7f29\u653e\u89c6\u89c9\u60f3\u8c61\u3002", "result": "\u5728\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6(SAT, MMSI)\u548c\u5177\u8eab\u5bfc\u822a\u57fa\u51c6(R2R)\u4e0a\uff0c\u7ed3\u679c\u663e\u793a\u9009\u62e9\u6027\u63a7\u5236\u53ef\u4ee5\u5339\u914d\u6216\u4f18\u4e8e\u56fa\u5b9a\u60f3\u8c61\u7b56\u7565\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e16\u754c\u6a21\u578b\u8c03\u7528\u548c\u8bed\u8a00\u6807\u8bb0\u6570\u91cf\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9\u60f3\u8c61\u5728\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u573a\u666f\uff0c\u5f3a\u8c03\u5206\u6790\u548c\u63a7\u5236\u6d4b\u8bd5\u65f6\u60f3\u8c61\u5bf9\u4e8e\u9ad8\u6548\u53ef\u9760\u7a7a\u95f4\u63a8\u7406\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.08262", "pdf": "https://arxiv.org/pdf/2602.08262", "abs": "https://arxiv.org/abs/2602.08262", "authors": ["Guoqi Yu", "Xiaowei Hu", "Angelica I. Aviles-Rivero", "Anqi Qiu", "Shujun Wang"], "title": "Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification", "categories": ["cs.CV"], "comment": "This paper has been accepted by IEEE Transactions on Medical Imaging", "summary": "Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.", "AI": {"tldr": "\u63d0\u51faDeCI\u6846\u67b6\uff0c\u901a\u8fc7\u5468\u671f-\u6f02\u79fb\u5206\u89e3\u548c\u901a\u9053\u72ec\u7acb\u5efa\u6a21\uff0c\u76f4\u63a5\u5728\u539f\u59cbBOLD\u4fe1\u53f7\u4e0a\u8fdb\u884c\u8111\u75be\u75c5\u5206\u7c7b\uff0c\u4f18\u4e8e\u4f20\u7edf\u529f\u80fd\u8fde\u63a5\u65b9\u6cd5", "motivation": "\u73b0\u6709fMRI\u8111\u75be\u75c5\u5206\u7c7b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u76ae\u5c14\u900a\u76f8\u5173\u7684\u529f\u80fd\u8fde\u63a5\uff0c\u5c064D BOLD\u4fe1\u53f7\u7b80\u5316\u4e3a\u9759\u60012D\u77e9\u9635\uff0c\u4e22\u5931\u4e86\u65f6\u95f4\u52a8\u6001\u4fe1\u606f\u4e14\u53ea\u80fd\u6355\u6349\u7ebf\u6027\u5173\u7cfb", "method": "\u63d0\u51faDeCI\u6846\u67b6\uff1a1) \u5468\u671f-\u6f02\u79fb\u5206\u89e3\uff1a\u5c06\u6bcf\u4e2aROI\u7684\u4fe1\u53f7\u5206\u89e3\u4e3a\u5468\u671f\u6027\u548c\u6f02\u79fb\u6027\u6210\u5206\uff1b2) \u901a\u9053\u72ec\u7acb\uff1a\u5206\u522b\u5efa\u6a21\u6bcf\u4e2aROI\uff0c\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u51cf\u5c11\u8fc7\u62df\u5408", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDeCI\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8e\u529f\u80fd\u8fde\u63a5\u7684\u65b9\u6cd5\u548c\u5176\u4ed6\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u57fa\u7ebf", "conclusion": "\u7814\u7a76\u652f\u6301\u5728fMRI\u5206\u6790\u4e2d\u8f6c\u5411\u7aef\u5230\u7aef\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u590d\u6742\u7684\u5927\u8111\u52a8\u6001\uff0cDeCI\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08277", "pdf": "https://arxiv.org/pdf/2602.08277", "abs": "https://arxiv.org/abs/2602.08277", "authors": ["Xiangbo Gao", "Renjie Li", "Xinghao Chen", "Yuheng Wu", "Suofei Feng", "Qing Yin", "Zhengzhong Tu"], "title": "PISCO: Precise Video Instance Insertion with Sparse Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.", "AI": {"tldr": "PISCO\u662f\u4e00\u4e2a\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7\u4efb\u610f\u7a00\u758f\u5173\u952e\u5e27\u63a7\u5236\u5b9e\u73b0\u7cbe\u786e\u7684\u89c6\u9891\u5b9e\u4f8b\u63d2\u5165\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u9891\u7f16\u8f91\u4e2d\u7a7a\u95f4-\u65f6\u95f4\u5b9a\u4f4d\u3001\u7269\u7406\u4e00\u81f4\u6027\u548c\u539f\u59cb\u52a8\u6001\u4fdd\u6301\u7684\u6311\u6218\u3002", "motivation": "AI\u89c6\u9891\u751f\u6210\u6b63\u4ece\u4f9d\u8d56\u5927\u91cf\u63d0\u793a\u5de5\u7a0b\u548c\u7b5b\u9009\u7684\u901a\u7528\u751f\u6210\u8f6c\u5411\u7ec6\u7c92\u5ea6\u53ef\u63a7\u751f\u6210\u548c\u9ad8\u4fdd\u771f\u540e\u5904\u7406\u3002\u5728\u4e13\u4e1aAI\u8f85\u52a9\u7535\u5f71\u5236\u4f5c\u4e2d\uff0c\u9700\u8981\u5bf9\u73b0\u6709\u7d20\u6750\u8fdb\u884c\u7cbe\u786e\u3001\u6709\u9488\u5bf9\u6027\u7684\u4fee\u6539\uff0c\u89c6\u9891\u5b9e\u4f8b\u63d2\u5165\u662f\u8fd9\u4e00\u8f6c\u53d8\u7684\u5173\u952e\uff0c\u9700\u8981\u5c06\u7279\u5b9a\u5b9e\u4f8b\u63d2\u5165\u73b0\u6709\u955c\u5934\u540c\u65f6\u4fdd\u6301\u573a\u666f\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51faPISCO\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u5355\u5173\u952e\u5e27\u3001\u8d77\u59cb-\u7ed3\u675f\u5173\u952e\u5e27\u6216\u4efb\u610f\u65f6\u95f4\u6233\u7684\u7a00\u758f\u5173\u952e\u5e27\u63a7\u5236\u3002\u5f15\u5165\u53ef\u53d8\u4fe1\u606f\u5f15\u5bfc\u7528\u4e8e\u9c81\u68d2\u6761\u4ef6\u5316\uff0c\u5206\u5e03\u4fdd\u6301\u65f6\u95f4\u63a9\u7801\u7528\u4e8e\u7a33\u5b9a\u65f6\u95f4\u751f\u6210\uff0c\u4ee5\u53ca\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\u5316\u5b9e\u73b0\u771f\u5b9e\u573a\u666f\u9002\u5e94\u3002", "result": "\u6784\u5efa\u4e86PISCO-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u9a8c\u8bc1\u8fc7\u7684\u5b9e\u4f8b\u6807\u6ce8\u548c\u914d\u5bf9\u5e72\u51c0\u80cc\u666f\u89c6\u9891\u3002\u5b9e\u9a8c\u8868\u660ePISCO\u5728\u7a00\u758f\u63a7\u5236\u4e0b\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff08\u4fee\u590d\u548c\u89c6\u9891\u7f16\u8f91\uff09\uff0c\u5e76\u968f\u7740\u63d0\u4f9b\u989d\u5916\u63a7\u5236\u4fe1\u53f7\u5c55\u73b0\u51fa\u6e05\u6670\u3001\u5355\u8c03\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PISCO\u901a\u8fc7\u7a00\u758f\u5173\u952e\u5e27\u63a7\u5236\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u89c6\u9891\u5b9e\u4f8b\u63d2\u5165\uff0c\u4e3a\u4e13\u4e1aAI\u8f85\u52a9\u7535\u5f71\u5236\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86AI\u89c6\u9891\u751f\u6210\u5411\u66f4\u53ef\u63a7\u3001\u66f4\u7cbe\u7ec6\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2602.08282", "pdf": "https://arxiv.org/pdf/2602.08282", "abs": "https://arxiv.org/abs/2602.08282", "authors": ["Haixu Liu", "Yufei Wang", "Tianxiang Xu", "Chuancheng Shi", "Hongsheng Xing"], "title": "Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408PA\u548cPO\u6570\u636e\u4f18\u52bf\uff0c\u901a\u8fc7\u5730\u7406\u5bf9\u9f50\u548c\u4e13\u5bb6\u6df7\u5408\u7b56\u7565\u89e3\u51b3\u690d\u7269\u5206\u5e03\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u758f\u3001\u504f\u5dee\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u8de8\u7269\u79cd\u690d\u7269\u5206\u5e03\u9884\u6d4b\u5bf9\u751f\u7269\u591a\u6837\u6027\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u89c2\u6d4b\u6570\u636e\u7a00\u758f\u3001\u504f\u5dee\u7684\u6311\u6218\u3002PA\u6570\u636e\u51c6\u786e\u4f46\u6210\u672c\u9ad8\u3001\u6570\u91cf\u6709\u9650\uff1bPO\u6570\u636e\u8986\u76d6\u5e7f\u4f46\u8d1f\u6837\u672c\u6807\u7b7e\u566a\u58f0\u4e25\u91cd\u3002\u9700\u8981\u5145\u5206\u5229\u7528\u4e24\u79cd\u6570\u636e\u4f18\u52bf\u3002", "method": "1) \u57fa\u4e8e\u536b\u661f\u5f71\u50cf\u5730\u7406\u8986\u76d6\u7684PO\u6570\u636e\u4f2a\u6807\u7b7e\u805a\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u6807\u7b7e\u7a7a\u95f4\u4e0e\u9065\u611f\u7279\u5f81\u7a7a\u95f4\u7684\u5730\u7406\u5bf9\u9f50\uff1b2) \u591a\u6a21\u6001\u67b6\u6784\uff1aSwin Transformer\u5904\u7406\u536b\u661f\u5f71\u50cf\uff0cTabM\u7f51\u7edc\u63d0\u53d6\u8868\u683c\u7279\u5f81\uff0cTemporal Swin Transformer\u5904\u7406\u65f6\u95f4\u5e8f\u5217\uff0c\u4e32\u884c\u4e09\u6a21\u6001\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u5f02\u8d28\u6a21\u6001\uff1b3) \u4e13\u5bb6\u6df7\u5408\u7b56\u7565\uff1a\u6839\u636e\u7a7a\u95f4\u90bb\u8fd1\u6027\u5212\u5206\u6d4b\u8bd5\u6837\u672c\uff0c\u5728\u4e0d\u540c\u5206\u533a\u4f7f\u7528\u4e0d\u540c\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u548c\u540e\u5904\u7406\u3002", "result": "\u5728GeoLifeCLEF 2025\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728PA\u8986\u76d6\u6709\u9650\u4e14\u5206\u5e03\u504f\u79fb\u660e\u663e\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86PA\u548cPO\u6570\u636e\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u5730\u7406\u5bf9\u9f50\u548c\u4e13\u5bb6\u6df7\u5408\u7b56\u7565\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u758f\u3001\u6807\u7b7e\u566a\u58f0\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u690d\u7269\u5206\u5e03\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08309", "pdf": "https://arxiv.org/pdf/2602.08309", "abs": "https://arxiv.org/abs/2602.08309", "authors": ["Yunzuo Hu", "Wen Li", "Jing Zhang"], "title": "CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment", "categories": ["cs.CV"], "comment": "13 pages, 8 figures", "summary": "Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.", "AI": {"tldr": "CAE-AV\u6846\u67b6\u901a\u8fc7\u4e24\u4e2a\u4e92\u8865\u6a21\u5757\uff08CASTE\u548cCASE\uff09\u89e3\u51b3\u97f3\u89c6\u9891\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5229\u7528\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u6307\u5bfc\u548c\u5b57\u5e55\u5bf9\u9f50\u589e\u5f3a\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u97f3\u89c6\u9891\u5b66\u4e60\u9762\u4e34\u6a21\u6001\u4e0d\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5305\u62ec\u79bb\u5c4f\u58f0\u6e90\u548c\u80cc\u666f\u5e72\u6270\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u653e\u5927\u4e0d\u76f8\u5173\u533a\u57df\u6216\u65f6\u523b\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8868\u5f81\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faCAE-AV\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u6a21\u5757\uff1a1) CASTE\u901a\u8fc7\u8bc4\u4f30\u5e27\u7ea7\u97f3\u89c6\u9891\u4e00\u81f4\u6027\u52a8\u6001\u5e73\u8861\u65f6\u7a7a\u5173\u7cfb\uff1b2) CASE\u5c06\u8de8\u6a21\u6001\u8bed\u4e49\u6307\u5bfc\u6ce8\u5165\u9009\u5b9a\u7684\u65f6\u7a7a\u4f4d\u7f6e\u3002\u8fd8\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u76ee\u6807\u51fd\u6570\uff1a\u5b57\u5e55\u5230\u6a21\u6001InfoNCE\u3001\u89c6\u89c9-\u97f3\u9891\u4e00\u81f4\u6027\u548c\u71b5\u6b63\u5219\u5316\u3002", "result": "\u5728\u51bb\u7ed3\u9aa8\u5e72\u7f51\u7edc\u7684\u60c5\u51b5\u4e0b\uff0cCAE-AV\u5728AVE\u3001AVVP\u3001AVS\u548cAVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b9a\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u5bf9\u97f3\u89c6\u9891\u4e0d\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CAE-AV\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u6307\u5bfc\u548c\u5b57\u5e55\u5bf9\u9f50\u589e\u5f3a\u6709\u6548\u7f13\u89e3\u97f3\u89c6\u9891\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u97f3\u89c6\u9891\u5b66\u4e60\u7684\u8868\u5f81\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.08337", "pdf": "https://arxiv.org/pdf/2602.08337", "abs": "https://arxiv.org/abs/2602.08337", "authors": ["Sheng Yan", "Yong Wang", "Xin Du", "Junsong Yuan", "Mengyuan Liu"], "title": "Language-Guided Transformer Tokenizer for Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u8a00\u5f15\u5bfc\u7684\u6807\u8bb0\u5316\u65b9\u6cd5(LG-Tok)\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u8fd0\u52a8\u6570\u636e\u7684\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\u964d\u4f4e\u751f\u6210\u590d\u6742\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u79bb\u6563\u6807\u8bb0\u5316\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1a\u589e\u52a0\u6807\u8bb0\u6570\u91cf\u53ef\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\uff0c\u4f46\u4f1a\u589e\u52a0\u751f\u6210\u6a21\u578b\u5b66\u4e60\u96be\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u53c8\u80fd\u964d\u4f4e\u751f\u6210\u590d\u6742\u5ea6\u7684\u6807\u8bb0\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8bed\u8a00\u5f15\u5bfc\u6807\u8bb0\u5316(LG-Tok)\uff0c\u5728\u6807\u8bb0\u5316\u9636\u6bb5\u5bf9\u9f50\u81ea\u7136\u8bed\u8a00\u4e0e\u8fd0\u52a8\u6570\u636e\uff0c\u751f\u6210\u7d27\u51d1\u7684\u9ad8\u5c42\u8bed\u4e49\u8868\u793a\u3002\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u6807\u8bb0\u5668\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u652f\u6301\u5168\u5c40\u8bed\u8a00\u6307\u5bfc\uff0c\u5e76\u8bbe\u8ba1\u8bed\u8a00\u4e22\u5f03\u65b9\u6848\uff0c\u4f7f\u89e3\u6807\u8bb0\u5668\u652f\u6301\u65e0\u8bed\u8a00\u6307\u5bfc\u7684\u751f\u6210\u3002", "result": "\u5728HumanML3D\u548cMotion-X\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLG-Tok\u83b7\u5f97Top-1\u5206\u65700.542\u548c0.582\uff0c\u4f18\u4e8eSOTA\u65b9\u6cd5(MARDM: 0.500\u548c0.528)\uff1bFID\u5206\u6570\u5206\u522b\u4e3a0.057\u548c0.088\uff0c\u4f18\u4e8e0.114\u548c0.147\u3002LG-Tok-mini\u4ec5\u4f7f\u7528\u4e00\u534a\u6807\u8bb0\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684\u6807\u8bb0\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fd0\u52a8\u6807\u8bb0\u5316\u7684\u8d28\u91cf\u4e0e\u590d\u6742\u5ea6\u77db\u76fe\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8fd0\u52a8\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2602.08342", "pdf": "https://arxiv.org/pdf/2602.08342", "abs": "https://arxiv.org/abs/2602.08342", "authors": ["Jie Zhang", "Xingtong Yu", "Yuan Fang", "Rudi Stouffs", "Zdravko Trivic"], "title": "UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.", "AI": {"tldr": "UGData\u6570\u636e\u96c6\u5c06\u8857\u666f\u56fe\u50cf\u4e0e\u7a7a\u95f4\u56fe\u5bf9\u9f50\uff0cUGE\u8bad\u7ec3\u7b56\u7565\u901a\u8fc7\u6307\u4ee4\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u548c\u56fe\u7f16\u7801\u5bf9\u9f50\u56fe\u50cf\u3001\u6587\u672c\u4e0e\u7a7a\u95f4\u7ed3\u6784\uff0cUGBench\u57fa\u51c6\u8bc4\u4f30\u7a7a\u95f4\u5d4c\u5165\u5728\u591a\u79cd\u57ce\u5e02\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u57ce\u5e02\u7406\u89e3\u672c\u8d28\u4e0a\u662f\u7a7a\u95f4\u6027\u7684\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u8857\u666f\u56fe\u50cf\u4e0e\u57ce\u5e02\u7ed3\u6784\u7684\u660e\u786e\u5bf9\u9f50\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u8fc1\u79fb\u591a\u6a21\u6001\u5d4c\u5165\u7684\u5b66\u4e60\u3002", "method": "1) \u5f15\u5165UGData\u6570\u636e\u96c6\uff0c\u5c06\u8857\u666f\u56fe\u50cf\u951a\u5b9a\u5230\u7ed3\u6784\u5316\u7a7a\u95f4\u56fe\uff0c\u63d0\u4f9b\u7a7a\u95f4\u63a8\u7406\u8def\u5f84\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u63cf\u8ff0\uff1b2) \u63d0\u51faUGE\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u6307\u4ee4\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u548c\u57fa\u4e8e\u56fe\u7684\u7a7a\u95f4\u7f16\u7801\uff0c\u9010\u6b65\u5bf9\u9f50\u56fe\u50cf\u3001\u6587\u672c\u548c\u7a7a\u95f4\u7ed3\u6784\uff1b3) \u4f7f\u7528LoRA\u5fae\u8c03\u5728\u591a\u4e2aVLM\u9aa8\u5e72\u4e0a\u8bad\u7ec3\u56fa\u5b9a\u7ef4\u5ea6\u7a7a\u95f4\u5d4c\u5165\u3002", "result": "\u57fa\u4e8eQwen2.5-VL-7B\u9aa8\u5e72\u7684UGE\u5728\u8bad\u7ec3\u57ce\u5e02\u4e0a\u56fe\u50cf\u68c0\u7d22\u63d0\u534744%\uff0c\u5730\u7406\u4f4d\u7f6e\u6392\u5e8f\u63d0\u534730%\uff1b\u5728\u672a\u89c1\u57ce\u5e02\u4e0a\u5206\u522b\u63d0\u534730%\u548c22%\uff0c\u8bc1\u660e\u663e\u5f0f\u7a7a\u95f4\u63a5\u5730\u5bf9\u7a7a\u95f4\u5bc6\u96c6\u578b\u57ce\u5e02\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u663e\u5f0f\u7a7a\u95f4\u63a5\u5730\u5bf9\u4e8e\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u57ce\u5e02\u591a\u6a21\u6001\u5d4c\u5165\u81f3\u5173\u91cd\u8981\uff0cUGData\u3001UGE\u548cUGBench\u4e3a\u7a7a\u95f4\u5bc6\u96c6\u578b\u57ce\u5e02\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2602.08346", "pdf": "https://arxiv.org/pdf/2602.08346", "abs": "https://arxiv.org/abs/2602.08346", "authors": ["Yujin Zhou", "Pengcheng Wen", "Jiale Chen", "Boqin Yin", "Han Zhu", "Jiaming Ji", "Juntao Dai", "Chi-Min Chan", "Sirui Han"], "title": "What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\"\u56fe\u50cf\u601d\u7ef4\"\u8303\u5f0f\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u57fa\u51c6\uff0c\u5305\u542b1,206\u6761\u4eba\u5de5\u6807\u6ce8\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5b9a\u4e49\u4e867\u79cd\u7ec6\u7c92\u5ea6\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u53d1\u73b0\u5f53\u524dLVLM\u4f5c\u4e3aPRM\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u53d1\u5c55\uff0c\"\u56fe\u50cf\u601d\u7ef4\"\u8303\u5f0f\u5141\u8bb8\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u7f16\u8f91\u548c\u91cd\u65b0\u7f16\u7801\u89c6\u89c9\u4fe1\u606f\uff0c\u4f46\u8fd9\u5f15\u5165\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u591a\u6837\u5316\u9519\u8bef\u3002\u73b0\u6709\u7684PRM\u57fa\u51c6\u4e3b\u8981\u662f\u6587\u672c\u4e2d\u5fc3\u7684\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8fd9\u4e00\u65b0\u8303\u5f0f\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "1. \u901a\u8fc7\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u548cPRM\u5f15\u5bfc\u641c\u7d22\u5b9e\u9a8c\uff0c\u5b9a\u4e49\u4e867\u79cd\u7ec6\u7c92\u5ea6\u9519\u8bef\u7c7b\u578b\uff1b2. \u6784\u5efa\u5305\u542b1,206\u6761\u4eba\u5de5\u6807\u6ce8\u63a8\u7406\u8f68\u8ff9\u7684\u57fa\u51c6\uff0c\u6db5\u76d64\u4e2a\u7c7b\u522b\u548c16\u4e2a\u5b50\u7c7b\u522b\uff1b3. \u5b9e\u9a8c\u5206\u6790\u5f53\u524dLVLM\u4f5c\u4e3aPRM\u7684\u6027\u80fd\u3002", "result": "\u5f53\u524dLVLM\u4f5c\u4e3aPRM\u8868\u73b0\u4e0d\u8db3\uff1a\u89c6\u89c9\u63a8\u7406\u8fc7\u7a0b\u8bc4\u4f30\u80fd\u529b\u6709\u9650\uff0c\u5728\u4e0d\u540c\u9519\u8bef\u7c7b\u578b\u95f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5b58\u5728\u6b63\u5411\u8bc4\u4f30\u504f\u5dee\uff0c\u4e14\u5bf9\u63a8\u7406\u6b65\u9aa4\u4f4d\u7f6e\u654f\u611f\u3002\u57fa\u51c6\u9a8c\u8bc1\u4e86\u4e13\u95e8PRM\u7684\u5fc5\u8981\u6027\u548c\u6539\u8fdb\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLVLM\u4e2dPRM\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u5efa\u7acb\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u6709\u6548\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765PRM\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u5173\u952e\u6307\u5bfc\u3002"}}
{"id": "2602.08355", "pdf": "https://arxiv.org/pdf/2602.08355", "abs": "https://arxiv.org/abs/2602.08355", "authors": ["Xianjie Liu", "Yiman Hu", "Liang Wu", "Ping Hu", "Yixiong Zou", "Jian Xu", "Bo Zheng"], "title": "E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs", "categories": ["cs.CV"], "comment": null, "summary": "E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \\textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \\textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \\textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \\textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u7535\u5546\u77ed\u89c6\u9891\u7406\u89e3\u7684\u57fa\u51c6\u6570\u636e\u96c6E-VAds\uff0c\u5305\u542b3,961\u4e2a\u9ad8\u8d28\u91cf\u89c6\u9891\u548c19,785\u4e2a\u95ee\u7b54\u5bf9\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u6a21\u578bE-VAds-R1\uff0c\u5728\u5546\u4e1a\u610f\u56fe\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86109.2%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7535\u5546\u77ed\u89c6\u9891\u4f5c\u4e3a\u5728\u7ebf\u89c6\u9891\u884c\u4e1a\u7684\u9ad8\u6536\u5165\u7ec6\u5206\u9886\u57df\uff0c\u5177\u6709\u76ee\u6807\u9a71\u52a8\u548c\u591a\u6a21\u6001\u4fe1\u53f7\u5bc6\u96c6\u7684\u7279\u70b9\u3002\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u8fd9\u7c7b\u89c6\u9891\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u56e0\u4e3a\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u5546\u4e1a\u610f\u56fe\u63a8\u7406\u3002\u7535\u5546\u5185\u5bb9\u5728\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\u4e0a\u7684\u4fe1\u606f\u5bc6\u5ea6\u8fdc\u9ad8\u4e8e\u4e3b\u6d41\u6570\u636e\u96c6\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u5e26\u6765\u4e86\u66f4\u5927\u6311\u6218\u3002", "method": "1. \u63d0\u51fa\u591a\u6a21\u6001\u4fe1\u606f\u5bc6\u5ea6\u8bc4\u4f30\u6846\u67b6\u6765\u91cf\u5316\u7535\u5546\u9886\u57df\u7684\u590d\u6742\u6027\uff1b2. \u6784\u5efaE-VAds\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b3,961\u4e2a\u6dd8\u5b9d\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u751f\u621019,785\u4e2a\u5f00\u653e\u5f0f\u95ee\u7b54\u5bf9\uff0c\u7ec4\u7ec7\u4e3a\u611f\u77e5\u4e0e\u8ba4\u77e5\u63a8\u7406\u4e24\u4e2a\u7ef4\u5ea6\u4e94\u4e2a\u4efb\u52a1\uff1b3. \u5f00\u53d1E-VAds-R1\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u591a\u7c92\u5ea6\u5956\u52b1\u7b56\u7565MG-GRPO\uff0c\u4e3a\u65e9\u671f\u63a2\u7d22\u63d0\u4f9b\u5e73\u6ed1\u6307\u5bfc\uff0c\u540c\u65f6\u4e3a\u4e13\u5bb6\u7ea7\u7cbe\u5ea6\u521b\u5efa\u975e\u7ebf\u6027\u6fc0\u52b1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cE-VAds-R1\u5728\u4ec5\u4f7f\u7528\u6570\u767e\u4e2a\u8bad\u7ec3\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5546\u4e1a\u610f\u56fe\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86109.2%\u7684\u6027\u80fd\u63d0\u5347\u3002\u8bc4\u4f30\u663e\u793a\u7535\u5546\u5185\u5bb9\u5728\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\u4e0a\u7684\u4fe1\u606f\u5bc6\u5ea6\u663e\u8457\u9ad8\u4e8e\u4e3b\u6d41\u6570\u636e\u96c6\uff0c\u8bc1\u5b9e\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u7535\u5546\u77ed\u89c6\u9891\u7406\u89e3\u9886\u57df\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u6784\u5efa\u4e13\u95e8\u7684\u57fa\u51c6\u6570\u636e\u96c6E-VAds\u548c\u5f00\u53d1\u9ad8\u6548\u7684\u63a8\u7406\u6a21\u578bE-VAds-R1\uff0c\u4e3a\u7535\u5546\u89c6\u9891\u7406\u89e3\u5efa\u7acb\u4e86\u65b0\u7684\u7814\u7a76\u524d\u6cbf\u3002\u63d0\u51fa\u7684\u591a\u7c92\u5ea6\u5956\u52b1\u7b56\u7565MG-GRPO\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u7535\u5546\u77ed\u89c6\u9891\u7684\u5546\u4e1a\u610f\u56fe\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08388", "pdf": "https://arxiv.org/pdf/2602.08388", "abs": "https://arxiv.org/abs/2602.08388", "authors": ["Shuo Zhang", "Wenzhuo Wu", "Huayu Zhang", "Jiarong Cheng", "Xianghao Zang", "Chao Ban", "Hao Sun", "Zhongjiang He", "Tianwei Cao", "Kongming Liang", "Zhanyu Ma"], "title": "Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.", "AI": {"tldr": "GeoEdit\uff1a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u51e0\u4f55\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u751f\u6210\u548c\u6548\u679c\u654f\u611f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u7269\u4f53\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u7f29\u653e\u7f16\u8f91\uff0c\u5e76\u751f\u6210\u903c\u771f\u7684\u5149\u5f71\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u51e0\u4f55\u53d8\u6362\uff08\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u7f29\u653e\uff09\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a1\uff09\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u51e0\u4f55\u7f16\u8f91\uff1b2\uff09\u5bf9\u590d\u6742\u5149\u5f71\u6548\u679c\u5efa\u6a21\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u771f\u5b9e\u3002", "method": "\u63d0\u51faGeoEdit\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u6a21\u5757\uff0c\u96c6\u6210\u51e0\u4f55\u53d8\u6362\u5b9e\u73b0\u7cbe\u786e\u7269\u4f53\u7f16\u8f91\uff1b2\uff09\u6548\u679c\u654f\u611f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u590d\u6742\u5149\u5f71\u6548\u679c\u5efa\u6a21\uff1b3\uff09\u6784\u5efaRS-Objects\u5927\u89c4\u6a21\u51e0\u4f55\u7f16\u8f91\u6570\u636e\u96c6\uff0812\u4e07+\u9ad8\u8d28\u91cf\u56fe\u50cf\u5bf9\uff09\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeoEdit\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u771f\u5b9e\u611f\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GeoEdit\u901a\u8fc7\u521b\u65b0\u7684\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u548c\u6548\u679c\u654f\u611f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u573a\u666f\u4e2d\u51e0\u4f55\u7f16\u8f91\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7269\u4f53\u53d8\u6362\u548c\u903c\u771f\u7684\u5149\u5f71\u6548\u679c\u751f\u6210\u3002"}}
{"id": "2602.08395", "pdf": "https://arxiv.org/pdf/2602.08395", "abs": "https://arxiv.org/abs/2602.08395", "authors": ["Jianfeng Liang", "Shaocheng Shen", "Botao Xu", "Qiang Hu", "Xiaoyun Zhang"], "title": "D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy", "categories": ["cs.CV"], "comment": null, "summary": "The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \\textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \\textbf{12$\\times$}", "AI": {"tldr": "D\u00b2-VR\uff1a\u4e00\u79cd\u57fa\u4e8e\u5355\u56fe\u50cf\u6269\u6563\u7684\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u964d\u566a\u9c81\u68d2\u6d41\u5bf9\u9f50\u548c\u5bf9\u6297\u84b8\u998f\u5b9e\u73b012\u500d\u52a0\u901f\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u65f6\u95f4\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u4f9b\u51fa\u8272\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u771f\u5b9e\u4e16\u754c\u9000\u5316\u65f6\u9762\u4e34\u63a8\u7406\u5ef6\u8fdf\u9ad8\u548c\u65f6\u95f4\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72", "method": "1. \u8bbe\u8ba1\u964d\u566a\u9c81\u68d2\u6d41\u5bf9\u9f50\u6a21\u5757\uff0c\u5229\u7528\u7f6e\u4fe1\u5ea6\u611f\u77e5\u6ce8\u610f\u529b\u8fc7\u6ee4\u4e0d\u53ef\u9760\u8fd0\u52a8\u7ebf\u7d22\uff1b2. \u91c7\u7528\u5bf9\u6297\u84b8\u998f\u8303\u5f0f\u5c06\u6269\u6563\u91c7\u6837\u8f68\u8ff9\u538b\u7f29\u5230\u5feb\u901f\u5c11\u6b65\u673a\u5236\uff1b3. \u8bbe\u8ba1\u534f\u540c\u4f18\u5316\u7b56\u7565\u5e73\u8861\u611f\u77e5\u8d28\u91cf\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027", "result": "D\u00b2-VR\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u91c7\u6837\u8fc7\u7a0b\u52a0\u901f12\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u4fee\u590d\u6846\u67b6\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5ef6\u8fdf\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u4fee\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08397", "pdf": "https://arxiv.org/pdf/2602.08397", "abs": "https://arxiv.org/abs/2602.08397", "authors": ["Chiara Lena", "Davide Milesi", "Alessandro Casella", "Luca Carlini", "Joseph C. Norton", "James Martin", "Bruno Scaglioni", "Keith L. Obstein", "Roberto De Sire", "Marco Spadaccini", "Cesare Hassan", "Pietro Valdastri", "Elena De Momi"], "title": "RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.", "AI": {"tldr": "RealSynCol\u662f\u4e00\u4e2a\u9ad8\u5ea6\u903c\u771f\u7684\u5408\u6210\u7ed3\u80a0\u955c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u7ed3\u80a0\u955c3D\u91cd\u5efa\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e34\u5e8a\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6709\u6f5c\u529b\u901a\u8fc73D\u91cd\u5efa\u7ed3\u80a0\u6765\u6539\u8fdb\u7ed3\u80a0\u955c\u68c0\u67e5\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u9ecf\u819c\u8868\u9762\u548c\u75c5\u53d8\u89c6\u56fe\uff0c\u5e76\u5e2e\u52a9\u8bc6\u522b\u672a\u63a2\u7d22\u533a\u57df\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u9650\u5236\u4e86\u7a33\u5065\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u4ece10\u4e2aCT\u626b\u63cf\u4e2d\u63d0\u53d6\u7ed3\u80a0\u51e0\u4f55\u7ed3\u6784\uff0c\u5bfc\u5165\u5230\u6a21\u62df\u672f\u4e2d\u6761\u4ef6\u7684\u865a\u62df\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u771f\u5b9e\u8840\u7ba1\u7eb9\u7406\u8fdb\u884c\u6e32\u67d3\u3002\u6570\u636e\u96c6\u5305\u542b28,130\u5e27\u56fe\u50cf\uff0c\u914d\u6709\u6df1\u5ea6\u56fe\u3001\u5149\u6d41\u30013D\u7f51\u683c\u548c\u76f8\u673a\u8f68\u8ff9\u7b49\u771f\u5b9e\u6807\u7b7e\u3002", "result": "\u57fa\u51c6\u7814\u7a76\u8868\u660e\uff0cRealSynCol\u7684\u9ad8\u771f\u5b9e\u6027\u548c\u53d8\u5f02\u6027\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e34\u5e8a\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8bc1\u660e\u5b83\u662f\u5f00\u53d1\u652f\u6301\u5185\u955c\u8bca\u65ad\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u5f3a\u5927\u5de5\u5177\u3002", "conclusion": "RealSynCol\u5408\u6210\u6570\u636e\u96c6\u4e3a\u89e3\u51b3\u7ed3\u80a0\u955c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u9ad8\u771f\u5b9e\u6027\u548c\u4e30\u5bcc\u6807\u6ce8\u4f7f\u5176\u6210\u4e3a\u5f00\u53d1\u4e34\u5e8a\u9002\u7528\u7b97\u6cd5\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.08430", "pdf": "https://arxiv.org/pdf/2602.08430", "abs": "https://arxiv.org/abs/2602.08430", "authors": ["Qiang Wang"], "title": "Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features", "categories": ["cs.CV"], "comment": null, "summary": "We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.", "AI": {"tldr": "\u91cd\u65b0\u5ba1\u89c6\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7a00\u758f\u56fe\u50cf\u5339\u914d\u6a21\u578b\u8bad\u7ec3\uff0c\u53d1\u73b0\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u5f71\u54cdLightGlue\u6027\u80fd\uff0c\u68c0\u6d4b\u5668\u6bd4\u63cf\u8ff0\u7b26\u5bf9\u6027\u80fd\u5f71\u54cd\u66f4\u5927\uff0c\u63d0\u51fa\u7528\u591a\u79cd\u68c0\u6d4b\u5668\u5173\u952e\u70b9\u5fae\u8c03\u7684\u901a\u7528\u6a21\u578b\uff0c\u5728\u96f6\u6837\u672c\u5339\u914d\u4e2d\u8fbe\u5230\u6216\u8d85\u8fc7\u4e13\u7528\u8bad\u7ec3\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7a00\u758f\u56fe\u50cf\u5339\u914d\u6a21\u578b\u8bad\u7ec3\u95ee\u9898\uff0c\u8bc6\u522b\u4e4b\u524d\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u7814\u7a76\u68c0\u6d4b\u5668\u548c\u63cf\u8ff0\u7b26\u5728\u57fa\u4e8etransformer\u7684\u5339\u914d\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\uff0c\u65e8\u5728\u5f00\u53d1\u901a\u7528\u7684\u3001\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u56fe\u50cf\u5339\u914d\u6a21\u578b\u3002", "method": "\u9996\u5148\u8bc6\u522b\u5f71\u54cdLightGlue\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff1b\u7136\u540e\u5206\u6790\u68c0\u6d4b\u5668\u548c\u63cf\u8ff0\u7b26\u5728transformer\u5339\u914d\u6846\u67b6\u4e2d\u7684\u89d2\u8272\uff1b\u6700\u540e\u63d0\u51fa\u4f7f\u7528\u591a\u79cd\u68c0\u6d4b\u5668\u7684\u5173\u952e\u70b9\u5bf9\u73b0\u6709\u56fe\u50cf\u5339\u914d\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u901a\u7528\u7684\u68c0\u6d4b\u5668\u65e0\u5173\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u68c0\u6d4b\u5668\uff08\u800c\u975e\u63cf\u8ff0\u7b26\uff09\u901a\u5e38\u662f\u6027\u80fd\u5dee\u5f02\u7684\u4e3b\u8981\u539f\u56e0\uff1b\u63d0\u51fa\u7684\u901a\u7528\u6a21\u578b\u5728\u4f5c\u4e3a\u96f6\u6837\u672c\u5339\u914d\u5668\u7528\u4e8e\u65b0\u68c0\u6d4b\u5668\u65f6\uff0c\u80fd\u591f\u8fbe\u5230\u6216\u8d85\u8fc7\u4e13\u95e8\u4e3a\u8fd9\u4e9b\u7279\u5f81\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u57fa\u4e8etransformer\u7684\u5339\u914d\u6a21\u578b\u7684\u90e8\u7f72\u548c\u672a\u6765\u5c40\u90e8\u7279\u5f81\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u5f00\u53d1\u901a\u7528\u3001\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u56fe\u50cf\u5339\u914d\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.08439", "pdf": "https://arxiv.org/pdf/2602.08439", "abs": "https://arxiv.org/abs/2602.08439", "authors": ["Yuhao Dong", "Shulin Tian", "Shuai Liu", "Shuangrui Ding", "Yuhang Zang", "Xiaoyi Dong", "Yuhang Cao", "Jiaqi Wang", "Ziwei Liu"], "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition", "categories": ["cs.CV"], "comment": null, "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.", "AI": {"tldr": "\u63d0\u51faDemo-driven Video In-Context Learning\u4efb\u52a1\u548cDemo-ICL-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs\u4ece\u5c11\u91cf\u89c6\u9891\u793a\u4f8b\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86Demo-ICL\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u6a21\u578b\u57fa\u4e8e\u9759\u6001\u5185\u90e8\u77e5\u8bc6\u7406\u89e3\u89c6\u9891\u7684\u80fd\u529b\uff0c\u800c\u975e\u4ece\u52a8\u6001\u3001\u65b0\u9896\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u5c11\u91cf\u793a\u4f8b\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1) \u63d0\u51faDemo-driven Video In-Context Learning\u4efb\u52a1\uff1b2) \u6784\u5efaDemo-ICL-Bench\u57fa\u51c6\uff0c\u5305\u542b1200\u4e2a\u6559\u5b66YouTube\u89c6\u9891\u53ca\u76f8\u5173\u95ee\u9898\uff1b3) \u5f00\u53d1Demo-ICL\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u89c6\u9891\u76d1\u7763\u5fae\u8c03\u548c\u4fe1\u606f\u8f85\u52a9\u76f4\u63a5\u504f\u597d\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDemo-ICL-Bench\u5bf9\u73b0\u6709\u6700\u5148\u8fdbMLLMs\u5177\u6709\u6311\u6218\u6027\uff0cDemo-ICL\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u89c6\u9891\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u4efb\u52a1\u3001\u57fa\u51c6\u548c\u6a21\u578b\u4e3a\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.08448", "pdf": "https://arxiv.org/pdf/2602.08448", "abs": "https://arxiv.org/abs/2602.08448", "authors": ["Haocheng Lu", "Nan Zhang", "Wei Tao", "Xiaoyang Qu", "Guokuan Li", "Jiguang Wan", "Jianzong Wang"], "title": "Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to AAAI 2026 (Main Technical Track)", "summary": "Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.", "AI": {"tldr": "Vista\u662f\u4e00\u4e2a\u7528\u4e8e\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u573a\u666f\u611f\u77e5\u7684\u5206\u5272\u3001\u538b\u7f29\u548c\u53ec\u56de\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u8fde\u7eed\u89c6\u9891\u6d41\u63a8\u7406\u3002", "motivation": "\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8e\u56fa\u5b9a\u5927\u5c0f\u5185\u5b58\u6216\u7b80\u5355\u538b\u7f29\u7684\u65b9\u6cd5\u5e38\u5bfc\u81f4\u4e0a\u4e0b\u6587\u4e22\u5931\u6216\u5185\u5b58\u6ea2\u51fa\uff0c\u9650\u5236\u4e86\u5728\u957f\u65f6\u3001\u5b9e\u65f6\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "Vista\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u65b9\u9762\uff1a1) \u573a\u666f\u611f\u77e5\u5206\u5272\uff1a\u52a8\u6001\u805a\u7c7b\u8f93\u5165\u5e27\u4e3a\u65f6\u7a7a\u548c\u89c6\u89c9\u8fde\u8d2f\u7684\u573a\u666f\u5355\u5143\uff1b2) \u573a\u666f\u611f\u77e5\u538b\u7f29\uff1a\u5c06\u6bcf\u4e2a\u573a\u666f\u538b\u7f29\u4e3a\u7d27\u51d1\u4ee4\u724c\u8868\u793a\u5b58\u50a8\u5728GPU\u5185\u5b58\uff0c\u5168\u5206\u8fa8\u7387\u5e27\u5378\u8f7d\u5230CPU\u5185\u5b58\uff1b3) \u573a\u666f\u611f\u77e5\u53ec\u56de\uff1a\u67e5\u8be2\u65f6\u9009\u62e9\u6027\u53ec\u56de\u76f8\u5173\u573a\u666f\u5e76\u91cd\u65b0\u6574\u5408\u5230\u6a21\u578b\u8f93\u5165\u3002", "result": "\u5728StreamingBench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVista\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u5efa\u7acb\u4e86\u5f3a\u5927\u57fa\u7ebf\u3002", "conclusion": "Vista\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\uff0c\u5728\u4e0d\u5f71\u54cd\u5ef6\u8fdf\u6216\u5185\u5b58\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u4e3a\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08462", "pdf": "https://arxiv.org/pdf/2602.08462", "abs": "https://arxiv.org/abs/2602.08462", "authors": ["Yiyang Cao", "Yunze Deng", "Ziyu Lin", "Bin Feng", "Xinggang Wang", "Wenyu Liu", "Dandan Zheng", "Jingdong Chen"], "title": "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.", "AI": {"tldr": "TriC-Motion\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4-\u9891\u7387\u4e09\u57df\u8054\u5408\u5efa\u6a21\u4e0e\u56e0\u679c\u5e72\u9884\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u57df\u4fe1\u606f\u878d\u5408\u4e0d\u8db3\u548c\u566a\u58f0\u7ea0\u7f20\u95ee\u9898\uff0c\u5728HumanML3D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4-\u65f6\u95f4\u5efa\u6a21\u6216\u72ec\u7acb\u7684\u9891\u7387\u57df\u5206\u6790\uff0c\u7f3a\u4e4f\u8de8\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u9891\u7387\u57df\u7684\u7edf\u4e00\u8054\u5408\u4f18\u5316\u6846\u67b6\u3002\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u540c\u65f6\u5229\u7528\u6240\u6709\u57df\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u4e2d\uff0c\u566a\u58f0\u5f15\u8d77\u7684\u52a8\u4f5c\u65e0\u5173\u7ebf\u7d22\u5e38\u5e38\u4e0e\u6709\u76ca\u7279\u5f81\u7ea0\u7f20\uff0c\u5bfc\u81f4\u52a8\u4f5c\u5931\u771f\u3002", "method": "\u63d0\u51faTriC-Motion\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u5efa\u6a21\u6a21\u5757\uff1a\u65f6\u95f4\u52a8\u4f5c\u7f16\u7801\u3001\u7a7a\u95f4\u62d3\u6251\u5efa\u6a21\u548c\u6df7\u5408\u9891\u7387\u5206\u6790\u3002\u901a\u8fc7\u8bc4\u5206\u5f15\u5bfc\u7684\u4e09\u57df\u878d\u5408\u6a21\u5757\u6574\u5408\u4e09\u57df\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7a7a\u95f4\u62d3\u6251\u3001\u52a8\u4f5c\u8d8b\u52bf\u548c\u52a8\u6001\u7279\u6027\u3002\u540c\u65f6\u8bbe\u8ba1\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u53cd\u4e8b\u5b9e\u52a8\u4f5c\u89e3\u8026\u5668\uff0c\u66b4\u9732\u52a8\u4f5c\u65e0\u5173\u7ebf\u7d22\u4ee5\u6d88\u9664\u566a\u58f0\uff0c\u89e3\u8026\u5404\u57df\u7684\u771f\u5b9e\u5efa\u6a21\u8d21\u732e\u3002", "result": "\u5728HumanML3D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cR@1\u6307\u6807\u4e3a0.612\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eTriC-Motion\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u8fde\u8d2f\u3001\u591a\u6837\u4e14\u4e0e\u6587\u672c\u5bf9\u9f50\u7684\u52a8\u4f5c\u5e8f\u5217\u3002", "conclusion": "TriC-Motion\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4-\u9891\u7387\u4e09\u57df\u8054\u5408\u5efa\u6a21\u4e0e\u56e0\u679c\u5e72\u9884\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u57df\u4fe1\u606f\u878d\u5408\u4e0d\u8db3\u548c\u566a\u58f0\u7ea0\u7f20\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u6027\u80fd\u548c\u8d28\u91cf\u3002"}}
{"id": "2602.08479", "pdf": "https://arxiv.org/pdf/2602.08479", "abs": "https://arxiv.org/abs/2602.08479", "authors": ["Alif Rizqullah Mahdi", "Mahdi Rezaei", "Natasha Merat"], "title": "Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.HC", "cs.LG"], "comment": "9th International Conference on Instrumentation, Control, and Automation (ICA)", "summary": "Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e2D\u59ff\u6001\u4f30\u8ba1\u7684\u624b\u52bf\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7406\u89e3\u884c\u4eba\u624b\u52bf\uff0c\u5728\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\u4e0a\u5b9e\u73b087%\u7684\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u624b\u52bf\u662f\u4ea4\u901a\u4e2d\u975e\u8bed\u8a00\u4ea4\u6d41\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u6709\u52a9\u4e8e\u884c\u4eba\u4e0e\u9a7e\u9a76\u5458\u4e92\u52a8\uff0c\u4f46\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u96be\u4ee5\u89e3\u91ca\u8fd9\u4e9b\u624b\u52bf\uff0c\u9700\u8981\u4e13\u95e8\u7684\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u6765\u6539\u5584AV\u7cfb\u7edf\u7684\u611f\u77e5\u80fd\u529b", "method": "\u4f7f\u75282D\u59ff\u6001\u4f30\u8ba1\u5904\u7406WIVW\u6570\u636e\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u5e8f\u5217\uff0c\u5c06\u624b\u52bf\u5206\u4e3a\u56db\u7c7b\uff08\u505c\u6b62\u3001\u524d\u8fdb\u3001\u611f\u8c22\u95ee\u5019\u3001\u65e0\u624b\u52bf\uff09\uff0c\u4ece\u5f52\u4e00\u5316\u7684\u5173\u952e\u70b9\u63d0\u53d676\u4e2a\u9759\u6001\u548c\u52a8\u6001\u7279\u5f81", "result": "\u5206\u6790\u663e\u793a\u624b\u90e8\u4f4d\u7f6e\u548c\u8fd0\u52a8\u901f\u5ea6\u5bf9\u533a\u5206\u624b\u52bf\u7c7b\u522b\u7279\u522b\u6709\u6548\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523087%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u611f\u77e5\u80fd\u529b", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u624b\u52bf\u7406\u89e3\u80fd\u529b\uff0c\u8fd8\u4e3a\u66f4\u5e7f\u6cdb\u5730\u7406\u89e3\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u884c\u4eba\u884c\u4e3a\u505a\u51fa\u4e86\u8d21\u732e"}}
{"id": "2602.08491", "pdf": "https://arxiv.org/pdf/2602.08491", "abs": "https://arxiv.org/abs/2602.08491", "authors": ["Keonvin Park", "Aditya Pal", "Jin Hong Mok"], "title": "Enhanced Food Category Recognition under Illumination-Induced Domain Shift", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.\n  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.\n  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5149\u7167\u53d8\u5316\u5bf9\u591a\u7c7b\u522b\u98df\u7269\u8bc6\u522b\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5408\u6210\u5149\u7167\u589e\u5f3a\u6570\u636e\u96c6\u548c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u63d0\u5347\u5149\u7167\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u89c6\u89c9\u98df\u7269\u8bc6\u522b\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u4f20\u9001\u5e26\u68c0\u6d4b\uff09\u5bf9\u5149\u7167\u53d8\u5316\u5f15\u8d77\u7684\u57df\u504f\u79fb\u975e\u5e38\u654f\u611f\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u98df\u7269\u7c7b\u522b\u6216\u53d7\u63a7\u73af\u5883\uff0c\u4e14\u5927\u591a\u6570\u516c\u5171\u98df\u7269\u6570\u636e\u96c6\u7f3a\u4e4f\u660e\u786e\u7684\u5149\u7167\u6807\u6ce8\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5149\u7167\u5f15\u8d77\u7684\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "\u4f7f\u7528Food-101\u548cFruits-360\u4e24\u4e2a\u5e7f\u6cdb\u91c7\u7528\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u53d8\u5316\u5149\u6e29\u548c\u5f3a\u5ea6\u6784\u5efa\u5408\u6210\u5149\u7167\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u57df\u6cdb\u5316\u7814\u7a76\uff0c\u7279\u522b\u5173\u6ce8\u82f9\u679c\u7c7b\u7b49\u5149\u7167\u654f\u611f\u76ee\u6807\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u7531\u4e8e\u89c6\u89c9\u6761\u4ef6\u4e0d\u5339\u914d\u5bfc\u81f4\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff0c\u800c\u5149\u7167\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u9ad8\u4e86\u57df\u504f\u79fb\u4e0b\u7684\u8bc6\u522b\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u5149\u7167\u9c81\u68d2\u6027\u5bf9\u4e8e\u5728\u73b0\u5b9e\u4e16\u754c\u68c0\u6d4b\u573a\u666f\u4e2d\u90e8\u7f72\u53ef\u9760\u7684\u98df\u7269\u8bc6\u522b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u5149\u7167\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u5728\u5149\u7167\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.08503", "pdf": "https://arxiv.org/pdf/2602.08503", "abs": "https://arxiv.org/abs/2602.08503", "authors": ["Yi Ding", "Ziliang Qiu", "Bolian Li", "Ruqi Zhang"], "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "17 pages", "summary": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\\times$ training time per step.", "AI": {"tldr": "Octopus\u6846\u67b6\u901a\u8fc7\u91cd\u7ec4\u73b0\u6709rollouts\u5408\u6210\u5bc6\u96c6\u7684\u81ea\u6821\u6b63\u793a\u4f8b\uff0c\u89e3\u51b3RL\u4e2d\u81ea\u6821\u6b63\u884c\u4e3a\u7a00\u758f\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u54cd\u5e94\u63a9\u7801\u7b56\u7565\u89e3\u8026\u81ea\u6821\u6b63\u4e0e\u76f4\u63a5\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u81ea\u6821\u6b63\u7684VLM\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b66\u4e60\u81ea\u6821\u6b63\u884c\u4e3a\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u6709\u6548\u7684\u81ea\u6821\u6b63\u884c\u4e3a\u51fa\u73b0\u9891\u7387\u6781\u4f4e\uff0c\u5bfc\u81f4\u5b66\u4e60\u4fe1\u53f7\u6781\u5176\u7a00\u758f\uff0c\u96be\u4ee5\u6709\u6548\u5b66\u4e60\u3002", "method": "\u63d0\u51facorrection-specific rollouts (Octopus)\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u91cd\u7ec4\u73b0\u6709rollouts\u5408\u6210\u5bc6\u96c6\u7684\u81ea\u6821\u6b63\u793a\u4f8b\uff1b2\uff09\u5f15\u5165\u54cd\u5e94\u63a9\u7801\u7b56\u7565\uff0c\u89e3\u8026\u81ea\u6821\u6b63\u4e0e\u76f4\u63a5\u63a8\u7406\uff1b3\uff09\u57fa\u4e8e\u6b64\u6784\u5efaOctopus-8B\u6a21\u578b\uff0c\u5177\u5907\u53ef\u63a7\u81ea\u6821\u6b63\u80fd\u529b\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOctopus-8B\u5728\u5f00\u6e90VLM\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u6700\u4f73RLVR\u57fa\u7ebf\u63d0\u9ad81.0\u5206\uff0c\u540c\u65f6\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u4ec5\u97000.72\u500d\u7684\u65f6\u95f4\u3002", "conclusion": "Octopus\u6846\u67b6\u901a\u8fc7rollout\u91cd\u7ec4\u548c\u54cd\u5e94\u63a9\u7801\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u6821\u6b63\u5b66\u4e60\u4e2d\u7684\u7a00\u758f\u4fe1\u53f7\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u63a7\u7684\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08505", "pdf": "https://arxiv.org/pdf/2602.08505", "abs": "https://arxiv.org/abs/2602.08505", "authors": ["Caterina Fuster-Barcel\u00f3", "Virginie Uhlmann"], "title": "Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?", "categories": ["cs.CV"], "comment": null, "summary": "Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fr\u00e9chet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u53d1\u73b0\u867d\u7136\u5355\u4e2a\u7535\u5b50\u663e\u5fae\u955c\u6570\u636e\u96c6\u4e0a\u80fd\u83b7\u5f97\u826f\u597d\u5206\u5272\u6027\u80fd\uff0c\u4f46\u8de8\u5f02\u6784\u6570\u636e\u96c6\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5f53\u524d\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u8de8\u57df\u9c81\u68d2\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5176\u6f5c\u5728\u8868\u793a\u662f\u5426\u8db3\u591f\u901a\u7528\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u8de8\u5f02\u6784\u663e\u5fae\u955c\u56fe\u50cf\u6570\u636e\u96c6\u7684\u8fc1\u79fb\u548c\u91cd\u7528\u3002\u672c\u7814\u7a76\u9488\u5bf9\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u7ebf\u7c92\u4f53\u5206\u5272\u95ee\u9898\uff0c\u63a2\u7a76\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u516c\u5171\u7535\u5b50\u663e\u5fae\u955c\u6570\u636e\u96c6\uff08Lucchi++\u548cVNC\uff09\u548c\u4e09\u4e2a\u4ee3\u8868\u6027\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08DINOv2\u3001DINOv3\u548cOpenCLIP\uff09\u3002\u8bc4\u4f30\u4e24\u79cd\u5b9e\u9645\u6a21\u578b\u9002\u5e94\u673a\u5236\uff1a\u51bb\u7ed3\u9aa8\u5e72\u7f51\u7edc\u4ec5\u8bad\u7ec3\u8f7b\u91cf\u5206\u5272\u5934\uff0c\u4ee5\u53ca\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002\u91c7\u7528\u591a\u79cd\u6280\u672f\uff08PCA\u3001Fr\u00e9chet Dinov2\u8ddd\u79bb\u3001\u7ebf\u6027\u63a2\u9488\uff09\u63a2\u7d22\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u3002", "result": "\u5728\u6240\u6709\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0c\u5355\u4e2aEM\u6570\u636e\u96c6\u8bad\u7ec3\u53ef\u83b7\u5f97\u826f\u597d\u5206\u5272\u6027\u80fd\uff08\u524d\u666f\u4ea4\u5e76\u6bd4\uff09\uff0cLoRA\u6301\u7eed\u63d0\u5347\u57df\u5185\u6027\u80fd\u3002\u7136\u800c\uff0c\u8de8\u591a\u4e2aEM\u6570\u636e\u96c6\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6240\u6709\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0cPEFT\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u5584\u3002\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u5206\u6790\u663e\u793a\u4e24\u4e2aEM\u6570\u636e\u96c6\u95f4\u5b58\u5728\u663e\u8457\u4e14\u6301\u7eed\u7684\u57df\u4e0d\u5339\u914d\uff0c\u5c3d\u7ba1\u5b83\u4eec\u89c6\u89c9\u76f8\u4f3c\u3002", "conclusion": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u8f7b\u91cf\u9002\u5e94\u4e0b\u53ef\u5728\u5355\u4e2a\u57df\u5185\u4e3aEM\u5206\u5272\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u4f46\u5f53\u524dPEFT\u7b56\u7565\u4e0d\u8db3\u4ee5\u83b7\u5f97\u8de8\u5f02\u6784EM\u6570\u636e\u96c6\u7684\u5355\u4e00\u9c81\u68d2\u6a21\u578b\uff0c\u9700\u8981\u989d\u5916\u7684\u57df\u5bf9\u9f50\u673a\u5236\u3002"}}
{"id": "2602.08524", "pdf": "https://arxiv.org/pdf/2602.08524", "abs": "https://arxiv.org/abs/2602.08524", "authors": ["Linger Deng", "Yuliang Liu", "Wenwen Yu", "Zujia Zhang", "Jianzhong Ju", "Zhenbo Luo", "Xiang Bai"], "title": "GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving", "categories": ["cs.CV"], "comment": null, "summary": "Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus", "AI": {"tldr": "GeoFocus\u662f\u4e00\u4e2a\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\u7684\u591a\u6a21\u6001\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u5c40\u90e8\u611f\u77e5\u5668\u548cVertexLang\u62d3\u6251\u8bed\u8a00\uff0c\u663e\u8457\u63d0\u5347\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u80fd\u529b\u3002", "motivation": "\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u5bf9\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5168\u5c40\u5f62\u72b6\u8bc6\u522b\u548c\u5173\u6ce8\u4e0e\u51e0\u4f55\u7406\u8bba\u76f8\u5173\u7684\u590d\u6742\u5c40\u90e8\u5173\u7cfb\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5c40\u90e8\u7279\u5f81\u8986\u76d6\u548c\u5168\u5c40\u62d3\u6251\u8868\u793a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGeoFocus\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u5173\u952e\u5c40\u90e8\u611f\u77e5\u5668\uff0c\u901a\u8fc713\u4e2a\u57fa\u4e8e\u7406\u8bba\u7684\u611f\u77e5\u6a21\u677f\u81ea\u52a8\u8bc6\u522b\u548c\u5f3a\u8c03\u5173\u952e\u5c40\u90e8\u7ed3\u6784\uff1b2) VertexLang\uff0c\u4e00\u79cd\u7d27\u51d1\u7684\u62d3\u6251\u5f62\u5f0f\u8bed\u8a00\uff0c\u901a\u8fc7\u9876\u70b9\u5750\u6807\u548c\u8fde\u63a5\u5173\u7cfb\u7f16\u7801\u5168\u5c40\u56fe\u5f62\u3002", "result": "\u5728Geo3K\u3001GeoQA\u548cFormalGeo7K\u6570\u636e\u96c6\u4e0a\uff0cGeoFocus\u6bd4\u9886\u5148\u7684\u4e13\u7528\u6a21\u578b\u51c6\u786e\u7387\u63d0\u53474.7%\uff1b\u5173\u952e\u5c40\u90e8\u7279\u5f81\u8986\u76d6\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad861%\uff1b\u5168\u5c40\u611f\u77e5\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1120%\u540c\u65f6\u63d0\u5347\u62d3\u6251\u8bc6\u522b\u51c6\u786e\u7387\uff1b\u5728MATHVERSE\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "GeoFocus\u901a\u8fc7\u7ed3\u5408\u7406\u8bba\u9a71\u52a8\u7684\u5c40\u90e8\u611f\u77e5\u548c\u7d27\u51d1\u7684\u62d3\u6251\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u51e0\u4f55\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08528", "pdf": "https://arxiv.org/pdf/2602.08528", "abs": "https://arxiv.org/abs/2602.08528", "authors": ["Chuyang Wu", "Samuli Siltanen"], "title": "Automatic regularization parameter choice for tomography using a double model approach", "categories": ["cs.CV", "math.OC"], "comment": null, "summary": "Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53cc\u7f51\u683c\u79bb\u6563\u5316\u7684\u81ea\u52a8\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u9988\u63a7\u5236\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u53c2\u6570\uff0c\u4f7f\u4e24\u4e2a\u7f51\u683c\u4e0a\u7684\u91cd\u5efa\u7ed3\u679c\u8fbe\u5230\u8db3\u591f\u76f8\u4f3c\u5ea6", "motivation": "X\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u91cd\u5efa\u662f\u75c5\u6001\u9006\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u6b63\u5219\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u6b63\u5219\u5316\u53c2\u6570\u7684\u9009\u62e9\uff0c\u9700\u8981\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u5148\u9a8c\u4fe1\u606f\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u4f20\u7edf\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u624b\u52a8\u8c03\u6574\u6216\u590d\u6742\u7684\u8ba1\u7b97\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e24\u4e2a\u4e0d\u540c\u8ba1\u7b97\u79bb\u6563\u5316\u7684\u81ea\u52a8\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\uff1a\u4f7f\u7528\u53cd\u9988\u63a7\u5236\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff0c\u9a71\u52a8\u8fed\u4ee3\u91cd\u5efa\u8fc7\u7a0b\uff0c\u5bfb\u627e\u80fd\u4f7f\u4e24\u4e2a\u7f51\u683c\u4e0a\u91cd\u5efa\u7ed3\u679c\u8fbe\u5230\u8db3\u591f\u76f8\u4f3c\u5ea6\u7684\u6700\u5c0f\u53c2\u6570\u503c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u65ad\u5c42\u626b\u63cf\u6570\u636e\u4e0a\u8bc1\u660e\u4e86\u6709\u6548\u6027\uff0c\u80fd\u591f\u81ea\u52a8\u9009\u62e9\u9002\u5f53\u7684\u6b63\u5219\u5316\u53c2\u6570\uff0c\u6539\u5584\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u7f51\u683c\u79bb\u6563\u5316\u65b9\u6cd5\u4e3aX\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u4e2d\u7684\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u624b\u52a8\u8c03\u53c2\u7684\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.08531", "pdf": "https://arxiv.org/pdf/2602.08531", "abs": "https://arxiv.org/abs/2602.08531", "authors": ["Anastasiia Kornilova", "Ivan Moskalenko", "Arabella Gromova", "Gonzalo Ferrer", "Alexander Menshchikov"], "title": "Thegra: Graph-based SLAM for Thermal Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u5355\u76ee\u56fe\u7684\u70ed\u6210\u50cfSLAM\u7cfb\u7edf\uff0c\u5229\u7528\u5728\u53ef\u89c1\u5149\u8c31\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u901a\u7528\u5b66\u4e60\u7279\u5f81\uff08SuperPoint\u68c0\u6d4b\u5668\u548cLightGlue\u5339\u914d\u5668\uff09\uff0c\u901a\u8fc7\u9884\u5904\u7406\u589e\u5f3a\u70ed\u6570\u636e\u9002\u5e94\u6027\uff0c\u5e76\u5f15\u5165\u7f6e\u4fe1\u5ea6\u52a0\u6743\u56e0\u5b50\u56fe\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u70ed\u6210\u50cf\u5728\u89c6\u89c9\u9000\u5316\u73af\u5883\uff08\u5982\u4f4e\u5149\u7167\u3001\u70df\u96fe\u3001\u6076\u52a3\u5929\u6c14\uff09\u4e2d\u4e3a\u89c6\u89c9SLAM\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u611f\u77e5\u65b9\u5f0f\uff0c\u4f46\u70ed\u56fe\u50cf\u901a\u5e38\u5177\u6709\u4f4e\u7eb9\u7406\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u9ad8\u566a\u58f0\u7684\u7279\u70b9\uff0c\u8fd9\u4f7f\u5f97\u57fa\u4e8e\u7279\u5f81\u7684SLAM\u53d8\u5f97\u590d\u6742\u3002", "method": "1. \u4f7f\u7528\u5728\u53ef\u89c1\u5149\u8c31\u6570\u636e\u4e0a\u8bad\u7ec3\u7684SuperPoint\u68c0\u6d4b\u5668\u548cLightGlue\u5339\u914d\u5668\u4f5c\u4e3a\u901a\u7528\u5b66\u4e60\u7279\u5f81\uff1b2. \u5f15\u5165\u9884\u5904\u7406\u6d41\u7a0b\u589e\u5f3a\u70ed\u6570\u636e\u8f93\u5165\u9002\u5e94\u6027\uff1b3. \u4fee\u6539\u6838\u5fc3SLAM\u6a21\u5757\u4ee5\u5904\u7406\u7a00\u758f\u548c\u5f02\u5e38\u503c\u8f83\u591a\u7684\u7279\u5f81\u5339\u914d\uff1b4. \u5c06SuperPoint\u7684\u5173\u952e\u70b9\u7f6e\u4fe1\u5ea6\u5206\u6570\u6574\u5408\u5230\u7f6e\u4fe1\u5ea6\u52a0\u6743\u56e0\u5b50\u56fe\u4e2d\u3002", "result": "\u5728\u516c\u5f00\u70ed\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6027\u80fd\uff0c\u65e0\u9700\u7279\u5b9a\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u6216\u5fae\u8c03\u7279\u5f81\u68c0\u6d4b\u5668\uff0c\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u70ed\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u5229\u7528\u53ef\u89c1\u5149\u8c31\u8bad\u7ec3\u7684\u901a\u7528\u5b66\u4e60\u7279\u5f81\u8fdb\u884c\u70ed\u6210\u50cfSLAM\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u9884\u5904\u7406\u548c\u7f6e\u4fe1\u5ea6\u52a0\u6743\u673a\u5236\u6709\u6548\u5e94\u5bf9\u70ed\u56fe\u50cf\u7684\u6311\u6218\uff0c\u4e3a\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e0b\u7684SLAM\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08540", "pdf": "https://arxiv.org/pdf/2602.08540", "abs": "https://arxiv.org/abs/2602.08540", "authors": ["He Wu", "Xia Yan", "Yanghui Xu", "Liegang Xia", "Jiazhou Chen"], "title": "TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation", "categories": ["cs.CV", "cs.GR"], "comment": "13 pages, 6 figures, 4 tables", "summary": "Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.", "AI": {"tldr": "\u63d0\u51faTIBR4D\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fed\u4ee3\u8fb9\u754c\u7cbe\u5316\u5b9e\u73b0\u52a8\u60014D\u9ad8\u65af\u573a\u666f\u7684\u9ad8\u6548\u65e0\u5b66\u4e60\u5bf9\u8c61\u5206\u5272", "motivation": "\u52a8\u60014D\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u7ea7\u5206\u5272\u9762\u4e34\u590d\u6742\u8fd0\u52a8\u3001\u906e\u6321\u548c\u6a21\u7cca\u8fb9\u754c\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u4e9b\u95ee\u9898", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8fed\u4ee3\u8fb9\u754c\u7cbe\u5316\u6846\u67b6\uff1a1) IGIT\u9636\u6bb5\u901a\u8fc7\u8fed\u4ee3\u8ffd\u8e2a\u7cbe\u5316\u9ad8\u65af-\u5b9e\u4f8b\u6982\u7387\uff0c\u63d0\u53d6\u66f4\u5b8c\u6574\u7684\u5bf9\u8c61\u70b9\u4e91\uff1b2) RCC\u9636\u6bb5\u901a\u8fc7\u6291\u5236\u8fb9\u754c\u9644\u8fd1\u4e0d\u786e\u5b9a\u9ad8\u65af\u6765\u83b7\u5f97\u66f4\u51c6\u786e\u8fb9\u754c\uff1b\u540c\u65f6\u63d0\u51fa\u65f6\u95f4\u5206\u5272\u5408\u5e76\u7b56\u7565\u5e73\u8861\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u52a8\u6001\u611f\u77e5", "result": "\u5728HyperNeRF\u548cNeu3D\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4SOTA\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u8fb9\u754c\u66f4\u6e05\u6670\u3001\u66f4\u51c6\u786e\u7684\u5bf9\u8c61\u9ad8\u65af\u70b9\u4e91\uff0c\u4e14\u6548\u7387\u66f4\u9ad8", "conclusion": "TIBR4D\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u8fb9\u754c\u7cbe\u5316\u548c\u65f6\u95f4\u5206\u5272\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u60014D\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u5206\u5272\u95ee\u9898\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2602.08550", "pdf": "https://arxiv.org/pdf/2602.08550", "abs": "https://arxiv.org/abs/2602.08550", "authors": ["Shih-Fang Chen", "Jun-Cheng Chen", "I-Hong Jhuo", "Yen-Yu Lin"], "title": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "comment": "ICLR 2026. This is a preprint version. The camera-ready version will be updated soon", "summary": "Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.", "AI": {"tldr": "GOT-Edit\uff1a\u4e00\u79cd\u5728\u7ebf\u8de8\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u51e0\u4f55\u611f\u77e5\u7ebf\u7d22\u6574\u5408\u5230\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u5668\u4e2d\uff0c\u7ed3\u54082D\u8bed\u4e49\u548c3D\u51e0\u4f55\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd", "motivation": "\u4eba\u7c7b\u611f\u77e5\u5229\u7528\u5148\u9a8c3D\u77e5\u8bc6\u548c\u8bed\u4e49\u63a8\u7406\u8fdb\u884c\u6709\u6548\u76ee\u6807\u8ddf\u8e2a\uff0c\u800c\u73b0\u6709\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d562D\u7279\u5f81\uff0c\u5ffd\u7565\u4e863D\u51e0\u4f55\u7ebf\u7d22\uff0c\u5bfc\u81f4\u5bf9\u90e8\u5206\u906e\u6321\u3001\u5e72\u6270\u7269\u4ee5\u53ca\u51e0\u4f55\u548c\u5916\u89c2\u53d8\u5316\u7684\u9c81\u68d2\u6027\u4e0d\u8db3", "method": "\u63d0\u51faGOT-Edit\u5728\u7ebf\u8de8\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u51e0\u4f55\u57fa\u7840Transformer\u4ece\u5c11\u91cf2D\u56fe\u50cf\u63a8\u65ad\u51e0\u4f55\u7ebf\u7d22\uff1b2\uff09\u901a\u8fc7\u96f6\u7a7a\u95f4\u7ea6\u675f\u66f4\u65b0\u8fdb\u884c\u5728\u7ebf\u6a21\u578b\u7f16\u8f91\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u533a\u5206\u80fd\u529b\u7684\u540c\u65f6\u6574\u5408\u51e0\u4f55\u4fe1\u606f", "result": "\u5728\u591a\u4e2a\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGOT-Edit\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u906e\u6321\u548c\u6742\u4e71\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u7ed3\u54082D\u8bed\u4e49\u548c3D\u51e0\u4f55\u63a8\u7406\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f", "conclusion": "\u901a\u8fc7\u5c06\u51e0\u4f55\u611f\u77e5\u7ebf\u7d22\u6574\u5408\u5230\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u5668\u4e2d\uff0cGOT-Edit\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u75653D\u51e0\u4f55\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u533a\u5206\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b"}}
{"id": "2602.08558", "pdf": "https://arxiv.org/pdf/2602.08558", "abs": "https://arxiv.org/abs/2602.08558", "authors": ["Guan Yuan Tan", "Ngoc Tuan Vu", "Arghya Pal", "Sailaja Rajanala", "Raphael Phan C. -W.", "Mettu Srinivas", "Chee-Ming Ting"], "title": "FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction", "categories": ["cs.CV", "cs.GT"], "comment": null, "summary": "We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.", "AI": {"tldr": "FLAG-4D\u662f\u4e00\u4e2a\u7528\u4e8e\u52a8\u6001\u573a\u666f\u65b0\u89c6\u89d2\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u53d8\u5f62\u7f51\u7edc\u5efa\u6a213D\u9ad8\u65af\u539f\u8bed\u5728\u65f6\u7a7a\u4e2d\u7684\u6f14\u5316\uff0c\u7ed3\u5408\u77ac\u65f6\u53d8\u5f62\u7f51\u7edc\u548c\u5168\u5c40\u8fd0\u52a8\u7f51\u7edc\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u5149\u6d41\u7279\u5f81\u786e\u4fdd\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5355\u4e2aMLP\u5efa\u6a21\u65f6\u95f4\u53d8\u5f62\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u70b9\u8fd0\u52a8\u548c\u7ec6\u7c92\u5ea6\u52a8\u6001\u7ec6\u8282\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u8f93\u5165\u89c6\u89d2\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u66f4\u51c6\u786e\u5efa\u6a21\u65f6\u7a7a\u53d8\u5f62\u5e76\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cc\u53d8\u5f62\u7f51\u7edc\u52a8\u6001\u626d\u66f2\u89c4\u83033D\u9ad8\u65af\u96c6\uff1a\u77ac\u65f6\u53d8\u5f62\u7f51\u7edc(IDN)\u5efa\u6a21\u7ec6\u7c92\u5ea6\u5c40\u90e8\u53d8\u5f62\uff0c\u5168\u5c40\u8fd0\u52a8\u7f51\u7edc(GMN)\u6355\u6349\u957f\u7a0b\u52a8\u6001\uff0c\u901a\u8fc7\u4e92\u5b66\u4e60\u8fdb\u884c\u7cbe\u70bc\u3002\u7ed3\u5408\u9884\u8bad\u7ec3\u5149\u6d41\u9aa8\u5e72\u7684\u5bc6\u96c6\u8fd0\u52a8\u7279\u5f81\uff0c\u4f7f\u7528\u53d8\u5f62\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u9f50\u5149\u6d41\u4fe1\u606f\u4e0e3D\u9ad8\u65af\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFLAG-4D\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u4fdd\u771f\u5ea6\u3001\u66f4\u65f6\u95f4\u4e00\u81f4\u7684\u91cd\u5efa\uff0c\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u7ec6\u8282\u3002", "conclusion": "FLAG-4D\u901a\u8fc7\u53cc\u53d8\u5f62\u7f51\u7edc\u548c\u5149\u6d41\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u590d\u6742\u8fd0\u52a8\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u65f6\u7a7a\u4e00\u81f4\u65b0\u89c6\u89d2\u751f\u6210\u3002"}}
{"id": "2602.08582", "pdf": "https://arxiv.org/pdf/2602.08582", "abs": "https://arxiv.org/abs/2602.08582", "authors": ["Melany Yang", "Yuhang Yu", "Diwang Weng", "Jinwei Chen", "Wei Dong"], "title": "SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.", "AI": {"tldr": "SemiNFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563Transformer\u7684\u8c03\u8272\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u827a\u672f\u8bad\u7ec3\u8fc7\u7a0b\uff1a\u4ece\u521a\u6027\u6a21\u4eff\u5230\u76f4\u89c9\u521b\u4f5c\uff0c\u901a\u8fc7\u914d\u5bf9\u6570\u636e\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u7684\u8c03\u8272", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53c2\u8003\u7684\u8c03\u8272\u65b9\u6cd5\u901a\u5e38\u53ea\u8fdb\u884c\u5168\u5c40\u989c\u8272\u6620\u5c04\uff0c\u7f3a\u4e4f\u5bf9\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u4eba\u7c7b\u7f8e\u5b66\u7684\u771f\u6b63\u7406\u89e3\uff0c\u65e0\u6cd5\u50cf\u4e13\u4e1a\u827a\u672f\u5bb6\u90a3\u6837\u8fdb\u884c\u667a\u80fd\u8c03\u8272", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u4f7f\u7528\u914d\u5bf9\u4e09\u5143\u7ec4\u5b66\u4e60\u57fa\u672c\u7ed3\u6784\u4fdd\u6301\u548c\u989c\u8272\u6620\u5c04\u6280\u80fd\uff1b2\uff09\u5728\u65e0\u914d\u5bf9\u6570\u636e\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u57f9\u517b\u7ec6\u817b\u7f8e\u5b66\u611f\u77e5\uff0c\u8bbe\u8ba1\u6df7\u5408\u5728\u7ebf-\u79bb\u7ebf\u5956\u52b1\u673a\u5236\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8", "result": "\u5728\u6807\u51c6\u9884\u8bbe\u8f6c\u79fb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u4efb\u52a1\uff08\u5982\u9ed1\u767d\u7167\u7247\u7740\u8272\u548c\u8de8\u57df\u9884\u8bbe\u8f6c\u79fb\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u7edf\u8ba1\u5339\u914d", "conclusion": "SemiNFT\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u827a\u672f\u8bad\u7ec3\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7f8e\u5b66\u7406\u89e3\u7684\u590d\u6742\u5c42\u6b21\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7edf\u8ba1\u5339\u914d\u65b9\u6cd5\uff0c\u5728\u8c03\u8272\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u667a\u80fd\u7684\u8bed\u4e49\u611f\u77e5\u80fd\u529b"}}
{"id": "2602.08613", "pdf": "https://arxiv.org/pdf/2602.08613", "abs": "https://arxiv.org/abs/2602.08613", "authors": ["Wei Gao", "Wenxu Gao", "Xingming Mu", "Changhao Peng", "Ge Li"], "title": "Overview and Comparison of AVS Point Cloud Compression Standard", "categories": ["cs.CV"], "comment": "3 figures, 3 tables", "summary": "Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4e2d\u56fdAVS PCC\u70b9\u4e91\u538b\u7f29\u6807\u51c6\uff0c\u4ece\u6280\u672f\u548c\u6027\u80fd\u6bd4\u8f83\u4e24\u4e2a\u89d2\u5ea6\u5206\u6790\u4e86\u8be5\u6807\u51c6\u7684\u7279\u70b9\u548c\u4f18\u52bf\u3002", "motivation": "\u70b9\u4e91\u6570\u636e\u91cf\u5927\uff0c\u4f20\u8f93\u548c\u5b58\u50a8\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u6280\u672f\u3002\u867d\u7136MPEG\u5df2\u6709G-PCC\u548cV-PCC\u6807\u51c6\uff0c\u4f46\u4e2d\u56fdAVS\u5de5\u4f5c\u7ec4\u5f00\u53d1\u4e86\u65b0\u4e00\u4ee3\u70b9\u4e91\u538b\u7f29\u6807\u51c6AVS PCC\uff0c\u91c7\u7528\u4e86\u8bb8\u591a\u65b0\u7684\u7f16\u7801\u5de5\u5177\u548c\u6280\u672f\u3002", "method": "\u4ece\u4e24\u4e2a\u89c6\u89d2\u7efc\u8ff0AVS PCC\u6807\u51c6\uff1a1) \u76f8\u5173\u6280\u672f\u5206\u6790\uff0c\u4ecb\u7ecd\u6807\u51c6\u91c7\u7528\u7684\u65b0\u7f16\u7801\u5de5\u5177\u548c\u6280\u672f\uff1b2) \u6027\u80fd\u6bd4\u8f83\uff0c\u4e0e\u5176\u4ed6\u6807\u51c6\uff08\u5982MPEG\u7684G-PCC\u548cV-PCC\uff09\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "AVS PCC\u6807\u51c6\u91c7\u7528\u4e86\u4e0eMPEG\u6807\u51c6\u4e0d\u540c\u7684\u65b0\u7f16\u7801\u5de5\u5177\u548c\u6280\u672f\uff0c\u5728\u70b9\u4e91\u538b\u7f29\u65b9\u9762\u5c55\u73b0\u51fa\u72ec\u7279\u7684\u6027\u80fd\u7279\u70b9\u3002", "conclusion": "AVS PCC\u4f5c\u4e3a\u4e2d\u56fd\u81ea\u4e3b\u5f00\u53d1\u7684\u70b9\u4e91\u538b\u7f29\u6807\u51c6\uff0c\u901a\u8fc7\u91c7\u7528\u521b\u65b0\u6280\u672f\uff0c\u4e3a\u70b9\u4e91\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u63a8\u52a8\u70b9\u4e91\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.08615", "pdf": "https://arxiv.org/pdf/2602.08615", "abs": "https://arxiv.org/abs/2602.08615", "authors": ["Kfir Goldberg", "Elad Richardson", "Yael Vinker"], "title": "Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration", "categories": ["cs.CV"], "comment": "Project page available at https://inspirationseedspaper.github.io/InspirationSeeds/", "summary": "While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.", "AI": {"tldr": "\u63d0\u51faInspiration Seeds\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u751f\u6210\u4ece\u6700\u7ec8\u6267\u884c\u8f6c\u5411\u63a2\u7d22\u6027\u6784\u601d\uff0c\u901a\u8fc7\u89c6\u89c9\u65b9\u5f0f\u8fde\u63a5\u8f93\u5165\u56fe\u50cf\uff0c\u65e0\u9700\u6587\u672c\u63d0\u793a", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6587\u672c\u63d0\u793a\u8fdb\u884c\u4f18\u5316\uff0c\u7f3a\u4e4f\u5bf9\u5f00\u653e\u5f0f\u89c6\u89c9\u63a2\u7d22\u7684\u652f\u6301\uff0c\u800c\u8bbe\u8ba1\u5e08\u901a\u5e38\u4ece\u677e\u6563\u8fde\u63a5\u7684\u89c6\u89c9\u53c2\u8003\u4e2d\u5bfb\u627e\u7075\u611f\uff0c\u5bfb\u6c42\u6fc0\u53d1\u65b0\u60f3\u6cd5\u7684\u6d8c\u73b0\u6027\u8fde\u63a5", "method": "\u4f7f\u7528\u524d\u9988\u6a21\u578b\uff0c\u5728\u5408\u6210\u4e09\u5143\u7ec4\u4e0a\u8bad\u7ec3\uff0c\u901a\u8fc7CLIP\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6CLIP\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7f16\u8f91\u65b9\u5411\uff0c\u5b8c\u5168\u901a\u8fc7\u89c6\u89c9\u65b9\u5f0f\u5206\u89e3\u89c6\u89c9\u65b9\u9762\u5e76\u9694\u79bb\u6982\u5ff5\u5bf9", "result": "\u6a21\u578b\u80fd\u591f\u7ed9\u5b9a\u4e24\u4e2a\u8f93\u5165\u56fe\u50cf\uff0c\u751f\u6210\u591a\u6837\u5316\u3001\u89c6\u89c9\u8fde\u8d2f\u7684\u7ec4\u5408\uff0c\u63ed\u793a\u8f93\u5165\u4e4b\u95f4\u7684\u6f5c\u5728\u5173\u7cfb\uff0c\u65e0\u9700\u4f9d\u8d56\u7528\u6237\u6307\u5b9a\u7684\u6587\u672c\u63d0\u793a", "conclusion": "\u901a\u8fc7\u6d88\u9664\u5bf9\u8bed\u8a00\u7684\u4f9d\u8d56\u5e76\u5b9e\u73b0\u5feb\u901f\u76f4\u89c2\u7684\u91cd\u7ec4\uff0c\u8be5\u65b9\u6cd5\u652f\u6301\u521b\u610f\u5de5\u4f5c\u65e9\u671f\u548c\u6a21\u7cca\u9636\u6bb5\u7684\u89c6\u89c9\u6784\u601d"}}
{"id": "2602.08620", "pdf": "https://arxiv.org/pdf/2602.08620", "abs": "https://arxiv.org/abs/2602.08620", "authors": ["Siyu Liu", "Chujie Qin", "Hubery Yin", "Qixin Yan", "Zheng-Peng Duan", "Chen Li", "Jing Lyu", "Chun-Le Guo", "Chongyi Li"], "title": "Improving Reconstruction of Representation Autoencoder", "categories": ["cs.CV"], "comment": null, "summary": "Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.", "AI": {"tldr": "LV-RAE\u662f\u4e00\u79cd\u8868\u793a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u7279\u5f81\u7684\u4f4e\u7ea7\u4fe1\u606f\u6765\u63d0\u5347\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u56fe\u50cf\u7f16\u7801\u5668\u867d\u7136\u63d0\u5347\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6027\u80fd\uff0c\u4f46\u5176\u8bed\u4e49\u7279\u5f81\u7f3a\u4e4f\u4f4e\u7ea7\u4fe1\u606f\uff08\u5982\u989c\u8272\u548c\u7eb9\u7406\uff09\uff0c\u5bfc\u81f4\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e0b\u964d\uff0c\u6210\u4e3a\u8fdb\u4e00\u6b65\u6269\u5c55LDMs\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51faLV-RAE\u8868\u793a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u7279\u5f81\u7684\u4f4e\u7ea7\u4fe1\u606f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\uff1b\u540c\u65f6\u901a\u8fc7\u5fae\u8c03\u89e3\u7801\u5668\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u53d7\u63a7\u566a\u58f0\u6ce8\u5165\u5e73\u6ed1\u751f\u6210\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLV-RAE\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u62bd\u8c61\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "LV-RAE\u901a\u8fc7\u89e3\u51b3\u8bed\u4e49\u7279\u5f81\u7f3a\u4e4f\u4f4e\u7ea7\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6027\u80fd\uff0c\u4e3aLDMs\u7684\u8fdb\u4e00\u6b65\u6269\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08626", "pdf": "https://arxiv.org/pdf/2602.08626", "abs": "https://arxiv.org/abs/2602.08626", "authors": ["Alexis Marouani", "Oriane Sim\u00e9oni", "Herv\u00e9 J\u00e9gou", "Piotr Bojanowski", "Huy V. Vo"], "title": "Revisiting [CLS] and Patch Token Interaction in Vision Transformers", "categories": ["cs.CV"], "comment": "To be published as a conference paper at ICLR 2026", "summary": "Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u4e13\u95e8\u5904\u7406\u8def\u5f84\u5206\u79bbCLS\u7c7btoken\u548cpatch token\u7684\u8ba1\u7b97\u6d41\uff0c\u7279\u522b\u5728\u5f52\u4e00\u5316\u5c42\u548c\u65e9\u671fQKV\u6295\u5f71\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "Vision Transformers\u4e2d\uff0c\u53ef\u5b66\u4e60\u7684[CLS]\u7c7btoken\u548cpatch token\u867d\u7136\u6027\u8d28\u4e0d\u540c\uff0c\u4f46\u5728\u6574\u4e2a\u6a21\u578b\u4e2d\u5374\u4ee5\u76f8\u540c\u65b9\u5f0f\u5904\u7406\u3002\u4f5c\u8005\u7814\u7a76\u4e86\u4e0d\u540c\u9884\u8bad\u7ec3\u7b56\u7565\u4e0b\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u4e4b\u95f4\u7684\u6469\u64e6\uff0c\u53d1\u73b0\u6807\u51c6\u5f52\u4e00\u5316\u5c42\u5728\u8fd9\u4e24\u79cdtoken\u7c7b\u578b\u4e4b\u95f4\u5f15\u5165\u4e86\u9690\u5f0f\u533a\u5206\u3002", "method": "\u57fa\u4e8e\u5206\u6790\u53d1\u73b0\uff0c\u63d0\u51fa\u4e13\u95e8\u7684\u5904\u7406\u8def\u5f84\uff0c\u9009\u62e9\u6027\u5730\u89e3\u8026\u7c7btoken\u548cpatch token\u7684\u8ba1\u7b97\u6d41\uff0c\u7279\u522b\u662f\u5728\u5f52\u4e00\u5316\u5c42\u548c\u65e9\u671f\u67e5\u8be2-\u952e-\u503c\u6295\u5f71\u4e2d\u3002\u8fd9\u79cd\u9488\u5bf9\u6027\u4e13\u95e8\u5316\u663e\u8457\u63d0\u5347\u4e86patch\u8868\u793a\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u5272\u6027\u80fd\u63d0\u5347\u8d85\u8fc72 mIoU\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5206\u7c7b\u51c6\u786e\u7387\u3002\u63d0\u51fa\u7684\u4fee\u6539\u4ec5\u589e\u52a08%\u7684\u53c2\u6570\uff0c\u6ca1\u6709\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002\u901a\u8fc7\u5168\u9762\u6d88\u878d\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u54ea\u4e9b\u67b6\u6784\u7ec4\u4ef6\u4ece\u4e13\u95e8\u5316\u4e2d\u83b7\u76ca\u6700\u591a\u4ee5\u53ca\u8be5\u65b9\u6cd5\u5982\u4f55\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u5b66\u6846\u67b6\u4e2d\u6cdb\u5316\u7684\u89c1\u89e3\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u5904\u7406\u7c7btoken\u548cpatch token\uff0c\u7279\u522b\u662f\u89e3\u8026\u5b83\u4eec\u5728\u5f52\u4e00\u5316\u5c42\u548c\u65e9\u671fQKV\u6295\u5f71\u4e2d\u7684\u8ba1\u7b97\u6d41\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347Vision Transformers\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u7c7b\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2602.08652", "pdf": "https://arxiv.org/pdf/2602.08652", "abs": "https://arxiv.org/abs/2602.08652", "authors": ["Oskar Thaeter", "Tanja Niedermair", "Johannes Raffler", "Ralf Huss", "Peter J. Sch\u00fcffler"], "title": "Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology", "categories": ["cs.CV"], "comment": "17 pages, 8 figures, 7 tables", "summary": "Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to\n  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen\n  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.\n  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from\n  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,\n  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).\n  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and\n  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\\times$\n  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.\n  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for\n  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner\n  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other\n  low-resolution slide annotations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u9884\u626b\u63cf\u7f29\u7565\u56fe\u9884\u6d4b\u75c5\u7406\u5207\u7247\u56fa\u5b9a\u7c7b\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u8d28\u91cf\u63a7\u5236", "motivation": "\u75c5\u7406\u5207\u7247\u56fa\u5b9a\u7c7b\u578b\uff08FFPE/FS\uff09\u7684\u624b\u52a8\u6807\u6ce8\u5bb9\u6613\u51fa\u9519\uff0c\u5f71\u54cd\u4e0b\u6e38\u5206\u6790\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5168\u5206\u8fa8\u7387WSI\uff0c\u9650\u5236\u4e86\u9ad8\u901a\u91cf\u8d28\u91cf\u63a7\u5236\u7684\u6269\u5c55\u6027", "method": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u9884\u626b\u63cf\u7f29\u7565\u56fe\u50cf\u9884\u6d4b\u56fa\u5b9a\u7c7b\u578b\u3002\u5728TUM\u75c5\u7406\u7814\u7a76\u6240\u7684WSI\u4e0a\u8bad\u7ec3\uff08n=1,200\uff09\uff0c\u5728TCGA\u3001\u5965\u683c\u65af\u5821\u548c\u96f7\u6839\u65af\u5821\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30", "result": "\u6a21\u578b\u5728TCGA\u4e0aAUROC\u8fbe\u52300.88\uff0c\u6bd4\u540c\u7c7b\u9884\u626b\u63cf\u65b9\u6cd5\u63d0\u53474.8%\uff1b\u5728\u96f7\u6839\u65af\u5821\u548c\u5965\u683c\u65af\u5821\u6570\u636e\u96c6\u4e0aAUROC\u4e3a0.72\u3002\u5904\u7406\u6bcf\u5f20\u5207\u7247\u4ec5\u970021\u6beb\u79d2\uff0c\u6bd4\u73b0\u6709\u9ad8\u500d\u7387\u5168\u5206\u8fa8\u7387\u65b9\u6cd5\u5feb400\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u68c0\u6d4b\u6807\u6ce8\u9519\u8bef\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u9ad8\u500d\u7387\u626b\u63cf\uff0c\u662f\u75c5\u7406\u9ad8\u901a\u91cf\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u6709\u4ef7\u503c\u7684\u8d28\u91cf\u63a7\u5236\u5de5\u5177\u3002\u672a\u6765\u5c06\u6539\u8fdb\u6a21\u578b\u5bf9\u4e0d\u540c\u626b\u63cf\u4eea\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2602.08661", "pdf": "https://arxiv.org/pdf/2602.08661", "abs": "https://arxiv.org/abs/2602.08661", "authors": ["Yi Dao", "Lankai Zhang", "Hao Liu", "Haiwei Zhang", "Wenbo Wang"], "title": "WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling", "categories": ["cs.CV"], "comment": null, "summary": "Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.", "AI": {"tldr": "WiFlow\u662f\u4e00\u4e2a\u57fa\u4e8eWiFi\u4fe1\u53f7\u7684\u8fde\u7eed\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5728\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.00% PCK@20\u548c99.48% PCK@50\u7684\u7cbe\u5ea6\uff0c\u6a21\u578b\u53c2\u6570\u4ec54.82M\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u7269\u8054\u7f51\u667a\u80fd\u611f\u77e5\u7684\u57fa\u7840\uff0c\u4f46\u73b0\u6709WiFi\u65b9\u6cd5\u5728\u8fde\u7eed\u8fd0\u52a8\u548c\u9ad8\u8ba1\u7b97\u5f00\u9500\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u4f20\u7edf\u89c6\u89c9\u65b9\u6cd5\u5c06CSI\u89c6\u4e3a\u56fe\u50cf\u5904\u7406\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4fe1\u53f7\u7684\u539f\u59cb\u5e8f\u5217\u7ed3\u6784\u3002", "method": "\u63d0\u51faWiFlow\u6846\u67b6\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3002\u7f16\u7801\u5668\u4f7f\u7528\u65f6\u5e8f\u548c\u975e\u5bf9\u79f0\u5377\u79ef\u6355\u6349CSI\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u4fdd\u6301\u4fe1\u53f7\u539f\u59cb\u5e8f\u5217\u7ed3\u6784\uff1b\u901a\u8fc7\u8f74\u5411\u6ce8\u610f\u529b\u673a\u5236\u7cbe\u70bc\u4eba\u4f53\u5173\u952e\u70b9\u7279\u5f81\u5e76\u6355\u6349\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff1b\u89e3\u7801\u5668\u5c06\u7f16\u7801\u7684\u9ad8\u7ef4\u7279\u5f81\u6620\u5c04\u4e3a\u5173\u952e\u70b9\u5750\u6807\u3002", "result": "\u57285\u540d\u53d7\u8bd5\u8005\u6267\u884c8\u79cd\u65e5\u5e38\u6d3b\u52a8\u7684360,000\u4e2a\u540c\u6b65CSI-\u59ff\u6001\u6837\u672c\u6570\u636e\u96c6\u4e0a\uff0cWiFlow\u8fbe\u5230PCK@20\u4e3a97.00%\uff0cPCK@50\u4e3a99.48%\uff0c\u5e73\u5747\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee0.008m\u3002\u6a21\u578b\u53c2\u6570\u4ec54.82M\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "WiFlow\u4e3a\u5b9e\u7528\u7684WiFi\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u901a\u8fc7\u4fdd\u6301\u4fe1\u53f7\u539f\u59cb\u5e8f\u5217\u7ed3\u6784\u548c\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u8fde\u7eed\u59ff\u6001\u4f30\u8ba1\u3002"}}
{"id": "2602.08670", "pdf": "https://arxiv.org/pdf/2602.08670", "abs": "https://arxiv.org/abs/2602.08670", "authors": ["Yang Bai"], "title": "A Machine Learning accelerated geophysical fluid solver", "categories": ["cs.CV", "cs.CE", "cs.PF", "physics.comp-ph"], "comment": "Master Thesis", "summary": "Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5c06\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e8e\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u79bb\u6563\u5316\u65b9\u6cd5\u6539\u8fdb\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u7684\u4f20\u7edfPDE\u6c42\u89e3\u5668\uff0c\u5e76\u5728\u6d45\u6c34\u65b9\u7a0b\u548c\u6b27\u62c9\u65b9\u7a0b\u4e0a\u9a8c\u8bc1\u4e86ML\u6c42\u89e3\u5668\u7684\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5982\u4f55\u5c06\u5176\u5e94\u7528\u4e8e\u5177\u6709\u6570\u5b66\u7ea6\u675f\u7684\u9886\u57df\uff08\u5982\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff09\u4ecd\u662f\u4e00\u4e2a\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u4f4e\u5206\u8fa8\u7387\u6a21\u62df\u4e2d\u5b58\u5728\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u9650\u5236\uff0c\u9700\u8981\u63a2\u7d22ML\u65b9\u6cd5\u6765\u52a0\u901f\u548c\u6539\u8fdb\u73b0\u6709PDE\u6c42\u89e3\u5668\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u79bb\u6563\u5316\u65b9\u6cd5\uff0c\u5728\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u9884\u6d4b\u51c6\u7ebf\u6027\u6a21\u677f\u7684\u7cfb\u6570\uff0c\u7528\u4e8e\u8ba1\u7b97\u7ed9\u5b9a\u4f4d\u7f6e\u5904\u7684\u51fd\u6570\u503c\u6216\u5bfc\u6570\u503c\u3002\u5b9e\u73b0\u4e86\u6d45\u6c34\u65b9\u7a0b\u548c\u6b27\u62c9\u65b9\u7a0b\u7684\u4f20\u7edf\u6c42\u89e3\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7528\u4e8eML\u6c42\u89e3\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f20\u7edf\u6c42\u89e3\u5668\u6027\u80fd\u4f18\u4e8ePyclaw\u6c42\u89e3\u5668\u3002\u5728\u63d0\u51fa\u7684\u56db\u79cdML\u6c42\u89e3\u5668\u4e2d\uff0c\u6709\u4e24\u79cd\u65b9\u6cd5\u80fd\u591f\u8f93\u51fa\u4ee4\u4eba\u6ee1\u610f\u7684\u89e3\uff0c\u9a8c\u8bc1\u4e86ML\u65b9\u6cd5\u5728\u6539\u8fdbPDE\u6c42\u89e3\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u79bb\u6563\u5316\u65b9\u6cd5\u4e3a\u5c06\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u80fd\u591f\u7ed3\u5408\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u7684\u4f18\u52bf\uff08\u5982\u5b88\u6052\u5f8b\uff09\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u6a21\u62df\u4e2d\u5b9e\u73b0\u6bd4\u4f20\u7edf\u6709\u9650\u5dee\u5206\u6216\u6709\u9650\u4f53\u79ef\u683c\u5f0f\u66f4\u597d\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.08682", "pdf": "https://arxiv.org/pdf/2602.08682", "abs": "https://arxiv.org/abs/2602.08682", "authors": ["Ying Guo", "Qijun Gan", "Yifu Zhang", "Jinlai Liu", "Yifei Hu", "Pan Xie", "Dongjun Qian", "Yu Zhang", "Ruiqi Li", "Yuqi Zhang", "Ruibiao Lu", "Xiaofeng Mei", "Bo Han", "Xiang Yin", "Bingyue Peng", "Zehuan Yuan"], "title": "ALIVE: Animate Your World with Lifelike Audio-Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.", "AI": {"tldr": "ALIVE\u662f\u4e00\u4e2a\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u9020\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3cSora\u7684\u97f3\u9891\u89c6\u9891\u751f\u6210\u548c\u52a8\u753b\u529f\u80fd\uff0c\u5728\u97f3\u9891\u89c6\u89c9\u540c\u6b65\u548c\u53c2\u8003\u52a8\u753b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6b63\u5feb\u901f\u5411\u7edf\u4e00\u7684\u97f3\u9891-\u89c6\u9891\u751f\u6210\u53d1\u5c55\uff0c\u9700\u8981\u5c06\u73b0\u6709\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u6269\u5c55\u5230\u652f\u6301\u97f3\u9891\u89c6\u9891\u540c\u6b65\u751f\u6210\u548c\u53c2\u8003\u52a8\u753b\u7684\u80fd\u529b\u3002", "method": "1) \u5728MMDiT\u67b6\u6784\u57fa\u7840\u4e0a\u589e\u52a0\u8054\u5408\u97f3\u9891-\u89c6\u9891\u5206\u652f\uff0c\u5305\u542bTA-CrossAttn\u7528\u4e8e\u65f6\u95f4\u5bf9\u9f50\u7684\u8de8\u6a21\u6001\u878d\u5408\u548cUniTemp-RoPE\u7528\u4e8e\u7cbe\u786e\u7684\u97f3\u9891\u89c6\u89c9\u5bf9\u9f50\uff1b2) \u8bbe\u8ba1\u5168\u9762\u7684\u6570\u636e\u7ba1\u9053\u8fdb\u884c\u9ad8\u8d28\u91cf\u5fae\u8c03\u6570\u636e\u6536\u96c6\uff1b3) \u5f15\u5165\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u5728\u767e\u4e07\u7ea7\u522b\u9ad8\u8d28\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u540e\uff0cALIVE\u8868\u73b0\u51fa\u8272\uff0c\u4e00\u81f4\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ALIVE\u901a\u8fc7\u8be6\u7ec6\u7684\u65b9\u6cd5\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e2e\u52a9\u793e\u533a\u66f4\u9ad8\u6548\u5730\u5f00\u53d1\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u63a8\u52a8\u4e86\u7edf\u4e00\u97f3\u9891\u89c6\u9891\u751f\u6210\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.08683", "pdf": "https://arxiv.org/pdf/2602.08683", "abs": "https://arxiv.org/abs/2602.08683", "authors": ["Feilong Tang", "Xiang An", "Yunyao Yan", "Yin Xie", "Bin Qin", "Kaicheng Yang", "Yifei Shen", "Yuanhan Zhang", "Chunyuan Li", "Shikun Feng", "Changrui Chen", "Huajie Tan", "Ming Hu", "Manyuan Zhang", "Bo Li", "Ziyong Feng", "Ziwei Liu", "Zongyuan Ge", "Jiankang Deng"], "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.\n  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.\n  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faOneVision-Encoder\uff0c\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u538b\u7f29\u539f\u7406\u7684\u89c6\u9891\u7f16\u7801\u67b6\u6784\uff0c\u901a\u8fc7\u5173\u6ce8\u4fe1\u53f7\u71b5\u4e30\u5bcc\u7684\u7a00\u758f\u533a\u57df\uff083.1%-25%\uff09\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u63d0\u5347\u89c6\u89c9\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u89c6\u89c9\u67b6\u6784\u504f\u79bb\u4e86\u4fe1\u606f\u8bba\u57fa\u672c\u539f\u5219\uff1a\u89c6\u89c9\u4fe1\u53f7\u9ad8\u5ea6\u5197\u4f59\uff0c\u800c\u5224\u522b\u4fe1\u606f\u7a00\u758f\u3002\u5f53\u524d\u6a21\u578b\u5747\u5300\u5904\u7406\u5bc6\u96c6\u50cf\u7d20\u7f51\u683c\uff0c\u6d6a\u8d39\u5927\u91cf\u8ba1\u7b97\u5728\u9759\u6001\u80cc\u666f\u800c\u975e\u5b9a\u4e49\u8fd0\u52a8\u548c\u610f\u4e49\u7684\u9884\u6d4b\u6b8b\u5dee\u4e0a\u3002\u9700\u8981\u5c06\u67b6\u6784\u4e0e\u89c6\u9891\u7684\u4fe1\u606f\u8bba\u539f\u7406\uff08\u7f16\u89e3\u7801\u5668\uff09\u5bf9\u9f50\u3002", "method": "\u91c7\u7528Codec Patchification\u6280\u672f\uff0c\u653e\u5f03\u5747\u5300\u8ba1\u7b97\uff0c\u4e13\u6ce8\u4e8e\u4fe1\u53f7\u71b5\u4e30\u5bcc\u7684\u533a\u57df\uff083.1%-25%\uff09\u3002\u4f7f\u7528\u5171\u4eab3D RoPE\u7edf\u4e00\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u805a\u7c7b\u5224\u522b\u76ee\u6807\u5728\u8d85\u8fc7100\u4e07\u4e2a\u8bed\u4e49\u6982\u5ff5\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8054\u5408\u6355\u6349\u7269\u4f53\u6301\u4e45\u6027\u548c\u8fd0\u52a8\u52a8\u6001\u3002", "result": "\u572816\u4e2a\u56fe\u50cf\u3001\u89c6\u9891\u548c\u6587\u6863\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOV-Encoder\u6301\u7eed\u4f18\u4e8eQwen3-ViT\u548cSigLIP2\u7b49\u5f3a\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u5c3d\u7ba1\u4f7f\u7528\u66f4\u5c11\u7684\u89c6\u89c9token\u548c\u9884\u8bad\u7ec3\u6570\u636e\u3002\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\uff0c\u5e73\u5747\u6bd4Qwen3-ViT\u63d0\u53474.1%\u3002", "conclusion": "\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0d\u662f\u6743\u8861\u5173\u7cfb\uff0c\u800c\u662f\u6b63\u76f8\u5173\u7684\u3002\u7f16\u89e3\u7801\u5668\u5bf9\u9f50\u7684\u8865\u4e01\u7ea7\u7a00\u758f\u6027\u662f\u57fa\u7840\u539f\u5219\uff0c\u4f7fOV-Encoder\u6210\u4e3a\u4e0b\u4e00\u4ee3\u89c6\u89c9\u901a\u7528\u6a21\u578b\u7684\u53ef\u6269\u5c55\u5f15\u64ce\u3002"}}
{"id": "2602.08699", "pdf": "https://arxiv.org/pdf/2602.08699", "abs": "https://arxiv.org/abs/2602.08699", "authors": ["Xiaogang Xu", "Kun Zhou", "Tao Hu", "Jiafei Wu", "Ruixing Wang", "Hao Peng", "Bei Yu"], "title": "Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm", "categories": ["cs.CV"], "comment": null, "summary": "Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.", "AI": {"tldr": "\u63d0\u51faVLLVE\u548cVLLVE++\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u56fe\u65e0\u5173\u548c\u89c6\u56fe\u76f8\u5173\u5206\u91cf\u5206\u89e3\u7b56\u7565\u589e\u5f3a\u4f4e\u5149\u89c6\u9891\uff0c\u5f15\u5165\u6b8b\u5dee\u9879\u548c\u53cc\u5411\u5b66\u4e60\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f4e\u5149\u89c6\u9891\u589e\u5f3a\u9762\u4e34\u4e25\u91cd\u4e0d\u53ef\u89c1\u6027\u548c\u566a\u58f0\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u52a8\u6001\u573a\u666f\u548c\u590d\u6742\u9000\u5316\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5206\u89e3\u89c6\u9891\u5185\u5bb9\u3001\u4fdd\u6301\u4e00\u81f4\u6027\u5e76\u5904\u7406\u573a\u666f\u81ea\u9002\u5e94\u9000\u5316\u7684\u6846\u67b6\u3002", "method": "1. VLLVE\uff1a\u89c6\u56fe\u65e0\u5173\u5206\u91cf\uff08\u6355\u6349\u5185\u5728\u5916\u89c2\uff09\u4f7f\u7528\u52a8\u6001\u8de8\u5e27\u5bf9\u5e94\u5173\u7cfb\uff0c\u89c6\u56fe\u76f8\u5173\u5206\u91cf\uff08\u63cf\u8ff0\u5149\u7167\u6761\u4ef6\uff09\u65bd\u52a0\u573a\u666f\u7ea7\u8fde\u7eed\u6027\u7ea6\u675f\u30022. \u53cc\u7ed3\u6784\u589e\u5f3a\u7f51\u7edc\uff1a\u8de8\u5e27\u4ea4\u4e92\u673a\u5236\u76d1\u7763\u4e0d\u540c\u5e27\u83b7\u5f97\u5339\u914d\u5206\u89e3\u7279\u5f81\u30023. VLLVE++\uff1a\u5f15\u5165\u52a0\u6027\u6b8b\u5dee\u9879\u6a21\u62df\u573a\u666f\u81ea\u9002\u5e94\u9000\u5316\uff0c\u652f\u6301\u589e\u5f3a\u548c\u9000\u5316\u611f\u77e5\u5bf9\u5e94\u5173\u7cfb\u7ec6\u5316\u7684\u53cc\u5411\u5b66\u4e60\u3002", "result": "\u5728\u5e7f\u6cdb\u8ba4\u53ef\u7684\u4f4e\u5149\u89c6\u9891\u589e\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cVLLVE++\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u573a\u666f\u548c\u9ad8\u52a8\u6001\u89c6\u9891\u7b49\u6311\u6218\u6027\u6848\u4f8b\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c6\u9891\u5206\u89e3\u7b56\u7565\u548c\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u5149\u89c6\u9891\u589e\u5f3a\u6027\u80fd\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6b8b\u5dee\u9879\u548c\u53cc\u5411\u5b66\u4e60\u673a\u5236\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u573a\u666f\u548c\u52a8\u6001\u5185\u5bb9\u3002"}}
{"id": "2602.08711", "pdf": "https://arxiv.org/pdf/2602.08711", "abs": "https://arxiv.org/abs/2602.08711", "authors": ["Linli Yao", "Yuancheng Wei", "Yaojie Zhang", "Lei Li", "Xinlong Chen", "Feifan Song", "Ziyue Wang", "Kun Ouyang", "Yuanxin Liu", "Lingpeng Kong", "Qi Liu", "Pengfei Wan", "Kun Gai", "Yuanxing Zhang", "Xu Sun"], "title": "TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.", "AI": {"tldr": "\u63d0\u51faOmni Dense Captioning\u65b0\u4efb\u52a1\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u57fa\u51c6\u6570\u636e\u96c6OmniDCBench\u548c\u8bad\u7ec3\u6570\u636e\u96c6TimeChatCap-42K\uff0c\u5f00\u53d1TimeChat-Captioner-7B\u6a21\u578b\uff0c\u5728\u5bc6\u96c6\u63cf\u8ff0\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u901a\u5e38\u751f\u6210\u7b80\u77ed\u3001\u6982\u62ec\u6027\u7684\u63cf\u8ff0\uff0c\u7f3a\u4e4f\u8fde\u7eed\u3001\u7ec6\u7c92\u5ea6\u3001\u7ed3\u6784\u5316\u7684\u89c6\u542c\u53d9\u4e8b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u7535\u5f71\u5267\u672c\u7684\u8be6\u7ec6\u63cf\u8ff0\uff0c\u8ba9\u8bfb\u8005\u80fd\u591f\u9010\u573a\u666f\u751f\u52a8\u60f3\u8c61\u89c6\u9891\u5185\u5bb9\u3002", "method": "1) \u63d0\u51fa\u516d\u7ef4\u7ed3\u6784\u6a21\u5f0f\u521b\u5efa\"\u811a\u672c\u5f0f\"\u63cf\u8ff0\uff1b2) \u6784\u5efa\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6OmniDCBench\uff1b3) \u63d0\u51faSodaM\u8bc4\u4f30\u6307\u6807\uff1b4) \u6784\u5efa\u8bad\u7ec3\u6570\u636e\u96c6TimeChatCap-42K\uff1b5) \u5f00\u53d1TimeChat-Captioner-7B\u6a21\u578b\uff0c\u4f7f\u7528SFT\u548cGRPO\u8bad\u7ec3\u3002", "result": "TimeChat-Captioner-7B\u5728\u5bc6\u96c6\u63cf\u8ff0\u751f\u6210\u4e0a\u8d85\u8d8aGemini-2.5-Pro\u8fbe\u5230SOTA\uff0c\u5176\u751f\u6210\u7684\u5bc6\u96c6\u63cf\u8ff0\u663e\u8457\u63d0\u5347\u89c6\u542c\u63a8\u7406\uff08DailyOmni\u548cWorldSense\uff09\u548c\u65f6\u95f4\u5b9a\u4f4d\uff08Charades-STA\uff09\u7b49\u4e0b\u6e38\u4efb\u52a1\u80fd\u529b\u3002", "conclusion": "Omni Dense Captioning\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u4efb\u52a1\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u3001\u6307\u6807\u548c\u6a21\u578b\u4e3a\u8fde\u7eed\u3001\u7ec6\u7c92\u5ea6\u3001\u7ed3\u6784\u5316\u7684\u89c6\u542c\u53d9\u4e8b\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.08713", "pdf": "https://arxiv.org/pdf/2602.08713", "abs": "https://arxiv.org/abs/2602.08713", "authors": ["Lachin Naghashyar", "Hunar Batra", "Ashkan Khakzar", "Philip Torr", "Ronald Clark", "Christian Schroeder de Witt", "Constantin Venhoff"], "title": "Towards Understanding Multimodal Fine-Tuning: Spatial Features", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to \"see\". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u9002\u5e94\u8fc7\u7a0b\u7684\u673a\u5236\u5206\u6790\uff0c\u901a\u8fc7\u9636\u6bb5\u5f0f\u6a21\u578b\u5dee\u5f02\u6280\u672f\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5b66\u4e60\"\u770b\"\u7684\u80fd\u529b\uff0c\u8bc6\u522b\u51fa\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u6216\u91cd\u65b0\u5b9a\u5411\u7684\u89c6\u89c9\u504f\u597d\u7279\u5f81\uff0c\u5e76\u8ffd\u8e2a\u8fd9\u4e9b\u7279\u5f81\u5230\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u3002", "motivation": "\u5c3d\u7ba1\u5f53\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bed\u8a00\u4e3b\u5e72\u7f51\u7edc\u5728\u591a\u6a21\u6001\u8bad\u7ec3\u4e2d\u5982\u4f55\u9002\u5e94\u4ee5\u53ca\u89c6\u89c9\u7279\u5b9a\u80fd\u529b\u4f55\u65f6\u51fa\u73b0\u4ecd\u4e0d\u6e05\u695a\u3002\u9700\u8981\u7406\u89e3\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u83b7\u5f97\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u7684\u5185\u5728\u673a\u5236\u3002", "method": "\u4f7f\u7528\u9636\u6bb5\u5f0f\u6a21\u578b\u5dee\u5f02\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9694\u79bb\u591a\u6a21\u6001\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7684\u8868\u5f81\u53d8\u5316\u3002\u901a\u8fc7\u8bc6\u522b\u89c6\u89c9\u504f\u597d\u7279\u5f81\uff0c\u5206\u6790\u8fd9\u4e9b\u7279\u5f81\u5982\u4f55\u7f16\u7801\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u8ffd\u8e2a\u8fd9\u4e9b\u7279\u5f81\u7684\u56e0\u679c\u6fc0\u6d3b\u5230\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u8bc6\u522b\u51fa\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u6216\u91cd\u65b0\u5b9a\u5411\u7684\u89c6\u89c9\u504f\u597d\u7279\u5f81\uff1b2) \u8fd9\u4e9b\u7279\u5f81\u7684\u4e00\u4e2a\u9009\u62e9\u6027\u5b50\u96c6\u80fd\u591f\u53ef\u9760\u5730\u7f16\u7801\u7a7a\u95f4\u5173\u7cfb\uff1b3) \u8fd9\u4e9b\u7279\u5f81\u7684\u56e0\u679c\u6fc0\u6d3b\u53ef\u8ffd\u6eaf\u5230\u4e00\u5c0f\u7fa4\u6ce8\u610f\u529b\u5934\u3002", "conclusion": "\u9636\u6bb5\u5f0f\u6a21\u578b\u5dee\u5f02\u6280\u672f\u63ed\u793a\u4e86\u7a7a\u95f4\u57fa\u7840\u591a\u6a21\u6001\u7279\u5f81\u4f55\u65f6\u4f55\u5730\u51fa\u73b0\uff0c\u5c55\u793a\u4e86\u89c6\u89c9\u57fa\u7840\u5982\u4f55\u91cd\u5851\u539f\u672c\u4ec5\u7528\u4e8e\u6587\u672c\u7684\u7279\u5f81\uff0c\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u8bad\u7ec3\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.08717", "pdf": "https://arxiv.org/pdf/2602.08717", "abs": "https://arxiv.org/abs/2602.08717", "authors": ["Farnaz Khun Jush", "Grit Werner", "Mark Klemens", "Matthias Lenga"], "title": "Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 5 figures, 5 tables", "summary": "Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u7528\u4e8eCT/MR\u56fe\u50cf\u4f53\u90e8\u533a\u57df\u68c0\u6d4b\uff0c\u5176\u4e2d\u57fa\u4e8e\u5206\u5272\u7684\u89c4\u5219\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5728887\u4e2a\u626b\u63cf\u4e0a\u8fbe\u5230\u52a0\u6743F1\u5206\u65700.947(CT)\u548c0.914(MR)\u3002", "motivation": "\u73b0\u6709\u4f53\u90e8\u533a\u57df\u8bc6\u522b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684DICOM\u5143\u6570\u636e\uff0c\u4e14\u4e3b\u8981\u4f7f\u7528\u76d1\u7763\u5b66\u4e60\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u63a2\u7d22\u96f6\u6837\u672c\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u6d41\u6c34\u7ebf\uff1a1) \u57fa\u4e8e\u9884\u8bad\u7ec3\u591a\u5668\u5b98\u5206\u5272\u6a21\u578b\u7684\u89c4\u5219\u7cfb\u7edf\uff1b2) \u653e\u5c04\u79d1\u533b\u751f\u89c4\u5219\u6307\u5bfc\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\uff1b3) \u7ed3\u5408\u89c6\u89c9\u8f93\u5165\u548c\u89e3\u5256\u8bc1\u636e\u7684\u5206\u5272\u611f\u77e5MLLM\u3002", "result": "\u5728887\u4e2aCT\u548cMR\u626b\u63cf\u4e0a\u8bc4\u4f30\uff0c\u57fa\u4e8e\u5206\u5272\u7684\u89c4\u5219\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u4e14\u6700\u7a33\u5b9a\uff0c\u52a0\u6743F1\u5206\u6570\u4e3a0.947(CT)\u548c0.914(MR)\uff0c\u5728\u4e0d\u540c\u6a21\u6001\u548c\u975e\u5178\u578b\u626b\u63cf\u8303\u56f4\u4e0b\u5747\u7a33\u5065\u3002MLLM\u5728\u89c6\u89c9\u7279\u5f81\u660e\u663e\u533a\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5206\u5272\u611f\u77e5MLLM\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "conclusion": "\u57fa\u4e8e\u5206\u5272\u7684\u89c4\u5219\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u96f6\u6837\u672c\u4f53\u90e8\u533a\u57df\u68c0\u6d4b\uff0c\u8bc1\u660e\u4e86\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u4e0d\u4f9d\u8d56DICOM\u5143\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08724", "pdf": "https://arxiv.org/pdf/2602.08724", "abs": "https://arxiv.org/abs/2602.08724", "authors": ["Geng Lin", "Matthias Zwicker"], "title": "Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering", "categories": ["cs.CV", "cs.GR"], "comment": "Project Page: https://rotlight-ir.github.io/", "summary": "Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.", "AI": {"tldr": "RotLight\uff1a\u901a\u8fc7\u65cb\u8f6c\u7269\u4f53\u51cf\u5c11\u53cd\u6e32\u67d3\u4e2d\u7684\u6a21\u7cca\u6027\uff0c\u7ed3\u5408\u4ee3\u7406\u7f51\u683c\u63d0\u5347\u53cd\u7167\u7387\u4f30\u8ba1\u8d28\u91cf", "motivation": "\u5f53\u524d\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u9ad8\u65af\u6cfc\u6e85\u7684\u53cd\u6e32\u67d3\u65b9\u6cd5\u5728\u4f30\u8ba1\u6750\u8d28\u548c\u5149\u7167\u65f6\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u5bfc\u81f4\u53cd\u7167\u7387\u4f30\u8ba1\u4e2d\u51fa\u73b0\u4e0d\u51c6\u786e\u7684\u989c\u8272\u548c\u70d8\u7119\u9634\u5f71\uff0c\u5c3d\u7ba1\u6709\u6b63\u5219\u5316\u5904\u7406\u4f46\u4ecd\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51faRotLight\u7b80\u5355\u91c7\u96c6\u8bbe\u7f6e\uff0c\u8981\u6c42\u7269\u4f53\u5728\u91c7\u96c6\u8fc7\u7a0b\u4e2d\u65cb\u8f6c\u591a\u6b21\uff08\u6700\u5c11\u4e24\u6b21\uff09\u4ee5\u51cf\u5c11\u6a21\u7cca\u6027\uff1b\u540c\u65f6\u5f15\u5165\u4ee3\u7406\u7f51\u683c\uff0c\u652f\u6301\u7cbe\u786e\u5165\u5c04\u5149\u8ffd\u8e2a\u3001\u6b8b\u5dee\u7ea6\u675f\u548c\u6539\u8fdb\u5168\u5c40\u5149\u7167\u5904\u7406\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u53cd\u7167\u7387\u4f30\u8ba1\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "RotLight\u901a\u8fc7\u7b80\u5355\u7684\u65cb\u8f6c\u91c7\u96c6\u8bbe\u7f6e\u548c\u4ee3\u7406\u7f51\u683c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53cd\u6e32\u67d3\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6750\u8d28\u5206\u89e3\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.08725", "pdf": "https://arxiv.org/pdf/2602.08725", "abs": "https://arxiv.org/abs/2602.08725", "authors": ["Yongwen Lai", "Chaoqun Wang", "Shaobo Min"], "title": "FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing", "categories": ["cs.CV"], "comment": "Accepted by ICASSP 2026", "summary": "Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \\href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.", "AI": {"tldr": "FusionEdit\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u8f6f\u63a9\u7801\u548c\u6ce8\u610f\u529b\u878d\u5408\u5b9e\u73b0\u7cbe\u786e\u53ef\u63a7\u7684\u7f16\u8f91\uff0c\u907f\u514d\u786c\u63a9\u7801\u8fb9\u754c\u5e26\u6765\u7684\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4f7f\u7528\u663e\u5f0f\u4e8c\u8fdb\u5236\u63a9\u7801\u6765\u7ea6\u675f\u7f16\u8f91\u533a\u57df\uff0c\u4f46\u786c\u63a9\u7801\u8fb9\u754c\u4f1a\u5f15\u5165\u4f2a\u5f71\u5e76\u964d\u4f4e\u7f16\u8f91\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u7cbe\u786e\u63a7\u5236\u7f16\u8f91\u533a\u57df\uff0c\u53c8\u80fd\u4fdd\u6301\u81ea\u7136\u8fc7\u6e21\u7684\u65b9\u6cd5\u3002", "method": "1. \u901a\u8fc7\u6d4b\u91cf\u6e90\u63d0\u793a\u8bcd\u548c\u76ee\u6807\u63d0\u793a\u8bcd\u8bed\u4e49\u5dee\u5f02\u81ea\u52a8\u8bc6\u522b\u7f16\u8f91\u548c\u4fdd\u7559\u533a\u57df\uff1b2. \u5728\u533a\u57df\u8fb9\u754c\u8fdb\u884c\u8ddd\u79bb\u611f\u77e5\u7684\u6f5c\u5728\u878d\u5408\u751f\u6210\u8f6f\u63a9\u7801\uff0c\u5e76\u4f7f\u7528\u603b\u53d8\u5dee\u635f\u5931\u786e\u4fdd\u5e73\u6ed1\u8fc7\u6e21\uff1b3. \u5728DiT\u6ce8\u610f\u529b\u5c42\u4e2d\u91c7\u7528AdaIN\u8c03\u5236\u8fdb\u884c\u7edf\u8ba1\u6ce8\u610f\u529b\u878d\u5408\uff0c\u589e\u5f3a\u7f16\u8f91\u80fd\u529b\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFusionEdit\u5728\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u66f4\u81ea\u7136\u7684\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "FusionEdit\u901a\u8fc7\u8f6f\u63a9\u7801\u751f\u6210\u548c\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u786c\u63a9\u7801\u8fb9\u754c\u5e26\u6765\u7684\u4f2a\u5f71\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u53ef\u63a7\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7f16\u8f91\u7ed3\u679c\u7684\u89c6\u89c9\u81ea\u7136\u6027\u3002"}}
{"id": "2602.08726", "pdf": "https://arxiv.org/pdf/2602.08726", "abs": "https://arxiv.org/abs/2602.08726", "authors": ["Khadija Iddrisu", "Waseem Shariff", "Suzanne Little", "Noel OConnor"], "title": "SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training", "categories": ["cs.CV"], "comment": "Accepted to the 2nd Workshop on \"Event-based Vision in the Era of Generative AI - Transforming Perception and Visual Innovation, IEEE Winter Conference on Applications of Computer Vision (WACV 2026)", "summary": "The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.", "AI": {"tldr": "\u4f7f\u7528Blender\u751f\u6210\u5408\u6210\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\u6a21\u62df\u773c\u52a8\uff0c\u7ed3\u5408\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6548\u7684\u773c\u52a8\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe0.83\uff0c\u8ba1\u7b97\u6548\u7387\u4f18\u4e8e\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc", "motivation": "\u4f20\u7edf\u5e27\u76f8\u673a\u5b58\u5728\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\uff08DVS\uff09\u80fd\u5f02\u6b65\u8bb0\u5f55\u5149\u5f3a\u53d8\u5316\uff0c\u6d88\u9664\u8fd0\u52a8\u6a21\u7cca\u5e76\u63d0\u4f9b\u66f4\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u3002\u9700\u8981\u51c6\u786e\u5206\u7c7b\u773c\u52a8\uff08\u626b\u89c6\u548c\u6ce8\u89c6\uff09\u6765\u7406\u89e3\u4eba\u7c7b\u8ba4\u77e5\u673a\u5236\uff0c\u4f46\u7f3a\u4e4f\u5408\u9002\u7684\u5408\u6210\u6570\u636e\u96c6\u3002", "method": "1. \u4f7f\u7528Blender\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u6a21\u62df\u626b\u89c6\u548c\u6ce8\u89c6\u7684\u53d7\u63a7\u6761\u4ef6\uff1b2. \u91c7\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u67b6\u6784\uff1b3. \u5728\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff1b4. \u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "1. \u6a21\u578b\u51c6\u786e\u7387\u8fbe\u52300.83\uff1b2. \u5728\u4e0d\u540c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff1b3. \u76f8\u6bd4\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\uff0cSNN\u7ed3\u5408\u5408\u6210\u4e8b\u4ef6\u6d41\u5e26\u6765\u663e\u8457\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff1b4. \u8bc1\u660e\u4e86\u5408\u6210\u6570\u636e\u589e\u5f3a\u5728\u4e8b\u4ef6\u89c6\u89c9\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u96c6\u4e0e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\u4e3a\u773c\u52a8\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u751f\u7269\u89c6\u89c9\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4fc3\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2602.08727", "pdf": "https://arxiv.org/pdf/2602.08727", "abs": "https://arxiv.org/abs/2602.08727", "authors": ["Johannes Thalhammer", "Tina Dorosti", "Sebastian Peterhansl", "Daniela Pfeiffer", "Franz Pfeiffer", "Florian Schaff"], "title": "Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u54082D\u548c3D\u6a21\u578b\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6b20\u91c7\u6837CT\u4f53\u79ef\u4e2d\u53bb\u9664\u4f2a\u5f71\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u4f53\u79ef\u4e00\u81f4\u6027\u3002", "motivation": "\u6b20\u91c7\u6837CT\u4f53\u79ef\u867d\u7136\u51cf\u5c11\u4e86\u91c7\u96c6\u65f6\u95f4\u548c\u8f90\u5c04\u66b4\u9732\uff0c\u4f46\u4f1a\u5f15\u5165\u4f2a\u5f71\uff0c\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u548c\u8bca\u65ad\u4ef7\u503c\u3002\u9700\u8981\u9ad8\u6548\u53bb\u9664\u8fd9\u4e9b\u4f2a\u5f71\u4ee5\u83b7\u5f97\u9ad8\u8d28\u91cf3D CT\u56fe\u50cf\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6df7\u5408\u6846\u67b6\uff1a\u9996\u5148\u4f7f\u75282D U-Net\u5904\u7406\u6b20\u91c7\u6837CT\u4f53\u79ef\u7684\u5355\u4e2a\u5207\u7247\u63d0\u53d6\u7279\u5f81\u56fe\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u5207\u7247\u7279\u5f81\u56fe\u5806\u53e0\u6210\u4f53\u79ef\uff0c\u8f93\u5165\u52303D\u89e3\u7801\u5668\u4e2d\uff0c\u5229\u7528\u5207\u7247\u95f4\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u9884\u6d4b\u65e0\u4f2a\u5f71\u76843D CT\u4f53\u79ef\u3002", "result": "\u5728\u51a0\u72b6\u9762\u548c\u77e2\u72b6\u9762\u65b9\u5411\u4e0a\u663e\u8457\u6539\u5584\u4e86\u5207\u7247\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u8be5\u6846\u67b6\u4e3a\u9ad8\u8d28\u91cf3D CT\u56fe\u50cf\u540e\u5904\u7406\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6210\u529f\u5e73\u8861\u4e862D\u5904\u7406\u7684\u8ba1\u7b97\u6548\u7387\u548c3D\u5efa\u6a21\u7684\u4f53\u79ef\u4e00\u81f4\u6027\uff0c\u4e3a\u6b20\u91c7\u6837CT\u4f53\u79ef\u7684\u4f2a\u5f71\u53bb\u9664\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08730", "pdf": "https://arxiv.org/pdf/2602.08730", "abs": "https://arxiv.org/abs/2602.08730", "authors": ["Shanshan Wang", "Ziying Feng", "Xiaozheng Shen", "Xun Yang", "Pichao Wang", "Zhenwei He", "Xingyi Zhang"], "title": "Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA", "AI": {"tldr": "CGA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u6e90\u81ea\u7531\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u548c\u7f13\u89e3\u7c7b\u522b\u6df7\u6dc6\u95ee\u9898\uff0c\u5728\u7ec6\u7c92\u5ea6\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709SFDA\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5ffd\u7565\u4e86\u7c7b\u522b\u95f4\u7684\u4e0d\u5bf9\u79f0\u52a8\u6001\u6df7\u6dc6\u6a21\u5f0f\uff0c\u5bfc\u81f4\u4f2a\u6807\u7b7e\u566a\u58f0\u5927\u548c\u76ee\u6807\u5224\u522b\u6027\u5dee\u3002", "method": "\u63d0\u51faCLIP-Guided Alignment\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aMCA\u68c0\u6d4b\u6df7\u6dc6\u5bf9\uff0cMCC\u5229\u7528CLIP\u6784\u5efa\u6df7\u6dc6\u611f\u77e5\u6587\u672c\u63d0\u793a\uff0cFAM\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u7279\u5f81\u7a7a\u95f4\u4ee5\u51cf\u5c11\u6a21\u7cca\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCGA\u5728\u6613\u6df7\u6dc6\u548c\u7ec6\u7c92\u5ea6\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709SFDA\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u663e\u5f0f\u5efa\u6a21\u7c7b\u522b\u95f4\u6df7\u6dc6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u7c7b\u522b\u95f4\u6df7\u6dc6\u5bf9\u6709\u6548\u7684\u6e90\u81ea\u7531\u57df\u81ea\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0cCGA\u901a\u8fc7CLIP\u5f15\u5bfc\u7684\u6df7\u6dc6\u7f13\u89e3\u7b56\u7565\u5728\u7ec6\u7c92\u5ea6\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2602.08735", "pdf": "https://arxiv.org/pdf/2602.08735", "abs": "https://arxiv.org/abs/2602.08735", "authors": ["Masanari Oi", "Koki Maeda", "Ryuto Koike", "Daisuke Oba", "Nakamasa Inoue", "Naoaki Okazaki"], "title": "From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.", "AI": {"tldr": "HATCH\u662f\u4e00\u4e2a\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8865\u4e01\u7ea7\u7a7a\u95f4\u5bf9\u9f50\u548c\u884c\u52a8-\u7b54\u6848\u63a8\u7406\u4e24\u4e2a\u4e92\u8865\u76ee\u6807\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u5355\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u9700\u8981\u6574\u5408\u591a\u4e2a\u89c6\u89d2\u4fe1\u606f\u7684\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002\u4eba\u7c7b\u901a\u8fc7\u8de8\u89c6\u89d2\u5bf9\u5e94\u548c\u9010\u6b65\u89c6\u89d2\u53d8\u6362\u4e24\u79cd\u673a\u5236\u89e3\u51b3\u8fd9\u7c7b\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u53ea\u90e8\u5206\u4e14\u9690\u5f0f\u5730\u7ed3\u5408\u8fd9\u4e9b\u673a\u5236\uff0c\u7f3a\u4e4f\u5bf9\u4e24\u8005\u7684\u663e\u5f0f\u76d1\u7763\u3002", "method": "\u63d0\u51faHATCH\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u76ee\u6807\uff1a1) \u8865\u4e01\u7ea7\u7a7a\u95f4\u5bf9\u9f50\uff1a\u9f13\u52b1\u4e0d\u540c\u89c6\u89d2\u4e2d\u7a7a\u95f4\u5bf9\u5e94\u533a\u57df\u7684\u8865\u4e01\u8868\u793a\u5bf9\u9f50\uff1b2) \u884c\u52a8-\u7b54\u6848\u63a8\u7406\uff1a\u8981\u6c42\u6a21\u578b\u5728\u9884\u6d4b\u6700\u7ec8\u7b54\u6848\u524d\u751f\u6210\u663e\u5f0f\u7684\u89c6\u89d2\u8f6c\u6362\u884c\u52a8\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cHATCH\u59cb\u7ec8\u4ee5\u660e\u663e\u4f18\u52bf\u8d85\u8d8a\u540c\u7b49\u89c4\u6a21\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e0e\u66f4\u5927\u6a21\u578b\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u5355\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8de8\u89c6\u89d2\u5bf9\u5e94\u548c\u9010\u6b65\u89c6\u89d2\u53d8\u6362\u8fd9\u4e24\u79cd\u4eba\u7c7b\u8ba4\u77e5\u673a\u5236\uff0cHATCH\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.08749", "pdf": "https://arxiv.org/pdf/2602.08749", "abs": "https://arxiv.org/abs/2602.08749", "authors": ["Carmine Zaccagnino", "Fabio Quattrini", "Enis Simsar", "Marta Tintor\u00e9 Gazulla", "Rita Cucchiara", "Alessio Tonioni", "Silvia Cascianelli"], "title": "Shifting the Breaking Point of Flow Matching for Multi-Instance Editing", "categories": ["cs.CV"], "comment": null, "summary": "Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.", "AI": {"tldr": "\u63d0\u51faInstance-Disentangled Attention\u673a\u5236\uff0c\u89e3\u51b3\u73b0\u6709\u6d41\u5339\u914d\u6a21\u578b\u5728\u591a\u5b9e\u4f8b\u7f16\u8f91\u4e2d\u8bed\u4e49\u5e72\u6270\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5355\u6b21\u63a8\u7406\u7684\u5b9e\u4f8b\u7ea7\u7f16\u8f91", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u56fe\u50cf\u7f16\u8f91\u5668\u4e3b\u8981\u652f\u6301\u5168\u5c40\u6216\u5355\u6307\u4ee4\u7f16\u8f91\uff0c\u5728\u591a\u5b9e\u4f8b\u573a\u666f\u4e2d\u96be\u4ee5\u72ec\u7acb\u7f16\u8f91\u591a\u4e2a\u90e8\u5206\u800c\u4e0d\u4ea7\u751f\u8bed\u4e49\u5e72\u6270\uff0c\u8fd9\u6e90\u4e8e\u5168\u5c40\u6761\u4ef6\u5316\u7684\u901f\u5ea6\u573a\u548c\u8054\u5408\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u7f16\u8f91\u7ea0\u7f20", "method": "\u5f15\u5165Instance-Disentangled Attention\u673a\u5236\uff0c\u901a\u8fc7\u5206\u5272\u8054\u5408\u6ce8\u610f\u529b\u64cd\u4f5c\uff0c\u5728\u901f\u5ea6\u573a\u4f30\u8ba1\u671f\u95f4\u5f3a\u5236\u7ed1\u5b9a\u5b9e\u4f8b\u7279\u5b9a\u7684\u6587\u672c\u6307\u4ee4\u4e0e\u7a7a\u95f4\u533a\u57df", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u7136\u56fe\u50cf\u7f16\u8f91\u548c\u6587\u672c\u5bc6\u96c6\u4fe1\u606f\u56fe\u7f16\u8f91\u4e2d\u90fd\u80fd\u4fc3\u8fdb\u7f16\u8f91\u89e3\u8026\u548c\u5c40\u90e8\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u8f93\u51fa\u4e00\u81f4\u6027", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5355\u6b21\u63a8\u7406\u7684\u5b9e\u4f8b\u7ea7\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u591a\u5b9e\u4f8b\u7f16\u8f91\u4e2d\u7684\u8bed\u4e49\u5e72\u6270\u95ee\u9898"}}
{"id": "2602.08753", "pdf": "https://arxiv.org/pdf/2602.08753", "abs": "https://arxiv.org/abs/2602.08753", "authors": ["Tianyu Sun", "Zhoujie Fu", "Bang Zhang", "Guosheng Lin"], "title": "MVAnimate: Enhancing Character Animation with Multi-View Optimization", "categories": ["cs.CV"], "comment": null, "summary": "The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.", "AI": {"tldr": "MVAnimate\u662f\u4e00\u4e2a\u5229\u7528\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u751f\u6210\u9ad8\u8d28\u91cf2D\u548c3D\u89d2\u8272\u52a8\u753b\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8f93\u51fa\u8d28\u91cf\u4f4e\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e2D\u62163D\u4eba\u4f53\u59ff\u6001\u5efa\u6a21\u7684\u52a8\u753b\u751f\u6210\u7b97\u6cd5\u5b58\u5728\u8f93\u51fa\u8d28\u91cf\u4f4e\u3001\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u52a8\u753b\u89c6\u9891\u3002\u73b0\u5b9e\u4e14\u591a\u529f\u80fd\u7684\u89d2\u8272\u52a8\u753b\u9700\u6c42\u5728\u5404\u4e2a\u9886\u57df\u90fd\u5728\u589e\u957f\u3002", "method": "MVAnimate\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u5408\u6210\u52a8\u6001\u4eba\u7269\u76842D\u548c3D\u4fe1\u606f\u3002\u5b83\u5229\u7528\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u548c\u7a7a\u95f4\u8fde\u8d2f\u7684\u52a8\u753b\u8f93\u51fa\uff0c\u5e76\u4f18\u5316\u76ee\u6807\u89d2\u8272\u7684\u591a\u89c6\u89d2\u89c6\u9891\uff0c\u4ece\u4e0d\u540c\u89c6\u89d2\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5bf9\u5404\u7c7b\u8fd0\u52a8\u6a21\u5f0f\u548c\u5916\u89c2\u7684\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u52a8\u753b\u65b9\u6cd5\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "MVAnimate\u901a\u8fc7\u6574\u5408\u591a\u89c6\u89d2\u4fe1\u606f\u6709\u6548\u63d0\u5347\u4e86\u52a8\u753b\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89d2\u8272\u52a8\u753b\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08775", "pdf": "https://arxiv.org/pdf/2602.08775", "abs": "https://arxiv.org/abs/2602.08775", "authors": ["Vineet Kumar Rakesh", "Ahana Bhattacharjee", "Soumya Mazumdar", "Tapas Samanta", "Hemendra Kumar Pandey", "Amitabha Das", "Sarbajit Pal"], "title": "VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars", "categories": ["cs.CV", "cs.CG"], "comment": null, "summary": "Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u5420\u9640\u8ba1\u7b97\u7684\u8f7b\u91cf\u7ea7\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u6846\u67b6\uff0c\u53ef\u5728CPU\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u6559\u80b2\u73af\u5883", "motivation": "\u5f53\u524d\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56GPU\u6e32\u67d3\u3001\u5927\u8bad\u7ec3\u96c6\u6216\u9ad8\u5bb9\u91cf\u6269\u6563\u6a21\u578b\uff0c\u96be\u4ee5\u5728\u79bb\u7ebf\u6216\u8d44\u6e90\u53d7\u9650\u7684\u5b66\u4e60\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7CPU\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u7b26\u53f7\u5420\u9640\u8ba1\u7b97\u6846\u67b6\uff1a1) \u8bed\u97f3\u8f6c\u65f6\u95f4\u5bf9\u9f50\u97f3\u7d20\u6d41 2) \u97f3\u7d20\u6620\u5c04\u5230\u7d27\u51d1\u89c6\u7d20\u5e93 3) \u57fa\u4e8e\u5420\u9640\u7ecfUrdhva Tiryakbhyam\u542f\u53d1\u7684\u7b26\u53f7\u534f\u540c\u53d1\u97f3\u751f\u6210\u5e73\u6ed1\u89c6\u7d20\u8f68\u8ff9 4) \u8f7b\u91cf2D\u6e32\u67d3\u5668\u8fdb\u884cROI\u626d\u66f2\u548c\u5634\u90e8\u5408\u6210\u7a33\u5b9a", "result": "\u5728\u4ec5CPU\u6267\u884c\u4e0b\u5b9e\u73b0\u4e86\u53ef\u63a5\u53d7\u7684\u5507\u540c\u6b65\u8d28\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\u548c\u5ef6\u8fdf\uff0c\u652f\u6301\u5728\u4f4e\u7aef\u786c\u4ef6\u4e0a\u90e8\u7f72\u5b9e\u7528\u7684\u6559\u80b2\u5934\u50cf", "conclusion": "\u7b26\u53f7\u5420\u9640\u8ba1\u7b97\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u6559\u80b2\u6280\u672f\u4e2d\u7684\u79bb\u7ebf\u90e8\u7f72"}}
{"id": "2602.08792", "pdf": "https://arxiv.org/pdf/2602.08792", "abs": "https://arxiv.org/abs/2602.08792", "authors": ["Hao Dong", "Eleni Chatzi", "Olga Fink"], "title": "Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u6846\u67b6\u7ed3\u5408\u89c6\u89c9\u4e0e\u529b\u5b66\u6570\u636e\uff0c\u6539\u8fdb\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u63a5\u53e3\u7535\u5f27\u68c0\u6d4b\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u3001\u566a\u58f0\u5e72\u6270\u7b49\u95ee\u9898", "motivation": "\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u63a5\u53e3\u7684\u7535\u5f27\u73b0\u8c61\u5bf9\u94c1\u8def\u4f9b\u7535\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u5305\u62ec\u52a0\u901f\u90e8\u4ef6\u78e8\u635f\u3001\u6027\u80fd\u4e0b\u964d\u548c\u670d\u52a1\u4e2d\u65ad\u3002\u7535\u5f27\u68c0\u6d4b\u9762\u4e34\u77ac\u6001\u7279\u6027\u3001\u566a\u58f0\u73af\u5883\u3001\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u4e0e\u5176\u4ed6\u77ac\u6001\u73b0\u8c61\u96be\u4ee5\u533a\u5206\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6846\u67b6\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u529b\u5b66\u6d4b\u91cf\u6570\u636e\u3002\u6784\u5efa\u4e24\u4e2a\u7535\u5f27\u68c0\u6d4b\u6570\u636e\u96c6\uff08SBB\u6570\u636e\u548c\u516c\u5f00\u89c6\u9891+\u5408\u6210\u529b\u5b66\u6570\u636e\uff09\u3002\u63d0\u51faMultiDeepSAD\u7b97\u6cd5\uff0c\u6269\u5c55DeepSAD\u4ee5\u652f\u6301\u591a\u6a21\u6001\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u635f\u5931\u51fd\u6570\u3002\u9488\u5bf9\u6bcf\u79cd\u6570\u636e\u7c7b\u578b\u8bbe\u8ba1\u4e13\u95e8\u7684\u4f2a\u5f02\u5e38\u751f\u6210\u6280\u672f\uff0c\u5982\u56fe\u50cf\u4e2d\u7684\u5408\u6210\u7535\u5f27\u4f2a\u5f71\u548c\u6a21\u62df\u529b\u5b66\u5f02\u5e38\uff0c\u4ee5\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u9886\u57df\u504f\u79fb\u548c\u771f\u5b9e\u7535\u5f27\u89c2\u6d4b\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u771f\u5b9e\u7535\u5f27\u4e8b\u4ef6\u4e5f\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u529b\u5b66\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u7535\u5f27\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u94c1\u8def\u4f9b\u7535\u7cfb\u7edf\u7684\u53ef\u9760\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2602.08794", "pdf": "https://arxiv.org/pdf/2602.08794", "abs": "https://arxiv.org/abs/2602.08794", "authors": ["SII-OpenMOSS Team", ":", "Donghua Yu", "Mingshu Chen", "Qi Chen", "Qi Luo", "Qianyi Wu", "Qinyuan Cheng", "Ruixiao Li", "Tianyi Liang", "Wenbo Zhang", "Wenming Tu", "Xiangyu Peng", "Yang Gao", "Yanru Huo", "Ying Zhu", "Yinze Luo", "Yiyang Zhang", "Yuerong Song", "Zhe Xu", "Zhiyu Zhang", "Chenchen Yang", "Cheng Chang", "Chushu Zhou", "Hanfu Chen", "Hongnan Ma", "Jiaxi Li", "Jingqi Tong", "Junxi Liu", "Ke Chen", "Shimin Li", "Songlin Wang", "Wei Jiang", "Zhaoye Fei", "Zhiyuan Ning", "Chunguo Li", "Chenhui Li", "Ziwei He", "Zengfeng Huang", "Xie Chen", "Xipeng Qiu"], "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "categories": ["cs.CV", "cs.SD"], "comment": "Technical report for MOVA (open-source video-audio generation model). 38 pages, 10 figures, 22 tables. Project page: https://mosi.cn/models/mova Code: https://github.com/OpenMOSS/MOVA Models: https://huggingface.co/collections/OpenMOSS-Team/mova. Qinyuan Cheng and Tianyi Liang are project leader. Xie Chen and Xipeng Qiu are corresponding authors", "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "AI": {"tldr": "MOVA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u652f\u6301\u56fe\u50cf/\u6587\u672c\u5230\u89c6\u9891/\u97f3\u9891\u7684\u751f\u6210\u4efb\u52a1\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u540c\u6b65\u7684\u89c6\u542c\u5185\u5bb9\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5927\u591a\u5ffd\u89c6\u97f3\u9891\u7ec4\u4ef6\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7ea7\u8054\u7ba1\u9053\u5bfc\u81f4\u6210\u672c\u9ad8\u3001\u8bef\u5dee\u7d2f\u79ef\u548c\u8d28\u91cf\u4e0b\u964d\u3002\u73b0\u6709\u7cfb\u7edf\u5982Veo 3\u548cSora 2\u867d\u7136\u5f3a\u8c03\u540c\u65f6\u751f\u6210\uff0c\u4f46\u5b58\u5728\u67b6\u6784\u3001\u6570\u636e\u548c\u8bad\u7ec3\u6311\u6218\uff0c\u4e14\u95ed\u6e90\u7279\u6027\u9650\u5236\u4e86\u9886\u57df\u8fdb\u5c55\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u603b\u53c2\u6570\u91cf320\u4ebf\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b180\u4ebf\u53c2\u6570\u3002\u652f\u6301IT2VA\u4efb\u52a1\uff0c\u80fd\u591f\u751f\u6210\u5507\u8bed\u540c\u6b65\u7684\u8bed\u97f3\u3001\u73af\u5883\u611f\u77e5\u97f3\u6548\u548c\u5185\u5bb9\u5bf9\u9f50\u7684\u97f3\u4e50\u3002", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u540c\u6b65\u89c6\u542c\u5185\u5bb9\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5305\u62ec\u903c\u771f\u7684\u5507\u8bed\u540c\u6b65\u8bed\u97f3\u3001\u73af\u5883\u611f\u77e5\u97f3\u6548\u548c\u5185\u5bb9\u5bf9\u9f50\u97f3\u4e50\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\uff0c\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u53d1\u5c55\u5e76\u57f9\u80b2\u521b\u4f5c\u8005\u793e\u533a\u3002\u4ee3\u7801\u5e93\u652f\u6301\u9ad8\u6548\u63a8\u7406\u3001LoRA\u5fae\u8c03\u548c\u63d0\u793a\u589e\u5f3a\u3002"}}
{"id": "2602.08797", "pdf": "https://arxiv.org/pdf/2602.08797", "abs": "https://arxiv.org/abs/2602.08797", "authors": ["Jiaming Liu", "Cheng Ding", "Daoqiang Zhang"], "title": "Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 7 figures. Submitted to IEEE Journal of Biomedical and Health Informatics (JBHI)", "summary": "Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.", "AI": {"tldr": "\u63d0\u51fa\u534a\u76d1\u7763\u5e08\u751f\u6846\u67b6\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f2a\u6807\u7b7e\u6559\u5e08\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6e10\u8fdb\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u5b9e\u73b0\u9ad8\u6548\u8111\u80bf\u7624\u5206\u5272", "motivation": "\u8111\u80bf\u7624MRI\u5206\u5272\u9762\u4e34\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u6570\u636e\u5f02\u8d28\u6027\uff08\u4e0d\u540c\u626b\u63cf\u4eea\u548c\u7ad9\u70b9\uff09\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u5728\u6709\u9650\u76d1\u7763\u4e0b\u9c81\u68d2\u7684\u5206\u5272\u65b9\u6cd5", "method": "1) \u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6559\u5e08\u6a21\u578b\u751f\u6210\u6982\u7387\u63a9\u7801\u548c\u9010\u50cf\u7d20\u4e0d\u786e\u5b9a\u6027\uff1b2) \u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6e10\u8fdb\u8bfe\u7a0b\uff1a\u6309\u56fe\u50cf\u7ea7\u7f6e\u4fe1\u5ea6\u5bf9\u672a\u6807\u6ce8\u626b\u63cf\u6392\u5e8f\u5e76\u5206\u9636\u6bb5\u5f15\u5165\uff1b3) \u53cc\u91cd\u635f\u5931\u76ee\u6807\uff1a\u5b66\u751f\u4ece\u9ad8\u7f6e\u4fe1\u533a\u57df\u5b66\u4e60\uff0c\u4ece\u4f4e\u7f6e\u4fe1\u533a\u57df\"\u5fd8\u8bb0\"\uff1b4) \u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u4f2a\u6807\u7b7e\u7ec6\u5316", "result": "\u5728BraTS 2021\u4e0a\uff1a\u9a8c\u8bc1DSC\u4ece0.393\uff0810%\u6570\u636e\uff09\u63d0\u5347\u52300.872\uff08100%\uff09\uff0c\u65e9\u671f\u9636\u6bb5\u63d0\u5347\u6700\u5927\uff1b\u6559\u5e08DSC\u8fbe0.922\uff0c\u5b66\u751f\u5728\u80bf\u7624\u4e9a\u533a\u8d85\u8d8a\u6559\u5e08\uff08NCR/NET 0.797\uff0cEdema 0.980\uff09\uff0c\u7279\u522b\u662f\u5728\u6559\u5e08\u5931\u8d25\u7684\u589e\u5f3a\u7c7b\u522b\u4e0a\u6062\u590dDSC 0.620", "conclusion": "\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u8bfe\u7a0b\u5b66\u4e60\u548c\u9009\u62e9\u6027\"\u5fd8\u8bb0\"\u673a\u5236\u80fd\u591f\u5728\u6709\u9650\u76d1\u7763\u548c\u566a\u58f0\u4f2a\u6807\u7b7e\u4e0b\u63d0\u4f9b\u9c81\u68d2\u7684\u5206\u5272\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u636e\u6548\u7387"}}
{"id": "2602.08820", "pdf": "https://arxiv.org/pdf/2602.08820", "abs": "https://arxiv.org/abs/2602.08820", "authors": ["Hao Yang", "Zhiyu Tan", "Jia Gong", "Luozheng Qin", "Hesen Chen", "Xiaomeng Yang", "Yuqing Sun", "Yuetan Lin", "Mengping Yang", "Hao Li"], "title": "Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing", "categories": ["cs.CV"], "comment": "Technical Report, Project: https://howellyoung-s.github.io/Omni-Video2-project/", "summary": "We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \\emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.", "AI": {"tldr": "Omni-Video 2\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u89c6\u9891\u751f\u6210\u7f16\u8f91\u6a21\u578b\uff0c\u901a\u8fc7\u8fde\u63a5\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528MLLM\u7684\u7406\u89e3\u63a8\u7406\u80fd\u529b\u751f\u6210\u76ee\u6807\u63cf\u8ff0\u6765\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u548c\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7ec4\u5408\u6307\u4ee4\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u7528\u6237\u6307\u4ee4\u5e76\u6267\u884c\u7cbe\u7ec6\u7f16\u8f91\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "1. \u5229\u7528\u9884\u8bad\u7ec3MLLM\u7684\u7406\u89e3\u63a8\u7406\u80fd\u529b\u751f\u6210\u663e\u5f0f\u76ee\u6807\u63cf\u8ff0\u6765\u89e3\u91ca\u7528\u6237\u6307\u4ee4\uff1b2. \u5f00\u53d1\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5c06\u591a\u6a21\u6001\u6761\u4ef6\u6807\u8bb0\u6ce8\u5165\u9884\u8bad\u7ec3\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\uff1b3. \u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u6570\u636e\u4e0a\u6269\u5c55\u523014B\u53c2\u6570\u89c4\u6a21\u3002", "result": "\u5728FiVE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5904\u7406\u590d\u6742\u7ec4\u5408\u6307\u4ee4\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5728VBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u548c\u591a\u79cd\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "Omni-Video 2\u901a\u8fc7\u8fde\u63a5MLLM\u7406\u89e3\u548c\u89c6\u9891\u6269\u6563\u751f\u6210\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u7edf\u4e00\u89c6\u9891\u751f\u6210\u7f16\u8f91\u6846\u67b6\uff0c\u5728\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.08822", "pdf": "https://arxiv.org/pdf/2602.08822", "abs": "https://arxiv.org/abs/2602.08822", "authors": ["Yao Pu", "Yiming Shi", "Zhenxi Zhang", "Peixin Yu", "Yitao Zhuang", "Xiang Wang", "Hongzhao Chen", "Jing Cai", "Ge Ren"], "title": "Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications", "categories": ["cs.CV"], "comment": null, "summary": "Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4efb\u610f\u5230\u6240\u6709MRI\u5408\u6210\uff0c\u63d0\u5347\u9f3b\u54bd\u764c\u653e\u7597\u89c4\u5212\u51c6\u786e\u6027\u3002", "motivation": "MRI\u5bf9\u9f3b\u54bd\u764c\u653e\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u4e34\u5e8a\u4e2d\u5e38\u56e0\u60a3\u8005\u4e0d\u9002\u3001\u626b\u63cf\u65f6\u95f4\u957f\u3001\u6210\u672c\u9ad8\u7b49\u56e0\u7d20\u5bfc\u81f4\u6a21\u6001\u4e0d\u5b8c\u6574\uff0c\u5f71\u54cd\u653e\u7597\u89c4\u5212\u51c6\u786e\u6027\u3002\u4f20\u7edfMRI\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u7279\u5f02\u6027\u3001\u89e3\u5256\u9002\u5e94\u6027\u6709\u9650\u3001\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9f3b\u54bd\u764c\u653e\u7597\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u5bf9\u6bd4\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u3002\u4f7f\u7528\u5bf9\u6bd4\u7f16\u7801\u5668\u63d0\u53d6\u6a21\u6001\u4e0d\u53d8\u8868\u793a\uff0c\u57fa\u4e8eCLIP\u7684\u6587\u672c\u4fe1\u606f\u89e3\u7801\u5668\u8fdb\u884c\u8bed\u4e49\u4e00\u81f4\u7684\u5408\u6210\uff0c\u901a\u8fc7\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u652f\u6301\u4efb\u610f\u5230\u6240\u6709MRI\u5408\u6210\u3002", "result": "\u572813\u4e2a\u673a\u6784\u768440,825\u5f20\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u572826\u4e2a\u5185\u90e8/\u5916\u90e8\u9a8c\u8bc1\u7ad9\u70b9\uff0815,748\u5f20\u56fe\u50cf\uff09\u4e0a\u5b9e\u73b0\u4e00\u81f4\u9ad8\u6027\u80fd\uff08\u5e73\u5747SSIM 0.90\uff0cPSNR 27\uff09\uff0c\u5177\u6709\u4f18\u5f02\u7684\u5408\u6210\u4fdd\u771f\u5ea6\u4ee5\u53ca\u5bf9\u566a\u58f0\u548c\u57df\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002\u7edf\u4e00\u8868\u793a\u8fd8\u589e\u5f3a\u4e86\u4e0b\u6e38\u653e\u7597\u76f8\u5173\u4efb\u52a1\uff08\u5982\u5206\u5272\uff09\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u6865\u63a5\u6280\u672f\u5408\u6210\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u63a8\u8fdb\u4e86\u9f3b\u54bd\u764c\u62a4\u7406\u7684\u6570\u5b57\u533b\u5b66\u89e3\u51b3\u65b9\u6848\u3002\u6a21\u578b\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u7684MRI\u5408\u6210\u80fd\u529b\uff0c\u652f\u6301\u66f4\u7cbe\u51c6\u7684\u653e\u7597\u89c4\u5212\u3002"}}
{"id": "2602.08828", "pdf": "https://arxiv.org/pdf/2602.08828", "abs": "https://arxiv.org/abs/2602.08828", "authors": ["Hao Tan", "Jun Lan", "Senyuan Shi", "Zichang Tan", "Zijian Yu", "Huijia Zhu", "Weiqiang Wang", "Jun Wan", "Zhen Lei"], "title": "VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project: https://github.com/EricTan7/VideoVeritas", "summary": "The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.", "AI": {"tldr": "VideoVeritas\u6846\u67b6\u901a\u8fc7\u8054\u5408\u504f\u597d\u5bf9\u9f50\u548c\u611f\u77e5\u9884\u6587\u672c\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u4e8b\u5b9e\u63a8\u7406\u6765\u68c0\u6d4b\u751f\u6210\u89c6\u9891\uff0c\u5728MintVid\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u80fd\u529b\u589e\u5f3a\u5e26\u6765\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u5f3a\u4f46\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faVideoVeritas\u6846\u67b6\uff0c\u5305\u542b\u8054\u5408\u504f\u597d\u5bf9\u9f50\u548c\u611f\u77e5\u9884\u6587\u672c\u5f3a\u5316\u5b66\u4e60(PPRL)\u3002\u901a\u8fc7\u65f6\u7a7a\u5b9a\u4f4d\u548c\u81ea\u76d1\u7763\u7269\u4f53\u8ba1\u6570\u7b49\u611f\u77e5\u9884\u4efb\u52a1\u589e\u5f3a\u68c0\u6d4b\u6027\u80fd\uff0c\u800c\u975e\u76f4\u63a5\u4f18\u5316\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u5728MintVid\u6570\u636e\u96c6(\u5305\u542b9\u4e2aSOTA\u751f\u6210\u5668\u76843K\u89c6\u9891\u548c\u771f\u5b9e\u4e16\u754c\u9519\u8bef\u5185\u5bb9\u5b50\u96c6)\u4e0a\u6d4b\u8bd5\uff0cVideoVeritas\u5728\u591a\u6837\u5316\u57fa\u51c6\u4e0a\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u504f\u5411\u8868\u9762\u63a8\u7406\u6216\u673a\u68b0\u5206\u6790\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VideoVeritas\u901a\u8fc7\u7ed3\u5408\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u4e8b\u5b9e\u63a8\u7406\uff0c\u4e3a\u751f\u6210\u89c6\u9891\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u89c6\u9891\u751f\u6210\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2602.08858", "pdf": "https://arxiv.org/pdf/2602.08858", "abs": "https://arxiv.org/abs/2602.08858", "authors": ["Ruihan Xu", "Qingpei Guo", "Yao Zhu", "Xiangyang Ji", "Ming Yang", "Shiliang Zhang"], "title": "FlattenGPT: Depth Compression for Transformer with Layer Flattening", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to ICML 2026", "summary": "Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \\textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\\% of zero-shot performance with a compression ratio of 20\\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.", "AI": {"tldr": "FlattenGPT\u662f\u4e00\u79cd\u65b0\u9896\u7684Transformer\u6df1\u5ea6\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u6241\u5e73\u5316\u76f8\u90bb\u5757\u6765\u51cf\u5c11\u6df1\u5ea6\u5197\u4f59\uff0c\u540c\u65f6\u4fdd\u7559\u6240\u6709\u5757\u5b66\u5230\u7684\u77e5\u8bc6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u538b\u7f29\u65b9\u6cd5\uff08\u6574\u5757\u526a\u679d\uff09\u4f1a\u4e22\u5f03\u5757\u4e2d\u6709\u610f\u4e49\u7684\u7ebf\u7d22\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u901a\u9053\u526a\u679d\u867d\u7136\u80fd\u66f4\u597d\u4fdd\u7559\u6027\u80fd\u4f46\u65e0\u6cd5\u51cf\u5c11\u6a21\u578b\u6df1\u5ea6\u4e14\u9762\u4e34\u5404\u5c42\u526a\u679d\u6bd4\u4f8b\u4e0d\u4e00\u81f4\u7684\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u538b\u7f29\u6df1\u5ea6\u53c8\u80fd\u66f4\u597d\u4fdd\u7559\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFlattenGPT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e24\u4e2a\u76f8\u90bb\u7684Transformer\u5757\u6241\u5e73\u5316\u4e3a\u4e00\u4e2a\u5757\u6765\u538b\u7f29\u7f51\u7edc\u6df1\u5ea6\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u548c\u79fb\u9664\u53c2\u6570\u5197\u4f59\uff0c\u4fdd\u7559\u6240\u6709\u5757\u5b66\u5230\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cbTransformer\u67b6\u6784\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728LLaMA-2/3\u548cQwen-1.5\u7b49\u6a21\u578b\u4e0a\uff0cFlattenGPT\u572820%\u538b\u7f29\u6bd4\u4e0b\u80fd\u4fdd\u755990-96%\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002\u5728\u96f6\u6837\u672c\u51c6\u786e\u7387\u548cWikiText-2\u56f0\u60d1\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u526a\u679d\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u52a0\u901fLLM\u63a8\u7406\u65b9\u9762\u4e5f\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "FlattenGPT\u901a\u8fc7\u6241\u5e73\u5316\u76f8\u90bb\u5757\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6df1\u5ea6\u538b\u7f29\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u63d0\u5347Transformer\u6548\u7387\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08861", "pdf": "https://arxiv.org/pdf/2602.08861", "abs": "https://arxiv.org/abs/2602.08861", "authors": ["Xiangtian Zheng", "Zishuo Wang", "Yuxin Peng"], "title": "TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.", "AI": {"tldr": "TiFRe\u662f\u4e00\u4e2a\u6587\u672c\u5f15\u5bfc\u7684\u89c6\u9891\u5e27\u51cf\u5c11\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u5e27\u91c7\u6837\u548c\u5e27\u5339\u914d\u5408\u5e76\u673a\u5236\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8bed\u4e49\u4fe1\u606f\uff0c\u63d0\u5347\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u3002", "motivation": "\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5927\u91cf\u89c6\u9891\u5e27\u65f6\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u7279\u522b\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002\u4f20\u7edf\u7684\u56fa\u5b9a\u5e27\u7387\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\u4f1a\u4e22\u5931\u975e\u5173\u952e\u5e27\u4e2d\u7684\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faTiFRe\u6846\u67b6\uff1a1) \u6587\u672c\u5f15\u5bfc\u5e27\u91c7\u6837(TFS)\uff1a\u5229\u7528LLM\u751f\u6210CLIP\u98ce\u683c\u63d0\u793a\uff0c\u901a\u8fc7CLIP\u7f16\u7801\u5668\u8ba1\u7b97\u63d0\u793a\u4e0e\u6bcf\u5e27\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u9009\u62e9\u6700\u76f8\u5173\u7684\u5173\u952e\u5e27\uff1b2) \u5e27\u5339\u914d\u5408\u5e76(FMM)\uff1a\u5c06\u975e\u5173\u952e\u5e27\u4fe1\u606f\u6574\u5408\u5230\u9009\u5b9a\u7684\u5173\u952e\u5e27\u4e2d\uff0c\u6700\u5c0f\u5316\u4fe1\u606f\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTiFRe\u80fd\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5728\u89c6\u9891\u8bed\u8a00\u4efb\u52a1\u4e0a\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "TiFRe\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u7684\u667a\u80fd\u5e27\u9009\u62e9\u548c\u8bed\u4e49\u4fdd\u7559\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u89c6\u9891MLLMs\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u8f93\u5165\u5e27\u6570\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2602.08909", "pdf": "https://arxiv.org/pdf/2602.08909", "abs": "https://arxiv.org/abs/2602.08909", "authors": ["Zhendong Wang", "Cihan Ruan", "Jingchuan Xiao", "Chuqing Shi", "Wei Jiang", "Wei Wang", "Wenjie Liu", "Nam Ling"], "title": "Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4f18\u5316\u4e2d\u51fa\u73b0\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u79f0\u4e3a\u6e32\u67d3\u6700\u4f18\u53c2\u8003\uff08RORs\uff09\uff0c\u63ed\u793a\u4e86\u5176\u7edf\u8ba1\u7279\u6027\uff1a\u6df7\u5408\u7ed3\u6784\u5c3a\u5ea6\u548c\u53cc\u5cf0\u8f90\u5c04\u5206\u5e03\u3002\u7814\u7a76\u53d1\u73b0\u5bc6\u5ea6\u5206\u5c42\u73b0\u8c61\u2014\u2014\u5bc6\u96c6\u533a\u57df\u53c2\u6570\u53ef\u9884\u6d4b\uff0c\u7a00\u758f\u533a\u57df\u9700\u8981\u591a\u89c6\u89d2\u7ea6\u675f\u3002", "motivation": "\u7814\u7a76\u6807\u51c6\u591a\u89c6\u89d2\u4f18\u5316\u4e0b3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u89e3\u51b3\u65b9\u6848\u4e2d\u81ea\u7136\u51fa\u73b0\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u7406\u89e3\u8fd9\u4e9b\u6a21\u5f0f\u7684\u5f62\u6210\u673a\u5236\uff0c\u4ee5\u53ca\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u7406\u89e3\u6539\u8fdb\u8bad\u7ec3\u9c81\u68d2\u6027\u548c\u7cfb\u7edf\u67b6\u6784\u3002", "method": "1\uff09\u5206\u67903DGS\u4f18\u5316\u4ea7\u751f\u7684\u6e32\u67d3\u6700\u4f18\u53c2\u8003\uff08RORs\uff09\u7684\u7edf\u8ba1\u7279\u6027\uff1b2\uff09\u4f7f\u7528\u53ef\u5b66\u4e60\u6027\u63a2\u9488\u8bad\u7ec3\u9884\u6d4b\u5668\u4ece\u70b9\u4e91\u91cd\u5efaRORs\uff1b3\uff09\u901a\u8fc7\u65b9\u5dee\u5206\u89e3\u5f62\u5f0f\u5316\u5206\u6790\u5bc6\u5ea6\u5206\u5c42\u73b0\u8c61\uff1b4\uff09\u63d0\u51fa\u5bc6\u5ea6\u611f\u77e5\u7b56\u7565\u6539\u8fdb\u8bad\u7ec3\u3002", "result": "\u53d1\u73b0RORs\u5177\u6709\u7a33\u5b9a\u7684\u7edf\u8ba1\u6a21\u5f0f\uff1a\u6df7\u5408\u7ed3\u6784\u5c3a\u5ea6\u548c\u53cc\u5cf0\u8f90\u5c04\u5206\u5e03\u3002\u63ed\u793a\u4e86\u5bc6\u5ea6\u5206\u5c42\u73b0\u8c61\uff1a\u5bc6\u96c6\u533a\u57df\u53c2\u6570\u4e0e\u51e0\u4f55\u76f8\u5173\u4e14\u53ef\u9884\u6d4b\uff0c\u7a00\u758f\u533a\u57df\u53c2\u6570\u53d7\u53ef\u89c1\u6027\u5f02\u8d28\u6027\u4e3b\u5bfc\uff0c\u9700\u8981\u591a\u89c6\u89d2\u7ea6\u675f\u3002\u63d0\u51fa\u4e86\u6539\u8fdb\u8bad\u7ec3\u9c81\u68d2\u6027\u7684\u7b56\u7565\u3002", "conclusion": "RORs\u5177\u6709\u53cc\u91cd\u7279\u6027\uff1a\u5728\u5bc6\u96c6\u533a\u57df\u8868\u73b0\u4e3a\u51e0\u4f55\u57fa\u5143\uff08\u70b9\u4e91\u8db3\u591f\uff09\uff0c\u5728\u7a00\u758f\u533a\u57df\u8868\u73b0\u4e3a\u89c6\u56fe\u5408\u6210\u57fa\u5143\uff08\u9700\u8981\u591a\u89c6\u89d2\u7ea6\u675f\uff09\u3002\u8fd9\u4e3a\u81ea\u9002\u5e94\u5e73\u8861\u524d\u9988\u9884\u6d4b\u548c\u57fa\u4e8e\u6e32\u67d3\u4f18\u5316\u7684\u7cfb\u7edf\u67b6\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.08958", "pdf": "https://arxiv.org/pdf/2602.08958", "abs": "https://arxiv.org/abs/2602.08958", "authors": ["Weihan Luo", "Lily Goli", "Sherwin Bahmani", "Felix Taubner", "Andrea Tagliasacchi", "David B. Lindell"], "title": "Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields", "categories": ["cs.CV"], "comment": "Project page: https://weihanluo.ca/growflow/", "summary": "Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u690d\u7269\u751f\u957f\u5efa\u6a21\u76843D\u9ad8\u65af\u6d41\u573a\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u53d8\u5316\u7684\u9ad8\u65af\u53c2\u6570\u5bfc\u6570\u6765\u6a21\u62df\u975e\u7ebf\u6027\u8fde\u7eed\u751f\u957f\u52a8\u6001\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u690d\u7269\u751f\u957f\u5efa\u6a21\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u690d\u7269\u4f1a\u968f\u65f6\u95f4\u751f\u6210\u65b0\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u73b0\u6709\u52a8\u6001\u573a\u666f\u5efa\u6a21\u6280\u672f\uff08\u5982\u53d8\u5f62\u573a\u548c4D\u9ad8\u65af\u6cfc\u6e85\uff09\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u51e0\u4f55\u751f\u6210\u95ee\u9898\u3002", "method": "\u5f15\u51653D\u9ad8\u65af\u6d41\u573a\u8868\u793a\uff0c\u5c06\u690d\u7269\u751f\u957f\u5efa\u6a21\u4e3a\u9ad8\u65af\u53c2\u6570\uff08\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u3001\u65b9\u5411\u3001\u989c\u8272\u3001\u4e0d\u900f\u660e\u5ea6\uff09\u7684\u65f6\u95f4\u53d8\u5316\u5bfc\u6570\uff1b\u901a\u8fc7\u91cd\u5efa\u6210\u719f\u690d\u7269\u5e76\u5b66\u4e60\u53cd\u5411\u751f\u957f\u8fc7\u7a0b\u6765\u521d\u59cb\u5316\u9ad8\u65af\u57fa\u5143\u3002", "result": "\u5728\u690d\u7269\u751f\u957f\u7684\u591a\u89c6\u89d2\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u957f\u4e2d\u76843D\u7ed3\u6784\u5916\u89c2\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u80fd\u591f\u6709\u6548\u6a21\u62df\u690d\u7269\u7684\u975e\u7ebf\u6027\u8fde\u7eed\u751f\u957f\u52a8\u6001\u3002"}}
{"id": "2602.08961", "pdf": "https://arxiv.org/pdf/2602.08961", "abs": "https://arxiv.org/abs/2602.08961", "authors": ["Ruijie Zhu", "Jiahao Lu", "Wenbo Hu", "Xiaoguang Han", "Jianfei Cai", "Ying Shan", "Chuanxia Zheng"], "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE", "categories": ["cs.CV", "cs.AI", "cs.CG", "cs.LG"], "comment": "Project page: https://ruijiezhu94.github.io/MotionCrafter_Page", "summary": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page", "AI": {"tldr": "MotionCrafter\uff1a\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u7684\u6846\u67b6\uff0c\u4ece\u5355\u76ee\u89c6\u9891\u8054\u5408\u91cd\u5efa4D\u51e0\u4f55\u5e76\u4f30\u8ba1\u7a20\u5bc6\u8fd0\u52a8\uff0c\u65e0\u9700\u540e\u4f18\u5316\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5f3a\u52363D\u503c\u548c\u6f5c\u5728\u7a7a\u95f4\u4e0eRGB VAE\u6f5c\u5728\u7a7a\u95f4\u4e25\u683c\u5bf9\u9f50\uff0c\u5c3d\u7ba1\u5b83\u4eec\u7684\u5206\u5e03\u6839\u672c\u4e0d\u540c\uff0c\u8fd9\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u8054\u5408\u8868\u793a4D\u51e0\u4f55\u548c\u8fd0\u52a8\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u8054\u5408\u8868\u793a\u65b9\u6cd5\uff1a\u5728\u5171\u4eab\u5750\u6807\u7cfb\u4e2d\u8868\u793a\u7a20\u5bc63D\u70b9\u56fe\u548c3D\u573a\u666f\u6d41\uff1b\u5f00\u53d1\u65b0\u76844D VAE\u6765\u6709\u6548\u5b66\u4e60\u8fd9\u79cd\u8868\u793a\uff1b\u5f15\u5165\u6570\u636e\u5f52\u4e00\u5316\u548cVAE\u8bad\u7ec3\u7b56\u7565\uff0c\u66f4\u597d\u5730\u4f20\u9012\u6269\u6563\u5148\u9a8c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMotionCrafter\u5728\u51e0\u4f55\u91cd\u5efa\u548c\u7a20\u5bc6\u573a\u666f\u6d41\u4f30\u8ba1\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51e0\u4f55\u91cd\u5efa\u63d0\u534738.64%\uff0c\u8fd0\u52a8\u91cd\u5efa\u63d0\u534725.0%\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u540e\u4f18\u5316\u3002", "conclusion": "MotionCrafter\u8bc1\u660e\u4e86\u5f3a\u52363D\u503c\u4e0eRGB VAE\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u901a\u8fc7\u65b0\u7684\u8054\u5408\u8868\u793a\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u53474D\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u5355\u76ee\u89c6\u9891\u76844D\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08962", "pdf": "https://arxiv.org/pdf/2602.08962", "abs": "https://arxiv.org/abs/2602.08962", "authors": ["Guangxun Zhu", "Xuan Liu", "Nicolas Pugeault", "Chongfeng Wei", "Edmond S. L. Ho"], "title": "Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted for IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a3D\u8f66\u8f86\u6761\u4ef6\u884c\u4eba\u59ff\u6001\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u7ed3\u5408\u5468\u56f4\u8f66\u8f86\u4fe1\u606f\u6765\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u884c\u4eba\u8fd0\u52a8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u884c\u4eba\u8fd0\u52a8\u5bf9\u4e8e\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u8f66\u8f86\u5bf9\u884c\u4eba\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u9700\u8981\u663e\u5f0f\u5efa\u6a21\u884c\u4eba-\u8f66\u8f86\u4ea4\u4e92\u3002", "method": "1. \u589e\u5f3aWaymo-3DSkelMo\u6570\u636e\u96c6\uff0c\u6dfb\u52a0\u5bf9\u9f50\u76843D\u8f66\u8f86\u8fb9\u754c\u6846\uff1b2. \u63d0\u51fa\u91c7\u6837\u65b9\u6848\u6309\u884c\u4eba\u548c\u8f66\u8f86\u6570\u91cf\u5206\u7c7b\u573a\u666f\uff1b3. \u57fa\u4e8eTBIFormer\u67b6\u6784\uff0c\u6dfb\u52a0\u4e13\u7528\u8f66\u8f86\u7f16\u7801\u5668\u548c\u884c\u4eba-\u8f66\u8f86\u4ea4\u4e92\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u878d\u5408\u884c\u4eba\u548c\u8f66\u8f86\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u9884\u6d4b\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u884c\u4eba-\u8f66\u8f86\u4ea4\u4e92\u5efa\u6a21\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5f3a\u8c03\u4e86\u8f66\u8f86\u611f\u77e5\u76843D\u59ff\u6001\u9884\u6d4b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u63d0\u51fa\u76843D\u8f66\u8f86\u6761\u4ef6\u884c\u4eba\u59ff\u6001\u9884\u6d4b\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u7ed3\u5408\u8f66\u8f86\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u8fd0\u52a8\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u884c\u4eba\u884c\u4e3a\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2602.08971", "pdf": "https://arxiv.org/pdf/2602.08971", "abs": "https://arxiv.org/abs/2602.08971", "authors": ["Yu Shang", "Zhuohang Li", "Yiding Ma", "Weikang Su", "Xin Jin", "Ziyou Wang", "Xin Zhang", "Yinzhou Tang", "Chen Gao", "Wei Wu", "Xihui Liu", "Dhruv Shah", "Zhaoxiang Zhang", "Zhibo Chen", "Jun Zhu", "Yonghong Tian", "Tat-Seng Chua", "Wenwu Zhu", "Yong Li"], "title": "WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.", "AI": {"tldr": "WorldArena\uff1a\u9996\u4e2a\u7edf\u4e00\u8bc4\u4f30\u5177\u8eab\u4e16\u754c\u6a21\u578b\u7684\u57fa\u51c6\uff0c\u540c\u65f6\u8861\u91cf\u611f\u77e5\u8d28\u91cf\u548c\u529f\u80fd\u6548\u7528\uff0c\u63ed\u793a\u611f\u77e5\u4e0e\u529f\u80fd\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "motivation": "\u5f53\u524d\u5177\u8eab\u4e16\u754c\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u4fdd\u771f\u5ea6\uff08\u5982\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff09\uff0c\u5ffd\u89c6\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0b\u6e38\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u529f\u80fd\u6548\u7528\uff0c\u8bc4\u4f30\u4f53\u7cfb\u788e\u7247\u5316", "method": "\u63d0\u51faWorldArena\u57fa\u51c6\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\uff1a\u89c6\u9891\u611f\u77e5\u8d28\u91cf\uff0816\u4e2a\u6307\u6807\u8986\u76d66\u4e2a\u5b50\u7ef4\u5ea6\uff09\u3001\u5177\u8eab\u4efb\u52a1\u529f\u80fd\uff08\u4f5c\u4e3a\u6570\u636e\u5f15\u64ce\u3001\u7b56\u7565\u8bc4\u4f30\u5668\u548c\u52a8\u4f5c\u89c4\u5212\u5668\uff09\uff0c\u5e76\u5f15\u5165EWMScore\u7efc\u5408\u6307\u6807", "result": "\u5bf914\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u663e\u8457\u7684\u611f\u77e5-\u529f\u80fd\u5dee\u8ddd\uff1a\u9ad8\u89c6\u89c9\u8d28\u91cf\u4e0d\u4e00\u5b9a\u8f6c\u5316\u4e3a\u5f3a\u5927\u7684\u5177\u8eab\u4efb\u52a1\u80fd\u529b", "conclusion": "WorldArena\u4e3a\u8ffd\u8e2a\u5177\u8eabAI\u4e2d\u771f\u6b63\u529f\u80fd\u6027\u4e16\u754c\u6a21\u578b\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u516c\u5f00\u6392\u884c\u699c\u5df2\u53d1\u5e03\u5728https://worldarena.ai"}}
{"id": "2602.08996", "pdf": "https://arxiv.org/pdf/2602.08996", "abs": "https://arxiv.org/abs/2602.08996", "authors": ["Arushi Rai", "Adriana Kovashka"], "title": "Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study", "categories": ["cs.CV"], "comment": "to appear WACV 2026", "summary": "While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u8f85\u52a9\u7f51\u7edc\u6570\u636e\u63d0\u5347\u4f53\u80b2\u53cd\u9988\u751f\u6210\u6027\u80fd\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u89e3\u51b3\u4f20\u7edf\u6307\u6807\u4e0d\u9002\u7528\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u89c6\u9891-LLMs\u5728\u4f53\u80b2\u53cd\u9988\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff1b\u4f20\u7edf\u6587\u672c\u751f\u6210\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u4f53\u80b2\u53cd\u9988\u8d28\u91cf", "method": "\u4ee5\u6500\u5ca9\u4e3a\u4f8b\uff0c\u5229\u7528\u76ee\u6807\u9886\u57df\u7684\u514d\u8d39\u7f51\u7edc\u6570\u636e\uff08\u6bd4\u8d5b\u89c6\u9891\u548c\u6559\u7ec3\u624b\u518c\uff09\u4ee5\u53ca\u6e90\u9886\u57df\u7684\u73b0\u6709\u4f53\u80b2\u53cd\u9988\u6570\u636e\uff1b\u63d0\u51fa\u7279\u5f02\u6027(specificity)\u548c\u53ef\u64cd\u4f5c\u6027(actionability)\u4e24\u4e2a\u65b0\u8bc4\u4f30\u6307\u6807", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u5b9e\u73b0\u66f4\u6709\u610f\u4e49\u548c\u5b9e\u7528\u7684\u4f53\u80b2\u53cd\u9988\u751f\u6210", "conclusion": "\u901a\u8fc7\u5229\u7528\u8f85\u52a9\u7f51\u7edc\u6570\u636e\u548c\u8bbe\u8ba1\u9488\u5bf9\u6027\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u4f53\u80b2\u53cd\u9988\u751f\u6210\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027"}}
{"id": "2602.09014", "pdf": "https://arxiv.org/pdf/2602.09014", "abs": "https://arxiv.org/abs/2602.09014", "authors": ["Zihan Yang", "Shuyuan Tu", "Licheng Zhang", "Qi Dai", "Yu-Gang Jiang", "Zuxuan Wu"], "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.", "AI": {"tldr": "ArcFlow\uff1a\u4e00\u79cd\u4f7f\u7528\u975e\u7ebf\u6027\u6d41\u8f68\u8ff9\u903c\u8fd1\u6559\u5e08\u6a21\u578b\u8f68\u8ff9\u7684\u5c11\u6b65\u84b8\u998f\u6846\u67b6\uff0c\u4ec5\u9700\u5fae\u8c03\u4e0d\u52305%\u53c2\u6570\u5373\u53ef\u5b9e\u73b040\u500d\u52a0\u901f\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u51fa\u8272\uff0c\u4f46\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u9ad8\u3002\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u4f7f\u7528\u7ebf\u6027\u6377\u5f84\u903c\u8fd1\u6559\u5e08\u8f68\u8ff9\uff0c\u96be\u4ee5\u5339\u914d\u968f\u65f6\u95f4\u6b65\u53d8\u5316\u7684\u5207\u7ebf\u65b9\u5411\uff0c\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faArcFlow\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u7684\u5e95\u5c42\u901f\u5ea6\u573a\u53c2\u6570\u5316\u4e3a\u8fde\u7eed\u52a8\u91cf\u8fc7\u7a0b\u7684\u6df7\u5408\uff0c\u6355\u6349\u901f\u5ea6\u6f14\u5316\u5e76\u5916\u63a8\u8fde\u8d2f\u901f\u5ea6\u5f62\u6210\u975e\u7ebf\u6027\u8f68\u8ff9\u3002\u8be5\u53c2\u6570\u5316\u5141\u8bb8\u5bf9\u975e\u7ebf\u6027\u8f68\u8ff9\u8fdb\u884c\u89e3\u6790\u79ef\u5206\uff0c\u907f\u514d\u6570\u503c\u79bb\u6563\u8bef\u5dee\u3002\u901a\u8fc7\u8f7b\u91cf\u9002\u914d\u5668\u5728\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u4e0a\u8fdb\u884c\u8f68\u8ff9\u84b8\u998f\u8bad\u7ec3\u3002", "result": "\u5728Qwen-Image-20B\u548cFLUX.1-dev\u7b49\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\uff0c\u4ec5\u5fae\u8c03\u4e0d\u52305%\u53c2\u6570\uff0c\u4f7f\u75282\u4e2aNFE\u5373\u53ef\u5b9e\u73b040\u500d\u52a0\u901f\uff0c\u4e14\u65e0\u660e\u663e\u8d28\u91cf\u4e0b\u964d\u3002\u57fa\u51c6\u6d4b\u8bd5\u663e\u793aArcFlow\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "ArcFlow\u901a\u8fc7\u975e\u7ebf\u6027\u6d41\u8f68\u8ff9\u903c\u8fd1\u6559\u5e08\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7ebf\u6027\u84b8\u998f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5c11\u6b65\u84b8\u998f\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2602.09016", "pdf": "https://arxiv.org/pdf/2602.09016", "abs": "https://arxiv.org/abs/2602.09016", "authors": ["Hao Phung", "Hadar Averbuch-Elor"], "title": "Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction", "categories": ["cs.CV"], "comment": "Code: https://anonymous.4open.science/r/Raster2Seq-BE73/", "summary": "Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.", "AI": {"tldr": "Raster2Seq\uff1a\u5c06\u697c\u5c42\u5e73\u9762\u56fe\u91cd\u5efa\u89c6\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u89e3\u7801\u5668\u9884\u6d4b\u591a\u8fb9\u5f62\u89d2\u70b9\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u6280\u672f\u96be\u4ee5\u4ece\u590d\u6742\u697c\u5c42\u5e73\u9762\u56fe\u56fe\u50cf\u4e2d\u51c6\u786e\u91cd\u5efa\u7ed3\u6784\u548c\u8bed\u4e49\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5305\u542b\u5927\u91cf\u623f\u95f4\u548c\u4e0d\u540c\u591a\u8fb9\u5f62\u89d2\u70b9\u6570\u91cf\u7684\u5ba4\u5185\u7a7a\u95f4", "method": "\u5c06\u697c\u5c42\u5e73\u9762\u56fe\u91cd\u5efa\u6846\u67b6\u5316\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u4efb\u52a1\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u89e3\u7801\u5668\u5b66\u4e60\u5728\u56fe\u50cf\u7279\u5f81\u548c\u5148\u524d\u751f\u6210\u89d2\u70b9\u6761\u4ef6\u4e0b\u9884\u6d4b\u4e0b\u4e00\u4e2a\u89d2\u70b9\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u951a\u70b9\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u4fe1\u606f\u4e30\u5bcc\u7684\u56fe\u50cf\u533a\u57df", "result": "\u5728Structure3D\u3001CubiCasa5K\u548cRaster2Graph\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5305\u542b\u591a\u6837\u623f\u95f4\u7ed3\u6784\u548c\u590d\u6742\u51e0\u4f55\u53d8\u5316\u7684WAFFLE\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "Raster2Seq\u901a\u8fc7\u5e8f\u5217\u5316\u8868\u793a\u548c\u81ea\u56de\u5f52\u673a\u5236\uff0c\u80fd\u591f\u7075\u6d3b\u5904\u7406\u590d\u6742\u697c\u5c42\u5e73\u9762\u56fe\uff0c\u6709\u6548\u91cd\u5efa\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u81ea\u52a8\u5316\u7406\u89e3\u548cCAD\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u524d\u7f6e\u5904\u7406\u65b9\u6848"}}
{"id": "2602.09022", "pdf": "https://arxiv.org/pdf/2602.09022", "abs": "https://arxiv.org/abs/2602.09022", "authors": ["Zehan Wang", "Tengfei Wang", "Haiyu Zhang", "Xuhui Zuo", "Junta Wu", "Haoyuan Wang", "Wenqiang Sun", "Zhenwei Wang", "Chenjie Cao", "Hengshuang Zhao", "Chunchao Guo", "Zhou Zhao"], "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models", "categories": ["cs.CV"], "comment": "Project page: \\url{https://3d-models.hunyuan.tencent.com/world/}", "summary": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.", "AI": {"tldr": "WorldCompass\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u65f6\u7a0b\u4ea4\u4e92\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u526a\u8f91\u7ea7\u7b56\u7565\u3001\u4e92\u8865\u5956\u52b1\u51fd\u6570\u548c\u9ad8\u6548RL\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e16\u754c\u6a21\u578b\u7684\u4ea4\u4e92\u51c6\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u957f\u65f6\u7a0b\u4ea4\u4e92\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5728\u63a2\u7d22\u4e16\u754c\u65f6\u5b58\u5728\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u540e\u8bad\u7ec3\u6846\u67b6\u6765\u57fa\u4e8e\u4ea4\u4e92\u4fe1\u53f7\u5f15\u5bfc\u6a21\u578b\u66f4\u51c6\u786e\u3001\u4e00\u81f4\u5730\u63a2\u7d22\u4e16\u754c\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u526a\u8f91\u7ea7\u7b56\u7565\uff1a\u5728\u5355\u4e2a\u76ee\u6807\u526a\u8f91\u751f\u6210\u548c\u8bc4\u4f30\u591a\u4e2a\u6837\u672c\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\uff1b2) \u4e92\u8865\u5956\u52b1\u51fd\u6570\uff1a\u8bbe\u8ba1\u4ea4\u4e92\u8ddf\u968f\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u7684\u5956\u52b1\u51fd\u6570\uff0c\u63d0\u4f9b\u76f4\u63a5\u76d1\u7763\u5e76\u6291\u5236\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff1b3) \u9ad8\u6548RL\u7b97\u6cd5\uff1a\u91c7\u7528\u8d1f\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u914d\u5408\u591a\u79cd\u6548\u7387\u4f18\u5316\uff0c\u6709\u6548\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5728SoTA\u5f00\u6e90\u4e16\u754c\u6a21\u578bWorldPlay\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cWorldCompass\u5728\u5404\u79cd\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u51c6\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "WorldCompass\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u4ea4\u4e92\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7684\u63a2\u7d22\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u4e3a\u4e16\u754c\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2602.09024", "pdf": "https://arxiv.org/pdf/2602.09024", "abs": "https://arxiv.org/abs/2602.09024", "authors": ["Qihang Yu", "Qihao Liu", "Ju He", "Xinyang Zhang", "Yang Liu", "Liang-Chieh Chen", "Xi Chen"], "title": "Autoregressive Image Generation with Masked Bit Modeling", "categories": ["cs.CV"], "comment": "SOTA discrete visual generation defeats diffusion models with 0.99 FID score, project page is available at https://bar-gen.github.io/", "summary": "This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u89c6\u89c9\u751f\u6210\u4e2d\u8fde\u7eed\u7ba1\u9053\u7684\u7edf\u6cbb\u5730\u4f4d\uff0c\u8bc1\u660e\u79bb\u6563\u65b9\u6cd5\u901a\u8fc7\u6269\u5927\u7801\u672c\u89c4\u6a21\u53ef\u4ee5\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u8fde\u7eed\u65b9\u6cd5\uff0c\u5e76\u63d0\u51faBAR\u6846\u67b6\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u6311\u6218\u89c6\u89c9\u751f\u6210\u9886\u57df\u8fde\u7eed\u65b9\u6cd5\u7684\u7edf\u6cbb\u5730\u4f4d\uff0c\u7cfb\u7edf\u7814\u7a76\u79bb\u6563\u4e0e\u8fde\u7eed\u65b9\u6cd5\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u79bb\u6563\u65b9\u6cd5\u6027\u80fd\u4e0d\u8db3\u7684\u6839\u672c\u539f\u56e0\u5e76\u975e\u5185\u5728\u7f3a\u9677\uff0c\u800c\u662f\u538b\u7f29\u6bd4\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u63a9\u7801\u6bd4\u7279\u81ea\u56de\u5f52\u5efa\u6a21(BAR)\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u53d8\u6362\u5668\u914d\u5408\u63a9\u7801\u6bd4\u7279\u5efa\u6a21\u5934\uff0c\u652f\u6301\u4efb\u610f\u7801\u672c\u5927\u5c0f\uff0c\u901a\u8fc7\u9010\u6b65\u751f\u6210\u6bd4\u7279\u6765\u9884\u6d4b\u79bb\u6563\u6807\u8bb0\u3002", "result": "BAR\u5728ImageNet-256\u4e0a\u8fbe\u52300.99\u7684gFID\u65b0\u7eaa\u5f55\uff0c\u8d85\u8d8a\u4e86\u8fde\u7eed\u548c\u79bb\u6563\u65b9\u6cd5\u7684\u9886\u5148\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u91c7\u6837\u6210\u672c\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u5148\u524d\u8fde\u7eed\u65b9\u6cd5\u66f4\u5feb\u3002", "conclusion": "\u79bb\u6563\u6807\u8bb0\u5668\u901a\u8fc7\u6269\u5927\u7801\u672c\u89c4\u6a21\u53ef\u4ee5\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u8fde\u7eed\u65b9\u6cd5\uff0cBAR\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\u5728\u6269\u5927\u7801\u672c\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u6216\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\u95ee\u9898\uff0c\u4e3a\u89c6\u89c9\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.06968", "pdf": "https://arxiv.org/pdf/2602.06968", "abs": "https://arxiv.org/abs/2602.06968", "authors": ["Xubo Luo", "Zhaojin Li", "Xue Wan", "Wei Zhang", "Leizheng Shu"], "title": "Learning to Anchor Visual Odometry: KAN-Based Pose Regression for Planetary Landing", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, accepted by RA-L", "summary": "Accurate and real-time 6-DoF localization is mission-critical for autonomous lunar landing, yet existing approaches remain limited: visual odometry (VO) drifts unboundedly, while map-based absolute localization fails in texture-sparse or low-light terrain. We introduce KANLoc, a monocular localization framework that tightly couples VO with a lightweight but robust absolute pose regressor. At its core is a Kolmogorov-Arnold Network (KAN) that learns the complex mapping from image features to map coordinates, producing sparse but highly reliable global pose anchors. These anchors are fused into a bundle adjustment framework, effectively canceling drift while retaining local motion precision. KANLoc delivers three key advances: (i) a KAN-based pose regressor that achieves high accuracy with remarkable parameter efficiency, (ii) a hybrid VO-absolute localization scheme that yields globally consistent real-time trajectories (>=15 FPS), and (iii) a tailored data augmentation strategy that improves robustness to sensor occlusion. On both realistic synthetic and real lunar landing datasets, KANLoc reduces average translation and rotation error by 32% and 45%, respectively, with per-trajectory gains of up to 45%/48%, outperforming strong baselines.", "AI": {"tldr": "KANLoc\uff1a\u57fa\u4e8eKAN\u7684\u6708\u7403\u7740\u9646\u5355\u76ee\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e0e\u8f7b\u91cf\u7ea7\u7edd\u5bf9\u4f4d\u59ff\u56de\u5f52\u5668\uff0c\u5b9e\u73b0\u65e0\u6f02\u79fb\u7684\u5b9e\u65f66-DoF\u5b9a\u4f4d", "motivation": "\u73b0\u6709\u6708\u7403\u7740\u9646\u5b9a\u4f4d\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4f1a\u65e0\u754c\u6f02\u79fb\uff0c\u800c\u57fa\u4e8e\u5730\u56fe\u7684\u7edd\u5bf9\u5b9a\u4f4d\u5728\u7eb9\u7406\u7a00\u758f\u6216\u4f4e\u5149\u7167\u5730\u5f62\u4e2d\u5931\u6548\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6d88\u9664\u6f02\u79fb\u53c8\u80fd\u9002\u5e94\u6708\u7403\u7279\u6b8a\u73af\u5883\u7684\u5b9e\u65f6\u5b9a\u4f4d\u65b9\u6848\u3002", "method": "\u63d0\u51faKANLoc\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u4f7f\u7528Kolmogorov-Arnold Network\u5b66\u4e60\u4ece\u56fe\u50cf\u7279\u5f81\u5230\u5730\u56fe\u5750\u6807\u7684\u590d\u6742\u6620\u5c04\uff0c\u751f\u6210\u7a00\u758f\u4f46\u9ad8\u5ea6\u53ef\u9760\u7684\u5168\u5c40\u4f4d\u59ff\u951a\u70b9\u3002\u5c06\u8fd9\u4e9b\u951a\u70b9\u878d\u5408\u5230\u5149\u675f\u6cd5\u5e73\u5dee\u6846\u67b6\u4e2d\uff0c\u6709\u6548\u6d88\u9664\u6f02\u79fb\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u8fd0\u52a8\u7cbe\u5ea6\u3002\u8fd8\u5305\u62ec\u9488\u5bf9\u4f20\u611f\u5668\u906e\u6321\u7684\u5b9a\u5236\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6708\u7403\u7740\u9646\u6570\u636e\u96c6\u4e0a\uff0cKANLoc\u5c06\u5e73\u5747\u5e73\u79fb\u548c\u65cb\u8f6c\u8bef\u5dee\u5206\u522b\u964d\u4f4e32%\u548c45%\uff0c\u5355\u8f68\u8ff9\u589e\u76ca\u6700\u9ad8\u8fbe45%/48%\uff0c\u4ee5\u226515 FPS\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u7684\u5b9e\u65f6\u8f68\u8ff9\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "KANLoc\u901a\u8fc7KAN\u7f51\u7edc\u7684\u9ad8\u6548\u4f4d\u59ff\u56de\u5f52\u548cVO-\u7edd\u5bf9\u5b9a\u4f4d\u7684\u7d27\u5bc6\u8026\u5408\uff0c\u4e3a\u6708\u7403\u7740\u9646\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u5b9e\u65f6\u4e14\u65e0\u6f02\u79fb\u7684\u5355\u76ee\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u53c2\u6570\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.06974", "pdf": "https://arxiv.org/pdf/2602.06974", "abs": "https://arxiv.org/abs/2602.06974", "authors": ["Faith Johnson", "Bryan Bo Cao", "Shubham Jain", "Ashwin Ashok", "Kristin Dana"], "title": "FeudalNav: A Simple Framework for Visual Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "8 Pages, 6 figures and 4 tables. arXiv admin note: substantial text overlap with arXiv:2411.09893, arXiv:2402.12498", "summary": "Visual navigation for robotics is inspired by the human ability to navigate environments using visual cues and memory, eliminating the need for detailed maps. In unseen, unmapped, or GPS-denied settings, traditional metric map-based methods fall short, prompting a shift toward learning-based approaches with minimal exploration. In this work, we develop a hierarchical framework that decomposes the navigation decision-making process into multiple levels. Our method learns to select subgoals through a simple, transferable waypoint selection network. A key component of the approach is a latent-space memory module organized solely by visual similarity, as a proxy for distance. This alternative to graph-based topological representations proves sufficient for navigation tasks, providing a compact, light-weight, simple-to-train navigator that can find its way to the goal in novel locations. We show competitive results with a suite of SOTA methods in Habitat AI environments without using any odometry in training or inference. An additional contribution leverages the interpretablility of the framework for interactive navigation. We consider the question: how much direction intervention/interaction is needed to achieve success in all trials? We demonstrate that even minimal human involvement can significantly enhance overall navigation performance.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u89c6\u89c9\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u76f8\u4f3c\u6027\u7ec4\u7ec7\u8bb0\u5fc6\uff0c\u65e0\u9700\u91cc\u7a0b\u8ba1\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u5bfc\u822a\u589e\u5f3a\u6027\u80fd", "motivation": "\u5728\u672a\u77e5\u3001\u65e0\u5730\u56fe\u6216GPS\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u57fa\u4e8e\u5ea6\u91cf\u5730\u56fe\u7684\u65b9\u6cd5\u5931\u6548\uff0c\u9700\u8981\u8f6c\u5411\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u63a2\u7d22\u5b9e\u73b0\u5bfc\u822a", "method": "\u5206\u5c42\u6846\u67b6\u5c06\u5bfc\u822a\u51b3\u7b56\u5206\u89e3\u4e3a\u591a\u4e2a\u5c42\u7ea7\uff0c\u4f7f\u7528\u53ef\u8f6c\u79fb\u7684\u822a\u70b9\u9009\u62e9\u7f51\u7edc\u9009\u62e9\u5b50\u76ee\u6807\uff0c\u5173\u952e\u7ec4\u4ef6\u662f\u57fa\u4e8e\u89c6\u89c9\u76f8\u4f3c\u6027\u7ec4\u7ec7\u7684\u6f5c\u5728\u7a7a\u95f4\u8bb0\u5fc6\u6a21\u5757\uff0c\u66ff\u4ee3\u57fa\u4e8e\u56fe\u7684\u62d3\u6251\u8868\u793a", "result": "\u5728Habitat AI\u73af\u5883\u4e2d\u4e0eSOTA\u65b9\u6cd5\u7ade\u4e89\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u63a8\u7406\u4e2d\u4f7f\u7528\u91cc\u7a0b\u8ba1\uff0c\u6846\u67b6\u53ef\u89e3\u91ca\u6027\u652f\u6301\u4ea4\u4e92\u5bfc\u822a\uff0c\u5c11\u91cf\u4eba\u5de5\u5e72\u9884\u80fd\u663e\u8457\u63d0\u5347\u5bfc\u822a\u6210\u529f\u7387", "conclusion": "\u57fa\u4e8e\u89c6\u89c9\u76f8\u4f3c\u6027\u7684\u6f5c\u5728\u7a7a\u95f4\u8bb0\u5fc6\u6a21\u5757\u4e3a\u5bfc\u822a\u4efb\u52a1\u63d0\u4f9b\u7d27\u51d1\u3001\u8f7b\u91cf\u3001\u6613\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5728\u964c\u751f\u73af\u5883\u4e2d\u80fd\u5bfc\u822a\u81f3\u76ee\u6807\uff0c\u4ea4\u4e92\u5f0f\u589e\u5f3a\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd"}}
{"id": "2602.06991", "pdf": "https://arxiv.org/pdf/2602.06991", "abs": "https://arxiv.org/abs/2602.06991", "authors": ["Seongbo Ha", "Sibaek Lee", "Kyungsu Kang", "Joonyeol Choi", "Seungjun Tak", "Hyeonwoo Yu"], "title": "LangGS-SLAM: Real-Time Language-Feature Gaussian Splatting SLAM", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "17 pages, 4 figures", "summary": "In this paper, we propose a RGB-D SLAM system that reconstructs a language-aligned dense feature field while sustaining low-latency tracking and mapping. First, we introduce a Top-K Rendering pipeline, a high-throughput and semantic-distortion-free method for efficiently rendering high-dimensional feature maps. To address the resulting semantic-geometric discrepancy and mitigate the memory consumption, we further design a multi-criteria map management strategy that prunes redundant or inconsistent Gaussians while preserving scene integrity. Finally, a hybrid field optimization framework jointly refines the geometric and semantic fields under real-time constraints by decoupling their optimization frequencies according to field characteristics. The proposed system achieves superior geometric fidelity compared to geometric-only baselines and comparable semantic fidelity to offline approaches while operating at 15 FPS. Our results demonstrate that online SLAM with dense, uncompressed language-aligned feature fields is both feasible and effective, bridging the gap between 3D perception and language-based reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2aRGB-D SLAM\u7cfb\u7edf\uff0c\u80fd\u91cd\u5efa\u8bed\u8a00\u5bf9\u9f50\u7684\u5bc6\u96c6\u7279\u5f81\u573a\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u8ddf\u8e2a\u4e0e\u5efa\u56fe\uff0c\u5b9e\u73b015FPS\u5b9e\u65f6\u8fd0\u884c", "motivation": "\u73b0\u6709\u7cfb\u7edf\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u3001\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u9700\u8981\u5f25\u54083D\u611f\u77e5\u4e0e\u8bed\u8a00\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd", "method": "1) Top-K\u6e32\u67d3\u7ba1\u9053\u9ad8\u6548\u6e32\u67d3\u9ad8\u7ef4\u7279\u5f81\u56fe\uff1b2) \u591a\u51c6\u5219\u5730\u56fe\u7ba1\u7406\u7b56\u7565\u4fee\u526a\u5197\u4f59\u6216\u4e0d\u4e00\u81f4\u7684\u9ad8\u65af\u5206\u5e03\uff1b3) \u6df7\u5408\u573a\u4f18\u5316\u6846\u67b6\u6309\u9891\u7387\u89e3\u8026\u51e0\u4f55\u4e0e\u8bed\u4e49\u573a\u4f18\u5316", "result": "\u7cfb\u7edf\u5728\u51e0\u4f55\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u7eaf\u51e0\u4f55\u57fa\u7ebf\uff0c\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0e\u79bb\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u4ee515FPS\u5b9e\u65f6\u8fd0\u884c", "conclusion": "\u5728\u7ebfSLAM\u7ed3\u5408\u5bc6\u96c6\u3001\u672a\u538b\u7f29\u7684\u8bed\u8a00\u5bf9\u9f50\u7279\u5f81\u573a\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\uff0c\u4e3a3D\u611f\u77e5\u4e0e\u8bed\u8a00\u63a8\u7406\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2602.06994", "pdf": "https://arxiv.org/pdf/2602.06994", "abs": "https://arxiv.org/abs/2602.06994", "authors": ["Rongzhao He", "Dalin Zhu", "Ying Wang", "Songhong Yue", "Leilei Zhao", "Yu Fu", "Dan Wu", "Bin Hu", "Weihao Zheng"], "title": "SurfAge-Net: A Hierarchical Surface-Based Network for Interpretable Fine-Grained Brain Age Prediction", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Brain age prediction serves as a powerful framework for assessing brain status and detecting deviations associated with neurodevelopmental and neurodegenerative disorders. However, most existing approaches emphasize whole-brain age prediction and therefore overlook the pronounced regional heterogeneity of brain maturation that is crucial for detecting localized atypical trajectories. To address this limitation, we propose a novel spherical surface-based brain age prediction network (SurfAge-Net) that leverages multiple morphological metrics to capture region-specific developmental patterns with enhanced robustness and clinical interpretability. SurfAge-Net establishes a new modeling paradigm by incorporating the connectomic principles of cortical organization: it explicitly models both intra- and inter-hemispheric dependencies through a spatial-channel mixing and a lateralization-aware attention mechanism, enabling the network to characterize the coordinate maturation pattern uniquely associated with each target region. Validated on three fetal and neonatal datasets, SurfAge-Net outperforms existing approaches (global MAE = 0.54, regional MAE = 0.45 in gestational/postmenstrual weeks) and demonstrates strong generalizability across external cohorts. Importantly, it provides spatially precise and biologically interpretable maps of cortical maturation, effectively identifying heterogeneous delays and regional-specific abnormalities in atypical developmental populations. These results established fine-grained brain age prediction as a promising paradigm for advancing neurodevelopmental research and supporting early clinical assessment.", "AI": {"tldr": "\u63d0\u51faSurfAge-Net\u7f51\u7edc\uff0c\u5229\u7528\u7403\u9762\u8868\u9762\u5f62\u6001\u5b66\u6307\u6807\u8fdb\u884c\u533a\u57df\u7279\u5f02\u6027\u8111\u5e74\u9f84\u9884\u6d4b\uff0c\u901a\u8fc7\u5efa\u6a21\u534a\u7403\u5185\u548c\u534a\u7403\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u80ce\u513f\u548c\u65b0\u751f\u513f\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u5e76\u8bc6\u522b\u5f02\u5e38\u53d1\u80b2\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u8111\u5e74\u9f84\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5168\u8111\u6c34\u5e73\uff0c\u5ffd\u7565\u4e86\u5927\u8111\u53d1\u80b2\u7684\u533a\u57df\u5f02\u8d28\u6027\uff0c\u800c\u533a\u57df\u7279\u5f02\u6027\u6210\u719f\u6a21\u5f0f\u5bf9\u4e8e\u68c0\u6d4b\u5c40\u90e8\u5f02\u5e38\u53d1\u80b2\u8f68\u8ff9\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u7403\u9762\u8868\u9762\u8111\u5e74\u9f84\u9884\u6d4b\u7f51\u7edc(SurfAge-Net)\uff0c\u6574\u5408\u591a\u79cd\u5f62\u6001\u5b66\u6307\u6807\uff0c\u901a\u8fc7\u7a7a\u95f4-\u901a\u9053\u6df7\u5408\u548c\u4fa7\u5316\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u5f0f\u5efa\u6a21\u534a\u7403\u5185\u548c\u534a\u7403\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u6355\u6349\u6bcf\u4e2a\u76ee\u6807\u533a\u57df\u7684\u534f\u8c03\u6210\u719f\u6a21\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u80ce\u513f\u548c\u65b0\u751f\u513f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cSurfAge-Net\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5168\u5c40MAE=0.54\uff0c\u533a\u57dfMAE=0.45\u5b55\u5468\uff09\uff0c\u8de8\u961f\u5217\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u63d0\u4f9b\u7a7a\u95f4\u7cbe\u786e\u4e14\u751f\u7269\u5b66\u53ef\u89e3\u91ca\u7684\u76ae\u5c42\u6210\u719f\u56fe\u8c31\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u8111\u5e74\u9f84\u9884\u6d4b\u662f\u63a8\u8fdb\u795e\u7ecf\u53d1\u80b2\u7814\u7a76\u548c\u652f\u6301\u65e9\u671f\u4e34\u5e8a\u8bc4\u4f30\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0cSurfAge-Net\u80fd\u6709\u6548\u8bc6\u522b\u975e\u5178\u578b\u53d1\u80b2\u4eba\u7fa4\u7684\u5f02\u8d28\u6027\u5ef6\u8fdf\u548c\u533a\u57df\u7279\u5f02\u6027\u5f02\u5e38\u3002"}}
{"id": "2602.06995", "pdf": "https://arxiv.org/pdf/2602.06995", "abs": "https://arxiv.org/abs/2602.06995", "authors": ["Konstantinos Gounis", "Sotiris A. Tegos", "Dimitrios Tyrovolas", "Panagiotis D. Diamantoulakis", "George K. Karagiannidis"], "title": "When Simultaneous Localization and Mapping Meets Wireless Communications: A Survey", "categories": ["cs.RO", "cs.CV", "cs.IT", "cs.MA"], "comment": null, "summary": "The availability of commercial wireless communication and sensing equipment combined with the advancements in intelligent autonomous systems paves the way towards robust joint communications and simultaneous localization and mapping (SLAM). This paper surveys the state-of-the-art in the nexus of SLAM and Wireless Communications, attributing the bidirectional impact of each with a focus on visual SLAM (V-SLAM) integration. We provide an overview of key concepts related to wireless signal propagation, geometric channel modeling, and radio frequency (RF)-based localization and sensing. In addition to this, we show image processing techniques that can detect landmarks, proactively predicting optimal paths for wireless channels. Several dimensions are considered, including the prerequisites, techniques, background, and future directions and challenges of the intersection between SLAM and wireless communications. We analyze mathematical approaches such as probabilistic models, and spatial methods for signal processing, as well as key technological aspects. We expose techniques and items towards enabling a highly effective retrieval of the autonomous robot state. Among other interesting findings, we observe that monocular V-SLAM would benefit from RF relevant information, as the latter can serve as a proxy for the scale ambiguity resolution. Conversely, we find that wireless communications in the context of 5G and beyond can potentially benefit from visual odometry that is central in SLAM. Moreover, we examine other sources besides the camera for SLAM and describe the twofold relation with wireless communications. Finally, integrated solutions performing joint communications and SLAM are still in their infancy: theoretical and practical advancements are required to add higher-level localization and semantic perception capabilities to RF and multi-antenna technologies.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86SLAM\u4e0e\u65e0\u7ebf\u901a\u4fe1\u4ea4\u53c9\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u63a2\u8ba8\u89c6\u89c9SLAM\u4e0e\u65e0\u7ebf\u901a\u4fe1\u7684\u53cc\u5411\u5f71\u54cd\u5173\u7cfb\uff0c\u5206\u6790\u76f8\u5173\u6280\u672f\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5546\u4e1a\u65e0\u7ebf\u901a\u4fe1\u4e0e\u4f20\u611f\u8bbe\u5907\u7684\u666e\u53ca\u4ee5\u53ca\u667a\u80fd\u81ea\u4e3b\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u4e3a\u5b9e\u73b0\u9c81\u68d2\u7684\u8054\u5408\u901a\u4fe1\u4e0e\u540c\u65f6\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff08SLAM\uff09\u521b\u9020\u4e86\u6761\u4ef6\u3002\u672c\u6587\u65e8\u5728\u8c03\u67e5SLAM\u4e0e\u65e0\u7ebf\u901a\u4fe1\u4ea4\u53c9\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\uff0c\u7279\u522b\u5173\u6ce8\u89c6\u89c9SLAM\u4e0e\u65e0\u7ebf\u901a\u4fe1\u7684\u96c6\u6210\u3002", "method": "1. \u7efc\u8ff0\u65e0\u7ebf\u4fe1\u53f7\u4f20\u64ad\u3001\u51e0\u4f55\u4fe1\u9053\u5efa\u6a21\u3001RF\u5b9a\u4f4d\u4e0e\u611f\u77e5\u7b49\u5173\u952e\u6982\u5ff5\uff1b2. \u5206\u6790\u56fe\u50cf\u5904\u7406\u6280\u672f\u7528\u4e8e\u68c0\u6d4b\u5730\u6807\u548c\u9884\u6d4b\u65e0\u7ebf\u4fe1\u9053\u6700\u4f18\u8def\u5f84\uff1b3. \u4ece\u5148\u51b3\u6761\u4ef6\u3001\u6280\u672f\u3001\u80cc\u666f\u3001\u672a\u6765\u65b9\u5411\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8003\u5bdfSLAM\u4e0e\u65e0\u7ebf\u901a\u4fe1\u7684\u4ea4\u53c9\uff1b4. \u5206\u6790\u6982\u7387\u6a21\u578b\u3001\u7a7a\u95f4\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u7b49\u6570\u5b66\u65b9\u6cd5\uff1b5. \u63a2\u8ba8\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u72b6\u6001\u9ad8\u6548\u68c0\u7d22\u7684\u6280\u672f\u8981\u7d20\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1. \u5355\u76ee\u89c6\u89c9SLAM\u53ef\u4eceRF\u76f8\u5173\u4fe1\u606f\u4e2d\u53d7\u76ca\uff0cRF\u4fe1\u606f\u53ef\u4f5c\u4e3a\u5c3a\u5ea6\u6a21\u7cca\u6027\u89e3\u6790\u7684\u4ee3\u7406\uff1b2. 5G\u53ca\u4ee5\u540e\u7684\u65e0\u7ebf\u901a\u4fe1\u53ef\u4eceSLAM\u4e2d\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e2d\u53d7\u76ca\uff1b3. \u9664\u4e86\u76f8\u673a\u4e4b\u5916\u7684\u5176\u4ed6SLAM\u6e90\u4e0e\u65e0\u7ebf\u901a\u4fe1\u5b58\u5728\u53cc\u91cd\u5173\u7cfb\uff1b4. \u8054\u5408\u901a\u4fe1\u4e0eSLAM\u7684\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u4ecd\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\u3002", "conclusion": "\u8054\u5408\u901a\u4fe1\u4e0eSLAM\u7684\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u5c1a\u4e0d\u6210\u719f\uff0c\u9700\u8981\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u4e3aRF\u548c\u591a\u5929\u7ebf\u6280\u672f\u589e\u52a0\u66f4\u9ad8\u5c42\u6b21\u7684\u5b9a\u4f4d\u548c\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u3002\u672a\u6765\u7814\u7a76\u9700\u89e3\u51b3\u6280\u672f\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u4ea4\u53c9\u9886\u57df\u7684\u6df1\u5165\u53d1\u5c55\u3002"}}
{"id": "2602.07022", "pdf": "https://arxiv.org/pdf/2602.07022", "abs": "https://arxiv.org/abs/2602.07022", "authors": ["Yucheng Zhou", "Hao Li", "Jianbing Shen"], "title": "Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "ICLR 2026", "summary": "Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u6761\u4ef6\u7cbe\u70bc\u65b9\u6cd5\uff0c\u89e3\u51b3\u6761\u4ef6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u63a2\u7d22\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7528\u4e8e\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u81ea\u56de\u5f52\u6846\u67b6\u7ed3\u5408\u4ee5\u901a\u8fc7\u6269\u6563\u635f\u5931\u4f18\u5316\u56fe\u50cf\u751f\u6210\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u6269\u6563\u6a21\u578b\u548c\u5e26\u6269\u6563\u635f\u5931\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u63ed\u793a\u540e\u8005\u7684\u4f18\u52bf\uff0c\u5e76\u89e3\u51b3\u6761\u4ef6\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "1. \u7406\u8bba\u5206\u6790\u6269\u6563\u6a21\u578b\u548c\u5e26\u6269\u6563\u635f\u5931\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u7684\u6761\u4ef6\u7cbe\u70bc\u65b9\u6cd5\uff0c\u5c06\u6761\u4ef6\u7cbe\u70bc\u8868\u8ff0\u4e3aWasserstein\u68af\u5ea6\u6d41\uff1b3. \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1a\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u5757\u53bb\u566a\u4f18\u5316\u80fd\u6709\u6548\u7f13\u89e3\u6761\u4ef6\u8bef\u5dee\uff0c\u5bfc\u81f4\u7a33\u5b9a\u7684\u6761\u4ef6\u5206\u5e03\uff1b\u81ea\u56de\u5f52\u6761\u4ef6\u751f\u6210\u80fd\u7cbe\u70bc\u6761\u4ef6\uff0c\u4f7f\u6761\u4ef6\u8bef\u5dee\u5f71\u54cd\u5448\u6307\u6570\u8870\u51cf\u3002\u57fa\u4e8eOT\u7684\u6761\u4ef6\u7cbe\u70bc\u65b9\u6cd5\u80fd\u786e\u4fdd\u6536\u655b\u5230\u7406\u60f3\u6761\u4ef6\u5206\u5e03\uff0c\u6709\u6548\u7f13\u89e3\u6761\u4ef6\u4e0d\u4e00\u81f4\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u548c\u5e26\u6269\u6563\u635f\u5931\u7684\u81ea\u56de\u5f52\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u5728\u6761\u4ef6\u751f\u6210\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u6761\u4ef6\u7cbe\u70bc\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6761\u4ef6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07024", "pdf": "https://arxiv.org/pdf/2602.07024", "abs": "https://arxiv.org/abs/2602.07024", "authors": ["Valerio Belcamino", "Nhat Minh Dinh Le", "Quan Khanh Luu", "Alessandro Carf\u00ec", "Van Anh Ho", "Fulvio Mastrogiovanni"], "title": "A Distributed Multi-Modal Sensing Approach for Human Activity Recognition in Real-Time Human-Robot Collaboration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Human activity recognition (HAR) is fundamental in human-robot collaboration (HRC), enabling robots to respond to and dynamically adapt to human intentions. This paper introduces a HAR system combining a modular data glove equipped with Inertial Measurement Units and a vision-based tactile sensor to capture hand activities in contact with a robot. We tested our activity recognition approach under different conditions, including offline classification of segmented sequences, real-time classification under static conditions, and a realistic HRC scenario. The experimental results show a high accuracy for all the tasks, suggesting that multiple collaborative settings could benefit from this multi-modal approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u624b\u5957\u548c\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u7cfb\u7edf\uff0c\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u624b\u90e8\u6d3b\u52a8\u8bc6\u522b\uff0c\u5e76\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u5176\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u662f\u4eba\u673a\u534f\u4f5c\u7684\u57fa\u7840\uff0c\u80fd\u4f7f\u673a\u5668\u4eba\u7406\u89e3\u548c\u9002\u5e94\u4eba\u7c7b\u610f\u56fe\u3002\u5f53\u524d\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u624b\u90e8\u6d3b\u52a8\uff08\u7279\u522b\u662f\u4e0e\u673a\u5668\u4eba\u63a5\u89e6\u65f6\uff09\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001HAR\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u914d\u5907\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u7684\u6a21\u5757\u5316\u6570\u636e\u624b\u5957\u548c\u57fa\u4e8e\u89c6\u89c9\u7684\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u4ee5\u6355\u6349\u624b\u90e8\u6d3b\u52a8\u3002\u5728\u4e09\u79cd\u6761\u4ef6\u4e0b\u6d4b\u8bd5\uff1a\u79bb\u7ebf\u5206\u7c7b\u5206\u5272\u5e8f\u5217\u3001\u9759\u6001\u6761\u4ef6\u4e0b\u7684\u5b9e\u65f6\u5206\u7c7b\u3001\u4ee5\u53ca\u771f\u5b9e\u7684\u4eba\u673a\u534f\u4f5c\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6240\u6709\u4efb\u52a1\u90fd\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u8868\u660e\u8fd9\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u534f\u4f5c\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001HAR\u7cfb\u7edf\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u6709\u6548\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u624b\u90e8\u6d3b\u52a8\uff0c\u4e3a\u591a\u79cd\u534f\u4f5c\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u6709\u76ca\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2602.07029", "pdf": "https://arxiv.org/pdf/2602.07029", "abs": "https://arxiv.org/abs/2602.07029", "authors": ["Weiyun Jiang", "Haiyun Guo", "Christopher A. Metzler", "Ashok Veeraraghavan"], "title": "Guidestar-Free Adaptive Optics with Asymmetric Apertures", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This work introduces the first closed-loop adaptive optics (AO) system capable of optically correcting aberrations in real-time without a guidestar or a wavefront sensor. Nearly 40 years ago, Cederquist et al. demonstrated that asymmetric apertures enable phase retrieval (PR) algorithms to perform fully computational wavefront sensing, albeit at a high computational cost. More recently, Chimitt et al. extended this approach with machine learning and demonstrated real-time wavefront sensing using only a single (guidestar-based) point-spread-function (PSF) measurement. Inspired by these works, we introduce a guidestar-free AO framework built around asymmetric apertures and machine learning. Our approach combines three key elements: (1) an asymmetric aperture placed in the optical path that enables PR-based wavefront sensing, (2) a pair of machine learning algorithms that estimate the PSF from natural scene measurements and reconstruct phase aberrations, and (3) a spatial light modulator that performs optical correction. We experimentally validate this framework on dense natural scenes imaged through unknown obscurants. Our method outperforms state-of-the-art guidestar-free wavefront shaping methods, using an order of magnitude fewer measurements and three orders of magnitude less computation.", "AI": {"tldr": "\u9996\u4e2a\u65e0\u9700\u5bfc\u661f\u548c\u6ce2\u524d\u4f20\u611f\u5668\u7684\u95ed\u73af\u81ea\u9002\u5e94\u5149\u5b66\u7cfb\u7edf\uff0c\u5229\u7528\u975e\u5bf9\u79f0\u5b54\u5f84\u548c\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u5b9e\u65f6\u50cf\u5dee\u6821\u6b63", "motivation": "\u4f20\u7edf\u81ea\u9002\u5e94\u5149\u5b66\u7cfb\u7edf\u4f9d\u8d56\u5bfc\u661f\u548c\u6ce2\u524d\u4f20\u611f\u5668\uff0c\u9650\u5236\u4e86\u5176\u5728\u81ea\u7136\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u65e0\u9700\u5bfc\u661f\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u5149\u5b66\u7cfb\u7edf\uff0c\u89e3\u51b3\u81ea\u7136\u573a\u666f\u6210\u50cf\u4e2d\u7684\u50cf\u5dee\u95ee\u9898", "method": "\u7ed3\u5408\u4e09\u4e2a\u5173\u952e\u8981\u7d20\uff1a1) \u5149\u5b66\u8def\u5f84\u4e2d\u7684\u975e\u5bf9\u79f0\u5b54\u5f84\u5b9e\u73b0\u76f8\u4f4d\u6062\u590d\uff1b2) \u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4ece\u81ea\u7136\u573a\u666f\u6d4b\u91cf\u4f30\u8ba1\u70b9\u6269\u6563\u51fd\u6570\u5e76\u91cd\u5efa\u76f8\u4f4d\u50cf\u5dee\uff1b3) \u7a7a\u95f4\u5149\u8c03\u5236\u5668\u8fdb\u884c\u5149\u5b66\u6821\u6b63", "result": "\u5728\u672a\u77e5\u906e\u6321\u7269\u7684\u5bc6\u96c6\u81ea\u7136\u573a\u666f\u6210\u50cf\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u65e0\u5bfc\u661f\u6ce2\u524d\u6574\u5f62\u65b9\u6cd5\uff0c\u6d4b\u91cf\u6b21\u6570\u51cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e09\u4e2a\u6570\u91cf\u7ea7", "conclusion": "\u6210\u529f\u5b9e\u73b0\u4e86\u9996\u4e2a\u65e0\u9700\u5bfc\u661f\u548c\u6ce2\u524d\u4f20\u611f\u5668\u7684\u95ed\u73af\u81ea\u9002\u5e94\u5149\u5b66\u7cfb\u7edf\uff0c\u4e3a\u81ea\u7136\u573a\u666f\u5b9e\u65f6\u50cf\u5dee\u6821\u6b63\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.07037", "pdf": "https://arxiv.org/pdf/2602.07037", "abs": "https://arxiv.org/abs/2602.07037", "authors": ["Huannan Zheng", "Jingli Liu", "Kezhou Yang"], "title": "Stochastic Spiking Neuron Based SNN Can be Inherently Bayesian", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.ET"], "comment": null, "summary": "Uncertainty in biological neural systems appears to be computationally beneficial rather than detrimental. However, in neuromorphic computing systems, device variability often limits performance, including accuracy and efficiency. In this work, we propose a spiking Bayesian neural network (SBNN) framework that unifies the dynamic models of intrinsic device stochasticity (based on Magnetic Tunnel Junctions) and stochastic threshold neurons to leverage noise as a functional Bayesian resource. Experiments demonstrate that SBNN achieves high accuracy (99.16% on MNIST, 94.84% on CIFAR10) with 8-bit precision. Meanwhile rate estimation method provides a ~20-fold training speedup. Furthermore, SBNN exhibits superior robustness, showing a 67% accuracy improvement under synaptic weight noise and 12% under input noise compared to standard spiking neural networks. Crucially, hardware validation confirms that physical device implementation causes invisible accuracy and calibration loss compared to the algorithmic model. Converting device stochasticity into neuronal uncertainty offers a route to compact, energy-efficient neuromorphic computing under uncertainty.", "AI": {"tldr": "\u63d0\u51faSBNN\u6846\u67b6\uff0c\u5c06\u78c1\u6027\u96a7\u9053\u7ed3\u5668\u4ef6\u968f\u673a\u6027\u4e0e\u968f\u673a\u9608\u503c\u795e\u7ecf\u5143\u7edf\u4e00\u5efa\u6a21\uff0c\u5c06\u566a\u58f0\u8f6c\u5316\u4e3a\u8d1d\u53f6\u65af\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u8bad\u7ec3\u548c\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u7684\u968f\u673a\u6027\u5177\u6709\u8ba1\u7b97\u4f18\u52bf\uff0c\u4f46\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7cfb\u7edf\u7684\u5668\u4ef6\u53d8\u5f02\u6027\u901a\u5e38\u9650\u5236\u6027\u80fd\u3002\u9700\u8981\u5c06\u5668\u4ef6\u968f\u673a\u6027\u8f6c\u5316\u4e3a\u8ba1\u7b97\u8d44\u6e90\u800c\u975e\u969c\u788d\u3002", "method": "\u63d0\u51fa\u8109\u51b2\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc(SBNN)\u6846\u67b6\uff0c\u7edf\u4e00\u78c1\u6027\u96a7\u9053\u7ed3\u5668\u4ef6\u968f\u673a\u6027\u6a21\u578b\u548c\u968f\u673a\u9608\u503c\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u5229\u7528\u566a\u58f0\u4f5c\u4e3a\u8d1d\u53f6\u65af\u8ba1\u7b97\u8d44\u6e90\uff0c\u91c7\u7528\u901f\u7387\u4f30\u8ba1\u65b9\u6cd5\u52a0\u901f\u8bad\u7ec3\u3002", "result": "SBNN\u5728MNIST\u4e0a\u8fbe\u523099.16%\u51c6\u786e\u7387\uff0cCIFAR10\u4e0a94.84%\uff088\u4f4d\u7cbe\u5ea6\uff09\uff1b\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u7ea620\u500d\uff1b\u5728\u7a81\u89e6\u6743\u91cd\u566a\u58f0\u4e0b\u51c6\u786e\u7387\u63d0\u534767%\uff0c\u8f93\u5165\u566a\u58f0\u4e0b\u63d0\u534712%\uff1b\u786c\u4ef6\u9a8c\u8bc1\u663e\u793a\u7269\u7406\u5b9e\u73b0\u4e0e\u7b97\u6cd5\u6a21\u578b\u51e0\u4e4e\u65e0\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u5c06\u5668\u4ef6\u968f\u673a\u6027\u8f6c\u5316\u4e3a\u795e\u7ecf\u5143\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u7d27\u51d1\u3001\u9ad8\u80fd\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u566a\u58f0\u4ece\u969c\u788d\u5230\u8ba1\u7b97\u8d44\u6e90\u7684\u8f6c\u53d8\u3002"}}
{"id": "2602.07054", "pdf": "https://arxiv.org/pdf/2602.07054", "abs": "https://arxiv.org/abs/2602.07054", "authors": ["Ashutosh Chaubey", "Jiacheng Pang", "Maksim Siniukov", "Mohammad Soleymani"], "title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization", "categories": ["cs.LG", "cs.CV", "cs.HC"], "comment": "Accepted as a conference paper at ICLR 2026. Project page: https://avere-iclr.github.io", "summary": "Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEmoReAlM\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u4e2d\u7684\u865a\u5047\u5173\u8054\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u63d0\u51faAVEm-DPO\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u60c5\u611f\u4e0e\u65e0\u5173\u89c6\u542c\u7ebf\u7d22\u4e4b\u95f4\u7684\u865a\u5047\u5173\u8054\uff1b2\uff09\u8bed\u8a00\u6a21\u578b\u4e3b\u5e72\u4e2d\u7684\u6587\u672c\u5148\u9a8c\u9a71\u52a8\u7684\u89c6\u542c\u7ebf\u7d22\u5e7b\u89c9\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u6a21\u578b\u5728\u793e\u4ea4\u667a\u80fd\u4f53\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u9996\u5148\u5f15\u5165EmoReAlM\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u7ebf\u7d22-\u60c5\u611f\u5173\u8054\u3001\u5e7b\u89c9\u548c\u6a21\u6001\u4e00\u81f4\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002\u7136\u540e\u63d0\u51faAVEm-DPO\u504f\u597d\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u6784\u5efa\u5bf9\u865a\u5047\u5173\u8054\u6216\u5e7b\u89c9\u54cd\u5e94\u7684\u504f\u597d\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u89c6\u542c\u8f93\u5165\u5bf9\u504f\u597d\u6765\u5bf9\u9f50\u6a21\u578b\u54cd\u5e94\u3002\u8fd8\u5305\u62ec\u4e00\u4e2a\u6b63\u5219\u5316\u9879\u6765\u60e9\u7f5a\u5bf9\u6587\u672c\u5148\u9a8c\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u51cf\u8f7b\u6a21\u6001\u7279\u5b9a\u7ebf\u7d22\u7684\u5e7b\u89c9\u3002", "result": "\u5728DFEW\u3001RAVDESS\u548cEMER\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u53c2\u8003\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u83b7\u5f97\u4e866-19%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u7a33\u5065\u7684\u4f18\u5316\u6846\u67b6\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u60c5\u611f\u7406\u89e3\u548c\u793e\u4ea4AI\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u5728\u6307\u5b9a\u7f51\u7ad9\u53d1\u5e03\u3002"}}
{"id": "2602.07056", "pdf": "https://arxiv.org/pdf/2602.07056", "abs": "https://arxiv.org/abs/2602.07056", "authors": ["Mehmet Yamac", "Lei Xu", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "MTS-CSNet: Multiscale Tensor Factorization for Deep Compressive Sensing on RGB Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "6 pages, 5 figures", "summary": "Deep learning based compressive sensing (CS) methods typically learn sampling operators using convolutional or block wise fully connected layers, which limit receptive fields and scale poorly for high dimensional data. We propose MTSCSNet, a CS framework based on Multiscale Tensor Summation (MTS) factorization, a structured operator for efficient multidimensional signal processing. MTS performs mode-wise linear transformations with multiscale summation, enabling large receptive fields and effective modeling of cross-dimensional correlations. In MTSCSNet, MTS is first used as a learnable CS operator that performs linear dimensionality reduction in tensor space, with its adjoint defining the initial back-projection, and is then applied in the reconstruction stage to directly refine this estimate. This results in a simple feed-forward architecture without iterative or proximal optimization, while remaining parameter and computation efficient. Experiments on standard CS benchmarks show that MTSCSNet achieves state-of-the-art reconstruction performance on RGB images, with notable PSNR gains and faster inference, even compared to recent diffusion-based CS methods, while using a significantly more compact feed-forward architecture.", "AI": {"tldr": "MTSCSNet\u63d0\u51fa\u57fa\u4e8e\u591a\u5c3a\u5ea6\u5f20\u91cf\u6c42\u548c\u5206\u89e3\u7684\u538b\u7f29\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5f0f\u7ebf\u6027\u53d8\u6362\u548c\u591a\u5c3a\u5ea6\u6c42\u548c\u5b9e\u73b0\u5927\u611f\u53d7\u91ce\u548c\u8de8\u7ef4\u5ea6\u76f8\u5173\u6027\u5efa\u6a21\uff0c\u5728RGB\u56fe\u50cf\u91cd\u5efa\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u5377\u79ef\u6216\u5757\u72b6\u5168\u8fde\u63a5\u5c42\u5b66\u4e60\u91c7\u6837\u7b97\u5b50\uff0c\u611f\u53d7\u91ce\u53d7\u9650\u4e14\u5bf9\u9ad8\u7ef4\u6570\u636e\u6269\u5c55\u6027\u5dee\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u7ef4\u4fe1\u53f7\u5904\u7406\u7b97\u5b50\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u5f20\u91cf\u6c42\u548c\u5206\u89e3\u4f5c\u4e3a\u7ed3\u6784\u5316\u7b97\u5b50\uff0c\u6267\u884c\u6a21\u5f0f\u7ebf\u6027\u53d8\u6362\u548c\u591a\u5c3a\u5ea6\u6c42\u548c\uff0c\u7528\u4e8e\u53ef\u5b66\u4e60\u7684\u538b\u7f29\u611f\u77e5\u7b97\u5b50\uff08\u7ebf\u6027\u964d\u7ef4\uff09\u548c\u91cd\u5efa\u9636\u6bb5\u76f4\u63a5\u7ec6\u5316\u4f30\u8ba1\uff0c\u5f62\u6210\u7b80\u5355\u524d\u9988\u67b6\u6784\u3002", "result": "\u5728\u6807\u51c6\u538b\u7f29\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMTSCSNet\u5728RGB\u56fe\u50cf\u91cd\u5efa\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cPSNR\u663e\u8457\u63d0\u5347\u4e14\u63a8\u7406\u66f4\u5feb\uff0c\u5373\u4f7f\u76f8\u6bd4\u6700\u8fd1\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u7d27\u51d1\u7684\u524d\u9988\u67b6\u6784\u3002", "conclusion": "MTSCSNet\u901a\u8fc7\u591a\u5c3a\u5ea6\u5f20\u91cf\u6c42\u548c\u5206\u89e3\u5b9e\u73b0\u4e86\u53c2\u6570\u548c\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u538b\u7f29\u611f\u77e5\u6846\u67b6\uff0c\u65e0\u9700\u8fed\u4ee3\u6216\u8fd1\u7aef\u4f18\u5316\uff0c\u5728\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u548c\u901f\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.07060", "pdf": "https://arxiv.org/pdf/2602.07060", "abs": "https://arxiv.org/abs/2602.07060", "authors": ["Haochen Wang", "Pei Yu", "Liangwen Chen", "Weibo He", "Yu Zhang", "Yuhong Yu", "Xueheng Zhang", "Lei Yang", "Zhiyu Sun"], "title": "U-Net Based Image Enhancement for Short-time Muon Scattering Tomography", "categories": ["eess.IV", "cs.CV", "physics.ins-det", "physics.med-ph"], "comment": null, "summary": "Muon Scattering Tomography (MST) is a promising non-invasive inspection technique, yet the practical application of short-time MST is hindered by poor image quality due to limited muon flux. To address this limitation, we propose a U-Net-based framework trained on Point of Closest Approach (PoCA) images reconstructed with simulation MST data to enhance image quality. When applied to experimental MST data, the framework significantly improves image quality, increasing the Structural Similarity Index Measure (SSIM) from 0.7232 to 0.9699 and decreasing the Learned Perceptual Image Patch Similarity (LPIPS) from 0.3604 to 0.0270. These results demonstrate that our method can effectively enhance low-statistics MST images, thereby paving the way for the practical deployment of short-time MST.", "AI": {"tldr": "\u4f7f\u7528U-Net\u6846\u67b6\u589e\u5f3a\u4f4e\u7edf\u8ba1\u91cf\u03bc\u5b50\u6563\u5c04\u65ad\u5c42\u626b\u63cf\u56fe\u50cf\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347SSIM\u4ece0.7232\u52300.9699\uff0c\u964d\u4f4eLPIPS\u4ece0.3604\u52300.0270", "motivation": "\u03bc\u5b50\u6563\u5c04\u65ad\u5c42\u626b\u63cf(MST)\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u975e\u4fb5\u5165\u5f0f\u68c0\u6d4b\u6280\u672f\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u77ed\u65f6\u95f4MST\u56e0\u03bc\u5b50\u901a\u91cf\u6709\u9650\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u5dee\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528", "method": "\u63d0\u51fa\u57fa\u4e8eU-Net\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u6a21\u62dfMST\u6570\u636e\u91cd\u5efa\u7684\u6700\u63a5\u8fd1\u70b9(PoCA)\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u7528\u4e8e\u589e\u5f3a\u5b9e\u9a8cMST\u6570\u636e\u7684\u56fe\u50cf\u8d28\u91cf", "result": "\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\uff1a\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u4ece0.7232\u63d0\u9ad8\u52300.9699\uff0c\u5b66\u4e60\u611f\u77e5\u56fe\u50cf\u5757\u76f8\u4f3c\u6027\u4ece0.3604\u964d\u4f4e\u52300.0270", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u4f4e\u7edf\u8ba1\u91cfMST\u56fe\u50cf\uff0c\u4e3a\u77ed\u65f6\u95f4MST\u7684\u5b9e\u9645\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2602.07063", "pdf": "https://arxiv.org/pdf/2602.07063", "abs": "https://arxiv.org/abs/2602.07063", "authors": ["Serkan Sulun"], "title": "Video-based Music Generation", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "comment": "PhD thesis, University of Porto", "summary": "As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called \"boundary offset encodings,\" aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.", "AI": {"tldr": "EMSYNC\u662f\u4e00\u4e2a\u5feb\u901f\u3001\u514d\u8d39\u3001\u81ea\u52a8\u7684\u89c6\u9891\u914d\u4e50\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u60c5\u611f\u5206\u7c7b\u3001\u60c5\u611fMIDI\u751f\u6210\u548c\u65f6\u5e8f\u8fb9\u754c\u5bf9\u9f50\u6280\u672f\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u60c5\u611f\u548c\u8282\u594f\u540c\u6b65\u7684\u97f3\u4e50\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u89c6\u9891\u5185\u5bb9\u5feb\u901f\u589e\u957f\uff0c\u5bfb\u627e\u5408\u9002\u7684\u914d\u4e50\u6210\u4e3a\u91cd\u5927\u6311\u6218\u3002\u5185\u5bb9\u521b\u4f5c\u8005\u9700\u8981\u65e0\u9700\u4f5c\u66f2\u6216\u6388\u6743\u5373\u53ef\u589e\u5f3a\u89c6\u9891\u8d28\u91cf\u7684\u97f3\u4e50\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u65b0\u9896\u7684\u89c6\u9891\u60c5\u611f\u5206\u7c7b\u5668\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u6df1\u5ea6\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\uff0c\u4ec5\u8bad\u7ec3\u878d\u5408\u5c42\uff1b2) \u6784\u5efa\u5927\u89c4\u6a21\u60c5\u611f\u6807\u6ce8MIDI\u6570\u636e\u96c6\uff1b3) \u9996\u4e2a\u57fa\u4e8e\u8fde\u7eed\u60c5\u611f\u503c\u800c\u975e\u79bb\u6563\u7c7b\u522b\u7684MIDI\u751f\u6210\u5668\uff1b4) \u5f15\u5165\"\u8fb9\u754c\u504f\u79fb\u7f16\u7801\"\u5b9e\u73b0\u97f3\u4e50\u548c\u5f26\u4e0e\u573a\u666f\u53d8\u5316\u7684\u65f6\u5e8f\u5bf9\u9f50\u3002", "result": "\u5728Ekman-6\u548cMovieNet\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff1b\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u5728\u97f3\u4e50\u4e30\u5bcc\u5ea6\u3001\u60c5\u611f\u5bf9\u9f50\u3001\u65f6\u5e8f\u540c\u6b65\u548c\u6574\u4f53\u504f\u597d\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u89c6\u9891\u914d\u4e50\u751f\u6210\u7684\u65b0\u6807\u6746\u3002", "conclusion": "EMSYNC\u4f5c\u4e3a\u4e00\u4e2a\u5168\u81ea\u52a8\u89c6\u9891\u914d\u4e50\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u60c5\u611f\u5206\u7c7b\u3001\u60c5\u611f\u97f3\u4e50\u751f\u6210\u548c\u65f6\u5e8f\u540c\u6b65\u6280\u672f\u7684\u7ed3\u5408\uff0c\u4e3a\u5185\u5bb9\u521b\u4f5c\u8005\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f18\u8d28\u7684\u914d\u4e50\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u89c6\u9891\u914d\u4e50\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07068", "pdf": "https://arxiv.org/pdf/2602.07068", "abs": "https://arxiv.org/abs/2602.07068", "authors": ["Ali Alqutayfi", "Sadam Al-Azani"], "title": "MRI Cross-Modal Synthesis: A Comparative Study of Generative Models for T1-to-T2 Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "MRI cross-modal synthesis involves generating images from one acquisition protocol using another, offering considerable clinical value by reducing scan time while maintaining diagnostic information. This paper presents a comprehensive comparison of three state-of-the-art generative models for T1-to-T2 MRI reconstruction: Pix2Pix GAN, CycleGAN, and Variational Autoencoder (VAE). Using the BraTS 2020 dataset (11,439 training and 2,000 testing slices), we evaluate these models based on established metrics including Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM). Our experiments demonstrate that all models can successfully synthesize T2 images from T1 inputs, with CycleGAN achieving the highest PSNR (32.28 dB) and SSIM (0.9008), while Pix2Pix GAN provides the lowest MSE (0.005846). The VAE, though showing lower quantitative performance (MSE: 0.006949, PSNR: 24.95 dB, SSIM: 0.6573), offers advantages in latent space representation and sampling capabilities. This comparative study provides valuable insights for researchers and clinicians selecting appropriate generative models for MRI synthesis applications based on their specific requirements and data constraints.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u751f\u6210\u6a21\u578b\uff08Pix2Pix GAN\u3001CycleGAN\u3001VAE\uff09\u5728T1\u5230T2 MRI\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0CycleGAN\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cPix2Pix GAN\u7684MSE\u6700\u4f4e\uff0cVAE\u5219\u5728\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u65b9\u9762\u6709\u4f18\u52bf\u3002", "motivation": "MRI\u8de8\u6a21\u6001\u5408\u6210\u80fd\u591f\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u540c\u65f6\u4fdd\u6301\u8bca\u65ad\u4fe1\u606f\uff0c\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u4ef7\u503c\u3002\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e09\u79cd\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u5728T1\u5230T2 MRI\u91cd\u5efa\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u6a21\u578b\u9009\u62e9\u7684\u53c2\u8003\u4f9d\u636e\u3002", "method": "\u4f7f\u7528BraTS 2020\u6570\u636e\u96c6\uff0811,439\u4e2a\u8bad\u7ec3\u5207\u7247\u548c2,000\u4e2a\u6d4b\u8bd5\u5207\u7247\uff09\uff0c\u5bf9Pix2Pix GAN\u3001CycleGAN\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u4e09\u79cd\u751f\u6210\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u3001\u5cf0\u503c\u4fe1\u566a\u6bd4\uff08PSNR\uff09\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\uff08SSIM\uff09\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u80fd\u6210\u529f\u4eceT1\u8f93\u5165\u5408\u6210T2\u56fe\u50cf\uff1aCycleGAN\u83b7\u5f97\u6700\u9ad8PSNR\uff0832.28 dB\uff09\u548cSSIM\uff080.9008\uff09\uff0cPix2Pix GAN\u63d0\u4f9b\u6700\u4f4eMSE\uff080.005846\uff09\uff0cVAE\u5b9a\u91cf\u6027\u80fd\u8f83\u4f4e\uff08MSE: 0.006949\uff0cPSNR: 24.95 dB\uff0cSSIM: 0.6573\uff09\u4f46\u5728\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u548c\u91c7\u6837\u80fd\u529b\u65b9\u9762\u6709\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u9879\u6bd4\u8f83\u7814\u7a76\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4e34\u5e8a\u533b\u751f\u6839\u636e\u7279\u5b9a\u9700\u6c42\u548c\u6570\u636e\u7ea6\u675f\u9009\u62e9\u9002\u5f53\u7684MRI\u5408\u6210\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002\u4e0d\u540c\u6a21\u578b\u5404\u6709\u4f18\u52a3\uff0c\u9009\u62e9\u5e94\u57fa\u4e8e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2602.07081", "pdf": "https://arxiv.org/pdf/2602.07081", "abs": "https://arxiv.org/abs/2602.07081", "authors": ["Thu Hang Phung", "Duong M. Nguyen", "Thanh Trung Huynh", "Quoc Viet Hung Nguyen", "Trong Nghia Hoang", "Phi Le Nguyen"], "title": "Federated Prompt-Tuning with Heterogeneous and Incomplete Multimodal Client Data", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper introduces a generalized federated prompt-tuning framework for practical scenarios where local datasets are multi-modal and exhibit different distributional patterns of missing features at the input level. The proposed framework bridges the gap between federated learning and multi-modal prompt-tuning which have traditionally focused on either uni-modal or centralized data. A key challenge in this setting arises from the lack of semantic alignment between prompt instructions that encode similar distributional patterns of missing data across different clients. To address this, our framework introduces specialized client-tuning and server-aggregation designs that simultaneously optimize, align, and aggregate prompt-tuning instructions across clients and data modalities. This allows prompt instructions to complement one another and be combined effectively. Extensive evaluations on diverse multimodal benchmark datasets demonstrate that our work consistently outperforms state-of-the-art (SOTA) baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8054\u90a6\u63d0\u793a\u8c03\u4f18\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001\u3001\u7279\u5f81\u7f3a\u5931\u5206\u5e03\u4e0d\u540c\u7684\u672c\u5730\u6570\u636e\u96c6\u573a\u666f\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u8c03\u4f18\u548c\u670d\u52a1\u5668\u805a\u5408\u8bbe\u8ba1\u4f18\u5316\u3001\u5bf9\u9f50\u548c\u805a\u5408\u63d0\u793a\u6307\u4ee4\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u63d0\u793a\u8c03\u4f18\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6216\u96c6\u4e2d\u5f0f\u6570\u636e\uff0c\u800c\u5b9e\u9645\u573a\u666f\u4e2d\u672c\u5730\u6570\u636e\u96c6\u5f80\u5f80\u591a\u6a21\u6001\u4e14\u8f93\u5165\u5c42\u7279\u5f81\u7f3a\u5931\u6a21\u5f0f\u5206\u5e03\u4e0d\u540c\uff0c\u9700\u8981\u89e3\u51b3\u8de8\u5ba2\u6237\u7aef\u76f8\u4f3c\u7f3a\u5931\u6a21\u5f0f\u63d0\u793a\u6307\u4ee4\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e13\u95e8\u7684\u5ba2\u6237\u7aef\u8c03\u4f18\u548c\u670d\u52a1\u5668\u805a\u5408\u8bbe\u8ba1\uff0c\u540c\u65f6\u4f18\u5316\u3001\u5bf9\u9f50\u548c\u805a\u5408\u8de8\u5ba2\u6237\u7aef\u548c\u6570\u636e\u6a21\u6001\u7684\u63d0\u793a\u8c03\u4f18\u6307\u4ee4\uff0c\u4f7f\u63d0\u793a\u6307\u4ee4\u80fd\u591f\u76f8\u4e92\u8865\u5145\u5e76\u6709\u6548\u7ed3\u5408\u3002", "result": "\u5728\u591a\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u586b\u8865\u4e86\u8054\u90a6\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u63d0\u793a\u8c03\u4f18\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u591a\u6a21\u6001\u3001\u7279\u5f81\u7f3a\u5931\u5206\u5e03\u4e0d\u540c\u7684\u672c\u5730\u6570\u636e\u96c6\u7684\u8054\u90a6\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2602.07094", "pdf": "https://arxiv.org/pdf/2602.07094", "abs": "https://arxiv.org/abs/2602.07094", "authors": ["Quentin Gabot", "Joana Frontera-Pons", "J\u00e9r\u00e9my Fix", "Chengfang Ren", "Jean-Philippe Ovarlez"], "title": "Exploring Polarimetric Properties Preservation during Reconstruction of PolSAR images using Complex-valued Convolutional Neural Networks", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted with minor revisions at IET Radar, Sonar & Navigation", "summary": "The inherently complex-valued nature of Polarimetric SAR data necessitates using specialized algorithms capable of directly processing complex-valued representations. However, this aspect remains underexplored in the deep learning community, with many studies opting to convert complex signals into the real domain before applying conventional real-valued models. In this work, we leverage complex-valued neural networks and investigate the performance of complex-valued Convolutional AutoEncoders. We show that these networks can effectively compress and reconstruct fully polarimetric SAR data while preserving essential physical characteristics, as demonstrated through Pauli, Krogager, and Cameron coherent decompositions, as well as the non-coherent $H-\u03b1$ decomposition. Finally, we highlight the advantages of complex-valued neural networks over their real-valued counterparts. These insights pave the way for developing robust, physics-informed, complex-valued generative models for SAR data processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u590d\u6570\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u6781\u5316SAR\u6570\u636e\uff0c\u901a\u8fc7\u590d\u6570\u5377\u79ef\u81ea\u7f16\u7801\u5668\u6709\u6548\u538b\u7f29\u548c\u91cd\u5efa\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u7406\u7279\u6027\uff0c\u4f18\u4e8e\u5b9e\u6570\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "\u6781\u5316SAR\u6570\u636e\u672c\u8d28\u4e0a\u662f\u590d\u6570\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u793e\u533a\u5bf9\u6b64\u63a2\u7d22\u4e0d\u8db3\uff0c\u8bb8\u591a\u7814\u7a76\u5c06\u590d\u6570\u4fe1\u53f7\u8f6c\u6362\u5230\u5b9e\u6570\u57df\u518d\u5904\u7406\u3002\u9700\u8981\u4e13\u95e8\u7b97\u6cd5\u76f4\u63a5\u5904\u7406\u590d\u6570\u8868\u793a\u3002", "method": "\u91c7\u7528\u590d\u6570\u795e\u7ecf\u7f51\u7edc\uff0c\u7814\u7a76\u590d\u6570\u5377\u79ef\u81ea\u7f16\u7801\u5668\u7684\u6027\u80fd\uff0c\u7528\u4e8e\u538b\u7f29\u548c\u91cd\u5efa\u5168\u6781\u5316SAR\u6570\u636e\uff0c\u5e76\u901a\u8fc7Pauli\u3001Krogager\u3001Cameron\u76f8\u5e72\u5206\u89e3\u4ee5\u53ca\u975e\u76f8\u5e72H-\u03b1\u5206\u89e3\u9a8c\u8bc1\u7269\u7406\u7279\u6027\u4fdd\u6301\u3002", "result": "\u590d\u6570\u795e\u7ecf\u7f51\u7edc\u80fd\u6709\u6548\u538b\u7f29\u548c\u91cd\u5efa\u6781\u5316SAR\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u8981\u7684\u7269\u7406\u7279\u6027\uff0c\u5728\u591a\u4e2a\u5206\u89e3\u65b9\u6cd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e14\u4f18\u4e8e\u5b9e\u6570\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "\u590d\u6570\u795e\u7ecf\u7f51\u7edc\u5728\u6781\u5316SAR\u6570\u636e\u5904\u7406\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u7684\u3001\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u7684\u590d\u6570\u751f\u6210\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.07125", "pdf": "https://arxiv.org/pdf/2602.07125", "abs": "https://arxiv.org/abs/2602.07125", "authors": ["Jianrui Zhang", "Anirudh Sundara Rajan", "Brandon Han", "Soochahn Lee", "Sukanta Ganguly", "Yong Jae Lee"], "title": "Reasoning-Augmented Representations for Multimodal Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry \"silent\" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.", "AI": {"tldr": "UMR\u63d0\u51fa\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u5316\u63a8\u7406\u6765\u6539\u5584\u591a\u6a21\u6001\u68c0\u7d22\uff0c\u5728M-BEIR\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u5728\u9700\u8981\u6f5c\u5728\u63a8\u7406\uff08\u5982\u89e3\u51b3\u6a21\u7cca\u5f15\u7528\u6216\u5339\u914d\u7ec4\u5408\u7ea6\u675f\uff09\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u8fd9\u79cd\u8106\u5f31\u6027\u6e90\u4e8e\u6570\u636e\u95ee\u9898\uff1a\u56fe\u50cf\u5305\u542b\"\u6c89\u9ed8\"\u8bc1\u636e\u800c\u67e5\u8be2\u9690\u542b\u5173\u952e\u8bed\u4e49\uff0c\u5bfc\u81f4\u5355\u6b21\u5d4c\u5165\u9700\u8981\u540c\u65f6\u5b8c\u6210\u63a8\u7406\u548c\u538b\u7f29\uff0c\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u7279\u5f81\u5339\u914d", "method": "\u63d0\u51fa\u6570\u636e\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u4e0e\u68c0\u7d22\u89e3\u8026\uff1a1\uff09\u4f7f\u7528\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u8bed\u6599\u5e93\u6761\u76ee\u751f\u6210\u5bc6\u96c6\u63cf\u8ff0\uff0c\u4f7f\u89c6\u89c9\u8bc1\u636e\u663e\u5f0f\u5316\uff1b2\uff09\u89e3\u6790\u67e5\u8be2\u4e2d\u7684\u6a21\u7cca\u591a\u6a21\u6001\u5f15\u7528\uff1b3\uff09\u5c06\u5197\u957f\u6307\u4ee4\u91cd\u5199\u4e3a\u7b80\u6d01\u68c0\u7d22\u7ea6\u675f\u3002\u540c\u65f6\u8bad\u7ec3\u68c0\u7d22\u5668\u5728\u8fd9\u4e9b\u8bed\u4e49\u5bc6\u96c6\u8868\u793a\u4e0a\uff0c\u907f\u514d\u5206\u5e03\u504f\u79fb", "result": "\u5728M-BEIR\u57fa\u51c6\u4e0a\uff0c\u63a8\u7406\u589e\u5f3a\u8bad\u7ec3\u65b9\u6cd5\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff1a\u8bed\u6599\u589e\u5f3a\u4e3b\u8981\u5e2e\u52a9\u77e5\u8bc6\u5bc6\u96c6\u578b\u67e5\u8be2\uff0c\u800c\u67e5\u8be2\u589e\u5f3a\u5bf9\u7ec4\u5408\u4fee\u6539\u8bf7\u6c42\u81f3\u5173\u91cd\u8981", "conclusion": "\u901a\u8fc7\u5916\u90e8\u5316\u63a8\u7406\u5e76\u5c06\u68c0\u7d22\u5668\u8bad\u7ec3\u5728\u8bed\u4e49\u5bc6\u96c6\u8868\u793a\u4e0a\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u6f5c\u5728\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027"}}
{"id": "2602.07156", "pdf": "https://arxiv.org/pdf/2602.07156", "abs": "https://arxiv.org/abs/2602.07156", "authors": ["Asher Trockman", "J. Zico Kolter"], "title": "Mimetic Initialization of MLPs", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space layers. In this work, we present the first attempt to apply the method to channel mixing layers, namely multilayer perceptrons (MLPs). Our extremely simple technique for MLPs -- to give the first layer a nonzero mean -- speeds up training on small-scale vision tasks like CIFAR-10 and ImageNet-1k. Though its effect is much smaller than spatial mixing initializations, it can be used in conjunction with them for an additional positive effect.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5c06\u6a21\u4eff\u521d\u59cb\u5316\u5e94\u7528\u4e8e\u901a\u9053\u6df7\u5408\u5c42\uff08MLP\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed9\u7b2c\u4e00\u5c42\u8d4b\u4e88\u975e\u96f6\u5747\u503c\u6765\u52a0\u901f\u5c0f\u89c4\u6a21\u89c6\u89c9\u4efb\u52a1\u7684\u8bad\u7ec3", "motivation": "\u6a21\u4eff\u521d\u59cb\u5316\u5148\u524d\u53ea\u5e94\u7528\u4e8e\u7a7a\u95f4\u6df7\u5408\u5c42\uff08\u5377\u79ef\u3001\u81ea\u6ce8\u610f\u529b\u3001\u72b6\u6001\u7a7a\u95f4\u5c42\uff09\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8e\u901a\u9053\u6df7\u5408\u5c42\uff08MLP\uff09\uff0c\u9700\u8981\u63a2\u7d22MLP\u7684\u521d\u59cb\u5316\u65b9\u6cd5", "method": "\u7ed9\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u7684\u7b2c\u4e00\u5c42\u8d4b\u4e88\u975e\u96f6\u5747\u503c\uff0c\u8fd9\u662f\u4e00\u79cd\u6781\u5176\u7b80\u5355\u7684\u521d\u59cb\u5316\u6280\u672f", "result": "\u8be5\u65b9\u6cd5\u5728CIFAR-10\u548cImageNet-1k\u7b49\u5c0f\u89c4\u6a21\u89c6\u89c9\u4efb\u52a1\u4e0a\u52a0\u901f\u4e86\u8bad\u7ec3\uff0c\u867d\u7136\u6548\u679c\u6bd4\u7a7a\u95f4\u6df7\u5408\u521d\u59cb\u5316\u5c0f\uff0c\u4f46\u53ef\u4ee5\u4e0e\u5b83\u4eec\u7ed3\u5408\u4f7f\u7528\u83b7\u5f97\u989d\u5916\u589e\u76ca", "conclusion": "\u6210\u529f\u5c06\u6a21\u4eff\u521d\u59cb\u5316\u6269\u5c55\u5230\u901a\u9053\u6df7\u5408\u5c42\uff0c\u4e3aMLP\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u53ef\u4e0e\u73b0\u6709\u7a7a\u95f4\u6df7\u5408\u521d\u59cb\u5316\u6280\u672f\u4e92\u8865\u4f7f\u7528"}}
{"id": "2602.07233", "pdf": "https://arxiv.org/pdf/2602.07233", "abs": "https://arxiv.org/abs/2602.07233", "authors": ["Eric V. Strobl"], "title": "Extracting Root-Causal Brain Activity Driving Psychopathology from Resting State fMRI", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Neuroimaging studies of psychiatric disorders often correlate imaging patterns with diagnostic labels or composite symptom scores, yielding diffuse associations that obscure underlying mechanisms. We instead seek to identify root-causal maps -- localized BOLD disturbances that initiate pathological cascades -- and to link them selectively to symptom dimensions. We introduce a bilevel structural causal model that connects between-subject symptom structure to within-subject resting-state fMRI via independent latent sources with localized direct effects. Based on this model, we develop SOURCE (Symptom-Oriented Uncovering of Root-Causal Elements), a procedure that links interpretable symptom axes to a parsimonious set of localized drivers. Experiments show that SOURCE recovers localized maps consistent with root-causal BOLD drivers and increases interpretability and anatomical specificity relative to existing comparators.", "AI": {"tldr": "SOURCE\u65b9\u6cd5\u901a\u8fc7\u53cc\u5c42\u56e0\u679c\u6a21\u578b\uff0c\u5c06\u75c7\u72b6\u7ef4\u5ea6\u4e0e\u5c40\u90e8BOLD\u6270\u52a8\u76f4\u63a5\u5173\u8054\uff0c\u8bc6\u522b\u6839\u56e0\u6027\u795e\u7ecf\u6d3b\u52a8\u6a21\u5f0f\uff0c\u63d0\u9ad8\u4e86\u89e3\u91ca\u6027\u548c\u89e3\u5256\u7279\u5f02\u6027\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u5f71\u50cf\u7814\u7a76\u901a\u5e38\u5c06\u5f71\u50cf\u6a21\u5f0f\u4e0e\u8bca\u65ad\u6807\u7b7e\u6216\u7efc\u5408\u75c7\u72b6\u8bc4\u5206\u5173\u8054\uff0c\u5bfc\u81f4\u5173\u8054\u6027\u5206\u6563\uff0c\u63a9\u76d6\u4e86\u6f5c\u5728\u7684\u75c5\u7406\u673a\u5236\u3002\u9700\u8981\u8bc6\u522b\u80fd\u591f\u5f15\u53d1\u75c5\u7406\u7ea7\u8054\u7684\u6839\u56e0\u6027BOLD\u6270\u52a8\uff0c\u5e76\u5c06\u5176\u4e0e\u7279\u5b9a\u75c7\u72b6\u7ef4\u5ea6\u9009\u62e9\u6027\u5173\u8054\u3002", "method": "\u63d0\u51fa\u53cc\u5c42\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u901a\u8fc7\u5177\u6709\u5c40\u90e8\u76f4\u63a5\u6548\u5e94\u7684\u72ec\u7acb\u6f5c\u5728\u6e90\uff0c\u5c06\u53d7\u8bd5\u8005\u95f4\u7684\u75c7\u72b6\u7ed3\u6784\u4e0e\u53d7\u8bd5\u8005\u5185\u7684\u9759\u606f\u6001fMRI\u8fde\u63a5\u3002\u57fa\u4e8e\u6b64\u6a21\u578b\u5f00\u53d1SOURCE\u65b9\u6cd5\uff0c\u5c06\u53ef\u89e3\u91ca\u7684\u75c7\u72b6\u8f74\u4e0e\u4e00\u7ec4\u7b80\u7ea6\u7684\u5c40\u90e8\u9a71\u52a8\u56e0\u7d20\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSOURCE\u80fd\u591f\u6062\u590d\u4e0e\u6839\u56e0\u6027BOLD\u9a71\u52a8\u4e00\u81f4\u7684\u5c40\u90e8\u6620\u5c04\u56fe\uff0c\u76f8\u5bf9\u4e8e\u73b0\u6709\u6bd4\u8f83\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u89e3\u5256\u7279\u5f02\u6027\u3002", "conclusion": "SOURCE\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u6839\u56e0\u6027\u795e\u7ecf\u6d3b\u52a8\u6a21\u5f0f\u5e76\u5c06\u5176\u4e0e\u7279\u5b9a\u75c7\u72b6\u7ef4\u5ea6\u5173\u8054\uff0c\u4e3a\u7406\u89e3\u7cbe\u795e\u75be\u75c5\u7684\u795e\u7ecf\u673a\u5236\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u3001\u66f4\u5177\u89e3\u91ca\u6027\u7684\u6846\u67b6\u3002"}}
{"id": "2602.07393", "pdf": "https://arxiv.org/pdf/2602.07393", "abs": "https://arxiv.org/abs/2602.07393", "authors": ["Yang Zhang", "Zhangkai Ni", "Wenhan Yang", "Hanli Wang"], "title": "Wavelet-Domain Masked Image Modeling for Color-Consistent HDR Video Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High Dynamic Range (HDR) video reconstruction aims to recover fine brightness, color, and details from Low Dynamic Range (LDR) videos. However, existing methods often suffer from color inaccuracies and temporal inconsistencies. To address these challenges, we propose WMNet, a novel HDR video reconstruction network that leverages Wavelet domain Masked Image Modeling (W-MIM). WMNet adopts a two-phase training strategy: In Phase I, W-MIM performs self-reconstruction pre-training by selectively masking color and detail information in the wavelet domain, enabling the network to develop robust color restoration capabilities. A curriculum learning scheme further refines the reconstruction process. Phase II fine-tunes the model using the pre-trained weights to improve the final reconstruction quality. To improve temporal consistency, we introduce the Temporal Mixture of Experts (T-MoE) module and the Dynamic Memory Module (DMM). T-MoE adaptively fuses adjacent frames to reduce flickering artifacts, while DMM captures long-range dependencies, ensuring smooth motion and preservation of fine details. Additionally, since existing HDR video datasets lack scene-based segmentation, we reorganize HDRTV4K into HDRTV4K-Scene, establishing a new benchmark for HDR video reconstruction. Extensive experiments demonstrate that WMNet achieves state-of-the-art performance across multiple evaluation metrics, significantly improving color fidelity, temporal coherence, and perceptual quality. The code is available at: https://github.com/eezkni/WMNet", "AI": {"tldr": "WMNet\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u57df\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u7684HDR\u89c6\u9891\u91cd\u5efa\u7f51\u7edc\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7T-MoE\u548cDMM\u6a21\u5757\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u91cd\u6784\u7684HDRTV4K-Scene\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709HDR\u89c6\u9891\u91cd\u5efa\u65b9\u6cd5\u5b58\u5728\u989c\u8272\u4e0d\u51c6\u786e\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6062\u590d\u7cbe\u7ec6\u4eae\u5ea6\u3001\u989c\u8272\u548c\u7ec6\u8282\uff0c\u5e76\u4fdd\u6301\u65f6\u95f4\u8fde\u8d2f\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faWMNet\u7f51\u7edc\uff0c\u91c7\u7528\u5c0f\u6ce2\u57df\u63a9\u7801\u56fe\u50cf\u5efa\u6a21(W-MIM)\u8fdb\u884c\u81ea\u91cd\u5efa\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002\u7b2c\u4e8c\u9636\u6bb5\u5fae\u8c03\u6a21\u578b\u3002\u5f15\u5165T-MoE\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u76f8\u90bb\u5e27\u51cf\u5c11\u95ea\u70c1\uff0cDMM\u6a21\u5757\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u786e\u4fdd\u8fd0\u52a8\u5e73\u6ed1\u3002\u91cd\u6784HDRTV4K\u6570\u636e\u96c6\u4e3aHDRTV4K-Scene\u63d0\u4f9b\u573a\u666f\u5206\u5272\u57fa\u51c6\u3002", "result": "WMNet\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u6539\u5584\u4e86\u989c\u8272\u4fdd\u771f\u5ea6\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "WMNet\u901a\u8fc7\u5c0f\u6ce2\u57df\u63a9\u7801\u5efa\u6a21\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86HDR\u89c6\u9891\u91cd\u5efa\u4e2d\u7684\u989c\u8272\u4e0d\u51c6\u786e\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3aHDR\u89c6\u9891\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07399", "pdf": "https://arxiv.org/pdf/2602.07399", "abs": "https://arxiv.org/abs/2602.07399", "authors": ["Changhua Xu", "Jie Lu", "Junyu Xuan", "En Yu"], "title": "VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation", "categories": ["cs.AI", "cs.CV"], "comment": "Preprint", "summary": "Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \\emph{generation--selection} perspective and propose a novel framework \\textbf{VGAS} (\\textbf{V}alue-\\textbf{G}uided \\textbf{A}ction-chunk \\textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \\textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \\textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \\textit{Explicit Geometric Regularization} (\\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \\textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.", "AI": {"tldr": "VGAS\u6846\u67b6\u901a\u8fc7\u751f\u6210-\u9009\u62e9\u673a\u5236\u89e3\u51b3VLA\u6a21\u578b\u5728\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u4f7f\u7528\u4ef7\u503c\u5f15\u5bfc\u7684\u52a8\u4f5c\u5757\u9009\u62e9\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u5c11\u6837\u672c\u9002\u5e94\u4efb\u52a1\u4e2d\u4e0d\u53ef\u9760\uff0c\u867d\u7136\u80fd\u751f\u6210\u8bed\u4e49\u5408\u7406\u7684\u8f68\u8ff9\uff0c\u4f46\u5e38\u56e0\u51e0\u4f55\u6a21\u7cca\u6027\u5bfc\u81f4\u6267\u884c\u5931\u8d25\uff0c\u9700\u8981\u89e3\u51b3\u6709\u9650\u76d1\u7763\u4e0b\u7684\u51e0\u4f55\u7cbe\u5ea6\u95ee\u9898", "method": "\u63d0\u51faVGAS\u6846\u67b6\uff1a1) \u5fae\u8c03VLA\u4f5c\u4e3a\u9ad8\u53ec\u56de\u7387\u63d0\u8bae\u751f\u6210\u5668\uff1b2) \u5f15\u5165Q-Chunk-Former\u4f5c\u4e3a\u51e0\u4f55\u57fa\u7840\u7684Transformer\u6279\u8bc4\u5668\uff1b3) \u63d0\u51fa\u663e\u5f0f\u51e0\u4f55\u6b63\u5219\u5316(EGR)\u4fdd\u6301\u52a8\u4f5c\u6392\u5e8f\u5206\u8fa8\u7387", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u8868\u660eVGAS\u5728\u6709\u9650\u6f14\u793a\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u80fd\u6301\u7eed\u63d0\u9ad8\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027", "conclusion": "VGAS\u901a\u8fc7\u751f\u6210-\u9009\u62e9\u89c6\u89d2\u89e3\u51b3VLA\u5c11\u6837\u672c\u9002\u5e94\u95ee\u9898\uff0c\u6709\u6548\u5904\u7406\u51e0\u4f55\u6a21\u7cca\u6027\uff0c\u4e3aVLA\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.07403", "pdf": "https://arxiv.org/pdf/2602.07403", "abs": "https://arxiv.org/abs/2602.07403", "authors": ["Yanwei Jiang", "Wei Sun", "Yingjie Zhou", "Xiangyang Zhu", "Yuqin Cao", "Jun Jia", "Yunhao Li", "Sijing Wu", "Dandan Zhu", "Xingkuo Min", "Guangtao Zhai"], "title": "Surveillance Facial Image Quality Assessment: A Multi-dimensional Dataset and Lightweight Model", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Surveillance facial images are often captured under unconstrained conditions, resulting in severe quality degradation due to factors such as low resolution, motion blur, occlusion, and poor lighting. Although recent face restoration techniques applied to surveillance cameras can significantly enhance visual quality, they often compromise fidelity (i.e., identity-preserving features), which directly conflicts with the primary objective of surveillance images -- reliable identity verification. Existing facial image quality assessment (FIQA) predominantly focus on either visual quality or recognition-oriented evaluation, thereby failing to jointly address visual quality and fidelity, which are critical for surveillance applications. To bridge this gap, we propose the first comprehensive study on surveillance facial image quality assessment (SFIQA), targeting the unique challenges inherent to surveillance scenarios. Specifically, we first construct SFIQA-Bench, a multi-dimensional quality assessment benchmark for surveillance facial images, which consists of 5,004 surveillance facial images captured by three widely deployed surveillance cameras in real-world scenarios. A subjective experiment is conducted to collect six dimensional quality ratings, including noise, sharpness, colorfulness, contrast, fidelity and overall quality, covering the key aspects of SFIQA. Furthermore, we propose SFIQA-Assessor, a lightweight multi-task FIQA model that jointly exploits complementary facial views through cross-view feature interaction, and employs learnable task tokens to guide the unified regression of multiple quality dimensions. The experiment results on the proposed dataset show that our method achieves the best performance compared with the state-of-the-art general image quality assessment (IQA) and FIQA methods, validating its effectiveness for real-world surveillance applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u76d1\u63a7\u4eba\u8138\u56fe\u50cf\u7684\u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u5305\u542b5004\u5f20\u771f\u5b9e\u76d1\u63a7\u56fe\u50cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u591a\u4efb\u52a1\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8d28\u91cf\u6216\u8bc6\u522b\u6027\u80fd\uff0c\u4f46\u76d1\u63a7\u573a\u666f\u9700\u8981\u540c\u65f6\u8003\u8651\u89c6\u89c9\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u771f\u5ea6\u3002\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u76d1\u63a7\u4eba\u8138\u56fe\u50cf\u7684\u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\uff0c\u65e0\u6cd5\u6ee1\u8db3\u76d1\u63a7\u5e94\u7528\u4e2d\u53ef\u9760\u8eab\u4efd\u9a8c\u8bc1\u7684\u9700\u6c42\u3002", "method": "1. \u6784\u5efaSFIQA-Bench\u57fa\u51c6\u6570\u636e\u96c6\uff1a\u5305\u542b5004\u5f20\u7531\u4e09\u79cd\u5e38\u7528\u76d1\u63a7\u6444\u50cf\u5934\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6355\u83b7\u7684\u56fe\u50cf\uff0c\u901a\u8fc7\u4e3b\u89c2\u5b9e\u9a8c\u6536\u96c6\u566a\u58f0\u3001\u6e05\u6670\u5ea6\u3001\u8272\u5f69\u4e30\u5bcc\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u4fdd\u771f\u5ea6\u548c\u6574\u4f53\u8d28\u91cf\u516d\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u5206\u3002\n2. \u63d0\u51faSFIQA-Assessor\u6a21\u578b\uff1a\u8f7b\u91cf\u7ea7\u591a\u4efb\u52a1FIQA\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u89c6\u56fe\u7279\u5f81\u4ea4\u4e92\u5229\u7528\u4e92\u8865\u7684\u9762\u90e8\u89c6\u56fe\u4fe1\u606f\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u4efb\u52a1\u4ee4\u724c\u6307\u5bfc\u591a\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u7684\u7edf\u4e00\u56de\u5f52\u3002", "result": "\u5728\u63d0\u51fa\u7684\u6570\u636e\u96c6\u4e0a\uff0cSFIQA-Assessor\u6a21\u578b\u5728\u76d1\u63a7\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u7528\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u548cFIQA\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u76d1\u63a7\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u76d1\u63a7\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6a21\u578b\u80fd\u591f\u540c\u65f6\u8003\u8651\u89c6\u89c9\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u771f\u5ea6\uff0c\u4e3a\u76d1\u63a7\u5e94\u7528\u4e2d\u53ef\u9760\u7684\u8eab\u4efd\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07570", "pdf": "https://arxiv.org/pdf/2602.07570", "abs": "https://arxiv.org/abs/2602.07570", "authors": ["Prachi Jindal", "Anant Khandelwal", "Manish Gupta", "Bapi S. Raju", "Subba Reddy Oota", "Tanmoy Chakraborty"], "title": "How does longer temporal context enhance multimodal narrative video processing in the brain?", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "cs.LG"], "comment": "22 pages, 15 figures", "summary": "Understanding how humans and artificial intelligence systems process complex narrative videos is a fundamental challenge at the intersection of neuroscience and machine learning. This study investigates how the temporal context length of video clips (3--12 s clips) and the narrative-task prompting shape brain-model alignment during naturalistic movie watching. Using fMRI recordings from participants viewing full-length movies, we examine how brain regions sensitive to narrative context dynamically represent information over varying timescales and how these neural patterns align with model-derived features. We find that increasing clip duration substantially improves brain alignment for multimodal large language models (MLLMs), whereas unimodal video models show little to no gain. Further, shorter temporal windows align with perceptual and early language regions, while longer windows preferentially align higher-order integrative regions, mirrored by a layer-to-cortex hierarchy in MLLMs. Finally, narrative-task prompts (multi-scene summary, narrative summary, character motivation, and event boundary detection) elicit task-specific, region-dependent brain alignment patterns and context-dependent shifts in clip-level tuning in higher-order regions. Together, our results position long-form narrative movies as a principled testbed for probing biologically relevant temporal integration and interpretable representations in long-context MLLMs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u89c6\u9891\u7247\u6bb5\u65f6\u957f\uff083-12\u79d2\uff09\u548c\u53d9\u4e8b\u4efb\u52a1\u63d0\u793a\u5982\u4f55\u5f71\u54cd\u5927\u8111\u4e0eAI\u6a21\u578b\u5728\u89c2\u770b\u81ea\u7136\u7535\u5f71\u65f6\u7684\u5bf9\u9f50\uff0c\u53d1\u73b0\u957f\u7247\u6bb5\u80fd\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5927\u8111\u5bf9\u9f50\uff0c\u4e14\u4e0d\u540c\u65f6\u957f\u5bf9\u5e94\u4e0d\u540c\u8111\u533a\uff0c\u53d9\u4e8b\u4efb\u52a1\u63d0\u793a\u80fd\u5f15\u53d1\u7279\u5b9a\u8111\u533a\u5bf9\u9f50\u6a21\u5f0f\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u548cAI\u7cfb\u7edf\u5982\u4f55\u5904\u7406\u590d\u6742\u53d9\u4e8b\u89c6\u9891\u662f\u795e\u7ecf\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u7684\u4ea4\u53c9\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u89c6\u9891\u7247\u6bb5\u65f6\u957f\u548c\u53d9\u4e8b\u4efb\u52a1\u63d0\u793a\u5982\u4f55\u5f71\u54cd\u5927\u8111\u4e0e\u6a21\u578b\u5728\u81ea\u7136\u7535\u5f71\u89c2\u770b\u8fc7\u7a0b\u4e2d\u7684\u5bf9\u9f50\uff0c\u4e3a\u7814\u7a76\u751f\u7269\u76f8\u5173\u7684\u65f6\u95f4\u6574\u5408\u548c\u53ef\u89e3\u91ca\u8868\u5f81\u63d0\u4f9b\u6d4b\u8bd5\u5e73\u53f0\u3002", "method": "\u4f7f\u7528fMRI\u8bb0\u5f55\u53c2\u4e0e\u8005\u89c2\u770b\u5b8c\u6574\u7535\u5f71\u65f6\u7684\u8111\u6d3b\u52a8\uff0c\u5206\u6790\u4e0d\u540c\u65f6\u957f\u89c6\u9891\u7247\u6bb5\uff083-12\u79d2\uff09\u548c\u4e0d\u540c\u53d9\u4e8b\u4efb\u52a1\u63d0\u793a\uff08\u591a\u573a\u666f\u6458\u8981\u3001\u53d9\u4e8b\u6458\u8981\u3001\u89d2\u8272\u52a8\u673a\u3001\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\uff09\u4e0b\uff0c\u5927\u8111\u4e0e\u6a21\u578b\u7279\u5f81\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u7279\u522b\u5173\u6ce8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5355\u6a21\u6001\u89c6\u9891\u6a21\u578b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u589e\u52a0\u89c6\u9891\u7247\u6bb5\u65f6\u957f\u80fd\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5927\u8111\u5bf9\u9f50\uff0c\u4f46\u5bf9\u5355\u6a21\u6001\u89c6\u9891\u6a21\u578b\u5f71\u54cd\u5f88\u5c0f\uff1b\u77ed\u65f6\u95f4\u7a97\u53e3\u4e0e\u611f\u77e5\u548c\u65e9\u671f\u8bed\u8a00\u533a\u57df\u5bf9\u9f50\uff0c\u957f\u65f6\u95f4\u7a97\u53e3\u4e0e\u9ad8\u9636\u6574\u5408\u533a\u57df\u5bf9\u9f50\uff1b\u4e0d\u540c\u53d9\u4e8b\u4efb\u52a1\u63d0\u793a\u80fd\u5f15\u53d1\u4efb\u52a1\u7279\u5f02\u6027\u3001\u533a\u57df\u4f9d\u8d56\u7684\u5927\u8111\u5bf9\u9f50\u6a21\u5f0f\uff0c\u5e76\u5728\u9ad8\u9636\u533a\u57df\u5f15\u8d77\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u7247\u6bb5\u7ea7\u8c03\u8c10\u53d8\u5316\u3002", "conclusion": "\u957f\u5f62\u5f0f\u53d9\u4e8b\u7535\u5f71\u53ef\u4f5c\u4e3a\u7814\u7a76\u751f\u7269\u76f8\u5173\u65f6\u95f4\u6574\u5408\u548c\u957f\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u89e3\u91ca\u8868\u5f81\u7684\u539f\u5219\u6027\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u5927\u8111\u4e0eAI\u6a21\u578b\u5728\u590d\u6742\u53d9\u4e8b\u5904\u7406\u4e2d\u7684\u52a8\u6001\u5bf9\u9f50\u673a\u5236\u3002"}}
{"id": "2602.07736", "pdf": "https://arxiv.org/pdf/2602.07736", "abs": "https://arxiv.org/abs/2602.07736", "authors": ["Omar Tahri"], "title": "Global Symmetry and Orthogonal Transformations from Geometrical Moment $n$-tuples", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Detecting symmetry is crucial for effective object grasping for several reasons. Recognizing symmetrical features or axes within an object helps in developing efficient grasp strategies, as grasping along these axes typically results in a more stable and balanced grip, thereby facilitating successful manipulation. This paper employs geometrical moments to identify symmetries and estimate orthogonal transformations, including rotations and mirror transformations, for objects centered at the frame origin. It provides distinctive metrics for detecting symmetries and estimating orthogonal transformations, encompassing rotations, reflections, and their combinations. A comprehensive methodology is developed to obtain these functions in n-dimensional space, specifically moment \\( n \\)-tuples. Extensive validation tests are conducted on both 2D and 3D objects to ensure the robustness and reliability of the proposed approach. The proposed method is also compared to state-of-the-art work using iterative optimization for detecting multiple planes of symmetry. The results indicate that combining our method with the iterative one yields satisfactory outcomes in terms of the number of symmetry planes detected and computation time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u51e0\u4f55\u77e9\u68c0\u6d4b\u7269\u4f53\u5bf9\u79f0\u6027\u5e76\u4f30\u8ba1\u6b63\u4ea4\u53d8\u6362\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e9n\u5143\u7ec4\u5728n\u7ef4\u7a7a\u95f4\u4e2d\u8bc6\u522b\u65cb\u8f6c\u3001\u53cd\u5c04\u7b49\u5bf9\u79f0\u53d8\u6362\uff0c\u5e76\u4e0e\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u7ed3\u5408\u83b7\u5f97\u66f4\u597d\u7684\u5bf9\u79f0\u5e73\u9762\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u68c0\u6d4b\u5bf9\u79f0\u6027\u5bf9\u4e8e\u7269\u4f53\u6293\u53d6\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u8bc6\u522b\u7269\u4f53\u7684\u5bf9\u79f0\u7279\u5f81\u6216\u8f74\u7ebf\u6709\u52a9\u4e8e\u5236\u5b9a\u9ad8\u6548\u7684\u6293\u53d6\u7b56\u7565\uff0c\u6cbf\u7740\u8fd9\u4e9b\u8f74\u7ebf\u6293\u53d6\u901a\u5e38\u80fd\u83b7\u5f97\u66f4\u7a33\u5b9a\u5e73\u8861\u7684\u63e1\u6301\uff0c\u4ece\u800c\u4fc3\u8fdb\u6210\u529f\u7684\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u77e9\u6765\u8bc6\u522b\u5bf9\u79f0\u6027\u5e76\u4f30\u8ba1\u6b63\u4ea4\u53d8\u6362\uff08\u5305\u62ec\u65cb\u8f6c\u548c\u955c\u50cf\u53d8\u6362\uff09\uff0c\u4e3a\u4ee5\u5750\u6807\u7cfb\u539f\u70b9\u4e3a\u4e2d\u5fc3\u7684\u7269\u4f53\u63d0\u4f9b\u72ec\u7279\u7684\u5bf9\u79f0\u6027\u68c0\u6d4b\u548c\u6b63\u4ea4\u53d8\u6362\u4f30\u8ba1\u5ea6\u91cf\u3002\u5f00\u53d1\u4e86\u5728n\u7ef4\u7a7a\u95f4\u4e2d\u83b7\u53d6\u8fd9\u4e9b\u51fd\u6570\uff08\u77e9n\u5143\u7ec4\uff09\u7684\u5168\u9762\u65b9\u6cd5\u3002", "result": "\u57282D\u548c3D\u7269\u4f53\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u9a8c\u8bc1\u6d4b\u8bd5\uff0c\u786e\u4fdd\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002\u4e0e\u4f7f\u7528\u8fed\u4ee3\u4f18\u5316\u68c0\u6d4b\u591a\u4e2a\u5bf9\u79f0\u5e73\u9762\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u5c06\u672c\u6587\u65b9\u6cd5\u4e0e\u8fed\u4ee3\u65b9\u6cd5\u7ed3\u5408\u5728\u68c0\u6d4b\u5230\u7684\u5bf9\u79f0\u5e73\u9762\u6570\u91cf\u548c\u8ba1\u7b97\u65f6\u95f4\u65b9\u9762\u90fd\u80fd\u83b7\u5f97\u6ee1\u610f\u7ed3\u679c\u3002", "conclusion": "\u57fa\u4e8e\u51e0\u4f55\u77e9\u7684\u5bf9\u79f0\u6027\u68c0\u6d4b\u65b9\u6cd5\u6709\u6548\uff0c\u4e0e\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u7ed3\u5408\u80fd\u591f\u63d0\u9ad8\u5bf9\u79f0\u5e73\u9762\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u7269\u4f53\u6293\u53d6\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5bf9\u79f0\u6027\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2602.07819", "pdf": "https://arxiv.org/pdf/2602.07819", "abs": "https://arxiv.org/abs/2602.07819", "authors": ["Xinyu Liu", "Guolei Sun"], "title": "DINO-Mix: Distilling Foundational Knowledge with Cross-Domain CutMix for Semi-supervised Class-imbalanced Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "AAAI 2026 Workshop on Artificial Intelligence with Biased or Scarce Data (Oral)", "summary": "Semi-supervised learning (SSL) has emerged as a critical paradigm for medical image segmentation, mitigating the immense cost of dense annotations. However, prevailing SSL frameworks are fundamentally \"inward-looking\", recycling information and biases solely from within the target dataset. This design triggers a vicious cycle of confirmation bias under class imbalance, leading to the catastrophic failure to recognize minority classes. To dismantle this systemic issue, we propose a paradigm shift to a multi-level \"outward-looking\" framework. Our primary innovation is Foundational Knowledge Distillation (FKD), which looks outward beyond the confines of medical imaging by introducing a pre-trained visual foundation model, DINOv3, as an unbiased external semantic teacher. Instead of trusting the student's biased high confidence, our method distills knowledge from DINOv3's robust understanding of high semantic uniqueness, providing a stable, cross-domain supervisory signal that anchors the learning of minority classes. To complement this core strategy, we further look outward within the data by proposing Progressive Imbalance-aware CutMix (PIC), which creates a dynamic curriculum that adaptively forces the model to focus on minority classes in both labeled and unlabeled subsets. This layered strategy forms our framework, DINO-Mix, which breaks the vicious cycle of bias and achieves remarkable performance on challenging semi-supervised class-imbalanced medical image segmentation benchmarks Synapse and AMOS.", "AI": {"tldr": "\u63d0\u51faDINO-Mix\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u89c6\u89c9\u57fa\u7840\u6a21\u578bDINOv3\u7684\u77e5\u8bc6\u84b8\u998f\u548c\u6e10\u8fdb\u5f0f\u7c7b\u522b\u4e0d\u5e73\u8861\u611f\u77e5\u6570\u636e\u589e\u5f3a\uff0c\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u534a\u76d1\u7763\u5b66\u4e60\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u4e0b\u7684\u786e\u8ba4\u504f\u8bef\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u662f\"\u5185\u5411\u578b\"\u7684\uff0c\u4ec5\u4ece\u76ee\u6807\u6570\u636e\u96c6\u5185\u90e8\u5faa\u73af\u5229\u7528\u4fe1\u606f\u548c\u504f\u8bef\uff0c\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u4f1a\u9677\u5165\u786e\u8ba4\u504f\u8bef\u7684\u6076\u6027\u5faa\u73af\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8bc6\u522b\u5c11\u6570\u7c7b\u522b\u3002\u9700\u8981\u6253\u7834\u8fd9\u79cd\u7cfb\u7edf\u6027\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u591a\u5c42\u6b21\"\u5916\u5411\u578b\"\u6846\u67b6DINO-Mix\uff1a1) \u57fa\u7840\u77e5\u8bc6\u84b8\u998f(FKD)\uff1a\u5f15\u5165\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578bDINOv3\u4f5c\u4e3a\u65e0\u504f\u5916\u90e8\u8bed\u4e49\u6559\u5e08\uff0c\u84b8\u998f\u5176\u9ad8\u8bed\u4e49\u72ec\u7279\u6027\u7684\u7406\u89e3\uff1b2) \u6e10\u8fdb\u5f0f\u4e0d\u5e73\u8861\u611f\u77e5CutMix(PIC)\uff1a\u521b\u5efa\u52a8\u6001\u8bfe\u7a0b\uff0c\u81ea\u9002\u5e94\u5730\u8feb\u4f7f\u6a21\u578b\u5173\u6ce8\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u5b50\u96c6\u4e2d\u7684\u5c11\u6570\u7c7b\u522b\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u534a\u76d1\u7763\u7c7b\u522b\u4e0d\u5e73\u8861\u533b\u5b66\u56fe\u50cf\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5Synapse\u548cAMOS\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u6253\u7834\u4e86\u504f\u8bef\u7684\u6076\u6027\u5faa\u73af\u3002", "conclusion": "\u901a\u8fc7\u5411\u5916\u5bfb\u627e\u5916\u90e8\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\u548c\u6570\u636e\u5185\u90e8\u589e\u5f3a\u7b56\u7565\uff0cDINO-Mix\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2602.07888", "pdf": "https://arxiv.org/pdf/2602.07888", "abs": "https://arxiv.org/abs/2602.07888", "authors": ["Ning Hu", "Shuai Li", "Jindong Tan"], "title": "Research on a Camera Position Measurement Method based on a Parallel Perspective Error Transfer Model", "categories": ["cs.RO", "cs.CV"], "comment": "32 pages, 19 figures", "summary": "Camera pose estimation from sparse correspondences is a fundamental problem in geometric computer vision and remains particularly challenging in near-field scenarios, where strong perspective effects and heterogeneous measurement noise can significantly degrade the stability of analytic PnP solutions. In this paper, we present a geometric error propagation framework for camera pose estimation based on a parallel perspective approximation. By explicitly modeling how image measurement errors propagate through perspective geometry, we derive an error transfer model that characterizes the relationship between feature point distribution, camera depth, and pose estimation uncertainty. Building on this analysis, we develop a pose estimation method that leverages parallel perspective initialization and error-aware weighting within a Gauss-Newton optimization scheme, leading to improved robustness in proximity operations. Extensive experiments on both synthetic data and real-world images, covering diverse conditions such as strong illumination, surgical lighting, and underwater low-light environments, demonstrate that the proposed approach achieves accuracy and robustness comparable to state-of-the-art analytic and iterative PnP methods, while maintaining high computational efficiency. These results highlight the importance of explicit geometric error modeling for reliable camera pose estimation in challenging near-field settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5e73\u884c\u900f\u89c6\u8fd1\u4f3c\u7684\u51e0\u4f55\u8bef\u5dee\u4f20\u64ad\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u5584\u8fd1\u573a\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u8bef\u5dee\u611f\u77e5\u52a0\u6743\u548cGauss-Newton\u4f18\u5316\u63d0\u5347\u7cbe\u5ea6", "motivation": "\u8fd1\u573a\u573a\u666f\u4e2d\uff0c\u5f3a\u70c8\u7684\u900f\u89c6\u6548\u5e94\u548c\u5f02\u6784\u6d4b\u91cf\u566a\u58f0\u4f1a\u663e\u8457\u964d\u4f4e\u4f20\u7edfPnP\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5", "method": "\u5efa\u7acb\u51e0\u4f55\u8bef\u5dee\u4f20\u64ad\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u884c\u900f\u89c6\u8fd1\u4f3c\u663e\u5f0f\u5efa\u6a21\u56fe\u50cf\u6d4b\u91cf\u8bef\u5dee\u5728\u900f\u89c6\u51e0\u4f55\u4e2d\u7684\u4f20\u64ad\uff0c\u63a8\u5bfc\u8bef\u5dee\u4f20\u9012\u6a21\u578b\uff0c\u7ed3\u5408\u5e73\u884c\u900f\u89c6\u521d\u59cb\u5316\u548c\u8bef\u5dee\u611f\u77e5\u52a0\u6743\u7684Gauss-Newton\u4f18\u5316", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u56fe\u50cf\uff08\u5f3a\u5149\u7167\u3001\u624b\u672f\u7167\u660e\u3001\u6c34\u4e0b\u4f4e\u5149\u7b49\u591a\u6837\u6761\u4ef6\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u89e3\u6790\u548c\u8fed\u4ee3PnP\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027", "conclusion": "\u663e\u5f0f\u51e0\u4f55\u8bef\u5dee\u5efa\u6a21\u5bf9\u4e8e\u6311\u6218\u6027\u8fd1\u573a\u8bbe\u7f6e\u4e2d\u7684\u53ef\u9760\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u8fd1\u573a\u64cd\u4f5c\u7684\u9c81\u68d2\u6027"}}
{"id": "2602.07919", "pdf": "https://arxiv.org/pdf/2602.07919", "abs": "https://arxiv.org/abs/2602.07919", "authors": ["Mansi", "Avinash Kori", "Francesca Toni", "Soteris Demetriou"], "title": "Selective Fine-Tuning for Targeted and Robust Concept Unlearning", "categories": ["cs.AI", "cs.CV"], "comment": "Given the brittle nature of existing methods in unlearning harmful content in diffusion models, we propose TRuST, a novel approach for dynamically estimating target concept neurons and unlearning them by selectively fine-tuning", "summary": "Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.", "AI": {"tldr": "TRUST\u662f\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5b9a\u4f4d\u76ee\u6807\u6982\u5ff5\u795e\u7ecf\u5143\u5e76\u8fdb\u884c\u9009\u62e9\u6027\u5fae\u8c03\uff0c\u7ed3\u5408Hessian\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u6982\u5ff5\u9057\u5fd8\u3002", "motivation": "\u6587\u672c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u6613\u88ab\u5229\u7528\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u73b0\u6709\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a1) \u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u6982\u5ff5\uff0c\u800c\u73b0\u5b9e\u9700\u8981\u5904\u7406\u6982\u5ff5\u7ec4\u5408\uff1b2) \u6700\u5148\u8fdb\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u5168\u5fae\u8c03\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b3) \u73b0\u6709\u6982\u5ff5\u5b9a\u4f4d\u65b9\u6cd5\u662f\u9759\u6001\u7684\uff0c\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faTRUST\u65b9\u6cd5\uff1a1) \u52a8\u6001\u4f30\u8ba1\u76ee\u6807\u6982\u5ff5\u795e\u7ecf\u5143\uff1b2) \u901a\u8fc7\u9009\u62e9\u6027\u5fae\u8c03\u8fdb\u884c\u6982\u5ff5\u9057\u5fd8\uff1b3) \u91c7\u7528\u57fa\u4e8eHessian\u7684\u6b63\u5219\u5316\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTRUST\uff1a1) \u5bf9\u5bf9\u6297\u6027\u63d0\u793a\u5177\u6709\u9c81\u68d2\u6027\uff1b2) \u663e\u8457\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff1b3) \u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u663e\u8457\u66f4\u5feb\uff1b4) \u80fd\u591f\u9057\u5fd8\u5355\u4e2a\u6982\u5ff5\u3001\u6982\u5ff5\u7ec4\u5408\u548c\u6761\u4ef6\u6982\u5ff5\uff0c\u65e0\u9700\u7279\u5b9a\u6b63\u5219\u5316\u3002", "conclusion": "TRUST\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u6982\u5ff5\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u6982\u5ff5\u7ec4\u5408\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.08029", "pdf": "https://arxiv.org/pdf/2602.08029", "abs": "https://arxiv.org/abs/2602.08029", "authors": ["Berthy T. Feng", "Andrew A. Chael", "David Bromley", "Aviad Levis", "William T. Freeman", "Katherine L. Bouman"], "title": "Dynamic Black-hole Emission Tomography with Physics-informed Neural Fields", "categories": ["gr-qc", "astro-ph.IM", "cs.CV"], "comment": null, "summary": "With the success of static black-hole imaging, the next frontier is the dynamic and 3D imaging of black holes. Recovering the dynamic 3D gas near a black hole would reveal previously-unseen parts of the universe and inform new physics models. However, only sparse radio measurements from a single viewpoint are possible, making the dynamic 3D reconstruction problem significantly ill-posed. Previously, BH-NeRF addressed the ill-posed problem by assuming Keplerian dynamics of the gas, but this assumption breaks down near the black hole, where the strong gravitational pull of the black hole and increased electromagnetic activity complicate fluid dynamics. To overcome the restrictive assumptions of BH-NeRF, we propose PI-DEF, a physics-informed approach that uses differentiable neural rendering to fit a 4D (time + 3D) emissivity field given EHT measurements. Our approach jointly reconstructs the 3D velocity field with the 4D emissivity field and enforces the velocity as a soft constraint on the dynamics of the emissivity. In experiments on simulated data, we find significantly improved reconstruction accuracy over both BH-NeRF and a physics-agnostic approach. We demonstrate how our method may be used to estimate other physics parameters of the black hole, such as its spin.", "AI": {"tldr": "PI-DEF\uff1a\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u5316\u7684\u53ef\u5fae\u5206\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u5c04\u7535\u6d4b\u91cf\u4e2d\u91cd\u5efa\u9ed1\u6d1e\u5468\u56f4\u7684\u52a8\u60013D\u6c14\u4f53\u5206\u5e03\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740\u9759\u6001\u9ed1\u6d1e\u6210\u50cf\u7684\u6210\u529f\uff0c\u4e0b\u4e00\u4e2a\u524d\u6cbf\u662f\u9ed1\u6d1e\u7684\u52a8\u60013D\u6210\u50cf\u3002\u6062\u590d\u9ed1\u6d1e\u9644\u8fd1\u7684\u52a8\u60013D\u6c14\u4f53\u5206\u5e03\u5c06\u63ed\u793a\u5b87\u5b99\u4e2d\u524d\u6240\u672a\u89c1\u7684\u90e8\u5206\u5e76\u6307\u5bfc\u65b0\u7269\u7406\u6a21\u578b\u3002\u7136\u800c\uff0c\u53ea\u80fd\u4ece\u5355\u4e00\u89c6\u89d2\u83b7\u53d6\u7a00\u758f\u7684\u5c04\u7535\u6d4b\u91cf\u6570\u636e\uff0c\u4f7f\u5f97\u52a8\u60013D\u91cd\u5efa\u95ee\u9898\u4e25\u91cd\u4e0d\u9002\u5b9a\u3002\u4e4b\u524d\u7684BH-NeRF\u65b9\u6cd5\u5047\u8bbe\u6c14\u4f53\u9075\u5faa\u5f00\u666e\u52d2\u52a8\u529b\u5b66\uff0c\u4f46\u8fd9\u4e2a\u5047\u8bbe\u5728\u9ed1\u6d1e\u9644\u8fd1\u5931\u6548\uff0c\u56e0\u4e3a\u5f3a\u5f15\u529b\u573a\u548c\u7535\u78c1\u6d3b\u52a8\u4f7f\u6d41\u4f53\u52a8\u529b\u5b66\u53d8\u5f97\u590d\u6742\u3002", "method": "\u63d0\u51faPI-DEF\u65b9\u6cd5\uff0c\u91c7\u7528\u7269\u7406\u4fe1\u606f\u5316\u7684\u53ef\u5fae\u5206\u795e\u7ecf\u6e32\u67d3\u6280\u672f\uff0c\u7ed9\u5b9aEHT\u6d4b\u91cf\u6570\u636e\u62df\u54084D\uff08\u65f6\u95f4+3D\uff09\u53d1\u5c04\u7387\u573a\u3002\u8be5\u65b9\u6cd5\u8054\u5408\u91cd\u5efa3D\u901f\u5ea6\u573a\u548c4D\u53d1\u5c04\u7387\u573a\uff0c\u5e76\u5c06\u901f\u5ea6\u4f5c\u4e3a\u53d1\u5c04\u7387\u52a8\u529b\u5b66\u7684\u8f6f\u7ea6\u675f\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4BH-NeRF\u548c\u7269\u7406\u65e0\u5173\u65b9\u6cd5\uff0cPI-DEF\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002\u8be5\u65b9\u6cd5\u8fd8\u53ef\u7528\u4e8e\u4f30\u8ba1\u9ed1\u6d1e\u7684\u5176\u4ed6\u7269\u7406\u53c2\u6570\uff0c\u5982\u81ea\u65cb\u3002", "conclusion": "PI-DEF\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u7684\u53ef\u5fae\u5206\u795e\u7ecf\u6e32\u67d3\uff0c\u514b\u670d\u4e86\u5148\u524d\u65b9\u6cd5\u5bf9\u6c14\u4f53\u52a8\u529b\u5b66\u7684\u9650\u5236\u6027\u5047\u8bbe\uff0c\u4e3a\u9ed1\u6d1e\u9644\u8fd1\u7684\u52a8\u60013D\u6210\u50cf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u53d6\u9ed1\u6d1e\u7684\u7269\u7406\u53c2\u6570\u3002"}}
{"id": "2602.08145", "pdf": "https://arxiv.org/pdf/2602.08145", "abs": "https://arxiv.org/abs/2602.08145", "authors": ["Xinyu Yang", "Junlin Han", "Rishi Bommasani", "Jinqi Luo", "Wenjie Qu", "Wangchunshu Zhou", "Adel Bibi", "Xiyao Wang", "Jaehong Yoon", "Elias Stengel-Eskin", "Shengbang Tong", "Lingfeng Shen", "Rafael Rafailov", "Runjia Li", "Zhaoyang Wang", "Yiyang Zhou", "Chenhang Cui", "Yu Wang", "Wenhao Zheng", "Huichi Zhou", "Jindong Gu", "Zhaorun Chen", "Peng Xia", "Tony Lee", "Thomas Zollo", "Vikash Sehwag", "Jixuan Leng", "Jiuhai Chen", "Yuxin Wen", "Huan Zhang", "Zhun Deng", "Linjun Zhang", "Pavel Izmailov", "Pang Wei Koh", "Yulia Tsvetkov", "Andrew Wilson", "Jiaheng Zhang", "James Zou", "Cihang Xie", "Hao Wang", "Philip Torr", "Julian McAuley", "David Alvarez-Melis", "Florian Tram\u00e8r", "Kaidi Xu", "Suman Jana", "Chris Callison-Burch", "Rene Vidal", "Filippos Kokkinos", "Mohit Bansal", "Beidi Chen", "Huaxiu Yao"], "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "comment": "TMLR camera-ready version", "summary": "Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u57fa\u7840\u6a21\u578b\u53ef\u9760\u6027\u4e0e\u8d23\u4efb\u6027\u7684\u7efc\u8ff0\uff0c\u6db5\u76d6\u504f\u89c1\u516c\u5e73\u3001\u5b89\u5168\u9690\u79c1\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u5206\u5e03\u504f\u79fb\u7b49\u5173\u952e\u95ee\u9898\uff0c\u4ee5\u53ca\u5e7b\u89c9\u3001\u5bf9\u9f50\u3001AIGC\u68c0\u6d4b\u7b49\u65b9\u6cd5\u548c\u6311\u6218\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\uff08LLMs\u3001MLLMs\u3001\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3001\u89c6\u9891\u751f\u6210\u6a21\u578b\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u548c\u8d23\u4efb\u6027\u5df2\u6210\u4e3a\u5b66\u672f\u754c\u3001\u5de5\u4e1a\u754c\u548c\u653f\u5e9c\u7684\u5173\u952e\u9700\u6c42\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u6559\u80b2\u3001\u91d1\u878d\u3001\u79d1\u5b66\u7b49\u9886\u57df\u7684\u90e8\u7f72\u9700\u8981\u89e3\u51b3\u4f26\u7406\u3001\u4fe1\u4efb\u548c\u793e\u4f1a\u8d23\u4efb\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u57fa\u7840\u6a21\u578b\u53ef\u9760\u6027\u4e0e\u8d23\u4efb\u6027\u7684\u591a\u4e2a\u5173\u952e\u9886\u57df\uff1a\u504f\u89c1\u4e0e\u516c\u5e73\u3001\u5b89\u5168\u4e0e\u9690\u79c1\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u5206\u5e03\u504f\u79fb\u3002\u540c\u65f6\u7814\u7a76\u6a21\u578b\u5c40\u9650\u6027\uff08\u5982\u5e7b\u89c9\uff09\u4ee5\u53ca\u5bf9\u9f50\u548cAIGC\u68c0\u6d4b\u7b49\u65b9\u6cd5\u3002\u5bf9\u6bcf\u4e2a\u9886\u57df\u8fdb\u884c\u73b0\u72b6\u68b3\u7406\u5e76\u89c4\u5212\u5177\u4f53\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u57fa\u7840\u6a21\u578b\u53ef\u9760\u6027\u4e0e\u8d23\u4efb\u6027\u7814\u7a76\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u73b0\u72b6\u548c\u672a\u6765\u65b9\u5411\u3002\u901a\u8fc7\u5206\u6790\u5404\u9886\u57df\u4e4b\u95f4\u7684\u4ea4\u53c9\u8054\u7cfb\uff0c\u63ed\u793a\u4e86\u5171\u4eab\u6311\u6218\u548c\u76f8\u4e92\u5173\u8054\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u8d1f\u8d23\u4efb\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u65e8\u5728\u4fc3\u8fdb\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u4e0d\u4ec5\u8ffd\u6c42\u5f3a\u5927\u6027\u80fd\uff0c\u66f4\u8981\u786e\u4fdd\u5176\u7b26\u5408\u4f26\u7406\u3001\u53ef\u4fe1\u8d56\u3001\u53ef\u9760\u4e14\u5bf9\u793e\u4f1a\u8d1f\u8d23\u3002\u901a\u8fc7\u7cfb\u7edf\u68b3\u7406\u5173\u952e\u95ee\u9898\u548c\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u6784\u5efa\u66f4\u8d1f\u8d23\u4efb\u7684\u4eba\u5de5\u667a\u80fd\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2602.08167", "pdf": "https://arxiv.org/pdf/2602.08167", "abs": "https://arxiv.org/abs/2602.08167", "authors": ["Milan Ganai", "Katie Luo", "Jonas Frey", "Clark Barrett", "Marco Pavone"], "title": "Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.", "AI": {"tldr": "R&B-EnCoRe\uff1a\u901a\u8fc7\u81ea\u76d1\u7763\u7cbe\u70bc\u4ece\u4e92\u8054\u7f51\u89c4\u6a21\u77e5\u8bc6\u4e2d\u5f15\u5bfc\u5177\u8eab\u63a8\u7406\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6216\u4eba\u5de5\u6807\u6ce8\uff0c\u5728\u591a\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5177\u8eab\u601d\u7ef4\u94fe\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6a21\u677f\u6307\u5b9a\u63a8\u7406\u539f\u8bed\uff0c\u8fd9\u4f1a\u5f3a\u5236\u7b56\u7565\u5904\u7406\u65e0\u5173\u4fe1\u606f\uff0c\u5206\u6563\u5173\u952e\u52a8\u4f5c\u9884\u6d4b\u4fe1\u53f7\u3002\u8fd9\u5f62\u6210\u4e86\u4e00\u4e2a\u74f6\u9888\uff1a\u6ca1\u6709\u6210\u529f\u7684\u7b56\u7565\u5c31\u65e0\u6cd5\u9a8c\u8bc1\u63a8\u7406\u8d28\u91cf\uff0c\u6ca1\u6709\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u5c31\u65e0\u6cd5\u6784\u5efa\u9c81\u68d2\u7684\u7b56\u7565\u3002", "method": "\u5c06\u63a8\u7406\u89c6\u4e3a\u91cd\u8981\u6027\u52a0\u6743\u53d8\u5206\u63a8\u65ad\u4e2d\u7684\u9690\u53d8\u91cf\uff0c\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u5e76\u7cbe\u70bc\u4e00\u4e2a\u5177\u8eab\u7279\u5b9a\u7b56\u7565\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u3001\u9a8c\u8bc1\u5668\u6216\u4eba\u5de5\u6807\u6ce8\u3002\u901a\u8fc7\u81ea\u76d1\u7763\u7cbe\u70bc\u4ece\u4e92\u8054\u7f51\u89c4\u6a21\u77e5\u8bc6\u4e2d\u5f15\u5bfc\u5177\u8eab\u63a8\u7406\u3002", "result": "\u5728\u64cd\u4f5c\u4efb\u52a1\uff08\u4eff\u771f\u548c\u786c\u4ef6\uff09\u3001\u817f\u90e8\u5bfc\u822a\uff08\u53cc\u8db3\u3001\u8f6e\u5f0f\u3001\u81ea\u884c\u8f66\u3001\u56db\u8db3\uff09\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u591a\u79cd\u5177\u8eab\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u4f7f\u75281B\u523030B\u53c2\u6570\u7684VLA\u67b6\u6784\u3002\u76f8\u6bd4\u65e0\u5dee\u522b\u63a8\u7406\u6240\u6709\u53ef\u7528\u539f\u8bed\u7684\u6a21\u578b\uff0c\u5b9e\u73b0\u4e8628%\u7684\u64cd\u4f5c\u6210\u529f\u7387\u63d0\u5347\u3001101%\u7684\u5bfc\u822a\u5206\u6570\u6539\u8fdb\u548c21%\u7684\u78b0\u649e\u7387\u964d\u4f4e\u3002", "conclusion": "R&B-EnCoRe\u4f7f\u6a21\u578b\u80fd\u591f\u7cbe\u70bc\u51fa\u5bf9\u6210\u529f\u63a7\u5236\u5177\u6709\u9884\u6d4b\u6027\u7684\u63a8\u7406\uff0c\u7ed5\u8fc7\u624b\u52a8\u6807\u6ce8\u5de5\u7a0b\uff0c\u540c\u65f6\u5c06\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u77e5\u8bc6\u624e\u6839\u4e8e\u7269\u7406\u6267\u884c\u4e2d\u3002"}}
{"id": "2602.08189", "pdf": "https://arxiv.org/pdf/2602.08189", "abs": "https://arxiv.org/abs/2602.08189", "authors": ["Seoyeon Jang", "Alex Junho Lee", "I Made Aswin Nahrendra", "Hyun Myung"], "title": "Chamelion: Reliable Change Detection for Long-Term LiDAR Mapping in Transient Environments", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, IEEE Robot. Automat. Lett. (RA-L) 2026", "summary": "Online change detection is crucial for mobile robots to efficiently navigate through dynamic environments. Detecting changes in transient settings, such as active construction sites or frequently reconfigured indoor spaces, is particularly challenging due to frequent occlusions and spatiotemporal variations. Existing approaches often struggle to detect changes and fail to update the map across different observations. To address these limitations, we propose a dual-head network designed for online change detection and long-term map maintenance. A key difficulty in this task is the collection and alignment of real-world data, as manually registering structural differences over time is both labor-intensive and often impractical. To overcome this, we develop a data augmentation strategy that synthesizes structural changes by importing elements from different scenes, enabling effective model training without the need for extensive ground-truth annotations. Experiments conducted at real-world construction sites and in indoor office environments demonstrate that our approach generalizes well across diverse scenarios, achieving efficient and accurate map updates.\\resubmit{Our source code and additional material are available at: https://chamelion-pages.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\u548c\u957f\u671f\u5730\u56fe\u7ef4\u62a4\u7684\u53cc\u5934\u7f51\u7edc\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u7b56\u7565\u5408\u6210\u7ed3\u6784\u53d8\u5316\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u5efa\u7b51\u5de5\u5730\u548c\u5ba4\u5185\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bfc\u822a\u9700\u8981\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\uff0c\u4f46\u5728\u77ac\u6001\u73af\u5883\uff08\u5982\u5efa\u7b51\u5de5\u5730\u3001\u9891\u7e41\u91cd\u65b0\u914d\u7f6e\u7684\u5ba4\u5185\u7a7a\u95f4\uff09\u4e2d\uff0c\u7531\u4e8e\u9891\u7e41\u906e\u6321\u548c\u65f6\u7a7a\u53d8\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u53d8\u5316\u5e76\u66f4\u65b0\u5730\u56fe\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53cc\u5934\u7f51\u7edc\u7528\u4e8e\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\u548c\u957f\u671f\u5730\u56fe\u7ef4\u62a4\uff0c\u5f00\u53d1\u6570\u636e\u589e\u5f3a\u7b56\u7565\u901a\u8fc7\u4ece\u4e0d\u540c\u573a\u666f\u5bfc\u5165\u5143\u7d20\u6765\u5408\u6210\u7ed3\u6784\u53d8\u5316\uff0c\u65e0\u9700\u5927\u91cf\u771f\u5b9e\u6807\u6ce8\u5373\u53ef\u6709\u6548\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u5efa\u7b51\u5de5\u5730\u548c\u5ba4\u5185\u529e\u516c\u73af\u5883\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u6cdb\u5316\u826f\u597d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u5730\u56fe\u66f4\u65b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\u548c\u5730\u56fe\u7ef4\u62a4\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u7b56\u7565\u514b\u670d\u4e86\u771f\u5b9e\u6570\u636e\u6536\u96c6\u548c\u5bf9\u9f50\u7684\u56f0\u96be\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.08241", "pdf": "https://arxiv.org/pdf/2602.08241", "abs": "https://arxiv.org/abs/2602.08241", "authors": ["Siqu Ou", "Tianrui Wan", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"], "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.", "AI": {"tldr": "SAYO\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff0c\u5f15\u5165\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\u673a\u5236\uff0c\u89e3\u51b3\u73b0\u6709MLLMs\u89c6\u89c9\u6ce8\u610f\u529b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u601d\u7ef4\u94fe\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u89c6\u89c9\u6ce8\u610f\u529b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff1a\u65e9\u671f\u89c6\u89c9\u5bf9\u9f50\u9519\u8bef\u5f88\u5c11\u5728\u540e\u7eed\u63a8\u7406\u4e2d\u88ab\u7ea0\u6b63\uff0c\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u548c\u63a8\u7406\u5931\u8d25\u3002\u8fd9\u79cd\u9650\u5236\u6e90\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u89c6\u89c9\u6ce8\u610f\u529b\u7684\u4fe1\u7528\u5206\u914d\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSAYO\u6a21\u578b\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\uff0c\u5f15\u5165\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\u673a\u5236\u3002\u8be5\u5956\u52b1\u660e\u786e\u5c06\u4f18\u5316\u4fe1\u53f7\u4e0e\u57fa\u4e8e\u89c6\u89c9\u7684\u63a8\u7406\u6b65\u9aa4\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u66f4\u53ef\u9760\u7684\u6ce8\u610f\u529b\u884c\u4e3a\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAYO\u5728\u591a\u6837\u5316\u7684\u63a8\u7406\u548c\u611f\u77e5\u4efb\u52a1\u4e0a\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\u673a\u5236\uff0cSAYO\u80fd\u591f\u5b66\u4e60\u66f4\u7a33\u5b9a\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u73b0\u6709MLLMs\u89c6\u89c9\u6ce8\u610f\u529b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2602.08249", "pdf": "https://arxiv.org/pdf/2602.08249", "abs": "https://arxiv.org/abs/2602.08249", "authors": ["Weijie Gan", "Xucheng Wang", "Tongyao Wang", "Wenshang Wang", "Chunwei Ying", "Yuyang Hu", "Yasheng Chen", "Hongyu An", "Ulugbek S. Kamilov"], "title": "A Unified Framework for Multimodal Image Reconstruction and Synthesis using Denoising Diffusion Models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Image reconstruction and image synthesis are important for handling incomplete multimodal imaging data, but existing methods require various task-specific models, complicating training and deployment workflows. We introduce Any2all, a unified framework that addresses this limitation by formulating these disparate tasks as a single virtual inpainting problem. We train a single, unconditional diffusion model on the complete multimodal data stack. This model is then adapted at inference time to ``inpaint'' all target modalities from any combination of inputs of available clean images or noisy measurements. We validated Any2all on a PET/MR/CT brain dataset. Our results show that Any2all can achieve excellent performance on both multimodal reconstruction and synthesis tasks, consistently yielding images with competitive distortion-based performance and superior perceptual quality over specialized methods.", "AI": {"tldr": "Any2all\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u91cd\u5efa\u4e0e\u5408\u6210\u6846\u67b6\uff0c\u5c06\u591a\u6a21\u6001\u6210\u50cf\u4efb\u52a1\u8f6c\u5316\u4e3a\u865a\u62df\u4fee\u590d\u95ee\u9898\uff0c\u4f7f\u7528\u5355\u4e00\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5904\u7406\u4efb\u610f\u8f93\u5165\u7ec4\u5408", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8bad\u7ec3\u591a\u4e2a\u4e13\u7528\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u6765\u5904\u7406\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u6210\u50cf\u6570\u636e", "method": "\u5c06\u591a\u6a21\u6001\u56fe\u50cf\u91cd\u5efa\u548c\u5408\u6210\u4efb\u52a1\u7edf\u4e00\u4e3a\u865a\u62df\u4fee\u590d\u95ee\u9898\uff0c\u5728\u5b8c\u6574\u591a\u6a21\u6001\u6570\u636e\u6808\u4e0a\u8bad\u7ec3\u5355\u4e00\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u63a8\u7406\u65f6\u901a\u8fc7\"\u4fee\u590d\"\u65b9\u5f0f\u4ece\u4efb\u610f\u53ef\u7528\u8f93\u5165\u751f\u6210\u76ee\u6807\u6a21\u6001", "result": "\u5728PET/MR/CT\u8111\u90e8\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cAny2all\u5728\u591a\u6a21\u6001\u91cd\u5efa\u548c\u5408\u6210\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5728\u5931\u771f\u5ea6\u6307\u6807\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4e13\u7528\u65b9\u6cd5", "conclusion": "Any2all\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7528\u5355\u4e00\u6a21\u578b\u5904\u7406\u591a\u79cd\u591a\u6a21\u6001\u6210\u50cf\u4efb\u52a1\uff0c\u7b80\u5316\u4e86\u5de5\u4f5c\u6d41\u7a0b\u5e76\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u8f93\u51fa"}}
{"id": "2602.08266", "pdf": "https://arxiv.org/pdf/2602.08266", "abs": "https://arxiv.org/abs/2602.08266", "authors": ["Seunghoon Jeong", "Eunho Lee", "Jeongyun Kim", "Ayoung Kim"], "title": "Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes", "categories": ["cs.RO", "cs.CV"], "comment": "9 pages, 8 figures, 4 tables, accepted to ICRA 2026", "summary": "In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u4f8b\u611f\u77e5\u7684Next Best View\u7b56\u7565\uff0c\u5229\u7528\u7269\u4f53\u7279\u5f81\u4f18\u5148\u63a2\u7d22\u672a\u5145\u5206\u89c2\u6d4b\u533a\u57df\uff0c\u663e\u8457\u964d\u4f4e\u6df1\u5ea6\u8bef\u5dee\u5e76\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5728\u906e\u6321\u548c\u89c2\u6d4b\u4e0d\u5b8c\u6574\u7684\u6742\u4e71\u573a\u666f\u4e2d\uff0c\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u89d2\u5bf9\u4e8e\u6784\u5efa\u53ef\u9760\u8868\u793a\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u51e0\u4f55\u7ebf\u7d22\uff0c\u5ffd\u7565\u4e86\u4e0e\u64cd\u4f5c\u76f8\u5173\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4e14\u503e\u5411\u4e8e\u5229\u7528\u800c\u975e\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u5b9e\u4f8b\u611f\u77e5\u7684NBV\u7b56\u7565\uff1a1) \u7269\u4f53\u611f\u77e5\u76843D\u9ad8\u65af\u6cfc\u6e85\u5c06\u5b9e\u4f8b\u7ea7\u4fe1\u606f\u84b8\u998f\u4e3a\u72ec\u70ed\u7269\u4f53\u5411\u91cf\uff1b2) \u8ba1\u7b97\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u6307\u5bfc\u8bc6\u522b\u4e0e\u9519\u8bef\u548c\u4e0d\u786e\u5b9a\u9ad8\u65af\u76f8\u5173\u7684\u533a\u57df\uff1b3) \u53ef\u8f7b\u677e\u9002\u5e94\u7269\u4f53\u4e2d\u5fc3\u7684NBV\uff0c\u4e13\u6ce8\u4e8e\u76ee\u6807\u7269\u4f53\u7684\u89c6\u89d2\u9009\u62e9\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u6df1\u5ea6\u8bef\u5dee\u964d\u4f4e\u8fbe77.14%\uff0c\u5728\u771f\u5b9e\u4e16\u754cGraspNet\u6570\u636e\u96c6\u4e0a\u964d\u4f4e34.10%\u3002\u9488\u5bf9\u7279\u5b9a\u7269\u4f53\u7684NBV\u76f8\u6bd4\u6574\u4e2a\u573a\u666f\uff0c\u53ef\u989d\u5916\u964d\u4f4e25.60%\u7684\u6df1\u5ea6\u8bef\u5dee\u3002\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b9e\u4f8b\u611f\u77e5NBV\u7b56\u7565\u901a\u8fc7\u5229\u7528\u7269\u4f53\u7279\u5f81\u4f18\u5148\u63a2\u7d22\u672a\u5145\u5206\u89c2\u6d4b\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u5728\u6742\u4e71\u573a\u666f\u4e2d\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.08336", "pdf": "https://arxiv.org/pdf/2602.08336", "abs": "https://arxiv.org/abs/2602.08336", "authors": ["Cheng Yang", "Chufan Shi", "Bo Shui", "Yaokang Wu", "Muzi Tao", "Huijuan Wang", "Ivan Yee Lee", "Yong Liu", "Xuezhe Ma", "Taylor Berg-Kirkpatrick"], "title": "UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models", "categories": ["cs.CL", "cs.CV"], "comment": "Project page: https://ureason.github.io", "summary": "To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.", "AI": {"tldr": "UReason\u662f\u4e00\u4e2a\u8bca\u65ad\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u63a8\u7406\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u63ed\u793a\u4e86\"\u63a8\u7406\u6096\u8bba\"\uff1a\u63a8\u7406\u75d5\u8ff9\u901a\u5e38\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5c06\u4e2d\u95f4\u601d\u8003\u4f5c\u4e3a\u6761\u4ef6\u4e0a\u4e0b\u6587\u53cd\u800c\u4f1a\u963b\u788d\u89c6\u89c9\u5408\u6210\u3002", "motivation": "\u5f53\u524d\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u91c7\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u6307\u5bfc\u56fe\u50cf\u751f\u6210\uff0c\u4f46\u63a8\u7406\u5bf9\u89c6\u89c9\u5408\u6210\u7684\u5b9e\u9645\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002\u9700\u8981\u8bc4\u4f30\u63a8\u7406\u662f\u5426\u80fd\u5728\u50cf\u7d20\u5c42\u9762\u5fe0\u5b9e\u6267\u884c\u3002", "method": "\u63d0\u51fa\u4e86UReason\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,000\u4e2a\u5b9e\u4f8b\uff0c\u6db5\u76d6\u4ee3\u7801\u3001\u7b97\u672f\u3001\u7a7a\u95f4\u3001\u5c5e\u6027\u548c\u6587\u672c\u63a8\u7406\u4e94\u4e2a\u4efb\u52a1\u5bb6\u65cf\u3002\u5f15\u5165\u8bc4\u4f30\u6846\u67b6\u6bd4\u8f83\u76f4\u63a5\u751f\u6210\u3001\u63a8\u7406\u5f15\u5bfc\u751f\u6210\u548c\u53bb\u4e0a\u4e0b\u6587\u5316\u751f\u6210\uff08\u4ec5\u57fa\u4e8e\u7cbe\u70bc\u63d0\u793a\uff09\u3002", "result": "\u5728\u516b\u4e2a\u5f00\u6e90\u7edf\u4e00\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\"\u63a8\u7406\u6096\u8bba\"\uff1a\u63a8\u7406\u75d5\u8ff9\u901a\u5e38\u6bd4\u76f4\u63a5\u751f\u6210\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4fdd\u7559\u4e2d\u95f4\u601d\u8003\u4f5c\u4e3a\u6761\u4ef6\u4e0a\u4e0b\u6587\u4f1a\u963b\u788d\u89c6\u89c9\u5408\u6210\uff0c\u800c\u4ec5\u57fa\u4e8e\u7cbe\u70bc\u63d0\u793a\u7684\u6761\u4ef6\u80fd\u5e26\u6765\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u74f6\u9888\u5728\u4e8e\u4e0a\u4e0b\u6587\u5e72\u6270\u800c\u975e\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002UReason\u4e3a\u7814\u7a76\u7edf\u4e00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u6fc0\u52b1\u672a\u6765\u65b9\u6cd5\u5728\u6709\u6548\u6574\u5408\u63a8\u7406\u8fdb\u884c\u89c6\u89c9\u751f\u6210\u7684\u540c\u65f6\u51cf\u8f7b\u5e72\u6270\u3002"}}
{"id": "2602.08339", "pdf": "https://arxiv.org/pdf/2602.08339", "abs": "https://arxiv.org/abs/2602.08339", "authors": ["Chengyi Du", "Yazhe Niu", "Dazhong Shen", "Luxin Xu"], "title": "CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT", "categories": ["cs.AI", "cs.CV"], "comment": "16 pages 6 figures", "summary": "Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.", "AI": {"tldr": "CoTZero\uff1a\u65e0\u9700\u6807\u6ce8\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u6570\u636e\u5408\u6210\u548c\u8ba4\u77e5\u5bf9\u9f50\u8bad\u7ec3\uff0c\u63d0\u5347\u89c6\u89c9\u63a8\u7406\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u53ef\u9a8c\u8bc1\u6027", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u8868\u9762\u76f8\u5173\u6027\u800c\u975e\u903b\u8f91\u4e00\u81f4\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5bfc\u81f4\u9ad8\u5c42\u6b21\u8bed\u4e49\u7ed3\u6784\u548c\u975e\u56e0\u679c\u5173\u7cfb\u7406\u89e3\u4e0d\u8db3\uff0c\u963b\u788d\u7ec4\u5408\u5f0f\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406", "method": "1) \u53cc\u9636\u6bb5\u6570\u636e\u5408\u6210\uff1a\u81ea\u5e95\u5411\u4e0a\u63d0\u53d6\u539f\u5b50\u89c6\u89c9\u57fa\u5143\u5e76\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u95ee\u9898\u63a8\u7406\u5f62\u5f0f\uff1b\u81ea\u9876\u5411\u4e0b\u5229\u7528\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\u5c40\u90e8\u7ec6\u8282\u548c\u56e0\u679c\u5173\u7cfb\u89e3\u91ca\u30022) \u8ba4\u77e5\u5bf9\u9f50\u8bad\u7ec3\uff1a\u5728\u5f3a\u5316\u5fae\u8c03\u4e2d\u5f15\u5165\u8ba4\u77e5\u4e00\u81f4\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u63d0\u4f9b\u63a8\u7406\u4e00\u81f4\u6027\u548c\u4e8b\u5b9e\u6b63\u786e\u6027\u7684\u9010\u6b65\u53cd\u9988", "result": "\u5728\u5305\u542b\u8bcd\u6c47\u6270\u52a8\u8d1f\u4f8b\u7684\u591a\u5c42\u6b21\u8bed\u4e49\u4e0d\u4e00\u81f4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoTZero\u5728\u57df\u5185\u548c\u57df\u5916\u8bbe\u7f6e\u4e0b\u8fbe\u523083.33%\u7684F1\u5206\u6570\uff0c\u5404\u7ec4\u4ef6\u5747\u5bf9\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684\u89c6\u89c9\u63a8\u7406\u6709\u8d21\u732e", "conclusion": "CoTZero\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u8ba4\u77e5\u6a21\u578b\u5230\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u8868\u793a\u548c\u903b\u8f91\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b"}}
{"id": "2602.08392", "pdf": "https://arxiv.org/pdf/2602.08392", "abs": "https://arxiv.org/abs/2602.08392", "authors": ["Xin Wu", "Zhixuan Liang", "Yue Ma", "Mengkang Hu", "Zhiyuan Qin", "Xiu Li"], "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "38 pages, 9 figures. Project page:https://bimanibench.github.io/", "summary": "Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86BiManiBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u534f\u8c03\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709MLLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u81c2\u64cd\u4f5c\uff0c\u65e0\u6cd5\u8bc4\u4f30\u53cc\u81c2\u4efb\u52a1\u6240\u9700\u7684\u65f6\u7a7a\u534f\u8c03\u80fd\u529b\uff0c\u5982\u62ac\u8d77\u91cd\u9505\u7b49\u9700\u8981\u53cc\u81c2\u534f\u4f5c\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51faBiManiBench\u5206\u5c42\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5c42\u7ea7\uff1a\u57fa\u7840\u7a7a\u95f4\u63a8\u7406\u3001\u9ad8\u5c42\u52a8\u4f5c\u89c4\u5212\u548c\u4f4e\u5c42\u672b\u7aef\u6267\u884c\u5668\u63a7\u5236\uff0c\u4e13\u95e8\u9488\u5bf9\u53cc\u81c2\u64cd\u4f5c\u4e2d\u7684\u72ec\u7279\u6311\u6218\uff08\u5982\u624b\u81c2\u53ef\u8fbe\u6027\u548c\u8fd0\u52a8\u5b66\u7ea6\u675f\uff09\u8fdb\u884c\u8bbe\u8ba1\u3002", "result": "\u5bf930\u591a\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5206\u6790\u663e\u793a\uff0c\u5c3d\u7ba1MLLMs\u5728\u9ad8\u5c42\u63a8\u7406\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u53cc\u81c2\u7a7a\u95f4\u5b9a\u4f4d\u548c\u63a7\u5236\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7ecf\u5e38\u51fa\u73b0\u76f8\u4e92\u5e72\u6270\u548c\u5e8f\u5217\u9519\u8bef\u3002", "conclusion": "\u5f53\u524dMLLM\u8303\u5f0f\u7f3a\u4e4f\u5bf9\u53cc\u81c2\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u6df1\u5165\u7406\u89e3\uff0c\u672a\u6765\u7814\u7a76\u9700\u8981\u5173\u6ce8\u53cc\u81c2\u78b0\u649e\u907f\u514d\u548c\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u89c4\u5212\u3002"}}
{"id": "2602.08426", "pdf": "https://arxiv.org/pdf/2602.08426", "abs": "https://arxiv.org/abs/2602.08426", "authors": ["Xinghao Wang", "Pengyu Wang", "Xiaoran Liu", "Fangxu Liu", "Jason Chu", "Kai Song", "Xipeng Qiu"], "title": "Prism: Spectral-Aware Block-Sparse Attention", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\\mathbf{5.1\\times}$ speedup.", "AI": {"tldr": "Prism\u901a\u8fc7\u9891\u8c31\u611f\u77e5\u65b9\u6cd5\u89e3\u51b3\u5757\u7a00\u758f\u6ce8\u610f\u529b\u4e2d\u5757\u9009\u62e9\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u5229\u7528\u80fd\u91cf\u6e29\u5ea6\u6821\u51c6\u6062\u590d\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5b9e\u73b0\u7eaf\u5757\u7ea7\u64cd\u4f5c\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u83b7\u5f975.1\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u5757\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4f7f\u7528\u7c97\u7c92\u5ea6\u6ce8\u610f\u529b\u4f5c\u4e3a\u5757\u91cd\u8981\u6027\u4f30\u8ba1\u7684\u4ee3\u7406\uff0c\u4f46\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684token\u7ea7\u641c\u7d22\u6216\u8bc4\u5206\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u9009\u62e9\u5f00\u9500\u3002\u6807\u51c6\u7c97\u7c92\u5ea6\u6ce8\u610f\u529b\uff08\u901a\u8fc7\u5e73\u5747\u6c60\u5316\uff09\u7684\u4e0d\u51c6\u786e\u6027\u6e90\u4e8e\u5176\u4e0eRoPE\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e73\u5747\u6c60\u5316\u4f5c\u4e3a\u4f4e\u901a\u6ee4\u6ce2\u5668\u4f1a\u7834\u574f\u9ad8\u9891\u7ef4\u5ea6\u4e2d\u7684\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\u3002", "method": "\u63d0\u51faPrism\u65b9\u6cd5\uff1a1\uff09\u7406\u8bba\u5206\u6790\u8868\u660e\u5e73\u5747\u6c60\u5316\u4e0eRoPE\u76f8\u4e92\u4f5c\u7528\u4f1a\u7834\u574f\u9ad8\u9891\u7ef4\u5ea6\u4e2d\u7684\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\uff1b2\uff09\u5c06\u5757\u9009\u62e9\u5206\u89e3\u4e3a\u9ad8\u9891\u548c\u4f4e\u9891\u5206\u652f\uff1b3\uff09\u5e94\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u6e29\u5ea6\u6821\u51c6\uff0c\u76f4\u63a5\u4ece\u6c60\u5316\u8868\u793a\u4e2d\u6062\u590d\u8870\u51cf\u7684\u4f4d\u7f6e\u4fe1\u53f7\uff1b4\uff09\u5b9e\u73b0\u7eaf\u5757\u7ea7\u64cd\u4f5c\u7684\u5757\u91cd\u8981\u6027\u4f30\u8ba1\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u5b9ePrism\u5728\u4fdd\u6301\u4e0e\u5b8c\u6574\u6ce8\u610f\u529b\u7cbe\u5ea6\u76f8\u5f53\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.1\u500d\u7684\u52a0\u901f\u3002", "conclusion": "Prism\u901a\u8fc7\u9891\u8c31\u611f\u77e5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5757\u7a00\u758f\u6ce8\u610f\u529b\u4e2d\u5757\u9009\u62e9\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7eaf\u5757\u7ea7\u64cd\u4f5c\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587LLM\u9884\u586b\u5145\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2602.08466", "pdf": "https://arxiv.org/pdf/2602.08466", "abs": "https://arxiv.org/abs/2602.08466", "authors": ["Ning Hu", "Senhao Cao", "Maochen Li"], "title": "Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment", "categories": ["cs.RO", "cs.CV"], "comment": "7 pages, 1 figure", "summary": "Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9760\u6027\u611f\u77e5\u7684\u6267\u884c\u95e8\u63a7\u673a\u5236\uff0c\u901a\u8fc7\u8bc4\u4f30\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u914d\u7f6e\u98ce\u9669\u6765\u7b5b\u9009\u9ad8\u98ce\u9669\u7684\u59ff\u6001\u66f4\u65b0\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u89c6\u89c9\u5f15\u5bfc\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6267\u884c\u53ef\u9760\u6027\u3002", "motivation": "\u867d\u7136\u59ff\u6001\u4f30\u8ba1\u7684\u6570\u503c\u7cbe\u5ea6\u5df2\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6267\u884c\u89c6\u89c9\u5bf9\u9f50\u4efb\u52a1\u65f6\u4ecd\u9891\u7e41\u5931\u8d25\uff0c\u7279\u522b\u662f\u5728\u8fd1\u573a\u548c\u79bb\u8f74\u914d\u7f6e\u4e0b\u3002\u8fd9\u8868\u660e\u4ec5\u9760\u59ff\u6001\u7cbe\u5ea6\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u6267\u884c\u5c42\u9762\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u89e3\u51b3\u51e0\u4f55\u8bef\u5dee\u653e\u5927\u673a\u5236\u5e26\u6765\u7684\u6267\u884c\u5931\u8d25\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53ef\u9760\u6027\u611f\u77e5\u6267\u884c\u95e8\u63a7\u673a\u5236\uff0c\u4e0d\u4fee\u6539\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5\uff0c\u800c\u662f\u5728\u6267\u884c\u5c42\u9762\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u5728\u6267\u884c\u524d\u8bc4\u4f30\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u914d\u7f6e\u98ce\u9669\uff0c\u9009\u62e9\u6027\u5730\u62d2\u7edd\u6216\u7f29\u653e\u9ad8\u98ce\u9669\u7684\u59ff\u6001\u66f4\u65b0\uff0c\u4ece\u800c\u907f\u514d\u7531\u5c0f\u8bef\u5dee\u653e\u5927\u5bfc\u81f4\u7684\u4e0d\u7a33\u5b9a\u6216\u5931\u8d25\u5bf9\u9f50\u3002", "result": "\u5728UR5\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u6267\u884c\u95e8\u63a7\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u51cf\u5c11\u4e86\u6267\u884c\u65b9\u5dee\uff0c\u6291\u5236\u4e86\u5c3e\u90e8\u98ce\u9669\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5e73\u5747\u59ff\u6001\u7cbe\u5ea6\u57fa\u672c\u4e0d\u53d8\u3002\u8be5\u65b9\u6cd5\u4e0e\u59ff\u6001\u4f30\u8ba1\u5668\u65e0\u5173\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u57fa\u4e8e\u51e0\u4f55\u548c\u5b66\u4e60\u7684\u59ff\u6001\u4f30\u8ba1\u6d41\u7a0b\u4e2d\u3002", "conclusion": "\u6267\u884c\u5c42\u9762\u7684\u53ef\u9760\u6027\u5efa\u6a21\u5bf9\u4e8e\u63d0\u9ad8\u89c6\u89c9\u5f15\u5bfc\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u63d0\u51fa\u7684\u6267\u884c\u95e8\u63a7\u673a\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u6539\u5584\u6267\u884c\u53ef\u9760\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fd1\u573a\u89c6\u89c9\u5f15\u5bfc\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2602.08580", "pdf": "https://arxiv.org/pdf/2602.08580", "abs": "https://arxiv.org/abs/2602.08580", "authors": ["Jose D. Vargas Quiros", "Michael J. Beyeler", "Sofia Ortin Vela", "EyeNED Reading Center", "Sven Bergmann", "Caroline C. W. Klaver", "Bart Liefers"], "title": "retinalysis-vascx: An explainable software toolbox for the extraction of retinal vascular biomarkers", "categories": ["q-bio.TO", "cs.CV"], "comment": null, "summary": "The automatic extraction of retinal vascular biomarkers from color fundus images (CFI) is essential for large-scale studies of the retinal vasculature. We present VascX, an open-source Python toolbox designed for the automated extraction of biomarkers from artery and vein segmentations. The VascX workflow processes vessel segmentation masks into skeletons to build undirected and directed vessel graphs, which are then used to resolve segments into continuous vessels. This architecture enables the calculation of a comprehensive suite of biomarkers, including vascular density, bifurcation angles, central retinal equivalents (CREs), tortuosity, and temporal angles, alongside image quality metrics.\n  A distinguishing feature of VascX is its region awareness; by utilizing the fovea, optic disc, and CFI boundaries as anatomical landmarks, the tool ensures spatially standardized measurements and identifies when specific biomarkers are not computable. Spatially localized biomarkers are calculated over grids relative to these landmarks, facilitating precise clinical analysis. Released via GitHub and PyPI, VascX provides an explainable and modifiable framework that supports reproducible vascular research through integrated visualizations. By enabling the rapid extraction of established biomarkers and the development of new ones, VascX advances the field of oculomics, offering a robust, computationally efficient solution for scalable deployment in large-scale clinical and epidemiological databases.", "AI": {"tldr": "VascX\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5de5\u5177\u7bb1\uff0c\u7528\u4e8e\u4ece\u773c\u5e95\u5f69\u8272\u56fe\u50cf\u4e2d\u81ea\u52a8\u63d0\u53d6\u89c6\u7f51\u819c\u8840\u7ba1\u751f\u7269\u6807\u5fd7\u7269\uff0c\u652f\u6301\u5927\u89c4\u6a21\u89c6\u7f51\u819c\u8840\u7ba1\u7814\u7a76\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u7f51\u819c\u8840\u7ba1\u7814\u7a76\u9700\u8981\u4ece\u773c\u5e95\u5f69\u8272\u56fe\u50cf\u4e2d\u81ea\u52a8\u63d0\u53d6\u8840\u7ba1\u751f\u7269\u6807\u5fd7\u7269\uff0c\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u6807\u51c6\u5316\u3001\u53ef\u91cd\u590d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u8840\u7ba1\u5206\u5272\u63a9\u7801\u5904\u7406\u4e3a\u9aa8\u67b6\u56fe\uff0c\u6784\u5efa\u65e0\u5411\u548c\u6709\u5411\u8840\u7ba1\u56fe\uff0c\u89e3\u6790\u8fde\u7eed\u8840\u7ba1\u6bb5\uff0c\u5229\u7528\u9ec4\u6591\u3001\u89c6\u76d8\u7b49\u89e3\u5256\u6807\u5fd7\u8fdb\u884c\u7a7a\u95f4\u6807\u51c6\u5316\u6d4b\u91cf\u3002", "result": "\u5f00\u53d1\u4e86VascX\u5de5\u5177\u7bb1\uff0c\u80fd\u591f\u8ba1\u7b97\u8840\u7ba1\u5bc6\u5ea6\u3001\u5206\u53c9\u89d2\u5ea6\u3001\u89c6\u7f51\u819c\u4e2d\u592e\u7b49\u6548\u503c\u3001\u8fc2\u66f2\u5ea6\u3001\u65f6\u95f4\u89d2\u5ea6\u7b49\u7efc\u5408\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5e76\u63d0\u4f9b\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u3002", "conclusion": "VascX\u4e3a\u773c\u7ec4\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u4fee\u6539\u7684\u6846\u67b6\uff0c\u652f\u6301\u53ef\u91cd\u590d\u7684\u8840\u7ba1\u7814\u7a76\uff0c\u4e3a\u5927\u89c4\u6a21\u4e34\u5e8a\u548c\u6d41\u884c\u75c5\u5b66\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08632", "pdf": "https://arxiv.org/pdf/2602.08632", "abs": "https://arxiv.org/abs/2602.08632", "authors": ["Adi Haviv", "Niva Elkin-Koren", "Uri Hacohen", "Roi Livni", "Shay Moran"], "title": "We Should Separate Memorization from Copyright", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "The widespread use of foundation models has introduced a new risk factor of copyright issue. This issue is leading to an active, lively and on-going debate amongst the data-science community as well as amongst legal scholars. Where claims and results across both sides are often interpreted in different ways and leading to different implications. Our position is that much of the technical literature relies on traditional reconstruction techniques that are not designed for copyright analysis. As a result, memorization and copying have been conflated across both technical and legal communities and in multiple contexts. We argue that memorization, as commonly studied in data science, should not be equated with copying and should not be used as a proxy for copyright infringement. We distinguish technical signals that meaningfully indicate infringement risk from those that instead reflect lawful generalization or high-frequency content. Based on this analysis, we advocate for an output-level, risk-based evaluation process that aligns technical assessments with established copyright standards and provides a more principled foundation for research, auditing, and policy.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u5f53\u524d\u5173\u4e8e\u57fa\u7840\u6a21\u578b\u7248\u6743\u95ee\u9898\u7684\u6280\u672f\u6587\u732e\u8fc7\u5ea6\u4f9d\u8d56\u4f20\u7edf\u91cd\u5efa\u6280\u672f\uff0c\u5c06\u8bb0\u5fc6\u4e0e\u590d\u5236\u6df7\u4e3a\u4e00\u8c08\uff0c\u4e3b\u5f20\u8bb0\u5fc6\u4e0d\u5e94\u7b49\u540c\u4e8e\u590d\u5236\u6216\u4f5c\u4e3a\u7248\u6743\u4fb5\u6743\u7684\u4ee3\u7406\u6307\u6807\uff0c\u63d0\u5021\u57fa\u4e8e\u8f93\u51fa\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u7684\u5e7f\u6cdb\u4f7f\u7528\u5e26\u6765\u4e86\u65b0\u7684\u7248\u6743\u98ce\u9669\u56e0\u7d20\uff0c\u5f53\u524d\u6570\u636e\u79d1\u5b66\u754c\u548c\u6cd5\u5f8b\u5b66\u754c\u5bf9\u6b64\u5b58\u5728\u6d3b\u8dc3\u4f46\u6df7\u4e71\u7684\u8ba8\u8bba\uff0c\u6280\u672f\u6587\u732e\u4e2d\u7684\u4e3b\u5f20\u548c\u7ed3\u679c\u5e38\u88ab\u4e0d\u540c\u89e3\u8bfb\u5e76\u5bfc\u81f4\u4e0d\u540c\u542b\u4e49\u3002", "method": "\u5206\u6790\u5f53\u524d\u6280\u672f\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u4f20\u7edf\u91cd\u5efa\u6280\u672f\uff0c\u533a\u5206\u771f\u6b63\u8868\u660e\u4fb5\u6743\u98ce\u9669\u7684\u6280\u672f\u4fe1\u53f7\u4e0e\u53cd\u6620\u5408\u6cd5\u6cdb\u5316\u6216\u9ad8\u9891\u5185\u5bb9\u7684\u6280\u672f\u4fe1\u53f7\uff0c\u63d0\u51fa\u8f93\u51fa\u5c42\u9762\u7684\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u8bba\u6587\u8bba\u8bc1\u4e86\u8bb0\u5fc6\uff08\u5982\u6570\u636e\u79d1\u5b66\u4e2d\u5e38\u7814\u7a76\u7684\uff09\u4e0d\u5e94\u7b49\u540c\u4e8e\u590d\u5236\uff0c\u4e5f\u4e0d\u5e94\u4f5c\u4e3a\u7248\u6743\u4fb5\u6743\u7684\u4ee3\u7406\u6307\u6807\uff0c\u9700\u8981\u533a\u5206\u4e0d\u540c\u6280\u672f\u4fe1\u53f7\u7684\u5b9e\u9645\u542b\u4e49\u3002", "conclusion": "\u63d0\u5021\u57fa\u4e8e\u8f93\u51fa\u7684\u98ce\u9669\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4f7f\u6280\u672f\u8bc4\u4f30\u4e0e\u65e2\u5b9a\u7684\u7248\u6743\u6807\u51c6\u4fdd\u6301\u4e00\u81f4\uff0c\u4e3a\u7814\u7a76\u3001\u5ba1\u8ba1\u548c\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u66f4\u6709\u539f\u5219\u7684\u57fa\u7840\u3002"}}
{"id": "2602.08764", "pdf": "https://arxiv.org/pdf/2602.08764", "abs": "https://arxiv.org/abs/2602.08764", "authors": ["Hjalti Thrastarson", "Lotta M. Ellingsen"], "title": "Efficient Brain Extraction of MRI Scans with Mild to Moderate Neuropathology", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted for publication in the Proceedings of SPIE Medical Imaging 2026", "summary": "Skull stripping magnetic resonance images (MRI) of the human brain is an important process in many image processing techniques, such as automatic segmentation of brain structures. Numerous methods have been developed to perform this task, however, they often fail in the presence of neuropathology and can be inconsistent in defining the boundary of the brain mask. Here, we propose a novel approach to skull strip T1-weighted images in a robust and efficient manner, aiming to consistently segment the outer surface of the brain, including the sulcal cerebrospinal fluid (CSF), while excluding the full extent of the subarachnoid space and meninges. We train a modified version of the U-net on silver-standard ground truth data using a novel loss function based on the signed-distance transform (SDT). We validate our model both qualitatively and quantitatively using held-out data from the training dataset, as well as an independent external dataset. The brain masks used for evaluation partially or fully include the subarachnoid space, which may introduce bias into the comparison; nonetheless, our model demonstrates strong performance on the held-out test data, achieving a consistent mean Dice similarity coefficient (DSC) of 0.964$\\pm$0.006 and an average symmetric surface distance (ASSD) of 1.4mm$\\pm$0.2mm. Performance on the external dataset is comparable, with a DSC of 0.958$\\pm$0.006 and an ASSD of 1.7$\\pm$0.2mm. Our method achieves performance comparable to or better than existing state-of-the-art methods for brain extraction, particularly in its highly consistent preservation of the brain's outer surface. The method is publicly available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbU-net\u548c\u7b26\u53f7\u8ddd\u79bb\u53d8\u6362\u635f\u5931\u51fd\u6570\u7684\u9c81\u68d2\u6027\u5934\u9aa8\u5265\u79bb\u65b9\u6cd5\uff0c\u7528\u4e8eT1\u52a0\u6743MRI\u56fe\u50cf\uff0c\u80fd\u4e00\u81f4\u6027\u5730\u5206\u5272\u5927\u8111\u5916\u8868\u9762\u5e76\u6392\u9664\u86db\u7f51\u819c\u4e0b\u8154\u3002", "motivation": "\u73b0\u6709\u5934\u9aa8\u5265\u79bb\u65b9\u6cd5\u5728\u795e\u7ecf\u75c5\u7406\u5b66\u5b58\u5728\u65f6\u5bb9\u6613\u5931\u8d25\uff0c\u4e14\u5728\u5927\u8111\u63a9\u6a21\u8fb9\u754c\u5b9a\u4e49\u4e0a\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u548c\u4e00\u81f4\u7684\u65b9\u6cd5\u6765\u51c6\u786e\u5206\u5272\u5927\u8111\u5916\u8868\u9762\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684U-net\u67b6\u6784\uff0c\u57fa\u4e8e\u94f6\u6807\u51c6ground truth\u6570\u636e\u8bad\u7ec3\uff0c\u91c7\u7528\u57fa\u4e8e\u7b26\u53f7\u8ddd\u79bb\u53d8\u6362(SDT)\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u65e8\u5728\u4e00\u81f4\u5206\u5272\u5927\u8111\u5916\u8868\u9762\uff08\u5305\u62ec\u8111\u6c9f\u8111\u810a\u6db2\uff09\u5e76\u6392\u9664\u86db\u7f51\u819c\u4e0b\u8154\u548c\u8111\u819c\u3002", "result": "\u5728\u4fdd\u7559\u6d4b\u8bd5\u6570\u636e\u4e0a\u83b7\u5f97\u5e73\u5747Dice\u76f8\u4f3c\u7cfb\u65700.964\u00b10.006\u548c\u5e73\u5747\u5bf9\u79f0\u8868\u9762\u8ddd\u79bb1.4mm\u00b10.2mm\uff1b\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u83b7\u5f97DSC 0.958\u00b10.006\u548cASSD 1.7\u00b10.2mm\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5934\u9aa8\u5265\u79bb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5927\u8111\u5916\u8868\u9762\u7684\u4e00\u81f4\u6027\u4fdd\u7559\u65b9\u9762\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u3002"}}
{"id": "2602.08882", "pdf": "https://arxiv.org/pdf/2602.08882", "abs": "https://arxiv.org/abs/2602.08882", "authors": ["Puqi Zhou", "Ali Asgarov", "Aafiya Hussain", "Wonjoon Park", "Amit Paudyal", "Sameep Shrestha", "Chia-wei Tang", "Michael F. Lighthiser", "Michael R. Hieb", "Xuesu Xiao", "Chris Thomas", "Sungsoo Ray Hong"], "title": "Designing Multi-Robot Ground Video Sensemaking with Public Safety Professionals", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Videos from fleets of ground robots can advance public safety by providing scalable situational awareness and reducing professionals' burden. Yet little is known about how to design and integrate multi-robot videos into public safety workflows. Collaborating with six police agencies, we examined how such videos could be made practical. In Study 1, we presented the first testbed for multi-robot ground video sensemaking. The testbed includes 38 events-of-interest (EoI) relevant to public safety, a dataset of 20 robot patrol videos (10 day/night pairs) covering EoI types, and 6 design requirements aimed at improving current video sensemaking practices. In Study 2, we built MRVS, a tool that augments multi-robot patrol video streams with a prompt-engineered video understanding model. Participants reported reduced manual workload and greater confidence with LLM-based explanations, while noting concerns about false alarms and privacy. We conclude with implications for designing future multi-robot video sensemaking tools. The testbed is available at https://github.com/Puqi7/MRVS\\_VideoSensemaking", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u673a\u5668\u4eba\u5730\u9762\u89c6\u9891\u611f\u77e5\u6d4b\u8bd5\u5e73\u53f0\u548cMRVS\u5de5\u5177\uff0c\u901a\u8fc7\u4e0e\u8b66\u65b9\u5408\u4f5c\u6539\u8fdb\u516c\u5171\u5b89\u5168\u4e2d\u7684\u591a\u673a\u5668\u4eba\u89c6\u9891\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u5730\u9762\u673a\u5668\u4eba\u8230\u961f\u89c6\u9891\u53ef\u63d0\u5347\u516c\u5171\u5b89\u5168\u6001\u52bf\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5982\u4f55\u5c06\u591a\u673a\u5668\u4eba\u89c6\u9891\u96c6\u6210\u5230\u516c\u5171\u5b89\u5168\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8bbe\u8ba1\u77e5\u8bc6\u3002\u901a\u8fc7\u4e0e\u516d\u4e2a\u8b66\u5bdf\u673a\u6784\u5408\u4f5c\uff0c\u7814\u7a76\u5982\u4f55\u4f7f\u8fd9\u7c7b\u89c6\u9891\u5206\u6790\u53d8\u5f97\u5b9e\u7528\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u7814\u7a761\u521b\u5efa\u4e86\u9996\u4e2a\u591a\u673a\u5668\u4eba\u5730\u9762\u89c6\u9891\u611f\u77e5\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b38\u4e2a\u516c\u5171\u5b89\u5168\u76f8\u5173\u4e8b\u4ef6\u300120\u4e2a\u673a\u5668\u4eba\u5de1\u903b\u89c6\u9891\u6570\u636e\u96c6\u548c6\u4e2a\u8bbe\u8ba1\u9700\u6c42\uff1b\u7814\u7a762\u5f00\u53d1\u4e86MRVS\u5de5\u5177\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u89c6\u9891\u7406\u89e3\u6a21\u578b\u589e\u5f3a\u591a\u673a\u5668\u4eba\u5de1\u903b\u89c6\u9891\u6d41\u3002", "result": "\u53c2\u4e0e\u8005\u62a5\u544a\u4f7f\u7528LLM\u89e3\u91ca\u51cf\u5c11\u4e86\u624b\u52a8\u5de5\u4f5c\u91cf\u5e76\u589e\u5f3a\u4e86\u4fe1\u5fc3\uff0c\u4f46\u4e5f\u6ce8\u610f\u5230\u8bef\u62a5\u548c\u9690\u79c1\u95ee\u9898\u3002\u6d4b\u8bd5\u5e73\u53f0\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u672a\u6765\u591a\u673a\u5668\u4eba\u89c6\u9891\u611f\u77e5\u5de5\u5177\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5f3a\u8c03\u4e86\u81ea\u52a8\u5316\u5206\u6790\u5728\u51cf\u8f7b\u4e13\u4e1a\u4eba\u5458\u8d1f\u62c5\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u9700\u8981\u89e3\u51b3\u8bef\u62a5\u548c\u9690\u79c1\u95ee\u9898\u3002"}}
{"id": "2602.09007", "pdf": "https://arxiv.org/pdf/2602.09007", "abs": "https://arxiv.org/abs/2602.09007", "authors": ["Haodong Li", "Jingwei Wu", "Quan Sun", "Guopeng Li", "Juanxi Tian", "Huanyu Zhang", "Yanlin Lai", "Ruichuan An", "Hongbo Peng", "Yuhong Dai", "Chenxi Li", "Chunmei Qing", "Jia Wang", "Ziyang Meng", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang"], "title": "GEBench: Benchmarking Image Generation Models as GUI Environments", "categories": ["cs.AI", "cs.CV"], "comment": "23 pages, 5 figures, 4 tables", "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86GEBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30GUI\u751f\u6210\u4e2d\u7684\u52a8\u6001\u4ea4\u4e92\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5305\u542b700\u4e2a\u6837\u672c\u548c\u4e94\u7ef4\u5ea6\u7684GE-Score\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u80fd\u9884\u6d4b\u672a\u6765GUI\u72b6\u6001\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u7f3a\u4e4f\u5bf9GUI\u7279\u5b9a\u573a\u666f\u4e2d\u72b6\u6001\u8f6c\u6362\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u3002", "method": "\u6784\u5efaGEBench\u57fa\u51c6\uff0c\u5305\u542b700\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6837\u672c\uff0c\u6db5\u76d65\u4e2a\u4efb\u52a1\u7c7b\u522b\uff1b\u63d0\u51faGE-Score\u4e94\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\uff08\u76ee\u6807\u8fbe\u6210\u3001\u4ea4\u4e92\u903b\u8f91\u3001\u5185\u5bb9\u4e00\u81f4\u6027\u3001UI\u5408\u7406\u6027\u3001\u89c6\u89c9\u8d28\u91cf\uff09\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u5355\u6b65\u8f6c\u6362\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4fdd\u6301\u957f\u65f6\u95f4\u4ea4\u4e92\u5e8f\u5217\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\uff1b\u56fe\u6807\u89e3\u91ca\u3001\u6587\u672c\u6e32\u67d3\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u662f\u5173\u952e\u74f6\u9888\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u6784\u5efa\u9ad8\u4fdd\u771f\u751f\u6210GUI\u73af\u5883\u6307\u660e\u4e86\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.09013", "pdf": "https://arxiv.org/pdf/2602.09013", "abs": "https://arxiv.org/abs/2602.09013", "authors": ["Hongyi Chen", "Tony Dong", "Tiancheng Wu", "Liquan Wang", "Yash Jangir", "Yaru Niu", "Yufei Ye", "Homanga Bharadhwaj", "Zackory Erickson", "Jeffrey Ichnowski"], "title": "Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.", "AI": {"tldr": "VIDEOMANIP\uff1a\u65e0\u9700\u7a7f\u6234\u8bbe\u5907\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4eceRGB\u4eba\u7c7b\u89c6\u9891\u5b66\u4e60\u7075\u5de7\u64cd\u4f5c\uff0c\u901a\u8fc7\u91cd\u5efa4D\u673a\u5668\u4eba-\u7269\u4f53\u8f68\u8ff9\u548c\u63a5\u89e6\u4f18\u5316\u5b9e\u73b0\u901a\u7528\u7b56\u7565\u5b66\u4e60", "motivation": "\u591a\u6307\u673a\u5668\u4eba\u624b\u64cd\u4f5c\u548c\u6293\u53d6\u9762\u4e34\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u548c\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a7f\u6234\u8bbe\u5907\u6216\u4e13\u7528\u4f20\u611f\u8bbe\u5907\u8fdb\u884c\u4eba\u7c7b\u9065\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faVIDEOMANIP\u6846\u67b6\uff1a1) \u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u663e\u5f0f4D\u673a\u5668\u4eba-\u7269\u4f53\u8f68\u8ff9\uff08\u4f30\u8ba1\u4eba\u624b\u59ff\u6001\u3001\u7269\u4f53\u7f51\u683c\uff09\uff1b2) \u901a\u8fc7\u63a5\u89e6\u4f18\u5316\u548c\u4ea4\u4e92\u4e2d\u5fc3\u6293\u53d6\u5efa\u6a21\u6539\u8fdb\u91cd\u5efa\u6570\u636e\uff1b3) \u6f14\u793a\u5408\u6210\u7b56\u7565\u4ece\u5355\u4e2a\u89c6\u9891\u751f\u6210\u591a\u6837\u5316\u8bad\u7ec3\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u4e2d\uff1a\u5728Inspire Hand\u4e0a\u5bf920\u4e2a\u4e0d\u540c\u7269\u4f53\u5b9e\u73b070.25%\u6210\u529f\u7387\u3002\u771f\u5b9e\u4e16\u754c\uff1a\u5728LEAP Hand\u4e0a\u5bf97\u4e2a\u4efb\u52a1\u5e73\u5747\u6210\u529f\u738762.86%\uff0c\u6bd4\u57fa\u4e8e\u91cd\u5b9a\u5411\u7684\u65b9\u6cd5\u63d0\u534715.87%\u3002", "conclusion": "VIDEOMANIP\u5c55\u793a\u4e86\u4eceRGB\u4eba\u7c7b\u89c6\u9891\u76f4\u63a5\u5b66\u4e60\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u884c\u6027\uff0c\u65e0\u9700\u989d\u5916\u673a\u5668\u4eba\u6f14\u793a\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.09018", "pdf": "https://arxiv.org/pdf/2602.09018", "abs": "https://arxiv.org/abs/2602.09018", "authors": ["Amir Mallak", "Alaa Maalouf"], "title": "Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \\in \\{0,1,2,3\\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\\rightarrow$ urban and day $\\rightarrow$ night ($\\sim 31\\%$ each); actor swaps $\\sim 10\\%$, moderate rain $\\sim 7\\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\\% \\rightarrow 70.1\\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5206\u89e3\u81ea\u52a8\u9a7e\u9a76OOD\u9c81\u68d2\u6027\uff0c\u901a\u8fc75\u4e2a\u73af\u5883\u7ef4\u5ea6\uff08\u573a\u666f\u3001\u5b63\u8282\u3001\u5929\u6c14\u3001\u65f6\u95f4\u3001\u667a\u80fd\u4f53\u7ec4\u5408\uff09\u548ck\u56e0\u5b50\u6270\u52a8\u6d4b\u8bd5\uff0c\u53d1\u73b0ViT\u7b56\u7565\u6bd4CNN/FC\u66f4\u9c81\u68d2\uff0cFM\u7279\u5f81\u8868\u73b0\u6700\u4f73\u4f46\u5ef6\u8fdf\u9ad8\uff0c\u591a\u5e27\u8f93\u5165\u4e0d\u4f18\u4e8e\u5355\u5e27\uff0c\u73af\u5883\u53d8\u5316\u5f71\u54cd\u975e\u7ebf\u6027\uff0c\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u548c\u591a\u6837\u6027\u662f\u5173\u952e\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5206\u5e03\u5916\uff08OOD\uff09\u9c81\u68d2\u6027\u901a\u5e38\u88ab\u7b80\u5316\u4e3a\u5355\u4e00\u6570\u5b57\u6307\u6807\uff0c\u63a9\u76d6\u4e86\u7b56\u7565\u5931\u6548\u7684\u5177\u4f53\u539f\u56e0\u3002\u7814\u7a76\u8005\u5e0c\u671b\u7cfb\u7edf\u6027\u5730\u5206\u89e3\u73af\u5883\u53d8\u5316\u56e0\u7d20\uff0c\u7406\u89e3\u4e0d\u540c\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u7b56\u7565\u6027\u80fd\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u9a7e\u9a76\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5c06\u73af\u5883\u5206\u89e3\u4e3a5\u4e2a\u7ef4\u5ea6\uff1a\u573a\u666f\uff08\u4e61\u6751/\u57ce\u5e02\uff09\u3001\u5b63\u8282\u3001\u5929\u6c14\u3001\u65f6\u95f4\uff08\u767d\u5929/\u591c\u665a\uff09\u3001\u667a\u80fd\u4f53\u7ec4\u5408\uff1b\u4f7f\u7528k\u56e0\u5b50\u6270\u52a8\uff08k\u2208{0,1,2,3}\uff09\u8fdb\u884c\u63a7\u5236\u6d4b\u8bd5\uff1b\u5728VISTA\u6a21\u62df\u5668\u4e2d\u6d4b\u8bd5FC\u3001CNN\u3001ViT\u7b56\u7565\uff1b\u5728\u51bb\u7ed3\u7684\u57fa\u7840\u6a21\u578b\u7279\u5f81\u4e0a\u8bad\u7ec3\u7d27\u51d1ViT\u5934\uff1b\u6539\u53d8\u8bad\u7ec3\u6570\u636e\u7684\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002", "result": "1) ViT\u7b56\u7565\u6bd4\u540c\u7b49\u89c4\u6a21\u7684CNN/FC\u66f4\u9c81\u68d2\uff0cFM\u7279\u5f81\u8fbe\u5230SOTA\u4f46\u5ef6\u8fdf\u8f83\u9ad8\uff1b2) \u591a\u5e27\u8f93\u5165\u4e0d\u4f18\u4e8e\u6700\u4f73\u5355\u5e27\u57fa\u7ebf\uff1b3) \u6700\u5927\u6027\u80fd\u4e0b\u964d\u6765\u81ea\u4e61\u6751\u2192\u57ce\u5e02\u548c\u767d\u5929\u2192\u591c\u665a\uff08\u5404\u7ea631%\uff09\uff1b4) FM\u7279\u5f81\u7b56\u7565\u57283\u4e2a\u540c\u65f6\u53d8\u5316\u4e0b\u4ecd\u4fdd\u630185%\u4ee5\u4e0a\u6027\u80fd\uff1b5) \u73af\u5883\u53d8\u5316\u5f71\u54cd\u975e\u7ebf\u6027\uff1b6) \u51ac\u5b63/\u96ea\u5929\u8bad\u7ec3\u5bf9\u5355\u56e0\u7d20\u53d8\u5316\u6700\u9c81\u68d2\uff1b7) \u589e\u52a0\u6570\u636e\u8f68\u8ff9\u53ef\u63d0\u5347\u9c81\u68d2\u6027\uff1b8) \u591a\u73af\u5883\u8bad\u7ec3\u53ef\u63d0\u5347\u8986\u76d6\u8303\u56f4\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u81ea\u52a8\u9a7e\u9a76OOD\u9c81\u68d2\u6027\u7684\u53ef\u64cd\u4f5c\u8bbe\u8ba1\u89c4\u5219\uff1a\u4f18\u5148\u4f7f\u7528ViT\u67b6\u6784\u548cFM\u7279\u5f81\uff0c\u5173\u6ce8\u6570\u636e\u591a\u6837\u6027\u548c\u89c4\u6a21\uff0c\u7406\u89e3\u73af\u5883\u53d8\u5316\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\u6548\u5e94\uff0c\u9488\u5bf9\u7279\u5b9a\u73af\u5883\u53d8\u5316\u8fdb\u884c\u9488\u5bf9\u6027\u8bad\u7ec3\uff0c\u5e73\u8861\u5355\u73af\u5883\u5cf0\u503c\u6027\u80fd\u4e0e\u591a\u73af\u5883\u8986\u76d6\u8303\u56f4\u3002"}}
{"id": "2602.09021", "pdf": "https://arxiv.org/pdf/2602.09021", "abs": "https://arxiv.org/abs/2602.09021", "authors": ["Checheng Yu", "Chonghao Sima", "Gangcheng Jiang", "Hai Zhang", "Haoguang Mai", "Hongyang Li", "Huijie Wang", "Jin Chen", "Kaiyang Wu", "Li Chen", "Lirui Zhao", "Modi Shi", "Ping Luo", "Qingwen Bu", "Shijia Peng", "Tianyu Li", "Yibo Yuan"], "title": "$\u03c7_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $\u03c7_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $\u03c7_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $\u03c7_{0}$ surpasses the state-of-the-art $\u03c0_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.", "AI": {"tldr": "\u63d0\u51fa\u03c7\u2080\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u7b97\u672f\u3001\u9636\u6bb5\u4f18\u52bf\u4f30\u8ba1\u548c\u8bad\u7ec3-\u90e8\u7f72\u5bf9\u9f50\u4e09\u4e2a\u6280\u672f\u652f\u67f1\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5b9e\u73b024\u5c0f\u65f6\u8fde\u7eed\u8fd0\u884c\u7684\u9ad8\u53ef\u9760\u6027\u53cc\u624b\u673a\u5668\u4eba\u670d\u88c5\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u9ad8\u53ef\u9760\u6027\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u548c\u8ba1\u7b97\uff0c\u4f46\u771f\u6b63\u7684\u74f6\u9888\u662f\u6f14\u793a\u5206\u5e03\u3001\u7b56\u7565\u5b66\u4e60\u504f\u7f6e\u548c\u6267\u884c\u5206\u5e03\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u3002", "method": "1) \u6a21\u578b\u7b97\u672f\uff1a\u6743\u91cd\u7a7a\u95f4\u5408\u5e76\u7b56\u7565\uff0c\u9ad8\u6548\u5438\u6536\u4e0d\u540c\u6f14\u793a\u7684\u591a\u6837\u5316\u5206\u5e03\uff1b2) \u9636\u6bb5\u4f18\u52bf\uff1a\u9636\u6bb5\u611f\u77e5\u7684\u4f18\u52bf\u4f30\u8ba1\u5668\uff0c\u63d0\u4f9b\u7a33\u5b9a\u5bc6\u96c6\u7684\u8fdb\u5ea6\u4fe1\u53f7\uff1b3) \u8bad\u7ec3-\u90e8\u7f72\u5bf9\u9f50\uff1a\u901a\u8fc7\u65f6\u7a7a\u589e\u5f3a\u3001\u542f\u53d1\u5f0fDAgger\u6821\u6b63\u548c\u65f6\u95f4\u5206\u5757\u5e73\u6ed1\u6765\u5f25\u5408\u5206\u5e03\u5dee\u8ddd\u3002", "result": "\u03c7\u2080\u4f7f\u53cc\u624b\u673a\u5668\u4eba\u80fd\u591f\u534f\u4f5c\u5b8c\u6210\u670d\u88c5\u64cd\u4f5c\u4efb\u52a1\uff08\u94fa\u5e73\u3001\u6298\u53e0\u3001\u60ac\u6302\uff09\uff0c\u5b9e\u73b024\u5c0f\u65f6\u8fde\u7eed\u8fd0\u884c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u03c7\u2080\u5728\u6210\u529f\u7387\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u03c0\u2080.5\u9ad8\u51fa\u8fd1250%\uff0c\u4ec5\u970020\u5c0f\u65f6\u6570\u636e\u548c8\u4e2aA100 GPU\u3002", "conclusion": "\u03c7\u2080\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u5206\u5e03\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4ee5\u8d44\u6e90\u9ad8\u6548\u7684\u65b9\u5f0f\u5b9e\u73b0\u4e86\u751f\u4ea7\u7ea7\u522b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u9c81\u68d2\u6027\uff0c\u4e3a\u957f\u65f6\u7a0b\u591a\u9636\u6bb5\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
