{"id": "2505.04672", "pdf": "https://arxiv.org/pdf/2505.04672", "abs": "https://arxiv.org/abs/2505.04672", "authors": ["Lucas Sanc\u00e9r\u00e9", "Carina Lorenz", "Doris Helbig", "Oana-Diana Persa", "Sonja Dengler", "Alexander Kreuter", "Martim Laimer", "Anne Fr\u00f6hlich", "Jennifer Landsberg", "Johannes Br\u00e4gelmann", "Katarzyna Bozek"], "title": "Histo-Miner: Deep Learning based Tissue Features Extraction Pipeline from H&E Whole Slide Images of Cutaneous Squamous Cell Carcinoma", "categories": ["cs.CV", "q-bio.QM"], "comment": "31 pages including supplement, 5 core figures, 5 supplement figures", "summary": "Recent advancements in digital pathology have enabled comprehensive analysis\nof Whole-Slide Images (WSI) from tissue samples, leveraging high-resolution\nmicroscopy and computational capabilities. Despite this progress, there is a\nlack of labeled datasets and open source pipelines specifically tailored for\nanalysis of skin tissue. Here we propose Histo-Miner, a deep learning-based\npipeline for analysis of skin WSIs and generate two datasets with labeled\nnuclei and tumor regions. We develop our pipeline for the analysis of patient\nsamples of cutaneous squamous cell carcinoma (cSCC), a frequent non-melanoma\nskin cancer. Utilizing the two datasets, comprising 47,392 annotated cell\nnuclei and 144 tumor-segmented WSIs respectively, both from cSCC patients,\nHisto-Miner employs convolutional neural networks and vision transformers for\nnucleus segmentation and classification as well as tumor region segmentation.\nPerformance of trained models positively compares to state of the art with\nmulti-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation,\nmacro-averaged F1 of 0.832 for nucleus classification and mean Intersection\nover Union (mIoU) of 0.884 for tumor region segmentation. From these\npredictions we generate a compact feature vector summarizing tissue morphology\nand cellular interactions, which can be used for various downstream tasks.\nHere, we use Histo-Miner to predict cSCC patient response to immunotherapy\nbased on pre-treatment WSIs from 45 patients. Histo-Miner identifies\npercentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor\nvicinity and the distances between granulocytes and plasma cells in tumors as\npredictive features for therapy response. This highlights the applicability of\nHisto-Miner to clinically relevant scenarios, providing direct interpretation\nof the classification and insights into the underlying biology.", "AI": {"tldr": "Histo-Miner\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u5206\u6790\u76ae\u80a4\u7ec4\u7ec7\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\uff0c\u5e76\u751f\u6210\u4e86\u4e24\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\u3002\u5b83\u5728\u76ae\u80a4\u9cde\u72b6\u7ec6\u80de\u764c\uff08cSCC\uff09\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u7528\u4e8e\u9884\u6d4b\u514d\u75ab\u6cbb\u7597\u53cd\u5e94\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u76ae\u80a4\u7ec4\u7ec7\u7684\u6807\u6ce8\u6570\u636e\u96c6\u548c\u5f00\u6e90\u5206\u6790\u7ba1\u9053\uff0c\u7279\u522b\u662f\u5728cSCC\u9886\u57df\u3002", "method": "\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff0c\u5f00\u53d1\u4e86Histo-Miner\u7ba1\u9053\uff0c\u7528\u4e8e\u7ec6\u80de\u6838\u5206\u5272\u4e0e\u5206\u7c7b\u4ee5\u53ca\u80bf\u7624\u533a\u57df\u5206\u5272\u3002", "result": "\u6a21\u578b\u6027\u80fd\u4f18\u5f02\uff0c\u7ec6\u80de\u6838\u5206\u5272mPQ\u4e3a0.569\uff0c\u5206\u7c7bF1\u4e3a0.832\uff0c\u80bf\u7624\u5206\u5272mIoU\u4e3a0.884\u3002\u9884\u6d4b\u7279\u5f81\u53ef\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff0c\u5982\u514d\u75ab\u6cbb\u7597\u53cd\u5e94\u9884\u6d4b\u3002", "conclusion": "Histo-Miner\u5728\u4e34\u5e8a\u76f8\u5173\u573a\u666f\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u5206\u7c7b\u63d0\u4f9b\u4e86\u76f4\u63a5\u89e3\u91ca\uff0c\u5e76\u63ed\u793a\u4e86\u6f5c\u5728\u7684\u751f\u7269\u5b66\u673a\u5236\u3002"}}
{"id": "2505.04713", "pdf": "https://arxiv.org/pdf/2505.04713", "abs": "https://arxiv.org/abs/2505.04713", "authors": ["Luis F. Gomez", "Gonzalo Garrido-Lopez", "Julian Fierrez", "Aythami Morales", "Ruben Tolosana", "Javier Rueda", "Enrique Navarro"], "title": "Comparison of Visual Trackers for Biomechanical Analysis of Running", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint of the paper presented to the Third Workshop on Learning\n  with Few or Without Annotated Face, Body, and Gesture Data on 19th IEEE\n  Conference on Automatic Face and Gesture Recognition 2025", "summary": "Human pose estimation has witnessed significant advancements in recent years,\nmainly due to the integration of deep learning models, the availability of a\nvast amount of data, and large computational resources. These developments have\nled to highly accurate body tracking systems, which have direct applications in\nsports analysis and performance evaluation.\n  This work analyzes the performance of six trackers: two point trackers and\nfour joint trackers for biomechanical analysis in sprints. The proposed\nframework compares the results obtained from these pose trackers with the\nmanual annotations of biomechanical experts for more than 5870 frames. The\nexperimental framework employs forty sprints from five professional runners,\nfocusing on three key angles in sprint biomechanics: trunk inclination, hip\nflex extension, and knee flex extension. We propose a post-processing module\nfor outlier detection and fusion prediction in the joint angles.\n  The experimental results demonstrate that using joint-based models yields\nroot mean squared errors ranging from 11.41{\\deg} to 4.37{\\deg}. When\nintegrated with the post-processing modules, these errors can be reduced to\n6.99{\\deg} and 3.88{\\deg}, respectively. The experimental findings suggest that\nhuman pose tracking approaches can be valuable resources for the biomechanical\nanalysis of running. However, there is still room for improvement in\napplications where high accuracy is required.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u516d\u79cd\u59ff\u6001\u8ddf\u8e2a\u5668\u5728\u77ed\u8dd1\u751f\u7269\u529b\u5b66\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u5904\u7406\u6a21\u5757\u4ee5\u51cf\u5c11\u8bef\u5dee\uff0c\u7ed3\u679c\u8868\u660e\u57fa\u4e8e\u5173\u8282\u7684\u6a21\u578b\u5728\u751f\u7269\u529b\u5b66\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u5728\u77ed\u8dd1\u751f\u7269\u529b\u5b66\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7ed3\u679c\u5bf9\u6bd4\uff0c\u4ee5\u9a8c\u8bc1\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u516d\u79cd\u59ff\u6001\u8ddf\u8e2a\u5668\uff08\u4e24\u79cd\u70b9\u8ddf\u8e2a\u5668\u548c\u56db\u79cd\u5173\u8282\u8ddf\u8e2a\u5668\uff09\u5206\u67905870\u5e27\u6570\u636e\uff0c\u63d0\u51fa\u540e\u5904\u7406\u6a21\u5757\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u548c\u89d2\u5ea6\u878d\u5408\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u5173\u8282\u7684\u6a21\u578b\u8bef\u5dee\u8303\u56f4\u4e3a11.41\u00b0\u81f34.37\u00b0\uff0c\u540e\u5904\u7406\u540e\u8bef\u5dee\u964d\u81f36.99\u00b0\u548c3.88\u00b0\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u59ff\u6001\u8ddf\u8e2a\u65b9\u6cd5\u5728\u77ed\u8dd1\u751f\u7269\u529b\u5b66\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u9ad8\u7cbe\u5ea6\u5e94\u7528\u4e2d\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2505.04718", "pdf": "https://arxiv.org/pdf/2505.04718", "abs": "https://arxiv.org/abs/2505.04718", "authors": ["Divyansh Srivastava", "Xiang Zhang", "He Wen", "Chenru Wen", "Zhuowen Tu"], "title": "Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout\ngeneration pipeline for natural scenes. Prior scene layout generation methods\nare either closed-vocabulary or use proprietary large language models for\nopen-vocabulary generation, limiting their modeling capabilities and broader\napplicability in controllable image generation. In this work, we propose to use\nlightweight open-source language models to obtain scene elements from text\nprompts and a novel aspect-aware diffusion Transformer architecture trained in\nan open-vocabulary manner for conditional layout generation. Extensive\nexperiments demonstrate that LayouSyn outperforms existing methods and achieves\nstate-of-the-art performance on challenging spatial and numerical reasoning\nbenchmarks. Additionally, we present two applications of LayouSyn. First, we\nshow that coarse initialization from large language models can be seamlessly\ncombined with our method to achieve better results. Second, we present a\npipeline for adding objects to images, demonstrating the potential of LayouSyn\nin image editing applications.", "AI": {"tldr": "LayouSyn\u662f\u4e00\u79cd\u65b0\u7684\u6587\u672c\u5230\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563Transformer\u67b6\u6784\uff0c\u5728\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u5e03\u5c40\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u573a\u666f\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u8bcd\u6c47\u5c01\u95ed\uff0c\u8981\u4e48\u4f9d\u8d56\u4e13\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u5efa\u6a21\u80fd\u529b\u548c\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u4ece\u6587\u672c\u63d0\u793a\u4e2d\u63d0\u53d6\u573a\u666f\u5143\u7d20\uff0c\u5e76\u7ed3\u5408\u65b0\u578b\u7684aspect-aware\u6269\u6563Transformer\u67b6\u6784\u8fdb\u884c\u6761\u4ef6\u5e03\u5c40\u751f\u6210\u3002", "result": "LayouSyn\u5728\u7a7a\u95f4\u548c\u6570\u503c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "LayouSyn\u901a\u8fc7\u5f00\u6e90\u6a21\u578b\u548c\u65b0\u578b\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5f00\u653e\u7684\u573a\u666f\u5e03\u5c40\u751f\u6210\uff0c\u6269\u5c55\u4e86\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2505.04720", "pdf": "https://arxiv.org/pdf/2505.04720", "abs": "https://arxiv.org/abs/2505.04720", "authors": ["Evangelia Christodoulou", "Annika Reinke", "Pascaline Andr\u00e8", "Patrick Godau", "Piotr Kalinowski", "Rola Houhou", "Selen Erkan", "Carole H. Sudre", "Ninon Burgos", "Sofi\u00e8ne Boutaj", "Sophie Loizillon", "Ma\u00eblys Solal", "Veronika Cheplygina", "Charles Heitz", "Michal Kozubek", "Michela Antonelli", "Nicola Rieke", "Antoine Gilson", "Leon D. Mayer", "Minu D. Tizabi", "M. Jorge Cardoso", "Amber Simpson", "Annette Kopp-Schneider", "Ga\u00ebl Varoquaux", "Olivier Colliot", "Lena Maier-Hein"], "title": "False Promises in Medical Imaging AI? Assessing Validity of Outperformance Claims", "categories": ["cs.CV"], "comment": null, "summary": "Performance comparisons are fundamental in medical imaging Artificial\nIntelligence (AI) research, often driving claims of superiority based on\nrelative improvements in common performance metrics. However, such claims\nfrequently rely solely on empirical mean performance. In this paper, we\ninvestigate whether newly proposed methods genuinely outperform the state of\nthe art by analyzing a representative cohort of medical imaging papers. We\nquantify the probability of false claims based on a Bayesian approach that\nleverages reported results alongside empirically estimated model congruence to\nestimate whether the relative ranking of methods is likely to have occurred by\nchance. According to our results, the majority (>80%) of papers claims\noutperformance when introducing a new method. Our analysis further revealed a\nhigh probability (>5%) of false outperformance claims in 86% of classification\npapers and 53% of segmentation papers. These findings highlight a critical flaw\nin current benchmarking practices: claims of outperformance in medical imaging\nAI are frequently unsubstantiated, posing a risk of misdirecting future\nresearch efforts.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u533b\u5b66\u5f71\u50cfAI\u7814\u7a76\u4e2d\u6027\u80fd\u6bd4\u8f83\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u591a\u6570\u8bba\u6587\u58f0\u79f0\u65b0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4f46\u5b9e\u9645\u5b58\u5728\u9ad8\u6982\u7387\u7684\u865a\u5047\u58f0\u79f0\u3002", "motivation": "\u63ed\u793a\u533b\u5b66\u5f71\u50cfAI\u7814\u7a76\u4e2d\u57fa\u4e8e\u6027\u80fd\u6bd4\u8f83\u7684\u865a\u5047\u58f0\u79f0\u95ee\u9898\uff0c\u4ee5\u6539\u8fdb\u672a\u6765\u7814\u7a76\u5b9e\u8df5\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u5206\u6790\u4ee3\u8868\u6027\u533b\u5b66\u5f71\u50cf\u8bba\u6587\uff0c\u91cf\u5316\u865a\u5047\u58f0\u79f0\u7684\u6982\u7387\u3002", "result": "\u8d85\u8fc780%\u7684\u8bba\u6587\u58f0\u79f0\u65b0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c86%\u7684\u5206\u7c7b\u8bba\u6587\u548c53%\u7684\u5206\u5272\u8bba\u6587\u5b58\u5728\u9ad8\u6982\u7387\uff08>5%\uff09\u7684\u865a\u5047\u58f0\u79f0\u3002", "conclusion": "\u5f53\u524d\u533b\u5b66\u5f71\u50cfAI\u7684\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u865a\u5047\u58f0\u79f0\u53ef\u80fd\u8bef\u5bfc\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.04740", "pdf": "https://arxiv.org/pdf/2505.04740", "abs": "https://arxiv.org/abs/2505.04740", "authors": ["Sainath Dey", "Mitul Goswami", "Jashika Sethi", "Prasant Kumar Pattnaik"], "title": "Hyb-KAN ViT: Hybrid Kolmogorov-Arnold Networks Augmented Vision Transformer", "categories": ["cs.CV"], "comment": null, "summary": "This study addresses the inherent limitations of Multi-Layer Perceptrons\n(MLPs) in Vision Transformers (ViTs) by introducing Hybrid Kolmogorov-Arnold\nNetwork (KAN)-ViT (Hyb-KAN ViT), a novel framework that integrates\nwavelet-based spectral decomposition and spline-optimized activation functions,\nprior work has failed to focus on the prebuilt modularity of the ViT\narchitecture and integration of edge detection capabilities of Wavelet\nfunctions. We propose two key modules: Efficient-KAN (Eff-KAN), which replaces\nMLP layers with spline functions and Wavelet-KAN (Wav-KAN), leveraging\northogonal wavelet transforms for multi-resolution feature extraction. These\nmodules are systematically integrated in ViT encoder layers and classification\nheads to enhance spatial-frequency modeling while mitigating computational\nbottlenecks. Experiments on ImageNet-1K (Image Recognition), COCO (Object\nDetection and Instance Segmentation), and ADE20K (Semantic Segmentation)\ndemonstrate state-of-the-art performance with Hyb-KAN ViT. Ablation studies\nvalidate the efficacy of wavelet-driven spectral priors in segmentation and\nspline-based efficiency in detection tasks. The framework establishes a new\nparadigm for balancing parameter efficiency and multi-scale representation in\nvision architectures.", "AI": {"tldr": "\u63d0\u51faHyb-KAN ViT\u6846\u67b6\uff0c\u7ed3\u5408\u5c0f\u6ce2\u8c31\u5206\u89e3\u548c\u6837\u6761\u4f18\u5316\u6fc0\u6d3b\u51fd\u6570\uff0c\u6539\u8fdbViT\u4e2d\u7684MLP\u5c42\uff0c\u63d0\u5347\u591a\u5c3a\u5ea6\u8868\u793a\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3ViT\u4e2dMLP\u5c42\u7684\u5c40\u9650\u6027\uff0c\u5229\u7528\u5c0f\u6ce2\u51fd\u6570\u8fb9\u7f18\u68c0\u6d4b\u80fd\u529b\u548cViT\u6a21\u5757\u5316\u7279\u6027\u3002", "method": "\u5f15\u5165Eff-KAN\uff08\u6837\u6761\u51fd\u6570\u66ff\u4ee3MLP\uff09\u548cWav-KAN\uff08\u5c0f\u6ce2\u53d8\u6362\u591a\u5206\u8fa8\u7387\u7279\u5f81\u63d0\u53d6\uff09\uff0c\u96c6\u6210\u5230ViT\u7f16\u7801\u5668\u548c\u5206\u7c7b\u5934\u3002", "result": "\u5728ImageNet-1K\u3001COCO\u548cADE20K\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1\u5c0f\u6ce2\u8c31\u5148\u9a8c\u548c\u6837\u6761\u6548\u7387\u3002", "conclusion": "Hyb-KAN ViT\u4e3a\u89c6\u89c9\u67b6\u6784\u5e73\u8861\u53c2\u6570\u6548\u7387\u548c\u591a\u5c3a\u5ea6\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.04758", "pdf": "https://arxiv.org/pdf/2505.04758", "abs": "https://arxiv.org/abs/2505.04758", "authors": ["Songsong Duan", "Xi Yang", "Nannan Wang", "Xinbo Gao"], "title": "Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective", "categories": ["cs.CV"], "comment": "Accepted by TIP 2025", "summary": "Current RGB-D methods usually leverage large-scale backbones to improve\naccuracy but sacrifice efficiency. Meanwhile, several existing lightweight\nmethods are difficult to achieve high-precision performance. To balance the\nefficiency and performance, we propose a Speed-Accuracy Tradeoff Network\n(SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth\nquality, modality fusion, and feature representation. Concerning depth quality,\nwe introduce the Depth Anything Model to generate high-quality depth maps,which\neffectively alleviates the multi-modal gaps in the current datasets. For\nmodality fusion, we propose a Decoupled Attention Module (DAM) to explore the\nconsistency within and between modalities. Here, the multi-modal features are\ndecoupled into dual-view feature vectors to project discriminable information\nof feature maps. For feature representation, we develop a Dual Information\nRepresentation Module (DIRM) with a bi-directional inverted framework to\nenlarge the limited feature space generated by the lightweight backbones. DIRM\nmodels texture features and saliency features to enrich feature space, and\nemploy two-way prediction heads to optimal its parameters through a\nbi-directional backpropagation. Finally, we design a Dual Feature Aggregation\nModule (DFAM) in the decoder to aggregate texture and saliency features.\nExtensive experiments on five public RGB-D SOD datasets indicate that the\nproposed SATNet excels state-of-the-art (SOTA) CNN-based heavyweight models and\nachieves a lightweight framework with 5.2 M parameters and 415 FPS.", "AI": {"tldr": "\u63d0\u51faSATNet\u7f51\u7edc\uff0c\u901a\u8fc7\u6df1\u5ea6\u8d28\u91cf\u3001\u6a21\u6001\u878d\u5408\u548c\u7279\u5f81\u8868\u793a\u4e09\u65b9\u9762\u5e73\u8861RGB-D\u663e\u8457\u6027\u68c0\u6d4b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RGB-D\u65b9\u6cd5\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u5927\u578b\u9aa8\u5e72\u7f51\u7edc\u6548\u7387\u4f4e\u3002", "method": "\u5f15\u5165Depth Anything Model\u63d0\u5347\u6df1\u5ea6\u8d28\u91cf\uff1b\u63d0\u51faDAM\u6a21\u5757\u89e3\u8026\u591a\u6a21\u6001\u7279\u5f81\uff1b\u5f00\u53d1DIRM\u6a21\u5757\u53cc\u5411\u6269\u5c55\u7279\u5f81\u7a7a\u95f4\uff1b\u8bbe\u8ba1DFAM\u6a21\u5757\u805a\u5408\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eSOTA\u6a21\u578b\uff0c\u53c2\u6570\u4ec55.2M\uff0c\u901f\u5ea6\u8fbe415FPS\u3002", "conclusion": "SATNet\u6210\u529f\u5e73\u8861\u4e86\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u8f7b\u91cf\u7ea7RGB-D\u663e\u8457\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2505.04769", "pdf": "https://arxiv.org/pdf/2505.04769", "abs": "https://arxiv.org/abs/2505.04769", "authors": ["Ranjan Sapkota", "Yang Cao", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "Vision-Language-Action Models: Concepts, Progress, Applications and Challenges", "categories": ["cs.CV"], "comment": "36 pages, 18 Figures, 4 Tables", "summary": "Vision-Language-Action (VLA) models mark a transformative advancement in\nartificial intelligence, aiming to unify perception, natural language\nunderstanding, and embodied action within a single computational framework.\nThis foundational review presents a comprehensive synthesis of recent\nadvancements in Vision-Language-Action models, systematically organized across\nfive thematic pillars that structure the landscape of this rapidly evolving\nfield. We begin by establishing the conceptual foundations of VLA systems,\ntracing their evolution from cross-modal learning architectures to generalist\nagents that tightly integrate vision-language models (VLMs), action planners,\nand hierarchical controllers. Our methodology adopts a rigorous literature\nreview framework, covering over 80 VLA models published in the past three\nyears. Key progress areas include architectural innovations,\nparameter-efficient training strategies, and real-time inference accelerations.\nWe explore diverse application domains such as humanoid robotics, autonomous\nvehicles, medical and industrial robotics, precision agriculture, and augmented\nreality navigation. The review further addresses major challenges across\nreal-time control, multimodal action representation, system scalability,\ngeneralization to unseen tasks, and ethical deployment risks. Drawing from the\nstate-of-the-art, we propose targeted solutions including agentic AI\nadaptation, cross-embodiment generalization, and unified neuro-symbolic\nplanning. In our forward-looking discussion, we outline a future roadmap where\nVLA models, VLMs, and agentic AI converge to power socially aligned, adaptive,\nand general-purpose embodied agents. This work serves as a foundational\nreference for advancing intelligent, real-world robotics and artificial general\nintelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language\nModels", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u5176\u6982\u5ff5\u57fa\u7840\u3001\u67b6\u6784\u521b\u65b0\u3001\u5e94\u7528\u9886\u57df\u53ca\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u65e8\u5728\u7edf\u4e00\u611f\u77e5\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u52a8\u4f5c\u6267\u884c\u7684VLA\u6a21\u578b\u4ee3\u8868\u4e86\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u91cd\u5927\u7a81\u7834\uff0c\u672c\u6587\u8bd5\u56fe\u7cfb\u7edf\u68b3\u7406\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u4e0e\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e25\u683c\u7684\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u8fc7\u53bb\u4e09\u5e74\u53d1\u8868\u768480\u591a\u4e2aVLA\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u67b6\u6784\u521b\u65b0\u3001\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\u548c\u5b9e\u65f6\u63a8\u7406\u52a0\u901f\u3002", "result": "\u603b\u7ed3\u4e86VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u5b9e\u65f6\u63a7\u5236\u3001\u6cdb\u5316\u80fd\u529b\u7b49\u6311\u6218\u7684\u65b9\u6848\u3002", "conclusion": "VLA\u6a21\u578b\u4e0e\u4ee3\u7406AI\u7684\u878d\u5408\u5c06\u63a8\u52a8\u667a\u80fd\u673a\u5668\u4eba\u548c\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u672c\u6587\u4e3a\u5176\u63d0\u4f9b\u4e86\u57fa\u7840\u53c2\u8003\u3002"}}
{"id": "2505.04787", "pdf": "https://arxiv.org/pdf/2505.04787", "abs": "https://arxiv.org/abs/2505.04787", "authors": ["Sriram Mandalika", "Harsha Vardhan", "Athira Nambiar"], "title": "Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Submitted to the 28th European Conference on Artificial Intelligence\n  (ECAI-2025)", "summary": "Continual Learning entails progressively acquiring knowledge from new data\nwhile retaining previously acquired knowledge, thereby mitigating\n``Catastrophic Forgetting'' in neural networks. Our work presents a novel\nuncertainty-driven Unsupervised Continual Learning framework using Generative\nReplay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture\nefficiently uses unlabelled and synthetic labelled data in a balanced\nproportion using a cluster-level uncertainty-driven feedback mechanism and a\nVLM-powered generative replay module. Unlike traditional memory-buffer methods\nthat depend on pretrained models and pseudo-labels, our R2R framework operates\nwithout any prior training. It leverages visual features from unlabeled data\nand adapts continuously using clustering-based uncertainty estimation coupled\nwith dynamic thresholding. Concurrently, a generative replay mechanism along\nwith DeepSeek-R1 powered CLIP VLM produces labelled synthetic data\nrepresentative of past experiences, resembling biological visual thinking that\nreplays memory to remember and act in new, unseen tasks. Extensive experimental\nanalyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and\nTinyImageNet datasets. Our proposed R2R approach improves knowledge retention,\nachieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%,\n59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u65e0\u76d1\u7763\u6301\u7eed\u5b66\u4e60\u6846\u67b6R2R\uff0c\u901a\u8fc7\u751f\u6210\u56de\u653e\u548c\u805a\u7c7b\u7ea7\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u65e0\u9700\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u548c\u5408\u6210\u6807\u8bb0\u6570\u636e\u3002", "method": "\u91c7\u7528\u805a\u7c7b\u7ea7\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u53cd\u9988\u673a\u5236\u548cVLM\u652f\u6301\u7684\u751f\u6210\u56de\u653e\u6a21\u5757\uff0c\u52a8\u6001\u9002\u5e94\u65b0\u4efb\u52a1\u5e76\u751f\u6210\u4ee3\u8868\u6027\u5408\u6210\u6570\u636e\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u7b49\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098.13%\u300173.06%\u7b49\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd54.36%\u3002", "conclusion": "R2R\u6846\u67b6\u5728\u65e0\u76d1\u7763\u6301\u7eed\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.04788", "pdf": "https://arxiv.org/pdf/2505.04788", "abs": "https://arxiv.org/abs/2505.04788", "authors": ["Bangyan Liao", "Zhenjun Zhao", "Haoang Li", "Yi Zhou", "Yingping Zeng", "Hao Li", "Peidong Liu"], "title": "Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 as Award Candidate & Oral Presentation. The\n  first two authors contributed equally to this work. Code:\n  https://github.com/WU-CVGL/GlobustVP", "summary": "Determining the vanishing points (VPs) in a Manhattan world, as a fundamental\ntask in many 3D vision applications, consists of jointly inferring the line-VP\nassociation and locating each VP. Existing methods are, however, either\nsub-optimal solvers or pursuing global optimality at a significant cost of\ncomputing time. In contrast to prior works, we introduce convex relaxation\ntechniques to solve this task for the first time. Specifically, we employ a\n``soft'' association scheme, realized via a truncated multi-selection error,\nthat allows for joint estimation of VPs' locations and line-VP associations.\nThis approach leads to a primal problem that can be reformulated into a\nquadratically constrained quadratic programming (QCQP) problem, which is then\nrelaxed into a convex semidefinite programming (SDP) problem. To solve this SDP\nproblem efficiently, we present a globally optimal outlier-robust iterative\nsolver (called \\textbf{GlobustVP}), which independently searches for one VP and\nits associated lines in each iteration, treating other lines as outliers. After\neach independent update of all VPs, the mutual orthogonality between the three\nVPs in a Manhattan world is reinforced via local refinement. Extensive\nexperiments on both synthetic and real-world data demonstrate that\n\\textbf{GlobustVP} achieves a favorable balance between efficiency, robustness,\nand global optimality compared to previous works. The code is publicly\navailable at https://github.com/WU-CVGL/GlobustVP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51f8\u677e\u5f1b\u6280\u672f\u7684\u65b9\u6cd5GlobustVP\uff0c\u7528\u4e8e\u5728\u66fc\u54c8\u987f\u4e16\u754c\u4e2d\u8054\u5408\u4f30\u8ba1\u6d88\u5931\u70b9\u4f4d\u7f6e\u548c\u7ebf-VP\u5173\u8054\uff0c\u5e73\u8861\u4e86\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u5168\u5c40\u6700\u4f18\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u662f\u6b21\u4f18\u89e3\u6cd5\uff0c\u8981\u4e48\u8ffd\u6c42\u5168\u5c40\u6700\u4f18\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8f6f\u5173\u8054\u65b9\u6848\u548c\u622a\u65ad\u591a\u9009\u62e9\u8bef\u5dee\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3aQCQP\u5e76\u677e\u5f1b\u4e3a\u51f8SDP\u95ee\u9898\uff0c\u901a\u8fc7\u8fed\u4ee3\u6c42\u89e3\u5668GlobustVP\u72ec\u7acb\u66f4\u65b0\u6bcf\u4e2aVP\u53ca\u5176\u5173\u8054\u7ebf\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGlobustVP\u5728\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u5168\u5c40\u6700\u4f18\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GlobustVP\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u66fc\u54c8\u987f\u4e16\u754c\u4e2d\u7684\u6d88\u5931\u70b9\u4f30\u8ba1\u4efb\u52a1\u3002"}}
{"id": "2505.04793", "pdf": "https://arxiv.org/pdf/2505.04793", "abs": "https://arxiv.org/abs/2505.04793", "authors": ["Kailash A. Hambarde", "Nzakiese Mbongo", "Pavan Kumar MP", "Satish Mekewad", "Carolina Fernandes", "G\u00f6khan Silahtaro\u011flu", "Alice Nithya", "Pawan Wasnik", "MD. Rashidunnabi", "Pranita Samale", "Hugo Proen\u00e7a"], "title": "DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Person reidentification (ReID) technology has been considered to perform\nrelatively well under controlled, ground-level conditions, but it breaks down\nwhen deployed in challenging real-world settings. Evidently, this is due to\nextreme data variability factors such as resolution, viewpoint changes, scale\nvariations, occlusions, and appearance shifts from clothing or session drifts.\nMoreover, the publicly available data sets do not realistically incorporate\nsuch kinds and magnitudes of variability, which limits the progress of this\ntechnology. This paper introduces DetReIDX, a large-scale aerial-ground person\ndataset, that was explicitly designed as a stress test to ReID under real-world\nconditions. DetReIDX is a multi-session set that includes over 13 million\nbounding boxes from 509 identities, collected in seven university campuses from\nthree continents, with drone altitudes between 5.8 and 120 meters. More\nimportant, as a key novelty, DetReIDX subjects were recorded in (at least) two\nsessions on different days, with changes in clothing, daylight and location,\nmaking it suitable to actually evaluate long-term person ReID. Plus, data were\nannotated from 16 soft biometric attributes and multitask labels for detection,\ntracking, ReID, and action recognition. In order to provide empirical evidence\nof DetReIDX usefulness, we considered the specific tasks of human detection and\nReID, where SOTA methods catastrophically degrade performance (up to 80% in\ndetection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs\nconditions. The dataset, annotations, and official evaluation protocols are\npublicly available at https://www.it.ubi.pt/DetReIDX/", "AI": {"tldr": "DetReIDX\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u7a7a\u5730\u884c\u4eba\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6d4b\u8bd5\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u884c\u4eba\u91cd\u8bc6\u522b\uff08ReID\uff09\u6280\u672f\uff0c\u5305\u542b\u591a\u4f1a\u8bdd\u3001\u591a\u573a\u666f\u6570\u636e\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709ReID\u6280\u672f\u5728\u771f\u5b9e\u4e16\u754c\u6781\u7aef\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u516c\u5f00\u6570\u636e\u96c6\u672a\u80fd\u5145\u5206\u6a21\u62df\u8fd9\u4e9b\u6311\u6218\u3002DetReIDX\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u65e0\u4eba\u673a\u5728\u591a\u4e2a\u5927\u5b66\u6821\u56ed\u6536\u96c6509\u4e2a\u8eab\u4efd\u7684\u8d851300\u4e07\u8fb9\u754c\u6846\u6570\u636e\uff0c\u6db5\u76d6\u4e0d\u540c\u9ad8\u5ea6\u3001\u4f1a\u8bdd\u548c\u670d\u88c5\u53d8\u5316\uff0c\u5e76\u6807\u6ce8\u591a\u4efb\u52a1\u6807\u7b7e\u548c\u8f6f\u751f\u7269\u7279\u5f81\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u73b0\u6709SOTA\u65b9\u6cd5\u5728DetReIDX\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u68c0\u6d4b\u51c6\u786e\u7387\u4e0b\u964d80%\uff0cRank-1 ReID\u4e0b\u964d70%\u4ee5\u4e0a\uff09\u3002", "conclusion": "DetReIDX\u4e3a\u771f\u5b9e\u4e16\u754cReID\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.04835", "pdf": "https://arxiv.org/pdf/2505.04835", "abs": "https://arxiv.org/abs/2505.04835", "authors": ["Shashank Agnihotri", "David Schader", "Nico Sharei", "Mehmet Ege Ka\u00e7ar", "Margret Keuper"], "title": "Are Synthetic Corruptions A Reliable Proxy For Real-World Corruptions?", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 Workshop on Synthetic Data for Computer Vision", "summary": "Deep learning (DL) models are widely used in real-world applications but\nremain vulnerable to distribution shifts, especially due to weather and\nlighting changes. Collecting diverse real-world data for testing the robustness\nof DL models is resource-intensive, making synthetic corruptions an attractive\nalternative for robustness testing. However, are synthetic corruptions a\nreliable proxy for real-world corruptions? To answer this, we conduct the\nlargest benchmarking study on semantic segmentation models, comparing\nperformance on real-world corruptions and synthetic corruptions datasets. Our\nresults reveal a strong correlation in mean performance, supporting the use of\nsynthetic corruptions for robustness evaluation. We further analyze\ncorruption-specific correlations, providing key insights to understand when\nsynthetic corruptions succeed in representing real-world corruptions.\nOpen-source Code:\nhttps://github.com/shashankskagnihotri/benchmarking_robustness/tree/segmentation_david/semantic_segmentation", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5408\u6210\u635f\u574f\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3a\u771f\u5b9e\u4e16\u754c\u635f\u574f\u7684\u53ef\u9760\u66ff\u4ee3\u54c1\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\u4e24\u8005\u5728\u5e73\u5747\u6027\u80fd\u4e0a\u5177\u6709\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u6613\u53d7\u5206\u5e03\u53d8\u5316\uff08\u5982\u5929\u6c14\u548c\u5149\u7167\u53d8\u5316\uff09\u7684\u5f71\u54cd\uff0c\u4f46\u6536\u96c6\u591a\u6837\u5316\u7684\u771f\u5b9e\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u9a8c\u8bc1\u5408\u6210\u635f\u574f\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u635f\u574f\u548c\u5408\u6210\u635f\u574f\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e24\u8005\u5728\u5e73\u5747\u6027\u80fd\u4e0a\u5177\u6709\u5f3a\u76f8\u5173\u6027\uff0c\u652f\u6301\u5408\u6210\u635f\u574f\u7528\u4e8e\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "conclusion": "\u5408\u6210\u635f\u574f\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u53ef\u4ee5\u53ef\u9760\u5730\u4ee3\u8868\u771f\u5b9e\u4e16\u754c\u635f\u574f\uff0c\u4e3a\u9c81\u68d2\u6027\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.04838", "pdf": "https://arxiv.org/pdf/2505.04838", "abs": "https://arxiv.org/abs/2505.04838", "authors": ["Youjia Zhang"], "title": "Seeing Cells Clearly: Evaluating Machine Vision Strategies for Microglia Centroid Detection in 3D Images", "categories": ["cs.CV"], "comment": null, "summary": "Microglia are important cells in the brain, and their shape can tell us a lot\nabout brain health. In this project, I test three different tools for finding\nthe center points of microglia in 3D microscope images. The tools include\nilastik, 3D Morph, and Omnipose. I look at how well each one finds the cells\nand how their results compare. My findings show that each tool sees the cells\nin its own way, and this can affect the kind of information we get from the\nimages.", "AI": {"tldr": "\u6bd4\u8f83\u4e09\u79cd\u5de5\u5177\uff08ilastik\u30013D Morph\u3001Omnipose\uff09\u57283D\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u5c0f\u80f6\u8d28\u7ec6\u80de\u4e2d\u5fc3\u70b9\u7684\u6548\u679c\u3002", "motivation": "\u5c0f\u80f6\u8d28\u7ec6\u80de\u7684\u5f62\u6001\u5bf9\u8111\u5065\u5eb7\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u51c6\u786e\u8bc6\u522b\u5176\u4e2d\u5fc3\u70b9\u6709\u52a9\u4e8e\u83b7\u53d6\u66f4\u7cbe\u786e\u7684\u4fe1\u606f\u3002", "method": "\u6d4b\u8bd5\u5e76\u6bd4\u8f83ilastik\u30013D Morph\u548cOmnipose\u4e09\u79cd\u5de5\u5177\u57283D\u56fe\u50cf\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6bcf\u79cd\u5de5\u5177\u5bf9\u5c0f\u80f6\u8d28\u7ec6\u80de\u7684\u8bc6\u522b\u65b9\u5f0f\u4e0d\u540c\uff0c\u5f71\u54cd\u4ece\u56fe\u50cf\u4e2d\u83b7\u53d6\u7684\u4fe1\u606f\u3002", "conclusion": "\u4e0d\u540c\u5de5\u5177\u7684\u7ed3\u679c\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u6839\u636e\u7814\u7a76\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\u3002"}}
{"id": "2505.04850", "pdf": "https://arxiv.org/pdf/2505.04850", "abs": "https://arxiv.org/abs/2505.04850", "authors": ["Qingyuan Wang", "Guoxin Wang", "Barry Cardiff", "Deepu John"], "title": "ORXE: Orchestrating Experts for Dynamically Configurable Efficiency", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents ORXE, a modular and adaptable framework for achieving\nreal-time configurable efficiency in AI models. By leveraging a collection of\npre-trained experts with diverse computational costs and performance levels,\nORXE dynamically adjusts inference pathways based on the complexity of input\nsamples. Unlike conventional approaches that require complex metamodel\ntraining, ORXE achieves high efficiency and flexibility without complicating\nthe development process. The proposed system utilizes a confidence-based gating\nmechanism to allocate appropriate computational resources for each input. ORXE\nalso supports adjustments to the preference between inference cost and\nprediction performance across a wide range during runtime. We implemented a\ntraining-free ORXE system for image classification tasks, evaluating its\nefficiency and accuracy across various devices. The results demonstrate that\nORXE achieves superior performance compared to individual experts and other\ndynamic models in most cases. This approach can be extended to other\napplications, providing a scalable solution for diverse real-world deployment\nscenarios.", "AI": {"tldr": "ORXE\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u9002\u5e94\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u8def\u5f84\u5b9e\u73b0AI\u6a21\u578b\u7684\u9ad8\u6548\u5b9e\u65f6\u914d\u7f6e\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u5143\u6a21\u578b\u8bad\u7ec3\uff0c\u800cORXE\u65e8\u5728\u7b80\u5316\u5f00\u53d1\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u548c\u7075\u6d3b\u6027\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u4e13\u5bb6\u96c6\u5408\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u95e8\u63a7\u673a\u5236\uff0c\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u652f\u6301\u8fd0\u884c\u65f6\u8c03\u6574\u6210\u672c\u4e0e\u6027\u80fd\u7684\u6743\u8861\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cORXE\u8868\u73b0\u4f18\u4e8e\u5355\u4e2a\u4e13\u5bb6\u548c\u5176\u4ed6\u52a8\u6001\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bbe\u5907\u3002", "conclusion": "ORXE\u4e3a\u591a\u6837\u5316\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04861", "pdf": "https://arxiv.org/pdf/2505.04861", "abs": "https://arxiv.org/abs/2505.04861", "authors": ["Navin Ranjan", "Andreas Savakis"], "title": "Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model", "categories": ["cs.CV"], "comment": "12 pages, 2 Figures", "summary": "The Segment Anything Model (SAM) is a popular vision foundation model;\nhowever, its high computational and memory demands make deployment on\nresource-constrained devices challenging. While Post-Training Quantization\n(PTQ) is a practical approach for reducing computational overhead, existing PTQ\nmethods rely on fixed bit-width quantization, leading to suboptimal accuracy\nand efficiency. To address this limitation, we propose Mix-QSAM, a\nmixed-precision PTQ framework for SAM. First, we introduce a layer-wise\nimportance score, derived using Kullback-Leibler (KL) divergence, to quantify\neach layer's contribution to the model's output. Second, we introduce\ncross-layer synergy, a novel metric based on causal mutual information, to\ncapture dependencies between adjacent layers. This ensures that highly\ninterdependent layers maintain similar bit-widths, preventing abrupt precision\nmismatches that degrade feature propagation and numerical stability. Using\nthese metrics, we formulate an Integer Quadratic Programming (IQP) problem to\ndetermine optimal bit-width allocation under model size and bit-operation\nconstraints, assigning higher precision to critical layers while minimizing\nbit-width in less influential layers. Experimental results demonstrate that\nMix-QSAM consistently outperforms existing PTQ methods on instance segmentation\nand object detection tasks, achieving up to 20% higher average precision under\n6-bit and 4-bit mixed-precision settings, while maintaining computational\nefficiency.", "AI": {"tldr": "Mix-QSAM\u662f\u4e00\u79cd\u6df7\u5408\u7cbe\u5ea6\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316Segment Anything Model (SAM)\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u901a\u8fc7\u5c42\u95f4\u534f\u540c\u548c\u91cd\u8981\u6027\u8bc4\u5206\u5b9e\u73b0\u9ad8\u6548\u91cf\u5316\u3002", "motivation": "SAM\u7684\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\uff0c\u73b0\u6709\u56fa\u5b9a\u4f4d\u5bbd\u91cf\u5316\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMix-QSAM\u6846\u67b6\uff0c\u5229\u7528KL\u6563\u5ea6\u8ba1\u7b97\u5c42\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u5f15\u5165\u8de8\u5c42\u534f\u540c\u5ea6\u91cf\uff0c\u901a\u8fc7\u6574\u6570\u4e8c\u6b21\u89c4\u5212\u4f18\u5316\u4f4d\u5bbd\u5206\u914d\u3002", "result": "\u57286\u4f4d\u548c4\u4f4d\u6df7\u5408\u7cbe\u5ea6\u8bbe\u7f6e\u4e0b\uff0cMix-QSAM\u5728\u5b9e\u4f8b\u5206\u5272\u548c\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5e73\u5747\u7cbe\u5ea6\u63d0\u5347\u9ad8\u8fbe20%\u3002", "conclusion": "Mix-QSAM\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6a21\u578b\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2505.04864", "pdf": "https://arxiv.org/pdf/2505.04864", "abs": "https://arxiv.org/abs/2505.04864", "authors": ["Kanggeon Lee", "Soochahn Lee", "Kyoung Mu Lee"], "title": "Auto-regressive transformation for image alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing methods for image alignment struggle in cases involving\nfeature-sparse regions, extreme scale and field-of-view differences, and large\ndeformations, often resulting in suboptimal accuracy. Robustness to these\nchallenges improves through iterative refinement of the transformation field\nwhile focusing on critical regions in multi-scale image representations. We\nthus propose Auto-Regressive Transformation (ART), a novel method that\niteratively estimates the coarse-to-fine transformations within an\nauto-regressive framework. Leveraging hierarchical multi-scale features, our\nnetwork refines the transformations using randomly sampled points at each\nscale. By incorporating guidance from the cross-attention layer, the model\nfocuses on critical regions, ensuring accurate alignment even in challenging,\nfeature-limited conditions. Extensive experiments across diverse datasets\ndemonstrate that ART significantly outperforms state-of-the-art methods,\nestablishing it as a powerful new method for precise image alignment with broad\napplicability.", "AI": {"tldr": "ART\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u56de\u5f52\u6846\u67b6\u8fed\u4ee3\u4f30\u8ba1\u7c97\u5230\u7ec6\u53d8\u6362\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5bf9\u9f50\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5bf9\u9f50\u65b9\u6cd5\u5728\u7279\u5f81\u7a00\u758f\u533a\u57df\u3001\u6781\u7aef\u5c3a\u5ea6\u548c\u89c6\u91ce\u5dee\u5f02\u4ee5\u53ca\u5927\u53d8\u5f62\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAuto-Regressive Transformation (ART)\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u968f\u673a\u91c7\u6837\u70b9\u8fed\u4ee3\u4f18\u5316\u53d8\u6362\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u805a\u7126\u5173\u952e\u533a\u57df\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cART\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ART\u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u7cbe\u786e\u56fe\u50cf\u5bf9\u9f50\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.04877", "pdf": "https://arxiv.org/pdf/2505.04877", "abs": "https://arxiv.org/abs/2505.04877", "authors": ["Lianbo Ma", "Jianlun Ma", "Yuee Zhou", "Guoyang Xie", "Qiang He", "Zhichao Lu"], "title": "Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mixed Precision Quantization (MPQ) has become an essential technique for\noptimizing neural network by determining the optimal bitwidth per layer.\nExisting MPQ methods, however, face a major hurdle: they require a\ncomputationally expensive search for quantization policies on large-scale\ndatasets. To resolve this issue, we introduce a novel approach that first\nsearches for quantization policies on small datasets and then generalizes them\nto large-scale datasets. This approach simplifies the process, eliminating the\nneed for large-scale quantization fine-tuning and only necessitating model\nweight adjustment. Our method is characterized by three key techniques:\nsharpness-aware minimization for enhanced quantization generalization, implicit\ngradient direction alignment to handle gradient conflicts among different\noptimization objectives, and an adaptive perturbation radius to accelerate\noptimization. Both theoretical analysis and experimental results validate our\napproach. Using the CIFAR10 dataset (just 0.5\\% the size of ImageNet training\ndata) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a\nsignificantly lower computational cost, while improving efficiency by up to\n150% over the baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u641c\u7d22\u91cf\u5316\u7b56\u7565\u5e76\u6cdb\u5316\u81f3\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709MPQ\u65b9\u6cd5\u9700\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6602\u8d35\u641c\u7d22\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u5728\u5c0f\u6570\u636e\u96c6\u641c\u7d22\u91cf\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u3001\u9690\u5f0f\u68af\u5ea6\u65b9\u5411\u5bf9\u9f50\u548c\u81ea\u9002\u5e94\u6270\u52a8\u534a\u5f84\u6280\u672f\u3002", "result": "\u5728CIFAR10\u4e0a\u641c\u7d22\u7b56\u7565\uff0cImageNet\u4e0a\u8fbe\u5230\u540c\u7b49\u7cbe\u5ea6\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\uff0c\u6548\u7387\u63d0\u5347150%\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u7b80\u5316\u91cf\u5316\u8fc7\u7a0b\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u5fae\u8c03\uff0c\u4ec5\u9700\u8c03\u6574\u6a21\u578b\u6743\u91cd\uff0c\u9ad8\u6548\u4e14\u5b9e\u7528\u3002"}}
{"id": "2505.04888", "pdf": "https://arxiv.org/pdf/2505.04888", "abs": "https://arxiv.org/abs/2505.04888", "authors": ["Tharindu Fernando", "Clinton Fookes", "Sridha Sridharan", "Simon Denman"], "title": "Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Remarkable advancements in generative AI technology have given rise to a\nspectrum of novel deepfake categories with unprecedented leaps in their\nrealism, and deepfakes are increasingly becoming a nuisance to law enforcement\nauthorities and the general public. In particular, we observe alarming levels\nof confusion, deception, and loss of faith regarding multimedia content within\nsociety caused by face deepfakes, and existing deepfake detectors are\nstruggling to keep up with the pace of improvements in deepfake generation.\nThis is primarily due to their reliance on specific forgery artifacts, which\nlimits their ability to generalise and detect novel deepfake types. To combat\nthe spread of malicious face deepfakes, this paper proposes a new strategy that\nleverages coarse-to-fine spatial information, semantic information, and their\ninteractions while ensuring feature distinctiveness and reducing the redundancy\nof the modelled features. A novel feature orthogonality-based disentanglement\nstrategy is introduced to ensure branch-level and cross-branch feature\ndisentanglement, which allows us to integrate multiple feature vectors without\nadding complexity to the feature space or compromising generalisation.\nComprehensive experiments on three public benchmarks: FaceForensics++,\nCeleb-DF, and the Deepfake Detection Challenge (DFDC) show that these design\nchoices enable the proposed approach to outperform current state-of-the-art\nmethods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a\ncross-dataset evaluation setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\uff0c\u5229\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u7a7a\u95f4\u4fe1\u606f\u3001\u8bed\u4e49\u4fe1\u606f\u53ca\u5176\u4ea4\u4e92\u6765\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\uff0c\u901a\u8fc7\u7279\u5f81\u6b63\u4ea4\u6027\u89e3\u8026\u7b56\u7565\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u793e\u4f1a\u9020\u6210\u6df7\u6dc6\u3001\u6b3a\u9a97\u548c\u4fe1\u4efb\u5371\u673a\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u56e0\u4f9d\u8d56\u7279\u5b9a\u4f2a\u9020\u75d5\u8ff9\u800c\u96be\u4ee5\u5e94\u5bf9\u65b0\u578b\u6df1\u5ea6\u4f2a\u9020\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u6b63\u4ea4\u6027\u89e3\u8026\u7684\u7b56\u7565\uff0c\u6574\u5408\u591a\u7279\u5f81\u5411\u91cf\uff0c\u786e\u4fdd\u7279\u5f81\u72ec\u7279\u6027\u548c\u51cf\u5c11\u5197\u4f59\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u7279\u5f81\u7a7a\u95f4\u590d\u6742\u6027\u3002", "result": "\u5728FaceForensics++\u3001Celeb-DF\u548cDFDC\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u53475%\u548c7%\u3002", "conclusion": "\u65b0\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u4e3a\u5e94\u5bf9\u6076\u610f\u6df1\u5ea6\u4f2a\u9020\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.04899", "pdf": "https://arxiv.org/pdf/2505.04899", "abs": "https://arxiv.org/abs/2505.04899", "authors": ["Sifan Song", "Siyeop Yoon", "Pengfei Jin", "Sekeun Kim", "Matthew Tivnan", "Yujin Oh", "Runqi Meng", "Ling Chen", "Zhiliang Lyu", "Dufan Wu", "Ning Guo", "Xiang Li", "Quanzheng Li"], "title": "OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in representation learning often rely on holistic, black-box\nembeddings that entangle multiple semantic components, limiting\ninterpretability and generalization. These issues are especially critical in\nmedical imaging. To address these limitations, we propose an Organ-Wise\nTokenization (OWT) framework with a Token Group-based Reconstruction (TGR)\ntraining paradigm. Unlike conventional approaches that produce holistic\nfeatures, OWT explicitly disentangles an image into separable token groups,\neach corresponding to a distinct organ or semantic entity. Our design ensures\neach token group encapsulates organ-specific information, boosting\ninterpretability, generalization, and efficiency while allowing fine-grained\ncontrol in downstream tasks. Experiments on CT and MRI datasets demonstrate the\neffectiveness of OWT in not only achieving strong image reconstruction and\nsegmentation performance, but also enabling novel semantic-level generation and\nretrieval applications that are out of reach for standard holistic embedding\nmethods. These findings underscore the potential of OWT as a foundational\nframework for semantically disentangled representation learning, offering broad\nscalability and applicability to real-world medical imaging scenarios and\nbeyond.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5668\u5b98\u7ea7\u6807\u8bb0\u5316\uff08OWT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u8bb0\u7ec4\u91cd\u5efa\uff08TGR\uff09\u8bad\u7ec3\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6574\u4f53\u5d4c\u5165\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6574\u4f53\u5d4c\u5165\uff0c\u5bfc\u81f4\u8bed\u4e49\u6210\u5206\u7ea0\u7f20\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\uff0c\u5c24\u5176\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u95ee\u9898\u7a81\u51fa\u3002", "method": "OWT\u6846\u67b6\u5c06\u56fe\u50cf\u663e\u5f0f\u89e3\u8026\u4e3a\u72ec\u7acb\u7684\u6807\u8bb0\u7ec4\uff0c\u6bcf\u4e2a\u7ec4\u5bf9\u5e94\u7279\u5b9a\u5668\u5b98\u6216\u8bed\u4e49\u5b9e\u4f53\uff0c\u5e76\u901a\u8fc7TGR\u8bad\u7ec3\u8303\u5f0f\u4f18\u5316\u3002", "result": "\u5728CT\u548cMRI\u6570\u636e\u96c6\u4e0a\uff0cOWT\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u5f3a\u56fe\u50cf\u91cd\u5efa\u548c\u5206\u5272\u6027\u80fd\uff0c\u8fd8\u652f\u6301\u8bed\u4e49\u7ea7\u751f\u6210\u548c\u68c0\u7d22\u5e94\u7528\u3002", "conclusion": "OWT\u4f5c\u4e3a\u4e00\u79cd\u8bed\u4e49\u89e3\u8026\u8868\u793a\u5b66\u4e60\u7684\u57fa\u7840\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.04905", "pdf": "https://arxiv.org/pdf/2505.04905", "abs": "https://arxiv.org/abs/2505.04905", "authors": ["Xi Yang", "Songsong Duan", "Nannan Wang", "Xinbo Gao"], "title": "Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization", "categories": ["cs.CV"], "comment": "Accepted by ECCV 2024", "summary": "Weakly Supervised Object Localization (WSOL), which aims to localize objects\nby only using image-level labels, has attracted much attention because of its\nlow annotation cost in real applications. Current studies focus on the Class\nActivation Map (CAM) of CNN and the self-attention map of transformer to\nidentify the region of objects. However, both CAM and self-attention maps can\nnot learn pixel-level fine-grained information on the foreground objects, which\nhinders the further advance of WSOL. To address this problem, we initiatively\nleverage the capability of zero-shot generalization and fine-grained\nsegmentation in Segment Anything Model (SAM) to boost the activation of\nintegral object regions. Further, to alleviate the semantic ambiguity issue\naccrued in single point prompt-based SAM, we propose an innovative mask prompt\nto SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a\nGlobal Token Transformer (GTFormer) to generate a coarse-grained foreground map\nas a flexible mask prompt, where the GTFormer jointly embeds patch tokens and\nnovel global tokens to learn foreground semantics. Secondly, we deliver grid\npoints as dense prompts into SAM to maximize the probability of foreground\nmask, which avoids the lack of objects caused by a single point/box prompt.\nFinally, we propose a pixel-level similarity metric to come true the mask\nmatching from mask prompt to SAM, where the mask with the highest score is\nviewed as the final localization map. Experiments show that the proposed\nPro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC,\nwith 84.03\\% and 66.85\\% Top-1 Loc, respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSegment Anything Model (SAM)\u7684\u65b0\u65b9\u6cd5Pro2SAM\uff0c\u7528\u4e8e\u5f31\u76d1\u7763\u76ee\u6807\u5b9a\u4f4d\uff08WSOL\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u63a9\u7801\u63d0\u793a\u548c\u7f51\u683c\u70b9\u6280\u672f\u63d0\u5347\u76ee\u6807\u533a\u57df\u7684\u6fc0\u6d3b\u6548\u679c\u3002", "motivation": "\u5f53\u524dWSOL\u65b9\u6cd5\uff08\u5982CAM\u548c\u81ea\u6ce8\u610f\u529b\u56fe\uff09\u65e0\u6cd5\u5b66\u4e60\u50cf\u7d20\u7ea7\u7ec6\u7c92\u5ea6\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5229\u7528SAM\u7684\u96f6\u6837\u672c\u6cdb\u5316\u548c\u7ec6\u7c92\u5ea6\u5206\u5272\u80fd\u529b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faPro2SAM\u7f51\u7edc\uff0c\u5305\u62ec\uff1a1\uff09\u4f7f\u7528GTFormer\u751f\u6210\u7c97\u7c92\u5ea6\u524d\u666f\u56fe\u4f5c\u4e3a\u63a9\u7801\u63d0\u793a\uff1b2\uff09\u901a\u8fc7\u7f51\u683c\u70b9\u5bc6\u96c6\u63d0\u793aSAM\u4ee5\u6700\u5927\u5316\u524d\u666f\u63a9\u7801\u6982\u7387\uff1b3\uff09\u63d0\u51fa\u50cf\u7d20\u7ea7\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u8fdb\u884c\u63a9\u7801\u5339\u914d\u3002", "result": "\u5728CUB-200-2011\u548cILSVRC\u6570\u636e\u96c6\u4e0a\uff0cPro2SAM\u5206\u522b\u8fbe\u523084.03%\u548c66.85%\u7684Top-1\u5b9a\u4f4d\u51c6\u786e\u7387\uff0c\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "Pro2SAM\u901a\u8fc7\u7ed3\u5408SAM\u7684\u5f3a\u5206\u5272\u80fd\u529b\u548c\u521b\u65b0\u7684\u63d0\u793a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86WSOL\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.04911", "pdf": "https://arxiv.org/pdf/2505.04911", "abs": "https://arxiv.org/abs/2505.04911", "authors": ["Shun Taguchi", "Hideki Deguchi", "Takumi Hamazaki", "Hiroyuki Sakai"], "title": "SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "18 pages, 11 figures", "summary": "This study introduces SpatialPrompting, a novel framework that harnesses the\nemergent reasoning capabilities of off-the-shelf multimodal large language\nmodels to achieve zero-shot spatial reasoning in three-dimensional (3D)\nenvironments. Unlike existing methods that rely on expensive 3D-specific\nfine-tuning with specialized 3D inputs such as point clouds or voxel-based\nfeatures, SpatialPrompting employs a keyframe-driven prompt generation\nstrategy. This framework uses metrics such as vision-language similarity,\nMahalanobis distance, field of view, and image sharpness to select a diverse\nand informative set of keyframes from image sequences and then integrates them\nwith corresponding camera pose data to effectively abstract spatial\nrelationships and infer complex 3D structures. The proposed framework not only\nestablishes a new paradigm for flexible spatial reasoning that utilizes\nintuitive visual and positional cues but also achieves state-of-the-art\nzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across\nseveral metrics. The proposed method effectively eliminates the need for\nspecialized 3D inputs and fine-tuning, offering a simpler and more scalable\nalternative to conventional approaches.", "AI": {"tldr": "SpatialPrompting\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u5229\u7528\u73b0\u6210\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u96f6\u6837\u672c3D\u7a7a\u95f4\u63a8\u7406\uff0c\u65e0\u9700\u6602\u8d35\u76843D\u5fae\u8c03\u6216\u4e13\u7528\u8f93\u5165\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u76843D\u5fae\u8c03\u548c\u4e13\u7528\u8f93\u5165\uff08\u5982\u70b9\u4e91\u6216\u4f53\u7d20\u7279\u5f81\uff09\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u5173\u952e\u5e27\u9a71\u52a8\u7684\u63d0\u793a\u751f\u6210\u7b56\u7565\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u76f8\u4f3c\u6027\u3001\u9a6c\u6c0f\u8ddd\u79bb\u7b49\u6307\u6807\u9009\u62e9\u591a\u6837\u5316\u5173\u952e\u5e27\uff0c\u5e76\u6574\u5408\u76f8\u673a\u4f4d\u59ff\u6570\u636e\u3002", "result": "\u5728ScanQA\u548cSQA3D\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u7b80\u5316\u4e863D\u63a8\u7406\u6d41\u7a0b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.04915", "pdf": "https://arxiv.org/pdf/2505.04915", "abs": "https://arxiv.org/abs/2505.04915", "authors": ["Tong Wang", "Ting Liu", "Xiaochao Qu", "Chengjing Wu", "Luoqi Liu", "Xiaolin Hu"], "title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene text editing, a subfield of image editing, requires modifying texts in\nimages while preserving style consistency and visual coherence with the\nsurrounding environment. While diffusion-based methods have shown promise in\ntext generation, they still struggle to produce high-quality results. These\nmethods often generate distorted or unrecognizable characters, particularly\nwhen dealing with complex characters like Chinese. In such systems, characters\nare composed of intricate stroke patterns and spatial relationships that must\nbe precisely maintained. We present GlyphMastero, a specialized glyph encoder\ndesigned to guide the latent diffusion model for generating texts with\nstroke-level precision. Our key insight is that existing methods, despite using\npretrained OCR models for feature extraction, fail to capture the hierarchical\nnature of text structures - from individual strokes to stroke-level\ninteractions to overall character-level structure. To address this, our glyph\nencoder explicitly models and captures the cross-level interactions between\nlocal-level individual characters and global-level text lines through our novel\nglyph attention module. Meanwhile, our model implements a feature pyramid\nnetwork to fuse the multi-scale OCR backbone features at the global-level.\nThrough these cross-level and multi-scale fusions, we obtain more detailed\nglyph-aware guidance, enabling precise control over the scene text generation\nprocess. Our method achieves an 18.02\\% improvement in sentence accuracy over\nthe state-of-the-art multi-lingual scene text editing baseline, while\nsimultaneously reducing the text-region Fr\\'echet inception distance by\n53.28\\%.", "AI": {"tldr": "GlyphMastero\u662f\u4e00\u79cd\u4e13\u95e8\u8bbe\u8ba1\u7684\u5b57\u5f62\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5b57\u5f62\u6ce8\u610f\u529b\u6a21\u5757\u548c\u591a\u5c3a\u5ea6OCR\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u6587\u672c\u7f16\u8f91\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5728\u751f\u6210\u590d\u6742\u5b57\u7b26\uff08\u5982\u4e2d\u6587\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u4fdd\u6301\u7b14\u753b\u7ea7\u7cbe\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faGlyphMastero\uff0c\u7ed3\u5408\u5b57\u5f62\u6ce8\u610f\u529b\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff0c\u6355\u6349\u4ece\u7b14\u753b\u5230\u5b57\u7b26\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u5728\u53e5\u5b50\u51c6\u786e\u7387\u4e0a\u63d0\u534718.02%\uff0c\u6587\u672c\u533a\u57dfFr\u00e9chet\u8d77\u59cb\u8ddd\u79bb\u964d\u4f4e53.28%\u3002", "conclusion": "GlyphMastero\u901a\u8fc7\u8de8\u5c42\u6b21\u548c\u591a\u5c3a\u5ea6\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u5bf9\u573a\u666f\u6587\u672c\u751f\u6210\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.04917", "pdf": "https://arxiv.org/pdf/2505.04917", "abs": "https://arxiv.org/abs/2505.04917", "authors": ["Chenxu Peng", "Chenxu Wang", "Minrui Zou", "Danyang Li", "Zhengpeng Yang", "Yimian Dai", "Ming-Ming Cheng", "Xiang Li"], "title": "A Simple Detector with Frame Dynamics is a Strong Tracker", "categories": ["cs.CV"], "comment": "2025 CVPR Anti-UAV Workshop", "summary": "Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle\n(Anti-UAV) applications. Existing trackers often depend on cropped template\nregions and have limited motion modeling capabilities, which pose challenges\nwhen dealing with tiny targets. To address this, we propose a simple yet\neffective infrared tiny-object tracker that enhances tracking performance by\nintegrating global detection and motion-aware learning with temporal priors.\nOur method is based on object detection and achieves significant improvements\nthrough two key innovations. First, we introduce frame dynamics, leveraging\nframe difference and optical flow to encode both prior target features and\nmotion characteristics at the input level, enabling the model to better\ndistinguish the target from background clutter. Second, we propose a trajectory\nconstraint filtering strategy in the post-processing stage, utilizing\nspatio-temporal priors to suppress false positives and enhance tracking\nrobustness. Extensive experiments show that our method consistently outperforms\nexisting approaches across multiple metrics in challenging infrared UAV\ntracking scenarios. Notably, we achieve state-of-the-art performance in the 4th\nAnti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea2\u5916\u5c0f\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u68c0\u6d4b\u548c\u8fd0\u52a8\u611f\u77e5\u5b66\u4e60\u63d0\u5347\u6027\u80fd\uff0c\u5728Anti-UAV\u6311\u6218\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u8ddf\u8e2a\u5668\u4f9d\u8d56\u88c1\u526a\u6a21\u677f\u533a\u57df\u4e14\u8fd0\u52a8\u5efa\u6a21\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5904\u7406\u5fae\u5c0f\u76ee\u6807\u3002", "method": "\u7ed3\u5408\u5168\u5c40\u68c0\u6d4b\u4e0e\u8fd0\u52a8\u611f\u77e5\u5b66\u4e60\uff0c\u5229\u7528\u5e27\u52a8\u6001\u548c\u8f68\u8ff9\u7ea6\u675f\u8fc7\u6ee4\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728Anti-UAV\u6311\u6218\u4e2d\u53d6\u5f97\u9886\u5148\u6210\u7ee9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u8ddf\u8e2a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.04921", "pdf": "https://arxiv.org/pdf/2505.04921", "abs": "https://arxiv.org/abs/2505.04921", "authors": ["Yunxin Li", "Zhenyu Liu", "Zitao Li", "Xuanyu Zhang", "Zhenran Xu", "Xinyu Chen", "Haoyuan Shi", "Shenyuan Jiang", "Xintong Wang", "Jifang Wang", "Shouzheng Huang", "Xinping Zhao", "Borui Jiang", "Lanqing Hong", "Longyue Wang", "Zhuotao Tian", "Baoxing Huai", "Wenhan Luo", "Weihua Luo", "Zheng Zhang", "Baotian Hu", "Min Zhang"], "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models", "categories": ["cs.CV", "cs.CL"], "comment": "75 Pages,10 figures; Project:\n  https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models", "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u4ece\u65e9\u671f\u7684\u6a21\u5757\u5316\u65b9\u6cd5\u5230\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u539f\u751f\u5927\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff08N-LMRMs\uff09\u7684\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u5f00\u653e\u3001\u4e0d\u786e\u5b9a\u548c\u591a\u6a21\u6001\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u63a8\u7406\u80fd\u529b\u6210\u4e3a\u5b9e\u73b0\u7a33\u5065\u548c\u81ea\u9002\u5e94\u884c\u4e3a\u7684\u5173\u952e\u3002\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff08LMRMs\uff09\u901a\u8fc7\u6574\u5408\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u65e8\u5728\u5b9e\u73b0\u5168\u9762\u7684\u611f\u77e5\u3001\u7cbe\u786e\u7684\u7406\u89e3\u548c\u6df1\u5ea6\u63a8\u7406\u3002", "method": "\u672c\u6587\u901a\u8fc7\u56db\u9636\u6bb5\u53d1\u5c55\u8def\u7ebf\u56fe\u7efc\u8ff0\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\uff1a1\uff09\u57fa\u4e8e\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u7684\u65e9\u671f\u65b9\u6cd5\uff1b2\uff09\u7edf\u4e00\u5230\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fd1\u671f\u65b9\u6cd5\uff1b3\uff09\u5f15\u5165\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff1b4\uff09\u63a2\u8ba8\u539f\u751f\u5927\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff08N-LMRMs\uff09\u7684\u672a\u6765\u65b9\u5411\u3002", "result": "\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u5df2\u4ece\u6a21\u5757\u5316\u65b9\u6cd5\u53d1\u5c55\u4e3a\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u5728\u63a8\u7406\u6df1\u5ea6\u548c\u8de8\u6a21\u6001\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5168\u6a21\u6001\u6cdb\u5316\u3001\u63a8\u7406\u6df1\u5ea6\u548c\u4ee3\u7406\u884c\u4e3a\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u662f\u5f00\u53d1\u539f\u751f\u5927\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff08N-LMRMs\uff09\uff0c\u4ee5\u652f\u6301\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u3001\u4ee3\u7406\u548c\u81ea\u9002\u5e94\u63a8\u7406\u4e0e\u89c4\u5212\u3002"}}
{"id": "2505.04922", "pdf": "https://arxiv.org/pdf/2505.04922", "abs": "https://arxiv.org/abs/2505.04922", "authors": ["Xingzeng Lan", "Xing Duan", "Chen Chen", "Weiyu Lin", "Bo Wang"], "title": "Canny2Palm: Realistic and Controllable Palmprint Generation for Large-scale Pre-training", "categories": ["cs.CV"], "comment": null, "summary": "Palmprint recognition is a secure and privacy-friendly method of biometric\nidentification. One of the major challenges to improve palmprint recognition\naccuracy is the scarcity of palmprint data. Recently, a popular line of\nresearch revolves around the synthesis of virtual palmprints for large-scale\npre-training purposes. In this paper, we propose a novel synthesis method named\nCanny2Palm that extracts palm textures with Canny edge detector and uses them\nto condition a Pix2Pix network for realistic palmprint generation. By\nre-assembling palmprint textures from different identities, we are able to\ncreate new identities by seeding the generator with new assemblies. Canny2Palm\nnot only synthesizes realistic data following the distribution of real\npalmprints but also enables controllable diversity to generate large-scale new\nidentities. On open-set palmprint recognition benchmarks, models pre-trained\nwith Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2%\nhigher identification accuracy. Moreover, the performance of models pre-trained\nwith Canny2Palm continues to improve given 10,000 synthetic IDs while those\nwith existing methods already saturate, demonstrating the potential of our\nmethod for large-scale pre-training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCanny2Palm\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7Canny\u8fb9\u7f18\u68c0\u6d4b\u5668\u548cPix2Pix\u7f51\u7edc\u751f\u6210\u865a\u62df\u638c\u7eb9\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u638c\u7eb9\u8bc6\u522b\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u638c\u7eb9\u8bc6\u522b\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u7684\u63d0\u5347\u3002", "method": "\u4f7f\u7528Canny\u8fb9\u7f18\u68c0\u6d4b\u5668\u63d0\u53d6\u638c\u7eb9\u7eb9\u7406\uff0c\u7ed3\u5408Pix2Pix\u7f51\u7edc\u751f\u6210\u865a\u62df\u638c\u7eb9\uff0c\u5e76\u901a\u8fc7\u91cd\u65b0\u7ec4\u5408\u7eb9\u7406\u751f\u6210\u65b0\u8eab\u4efd\u3002", "result": "\u5728\u5f00\u653e\u96c6\u638c\u7eb9\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bc6\u522b\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa7.2%\uff0c\u4e14\u6027\u80fd\u968f\u5408\u6210\u6570\u636e\u91cf\u589e\u52a0\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "Canny2Palm\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u865a\u62df\u638c\u7eb9\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2505.04938", "pdf": "https://arxiv.org/pdf/2505.04938", "abs": "https://arxiv.org/abs/2505.04938", "authors": ["Ying Zhang", "Shuai Guo", "Chenxi Sun", "Yuchen Zhu", "Jinhai Xiang"], "title": "FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image Registration", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "In recent years, deformable medical image registration techniques have made\nsignificant progress. However, existing models still lack efficiency in\nparallel extraction of coarse and fine-grained features. To address this, we\nconstruct a new pyramid registration network based on feature and deformation\nfield (FF-PNet). For coarse-grained feature extraction, we design a Residual\nFeature Fusion Module (RFFM), for fine-grained image deformation, we propose a\nResidual Deformation Field Fusion Module (RDFFM). Through the parallel\noperation of these two modules, the model can effectively handle complex image\ndeformations. It is worth emphasizing that the encoding stage of FF-PNet only\nemploys traditional convolutional neural networks without any attention\nmechanisms or multilayer perceptrons, yet it still achieves remarkable\nimprovements in registration accuracy, fully demonstrating the superior feature\ndecoding capabilities of RFFM and RDFFM. We conducted extensive experiments on\nthe LPBA and OASIS datasets. The results show our network consistently\noutperforms popular methods in metrics like the Dice Similarity Coefficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u548c\u53d8\u5f62\u573a\u7684\u91d1\u5b57\u5854\u914d\u51c6\u7f51\u7edc\uff08FF-PNet\uff09\uff0c\u901a\u8fc7\u5e76\u884c\u63d0\u53d6\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5e76\u884c\u63d0\u53d6\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u65f6\u6548\u7387\u4e0d\u8db3\uff0cFF-PNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u6b8b\u5dee\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08RFFM\uff09\u548c\u6b8b\u5dee\u53d8\u5f62\u573a\u878d\u5408\u6a21\u5757\uff08RDFFM\uff09\uff0c\u5e76\u884c\u5904\u7406\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u4ec5\u4f7f\u7528\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728LPBA\u548cOASIS\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cDice\u76f8\u4f3c\u7cfb\u6570\u7b49\u6307\u6807\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FF-PNet\u5c55\u793a\u4e86RFFM\u548cRDFFM\u7684\u4f18\u8d8a\u7279\u5f81\u89e3\u7801\u80fd\u529b\uff0c\u65e0\u9700\u6ce8\u610f\u529b\u673a\u5236\u6216\u591a\u5c42\u611f\u77e5\u5668\u5373\u53ef\u663e\u8457\u63d0\u5347\u914d\u51c6\u7cbe\u5ea6\u3002"}}
{"id": "2505.04941", "pdf": "https://arxiv.org/pdf/2505.04941", "abs": "https://arxiv.org/abs/2505.04941", "authors": ["Jiepan Li", "He Huang", "Yu Sheng", "Yujun Guo", "Wei He"], "title": "Building-Guided Pseudo-Label Learning for Cross-Modal Building Damage Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Accurate building damage assessment using bi-temporal multi-modal remote\nsensing images is essential for effective disaster response and recovery\nplanning. This study proposes a novel Building-Guided Pseudo-Label Learning\nFramework to address the challenges of mapping building damage from\npre-disaster optical and post-disaster SAR images. First, we train a series of\nbuilding extraction models using pre-disaster optical images and building\nlabels. To enhance building segmentation, we employ multi-model fusion and\ntest-time augmentation strategies to generate pseudo-probabilities, followed by\na low-uncertainty pseudo-label training method for further refinement. Next, a\nchange detection model is trained on bi-temporal cross-modal images and damaged\nbuilding labels. To improve damage classification accuracy, we introduce a\nbuilding-guided low-uncertainty pseudo-label refinement strategy, which\nleverages building priors from the previous step to guide pseudo-label\ngeneration for damaged buildings, reducing uncertainty and enhancing\nreliability. Experimental results on the 2025 IEEE GRSS Data Fusion Contest\ndataset demonstrate the effectiveness of our approach, which achieved the\nhighest mIoU score (54.28%) and secured first place in the competition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5efa\u7b51\u7269\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u4e2d\u8bc4\u4f30\u5efa\u7b51\u7269\u635f\u574f\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u878d\u5408\u548c\u4f4e\u4e0d\u786e\u5b9a\u6027\u4f2a\u6807\u7b7e\u8bad\u7ec3\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u51c6\u786e\u8bc4\u4f30\u5efa\u7b51\u7269\u635f\u574f\u5bf9\u707e\u5bb3\u54cd\u5e94\u548c\u6062\u590d\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u591a\u6a21\u6001\u56fe\u50cf\u95f4\u7684\u5dee\u5f02\u548c\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u5148\u8bad\u7ec3\u5efa\u7b51\u7269\u63d0\u53d6\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u591a\u6a21\u578b\u878d\u5408\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u751f\u6210\u4f2a\u6982\u7387\uff0c\u6700\u540e\u7528\u4f4e\u4e0d\u786e\u5b9a\u6027\u4f2a\u6807\u7b7e\u8bad\u7ec3\u4f18\u5316\u3002\u968f\u540e\u8bad\u7ec3\u53d8\u5316\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u5efa\u7b51\u7269\u5f15\u5bfc\u7684\u4f4e\u4e0d\u786e\u5b9a\u6027\u4f2a\u6807\u7b7e\u7ec6\u5316\u7b56\u7565\u3002", "result": "\u57282025 IEEE GRSS\u6570\u636e\u878d\u5408\u7ade\u8d5b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u9ad8mIoU\u5206\u6570\uff0854.28%\uff09\uff0c\u5e76\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u7b51\u7269\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u5efa\u7b51\u7269\u635f\u574f\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.04946", "pdf": "https://arxiv.org/pdf/2505.04946", "abs": "https://arxiv.org/abs/2505.04946", "authors": ["Xuyang Guo", "Jiayan Huo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang", "Jiale Zhao"], "title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis.", "AI": {"tldr": "T2VTextBench\u662f\u9996\u4e2a\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e2d\u5c4f\u5e55\u6587\u672c\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u751f\u6210\u6e05\u6670\u3001\u4e00\u81f4\u7684\u6587\u672c\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u751f\u6210\u7cbe\u786e\u5c4f\u5e55\u6587\u672c\uff08\u5982\u5b57\u5e55\u6216\u6570\u5b66\u516c\u5f0f\uff09\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\uff0c\u8fd9\u5bf9\u9700\u8981\u9ad8\u6587\u672c\u51c6\u786e\u6027\u7684\u5e94\u7528\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86T2VTextBench\uff0c\u4e00\u4e2a\u4eba\u7c7b\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u7ed3\u5408\u590d\u6742\u6587\u672c\u5b57\u7b26\u4e32\u548c\u52a8\u6001\u573a\u666f\u53d8\u5316\u7684\u63d0\u793a\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u591a\u5e27\u4e2d\u4fdd\u6301\u8be6\u7ec6\u6307\u4ee4\u7684\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e86\u5341\u79cd\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u96be\u4ee5\u751f\u6210\u6e05\u6670\u3001\u4e00\u81f4\u7684\u6587\u672c\u3002", "conclusion": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u5668\u5728\u6587\u672c\u5904\u7406\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002"}}
{"id": "2505.04962", "pdf": "https://arxiv.org/pdf/2505.04962", "abs": "https://arxiv.org/abs/2505.04962", "authors": ["Utsav Rai", "Hardik Mehta", "Vismay Vakharia", "Aditya Choudhary", "Amit Parmar", "Rolif Lima", "Kaushik Das"], "title": "An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted in IEEE/RSJ IROS 2022 Workshop on Mobile Manipulation and\n  Embodied Intelligence (MOMA)", "summary": "The proposed system outlined in this paper is a solution to a use case that\nrequires the autonomous picking of cuboidal objects from an organized or\nunorganized pile with high precision. This paper presents an efficient method\nfor precise pose estimation of cuboid-shaped objects, which aims to reduce\nerrors in target pose in a time-efficient manner. Typical pose estimation\nmethods like global point cloud registrations are prone to minor pose errors\nfor which local registration algorithms are generally used to improve pose\naccuracy. However, due to the execution time overhead and uncertainty in the\nerror of the final achieved pose, an alternate, linear time approach is\nproposed for pose error estimation and correction. This paper presents an\noverview of the solution followed by a detailed description of individual\nmodules of the proposed algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u4f30\u8ba1\u7acb\u65b9\u4f53\u5f62\u72b6\u7269\u4f53\u7684\u4f4d\u59ff\uff0c\u4ee5\u51cf\u5c11\u76ee\u6807\u4f4d\u59ff\u8bef\u5dee\u5e76\u8282\u7701\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u5728\u81ea\u4e3b\u62fe\u53d6\u7acb\u65b9\u4f53\u7269\u4f53\u65f6\u7684\u9ad8\u7cbe\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u8bef\u5dee\u548c\u65f6\u95f4\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ebf\u6027\u65f6\u95f4\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4d\u59ff\u8bef\u5dee\u4f30\u8ba1\u548c\u6821\u6b63\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u5168\u5c40\u70b9\u4e91\u914d\u51c6\u548c\u5c40\u90e8\u914d\u51c6\u7b97\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u4e14\u7cbe\u786e\u5730\u4f30\u8ba1\u548c\u6821\u6b63\u7acb\u65b9\u4f53\u7269\u4f53\u7684\u4f4d\u59ff\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u7acb\u65b9\u4f53\u7269\u4f53\u7684\u9ad8\u7cbe\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04963", "pdf": "https://arxiv.org/pdf/2505.04963", "abs": "https://arxiv.org/abs/2505.04963", "authors": ["Onkar Susladkar", "Gayatri Deshmukh", "Yalcin Tur", "Ulas Bagci"], "title": "ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research.", "AI": {"tldr": "ViCTr\u662f\u4e00\u79cd\u65b0\u578b\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u4fee\u6b63\u6d41\u8f68\u8ff9\u548cTweedie\u6821\u6b63\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u75c5\u7406\u611f\u77e5\u7684\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5408\u6210\u9762\u4e34\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u6a21\u6001\u5dee\u5f02\u548c\u590d\u6742\u75c5\u7406\u8868\u793a\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u89e3\u5256\u5b66\u4fdd\u771f\u5ea6\u548c\u75c5\u7406\u7279\u5f81\u51c6\u786e\u6027\u3002", "method": "ViCTr\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9884\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528EWC\u4fdd\u62a4\u89e3\u5256\u7ed3\u6784\uff0c\u5fae\u8c03\u9636\u6bb5\u901a\u8fc7LoRA\u6a21\u5757\u63a7\u5236\u75c5\u7406\u4e25\u91cd\u7a0b\u5ea6\uff0c\u5e76\u5229\u7528Tweedie\u516c\u5f0f\u5b9e\u73b0\u4e00\u6b65\u91c7\u6837\u3002", "result": "ViCTr\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cMFID\u4e3a17.01\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u4f4e28%\uff0c\u4e14\u80fd\u63d0\u5347nnUNet\u5206\u5272\u6027\u80fd3.8%\u3002", "conclusion": "ViCTr\u9996\u6b21\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u75c5\u7406\u611f\u77e5\u7684MRI\u5408\u6210\uff0c\u586b\u8865\u4e86AI\u533b\u5b66\u5f71\u50cf\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2505.04964", "pdf": "https://arxiv.org/pdf/2505.04964", "abs": "https://arxiv.org/abs/2505.04964", "authors": ["Yuto Nakamura", "Satoshi Kodera", "Haruki Settai", "Hiroki Shinohara", "Masatsugu Tamura", "Tomohiro Noguchi", "Tatsuki Furusawa", "Ryo Takizawa", "Tempei Kabayama", "Norihiko Takeda"], "title": "CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic Images for Next-Generation Diagnostic Systems", "categories": ["cs.CV"], "comment": null, "summary": "Coronary angiography (CAG) is the gold-standard imaging modality for\nevaluating coronary artery disease, but its interpretation and subsequent\ntreatment planning rely heavily on expert cardiologists. To enable AI-based\ndecision support, we introduce a two-stage, physician-curated pipeline and a\nbilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686\nframes from 539 exams and annotate them for key-frame detection and left/right\nlaterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on\nlaterality classification, even on low-contrast frames. Second, we apply the\nCNN to 243 independent exams, extract 1,114 key frames, and pair each with its\npre-procedure report and expert-validated diagnostic and treatment summary,\nyielding a parallel corpus. We then fine-tune three open-source VLMs\n(PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate\nthem using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains\nthe highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean\n7.20/10); we designate this best-performing model as CAG-VLM. These results\ndemonstrate that specialized, fine-tuned VLMs can effectively assist\ncardiologists in generating clinical reports and treatment recommendations from\nCAG images.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684AI\u652f\u6301\u51b3\u7b56\u6d41\u7a0b\uff0c\u7528\u4e8e\u51a0\u72b6\u52a8\u8109\u9020\u5f71\uff08CAG\uff09\u56fe\u50cf\u5206\u6790\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u53cc\u8bed\u6570\u636e\u96c6\u3002\u901a\u8fc7\u8bad\u7ec3CNN\u548c\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u4fa7\u5411\u5206\u7c7b\u548c\u4e34\u5e8a\u62a5\u544a\u751f\u6210\u3002", "motivation": "CAG\u662f\u8bc4\u4f30\u51a0\u72b6\u52a8\u8109\u75be\u75c5\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u5176\u89e3\u8bfb\u548c\u6cbb\u7597\u89c4\u5212\u4f9d\u8d56\u4e13\u5bb6\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u8f85\u52a9\u51b3\u7b56\u51cf\u8f7b\u4e13\u5bb6\u8d1f\u62c5\u3002", "method": "1. \u4ece539\u4f8b\u68c0\u67e5\u4e2d\u91c7\u683714,686\u5e27\u56fe\u50cf\uff0c\u6807\u6ce8\u5173\u952e\u5e27\u548c\u4fa7\u5411\u6027\uff0c\u8bad\u7ec3CNN\u30022. \u5e94\u7528CNN\u63d0\u53d6\u5173\u952e\u5e27\uff0c\u6784\u5efa\u53cc\u8bed\u62a5\u544a\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u4e09\u79cdVLM\u6a21\u578b\u3002", "result": "CNN\u5728\u4fa7\u5411\u5206\u7c7b\u4e0aF1\u8fbe0.96\uff1b\u5fae\u8c03\u540e\u7684Gemma3\u6a21\u578b\u83b7\u5f97\u6700\u9ad8\u4e34\u5e8a\u8bc4\u5206\uff087.20/10\uff09\uff0c\u88ab\u547d\u540d\u4e3aCAG-VLM\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684VLM\u80fd\u6709\u6548\u8f85\u52a9\u751f\u6210CAG\u56fe\u50cf\u7684\u4e34\u5e8a\u62a5\u544a\u548c\u6cbb\u7597\u5efa\u8bae\u3002"}}
{"id": "2505.04965", "pdf": "https://arxiv.org/pdf/2505.04965", "abs": "https://arxiv.org/abs/2505.04965", "authors": ["Henry Zheng", "Hao Shi", "Qihang Peng", "Yong Xien Chng", "Rui Huang", "Yepeng Weng", "Zhongchao Shi", "Gao Huang"], "title": "DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "Enabling intelligent agents to comprehend and interact with 3D environments\nthrough natural language is crucial for advancing robotics and human-computer\ninteraction. A fundamental task in this field is ego-centric 3D visual\ngrounding, where agents locate target objects in real-world 3D spaces based on\nverbal descriptions. However, this task faces two significant challenges: (1)\nloss of fine-grained visual semantics due to sparse fusion of point clouds with\nego-centric multi-view images, (2) limited textual semantic context due to\narbitrary language descriptions. We propose DenseGrounding, a novel approach\ndesigned to address these issues by enhancing both visual and textual\nsemantics. For visual features, we introduce the Hierarchical Scene Semantic\nEnhancer, which retains dense semantics by capturing fine-grained global scene\nfeatures and facilitating cross-modal alignment. For text descriptions, we\npropose a Language Semantic Enhancer that leverages large language models to\nprovide rich context and diverse language descriptions with additional context\nduring model training. Extensive experiments show that DenseGrounding\nsignificantly outperforms existing methods in overall accuracy, with\nimprovements of 5.81% and 7.56% when trained on the comprehensive full dataset\nand smaller mini subset, respectively, further advancing the SOTA in egocentric\n3D visual grounding. Our method also achieves 1st place and receives the\nInnovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D\nVisual Grounding Track, validating its effectiveness and robustness.", "AI": {"tldr": "DenseGrounding\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u548c\u6587\u672c\u8bed\u4e49\uff0c\u89e3\u51b3\u4e863D\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u7a00\u758f\u70b9\u4e91\u878d\u5408\u548c\u8bed\u8a00\u63cf\u8ff0\u9650\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u63a8\u52a8\u667a\u80fd\u4ee3\u7406\u57283D\u73af\u5883\u4e2d\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u4ea4\u4e92\uff0c\u89e3\u51b3\u7a00\u758f\u70b9\u4e91\u878d\u5408\u548c\u8bed\u8a00\u63cf\u8ff0\u9650\u5236\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDenseGrounding\u65b9\u6cd5\uff0c\u5305\u62ecHierarchical Scene Semantic Enhancer\uff08\u89c6\u89c9\u7279\u5f81\u589e\u5f3a\uff09\u548cLanguage Semantic Enhancer\uff08\u6587\u672c\u63cf\u8ff0\u589e\u5f3a\uff09\u3002", "result": "\u5728\u5b8c\u6574\u6570\u636e\u96c6\u548c\u5c0f\u578b\u5b50\u96c6\u4e0a\u5206\u522b\u63d0\u53475.81%\u548c7.56%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728CVPR 2024\u6bd4\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\u548c\u521b\u65b0\u5956\u3002", "conclusion": "DenseGrounding\u57283D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.04974", "pdf": "https://arxiv.org/pdf/2505.04974", "abs": "https://arxiv.org/abs/2505.04974", "authors": ["Wanjiang Weng", "Xiaofeng Tan", "Hongsong Wang", "Pan Zhou"], "title": "ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "Bilingual text-to-motion generation, which synthesizes 3D human motions from\nbilingual text inputs, holds immense potential for cross-linguistic\napplications in gaming, film, and robotics. However, this task faces critical\nchallenges: the absence of bilingual motion-language datasets and the\nmisalignment between text and motion distributions in diffusion models, leading\nto semantically inconsistent or low-quality motions. To address these\nchallenges, we propose BiHumanML3D, a novel bilingual human motion dataset,\nwhich establishes a crucial benchmark for bilingual text-to-motion generation\nmodels. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD),\nwhich leverages cross-lingual aligned representations to capture semantics,\nthereby achieving a unified bilingual model. Building upon this, we propose\nReward-guided sampling Alignment (ReAlign) method, comprising a step-aware\nreward model to assess alignment quality during sampling and a reward-guided\nstrategy that directs the diffusion process toward an optimally aligned\ndistribution. This reward model integrates step-aware tokens and combines a\ntext-aligned module for semantic consistency and a motion-aligned module for\nrealism, refining noisy motions at each timestep to balance probability density\nand alignment. Experiments demonstrate that our approach significantly improves\ntext-motion alignment and motion quality compared to existing state-of-the-art\nmethods. Project page: https://wengwanjiang.github.io/ReAlign-page/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBiHumanML3D\u53cc\u8bed\u6570\u636e\u96c6\u548cBiMD\u6a21\u578b\uff0c\u7ed3\u5408ReAlign\u65b9\u6cd5\u63d0\u5347\u53cc\u8bed\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u53cc\u8bed\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u5728\u8de8\u8bed\u8a00\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7f3a\u4e4f\u53cc\u8bed\u6570\u636e\u96c6\u4e14\u73b0\u6709\u6a21\u578b\u5b58\u5728\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u63d0\u51faBiHumanML3D\u6570\u636e\u96c6\u548cBiMD\u6a21\u578b\uff0c\u7ed3\u5408Reward-guided sampling Alignment (ReAlign)\u65b9\u6cd5\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u4e0e\u52a8\u4f5c\u7684\u5bf9\u9f50\u6027\u548c\u52a8\u4f5c\u8d28\u91cf\u3002", "conclusion": "BiHumanML3D\u548cBiMD\u6a21\u578b\u7ed3\u5408ReAlign\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u8bed\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u6311\u6218\u3002"}}
{"id": "2505.04979", "pdf": "https://arxiv.org/pdf/2505.04979", "abs": "https://arxiv.org/abs/2505.04979", "authors": ["Zhuang Qi", "Sijin Zhou", "Lei Meng", "Han Hu", "Han Yu", "Xiangxu Meng"], "title": "Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization", "categories": ["cs.CV"], "comment": "IJCAI-25 Accepted", "summary": "Attribute bias in federated learning (FL) typically leads local models to\noptimize inconsistently due to the learning of non-causal associations,\nresulting degraded performance. Existing methods either use data augmentation\nfor increasing sample diversity or knowledge distillation for learning\ninvariant representations to address this problem. However, they lack a\ncomprehensive analysis of the inference paths, and the interference from\nconfounding factors limits their performance. To address these limitations, we\npropose the \\underline{Fed}erated \\underline{D}econfounding and\n\\underline{D}ebiasing \\underline{L}earning (FedDDL) method. It constructs a\nstructured causal graph to analyze the model inference process, and performs\nbackdoor adjustment to eliminate confounding paths. Specifically, we design an\nintra-client deconfounding learning module for computer vision tasks to\ndecouple background and objects, generating counterfactual samples that\nestablish a connection between the background and any label, which stops the\nmodel from using the background to infer the label. Moreover, we design an\ninter-client debiasing learning module to construct causal prototypes to reduce\nthe proportion of the background in prototype components. Notably, it bridges\nthe gap between heterogeneous representations via causal prototypical\nregularization. Extensive experiments on 2 benchmarking datasets demonstrate\nthat \\methodname{} significantly enhances the model capability to focus on main\nobjects in unseen data, leading to 4.5\\% higher Top-1 Accuracy on average over\n9 state-of-the-art existing methods.", "AI": {"tldr": "FedDDL\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u5316\u56e0\u679c\u56fe\u548c\u540e\u95e8\u8c03\u6574\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5c5e\u6027\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5c5e\u6027\u504f\u5dee\u5bfc\u81f4\u5c40\u90e8\u6a21\u578b\u4f18\u5316\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5168\u9762\u5206\u6790\u63a8\u7406\u8def\u5f84\u4e14\u53d7\u6df7\u6742\u56e0\u7d20\u5e72\u6270\u3002", "method": "\u63d0\u51faFedDDL\u65b9\u6cd5\uff0c\u5305\u62ec\u5ba2\u6237\u7aef\u5185\u53bb\u6df7\u6742\u6a21\u5757\u548c\u5ba2\u6237\u7aef\u95f4\u53bb\u504f\u6a21\u5757\uff0c\u901a\u8fc7\u56e0\u679c\u539f\u578b\u6b63\u5219\u5316\u8fde\u63a5\u5f02\u6784\u8868\u793a\u3002", "result": "\u57282\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFedDDL\u5e73\u5747Top-1\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad84.5%\u3002", "conclusion": "FedDDL\u6709\u6548\u63d0\u5347\u6a21\u578b\u5bf9\u4e3b\u5bf9\u8c61\u7684\u5173\u6ce8\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.05001", "pdf": "https://arxiv.org/pdf/2505.05001", "abs": "https://arxiv.org/abs/2505.05001", "authors": ["Lang Nie", "Chunyu Lin", "Kang Liao", "Yun Zhang", "Shuaicheng Liu", "Yao Zhao"], "title": "StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps", "categories": ["cs.CV", "cs.AI"], "comment": "TPAMI2025; https://github.com/nie-lang/StabStitch2. arXiv admin note:\n  text overlap with arXiv:2403.06378", "summary": "We retarget video stitching to an emerging issue, named warping shake, which\nunveils the temporal content shakes induced by sequentially unsmooth warps when\nextending image stitching to video stitching. Even if the input videos are\nstable, the stitched video can inevitably cause undesired warping shakes and\naffect the visual experience. To address this issue, we propose StabStitch++, a\nnovel video stitching framework to realize spatial stitching and temporal\nstabilization with unsupervised learning simultaneously. First, different from\nexisting learning-based image stitching solutions that typically warp one image\nto align with another, we suppose a virtual midplane between original image\nplanes and project them onto it. Concretely, we design a differentiable\nbidirectional decomposition module to disentangle the homography transformation\nand incorporate it into our spatial warp, evenly spreading alignment burdens\nand projective distortions across two views. Then, inspired by camera paths in\nvideo stabilization, we derive the mathematical expression of stitching\ntrajectories in video stitching by elaborately integrating spatial and temporal\nwarps. Finally, a warp smoothing model is presented to produce stable stitched\nvideos with a hybrid loss to simultaneously encourage content alignment,\ntrajectory smoothness, and online collaboration. Compared with StabStitch that\nsacrifices alignment for stabilization, StabStitch++ makes no compromise and\noptimizes both of them simultaneously, especially in the online mode. To\nestablish an evaluation benchmark and train the learning framework, we build a\nvideo stitching dataset with a rich diversity in camera motions and scenes.\nExperiments exhibit that StabStitch++ surpasses current solutions in stitching\nperformance, robustness, and efficiency, offering compelling advancements in\nthis field by building a real-time online video stitching system.", "AI": {"tldr": "StabStitch++\u662f\u4e00\u4e2a\u65b0\u578b\u89c6\u9891\u62fc\u63a5\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u540c\u65f6\u5b9e\u73b0\u7a7a\u95f4\u62fc\u63a5\u548c\u65f6\u95f4\u7a33\u5b9a\u5316\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u62fc\u63a5\u4e2d\u7684\u201cwarping shake\u201d\u95ee\u9898\u3002", "motivation": "\u89c6\u9891\u62fc\u63a5\u4e2d\u7531\u4e8e\u8fde\u7eed\u4e0d\u5e73\u6ed1\u7684\u626d\u66f2\u5bfc\u81f4\u7684\u65f6\u95f4\u5185\u5bb9\u6296\u52a8\uff08warping shake\uff09\u4f1a\u5f71\u54cd\u89c6\u89c9\u4f53\u9a8c\uff0c\u5373\u4f7f\u8f93\u5165\u89c6\u9891\u7a33\u5b9a\uff0c\u62fc\u63a5\u89c6\u9891\u4ecd\u53ef\u80fd\u4ea7\u751f\u4e0d\u5e0c\u671b\u7684\u6296\u52a8\u3002", "method": "\u63d0\u51fa\u865a\u62df\u4e2d\u95f4\u5e73\u9762\u548c\u53cc\u5411\u5206\u89e3\u6a21\u5757\uff0c\u5747\u5300\u5206\u914d\u5bf9\u9f50\u8d1f\u62c5\u548c\u6295\u5f71\u5931\u771f\uff1b\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u626d\u66f2\u63a8\u5bfc\u62fc\u63a5\u8f68\u8ff9\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\uff1b\u63d0\u51fa\u5e73\u6ed1\u6a21\u578b\uff0c\u4f7f\u7528\u6df7\u5408\u635f\u5931\u4f18\u5316\u5185\u5bb9\u5bf9\u9f50\u548c\u8f68\u8ff9\u5e73\u6ed1\u3002", "result": "StabStitch++\u5728\u62fc\u63a5\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5728\u7ebf\u89c6\u9891\u62fc\u63a5\u7cfb\u7edf\u3002", "conclusion": "StabStitch++\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u5bf9\u9f50\u548c\u7a33\u5b9a\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u62fc\u63a5\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.05004", "pdf": "https://arxiv.org/pdf/2505.05004", "abs": "https://arxiv.org/abs/2505.05004", "authors": ["Hendrik M\u00f6ller", "Hanna Sch\u00f6n", "Alina Dima", "Benjamin Keinert-Weth", "Robert Graf", "Matan Atad", "Johannes Paetzold", "Friederike Jungmann", "Rickmer Braren", "Florian Kofler", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke"], "title": "Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar\ntransitional vertebrae or enumeration anomalies. While some studies manually\nassess these anomalies and describe the ribs qualitatively, this study aims to\nautomate thoracolumbar stump rib detection and analyze their morphology\nquantitatively. To this end, we train a high-resolution deep-learning model for\nrib segmentation and show significant improvements compared to existing models\n(Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative\nalgorithm and piece-wise linear interpolation to assess the length of the ribs,\nshowing a success rate of 98.2%. When analyzing morphological features, we show\nthat stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs\n-13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1,\np-value < 0.01), and are oriented more downwards and sideways within the first\ncentimeters in contrast to full-length ribs. We show that with partially\nvisible ribs, these features can achieve an F1-score of 0.84 in differentiating\nstump ribs from regular ones. We publish the model weights and masks for public\nuse.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u81ea\u52a8\u5316\u68c0\u6d4b\u80f8\u8170\u690e\u6b8b\u6839\u808b\u9aa8\uff0c\u5e76\u5b9a\u91cf\u5206\u6790\u5176\u5f62\u6001\u7279\u5f81\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u80f8\u8170\u690e\u6b8b\u6839\u808b\u9aa8\u662f\u80f8\u8170\u690e\u8fc7\u6e21\u690e\u6216\u8ba1\u6570\u5f02\u5e38\u7684\u91cd\u8981\u6307\u6807\uff0c\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u4eba\u5de5\u5b9a\u6027\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u81ea\u52a8\u5316\u68c0\u6d4b\u4e0e\u5b9a\u91cf\u5206\u6790\u3002", "method": "\u8bad\u7ec3\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u808b\u9aa8\u5206\u5272\uff0c\u5e76\u91c7\u7528\u8fed\u4ee3\u7b97\u6cd5\u548c\u5206\u6bb5\u7ebf\u6027\u63d2\u503c\u8bc4\u4f30\u808b\u9aa8\u957f\u5ea6\u3002", "result": "\u6a21\u578b\u5206\u5272\u6548\u679c\u663e\u8457\u63d0\u5347\uff08Dice\u5206\u65700.997 vs. 0.779\uff09\uff0c\u808b\u9aa8\u957f\u5ea6\u8bc4\u4f30\u6210\u529f\u7387\u8fbe98.2%\u3002\u6b8b\u6839\u808b\u9aa8\u5f62\u6001\u7279\u5f81\u660e\u663e\u4e0d\u540c\uff08\u5982\u66f4\u8584\u3001\u66f4\u9760\u540e\u7b49\uff09\uff0c\u533a\u5206\u6b8b\u6839\u808b\u9aa8\u4e0e\u6b63\u5e38\u808b\u9aa8\u7684F1\u5206\u6570\u8fbe0.84\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u80f8\u8170\u690e\u6b8b\u6839\u808b\u9aa8\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u4e0e\u5b9a\u91cf\u5206\u6790\uff0c\u6a21\u578b\u6743\u91cd\u548c\u63a9\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.05007", "pdf": "https://arxiv.org/pdf/2505.05007", "abs": "https://arxiv.org/abs/2505.05007", "authors": ["Xin Bi", "Zhichao Li", "Yuxuan Xia", "Panpan Tong", "Lijuan Zhang", "Yang Chen", "Junsheng Fu"], "title": "Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition", "categories": ["cs.CV"], "comment": "9 pages and 12 figures. Under review at IEEE RA-L", "summary": "Accurate online map matching is fundamental to vehicle navigation and the\nactivation of intelligent driving functions. Current online map matching\nmethods are prone to errors in complex road networks, especially in multilevel\nroad area. To address this challenge, we propose an online Standard Definition\n(SD) map matching method by constructing a Hidden Markov Model (HMM) with\nmultiple probability factors. Our proposed method can achieve accurate map\nmatching even in complex road networks by carefully leveraging lane markings\nand scenario recognition in the designing of the probability factors. First,\nthe lane markings are generated by a multi-lane tracking method and associated\nwith the SD map using HMM to build an enriched SD map. In areas covered by the\nenriched SD map, the vehicle can re-localize itself by performing Iterative\nClosest Point (ICP) registration for the lane markings. Then, the probability\nfactor accounting for the lane marking detection can be obtained using the\nassociation probability between adjacent lanes and roads. Second, the driving\nscenario recognition model is applied to generate the emission probability\nfactor of scenario recognition, which improves the performance of map matching\non elevated roads and ordinary urban roads underneath them. We validate our\nmethod through extensive road tests in Europe and China, and the experimental\nresults show that our proposed method effectively improves the online map\nmatching accuracy as compared to other existing methods, especially in\nmultilevel road area. Specifically, the experiments show that our proposed\nmethod achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset\nand test data of multilevel road areas in Shanghai respectively, significantly\noutperforming benchmark methods. The implementation is available at\nhttps://github.com/TRV-Lab/LMSR-OMM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHMM\u548c\u591a\u6982\u7387\u56e0\u5b50\u7684\u5728\u7ebfSD\u5730\u56fe\u5339\u914d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u8def\u7f51\u4e2d\u7684\u5339\u914d\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u5728\u7ebf\u5730\u56fe\u5339\u914d\u65b9\u6cd5\u5728\u590d\u6742\u8def\u7f51\uff08\u5c24\u5176\u662f\u591a\u5c42\u9053\u8def\u533a\u57df\uff09\u4e2d\u5bb9\u6613\u51fa\u9519\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u591a\u8f66\u9053\u8ddf\u8e2a\u751f\u6210\u8f66\u9053\u6807\u8bb0\uff0c\u7ed3\u5408HMM\u6784\u5efa\u589e\u5f3aSD\u5730\u56fe\uff1b\u5229\u7528ICP\u6ce8\u518c\u548c\u573a\u666f\u8bc6\u522b\u6a21\u578b\u8bbe\u8ba1\u6982\u7387\u56e0\u5b50\u3002", "result": "\u5728\u6b27\u6d32\u548c\u4e2d\u56fd\u7684\u9053\u8def\u6d4b\u8bd5\u4e2d\uff0cF1\u5206\u6570\u5206\u522b\u8fbe\u523098.04%\u548c94.60%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u8def\u7f51\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5728\u7ebf\u5730\u56fe\u5339\u914d\uff0c\u5c24\u5176\u5728\u591a\u5c42\u9053\u8def\u533a\u57df\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.05008", "pdf": "https://arxiv.org/pdf/2505.05008", "abs": "https://arxiv.org/abs/2505.05008", "authors": ["Xuesong Liu", "Tianyu Hao", "Emmett J. Ientilucci"], "title": "Adaptive Contextual Embedding for Robust Far-View Borehole Detection", "categories": ["cs.CV"], "comment": null, "summary": "In controlled blasting operations, accurately detecting densely distributed\ntiny boreholes from far-view imagery is critical for operational safety and\nefficiency. However, existing detection methods often struggle due to small\nobject scales, highly dense arrangements, and limited distinctive visual\nfeatures of boreholes. To address these challenges, we propose an adaptive\ndetection approach that builds upon existing architectures (e.g., YOLO) by\nexplicitly leveraging consistent embedding representations derived through\nexponential moving average (EMA)-based statistical updates.\n  Our method introduces three synergistic components: (1) adaptive augmentation\nutilizing dynamically updated image statistics to robustly handle illumination\nand texture variations; (2) embedding stabilization to ensure consistent and\nreliable feature extraction; and (3) contextual refinement leveraging spatial\ncontext for improved detection accuracy. The pervasive use of EMA in our method\nis particularly advantageous given the limited visual complexity and small\nscale of boreholes, allowing stable and robust representation learning even\nunder challenging visual conditions. Experiments on a challenging proprietary\nquarry-site dataset demonstrate substantial improvements over baseline\nYOLO-based architectures, highlighting our method's effectiveness in realistic\nand complex industrial scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7EMA\u7edf\u8ba1\u66f4\u65b0\u6539\u8fdbYOLO\u67b6\u6784\uff0c\u89e3\u51b3\u5c0f\u5c3a\u5ea6\u3001\u9ad8\u5bc6\u5ea6\u5206\u5e03\u7684\u94bb\u5b54\u68c0\u6d4b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u5c0f\u5c3a\u5ea6\u3001\u9ad8\u5bc6\u5ea6\u5206\u5e03\u7684\u94bb\u5b54\uff0c\u5f71\u54cd\u7206\u7834\u64cd\u4f5c\u7684\u5b89\u5168\u548c\u6548\u7387\u3002", "method": "\u5f15\u5165\u81ea\u9002\u5e94\u589e\u5f3a\u3001\u5d4c\u5165\u7a33\u5b9a\u5316\u548c\u4e0a\u4e0b\u6587\u7ec6\u5316\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5229\u7528EMA\u7edf\u8ba1\u66f4\u65b0\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u548c\u68c0\u6d4b\u3002", "result": "\u5728\u91c7\u77f3\u573a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfYOLO\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5de5\u4e1a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86\u94bb\u5b54\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.05022", "pdf": "https://arxiv.org/pdf/2505.05022", "abs": "https://arxiv.org/abs/2505.05022", "authors": ["Tingting Liao", "Yujian Zheng", "Adilbek Karmanov", "Liwen Hu", "Leyang Jin", "Yuliang Xiu", "Hao Li"], "title": "SOAP: Style-Omniscient Animatable Portraits", "categories": ["cs.CV"], "comment": null, "summary": "Creating animatable 3D avatars from a single image remains challenging due to\nstyle limitations (realistic, cartoon, anime) and difficulties in handling\naccessories or hairstyles. While 3D diffusion models advance single-view\nreconstruction for general objects, outputs often lack animation controls or\nsuffer from artifacts because of the domain gap. We propose SOAP, a\nstyle-omniscient framework to generate rigged, topology-consistent avatars from\nany portrait. Our method leverages a multiview diffusion model trained on 24K\n3D heads with multiple styles and an adaptive optimization pipeline to deform\nthe FLAME mesh while maintaining topology and rigging via differentiable\nrendering. The resulting textured avatars support FACS-based animation,\nintegrate with eyeballs and teeth, and preserve details like braided hair or\naccessories. Extensive experiments demonstrate the superiority of our method\nover state-of-the-art techniques for both single-view head modeling and\ndiffusion-based generation of Image-to-3D. Our code and data are publicly\navailable for research purposes at https://github.com/TingtingLiao/soap.", "AI": {"tldr": "SOAP\u662f\u4e00\u4e2a\u98ce\u683c\u5168\u77e5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4efb\u4f55\u8096\u50cf\u751f\u6210\u5177\u6709\u62d3\u6251\u4e00\u81f4\u6027\u548c\u7ed1\u5b9a\u63a7\u5236\u76843D\u5934\u50cf\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u5f20\u56fe\u50cf\u521b\u5efa\u53ef\u52a8\u753b3D\u5934\u50cf\u65f6\u7684\u98ce\u683c\u9650\u5236\u548c\u914d\u4ef6/\u53d1\u578b\u5904\u7406\u56f0\u96be\u3002", "method": "\u5229\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u548c\u81ea\u9002\u5e94\u4f18\u5316\u7ba1\u9053\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u4fdd\u6301FLAME\u7f51\u683c\u7684\u62d3\u6251\u548c\u7ed1\u5b9a\u3002", "result": "\u751f\u6210\u7684\u7eb9\u7406\u5316\u5934\u50cf\u652f\u6301FACS\u52a8\u753b\uff0c\u4fdd\u7559\u7ec6\u8282\uff08\u5982\u7f16\u7ec7\u5934\u53d1\u6216\u914d\u4ef6\uff09\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "SOAP\u5728\u5355\u89c6\u89d2\u5934\u50cf\u5efa\u6a21\u548c\u57fa\u4e8e\u6269\u6563\u7684Image-to-3D\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.05023", "pdf": "https://arxiv.org/pdf/2505.05023", "abs": "https://arxiv.org/abs/2505.05023", "authors": ["Jialei Chen", "Xu Zheng", "Dongyue Li", "Chong Yi", "Seigo Ito", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "title": "Split Matching for Inductive Zero-shot Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not\nannotated during training. While fine-tuning vision-language models has\nachieved promising results, these models often overfit to seen categories due\nto the lack of supervision for unseen classes. As an alternative to fully\nsupervised approaches, query-based segmentation has shown great latent in ZSS,\nas it enables object localization without relying on explicit labels. However,\nconventional Hungarian matching, a core component in query-based frameworks,\nneeds full supervision and often misclassifies unseen categories as background\nin the setting of ZSS. To address this issue, we propose Split Matching (SM), a\nnovel assignment strategy that decouples Hungarian matching into two\ncomponents: one for seen classes in annotated regions and another for latent\nclasses in unannotated regions (referred to as unseen candidates).\nSpecifically, we partition the queries into seen and candidate groups, enabling\neach to be optimized independently according to its available supervision. To\ndiscover unseen candidates, we cluster CLIP dense features to generate pseudo\nmasks and extract region-level embeddings using CLS tokens. Matching is then\nconducted separately for the two groups based on both class-level similarity\nand mask-level consistency. Additionally, we introduce a Multi-scale Feature\nEnhancement (MFE) module that refines decoder features through residual\nmulti-scale aggregation, improving the model's ability to capture spatial\ndetails across resolutions. SM is the first to introduce decoupled Hungarian\nmatching under the inductive ZSS setting, and achieves state-of-the-art\nperformance on two standard benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSplit Matching\uff08SM\uff09\u7684\u65b0\u5206\u914d\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u4f20\u7edf\u5308\u7259\u5229\u5339\u914d\u5bf9\u672a\u89c1\u7c7b\u522b\u5206\u7c7b\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5c06\u5339\u914d\u5206\u4e3a\u53ef\u89c1\u7c7b\u522b\u548c\u6f5c\u5728\u7c7b\u522b\u4e24\u90e8\u5206\uff0c\u5e76\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0cSM\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\uff08ZSS\uff09\u7684\u76ee\u6807\u662f\u5206\u5272\u8bad\u7ec3\u4e2d\u672a\u6807\u6ce8\u7684\u7c7b\u522b\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u76d1\u7763\u800c\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u5df2\u89c1\u7c7b\u522b\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u5339\u914d\u7b56\u7565\u548c\u7279\u5f81\u589e\u5f3a\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faSplit Matching\uff08SM\uff09\uff0c\u5c06\u5308\u7259\u5229\u5339\u914d\u89e3\u8026\u4e3a\u53ef\u89c1\u7c7b\u522b\u548c\u6f5c\u5728\u7c7b\u522b\u4e24\u90e8\u5206\uff0c\u5206\u522b\u4f18\u5316\u3002\u901a\u8fc7\u805a\u7c7bCLIP\u5bc6\u96c6\u7279\u5f81\u751f\u6210\u4f2a\u63a9\u7801\uff0c\u5e76\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff08MFE\uff09\u63d0\u5347\u7a7a\u95f4\u7ec6\u8282\u6355\u6349\u80fd\u529b\u3002", "result": "SM\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "SM\u901a\u8fc7\u89e3\u8026\u5339\u914d\u7b56\u7565\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.05043", "pdf": "https://arxiv.org/pdf/2505.05043", "abs": "https://arxiv.org/abs/2505.05043", "authors": ["Mani Kumar Tellamekala", "Shashank Jaiswal", "Thomas Smith", "Timur Alamev", "Gary McKeown", "Anthony Brown", "Michel Valstar"], "title": "xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous Affect Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recognising expressive behaviours in face videos is a long-standing challenge\nin Affective Computing. Despite significant advancements in recent years, it\nstill remains a challenge to build a robust and reliable system for\nnaturalistic and in-the-wild facial expressive behaviour analysis in real time.\nThis paper addresses two key challenges in building such a system: (1). The\npaucity of large-scale labelled facial affect video datasets with extensive\ncoverage of the 2D emotion space, and (2). The difficulty of extracting facial\nvideo features that are discriminative, interpretable, robust, and\ncomputationally efficient. Toward addressing these challenges, we introduce\nxTrace, a robust tool for facial expressive behaviour analysis and predicting\ncontinuous values of dimensional emotions, namely valence and arousal, from\nin-the-wild face videos.\n  To address challenge (1), our affect recognition model is trained on the\nlargest facial affect video data set, containing ~450k videos that cover most\nemotion zones in the dimensional emotion space, making xTrace highly versatile\nin analysing a wide spectrum of naturalistic expressive behaviours. To address\nchallenge (2), xTrace uses facial affect descriptors that are not only\nexplainable, but can also achieve a high degree of accuracy and robustness with\nlow computational complexity. The key components of xTrace are benchmarked\nagainst three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox.\nOn an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86\nmean CCC and 0.13 mean absolute error values. We present a detailed error\nanalysis of affect predictions from xTrace, illustrating (a). its ability to\nrecognise emotions with high accuracy across most bins in the 2D emotion space,\n(b). its robustness to non-frontal head pose angles, and (c). a strong\ncorrelation between its uncertainty estimates and its accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faxTrace\u5de5\u5177\uff0c\u89e3\u51b3\u81ea\u7136\u73af\u5883\u4e0b\u5b9e\u65f6\u9762\u90e8\u8868\u60c5\u5206\u6790\u7684\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u8bb0\u6570\u636e\u96c6\u548c\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u3002xTrace\u572850k\u89c6\u9891\u9a8c\u8bc1\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u73af\u5883\u4e0b\u5b9e\u65f6\u9762\u90e8\u8868\u60c5\u5206\u6790\u7684\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u8bb0\u6570\u636e\u96c6\u548c\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u3002", "method": "xTrace\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08450k\u89c6\u9891\uff09\u548c\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u9762\u90e8\u7279\u5f81\u63cf\u8ff0\u7b26\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u572850k\u89c6\u9891\u9a8c\u8bc1\u96c6\u4e0a\uff0cxTrace\u8fbe\u52300.86\u5e73\u5747CCC\u548c0.13\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u3002", "conclusion": "xTrace\u57282D\u60c5\u611f\u7a7a\u95f4\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u975e\u6b63\u9762\u5934\u90e8\u59ff\u6001\u9c81\u68d2\uff0c\u4e14\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0e\u51c6\u786e\u6027\u9ad8\u5ea6\u76f8\u5173\u3002"}}
{"id": "2505.05049", "pdf": "https://arxiv.org/pdf/2505.05049", "abs": "https://arxiv.org/abs/2505.05049", "authors": ["Timo Kaiser", "Thomas Norrenbrock", "Bodo Rosenhahn"], "title": "UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model", "categories": ["cs.CV"], "comment": "Accepted to ICML'25", "summary": "The introduction of the Segment Anything Model (SAM) has paved the way for\nnumerous semantic segmentation applications. For several tasks, quantifying the\nuncertainty of SAM is of particular interest. However, the ambiguous nature of\nthe class-agnostic foundation model SAM challenges current uncertainty\nquantification (UQ) approaches. This paper presents a theoretically motivated\nuncertainty quantification model based on a Bayesian entropy formulation\njointly respecting aleatoric, epistemic, and the newly introduced task\nuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ\nmethod. Our model traces the root of uncertainty back to under-parameterised\nmodels, insufficient prompts or image ambiguities. Our proposed deterministic\nUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,\nDAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ\nalternative that can support user-prompting, enhance semi-supervised pipelines,\nor balance the tradeoff between accuracy and cost efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u71b5\u7684\u8f7b\u91cf\u7ea7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5USAM\uff0c\u7528\u4e8e\u89e3\u51b3Segment Anything Model\uff08SAM\uff09\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eSAM\u6a21\u578b\u7684\u7c7b\u4e0d\u53ef\u77e5\u6027\u548c\u6a21\u7cca\u6027\uff0c\u73b0\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u96be\u4ee5\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u5176\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u71b5\u7684\u7406\u8bba\u6a21\u578b\uff0c\u8054\u5408\u8003\u8651\u5076\u7136\u6027\u3001\u8ba4\u77e5\u6027\u548c\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u8bad\u7ec3\u4e86\u8f7b\u91cf\u7ea7\u7684\u540e\u5904\u7406\u65b9\u6cd5USAM\u3002", "result": "USAM\u5728SA-V\u3001MOSE\u3001ADE20k\u3001DAVIS\u548cCOCO\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3001\u6613\u4e8e\u4f7f\u7528\u3002", "conclusion": "USAM\u4e3aSAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u7528\u6237\u63d0\u793a\u3001\u589e\u5f3a\u534a\u76d1\u7763\u6d41\u7a0b\uff0c\u5e76\u5e73\u8861\u4e86\u51c6\u786e\u6027\u4e0e\u6210\u672c\u6548\u7387\u3002"}}
{"id": "2505.05062", "pdf": "https://arxiv.org/pdf/2505.05062", "abs": "https://arxiv.org/abs/2505.05062", "authors": ["Enhao Zhang", "Chaohua Li", "Chuanxing Geng", "Songcan Chen"], "title": "ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Based on the success of large-scale visual foundation models like CLIP in\nvarious downstream tasks, this paper initially attempts to explore their impact\non Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation\nmodel with three strategies: Linear Probing (LP), Lightweight Fine-Tuning\n(LFT), and Full Fine-Tuning (FFT). Our analysis presents the following\ninsights: i) Compared to LTSSL algorithms trained from scratch, FFT results in\na decline in model performance, whereas LP and LFT, although boosting overall\nmodel performance, exhibit negligible benefits to tail classes. ii) LP produces\nnumerous false pseudo-labels due to \\textit{underlearned} training data, while\nLFT can reduce the number of these false labels but becomes overconfident about\nthem owing to \\textit{biased fitting} training data. This exacerbates the\npseudo-labeled and classifier biases inherent in LTSSL, limiting performance\nimprovement in the tail classes. With these insights, we propose a Unbiased\nLightweight Fine-tuning strategy, \\textbf{ULFine}, which mitigates the\noverconfidence via confidence-aware adaptive fitting of textual prototypes and\ncounteracts the pseudo-labeled and classifier biases via complementary fusion\nof dual logits. Extensive experiments demonstrate that ULFine markedly\ndecreases training costs by over ten times and substantially increases\nprediction accuracies compared to state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u89c4\u6a21\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u5bf9\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\uff08LTSSL\uff09\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u7b56\u7565\uff08LP\u3001LFT\u3001FFT\uff09\uff0c\u53d1\u73b0FFT\u6027\u80fd\u4e0b\u964d\uff0cLP\u548cLFT\u5bf9\u5c3e\u90e8\u7c7b\u522b\u5e2e\u52a9\u6709\u9650\u3002\u4f5c\u8005\u63d0\u51faULFine\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u62df\u5408\u548c\u53cclogit\u878d\u5408\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u57fa\u7840\u6a21\u578b\u5728LTSSL\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u73b0\u6709\u7b56\u7565\u5728\u5c3e\u90e8\u7c7b\u522b\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528LP\u3001LFT\u3001FFT\u4e09\u79cd\u7b56\u7565\u5206\u6790\u57fa\u7840\u6a21\u578b\u5728LTSSL\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51faULFine\u7b56\u7565\u3002", "result": "FFT\u6027\u80fd\u4e0b\u964d\uff0cLP\u548cLFT\u5bf9\u5c3e\u90e8\u7c7b\u522b\u5e2e\u52a9\u6709\u9650\uff1bULFine\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "ULFine\u901a\u8fc7\u81ea\u9002\u5e94\u62df\u5408\u548c\u53cclogit\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86LTSSL\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.05071", "pdf": "https://arxiv.org/pdf/2505.05071", "abs": "https://arxiv.org/abs/2505.05071", "authors": ["Chunyu Xie", "Bin Wang", "Fanjing Kong", "Jincheng Li", "Dawei Liang", "Gengshen Zhang", "Dawei Leng", "Yuhui Yin"], "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.", "AI": {"tldr": "FG-CLIP\u901a\u8fc7\u751f\u6210\u957f\u6807\u9898-\u56fe\u50cf\u5bf9\u3001\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u5f15\u5165\u56f0\u96be\u8d1f\u6837\u672c\uff0c\u63d0\u5347\u4e86CLIP\u5728\u7ec6\u7c92\u5ea6\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "CLIP\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u7406\u89e3\u4e0a\u56e0\u4f9d\u8d56\u7c97\u7c92\u5ea6\u77ed\u6807\u9898\u800c\u53d7\u9650\u3002", "method": "1. \u751f\u621016\u4ebf\u957f\u6807\u9898-\u56fe\u50cf\u5bf9\uff1b2. \u6784\u5efa1200\u4e07\u56fe\u50cf\u548c4000\u4e07\u533a\u57df\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b3. \u5f15\u51651000\u4e07\u56f0\u96be\u8d1f\u6837\u672c\u3002", "result": "FG-CLIP\u5728\u7ec6\u7c92\u5ea6\u7406\u89e3\u3001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8eCLIP\u548c\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FG-CLIP\u6709\u6548\u63d0\u5347\u7ec6\u7c92\u5ea6\u7ec6\u8282\u6355\u6349\u80fd\u529b\uff0c\u663e\u8457\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.05074", "pdf": "https://arxiv.org/pdf/2505.05074", "abs": "https://arxiv.org/abs/2505.05074", "authors": ["Tommaso Apicella", "Alessio Xompero", "Andrea Cavallaro"], "title": "Visual Affordances: Enabling Robots to Understand Object Functionality", "categories": ["cs.CV", "cs.RO"], "comment": "24 pages, 12 figures, 10 tables. Project website at\n  https://apicis.github.io/aff-survey/", "summary": "Human-robot interaction for assistive technologies relies on the prediction\nof affordances, which are the potential actions a robot can perform on objects.\nPredicting object affordances from visual perception is formulated differently\nfor tasks such as grasping detection, affordance classification, affordance\nsegmentation, and hand-object interaction synthesis. In this work, we highlight\nthe reproducibility issue in these redefinitions, making comparative benchmarks\nunfair and unreliable. To address this problem, we propose a unified\nformulation for visual affordance prediction, provide a comprehensive and\nsystematic review of previous works highlighting strengths and limitations of\nmethods and datasets, and analyse what challenges reproducibility. To favour\ntransparency, we introduce the Affordance Sheet, a document to detail the\nproposed solution, the datasets, and the validation. As the physical properties\nof an object influence the interaction with the robot, we present a generic\nframework that links visual affordance prediction to the physical world. Using\nthe weight of an object as an example for this framework, we discuss how\nestimating object mass can affect the affordance prediction. Our approach\nbridges the gap between affordance perception and robot actuation, and accounts\nfor the complete information about objects of interest and how the robot\ninteracts with them to accomplish its task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9\u53ef\u4f9b\u6027\u9884\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u5b9a\u4e49\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u590d\u73b0\u6027\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86Affordance Sheet\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u53ef\u4f9b\u6027\u9884\u6d4b\u56e0\u4efb\u52a1\u4e0d\u540c\u800c\u5b9a\u4e49\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u590d\u73b0\u6027\u5dee\uff0c\u96be\u4ee5\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u53ef\u4f9b\u6027\u9884\u6d4b\u6846\u67b6\uff0c\u7cfb\u7edf\u56de\u987e\u73b0\u6709\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165Affordance Sheet\u8bb0\u5f55\u89e3\u51b3\u65b9\u6848\u548c\u9a8c\u8bc1\u7ec6\u8282\u3002", "result": "\u901a\u8fc7\u5c06\u89c6\u89c9\u53ef\u4f9b\u6027\u9884\u6d4b\u4e0e\u7269\u7406\u4e16\u754c\u5173\u8054\uff0c\u5c55\u793a\u4e86\u7269\u4f53\u8d28\u91cf\u5bf9\u53ef\u4f9b\u6027\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u53ef\u4f9b\u6027\u611f\u77e5\u4e0e\u673a\u5668\u4eba\u6267\u884c\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u4efb\u52a1\u5b8c\u6210\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u4fe1\u606f\u3002"}}
{"id": "2505.05081", "pdf": "https://arxiv.org/pdf/2505.05081", "abs": "https://arxiv.org/abs/2505.05081", "authors": ["Jinyu Gu", "Haipeng Liu", "Meng Wang", "Yang Wang"], "title": "PIDiff: Image Customization for Personalized Identities with Diffusion Models", "categories": ["cs.CV"], "comment": "9 pages, 11 figures", "summary": "Text-to-image generation for personalized identities aims at incorporating\nthe specific identity into images using a text prompt and an identity image.\nBased on the powerful generative capabilities of DDPMs, many previous works\nadopt additional prompts, such as text embeddings and CLIP image embeddings, to\nrepresent the identity information, while they fail to disentangle the identity\ninformation and background information. As a result, the generated images not\nonly lose key identity characteristics but also suffer from significantly\nreduced diversity. To address this issue, previous works have combined the W+\nspace from StyleGAN with diffusion models, leveraging this space to provide a\nmore accurate and comprehensive representation of identity features through\nmulti-level feature extraction. However, the entanglement of identity and\nbackground information in in-the-wild images during training prevents accurate\nidentity localization, resulting in severe semantic interference between\nidentity and background. In this paper, we propose a novel fine-tuning-based\ndiffusion model for personalized identities text-to-image generation, named\nPIDiff, which leverages the W+ space and an identity-tailored fine-tuning\nstrategy to avoid semantic entanglement and achieves accurate feature\nextraction and localization. Style editing can also be achieved by PIDiff\nthrough preserving the characteristics of identity features in the W+ space,\nwhich vary from coarse to fine. Through the combination of the proposed\ncross-attention block and parameter optimization strategy, PIDiff preserves the\nidentity information and maintains the generation capability for in-the-wild\nimages of the pre-trained model during inference. Our experimental results\nvalidate the effectiveness of our method in this task.", "AI": {"tldr": "PIDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7W+\u7a7a\u95f4\u548c\u8eab\u4efd\u5b9a\u5236\u5fae\u8c03\u7b56\u7565\u89e3\u51b3\u8eab\u4efd\u4e0e\u80cc\u666f\u4fe1\u606f\u7ea0\u7f20\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u672a\u80fd\u6709\u6548\u5206\u79bb\u8eab\u4efd\u4e0e\u80cc\u666f\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u5931\u53bb\u5173\u952e\u8eab\u4efd\u7279\u5f81\u4e14\u591a\u6837\u6027\u964d\u4f4e\u3002", "method": "\u63d0\u51faPIDiff\u6a21\u578b\uff0c\u5229\u7528W+\u7a7a\u95f4\u548c\u591a\u7ea7\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u5757\u548c\u53c2\u6570\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u8eab\u4efd\u7279\u5f81\u7684\u7cbe\u786e\u5b9a\u4f4d\u548c\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1PIDiff\u5728\u4fdd\u7559\u8eab\u4efd\u4fe1\u606f\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u91ce\u5916\u56fe\u50cf\u7684\u751f\u6210\u80fd\u529b\u3002", "conclusion": "PIDiff\u901a\u8fc7\u907f\u514d\u8bed\u4e49\u7ea0\u7f20\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2505.05089", "pdf": "https://arxiv.org/pdf/2505.05089", "abs": "https://arxiv.org/abs/2505.05089", "authors": ["Zuntao Liu", "Hao Zhuang", "Junjie Jiang", "Yuhang Song", "Zheng Fang"], "title": "Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow", "categories": ["cs.CV"], "comment": "Accepted to ICRA 2025. Project Page:\n  https://wynelio.github.io/E-NMSTFlow", "summary": "Event cameras have the potential to capture continuous motion information\nover time and space, making them well-suited for optical flow estimation.\nHowever, most existing learning-based methods for event-based optical flow\nadopt frame-based techniques, ignoring the spatio-temporal characteristics of\nevents. Additionally, these methods assume linear motion between consecutive\nevents within the loss time window, which increases optical flow errors in\nlong-time sequences. In this work, we observe that rich spatio-temporal\ninformation and accurate nonlinear motion between events are crucial for\nevent-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel\nunsupervised event-based optical flow network focusing on long-time sequences.\nWe propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an\nAdaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich\nspatio-temporal information to learn spatio-temporal data associations.\nMeanwhile, we propose a nonlinear motion compensation loss that utilizes the\naccurate nonlinear motion between events to improve the unsupervised learning\nof our network. Extensive experiments demonstrate the effectiveness and\nsuperiority of our method. Remarkably, our method ranks first among\nunsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project\npage is available at https://wynelio.github.io/E-NMSTFlow.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aE-NMSTFlow\u7684\u65e0\u76d1\u7763\u4e8b\u4ef6\u5149\u6d41\u7f51\u7edc\uff0c\u4e13\u6ce8\u4e8e\u957f\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u5229\u7528\u4e30\u5bcc\u7684\u65f6\u7a7a\u4fe1\u606f\u548c\u975e\u7ebf\u6027\u8fd0\u52a8\u8865\u507f\u635f\u5931\u6765\u63d0\u5347\u5149\u6d41\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\u591a\u91c7\u7528\u5e27\u57fa\u6280\u672f\uff0c\u5ffd\u7565\u4e86\u4e8b\u4ef6\u7684\u65f6\u7a7a\u7279\u6027\uff0c\u4e14\u5047\u8bbe\u4e8b\u4ef6\u95f4\u4e3a\u7ebf\u6027\u8fd0\u52a8\uff0c\u5bfc\u81f4\u957f\u65f6\u95f4\u5e8f\u5217\u4e2d\u8bef\u5dee\u589e\u52a0\u3002", "method": "\u63d0\u51fa\u4e86\u65f6\u7a7a\u8fd0\u52a8\u7279\u5f81\u611f\u77e5\u6a21\u5757\uff08STMFA\uff09\u548c\u81ea\u9002\u5e94\u8fd0\u52a8\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff08AMFE\uff09\uff0c\u5e76\u5f15\u5165\u975e\u7ebf\u6027\u8fd0\u52a8\u8865\u507f\u635f\u5931\u3002", "result": "\u5728MVSEC\u548cDSEC-Flow\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "E-NMSTFlow\u901a\u8fc7\u5145\u5206\u5229\u7528\u65f6\u7a7a\u4fe1\u606f\u548c\u975e\u7ebf\u6027\u8fd0\u52a8\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u5149\u6d41\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.05091", "pdf": "https://arxiv.org/pdf/2505.05091", "abs": "https://arxiv.org/abs/2505.05091", "authors": ["Shashank Agnihotri", "Amaan Ansari", "Annika Dackermann", "Fabian R\u00f6sch", "Margret Keuper"], "title": "DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at CVPR 2025 Workshop on Synthetic Data for Computer Vision", "summary": "Deep learning (DL) has surpassed human performance on standard benchmarks,\ndriving its widespread adoption in computer vision tasks. One such task is\ndisparity estimation, estimating the disparity between matching pixels in\nstereo image pairs, which is crucial for safety-critical applications like\nmedical surgeries and autonomous navigation. However, DL-based disparity\nestimation methods are highly susceptible to distribution shifts and\nadversarial attacks, raising concerns about their reliability and\ngeneralization. Despite these concerns, a standardized benchmark for evaluating\nthe robustness of disparity estimation methods remains absent, hindering\nprogress in the field.\n  To address this gap, we introduce DispBench, a comprehensive benchmarking\ntool for systematically assessing the reliability of disparity estimation\nmethods. DispBench evaluates robustness against synthetic image corruptions\nsuch as adversarial attacks and out-of-distribution shifts caused by 2D Common\nCorruptions across multiple datasets and diverse corruption scenarios. We\nconduct the most extensive performance and robustness analysis of disparity\nestimation methods to date, uncovering key correlations between accuracy,\nreliability, and generalization. Open-source code for DispBench:\nhttps://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DispBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u89c6\u5dee\u4f30\u8ba1\u65b9\u6cd5\u53ef\u9760\u6027\u7684\u7efc\u5408\u57fa\u51c6\u5de5\u5177\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u89c6\u5dee\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bf9\u5206\u5e03\u504f\u79fb\u548c\u5bf9\u6297\u653b\u51fb\u7684\u654f\u611f\u6027\u5f15\u53d1\u4e86\u5bf9\u53ef\u9760\u6027\u548c\u6cdb\u5316\u6027\u7684\u62c5\u5fe7\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u963b\u788d\u4e86\u8fdb\u5c55\u3002", "method": "\u63d0\u51faDispBench\uff0c\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u635f\u574f\uff08\u5982\u5bf9\u6297\u653b\u51fb\u548c\u5206\u5e03\u5916\u504f\u79fb\uff09\u8bc4\u4f30\u89c6\u5dee\u4f30\u8ba1\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u8986\u76d6\u591a\u6570\u636e\u96c6\u548c\u591a\u6837\u635f\u574f\u573a\u666f\u3002", "result": "\u8fdb\u884c\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5e7f\u6cdb\u7684\u89c6\u5dee\u4f30\u8ba1\u65b9\u6cd5\u6027\u80fd\u548c\u9c81\u68d2\u6027\u5206\u6790\uff0c\u63ed\u793a\u4e86\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u6cdb\u5316\u6027\u4e4b\u95f4\u7684\u5173\u952e\u76f8\u5173\u6027\u3002", "conclusion": "DispBench\u4e3a\u89c6\u5dee\u4f30\u8ba1\u9886\u57df\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2505.05101", "pdf": "https://arxiv.org/pdf/2505.05101", "abs": "https://arxiv.org/abs/2505.05101", "authors": ["Hongyang Zhu", "Haipeng Liu", "Bo Fu", "Yang Wang"], "title": "MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models", "categories": ["cs.CV"], "comment": "9 pages, 7 figures", "summary": "Multi-object editing aims to modify multiple objects or regions in complex\nscenes while preserving structural coherence. This task faces significant\nchallenges in scenarios involving overlapping or interacting objects: (1)\nInaccurate localization of target objects due to attention misalignment,\nleading to incomplete or misplaced edits; (2) Attribute-object mismatch, where\ncolor or texture changes fail to align with intended regions due to\ncross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color\nbleeding into non-target areas). Existing methods struggle with these\nchallenges: approaches relying on global cross-attention mechanisms suffer from\nattention dilution and spatial interference between objects, while mask-based\nmethods fail to bind attributes to geometrically accurate regions due to\nfeature entanglement in multi-object scenarios. To address these limitations,\nwe propose a training-free, inference-stage optimization approach that enables\nprecise localized image manipulation in complex multi-object scenes, named\nMDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via\ntwo key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention\nwith segmentation masks for precise object positioning, and Color Consistency\nLoss (CCL) amplifies target attribute attention within masks while suppressing\nleakage to adjacent regions. This dual-loss design ensures localized and\ncoherent multi-object edits. Extensive experiments demonstrate that MDE-Edit\noutperforms state-of-the-art methods in editing accuracy and visual quality,\noffering a robust solution for complex multi-object image manipulation tasks.", "AI": {"tldr": "MDE-Edit\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u635f\u5931\u8bbe\u8ba1\uff08OAL\u548cCCL\uff09\u89e3\u51b3\u591a\u76ee\u6807\u7f16\u8f91\u4e2d\u7684\u5b9a\u4f4d\u4e0d\u51c6\u548c\u5c5e\u6027\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u591a\u76ee\u6807\u7f16\u8f91\u5728\u590d\u6742\u573a\u666f\u4e2d\u9762\u4e34\u5b9a\u4f4d\u4e0d\u51c6\u548c\u5c5e\u6027\u4e0d\u5339\u914d\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faMDE-Edit\u65b9\u6cd5\uff0c\u901a\u8fc7Object Alignment Loss\uff08OAL\uff09\u548cColor Consistency Loss\uff08CCL\uff09\u4f18\u5316\u6269\u6563\u6a21\u578b\u4e2d\u7684\u566a\u58f0\u6f5c\u5728\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMDE-Edit\u5728\u7f16\u8f91\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MDE-Edit\u4e3a\u590d\u6742\u591a\u76ee\u6807\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05136", "pdf": "https://arxiv.org/pdf/2505.05136", "abs": "https://arxiv.org/abs/2505.05136", "authors": ["Clara Tomasini", "Javier Rodriguez-Puigvert", "Dinora Polanco", "Manuel Vi\u00f1uales", "Luis Riazuelo", "Ana Cristina Murillo"], "title": "Automated vision-based assistance tools in bronchoscopy: stenosis severity estimation", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: Subglottic stenosis refers to the narrowing of the subglottis, the\nairway between the vocal cords and the trachea. Its severity is typically\nevaluated by estimating the percentage of obstructed airway. This estimation\ncan be obtained from CT data or through visual inspection by experts exploring\nthe region. However, visual inspections are inherently subjective, leading to\nless consistent and robust diagnoses. No public methods or datasets are\ncurrently available for automated evaluation of this condition from\nbronchoscopy video.\n  Methods: We propose a pipeline for automated subglottic stenosis severity\nestimation during the bronchoscopy exploration, without requiring the physician\nto traverse the stenosed region. Our approach exploits the physical effect of\nillumination decline in endoscopy to segment and track the lumen and obtain a\n3D model of the airway. This 3D model is obtained from a single frame and is\nused to measure the airway narrowing.\n  Results: Our pipeline is the first to enable automated and robust subglottic\nstenosis severity measurement using bronchoscopy images. The results show\nconsistency with ground-truth estimations from CT scans and expert estimations,\nand reliable repeatability across multiple estimations on the same patient. Our\nevaluation is performed on our new Subglottic Stenosis Dataset of real\nbronchoscopy procedures data.\n  Conclusion: We demonstrate how to automate evaluation of subglottic stenosis\nseverity using only bronchoscopy. Our approach can assist with and shorten\ndiagnosis and monitoring procedures, with automated and repeatable estimations\nand less exploration time, and save radiation exposure to patients as no CT is\nrequired. Additionally, we release the first public benchmark for subglottic\nstenosis severity assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u652f\u6c14\u7ba1\u955c\u56fe\u50cf\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u58f0\u95e8\u4e0b\u72ed\u7a84\u4e25\u91cd\u7a0b\u5ea6\u7684\u65b9\u6cd5\uff0c\u65e0\u9700CT\u626b\u63cf\uff0c\u51cf\u5c11\u4e86\u4e3b\u89c2\u6027\u548c\u8f90\u5c04\u66b4\u9732\u3002", "motivation": "\u58f0\u95e8\u4e0b\u72ed\u7a84\u7684\u8bc4\u4f30\u901a\u5e38\u4f9d\u8d56\u4e3b\u89c2\u89c6\u89c9\u68c0\u67e5\u6216CT\u626b\u63cf\uff0c\u524d\u8005\u4e0d\u51c6\u786e\uff0c\u540e\u8005\u6709\u8f90\u5c04\u98ce\u9669\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u652f\u6c14\u7ba1\u955c\u56fe\u50cf\u7684\u5149\u7167\u8870\u51cf\u6548\u5e94\u5206\u5272\u548c\u8ddf\u8e2a\u7ba1\u8154\uff0c\u6784\u5efa3D\u6a21\u578b\u4ee5\u6d4b\u91cf\u72ed\u7a84\u7a0b\u5ea6\u3002", "result": "\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u7ed3\u679c\u4e0eCT\u548c\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\uff0c\u5177\u6709\u53ef\u9760\u91cd\u590d\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u8f85\u52a9\u8bca\u65ad\uff0c\u51cf\u5c11\u68c0\u67e5\u65f6\u95f4\u548c\u8f90\u5c04\u66b4\u9732\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u3002"}}
{"id": "2505.05163", "pdf": "https://arxiv.org/pdf/2505.05163", "abs": "https://arxiv.org/abs/2505.05163", "authors": ["Aishwarya Venkataramanan", "Paul Bodesheim", "Joachim Denzler"], "title": "Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models", "categories": ["cs.CV", "cs.LG"], "comment": "UAI 2025, 22 pages", "summary": "Vision-Language Models (VLMs) learn joint representations by mapping images\nand text into a shared latent space. However, recent research highlights that\ndeterministic embeddings from standard VLMs often struggle to capture the\nuncertainties arising from the ambiguities in visual and textual descriptions\nand the multiple possible correspondences between images and texts. Existing\napproaches tackle this by learning probabilistic embeddings during VLM\ntraining, which demands large datasets and does not leverage the powerful\nrepresentations already learned by large-scale VLMs like CLIP. In this paper,\nwe propose GroVE, a post-hoc approach to obtaining probabilistic embeddings\nfrom frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model\n(GPLVM) to learn a shared low-dimensional latent space where image and text\ninputs are mapped to a unified representation, optimized through single-modal\nembedding reconstruction and cross-modal alignment objectives. Once trained,\nthe Gaussian Process model generates uncertainty-aware probabilistic\nembeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty\ncalibration across multiple downstream tasks, including cross-modal retrieval,\nvisual question answering, and active learning.", "AI": {"tldr": "GroVE\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u4ece\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u83b7\u53d6\u6982\u7387\u5d4c\u5165\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff08GPLVM\uff09\u4f18\u5316\u5355\u6a21\u6001\u91cd\u5efa\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u76ee\u6807\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5d4c\u5165\u3002", "motivation": "\u6807\u51c6VLM\u7684\u786e\u5b9a\u6027\u5d4c\u5165\u96be\u4ee5\u6355\u6349\u89c6\u89c9\u548c\u6587\u672c\u63cf\u8ff0\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u8bad\u7ec3\u4e14\u672a\u5145\u5206\u5229\u7528\u5df2\u6709VLM\u8868\u793a\u3002", "method": "\u57fa\u4e8eGPLVM\u6784\u5efa\u5171\u4eab\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f18\u5316\u5355\u6a21\u6001\u5d4c\u5165\u91cd\u5efa\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u76ee\u6807\uff0c\u751f\u6210\u6982\u7387\u5d4c\u5165\u3002", "result": "GroVE\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u4e3b\u52a8\u5b66\u4e60\u7b49\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u3002", "conclusion": "GroVE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86VLM\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2505.05183", "pdf": "https://arxiv.org/pdf/2505.05183", "abs": "https://arxiv.org/abs/2505.05183", "authors": ["Elad Feldman", "Jacob Shams", "Dudi Biton", "Alfred Chen", "Shaoyuan Xie", "Satoru Koda", "Yisroel Mirsky", "Asaf Shabtai", "Yuval Elovici", "Ben Nassi"], "title": "PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The safety of autonomous cars has come under scrutiny in recent years,\nespecially after 16 documented incidents involving Teslas (with autopilot\nengaged) crashing into parked emergency vehicles (police cars, ambulances, and\nfiretrucks). While previous studies have revealed that strong light sources\noften introduce flare artifacts in the captured image, which degrade the image\nquality, the impact of flare on object detection performance remains unclear.\nIn this research, we unveil PaniCar, a digital phenomenon that causes an object\ndetector's confidence score to fluctuate below detection thresholds when\nexposed to activated emergency vehicle lighting. This vulnerability poses a\nsignificant safety risk, and can cause autonomous vehicles to fail to detect\nobjects near emergency vehicles. In addition, this vulnerability could be\nexploited by adversaries to compromise the security of advanced driving\nassistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3,\n\"manufacturer C\", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors\n(YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle\nlighting to understand the influence of various technical and environmental\nfactors. We also evaluate four SOTA flare removal methods and show that their\nperformance and latency are insufficient for real-time driving constraints. To\nmitigate this risk, we propose Caracetamol, a robust framework designed to\nenhance the resilience of object detectors against the effects of activated\nemergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster\nRCNN, Caracetamol improves the models' average confidence of car detection by\n0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by\n0.33. In addition, Caracetamol is capable of processing frames at a rate of\nbetween 30-50 FPS, enabling real-time ADAS car detection.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u7d27\u6025\u8f66\u8f86\u706f\u5149\u4e0b\u5b58\u5728\u68c0\u6d4b\u6f0f\u6d1e\uff0c\u63d0\u51faCaracetamol\u6846\u67b6\u63d0\u5347\u68c0\u6d4b\u7a33\u5b9a\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u7d27\u6025\u8f66\u8f86\u706f\u5149\u4e0b\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u8bc4\u4f30\u591a\u79cdADAS\u548c\u68c0\u6d4b\u5668\uff0c\u63d0\u51faCaracetamol\u6846\u67b6\u4f18\u5316\u68c0\u6d4b\u6027\u80fd\u3002", "result": "Caracetamol\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u5e76\u964d\u4f4e\u6ce2\u52a8\uff0c\u652f\u6301\u5b9e\u65f6\u5904\u7406\u3002", "conclusion": "Caracetamol\u6709\u6548\u7f13\u89e3\u7d27\u6025\u8f66\u8f86\u706f\u5149\u5bf9\u68c0\u6d4b\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u5b89\u5168\u6027\u3002"}}
{"id": "2505.05189", "pdf": "https://arxiv.org/pdf/2505.05189", "abs": "https://arxiv.org/abs/2505.05189", "authors": ["Wei Peng", "Kang Liu", "Jianchen Hu", "Meng Zhang"], "title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}.", "AI": {"tldr": "Biomed-DPT\u662f\u4e00\u79cd\u77e5\u8bc6\u589e\u5f3a\u7684\u53cc\u6a21\u6001\u63d0\u793a\u8c03\u4f18\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\uff0c\u4f18\u5316\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f7f\u7528\u6587\u672c\u63d0\u793a\uff0c\u5ffd\u7565\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7684\u7279\u6b8a\u7ed3\u6784\uff08\u5982\u590d\u6742\u89e3\u5256\u7ed3\u6784\u548c\u7ec6\u5fae\u75c5\u7406\u7279\u5f81\uff09\u3002", "method": "Biomed-DPT\u8bbe\u8ba1\u6587\u672c\u63d0\u793a\uff08\u4e34\u5e8a\u63d0\u793a\u548c\u9886\u57df\u9002\u5e94\u63d0\u793a\uff09\u548c\u89c6\u89c9\u63d0\u793a\uff08\u96f6\u5411\u91cf\u8f6f\u63d0\u793a\uff09\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u63d0\u53d6\u4e34\u5e8a\u77e5\u8bc6\u3002", "result": "\u572811\u4e2a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u5206\u7c7b\u51c6\u786e\u7387\u4e3a66.14%\uff0c\u4f18\u4e8eCoOp\u65b9\u6cd5\u3002", "conclusion": "Biomed-DPT\u901a\u8fc7\u53cc\u6a21\u6001\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2505.05209", "pdf": "https://arxiv.org/pdf/2505.05209", "abs": "https://arxiv.org/abs/2505.05209", "authors": ["Haizhen Xie", "Kunpeng Du", "Qiangyu Yan", "Sen Lu", "Jianhong Han", "Hanting Chen", "Hailin Hu", "Jie Hu"], "title": "EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind\nSuper-Resolution (BSR) has become a predominant approach in the field. While\nT2I models have traditionally relied on U-Net architectures, recent\nadvancements have demonstrated that Diffusion Transformers (DiT) achieve\nsignificantly higher performance in this domain. In this work, we introduce\nEnhancing Anything Model (EAM), a novel BSR method that leverages DiT and\noutperforms previous U-Net-based approaches. We introduce a novel block,\n$\\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This\nblock employs a low-resolution latent as a separable flow injection control,\nforming a triple-flow architecture that effectively leverages the prior\nknowledge embedded in the pre-trained DiT. To fully exploit the prior guidance\ncapabilities of T2I models and enhance their generalization in BSR, we\nintroduce a progressive Masked Image Modeling strategy, which also reduces\ntraining costs. Additionally, we propose a subject-aware prompt generation\nstrategy that employs a robust multi-modal model in an in-context learning\nframework. This strategy automatically identifies key image areas, provides\ndetailed descriptions, and optimizes the utilization of T2I diffusion priors.\nOur experiments demonstrate that EAM achieves state-of-the-art results across\nmultiple datasets, outperforming existing methods in both quantitative metrics\nand visual quality.", "AI": {"tldr": "EAM\u662f\u4e00\u79cd\u57fa\u4e8eDiT\u7684\u65b0\u578bBSR\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u03a8-DiT\u5757\u548c\u6e10\u8fdb\u5f0f\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u9884\u8bad\u7ec3\u7684T2I\u6269\u6563\u6a21\u578b\u6307\u5bfcBSR\u662f\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46\u4f20\u7edfU-Net\u67b6\u6784\u6027\u80fd\u6709\u9650\uff0cDiT\u8868\u73b0\u66f4\u4f18\u3002", "method": "\u63d0\u51faEAM\u65b9\u6cd5\uff0c\u7ed3\u5408\u03a8-DiT\u5757\u548c\u6e10\u8fdb\u5f0f\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u7b56\u7565\uff0c\u4f18\u5316T2I\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u5229\u7528\u3002", "result": "EAM\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\uff0c\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EAM\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u548c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86BSR\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86DiT\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05212", "pdf": "https://arxiv.org/pdf/2505.05212", "abs": "https://arxiv.org/abs/2505.05212", "authors": ["Xiaotong Yu", "Chang Wen Chen"], "title": "HQC-NBV: A Hybrid Quantum-Classical View Planning Approach", "categories": ["cs.CV"], "comment": null, "summary": "Efficient view planning is a fundamental challenge in computer vision and\nrobotic perception, critical for tasks ranging from search and rescue\noperations to autonomous navigation. While classical approaches, including\nsampling-based and deterministic methods, have shown promise in planning camera\nviewpoints for scene exploration, they often struggle with computational\nscalability and solution optimality in complex settings. This study introduces\nHQC-NBV, a hybrid quantum-classical framework for view planning that leverages\nquantum properties to efficiently explore the parameter space while maintaining\nrobustness and scalability. We propose a specific Hamiltonian formulation with\nmulti-component cost terms and a parameter-centric variational ansatz with\nbidirectional alternating entanglement patterns that capture the hierarchical\ndependencies between viewpoint parameters. Comprehensive experiments\ndemonstrate that quantum-specific components provide measurable performance\nadvantages. Compared to the classical methods, our approach achieves up to\n49.2% higher exploration efficiency across diverse environments. Our analysis\nof entanglement architecture and coherence-preserving terms provides insights\ninto the mechanisms of quantum advantage in robotic exploration tasks. This\nwork represents a significant advancement in integrating quantum computing into\nrobotic perception systems, offering a paradigm-shifting solution for various\nrobot vision tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u6846\u67b6HQC-NBV\uff0c\u7528\u4e8e\u9ad8\u6548\u89c6\u56fe\u89c4\u5212\uff0c\u901a\u8fc7\u91cf\u5b50\u7279\u6027\u63d0\u5347\u63a2\u7d22\u6548\u7387\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u63d0\u9ad849.2%\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89c6\u56fe\u89c4\u5212\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u548c\u89e3\u51b3\u65b9\u6848\u6700\u4f18\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u54c8\u5bc6\u987f\u91cf\u516c\u5f0f\u7684\u591a\u7ec4\u4ef6\u6210\u672c\u9879\u548c\u53c2\u6570\u4e2d\u5fc3\u53d8\u5206ansatz\uff0c\u5229\u7528\u53cc\u5411\u4ea4\u66ff\u7ea0\u7f20\u6a21\u5f0f\u6355\u83b7\u53c2\u6570\u5c42\u6b21\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u91cf\u5b50\u7ec4\u4ef6\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u63a2\u7d22\u6548\u7387\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad849.2%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u91cf\u5b50\u8ba1\u7b97\u878d\u5165\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\uff0c\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u8303\u5f0f\u8f6c\u53d8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05215", "pdf": "https://arxiv.org/pdf/2505.05215", "abs": "https://arxiv.org/abs/2505.05215", "authors": ["Qian Zeng", "Chenggong Hu", "Mingli Song", "Jie Song"], "title": "Diffusion Model Quantization: A Review", "categories": ["cs.CV"], "comment": "40 pages, 8 figures", "summary": "Recent success of large text-to-image models has empirically underscored the\nexceptional performance of diffusion models in generative tasks. To facilitate\ntheir efficient deployment on resource-constrained edge devices, model\nquantization has emerged as a pivotal technique for both compression and\nacceleration. This survey offers a thorough review of the latest advancements\nin diffusion model quantization, encapsulating and analyzing the current state\nof the art in this rapidly advancing domain. First, we provide an overview of\nthe key challenges encountered in the quantization of diffusion models,\nincluding those based on U-Net architectures and Diffusion Transformers (DiT).\nWe then present a comprehensive taxonomy of prevalent quantization techniques,\nengaging in an in-depth discussion of their underlying principles.\nSubsequently, we perform a meticulous analysis of representative diffusion\nmodel quantization schemes from both qualitative and quantitative perspectives.\nFrom a quantitative standpoint, we rigorously benchmark a variety of methods\nusing widely recognized datasets, delivering an extensive evaluation of the\nmost recent and impactful research in the field. From a qualitative standpoint,\nwe categorize and synthesize the effects of quantization errors, elucidating\nthese impacts through both visual analysis and trajectory examination. In\nconclusion, we outline prospective avenues for future research, proposing novel\ndirections for the quantization of generative models in practical applications.\nThe list of related papers, corresponding codes, pre-trained models and\ncomparison results are publicly available at the survey project homepage\nhttps://github.com/TaylorJocelyn/Diffusion-Model-Quantization.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6269\u6563\u6a21\u578b\u91cf\u5316\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e86\u6311\u6218\u3001\u6280\u672f\u5206\u7c7b\u53ca\u4ee3\u8868\u6027\u65b9\u6848\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u6269\u6563\u6a21\u578b\uff0c\u91cf\u5316\u6280\u672f\u6210\u4e3a\u5173\u952e\u3002\u672c\u6587\u65e8\u5728\u603b\u7ed3\u548c\u8bc4\u4f30\u5f53\u524d\u6269\u6563\u6a21\u578b\u91cf\u5316\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u8ba8\u8bba\u91cf\u5316\u6280\u672f\u539f\u7406\uff0c\u5e76\u4ece\u5b9a\u6027\u548c\u5b9a\u91cf\u89d2\u5ea6\u5206\u6790\u4ee3\u8868\u6027\u65b9\u6848\uff0c\u5305\u62ec\u57fa\u4e8eU-Net\u548cDiT\u7684\u6a21\u578b\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u4e86\u591a\u79cd\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5b9a\u6027\u5206\u6790\u4e86\u91cf\u5316\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u751f\u6210\u6a21\u578b\u91cf\u5316\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u8d44\u6e90\u3002"}}
{"id": "2505.05229", "pdf": "https://arxiv.org/pdf/2505.05229", "abs": "https://arxiv.org/abs/2505.05229", "authors": ["Andrea Asperti", "Leonardo Dess\u00ec", "Maria Chiara Tonetti", "Nico Wu"], "title": "Does CLIP perceive art the same way we do?", "categories": ["cs.CV", "cs.MM", "68T45, 68T07 (Primary) 68T50, 68U10 (Secondary)", "I.2.7; I.2.10"], "comment": null, "summary": "CLIP has emerged as a powerful multimodal model capable of connecting images\nand text through joint embeddings, but to what extent does it \"see\" the same\nway humans do - especially when interpreting artworks? In this paper, we\ninvestigate CLIP's ability to extract high-level semantic and stylistic\ninformation from paintings, including both human-created and AI-generated\nimagery. We evaluate its perception across multiple dimensions: content, scene\nunderstanding, artistic style, historical period, and the presence of visual\ndeformations or artifacts. By designing targeted probing tasks and comparing\nCLIP's responses to human annotations and expert benchmarks, we explore its\nalignment with human perceptual and contextual understanding. Our findings\nreveal both strengths and limitations in CLIP's visual representations,\nparticularly in relation to aesthetic cues and artistic intent. We further\ndiscuss the implications of these insights for using CLIP as a guidance\nmechanism during generative processes, such as style transfer or prompt-based\nimage synthesis. Our work highlights the need for deeper interpretability in\nmultimodal systems, especially when applied to creative domains where nuance\nand subjectivity play a central role.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86CLIP\u6a21\u578b\u5728\u7406\u89e3\u7ed8\u753b\uff08\u5305\u62ec\u4eba\u7c7b\u521b\u4f5c\u548cAI\u751f\u6210\u56fe\u50cf\uff09\u65f6\u4e0e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u7684\u5f02\u540c\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u5185\u5bb9\u3001\u98ce\u683c\u3001\u5386\u53f2\u65f6\u671f\u7b49\u591a\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u751f\u6210\u827a\u672f\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22CLIP\u6a21\u578b\u5728\u827a\u672f\u9886\u57df\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5176\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u751f\u6210\u827a\u672f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u9488\u5bf9\u6027\u4efb\u52a1\uff0c\u6bd4\u8f83CLIP\u7684\u54cd\u5e94\u4e0e\u4eba\u7c7b\u6807\u6ce8\u548c\u4e13\u5bb6\u57fa\u51c6\uff0c\u5206\u6790\u5176\u5728\u5185\u5bb9\u3001\u98ce\u683c\u3001\u5386\u53f2\u65f6\u671f\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0CLIP\u5728\u89c6\u89c9\u8868\u5f81\u4e0a\u5b58\u5728\u4f18\u52bf\u4e0e\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u7f8e\u5b66\u7ebf\u7d22\u548c\u827a\u672f\u610f\u56fe\u7684\u7406\u89e3\u4e0a\uff0c\u4e0e\u4eba\u7c7b\u611f\u77e5\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u5728\u521b\u610f\u9886\u57df\u5e94\u7528\u65f6\u9700\u66f4\u6df1\u5165\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u4e3b\u89c2\u6027\u548c\u7ec6\u5fae\u5dee\u5f02\u7684\u827a\u672f\u9886\u57df\u3002"}}
{"id": "2505.05240", "pdf": "https://arxiv.org/pdf/2505.05240", "abs": "https://arxiv.org/abs/2505.05240", "authors": ["Genghua Kou", "Fan Jia", "Weixin Mao", "Yingfei Liu", "Yucheng Zhao", "Ziheng Zhang", "Osamu Yoshie", "Tiancai Wang", "Ying Li", "Xiangyu Zhang"], "title": "PADriver: Towards Personalized Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose PADriver, a novel closed-loop framework for\npersonalized autonomous driving (PAD). Built upon Multi-modal Large Language\nModel (MLLM), PADriver takes streaming frames and personalized textual prompts\nas inputs. It autoaggressively performs scene understanding, danger level\nestimation and action decision. The predicted danger level reflects the risk of\nthe potential action and provides an explicit reference for the final action,\nwhich corresponds to the preset personalized prompt. Moreover, we construct a\nclosed-loop benchmark named PAD-Highway based on Highway-Env simulator to\ncomprehensively evaluate the decision performance under traffic rules. The\ndataset contains 250 hours videos with high-quality annotation to facilitate\nthe development of PAD behavior analysis. Experimental results on the\nconstructed benchmark show that PADriver outperforms state-of-the-art\napproaches on different evaluation metrics, and enables various driving modes.", "AI": {"tldr": "PADriver\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e2a\u6027\u5316\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u8bc4\u4f30\u5728\u4ea4\u901a\u89c4\u5219\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u63d0\u51fa\u4e2a\u6027\u5316\u81ea\u52a8\u9a7e\u9a76\u7684\u9700\u6c42\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u8f93\u5165\u548c\u95ed\u73af\u8bc4\u4f30\u63d0\u5347\u9a7e\u9a76\u51b3\u7b56\u6027\u80fd\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6d41\u5f0f\u5e27\u548c\u4e2a\u6027\u5316\u6587\u672c\u63d0\u793a\uff0c\u8fdb\u884c\u573a\u666f\u7406\u89e3\u3001\u5371\u9669\u7b49\u7ea7\u4f30\u8ba1\u548c\u52a8\u4f5c\u51b3\u7b56\u3002", "result": "\u5728PAD-Highway\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u9a7e\u9a76\u6a21\u5f0f\u3002", "conclusion": "PADriver\u5728\u4e2a\u6027\u5316\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002"}}
{"id": "2505.05288", "pdf": "https://arxiv.org/pdf/2505.05288", "abs": "https://arxiv.org/abs/2505.05288", "authors": ["Ahmed Abdelreheem", "Filippo Aleotti", "Jamie Watson", "Zawar Qureshi", "Abdelrahman Eldesokey", "Peter Wonka", "Gabriel Brostow", "Sara Vicente", "Guillermo Garcia-Hernando"], "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Tech report. Project page: https://nianticlabs.github.io/placeit3d/", "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\uff1a\u8bed\u8a00\u5f15\u5bfc\u76843D\u573a\u666f\u7269\u4f53\u653e\u7f6e\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5173\u57fa\u51c6\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b33D\u573a\u666f\u4e2d\u8bed\u8a00\u5f15\u5bfc\u7269\u4f53\u653e\u7f6e\u7684\u6a21\u7cca\u6027\u548c\u51e0\u4f55\u5173\u7cfb\u63a8\u7406\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65b0\u57fa\u51c6\u3001\u6570\u636e\u96c6\u548c\u9996\u4e2a\u975e\u5e73\u51e1\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u4efb\u52a1\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u521d\u6b65\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u4efb\u52a1\u6709\u671b\u6210\u4e3a\u8bc4\u4f30\u901a\u75283D LLM\u6a21\u578b\u7684\u6807\u51c6\u4e4b\u4e00\u3002"}}
{"id": "2505.05307", "pdf": "https://arxiv.org/pdf/2505.05307", "abs": "https://arxiv.org/abs/2505.05307", "authors": ["Ciyu Ruan", "Ruishan Guo", "Zihang Gong", "Jingao Xu", "Wenhan Yang", "Xinlei Chen"], "title": "PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras excel in high temporal resolution and dynamic range but suffer\nfrom dense noise in rainy conditions. Existing event deraining methods face\ntrade-offs between temporal precision, deraining effectiveness, and\ncomputational efficiency. In this paper, we propose PRE-Mamba, a novel\npoint-based event camera deraining framework that fully exploits the\nspatiotemporal characteristics of raw event and rain. Our framework introduces\na 4D event cloud representation that integrates dual temporal scales to\npreserve high temporal precision, a Spatio-Temporal Decoupling and Fusion\nmodule (STDF) that enhances deraining capability by enabling shallow decoupling\nand interaction of temporal and spatial information, and a Multi-Scale State\nSpace Model (MS3M) that captures deeper rain dynamics across dual-temporal and\nmulti-spatial scales with linear computational complexity. Enhanced by\nfrequency-domain regularization, PRE-Mamba achieves superior performance (0.95\nSR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a\ncomprehensive dataset with labeled synthetic and real-world sequences.\nMoreover, our method generalizes well across varying rain intensities,\nviewpoints, and even snowy conditions.", "AI": {"tldr": "PRE-Mamba\u662f\u4e00\u79cd\u65b0\u578b\u4e8b\u4ef6\u76f8\u673a\u53bb\u96e8\u6846\u67b6\uff0c\u901a\u8fc74D\u4e8b\u4ef6\u4e91\u8868\u793a\u548c\u65f6\u7a7a\u89e3\u8026\u878d\u5408\u6a21\u5757\uff0c\u9ad8\u6548\u53bb\u9664\u96e8\u5929\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u65f6\u95f4\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u96e8\u5929\u6761\u4ef6\u4e0b\u4f1a\u53d7\u5230\u5bc6\u96c6\u566a\u58f0\u5e72\u6270\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u7cbe\u5ea6\u3001\u53bb\u96e8\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u63d0\u51fa4D\u4e8b\u4ef6\u4e91\u8868\u793a\u3001\u65f6\u7a7a\u89e3\u8026\u878d\u5408\u6a21\u5757\uff08STDF\uff09\u548c\u591a\u5c3a\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08MS3M\uff09\uff0c\u7ed3\u5408\u9891\u57df\u6b63\u5219\u5316\u3002", "result": "\u5728EventRain-27K\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff08SR 0.95\uff0cNR 0.91\uff09\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff080.4s/M\u4e8b\u4ef6\uff09\uff0c\u53c2\u6570\u4ec50.26M\u3002", "conclusion": "PRE-Mamba\u5728\u591a\u79cd\u96e8\u5929\u548c\u96ea\u5929\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u53bb\u96e8\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05318", "pdf": "https://arxiv.org/pdf/2505.05318", "abs": "https://arxiv.org/abs/2505.05318", "authors": ["Agnese Chiatti", "Sara Bernardini", "Lara Shibelski Godoy Piccolo", "Viola Schiaffonati", "Matteo Matteucci"], "title": "Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.HC", "cs.RO"], "comment": null, "summary": "The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7528\u6237\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ea4\u4e92\u4e2d\u7684\u4fe1\u4efb\u52a8\u6001\uff0c\u901a\u8fc7\u591a\u5b66\u79d1\u5206\u7c7b\u6cd5\u63a2\u8ba8\u8ba4\u77e5\u79d1\u5b66\u80fd\u529b\u3001\u534f\u4f5c\u6a21\u5f0f\u548c\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765VLM\u4fe1\u4efb\u7814\u7a76\u7684\u521d\u6b65\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5feb\u901f\u666e\u53ca\uff0c\u9700\u8981\u4fdd\u62a4\u7528\u6237\u5e76\u544a\u77e5\u4ed6\u4eec\u4f55\u65f6\u4fe1\u4efb\u8fd9\u4e9b\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u591a\u5b66\u79d1\u5206\u7c7b\u6cd5\u5206\u6790\u7528\u6237-VLM\u4ea4\u4e92\u4e2d\u7684\u4fe1\u4efb\u52a8\u6001\uff0c\u5e76\u7ed3\u5408\u6587\u732e\u548c\u7528\u6237\u7814\u8ba8\u4f1a\u7684\u7ed3\u679c\u3002", "result": "\u63d0\u51fa\u4e86\u672a\u6765VLM\u4fe1\u4efb\u7814\u7a76\u7684\u521d\u6b65\u9700\u6c42\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u7528\u6237\u4e0eVLM\u4e4b\u95f4\u7684\u4fe1\u4efb\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u7528\u6237\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2505.05321", "pdf": "https://arxiv.org/pdf/2505.05321", "abs": "https://arxiv.org/abs/2505.05321", "authors": ["Chintan B. Maniyar", "Minakshi Kumar", "Gengchen Mai"], "title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery", "categories": ["cs.CV", "cs.AI", "I.4.6; I.4.10; I.5.1; I.2.10"], "comment": "in preparation for journal submission, 25 pages, 11 figures", "summary": "Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u5c3a\u5ea6\u5efa\u7b51\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408\u7279\u5f81\u589e\u5f3a\u548c\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86RGB\u5f71\u50cf\u4e2d\u5efa\u7b51\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u5efa\u7b51\u4e0e\u975e\u5efa\u7b51\u7279\u5f81\u7684\u5149\u8c31\u76f8\u4f3c\u6027\u3001\u9634\u5f71\u548c\u4e0d\u89c4\u5219\u51e0\u4f55\u5f62\u72b6\uff0c\u9ad8\u5206\u8fa8\u7387RGB\u5f71\u50cf\u4e2d\u7684\u5efa\u7b51\u5206\u5272\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\uff0c\u901a\u8fc7PCA\u3001VDVI\u3001MBI\u548cSobel\u8fb9\u7f18\u6ee4\u6ce2\u5668\u589e\u5f3a\u8f93\u5165\u7279\u5f81\uff0c\u5e76\u91c7\u7528Res-U-Net\u67b6\u6784\u7ed3\u5408\u5c42\u51bb\u7ed3\u3001\u5faa\u73af\u5b66\u4e60\u7387\u548cSuperConvergence\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u6a21\u578b\u5728WorldView-3\u5f71\u50cf\u4e0a\u5b9e\u73b0\u4e8696.5%\u7684\u603b\u4f53\u51c6\u786e\u7387\u30010.86\u7684F1\u5206\u6570\u548c0.80\u7684IoU\uff0c\u4f18\u4e8e\u73b0\u6709RGB\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u591a\u5206\u8fa8\u7387\u5f71\u50cf\u3001\u7279\u5f81\u589e\u5f3a\u548c\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u9065\u611f\u5e94\u7528\u4e2d\u5efa\u7b51\u5206\u5272\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.05331", "pdf": "https://arxiv.org/pdf/2505.05331", "abs": "https://arxiv.org/abs/2505.05331", "authors": ["C. Alejandro Parraga", "Olivier Penacchio", "Marcos Mu\u0148oz Gonzalez", "Bogdan Raducanu", "Xavier Otazu"], "title": "Aesthetics Without Semantics", "categories": ["cs.CV", "q-bio.NC", "stat.CO"], "comment": "Parts of this work were presented in abstract format at the Vision\n  Science of Art Conference (VSAC2016), the Iberian Conference on Perception\n  (CIP2022), and the European Conference on Visual Perception (ECVP2022). See\n  Perception 51, No1 (Suppl.) pp139, 2022)", "summary": "While it is easy for human observers to judge an image as beautiful or ugly,\naesthetic decisions result from a combination of entangled perceptual and\ncognitive (semantic) factors, making the understanding of aesthetic judgements\nparticularly challenging from a scientific point of view. Furthermore, our\nresearch shows a prevailing bias in current databases, which include mostly\nbeautiful images, further complicating the study and prediction of aesthetic\nresponses. We address these limitations by creating a database of images with\nminimal semantic content and devising, and next exploiting, a method to\ngenerate images on the ugly side of aesthetic valuations. The resulting Minimum\nSemantic Content (MSC) database consists of a large and balanced collection of\n10,426 images, each evaluated by 100 observers. We next use established image\nmetrics to demonstrate how augmenting an image set biased towards beautiful\nimages with ugly images can modify, or even invert, an observed relationship\nbetween image features and aesthetics valuation. Taken together, our study\nreveals that works in empirical aesthetics attempting to link image content and\naesthetic judgements may magnify, underestimate, or simply miss interesting\neffects due to a limitation of the range of aesthetic values they consider.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u521b\u5efa\u6700\u5c0f\u8bed\u4e49\u5185\u5bb9\uff08MSC\uff09\u6570\u636e\u5e93\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7f8e\u5b66\u7814\u7a76\u4e2d\u6570\u636e\u5e93\u504f\u5411\u7f8e\u4e3d\u56fe\u50cf\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u4e11\u964b\u56fe\u50cf\u5982\u4f55\u5f71\u54cd\u7f8e\u5b66\u8bc4\u4ef7\u4e0e\u56fe\u50cf\u7279\u5f81\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7f8e\u5b66\u7814\u7a76\u6570\u636e\u5e93\u591a\u504f\u5411\u7f8e\u4e3d\u56fe\u50cf\uff0c\u5bfc\u81f4\u7f8e\u5b66\u8bc4\u4ef7\u7814\u7a76\u53d7\u9650\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u7f8e\u5b66\u5224\u65ad\u7684\u591a\u6837\u6027\u3002", "method": "\u521b\u5efaMSC\u6570\u636e\u5e93\uff0c\u5305\u542b10,426\u5f20\u56fe\u50cf\uff08\u6bcf\u5f20\u7531100\u540d\u89c2\u5bdf\u8005\u8bc4\u4ef7\uff09\uff0c\u5e76\u5229\u7528\u56fe\u50cf\u6307\u6807\u5206\u6790\u7f8e\u4e3d\u4e0e\u4e11\u964b\u56fe\u50cf\u5bf9\u7f8e\u5b66\u8bc4\u4ef7\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u52a0\u5165\u4e11\u964b\u56fe\u50cf\u53ef\u4ee5\u6539\u53d8\u751a\u81f3\u9006\u8f6c\u56fe\u50cf\u7279\u5f81\u4e0e\u7f8e\u5b66\u8bc4\u4ef7\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7f8e\u5b66\u7814\u7a76\u4e2d\u4ec5\u5173\u6ce8\u7f8e\u4e3d\u56fe\u50cf\u53ef\u80fd\u653e\u5927\u3001\u4f4e\u4f30\u6216\u5ffd\u7565\u91cd\u8981\u6548\u5e94\uff0c\u9700\u6269\u5c55\u7f8e\u5b66\u8bc4\u4ef7\u8303\u56f4\u4ee5\u83b7\u5f97\u66f4\u5168\u9762\u7684\u7406\u89e3\u3002"}}
{"id": "2505.05336", "pdf": "https://arxiv.org/pdf/2505.05336", "abs": "https://arxiv.org/abs/2505.05336", "authors": ["Zunjie Zhu", "Yan Zhao", "Yihan Hu", "Guoxiang Wang", "Hai Qiu", "Bolun Zheng", "Chenggang Yan", "Feng Xu"], "title": "Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors", "categories": ["cs.CV"], "comment": null, "summary": "The motion capture system that supports full-body virtual representation is\nof key significance for virtual reality. Compared to vision-based systems,\nfull-body pose estimation from sparse tracking signals is not limited by\nenvironmental conditions or recording range. However, previous works either\nface the challenge of wearing additional sensors on the pelvis and lower-body\nor rely on external visual sensors to obtain global positions of key joints. To\nimprove the practicality of the technology for virtual reality applications, we\nestimate full-body poses using only inertial data obtained from three Inertial\nMeasurement Unit (IMU) sensors worn on the head and wrists, thereby reducing\nthe complexity of the hardware system. In this work, we propose a method called\nProgressive Inertial Poser (ProgIP) for human pose estimation, which combines\nneural network estimation with a human dynamics model, considers the\nhierarchical structure of the kinematic chain, and employs a multi-stage\nprogressive network estimation with increased depth to reconstruct full-body\nmotion in real time. The encoder combines Transformer Encoder and bidirectional\nLSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial\nsequence, while the decoder based on multi-layer perceptrons (MLPs) transforms\nhigh-dimensional features and accurately projects them onto Skinned\nMulti-Person Linear (SMPL) model parameters. Quantitative and qualitative\nexperimental results on multiple public datasets show that our method\noutperforms state-of-the-art methods with the same inputs, and is comparable to\nrecent works using six IMU sensors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u5934\u6234\u548c\u624b\u8155\u4e09\u4e2aIMU\u4f20\u611f\u5668\u7684\u5168\u8eab\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5ProgIP\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u4eba\u4f53\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u63d0\u9ad8\u865a\u62df\u73b0\u5b9e\u4e2d\u5168\u8eab\u59ff\u6001\u4f30\u8ba1\u7684\u5b9e\u7528\u6027\uff0c\u51cf\u5c11\u786c\u4ef6\u590d\u6742\u6027\uff0c\u907f\u514d\u4f9d\u8d56\u989d\u5916\u4f20\u611f\u5668\u6216\u5916\u90e8\u89c6\u89c9\u8bbe\u5907\u3002", "method": "\u7ed3\u5408Transformer Encoder\u548c\u53cc\u5411LSTM\uff08TE-biLSTM\uff09\u6355\u6349\u60ef\u6027\u5e8f\u5217\u7684\u65f6\u5e8f\u4f9d\u8d56\uff0c\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a\uff08MLPs\uff09\u89e3\u7801\u5e76\u6620\u5c04\u5230SMPL\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u540c\u7c7b\u8f93\u5165\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u4f7f\u7528\u516d\u4e2aIMU\u4f20\u611f\u5668\u7684\u5de5\u4f5c\u3002", "conclusion": "ProgIP\u65b9\u6cd5\u5728\u51cf\u5c11\u4f20\u611f\u5668\u6570\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u5168\u8eab\u8fd0\u52a8\u91cd\u5efa\uff0c\u9002\u7528\u4e8e\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u3002"}}
{"id": "2505.05343", "pdf": "https://arxiv.org/pdf/2505.05343", "abs": "https://arxiv.org/abs/2505.05343", "authors": ["Sooyoung Park", "Arda Senocak", "Joon Son Chung"], "title": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Journal Extension of WACV 2024 paper (arXiv:2311.04066). Code is\n  available at https://github.com/swimmiing/ACL-SSL", "summary": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5c06CLIP\u6a21\u578b\u6269\u5c55\u5e94\u7528\u4e8e\u58f0\u97f3\u6e90\u5b9a\u4f4d\uff0c\u65e0\u9700\u663e\u5f0f\u6587\u672c\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u97f3\u9891\u9a71\u52a8\u7684\u5d4c\u5165\u751f\u6210\u58f0\u97f3\u533a\u57df\u63a9\u7801\uff0c\u5b9e\u73b0\u97f3\u9891-\u89c6\u89c9\u5bf9\u9f50\u3002", "motivation": "\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u5bf9\u9f50\u77e5\u8bc6\uff0c\u63d0\u5347\u58f0\u97f3\u6e90\u5b9a\u4f4d\u7684\u5b8c\u6574\u6027\u548c\u7d27\u51d1\u6027\u3002", "method": "\u63d0\u51fa\u6846\u67b6\u5c06\u97f3\u9891\u6620\u5c04\u4e3a\u4e0eCLIP\u6587\u672c\u7f16\u7801\u5668\u517c\u5bb9\u7684\u6807\u8bb0\uff0c\u751f\u6210\u97f3\u9891\u9a71\u52a8\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u97f3\u9891-\u89c6\u89c9\u5bf9\u5e94\u76ee\u6807\u5bf9\u9f50\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u6269\u5c55\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u97f3\u9891-\u89c6\u89c9\u573a\u666f\u7406\u89e3\uff0c\u9a8c\u8bc1\u4e86\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u5728\u58f0\u97f3\u6e90\u5b9a\u4f4d\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05367", "pdf": "https://arxiv.org/pdf/2505.05367", "abs": "https://arxiv.org/abs/2505.05367", "authors": ["Jie Deng", "Danfeng Hong", "Chenyu Li", "Naoto Yokoya"], "title": "Joint Super-Resolution and Segmentation for 1-m Impervious Surface Area Mapping in China's Yangtze River Economic Belt", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "We propose a novel joint framework by integrating super-resolution and\nsegmentation, called JointSeg, which enables the generation of 1-meter ISA maps\ndirectly from freely available Sentinel-2 imagery. JointSeg was trained on\nmultimodal cross-resolution inputs, offering a scalable and affordable\nalternative to traditional approaches. This synergistic design enables gradual\nresolution enhancement from 10m to 1m while preserving fine-grained spatial\ntextures, and ensures high classification fidelity through effective\ncross-scale feature fusion. This method has been successfully applied to the\nYangtze River Economic Belt (YREB), a region characterized by complex\nurban-rural patterns and diverse topography. As a result, a comprehensive ISA\nmapping product for 2021, referred to as ISA-1, was generated, covering an area\nof over 2.2 million square kilometers. Quantitative comparisons against the 10m\nESA WorldCover and other benchmark products reveal that ISA-1 achieves an\nF1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by\n9.5%, and surpassing other ISA datasets by 21.43%-61.07%. In densely urbanized\nareas (e.g., Suzhou, Nanjing), ISA-1 reduces ISA overestimation through\nimproved discrimination of green spaces and water bodies. Conversely, in\nmountainous regions (e.g., Ganzi, Zhaotong), it identifies significantly more\nISA due to its enhanced ability to detect fragmented anthropogenic features\nsuch as rural roads and sparse settlements, demonstrating its robustness across\ndiverse landscapes. Moreover, we present biennial ISA maps from 2017 to 2023,\ncapturing spatiotemporal urbanization dynamics across representative cities.\nThe results highlight distinct regional growth patterns: rapid expansion in\nupstream cities, moderate growth in midstream regions, and saturation in\ndownstream metropolitan areas.", "AI": {"tldr": "JointSeg\u6846\u67b6\u7ed3\u5408\u8d85\u5206\u8fa8\u7387\u548c\u5206\u5272\u6280\u672f\uff0c\u76f4\u63a5\u4eceSentinel-2\u5f71\u50cf\u751f\u62101\u7c73\u5206\u8fa8\u7387\u7684\u5730\u8868\u4e0d\u900f\u6c34\u9762\uff08ISA\uff09\u5730\u56fe\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u5730\u5f62\u548c\u57ce\u4e61\u6a21\u5f0f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cJointSeg\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u6001\u8de8\u5206\u8fa8\u7387\u8f93\u5165\u8bad\u7ec3\uff0c\u9010\u6b65\u63d0\u5347\u5206\u8fa8\u7387\uff0810m\u52301m\uff09\uff0c\u5e76\u4fdd\u7559\u7a7a\u95f4\u7eb9\u7406\uff0c\u5b9e\u73b0\u8de8\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u957f\u6c5f\u7ecf\u6d4e\u5e26\u751f\u62102021\u5e74ISA-1\u5730\u56fe\uff0cF1-score\u8fbe85.71%\uff0c\u4f18\u4e8e\u57fa\u51c6\u4ea7\u54c19.5%-61.07%\u3002", "conclusion": "JointSeg\u5728\u57ce\u4e61\u548c\u5c71\u533a\u5747\u8868\u73b0\u7a33\u5065\uff0c\u5e76\u6210\u529f\u6355\u63492017-2023\u5e74\u57ce\u5e02\u5316\u52a8\u6001\u3002"}}
{"id": "2505.05375", "pdf": "https://arxiv.org/pdf/2505.05375", "abs": "https://arxiv.org/abs/2505.05375", "authors": ["Kejie Zhao", "Wenjia Hua", "Aiersi Tuerhong", "Luziwei Leng", "Yuxin Ma", "Qinghua Guo"], "title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "Accepted by IJCNN 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing this material for advertising or\n  promotional purposes, collecting new collected works for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u4f4e\u529f\u8017\u5728\u7ebf\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u6846\u67b6\uff08TM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u5143\u9608\u503c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3SNNs\u5728\u90e8\u7f72\u540e\u9002\u5e94\u5206\u5e03\u53d8\u5316\u7684\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5728\u7ebf\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8eSNNs\u3002", "method": "\u63d0\u51fa\u9608\u503c\u8c03\u5236\uff08TM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u52a8\u6001\u542f\u53d1\u7684\u5f52\u4e00\u5316\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u5143\u9608\u503c\uff0c\u517c\u5bb9\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86TM\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86SNNs\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "TM\u65b9\u6cd5\u4e3aSNNs\u7684\u5728\u7ebf\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u795e\u7ecf\u5f62\u6001\u82af\u7247\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u542f\u53d1\u3002"}}
{"id": "2505.05376", "pdf": "https://arxiv.org/pdf/2505.05376", "abs": "https://arxiv.org/abs/2505.05376", "authors": ["Rachmadio Noval Lazuardi", "Artem Sevastopolsky", "Egor Zakharov", "Matthias Niessner", "Vanessa Sklyarova"], "title": "GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans", "categories": ["cs.CV"], "comment": "15 pages, 9 figures, 1 table", "summary": "We propose a novel method that reconstructs hair strands directly from\ncolorless 3D scans by leveraging multi-modal hair orientation extraction. Hair\nstrand reconstruction is a fundamental problem in computer vision and graphics\nthat can be used for high-fidelity digital avatar synthesis, animation, and\nAR/VR applications. However, accurately recovering hair strands from raw scan\ndata remains challenging due to human hair's complex and fine-grained\nstructure. Existing methods typically rely on RGB captures, which can be\nsensitive to the environment and can be a challenging domain for extracting the\norientation of guiding strands, especially in the case of challenging\nhairstyles. To reconstruct the hair purely from the observed geometry, our\nmethod finds sharp surface features directly on the scan and estimates strand\norientation through a neural 2D line detector applied to the renderings of scan\nshading. Additionally, we incorporate a diffusion prior trained on a diverse\nset of synthetic hair scans, refined with an improved noise schedule, and\nadapted to the reconstructed contents via a scan-specific text prompt. We\ndemonstrate that this combination of supervision signals enables accurate\nreconstruction of both simple and intricate hairstyles without relying on color\ninformation. To facilitate further research, we introduce Strands400, the\nlargest publicly available dataset of hair strands with detailed surface\ngeometry extracted from real-world data, which contains reconstructed hair\nstrands from the scans of 400 subjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u65e0\u82723D\u626b\u63cf\u4e2d\u91cd\u5efa\u5934\u53d1\u4e1d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5934\u53d1\u65b9\u5411\u63d0\u53d6\u5b9e\u73b0\uff0c\u65e0\u9700\u4f9d\u8d56\u989c\u8272\u4fe1\u606f\u3002", "motivation": "\u89e3\u51b3\u4ece\u539f\u59cb\u626b\u63cf\u6570\u636e\u4e2d\u51c6\u786e\u6062\u590d\u5934\u53d1\u4e1d\u7684\u6311\u6218\uff0c\u9002\u7528\u4e8e\u6570\u5b57\u5934\u50cf\u5408\u6210\u3001\u52a8\u753b\u548cAR/VR\u5e94\u7528\u3002", "method": "\u5229\u7528\u626b\u63cf\u7684\u9510\u5229\u8868\u9762\u7279\u5f81\u548c\u795e\u7ecf2D\u7ebf\u68c0\u6d4b\u5668\u4f30\u8ba1\u5934\u53d1\u65b9\u5411\uff0c\u7ed3\u5408\u6269\u6563\u5148\u9a8c\u548c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u3002", "result": "\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u7b80\u5355\u548c\u590d\u6742\u53d1\u578b\uff0c\u5e76\u53d1\u5e03\u4e86Strands400\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u989c\u8272\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5934\u53d1\u4e1d\u91cd\u5efa\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2505.05391", "pdf": "https://arxiv.org/pdf/2505.05391", "abs": "https://arxiv.org/abs/2505.05391", "authors": ["Ciyu Ruan", "Zihang Gong", "Ruishan Guo", "Jingao Xu", "Xinlei Chen"], "title": "EDmamba: A Simple yet Effective Event Denoising Method with State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras excel in high-speed vision due to their high temporal\nresolution, high dynamic range, and low power consumption. However, as dynamic\nvision sensors, their output is inherently noisy, making efficient denoising\nessential to preserve their ultra-low latency and real-time processing\ncapabilities. Existing event denoising methods struggle with a critical\ndilemma: computationally intensive approaches compromise the sensor's\nhigh-speed advantage, while lightweight methods often lack robustness across\nvarying noise levels. To address this, we propose a novel event denoising\nframework based on State Space Models (SSMs). Our approach represents events as\n4D event clouds and includes a Coarse Feature Extraction (CFE) module that\nextracts embedding features from both geometric and polarity-aware subspaces.\nThe model is further composed of two essential components: A Spatial Mamba\n(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)\nthat captures global temporal dynamics, efficiently propagating spatiotemporal\nfeatures across events. Experiments demonstrate that our method achieves\nstate-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per\n100K events inference time, and a 0.982 accuracy score, outperforming\nTransformer-based methods by 2.08% in denoising accuracy and 36X faster.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u7684\u65b0\u578b\u4e8b\u4ef6\u53bb\u566a\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4Mamba\u6a21\u5757\u9ad8\u6548\u5904\u7406\u4e8b\u4ef6\u4e91\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7279\u6027\u4f7f\u5176\u5728\u9ad8\u901f\u89c6\u89c9\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8f93\u51fa\u566a\u58f0\u5927\uff0c\u73b0\u6709\u53bb\u566a\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5c06\u4e8b\u4ef6\u8868\u793a\u4e3a4D\u4e8b\u4ef6\u4e91\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u63d0\u53d6\u51e0\u4f55\u548c\u6781\u6027\u611f\u77e5\u7279\u5f81\uff0c\u7ed3\u5408\u7a7a\u95f4Mamba\uff08S-SSM\uff09\u548c\u65f6\u95f4Mamba\uff08T-SSM\uff09\u6a21\u5757\u5efa\u6a21\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u548c\u5168\u5c40\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u6a21\u578b\u53c2\u6570\u4e3a88.89K\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a0.0685s/100K\u4e8b\u4ef6\uff0c\u51c6\u786e\u7387\u4e3a0.982\uff0c\u6bd4\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u51c6\u786e\u7387\u9ad82.08%\uff0c\u901f\u5ea6\u5feb36\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4e8b\u4ef6\u76f8\u673a\u9ad8\u901f\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u53bb\u566a\uff0c\u4e3a\u5b9e\u65f6\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.05397", "pdf": "https://arxiv.org/pdf/2505.05397", "abs": "https://arxiv.org/abs/2505.05397", "authors": ["Zhang Zhang", "Chao Sun", "Chao Yue", "Da Wen", "Tianze Wang", "Jianghao Leng"], "title": "PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything\n(V2X) tasks, roadside perception has received increasing attention in recent\nyears, as it can extend the perception range of connected vehicles and improve\ntraffic safety. However, roadside point cloud oriented 3D object detection has\nnot been effectively explored. To some extent, the key to the performance of a\npoint cloud detector lies in the receptive field of the network and the ability\nto effectively utilize the scene context. The recent emergence of Mamba, based\non State Space Model (SSM), has shaken up the traditional convolution and\ntransformers that have long been the foundational building blocks, due to its\nefficient global receptive field. In this work, we introduce Mamba to\npillar-based roadside point cloud perception and propose a framework based on\nCross-stage State-space Group (CSG), called PillarMamba. It enhances the\nexpressiveness of the network and achieves efficient computation through\ncross-stage feature fusion. However, due to the limitations of scan directions,\nstate space model faces local connection disrupted and historical relationship\nforgotten. To address this, we propose the Hybrid State-space Block (HSB) to\nobtain the local-global context of roadside point cloud. Specifically, it\nenhances neighborhood connections through local convolution and preserves\nhistorical memory through residual attention. The proposed method outperforms\nthe state-of-the-art methods on the popular large scale roadside benchmark:\nDAIR-V2X-I. The code will be released soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPillarMamba\u6846\u67b6\uff0c\u7ed3\u5408Cross-stage State-space Group\uff08CSG\uff09\u548cHybrid State-space Block\uff08HSB\uff09\uff0c\u7528\u4e8e\u8def\u8fb9\u70b9\u4e91\u76843D\u7269\u4f53\u68c0\u6d4b\uff0c\u63d0\u5347\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5728DAIR-V2X-I\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8def\u8fb9\u70b9\u4e91\u76843D\u7269\u4f53\u68c0\u6d4b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u800c\u70b9\u4e91\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u5173\u952e\u5728\u4e8e\u7f51\u7edc\u7684\u611f\u53d7\u91ce\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u5229\u7528\u80fd\u529b\u3002Mamba\uff08\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\u56e0\u5176\u9ad8\u6548\u7684\u5168\u5c40\u611f\u53d7\u91ce\u6210\u4e3a\u65b0\u9009\u62e9\u3002", "method": "\u63d0\u51faPillarMamba\u6846\u67b6\uff0c\u7ed3\u5408CSG\u5b9e\u73b0\u8de8\u9636\u6bb5\u7279\u5f81\u878d\u5408\uff0c\u5e76\u901a\u8fc7HSB\u89e3\u51b3\u5c40\u90e8\u8fde\u63a5\u4e2d\u65ad\u548c\u5386\u53f2\u5173\u7cfb\u9057\u5fd8\u95ee\u9898\uff0c\u589e\u5f3a\u5c40\u90e8-\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5728DAIR-V2X-I\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PillarMamba\u901a\u8fc7\u7ed3\u5408CSG\u548cHSB\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8def\u8fb9\u70b9\u4e91\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.05422", "pdf": "https://arxiv.org/pdf/2505.05422", "abs": "https://arxiv.org/abs/2505.05422", "authors": ["Haokun Lin", "Teng Wang", "Yixiao Ge", "Yuying Ge", "Zhichao Lu", "Ying Wei", "Qingfu Zhang", "Zhenan Sun", "Ying Shan"], "title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Technical Report", "summary": "Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP.", "AI": {"tldr": "TokLIP\u662f\u4e00\u79cd\u89c6\u89c9\u6807\u8bb0\u5668\uff0c\u901a\u8fc7\u8bed\u4e49\u5316\u5411\u91cf\u91cf\u5316\u6807\u8bb0\u5e76\u878d\u5165CLIP\u7ea7\u8bed\u4e49\uff0c\u63d0\u5347\u591a\u6a21\u6001\u7edf\u4e00\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982Chameleon\u548cEmu3\uff09\u5b58\u5728\u8bad\u7ec3\u8ba1\u7b97\u5f00\u9500\u9ad8\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0cTokLIP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "TokLIP\u7ed3\u5408\u4f4e\u5c42\u79bb\u6563VQ\u6807\u8bb0\u5668\u4e0eViT\u7f16\u7801\u5668\uff0c\u5206\u79bb\u7406\u89e3\u548c\u751f\u6210\u76ee\u6807\uff0c\u65e0\u9700\u5b9a\u5236\u91cf\u5316\u64cd\u4f5c\u3002", "result": "TokLIP\u5728\u6570\u636e\u6548\u7387\u548c\u8bed\u4e49\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u589e\u5f3a\u751f\u6210\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u81ea\u56de\u5f52Transformer\u3002", "conclusion": "TokLIP\u4e3a\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05446", "pdf": "https://arxiv.org/pdf/2505.05446", "abs": "https://arxiv.org/abs/2505.05446", "authors": ["Han Xiao", "Yina Xie", "Guanxin Tan", "Yinghao Chen", "Rui Hu", "Ke Wang", "Aojun Zhou", "Hao Li", "Hao Shao", "Xudong Lu", "Peng Gao", "Yafei Wen", "Xiaoxin Chen", "Shuai Ren", "Hongsheng Li"], "title": "Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR2025", "summary": "Visual Document Understanding has become essential with the increase of\ntext-rich visual content. This field poses significant challenges due to the\nneed for effective integration of visual perception and textual comprehension,\nparticularly across diverse document types with complex layouts. Moreover,\nexisting fine-tuning datasets for this domain often fall short in providing the\ndetailed contextual information for robust understanding, leading to\nhallucinations and limited comprehension of spatial relationships among visual\nelements. To address these challenges, we propose an innovative pipeline that\nutilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,\nand TiKZ, to build highly structured document representations and deliver\ncontextually-grounded responses. We introduce two fine-grained structured\ndatasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs\nfor document parsing, and DocMark-Instruct, featuring 624k fine-tuning data\nannotations for grounded instruction following. Extensive experiments\ndemonstrate that our proposed model significantly outperforms existing\nstate-of-theart MLLMs across a range of visual document understanding\nbenchmarks, facilitating advanced reasoning and comprehension capabilities in\ncomplex visual scenarios. Our code and models are released at https://github.\ncom/Euphoria16/DocMark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6807\u8bb0\u8bed\u8a00\u751f\u6210\u7ed3\u6784\u5316\u6587\u6863\u8868\u793a\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6587\u6863\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9\u6587\u6863\u7406\u89e3\u9886\u57df\u56e0\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u590d\u6742\u5e03\u5c40\u7684\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u652f\u6301\u9c81\u68d2\u7406\u89e3\uff0c\u5bfc\u81f4\u5e7b\u89c9\u548c\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u751f\u6210\u6807\u8bb0\u8bed\u8a00\uff08\u5982Markdown\u3001JSON\u7b49\uff09\u6784\u5efa\u7ed3\u6784\u5316\u6587\u6863\u8868\u793a\uff0c\u5e76\u5f15\u5165DocMark-Pile\u548cDocMark-Instruct\u4e24\u4e2a\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u89c6\u89c9\u6587\u6863\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u63a8\u7406\u548c\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u6587\u6863\u7406\u89e3\u7684\u6311\u6218\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2505.05456", "pdf": "https://arxiv.org/pdf/2505.05456", "abs": "https://arxiv.org/abs/2505.05456", "authors": ["Wenqi Wang", "Reuben Tan", "Pengyue Zhu", "Jianwei Yang", "Zhengyuan Yang", "Lijuan Wang", "Andrey Kolobov", "Jianfeng Gao", "Boqing Gong"], "title": "SITE: towards Spatial Intelligence Thorough Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Spatial intelligence (SI) represents a cognitive ability encompassing the\nvisualization, manipulation, and reasoning about spatial relationships,\nunderpinning disciplines from neuroscience to robotics. We introduce SITE, a\nbenchmark dataset towards SI Thorough Evaluation in a standardized format of\nmulti-choice visual question-answering, designed to assess large\nvision-language models' spatial intelligence across diverse visual modalities\n(single-image, multi-image, and video) and SI factors (figural to environmental\nscales, spatial visualization and orientation, intrinsic and extrinsic, static\nand dynamic). Our approach to curating the benchmark combines a bottom-up\nsurvey about 31 existing datasets and a top-down strategy drawing upon three\nclassification systems in cognitive science, which prompt us to design two\nnovel types of tasks about view-taking and dynamic scenes. Extensive\nexperiments reveal that leading models fall behind human experts especially in\nspatial orientation, a fundamental SI factor. Moreover, we demonstrate a\npositive correlation between a model's spatial reasoning proficiency and its\nperformance on an embodied AI task.", "AI": {"tldr": "SITE\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u667a\u80fd\u7684\u6807\u51c6\u5316\u591a\u9009\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u89c6\u89c9\u6a21\u6001\u548c\u7a7a\u95f4\u667a\u80fd\u56e0\u7d20\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u9886\u5148\u6a21\u578b\u5728\u7a7a\u95f4\u5b9a\u5411\u7b49\u57fa\u672c\u56e0\u7d20\u4e0a\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4e14\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0e\u5177\u8eabAI\u4efb\u52a1\u8868\u73b0\u6b63\u76f8\u5173\u3002", "motivation": "\u7a7a\u95f4\u667a\u80fd\uff08SI\uff09\u5728\u591a\u4e2a\u5b66\u79d1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u5168\u9762\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30SI\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u81ea\u4e0b\u800c\u4e0a\u768431\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u8c03\u67e5\u548c\u81ea\u4e0a\u800c\u4e0b\u7684\u8ba4\u77e5\u79d1\u5b66\u5206\u7c7b\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u4e86SITE\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e24\u79cd\u65b0\u578b\u4efb\u52a1\uff08\u89c6\u89d2\u8f6c\u6362\u548c\u52a8\u6001\u573a\u666f\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u9886\u5148\u6a21\u578b\u5728\u7a7a\u95f4\u5b9a\u5411\u7b49\u57fa\u672cSI\u56e0\u7d20\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0e\u5177\u8eabAI\u4efb\u52a1\u8868\u73b0\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "SITE\u4e3a\u8bc4\u4f30\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728SI\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0e\u5177\u8eabAI\u4efb\u52a1\u7684\u76f8\u5173\u6027\u3002"}}
{"id": "2505.05467", "pdf": "https://arxiv.org/pdf/2505.05467", "abs": "https://arxiv.org/abs/2505.05467", "authors": ["Haibo Wang", "Bo Feng", "Zhengfeng Lai", "Mingze Xu", "Shiyu Li", "Weifeng Ge", "Afshin Dehghan", "Meng Cao", "Ping Huang"], "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.", "AI": {"tldr": "StreamBridge\u6846\u67b6\u5c06\u79bb\u7ebfVideo-LLMs\u8f6c\u5316\u4e3a\u6d41\u5f0f\u6a21\u578b\uff0c\u89e3\u51b3\u591a\u8f6e\u5b9e\u65f6\u7406\u89e3\u548c\u4e3b\u52a8\u54cd\u5e94\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8eGPT-4o\u548cGemini 1.5 Pro\u3002", "motivation": "\u9002\u5e94\u5728\u7ebf\u573a\u666f\u65f6\uff0c\u73b0\u6709\u6a21\u578b\u5728\u591a\u8f6e\u5b9e\u65f6\u7406\u89e3\u548c\u4e3b\u52a8\u54cd\u5e94\u673a\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u5185\u5b58\u7f13\u51b2\u4e0e\u8f6e\u8870\u51cf\u538b\u7f29\u7b56\u7565\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u6fc0\u6d3b\u6a21\u578b\u5b9e\u73b0\u4e3b\u52a8\u54cd\u5e94\u3002", "result": "\u663e\u8457\u63d0\u5347\u6d41\u5f0f\u7406\u89e3\u80fd\u529b\uff0c\u5728\u6807\u51c6\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "StreamBridge\u4e3a\u79bb\u7ebfVideo-LLMs\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6d41\u5f0f\u9002\u5e94\u65b9\u6848\u3002"}}
{"id": "2505.05469", "pdf": "https://arxiv.org/pdf/2505.05469", "abs": "https://arxiv.org/abs/2505.05469", "authors": ["Ava Pun", "Kangle Deng", "Ruixuan Liu", "Deva Ramanan", "Changliu Liu", "Jun-Yan Zhu"], "title": "Generating Physically Stable and Buildable LEGO Designs from Text", "categories": ["cs.CV"], "comment": "Project page: https://avalovelace1.github.io/LegoGPT/", "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.", "AI": {"tldr": "LegoGPT\u662f\u9996\u4e2a\u901a\u8fc7\u6587\u672c\u63d0\u793a\u751f\u6210\u7269\u7406\u7a33\u5b9aLEGO\u6a21\u578b\u7684\u65b9\u6848\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u7269\u7406\u7ea6\u675f\u4f18\u5316\u751f\u6210\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u4ece\u6587\u672c\u751f\u6210\u7269\u7406\u7a33\u5b9aLEGO\u6a21\u578b\u7684\u6311\u6218\uff0c\u63a8\u52a8\u521b\u610f\u8bbe\u8ba1\u4e0e\u81ea\u52a8\u5316\u6784\u5efa\u7684\u7ed3\u5408\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u7269\u7406\u7a33\u5b9aLEGO\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u751f\u6210\u7a33\u5b9a\u3001\u591a\u6837\u4e14\u7f8e\u89c2\u7684LEGO\u8bbe\u8ba1\uff0c\u652f\u6301\u624b\u52a8\u548c\u81ea\u52a8\u7ec4\u88c5\uff0c\u5e76\u53d1\u5e03\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "conclusion": "LegoGPT\u4e3a\u6587\u672c\u5230LEGO\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.05470", "pdf": "https://arxiv.org/pdf/2505.05470", "abs": "https://arxiv.org/abs/2505.05470", "authors": ["Jie Liu", "Gongye Liu", "Jiajun Liang", "Yangguang Li", "Jiaheng Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Wanli Ouyang"], "title": "Flow-GRPO: Training Flow Matching Models via Online RL", "categories": ["cs.CV", "cs.AI"], "comment": "Code: https://github.com/yifan123/flow_grpo", "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.", "AI": {"tldr": "Flow-GRPO\u9996\u6b21\u5c06\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u878d\u5165\u6d41\u5339\u914d\u6a21\u578b\uff0c\u901a\u8fc7ODE-to-SDE\u8f6c\u6362\u548c\u964d\u566a\u51cf\u5c11\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u91c7\u6837\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u5c06\u5f3a\u5316\u5b66\u4e60\u5f15\u5165\u6d41\u5339\u914d\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "1. ODE-to-SDE\u8f6c\u6362\uff0c\u652f\u6301RL\u63a2\u7d22\uff1b2. \u964d\u566a\u51cf\u5c11\u7b56\u7565\uff0c\u63d0\u5347\u91c7\u6837\u6548\u7387\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cGenEval\u51c6\u786e\u7387\u4ece63%\u63d0\u5347\u81f395%\uff0c\u6587\u672c\u6e32\u67d3\u51c6\u786e\u7387\u4ece59%\u63d0\u5347\u81f392%\uff0c\u4e14\u672a\u727a\u7272\u56fe\u50cf\u8d28\u91cf\u6216\u591a\u6837\u6027\u3002", "conclusion": "Flow-GRPO\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u5f3a\u5316\u5b66\u4e60\u96c6\u6210\uff0c\u4e14\u907f\u514d\u4e86\u5956\u52b1\u4f5c\u5f0a\u95ee\u9898\u3002"}}
{"id": "2505.05472", "pdf": "https://arxiv.org/pdf/2505.05472", "abs": "https://arxiv.org/abs/2505.05472", "authors": ["Chao Liao", "Liyang Liu", "Xun Wang", "Zhengxiong Luo", "Xinyu Zhang", "Wenliang Zhao", "Jie Wu", "Liang Li", "Zhi Tian", "Weilin Huang"], "title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation", "categories": ["cs.CV"], "comment": "Mogao Technical Report", "summary": "Recent progress in unified models for image understanding and generation has\nbeen impressive, yet most approaches remain limited to single-modal generation\nconditioned on multiple modalities. In this paper, we present Mogao, a unified\nframework that advances this paradigm by enabling interleaved multi-modal\ngeneration through a causal approach. Mogao integrates a set of key technical\nimprovements in architecture design, including a deep-fusion design, dual\nvision encoders, interleaved rotary position embeddings, and multi-modal\nclassifier-free guidance, which allow it to harness the strengths of both\nautoregressive models for text generation and diffusion models for high-quality\nimage synthesis. These practical improvements also make Mogao particularly\neffective to process interleaved sequences of text and images arbitrarily. To\nfurther unlock the potential of unified models, we introduce an efficient\ntraining strategy on a large-scale, in-house dataset specifically curated for\njoint text and image generation. Extensive experiments show that Mogao not only\nachieves state-of-the-art performance in multi-modal understanding and\ntext-to-image generation, but also excels in producing high-quality, coherent\ninterleaved outputs. Its emergent capabilities in zero-shot image editing and\ncompositional generation highlight Mogao as a practical omni-modal foundation\nmodel, paving the way for future development and scaling the unified\nmulti-modal systems.", "AI": {"tldr": "Mogao\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u65b9\u6cd5\u5b9e\u73b0\u4ea4\u9519\u591a\u6a21\u6001\u751f\u6210\uff0c\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5728\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u591a\u9650\u4e8e\u5355\u6a21\u6001\u751f\u6210\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4ea4\u9519\u591a\u6a21\u6001\u5e8f\u5217\u3002Mogao\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "Mogao\u91c7\u7528\u6df1\u5ea6\u878d\u5408\u8bbe\u8ba1\u3001\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u3001\u4ea4\u9519\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548c\u591a\u6a21\u6001\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7b49\u6280\u672f\uff0c\u7ed3\u5408\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\u3002", "result": "Mogao\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4ea4\u9519\u8f93\u51fa\u3002", "conclusion": "Mogao\u5c55\u793a\u4e86\u4f5c\u4e3a\u5168\u80fd\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4e3a\u96f6\u6837\u672c\u56fe\u50cf\u7f16\u8f91\u548c\u7ec4\u5408\u751f\u6210\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05473", "pdf": "https://arxiv.org/pdf/2505.05473", "abs": "https://arxiv.org/abs/2505.05473", "authors": ["Qitao Zhao", "Amy Lin", "Jeff Tan", "Jason Y. Zhang", "Deva Ramanan", "Shubham Tulsiani"], "title": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion", "categories": ["cs.CV"], "comment": "CVPR 2025. Project website: https://qitaozhao.github.io/DiffusionSfM", "summary": "Current Structure-from-Motion (SfM) methods typically follow a two-stage\npipeline, combining learned or geometric pairwise reasoning with a subsequent\nglobal optimization step. In contrast, we propose a data-driven multi-view\nreasoning approach that directly infers 3D scene geometry and camera poses from\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\nand cameras as pixel-wise ray origins and endpoints in a global frame and\nemploys a transformer-based denoising diffusion model to predict them from\nmulti-view inputs. To address practical challenges in training diffusion models\nwith missing data and unbounded scene coordinates, we introduce specialized\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\nboth synthetic and real datasets, demonstrating that it outperforms classical\nand learning-based approaches while naturally modeling uncertainty.", "AI": {"tldr": "DiffusionSfM\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u591a\u89c6\u56fe\u63a8\u7406\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u591a\u89c6\u56fe\u56fe\u50cf\u63a8\u65ad3D\u573a\u666f\u51e0\u4f55\u548c\u76f8\u673a\u59ff\u6001\uff0c\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfSfM\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u7ed3\u5408\u5b66\u4e60\u6216\u51e0\u4f55\u5bf9\u5076\u63a8\u7406\u4e0e\u5168\u5c40\u4f18\u5316\u6b65\u9aa4\uff0c\u800cDiffusionSfM\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u76f4\u63a5\u63a8\u65ad3D\u51e0\u4f55\u548c\u76f8\u673a\u59ff\u6001\uff0c\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "DiffusionSfM\u5c06\u573a\u666f\u51e0\u4f55\u548c\u76f8\u673a\u53c2\u6570\u5316\u4e3a\u5168\u5c40\u5e27\u4e2d\u7684\u50cf\u7d20\u7ea7\u5c04\u7ebf\u8d77\u70b9\u548c\u7ec8\u70b9\uff0c\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u5f15\u5165\u4e13\u95e8\u673a\u5236\u89e3\u51b3\u8bad\u7ec3\u4e2d\u7684\u7f3a\u5931\u6570\u636e\u548c\u65e0\u754c\u573a\u666f\u5750\u6807\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDiffusionSfM\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u80fd\u81ea\u7136\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "DiffusionSfM\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u591a\u89c6\u56fe\u63a8\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u91cd\u5efa\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.05474", "pdf": "https://arxiv.org/pdf/2505.05474", "abs": "https://arxiv.org/abs/2505.05474", "authors": ["Beichen Wen", "Haozhe Xie", "Zhaoxi Chen", "Fangzhou Hong", "Ziwei Liu"], "title": "3D Scene Generation: A Survey", "categories": ["cs.CV"], "comment": "Project Page: https://github.com/hzxie/Awesome-3D-Scene-Generation", "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e863D\u573a\u666f\u751f\u6210\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5c06\u5176\u5206\u4e3a\u56db\u79cd\u8303\u5f0f\uff0c\u5e76\u5206\u6790\u4e86\u6280\u672f\u57fa\u7840\u3001\u4f18\u7f3a\u70b9\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "3D\u573a\u666f\u751f\u6210\u5728\u6c89\u6d78\u5f0f\u5a92\u4f53\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u65e9\u671f\u65b9\u6cd5\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9700\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "\u5c06\u65b9\u6cd5\u5206\u4e3a\u56db\u7c7b\uff1a\u7a0b\u5e8f\u5316\u751f\u6210\u3001\u57fa\u4e8e\u795e\u7ecf3D\u7684\u751f\u6210\u3001\u57fa\u4e8e\u56fe\u50cf\u7684\u751f\u6210\u548c\u57fa\u4e8e\u89c6\u9891\u7684\u751f\u6210\uff0c\u5e76\u5206\u6790\u5176\u6280\u672f\u57fa\u7840\u3002", "result": "\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u6210\u679c\uff0c\u5e76\u8ba8\u8bba\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u4e0b\u6e38\u5e94\u7528\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u66f4\u9ad8\u4fdd\u771f\u5ea6\u3001\u7269\u7406\u611f\u77e5\u751f\u6210\u548c\u7edf\u4e00\u611f\u77e5-\u751f\u6210\u6a21\u578b\uff0c\u540c\u65f6\u7ef4\u62a4\u4e86\u9879\u76ee\u9875\u4ee5\u8ddf\u8e2a\u8fdb\u5c55\u3002"}}
{"id": "2505.05475", "pdf": "https://arxiv.org/pdf/2505.05475", "abs": "https://arxiv.org/abs/2505.05475", "authors": ["Yonwoo Choi"], "title": "SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 SyntaGen Workshop, Project Page:\n  https://yc4ny.github.io/SVAD/", "summary": "Creating high-quality animatable 3D human avatars from a single image remains\na significant challenge in computer vision due to the inherent difficulty of\nreconstructing complete 3D information from a single viewpoint. Current\napproaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods\nproduce high-quality results but require multiple views or video sequences,\nwhile video diffusion models can generate animations from single images but\nstruggle with consistency and identity preservation. We present SVAD, a novel\napproach that addresses these limitations by leveraging complementary strengths\nof existing techniques. Our method generates synthetic training data through\nvideo diffusion, enhances it with identity preservation and image restoration\nmodules, and utilizes this refined data to train 3DGS avatars. Comprehensive\nevaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)\nsingle-image methods in maintaining identity consistency and fine details\nacross novel poses and viewpoints, while enabling real-time rendering\ncapabilities. Through our data augmentation pipeline, we overcome the\ndependency on dense monocular or multi-view training data typically required by\ntraditional 3DGS approaches. Extensive quantitative, qualitative comparisons\nshow our method achieves superior performance across multiple metrics against\nbaseline models. By effectively combining the generative power of diffusion\nmodels with both the high-quality results and rendering efficiency of 3DGS, our\nwork establishes a new approach for high-fidelity avatar generation from a\nsingle image input.", "AI": {"tldr": "SVAD\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u52a8\u753b\u76843D\u4eba\u4f53\u5316\u8eab\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u5355\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa3D\u4fe1\u606f\u7684\u56f0\u96be\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u4e00\u81f4\u6027\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u8bad\u7ec3\u6570\u636e\u4f9d\u8d56\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u89c6\u9891\u6269\u6563\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u7ed3\u5408\u8eab\u4efd\u4fdd\u6301\u548c\u56fe\u50cf\u6062\u590d\u6a21\u5757\u4f18\u5316\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec33DGS\u5316\u8eab\u3002", "result": "SVAD\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u6301\u4e0a\u4f18\u4e8e\u73b0\u6709\u5355\u56fe\u50cf\u65b9\u6cd5\uff0c\u652f\u6301\u5b9e\u65f6\u6e32\u67d3\uff0c\u51cf\u5c11\u5bf9\u5bc6\u96c6\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002", "conclusion": "SVAD\u4e3a\u5355\u56fe\u50cf\u751f\u6210\u9ad8\u4fdd\u771f3D\u5316\u8eab\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u548c3DGS\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.04647", "pdf": "https://arxiv.org/pdf/2505.04647", "abs": "https://arxiv.org/abs/2505.04647", "authors": ["Md Rahat-uz- Zaman", "Bei Wang", "Paul Rosen"], "title": "ChannelExplorer: Exploring Class Separability Through Activation Channel Visualization", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) achieve state-of-the-art performance in many\nvision tasks, yet understanding their internal behavior remains challenging,\nparticularly how different layers and activation channels contribute to class\nseparability. We introduce ChannelExplorer, an interactive visual analytics\ntool for analyzing image-based outputs across model layers, emphasizing\ndata-driven insights over architecture analysis for exploring class\nseparability. ChannelExplorer summarizes activations across layers and\nvisualizes them using three primary coordinated views: a Scatterplot View to\nreveal inter- and intra-class confusion, a Jaccard Similarity View to quantify\nactivation overlap, and a Heatmap View to inspect activation channel patterns.\nOur technique supports diverse model architectures, including CNNs, GANs,\nResNet and Stable Diffusion models. We demonstrate the capabilities of\nChannelExplorer through four use-case scenarios: (1) generating class hierarchy\nin ImageNet, (2) finding mislabeled images, (3) identifying activation channel\ncontributions, and(4) locating latent states' position in Stable Diffusion\nmodel. Finally, we evaluate the tool with expert users.", "AI": {"tldr": "ChannelExplorer\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790DNN\u6a21\u578b\u5404\u5c42\u7684\u6fc0\u6d3b\u901a\u9053\u5bf9\u7c7b\u522b\u53ef\u5206\u6027\u7684\u8d21\u732e\uff0c\u652f\u6301\u591a\u79cd\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u7528\u4f8b\u5c55\u793a\u5176\u529f\u80fd\u3002", "motivation": "\u7406\u89e3DNN\u5185\u90e8\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u4e0d\u540c\u5c42\u548c\u6fc0\u6d3b\u901a\u9053\u5bf9\u7c7b\u522b\u53ef\u5206\u6027\u7684\u8d21\u732e\uff0c\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86ChannelExplorer\u5de5\u5177\uff0c\u5305\u542b\u6563\u70b9\u56fe\u89c6\u56fe\u3001Jaccard\u76f8\u4f3c\u6027\u89c6\u56fe\u548c\u70ed\u56fe\u89c6\u56fe\uff0c\u7528\u4e8e\u5206\u6790\u6fc0\u6d3b\u901a\u9053\u3002", "result": "\u5de5\u5177\u652f\u6301\u591a\u79cd\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u7528\u4f8b\u5c55\u793a\u4e86\u5176\u5728\u751f\u6210\u7c7b\u522b\u5c42\u6b21\u7ed3\u6784\u3001\u53d1\u73b0\u9519\u8bef\u6807\u7b7e\u7b49\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "ChannelExplorer\u4e3a\u5206\u6790DNN\u6a21\u578b\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.04652", "pdf": "https://arxiv.org/pdf/2505.04652", "abs": "https://arxiv.org/abs/2505.04652", "authors": ["Yi Lin", "Dong Zhang", "Xiao Fang", "Yufan Chen", "Kwang-Ting Cheng", "Hao Chen"], "title": "Rethinking Boundary Detection in Deep Learning-Based Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by Medical Image Analysis", "summary": "Medical image segmentation is a pivotal task within the realms of medical\nimage analysis and computer vision. While current methods have shown promise in\naccurately segmenting major regions of interest, the precise segmentation of\nboundary areas remains challenging. In this study, we propose a novel network\narchitecture named CTO, which combines Convolutional Neural Networks (CNNs),\nVision Transformer (ViT) models, and explicit edge detection operators to\ntackle this challenge. CTO surpasses existing methods in terms of segmentation\naccuracy and strikes a better balance between accuracy and efficiency, without\nthe need for additional data inputs or label injections. Specifically, CTO\nadheres to the canonical encoder-decoder network paradigm, with a dual-stream\nencoder network comprising a mainstream CNN stream for capturing local features\nand an auxiliary StitchViT stream for integrating long-range dependencies.\nFurthermore, to enhance the model's ability to learn boundary areas, we\nintroduce a boundary-guided decoder network that employs binary boundary masks\ngenerated by dedicated edge detection operators to provide explicit guidance\nduring the decoding process. We validate the performance of CTO through\nextensive experiments conducted on seven challenging medical image segmentation\ndatasets, namely ISIC 2016, PH2, ISIC 2018, CoNIC, LiTS17, and BTCV. Our\nexperimental results unequivocally demonstrate that CTO achieves\nstate-of-the-art accuracy on these datasets while maintaining competitive model\ncomplexity. The codes have been released at:\nhttps://github.com/xiaofang007/CTO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCTO\u7684\u65b0\u578b\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408CNN\u3001ViT\u548c\u8fb9\u7f18\u68c0\u6d4b\u7b97\u5b50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u8fb9\u754c\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u8fb9\u754c\u533a\u57df\u7684\u7cbe\u786e\u5206\u5272\u4ecd\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u6548\u7387\u3002", "method": "CTO\u91c7\u7528\u53cc\u6d41\u7f16\u7801\u5668\uff08CNN\u548cStitchViT\uff09\u548c\u8fb9\u754c\u5f15\u5bfc\u89e3\u7801\u5668\uff0c\u5229\u7528\u8fb9\u7f18\u68c0\u6d4b\u7b97\u5b50\u751f\u6210\u8fb9\u754c\u63a9\u7801\u6307\u5bfc\u89e3\u7801\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cCTO\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u590d\u6742\u5ea6\u7ade\u4e89\u529b\u3002", "conclusion": "CTO\u4e3a\u533b\u5b66\u56fe\u50cf\u8fb9\u754c\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04653", "pdf": "https://arxiv.org/pdf/2505.04653", "abs": "https://arxiv.org/abs/2505.04653", "authors": ["Khaled Saab", "Jan Freyberg", "Chunjong Park", "Tim Strother", "Yong Cheng", "Wei-Hung Weng", "David G. T. Barrett", "David Stutz", "Nenad Tomasev", "Anil Palepu", "Valentin Li\u00e9vin", "Yash Sharma", "Roma Ruparel", "Abdullah Ahmed", "Elahe Vedadi", "Kimberly Kanada", "Cian Hughes", "Yun Liu", "Geoff Brown", "Yang Gao", "Sean Li", "S. Sara Mahdavi", "James Manyika", "Katherine Chou", "Yossi Matias", "Avinatan Hassidim", "Dale R. Webster", "Pushmeet Kohli", "S. M. Ali Eslami", "Jo\u00eblle Barral", "Adam Rodman", "Vivek Natarajan", "Mike Schaekermann", "Tao Tu", "Alan Karthikesalingam", "Ryutaro Tanno"], "title": "Advancing Conversational Diagnostic AI with Multimodal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated great potential for conducting\ndiagnostic conversations but evaluation has been largely limited to\nlanguage-only interactions, deviating from the real-world requirements of\nremote care delivery. Instant messaging platforms permit clinicians and\npatients to upload and discuss multimodal medical artifacts seamlessly in\nmedical consultation, but the ability of LLMs to reason over such data while\npreserving other attributes of competent diagnostic conversation remains\nunknown. Here we advance the conversational diagnosis and management\nperformance of the Articulate Medical Intelligence Explorer (AMIE) through a\nnew capability to gather and interpret multimodal data, and reason about this\nprecisely during consultations. Leveraging Gemini 2.0 Flash, our system\nimplements a state-aware dialogue framework, where conversation flow is\ndynamically controlled by intermediate model outputs reflecting patient states\nand evolving diagnoses. Follow-up questions are strategically directed by\nuncertainty in such patient states, leading to a more structured multimodal\nhistory-taking process that emulates experienced clinicians. We compared AMIE\nto primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of\nchat-based consultations with patient actors. We constructed 105 evaluation\nscenarios using artifacts like smartphone skin photos, ECGs, and PDFs of\nclinical documents across diverse conditions and demographics. Our rubric\nassessed multimodal capabilities and other clinically meaningful axes like\nhistory-taking, diagnostic accuracy, management reasoning, communication, and\nempathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9\nmultimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The\nresults show clear progress in multimodal conversational diagnostic AI, but\nreal-world translation needs further research.", "AI": {"tldr": "AMIE\uff08Articulate Medical Intelligence Explorer\uff09\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u80fd\u529b\u63d0\u5347\u4e86\u8bca\u65ad\u5bf9\u8bdd\u6027\u80fd\uff0c\u5e76\u5728\u4e0e\u521d\u7ea7\u4fdd\u5065\u533b\u751f\u7684\u6bd4\u8f83\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6a21\u6001\u533b\u7597\u5bf9\u8bdd\u4e2d\u7684\u80fd\u529b\uff0c\u4ee5\u66f4\u8d34\u8fd1\u5b9e\u9645\u8fdc\u7a0b\u533b\u7597\u9700\u6c42\u3002", "method": "\u5229\u7528Gemini 2.0 Flash\u5b9e\u73b0\u72b6\u6001\u611f\u77e5\u5bf9\u8bdd\u6846\u67b6\uff0c\u52a8\u6001\u63a7\u5236\u5bf9\u8bdd\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u76ae\u80a4\u7167\u7247\u3001\u5fc3\u7535\u56fe\u7b49\uff09\u8fdb\u884c\u8bca\u65ad\u3002", "result": "AMIE\u5728\u591a\u6a21\u6001\u548c\u975e\u591a\u6a21\u6001\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u521d\u7ea7\u4fdd\u5065\u533b\u751f\uff0c\u8bca\u65ad\u51c6\u786e\u6027\u66f4\u9ad8\u3002", "conclusion": "\u591a\u6a21\u6001\u5bf9\u8bdd\u8bca\u65adAI\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.04660", "pdf": "https://arxiv.org/pdf/2505.04660", "abs": "https://arxiv.org/abs/2505.04660", "authors": ["Sana Alamgeer", "Yasine Souissi", "Anne H. H. Ngu"], "title": "AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Training fall detection systems is challenging due to the scarcity of\nreal-world fall data, particularly from elderly individuals. To address this,\nwe explore the potential of Large Language Models (LLMs) for generating\nsynthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and\ntext-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall\nscenarios. We generate synthetic datasets and integrate them with four\nreal-world baseline datasets to assess their impact on fall detection\nperformance using a Long Short-Term Memory (LSTM) model. Additionally, we\ncompare LLM-generated synthetic data with a diffusion-based method to evaluate\ntheir alignment with real accelerometer distributions. Results indicate that\ndataset characteristics significantly influence the effectiveness of synthetic\ndata, with LLM-generated data performing best in low-frequency settings (e.g.,\n20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While\ntext-to-motion models produce more realistic biomechanical data than\ntext-to-text models, their impact on fall detection varies. Diffusion-based\nsynthetic data demonstrates the closest alignment to real data but does not\nconsistently enhance model performance. An ablation study further confirms that\nthe effectiveness of synthetic data depends on sensor placement and fall\nrepresentation. These findings provide insights into optimizing synthetic data\ngeneration for fall detection models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u5408\u6210\u8dcc\u5012\u6570\u636e\u4ee5\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u8bc4\u4f30\u4e86\u6587\u672c\u5230\u52a8\u4f5c\u548c\u6587\u672c\u5230\u6587\u672c\u6a21\u578b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u7279\u6027\u548c\u4f20\u611f\u5668\u4f4d\u7f6e\u5bf9\u5408\u6210\u6570\u636e\u6709\u6548\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u771f\u5b9e\u8dcc\u5012\u6570\u636e\uff08\u5c24\u5176\u662f\u8001\u5e74\u4eba\uff09\u7a00\u7f3a\uff0c\u8bad\u7ec3\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u63a2\u7d22LLM\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5230\u52a8\u4f5c\uff08T2M, SATO, ParCo\uff09\u548c\u6587\u672c\u5230\u6587\u672c\uff08GPT4o, GPT4, Gemini\uff09\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u7ed3\u5408\uff0c\u901a\u8fc7LSTM\u6a21\u578b\u8bc4\u4f30\u5176\u5bf9\u8dcc\u5012\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5408\u6210\u6570\u636e\u7684\u6548\u679c\u53d7\u6570\u636e\u96c6\u7279\u6027\u5f71\u54cd\uff0cLLM\u751f\u6210\u6570\u636e\u5728\u4f4e\u9891\uff0820Hz\uff09\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u9ad8\u9891\uff08200Hz\uff09\u4e0d\u7a33\u5b9a\uff1b\u6269\u6563\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u6700\u63a5\u8fd1\uff0c\u4f46\u672a\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u4f20\u611f\u5668\u4f4d\u7f6e\u548c\u8dcc\u5012\u8868\u793a\u65b9\u5f0f\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u4f18\u5316\u8dcc\u5012\u68c0\u6d4b\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2505.04664", "pdf": "https://arxiv.org/pdf/2505.04664", "abs": "https://arxiv.org/abs/2505.04664", "authors": ["Ziyuan Huang", "Kevin Huggins", "Srikar Bellur"], "title": "Advancing 3D Medical Image Segmentation: Unleashing the Potential of Planarian Neural Networks in Artificial Intelligence", "categories": ["eess.IV", "cs.AI", "cs.CV", "68T07"], "comment": "36 pages, 8 figures, 21 tables", "summary": "Our study presents PNN-UNet as a method for constructing deep neural networks\nthat replicate the planarian neural network (PNN) structure in the context of\n3D medical image data. Planarians typically have a cerebral structure\ncomprising two neural cords, where the cerebrum acts as a coordinator, and the\nneural cords serve slightly different purposes within the organism's\nneurological system. Accordingly, PNN-UNet comprises a Deep-UNet and a\nWide-UNet as the nerve cords, with a densely connected autoencoder performing\nthe role of the brain. This distinct architecture offers advantages over both\nmonolithic (UNet) and modular networks (Ensemble-UNet). Our outcomes on a 3D\nMRI hippocampus dataset, with and without data augmentation, demonstrate that\nPNN-UNet outperforms the baseline UNet and several other UNet variants in image\nsegmentation.", "AI": {"tldr": "PNN-UNet\u662f\u4e00\u79cd\u6a21\u4eff\u6da1\u866b\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfUNet\u53ca\u5176\u53d8\u4f53\u3002", "motivation": "\u53d7\u6da1\u866b\u795e\u7ecf\u7cfb\u7edf\u4e2d\u8111\u548c\u795e\u7ecf\u7d22\u7684\u534f\u8c03\u4f5c\u7528\u542f\u53d1\uff0c\u8bbe\u8ba1\u4e00\u79cd\u66f4\u9ad8\u6548\u76843D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7f51\u7edc\u3002", "method": "\u7ed3\u5408Deep-UNet\u548cWide-UNet\u4f5c\u4e3a\u795e\u7ecf\u7d22\uff0c\u4f7f\u7528\u5bc6\u96c6\u8fde\u63a5\u81ea\u7f16\u7801\u5668\u4f5c\u4e3a\u8111\u90e8\u534f\u8c03\u5668\uff0c\u6784\u5efaPNN-UNet\u3002", "result": "\u57283D MRI\u6d77\u9a6c\u4f53\u6570\u636e\u96c6\u4e0a\uff0cPNN-UNet\u5728\u6709\u65e0\u6570\u636e\u589e\u5f3a\u7684\u60c5\u51b5\u4e0b\u5747\u4f18\u4e8e\u57fa\u51c6UNet\u53ca\u5176\u4ed6\u53d8\u4f53\u3002", "conclusion": "PNN-UNet\u901a\u8fc7\u6a21\u4eff\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2505.04813", "pdf": "https://arxiv.org/pdf/2505.04813", "abs": "https://arxiv.org/abs/2505.04813", "authors": ["Richard Liu", "Daniel Fu", "Noah Tan", "Itai Lang", "Rana Hanocka"], "title": "WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://threedle.github.io/wir3d/", "summary": "We present WIR3D, a technique for abstracting 3D shapes through a sparse set\nof visually meaningful curves in 3D. We optimize the parameters of Bezier\ncurves such that they faithfully represent both the geometry and salient visual\nfeatures (e.g. texture) of the shape from arbitrary viewpoints. We leverage the\nintermediate activations of a pre-trained foundation model (CLIP) to guide our\noptimization process. We divide our optimization into two phases: one for\ncapturing the coarse geometry of the shape, and the other for representing\nfine-grained features. Our second phase supervision is spatially guided by a\nnovel localized keypoint loss. This spatial guidance enables user control over\nabstracted features. We ensure fidelity to the original surface through a\nneural SDF loss, which allows the curves to be used as intuitive deformation\nhandles. We successfully apply our method for shape abstraction over a broad\ndataset of shapes with varying complexity, geometric structure, and texture,\nand demonstrate downstream applications for feature control and shape\ndeformation.", "AI": {"tldr": "WIR3D\u662f\u4e00\u79cd\u901a\u8fc7\u7a00\u758f\u7684\u89c6\u89c9\u610f\u4e49\u66f2\u7ebf\u62bd\u8c613D\u5f62\u72b6\u7684\u6280\u672f\uff0c\u5229\u7528Bezier\u66f2\u7ebf\u4f18\u5316\u548cCLIP\u6a21\u578b\u6307\u5bfc\uff0c\u5206\u4e24\u9636\u6bb5\u4f18\u5316\u51e0\u4f55\u548c\u7ec6\u8282\u7279\u5f81\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7a00\u758f\u66f2\u7ebf\u9ad8\u6548\u4e14\u76f4\u89c2\u5730\u8868\u793a3D\u5f62\u72b6\u7684\u51e0\u4f55\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u652f\u6301\u7528\u6237\u63a7\u5236\u548c\u5f62\u72b6\u53d8\u5f62\u3002", "method": "\u5206\u4e24\u9636\u6bb5\u4f18\u5316Bezier\u66f2\u7ebf\u53c2\u6570\uff0c\u5229\u7528CLIP\u6a21\u578b\u6fc0\u6d3b\u548c\u5c40\u90e8\u5173\u952e\u70b9\u635f\u5931\u6307\u5bfc\uff0c\u7ed3\u5408\u795e\u7ecfSDF\u635f\u5931\u786e\u4fdd\u8868\u9762\u4fdd\u771f\u5ea6\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u591a\u79cd\u590d\u6742\u5ea6\u548c\u7eb9\u7406\u76843D\u5f62\u72b6\u62bd\u8c61\uff0c\u652f\u6301\u7279\u5f81\u63a7\u5236\u548c\u5f62\u72b6\u53d8\u5f62\u3002", "conclusion": "WIR3D\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u63a7\u76843D\u5f62\u72b6\u62bd\u8c61\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.04836", "pdf": "https://arxiv.org/pdf/2505.04836", "abs": "https://arxiv.org/abs/2505.04836", "authors": ["Cien Zhang", "Jiaming Zhang", "Jiajun He", "Okan Yurduseven"], "title": "Integrated Image Reconstruction and Target Recognition based on Deep Learning Technique", "categories": ["eess.SP", "cs.CV"], "comment": "Submitted to The 2025 15th IEEE International Conference on Signal\n  Processing, Communications and Computing (ICSPCC 2025)", "summary": "Computational microwave imaging (CMI) has gained attention as an alternative\ntechnique for conventional microwave imaging techniques, addressing their\nlimitations such as hardware-intensive physical layer and slow data collection\nacquisition speed to name a few. Despite these advantages, CMI still encounters\nnotable computational bottlenecks, especially during the image reconstruction\nstage. In this setting, both image recovery and object classification present\nsignificant processing demands. To address these challenges, our previous work\nintroduced ClassiGAN, which is a generative deep learning model designed to\nsimultaneously reconstruct images and classify targets using only\nback-scattered signals. In this study, we build upon that framework by\nincorporating attention gate modules into ClassiGAN. These modules are intended\nto refine feature extraction and improve the identification of relevant\ninformation. By dynamically focusing on important features and suppressing\nirrelevant ones, the attention mechanism enhances the overall model\nperformance. The proposed architecture, named Att-ClassiGAN, significantly\nreduces the reconstruction time compared to traditional CMI approaches.\nFurthermore, it outperforms current advanced methods, delivering improved\nNormalized Mean Squared Error (NMSE), higher Structural Similarity Index\n(SSIM), and better classification outcomes for the reconstructed targets.", "AI": {"tldr": "Att-ClassiGAN\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdbClassiGAN\uff0c\u63d0\u5347\u8ba1\u7b97\u5fae\u6ce2\u6210\u50cf\u7684\u56fe\u50cf\u91cd\u5efa\u548c\u5206\u7c7b\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u91cd\u5efa\u65f6\u95f4\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5fae\u6ce2\u6210\u50cf\u6280\u672f\u5b58\u5728\u786c\u4ef6\u9700\u6c42\u9ad8\u548c\u6570\u636e\u91c7\u96c6\u6162\u7684\u95ee\u9898\uff0c\u8ba1\u7b97\u5fae\u6ce2\u6210\u50cf\uff08CMI\uff09\u867d\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5728\u56fe\u50cf\u91cd\u5efa\u9636\u6bb5\u4ecd\u9762\u4e34\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u5728ClassiGAN\u4e2d\u5f15\u5165\u6ce8\u610f\u529b\u95e8\u6a21\u5757\uff0c\u52a8\u6001\u805a\u7126\u91cd\u8981\u7279\u5f81\u4ee5\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u548c\u4fe1\u606f\u8bc6\u522b\u3002", "result": "Att-ClassiGAN\u663e\u8457\u51cf\u5c11\u91cd\u5efa\u65f6\u95f4\uff0c\u5e76\u5728NMSE\u3001SSIM\u548c\u5206\u7c7b\u7ed3\u679c\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Att-ClassiGAN\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86CMI\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u91cd\u5efa\u548c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2505.04851", "pdf": "https://arxiv.org/pdf/2505.04851", "abs": "https://arxiv.org/abs/2505.04851", "authors": ["Viacheslav Vasilev", "Vladimir Arkhipkin", "Julia Agafonova", "Tatiana Nikulina", "Evelina Mironova", "Alisa Shichanina", "Nikolai Gerasimenko", "Mikhail Shoytov", "Denis Dimitrov"], "title": "CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG"], "comment": "This is arxiv version of the paper which was accepted for the Doklady\n  Mathematics Journal in 2024", "summary": "Despite the fact that popular text-to-image generation models cope well with\ninternational and general cultural queries, they have a significant knowledge\ngap regarding individual cultures. This is due to the content of existing large\ntraining datasets collected on the Internet, which are predominantly based on\nWestern European or American popular culture. Meanwhile, the lack of cultural\nadaptation of the model can lead to incorrect results, a decrease in the\ngeneration quality, and the spread of stereotypes and offensive content. In an\neffort to address this issue, we examine the concept of cultural code and\nrecognize the critical importance of its understanding by modern image\ngeneration models, an issue that has not been sufficiently addressed in the\nresearch community to date. We propose the methodology for collecting and\nprocessing the data necessary to form a dataset based on the cultural code, in\nparticular the Russian one. We explore how the collected data affects the\nquality of generations in the national domain and analyze the effectiveness of\nour approach using the Kandinsky 3.1 text-to-image model. Human evaluation\nresults demonstrate an increase in the level of awareness of Russian culture in\nthe model.", "AI": {"tldr": "\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u901a\u7528\u6587\u5316\u67e5\u8be2\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e2a\u4f53\u6587\u5316\u4e0a\u5b58\u5728\u77e5\u8bc6\u7f3a\u53e3\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u504f\u5411\u6b27\u7f8e\u6587\u5316\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6587\u5316\u4ee3\u7801\u7684\u6570\u636e\u6536\u96c6\u4e0e\u5904\u7406\u65b9\u6cd5\uff0c\u4ee5\u4fc4\u7f57\u65af\u6587\u5316\u4e3a\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u6a21\u578b\u5bf9\u7279\u5b9a\u6587\u5316\u7684\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u4e2a\u4f53\u6587\u5316\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u7ed3\u679c\u6216\u4f20\u64ad\u523b\u677f\u5370\u8c61\u3002\u7814\u7a76\u6587\u5316\u4ee3\u7801\u5bf9\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6587\u5316\u4ee3\u7801\u7684\u6570\u636e\u6536\u96c6\u4e0e\u5904\u7406\u65b9\u6cd5\uff0c\u4ee5\u4fc4\u7f57\u65af\u6587\u5316\u4e3a\u4f8b\uff0c\u4f7f\u7528Kandinsky 3.1\u6a21\u578b\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\uff0c\u6a21\u578b\u5bf9\u4fc4\u7f57\u65af\u6587\u5316\u7684\u7406\u89e3\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6587\u5316\u4ee3\u7801\u7684\u6570\u636e\u5904\u7406\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5bf9\u7279\u5b9a\u6587\u5316\u7684\u751f\u6210\u8d28\u91cf\uff0c\u51cf\u5c11\u523b\u677f\u5370\u8c61\u3002"}}
{"id": "2505.04860", "pdf": "https://arxiv.org/pdf/2505.04860", "abs": "https://arxiv.org/abs/2505.04860", "authors": ["I-Chun Arthur Liu", "Jason Chen", "Gaurav Sukhatme", "Daniel Seita"], "title": "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Learning bimanual manipulation is challenging due to its high dimensionality\nand tight coordination required between two arms. Eye-in-hand imitation\nlearning, which uses wrist-mounted cameras, simplifies perception by focusing\non task-relevant views. However, collecting diverse demonstrations remains\ncostly, motivating the need for scalable data augmentation. While prior work\nhas explored visual augmentation in single-arm settings, extending these\napproaches to bimanual manipulation requires generating viewpoint-consistent\nobservations across both arms and producing corresponding action labels that\nare both valid and feasible. In this work, we propose Diffusion for COordinated\nDual-arm Data Augmentation (D-CODA), a method for offline data augmentation\ntailored to eye-in-hand bimanual imitation learning that trains a diffusion\nmodel to synthesize novel, viewpoint-consistent wrist-camera images for both\narms while simultaneously generating joint-space action labels. It employs\nconstrained optimization to ensure that augmented states involving\ngripper-to-object contacts adhere to constraints suitable for bimanual\ncoordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our\nresults across 2250 simulation trials and 300 real-world trials demonstrate\nthat it outperforms baselines and ablations, showing its potential for scalable\ndata augmentation in eye-in-hand bimanual manipulation. Our project website is\nat: https://dcodaaug.github.io/D-CODA/.", "AI": {"tldr": "D-CODA\u662f\u4e00\u79cd\u9488\u5bf9\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u89d2\u4e00\u81f4\u7684\u56fe\u50cf\u548c\u52a8\u4f5c\u6807\u7b7e\uff0c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u7684\u9ad8\u7ef4\u5ea6\u548c\u534f\u8c03\u9700\u6c42\u4f7f\u5f97\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faD-CODA\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u89d2\u4e00\u81f4\u7684\u53cc\u81c2\u56fe\u50cf\u548c\u52a8\u4f5c\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u786e\u4fdd\u53ef\u884c\u6027\u3002", "result": "\u57285\u4e2a\u6a21\u62df\u548c3\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cD-CODA\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "D-CODA\u4e3a\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.04913", "pdf": "https://arxiv.org/pdf/2505.04913", "abs": "https://arxiv.org/abs/2505.04913", "authors": ["Gugeong Sung"], "title": "Advanced 3D Imaging Approach to TSV/TGV Metrology and Inspection Using Only Optical Microscopy", "categories": ["eess.IV", "cs.CV", "physics.optics"], "comment": "6 pages, 6 figures, Submitted to arXiv for preprint", "summary": "This paper introduces an innovative approach to silicon and glass via\ninspection, which combines hybrid field microscopy with photometric stereo.\nConventional optical microscopy techniques are generally limited to superficial\ninspections and struggle to effectively visualize the internal structures of\nsilicon and glass vias. By utilizing various lighting conditions for 3D\nreconstruction, the proposed method surpasses these limitations. By integrating\nphotometric stereo to the traditional optical microscopy, the proposed method\nnot only enhances the capability to detect micro-scale defects but also\nprovides a detailed visualization of depth and edge abnormality, which are\ntypically not visible with conventional optical microscopy inspection. The\nexperimental results demonstrated that the proposed method effectively captures\nintricate surface details and internal structures. Quantitative comparisons\nbetween the reconstructed models and actual measurements present the capability\nof the proposed method to significantly improve silicon and glass via\ninspection process. As a result, the proposed method achieves enhanced\ncost-effectiveness while maintaining high accuracy and repeatability,\nsuggesting substantial advancements in silicon and glass via inspection\ntechniques", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df7\u5408\u573a\u663e\u5fae\u955c\u548c\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7845\u548c\u73bb\u7483\u901a\u5b54\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u80fd\u529b\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u4f20\u7edf\u5149\u5b66\u663e\u5fae\u955c\u6280\u672f\u4ec5\u80fd\u8fdb\u884c\u8868\u9762\u68c0\u6d4b\uff0c\u96be\u4ee5\u6709\u6548\u53ef\u89c6\u5316\u7845\u548c\u73bb\u7483\u901a\u5b54\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u4e0e\u4f20\u7edf\u5149\u5b66\u663e\u5fae\u955c\uff0c\u5229\u7528\u591a\u79cd\u5149\u7167\u6761\u4ef6\u8fdb\u884c3D\u91cd\u5efa\uff0c\u589e\u5f3a\u4e86\u5bf9\u5fae\u5c3a\u5ea6\u7f3a\u9677\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u8868\u9762\u7ec6\u8282\u548c\u5185\u90e8\u7ed3\u6784\uff0c\u5b9a\u91cf\u6bd4\u8f83\u663e\u793a\u5176\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u8fc7\u7a0b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u6210\u672c\u6548\u76ca\uff0c\u4e3a\u7845\u548c\u73bb\u7483\u901a\u5b54\u68c0\u6d4b\u6280\u672f\u5e26\u6765\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2505.04959", "pdf": "https://arxiv.org/pdf/2505.04959", "abs": "https://arxiv.org/abs/2505.04959", "authors": ["Tengya Peng", "Ruyi Zha", "Qing Zou"], "title": "MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing pulmonary MRI based on 3D Gaussian representation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This study presents an unsupervised, motion-resolved reconstruction framework\nfor high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI),\nutilizing a three-dimensional Gaussian representation (3DGS). The proposed\nmethod leverages 3DGS to address the challenges of motion-resolved 3D isotropic\npulmonary MRI reconstruction by enabling data smoothing between voxels for\ncontinuous spatial representation. Pulmonary MRI data acquisition is performed\nusing a golden-angle radial sampling trajectory, with respiratory motion\nsignals extracted from the center of k-space in each radial spoke. Based on the\nestimated motion signal, the k-space data is sorted into multiple respiratory\nphases. A 3DGS framework is then applied to reconstruct a reference image\nvolume from the first motion state. Subsequently, a patient-specific\nconvolutional neural network is trained to estimate the deformation vector\nfields (DVFs), which are used to generate the remaining motion states through\nspatial transformation of the reference volume. The proposed reconstruction\npipeline is evaluated on six datasets from six subjects and bench-marked\nagainst three state-of-the-art reconstruction methods. The experimental\nfindings demonstrate that the proposed reconstruction framework effectively\nreconstructs high-resolution, motion-resolved pulmonary MR images. Compared\nwith existing approaches, it achieves superior image quality, reflected by\nhigher signal-to-noise ratio and contrast-to-noise ratio. The proposed\nunsupervised 3DGS-based reconstruction method enables accurate motion-resolved\npulmonary MRI with isotropic spatial resolution. Its superior performance in\nimage quality metrics over state-of-the-art methods highlights its potential as\na robust solution for clinical pulmonary MR imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u8868\u793a\u7684\u65e0\u76d1\u7763\u8fd0\u52a8\u89e3\u6790\u91cd\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u81ea\u7531\u547c\u5438\u80ba\u90e8MRI\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u81ea\u7531\u547c\u5438\u80ba\u90e8MRI\u4e2d\u8fd0\u52a8\u89e3\u6790\u548c3D\u5404\u5411\u540c\u6027\u91cd\u5efa\u7684\u6311\u6218\u3002", "method": "\u4f7f\u75283D\u9ad8\u65af\u8868\u793a\u548c\u5f84\u5411\u91c7\u6837\u8f68\u8ff9\uff0c\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u53d8\u5f62\u5411\u91cf\u573a\uff0c\u5b9e\u73b0\u8fd0\u52a8\u89e3\u6790\u91cd\u5efa\u3002", "result": "\u5728\u516d\u7ec4\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u56fe\u50cf\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4fe1\u566a\u6bd4\u548c\u5bf9\u6bd4\u566a\u58f0\u6bd4\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u80ba\u90e8MRI\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.04961", "pdf": "https://arxiv.org/pdf/2505.04961", "abs": "https://arxiv.org/abs/2505.04961", "authors": ["Ziyu Zhang", "Sergey Bashkirov", "Dun Yang", "Michael Taylor", "Xue Bin Peng"], "title": "ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.RO"], "comment": "19 pages, 15 figures", "summary": "Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u591a\u76ee\u6807\u4f18\u5316\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u8fd0\u52a8\u8ddf\u8e2a\u9886\u57df\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u5956\u52b1\u51fd\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u8c03\u6574\u7684\u805a\u5408\u51fd\u6570\uff0c\u8017\u65f6\u4e14\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u5dee\u5206\u5224\u522b\u5668\uff0c\u4ec5\u9700\u5355\u4e2a\u6b63\u6837\u672c\u5373\u53ef\u6709\u6548\u6307\u5bfc\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u80fd\u591f\u5b9e\u73b0\u591a\u79cd\u9ad8\u96be\u5ea6\u52a8\u4f5c\u7684\u7cbe\u786e\u8ddf\u8e2a\uff0c\u6548\u679c\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5e7f\u6cdb\u9002\u7528\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u8fd0\u52a8\u8ddf\u8e2a\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u5956\u52b1\u51fd\u6570\u3002"}}
{"id": "2505.04969", "pdf": "https://arxiv.org/pdf/2505.04969", "abs": "https://arxiv.org/abs/2505.04969", "authors": ["Gekko Budiutama", "Shunsuke Daimon", "Hirofumi Nishi", "Yu-ichiro Matsushita"], "title": "General Transform: A Unified Framework for Adaptive Transform to Enhance Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Discrete transforms, such as the discrete Fourier transform, are widely used\nin machine learning to improve model performance by extracting meaningful\nfeatures. However, with numerous transforms available, selecting an appropriate\none often depends on understanding the dataset's properties, making the\napproach less effective when such knowledge is unavailable. In this work, we\npropose General Transform (GT), an adaptive transform-based representation\ndesigned for machine learning applications. Unlike conventional transforms, GT\nlearns data-driven mapping tailored to the dataset and task of interest. Here,\nwe demonstrate that models incorporating GT outperform conventional\ntransform-based approaches across computer vision and natural language\nprocessing tasks, highlighting its effectiveness in diverse learning scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u53d8\u6362\u65b9\u6cd5\uff08GT\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u7684\u6620\u5c04\uff0c\u4f18\u4e8e\u4f20\u7edf\u53d8\u6362\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563\u53d8\u6362\u4f9d\u8d56\u5bf9\u6570\u636e\u96c6\u7279\u6027\u7684\u4e86\u89e3\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faGT\u65b9\u6cd5\uff0c\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u7684\u6620\u5c04\u4ee5\u9002\u5e94\u4efb\u52a1\u548c\u6570\u636e\u96c6\u3002", "result": "GT\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u53d8\u6362\u65b9\u6cd5\u3002", "conclusion": "GT\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u9002\u5e94\u53d8\u6362\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2505.04972", "pdf": "https://arxiv.org/pdf/2505.04972", "abs": "https://arxiv.org/abs/2505.04972", "authors": ["Mattia Sartori", "Chetna Singhal", "Neelabhro Roy", "Davide Brunelli", "James Gross"], "title": "AI and Vision based Autonomous Navigation of Nano-Drones in Partially-Known Environments", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.NI"], "comment": "in DCOSS-IoT 2025, Wi-DroIT 2025", "summary": "The miniaturisation of sensors and processors, the advancements in connected\nedge intelligence, and the exponential interest in Artificial Intelligence are\nboosting the affirmation of autonomous nano-size drones in the Internet of\nRobotic Things ecosystem. However, achieving safe autonomous navigation and\nhigh-level tasks such as exploration and surveillance with these tiny platforms\nis extremely challenging due to their limited resources. This work focuses on\nenabling the safe and autonomous flight of a pocket-size, 30-gram platform\ncalled Crazyflie 2.1 in a partially known environment. We propose a novel\nAI-aided, vision-based reactive planning method for obstacle avoidance under\nthe ambit of Integrated Sensing, Computing and Communication paradigm. We deal\nwith the constraints of the nano-drone by splitting the navigation task into\ntwo parts: a deep learning-based object detector runs on the edge (external\nhardware) while the planning algorithm is executed onboard. The results show\nthe ability to command the drone at $\\sim8$ frames-per-second and a model\nperformance reaching a COCO mean-average-precision of $60.8$. Field experiments\ndemonstrate the feasibility of the solution with the drone flying at a top\nspeed of $1$ m/s while steering away from an obstacle placed in an unknown\nposition and reaching the target destination. The outcome highlights the\ncompatibility of the communication delay and the model performance with the\nrequirements of the real-time navigation task. We provide a feasible\nalternative to a fully onboard implementation that can be extended to\nautonomous exploration with nano-drones.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u89c6\u89c9\u53cd\u5e94\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b330\u514b\u5fae\u578b\u65e0\u4eba\u673a\u5728\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\u7684\u5b89\u5168\u81ea\u4e3b\u98de\u884c\u95ee\u9898\uff0c\u901a\u8fc7\u8fb9\u7f18\u8ba1\u7b97\u548c\u673a\u8f7d\u7b97\u6cd5\u7ed3\u5408\u5b9e\u73b0\u907f\u969c\u3002", "motivation": "\u5fae\u578b\u65e0\u4eba\u673a\u8d44\u6e90\u6709\u9650\uff0c\u5b9e\u73b0\u5b89\u5168\u81ea\u4e3b\u5bfc\u822a\u548c\u9ad8\u9636\u4efb\u52a1\uff08\u5982\u63a2\u7d22\u548c\u76d1\u89c6\uff09\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u5c06\u5bfc\u822a\u4efb\u52a1\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u8fb9\u7f18\u8bbe\u5907\u8fd0\u884c\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\uff0c\u673a\u8f7d\u6267\u884c\u89c4\u5212\u7b97\u6cd5\u3002", "result": "\u65e0\u4eba\u673a\u80fd\u4ee5\u6bcf\u79d28\u5e27\u7684\u901f\u5ea6\u8fd0\u884c\uff0c\u6a21\u578b\u6027\u80fd\u8fbe\u523060.8 COCO mAP\uff0c\u73b0\u573a\u5b9e\u9a8c\u663e\u793a\u5176\u80fd\u4ee51 m/s\u901f\u5ea6\u907f\u969c\u5e76\u5230\u8fbe\u76ee\u6807\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5fae\u578b\u65e0\u4eba\u673a\u7684\u5b9e\u65f6\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\u3002"}}
{"id": "2505.04996", "pdf": "https://arxiv.org/pdf/2505.04996", "abs": "https://arxiv.org/abs/2505.04996", "authors": ["Jinhe Huang", "Yongkang Cheng", "Yuming Hang", "Gaoge Han", "Jinewei Li", "Jing Zhang", "Xingjian Gu"], "title": "Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": "accepted by ICMR 2025", "summary": "Full-body gestures play a pivotal role in natural interactions and are\ncrucial for achieving effective communication. Nevertheless, most existing\nstudies primarily focus on the gesture generation of speakers, overlooking the\nvital role of listeners in the interaction process and failing to fully explore\nthe dynamic interaction between them. This paper innovatively proposes an\nInter-Diffusion Generation Model of Speakers and Listeners for Effective\nCommunication. For the first time, we integrate the full-body gestures of\nlisteners into the generation framework. By devising a novel inter-diffusion\nmechanism, this model can accurately capture the complex interaction patterns\nbetween speakers and listeners during communication. In the model construction\nprocess, based on the advanced diffusion model architecture, we innovatively\nintroduce interaction conditions and the GAN model to increase the denoising\nstep size. As a result, when generating gesture sequences, the model can not\nonly dynamically generate based on the speaker's speech information but also\nrespond in realtime to the listener's feedback, enabling synergistic\ninteraction between the two. Abundant experimental results demonstrate that\ncompared with the current state-of-the-art gesture generation methods, the\nmodel we proposed has achieved remarkable improvements in the naturalness,\ncoherence, and speech-gesture synchronization of the generated gestures. In the\nsubjective evaluation experiments, users highly praised the generated\ninteraction scenarios, believing that they are closer to real life human\ncommunication situations. Objective index evaluations also show that our model\noutperforms the baseline methods in multiple key indicators, providing more\npowerful support for effective communication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u4ea4\u4e92\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u9996\u6b21\u5c06\u542c\u4f17\u7684\u5168\u8eab\u624b\u52bf\u7eb3\u5165\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u578b\u4ea4\u4e92\u6269\u6563\u673a\u5236\u6355\u6349\u8bf4\u8bdd\u8005\u548c\u542c\u4f17\u95f4\u7684\u590d\u6742\u4e92\u52a8\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8bf4\u8bdd\u8005\u7684\u624b\u52bf\u751f\u6210\uff0c\u5ffd\u89c6\u4e86\u542c\u4f17\u5728\u4e92\u52a8\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u53ca\u5176\u52a8\u6001\u4ea4\u4e92\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u67b6\u6784\uff0c\u5f15\u5165\u4ea4\u4e92\u6761\u4ef6\u548cGAN\u6a21\u578b\u4ee5\u589e\u52a0\u53bb\u566a\u6b65\u957f\uff0c\u5b9e\u73b0\u57fa\u4e8e\u8bf4\u8bdd\u8005\u4fe1\u606f\u548c\u542c\u4f17\u53cd\u9988\u7684\u52a8\u6001\u624b\u52bf\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u624b\u52bf\u81ea\u7136\u6027\u3001\u8fde\u8d2f\u6027\u548c\u8bed\u97f3-\u624b\u52bf\u540c\u6b65\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u6237\u8bc4\u4ef7\u63a5\u8fd1\u771f\u5b9e\u4e92\u52a8\u573a\u666f\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u6709\u6548\u6c9f\u901a\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u652f\u6301\uff0c\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.05040", "pdf": "https://arxiv.org/pdf/2505.05040", "abs": "https://arxiv.org/abs/2505.05040", "authors": ["Mat\u012bss Rikters", "Edison Marrese-Taylor"], "title": "Image-Text Relation Prediction for Multilingual Tweets", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Various social networks have been allowing media uploads for over a decade\nnow. Still, it has not always been clear what is their relation with the posted\ntext or even if there is any at all. In this work, we explore how multilingual\nvision-language models tackle the task of image-text relation prediction in\ndifferent languages, and construct a dedicated balanced benchmark data set from\nTwitter posts in Latvian along with their manual translations into English. We\ncompare our results to previous work and show that the more recently released\nvision-language model checkpoints are becoming increasingly capable at this\ntask, but there is still much room for further improvement.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u9884\u6d4b\u56fe\u50cf-\u6587\u672c\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5e73\u8861\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u5173\u7cfb\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u63a2\u7d22\u591a\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u62c9\u8131\u7ef4\u4e9a\u8bed\u548c\u82f1\u8bed\u7684Twitter\u5e16\u5b50\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u6d4b\u8bd5\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u6700\u65b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5173\u7cfb\u9884\u6d4b\u4e0a\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2505.05041", "pdf": "https://arxiv.org/pdf/2505.05041", "abs": "https://arxiv.org/abs/2505.05041", "authors": ["Chenxi Zhao", "Jianqiang Li", "Qing Zhao", "Jing Bai", "Susana Boluda", "Benoit Delatour", "Lev Stimmer", "Daniel Racoceanu", "Gabriel Jimenez", "Guanghui Fu"], "title": "ADNP-15: An Open-Source Histopathological Dataset for Neuritic Plaque Segmentation in Human Brain Whole Slide Images with Frequency Domain Image Enhancement for Stain Normalization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by\namyloid-beta plaques and tau neurofibrillary tangles, which serve as key\nhistopathological features. The identification and segmentation of these\nlesions are crucial for understanding AD progression but remain challenging due\nto the lack of large-scale annotated datasets and the impact of staining\nvariations on automated image analysis. Deep learning has emerged as a powerful\ntool for pathology image segmentation; however, model performance is\nsignificantly influenced by variations in staining characteristics,\nnecessitating effective stain normalization and enhancement techniques. In this\nstudy, we address these challenges by introducing an open-source dataset\n(ADNP-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of\ndystrophic tau-positive neurites) in human brain whole slide images. We\nestablish a comprehensive benchmark by evaluating five widely adopted deep\nlearning models across four stain normalization techniques, providing deeper\ninsights into their influence on neuritic plaque segmentation. Additionally, we\npropose a novel image enhancement method that improves segmentation accuracy,\nparticularly in complex tissue structures, by enhancing structural details and\nmitigating staining inconsistencies. Our experimental results demonstrate that\nthis enhancement strategy significantly boosts model generalization and\nsegmentation accuracy. All datasets and code are open-source, ensuring\ntransparency and reproducibility while enabling further advancements in the\nfield.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u96c6ADNP-15\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u75c5\u7406\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u67d3\u8272\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u75c5\u7406\u7279\u5f81\uff08\u5982\u6dc0\u7c89\u6837\u6591\u5757\u548ctau\u86cb\u767d\u7f20\u7ed3\uff09\u7684\u5206\u5272\u5bf9\u7406\u89e3\u75be\u75c5\u8fdb\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u67d3\u8272\u5dee\u5f02\u548c\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86ADNP-15\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u56db\u79cd\u67d3\u8272\u5f52\u4e00\u5316\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u6240\u6709\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5747\u5df2\u5f00\u6e90\uff0c\u4e3a\u9886\u57df\u5185\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2505.05054", "pdf": "https://arxiv.org/pdf/2505.05054", "abs": "https://arxiv.org/abs/2505.05054", "authors": ["Navya Sonal Agarwal", "Jan Philipp Schneider", "Kanchana Vaishnavi Gandikota", "Syed Muhammad Kazim", "John Meshreki", "Ivo Ihrke", "Michael Moeller"], "title": "Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "ISCS 2025", "summary": "The computational imaging technique of Fourier Ptychographic Microscopy (FPM)\nenables high-resolution imaging with a wide field of view and can serve as an\nextremely valuable tool, e.g. in the classification of cells in medical\napplications. However, reconstructing a high-resolution image from tens or even\nhundreds of measurements is computationally expensive, particularly for a wide\nfield of view. Therefore, in this paper, we investigate the idea of classifying\nthe image content in the FPM measurements directly without performing a\nreconstruction step first. We show that Convolutional Neural Networks (CNN) can\nextract meaningful information from measurement sequences, significantly\noutperforming the classification on a single band-limited image (up to 12 %)\nwhile being significantly more efficient than a reconstruction of a\nhigh-resolution image. Furthermore, we demonstrate that a learned multiplexing\nof several raw measurements allows maintaining the classification accuracy\nwhile reducing the amount of data (and consequently also the acquisition time)\nsignificantly.", "AI": {"tldr": "FPM\u6280\u672f\u7ed3\u5408CNN\u76f4\u63a5\u5206\u7c7b\u6d4b\u91cf\u6570\u636e\uff0c\u907f\u514d\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u63d0\u5347\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "FPM\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u63a2\u7d22\u76f4\u63a5\u5206\u7c7b\u6d4b\u91cf\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528CNN\u76f4\u63a5\u4eceFPM\u6d4b\u91cf\u5e8f\u5217\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u907f\u514d\u91cd\u5efa\u6b65\u9aa4\u3002", "result": "CNN\u5206\u7c7b\u6027\u80fd\u4f18\u4e8e\u5355\u5e45\u56fe\u50cf\uff08\u63d0\u534712%\uff09\uff0c\u4e14\u66f4\u9ad8\u6548\uff1b\u6570\u636e\u590d\u7528\u51cf\u5c11\u91c7\u96c6\u65f6\u95f4\u3002", "conclusion": "\u76f4\u63a5\u5206\u7c7bFPM\u6d4b\u91cf\u6570\u636e\u53ef\u884c\u4e14\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u7ec6\u80de\u5206\u7c7b\u7b49\u5e94\u7528\u3002"}}
{"id": "2505.05073", "pdf": "https://arxiv.org/pdf/2505.05073", "abs": "https://arxiv.org/abs/2505.05073", "authors": ["Shengchun Xiong", "Xiangru Li", "Yunpeng Zhong", "Wanfen Peng"], "title": "RepSNet: A Nucleus Instance Segmentation model based on Boundary Regression and Structural Re-parameterization", "categories": ["eess.IV", "cs.CV"], "comment": "25 pages, 7 figures, 5 tables", "summary": "Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus\ninstance segmentation is a key step in digital pathology analysis and\npathological diagnosis. However, the computational efficiency of the model and\nthe treatment of overlapping targets are the major challenges in the studies of\nthis problem. To this end, a neural network model RepSNet was designed based on\na nucleus boundary regression and a structural re-parameterization scheme for\nsegmenting and classifying the nuclei in H\\&E-stained histopathological images.\nFirst, RepSNet estimates the boundary position information (BPI) of the parent\nnucleus for each pixel. The BPI estimation incorporates the local information\nof the pixel and the contextual information of the parent nucleus. Then, the\nnucleus boundary is estimated by aggregating the BPIs from a series of pixels\nusing a proposed boundary voting mechanism (BVM), and the instance segmentation\nresults are computed from the estimated nucleus boundary using a connected\ncomponent analysis procedure. The BVM intrinsically achieves a kind of\nsynergistic belief enhancement among the BPIs from various pixels. Therefore,\ndifferent from the methods available in literature that obtain nucleus\nboundaries based on a direct pixel recognition scheme, RepSNet computes its\nboundary decisions based on some guidances from macroscopic information using\nan integration mechanism. In addition, RepSNet employs a re-parametrizable\nencoder-decoder structure. This model can not only aggregate features from some\nreceptive fields with various scales which helps segmentation accuracy\nimprovement, but also reduce the parameter amount and computational burdens in\nthe model inference phase through the structural re-parameterization technique.\nExtensive experiments demonstrated the superiorities of RepSNet compared to\nseveral typical benchmark models.", "AI": {"tldr": "RepSNet\u662f\u4e00\u79cd\u57fa\u4e8e\u6838\u8fb9\u754c\u56de\u5f52\u548c\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8eH&E\u67d3\u8272\u7ec4\u7ec7\u75c5\u7406\u56fe\u50cf\u4e2d\u7684\u6838\u5206\u5272\u548c\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u53e0\u76ee\u6807\u5904\u7406\u7684\u6311\u6218\u3002", "motivation": "\u75c5\u7406\u8bca\u65ad\u662f\u80bf\u7624\u8bca\u65ad\u7684\u91d1\u6807\u51c6\uff0c\u6838\u5b9e\u4f8b\u5206\u5272\u662f\u6570\u5b57\u75c5\u7406\u5206\u6790\u548c\u75c5\u7406\u8bca\u65ad\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u53e0\u76ee\u6807\u5904\u7406\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "RepSNet\u901a\u8fc7\u6838\u8fb9\u754c\u4f4d\u7f6e\u4fe1\u606f\uff08BPI\uff09\u4f30\u8ba1\u548c\u8fb9\u754c\u6295\u7968\u673a\u5236\uff08BVM\uff09\u5b9e\u73b0\u6838\u5206\u5272\uff0c\u91c7\u7528\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u6280\u672f\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRepSNet\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u5178\u578b\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "RepSNet\u901a\u8fc7\u5b8f\u89c2\u4fe1\u606f\u6574\u5408\u548c\u7ed3\u6784\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6838\u5206\u5272\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.05076", "pdf": "https://arxiv.org/pdf/2505.05076", "abs": "https://arxiv.org/abs/2505.05076", "authors": ["Hyunho Song", "Dongjae Lee", "Seunghun Oh", "Minwoo Jung", "Ayoung Kim"], "title": "The City that Never Settles: Simulation-based LiDAR Dataset for Long-Term Place Recognition Under Extreme Structural Changes", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Large-scale construction and demolition significantly challenge long-term\nplace recognition (PR) by drastically reshaping urban and suburban\nenvironments. Existing datasets predominantly reflect limited or indoor-focused\nchanges, failing to adequately represent extensive outdoor transformations. To\nbridge this gap, we introduce the City that Never Settles (CNS) dataset, a\nsimulation-based dataset created using the CARLA simulator, capturing major\nstructural changes-such as building construction and demolition-across diverse\nmaps and sequences. Additionally, we propose TCR_sym, a symmetric version of\nthe original TCR metric, enabling consistent measurement of structural changes\nirrespective of source-target ordering. Quantitative comparisons demonstrate\nthat CNS encompasses more extensive transformations than current real-world\nbenchmarks. Evaluations of state-of-the-art LiDAR-based PR methods on CNS\nreveal substantial performance degradation, underscoring the need for robust\nalgorithms capable of handling significant environmental changes. Our dataset\nis available at https://github.com/Hyunho111/CNS_dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86City that Never Settles (CNS)\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6a21\u62df\u5927\u89c4\u6a21\u57ce\u5e02\u7ed3\u6784\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u79f0\u7248\u672c\u7684TCR_sym\u5ea6\u91cf\u6807\u51c6\uff0c\u4ee5\u8bc4\u4f30\u73b0\u6709LiDAR\u5b9a\u4f4d\u65b9\u6cd5\u5728\u73af\u5883\u5267\u70c8\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u672a\u80fd\u5145\u5206\u53cd\u6620\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u53d8\u5316\uff0c\u5c24\u5176\u662f\u5efa\u7b51\u62c6\u9664\u4e0e\u5efa\u8bbe\u5bf9\u957f\u671f\u5730\u70b9\u8bc6\u522b\uff08PR\uff09\u7684\u6311\u6218\u3002", "method": "\u5229\u7528CARLA\u6a21\u62df\u5668\u521b\u5efaCNS\u6570\u636e\u96c6\uff0c\u6355\u6349\u591a\u6837\u5316\u7684\u7ed3\u6784\u53d8\u5316\uff0c\u5e76\u63d0\u51faTCR_sym\u5ea6\u91cf\u6807\u51c6\u3002", "result": "CNS\u6570\u636e\u96c6\u6bd4\u73b0\u6709\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6db5\u76d6\u66f4\u5e7f\u6cdb\u7684\u53d8\u5316\uff0c\u73b0\u6709LiDAR PR\u65b9\u6cd5\u5728CNS\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "CNS\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u7a7a\u767d\uff0c\u7a81\u663e\u4e86\u5f00\u53d1\u9c81\u68d2\u7b97\u6cd5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2505.05088", "pdf": "https://arxiv.org/pdf/2505.05088", "abs": "https://arxiv.org/abs/2505.05088", "authors": ["Wenyang Liu", "Jianjun Gao", "Kim-Hui Yap"], "title": "SSH-Net: A Self-Supervised and Hybrid Network for Noisy Image Watermark Removal", "categories": ["cs.MM", "cs.CV", "eess.IV"], "comment": "Under Review in JVCI", "summary": "Visible watermark removal is challenging due to its inherent complexities and\nthe noise carried within images. Existing methods primarily rely on supervised\nlearning approaches that require paired datasets of watermarked and\nwatermark-free images, which are often impractical to obtain in real-world\nscenarios. To address this challenge, we propose SSH-Net, a Self-Supervised and\nHybrid Network specifically designed for noisy image watermark removal. SSH-Net\nsynthesizes reference watermark-free images using the watermark distribution in\na self-supervised manner and adopts a dual-network design to address the task.\nThe upper network, focused on the simpler task of noise removal, employs a\nlightweight CNN-based architecture, while the lower network, designed to handle\nthe more complex task of simultaneously removing watermarks and noise,\nincorporates Transformer blocks to model long-range dependencies and capture\nintricate image features. To enhance the model's effectiveness, a shared\nCNN-based feature encoder is introduced before dual networks to extract common\nfeatures that both networks can leverage. Our code will be available at\nhttps://github.com/wenyang001/SSH-Net.", "AI": {"tldr": "SSH-Net\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6df7\u5408\u7f51\u7edc\uff0c\u7528\u4e8e\u53bb\u9664\u56fe\u50cf\u4e2d\u7684\u53ef\u89c1\u6c34\u5370\u548c\u566a\u58f0\uff0c\u65e0\u9700\u6210\u5bf9\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6210\u5bf9\u6570\u636e\u96c6\uff0c\u5b9e\u9645\u4e2d\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u63d0\u51fa\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cc\u7f51\u7edc\u8bbe\u8ba1\uff1a\u4e0a\u5c42CNN\u53bb\u566a\uff0c\u4e0b\u5c42Transformer\u53bb\u6c34\u5370\u548c\u566a\u58f0\uff0c\u5171\u4eab\u7279\u5f81\u7f16\u7801\u5668\u3002", "result": "\u6709\u6548\u53bb\u9664\u6c34\u5370\u548c\u566a\u58f0\uff0c\u65e0\u9700\u6210\u5bf9\u6570\u636e\u3002", "conclusion": "SSH-Net\u4e3a\u81ea\u76d1\u7763\u6c34\u5370\u53bb\u9664\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05098", "pdf": "https://arxiv.org/pdf/2505.05098", "abs": "https://arxiv.org/abs/2505.05098", "authors": ["Wei Liu", "Jiyuan Zhang", "Binxiong Zheng", "Yufeng Hu", "Yingzhan Lin", "Zengfeng Zeng"], "title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.ET"], "comment": null, "summary": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving.", "AI": {"tldr": "X-Driver\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u548c\u81ea\u56de\u5f52\u5efa\u6a21\u63d0\u5347\u6027\u80fd\uff0c\u5728\u95ed\u73af\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u5728\u95ed\u73af\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faX-Driver\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u94fe\u5f0f\u601d\u7ef4\u548c\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u4f18\u5316\u611f\u77e5\u4e0e\u51b3\u7b56\u3002", "result": "\u5728CARLA\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0cX-Driver\u5728\u95ed\u73af\u6027\u80fd\u4e0a\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "X-Driver\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u95ed\u73af\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2505.05112", "pdf": "https://arxiv.org/pdf/2505.05112", "abs": "https://arxiv.org/abs/2505.05112", "authors": ["Xiaolong Niu", "Zanting Ye", "Xu Han", "Yanchao Huang", "Hao Sun", "Hubing Wu", "Lijun Lu"], "title": "MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for PET Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Acquiring high-quality Positron Emission Tomography (PET) images requires\nadministering high-dose radiotracers, which increases radiation exposure risks.\nGenerating standard-dose PET (SPET) from low-dose PET (LPET) has become a\npotential solution. However, previous studies have primarily focused on single\nlow-dose PET denoising, neglecting two critical factors: discrepancies in dose\nresponse caused by inter-patient variability, and complementary anatomical\nconstraints derived from CT images. In this work, we propose a novel CT-Guided\nMulti-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for\nmulti-dose PET denoising. Our approach integrates anatomical guidance and\ndose-level adaptation to achieve superior denoising performance under low-dose\nconditions. Specifically, this approach incorporates a CT-Guided High-frequency\nWavelet Attention (HWA) module, which uses wavelet transforms to separate\nhigh-frequency anatomical boundary features from CT images. These extracted\nfeatures are then incorporated into PET imaging through an adaptive weighted\nfusion mechanism to enhance edge details. Additionally, we propose the\nDose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism\nthat dynamically integrates dose levels into channel-spatial attention weight\ncalculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets\ndemonstrate that MDAA-Diff outperforms state-of-the-art approaches in\npreserving diagnostic quality under reduced-dose conditions. Our code is\npublicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdCT\u5f15\u5bfc\u7684\u591a\u5242\u91cf\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u53bb\u566a\u6269\u6563\u6a21\u578b\uff08MDAA-Diff\uff09\uff0c\u7528\u4e8e\u591a\u5242\u91cfPET\u53bb\u566a\uff0c\u7ed3\u5408\u89e3\u5256\u5b66\u6307\u5bfc\u548c\u5242\u91cf\u6c34\u5e73\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5242\u91cf\u6761\u4ef6\u4e0b\u7684\u53bb\u566a\u6027\u80fd\u3002", "motivation": "\u9ad8\u5242\u91cf\u653e\u5c04\u6027\u793a\u8e2a\u5242\u4f1a\u589e\u52a0\u8f90\u5c04\u98ce\u9669\uff0c\u800c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u5242\u91cf\u53bb\u566a\uff0c\u5ffd\u7565\u4e86\u60a3\u8005\u95f4\u5242\u91cf\u54cd\u5e94\u5dee\u5f02\u548cCT\u56fe\u50cf\u7684\u89e3\u5256\u5b66\u7ea6\u675f\u3002", "method": "\u63d0\u51faCT\u5f15\u5bfc\u7684\u9ad8\u9891\u5c0f\u6ce2\u6ce8\u610f\u529b\u6a21\u5757\uff08HWA\uff09\u548c\u5242\u91cf\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\uff08DAA\uff09\uff0c\u5206\u522b\u5229\u7528CT\u56fe\u50cf\u89e3\u5256\u7279\u5f81\u548c\u52a8\u6001\u6574\u5408\u5242\u91cf\u6c34\u5e73\u3002", "result": "\u572818F-FDG\u548c68Ga-FAPI\u6570\u636e\u96c6\u4e0a\uff0cMDAA-Diff\u5728\u4f4e\u5242\u91cf\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4fdd\u6301\u4e86\u8bca\u65ad\u8d28\u91cf\u3002", "conclusion": "MDAA-Diff\u901a\u8fc7\u89e3\u5256\u5b66\u6307\u5bfc\u548c\u5242\u91cf\u9002\u5e94\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5242\u91cfPET\u56fe\u50cf\u7684\u53bb\u566a\u6548\u679c\u3002"}}
{"id": "2505.05132", "pdf": "https://arxiv.org/pdf/2505.05132", "abs": "https://arxiv.org/abs/2505.05132", "authors": ["Luis Alvarez", "Jean-Michel Morel"], "title": "An Active Contour Model for Silhouette Vectorization using B\u00e9zier Curves", "categories": ["cs.GR", "cs.CV", "math.FA"], "comment": "14 pages, 5 figures and 1 table", "summary": "In this paper, we propose an active contour model for silhouette\nvectorization using cubic B\\'ezier curves. Among the end points of the B\\'ezier\ncurves, we distinguish between corner and regular points where the orientation\nof the tangent vector is prescribed. By minimizing the distance of the B\\'ezier\ncurves to the silhouette boundary, the active contour model optimizes the\nlocation of the B\\'ezier curves end points, the orientation of the tangent\nvectors in the regular points, and the estimation of the B\\'ezier curve\nparameters. This active contour model can use the silhouette vectorization\nobtained by any method as an initial guess. The proposed method significantly\nreduces the average distance between the silhouette boundary and its\nvectorization obtained by the world-class graphic software Inkscape, Adobe\nIllustrator, and a curvature-based vectorization method, which we introduce for\ncomparison. Our method also allows us to impose additional regularity on the\nB\\'ezier curves by reducing their lengths.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u6b21\u8d1d\u585e\u5c14\u66f2\u7ebf\u7684\u4e3b\u52a8\u8f6e\u5ed3\u6a21\u578b\uff0c\u7528\u4e8e\u8f6e\u5ed3\u77e2\u91cf\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8f6e\u5ed3\u77e2\u91cf\u5316\u65b9\u6cd5\uff08\u5982Inkscape\u3001Adobe Illustrator\u7b49\uff09\u5728\u7cbe\u5ea6\u548c\u89c4\u5219\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u8d1d\u585e\u5c14\u66f2\u7ebf\u4e0e\u8f6e\u5ed3\u8fb9\u754c\u7684\u8ddd\u79bb\uff0c\u4f18\u5316\u7aef\u70b9\u4f4d\u7f6e\u3001\u5207\u7ebf\u65b9\u5411\u53ca\u66f2\u7ebf\u53c2\u6570\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u8f6e\u5ed3\u8fb9\u754c\u4e0e\u77e2\u91cf\u5316\u7ed3\u679c\u7684\u5e73\u5747\u8ddd\u79bb\uff0c\u4f18\u4e8e\u4e16\u754c\u7ea7\u8f6f\u4ef6\u548c\u66f2\u7387\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u89c4\u5219\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u53ef\u5e94\u7528\u4e8e\u8f6e\u5ed3\u77e2\u91cf\u5316\u9886\u57df\u3002"}}
{"id": "2505.05137", "pdf": "https://arxiv.org/pdf/2505.05137", "abs": "https://arxiv.org/abs/2505.05137", "authors": ["Yi Chen"], "title": "Research on Anomaly Detection Methods Based on Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "6 pages, 3 table", "summary": "Anomaly detection is a fundamental task in machine learning and data mining,\nwith significant applications in cybersecurity, industrial fault diagnosis, and\nclinical disease monitoring. Traditional methods, such as statistical modeling\nand machine learning-based approaches, often face challenges in handling\ncomplex, high-dimensional data distributions. In this study, we explore the\npotential of diffusion models for anomaly detection, proposing a novel\nframework that leverages the strengths of diffusion probabilistic models (DPMs)\nto effectively identify anomalies in both image and audio data. The proposed\nmethod models the distribution of normal data through a diffusion process and\nreconstructs input data via reverse diffusion, using a combination of\nreconstruction errors and semantic discrepancies as anomaly indicators. To\nenhance the framework's performance, we introduce multi-scale feature\nextraction, attention mechanisms, and wavelet-domain representations, enabling\nthe model to capture fine-grained structures and global dependencies in the\ndata. Extensive experiments on benchmark datasets, including MVTec AD and\nUrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly\ndetection techniques, achieving superior accuracy and robustness across diverse\ndata modalities. This research highlights the effectiveness of diffusion models\nin anomaly detection and provides a robust and efficient solution for\nreal-world applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DPMs\uff09\u7684\u65b0\u578b\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u56fe\u50cf\u548c\u97f3\u9891\u6570\u636e\u4e2d\u9ad8\u6548\u8bc6\u522b\u5f02\u5e38\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u9ad8\u7ef4\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u5206\u5e03\u5efa\u6a21\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "method": "\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u6b63\u5e38\u6570\u636e\u5206\u5e03\uff0c\u901a\u8fc7\u53cd\u5411\u6269\u6563\u91cd\u6784\u8f93\u5165\u6570\u636e\uff0c\u7ed3\u5408\u91cd\u5efa\u8bef\u5dee\u548c\u8bed\u4e49\u5dee\u5f02\u4f5c\u4e3a\u5f02\u5e38\u6307\u6807\u3002", "result": "\u5728MVTec AD\u548cUrbanSound8K\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6548\u679c\u663e\u8457\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05195", "pdf": "https://arxiv.org/pdf/2505.05195", "abs": "https://arxiv.org/abs/2505.05195", "authors": ["Xinyue Xu", "Yueying Hu", "Hui Tang", "Yi Qin", "Lu Mi", "Hao Wang", "Xiaomeng Li"], "title": "Concept-Based Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86CUDA\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u548c\u677e\u5f1b\u9608\u503c\u63d0\u5347\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u5728\u57df\u9002\u5e94\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u76ee\u6807\u57df\u6807\u8bb0\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u76f8\u540c\uff0c\u4f46\u5728\u57df\u504f\u79fb\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "CUDA\u6846\u67b6\u5305\u62ec\uff1a\u5bf9\u6297\u8bad\u7ec3\u5bf9\u9f50\u6982\u5ff5\u8868\u793a\u3001\u677e\u5f1b\u9608\u503c\u5141\u8bb8\u57df\u95f4\u5dee\u5f02\u3001\u65e0\u76d1\u7763\u6982\u5ff5\u63a8\u65ad\u3001\u7406\u8bba\u4fdd\u8bc1\u7684\u6982\u5ff5\u5b66\u4e60\u4e0e\u57df\u9002\u5e94\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCUDA\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CUDA\u63d0\u5347\u4e86\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u57df\u9002\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.05208", "pdf": "https://arxiv.org/pdf/2505.05208", "abs": "https://arxiv.org/abs/2505.05208", "authors": ["Muhammad Irfan", "Anum Nawaz", "Riku Klen", "Abdulhamit Subasi", "Tomi Westerlund", "Wei Chen"], "title": "Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE IJCNN 2025 has accepted the paper", "summary": "Early detection and accurate diagnosis are essential to improving patient\noutcomes. The use of convolutional neural networks (CNNs) for tumor detection\nhas shown promise, but existing models often suffer from overparameterization,\nwhich limits their performance gains. In this study, fuzzy sigmoid convolution\n(FSC) is introduced along with two additional modules: top-of-the-funnel and\nmiddle-of-the-funnel. The proposed methodology significantly reduces the number\nof trainable parameters without compromising classification accuracy. A novel\nconvolutional operator is central to this approach, effectively dilating the\nreceptive field while preserving input data integrity. This enables efficient\nfeature map reduction and enhances the model's tumor detection capability. In\nthe FSC-based model, fuzzy sigmoid activation functions are incorporated within\nconvolutional layers to improve feature extraction and classification. The\ninclusion of fuzzy logic into the architecture improves its adaptability and\nrobustness. Extensive experiments on three benchmark datasets demonstrate the\nsuperior performance and efficiency of the proposed model. The FSC-based\narchitecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%\non three different datasets. The model employs 100 times fewer parameters than\nlarge-scale transfer learning architectures, highlighting its computational\nefficiency and suitability for detecting brain tumors early. This research\noffers lightweight, high-performance deep-learning models for medical imaging\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u7ccaSigmoid\u5377\u79ef\uff08FSC\uff09\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u65e9\u671f\u8111\u80bf\u7624\u68c0\u6d4b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u5e76\u4fdd\u6301\u4e86\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u548c\u51c6\u786e\u8bca\u65ad\u5bf9\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709CNN\u6a21\u578b\u5b58\u5728\u8fc7\u53c2\u6570\u5316\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u5f15\u5165\u4e86\u6a21\u7ccaSigmoid\u5377\u79ef\uff08FSC\uff09\u53ca\u4e24\u4e2a\u9644\u52a0\u6a21\u5757\uff08\u6f0f\u6597\u9876\u90e8\u548c\u6f0f\u6597\u4e2d\u90e8\uff09\uff0c\u901a\u8fc7\u65b0\u578b\u5377\u79ef\u64cd\u4f5c\u6269\u5c55\u611f\u53d7\u91ce\u5e76\u4fdd\u6301\u6570\u636e\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFSC\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523099.17%\u300199.75%\u548c99.89%\uff0c\u53c2\u6570\u6570\u91cf\u6bd4\u5927\u89c4\u6a21\u8fc1\u79fb\u5b66\u4e60\u67b6\u6784\u5c11100\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u9ad8\u6027\u80fd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u6548\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u65e9\u671f\u80bf\u7624\u68c0\u6d4b\u6f5c\u529b\u3002"}}
{"id": "2505.05223", "pdf": "https://arxiv.org/pdf/2505.05223", "abs": "https://arxiv.org/abs/2505.05223", "authors": ["Hendrik Surmann", "Jorge de Heuvel", "Maren Bennewitz"], "title": "Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Human drivers exhibit individual preferences regarding driving style.\nAdapting autonomous vehicles to these preferences is essential for user trust\nand satisfaction. However, existing end-to-end driving approaches often rely on\npredefined driving styles or require continuous user feedback for adaptation,\nlimiting their ability to support dynamic, context-dependent preferences. We\npropose a novel approach using multi-objective reinforcement learning (MORL)\nwith preference-driven optimization for end-to-end autonomous driving that\nenables runtime adaptation to driving style preferences. Preferences are\nencoded as continuous weight vectors to modulate behavior along interpretable\nstyle objectives$\\unicode{x2013}$including efficiency, comfort, speed, and\naggressiveness$\\unicode{x2013}$without requiring policy retraining. Our\nsingle-policy agent integrates vision-based perception in complex mixed-traffic\nscenarios and is evaluated in diverse urban environments using the CARLA\nsimulator. Experimental results demonstrate that the agent dynamically adapts\nits driving behavior according to changing preferences while maintaining\nperformance in terms of collision avoidance and route completion.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u9a71\u52a8\u4f18\u5316\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u7684\u52a8\u6001\u98ce\u683c\u9002\u5e94\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u96be\u4ee5\u52a8\u6001\u9002\u5e94\u7528\u6237\u7684\u9a7e\u9a76\u98ce\u683c\u504f\u597d\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u4efb\u3002", "method": "\u4f7f\u7528MORL\u548c\u8fde\u7eed\u6743\u91cd\u5411\u91cf\u7f16\u7801\u504f\u597d\uff0c\u8c03\u8282\u6548\u7387\u3001\u8212\u9002\u5ea6\u3001\u901f\u5ea6\u548c\u6fc0\u8fdb\u6027\u7b49\u76ee\u6807\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\uff0c\u4ee3\u7406\u80fd\u52a8\u6001\u9002\u5e94\u504f\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u907f\u649e\u548c\u8def\u7ebf\u5b8c\u6210\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u9a7e\u9a76\u98ce\u683c\u504f\u597d\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u9002\u5e94\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2505.05248", "pdf": "https://arxiv.org/pdf/2505.05248", "abs": "https://arxiv.org/abs/2505.05248", "authors": ["Jose Angel Nu\u00f1ez", "Fabian Vazquez", "Diego Adame", "Xiaoyan Fu", "Pengfei Gu", "Bin Fu"], "title": "White Light Specular Reflection Data Augmentation for Deep Learning Polyp Detection", "categories": ["eess.IV", "cs.CV"], "comment": "5 pages, 4 Figures, paper accepted by the ISBI (International\n  Symposium on Biomedical Imaging) 2025 Conference", "summary": "Colorectal cancer is one of the deadliest cancers today, but it can be\nprevented through early detection of malignant polyps in the colon, primarily\nvia colonoscopies. While this method has saved many lives, human error remains\na significant challenge, as missing a polyp could have fatal consequences for\nthe patient. Deep learning (DL) polyp detectors offer a promising solution.\nHowever, existing DL polyp detectors often mistake white light reflections from\nthe endoscope for polyps, which can lead to false positives.To address this\nchallenge, in this paper, we propose a novel data augmentation approach that\nartificially adds more white light reflections to create harder training\nscenarios. Specifically, we first generate a bank of artificial lights using\nthe training dataset. Then we find the regions of the training images that we\nshould not add these artificial lights on. Finally, we propose a sliding window\nmethod to add the artificial light to the areas that fit of the training\nimages, resulting in augmented images. By providing the model with more\nopportunities to make mistakes, we hypothesize that it will also have more\nchances to learn from those mistakes, ultimately improving its performance in\npolyp detection. Experimental results demonstrate the effectiveness of our new\ndata augmentation method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u56fe\u50cf\u4e2d\u4eba\u5de5\u6dfb\u52a0\u767d\u5149\u53cd\u5c04\uff0c\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u606f\u8089\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u7ed3\u80a0\u955c\u68c0\u67e5\u4e2d\uff0c\u4eba\u7c7b\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u606f\u8089\u6f0f\u68c0\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u5668\u6613\u5c06\u5185\u7aa5\u955c\u7684\u767d\u5149\u53cd\u5c04\u8bef\u8ba4\u4e3a\u606f\u8089\uff0c\u5bfc\u81f4\u5047\u9633\u6027\u3002", "method": "\u751f\u6210\u4eba\u5de5\u767d\u5149\u53cd\u5c04\u5e93\uff0c\u786e\u5b9a\u4e0d\u5e94\u6dfb\u52a0\u53cd\u5c04\u7684\u533a\u57df\uff0c\u5e76\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u5c06\u53cd\u5c04\u6dfb\u52a0\u5230\u5408\u9002\u533a\u57df\uff0c\u751f\u6210\u589e\u5f3a\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u606f\u8089\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u589e\u52a0\u6a21\u578b\u72af\u9519\u7684\u673a\u4f1a\uff0c\u4f7f\u5176\u4ece\u4e2d\u5b66\u4e60\uff0c\u6700\u7ec8\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2505.05279", "pdf": "https://arxiv.org/pdf/2505.05279", "abs": "https://arxiv.org/abs/2505.05279", "authors": ["Yi Yu", "Song Xia", "Siyuan Yang", "Chenqi Kong", "Wenhan Yang", "Shijian Lu", "Yap-Peng Tan", "Alex C. Kot"], "title": "MTL-UE: Learning to Learn Nothing for Multi-Task Learning", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Most existing unlearnable strategies focus on preventing unauthorized users\nfrom training single-task learning (STL) models with personal data.\nNevertheless, the paradigm has recently shifted towards multi-task data and\nmulti-task learning (MTL), targeting generalist and foundation models that can\nhandle multiple tasks simultaneously. Despite their growing importance, MTL\ndata and models have been largely neglected while pursuing unlearnable\nstrategies. This paper presents MTL-UE, the first unified framework for\ngenerating unlearnable examples for multi-task data and MTL models. Instead of\noptimizing perturbations for each sample, we design a generator-based structure\nthat introduces label priors and class-wise feature embeddings which leads to\nmuch better attacking performance. In addition, MTL-UE incorporates intra-task\nand inter-task embedding regularization to increase inter-class separation and\nsuppress intra-class variance which enhances the attack robustness greatly.\nFurthermore, MTL-UE is versatile with good supports for dense prediction tasks\nin MTL. It is also plug-and-play allowing integrating existing\nsurrogate-dependent unlearnable methods with little adaptation. Extensive\nexperiments show that MTL-UE achieves superior attacking performance\nconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5\nMTL task-weighting strategies.", "AI": {"tldr": "MTL-UE\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u4efb\u52a1\u6570\u636e\u548cMTL\u6a21\u578b\u751f\u6210\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5668\u7ed3\u6784\u548c\u5d4c\u5165\u6b63\u5219\u5316\u663e\u8457\u63d0\u5347\u653b\u51fb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u7b56\u7565\u4e3b\u8981\u9488\u5bf9\u5355\u4efb\u52a1\u5b66\u4e60\uff0c\u800c\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u6570\u636e\u548c\u6a21\u578b\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u4e0d\u8db3\u3002", "method": "MTL-UE\u91c7\u7528\u751f\u6210\u5668\u7ed3\u6784\uff0c\u5f15\u5165\u6807\u7b7e\u5148\u9a8c\u548c\u7c7b\u7279\u5f81\u5d4c\u5165\uff0c\u5e76\u7ed3\u5408\u4efb\u52a1\u5185\u548c\u4efb\u52a1\u95f4\u5d4c\u5165\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMTL-UE\u57284\u4e2aMTL\u6570\u636e\u96c6\u30013\u79cd\u57fa\u7840UE\u65b9\u6cd5\u30015\u79cd\u6a21\u578b\u67b6\u6784\u548c5\u79cdMTL\u4efb\u52a1\u52a0\u6743\u7b56\u7565\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MTL-UE\u4e3a\u591a\u4efb\u52a1\u6570\u636e\u548c\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u751f\u6210\u6846\u67b6\u3002"}}
{"id": "2505.05291", "pdf": "https://arxiv.org/pdf/2505.05291", "abs": "https://arxiv.org/abs/2505.05291", "authors": ["Benjamin A. Cohen", "Jonathan Fhima", "Meishar Meisel", "Baskin Meital", "Luis Filipe Nakayama", "Eran Berkowitz", "Joachim A. Behar"], "title": "Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "q-bio.TO"], "comment": "10 pages, 3 figures", "summary": "Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.", "AI": {"tldr": "\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u4f7fViT\u80fd\u4ece\u5927\u89c4\u6a21\u81ea\u7136\u56fe\u50cf\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u8868\u793a\uff0c\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u5728\u89c6\u7f51\u819c\u6210\u50cf\u4e2d\uff0c\u57fa\u4e8e\u81ea\u7136\u6216\u773c\u79d1\u6570\u636e\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u57df\u5185\u9884\u8bad\u7ec3\u7684\u4f18\u52bf\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u57fa\u4e8e\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u7684iBOT\u6a21\u578b\u5728AMD\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6311\u6218\u4e86\u57df\u5185\u9884\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u5728\u89c6\u7f51\u819c\u56fe\u50cf\u4efb\u52a1\u4e2d\uff0c\u57df\u5185\u9884\u8bad\u7ec3\u662f\u5426\u6bd4\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u66f4\u5177\u4f18\u52bf\u3002", "method": "\u57287\u4e2aDFI\u6570\u636e\u96c6\uff08\u517170,000\u5f20\u4e13\u5bb6\u6807\u6ce8\u56fe\u50cf\uff09\u4e0a\uff0c\u5bf96\u79cdSSL\u9884\u8bad\u7ec3\u7684ViT\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4efb\u52a1\u4e3a\u4e2d\u5ea6\u81f3\u665a\u671fAMD\u8bc6\u522b\u3002", "result": "iBOT\u6a21\u578b\uff08\u57fa\u4e8e\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\uff09\u5728\u8de8\u57df\u6cdb\u5316\u4e2d\u8868\u73b0\u6700\u4f73\uff08AUROC 0.80-0.97\uff09\uff0c\u4f18\u4e8e\u57df\u5185\u9884\u8bad\u7ec3\u6a21\u578b\uff08AUROC 0.78-0.96\uff09\u548c\u672a\u9884\u8bad\u7ec3\u7684ViT-L\uff08AUROC 0.68-0.91\uff09\u3002", "conclusion": "\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728AMD\u8bc6\u522b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u6311\u6218\u4e86\u57df\u5185\u9884\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u3002\u540c\u65f6\u53d1\u5e03\u4e86\u5df4\u897f\u7684\u5f00\u653e\u6570\u636e\u96c6BRAMD\u3002"}}
{"id": "2505.05309", "pdf": "https://arxiv.org/pdf/2505.05309", "abs": "https://arxiv.org/abs/2505.05309", "authors": ["Yifan Bian", "Chuanbo Tang", "Li Li", "Dong Liu"], "title": "Augmented Deep Contexts for Spatially Embedded Video Coding", "categories": ["eess.IV", "cs.CV"], "comment": "15 pages,CVPR", "summary": "Most Neural Video Codecs (NVCs) only employ temporal references to generate\ntemporal-only contexts and latent prior. These temporal-only NVCs fail to\nhandle large motions or emerging objects due to limited contexts and misaligned\nlatent prior. To relieve the limitations, we propose a Spatially Embedded Video\nCodec (SEVC), in which the low-resolution video is compressed for spatial\nreferences. Firstly, our SEVC leverages both spatial and temporal references to\ngenerate augmented motion vectors and hybrid spatial-temporal contexts.\nSecondly, to address the misalignment issue in latent prior and enrich the\nprior information, we introduce a spatial-guided latent prior augmented by\nmultiple temporal latent representations. At last, we design a joint\nspatial-temporal optimization to learn quality-adaptive bit allocation for\nspatial references, further boosting rate-distortion performance. Experimental\nresults show that our SEVC effectively alleviates the limitations in handling\nlarge motions or emerging objects, and also reduces 11.9% more bitrate than the\nprevious state-of-the-art NVC while providing an additional low-resolution\nbitstream. Our code and model are available at https://github.com/EsakaK/SEVC.", "AI": {"tldr": "SEVC\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u53c2\u8003\u7684\u89c6\u9891\u7f16\u7801\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u795e\u7ecf\u89c6\u9891\u7f16\u7801\u5668\u5728\u5904\u7406\u5927\u8fd0\u52a8\u6216\u65b0\u7269\u4f53\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u89c6\u9891\u7f16\u7801\u5668\u4ec5\u4f9d\u8d56\u65f6\u95f4\u53c2\u8003\uff0c\u5bfc\u81f4\u5728\u5904\u7406\u5927\u8fd0\u52a8\u6216\u65b0\u7269\u4f53\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "SEVC\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u53c2\u8003\u751f\u6210\u589e\u5f3a\u7684\u8fd0\u52a8\u5411\u91cf\u548c\u6df7\u5408\u4e0a\u4e0b\u6587\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u5f15\u5bfc\u7684\u6f5c\u5728\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSEVC\u5728\u5904\u7406\u5927\u8fd0\u52a8\u6216\u65b0\u7269\u4f53\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u6bd4\u7279\u7387\u964d\u4f4e\u4e8611.9%\u3002", "conclusion": "SEVC\u901a\u8fc7\u7a7a\u95f4\u5d4c\u5165\u548c\u8054\u5408\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7f16\u7801\u7684\u6027\u80fd\u3002"}}
{"id": "2505.05356", "pdf": "https://arxiv.org/pdf/2505.05356", "abs": "https://arxiv.org/abs/2505.05356", "authors": ["Runfeng Li", "Mikhail Okunev", "Zixuan Guo", "Anh Ha Duong", "Christian Richardt", "Matthew O'Toole", "James Tompkin"], "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "We present a method to reconstruct dynamic scenes from monocular\ncontinuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that\nachieves similar or better accuracy than neural volumetric approaches and is\n100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a\nsingle viewpoint is a significant challenge in computer vision. In C-ToF\nradiance field reconstruction, the property of interest-depth-is not directly\nmeasured, causing an additional challenge. This problem has a large and\nunderappreciated impact upon the optimization when using a fast primitive-based\nscene representation like 3D Gaussian splatting, which is commonly used with\nmulti-view data to produce satisfactory results and is brittle in its\noptimization otherwise. We incorporate two heuristics into the optimization to\nimprove the accuracy of scene geometry represented by Gaussians. Experimental\nresults show that our approach produces accurate reconstructions under\nconstrained C-ToF sensing conditions, including for fast motions like swinging\nbaseball bats. https://visual.cs.brown.edu/gftorf", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u8fde\u7eed\u6ce2\u98de\u884c\u65f6\u95f4\uff08C-ToF\uff09\u76f8\u673a\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u6bd4\u795e\u7ecf\u4f53\u79ef\u65b9\u6cd5\u66f4\u51c6\u786e\u4e14\u5feb100\u500d\u3002", "motivation": "\u5feb\u901f\u4ece\u5355\u4e00\u89c6\u89d2\u5b9e\u73b0\u9ad8\u4fdd\u771f\u52a8\u60013D\u91cd\u5efa\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u5728C-ToF\u8f90\u5c04\u573a\u91cd\u5efa\u4e2d\uff0c\u6df1\u5ea6\u4fe1\u606f\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\uff0c\u589e\u52a0\u4e86\u4f18\u5316\u96be\u5ea6\u3002", "method": "\u5728\u4f18\u5316\u4e2d\u5f15\u5165\u4e24\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6539\u8fdb\u9ad8\u65af\u8868\u793a\u7684\u573a\u666f\u51e0\u4f55\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7b49\u5feb\u901f\u57fa\u5143\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53d7\u9650C-ToF\u611f\u77e5\u6761\u4ef6\u4e0b\uff08\u5982\u5feb\u901f\u6325\u68d2\u52a8\u4f5c\uff09\u80fd\u751f\u6210\u51c6\u786e\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86C-ToF\u6570\u636e\u4f18\u5316\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2505.05374", "pdf": "https://arxiv.org/pdf/2505.05374", "abs": "https://arxiv.org/abs/2505.05374", "authors": ["Naveenkumar G Venkataswamy", "Poorna Ravi", "Stephanie Schuckers", "Masudul H. Imtiaz"], "title": "OcularAge: A Comparative Study of Iris and Periocular Images for Pediatric Age Estimation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Estimating a child's age from ocular biometric images is challenging due to\nsubtle physiological changes and the limited availability of longitudinal\ndatasets. Although most biometric age estimation studies have focused on facial\nfeatures and adult subjects, pediatric-specific analysis, particularly of the\niris and periocular regions, remains relatively unexplored. This study presents\na comparative evaluation of iris and periocular images for estimating the ages\nof children aged between 4 and 16 years. We utilized a longitudinal dataset\ncomprising more than 21,000 near-infrared (NIR) images, collected from 288\npediatric subjects over eight years using two different imaging sensors. A\nmulti-task deep learning framework was employed to jointly perform age\nprediction and age-group classification, enabling a systematic exploration of\nhow different convolutional neural network (CNN) architectures, particularly\nthose adapted for non-square ocular inputs, capture the complex variability\ninherent in pediatric eye images. The results show that periocular models\nconsistently outperform iris-based models, achieving a mean absolute error\n(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These\nresults mark the first demonstration that reliable age estimation is feasible\nfrom children's ocular images, enabling privacy-preserving age checks in\nchild-centric applications. This work establishes the first longitudinal\nbenchmark for pediatric ocular age estimation, providing a foundation for\ndesigning robust, child-focused biometric systems. The developed models proved\nresilient across different imaging sensors, confirming their potential for\nreal-world deployment. They also achieved inference speeds of less than 10\nmilliseconds per image on resource-constrained VR headsets, demonstrating their\nsuitability for real-time applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u8679\u819c\u548c\u773c\u5468\u56fe\u50cf\u5728\u513f\u7ae5\u5e74\u9f84\u4f30\u8ba1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u773c\u5468\u6a21\u578b\u4f18\u4e8e\u8679\u819c\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u9690\u79c1\u4fdd\u62a4\u5e74\u9f84\u9a8c\u8bc1\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u513f\u7ae5\u5e74\u9f84\u4f30\u8ba1\u56e0\u751f\u7406\u53d8\u5316\u7ec6\u5fae\u548c\u7eb5\u5411\u6570\u636e\u7a00\u7f3a\u800c\u5177\u6311\u6218\u6027\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u6210\u4eba\u9762\u90e8\u7279\u5f81\uff0c\u513f\u79d1\u773c\u90e8\u5206\u6790\u8f83\u5c11\u3002", "method": "\u4f7f\u7528\u5305\u542b21,000\u591a\u5f20\u8fd1\u7ea2\u5916\u56fe\u50cf\u7684\u7eb5\u5411\u6570\u636e\u96c6\uff0c\u91c7\u7528\u591a\u4efb\u52a1\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u5e74\u9f84\u9884\u6d4b\u548c\u5e74\u9f84\u7ec4\u5206\u7c7b\u3002", "result": "\u773c\u5468\u6a21\u578b\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.33\u5e74\uff0c\u5e74\u9f84\u7ec4\u5206\u7c7b\u51c6\u786e\u7387\u8fbe83.82%\uff0c\u4e14\u6a21\u578b\u5728\u4e0d\u540c\u4f20\u611f\u5668\u4e0a\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u7814\u7a76\u9996\u6b21\u8bc1\u660e\u513f\u7ae5\u773c\u5468\u56fe\u50cf\u53ef\u7528\u4e8e\u53ef\u9760\u5e74\u9f84\u4f30\u8ba1\uff0c\u4e3a\u513f\u7ae5\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
