{"id": "2511.03765", "pdf": "https://arxiv.org/pdf/2511.03765", "abs": "https://arxiv.org/abs/2511.03765", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Jae-Jin Lee", "Woojoo Lee"], "title": "LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices", "categories": ["cs.CV", "cs.AR"], "comment": "8 pages, 6 figures, 2 tables, DATE 2026 accepted paper", "summary": "On-device fine-tuning of CNNs is essential to withstand domain shift in edge\napplications such as Human Activity Recognition (HAR), yet full fine-tuning is\ninfeasible under strict memory, compute, and energy budgets. We present\nLoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on\nLow-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies\nTensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional\nlayers, (ii) selectively updates only the output-side core with\nzero-initialization to keep the auxiliary path inactive at the start, and (iii)\nfuses the update back into dense kernels, leaving inference cost unchanged.\nThis design preserves convolutional structure and reduces the number of\ntrainable parameters by up to two orders of magnitude compared to full\nfine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves\naccuracy within 4.7% of full fine-tuning while updating at most 1.49% of\nparameters, consistently outperforming prior parameter-efficient baselines\nunder similar budgets. On a Jetson Orin Nano, TT-SVD initialization and\nselective-core training yield 1.4-3.8x faster convergence to target F1.\nLoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN\nadaptation practical for edge platforms.", "AI": {"tldr": "LoRA-Edge\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f20\u91cf\u8bad\u7ec3\u8f85\u52a9\u7684\u4f4e\u79e9\u9002\u5e94\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0CNN\u7684\u8f7b\u91cf\u7ea7\u5fae\u8c03\uff0c\u5927\u5e45\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u5728\u8fb9\u7f18\u5e94\u7528\u4e2d\u5982\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff0c\u9886\u57df\u504f\u79fb\u95ee\u9898\u9700\u8981\u8bbe\u5907\u7aef\u5fae\u8c03CNN\uff0c\u4f46\u5b8c\u5168\u5fae\u8c03\u5728\u4e25\u683c\u7684\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u80fd\u8017\u9884\u7b97\u4e0b\u4e0d\u53ef\u884c\u3002", "method": "\u5bf9\u9884\u8bad\u7ec3\u5377\u79ef\u5c42\u5e94\u7528TT-SVD\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u8f93\u51fa\u4fa7\u6838\u5fc3\u5e76\u91c7\u7528\u96f6\u521d\u59cb\u5316\uff0c\u6700\u540e\u5c06\u66f4\u65b0\u878d\u5408\u56de\u5bc6\u96c6\u6838\u4e2d\uff0c\u4fdd\u6301\u63a8\u7406\u6210\u672c\u4e0d\u53d8\u3002", "result": "\u5728\u591a\u79cdHAR\u6570\u636e\u96c6\u548cCNN\u9aa8\u5e72\u4e0a\uff0cLoRA-Edge\u4ec5\u66f4\u65b0\u6700\u591a1.49%\u7684\u53c2\u6570\uff0c\u5c31\u80fd\u8fbe\u5230\u5b8c\u5168\u5fae\u8c03\u51c6\u786e\u7387\u768495.3%\u4ee5\u5185\uff0c\u5728Jetson Orin Nano\u4e0a\u5b9e\u73b01.4-3.8\u500d\u7684\u6536\u655b\u52a0\u901f\u3002", "conclusion": "LoRA-Edge\u4f7f\u5f97\u7ed3\u6784\u5bf9\u9f50\u3001\u53c2\u6570\u9ad8\u6548\u7684\u8bbe\u5907\u7aefCNN\u9002\u5e94\u5728\u8fb9\u7f18\u5e73\u53f0\u4e0a\u53d8\u5f97\u5b9e\u7528\u53ef\u884c\u3002"}}
{"id": "2511.03819", "pdf": "https://arxiv.org/pdf/2511.03819", "abs": "https://arxiv.org/abs/2511.03819", "authors": ["Ozan Kanbertay", "Richard Vogg", "Elif Karakoc", "Peter M. Kappeler", "Claudia Fichtel", "Alexander S. Ecker"], "title": "SILVI: Simple Interface for Labeling Video Interactions", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Computer vision methods are increasingly used for the automated analysis of\nlarge volumes of video data collected through camera traps, drones, or direct\nobservations of animals in the wild. While recent advances have focused\nprimarily on detecting individual actions, much less work has addressed the\ndetection and annotation of interactions -- a crucial aspect for understanding\nsocial and individualized animal behavior. Existing open-source annotation\ntools support either behavioral labeling without localization of individuals,\nor localization without the capacity to capture interactions. To bridge this\ngap, we present SILVI, an open-source labeling software that integrates both\nfunctionalities. SILVI enables researchers to annotate behaviors and\ninteractions directly within video data, generating structured outputs suitable\nfor training and validating computer vision models. By linking behavioral\necology with computer vision, SILVI facilitates the development of automated\napproaches for fine-grained behavioral analyses. Although developed primarily\nin the context of animal behavior, SILVI could be useful more broadly to\nannotate human interactions in other videos that require extracting dynamic\nscene graphs. The software, along with documentation and download instructions,\nis available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.", "AI": {"tldr": "SILVI\u662f\u4e00\u4e2a\u5f00\u6e90\u6807\u6ce8\u8f6f\u4ef6\uff0c\u7528\u4e8e\u5728\u89c6\u9891\u6570\u636e\u4e2d\u6807\u6ce8\u884c\u4e3a\u548c\u4ea4\u4e92\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u5728\u4ea4\u4e92\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6807\u6ce8\u5de5\u5177\u8981\u4e48\u652f\u6301\u884c\u4e3a\u6807\u6ce8\u4f46\u4e0d\u5b9a\u4f4d\u4e2a\u4f53\uff0c\u8981\u4e48\u652f\u6301\u5b9a\u4f4d\u4f46\u4e0d\u6355\u6349\u4ea4\u4e92\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7406\u89e3\u52a8\u7269\u793e\u4ea4\u548c\u4e2a\u4f53\u5316\u884c\u4e3a\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1SILVI\u8f6f\u4ef6\uff0c\u6574\u5408\u884c\u4e3a\u548c\u4ea4\u4e92\u6807\u6ce8\u529f\u80fd\uff0c\u76f4\u63a5\u5728\u89c6\u9891\u6570\u636e\u4e2d\u6807\u6ce8\uff0c\u751f\u6210\u9002\u5408\u8bad\u7ec3\u548c\u9a8c\u8bc1\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u3002", "result": "SILVI\u6210\u529f\u5b9e\u73b0\u4e86\u884c\u4e3a\u548c\u4ea4\u4e92\u7684\u8054\u5408\u6807\u6ce8\uff0c\u4e3a\u7cbe\u7ec6\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5e76\u53ef\u7528\u4e8e\u4eba\u7c7b\u4ea4\u4e92\u6807\u6ce8\u3002", "conclusion": "SILVI\u901a\u8fc7\u8fde\u63a5\u884c\u4e3a\u751f\u6001\u5b66\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u4fc3\u8fdb\u4e86\u7cbe\u7ec6\u884c\u4e3a\u5206\u6790\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.03855", "pdf": "https://arxiv.org/pdf/2511.03855", "abs": "https://arxiv.org/abs/2511.03855", "authors": ["Duong Mai", "Lawrence Hall"], "title": "Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets", "categories": ["cs.CV", "cs.AI"], "comment": "Abstract accepted for oral presentation at SPIE Medical Imaging 2026:\n  Computer-Aided Diagnosis", "summary": "Deep learned (DL) models for image recognition have been shown to fail to\ngeneralize to data from different devices, populations, etc. COVID-19 detection\nfrom Chest X-rays (CXRs), in particular, has been shown to fail to generalize\nto out-of-distribution (OOD) data from new clinical sources not covered in the\ntraining set. This occurs because models learn to exploit shortcuts -\nsource-specific artifacts that do not translate to new distributions - rather\nthan reasonable biomarkers to maximize performance on in-distribution (ID)\ndata. Rendering the models more robust to distribution shifts, our study\ninvestigates the use of fundamental noise injection techniques (Gaussian,\nSpeckle, Poisson, and Salt and Pepper) during training. Our empirical results\ndemonstrate that this technique can significantly reduce the performance gap\nbetween ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results\naveraged over ten random seeds across key metrics such as AUC, F1, accuracy,\nrecall and specificity. Our source code is publicly available at\nhttps://github.com/Duongmai127/Noisy-ood", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u57fa\u672c\u566a\u58f0\u6ce8\u5165\u6280\u672f\uff08\u9ad8\u65af\u3001\u6591\u70b9\u3001\u6cca\u677e\u548c\u6912\u76d0\u566a\u58f0\uff09\u6765\u63d0\u9ad8COVID-19\u80f8\u90e8X\u5149\u56fe\u50cf\u8bc6\u522b\u6a21\u578b\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u56fe\u50cf\u8bc6\u522b\u4e2d\u5bb9\u6613\u5b66\u4e60\u6e90\u7279\u5b9a\u7684\u4f2a\u5f71\uff08\u6377\u5f84\uff09\u800c\u975e\u5408\u7406\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728COVID-19\u80f8\u90e8X\u5149\u68c0\u6d4b\u4e2d\u3002", "method": "\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e94\u7528\u56db\u79cd\u57fa\u672c\u566a\u58f0\u6ce8\u5165\u6280\u672f\uff1a\u9ad8\u65af\u566a\u58f0\u3001\u6591\u70b9\u566a\u58f0\u3001\u6cca\u677e\u566a\u58f0\u548c\u6912\u76d0\u566a\u58f0\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "result": "\u566a\u58f0\u6ce8\u5165\u6280\u672f\u663e\u8457\u51cf\u5c11\u4e86\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8bc4\u4f30\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4ece0.10-0.20\u964d\u4f4e\u52300.01-0.06\uff0c\u57fa\u4e8eAUC\u3001F1\u3001\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u7279\u5f02\u6027\u7b49\u5173\u952e\u6307\u6807\u5728\u5341\u4e2a\u968f\u673a\u79cd\u5b50\u4e0a\u7684\u5e73\u5747\u7ed3\u679c\u3002", "conclusion": "\u57fa\u672c\u566a\u58f0\u6ce8\u5165\u6280\u672f\u662f\u63d0\u9ad8COVID-19\u80f8\u90e8X\u5149\u8bc6\u522b\u6a21\u578b\u5bf9\u5206\u5e03\u5916\u6570\u636e\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.03882", "pdf": "https://arxiv.org/pdf/2511.03882", "abs": "https://arxiv.org/abs/2511.03882", "authors": ["Florence Klitzner", "Blanca Inigo", "Benjamin D. Killeen", "Lalithkumar Seenivasan", "Michelle Song", "Axel Krieger", "Mathias Unberath"], "title": "Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation learning-based robot control policies are enjoying renewed interest\nin video-based robotics. However, it remains unclear whether this approach\napplies to X-ray-guided procedures, such as spine instrumentation. This is\nbecause interpretation of multi-view X-rays is complex. We examine\nopportunities and challenges for imitation policy learning in bi-plane-guided\ncannula insertion. We develop an in silico sandbox for scalable, automated\nsimulation of X-ray-guided spine procedures with a high degree of realism. We\ncurate a dataset of correct trajectories and corresponding bi-planar X-ray\nsequences that emulate the stepwise alignment of providers. We then train\nimitation learning policies for planning and open-loop control that iteratively\nalign a cannula solely based on visual information. This precisely controlled\nsetup offers insights into limitations and capabilities of this method. Our\npolicy succeeded on the first attempt in 68.5% of cases, maintaining safe\nintra-pedicular trajectories across diverse vertebral levels. The policy\ngeneralized to complex anatomy, including fractures, and remained robust to\nvaried initializations. Rollouts on real bi-planar X-rays further suggest that\nthe model can produce plausible trajectories, despite training exclusively in\nsimulation. While these preliminary results are promising, we also identify\nlimitations, especially in entry point precision. Full closed-look control will\nrequire additional considerations around how to provide sufficiently frequent\nfeedback. With more robust priors and domain knowledge, such models may provide\na foundation for future efforts toward lightweight and CT-free robotic\nintra-operative spinal navigation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u53cc\u5e73\u9762X\u5c04\u7ebf\u5f15\u5bfc\u4e0b\u810a\u67f1\u624b\u672f\u4e2d\u5e94\u7528\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u53ef\u884c\u6027\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u771f\u5b9e\u5ea6\u7684\u6a21\u62df\u73af\u5883\uff0c\u8bad\u7ec3\u57fa\u4e8e\u89c6\u89c9\u4fe1\u606f\u7684\u63d2\u7ba1\u5bf9\u9f50\u7b56\u7565\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9eX\u5c04\u7ebf\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728X\u5c04\u7ebf\u5f15\u5bfc\u7684\u810a\u67f1\u624b\u672f\uff08\u5982\u810a\u67f1\u5668\u68b0\u690d\u5165\uff09\u4e2d\u7684\u9002\u7528\u6027\uff0c\u56e0\u4e3a\u591a\u89c6\u56feX\u5c04\u7ebf\u7684\u89e3\u91ca\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u9ad8\u771f\u5b9e\u5ea6\u7684\u6a21\u62df\u73af\u5883\uff0c\u6536\u96c6\u6b63\u786e\u8f68\u8ff9\u548c\u5bf9\u5e94\u53cc\u5e73\u9762X\u5c04\u7ebf\u5e8f\u5217\u7684\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u57fa\u4e8e\u89c6\u89c9\u4fe1\u606f\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u89c4\u5212\u548c\u5f00\u73af\u63a7\u5236\u63d2\u7ba1\u7684\u9010\u6b65\u5bf9\u9f50\u3002", "result": "\u7b56\u7565\u572868.5%\u7684\u60c5\u51b5\u4e0b\u9996\u6b21\u5c1d\u8bd5\u6210\u529f\uff0c\u80fd\u4fdd\u6301\u5b89\u5168\u7684\u690e\u5f13\u6839\u5185\u8f68\u8ff9\uff0c\u9002\u5e94\u590d\u6742\u89e3\u5256\u7ed3\u6784\uff08\u5305\u62ec\u9aa8\u6298\uff09\uff0c\u5bf9\u591a\u79cd\u521d\u59cb\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u771f\u5b9eX\u5c04\u7ebf\u4e0a\u4e5f\u80fd\u4ea7\u751f\u5408\u7406\u8f68\u8ff9\u3002", "conclusion": "\u867d\u7136\u521d\u6b65\u7ed3\u679c\u6709\u524d\u666f\uff0c\u4f46\u5728\u5165\u53e3\u70b9\u7cbe\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5b8c\u5168\u95ed\u73af\u63a7\u5236\u9700\u8981\u66f4\u9891\u7e41\u7684\u53cd\u9988\u673a\u5236\u3002\u7ed3\u5408\u66f4\u5f3a\u7684\u5148\u9a8c\u77e5\u8bc6\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u8fd9\u7c7b\u6a21\u578b\u53ef\u4e3a\u8f7b\u91cf\u7ea7\u3001\u65e0CT\u7684\u673a\u5668\u4eba\u672f\u4e2d\u810a\u67f1\u5bfc\u822a\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2511.03888", "pdf": "https://arxiv.org/pdf/2511.03888", "abs": "https://arxiv.org/abs/2511.03888", "authors": ["Abdulmumin Sa'ad", "Sulaimon Oyeniyi Adebayo", "Abdul Jabbar Siddiqui"], "title": "Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages", "summary": "The global waste crisis is escalating, with solid waste generation expected\nto increase by 70% by 2050. Traditional waste collection methods, particularly\nin remote or harsh environments like deserts, are labor-intensive, inefficient,\nand often hazardous. Recent advances in computer vision and deep learning have\nopened the door to automated waste detection systems, yet most research focuses\non urban environments and recyclable materials, overlooking organic and\nhazardous waste and underexplored terrains such as deserts. In this work, we\npropose an enhanced real-time object detection framework based on a pruned,\nlightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)\nand specialized data augmentation strategies. Using the DroneTrashNet dataset,\nwe demonstrate significant improvements in precision, recall, and mean average\nprecision (mAP), while achieving low latency and compact model size suitable\nfor deployment on resource-constrained aerial drones. Benchmarking our model\nagainst state-of-the-art lightweight YOLO variants further highlights its\noptimal balance of accuracy and efficiency. Our results validate the\neffectiveness of combining data-centric and model-centric enhancements for\nrobust, real-time waste detection in desert environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8f7b\u91cf\u5316YOLOv12\u7ed3\u5408\u81ea\u5bf9\u6297\u8bad\u7ec3\u548c\u6570\u636e\u589e\u5f3a\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u6c99\u6f20\u73af\u5883\u4e2d\u7684\u5783\u573e\u68c0\u6d4b\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5168\u7403\u5783\u573e\u5371\u673a\u65e5\u76ca\u4e25\u91cd\uff0c\u4f20\u7edf\u5783\u573e\u6536\u96c6\u65b9\u6cd5\u5728\u504f\u8fdc\u73af\u5883\u6548\u7387\u4f4e\u4e0b\u4e14\u5371\u9669\u3002\u73b0\u6709\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57ce\u5e02\u73af\u5883\u548c\u53ef\u56de\u6536\u5783\u573e\uff0c\u5ffd\u89c6\u4e86\u6709\u673a/\u5371\u9669\u5783\u573e\u548c\u6c99\u6f20\u7b49\u672a\u5145\u5206\u63a2\u7d22\u7684\u5730\u5f62\u3002", "method": "\u4f7f\u7528\u4fee\u526a\u8f7b\u91cf\u5316\u7684YOLOv12\u6a21\u578b\uff0c\u96c6\u6210\u81ea\u5bf9\u6297\u8bad\u7ec3(SAT)\u548c\u4e13\u95e8\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5728DroneTrashNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u5e73\u5747\u7cbe\u5ea6(mAP)\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u5c0f\u6a21\u578b\u5c3a\u5bf8\uff0c\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u4e0a\u90e8\u7f72\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u8f7b\u91cf\u7ea7YOLO\u53d8\u4f53\u76f8\u6bd4\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u6570\u636e\u4e2d\u5fc3\u548c\u6a21\u578b\u4e2d\u5fc3\u589e\u5f3a\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u5bf9\u4e8e\u6c99\u6f20\u73af\u5883\u4e2d\u7a33\u5065\u5b9e\u65f6\u5783\u573e\u68c0\u6d4b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.03891", "pdf": "https://arxiv.org/pdf/2511.03891", "abs": "https://arxiv.org/abs/2511.03891", "authors": ["Hlali Azzeddine", "Majid Ben Yakhlef", "Soulaiman El Hazzat"], "title": "Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition", "categories": ["cs.CV", "cs.AI", "cs.DB"], "comment": null, "summary": "Small, imbalanced datasets and poor input image quality can lead to high\nfalse predictions rates with deep learning models. This paper introduces\nClass-Based Image Composition, an approach that allows us to reformulate\ntraining inputs through a fusion of multiple images of the same class into\ncombined visual composites, named Composite Input Images (CoImg). That enhances\nthe intra-class variance and improves the valuable information density per\ntraining sample and increases the ability of the model to distinguish between\nsubtle disease patterns. Our method was evaluated on the Optical Coherence\nTomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et\nal., 2024), which contains 2,064 high-resolution optical coherence tomography\n(OCT) scans of the human retina, representing seven distinct diseases with a\nsignificant class imbalance. We constructed a perfectly class-balanced version\nof this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout\ncomposite image. To assess the effectiveness of this new representation, we\nconducted a comparative analysis between the original dataset and its variant\nusing a VGG16 model. A fair comparison was ensured by utilizing the identical\nmodel architecture and hyperparameters for all experiments. The proposed\napproach markedly improved diagnostic results.The enhanced Dataset achieved\nnear-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared\nto a baseline model trained on raw dataset. The false prediction rate was also\nsignificantly lower, this demonstrates that the method can producehigh-quality\npredictions even for weak datasets affected by class imbalance or small sample\nsize.", "AI": {"tldr": "\u63d0\u51faClass-Based Image Composition\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u540c\u7c7b\u591a\u5f20\u56fe\u50cf\u878d\u5408\u6210Composite Input Images\u6765\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u5c0f\u6837\u672c\u3001\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u548c\u56fe\u50cf\u8d28\u91cf\u5dee\u5bfc\u81f4\u7684\u8bef\u5224\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5c0f\u6837\u672c\u3001\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u548c\u8f93\u5165\u56fe\u50cf\u8d28\u91cf\u5dee\u5bfc\u81f4\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bef\u5224\u7387\u9ad8\u7684\u95ee\u9898\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u7ec6\u5fae\u75be\u75c5\u6a21\u5f0f\u7684\u533a\u5206\u80fd\u529b\u3002", "method": "\u4f7f\u7528Class-Based Image Composition\u65b9\u6cd5\uff0c\u5c06\u540c\u7c7b\u591a\u5f20\u56fe\u50cf\u878d\u5408\u62103x1\u5e03\u5c40\u7684Composite Input Images\uff0c\u6784\u5efa\u5e73\u8861\u6570\u636e\u96c6Co-OCTDL\uff0c\u5e76\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u5728\u76f8\u540cVGG16\u67b6\u6784\u4e0b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5728OCTDL\u89c6\u7f51\u819c\u626b\u63cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u51c6\u786e\u7387\u63d0\u5347\u81f399.6%\uff0cF1-score\u8fbe0.995\uff0cAUC\u8fbe0.9996\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5224\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u5c0f\u6837\u672c\u3001\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u9884\u6d4b\u8d28\u91cf\uff0c\u5373\u4f7f\u5bf9\u4e8e\u5f31\u6570\u636e\u96c6\u4e5f\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u9884\u6d4b\u7ed3\u679c\u3002"}}
{"id": "2511.03912", "pdf": "https://arxiv.org/pdf/2511.03912", "abs": "https://arxiv.org/abs/2511.03912", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "William CW Chen", "KC Santosh"], "title": "I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unknown anomaly detection in medical imaging remains a fundamental challenge\ndue to the scarcity of labeled anomalies and the high cost of expert\nsupervision. We introduce an unsupervised, oracle-free framework that\nincrementally expands a trusted set of normal samples without any anomaly\nlabels. Starting from a small, verified seed of normal images, our method\nalternates between lightweight adapter updates and uncertainty-gated sample\nadmission. A frozen pretrained vision backbone is augmented with tiny\nconvolutional adapters, ensuring rapid domain adaptation with negligible\ncomputational overhead. Extracted embeddings are stored in a compact coreset\nenabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during\nincremental expansion is enforced by dual probabilistic gates, a sample is\nadmitted into the normal memory only if its distance to the existing coreset\nlies within a calibrated z-score threshold, and its SWAG-based epistemic\nuncertainty remains below a seed-calibrated bound. This mechanism prevents\ndrift and false inclusions without relying on generative reconstruction or\nreplay buffers. Empirically, our system steadily refines the notion of\nnormality as unlabeled data arrive, producing substantial gains over baselines.\nOn COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on\nPneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,\nROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These\nresults highlight the effectiveness and efficiency of the proposed framework\nfor real-world, label-scarce medical imaging applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u3001\u65e0\u9700\u4e13\u5bb6\u6807\u6ce8\u7684\u533b\u5b66\u5f71\u50cf\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u91cf\u6269\u5c55\u6b63\u5e38\u6837\u672c\u96c6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u66f4\u65b0\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u6837\u672c\u51c6\u5165\u673a\u5236\uff0c\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u672a\u77e5\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\u548c\u4e13\u5bb6\u76d1\u7763\u6210\u672c\u9ad8\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u5f02\u5e38\u6807\u7b7e\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u6dfb\u52a0\u5c0f\u578b\u5377\u79ef\u9002\u914d\u5668\u8fdb\u884c\u5feb\u901f\u57df\u9002\u5e94\u3002\u901a\u8fc7k\u8fd1\u90bb\u5f02\u5e38\u8bc4\u5206\u548c\u53cc\u6982\u7387\u95e8\u63a7\u673a\u5236\uff08\u8ddd\u79bbz-score\u9608\u503c\u548cSWAG\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff09\u786e\u4fdd\u589e\u91cf\u6269\u5c55\u7684\u5b89\u5168\u6027\u3002", "result": "\u5728COVID-CXR\u6570\u636e\u96c6\u4e0aROC-AUC\u4ece0.9489\u63d0\u5347\u52300.9982\uff0cF1\u4ece0.8048\u63d0\u5347\u52300.9746\uff1b\u5728Pneumonia CXR\u4e0aROC-AUC\u4ece0.6834\u63d0\u5347\u52300.8968\uff1b\u5728Brain MRI ND-5\u4e0aROC-AUC\u4ece0.6041\u63d0\u5347\u52300.7269\uff0cPR-AUC\u4ece0.7539\u63d0\u5347\u52300.8211\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u4e16\u754c\u6807\u7b7e\u7a00\u7f3a\u7684\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u6301\u7eed\u4f18\u5316\u6b63\u5e38\u6027\u6982\u5ff5\u7684\u5b9a\u4e49\u3002"}}
{"id": "2511.03943", "pdf": "https://arxiv.org/pdf/2511.03943", "abs": "https://arxiv.org/abs/2511.03943", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization", "categories": ["cs.CV"], "comment": null, "summary": "Temporal action localization requires precise boundary detection; however,\ncurrent methods apply uniform computation despite significant variations in\ndifficulty across boundaries. We present two complementary contributions.\nFirst, Boundary Distance Regression (BDR) provides information-theoretically\noptimal localization through signed-distance regression rather than\nclassification, achieving 43\\% sharper boundary peaks. BDR retrofits to\nexisting methods with approximately 50 lines of code, yielding consistent 1.8\nto 3.1\\% mAP@0.7 improvements across diverse architectures. Second, Adaptive\nTemporal Refinement (ATR) allocates computation via continuous depth selection\n$\\tau \\in [0,1]$, enabling end-to-end differentiable optimization without\nreinforcement learning. On THUMOS14, ATR achieves 56.5\\% mAP@0.7 at 162G FLOPs,\ncompared to 53.6\\% at 198G for uniform processing, providing a 2.9\\%\nimprovement with 18\\% less compute. Gains scale with boundary heterogeneity,\nshowing 4.2\\% improvement on short actions. Training cost is mitigated via\nknowledge distillation, with lightweight students retaining 99\\% performance at\nbaseline cost. Results are validated across four benchmarks with rigorous\nstatistical testing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u4e92\u8865\u7684\u8d21\u732e\uff1a\u8fb9\u754c\u8ddd\u79bb\u56de\u5f52(BDR)\u901a\u8fc7\u6709\u7b26\u53f7\u8ddd\u79bb\u56de\u5f52\u66ff\u4ee3\u5206\u7c7b\u5b9e\u73b0\u4fe1\u606f\u7406\u8bba\u6700\u4f18\u5b9a\u4f4d\uff0c\u81ea\u9002\u5e94\u65f6\u95f4\u7ec6\u5316(ATR)\u901a\u8fc7\u8fde\u7eed\u6df1\u5ea6\u9009\u62e9\u5206\u914d\u8ba1\u7b97\u91cf\uff0c\u5728THUMOS14\u4e0a\u4ee5\u66f4\u5c11\u8ba1\u7b97\u91cf\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\u5bf9\u6240\u6709\u8fb9\u754c\u91c7\u7528\u7edf\u4e00\u8ba1\u7b97\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u8fb9\u754c\u96be\u5ea6\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1. BDR\uff1a\u4f7f\u7528\u6709\u7b26\u53f7\u8ddd\u79bb\u56de\u5f52\u66ff\u4ee3\u8fb9\u754c\u5206\u7c7b\uff0c\u63d0\u4f9b\u4fe1\u606f\u7406\u8bba\u6700\u4f18\u5b9a\u4f4d\uff1b2. ATR\uff1a\u901a\u8fc7\u8fde\u7eed\u6df1\u5ea6\u9009\u62e9\u03c4\u2208[0,1]\u5206\u914d\u8ba1\u7b97\u91cf\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u4f18\u5316\u3002", "result": "BDR\u5728\u73b0\u6709\u65b9\u6cd5\u4e0a\u4ec5\u9700\u7ea650\u884c\u4ee3\u7801\u5373\u53ef\u5b9e\u73b01.8-3.1% mAP@0.7\u63d0\u5347\uff1bATR\u5728THUMOS14\u4e0a\u8fbe\u523056.5% mAP@0.7\uff08162G FLOPs\uff09\uff0c\u76f8\u6bd4\u5747\u5300\u5904\u7406\u768453.6%\uff08198G FLOPs\uff09\u63d0\u53472.9%\u4e14\u51cf\u5c1118%\u8ba1\u7b97\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u8ba1\u7b97\u5206\u914d\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u8fb9\u754c\u5f02\u8d28\u6027\u9ad8\u7684\u77ed\u52a8\u4f5c\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6709\u6548\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u3002"}}
{"id": "2511.03950", "pdf": "https://arxiv.org/pdf/2511.03950", "abs": "https://arxiv.org/abs/2511.03950", "authors": ["Zhejia Cai", "Puhua Jiang", "Shiwei Mao", "Hongkun Cao", "Ruqi Huang"], "title": "Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages", "summary": "Reconstructing real-world objects from multi-view images is essential for\napplications in 3D editing, AR/VR, and digital content creation. Existing\nmethods typically prioritize either geometric accuracy (Multi-View Stereo) or\nphotorealistic rendering (Novel View Synthesis), often decoupling geometry and\nappearance optimization, which hinders downstream editing tasks. This paper\nadvocates an unified treatment on geometry and appearance optimization for\nseamless Gaussian-mesh joint optimization. More specifically, we propose a\nnovel framework that simultaneously optimizes mesh geometry (vertex positions\nand faces) and vertex colors via Gaussian-guided mesh differentiable rendering,\nleveraging photometric consistency from input images and geometric\nregularization from normal and depth maps. The obtained high-quality 3D\nreconstruction can be further exploit in down-stream editing tasks, such as\nrelighting and shape deformation. The code will be publicly available upon\nacceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u51e0\u4f55\u548c\u5916\u89c2\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u5f15\u5bfc\u7684\u7f51\u683c\u53ef\u5fae\u5206\u6e32\u67d3\u540c\u65f6\u4f18\u5316\u7f51\u683c\u51e0\u4f55\uff08\u9876\u70b9\u4f4d\u7f6e\u548c\u9762\uff09\u548c\u9876\u70b9\u989c\u8272\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u51e0\u4f55\u7cbe\u5ea6\uff08\u591a\u89c6\u56fe\u7acb\u4f53\uff09\u548c\u771f\u5b9e\u611f\u6e32\u67d3\uff08\u65b0\u89c6\u89d2\u5408\u6210\uff09\u4e4b\u95f4\u6743\u8861\uff0c\u5c06\u51e0\u4f55\u548c\u5916\u89c2\u4f18\u5316\u89e3\u8026\uff0c\u8fd9\u963b\u788d\u4e86\u4e0b\u6e38\u7f16\u8f91\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u9ad8\u65af\u5f15\u5bfc\u7684\u7f51\u683c\u53ef\u5fae\u5206\u6e32\u67d3\uff0c\u5229\u7528\u8f93\u5165\u56fe\u50cf\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u548c\u6cd5\u7ebf/\u6df1\u5ea6\u56fe\u7684\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u540c\u65f6\u4f18\u5316\u7f51\u683c\u51e0\u4f55\u548c\u9876\u70b9\u989c\u8272\u3002", "result": "\u83b7\u5f97\u4e86\u9ad8\u8d28\u91cf\u76843D\u91cd\u5efa\u7ed3\u679c\uff0c\u53ef\u7528\u4e8e\u4e0b\u6e38\u7f16\u8f91\u4efb\u52a1\u5982\u91cd\u5149\u7167\u548c\u5f62\u72b6\u53d8\u5f62\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u51e0\u4f55\u548c\u5916\u89c2\u7684\u7edf\u4e00\u4f18\u5316\uff0c\u4e3a3D\u7f16\u8f91\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u57fa\u7840\u3002"}}
{"id": "2511.03962", "pdf": "https://arxiv.org/pdf/2511.03962", "abs": "https://arxiv.org/abs/2511.03962", "authors": ["Zhong Chen", "Changfeng Chen"], "title": "A Linear Fractional Transformation Model and Calibration Method for Light Field Camera", "categories": ["cs.CV"], "comment": null, "summary": "Accurate calibration of internal parameters is a crucial yet challenging\nprerequisite for 3D reconstruction using light field cameras. In this paper, we\npropose a linear fractional transformation(LFT) parameter $\\alpha$ to decoupled\nthe main lens and micro lens array (MLA). The proposed method includes an\nanalytical solution based on least squares, followed by nonlinear refinement.\nThe method for detecting features from the raw images is also introduced.\nExperimental results on both physical and simulated data have verified the\nperformance of proposed method. Based on proposed model, the simulation of raw\nlight field images becomes faster, which is crucial for data-driven deep\nlearning methods. The corresponding code can be obtained from the author's\nwebsite.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u5206\u6570\u53d8\u6362\u53c2\u6570\u03b1\u7684\u5149\u573a\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u4e3b\u955c\u5934\u548c\u5fae\u900f\u955c\u9635\u5217\uff0c\u5305\u542b\u6700\u5c0f\u4e8c\u4e58\u89e3\u6790\u89e3\u548c\u975e\u7ebf\u6027\u4f18\u5316\uff0c\u5e76\u4ecb\u7ecd\u4e86\u539f\u59cb\u56fe\u50cf\u7279\u5f81\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5149\u573a\u76f8\u673a\u5185\u90e8\u53c2\u6570\u7684\u7cbe\u786e\u5b9a\u6807\u662f3D\u91cd\u5efa\u7684\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u524d\u63d0\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u7ebf\u6027\u5206\u6570\u53d8\u6362\u53c2\u6570\u03b1\u89e3\u8026\u4e3b\u955c\u5934\u548c\u5fae\u900f\u955c\u9635\u5217\uff0c\u91c7\u7528\u57fa\u4e8e\u6700\u5c0f\u4e8c\u4e58\u7684\u89e3\u6790\u89e3\u548c\u975e\u7ebf\u6027\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u539f\u59cb\u56fe\u50cf\u7279\u5f81\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u5728\u7269\u7406\u548c\u4eff\u771f\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u57fa\u4e8e\u8be5\u6a21\u578b\u7684\u539f\u59cb\u5149\u573a\u56fe\u50cf\u4eff\u771f\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5feb\u7684\u4eff\u771f\u80fd\u529b\uff0c\u76f8\u5173\u4ee3\u7801\u53ef\u4ece\u4f5c\u8005\u7f51\u7ad9\u83b7\u53d6\u3002"}}
{"id": "2511.03970", "pdf": "https://arxiv.org/pdf/2511.03970", "abs": "https://arxiv.org/abs/2511.03970", "authors": ["Sam Bahrami", "Dylan Campbell"], "title": "Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images", "categories": ["cs.CV"], "comment": null, "summary": "Modern scene reconstruction methods are able to accurately recover 3D\nsurfaces that are visible in one or more images. However, this leads to\nincomplete reconstructions, missing all occluded surfaces. While much progress\nhas been made on reconstructing entire objects given partial observations using\ngenerative models, the structural elements of a scene, like the walls, floors\nand ceilings, have received less attention. We argue that these scene elements\nshould be relatively easy to predict, since they are typically planar,\nrepetitive and simple, and so less costly approaches may be suitable. In this\nwork, we present a synthetic dataset -- Room Envelopes -- that facilitates\nprogress on this task by providing a set of RGB images and two associated\npointmaps for each image: one capturing the visible surface and one capturing\nthe first surface once fittings and fixtures are removed, that is, the\nstructural layout. As we show, this enables direct supervision for feed-forward\nmonocular geometry estimators that predict both the first visible surface and\nthe first layout surface. This confers an understanding of the scene's extent,\nas well as the shape and location of its objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRoom Envelopes\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u5355\u76ee\u51e0\u4f55\u4f30\u8ba1\u5668\u9884\u6d4b\u53ef\u89c1\u8868\u9762\u548c\u7ed3\u6784\u5e03\u5c40\u8868\u9762\uff0c\u4ece\u800c\u7406\u89e3\u573a\u666f\u8303\u56f4\u548c\u7269\u4f53\u5f62\u72b6\u4f4d\u7f6e\u3002", "motivation": "\u73b0\u6709\u7684\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u53ea\u80fd\u6062\u590d\u53ef\u89c1\u8868\u9762\uff0c\u65e0\u6cd5\u91cd\u5efa\u88ab\u906e\u6321\u7684\u7ed3\u6784\u5143\u7d20\uff08\u5982\u5899\u58c1\u3001\u5730\u677f\u3001\u5929\u82b1\u677f\uff09\u3002\u8fd9\u4e9b\u7ed3\u6784\u5143\u7d20\u76f8\u5bf9\u5bb9\u6613\u9884\u6d4b\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u662f\u5e73\u9762\u3001\u91cd\u590d\u4e14\u7b80\u5355\u7684\u3002", "method": "\u521b\u5efa\u5408\u6210\u6570\u636e\u96c6Room Envelopes\uff0c\u63d0\u4f9bRGB\u56fe\u50cf\u548c\u4e24\u4e2a\u5173\u8054\u7684\u70b9\u56fe\uff1a\u4e00\u4e2a\u6355\u6349\u53ef\u89c1\u8868\u9762\uff0c\u53e6\u4e00\u4e2a\u6355\u6349\u79fb\u9664\u5bb6\u5177\u540e\u7684\u7ed3\u6784\u5e03\u5c40\u8868\u9762\u3002\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u8fdb\u884c\u524d\u9988\u5355\u76ee\u51e0\u4f55\u4f30\u8ba1\u5668\u7684\u76f4\u63a5\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u9884\u6d4b\u7b2c\u4e00\u53ef\u89c1\u8868\u9762\u548c\u7b2c\u4e00\u5e03\u5c40\u8868\u9762\uff0c\u4ece\u800c\u7406\u89e3\u573a\u666f\u7684\u5b8c\u6574\u8303\u56f4\u548c\u7269\u4f53\u7684\u5f62\u72b6\u4f4d\u7f6e\u3002", "conclusion": "\u901a\u8fc7Room Envelopes\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u8bad\u7ec3\u51e0\u4f55\u4f30\u8ba1\u5668\u7406\u89e3\u573a\u666f\u7684\u7ed3\u6784\u5e03\u5c40\uff0c\u586b\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u5efa\u88ab\u906e\u6321\u8868\u9762\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2511.03988", "pdf": "https://arxiv.org/pdf/2511.03988", "abs": "https://arxiv.org/abs/2511.03988", "authors": ["Wenshuo Qin", "Leyla Isik"], "title": "Simple 3D Pose Features Support Human and Machine Social Scene Understanding", "categories": ["cs.CV", "q-bio.NC"], "comment": "28 pages, 6 figures", "summary": "Humans can quickly and effortlessly extract a variety of information about\nothers' social interactions from visual input, ranging from visuospatial cues\nlike whether two people are facing each other to higher-level information. Yet,\nthe computations supporting these abilities remain poorly understood, and\nsocial interaction recognition continues to challenge even the most advanced AI\nvision systems. Here, we hypothesized that humans rely on 3D visuospatial pose\ninformation to make social interaction judgments, which is absent in most AI\nvision models. To test this, we combined state-of-the-art pose and depth\nestimation algorithms to extract 3D joint positions of people in short video\nclips depicting everyday human actions and compared their ability to predict\nhuman social interaction judgments with current AI vision models. Strikingly,\n3D joint positions outperformed most current AI vision models, revealing that\nkey social information is available in explicit body position but not in the\nlearned features of most vision models, including even the layer-wise\nembeddings of the pose models used to extract joint positions. To uncover the\ncritical pose features humans use to make social judgments, we derived a\ncompact set of 3D social pose features describing only the 3D position and\ndirection of faces in the videos. We found that these minimal descriptors\nmatched the predictive strength of the full set of 3D joints and significantly\nimproved the performance of off-the-shelf AI vision models when combined with\ntheir embeddings. Moreover, the degree to which 3D social pose features were\nrepresented in each off-the-shelf AI vision model predicted the model's ability\nto match human social judgments. Together, our findings provide strong evidence\nthat human social scene understanding relies on explicit representations of 3D\npose and can be supported by simple, structured visuospatial primitives.", "AI": {"tldr": "\u4eba\u7c7b\u4f9d\u8d563D\u59ff\u6001\u4fe1\u606f\u8fdb\u884c\u793e\u4ea4\u4e92\u52a8\u5224\u65ad\uff0c\u800c\u5927\u591a\u6570AI\u89c6\u89c9\u6a21\u578b\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b03D\u5173\u8282\u4f4d\u7f6e\u6bd4\u5f53\u524dAI\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u7b80\u5355\u76843D\u793e\u4ea4\u59ff\u6001\u7279\u5f81\u5c31\u80fd\u6709\u6548\u9884\u6d4b\u4eba\u7c7b\u793e\u4ea4\u5224\u65ad\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u5982\u4f55\u4ece\u89c6\u89c9\u8f93\u5165\u4e2d\u5feb\u901f\u63d0\u53d6\u793e\u4ea4\u4e92\u52a8\u4fe1\u606f\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u5148\u8fdb\u7684AI\u89c6\u89c9\u7cfb\u7edf\u5728\u8fd9\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6700\u5148\u8fdb\u7684\u59ff\u6001\u548c\u6df1\u5ea6\u4f30\u8ba1\u7b97\u6cd5\u63d0\u53d6\u89c6\u9891\u4e2d\u4eba\u7269\u76843D\u5173\u8282\u4f4d\u7f6e\uff0c\u5e76\u4e0eAI\u89c6\u89c9\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002\u63a8\u5bfc\u51fa\u4e00\u7ec4\u7d27\u51d1\u76843D\u793e\u4ea4\u59ff\u6001\u7279\u5f81\u6765\u63cf\u8ff0\u9762\u90e8\u4f4d\u7f6e\u548c\u65b9\u5411\u3002", "result": "3D\u5173\u8282\u4f4d\u7f6e\u8868\u73b0\u4f18\u4e8e\u5927\u591a\u6570AI\u89c6\u89c9\u6a21\u578b\u3002\u6700\u5c0f\u5316\u76843D\u793e\u4ea4\u59ff\u6001\u7279\u5f81\u4e0e\u5b8c\u6574\u5173\u8282\u96c6\u9884\u6d4b\u80fd\u529b\u76f8\u5f53\uff0c\u4e14\u80fd\u663e\u8457\u63d0\u5347\u73b0\u6210AI\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u4eba\u7c7b\u793e\u4ea4\u573a\u666f\u7406\u89e3\u4f9d\u8d56\u4e8e3D\u59ff\u6001\u7684\u663e\u5f0f\u8868\u5f81\uff0c\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u7ed3\u6784\u5316\u89c6\u89c9\u7a7a\u95f4\u539f\u8bed\u6765\u652f\u6301\u3002"}}
{"id": "2511.03992", "pdf": "https://arxiv.org/pdf/2511.03992", "abs": "https://arxiv.org/abs/2511.03992", "authors": ["Yuwen Tao", "Kanglei Zhou", "Xin Tan", "Yuan Xie"], "title": "CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret\nfree-form language expressions and localize the corresponding 3D regions in\nGaussian fields. While recent advances have introduced cross-modal alignment\nbetween language and 3D geometry, existing pipelines still struggle with\ncross-view consistency due to their reliance on 2D rendered pseudo supervision\nand view specific feature learning. In this work, we present Camera Aware\nReferring Field (CaRF), a fully differentiable framework that operates directly\nin the 3D Gaussian space and achieves multi view consistency. Specifically,\nCaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates\ncamera geometry into Gaussian text interactions to explicitly model view\ndependent variations and enhance geometric reasoning. Building on this, In\nTraining Paired View Supervision (ITPVS) is proposed to align per Gaussian\nlogits across calibrated views during training, effectively mitigating single\nview overfitting and exposing inter view discrepancies for optimization.\nExtensive experiments on three representative benchmarks demonstrate that CaRF\nachieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of\nthe art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.\nMoreover, this work promotes more reliable and view consistent 3D scene\nunderstanding, with potential benefits for embodied AI, AR/VR interaction, and\nautonomous perception.", "AI": {"tldr": "CaRF\u662f\u4e00\u4e2a\u5b8c\u5168\u53ef\u5fae\u5206\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u57283D\u9ad8\u65af\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u65af\u573a\u76f8\u673a\u7f16\u7801\u548c\u8bad\u7ec3\u914d\u5bf9\u89c6\u56fe\u76d1\u7763\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u76843D\u9ad8\u65af\u5206\u5272\u65b9\u6cd5\u4f9d\u8d562D\u6e32\u67d3\u4f2a\u76d1\u7763\u548c\u89c6\u56fe\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\uff0c\u5bfc\u81f4\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u4e0d\u8db3\u3002\u9700\u8981\u5f00\u53d1\u76f4\u63a5\u57283D\u9ad8\u65af\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u3001\u80fd\u591f\u5b9e\u73b0\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCaRF\u6846\u67b6\uff1a1\uff09\u9ad8\u65af\u573a\u76f8\u673a\u7f16\u7801\uff08GFCE\uff09\u5c06\u76f8\u673a\u51e0\u4f55\u878d\u5165\u9ad8\u65af\u6587\u672c\u4ea4\u4e92\uff0c\u663e\u5f0f\u5efa\u6a21\u89c6\u56fe\u4f9d\u8d56\u53d8\u5316\uff1b2\uff09\u8bad\u7ec3\u914d\u5bf9\u89c6\u56fe\u76d1\u7763\uff08ITPVS\uff09\u5728\u6821\u51c6\u89c6\u56fe\u95f4\u5bf9\u9f50\u9ad8\u65af\u5bf9\u6570\u6982\u7387\uff0c\u7f13\u89e3\u5355\u89c6\u56fe\u8fc7\u62df\u5408\u3002", "result": "\u5728Ref LERF\u3001LERF OVS\u548c3D OVS\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cmIoU\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u63d0\u534716.8%\u30014.3%\u548c2.0%\u3002", "conclusion": "CaRF\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u548c\u89c6\u56fe\u4e00\u81f4\u76843D\u573a\u666f\u7406\u89e3\uff0c\u5bf9\u5177\u8eabAI\u3001AR/VR\u4ea4\u4e92\u548c\u81ea\u4e3b\u611f\u77e5\u5177\u6709\u6f5c\u5728\u76ca\u5904\u3002"}}
{"id": "2511.03997", "pdf": "https://arxiv.org/pdf/2511.03997", "abs": "https://arxiv.org/abs/2511.03997", "authors": ["Peiyao Wang", "Weining Wang", "Qi Li"], "title": "PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-video generation have achieved impressive\nperceptual quality, yet generated content often violates fundamental principles\nof physical plausibility - manifesting as implausible object dynamics,\nincoherent interactions, and unrealistic motion patterns. Such failures hinder\nthe deployment of video generation models in embodied AI, robotics, and\nsimulation-intensive domains. To bridge this gap, we propose PhysCorr, a\nunified framework for modeling, evaluating, and optimizing physical consistency\nin video generation. Specifically, we introduce PhysicsRM, the first\ndual-dimensional reward model that quantifies both intra-object stability and\ninter-object interactions. On this foundation, we develop PhyDPO, a novel\ndirect preference optimization pipeline that leverages contrastive feedback and\nphysics-aware reweighting to guide generation toward physically coherent\noutputs. Our approach is model-agnostic and scalable, enabling seamless\nintegration into a wide range of video diffusion and transformer-based\nbackbones. Extensive experiments across multiple benchmarks demonstrate that\nPhysCorr achieves significant improvements in physical realism while preserving\nvisual fidelity and semantic alignment. This work takes a critical step toward\nphysically grounded and trustworthy video generation.", "AI": {"tldr": "\u63d0\u51faPhysCorr\u6846\u67b6\uff0c\u901a\u8fc7PhysicsRM\u5956\u52b1\u6a21\u578b\u548cPhyDPO\u4f18\u5316\u65b9\u6cd5\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7269\u7406\u5408\u7406\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5e38\u8fdd\u53cd\u7269\u7406\u5408\u7406\u6027\u539f\u5219\uff0c\u8868\u73b0\u4e3a\u4e0d\u5408\u7406\u7684\u7269\u4f53\u52a8\u6001\u3001\u4e0d\u8fde\u8d2f\u7684\u4ea4\u4e92\u548c\u4e0d\u771f\u5b9e\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5728\u5177\u8eabAI\u3001\u673a\u5668\u4eba\u548c\u4eff\u771f\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faPhysCorr\u7edf\u4e00\u6846\u67b6\uff1a1) PhysicsRM - \u9996\u4e2a\u53cc\u7ef4\u5ea6\u5956\u52b1\u6a21\u578b\uff0c\u91cf\u5316\u7269\u4f53\u5185\u90e8\u7a33\u5b9a\u6027\u548c\u7269\u4f53\u95f4\u4ea4\u4e92\uff1b2) PhyDPO - \u57fa\u4e8e\u5bf9\u6bd4\u53cd\u9988\u548c\u7269\u7406\u611f\u77e5\u91cd\u52a0\u6743\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7ba1\u9053\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPhysCorr\u5728\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u5bf9\u9f50\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7269\u7406\u771f\u5b9e\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5411\u7269\u7406\u57fa\u7840\u548c\u53ef\u4fe1\u7684\u89c6\u9891\u751f\u6210\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u96c6\u6210\u5230\u5404\u79cd\u89c6\u9891\u6269\u6563\u548c\u57fa\u4e8etransformer\u7684\u9aa8\u5e72\u7f51\u7edc\u4e2d\u3002"}}
{"id": "2511.04008", "pdf": "https://arxiv.org/pdf/2511.04008", "abs": "https://arxiv.org/abs/2511.04008", "authors": ["Mahmoud Soliman", "Omar Abdelaziz", "Ahmed Radwan", "Anand", "Mohamed Shehata"], "title": "GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization", "categories": ["cs.CV"], "comment": "6 pages, 3 figures", "summary": "Domain generalization (DG) seeks robust Vision Transformer (ViT) performance\non unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;\nstandard fine-tuning is costly and can impair generalization. We propose\nGNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a\nMixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead\nof token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,\nSAGE) operates on inter-patch graphs to dynamically assign patches to\nspecialized experts. This context-aware GNN routing leverages inter-patch\nrelationships for better adaptation to domain shifts. GNN-MoE achieves\nstate-of-the-art or competitive DG benchmark performance with high parameter\nefficiency, highlighting the utility of graph-based contextual routing for\nrobust, lightweight DG.", "AI": {"tldr": "\u63d0\u51fa\u4e86GNN-MoE\u65b9\u6cd5\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8def\u7531\u5668\u548cKronecker\u9002\u914d\u5668\u589e\u5f3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u7528\u4e8e\u89c6\u89c9Transformer\u7684\u9886\u57df\u6cdb\u5316", "motivation": "\u6807\u51c6\u5fae\u8c03\u65b9\u6cd5\u5728\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u6210\u672c\u9ad8\u6602\u4e14\u53ef\u80fd\u635f\u5bb3\u6cdb\u5316\u6027\u80fd\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9002\u914d\u65b9\u6cd5", "method": "\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GCN\u3001GAT\u3001SAGE\uff09\u5728\u8865\u4e01\u95f4\u56fe\u4e0a\u8fdb\u884c\u52a8\u6001\u8def\u7531\uff0c\u5c06\u8865\u4e01\u5206\u914d\u7ed9\u4e13\u95e8\u4e13\u5bb6\uff0c\u7ed3\u5408\u9ad8\u6548\u7684Kronecker\u9002\u914d\u5668", "result": "\u5728\u9886\u57df\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6216\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53c2\u6570\u6548\u7387", "conclusion": "\u57fa\u4e8e\u56fe\u7684\u4e0a\u4e0b\u6587\u8def\u7531\u5bf9\u4e8e\u5b9e\u73b0\u9c81\u68d2\u3001\u8f7b\u91cf\u7ea7\u7684\u9886\u57df\u6cdb\u5316\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2511.04016", "pdf": "https://arxiv.org/pdf/2511.04016", "abs": "https://arxiv.org/abs/2511.04016", "authors": ["Mahmoud Soliman", "Islam Osman", "Mohamed S. Shehata", "Rasika Rajapakshe"], "title": "MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging", "categories": ["cs.CV"], "comment": "10 pages, 2 figures", "summary": "The performance of vision models in medical imaging is often hindered by the\nprevailing paradigm of fine-tuning backbones pre-trained on out-of-domain\nnatural images. To address this fundamental domain gap, we propose MedDChest, a\nnew foundational Vision Transformer (ViT) model optimized specifically for\nthoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,\nmultimodal dataset of over 1.2 million images, encompassing different\nmodalities including Chest X-ray and Computed Tomography (CT) compiled from 10\npublic sources. A core technical contribution of our work is Guided Random\nResized Crops, a novel content-aware data augmentation strategy that biases\nsampling towards anatomically relevant regions, overcoming the inefficiency of\nstandard cropping techniques on medical scans. We validate our model's\neffectiveness by fine-tuning it on a diverse set of downstream diagnostic\ntasks. Comprehensive experiments empirically demonstrate that MedDChest\nsignificantly outperforms strong, publicly available ImageNet-pretrained\nmodels. By establishing the superiority of large-scale, in-domain pre-training\ncombined with domain-specific data augmentation, MedDChest provides a powerful\nand robust feature extractor that serves as a significantly better starting\npoint for a wide array of thoracic diagnostic tasks. The model weights will be\nmade publicly available to foster future research and applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedDChest\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u80f8\u90e8\u5f71\u50cf\u4f18\u5316\u7684\u57fa\u7840\u89c6\u89c9Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u5728120\u4e07\u5f20\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u4e0a\u4ece\u5934\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80f8\u90e8\u8bca\u65ad\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u56e0\u4f7f\u7528\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\u800c\u5b58\u5728\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u63d0\u5347\u80f8\u90e8\u5f71\u50cf\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u8d85\u8fc7120\u4e07\u5f20\u591a\u6a21\u6001\u80f8\u90e8\u5f71\u50cf\uff08\u5305\u62ecX\u5149\u548cCT\uff09\u4ece\u5934\u9884\u8bad\u7ec3ViT\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u5f15\u5bfc\u968f\u673a\u8c03\u6574\u88c1\u526a\uff08Guided Random Resized Crops\uff09\u8fd9\u4e00\u5185\u5bb9\u611f\u77e5\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "MedDChest\u5728\u591a\u79cd\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eImageNet\u9884\u8bad\u7ec3\u7684\u516c\u5f00\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5927\u89c4\u6a21\u9886\u57df\u5185\u9884\u8bad\u7ec3\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u6570\u636e\u589e\u5f3a\u7684\u6709\u6548\u6027\u3002", "conclusion": "MedDChest\u4e3a\u80f8\u90e8\u8bca\u65ad\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u9c81\u68d2\u7684\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u6a21\u578b\u6743\u91cd\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2511.04029", "pdf": "https://arxiv.org/pdf/2511.04029", "abs": "https://arxiv.org/abs/2511.04029", "authors": ["Yihao Luo", "Xianglong He", "Chuanyu Pan", "Yiwen Chen", "Jiaqi Wu", "Yangguang Li", "Wanli Ouyang", "Yuanming Hu", "Guang Yang", "ChoonHwai Yap"], "title": "Near-Lossless 3D Voxel Representation Free from Iso-surface", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Accurate and efficient voxelized representations of 3D meshes are the\nfoundation of 3D reconstruction and generation. However, existing\nrepresentations based on iso-surface heavily rely on water-tightening or\nrendering optimization, which inevitably compromise geometric fidelity. We\npropose Faithful Contouring, a sparse voxelized representation that supports\n2048+ resolutions for arbitrary meshes, requiring neither converting meshes to\nfield functions nor extracting the isosurface during remeshing. It achieves\nnear-lossless fidelity by preserving sharpness and internal structures, even\nfor challenging cases with complex geometry and topology. The proposed method\nalso shows flexibility for texturing, manipulation, and editing. Beyond\nrepresentation, we design a dual-mode autoencoder for Faithful Contouring,\nenabling scalable and detail-preserving shape reconstruction. Extensive\nexperiments show that Faithful Contouring surpasses existing methods in\naccuracy and efficiency for both representation and reconstruction. For direct\nrepresentation, it achieves distance errors at the $10^{-5}$ level; for mesh\nreconstruction, it yields a 93\\% reduction in Chamfer Distance and a 35\\%\nimprovement in F-score over strong baselines, confirming superior fidelity as a\nrepresentation for 3D learning tasks.", "AI": {"tldr": "Faithful Contouring\u662f\u4e00\u79cd\u7a00\u758f\u4f53\u7d20\u5316\u8868\u793a\u65b9\u6cd5\uff0c\u652f\u63012048+\u5206\u8fa8\u7387\uff0c\u65e0\u9700\u5c06\u7f51\u683c\u8f6c\u6362\u4e3a\u573a\u51fd\u6570\u6216\u63d0\u53d6\u7b49\u503c\u9762\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u65e0\u635f\u7684\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7b49\u503c\u9762\u7684\u4f53\u7d20\u5316\u8868\u793a\u4e25\u91cd\u4f9d\u8d56\u6c34\u5bc6\u5316\u6216\u6e32\u67d3\u4f18\u5316\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u635f\u5bb3\u4e86\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faFaithful Contouring\u7a00\u758f\u4f53\u7d20\u5316\u8868\u793a\uff0c\u8bbe\u8ba1\u53cc\u6a21\u5f0f\u81ea\u7f16\u7801\u5668\uff0c\u652f\u6301\u53ef\u6269\u5c55\u4e14\u7ec6\u8282\u4fdd\u6301\u7684\u5f62\u72b6\u91cd\u5efa\u3002", "result": "\u5728\u76f4\u63a5\u8868\u793a\u65b9\u9762\u8fbe\u523010^-5\u7ea7\u522b\u7684\u8ddd\u79bb\u8bef\u5dee\uff1b\u5728\u7f51\u683c\u91cd\u5efa\u65b9\u9762\uff0cChamfer Distance\u51cf\u5c1193%\uff0cF-score\u63d0\u9ad835%\u3002", "conclusion": "Faithful Contouring\u57283D\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f5c\u4e3a\u8868\u793a\u65b9\u6cd5\u5177\u6709\u5353\u8d8a\u7684\u4fdd\u771f\u5ea6\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.04037", "pdf": "https://arxiv.org/pdf/2511.04037", "abs": "https://arxiv.org/abs/2511.04037", "authors": ["Arfina Rahman", "Mahesh Banavar"], "title": "A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals", "categories": ["cs.CV", "eess.SP"], "comment": "This work has been submitted to IEEE Transactions on Biometrics,\n  Behavior, and Identity Science (TBIOM) for possible publication", "summary": "Photoplethysmography (PPG) signals, which measure changes in blood volume in\nthe skin using light, have recently gained attention in biometric\nauthentication because of their non-invasive acquisition, inherent liveness\ndetection, and suitability for low-cost wearable devices. However, PPG signal\nquality is challenged by motion artifacts, illumination changes, and\ninter-subject physiological variability, making robust feature extraction and\nclassification crucial. This study proposes a lightweight and cost-effective\nbiometric authentication framework based on PPG signals extracted from\nlow-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings\nfrom 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The\nraw PPG signals undergo a standard preprocessing pipeline involving baseline\ndrift removal, motion artifact suppression using Principal Component Analysis\n(PCA), bandpass filtering, Fourier-based resampling, and amplitude\nnormalization. To generate robust representations, each one-dimensional PPG\nsegment is converted into a two-dimensional time-frequency scalogram via the\nContinuous Wavelet Transform (CWT), effectively capturing transient\ncardiovascular dynamics. We developed a hybrid deep learning model, termed\nCVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision\nTransformer (CVT) and ConvMixer branches with temporal features from a Long\nShort-Term Memory network (LSTM). The experimental results on 46 subjects\ndemonstrate an authentication accuracy of 98%, validating the robustness of the\nmodel to noise and variability between subjects. Due to its efficiency,\nscalability, and inherent liveness detection capability, the proposed system is\nwell-suited for real-world mobile and embedded biometric security applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePPG\u4fe1\u53f7\u7684\u8f7b\u91cf\u7ea7\u751f\u7269\u8ba4\u8bc1\u6846\u67b6\uff0c\u4f7f\u7528\u4f4e\u5e27\u7387\u6307\u5c16\u89c6\u9891\u63d0\u53d6PPG\u4fe1\u53f7\uff0c\u901a\u8fc7\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b098%\u7684\u8ba4\u8bc1\u51c6\u786e\u7387\u3002", "motivation": "PPG\u4fe1\u53f7\u56e0\u5176\u975e\u4fb5\u5165\u6027\u91c7\u96c6\u3001\u56fa\u6709\u7684\u6d3b\u6027\u68c0\u6d4b\u80fd\u529b\u548c\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u53ef\u7a7f\u6234\u8bbe\u5907\u800c\u53d7\u5230\u751f\u7269\u8ba4\u8bc1\u9886\u57df\u7684\u5173\u6ce8\uff0c\u4f46\u4fe1\u53f7\u8d28\u91cf\u53d7\u8fd0\u52a8\u4f2a\u5f71\u3001\u5149\u7167\u53d8\u5316\u548c\u4e2a\u4f53\u751f\u7406\u5dee\u5f02\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u6807\u51c6\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u57fa\u7ebf\u6f02\u79fb\u53bb\u9664\u3001PCA\u8fd0\u52a8\u4f2a\u5f71\u6291\u5236\u3001\u5e26\u901a\u6ee4\u6ce2\u3001\u5085\u91cc\u53f6\u91cd\u91c7\u6837\u548c\u5e45\u5ea6\u5f52\u4e00\u5316\u3002\u5c06\u4e00\u7ef4PPG\u4fe1\u53f7\u901a\u8fc7\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u8f6c\u6362\u4e3a\u65f6\u9891\u6807\u91cf\u56fe\uff0c\u5e76\u5f00\u53d1\u4e86\u7ed3\u5408CVT\u3001ConvMixer\u548cLSTM\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u572846\u540d\u53d7\u8bd5\u8005\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8ba4\u8bc1\u51c6\u786e\u7387\u8fbe\u523098%\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5bf9\u566a\u58f0\u548c\u4e2a\u4f53\u95f4\u53d8\u5f02\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5177\u6709\u9ad8\u6548\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u56fa\u6709\u7684\u6d3b\u6027\u68c0\u6d4b\u80fd\u529b\uff0c\u975e\u5e38\u9002\u5408\u73b0\u5b9e\u4e16\u754c\u7684\u79fb\u52a8\u548c\u5d4c\u5165\u5f0f\u751f\u7269\u5b89\u5168\u5e94\u7528\u3002"}}
