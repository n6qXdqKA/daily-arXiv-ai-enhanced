{"id": "2505.00733", "pdf": "https://arxiv.org/pdf/2505.00733", "abs": "https://arxiv.org/abs/2505.00733", "authors": ["Gustavo Rezende Silva", "Juliane P\u00e4\u00dfler", "S. Lizeth Tapia Tarifa", "Einar Broch Johnsen", "Carlos Hern\u00e1ndez Corbato"], "title": "ROSA: A Knowledge-based Solution for Robot Self-Adaptation", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Autonomous robots must operate in diverse environments and handle multiple\ntasks despite uncertainties. This creates challenges in designing software\narchitectures and task decision-making algorithms, as different contexts may\nrequire distinct task logic and architectural configurations. To address this,\nrobotic systems can be designed as self-adaptive systems capable of adapting\ntheir task execution and software architecture at runtime based on their\ncontext.This paper introduces ROSA, a novel knowledge-based framework for RObot\nSelf-Adaptation, which enables task-and-architecture co-adaptation (TACA) in\nrobotic systems. ROSA achieves this by providing a knowledge model that\ncaptures all application-specific knowledge required for adaptation and by\nreasoning over this knowledge at runtime to determine when and how adaptation\nshould occur. In addition to a conceptual framework, this work provides an\nopen-source ROS 2-based reference implementation of ROSA and evaluates its\nfeasibility and performance in an underwater robotics application. Experimental\nresults highlight ROSA's advantages in reusability and development effort for\ndesigning self-adaptive robotic systems.", "AI": {"tldr": "ROSA\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u7684\u673a\u5668\u4eba\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u652f\u6301\u4efb\u52a1\u4e0e\u67b6\u6784\u534f\u540c\u9002\u5e94\uff08TACA\uff09\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u63a8\u7406\u5b9e\u73b0\u52a8\u6001\u8c03\u6574\uff0c\u5e76\u5728\u6c34\u4e0b\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u9700\u5728\u591a\u6837\u73af\u5883\u4e2d\u5904\u7406\u591a\u4efb\u52a1\uff0c\u4f46\u4e0d\u540c\u60c5\u5883\u9700\u8981\u4e0d\u540c\u7684\u4efb\u52a1\u903b\u8f91\u548c\u67b6\u6784\u914d\u7f6e\uff0c\u8bbe\u8ba1\u81ea\u9002\u5e94\u7cfb\u7edf\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faROSA\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u6a21\u578b\u6355\u83b7\u5e94\u7528\u7279\u5b9a\u77e5\u8bc6\uff0c\u5e76\u5728\u8fd0\u884c\u65f6\u63a8\u7406\u51b3\u5b9a\u9002\u5e94\u65f6\u673a\u548c\u65b9\u5f0f\uff0c\u63d0\u4f9bROS 2\u5f00\u6e90\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eROSA\u5728\u53ef\u91cd\u7528\u6027\u548c\u5f00\u53d1\u6548\u7387\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u81ea\u9002\u5e94\u673a\u5668\u4eba\u7cfb\u7edf\u8bbe\u8ba1\u3002", "conclusion": "ROSA\u4e3a\u673a\u5668\u4eba\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u4efb\u52a1\u6267\u884c\u3002"}}
{"id": "2505.00795", "pdf": "https://arxiv.org/pdf/2505.00795", "abs": "https://arxiv.org/abs/2505.00795", "authors": ["Dibyangshu Mukherjee", "Shivaram Kalyanakrishnan"], "title": "Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor", "categories": ["cs.AI"], "comment": null, "summary": "Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov\nDecision Problems (MDPs). HPI uses a \"greedy\" switching rule to update from any\nnon-optimal policy to a dominating one, iterating until an optimal policy is\nfound. Despite its introduction over 60 years ago, the best-known upper bounds\non HPI's running time remain exponential in the number of states -- indeed even\non the restricted class of MDPs with only deterministic transitions (DMDPs).\nMeanwhile, the tightest lower bound for HPI for MDPs with a constant number of\nactions per state is only linear. In this paper, we report a significant\nimprovement: a subexponential upper bound for HPI on DMDPs, which is\nparameterised by the bit-size of the rewards, while independent of the discount\nfactor. The same upper bound also applies to DMDPs with only two possible\nrewards (which may be of arbitrary size).", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86Howard\u7b56\u7565\u8fed\u4ee3\uff08HPI\uff09\u5728\u786e\u5b9a\u6027MDP\uff08DMDP\uff09\u4e0a\u7684\u8fd0\u884c\u65f6\u95f4\u4e0a\u754c\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6b21\u6307\u6570\u7ea7\u7684\u4e0a\u754c\uff0c\u4e14\u4e0e\u6298\u6263\u56e0\u5b50\u65e0\u5173\u3002", "motivation": "\u5c3d\u7ba1HPI\u7b97\u6cd5\u5df2\u670960\u591a\u5e74\u5386\u53f2\uff0c\u4f46\u5176\u5728\u786e\u5b9a\u6027MDP\u4e0a\u7684\u8fd0\u884c\u65f6\u95f4\u4e0a\u754c\u4ecd\u4e3a\u6307\u6570\u7ea7\uff0c\u800c\u73b0\u6709\u4e0b\u754c\u4ec5\u4e3a\u7ebf\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u5206\u6790HPI\u5728DMDP\u4e0a\u7684\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5956\u52b1\u7684\u6bd4\u7279\u5927\u5c0f\u800c\u975e\u6298\u6263\u56e0\u5b50\u3002", "result": "\u8bc1\u660e\u4e86HPI\u5728DMDP\u4e0a\u7684\u8fd0\u884c\u65f6\u95f4\u4e0a\u754c\u4e3a\u6b21\u6307\u6570\u7ea7\uff0c\u4e14\u9002\u7528\u4e8e\u4ec5\u542b\u4e24\u79cd\u5956\u52b1\u7684DMDP\u3002", "conclusion": "\u672c\u6587\u663e\u8457\u6539\u8fdb\u4e86HPI\u5728DMDP\u4e0a\u7684\u7406\u8bba\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2505.00802", "pdf": "https://arxiv.org/pdf/2505.00802", "abs": "https://arxiv.org/abs/2505.00802", "authors": ["Vasiliki Papanikou", "Danae Pla Karidi", "Evaggelia Pitoura", "Emmanouil Panagiotou", "Eirini Ntoutsi"], "title": "Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As Artificial Intelligence (AI) is increasingly used in areas that\nsignificantly impact human lives, concerns about fairness and transparency have\ngrown, especially regarding their impact on protected groups. Recently, the\nintersection of explainability and fairness has emerged as an important area to\npromote responsible AI systems. This paper explores how explainability methods\ncan be leveraged to detect and interpret unfairness. We propose a pipeline that\nintegrates local post-hoc explanation methods to derive fairness-related\ninsights. During the pipeline design, we identify and address critical\nquestions arising from the use of explanations as bias detectors such as the\nrelationship between distributive and procedural fairness, the effect of\nremoving the protected attribute, the consistency and quality of results across\ndifferent explanation methods, the impact of various aggregation strategies of\nlocal explanations on group fairness evaluations, and the overall\ntrustworthiness of explanations as bias detectors. Our results show the\npotential of explanation methods used for fairness while highlighting the need\nto carefully consider the aforementioned critical aspects.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u89e3\u91ca\u6027\u65b9\u6cd5\u5982\u4f55\u7528\u4e8e\u68c0\u6d4b\u548c\u89e3\u91caAI\u7cfb\u7edf\u4e2d\u7684\u4e0d\u516c\u5e73\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c40\u90e8\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\u7684\u6d41\u7a0b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u5728\u5f71\u54cd\u4eba\u7c7b\u751f\u6d3b\u7684\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u516c\u5e73\u6027\u548c\u900f\u660e\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u5bf9\u53d7\u4fdd\u62a4\u7fa4\u4f53\u7684\u5f71\u54cd\u3002\u89e3\u91ca\u6027\u4e0e\u516c\u5e73\u6027\u7684\u7ed3\u5408\u6210\u4e3a\u63a8\u52a8\u8d1f\u8d23\u4efbAI\u7cfb\u7edf\u7684\u91cd\u8981\u65b9\u5411\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u7a0b\uff0c\u6574\u5408\u5c40\u90e8\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\uff0c\u4ee5\u83b7\u53d6\u4e0e\u516c\u5e73\u6027\u76f8\u5173\u7684\u89c1\u89e3\uff0c\u5e76\u89e3\u51b3\u4e86\u4f7f\u7528\u89e3\u91ca\u4f5c\u4e3a\u504f\u89c1\u68c0\u6d4b\u5668\u65f6\u7684\u5173\u952e\u95ee\u9898\u3002", "result": "\u7ed3\u679c\u663e\u793a\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u516c\u5e73\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4f46\u4e5f\u5f3a\u8c03\u4e86\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u7684\u5173\u952e\u95ee\u9898\u3002", "conclusion": "\u89e3\u91ca\u6027\u65b9\u6cd5\u53ef\u7528\u4e8e\u516c\u5e73\u6027\u68c0\u6d4b\uff0c\u4f46\u9700\u6ce8\u610f\u5176\u5c40\u9650\u6027\u53ca\u5173\u952e\u95ee\u9898\u7684\u5904\u7406\u3002"}}
{"id": "2505.00827", "pdf": "https://arxiv.org/pdf/2505.00827", "abs": "https://arxiv.org/abs/2505.00827", "authors": ["Jing Wang", "Xing Niu", "Juyong Kim", "Jie Shen", "Tong Zhang", "Jeremy C. Weiss"], "title": "MIMIC-\\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Clinical risk prediction based on machine learning algorithms plays a vital\nrole in modern healthcare. A crucial component in developing a reliable\nprediction model is collecting high-quality time series clinical events. In\nthis work, we release such a dataset that consists of 22,588,586 Clinical Time\nSeries events, which we term MIMIC-\\RNum{4}-Ext-22MCTS. Our source data are\ndischarge summaries selected from the well-known yet unstructured MIMIC-IV-Note\n\\cite{Johnson2023-pg}. We then extract clinical events as short text span from\nthe discharge summaries, along with the timestamps of these events as temporal\ninformation. The general-purpose MIMIC-IV-Note pose specific challenges for our\nwork: it turns out that the discharge summaries are too lengthy for typical\nnatural language models to process, and the clinical events of interest often\nare not accompanied with explicit timestamps. Therefore, we propose a new\nframework that works as follows: 1) we break each discharge summary into\nmanageably small text chunks; 2) we apply contextual BM25 and contextual\nsemantic search to retrieve chunks that have a high potential of containing\nclinical events; and 3) we carefully design prompts to teach the recently\nreleased Llama-3.1-8B \\cite{touvron2023llama} model to identify or infer\ntemporal information of the chunks. We show that the obtained dataset is so\ninformative and transparent that standard models fine-tuned on our dataset are\nachieving significant improvements in healthcare applications. In particular,\nthe BERT model fine-tuned based on our dataset achieves 10\\% improvement in\naccuracy on medical question answering task, and 3\\% improvement in clinical\ntrial matching task compared with the classic BERT. The GPT-2 model, fine-tuned\non our dataset, produces more clinically reliable results for clinical\nquestions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eceMIMIC-IV-Note\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u4e34\u5e8a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86MIMIC-4-Ext-22MCTS\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5206\u5757\u5904\u7406\u3001\u68c0\u7d22\u548c\u63d0\u793a\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3\u533b\u7597\u4e2d\uff0c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684MIMIC-IV-Note\u6570\u636e\u5b58\u5728\u6587\u672c\u8fc7\u957f\u548c\u65f6\u95f4\u4fe1\u606f\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "1) \u5c06\u51fa\u9662\u6458\u8981\u5206\u5757\uff1b2) \u4f7f\u7528\u4e0a\u4e0b\u6587BM25\u548c\u8bed\u4e49\u641c\u7d22\u68c0\u7d22\u542b\u4e34\u5e8a\u4e8b\u4ef6\u7684\u5757\uff1b3) \u8bbe\u8ba1\u63d0\u793a\u8ba9Llama-3.1-8B\u6a21\u578b\u8bc6\u522b\u6216\u63a8\u65ad\u65f6\u95f4\u4fe1\u606f\u3002", "result": "\u6784\u5efa\u7684\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff1aBERT\u5728\u533b\u7597\u95ee\u7b54\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u63d0\u534710%\uff0c\u4e34\u5e8a\u8bd5\u9a8c\u5339\u914d\u4efb\u52a1\u63d0\u53473%\uff1bGPT-2\u751f\u6210\u7684\u7ed3\u679c\u66f4\u5177\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u533b\u7597AI\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2505.00734", "pdf": "https://arxiv.org/pdf/2505.00734", "abs": "https://arxiv.org/abs/2505.00734", "authors": ["Neil Joshi", "Joshua Carney", "Nathanael Kuo", "Homer Li", "Cheng Peng", "Myron Brown"], "title": "Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Production of photorealistic, navigable 3D site models requires a large\nvolume of carefully collected images that are often unavailable to first\nresponders for disaster relief or law enforcement. Real-world challenges\ninclude limited numbers of images, heterogeneous unposed cameras, inconsistent\nlighting, and extreme viewpoint differences for images collected from varying\naltitudes. To promote research aimed at addressing these challenges, we have\ndeveloped the first public benchmark dataset for 3D reconstruction and novel\nview synthesis based on multiple calibrated ground-level, security-level, and\nairborne cameras. We present datasets that pose real-world challenges,\nindependently evaluate calibration of unposed cameras and quality of novel\nrendered views, demonstrate baseline performance using recent state-of-practice\nmethods, and identify challenges for further research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u89c6\u89d23D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u7684\u5b9e\u9645\u6311\u6218\uff0c\u5982\u56fe\u50cf\u6570\u91cf\u6709\u9650\u3001\u76f8\u673a\u672a\u6807\u5b9a\u3001\u5149\u7167\u4e0d\u4e00\u81f4\u548c\u6781\u7aef\u89c6\u89d2\u5dee\u5f02\u3002", "motivation": "\u4e3a\u707e\u96be\u6551\u63f4\u6216\u6267\u6cd5\u7b49\u573a\u666f\u63d0\u4f9b\u903c\u771f\u3001\u53ef\u5bfc\u822a\u76843D\u573a\u666f\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u56fe\u50cf\u6570\u636e\u4e0d\u8db3\u4e14\u8d28\u91cf\u4e0d\u5747\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u591a\u7c7b\u578b\u76f8\u673a\uff08\u5730\u9762\u3001\u5b89\u9632\u3001\u7a7a\u4e2d\uff09\u7684\u6807\u5b9a\u6570\u636e\u96c6\uff0c\u72ec\u7acb\u8bc4\u4f30\u672a\u6807\u5b9a\u76f8\u673a\u6821\u51c6\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u8d28\u91cf\u3002", "result": "\u5c55\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u57fa\u7ebf\u6027\u80fd\uff0c\u5e76\u6307\u51fa\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a3D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u9645\u6311\u6218\u7684\u57fa\u51c6\uff0c\u63a8\u52a8\u76f8\u5173\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2505.00875", "pdf": "https://arxiv.org/pdf/2505.00875", "abs": "https://arxiv.org/abs/2505.00875", "authors": ["Ramesh Manuvinakurike", "Emanuel Moss", "Elizabeth Anne Watkins", "Saurav Sahay", "Giuseppe Raffa", "Lama Nachman"], "title": "Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines", "categories": ["cs.AI"], "comment": null, "summary": "Agentic pipelines present novel challenges and opportunities for\nhuman-centered explainability. The HCXAI community is still grappling with how\nbest to make the inner workings of LLMs transparent in actionable ways. Agentic\npipelines consist of multiple LLMs working in cooperation with minimal human\ncontrol. In this research paper, we present early findings from an agentic\npipeline implementation of a perceptive task guidance system. Through\nquantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)\nreasoning, a common vehicle for explainability in LLMs, operates within agentic\npipelines. We demonstrate that CoT reasoning alone does not lead to better\noutputs, nor does it offer explainability, as it tends to produce explanations\nwithout explainability, in that they do not improve the ability of end users to\nbetter understand systems or achieve their goals.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u4ee3\u7406\u7ba1\u9053\u4e2d\uff0cChain-of-Thought\uff08CoT\uff09\u63a8\u7406\u5e76\u4e0d\u80fd\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\u6216\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6027\u89e3\u91ca\u3002", "motivation": "\u63a2\u8ba8\u4ee3\u7406\u7ba1\u9053\u4e2dLLMs\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u662fCoT\u63a8\u7406\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\uff0c\u7814\u7a76\u4ee3\u7406\u7ba1\u9053\u4e2dCoT\u63a8\u7406\u7684\u8868\u73b0\u3002", "result": "CoT\u63a8\u7406\u672a\u80fd\u6539\u5584\u8f93\u51fa\u8d28\u91cf\u6216\u63d0\u4f9b\u6709\u6548\u89e3\u91ca\uff0c\u5176\u751f\u6210\u7684\u89e3\u91ca\u7f3a\u4e4f\u5b9e\u7528\u6027\u3002", "conclusion": "CoT\u63a8\u7406\u5728\u4ee3\u7406\u7ba1\u9053\u4e2d\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u4ed6\u65b9\u6cd5\u3002"}}
{"id": "2505.00739", "pdf": "https://arxiv.org/pdf/2505.00739", "abs": "https://arxiv.org/abs/2505.00739", "authors": ["Qiushi Yang", "Yuan Yao", "Miaomiao Cui", "Liefeng Bo"], "title": "MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional\ncapabilities in interactive object segmentation for both images and videos.\nHowever, as a foundational model on interactive segmentation, SAM2 performs\nsegmentation directly based on mask memory from the past six frames, leading to\ntwo significant challenges. Firstly, during inference in videos, objects may\ndisappear since SAM2 relies solely on memory without accounting for object\nmotion information, which limits its long-range object tracking capabilities.\nSecondly, its memory is constructed from fixed past frames, making it\nsusceptible to challenges associated with object disappearance or occlusion,\ndue to potentially inaccurate segmentation results in memory. To address these\nproblems, we present MoSAM, incorporating two key strategies to integrate\nobject motion cues into the model and establish more reliable feature memory.\nFirstly, we propose Motion-Guided Prompting (MGP), which represents the object\nmotion in both sparse and dense manners, then injects them into SAM2 through a\nset of motion-guided prompts. MGP enables the model to adjust its focus towards\nthe direction of motion, thereby enhancing the object tracking capabilities.\nFurthermore, acknowledging that past segmentation results may be inaccurate, we\ndevise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically\nidentifies frames likely to contain accurate segmentation in both pixel- and\nframe-level. By eliminating potentially inaccurate mask predictions from\nmemory, we can leverage more reliable memory features to exploit similar\nregions for improving segmentation results. Extensive experiments on various\nbenchmarks of video object segmentation and video instance segmentation\ndemonstrate that our MoSAM achieves state-of-the-art results compared to other\ncompetitors.", "AI": {"tldr": "MoSAM\u901a\u8fc7\u5f15\u5165\u8fd0\u52a8\u5f15\u5bfc\u63d0\u793a\u548c\u52a8\u6001\u65f6\u7a7a\u8bb0\u5fc6\u9009\u62e9\u673a\u5236\uff0c\u89e3\u51b3\u4e86SAM2\u5728\u89c6\u9891\u5206\u5272\u4e2d\u4f9d\u8d56\u56fa\u5b9a\u5e27\u8bb0\u5fc6\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u957f\u8ddd\u79bb\u76ee\u6807\u8ddf\u8e2a\u80fd\u529b\u3002", "motivation": "SAM2\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u5728\u89c6\u9891\u5206\u5272\u4e2d\u4ec5\u4f9d\u8d56\u8fc7\u53bb\u516d\u5e27\u7684\u63a9\u7801\u8bb0\u5fc6\uff0c\u5bfc\u81f4\u76ee\u6807\u6d88\u5931\u548c\u906e\u6321\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u957f\u8ddd\u79bb\u8ddf\u8e2a\u80fd\u529b\u3002", "method": "\u63d0\u51faMoSAM\uff0c\u7ed3\u5408\u8fd0\u52a8\u5f15\u5bfc\u63d0\u793a\uff08MGP\uff09\u548c\u65f6\u7a7a\u8bb0\u5fc6\u9009\u62e9\uff08ST-MS\uff09\u673a\u5236\uff0c\u52a8\u6001\u6574\u5408\u8fd0\u52a8\u4fe1\u606f\u548c\u53ef\u9760\u8bb0\u5fc6\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoSAM\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MoSAM\u901a\u8fc7\u8fd0\u52a8\u4fe1\u606f\u548c\u52a8\u6001\u8bb0\u5fc6\u9009\u62e9\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5206\u5272\u548c\u76ee\u6807\u8ddf\u8e2a\u80fd\u529b\u3002"}}
{"id": "2505.00876", "pdf": "https://arxiv.org/pdf/2505.00876", "abs": "https://arxiv.org/abs/2505.00876", "authors": ["Sahar Torkhesari", "Behnam Yousefimehr", "Mehdi Ghatee"], "title": "Car Sensors Health Monitoring by Verification Based on Autoencoder and Random Forest Regression", "categories": ["cs.AI", "cs.LG", "68T05", "I.2.1"], "comment": "9Pages, 3 Figures and 5 Tables", "summary": "Driver assistance systems provide a wide range of crucial services, including\nclosely monitoring the condition of vehicles. This paper showcases a\ngroundbreaking sensor health monitoring system designed for the automotive\nindustry. The ingenious system leverages cutting-edge techniques to process\ndata collected from various vehicle sensors. It compares their outputs within\nthe Electronic Control Unit (ECU) to evaluate the health of each sensor. To\nunravel the intricate correlations between sensor data, an extensive\nexploration of machine learning and deep learning methodologies was conducted.\nThrough meticulous analysis, the most correlated sensor data were identified.\nThese valuable insights were then utilized to provide accurate estimations of\nsensor values. Among the diverse learning methods examined, the combination of\nautoencoders for detecting sensor failures and random forest regression for\nestimating sensor values proved to yield the most impressive outcomes. A\nstatistical model using the normal distribution has been developed to identify\npossible sensor failures proactively. By comparing the actual values of the\nsensors with their estimated values based on correlated sensors, faulty sensors\ncan be detected early. When a defective sensor is detected, both the driver and\nthe maintenance department are promptly alerted. Additionally, the system\nreplaces the value of the faulty sensor with the estimated value obtained\nthrough analysis. This proactive approach was evaluated using data from twenty\nessential sensors in the Saipa's Quick vehicle's ECU, resulting in an\nimpressive accuracy rate of 99\\%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u4f20\u611f\u5668\u5065\u5eb7\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u8bc4\u4f30\u4f20\u611f\u5668\u5065\u5eb7\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u68c0\u6d4b\u548c\u66ff\u6362\u6545\u969c\u4f20\u611f\u5668\u503c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff0899%\uff09\u7684\u76d1\u63a7\u3002", "motivation": "\u4e3a\u6c7d\u8f66\u884c\u4e1a\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u76d1\u6d4b\u4f20\u611f\u5668\u5065\u5eb7\u72b6\u6001\u7684\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u8f66\u8f86\u5b89\u5168\u6027\u548c\u7ef4\u62a4\u6548\u7387\u3002", "method": "\u7ed3\u5408\u81ea\u52a8\u7f16\u7801\u5668\u68c0\u6d4b\u4f20\u611f\u5668\u6545\u969c\u548c\u968f\u673a\u68ee\u6797\u56de\u5f52\u4f30\u8ba1\u4f20\u611f\u5668\u503c\uff0c\u5e76\u5229\u7528\u6b63\u6001\u5206\u5e03\u6a21\u578b\u4e3b\u52a8\u8bc6\u522b\u6f5c\u5728\u6545\u969c\u3002", "result": "\u5728Saipa's Quick\u8f66\u8f86\u768420\u4e2a\u5173\u952e\u4f20\u611f\u5668\u4e0a\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u51c6\u786e\u7387\u8fbe\u523099%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u4e3b\u52a8\u76d1\u6d4b\u548c\u6545\u969c\u66ff\u6362\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u611f\u5668\u5065\u5eb7\u76d1\u63a7\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.00740", "pdf": "https://arxiv.org/pdf/2505.00740", "abs": "https://arxiv.org/abs/2505.00740", "authors": ["Zhengbin Zhang", "Yan Wu", "Hongkun Zhang"], "title": "Fast2comm:Collaborative perception combined with prior knowledge", "categories": ["cs.CV", "cs.MA"], "comment": "8pages,8figures", "summary": "Collaborative perception has the potential to significantly enhance\nperceptual accuracy through the sharing of complementary information among\nagents. However, real-world collaborative perception faces persistent\nchallenges, particularly in balancing perception performance and bandwidth\nlimitations, as well as coping with localization errors. To address these\nchallenges, we propose Fast2comm, a prior knowledge-based collaborative\nperception framework. Specifically, (1)we propose a prior-supervised confidence\nfeature generation method, that effectively distinguishes foreground from\nbackground by producing highly discriminative confidence features; (2)we\npropose GT Bounding Box-based spatial prior feature selection strategy to\nensure that only the most informative prior-knowledge features are selected and\nshared, thereby minimizing background noise and optimizing bandwidth efficiency\nwhile enhancing adaptability to localization inaccuracies; (3)we decouple the\nfeature fusion strategies between model training and testing phases, enabling\ndynamic bandwidth adaptation. To comprehensively validate our framework, we\nconduct extensive experiments on both real-world and simulated datasets. The\nresults demonstrate the superior performance of our model and highlight the\nnecessity of the proposed methods. Our code is available at\nhttps://github.com/Zhangzhengbin-TJ/Fast2comm.", "AI": {"tldr": "Fast2comm\u662f\u4e00\u4e2a\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u533a\u5206\u5ea6\u7684\u7f6e\u4fe1\u5ea6\u7279\u5f81\u548c\u4f18\u5316\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u6027\u80fd\u4e0e\u5e26\u5bbd\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u534f\u4f5c\u611f\u77e5\u901a\u8fc7\u5171\u4eab\u4e92\u8865\u4fe1\u606f\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u9762\u4e34\u5e26\u5bbd\u9650\u5236\u548c\u5b9a\u4f4d\u8bef\u5dee\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5148\u9a8c\u76d1\u7763\u7684\u7f6e\u4fe1\u5ea6\u7279\u5f81\u751f\u6210\u65b9\u6cd5\u3001\u57fa\u4e8eGT\u8fb9\u754c\u6846\u7684\u7a7a\u95f4\u5148\u9a8c\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u89e3\u8026\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u9636\u6bb5\u7684\u7279\u5f81\u878d\u5408\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFast2comm\u6027\u80fd\u4f18\u8d8a\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "Fast2comm\u6709\u6548\u89e3\u51b3\u4e86\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u5e26\u5bbd\u6548\u7387\u3002"}}
{"id": "2505.00972", "pdf": "https://arxiv.org/pdf/2505.00972", "abs": "https://arxiv.org/abs/2505.00972", "authors": ["Yuewen Mei", "Tong Nie", "Jian Sun", "Ye Tian"], "title": "Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Simulation-based testing is crucial for validating autonomous vehicles (AVs),\nyet existing scenario generation methods either overfit to common driving\npatterns or operate in an offline, non-interactive manner that fails to expose\nrare, safety-critical corner cases. In this paper, we introduce an online,\nretrieval-augmented large language model (LLM) framework for generating\nsafety-critical driving scenarios. Our method first employs an LLM-based\nbehavior analyzer to infer the most dangerous intent of the background vehicle\nfrom the observed state, then queries additional LLM agents to synthesize\nfeasible adversarial trajectories. To mitigate catastrophic forgetting and\naccelerate adaptation, we augment the framework with a dynamic memorization and\nretrieval bank of intent-planner pairs, automatically expanding its behavioral\nlibrary when novel intents arise. Evaluations using the Waymo Open Motion\nDataset demonstrate that our model reduces the mean minimum time-to-collision\nfrom 1.62 to 1.08 s and incurs a 75% collision rate, substantially\noutperforming baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5b89\u5168\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6d4b\u8bd5\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u573a\u666f\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u8fc7\u5ea6\u62df\u5408\u5e38\u89c1\u9a7e\u9a76\u6a21\u5f0f\uff0c\u8981\u4e48\u65e0\u6cd5\u4ea4\u4e92\u5f0f\u5730\u66b4\u9732\u7f55\u89c1\u4f46\u5b89\u5168\u5173\u952e\u7684\u6781\u7aef\u60c5\u51b5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5728\u7ebf\u3001\u4ea4\u4e92\u5f0f\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u884c\u4e3a\u5206\u6790\u5668\u63a8\u65ad\u80cc\u666f\u8f66\u8f86\u7684\u6700\u5371\u9669\u610f\u56fe\uff0c\u5e76\u67e5\u8be2\u5176\u4ed6LLM\u4ee3\u7406\u5408\u6210\u53ef\u884c\u7684\u5bf9\u6297\u8f68\u8ff9\uff0c\u540c\u65f6\u901a\u8fc7\u52a8\u6001\u8bb0\u5fc6\u548c\u68c0\u7d22\u5e93\u6269\u5c55\u884c\u4e3a\u5e93\u3002", "result": "\u5728Waymo Open Motion Dataset\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578b\u5c06\u5e73\u5747\u6700\u5c0f\u78b0\u649e\u65f6\u95f4\u4ece1.62\u79d2\u964d\u81f31.08\u79d2\uff0c\u78b0\u649e\u7387\u63d0\u9ad875%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u6d4b\u8bd5\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u4eff\u771f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.00741", "pdf": "https://arxiv.org/pdf/2505.00741", "abs": "https://arxiv.org/abs/2505.00741", "authors": ["Srinivas Kanakala", "Sneha Ningappa"], "title": "Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Plant diseases pose a serious challenge to agriculture by reducing crop yield\nand affecting food quality. Early detection and classification of these\ndiseases are essential for minimising losses and improving crop management\npractices. This study applies Convolutional Neural Networks (CNN) and Long\nShort-Term Memory (LSTM) models to classify plant leaf diseases using a dataset\ncontaining 70,295 training images and 17,572 validation images across 38\ndisease classes. The CNN model was trained using the Adam optimiser with a\nlearning rate of 0.0001 and categorical cross-entropy as the loss function.\nAfter 10 training epochs, the model achieved a training accuracy of 99.1% and a\nvalidation accuracy of 96.4%. The LSTM model reached a validation accuracy of\n93.43%. Performance was evaluated using precision, recall, F1-score, and\nconfusion matrix, confirming the reliability of the CNN-based approach. The\nresults suggest that deep learning models, particularly CNN, enable an\neffective solution for accurate and scalable plant disease classification,\nsupporting practical applications in agricultural monitoring.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528CNN\u548cLSTM\u6a21\u578b\u5bf9\u690d\u7269\u53f6\u7247\u75be\u75c5\u8fdb\u884c\u5206\u7c7b\uff0cCNN\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523096.4%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eLSTM\u768493.43%\u3002", "motivation": "\u690d\u7269\u75c5\u5bb3\u4e25\u91cd\u5f71\u54cd\u519c\u4e1a\u4ea7\u91cf\u548c\u98df\u54c1\u8d28\u91cf\uff0c\u65e9\u671f\u68c0\u6d4b\u548c\u5206\u7c7b\u5bf9\u51cf\u5c11\u635f\u5931\u548c\u6539\u5584\u4f5c\u7269\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528CNN\u548cLSTM\u6a21\u578b\uff0c\u57fa\u4e8e\u5305\u542b70,295\u5f20\u8bad\u7ec3\u56fe\u50cf\u548c17,572\u5f20\u9a8c\u8bc1\u56fe\u50cf\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\u3002CNN\u91c7\u7528Adam\u4f18\u5316\u5668\u548c\u5206\u7c7b\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u3002", "result": "CNN\u6a21\u578b\u8bad\u7ec3\u51c6\u786e\u7387\u4e3a99.1%\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a96.4%\uff1bLSTM\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a93.43%\u3002\u6027\u80fd\u6307\u6807\u8bc1\u5b9eCNN\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662fCNN\uff09\u4e3a\u690d\u7269\u75c5\u5bb3\u5206\u7c7b\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u519c\u4e1a\u76d1\u6d4b\u3002"}}
{"id": "2505.01009", "pdf": "https://arxiv.org/pdf/2505.01009", "abs": "https://arxiv.org/abs/2505.01009", "authors": ["Xinran Zhao", "Hanie Sedghi", "Bernd Bohnet", "Dale Schuurmans", "Azade Nova"], "title": "Improving Large Language Model Planning with Action Sequence Similarity", "categories": ["cs.AI"], "comment": "25 pages, 11 figures", "summary": "Planning is essential for artificial intelligence systems to look ahead and\nproactively determine a course of actions to reach objectives in the virtual\nand real world. Recent work on large language models (LLMs) sheds light on\ntheir planning capability in various tasks. However, it remains unclear what\nsignals in the context influence the model performance. In this work, we\nexplore how to improve the model planning capability through in-context\nlearning (ICL), specifically, what signals can help select the exemplars.\nThrough extensive experiments, we observe that commonly used problem similarity\nmay result in false positives with drastically different plans, which can\nmislead the model. In response, we propose to sample and filter exemplars\nleveraging plan side action sequence similarity (AS). We propose GRASE-DC: a\ntwo-stage pipeline that first re-samples high AS exemplars and then curates the\nselected exemplars with dynamic clustering on AS to achieve a balance of\nrelevance and diversity. Our experimental result confirms that GRASE-DC\nachieves significant performance improvement on various planning tasks (up to\n~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on\naverage). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a\nvalidator, we are able to even boost the performance by 18.9% more.\n  Extensive analysis validates the consistent performance improvement of\nGRASE-DC with various backbone LLMs and on both classical planning and natural\nlanguage planning benchmarks. GRASE-DC can further boost the planning accuracy\nby ~24 absolute points on harder problems using simpler problems as exemplars\nover a random baseline. This demonstrates its ability to generalize to\nout-of-distribution problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGRASE-DC\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u4f5c\u5e8f\u5217\u76f8\u4f3c\u6027\uff08AS\uff09\u7b5b\u9009\u793a\u4f8b\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u89c4\u5212\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u63d0\u5347LLM\u7684\u89c4\u5212\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5982\u4f55\u9009\u62e9\u6709\u6548\u7684\u793a\u4f8b\u4fe1\u53f7\u3002", "method": "\u63d0\u51faGRASE-DC\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u57fa\u4e8eAS\u91cd\u65b0\u91c7\u6837\u793a\u4f8b\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u805a\u7c7b\u5e73\u8861\u76f8\u5173\u6027\u548c\u591a\u6837\u6027\u3002", "result": "GRASE-DC\u5728\u591a\u79cd\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u6700\u9ad840\u70b9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e73\u5747\u51cf\u5c1127.3%\u793a\u4f8b\u9700\u6c42\uff09\u3002\u7ed3\u5408\u9a8c\u8bc1\u5668\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u534718.9%\u3002", "conclusion": "GRASE-DC\u80fd\u6709\u6548\u63d0\u5347LLM\u89c4\u5212\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5206\u5e03\u5916\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.00742", "pdf": "https://arxiv.org/pdf/2505.00742", "abs": "https://arxiv.org/abs/2505.00742", "authors": ["Jiaxu Qian", "Chendong Wang", "Yifan Yang", "Chaoyun Zhang", "Huiqiang Jiang", "Xufang Luo", "Yu Kang", "Qingwei Lin", "Anlan Zhang", "Shiqi Jiang", "Ting Cao", "Tianjun Mao", "Suman Banerjee", "Guyue Liu", "Saravan Rajmohan", "Dongmei Zhang", "Yuqing Yang", "Qi Zhang", "Lili Qiu"], "title": "Zoomer: Adaptive Image Focus Optimization for Black-box MLLM", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have\nbroadened the scope of vision-language tasks, excelling in applications like\nimage captioning and interactive question-answering. However, these models\nstruggle with accurately processing visual data, particularly in tasks\nrequiring precise object recognition and fine visual details. Stringent token\nlimits often result in the omission of critical information, hampering\nperformance. To address these limitations, we introduce \\SysName, a novel\nvisual prompting mechanism designed to enhance MLLM performance while\npreserving essential visual details within token limits. \\SysName features\nthree key innovations: a prompt-aware strategy that dynamically highlights\nrelevant image regions, a spatial-preserving orchestration schema that\nmaintains object integrity, and a budget-aware prompting method that balances\nglobal context with crucial visual details. Comprehensive evaluations across\nmultiple datasets demonstrate that \\SysName consistently outperforms baseline\nmethods, achieving up to a $26.9\\%$ improvement in accuracy while significantly\nreducing token consumption.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\SysName\u7684\u65b0\u578b\u89c6\u89c9\u63d0\u793a\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u9ad8\u4eae\u76f8\u5173\u56fe\u50cf\u533a\u57df\u3001\u4fdd\u6301\u5bf9\u8c61\u5b8c\u6574\u6027\u548c\u5e73\u8861\u5168\u5c40\u4e0e\u7ec6\u8282\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLM\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4ee4\u724c\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5904\u7406\u89c6\u89c9\u6570\u636e\u65f6\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u548c\u4ee4\u724c\u9650\u5236\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u4efb\u52a1\u8868\u73b0\u3002", "method": "\\SysName\u91c7\u7528\u4e09\u79cd\u521b\u65b0\u65b9\u6cd5\uff1a\u52a8\u6001\u9ad8\u4eae\u76f8\u5173\u533a\u57df\u7684\u63d0\u793a\u611f\u77e5\u7b56\u7565\u3001\u4fdd\u6301\u5bf9\u8c61\u7a7a\u95f4\u5b8c\u6574\u6027\u7684\u7a7a\u95f4\u4fdd\u7559\u7f16\u6392\u6a21\u5f0f\uff0c\u4ee5\u53ca\u5e73\u8861\u5168\u5c40\u4e0e\u7ec6\u8282\u7684\u9884\u7b97\u611f\u77e5\u63d0\u793a\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\\SysName\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe26.9%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u4ee4\u724c\u6d88\u8017\u3002", "conclusion": "\\SysName\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2505.01028", "pdf": "https://arxiv.org/pdf/2505.01028", "abs": "https://arxiv.org/abs/2505.01028", "authors": ["Huy Q. Ngo", "Mingyu Guo", "Hung Nguyen"], "title": "Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory", "categories": ["cs.AI", "cs.CR"], "comment": "To be appear in IJCAI 2025", "summary": "Security vulnerabilities in Windows Active Directory (AD) systems are\ntypically modeled using an attack graph and hardening AD systems involves an\niterative workflow: security teams propose an edge to remove, and IT operations\nteams manually review these fixes before implementing the removal. As\nverification requires significant manual effort, we formulate an Adaptive Path\nRemoval Problem to minimize the number of steps in this iterative removal\nprocess. In our model, a wizard proposes an attack path in each step and\npresents it as a set of multiple-choice options to the IT admin. The IT admin\nthen selects one edge from the proposed set to remove. This process continues\nuntil the target $t$ is disconnected from source $s$ or the number of proposed\npaths reaches $B$. The model aims to optimize the human effort by minimizing\nthe expected number of interactions between the IT admin and the security\nwizard. We first prove that the problem is $\\mathcal{\\#P}$-hard. We then\npropose a set of solutions including an exact algorithm, an approximate\nalgorithm, and several scalable heuristics. Our best heuristic, called DPR, can\noperate effectively on larger-scale graphs compared to the exact algorithm and\nconsistently outperforms the approximate algorithm across all graphs. We verify\nthe effectiveness of our algorithms on several synthetic AD graphs and an AD\nattack graph collected from a real organization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316Windows AD\u7cfb\u7edf\u5b89\u5168\u6f0f\u6d1e\u4fee\u590d\u6d41\u7a0b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316IT\u7ba1\u7406\u5458\u4e0e\u5b89\u5168\u5411\u5bfc\u7684\u4ea4\u4e92\u6b21\u6570\u6765\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "motivation": "\u7531\u4e8e\u624b\u52a8\u9a8c\u8bc1AD\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u4fee\u590d\u9700\u8981\u5927\u91cf\u4eba\u5de5\u52aa\u529b\uff0c\u7814\u7a76\u65e8\u5728\u4f18\u5316\u8fd9\u4e00\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u8def\u5f84\u79fb\u9664\u95ee\u9898\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7cbe\u786e\u7b97\u6cd5\u3001\u8fd1\u4f3c\u7b97\u6cd5\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08\u5982DPR\uff09\u6765\u89e3\u51b3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDPR\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u771f\u5b9eAD\u653b\u51fb\u56fe\u3002", "conclusion": "\u8be5\u6a21\u578b\u548c\u7b97\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u4eba\u5de5\u4ea4\u4e92\u6b21\u6570\uff0c\u63d0\u5347AD\u7cfb\u7edf\u5b89\u5168\u52a0\u56fa\u6548\u7387\u3002"}}
{"id": "2505.00743", "pdf": "https://arxiv.org/pdf/2505.00743", "abs": "https://arxiv.org/abs/2505.00743", "authors": ["Yinfeng Yu", "Dongsheng Yang"], "title": "DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": "Main paper (10 pages). Accepted for publication by ICMR(International\n  Conference on Multimedia Retrieval) 2025", "summary": "Vision-and-Language Navigation (VLN) is a challenging task where an agent\nmust understand language instructions and navigate unfamiliar environments\nusing visual cues. The agent must accurately locate the target based on visual\ninformation from the environment and complete tasks through interaction with\nthe surroundings. Despite significant advancements in this field, two major\nlimitations persist: (1) Many existing methods input complete language\ninstructions directly into multi-layer Transformer networks without fully\nexploiting the detailed information within the instructions, thereby limiting\nthe agent's language understanding capabilities during task execution; (2)\nCurrent approaches often overlook the modeling of object relationships across\ndifferent modalities, failing to effectively utilize latent clues between\nobjects, which affects the accuracy and robustness of navigation decisions. We\npropose a Dual Object Perception-Enhancement Network (DOPE) to address these\nissues to improve navigation performance. First, we design a Text Semantic\nExtraction (TSE) to extract relatively essential phrases from the text and\ninput them into the Text Object Perception-Augmentation (TOPA) to fully\nleverage details such as objects and actions within the instructions. Second,\nwe introduce an Image Object Perception-Augmentation (IOPA), which performs\nadditional modeling of object information across different modalities, enabling\nthe model to more effectively utilize latent clues between objects in images\nand text, enhancing decision-making accuracy. Extensive experiments on the R2R\nand REVERIE datasets validate the efficacy of the proposed approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdDOPE\u7f51\u7edc\uff0c\u901a\u8fc7\u589e\u5f3a\u6587\u672c\u548c\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u611f\u77e5\uff0c\u89e3\u51b3\u4e86VLN\u4efb\u52a1\u4e2d\u8bed\u8a00\u7406\u89e3\u4e0d\u8db3\u548c\u8de8\u6a21\u6001\u5bf9\u8c61\u5173\u7cfb\u5efa\u6a21\u7f3a\u5931\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VLN\u65b9\u6cd5\u5728\u8bed\u8a00\u6307\u4ee4\u7406\u89e3\u548c\u8de8\u6a21\u6001\u5bf9\u8c61\u5173\u7cfb\u5efa\u6a21\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5bfc\u822a\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86TSE\u548cTOPA\u6a21\u5757\u589e\u5f3a\u6587\u672c\u8bed\u4e49\u63d0\u53d6\uff0c\u5f15\u5165IOPA\u6a21\u5757\u5efa\u6a21\u8de8\u6a21\u6001\u5bf9\u8c61\u5173\u7cfb\u3002", "result": "\u5728R2R\u548cREVERIE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DOPE\u7684\u6709\u6548\u6027\u3002", "conclusion": "DOPE\u901a\u8fc7\u53cc\u5bf9\u8c61\u611f\u77e5\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86VLN\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.01073", "pdf": "https://arxiv.org/pdf/2505.01073", "abs": "https://arxiv.org/abs/2505.01073", "authors": ["Zongyuan Li", "Pengfei Li", "Runnan Qi", "Yanan Ni", "Lumin Jiang", "Hui Wu", "Xuebo Zhang", "Kuihua Huang", "Xian Guo"], "title": "Retrieval Augmented Learning: A Retrial-based Large Language Model Self-Supervised Learning and Autonomous Knowledge Generation", "categories": ["cs.AI"], "comment": null, "summary": "The lack of domain-specific data in the pre-training of Large Language Models\n(LLMs) severely limits LLM-based decision systems in specialized applications,\nwhile post-training a model in the scenarios requires significant computational\nresources. In this paper, we present Retrial-Augmented Learning (RAL), a\nreward-free self-supervised learning framework for LLMs that operates without\nmodel training. By developing Retrieval-Augmented Generation (RAG) into a\nmodule for organizing intermediate data, we realized a three-stage autonomous\nknowledge generation of proposing a hypothesis, validating the hypothesis, and\ngenerating the knowledge. The method is evaluated in the LLM-PySC2 environment,\na representative decision-making platform that combines sufficient complexity\nwith domain-specific knowledge requirements. Experiments demonstrate that the\nproposed method effectively reduces hallucination by generating and utilizing\nvalidated knowledge, and increases decision-making performance at an extremely\nlow cost. Meanwhile, the approach exhibits potential in\nout-of-distribution(OOD) tasks, robustness, and transferability, making it a\ncost-friendly but effective solution for decision-making problems and\nautonomous knowledge generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u7d22\u589e\u5f3a\u5b66\u4e60\u6846\u67b6\uff08RAL\uff09\uff0c\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u548c\u9a8c\u8bc1\u77e5\u8bc6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5e7b\u89c9\u5e76\u63d0\u5347\u4e86\u51b3\u7b56\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0d\u8db3\u5bf9LLM\u5728\u4e13\u4e1a\u5e94\u7528\u4e2d\u51b3\u7b56\u7cfb\u7edf\u7684\u9650\u5236\uff0c\u540c\u65f6\u907f\u514d\u9ad8\u6602\u7684\u518d\u8bad\u7ec3\u6210\u672c\u3002", "method": "\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u5757\u5316\uff0c\u5b9e\u73b0\u5047\u8bbe\u63d0\u51fa\u3001\u9a8c\u8bc1\u548c\u77e5\u8bc6\u751f\u6210\u7684\u4e09\u9636\u6bb5\u81ea\u4e3b\u5b66\u4e60\u3002", "result": "\u5728LLM-PySC2\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u5347\u51b3\u7b56\u6027\u80fd\uff0c\u6210\u672c\u6781\u4f4e\u3002", "conclusion": "RAL\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u9ad8\u6548\u7684\u51b3\u7b56\u548c\u81ea\u4e3b\u77e5\u8bc6\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709OOD\u4efb\u52a1\u3001\u9c81\u68d2\u6027\u548c\u8fc1\u79fb\u6027\u6f5c\u529b\u3002"}}
{"id": "2505.00744", "pdf": "https://arxiv.org/pdf/2505.00744", "abs": "https://arxiv.org/abs/2505.00744", "authors": ["Dung Nguyen", "Minh Khoi Ho", "Huy Ta", "Thanh Tam Nguyen", "Qi Chen", "Kumar Rav", "Quy Duong Dang", "Satwik Ramchandre", "Son Lam Phung", "Zhibin Liao", "Minh-Son To", "Johan Verjans", "Phi Le Nguyen", "Vu Minh Hieu Phan"], "title": "Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering", "categories": ["cs.CV"], "comment": "Accepted at Joint Conference on Artificial Intelligence (IJCAI) 2025", "summary": "Medical Large Multi-modal Models (LMMs) have demonstrated remarkable\ncapabilities in medical data interpretation. However, these models frequently\ngenerate hallucinations contradicting source evidence, particularly due to\ninadequate localization reasoning. This work reveals a critical limitation in\ncurrent medical LMMs: instead of analyzing relevant pathological regions, they\noften rely on linguistic patterns or attend to irrelevant image areas when\nresponding to disease-related queries. To address this, we introduce\nHEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive\nbenchmark designed to evaluate LMMs' localization abilities and hallucination\nrobustness. HEAL-MedVQA features (i) two innovative evaluation protocols to\nassess visual and textual shortcut learning, and (ii) a dataset of 67K VQA\npairs, with doctor-annotated anatomical segmentation masks for pathological\nregions. To improve visual reasoning, we propose the Localize-before-Answer\n(LobA) framework, which trains LMMs to localize target regions of interest and\nself-prompt to emphasize segmented pathological areas, generating grounded and\nreliable answers. Experimental results demonstrate that our approach\nsignificantly outperforms state-of-the-art biomedical LMMs on the challenging\nHEAL-MedVQA benchmark, advancing robustness in medical VQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHEAL-MedVQA\u57fa\u51c6\u548cLobA\u6846\u67b6\uff0c\u89e3\u51b3\u533b\u5b66LMMs\u5728\u5b9a\u4f4d\u63a8\u7406\u4e0d\u8db3\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u533b\u5b66LMMs\u56e0\u7f3a\u4e4f\u5b9a\u4f4d\u63a8\u7406\u5e38\u751f\u6210\u4e0e\u6e90\u8bc1\u636e\u77db\u76fe\u7684\u5e7b\u89c9\uff0c\u9700\u6539\u8fdb\u5176\u75c5\u7406\u533a\u57df\u5206\u6790\u80fd\u529b\u3002", "method": "\u5f15\u5165HEAL-MedVQA\u57fa\u51c6\u8bc4\u4f30\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u63d0\u51faLobA\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4f4d\u76ee\u6807\u533a\u57df\u548c\u81ea\u63d0\u793a\u751f\u6210\u53ef\u9760\u7b54\u6848\u3002", "result": "LobA\u6846\u67b6\u5728HEAL-MedVQA\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u533b\u5b66LMMs\uff0c\u63d0\u5347\u4e86\u533b\u5b66VQA\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "HEAL-MedVQA\u548cLobA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66LMMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u533b\u5b66VQA\u7684\u8fdb\u6b65\u3002"}}
{"id": "2505.01081", "pdf": "https://arxiv.org/pdf/2505.01081", "abs": "https://arxiv.org/abs/2505.01081", "authors": ["S\u00e9bastien Ferr\u00e9"], "title": "MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC Benchmark", "categories": ["cs.AI"], "comment": "54 pages", "summary": "Artificial Intelligence (AI) has achieved remarkable success in specialized\ntasks but struggles with efficient skill acquisition and generalization. The\nAbstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based\non minimal training requirements. While Large Language Models (LLMs) have\nrecently improved ARC performance, they rely on extensive pre-training and high\ncomputational costs. We introduce MADIL (MDL-based AI), a novel approach\nleveraging the Minimum Description Length (MDL) principle for efficient\ninductive learning. MADIL performs pattern-based decomposition, enabling\nstructured generalization. While its performance (7% at ArcPrize 2024) remains\nbelow LLM-based methods, it offers greater efficiency and interpretability.\nThis paper details MADIL's methodology, its application to ARC, and\nexperimental evaluations.", "AI": {"tldr": "MADIL\uff08\u57fa\u4e8eMDL\u7684AI\uff09\u662f\u4e00\u79cd\u5229\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u539f\u5219\u7684\u9ad8\u6548\u5f52\u7eb3\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6a21\u5f0f\u5206\u89e3\u548c\u7ed3\u6784\u5316\u6cdb\u5316\uff0c\u5728ARC\u4efb\u52a1\u4e2d\u8868\u73b0\u867d\u4e0d\u53caLLM\uff0c\u4f46\u66f4\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3AI\u5728\u6280\u80fd\u83b7\u53d6\u548c\u6cdb\u5316\u65b9\u9762\u7684\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728ARC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51cf\u5c11\u5bf9\u5927\u91cf\u9884\u8bad\u7ec3\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528MDL\u539f\u5219\u8fdb\u884c\u6a21\u5f0f\u5206\u89e3\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u6cdb\u5316\uff0c\u5e94\u7528\u4e8eARC\u4efb\u52a1\u3002", "result": "MADIL\u5728ArcPrize 2024\u4e2d\u8868\u73b07%\uff0c\u4f4e\u4e8eLLM\u65b9\u6cd5\uff0c\u4f46\u66f4\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u3002", "conclusion": "MADIL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684AI\u5b66\u4e60\u65b9\u6cd5\uff0c\u867d\u6027\u80fd\u6709\u5f85\u63d0\u5347\uff0c\u4f46\u5728\u8d44\u6e90\u6548\u7387\u4e0a\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2505.00745", "pdf": "https://arxiv.org/pdf/2505.00745", "abs": "https://arxiv.org/abs/2505.00745", "authors": ["Maozhe Zhao", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations", "categories": ["cs.CV", "cs.LG"], "comment": "Sensys 2025 final version", "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.", "AI": {"tldr": "MOCHA\u6846\u67b6\u901a\u8fc7\u79fb\u52a8\u4e0e\u4e91\u8d44\u6e90\u7684\u5c42\u6b21\u534f\u4f5c\u4f18\u5316\u6a21\u578b\u9002\u5e94\u6027\uff0c\u63d0\u5347\u54cd\u5e94\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u79fb\u52a8\u89c6\u9891\u5206\u6790\u7cfb\u7edf\u5728\u73af\u5883\u53d8\u5316\u65f6\u9700\u8981\u5feb\u901f\u9002\u5e94\uff0c\u73b0\u6709\u4e91\u4e2d\u5fc3\u6846\u67b6\u6027\u80fd\u4e0d\u8db3\u3002", "method": "MOCHA\u901a\u8fc7\u8bbe\u5907\u7aef\u6a21\u578b\u590d\u7528\u3001\u5feb\u901f\u5fae\u8c03\u3001\u7ed3\u6784\u5316\u6a21\u578b\u68c0\u7d22\u548c\u672c\u5730\u7f13\u5b58\u4f18\u5316\u9002\u5e94\u6027\u3002", "result": "MOCHA\u5728\u4e09\u4e2aDNN\u4efb\u52a1\u4e2d\u63d0\u5347\u6a21\u578b\u51c6\u786e\u60276.8%\uff0c\u51cf\u5c11\u54cd\u5e94\u5ef6\u8fdf35.5\u500d\u548c\u91cd\u8bad\u7ec3\u65f6\u95f43\u500d\u3002", "conclusion": "MOCHA\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9002\u5e94\u6027\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u6027\u80fd\u3002"}}
{"id": "2505.01181", "pdf": "https://arxiv.org/pdf/2505.01181", "abs": "https://arxiv.org/abs/2505.01181", "authors": ["Mehrdad Asadi", "Roxana R\u0103dulescu", "Ann Now\u00e9"], "title": "Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms", "categories": ["cs.AI"], "comment": "To appear in short form in Genetic and Evolutionary Computation\n  Conference (GECCO '25 Companion), 2025", "summary": "Swarming systems, such as for example multi-drone networks, excel at\ncooperative tasks like monitoring, surveillance, or disaster assistance in\ncritical environments, where autonomous agents make decentralized decisions in\norder to fulfill team-level objectives in a robust and efficient manner.\nUnfortunately, team-level coordinated strategies in the wild are vulnerable to\ndata poisoning attacks, resulting in either inaccurate coordination or\nadversarial behavior among the agents. To address this challenge, we contribute\na framework that investigates the effects of such data poisoning attacks, using\nexplainable AI methods. We model the interaction among agents using\nevolutionary intelligence, where an optimal coalition strategically emerges to\nperform coordinated tasks. Then, through a rigorous evaluation, the swarm model\nis systematically poisoned using data manipulation attacks. We showcase the\napplicability of explainable AI methods to quantify the effects of poisoning on\nthe team strategy and extract footprint characterizations that enable\ndiagnosing. Our findings indicate that when the model is poisoned above 10%,\nnon-optimal strategies resulting in inefficient cooperation can be identified.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u65e0\u4eba\u673a\u7f51\u7edc\u7b49\u7fa4\u4f53\u7cfb\u7edf\u4e2d\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684\u6846\u67b6\u6765\u8bca\u65ad\u548c\u91cf\u5316\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u7fa4\u4f53\u7cfb\u7edf\u5728\u5173\u952e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u56e2\u961f\u7ea7\u534f\u8c03\u7b56\u7565\u6613\u53d7\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u5bfc\u81f4\u4e0d\u51c6\u786e\u534f\u8c03\u6216\u5bf9\u6297\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u667a\u80fd\u5efa\u6a21\u4ee3\u7406\u95f4\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u64cd\u7eb5\u653b\u51fb\u7cfb\u7edf\u6027\u5730\u6bd2\u5bb3\u7fa4\u4f53\u6a21\u578b\uff0c\u5e94\u7528\u53ef\u89e3\u91caAI\u65b9\u6cd5\u91cf\u5316\u653b\u51fb\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u6a21\u578b\u88ab\u6bd2\u5bb3\u8d85\u8fc710%\u65f6\uff0c\u4f1a\u5bfc\u81f4\u975e\u6700\u4f18\u7b56\u7565\u548c\u4f4e\u6548\u5408\u4f5c\u3002", "conclusion": "\u53ef\u89e3\u91caAI\u65b9\u6cd5\u80fd\u6709\u6548\u8bca\u65ad\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u4e3a\u7fa4\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.00746", "pdf": "https://arxiv.org/pdf/2505.00746", "abs": "https://arxiv.org/abs/2505.00746", "authors": ["Alexei Kaltchenko"], "title": "Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis", "categories": ["cs.CV"], "comment": "22 pages", "summary": "Vision-language models such as OpenAI GPT-4o can transcribe mathematical\ndocuments directly from images, yet their token-level confidence signals are\nseldom used to pinpoint local recognition mistakes. We present an\nentropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into\na visual ''uncertainty landscape''. By scanning the entropy sequence with a\nfixed-length sliding window, we obtain hotspots that are likely to contain OCR\nerrors such as missing symbols, mismatched braces, or garbled prose. Using a\nsmall, curated set of scanned research pages rendered at several resolutions,\nwe compare the highlighted hotspots with the actual transcription errors\nproduced by GPT-4o. Our analysis shows that the vast majority of true errors\nare indeed concentrated inside the high-entropy regions. This study\ndemonstrates--in a minimally engineered setting--that sliding-window entropy\ncan serve as a practical, lightweight aid for post-editing GPT-based OCR. All\ncode, sample data, and annotation guidelines are released to encourage\nreplication and further research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u70ed\u56fe\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5206\u6790GPT-4o\u7684\u4ee4\u724c\u7ea7\u7f6e\u4fe1\u5ea6\uff0c\u5b9a\u4f4dOCR\u9519\u8bef\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u867d\u7136\u80fd\u4ece\u56fe\u50cf\u4e2d\u8f6c\u5f55\u6570\u5b66\u6587\u6863\uff0c\u4f46\u5176\u4ee4\u724c\u7ea7\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u672a\u88ab\u5145\u5206\u5229\u7528\u4ee5\u8bc6\u522b\u5c40\u90e8\u9519\u8bef\u3002", "method": "\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u5c06\u6bcf\u4e2a\u4ee4\u724c\u7684\u9999\u519c\u71b5\u8f6c\u5316\u4e3a\u89c6\u89c9\u5316\u7684\u201c\u4e0d\u786e\u5b9a\u6027\u666f\u89c2\u201d\uff0c\u5e76\u901a\u8fc7\u9ad8\u71b5\u533a\u57df\u5b9a\u4f4dOCR\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u591a\u6570\u771f\u5b9e\u9519\u8bef\u96c6\u4e2d\u5728\u9ad8\u71b5\u533a\u57df\u3002", "conclusion": "\u6ed1\u52a8\u7a97\u53e3\u71b5\u5206\u6790\u53ef\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5de5\u5177\uff0c\u8f85\u52a9GPT-4o OCR\u7684\u540e\u7f16\u8f91\u3002"}}
{"id": "2505.01192", "pdf": "https://arxiv.org/pdf/2505.01192", "abs": "https://arxiv.org/abs/2505.01192", "authors": ["Federico Maria Cau", "Lucio Davide Spano"], "title": "Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions", "categories": ["cs.AI", "cs.HC"], "comment": "30 pages, 7 figures", "summary": "Artificial Intelligence (AI) systems are increasingly used for\ndecision-making across domains, raising debates over the information and\nexplanations they should provide. Most research on Explainable AI (XAI) has\nfocused on feature-based explanations, with less attention on alternative\nstyles. Personality traits like the Need for Cognition (NFC) can also lead to\ndifferent decision-making outcomes among low and high NFC individuals. We\ninvestigated how presenting AI information (prediction, confidence, and\naccuracy) and different explanation styles (example-based, feature-based,\nrule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive\nload in a loan application scenario. We also examined low and high NFC\nindividuals' differences in prioritizing XAI interface elements (loan\nattributes, AI information, and explanations), accuracy, and cognitive load.\nOur findings show that high AI confidence significantly increases reliance on\nAI while reducing cognitive load. Feature-based explanations did not enhance\naccuracy compared to other conditions. Although counterfactual explanations\nwere less understandable, they enhanced overall accuracy, increasing reliance\non AI and reducing cognitive load when AI predictions were correct. Both low\nand high NFC individuals prioritized explanations after loan attributes,\nleaving AI information as the least important. However, we found no significant\ndifferences between low and high NFC groups in accuracy or cognitive load,\nraising questions about the role of personality traits in AI-assisted\ndecision-making. These findings highlight the need for user-centric\npersonalization in XAI interfaces, incorporating diverse explanation styles and\nexploring multiple personality traits and other user characteristics to\noptimize human-AI collaboration.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e0d\u540c\u89e3\u91ca\u98ce\u683c\uff08\u5982\u57fa\u4e8e\u793a\u4f8b\u3001\u7279\u5f81\u3001\u89c4\u5219\u548c\u53cd\u4e8b\u5b9e\uff09\u548cAI\u4fe1\u606f\u5bf9\u8d37\u6b3e\u7533\u8bf7\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8AI\u4fe1\u5fc3\u589e\u52a0\u4f9d\u8d56\u5e76\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff0c\u53cd\u4e8b\u5b9e\u89e3\u91ca\u867d\u96be\u7406\u89e3\u4f46\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u89e3\u91ca\u98ce\u683c\u548cAI\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9AI\u7684\u4f9d\u8d56\u3001\u51b3\u7b56\u51c6\u786e\u6027\u548c\u8ba4\u77e5\u8d1f\u8377\uff0c\u540c\u65f6\u7814\u7a76\u4e2a\u6027\u7279\u5f81\uff08\u5982NFC\uff09\u7684\u4f5c\u7528\u3002", "method": "\u5728\u8d37\u6b3e\u7533\u8bf7\u573a\u666f\u4e2d\uff0c\u6d4b\u8bd5\u4e0d\u540c\u89e3\u91ca\u98ce\u683c\u548cAI\u4fe1\u606f\u5bf9\u7528\u6237\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u9ad8\u4f4eNFC\u4e2a\u4f53\u7684\u5dee\u5f02\u3002", "result": "\u9ad8AI\u4fe1\u5fc3\u589e\u52a0\u4f9d\u8d56\u5e76\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff1b\u53cd\u4e8b\u5b9e\u89e3\u91ca\u63d0\u5347\u51c6\u786e\u6027\uff1bNFC\u4e2a\u4f53\u95f4\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u9700\u4e2a\u6027\u5316XAI\u754c\u9762\uff0c\u7ed3\u5408\u591a\u79cd\u89e3\u91ca\u98ce\u683c\u548c\u7528\u6237\u7279\u5f81\u4ee5\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2505.00751", "pdf": "https://arxiv.org/pdf/2505.00751", "abs": "https://arxiv.org/abs/2505.00751", "authors": ["Xingxi Yin", "Jingfeng Zhang", "Zhi Li", "Yicheng Li", "Yin Zhang"], "title": "InstructAttribute: Fine-grained Object Attributes editing with Instruction", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) diffusion models, renowned for their advanced generative\nabilities, are extensively utilized in image editing applications,\ndemonstrating remarkable effectiveness. However, achieving precise control over\nfine-grained attributes still presents considerable challenges. Existing image\nediting techniques either fail to modify the attributes of an object or\nstruggle to preserve its structure and maintain consistency in other areas of\nthe image. To address these challenges, we propose the Structure-Preserving and\nAttribute Amplification (SPAA), a training-free method which enables precise\ncontrol over the color and material transformations of objects by editing the\nself-attention maps and cross-attention values. Furthermore, we constructed the\nAttribute Dataset, which encompasses nearly all colors and materials associated\nwith various objects, by integrating multimodal large language models (MLLM) to\ndevelop an automated pipeline for data filtering and instruction labeling.\nTraining on this dataset, we present our InstructAttribute, an\ninstruction-based model designed to facilitate fine-grained editing of color\nand material attributes. Extensive experiments demonstrate that our method\nachieves superior performance in object-level color and material editing,\noutperforming existing instruction-based image editing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPAA\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u8f91\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff0c\u5b9e\u73b0\u5bf9\u7269\u4f53\u989c\u8272\u548c\u6750\u8d28\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5c5e\u6027\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u6307\u4ee4\u6a21\u578bInstructAttribute\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u5728\u7ec6\u7c92\u5ea6\u5c5e\u6027\u63a7\u5236\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u4fee\u6539\u7269\u4f53\u5c5e\u6027\u6216\u4fdd\u6301\u56fe\u50cf\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faSPAA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u8f91\u6ce8\u610f\u529b\u56fe\u5b9e\u73b0\u5c5e\u6027\u63a7\u5236\uff1b\u6784\u5efa\u5c5e\u6027\u6570\u636e\u96c6\uff0c\u5229\u7528MLLM\u81ea\u52a8\u8fc7\u6ee4\u548c\u6807\u6ce8\u6570\u636e\uff1b\u8bad\u7ec3\u6307\u4ee4\u6a21\u578bInstructAttribute\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u7ea7\u989c\u8272\u548c\u6750\u8d28\u7f16\u8f91\u4e0a\u4f18\u4e8e\u73b0\u6709\u6307\u4ee4\u57fa\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u3002", "conclusion": "SPAA\u548cInstructAttribute\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.01305", "pdf": "https://arxiv.org/pdf/2505.01305", "abs": "https://arxiv.org/abs/2505.01305", "authors": ["Lo Pang-Yun Ting", "Hong-Pei Chen", "An-Shan Liu", "Chun-Yin Yeh", "Po-Lin Chen", "Kun-Ta Chuang"], "title": "Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System", "categories": ["cs.AI"], "comment": null, "summary": "Early detection of patient deterioration is crucial for reducing mortality\nrates. Heart rate data has shown promise in assessing patient health, and\nwearable devices offer a cost-effective solution for real-time monitoring.\nHowever, extracting meaningful insights from diverse heart rate data and\nhandling missing values in wearable device data remain key challenges. To\naddress these challenges, we propose TARL, an innovative approach that models\nthe structural relationships of representative subsequences, known as\nshapelets, in heart rate time series. TARL creates a shapelet-transition\nknowledge graph to model shapelet dynamics in heart rate time series,\nindicating illness progression and potential future changes. We further\nintroduce a transition-aware knowledge embedding to reinforce relationships\namong shapelets and quantify the impact of missing values, enabling the\nformulation of comprehensive heart rate representations. These representations\ncapture explanatory structures and predict future heart rate trends, aiding\nearly illness detection. We collaborate with physicians and nurses to gather\nICU patient heart rate data from wearables and diagnostic metrics assessing\nillness severity for evaluating deterioration. Experiments on real-world ICU\ndata demonstrate that TARL achieves both high reliability and early detection.\nA case study further showcases TARL's explainable detection process,\nhighlighting its potential as an AI-driven tool to assist clinicians in\nrecognizing early signs of patient deterioration.", "AI": {"tldr": "TARL\u662f\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u5fc3\u7387\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u4ee3\u8868\u6027\u5b50\u5e8f\u5217\uff08shapelets\uff09\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u4ee5\u9884\u6d4b\u75c5\u60c5\u53d1\u5c55\uff0c\u5e76\u5904\u7406\u7f3a\u5931\u503c\uff0c\u5b9e\u73b0\u65e9\u671f\u75c5\u60c5\u68c0\u6d4b\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u60a3\u8005\u75c5\u60c5\u6076\u5316\u5bf9\u964d\u4f4e\u6b7b\u4ea1\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5fc3\u7387\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u7f3a\u5931\u503c\u5904\u7406\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51faTARL\u65b9\u6cd5\uff0c\u5efa\u6a21\u5fc3\u7387\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684shapelets\u52a8\u6001\u5173\u7cfb\uff0c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u5f15\u5165\u8fc7\u6e21\u611f\u77e5\u77e5\u8bc6\u5d4c\u5165\u5f3a\u5316\u5173\u7cfb\u3002", "result": "\u5728\u771f\u5b9eICU\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cTARL\u5177\u6709\u9ad8\u53ef\u9760\u6027\u548c\u65e9\u671f\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u8fc7\u7a0b\u3002", "conclusion": "TARL\u4f5c\u4e3aAI\u5de5\u5177\uff0c\u6709\u671b\u8f85\u52a9\u4e34\u5e8a\u533b\u751f\u65e9\u671f\u8bc6\u522b\u60a3\u8005\u75c5\u60c5\u6076\u5316\u3002"}}
{"id": "2505.00752", "pdf": "https://arxiv.org/pdf/2505.00752", "abs": "https://arxiv.org/abs/2505.00752", "authors": ["Xuzhao Li", "Xuchen Li", "Shiyu Hu"], "title": "DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICMR 2025", "summary": "Nighttime UAV tracking presents significant challenges due to extreme\nillumination variations and viewpoint changes, which severely degrade tracking\nperformance. Existing approaches either rely on light enhancers with high\ncomputational costs or introduce redundant domain adaptation mechanisms,\nfailing to fully utilize the dynamic features in varying perspectives. To\naddress these issues, we propose \\textbf{DARTer} (\\textbf{D}ynamic\n\\textbf{A}daptive \\textbf{R}epresentation \\textbf{T}racker), an end-to-end\ntracking framework designed for nighttime UAV scenarios. DARTer leverages a\nDynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime\nfeatures from static and dynamic templates, enhancing representation\nrobustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates\nVision Transformer layers based on extracted features, significantly improving\nefficiency by reducing redundant computations. Our model eliminates the need\nfor complex multi-task loss functions, enabling a streamlined training process.\nExtensive experiments on multiple nighttime UAV tracking benchmarks demonstrate\nthe superiority of DARTer over state-of-the-art trackers. These results confirm\nthat DARTer effectively balances tracking accuracy and efficiency, making it a\npromising solution for real-world nighttime UAV tracking applications.", "AI": {"tldr": "DARTer\u662f\u4e00\u79cd\u7528\u4e8e\u591c\u95f4\u65e0\u4eba\u673a\u8ddf\u8e2a\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u6fc0\u6d3b\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591c\u95f4\u65e0\u4eba\u673a\u8ddf\u8e2a\u4e2d\u5149\u7167\u53d8\u5316\u548c\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u7279\u5f81\u6df7\u5408\u5668\uff08DFB\uff09\u548c\u52a8\u6001\u7279\u5f81\u6fc0\u6d3b\u5668\uff08DFA\uff09\uff0c\u4f18\u5316\u7279\u5f81\u878d\u5408\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u591c\u95f4\u65e0\u4eba\u673a\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DARTer\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.01343", "pdf": "https://arxiv.org/pdf/2505.01343", "abs": "https://arxiv.org/abs/2505.01343", "authors": ["Dongliang Guo", "Mengxuan Hu", "Zihan Guan", "Thomas Hartvigsen", "Sheng Li"], "title": "BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing", "categories": ["cs.AI"], "comment": null, "summary": "Large multi-modal models inevitably decay over time as facts change and\npreviously learned information becomes outdated. Traditional approaches such as\nfine-tuning are often impractical for updating these models due to their size\nand complexity. Instead, direct knowledge editing within the models presents a\nmore viable solution. Current model editing techniques, however, typically\noverlook the unique influence ranges of different facts, leading to compromised\nmodel performance in terms of both generality and locality. To address this\nissue, we introduce the concept of the generality-locality trade-off in\nmulti-modal model editing. We develop a new model editing dataset named OKEDIT,\nspecifically designed to effectively evaluate this trade-off. Building on this\nfoundation, we propose BalancEdit, a novel method for balanced model editing\nthat dynamically achieves an optimal balance between generality and locality.\nBalancEdit utilizes a unique mechanism that generates both positive and\nnegative samples for each fact to accurately determine its influence scope and\nincorporates these insights into the model's latent space using a discrete,\nlocalized codebook of edits, without modifying the underlying model weights. To\nour knowledge, this is the first approach explicitly addressing the\ngenerality-locality trade-off in multi-modal model editing. Our comprehensive\nresults confirm the effectiveness of BalancEdit, demonstrating minimal\ntrade-offs while maintaining robust editing capabilities. Our code and dataset\nwill be available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBalancEdit\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u4e2d\u7684\u6cdb\u5316\u6027\u4e0e\u5c40\u90e8\u6027\u6743\u8861\u95ee\u9898\uff0c\u5e76\u5f15\u5165OKEDIT\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4f1a\u968f\u65f6\u95f4\u8870\u51cf\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u4e0d\u9002\u7528\uff0c\u73b0\u6709\u7f16\u8f91\u6280\u672f\u5ffd\u89c6\u4e0d\u540c\u4e8b\u5b9e\u7684\u5f71\u54cd\u8303\u56f4\u3002", "method": "\u63d0\u51faBalancEdit\u65b9\u6cd5\uff0c\u52a8\u6001\u5e73\u8861\u6cdb\u5316\u6027\u4e0e\u5c40\u90e8\u6027\uff0c\u901a\u8fc7\u751f\u6210\u6b63\u8d1f\u6837\u672c\u786e\u5b9a\u5f71\u54cd\u8303\u56f4\uff0c\u5e76\u4f7f\u7528\u79bb\u6563\u7f16\u8f91\u7801\u672c\u3002", "result": "BalancEdit\u5728\u4fdd\u6301\u7f16\u8f91\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u5c0f\u7684\u6743\u8861\u3002", "conclusion": "BalancEdit\u662f\u9996\u4e2a\u660e\u786e\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u4e2d\u6cdb\u5316\u6027\u4e0e\u5c40\u90e8\u6027\u6743\u8861\u7684\u65b9\u6cd5\uff0c\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2505.00755", "pdf": "https://arxiv.org/pdf/2505.00755", "abs": "https://arxiv.org/abs/2505.00755", "authors": ["Atsuya Watanabe", "Ratna Aisuwarya", "Lei Jing"], "title": "P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work presents P2P-Insole, a low-cost approach for estimating and\nvisualizing 3D human skeletal data using insole-type sensors integrated with\nIMUs. Each insole, fabricated with e-textile garment techniques, costs under\nUSD 1, making it significantly cheaper than commercial alternatives and ideal\nfor large-scale production. Our approach uses foot pressure distribution,\nacceleration, and rotation data to overcome limitations, providing a\nlightweight, minimally intrusive, and privacy-aware solution. The system\nemploys a Transformer model for efficient temporal feature extraction, enriched\nby first and second derivatives in the input stream. Including multimodal\ninformation, such as accelerometers and rotational measurements, improves the\naccuracy of complex motion pattern recognition. These facts are demonstrated\nexperimentally, while error metrics show the robustness of the approach in\nvarious posture estimation tasks. This work could be the foundation for a\nlow-cost, practical application in rehabilitation, injury prevention, and\nhealth monitoring while enabling further development through sensor\noptimization and expanded datasets.", "AI": {"tldr": "P2P-Insole\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210IMU\u7684\u978b\u57ab\u4f20\u611f\u5668\u4f30\u8ba1\u548c\u53ef\u89c6\u53163D\u4eba\u4f53\u9aa8\u9abc\u6570\u636e\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u4ea7\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u8f7b\u91cf\u7ea7\u4e14\u9690\u79c1\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5eb7\u590d\u3001\u4f24\u5bb3\u9884\u9632\u548c\u5065\u5eb7\u76d1\u6d4b\u3002", "method": "\u4f7f\u7528\u978b\u57ab\u4f20\u611f\u5668\u91c7\u96c6\u8db3\u538b\u5206\u5e03\u3001\u52a0\u901f\u5ea6\u548c\u65cb\u8f6c\u6570\u636e\uff0c\u7ed3\u5408Transformer\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u65f6\u95f4\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u8bc6\u522b\u4e2d\u5177\u6709\u9ad8\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u3002", "conclusion": "P2P-Insole\u4e3a\u4f4e\u6210\u672c\u5b9e\u7528\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u4f20\u611f\u5668\u4f18\u5316\u548c\u6570\u636e\u96c6\u6269\u5c55\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2411.09200", "pdf": "https://arxiv.org/pdf/2411.09200", "abs": "https://arxiv.org/abs/2411.09200", "authors": ["Sabbir M. Saleh", "Ibrahim Mohammed Sayem", "Nazim Madhavji", "John Steinbacher"], "title": "Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "10 pages", "summary": "Continuous Integration/Continuous Deployment (CI/CD) is fundamental for\nadvanced software development, supporting faster and more efficient delivery of\ncode changes into cloud environments. However, security issues in the CI/CD\npipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are\nhappening over the cloud environments. While plenty of literature discusses\nstatic security testing and CI/CD practices, only a few deal with network\ntraffic pattern analysis to detect different cyberattacks. This research aims\nto enhance CI/CD pipeline security by implementing anomaly detection through AI\n(Artificial Intelligence) support. The goal is to identify unusual behaviour or\nvariations from network traffic patterns in pipeline and cloud platforms. The\nsystem shall integrate into the workflow to continuously monitor pipeline\nactivities and cloud infrastructure. Additionally, it aims to explore adaptive\nresponse mechanisms to mitigate the detected anomalies or security threats.\nThis research employed two popular network traffic datasets, CSE-CIC-IDS2018\nand CSE-CIC-IDS2017. We implemented a combination of Convolution Neural\nNetwork(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic\npatterns. We achieved an accuracy of 98.69% and 98.30% and generated log files\nin different CI/CD pipeline stages that resemble the network anomalies affected\nto address security challenges in modern DevOps practices, contributing to\nadvancing software security and reliability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u652f\u6301\u7684\u5f02\u5e38\u68c0\u6d4b\u589e\u5f3aCI/CD\u7ba1\u9053\u7684\u5b89\u5168\u6027\uff0c\u8bc6\u522b\u7f51\u7edc\u6d41\u91cf\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u5e76\u63a2\u7d22\u81ea\u9002\u5e94\u54cd\u5e94\u673a\u5236\u3002", "motivation": "CI/CD\u7ba1\u9053\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff08\u5982DDoS\u3001Bot\u3001Log4j\u7b49\uff09\u9891\u53d1\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u9759\u6001\u5b89\u5168\u6d4b\u8bd5\uff0c\u800c\u7f51\u7edc\u6d41\u91cf\u6a21\u5f0f\u5206\u6790\u8f83\u5c11\u3002", "method": "\u7ed3\u5408CNN\u548cLSTM\u6a21\u578b\uff0c\u4f7f\u7528CSE-CIC-IDS2018\u548cCSE-CIC-IDS2017\u6570\u636e\u96c6\u68c0\u6d4b\u5f02\u5e38\u6d41\u91cf\u6a21\u5f0f\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe98.69%\u548c98.30%\uff0c\u5e76\u751f\u6210\u5f02\u5e38\u65e5\u5fd7\u4ee5\u5e94\u5bf9\u5b89\u5168\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u73b0\u4ee3DevOps\u5b9e\u8df5\u63d0\u4f9b\u4e86\u589e\u5f3a\u8f6f\u4ef6\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00757", "pdf": "https://arxiv.org/pdf/2505.00757", "abs": "https://arxiv.org/abs/2505.00757", "authors": ["Woong-Chan Byun", "Dong-Hee Paek", "Seung-Hyun Song", "Seung-Hyun Kong"], "title": "Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L", "categories": ["cs.CV", "cs.AI"], "comment": "4pages, 2 figures", "summary": "4D radar has attracted attention in autonomous driving due to its ability to\nenable robust 3D object detection even under adverse weather conditions. To\npractically deploy such technologies, it is essential to achieve real-time\nprocessing within low-power embedded environments. Addressing this, we present\nthe first on-chip implementation of a 4D radar-based 3D object detection model\non the Hailo-8L AI accelerator. Although conventional 3D convolutional neural\nnetwork (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D\ntensors, posing a significant challenge. To overcome this limitation, we\nintroduce a tensor transformation method that reshapes 5D inputs into 4D\nformats during the compilation process, enabling direct deployment without\naltering the model structure. The proposed system achieves 46.47% AP_3D and\n52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while\nachieving an inference speed of 13.76 Hz. These results demonstrate the\napplicability of 4D radar-based perception technologies to autonomous driving\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728Hailo-8L AI\u52a0\u901f\u5668\u4e0a\u5b9e\u73b04D\u96f7\u8fbe3D\u76ee\u6807\u68c0\u6d4b\u7684\u82af\u7247\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f20\u91cf\u53d8\u6362\u89e3\u51b35D\u8f93\u5165\u4e0e4D\u652f\u6301\u7684\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5904\u7406\u548c\u9ad8\u7cbe\u5ea6\u3002", "motivation": "4D\u96f7\u8fbe\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u9700\u5728\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u3002", "method": "\u5f15\u5165\u5f20\u91cf\u53d8\u6362\u65b9\u6cd5\uff0c\u5c065D\u8f93\u5165\u91cd\u5851\u4e3a4D\u683c\u5f0f\uff0c\u4fdd\u6301\u6a21\u578b\u7ed3\u6784\u4e0d\u53d8\u3002", "result": "\u7cfb\u7edf\u8fbe\u523046.47% AP_3D\u548c52.75% AP_BEV\uff0c\u63a8\u7406\u901f\u5ea613.76 Hz\uff0c\u4e0eGPU\u6a21\u578b\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "\u8bc1\u660e\u4e864D\u96f7\u8fbe\u611f\u77e5\u6280\u672f\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.20605", "pdf": "https://arxiv.org/pdf/2504.20605", "abs": "https://arxiv.org/abs/2504.20605", "authors": ["Mihai Nadas", "Laura Diosan", "Andrei Piscoran", "Andreea Tomescu"], "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.LG"], "comment": null, "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models.", "AI": {"tldr": "TF1-EN-3M\u662f\u4e00\u4e2a\u75318B\u53c2\u6570\u6a21\u578b\u751f\u6210\u7684300\u4e07\u82f1\u8bed\u5bd3\u8a00\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u4ee3NLP\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u5316\u9053\u5fb7\u53d9\u4e8b\u6570\u636e\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u4ee3NLP\u7f3a\u4e4f\u7ed3\u5408\u660e\u786e\u9053\u5fb7\u6559\u8bad\u7684\u53d9\u4e8b\u6570\u636e\u96c6\uff0cTF1-EN-3M\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u7ec4\u5408\u63d0\u793a\u5f15\u64ce\u751f\u6210\u516d\u69fd\u7ed3\u6784\u7684\u5bd3\u8a00\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u8bc4\u4f30\u6d41\u7a0b\uff08GPT\u8bc4\u5206\u548c\u65e0\u53c2\u8003\u6307\u6807\uff09\u8bc4\u4f30\u8d28\u91cf\u3002", "result": "8B\u53c2\u6570\u7684Llama-3\u53d8\u4f53\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5355GPU\u53ef\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5bd3\u8a00\u3002", "conclusion": "TF1-EN-3M\u4e3a\u6307\u4ee4\u9075\u5faa\u3001\u53d9\u4e8b\u667a\u80fd\u7b49\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90\uff0c\u8bc1\u660e\u5927\u89c4\u6a21\u9053\u5fb7\u53d9\u4e8b\u65e0\u9700\u4e13\u6709\u5927\u6a21\u578b\u3002"}}
{"id": "2505.00759", "pdf": "https://arxiv.org/pdf/2505.00759", "abs": "https://arxiv.org/abs/2505.00759", "authors": ["Jiahui Chen", "Candace Ross", "Reyhane Askari-Hemmat", "Koustuv Sinha", "Melissa Hall", "Michal Drozdzal", "Adriana Romero-Soriano"], "title": "Multi-Modal Language Models as Text-to-Image Model Evaluators", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The steady improvements of text-to-image (T2I) generative models lead to slow\ndeprecation of automatic evaluation benchmarks that rely on static datasets,\nmotivating researchers to seek alternative ways to evaluate the T2I progress.\nIn this paper, we explore the potential of multi-modal large language models\n(MLLMs) as evaluator agents that interact with a T2I model, with the objective\nof assessing prompt-generation consistency and image aesthetics. We present\nMultimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively\ngenerates prompts for evaluation, scores generated images and matches T2I\nevaluation of existing benchmarks with a fraction of the prompts used in\nexisting static benchmarks. Moreover, we show that MT2IE's prompt-generation\nconsistency scores have higher correlation with human judgment than scores\npreviously introduced in the literature. MT2IE generates prompts that are\nefficient at probing T2I model performance, producing the same relative T2I\nmodel rankings as existing benchmarks while using only 1/80th the number of\nprompts for evaluation.", "AI": {"tldr": "MT2IE\u662f\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u63d0\u793a\u8bcd\u548c\u8bc4\u5206\uff0c\u663e\u8457\u51cf\u5c11\u8bc4\u4f30\u6240\u9700\u7684\u63d0\u793a\u8bcd\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u968f\u7740T2I\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u7684\u81ea\u52a8\u8bc4\u4f30\u57fa\u51c6\u9010\u6e10\u8fc7\u65f6\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMT2IE\u6846\u67b6\uff0c\u5229\u7528MLLMs\u52a8\u6001\u751f\u6210\u63d0\u793a\u8bcd\u5e76\u8bc4\u4f30\u56fe\u50cf\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "result": "MT2IE\u4ec5\u97001/80\u7684\u63d0\u793a\u8bcd\u5373\u53ef\u8fbe\u5230\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u540c\u7684\u6a21\u578b\u6392\u540d\uff0c\u4e14\u8bc4\u5206\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u66f4\u9ad8\u3002", "conclusion": "MT2IE\u4e3aT2I\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u52a8\u6001\u65b9\u6cd5\u3002"}}
{"id": "2505.00737", "pdf": "https://arxiv.org/pdf/2505.00737", "abs": "https://arxiv.org/abs/2505.00737", "authors": ["Jiajia Li", "Xinda Qi", "Seyed Hamidreza Nabaei", "Meiqi Liu", "Dong Chen", "Xin Zhang", "Xunyuan Yin", "Zhaojian Li"], "title": "A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "17 pages, 7 figures, 4 tables", "summary": "Plant phenotyping plays a pivotal role in understanding plant traits and\ntheir interactions with the environment, making it crucial for advancing\nprecision agriculture and crop improvement. 3D reconstruction technologies have\nemerged as powerful tools for capturing detailed plant morphology and\nstructure, offering significant potential for accurate and automated\nphenotyping. This paper provides a comprehensive review of the 3D\nreconstruction techniques for plant phenotyping, covering classical\nreconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel\n3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on\nhigh-resolution sensors, are widely adopted due to their simplicity and\nflexibility in representing plant structures. However, they face challenges\nsuch as data density, noise, and scalability. NeRF, a recent advancement,\nenables high-quality, photorealistic 3D reconstructions from sparse viewpoints,\nbut its computational cost and applicability in outdoor environments remain\nareas of active research. The emerging 3DGS technique introduces a new paradigm\nin reconstructing plant structures by representing geometry through Gaussian\nprimitives, offering potential benefits in both efficiency and scalability. We\nreview the methodologies, applications, and performance of these approaches in\nplant phenotyping and discuss their respective strengths, limitations, and\nfuture prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants).\nThrough this review, we aim to provide insights into how these diverse 3D\nreconstruction techniques can be effectively leveraged for automated and\nhigh-throughput plant phenotyping, contributing to the next generation of\nagricultural technology.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u690d\u7269\u8868\u578b\u5206\u6790\u4e2d\u76843D\u91cd\u5efa\u6280\u672f\uff0c\u5305\u62ec\u7ecf\u5178\u65b9\u6cd5\u3001NeRF\u548c3DGS\uff0c\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\u53ca\u672a\u6765\u524d\u666f\u3002", "motivation": "\u690d\u7269\u8868\u578b\u5206\u6790\u5bf9\u7cbe\u51c6\u519c\u4e1a\u548c\u4f5c\u7269\u6539\u826f\u81f3\u5173\u91cd\u8981\uff0c3D\u91cd\u5efa\u6280\u672f\u4e3a\u81ea\u52a8\u5316\u8868\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "method": "\u7efc\u8ff0\u4e86\u7ecf\u5178\u91cd\u5efa\u65b9\u6cd5\u3001NeRF\u548c3DGS\u7684\u6280\u672f\u539f\u7406\u3001\u5e94\u7528\u53ca\u6027\u80fd\u3002", "result": "\u7ecf\u5178\u65b9\u6cd5\u7b80\u5355\u7075\u6d3b\u4f46\u9762\u4e34\u6570\u636e\u5bc6\u5ea6\u548c\u566a\u58f0\u95ee\u9898\uff1bNeRF\u9ad8\u8d28\u91cf\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b3DGS\u5728\u6548\u7387\u548c\u6269\u5c55\u6027\u4e0a\u5177\u6f5c\u529b\u3002", "conclusion": "\u4e0d\u540c3D\u91cd\u5efa\u6280\u672f\u5404\u6709\u4f18\u52a3\uff0c\u672a\u6765\u9700\u7ed3\u5408\u5e94\u7528\u573a\u666f\u4f18\u5316\uff0c\u63a8\u52a8\u519c\u4e1a\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2505.00772", "pdf": "https://arxiv.org/pdf/2505.00772", "abs": "https://arxiv.org/abs/2505.00772", "authors": ["Branko Brklja\u010d", "Milan Brklja\u010d"], "title": "Person detection and re-identification in open-world settings of retail stores and public spaces", "categories": ["cs.CV", "68T10, 68T07, 68T45, 94A08, 94A13,", "I.4.9; I.4.8; I.5.4; I.5.5; I.2.10; C.3; J.4; J.7"], "comment": "6 pages, 3 figures, 1 table, associated code implementation and\n  accompanying test videos with experimental results are available at the\n  following link: https://github.com/brkljac/personReID , paper submitted to\n  the 2nd International Scientific Conference 'ALFATECH - Smart Cities and\n  modern technologies - 2025', Belgrade, Serbia, Feb. 28, 2025", "summary": "Practical applications of computer vision in smart cities usually assume\nsystem integration and operation in challenging open-world environments. In the\ncase of person re-identification task the main goal is to retrieve information\nwhether the specific person has appeared in another place at a different time\ninstance of the same video, or over multiple camera feeds. This typically\nassumes collecting raw data from video surveillance cameras in different places\nand under varying illumination conditions. In the considered open-world setting\nit also requires detection and localization of the person inside the analyzed\nvideo frame before the main re-identification step. With multi-person and\nmulti-camera setups the system complexity becomes higher, requiring\nsophisticated tracking solutions and re-identification models. In this work we\nwill discuss existing challenges in system design architectures, consider\npossible solutions based on different computer vision techniques, and describe\napplications of such systems in retail stores and public spaces for improved\nmarketing analytics. In order to analyse sensitivity of person\nre-identification task under different open-world environments, a performance\nof one close to real-time solution will be demonstrated over several video\ncaptures and live camera feeds. Finally, based on conducted experiments we will\nindicate further research directions and possible system improvements.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u667a\u80fd\u57ce\u5e02\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u7279\u522b\u662f\u4eba\u5458\u91cd\u8bc6\u522b\u4efb\u52a1\u7684\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5728\u590d\u6742\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\uff08\u5982\u591a\u6444\u50cf\u5934\u3001\u591a\u53d8\u5149\u7167\u6761\u4ef6\u4e0b\uff09\u7684\u4eba\u5458\u91cd\u8bc6\u522b\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u96f6\u552e\u548c\u516c\u5171\u7a7a\u95f4\u7684\u8425\u9500\u5206\u6790\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7cfb\u7edf\u8bbe\u8ba1\u67b6\u6784\u7684\u8ba8\u8bba\u3001\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u53ca\u5bf9\u63a5\u8fd1\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u89c6\u9891\u548c\u5b9e\u65f6\u6444\u50cf\u5934\u6570\u636e\u5c55\u793a\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u5f00\u653e\u4e16\u754c\u73af\u5883\u5bf9\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u4e86\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u65b9\u5411\u548c\u7cfb\u7edf\u6539\u8fdb\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.00786", "pdf": "https://arxiv.org/pdf/2505.00786", "abs": "https://arxiv.org/abs/2505.00786", "authors": ["Oluwanisola Ibikunle", "Hara Talasila", "Debvrat Varshney", "Jilu Li", "John Paden", "Maryam Rahnemoonfar"], "title": "AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Tracking internal layers in radar echograms with high accuracy is essential\nfor understanding ice sheet dynamics and quantifying the impact of accelerated\nice discharge in Greenland and other polar regions due to contemporary global\nclimate warming. Deep learning algorithms have become the leading approach for\nautomating this task, but the absence of a standardized and well-annotated\nechogram dataset has hindered the ability to test and compare algorithms\nreliably, limiting the advancement of state-of-the-art methods for the radar\nechogram layer tracking problem. This study introduces the first comprehensive\n``deep learning ready'' radar echogram dataset derived from Snow Radar airborne\ndata collected during the National Aeronautics and Space Administration\nOperation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled\nand 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation,\nwet) with varying along-track resolutions. To demonstrate its utility, we\nevaluated the performance of five deep learning models on the dataset. Our\nresults show that while current computer vision segmentation algorithms can\nidentify and track snow layer pixels in echogram images, advanced end-to-end\nmodels are needed to directly extract snow depth and annual accumulation from\nechograms, reducing or eliminating post-processing. The dataset and\naccompanying benchmarking framework provide a valuable resource for advancing\nradar echogram layer tracking and snow accumulation estimation, advancing our\nunderstanding of polar ice sheets response to climate warming.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u96f7\u8fbe\u56de\u6ce2\u56fe\u5185\u90e8\u5c42\u8ffd\u8e2a\u7684\u6df1\u5ea6\u5b66\u4e60\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cd\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6307\u51fa\u9700\u8981\u66f4\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6a21\u578b\u3002", "motivation": "\u9ad8\u7cbe\u5ea6\u8ffd\u8e2a\u96f7\u8fbe\u56de\u6ce2\u56fe\u5185\u90e8\u5c42\u5bf9\u7406\u89e3\u51b0\u76d6\u52a8\u6001\u548c\u91cf\u5316\u5168\u7403\u6c14\u5019\u53d8\u6696\u5bf9\u6781\u5730\u51b0\u5c42\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u9650\u5236\u4e86\u7b97\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8eNASA OIB\u4efb\u52a1\u6570\u636e\u7684\u6df1\u5ea6\u5b66\u4e60\u6570\u636e\u96c6\uff0c\u5305\u542b13,717\u6807\u6ce8\u548c57,815\u5f31\u6807\u6ce8\u7684\u56de\u6ce2\u56fe\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5f53\u524d\u8ba1\u7b97\u673a\u89c6\u89c9\u5206\u5272\u7b97\u6cd5\u80fd\u8bc6\u522b\u56de\u6ce2\u56fe\u4e2d\u7684\u96ea\u5c42\u50cf\u7d20\uff0c\u4f46\u9700\u8981\u66f4\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6a21\u578b\u4ee5\u76f4\u63a5\u63d0\u53d6\u96ea\u6df1\u548c\u5e74\u79ef\u7d2f\u91cf\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6846\u67b6\u4e3a\u96f7\u8fbe\u56de\u6ce2\u56fe\u5c42\u8ffd\u8e2a\u548c\u96ea\u79ef\u7d2f\u91cf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6781\u5730\u51b0\u76d6\u5bf9\u6c14\u5019\u53d8\u6696\u7684\u54cd\u5e94\u3002"}}
{"id": "2505.00749", "pdf": "https://arxiv.org/pdf/2505.00749", "abs": "https://arxiv.org/abs/2505.00749", "authors": ["Roman J. Georgio", "Caelum Forder", "Suman Deb", "Peter Carroll", "\u00d6nder G\u00fcrcan"], "title": "The Coral Protocol: Open Infrastructure Connecting The Internet of Agents", "categories": ["cs.MA", "cs.AI"], "comment": "31 pages, 3 figures, Whitepaper", "summary": "The Coral Protocol is an open and decentralized collaboration infrastructure\nthat enables communication, coordination, trust and payments for The Internet\nof Agents. It addresses the growing need for interoperability in a world where\norganizations are deploying multiple specialized AI agents that must work\ntogether across domains and vendors. As a foundational platform for multi-agent\nAI ecosystems, Coral establishes a common language and coordination framework\nallowing any agent to participate in complex workflows with others. Its design\nemphasizes broad compatibility, security, and vendor neutrality, ensuring that\nagent interactions are efficient and trustworthy. In particular, Coral\nintroduces standardized messaging formats for agent communication, a modular\ncoordination mechanism for orchestrating multi-agent tasks, and secure team\nformation capabilities for dynamically assembling trusted groups of agents.\nTogether, these innovations position Coral Protocol as a cornerstone of the\nemerging \"Internet of Agents,\" unlocking new levels of automation, collective\nintelligence, and business value through open agent collaboration.", "AI": {"tldr": "Coral Protocol\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u3001\u53bb\u4e2d\u5fc3\u5316\u7684\u534f\u4f5c\u57fa\u7840\u8bbe\u65bd\uff0c\u65e8\u5728\u4e3a\u201c\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u201d\u63d0\u4f9b\u901a\u4fe1\u3001\u534f\u8c03\u3001\u4fe1\u4efb\u548c\u652f\u4ed8\u529f\u80fd\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8de8\u9886\u57df\u534f\u4f5c\u7684\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u7ec4\u7ec7\u90e8\u7f72\u591a\u4e2a\u4e13\u4e1a\u5316\u7684AI\u667a\u80fd\u4f53\uff0c\u8de8\u9886\u57df\u548c\u8de8\u5382\u5546\u7684\u534f\u4f5c\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u4e92\u64cd\u4f5c\u7684\u534f\u4f5c\u57fa\u7840\u8bbe\u65bd\u3002", "method": "Coral Protocol\u8bbe\u8ba1\u4e86\u4e00\u79cd\u901a\u7528\u8bed\u8a00\u548c\u534f\u8c03\u6846\u67b6\uff0c\u5305\u62ec\u6807\u51c6\u5316\u7684\u6d88\u606f\u683c\u5f0f\u3001\u6a21\u5757\u5316\u7684\u534f\u8c03\u673a\u5236\u4ee5\u53ca\u5b89\u5168\u7684\u56e2\u961f\u7ec4\u5efa\u529f\u80fd\u3002", "result": "\u8be5\u534f\u8bae\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53\u7684\u9ad8\u6548\u3001\u53ef\u4fe1\u534f\u4f5c\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "Coral Protocol\u4f5c\u4e3a\u201c\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u201d\u7684\u6838\u5fc3\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u5f00\u653e\u7684\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u3001\u96c6\u4f53\u667a\u80fd\u548c\u5546\u4e1a\u4ef7\u503c\u7684\u63d0\u5347\u3002"}}
{"id": "2505.00788", "pdf": "https://arxiv.org/pdf/2505.00788", "abs": "https://arxiv.org/abs/2505.00788", "authors": ["Wufei Ma", "Luoxin Ye", "Nessa McWeeney", "Celso M de Melo", "Alan Yuille", "Jieneng Chen"], "title": "SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models", "categories": ["cs.CV"], "comment": "CVPR 2025 highlight, camera ready version", "summary": "Humans naturally understand 3D spatial relationships, enabling complex\nreasoning like predicting collisions of vehicles from different directions.\nCurrent large multimodal models (LMMs), however, lack of this capability of 3D\nspatial reasoning. This limitation stems from the scarcity of 3D training data\nand the bias in current model designs toward 2D data. In this paper, we\nsystematically study the impact of 3D-informed data, architecture, and training\nsetups, introducing SpatialLLM, a large multi-modal model with advanced 3D\nspatial reasoning abilities. To address data limitations, we develop two types\nof 3D-informed training datasets: (1) 3D-informed probing data focused on\nobject's 3D location and orientation, and (2) 3D-informed conversation data for\ncomplex spatial relationships. Notably, we are the first to curate VQA data\nthat incorporate 3D orientation relationships on real images. Furthermore, we\nsystematically integrate these two types of training data with the\narchitectural and training designs of LMMs, providing a roadmap for optimal\ndesign aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM\nadvances machines toward highly capable 3D-informed reasoning, surpassing\nGPT-4o performance by 8.7%. Our systematic empirical design and the resulting\nfindings offer valuable insights for future research in this direction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSpatialLLM\uff0c\u901a\u8fc73D\u6570\u636e\u589e\u5f3a\u548c\u67b6\u6784\u8bbe\u8ba1\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u76843D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u6027\u80fd\u8d85\u8d8aGPT-4o 8.7%\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u7f3a\u4e4f3D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e3b\u8981\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u8bbe\u8ba1\u504f\u54112D\u6570\u636e\u3002", "method": "\u5f00\u53d1\u4e24\u7c7b3D\u8bad\u7ec3\u6570\u636e\uff083D\u63a2\u6d4b\u6570\u636e\u548c\u5bf9\u8bdd\u6570\u636e\uff09\uff0c\u5e76\u7cfb\u7edf\u6574\u5408\u5230\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u4e2d\u3002", "result": "SpatialLLM\u57283D\u63a8\u7406\u80fd\u529b\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u8d85\u8d8aGPT-4o 8.7%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u67653D\u63a8\u7406\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\u548c\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2505.00805", "pdf": "https://arxiv.org/pdf/2505.00805", "abs": "https://arxiv.org/abs/2505.00805", "authors": ["Fadi Abdeladhim Zidi", "Abdelkrim Ouafi", "Fares Bougourzi", "Cosimo Distante", "Abdelmalik Taleb-Ahmed"], "title": "Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "As one of the most widely cultivated and consumed crops, wheat is essential\nto global food security. However, wheat production is increasingly challenged\nby pests, diseases, climate change, and water scarcity, threatening yields.\nTraditional crop monitoring methods are labor-intensive and often ineffective\nfor early issue detection. Hyperspectral imaging (HSI) has emerged as a\nnon-destructive and efficient technology for remote crop health assessment.\nHowever, the high dimensionality of HSI data and limited availability of\nlabeled samples present notable challenges. In recent years, deep learning has\nshown great promise in addressing these challenges due to its ability to\nextract and analysis complex structures. Despite advancements in applying deep\nlearning methods to HSI data for wheat crop analysis, no comprehensive survey\ncurrently exists in this field. This review addresses this gap by summarizing\nbenchmark datasets, tracking advancements in deep learning methods, and\nanalyzing key applications such as variety classification, disease detection,\nand yield estimation. It also highlights the strengths, limitations, and future\nopportunities in leveraging deep learning methods for HSI-based wheat crop\nanalysis. We have listed the current state-of-the-art papers and will continue\ntracking updating them in the following\nhttps://github.com/fadi-07/Awesome-Wheat-HSI-DeepLearning.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u5c0f\u9ea6\u4f5c\u7269\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u3001\u65b9\u6cd5\u8fdb\u5c55\u53ca\u5173\u952e\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u673a\u4f1a\u3002", "motivation": "\u5c0f\u9ea6\u751f\u4ea7\u9762\u4e34\u75c5\u866b\u5bb3\u3001\u6c14\u5019\u53d8\u5316\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\uff0cHSI\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u73b0\u6709\u7814\u7a76\uff0c\u603b\u7ed3\u6df1\u5ea6\u5b66\u4e60\u5728HSI\u5c0f\u9ea6\u5206\u6790\u4e2d\u7684\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u5e94\u7528\u3002", "result": "\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u54c1\u79cd\u5206\u7c7b\u3001\u75c5\u5bb3\u68c0\u6d4b\u548c\u4ea7\u91cf\u4f30\u8ba1\u7b49\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u5c40\u9650\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728HSI\u5c0f\u9ea6\u5206\u6790\u4e2d\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u4ecd\u9700\u66f4\u591a\u6570\u636e\u548c\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2505.00836", "pdf": "https://arxiv.org/pdf/2505.00836", "abs": "https://arxiv.org/abs/2505.00836", "authors": ["Conor Flynn", "Christopher Ebersole", "Edmund Zelnio"], "title": "The Comparability of Model Fusion to Measured Data in Confuser Rejection", "categories": ["cs.CV"], "comment": "Conference paper for SPIE Defense and Commercial Sensing Algorithms\n  for Synthetic Aperture Radar Imagery XXXII. 14 pages, 9 figures", "summary": "Data collection has always been a major issue in the modeling and training of\nlarge deep learning networks, as no dataset can account for every slight\ndeviation we might see in live usage. Collecting samples can be especially\ncostly for Synthetic Aperture Radar (SAR), limiting the amount of unique\ntargets and operating conditions we are able to observe from. To counter this\nlack of data, simulators have been developed utilizing the shooting and\nbouncing ray method to allow for the generation of synthetic SAR data on 3D\nmodels. While effective, the synthetically generated data does not perfectly\ncorrelate to the measured data leading to issues when training models solely on\nsynthetic data. We aim to use computational power as a substitution for this\nlack of quality measured data, by ensembling many models trained on synthetic\ndata. Synthetic data is also not complete, as we do not know what targets might\nbe present in a live environment. Therefore we need to have our ensembling\ntechniques account for these unknown targets by applying confuser rejection in\nwhich our models will reject unknown targets it is presented with, and only\nclassify those it has been trained on.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u6570\u636e\u6536\u96c6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u5e72\u6270\u7269\u62d2\u7edd\u673a\u5236\u4ee5\u5e94\u5bf9\u672a\u77e5\u76ee\u6807\u3002", "motivation": "\u7531\u4e8eSAR\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u6837\u672c\u6709\u9650\uff0c\u5408\u6210\u6570\u636e\u867d\u80fd\u8865\u5145\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u4e0e\u5b9e\u6d4b\u6570\u636e\u4e0d\u5b8c\u5168\u4e00\u81f4\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u53d7\u9650\u3002", "method": "\u5229\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u591a\u4e2a\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u7ed3\u5408\u8fd9\u4e9b\u6a21\u578b\uff0c\u540c\u65f6\u91c7\u7528\u5e72\u6270\u7269\u62d2\u7edd\u673a\u5236\u5904\u7406\u672a\u77e5\u76ee\u6807\u3002", "result": "\u901a\u8fc7\u96c6\u6210\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u5e72\u6270\u7269\u62d2\u7edd\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5b9e\u6d4b\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86SAR\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.00866", "pdf": "https://arxiv.org/pdf/2505.00866", "abs": "https://arxiv.org/abs/2505.00866", "authors": ["Viktor Kocur", "Charalambos Tzamos", "Yaqing Ding", "Zuzana Berger Haladova", "Torsten Sattler", "Zuzana Kukelova"], "title": "Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2410.05984", "summary": "Estimating the relative pose between two cameras is a fundamental step in\nmany applications such as Structure-from-Motion. The common approach to\nrelative pose estimation is to apply a minimal solver inside a RANSAC loop.\nHighly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras\nexhibit radial distortion. Not modeling radial distortion leads to\n(significantly) worse results. However, minimal radial distortion solvers are\nsignificantly more complex than pinhole solvers, both in terms of run-time and\nimplementation efforts. This paper compares radial distortion solvers with two\nsimple-to-implement approaches that do not use minimal radial distortion\nsolvers: The first approach combines an efficient pinhole solver with sampled\nradial undistortion parameters, where the sampled parameters are used for\nundistortion prior to applying the pinhole solver. The second approach uses a\nstate-of-the-art neural network to estimate the distortion parameters rather\nthan sampling them from a set of potential values. Extensive experiments on\nmultiple datasets, and different camera setups, show that complex minimal\nradial distortion solvers are not necessary in practice. We discuss under which\nconditions a simple sampling of radial undistortion parameters is preferable\nover calibrating cameras using a learning-based prior approach. Code and newly\ncreated benchmark for relative pose estimation under radial distortion are\navailable at https://github.com/kocurvik/rdnet.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u7b80\u5355\u5b9e\u73b0\u65b9\u6cd5\u4e0e\u4f20\u7edf\u5f84\u5411\u7578\u53d8\u6c42\u89e3\u5668\uff0c\u53d1\u73b0\u590d\u6742\u6c42\u89e3\u5668\u5728\u5b9e\u9645\u4e2d\u5e76\u975e\u5fc5\u8981\u3002", "motivation": "\u89e3\u51b3\u76f8\u673a\u5f84\u5411\u7578\u53d8\u5bf9\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u907f\u514d\u590d\u6742\u6c42\u89e3\u5668\u7684\u9ad8\u6210\u672c\u548c\u5b9e\u73b0\u96be\u5ea6\u3002", "method": "1. \u7ed3\u5408\u9ad8\u6548\u9488\u5b54\u6c42\u89e3\u5668\u4e0e\u91c7\u6837\u5f84\u5411\u7578\u53d8\u53c2\u6570\uff1b2. \u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u7578\u53d8\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u590d\u6742\u5f84\u5411\u7578\u53d8\u6c42\u89e3\u5668\u5728\u5b9e\u9645\u4e2d\u4e0d\u5fc5\u8981\uff0c\u5e76\u8ba8\u8bba\u4e86\u91c7\u6837\u65b9\u6cd5\u4e0e\u5b66\u4e60\u65b9\u6cd5\u7684\u9002\u7528\u6761\u4ef6\u3002", "conclusion": "\u7b80\u5355\u91c7\u6837\u65b9\u6cd5\u6216\u5b66\u4e60\u5148\u9a8c\u65b9\u6cd5\u53ef\u66ff\u4ee3\u590d\u6742\u6c42\u89e3\u5668\uff0c\u5177\u4f53\u9009\u62e9\u53d6\u51b3\u4e8e\u6761\u4ef6\u3002"}}
{"id": "2505.00938", "pdf": "https://arxiv.org/pdf/2505.00938", "abs": "https://arxiv.org/abs/2505.00938", "authors": ["Boyuan Meng", "Xiaohan Zhang", "Peilin Li", "Zhe Wu", "Yiming Li", "Wenkai Zhao", "Beinan Yu", "Hui-Liang Shen"], "title": "CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects\nacross different domains with limited class instances. Feature confusion,\nincluding object-background confusion and object-object confusion, presents\nsignificant challenges in both cross-domain and few-shot settings. In this\nwork, we introduce CDFormer, a cross-domain few-shot object detection\ntransformer against feature confusion, to address these challenges. The method\nspecifically tackles feature confusion through two key modules:\nobject-background distinguishing (OBD) and object-object distinguishing (OOD).\nThe OBD module leverages a learnable background token to differentiate between\nobjects and background, while the OOD module enhances the distinction between\nobjects of different classes. Experimental results demonstrate that CDFormer\noutperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%\nmAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,\nwhen fine-tuned.", "AI": {"tldr": "CDFormer\u662f\u4e00\u79cd\u9488\u5bf9\u8de8\u57df\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7279\u5f81\u6df7\u6dc6\u95ee\u9898\u7684Transformer\u65b9\u6cd5\uff0c\u901a\u8fc7OBD\u548cOOD\u6a21\u5757\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8de8\u57df\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff08CD-FSOD\uff09\u4e2d\uff0c\u7279\u5f81\u6df7\u6dc6\uff08\u5982\u5bf9\u8c61-\u80cc\u666f\u6df7\u6dc6\u548c\u5bf9\u8c61\u95f4\u6df7\u6dc6\uff09\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51faCDFormer\uff0c\u5305\u542b\u5bf9\u8c61-\u80cc\u666f\u533a\u5206\uff08OBD\uff09\u548c\u5bf9\u8c61\u95f4\u533a\u5206\uff08OOD\uff09\u6a21\u5757\uff0c\u5206\u522b\u89e3\u51b3\u4e24\u7c7b\u6df7\u6dc6\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cCDFormer\u57281/5/10 shot\u8bbe\u7f6e\u4e0b\u5206\u522b\u63d0\u534712.9%\u300111.0%\u548c10.4% mAP\u3002", "conclusion": "CDFormer\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u6df7\u6dc6\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.00787", "pdf": "https://arxiv.org/pdf/2505.00787", "abs": "https://arxiv.org/abs/2505.00787", "authors": ["Lucas N. Alegre", "Ana L. C. Bazzan", "Andr\u00e9 Barreto", "Bruno C. da Silva"], "title": "Constructing an Optimal Behavior Basis for the Option Keyboard", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": null, "summary": "Multi-task reinforcement learning aims to quickly identify solutions for new\ntasks with minimal or no additional interaction with the environment.\nGeneralized Policy Improvement (GPI) addresses this by combining a set of base\npolicies to produce a new one that is at least as good -- though not\nnecessarily optimal -- as any individual base policy. Optimality can be\nensured, particularly in the linear-reward case, via techniques that compute a\nConvex Coverage Set (CCS). However, these are computationally expensive and do\nnot scale to complex domains. The Option Keyboard (OK) improves upon GPI by\nproducing policies that are at least as good -- and often better. It achieves\nthis through a learned meta-policy that dynamically combines base policies.\nHowever, its performance critically depends on the choice of base policies.\nThis raises a key question: is there an optimal set of base policies -- an\noptimal behavior basis -- that enables zero-shot identification of optimal\nsolutions for any linear tasks? We solve this open problem by introducing a\nnovel method that efficiently constructs such an optimal behavior basis. We\nshow that it significantly reduces the number of base policies needed to ensure\noptimality in new tasks. We also prove that it is strictly more expressive than\na CCS, enabling particular classes of non-linear tasks to be solved optimally.\nWe empirically evaluate our technique in challenging domains and show that it\noutperforms state-of-the-art approaches, increasingly so as task complexity\nincreases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u6784\u5efa\u6700\u4f18\u884c\u4e3a\u57fa\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u786e\u4fdd\u65b0\u4efb\u52a1\u6700\u4f18\u6027\u6240\u9700\u7684\u57fa\u7b56\u7565\u6570\u91cf\uff0c\u5e76\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u662f\u901a\u8fc7\u6700\u5c0f\u5316\u73af\u5883\u4ea4\u4e92\u5feb\u901f\u89e3\u51b3\u65b0\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u5982GPI\u548cOK\u867d\u6709\u6548\uff0c\u4f46\u4f9d\u8d56\u4e8e\u57fa\u7b56\u7565\u7684\u9009\u62e9\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u6784\u5efa\u6700\u4f18\u884c\u4e3a\u57fa\u4ee5\u5b9e\u73b0\u96f6\u6837\u672c\u6700\u4f18\u89e3\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u9ad8\u6548\u6784\u5efa\u6700\u4f18\u884c\u4e3a\u57fa\uff0c\u786e\u4fdd\u65b0\u4efb\u52a1\u7684\u6700\u4f18\u6027\uff0c\u5e76\u8bc1\u660e\u5176\u8868\u8fbe\u80fd\u529b\u4f18\u4e8eCCS\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "\u63d0\u51fa\u7684\u6700\u4f18\u884c\u4e3a\u57fa\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.00975", "pdf": "https://arxiv.org/pdf/2505.00975", "abs": "https://arxiv.org/abs/2505.00975", "authors": ["Yeonsang Shin", "Jihwan Kim", "Yumin Song", "Kyungseung Lee", "Hyunhee Chung", "Taeyoung Na"], "title": "Generating Animated Layouts as Structured Text Representations", "categories": ["cs.CV"], "comment": "AI for Content Creation (AI4CC) Workshop at CVPR 2025", "summary": "Despite the remarkable progress in text-to-video models, achieving precise\ncontrol over text elements and animated graphics remains a significant\nchallenge, especially in applications such as video advertisements. To address\nthis limitation, we introduce Animated Layout Generation, a novel approach to\nextend static graphic layouts with temporal dynamics. We propose a Structured\nText Representation for fine-grained video control through hierarchical visual\nelements. To demonstrate the effectiveness of our approach, we present VAKER\n(Video Ad maKER), a text-to-video advertisement generation pipeline that\ncombines a three-stage generation process with Unstructured Text Reasoning for\nseamless integration with LLMs. VAKER fully automates video advertisement\ngeneration by incorporating dynamic layout trajectories for objects and\ngraphics across specific video frames. Through extensive evaluations, we\ndemonstrate that VAKER significantly outperforms existing methods in generating\nvideo advertisements. Project Page:\nhttps://yeonsangshin.github.io/projects/Vaker", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnimated Layout Generation\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u9759\u6001\u56fe\u5f62\u5e03\u5c40\u5b9e\u73b0\u65f6\u95f4\u52a8\u6001\u63a7\u5236\uff0c\u5e76\u5f00\u53d1\u4e86VAKER\u7cfb\u7edf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u5e7f\u544a\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u6587\u672c\u5143\u7d20\u548c\u52a8\u753b\u56fe\u5f62\u7684\u7cbe\u786e\u63a7\u5236\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u89c6\u9891\u5e7f\u544a\u7b49\u5e94\u7528\u4e2d\u3002", "method": "\u63d0\u51faAnimated Layout Generation\u65b9\u6cd5\uff0c\u7ed3\u5408Structured Text Representation\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u89c6\u9891\u63a7\u5236\uff0c\u5e76\u901a\u8fc7VAKER\u7cfb\u7edf\u5b9e\u73b0\u4e09\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\u4e0eLLMs\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "result": "VAKER\u5728\u89c6\u9891\u5e7f\u544a\u751f\u6210\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u5e03\u5c40\u8f68\u8ff9\u7684\u81ea\u52a8\u5316\u751f\u6210\u3002", "conclusion": "VAKER\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u5e03\u5c40\u8f68\u8ff9\u548c\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\uff0c\u4e3a\u89c6\u9891\u5e7f\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00793", "pdf": "https://arxiv.org/pdf/2505.00793", "abs": "https://arxiv.org/abs/2505.00793", "authors": ["Iurii Kemaev", "Dan A Calian", "Luisa M Zintgraf", "Gregory Farquhar", "Hado van Hasselt"], "title": "Scalable Meta-Learning via Mixed-Mode Differentiation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Gradient-based bilevel optimisation is a powerful technique with applications\nin hyperparameter optimisation, task adaptation, algorithm discovery,\nmeta-learning more broadly, and beyond. It often requires differentiating\nthrough the gradient-based optimisation process itself, leading to\n\"gradient-of-a-gradient\" calculations with computationally expensive\nsecond-order and mixed derivatives. While modern automatic differentiation\nlibraries provide a convenient way to write programs for calculating these\nderivatives, they oftentimes cannot fully exploit the specific structure of\nthese problems out-of-the-box, leading to suboptimal performance. In this\npaper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or\nMixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to\nconstruct more efficient and scalable computational graphs yielding over 10x\nmemory and up to 25% wall-clock time improvements over standard implementations\nin modern meta-learning setups.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMixFlow-MG\u7b97\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u6a21\u5f0f\u5fae\u5206\u4f18\u5316\u68af\u5ea6\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u5185\u5b58\u548c\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u68af\u5ea6\u53cc\u5c42\u4f18\u5316\u4e2d\u4e8c\u9636\u548c\u6df7\u5408\u5bfc\u6570\u8ba1\u7b97\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u63d0\u5347\u73b0\u4ee3\u81ea\u52a8\u5fae\u5206\u5e93\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faMixed-Flow Meta-Gradients\uff08MixFlow-MG\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u6df7\u5408\u6a21\u5f0f\u5fae\u5206\u6784\u5efa\u9ad8\u6548\u8ba1\u7b97\u56fe\u3002", "result": "\u5728\u5143\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u5185\u5b58\u8282\u7701\u8d85\u8fc710\u500d\uff0c\u8fd0\u884c\u65f6\u95f4\u63d0\u5347\u8fbe25%\u3002", "conclusion": "MixFlow-MG\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u68af\u5ea6\u53cc\u5c42\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2505.00980", "pdf": "https://arxiv.org/pdf/2505.00980", "abs": "https://arxiv.org/abs/2505.00980", "authors": ["Jiahuan Long", "Xin Zhou"], "title": "LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment", "categories": ["cs.CV"], "comment": null, "summary": "Monocular depth estimation provides an additional depth dimension to RGB\nimages, making it widely applicable in various fields such as virtual reality,\nautonomous driving and robotic navigation. However, existing depth estimation\nalgorithms often struggle to effectively balance performance and computational\nefficiency, which poses challenges for deployment on resource-constrained\ndevices. To address this, we propose LMDepth, a lightweight Mamba-based\nmonocular depth estimation network, designed to reconstruct high-precision\ndepth information while maintaining low computational overhead. Specifically,\nwe propose a modified pyramid spatial pooling module that serves as a\nmulti-scale feature aggregator and context extractor, ensuring global spatial\ninformation for accurate depth estimation. Moreover, we integrate multiple\ndepth Mamba blocks into the decoder. Designed with linear computations, the\nMamba Blocks enable LMDepth to efficiently decode depth information from global\nfeatures, providing a lightweight alternative to Transformer-based\narchitectures that depend on complex attention mechanisms. Extensive\nexperiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of\nour proposed LMDepth. Compared to previous lightweight depth estimation\nmethods, LMDepth achieves higher performance with fewer parameters and lower\ncomputational complexity (measured by GFLOPs). We further deploy LMDepth on an\nembedded platform with INT8 quantization, validating its practicality for\nreal-world edge applications.", "AI": {"tldr": "LMDepth\u662f\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u8f7b\u91cf\u7ea7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u65e8\u5728\u5e73\u8861\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u7b97\u6cd5\u96be\u4ee5\u5e73\u8861\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u91d1\u5b57\u5854\u7a7a\u95f4\u6c60\u5316\u6a21\u5757\u548c\u591a\u6df1\u5ea6Mamba\u5757\uff0c\u7ed3\u5408\u7ebf\u6027\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u89e3\u7801\u3002", "result": "\u5728NYUDv2\u548cKITTI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "LMDepth\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\uff0c\u9002\u5408\u8fb9\u7f18\u5e94\u7528\u3002"}}
{"id": "2505.00803", "pdf": "https://arxiv.org/pdf/2505.00803", "abs": "https://arxiv.org/abs/2505.00803", "authors": ["Jonathan Heins", "Darrell Whitley", "Pascal Kerschke"], "title": "To Repair or Not to Repair? Investigating the Importance of AB-Cycles for the State-of-the-Art TSP Heuristic EAX", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "The Edge Assembly Crossover (EAX) algorithm is the state-of-the-art heuristic\nfor solving the Traveling Salesperson Problem (TSP). It regularly outperforms\nother methods, such as the Lin-Kernighan-Helsgaun heuristic (LKH), across\ndiverse sets of TSP instances. Essentially, EAX employs a two-stage mechanism\nthat focuses on improving the current solutions, first, at the local and,\nsubsequently, at the global level. Although the second phase of the algorithm\nhas been thoroughly studied, configured, and refined in the past, in\nparticular, its first stage has hardly been examined.\n  In this paper, we thus focus on the first stage of EAX and introduce a novel\nmethod that quickly verifies whether the AB-cycles, generated during its\ninternal optimization procedure, yield valid tours -- or whether they need to\nbe repaired. Knowledge of the latter is also particularly relevant before\napplying other powerful crossover operators such as the Generalized Partition\nCrossover (GPX). Based on our insights, we propose and evaluate several\nimproved versions of EAX. According to our benchmark study across 10 000\ndifferent TSP instances, the most promising of our proposed EAX variants\ndemonstrates improved computational efficiency and solution quality on\npreviously rather difficult instances compared to the current state-of-the-art\nEAX algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684EAX\u7b97\u6cd5\u7b2c\u4e00\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5feb\u901f\u9a8c\u8bc1AB-cycles\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "EAX\u7b97\u6cd5\u7684\u7b2c\u4e00\u9636\u6bb5\u5c1a\u672a\u6df1\u5165\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f18\u5316\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5feb\u901f\u9a8c\u8bc1AB-cycles\u7684\u6709\u6548\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u6539\u8fdb\u4e86EAX\u7b97\u6cd5\u3002", "result": "\u572810,000\u4e2aTSP\u5b9e\u4f8b\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u6539\u8fdb\u7684EAX\u7b97\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6539\u8fdb\u7684EAX\u7b97\u6cd5\u5728\u89e3\u51b3\u56f0\u96be\u5b9e\u4f8b\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u4e3aTSP\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00998", "pdf": "https://arxiv.org/pdf/2505.00998", "abs": "https://arxiv.org/abs/2505.00998", "authors": ["Yu Hua", "Weiming Liu", "Gui Xu", "Yaqing Hou", "Yew-Soon Ong", "Qiang Zhang"], "title": "Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Human motion synthesis aims to generate plausible human motion sequences,\nwhich has raised widespread attention in computer animation. Recent score-based\ngenerative models (SGMs) have demonstrated impressive results on this task.\nHowever, their training process involves complex curvature trajectories,\nleading to unstable training process. In this paper, we propose a\nDeterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for\nhuman motion synthesis. DSDFM consists of two stages. The first human motion\nreconstruction stage aims to learn the latent space distribution of human\nmotions. The second diverse motion generation stage aims to build connections\nbetween the Gaussian distribution and the latent space distribution of human\nmotions, thereby enhancing the diversity and accuracy of the generated human\nmotions. This stage is achieved by the designed deterministic feature mapping\nprocedure with DerODE and stochastic diverse output generation procedure with\nDivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can\nenhance diversity without introducing additional training parameters.Through\nqualitative and quantitative experiments, DSDFM achieves state-of-the-art\nresults surpassing the latest methods, validating its superiority in human\nmotion synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u5230\u968f\u673a\u6027\u7684\u591a\u6837\u5316\u6f5c\u5728\u7279\u5f81\u6620\u5c04\uff08DSDFM\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eba\u7c7b\u8fd0\u52a8\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u5206\u6570\u751f\u6210\u6a21\u578b\uff08SGMs\uff09\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5206\u6570\u751f\u6210\u6a21\u578b\uff08SGMs\uff09\u7684\u8bad\u7ec3\u8fc7\u7a0b\u590d\u6742\u4e14\u4e0d\u7a33\u5b9a\uff0c\u4e14\u751f\u6210\u7684\u8fd0\u52a8\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "DSDFM\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u4eba\u7c7b\u8fd0\u52a8\u91cd\u5efa\u9636\u6bb5\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\u5206\u5e03\uff1b2\uff09\u591a\u6837\u5316\u8fd0\u52a8\u751f\u6210\u9636\u6bb5\u901a\u8fc7\u786e\u5b9a\u6027\u7279\u5f81\u6620\u5c04\uff08DerODE\uff09\u548c\u968f\u673a\u591a\u6837\u5316\u8f93\u51fa\u751f\u6210\uff08DivSDE\uff09\u8fde\u63a5\u9ad8\u65af\u5206\u5e03\u4e0e\u6f5c\u5728\u7a7a\u95f4\u5206\u5e03\u3002", "result": "DSDFM\u8bad\u7ec3\u7b80\u5355\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u5373\u53ef\u589e\u5f3a\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u4eba\u7c7b\u8fd0\u52a8\u5408\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DSDFM\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u751f\u6210\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u4eba\u7c7b\u8fd0\u52a8\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00808", "pdf": "https://arxiv.org/pdf/2505.00808", "abs": "https://arxiv.org/abs/2505.00808", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "title": "A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "15 pages (plus appendices), 2 figures", "summary": "Mechanistic Interpretability aims to understand neural networks through\ncausal explanations. We argue for the Explanatory View Hypothesis: that\nMechanistic Interpretability research is a principled approach to understanding\nmodels because neural networks contain implicit explanations which can be\nextracted and understood. We hence show that Explanatory Faithfulness, an\nassessment of how well an explanation fits a model, is well-defined. We propose\na definition of Mechanistic Interpretability (MI) as the practice of producing\nModel-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural\nnetworks, allowing us to distinguish MI from other interpretability paradigms\nand detail MI's inherent limits. We formulate the Principle of Explanatory\nOptimism, a conjecture which we argue is a necessary precondition for the\nsuccess of Mechanistic Interpretability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u89e3\u91ca\u6027\u89c6\u56fe\u5047\u8bf4\uff0c\u8ba4\u4e3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u80fd\u63d0\u53d6\u548c\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9690\u542b\u89e3\u91ca\uff0c\u5e76\u5b9a\u4e49\u4e86\u89e3\u91ca\u5fe0\u5b9e\u6027\u3002", "motivation": "\u63a2\u8ba8\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u7406\u8bba\u57fa\u7840\uff0c\u660e\u786e\u5176\u5b9a\u4e49\u548c\u754c\u9650\u3002", "method": "\u63d0\u51fa\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5b9a\u4e49\uff0c\u5305\u62ec\u6a21\u578b\u7ea7\u3001\u672c\u4f53\u6027\u3001\u56e0\u679c\u673a\u5236\u6027\u548c\u53ef\u8bc1\u4f2a\u6027\uff0c\u5e76\u9610\u8ff0\u89e3\u91ca\u6027\u4e50\u89c2\u539f\u5219\u3002", "result": "\u5b9a\u4e49\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u91ca\u6027\u4e50\u89c2\u539f\u5219\u4f5c\u4e3a\u5176\u6210\u529f\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "conclusion": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u662f\u4e00\u79cd\u6709\u539f\u5219\u7684\u7814\u7a76\u65b9\u6cd5\uff0c\u5176\u6210\u529f\u4f9d\u8d56\u4e8e\u89e3\u91ca\u6027\u4e50\u89c2\u539f\u5219\u3002"}}
{"id": "2505.01003", "pdf": "https://arxiv.org/pdf/2505.01003", "abs": "https://arxiv.org/abs/2505.01003", "authors": ["Kamel Aouaidjia", "Aofan Li", "Wenhao Zhang", "Chongsheng Zhang"], "title": "3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer", "categories": ["cs.CV"], "comment": "16 pages, 9 figures, 7 tables", "summary": "Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the\nprevailing techniques for 3D human pose estimation. However, Transformer-based\nmethods either ignore the spatial neighborhood relationships between the joints\nwhen used for skeleton representations or disregard the local temporal patterns\nof the local joint movements in skeleton sequence modeling, while GCN-based\nmethods often neglect the need for pose-specific representations. To address\nthese problems, we propose a new method that exploits the graph modeling\ncapability of GCN to represent each skeleton with multiple graphs of different\norders, incorporated with a newly introduced Graph Order Attention module that\ndynamically emphasizes the most representative orders for each joint. The\nresulting spatial features of the sequence are further processed using a\nproposed temporal Body Aware Transformer that models the global body feature\ndependencies in the sequence with awareness of the local inter-skeleton feature\ndependencies of joints. Given that our 3D pose output aligns with the central\n2D pose in the sequence, we improve the self-attention mechanism to be aware of\nthe central pose while diminishing its focus gradually towards the first and\nthe last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I\ndatasets demonstrate the effectiveness of the proposed method. Code and models\nare made available on Github.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GCN\u548cTransformer\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u56fe\u8868\u793a\u548c\u52a8\u6001\u6ce8\u610f\u529b\u6a21\u5757\u6539\u8fdb3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709Transformer\u548cGCN\u65b9\u6cd5\u57283D\u59ff\u6001\u4f30\u8ba1\u4e2d\u5206\u522b\u5ffd\u7565\u4e86\u7a7a\u95f4\u90bb\u57df\u5173\u7cfb\u6216\u5c40\u90e8\u65f6\u95f4\u6a21\u5f0f\uff0c\u4e14GCN\u7f3a\u4e4f\u59ff\u6001\u7279\u5b9a\u8868\u793a\u3002", "method": "\u4f7f\u7528\u591a\u9636\u56fe\u8868\u793a\u9aa8\u67b6\uff0c\u5f15\u5165Graph Order Attention\u6a21\u5757\u52a8\u6001\u9009\u62e9\u4ee3\u8868\u6027\u9636\u6570\uff0c\u5e76\u7ed3\u5408Body Aware Transformer\u5904\u7406\u65f6\u7a7a\u7279\u5f81\u3002", "result": "\u5728Human3.6m\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408GCN\u548cTransformer\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2505.00812", "pdf": "https://arxiv.org/pdf/2505.00812", "abs": "https://arxiv.org/abs/2505.00812", "authors": ["Kuan Zhang", "Chengliang Chai", "Jingzhe Xu", "Chi Zhang", "Ye Yuan", "Guoren Wang", "Lei Cao"], "title": "Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent studies indicate that deep neural networks degrade in generalization\nperformance under noisy supervision. Existing methods focus on isolating clean\nsubsets or correcting noisy labels, facing limitations such as high\ncomputational costs, heavy hyperparameter tuning process, and coarse-grained\noptimization. To address these challenges, we propose a novel two-stage noisy\nlearning framework that enables instance-level optimization through a\ndynamically weighted loss function, avoiding hyperparameter tuning. To obtain\nstable and accurate information about noise modeling, we introduce a simple yet\neffective metric, termed wrong event, which dynamically models the cleanliness\nand difficulty of individual samples while maintaining computational costs. Our\nframework first collects wrong event information and builds a strong base\nmodel. Then we perform noise-robust training on the base model, using a\nprobabilistic model to handle the wrong event information of samples.\nExperiments on five synthetic and real-world LNL benchmarks demonstrate our\nmethod surpasses state-of-the-art methods in performance, achieves a nearly 75%\nreduction in computational time and improves model scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u566a\u58f0\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u4f18\u5316\uff0c\u907f\u514d\u4e86\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u566a\u58f0\u76d1\u7763\u4e0b\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8d85\u53c2\u6570\u8c03\u4f18\u590d\u6742\u548c\u7c97\u7c92\u5ea6\u4f18\u5316\u7b49\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u9519\u8bef\u4e8b\u4ef6\u201d\u7684\u7b80\u5355\u6709\u6548\u5ea6\u91cf\uff0c\u52a8\u6001\u5efa\u6a21\u6837\u672c\u7684\u6e05\u6d01\u5ea6\u548c\u96be\u5ea6\uff1b\u6846\u67b6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u6536\u96c6\u9519\u8bef\u4e8b\u4ef6\u4fe1\u606f\u6784\u5efa\u57fa\u7840\u6a21\u578b\uff0c\u7136\u540e\u57fa\u4e8e\u6982\u7387\u6a21\u578b\u8fdb\u884c\u566a\u58f0\u9c81\u68d2\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684LNL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u8fd175%\uff0c\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u566a\u58f0\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.01016", "pdf": "https://arxiv.org/pdf/2505.01016", "abs": "https://arxiv.org/abs/2505.01016", "authors": ["Vishal Gandhi", "Sagar Gandhi"], "title": "Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The success of large pre-trained object detectors hinges on their\nadaptability to diverse downstream tasks. While fine-tuning is the standard\nadaptation method, specializing these models for challenging fine-grained\ndomains necessitates careful consideration of feature granularity. The critical\nquestion remains: how deeply should the pre-trained backbone be fine-tuned to\noptimize for the specialized task without incurring catastrophic forgetting of\nthe original general capabilities? Addressing this, we present a systematic\nempirical study evaluating the impact of fine-tuning depth. We adapt a standard\nYOLOv8n model to a custom, fine-grained fruit detection dataset by\nprogressively unfreezing backbone layers (freeze points at layers 22, 15, and\n10) and training. Performance was rigorously evaluated on both the target fruit\ndataset and, using a dual-head evaluation architecture, on the original COCO\nvalidation set. Our results demonstrate unequivocally that deeper fine-tuning\n(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\\%\nabsolute mAP50) on the fine-grained fruit task compared to only training the\nhead. Strikingly, this significant adaptation and specialization resulted in\nnegligible performance degradation (<0.1\\% absolute mAP difference) on the COCO\nbenchmark across all tested freeze levels. We conclude that adapting\nmid-to-late backbone features is highly effective for fine-grained\nspecialization. Critically, our results demonstrate this adaptation can be\nachieved without the commonly expected penalty of catastrophic forgetting,\npresenting a compelling case for exploring deeper fine-tuning strategies,\nparticularly when targeting complex domains or when maximizing specialized\nperformance is paramount.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u6df1\u5ea6\u5fae\u8c03\uff08\u89e3\u51bb\u81f3\u7b2c10\u5c42\uff09\u5728\u7ec6\u7c92\u5ea6\u6c34\u679c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08+10% mAP50\uff09\uff0c\u4e14\u5bf9\u539f\u59cbCOCO\u4efb\u52a1\u5f71\u54cd\u6781\u5c0f\uff08<0.1% mAP\u5dee\u5f02\uff09\u3002", "motivation": "\u63a2\u8ba8\u9884\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u7ec6\u7c92\u5ea6\u9886\u57df\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u901a\u8fc7\u9010\u6b65\u89e3\u51bbYOLOv8n\u6a21\u578b\u7684\u9aa8\u5e72\u5c42\uff0822\u300115\u300110\u5c42\uff09\u5e76\u8bad\u7ec3\uff0c\u8bc4\u4f30\u5fae\u8c03\u6df1\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u6df1\u5ea6\u5fae\u8c03\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u5bf9\u539f\u59cb\u4efb\u52a1\u5f71\u54cd\u53ef\u5ffd\u7565\u3002", "conclusion": "\u4e2d\u665a\u671f\u9aa8\u5e72\u7279\u5f81\u5fae\u8c03\u5bf9\u7ec6\u7c92\u5ea6\u4efb\u52a1\u9ad8\u6548\u4e14\u5b89\u5168\uff0c\u65e0\u9700\u62c5\u5fc3\u707e\u96be\u6027\u9057\u5fd8\u3002"}}
{"id": "2505.00817", "pdf": "https://arxiv.org/pdf/2505.00817", "abs": "https://arxiv.org/abs/2505.00817", "authors": ["Andrew Adiletta", "Berk Sunar"], "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models", "categories": ["cs.CR", "cs.AI", "K.6.5"], "comment": null, "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpill The Beans\u7684\u7f13\u5b58\u4fa7\u4fe1\u9053\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u6cc4\u9732\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u4ee4\u724c\uff0c\u63ed\u793a\u4e86LLM\u5728\u5171\u4eab\u786c\u4ef6\u4e0a\u7684\u65b0\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u5171\u4eab\u786c\u4ef6\u4e0a\u7684\u4fa7\u4fe1\u9053\u653b\u51fb\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u8981\u7814\u7a76\u5176\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5728\u540c\u4e00\u786c\u4ef6\u4e0a\u8fd0\u884c\u653b\u51fb\u8fdb\u7a0b\uff0c\u5229\u7528\u7f13\u5b58\u4fa7\u4fe1\u9053\u6280\u672f\uff08flush and reload\uff09\u76d1\u6d4bLLM\u5d4c\u5165\u5c42\u7684\u4ee4\u724c\u751f\u6210\uff0c\u68c0\u6d4b\u7f13\u5b58\u547d\u4e2d\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\uff0c\u80fd\u6cc4\u973280%-90%\u7684\u9ad8\u71b5API\u5bc6\u94a5\u621640%\u7684\u82f1\u6587\u6587\u672c\u4ee4\u724c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5bf9\u4f20\u7edf\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u6b64\u7c7b\u5a01\u80c1\u7684\u5efa\u8bae\u3002"}}
{"id": "2505.01032", "pdf": "https://arxiv.org/pdf/2505.01032", "abs": "https://arxiv.org/abs/2505.01032", "authors": ["Ruyu Yan", "Da-Qing Zhang"], "title": "Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection is crucial in image processing, but existing methods often\nproduce overly detailed edge maps, affecting clarity. Fixed-window statistical\ntesting faces issues like scale mismatch and computational redundancy. To\naddress these, we propose a novel Multi-scale Adaptive Independence\nTesting-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive\nStatistical Testing-based edge detection and denoising method that integrates a\nchannel attention mechanism with independence testing. A gradient-driven\nadaptive window strategy adjusts window sizes dynamically, improving detail\npreservation and noise suppression. EDD-MAIT achieves better robustness,\naccuracy, and efficiency, outperforming traditional and learning-based methods\non BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and\nreduced runtime. It also shows robustness against Gaussian noise, generating\naccurate and clean edge maps in noisy environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u72ec\u7acb\u6027\u6d4b\u8bd5\u7684\u8fb9\u7f18\u68c0\u6d4b\u4e0e\u53bb\u566a\u65b9\u6cd5\uff08EDD-MAIT\uff09\uff0c\u901a\u8fc7\u68af\u5ea6\u9a71\u52a8\u81ea\u9002\u5e94\u7a97\u53e3\u7b56\u7565\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u751f\u6210\u7684\u8fb9\u7f18\u56fe\u8fc7\u4e8e\u8be6\u7ec6\uff0c\u5f71\u54cd\u6e05\u6670\u5ea6\uff0c\u4e14\u56fa\u5b9a\u7a97\u53e3\u7edf\u8ba1\u6d4b\u8bd5\u5b58\u5728\u5c3a\u5ea6\u4e0d\u5339\u914d\u548c\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0c\u91c7\u7528\u68af\u5ea6\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u7a97\u53e3\u7b56\u7565\u52a8\u6001\u8c03\u6574\u7a97\u53e3\u5927\u5c0f\u3002", "result": "\u5728BSDS500\u548cBIPED\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0cF-score\u3001MSE\u3001PSNR\u7b49\u6307\u6807\u5747\u6709\u63d0\u5347\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u3002", "conclusion": "EDD-MAIT\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4ecd\u80fd\u751f\u6210\u51c6\u786e\u4e14\u5e72\u51c0\u7684\u8fb9\u7f18\u56fe\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.00841", "pdf": "https://arxiv.org/pdf/2505.00841", "abs": "https://arxiv.org/abs/2505.00841", "authors": ["Tao Li", "Ya-Ting Yang", "Yunian Pan", "Quanyan Zhu"], "title": "From Texts to Shields: Convergence of Large Language Models and Cybersecurity", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This report explores the convergence of large language models (LLMs) and\ncybersecurity, synthesizing interdisciplinary insights from network security,\nartificial intelligence, formal methods, and human-centered design. It examines\nemerging applications of LLMs in software and network security, 5G\nvulnerability analysis, and generative security engineering. The report\nhighlights the role of agentic LLMs in automating complex tasks, improving\noperational efficiency, and enabling reasoning-driven security analytics.\nSocio-technical challenges associated with the deployment of LLMs -- including\ntrust, transparency, and ethical considerations -- can be addressed through\nstrategies such as human-in-the-loop systems, role-specific training, and\nproactive robustness testing. The report further outlines critical research\nchallenges in ensuring interpretability, safety, and fairness in LLM-based\nsystems, particularly in high-stakes domains. By integrating technical advances\nwith organizational and societal considerations, this report presents a\nforward-looking research agenda for the secure and effective adoption of LLMs\nin cybersecurity.", "AI": {"tldr": "\u62a5\u544a\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u7f51\u7edc\u5b89\u5168\u7684\u878d\u5408\uff0c\u603b\u7ed3\u4e86\u5176\u5728\u8f6f\u4ef6\u548c\u7f51\u7edc\u5b89\u5168\u30015G\u6f0f\u6d1e\u5206\u6790\u53ca\u751f\u6210\u5f0f\u5b89\u5168\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u4fe1\u4efb\u3001\u900f\u660e\u5ea6\u548c\u4f26\u7406\u95ee\u9898\u7684\u7b56\u7565\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u5316\u4efb\u52a1\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u5176\u90e8\u7f72\u4e2d\u7684\u793e\u4f1a\u6280\u672f\u6311\u6218\u3002", "method": "\u7efc\u5408\u7f51\u7edc\u5b89\u5168\u3001\u4eba\u5de5\u667a\u80fd\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u548c\u4eba\u672c\u8bbe\u8ba1\u7684\u8de8\u5b66\u79d1\u89c6\u89d2\uff0c\u5206\u6790LLMs\u7684\u5e94\u7528\u53ca\u6311\u6218\u3002", "result": "LLMs\u80fd\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u3001\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u3001\u9488\u5bf9\u6027\u8bad\u7ec3\u548c\u9c81\u68d2\u6027\u6d4b\u8bd5\u89e3\u51b3\u4fe1\u4efb\u548c\u4f26\u7406\u95ee\u9898\u3002", "conclusion": "\u62a5\u544a\u63d0\u51fa\u4e86\u524d\u77bb\u6027\u7814\u7a76\u8bae\u7a0b\uff0c\u5f3a\u8c03\u6280\u672f\u8fdb\u5c55\u4e0e\u793e\u4f1a\u7ec4\u7ec7\u7ed3\u5408\uff0c\u4ee5\u786e\u4fddLLMs\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5b89\u5168\u6709\u6548\u5e94\u7528\u3002"}}
{"id": "2505.01040", "pdf": "https://arxiv.org/pdf/2505.01040", "abs": "https://arxiv.org/abs/2505.01040", "authors": ["Ru-yu Yan", "Da-Qing Zhang"], "title": "Edge Detection based on Channel Attention and Inter-region Independence Test", "categories": ["cs.CV"], "comment": null, "summary": "Existing edge detection methods often suffer from noise amplification and\nexcessive retention of non-salient details, limiting their applicability in\nhigh-precision industrial scenarios. To address these challenges, we propose\nCAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM)\nand Edge Detection via Independence Testing (EDIT). The CAM module adaptively\nenhances discriminative edge features through multi-channel fusion, while the\nEDIT module employs region-wise statistical independence analysis (using\nFisher's exact test and chi-square test) to suppress uncorrelated\nnoise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate\nstate-of-the-art performance. Among the nine comparison algorithms, the\nF-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of\n19.2\\% to 26.5\\% over traditional methods (Canny, CannySR), and better than the\nlatest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations\nfurther reveal a 2.2\\% PSNR improvement under Gaussian noise compared to\nbaseline methods. Qualitative results exhibit cleaner edge maps with reduced\nartifacts, demonstrating its potential for high-precision industrial\napplications.", "AI": {"tldr": "CAM-EDIT\u662f\u4e00\u79cd\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u548c\u72ec\u7acb\u6027\u6d4b\u8bd5\u7684\u8fb9\u7f18\u68c0\u6d4b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u566a\u58f0\u653e\u5927\u548c\u975e\u663e\u8457\u7ec6\u8282\u4fdd\u7559\u8fc7\u591a\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u7cbe\u5ea6\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "CAM-EDIT\u6574\u5408\u4e86\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff08CAM\uff09\u548c\u591a\u901a\u9053\u878d\u5408\u589e\u5f3a\u8fb9\u7f18\u7279\u5f81\uff0c\u4ee5\u53ca\u901a\u8fc7\u72ec\u7acb\u6027\u6d4b\u8bd5\uff08EDIT\uff09\u6291\u5236\u65e0\u5173\u566a\u58f0\u3002", "result": "\u5728BSDS500\u548cNYUDv2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cF-measure\u5206\u6570\u5206\u522b\u8fbe\u52300.635\u548c0.460\uff0c\u566a\u58f0\u9c81\u68d2\u6027\u8bc4\u4f30\u4e2dPSNR\u63d0\u53472.2%\u3002", "conclusion": "CAM-EDIT\u5728\u9ad8\u7cbe\u5ea6\u5de5\u4e1a\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u751f\u6210\u66f4\u5e72\u51c0\u7684\u8fb9\u7f18\u56fe\u5e76\u51cf\u5c11\u4f2a\u5f71\u3002"}}
{"id": "2505.00843", "pdf": "https://arxiv.org/pdf/2505.00843", "abs": "https://arxiv.org/abs/2505.00843", "authors": ["Jinsheng Pan", "Xiaogeng Liu", "Chaowei Xiao"], "title": "OET: Optimization-based prompt injection Evaluation Toolkit", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation, enabling their widespread\nadoption across various domains. However, their susceptibility to prompt\ninjection attacks poses significant security risks, as adversarial inputs can\nmanipulate model behavior and override intended instructions. Despite numerous\ndefense strategies, a standardized framework to rigorously evaluate their\neffectiveness, especially under adaptive adversarial scenarios, is lacking. To\naddress this gap, we introduce OET, an optimization-based evaluation toolkit\nthat systematically benchmarks prompt injection attacks and defenses across\ndiverse datasets using an adaptive testing framework. Our toolkit features a\nmodular workflow that facilitates adversarial string generation, dynamic attack\nexecution, and comprehensive result analysis, offering a unified platform for\nassessing adversarial robustness. Crucially, the adaptive testing framework\nleverages optimization methods with both white-box and black-box access to\ngenerate worst-case adversarial examples, thereby enabling strict red-teaming\nevaluations. Extensive experiments underscore the limitations of current\ndefense mechanisms, with some models remaining susceptible even after\nimplementing security enhancements.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86OET\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u548c\u9632\u5fa1\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u81ea\u9002\u5e94\u5bf9\u6297\u573a\u666f\u4e0b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6613\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0cOET\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "OET\u91c7\u7528\u57fa\u4e8e\u4f18\u5316\u7684\u8bc4\u4f30\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u751f\u6210\u5bf9\u6297\u5b57\u7b26\u4e32\u3001\u6267\u884c\u52a8\u6001\u653b\u51fb\u5e76\u5206\u6790\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u9632\u5fa1\u673a\u5236\u5b58\u5728\u5c40\u9650\u6027\uff0c\u90e8\u5206\u6a21\u578b\u5373\u4f7f\u589e\u5f3a\u540e\u4ecd\u6613\u53d7\u653b\u51fb\u3002", "conclusion": "OET\u4e3a\u8bc4\u4f30\u5bf9\u6297\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u9632\u5fa1\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.01050", "pdf": "https://arxiv.org/pdf/2505.01050", "abs": "https://arxiv.org/abs/2505.01050", "authors": ["Kai Hu", "Weichen Yu", "Li Zhang", "Alexander Robey", "Andy Zou", "Chengming Xu", "Haoqi Hu", "Matt Fredrikson"], "title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision Large Language Models (VLLMs) are increasingly deployed to offer\nadvanced capabilities on inputs comprising both text and images. While prior\nresearch has shown that adversarial attacks can transfer from open-source to\nproprietary black-box models in text-only and vision-only contexts, the extent\nand effectiveness of such vulnerabilities remain underexplored for VLLMs. We\npresent a comprehensive analysis demonstrating that targeted adversarial\nexamples are highly transferable to widely-used proprietary VLLMs such as\nGPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to\ninduce specific attacker-chosen interpretations of visual information, such as\nmisinterpreting hazardous content as safe, overlooking sensitive or restricted\nmaterial, or generating detailed incorrect responses aligned with the\nattacker's intent. Furthermore, we discover that universal perturbations --\nmodifications applicable to a wide set of images -- can consistently induce\nthese misinterpretations across multiple proprietary VLLMs. Our experimental\nresults on object recognition, visual question answering, and image captioning\nshow that this vulnerability is common across current state-of-the-art models,\nand underscore an urgent need for robust mitigations to ensure the safe and\nsecure deployment of VLLMs.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u9488\u5bf9\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLMs\uff09\u7684\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u9ad8\u5ea6\u53ef\u8fc1\u79fb\u6027\uff0c\u80fd\u591f\u8bf1\u5bfc\u6a21\u578b\u4ea7\u751f\u653b\u51fb\u8005\u9884\u671f\u7684\u9519\u8bef\u8f93\u51fa\uff0c\u7a81\u663e\u4e86\u5176\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u63a2\u7d22VLLMs\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u8f93\u5165\uff08\u6587\u672c\u548c\u56fe\u50cf\uff09\u573a\u666f\u4e2d\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u751f\u6210\u9488\u5bf9\u6027\u5bf9\u6297\u6837\u672c\u548c\u901a\u7528\u6270\u52a8\uff0c\u6d4b\u8bd5\u5176\u5728\u591a\u4e2a\u4e3b\u6d41VLLMs\uff08\u5982GPT-4o\u3001Claude\u548cGemini\uff09\u4e0a\u7684\u8fc1\u79fb\u6027\u548c\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u6297\u6027\u653b\u51fb\u80fd\u6709\u6548\u8bf1\u5bfc\u6a21\u578b\u9519\u8bef\u89e3\u8bfb\u89c6\u89c9\u4fe1\u606f\uff0c\u4e14\u901a\u7528\u6270\u52a8\u5728\u591a\u6a21\u578b\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u5f53\u524dVLLMs\u666e\u904d\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e9f\u9700\u5f00\u53d1\u9c81\u68d2\u6027\u66f4\u5f3a\u7684\u9632\u5fa1\u673a\u5236\u4ee5\u786e\u4fdd\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2505.00850", "pdf": "https://arxiv.org/pdf/2505.00850", "abs": "https://arxiv.org/abs/2505.00850", "authors": ["Xinlin Li", "Osama Hanna", "Christina Fragouli", "Suhas Diggavi"], "title": "ICQuant: Index Coding enables Low-bit LLM Quantization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid deployment of Large Language Models (LLMs) highlights the need for\nefficient low-bit post-training quantization (PTQ), due to their high memory\ncosts. A key challenge in weight quantization is the presence of outliers,\nwhich inflate quantization ranges and lead to large errors. While a number of\noutlier suppression techniques have been proposed, they either: fail to\neffectively shrink the quantization range, or incur (relatively) high bit\noverhead. In this paper, we present ICQuant, a novel framework that leverages\noutlier statistics to design an efficient index coding scheme for outlier-aware\nweight-only quantization. Compared to existing outlier suppression techniques\nrequiring $\\approx 1$ bit overhead to halve the quantization range, ICQuant\nrequires only $\\approx 0.3$ bits; a significant saving in extreme compression\nregimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing\nquantizers to eliminate outliers, improving the quantization quality. Using\njust 2.3 bits per weight and simple scalar quantizers, ICQuant improves the\nzero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150%\nrelative to QTIP and QuIP#; and it achieves comparable performance to the\nbest-known fine-tuned quantizer (PV-tuning) without fine-tuning.", "AI": {"tldr": "ICQuant\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4f4e\u6bd4\u7279\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u79bb\u7fa4\u503c\u8bbe\u8ba1\u9ad8\u6548\u7684\u7d22\u5f15\u7f16\u7801\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u91cf\u5316\u8303\u56f4\uff0c\u4ec5\u9700\u7ea60.3\u6bd4\u7279\u5f00\u9500\u5373\u53ef\u5c06\u91cf\u5316\u8303\u56f4\u51cf\u534a\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9ad8\u5185\u5b58\u6210\u672c\u9700\u8981\u9ad8\u6548\u7684\u4f4e\u6bd4\u7279\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\uff0c\u800c\u6743\u91cd\u91cf\u5316\u4e2d\u7684\u79bb\u7fa4\u503c\u95ee\u9898\u5bfc\u81f4\u91cf\u5316\u8303\u56f4\u6269\u5927\u548c\u8bef\u5dee\u589e\u52a0\u3002", "method": "ICQuant\u5229\u7528\u79bb\u7fa4\u503c\u7edf\u8ba1\u8bbe\u8ba1\u7d22\u5f15\u7f16\u7801\u65b9\u6848\uff0c\u652f\u6301\u79bb\u7fa4\u503c\u611f\u77e5\u7684\u4ec5\u6743\u91cd\u91cf\u5316\uff0c\u517c\u5bb9\u73b0\u6709\u91cf\u5316\u5668\u3002", "result": "\u57282.3\u6bd4\u7279/\u6743\u91cd\u4e0b\uff0cICQuant\u5c062-bit Llama3-70B\u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\u63d0\u5347130%\u548c150%\uff08\u76f8\u5bf9\u4e8eQTIP\u548cQuIP#\uff09\uff0c\u6027\u80fd\u5ab2\u7f8e\u65e0\u9700\u5fae\u8c03\u7684\u6700\u4f73\u91cf\u5316\u5668\uff08PV-tuning\uff09\u3002", "conclusion": "ICQuant\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u6027\u5f3a\u7684\u91cf\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u6bd4\u7279\u91cf\u5316\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u6781\u7aef\u538b\u7f29\u573a\u666f\u3002"}}
{"id": "2505.01057", "pdf": "https://arxiv.org/pdf/2505.01057", "abs": "https://arxiv.org/abs/2505.01057", "authors": ["Boris Kriuk", "Matey Yordanov"], "title": "GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation", "categories": ["cs.CV"], "comment": "13 pages, 3 figures, 3 tables", "summary": "This paper introduces GeloVec, a new CNN-based attention smoothing framework\nfor semantic segmentation that addresses critical limitations in conventional\napproaches. While existing attention-backed segmentation methods suffer from\nboundary instability and contextual discontinuities during feature mapping, our\nframework implements a higher-dimensional geometric smoothing method to\nestablish a robust manifold relationships between visually coherent regions.\nGeloVec combines modified Chebyshev distance metrics with multispatial\ntransformations to enhance segmentation accuracy through stabilized feature\nextraction. The core innovation lies in the adaptive sampling weights system\nthat calculates geometric distances in n-dimensional feature space, achieving\nsuperior edge preservation while maintaining intra-class homogeneity. The\nmultispatial transformation matrix incorporates tensorial projections with\northogonal basis vectors, creating more discriminative feature representations\nwithout sacrificing computational efficiency. Experimental validation across\nmultiple benchmark datasets demonstrates significant improvements in\nsegmentation performance, with mean Intersection over Union (mIoU) gains of\n2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets\nrespectively compared to state-of-the-art methods. GeloVec's mathematical\nfoundation in Riemannian geometry provides theoretical guarantees on\nsegmentation stability. Importantly, our framework maintains computational\nefficiency through parallelized implementation of geodesic transformations and\nexhibits strong generalization capabilities across disciplines due to the\nabsence of information loss during transformations.", "AI": {"tldr": "GeloVec\u662f\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u6ce8\u610f\u529b\u5e73\u6ed1\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u7ef4\u51e0\u4f55\u5e73\u6ed1\u65b9\u6cd5\u89e3\u51b3\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u8fb9\u754c\u4e0d\u7a33\u5b9a\u548c\u4e0a\u4e0b\u6587\u4e0d\u8fde\u7eed\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u5206\u5272\u65b9\u6cd5\u5728\u7279\u5f81\u6620\u5c04\u4e2d\u5b58\u5728\u8fb9\u754c\u4e0d\u7a33\u5b9a\u548c\u4e0a\u4e0b\u6587\u4e0d\u8fde\u7eed\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684Chebyshev\u8ddd\u79bb\u5ea6\u91cf\u548c\u591a\u7a7a\u95f4\u53d8\u6362\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u6743\u91cd\u7cfb\u7edf\u8ba1\u7b97n\u7ef4\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u8ddd\u79bb\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cmIoU\u5206\u522b\u63d0\u5347\u4e862.1%\u30012.7%\u548c2.4%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "GeloVec\u901a\u8fc7\u9ece\u66fc\u51e0\u4f55\u7406\u8bba\u4fdd\u8bc1\u5206\u5272\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2505.00871", "pdf": "https://arxiv.org/pdf/2505.00871", "abs": "https://arxiv.org/abs/2505.00871", "authors": ["Jun Takamatsu", "Atsushi Kanehira", "Kazuhiro Sasabuchi", "Naoki Wake", "Katsushi Ikeuchi"], "title": "IK Seed Generator for Dual-Arm Human-like Physicality Robot with Mobile Base", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 12 figures, 4 tables", "summary": "Robots are strongly expected as a means of replacing human tasks. If a robot\nhas a human-like physicality, the possibility of replacing human tasks\nincreases. In the case of household service robots, it is desirable for them to\nbe on a human-like size so that they do not become excessively large in order\nto coexist with humans in their operating environment. However, robots with\nsize limitations tend to have difficulty solving inverse kinematics (IK) due to\nmechanical limitations, such as joint angle limitations. Conversely, if the\ndifficulty coming from this limitation could be mitigated, one can expect that\nthe use of such robots becomes more valuable. In numerical IK solver, which is\ncommonly used for robots with higher degrees-of-freedom (DOF), the solvability\nof IK depends on the initial guess given to the solver. Thus, this paper\nproposes a method for generating a good initial guess for a numerical IK solver\ngiven the target hand configuration. For the purpose, we define the goodness of\nan initial guess using the scaled Jacobian matrix, which can calculate the\nmanipulability index considering the joint limits. These two factors are\nrelated to the difficulty of solving IK. We generate the initial guess by\noptimizing the goodness using the genetic algorithm (GA). To enumerate much\npossible IK solutions, we use the reachability map that represents the\nreachable area of the robot hand in the arm-base coordinate system. We conduct\nquantitative evaluation and prove that using an initial guess that is judged to\nbe better using the goodness value increases the probability that IK is solved.\nFinally, as an application of the proposed method, we show that by generating\ngood initial guesses for IK a robot actually achieves three typical scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u521d\u59cb\u731c\u6d4b\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6570\u503c\u9006\u8fd0\u52a8\u5b66\uff08IK\uff09\u6c42\u89e3\u5668\u7684\u6210\u529f\u7387\uff0c\u4ece\u800c\u5e2e\u52a9\u5c3a\u5bf8\u53d7\u9650\u7684\u673a\u5668\u4eba\u66f4\u597d\u5730\u5b8c\u6210\u4efb\u52a1\u3002", "motivation": "\u5c3a\u5bf8\u53d7\u9650\u7684\u673a\u5668\u4eba\u56e0\u673a\u68b0\u9650\u5236\uff08\u5982\u5173\u8282\u89d2\u5ea6\u9650\u5236\uff09\u96be\u4ee5\u89e3\u51b3\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u4ef7\u503c\u3002\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u731c\u6d4b\uff0c\u53ef\u4ee5\u63d0\u5347IK\u6c42\u89e3\u7684\u6210\u529f\u7387\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u521d\u59cb\u731c\u6d4b\uff0c\u57fa\u4e8e\u7f29\u653e\u96c5\u53ef\u6bd4\u77e9\u9635\u5b9a\u4e49\u521d\u59cb\u731c\u6d4b\u7684\u201c\u4f18\u52a3\u201d\uff0c\u5e76\u7ed3\u5408\u53ef\u8fbe\u6027\u5730\u56fe\u679a\u4e3e\u53ef\u80fd\u7684IK\u89e3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4f18\u5316\u540e\u7684\u521d\u59cb\u731c\u6d4b\u663e\u8457\u63d0\u9ad8\u4e86IK\u6c42\u89e3\u7684\u6210\u529f\u7387\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u4e09\u79cd\u5178\u578b\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5c3a\u5bf8\u53d7\u9650\u673a\u5668\u4eba\u7684IK\u6c42\u89e3\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.01064", "pdf": "https://arxiv.org/pdf/2505.01064", "abs": "https://arxiv.org/abs/2505.01064", "authors": ["Hari Chandana Kuchibhotla", "Sai Srinivas Kancheti", "Abbavaram Gowtham Reddy", "Vineeth N Balasubramanian"], "title": "Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs", "categories": ["cs.CV", "cs.LG"], "comment": "preprint; earlier version accepted at NeurIPS 2024 Workshop on\n  Adaptive Foundation Models", "summary": "Fine-grained Visual Recognition (FGVR) involves distinguishing between\nvisually similar categories, which is inherently challenging due to subtle\ninter-class differences and the need for large, expert-annotated datasets. In\ndomains like medical imaging, such curated datasets are unavailable due to\nissues like privacy concerns and high annotation costs. In such scenarios\nlacking labeled data, an FGVR model cannot rely on a predefined set of training\nlabels, and hence has an unconstrained output space for predictions. We refer\nto this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict\nlabels from an unconstrained output space without prior label information.\nWhile recent Multimodal Large Language Models (MLLMs) show potential for\nVF-FGVR, querying these models for each test input is impractical because of\nhigh costs and prohibitive inference times. To address these limitations, we\nintroduce \\textbf{Nea}rest-Neighbor Label \\textbf{R}efinement (NeaR), a novel\napproach that fine-tunes a downstream CLIP model using labels generated by an\nMLLM. Our approach constructs a weakly supervised dataset from a small,\nunlabeled training set, leveraging MLLMs for label generation. NeaR is designed\nto handle the noise, stochasticity, and open-endedness inherent in labels\ngenerated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeaR\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\uff08VF-FGVR\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u751f\u6210\u6807\u7b7e\u5e76\u5fae\u8c03CLIP\u6a21\u578b\u3002", "motivation": "\u5728\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u9886\u57df\uff08\u5982\u533b\u5b66\u5f71\u50cf\uff09\uff0c\u4f20\u7edf\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u65b9\u6cd5\u65e0\u6cd5\u9002\u7528\uff0c\u800c\u76f4\u63a5\u4f7f\u7528MLLM\u6210\u672c\u9ad8\u6602\u4e14\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51faNeaR\u65b9\u6cd5\uff0c\u5229\u7528MLLM\u4e3a\u5c11\u91cf\u65e0\u6807\u6ce8\u8bad\u7ec3\u96c6\u751f\u6210\u6807\u7b7e\uff0c\u6784\u5efa\u5f31\u76d1\u7763\u6570\u636e\u96c6\uff0c\u5e76\u5fae\u8c03\u4e0b\u6e38CLIP\u6a21\u578b\u3002", "result": "NeaR\u80fd\u591f\u6709\u6548\u5904\u7406MLLM\u751f\u6210\u6807\u7b7e\u7684\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3aVF-FGVR\u4efb\u52a1\u5efa\u7acb\u4e86\u65b0\u7684\u9ad8\u6548\u57fa\u51c6\u3002", "conclusion": "NeaR\u4e3a\u65e0\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00886", "pdf": "https://arxiv.org/pdf/2505.00886", "abs": "https://arxiv.org/abs/2505.00886", "authors": ["Milad Sabouri", "Masoud Mansoury", "Kun Lin", "Bamshad Mobasher"], "title": "Towards Explainable Temporal User Profiling with LLMs", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Accurately modeling user preferences is vital not only for improving\nrecommendation performance but also for enhancing transparency in recommender\nsystems. Conventional user profiling methods, such as averaging item\nembeddings, often overlook the evolving, nuanced nature of user interests,\nparticularly the interplay between short-term and long-term preferences. In\nthis work, we leverage large language models (LLMs) to generate natural\nlanguage summaries of users' interaction histories, distinguishing recent\nbehaviors from more persistent tendencies. Our framework not only models\ntemporal user preferences but also produces natural language profiles that can\nbe used to explain recommendations in an interpretable manner. These textual\nprofiles are encoded via a pre-trained model, and an attention mechanism\ndynamically fuses the short-term and long-term embeddings into a comprehensive\nuser representation. Beyond boosting recommendation accuracy over multiple\nbaselines, our approach naturally supports explainability: the interpretable\ntext summaries and attention weights can be exposed to end users, offering\ninsights into why specific items are suggested. Experiments on real-world\ndatasets underscore both the performance gains and the promise of generating\nclearer, more transparent justifications for content-based recommendations.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u7684\u81ea\u7136\u8bed\u8a00\u6458\u8981\uff0c\u533a\u5206\u77ed\u671f\u548c\u957f\u671f\u504f\u597d\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u7528\u6237\u753b\u50cf\u65b9\u6cd5\uff08\u5982\u5e73\u5747\u9879\u76ee\u5d4c\u5165\uff09\u5ffd\u7565\u4e86\u7528\u6237\u5174\u8da3\u7684\u52a8\u6001\u6027\u548c\u590d\u6742\u6027\uff0c\u5c24\u5176\u662f\u77ed\u671f\u4e0e\u957f\u671f\u504f\u597d\u7684\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7LLM\u751f\u6210\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u7684\u81ea\u7136\u8bed\u8a00\u6458\u8981\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u878d\u5408\u77ed\u671f\u4e0e\u957f\u671f\u5d4c\u5165\uff0c\u5f62\u6210\u7efc\u5408\u7528\u6237\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u63a8\u8350\u51c6\u786e\u6027\uff0c\u8fd8\u652f\u6301\u751f\u6210\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u7406\u7531\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6458\u8981\u548c\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.01079", "pdf": "https://arxiv.org/pdf/2505.01079", "abs": "https://arxiv.org/abs/2505.01079", "authors": ["Daneul Kim", "Jaeah Lee", "Jaesik Park"], "title": "Improving Editability in Image Generation with Layer-wise Memory", "categories": ["cs.CV", "eess.IV"], "comment": "CVPR 2025. Project page :\n  https://carpedkm.github.io/projects/improving_edit/index.html", "summary": "Most real-world image editing tasks require multiple sequential edits to\nachieve desired results. Current editing approaches, primarily designed for\nsingle-object modifications, struggle with sequential editing: especially with\nmaintaining previous edits along with adapting new objects naturally into the\nexisting content. These limitations significantly hinder complex editing\nscenarios where multiple objects need to be modified while preserving their\ncontextual relationships. We address this fundamental challenge through two key\nproposals: enabling rough mask inputs that preserve existing content while\nnaturally integrating new elements and supporting consistent editing across\nmultiple modifications. Our framework achieves this through layer-wise memory,\nwhich stores latent representations and prompt embeddings from previous edits.\nWe propose Background Consistency Guidance that leverages memorized latents to\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\nthat ensures natural adaptation to existing content. To evaluate our method, we\npresent a new benchmark dataset incorporating semantic alignment metrics and\ninteractive editing scenarios. Through comprehensive experiments, we\ndemonstrate superior performance in iterative image editing tasks with minimal\nuser effort, requiring only rough masks while maintaining high-quality results\nthroughout multiple editing steps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u591a\u6b65\u56fe\u50cf\u7f16\u8f91\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u8bb0\u5fc6\u548c\u4e00\u81f4\u6027\u5f15\u5bfc\u4fdd\u6301\u7f16\u8f91\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6b65\u7f16\u8f91\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u4fdd\u6301\u5148\u524d\u7f16\u8f91\u5185\u5bb9\u5e76\u81ea\u7136\u878d\u5165\u65b0\u5bf9\u8c61\u3002", "method": "\u91c7\u7528\u5c42\u8bb0\u5fc6\u5b58\u50a8\u6f5c\u5728\u8868\u793a\u548c\u63d0\u793a\u5d4c\u5165\uff0c\u63d0\u51fa\u80cc\u666f\u4e00\u81f4\u6027\u5f15\u5bfc\u548c\u591a\u67e5\u8be2\u89e3\u8026\u6280\u672f\u3002", "result": "\u5728\u8fed\u4ee3\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u9700\u7c97\u7565\u63a9\u7801\u5373\u53ef\u4fdd\u6301\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6b65\u56fe\u50cf\u7f16\u8f91\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2505.00887", "pdf": "https://arxiv.org/pdf/2505.00887", "abs": "https://arxiv.org/abs/2505.00887", "authors": ["Xi Chen", "Yateng Tang", "Jiarong Xu", "Jiawei Zhang", "Siwei Zhang", "Sijia Peng", "Xuehao Zheng", "Yun Xiong"], "title": "Rethinking Time Encoding via Learnable Transformation Functions", "categories": ["cs.LG", "cs.AI"], "comment": "26 pages, 13 figures, 10 tables", "summary": "Effectively modeling time information and incorporating it into applications\nor models involving chronologically occurring events is crucial. Real-world\nscenarios often involve diverse and complex time patterns, which pose\nsignificant challenges for time encoding methods. While previous methods focus\non capturing time patterns, many rely on specific inductive biases, such as\nusing trigonometric functions to model periodicity. This narrow focus on\nsingle-pattern modeling makes them less effective in handling the diversity and\ncomplexities of real-world time patterns. In this paper, we investigate to\nimprove the existing commonly used time encoding methods and introduce\nLearnable Transformation-based Generalized Time Encoding (LeTE). We propose\nusing deep function learning techniques to parameterize non-linear\ntransformations in time encoding, making them learnable and capable of modeling\ngeneralized time patterns, including diverse and complex temporal dynamics. By\nenabling learnable transformations, LeTE encompasses previous methods as\nspecific cases and allows seamless integration into a wide range of tasks.\nThrough extensive experiments across diverse domains, we demonstrate the\nversatility and effectiveness of LeTE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLeTE\u7684\u53ef\u5b66\u4e60\u65f6\u95f4\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u51fd\u6570\u5b66\u4e60\u6280\u672f\u53c2\u6570\u5316\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u4ee5\u5904\u7406\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u65f6\u95f4\u6a21\u5f0f\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u65f6\u95f4\u6a21\u5f0f\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u5bf9\u4f20\u7edf\u65f6\u95f4\u7f16\u7801\u65b9\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u6a21\u5f0f\u5efa\u6a21\uff0c\u96be\u4ee5\u5e94\u5bf9\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u51fd\u6570\u5b66\u4e60\u6280\u672f\u53c2\u6570\u5316\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u63d0\u51faLearnable Transformation-based Generalized Time Encoding (LeTE)\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLeTE\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "LeTE\u80fd\u591f\u6db5\u76d6\u73b0\u6709\u65b9\u6cd5\u5e76\u7075\u6d3b\u5e94\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\uff0c\u4e3a\u65f6\u95f4\u7f16\u7801\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01091", "pdf": "https://arxiv.org/pdf/2505.01091", "abs": "https://arxiv.org/abs/2505.01091", "authors": ["Daniele Molino", "Francesco di Feola", "Linlin Shen", "Paolo Soda", "Valerio Guarrasi"], "title": "Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2501.04614", "summary": "Generative models have revolutionized Artificial Intelligence (AI),\nparticularly in multimodal applications. However, adapting these models to the\nmedical domain poses unique challenges due to the complexity of medical data\nand the stringent need for clinical accuracy. In this work, we introduce a\nframework specifically designed for multimodal medical data generation. By\nenabling the generation of multi-view chest X-rays and their associated\nclinical report, it bridges the gap between general-purpose vision-language\nmodels and the specialized requirements of healthcare. Leveraging the MIMIC-CXR\ndataset, the proposed framework shows superior performance in generating\nhigh-fidelity images and semantically coherent reports. Our quantitative\nevaluation reveals significant results in terms of FID and BLEU scores,\nshowcasing the quality of the generated data. Notably, our framework achieves\ncomparable or even superior performance compared to real data on downstream\ndisease classification tasks, underlining its potential as a tool for medical\nresearch and diagnostics. This study highlights the importance of\ndomain-specific adaptations in enhancing the relevance and utility of\ngenerative models for clinical applications, paving the way for future\nadvancements in synthetic multimodal medical data generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u751f\u6210\u7684\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u591a\u89c6\u89d2\u80f8\u90e8X\u5149\u53ca\u5176\u4e34\u5e8a\u62a5\u544a\uff0c\u586b\u8865\u4e86\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u533b\u7597\u9886\u57df\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u533b\u5b66\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u4e34\u5e8a\u51c6\u786e\u6027\u8981\u6c42\u4f7f\u5f97\u901a\u7528\u751f\u6210\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u533b\u7597\u9886\u57df\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528MIMIC-CXR\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\u548c\u8bed\u4e49\u8fde\u8d2f\u7684\u4e34\u5e8a\u62a5\u544a\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\uff0c\u751f\u6210\u7684\u6570\u636e\u5728FID\u548cBLEU\u5206\u6570\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u4e0b\u6e38\u75be\u75c5\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e0e\u771f\u5b9e\u6570\u636e\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u9886\u57df\u7279\u5b9a\u9002\u914d\u5bf9\u63d0\u5347\u751f\u6210\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u76f8\u5173\u6027\u548c\u5b9e\u7528\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u5408\u6210\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u751f\u6210\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.00913", "pdf": "https://arxiv.org/pdf/2505.00913", "abs": "https://arxiv.org/abs/2505.00913", "authors": ["Han Wang", "Adam White", "Martha White"], "title": "Fine-Tuning without Performance Degradation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fine-tuning policies learned offline remains a major challenge in application\ndomains. Monotonic performance improvement during \\emph{fine-tuning} is often\nchallenging, as agents typically experience performance degradation at the\nearly fine-tuning stage. The community has identified multiple difficulties in\nfine-tuning a learned network online, however, the majority of progress has\nfocused on improving learning efficiency during fine-tuning. In practice, this\ncomes at a serious cost during fine-tuning: initially, agent performance\ndegrades as the agent explores and effectively overrides the policy learned\noffline. We show across a range of settings, many offline-to-online algorithms\nexhibit either (1) performance degradation or (2) slow learning (sometimes\neffectively no improvement) during fine-tuning. We introduce a new fine-tuning\nalgorithm, based on an algorithm called Jump Start, that gradually allows more\nexploration based on online estimates of performance. Empirically, this\napproach achieves fast fine-tuning and significantly reduces performance\ndegradations compared with existing algorithms designed to do the same.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u7b97\u6cd5Jump Start\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u63a2\u7d22\u6765\u51cf\u5c11\u6027\u80fd\u4e0b\u964d\uff0c\u76f8\u6bd4\u73b0\u6709\u7b97\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u79bb\u7ebf\u5b66\u4e60\u7b56\u7565\u5728\u5fae\u8c03\u65f6\u6027\u80fd\u4e0b\u964d\u6216\u5b66\u4e60\u7f13\u6162\u662f\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8eJump Start\u7b97\u6cd5\uff0c\u6839\u636e\u5728\u7ebf\u6027\u80fd\u4f30\u8ba1\u9010\u6b65\u589e\u52a0\u63a2\u7d22\u3002", "result": "\u65b0\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u6027\u80fd\u4e0b\u964d\uff0c\u5b9e\u73b0\u5feb\u901f\u5fae\u8c03\u3002", "conclusion": "Jump Start\u7b97\u6cd5\u5728\u5fae\u8c03\u9636\u6bb5\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.01096", "pdf": "https://arxiv.org/pdf/2505.01096", "abs": "https://arxiv.org/abs/2505.01096", "authors": ["Marco Salm\u00e8", "Rosa Sicilia", "Paolo Soda", "Valerio Guarrasi"], "title": "Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The integration of artificial intelligence in healthcare has opened new\nhorizons for improving medical diagnostics and patient care. However,\nchallenges persist in developing systems capable of generating accurate and\ncontextually relevant radiology reports, particularly in low-resource\nlanguages. In this study, we present a comprehensive benchmark to evaluate the\nperformance of instruction-tuned Vision-Language Models (VLMs) in the\nspecialized task of radiology report generation across three low-resource\nlanguages: Italian, German, and Spanish. Employing the LLaVA architectural\nframework, we conducted a systematic evaluation of pre-trained models utilizing\ngeneral datasets, domain-specific datasets, and low-resource language-specific\ndatasets. In light of the unavailability of models that possess prior knowledge\nof both the medical domain and low-resource languages, we analyzed various\nadaptations to determine the most effective approach for these contexts. The\nresults revealed that language-specific models substantially outperformed both\ngeneral and domain-specific models in generating radiology reports, emphasizing\nthe critical role of linguistic adaptation. Additionally, models fine-tuned\nwith medical terminology exhibited enhanced performance across all languages\ncompared to models with generic knowledge, highlighting the importance of\ndomain-specific training. We also explored the influence of the temperature\nparameter on the coherence of report generation, providing insights for optimal\nmodel settings. Our findings highlight the importance of tailored language and\ndomain-specific training for improving the quality and accuracy of radiological\nreports in multilingual settings. This research not only advances our\nunderstanding of VLMs adaptability in healthcare but also points to significant\navenues for future investigations into model tuning and language-specific\nadaptations.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u6307\u4ee4\u8c03\u4f18\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u610f\u5927\u5229\u8bed\u3001\u5fb7\u8bed\u3001\u897f\u73ed\u7259\u8bed\uff09\u4e2d\u751f\u6210\u653e\u5c04\u5b66\u62a5\u544a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8bed\u8a00\u548c\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u751f\u6210\u51c6\u786e\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528LLaVA\u67b6\u6784\uff0c\u7cfb\u7edf\u8bc4\u4f30\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u901a\u7528\u3001\u9886\u57df\u7279\u5b9a\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e0d\u540c\u9002\u5e94\u65b9\u6cd5\u3002", "result": "\u8bed\u8a00\u7279\u5b9a\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u533b\u5b66\u672f\u8bed\u5fae\u8c03\u63d0\u5347\u6027\u80fd\uff0c\u6e29\u5ea6\u53c2\u6570\u5f71\u54cd\u62a5\u544a\u8fde\u8d2f\u6027\u3002", "conclusion": "\u8bed\u8a00\u548c\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u5bf9\u63d0\u5347\u591a\u8bed\u8a00\u653e\u5c04\u5b66\u62a5\u544a\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8c03\u4f18\u548c\u8bed\u8a00\u9002\u5e94\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2505.00917", "pdf": "https://arxiv.org/pdf/2505.00917", "abs": "https://arxiv.org/abs/2505.00917", "authors": ["Tian Bai", "Yue Zhao", "Xiang Yu", "Archer Y. Yang"], "title": "Multivariate Conformal Selection", "categories": ["stat.ME", "cs.AI", "cs.LG", "stat.ML"], "comment": "25 pages, 4 figures. Accepted to ICML 2025", "summary": "Selecting high-quality candidates from large datasets is critical in\napplications such as drug discovery, precision medicine, and alignment of large\nlanguage models (LLMs). While Conformal Selection (CS) provides rigorous\nuncertainty quantification, it is limited to univariate responses and scalar\ncriteria. To address this issue, we propose Multivariate Conformal Selection\n(mCS), a generalization of CS designed for multivariate response settings. Our\nmethod introduces regional monotonicity and employs multivariate nonconformity\nscores to construct conformal p-values, enabling finite-sample False Discovery\nRate (FDR) control. We present two variants: mCS-dist, using distance-based\nscores, and mCS-learn, which learns optimal scores via differentiable\noptimization. Experiments on simulated and real-world datasets demonstrate that\nmCS significantly improves selection power while maintaining FDR control,\nestablishing it as a robust framework for multivariate selection tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u53d8\u91cf\u9002\u5e94\u6027\u9009\u62e9\u65b9\u6cd5\uff08mCS\uff09\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u5355\u53d8\u91cf\u9002\u5e94\u6027\u9009\u62e9\uff08CS\uff09\uff0c\u9002\u7528\u4e8e\u591a\u53d8\u91cf\u54cd\u5e94\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u836f\u7269\u53d1\u73b0\u3001\u7cbe\u51c6\u533b\u5b66\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7b49\u5e94\u7528\u4e2d\uff0c\u4ece\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u9ad8\u8d28\u91cf\u5019\u9009\u8005\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\uff08CS\uff09\u4ec5\u9002\u7528\u4e8e\u5355\u53d8\u91cf\u54cd\u5e94\u548c\u6807\u91cf\u6807\u51c6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u53d8\u91cf\u9700\u6c42\u3002", "method": "\u63d0\u51famCS\u65b9\u6cd5\uff0c\u5f15\u5165\u533a\u57df\u5355\u8c03\u6027\u5e76\u4f7f\u7528\u591a\u53d8\u91cf\u975e\u9002\u5e94\u6027\u8bc4\u5206\u6784\u5efa\u9002\u5e94\u6027p\u503c\uff0c\u5b9e\u73b0\u6709\u9650\u6837\u672c\u7684\u5047\u53d1\u73b0\u7387\uff08FDR\uff09\u63a7\u5236\u3002\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1a\u57fa\u4e8e\u8ddd\u79bb\u7684mCS-dist\u548c\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u5b66\u4e60\u6700\u4f18\u8bc4\u5206\u7684mCS-learn\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cmCS\u5728\u4fdd\u6301FDR\u63a7\u5236\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u9009\u62e9\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u53d8\u91cf\u9009\u62e9\u4efb\u52a1\u3002", "conclusion": "mCS\u662f\u4e00\u4e2a\u9c81\u68d2\u7684\u591a\u53d8\u91cf\u9009\u62e9\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.01104", "pdf": "https://arxiv.org/pdf/2505.01104", "abs": "https://arxiv.org/abs/2505.01104", "authors": ["Do Huu Dat", "Nam Hyeonu", "Po-Yuan Mao", "Tae-Hyun Oh"], "title": "VSC: Visual Search Compositional Text-to-Image Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models have shown impressive capabilities in\ngenerating realistic visuals from natural-language prompts, yet they often\nstruggle with accurately binding attributes to corresponding objects,\nespecially in prompts containing multiple attribute-object pairs. This\nchallenge primarily arises from the limitations of commonly used text encoders,\nsuch as CLIP, which can fail to encode complex linguistic relationships and\nmodifiers effectively. Existing approaches have attempted to mitigate these\nissues through attention map control during inference and the use of layout\ninformation or fine-tuning during training, yet they face performance drops\nwith increased prompt complexity. In this work, we introduce a novel\ncompositional generation method that leverages pairwise image embeddings to\nimprove attribute-object binding. Our approach decomposes complex prompts into\nsub-prompts, generates corresponding images, and computes visual prototypes\nthat fuse with text embeddings to enhance representation. By applying\nsegmentation-based localization training, we address cross-attention\nmisalignment, achieving improved accuracy in binding multiple attributes to\nobjects. Our approaches outperform existing compositional text-to-image\ndiffusion models on the benchmark T2I CompBench, achieving better image\nquality, evaluated by humans, and emerging robustness under scaling number of\nbinding pairs in the prompt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ec4\u5408\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6210\u5bf9\u56fe\u50cf\u5d4c\u5165\u6765\u6539\u8fdb\u5c5e\u6027-\u5bf9\u8c61\u7ed1\u5b9a\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u591a\u5c5e\u6027-\u5bf9\u8c61\u5bf9\u63d0\u793a\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u63d0\u793a\u4e2d\u96be\u4ee5\u51c6\u786e\u7ed1\u5b9a\u5c5e\u6027\u4e0e\u5bf9\u8c61\uff0c\u4e3b\u8981\u7531\u4e8e\u6587\u672c\u7f16\u7801\u5668\u7684\u5c40\u9650\u6027\u3002", "method": "\u5206\u89e3\u590d\u6742\u63d0\u793a\u4e3a\u5b50\u63d0\u793a\uff0c\u751f\u6210\u5bf9\u5e94\u56fe\u50cf\u5e76\u8ba1\u7b97\u89c6\u89c9\u539f\u578b\uff0c\u7ed3\u5408\u6587\u672c\u5d4c\u5165\u589e\u5f3a\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u5206\u5272\u7684\u5b9a\u4f4d\u8bad\u7ec3\u89e3\u51b3\u6ce8\u610f\u529b\u9519\u4f4d\u3002", "result": "\u5728T2I CompBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u56fe\u50cf\u8d28\u91cf\u66f4\u9ad8\uff0c\u4e14\u5728\u591a\u7ed1\u5b9a\u5bf9\u63d0\u793a\u4e0b\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5c5e\u6027-\u5bf9\u8c61\u7ed1\u5b9a\u7684\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u63d0\u793a\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2505.00918", "pdf": "https://arxiv.org/pdf/2505.00918", "abs": "https://arxiv.org/abs/2505.00918", "authors": ["Shubham Vaishnav", "Praveen Kumar Donta", "Sindri Magn\u00fasson"], "title": "Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI"], "comment": null, "summary": "The last few decades have witnessed a rapid increase in IoT devices owing to\ntheir wide range of applications, such as smart healthcare monitoring systems,\nsmart cities, and environmental monitoring. A critical task in IoT networks is\nsensing and transmitting information over the network. The IoT nodes gather\ndata by sensing the environment and then transmit this data to a destination\nnode via multi-hop communication, following some routing protocols. These\nprotocols are usually designed to optimize possibly contradictory objectives,\nsuch as maximizing packet delivery ratio and energy efficiency. While most\nliterature has focused on optimizing a static objective that remains unchanged,\nmany real-world IoT applications require adapting to rapidly shifting\npriorities. For example, in monitoring systems, some transmissions are\ntime-critical and require a high priority on low latency, while other\ntransmissions are less urgent and instead prioritize energy efficiency. To meet\nsuch dynamic demands, we propose novel dynamic and distributed routing based on\nmultiobjective Q-learning that can adapt to changes in preferences in\nreal-time. Our algorithm builds on ideas from both multi-objective optimization\nand Q-learning. We also propose a novel greedy interpolation policy scheme to\ntake near-optimal decisions for unexpected preference changes. The proposed\nscheme can approximate and utilize the Pareto-efficient solutions for dynamic\npreferences, thus utilizing past knowledge to adapt to unpredictable\npreferences quickly during runtime. Simulation results show that the proposed\nscheme outperforms state-of-the-art algorithms for various exploration\nstrategies, preference variation patterns, and important metrics like overall\nreward, energy efficiency, and packet delivery ratio.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u76ee\u6807Q\u5b66\u4e60\u7684\u52a8\u6001\u5206\u5e03\u5f0f\u8def\u7531\u7b97\u6cd5\uff0c\u4ee5\u9002\u5e94\u7269\u8054\u7f51\u4e2d\u5b9e\u65f6\u53d8\u5316\u7684\u4f18\u5148\u7ea7\u9700\u6c42\u3002", "motivation": "\u7269\u8054\u7f51\u5e94\u7528\u4e2d\u9700\u8981\u52a8\u6001\u8c03\u6574\u4f18\u5148\u7ea7\uff08\u5982\u4f4e\u5ef6\u8fdf\u6216\u80fd\u6548\uff09\uff0c\u800c\u73b0\u6709\u8def\u7531\u534f\u8bae\u901a\u5e38\u4f18\u5316\u9759\u6001\u76ee\u6807\uff0c\u65e0\u6cd5\u6ee1\u8db3\u52a8\u6001\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u591a\u76ee\u6807\u4f18\u5316\u548cQ\u5b66\u4e60\uff0c\u63d0\u51fa\u52a8\u6001\u5206\u5e03\u5f0f\u8def\u7531\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u8d2a\u5a6a\u63d2\u503c\u7b56\u7565\u4ee5\u5feb\u901f\u9002\u5e94\u504f\u597d\u53d8\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u6574\u4f53\u5956\u52b1\u3001\u80fd\u6548\u548c\u5305\u4ea4\u4ed8\u7387\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6848\u80fd\u5feb\u901f\u9002\u5e94\u52a8\u6001\u504f\u597d\uff0c\u4e3a\u7269\u8054\u7f51\u8def\u7531\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01109", "pdf": "https://arxiv.org/pdf/2505.01109", "abs": "https://arxiv.org/abs/2505.01109", "authors": ["Ali Mammadov", "Loic Le Folgoc", "Julien Adam", "Anne Buronfosse", "Gilles Hayem", "Guillaume Hocquet", "Pietro Gori"], "title": "Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication in the Journal of Medical Imaging (SPIE)", "summary": "Multiple Instance Learning (MIL) has emerged as the best solution for Whole\nSlide Image (WSI) classification. It consists of dividing each slide into\npatches, which are treated as a bag of instances labeled with a global label.\nMIL includes two main approaches: instance-based and embedding-based. In the\nformer, each patch is classified independently, and then the patch scores are\naggregated to predict the bag label. In the latter, bag classification is\nperformed after aggregating patch embeddings. Even if instance-based methods\nare naturally more interpretable, embedding-based MILs have usually been\npreferred in the past due to their robustness to poor feature extractors.\nHowever, recently, the quality of feature embeddings has drastically increased\nusing self-supervised learning (SSL). Nevertheless, many authors continue to\nendorse the superiority of embedding-based MIL. To investigate this further, we\nconduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6\nself-supervised methods with 4 backbones, 4 foundation models, and various\npathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL\nmethods never used before in the pathology domain. Through these extensive\nexperiments, we show that with a good SSL feature extractor, simple\ninstance-based MILs, with very few parameters, obtain similar or better\nperformance than complex, state-of-the-art (SOTA) embedding-based MIL methods,\nsetting new SOTA results on the BRACS and Camelyon16 datasets. Since simple\ninstance-based MIL methods are naturally more interpretable and explainable to\nclinicians, our results suggest that more effort should be put into\nwell-adapted SSL methods for WSI rather than into complex embedding-based MIL\nmethods.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u9ad8\u8d28\u91cf\u81ea\u76d1\u7763\u5b66\u4e60\u7279\u5f81\u63d0\u53d6\u5668\u652f\u6301\u4e0b\uff0c\u7b80\u5355\u7684\u57fa\u4e8e\u5b9e\u4f8b\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\uff08MIL\uff09\u6027\u80fd\u4f18\u4e8e\u590d\u6742\u7684\u57fa\u4e8e\u5d4c\u5165\u7684MIL\u65b9\u6cd5\uff0c\u4e14\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63a2\u8ba8\u5728\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u57fa\u4e8e\u5b9e\u4f8b\u7684MIL\u65b9\u6cd5\u662f\u5426\u56e0\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7279\u5f81\u63d0\u53d6\u5668\u7684\u8fdb\u6b65\u800c\u4f18\u4e8e\u57fa\u4e8e\u5d4c\u5165\u7684MIL\u65b9\u6cd5\u3002", "method": "\u8fdb\u884c\u4e86710\u6b21\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e8610\u79cdMIL\u7b56\u7565\u30016\u79cdSSL\u65b9\u6cd5\u30014\u79cd\u57fa\u7840\u6a21\u578b\u53ca\u591a\u79cd\u75c5\u7406\u9002\u5e94\u6280\u672f\uff0c\u5e76\u5f15\u5165\u4e864\u79cd\u65b0\u7684\u57fa\u4e8e\u5b9e\u4f8b\u7684MIL\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u9ad8\u8d28\u91cfSSL\u7279\u5f81\u63d0\u53d6\u5668\u65f6\uff0c\u7b80\u5355\u7684\u57fa\u4e8e\u5b9e\u4f8b\u7684MIL\u65b9\u6cd5\u6027\u80fd\u4e0e\u590d\u6742SOTA\u57fa\u4e8e\u5d4c\u5165\u7684MIL\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u5e76\u5728BRACS\u548cCamelyon16\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u65b0SOTA\u7ed3\u679c\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u7814\u7a76\u5e94\u66f4\u5173\u6ce8\u9002\u5e94\u75c5\u7406\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u800c\u975e\u590d\u6742\u7684\u57fa\u4e8e\u5d4c\u5165\u7684MIL\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.00931", "pdf": "https://arxiv.org/pdf/2505.00931", "abs": "https://arxiv.org/abs/2505.00931", "authors": ["Timur Jaganov", "John Blake", "Juli\u00e1n Villegas", "Nicholas Carr"], "title": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 8 Figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u52a8\u6001\u8bc4\u4f30\uff08DA\uff09\u4e2d\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u5f00\u53d1\u4e86DynaWrite\u5e94\u7528\u6d4b\u8bd5\u591a\u79cdLLMs\uff0c\u53d1\u73b0GPT-4o\u5728\u751f\u6210\u52a8\u6001\u53cd\u9988\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22LLMs\u80fd\u5426\u6269\u5c55\u52a8\u6001\u8bc4\u4f30\uff0c\u4ee5\u652f\u6301\u66f4\u5927\u89c4\u6a21\u7684\u8bed\u8a00\u5b66\u4e60\u3002", "method": "\u5f00\u53d1DynaWrite\u5e94\u7528\uff0c\u6d4b\u8bd521\u79cdLLMs\uff0c\u7b5b\u9009\u51faGPT-4o\u548cNeural Chat\u8fdb\u4e00\u6b65\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "result": "GPT-4o\u5728\u53cd\u9988\u8d28\u91cf\u548c\u7cfb\u7edf\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8eNeural Chat\uff0c\u9002\u5408\u6269\u5c55\u52a8\u6001\u8bc4\u4f30\u3002", "conclusion": "LLMs\uff08\u5c24\u5176\u662fGPT-4o\uff09\u53ef\u6709\u6548\u6269\u5c55\u52a8\u6001\u8bc4\u4f30\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2505.01172", "pdf": "https://arxiv.org/pdf/2505.01172", "abs": "https://arxiv.org/abs/2505.01172", "authors": ["Jiangtong Tan", "Hu Yu", "Jie Huang", "Jie Xiao", "Feng Zhao"], "title": "FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Long video generation involves generating extended videos using models\ntrained on short videos, suffering from distribution shifts due to varying\nframe counts. It necessitates the use of local information from the original\nshort frames to enhance visual and motion quality, and global information from\nthe entire long frames to ensure appearance consistency. Existing training-free\nmethods struggle to effectively integrate the benefits of both, as appearance\nand motion in videos are closely coupled, leading to motion inconsistency and\nvisual quality. In this paper, we reveal that global and local information can\nbe precisely decoupled into consistent appearance and motion intensity\ninformation by applying Principal Component Analysis (PCA), allowing for\nrefined complementary integration of global consistency and local quality. With\nthis insight, we propose FreePCA, a training-free long video generation\nparadigm based on PCA that simultaneously achieves high consistency and\nquality. Concretely, we decouple consistent appearance and motion intensity\nfeatures by measuring cosine similarity in the principal component space.\nCritically, we progressively integrate these features to preserve original\nquality and ensure smooth transitions, while further enhancing consistency by\nreusing the mean statistics of the initial noise. Experiments demonstrate that\nFreePCA can be applied to various video diffusion models without requiring\ntraining, leading to substantial improvements. Code is available at\nhttps://github.com/JosephTiTan/FreePCA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePCA\u7684\u65e0\u8bad\u7ec3\u957f\u89c6\u9891\u751f\u6210\u65b9\u6cd5FreePCA\uff0c\u901a\u8fc7\u89e3\u8026\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u957f\u89c6\u9891\u751f\u6210\u56e0\u5e27\u6570\u53d8\u5316\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\uff0c\u73b0\u6709\u65e0\u8bad\u7ec3\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\uff0c\u5f71\u54cd\u89c6\u89c9\u548c\u8fd0\u52a8\u8d28\u91cf\u3002", "method": "\u5229\u7528PCA\u5c06\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u89e3\u8026\u4e3a\u4e00\u81f4\u5916\u89c2\u548c\u8fd0\u52a8\u5f3a\u5ea6\u7279\u5f81\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u548c\u6e10\u8fdb\u5f0f\u7279\u5f81\u6574\u5408\u5b9e\u73b0\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreePCA\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "conclusion": "FreePCA\u901a\u8fc7PCA\u89e3\u8026\u548c\u6574\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u7684\u957f\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2505.00932", "pdf": "https://arxiv.org/pdf/2505.00932", "abs": "https://arxiv.org/abs/2505.00932", "authors": ["Yin Huang", "Yongqi Dong", "Youhua Tang", "Alvaro Garc\u00eda Hernandez"], "title": "A Self-Supervised Transformer for Unusable Shared Bike Detection", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "6 pages, 5 figures, under review by the 2025 IEEE International\n  Conference on Intelligent Transportation Systems (IEEE ITSC 2025)", "summary": "The rapid expansion of bike-sharing systems (BSS) has greatly improved urban\n\"last-mile\" connectivity, yet large-scale deployments face escalating\noperational challenges, particularly in detecting faulty bikes. Existing\ndetection approaches either rely on static model-based thresholds that overlook\ndynamic spatiotemporal (ST) usage patterns or employ supervised learning\nmethods that struggle with label scarcity and class imbalance. To address these\nlimitations, this paper proposes a novel Self-Supervised Transformer\n(SSTransformer) framework for automatically detecting unusable shared bikes,\nleveraging ST features extracted from GPS trajectories and trip records. The\nmodel incorporates a self-supervised pre-training strategy to enhance its\nfeature extraction capabilities, followed by fine-tuning for efficient status\nrecognition. In the pre-training phase, the Transformer encoder learns\ngeneralized representations of bike movement via a self-supervised objective;\nin the fine-tuning phase, the encoder is adapted to a downstream binary\nclassification task. Comprehensive experiments on a real-world dataset of\n10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate\nthat SSTransformer significantly outperforms traditional machine learning,\nensemble learning, and deep learning baselines, achieving the best accuracy\n(97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the\neffectiveness of self-supervised Transformer on ST data for capturing complex\nanomalies in BSS, paving the way toward more reliable and scalable maintenance\nsolutions for shared mobility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763Transformer\u6846\u67b6\uff08SSTransformer\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u5171\u4eab\u5355\u8f66\u6545\u969c\uff0c\u901a\u8fc7\u63d0\u53d6GPS\u8f68\u8ff9\u548c\u884c\u7a0b\u8bb0\u5f55\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\uff08BSS\uff09\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u9762\u4e34\u8fd0\u8425\u6311\u6218\uff0c\u5c24\u5176\u662f\u6545\u969c\u5355\u8f66\u68c0\u6d4b\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u52a8\u6001\u65f6\u7a7a\u6a21\u5f0f\uff0c\u8981\u4e48\u53d7\u9650\u4e8e\u6807\u7b7e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51faSSTransformer\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528Transformer\u7f16\u7801\u5668\u5b66\u4e60\u5355\u8f66\u8fd0\u52a8\u7684\u901a\u7528\u8868\u793a\uff0c\u5e76\u9002\u5e94\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5728\u6210\u90fd\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cSSTransformer\u5728\u51c6\u786e\u7387\uff0897.81%\uff09\u3001\u7cbe\u786e\u7387\uff080.8889\uff09\u548cF1\u5206\u6570\uff080.9358\uff09\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u76d1\u7763Transformer\u5728\u65f6\u7a7a\u6570\u636e\u4e0a\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u5f02\u5e38\uff0c\u4e3a\u5171\u4eab\u51fa\u884c\u63d0\u4f9b\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u7ef4\u62a4\u65b9\u6848\u3002"}}
{"id": "2505.01182", "pdf": "https://arxiv.org/pdf/2505.01182", "abs": "https://arxiv.org/abs/2505.01182", "authors": ["Ziyan Guo", "Haoxuan Qu", "Hossein Rahmani", "Dewen Soh", "Ping Hu", "Qiuhong Ke", "Jun Liu"], "title": "TSTMotion: Training-free Scene-awarenText-to-motion Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICME2025", "summary": "Text-to-motion generation has recently garnered significant research\ninterest, primarily focusing on generating human motion sequences in blank\nbackgrounds. However, human motions commonly occur within diverse 3D scenes,\nwhich has prompted exploration into scene-aware text-to-motion generation\nmethods. Yet, existing scene-aware methods often rely on large-scale\nground-truth motion sequences in diverse 3D scenes, which poses practical\nchallenges due to the expensive cost. To mitigate this challenge, we are the\nfirst to propose a \\textbf{T}raining-free \\textbf{S}cene-aware\n\\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that\nefficiently empowers pre-trained blank-background motion generators with the\nscene-aware capability. Specifically, conditioned on the given 3D scene and\ntext description, we adopt foundation models together to reason, predict and\nvalidate a scene-aware motion guidance. Then, the motion guidance is\nincorporated into the blank-background motion generators with two\nmodifications, resulting in scene-aware text-driven motion sequences. Extensive\nexperiments demonstrate the efficacy and generalizability of our proposed\nframework. We release our code in \\href{https://tstmotion.github.io/}{Project\nPage}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u573a\u666f\u611f\u77e5\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u6846\u67b6TSTMotion\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u573a\u666f\u611f\u77e5\u8fd0\u52a8\u6307\u5bfc\u751f\u6210\u573a\u666f\u611f\u77e5\u7684\u8fd0\u52a8\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u573a\u666f\u611f\u77e5\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u771f\u5b9e\u8fd0\u52a8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u573a\u666f\u611f\u77e5\u8fd0\u52a8\u6307\u5bfc\uff0c\u5e76\u5c06\u5176\u878d\u5165\u7a7a\u767d\u80cc\u666f\u8fd0\u52a8\u751f\u6210\u5668\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u6709\u6548\u4e14\u901a\u7528\u3002", "conclusion": "TSTMotion\u4e3a\u573a\u666f\u611f\u77e5\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00935", "pdf": "https://arxiv.org/pdf/2505.00935", "abs": "https://arxiv.org/abs/2505.00935", "authors": ["Roberto Bigazzi"], "title": "Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Ph.D. Dissertation", "summary": "The increase in available computing power and the Deep Learning revolution\nhave allowed the exploration of new topics and frontiers in Artificial\nIntelligence research. A new field called Embodied Artificial Intelligence,\nwhich places at the intersection of Computer Vision, Robotics, and Decision\nMaking, has been gaining importance during the last few years, as it aims to\nfoster the development of smart autonomous robots and their deployment in\nsociety. The recent availability of large collections of 3D models for\nphotorealistic robotic simulation has allowed faster and safe training of\nlearning-based agents for millions of frames and a careful evaluation of their\nbehavior before deploying the models on real robotic platforms. These\nintelligent agents are intended to perform a certain task in a possibly unknown\nenvironment. To this end, during the training in simulation, the agents learn\nto perform continuous interactions with the surroundings, such as gathering\ninformation from the environment, encoding and extracting useful cues for the\ntask, and performing actions towards the final goal; where every action of the\nagent influences the interactions. This dissertation follows the complete\ncreation process of embodied agents for indoor environments, from their concept\nto their implementation and deployment. We aim to contribute to research in\nEmbodied AI and autonomous agents, in order to foster future work in this\nfield. We present a detailed analysis of the procedure behind implementing an\nintelligent embodied agent, comprehending a thorough description of the current\nstate-of-the-art in literature, technical explanations of the proposed methods,\nand accurate experimental studies on relevant robotic tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8ba1\u7b97\u80fd\u529b\u63d0\u5347\u548c\u6df1\u5ea6\u5b66\u4e60\u9769\u547d\u5982\u4f55\u63a8\u52a8\u5177\u8eab\u4eba\u5de5\u667a\u80fd\uff08Embodied AI\uff09\u7684\u53d1\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u667a\u80fd\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u4e0e\u90e8\u7f72\u3002", "motivation": "\u65e8\u5728\u4fc3\u8fdb\u5177\u8eabAI\u548c\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u7814\u7a76\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u5229\u75283D\u6a21\u578b\u8fdb\u884c\u4eff\u771f\u8bad\u7ec3\uff0c\u5b9e\u73b0\u667a\u80fd\u4ee3\u7406\u4e0e\u73af\u5883\u7684\u6301\u7eed\u4ea4\u4e92\uff0c\u5305\u62ec\u4fe1\u606f\u6536\u96c6\u3001\u7f16\u7801\u548c\u4efb\u52a1\u6267\u884c\u3002", "result": "\u63d0\u51fa\u4e86\u5b8c\u6574\u7684\u5177\u8eab\u4ee3\u7406\u521b\u5efa\u6d41\u7a0b\uff0c\u5305\u62ec\u6587\u732e\u7efc\u8ff0\u3001\u65b9\u6cd5\u6280\u672f\u8bf4\u660e\u548c\u76f8\u5173\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5b9e\u9a8c\u7814\u7a76\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5177\u8eabAI\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.01203", "pdf": "https://arxiv.org/pdf/2505.01203", "abs": "https://arxiv.org/abs/2505.01203", "authors": ["Andrej Macko", "Luk\u00e1\u0161 Gajdo\u0161ech", "Viktor Kocur"], "title": "Efficient Vision-based Vehicle Speed Estimation", "categories": ["cs.CV", "68T45", "I.4.9"], "comment": "Submitted to Journal of Real-Time Image Processing (JRTIP)", "summary": "This paper presents a computationally efficient method for vehicle speed\nestimation from traffic camera footage. Building upon previous work that\nutilizes 3D bounding boxes derived from 2D detections and vanishing point\ngeometry, we introduce several improvements to enhance real-time performance.\nWe evaluate our method in several variants on the BrnoCompSpeed dataset in\nterms of vehicle detection and speed estimation accuracy. Our extensive\nevaluation across various hardware platforms, including edge devices,\ndemonstrates significant gains in frames per second (FPS) compared to the prior\nstate-of-the-art, while maintaining comparable or improved speed estimation\naccuracy. We analyze the trade-off between accuracy and computational cost,\nshowing that smaller models utilizing post-training quantization offer the best\nbalance for real-world deployment. Our best performing model beats previous\nstate-of-the-art in terms of median vehicle speed estimation error (0.58 km/h\nvs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.\n83.32%) while also being 5.5 times faster.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u8f66\u8f86\u901f\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb3D\u8fb9\u754c\u6846\u548c\u6d88\u5931\u70b9\u51e0\u4f55\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6280\u672f\u4ee5\u9002\u5e94\u8fb9\u7f18\u8bbe\u5907\u7b49\u786c\u4ef6\u5e73\u53f0\u3002", "method": "\u57fa\u4e8e2D\u68c0\u6d4b\u548c\u6d88\u5931\u70b9\u51e0\u4f55\u76843D\u8fb9\u754c\u6846\u6280\u672f\uff0c\u5f15\u5165\u591a\u9879\u6539\u8fdb\u4ee5\u4f18\u5316\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u5728BrnoCompSpeed\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u901f\u5ea6\u4f30\u8ba1\u8bef\u5dee\uff080.58 km/h\uff09\u3001\u68c0\u6d4b\u7cbe\u5ea6\uff0891.02%\uff09\u548c\u53ec\u56de\u7387\uff0891.14%\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u901f\u5ea6\u5feb5.5\u500d\u3002", "conclusion": "\u901a\u8fc7\u6743\u8861\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\uff0c\u91cf\u5316\u540e\u7684\u5c0f\u6a21\u578b\u5728\u5b9e\u65f6\u90e8\u7f72\u4e2d\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2505.01207", "pdf": "https://arxiv.org/pdf/2505.01207", "abs": "https://arxiv.org/abs/2505.01207", "authors": ["Qingyu Xian", "Weiqin Jiao", "Hao Cheng", "Berend Jan van der Zwaag", "Yanqiu Huang"], "title": "T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph", "categories": ["cs.CV"], "comment": null, "summary": "Sparse-view camera pose estimation, which aims to estimate the\n6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from\ndifferent viewpoints, is a fundamental yet challenging problem in remote\nsensing applications. Existing methods often overlook the translation\ninformation between each pair of viewpoints, leading to suboptimal performance\nin sparse-view scenarios. To address this limitation, we introduce T-Graph, a\nlightweight, plug-and-play module to enhance camera pose estimation in\nsparse-view settings. T-graph takes paired image features as input and maps\nthem through a Multilayer Perceptron (MLP). It then constructs a fully\nconnected translation graph, where nodes represent cameras and edges encode\ntheir translation relationships. It can be seamlessly integrated into existing\nmodels as an additional branch in parallel with the original prediction,\nmaintaining efficiency and ease of use. Furthermore, we introduce two pairwise\ntranslation representations, relative-t and pair-t, formulated under different\nlocal coordinate systems. While relative-t captures intuitive spatial\nrelationships, pair-t offers a rotation-disentangled alternative. The two\nrepresentations contribute to enhanced adaptability across diverse application\nscenarios, further improving our module's robustness. Extensive experiments on\ntwo state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D\nand IMC PhotoTourism) validate both the effectiveness and generalizability of\nT-Graph. The results demonstrate consistent improvements across various\nmetrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8\nviewpoints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faT-Graph\u6a21\u5757\uff0c\u901a\u8fc7\u6784\u5efa\u5168\u8fde\u63a5\u5e73\u79fb\u56fe\u548c\u591a\u5c42\u611f\u77e5\u673a\uff0c\u63d0\u5347\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u89c6\u89d2\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u5728\u9065\u611f\u5e94\u7528\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u89c6\u89d2\u95f4\u7684\u5e73\u79fb\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "T-Graph\u6a21\u5757\u8f93\u5165\u6210\u5bf9\u56fe\u50cf\u7279\u5f81\uff0c\u901a\u8fc7MLP\u6620\u5c04\u5e76\u6784\u5efa\u5168\u8fde\u63a5\u5e73\u79fb\u56fe\uff0c\u652f\u6301\u4e24\u79cd\u5e73\u79fb\u8868\u793a\uff08relative-t\u548cpair-t\uff09\u3002", "result": "\u5728RelPose++\u548cForge\u65b9\u6cd5\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u673a\u4e2d\u5fc3\u7cbe\u5ea6\u63d0\u53471%\u81f36%\u3002", "conclusion": "T-Graph\u6a21\u5757\u6709\u6548\u63d0\u5347\u7a00\u758f\u89c6\u89d2\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949", "abs": "https://arxiv.org/abs/2505.00949", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "title": "Llama-Nemotron: Efficient Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.", "AI": {"tldr": "Llama-Nemotron\u7cfb\u5217\u6a21\u578b\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5f02\u6784\u63a8\u7406\u6a21\u578b\u5bb6\u65cf\uff0c\u63d0\u4f9b\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\u3001\u9ad8\u6548\u7684\u63a8\u7406\u901f\u5ea6\u548c\u5546\u4e1a\u53cb\u597d\u7684\u8bb8\u53ef\u3002", "motivation": "\u63a8\u52a8\u5f00\u6e90\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u63d0\u4f9b\u9ad8\u6027\u80fd\u4e14\u5546\u4e1a\u53ef\u7528\u7684\u6a21\u578b\uff0c\u652f\u6301\u52a8\u6001\u63a8\u7406\u5207\u6362\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u6301\u7eed\u9884\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\uff0c\u540e\u8bad\u7ec3\u9636\u6bb5\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u548c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u6a21\u578b\u5728\u63a8\u7406\u6548\u7387\u548c\u5185\u5b58\u5360\u7528\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff08\u5982DeepSeek-R1\uff09\uff0c\u5e76\u652f\u6301\u52a8\u6001\u63a8\u7406\u6a21\u5f0f\u5207\u6362\u3002", "conclusion": "Llama-Nemotron\u7cfb\u5217\u6a21\u578b\u4e3a\u5f00\u6e90\u793e\u533a\u548c\u4f01\u4e1a\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u7075\u6d3b\u7684\u63a8\u7406\u5de5\u5177\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u4ee3\u7801\u3002"}}
{"id": "2505.01212", "pdf": "https://arxiv.org/pdf/2505.01212", "abs": "https://arxiv.org/abs/2505.01212", "authors": ["Kaixuan Zhang", "Hu Wang", "Minxian Li", "Mingwu Ren", "Mao Ye", "Xiatian Zhu"], "title": "High Dynamic Range Novel View Synthesis with Single Exposure", "categories": ["cs.CV", "eess.IV"], "comment": "It has been accepted by ICML 2025", "summary": "High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D\nscene HDR model from Low Dynamic Range (LDR) imagery. Typically,\nmultiple-exposure LDR images are employed to capture a wider range of\nbrightness levels in a scene, as a single LDR image cannot represent both the\nbrightest and darkest regions simultaneously. While effective, this\nmultiple-exposure HDR-NVS approach has significant limitations, including\nsusceptibility to motion artifacts (e.g., ghosting and blurring), high capture\nand storage costs. To overcome these challenges, we introduce, for the first\ntime, the single-exposure HDR-NVS problem, where only single exposure LDR\nimages are available during training. We further introduce a novel approach,\nMono-HDR-3D, featuring two dedicated modules formulated by the LDR image\nformation principles, one for converting LDR colors to HDR counterparts, and\nthe other for transforming HDR images to LDR format so that unsupervised\nlearning is enabled in a closed loop. Designed as a meta-algorithm, our\napproach can be seamlessly integrated with existing NVS models. Extensive\nexperiments show that Mono-HDR-3D significantly outperforms previous methods.\nSource code will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u66dd\u5149HDR-NVS\u65b9\u6cd5Mono-HDR-3D\uff0c\u89e3\u51b3\u4e86\u591a\u66dd\u5149HDR-NVS\u7684\u5c40\u9650\u6027\uff0c\u5982\u8fd0\u52a8\u4f2a\u5f71\u548c\u9ad8\u6210\u672c\u3002", "motivation": "\u591a\u66dd\u5149HDR-NVS\u5b58\u5728\u8fd0\u52a8\u4f2a\u5f71\u548c\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u5355\u66dd\u5149LDR\u56fe\u50cf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMono-HDR-3D\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1aLDR\u8f6cHDR\u548cHDR\u8f6cLDR\uff0c\u652f\u6301\u65e0\u76d1\u7763\u95ed\u73af\u5b66\u4e60\uff0c\u5e76\u53ef\u96c6\u6210\u5230\u73b0\u6709NVS\u6a21\u578b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMono-HDR-3D\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Mono-HDR-3D\u4e3a\u5355\u66dd\u5149HDR-NVS\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.00968", "pdf": "https://arxiv.org/pdf/2505.00968", "abs": "https://arxiv.org/abs/2505.00968", "authors": ["Thanh Tran", "Viet-Hoang Tran", "Thanh Chu", "Trang Pham", "Laurent El Ghaoui", "Tam Le", "Tan M. Nguyen"], "title": "Tree-Sliced Wasserstein Distance with Nonlinear Projection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "Tree-Sliced methods have recently emerged as an alternative to the\ntraditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines\nwith tree-based metric spaces and incorporating a splitting mechanism for\nprojecting measures. This approach enhances the ability to capture the\ntopological structures of integration domains in Sliced Optimal Transport while\nmaintaining low computational costs. Building on this foundation, we propose a\nnovel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW)\ndistance, substituting the linear projections in earlier versions with general\nprojections, while ensuring the injectivity of the associated Radon Transform\nand preserving the well-definedness of the resulting metric. By designing\nappropriate projections, we construct efficient metrics for measures on both\nEuclidean spaces and spheres. Finally, we validate our proposed metric through\nextensive numerical experiments for Euclidean and spherical datasets.\nApplications include gradient flows, self-supervised learning, and generative\nmodels, where our methods demonstrate significant improvements over recent SW\nand TSW variants.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u5207\u7247Wasserstein\u8ddd\u79bb\u7684\u975e\u7ebf\u6027\u6295\u5f71\u6846\u67b6\uff0c\u66ff\u4ee3\u7ebf\u6027\u6295\u5f71\uff0c\u63d0\u5347\u4e86\u5ea6\u91cf\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u5207\u7247Wasserstein\u8ddd\u79bb\u4f7f\u7528\u4e00\u7ef4\u7ebf\u6295\u5f71\uff0c\u9650\u5236\u4e86\u62d3\u6251\u7ed3\u6784\u7684\u6355\u6349\u80fd\u529b\u3002\u6811\u5207\u7247\u65b9\u6cd5\u901a\u8fc7\u6811\u5f62\u5ea6\u91cf\u7a7a\u95f4\u6539\u8fdb\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u975e\u7ebf\u6027\u6295\u5f71\u6846\u67b6\uff0c\u786e\u4fddRadon\u53d8\u6362\u7684\u5355\u5c04\u6027\u548c\u5ea6\u91cf\u7684\u826f\u597d\u5b9a\u4e49\u6027\uff0c\u8bbe\u8ba1\u9002\u7528\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u548c\u7403\u9762\u7684\u6295\u5f71\u3002", "result": "\u5728\u6b27\u51e0\u91cc\u5f97\u548c\u7403\u9762\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e94\u7528\u5305\u62ec\u68af\u5ea6\u6d41\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u751f\u6210\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u975e\u7ebf\u6027\u6295\u5f71\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6811\u5207\u7247Wasserstein\u8ddd\u79bb\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u6570\u636e\u63d0\u4f9b\u4e86\u9ad8\u6548\u5ea6\u91cf\u5de5\u5177\u3002"}}
{"id": "2505.01224", "pdf": "https://arxiv.org/pdf/2505.01224", "abs": "https://arxiv.org/abs/2505.01224", "authors": ["Kui Jiang", "Yan Luo", "Junjun Jiang", "Xin Xu", "Fei Ma", "Fei Yu"], "title": "RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Underwater image enhancement (UIE) is a critical preprocessing step for\nmarine vision applications, where wavelength-dependent attenuation causes\nsevere content degradation and color distortion. While recent state space\nmodels like Mamba show potential for long-range dependency modeling, their\nunfolding operations and fixed scan paths on 1D sequences fail to adapt to\nlocal object semantics and global relation modeling, limiting their efficacy in\ncomplex underwater environments. To address this, we enhance conventional Mamba\nwith the sorting-based scanning mechanism that dynamically reorders scanning\nsequences based on statistical distribution of spatial correlation of all\npixels. In this way, it encourages the network to prioritize the most\ninformative components--structural and semantic features. Upon building this\nmechanism, we devise a Visually Self-adaptive State Block (VSSB) that\nharmonizes dynamic sorting of Mamba with input-dependent dynamic convolution,\nenabling coherent integration of global context and local relational cues. This\nexquisite design helps eliminate global focus bias, especially for widely\ndistributed contents, which greatly weakens the statistical frequency. For\nrobust feature extraction and refinement, we design a cross-feature bridge\n(CFB) to adaptively fuse multi-scale representations. These efforts compose the\nnovel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive\nexperiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms\nthe state-of-the-art approach WMamba in both quantitative metrics and visual\nfidelity, averagely achieving 0.55 dB performance gain on the three benchmarks.\nOur code is available at https://github.com/kkoucy/RD-UIE/tree/main", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u7cfb\u9a71\u52a8\u7684Mamba\u6846\u67b6\uff08RD-UIE\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u6392\u5e8f\u626b\u63cf\u673a\u5236\u548c\u89c6\u89c9\u81ea\u9002\u5e94\u6027\u72b6\u6001\u5757\uff08VSSB\uff09\u63d0\u5347\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u6548\u679c\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u56e0\u6ce2\u957f\u4f9d\u8d56\u6027\u8870\u51cf\u5bfc\u81f4\u5185\u5bb9\u9000\u5316\u548c\u989c\u8272\u5931\u771f\uff0c\u73b0\u6709\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Mamba\uff09\u7684\u56fa\u5b9a\u626b\u63cf\u8def\u5f84\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u6c34\u4e0b\u73af\u5883\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u6392\u5e8f\u626b\u63cf\u673a\u5236\u548c\u52a8\u6001\u5377\u79ef\uff0c\u8bbe\u8ba1VSSB\u548c\u8de8\u7279\u5f81\u6865\uff08CFB\uff09\u4ee5\u878d\u5408\u591a\u5c3a\u5ea6\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRD-UIE\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5WMamba\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53470.55 dB\u3002", "conclusion": "RD-UIE\u901a\u8fc7\u52a8\u6001\u6392\u5e8f\u548c\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u6548\u679c\u3002"}}
{"id": "2505.00976", "pdf": "https://arxiv.org/pdf/2505.00976", "abs": "https://arxiv.org/abs/2505.00976", "authors": ["Zhiyu Liao", "Kang Chen", "Yuanguo Lin", "Kangkang Li", "Yunxuan Liu", "Hefeng Chen", "Xingwang Huang", "Yuanhui Yu"], "title": "Attack and defense techniques in large language models: A survey and new perspectives", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8c03\u67e5\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u653b\u51fb\u4e0e\u9632\u5fa1\u6280\u672f\uff0c\u5206\u7c7b\u5206\u6790\u4e86\u653b\u51fb\u7c7b\u578b\u548c\u9632\u5fa1\u7b56\u7565\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5b89\u5168\u6027\u548c\u4f26\u7406\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\uff0c\u9700\u7cfb\u7edf\u7814\u7a76\u653b\u51fb\u4e0e\u9632\u5fa1\u6280\u672f\u3002", "method": "\u5206\u7c7b\u653b\u51fb\u7c7b\u578b\uff08\u5982\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u3001\u6a21\u578b\u7a83\u53d6\u7b49\uff09\uff0c\u5206\u6790\u9632\u5fa1\u7b56\u7565\uff08\u9884\u9632\u6027\u548c\u68c0\u6d4b\u6027\u65b9\u6cd5\uff09\uff0c\u5e76\u63a2\u8ba8\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "result": "\u5f53\u524d\u9632\u5fa1\u6280\u672f\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u5e94\u5bf9\u52a8\u6001\u5a01\u80c1\u3001\u5e73\u8861\u53ef\u7528\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u89e3\u51b3\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u9700\u53d1\u5c55\u81ea\u9002\u5e94\u9632\u5fa1\u3001\u53ef\u89e3\u91ca\u5b89\u5168\u6280\u672f\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u8de8\u5b66\u79d1\u5408\u4f5c\u4e0e\u4f26\u7406\u8003\u91cf\u4ee5\u63d0\u5347LLMs\u5b89\u5168\u6027\u3002"}}
{"id": "2505.01225", "pdf": "https://arxiv.org/pdf/2505.01225", "abs": "https://arxiv.org/abs/2505.01225", "authors": ["Keiller Nogueira", "Akram Zaytar", "Wanli Ma", "Ribana Roscher", "Ronny H\u00e4nsch", "Caleb Robinson", "Anthony Ortiz", "Simone Nsutezo", "Rahul Dodhia", "Juan M. Lavista Ferres", "Oktay Karaku\u015f", "Paul L. Rosin"], "title": "Core-Set Selection for Data-efficient Land Cover Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The increasing accessibility of remotely sensed data and the potential of\nsuch data to inform large-scale decision-making has driven the development of\ndeep learning models for many Earth Observation tasks. Traditionally, such\nmodels must be trained on large datasets. However, the common assumption that\nbroadly larger datasets lead to better outcomes tends to overlook the\ncomplexities of the data distribution, the potential for introducing biases and\nnoise, and the computational resources required for processing and storing vast\ndatasets. Therefore, effective solutions should consider both the quantity and\nquality of data. In this paper, we propose six novel core-set selection methods\nfor selecting important subsets of samples from remote sensing image\nsegmentation datasets that rely on imagery only, labels only, and a combination\nof each. We benchmark these approaches against a random-selection baseline on\nthree commonly used land cover classification datasets: DFC2022, Vaihingen, and\nPotsdam. In each of the datasets, we demonstrate that training on a subset of\nsamples outperforms the random baseline, and some approaches outperform\ntraining on all available data. This result shows the importance and potential\nof data-centric learning for the remote sensing domain. The code is available\nat https://github.com/keillernogueira/data-centric-rs-classification/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u516d\u79cd\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9065\u611f\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u91cd\u8981\u5b50\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u57fa\u7ebf\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f7f\u7528\u5168\u90e8\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u4f46\u5927\u6570\u636e\u7684\u590d\u6742\u6027\u3001\u504f\u89c1\u548c\u566a\u58f0\u95ee\u9898\u5e38\u88ab\u5ffd\u89c6\uff0c\u56e0\u6b64\u9700\u5173\u6ce8\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u3002", "method": "\u63d0\u51fa\u516d\u79cd\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u57fa\u4e8e\u56fe\u50cf\u3001\u6807\u7b7e\u6216\u4e24\u8005\u7ed3\u5408\uff0c\u5e76\u5728\u4e09\u4e2a\u571f\u5730\u8986\u76d6\u5206\u7c7b\u6570\u636e\u96c6\uff08DFC2022\u3001Vaihingen\u548cPotsdam\uff09\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8bad\u7ec3\u5b50\u96c6\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\uff0c\u90e8\u5206\u65b9\u6cd5\u751a\u81f3\u4f18\u4e8e\u4f7f\u7528\u5168\u90e8\u6570\u636e\u3002", "conclusion": "\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u5728\u9065\u611f\u9886\u57df\u5177\u6709\u91cd\u8981\u6f5c\u529b\u3002"}}
{"id": "2505.00979", "pdf": "https://arxiv.org/pdf/2505.00979", "abs": "https://arxiv.org/abs/2505.00979", "authors": ["Xuhui Jiang", "Shengjie Ma", "Chengjin Xu", "Cehao Yang", "Liyu Zhang", "Jian Guo"], "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability.", "AI": {"tldr": "SoG\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u4e0a\u4e0b\u6587\u56fe\u6574\u5408\u8de8\u6587\u6863\u77e5\u8bc6\u5173\u8054\uff0c\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u5c0f\u89c4\u6a21\u3001\u4e13\u4e1a\u8bed\u6599\u4e0a\u6570\u636e\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u8de8\u6587\u6863\u77e5\u8bc6\u5173\u8054\u3002", "method": "\u6784\u5efa\u4e0a\u4e0b\u6587\u56fe\u63d0\u53d6\u5b9e\u4f53\u548c\u6982\u5ff5\uff0c\u91c7\u7528\u56fe\u6e38\u8d70\u7b56\u7565\u91c7\u6837\uff0c\u7ed3\u5408CoT\u548cCC\u63d0\u5347\u6570\u636e\u8d28\u91cf\u3002", "result": "SoG\u5728\u591a\u8df3\u6587\u6863\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eSOTA\uff0c\u5728\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u76f8\u5f53\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "SoG\u4e3aLLMs\u5728\u6570\u636e\u6709\u9650\u9886\u57df\u7684\u9ad8\u6548\u77e5\u8bc6\u83b7\u53d6\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01235", "pdf": "https://arxiv.org/pdf/2505.01235", "abs": "https://arxiv.org/abs/2505.01235", "authors": ["Youngsik Yun", "Jeongmin Bae", "Hyunseung Son", "Seoha Kim", "Hahyun Lee", "Gun Bang", "Youngjung Uh"], "title": "Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025, Project page: https://bbangsik13.github.io/OR2", "summary": "Online reconstruction of dynamic scenes is significant as it enables learning\nscenes from live-streaming video inputs, while existing offline dynamic\nreconstruction methods rely on recorded video inputs. However, previous online\nreconstruction approaches have primarily focused on efficiency and rendering\nquality, overlooking the temporal consistency of their results, which often\ncontain noticeable artifacts in static regions. This paper identifies that\nerrors such as noise in real-world recordings affect temporal inconsistency in\nonline reconstruction. We propose a method that enhances temporal consistency\nin online reconstruction from observations with temporal inconsistency which is\ninevitable in cameras. We show that our method restores the ideal observation\nby subtracting the learned error. We demonstrate that applying our method to\nvarious baselines significantly enhances both temporal consistency and\nrendering quality across datasets. Code, video results, and checkpoints are\navailable at https://bbangsik13.github.io/OR2.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u5728\u7ebf\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u5e76\u51cf\u53bb\u8bef\u5dee\u6765\u6062\u590d\u7406\u60f3\u89c2\u6d4b\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u91cd\u5efa\u65b9\u6cd5\u5ffd\u89c6\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u9759\u6001\u533a\u57df\u51fa\u73b0\u660e\u663e\u4f2a\u5f71\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u8bef\u5dee\u5e76\u51cf\u53bb\u5b83\u6765\u6062\u590d\u7406\u60f3\u89c2\u6d4b\uff0c\u4ece\u800c\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6e32\u67d3\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u91cd\u5efa\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u8d28\u91cf\u3002"}}
{"id": "2505.00983", "pdf": "https://arxiv.org/pdf/2505.00983", "abs": "https://arxiv.org/abs/2505.00983", "authors": ["Xunkai Li", "Zhengyu Wu", "Kaichi Yu", "Hongchao Qin", "Guang Zeng", "Rong-Hua Li", "Guoren Wang"], "title": "Toward Data-centric Directed Graph Learning: An Entropy-driven Approach", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.SI"], "comment": "Accepted by ICML 2025", "summary": "The directed graph (digraph), as a generalization of undirected graphs,\nexhibits superior representation capability in modeling complex topology\nsystems and has garnered considerable attention in recent years. Despite the\nnotable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage\ndirected edges, they still fail to comprehensively delve into the abundant data\nknowledge concealed in the digraphs. This data-level limitation results in\nmodel-level sub-optimal predictive performance and underscores the necessity of\nfurther exploring the potential correlations between the directed edges\n(topology) and node profiles (feature and labels) from a data-centric\nperspective, thereby empowering model-centric neural networks with stronger\nencoding capabilities.\n  In this paper, we propose \\textbf{E}ntropy-driven \\textbf{D}igraph\nknowl\\textbf{E}dge distillatio\\textbf{N} (EDEN), which can serve as a\ndata-centric digraph learning paradigm or a model-agnostic hot-and-plug\ndata-centric Knowledge Distillation (KD) module. The core idea is to achieve\ndata-centric ML, guided by our proposed hierarchical encoding theory for\nstructured data. Specifically, EDEN first utilizes directed structural\nmeasurements from a topology perspective to construct a coarse-grained\nHierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual\ninformation of node profiles to refine knowledge flow in the HKT, enabling\ndata-centric KD supervision within model training. As a general framework, EDEN\ncan also naturally extend to undirected scenarios and demonstrate satisfactory\nperformance. In our experiments, EDEN has been widely evaluated on 14 (di)graph\ndatasets (homophily and heterophily) and across 4 downstream tasks. The results\ndemonstrate that EDEN attains SOTA performance and exhibits strong improvement\nfor prevalent (Di)GNNs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEDEN\u7684\u6570\u636e\u4e2d\u5fc3\u6709\u5411\u56fe\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u7406\u8bba\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6709\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\u672a\u80fd\u5145\u5206\u6316\u6398\u6709\u5411\u56fe\u4e2d\u9690\u85cf\u7684\u6570\u636e\u77e5\u8bc6\uff0c\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u4ece\u6570\u636e\u4e2d\u5fc3\u89d2\u5ea6\u63a2\u7d22\u62d3\u6251\u4e0e\u8282\u70b9\u7279\u5f81\u7684\u5173\u7cfb\u3002", "method": "EDEN\u901a\u8fc7\u6784\u5efa\u5206\u5c42\u77e5\u8bc6\u6811\uff08HKT\uff09\u5e76\u91cf\u5316\u8282\u70b9\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5b9e\u73b0\u6570\u636e\u4e2d\u5fc3\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u5347\u6a21\u578b\u7f16\u7801\u80fd\u529b\u3002", "result": "\u572814\u4e2a\u6570\u636e\u96c6\u548c4\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cEDEN\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "EDEN\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u6709\u5411\u56fe\uff0c\u8fd8\u80fd\u6269\u5c55\u5230\u65e0\u5411\u56fe\u573a\u666f\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u63d0\u5347\u6f5c\u529b\u3002"}}
{"id": "2505.01249", "pdf": "https://arxiv.org/pdf/2505.01249", "abs": "https://arxiv.org/abs/2505.01249", "authors": ["Christopher K. I. Williams"], "title": "Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design", "categories": ["cs.CV", "cs.LG"], "comment": "19 pages, 4 figures", "summary": "Humans (and many vertebrates) face the problem of fusing together multiple\nfixations of a scene in order to obtain a representation of the whole, where\neach fixation uses a high-resolution fovea and decreasing resolution in the\nperiphery. In this paper we explicitly represent the retinal transformation of\na fixation as a linear downsampling of a high-resolution latent image of the\nscene, exploiting the known geometry. This linear transformation allows us to\ncarry out exact inference for the latent variables in factor analysis (FA) and\nmixtures of FA models of the scene. Further, this allows us to formulate and\nsolve the choice of \"where to look next\" as a Bayesian experimental design\nproblem using the Expected Information Gain criterion. Experiments on the Frey\nfaces and MNIST datasets demonstrate the effectiveness of our models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u964d\u91c7\u6837\u7684\u89c6\u7f51\u819c\u53d8\u6362\u65b9\u6cd5\uff0c\u7528\u4e8e\u878d\u5408\u591a\u4e2a\u6ce8\u89c6\u70b9\u7684\u573a\u666f\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u4f18\u5316\u6ce8\u89c6\u70b9\u9009\u62e9\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u548c\u810a\u690e\u52a8\u7269\u5982\u4f55\u901a\u8fc7\u591a\u4e2a\u6ce8\u89c6\u70b9\u878d\u5408\u9ad8\u5206\u8fa8\u7387\u548c\u4f4e\u5206\u8fa8\u7387\u4fe1\u606f\u4ee5\u6784\u5efa\u5b8c\u6574\u573a\u666f\u8868\u793a\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5df2\u77e5\u51e0\u4f55\u5173\u7cfb\uff0c\u5c06\u89c6\u7f51\u819c\u53d8\u6362\u5efa\u6a21\u4e3a\u9ad8\u5206\u8fa8\u7387\u6f5c\u5728\u56fe\u50cf\u7684\u7ebf\u6027\u964d\u91c7\u6837\uff0c\u5e76\u5728\u56e0\u5b50\u5206\u6790\u53ca\u5176\u6df7\u5408\u6a21\u578b\u4e2d\u5b9e\u73b0\u7cbe\u786e\u63a8\u65ad\u3002", "result": "\u5728Frey\u9762\u5b54\u548cMNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u878d\u5408\u591a\u6ce8\u89c6\u70b9\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u4f18\u5316\u6ce8\u89c6\u70b9\u9009\u62e9\u3002"}}
{"id": "2505.01007", "pdf": "https://arxiv.org/pdf/2505.01007", "abs": "https://arxiv.org/abs/2505.01007", "authors": ["Ling Tang", "Yuefeng Chen", "Hui Xue", "Quanshi Zhang"], "title": "Towards the Resistance of Neural Network Watermarking to Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper proves a new watermarking method to embed the ownership\ninformation into a deep neural network (DNN), which is robust to fine-tuning.\nSpecifically, we prove that when the input feature of a convolutional layer\nonly contains low-frequency components, specific frequency components of the\nconvolutional filter will not be changed by gradient descent during the\nfine-tuning process, where we propose a revised Fourier transform to extract\nfrequency components from the convolutional filter. Additionally, we also prove\nthat these frequency components are equivariant to weight scaling and weight\npermutations. In this way, we design a watermark module to encode the watermark\ninformation to specific frequency components in a convolutional filter.\nPreliminary experiments demonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u5c06\u6240\u6709\u6743\u4fe1\u606f\u5d4c\u5165\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4e2d\uff0c\u4e14\u5bf9\u5fae\u8c03\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3DNN\u6a21\u578b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6c34\u5370\u4fe1\u606f\u6613\u4e22\u5931\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u4fee\u6b63\u7684\u5085\u91cc\u53f6\u53d8\u6362\u63d0\u53d6\u5377\u79ef\u6ee4\u6ce2\u5668\u7684\u7279\u5b9a\u9891\u7387\u6210\u5206\uff0c\u8bbe\u8ba1\u6c34\u5370\u6a21\u5757\u5c06\u4fe1\u606f\u7f16\u7801\u5230\u8fd9\u4e9b\u6210\u5206\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u6c34\u5370\u4fe1\u606f\u5728\u5fae\u8c03\u3001\u6743\u91cd\u7f29\u653e\u548c\u7f6e\u6362\u4e0b\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aDNN\u6a21\u578b\u7684\u6240\u6709\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01257", "pdf": "https://arxiv.org/pdf/2505.01257", "abs": "https://arxiv.org/abs/2505.01257", "authors": ["Vladimir Somers", "Baptiste Standaert", "Victor Joos", "Alexandre Alahi", "Christophe De Vleeschouwer"], "title": "CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Online multi-object tracking has been recently dominated by\ntracking-by-detection (TbD) methods, where recent advances rely on increasingly\nsophisticated heuristics for tracklet representation, feature fusion, and\nmulti-stage matching. The key strength of TbD lies in its modular design,\nenabling the integration of specialized off-the-shelf models like motion\npredictors and re-identification. However, the extensive usage of human-crafted\nrules for temporal associations makes these methods inherently limited in their\nability to capture the complex interplay between various tracking cues. In this\nwork, we introduce CAMEL, a novel association module for Context-Aware\nMulti-Cue ExpLoitation, that learns resilient association strategies directly\nfrom data, breaking free from hand-crafted heuristics while maintaining TbD's\nvaluable modularity. At its core, CAMEL employs two transformer-based modules\nand relies on a novel association-centric training scheme to effectively model\nthe complex interactions between tracked targets and their various association\ncues. Unlike end-to-end detection-by-tracking approaches, our method remains\nlightweight and fast to train while being able to leverage external\noff-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,\nachieves state-of-the-art performance on multiple tracking benchmarks. Our code\nis available at https://github.com/TrackingLaboratory/CAMELTrack.", "AI": {"tldr": "CAMEL\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5173\u8054\u6a21\u5757\uff0c\u901a\u8fc7\u6570\u636e\u5b66\u4e60\u5173\u8054\u7b56\u7565\uff0c\u6446\u8131\u624b\u5de5\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5728\u591a\u4e2a\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u89c4\u5219\u8fdb\u884c\u65f6\u95f4\u5173\u8054\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u8ddf\u8e2a\u7ebf\u7d22\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "CAMEL\u91c7\u7528\u4e24\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u5757\u548c\u4e00\u79cd\u65b0\u7684\u5173\u8054\u4e2d\u5fc3\u8bad\u7ec3\u65b9\u6848\uff0c\u5efa\u6a21\u76ee\u6807\u4e0e\u5173\u8054\u7ebf\u7d22\u7684\u590d\u6742\u4ea4\u4e92\u3002", "result": "CAMELTrack\u5728\u591a\u4e2a\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "CAMEL\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u63d0\u5347\u4e86\u5173\u8054\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u5757\u5316\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2505.01015", "pdf": "https://arxiv.org/pdf/2505.01015", "abs": "https://arxiv.org/abs/2505.01015", "authors": ["Jongwook Han", "Dongmin Choi", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "title": "Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "32 pages, 7 figures", "summary": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data.", "AI": {"tldr": "\u63d0\u51faValue Portrait\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u4ef7\u503c\u53d6\u5411\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u548c\u5fc3\u7406\u6d4b\u91cf\u9a8c\u8bc1\u63d0\u5347\u751f\u6001\u6548\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4f9d\u8d56\u4eba\u5de5\u6216\u673a\u5668\u6807\u6ce8\uff0c\u6613\u53d7\u4ef7\u503c\u76f8\u5173\u504f\u5dee\u5f71\u54cd\uff0c\u4e14\u6d4b\u8bd5\u573a\u666f\u4e0e\u771f\u5b9e\u4f7f\u7528\u573a\u666f\u8131\u8282\u3002", "method": "\u8bbe\u8ba1\u5305\u542b\u771f\u5b9e\u7528\u6237-LLM\u4ea4\u4e92\u7684\u57fa\u51c6\u9879\u76ee\uff0c\u901a\u8fc7\u4eba\u7c7b\u8bc4\u5206\u4e0e\u4ef7\u503c\u5206\u6570\u7684\u76f8\u5173\u6027\u8fdb\u884c\u5fc3\u7406\u6d4b\u91cf\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f3027\u4e2aLLM\uff0c\u53d1\u73b0\u5176\u66f4\u91cd\u89c6Benevolence\u3001Security\u548cSelf-Direction\u4ef7\u503c\uff0c\u800c\u5bf9Tradition\u3001Power\u548cAchievement\u4ef7\u503c\u5173\u6ce8\u8f83\u5c11\uff0c\u5e76\u63ed\u793a\u6a21\u578b\u5bf9\u4eba\u53e3\u7fa4\u4f53\u7684\u504f\u5dee\u3002", "conclusion": "Value Portrait\u57fa\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u8bc4\u4f30LLM\u7684\u4ef7\u503c\u53d6\u5411\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u504f\u5dee\u548c\u504f\u597d\u3002"}}
{"id": "2505.01267", "pdf": "https://arxiv.org/pdf/2505.01267", "abs": "https://arxiv.org/abs/2505.01267", "authors": ["Gaozheng Pei", "Ke Ma", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "title": "Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain", "categories": ["cs.CV"], "comment": null, "summary": "The diffusion-based adversarial purification methods attempt to drown\nadversarial perturbations into a part of isotropic noise through the forward\nprocess, and then recover the clean images through the reverse process. Due to\nthe lack of distribution information about adversarial perturbations in the\npixel domain, it is often unavoidable to damage normal semantics. We turn to\nthe frequency domain perspective, decomposing the image into amplitude spectrum\nand phase spectrum. We find that for both spectra, the damage caused by\nadversarial perturbations tends to increase monotonically with frequency. This\nmeans that we can extract the content and structural information of the\noriginal clean sample from the frequency components that are less damaged.\nMeanwhile, theoretical analysis indicates that existing purification methods\nindiscriminately damage all frequency components, leading to excessive damage\nto the image. Therefore, we propose a purification method that can eliminate\nadversarial perturbations while maximizing the preservation of the content and\nstructure of the original image. Specifically, at each time step during the\nreverse process, for the amplitude spectrum, we replace the low-frequency\ncomponents of the estimated image's amplitude spectrum with the corresponding\nparts of the adversarial image. For the phase spectrum, we project the phase of\nthe estimated image into a designated range of the adversarial image's phase\nspectrum, focusing on the low frequencies. Empirical evidence from extensive\nexperiments demonstrates that our method significantly outperforms most current\ndefense methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u57df\u7684\u5bf9\u6297\u51c0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u66ff\u6362\u4f4e\u9891\u632f\u5e45\u548c\u9650\u5236\u76f8\u4f4d\u8303\u56f4\uff0c\u6709\u6548\u53bb\u9664\u5bf9\u6297\u6270\u52a8\u5e76\u4fdd\u7559\u56fe\u50cf\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u51c0\u5316\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u6270\u52a8\u5206\u5e03\u4fe1\u606f\uff0c\u5bb9\u6613\u7834\u574f\u6b63\u5e38\u8bed\u4e49\uff0c\u9891\u57df\u89c6\u89d2\u4e0b\u53d1\u73b0\u6270\u52a8\u5bf9\u9ad8\u9891\u90e8\u5206\u5f71\u54cd\u66f4\u5927\uff0c\u56e0\u6b64\u63d0\u51fa\u9488\u5bf9\u6027\u51c0\u5316\u7b56\u7565\u3002", "method": "\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u632f\u5e45\u548c\u76f8\u4f4d\u8c31\uff0c\u66ff\u6362\u4f4e\u9891\u632f\u5e45\u5e76\u9650\u5236\u76f8\u4f4d\u8303\u56f4\uff0c\u4e13\u6ce8\u4e8e\u4f4e\u9891\u90e8\u5206\u4ee5\u51cf\u5c11\u5bf9\u56fe\u50cf\u7684\u8fc7\u5ea6\u7834\u574f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u53bb\u9664\u6270\u52a8\u5e76\u4fdd\u7559\u56fe\u50cf\u5185\u5bb9\u3002", "conclusion": "\u9891\u57df\u89c6\u89d2\u7684\u5bf9\u6297\u51c0\u5316\u65b9\u6cd5\u5728\u53bb\u9664\u6270\u52a8\u548c\u4fdd\u7559\u56fe\u50cf\u5185\u5bb9\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u5bf9\u6297\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.01322", "pdf": "https://arxiv.org/pdf/2505.01322", "abs": "https://arxiv.org/abs/2505.01322", "authors": ["Chenxi Li", "Weijie Wang", "Qiang Li", "Bruno Lepri", "Nicu Sebe", "Weizhi Nie"], "title": "FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors", "categories": ["cs.CV"], "comment": null, "summary": "Text-driven object insertion in 3D scenes is an emerging task that enables\nintuitive scene editing through natural language. However, existing 2D\nediting-based methods often rely on spatial priors such as 2D masks or 3D\nbounding boxes, and they struggle to ensure consistency of the inserted object.\nThese limitations hinder flexibility and scalability in real-world\napplications. In this paper, we propose FreeInsert, a novel framework that\nleverages foundation models including MLLMs, LGMs, and diffusion models to\ndisentangle object generation from spatial placement. This enables unsupervised\nand flexible object insertion in 3D scenes without spatial priors. FreeInsert\nstarts with an MLLM-based parser that extracts structured semantics, including\nobject types, spatial relationships, and attachment regions, from user\ninstructions. These semantics guide both the reconstruction of the inserted\nobject for 3D consistency and the learning of its degrees of freedom. We\nleverage the spatial reasoning capabilities of MLLMs to initialize object pose\nand scale. A hierarchical, spatially aware refinement stage further integrates\nspatial semantics and MLLM-inferred priors to enhance placement. Finally, the\nappearance of the object is improved using the inserted-object image to enhance\nvisual fidelity. Experimental results demonstrate that FreeInsert achieves\nsemantically coherent, spatially precise, and visually realistic 3D insertions\nwithout relying on spatial priors, offering a user-friendly and flexible\nediting experience.", "AI": {"tldr": "FreeInsert\u662f\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff08\u5982MLLMs\u3001LGMs\u548c\u6269\u6563\u6a21\u578b\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u57283D\u573a\u666f\u4e2d\u5b9e\u73b0\u65e0\u9700\u7a7a\u95f4\u5148\u9a8c\u7684\u7075\u6d3b\u5bf9\u8c61\u63d2\u5165\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a7a\u95f4\u5148\u9a8c\uff08\u59822D\u63a9\u7801\u62163D\u8fb9\u754c\u6846\uff09\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002FreeInsert\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7MLLM\u89e3\u6790\u7528\u6237\u6307\u4ee4\uff0c\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\u6307\u5bfc\u5bf9\u8c61\u751f\u6210\u548c\u7a7a\u95f4\u653e\u7f6e\uff0c\u7ed3\u5408\u5206\u5c42\u7ec6\u5316\u9636\u6bb5\u63d0\u5347\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreeInsert\u5b9e\u73b0\u4e86\u8bed\u4e49\u8fde\u8d2f\u3001\u7a7a\u95f4\u7cbe\u786e\u4e14\u89c6\u89c9\u903c\u771f\u76843D\u63d2\u5165\u3002", "conclusion": "FreeInsert\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7a7a\u95f4\u5148\u9a8c\u7684\u7528\u6237\u53cb\u597d\u7f16\u8f91\u65b9\u6848\u3002"}}
{"id": "2505.01036", "pdf": "https://arxiv.org/pdf/2505.01036", "abs": "https://arxiv.org/abs/2505.01036", "authors": ["Xiaojun Zhou"], "title": "Stagnation in Evolutionary Algorithms: Convergence $\\neq$ Optimality", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In the evolutionary computation community, it is widely believed that\nstagnation impedes convergence in evolutionary algorithms, and that convergence\ninherently indicates optimality. However, this perspective is misleading. In\nthis study, it is the first to highlight that the stagnation of an individual\ncan actually facilitate the convergence of the entire population, and\nconvergence does not necessarily imply optimality, not even local optimality.\nConvergence alone is insufficient to ensure the effectiveness of evolutionary\nalgorithms. Several counterexamples are provided to illustrate this argument.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u505c\u6ede\u53ef\u80fd\u4fc3\u8fdb\u79cd\u7fa4\u6536\u655b\uff0c\u4e14\u6536\u655b\u4e0d\u7b49\u4e8e\u6700\u4f18\u6027\u3002", "motivation": "\u6311\u6218\u8fdb\u5316\u8ba1\u7b97\u4e2d\u5173\u4e8e\u505c\u6ede\u548c\u6536\u655b\u7684\u4f20\u7edf\u89c2\u70b9\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u53cd\u4f8b\u8bf4\u660e\u3002", "result": "\u505c\u6ede\u6709\u52a9\u4e8e\u79cd\u7fa4\u6536\u655b\uff0c\u6536\u655b\u4e0d\u4fdd\u8bc1\u6700\u4f18\u6027\u3002", "conclusion": "\u6536\u655b\u672c\u8eab\u4e0d\u8db3\u4ee5\u8861\u91cf\u8fdb\u5316\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.01364", "pdf": "https://arxiv.org/pdf/2505.01364", "abs": "https://arxiv.org/abs/2505.01364", "authors": ["Enamundram Naga Karthik", "Sandrine B\u00e9dard", "Jan Valo\u0161ek", "Christoph S. Aigner", "Elise Bannier", "Josef Bedna\u0159\u00edk", "Virginie Callot", "Anna Combes", "Armin Curt", "Gergely David", "Falk Eippert", "Lynn Farner", "Michael G Fehlings", "Patrick Freund", "Tobias Granberg", "Cristina Granziera", "RHSCIR Network Imaging Group", "Ulrike Horn", "Tom\u00e1\u0161 Hor\u00e1k", "Suzanne Humphreys", "Markus Hupp", "Anne Kerbrat", "Nawal Kinany", "Shannon Kolind", "Petr Kudli\u010dka", "Anna Lebret", "Lisa Eunyoung Lee", "Caterina Mainero", "Allan R. Martin", "Megan McGrath", "Govind Nair", "Kristin P. O'Grady", "Jiwon Oh", "Russell Ouellette", "Nikolai Pfender", "Dario Pfyffer", "Pierre-Fran\u00e7ois Pradat", "Alexandre Prat", "Emanuele Pravat\u00e0", "Daniel S. Reich", "Ilaria Ricchi", "Naama Rotem-Kohavi", "Simon Schading-Sassenhausen", "Maryam Seif", "Andrew Smith", "Seth A Smith", "Grace Sweeney", "Roger Tam", "Anthony Traboulsee", "Constantina Andrada Treaba", "Charidimos Tsagkas", "Zachary Vavasour", "Dimitri Van De Ville", "Kenneth Arnold Weber II", "Sarath Chandar", "Julien Cohen-Adad"], "title": "Monitoring morphometric drift in lifelong learning segmentation of the spinal cord", "categories": ["cs.CV"], "comment": null, "summary": "Morphometric measures derived from spinal cord segmentations can serve as\ndiagnostic and prognostic biomarkers in neurological diseases and injuries\naffecting the spinal cord. While robust, automatic segmentation methods to a\nwide variety of contrasts and pathologies have been developed over the past few\nyears, whether their predictions are stable as the model is updated using new\ndatasets has not been assessed. This is particularly important for deriving\nnormative values from healthy participants. In this study, we present a spinal\ncord segmentation model trained on a multisite $(n=75)$ dataset, including 9\ndifferent MRI contrasts and several spinal cord pathologies. We also introduce\na lifelong learning framework to automatically monitor the morphometric drift\nas the model is updated using additional datasets. The framework is triggered\nby an automatic GitHub Actions workflow every time a new model is created,\nrecording the morphometric values derived from the model's predictions over\ntime. As a real-world application of the proposed framework, we employed the\nspinal cord segmentation model to update a recently-introduced normative\ndatabase of healthy participants containing commonly used measures of spinal\ncord morphometry. Results showed that: (i) our model outperforms previous\nversions and pathology-specific models on challenging lumbar spinal cord cases,\nachieving an average Dice score of $0.95 \\pm 0.03$; (ii) the automatic workflow\nfor monitoring morphometric drift provides a quick feedback loop for developing\nfuture segmentation models; and (iii) the scaling factor required to update the\ndatabase of morphometric measures is nearly constant among slices across the\ngiven vertebral levels, showing minimum drift between the current and previous\nversions of the model monitored by the framework. The model is freely available\nin Spinal Cord Toolbox v7.0.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7ad9\u70b9\u6570\u636e\u8bad\u7ec3\u7684\u810a\u9ad3\u5206\u5272\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6\u76d1\u63a7\u6a21\u578b\u66f4\u65b0\u65f6\u7684\u5f62\u6001\u6d4b\u91cf\u6f02\u79fb\uff0c\u5e94\u7528\u4e8e\u66f4\u65b0\u5065\u5eb7\u53c2\u4e0e\u8005\u7684\u89c4\u8303\u6570\u636e\u5e93\u3002", "motivation": "\u8bc4\u4f30\u810a\u9ad3\u5206\u5272\u6a21\u578b\u5728\u66f4\u65b0\u65f6\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u7528\u4e8e\u4ece\u5065\u5eb7\u53c2\u4e0e\u8005\u4e2d\u63d0\u53d6\u89c4\u8303\u503c\u3002", "method": "\u4f7f\u7528\u591a\u7ad9\u70b9\u6570\u636e\u96c6\uff08n=75\uff09\u8bad\u7ec3\u810a\u9ad3\u5206\u5272\u6a21\u578b\uff0c\u5f15\u5165\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6\u81ea\u52a8\u76d1\u63a7\u5f62\u6001\u6d4b\u91cf\u6f02\u79fb\uff0c\u5e76\u901a\u8fc7GitHub Actions\u5de5\u4f5c\u6d41\u89e6\u53d1\u3002", "result": "\u6a21\u578b\u5728\u8170\u690e\u810a\u9ad3\u75c5\u4f8b\u4e2d\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u7248\u672c\uff08\u5e73\u5747Dice\u5206\u65700.95\u00b10.03\uff09\uff0c\u5f62\u6001\u6d4b\u91cf\u6f02\u79fb\u76d1\u63a7\u63d0\u4f9b\u4e86\u5feb\u901f\u53cd\u9988\uff0c\u89c4\u8303\u6570\u636e\u5e93\u66f4\u65b0\u6240\u9700\u7684\u7f29\u653e\u56e0\u5b50\u51e0\u4e4e\u6052\u5b9a\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728Spinal Cord Toolbox v7.0\u4e2d\u514d\u8d39\u63d0\u4f9b\uff0c\u4e3a\u810a\u9ad3\u5f62\u6001\u6d4b\u91cf\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u5de5\u5177\u3002"}}
{"id": "2505.01043", "pdf": "https://arxiv.org/pdf/2505.01043", "abs": "https://arxiv.org/abs/2505.01043", "authors": ["Zhiwei Hao", "Jianyuan Guo", "Li Shen", "Yong Luo", "Han Hu", "Guoxia Wang", "Dianhai Yu", "Yonggang Wen", "Dacheng Tao"], "title": "Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several components$\\unicode{x2013}$such\nas weights, activations, and gradients$\\unicode{x2013}$each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c06\u5176\u5206\u4e3a\u4e09\u7c7b\uff08\u5b9a\u70b9/\u6574\u6570\u3001\u6d6e\u70b9\u3001\u81ea\u5b9a\u4e49\u683c\u5f0f\uff09\uff0c\u5e76\u8ba8\u8bba\u4e86\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u53ef\u63d0\u5347\u6548\u7387\uff0c\u4f46\u7814\u7a76\u9886\u57df\u788e\u7247\u5316\uff0c\u9700\u7cfb\u7edf\u6574\u7406\u3002", "method": "\u5c06\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff1a\u5b9a\u70b9/\u6574\u6570\u3001\u6d6e\u70b9\u3001\u81ea\u5b9a\u4e49\u683c\u5f0f\uff0c\u5e76\u8ba8\u8bba\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u3002", "result": "\u7cfb\u7edf\u5206\u7c7b\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u8bba\u6587\u96c6\u5408\u3002", "conclusion": "\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u7edf\u4e00\u548c\u4f18\u5316\uff0c\u672a\u6765\u65b9\u5411\u5305\u62ec\u786c\u4ef6\u517c\u5bb9\u6027\u548c\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2505.01385", "pdf": "https://arxiv.org/pdf/2505.01385", "abs": "https://arxiv.org/abs/2505.01385", "authors": ["Fahong Zhang", "Yilei Shi", "Xiao Xiang Zhu"], "title": "Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This paper addresses the challenge of mapping polygonal buildings from remote\nsensing images and introduces a novel algorithm, the Global Collinearity-aware\nPolygonizer (GCP). GCP, built upon an instance segmentation framework,\nprocesses binary masks produced by any instance segmentation model. The\nalgorithm begins by collecting polylines sampled along the contours of the\nbinary masks. These polylines undergo a refinement process using a\ntransformer-based regression module to ensure they accurately fit the contours\nof the targeted building instances. Subsequently, a collinearity-aware polygon\nsimplification module simplifies these refined polylines and generate the final\npolygon representation. This module employs dynamic programming technique to\noptimize an objective function that balances the simplicity and fidelity of the\npolygons, achieving globally optimal solutions. Furthermore, the optimized\ncollinearity-aware objective is seamlessly integrated into network training,\nenhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has\nbeen validated on two public benchmarks for polygonal building mapping. Further\nexperiments reveal that applying the collinearity-aware polygon simplification\nmodule to arbitrary polylines, without prior knowledge, enhances accuracy over\ntraditional methods such as the Douglas-Peucker algorithm. This finding\nunderscores the broad applicability of GCP. The code for the proposed method\nwill be made available at https://github.com/zhu-xlab.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGCP\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u9065\u611f\u56fe\u50cf\u4e2d\u6620\u5c04\u591a\u8fb9\u5f62\u5efa\u7b51\uff0c\u901a\u8fc7\u5168\u5c40\u5171\u7ebf\u6027\u611f\u77e5\u7684\u591a\u8fb9\u5f62\u7b80\u5316\u6a21\u5757\u4f18\u5316\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u4ece\u9065\u611f\u56fe\u50cf\u4e2d\u51c6\u786e\u6620\u5c04\u591a\u8fb9\u5f62\u5efa\u7b51\u7684\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u8f6e\u5ed3\u7ebf\u3001Transformer\u56de\u5f52\u6a21\u5757\u4f18\u5316\u8f6e\u5ed3\uff0c\u518d\u901a\u8fc7\u5171\u7ebf\u6027\u611f\u77e5\u7684\u591a\u8fb9\u5f62\u7b80\u5316\u6a21\u5757\u751f\u6210\u6700\u7ec8\u591a\u8fb9\u5f62\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86GCP\u7684\u6709\u6548\u6027\uff0c\u5176\u7b80\u5316\u6a21\u5757\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "GCP\u7b97\u6cd5\u5728\u5efa\u7b51\u591a\u8fb9\u5f62\u6620\u5c04\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.01059", "pdf": "https://arxiv.org/pdf/2505.01059", "abs": "https://arxiv.org/abs/2505.01059", "authors": ["An T. Le", "Khai Nguyen", "Minh Nhat Vu", "Jo\u00e3o Carvalho", "Jan Peters"], "title": "Model Tensor Planning", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "22 pages, 9 figures", "summary": "Sampling-based model predictive control (MPC) offers strong performance in\nnonlinear and contact-rich robotic tasks, yet often suffers from poor\nexploration due to locally greedy sampling schemes. We propose \\emph{Model\nTensor Planning} (MTP), a novel sampling-based MPC framework that introduces\nhigh-entropy control trajectory generation through structured tensor sampling.\nBy sampling over randomized multipartite graphs and interpolating control\ntrajectories with B-splines and Akima splines, MTP ensures smooth and globally\ndiverse control candidates. We further propose a simple $\\beta$-mixing strategy\nthat blends local exploitative and global exploratory samples within the\nmodified Cross-Entropy Method (CEM) update, balancing control refinement and\nexploration. Theoretically, we show that MTP achieves asymptotic path coverage\nand maximum entropy in the control trajectory space in the limit of infinite\ntensor depth and width.\n  Our implementation is fully vectorized using JAX and compatible with MuJoCo\nXLA, supporting \\emph{Just-in-time} (JIT) compilation and batched rollouts for\nreal-time control with online domain randomization. Through experiments on\nvarious challenging robotic tasks, ranging from dexterous in-hand manipulation\nto humanoid locomotion, we demonstrate that MTP outperforms standard MPC and\nevolutionary strategy baselines in task success and control robustness. Design\nand sensitivity ablations confirm the effectiveness of MTP tensor sampling\nstructure, spline interpolation choices, and mixing strategy. Altogether, MTP\noffers a scalable framework for robust exploration in model-based planning and\ncontrol.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMTP\u7684\u91c7\u6837\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5f20\u91cf\u91c7\u6837\u548c\u9ad8\u71b5\u63a7\u5236\u8f68\u8ff9\u751f\u6210\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u4f18\u5316\uff0c\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u91c7\u6837MPC\u5728\u975e\u7ebf\u6027\u53ca\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c40\u90e8\u8d2a\u5a6a\u91c7\u6837\u5bfc\u81f4\u63a2\u7d22\u4e0d\u8db3\uff0cMTP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MTP\u91c7\u7528\u968f\u673a\u591a\u90e8\u56fe\u91c7\u6837\u548cB\u6837\u6761/Akima\u6837\u6761\u63d2\u503c\u751f\u6210\u591a\u6837\u5316\u63a7\u5236\u8f68\u8ff9\uff0c\u7ed3\u5408\u03b2\u6df7\u5408\u7b56\u7565\u4f18\u5316CEM\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMTP\u5728\u591a\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6MPC\u548c\u8fdb\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u63a7\u5236\u9c81\u68d2\u6027\u3002", "conclusion": "MTP\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u4e0e\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u9c81\u68d2\u63a2\u7d22\u6846\u67b6\u3002"}}
{"id": "2505.01390", "pdf": "https://arxiv.org/pdf/2505.01390", "abs": "https://arxiv.org/abs/2505.01390", "authors": ["Alice Natalina Caragliano", "Claudia Tacconi", "Carlo Greco", "Lorenzo Nibid", "Edy Ippolito", "Michele Fiore", "Giuseppe Perrone", "Sara Ramella", "Paolo Soda", "Valerio Guarrasi"], "title": "Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.17503", "summary": "This study proposes a novel approach combining Multimodal Deep Learning with\nintrinsic eXplainable Artificial Intelligence techniques to predict\npathological response in non-small cell lung cancer patients undergoing\nneoadjuvant therapy. Due to the limitations of existing radiomics and unimodal\ndeep learning approaches, we introduce an intermediate fusion strategy that\nintegrates imaging and clinical data, enabling efficient interaction between\ndata modalities. The proposed Multimodal Doctor-in-the-Loop method further\nenhances clinical relevance by embedding clinicians' domain knowledge directly\ninto the training process, guiding the model's focus gradually from broader\nlung regions to specific lesions. Results demonstrate improved predictive\naccuracy and explainability, providing insights into optimal data integration\nstrategies for clinical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u548c\u53ef\u89e3\u91caAI\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u975e\u5c0f\u7ec6\u80de\u80ba\u764c\u60a3\u8005\u5bf9\u65b0\u8f85\u52a9\u6cbb\u7597\u7684\u75c5\u7406\u53cd\u5e94\u3002", "motivation": "\u73b0\u6709\u653e\u5c04\u7ec4\u5b66\u548c\u5355\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u6539\u8fdb\u6570\u636e\u6574\u5408\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "method": "\u91c7\u7528\u4e2d\u95f4\u878d\u5408\u7b56\u7565\u6574\u5408\u5f71\u50cf\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u533b\u751f\u9886\u57df\u77e5\u8bc6\u7684\"Multimodal Doctor-in-the-Loop\"\u65b9\u6cd5\u3002", "result": "\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u6570\u636e\u6574\u5408\u63d0\u4f9b\u4e86\u4f18\u5316\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6574\u5408\u548c\u6a21\u578b\u89e3\u91ca\u6027\u65b9\u9762\u3002"}}
{"id": "2505.01065", "pdf": "https://arxiv.org/pdf/2505.01065", "abs": "https://arxiv.org/abs/2505.01065", "authors": ["David Jin", "Qian Fu", "Yuekang Li"], "title": "Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, raising concerns about their potential for automated\nexploit generation (AEG). This paper presents the first systematic study on\nLLMs' effectiveness in AEG, evaluating both their cooperativeness and technical\nproficiency. To mitigate dataset bias, we introduce a benchmark with refactored\nversions of five software security labs. Additionally, we design an LLM-based\nattacker to systematically prompt LLMs for exploit generation. Our experiments\nreveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to\nuncensored models, while Llama3 is the most resistant. However, no model\nsuccessfully generates exploits for refactored labs, though GPT-4o's minimal\nerrors highlight the potential for LLM-driven AEG advancements.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u5316\u6f0f\u6d1e\u5229\u7528\u751f\u6210\uff08AEG\uff09\u4e2d\u7684\u6548\u679c\uff0c\u8bc4\u4f30\u4e86\u5176\u5408\u4f5c\u6027\u548c\u6280\u672f\u80fd\u529b\u3002\u901a\u8fc7\u5f15\u5165\u91cd\u6784\u7684\u8f6f\u4ef6\u5b89\u5168\u5b9e\u9a8c\u5ba4\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u653b\u51fb\u8005\uff0c\u5b9e\u9a8c\u53d1\u73b0GPT-4\u548cGPT-4o\u5408\u4f5c\u6027\u9ad8\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5747\u672a\u80fd\u6210\u529f\u751f\u6210\u6f0f\u6d1e\u5229\u7528\u3002", "motivation": "\u7814\u7a76LLMs\u5728AEG\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u8bc4\u4f30\u5176\u5b89\u5168\u98ce\u9669\u548c\u6280\u672f\u80fd\u529b\u3002", "method": "\u5f15\u5165\u91cd\u6784\u7684\u8f6f\u4ef6\u5b89\u5168\u5b9e\u9a8c\u5ba4\u57fa\u51c6\uff0c\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u653b\u51fb\u8005\u7cfb\u7edf\uff0c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u7684\u5408\u4f5c\u6027\u548c\u6280\u672f\u80fd\u529b\u3002", "result": "GPT-4\u548cGPT-4o\u5408\u4f5c\u6027\u9ad8\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5747\u672a\u80fd\u6210\u529f\u751f\u6210\u6f0f\u6d1e\u5229\u7528\uff0cGPT-4o\u7684\u9519\u8bef\u6700\u5c11\u3002", "conclusion": "LLMs\u5728AEG\u4e2d\u5c1a\u672a\u6210\u719f\uff0c\u4f46GPT-4o\u7684\u8868\u73b0\u663e\u793a\u4e86\u672a\u6765\u6539\u8fdb\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.01406", "pdf": "https://arxiv.org/pdf/2505.01406", "abs": "https://arxiv.org/abs/2505.01406", "authors": ["Mohammadreza Teymoorianfard", "Shiqing Ma", "Amir Houmansadr"], "title": "VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The rapid rise of video diffusion models has enabled the generation of highly\nrealistic and temporally coherent videos, raising critical concerns about\ncontent authenticity, provenance, and misuse. Existing watermarking approaches,\nwhether passive, post-hoc, or adapted from image-based techniques, often\nstruggle to withstand video-specific manipulations such as frame insertion,\ndropping, or reordering, and typically degrade visual quality. In this work, we\nintroduce VIDSTAMP, a watermarking framework that embeds per-frame or\nper-segment messages directly into the latent space of temporally-aware video\ndiffusion models. By fine-tuning the model's decoder through a two-stage\npipeline, first on static image datasets to promote spatial message separation,\nand then on synthesized video sequences to restore temporal consistency,\nVIDSTAMP learns to embed high-capacity, flexible watermarks with minimal\nperceptual impact. Leveraging architectural components such as 3D convolutions\nand temporal attention, our method imposes no additional inference cost and\noffers better perceptual quality than prior methods, while maintaining\ncomparable robustness against common distortions and tampering. VIDSTAMP embeds\n768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a\nlog P-value of -166.65 (lower is better), and maintains a video quality score\nof 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior\nmethods in capacity-quality tradeoffs. Code: Code:\n\\url{https://github.com/SPIN-UMass/VidStamp}", "AI": {"tldr": "VIDSTAMP\u662f\u4e00\u79cd\u89c6\u9891\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u65f6\u95f4\u611f\u77e5\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5d4c\u5165\u6c34\u5370\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u9891\u64cd\u4f5c\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5185\u5bb9\u771f\u5b9e\u6027\u548c\u6ee5\u7528\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u89c6\u9891\u7279\u5b9a\u64cd\u4f5c\u4e14\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7ba1\u9053\uff0c\u5148\u5728\u9759\u6001\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4ee5\u5206\u79bb\u7a7a\u95f4\u4fe1\u606f\uff0c\u518d\u5728\u5408\u6210\u89c6\u9891\u5e8f\u5217\u4e0a\u6062\u590d\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5229\u75283D\u5377\u79ef\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u5d4c\u5165\u9ad8\u5bb9\u91cf\u6c34\u5370\u3002", "result": "VIDSTAMP\u6bcf\u89c6\u9891\u5d4c\u5165768\u6bd4\u7279\uff08\u6bcf\u5e2748\u6bd4\u7279\uff09\uff0c\u6bd4\u7279\u51c6\u786e\u7387\u8fbe95.0%\uff0c\u89c6\u9891\u8d28\u91cf\u8bc4\u52060.836\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VIDSTAMP\u5728\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u5bb9\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u6c34\u5370\u6280\u672f\u3002"}}
{"id": "2505.01067", "pdf": "https://arxiv.org/pdf/2505.01067", "abs": "https://arxiv.org/abs/2505.01067", "authors": ["Ziqi Ding", "Qian Fu", "Junchen Ding", "Gelei Deng", "Yi Liu", "Yuekang Li"], "title": "A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have spurred the\ndevelopment of diverse AI applications from code generation and video editing\nto text generation; however, AI supply chains such as Hugging Face, which host\npretrained models and their associated configuration files contributed by the\npublic, face significant security challenges; in particular, configuration\nfiles originally intended to set up models by specifying parameters and initial\nsettings can be exploited to execute unauthorized code, yet research has\nlargely overlooked their security compared to that of the models themselves; in\nthis work, we present the first comprehensive study of malicious configurations\non Hugging Face, identifying three attack scenarios (file, website, and\nrepository operations) that expose inherent risks; to address these threats, we\nintroduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in\nthe context of their associated runtime code and critical libraries,\neffectively detecting suspicious elements with low false positive rates and\nhigh accuracy; our extensive evaluation uncovers thousands of suspicious\nrepositories and configuration files, underscoring the urgent need for enhanced\nsecurity validation in AI model hosting platforms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Hugging Face\u5e73\u53f0\u4e0a\u6076\u610f\u914d\u7f6e\u6587\u4ef6\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86CONFIGSCAN\u5de5\u5177\u4ee5\u68c0\u6d4b\u5a01\u80c1\u3002", "motivation": "\u5c3d\u7ba1AI\u4f9b\u5e94\u94fe\uff08\u5982Hugging Face\uff09\u6258\u7ba1\u4e86\u5927\u91cf\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u5176\u914d\u7f6e\u6587\u4ef6\u7684\u5b89\u5168\u95ee\u9898\u88ab\u5ffd\u89c6\uff0c\u53ef\u80fd\u88ab\u5229\u7528\u6267\u884c\u672a\u6388\u6743\u4ee3\u7801\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u4e09\u79cd\u653b\u51fb\u573a\u666f\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eLLM\u7684\u5de5\u5177CONFIGSCAN\uff0c\u5206\u6790\u914d\u7f6e\u6587\u4ef6\u53ca\u5176\u8fd0\u884c\u65f6\u4ee3\u7801\u548c\u5173\u952e\u5e93\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u6570\u5343\u4e2a\u53ef\u7591\u5b58\u50a8\u5e93\u548c\u914d\u7f6e\u6587\u4ef6\uff0c\u9a8c\u8bc1\u4e86\u5de5\u5177\u7684\u4f4e\u8bef\u62a5\u7387\u548c\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u5f3a\u8c03\u4e86AI\u6a21\u578b\u6258\u7ba1\u5e73\u53f0\u52a0\u5f3a\u5b89\u5168\u9a8c\u8bc1\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2505.00735", "pdf": "https://arxiv.org/pdf/2505.00735", "abs": "https://arxiv.org/abs/2505.00735", "authors": ["Jin Hyun Park", "Harine Choi", "Praewa Pitiphat"], "title": "Leveraging Depth and Attention Mechanisms for Improved RGB Image Inpainting", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Existing deep learning-based image inpainting methods typically rely on\nconvolutional networks with RGB images to reconstruct images. However, relying\nexclusively on RGB images may neglect important depth information, which plays\na critical role in understanding the spatial and structural context of a scene.\nJust as human vision leverages stereo cues to perceive depth, incorporating\ndepth maps into the inpainting process can enhance the model's ability to\nreconstruct images with greater accuracy and contextual awareness. In this\npaper, we propose a novel approach that incorporates both RGB and depth images\nfor enhanced image inpainting. Our models employ a dual encoder architecture,\nwhere one encoder processes the RGB image and the other handles the depth\nimage. The encoded features from both encoders are then fused in the decoder\nusing an attention mechanism, effectively integrating the RGB and depth\nrepresentations. We use two different masking strategies, line and square, to\ntest the robustness of the model under different types of occlusions. To\nfurther analyze the effectiveness of our approach, we use Gradient-weighted\nClass Activation Mapping (Grad-CAM) visualizations to examine the regions of\ninterest the model focuses on during inpainting. We show that incorporating\ndepth information alongside the RGB image significantly improves the\nreconstruction quality. Through both qualitative and quantitative comparisons,\nwe demonstrate that the depth-integrated model outperforms the baseline, with\nattention mechanisms further enhancing inpainting performance, as evidenced by\nmultiple evaluation metrics and visualization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u7684\u65b0\u578b\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u7f16\u7801\u5668\u67b6\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56RGB\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u6df1\u5ea6\u4fe1\u606f\u5bf9\u7a7a\u95f4\u548c\u7ed3\u6784\u7406\u89e3\u7684\u91cd\u8981\u6027\u3002\u7ed3\u5408\u6df1\u5ea6\u4fe1\u606f\u53ef\u4ee5\u63d0\u5347\u4fee\u590d\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5206\u522b\u5904\u7406RGB\u548c\u6df1\u5ea6\u56fe\u50cf\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u7279\u5f81\u3002\u4f7f\u7528\u7ebf\u548c\u65b9\u5f62\u4e24\u79cd\u63a9\u7801\u7b56\u7565\u6d4b\u8bd5\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7Grad-CAM\u53ef\u89c6\u5316\u5206\u6790\u6a21\u578b\u5173\u6ce8\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u6df1\u5ea6\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u8d28\u91cf\uff0c\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6027\u80fd\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6df1\u5ea6\u4fe1\u606f\u7684\u5f15\u5165\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.01068", "pdf": "https://arxiv.org/pdf/2505.01068", "abs": "https://arxiv.org/abs/2505.01068", "authors": ["Yijie Jin", "Junjie Peng", "Xuanchao Lin", "Haochen Yuan", "Lan Wang", "Cangzhi Zheng"], "title": "Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) is a rapidly developing field that\nintegrates multimodal information to recognize sentiments, and existing models\nhave made significant progress in this area. The central challenge in MSA is\nmultimodal fusion, which is predominantly addressed by Multimodal Transformers\n(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.\nIn this work, from the perspective of efficiency optimization, we propose and\nprove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and\nwe introduce the graph-structured representation pattern of MulTs. Based on\nthis pattern, we propose an Interlaced Mask (IM) mechanism to design the\nGraph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is\nformally equivalent to MulTs which achieves an efficient weight-sharing\nmechanism without information disorder through IM, enabling All-Modal-In-One\nfusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called\nDecomposition is implemented to ensure avoiding additional computational\noverhead. Moreover, it achieves significantly higher performance than\ntraditional MulTs. To further validate the effectiveness of GsiT itself and the\nHMHG concept, we integrate them into multiple state-of-the-art models and\ndemonstrate notable performance improvements and parameter reduction on widely\nused MSA datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684Multimodal Transformer\uff08GsiT\uff09\uff0c\u901a\u8fc7Interlaced Mask\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\uff0c\u53c2\u6570\u4ec5\u4e3a\u4f20\u7edfMulTs\u76841/3\uff0c\u540c\u65f6\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "Multimodal Transformers\uff08MulTs\uff09\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u6548\u7387\u95ee\u9898\u3002\u672c\u6587\u4ece\u6548\u7387\u4f18\u5316\u7684\u89d2\u5ea6\uff0c\u63d0\u51faMulTs\u5b9e\u9645\u4e0a\u662f\u5c42\u6b21\u5316\u6a21\u6001\u5f02\u6784\u56fe\uff08HMHGs\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51faGsiT\u3002", "method": "\u5c06MulTs\u5efa\u6a21\u4e3aHMHGs\uff0c\u5e76\u63d0\u51faInterlaced Mask\u673a\u5236\u8bbe\u8ba1GsiT\uff0c\u5b9e\u73b0\u53c2\u6570\u5171\u4eab\u548c\u4fe1\u606f\u6709\u5e8f\u878d\u5408\u3002\u540c\u65f6\u5f00\u53d1\u4e86Decomposition Triton\u5185\u6838\u4ee5\u907f\u514d\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "result": "GsiT\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfMulTs\uff0c\u53c2\u6570\u51cf\u5c11\u81f31/3\uff0c\u5e76\u5728\u591a\u4e2a\u5148\u8fdb\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86HMHG\u6982\u5ff5\u7684\u6709\u6548\u6027\u3002", "conclusion": "GsiT\u548cHMHG\u6982\u5ff5\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u878d\u5408\u548c\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.01070", "pdf": "https://arxiv.org/pdf/2505.01070", "abs": "https://arxiv.org/abs/2505.01070", "authors": ["Edvin Fasth", "Sagar Singh"], "title": "Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits", "categories": ["cs.LG", "cs.AI"], "comment": "8 pages", "summary": "Knowledge distillation (KD) has become a powerful tool for training compact\nstudent models using larger, pretrained teacher models, often requiring less\ndata and computational resources. Teacher models typically possess more layers\nand thus exhibit richer feature representations compared to their student\ncounterparts. Furthermore, student models tend to learn simpler, surface-level\nfeatures in their early layers. This discrepancy can increase errors in groups\nwhere labels spuriously correlate with specific input attributes, leading to a\ndecline in group fairness even when overall accuracy remains comparable to the\nteacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs),\nwhich enable predictions at multiple intermediate layers, have been employed.\nConfidence margins derived from these early exits have been utilized to\nreweight both cross-entropy and distillation losses on a per-instance basis. In\nthis paper, we propose that leveraging Laplace approximation-based methods to\nobtain well-calibrated uncertainty estimates can also effectively reweight\nchallenging instances and improve group fairness. We hypothesize that Laplace\napproximation offers a more robust identification of difficult or ambiguous\ninstances compared to margin-based approaches. To validate our claims, we\nbenchmark our approach using a Bert-based model on the MultiNLI dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u7ec4\u516c\u5e73\u6027\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u56f0\u96be\u5b9e\u4f8b\u3002", "motivation": "\u89e3\u51b3\u77e5\u8bc6\u84b8\u998f\u4e2d\u5b66\u751f\u6a21\u578b\u5728\u65e9\u671f\u5c42\u5b66\u4e60\u7b80\u5355\u7279\u5f81\u5bfc\u81f4\u7684\u7ec4\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u83b7\u53d6\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u91cd\u65b0\u52a0\u6743\u4ea4\u53c9\u71b5\u548c\u84b8\u998f\u635f\u5931\u3002", "result": "\u5728MultiNLI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u6bd4\u57fa\u4e8e\u8fb9\u9645\u7684\u65b9\u6cd5\u66f4\u80fd\u6709\u6548\u8bc6\u522b\u56f0\u96be\u5b9e\u4f8b\uff0c\u63d0\u5347\u7ec4\u516c\u5e73\u6027\u3002"}}
{"id": "2505.00747", "pdf": "https://arxiv.org/pdf/2505.00747", "abs": "https://arxiv.org/abs/2505.00747", "authors": ["Zhiying Song", "Tenghui Xie", "Fuxi Wen", "Jun Li"], "title": "Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey", "categories": ["cs.OH", "cs.CV", "cs.MA", "cs.RO"], "comment": null, "summary": "Cooperative perception extends the perception capabilities of autonomous\nvehicles by enabling multi-agent information sharing via Vehicle-to-Everything\n(V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic\n\"information sensor\" characterized by limited communication, heterogeneity,\nmobility, and scalability. This survey provides a comprehensive review of\nrecent advancements from the perspective of information-centric cooperative\nperception, focusing on three key dimensions: information representation,\ninformation fusion, and large-scale deployment. We categorize information\nrepresentation into data-level, feature-level, and object-level schemes, and\nhighlight emerging methods for reducing data volume and compressing messages\nunder communication constraints. In information fusion, we explore techniques\nunder both ideal and non-ideal conditions, including those addressing\nheterogeneity, localization errors, latency, and packet loss. Finally, we\nsummarize system-level approaches to support scalability in dense traffic\nscenarios. Compared with existing surveys, this paper introduces a new\nperspective by treating V2X communication as an information sensor and\nemphasizing the challenges of deploying cooperative perception in real-world\nintelligent transportation systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eV2X\u901a\u4fe1\u7684\u591a\u667a\u80fd\u4f53\u534f\u540c\u611f\u77e5\u6280\u672f\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u4fe1\u606f\u8868\u793a\u3001\u4fe1\u606f\u878d\u5408\u548c\u5927\u89c4\u6a21\u90e8\u7f72\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u5c06V2X\u89c6\u4e3a\u52a8\u6001\u4fe1\u606f\u4f20\u611f\u5668\u7684\u65b0\u89c6\u89d2\u3002", "motivation": "\u4f20\u7edf\u8f66\u8f7d\u4f20\u611f\u5668\u5728\u611f\u77e5\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800cV2X\u901a\u4fe1\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4fe1\u606f\u5171\u4eab\u6269\u5c55\u4e86\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u9762\u4e34\u901a\u4fe1\u9650\u5236\u3001\u5f02\u6784\u6027\u3001\u79fb\u52a8\u6027\u548c\u53ef\u6269\u5c55\u6027\u7b49\u6311\u6218\u3002", "method": "\u4ece\u4fe1\u606f\u8868\u793a\uff08\u6570\u636e\u7ea7\u3001\u7279\u5f81\u7ea7\u3001\u5bf9\u8c61\u7ea7\uff09\u3001\u4fe1\u606f\u878d\u5408\uff08\u7406\u60f3\u4e0e\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\u7684\u6280\u672f\uff09\u4ee5\u53ca\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u7cfb\u7edf\u7ea7\u65b9\u6cd5\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7efc\u8ff0\u3002", "result": "\u603b\u7ed3\u4e86\u51cf\u5c11\u6570\u636e\u91cf\u3001\u538b\u7f29\u6d88\u606f\u3001\u5904\u7406\u5f02\u6784\u6027\u3001\u5b9a\u4f4d\u8bef\u5dee\u3001\u5ef6\u8fdf\u548c\u6570\u636e\u5305\u4e22\u5931\u7684\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u652f\u6301\u5bc6\u96c6\u4ea4\u901a\u573a\u666f\u53ef\u6269\u5c55\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u5c06V2X\u89c6\u4e3a\u4fe1\u606f\u4f20\u611f\u5668\uff0c\u4e3a\u5b9e\u9645\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u90e8\u7f72\u534f\u540c\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u5f3a\u8c03\u4e86\u76f8\u5173\u6311\u6218\u3002"}}
{"id": "2505.01085", "pdf": "https://arxiv.org/pdf/2505.01085", "abs": "https://arxiv.org/abs/2505.01085", "authors": ["Alexander Wuttke", "Adrian Rauchfleisch", "Andreas Jungherr"], "title": "Artificial Intelligence in Government: Why People Feel They Lose Control", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The use of Artificial Intelligence (AI) in public administration is expanding\nrapidly, moving from automating routine tasks to deploying generative and\nagentic systems that autonomously act on goals. While AI promises greater\nefficiency and responsiveness, its integration into government functions raises\nconcerns about fairness, transparency, and accountability. This article applies\nprincipal-agent theory (PAT) to conceptualize AI adoption as a special case of\ndelegation, highlighting three core tensions: assessability (can decisions be\nunderstood?), dependency (can the delegation be reversed?), and contestability\n(can decisions be challenged?). These structural challenges may lead to a\n\"failure-by-success\" dynamic, where early functional gains obscure long-term\nrisks to democratic legitimacy. To test this framework, we conducted a\npre-registered factorial survey experiment across tax, welfare, and law\nenforcement domains. Our findings show that although efficiency gains initially\nbolster trust, they simultaneously reduce citizens' perceived control. When the\nstructural risks come to the foreground, institutional trust and perceived\ncontrol both drop sharply, suggesting that hidden costs of AI adoption\nsignificantly shape public attitudes. The study demonstrates that PAT offers a\npowerful lens for understanding the institutional and political implications of\nAI in government, emphasizing the need for policymakers to address delegation\nrisks transparently to maintain public trust.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8AI\u5728\u516c\u5171\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u59d4\u6258\u4ee3\u7406\u7406\u8bba\u5206\u6790\u5176\u6f5c\u5728\u98ce\u9669\uff0c\u53d1\u73b0\u6548\u7387\u63d0\u5347\u867d\u589e\u5f3a\u4fe1\u4efb\u4f46\u524a\u5f31\u516c\u6c11\u63a7\u5236\u611f\uff0c\u9700\u900f\u660e\u5e94\u5bf9\u4ee5\u7ef4\u6301\u516c\u4f17\u4fe1\u4efb\u3002", "motivation": "\u7814\u7a76AI\u5728\u653f\u5e9c\u804c\u80fd\u4e2d\u7684\u5e94\u7528\u5982\u4f55\u5f71\u54cd\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\uff0c\u63ed\u793a\u5176\u6f5c\u5728\u7684\u7ed3\u6784\u6027\u6311\u6218\u3002", "method": "\u91c7\u7528\u59d4\u6258\u4ee3\u7406\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6ce8\u518c\u7684\u56e0\u5b50\u8c03\u67e5\u5b9e\u9a8c\uff0c\u6db5\u76d6\u7a0e\u6536\u3001\u798f\u5229\u548c\u6267\u6cd5\u9886\u57df\u3002", "result": "\u6548\u7387\u63d0\u5347\u521d\u671f\u589e\u5f3a\u4fe1\u4efb\uff0c\u4f46\u964d\u4f4e\u516c\u6c11\u63a7\u5236\u611f\uff1b\u7ed3\u6784\u6027\u98ce\u9669\u663e\u73b0\u65f6\uff0c\u4fe1\u4efb\u548c\u63a7\u5236\u611f\u5747\u6025\u5267\u4e0b\u964d\u3002", "conclusion": "\u59d4\u6258\u4ee3\u7406\u7406\u8bba\u4e3a\u7406\u89e3AI\u5728\u653f\u5e9c\u4e2d\u7684\u653f\u6cbb\u548c\u5236\u5ea6\u5f71\u54cd\u63d0\u4f9b\u6709\u529b\u89c6\u89d2\uff0c\u653f\u7b56\u5236\u5b9a\u8005\u9700\u900f\u660e\u5e94\u5bf9\u59d4\u6258\u98ce\u9669\u4ee5\u7ef4\u6301\u516c\u4f17\u4fe1\u4efb\u3002"}}
{"id": "2505.00986", "pdf": "https://arxiv.org/pdf/2505.00986", "abs": "https://arxiv.org/abs/2505.00986", "authors": ["Xiao Ma", "Young D. Kwon", "Dong Ma"], "title": "On-demand Test-time Adaptation for Edge Devices", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual Test-time adaptation (CTTA) continuously adapts the deployed model\non every incoming batch of data. While achieving optimal accuracy, existing\nCTTA approaches present poor real-world applicability on resource-constrained\nedge devices, due to the substantial memory overhead and energy consumption. In\nthis work, we first introduce a novel paradigm -- on-demand TTA -- which\ntriggers adaptation only when a significant domain shift is detected. Then, we\npresent OD-TTA, an on-demand TTA framework for accurate and efficient\nadaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a\nlightweight domain shift detection mechanism to activate TTA only when it is\nneeded, drastically reducing the overall computation overhead, 2) a source\ndomain selection module that chooses an appropriate source model for\nadaptation, ensuring high and robust accuracy, 3) a decoupled Batch\nNormalization (BN) update scheme to enable memory-efficient adaptation with\nsmall batch sizes. Extensive experiments show that OD-TTA achieves comparable\nand even better performance while reducing the energy and computation overhead\nremarkably, making TTA a practical reality.", "AI": {"tldr": "OD-TTA\u662f\u4e00\u79cd\u6309\u9700\u89e6\u53d1\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u57df\u504f\u79fb\u68c0\u6d4b\u3001\u6e90\u57df\u9009\u62e9\u6a21\u5757\u548c\u89e3\u8026BN\u66f4\u65b0\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "\u73b0\u6709CTTA\u65b9\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u56e0\u5185\u5b58\u548c\u80fd\u8017\u95ee\u9898\u5b9e\u7528\u6027\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u8f7b\u91cf\u7ea7\u57df\u504f\u79fb\u68c0\u6d4b\uff1b2) \u6e90\u57df\u9009\u62e9\u6a21\u5757\uff1b3) \u89e3\u8026BN\u66f4\u65b0\u65b9\u6848\u3002", "result": "OD-TTA\u5728\u6027\u80fd\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "OD-TTA\u4f7fTTA\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.01094", "pdf": "https://arxiv.org/pdf/2505.01094", "abs": "https://arxiv.org/abs/2505.01094", "authors": ["Zuzanna Osika", "Roxana Radelescu", "Jazmin Zatarain Salazar", "Frans Oliehoek", "Pradeep K. Murukannaiah"], "title": "Multi-Objective Reinforcement Learning for Water Management", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to AAMAS 2025", "summary": "Many real-world problems (e.g., resource management, autonomous driving, drug\ndiscovery) require optimizing multiple, conflicting objectives. Multi-objective\nreinforcement learning (MORL) extends classic reinforcement learning to handle\nmultiple objectives simultaneously, yielding a set of policies that capture\nvarious trade-offs. However, the MORL field lacks complex, realistic\nenvironments and benchmarks. We introduce a water resource (Nile river basin)\nmanagement case study and model it as a MORL environment. We then benchmark\nexisting MORL algorithms on this task. Our results show that specialized water\nmanagement methods outperform state-of-the-art MORL approaches, underscoring\nthe scalability challenges MORL algorithms face in real-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u7684\u6c34\u8d44\u6e90\u7ba1\u7406\u6848\u4f8b\u7814\u7a76\uff0c\u5e76\u53d1\u73b0\u73b0\u6709MORL\u7b97\u6cd5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u9762\u4e34\u6269\u5c55\u6027\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u95ee\u9898\uff08\u5982\u8d44\u6e90\u7ba1\u7406\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u836f\u7269\u53d1\u73b0\uff09\u9700\u8981\u4f18\u5316\u591a\u4e2a\u51b2\u7a81\u76ee\u6807\uff0c\u4f46MORL\u9886\u57df\u7f3a\u4e4f\u590d\u6742\u3001\u771f\u5b9e\u7684\u73af\u5883\u548c\u57fa\u51c6\u3002", "method": "\u5f15\u5165\u5c3c\u7f57\u6cb3\u6d41\u57df\u6c34\u8d44\u6e90\u7ba1\u7406\u6848\u4f8b\uff0c\u5c06\u5176\u5efa\u6a21\u4e3aMORL\u73af\u5883\uff0c\u5e76\u5728\u6b64\u4efb\u52a1\u4e0a\u5bf9\u73b0\u6709MORL\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e13\u4e1a\u7684\u6c34\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684MORL\u65b9\u6cd5\u3002", "conclusion": "MORL\u7b97\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6269\u5c55\u6027\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2505.00995", "pdf": "https://arxiv.org/pdf/2505.00995", "abs": "https://arxiv.org/abs/2505.00995", "authors": ["Taewook Park", "Jinwoo Lee", "Hyondong Oh", "Won-Jae Yun", "Kyu-Wha Lee"], "title": "Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at 2025 ICRA workshop on field robotics", "summary": "As the agricultural workforce declines and labor costs rise, robotic yield\nestimation has become increasingly important. While unmanned ground vehicles\n(UGVs) are commonly used for indoor farm monitoring, their deployment in\ngreenhouses is often constrained by infrastructure limitations, sensor\nplacement challenges, and operational inefficiencies. To address these issues,\nwe develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D\ncamera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial\nodometry algorithm for precise navigation in GNSS-denied environments and\nutilizes a 3D multi-object tracking algorithm to estimate the count and weight\nof cherry tomatoes. We evaluate the system using two dataset: one from a\nharvesting row and another from a growing row. In the harvesting-row dataset,\nthe proposed system achieves 94.4\\% counting accuracy and 87.5\\% weight\nestimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For\nthe growing-row dataset, which consists of occluded unripened fruits, we\nqualitatively analyze tracking performance and highlight future research\ndirections for improving perception in greenhouse with strong occlusions. Our\nfindings demonstrate the potential of UAVs for efficient robotic yield\nestimation in commercial greenhouses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65e0\u4eba\u673a\uff08UAV\uff09\u7cfb\u7edf\uff0c\u7528\u4e8e\u6e29\u5ba4\u4e2d\u7684\u756a\u8304\u4ea7\u91cf\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u5730\u9762\u673a\u5668\u4eba\uff08UGV\uff09\u5728\u6e29\u5ba4\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u519c\u4e1a\u52b3\u52a8\u529b\u51cf\u5c11\u548c\u6210\u672c\u4e0a\u5347\uff0c\u673a\u5668\u4eba\u4ea7\u91cf\u4f30\u8ba1\u53d8\u5f97\u91cd\u8981\uff0c\u4f46UGV\u5728\u6e29\u5ba4\u4e2d\u90e8\u7f72\u53d7\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u914d\u5907RGB-D\u76f8\u673a\u30013D LiDAR\u548cIMU\u4f20\u611f\u5668\u7684UAV\uff0c\u91c7\u7528LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7b97\u6cd5\u5bfc\u822a\uff0c\u5e76\u4f7f\u75283D\u591a\u76ee\u6807\u8ddf\u8e2a\u7b97\u6cd5\u4f30\u8ba1\u756a\u8304\u6570\u91cf\u548c\u91cd\u91cf\u3002", "result": "\u5728\u6536\u83b7\u884c\u6570\u636e\u96c6\u4e2d\uff0c\u8ba1\u6570\u51c6\u786e\u738794.4%\uff0c\u91cd\u91cf\u4f30\u8ba1\u51c6\u786e\u738787.5%\uff1b\u5728\u751f\u957f\u884c\u6570\u636e\u96c6\u4e2d\u5b9a\u6027\u5206\u6790\u4e86\u906e\u6321\u95ee\u9898\u3002", "conclusion": "UAV\u5728\u5546\u4e1a\u6e29\u5ba4\u4e2d\u5177\u6709\u9ad8\u6548\u4ea7\u91cf\u4f30\u8ba1\u7684\u6f5c\u529b\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u906e\u6321\u73af\u5883\u4e0b\u7684\u611f\u77e5\u95ee\u9898\u3002"}}
{"id": "2505.01130", "pdf": "https://arxiv.org/pdf/2505.01130", "abs": "https://arxiv.org/abs/2505.01130", "authors": ["Marco C. Campi", "Algo Car\u00e8", "Luis G. Crespo", "Simone Garatti", "Federico A. Ramponi"], "title": "Risk Analysis and Design Against Adversarial Actions", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Learning models capable of providing reliable predictions in the face of\nadversarial actions has become a central focus of the machine learning\ncommunity in recent years. This challenge arises from observing that data\nencountered at deployment time often deviate from the conditions under which\nthe model was trained. In this paper, we address deployment-time adversarial\nactions and propose a versatile, well-principled framework to evaluate the\nmodel's robustness against attacks of diverse types and intensities. While we\ninitially focus on Support Vector Regression (SVR), the proposed approach\nextends naturally to the broad domain of learning via relaxed optimization\ntechniques. Our results enable an assessment of the model vulnerability without\nrequiring additional test data and operate in a distribution-free setup. These\nresults not only provide a tool to enhance trust in the model's applicability\nbut also aid in selecting among competing alternatives. Later in the paper, we\nshow that our findings also offer useful insights for establishing new results\nwithin the out-of-distribution framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u653b\u51fb\u7c7b\u578b\u548c\u5f3a\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u6d4b\u8bd5\u6570\u636e\u3002", "motivation": "\u7814\u7a76\u6a21\u578b\u5728\u90e8\u7f72\u65f6\u9762\u5bf9\u5bf9\u6297\u884c\u4e3a\u7684\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u8bad\u7ec3\u4e0e\u90e8\u7f72\u6761\u4ef6\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u677e\u5f1b\u4f18\u5316\u6280\u672f\u3002", "result": "\u65e0\u9700\u989d\u5916\u6d4b\u8bd5\u6570\u636e\u5373\u53ef\u8bc4\u4f30\u6a21\u578b\u8106\u5f31\u6027\uff0c\u7ed3\u679c\u6709\u52a9\u4e8e\u589e\u5f3a\u6a21\u578b\u53ef\u4fe1\u5ea6\u5e76\u652f\u6301\u6a21\u578b\u9009\u62e9\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5e76\u4e3a\u5206\u5e03\u5916\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2505.01113", "pdf": "https://arxiv.org/pdf/2505.01113", "abs": "https://arxiv.org/abs/2505.01113", "authors": ["Xun Li", "Jian Yang", "Fenli Jia", "Muyu Wang", "Qi Wu", "Jun Wu", "Jinpeng Mi", "Jilin Hu", "Peidong Liang", "Xuan Tang", "Ke Li", "Xiong You", "Xian Wei"], "title": "NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization", "categories": ["cs.RO", "cs.CV", "cs.NE"], "comment": null, "summary": "Recently, camera localization has been widely adopted in autonomous robotic\nnavigation due to its efficiency and convenience. However, autonomous\nnavigation in unknown environments often suffers from scene ambiguity,\nenvironmental disturbances, and dynamic object transformation in camera\nlocalization. To address this problem, inspired by the biological brain\nnavigation mechanism (such as grid cells, place cells, and head direction\ncells), we propose a novel neurobiological camera location method, namely\nNeuroLoc. Firstly, we designed a Hebbian learning module driven by place cells\nto save and replay historical information, aiming to restore the details of\nhistorical representations and solve the issue of scene fuzziness. Secondly, we\nutilized the head direction cell-inspired internal direction learning as\nmulti-head attention embedding to help restore the true orientation in similar\nscenes. Finally, we added a 3D grid center prediction in the pose regression\nmodule to reduce the final wrong prediction. We evaluate the proposed NeuroLoc\non commonly used benchmark indoor and outdoor datasets. The experimental\nresults show that our NeuroLoc can enhance the robustness in complex\nenvironments and improve the performance of pose regression by using only a\nsingle image.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u751f\u7269\u5927\u8111\u5bfc\u822a\u673a\u5236\u542f\u53d1\u7684\u795e\u7ecf\u751f\u7269\u5b66\u76f8\u673a\u5b9a\u4f4d\u65b9\u6cd5NeuroLoc\uff0c\u901a\u8fc7Hebbian\u5b66\u4e60\u6a21\u5757\u3001\u65b9\u5411\u5b66\u4e60\u5d4c\u5165\u548c3D\u7f51\u683c\u4e2d\u5fc3\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u573a\u666f\u6a21\u7cca\u6027\u548c\u65b9\u5411\u6062\u590d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u672a\u77e5\u73af\u5883\u4e2d\u76f8\u673a\u5b9a\u4f4d\u9762\u4e34\u7684\u573a\u666f\u6a21\u7cca\u6027\u3001\u73af\u5883\u5e72\u6270\u548c\u52a8\u6001\u7269\u4f53\u53d8\u5316\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1Hebbian\u5b66\u4e60\u6a21\u5757\u4fdd\u5b58\u5386\u53f2\u4fe1\u606f\uff0c\u5229\u7528\u65b9\u5411\u5b66\u4e60\u5d4c\u5165\u6062\u590d\u771f\u5b9e\u65b9\u5411\uff0c\u6dfb\u52a03D\u7f51\u683c\u4e2d\u5fc3\u9884\u6d4b\u51cf\u5c11\u9519\u8bef\u9884\u6d4b\u3002", "result": "\u5728\u5ba4\u5185\u5916\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cNeuroLoc\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4ec5\u7528\u5355\u5f20\u56fe\u50cf\u5373\u53ef\u6539\u8fdb\u59ff\u6001\u56de\u5f52\u6027\u80fd\u3002", "conclusion": "NeuroLoc\u901a\u8fc7\u4eff\u751f\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u76f8\u673a\u5b9a\u4f4d\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.01162", "pdf": "https://arxiv.org/pdf/2505.01162", "abs": "https://arxiv.org/abs/2505.01162", "authors": ["Chebrolu Niranjan", "Kokil Jaidka", "Gerard Christopher Yeo"], "title": "On the Limitations of Steering in Language Model Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5bfc\u5411\u5411\u91cf\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5176\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u4ef7\u503c\u89c2\u5bf9\u9f50\uff09\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e0b\u53ef\u80fd\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u5bfc\u5411\u5411\u91cf\u4f5c\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u673a\u5236\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u53d8\u538b\u5668\u94a9\u5e72\u9884\u548c\u53cd\u4e49\u8bcd\u529f\u80fd\u5411\u91cf\u6846\u67b6\uff0c\u5206\u6790\u63d0\u793a\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u5bf9\u5bfc\u5411\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5bfc\u5411\u5411\u91cf\u5728\u7279\u5b9a\u5bf9\u9f50\u4efb\u52a1\uff08\u5982\u4ef7\u503c\u89c2\u5bf9\u9f50\uff09\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e0b\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "conclusion": "\u5bfc\u5411\u5411\u91cf\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u6709\u524d\u666f\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u5176\u901a\u7528\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2505.01237", "pdf": "https://arxiv.org/pdf/2505.01237", "abs": "https://arxiv.org/abs/2505.01237", "authors": ["Edson Araujo", "Andrew Rouditchenko", "Yuan Gong", "Saurabhchand Bhati", "Samuel Thomas", "Brian Kingsbury", "Leonid Karlinsky", "Rogerio Feris", "James R. Glass"], "title": "CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": "To be published at CVPR 2025, code available at\n  https://github.com/edsonroteia/cav-mae-sync", "summary": "Recent advances in audio-visual learning have shown promising results in\nlearning representations across modalities. However, most approaches rely on\nglobal audio representations that fail to capture fine-grained temporal\ncorrespondences with visual frames. Additionally, existing methods often\nstruggle with conflicting optimization objectives when trying to jointly learn\nreconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync\nas a simple yet effective extension of the original CAV-MAE framework for\nself-supervised audio-visual learning. We address three key challenges: First,\nwe tackle the granularity mismatch between modalities by treating audio as a\ntemporal sequence aligned with video frames, rather than using global\nrepresentations. Second, we resolve conflicting optimization goals by\nseparating contrastive and reconstruction objectives through dedicated global\ntokens. Third, we improve spatial localization by introducing learnable\nregister tokens that reduce semantic load on patch tokens. We evaluate the\nproposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on\nzero-shot retrieval, classification and localization tasks demonstrating\nstate-of-the-art performance and outperforming more complex architectures.", "AI": {"tldr": "CAV-MAE Sync\u6539\u8fdb\u81eaCAV-MAE\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u5e8f\u5bf9\u9f50\u97f3\u9891\u4e0e\u89c6\u9891\u5e27\u3001\u5206\u79bb\u5bf9\u6bd4\u4e0e\u91cd\u5efa\u76ee\u6807\u3001\u5f15\u5165\u53ef\u5b66\u4e60\u6807\u8bb0\uff0c\u89e3\u51b3\u4e86\u97f3\u9891-\u89c6\u89c9\u5b66\u4e60\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u4f18\u5316\u51b2\u7a81\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u5b66\u4e60\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u5bf9\u9f50\u548c\u4f18\u5316\u76ee\u6807\u51b2\u7a81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCAV-MAE Sync\uff0c\u901a\u8fc7\u65f6\u5e8f\u5bf9\u9f50\u97f3\u9891\u4e0e\u89c6\u9891\u5e27\u3001\u5206\u79bb\u5bf9\u6bd4\u4e0e\u91cd\u5efa\u76ee\u6807\u3001\u5f15\u5165\u53ef\u5b66\u4e60\u6807\u8bb0\uff0c\u6539\u8fdb\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728AudioSet\u3001VGG Sound\u548cADE20K Sound\u6570\u636e\u96c6\u4e0a\uff0c\u96f6\u6837\u672c\u68c0\u7d22\u3001\u5206\u7c7b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CAV-MAE Sync\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u97f3\u9891-\u89c6\u89c9\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2505.01168", "pdf": "https://arxiv.org/pdf/2505.01168", "abs": "https://arxiv.org/abs/2505.01168", "authors": ["Zhaoyang Ma", "Zhihao Wu", "Wang Lu", "Xin Gao", "Jinghang Yue", "Taolin Zhang", "Lipo Wang", "Youfang Lin", "Jing Wang"], "title": "Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The development of model ensemble attacks has significantly improved the\ntransferability of adversarial examples, but this progress also poses severe\nthreats to the security of deep neural networks. Existing methods, however,\nface two critical challenges: insufficient capture of shared gradient\ndirections across models and a lack of adaptive weight allocation mechanisms.\nTo address these issues, we propose a novel method Harmonized Ensemble for\nAdversarial Transferability (HEAT), which introduces domain generalization into\nadversarial example generation for the first time. HEAT consists of two key\nmodules: Consensus Gradient Direction Synthesizer, which uses Singular Value\nDecomposition to synthesize shared gradient directions; and Dual-Harmony Weight\nOrchestrator which dynamically balances intra-domain coherence, stabilizing\ngradients within individual models, and inter-domain diversity, enhancing\ntransferability across models. Experimental results demonstrate that HEAT\nsignificantly outperforms existing methods across various datasets and\nsettings, offering a new perspective and direction for adversarial attack\nresearch.", "AI": {"tldr": "HEAT\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6837\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u57df\u6cdb\u5316\u548c\u52a8\u6001\u6743\u91cd\u5206\u914d\u63d0\u5347\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u5171\u4eab\u68af\u5ea6\u65b9\u5411\u548c\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u3002", "method": "HEAT\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a\u5171\u8bc6\u68af\u5ea6\u65b9\u5411\u5408\u6210\u5668\uff08\u4f7f\u7528SVD\u5408\u6210\u5171\u4eab\u68af\u5ea6\u65b9\u5411\uff09\u548c\u53cc\u548c\u8c10\u6743\u91cd\u534f\u8c03\u5668\uff08\u52a8\u6001\u5e73\u8861\u57df\u5185\u4e00\u81f4\u6027\u548c\u57df\u95f4\u591a\u6837\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHEAT\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HEAT\u4e3a\u5bf9\u6297\u653b\u51fb\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u65b9\u5411\u3002"}}
{"id": "2505.01239", "pdf": "https://arxiv.org/pdf/2505.01239", "abs": "https://arxiv.org/abs/2505.01239", "authors": ["Elena Mulero Ayll\u00f3n", "Massimiliano Mantegna", "Linlin Shen", "Paolo Soda", "Valerio Guarrasi", "Matteo Tortora"], "title": "Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate lung tumor segmentation is crucial for improving diagnosis,\ntreatment planning, and patient outcomes in oncology. However, the complexity\nof tumor morphology, size, and location poses significant challenges for\nautomated segmentation. This study presents a comprehensive benchmarking\nanalysis of deep learning-based segmentation models, comparing traditional\narchitectures such as U-Net and DeepLabV3, self-configuring models like nnUNet,\nand foundation models like MedSAM, and MedSAM~2. Evaluating performance across\ntwo lung tumor segmentation datasets, we assess segmentation accuracy and\ncomputational efficiency under various learning paradigms, including few-shot\nlearning and fine-tuning. The results reveal that while traditional models\nstruggle with tumor delineation, foundation models, particularly MedSAM~2,\noutperform them in both accuracy and computational efficiency. These findings\nunderscore the potential of foundation models for lung tumor segmentation,\nhighlighting their applicability in improving clinical workflows and patient\noutcomes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u80ba\u80bf\u7624\u5206\u5272\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\uff0c\u53d1\u73b0\u57fa\u7840\u6a21\u578b\uff08\u5982MedSAM~2\uff09\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u80ba\u80bf\u7624\u5206\u5272\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u80bf\u7624\u5f62\u6001\u3001\u5927\u5c0f\u548c\u4f4d\u7f6e\u7684\u590d\u6742\u6027\u4e3a\u81ea\u52a8\u5316\u5206\u5272\u5e26\u6765\u6311\u6218\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u6a21\u578b\uff08\u5982U-Net\u3001DeepLabV3\uff09\u3001\u81ea\u914d\u7f6e\u6a21\u578b\uff08\u5982nnUNet\uff09\u548c\u57fa\u7840\u6a21\u578b\uff08\u5982MedSAM\u3001MedSAM~2\uff09\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u4e24\u79cd\u80ba\u80bf\u7624\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u57fa\u7840\u6a21\u578b\uff08\u5c24\u5176\u662fMedSAM~2\uff09\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u80ba\u80bf\u7624\u5206\u5272\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u6539\u5584\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u548c\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2505.01169", "pdf": "https://arxiv.org/pdf/2505.01169", "abs": "https://arxiv.org/abs/2505.01169", "authors": ["Pramook Khungurn", "Pratch Piyawongwisal", "Sira Sriswadi", "Supasorn Suwajanakorn"], "title": "Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A flow matching model learns a time-dependent vector field $v_t(x)$ that\ngenerates a probability path $\\{ p_t \\}_{0 \\leq t \\leq 1}$ that interpolates\nbetween a well-known noise distribution ($p_0$) and the data distribution\n($p_1$). It can be distilled into a \\emph{two-timed flow model} (TTFM)\n$\\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an\ninitial time $s$ to another belonging to the distribution at a terminal time\n$t$ in one function evaluation. We present a new loss function for TTFM\ndistillation called the \\emph{initial/terminal velocity matching} (ITVM) loss\nthat extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi\net al. by adding redundant terms to match the initial velocities at time $s$,\nremoving the derivative from the terminal velocity term at time $t$, and using\na version of the model under training, stabilized by exponential moving\naveraging (EMA), to compute the target terminal average velocity. Preliminary\nexperiments show that our loss leads to better few-step generation performance\non multiple types of datasets and model architectures over baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570ITVM\uff0c\u7528\u4e8e\u84b8\u998f\u53cc\u65f6\u95f4\u6d41\u6a21\u578b\uff08TTFM\uff09\uff0c\u901a\u8fc7\u5339\u914d\u521d\u59cb\u548c\u7ec8\u7aef\u901f\u5ea6\uff0c\u6539\u8fdb\u4e86\u5c11\u6b65\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u7684LFMD\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u63d0\u5347\u53cc\u65f6\u95f4\u6d41\u6a21\u578b\u7684\u84b8\u998f\u6548\u679c\u548c\u751f\u6210\u6027\u80fd\u3002", "method": "\u63d0\u51faITVM\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u6dfb\u52a0\u521d\u59cb\u901f\u5ea6\u5339\u914d\u9879\u3001\u79fb\u9664\u7ec8\u7aef\u901f\u5ea6\u9879\u7684\u5bfc\u6570\uff0c\u5e76\u4f7f\u7528EMA\u7a33\u5b9a\u6a21\u578b\u6765\u8ba1\u7b97\u76ee\u6807\u7ec8\u7aef\u901f\u5ea6\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cITVM\u635f\u5931\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u63d0\u5347\u4e86\u5c11\u6b65\u751f\u6210\u6027\u80fd\u3002", "conclusion": "ITVM\u635f\u5931\u51fd\u6570\u6709\u6548\u6539\u8fdb\u4e86\u53cc\u65f6\u95f4\u6d41\u6a21\u578b\u7684\u84b8\u998f\u6548\u679c\uff0c\u4e3a\u5c11\u6b65\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.01263", "pdf": "https://arxiv.org/pdf/2505.01263", "abs": "https://arxiv.org/abs/2505.01263", "authors": ["Gaoxiang Cong", "Liang Li", "Jiadong Pan", "Zhedong Zhang", "Amin Beheshti", "Anton van den Hengel", "Yuankai Qi", "Qingming Huang"], "title": "FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Movie Dubbing aims to convert scripts into speeches that align with the given\nmovie clip in both temporal and emotional aspects while preserving the vocal\ntimbre of a given brief reference audio. Existing methods focus primarily on\nreducing the word error rate while ignoring the importance of lip-sync and\nacoustic quality. To address these issues, we propose a large language model\n(LLM) based flow matching architecture for dubbing, named FlowDubber, which\nachieves high-quality audio-visual sync and pronunciation by incorporating a\nlarge speech language model and dual contrastive aligning while achieving\nbetter acoustic quality via the proposed voice-enhanced flow matching than\nprevious works. First, we introduce Qwen2.5 as the backbone of LLM to learn the\nin-context sequence from movie scripts and reference audio. Then, the proposed\nsemantic-aware learning focuses on capturing LLM semantic knowledge at the\nphoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment\nwith lip movement, reducing ambiguities where similar phonemes might be\nconfused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves\nacoustic quality in two aspects, which introduces an LLM-based acoustics flow\nmatching guidance to strengthen clarity and uses affine style prior to enhance\nidentity when recovering noise into mel-spectrograms via gradient vector field\nprediction. Extensive experiments demonstrate that our method outperforms\nseveral state-of-the-art methods on two primary benchmarks. The demos are\navailable at\n{\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.", "AI": {"tldr": "FlowDubber\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u914d\u97f3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u97f3\u589e\u5f3a\u6d41\u5339\u914d\u548c\u53cc\u5bf9\u6bd4\u5bf9\u9f50\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u97f3\u9891-\u89c6\u89c9\u540c\u6b65\u548c\u53d1\u97f3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u914d\u97f3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u964d\u4f4e\u8bcd\u9519\u8bef\u7387\uff0c\u800c\u5ffd\u7565\u4e86\u5507\u540c\u6b65\u548c\u58f0\u5b66\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002FlowDubber\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528Qwen2.5\u4f5c\u4e3aLLM\u9aa8\u5e72\u7f51\u7edc\u5b66\u4e60\u7535\u5f71\u5267\u672c\u548c\u53c2\u8003\u97f3\u9891\u7684\u4e0a\u4e0b\u6587\u5e8f\u5217\u30022. \u8bed\u4e49\u611f\u77e5\u5b66\u4e60\u6355\u83b7LLM\u5728\u97f3\u7d20\u7ea7\u522b\u7684\u8bed\u4e49\u77e5\u8bc6\u30023. \u53cc\u5bf9\u6bd4\u5bf9\u9f50\uff08DCA\uff09\u589e\u5f3a\u5507\u90e8\u8fd0\u52a8\u7684\u5bf9\u9f50\u30024. \u57fa\u4e8e\u6d41\u7684\u8bed\u97f3\u589e\u5f3a\uff08FVE\uff09\u901a\u8fc7LLM\u58f0\u5b66\u6d41\u5339\u914d\u548c\u4eff\u5c04\u98ce\u683c\u5148\u9a8c\u63d0\u5347\u58f0\u5b66\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlowDubber\u5728\u4e24\u4e2a\u4e3b\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlowDubber\u901a\u8fc7\u7ed3\u5408LLM\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u914d\u97f3\u7684\u97f3\u9891-\u89c6\u89c9\u540c\u6b65\u548c\u58f0\u5b66\u8d28\u91cf\u3002"}}
{"id": "2505.01177", "pdf": "https://arxiv.org/pdf/2505.01177", "abs": "https://arxiv.org/abs/2505.01177", "authors": ["Francisco Aguilera-Mart\u00ednez", "Fernando Berzal"], "title": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u5a01\u80c1\u4e0e\u9632\u5fa1\u673a\u5236\uff0c\u5206\u7c7b\u5206\u6790\u4e86\u8bad\u7ec3\u9636\u6bb5\u548c\u90e8\u7f72\u540e\u7684\u653b\u51fb\uff0c\u5e76\u63a2\u8ba8\u4e86\u9884\u9632\u4e0e\u68c0\u6d4b\u4e24\u7c7b\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u968f\u7740LLMs\u7684\u53d1\u5c55\uff0c\u8bc4\u4f30\u5176\u5b89\u5168\u5a01\u80c1\u548c\u6f0f\u6d1e\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u9636\u6bb5\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u548c\u5206\u6790\u653b\u51fb\u7c7b\u578b\uff0c\u5e76\u603b\u7ed3\u9884\u9632\u4e0e\u68c0\u6d4b\u9632\u5fa1\u673a\u5236\uff0c\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\u6765\u4fdd\u62a4LLMs\uff0c\u5e76\u6307\u51fa\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u9886\u57df\u3002", "conclusion": "\u8bba\u6587\u4e3aLLMs\u7684\u5b89\u5168\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89c6\u89d2\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u672a\u6765\u7814\u7a76\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2505.01313", "pdf": "https://arxiv.org/pdf/2505.01313", "abs": "https://arxiv.org/abs/2505.01313", "authors": ["Shang Wang", "Huanrong Tang", "Jianquan Ouyang"], "title": "A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture", "categories": ["cs.NE", "cs.CV"], "comment": "GECCO 2023", "summary": "This paper proposes a neural architecture search space using ResNet as a\nframework, with search objectives including parameters for convolution,\npooling, fully connected layers, and connectivity of the residual network. In\naddition to recognition accuracy, this paper uses the loss value on the\nvalidation set as a secondary objective for optimization. The experimental\nresults demonstrate that the search space of this paper together with the\noptimisation approach can find competitive network architectures on the MNIST,\nFashion-MNIST and CIFAR100 datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eResNet\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7a7a\u95f4\uff0c\u4f18\u5316\u76ee\u6807\u5305\u62ec\u5377\u79ef\u3001\u6c60\u5316\u3001\u5168\u8fde\u63a5\u5c42\u53c2\u6570\u53ca\u6b8b\u5dee\u7f51\u7edc\u8fde\u63a5\u6027\uff0c\u5e76\u4f7f\u7528\u9a8c\u8bc1\u96c6\u635f\u5931\u503c\u4f5c\u4e3a\u6b21\u8981\u4f18\u5316\u76ee\u6807\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728MNIST\u3001Fashion-MNIST\u548cCIFAR100\u6570\u636e\u96c6\u4e0a\u80fd\u627e\u5230\u6709\u7ade\u4e89\u529b\u7684\u7f51\u7edc\u67b6\u6784\u3002", "motivation": "\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4f18\u5316ResNet\u6846\u67b6\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u63a2\u7d22\u591a\u76ee\u6807\u4f18\u5316\uff08\u5982\u9a8c\u8bc1\u96c6\u635f\u5931\u503c\uff09\u5bf9\u67b6\u6784\u641c\u7d22\u7684\u5f71\u54cd\u3002", "method": "\u4ee5ResNet\u4e3a\u6846\u67b6\uff0c\u8bbe\u8ba1\u641c\u7d22\u7a7a\u95f4\uff0c\u4f18\u5316\u5377\u79ef\u3001\u6c60\u5316\u3001\u5168\u8fde\u63a5\u5c42\u53c2\u6570\u53ca\u6b8b\u5dee\u7f51\u7edc\u8fde\u63a5\u6027\uff0c\u5e76\u5f15\u5165\u9a8c\u8bc1\u96c6\u635f\u5931\u503c\u4f5c\u4e3a\u6b21\u8981\u4f18\u5316\u76ee\u6807\u3002", "result": "\u5728MNIST\u3001Fashion-MNIST\u548cCIFAR100\u6570\u636e\u96c6\u4e0a\u627e\u5230\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7f51\u7edc\u67b6\u6784\u3002", "conclusion": "\u63d0\u51fa\u7684\u641c\u7d22\u7a7a\u95f4\u548c\u4f18\u5316\u65b9\u6cd5\u5728\u591a\u6570\u636e\u96c6\u4e0a\u6709\u6548\uff0c\u9a8c\u8bc1\u4e86\u591a\u76ee\u6807\u4f18\u5316\u5728\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.01425", "pdf": "https://arxiv.org/pdf/2505.01425", "abs": "https://arxiv.org/abs/2505.01425", "authors": ["Jiefeng Li", "Jinkun Cao", "Haotian Zhang", "Davis Rempe", "Jan Kautz", "Umar Iqbal", "Ye Yuan"], "title": "GENMO: A GENeralist Model for Human MOtion", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "Project page: https://research.nvidia.com/labs/dair/genmo/", "summary": "Human motion modeling traditionally separates motion generation and\nestimation into distinct tasks with specialized models. Motion generation\nmodels focus on creating diverse, realistic motions from inputs like text,\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\nmotion trajectories from observations like videos. Despite sharing underlying\nrepresentations of temporal dynamics and kinematics, this separation limits\nknowledge transfer between tasks and requires maintaining separate models. We\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\nestimation and generation in a single framework. Our key insight is to\nreformulate motion estimation as constrained motion generation, where the\noutput motion must precisely satisfy observed conditioning signals. Leveraging\nthe synergy between regression and diffusion, GENMO achieves accurate global\nmotion estimation while enabling diverse motion generation. We also introduce\nan estimation-guided training objective that exploits in-the-wild videos with\n2D annotations and text descriptions to enhance generative diversity.\nFurthermore, our novel architecture handles variable-length motions and mixed\nmultimodal conditions (text, audio, video) at different time intervals,\noffering flexible control. This unified approach creates synergistic benefits:\ngenerative priors improve estimated motions under challenging conditions like\nocclusions, while diverse video data enhances generation capabilities.\nExtensive experiments demonstrate GENMO's effectiveness as a generalist\nframework that successfully handles multiple human motion tasks within a single\nmodel.", "AI": {"tldr": "GENMO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4eba\u7c7b\u8fd0\u52a8\u901a\u7528\u6a21\u578b\uff0c\u5c06\u8fd0\u52a8\u751f\u6210\u548c\u4f30\u8ba1\u7ed3\u5408\u5728\u4e00\u4e2a\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u7ea6\u675f\u751f\u6210\u548c\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u8fd0\u52a8\u751f\u6210\u548c\u4f30\u8ba1\u5206\u5f00\uff0c\u9650\u5236\u4e86\u77e5\u8bc6\u5171\u4eab\u548c\u6a21\u578b\u6548\u7387\u3002GENMO\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GENMO\u5c06\u8fd0\u52a8\u4f30\u8ba1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7ea6\u675f\u751f\u6210\uff0c\u7ed3\u5408\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5229\u75282D\u6807\u6ce8\u548c\u6587\u672c\u63cf\u8ff0\u589e\u5f3a\u751f\u6210\u591a\u6837\u6027\u3002", "result": "GENMO\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u751f\u6210\u548c\u4f30\u8ba1\u6548\u679c\u5747\u4f18\u4e8e\u4f20\u7edf\u5206\u79bb\u6a21\u578b\u3002", "conclusion": "GENMO\u8bc1\u660e\u4e86\u7edf\u4e00\u6846\u67b6\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4eba\u7c7b\u8fd0\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u7075\u6d3b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.01185", "pdf": "https://arxiv.org/pdf/2505.01185", "abs": "https://arxiv.org/abs/2505.01185", "authors": ["Nahshon Mokua Obiri", "Kristof Van Laerhoven"], "title": "EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing", "categories": ["cs.NI", "cs.AI", "cs.LG", "eess.SP"], "comment": null, "summary": "LoRaWAN technology's extensive coverage positions it as a strong contender\nfor large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor\nlocalization remains challenging due to complex environmental conditions,\nmultipath fading, and transient obstructions. This paper proposes a lightweight\nbut robust approach combining adaptive filtering with an extended log-distance,\nmulti-wall path loss and shadowing (PLS) model. Our methodology augments\nconventional models with critical LoRaWAN parameters (received signal strength\nindicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic\nenvironmental indicators (temperature, humidity, carbon dioxide, particulate\nmatter, and barometric pressure). An adaptive Kalman filter reduces RSSI\nfluctuations, isolating persistent trends from momentary noise. Using a\nsix-month dataset of 1,328,334 field measurements, we evaluate three models:\nthe baseline COST 231 multi-wall model (MWM), the baseline model augmented with\nenvironmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered\nRSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF\nachieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP\n(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation\nreduces systematic errors by 41.22%, while Kalman filtering significantly\nenhances robustness under high RSSI volatility by 42.63%, on average across all\ndevices. These findings present an interpretable, efficient solution for\nprecise indoor LoRaWAN localization in dynamically changing environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u9002\u5e94\u6ee4\u6ce2\u4e0e\u6269\u5c55\u5bf9\u6570\u8ddd\u79bb\u591a\u5899\u8def\u5f84\u635f\u8017\u548c\u9634\u5f71\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LoRaWAN\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5ba4\u5185\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "LoRaWAN\u6280\u672f\u5728\u5927\u89c4\u6a21\u7269\u8054\u7f51\u90e8\u7f72\u4e2d\u5177\u6709\u5e7f\u6cdb\u8986\u76d6\u4f18\u52bf\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e9a10\u7c73\u7cbe\u5ea6\u7684\u5ba4\u5185\u5b9a\u4f4d\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e0e\u6269\u5c55\u5bf9\u6570\u8ddd\u79bb\u591a\u5899\u8def\u5f84\u635f\u8017\u548c\u9634\u5f71\u6a21\u578b\uff0c\u5e76\u5f15\u5165LoRaWAN\u53c2\u6570\uff08RSSI\u3001\u9891\u7387\u3001SNR\uff09\u548c\u52a8\u6001\u73af\u5883\u6307\u6807\uff08\u6e29\u5ea6\u3001\u6e7f\u5ea6\u7b49\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684MWM-EP-KF\u6a21\u578b\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a5.81\u7c73\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0817.98\u7c73\uff09\u548c\u73af\u5883\u589e\u5f3a\u6a21\u578b\uff0810.56\u7c73\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u5ba4\u5185LoRaWAN\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01186", "pdf": "https://arxiv.org/pdf/2505.01186", "abs": "https://arxiv.org/abs/2505.01186", "authors": ["M. Saeid HaghighiFard", "Sinem Coleri"], "title": "Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Hierarchical Federated Learning (HFL) has recently emerged as a promising\nsolution for intelligent decision-making in vehicular networks, helping to\naddress challenges such as limited communication resources, high vehicle\nmobility, and data heterogeneity. However, HFL remains vulnerable to\nadversarial and unreliable vehicles, whose misleading updates can significantly\ncompromise the integrity and convergence of the global model. To address these\nchallenges, we propose a novel defense framework that integrates dynamic\nvehicle selection with robust anomaly detection within a cluster-based HFL\narchitecture, specifically designed to counter Gaussian noise and gradient\nascent attacks. The framework performs a comprehensive reliability assessment\nfor each vehicle by evaluating historical accuracy, contribution frequency, and\nanomaly records. Anomaly detection combines Z-score and cosine similarity\nanalyses on model updates to identify both statistical outliers and directional\ndeviations in model updates. To further refine detection, an adaptive\nthresholding mechanism is incorporated into the cosine similarity metric,\ndynamically adjusting the threshold based on the historical accuracy of each\nvehicle to enforce stricter standards for consistently high-performing\nvehicles. In addition, a weighted gradient averaging mechanism is implemented,\nwhich assigns higher weights to gradient updates from more trustworthy\nvehicles. To defend against coordinated attacks, a cross-cluster consistency\ncheck is applied to identify collaborative attacks in which multiple\ncompromised clusters coordinate misleading updates. Together, these mechanisms\nform a multi-level defense strategy to filter out malicious contributions\neffectively. Simulation results show that the proposed algorithm significantly\nreduces convergence time compared to benchmark methods across both 1-hop and\n3-hop topologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5206\u5c42\u8054\u90a6\u5b66\u4e60\uff08HFL\uff09\u4e2d\u5bf9\u6297\u6027\u8f66\u8f86\u7684\u65b0\u578b\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8f66\u8f86\u9009\u62e9\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u591a\u7ea7\u9632\u5fa1\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6536\u655b\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "HFL\u5728\u8f66\u8054\u7f51\u4e2d\u9762\u4e34\u5bf9\u6297\u6027\u8f66\u8f86\u548c\u4e0d\u53ef\u9760\u6570\u636e\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\u6765\u4fdd\u62a4\u6a21\u578b\u5b8c\u6574\u6027\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u8f66\u8f86\u9009\u62e9\u3001Z-score\u548c\u4f59\u5f26\u76f8\u4f3c\u6027\u5206\u6790\u7684\u5f02\u5e38\u68c0\u6d4b\u3001\u81ea\u9002\u5e94\u9608\u503c\u673a\u5236\u3001\u52a0\u6743\u68af\u5ea6\u5e73\u5747\u4ee5\u53ca\u8de8\u96c6\u7fa4\u4e00\u81f4\u6027\u68c0\u67e5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u57281\u8df3\u548c3\u8df3\u62d3\u6251\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u6536\u655b\u65f6\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u7ea7\u9632\u5fa1\u6846\u67b6\u6709\u6548\u8fc7\u6ee4\u4e86\u6076\u610f\u8d21\u732e\uff0c\u63d0\u5347\u4e86HFL\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.01198", "pdf": "https://arxiv.org/pdf/2505.01198", "abs": "https://arxiv.org/abs/2505.01198", "authors": ["Mahdi Dhaini", "Ege Erdogan", "Nils Feldhus", "Gjergji Kasneci"], "title": "Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) 2025", "summary": "While research on applications and evaluations of explanation methods\ncontinues to expand, fairness of the explanation methods concerning disparities\nin their performance across subgroups remains an often overlooked aspect. In\nthis paper, we address this gap by showing that, across three tasks and five\nlanguage models, widely used post-hoc feature attribution methods exhibit\nsignificant gender disparity with respect to their faithfulness, robustness,\nand complexity. These disparities persist even when the models are pre-trained\nor fine-tuned on particularly unbiased datasets, indicating that the\ndisparities we observe are not merely consequences of biased training data. Our\nresults highlight the importance of addressing disparities in explanations when\ndeveloping and applying explainability methods, as these can lead to biased\noutcomes against certain subgroups, with particularly critical implications in\nhigh-stakes contexts. Furthermore, our findings underscore the importance of\nincorporating the fairness of explanations, alongside overall model fairness\nand explainability, as a requirement in regulatory frameworks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e7f\u6cdb\u4f7f\u7528\u7684\u540e\u9a8c\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u5728\u6027\u522b\u4e0a\u5b58\u5728\u663e\u8457\u7684\u5fe0\u5b9e\u6027\u3001\u9c81\u68d2\u6027\u548c\u590d\u6742\u6027\u5dee\u5f02\uff0c\u5373\u4f7f\u6a21\u578b\u5728\u65e0\u504f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002", "motivation": "\u63a2\u8ba8\u89e3\u91ca\u65b9\u6cd5\u5728\u516c\u5e73\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u5b50\u7ec4\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u5206\u6790\u4e09\u79cd\u4efb\u52a1\u548c\u4e94\u79cd\u8bed\u8a00\u6a21\u578b\u4e2d\u540e\u9a8c\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u7684\u6027\u522b\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u6027\u522b\u5dee\u5f02\uff0c\u4e14\u4e0e\u8bad\u7ec3\u6570\u636e\u65e0\u5173\u3002", "conclusion": "\u5f3a\u8c03\u5728\u5f00\u53d1\u548c\u5e94\u7528\u89e3\u91ca\u65b9\u6cd5\u65f6\u9700\u5173\u6ce8\u516c\u5e73\u6027\uff0c\u5e76\u5c06\u5176\u7eb3\u5165\u76d1\u7ba1\u6846\u67b6\u3002"}}
{"id": "2505.01238", "pdf": "https://arxiv.org/pdf/2505.01238", "abs": "https://arxiv.org/abs/2505.01238", "authors": ["Mahdi Dhaini", "Kafaite Zahra Hussain", "Efstratios Zaradoukas", "Gjergji Kasneci"], "title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to the xAI World Conference (2025) - System Demonstration", "summary": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP.", "AI": {"tldr": "EvalxNLP\u662f\u4e00\u4e2aPython\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30NLP\u6a21\u578b\u7684\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u89e3\u91ca\u6280\u672f\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u89e3\u91ca\u3002", "motivation": "\u968f\u7740NLP\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u89e3\u91ca\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u9700\u6c42\u9009\u62e9\u5408\u9002\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u516b\u79cdXAI\u6280\u672f\uff0c\u8bc4\u4f30\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3001\u5408\u7406\u6027\u548c\u590d\u6742\u6027\uff0c\u5e76\u63d0\u4f9bLLM\u751f\u6210\u7684\u4ea4\u4e92\u5f0f\u89e3\u91ca\u3002", "result": "\u7528\u6237\u6ee1\u610f\u5ea6\u9ad8\uff0c\u8868\u660eEvalxNLP\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6846\u67b6\u3002", "conclusion": "EvalxNLP\u65e8\u5728\u666e\u53ca\u89e3\u91ca\u5de5\u5177\uff0c\u652f\u6301XAI\u6280\u672f\u7684\u7cfb\u7edf\u6bd4\u8f83\u548c\u53d1\u5c55\u3002"}}
{"id": "2505.01261", "pdf": "https://arxiv.org/pdf/2505.01261", "abs": "https://arxiv.org/abs/2505.01261", "authors": ["Elie Saad", "Mariem Besbes", "Marc Zolghadri", "Victor Czmil", "Claude Baron", "Vincent Bourgeois"], "title": "Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The challenge of electronic component obsolescence is particularly critical\nin systems with long life cycles. Various obsolescence management methods are\nemployed to mitigate its impact, with obsolescence forecasting being a highly\nsought-after and prominent approach. As a result, numerous machine\nlearning-based forecasting methods have been proposed. However, machine\nlearning models require a substantial amount of relevant data to achieve high\nprecision, which is lacking in the current obsolescence landscape in some\nsituations. This work introduces a novel framework for obsolescence forecasting\nbased on deep learning. The proposed framework solves the lack of available\ndata through deep generative modeling, where new obsolescence cases are\ngenerated and used to augment the training dataset. The augmented dataset is\nthen used to train a classical machine learning-based obsolescence forecasting\nmodel. To train classical forecasting models using augmented datasets, existing\nclassical supervised-learning classifiers are adapted for semi-supervised\nlearning within this framework. The proposed framework demonstrates\nstate-of-the-art results on benchmarking datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7535\u5b50\u5143\u4ef6\u8fc7\u65f6\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u5148\u8fdb\u7ed3\u679c\u3002", "motivation": "\u7535\u5b50\u5143\u4ef6\u8fc7\u65f6\u5bf9\u957f\u751f\u547d\u5468\u671f\u7cfb\u7edf\u5f71\u54cd\u91cd\u5927\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u56e0\u6570\u636e\u4e0d\u8db3\u96be\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u751f\u6210\u65b0\u8fc7\u65f6\u6848\u4f8b\u4ee5\u6269\u5145\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u8c03\u6574\u7ecf\u5178\u76d1\u7763\u5b66\u4e60\u5206\u7c7b\u5668\u7528\u4e8e\u534a\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8fc7\u65f6\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.01273", "pdf": "https://arxiv.org/pdf/2505.01273", "abs": "https://arxiv.org/abs/2505.01273", "authors": ["Xuan Li", "Zhe Yin", "Xiaodong Gu", "Beijun Shen"], "title": "Anti-adversarial Learning: Desensitizing Prompts for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread use of LLMs, preserving privacy in user prompts has\nbecome crucial, as prompts risk exposing privacy and sensitive data to the\ncloud LLMs. Traditional techniques like homomorphic encryption, secure\nmulti-party computation, and federated learning face challenges due to heavy\ncomputational costs and user participation requirements, limiting their\napplicability in LLM scenarios. In this paper, we propose PromptObfus, a novel\nmethod for desensitizing LLM prompts. The core idea of PromptObfus is\n\"anti-adversarial\" learning, which perturbs privacy words in the prompt to\nobscure sensitive information while retaining the stability of model\npredictions. Specifically, PromptObfus frames prompt desensitization as a\nmasked language modeling task, replacing privacy-sensitive terms with a [MASK]\ntoken. A desensitization model is trained to generate candidate replacements\nfor each masked position. These candidates are subsequently selected based on\ngradient feedback from a surrogate model, ensuring minimal disruption to the\ntask output. We demonstrate the effectiveness of our approach on three NLP\ntasks. Results show that PromptObfus effectively prevents privacy inference\nfrom remote LLMs while preserving task performance.", "AI": {"tldr": "PromptObfus\u662f\u4e00\u79cd\u901a\u8fc7\u53cd\u5bf9\u6297\u5b66\u4e60\u6270\u52a8\u9690\u79c1\u8bcd\u4ee5\u4fdd\u62a4\u7528\u6237\u63d0\u793a\u9690\u79c1\u7684\u65b0\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "motivation": "\u968f\u7740LLMs\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u7528\u6237\u63d0\u793a\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u8ba1\u7b97\u6210\u672c\u548c\u7528\u6237\u53c2\u4e0e\u9700\u6c42\u800c\u53d7\u9650\u3002", "method": "PromptObfus\u5c06\u63d0\u793a\u8131\u654f\u89c6\u4e3a\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\uff0c\u7528[MASK]\u66ff\u6362\u9690\u79c1\u8bcd\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u6a21\u578b\u68af\u5ea6\u53cd\u9988\u9009\u62e9\u5019\u9009\u66ff\u6362\u8bcd\u3002", "result": "\u5728\u4e09\u4e2aNLP\u4efb\u52a1\u4e2d\uff0cPromptObfus\u6709\u6548\u9632\u6b62\u9690\u79c1\u6cc4\u9732\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "PromptObfus\u4e3aLLM\u63d0\u793a\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01281", "pdf": "https://arxiv.org/pdf/2505.01281", "abs": "https://arxiv.org/abs/2505.01281", "authors": ["Hao-Ran Yang", "Chuan-Xian Ren"], "title": "A Physics-preserved Transfer Learning Method for Differential Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While data-driven methods such as neural operator have achieved great success\nin solving differential equations (DEs), they suffer from domain shift problems\ncaused by different learning environments (with data bias or equation changes),\nwhich can be alleviated by transfer learning (TL). However, existing TL methods\nadopted in DEs problems lack either generalizability in general DEs problems or\nphysics preservation during training. In this work, we focus on a general\ntransfer learning method that adaptively correct the domain shift and preserve\nphysical information. Mathematically, we characterize the data domain as\nproduct distribution and the essential problems as distribution bias and\noperator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that\nsimultaneously admits generalizability to common DEs and physics preservation\nof specific problem is proposed to adapt the data-driven model to target domain\nutilizing the push-forward distribution induced by the POTT map. Extensive\nexperiments demonstrate the superior performance, generalizability and physics\npreservation of the proposed POTT method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fdd\u6301\u7684\u6700\u4f18\u5f20\u91cf\u4f20\u8f93\uff08POTT\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5fae\u5206\u65b9\u7a0b\u4e2d\u7684\u9886\u57df\u8f6c\u79fb\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff08\u5982\u795e\u7ecf\u7b97\u5b50\uff09\u5728\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u65f6\u5b58\u5728\u9886\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u73b0\u6709\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\u6216\u7269\u7406\u4fdd\u6301\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5c06\u6570\u636e\u57df\u5efa\u6a21\u4e3a\u4e58\u79ef\u5206\u5e03\uff0c\u63d0\u51faPOTT\u65b9\u6cd5\uff0c\u540c\u65f6\u89e3\u51b3\u5206\u5e03\u504f\u5dee\u548c\u7b97\u5b50\u504f\u5dee\uff0c\u5e76\u4fdd\u6301\u7269\u7406\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePOTT\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u901a\u7528\u6027\u548c\u7269\u7406\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "POTT\u65b9\u6cd5\u5728\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u9886\u57df\u8f6c\u79fb\u95ee\u9898\u65f6\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2505.01283", "pdf": "https://arxiv.org/pdf/2505.01283", "abs": "https://arxiv.org/abs/2505.01283", "authors": ["Hooman Danesh", "Maruthi Annamaraju", "Tim Brepols", "Stefanie Reese", "Surya R. Kalidindi"], "title": "Reduced-order structure-property linkages for stochastic metamaterials", "categories": ["cs.CE", "cs.AI", "cs.LG"], "comment": null, "summary": "The capabilities of additive manufacturing have facilitated the design and\nproduction of mechanical metamaterials with diverse unit cell geometries.\nEstablishing linkages between the vast design space of unit cells and their\neffective mechanical properties is critical for the efficient design and\nperformance evaluation of such metamaterials. However, physics-based\nsimulations of metamaterial unit cells across the entire design space are\ncomputationally expensive, necessitating a materials informatics framework to\nefficiently capture complex structure-property relationships. In this work,\nprincipal component analysis of 2-point correlation functions is performed to\nextract the salient features from a large dataset of randomly generated 2D\nmetamaterials. Physics-based simulations are performed using a fast Fourier\ntransform (FFT)-based homogenization approach to efficiently compute the\nhomogenized effective elastic stiffness across the extensive unit cell designs.\nSubsequently, Gaussian process regression is used to generate reduced-order\nsurrogates, mapping unit cell designs to their homogenized effective elastic\nconstant. It is demonstrated that the adopted workflow enables a high-value\nlow-dimensional representation of the voluminous stochastic metamaterial\ndataset, facilitating the construction of robust structure-property maps.\nFinally, an uncertainty-based active learning framework is utilized to train a\nsurrogate model with a significantly smaller number of data points compared to\nthe original full dataset. It is shown that a dataset as small as $0.61\\%$ of\nthe entire dataset is sufficient to generate accurate and robust\nstructure-property maps.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6750\u6599\u4fe1\u606f\u5b66\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u6210\u5206\u5206\u6790\u548c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff0c\u9ad8\u6548\u5730\u5efa\u7acb\u673a\u68b0\u8d85\u6750\u6599\u7684\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u51cf\u5c11\u6240\u9700\u6570\u636e\u91cf\u3002", "motivation": "\u673a\u68b0\u8d85\u6750\u6599\u7684\u8bbe\u8ba1\u7a7a\u95f4\u5e9e\u5927\uff0c\u4f20\u7edf\u7269\u7406\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6355\u6349\u590d\u6742\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\u63d0\u53d6\u968f\u673a\u751f\u6210\u76842D\u8d85\u6750\u6599\u6570\u636e\u96c6\u7279\u5f81\uff0c\u7ed3\u5408FFT\u5747\u8d28\u5316\u65b9\u6cd5\u8ba1\u7b97\u5f39\u6027\u521a\u5ea6\uff0c\u518d\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5efa\u7acb\u964d\u9636\u4ee3\u7406\u6a21\u578b\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u4ef7\u503c\u4f4e\u7ef4\u8868\u793a\uff0c\u4ec5\u97000.61%\u7684\u6570\u636e\u5373\u53ef\u751f\u6210\u51c6\u786e\u7684\u7ed3\u6784-\u6027\u80fd\u6620\u5c04\u3002", "conclusion": "\u63d0\u51fa\u7684\u5de5\u4f5c\u6d41\u7a0b\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u8d85\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2505.01286", "pdf": "https://arxiv.org/pdf/2505.01286", "abs": "https://arxiv.org/abs/2505.01286", "authors": ["Yajuan Zhang", "Jiahai Jiang", "Yule Yan", "Liang Yang", "Ping Zhang"], "title": "2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICDM 2024", "summary": "Accurate wind power forecasting can help formulate scientific dispatch plans,\nwhich is of great significance for maintaining the safety, stability, and\nefficient operation of the power system. In recent years, wind power\nforecasting methods based on deep learning have focused on extracting the\nspatiotemporal correlations among data, achieving significant improvements in\nforecasting accuracy. However, they exhibit two limitations. First, there is a\nlack of modeling for the inter-variable relationships, which limits the\naccuracy of the forecasts. Second, by treating endogenous and exogenous\nvariables equally, it leads to unnecessary interactions between the endogenous\nand exogenous variables, increasing the complexity of the model. In this paper,\nwe propose the 2DXformer, which, building upon the previous work's focus on\nspatiotemporal correlations, addresses the aforementioned two limitations.\nSpecifically, we classify the inputs of the model into three types: exogenous\nstatic variables, exogenous dynamic variables, and endogenous variables. First,\nwe embed these variables as variable tokens in a channel-independent manner.\nThen, we use the attention mechanism to capture the correlations among\nexogenous variables. Finally, we employ a multi-layer perceptron with residual\nconnections to model the impact of exogenous variables on endogenous variables.\nExperimental results on two real-world large-scale datasets indicate that our\nproposed 2DXformer can further improve the performance of wind power\nforecasting. The code is available in this repository:\n\\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa2DXformer\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u7c7b\u8f93\u5165\u53d8\u91cf\u5e76\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6539\u8fdb\u98ce\u7535\u529f\u7387\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u672a\u5145\u5206\u5efa\u6a21\u53d8\u91cf\u95f4\u5173\u7cfb\uff0c\u4e14\u672a\u533a\u5206\u5185\u751f\u4e0e\u5916\u751f\u53d8\u91cf\uff0c\u5bfc\u81f4\u9884\u6d4b\u7cbe\u5ea6\u53d7\u9650\u548c\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\u3002", "method": "\u5c06\u8f93\u5165\u5206\u4e3a\u5916\u751f\u9759\u6001\u3001\u5916\u751f\u52a8\u6001\u548c\u5185\u751f\u53d8\u91cf\uff0c\u72ec\u7acb\u5d4c\u5165\u540e\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u5916\u751f\u53d8\u91cf\u5173\u7cfb\uff0c\u518d\u7528\u591a\u5c42\u611f\u77e5\u673a\u5efa\u6a21\u5916\u751f\u5bf9\u5185\u751f\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c2DXformer\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u98ce\u7535\u529f\u7387\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "2DXformer\u901a\u8fc7\u6539\u8fdb\u53d8\u91cf\u5173\u7cfb\u5efa\u6a21\u548c\u533a\u5206\u53d8\u91cf\u7c7b\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2505.01288", "pdf": "https://arxiv.org/pdf/2505.01288", "abs": "https://arxiv.org/abs/2505.01288", "authors": ["Changhe Chen", "Quantao Yang", "Xiaohao Xu", "Nima Fazeli", "Olov Andersson"], "title": "ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "One of the central challenges preventing robots from acquiring complex\nmanipulation skills is the prohibitive cost of collecting large-scale robot\ndemonstrations. In contrast, humans are able to learn efficiently by watching\nothers interact with their environment. To bridge this gap, we introduce\nsemantic action flow as a core intermediate representation capturing the\nessential spatio-temporal manipulator-object interactions, invariant to\nsuperficial visual differences. We present ViSA-Flow, a framework that learns\nthis representation self-supervised from unlabeled large-scale video data.\nFirst, a generative model is pre-trained on semantic action flows automatically\nextracted from large-scale human-object interaction video data, learning a\nrobust prior over manipulation structure. Second, this prior is efficiently\nadapted to a target robot by fine-tuning on a small set of robot demonstrations\nprocessed through the same semantic abstraction pipeline. We demonstrate\nthrough extensive experiments on the CALVIN benchmark and real-world tasks that\nViSA-Flow achieves state-of-the-art performance, particularly in low-data\nregimes, outperforming prior methods by effectively transferring knowledge from\nhuman video observation to robotic execution. Videos are available at\nhttps://visaflow-web.github.io/ViSAFLOW.", "AI": {"tldr": "ViSA-Flow\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u52a8\u4f5c\u6d41\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u89c4\u6a21\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u83b7\u53d6\u590d\u6742\u64cd\u4f5c\u6280\u80fd\u7684\u6210\u672c\u3002", "motivation": "\u673a\u5668\u4eba\u83b7\u53d6\u590d\u6742\u64cd\u4f5c\u6280\u80fd\u7684\u6210\u672c\u9ad8\u6602\uff0c\u800c\u4eba\u7c7b\u901a\u8fc7\u89c2\u5bdf\u5b66\u4e60\u6548\u7387\u66f4\u9ad8\u3002ViSA-Flow\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u52a8\u4f5c\u6d41\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "ViSA-Flow\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u4ece\u65e0\u6807\u6ce8\u89c6\u9891\u6570\u636e\u4e2d\u63d0\u53d6\u8bed\u4e49\u52a8\u4f5c\u6d41\uff0c\u5e76\u5229\u7528\u751f\u6210\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u518d\u901a\u8fc7\u5c11\u91cf\u673a\u5668\u4eba\u6f14\u793a\u5fae\u8c03\u3002", "result": "\u5728CALVIN\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cViSA-Flow\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ViSA-Flow\u901a\u8fc7\u8bed\u4e49\u52a8\u4f5c\u6d41\u6709\u6548\u5b9e\u73b0\u4e86\u4ece\u4eba\u7c7b\u89c2\u5bdf\u5230\u673a\u5668\u4eba\u6267\u884c\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.01307", "pdf": "https://arxiv.org/pdf/2505.01307", "abs": "https://arxiv.org/abs/2505.01307", "authors": ["Regan Bolton", "Mohammadreza Sheikhfathollahi", "Simon Parkinson", "Vanessa Vulovic", "Gary Bamford", "Dan Basher", "Howard Parkinson"], "title": "Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Safety critical software assessment requires robust assessment against\ncomplex regulatory frameworks, a process traditionally limited by manual\nevaluation. This paper presents Document Retrieval-Augmented Fine-Tuning\n(DRAFT), a novel approach that enhances the capabilities of a large language\nmodel (LLM) for safety-critical compliance assessment. DRAFT builds upon\nexisting Retrieval-Augmented Generation (RAG) techniques by introducing a novel\nfine-tuning framework that accommodates our dual-retrieval architecture, which\nsimultaneously accesses both software documentation and applicable reference\nstandards. To fine-tune DRAFT, we develop a semi-automated dataset generation\nmethodology that incorporates variable numbers of relevant documents with\nmeaningful distractors, closely mirroring real-world assessment scenarios.\nExperiments with GPT-4o-mini demonstrate a 7% improvement in correctness over\nthe baseline model, with qualitative improvements in evidence handling,\nresponse structure, and domain-specific reasoning. DRAFT represents a practical\napproach to improving compliance assessment systems while maintaining the\ntransparency and evidence-based reasoning essential in regulatory domains.", "AI": {"tldr": "DRAFT\u662f\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5fae\u8c03\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u8f6f\u4ef6\u5408\u89c4\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684\u624b\u52a8\u8bc4\u4f30\u65b9\u6cd5\u5728\u5e94\u5bf9\u590d\u6742\u76d1\u7ba1\u6846\u67b6\u65f6\u6548\u7387\u4f4e\u4e0b\uff0cDRAFT\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "DRAFT\u91c7\u7528\u53cc\u68c0\u7d22\u67b6\u6784\uff0c\u540c\u65f6\u8bbf\u95ee\u8f6f\u4ef6\u6587\u6863\u548c\u53c2\u8003\u6807\u51c6\uff0c\u5e76\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cDRAFT\u5728GPT-4o-mini\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u6b63\u786e\u7387\u63d0\u53477%\uff0c\u4e14\u5728\u8bc1\u636e\u5904\u7406\u3001\u54cd\u5e94\u7ed3\u6784\u548c\u9886\u57df\u63a8\u7406\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "DRAFT\u4e3a\u5408\u89c4\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u900f\u660e\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u76d1\u7ba1\u9886\u57df\u3002"}}
{"id": "2505.01309", "pdf": "https://arxiv.org/pdf/2505.01309", "abs": "https://arxiv.org/abs/2505.01309", "authors": ["Anicet Lepetit Ondo", "Laurence Capus", "Mamadou Bousso"], "title": "Enhancing SPARQL Query Rewriting for Complex Ontology Alignments", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "SPARQL query rewriting is a fundamental mechanism for uniformly querying\nheterogeneous ontologies in the Linked Data Web. However, the complexity of\nontology alignments, particularly rich correspondences (c : c), makes this\nprocess challenging. Existing approaches primarily focus on simple (s : s) and\npartially complex ( s : c) alignments, thereby overlooking the challenges posed\nby more expressive alignments. Moreover, the intricate syntax of SPARQL\npresents a barrier for non-expert users seeking to fully exploit the knowledge\nencapsulated in ontologies. This article proposes an innovative approach for\nthe automatic rewriting of SPARQL queries from a source ontology to a target\nontology, based on a user's need expressed in natural language. It leverages\nthe principles of equivalence transitivity as well as the advanced capabilities\nof large language models such as GPT-4. By integrating these elements, this\napproach stands out for its ability to efficiently handle complex alignments,\nparticularly (c : c) correspondences , by fully exploiting their\nexpressiveness. Additionally, it facilitates access to aligned ontologies for\nusers unfamiliar with SPARQL, providing a flexible solution for querying\nheterogeneous data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u548cGPT-4\u7684SPARQL\u67e5\u8be2\u91cd\u5199\u65b9\u6cd5\uff0c\u89e3\u51b3\u590d\u6742\u5bf9\u9f50\uff08c:c\uff09\u548c\u7528\u6237\u53cb\u597d\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5904\u7406\u7b80\u5355\u5bf9\u9f50\uff08s:s\uff09\u548c\u90e8\u5206\u590d\u6742\u5bf9\u9f50\uff08s:c\uff09\uff0c\u96be\u4ee5\u5e94\u5bf9\u66f4\u590d\u6742\u7684\uff08c:c\uff09\u5bf9\u9f50\uff0c\u4e14SPARQL\u8bed\u6cd5\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u4e0d\u53cb\u597d\u3002", "method": "\u5229\u7528\u7b49\u4ef7\u4f20\u9012\u6027\u548cGPT-4\u7b49\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u7528\u6237\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u81ea\u52a8\u91cd\u5199\u4e3a\u8de8\u672c\u4f53\u7684SPARQL\u67e5\u8be2\u3002", "result": "\u80fd\u9ad8\u6548\u5904\u7406\u590d\u6742\u5bf9\u9f50\uff08c:c\uff09\uff0c\u5e76\u964d\u4f4e\u975e\u4e13\u5bb6\u7528\u6237\u4f7f\u7528SPARQL\u7684\u95e8\u69db\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u67e5\u8be2\u5f02\u6784\u6570\u636e\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01315", "pdf": "https://arxiv.org/pdf/2505.01315", "abs": "https://arxiv.org/abs/2505.01315", "authors": ["Sheikh Samit Muhaimin", "Spyridon Mastorakis"], "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3LLM\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u8fc7\u6ee4\u548c\u603b\u7ed3\u6a21\u5757\u6709\u6548\u8bc6\u522b\u548c\u62b5\u5fa1\u6076\u610f\u8f93\u5165\uff0c\u5b9e\u9a8c\u6210\u529f\u7387\u9ad8\u8fbe98.71%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\u548c\u6076\u610f\u8f93\u5165\u5f71\u54cd\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u5b9e\u7528\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e24\u90e8\u5206\uff1a1) \u63d0\u793a\u8fc7\u6ee4\u6a21\u5757\uff0c\u4f7f\u7528NLP\u6280\u672f\u68c0\u6d4b\u6076\u610f\u8f93\u5165\uff1b2) \u603b\u7ed3\u6a21\u5757\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u9632\u5fa1\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u8bc6\u522b\u6709\u5bb3\u6a21\u5f0f\u7684\u6210\u529f\u7387\u4e3a98.71%\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5bf9\u6076\u610f\u8f93\u5165\u7684\u62b5\u6297\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2505.01328", "pdf": "https://arxiv.org/pdf/2505.01328", "abs": "https://arxiv.org/abs/2505.01328", "authors": ["Anass Grini", "Oumaima Taheri", "Btissam El Khamlichi", "Amal El Fallah-Seghrouchni"], "title": "Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": null, "summary": "While machine learning has significantly advanced Network Intrusion Detection\nSystems (NIDS), particularly within IoT environments where devices generate\nlarge volumes of data and are increasingly susceptible to cyber threats, these\nmodels remain vulnerable to adversarial attacks. Our research reveals a\ncritical flaw in existing adversarial attack methodologies: the frequent\nviolation of domain-specific constraints, such as numerical and categorical\nlimits, inherent to IoT and network traffic. This leads to up to 80.3% of\nadversarial examples being invalid, significantly overstating real-world\nvulnerabilities. These invalid examples, though effective in fooling models, do\nnot represent feasible attacks within practical IoT deployments. Consequently,\nrelying on these results can mislead resource allocation for defense, inflating\nthe perceived susceptibility of IoT-enabled NIDS models to adversarial\nmanipulation. Furthermore, we demonstrate that simpler surrogate models like\nMulti-Layer Perceptron (MLP) generate more valid adversarial examples compared\nto complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,\nwe analyze the transferability of adversarial severity to other ML/DL models\ncommonly used in IoT contexts. This work underscores the importance of\nconsidering both domain constraints and model architecture when evaluating and\ndesigning robust ML/DL models for security-critical IoT and network\napplications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728IoT\u73af\u5883\u4e2d\u5e38\u8fdd\u53cd\u9886\u57df\u7ea6\u675f\uff0c\u5bfc\u81f480.3%\u7684\u5bf9\u6297\u6837\u672c\u65e0\u6548\uff0c\u8bef\u5bfc\u4e86\u771f\u5b9e\u6f0f\u6d1e\u8bc4\u4f30\u3002\u7b80\u5355\u6a21\u578b\uff08\u5982MLP\uff09\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u66f4\u6709\u6548\uff0c\u4e14\u9700\u8003\u8651\u9886\u57df\u7ea6\u675f\u548c\u6a21\u578b\u67b6\u6784\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728IoT\u73af\u5883\u4e2d\u5e38\u8fdd\u53cd\u9886\u57df\u7ea6\u675f\uff08\u5982\u6570\u503c\u548c\u5206\u7c7b\u9650\u5236\uff09\uff0c\u5bfc\u81f4\u5927\u91cf\u65e0\u6548\u5bf9\u6297\u6837\u672c\uff0c\u8bef\u5bfc\u4e86\u771f\u5b9e\u6f0f\u6d1e\u8bc4\u4f30\u548c\u9632\u5fa1\u8d44\u6e90\u5206\u914d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5bf9\u6297\u6837\u672c\u7684\u9886\u57df\u7ea6\u675f\u6709\u6548\u6027\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\uff08\u5982MLP\u3001CNN\u3001LSTM\uff09\u751f\u6210\u5bf9\u6297\u6837\u672c\u7684\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u5bf9\u6297\u4e25\u91cd\u6027\u7684\u53ef\u8f6c\u79fb\u6027\u3002", "result": "80.3%\u7684\u5bf9\u6297\u6837\u672c\u56e0\u8fdd\u53cd\u9886\u57df\u7ea6\u675f\u800c\u65e0\u6548\uff1bMLP\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u6bd4\u590d\u6742\u6a21\u578b\u66f4\u6709\u6548\u3002", "conclusion": "\u5728\u8bc4\u4f30\u548c\u8bbe\u8ba1IoT\u53ca\u7f51\u7edc\u5b89\u5168ML/DL\u6a21\u578b\u65f6\uff0c\u9700\u540c\u65f6\u8003\u8651\u9886\u57df\u7ea6\u675f\u548c\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.01353", "pdf": "https://arxiv.org/pdf/2505.01353", "abs": "https://arxiv.org/abs/2505.01353", "authors": ["Jonathan Frey", "Katrin Baumg\u00e4rtner", "Gianluca Frison", "Dirk Reinhardt", "Jasper Hoffmann", "Leonard Fichtner", "Sebastien Gros", "Moritz Diehl"], "title": "Differentiable Nonlinear Model Predictive Control", "categories": ["math.OC", "cs.AI", "cs.LG"], "comment": "19 page, 4 figures, 2 tables", "summary": "The efficient computation of parametric solution sensitivities is a key\nchallenge in the integration of learning-enhanced methods with nonlinear model\npredictive control (MPC), as their availability is crucial for many learning\nalgorithms. While approaches presented in the machine learning community are\nlimited to convex or unconstrained formulations, this paper discusses the\ncomputation of solution sensitivities of general nonlinear programs (NLPs)\nusing the implicit function theorem (IFT) and smoothed optimality conditions\ntreated in interior-point methods (IPM). We detail sensitivity computation\nwithin a sequential quadratic programming (SQP) method which employs an IPM for\nthe quadratic subproblems. The publication is accompanied by an efficient\nopen-source implementation within the framework, providing both forward and\nadjoint sensitivities for general optimal control problems, achieving speedups\nexceeding 3x over the state-of-the-art solver mpc.pytorch.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5728\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u4e2d\u8ba1\u7b97\u53c2\u6570\u89e3\u7075\u654f\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u9690\u51fd\u6570\u5b9a\u7406\u548c\u5149\u6ed1\u6700\u4f18\u6027\u6761\u4ef6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "motivation": "\u5b66\u4e60\u589e\u5f3a\u65b9\u6cd5\u4e0e\u975e\u7ebf\u6027MPC\u96c6\u6210\u65f6\uff0c\u53c2\u6570\u89e3\u7075\u654f\u5ea6\u7684\u8ba1\u7b97\u662f\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u51f8\u6216\u65e0\u7ea6\u675f\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9690\u51fd\u6570\u5b9a\u7406\uff08IFT\uff09\u548c\u5149\u6ed1\u6700\u4f18\u6027\u6761\u4ef6\uff08IPM\uff09\uff0c\u7ed3\u5408\u5e8f\u5217\u4e8c\u6b21\u89c4\u5212\uff08SQP\uff09\u65b9\u6cd5\uff0c\u8ba1\u7b97\u975e\u7ebf\u6027\u89c4\u5212\uff08NLP\uff09\u7684\u7075\u654f\u5ea6\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53c2\u6570\u89e3\u7075\u654f\u5ea6\u8ba1\u7b97\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u6700\u4f73\u6c42\u89e3\u5668mpc.pytorch\u5feb3\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u7ebf\u6027MPC\u4e2d\u7684\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7075\u654f\u5ea6\u8ba1\u7b97\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.01372", "pdf": "https://arxiv.org/pdf/2505.01372", "abs": "https://arxiv.org/abs/2505.01372", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "title": "Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": "13 pages (plus appendices), 5 figures", "summary": "Mechanistic Interpretability (MI) aims to understand neural networks through\ncausal explanations. Though MI has many explanation-generating methods,\nprogress has been limited by the lack of a universal approach to evaluating\nexplanations. Here we analyse the fundamental question \"What makes a good\nexplanation?\" We introduce a pluralist Explanatory Virtues Framework drawing on\nfour perspectives from the Philosophy of Science - the Bayesian, Kuhnian,\nDeutschian, and Nomological - to systematically evaluate and improve\nexplanations in MI. We find that Compact Proofs consider many explanatory\nvirtues and are hence a promising approach. Fruitful research directions\nimplied by our framework include (1) clearly defining explanatory simplicity,\n(2) focusing on unifying explanations and (3) deriving universal principles for\nneural networks. Improved MI methods enhance our ability to monitor, predict,\nand steer AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u54f2\u5b66\u79d1\u5b66\u7684\u591a\u5143\u89e3\u91ca\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u7d27\u51d1\u8bc1\u660e\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u901a\u7528\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u8fdb\u5c55\u53d7\u9650\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8d1d\u53f6\u65af\u3001\u5e93\u6069\u3001\u5fb7\u610f\u5fd7\u548c\u6cd5\u5219\u8bba\u7684\u591a\u5143\u89e3\u91ca\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdb\u89e3\u91ca\u65b9\u6cd5\u3002", "result": "\u7d27\u51d1\u8bc1\u660e\u65b9\u6cd5\u56e0\u5176\u7efc\u5408\u591a\u79cd\u89e3\u91ca\u4f18\u70b9\u800c\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u6539\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u76d1\u63a7\u3001\u9884\u6d4b\u548c\u5f15\u5bfcAI\u7cfb\u7edf\u3002"}}
{"id": "2505.01383", "pdf": "https://arxiv.org/pdf/2505.01383", "abs": "https://arxiv.org/abs/2505.01383", "authors": ["Yan Miao", "Will Shen", "Hang Cui", "Sayan Mitra"], "title": "FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing\nplatform for autonomy research. The hardware platform integrates a small\ncamera, a standard airframe, offboard computation, and radio communication for\nmanual overrides. We demonstrate FalconWing's capabilities by developing and\ndeploying a purely vision-based control policy for autonomous landing (without\nIMU or motion capture) using a novel real-to-sim-to-real learning approach. Our\nlearning approach: (1) constructs a photorealistic simulation environment via\n3D Gaussian splatting trained on real-world images; (2) identifies nonlinear\ndynamics from vision-estimated real-flight data; and (3) trains a multi-modal\nVision Transformer (ViT) policy through simulation-only imitation learning. The\nViT architecture fuses single RGB image with the history of control actions via\nself-attention, preserving temporal context while maintaining real-time 20 Hz\ninference. When deployed zero-shot on the hardware platform, this policy\nachieves an 80% success rate in vision-based autonomous landings. Together with\nthe hardware specifications, we also open-source the system dynamics, the\nsoftware for photorealistic simulator and the learning approach.", "AI": {"tldr": "FalconWing\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u8d85\u8f7b\u91cf\uff08150\u514b\uff09\u7684\u56fa\u5b9a\u7ffc\u5e73\u53f0\uff0c\u7528\u4e8e\u81ea\u4e3b\u6027\u7814\u7a76\uff0c\u652f\u6301\u7eaf\u89c6\u89c9\u63a7\u5236\u7684\u81ea\u4e3b\u7740\u9646\u3002", "motivation": "\u4e3a\u81ea\u4e3b\u6027\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u8f7b\u91cf\u3001\u5f00\u6e90\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u786c\u4ef6\u5e73\u53f0\uff0c\u5e76\u63a2\u7d22\u7eaf\u89c6\u89c9\u63a7\u5236\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6e85\u5c04\u6784\u5efa\u903c\u771f\u6a21\u62df\u73af\u5883\uff0c\u4ece\u89c6\u89c9\u4f30\u8ba1\u6570\u636e\u4e2d\u8bc6\u522b\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u591a\u6a21\u6001Vision Transformer\u7b56\u7565\u3002", "result": "\u5728\u786c\u4ef6\u5e73\u53f0\u4e0a\u96f6\u90e8\u7f72\u65f6\uff0c\u8be5\u7b56\u7565\u5b9e\u73b0\u4e8680%\u7684\u89c6\u89c9\u81ea\u4e3b\u7740\u9646\u6210\u529f\u7387\u3002", "conclusion": "FalconWing\u53ca\u5176\u5f00\u6e90\u7ec4\u4ef6\u4e3a\u81ea\u4e3b\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u9a8c\u8bc1\u4e86\u7eaf\u89c6\u89c9\u63a7\u5236\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.01396", "pdf": "https://arxiv.org/pdf/2505.01396", "abs": "https://arxiv.org/abs/2505.01396", "authors": ["Yang Jin", "Jun Lv", "Wenye Yu", "Hongjie Fang", "Yong-Lu Li", "Cewu Lu"], "title": "SIME: Enhancing Policy Self-Improvement with Modal-level Exploration", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Self-improvement requires robotic systems to initially learn from\nhuman-provided data and then gradually enhance their capabilities through\ninteraction with the environment. This is similar to how humans improve their\nskills through continuous practice. However, achieving effective\nself-improvement is challenging, primarily because robots tend to repeat their\nexisting abilities during interactions, often failing to generate new, valuable\ndata for learning. In this paper, we identify the key to successful\nself-improvement: modal-level exploration and data selection. By incorporating\na modal-level exploration mechanism during policy execution, the robot can\nproduce more diverse and multi-modal interactions. At the same time, we select\nthe most valuable trials and high-quality segments from these interactions for\nlearning. We successfully demonstrate effective robot self-improvement on both\nsimulation benchmarks and real-world experiments. The capability for\nself-improvement will enable us to develop more robust and high-success-rate\nrobotic control strategies at a lower cost. Our code and experiment scripts are\navailable at https://ericjin2002.github.io/SIME/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u81ea\u6211\u6539\u8fdb\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u6001\u7ea7\u63a2\u7d22\u548c\u6570\u636e\u9009\u62e9\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u7684\u4ea4\u4e92\u5e76\u9009\u62e9\u9ad8\u8d28\u91cf\u6570\u636e\u5b66\u4e60\u3002", "motivation": "\u673a\u5668\u4eba\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u81ea\u6211\u6539\u8fdb\u65f6\uff0c\u5bb9\u6613\u91cd\u590d\u73b0\u6709\u80fd\u529b\u800c\u65e0\u6cd5\u751f\u6210\u65b0\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5f15\u5165\u6a21\u6001\u7ea7\u63a2\u7d22\u673a\u5236\uff0c\u751f\u6210\u591a\u6837\u5316\u4ea4\u4e92\uff0c\u5e76\u9009\u62e9\u6700\u6709\u4ef7\u503c\u7684\u8bd5\u9a8c\u548c\u9ad8\u8d28\u91cf\u7247\u6bb5\u7528\u4e8e\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u57fa\u51c6\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u6210\u529f\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u673a\u5668\u4eba\u81ea\u6211\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u4ee5\u66f4\u4f4e\u6210\u672c\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u9ad8\u6210\u529f\u7387\u7684\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u3002"}}
