{"id": "2505.05487", "pdf": "https://arxiv.org/pdf/2505.05487", "abs": "https://arxiv.org/abs/2505.05487", "authors": ["Shrinivas Pundlik", "Seonggyu Choe", "Patrick Baker", "Chen-Yuan Lee", "Naser Al-Madi", "Alex R. Bowers", "Gang Luo"], "title": "Data extraction and processing methods to aid the study of driving behaviors at intersections in naturalistic driving", "categories": ["cs.CV", "cs.RO"], "comment": "19 pages, 11 figures", "summary": "Naturalistic driving studies use devices in participants' own vehicles to\nrecord daily driving over many months. Due to diverse and extensive amounts of\ndata recorded, automated processing is necessary. This report describes methods\nto extract and characterize driver head scans at intersections from data\ncollected from an in-car recording system that logged vehicle speed, GPS\nlocation, scene videos, and cabin videos. Custom tools were developed to mark\nthe intersections, synchronize location and video data, and clip the cabin and\nscene videos for +/-100 meters from the intersection location. A\ncustom-developed head pose detection AI model for wide angle head turns was run\non the cabin videos to estimate the driver head pose, from which head scans >20\ndeg were computed in the horizontal direction. The scene videos were processed\nusing a YOLO object detection model to detect traffic lights, stop signs,\npedestrians, and other vehicles on the road. Turning maneuvers were\nindependently detected using vehicle self-motion patterns. Stop lines on the\nroad surface were detected using changing intensity patterns over time as the\nvehicle moved. The information obtained from processing the scene videos, along\nwith the speed data was used in a rule-based algorithm to infer the\nintersection type, maneuver, and bounds. We processed 190 intersections from 3\nvehicles driven in cities and suburban areas from Massachusetts and California.\nThe automated video processing algorithm correctly detected intersection\nsignage and maneuvers in 100% and 94% of instances, respectively. The median\n[IQR] error in detecting vehicle entry into the intersection was 1.1[0.4-4.9]\nmeters and 0.2[0.1-0.54] seconds. The median overlap between ground truth and\nestimated intersection bounds was 0.88[0.82-0.93].", "AI": {"tldr": "\u8bba\u6587\u63cf\u8ff0\u4e86\u4ece\u81ea\u7136\u9a7e\u9a76\u7814\u7a76\u4e2d\u63d0\u53d6\u548c\u8868\u5f81\u9a7e\u9a76\u5458\u5728\u4ea4\u53c9\u8def\u53e3\u5934\u90e8\u626b\u63cf\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u6570\u636e\u5904\u7406\u6280\u672f\uff0c\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\u7684\u7ed3\u679c\u3002", "motivation": "\u81ea\u7136\u9a7e\u9a76\u7814\u7a76\u4ea7\u751f\u5927\u91cf\u591a\u6837\u5316\u6570\u636e\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5904\u7406\u4ee5\u63d0\u53d6\u9a7e\u9a76\u5458\u884c\u4e3a\u7279\u5f81\uff0c\u7279\u522b\u662f\u4ea4\u53c9\u8def\u53e3\u7684\u5934\u90e8\u626b\u63cf\u884c\u4e3a\u3002", "method": "\u5f00\u53d1\u5b9a\u5236\u5de5\u5177\u6807\u8bb0\u4ea4\u53c9\u8def\u53e3\uff0c\u540c\u6b65\u4f4d\u7f6e\u548c\u89c6\u9891\u6570\u636e\uff0c\u4f7f\u7528AI\u6a21\u578b\u68c0\u6d4b\u5934\u90e8\u59ff\u6001\uff0cYOLO\u6a21\u578b\u5904\u7406\u573a\u666f\u89c6\u9891\uff0c\u7ed3\u5408\u89c4\u5219\u7b97\u6cd5\u63a8\u65ad\u4ea4\u53c9\u8def\u53e3\u7c7b\u578b\u548c\u52a8\u4f5c\u3002", "result": "\u5904\u7406190\u4e2a\u4ea4\u53c9\u8def\u53e3\uff0c\u81ea\u52a8\u5316\u7b97\u6cd5\u5728\u68c0\u6d4b\u6807\u5fd7\u548c\u52a8\u4f5c\u4e0a\u5206\u522b\u8fbe\u5230100%\u548c94%\u7684\u51c6\u786e\u7387\uff0c\u65f6\u95f4\u548c\u7a7a\u95f4\u8bef\u5dee\u8f83\u5c0f\u3002", "conclusion": "\u81ea\u52a8\u5316\u65b9\u6cd5\u80fd\u9ad8\u6548\u51c6\u786e\u5730\u5904\u7406\u81ea\u7136\u9a7e\u9a76\u6570\u636e\uff0c\u4e3a\u9a7e\u9a76\u5458\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2505.05488", "pdf": "https://arxiv.org/pdf/2505.05488", "abs": "https://arxiv.org/abs/2505.05488", "authors": ["Yunfan Lu", "Xiaogang Xu", "Pengteng Li", "Yusheng Wang", "Yi Cui", "Huizai Yao", "Hui Xiong"], "title": "From Events to Enhancement: A Survey on Event-Based Imaging Technologies", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras offering high dynamic range and low latency have emerged as\ndisruptive technologies in imaging. Despite growing research on leveraging\nthese benefits for different imaging tasks, a comprehensive study of recently\nadvances and challenges are still lacking. This limits the broader\nunderstanding of how to utilize events in universal imaging applications. In\nthis survey, we first introduce a physical model and the characteristics of\ndifferent event sensors as the foundation. Following this, we highlight the\nadvancement and interaction of image/video enhancement tasks with events.\nAdditionally, we explore advanced tasks, which capture richer light information\nwith events, \\eg~light field estimation, multi-view generation, and\nphotometric. Finally, we discuss new challenges and open questions offering a\nperspective for this rapidly evolving field. More continuously updated\nresources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u6210\u50cf\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u4e0e\u6311\u6218\uff0c\u5305\u62ec\u5176\u7269\u7406\u6a21\u578b\u3001\u7279\u6027\u3001\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u4efb\u52a1\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u9ad8\u7ea7\u4efb\u52a1\u5982\u5149\u573a\u4f30\u8ba1\u548c\u591a\u89c6\u56fe\u751f\u6210\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u6210\u4e3a\u6210\u50cf\u9886\u57df\u7684\u98a0\u8986\u6027\u6280\u672f\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u6700\u65b0\u8fdb\u5c55\u548c\u6311\u6218\u7684\u5168\u9762\u7814\u7a76\uff0c\u9650\u5236\u4e86\u5176\u5728\u901a\u7528\u6210\u50cf\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u9996\u5148\u4ecb\u7ecd\u4e8b\u4ef6\u4f20\u611f\u5668\u7684\u7269\u7406\u6a21\u578b\u548c\u7279\u6027\uff0c\u968f\u540e\u5206\u6790\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u4efb\u52a1\u4e0e\u4e8b\u4ef6\u7684\u4ea4\u4e92\uff0c\u5e76\u63a2\u8ba8\u9ad8\u7ea7\u4efb\u52a1\u5982\u5149\u573a\u4f30\u8ba1\u548c\u591a\u89c6\u56fe\u751f\u6210\u3002", "result": "\u603b\u7ed3\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u6210\u50cf\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u548c\u5f00\u653e\u6027\u95ee\u9898\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u5728\u6210\u50cf\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u73b0\u6709\u6311\u6218\uff0c\u63a8\u52a8\u5176\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2505.05491", "pdf": "https://arxiv.org/pdf/2505.05491", "abs": "https://arxiv.org/abs/2505.05491", "authors": ["TianYi Yu"], "title": "MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Detection of small objects, especially traffic signs, is a critical\nsub-task in object detection and autonomous driving. Despite signficant\nprogress in previous research, two main challenges remain. First, the issue of\nfeature extraction being too singular. Second, the detection process struggles\nto efectively handle objects of varying sizes or scales. These problems are\nalso prevalent in general object detection tasks. To address these challenges,\nwe propose a novel object detection network, Mamba-based Dynamic Dual Fusion\nNetwork (MDDFNet), for traffic sign detection. The network integrates a dynamic\ndual fusion module and a Mamba-based backbone to simultaneously tackle the\naforementioned issues. Specifically, the dynamic dual fusion module utilizes\nmultiple branches to consolidate various spatial and semantic information, thus\nenhancing feature diversity. The Mamba-based backbone leverages global feature\nfusion and local feature interaction, combining features in an adaptive manner\nto generate unique classification characteristics. Extensive experiments\nconducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that\nMDDFNet outperforms other state-of-the-art detectors, maintaining real-time\nprocessing capabilities of single-stage models while achieving superior\nperformance. This confirms the efectiveness of MDDFNet in detecting small\ntraffic signs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMDDFNet\u7684\u65b0\u578b\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u5c0f\u76ee\u6807\uff08\u5982\u4ea4\u901a\u6807\u5fd7\uff09\u68c0\u6d4b\u4e2d\u7684\u7279\u5f81\u5355\u4e00\u6027\u548c\u591a\u5c3a\u5ea6\u95ee\u9898\u3002\u901a\u8fc7\u52a8\u6001\u53cc\u878d\u5408\u6a21\u5757\u548cMamba\u4e3b\u5e72\u7f51\u7edc\uff0cMDDFNet\u5728TT100K\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c0f\u76ee\u6807\u68c0\u6d4b\uff08\u5c24\u5176\u662f\u4ea4\u901a\u6807\u5fd7\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u5355\u4e00\u6027\u548c\u591a\u5c3a\u5ea6\u5904\u7406\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMDDFNet\uff0c\u7ed3\u5408\u52a8\u6001\u53cc\u878d\u5408\u6a21\u5757\uff08\u589e\u5f3a\u7279\u5f81\u591a\u6837\u6027\uff09\u548cMamba\u4e3b\u5e72\u7f51\u7edc\uff08\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\uff09\u3002", "result": "\u5728TT100K\u6570\u636e\u96c6\u4e0a\uff0cMDDFNet\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u68c0\u6d4b\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "MDDFNet\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2505.05492", "pdf": "https://arxiv.org/pdf/2505.05492", "abs": "https://arxiv.org/abs/2505.05492", "authors": ["Ignacy St\u0119pka", "Lukasz Sztukiewicz", "Micha\u0142 Wili\u0144ski", "Jerzy Stefanowski"], "title": "DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While machine learning fairness has made significant progress in recent\nyears, most existing solutions focus on tabular data and are poorly suited for\nvision-based classification tasks, which rely heavily on deep learning. To\nbridge this gap, we introduce DetoxAI, an open-source Python library for\nimproving fairness in deep learning vision classifiers through post-hoc\ndebiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness\nmetrics, and visualization tools. It supports debiasing via interventions in\ninternal representations and includes attribution-based visualization tools and\nquantitative algorithmic fairness metrics to show how bias is mitigated. This\npaper presents the motivation, design, and use cases of DetoxAI, demonstrating\nits tangible value to engineers and researchers.", "AI": {"tldr": "DetoxAI\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5e93\uff0c\u65e8\u5728\u901a\u8fc7\u540e\u5904\u7406\u53bb\u504f\u6280\u672f\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u89c6\u89c9\u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u6027\u89e3\u51b3\u65b9\u6848\u591a\u9488\u5bf9\u8868\u683c\u6570\u636e\uff0c\u4e0d\u9002\u7528\u4e8e\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\uff0cDetoxAI\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DetoxAI\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u53bb\u504f\u7b97\u6cd5\u3001\u516c\u5e73\u6027\u6307\u6807\u548c\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u652f\u6301\u901a\u8fc7\u5e72\u9884\u5185\u90e8\u8868\u5f81\u8fdb\u884c\u53bb\u504f\u3002", "result": "DetoxAI\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5f52\u56e0\u7684\u53ef\u89c6\u5316\u5de5\u5177\u548c\u5b9a\u91cf\u516c\u5e73\u6027\u6307\u6807\uff0c\u5c55\u793a\u4e86\u504f\u89c1\u7684\u7f13\u89e3\u6548\u679c\u3002", "conclusion": "DetoxAI\u4e3a\u5de5\u7a0b\u5e08\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2505.05495", "pdf": "https://arxiv.org/pdf/2505.05495", "abs": "https://arxiv.org/abs/2505.05495", "authors": ["Siyuan Zhou", "Yilun Du", "Yuncong Yang", "Lei Han", "Peihao Chen", "Dit-Yan Yeung", "Chuang Gan"], "title": "Learning 3D Persistent Embodied World Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The ability to simulate the effects of future actions on the world is a\ncrucial ability of intelligent embodied agents, enabling agents to anticipate\nthe effects of their actions and make plans accordingly. While a large body of\nexisting work has explored how to construct such world models using video\nmodels, they are often myopic in nature, without any memory of a scene not\ncaptured by currently observed images, preventing agents from making consistent\nlong-horizon plans in complex environments where many parts of the scene are\npartially observed. We introduce a new persistent embodied world model with an\nexplicit memory of previously generated content, enabling much more consistent\nlong-horizon simulation. During generation time, our video diffusion model\npredicts RGB-D video of the future observations of the agent. This generation\nis then aggregated into a persistent 3D map of the environment. By conditioning\nthe video model on this 3D spatial map, we illustrate how this enables video\nworld models to faithfully simulate both seen and unseen parts of the world.\nFinally, we illustrate the efficacy of such a world model in downstream\nembodied applications, enabling effective planning and policy learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u663e\u5f0f\u8bb0\u5fc6\u7684\u6301\u4e45\u6027\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u957f\u671f\u4e00\u81f4\u7684\u6a21\u62df\u548c\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6a21\u578b\u7f3a\u4e4f\u5bf9\u672a\u89c2\u6d4b\u573a\u666f\u7684\u8bb0\u5fc6\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u5728\u90e8\u5206\u89c2\u6d4b\u7684\u590d\u6742\u73af\u5883\u4e2d\u65e0\u6cd5\u8fdb\u884c\u957f\u671f\u4e00\u81f4\u7684\u89c4\u5212\u3002", "method": "\u91c7\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u9884\u6d4b\u672a\u6765RGB-D\u89c6\u9891\uff0c\u5e76\u5c06\u5176\u805a\u5408\u4e3a\u6301\u4e45\u60273D\u73af\u5883\u5730\u56fe\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u89c6\u9891\u6a21\u578b\u5b9e\u73b0\u4e16\u754c\u6a21\u62df\u3002", "result": "\u6a21\u578b\u80fd\u591f\u51c6\u786e\u6a21\u62df\u5df2\u89c2\u6d4b\u548c\u672a\u89c2\u6d4b\u7684\u4e16\u754c\u90e8\u5206\uff0c\u652f\u6301\u6709\u6548\u7684\u89c4\u5212\u548c\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "\u63d0\u51fa\u7684\u6301\u4e45\u6027\u4e16\u754c\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2505.05501", "pdf": "https://arxiv.org/pdf/2505.05501", "abs": "https://arxiv.org/abs/2505.05501", "authors": ["Pu Cao", "Feng Zhou", "Junyi Ji", "Qingye Kong", "Zhixiang Lv", "Mingjian Zhang", "Xuekun Zhao", "Siqi Wu", "Yinghui Lin", "Qing Song", "Lu Yang"], "title": "Preliminary Explorations with GPT-4o(mni) Native Image Generation", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Recently, the visual generation ability by GPT-4o(mni) has been unlocked by\nOpenAI. It demonstrates a very remarkable generation capability with excellent\nmultimodal condition understanding and varied task instructions. In this paper,\nwe aim to explore the capabilities of GPT-4o across various tasks. Inspired by\nprevious study, we constructed a task taxonomy along with a carefully curated\nset of test samples to conduct a comprehensive qualitative test. Benefiting\nfrom GPT-4o's powerful multimodal comprehension, its image-generation process\ndemonstrates abilities surpassing those of traditional image-generation tasks.\nThus, regarding the dimensions of model capabilities, we evaluate its\nperformance across six task categories: traditional image generation tasks,\ndiscriminative tasks, knowledge-based generation, commonsense-based generation,\nspatially-aware image generation, and temporally-aware image generation. These\ntasks not only assess the quality and conditional alignment of the model's\noutputs but also probe deeper into GPT-4o's understanding of real-world\nconcepts. Our results reveal that GPT-4o performs impressively well in\ngeneral-purpose synthesis tasks, showing strong capabilities in text-to-image\ngeneration, visual stylization, and low-level image processing. However,\nsignificant limitations remain in its ability to perform precise spatial\nreasoning, instruction-grounded generation, and consistent temporal prediction.\nFurthermore, when faced with knowledge-intensive or domain-specific scenarios,\nsuch as scientific illustrations or mathematical plots, the model often\nexhibits hallucinations, factual errors, or structural inconsistencies. These\nfindings suggest that while GPT-4o marks a substantial advancement in unified\nmultimodal generation, there is still a long way to go before it can be\nreliably applied to professional or safety-critical domains.", "AI": {"tldr": "GPT-4o\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u548c\u9886\u57df\u7279\u5b9a\u573a\u666f\u4e2d\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u63a2\u7d22GPT-4o\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u8bc4\u4f30\u5176\u5728\u56fe\u50cf\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4efb\u52a1\u5206\u7c7b\u548c\u6d4b\u8bd5\u6837\u672c\u96c6\uff0c\u5bf9GPT-4o\u5728\u516d\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9a\u6027\u6d4b\u8bd5\u3002", "result": "GPT-4o\u5728\u901a\u7528\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u548c\u9886\u57df\u7279\u5b9a\u573a\u666f\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "GPT-4o\u5728\u591a\u6a21\u6001\u751f\u6210\u65b9\u9762\u6709\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5c1a\u672a\u8fbe\u5230\u4e13\u4e1a\u6216\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u53ef\u9760\u5e94\u7528\u6c34\u5e73\u3002"}}
{"id": "2505.05505", "pdf": "https://arxiv.org/pdf/2505.05505", "abs": "https://arxiv.org/abs/2505.05505", "authors": ["Yiming Qin", "Zhu Xu", "Yang Liu"], "title": "Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation", "categories": ["cs.CV", "eess.IV"], "comment": "Project page here:\n  https://hierarchical-chain-of-generation.github.io/", "summary": "Recent text-to-3D models can render high-quality assets, yet they still\nstumble on objects with complex attributes. The key obstacles are: (1) existing\ntext-to-3D approaches typically lift text-to-image models to extract semantics\nvia text encoders, while the text encoder exhibits limited comprehension\nability for long descriptions, leading to deviated cross-attention focus,\nsubsequently wrong attribute binding in generated results. (2) Occluded object\nparts demand a disciplined generation order and explicit part disentanglement.\nThough some works introduce manual efforts to alleviate the above issues, their\nquality is unstable and highly reliant on manual information. To tackle above\nproblems, we propose a automated method Hierarchical-Chain-of-Generation\n(HCoG). It leverages a large language model to decompose the long description\ninto blocks representing different object parts, and orders them from inside\nout according to occlusions, forming a hierarchical chain. Within each block we\nfirst coarsely create components, then precisely bind attributes via\ntarget-region localization and corresponding 3D Gaussian kernel optimization.\nBetween blocks, we introduce Gaussian Extension and Label Elimination to\nseamlessly generate new parts by extending new Gaussian kernels, re-assigning\nsemantic labels, and eliminating unnecessary kernels, ensuring that only\nrelevant parts are added without disrupting previously optimized parts.\nExperiments confirm that HCoG yields structurally coherent, attribute-faithful\n3D objects with complex attributes. The code is available at\nhttps://github.com/Wakals/GASCOL .", "AI": {"tldr": "HCoG\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u751f\u6210\u94fe\u89e3\u51b3\u590d\u6742\u5c5e\u60273D\u5bf9\u8c61\u751f\u6210\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u52303D\u6a21\u578b\u5728\u590d\u6742\u5c5e\u6027\u5bf9\u8c61\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u6587\u672c\u7f16\u7801\u5668\u5bf9\u957f\u63cf\u8ff0\u7406\u89e3\u6709\u9650\uff0c\u4ee5\u53ca\u906e\u6321\u90e8\u5206\u9700\u660e\u786e\u751f\u6210\u987a\u5e8f\u548c\u89e3\u8026\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u89e3\u957f\u63cf\u8ff0\u4e3a\u5bf9\u8c61\u90e8\u5206\u5757\uff0c\u6309\u906e\u6321\u987a\u5e8f\u4ece\u5185\u5230\u5916\u751f\u6210\uff0c\u901a\u8fc7\u76ee\u6807\u533a\u57df\u5b9a\u4f4d\u548c3D\u9ad8\u65af\u6838\u4f18\u5316\u7cbe\u786e\u7ed1\u5b9a\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHCoG\u80fd\u751f\u6210\u7ed3\u6784\u8fde\u8d2f\u3001\u5c5e\u6027\u51c6\u786e\u7684\u590d\u67423D\u5bf9\u8c61\u3002", "conclusion": "HCoG\u901a\u8fc7\u5206\u5c42\u751f\u6210\u94fe\u548c\u4f18\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5c5e\u60273D\u5bf9\u8c61\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2505.05512", "pdf": "https://arxiv.org/pdf/2505.05512", "abs": "https://arxiv.org/abs/2505.05512", "authors": ["Zhang Zhang", "Qiang Zhang", "Wei Cui", "Shuai Shi", "Yijie Guo", "Gang Han", "Wen Zhao", "Jingkai Sun", "Jiahang Cao", "Jiaxu Wang", "Hao Cheng", "Xiaozhu Ju", "Zhengping Che", "Renjing Xu", "Jian Tang"], "title": "Occupancy World Model for Robots", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Understanding and forecasting the scene evolutions deeply affect the\nexploration and decision of embodied agents. While traditional methods simulate\nscene evolutions through trajectory prediction of potential instances, current\nworks use the occupancy world model as a generative framework for describing\nfine-grained overall scene dynamics. However, existing methods cluster on the\noutdoor structured road scenes, while ignoring the exploration of forecasting\n3D occupancy scene evolutions for robots in indoor scenes. In this work, we\nexplore a new framework for learning the scene evolutions of observed\nfine-grained occupancy and propose an occupancy world model based on the\ncombined spatio-temporal receptive field and guided autoregressive transformer\nto forecast the scene evolutions, called RoboOccWorld. We propose the\nConditional Causal State Attention (CCSA), which utilizes camera poses of next\nstate as conditions to guide the autoregressive transformer to adapt and\nunderstand the indoor robotics scenarios. In order to effectively exploit the\nspatio-temporal cues from historical observations, Hybrid Spatio-Temporal\nAggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive\nfield based on multi-scale spatio-temporal windows. In addition, we restructure\nthe OccWorld-ScanNet benchmark based on local annotations to facilitate the\nevaluation of the indoor 3D occupancy scene evolution prediction task.\nExperimental results demonstrate that our RoboOccWorld outperforms\nstate-of-the-art methods in indoor 3D occupancy scene evolution prediction\ntask. The code will be released soon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoboOccWorld\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5ba4\u51853D\u5360\u7528\u573a\u666f\u7684\u6f14\u5316\uff0c\u7ed3\u5408\u4e86\u65f6\u7a7a\u611f\u53d7\u91ce\u548c\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5ba4\u5916\u7ed3\u6784\u5316\u9053\u8def\u573a\u666f\uff0c\u800c\u5ffd\u7565\u4e86\u5ba4\u5185\u673a\u5668\u4eba\u573a\u666f\u76843D\u5360\u7528\u573a\u666f\u6f14\u5316\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6761\u4ef6\u56e0\u679c\u72b6\u6001\u6ce8\u610f\u529b\uff08CCSA\uff09\u548c\u6df7\u5408\u65f6\u7a7a\u805a\u5408\uff08HSTA\uff09\u7684\u5360\u7528\u4e16\u754c\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u65f6\u7a7a\u7a97\u53e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRoboOccWorld\u5728\u5ba4\u51853D\u5360\u7528\u573a\u666f\u6f14\u5316\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RoboOccWorld\u4e3a\u5ba4\u5185\u573a\u666f\u6f14\u5316\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u4ee3\u7801\u5373\u5c06\u53d1\u5e03\u3002"}}
{"id": "2505.05513", "pdf": "https://arxiv.org/pdf/2505.05513", "abs": "https://arxiv.org/abs/2505.05513", "authors": ["Muhammad Junaid Asif", "Hamza Khan", "Rabia Tehseen", "Syed Tahir Hussain Rizvi", "Mujtaba Asad", "Shazia Saqib", "Rana Fayyaz Ahmad"], "title": "Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach", "categories": ["cs.CV"], "comment": null, "summary": "Rice is an essential staple food worldwide that is important in promoting\ninternational trade, economic growth, and nutrition. Asian countries such as\nChina, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their\nsignificant contribution to the cultivation and utilization of rice. These\nnations are also known for cultivating different rice grains, including short\nand long grains. These sizes are further classified as basmati, jasmine, kainat\nsaila, ipsala, arborio, etc., catering to diverse culinary preferences and\ncultural traditions. For both local and international trade, inspecting and\nmaintaining the quality of rice grains to satisfy customers and preserve a\ncountry's reputation is necessary. Manual quality check and classification is\nquite a laborious and time-consuming process. It is also highly prone to\nmistakes. Therefore, an automatic solution must be proposed for the effective\nand efficient classification of different varieties of rice grains. This\nresearch paper presents an automatic framework based on a convolutional neural\nnetwork (CNN) for classifying different varieties of rice grains. We evaluated\nthe proposed model based on performance metrics such as accuracy, recall,\nprecision, and F1-Score. The CNN model underwent rigorous training and\nvalidation, achieving a remarkable accuracy rate and a perfect area under each\nclass's Receiver Operating Characteristic (ROC) curve. The confusion matrix\nanalysis confirmed the model's effectiveness in distinguishing between the\ndifferent rice varieties, indicating minimal misclassifications. Additionally,\nthe integration of explainability techniques such as LIME (Local Interpretable\nModel-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided\nvaluable insights into the model's decision-making process, revealing how\nspecific features of the rice grains influenced classification outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u81ea\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5206\u7c7b\u4e0d\u540c\u54c1\u79cd\u7684\u7a3b\u7c73\uff0c\u5e76\u901a\u8fc7\u6027\u80fd\u6307\u6807\u548c\u53ef\u89e3\u91ca\u6027\u6280\u672f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7a3b\u7c73\u662f\u5168\u7403\u91cd\u8981\u4e3b\u98df\uff0c\u5176\u8d28\u91cf\u68c0\u67e5\u4e0e\u5206\u7c7b\u4f20\u7edf\u4e0a\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5bf9\u7a3b\u7c73\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u7ed3\u5408LIME\u548cSHAP\u6280\u672f\u89e3\u91ca\u6a21\u578b\u51b3\u7b56\u3002", "result": "CNN\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u9ad8\uff0cROC\u66f2\u7ebf\u4e0b\u9762\u79ef\u5b8c\u7f8e\uff0c\u6df7\u6dc6\u77e9\u9635\u663e\u793a\u8bef\u5206\u7c7b\u6781\u5c11\u3002", "conclusion": "\u8be5\u81ea\u52a8\u6846\u67b6\u80fd\u9ad8\u6548\u533a\u5206\u4e0d\u540c\u7a3b\u7c73\u54c1\u79cd\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u6280\u672f\u589e\u5f3a\u4e86\u6a21\u578b\u900f\u660e\u5ea6\uff0c\u4e3a\u7a3b\u7c73\u8d28\u91cf\u68c0\u67e5\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05517", "pdf": "https://arxiv.org/pdf/2505.05517", "abs": "https://arxiv.org/abs/2505.05517", "authors": ["Hongyi Chen", "Yunchao Yao", "Yufei Ye", "Zhixuan Xu", "Homanga Bharadhwaj", "Jiashun Wang", "Shubham Tulsiani", "Zackory Erickson", "Jeffrey Ichnowski"], "title": "Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Functional grasp is essential for enabling dexterous multi-finger robot hands\nto manipulate objects effectively. However, most prior work either focuses on\npower grasping, which simply involves holding an object still, or relies on\ncostly teleoperated robot demonstrations to teach robots how to grasp each\nobject functionally. Instead, we propose extracting human grasp information\nfrom web images since they depict natural and functional object interactions,\nthereby bypassing the need for curated demonstrations. We reconstruct human\nhand-object interaction (HOI) 3D meshes from RGB images, retarget the human\nhand to multi-finger robot hands, and align the noisy object mesh with its\naccurate 3D shape. We show that these relatively low-quality HOI data from\ninexpensive web sources can effectively train a functional grasping model. To\nfurther expand the grasp dataset for seen and unseen objects, we use the\ninitially-trained grasping policy with web data in the IsaacGym simulator to\ngenerate physically feasible grasps while preserving functionality. We train\nthe grasping model on 10 object categories and evaluate it on 9 unseen objects,\nincluding challenging items such as syringes, pens, spray bottles, and tongs,\nwhich are underrepresented in existing datasets. The model trained on the web\nHOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across\nall objects in simulation, with a 6.7% improvement in success rate and a 1.8x\nincrease in functionality ratings over baselines. Simulator-augmented data\nfurther boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the\nLEAP Hand achieves a 85% success rate. Project website is at:\nhttps://webgrasp.github.io/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7f51\u7edc\u56fe\u50cf\u4e2d\u63d0\u53d6\u4eba\u7c7b\u6293\u53d6\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u529f\u80fd\u6027\u6293\u53d6\u6a21\u578b\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u673a\u5668\u4eba\u6f14\u793a\u9700\u6c42\u3002\u901a\u8fc7\u91cd\u5efa3D\u624b-\u7269\u4f53\u4ea4\u4e92\u7f51\u683c\u5e76\u5229\u7528\u6a21\u62df\u5668\u6269\u5c55\u6570\u636e\u96c6\uff0c\u6a21\u578b\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u529f\u80fd\u6027\u6293\u53d6\u5bf9\u673a\u5668\u4eba\u591a\u6307\u624b\u7684\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u5f3a\u529b\u6293\u53d6\u6216\u6602\u8d35\u7684\u6f14\u793a\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u7f51\u7edc\u56fe\u50cf\u4e2d\u7684\u81ea\u7136\u6293\u53d6\u4fe1\u606f\uff0c\u964d\u4f4e\u6570\u636e\u83b7\u53d6\u6210\u672c\u3002", "method": "\u4eceRGB\u56fe\u50cf\u91cd\u5efa3D\u624b-\u7269\u4f53\u4ea4\u4e92\u7f51\u683c\uff0c\u5c06\u4eba\u624b\u52a8\u4f5c\u8fc1\u79fb\u5230\u673a\u5668\u4eba\u624b\u4e0a\uff0c\u5e76\u4e0e\u7cbe\u786e3D\u7269\u4f53\u6a21\u578b\u5bf9\u9f50\u3002\u5229\u7528\u6a21\u62df\u5668\u751f\u6210\u66f4\u591a\u6293\u53d6\u6570\u636e\uff0c\u6269\u5c55\u8bad\u7ec3\u96c6\u3002", "result": "\u6a21\u578b\u5728\u4eff\u771f\u4e2d\u8fbe\u523075.8%\u7684\u6210\u529f\u7387\uff08\u5df2\u77e5\u7269\u4f53\uff09\u548c61.8%\uff08\u6240\u6709\u7269\u4f53\uff09\uff0c\u6a21\u62df\u5668\u589e\u5f3a\u540e\u63d0\u5347\u81f383.4%\u3002\u771f\u5b9e\u673a\u5668\u4eba\u6d4b\u8bd5\u6210\u529f\u7387\u4e3a85%\u3002", "conclusion": "\u7f51\u7edc\u56fe\u50cf\u53ef\u4f5c\u4e3a\u4f4e\u6210\u672c\u7684\u529f\u80fd\u6027\u6293\u53d6\u6570\u636e\u6e90\uff0c\u7ed3\u5408\u6a21\u62df\u5668\u6269\u5c55\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.05519", "pdf": "https://arxiv.org/pdf/2505.05519", "abs": "https://arxiv.org/abs/2505.05519", "authors": ["Minkyu Choi", "Yunhao Yang", "Neel P. Bhatt", "Kushagra Gupta", "Sahil Shah", "Aditya Rai", "David Fridovich-Keil", "Ufuk Topcu", "Sandeep P. Chinchali"], "title": "Real-Time Privacy Preservation for Robot Visual Perception", "categories": ["cs.CV"], "comment": null, "summary": "Many robots (e.g., iRobot's Roomba) operate based on visual observations from\nlive video streams, and such observations may inadvertently include\nprivacy-sensitive objects, such as personal identifiers. Existing approaches\nfor preserving privacy rely on deep learning models, differential privacy, or\ncryptography. They lack guarantees for the complete concealment of all\nsensitive objects. Guaranteeing concealment requires post-processing techniques\nand thus is inadequate for real-time video streams. We develop a method for\nprivacy-constrained video streaming, PCVS, that conceals sensitive objects\nwithin real-time video streams. PCVS takes a logical specification constraining\nthe existence of privacy-sensitive objects, e.g., never show faces when a\nperson exists. It uses a detection model to evaluate the existence of these\nobjects in each incoming frame. Then, it blurs out a subset of objects such\nthat the existence of the remaining objects satisfies the specification. We\nthen propose a conformal prediction approach to (i) establish a theoretical\nlower bound on the probability of the existence of these objects in a sequence\nof frames satisfying the specification and (ii) update the bound with the\narrival of each subsequent frame. Quantitative evaluations show that PCVS\nachieves over 95 percent specification satisfaction rate in multiple datasets,\nsignificantly outperforming other methods. The satisfaction rate is\nconsistently above the theoretical bounds across all datasets, indicating that\nthe established bounds hold. Additionally, we deploy PCVS on robots in\nreal-time operation and show that the robots operate normally without being\ncompromised when PCVS conceals objects.", "AI": {"tldr": "PCVS\u662f\u4e00\u79cd\u5b9e\u65f6\u89c6\u9891\u6d41\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u903b\u8f91\u89c4\u8303\u548c\u5bf9\u8c61\u6a21\u7cca\u5316\u786e\u4fdd\u654f\u611f\u5bf9\u8c61\u5b8c\u5168\u9690\u85cf\uff0c\u5e76\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u9690\u85cf\u654f\u611f\u5bf9\u8c61\u4e14\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u89c6\u9891\u6d41\uff0cPCVS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PCVS\u7ed3\u5408\u903b\u8f91\u89c4\u8303\u3001\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u548c\u6a21\u7cca\u5316\u6280\u672f\uff0c\u5b9e\u65f6\u5904\u7406\u89c6\u9891\u5e27\uff0c\u5e76\u901a\u8fc7\u4fdd\u5f62\u9884\u6d4b\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "PCVS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u8d85\u8fc795%\u7684\u89c4\u8303\u6ee1\u8db3\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u5b9e\u9645\u90e8\u7f72\u4e2d\u673a\u5668\u4eba\u8fd0\u884c\u6b63\u5e38\u3002", "conclusion": "PCVS\u4e3a\u5b9e\u65f6\u89c6\u9891\u6d41\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u7b49\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.05520", "pdf": "https://arxiv.org/pdf/2505.05520", "abs": "https://arxiv.org/abs/2505.05520", "authors": ["Chengwei Ye", "Huanzhen Zhang", "Yufei Lin", "Kangsheng Wang", "Linuo Xu", "Shuyan Liu"], "title": "GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gliomas are aggressive brain tumors that pose serious health risks. Deep\nlearning aids in lesion segmentation, but CNN and Transformer-based models\noften lack context modeling or demand heavy computation, limiting real-time use\non mobile medical devices. We propose GaMNet, integrating the NMamba module for\nglobal modeling and a multi-scale CNN for efficient local feature extraction.\nTo improve interpretability and mimic the human visual system, we apply Gabor\nfilters at multiple scales. Our method achieves high segmentation accuracy with\nfewer parameters and faster computation. Extensive experiments show GaMNet\noutperforms existing methods, notably reducing false positives and negatives,\nwhich enhances the reliability of clinical diagnosis.", "AI": {"tldr": "GaMNet\u7ed3\u5408NMamba\u6a21\u5757\u548c\u591a\u5c3a\u5ea6CNN\uff0c\u7528\u4e8e\u9ad8\u6548\u8111\u80f6\u8d28\u7624\u5206\u5272\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709CNN\u548cTransformer\u6a21\u578b\u5728\u8111\u80f6\u8d28\u7624\u5206\u5272\u4e2d\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u6216\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5728\u79fb\u52a8\u533b\u7597\u8bbe\u5907\u4e0a\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u63d0\u51faGaMNet\uff0c\u96c6\u6210NMamba\u6a21\u5757\u8fdb\u884c\u5168\u5c40\u5efa\u6a21\uff0c\u591a\u5c3a\u5ea6CNN\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u4f7f\u7528Gabor\u6ee4\u6ce2\u5668\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGaMNet\u5728\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u5206\u5272\u51c6\u786e\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u5047\u9633\u6027\u548c\u5047\u9634\u6027\u3002", "conclusion": "GaMNet\u4e3a\u8111\u80f6\u8d28\u7624\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u4e34\u5e8a\u8bca\u65ad\u5e94\u7528\u3002"}}
{"id": "2505.05528", "pdf": "https://arxiv.org/pdf/2505.05528", "abs": "https://arxiv.org/abs/2505.05528", "authors": ["Hanxun Huang", "Sarah Erfani", "Yige Li", "Xingjun Ma", "James Bailey"], "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly\nadopted for diverse downstream tasks and integrated into large vision-language\nmodels (VLMs), their susceptibility to adversarial perturbations has emerged as\na critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel\nattack method that exposes a universal adversarial vulnerability in CLIP.\nX-Transfer generates a Universal Adversarial Perturbation (UAP) capable of\ndeceiving various CLIP encoders and downstream VLMs across different samples,\ntasks, and domains. We refer to this property as \\textbf{super\ntransferability}--a single perturbation achieving cross-data, cross-domain,\ncross-model, and cross-task adversarial transferability simultaneously. This is\nachieved through \\textbf{surrogate scaling}, a key innovation of our approach.\nUnlike existing methods that rely on fixed surrogate models, which are\ncomputationally intensive to scale, X-Transfer employs an efficient surrogate\nscaling strategy that dynamically selects a small subset of suitable surrogates\nfrom a large search space. Extensive evaluations demonstrate that X-Transfer\nsignificantly outperforms previous state-of-the-art UAP methods, establishing a\nnew benchmark for adversarial transferability across CLIP models. The code is\npublicly available in our\n\\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.", "AI": {"tldr": "X-Transfer\u662f\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86CLIP\u6a21\u578b\u7684\u901a\u7528\u5bf9\u6297\u6f0f\u6d1e\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4ee3\u7406\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u653b\u51fb\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u4f7f\u5176\u5bf9\u6297\u6270\u52a8\u7684\u8106\u5f31\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "X-Transfer\u5229\u7528\u4ee3\u7406\u7f29\u653e\u7b56\u7565\u751f\u6210\u901a\u7528\u5bf9\u6297\u6270\u52a8\uff08UAP\uff09\uff0c\u52a8\u6001\u9009\u62e9\u5c11\u91cf\u5408\u9002\u7684\u4ee3\u7406\u6a21\u578b\u3002", "result": "X-Transfer\u663e\u8457\u4f18\u4e8e\u73b0\u6709UAP\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8de8\u6570\u636e\u3001\u8de8\u9886\u57df\u3001\u8de8\u6a21\u578b\u548c\u8de8\u4efb\u52a1\u7684\u5bf9\u6297\u8fc1\u79fb\u6027\u3002", "conclusion": "X-Transfer\u4e3aCLIP\u6a21\u578b\u7684\u5bf9\u6297\u8fc1\u79fb\u6027\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.05531", "pdf": "https://arxiv.org/pdf/2505.05531", "abs": "https://arxiv.org/abs/2505.05531", "authors": ["Hanie Moghaddasi", "Christina Chambers", "Sarah N. Mattson", "Jeffrey R. Wozniak", "Claire D. Coles", "Raja Mukherjee", "Michael Suttie"], "title": "OXSeg: Multidimensional attention UNet-based lip segmentation using semi-supervised lip contours", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Lip segmentation plays a crucial role in various domains, such as lip\nsynchronization, lipreading, and diagnostics. However, the effectiveness of\nsupervised lip segmentation is constrained by the availability of lip contour\nin the training phase. A further challenge with lip segmentation is its\nreliance on image quality , lighting, and skin tone, leading to inaccuracies in\nthe detected boundaries. To address these challenges, we propose a sequential\nlip segmentation method that integrates attention UNet and multidimensional\ninput. We unravel the micro-patterns in facial images using local binary\npatterns to build multidimensional inputs. Subsequently, the multidimensional\ninputs are fed into sequential attention UNets, where the lip contour is\nreconstructed. We introduce a mask generation method that uses a few anatomical\nlandmarks and estimates the complete lip contour to improve segmentation\naccuracy. This mask has been utilized in the training phase for lip\nsegmentation. To evaluate the proposed method, we use facial images to segment\nthe upper lips and subsequently assess lip-related facial anomalies in subjects\nwith fetal alcohol syndrome (FAS). Using the proposed lip segmentation method,\nwe achieved a mean dice score of 84.75%, and a mean pixel accuracy of 99.77% in\nupper lip segmentation. To further evaluate the method, we implemented\nclassifiers to identify those with FAS. Using a generative adversarial network\n(GAN), we reached an accuracy of 98.55% in identifying FAS in one of the study\npopulations. This method could be used to improve lip segmentation accuracy,\nespecially around Cupid's bow, and shed light on distinct lip-related\ncharacteristics of FAS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6ce8\u610f\u529bUNet\u548c\u591a\u7ef4\u8f93\u5165\u7684\u5507\u90e8\u5206\u5272\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\uff0c\u5e76\u5728\u80ce\u513f\u9152\u7cbe\u7efc\u5408\u5f81\uff08FAS\uff09\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5507\u90e8\u5206\u5272\u5728\u591a\u4e2a\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u5149\u7167\u6761\u4ef6\uff0c\u5bfc\u81f4\u8fb9\u754c\u68c0\u6d4b\u4e0d\u51c6\u786e\u3002", "method": "\u4f7f\u7528\u5c40\u90e8\u4e8c\u503c\u6a21\u5f0f\u63d0\u53d6\u9762\u90e8\u56fe\u50cf\u7684\u5fae\u6a21\u5f0f\uff0c\u6784\u5efa\u591a\u7ef4\u8f93\u5165\uff1b\u901a\u8fc7\u987a\u5e8f\u6ce8\u610f\u529bUNet\u91cd\u5efa\u5507\u90e8\u8f6e\u5ed3\uff1b\u5f15\u5165\u57fa\u4e8e\u89e3\u5256\u6807\u5fd7\u7684\u63a9\u6a21\u751f\u6210\u65b9\u6cd5\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002", "result": "\u4e0a\u5507\u5206\u5272\u7684\u5e73\u5747Dice\u5206\u6570\u4e3a84.75%\uff0c\u50cf\u7d20\u7cbe\u5ea6\u4e3a99.77%\uff1b\u5728FAS\u8bc6\u522b\u4e2d\uff0cGAN\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8fbe98.55%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5507\u90e8\u5206\u5272\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u4e18\u6bd4\u7279\u5f13\u533a\u57df\uff0c\u5e76\u4e3aFAS\u7684\u5507\u90e8\u7279\u5f81\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05540", "pdf": "https://arxiv.org/pdf/2505.05540", "abs": "https://arxiv.org/abs/2505.05540", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Sudipta Chowdhury", "Harshvardhan Sikka"], "title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages, 26 figures", "summary": "Vision-language-action (VLA) models represent an important step toward\ngeneral-purpose robotic systems by integrating visual perception, language\nunderstanding, and action execution. However, systematic evaluation of these\nmodels, particularly their zero-shot generalization capabilities in\nout-of-distribution (OOD) environments, remains limited. In this paper, we\nintroduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and\nanalyze the generalization performance of state-of-the-art VLM and VLA\nmodels-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse\nprocedural tasks from the Procgen benchmark. Our analysis reveals several\ncritical insights: (1) all evaluated models exhibit significant limitations in\nzero-shot generalization to OOD tasks, with performance heavily influenced by\nfactors such as action representation and task complexit; (2) VLAs generally\noutperform other models due to their robust architectural design; and (3) VLM\nvariants demonstrate substantial improvements when constrained appropriately,\nhighlighting the sensitivity of model performance to precise prompt\nengineering.", "AI": {"tldr": "MultiNet v0.2\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728OOD\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165MultiNet v0.2\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u5305\u62ecGPT-4o\u3001GPT-4.1\u3001OpenVLA\u7b49\u5728\u5185\u7684\u591a\u79cdVLA\u548cVLM\u6a21\u578b\u5728Procgen\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u6240\u6709\u6a21\u578b\u5728OOD\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0cVLA\u6a21\u578b\u56e0\u67b6\u6784\u8bbe\u8ba1\u66f4\u4f18\u800c\u8868\u73b0\u66f4\u597d\uff0cVLM\u6a21\u578b\u5728\u9002\u5f53\u7ea6\u675f\u4e0b\u6709\u660e\u663e\u6539\u8fdb\u3002", "conclusion": "\u6a21\u578b\u6027\u80fd\u53d7\u52a8\u4f5c\u8868\u793a\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u5f71\u54cd\uff0c\u63d0\u793a\u5de5\u7a0b\u5bf9VLM\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.05573", "pdf": "https://arxiv.org/pdf/2505.05573", "abs": "https://arxiv.org/abs/2505.05573", "authors": ["Mikhail Chaichuk", "Sushant Gautam", "Steven Hicks", "Elena Tutubalina"], "title": "Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.4.8; J.3"], "comment": "code available at\n  https://github.com/THunderCondOR/ImageCLEFmed-MEDVQA-GI-2024-MMCP-Team", "summary": "The generation of realistic medical images from text descriptions has\nsignificant potential to address data scarcity challenges in healthcare AI\nwhile preserving patient privacy. This paper presents a comprehensive study of\ntext-to-image synthesis in the medical domain, comparing two distinct\napproaches: (1) fine-tuning large pre-trained latent diffusion models and (2)\ntraining small, domain-specific models. We introduce a novel model named MSDM,\nan optimized architecture based on Stable Diffusion that integrates a clinical\ntext encoder, variational autoencoder, and cross-attention mechanisms to better\nalign medical text prompts with generated images. Our study compares two\napproaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus\ntraining compact domain-specific models (MSDM). Evaluation across colonoscopy\n(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models\nachieve higher fidelity, our optimized MSDM delivers comparable quality with\nlower computational costs. Quantitative metrics and qualitative evaluations by\nmedical experts reveal strengths and limitations of each approach.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u533b\u7597\u9886\u57df\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u7684\u4e24\u79cd\u65b9\u6cd5\uff1a\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u8bad\u7ec3\u5c0f\u578b\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u67b6\u6784MSDM\u3002", "motivation": "\u89e3\u51b3\u533b\u7597AI\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "method": "\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\uff1a\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08FLUX, Kandinsky\uff09\u548c\u8bad\u7ec3\u5c0f\u578b\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08MSDM\uff09\u3002MSDM\u6574\u5408\u4e86\u4e34\u5e8a\u6587\u672c\u7f16\u7801\u5668\u3001\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5927\u578b\u6a21\u578b\u751f\u6210\u56fe\u50cf\u4fdd\u771f\u5ea6\u66f4\u9ad8\uff0c\u4f46MSDM\u5728\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u7684\u60c5\u51b5\u4e0b\u8d28\u91cf\u76f8\u5f53\u3002", "conclusion": "MSDM\u5728\u533b\u7597\u9886\u57df\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.05587", "pdf": "https://arxiv.org/pdf/2505.05587", "abs": "https://arxiv.org/abs/2505.05587", "authors": ["Peihao Wang", "Yuehao Wang", "Dilin Wang", "Sreyas Mohan", "Zhiwen Fan", "Lemeng Wu", "Ruisi Cai", "Yu-Ying Yeh", "Zhangyang Wang", "Qiang Liu", "Rakesh Ranjan"], "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page: https://vita-group.github.io/SteepGS/", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for\nreal-time, high-resolution novel view synthesis. By representing scenes as a\nmixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for\nefficient rendering and reconstruction. To optimize scene coverage and capture\nfine details, 3DGS employs a densification algorithm to generate additional\npoints. However, this process often leads to redundant point clouds, resulting\nin excessive memory usage, slower performance, and substantial storage demands\n- posing significant challenges for deployment on resource-constrained devices.\nTo address this limitation, we propose a theoretical framework that demystifies\nand improves density control in 3DGS. Our analysis reveals that splitting is\ncrucial for escaping saddle points. Through an optimization-theoretic approach,\nwe establish the necessary conditions for densification, determine the minimal\nnumber of offspring Gaussians, identify the optimal parameter update direction,\nand provide an analytical solution for normalizing off-spring opacity. Building\non these insights, we introduce SteepGS, incorporating steepest density\ncontrol, a principled strategy that minimizes loss while maintaining a compact\npoint cloud. SteepGS achieves a ~50% reduction in Gaussian points without\ncompromising rendering quality, significantly enhancing both efficiency and\nscalability.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u662f\u4e00\u79cd\u9ad8\u6548\u5b9e\u65f6\u65b0\u89c6\u89d2\u5408\u6210\u6280\u672f\uff0c\u4f46\u70b9\u4e91\u5197\u4f59\u95ee\u9898\u5bfc\u81f4\u5185\u5b58\u548c\u6027\u80fd\u95ee\u9898\u3002\u672c\u6587\u63d0\u51faSteepGS\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4f18\u5316\u65b9\u6cd5\u51cf\u5c1150%\u9ad8\u65af\u70b9\uff0c\u63d0\u5347\u6548\u7387\u3002", "motivation": "3DGS\u5728\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u70b9\u4e91\u5197\u4f59\u5bfc\u81f4\u5185\u5b58\u548c\u6027\u80fd\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u5206\u6790\u5bc6\u5ea6\u63a7\u5236\uff0c\u786e\u5b9a\u9ad8\u65af\u5206\u88c2\u7684\u5fc5\u8981\u6761\u4ef6\u3001\u6700\u5c0f\u540e\u4ee3\u6570\u91cf\u3001\u53c2\u6570\u66f4\u65b0\u65b9\u5411\u53ca\u900f\u660e\u5ea6\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165SteepGS\u7b56\u7565\u3002", "result": "SteepGS\u51cf\u5c1150%\u9ad8\u65af\u70b9\uff0c\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SteepGS\u901a\u8fc7\u7406\u8bba\u4f18\u5316\u89e3\u51b3\u4e863DGS\u7684\u70b9\u4e91\u5197\u4f59\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05589", "pdf": "https://arxiv.org/pdf/2505.05589", "abs": "https://arxiv.org/abs/2505.05589", "authors": ["Jingzhong Lin", "Yuanyuan Qi", "Xinru Li", "Wenxuan Huang", "Xiangfeng Xu", "Bangyan Li", "Xuejiao Wang", "Gaoqi He"], "title": "ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Reactive dance generation (RDG) produces follower movements conditioned on\nguiding dancer and music while ensuring spatial coordination and temporal\ncoherence. However, existing methods overemphasize global constraints and\noptimization, overlooking local information, such as fine-grained spatial\ninteractions and localized temporal context. Therefore, we present ReactDance,\na novel diffusion-based framework for high-fidelity RDG with long-term\ncoherence and multi-scale controllability. Unlike existing methods that\nstruggle with interaction fidelity, synchronization, and temporal consistency\nin duet synthesis, our approach introduces two key innovations: 1)Group\nResidual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion\nrepresentation that captures interaction semantics from coarse body rhythms to\nfine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling\nstrategy eliminating error accumulation in long sequence generation via local\nblock causal masking and periodic positional encoding. Built on the decoupled\nmulti-scale GRFSQ representation, we implement a diffusion model\nwithLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control\nover motion semantics across scales. Extensive experiments on standard\nbenchmarks demonstrate that ReactDance surpasses existing methods, achieving\nstate-of-the-art performance.", "AI": {"tldr": "ReactDance\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u7684\u53cd\u5e94\u6027\u821e\u8e48\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5168\u5c40\u7ea6\u675f\u548c\u4f18\u5316\u4e2d\u5ffd\u89c6\u5c40\u90e8\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u53cd\u5e94\u6027\u821e\u8e48\u751f\u6210\u4e2d\u8fc7\u4e8e\u5f3a\u8c03\u5168\u5c40\u7ea6\u675f\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8\u4fe1\u606f\uff08\u5982\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u4ea4\u4e92\u548c\u5c40\u90e8\u65f6\u95f4\u4e0a\u4e0b\u6587\uff09\uff0c\u5bfc\u81f4\u4ea4\u4e92\u4fdd\u771f\u5ea6\u3001\u540c\u6b65\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u521b\u65b0\uff1a1) GRFSQ\uff08\u591a\u5c3a\u5ea6\u89e3\u8026\u8fd0\u52a8\u8868\u793a\uff09\uff0c\u6355\u6349\u4ece\u7c97\u7c92\u5ea6\u8eab\u4f53\u8282\u594f\u5230\u7ec6\u7c92\u5ea6\u5173\u8282\u52a8\u6001\u7684\u4ea4\u4e92\u8bed\u4e49\uff1b2) BLC\uff08\u91c7\u6837\u7b56\u7565\uff09\uff0c\u901a\u8fc7\u5c40\u90e8\u5757\u56e0\u679c\u63a9\u7801\u548c\u5468\u671f\u6027\u4f4d\u7f6e\u7f16\u7801\u6d88\u9664\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReactDance\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ReactDance\u901a\u8fc7\u591a\u5c3a\u5ea6\u89e3\u8026\u8868\u793a\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cd\u5e94\u6027\u821e\u8e48\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2505.05591", "pdf": "https://arxiv.org/pdf/2505.05591", "abs": "https://arxiv.org/abs/2505.05591", "authors": ["Yueh-Cheng Liu", "Lukas H\u00f6llein", "Matthias Nie\u00dfner", "Angela Dai"], "title": "QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization", "categories": ["cs.CV"], "comment": "Project page: https://liu115.github.io/quicksplat, Video:\n  https://youtu.be/2IA_gnFvFG8", "summary": "Surface reconstruction is fundamental to computer vision and graphics,\nenabling applications in 3D modeling, mixed reality, robotics, and more.\nExisting approaches based on volumetric rendering obtain promising results, but\noptimize on a per-scene basis, resulting in a slow optimization that can\nstruggle to model under-observed or textureless regions. We introduce\nQuickSplat, which learns data-driven priors to generate dense initializations\nfor 2D gaussian splatting optimization of large-scale indoor scenes. This\nprovides a strong starting point for the reconstruction, which accelerates the\nconvergence of the optimization and improves the geometry of flat wall\nstructures. We further learn to jointly estimate the densification and update\nof the scene parameters during each iteration; our proposed densifier network\npredicts new Gaussians based on the rendering gradients of existing ones,\nremoving the needs of heuristics for densification. Extensive experiments on\nlarge-scale indoor scene reconstruction demonstrate the superiority of our\ndata-driven optimization. Concretely, we accelerate runtime by 8x, while\ndecreasing depth errors by up to 48% in comparison to state of the art methods.", "AI": {"tldr": "QuickSplat\u5229\u7528\u6570\u636e\u9a71\u52a8\u5148\u9a8c\u751f\u62102D\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\u7684\u5bc6\u96c6\u521d\u59cb\u5316\uff0c\u52a0\u901f\u5927\u89c4\u6a21\u5ba4\u5185\u573a\u666f\u91cd\u5efa\uff0c\u63d0\u5347\u51e0\u4f55\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f53\u79ef\u6e32\u67d3\u7684\u65b9\u6cd5\u4f18\u5316\u901f\u5ea6\u6162\uff0c\u96be\u4ee5\u5904\u7406\u4f4e\u7eb9\u7406\u533a\u57df\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u5148\u9a8c\u751f\u6210\u5bc6\u96c6\u521d\u59cb\u5316\uff0c\u8054\u5408\u4f30\u8ba1\u573a\u666f\u53c2\u6570\u66f4\u65b0\uff0c\u63d0\u51fa\u57fa\u4e8e\u6e32\u67d3\u68af\u5ea6\u7684\u5bc6\u5ea6\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8fd0\u884c\u901f\u5ea6\u63d0\u53478\u500d\uff0c\u6df1\u5ea6\u8bef\u5dee\u964d\u4f4e48%\u3002", "conclusion": "QuickSplat\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u6548\u7387\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2505.05599", "pdf": "https://arxiv.org/pdf/2505.05599", "abs": "https://arxiv.org/abs/2505.05599", "authors": ["Seraj Al Mahmud Mostafa", "Chenxi Wang", "Jia Yue", "Yuta Hozumi", "Jianwu Wang"], "title": "Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted to International conference on Advanced\n  Machine Learning and Data Science (AMLDS) 2025", "summary": "Object localization in satellite imagery is particularly challenging due to\nthe high variability of objects, low spatial resolution, and interference from\nnoise and dominant features such as clouds and city lights. In this research,\nwe focus on three satellite datasets: upper atmospheric Gravity Waves (GW),\nmesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique\nchallenges. These challenges include the variability in the scale and\nappearance of the main object patterns, where the size, shape, and feature\nextent of objects of interest can differ significantly. To address these\nchallenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed\nto improve object localization in these complex scenarios. YOLO-DCAP\nincorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture\nmulti-scale features at scale with varying dilation rates, and an\nAttention-aided Spatial Pooling (AaSP) module to focus on the global relevant\nspatial regions, enhancing feature selection. These structural improvements\nhelp to better localize objects in satellite imagery. Experimental results\ndemonstrate that YOLO-DCAP significantly outperforms both the YOLO base model\nand state-of-the-art approaches, achieving an average improvement of 20.95% in\nmAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively\nover state-of-the-art alternatives, consistently across all three satellite\ndatasets. These consistent gains across all three satellite datasets highlight\nthe robustness and generalizability of the proposed approach. Our code is open\nsourced at\nhttps://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684YOLOv5\u6a21\u578bYOLO-DCAP\uff0c\u7528\u4e8e\u89e3\u51b3\u536b\u661f\u56fe\u50cf\u4e2d\u7269\u4f53\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u5305\u62ec\u591a\u5c3a\u5ea6\u7279\u5f81\u6355\u83b7\u548c\u5168\u5c40\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u548c\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u4e2d\u7269\u4f53\u5b9a\u4f4d\u9762\u4e34\u9ad8\u53d8\u5f02\u6027\u3001\u4f4e\u5206\u8fa8\u7387\u548c\u566a\u58f0\u5e72\u6270\u7b49\u6311\u6218\uff0c\u5c24\u5176\u662f\u91cd\u529b\u6ce2\u3001\u4e2d\u6c14\u5c42\u6ce2\u548c\u6d77\u6d0b\u6da1\u65cb\u7b49\u590d\u6742\u573a\u666f\u3002", "method": "YOLO-DCAP\u5f15\u5165\u591a\u5c3a\u5ea6\u6269\u5f20\u6b8b\u5dee\u5377\u79ef\u5757\uff08MDRC\uff09\u548c\u6ce8\u610f\u529b\u8f85\u52a9\u7a7a\u95f4\u6c60\u5316\u6a21\u5757\uff08AaSP\uff09\uff0c\u4ee5\u6355\u83b7\u591a\u5c3a\u5ea6\u7279\u5f81\u5e76\u805a\u7126\u5168\u5c40\u76f8\u5173\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cYOLO-DCAP\u5728mAP50\u548cIoU\u4e0a\u5206\u522b\u6bd4\u57fa\u51c6\u6a21\u578b\u5e73\u5747\u63d0\u534720.95%\u548c32.23%\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53477.35%\u548c9.84%\u3002", "conclusion": "YOLO-DCAP\u5728\u591a\u4e2a\u536b\u661f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a33\u5065\u4e14\u901a\u7528\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.05621", "pdf": "https://arxiv.org/pdf/2505.05621", "abs": "https://arxiv.org/abs/2505.05621", "authors": ["Hao Yang", "Yan Yang", "Ruikun Zhang", "Liyuan Pan"], "title": "A Preliminary Study for GPT-4o on Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an\nautoregressive architecture, has demonstrated unprecedented performance in\nimage generation. In this work, we investigate its potential impact on the\nimage restoration community. We present the first systematic evaluation of\nGPT-4o across diverse restoration tasks. Our experiments reveal that, although\nrestoration outputs from GPT-4o are visually appealing, they often suffer from\npixel-level structural fidelity when compared to ground-truth images. Common\nissues are variations in image proportions, shifts in object positions and\nquantities, and changes in viewpoint.To address it, taking image dehazing,\nderainning, and low-light enhancement as representative case studies, we show\nthat GPT-4o's outputs can serve as powerful visual priors, substantially\nenhancing the performance of existing dehazing networks. It offers practical\nguidelines and a baseline framework to facilitate the integration of GPT-4o\ninto future image restoration pipelines. We hope the study on GPT-4o image\nrestoration will accelerate innovation in the broader field of image generation\nareas. To support further research, we will release GPT-4o-restored images from\nover 10 widely used image restoration datasets.", "AI": {"tldr": "GPT-4o\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u5b58\u5728\u50cf\u7d20\u7ea7\u7ed3\u6784\u4fdd\u771f\u5ea6\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u5176\u8f93\u51fa\u53ef\u4f5c\u4e3a\u89c6\u89c9\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u73b0\u6709\u53bb\u96fe\u7f51\u7edc\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8GPT-4o\u5728\u56fe\u50cf\u4fee\u590d\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5e76\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30GPT-4o\u5728\u591a\u79cd\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53bb\u96fe\u3001\u53bb\u96e8\u548c\u4f4e\u5149\u589e\u5f3a\u4e3a\u4f8b\u3002", "result": "GPT-4o\u8f93\u51fa\u89c6\u89c9\u6548\u679c\u597d\uff0c\u4f46\u7ed3\u6784\u4fdd\u771f\u5ea6\u4e0d\u8db3\uff1b\u53ef\u4f5c\u4e3a\u89c6\u89c9\u5148\u9a8c\u63d0\u5347\u73b0\u6709\u7f51\u7edc\u6027\u80fd\u3002", "conclusion": "GPT-4o\u4e3a\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u6709\u671b\u52a0\u901f\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u521b\u65b0\u3002"}}
{"id": "2505.05626", "pdf": "https://arxiv.org/pdf/2505.05626", "abs": "https://arxiv.org/abs/2505.05626", "authors": ["Aarti Ghatkesar", "Uddeshya Upadhyay", "Ganesh Venkatesh"], "title": "Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Achieving deep alignment between vision and language remains a central\nchallenge for Multimodal Large Language Models (MLLMs). These models often fail\nto fully leverage visual input, defaulting to strong language priors. Our\napproach first provides insights into how MLLMs internally build visual\nunderstanding of image regions and then introduces techniques to amplify this\ncapability. Specifically, we explore techniques designed both to deepen the\nmodel's understanding of visual content and to ensure that these visual\ninsights actively guide language generation. We demonstrate the superior\nmultimodal understanding of our resultant model through a detailed upstream\nanalysis quantifying its ability to predict visually-dependent tokens as well\nas 10 pt boost on visually challenging tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u6765\u6539\u5584\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u6df1\u5ea6\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u5f53\u524dMLLMs\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u89c9\u8f93\u5165\uff0c\u8fc7\u4e8e\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u5bfc\u81f4\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bf9\u9f50\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u5bf9\u56fe\u50cf\u533a\u57df\u7684\u89c6\u89c9\u7406\u89e3\uff0c\u5e76\u5f15\u5165\u6280\u672f\u589e\u5f3a\u8fd9\u79cd\u80fd\u529b\uff0c\u540c\u65f6\u786e\u4fdd\u89c6\u89c9\u4fe1\u606f\u6307\u5bfc\u8bed\u8a00\u751f\u6210\u3002", "result": "\u6a21\u578b\u5728\u89c6\u89c9\u4f9d\u8d56\u7684\u6807\u8bb0\u9884\u6d4b\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u89c6\u89c9\u6311\u6218\u4efb\u52a1\u4e2d\u63d0\u5347\u4e8610\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u6df1\u5ea6\u5bf9\u9f50\u3002"}}
{"id": "2505.05635", "pdf": "https://arxiv.org/pdf/2505.05635", "abs": "https://arxiv.org/abs/2505.05635", "authors": ["Faizan Farooq Khan", "Jun Chen", "Youssef Mohamed", "Chun-Mei Feng", "Mohamed Elhoseiny"], "title": "VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models", "categories": ["cs.CV"], "comment": "7 figures", "summary": "Open-vocabulary recognition remains a challenging problem in computer vision,\nas it requires identifying objects from an unbounded set of categories. This is\nparticularly relevant in nature, where new species are discovered every year.\nIn this work, we focus on open-vocabulary bird species recognition, where the\ngoal is to classify species based on their descriptions without being\nconstrained to a predefined set of taxonomic categories. Traditional benchmarks\nlike CUB-200-2011 and Birdsnap have been evaluated in a closed-vocabulary\nparadigm, limiting their applicability to real-world scenarios where novel\nspecies continually emerge. We show that the performance of current systems\nwhen evaluated under settings closely aligned with open-vocabulary drops by a\nhuge margin. To address this gap, we propose a scalable framework integrating\nstructured textual knowledge from Wikipedia articles of 11,202 bird species\ndistilled via GPT-4o into concise, discriminative summaries. We propose Visual\nRe-ranking Retrieval-Augmented Generation(VR-RAG), a novel, retrieval-augmented\ngeneration framework that uses visual similarities to rerank the top m\ncandidates retrieved by a set of multimodal vision language encoders. This\nallows for the recognition of unseen taxa. Extensive experiments across five\nestablished classification benchmarks show that our approach is highly\neffective. By integrating VR-RAG, we improve the average performance of\nstate-of-the-art Large Multi-Modal Model QWEN2.5-VL by 15.4% across five\nbenchmarks. Our approach outperforms conventional VLM-based approaches, which\nstruggle with unseen species. By bridging the gap between encyclopedic\nknowledge and visual recognition, our work advances open-vocabulary\nrecognition, offering a flexible, scalable solution for biodiversity monitoring\nand ecological research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVR-RAG\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u9e1f\u7c7b\u7269\u79cd\u8bc6\u522b\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u76f8\u4f3c\u6027\u548c\u6587\u672c\u77e5\u8bc6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u81ea\u7136\u754c\u4e2d\uff0c\u65b0\u7269\u79cd\u4e0d\u65ad\u51fa\u73b0\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5f00\u653e\u8bcd\u6c47\u8bbe\u5b9a\u4e0b\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002", "method": "\u63d0\u51faVR-RAG\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u76f8\u4f3c\u6027\u91cd\u6392\u5e8f\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u7f16\u7801\u5668\u68c0\u7d22\u7684\u5019\u9009\u7ed3\u679c\uff0c\u5e76\u5229\u7528GPT-4o\u4ece\u7ef4\u57fa\u767e\u79d1\u63d0\u53d6\u6587\u672c\u77e5\u8bc6\u3002", "result": "\u5728\u4e94\u4e2a\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVR-RAG\u5c06\u6700\u5148\u8fdb\u6a21\u578bQWEN2.5-VL\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e8615.4%\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "VR-RAG\u901a\u8fc7\u7ed3\u5408\u767e\u79d1\u5168\u4e66\u77e5\u8bc6\u548c\u89c6\u89c9\u8bc6\u522b\uff0c\u63a8\u52a8\u4e86\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\u7684\u53d1\u5c55\uff0c\u4e3a\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05640", "pdf": "https://arxiv.org/pdf/2505.05640", "abs": "https://arxiv.org/abs/2505.05640", "authors": ["Anadil Hussein", "Anna Zamansky", "George Martvel"], "title": "Semantic Style Transfer for Enhancing Animal Facial Landmark Detection", "categories": ["cs.CV"], "comment": null, "summary": "Neural Style Transfer (NST) is a technique for applying the visual\ncharacteristics of one image onto another while preserving structural content.\nTraditionally used for artistic transformations, NST has recently been adapted,\ne.g., for domain adaptation and data augmentation. This study investigates the\nuse of this technique for enhancing animal facial landmark detectors training.\nAs a case study, we use a recently introduced Ensemble Landmark Detector for 48\nanatomical cat facial landmarks and the CatFLW dataset it was trained on,\nmaking three main contributions. First, we demonstrate that applying style\ntransfer to cropped facial images rather than full-body images enhances\nstructural consistency, improving the quality of generated images. Secondly,\nreplacing training images with style-transferred versions raised challenges of\nannotation misalignment, but Supervised Style Transfer (SST) - which selects\nstyle sources based on landmark accuracy - retained up to 98% of baseline\naccuracy. Finally, augmenting the dataset with style-transferred images further\nimproved robustness, outperforming traditional augmentation methods. These\nfindings establish semantic style transfer as an effective augmentation\nstrategy for enhancing the performance of facial landmark detection models for\nanimals and beyond. While this study focuses on cat facial landmarks, the\nproposed method can be generalized to other species and landmark detection\nmodels.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\uff08NST\uff09\u5728\u63d0\u5347\u52a8\u7269\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6539\u8fdb\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528NST\u6280\u672f\u589e\u5f3a\u52a8\u7269\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u589e\u5f3a\u548c\u57df\u9002\u5e94\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u76d1\u7763\u98ce\u683c\u8fc1\u79fb\uff08SST\uff09\u65b9\u6cd5\uff0c\u9009\u62e9\u98ce\u683c\u6e90\u65f6\u57fa\u4e8e\u5173\u952e\u70b9\u51c6\u786e\u6027\uff0c\u5e76\u5728\u732b\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u6570\u636e\u96c6CatFLW\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u98ce\u683c\u8fc1\u79fb\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0cSST\u4fdd\u7559\u4e8698%\u7684\u57fa\u7ebf\u51c6\u786e\u6027\uff0c\u6570\u636e\u589e\u5f3a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u8bed\u4e49\u98ce\u683c\u8fc1\u79fb\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u7269\u79cd\u548c\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u3002"}}
{"id": "2505.05644", "pdf": "https://arxiv.org/pdf/2505.05644", "abs": "https://arxiv.org/abs/2505.05644", "authors": ["Tom Sander", "Moritz Tenthoff", "Kay Wohlfarth", "Christian W\u00f6hler"], "title": "The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "14pages", "summary": "Multimodal learning is an emerging research topic across multiple disciplines\nbut has rarely been applied to planetary science. In this contribution, we\nidentify that reflectance parameter estimation and image-based 3D\nreconstruction of lunar images can be formulated as a multimodal learning\nproblem. We propose a single, unified transformer architecture trained to learn\nshared representations between multiple sources like grayscale images, digital\nelevation models, surface normals, and albedo maps. The architecture supports\nflexible translation from any input modality to any target modality. Predicting\nDEMs and albedo maps from grayscale images simultaneously solves the task of 3D\nreconstruction of planetary surfaces and disentangles photometric parameters\nand height information. Our results demonstrate that our foundation model\nlearns physically plausible relations across these four modalities. Adding more\ninput modalities in the future will enable tasks such as photometric\nnormalization and co-registration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5b66\u4e60\u7684\u7edf\u4e00Transformer\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u7070\u5ea6\u56fe\u50cf\u4e2d\u9884\u6d4bDEM\u548c\u53cd\u7167\u7387\u56fe\uff0c\u89e3\u51b3\u4e86\u884c\u661f\u8868\u97623D\u91cd\u5efa\u548c\u5149\u5ea6\u53c2\u6570\u5206\u79bb\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u884c\u661f\u79d1\u5b66\u4e2d\u5e94\u7528\u8f83\u5c11\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u89e3\u51b3\u53cd\u5c04\u7387\u53c2\u6570\u4f30\u8ba1\u548c\u57fa\u4e8e\u56fe\u50cf\u76843D\u91cd\u5efa\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684Transformer\u67b6\u6784\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\u6a21\u6001\uff08\u5982\u7070\u5ea6\u56fe\u50cf\u3001DEM\u3001\u8868\u9762\u6cd5\u7ebf\u548c\u53cd\u7167\u7387\u56fe\uff09\u4e4b\u95f4\u7684\u7075\u6d3b\u8f6c\u6362\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u56db\u79cd\u6a21\u6001\u4e4b\u95f4\u7684\u7269\u7406\u5408\u7406\u5173\u7cfb\uff0c\u5e76\u6210\u529f\u9884\u6d4bDEM\u548c\u53cd\u7167\u7387\u56fe\u3002", "conclusion": "\u8be5\u57fa\u7840\u6a21\u578b\u4e3a\u672a\u6765\u6269\u5c55\u66f4\u591a\u8f93\u5165\u6a21\u6001\uff08\u5982\u5149\u5ea6\u5f52\u4e00\u5316\u548c\u5171\u914d\u51c6\uff09\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2505.05666", "pdf": "https://arxiv.org/pdf/2505.05666", "abs": "https://arxiv.org/abs/2505.05666", "authors": ["Alexander Most", "Joseph Winjum", "Ayan Biswas", "Shawn Jones", "Nishath Rajiv Ranasinghe", "Dan O'Malley", "Manish Bhattarai"], "title": "Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a popular technique for\nenhancing the reliability and utility of Large Language Models (LLMs) by\ngrounding responses in external documents. Traditional RAG systems rely on\nOptical Character Recognition (OCR) to first process scanned documents into\ntext. However, even state-of-the-art OCRs can introduce errors, especially in\ndegraded or complex documents. Recent vision-language approaches, such as\nColPali, propose direct visual embedding of documents, eliminating the need for\nOCR. This study presents a systematic comparison between a vision-based RAG\nsystem (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2\n(90B) and Nougat OCR across varying document qualities. Beyond conventional\nretrieval accuracy metrics, we introduce a semantic answer evaluation benchmark\nto assess end-to-end question-answering performance. Our findings indicate that\nwhile vision-based RAG performs well on documents it has been fine-tuned on,\nOCR-based RAG is better able to generalize to unseen documents of varying\nquality. We highlight the key trade-offs between computational efficiency and\nsemantic accuracy, offering practical guidance for RAG practitioners in\nselecting between OCR-dependent and vision-based document retrieval systems in\nproduction environments.", "AI": {"tldr": "\u6bd4\u8f83\u89c6\u89c9\u57fa\u7840\u7684RAG\uff08ColPali\uff09\u4e0e\u4f20\u7edfOCR\u57fa\u7840\u7684RAG\uff08Llama 3.2\u548cNougat OCR\uff09\u5728\u4e0d\u540c\u6587\u6863\u8d28\u91cf\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u89c6\u89c9RAG\u5728\u7279\u5b9a\u6587\u6863\u4e0a\u8868\u73b0\u597d\uff0c\u800cOCR RAG\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "motivation": "\u4f20\u7edfOCR\u5728\u590d\u6742\u6587\u6863\u4e2d\u53ef\u80fd\u5f15\u5165\u9519\u8bef\uff0c\u89c6\u89c9\u8bed\u8a00\u65b9\u6cd5\uff08\u5982ColPali\uff09\u76f4\u63a5\u5d4c\u5165\u6587\u6863\u56fe\u50cf\uff0c\u907f\u514dOCR\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83\u89c6\u89c9RAG\u4e0eOCR RAG\u7684\u6027\u80fd\uff0c\u5f15\u5165\u8bed\u4e49\u7b54\u6848\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u89c6\u89c9RAG\u5728\u7279\u5b9a\u6587\u6863\u4e0a\u8868\u73b0\u597d\uff0cOCR RAG\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff1b\u8ba1\u7b97\u6548\u7387\u4e0e\u8bed\u4e49\u51c6\u786e\u6027\u9700\u6743\u8861\u3002", "conclusion": "\u4e3aRAG\u5b9e\u8df5\u8005\u63d0\u4f9b\u9009\u62e9OCR\u6216\u89c6\u89c9\u57fa\u7840\u7cfb\u7edf\u7684\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2505.05672", "pdf": "https://arxiv.org/pdf/2505.05672", "abs": "https://arxiv.org/abs/2505.05672", "authors": ["Gengyan Li", "Paulo Gotardo", "Timo Bolkart", "Stephan Garbin", "Kripasindhu Sarkar", "Abhimitra Meka", "Alexandros Lattas", "Thabo Beeler"], "title": "TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling", "categories": ["cs.CV", "I.3.7; I.3.5"], "comment": "10 pages, 9 figures, supplementary results found at:\n  https://syntec-research.github.io/UVGA/, to be published in SIGGRAPH 2025", "summary": "Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have\nrecently enabled animatable 3D head avatars that are rendered under arbitrary\nviewpoints with impressive photorealism. Today, such photoreal avatars are seen\nas a key component in emerging applications in telepresence, extended reality,\nand entertainment. Building a photoreal avatar requires estimating the complex\nnon-rigid motion of different facial components as seen in input video images;\ndue to inaccurate motion estimation, animatable models typically present a loss\nof fidelity and detail when compared to their non-animatable counterparts,\nbuilt from an individual facial expression. Also, recent state-of-the-art\nmodels are often affected by memory limitations that reduce the number of 3D\nGaussians used for modeling, leading to lower detail and quality. To address\nthese problems, we present a new high-detail 3D head avatar model that improves\nupon the state of the art, largely increasing the number of 3D Gaussians and\nmodeling quality for rendering at 4K resolution. Our high-quality model is\nreconstructed from multiview input video and builds on top of a mesh-based 3D\nmorphable model, which provides a coarse deformation layer for the head.\nPhotoreal appearance is modelled by 3D Gaussians embedded within the continuous\nUVD tangent space of this mesh, allowing for more effective densification where\nmost needed. Additionally, these Gaussians are warped by a novel UVD\ndeformation field to capture subtle, localized motion. Our key contribution is\nthe novel deformable Gaussian encoding and overall fitting procedure that\nallows our head model to preserve appearance detail, while capturing facial\nmotion and other transient high-frequency features such as skin wrinkling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684\u9ad8\u7ec6\u82823D\u5934\u90e8\u5934\u50cf\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u53d8\u5f62\u573a\u548c\u62df\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u753b\u4fdd\u771f\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u52a8\u753b\u6a21\u578b\u56e0\u8fd0\u52a8\u4f30\u8ba1\u4e0d\u51c6\u786e\u548c\u5185\u5b58\u9650\u5236\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u4fdd\u771f\u9700\u6c42\u3002", "method": "\u7ed3\u54083D\u53ef\u53d8\u5f62\u6a21\u578b\u548cUVD\u5207\u7ebf\u7a7a\u95f4\u4e2d\u7684\u9ad8\u65af\u6e85\u5c04\uff0c\u5f15\u5165\u65b0\u578bUVD\u53d8\u5f62\u573a\u4ee5\u6355\u6349\u5c40\u90e8\u8fd0\u52a8\u3002", "result": "\u6a21\u578b\u57284K\u5206\u8fa8\u7387\u4e0b\u6e32\u67d3\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u7ec6\u8282\u4fdd\u7559\u66f4\u4f18\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u753b\u6a21\u578b\u7ec6\u8282\u4e22\u5931\u95ee\u9898\uff0c\u4e3a\u9ad8\u4fdd\u771f\u5934\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.05678", "pdf": "https://arxiv.org/pdf/2505.05678", "abs": "https://arxiv.org/abs/2505.05678", "authors": ["Etai Sella", "Yanir Kleiman", "Hadar Averbuch-Elor"], "title": "InstanceGen: Image Generation with Instance-level Instructions", "categories": ["cs.CV"], "comment": "Project page: https://tau-vailab.github.io/InstanceGen/", "summary": "Despite rapid advancements in the capabilities of generative models,\npretrained text-to-image models still struggle in capturing the semantics\nconveyed by complex prompts that compound multiple objects and instance-level\nattributes. Consequently, we are witnessing growing interests in integrating\nadditional structural constraints, %leveraging additional structural inputs\ntypically in the form of coarse bounding boxes, to better guide the generation\nprocess in such challenging cases. In this work, we take the idea of structural\nguidance a step further by making the observation that contemporary image\ngeneration models can directly provide a plausible \\emph{fine-grained}\nstructural initialization. We propose a technique that couples this image-based\nstructural guidance with LLM-based instance-level instructions, yielding output\nimages that adhere to all parts of the text prompt, including object counts,\ninstance-level attributes, and spatial relations between instances.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u50cf\u751f\u6210\u6a21\u578b\u548cLLM\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u63d0\u793a\u4e0b\u8bed\u4e49\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u6355\u6349\u590d\u6742\u63d0\u793a\uff08\u5305\u542b\u591a\u4e2a\u5bf9\u8c61\u548c\u5b9e\u4f8b\u7ea7\u5c5e\u6027\uff09\u7684\u8bed\u4e49\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u989d\u5916\u7684\u7ed3\u6784\u7ea6\u675f\u3002", "method": "\u5229\u7528\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u521d\u59cb\u5316\uff0c\u7ed3\u5408LLM\u7684\u5b9e\u4f8b\u7ea7\u6307\u4ee4\uff0c\u751f\u6210\u7b26\u5408\u6587\u672c\u63d0\u793a\u7684\u56fe\u50cf\u3002", "result": "\u751f\u6210\u7684\u56fe\u50cf\u80fd\u591f\u51c6\u786e\u53cd\u6620\u6587\u672c\u63d0\u793a\u4e2d\u7684\u5bf9\u8c61\u6570\u91cf\u3001\u5b9e\u4f8b\u7ea7\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u7684\u7ed3\u6784\u5316\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63d0\u793a\u4e0b\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2505.05681", "pdf": "https://arxiv.org/pdf/2505.05681", "abs": "https://arxiv.org/abs/2505.05681", "authors": ["Giulio Cesare Mastrocinque Santo", "Patr\u00edcia Izar", "Irene Delval", "Victor de Napole Gregolin", "Nina S. T. Hirata"], "title": "Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos", "categories": ["cs.CV"], "comment": null, "summary": "Video recordings of nonhuman primates in their natural habitat are a common\nsource for studying their behavior in the wild. We fine-tune pre-trained\nvideo-text foundational models for the specific domain of capuchin monkeys,\nwith the goal of developing useful computational models to help researchers to\nretrieve useful clips from videos. We focus on the challenging problem of\ntraining a model based solely on raw, unlabeled video footage, using weak audio\ndescriptions sometimes provided by field collaborators. We leverage recent\nadvances in Multimodal Large Language Models (MLLMs) and Vision-Language Models\n(VLMs) to address the extremely noisy nature of both video and audio content.\nSpecifically, we propose a two-folded approach: an agentic data treatment\npipeline and a fine-tuning process. The data processing pipeline automatically\nextracts clean and semantically aligned video-text pairs from the raw videos,\nwhich are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP model\nthrough Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of\n$167\\%$ for the 16 frames model and an uplift of $114\\%$ for the 8 frame model\non our domain data. Moreover, based on $NDCG@K$ results, our model is able to\nrank well most of the considered behaviors, while the tested raw pre-trained\nmodels are not able to rank them at all. The code will be made available upon\nacceptance.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u89c6\u9891-\u6587\u672c\u57fa\u7840\u6a21\u578b\uff0c\u9488\u5bf9\u5377\u5c3e\u7334\u7684\u81ea\u7136\u884c\u4e3a\u89c6\u9891\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672a\u6807\u8bb0\u89c6\u9891\u548c\u5f31\u97f3\u9891\u63cf\u8ff0\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u91ce\u751f\u975e\u4eba\u7075\u957f\u7c7b\u52a8\u7269\u884c\u4e3a\u65f6\uff0c\u89c6\u9891\u662f\u91cd\u8981\u6570\u636e\u6e90\uff0c\u4f46\u7f3a\u4e4f\u6807\u8bb0\u6570\u636e\u4e14\u5185\u5bb9\u566a\u58f0\u5927\uff0c\u9700\u8981\u9ad8\u6548\u68c0\u7d22\u6709\u7528\u7247\u6bb5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4ee3\u7406\u6570\u636e\u5904\u7406\u7ba1\u9053\u4ece\u539f\u59cb\u89c6\u9891\u4e2d\u63d0\u53d6\u5e72\u51c0\u7684\u89c6\u9891-\u6587\u672c\u5bf9\uff1b2) \u4f7f\u7528LoRA\u5fae\u8c03\u9884\u8bad\u7ec3\u7684X-CLIP\u6a21\u578b\u3002", "result": "\u5728\u9886\u57df\u6570\u636e\u4e0a\uff0c16\u5e27\u6a21\u578bHits@5\u63d0\u5347167%\uff0c8\u5e27\u6a21\u578b\u63d0\u5347114%\uff1bNDCG@K\u7ed3\u679c\u663e\u793a\u6a21\u578b\u80fd\u6709\u6548\u6392\u5e8f\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u68c0\u7d22\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u566a\u58f0\u5927\u7684\u672a\u6807\u8bb0\u6570\u636e\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2505.05710", "pdf": "https://arxiv.org/pdf/2505.05710", "abs": "https://arxiv.org/abs/2505.05710", "authors": ["Wooyoung Jeong", "Hyun Jae Park", "Seonghun Jeong", "Jong Wook Jang", "Tae Hoon Lim", "Dae Seoung Kim"], "title": "HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Hyperspectral imagery provides rich spectral detail but poses unique\nchallenges because of its high dimensionality in both spatial and spectral\ndomains. We propose \\textit{HyperspectralMAE}, a Transformer-based foundation\nmodel for hyperspectral data that employs a \\textit{dual masking} strategy:\nduring pre-training we randomly occlude 50\\% of spatial patches and 50\\% of\nspectral bands. This forces the model to learn representations capable of\nreconstructing missing information across both dimensions. To encode spectral\norder, we introduce learnable harmonic Fourier positional embeddings based on\nwavelength. The reconstruction objective combines mean-squared error (MSE) with\nthe spectral angle mapper (SAM) to balance pixel-level accuracy and\nspectral-shape fidelity.\n  The resulting model contains about $1.8\\times10^{8}$ parameters and produces\n768-dimensional embeddings, giving it sufficient capacity for transfer\nlearning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --\nNASA EO-1 Hyperion ($\\sim$1\\,600 scenes, $\\sim$$3\\times10^{11}$ pixel spectra)\nand DLR EnMAP Level-0 ($\\sim$1\\,300 scenes, $\\sim$$3\\times10^{11}$ pixel\nspectra) -- and fine-tuned it for land-cover classification on the Indian Pines\nbenchmark. HyperspectralMAE achieves state-of-the-art transfer-learning\naccuracy on Indian Pines, confirming that masked dual-dimensional pre-training\nyields robust spectral-spatial representations. These results demonstrate that\ndual masking and wavelength-aware embeddings advance hyperspectral image\nreconstruction and downstream analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u53cc\u63a9\u7801\u9884\u8bad\u7ec3\u6a21\u578bHyperspectralMAE\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u6570\u636e\u7684\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u5149\u8c31\u7ef4\u5ea6\u7684\u53cc\u91cd\u63a9\u7801\u7b56\u7565\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u9ad8\u5149\u8c31\u6570\u636e\u7684\u9ad8\u7ef4\u7279\u6027\u5e26\u6765\u4e86\u72ec\u7279\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u7a7a\u95f4\u548c\u5149\u8c31\u4fe1\u606f\u7684\u5f3a\u5927\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cc\u63a9\u7801\u7b56\u7565\uff0850%\u7a7a\u95f4\u5757\u548c50%\u5149\u8c31\u5e26\u63a9\u7801\uff09\uff0c\u7ed3\u5408\u8c10\u6ce2\u5085\u91cc\u53f6\u4f4d\u7f6e\u5d4c\u5165\u548cMSE+SAM\u91cd\u5efa\u76ee\u6807\uff0c\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u3002", "result": "\u5728Indian Pines\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8fc1\u79fb\u5b66\u4e60\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u53cc\u63a9\u7801\u9884\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u53cc\u63a9\u7801\u548c\u6ce2\u957f\u611f\u77e5\u5d4c\u5165\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u91cd\u5efa\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2505.05711", "pdf": "https://arxiv.org/pdf/2505.05711", "abs": "https://arxiv.org/abs/2505.05711", "authors": ["Ho-Joong Kim", "Yearang Lee", "Jung-Ho Hong", "Seong-Whan Lee"], "title": "DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "In this paper, we examine a key limitation in query-based detectors for\ntemporal action detection (TAD), which arises from their direct adaptation of\noriginally designed architectures for object detection. Despite the\neffectiveness of the existing models, they struggle to fully address the unique\nchallenges of TAD, such as the redundancy in multi-scale features and the\nlimited ability to capture sufficient temporal context. To address these\nissues, we propose a multi-dilated gated encoder and central-adjacent region\nintegrated decoder for temporal action detection transformer (DiGIT). Our\napproach replaces the existing encoder that consists of multi-scale deformable\nattention and feedforward network with our multi-dilated gated encoder. Our\nproposed encoder reduces the redundant information caused by multi-level\nfeatures while maintaining the ability to capture fine-grained and long-range\ntemporal information. Furthermore, we introduce a central-adjacent region\nintegrated decoder that leverages a more comprehensive sampling strategy for\ndeformable cross-attention to capture the essential information. Extensive\nexperiments demonstrate that DiGIT achieves state-of-the-art performance on\nTHUMOS14, ActivityNet v1.3, and HACS-Segment. Code is available at:\nhttps://github.com/Dotori-HJ/DiGIT", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiGIT\u7684\u65f6\u5e8f\u52a8\u4f5c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6269\u5f20\u95e8\u63a7\u7f16\u7801\u5668\u548c\u4e2d\u5fc3-\u90bb\u57df\u96c6\u6210\u89e3\u7801\u5668\u89e3\u51b3\u4e86\u73b0\u6709\u67e5\u8be2\u5f0f\u68c0\u6d4b\u5668\u7684\u5197\u4f59\u548c\u4e0a\u4e0b\u6587\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u67e5\u8be2\u7684\u65f6\u5e8f\u52a8\u4f5c\u68c0\u6d4b\u5668\u76f4\u63a5\u6cbf\u7528\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\uff0c\u5bfc\u81f4\u591a\u5c3a\u5ea6\u7279\u5f81\u5197\u4f59\u548c\u65f6\u5e8f\u4e0a\u4e0b\u6587\u6355\u6349\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u591a\u6269\u5f20\u95e8\u63a7\u7f16\u7801\u5668\u548c\u4e2d\u5fc3-\u90bb\u57df\u96c6\u6210\u89e3\u7801\u5668\uff0c\u524d\u8005\u51cf\u5c11\u5197\u4f59\u4fe1\u606f\uff0c\u540e\u8005\u901a\u8fc7\u66f4\u5168\u9762\u7684\u91c7\u6837\u7b56\u7565\u6355\u6349\u5173\u952e\u4fe1\u606f\u3002", "result": "DiGIT\u5728THUMOS14\u3001ActivityNet v1.3\u548cHACS-Segment\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DiGIT\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u52a8\u4f5c\u68c0\u6d4b\u4e2d\u7684\u5197\u4f59\u548c\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2505.05721", "pdf": "https://arxiv.org/pdf/2505.05721", "abs": "https://arxiv.org/abs/2505.05721", "authors": ["Zixuan Li", "Lei Meng", "Guoqing Chao", "Wei Wu", "Xiaoshuo Yan", "Yimeng Yang", "Zhuang Qi", "Xiangxu Meng"], "title": "Semantic-Space-Intervened Diffusive Alignment for Visual Classification", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal alignment is an effective approach to improving visual\nclassification. Existing studies typically enforce a one-step mapping that uses\ndeep neural networks to project the visual features to mimic the distribution\nof textual features. However, they typically face difficulties in finding such\na projection due to the two modalities in both the distribution of class-wise\nsamples and the range of their feature values. To address this issue, this\npaper proposes a novel Semantic-Space-Intervened Diffusive Alignment method,\ntermed SeDA, models a semantic space as a bridge in the visual-to-textual\nprojection, considering both types of features share the same class-level\ninformation in classification. More importantly, a bi-stage diffusion framework\nis developed to enable the progressive alignment between the two modalities.\nSpecifically, SeDA first employs a Diffusion-Controlled Semantic Learner to\nmodel the semantic features space of visual features by constraining the\ninteractive features of the diffusion model and the category centers of visual\nfeatures. In the later stage of SeDA, the Diffusion-Controlled Semantic\nTranslator focuses on learning the distribution of textual features from the\nsemantic space. Meanwhile, the Progressive Feature Interaction Network\nintroduces stepwise feature interactions at each alignment step, progressively\nintegrating textual information into mapped features. Experimental results show\nthat SeDA achieves stronger cross-modal feature alignment, leading to superior\nperformance over existing methods across multiple scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSeDA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u7a7a\u95f4\u5e72\u9884\u7684\u6269\u6563\u5bf9\u9f50\u6280\u672f\uff0c\u9010\u6b65\u5b9e\u73b0\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u7684\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u7279\u5f81\u5230\u6587\u672c\u7279\u5f81\u7684\u4e00\u6b65\u6620\u5c04\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8e\u4e24\u79cd\u6a21\u6001\u5728\u6837\u672c\u5206\u5e03\u548c\u7279\u5f81\u503c\u8303\u56f4\u4e0a\u7684\u5dee\u5f02\u3002", "method": "SeDA\u91c7\u7528\u53cc\u9636\u6bb5\u6269\u6563\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u6269\u6563\u63a7\u5236\u8bed\u4e49\u5b66\u4e60\u5668\u5efa\u6a21\u89c6\u89c9\u7279\u5f81\u7684\u8bed\u4e49\u7a7a\u95f4\uff0c\u7136\u540e\u901a\u8fc7\u6269\u6563\u63a7\u5236\u8bed\u4e49\u7ffb\u8bd1\u5668\u5b66\u4e60\u6587\u672c\u7279\u5f81\u7684\u5206\u5e03\uff0c\u5e76\u7ed3\u5408\u9010\u6b65\u7279\u5f81\u4ea4\u4e92\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSeDA\u5728\u591a\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SeDA\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5bf9\u9f50\u548c\u8bed\u4e49\u7a7a\u95f4\u5e72\u9884\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u6548\u679c\u3002"}}
{"id": "2505.05722", "pdf": "https://arxiv.org/pdf/2505.05722", "abs": "https://arxiv.org/abs/2505.05722", "authors": ["Valay Bundele", "Mehran Hosseinzadeh", "Hendrik Lensch"], "title": "You Are Your Best Teacher: Semi-Supervised Surgical Point Tracking with Cycle-Consistent Self-Distillation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 SynData4CV Workshop", "summary": "Synthetic datasets have enabled significant progress in point tracking by\nproviding large-scale, densely annotated supervision. However, deploying these\nmodels in real-world domains remains challenging due to domain shift and lack\nof labeled data-issues that are especially severe in surgical videos, where\nscenes exhibit complex tissue deformation, occlusion, and lighting variation.\nWhile recent approaches adapt synthetic-trained trackers to natural videos\nusing teacher ensembles or augmentation-heavy pseudo-labeling pipelines, their\neffectiveness in high-shift domains like surgery remains unexplored. This work\npresents SurgTracker, a semi-supervised framework for adapting\nsynthetic-trained point trackers to surgical video using filtered\nself-distillation. Pseudo-labels are generated online by a fixed\nteacher-identical in architecture and initialization to the student-and are\nfiltered using a cycle consistency constraint to discard temporally\ninconsistent trajectories. This simple yet effective design enforces geometric\nconsistency and provides stable supervision throughout training, without the\ncomputational overhead of maintaining multiple teachers. Experiments on the\nSTIR benchmark show that SurgTracker improves tracking performance using only\n80 unlabeled videos, demonstrating its potential for robust adaptation in\nhigh-shift, data-scarce domains.", "AI": {"tldr": "SurgTracker\u662f\u4e00\u79cd\u534a\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u65b9\u6cd5\u5c06\u5408\u6210\u8bad\u7ec3\u7684\u70b9\u8ffd\u8e2a\u5668\u9002\u5e94\u4e8e\u624b\u672f\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u9886\u57df\u504f\u79fb\u548c\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5408\u6210\u6570\u636e\u96c6\u5728\u70b9\u8ffd\u8e2a\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u5b9e\u9645\u624b\u672f\u89c6\u9891\u4e2d\u90e8\u7f72\u65f6\uff0c\u7531\u4e8e\u590d\u6742\u7684\u7ec4\u7ec7\u53d8\u5f62\u3001\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\uff0c\u9886\u57df\u504f\u79fb\u548c\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u6210\u4e3a\u6311\u6218\u3002", "method": "SurgTracker\u91c7\u7528\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u6559\u5e08\u6a21\u578b\u5728\u7ebf\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u5e76\u5229\u7528\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u8fc7\u6ee4\u4e0d\u4e00\u81f4\u7684\u8f68\u8ff9\uff0c\u786e\u4fdd\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u5728STIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSurgTracker\u4ec5\u4f7f\u752880\u4e2a\u672a\u6807\u8bb0\u89c6\u9891\u5c31\u663e\u8457\u63d0\u5347\u4e86\u8ffd\u8e2a\u6027\u80fd\u3002", "conclusion": "SurgTracker\u5c55\u793a\u4e86\u5728\u9ad8\u504f\u79fb\u3001\u6570\u636e\u7a00\u7f3a\u9886\u57df\u4e2d\u7a33\u5065\u9002\u5e94\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05741", "pdf": "https://arxiv.org/pdf/2505.05741", "abs": "https://arxiv.org/abs/2505.05741", "authors": ["Zhangchi Hu", "Peixi Wu", "Jie Chen", "Huyue Zhu", "Yijun Wang", "Yansong Peng", "Hebei Li", "Xiaoyan Sun"], "title": "Dome-DETR: DETR with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Tiny object detection plays a vital role in drone surveillance, remote\nsensing, and autonomous systems, enabling the identification of small targets\nacross vast landscapes. However, existing methods suffer from inefficient\nfeature leverage and high computational costs due to redundant feature\nprocessing and rigid query allocation. To address these challenges, we propose\nDome-DETR, a novel framework with Density-Oriented Feature-Query Manipulation\nfor Efficient Tiny Object Detection. To reduce feature redundancies, we\nintroduce a lightweight Density-Focal Extractor (DeFE) to produce clustered\ncompact foreground masks. Leveraging these masks, we incorporate Masked Window\nAttention Sparsification (MWAS) to focus computational resources on the most\ninformative regions via sparse attention. Besides, we propose Progressive\nAdaptive Query Initialization (PAQI), which adaptively modulates query density\nacross spatial areas for better query allocation. Extensive experiments\ndemonstrate that Dome-DETR achieves state-of-the-art performance (+3.3 AP on\nAI-TOD-V2 and +2.5 AP on VisDrone) while maintaining low computational\ncomplexity and a compact model size. Code will be released upon acceptance.", "AI": {"tldr": "Dome-DETR\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5c0f\u7269\u4f53\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5bc6\u5ea6\u5bfc\u5411\u7684\u7279\u5f81\u67e5\u8be2\u64cd\u4f5c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c0f\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u5229\u7528\u6548\u7387\u4f4e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0cDome-DETR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528DeFE\u751f\u6210\u7d27\u51d1\u524d\u666f\u63a9\u7801\uff0c\u7ed3\u5408MWAS\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548cPAQI\u81ea\u9002\u5e94\u67e5\u8be2\u5206\u914d\u3002", "result": "\u5728AI-TOD-V2\u548cVisDrone\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u53473.3 AP\u548c2.5 AP\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "Dome-DETR\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u76d1\u63a7\u7b49\u573a\u666f\u3002"}}
{"id": "2505.05748", "pdf": "https://arxiv.org/pdf/2505.05748", "abs": "https://arxiv.org/abs/2505.05748", "authors": ["Huan Yan", "Junjie Hu"], "title": "kFuse: A novel density based agglomerative clustering", "categories": ["cs.CV"], "comment": "13 pages, 11 figures", "summary": "Agglomerative clustering has emerged as a vital tool in data analysis due to\nits intuitive and flexible characteristics. However, existing agglomerative\nclustering methods often involve additional parameters for sub-cluster\npartitioning and inter-cluster similarity assessment. This necessitates\ndifferent parameter settings across various datasets, which is undoubtedly\nchallenging in the absence of prior knowledge. Moreover, existing agglomerative\nclustering techniques are constrained by the calculation method of connection\ndistance, leading to unstable clustering results. To address these issues, this\npaper introduces a novel density-based agglomerative clustering method, termed\nkFuse. kFuse comprises four key components: (1) sub-cluster partitioning based\non natural neighbors; (2) determination of boundary connectivity between\nsub-clusters through the computation of adjacent samples and shortest\ndistances; (3) assessment of density similarity between sub-clusters via the\ncalculation of mean density and variance; and (4) establishment of merging\nrules between sub-clusters based on boundary connectivity and density\nsimilarity. kFuse requires the specification of the number of clusters only at\nthe final merging stage. Additionally, by comprehensively considering adjacent\nsamples, distances, and densities among different sub-clusters, kFuse\nsignificantly enhances accuracy during the merging phase, thereby greatly\nimproving its identification capability. Experimental results on both synthetic\nand real-world datasets validate the effectiveness of kFuse.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u7684\u51dd\u805a\u805a\u7c7b\u65b9\u6cd5kFuse\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u53c2\u6570\u4f9d\u8d56\u548c\u7ed3\u679c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u51dd\u805a\u805a\u7c7b\u65b9\u6cd5\u9700\u8981\u989d\u5916\u53c2\u6570\u4e14\u7ed3\u679c\u4e0d\u7a33\u5b9a\uff0c\u7f3a\u4e4f\u5148\u9a8c\u77e5\u8bc6\u65f6\u96be\u4ee5\u9002\u7528\u3002", "method": "kFuse\u901a\u8fc7\u81ea\u7136\u90bb\u5c45\u5212\u5206\u5b50\u7c07\uff0c\u8ba1\u7b97\u8fb9\u754c\u8fde\u63a5\u6027\u548c\u5bc6\u5ea6\u76f8\u4f3c\u6027\uff0c\u5e76\u5236\u5b9a\u5408\u5e76\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86kFuse\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "kFuse\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u51c6\u786e\u6027\u548c\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2505.05752", "pdf": "https://arxiv.org/pdf/2505.05752", "abs": "https://arxiv.org/abs/2505.05752", "authors": ["Amin Ghafourian", "Andrew Lee", "Dechen Gao", "Tyler Beer", "Kin Yen", "Iman Soltani"], "title": "Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data", "categories": ["cs.CV", "cs.CY", "cs.LG", "cs.RO", "eess.IV"], "comment": "19 pages, 15 figures, 4 tables", "summary": "Automation can play a prominent role in improving efficiency, accuracy, and\nscalability in infrastructure surveying and assessing construction and\ncompliance standards. This paper presents a framework for automation of\ngeometric measurements and compliance assessment using point cloud data. The\nproposed approach integrates deep learning-based detection and segmentation, in\nconjunction with geometric and signal processing techniques, to automate\nsurveying tasks. As a proof of concept, we apply this framework to\nautomatically evaluate the compliance of curb ramps with the Americans with\nDisabilities Act (ADA), demonstrating the utility of point cloud data in survey\nautomation. The method leverages a newly collected, large annotated dataset of\ncurb ramps, made publicly available as part of this work, to facilitate robust\nmodel training and evaluation. Experimental results, including comparison with\nmanual field measurements of several ramps, validate the accuracy and\nreliability of the proposed method, highlighting its potential to significantly\nreduce manual effort and improve consistency in infrastructure assessment.\nBeyond ADA compliance, the proposed framework lays the groundwork for broader\napplications in infrastructure surveying and automated construction evaluation,\npromoting wider adoption of point cloud data in these domains. The annotated\ndatabase, manual ramp survey data, and developed algorithms are publicly\navailable on the project's GitHub page:\nhttps://github.com/Soltanilara/SurveyAutomation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u6570\u636e\u7684\u81ea\u52a8\u5316\u51e0\u4f55\u6d4b\u91cf\u548c\u5408\u89c4\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u51e0\u4f55\u4fe1\u53f7\u5904\u7406\u6280\u672f\uff0c\u7528\u4e8e\u57fa\u7840\u8bbe\u65bd\u8c03\u67e5\u3002\u4ee5ADA\u5408\u89c4\u6027\u4e3a\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u5728\u57fa\u7840\u8bbe\u65bd\u8c03\u67e5\u4e2d\u53ef\u63d0\u9ad8\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u70b9\u4e91\u6570\u636e\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u4e0e\u5206\u5272\u3001\u51e0\u4f55\u548c\u4fe1\u53f7\u5904\u7406\u6280\u672f\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u8c03\u67e5\u6846\u67b6\uff0c\u5e76\u5e94\u7528\u4e8eADA\u5408\u89c4\u6027\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u5e76\u63d0\u9ad8\u4e86\u8bc4\u4f30\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u57fa\u7840\u8bbe\u65bd\u8c03\u67e5\u548c\u81ea\u52a8\u5316\u5efa\u7b51\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u70b9\u4e91\u6570\u636e\u5728\u8fd9\u4e9b\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2505.05759", "pdf": "https://arxiv.org/pdf/2505.05759", "abs": "https://arxiv.org/abs/2505.05759", "authors": ["Fangxue Liu", "Lei Fan"], "title": "A review of advancements in low-light image enhancement using deep learning", "categories": ["cs.CV"], "comment": null, "summary": "In low-light environments, the performance of computer vision algorithms\noften deteriorates significantly, adversely affecting key vision tasks such as\nsegmentation, detection, and classification. With the rapid advancement of deep\nlearning, its application to low-light image processing has attracted\nwidespread attention and seen significant progress in recent years. However,\nthere remains a lack of comprehensive surveys that systematically examine how\nrecent deep-learning-based low-light image enhancement methods function and\nevaluate their effectiveness in enhancing downstream vison tasks. To address\nthis gap, this review provides a detailed elaboration on how various recent\napproaches (from 2020) operate and their enhancement mechanisms, supplemented\nwith clear illustrations. It also investigates the impact of different\nenhancement techniques on subsequent vision tasks, critically analyzing their\nstrengths and limitations. Additionally, it proposes future research\ndirections. This review serves as a useful reference for determining low-light\nimage enhancement techniques and optimizing vision task performance in\nlow-light conditions.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e862020\u5e74\u4ee5\u6765\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5176\u673a\u5236\u53ca\u5bf9\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u6027\u80fd\u4e0b\u964d\uff0c\u7f3a\u4e4f\u5bf9\u6df1\u5ea6\u5b66\u4e60\u589e\u5f3a\u65b9\u6cd5\u7684\u7cfb\u7edf\u7efc\u8ff0\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8be6\u7ec6\u9610\u8ff02020\u5e74\u4ee5\u6765\u5404\u79cd\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u7684\u64cd\u4f5c\u673a\u5236\uff0c\u5e76\u8f85\u4ee5\u56fe\u793a\u8bf4\u660e\u3002", "result": "\u5206\u6790\u4e86\u4e0d\u540c\u589e\u5f3a\u6280\u672f\u5bf9\u540e\u7eed\u89c6\u89c9\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u672c\u6587\u4e3a\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u9009\u62e9\u548c\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.05804", "pdf": "https://arxiv.org/pdf/2505.05804", "abs": "https://arxiv.org/abs/2505.05804", "authors": ["Xi Xiao", "Yunbei Zhang", "Thanh-Huy Nguyen", "Ba-Thinh Lam", "Janet Wang", "Jihun Hamm", "Tianyang Wang", "Xingjian Li", "Xiao Wang", "Hao Xu", "Tianming Liu", "Min Xu"], "title": "Describe Anything in Medical Images", "categories": ["cs.CV"], "comment": null, "summary": "Localized image captioning has made significant progress with models like the\nDescribe Anything Model (DAM), which can generate detailed region-specific\ndescriptions without explicit region-text supervision. However, such\ncapabilities have yet to be widely applied to specialized domains like medical\nimaging, where diagnostic interpretation relies on subtle regional findings\nrather than global understanding. To mitigate this gap, we propose MedDAM, the\nfirst comprehensive framework leveraging large vision-language models for\nregion-specific captioning in medical images. MedDAM employs medical\nexpert-designed prompts tailored to specific imaging modalities and establishes\na robust evaluation benchmark comprising a customized assessment protocol, data\npre-processing pipeline, and specialized QA template library. This benchmark\nevaluates both MedDAM and other adaptable large vision-language models,\nfocusing on clinical factuality through attribute-level verification tasks,\nthereby circumventing the absence of ground-truth region-caption pairs in\nmedical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and\nSkinCon datasets demonstrate MedDAM's superiority over leading peers (including\nGPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and\nOMG-LLaVA) in the task, revealing the importance of region-level semantic\nalignment in medical image understanding and establishing MedDAM as a promising\nfoundation for clinical vision-language integration.", "AI": {"tldr": "MedDAM\u662f\u4e00\u4e2a\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u7684\u5c40\u90e8\u63cf\u8ff0\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u4e2d\u533a\u57df\u7279\u5f02\u6027\u63cf\u8ff0\u751f\u6210\u7684\u9700\u6c42\uff0c\u5f25\u8865\u73b0\u6709\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u533b\u5b66\u4e13\u5bb6\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u548cQA\u6a21\u677f\u5e93\uff0c\u9a8c\u8bc1\u4e34\u5e8a\u4e8b\u5b9e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eGPT-4o\u7b49\u9886\u5148\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u533a\u57df\u7ea7\u8bed\u4e49\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MedDAM\u4e3a\u4e34\u5e8a\u89c6\u89c9\u8bed\u8a00\u96c6\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002"}}
{"id": "2505.05806", "pdf": "https://arxiv.org/pdf/2505.05806", "abs": "https://arxiv.org/abs/2505.05806", "authors": ["Kaili Qi", "Wenli Yang", "Ye Li", "Zhongyi Huang"], "title": "Image Segmentation via Variational Model Based Tailored UNet: A Deep Variational Framework", "categories": ["cs.CV"], "comment": null, "summary": "Traditional image segmentation methods, such as variational models based on\npartial differential equations (PDEs), offer strong mathematical\ninterpretability and precise boundary modeling, but often suffer from\nsensitivity to parameter settings and high computational costs. In contrast,\ndeep learning models such as UNet, which are relatively lightweight in\nparameters, excel in automatic feature extraction but lack theoretical\ninterpretability and require extensive labeled data. To harness the\ncomplementary strengths of both paradigms, we propose Variational Model Based\nTailored UNet (VM_TUNet), a novel hybrid framework that integrates the\nfourth-order modified Cahn-Hilliard equation with the deep learning backbone of\nUNet, which combines the interpretability and edge-preserving properties of\nvariational methods with the adaptive feature learning of neural networks.\nSpecifically, a data-driven operator is introduced to replace manual parameter\ntuning, and we incorporate the tailored finite point method (TFPM) to enforce\nhigh-precision boundary preservation. Experimental results on benchmark\ndatasets demonstrate that VM_TUNet achieves superior segmentation performance\ncompared to existing approaches, especially for fine boundary delineation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u6a21\u578b\u548cUNet\u7684\u6df7\u5408\u6846\u67b6VM_TUNet\uff0c\u517c\u5177\u6570\u5b66\u53ef\u89e3\u91ca\u6027\u548c\u81ea\u9002\u5e94\u7279\u5f81\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u53d8\u5206\u6a21\u578b\u6570\u5b66\u53ef\u89e3\u91ca\u6027\u5f3a\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982UNet\u8f7b\u91cf\u4f46\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\u6027\uff0c\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u5c06\u56db\u9636\u4fee\u6b63Cahn-Hilliard\u65b9\u7a0b\u4e0eUNet\u7ed3\u5408\uff0c\u5f15\u5165\u6570\u636e\u9a71\u52a8\u7b97\u5b50\u66ff\u4ee3\u624b\u52a8\u8c03\u53c2\uff0c\u91c7\u7528TFPM\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\u8fb9\u754c\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u7cbe\u7ec6\u8fb9\u754c\u5206\u5272\u4e0a\u3002", "conclusion": "VM_TUNet\u6210\u529f\u7ed3\u5408\u4e86\u53d8\u5206\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2505.05829", "pdf": "https://arxiv.org/pdf/2505.05829", "abs": "https://arxiv.org/abs/2505.05829", "authors": ["Zhiyuan Chen", "Keyi Li", "Yifan Jia", "Le Ye", "Yufei Ma"], "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "accepted by CVPR2025", "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684DiT\u52a0\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u91cf\u6821\u51c6\u7f13\u5b58\u548c\u901a\u9053\u611f\u77e5SVD\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u90e8\u7f72\u6548\u7387\uff0c\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u7f3a\u4e4f\u6821\u6b63\u53ef\u80fd\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u589e\u91cf\u6821\u51c6\u7f13\u5b58\u548c\u901a\u9053\u611f\u77e5SVD\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u6821\u51c6\u53c2\u6570\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5728\u76f8\u4f3c\u8ba1\u7b97\u8d44\u6e90\u4e0b\u4f18\u4e8e\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\uff0c\u51cf\u5c1145%\u8ba1\u7b97\u91cf\uff0cIS\u63d0\u534712\uff0cFID\u4ec5\u589e\u52a00.06\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86DiT\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.05834", "pdf": "https://arxiv.org/pdf/2505.05834", "abs": "https://arxiv.org/abs/2505.05834", "authors": ["Chunlai Dong", "Haochao Ying", "Qibo Qiu", "Jinhong Wang", "Danny Chen", "Jian Wu"], "title": "Dual-level Fuzzy Learning with Patch Guidance for Image Ordinal Regression", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Ordinal regression bridges regression and classification by assigning objects\nto ordered classes. While human experts rely on discriminative patch-level\nfeatures for decisions, current approaches are limited by the availability of\nonly image-level ordinal labels, overlooking fine-grained patch-level\ncharacteristics. In this paper, we propose a Dual-level Fuzzy Learning with\nPatch Guidance framework, named DFPG that learns precise feature-based grading\nboundaries from ambiguous ordinal labels, with patch-level supervision.\nSpecifically, we propose patch-labeling and filtering strategies to enable the\nmodel to focus on patch-level features exclusively with only image-level\nordinal labels available. We further design a dual-level fuzzy learning module,\nwhich leverages fuzzy logic to quantitatively capture and handle label\nambiguity from both patch-wise and channel-wise perspectives. Extensive\nexperiments on various image ordinal regression datasets demonstrate the\nsuperiority of our proposed method, further confirming its ability in\ndistinguishing samples from difficult-to-classify categories. The code is\navailable at https://github.com/ZJUMAI/DFPG-ord.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDFPG\u7684\u53cc\u5c42\u6b21\u6a21\u7cca\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8865\u4e01\u5f15\u5bfc\u4ece\u6a21\u7cca\u7684\u5e8f\u6570\u6807\u7b7e\u4e2d\u5b66\u4e60\u7cbe\u786e\u7684\u7279\u5f81\u5206\u7ea7\u8fb9\u754c\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7ea7\u5e8f\u6570\u6807\u7b7e\uff0c\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u7684\u8865\u4e01\u7ea7\u7279\u5f81\uff0c\u800c\u4eba\u7c7b\u4e13\u5bb6\u4f9d\u8d56\u8865\u4e01\u7ea7\u7279\u5f81\u8fdb\u884c\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u8865\u4e01\u6807\u8bb0\u548c\u8fc7\u6ee4\u7b56\u7565\uff0c\u8bbe\u8ba1\u53cc\u5c42\u6b21\u6a21\u7cca\u5b66\u4e60\u6a21\u5757\uff0c\u4ece\u8865\u4e01\u548c\u901a\u9053\u89d2\u5ea6\u5904\u7406\u6807\u7b7e\u6a21\u7cca\u6027\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u5e8f\u6570\u56de\u5f52\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u96be\u5206\u7c7b\u6837\u672c\u533a\u5206\u4e0a\u3002", "conclusion": "DFPG\u6846\u67b6\u80fd\u6709\u6548\u5229\u7528\u8865\u4e01\u7ea7\u7279\u5f81\uff0c\u63d0\u5347\u5e8f\u6570\u56de\u5f52\u6027\u80fd\u3002"}}
{"id": "2505.05845", "pdf": "https://arxiv.org/pdf/2505.05845", "abs": "https://arxiv.org/abs/2505.05845", "authors": ["Guohao Lin", "Shidong Pan", "Rasul Khanbayov", "Changxi Yang", "Ani Khaloian-Sarnaghi", "Andriy Kovryga"], "title": "Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Knots in wood are critical to both aesthetics and structural integrity,\nmaking their detection and pairing essential in timber processing. However,\ntraditional manual annotation was labor-intensive and inefficient,\nnecessitating automation. This paper proposes a lightweight and fully automated\npipeline for knot detection and pairing based on machine learning techniques.\nIn the detection stage, high-resolution surface images of wooden boards were\ncollected using industrial-grade cameras, and a large-scale dataset was\nmanually annotated and preprocessed. After the transfer learning, the YOLOv8l\nachieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were\nanalyzed and paired based on multidimensional feature extraction. A triplet\nneural network was used to map the features into a latent space, enabling\nclustering algorithms to identify and pair corresponding knots. The triplet\nnetwork with learnable weights achieved a pairing accuracy of 0.85. Further\nanalysis revealed that he distances from the knot's start and end points to the\nbottom of the wooden board, and the longitudinal coordinates play crucial roles\nin achieving high pairing accuracy. Our experiments validate the effectiveness\nof the proposed solution, demonstrating the potential of AI in advancing wood\nscience and industry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u6728\u6750\u4e2d\u7ed3\u8282\u7684\u68c0\u6d4b\u4e0e\u914d\u5bf9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u6728\u6750\u4e2d\u7684\u7ed3\u8282\u5bf9\u7f8e\u89c2\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u624b\u52a8\u6807\u6ce8\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5de5\u4e1a\u7ea7\u76f8\u673a\u91c7\u96c6\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u6784\u5efa\u6570\u636e\u96c6\u5e76\u901a\u8fc7YOLOv8l\u8fdb\u884c\u68c0\u6d4b\uff1b\u91c7\u7528\u4e09\u5143\u7ec4\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u7ef4\u7279\u5f81\u63d0\u53d6\u5b9e\u73b0\u7ed3\u8282\u914d\u5bf9\u3002", "result": "\u68c0\u6d4b\u9636\u6bb5mAP@0.5\u8fbe0.887\uff0c\u914d\u5bf9\u9636\u6bb5\u51c6\u786e\u7387\u4e3a0.85\uff1b\u7ed3\u8282\u8d77\u59cb\u70b9\u548c\u7ec8\u70b9\u7684\u4f4d\u7f6e\u4fe1\u606f\u5bf9\u914d\u5bf9\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86AI\u5728\u6728\u6750\u79d1\u5b66\u4e0e\u5de5\u4e1a\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05848", "pdf": "https://arxiv.org/pdf/2505.05848", "abs": "https://arxiv.org/abs/2505.05848", "authors": ["Yue Yin", "Enze Tao", "Weijian Deng", "Dylan Campbell"], "title": "RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects", "categories": ["cs.CV"], "comment": null, "summary": "Modern 3D reconstruction and novel view synthesis approaches have\ndemonstrated strong performance on scenes with opaque Lambertian objects.\nHowever, most assume straight light paths and therefore cannot properly handle\nrefractive and reflective materials. Moreover, datasets specialized for these\neffects are limited, stymieing efforts to evaluate performance and develop\nsuitable techniques. In this work, we introduce a synthetic RefRef dataset and\nbenchmark for reconstructing scenes with refractive and reflective objects from\nposed images. Our dataset has 50 such objects of varying complexity, from\nsingle-material convex shapes to multi-material non-convex shapes, each placed\nin three different background types, resulting in 150 scenes. We also propose\nan oracle method that, given the object geometry and refractive indices,\ncalculates accurate light paths for neural rendering, and an approach based on\nthis that avoids these assumptions. We benchmark these against several\nstate-of-the-art methods and show that all methods lag significantly behind the\noracle, highlighting the challenges of the task and dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RefRef\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u4ece\u59ff\u6001\u56fe\u50cf\u91cd\u5efa\u5305\u542b\u6298\u5c04\u548c\u53cd\u5c04\u7269\u4f53\u7684\u573a\u666f\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u67093D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u5728\u5904\u7406\u6298\u5c04\u548c\u53cd\u5c04\u6750\u6599\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u5408\u6210RefRef\u6570\u636e\u96c6\uff0c\u5305\u542b150\u4e2a\u573a\u666f\uff1b\u63d0\u51fa\u57fa\u4e8e\u51e0\u4f55\u548c\u6298\u5c04\u7387\u7684oracle\u65b9\u6cd5\u53ca\u65e0\u9700\u8fd9\u4e9b\u5047\u8bbe\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u5728RefRef\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u663e\u8457\u843d\u540e\u4e8eoracle\u65b9\u6cd5\u3002", "conclusion": "\u6298\u5c04\u548c\u53cd\u5c04\u573a\u666f\u7684\u91cd\u5efa\u4ecd\u5177\u6311\u6218\u6027\uff0cRefRef\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2505.05853", "pdf": "https://arxiv.org/pdf/2505.05853", "abs": "https://arxiv.org/abs/2505.05853", "authors": ["Tongda Xu", "Jiahao Li", "Bin Li", "Yan Wang", "Ya-Qin Zhang", "Yan Lu"], "title": "PICD: Versatile Perceptual Image Compression with Diffusion Rendering", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recently, perceptual image compression has achieved significant advancements,\ndelivering high visual quality at low bitrates for natural images. However, for\nscreen content, existing methods often produce noticeable artifacts when\ncompressing text. To tackle this challenge, we propose versatile perceptual\nscreen image compression with diffusion rendering (PICD), a codec that works\nwell for both screen and natural images. More specifically, we propose a\ncompression framework that encodes the text and image separately, and renders\nthem into one image using diffusion model. For this diffusion rendering, we\nintegrate conditional information into diffusion models at three distinct\nlevels: 1). Domain level: We fine-tune the base diffusion model using text\ncontent prompts with screen content. 2). Adaptor level: We develop an efficient\nadaptor to control the diffusion model using compressed image and text as\ninput. 3). Instance level: We apply instance-wise guidance to further enhance\nthe decoding process. Empirically, our PICD surpasses existing perceptual\ncodecs in terms of both text accuracy and perceptual quality. Additionally,\nwithout text conditions, our approach serves effectively as a perceptual codec\nfor natural images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6e32\u67d3\u7684\u901a\u7528\u611f\u77e5\u5c4f\u5e55\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff08PICD\uff09\uff0c\u9002\u7528\u4e8e\u5c4f\u5e55\u548c\u81ea\u7136\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u538b\u7f29\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u611f\u77e5\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u5728\u5904\u7406\u5c4f\u5e55\u5185\u5bb9\uff08\u5c24\u5176\u662f\u6587\u672c\uff09\u65f6\u4f1a\u4ea7\u751f\u660e\u663e\u4f2a\u5f71\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u79bb\u7f16\u7801\u6587\u672c\u548c\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u5728\u4e09\u4e2a\u5c42\u6b21\uff08\u57df\u3001\u9002\u914d\u5668\u548c\u5b9e\u4f8b\uff09\u4e0a\u6574\u5408\u6761\u4ef6\u4fe1\u606f\u8fdb\u884c\u6e32\u67d3\u3002", "result": "PICD\u5728\u6587\u672c\u51c6\u786e\u6027\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u81ea\u7136\u56fe\u50cf\u538b\u7f29\u3002", "conclusion": "PICD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u5c4f\u5e55\u5185\u5bb9\u548c\u81ea\u7136\u56fe\u50cf\u3002"}}
{"id": "2505.05855", "pdf": "https://arxiv.org/pdf/2505.05855", "abs": "https://arxiv.org/abs/2505.05855", "authors": ["Hongyu Rui", "Yinzhe Wu", "Fanwen Wang", "Jiahao Huang", "Liutao Yang", "Zi Wang", "Guang Yang"], "title": "Decoupling Multi-Contrast Super-Resolution: Pairing Unpaired Synthesis with Implicit Representations", "categories": ["cs.CV"], "comment": null, "summary": "Magnetic Resonance Imaging (MRI) is critical for clinical diagnostics but is\noften limited by long acquisition times and low signal-to-noise ratios,\nespecially in modalities like diffusion and functional MRI. The multi-contrast\nnature of MRI presents a valuable opportunity for cross-modal enhancement,\nwhere high-resolution (HR) modalities can serve as references to boost the\nquality of their low-resolution (LR) counterparts-motivating the development of\nMulti-Contrast Super-Resolution (MCSR) techniques. Prior work has shown that\nleveraging complementary contrasts can improve SR performance; however,\neffective feature extraction and fusion across modalities with varying\nresolutions remains a major challenge. Moreover, existing MCSR methods often\nassume fixed resolution settings and all require large, perfectly paired\ntraining datasets-conditions rarely met in real-world clinical environments. To\naddress these challenges, we propose a novel Modular Multi-Contrast\nSuper-Resolution (MCSR) framework that eliminates the need for paired training\ndata and supports arbitrary upscaling. Our method decouples the MCSR task into\ntwo stages: (1) Unpaired Cross-Modal Synthesis (U-CMS), which translates a\nhigh-resolution reference modality into a synthesized version of the target\ncontrast, and (2) Unsupervised Super-Resolution (U-SR), which reconstructs the\nfinal output using implicit neural representations (INRs) conditioned on\nspatial coordinates. This design enables scale-agnostic and anatomically\nfaithful reconstruction by bridging un-paired cross-modal synthesis with\nunsupervised resolution enhancement. Experiments show that our method achieves\nsuperior performance at 4x and 8x upscaling, with improved fidelity and\nanatomical consistency over existing baselines. Our framework demonstrates\nstrong potential for scalable, subject-specific, and data-efficient MCSR in\nreal-world clinical settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u591a\u5bf9\u6bd4\u8d85\u5206\u8fa8\u7387\uff08MCSR\uff09\u6846\u67b6\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u652f\u6301\u4efb\u610f\u653e\u5927\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002", "motivation": "MRI\u7684\u591a\u5bf9\u6bd4\u7279\u6027\u4e3a\u8de8\u6a21\u6001\u589e\u5f3a\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\u548c\u56fa\u5b9a\u5206\u8fa8\u7387\uff0c\u96be\u4ee5\u9002\u5e94\u4e34\u5e8a\u73af\u5883\u3002", "method": "\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u65e0\u914d\u5bf9\u8de8\u6a21\u6001\u5408\u6210\uff08U-CMS\uff09\u548c\u65e0\u76d1\u7763\u8d85\u5206\u8fa8\u7387\uff08U-SR\uff09\uff0c\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u5b9e\u73b0\u4efb\u610f\u653e\u5927\u3002", "result": "\u57284\u500d\u548c8\u500d\u653e\u5927\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4fdd\u771f\u5ea6\u548c\u89e3\u5256\u4e00\u81f4\u6027\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4e2a\u6027\u5316\u4e14\u6570\u636e\u9ad8\u6548MCSR\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05870", "pdf": "https://arxiv.org/pdf/2505.05870", "abs": "https://arxiv.org/abs/2505.05870", "authors": ["Yimin Zhou", "Yichong Xia", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "Towards Facial Image Compression with Consistency Preserving Diffusion Prior", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "With the widespread application of facial image data across various domains,\nthe efficient storage and transmission of facial images has garnered\nsignificant attention. However, the existing learned face image compression\nmethods often produce unsatisfactory reconstructed image quality at low bit\nrates. Simply adapting diffusion-based compression methods to facial\ncompression tasks results in reconstructed images that perform poorly in\ndownstream applications due to insufficient preservation of high-frequency\ninformation. To further explore the diffusion prior in facial image\ncompression, we propose Facial Image Compression with a Stable Diffusion Prior\n(FaSDiff), a method that preserves consistency through frequency enhancement.\nFaSDiff employs a high-frequency-sensitive compressor in an end-to-end\nframework to capture fine image details and produce robust visual prompts.\nAdditionally, we introduce a hybrid low-frequency enhancement module that\ndisentangles low-frequency facial semantics and stably modulates the diffusion\nprior alongside visual prompts. The proposed modules allow FaSDiff to leverage\ndiffusion priors for superior human visual perception while minimizing\nperformance loss in machine vision due to semantic inconsistency. Extensive\nexperiments show that FaSDiff outperforms state-of-the-art methods in balancing\nhuman visual quality and machine vision accuracy. The code will be released\nafter the paper is accepted.", "AI": {"tldr": "FaSDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563\u5148\u9a8c\u7684\u4eba\u8138\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u589e\u5f3a\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u5e73\u8861\u4eba\u7c7b\u89c6\u89c9\u8d28\u91cf\u548c\u673a\u5668\u89c6\u89c9\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u91cd\u5efa\u8d28\u91cf\u4e0d\u4f73\uff0c\u4e14\u9ad8\u9891\u4fe1\u606f\u4fdd\u7559\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u3002", "method": "FaSDiff\u91c7\u7528\u9ad8\u9891\u654f\u611f\u538b\u7f29\u5668\u548c\u4f4e\u9891\u589e\u5f3a\u6a21\u5757\uff0c\u7ed3\u5408\u89c6\u89c9\u63d0\u793a\u548c\u6269\u6563\u5148\u9a8c\uff0c\u4f18\u5316\u56fe\u50cf\u7ec6\u8282\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFaSDiff\u5728\u4eba\u7c7b\u89c6\u89c9\u8d28\u91cf\u548c\u673a\u5668\u89c6\u89c9\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FaSDiff\u901a\u8fc7\u9891\u7387\u589e\u5f3a\u548c\u6269\u6563\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4eba\u8138\u56fe\u50cf\u538b\u7f29\u3002"}}
{"id": "2505.05892", "pdf": "https://arxiv.org/pdf/2505.05892", "abs": "https://arxiv.org/abs/2505.05892", "authors": ["Alexander Lappe", "Martin A. Giese"], "title": "Register and CLS tokens yield a decoupling of local and global features in large ViTs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent work has shown that the attention maps of the widely popular DINOv2\nmodel exhibit artifacts, which hurt both model interpretability and performance\non dense image tasks. These artifacts emerge due to the model repurposing patch\ntokens with redundant local information for the storage of global image\ninformation. To address this problem, additional register tokens have been\nincorporated in which the model can store such information instead. We\ncarefully examine the influence of these register tokens on the relationship\nbetween global and local image features, showing that while register tokens\nyield cleaner attention maps, these maps do not accurately reflect the\nintegration of local image information in large models. Instead, global\ninformation is dominated by information extracted from register tokens, leading\nto a disconnect between local and global features. Inspired by these findings,\nwe show that the CLS token itself, which can be interpreted as a register,\nleads to a very similar phenomenon in models without explicit register tokens.\nOur work shows that care must be taken when interpreting attention maps of\nlarge ViTs. Further, by clearly attributing the faulty behaviour to register\nand CLS tokens, we show a path towards more interpretable vision models.", "AI": {"tldr": "DINOv2\u6a21\u578b\u7684\u6ce8\u610f\u529b\u56fe\u5b58\u5728\u4f2a\u5f71\uff0c\u5f71\u54cd\u89e3\u91ca\u6027\u548c\u5bc6\u96c6\u56fe\u50cf\u4efb\u52a1\u6027\u80fd\u3002\u5f15\u5165\u5bc4\u5b58\u5668\u4ee4\u724c\u867d\u6539\u5584\u6ce8\u610f\u529b\u56fe\uff0c\u4f46\u5168\u5c40\u4fe1\u606f\u4ecd\u4e3b\u5bfc\u5c40\u90e8\u4fe1\u606f\u3002CLS\u4ee4\u724c\u4e5f\u6709\u7c7b\u4f3c\u95ee\u9898\uff0c\u9700\u8c28\u614e\u89e3\u91ca\u5927\u578bViT\u7684\u6ce8\u610f\u529b\u56fe\u3002", "motivation": "\u89e3\u51b3DINOv2\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u56fe\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6027\u548c\u5bc6\u96c6\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u5f15\u5165\u5bc4\u5b58\u5668\u4ee4\u724c\u5b58\u50a8\u5168\u5c40\u4fe1\u606f\uff0c\u5206\u6790\u5176\u5bf9\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\u5173\u7cfb\u7684\u5f71\u54cd\u3002", "result": "\u5bc4\u5b58\u5668\u4ee4\u724c\u4f7f\u6ce8\u610f\u529b\u56fe\u66f4\u6e05\u6670\uff0c\u4f46\u5168\u5c40\u4fe1\u606f\u4e3b\u5bfc\u5c40\u90e8\u4fe1\u606f\uff0c\u5bfc\u81f4\u7279\u5f81\u5206\u79bb\u3002CLS\u4ee4\u724c\u4e5f\u6709\u7c7b\u4f3c\u95ee\u9898\u3002", "conclusion": "\u9700\u8c28\u614e\u89e3\u91ca\u5927\u578bViT\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u660e\u786e\u5bc4\u5b58\u5668\u4e0eCLS\u4ee4\u724c\u7684\u95ee\u9898\uff0c\u4e3a\u66f4\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2505.05895", "pdf": "https://arxiv.org/pdf/2505.05895", "abs": "https://arxiv.org/abs/2505.05895", "authors": ["Benjamin Raphael Ernhofer", "Daniil Prokhorov", "Jannica Langner", "Dominik Bollmann"], "title": "Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern automotive infotainment systems require intelligent and adaptive\nsolutions to handle frequent User Interface (UI) updates and diverse design\nvariations. We introduce a vision-language framework for understanding and\ninteracting with automotive infotainment systems, enabling seamless adaptation\nacross different UI designs. To further support research in this field, we\nrelease AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208\nannotations. Additionally, we present a synthetic data pipeline to generate\ntraining data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation\n(LoRa) and incorporating reasoning generated by our pipeline, along with visual\ngrounding and evaluation capabilities. The fine-tuned Evaluative Large Action\nModel (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and\ndataset are available on Hugging Face) and demonstrating strong cross-domain\ngeneralization, including a +5.2% improvement on ScreenSpot over the baseline\nmodel. Notably, our approach achieves 80.4% average accuracy on ScreenSpot,\nclosely matching or even surpassing specialized models for desktop, mobile, and\nweb, such as ShowUI, despite being trained for the infotainment domain. This\nresearch investigates how data collection and subsequent fine-tuning can lead\nto AI-driven progress within automotive UI understanding and interaction. The\napplied method is cost-efficient and fine-tuned models can be deployed on\nconsumer-grade GPUs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\u7684\u6c7d\u8f66\u4fe1\u606f\u5a31\u4e50\u7cfb\u7edf\u4ea4\u4e92\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u6e90\u6570\u636e\u96c6\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u5de5\u5177\uff0c\u901a\u8fc7LoRa\u5fae\u8c03\u6a21\u578bELAM\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3\u6c7d\u8f66\u4fe1\u606f\u5a31\u4e50\u7cfb\u7edf\u9700\u8981\u667a\u80fd\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u5e94\u5bf9\u9891\u7e41\u7684UI\u66f4\u65b0\u548c\u591a\u6837\u5316\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u751f\u6210\u548cLoRa\u5fae\u8c03\u6280\u672f\uff0c\u5f00\u53d1\u4e86ELAM\u6a21\u578b\u3002", "result": "ELAM\u5728AutomotiveUI-Bench-4K\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u63d0\u53475.2%\uff0cScreenSpot\u5e73\u5747\u51c6\u786e\u7387\u8fbe80.4%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6570\u636e\u6536\u96c6\u548c\u5fae\u8c03\u53ef\u63a8\u52a8\u6c7d\u8f66UI\u7406\u89e3\u7684AI\u8fdb\u5c55\uff0c\u65b9\u6cd5\u6210\u672c\u4f4e\u4e14\u9002\u7528\u4e8e\u6d88\u8d39\u7ea7GPU\u3002"}}
{"id": "2505.05901", "pdf": "https://arxiv.org/pdf/2505.05901", "abs": "https://arxiv.org/abs/2505.05901", "authors": ["Hanzhe Liang", "Aoran Wang", "Jie Zhou", "Xin Jin", "Can Gao", "Jinbao Wang"], "title": "Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages", "summary": "In this paper, we go beyond identifying anomalies only in structural terms\nand think about better anomaly detection motivated by anomaly causes. Most\nanomalies are regarded as the result of unpredictable defective forces from\ninternal and external sources, and their opposite forces are sought to correct\nthe anomalies. We introduced a Mechanics Complementary framework for 3D anomaly\ndetection (MC4AD) to generate internal and external Corrective forces for each\npoint. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to\nsimulate various anomalies. Then, we present a Corrective Force Prediction\nNetwork (CFP-Net) with complementary representations for point-level\nrepresentation to simulate the different contributions of internal and external\ncorrective forces. A combined loss was proposed, including a new symmetric loss\nand an overall loss, to constrain the corrective forces properly. As a\nhighlight, we consider 3D anomaly detection in industry more comprehensively,\ncreating a hierarchical quality control strategy based on a three-way decision\nand contributing a dataset named Anomaly-IntraVariance with intraclass variance\nto evaluate the model. On the proposed and existing five datasets, we obtained\nnine state-of-the-art performers with the minimum parameters and the fastest\ninference speed. The source is available at\nhttps://github.com/hzzzzzhappy/MC4AD", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u5e38\u539f\u56e0\u76843D\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6MC4AD\uff0c\u901a\u8fc7\u751f\u6210\u7ea0\u6b63\u529b\u6765\u68c0\u6d4b\u5f02\u5e38\uff0c\u5e76\u7ed3\u5408\u591a\u6837\u5316\u7684\u5f02\u5e38\u751f\u6210\u6a21\u5757\u548c\u7ea0\u6b63\u529b\u9884\u6d4b\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53c2\u6570\u5c11\u7684\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u4ec5\u5173\u6ce8\u7ed3\u6784\u7279\u5f81\uff0c\u800c\u5ffd\u7565\u4e86\u5f02\u5e38\u539f\u56e0\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u5185\u90e8\u548c\u5916\u90e8\u7ea0\u6b63\u529b\uff0c\u66f4\u5168\u9762\u5730\u68c0\u6d4b\u5f02\u5e38\u3002", "method": "\u63d0\u51faMC4AD\u6846\u67b6\uff0c\u5305\u62ecDA-Gen\u6a21\u5757\u6a21\u62df\u5f02\u5e38\uff0cCFP-Net\u9884\u6d4b\u7ea0\u6b63\u529b\uff0c\u5e76\u7ed3\u5408\u5bf9\u79f0\u635f\u5931\u548c\u6574\u4f53\u635f\u5931\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e5d\u9879\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u6700\u5c11\u3001\u63a8\u7406\u901f\u5ea6\u6700\u5feb\u3002", "conclusion": "MC4AD\u6846\u67b6\u4e3a3D\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.05913", "pdf": "https://arxiv.org/pdf/2505.05913", "abs": "https://arxiv.org/abs/2505.05913", "authors": ["Jianjian Yin", "Yi Chen", "Chengyu Li", "Zhichao Zheng", "Yanhui Gu", "Junsheng Zhou"], "title": "DFEN: Dual Feature Equalization Network for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Current methods for medical image segmentation primarily focus on extracting\ncontextual feature information from the perspective of the whole image. While\nthese methods have shown effective performance, none of them take into account\nthe fact that pixels at the boundary and regions with a low number of class\npixels capture more contextual feature information from other classes, leading\nto misclassification of pixels by unequal contextual feature information. In\nthis paper, we propose a dual feature equalization network based on the hybrid\narchitecture of Swin Transformer and Convolutional Neural Network, aiming to\naugment the pixel feature representations by image-level equalization feature\ninformation and class-level equalization feature information. Firstly, the\nimage-level feature equalization module is designed to equalize the contextual\ninformation of pixels within the image. Secondly, we aggregate regions of the\nsame class to equalize the pixel feature representations of the corresponding\nclass by class-level feature equalization module. Finally, the pixel feature\nrepresentations are enhanced by learning weights for image-level equalization\nfeature information and class-level equalization feature information. In\naddition, Swin Transformer is utilized as both the encoder and decoder, thereby\nbolstering the ability of the model to capture long-range dependencies and\nspatial correlations. We conducted extensive experiments on Breast Ultrasound\nImages (BUSI), International Skin Imaging Collaboration (ISIC2017), Automated\nCardiac Diagnosis Challenge (ACDC) and PH$^2$ datasets. The experimental\nresults demonstrate that our method have achieved state-of-the-art performance.\nOur code is publicly available at https://github.com/JianJianYin/DFEN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSwin Transformer\u548cCNN\u7684\u53cc\u7279\u5f81\u5747\u8861\u7f51\u7edc\uff0c\u901a\u8fc7\u56fe\u50cf\u7ea7\u548c\u7c7b\u522b\u7ea7\u7279\u5f81\u5747\u8861\u589e\u5f3a\u50cf\u7d20\u7279\u5f81\u8868\u793a\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u672a\u8003\u8651\u8fb9\u754c\u50cf\u7d20\u548c\u4f4e\u7c7b\u522b\u50cf\u7d20\u533a\u57df\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u4fe1\u606f\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u8bef\u5206\u7c7b\u3002", "method": "\u8bbe\u8ba1\u4e86\u56fe\u50cf\u7ea7\u548c\u7c7b\u522b\u7ea7\u7279\u5f81\u5747\u8861\u6a21\u5757\uff0c\u7ed3\u5408Swin Transformer\u548cCNN\uff0c\u589e\u5f3a\u50cf\u7d20\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08BUSI\u3001ISIC2017\u3001ACDC\u3001PH2\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u53cc\u7279\u5f81\u5747\u8861\u7f51\u7edc\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u4fe1\u606f\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2505.05936", "pdf": "https://arxiv.org/pdf/2505.05936", "abs": "https://arxiv.org/abs/2505.05936", "authors": ["Weihong Li", "Xiaoqiong Liu", "Heng Fan", "Libo Zhang"], "title": "CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking", "categories": ["cs.CV"], "comment": "Accepted by ICRA 2025", "summary": "Recent advancements in visual object tracking have markedly improved the\ncapabilities of unmanned aerial vehicle (UAV) tracking, which is a critical\ncomponent in real-world robotics applications. While the integration of\nhierarchical lightweight networks has become a prevalent strategy for enhancing\nefficiency in UAV tracking, it often results in a significant drop in network\ncapacity, which further exacerbates challenges in UAV scenarios, such as\nfrequent occlusions and extreme changes in viewing angles. To address these\nissues, we introduce a novel family of UAV trackers, termed CGTrack, which\ncombines explicit and implicit techniques to expand network capacity within a\ncoarse-to-fine framework. Specifically, we first introduce a Hierarchical\nFeature Cascade (HFC) module that leverages the spirit of feature reuse to\nincrease network capacity by integrating the deep semantic cues with the rich\nspatial information, incurring minimal computational costs while enhancing\nfeature representation. Based on this, we design a novel Lightweight Gated\nCenter Head (LGCH) that utilizes gating mechanisms to decouple target-oriented\ncoordinates from previously expanded features, which contain dense local\ndiscriminative information. Extensive experiments on three challenging UAV\ntracking benchmarks demonstrate that CGTrack achieves state-of-the-art\nperformance while running fast. Code will be available at\nhttps://github.com/Nightwatch-Fox11/CGTrack.", "AI": {"tldr": "CGTrack\u662f\u4e00\u79cd\u65b0\u578b\u65e0\u4eba\u673a\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u6280\u672f\u63d0\u5347\u7f51\u7edc\u5bb9\u91cf\uff0c\u89e3\u51b3\u4e86\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5728\u65e0\u4eba\u673a\u8ddf\u8e2a\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u8ddf\u8e2a\u4e2d\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5bb9\u91cf\u4e0d\u8db3\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u906e\u6321\u548c\u89c6\u89d2\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faHFC\u6a21\u5757\u548cLGCH\u5934\uff0c\u524d\u8005\u901a\u8fc7\u7279\u5f81\u91cd\u7528\u589e\u5f3a\u7f51\u7edc\u5bb9\u91cf\uff0c\u540e\u8005\u5229\u7528\u95e8\u63a7\u673a\u5236\u89e3\u8026\u76ee\u6807\u5750\u6807\u3002", "result": "\u5728\u4e09\u4e2a\u65e0\u4eba\u673a\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u4e5f\u5feb\u3002", "conclusion": "CGTrack\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2505.05943", "pdf": "https://arxiv.org/pdf/2505.05943", "abs": "https://arxiv.org/abs/2505.05943", "authors": ["Maan Alhazmi", "Abdulrahman Altahhan"], "title": "Achieving 3D Attention via Triplet Squeeze and Excitation Block", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of ConvNeXt and its variants has reaffirmed the conceptual and\nstructural suitability of CNN-based models for vision tasks, re-establishing\nthem as key players in image classification in general, and in facial\nexpression recognition (FER) in particular. In this paper, we propose a new set\nof models that build on these advancements by incorporating a new set of\nattention mechanisms that combines Triplet attention with\nSqueeze-and-Excitation (TripSE) in four different variants. We demonstrate the\neffectiveness of these variants by applying them to the ResNet18, DenseNet and\nConvNext architectures to validate their versatility and impact. Our study\nshows that incorporating a TripSE block in these CNN models boosts their\nperformances, particularly for the ConvNeXt architecture, indicating its\nutility. We evaluate the proposed mechanisms and associated models across four\ndatasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where\nConvNext with TripSE achieves state-of-the-art results with an accuracy of\n\\textbf{78.27\\%} on the popular FER2013 dataset, a new feat for this dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Triplet\u6ce8\u610f\u529b\u4e0eSqueeze-and-Excitation\uff08TripSE\uff09\u7684\u65b0\u673a\u5236\uff0c\u5e76\u5728\u591a\u79cdCNN\u67b6\u6784\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728ConvNeXt\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "CNN\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u4ecd\u5177\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u9886\u57df\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u56db\u79cdTripSE\u53d8\u4f53\uff0c\u5e94\u7528\u4e8eResNet18\u3001DenseNet\u548cConvNeXt\u67b6\u6784\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "TripSE\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0cConvNeXt\u7ed3\u5408TripSE\u5728FER2013\u6570\u636e\u96c6\u4e0a\u8fbe\u523078.27%\u7684\u51c6\u786e\u7387\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "conclusion": "TripSE\u673a\u5236\u6709\u6548\u589e\u5f3a\u4e86CNN\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728ConvNeXt\u67b6\u6784\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u4e3aFER\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06002", "pdf": "https://arxiv.org/pdf/2505.06002", "abs": "https://arxiv.org/abs/2505.06002", "authors": ["Congqi Cao", "Peiheng Han", "Yueran zhang", "Yating Yu", "Qinyi Lv", "Lingtong Min", "Yanning zhang"], "title": "Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for Few-shot Action Recognition", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2408.00249", "summary": "Large-scale pre-trained models have achieved remarkable success in language\nand image tasks, leading an increasing number of studies to explore the\napplication of pre-trained image models, such as CLIP, in the domain of\nfew-shot action recognition (FSAR). However, current methods generally suffer\nfrom several problems: 1) Direct fine-tuning often undermines the\ngeneralization capability of the pre-trained model; 2) The exploration of\ntask-specific information is insufficient in the visual tasks; 3) The semantic\norder information is typically overlooked during text modeling; 4) Existing\ncross-modal alignment techniques ignore the temporal coupling of multimodal\ninformation. To address these, we propose Task-Adapter++, a parameter-efficient\ndual adaptation method for both image and text encoders. Specifically, to make\nfull use of the variations across different few-shot learning tasks, we design\na task-specific adaptation for the image encoder so that the most\ndiscriminative information can be well noticed during feature extraction.\nFurthermore, we leverage large language models (LLMs) to generate detailed\nsequential sub-action descriptions for each action class, and introduce\nsemantic order adapters into the text encoder to effectively model the\nsequential relationships between these sub-actions. Finally, we develop an\ninnovative fine-grained cross-modal alignment strategy that actively maps\nvisual features to reside in the same temporal stage as semantic descriptions.\nExtensive experiments fully demonstrate the effectiveness and superiority of\nthe proposed method, which achieves state-of-the-art performance on 5\nbenchmarks consistently. The code is open-sourced at\nhttps://github.com/Jaulin-Bage/Task-Adapter-pp.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTask-Adapter++\uff0c\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u53cc\u91cd\u9002\u5e94\u65b9\u6cd5\uff0c\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u95ee\u9898\uff0c\u5305\u62ec\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3001\u4efb\u52a1\u4fe1\u606f\u4e0d\u8db3\u3001\u8bed\u4e49\u987a\u5e8f\u5ffd\u7565\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3001\u4efb\u52a1\u4fe1\u606f\u4e0d\u8db3\u3001\u8bed\u4e49\u987a\u5e8f\u5ffd\u7565\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4efb\u52a1\u7279\u5b9a\u7684\u56fe\u50cf\u7f16\u7801\u5668\u9002\u914d\u5668\uff0c\u5229\u7528LLM\u751f\u6210\u5b50\u52a8\u4f5c\u63cf\u8ff0\u5e76\u5f15\u5165\u8bed\u4e49\u987a\u5e8f\u9002\u914d\u5668\uff0c\u5f00\u53d1\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "Task-Adapter++\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2505.06003", "pdf": "https://arxiv.org/pdf/2505.06003", "abs": "https://arxiv.org/abs/2505.06003", "authors": ["Moritz Vandenhirtz", "Julia E. Vogt"], "title": "From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection", "categories": ["cs.CV", "cs.LG"], "comment": "International Conference on Machine Learning", "summary": "Understanding the decision-making process of machine learning models provides\nvaluable insights into the task, the data, and the reasons behind a model's\nfailures. In this work, we propose a method that performs inherently\ninterpretable predictions through the instance-wise sparsification of input\nimages. To align the sparsification with human perception, we learn the masking\nin the space of semantically meaningful pixel regions rather than on\npixel-level. Additionally, we introduce an explicit way to dynamically\ndetermine the required level of sparsity for each instance. We show empirically\non semi-synthetic and natural image datasets that our inherently interpretable\nclassifier produces more meaningful, human-understandable predictions than\nstate-of-the-art benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u7a00\u758f\u5316\u8f93\u5165\u56fe\u50cf\u5b9e\u73b0\u53ef\u89e3\u91ca\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u534a\u5408\u6210\u548c\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u5bf9\u4efb\u52a1\u3001\u6570\u636e\u548c\u6a21\u578b\u5931\u8d25\u539f\u56e0\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5728\u8bed\u4e49\u6709\u610f\u4e49\u7684\u50cf\u7d20\u533a\u57df\u5b66\u4e60\u63a9\u7801\uff0c\u800c\u975e\u50cf\u7d20\u7ea7\uff0c\u5e76\u52a8\u6001\u786e\u5b9a\u6bcf\u4e2a\u5b9e\u4f8b\u6240\u9700\u7684\u7a00\u758f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u9884\u6d4b\u66f4\u5177\u4eba\u7c7b\u53ef\u7406\u89e3\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u76f4\u89c2\u3001\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u65b9\u5f0f\u3002"}}
{"id": "2505.06038", "pdf": "https://arxiv.org/pdf/2505.06038", "abs": "https://arxiv.org/abs/2505.06038", "authors": ["Heng Li", "Xiangping Wu", "Qingcai Chen"], "title": "Document Image Rectification Bases on Self-Adaptive Multitask Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Deformed document image rectification is essential for real-world document\nunderstanding tasks, such as layout analysis and text recognition. However,\ncurrent multi-task methods -- such as background removal, 3D coordinate\nprediction, and text line segmentation -- often overlook the complementary\nfeatures between tasks and their interactions. To address this gap, we propose\na self-adaptive learnable multi-task fusion rectification network named\nSalmRec. This network incorporates an inter-task feature aggregation module\nthat adaptively improves the perception of geometric distortions, enhances\nfeature complementarity, and reduces negative interference. We also introduce a\ngating mechanism to balance features both within global tasks and between local\ntasks effectively. Experimental results on two English benchmarks (DIR300 and\nDocUNet) and one Chinese benchmark (DocReal) demonstrate that our method\nsignificantly improves rectification performance. Ablation studies further\nhighlight the positive impact of different tasks on dewarping and the\neffectiveness of our proposed module.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u591a\u4efb\u52a1\u878d\u5408\u7f51\u7edcSalmRec\uff0c\u901a\u8fc7\u4efb\u52a1\u95f4\u7279\u5f81\u805a\u5408\u548c\u95e8\u63a7\u673a\u5236\u63d0\u5347\u6587\u6863\u56fe\u50cf\u77eb\u6b63\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u591a\u4efb\u52a1\u95f4\u7684\u4e92\u8865\u7279\u5f81\u548c\u4ea4\u4e92\u4f5c\u7528\uff0c\u5f71\u54cd\u4e86\u6587\u6863\u56fe\u50cf\u77eb\u6b63\u7684\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u4e86SalmRec\u7f51\u7edc\uff0c\u5305\u542b\u4efb\u52a1\u95f4\u7279\u5f81\u805a\u5408\u6a21\u5757\u548c\u95e8\u63a7\u673a\u5236\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u63d0\u5347\u51e0\u4f55\u7578\u53d8\u611f\u77e5\u548c\u7279\u5f81\u4e92\u8865\u6027\u3002", "result": "\u5728DIR300\u3001DocUNet\u548cDocReal\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77eb\u6b63\u6027\u80fd\u3002", "conclusion": "SalmRec\u901a\u8fc7\u591a\u4efb\u52a1\u534f\u540c\u548c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u6863\u56fe\u50cf\u77eb\u6b63\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.06055", "pdf": "https://arxiv.org/pdf/2505.06055", "abs": "https://arxiv.org/abs/2505.06055", "authors": ["Dongqian Guo", "Wencheng Han", "Pang Lyu", "Yuxi Zhou", "Jianbing Shen"], "title": "Towards Better Cephalometric Landmark Detection with Diffusion Data Generation", "categories": ["cs.CV"], "comment": null, "summary": "Cephalometric landmark detection is essential for orthodontic diagnostics and\ntreatment planning. Nevertheless, the scarcity of samples in data collection\nand the extensive effort required for manual annotation have significantly\nimpeded the availability of diverse datasets. This limitation has restricted\nthe effectiveness of deep learning-based detection methods, particularly those\nbased on large-scale vision models. To address these challenges, we have\ndeveloped an innovative data generation method capable of producing diverse\ncephalometric X-ray images along with corresponding annotations without human\nintervention. To achieve this, our approach initiates by constructing new\ncephalometric landmark annotations using anatomical priors. Then, we employ a\ndiffusion-based generator to create realistic X-ray images that correspond\nclosely with these annotations. To achieve precise control in producing samples\nwith different attributes, we introduce a novel prompt cephalometric X-ray\nimage dataset. This dataset includes real cephalometric X-ray images and\ndetailed medical text prompts describing the images. By leveraging these\ndetailed prompts, our method improves the generation process to control\ndifferent styles and attributes. Facilitated by the large, diverse generated\ndata, we introduce large-scale vision detection models into the cephalometric\nlandmark detection task to improve accuracy. Experimental results demonstrate\nthat training with the generated data substantially enhances the performance.\nCompared to methods without using the generated data, our approach improves the\nSuccess Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and\ndata are available at: https://um-lab.github.io/cepha-generation", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u5316\u7684\u5934\u9885X\u5149\u56fe\u50cf\u53ca\u6807\u6ce8\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5934\u9885\u6807\u5fd7\u70b9\u68c0\u6d4b\u5bf9\u6b63\u7578\u8bca\u65ad\u548c\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u548c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u89e3\u5256\u5b66\u5148\u9a8c\u6784\u5efa\u65b0\u6807\u6ce8\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771fX\u5149\u56fe\u50cf\uff0c\u5e76\u5f15\u5165\u63d0\u793a\u6570\u636e\u96c6\u4ee5\u63a7\u5236\u6837\u672c\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u751f\u6210\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u68c0\u6d4b\u6210\u529f\u7387\uff08SDR\uff09\u63d0\u9ad8\u4e866.5%\uff0c\u8fbe\u523082.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5934\u9885\u6807\u5fd7\u70b9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06068", "pdf": "https://arxiv.org/pdf/2505.06068", "abs": "https://arxiv.org/abs/2505.06068", "authors": ["Kunpeng Qiu", "Zhiqiang Gao", "Zhiying Zhou", "Mingjie Sun", "Yongxin Guo"], "title": "Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Deep learning has revolutionized medical image segmentation, yet its full\npotential remains constrained by the paucity of annotated datasets. While\ndiffusion models have emerged as a promising approach for generating synthetic\nimage-mask pairs to augment these datasets, they paradoxically suffer from the\nsame data scarcity challenges they aim to mitigate. Traditional mask-only\nmodels frequently yield low-fidelity images due to their inability to\nadequately capture morphological intricacies, which can critically compromise\nthe robustness and reliability of segmentation models. To alleviate this\nlimitation, we introduce Siamese-Diffusion, a novel dual-component model\ncomprising Mask-Diffusion and Image-Diffusion. During training, a Noise\nConsistency Loss is introduced between these components to enhance the\nmorphological fidelity of Mask-Diffusion in the parameter space. During\nsampling, only Mask-Diffusion is used, ensuring diversity and scalability.\nComprehensive experiments demonstrate the superiority of our method.\nSiamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps,\nwhile UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at\nGitHub.", "AI": {"tldr": "Siamese-Diffusion\u6a21\u578b\u901a\u8fc7\u53cc\u7ec4\u4ef6\u8bbe\u8ba1\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u5f62\u6001\u4fdd\u771f\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u4f20\u7edf\u6a21\u578b\u751f\u6210\u56fe\u50cf\u4fdd\u771f\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSiamese-Diffusion\u6a21\u578b\uff0c\u5305\u542bMask-Diffusion\u548cImage-Diffusion\uff0c\u901a\u8fc7\u566a\u58f0\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u5f62\u6001\u4fdd\u771f\u5ea6\u3002", "result": "\u5728Polyps\u548cISIC2018\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u522b\u63d0\u5347SANet\u548cUNet\u7684\u5206\u5272\u6027\u80fd\u3002", "conclusion": "Siamese-Diffusion\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u5f62\u6001\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.06113", "pdf": "https://arxiv.org/pdf/2505.06113", "abs": "https://arxiv.org/abs/2505.06113", "authors": ["Anupkumar Bochare"], "title": "Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous vehicle perception systems have traditionally relied on costly\nLiDAR sensors to generate precise environmental representations. In this paper,\nwe propose a camera-only perception framework that produces Bird's Eye View\n(BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combines\nYOLOv11-based object detection with DepthAnythingV2 monocular depth estimation\nacross multi-camera inputs to achieve comprehensive 360-degree scene\nunderstanding. We evaluate our approach on the OpenLane-V2 and NuScenes\ndatasets, achieving up to 85% road segmentation accuracy and 85-90% vehicle\ndetection rates when compared against LiDAR ground truth, with average\npositional errors limited to 1.2 meters. These results highlight the potential\nof deep learning to extract rich spatial information using only camera inputs,\nenabling cost-efficient autonomous navigation without sacrificing accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u76f8\u673a\u7684\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408YOLOv11\u76ee\u6807\u68c0\u6d4b\u548cDepthAnythingV2\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5b9e\u73b0360\u5ea6\u573a\u666f\u7406\u89e3\uff0c\u6027\u80fd\u63a5\u8fd1LiDAR\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4f9d\u8d56\u6602\u8d35\u7684LiDAR\u4f20\u611f\u5668\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4ec5\u4f7f\u7528\u76f8\u673a\u5b9e\u73b0\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u73af\u5883\u611f\u77e5\u3002", "method": "\u6269\u5c55Lift-Splat-Shoot\u67b6\u6784\uff0c\u7ed3\u5408YOLOv11\u76ee\u6807\u68c0\u6d4b\u548cDepthAnythingV2\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5904\u7406\u591a\u6444\u50cf\u5934\u8f93\u5165\u751f\u6210BEV\u5730\u56fe\u3002", "result": "\u5728OpenLane-V2\u548cNuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u9053\u8def\u5206\u5272\u51c6\u786e\u7387\u8fbe85%\uff0c\u8f66\u8f86\u68c0\u6d4b\u738785-90%\uff0c\u4f4d\u7f6e\u8bef\u5dee\u5e73\u57471.2\u7c73\u3002", "conclusion": "\u4ec5\u4f7f\u7528\u76f8\u673a\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u9ad8\u6548\u63d0\u53d6\u7a7a\u95f4\u4fe1\u606f\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u4e14\u9ad8\u7cbe\u5ea6\u7684\u81ea\u52a8\u9a7e\u9a76\u5bfc\u822a\u3002"}}
{"id": "2505.06117", "pdf": "https://arxiv.org/pdf/2505.06117", "abs": "https://arxiv.org/abs/2505.06117", "authors": ["Dongying Li", "Binyi Su", "Hua Zhang", "Yong Li", "Haiyong Chen"], "title": "Photovoltaic Defect Image Generator with Boundary Alignment Smoothing Constraint for Domain Shift Mitigation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate defect detection of photovoltaic (PV) cells is critical for ensuring\nquality and efficiency in intelligent PV manufacturing systems. However, the\nscarcity of rich defect data poses substantial challenges for effective model\ntraining. While existing methods have explored generative models to augment\ndatasets, they often suffer from instability, limited diversity, and domain\nshifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image\nGenerator based on Stable Diffusion (SD). PDIG leverages the strong priors\nlearned from large-scale datasets to enhance generation quality under limited\ndata. Specifically, we introduce a Semantic Concept Embedding (SCE) module that\nincorporates text-conditioned priors to capture the relational concepts between\ndefect types and their appearances. To further enrich the domain distribution,\nwe design a Lightweight Industrial Style Adaptor (LISA), which injects\nindustrial defect characteristics into the SD model through cross-disentangled\nattention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC)\nmodule, enforcing the quality of generated images via positional consistency\nand spatial smoothing alignment. Extensive experiments demonstrate that PDIG\nachieves superior realism and diversity compared to state-of-the-art methods.\nSpecifically, our approach improves Frechet Inception Distance (FID) by 19.16\npoints over the second-best method and significantly enhances the performance\nof downstream defect detection tasks.", "AI": {"tldr": "PDIG\u662f\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563\u7684\u5149\u4f0f\u7f3a\u9677\u56fe\u50cf\u751f\u6210\u5668\uff0c\u901a\u8fc7\u8bed\u4e49\u6982\u5ff5\u5d4c\u5165\u548c\u5de5\u4e1a\u98ce\u683c\u9002\u914d\u5668\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5149\u4f0f\u7535\u6c60\u7f3a\u9677\u68c0\u6d4b\u5bf9\u667a\u80fd\u5236\u9020\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u6a21\u578b\u8bad\u7ec3\u56f0\u96be\uff0c\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e0d\u7a33\u5b9a\u3001\u591a\u6837\u6027\u4e0d\u8db3\u548c\u9886\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "PDIG\u7ed3\u5408\u8bed\u4e49\u6982\u5ff5\u5d4c\u5165\u6a21\u5757\uff08SCE\uff09\u548c\u8f7b\u91cf\u7ea7\u5de5\u4e1a\u98ce\u683c\u9002\u914d\u5668\uff08LISA\uff09\uff0c\u5229\u7528\u6587\u672c-\u56fe\u50cf\u53cc\u7a7a\u95f4\u7ea6\u675f\uff08TIDSC\uff09\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "result": "PDIG\u5728FID\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd519.16\u5206\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "PDIG\u901a\u8fc7\u7ed3\u5408\u7a33\u5b9a\u6269\u6563\u548c\u9886\u57df\u7279\u5b9a\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5149\u4f0f\u7f3a\u9677\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u9ad8\u4e14\u591a\u6837\u3002"}}
{"id": "2505.06133", "pdf": "https://arxiv.org/pdf/2505.06133", "abs": "https://arxiv.org/abs/2505.06133", "authors": ["Hongming Wang", "Yifeng Wu", "Huimin Huang", "Hongtao Wu", "Jia-Xuan Jiang", "Xiaodong Zhang", "Hao Zheng", "Xian Wu", "Yefeng Zheng", "Jinping Xu", "Jing Cheng"], "title": "BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The segmentation of substantial brain lesions is a significant and\nchallenging task in the field of medical image segmentation. Substantial brain\nlesions in brain imaging exhibit high heterogeneity, with indistinct boundaries\nbetween lesion regions and normal brain tissue. Small lesions in single slices\nare difficult to identify, making the accurate and reproducible segmentation of\nabnormal regions, as well as their feature description, highly complex.\nExisting methods have the following limitations: 1) They rely solely on\nsingle-modal information for learning, neglecting the multi-modal information\ncommonly used in diagnosis. This hampers the ability to comprehensively acquire\nbrain lesion information from multiple perspectives and prevents the effective\nintegration and utilization of multi-modal data inputs, thereby limiting a\nholistic understanding of lesions. 2) They are constrained by the amount of\ndata available, leading to low sensitivity to small lesions and difficulty in\ndetecting subtle pathological changes. 3) Current SAM-based models rely on\nexternal prompts, which cannot achieve automatic segmentation and, to some\nextent, affect diagnostic efficiency.To address these issues, we have developed\na large-scale fully automated segmentation model specifically designed for\nbrain lesion segmentation, named BrainSegDMLF. This model has the following\nfeatures: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and\nintegrates multi-modal data during the encoding process, providing the SAM\nencoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling\nDecoder, enabling the model to extract rich low-level and high-level features\neven with limited data, thereby detecting the presence of small lesions. 3)\nAutomatic segmentation masks, allowing the model to generate lesion masks\nautomatically without requiring manual prompts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBrainSegDMLF\u7684\u5168\u81ea\u52a8\u8111\u90e8\u75c5\u53d8\u5206\u5272\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u3001\u5c0f\u75c5\u53d8\u68c0\u6d4b\u548c\u81ea\u52a8\u5316\u5206\u5272\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u8111\u90e8\u75c5\u53d8\u5206\u5272\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u591a\u6a21\u6001\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u3001\u5c0f\u75c5\u53d8\u68c0\u6d4b\u56f0\u96be\u4ee5\u53ca\u4f9d\u8d56\u5916\u90e8\u63d0\u793a\u7b49\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86BrainSegDMLF\u6a21\u578b\uff0c\u5305\u542b\u52a8\u6001\u6a21\u6001\u4ea4\u4e92\u878d\u5408\u6a21\u5757\u3001\u9010\u5c42\u4e0a\u91c7\u6837\u89e3\u7801\u5668\u548c\u81ea\u52a8\u5206\u5272\u63a9\u7801\u529f\u80fd\u3002", "result": "\u6a21\u578b\u80fd\u591f\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3001\u68c0\u6d4b\u5c0f\u75c5\u53d8\u5e76\u5b9e\u73b0\u5168\u81ea\u52a8\u5206\u5272\u3002", "conclusion": "BrainSegDMLF\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u8111\u90e8\u75c5\u53d8\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2505.06152", "pdf": "https://arxiv.org/pdf/2505.06152", "abs": "https://arxiv.org/abs/2505.06152", "authors": ["Wenqi Zeng", "Yuqi Sun", "Chenxi Ma", "Weimin Tan", "Bo Yan"], "title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical vision-language models (VLMs) have shown promise as clinical\nassistants across various medical fields. However, specialized dermatology VLM\ncapable of delivering professional and detailed diagnostic analysis remains\nunderdeveloped, primarily due to less specialized text descriptions in current\ndermatology multimodal datasets. To address this issue, we propose MM-Skin, the\nfirst large-scale multimodal dermatology dataset that encompasses 3 imaging\nmodalities, including clinical, dermoscopic, and pathological and nearly 10k\nhigh-quality image-text pairs collected from professional textbooks. In\naddition, we generate over 27k diverse, instruction-following vision question\nanswering (VQA) samples (9 times the size of current largest dermatology VQA\ndataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a\ndermatology-specific VLM designed for precise and nuanced skin disease\ninterpretation. Comprehensive benchmark evaluations of SkinVL on VQA,\nsupervised fine-tuning (SFT) and zero-shot classification tasks across 8\ndatasets, reveal its exceptional performance for skin diseases in comparison to\nboth general and medical VLM models. The introduction of MM-Skin and SkinVL\noffers a meaningful contribution to advancing the development of clinical\ndermatology VLM assistants. MM-Skin is available at\nhttps://github.com/ZwQ803/MM-Skin", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MM-Skin\u6570\u636e\u96c6\u548cSkinVL\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u76ae\u80a4\u75c5\u9886\u57df\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7f3a\u4e4f\u4e13\u4e1a\u6587\u672c\u63cf\u8ff0\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u76ae\u80a4\u75c5\u9886\u57df\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u7f3a\u4e4f\u4e13\u4e1a\u6587\u672c\u63cf\u8ff0\uff0c\u9650\u5236\u4e86\u76ae\u80a4\u75c5VLM\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b3\u79cd\u6210\u50cf\u6a21\u6001\u548c\u8fd110k\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u7684MM-Skin\u6570\u636e\u96c6\uff0c\u5e76\u751f\u621027k\u591a\u6837\u5316\u7684VQA\u6837\u672c\uff1b\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u76ae\u80a4\u75c5\u4e13\u7528VLM\u6a21\u578bSkinVL\u3002", "result": "SkinVL\u5728VQA\u3001\u76d1\u7763\u5fae\u8c03\u548c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u901a\u7528\u548c\u533b\u5b66VLM\u6a21\u578b\u3002", "conclusion": "MM-Skin\u548cSkinVL\u4e3a\u76ae\u80a4\u75c5VLM\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2505.06166", "pdf": "https://arxiv.org/pdf/2505.06166", "abs": "https://arxiv.org/abs/2505.06166", "authors": ["Radu Alexandru Rosu", "Keyu Wu", "Yao Feng", "Youyi Zheng", "Michael J. Black"], "title": "DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We address the task of generating 3D hair geometry from a single image, which\nis challenging due to the diversity of hairstyles and the lack of paired\nimage-to-3D hair data. Previous methods are primarily trained on synthetic data\nand cope with the limited amount of such data by using low-dimensional\nintermediate representations, such as guide strands and scalp-level embeddings,\nthat require post-processing to decode, upsample, and add realism. These\napproaches fail to reconstruct detailed hair, struggle with curly hair, or are\nlimited to handling only a few hairstyles. To overcome these limitations, we\npropose DiffLocks, a novel framework that enables detailed reconstruction of a\nwide variety of hairstyles directly from a single image. First, we address the\nlack of 3D hair data by automating the creation of the largest synthetic hair\ndataset to date, containing 40K hairstyles. Second, we leverage the synthetic\nhair dataset to learn an image-conditioned diffusion-transfomer model that\ngenerates accurate 3D strands from a single frontal image. By using a\npretrained image backbone, our method generalizes to in-the-wild images despite\nbeing trained only on synthetic data. Our diffusion model predicts a scalp\ntexture map in which any point in the map contains the latent code for an\nindividual hair strand. These codes are directly decoded to 3D strands without\npost-processing techniques. Representing individual strands, instead of guide\nstrands, enables the transformer to model the detailed spatial structure of\ncomplex hairstyles. With this, DiffLocks can recover highly curled hair, like\nafro hairstyles, from a single image for the first time. Data and code is\navailable at https://radualexandru.github.io/difflocks/", "AI": {"tldr": "DiffLocks\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u6269\u6563\u53d8\u6362\u6a21\u578b\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u76f4\u63a5\u751f\u6210\u8be6\u7ec6\u76843D\u5934\u53d1\u51e0\u4f55\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6837\u53d1\u578b\u548c\u7ec6\u8282\u91cd\u5efa\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u914d\u5bf9\u6570\u636e\u548c\u4f4e\u7ef4\u8868\u793a\u9650\u5236\uff0c\u96be\u4ee5\u91cd\u5efa\u590d\u6742\u53d1\u578b\uff08\u5982\u5377\u53d1\uff09\uff0c\u4e14\u4f9d\u8d56\u540e\u5904\u7406\u3002", "method": "1. \u521b\u5efa\u5927\u89c4\u6a21\u5408\u6210\u5934\u53d1\u6570\u636e\u96c6\uff0840K\u53d1\u578b\uff09\uff1b2. \u57fa\u4e8e\u6269\u6563\u53d8\u6362\u6a21\u578b\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u62103D\u53d1\u4e1d\uff0c\u65e0\u9700\u540e\u5904\u7406\u3002", "result": "DiffLocks\u9996\u6b21\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u9ad8\u5ea6\u5377\u66f2\u53d1\u578b\uff08\u5982\u975e\u6d32\u53d1\u578b\uff09\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "DiffLocks\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u76f4\u63a5\u89e3\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u56fe\u50cf3D\u5934\u53d1\u91cd\u5efa\u7684\u591a\u6837\u6027\u548c\u7ec6\u8282\u8868\u73b0\u3002"}}
{"id": "2505.06217", "pdf": "https://arxiv.org/pdf/2505.06217", "abs": "https://arxiv.org/abs/2505.06217", "authors": ["Pengfei Gu", "Haoteng Tang", "Islam A. Ebeid", "Jose A. Nunez", "Fabian Vazquez", "Diego Adame", "Marcus Zhan", "Huimin Li", "Bin Fu", "Danny Z. Chen"], "title": "Adapting a Segmentation Foundation Model for Medical Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in foundation models, such as the Segment Anything Model\n(SAM), have shown strong performance in various vision tasks, particularly\nimage segmentation, due to their impressive zero-shot segmentation\ncapabilities. However, effectively adapting such models for medical image\nclassification is still a less explored topic. In this paper, we introduce a\nnew framework to adapt SAM for medical image classification. First, we utilize\nthe SAM image encoder as a feature extractor to capture segmentation-based\nfeatures that convey important spatial and contextual details of the image,\nwhile freezing its weights to avoid unnecessary overhead during training. Next,\nwe propose a novel Spatially Localized Channel Attention (SLCA) mechanism to\ncompute spatially localized attention weights for the feature maps. The\nfeatures extracted from SAM's image encoder are processed through SLCA to\ncompute attention weights, which are then integrated into deep learning\nclassification models to enhance their focus on spatially relevant or\nmeaningful regions of the image, thus improving classification performance.\nExperimental results on three public medical image classification datasets\ndemonstrate the effectiveness and data-efficiency of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5c06Segment Anything Model (SAM) \u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\uff0c\u901a\u8fc7\u51bb\u7ed3SAM\u7f16\u7801\u5668\u6743\u91cd\u5e76\u5f15\u5165\u7a7a\u95f4\u5c40\u90e8\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236 (SLCA) \u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1SAM\u5728\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528SAM\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408SLCA\u673a\u5236\u8ba1\u7b97\u7a7a\u95f4\u5c40\u90e8\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u589e\u5f3a\u5206\u7c7b\u6a21\u578b\u5bf9\u5173\u952e\u533a\u57df\u7684\u5173\u6ce8\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06SAM\u9002\u5e94\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06219", "pdf": "https://arxiv.org/pdf/2505.06219", "abs": "https://arxiv.org/abs/2505.06219", "authors": ["Noah Frahm", "Dongxu Zhao", "Andrea Dunn Beltran", "Ron Alterovitz", "Jan-Michael Frahm", "Junier Oliva", "Roni Sengupta"], "title": "VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction", "categories": ["cs.CV", "cs.RO", "I.2.10; I.2.9"], "comment": "19 pages, 11 figures", "summary": "Next Best View (NBV) algorithms aim to acquire an optimal set of images using\nminimal resources, time, or number of captures to enable efficient 3D\nreconstruction of a scene. Existing approaches often rely on prior scene\nknowledge or additional image captures and often develop policies that maximize\ncoverage. Yet, for many real scenes with complex geometry and self-occlusions,\ncoverage maximization does not lead to better reconstruction quality directly.\nIn this paper, we propose the View Introspection Network (VIN), which is\ntrained to predict the reconstruction quality improvement of views directly,\nand the VIN-NBV policy. A greedy sequential sampling-based policy, where at\neach acquisition step, we sample multiple query views and choose the one with\nthe highest VIN predicted improvement score. We design the VIN to perform\n3D-aware featurization of the reconstruction built from prior acquisitions, and\nfor each query view create a feature that can be decoded into an improvement\nscore. We then train the VIN using imitation learning to predict the\nreconstruction improvement score. We show that VIN-NBV improves reconstruction\nquality by ~30% over a coverage maximization baseline when operating with\nconstraints on the number of acquisitions or the time in motion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u56fe\u5185\u7701\u7f51\u7edc\uff08VIN\uff09\u7684NBV\u7b97\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u89c6\u56fe\u5bf9\u91cd\u5efa\u8d28\u91cf\u7684\u76f4\u63a5\u6539\u8fdb\u6765\u9009\u62e9\u6700\u4f18\u89c6\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709NBV\u7b97\u6cd5\u4f9d\u8d56\u573a\u666f\u5148\u9a8c\u77e5\u8bc6\u6216\u989d\u5916\u56fe\u50cf\u6355\u83b7\uff0c\u4e14\u901a\u5e38\u4ee5\u8986\u76d6\u7387\u4e3a\u76ee\u6807\uff0c\u4f46\u590d\u6742\u573a\u666f\u4e2d\u8986\u76d6\u7387\u6700\u5927\u5316\u672a\u5fc5\u76f4\u63a5\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u8bbe\u8ba1VIN\u7f51\u7edc\uff0c\u8fdb\u884c3D\u611f\u77e5\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u9884\u6d4b\u91cd\u5efa\u6539\u8fdb\u5206\u6570\uff1b\u63d0\u51faVIN-NBV\u7b56\u7565\uff0c\u8d2a\u5a6a\u9009\u62e9\u6539\u8fdb\u5206\u6570\u6700\u9ad8\u7684\u89c6\u56fe\u3002", "result": "VIN-NBV\u5728\u6355\u83b7\u6b21\u6570\u6216\u65f6\u95f4\u53d7\u9650\u65f6\uff0c\u91cd\u5efa\u8d28\u91cf\u6bd4\u8986\u76d6\u7387\u6700\u5927\u5316\u57fa\u7ebf\u63d0\u5347\u7ea630%\u3002", "conclusion": "VIN-NBV\u901a\u8fc7\u76f4\u63a5\u9884\u6d4b\u91cd\u5efa\u6539\u8fdb\u5206\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u76843D\u91cd\u5efa\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2505.05477", "pdf": "https://arxiv.org/pdf/2505.05477", "abs": "https://arxiv.org/abs/2505.05477", "authors": ["Sainan xiao", "Wangdong Yang", "Buwen Cao", "Jintao Wu"], "title": "ECGDeDRDNet: A deep learning-based method for Electrocardiogram noise removal using a double recurrent dense network", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Electrocardiogram (ECG) signals are frequently corrupted by noise, such as\nbaseline wander (BW), muscle artifacts (MA), and electrode motion (EM), which\nsignificantly degrade their diagnostic utility. To address this issue, we\npropose ECGDeDRDNet, a deep learning-based ECG Denoising framework leveraging a\nDouble Recurrent Dense Network architecture. In contrast to traditional\napproaches, we introduce a double recurrent scheme to enhance information reuse\nfrom both ECG waveforms and the estimated clean image. For ECG waveform\nprocessing, our basic model employs LSTM layers cascaded with DenseNet blocks.\nThe estimated clean ECG image, obtained by subtracting predicted noise\ncomponents from the noisy input, is iteratively fed back into the model. This\ndual recurrent architecture enables comprehensive utilization of both temporal\nwaveform features and spatial image details, leading to more effective noise\nsuppression. Experimental results on the MIT-BIH dataset demonstrate that our\nmethod achieves superior performance compared to conventional image denoising\nmethods in terms of PSNR and SSIM while also surpassing classical ECG denoising\ntechniques in both SNR and RMSE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684ECG\u53bb\u566a\u6846\u67b6ECGDeDRDNet\uff0c\u91c7\u7528\u53cc\u5faa\u73af\u5bc6\u96c6\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6ce2\u5f62\u548c\u56fe\u50cf\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u53bb\u566a\u6548\u679c\u3002", "motivation": "ECG\u4fe1\u53f7\u5e38\u53d7\u566a\u58f0\u5e72\u6270\uff08\u5982\u57fa\u7ebf\u6f02\u79fb\u3001\u808c\u8089\u4f2a\u5f71\u7b49\uff09\uff0c\u5f71\u54cd\u8bca\u65ad\u4ef7\u503c\uff0c\u9700\u8981\u9ad8\u6548\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LSTM\u5c42\u4e0eDenseNet\u5757\u7ed3\u5408\u7684\u53cc\u5faa\u73af\u67b6\u6784\uff0c\u8fed\u4ee3\u5229\u7528ECG\u6ce2\u5f62\u548c\u4f30\u8ba1\u7684\u5e72\u51c0\u56fe\u50cf\u4fe1\u606f\u3002", "result": "\u5728MIT-BIH\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u548cSSIM\u4f18\u4e8e\u4f20\u7edf\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0cSNR\u548cRMSE\u4f18\u4e8e\u7ecf\u5178ECG\u53bb\u566a\u6280\u672f\u3002", "conclusion": "ECGDeDRDNet\u901a\u8fc7\u53cc\u5faa\u73af\u8bbe\u8ba1\u6709\u6548\u7ed3\u5408\u65f6\u7a7a\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347ECG\u53bb\u566a\u6027\u80fd\u3002"}}
{"id": "2505.05504", "pdf": "https://arxiv.org/pdf/2505.05504", "abs": "https://arxiv.org/abs/2505.05504", "authors": ["Xingyu Jiang", "Ning Gao", "Xiuhui Zhang", "Hongkun Dou", "Shaowen Fu", "Xiaoqing Zhong", "Hongjue Li", "Yue Deng"], "title": "Image Restoration via Multi-domain Learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Due to adverse atmospheric and imaging conditions, natural images suffer from\nvarious degradation phenomena. Consequently, image restoration has emerged as a\nkey solution and garnered substantial attention. Although recent Transformer\narchitectures have demonstrated impressive success across various restoration\ntasks, their considerable model complexity poses significant challenges for\nboth training and real-time deployment. Furthermore, instead of investigating\nthe commonalities among different degradations, most existing restoration\nmethods focus on modifying Transformer under limited restoration priors. In\nthis work, we first review various degradation phenomena under multi-domain\nperspective, identifying common priors. Then, we introduce a novel restoration\nframework, which integrates multi-domain learning into Transformer.\nSpecifically, in Token Mixer, we propose a Spatial-Wavelet-Fourier multi-domain\nstructure that facilitates local-region-global multi-receptive field modeling\nto replace vanilla self-attention. Additionally, in Feed-Forward Network, we\nincorporate multi-scale learning to fuse multi-domain features at different\nresolutions. Comprehensive experimental results across ten restoration tasks,\nsuch as dehazing, desnowing, motion deblurring, defocus deblurring, rain\nstreak/raindrop removal, cloud removal, shadow removal, underwater enhancement\nand low-light enhancement, demonstrate that our proposed model outperforms\nstate-of-the-art methods and achieves a favorable trade-off among restoration\nperformance, parameter size, computational cost and inference latency. The code\nis available at: https://github.com/deng-ai-lab/SWFormer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u57df\u5b66\u4e60\u7684Transformer\u6846\u67b6\uff0c\u7528\u4e8e\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u57df\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u81ea\u7136\u56fe\u50cf\u56e0\u5927\u6c14\u548c\u6210\u50cf\u6761\u4ef6\u5bfc\u81f4\u591a\u79cd\u9000\u5316\u73b0\u8c61\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u4e00\u4efb\u52a1\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\uff0c\u7f3a\u4e4f\u5bf9\u9000\u5316\u5171\u6027\u7684\u7814\u7a76\u3002", "method": "\u63d0\u51faSpatial-Wavelet-Fourier\u591a\u57df\u7ed3\u6784\u66ff\u4ee3\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\uff0c\u5e76\u5728\u524d\u9988\u7f51\u7edc\u4e2d\u878d\u5165\u591a\u5c3a\u5ea6\u5b66\u4e60\u3002", "result": "\u5728\u5341\u9879\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u3001\u53c2\u6570\u89c4\u6a21\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u591a\u57df\u5b66\u4e60\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u6548\u679c\uff0c\u4e3a\u590d\u6742\u9000\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05509", "pdf": "https://arxiv.org/pdf/2505.05509", "abs": "https://arxiv.org/abs/2505.05509", "authors": ["Yi Liu", "Xinyi Liu", "Panwang Xia", "Qiong Wu", "Yi Wan", "Yongjun Zhang"], "title": "StereoINR: Cross-View Geometry Consistent Stereo Super Resolution with Implicit Neural Representation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Stereo image super-resolution (SSR) aims to enhance high-resolution details\nby leveraging information from stereo image pairs. However, existing stereo\nsuper-resolution (SSR) upsampling methods (e.g., pixel shuffle) often overlook\ncross-view geometric consistency and are limited to fixed-scale upsampling. The\nkey issue is that previous upsampling methods use convolution to independently\nprocess deep features of different views, lacking cross-view and non-local\ninformation perception, making it difficult to select beneficial information\nfrom multi-view scenes adaptively. In this work, we propose Stereo Implicit\nNeural Representation (StereoINR), which innovatively models stereo image pairs\nas continuous implicit representations. This continuous representation breaks\nthrough the scale limitations, providing a unified solution for arbitrary-scale\nstereo super-resolution reconstruction of left-right views. Furthermore, by\nincorporating spatial warping and cross-attention mechanisms, StereoINR enables\neffective cross-view information fusion and achieves significant improvements\nin pixel-level geometric consistency. Extensive experiments across multiple\ndatasets show that StereoINR outperforms out-of-training-distribution scale\nupsampling and matches state-of-the-art SSR methods within\ntraining-distribution scales.", "AI": {"tldr": "\u63d0\u51faStereoINR\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5b9e\u73b0\u4efb\u610f\u5c3a\u5ea6\u7acb\u4f53\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u7ed3\u5408\u7a7a\u95f4\u626d\u66f2\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7acb\u4f53\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5ffd\u89c6\u8de8\u89c6\u89d2\u51e0\u4f55\u4e00\u81f4\u6027\u4e14\u4ec5\u9650\u4e8e\u56fa\u5b9a\u5c3a\u5ea6\u4e0a\u91c7\u6837\uff0c\u7f3a\u4e4f\u8de8\u89c6\u89d2\u548c\u975e\u5c40\u90e8\u4fe1\u606f\u611f\u77e5\u3002", "method": "\u63d0\u51faStereoINR\uff0c\u5c06\u7acb\u4f53\u56fe\u50cf\u5bf9\u5efa\u6a21\u4e3a\u8fde\u7eed\u9690\u5f0f\u8868\u793a\uff0c\u7ed3\u5408\u7a7a\u95f4\u626d\u66f2\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u8de8\u89c6\u89d2\u4fe1\u606f\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cStereoINR\u5728\u8bad\u7ec3\u5206\u5e03\u5185\u5916\u5c3a\u5ea6\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u51e0\u4f55\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "StereoINR\u4e3a\u7acb\u4f53\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u5c3a\u5ea6\u9650\u5236\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.05510", "pdf": "https://arxiv.org/pdf/2505.05510", "abs": "https://arxiv.org/abs/2505.05510", "authors": ["Thomas Sommariva", "Simone Calderara", "Angelo Porrello"], "title": "How to Train Your Metamorphic Deep Neural Network", "categories": ["cs.NE", "cs.CV", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural\nnetworks of varying width and depth. Based on Implicit Neural Representation\n(INR), NeuMeta learns a continuous weight manifold, enabling the direct\ngeneration of compressed models, including those with configurations not seen\nduring training. While promising, the original formulation of NeuMeta proves\neffective only for the final layers of the undelying model, limiting its\nbroader applicability. In this work, we propose a training algorithm that\nextends the capabilities of NeuMeta to enable full-network metamorphosis with\nminimal accuracy degradation. Our approach follows a structured recipe\ncomprising block-wise incremental training, INR initialization, and strategies\nfor replacing batch normalization. The resulting metamorphic networks maintain\ncompetitive accuracy across a wide range of compression ratios, offering a\nscalable solution for adaptable and efficient deployment of deep models. The\ncode is available at: https://github.com/TSommariva/HTTY_NeuMeta.", "AI": {"tldr": "NeuMeta\u662f\u4e00\u79cd\u57fa\u4e8eINR\u7684\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u65b9\u6cd5\uff0c\u652f\u6301\u4e0d\u540c\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u7f51\u7edc\u751f\u6210\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u7b97\u6cd5\uff0c\u6269\u5c55NeuMeta\u80fd\u529b\u4ee5\u5b9e\u73b0\u5168\u7f51\u7edc\u53d8\u5f62\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u539f\u59cbNeuMeta\u4ec5\u9002\u7528\u4e8e\u6a21\u578b\u7684\u6700\u540e\u51e0\u5c42\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55\u5176\u80fd\u529b\uff0c\u5b9e\u73b0\u5168\u7f51\u7edc\u53d8\u5f62\u3002", "method": "\u91c7\u7528\u5206\u5757\u589e\u91cf\u8bad\u7ec3\u3001INR\u521d\u59cb\u5316\u548c\u66ff\u6362\u6279\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u63d0\u5347NeuMeta\u7684\u5168\u7f51\u7edc\u53d8\u5f62\u80fd\u529b\u3002", "result": "\u751f\u6210\u7684\u53d8\u5f62\u7f51\u7edc\u5728\u591a\u79cd\u538b\u7f29\u6bd4\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u9002\u7528\u4e8e\u9ad8\u6548\u90e8\u7f72\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u6a21\u578b\u7684\u53ef\u9002\u5e94\u548c\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05518", "pdf": "https://arxiv.org/pdf/2505.05518", "abs": "https://arxiv.org/abs/2505.05518", "authors": ["Jaeyoung Huh", "Ankur Kapoor", "Young-Ho Kim"], "title": "Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility", "categories": ["eess.IV", "cs.CV", "cs.RO"], "comment": null, "summary": "Intra-cardiac Echocardiography (ICE) plays a critical role in\nElectrophysiology (EP) and Structural Heart Disease (SHD) interventions by\nproviding real-time visualization of intracardiac structures. However,\nmaintaining continuous visibility of the therapy device tip remains a challenge\ndue to frequent adjustments required during manual ICE catheter manipulation.\nTo address this, we propose an AI-driven tracking model that estimates the\ndevice tip incident angle and passing point within the ICE imaging plane,\nensuring continuous visibility and facilitating robotic ICE catheter control.\n  A key innovation of our approach is the hybrid dataset generation strategy,\nwhich combines clinical ICE sequences with synthetic data augmentation to\nenhance model robustness. We collected ICE images in a water chamber setup,\nequipping both the ICE catheter and device tip with electromagnetic (EM)\nsensors to establish precise ground-truth locations. Synthetic sequences were\ncreated by overlaying catheter tips onto real ICE images, preserving motion\ncontinuity while simulating diverse anatomical scenarios. The final dataset\nconsists of 5,698 ICE-tip image pairs, ensuring comprehensive training\ncoverage.\n  Our model architecture integrates a pretrained ultrasound (US) foundation\nmodel, trained on 37.4M echocardiography images, for feature extraction. A\ntransformer-based network processes sequential ICE frames, leveraging\nhistorical passing points and incident angles to improve prediction accuracy.\n  Experimental results demonstrate that our method achieves 3.32 degree entry\nangle error, 12.76 degree rotation angle error. This AI-driven framework lays\nthe foundation for real-time robotic ICE catheter adjustments, minimizing\noperator workload while ensuring consistent therapy device visibility. Future\nwork will focus on expanding clinical datasets to further enhance model\ngeneralization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cdAI\u9a71\u52a8\u7684\u8ddf\u8e2a\u6a21\u578b\uff0c\u901a\u8fc7\u4f30\u8ba1\u8bbe\u5907\u5c16\u7aef\u5165\u5c04\u89d2\u548c\u901a\u8fc7\u70b9\uff0c\u786e\u4fdd\u5728ICE\u6210\u50cf\u5e73\u9762\u4e2d\u7684\u8fde\u7eed\u53ef\u89c1\u6027\uff0c\u5e76\u652f\u6301\u673a\u5668\u4ebaICE\u5bfc\u7ba1\u63a7\u5236\u3002", "motivation": "\u624b\u52a8ICE\u5bfc\u7ba1\u64cd\u4f5c\u4e2d\uff0c\u4fdd\u6301\u6cbb\u7597\u8bbe\u5907\u5c16\u7aef\u7684\u8fde\u7eed\u53ef\u89c1\u6027\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u9891\u7e41\u8c03\u6574\u3002", "method": "\u7ed3\u5408\u4e34\u5e8aICE\u5e8f\u5217\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\u751f\u6210\u6df7\u5408\u6570\u636e\u96c6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u8d85\u58f0\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8etransformer\u7684\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e863.32\u5ea6\u7684\u5165\u5c04\u89d2\u8bef\u5dee\u548c12.76\u5ea6\u7684\u65cb\u8f6c\u89d2\u8bef\u5dee\u3002", "conclusion": "\u8be5AI\u6846\u67b6\u4e3a\u5b9e\u65f6\u673a\u5668\u4ebaICE\u5bfc\u7ba1\u8c03\u6574\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u5c06\u6269\u5c55\u4e34\u5e8a\u6570\u636e\u96c6\u4ee5\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.05592", "pdf": "https://arxiv.org/pdf/2505.05592", "abs": "https://arxiv.org/abs/2505.05592", "authors": ["Noriaki Hirose", "Lydia Ignatova", "Kyle Stachowicz", "Catherine Glossop", "Sergey Levine", "Dhruv Shah"], "title": "Learning to Drive Anywhere with Model-Based Reannotation11", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "19 pages, 11 figures, 8 tables", "summary": "Developing broadly generalizable visual navigation policies for robots is a\nsignificant challenge, primarily constrained by the availability of\nlarge-scale, diverse training data. While curated datasets collected by\nresearchers offer high quality, their limited size restricts policy\ngeneralization. To overcome this, we explore leveraging abundant, passively\ncollected data sources, including large volumes of crowd-sourced teleoperation\ndata and unlabeled YouTube videos, despite their potential for lower quality or\nmissing action labels. We propose Model-Based ReAnnotation (MBRA), a framework\nthat utilizes a learned short-horizon, model-based expert model to relabel or\ngenerate high-quality actions for these passive datasets. This relabeled data\nis then distilled into LogoNav, a long-horizon navigation policy conditioned on\nvisual goals or GPS waypoints. We demonstrate that LogoNav, trained using\nMBRA-processed data, achieves state-of-the-art performance, enabling robust\nnavigation over distances exceeding 300 meters in previously unseen indoor and\noutdoor environments. Our extensive real-world evaluations, conducted across a\nfleet of robots (including quadrupeds) in six cities on three continents,\nvalidate the policy's ability to generalize and navigate effectively even\namidst pedestrians in crowded settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMBRA\u6846\u67b6\uff0c\u5229\u7528\u6a21\u578b\u91cd\u65b0\u6807\u6ce8\u88ab\u52a8\u6536\u96c6\u7684\u6570\u636e\uff0c\u8bad\u7ec3\u51faLogoNav\u5bfc\u822a\u7b56\u7565\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\u6cdb\u5316\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56e0\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u6709\u9650\u3002", "method": "\u5229\u7528\u88ab\u52a8\u6536\u96c6\u7684\u6570\u636e\uff08\u5982\u4f17\u5305\u9065\u64cd\u4f5c\u6570\u636e\u548cYouTube\u89c6\u9891\uff09\uff0c\u901a\u8fc7MBRA\u6846\u67b6\u91cd\u65b0\u6807\u6ce8\u52a8\u4f5c\uff0c\u8bad\u7ec3LogoNav\u7b56\u7565\u3002", "result": "LogoNav\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5b9e\u73b0\u8d85\u8fc7300\u7c73\u7684\u7a33\u5065\u5bfc\u822a\uff0c\u5e76\u5728\u591a\u57ce\u5e02\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MBRA\u548cLogoNav\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u7b56\u7565\u7684\u6cdb\u5316\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.05631", "pdf": "https://arxiv.org/pdf/2505.05631", "abs": "https://arxiv.org/abs/2505.05631", "authors": ["Jiachen Tu", "Yaokun Shi", "Fan Lam"], "title": "Score-based Self-supervised MRI Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging\ntool that provides unparalleled soft tissue contrast and anatomical detail.\nNoise contamination, especially in accelerated and/or low-field acquisitions,\ncan significantly degrade image quality and diagnostic accuracy. Supervised\nlearning based denoising approaches have achieved impressive performance but\nrequire high signal-to-noise ratio (SNR) labels, which are often unavailable.\nSelf-supervised learning holds promise to address the label scarcity issue, but\nexisting self-supervised denoising methods tend to oversmooth fine spatial\nfeatures and often yield inferior performance than supervised methods. We\nintroduce Corruption2Self (C2S), a novel score-based self-supervised framework\nfor MRI denoising. At the core of C2S is a generalized denoising score matching\n(GDSM) loss, which extends denoising score matching to work directly with noisy\nobservations by modeling the conditional expectation of higher-SNR images given\nfurther corrupted observations. This allows the model to effectively learn\ndenoising across multiple noise levels directly from noisy data. Additionally,\nwe incorporate a reparameterization of noise levels to stabilize training and\nenhance convergence, and introduce a detail refinement extension to balance\nnoise reduction with the preservation of fine spatial features. Moreover, C2S\ncan be extended to multi-contrast denoising by leveraging complementary\ninformation across different MRI contrasts. We demonstrate that our method\nachieves state-of-the-art performance among self-supervised methods and\ncompetitive results compared to supervised counterparts across varying noise\nconditions and MRI contrasts on the M4Raw and fastMRI dataset.", "AI": {"tldr": "C2S\u662f\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u76d1\u7763MRI\u53bb\u566a\u6846\u67b6\uff0c\u901a\u8fc7\u5e7f\u4e49\u53bb\u566a\u5206\u6570\u5339\u914d\u635f\u5931\u548c\u566a\u58f0\u6c34\u5e73\u91cd\u53c2\u6570\u5316\uff0c\u76f4\u63a5\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u5b66\u4e60\u53bb\u566a\uff0c\u5e76\u5728\u591a\u566a\u58f0\u6761\u4ef6\u548cMRI\u5bf9\u6bd4\u5ea6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "MRI\u56fe\u50cf\u4e2d\u7684\u566a\u58f0\u4f1a\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u548c\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u9ad8SNR\u6807\u7b7e\uff0c\u800c\u81ea\u76d1\u7763\u65b9\u6cd5\u5bb9\u6613\u8fc7\u5ea6\u5e73\u6ed1\u7ec6\u8282\u3002C2S\u65e8\u5728\u89e3\u51b3\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u5e76\u63d0\u5347\u53bb\u566a\u6027\u80fd\u3002", "method": "\u63d0\u51faC2S\u6846\u67b6\uff0c\u91c7\u7528\u5e7f\u4e49\u53bb\u566a\u5206\u6570\u5339\u914d\u635f\u5931\uff08GDSM\uff09\u76f4\u63a5\u4ece\u566a\u58f0\u6570\u636e\u5b66\u4e60\uff0c\u5f15\u5165\u566a\u58f0\u6c34\u5e73\u91cd\u53c2\u6570\u5316\u548c\u7ec6\u8282\u7ec6\u5316\u6269\u5c55\uff0c\u652f\u6301\u591a\u5bf9\u6bd4\u5ea6\u53bb\u566a\u3002", "result": "\u5728M4Raw\u548cfastMRI\u6570\u636e\u96c6\u4e0a\uff0cC2S\u5728\u81ea\u76d1\u7763\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4\u8868\u73b0\u7ade\u4e89\u529b\u3002", "conclusion": "C2S\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u76d1\u7763MRI\u53bb\u566a\u65b9\u6cd5\uff0c\u80fd\u591f\u5e73\u8861\u566a\u58f0\u53bb\u9664\u4e0e\u7ec6\u8282\u4fdd\u7559\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u566a\u58f0\u6761\u4ef6\u548cMRI\u5bf9\u6bd4\u5ea6\u3002"}}
{"id": "2505.05643", "pdf": "https://arxiv.org/pdf/2505.05643", "abs": "https://arxiv.org/abs/2505.05643", "authors": ["Mark C. Eid", "Ana I. L. Namburete", "Jo\u00e3o F. Henriques"], "title": "UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Ultrasound imaging is widely used due to its safety, affordability, and\nreal-time capabilities, but its 2D interpretation is highly operator-dependent,\nleading to variability and increased cognitive demand. 2D-to-3D reconstruction\nmitigates these challenges by providing standardized volumetric views, yet\nexisting methods are often computationally expensive, memory-intensive, or\nincompatible with ultrasound physics. We introduce UltraGauss: the first\nultrasound-specific Gaussian Splatting framework, extending view synthesis\ntechniques to ultrasound wave propagation. Unlike conventional\nperspective-based splatting, UltraGauss models probe-plane intersections in 3D,\naligning with acoustic image formation. We derive an efficient rasterization\nboundary formulation for GPU parallelization and introduce a numerically stable\ncovariance parametrization, improving computational efficiency and\nreconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves\nstate-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20\nminutes on a single GPU. A survey of expert clinicians confirms UltraGauss'\nreconstructions are the most realistic among competing methods. Our CUDA\nimplementation will be released upon publication.", "AI": {"tldr": "UltraGauss\u662f\u4e00\u79cd\u65b0\u578b\u76842D\u52303D\u8d85\u58f0\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u5efa\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf2D\u8d85\u58f0\u6210\u50cf\u64cd\u4f5c\u4f9d\u8d56\u6027\u5f3a\u3001\u53d8\u5f02\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u73b0\u67093D\u91cd\u5efa\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u4e0d\u7b26\u5408\u8d85\u58f0\u7269\u7406\u7279\u6027\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faUltraGauss\u6846\u67b6\uff0c\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u6a21\u62df\u8d85\u58f0\u6ce2\u76843D\u4f20\u64ad\uff0c\u4f18\u5316GPU\u5e76\u884c\u5316\u548c\u6570\u503c\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e0a\uff0cUltraGauss\u57285\u5206\u949f\u5185\u5b9e\u73b0\u6700\u4f73\u91cd\u5efa\u6548\u679c\uff0c20\u5206\u949f\u5185SSIM\u8fbe0.99\uff0c\u4e13\u5bb6\u8bc4\u4ef7\u5176\u91cd\u5efa\u6548\u679c\u6700\u771f\u5b9e\u3002", "conclusion": "UltraGauss\u4e3a\u8d85\u58f0\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u76843D\u91cd\u5efa\u65b9\u6cd5\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.05647", "pdf": "https://arxiv.org/pdf/2505.05647", "abs": "https://arxiv.org/abs/2505.05647", "authors": ["Chin-Cheng Chan", "Justin P. Haldar"], "title": "A New k-Space Model for Non-Cartesian Fourier Imaging", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "For the past several decades, it has been popular to reconstruct Fourier\nimaging data using model-based approaches that can easily incorporate physical\nconstraints and advanced regularization/machine learning priors. The most\ncommon modeling approach is to represent the continuous image as a linear\ncombination of shifted \"voxel\" basis functions. Although well-studied and\nwidely-deployed, this voxel-based model is associated with longstanding\nlimitations, including high computational costs, slow convergence, and a\npropensity for artifacts. In this work, we reexamine this model from a fresh\nperspective, identifying new issues that may have been previously overlooked\n(including undesirable approximation, periodicity, and nullspace\ncharacteristics). Our insights motivate us to propose a new model that is more\nresilient to the limitations (old and new) of the previous approach.\nSpecifically, the new model is based on a Fourier-domain basis expansion rather\nthan the standard image-domain voxel-based approach. Illustrative results,\nwhich are presented in the context of non-Cartesian MRI reconstruction,\ndemonstrate that the new model enables improved image quality (reduced\nartifacts) and/or reduced computational complexity (faster computations and\nimproved convergence).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u57df\u57fa\u5c55\u5f00\u7684\u65b0\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4f53\u7d20\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u6162\u6536\u655b\u548c\u4f2a\u5f71\u95ee\u9898\uff0c\u5e76\u5728\u975e\u7b1b\u5361\u5c14MRI\u91cd\u5efa\u4e2d\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u4f53\u7d20\u6a21\u578b\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u6162\u6536\u655b\u548c\u4f2a\u5f71\u7b49\u957f\u671f\u9650\u5236\uff0c\u4e14\u53ef\u80fd\u88ab\u5ffd\u89c6\u7684\u65b0\u95ee\u9898\uff08\u5982\u8fd1\u4f3c\u8bef\u5dee\u3001\u5468\u671f\u6027\u548c\u96f6\u7a7a\u95f4\u7279\u6027\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u57df\u57fa\u5c55\u5f00\u7684\u65b0\u6a21\u578b\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u56fe\u50cf\u57df\u4f53\u7d20\u6a21\u578b\u3002", "result": "\u65b0\u6a21\u578b\u5728\u975e\u7b1b\u5361\u5c14MRI\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u56fe\u50cf\u8d28\u91cf\uff08\u51cf\u5c11\u4f2a\u5f71\uff09\u548c/\u6216\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff08\u66f4\u5feb\u8ba1\u7b97\u548c\u66f4\u597d\u6536\u655b\uff09\u3002", "conclusion": "\u65b0\u6a21\u578b\u5bf9\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff08\u5305\u62ec\u65b0\u53d1\u73b0\u7684\u95ee\u9898\uff09\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4e3a\u5085\u91cc\u53f6\u6210\u50cf\u6570\u636e\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2505.05659", "pdf": "https://arxiv.org/pdf/2505.05659", "abs": "https://arxiv.org/abs/2505.05659", "authors": ["Guilherme Vieira Neto", "Marcos Eduardo Valle"], "title": "V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "EfficientNet models are convolutional neural networks optimized for parameter\nallocation by jointly balancing network width, depth, and resolution. Renowned\nfor their exceptional accuracy, these models have become a standard for image\nclassification tasks across diverse computer vision benchmarks. While\ntraditional neural networks learn correlations between feature channels during\ntraining, vector-valued neural networks inherently treat multidimensional data\nas coherent entities, taking for granted the inter-channel relationships. This\npaper introduces vector-valued EfficientNets (V-EfficientNets), a novel\nextension of EfficientNet designed to process arbitrary vector-valued data. The\nproposed models are evaluated on a medical image classification task, achieving\nan average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute\nlymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency,\nsignificantly reducing parameters while outperforming state-of-the-art models,\nincluding the original EfficientNet. The source code is available at\nhttps://github.com/mevalle/v-nets.", "AI": {"tldr": "V-EfficientNets\u662fEfficientNet\u7684\u6269\u5c55\uff0c\u7528\u4e8e\u5904\u7406\u5411\u91cf\u503c\u6570\u636e\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8fbe99.46%\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u4e2d\u5b66\u4e60\u7279\u5f81\u901a\u9053\u95f4\u7684\u76f8\u5173\u6027\uff0c\u800c\u5411\u91cf\u503c\u795e\u7ecf\u7f51\u7edc\u5c06\u591a\u7ef4\u6570\u636e\u89c6\u4e3a\u6574\u4f53\uff0c\u5229\u7528\u901a\u9053\u95f4\u56fa\u6709\u5173\u7cfb\u3002", "method": "\u63d0\u51faV-EfficientNets\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7f51\u7edc\u5bbd\u5ea6\u3001\u6df1\u5ea6\u548c\u5206\u8fa8\u7387\uff0c\u5904\u7406\u4efb\u610f\u5411\u91cf\u503c\u6570\u636e\u3002", "result": "\u5728ALL-IDB2\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.46%\u7684\u51c6\u786e\u7387\uff0c\u53c2\u6570\u66f4\u5c11\u4e14\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "V-EfficientNets\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5411\u91cf\u503c\u6570\u636e\u4efb\u52a1\u3002"}}
{"id": "2505.05689", "pdf": "https://arxiv.org/pdf/2505.05689", "abs": "https://arxiv.org/abs/2505.05689", "authors": ["Fuyao Chen", "Yuexi Du", "Tal Zeevi", "Nicha C. Dvornek", "John A. Onofrey"], "title": "Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Accepted by MIDL 2025", "summary": "Histopathology evaluation of tissue specimens through microscopic examination\nis essential for accurate disease diagnosis and prognosis. However, traditional\nmanual analysis by specially trained pathologists is time-consuming,\nlabor-intensive, cost-inefficient, and prone to inter-rater variability,\npotentially affecting diagnostic consistency and accuracy. As digital pathology\nimages continue to proliferate, there is a pressing need for automated analysis\nto address these challenges. Recent advancements in artificial\nintelligence-based tools such as machine learning (ML) models, have\nsignificantly enhanced the precision and efficiency of analyzing\nhistopathological slides. However, despite their impressive performance, ML\nmodels are invariant only to translation, lacking invariance to rotation and\nreflection. This limitation restricts their ability to generalize effectively,\nparticularly in histopathology, where images intrinsically lack meaningful\norientation. In this study, we develop robust, equivariant histopathological\nbiomarkers through a novel symmetric convolutional kernel via unsupervised\nsegmentation. The approach is validated using prostate tissue micro-array (TMA)\nimages from 50 patients in the Gleason 2019 Challenge public dataset. The\nbiomarkers extracted through this approach demonstrate enhanced robustness and\ngeneralizability against rotation compared to models using standard convolution\nkernels, holding promise for enhancing the accuracy, consistency, and\nrobustness of ML models in digital pathology. Ultimately, this work aims to\nimprove diagnostic and prognostic capabilities of histopathology beyond\nprostate cancer through equivariant imaging.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u79f0\u5377\u79ef\u6838\u7684\u65e0\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u53d6\u5177\u6709\u65cb\u8f6c\u4e0d\u53d8\u6027\u7684\u75c5\u7406\u56fe\u50cf\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u75c5\u7406\u5b66\u5206\u6790\u8017\u65f6\u4e14\u6613\u53d7\u4e3b\u89c2\u5f71\u54cd\uff0c\u800c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u5bf9\u65cb\u8f6c\u548c\u53cd\u5c04\u7684\u4e0d\u53d8\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u75c5\u7406\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u65e0\u76d1\u7763\u5206\u5272\u548c\u5bf9\u79f0\u5377\u79ef\u6838\u5f00\u53d1\u4e86\u5177\u6709\u65cb\u8f6c\u4e0d\u53d8\u6027\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5e76\u5728\u524d\u5217\u817a\u7ec4\u7ec7\u5fae\u9635\u5217\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u53d6\u7684\u751f\u7269\u6807\u5fd7\u7269\u6bd4\u6807\u51c6\u5377\u79ef\u6838\u6a21\u578b\u66f4\u5177\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6709\u671b\u63d0\u5347\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u6269\u5c55\u81f3\u5176\u4ed6\u764c\u75c7\u7684\u8bca\u65ad\u548c\u9884\u540e\u3002"}}
{"id": "2505.05703", "pdf": "https://arxiv.org/pdf/2505.05703", "abs": "https://arxiv.org/abs/2505.05703", "authors": ["Haoyang Pei", "Ding Xia", "Xiang Xu", "William Moore", "Yao Wang", "Hersh Chandarana", "Li Feng"], "title": "Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for MRI Reconstruction without High-Quality Training Reference", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Purpose: Deep learning has demonstrated strong potential for MRI\nreconstruction, but conventional supervised learning methods require\nhigh-quality reference images, which are often unavailable in practice.\nSelf-supervised learning offers an alternative, yet its performance degrades at\nhigh acceleration rates. To overcome these limitations, we propose hybrid\nlearning, a novel two-stage training framework that combines self-supervised\nand supervised learning for robust image reconstruction.\n  Methods: Hybrid learning is implemented in two sequential stages. In the\nfirst stage, self-supervised learning is employed to generate improved images\nfrom noisy or undersampled reference data. These enhanced images then serve as\npseudo-ground truths for the second stage, which uses supervised learning to\nrefine reconstruction performance and support higher acceleration rates. We\nevaluated hybrid learning in two representative applications: (1) accelerated\n0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of\nthe brain without access to fully sampled ground truth.\n  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image\nquality over both self-supervised and conventional supervised methods across\ndifferent acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping,\nhybrid learning achieved superior T1 quantification accuracy across a wide\ndynamic range, outperforming self-supervised learning in all tested conditions.\n  Conclusions: Hybrid learning provides a practical and effective solution for\ntraining deep MRI reconstruction networks when only low-quality or incomplete\nreference data are available. It enables improved image quality and accurate\nquantitative mapping across different applications and field strengths,\nrepresenting a promising technique toward broader clinical deployment of deep\nlearning-based MRI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u548c\u76d1\u7763\u5b66\u4e60\uff0c\u7528\u4e8e\u5728\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u4f4e\u6216\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u8fdb\u884cMRI\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u9700\u8981\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\uff0c\u800c\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u9ad8\u52a0\u901f\u7387\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u6df7\u5408\u5b66\u4e60\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a\u81ea\u76d1\u7763\u5b66\u4e60\u751f\u6210\u4f2a\u771f\u503c\uff0c\u76d1\u7763\u5b66\u4e60\u8fdb\u4e00\u6b65\u4f18\u5316\u91cd\u5efa\u6027\u80fd\u3002", "result": "\u5728\u80baMRI\u548c\u8111T1\u6620\u5c04\u4e2d\uff0c\u6df7\u5408\u5b66\u4e60\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u5b9a\u91cf\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u81ea\u76d1\u7763\u548c\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u5b66\u4e60\u4e3a\u4f4e\u8d28\u91cf\u53c2\u8003\u6570\u636e\u4e0b\u7684MRI\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u4e34\u5e8a\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2505.05732", "pdf": "https://arxiv.org/pdf/2505.05732", "abs": "https://arxiv.org/abs/2505.05732", "authors": ["Limai Jiang", "Yunpeng Cai"], "title": "Automated Learning of Semantic Embedding Representations for Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "Extended version of the paper published in SDM25", "summary": "Generative models capture the true distribution of data, yielding\nsemantically rich representations. Denoising diffusion models (DDMs) exhibit\nsuperior generative capabilities, though efficient representation learning for\nthem are lacking. In this work, we employ a multi-level denoising autoencoder\nframework to expand the representation capacity of DDMs, which introduces\nsequentially consistent Diffusion Transformers and an additional\ntimestep-dependent encoder to acquire embedding representations on the\ndenoising Markov chain through self-conditional diffusion learning.\nIntuitively, the encoder, conditioned on the entire diffusion process,\ncompresses high-dimensional data into directional vectors in latent under\ndifferent noise levels, facilitating the learning of image embeddings across\nall timesteps. To verify the semantic adequacy of embeddings generated through\nthis approach, extensive experiments are conducted on various datasets,\ndemonstrating that optimally learned embeddings by DDMs surpass\nstate-of-the-art self-supervised representation learning methods in most cases,\nachieving remarkable discriminative semantic representation quality. Our work\njustifies that DDMs are not only suitable for generative tasks, but also\npotentially advantageous for general-purpose deep learning applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u53bb\u566a\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u6761\u4ef6\u6269\u6563\u5b66\u4e60\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u5d4c\u5165\u8868\u793a\u3002", "motivation": "\u53bb\u566a\u6269\u6563\u6a21\u578b\uff08DDMs\uff09\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9ad8\u6548\u8868\u793a\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6269\u5c55\u5176\u8868\u793a\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u7ea7\u53bb\u566a\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u5f15\u5165\u5e8f\u5217\u4e00\u81f4\u7684\u6269\u6563\u53d8\u6362\u5668\u548c\u65f6\u95f4\u6b65\u76f8\u5173\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u81ea\u6761\u4ef6\u6269\u6563\u5b66\u4e60\u83b7\u53d6\u5d4c\u5165\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5d4c\u5165\u8868\u793a\u5728\u8bed\u4e49\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "DDMs\u4e0d\u4ec5\u9002\u7528\u4e8e\u751f\u6210\u4efb\u52a1\uff0c\u8fd8\u5177\u6709\u901a\u7528\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05736", "pdf": "https://arxiv.org/pdf/2505.05736", "abs": "https://arxiv.org/abs/2505.05736", "authors": ["Da Wu", "Zhanliang Wang", "Quan Nguyen", "Zhuoran Xu", "Kai Wang"], "title": "Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications", "categories": ["q-bio.QM", "cs.CL", "cs.CV", "cs.LG"], "comment": "First Draft", "summary": "The scarcity of high-quality multimodal biomedical data limits the ability to\neffectively fine-tune pretrained Large Language Models (LLMs) for specialized\nbiomedical tasks. To address this challenge, we introduce MINT (Multimodal\nIntegrated kNowledge Transfer), a framework that aligns unimodal large decoder\nmodels with domain-specific decision patterns from multimodal biomedical data\nthrough preference optimization. While MINT supports different optimization\ntechniques, we primarily implement it with the Odds Ratio Preference\nOptimization (ORPO) framework as its backbone. This strategy enables the\naligned LLMs to perform predictive tasks using text-only or image-only inputs\nwhile retaining knowledge learnt from multimodal data. MINT leverages an\nupstream multimodal machine learning (MML) model trained on high-quality\nmultimodal data to transfer domain-specific insights to downstream text-only or\nimage-only LLMs. We demonstrate its effectiveness through two key applications:\n(1) Rare genetic disease prediction from texts, where MINT uses a multimodal\nencoder model, trained on facial photos and clinical notes, to generate a\npreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite\nrelying on text input only, the MINT-derived model outperforms models trained\nwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue\ntype classification using cell nucleus images, where MINT uses a\nvision-language foundation model as the preference generator, containing\nknowledge learnt from both text and histopathological images to align\ndownstream image-only models. The resulting MINT-derived model significantly\nimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue type\nclassification. In summary, MINT provides an effective strategy to align\nunimodal LLMs with high-quality multimodal expertise through preference\noptimization.", "AI": {"tldr": "MINT\u6846\u67b6\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5c06\u5355\u6a21\u6001\u5927\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u6570\u636e\u5bf9\u9f50\uff0c\u63d0\u5347\u5176\u5728\u6587\u672c\u6216\u56fe\u50cf\u8f93\u5165\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u5728\u4e13\u4e1a\u4efb\u52a1\u4e2d\u7684\u5fae\u8c03\u6548\u679c\u3002", "method": "\u91c7\u7528MINT\u6846\u67b6\uff0c\u7ed3\u5408ORPO\u504f\u597d\u4f18\u5316\uff0c\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u5c06\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u81f3\u5355\u6a21\u6001\u6a21\u578b\u3002", "result": "\u5728\u7f55\u89c1\u9057\u4f20\u75c5\u9884\u6d4b\u548c\u7ec4\u7ec7\u7c7b\u578b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cMINT\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "MINT\u4e3a\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5c06\u5355\u6a21\u6001\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u4e13\u4e1a\u77e5\u8bc6\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2505.05768", "pdf": "https://arxiv.org/pdf/2505.05768", "abs": "https://arxiv.org/abs/2505.05768", "authors": ["Weiyi Zhang", "Peranut Chotcomwongse", "Yinwen Li", "Pusheng Xu", "Ruijie Yao", "Lianhao Zhou", "Yuxuan Zhou", "Hui Feng", "Qiping Zhou", "Xinyue Wang", "Shoujin Huang", "Zihao Jin", "Florence H. T. Chung", "Shujun Wang", "Yalin Zheng", "Mingguang He", "Danli Shi", "Paisan Ruamviboonsuk"], "title": "Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "42 pages,5 tables, 12 figures, challenge report", "summary": "Diabetic macular edema (DME) significantly contributes to visual impairment\nin diabetic patients. Treatment responses to intravitreal therapies vary,\nhighlighting the need for patient stratification to predict therapeutic\nbenefits and enable personalized strategies. To our knowledge, this study is\nthe first to explore pre-treatment stratification for predicting DME treatment\nresponses. To advance this research, we organized the 2nd Asia-Pacific\nTele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The\ncompetition focused on improving predictive accuracy for anti-VEGF therapy\nresponses using ophthalmic OCT images. We provided a dataset containing tens of\nthousands of OCT images from 2,000 patients with labels across four sub-tasks.\nThis paper details the competition's structure, dataset, leading methods, and\nevaluation metrics. The competition attracted strong scientific community\nparticipation, with 170 teams initially registering and 41 reaching the final\nround. The top-performing team achieved an AUC of 80.06%, highlighting the\npotential of AI in personalized DME treatment and clinical decision-making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e9a\u592a\u8fdc\u7a0b\u773c\u79d1\u5b66\u4f1a\u5927\u6570\u636e\u7ade\u8d5b\uff0c\u63a2\u7d22\u4e86\u5229\u7528OCT\u56fe\u50cf\u9884\u6d4b\u7cd6\u5c3f\u75c5\u9ec4\u6591\u6c34\u80bf\uff08DME\uff09\u60a3\u8005\u5bf9\u6297VEGF\u6cbb\u7597\u53cd\u5e94\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86AI\u5728\u4e2a\u6027\u5316\u6cbb\u7597\u4e2d\u7684\u4ef7\u503c\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u9ec4\u6591\u6c34\u80bf\uff08DME\uff09\u60a3\u8005\u7684\u6cbb\u7597\u53cd\u5e94\u5dee\u5f02\u5927\uff0c\u9700\u8981\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7ec4\u7ec7\u5927\u6570\u636e\u7ade\u8d5b\uff0c\u63d0\u4f9b\u5305\u542b\u6570\u5343\u5f20OCT\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u9f13\u52b1\u56e2\u961f\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u7ade\u8d5b\u5438\u5f15\u4e86170\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u6700\u7ec841\u4e2a\u56e2\u961f\u8fdb\u5165\u51b3\u8d5b\uff0c\u6700\u4f73\u56e2\u961f\u7684AUC\u8fbe\u523080.06%\u3002", "conclusion": "AI\u5728\u9884\u6d4bDME\u6cbb\u7597\u53cd\u5e94\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2505.05798", "pdf": "https://arxiv.org/pdf/2505.05798", "abs": "https://arxiv.org/abs/2505.05798", "authors": ["Youngjoon Lee", "Jinu Gong", "Joonhyuk Kang"], "title": "Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes", "categories": ["cs.LG", "cs.CV", "eess.IV", "eess.SP"], "comment": "4 pages", "summary": "Kolmogorov-Arnold Networks (KAN) offer universal function approximation using\nunivariate spline compositions without nonlinear activations. In this work, we\nintegrate Error-Correcting Output Codes (ECOC) into the KAN framework to\ntransform multi-class classification into multiple binary tasks, improving\nrobustness via Hamming-distance decoding. Our proposed KAN with ECOC method\noutperforms vanilla KAN on a challenging blood cell classification dataset,\nachieving higher accuracy under diverse hyperparameter settings. Ablation\nstudies further confirm that ECOC consistently enhances performance across\nFastKAN and FasterKAN variants. These results demonstrate that ECOC integration\nsignificantly boosts KAN generalizability in critical healthcare AI\napplications. To the best of our knowledge, this is the first integration of\nECOC with KAN for enhancing multi-class medical image classification\nperformance.", "AI": {"tldr": "\u5c06ECOC\u96c6\u6210\u5230KAN\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u591a\u4e8c\u8fdb\u5236\u4efb\u52a1\u63d0\u5347\u591a\u5206\u7c7b\u6027\u80fd\uff0c\u5728\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63d0\u5347KAN\u5728\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u533b\u7597AI\u5e94\u7528\u4e2d\u3002", "method": "\u5c06ECOC\u4e0eKAN\u7ed3\u5408\uff0c\u5229\u7528\u6c49\u660e\u8ddd\u79bb\u89e3\u7801\u5c06\u591a\u5206\u7c7b\u8f6c\u5316\u4e3a\u591a\u4e2a\u4e8c\u8fdb\u5236\u4efb\u52a1\u3002", "result": "\u5728\u8840\u6db2\u7ec6\u80de\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u539f\u59cbKAN\uff0c\u4e14\u5728\u4e0d\u540c\u8d85\u53c2\u6570\u4e0b\u5747\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "ECOC\u663e\u8457\u63d0\u5347\u4e86KAN\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u7b49\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2505.05800", "pdf": "https://arxiv.org/pdf/2505.05800", "abs": "https://arxiv.org/abs/2505.05800", "authors": ["Vineet Bhat", "Yu-Hsiang Lan", "Prashanth Krishnamurthy", "Ramesh Karri", "Farshad Khorrami"], "title": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at the 1st Workshop on 3D LLM/VLA, CVPR 2025", "summary": "Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint\nspace trajectory of a robot manipulator. Robots must possess semantic and\nvisual perception abilities to transform real-world mappings of their workspace\ninto the low-level control necessary for object manipulation. Recent work has\ndemonstrated the capabilities of fine-tuning large Vision-Language Models\n(VLMs) to learn the mapping between RGB images, language instructions, and\njoint space control. These models typically take as input RGB images of the\nworkspace and language instructions, and are trained on large datasets of\nteleoperated robot demonstrations. In this work, we explore methods to improve\nthe scene context awareness of a popular recent Vision-Language-Action model by\nintegrating chain-of-thought reasoning, depth perception, and task-oriented\nregion of interest detection. Our experiments in the LIBERO simulation\nenvironment show that our proposed model, 3D-CAVLA, improves the success rate\nacross various LIBERO task suites, achieving an average success rate of\n98.1$\\%$. We also evaluate the zero-shot capabilities of our method,\ndemonstrating that 3D scene awareness leads to robust learning and adaptation\nfor completely unseen tasks. 3D-CAVLA achieves an absolute improvement of\n8.8$\\%$ on unseen tasks. We will open-source our code and the unseen tasks\ndataset to promote community-driven research here: https://3d-cavla.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u3001\u6df1\u5ea6\u611f\u77e5\u548c\u4efb\u52a1\u5bfc\u5411\u7684\u5174\u8da3\u533a\u57df\u68c0\u6d4b\uff0c\u63d0\u5347\u4e86\u573a\u666f\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728LIBERO\u4eff\u771f\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba3D\u64cd\u4f5c\u4e2d\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u66f4\u597d\u5730\u5c06\u89c6\u89c9\u548c\u8bed\u8a00\u8f93\u5165\u6620\u5c04\u5230\u4f4e\u7ea7\u522b\u63a7\u5236\u3002", "method": "\u6574\u5408\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u3001\u6df1\u5ea6\u611f\u77e5\u548c\u4efb\u52a1\u5bfc\u5411\u7684\u5174\u8da3\u533a\u57df\u68c0\u6d4b\uff0c\u6539\u8fdb\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u3002", "result": "\u5728LIBERO\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u523098.1%\uff0c\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u7edd\u5bf9\u63d0\u53478.8%\u3002", "conclusion": "3D\u573a\u666f\u611f\u77e5\u80fd\u529b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u672a\u89c1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.05812", "pdf": "https://arxiv.org/pdf/2505.05812", "abs": "https://arxiv.org/abs/2505.05812", "authors": ["Ashkan Pakzad", "Robert Turnbull", "Simon J. Mutch", "Thomas A. Leatham", "Darren Lockie", "Jane Fox", "Beena Kumar", "Daniel H\u00e4sermann", "Christopher J. Hall", "Anton Maksimenko", "Benedicta D. Arhatari", "Yakov I. Nesterets", "Amir Entezam", "Seyedamir T. Taba", "Patrick C. Brennan", "Timur E. Gureyev", "Harry M. Quiney"], "title": "Towards order of magnitude X-ray dose reduction in breast cancer imaging using phase contrast and deep denoising", "categories": ["physics.med-ph", "cs.CV"], "comment": "16 pages, 3 figures, 1 table", "summary": "Breast cancer is the most frequently diagnosed human cancer in the United\nStates at present. Early detection is crucial for its successful treatment.\nX-ray mammography and digital breast tomosynthesis are currently the main\nmethods for breast cancer screening. However, both have known limitations in\nterms of their sensitivity and specificity to breast cancers, while also\nfrequently causing patient discomfort due to the requirement for breast\ncompression. Breast computed tomography is a promising alternative, however, to\nobtain high-quality images, the X-ray dose needs to be sufficiently high. As\nthe breast is highly radiosensitive, dose reduction is particularly important.\nPhase-contrast computed tomography (PCT) has been shown to produce\nhigher-quality images at lower doses and has no need for breast compression. It\nis demonstrated in the present study that, when imaging full fresh mastectomy\nsamples with PCT, deep learning-based image denoising can further reduce the\nradiation dose by a factor of 16 or more, without any loss of image quality.\nThe image quality has been assessed both in terms of objective metrics, such as\nspatial resolution and contrast-to-noise ratio, as well as in an observer study\nby experienced medical imaging specialists and radiologists. This work was\ncarried out in preparation for live patient PCT breast cancer imaging,\ninitially at specialized synchrotron facilities.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u7ed3\u5408\u76f8\u4f4d\u5bf9\u6bd4\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08PCT\uff09\uff0c\u53ef\u5728\u964d\u4f4e16\u500d\u8f90\u5c04\u5242\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e73\u817a\u764c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u4e73\u817a\u764c\u7b5b\u67e5\u65b9\u6cd5\uff08\u5982X\u5c04\u7ebf\u4e73\u817a\u6444\u5f71\u548c\u6570\u5b57\u4e73\u817a\u65ad\u5c42\u5408\u6210\uff09\u5b58\u5728\u7075\u654f\u5ea6\u3001\u7279\u5f02\u6027\u4e0d\u8db3\u53ca\u60a3\u8005\u4e0d\u9002\u7684\u95ee\u9898\uff0cPCT\u867d\u4e3a\u6f5c\u5728\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u9ad8\u5242\u91cf\u9700\u6c42\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u4f7f\u7528PCT\u5bf9\u65b0\u9c9c\u4e73\u817a\u5207\u9664\u6837\u672c\u6210\u50cf\uff0c\u5e76\u5e94\u7528\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u53bb\u566a\u6280\u672f\uff0c\u8bc4\u4f30\u5176\u5728\u964d\u4f4e\u8f90\u5c04\u5242\u91cf\u65f6\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6df1\u5ea6\u5b66\u4e60\u53bb\u566a\u53ef\u5c06PCT\u8f90\u5c04\u5242\u91cf\u964d\u4f4e16\u500d\u4ee5\u4e0a\uff0c\u4e14\u56fe\u50cf\u8d28\u91cf\uff08\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u5bf9\u6bd4\u566a\u58f0\u6bd4\uff09\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u5728\u540c\u6b65\u8f90\u5c04\u8bbe\u65bd\u4e2d\u5f00\u5c55\u6d3b\u4f53\u60a3\u8005PCT\u4e73\u817a\u764c\u6210\u50cf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.05957", "pdf": "https://arxiv.org/pdf/2505.05957", "abs": "https://arxiv.org/abs/2505.05957", "authors": ["Peter R\u00f6seler", "Oliver Schaudt", "Helmut Berg", "Christian Bauckhage", "Matthias Koch"], "title": "Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints", "categories": ["quant-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "While classical convolutional neural networks (CNNs) have revolutionized\nimage classification, the emergence of quantum computing presents new\nopportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)\nleverage quantum mechanical properties and hold potential to outperform\nclassical approaches. However, their implementation on current noisy\nintermediate-scale quantum (NISQ) devices remains challenging due to hardware\nlimitations. In our research, we address this challenge by introducing an\nencoding scheme that significantly reduces the input dimensionality. We\ndemonstrate that a primitive QCNN architecture with 49 qubits is sufficient to\ndirectly process $28\\times 28$ pixel MNIST images, eliminating the need for\nclassical dimensionality reduction pre-processing. Additionally, we propose an\nautomated framework based on expressibility, entanglement, and complexity\ncharacteristics to identify the building blocks of QCNNs, parameterized quantum\ncircuits (PQCs). Our approach demonstrates advantages in accuracy and\nconvergence speed with a similar parameter count compared to both hybrid QCNNs\nand classical CNNs. We validated our experiments on IBM's Heron r2 quantum\nprocessor, achieving $96.08\\%$ classification accuracy, surpassing the\n$71.74\\%$ benchmark of traditional approaches under identical training\nconditions. These results represent one of the first implementations of image\nclassifications on real quantum hardware and validate the potential of quantum\ncomputing in this area.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QCNN\uff09\u7684\u7f16\u7801\u65b9\u6848\uff0c\u51cf\u5c11\u8f93\u5165\u7ef4\u5ea6\uff0c\u5e76\u5728NISQ\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u56fe\u50cf\u5206\u7c7b\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u4e3a\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5f53\u524dNISQ\u8bbe\u5907\u7684\u786c\u4ef6\u9650\u5236\u963b\u788d\u4e86QCNN\u7684\u5b9e\u73b0\u3002", "method": "\u5f15\u5165\u7f16\u7801\u65b9\u6848\u964d\u4f4e\u8f93\u5165\u7ef4\u5ea6\uff0c\u63d0\u51fa\u57fa\u4e8e\u53ef\u8868\u8fbe\u6027\u3001\u7ea0\u7f20\u548c\u590d\u6742\u5ea6\u7684\u81ea\u52a8\u5316\u6846\u67b6\u8bbe\u8ba1QCNN\u3002", "result": "\u5728IBM Heron r2\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u5b9e\u73b096.08%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u768471.74%\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.06020", "pdf": "https://arxiv.org/pdf/2505.06020", "abs": "https://arxiv.org/abs/2505.06020", "authors": ["Shuai Wang", "Ivona Najdenkoska", "Hongyi Zhu", "Stevan Rudinac", "Monika Kackovic", "Nachoem Wijnberg", "Marcel Worring"], "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations.", "AI": {"tldr": "ArtRAG\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u4e3a\u827a\u672f\u54c1\u63d0\u4f9b\u591a\u89c6\u89d2\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u827a\u672f\u54c1\u89e3\u91ca\u4e2d\u7f3a\u4e4f\u6587\u5316\u3001\u5386\u53f2\u548c\u98ce\u683c\u7684\u6df1\u5ea6\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u827a\u672f\u4e0a\u4e0b\u6587\u77e5\u8bc6\u56fe\uff08ACKG\uff09\u548c\u68c0\u7d22\u76f8\u5173\u5b50\u56fe\uff0c\u5f15\u5bfcMLLMs\u751f\u6210\u89e3\u91ca\u3002", "result": "\u5728SemArt\u548cArtpedia\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u751f\u6210\u5185\u5bb9\u66f4\u5177\u6587\u5316\u6df1\u5ea6\u548c\u8fde\u8d2f\u6027\u3002", "conclusion": "ArtRAG\u4e3a\u827a\u672f\u54c1\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6587\u5316\u654f\u611f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.06030", "pdf": "https://arxiv.org/pdf/2505.06030", "abs": "https://arxiv.org/abs/2505.06030", "authors": ["Tobias Preintner", "Weixuan Yuan", "Qi Huang", "Adrian K\u00f6nig", "Thomas B\u00e4ck", "Elena Raponi", "Niki van Stein"], "title": "Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Combining natural language and geometric shapes is an emerging research area\nwith multiple applications in robotics and language-assisted design. A crucial\ntask in this domain is object referent identification, which involves selecting\na 3D object given a textual description of the target. Variability in language\ndescriptions and spatial relationships of 3D objects makes this a complex task,\nincreasing the need to better understand the behavior of neural network models\nin this domain. However, limited research has been conducted in this area.\nSpecifically, when a model makes an incorrect prediction despite being provided\nwith a seemingly correct object description, practitioners are left wondering:\n\"Why is the model wrong?\". In this work, we present a method answering this\nquestion by generating counterfactual examples. Our method takes a\nmisclassified sample, which includes two objects and a text description, and\ngenerates an alternative yet similar formulation that would have resulted in a\ncorrect prediction by the model. We have evaluated our approach with data from\nthe ShapeTalk dataset along with three distinct models. Our counterfactual\nexamples maintain the structure of the original description, are semantically\nsimilar and meaningful. They reveal weaknesses in the description, model bias\nand enhance the understanding of the models behavior. Theses insights help\npractitioners to better interact with systems as well as engineers to improve\nmodels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u53cd\u4e8b\u5b9e\u793a\u4f8b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u91ca\u6a21\u578b\u5728\u5bf9\u8c61\u6307\u4ee3\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u9884\u6d4b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u8bed\u8a00\u63cf\u8ff0\u548c3D\u5bf9\u8c61\u7a7a\u95f4\u5173\u7cfb\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u9884\u6d4b\u9519\u8bef\u65f6\u63d0\u4f9b\u89e3\u91ca\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9e\u793a\u4f8b\uff0c\u5373\u5bf9\u9519\u8bef\u5206\u7c7b\u6837\u672c\u751f\u6210\u8bed\u4e49\u76f8\u4f3c\u4f46\u80fd\u5bfc\u81f4\u6b63\u786e\u9884\u6d4b\u7684\u66ff\u4ee3\u63cf\u8ff0\u3002", "result": "\u5b9e\u9a8c\u5728ShapeTalk\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\uff0c\u7ed3\u679c\u663e\u793a\u53cd\u4e8b\u5b9e\u793a\u4f8b\u80fd\u63ed\u793a\u63cf\u8ff0\u5f31\u70b9\u3001\u6a21\u578b\u504f\u5dee\uff0c\u5e76\u589e\u5f3a\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u7406\u89e3\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u5b9e\u8df5\u8005\u66f4\u597d\u5730\u4e0e\u7cfb\u7edf\u4ea4\u4e92\uff0c\u5e76\u5e2e\u52a9\u5de5\u7a0b\u5e08\u6539\u8fdb\u6a21\u578b\u3002"}}
{"id": "2505.06079", "pdf": "https://arxiv.org/pdf/2505.06079", "abs": "https://arxiv.org/abs/2505.06079", "authors": ["Shuaiyi Huang", "Mara Levy", "Anubhav Gupta", "Daniel Ekpo", "Ruijie Zheng", "Abhinav Shrivastava"], "title": "TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations", "categories": ["cs.RO", "cs.CV"], "comment": "ICRA 2025", "summary": "Preference feedback collected by human or VLM annotators is often noisy,\npresenting a significant challenge for preference-based reinforcement learning\nthat relies on accurate preference labels. To address this challenge, we\npropose TREND, a novel framework that integrates few-shot expert demonstrations\nwith a tri-teaching strategy for effective noise mitigation. Our method trains\nthree reward models simultaneously, where each model views its small-loss\npreference pairs as useful knowledge and teaches such useful pairs to its peer\nnetwork for updating the parameters. Remarkably, our approach requires as few\nas one to three expert demonstrations to achieve high performance. We evaluate\nTREND on various robotic manipulation tasks, achieving up to 90% success rates\neven with noise levels as high as 40%, highlighting its effective robustness in\nhandling noisy preference feedback. Project page:\nhttps://shuaiyihuang.github.io/publications/TREND.", "AI": {"tldr": "TREND\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5c11\u91cf\u4e13\u5bb6\u6f14\u793a\u548c\u4e09\u91cd\u6559\u5b66\u7b56\u7565\uff0c\u6709\u6548\u51cf\u5c11\u504f\u597d\u53cd\u9988\u4e2d\u7684\u566a\u58f0\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u504f\u597d\u53cd\u9988\u4e2d\u7684\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u9ad8\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faTREND\u6846\u67b6\uff0c\u8bad\u7ec3\u4e09\u4e2a\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5c0f\u635f\u5931\u504f\u597d\u5bf9\u4e92\u76f8\u6559\u5b66\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u5373\u4f7f\u566a\u58f0\u9ad8\u8fbe40%\uff0c\u6210\u529f\u7387\u4ecd\u8fbe90%\u3002", "conclusion": "TREND\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u9700\u5c11\u91cf\u4e13\u5bb6\u6f14\u793a\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2505.06105", "pdf": "https://arxiv.org/pdf/2505.06105", "abs": "https://arxiv.org/abs/2505.06105", "authors": ["Xilin Gong", "Yongkai Chen", "Shushan Wu", "Fang Wang", "Ping Ma", "Wenxuan Zhong"], "title": "S2MNet: Speckle-To-Mesh Net for Three-Dimensional Cardiac Morphology Reconstruction via Echocardiogram", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Echocardiogram is the most commonly used imaging modality in cardiac\nassessment duo to its non-invasive nature, real-time capability, and\ncost-effectiveness. Despite its advantages, most clinical echocardiograms\nprovide only two-dimensional views, limiting the ability to fully assess\ncardiac anatomy and function in three dimensions. While three-dimensional\nechocardiography exists, it often suffers from reduced resolution, limited\navailability, and higher acquisition costs. To overcome these challenges, we\npropose a deep learning framework S2MNet that reconstructs continuous and\nhigh-fidelity 3D heart models by integrating six slices of routinely acquired\n2D echocardiogram views. Our method has three advantages. First, our method\navoid the difficulties on training data acquasition by simulate six of 2D\nechocardiogram images from corresponding slices of a given 3D heart mesh.\nSecond, we introduce a deformation field-based method, which avoid spatial\ndiscontinuities or structural artifacts in 3D echocardiogram reconstructions.\nWe validate our method using clinically collected echocardiogram and\ndemonstrate that our estimated left ventricular volume, a key clinical\nindicator of cardiac function, is strongly correlated with the doctor measured\nGLPS, a clinical measurement that should demonstrate a negative correlation\nwith LVE in medical theory. This association confirms the reliability of our\nproposed 3D construction method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS2MNet\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u516d\u5f20\u5e38\u89c42D\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u56fe\uff0c\u91cd\u5efa\u8fde\u7eed\u4e14\u9ad8\u4fdd\u771f\u76843D\u5fc3\u810f\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u8d85\u58f0\u5fc3\u52a8\u56fe\u56e0\u5176\u65e0\u521b\u6027\u3001\u5b9e\u65f6\u6027\u548c\u6210\u672c\u6548\u76ca\u6210\u4e3a\u5fc3\u810f\u8bc4\u4f30\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u51762D\u89c6\u56fe\u9650\u5236\u4e86\u4e09\u7ef4\u89e3\u5256\u548c\u529f\u80fd\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u800c\u73b0\u67093D\u6280\u672f\u5b58\u5728\u5206\u8fa8\u7387\u4f4e\u3001\u53ef\u7528\u6027\u6709\u9650\u548c\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "S2MNet\u901a\u8fc7\u6a21\u62df\u516d\u5f202D\u8d85\u58f0\u5fc3\u52a8\u56fe\u56fe\u50cf\u6765\u907f\u514d\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u7684\u56f0\u96be\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u53d8\u5f62\u573a\u7684\u65b9\u6cd5\u4ee5\u907f\u514d\u7a7a\u95f4\u4e0d\u8fde\u7eed\u6216\u7ed3\u6784\u4f2a\u5f71\u3002", "result": "\u9a8c\u8bc1\u8868\u660e\uff0c\u91cd\u5efa\u7684\u5de6\u5fc3\u5ba4\u4f53\u79ef\u4e0e\u533b\u751f\u6d4b\u91cf\u7684GLPS\uff08\u4e00\u79cd\u4e34\u5e8a\u6307\u6807\uff09\u5448\u5f3a\u76f8\u5173\u6027\uff0c\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u3002", "conclusion": "S2MNet\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u9ad8\u6548\u76843D\u5fc3\u810f\u6a21\u578b\u91cd\u5efa\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.06118", "pdf": "https://arxiv.org/pdf/2505.06118", "abs": "https://arxiv.org/abs/2505.06118", "authors": ["Jingguo Qu", "Xinyang Han", "Man-Lik Chui", "Yao Pu", "Simon Takadiyi Gunda", "Ziman Chen", "Jing Qin", "Ann Dorothy King", "Winnie Chiu-Wing Chu", "Jing Cai", "Michael Tin-Cheung Ying"], "title": "The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Automatic lymph node segmentation is the cornerstone for advances in computer\nvision tasks for early detection and staging of cancer. Traditional\nsegmentation methods are constrained by manual delineation and variability in\noperator proficiency, limiting their ability to achieve high accuracy. The\nintroduction of deep learning technologies offers new possibilities for\nimproving the accuracy of lymph node image analysis. This study evaluates the\napplication of deep learning in lymph node segmentation and discusses the\nmethodologies of various deep learning architectures such as convolutional\nneural networks, encoder-decoder networks, and transformers in analyzing\nmedical imaging data across different modalities. Despite the advancements, it\nstill confronts challenges like the shape diversity of lymph nodes, the\nscarcity of accurately labeled datasets, and the inadequate development of\nmethods that are robust and generalizable across different imaging modalities.\nTo the best of our knowledge, this is the first study that provides a\ncomprehensive overview of the application of deep learning techniques in lymph\nnode segmentation task. Furthermore, this study also explores potential future\nresearch directions, including multimodal fusion techniques, transfer learning,\nand the use of large-scale pre-trained models to overcome current limitations\nwhile enhancing cancer diagnosis and treatment planning strategies.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u6dcb\u5df4\u7ed3\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u67b6\u6784\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u6dcb\u5df4\u7ed3\u5206\u5272\u65b9\u6cd5\u53d7\u9650\u4e8e\u4eba\u5de5\u6807\u6ce8\u548c\u64cd\u4f5c\u8005\u6c34\u5e73\uff0c\u6df1\u5ea6\u5b66\u4e60\u4e3a\u63d0\u9ad8\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002", "method": "\u8bc4\u4f30\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u548cTransformer\u7b49\u67b6\u6784\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5c3d\u7ba1\u6709\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u6dcb\u5df4\u7ed3\u5f62\u72b6\u591a\u6837\u3001\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u53ca\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u6dcb\u5df4\u7ed3\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u591a\u6a21\u6001\u878d\u5408\u548c\u8fc1\u79fb\u5b66\u4e60\u3002"}}
{"id": "2505.06123", "pdf": "https://arxiv.org/pdf/2505.06123", "abs": "https://arxiv.org/abs/2505.06123", "authors": ["Philip Naumann", "Jacob Kauffmann", "Gr\u00e9goire Montavon"], "title": "Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Wasserstein distances provide a powerful framework for comparing data\ndistributions. They can be used to analyze processes over time or to detect\ninhomogeneities within data. However, simply calculating the Wasserstein\ndistance or analyzing the corresponding transport map (or coupling) may not be\nsufficient for understanding what factors contribute to a high or low\nWasserstein distance. In this work, we propose a novel solution based on\nExplainable AI that allows us to efficiently and accurately attribute\nWasserstein distances to various data components, including data subgroups,\ninput features, or interpretable subspaces. Our method achieves high accuracy\nacross diverse datasets and Wasserstein distance specifications, and its\npractical utility is demonstrated in two use cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u5f52\u56e0Wasserstein\u8ddd\u79bb\u5230\u6570\u636e\u7684\u4e0d\u540c\u7ec4\u6210\u90e8\u5206\u3002", "motivation": "\u5355\u7eaf\u8ba1\u7b97Wasserstein\u8ddd\u79bb\u6216\u5206\u6790\u5176\u4f20\u8f93\u56fe\u4e0d\u8db3\u4ee5\u7406\u89e3\u5f71\u54cd\u8ddd\u79bb\u9ad8\u4f4e\u7684\u56e0\u7d20\u3002", "method": "\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f52\u56e0Wasserstein\u8ddd\u79bb\u5230\u6570\u636e\u5b50\u7ec4\u3001\u8f93\u5165\u7279\u5f81\u6216\u53ef\u89e3\u91ca\u5b50\u7a7a\u95f4\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u548cWasserstein\u8ddd\u79bb\u89c4\u683c\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u7528\u4f8b\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3Wasserstein\u8ddd\u79bb\u7684\u8d21\u732e\u56e0\u7d20\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.06176", "pdf": "https://arxiv.org/pdf/2505.06176", "abs": "https://arxiv.org/abs/2505.06176", "authors": ["Niladri Shekhar Dutt", "Duygu Ceylan", "Niloy J. Mitra"], "title": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted at SIGGRAPH 2025 [ACM Transactions on Graphics]; Project\n  website: https://monetgpt.github.io", "summary": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8fdb\u884c\u7167\u7247\u540e\u671f\u4fee\u9970\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u7406\u89e3\u56fe\u50cf\u5904\u7406\u64cd\u4f5c\u5e76\u751f\u6210\u7f16\u8f91\u5e8f\u5217\uff0c\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u5f0f\u548c\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4fee\u9970\u65b9\u6cd5\u4fdd\u5b88\u4f46\u4e13\u4e1a\uff0c\u800c\u751f\u6210\u5f0f\u7f16\u8f91\u6613\u6539\u53d8\u539f\u59cb\u5bf9\u8c61\u8eab\u4efd\u3002\u7814\u7a76\u65e8\u5728\u7ed3\u5408MLLM\u7684\u80fd\u529b\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u4fdd\u7559\u7ec6\u8282\u7684\u4fee\u9970\u65b9\u6848\u3002", "method": "\u8bad\u7ec3MLLM\u901a\u8fc7\u89c6\u89c9\u8c1c\u9898\u7406\u89e3\u56fe\u50cf\u64cd\u4f5c\uff0c\u5229\u7528\u4e13\u5bb6\u7f16\u8f91\u7167\u7247\u5408\u6210\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u6a21\u578b\u4ee5\u89c4\u5212\u548c\u751f\u6210\u7f16\u8f91\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8eab\u4efd\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u751f\u6210\u5f0f\u548c\u5176\u4ed6\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "MLLM\u53ef\u6709\u6548\u7528\u4e8e\u7167\u7247\u4fee\u9970\uff0c\u63d0\u4f9b\u7528\u6237\u53ef\u7406\u89e3\u4e14\u7075\u6d3b\u7684\u7f16\u8f91\u65b9\u6848\u3002"}}
{"id": "2505.06185", "pdf": "https://arxiv.org/pdf/2505.06185", "abs": "https://arxiv.org/abs/2505.06185", "authors": ["Kodai Hirata", "Tsuyoshi Okita"], "title": "Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet", "categories": ["cs.LG", "cs.CV"], "comment": "8 pages,4 figures", "summary": "This paper proposes a method MTL-Swin-Unet which is multi-task learning using\ntransformers for classification and semantic segmentation. For\nspurious-correlation problems, this method allows us to enhance the image\nrepresentation with two other image representations: representation obtained by\nsemantic segmentation and representation obtained by image reconstruction. In\nour experiments, the proposed method outperformed in F-value measure than other\nclassifiers when the test data included slices from the same patient (no\ncovariate shift). Similarly, when the test data did not include slices from the\nsame patient (covariate shift setting), the proposed method outperformed in AUC\nmeasure.", "AI": {"tldr": "MTL-Swin-Unet\u65b9\u6cd5\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548cTransformer\u63d0\u5347\u5206\u7c7b\u4e0e\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u89e3\u51b3\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u865a\u5047\u76f8\u5173\u6027\uff08spurious-correlation\uff09\u95ee\u9898\uff0c\u63d0\u5347\u56fe\u50cf\u8868\u793a\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u56fe\u50cf\u91cd\u5efa\u7684\u8868\u793a\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548cTransformer\u67b6\u6784\u5b9e\u73b0\u5206\u7c7b\u4e0e\u5206\u5272\u3002", "result": "\u5728\u65e0\u534f\u53d8\u91cf\u504f\u79fb\uff08\u76f8\u540c\u60a3\u8005\u5207\u7247\uff09\u65f6F\u503c\u66f4\u4f18\uff0c\u6709\u534f\u53d8\u91cf\u504f\u79fb\uff08\u4e0d\u540c\u60a3\u8005\u5207\u7247\uff09\u65f6AUC\u66f4\u4f18\u3002", "conclusion": "MTL-Swin-Unet\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u548cTransformer\u652f\u6301\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u4e0e\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2505.06191", "pdf": "https://arxiv.org/pdf/2505.06191", "abs": "https://arxiv.org/abs/2505.06191", "authors": ["Jiayuan Mao", "Joshua B. Tenenbaum", "Jiajun Wu"], "title": "Neuro-Symbolic Concepts", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "comment": "To appear in Communications of the ACM", "summary": "This article presents a concept-centric paradigm for building agents that can\nlearn continually and reason flexibly. The concept-centric agent utilizes a\nvocabulary of neuro-symbolic concepts. These concepts, such as object,\nrelation, and action concepts, are grounded on sensory inputs and actuation\noutputs. They are also compositional, allowing for the creation of novel\nconcepts through their structural combination. To facilitate learning and\nreasoning, the concepts are typed and represented using a combination of\nsymbolic programs and neural network representations. Leveraging such\nneuro-symbolic concepts, the agent can efficiently learn and recombine them to\nsolve various tasks across different domains, ranging from 2D images, videos,\n3D scenes, and robotic manipulation tasks. This concept-centric framework\noffers several advantages, including data efficiency, compositional\ngeneralization, continual learning, and zero-shot transfer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6982\u5ff5\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\uff0c\u7528\u4e8e\u6784\u5efa\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u548c\u7075\u6d3b\u63a8\u7406\u7684\u667a\u80fd\u4f53\u3002\u8be5\u667a\u80fd\u4f53\u5229\u7528\u795e\u7ecf\u7b26\u53f7\u6982\u5ff5\u7684\u8bcd\u6c47\uff0c\u8fd9\u4e9b\u6982\u5ff5\u57fa\u4e8e\u611f\u5b98\u8f93\u5165\u548c\u52a8\u4f5c\u8f93\u51fa\uff0c\u5177\u6709\u7ec4\u5408\u6027\uff0c\u5e76\u80fd\u901a\u8fc7\u7ed3\u6784\u7ec4\u5408\u521b\u5efa\u65b0\u6982\u5ff5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u5b66\u4e60\u3001\u7ec4\u5408\u6cdb\u5316\u3001\u6301\u7eed\u5b66\u4e60\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u95ee\u9898\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6982\u5ff5\uff0c\u7ed3\u5408\u7b26\u53f7\u7a0b\u5e8f\u548c\u795e\u7ecf\u7f51\u7edc\u8868\u793a\uff0c\u652f\u6301\u6982\u5ff5\u7684\u7ec4\u5408\u548c\u63a8\u7406\u3002", "result": "\u667a\u80fd\u4f53\u80fd\u591f\u57282D\u56fe\u50cf\u3001\u89c6\u9891\u30013D\u573a\u666f\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9ad8\u6548\u5b66\u4e60\u548c\u91cd\u7ec4\u6982\u5ff5\u3002", "conclusion": "\u6982\u5ff5\u4e2d\u5fc3\u6846\u67b6\u5177\u6709\u6570\u636e\u9ad8\u6548\u6027\u3001\u7ec4\u5408\u6cdb\u5316\u3001\u6301\u7eed\u5b66\u4e60\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.06210", "pdf": "https://arxiv.org/pdf/2505.06210", "abs": "https://arxiv.org/abs/2505.06210", "authors": ["Diego Adame", "Jose A. Nunez", "Fabian Vazquez", "Nayeli Gurrola", "Huimin Li", "Haoteng Tang", "Bin Fu", "Pengfei Gu"], "title": "Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Convolutional neural network (CNN) and Transformer-based architectures are\ntwo dominant deep learning models for polyp segmentation. However, CNNs have\nlimited capability for modeling long-range dependencies, while Transformers\nincur quadratic computational complexity. Recently, State Space Models such as\nMamba have been recognized as a promising approach for polyp segmentation\nbecause they not only model long-range interactions effectively but also\nmaintain linear computational complexity. However, Mamba-based architectures\nstill struggle to capture topological features (e.g., connected components,\nloops, voids), leading to inaccurate boundary delineation and polyp\nsegmentation. To address these limitations, we propose a new approach called\nTopo-VM-UNetV2, which encodes topological features into the Mamba-based\nstate-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of\ntwo stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for\nthe training and test images, which are then used to compute topology attention\nmaps. Specifically, we first compute persistence diagrams of the PMs, then we\ngenerate persistence score maps by assigning persistence values (i.e., the\ndifference between death and birth times) of each topological feature to its\nbirth location, finally we transform persistence scores into attention weights\nusing the sigmoid function. Stage 2: These topology attention maps are\nintegrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to\nform a topology-guided semantics and detail infusion (Topo-SDI) module for\nenhancing the segmentation results. Extensive experiments on five public polyp\nsegmentation datasets demonstrate the effectiveness of our proposed method. The\ncode will be made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTopo-VM-UNetV2\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u62d3\u6251\u7279\u5f81\u7f16\u7801\u5230\u57fa\u4e8eMamba\u7684VM-UNetV2\u6a21\u578b\u4e2d\uff0c\u6539\u8fdb\u4e86\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684CNN\u548cTransformer\u6a21\u578b\u5728\u606f\u8089\u5206\u5272\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1aCNN\u96be\u4ee5\u5efa\u6a21\u957f\u8ddd\u79bb\u4f9d\u8d56\uff0c\u800cTransformer\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002Mamba\u6a21\u578b\u867d\u7136\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u4ecd\u65e0\u6cd5\u6709\u6548\u6355\u6349\u62d3\u6251\u7279\u5f81\uff0c\u5bfc\u81f4\u8fb9\u754c\u5206\u5272\u4e0d\u51c6\u786e\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1\uff09\u4f7f\u7528VM-UNetV2\u751f\u6210\u6982\u7387\u56fe\u5e76\u8ba1\u7b97\u62d3\u6251\u6ce8\u610f\u529b\u56fe\uff1b2\uff09\u5c06\u62d3\u6251\u6ce8\u610f\u529b\u56fe\u96c6\u6210\u5230VM-UNetV2\u7684SDI\u6a21\u5757\u4e2d\uff0c\u5f62\u6210Topo-SDI\u6a21\u5757\u4ee5\u589e\u5f3a\u5206\u5272\u7ed3\u679c\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u7684\u606f\u8089\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Topo-VM-UNetV2\u901a\u8fc7\u5f15\u5165\u62d3\u6251\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2505.06218", "pdf": "https://arxiv.org/pdf/2505.06218", "abs": "https://arxiv.org/abs/2505.06218", "authors": ["Kwan-Yee Lin", "Stella X. Yu"], "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "CVPR 2025. Project page:\n  https://lego-h-humanoidrobothiking.github.io/", "summary": "Hiking on complex trails demands balance, agility, and adaptive\ndecision-making over unpredictable terrain. Current humanoid research remains\nfragmented and inadequate for hiking: locomotion focuses on motor skills\nwithout long-term goals or situational awareness, while semantic navigation\noverlooks real-world embodiment and local terrain variability. We propose\ntraining humanoids to hike on complex trails, driving integrative skill\ndevelopment across visual perception, decision making, and motor execution. We\ndevelop a learning framework, LEGO-H, that enables a vision-equipped humanoid\nrobot to hike complex trails autonomously. We introduce two technical\ninnovations: 1) A temporal vision transformer variant - tailored into\nHierarchical Reinforcement Learning framework - anticipates future local goals\nto guide movement, seamlessly integrating locomotion with goal-directed\nnavigation. 2) Latent representations of joint movement patterns, combined with\nhierarchical metric learning - enhance Privileged Learning scheme - enable\nsmooth policy transfer from privileged training to onboard execution. These\ncomponents allow LEGO-H to handle diverse physical and environmental challenges\nwithout relying on predefined motion patterns. Experiments across varied\nsimulated trails and robot morphologies highlight LEGO-H's versatility and\nrobustness, positioning hiking as a compelling testbed for embodied autonomy\nand LEGO-H as a baseline for future humanoid development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLEGO-H\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u8ba9\u4eff\u4eba\u673a\u5668\u4eba\u80fd\u591f\u5728\u590d\u6742\u7684\u5c0f\u5f84\u4e0a\u81ea\u4e3b\u5f92\u6b65\uff0c\u7ed3\u5408\u89c6\u89c9\u611f\u77e5\u3001\u51b3\u7b56\u548c\u8fd0\u52a8\u6267\u884c\u3002", "motivation": "\u5f53\u524d\u4eff\u4eba\u673a\u5668\u4eba\u7814\u7a76\u5728\u5f92\u6b65\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd0\u52a8\u6280\u80fd\u7f3a\u4e4f\u957f\u671f\u76ee\u6807\u548c\u60c5\u5883\u611f\u77e5\uff0c\u800c\u8bed\u4e49\u5bfc\u822a\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u5177\u4f53\u5316\u548c\u5730\u5f62\u53d8\u5316\u3002", "method": "1) \u4f7f\u7528\u65f6\u95f4\u89c6\u89c9\u53d8\u6362\u5668\u53d8\u4f53\uff0c\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9884\u6d4b\u672a\u6765\u5c40\u90e8\u76ee\u6807\u4ee5\u6307\u5bfc\u8fd0\u52a8\uff1b2) \u901a\u8fc7\u6f5c\u5728\u5173\u8282\u8fd0\u52a8\u6a21\u5f0f\u8868\u793a\u548c\u5206\u5c42\u5ea6\u91cf\u5b66\u4e60\uff0c\u5b9e\u73b0\u4ece\u7279\u6743\u8bad\u7ec3\u5230\u5b9e\u9645\u6267\u884c\u7684\u5e73\u6ed1\u7b56\u7565\u8fc1\u79fb\u3002", "result": "LEGO-H\u5728\u591a\u6837\u5316\u7684\u6a21\u62df\u5c0f\u5f84\u548c\u673a\u5668\u4eba\u5f62\u6001\u4e2d\u8868\u73b0\u51fa\u591a\u529f\u80fd\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f92\u6b65\u4efb\u52a1\u4e3a\u5177\u8eab\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u6d4b\u8bd5\u5e73\u53f0\uff0cLEGO-H\u4e3a\u672a\u6765\u4eff\u4eba\u673a\u5668\u4eba\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.06227", "pdf": "https://arxiv.org/pdf/2505.06227", "abs": "https://arxiv.org/abs/2505.06227", "authors": ["Yufan Deng", "Yuhao Zhang", "Chen Geng", "Shangzhe Wu", "Jiajun Wu"], "title": "Anymate: A Dataset and Baselines for Learning 3D Object Rigging", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025. Project page: https://anymate3d.github.io/", "summary": "Rigging and skinning are essential steps to create realistic 3D animations,\noften requiring significant expertise and manual effort. Traditional attempts\nat automating these processes rely heavily on geometric heuristics and often\nstruggle with objects of complex geometry. Recent data-driven approaches show\npotential for better generality, but are often constrained by limited training\ndata. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets\npaired with expert-crafted rigging and skinning information -- 70 times larger\nthan existing datasets. Using this dataset, we propose a learning-based\nauto-rigging framework with three sequential modules for joint, connectivity,\nand skinning weight prediction. We systematically design and experiment with\nvarious architectures as baselines for each module and conduct comprehensive\nevaluations on our dataset to compare their performance. Our models\nsignificantly outperform existing methods, providing a foundation for comparing\nfuture methods in automated rigging and skinning. Code and dataset can be found\nat https://anymate3d.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAnymate\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u52a8\u7ed1\u5b9a\u6846\u67b6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u7ed1\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u51e0\u4f55\u542f\u53d1\u5f0f\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u51e0\u4f55\u4f53\uff1b\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u987a\u5e8f\u6a21\u5757\uff08\u5173\u8282\u3001\u8fde\u63a5\u3001\u8499\u76ae\u6743\u91cd\u9884\u6d4b\uff09\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u591a\u79cd\u67b6\u6784\u4f5c\u4e3a\u57fa\u7ebf\u3002", "result": "\u6a21\u578b\u5728Anymate\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Anymate\u6570\u636e\u96c6\u548c\u6846\u67b6\u4e3a\u81ea\u52a8\u7ed1\u5b9a\u548c\u8499\u76ae\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002"}}
