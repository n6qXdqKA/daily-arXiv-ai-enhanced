[[toc]]

## cs.CV

### [1] [Team PA-VCG's Solution for Competition on Understanding Chinese College Entrance Exam Papers in ICDAR'25](https://arxiv.org/abs/2508.00834)
*Wei Wu,Wenjie Wang,Yang Tan,Ying Liu,Liang Diao,Lin Huang,Kaihe Xu,Wenfeng Xie,Ziling Lin*

Main category: cs.CV

TL;DR: Team PA-VGG提出了一种针对高考试卷OCR提取和复杂布局的解决方案，结合高分辨率图像处理和多图像端到端输入策略，并通过领域特定后训练策略，最终以89.6%的准确率获得第一名。

- Motivation: 解决高考试卷中密集OCR提取和复杂文档布局的挑战。
- Method: 采用高分辨率图像处理、多图像端到端输入策略及领域特定后训练策略。
- Result: 实验结果显示后训练策略表现最佳，准确率达89.6%，获得第一名。
- Conclusion: 该方法在高考试卷理解任务中表现出色，证明了后训练策略的有效性。


### [2] [Inclusive Review on Advances in Masked Human Face Recognition Technologies](https://arxiv.org/abs/2508.00841)
*Ali Haitham Abdul Amir,Zainab N. Nemer*

Main category: cs.CV

TL;DR: 该论文综述了口罩人脸识别（MFR）的最新进展，重点探讨了深度学习技术（如CNN和Siamese网络）、面临的挑战（如光照变化、遮挡等）及解决方案（如数据增强和网络设计优化），并展望了未来研究方向。

- Motivation: 由于COVID-19疫情期间口罩的广泛使用，部分遮挡的人脸识别成为重要挑战，推动了该领域的研究需求。
- Method: 论文采用文献综述方法，分析了深度学习技术（如CNN和Siamese网络）在MFR中的应用，并探讨了数据增强、网络设计优化等解决方案。
- Result: 总结了MFR领域的关键挑战、技术进展（如特征提取和数据集优化）及其在安全和医疗领域的应用。
- Conclusion: 未来研究应聚焦于开发更高效算法和集成多媒体技术，以提升MFR系统在真实环境中的性能和应用范围。


### [3] [HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models](https://arxiv.org/abs/2508.00892)
*Zhihao Zhu,Jiale Han,Yi Yang*

Main category: cs.CV

TL;DR: HoneyImage是一种新型图像数据集所有权验证方法，通过选择性修改少量困难样本嵌入可验证痕迹，实现高效验证且不影响数据完整性。

- Motivation: 图像数据集常包含敏感或专有内容，现有验证方法在效果与数据完整性间存在权衡，需更可靠的验证机制。
- Method: HoneyImage选择性修改少量困难样本，嵌入不可察觉但可验证的痕迹。
- Result: 在四个基准数据集和多种模型架构上，HoneyImage验证准确率高，对下游任务影响极小。
- Conclusion: HoneyImage为数据所有者提供实用工具，保护数据集所有权，促进安全共享和AI潜力发挥。


### [4] [Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis](https://arxiv.org/abs/2508.00896)
*Hoang Hai Nam Nguyen,Minh Tien Tran,Hoheok Kim,Ho Won Lee*

Main category: cs.CV

TL;DR: PF-DiffSeg是一种基于相分数控制的单阶段去噪扩散框架，用于同时生成显微结构图像及其分割掩码，显著提高了分割精度，尤其在少数类别中表现突出。

- Motivation: 解决金属合金显微结构分割中因缺乏人工标注的相掩码（尤其是稀有或复杂形态）而导致的机器学习效果受限问题。
- Method: 通过全局相分数向量控制，生成结构一致且成分有效的显微结构图像和掩码样本，提升数据多样性和训练效率。
- Result: 在MetalDAM基准测试中，该方法显著优于标准增强策略和两阶段掩码引导扩散及GAN基线，尤其在少数类别中表现更优，同时减少了推理时间。
- Conclusion: PF-DiffSeg将生成与条件控制集成到统一框架中，为金相应用中的数据增强提供了可扩展的解决方案。


### [5] [Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models](https://arxiv.org/abs/2508.00898)
*Jose M. Sánchez Velázquez,Mingbo Cai,Andrew Coney,Álvaro J. García- Tejedor,Alberto Nogales*

Main category: cs.CV

TL;DR: 论文探讨了结合自编码器和RNN、3D CNN等架构的混合深度学习方法在视频帧预测中的应用，结果显示3D CNN和ConvLSTM的混合模型效果最佳。

- Motivation: 近年来AI在计算机视觉领域的进展显著，视频帧预测在天气预报和自动驾驶等应用中具有重要价值，但现有模型仍有改进空间。
- Method: 采用混合深度学习方法，结合自编码器的特征提取能力和RNN、3D CNN等架构的时间序列建模能力，并在三个不同数据集上进行评估。
- Result: 实验表明，混合模型性能良好，SSIM指标从0.69提升至0.82，其中3D CNN和ConvLSTM的组合效果最佳，且灰度视频的真实数据预测最容易。
- Conclusion: 混合深度学习方法能有效提升视频帧预测性能，3D CNN和ConvLSTM的组合尤为突出，未来可进一步优化模型。


### [6] [TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras](https://arxiv.org/abs/2508.00913)
*Mohammad Mohammadi,Ziyi Wu,Igor Gilitschenski*

Main category: cs.CV

TL;DR: TESPEC是一种自监督预训练框架，专为学习事件数据的长时空信息设计，适用于循环模型，并在多个下游任务中取得先进结果。

- Motivation: 现有自监督学习方法忽略了事件数据的时间信息，且主要针对前馈模型，无法充分利用循环模型的潜力。
- Method: TESPEC采用掩码图像建模范式，设计了一种新的事件累积方法生成伪灰度视频作为重建目标。
- Result: 实验表明，TESPEC在目标检测、语义分割和单目深度估计等任务中表现优异。
- Conclusion: TESPEC是首个利用长事件序列进行预训练的框架，显著提升了循环模型在下游任务中的性能。


### [7] [Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition](https://arxiv.org/abs/2508.00941)
*Hassan Ugail,Hamad Mansour Alawar,AbdulNasser Abbas Zehi,Ahmed Mohammad Alkendi,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 论文提出了一种基于潜在扩散增强的方法，显著提高了低质量法医图像中的人脸识别准确率。

- Motivation: 法医图像质量低导致人脸识别系统性能下降，需要有效增强方法。
- Method: 使用Flux.1 Kontext Dev管道和Facezoom LoRA适应，测试了7种退化类型。
- Result: 识别准确率从29.1%提升至84.5%，效果显著。
- Conclusion: 扩散增强技术在法医人脸识别中具有重要潜力。


### [8] [Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment](https://arxiv.org/abs/2508.00945)
*Yifan Wang,Hongfeng Ai,Quangao Liu,Maowei Jiang,Ruiyuan Kang,Ruiqi Li,Jiahua Dong,Mengting Xiao,Cheng Jiang,Chenzhong Li*

Main category: cs.CV

TL;DR: 论文提出CCRA方法，通过LPWCA和PAI协调跨模态注意力机制，提升视觉语言模型的性能与可解释性。

- Motivation: 视觉语言模型在跨模态嵌入学习中存在注意力不匹配和性能不佳的问题。
- Method: 提出CCRA方法，结合LPWCA（层-块交叉注意力）和PAI（渐进注意力整合），协调多种注意力机制。
- Result: 在十个基准测试中，CCRA增强的LLaVA-v1.5-7B模型表现最优，仅增加3.55M参数。
- Conclusion: CCRA通过区域聚焦和语义对齐的注意力模式，显著提升了模型性能与可解释性。


### [9] [ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling](https://arxiv.org/abs/2508.00974)
*Daniel Andrés López,Vincent Weber,Severin Zentgraf,Barlo Hillen,Perikles Simon,Elmar Schömer*

Main category: cs.CV

TL;DR: 红外热成像在运动医学中的应用，通过自动标注和手动标注结合的方法优化深度学习网络性能。

- Motivation: 将基于跑步机的立体和多模态标注方法扩展到自行车运动，以提升红外热成像在运动医学中的应用效果。
- Method: 使用自动生成的标签训练语义分割网络，并在高质量手动标注图像上进行微调，比较不同数据集组合的效果。
- Result: 结果表明，少量手动标注数据的微调足以显著提升深度学习网络的性能。
- Conclusion: 自动生成标签与少量手动标注数据结合，可加速深度学习网络在新场景（如从跑步机到自行车）中的适应。


### [10] [ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation](https://arxiv.org/abs/2508.01008)
*Cihang Peng,Qiming Hou,Zhong Ren,Kun Zhou*

Main category: cs.CV

TL;DR: ROVI是一个高质量的合成数据集，用于实例接地的文本到图像生成，通过重新标注策略和VLM与LLM的结合，显著提升了检测数据集的性能。

- Motivation: 现有检测数据集在图像质量、分辨率和类别覆盖上存在不足，ROVI旨在解决这些问题。
- Method: 采用重新标注策略，结合VLM生成视觉描述和LLM提取类别列表，供OVDs检测。
- Result: ROVI在图像质量、分辨率和类别数量上优于现有数据集，GLIGEN模型在ROVI上训练后表现优异。
- Conclusion: ROVI为文本到图像生成提供了高质量的数据支持，显著提升了实例接地和提示保真度。


### [11] [AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise](https://arxiv.org/abs/2508.01015)
*Byron Dowling,Jozef Probcin,Adam Czajka*

Main category: cs.CV

TL;DR: AutoSIGHT利用眼动追踪数据自动评估人类在视觉任务中的专业水平，通过5秒和30秒的评估窗口分别达到0.751和0.8306的AUROC性能。

- Motivation: 研究如何通过眼动追踪数据自动区分专家与非专家，以优化人机协作中的动态配对。
- Method: 基于眼动追踪数据的特征集成方法AutoSIGHT，用于分类专家与非专家。
- Result: 在5秒和30秒评估窗口下，AUROC分别为0.751和0.8306。
- Conclusion: AutoSIGHT展示了自动评估人类专业水平的可行性，为人机协作中的动态配对提供了新思路。


### [12] [3D Reconstruction via Incremental Structure From Motion](https://arxiv.org/abs/2508.01019)
*Muhammad Zeeshan,Umer Zaki,Syed Ahmed Pasha,Zaar Khizar*

Main category: cs.CV

TL;DR: 本文提出了一种增量式SfM方法，用于从非结构化图像集合中实现准确的3D重建，适用于稀疏或部分重叠的数据集。

- Motivation: 全局SfM技术对噪声和缺失数据敏感，而增量式SfM更具灵活性，能够逐步整合新视图，适用于稀疏或部分重叠的数据集。
- Method: 详细实现增量式SfM流程，重点研究几何估计的一致性和通过束调整的迭代优化效果。
- Result: 使用真实数据集验证方法，通过重投影误差和相机轨迹一致性评估重建质量。
- Conclusion: 增量式SfM是一种可靠的方法，适用于视觉结构化环境中的稀疏3D重建。


### [13] [Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans](https://arxiv.org/abs/2508.01045)
*Theo Di Piazza,Carole Lazarus,Olivier Nempont,Loic Boussel*

Main category: cs.CV

TL;DR: 提出一种基于图结构的新方法，用于3D CT扫描的多标签分类，解决现有方法的局限性。

- Motivation: 随着CT扫描检查数量的增加，需要自动化方法来辅助放射科医生处理日益增长的工作量，但现有方法在建模长距离依赖或计算成本方面存在不足。
- Method: 将CT扫描建模为结构化图，利用轴向切片三元组节点和谱域卷积增强多标签异常分类性能。
- Result: 方法表现出强大的跨数据集泛化能力、竞争性性能以及对z轴平移的鲁棒性。
- Conclusion: 通过消融研究验证了各组件的作用，新方法为3D CT扫描分类提供了有效替代方案。


### [14] [Evading Data Provenance in Deep Neural Networks](https://arxiv.org/abs/2508.01074)
*Hongyu Zhu,Sichu Liang,Wenwen Wang,Zhuomeng Zhang,Fangqi Li,Shi-Lin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种统一的规避框架，通过教师模型学习版权数据集，并利用OOD数据集将任务相关但标识无关的知识转移到学生模型，显著提升了规避效果。

- Motivation: 现有数据集所有权验证（DOV）方法在评估中依赖过于简化的规避攻击，导致虚假的安全感。本文旨在揭示DOV方法的漏洞并提出更有效的规避策略。
- Method: 引入教师-学生模型框架，利用OOD数据集和Vision-Language Models/Large Language Models筛选信息子集，选择性转移任务导向知识。
- Result: 实验表明，该方法在11种DOV方法中完全消除版权标识，并在泛化性和有效性上显著优于9种现有规避攻击。
- Conclusion: 当前DOV方法存在关键漏洞，需要长期发展以提升实用性。


### [15] [DreamSat-2.0: Towards a General Single-View Asteroid 3D Reconstruction](https://arxiv.org/abs/2508.01079)
*Santiago Diaz,Xinghui Hu,Josiane Uwumukiza,Giovanni Lavezzi,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.CV

TL;DR: DreamSat-2.0评估了三种3D重建模型在航天器和小行星数据集上的表现，发现模型性能因领域而异。Hunyuan-3D在航天器图像质量和小行星几何精度上表现最佳。

- Motivation: 提升小行星探索和自主航天器导航能力。
- Method: 使用2D感知和3D几何指标，系统评估Hunyuan-3D、Trellis-3D和Ouroboros-3D在航天器和小行星数据集上的表现。
- Result: 模型性能因领域不同：航天器图像质量更高，小行星几何重建更准确。Hunyuan-3D在两项指标中均表现最佳。
- Conclusion: DreamSat-2.0为3D重建模型在不同领域的应用提供了新基准，显著优于先前工作。


### [16] [COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition](https://arxiv.org/abs/2508.01087)
*Ryan Rabinowitz,Steve Cruz,Walter Scheirer,Terrance E. Boult*

Main category: cs.CV

TL;DR: COSTARR提出了一种新的衰减假设，利用训练中学习的小权重来区分已知类别和未知类别，显著提升了开放集识别性能。

- Motivation: 现有开放集识别方法依赖熟悉特征假设，忽略了衰减特征的作用，COSTARR旨在利用这些被忽视的信息。
- Method: COSTARR结合熟悉特征和衰减特征，通过概率解释其评分，并通过消融实验验证特征贡献。
- Result: COSTARR在多种预训练架构上显著优于现有方法，证明了其泛化能力和性能提升。
- Conclusion: COSTARR通过利用衰减信息，推动了开放集识别领域的发展。


### [17] [AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions](https://arxiv.org/abs/2508.01095)
*Mikhail Bychkov,Matey Yordanov,Andrei Kuchma*

Main category: cs.CV

TL;DR: AURA是一种新型混合时空-色彩框架，用于实时检测和分类工业烟雾排放，解决现有监测系统的局限性。

- Motivation: 当前监测系统缺乏区分烟雾类型的能力，且难以应对环境变化，AURA旨在解决这些问题。
- Method: AURA结合烟雾的动态运动模式和独特颜色特征，提高检测准确性和减少误报。
- Result: 该框架显著提升了工业排放的精确自动化监测能力。
- Conclusion: AURA有望改善环境合规性、操作安全和公共健康结果。


### [18] [Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting](https://arxiv.org/abs/2508.01098)
*Yuekun Dai,Haitian Li,Shangchen Zhou,Chen Change Loy*

Main category: cs.CV

TL;DR: 论文提出了一种名为Trans-Adapter的插件适配器，使基于扩散的图像修复模型能够直接处理透明图像（RGBA），解决了传统方法中透明区域一致性和边缘锯齿的问题。

- Motivation: 现有图像修复方法仅适用于RGB图像，而透明图像修复通常需要两阶段处理（修复+抠图），导致透明区域不一致和边缘锯齿。
- Method: 提出Trans-Adapter插件，支持直接处理RGBA图像，并兼容ControlNet可控编辑，可集成到多种社区模型中。
- Result: 通过LayerBench数据集和新的透明度边缘质量评估指标验证了方法的有效性。
- Conclusion: Trans-Adapter为透明图像修复提供了高效且可控的解决方案。


### [19] [MASIV: Toward Material-Agnostic System Identification from Videos](https://arxiv.org/abs/2508.01112)
*Yizhou Zhao,Haoyu Chen,Chunjiang Liu,Zhenyang Li,Charles Herrmann,Junhwa Hur,Yinxiao Li,Ming-Hsuan Yang,Bhiksha Raj,Min Xu*

Main category: cs.CV

TL;DR: MASIV是一个基于视觉的材料无关系统识别框架，通过可学习的神经本构模型推断物体动力学，无需依赖预定义材料先验。

- Motivation: 现有方法依赖预定义材料先验，限制了处理未知材料的能力。
- Method: MASIV采用可学习的神经本构模型，并通过重建连续粒子轨迹提供密集几何引导。
- Result: MASIV在几何精度、渲染质量和泛化能力上达到最先进水平。
- Conclusion: MASIV通过材料无关的方法和密集几何引导，解决了现有方法的局限性。


### [20] [The Promise of RL for Autoregressive Image Editing](https://arxiv.org/abs/2508.01119)
*Saba Ahmadi,Rabiul Awal,Ankur Sikarwar,Amirhossein Kazemnejad,Ge Ya Luo,Juan A. Rodriguez,Sai Rajeswar,Siva Reddy,Christopher Pal,Benno Krojer,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 论文提出EARL模型，结合强化学习（RL）和多模态LLM验证器，在图像编辑任务中表现优异。

- Motivation: 探索监督微调（SFT）、强化学习（RL）和思维链（CoT）推理三种策略，提升图像编辑任务的性能。
- Method: 采用自回归多模态模型统一处理文本和视觉标记，结合RL和多模态LLM验证器。
- Result: EARL模型在多种编辑任务中表现优异，且训练数据需求较低。
- Conclusion: EARL推动了自回归多模态模型在图像编辑领域的进展，并开源了代码和数据。


### [21] [UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation](https://arxiv.org/abs/2508.01126)
*Chaitanya Patel,Hiroki Nakamura,Yuta Kyuragi,Kazuki Kozuka,Juan Carlos Niebles,Ehsan Adeli*

Main category: cs.CV

TL;DR: 论文提出了一种基于第一人称视角的运动生成和预测方法，解决了现有方法在真实场景中的局限性，并引入了一个统一的框架和数据集。

- Motivation: 提升AR/VR体验、人机交互、辅助技术和适应性医疗解决方案，通过准确预测和模拟第一人称视角的运动。
- Method: 提出UniEgoMotion，一种基于条件运动扩散模型的统一框架，结合头部中心运动表示和图像场景上下文。
- Result: UniEgoMotion在运动重建和生成任务中表现优异，首次实现从单张第一人称图像生成运动。
- Conclusion: UniEgoMotion为第一人称运动建模设定了新基准，拓展了相关应用的可能性。


### [22] [Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach](https://arxiv.org/abs/2508.01137)
*Zeduo Zhang,Yalda Mohsenzadeh*

Main category: cs.CV

TL;DR: 提出了一种基于深度强化学习的半监督异常检测框架，适用于多领域，解决了大规模数据、过拟合和类别不平衡问题，在脑MRI和工业数据集上表现优异。

- Motivation: 开发一个领域无关的半监督异常检测框架，以应对脑MRI数据中的大规模数据、过拟合和类别不平衡等挑战。
- Method: 结合深度强化学习（DRL）与特征表示，处理标签稀缺、大规模数据和过拟合问题。预处理包括归一化、去颅骨和配准。
- Result: 在脑MRI数据集上，AUROC达到88.7%（像素级）和96.7%（图像级）；在工业数据集上AUROC为99.8%（像素级）和99.3%（图像级）。
- Conclusion: 该框架在医学和工业领域均表现出强大的泛化能力和效率，具有实际临床应用潜力。


### [23] [Dataset Condensation with Color Compensation](https://arxiv.org/abs/2508.01139)
*Huyu Wu,Duo Su,Junjie Hou,Guang Li*

Main category: cs.CV

TL;DR: 论文提出DC3框架，通过颜色补偿提升数据集压缩的性能和保真度，解决了现有方法在极端压缩下的瓶颈问题。

- Motivation: 现有数据集压缩方法在图像级选择和像素级优化上存在效率低和语义失真的问题，忽视了颜色作为信息载体和语义单元的双重作用。
- Method: 提出DC3框架，结合校准选择策略和潜在扩散模型增强图像颜色多样性，而非生成全新图像。
- Result: 实验表明DC3在多个基准测试中优于现有方法，且首次利用预训练扩散模型微调压缩数据集。
- Conclusion: DC3通过颜色补偿显著提升数据集压缩质量，解决了模型崩溃等问题，证明了其可行性和优越性。


### [24] [OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding](https://arxiv.org/abs/2508.01150)
*Dianyi Yang,Xihan Wang,Yu Gao,Shiyang Liu,Bohan Ren,Yufeng Yue,Yi Yang*

Main category: cs.CV

TL;DR: OpenGS-Fusion提出了一种创新的开放词汇密集映射框架，结合3D高斯表示和截断符号距离场，提升语义建模和对象级理解。

- Motivation: 现有方法在开放词汇查询下无法提供精确的3D对象级理解，且受限于离线流程。
- Method: 结合3D高斯表示与截断符号距离场，动态融合语义特征；引入多模态语言引导的自适应阈值方法优化分割。
- Result: 在3D mIoU上比固定阈值策略提升17%，在3D对象理解和场景重建质量上优于现有方法。
- Conclusion: OpenGS-Fusion在语言引导的场景交互中表现出色，代码已开源。


### [25] [Personalized Safety Alignment for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.01151)
*Yu Lei,Jinbin Bai,Qingyu Shi,Aosong Feng,Kaidong Yu*

Main category: cs.CV

TL;DR: 提出了一种个性化安全对齐框架（PSA），通过用户特定配置文件调整扩散模型的安全行为，以适应不同用户的安全偏好。

- Motivation: 当前文本到图像扩散模型的安全机制采用统一标准，忽视了用户因年龄、心理健康和个人信仰等因素形成的多样化安全边界。
- Method: 提出PSA框架，整合个性化用户配置文件到扩散过程中，通过交叉注意力机制调整模型行为。
- Result: PSA在有害内容抑制和用户约束对齐方面优于现有方法，Win Rate和Pass Rate得分更高。
- Conclusion: PSA有效实现了个性化安全控制，同时保持图像质量，代码和数据已公开。


### [26] [LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation](https://arxiv.org/abs/2508.01152)
*Xinyu Yan,Meijun Sun,Ge-Peng Ji,Fahad Shahbaz Khan,Salman Khan,Deng-Ping Fan*

Main category: cs.CV

TL;DR: LawDIS是一个基于语言窗口的可控二分图像分割框架，通过语言提示和窗口细化策略生成高质量对象掩码。

- Motivation: 解决现有方法在用户控制和个性化需求上的不足，提升图像分割的准确性和灵活性。
- Method: 结合语言控制的分割策略（LS）和窗口控制的细化策略（WR），通过模式切换器协调两种模式。
- Result: 在DIS5K基准测试中显著优于11种先进方法，Fβω增益最高达4.6%。
- Conclusion: LawDIS框架在图像分割任务中表现出色，适用于高精度和个性化应用。


### [27] [TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition](https://arxiv.org/abs/2508.01153)
*Xiahan Yang,Hui Zheng*

Main category: cs.CV

TL;DR: TEACH是一种新的训练范式，通过逐步减少辅助输入的影响，提升场景文本识别（STR）的准确性。

- Motivation: 解决STR任务中因复杂视觉表现和有限语义先验带来的挑战。
- Method: 注入真实文本作为辅助输入，通过嵌入空间编码和损失感知掩码，模拟课程学习过程。
- Result: 在多个公开基准测试中，TEACH显著提高了模型准确性，尤其在复杂条件下表现优异。
- Conclusion: TEACH无需外部预训练，无推理开销，具有模型无关性和广泛适用性。


### [28] [DELTAv2: Accelerating Dense 3D Tracking](https://arxiv.org/abs/2508.01170)
*Tuan Duc Ngo,Ashkan Mirzaei,Guocheng Qian,Hanwen Liang,Chuang Gan,Evangelos Kalogerakis,Peter Wonka,Chaoyang Wang*

Main category: cs.CV

TL;DR: 提出一种新算法，加速视频中的密集长期3D点跟踪，通过粗到细策略和优化特征计算，实现5-100倍加速且保持高精度。

- Motivation: 现有方法在处理大量轨迹时计算成本高，尤其是基于Transformer的迭代跟踪和特征计算成为瓶颈。
- Method: 采用粗到细策略，逐步扩展跟踪轨迹，并引入可学习插值模块；优化相关特征计算以减少成本。
- Result: 算法比现有方法快5-100倍，同时保持最先进的跟踪精度。
- Conclusion: 新方法显著提升了计算效率，适用于大规模3D点跟踪任务。


### [29] [No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2508.01171)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: SPFSplat是一种无需真实姿态的3D高斯溅射框架，通过共享特征提取和单步前馈设计，实现高效的新视角合成和姿态估计。

- Motivation: 解决多视角图像稀疏且无真实姿态信息时，高效生成3D高斯溅射的问题。
- Method: 采用共享特征提取主干，单步预测3D高斯基元和相机姿态，结合渲染损失和重投影损失优化几何约束。
- Result: 在无姿态监督下，SPFSplat在新视角合成和相对姿态估计中表现优异。
- Conclusion: SPFSplat为实际应用提供了一种高效且无需姿态监督的解决方案。


### [30] [Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning](https://arxiv.org/abs/2508.01184)
*Xinhang Wan,Dongqiang Gou,Xinwang Liu,En Zhu,Xuming He*

Main category: cs.CV

TL;DR: 提出一种新方法，通过多模态3D表示和两阶段推理策略，解决3D物体功能区域定位与分类的依赖性问题，提升性能。

- Motivation: 现有方法通常将3D功能区域定位与分类分开处理，导致预测不一致，且无法完整预测功能区域或适应不同尺度。
- Method: 开发跨模态3D表示，融合多尺度几何特征，并采用两阶段预测机制耦合定位与分类任务。
- Result: 实验表明，该方法在功能区域定位与分类任务中均表现更优。
- Conclusion: 该方法通过建模任务依赖性和多尺度特征，显著提升了3D功能区域的理解能力。


### [31] [A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding](https://arxiv.org/abs/2508.01197)
*Zhan Shi,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D体素占用的视觉定位方法，解决了传统边界框方法在精细细节捕捉上的不足。

- Motivation: 传统视觉定位任务依赖边界框，无法精确表示物体细节，尤其是在复杂户外场景中。
- Method: 提出了GroundingOcc模型，结合视觉、文本和点云特征，通过多模态学习实现从粗到细的3D占用定位。
- Result: 在nuScenes数据集上的实验表明，该方法优于现有基线。
- Conclusion: 论文提出的3D占用定位方法和数据集为精细物体感知提供了新方向。


### [32] [Deep Learning for Pavement Condition Evaluation Using Satellite Imagery](https://arxiv.org/abs/2508.01206)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Pan Lu,Jingran Sun*

Main category: cs.CV

TL;DR: 该研究利用深度学习模型分析卫星图像，评估路面状况，准确率超过90%，为快速、低成本的路面网络评估提供了新方法。

- Motivation: 传统基础设施检查方法耗时耗力，卫星系统和图像处理技术的进步为更高效的路面监测提供了可能。
- Method: 收集了3000多张卫星图像，结合路面评估数据，使用深度学习模型进行分析。
- Result: 研究结果显示准确率超过90%。
- Conclusion: 该研究为未来快速、低成本的路面网络评估奠定了基础。


### [33] [RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification](https://arxiv.org/abs/2508.01210)
*Tianze Wang,Zhang Zhang,Chao Yue,Nuoran Li,Chao Sun*

Main category: cs.CV

TL;DR: 论文提出RoadMamba方法，结合全局和局部感知，用于道路表面分类任务，通过DualSSM和DAF实现特征提取与融合，并在大规模数据集上取得最优性能。

- Motivation: 基于视觉技术提前获取道路表面条件对自动驾驶车辆的安全性和舒适性至关重要，但现有Mamba架构在局部纹理提取上表现不足。
- Method: 提出RoadMamba方法，利用DualSSM提取全局语义和局部纹理，通过DAF解码和融合双特征，并引入双辅助损失约束网络。
- Result: 在包含100万样本的大规模道路表面分类数据集上，RoadMamba取得了最优性能。
- Conclusion: RoadMamba通过结合全局和局部感知，显著提升了道路表面分类的准确性。


### [34] [StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling](https://arxiv.org/abs/2508.01215)
*Yuanlin Yang,Quanjian Song,Zhexian Gao,Ge Wang,Shanshan Li,Xiaoyan Zhang*

Main category: cs.CV

TL;DR: StyDeco提出了一种无监督框架，通过特定任务优化的文本表示解决扩散模型在风格转移中的语义鸿沟问题。

- Motivation: 现有扩散模型将文本描述视为统一的指导，忽略了文本的非空间性与视觉风格的空间属性之间的语义差距，导致风格化过程中语义结构和细节丢失。
- Method: 框架包括Prior-Guided Data Distillation（PGD）和Contrastive Semantic Decoupling（CSD）。PGD通过冻结生成模型合成伪配对数据，CSD通过两类别聚类优化文本编码器。
- Result: 在三个经典基准测试中，StyDeco在风格保真度和结构保留方面优于现有方法，并支持独特的去风格化过程。
- Conclusion: StyDeco有效解决了风格转移中的语义鸿沟问题，展示了其扩展性和实用性。


### [35] [Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?](https://arxiv.org/abs/2508.01216)
*Bolei Chen,Shengsheng Yan,Yongzheng Cui,Jiaxu Kang,Ping Zhong,Jianxin Wang*

Main category: cs.CV

TL;DR: 论文提出一种利用视觉场景上下文信息增强楼层平面定位（FLoc）的方法，通过无监督学习和聚类约束预训练房间判别器，显著提高了定位的鲁棒性和准确性。

- Motivation: 楼层平面定位因重复结构易导致定位模糊，现有方法忽略视觉图像提供的丰富上下文信息。
- Method: 提出无监督学习技术，预训练房间判别器，提取隐藏房间类型信息，并将其注入FLoc算法。
- Result: 在标准FLoc基准测试中，方法优于现有技术，显著提升鲁棒性和准确性。
- Conclusion: 利用视觉场景上下文信息可有效消除定位不确定性，提升FLoc性能。


### [36] [MoGaFace: Momentum-Guided and Texture-Aware Gaussian Avatars for Consistent Facial Geometry](https://arxiv.org/abs/2508.01218)
*Yujian Liu,Linlang Cao,Chuang Chen,Fanyu Geng,Dongxu Shen,Peng Cao,Shidang Xu,Xiaoli Liu*

Main category: cs.CV

TL;DR: MoGaFace提出了一种新的3D头部虚拟形象建模框架，通过动态优化几何和纹理，解决了现有方法因网格与图像不对齐导致的渲染质量下降问题。

- Motivation: 现有方法依赖FLAME网格和基于高斯的渲染，但网格与目标图像的对齐问题导致渲染质量不佳。
- Method: 引入Momentum-Guided Consistent Geometry模块和Latent Texture Attention机制，动态优化几何和纹理。
- Result: 实验表明，MoGaFace在高保真头部虚拟形象重建和新视角合成质量上表现优异。
- Conclusion: MoGaFace在网格初始化不准确和真实场景下仍能显著提升渲染质量。


### [37] [Eigen Neural Network: Unlocking Generalizable Vision with Eigenbasis](https://arxiv.org/abs/2508.01219)
*Anzhe Cheng,Chenzhong Yin,Mingxi Cheng,Shukai Duan,Shahin Nazarian,Paul Bogdan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Eigen Neural Network (ENN)的新架构，通过层共享的、学习的正交特征基重新参数化权重，解决了传统梯度优化中权重结构无序的问题，提升了特征表示和性能。

- Motivation: 传统深度神经网络的梯度优化容易产生无序的权重结构，损害特征清晰度和学习动态。
- Method: ENN通过层共享的、学习的正交特征基重新参数化权重，强制权重动态解耦和对齐，无需正则化。
- Result: ENN在ImageNet等大规模图像分类任务中表现优异，并在跨模态图像-文本检索中创下新基准。此外，其变体ENN-ℓ实现了无反向传播的高效训练，速度提升2倍以上且精度更高。
- Conclusion: ENN直接解决了反向传播的表征缺陷，提供了更高性能和更高效的并行训练范式。


### [38] [ParaRevSNN: A Parallel Reversible Spiking Neural Network for Efficient Training and Inference](https://arxiv.org/abs/2508.01223)
*Changqing Xu,Guoqing Sun,Yi Liu,Xinfang Liao,Yintang Yang*

Main category: cs.CV

TL;DR: ParaRevSNN是一种并行可逆SNN架构，通过解耦可逆块间的顺序依赖，显著加速训练和推理，同时保持内存高效性。

- Motivation: 传统的可逆SNN（RevSNNs）因严格顺序计算导致高延迟，限制了其实际应用。
- Method: 提出ParaRevSNN，通过解耦可逆块间的顺序依赖实现并行计算，同时保留可逆性。
- Result: 在多个数据集上，ParaRevSNN的准确率与标准RevSNNs相当或更高，训练时间减少35.2%，推理时间降至18.15%。
- Conclusion: ParaRevSNN适合资源受限场景，兼具高效性和性能优势。


### [39] [Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models](https://arxiv.org/abs/2508.01225)
*Xinyu Chen,Haotian Zhai,Can Zhang,Xiupeng Shi,Ruirui Li*

Main category: cs.CV

TL;DR: 论文提出了一种多缓存增强的原型测试时间适应方法（MCP），通过三个缓存（熵缓存、对齐缓存和负缓存）提升模型在零样本设置下的性能，并进一步开发了MCP++框架，实现了最先进的泛化性能。

- Motivation: 现有基于缓存的测试时间适应方法依赖低熵样本构建原型，但在分布偏移下可能不可靠，且无法保证类内紧凑性。研究发现性能与类内紧凑性正相关，因此提出改进方法。
- Method: 提出MCP方法，包含三个缓存：熵缓存（初始化原型）、对齐缓存（整合视觉和文本信息）、负缓存（校准预测）。进一步开发MCP++框架，引入跨模态原型对齐和残差学习。
- Result: 在15个下游任务上的实验表明，MCP和MCP++实现了最先进的泛化性能。
- Conclusion: MCP和MCP++通过多缓存和跨模态对齐显著提升了测试时间适应的性能，尤其在分布偏移下表现优异。


### [40] [Enhancing Multi-view Open-set Learning via Ambiguity Uncertainty Calibration and View-wise Debiasing](https://arxiv.org/abs/2508.01227)
*Zihan Fang,Zhiyong Xu,Lan Du,Shide Du,Zhiling Cai,Shiping Wang*

Main category: cs.CV

TL;DR: 提出了一种多视图开放集学习框架，通过模糊不确定性校准和视图去偏解决现有模型的类完整性和视图偏差问题。

- Motivation: 现有多视图学习模型在开放集场景中表现不佳，因其隐含假设类完整性，且视图诱导的静态偏差进一步削弱了识别未知类别的能力。
- Method: 设计了O-Mix合成策略生成虚拟样本以校准模糊不确定性，并引入辅助模糊感知网络和HSIC对比去偏模块。
- Result: 在多个多视图基准测试中，该框架显著提升了未知类识别能力，同时保持了封闭集性能。
- Conclusion: 该框架通过模糊校准和视图去偏有效解决了开放集学习中的关键挑战。


### [41] [Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models](https://arxiv.org/abs/2508.01236)
*Mingyu Fu,Wei Suo,Ji Ma,Lin Yuanbo Wu,Peng Wang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种自适应内容补偿方法（ACCM），通过图像描述减少视觉信息损失，显著优于现有方法。

- Motivation: 大型视觉语言模型（LVLMs）的高计算成本限制了其广泛应用，现有方法在高剪枝率下性能下降严重。
- Method: ACCM包含轻量级描述模型和选择器，生成与问题相关的描述并选择最佳描述，无需人工标注。
- Result: 在七个基准测试中，ACCM以更低的计算成本（如减少6.5% FLOPs）超越现有方法20.6%。
- Conclusion: ACCM有效解决了视觉信息损失问题，显著提升了模型性能与效率。


### [42] [OCSplats: Observation Completeness Quantification and Label Noise Separation in 3DGS](https://arxiv.org/abs/2508.01239)
*Han Ling,Xian Xu,Yinghui Sun,Quansen Sun*

Main category: cs.CV

TL;DR: 3DGS在真实场景中因标签噪声导致重建错误，OCSplats通过混合噪声评估和动态锚点分类，显著提升抗噪重建性能。

- Motivation: 解决3DGS在真实场景中因标签噪声（如移动物体、非朗伯表面和阴影）导致的重建错误问题。
- Method: 提出OCSplats框架，结合混合噪声评估和动态锚点分类技术，无需场景特定参数调整。
- Result: 实验表明OCSplats在不同复杂度的场景中均实现领先的重建性能和精确的噪声分类。
- Conclusion: OCSplats为抗噪3D重建提供了一种高效且通用的解决方案。


### [43] [NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2508.01248)
*Jiazhen Yan,Fan Wang,Weiwei Jiang,Ziqiang Li,Zhangjie Fu*

Main category: cs.CV

TL;DR: 论文提出NS-Net，通过NULL-Space投影和对比学习改进CLIP特征，解决AI生成图像检测中的语义干扰问题，检测准确率提升7.4%。

- Motivation: 生成模型（如GAN和扩散模型）生成的图像越来越逼真，现有检测器对未知模型泛化能力不足，尤其是语义内容相近时。
- Method: 提出NS-Net，利用NULL-Space投影解耦CLIP视觉特征中的语义信息，结合对比学习捕捉真实与生成图像的分布差异，并设计Patch Selection策略保留细粒度伪影。
- Result: 在包含40种生成模型的开放世界基准测试中，NS-Net检测准确率提升7.4%，泛化能力优于现有方法。
- Conclusion: NS-Net通过解耦语义信息和保留细粒度特征，显著提升了AI生成图像检测的泛化能力和准确性。


### [44] [DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing](https://arxiv.org/abs/2508.01250)
*Xiaoqin Wang,Xianxu Hou,Meidan Ding,Junliang Chen,Kaijun Deng,Jinheng Xie,Linlin Shen*

Main category: cs.CV

TL;DR: 论文提出了一种弱监督面部解析方法（WSFP），通过解耦表示框架DisFaceRep，减少对密集标注的依赖，显著提升了性能。

- Motivation: 现有面部解析方法依赖密集像素级标注，成本高且耗时。WSFP旨在通过弱监督（如图像级标签和自然语言描述）降低标注成本。
- Method: 提出DisFaceRep框架，包含显式（共现组件解耦策略）和隐式（文本引导解耦损失）机制，分离共现的面部组件。
- Result: 在CelebAMask-HQ、LaPa和Helen数据集上，DisFaceRep显著优于现有弱监督语义分割方法。
- Conclusion: WSFP任务具有挑战性，但DisFaceRep通过解耦表示有效解决了共现组件问题，为弱监督面部解析提供了新思路。


### [45] [ODOV: Towards Open-Domain Open-Vocabulary Object Detection](https://arxiv.org/abs/2508.01253)
*Yupeng Zhang,Ruize Han,Fangnan Zhou,Song Wang,Wei Feng,Liang Wan*

Main category: cs.CV

TL;DR: 该论文提出了一种开放域开放词汇（ODOV）物体检测的新方法，并构建了OD-LVIS基准数据集，涵盖18个复杂现实领域和1,203个类别。通过结合大型语言模型生成领域无关的文本提示，并学习图像中的领域嵌入，实现了领域自适应的检测。

- Motivation: 解决现实世界中物体检测的领域和类别变化问题，提升模型的适应能力。
- Method: 利用大型语言模型生成领域无关的文本提示，学习图像中的领域嵌入，并将其整合到类别嵌入中，形成领域特定的类别嵌入。
- Result: 构建了OD-LVIS数据集，并通过实验验证了方法的有效性，展示了其在ODOV检测任务中的优越性。
- Conclusion: 提出的ODOV检测方法和OD-LVIS数据集为现实世界物体检测提供了新的解决方案和评估基准。


### [46] [Self-Enhanced Image Clustering with Cross-Modal Semantic Consistency](https://arxiv.org/abs/2508.01254)
*Zihan Li,Wei Sun,Jing Hu,Jianhua Yin,Jianlong Wu,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出了一种基于跨模态语义一致性的自增强框架，用于高效图像聚类，通过两阶段方法显著提升性能。

- Motivation: 现有方法通常冻结预训练模型的编码器，导致任务无关表示与特定聚类任务需求不匹配，限制了性能。
- Method: 第一阶段通过跨模态语义一致性训练轻量级聚类头；第二阶段通过自增强微调策略联合优化视觉编码器和聚类头。
- Result: 在六个主流数据集上显著优于现有深度聚类方法，ViT-B/32模型甚至超越基于更大ViT-L/14的最先进方法。
- Conclusion: 该框架通过跨模态语义一致性和自增强微调，有效提升了图像聚类的性能。


### [47] [SpatioTemporal Difference Network for Video Depth Super-Resolution](https://arxiv.org/abs/2508.01259)
*Zhengxue Wang,Yuan Wu,Xiang Li,Zhiqiang Yan,Jian Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为STDNet的新方法，通过空间和时间差异分支解决视频深度超分辨率中的长尾分布问题。

- Motivation: 视频深度超分辨率在空间非平滑区域和时间变化区域存在长尾分布问题，影响了重建质量。
- Method: STDNet包含空间差异分支和时间差异分支，分别通过动态对齐RGB特征和优先传播时间变化信息来解决长尾问题。
- Result: 在多个数据集上的实验表明，STDNet优于现有方法。
- Conclusion: STDNet有效解决了视频深度超分辨率中的长尾分布问题，提升了重建质量。


### [48] [Enhancing Diffusion-based Dataset Distillation via Adversary-Guided Curriculum Sampling](https://arxiv.org/abs/2508.01264)
*Lexiao Zou,Gongwei Chen,Yanda Chen,Miao Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Adversary-guided Curriculum Sampling (ACS)的方法，通过对抗性损失引导扩散模型采样，以减少信息冗余并提升蒸馏数据集的多样性。

- Motivation: 现有数据集蒸馏方法在大规模或高分辨率图像下性能下降，且扩散模型生成的图像多样性不足导致信息冗余。
- Method: ACS将蒸馏数据集分为多个课程，通过对抗性损失引导扩散采样，逐步生成从简单到复杂的图像。
- Result: 实验显示ACS在Imagewoof和ImageNet-1k上分别提升了4.1%和2.1%。
- Conclusion: ACS有效提升了数据集蒸馏的多样性和性能。


### [49] [ModelNet40-E: An Uncertainty-Aware Benchmark for Point Cloud Classification](https://arxiv.org/abs/2508.01269)
*Pedro Alonso,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: ModelNet40-E是一个新的基准测试，用于评估点云分类模型在合成LiDAR噪声下的鲁棒性和校准性。

- Motivation: 现有基准测试缺乏对噪声和不确定性建模的细粒度评估，ModelNet40-E填补了这一空白。
- Method: 通过引入噪声污染的点云和点级不确定性标注（高斯噪声参数σ、μ），评估了PointNet、DGCNN和Point Transformer v3三种模型。
- Result: 所有模型在噪声增加时性能下降，但Point Transformer v3表现出更好的校准性，其预测的不确定性与测量不确定性更一致。
- Conclusion: ModelNet40-E为点云分类模型的鲁棒性和不确定性建模提供了新的评估工具，Point Transformer v3在噪声环境下表现最佳。


### [50] [SGCap: Decoding Semantic Group for Zero-shot Video Captioning](https://arxiv.org/abs/2508.01270)
*Zeyu Pan,Ping Li,Wenxiao Wang*

Main category: cs.CV

TL;DR: SGCap方法通过语义组解码和多样化句子选择模块，显著提升了零样本视频描述性能。

- Motivation: 现有零样本视频描述方法忽视时间动态性，且缺乏视频级监督。
- Method: 提出SGCap方法，包括语义组解码（SGD）、关键句子选择（KSS）和概率采样监督（PSS）模块。
- Result: 在多个基准测试中，SGCap显著优于现有零样本方法，甚至接近全监督方法性能。
- Conclusion: SGCap通过建模时间动态性和多样化监督，有效提升了零样本视频描述能力。


### [51] [PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation](https://arxiv.org/abs/2508.01272)
*Zonglei Jing,Xiao Yang,Xiaoqian Li,Siyuan Liang,Aishan Liu,Mingchuan Zhang,Xianglong Liu*

Main category: cs.CV

TL;DR: PromptSafe提出了一种轻量级的门控提示调优框架，通过文本监督和门控机制动态调整防御强度，有效减少T2I模型生成不安全内容，同时保持良性生成质量。

- Motivation: T2I模型易生成不安全内容，现有防御方法计算成本高且适应性差，PromptSafe旨在解决这些问题。
- Method: 利用LLM重写不安全提示构建文本训练语料，优化通用软提示，并通过门控机制动态调整防御强度。
- Result: PromptSafe在多个基准测试中实现了最低的不安全生成率（2.36%），同时保持高良性保真度。
- Conclusion: PromptSafe在安全性、泛化性和适应性方面表现出色，具有实际部署价值。


### [52] [Integrating Disparity Confidence Estimation into Relative Depth Prior-Guided Unsupervised Stereo Matching](https://arxiv.org/abs/2508.01275)
*Chuang-Wei Liu,Mingjian Sun,Cairong Zhao,Hanli Wang,Alexander Dvorkovich,Rui Fan*

Main category: cs.CV

TL;DR: 提出了一种无监督立体匹配框架，通过置信度估计和深度先验损失函数解决多视角一致性假设的模糊性问题。

- Motivation: 无监督立体匹配依赖多视角一致性假设，但易受重复模式和纹理缺失区域影响，现有方法利用稀疏对应关系学习深度排名信息效率低且引入噪声。
- Method: 提出一个包含置信度估计算法和两种深度先验损失函数的框架，通过局部一致性检查获得置信度，构建准密集对应关系学习深度排名，并提出双视差平滑损失。
- Result: 在KITTI Stereo基准测试中，该方法在所有无监督立体匹配方法中达到了最先进的匹配精度。
- Conclusion: 该方法通过高效利用3D几何知识和减少噪声，显著提升了无监督立体匹配的性能。


### [53] [GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification](https://arxiv.org/abs/2508.01293)
*Ngoc Bui Lam Quang,Nam Le Nguyen Binh,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Quan Nguyen,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出了一种结合视觉语言模型和多实例学习的框架，通过多代理生成临床描述和列表式文本编码策略，提升病理图像分类性能。

- Motivation: 现有方法依赖大型语言模型生成临床描述或固定长度提示，限制了信息的丰富性和领域特异性，导致视觉特征对齐不佳。
- Method: 1. 使用多代理系统生成基于病理教材的多样化临床描述；2. 采用列表式文本编码策略捕捉细粒度临床信号。
- Result: 在肾癌和肺癌数据集上表现优于单提示基线，接近最先进模型。
- Conclusion: 提出的框架通过更丰富的文本描述和编码策略，显著提升了病理图像分类的准确性和领域适应性。


### [54] [Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation](https://arxiv.org/abs/2508.01303)
*Shuangli Du,Jing Wang,Minghua Zhao,Zhenyu Xu,Jie Li*

Main category: cs.CV

TL;DR: 提出了一种基于不确定性引导的数据增强方法（UgDA），通过扰动RGB空间的图像统计特征生成未见域样本，提升立体匹配模型的跨域泛化能力。

- Motivation: 解决合成数据训练的立体匹配模型在真实数据上泛化能力差的问题，避免模型依赖域相关捷径。
- Method: 利用RGB空间的均值和标准差作为域特征，通过高斯分布建模扰动方向和强度的不确定性，生成多样化域样本，并强制特征一致性。
- Result: 在多个挑战性基准测试中显著提升了现有立体匹配网络的泛化性能。
- Conclusion: UgDA方法简单、架构无关，能有效提升模型对未见域的适应能力。


### [55] [C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor](https://arxiv.org/abs/2508.01311)
*Haoquan Lu,Hanzhe Liang,Jie Zhang,Chenxi Hu,Jinbao Wang,Can Gao*

Main category: cs.CV

TL;DR: 提出了一种名为C3D-AD的持续学习框架，用于3D异常检测，能够处理多类点云并适应新类别的出现。

- Motivation: 现有3D异常检测方法通常针对特定类别训练，且无法适应新类别的出现，限制了其实际应用。
- Method: 引入KAL提取通用局部特征，KAA机制学习新类别信息并丢弃冗余旧信息，RPP模块通过表示排练损失保持任务间表示一致性。
- Result: 在三个公开数据集上平均AUROC分别为66.4%、83.1%和63.4%。
- Conclusion: C3D-AD框架在多类3D异常检测中表现出色，并能持续适应新类别。


### [56] [P3P Made Easy](https://arxiv.org/abs/2508.01312)
*Seong Hun Lee,Patrick Vandewalle,Javier Civera*

Main category: cs.CV

TL;DR: 提出了一种新的代数解法解决P3P问题，通过简化多项式实现高效计算，性能与现有方法相当。

- Motivation: 解决P3P问题中需要高效且可靠的方法，适用于实时系统和教育场景。
- Method: 将问题转化为系数简单、计算高效的四次多项式。
- Result: 在合成数据集上验证了方法的鲁棒性和效率，性能与现有最优方法相当。
- Conclusion: 该方法简单高效，适用于实时系统和教育用途。


### [57] [Multimodal Attention-Aware Fusion for Diagnosing Distal Myopathy: Evaluating Model Interpretability and Clinician Trust](https://arxiv.org/abs/2508.01316)
*Mohsen Abbaspour Onari,Lucie Charlotte Magister,Yaoxin Wu,Amalia Lupi,Dario Creazzo,Mattia Tordin,Luigi Di Donatantonio,Emilio Quaia,Chao Zhang,Isel Grau,Marco S. Nobile,Yingqian Zhang,Pietro Liò*

Main category: cs.CV

TL;DR: 提出一种多模态注意力感知融合架构，结合全局和局部特征的深度学习模型，提升远端肌病的诊断准确性和可解释性。

- Motivation: 远端肌病具有遗传异质性和广泛临床表现，诊断困难，需结合全局和局部特征以提高准确性和可解释性。
- Method: 通过注意力门机制融合两种深度学习模型的特征，分别捕捉全局上下文和局部细节。
- Result: 在BUSI基准和专有数据集上实现高分类准确性，生成临床相关显著性图，但解剖特异性和临床实用性仍有不足。
- Conclusion: 需开发更丰富的上下文感知可解释性方法，并结合人类反馈，以满足临床实际需求。


### [58] [Referring Remote Sensing Image Segmentation with Cross-view Semantics Interaction Network](https://arxiv.org/abs/2508.01331)
*Jiaxing Yang,Lihe Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了一种名为CSINet的并行统一分割框架，通过跨视图语义交互网络解决现有方法在处理微小、模糊目标时的局限性。

- Motivation: 现有方法在处理遥感图像分割时，仅使用全图输入并嵌套跨尺度信息交互技术，对视觉显著目标有效，但对微小、模糊目标效果不佳。
- Method: 提出CSINet框架，通过跨视图窗口注意力模块（CVWin）补充全局和局部语义，并开发协作扩张注意力增强解码器（CDAD）整合多尺度特征。
- Result: 网络显著提升了全局和局部语义的利用，性能优于其他方法，同时保持较高速度。
- Conclusion: CSINet通过跨视图语义交互和协作解码器，有效解决了遥感图像分割中的尺度变化和目标模糊问题。


### [59] [Zero-shot Segmentation of Skin Conditions: Erythema with Edit-Friendly Inversion](https://arxiv.org/abs/2508.01334)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的零样本图像分割框架，用于检测皮肤红斑，通过生成编辑减少对标注数据的依赖。

- Motivation: 解决皮肤红斑检测中对大量标注数据的依赖问题，提供一种无需训练掩码的灵活诊断工具。
- Method: 利用扩散模型生成无红斑的参考图像，并通过颜色空间分析识别红斑区域。
- Result: 在初步实验中，该方法成功分离面部红斑，性能优于基线阈值技术。
- Conclusion: 结合生成扩散模型和统计颜色分割，为计算机辅助皮肤病学提供高效的红斑检测方法。


### [60] [StyleSentinel: Reliable Artistic Copyright Verification via Stylistic Fingerprints](https://arxiv.org/abs/2508.01335)
*Lingxiao Chen,Liqin Wang,Wei Lu*

Main category: cs.CV

TL;DR: StyleSentinel通过验证艺术作品的固有风格指纹来保护版权，优于现有方法。

- Motivation: 扩散模型的广泛使用导致个人艺术作品被未经授权使用，现有保护方法能力有限。
- Method: 采用语义自重建增强风格表达，融合多层图像特征编码风格指纹，建模为最小包围超球边界。
- Result: 在单样本验证任务中表现优异，并在在线平台验证了有效性。
- Conclusion: StyleSentinel为艺术作品版权保护提供了高效解决方案。


### [61] [Weakly-Supervised Image Forgery Localization via Vision-Language Collaborative Reasoning Framework](https://arxiv.org/abs/2508.01338)
*Ziqi Sheng,Junyan Wu,Wei Lu,Jiantao Zhou*

Main category: cs.CV

TL;DR: ViLaCo是一种视觉-语言协同推理框架，利用预训练的视觉-语言模型引入辅助语义监督，仅需图像级标签即可实现像素级伪造定位。

- Motivation: 现有弱监督图像伪造定位（WSIFL）方法主要依赖图像内一致性线索，缺乏外部语义指导，导致定位性能有限。
- Method: ViLaCo通过视觉-语言特征建模网络提取文本和视觉先验，自适应推理网络对齐语义和视觉特征，双预测头实现图像级分类和像素级定位，并引入对比块一致性模块增强伪造判别。
- Result: 在多个公开数据集上，ViLaCo显著优于现有WSIFL方法，检测和定位精度达到最优。
- Conclusion: ViLaCo通过视觉-语言协同推理有效弥补弱监督不足，实现高精度的伪造定位。


### [62] [SBP-YOLO:A Lightweight Real-Time Model for Detecting Speed Bumps and Potholes](https://arxiv.org/abs/2508.01339)
*Chuanqi Liang,Jie Fu,Lei Luo,Miao Yu*

Main category: cs.CV

TL;DR: SBP-YOLO是一种基于YOLOv11的轻量级检测框架，用于实时检测减速带和坑洼，优化了计算效率和嵌入式部署。

- Motivation: 随着新能源汽车对乘坐舒适性需求的增加，实时准确检测减速带和坑洼对预测性悬挂控制至关重要。
- Method: 模型集成了GhostConv、VoVGSCSPC和轻量级效率检测头（LEDH），并采用混合训练策略（NWD损失、知识蒸馏和天气增强）。
- Result: 实验显示，SBP-YOLO在mAP上达到87.0%（优于YOLOv11n 5.8%），并在Jetson AGX Xavier上以139.5 FPS运行。
- Conclusion: SBP-YOLO在智能悬挂系统中表现出高效实时性，适用于道路状况感知。


### [63] [Predicting Video Slot Attention Queries from Random Slot-Feature Pairs](https://arxiv.org/abs/2508.01345)
*Rongzhen Zhao,Jian Li,Juho Kannala,Joni Pajarinen*

Main category: cs.CV

TL;DR: 论文提出了一种名为RandSF.Q的新方法，通过改进视频对象中心学习（OCL）中的查询预测机制，显著提升了对象发现和动态建模的性能。

- Motivation: 当前视频OCL方法在查询预测中忽视了下一帧特征和动态学习，导致性能受限。
- Method: 设计了新的转换器，结合槽位和特征信息，并通过随机采样槽位-特征对训练转换器以学习动态。
- Result: 实验显示，该方法在对象发现上提升高达10分，并在动态建模等下游任务中表现优越。
- Conclusion: RandSF.Q通过改进查询预测机制，显著提升了视频OCL的性能，为相关任务提供了新思路。


### [64] [Effective Damage Data Generation by Fusing Imagery with Human Knowledge Using Vision-Language Models](https://arxiv.org/abs/2508.01380)
*Jie Wei,Erika Ardiles-Cruz,Aleksey Panasyuk,Erik Blasch*

Main category: cs.CV

TL;DR: 论文提出利用视觉语言模型（VLMs）融合图像与人类知识，解决HADR中数据不平衡和标注不准确问题，生成多样化损伤数据，提升分类效果。

- Motivation: 在HADR中，快速准确评估损伤至关重要，但现有深度学习方法因数据类别不平衡、中度损伤样本稀缺及人工标注不准确而难以泛化。
- Method: 利用视觉语言模型（VLMs）融合图像与人类知识，生成多样化的损伤数据。
- Result: 初步实验结果显示生成的数据质量良好，提升了不同级别结构损伤（建筑、道路等）的分类效果。
- Conclusion: 该方法为HADR中的损伤评估提供了更高效的数据生成和分类解决方案。


### [65] [A Full-Stage Refined Proposal Algorithm for Suppressing False Positives in Two-Stage CNN-Based Detection Methods](https://arxiv.org/abs/2508.01382)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: 本文提出了一种全阶段细化提案（FRP）算法，用于在基于CNN的两阶段行人检测框架中消除误检。通过训练和测试阶段的多种行人特征重评估策略，有效过滤低质量提案。

- Motivation: 解决行人检测中的误检问题，提升模型在资源受限设备上的性能。
- Method: 提出FRP算法，包括训练阶段的TFRP和测试阶段的CFRP与SFRP策略，通过行人特征重评估过滤低质量提案。
- Result: 在多个基准测试和SY-Metro数据集上验证了算法的有效性，显著减少了误检，并在嵌入式平台上提升了小行人检测器的性能。
- Conclusion: FRP算法通过全阶段优化，显著提升了行人检测的准确性，尤其在资源受限设备上表现优异。


### [66] [Lightweight Backbone Networks Only Require Adaptive Lightweight Self-Attention Mechanisms](https://arxiv.org/abs/2508.01385)
*Fengyun Li,Chao Zheng,Yangyang Fang,Jialiang Lan,Jianhua Liang,Luhao Zhang,Fa Si*

Main category: cs.CV

TL;DR: 论文提出了一种名为FWA的轻量级SoftMax注意力机制，通过自适应特征图大小解决计算效率不平衡问题，并结合GhostNet设计了轻量级混合主干网络LOLViT，在多项视觉任务中表现优异。

- Motivation: 解决卷积神经网络（CNNs）与注意力机制之间计算效率不平衡的问题，尤其是轻量级设计在长序列建模中的不足。
- Method: 提出FWA机制，通过窗口聚合生成少量关键序列进行注意力计算，并设计全局-局部特征融合机制，结合GhostNet构建LOLViT网络。
- Result: LOLViT在分类、检测和分割任务中表现优于同级CNN模型，推理速度显著提升（如LOLViT-X比MobileViT-X快5倍）。
- Conclusion: FWA和LOLViT有效解决了计算效率问题，为轻量级混合模型提供了高效解决方案。


### [67] [Construction of Digital Terrain Maps from Multi-view Satellite Imagery using Neural Volume Rendering](https://arxiv.org/abs/2508.01386)
*Josef X. Biberstein,Guilherme Cavalheiro,Juyeop Han,Sertac Karaman*

Main category: cs.CV

TL;DR: 本文提出了一种名为神经地形图（NTM）的方法，通过神经体积渲染技术直接从卫星图像中学习数字地形图，避免了传统多视图立体流程的繁琐预处理。

- Motivation: 随着机器人探索任务的复杂性增加，高质量数字地形图（DTM）的需求日益增长，但现有方法依赖繁琐的手动预处理。
- Method: 采用神经体积渲染技术，仅需每个图像像素的位置信息，无需深度或其他结构先验。
- Result: 在合成和真实卫星数据（地球和火星）上验证，地形预测精度接近卫星图像分辨率，即使相机参数不完美。
- Conclusion: NTM方法展示了直接从卫星图像生成高质量DTM的潜力，为行星探索提供了更高效的解决方案。


### [68] [Video-based Vehicle Surveillance in the Wild: License Plate, Make, and Model Recognition with Self Reflective Vision-Language Models](https://arxiv.org/abs/2508.01387)
*Pouya Parsa,Keya Li,Kara M. Kockelman,Seongjin Choi*

Main category: cs.CV

TL;DR: 该论文研究了利用视觉语言模型（VLMs）进行车牌识别（ALPR）和车辆品牌型号识别的方法，适用于手持智能手机或非静态车载摄像头拍摄的视频，解决了传统方法在动态场景下的局限性。

- Motivation: 传统车牌识别方法依赖专用硬件和手工OCR流程，在动态场景（如摄像头移动、遮挡等）下性能下降。视觉语言模型（VLMs）的发展为直接从任意图像中识别文本和语义属性提供了新可能。
- Method: 提出两种流程：1) 车牌识别：筛选清晰帧后，通过多模态提示策略调用VLM；2) 品牌型号识别：使用相同VLM，结合自反思模块对比查询图像与参考数据集。
- Result: 在智能手机数据集上，车牌识别准确率为91.67%，品牌型号识别为66.67%；在公开数据集UFPR-ALPR上分别为83.05%和61.07%。自反思模块平均提升品牌型号识别5.72%。
- Conclusion: VLMs为动态交通视频分析提供了一种低成本、可扩展的解决方案。


### [69] [Open-Attribute Recognition for Person Retrieval: Finding People Through Distinctive and Novel Attributes](https://arxiv.org/abs/2508.01389)
*Minjeong Park,Hongbeen Park,Sangwon Lee,Yoonha Jang,Jinkyu Kim*

Main category: cs.CV

TL;DR: 论文提出了开放属性识别人物检索任务（OAPR），解决现有方法在训练和推理中属性类别不一致的问题，并提出了一个学习通用身体部位表征的框架。

- Motivation: 现有基于属性的人物检索方法假设训练和推理时的属性类别一致，限制了实际应用；且预定义属性缺乏区分性。
- Method: 提出OAPR任务，设计了一个学习通用身体部位表征的框架，并重构了四个数据集用于开放属性识别。
- Result: 实验验证了OAPR任务的必要性和框架的有效性。
- Conclusion: OAPR任务和提出的框架解决了开放属性识别的挑战，为实际应用提供了更灵活的方法。


### [70] [Spatial-Frequency Aware for Object Detection in RAW Image](https://arxiv.org/abs/2508.01396)
*Zhuohua Ye,Liming Zhang,Hongru Han*

Main category: cs.CV

TL;DR: 提出了一种基于空间和频率域结合的RAW图像目标检测增强框架SFAE，通过频率域恢复被抑制的细节，并实现跨域融合。

- Motivation: 现有方法主要在空间域处理RAW图像，难以有效恢复被抑制的细节，因此转向频率域以分离关键特征。
- Method: 提出SFAE框架，包括频率带的空间化、跨域融合注意力模块和自适应非线性调整。
- Result: 通过结合空间和频率域特征，有效恢复了RAW图像中被抑制的细节。
- Conclusion: SFAE框架在RAW图像目标检测中表现出色，为跨域特征融合提供了新思路。


### [71] [ForenX: Towards Explainable AI-Generated Image Detection with Multimodal Large Language Models](https://arxiv.org/abs/2508.01402)
*Chuangchuang Tan,Jinglu Wang,Xiang Ming,Renshuai Tao,Yunchao Wei,Yao Zhao,Yan Lu*

Main category: cs.CV

TL;DR: ForenX是一种新方法，利用多模态大语言模型（MLLMs）检测AI生成图像的真实性，并提供符合人类认知的解释。通过专用提示和数据集ForgReason，提升了检测和解释能力。

- Motivation: 现有AI生成图像检测方法与人类认知分析存在差距，需要一种既能检测又能提供解释的方法。
- Method: 结合MLLMs和专用提示，分析伪造线索，并使用数据集ForgReason优化模型。
- Result: ForenX在两大基准测试中表现优异，解释能力通过主观评估验证。
- Conclusion: ForenX在检测AI生成图像和提供解释方面取得了显著进展，数据集和专用提示是关键。


### [72] [3DRot: 3D Rotation Augmentation for RGB-Based 3D Tasks](https://arxiv.org/abs/2508.01423)
*Shitian Yang,Deyu Li,Xiaoke Jiang,Lei Zhang*

Main category: cs.CV

TL;DR: 3DRot是一种几何一致的数据增强方法，通过旋转和镜像图像并同步更新相关数据，显著提升了3D检测任务的性能。

- Motivation: 解决RGB-based 3D任务中标注稀缺、昂贵以及传统图像变换破坏几何一致性的问题。
- Method: 提出3DRot，一种基于相机光学中心的旋转和镜像增强方法，同步更新RGB图像、相机内参、物体位姿和3D标注。
- Result: 在SUN RGB-D数据集上，3DRot显著提升了3D检测的IoU、旋转误差和mAP指标。
- Conclusion: 3DRot是一种通用且高效的数据增强方法，适用于多种3D任务。


### [73] [Capturing More: Learning Multi-Domain Representations for Robust Online Handwriting Verification](https://arxiv.org/abs/2508.01427)
*Peirong Zhang,Kai Ding,Lianwen Jin*

Main category: cs.CV

TL;DR: SPECTRUM是一种时频协同模型，通过多域表示学习提升在线手写验证（OHV）性能。

- Motivation: 探索多域表示学习在手写验证中的潜力，超越传统仅依赖时域特征的方法。
- Method: 结合多尺度交互器和自门控融合模块，实现时频特征的动态整合，并利用多域距离验证器提升判别能力。
- Result: 实验表明SPECTRUM性能优于现有OHV方法，验证了多域学习的有效性。
- Conclusion: 多域学习不仅提升手写验证性能，还为跨域研究提供了新方向。


### [74] [Hyperspectral Image Recovery Constrained by Multi-Granularity Non-Local Self-Similarity Priors](https://arxiv.org/abs/2508.01435)
*Zhuoran Peng,Yiqing Shen*

Main category: cs.CV

TL;DR: 提出了一种基于多粒度非局部自相似性先验的高光谱图像恢复模型，通过交替粗粒度和细粒度分解实现全局和局部信息的统一表示。

- Motivation: 现有非局部先验方法因固定格式因子无法适应多样缺失场景，需改进。
- Method: 交替使用粗粒度（Tucker分解）和细粒度（FCTN分解）处理非局部自相似性张量群。
- Result: 模型在像素和条纹等多种缺失场景中表现优异。
- Conclusion: 多粒度分解方法有效统一了全局、局部和非局部先验，提升了恢复效果。


### [75] [Uncertainty-Aware Segmentation Quality Prediction via Deep Learning Bayesian Modeling: Comprehensive Evaluation and Interpretation on Skin Cancer and Liver Segmentation](https://arxiv.org/abs/2508.01460)
*Sikha O K,Meritxell Riera-Marín,Adrian Galdran,Javier García Lopez,Julia Rodríguez-Comas,Gemma Piella,Miguel A. González Ballester*

Main category: cs.CV

TL;DR: 提出了一种无需真实标注即可预测图像分割质量的新框架，结合不确定性估计和聚合策略，显著提升了评估性能。

- Motivation: 临床环境中缺乏手动标注时，评估分割质量具有挑战性，现有模型缺乏可靠性指标。
- Method: 提出两种互补框架：一种基于预测分割和不确定性图，另一种整合原始图像、不确定性图和预测分割图；采用贝叶斯方法量化不确定性。
- Result: 在2D皮肤病变和3D肝脏分割数据集上表现优异，HAM10000数据集的R2分数达93.25，Pearson相关性为96.58。
- Conclusion: 框架在跨模态任务中表现稳健，不确定性整合显著提升了分割质量评估的可靠性。


### [76] [Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians](https://arxiv.org/abs/2508.01464)
*Quankai Gao,Iliyan Georgiev,Tuanfeng Y. Wang,Krishna Kumar Singh,Ulrich Neumann,Jae Shin Yoon*

Main category: cs.CV

TL;DR: Can3Tok是首个能够将大量高斯基元编码为低维潜在嵌入的3D场景级变分自编码器（VAE），解决了3D场景生成中的尺度不一致问题，并在DL3DV-10K数据集上验证了其有效性。

- Motivation: 现有的3D生成技术主要集中在对象级别，而场景级生成由于缺乏能够处理无边界和尺度不一致问题的模型，研究较少。
- Method: 提出了Can3Tok，一种3D场景级VAE，能够编码高斯基元为低维潜在嵌入，并设计了处理3D场景数据尺度不一致的通用流程。
- Result: 在DL3DV-10K数据集上，Can3Tok成功泛化到新场景，而其他方法在训练中无法收敛且无泛化能力。
- Conclusion: Can3Tok为3D场景生成提供了有效解决方案，并展示了在图像到3DGS和文本到3DGS生成中的应用潜力。


### [77] [EfficientGFormer: Multimodal Brain Tumor Segmentation via Pruned Graph-Augmented Transformer](https://arxiv.org/abs/2508.01465)
*Fatemeh Ziaeetabar*

Main category: cs.CV

TL;DR: 提出了一种名为EfficientGFormer的新型架构，结合预训练基础模型和图推理，用于高效且准确的3D脑肿瘤分割。

- Motivation: 脑肿瘤分割在神经影像学中仍具挑战性，主要由于肿瘤区域的异质性和体积推断的高计算成本。
- Method: 采用nnFormer作为模态感知编码器，将多模态MRI数据转换为补丁级嵌入，构建双边缘图以捕获空间邻接和语义相似性，并通过剪枝的GAT进行关系推理，同时使用蒸馏模块实现实时部署。
- Result: 在MSD Task01和BraTS 2021数据集上，EfficientGFormer在减少内存和推理时间的同时，达到了最先进的精度。
- Conclusion: 该工作为快速且准确的体积肿瘤分割提供了临床可行的解决方案，兼具可扩展性、可解释性和泛化性。


### [78] [MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2508.01525)
*Kuo Shi,Jie Lu,Shanshan Ye,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 提出了一种名为MiraGe的方法，通过学习生成器不变特征来检测AI生成图像，并在未见过的生成器上保持鲁棒性。

- Motivation: 现有方法在已知生成器上表现良好，但在新兴或未见过的生成器上性能下降，原因是特征嵌入重叠导致跨生成器分类不准确。
- Method: MiraGe通过最小化类内变化和最大化类间分离学习生成器不变特征，并利用多模态提示学习进一步优化CLIP。
- Result: 在多个基准测试中，MiraGe实现了最先进的性能，即使对未见过的生成器（如Sora）也保持鲁棒性。
- Conclusion: MiraGe通过学习生成器不变特征和多模态提示学习，显著提升了AI生成图像检测的泛化能力。


### [79] [ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models](https://arxiv.org/abs/2508.01533)
*Jiaxin Liu,Zhaolu Kang*

Main category: cs.CV

TL;DR: ReasonAct通过三阶段训练提升小模型在视频理解中的细粒度时序推理能力，结合T-GRPO与时序一致性建模，并在多个数据集上显著优于基线。

- Motivation: 当前小规模多模态模型在视频理解的细粒度时序推理上表现不足，需改进。
- Method: 三阶段训练：文本推理基础、视频微调、时序感知强化学习；结合T-GRPO与时序一致性建模；提出子动作分解机制。
- Result: 在HMDB51、UCF-101和Kinetics-400上分别达到67.2%、94.1%和78.9%准确率，显著优于基线。
- Conclusion: 渐进式训练方法使小模型在视频推理任务中表现优异且计算高效。


### [80] [MagicVL-2B: Empowering Vision-Language Models on Mobile Devices with Lightweight Visual Encoders via Curriculum Learning](https://arxiv.org/abs/2508.01540)
*Yi Liu,Xiao Xu,Zeyu Xu,Meng Zhang,Yibo Li,Haoyu Chen,Junkang Zhang,Qiang Wang,Jifa Sun,Siling Lin,Shengxun Cheng,Lingshu Zhang,Kang Wang*

Main category: cs.CV

TL;DR: MagicVL-2B是一种针对智能手机优化的轻量级视觉语言模型，通过动态分辨率方案和多模态课程学习策略，在保持高性能的同时显著降低功耗。

- Motivation: 解决视觉语言模型在移动设备上部署时的高计算和存储需求问题。
- Method: 采用轻量级视觉编码器（参数少于1亿）和动态分辨率方案，结合多模态课程学习策略逐步提升任务难度和数据密度。
- Result: 在标准基准测试中，MagicVL-2B性能与当前最优模型相当，同时设备功耗降低41.1%。
- Conclusion: MagicVL-2B为移动设备上的视觉语言应用提供了实用且高效的解决方案。


### [81] [E-VRAG: Enhancing Long Video Understanding with Resource-Efficient Retrieval Augmented Generation](https://arxiv.org/abs/2508.01546)
*Zeyu Xu,Junkang Zhang,Qiang Wang,Yi Liu*

Main category: cs.CV

TL;DR: E-VRAG是一种高效的视频检索增强生成框架，通过分层查询分解和轻量级VLM评分减少计算成本，同时利用全局统计分布和多视角问答提升准确性。

- Motivation: 现有视频RAG方法在检索效率和准确性之间难以平衡，尤其是处理复杂视频内容时。
- Method: 提出E-VRAG框架，包括帧预过滤、轻量级VLM评分、全局统计分布检索策略和多视角问答。
- Result: 在四个公开基准测试中，E-VRAG减少约70%计算成本且准确性更高。
- Conclusion: E-VRAG无需额外训练即可显著提升视频RAG任务的效率和准确性。


### [82] [A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.01548)
*Quan-Sheng Zeng,Yunheng Li,Qilong Wang,Peng-Tao Jiang,Zuxuan Wu,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: GlimpsePrune是一种动态剪枝框架，通过数据驱动的“一瞥”方法高效剪枝视觉令牌，保留基线性能的同时显著降低计算成本。

- Motivation: 现有固定压缩比方法无法适应不同复杂度的场景，导致信息丢失和性能下降。
- Method: 采用动态剪枝框架GlimpsePrune，通过单次前向传递剪枝无关视觉令牌。
- Result: 剪枝92.6%视觉令牌，性能与基线相当；GlimpsePrune+性能提升至基线110%。
- Conclusion: GlimpsePrune为构建更强大高效的LVLMs提供了新思路。


### [83] [EvoVLMA: Evolutionary Vision-Language Model Adaptation](https://arxiv.org/abs/2508.01558)
*Kun Ding,Ying Wang,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出了一种基于进化算法的自动搜索方法（EvoVLMA），用于优化预训练视觉语言模型的适应算法，无需人工设计。

- Motivation: 现有视觉语言模型适应方法依赖人工设计，耗时且需要专业知识，希望通过自动化方法解决这一问题。
- Method: 采用两阶段LLM辅助进化算法，优化特征选择和逻辑计算，并通过低精度代码转换、基于网络的代码执行和过程监控提升搜索效率。
- Result: 在8-shot图像分类任务中，EvoVLMA发现的算法比人工设计的APE算法准确率提高了1.91个百分点。
- Conclusion: EvoVLMA为预训练多模态模型的适应算法自动化优化提供了新思路。


### [84] [Adaptive LiDAR Scanning: Harnessing Temporal Cues for Efficient 3D Object Detection via Multi-Modal Fusion](https://arxiv.org/abs/2508.01562)
*Sara Shoouri,Morteza Tavakoli Taba,Hun-Seok Kim*

Main category: cs.CV

TL;DR: 提出了一种基于历史感知的自适应激光雷达扫描框架，显著降低能耗并保持3D物体检测性能。

- Motivation: 传统激光雷达扫描冗余且耗能高，限制了在资源受限平台上的实用性。
- Method: 使用轻量级预测网络和可微分掩码生成器，基于历史数据预测感兴趣区域，减少冗余扫描。
- Result: 在nuScenes和Lyft基准测试中，能耗降低65%以上，检测性能与传统方法相当或更优。
- Conclusion: 自适应扫描框架有效解决了激光雷达的冗余问题，提升了实用性。


### [85] [LetheViT: Selective Machine Unlearning for Vision Transformers via Attention-Guided Contrastive Learning](https://arxiv.org/abs/2508.01569)
*Yujia Tong,Tian Zhang,Jingling Yuan,Yuze Wang,Chuang Hu*

Main category: cs.CV

TL;DR: LetheViT是一种针对Vision Transformers (ViTs)的对比性遗忘方法，通过选择性掩码技术实现隐私合规与模型性能的平衡。

- Motivation: 隐私法规（如GDPR和CCPA）要求用户数据可撤回，需从模型中完全消除其影响，而传统精确遗忘方法计算成本高，近似方法更实用。
- Method: 通过选择性掩码实验揭示ViTs特性，提出LetheViT方法：使用掩码图像生成正对数，原始图像生成负对数，引导模型遗忘特定数据。
- Result: LetheViT在实验中表现优异，实现了隐私合规与模型性能的最佳平衡。
- Conclusion: LetheViT为ViTs提供了一种高效的近似遗忘方法，解决了随机数据遗忘的挑战。


### [86] [TopoImages: Incorporating Local Topology Encoding into Deep Learning Models for Medical Image Classification](https://arxiv.org/abs/2508.01574)
*Pengfei Gu,Hongxiao Wang,Yejia Zhang,Huimin Li,Chaoli Wang,Danny Chen*

Main category: cs.CV

TL;DR: 提出了一种名为TopoImages的新方法，通过编码图像局部拓扑结构来增强深度学习框架对拓扑特征的敏感性。

- Motivation: 现有图像处理方法在深度学习中缺乏对拓扑结构的敏感性，而拓扑结构（如连通性和环路）对理解图像内容（如生物医学对象）至关重要。
- Method: 利用持久同调（PH）编码图像块的几何和拓扑特征，生成称为TopoImage的多通道图像形式表示，并通过多视角TopoImages进一步丰富表示。
- Result: 在三个公共医学图像分类数据集上的实验表明，TopoImages显著提高了分类准确性。
- Conclusion: TopoImages方法具有高度通用性，可无缝集成到常见深度学习框架中，为数据分析提供了新视角。


### [87] [Harnessing Textual Semantic Priors for Knowledge Transfer and Refinement in CLIP-Driven Continual Learning](https://arxiv.org/abs/2508.01579)
*Lingfeng He,De Cheng,Huaijie Wang,Nannan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SECA的统一框架，利用文本先验的防遗忘和结构化特性，指导语义感知的知识迁移，并通过语义增强视觉原型优化模块提升分类器性能。

- Motivation: 解决持续学习中稳定性与可塑性之间的平衡问题，利用CLIP模型的文本语义先验来优化知识迁移和分类器设计。
- Method: 提出SG-AKT模块评估新图像与历史知识的语义相关性，并自适应聚合相关知识；引入SE-VPR模块利用文本嵌入优化视觉原型。
- Result: 在多个基准测试中验证了SECA框架的有效性。
- Conclusion: SECA框架通过结合文本语义先验，显著提升了持续学习的性能，解决了稳定性与可塑性的矛盾。


### [88] [Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models](https://arxiv.org/abs/2508.01582)
*Xinhui Li,Xinyu He,Qiming Hu,Xiaojie Guo*

Main category: cs.CV

TL;DR: 论文提出了Set Pivot Learning (SPL)方法，重新定义了基于视觉基础模型(VFMs)的领域泛化(DG)，强调动态适应和VFM为中心的调整。

- Motivation: 传统DG假设目标域在训练时不可访问，但VFMs的出现使这一假设过时。SPL旨在更适应当前研究和应用需求。
- Method: SPL通过动态适应和VFM为中心的调整，提出动态提示微调方法，结合动态类感知提示器和提示引导的特征聚焦器。
- Result: 在基准数据集上的实验表明，该方法优于现有技术，尤其在广义分割任务中表现突出。
- Conclusion: SPL为领域迁移任务提供了更灵活和有效的解决方案，适用于现实世界的动态场景。


### [89] [A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction](https://arxiv.org/abs/2508.01585)
*Hua Yu,Yaqing Hou,Xu Gui,Shanshan Feng,Dongsheng Zhou,Qiang Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为STCN的新方法，用于解决随机和连续的人体运动预测问题，通过两阶段方法提升运动序列的平滑性和多样性。

- Motivation: 现有方法在学习连续时间动态和预测随机运动序列时面临挑战，容易忽视复杂人体运动的灵活性并陷入模式崩溃。
- Method: STCN分为两阶段：第一阶段通过时空连续网络生成平滑运动序列，并引入锚点集防止模式崩溃；第二阶段学习运动序列的高斯混合分布，并通过锚点采样缓解类内差异。
- Result: 在Human3.6M和HumanEva-I数据集上，STCN在多样性和准确性上表现出竞争力。
- Conclusion: STCN通过两阶段设计和锚点集策略，有效提升了随机人体运动预测的性能。


### [90] [Lifelong Person Re-identification via Privacy-Preserving Data Replay](https://arxiv.org/abs/2508.01587)
*Mingyu Wang,Haojie Liu,Zhiyong Li,Wei Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种隐私保护的终身行人重识别方法Pr^2R，通过将信息压缩到像素空间，避免存储原始数据，同时提升重放样本的代表性。

- Motivation: 解决终身行人重识别中数据隐私问题，同时避免因不存储原始样本导致的性能下降。
- Method: 通过将多个真实图像的信息蒸馏到单个图像中，生成压缩样本，并在风格重放阶段进行双重对齐。
- Result: 在多个基准测试中，Pr^2R比现有方法在顺序任务中准确率提高了4%和6%。
- Conclusion: Pr^2R有效保护数据隐私并提升性能，适用于终身学习场景。


### [91] [Self-Navigated Residual Mamba for Universal Industrial Anomaly Detection](https://arxiv.org/abs/2508.01591)
*Hanxi Li,Jingqi Wu,Lin Yuanbo Wu,Mingliang Li,Deyin Liu,Jialie Shen,Chunhua Shen*

Main category: cs.CV

TL;DR: SNARM是一种新型工业异常检测框架，通过自参考学习和动态残差比较提升检测性能。

- Motivation: 传统方法依赖预训练特征，无法动态优化异常检测。SNARM旨在通过自参考学习增强异常判别能力。
- Method: 通过计算测试图像块与训练特征库的残差，选择高正常性块作为参考，生成残差特征，并通过多头部Mamba模块动态聚焦异常区域。
- Result: 在MVTec AD、MVTec 3D和VisA基准测试中，SNARM实现了SOTA性能，各项指标均有显著提升。
- Conclusion: SNARM通过自参考学习和动态残差比较，显著提升了工业异常检测的准确性和鲁棒性。


### [92] [DMTrack: Spatio-Temporal Multimodal Tracking via Dual-Adapter](https://arxiv.org/abs/2508.01592)
*Weihong Li,Shaohua Dong,Haonan Lu,Yanhao Zhang,Heng Fan,Libo Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为DMTrack的双适配器架构，用于时空多模态跟踪，通过STMA和PMCA模块实现高效特征调整和跨模态融合，仅需0.93M可训练参数即达到SOTA性能。

- Motivation: 解决多模态跟踪中时空特征调整和跨模态融合的挑战，提出轻量高效的适配器架构。
- Method: 引入STMA模块调整单模态时空特征，PMCA模块通过浅层和深层适配器逐步实现跨模态提示和融合。
- Result: 在五个基准测试中达到SOTA性能，仅需0.93M可训练参数。
- Conclusion: DMTrack通过双适配器设计实现了高效的多模态跟踪，为轻量级跨模态融合提供了新思路。


### [93] [CLIMD: A Curriculum Learning Framework for Imbalanced Multimodal Diagnosis](https://arxiv.org/abs/2508.01594)
*Kai Han,Chongwen Lyu,Lele Ma,Chengxuan Qian,Siqi Ma,Zheng Pang,Jun Chen,Zhe Liu*

Main category: cs.CV

TL;DR: 提出了一种名为CLIMD的课程学习框架，用于解决多模态医学数据中的类别不平衡问题，通过结合模态内置信度和模态间互补性指标，逐步适应复杂类别分布。

- Motivation: 解决多模态医学数据中因类别不平衡导致少数类特征学习不足的问题，现有方法易过拟合或欠拟合且难以捕捉跨模态交互。
- Method: 设计多模态课程测量器（结合模态内置信度和模态间互补性）和类别分布引导的训练调度器，逐步适应不平衡分布。
- Result: 在多个多模态医学数据集上表现优于现有方法，能有效处理不平衡数据。
- Conclusion: CLIMD作为一种即插即用框架，可提升多模态疾病诊断准确性，代码已开源。


### [94] [Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment](https://arxiv.org/abs/2508.01602)
*Lubin Gan,Jing Zhang,Linhao Qu,Yijun Wang,Siying Wu,Xiaoyan Sun*

Main category: cs.CV

TL;DR: FG-PAN是一种针对数字病理学的零样本框架，通过局部特征细化模块和细粒度文本描述生成模块，提高了脑肿瘤亚型分类的准确性。

- Motivation: 脑肿瘤亚型的细粒度分类因形态学变化细微和标注数据稀缺而极具挑战性，现有视觉语言模型在细粒度特征捕捉上表现不足。
- Method: FG-PAN包含局部特征细化模块（增强视觉特征）和细粒度文本描述生成模块（利用大语言模型生成语义原型），通过视觉与语义特征对齐提高分类性能。
- Result: 在多个公共病理数据集（如EBRAINS和TCGA）上，FG-PAN实现了零样本脑肿瘤亚型分类的最优性能和强泛化能力。
- Conclusion: FG-PAN通过结合视觉与语义特征对齐，显著提升了细粒度脑肿瘤亚型分类的准确性和鲁棒性。


### [95] [Towards Generalizable AI-Generated Image Detection via Image-Adaptive Prompt Learning](https://arxiv.org/abs/2508.01603)
*Yiheng Li,Zichang Tan,Zhen Lei,Xu Zhou,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种名为IAPL的新框架，通过自适应模块提升AI生成图像检测的灵活性，针对未见过的生成器表现优异。

- Motivation: 现有方法在窄范围生成器上训练的模型难以泛化到未知来源，需要更灵活的检测方法。
- Method: IAPL包含两个模块：条件信息学习器和置信驱动自适应预测，前者学习伪造和图像特定条件，后者优化浅层可学习标记并选择高置信度视图。
- Result: 在UniversalFakeDetect和GenImage数据集上分别达到95.61%和96.7%的平均准确率。
- Conclusion: IAPL通过自适应调整输入图像的提示，显著提升了检测性能，适用于多样化伪造图像。


### [96] [From Pixels to Places: A Systematic Benchmark for Evaluating Image Geolocalization Ability in Large Language Models](https://arxiv.org/abs/2508.01608)
*Lingyao Li,Runlong Yu,Qikai Hu,Bowei Li,Min Deng,Yang Zhou,Xiaowei Jia*

Main category: cs.CV

TL;DR: IMAGEO-Bench是一个评估大型语言模型（LLMs）在图像地理定位任务中表现的基准，揭示了性能差异和地理偏见。

- Motivation: 探索LLMs在图像地理定位任务中的潜力，填补现有研究的空白。
- Method: 引入IMAGEO-Bench基准，使用三个多样化数据集评估10种先进LLMs的准确性、距离误差、地理偏见和推理过程。
- Result: 闭源模型表现更优，但存在地理偏见（高资源地区表现更好）。成功定位依赖于识别城市环境、户外场景和地标。
- Conclusion: IMAGEO-Bench为LLMs的空间推理能力提供了严格评估，对构建地理定位感知AI系统有重要意义。


### [97] [LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding](https://arxiv.org/abs/2508.01617)
*Xuanzhao Dong,Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Peijie Qiu,Shao Tang,Xin Li,Yalin Wang*

Main category: cs.CV

TL;DR: LLaDA-MedV是一种基于扩散模型的大型语言模型，专为生物医学图像理解设计，性能优于现有模型。

- Motivation: 填补扩散模型在生物医学视觉语言模型领域的空白，提升性能。
- Method: 通过视觉指令调优，结合扩散模型技术，优化训练和推理策略。
- Result: 在开放和封闭任务中表现优异，刷新多项基准记录。
- Conclusion: LLaDA-MedV在生物医学图像理解中表现出色，为未来研究提供新方向。


### [98] [Rate-distortion Optimized Point Cloud Preprocessing for Geometry-based Point Cloud Compression](https://arxiv.org/abs/2508.01633)
*Wanhao Ma,Wei Zhang,Shuai Wan,Fuzheng Yang*

Main category: cs.CV

TL;DR: 提出了一种结合深度学习的预处理框架，显著提升了G-PCC的压缩效率，同时保持其兼容性和低计算开销。

- Motivation: G-PCC作为国际标准，虽然通用性强，但压缩效率不及深度学习方法。研究旨在提升其效率而不牺牲兼容性。
- Method: 通过压缩导向的体素化网络和可微G-PCC替代模型联合优化，实现端到端梯度传播。
- Result: 实验显示平均BD-rate降低了38.84%，且无需修改解码器。
- Conclusion: 该工作为传统编解码器与深度学习的结合提供了实用路径，适合实际部署。


### [99] [Glass Surface Segmentation with an RGB-D Camera via Weighted Feature Fusion for Service Robots](https://arxiv.org/abs/2508.01639)
*Henghong Lin,Zihan Zhu,Tao Wang,Anastasia Ioannou,Yuanshui Huang*

Main category: cs.CV

TL;DR: 提出了一种动态融合RGB和深度特征的加权特征融合（WFF）模块，用于玻璃表面分割，并发布了MJU-Glass数据集。实验显示WFF显著提升了分割精度。

- Motivation: 解决玻璃表面分割中RGB和深度信息融合的问题，应对透明度、反射和遮挡等挑战。
- Method: 设计WFF模块，动态自适应融合RGB和深度特征，可即插即用集成到不同网络架构中。
- Result: WFF模块显著提升分割性能，mIoU和bIoU均有提升，PSPNet集成后bIoU提高7.49%。
- Conclusion: WFF模块和MJU-Glass数据集为机器人玻璃分割提供了有效框架，减少碰撞风险。


### [100] [Minimal High-Resolution Patches Are Sufficient for Whole Slide Image Representation via Cascaded Dual-Scale Reconstruction](https://arxiv.org/abs/2508.01641)
*Yujian Liu,Yuechuan Lin,Dongxu Shen,Haoran Li,Yutong Wang,Xiaoli Liu,Shidang Xu*

Main category: cs.CV

TL;DR: 提出了一种名为CDSR的双尺度重建框架，通过选择性采样和局部到全局网络，仅需少量高分辨率补丁即可实现高效的WSI分析。

- Motivation: 解决WSI分析中因领域差距和补丁冗余导致的性能下降和计算成本高的问题。
- Method: 采用两阶段选择性采样策略和局部到全局网络，结合精细局部细节与全局上下文信息。
- Result: 在多个数据集上，CDSR仅需少量补丁即可显著提升分类任务的准确率和AUC。
- Conclusion: CDSR在效率和形态保真度上优于现有方法，为WSI分析提供了更优解决方案。


### [101] [StrandDesigner: Towards Practical Strand Generation with Sketch Guidance](https://arxiv.org/abs/2508.01650)
*Na Zhang,Moran Li,Chengming Xu,Han Feng,Xiaobin Hu,Jiangning Zhang,Weijian Cao,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出首个基于草图的发丝生成模型，通过可学习的发丝上采样策略和多尺度自适应条件机制，实现更精细的控制和更高的真实性。

- Motivation: 传统基于文本或图像的扩散模型在生成发型时缺乏精确性和用户友好性，草图输入能提供更精细的控制。
- Method: 采用可学习的发丝上采样策略和多尺度自适应条件机制，结合Transformer和扩散头确保多尺度一致性。
- Result: 在多个基准数据集上表现优于现有方法，生成结果更真实且精确。
- Conclusion: 提出的方法在发丝生成任务中表现出色，代码将开源。


### [102] [DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding](https://arxiv.org/abs/2508.01651)
*Hanqing Wang,Zhenhao Zhang,Kaiyang Ji,Mingyu Liu,Wenti Yin,Yuchao Chen,Zhirui Liu,Xiangyu Zeng,Tianxiang Gui,Hangxing Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的3D物体可供性预测方法DAG，通过利用文本到图像扩散模型的内部表示空间，解决了现有方法泛化能力不足的问题。

- Motivation: 现有方法通过学习演示图像预测3D物体的可触摸区域，但未能捕捉图像中的通用可供性知识，导致泛化能力差。
- Method: 提出DAG框架，利用文本到图像扩散模型的冻结内部表示空间，并结合可供性块和多源可供性解码器，实现3D密集可供性预测。
- Result: 实验表明，DAG在性能上优于现有方法，并展现出开放世界的泛化能力。
- Conclusion: DAG通过扩散模型的内部表示空间，成功提取了通用可供性知识，为3D可供性预测提供了新思路。


### [103] [MAP: Mitigating Hallucinations in Large Vision-Language Models with Map-Level Attention Processing](https://arxiv.org/abs/2508.01653)
*Chenxi Li,Yichen Guo,Benfang Qian,Jinhao You,Kai Tang,Yaosong Du,Zonghao Zhang,Xiande Huang*

Main category: cs.CV

TL;DR: 提出了一种新的Map-Level Attention Processing（MAP）方法，通过注意力机制改进大型视觉语言模型（LVLM）的事实一致性，减少幻觉问题。

- Motivation: 大型视觉语言模型在多模态任务中表现优异，但仍存在幻觉问题，即生成内容与视觉输入不一致。
- Method: 采用Layer-Wise Criss-Cross Attention和Global-Local Logit Fusion机制，通过注意力操作和全局-局部预测融合提升事实一致性。
- Result: 在POPE、MME和MMHal-Bench等基准测试中，MAP方法显著提高了模型的真实性和性能。
- Conclusion: MAP方法展示了基于地图级解码策略的潜力，为减少LVLM的幻觉问题提供了有效解决方案。


### [104] [Single Point, Full Mask: Velocity-Guided Level Set Evolution for End-to-End Amodal Segmentation](https://arxiv.org/abs/2508.01661)
*Zhixuan Li,Yujia Liu,Chen Hui,Weisi Lin*

Main category: cs.CV

TL;DR: VELA是一种基于速度驱动的水平集方法，通过点提示实现端到端的模态分割，优于现有方法。

- Motivation: 现有模态分割方法依赖强提示（如掩码或边界框），成本高且不实用；同时缺乏几何可解释性。
- Method: VELA通过点提示构建初始水平集函数，利用可微分网络预测形状运动场，逐步演化轮廓。
- Result: 在COCOA-cls、D2SA和KINS基准测试中，VELA优于现有方法，仅需单点提示。
- Conclusion: VELA通过几何建模在弱提示下实现高效模态分割，具有可解释性和灵活性。


### [105] [Shape Distribution Matters: Shape-specific Mixture-of-Experts for Amodal Segmentation under Diverse Occlusions](https://arxiv.org/abs/2508.01664)
*Zhixuan Li,Yujia Liu,Chen Hui,Jeonghaeng Lee,Sanghoon Lee,Weisi Lin*

Main category: cs.CV

TL;DR: ShapeMoE提出了一种基于形状的稀疏混合专家框架，用于解决模态分割任务中复杂遮挡和形状多样性的问题。

- Motivation: 现有方法难以处理多样化的形状模式，导致模态分割效果不佳。ShapeMoE通过动态路由和轻量级专家设计，提升分割精度。
- Method: ShapeMoE学习潜在形状分布空间，动态将对象路由到适合其形状特征的专家。每个专家专注于特定形状模式的遮挡区域预测。
- Result: 在COCOA-cls、KINS和D2SA数据集上，ShapeMoE表现优于现有方法，尤其在遮挡区域分割上效果显著。
- Conclusion: ShapeMoE通过形状感知路由和轻量级专家设计，有效提升了模态分割的性能和效率。


### [106] [Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models](https://arxiv.org/abs/2508.01667)
*Zhixiang Wei,Xiaoxiao Ma,Ruishen Yan,Tao Tu,Huaian Chen,Jinjin Zheng,Yi Jin,Enhong Chen*

Main category: cs.CV

TL;DR: Rein++是一个基于视觉基础模型（VFM）的高效语义分割框架，通过域泛化（Rein-G）和域适应（Rein-A）解决数据规模差异和域分布偏移问题，显著提升了模型在有限数据和多样化场景下的性能。

- Motivation: 语义分割任务中，视觉基础模型面临数据规模差异和域分布偏移的挑战，限制了其应用效果。
- Method: Rein++结合了域泛化（Rein-G）和域适应（Rein-A）方法，通过实例感知令牌和语义转移模块优化模型，仅微调少量参数。
- Result: 实验表明，Rein++在高效训练下显著优于现有方法，适用于大规模模型。
- Conclusion: Rein++是一种高效、泛化能力强且适应性强的语义分割解决方案，适用于多样化场景。


### [107] [Benchmarking Adversarial Patch Selection and Location](https://arxiv.org/abs/2508.01676)
*Shai Kimhi,Avi Mendlson,Moshe Kimhi*

Main category: cs.CV

TL;DR: PatchMap是首个空间详尽的对抗性补丁放置基准，通过评估1.5亿次前向传播揭示了小补丁（仅占图像的2%）在特定热点区域能显著降低模型置信度。提出的分割引导放置方法提升了攻击成功率8-13%。

- Motivation: 对抗性补丁攻击威胁现代视觉模型的可靠性，需要系统性研究补丁放置的影响。
- Method: 构建PatchMap基准，评估1.5亿次前向传播；提出基于分割的补丁放置启发式方法，无需梯度查询。
- Result: 揭示了补丁放置的热点区域，分割引导方法在五种架构上提升攻击成功率8-13%。
- Conclusion: PatchMap为位置感知防御和自适应攻击研究提供了工具，未来将发布更全面的基准。


### [108] [Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models](https://arxiv.org/abs/2508.01678)
*Zhaochen Wang,Yiwei Wang,Yujun Cai*

Main category: cs.CV

TL;DR: Prompt-in-Image方法将文本指令嵌入图像中，统一通过视觉通道处理，减少了多模态对齐的挑战。Qwen2.5-VL性能提升，而LLaVA-1.5和InstructBLIP性能下降。

- Motivation: 解决视觉语言模型（VLMs）因多模态信息对齐困难导致的幻觉问题。
- Method: 提出Prompt-in-Image方法，将文本指令直接嵌入图像，强制模型通过视觉通道处理所有内容。
- Result: Qwen2.5-VL性能提升（POPE准确率提高4.1%），LLaVA-1.5和InstructBLIP性能显著下降。
- Conclusion: Prompt-in-Image方法对某些VLMs有效，但效果因模型视觉编码器特性而异。


### [109] [DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing](https://arxiv.org/abs/2508.01684)
*Yufeng Chi,Huimin Ma,Kafeng Wang,Jianmin Li*

Main category: cs.CV

TL;DR: DisCo3D提出了一种新框架，通过将3D一致性先验知识蒸馏到2D编辑器中，解决了3D编辑中的多视角一致性问题。

- Motivation: 扩散模型在2D图像生成和编辑中表现出色，但扩展到3D编辑时面临多视角一致性的挑战，现有方法存在收敛慢和模糊伪影问题。
- Method: DisCo3D通过多视角输入微调3D生成器进行场景适应，然后通过一致性蒸馏训练2D编辑器，最终通过高斯泼溅优化编辑后的多视角输出为3D表示。
- Result: 实验结果表明，DisCo3D在多视角一致性上表现稳定，编辑质量优于现有方法。
- Conclusion: DisCo3D为3D编辑提供了一种高效且一致的新方法。


### [110] [Register Anything: Estimating "Corresponding Prompts" for Segment Anything Model](https://arxiv.org/abs/2508.01697)
*Shiqi Huang,Tingfa Xu,Wen Yan,Dean Barratt,Yipeng Hu*

Main category: cs.CV

TL;DR: 论文提出了一种名为PromptReg的训练免费图像配准方法，通过直接搜索对应提示简化了传统的两步区域匹配过程，利用预训练分割模型实现高效配准。

- Motivation: 传统图像配准方法需要先分割感兴趣区域再匹配，步骤繁琐。本文旨在简化这一过程，通过直接搜索对应提示实现高效配准。
- Method: 提出“对应提示问题”，并通过“反向提示”解决方案生成主提示和辅助提示，结合新颖的配准算法实现多对ROI匹配。
- Result: 在多种3D和2D图像配准任务中表现优异，优于基于强度的迭代算法和学习型网络，甚至与需要全分割训练数据的弱监督方法竞争。
- Conclusion: PromptReg方法简化了配准流程，无需训练即可实现高效区域匹配，为图像配准提供了新思路。


### [111] [Versatile Transition Generation with Image-to-Video Diffusion](https://arxiv.org/abs/2508.01698)
*Zuhao Yang,Jiahui Zhang,Yingchen Yu,Shijian Lu,Song Bai*

Main category: cs.CV

TL;DR: VTG是一个多功能过渡视频生成框架，通过插值初始化、双向运动微调和表示对齐正则化，解决了现有方法在平滑性和语义一致性上的不足。

- Motivation: 现有方法在给定首尾帧和文本提示下生成平滑且合理的过渡视频方面研究不足。
- Method: VTG采用插值初始化、双向运动微调和表示对齐正则化，提升生成质量。
- Result: VTG在TransitBench基准测试中表现优异，覆盖概念混合和场景过渡任务。
- Conclusion: VTG在过渡视频生成中表现出色，为未来研究提供了统一基准。


### [112] [TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding](https://arxiv.org/abs/2508.01699)
*Zuhao Yang,Yingchen Yu,Yunqing Zhao,Shijian Lu,Song Bai*

Main category: cs.CV

TL;DR: TimeExpert是一种基于Mixture-of-Experts的视频大语言模型，通过动态路由任务特定令牌（如时间戳、显著性分数）到专门专家，提高了视频时间定位任务的性能。

- Motivation: 现有视频大语言模型对所有任务令牌采用相同静态处理路径，无法区分时间定位、显著性评估和文本生成等不同任务的需求。
- Method: 提出TimeExpert，利用Mixture-of-Experts架构，动态路由任务特定令牌到专门专家，实现高效计算和精确处理。
- Result: TimeExpert在密集视频描述、时刻检索和视频高光检测等任务中表现优异，达到最先进水平。
- Conclusion: TimeExpert通过任务分解和动态路由，显著提升了视频时间定位任务的性能，适用于多样化应用。


### [113] [LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for Autonomous Driving](https://arxiv.org/abs/2508.01704)
*Luqi Cheng,Zhangshuo Qi,Zijie Zhou,Chao Lu,Guangming Xiong*

Main category: cs.CV

TL;DR: 提出LT-Gaussian方法，用于高效更新基于3D高斯泼溅的自动驾驶地图，通过多模态高斯泼溅、结构变化检测和目标更新模块实现。

- Motivation: 解决3D高斯泼溅地图因时间和计算成本高而难以更新的问题。
- Method: 结合多模态高斯泼溅、结构变化检测和目标更新模块，对比新旧数据实现地图更新。
- Result: 在nuScenes数据集上验证，LT-Gaussian能高效更新地图并提升重建质量。
- Conclusion: LT-Gaussian通过利用新旧场景信息，实现了高效且高质量的地图更新。


### [114] [GAID: Frame-Level Gated Audio-Visual Integration with Directional Perturbation for Text-Video Retrieval](https://arxiv.org/abs/2508.01711)
*Bowen Yang,Yun Cao,Chen He,Xiaosu Su*

Main category: cs.CV

TL;DR: GAID框架通过帧级门控融合和方向性自适应语义扰动，提升了文本-视频检索的多模态表示性能。

- Motivation: 现有方法主要依赖视觉线索，忽略了音频语义或采用粗粒度融合策略，导致多模态表示不理想。
- Method: GAID包含帧级门控融合（FGF）和方向性自适应语义扰动（DASP），分别实现细粒度对齐和增强鲁棒性。
- Result: 在多个数据集上取得最先进结果，且效率显著提升。
- Conclusion: GAID通过互补模块优化多模态表示，实现了高效且鲁棒的文本-视频检索。


### [115] [HateClipSeg: A Segment-Level Annotated Dataset for Fine-Grained Hate Video Detection](https://arxiv.org/abs/2508.01712)
*Han Wang,Zhuoran Wang,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: HateClipSeg是一个大规模多模态数据集，提供视频级和片段级标注，用于检测视频中的仇恨言论，并提出了三个任务来评估模型性能。

- Motivation: 现有数据集缺乏细粒度标注，且多模态内容的复杂性使得视频中的仇恨言论检测具有挑战性。
- Method: 通过三阶段标注流程构建HateClipSeg数据集，包含11,714个标注片段，分为正常和五类攻击性内容。提出三个任务评估模型性能。
- Result: 当前模型在任务中表现不佳，突显需要更复杂的多模态和时间感知方法。
- Conclusion: HateClipSeg数据集公开可用，为仇恨言论检测研究提供了新资源。


### [116] [Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation](https://arxiv.org/abs/2508.01713)
*Julia Hindel,Ema Mekic,Enamundram Naga Karthik,Rohit Mohan,Daniele Cattaneo,Maria Kalweit,Abhinav Valada*

Main category: cs.CV

TL;DR: 论文提出了一种改进的类增量语义分割方法TOPICS+，专为手术场景设计，通过引入Dice损失、分层伪标签和定制标签分类法，提升了模型在动态手术环境中的表现。

- Motivation: 解决静态数据集训练的模型在动态手术环境中适应性不足的问题，同时避免灾难性遗忘。
- Method: 在TOPICS方法基础上，引入Dice损失、分层伪标签和定制标签分类法，并设计了六个新的CISS基准测试。
- Result: 提出了TOPICS+方法，并在Syn-Mediverse数据集上扩展了144个类别的标签，代码和模型已公开。
- Conclusion: TOPICS+在手术场景中表现出色，为类增量语义分割提供了新的解决方案。


### [117] [Granular Concept Circuits: Toward a Fine-Grained Circuit Discovery for Concept Representations](https://arxiv.org/abs/2508.01728)
*Dahee Kwon,Sehyun Lee,Jaesik Choi*

Main category: cs.CV

TL;DR: 论文提出了一种名为GCC的电路发现方法，用于识别深度视觉模型中与特定视觉概念相关的神经元电路。

- Motivation: 深度视觉模型的分布式表示使得定位特定视觉概念的编码位置具有挑战性，需要一种有效的方法来揭示模型内部的细粒度概念电路。
- Method: 通过迭代评估神经元间的功能依赖性和语义对齐，构建与查询相关的多个概念电路（GCC）。
- Result: GCC方法在多种深度图像分类模型中验证了其多功能性和有效性。
- Conclusion: GCC首次实现了细粒度视觉概念电路的自动发现，为模型提供了概念层面的解释。


### [118] [Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos](https://arxiv.org/abs/2508.01730)
*Jianbo Ma,Hui Luo,Qi Chen,Yuankai Qi,Yumei Sun,Amin Beheshti,Jianlin Zhang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: AMOT是一种多目标跟踪方法，通过结合外观和运动线索，解决了无人机视频中视角变化和复杂运动带来的跟踪挑战。

- Motivation: 无人机视频中频繁的视角变化和复杂的运动动态导致跟踪不稳定和身份关联模糊，现有方法未能充分利用外观和运动的时空交互。
- Method: AMOT提出外观-运动一致性矩阵（AMC）和运动感知轨迹延续模块（MTC），分别用于增强身份关联的可靠性和减少因漏检导致的轨迹断裂。
- Result: 在VisDrone2019、UAVDT和VT-MOT-UAV三个基准测试中，AMOT表现优于现有方法，且无需额外训练即可泛化。
- Conclusion: AMOT通过联合利用外观和运动线索，显著提升了无人机视频中的多目标跟踪性能。


### [119] [SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models](https://arxiv.org/abs/2508.01731)
*Yuxiang Zhang,Wei Li,Mengmeng Zhang,Jiawei Han,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: SpectralX是一个参数高效微调框架，通过两阶段训练将现有遥感基础模型（RSFM）适应于多光谱/高光谱数据，提升领域泛化性能。

- Motivation: 现有RSFM主要针对光学图像，缺乏多光谱/高光谱数据的支持，而后者在地球观测中具有优势。
- Method: 提出两阶段训练：第一阶段通过掩码重建任务和Hyper Tokenizer提取空间与光谱属性；第二阶段引入Attribute-refined Adapter，结合高低级语义特征优化任务性能。
- Result: SpectralX能够适应新区域或季节的光谱图像，显著提升领域泛化能力。
- Conclusion: SpectralX为多光谱/高光谱数据的处理提供了一种高效且通用的解决方案。


### [120] [AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing](https://arxiv.org/abs/2508.01740)
*Zhaonan Wang,Manyi Li,Changhe Tu*

Main category: cs.CV

TL;DR: AG$^2$aussian提出了一种基于锚图结构的3D高斯表示框架，用于优化语义感知的3D高斯分布，提升场景理解和编辑任务的准确性。

- Motivation: 现有方法将语义特征附加到自由高斯分布上，导致分割噪声和高斯选择混乱，需要更高效的语义感知3D高斯表示。
- Method: 利用锚图结构组织语义特征并规范高斯基元，实现紧凑且实例感知的高斯分布，支持基于图的传播。
- Result: 在四种应用（交互式点击查询、开放词汇文本查询、对象移除编辑和物理模拟）中验证了方法的优势。
- Conclusion: AG$^2$aussian通过锚图结构显著提升了语义感知3D高斯的准确性和应用效果。


### [121] [Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models](https://arxiv.org/abs/2508.01741)
*Ruofan Wang,Xin Wang,Yang Yao,Xuan Tong,Xingjun Ma*

Main category: cs.CV

TL;DR: 论文提出了一种名为SEA的新型灰盒越狱攻击方法，通过模拟微调轨迹和针对性提示引导，成功攻击了微调后的视觉语言模型，揭示了开源基础模型的安全隐患。

- Motivation: 研究动机在于揭示开源视觉语言模型（VLMs）微调后可能保留的漏洞，这些漏洞可能被攻击者利用，导致微调后的模型仍然易受越狱攻击。
- Method: 方法包括Simulated Ensemble Attack (SEA)，结合Fine-tuning Trajectory Simulation (FTS)模拟视觉编码器的参数变化，以及Targeted Prompt Guidance (TPG)引导语言解码器生成对抗性输出。
- Result: 实验结果表明，SEA在Qwen2-VL家族（2B和7B）上实现了超过86.5%的转移攻击成功率，毒性率接近49.5%，显著提升了攻击的转移性。
- Conclusion: 结论强调了保护微调后的专有VLMs免受开源基础模型漏洞影响的紧迫性，呼吁开发全生命周期防御措施。


### [122] [Intention-Guided Cognitive Reasoning for Egocentric Long-Term Action Anticipation](https://arxiv.org/abs/2508.01742)
*Qiaohui Chu,Haoyu Zhang,Meng Liu,Yisen Feng,Haoxiang Shi,Liqiang Nie*

Main category: cs.CV

TL;DR: INSIGHT是一个两阶段框架，通过手-物体交互特征和动词-名词共现矩阵提升动作表示，并结合强化学习模拟认知推理，显著提升了长期动作预测性能。

- Motivation: 长期动作预测在人机交互和辅助技术中至关重要，但现有方法在细粒度视觉线索、语义依赖和认知推理方面存在不足。
- Method: 提出两阶段框架：1）提取手-物体交互特征并利用动词-名词共现矩阵增强动作表示；2）引入强化学习模块模拟认知推理（感知->推理->预测）。
- Result: 在Ego4D、EPIC-Kitchens-55和EGTEA Gaze+基准测试中达到最优性能。
- Conclusion: INSIGHT通过结合细粒度特征和认知推理，显著提升了长期动作预测的泛化能力和准确性。


### [123] [Improving Noise Efficiency in Privacy-preserving Dataset Distillation](https://arxiv.org/abs/2508.01749)
*Runkai Zheng,Vishnu Asutosh Dasu,Yinong Oliver Wang,Haohan Wang,Fernando De la Torre*

Main category: cs.CV

TL;DR: 论文提出了一种新框架，通过解耦采样与优化过程，并在信息子空间中进行匹配，提高了差分隐私数据集蒸馏的效率。

- Motivation: 现代机器学习模型依赖大量敏感数据，差分隐私数据生成虽能保护隐私但需要大量数据，数据集蒸馏（DD）效率高但现有方法存在采样-优化同步和噪声训练信号的问题。
- Method: 提出新框架，解耦采样与优化，通过信息子空间匹配减少差分隐私噪声的影响。
- Result: 在CIFAR-10上，每类50张图像时性能提升10.0%，蒸馏集大小仅为先前方法的五分之一时提升8.3%。
- Conclusion: 新框架显著提升了隐私保护数据集蒸馏的效率，具有重要应用潜力。


### [124] [Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring](https://arxiv.org/abs/2508.01752)
*Kumail Abbas,Zeeshan Afzal,Aqeel Raza,Taha Mansouri,Andrew W. Dowsey,Chaidate Inchaisri,Ali Alameer*

Main category: cs.CV

TL;DR: 开发了一种多摄像头实时追踪系统，用于监测奶牛活动，结合计算机视觉技术，显著提高了追踪准确性和效率。

- Motivation: 奶牛活动和行为与健康及福利相关，传统人工监测效率低且不一致，需要自动化解决方案。
- Method: 使用多摄像头系统，结合YOLO11-m模型和SAMURAI算法进行实例分割与追踪，通过运动感知卡尔曼滤波实现可靠追踪。
- Result: 系统在多目标追踪准确率（MOTA）和身份一致性（IDF1）上表现优异，显著优于Deep SORT Realtime。
- Conclusion: 该系统能实时追踪复杂环境中的奶牛，减少冗余检测，有望通过行为量化提升疾病早期预测能力。


### [125] [VPN: Visual Prompt Navigation](https://arxiv.org/abs/2508.01766)
*Shuo Feng,Zihan Wang,Yuchen Li,Rui Kong,Hengyi Cai,Shuaiqiang Wang,Gim Hee Lee,Piji Li,Shuqiang Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉提示的导航方法（VPN），通过用户提供的2D俯视图标记导航轨迹，避免了语言指导的模糊性，并构建了两个新数据集和基线网络VPNet。

- Motivation: 自然语言在指导导航时存在模糊性和冗长问题，影响了复杂环境中的导航效果。
- Method: 提出视觉提示导航（VPN），使用2D俯视图标记导航轨迹，构建数据集R2R-VP和R2R-CE-VP，并设计VPNet网络及数据增强策略。
- Result: 实验评估了视觉提示形式、俯视图格式和数据增强策略对导航性能的影响。
- Conclusion: VPN提供了一种直观且空间明确的导航指导方式，更适合非专业用户，并减少了语言模糊性。


### [126] [DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion](https://arxiv.org/abs/2508.01778)
*Zhigang Sun,Yiru Wang,Anqing Jiang,Shuo Wang,Yu Gao,Yuwen Heng,Shouyi Zhang,An He,Hao Jiang,Jinhao Chai,Zichong Gu,Wang Jijun,Shichen Tang,Lavdim Halilaj,Juergen Luettin,Hao Sun*

Main category: cs.CV

TL;DR: DiffSemanticFusion是一个多模态轨迹预测和规划的融合框架，结合了栅格和图的优势，通过地图扩散模块提升在线高精地图的稳定性和表达能力。

- Motivation: 自动驾驶需要精确的场景理解，但现有栅格表示缺乏几何精度，而图表示在不精确地图下不稳定。
- Method: 提出DiffSemanticFusion框架，在语义栅格融合的BEV空间中进行推理，并通过地图扩散模块增强地图表示。
- Result: 在nuScenes和NAVSIM基准测试中表现优异，轨迹预测任务提升5.1%，端到端自动驾驶任务提升15%。
- Conclusion: DiffSemanticFusion有效结合了栅格和图的优势，显著提升了自动驾驶任务的性能。


### [127] [Skip priors and add graph-based anatomical information, for point-based Couinaud segmentation](https://arxiv.org/abs/2508.01785)
*Xiaotong Zhang,Alexander Broersen,Gonnie CM van Erp,Silvia L. Pintea,Jouke Dijkstra*

Main category: cs.CV

TL;DR: 提出了一种基于点的方法用于Couinaud分割，无需显式提供肝脏血管结构先验知识，通过图推理模块学习点邻域间的亲和力。

- Motivation: 减少术前规划中对肝脏血管结构先验知识的依赖，提高分割效率。
- Method: 使用3D点表示CT图像，添加图推理模块学习点邻域间的亲和力，隐式学习肝脏血管结构。
- Result: 在MSD和LiTS数据集上，Dice系数和平均表面距离得分优于四种前沿点基方法。
- Conclusion: 该方法无需显式先验知识，通过图推理模块隐式学习解剖结构，性能优越。


### [128] [CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase](https://arxiv.org/abs/2508.01791)
*Fatimah Mohamed Emad Elden*

Main category: cs.CV

TL;DR: 本文提出了一种数据驱动的方法，通过特征工程、预处理流程和优化的CSLRConformer架构，解决了连续手语识别中的跨签名者泛化问题，取得了竞争性结果。

- Motivation: 解决连续手语识别（CSLR）中的技术挑战，特别是跨签名者的泛化能力问题。
- Method: 采用数据驱动方法，包括特征选择、预处理（DBSCAN异常值过滤和空间归一化）和CSLRConformer架构（基于CNN-Transformer的混合设计）。
- Result: 在开发集和测试集上的词错误率（WER）分别为5.60%和12.01%，在竞赛中排名第三。
- Conclusion: 验证了跨领域架构适应的有效性，Conformer模型在手语识别中表现优异，达到了新的先进水平。


### [129] [SoccerTrack v2: A Full-Pitch Multi-View Soccer Dataset for Game State Reconstruction](https://arxiv.org/abs/2508.01802)
*Atom Scott,Ikuma Uchida,Kento Kuroda,Yufi Kim,Keisuke Fujii*

Main category: cs.CV

TL;DR: SoccerTrack v2是一个新的公开数据集，用于推动足球分析中的多目标跟踪（MOT）、比赛状态重建（GSR）和球动作识别（BAS）。

- Motivation: 现有的数据集通常使用广播视角或有限场景，而SoccerTrack v2提供了10场完整的全景4K录像，覆盖大学级别比赛，确保球员完全可见，以推动计算机视觉和足球分析的研究。
- Method: 数据集通过BePro相机拍摄，包含GSR标签（2D场地坐标、球衣ID、角色、队伍）和BAS标签（12种动作类别），并详细描述了数据集的构建、收集流程和标注过程。
- Result: SoccerTrack v2为研究提供了新的基准，支持战术分析和自动化工具的实际应用。
- Conclusion: SoccerTrack v2旨在推动计算机视觉和足球分析领域的研究，为战术分析和自动化工具提供实用支持。


### [130] [Diffusion-based 3D Hand Motion Recovery with Intuitive Physics](https://arxiv.org/abs/2508.01835)
*Yufei Zhang,Zijun Cui,Jeffrey O. Kephart,Qiang Ji*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型和物理增强的3D手部运动恢复框架，显著提升了视频中手部运动的准确性和时序一致性。

- Motivation: 尽管单目图像中的3D手部重建已有显著进展，但在视频中生成准确且时序一致的运动估计仍具挑战性，尤其是在手与物体交互时。
- Method: 通过扩散模型和物理增强的运动细化模型，利用运动捕捉数据训练模型，并结合手与物体交互中的物理知识（如关键运动状态和约束）。
- Result: 实验表明，该方法显著提升了多种帧级重建方法的性能，在现有基准测试中达到SOTA水平。
- Conclusion: 该框架通过结合扩散模型和物理知识，有效提升了3D手部运动恢复的准确性和时序一致性。


### [131] [A Simple Algebraic Solution for Estimating the Pose of a Camera from Planar Point Features](https://arxiv.org/abs/2508.01836)
*Tarek Bouazza,Tarek Hamel,Claude Samson*

Main category: cs.CV

TL;DR: 提出了一种基于代数方法的相机位姿估计方法，适用于平面目标，通过参考点和测量值计算相机位姿。

- Motivation: 解决从平面目标估计相机位姿的问题，尤其是在存在测量噪声的情况下提高鲁棒性。
- Method: 采用分层结构：先确定目标平面的法向量，再计算相机位置向量、距离和完整方向，并引入平均方法优化法向量估计。
- Result: 通过大量实验验证了方法的准确性和鲁棒性。
- Conclusion: 该方法在噪声环境下仍能有效估计相机位姿，具有实用价值。


### [132] [OmniEvent: Unified Event Representation Learning](https://arxiv.org/abs/2508.01842)
*Weiqi Yan,Chenlu Lin,Youbiao Wang,Zhipeng Cai,Xiuhong Lin,Yangyang Shi,Weiquan Liu,Yu Zang*

Main category: cs.CV

TL;DR: OmniEvent是一个统一的事件表示学习框架，通过解耦-增强-融合范式解决事件数据的不均匀性问题，无需任务特定设计。

- Motivation: 事件相机数据的不均匀性和任务特定设计限制了其通用性，需要一个统一框架。
- Method: 采用解耦-增强-融合范式，独立处理时空特征，利用空间填充曲线提高效率，并通过注意力融合特征。
- Result: 在3个代表性任务和10个数据集上，OmniEvent比任务特定方法性能提升高达68.2%。
- Conclusion: OmniEvent证明了统一框架的可行性，为事件数据处理提供了高效通用的解决方案。


### [133] [Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems](https://arxiv.org/abs/2508.01845)
*Zhongliang Guo,Yifei Qian,Yanli Li,Weiye Li,Chun Tong Lei,Shuai Zhao,Lei Fang,Ognjen Arandjelović,Chun Pong Lau*

Main category: cs.CV

TL;DR: 该论文综述了对抗性攻击在计算机视觉系统中的双重作用，分析了像素空间、物理可实现和潜在空间攻击方法，并探讨了其在防御和漏洞评估中的应用。

- Motivation: 研究对抗性攻击的动机在于揭示神经网络鲁棒性和安全性的根本假设，并探索其作为安全威胁和防御工具的双重性质。
- Method: 论文系统分析了对抗性攻击方法，包括梯度基方法（如FGSM和PGD）、物理可实现攻击（如对抗性补丁和3D纹理）以及潜在空间攻击。
- Result: 研究发现对抗性攻击不仅威胁系统安全，还可用于漏洞评估和防御生成模型，同时指出了神经风格转移保护和计算效率的研究空白。
- Conclusion: 论文提出了全面的分类法和未来研究方向，旨在推动对抗性漏洞的理解，并促进更鲁棒的计算机视觉系统发展。


### [134] [Context Guided Transformer Entropy Modeling for Video Compression](https://arxiv.org/abs/2508.01852)
*Junlong Tong,Wei Zhang,Yaohui Jin,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的上下文引导熵模型（CGT），通过重新采样时间上下文和加权空间上下文来减少视频冗余，同时降低计算成本。

- Motivation: 现有方法在利用时间上下文时增加了模型复杂度和计算成本，而空间上下文模型缺乏对依赖顺序的显式建模，限制了解码时的上下文可用性。
- Method: CGT模型结合时间上下文重采样器和依赖加权的空间上下文分配器，前者通过Transformer编码器提取关键时间信息，后者通过师生网络显式建模空间依赖顺序。
- Result: 实验表明，CGT模型将熵建模时间减少约65%，BD-Rate降低11%。
- Conclusion: CGT模型在减少计算开销的同时，显著提升了熵建模效率。


### [135] [Distinguishing Target and Non-Target Fixations with EEG and Eye Tracking in Realistic Visual Scenes](https://arxiv.org/abs/2508.01853)
*Mansi Sharma,Camilo Andrés Martínez Martínez,Benedikt Emanuel Wirth,Antonio Krüger,Philipp Müller*

Main category: cs.CV

TL;DR: 研究通过结合眼动和脑电图数据，在自由视觉搜索中区分目标与非目标注视点，显著提升了现实场景中的分类准确性。

- Motivation: 现有研究在抽象场景中分类目标与非目标注视点，但忽视了场景上下文对视觉搜索的影响，限制了其现实适用性。
- Method: 采用36名参与者的用户研究，使用140个现实场景（如桌面图标和工具搜索），结合眼动和脑电图特征进行分类。
- Result: 新方法在跨用户评估中达到83.6%的准确率，显著优于基于扫视相关电位的现有方法（56.9%）。
- Conclusion: 研究填补了现实场景中目标与非目标注视点分类的空白，为辅助系统提供了更可靠的基础。


### [136] [DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization](https://arxiv.org/abs/2508.01873)
*Siran Peng,Haoyuan Zhang,Li Gao,Tianshuo Zhang,Bao Li,Zhen Lei*

Main category: cs.CV

TL;DR: DiffusionFF框架通过扩散模型生成DSSIM图，结合预训练检测器特征，提升深度伪造检测的准确性和可解释性。

- Motivation: 深度伪造技术快速发展，需要更鲁棒和准确的检测算法，尤其是能够精确定位伪造痕迹的方法，以提高模型可解释性和用户信任。
- Method: 提出DiffusionFF框架，利用去噪扩散模型生成高质量的DSSIM图，捕捉细微伪造痕迹，并与预训练检测器的高层语义特征融合。
- Result: 在跨数据集和数据集内基准测试中，DiffusionFF表现出卓越的检测性能和精细的伪造痕迹定位能力。
- Conclusion: DiffusionFF在深度伪造检测和定位方面具有显著优势，验证了其整体有效性。


### [137] [StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding](https://arxiv.org/abs/2508.01875)
*Haolin Yang,Feilong Tang,Linxiao Zhao,Xiang An,Ming Hu,Huifa Li,Xinlin Zhuang,Boqian Wang,Yifan Lu,Xiaofeng Zhang,Abdalla Swikir,Junjun He,Zongyuan Ge,Imran Razzak*

Main category: cs.CV

TL;DR: 提出StreamAgent，通过预测未来任务相关信息的时间和空间区域，实现实时视频流中的主动决策和响应。

- Motivation: 现有方法缺乏任务驱动的规划和未来预测，限制了实时响应和主动决策能力。
- Method: 整合问题语义和历史观察，通过预测关键事件的时间进展和对齐当前观察与未来证据，调整感知动作。设计流式KV缓存机制，高效检索语义。
- Result: 在实时和长视频理解任务中，方法在响应准确性和实时效率上优于现有方法。
- Conclusion: StreamAgent在实时视频流场景中具有实用价值。


### [138] [Medical Image De-Identification Resources: Synthetic DICOM Data and Tools for Validation](https://arxiv.org/abs/2508.01889)
*Michael W. Rutherford,Tracy Nolan,Linmin Pei,Ulrike Wagner,Qinyan Pan,Phillip Farmer,Kirk Smith,Benjamin Kopchick,Laura Opsahl-Ong,Granger Sutton,David Clunie,Keyvan Farahani,Fred Prior*

Main category: cs.CV

TL;DR: 论文提出了一种评估医学图像去标识化效果的开源框架和数据集（MIDI），以解决现有工具缺乏客观评估的问题。

- Motivation: 医学影像研究需要大规模数据共享，但患者隐私保护是关键挑战。现有去标识化工具缺乏客观评估方法，限制了可重复性和监管信心。
- Method: 开发了包含合成PHI/PII的DICOM数据集（MIDI），并提供了评估工具（Python脚本、答案键和映射文件），支持自动化比较。
- Result: MIDI数据集包含538名受试者、53,581个DICOM图像实例，涵盖多种设备和癌症类型。评估框架符合HIPAA和DICOM标准。
- Conclusion: 该框架为医学图像去标识化提供了客观、标准化的评估方法，有助于更安全、一致的数据共享。


### [139] [EgoTrigger: Toward Audio-Driven Image Capture for Human Memory Enhancement in All-Day Energy-Efficient Smart Glasses](https://arxiv.org/abs/2508.01915)
*Akshay Paruchuri,Sinan Hersek,Lavisha Aggarwal,Qiao Yang,Xin Liu,Achin Kulshrestha,Andrea Colaco,Henry Fuchs,Ishan Chatterjee*

Main category: cs.CV

TL;DR: EgoTrigger利用音频线索智能激活摄像头，显著降低能耗，同时保持人类记忆增强功能。

- Motivation: 解决全天候智能眼镜在连续传感和多模态AI代理集成中的能源效率问题。
- Method: 使用轻量级音频模型（YAMNet）和自定义分类头，通过手-物交互音频线索触发图像捕捉。
- Result: EgoTrigger平均减少54%的帧数，显著节省能耗，同时在记忆任务中表现相当。
- Conclusion: 上下文感知触发策略为全天候智能眼镜的能源高效和功能实现提供了可行方向。


### [140] [InspectVLM: Unified in Theory, Unreliable in Practice](https://arxiv.org/abs/2508.01921)
*Conor Wallace,Isaac Corley,Jonathan Lwowski*

Main category: cs.CV

TL;DR: InspectVLM，一种基于Florence-2的统一视觉语言模型，在工业检测任务中表现不佳，未能超越传统ResNet模型，尤其是在低提示多样性和细粒度目标检测方面。

- Motivation: 通过统一的视觉语言模型简化工业检测中的多任务管理，减少复杂性和维护成本。
- Method: 使用InspectVLM（基于Florence-2）和InspectMM数据集进行评估。
- Result: InspectVLM在分类和关键点任务中表现尚可，但在核心检测指标上不如ResNet模型，且对提示变化敏感，输出不稳定。
- Conclusion: 当前统一视觉语言模型在视觉基础和鲁棒性上不足，不适合高精度工业检测。


### [141] [IAUNet: Instance-Aware U-Net](https://arxiv.org/abs/2508.01928)
*Yaroslav Prytula,Illia Tsiporenko,Ali Zeynalli,Dmytro Fishman*

Main category: cs.CV

TL;DR: IAUNet是一种基于查询的U-Net架构，结合轻量级卷积像素解码器和Transformer解码器，显著提升了生物医学图像中细胞实例分割的性能。

- Motivation: 解决生物医学图像中细胞重叠和尺寸变化带来的实例分割挑战，探索U-Net在查询式方法中的潜力。
- Method: 提出IAUNet，结合U-Net架构、轻量级卷积像素解码器和多尺度Transformer解码器，优化参数效率和特征细化。
- Result: 在多个公共数据集和自建数据集上，IAUNet优于现有全卷积、Transformer和查询式模型，成为细胞实例分割的新基准。
- Conclusion: IAUNet为生物医学实例分割任务提供了高效且性能优越的解决方案，并发布了新的数据集以推动研究。


### [142] [Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense](https://arxiv.org/abs/2508.01932)
*Kyle Stein,Andrew A. Mahyari,Guillermo Francia III,Eman El-Sheikh*

Main category: cs.CV

TL;DR: DBOM框架通过解耦触发器和对象表示，利用视觉语言模型检测和防御多触发器后门攻击，提升DNN训练安全性。

- Motivation: 传统后门攻击检测方法难以应对多触发器和未见配置的攻击，需开发新方法以主动识别和防御此类威胁。
- Method: DBOM利用视觉语言模型解耦触发器和对象表示，通过视觉提示库和损失函数分离特征，实现零样本泛化。
- Result: 在CIFAR-10和GTSRB数据集上，DBOM能有效检测中毒图像，显著提升训练管道的安全性。
- Conclusion: DBOM为后门攻击提供了一种主动防御框架，能够处理未见配置，增强模型安全性。


### [143] [CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes](https://arxiv.org/abs/2508.01936)
*Yaxuan Li,Yewei Huang,Bijay Gaudel,Hamidreza Jafarnejadsani,Brendan Englot*

Main category: cs.CV

TL;DR: 提出了一种新颖的多高度相机姿态估计系统，通过整合跨视图变换器、深度特征和运动结构，解决了稀疏图像输入下不同高度的鲁棒定位问题。

- Motivation: 解决在稀疏图像输入下，不同高度相机姿态估计的鲁棒性和准确性挑战。
- Method: 整合跨视图变换器、深度特征和运动结构到一个统一框架中。
- Result: 在新建的两个多高度数据集上验证，系统在准确性和鲁棒性上优于现有方法。
- Conclusion: 该系统适用于现实机器人应用，如空中导航、搜索救援和自动化检查。


### [144] [Self-Supervised YOLO: Leveraging Contrastive Learning for Label-Efficient Object Detection](https://arxiv.org/abs/2508.01966)
*Manikanta Kotthapalli,Reshma Bhatia,Nainsi Jain*

Main category: cs.CV

TL;DR: 论文研究了对比自监督学习（SSL）在YOLO系列单阶段目标检测器中的应用，通过SimCLR框架在无标签图像上预训练，减少对大规模标注数据的依赖。实验表明，SSL预训练能显著提升性能，尤其在低标注数据情况下。

- Motivation: 减少单阶段目标检测器（如YOLO）对大规模标注数据的依赖，探索自监督学习在目标检测中的潜力。
- Method: 使用SimCLR框架预训练YOLOv5和YOLOv8的卷积骨干网络，通过全局池化和投影头优化对比损失，并在无标签的COCO数据集（120k图像）上进行训练。预训练后的骨干网络在有限标注数据的任务上进行微调。
- Result: SSL预训练显著提升了性能，例如SimCLR预训练的YOLOv8在mAP@50:95上达到0.7663，优于监督学习版本。
- Conclusion: 对比自监督学习为单阶段检测器提供了高效的预训练方法，展示了无标签数据在目标检测中的潜力。


### [145] [On-the-Fly Object-aware Representative Point Selection in Point Cloud](https://arxiv.org/abs/2508.01980)
*Xiaoyu Zhang,Ziwei Wang,Hai Dong,Zhifeng Bao,Jiajun Liu*

Main category: cs.CV

TL;DR: 提出了一种点云降采样框架，通过保留关键物体信息并过滤无关背景点，解决了自动驾驶车辆数据存储和处理成本问题。

- Motivation: 自动驾驶车辆生成的点云数据量大，存储、带宽和处理成本高，需要高效降采样方法。
- Method: 分两步：1) 无监督密度峰值分类器和有监督朴素贝叶斯分类器检测物体存在；2) 采样预算分配策略选择物体相关点。
- Result: 在KITTI和nuScenes数据集上表现优于现有方法，效率和效果均优。
- Conclusion: 该方法模型无关，可无缝集成到下游模型，是自动驾驶点云降采样的高效解决方案。


### [146] [IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A](https://arxiv.org/abs/2508.01984)
*Chen Li,Chinthani Sugandhika,Yeo Keat Ee,Eric Peh,Hao Zhang,Hong Yang,Deepu Rajan,Basura Fernando*

Main category: cs.CV

TL;DR: 提出了一种隐式程序引导的运动推理框架（IMoRe），无需手动设计模块，统一处理多种查询类型，并在性能上达到最优。

- Motivation: 现有方法依赖显式程序执行和手动定义的功能模块，限制了可扩展性和适应性。
- Method: 通过结构化程序功能直接指导推理步骤，并引入程序引导的阅读机制动态选择多级运动表示。
- Result: 在Babel-QA上达到最优性能，并在HuMMan数据集上展示出良好的适应性。
- Conclusion: IMoRe框架在运动推理任务中表现出高效性和适应性。


### [147] [Deeply Dual Supervised learning for melanoma recognition](https://arxiv.org/abs/2508.01994)
*Rujosh Polma,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 本文提出了一种新型的深度双重监督学习框架，通过结合局部和全局特征提取，显著提高了黑色素瘤识别的准确性。

- Motivation: 尽管深度学习在皮肤科的应用不断增长，但现有模型在识别区分黑色素瘤和良性病变的细微视觉线索方面仍面临挑战。
- Method: 采用双通路结构，结合局部细粒度特征和全局上下文信息，并引入双重注意力机制和多尺度特征聚合策略。
- Result: 在基准数据集上的实验表明，该框架在黑色素瘤检测中显著优于现有方法，准确率更高且假阳性率更低。
- Conclusion: 该研究为未来自动化皮肤癌识别奠定了基础，并展示了双重监督学习在医学图像分析中的有效性。


### [148] [Fast and Memory-efficient Non-line-of-sight Imaging with Quasi-Fresnel Transform](https://arxiv.org/abs/2508.02003)
*Yijun Wei,Jianyu Wang,Leping Xiao,Zuoqiang Shi,Xing Fu,Lingyun Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于二维函数和准菲涅尔变换的非视距成像新方法，显著降低了计算复杂度和内存需求。

- Motivation: 现有方法通常对测量数据和隐藏场景进行三维建模，忽略了隐藏物体的二维特性，导致高计算成本和内存消耗，限制了实际应用。
- Method: 使用二维函数表示隐藏场景，并采用准菲涅尔变换建立测量数据与隐藏场景之间的直接反演公式。
- Result: 算法显著降低了运行时间和内存需求，同时保持成像质量，适用于轻量级设备。
- Conclusion: 该方法有望实现实时高分辨率非视距成像，并扩展其应用范围。


### [149] [Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention](https://arxiv.org/abs/2508.02004)
*Kyungmin Jo,Jooyeol Yun,Jaegul Choo*

Main category: cs.CV

TL;DR: 论文提出了一种新的自注意力修改方法（Stratified Attention），解决了现有图像提示方法在分类器自由引导中的冲突问题，并平衡了生成图像的逼真度与图像提示的对齐。

- Motivation: 现有文本到图像扩散模型难以捕捉复杂细节（如纹理），导致用户意图无法充分体现。虽然已有方法通过修改自注意力机制引入图像提示，但仍存在分类器自由引导中的信号冲突及逼真度与对齐的权衡问题。
- Method: 提出冲突自由引导（conflict-free guidance），仅将图像提示作为期望条件；并设计分层注意力（Stratified Attention），联合使用图像提示和生成图像的键值，而非二选一。
- Result: 在三个图像生成任务中，新方法在忠实反映图像提示方面优于现有模型。
- Conclusion: 通过冲突自由引导和分层注意力，新方法有效解决了现有问题，显著提升了生成图像的质量和对齐性。


### [150] [Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in Autonomous Driving](https://arxiv.org/abs/2508.02028)
*Tianyuan Zhang,Ting Jin,Lu Wang,Jiangfan Liu,Siyuan Liang,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: 论文提出了Bench2ADVLM框架，用于在闭环设置中评估基于视觉语言模型（VLM）的自动驾驶系统（ADVLM），填补了现有评估方法的不足。

- Motivation: 当前对ADVLM的性能评估主要局限于开环设置，忽略了更真实和具有反馈的闭环环境，无法全面评估其交互行为和安全性。
- Method: 通过双系统适应架构将ADVLM适配到仿真环境，并设计物理控制抽象层实现仿真与现实的闭环测试。引入自反思场景生成模块以发现潜在故障模式。
- Result: 实验表明，现有ADVLM在闭环条件下的性能仍有局限，验证了Bench2ADVLM的诊断能力。
- Conclusion: Bench2ADVLM为ADVLM提供了一个统一的闭环评估框架，揭示了其在真实场景中的不足，推动了进一步研究。


### [151] [Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure](https://arxiv.org/abs/2508.02034)
*Ziling Wang,Shuya Yang,Jialin Lu,Ka-Ho Chow*

Main category: cs.CV

TL;DR: Protego是一种用户中心的隐私保护方法，通过动态变形3D面具保护面部图像，防止基于检索的隐私侵犯。

- Motivation: 面部识别技术的大规模应用引发了严重的隐私问题，如未经同意的身份推断和数字足迹暴露。
- Method: Protego将用户的3D面部特征封装为姿态不变的2D表示，并动态变形为自然外观的3D面具，应用于在线共享前的图像。
- Result: 实验表明，Protego显著降低了黑盒面部识别模型的检索准确率，性能至少是现有方法的2倍。
- Conclusion: Protego有效对抗面部识别技术的滥用，为大规模监视和非自愿身份追踪提供了解决方案。


### [152] [Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction](https://arxiv.org/abs/2508.02043)
*Hui Xie,Haiqin Hu,Lijuan Ding,Qing Li,Yue Sun,Tao Tan*

Main category: cs.CV

TL;DR: ADDiff-Dose是一种基于条件扩散模型的放疗剂量预测方法，显著提高了预测精度和临床适用性。

- Motivation: 解决放疗规划中依赖专家经验、现有深度学习方法泛化性和准确性不足的问题。
- Method: 采用LightweightVAE3D压缩CT数据，结合多模态输入和渐进噪声去噪框架，使用多注意力机制和复合损失函数。
- Result: 在多个数据集上表现优异，MAE为0.101-0.154，DICE系数0.927，单例规划时间降至22秒。
- Conclusion: ADDiff-Dose为放疗剂量预测提供了高效、通用的解决方案，有望显著提升临床效率。


### [153] [Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark Revealing Vision-Language Model Limitations](https://arxiv.org/abs/2508.02047)
*Sparsh Garg,Abhishek Aich*

Main category: cs.CV

TL;DR: 论文提出了Mapillary Vistas Validation for Traffic Signs (MVV)数据集，用于细粒度交通标志标注，并评估了DINOv2模型在细粒度视觉理解中的优越性。

- Motivation: 现有数据集（如Mapillary）的标注粒度较粗，无法区分语义重要的交通标志类型，影响自动驾驶决策的准确性。
- Method: 构建MVV数据集，分解复合交通标志为细粒度类别，提供像素级实例掩码，并对比评估DINOv2与多种视觉语言模型（VLM）。
- Result: DINOv2在交通标志识别及其他类别（如车辆和行人）上均优于VLM基线，揭示了当前VLM在细粒度视觉理解中的局限性。
- Conclusion: MVV数据集和DINOv2为自动驾驶场景中的密集语义匹配提供了可靠基准，推动了更可解释和可扩展的感知系统发展。


### [154] [HCF: Hierarchical Cascade Framework for Distributed Multi-Stage Image Compression](https://arxiv.org/abs/2508.02051)
*Junhao Cai,Taegun An,Chengjun Jin,Sung Il Choi,JuHyun Park,Changhee Joo*

Main category: cs.CV

TL;DR: 提出了一种分层级联框架（HCF），用于分布式多阶段图像压缩，通过潜在空间转换提升性能，显著优于现有方法。

- Motivation: 分布式多阶段图像压缩面临计算资源利用不足、重复操作导致质量损失等问题，需要更高效的解决方案。
- Method: 开发了HCF框架，利用潜在空间转换和策略驱动的量化控制优化率失真性能。
- Result: 在多个数据集上，HCF显著提升了性能（如PSNR增益达0.6dB），并大幅节省计算资源（如减少97.8% FLOPs）。
- Conclusion: HCF框架在多阶段图像压缩中表现出色，兼具高性能和高效计算，优于现有方法。


### [155] [StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive Diffusion](https://arxiv.org/abs/2508.02056)
*Haoxin Yang,Weihong Chen,Xuemiao Xu,Cheng Xu,Peng Xiao,Cuifeng Sun,Shaoyu Huang,Shengfeng He*

Main category: cs.CV

TL;DR: StarPose提出了一种自回归扩散框架，通过结合历史3D姿态预测和时空物理引导，显著提升了单目3D人体姿态估计的准确性和时间一致性。

- Motivation: 传统方法在处理单目3D人体姿态估计时存在深度模糊和遮挡问题，而现有扩散方法缺乏时空相关性建模，导致时间一致性差。
- Method: StarPose将2D到3D姿态映射建模为自回归扩散过程，通过历史姿态集成模块（HPIM）和时空物理引导机制（STPG）优化预测。
- Result: 在基准数据集上，StarPose优于现有方法，实现了更高的准确性和时间一致性。
- Conclusion: StarPose通过自回归扩散框架和时空引导机制，显著提升了3D姿态估计的性能。


### [156] [YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges](https://arxiv.org/abs/2508.02067)
*Manikanta Kotthapalli,Deepika Ravipati,Reshma Bhatia*

Main category: cs.CV

TL;DR: 本文综述了YOLO系列模型的演进，从YOLOv1到YOLOv9，强调其在速度、精度和部署效率上的平衡改进，并探讨了其在实例分割、姿态估计等扩展任务中的应用。

- Motivation: 回顾YOLO系列模型的进展，分析其架构创新和性能提升，为实时视觉应用提供参考。
- Method: 通过系统回顾YOLO各版本的架构和算法改进，对比性能指标，并探讨其扩展任务。
- Result: YOLO系列在速度和精度上持续优化，支持多种扩展任务，广泛应用于医疗影像和工业自动化等领域。
- Conclusion: YOLO系列模型在计算机视觉领域具有广泛影响，未来研究方向包括进一步优化和扩展应用场景。


### [157] [S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained Evaluation Framework](https://arxiv.org/abs/2508.02082)
*Yingshu Li,Yunyi Liu,Zhanyu Wang,Xinyu Liang,Lingqiao Liu,Lei Wang,Luping Zhou*

Main category: cs.CV

TL;DR: 提出了一种结构化放射学报告生成（S-RRG）的新方法，包括数据集构建、模型训练和新的评估框架，以解决传统报告的冗余和不一致问题。

- Motivation: 传统自由文本放射学报告存在冗余和语言不一致问题，难以提取关键临床细节，而现有结构化方法依赖预定义标签或模板，表达受限且可能遗漏重要信息。
- Method: 构建了一个结构化的胸部X光数据集（MIMIC-STRUC），训练了一个基于LLM的模型生成标准化报告，并提出了新的评估指标（S-Score）。
- Result: 该方法生成了高质量的结构化报告，并通过S-Score评估，显示出与人类评估更强的对齐性。
- Conclusion: 结构化报告和定制化评估指标显著提升了放射学报告的质量和临床相关性。


### [158] [VLM4D: Towards Spatiotemporal Awareness in Vision Language Models](https://arxiv.org/abs/2508.02095)
*Shijie Zhou,Alexander Vilesov,Xuehai He,Ziyu Wan,Shuwang Zhang,Aditya Nagachandra,Di Chang,Dongdong Chen,Xin Eric Wang,Achuta Kadambi*

Main category: cs.CV

TL;DR: VLM4D是一个专门评估视觉语言模型（VLMs）时空推理能力的基准测试，揭示了现有模型在动态时空交互理解上的不足。

- Motivation: 当前VLMs在动态时空交互理解上存在显著缺陷，而人类能轻松处理物体运动、旋转和视角变化。
- Method: 设计了包含真实和合成视频的VLM4D基准测试，通过精心设计的问题对评估VLMs的时空推理能力。
- Result: 评估显示VLMs在整合多视觉线索和保持时间连贯性上表现不佳，与人类基线存在显著差距。
- Conclusion: 通过4D特征场重建和针对性监督微调等方法，有望提升VLMs的时空理解能力，推动动态环境下的视觉智能发展。


### [159] [Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis](https://arxiv.org/abs/2508.02106)
*Kaiyang Ji,Ye Shi,Zichen Jin,Kangyi Chen,Lan Xu,Yuexin Ma,Jingyi Yu,Jingya Wang*

Main category: cs.CV

TL;DR: Human-X框架通过实时预测动作和反应，结合强化学习的运动跟踪策略，提升了人机交互的物理合理性和安全性。

- Motivation: 解决现有方法在实时响应、物理可行性和安全性之间的冲突，提升人机交互的沉浸感和物理合理性。
- Method: 采用自回归反应扩散规划器实时预测动作和反应，并结合强化学习的运动跟踪策略动态适应交互伙伴的动作。
- Result: 在Inter-X和InterHuman数据集上显著提升了运动质量、交互连续性和物理合理性，优于现有方法。
- Conclusion: Human-X框架在虚拟现实和机器人交互中展示了潜力，推动了人机协作的进步。


### [160] [AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation](https://arxiv.org/abs/2508.02107)
*Zhiwen Li,Zhongjie Duan,Die Chen,Cen Chen,Daoyuan Chen,Yaliang Li,Yingda Chen*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过语义驱动的LoRA检索和动态聚合解决分布式开源LoRA模块的三大挑战：稀疏元数据标注、零样本适应需求和多LoRA融合策略。

- Motivation: 尽管大规模模型在图像生成方面取得进展，但参数微调的不易性限制了实际部署。现有LoRA模块面临元数据稀疏、零样本适应和多模块融合效率低的问题。
- Method: 框架包含两部分：(1)基于权重编码的LoRA检索器，建立LoRA参数矩阵与文本提示的共享语义空间；(2)细粒度门控融合机制，动态计算融合权重。
- Result: 显著提升了图像生成性能，实现了基础模型的可扩展和数据高效增强。
- Conclusion: 该工作为社区开发的LoRA模块与实际部署需求之间建立了桥梁，支持标准化适配器集成的协作模型演进。


### [161] [DeflareMamba: Hierarchical Vision Mamba for Contextually Consistent Lens Flare Removal](https://arxiv.org/abs/2508.02113)
*Yihang Huang,Yuanfei Huang,Junhui Lin,Hua Huang*

Main category: cs.CV

TL;DR: 论文提出DeflareMamba，利用状态空间模型高效序列建模能力，解决镜头光晕去除中的信息混淆问题，同时保持局部-全局依赖关系。

- Motivation: 镜头光晕去除因光与相机镜头的复杂光学交互而具挑战性，现有方法难以保持上下文一致性。
- Method: 设计分层框架，通过多步长采样模式建立长距离像素关联，并采用局部增强状态空间模型保留细节。
- Result: 实验表明，该方法能有效去除多种光晕伪影，同时保持非光晕区域的自然外观。
- Conclusion: DeflareMamba首次将状态空间模型引入光晕去除任务，并在下游应用中展示了潜力。


### [162] [Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps](https://arxiv.org/abs/2508.02127)
*Mingjie Liu,Hanqing Liu,Chuang Zhu*

Main category: cs.CV

TL;DR: 提出NRE-Net，一种多模态检测框架，结合RGB图像、事件流和单目预测的法线图，通过自适应融合模块提升恶劣光照条件下的物体检测精度。

- Motivation: 恶劣光照条件下的物体检测对自动驾驶等应用至关重要，但现有RGB或事件数据单独使用效果不佳，且缺乏无需额外传感器的解决方案。
- Method: 提出NRE-Net框架，融合RGB图像、事件流和单目预测的法线图，通过ADFM和EAFM模块优化多模态特征融合。
- Result: 在DSEC-Det-sub和PKU-DAVIS-SOD数据集上显著优于现有方法，mAP50分别提升7.9%和6.1%。
- Conclusion: NRE-Net通过多模态融合有效抑制误检，提升检测精度，为恶劣光照条件下的物体检测提供了新思路。


### [163] [VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling](https://arxiv.org/abs/2508.02129)
*Yuru Xiao,Zihan Lin,Chao Lu,Deming Zhai,Kui Jiang,Wenbo Zhao,Wei Zhang,Junjun Jiang,Huanran Wang,Xianming Liu*

Main category: cs.CV

TL;DR: 提出了一种基于视频扩散增强的4D高斯泼溅框架，用于动态城市场景建模，显著提升了快速移动物体的建模效果。

- Motivation: 当前方法依赖预校准物体轨迹或难以从欠采样捕获中准确建模快速移动物体，存在时间不连续性问题。
- Method: 结合视频扩散模型提取时间一致先验，引入联合时间戳优化策略和不确定性蒸馏方法。
- Result: 实验表明，新方法显著提升动态建模效果，PSNR增益约2 dB。
- Conclusion: 该方法有效解决了动态场景建模中的时间不连续性问题，提升了快速移动物体的建模精度。


### [164] [A Neural Quality Metric for BRDF Models](https://arxiv.org/abs/2508.02131)
*Behnaz Kavoosighafi,Rafal K. Mantiuk,Saghi Hajisharif,Ehsan Miandji,Jonas Unger*

Main category: cs.CV

TL;DR: 提出了一种基于感知的神经网络质量度量方法，用于直接评估BRDF模型的质量，无需渲染。

- Motivation: 传统BRDF空间度量方法无法捕捉渲染图像中的感知差异，需要一种更有效的评估方法。
- Method: 使用多层感知器（MLP）构建度量模型，训练数据包括实测BRDF和合成数据，标签为感知验证的图像空间度量。
- Result: 该神经网络度量与人类判断的相关性显著高于现有BRDF空间度量。
- Conclusion: 该方法为BRDF模型评估提供了基于感知的替代方案，尽管在BRDF拟合中的性能有限。


### [165] [Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference](https://arxiv.org/abs/2508.02134)
*Kuo Wang,Quanlong Zheng,Junlin Xie,Yanhao Zhang,Jinguo Luo,Haonan Lu,Liang Lin,Fan Zhou,Guanbin Li*

Main category: cs.CV

TL;DR: Free-MoRef是一种无需训练的方法，通过多参考注意力机制高效处理长视频输入，提升Video-MLLMs的性能。

- Motivation: 现有Video-MLLMs因上下文长度限制在长视频场景表现不佳，需高效解决方案。
- Method: 提出Free-MoRef，重构视觉标记为多参考序列，并行处理并融合关键标记。
- Result: 在多个基准测试中，Free-MoRef支持2至8倍长输入帧，性能显著提升。
- Conclusion: Free-MoRef高效且有效，优于专用长视频模型。


### [166] [AID4AD: Aerial Image Data for Automated Driving Perception](https://arxiv.org/abs/2508.02140)
*Daniel Lengerer,Mathias Pechinger,Klaus Bogenberger,Carsten Markgraf*

Main category: cs.CV

TL;DR: 论文提出了AID4AD数据集，将高分辨率航拍图像与nuScenes数据集对齐，用于自动驾驶感知任务，展示了航拍图像在在线地图构建和运动预测中的性能提升。

- Motivation: 研究航拍图像与自动驾驶感知任务的结合，解决高精度地图不可用或维护成本高的问题。
- Method: 通过SLAM点云地图将航拍图像与nuScenes坐标系对齐，提出校正工作流并进行人工质量控制。
- Result: 航拍图像使地图构建精度提升15-23%，轨迹预测性能提升2%。
- Conclusion: 航拍图像是自动驾驶系统中可扩展的环境上下文来源，AID4AD数据集公开以促进相关研究。


### [167] [TrackletGait: A Robust Framework for Gait Recognition in the Wild](https://arxiv.org/abs/2508.02143)
*Shaoxiong Zhang,Jinkai Zheng,Shangdong Zhu,Chenggang Yan*

Main category: cs.CV

TL;DR: TrackletGait框架通过随机轨迹采样、Haar小波下采样和硬度排除三元组损失，解决了实际监控场景中步态识别的挑战，并在Gait3D和GREW数据集上取得了最先进的结果。

- Motivation: 实际监控场景中的步态识别存在非周期性和遮挡问题，传统方法难以应对。
- Method: 提出TrackletGait框架，包括随机轨迹采样、Haar小波下采样和硬度排除三元组损失。
- Result: 在Gait3D和GREW数据集上分别达到77.8和80.4的rank-1准确率，仅使用10.3M参数。
- Conclusion: TrackletGait在复杂场景中表现优异，为步态识别提供了新思路。


### [168] [AURORA: Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation](https://arxiv.org/abs/2508.02149)
*Ziyang Luo,Nian Liu,Fahad Shahbaz Khan,Junwei Han*

Main category: cs.CV

TL;DR: AURORA框架通过结构化思维链提示和特征蒸馏损失提升参考音频-视觉分割任务的语义理解和分割精度。

- Motivation: 现有方法缺乏真正的语义理解，且联合训练推理和分割会降低像素级精度。
- Method: 采用结构化思维链提示机制和特征蒸馏损失，分两阶段训练（自校正和强化学习）。
- Result: AURORA在Ref-AVS基准测试中达到最优性能，并能泛化到未参考的分割任务。
- Conclusion: AURORA通过增强推理能力和语言理解，显著提升了参考音频-视觉分割的性能。


### [169] [AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models](https://arxiv.org/abs/2508.02151)
*Die Chen,Zhongjie Duan,Zhiwen Li,Cen Chen,Daoyuan Chen,Yaliang Li,Yinda Chen*

Main category: cs.CV

TL;DR: AttriCtrl是一个即插即用的框架，用于精确连续控制生成图像的美学属性，解决了现有方法依赖模糊文本提示或高成本人类偏好数据的局限性。

- Motivation: 现有方法在美学属性的细粒度控制上存在不足，尤其是需要连续和强度特定调整时，依赖模糊文本提示或高成本人类偏好数据，限制了可扩展性和实用性。
- Method: 通过预训练视觉语言模型的语义相似性量化抽象美学，并采用轻量级值编码器将标量强度映射到可学习嵌入中，实现美学属性的直观定制控制。
- Result: 实验表明，AttriCtrl能准确控制单个属性并灵活组合多属性，且与开源可控生成框架完全兼容。
- Conclusion: AttriCtrl展示了强大的集成能力和实际应用价值，适用于多样化的生成场景。


### [170] [Efficient Chambolle-Pock based algorithms for Convoltional sparse representation](https://arxiv.org/abs/2508.02152)
*Yi Liu,Junjing Li,Yang Chen,Haowei Tang,Pengcheng Zhang,Tianling Lyu,Zhiguo Gui*

Main category: cs.CV

TL;DR: 论文提出了一种基于Chambolle-Pock框架的快速高效方法，用于解决卷积稀疏编码（CSC）和卷积字典学习（CDL）问题，无需手动选择参数且收敛速度更快。

- Motivation: 传统ADMM方法需要手动选择惩罚参数，可能导致收敛问题或速度慢，因此需要一种更高效的方法。
- Method: 采用Chambolle-Pock框架解决CSC和CDL问题，并引入各向异性总变差惩罚项。
- Result: 在无噪声图像上，新方法与ADMM方法效果相当；在去除高斯噪声方面表现更优。
- Conclusion: 新方法在效率和性能上优于传统ADMM方法，尤其在噪声去除方面表现突出。


### [171] [DreamPainter: Image Background Inpainting for E-commerce Scenarios](https://arxiv.org/abs/2508.02155)
*Sijie Zhao,Jing Cheng,Yaoyao Wu,Hao Xu,Shaohui Jiao*

Main category: cs.CV

TL;DR: 论文提出DreamPainter框架，结合文本提示和参考图像信息，解决电商场景中背景生成的挑战，并基于高质量数据集DreamEcom-400K实现显著性能提升。

- Motivation: 电商场景中背景生成任务面临产品一致性、空间布局合理性和视觉信息整合的挑战，现有方法因缺乏领域数据和精确控制手段而表现不佳。
- Method: 提出DreamPainter框架，利用文本提示和参考图像信息作为控制信号，基于高质量数据集DreamEcom-400K进行训练。
- Result: 实验表明，DreamPainter在保持产品一致性的同时，显著优于现有方法，有效整合文本和视觉信息。
- Conclusion: DreamPainter通过多模态控制信号解决了电商背景生成的关键问题，为实际应用提供了高效解决方案。


### [172] [Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes](https://arxiv.org/abs/2508.02157)
*Tom Fischer,Xiaojie Zhang,Eddy Ilg*

Main category: cs.CV

TL;DR: 提出了一种统一模型，将检测和姿态估计整合到单一框架中，通过神经网格模型和多模型RANSAC，在RGB图像上实现了最先进的类别级姿态估计。

- Motivation: 传统方法依赖RGB-D输入或两阶段模型，限制了应用范围和效率。
- Method: 利用神经网格模型与多模型RANSAC，构建了一个统一的检测和姿态估计框架。
- Result: 在REAL275数据集上，平均性能提升了22.9%，且表现出更强的鲁棒性。
- Conclusion: 统一模型在RGB图像上的类别级姿态估计中表现出色，优于现有方法。


### [173] [Subject or Style: Adaptive and Training-Free Mixture of LoRAs](https://arxiv.org/abs/2508.02165)
*Jia-Chen Zhang,Yu-Jie Xiong*

Main category: cs.CV

TL;DR: EST-LoRA是一种无需训练的LoRA融合方法，通过综合考虑矩阵能量、风格差异分数和时间步长，自适应选择主题和风格LoRA，在生成过程中实现平衡。

- Motivation: 现有方法难以平衡主题和风格，且需要额外训练或涉及复杂超参数调整。
- Method: 提出EST-LoRA，结合矩阵能量、风格差异分数和时间步长，类似MoE架构自适应选择LoRA。
- Result: 实验表明EST-LoRA在质量和速度上优于现有方法。
- Conclusion: EST-LoRA无需训练，高效平衡主题与风格，性能优越。


### [174] [After the Party: Navigating the Mapping From Color to Ambient Lighting](https://arxiv.org/abs/2508.02168)
*Florin-Alexandru Vasluianu,Tim Seizinger,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: CL3AN是一个大规模高分辨率数据集，旨在解决多色光源下图像恢复的复杂性，提出了一种基于Retinex模型的学习框架，显著提升了光照与反射分离的精度。

- Motivation: 现有方法通常假设单一光源或均匀光照，忽略了实际场景中的复杂光照和材质反射效应，导致恢复图像出现不一致性和失真。
- Method: 提出了一种新颖的学习框架，通过显式色度和亮度组件引导，实现光照与反射的精确分离。
- Result: 在现有基准和CL3AN数据集上的评估表明，该方法在非均匀彩色光照和材质反射变化下表现出更强的鲁棒性，同时保持较低的计算成本。
- Conclusion: CL3AN数据集和提出的方法为复杂光照条件下的图像恢复提供了有效解决方案，显著提升了恢复质量。


### [175] [GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting](https://arxiv.org/abs/2508.02172)
*Lei Yao,Yi Wang,Yi Zhang,Moyun Liu,Lap-Pui Chau*

Main category: cs.CV

TL;DR: GaussianCross提出了一种跨模态自监督3D表示学习架构，通过3D高斯散射技术和三属性自适应蒸馏模块，解决了现有方法在点云表示中的模型坍塌和结构信息不足问题，并在多个基准测试中表现出色。

- Motivation: 现有自监督预训练方法在3D场景理解中存在模型坍塌和结构信息不足的问题，导致表达不可靠和性能不佳。
- Method: GaussianCross结合3D高斯散射技术，将点云转换为统一的立方体归一化高斯表示，并通过三属性自适应蒸馏模块构建3D特征场，捕获外观、几何和语义信息。
- Result: 在ScanNet等基准测试中，GaussianCross表现出卓越的参数和数据效率，线性探测和有限数据训练性能优于现有方法，并在ScanNet200任务中显著提升精度。
- Conclusion: GaussianCross通过跨模态一致性学习和高效表示方法，显著提升了3D场景理解的性能，具有广泛的应用潜力。


### [176] [Deep classification algorithm for De-identification of DICOM medical images](https://arxiv.org/abs/2508.02177)
*Bufano Michele,Kotter Elmar*

Main category: cs.CV

TL;DR: 论文提出了一种基于HIPAA安全港方法的Python算法，用于DICOM文件中的PII和PHI信息去标识化。

- Motivation: 由于HIPAA和隐私法规要求，医疗图像研究需要去除或隐藏DICOM文件中的个人可识别信息（PII）和个人健康识别信息（PHI）。
- Method: 研究实现了一种基于HIPAA安全港方法的算法，通过可定制参数对DICOM标签进行分类和去标识化。
- Result: 算法成功识别并去标识化了敏感信息（如姓名、病史、个人数据和机构）。
- Conclusion: 开发的Python算法具有灵活性，适用于日常使用和研究，代码已开源。


### [177] [Weakly Supervised Multimodal Temporal Forgery Localization via Multitask Learning](https://arxiv.org/abs/2508.02179)
*Wenbo Xu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: 论文提出了一种弱监督多模态时序伪造定位方法（WMMT），通过多任务学习实现视频级标注下的细粒度Deepfake检测和时序局部伪造定位。

- Motivation: Deepfake视频的传播引发了信任危机和社会不稳定，但目前缺乏对弱监督多模态细粒度时序伪造定位（WS-MTFL）的系统研究。
- Method: 采用多任务学习范式，将视觉和音频模态检测作为两个二分类任务，并通过Mixture-of-Experts结构自适应选择特征和定位头。提出特征增强模块和可扩展的偏差感知损失以优化性能。
- Result: 实验表明，WMMT在WS-MTFL任务中表现优异，多项指标接近全监督方法。
- Conclusion: WMMT通过多任务学习和创新模块设计，有效解决了弱监督下的多模态时序伪造定位问题。


### [178] [Test-Time Model Adaptation for Quantized Neural Networks](https://arxiv.org/abs/2508.02180)
*Zeshuai Deng,Guohao Chen,Shuaicheng Niu,Hui Luo,Shuhai Zhang,Yifan Yang,Renjie Chen,Wei Luo,Mingkui Tan*

Main category: cs.CV

TL;DR: 论文提出了一种针对量化模型的测试时自适应（TTA）方法，通过零阶优化（ZOA）框架和领域知识管理方案，显著提升了量化模型在动态环境中的鲁棒性和泛化能力。

- Motivation: 量化模型在动态环境中性能下降严重，现有TTA方法因依赖梯度回传而不适用于量化模型。本文旨在解决这一问题。
- Method: 提出零阶优化（ZOA）框架，仅需两次前向传播即可完成模型自适应，并设计领域知识管理方案以减少内存消耗和知识干扰。
- Result: 在量化W6A6 ViT-B模型上，ZOA在ImageNet-C数据集上比现有方法提升5.0%。
- Conclusion: ZOA框架高效且实用，显著提升了量化模型在动态环境中的适应性。


### [179] [Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training](https://arxiv.org/abs/2508.02186)
*Yanyun Wang,Li Liu*

Main category: cs.CV

TL;DR: 论文提出对抗训练（AT）中硬对抗样本的过度学习是导致干净准确性和对抗鲁棒性之间权衡的原因，并提出了一种新的鲁棒感知对抗训练（RPAT）方法来解决这一问题。

- Motivation: 揭示对抗训练中硬对抗样本的过度学习是导致模型决策边界退化和权衡问题的原因，而非传统认为的不足学习。
- Method: 定义鲁棒感知目标，提出RPAT方法，鼓励模型感知随输入扰动平滑变化。
- Result: 在多个数据集和模型上实验，RPAT优于4种常见基线和12种SOTA方法。
- Conclusion: RPAT有效缓解了准确性和鲁棒性的权衡问题，为对抗训练提供了新视角。


### [180] [CMIC: Content-Adaptive Mamba for Learned Image Compression](https://arxiv.org/abs/2508.02192)
*Yunuo Chen,Zezheng Lyu,Bing He,Hongwei Hu,Qi Wang,Yuan Tian,Li Song,Wenjun Zhang,Guo Lu*

Main category: cs.CV

TL;DR: CAM（Content-Adaptive Mamba）是一种动态状态空间模型，通过内容感知的令牌重组和全局先验集成，改进了传统Mamba的内容无关性问题，提升了图像压缩的性能。

- Motivation: 传统Mamba模型因内容无关性和固定扫描方式，无法充分利用内容依赖关系，限制了图像压缩的性能。
- Method: CAM通过内容感知的令牌重组（基于内容相似性聚类和排序）和全局先验集成（通过提示字典）改进Mamba的动态性和全局依赖性捕捉能力。
- Result: CMIC（基于CAM的图像压缩模型）在Kodak、Tecnick和CLIC基准测试中，BD-rate分别比VTM-21.0提升了-15.91%、-21.34%和-17.58%。
- Conclusion: CAM通过动态内容适应和全局依赖性捕捉，显著提升了图像压缩的性能，成为当前最优的解决方案。


### [181] [Welcome New Doctor: Continual Learning with Expert Consultation and Autoregressive Inference for Whole Slide Image Analysis](https://arxiv.org/abs/2508.02220)
*Doanh Cao Bui,Jin Tae Kwak*

Main category: cs.CV

TL;DR: COSFormer是一种基于Transformer的持续学习框架，专为多任务WSI分析设计，能够高效处理新任务而无需重新训练历史数据。

- Motivation: 由于WSI数据量大且临床需求增长，需要一种资源高效且高性能的持续学习系统。
- Method: 提出COSFormer框架，避免重新训练历史数据，支持多任务学习。
- Result: 在七个WSI数据集上测试，COSFormer表现优于现有持续学习框架。
- Conclusion: COSFormer是临床WSI分析的强大解决方案。


### [182] [An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception](https://arxiv.org/abs/2508.02238)
*Xin Dong,Yiwei Zhang,Yangjie Cui,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.CV

TL;DR: 提出了一种基于事件的单积分（ESI）方法，用于高效实时重建强度图像，适用于无人机视觉跟踪等场景。

- Motivation: 事件相机具有高动态范围和高时间分辨率等优势，但如何从异步事件流中提取有效信息并实现高效处理是关键挑战。
- Method: ESI通过单次积分事件流结合增强衰减算法重建强度图像，实现高帧率（100 FPS）实时处理。
- Result: ESI在运行时效率、重建质量和帧率上优于现有算法，显著提升了无人机在恶劣视觉环境下的感知能力。
- Conclusion: ESI是一种高效、便携的方法，适用于事件相机的实时处理，尤其在低光照条件下表现出色。


### [183] [Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor](https://arxiv.org/abs/2508.02240)
*Xiaoliu Guan,Lielin Jiang,Hanqi Chen,Xu Zhang,Jiaxing Yan,Guanzhong Wang,Yi Liu,Zetao Zhang,Yu Wu*

Main category: cs.CV

TL;DR: 论文提出了一种基于泰勒展开的动态缓存机制，用于加速扩散变换器（DiTs）的推理速度，同时保持生成质量。

- Motivation: 现有方法（如TaylorSeer）在模块级预测中存储过多中间特征，导致内存和计算开销大，且固定缓存调度无法适应不同时间步的预测准确性。
- Method: 将泰勒预测目标从模块级转移到最后一个块级，减少缓存特征数量，并通过第一块的预测误差动态调整缓存机制。
- Result: 实验显示，该方法在FLUX、DiT和Wan Video上分别实现了3.17x、2.36x和4.14x的加速，且质量损失可忽略。
- Conclusion: 提出的动态缓存机制在速度和生成质量之间取得了更好的平衡，适用于低资源应用。


### [184] [I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking](https://arxiv.org/abs/2508.02243)
*Ziyan Liu,Junwen Li,Kaiwen Li,Tong Ruan,Chao Wang,Xinyan He,Zongyu Wang,Xuezhi Cao,Jingping Liu*

Main category: cs.CV

TL;DR: 提出了一种基于LLM的多模态实体链接框架I2CR，通过文本优先和视觉辅助的多轮迭代策略提升性能。

- Motivation: 解决现有方法中图像数据冗余和视觉特征一次性提取的问题。
- Method: 采用文本优先策略，当文本不足时通过多轮迭代整合视觉线索。
- Result: 在三个公开数据集上性能提升3.2%、5.1%和1.6%。
- Conclusion: I2CR框架显著优于现有方法，代码已开源。


### [185] [Semi-Supervised Semantic Segmentation via Derivative Label Propagation](https://arxiv.org/abs/2508.02254)
*Yuanbin Fu,Xiaojie Guo*

Main category: cs.CV

TL;DR: DerProp框架通过导数标签传播改进伪标签可靠性，提升半监督语义分割性能。

- Motivation: 减轻标注负担，提升伪标签的可靠性。
- Method: 采用导数标签传播技术，通过离散导数操作约束特征向量，生成严格正则化的相似性度量。
- Result: 实验验证了设计的合理性，性能优于其他方法。
- Conclusion: DerProp通过导数标签传播有效解决了伪标签不可靠的问题。


### [186] [Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning](https://arxiv.org/abs/2508.02258)
*Wenchuan Zhang,Jingru Guo,Hengzhe Zhang,Penghao Zhang,Jie Chen,Shuwan Zhang,Zhang Zhang,Yuhao Yi,Hong Bu*

Main category: cs.CV

TL;DR: Patho-AgenticRAG提出了一种多模态RAG框架，结合文本和图像检索，解决了病理学中视觉语言模型的幻觉问题，显著提升了诊断准确性。

- Motivation: 病理学中的视觉语言模型因超高分辨率、复杂组织结构和临床语义的细微差别，容易产生幻觉输出，影响临床信任。现有RAG方法主要依赖文本知识库，无法充分利用视觉线索。
- Method: 提出Patho-AgenticRAG，基于权威病理学教材的页面级嵌入构建数据库，支持联合文本-图像搜索，避免图像信息的丢失，并支持推理、任务分解和多轮搜索。
- Result: 实验表明，Patho-AgenticRAG在多选诊断和视觉问答等复杂病理任务中显著优于现有多模态模型。
- Conclusion: Patho-AgenticRAG通过多模态检索和推理能力，有效解决了病理学中的幻觉问题，提升了诊断准确性。


### [187] [SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene Completion](https://arxiv.org/abs/2508.02261)
*Rui Qian,Haozhi Cao,Tianchen Deng,Shenghai Yuan,Lihua Xie*

Main category: cs.CV

TL;DR: SplatSSC提出了一种基于深度引导初始化和解耦高斯聚合器的单目3D语义场景补全框架，显著提升了效率和准确性。

- Motivation: 现有基于3D高斯基元的对象中心方法存在初始化效率低和异常基元引入误差的问题。
- Method: 采用深度分支（GMF模块）生成稀疏高斯基元，并通过DGA在体素化过程中解耦几何与语义预测。
- Result: 在Occ-ScanNet数据集上，IoU和mIoU分别提升6.3%和4.1%，延迟和内存消耗降低9.3%。
- Conclusion: SplatSSC通过优化初始化和聚合策略，实现了高效且准确的3D语义场景补全。


### [188] [Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image Classification and Segmentation](https://arxiv.org/abs/2508.02265)
*Peng Zhang,Zhihui Lai,Heng Kong*

Main category: cs.CV

TL;DR: 论文提出了一种名为Hermes的半监督双阈值对比学习策略，用于解决超声图像分类和分割中的伪标签选择问题，并通过任务间一致性学习提升性能。

- Motivation: 现有方法中，基于置信度的伪标签选择容易产生过度自信但错误的预测，且分类和分割任务未充分结合，影响了半监督对比学习的性能。
- Method: 提出双阈值对比学习策略，结合伪标签和对比学习；开发任务间注意力和显著性模块以促进信息共享；设计任务间一致性学习策略以减少特征差异。
- Result: 在两个公共超声数据集和一个私有数据集上的实验表明，Hermes在半监督设置下优于多种先进方法。
- Conclusion: Hermes通过结合对比学习和半监督学习，有效解决了伪标签选择和任务间信息共享问题，提升了超声图像分类和分割的性能。


### [189] [SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching](https://arxiv.org/abs/2508.02278)
*Xiangzeng Liu,Chi Wang,Guanglu Shi,Xiaodong Zhang,Qiguang Miao,Miao Fan*

Main category: cs.CV

TL;DR: SGAD提出了一种新的语义和几何感知描述符网络，通过直接匹配高区分度的区域描述符，显著提高了匹配的准确性和效率。

- Motivation: 现有基于A2PM框架的方法依赖低效的像素级比较和复杂的图匹配，限制了可扩展性。
- Method: SGAD通过生成高区分度的区域描述符实现直接匹配，无需复杂图优化；采用分类和排序子任务的监督策略；引入HCRF消除重叠区域。
- Result: SGAD将运行时间减少60倍（0.82s vs. 60.23s），在户外和室内姿态估计中均表现出更高的准确性和效率。
- Conclusion: SGAD通过创新的描述符和监督策略，显著提升了区域匹配的性能，成为新的SOTA方法。


### [190] [Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical Image Segmentation](https://arxiv.org/abs/2508.02281)
*Paul Zaha,Lars Böcking,Simeon Allmendinger,Leopold Müller,Niklas Kühl*

Main category: cs.CV

TL;DR: 研究了边缘增强预训练对医学图像分割模型性能的影响，并提出了一种基于元学习的策略来选择最佳预训练方法。

- Motivation: 医学图像分割需要大量计算资源和数据，现有研究未系统探讨边缘增强预训练对多模态分割性能的影响。
- Method: 通过两种预训练方式（原始数据和边缘增强数据）训练基础模型，并设计元学习策略选择最佳预训练方法。
- Result: 边缘增强预训练在不同模态中表现不一，元学习策略显著提升了整体分割性能（16.42%-19.30%）。
- Conclusion: 选择性应用边缘增强预训练并结合元学习策略可优化医学图像分割性能。


### [191] [Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection](https://arxiv.org/abs/2508.02288)
*Jae-Young Kang,Hoonhee Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出了一种仅依赖事件相机的立体3D物体检测框架，通过双滤波器机制提取语义和几何信息，并在动态环境中表现优于现有方法。

- Motivation: 传统LiDAR和RGB相机在高动态场景中存在感知间隙，而现有的事件相机融合方法依赖同步传感器，难以应对快速运动。
- Method: 设计了一种仅使用事件相机的立体3D检测框架，引入双滤波器提取语义和几何信息，并通过目标中心信息对齐边界框。
- Result: 实验表明，该方法在动态环境中优于现有方法，验证了事件相机在连续时间3D感知中的潜力。
- Conclusion: 该框架展示了事件相机在高动态场景中的鲁棒性和潜力，代码已开源。


### [192] [Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning](https://arxiv.org/abs/2508.02293)
*Muhammad Aqeel,Shakiba Sharifi,Marco Cristani,Francesco Setti*

Main category: cs.CV

TL;DR: CoMet是一种新的训练策略，用于深度异常检测，能够在未筛选的数据集上学习，无需手动过滤。

- Motivation: 传统无监督异常检测需要手动筛选数据，存在偏差和适应性限制。
- Method: 结合Soft Confident Learning和Meta-Learning，降低低置信度样本权重并稳定训练。
- Result: 在多个数据集上优于基线方法，对训练集中的异常不敏感，达到新SOTA。
- Conclusion: CoMet有效解决了数据筛选问题，提升了模型的鲁棒性和性能。


### [193] [Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment](https://arxiv.org/abs/2508.02307)
*Dmitrii Seletkov,Sophie Starck,Ayhan Can Erdur,Yundi Zhang,Daniel Rueckert,Rickmer Braren*

Main category: cs.CV

TL;DR: 提出一种全身体自监督表示学习方法，用于竞争风险模型下的临床前疾病风险评估，优于传统方法，并在多种疾病中表现优异。

- Motivation: 传统图像风险预测方法通常仅针对单一疾病且依赖手工特征，难以满足临床前多疾病风险评估需求。
- Method: 采用全身体自监督表示学习，结合竞争风险模型，模拟临床前筛查场景并与心脏MRI结合。
- Result: 在心血管疾病、2型糖尿病、慢性阻塞性肺病和慢性肾病中表现优于传统方法，并进一步提升了心血管疾病亚组的预测能力。
- Conclusion: 全身体表示学习具有作为独立筛查工具或临床多模态框架的潜力，可用于早期个性化风险分层。


### [194] [Is Uncertainty Quantification a Viable Alternative to Learned Deferral?](https://arxiv.org/abs/2508.02319)
*Anna M. Wundram,Christian F. Baumgartner*

Main category: cs.CV

TL;DR: 论文探讨了AI在医疗中的安全性，提出不确定性量化方法可能比学习型延迟模型更能应对分布外数据，并通过眼科数据集验证了这一假设。

- Motivation: AI在医疗中的应用需要确保安全性，尤其是在模型可能误分类时能延迟决策给人类专家。现有方法依赖特定训练分布，可能无法应对数据偏移。
- Method: 研究比较了学习型延迟模型和不确定性量化方法，在大规模眼科数据集上评估它们在分布内和分布外的表现。
- Result: 不确定性量化方法在AI延迟决策中表现更优，尤其是在处理分布外数据时。
- Conclusion: 不确定性量化方法可能是实现AI安全延迟决策的更可靠选择。


### [195] [Zero-shot Compositional Action Recognition with Neural Logic Constraints](https://arxiv.org/abs/2508.02320)
*Gefan Ye,Lin Li,Kexin Li,Jun Xiao,Long chen*

Main category: cs.CV

TL;DR: 论文提出了一种逻辑驱动的零样本组合动作识别框架LogicCAR，通过显式建模组合和层次结构约束，解决了现有方法中的组合结构缺失和语义层次模糊问题。

- Motivation: 零样本组合动作识别（ZS-CAR）面临组合结构约束缺失和语义层次模糊的挑战，导致模型性能受限。
- Method: 提出LogicCAR框架，结合显式组合逻辑和层次原始逻辑，通过一阶逻辑形式化约束并嵌入神经网络架构。
- Result: 在Sth-com数据集上的实验表明，LogicCAR优于现有基线方法。
- Conclusion: 逻辑驱动的约束有效提升了零样本组合动作识别的性能。


### [196] [Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth Distillation from Single Images](https://arxiv.org/abs/2508.02323)
*Philipp Wulff,Felix Wimbauer,Dominik Muhle,Daniel Cremers*

Main category: cs.CV

TL;DR: 利用预训练的2D扩散模型和深度预测模型从单张图像生成合成场景几何，用于训练前馈场景重建模型，性能优于多视角监督的基线方法。

- Motivation: 单幅图像的体积场景重建在自动驾驶和机器人等领域有广泛应用，但现有方法通常依赖昂贵的3D真值或多视角监督，限制了其应用范围。
- Method: 提出利用预训练的2D扩散模型和深度预测模型生成合成场景几何，并用于训练前馈场景重建模型。
- Result: 在KITTI-360和Waymo数据集上的实验表明，该方法性能匹配或优于多视角监督的基线方法，尤其在动态场景中表现突出。
- Conclusion: 该方法无需多视角监督即可实现高质量场景重建，具有独特优势。


### [197] [Qwen-Image Technical Report](https://arxiv.org/abs/2508.02324)
*Chenfei Wu,Jiahao Li,Jingren Zhou,Junyang Lin,Kaiyuan Gao,Kun Yan,Sheng-ming Yin,Shuai Bai,Xiao Xu,Yilei Chen,Yuxiang Chen,Zecheng Tang,Zekai Zhang,Zhengyi Wang,An Yang,Bowen Yu,Chen Cheng,Dayiheng Liu,Deqing Li,Hang Zhang,Hao Meng,Hu Wei,Jingyuan Ni,Kai Chen,Kuan Cao,Liang Peng,Lin Qu,Minggang Wu,Peng Wang,Shuting Yu,Tingkun Wen,Wensen Feng,Xiaoxiao Xu,Yi Wang,Yichang Zhang,Yongqiang Zhu,Yujia Wu,Yuxuan Cai,Zenan Liu*

Main category: cs.CV

TL;DR: Qwen-Image是一个图像生成基础模型，通过复杂文本渲染和精确图像编辑的先进技术，实现了在字母和象形文字语言上的优异表现。

- Motivation: 解决复杂文本渲染和图像编辑一致性的挑战。
- Method: 采用渐进式训练策略和多任务训练范式，结合双编码机制。
- Result: 在多个基准测试中达到最先进的性能。
- Conclusion: Qwen-Image在图像生成和编辑方面表现出强大的能力。


### [198] [CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions](https://arxiv.org/abs/2508.02329)
*Ziteng Wang,Siqi Yang,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: CLIP-IN通过指令编辑数据集和长描述性标题增强CLIP的细粒度视觉理解能力，显著提升性能并减少视觉幻觉。

- Motivation: 尽管CLIP等视觉语言模型在视觉与语言对齐方面表现优异，但其细粒度视觉理解能力仍有不足。
- Method: 利用指令编辑数据集生成硬负样本对，结合对称硬负对比损失；引入长描述性标题并使用旋转位置编码。
- Result: 在MMVP基准和细粒度视觉任务中表现显著提升，同时保持零样本性能，并减少多模态大语言模型的视觉幻觉。
- Conclusion: 结合指令驱动的对比学习和丰富描述信息，可显著提升视觉语言模型的细粒度理解能力。


### [199] [Correspondence-Free Fast and Robust Spherical Point Pattern Registration](https://arxiv.org/abs/2508.02339)
*Anik Sarker,Alan T. Asbeck*

Main category: cs.CV

TL;DR: 提出了一种线性时间复杂度的旋转估计算法，通过将球面模式表示为离散3D点集，解决了现有方法计算复杂度高和抗噪能力不足的问题。

- Motivation: 现有球面模式旋转估计方法计算复杂度高（O(n^3)）且对异常值敏感，需要更高效、鲁棒的解决方案。
- Method: 将球面模式表示为离散3D点集，提出三种算法：SPMC、FRS及其混合方法，基于Wahba问题框架。
- Result: 在S^2域和无对应关系设置下，算法比现有方法快10倍以上且精度高10倍以上。
- Conclusion: 新算法在速度和精度上显著优于现有方法，适用于点云配准和球面图像旋转估计等实际任务。


### [200] [Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search](https://arxiv.org/abs/2508.02340)
*Fan Hu,Zijie Xin,Xirong Li*

Main category: cs.CV

TL;DR: 论文提出LPD方法，通过构建特征特定的共同空间和去相关损失，提升视频搜索的多样性和全面性。

- Motivation: 解决视频搜索中视觉多样性带来的挑战，现有方法忽略了多样性空间的需求。
- Method: 提出LPD方法，包括特征特定的共同空间构建和去相关损失，以及熵基多空间三元组排序损失。
- Result: 在TRECVID AVS基准测试中验证了LPD的有效性，并展示了其提升结果多样性的能力。
- Conclusion: LPD通过多样化空间设计，显著提升了视频搜索的全面性和多样性。


### [201] [mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera](https://arxiv.org/abs/2508.02348)
*Byeonggyu Park,Hee-Yeun Kim,Byonghyok Choi,Hansang Cho,Byungkwan Kim,Soomok Lee,Mingu Jeon,Seong-Woo Kim*

Main category: cs.CV

TL;DR: 提出了一种利用摄像头推断道路布局来解释雷达点云数据的新框架，用于定位非视距（NLoS）区域的行人。

- Motivation: 解决自动驾驶系统中非视距区域行人定位的难题，结合雷达和摄像头的优势。
- Method: 通过摄像头视觉信息解释2D雷达点云数据，实现空间场景重建。
- Result: 在真实车辆上进行的实验验证了方法的有效性，定位性能在室外NLoS环境中表现良好。
- Conclusion: 该方法结合雷达和摄像头数据，为NLoS行人定位提供了实用解决方案。


### [202] [Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering](https://arxiv.org/abs/2508.02362)
*Xu Wang,Shengeng Tang,Fei Wang,Lechao Cheng,Dan Guo,Feng Xue,Richang Hong*

Main category: cs.CV

TL;DR: Text2Lip提出了一种基于文本输入的唇动生成框架，通过结构化音素-视觉单元（viseme）作为桥梁，结合渐进式音素-音频替换策略，实现了高语义保真度和视觉真实感的说话人脸生成。

- Motivation: 解决音频驱动方法对高质量配对数据的依赖以及声学与唇动映射的模糊性问题，提升生成的可扩展性和鲁棒性。
- Method: 提出viseme-centric框架，嵌入文本输入为结构化viseme序列；设计渐进式viseme-音频替换策略；利用地标引导的渲染器生成视频。
- Result: Text2Lip在语义保真度、视觉真实感和模态鲁棒性上优于现有方法。
- Conclusion: Text2Lip为可控、灵活的说话人脸生成提供了新范式。


### [203] [Transport-Guided Rectified Flow Inversion: Improved Image Editing Using Optimal Transport Theory](https://arxiv.org/abs/2508.02363)
*Marian Lupascu,Mihai-Sorin Stupariu*

Main category: cs.CV

TL;DR: OTIP是一种基于最优传输理论的零样本框架，用于在整流流模型中实现高保真图像重建与编辑灵活性之间的平衡。

- Motivation: 解决图像反转中重建保真度与编辑灵活性之间的平衡问题。
- Method: 利用最优传输理论指导反转过程，通过轨迹优化平衡重建精度与编辑可控性。
- Result: 在多个数据集上表现优异，如LPIPS得分0.001，SSIM得分0.992，并在编辑任务中显著优于基线方法。
- Conclusion: OTIP在图像编辑中实现了高保真重建与语义一致性的平衡，且计算效率高。


### [204] [TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage Recognition System for Transportation Unit Identification](https://arxiv.org/abs/2508.02372)
*Emre Gülsoylu,André Kelm,Lennart Bengtson,Matthias Hirsch,Christian Wilms,Tim Rolff,Janick Edinger,Simone Frintrop*

Main category: cs.CV

TL;DR: 论文提出了TRUDI数据集和TITUS管道，用于识别港口物流中的运输单元（TUs），解决了缺乏公开基准数据集的问题，并在多样化的实际环境中验证了其可靠性。

- Motivation: 港口物流中运输单元（TUs）的识别效率低，缺乏公开的多样化基准数据集阻碍了研究进展。
- Method: 提出TRUDI数据集（35,034个标注实例）和TITUS管道（三阶段：分割、ID文本定位、识别与验证）。
- Result: TITUS在不同视角、光照和天气条件下可靠识别TUs，优于依赖特定场景的系统。
- Conclusion: TRUDI数据集和TITUS管道为港口物流数字化提供了基准，提升了物流链效率。


### [205] [Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation](https://arxiv.org/abs/2508.02374)
*Shuo Lu,Yanyin Chen,Wei Feng,Jiahao Fan,Fengheng Li,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Jian Liang*

Main category: cs.CV

TL;DR: 论文提出Uni-Layout框架，统一布局生成与评估，通过自然语言提示处理多种任务，并引入人类反馈数据集和动态优化方法，显著优于现有方法。

- Motivation: 当前布局生成方法存在任务特定性和评估指标不匹配问题，限制了应用范围和效果。
- Method: 提出Uni-Layout框架，包括统一生成器、人类反馈数据集Layout-HF100k、人类模拟评估器及动态优化方法DMPO。
- Result: 实验表明Uni-Layout显著优于任务特定和通用方法。
- Conclusion: Uni-Layout通过统一生成与评估，结合人类反馈和动态优化，提升了布局生成的通用性和效果。


### [206] [SMART-Ship: A Comprehensive Synchronized Multi-modal Aligned Remote Sensing Targets Dataset and Benchmark for Berthed Ships Analysis](https://arxiv.org/abs/2508.02384)
*Chen-Chen Fan,Peiyao Guo,Linping Zhang,Kehan Qi,Haolin Huang,Yong-Qiang Mao,Yuxi Suo,Zhizhuo Jiang,Yu Liu,You He*

Main category: cs.CV

TL;DR: SMART-Ship数据集是一个多模态遥感数据集，用于停泊船舶分析，包含五种模态的图像，支持多种遥感任务。

- Motivation: 解决多尺度目标和动态环境下海上监视的挑战，提供时空一致的多模态数据。
- Method: 构建包含1092组多模态图像的数据集，每组图像在一周内获取并注册，提供细粒度标注。
- Result: 数据集覆盖38,838艘船舶，支持五种基准任务，验证了其多模态遥感任务的适用性。
- Conclusion: SMART-Ship数据集为多模态遥感解释任务提供了支持，并指明了未来研究方向。


### [207] [Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection](https://arxiv.org/abs/2508.02386)
*Xingyu Feng,Hebei Gao,Hong Li*

Main category: cs.CV

TL;DR: COLER是一种无监督实例分割和对象检测方法，通过CutOnce生成粗伪标签，并通过自训练提升性能。

- Motivation: 推动无监督对象定位领域的发展，提出一种简单且高效的方法。
- Method: 使用CutOnce生成粗伪标签，结合自训练优化检测器性能。
- Result: 在多个基准测试中优于现有方法，无需专门设计的损失函数。
- Conclusion: COLER是一种零样本无监督模型，具有潜力推动无监督对象定位的进展。


### [208] [Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera Fusion](https://arxiv.org/abs/2508.02409)
*Yimeng Liu,Maolin Gan,Huaili Zeng,Li Liu,Younsuk Dong,Zhichao Cao*

Main category: cs.CV

TL;DR: Hydra是一种结合毫米波雷达和摄像头技术的新方法，用于检测叶片湿润度（LWD），通过融合多模态数据和深度学习模型，实现了高精度和鲁棒性。

- Motivation: 现有LWD检测方法缺乏标准化且适应性差，无法直接测量自然叶片或在多变环境中保持稳定。
- Method: 结合毫米波雷达和摄像头，设计CNN融合多模态数据，并使用Transformer编码器生成特征图，最后通过分类器检测叶片湿润度。
- Result: 在多种场景下分类准确率达96%，在恶劣环境中仍保持约90%的准确率。
- Conclusion: Hydra在LWD检测中表现出高精度和鲁棒性，适用于实际农业应用。


### [209] [HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time Series Analysis](https://arxiv.org/abs/2508.02411)
*Xiao Wang,Hao Si,Fan Zhang,Xiaoya Zhou,Dengdi Sun,Wanli Lyu,Qingquan Yang,Jin Tang*

Main category: cs.CV

TL;DR: 论文提出了一种基于超图的时间序列变换器网络HGTS-Former，用于解决多变量时间序列数据的高维性和复杂耦合问题。

- Motivation: 多变量时间序列分析因其高维性和复杂交互性而具有挑战性，超图的结构建模能力为此提供了灵感。
- Method: 通过多头部自注意力增强时间表示，构建分层超图以聚合时间模式和变量间关系，并通过EdgeToNode模块和FFN增强特征。
- Result: 在多个数据集上的实验验证了HGTS-Former的有效性。
- Conclusion: HGTS-Former在多变量时间序列任务中表现出色，代码将开源。


### [210] [Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens](https://arxiv.org/abs/2508.02419)
*Haohan Zheng,Zhenguo Zhang*

Main category: cs.CV

TL;DR: 大型视觉语言模型（LVLMs）存在严重的物体幻觉问题，研究发现其不仅忽视视觉信息，还可能忽略文本模态，称为模态偏差。作者提出一种无需训练的方法，通过调整注意力权重和对比解码策略，有效缓解幻觉问题。

- Motivation: LVLMs在理解和推理方面表现出色，但存在物体幻觉问题。传统研究认为问题源于视觉编码器与大型语言模型（LLMs）的规模不匹配，但作者发现LVLMs还可能忽视文本模态，导致对用户指令的碎片化理解。
- Method: 提出一种无需训练的方法，通过干预和调整视觉与文本标记的注意力权重，平衡跨模态兼容性。同时采用对比解码策略，减少对参数知识的过度依赖。
- Result: 实验证实模态偏差在LVLMs中普遍存在。所提方法在多个开源LVLMs和基准测试中有效缓解了幻觉问题，展示了其通用性和高效性。
- Conclusion: 研究揭示了LVLMs的模态偏差问题，并提出了一种简单有效的方法来缓解物体幻觉，为未来研究提供了新方向。


### [211] [Glioblastoma Overall Survival Prediction With Vision Transformers](https://arxiv.org/abs/2508.02439)
*Yin Lin,iccardo Barbieri,Domenico Aquino,Giuseppe Lauria,Marina Grisoli,Elena De Momi,Alberto Redaelli,Simona Ferrante*

Main category: cs.CV

TL;DR: 提出了一种基于Vision Transformers（ViTs）的AI方法，直接从MRI图像预测胶质母细胞瘤患者的总生存期（OS），无需肿瘤分割，简化流程并减少计算资源需求。

- Motivation: 胶质母细胞瘤是一种高度侵袭性的脑肿瘤，生存期预测对个性化治疗和临床决策至关重要。
- Method: 利用ViTs直接从MRI图像提取特征，避免传统方法中的肿瘤分割步骤。
- Result: 在BRATS数据集上测试，准确率达62.5%，在精确率、召回率和F1分数上表现均衡。
- Conclusion: ViTs适用于医学影像任务，为高效且无需分割的OS预测模型奠定了基础，但数据集规模限制了泛化能力。


### [212] [InfoSyncNet: Information Synchronization Temporal Convolutional Network for Visual Speech Recognition](https://arxiv.org/abs/2508.02460)
*Junxiao Xue,Xiaozhen Liu,Xuecheng Wu,Fei Yu,Jun Wang*

Main category: cs.CV

TL;DR: InfoSyncNet通过非均匀序列建模和数据增强技术，显著提升了从无声视频中估计语音内容的准确性。

- Motivation: 从无声视频中准确估计语音内容对辅助技术和增强现实应用至关重要，但现有方法因序列变异性和信息分布不均而面临挑战。
- Method: 提出InfoSyncNet，包含非均匀量化模块和多种训练策略，动态调整网络焦点并处理视觉语音数据的不一致性。
- Result: 在LRW和LRW1000数据集上分别达到92.0%和60.7%的Top-1准确率，创下新纪录。
- Conclusion: InfoSyncNet通过创新设计和训练策略，显著提升了视觉语音识别的性能，为相关应用提供了有效解决方案。


### [213] [SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models](https://arxiv.org/abs/2508.02464)
*Yonghuang Wu,Wenwen Zeng,Xuan Xie,Chengqian Zhao,Guoqing Wu,Jinhua Yu*

Main category: cs.CV

TL;DR: SAMPO通过偏好优化教视觉基础模型从稀疏视觉交互中推断高级类别意图，显著提升密集同质对象的分割性能。

- Motivation: 解决SAM模型在密集同质对象（如生物医学核分割）中因稀疏视觉提示导致分割不完整的问题。
- Method: 提出SAMPO框架，通过偏好优化隐式捕捉目标类别特征，不依赖语言模型。
- Result: 在三个医学分割任务中表现优异，仅用10%训练数据即超越基线方法，性能提升9个百分点。
- Conclusion: SAMPO为视觉基础模型建立了意图感知对齐的新范式，无需辅助提示生成器或语言模型。


### [214] [Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions](https://arxiv.org/abs/2508.02477)
*Jaehyuk Heo,Pilsung Kang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Hierarchical Coreset (HierCore)的新框架，用于多类别图像异常检测，通过分层记忆库估计类别决策标准，并在不同标签条件下验证其性能。

- Motivation: 多类别图像异常检测中，单模型性能通常低于类别专用模型，且类别信息的使用方式对检测阈值定义影响未被充分研究。
- Method: 提出HierCore框架，利用分层记忆库估计类别决策标准，无需类别标签即可有效工作。
- Result: 实验表明HierCore在所有四种标签条件下均满足要求，性能稳定且优于现有方法。
- Conclusion: HierCore在多类别异常检测任务中具有实际应用潜力，性能稳定且适应性强。


### [215] [Fine-grained Multiple Supervisory Network for Multi-modal Manipulation Detecting and Grounding](https://arxiv.org/abs/2508.02479)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: 提出了一种细粒度多监督（FMS）网络，通过模态可靠性监督、单模态内部监督和跨模态监督，提升多模态媒体篡改检测性能。

- Motivation: 现有方法因忽略不可靠单模态数据的干扰和缺乏全面篡改监督，导致性能受限。
- Method: FMS网络包含三个模块：MDSC（多模态决策监督校正）、UFMR（单模态篡改挖掘强化）和MFAR（多模态篡改对齐推理）。
- Result: 实验表明FMS优于现有方法。
- Conclusion: FMS通过多监督机制有效提升了篡改检测的细粒度和性能。


### [216] [MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding](https://arxiv.org/abs/2508.02480)
*Wenwen Zeng,Yonghuang Wu,Yifan Chen,Xuan Xie,Chengqian Zhao,Feiyu Yin,Guoqing Wu,Jinhua Yu*

Main category: cs.CV

TL;DR: 提出了一种新的多镜头fMRI视频重建框架，通过分解和语义解码解决信号混合和时间分辨率不匹配问题。

- Motivation: 从fMRI重建动态视频对理解视觉认知和开发脑机接口至关重要，但现有方法仅限于单镜头片段，无法处理真实世界中的多镜头场景。
- Method: 采用分而治之的解码框架，包括镜头边界预测模块、生成关键帧字幕的LLM模块和大规模数据合成。
- Result: 实验表明，该框架在多镜头重建保真度上优于现有方法，分解和语义字幕显著提升了性能。
- Conclusion: 该研究为多镜头fMRI重建提供了新范式，通过分解和语义提示准确恢复复杂视觉叙事。


### [217] [Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian Splatting](https://arxiv.org/abs/2508.02493)
*Jianchao Wang,Peng Zhou,Cen Li,Rong Quan,Jie Qin*

Main category: cs.CV

TL;DR: 论文提出EFA-GS方法，通过频域分析解决3D高斯泼溅中的浮动伪影问题，优化低频学习并减少细节损失。

- Motivation: 3D高斯泼溅（3DGS）在低质量初始化时会产生浮动伪影，影响视觉保真度，但其成因尚未完全探索。
- Method: 从频域角度分析伪影成因，提出EFA-GS方法，选择性扩展未优化的高斯分布，并结合深度和尺度策略动态优化。
- Result: 在合成和真实数据集上，EFA-GS显著减少伪影并保留高频细节，PSNR提升1.68 dB。
- Conclusion: EFA-GS有效解决浮动伪影问题，并在下游3D编辑任务中验证了其有效性。


### [218] [Rethinking Transparent Object Grasping: Depth Completion with Monocular Depth Estimation and Instance Mask](https://arxiv.org/abs/2508.02507)
*Yaofeng Cheng,Xinkai Gao,Sen Zhang,Chao Zeng,Fusheng Zha,Lining Sun,Chenguang Yang*

Main category: cs.CV

TL;DR: ReMake提出了一种基于实例掩码和单目深度估计的深度补全框架，显式区分透明区域以提高深度估计的准确性和泛化能力。

- Motivation: 透明物体的光学特性导致深度相机生成不完整或无效的深度数据，降低了机器人抓取的准确性和可靠性。现有方法依赖模型隐式推理，泛化能力不足。
- Method: 通过实例掩码显式区分透明区域，结合单目深度估计提供深度上下文，专注于透明区域的深度学习。
- Result: 实验表明，ReMake在基准数据集和真实场景中均优于现有方法，表现出更高的准确性和泛化能力。
- Conclusion: ReMake通过显式区分透明区域和利用单目深度估计，显著提升了深度补全的准确性和泛化性能。


### [219] [Engagement Prediction of Short Videos with Large Multimodal Models](https://arxiv.org/abs/2508.02516)
*Wei Sun,Linhan Cao,Yuqin Cao,Weixia Zhang,Wen Wen,Kaiwei Zhang,Zijian Chen,Fangfang Lu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文研究了大型多模态模型（LMMs）在短视频参与度预测中的应用，发现结合音频、视觉和语言模态的模型表现更优。

- Motivation: 短视频平台用户生成内容（UGC）激增，参与度预测对推荐系统和内容创作至关重要，但现有方法难以有效建模跨特征和跨模态交互。
- Method: 采用VideoLLaMA2（整合音频、视觉和语言模态）和Qwen2.5-VL（仅视觉和语言模态）两种LMMs，在SnapUGC数据集上训练。
- Result: 两种模型均表现优异，VideoLLaMA2优于Qwen2.5-VL，表明音频特征的重要性；集成模型在ICCV VQualA 2025挑战赛中夺冠。
- Conclusion: LMMs在参与度预测中有效，音频模态是关键因素；集成方法进一步提升性能。


### [220] [Understanding the Risks of Asphalt Art on the Reliability of Surveillance Perception Systems](https://arxiv.org/abs/2508.02530)
*Jin Ma,Abyad Enan,Long Cheng,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 研究探讨了艺术斑马线对预训练视觉目标检测模型行人检测性能的影响，发现复杂艺术图案会显著降低检测性能，且对抗性设计可能被利用来干扰系统。

- Motivation: 艺术斑马线虽提升行人安全，但其视觉复杂性可能干扰依赖视觉的监控系统，需研究其影响。
- Method: 通过合成不同艺术图案到固定监控场景，评估模型在正常和对抗性艺术斑马线上的行人检测表现。
- Result: 复杂艺术图案显著降低检测性能，对抗性设计可故意遮挡行人或生成虚假检测。
- Conclusion: 研究揭示了城市视觉监控系统的潜在漏洞，强调在设计鲁棒行人感知模型时需考虑环境视觉变化。


### [221] [Precision-Aware Video Compression for Reducing Bandwidth Requirements in Video Communication for Vehicle Detection-Based Applications](https://arxiv.org/abs/2508.02533)
*Abyad Enan,Jon C Calhoun,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: PAVC框架动态调整视频压缩级别，根据天气和光照条件优化带宽使用和车辆检测精度。

- Motivation: 智能交通系统中，带宽限制和视频质量下降影响实时车辆检测的准确性，需动态调整压缩级别以适应环境变化。
- Method: 使用PAVC框架，动态调整视频压缩级别，结合天气和光照条件，优化带宽和检测精度。
- Result: PAVC提升车辆检测精度13%，带宽需求减少8.23倍（中等带宽）或72倍（低带宽）。
- Conclusion: PAVC有效平衡带宽需求和检测精度，适用于不同带宽环境的智能交通系统。


### [222] [MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming](https://arxiv.org/abs/2508.02549)
*Shuo Wang,Yongcai Wang,Wanting Li,Yucheng Wang,Maiyue Chen,Kaihui Wang,Zhizhong Su,Xudong Cai,Yeying Jin,Deying Li,Zhaoxin Fan*

Main category: cs.CV

TL;DR: MonoDream是一个轻量级的视觉语言动作（VLA）框架，通过统一的导航表示（UNR）和潜在全景梦境（LPD）任务，显著提升了单目视觉导航的性能，缩小了与全景RGB-D方法的差距。

- Motivation: 全景RGB-D传感器在现实部署中成本高或难以获取，而现有的单目VLA模型性能仍落后于全景方法。
- Method: MonoDream通过UNR联合对齐视觉语义和语言动作意图，并引入LPD任务，基于单目输入预测全景RGB-D的潜在特征。
- Result: 在多个VLN基准测试中，MonoDream显著提升了单目导航性能，缩小了与全景方法的差距。
- Conclusion: MonoDream为低成本单目导航提供了高效解决方案，具有实际应用潜力。


### [223] [ReMoMask: Retrieval-Augmented Masked Motion Generation](https://arxiv.org/abs/2508.02605)
*Zhengdao Li,Siheng Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: ReMoMask提出了一种统一的文本到运动生成框架，通过双向动量文本-运动模型、语义时空注意力机制和RAG-无分类器引导，解决了现有方法的多样性和物理合理性问题。

- Motivation: 当前文本到运动生成方法存在多样性不足、物理不合理等问题，ReMoMask旨在通过创新技术解决这些挑战。
- Method: 1) 双向动量文本-运动模型；2) 语义时空注意力机制；3) RAG-无分类器引导。
- Result: 在HumanML3D和KIT-ML基准测试中，FID分数分别提升了3.88%和10.97%。
- Conclusion: ReMoMask在生成多样且物理合理的运动序列方面表现优异，优于现有方法。


### [224] [Evaluating Variance in Visual Question Answering Benchmarks](https://arxiv.org/abs/2508.02645)
*Nikitha SR*

Main category: cs.CV

TL;DR: 本文探讨了多模态大语言模型（MLLMs）在视觉问答（VQA）评估中的性能方差问题，提出了方差感知方法的重要性。

- Motivation: 当前MLLMs在VQA基准测试中的评估仅依赖点估计，忽视了性能方差的影响，如随机模型输出、训练种子敏感性和超参数配置。
- Method: 通过分析14个VQA基准测试，研究训练种子、框架非确定性、模型规模和指令微调对性能方差的影响，并探索Cloze式评估的有效性。
- Result: 研究发现当前评估方法存在局限性，性能方差显著，Cloze式评估能减少随机性并提高可靠性。
- Conclusion: 建议采用方差感知方法以促进MLLMs更稳健和可靠的发展。


### [225] [PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal Spans via 3D Gaussian Splatting](https://arxiv.org/abs/2508.02660)
*Yijun Xu,Jingrui Zhang,Yuhan Chen,Dingwen Wang,Lei Yu,Chu He*

Main category: cs.CV

TL;DR: PMGS提出了一种基于3D高斯抛射的运动重建方法，通过目标建模和运动恢复两阶段流程，结合物理一致性约束和动态学习率策略，显著提升了高速非线性刚体运动的重建效果。

- Motivation: 现有动态重建方法主要局限于短时小尺度变形，且缺乏物理一致性考虑，难以处理大时空跨度的复杂刚体运动。
- Method: 1) 目标建模：通过动态场景分解和改进的点密度控制实现对象集中重建；2) 运动恢复：通过逐帧SE(3)位姿学习恢复完整运动序列，引入加速度一致性约束和动态模拟退火策略。
- Result: 实验表明，PMGS在高速非线性刚体运动重建中优于主流动态方法。
- Conclusion: PMGS通过结合物理约束和自适应优化策略，有效解决了复杂刚体运动的动态重建问题。


### [226] [MedVLThinker: Simple Baselines for Multimodal Medical Reasoning](https://arxiv.org/abs/2508.02669)
*Xiaoke Huang,Juncheng Wu,Hui Liu,Xianfeng Tang,Yuyin Zhou*

Main category: cs.CV

TL;DR: MedVLThinker提出了一种开放且可复现的方法，用于构建基于推理的医学LMM，包括数据整理和两种训练范式（SFT和RLVR），RLVR表现更优，且仅使用文本数据训练效果更好。

- Motivation: 当前缺乏开放和可复现的方法来构建推理型医学LMM，阻碍了社区研究和比较。
- Method: 包括系统数据整理（文本和图像-文本数据）和两种训练范式（SFT和RLVR）。
- Result: RLVR显著优于SFT，仅使用文本数据训练效果更好，7B模型在公开VQA基准上达到新SOTA。
- Conclusion: MedVLThinker为多模态医学推理研究提供了强大的开放基础。


### [227] [Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models](https://arxiv.org/abs/2508.02671)
*Haoyang Li,Liang Wang,Chao Wang,Siyu Zhou,Jing Jiang,Yan Peng,Guodong Long*

Main category: cs.CV

TL;DR: AugPT是一种基于内部数据增强的提示调优方法，通过自监督增强和共识测试门控机制提升模型性能，无需外部知识。

- Motivation: 现有提示调优方法依赖外部知识，成本高且忽略图像模态特征，AugPT旨在利用内部数据增强更高效地利用已知特征。
- Method: AugPT通过自监督增强训练集未标注图像，并采用共识测试门控机制过滤噪声样本，提升增强视图质量。
- Result: 实验表明AugPT在不依赖外部知识的情况下，显著提升模型性能和泛化能力。
- Conclusion: AugPT为提示调优提供了一种高效、自给自足的数据增强方法，具有实际应用潜力。
## q-bio.GN

### [228] [A Large-Scale Benchmark of Cross-Modal Learning for Histology and Gene Expression in Spatial Transcriptomics](https://arxiv.org/abs/2508.01490)
*Rushin H. Gindra,Giovanni Palla,Mathias Nguyen,Sophia J. Wagner,Manuel Tran,Fabian J Theis,Dieter Saur,Lorin Crawford,Tingying Peng*

Main category: q-bio.GN

TL;DR: HESCAPE是一个用于空间转录组学中跨模态对比预训练的大规模基准，评估了图像和基因表达编码器，发现基因表达编码器对表征对齐起主要作用，但对比预训练在基因突变分类和基因表达预测任务中表现矛盾。

- Motivation: 空间转录组学缺乏评估多模态学习方法的综合基准，HESCAPE旨在填补这一空白。
- Method: 基于6种基因面板和54名捐赠者的数据集，系统评估了图像和基因表达编码器的预训练策略，并测试了其在基因突变分类和基因表达预测任务中的效果。
- Result: 基因表达编码器主导表征对齐，但对比预训练在基因突变分类中表现优异，而在基因表达预测中表现较差，批次效应是关键干扰因素。
- Conclusion: HESCAPE强调了批次鲁棒多模态学习方法的重要性，并提供了标准化数据集和工具以推动研究。
## cs.GR

### [229] [MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh](https://arxiv.org/abs/2508.01242)
*Shuangkang Fang,I-Chao Shen,Yufeng Wang,Yi-Hsuan Tsai,Yi Yang,Shuchang Zhou,Wenrui Ding,Takeo Igarashi,Ming-Hsuan Yang*

Main category: cs.GR

TL;DR: MeshLLM利用大语言模型理解和生成文本序列化的3D网格，通过Primitive-Mesh分解策略和局部网格组装训练，显著提升性能。

- Motivation: 解决现有方法在数据集规模和3D结构信息保留上的局限性。
- Method: 采用Primitive-Mesh分解策略，构建大规模数据集，并引入面连接推断和局部网格组装训练。
- Result: MeshLLM在生成质量和形状理解上优于现有方法LLaMA-Mesh。
- Conclusion: MeshLLM在处理文本序列化3D网格方面具有巨大潜力。


### [230] [ReMu: Reconstructing Multi-layer 3D Clothed Human from Image Layers](https://arxiv.org/abs/2508.01381)
*Onat Vuran,Hsuan-I Ho*

Main category: cs.GR

TL;DR: ReMu提出了一种基于单目RGB相机的新方法，用于重建多层3D服装，无需昂贵设备或模板，支持多样化的服装风格。

- Motivation: 传统多层3D服装重建需要昂贵的多视角捕捉设备和专业编辑，限制了真实感虚拟人的创建。
- Method: 通过单目相机捕捉多层服装图像，使用统一3D表示建模，并通过碰撞感知优化和隐式神经场细化边界。
- Result: 实验表明，该方法能重建几乎无穿透的3D服装，性能与类别特定方法相当。
- Conclusion: ReMu为低成本、高效的多层3D服装重建提供了新思路。


### [231] [A Plug-and-Play Multi-Criteria Guidance for Diverse In-Betweening Human Motion Generation](https://arxiv.org/abs/2508.01590)
*Hua Yu,Jiao Liu,Xu Gui,Melvin Wong,Yaqing Hou,Yew-Soon Ong*

Main category: cs.GR

TL;DR: MCG-IMM是一种用于人类运动生成的新方法，通过多标准优化提升预训练模型的多样性，无需额外参数。

- Motivation: 解决现有方法在生成多样且平滑的过渡运动时的挑战，特别是在批量采样中保持运动差异性的问题。
- Method: 将预训练生成模型的采样过程重新定义为多标准优化问题，引入优化过程以满足多样性和平滑性等标准。
- Result: 在四个流行的人类运动数据集上，MCG-IMM表现优于现有方法。
- Conclusion: MCG-IMM是一种即插即用的方法，能显著提升运动生成的多样性，适用于多种生成模型。


### [232] [Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility](https://arxiv.org/abs/2508.02443)
*Thomas Gottwald,Edgar Heinert,Matthias Rottmann*

Main category: cs.GR

TL;DR: 提出了一种新的高斯泼溅不确定性估计方法，通过训练误差和可见性的原始表示生成不确定性特征图，并通过回归模型聚合，优于现有方法。

- Motivation: 高斯泼溅在机器人和医学等关键应用中需要可靠的不确定性估计，现有方法通常仅估计高斯原始的方差，缺乏对训练视图误差和可见性的直接表示。
- Method: 通过将训练误差和可见性投影到原始表示上，生成不确定性特征图，并通过回归模型聚合这些特征图。
- Result: 实验表明，该方法与真实误差高度相关，尤其在前景对象上优于现有方法，回归模型还展示了对新场景的泛化能力。
- Conclusion: 该方法为高斯泼溅提供了有效的不确定性估计，适用于关键应用，且无需额外保留数据。
## cs.CL

### [233] [ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks](https://arxiv.org/abs/2508.01943)
*Philip Schroeder,Ondrej Biza,Thomas Weng,Hongyin Luo,James Glass*

Main category: cs.CL

TL;DR: ROVER框架通过递归分解长视频轨迹为短子任务段，提升视频推理能力，减少幻觉，并在多个任务中表现优于基线。

- Motivation: 现有视觉语言模型在长帧序列推理上表现不佳，限制了其在需要连续视觉输入的具身任务中的应用。
- Method: 提出ROVER框架，递归分解视频轨迹为子任务段，采用上下文学习方法实现。
- Result: 在OpenX Embodiment和RoboCasa数据集上，ROVER在任务进度估计、帧级自然语言推理和视频问答任务中优于基线。
- Conclusion: ROVER通过减少每步推理的帧数，有效减少幻觉，且时间复杂度线性增长，优于基线。
## cs.HC

### [234] [Visuo-Acoustic Hand Pose and Contact Estimation](https://arxiv.org/abs/2508.00852)
*Yuemin Ma,Uksang Yoo,Yunchao Yao,Shahram Najam Syed,Luca Bondi,Jonathan Francis,Jean Oh,Jeffrey Ichnowski*

Main category: cs.HC

TL;DR: VibeMesh是一种新型可穿戴系统，结合视觉和主动声学传感，用于高精度手部姿势和接触事件估计。

- Motivation: 手部姿势和接触事件的准确估计在机器人数据收集、虚拟现实和生物力学分析中至关重要，但视觉遮挡、接触信号微弱以及缺乏灵活的触觉传感技术使其具有挑战性。
- Method: VibeMesh通过骨传导扬声器和稀疏压电麦克风分布在人手上，发射结构化声学信号并捕捉其传播以推断接触变化。采用基于图的注意力网络处理同步音频频谱和RGB-D手部网格数据。
- Result: VibeMesh在准确性和鲁棒性上优于仅基于视觉的方法，尤其在遮挡或静态接触场景中表现突出。
- Conclusion: VibeMesh提供了一种轻量级、非侵入式的多模态传感平台，显著提升了手部姿势和接触事件的估计能力。


### [235] [Sonify Anything: Towards Context-Aware Sonic Interactions in AR](https://arxiv.org/abs/2508.01789)
*Laura Schütz,Sasan Matinfar,Ulrich Eck,Daniel Roth,Nassir Navab*

Main category: cs.HC

TL;DR: 论文提出了一种基于计算机视觉和物理建模的实时声音合成框架，以增强AR中虚拟与真实物体交互的声音真实感。

- Motivation: AR中虚拟物体缺乏物理性导致声音交互不自然，影响多感官体验和交互真实感。
- Method: 利用计算机视觉识别真实物体材料，结合物理属性和碰撞动态，实时生成基于材料的声音。
- Result: 用户研究表明，基于材料的声音显著提升了声音交互的真实感和材料辨识准确性。
- Conclusion: 上下文感知的基于材料的声音交互增强了AR的真实感和对现实环境的感知。


### [236] [Implicit Search Intent Recognition using EEG and Eye Tracking: Novel Dataset and Cross-User Prediction](https://arxiv.org/abs/2508.01860)
*Mansi Sharma,Shuang Chen,Philipp Müller,Maurice Rekrut,Antonio Krüger*

Main category: cs.HC

TL;DR: 论文提出了一种结合EEG和眼动追踪的方法，用于区分人类在视觉搜索中的导航意图和信息意图，解决了之前方法在真实场景中的局限性。

- Motivation: 为了帮助机器在视觉搜索任务中更有效地辅助人类，需要区分人类的导航意图和信息意图。之前的方法存在固定搜索时间和依赖用户特定训练数据的局限性。
- Method: 提出了一种新的跨用户预测方法，结合EEG和眼动追踪数据，用户自行决定搜索时间。
- Result: 在留一用户评估中，跨用户预测准确率达到84.5%，接近用户内预测准确率（85.5%）。
- Conclusion: 该方法显著提升了在真实场景中的适用性和灵活性。
## eess.SP

### [237] [Toward a reliable PWM-based light-emitting diode visual stimulus for improved SSVEP response with minimal visual fatigue](https://arxiv.org/abs/2508.02359)
*Surej Mouli,Ramaswamy Palaniappan*

Main category: eess.SP

TL;DR: 研究探讨了使用极高占空比的视觉刺激来减轻SSVEP应用中的视觉疲劳，并发现85%占空比时效果最佳。

- Motivation: SSVEP在视觉诊断和脑机接口中应用广泛，但视觉疲劳和PWM精度问题限制了其长期使用。
- Method: 通过精确的LED硬件生成50%至95%的PWM占空比，记录脑电图数据并测试十名受试者。
- Result: 增加占空比减少了视觉疲劳，85%占空比时SSVEP响应达到峰值。
- Conclusion: 高占空比刺激可提升SSVEP的实用性，85%占空比是理想选择。
## cs.CY

### [238] [FairFedMed: Benchmarking Group Fairness in Federated Medical Imaging with FairLoRA](https://arxiv.org/abs/2508.00873)
*Minghan Li,Congcong Wen,Yu Tian,Min Shi,Yan Luo,Hao Huang,Yi Fang,Mengyu Wang*

Main category: cs.CY

TL;DR: 论文提出了首个医疗联邦学习的公平性基准和数据集FairFedMed，并提出了FairLoRA框架，显著提升了医疗图像分类的公平性和性能。

- Motivation: 医疗领域的不公平问题（如服务获取和结果差异）亟待解决，而现有联邦学习研究多关注非医疗应用，缺乏针对医疗数据的公平性研究。
- Method: 建立了FairFedMed数据集，包含眼科和胸科数据，并提出了基于SVD低秩近似的FairLoRA框架，为不同人口群体定制奇异值矩阵。
- Result: 实验表明，FairLoRA在医疗图像分类中表现优异，同时显著提升了跨人口群体的公平性。
- Conclusion: FairFedMed和FairLoRA填补了医疗联邦学习公平性研究的空白，为未来研究提供了基准和工具。
## cs.RO

### [239] [Sparse 3D Perception for Rose Harvesting Robots: A Two-Stage Approach Bridging Simulation and Real-World Applications](https://arxiv.org/abs/2508.00900)
*Taha Samavati,Mohsen Soryani,Sina Mansouri*

Main category: cs.RO

TL;DR: 提出一种用于玫瑰采摘机器人的3D感知算法，结合2D检测与深度估计，利用合成数据训练，性能优于传统方法。

- Motivation: 全球药用植物需求增长，但人工采摘效率低，需自动化解决方案。
- Method: 两阶段算法：2D点检测+深度估计，使用合成数据训练轻量级神经网络。
- Result: 2D检测F1分数95.6%（合成）/74.4%（真实），深度误差3%（2米范围）。
- Conclusion: 算法高效且兼容资源受限系统，为农业自动化提供可扩展方案。


### [240] [A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles](https://arxiv.org/abs/2508.00917)
*Jiayuan Wang,Farhad Pourpanah,Q. M. Jonathan Wu,Ning Zhang*

Main category: cs.RO

TL;DR: 本文综述了多任务学习（MTL）在联网自动驾驶车辆（CAV）中的应用，探讨了其在感知、预测、规划等关键模块的潜力与挑战。

- Motivation: 传统方法使用独立模型处理CAV的多个任务，导致高成本和计算开销。MTL通过统一模型联合学习多任务，有望提升效率和资源利用率。
- Method: 综述了MTL在CAV中的应用，涵盖感知、预测、规划、控制和多智能体协作等模块。
- Result: 分析了现有方法的优缺点，指出了研究空白，并提出了未来研究方向。
- Conclusion: MTL为CAV系统提供了高效解决方案，但仍需进一步研究以优化其性能和应用范围。


### [241] [Hestia: Hierarchical Next-Best-View Exploration for Systematic Intelligent Autonomous Data Collection](https://arxiv.org/abs/2508.01014)
*Cheng-You Lu,Zhuoli Zhuang,Nguyen Thanh Trung Le,Da Xiao,Yu-Cheng Chang,Thomas Do,Srinath Sridhar,Chin-teng Lin*

Main category: cs.RO

TL;DR: Hestia利用强化学习提出了一种通用的5自由度最佳视角预测策略，解决了3D重建中数据收集的自动化问题。

- Motivation: 传统3D重建和新视角合成的数据收集过程耗时且依赖人工，Hestia旨在通过自动化提升效率。
- Method: Hestia通过定义数据集选择、观测设计、动作空间、奖励计算和学习方案等核心组件，结合强化学习实现最佳视角预测，并在真实环境中通过无人机验证。
- Result: 实验表明，Hestia在三个数据集和NVIDIA IsaacLab环境中表现稳健，且适用于实际部署。
- Conclusion: Hestia为自动化数据收集提供了一种系统化、可推广的解决方案，优于传统方法。


### [242] [From Photons to Physics: Autonomous Indoor Drones and the Future of Objective Property Assessment](https://arxiv.org/abs/2508.01965)
*Petteri Teikari,Mike Jarrell,Irene Bandera Moreno,Harri Pesola*

Main category: cs.RO

TL;DR: 综述探讨了自主室内无人机与物理感知技术结合如何推动房产评估从主观视觉检查转向客观定量测量，涵盖平台架构、先进感知、智能自主及工作流整合四大领域。

- Motivation: 通过技术融合提升房产评估的客观性和效率，解决传统主观检查的局限性。
- Method: 分析了四大技术领域：平台架构优化、先进感知模态、智能自主算法及与现有工作流的整合。
- Result: 展示了无人机技术在房产评估中的潜力，包括材料识别、表面表征及信息最大化采集等。
- Conclusion: 自主室内无人机与物理感知技术的结合为房产评估提供了革命性的解决方案，未来需进一步标准化和整合。


### [243] [ScrewSplat: An End-to-End Method for Articulated Object Recognition](https://arxiv.org/abs/2508.02146)
*Seungyeon Kim,Junsu Ha,Young Hun Kim,Yonghyeon Lee,Frank C. Park*

Main category: cs.RO

TL;DR: ScrewSplat是一种基于RGB图像的端到端方法，用于识别物体的几何和运动学关节，无需额外输入或复杂步骤，实现了最先进的识别精度。

- Motivation: 现有方法依赖强假设或额外输入，限制了实际应用。ScrewSplat旨在通过简单方法直接从RGB图像中恢复物体的运动学结构。
- Method: 随机初始化螺旋轴，通过迭代优化恢复运动学结构，结合高斯泼溅同时重建3D几何和分割刚性部件。
- Result: 方法在多样化物体上达到最先进的识别精度，并支持零样本、文本引导的操作。
- Conclusion: ScrewSplat提供了一种简单高效的解决方案，适用于实际场景中的关节物体识别与操作。


### [244] [A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration](https://arxiv.org/abs/2508.02187)
*Xingyi Li,Han Zhang,Ziliang Wang,Yukai Yang,Weidong Chen*

Main category: cs.RO

TL;DR: 提出了一种基于矩匹配的点云配准框架，适用于稀疏和噪声环境，无需显式点对点对应关系，实验证明其优于传统方法。

- Motivation: 在稀疏点和噪声严重的条件下，传统点云配准方法（如ICP和NDT）难以实现鲁棒且准确的配准。
- Method: 将点云视为从同一分布中抽取的独立同分布样本，通过匹配广义高斯径向基矩来估计刚性变换。
- Result: 在合成和真实数据集上，该方法比现有方法具有更高的准确性和鲁棒性，并成功集成到4D雷达SLAM系统中。
- Conclusion: 矩匹配技术在稀疏和噪声场景下具有强大的点云配准潜力，性能接近基于LiDAR的系统。


### [245] [Improving Generalization of Language-Conditioned Robot Manipulation](https://arxiv.org/abs/2508.02405)
*Chenglin Cui,Chaoran Zhu,Changjae Oh,Andrea Cavallaro*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言模型的机器人控制框架，通过少量演示学习物体排列任务，分为目标定位和区域确定两阶段，提升了泛化能力和零样本能力。

- Motivation: 现有方法需要大量数据微调视觉语言模型以适应新环境，而本文旨在通过少量演示实现高效学习。
- Method: 采用两阶段框架（目标定位和区域确定）和实例级语义融合模块，将图像裁剪与文本嵌入对齐。
- Result: 在仿真和真实机器人环境中验证，方法通过少量微调提升了泛化能力，并展示了零样本能力。
- Conclusion: 该框架为机器人控制提供了一种高效且泛化能力强的解决方案。


### [246] [QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots](https://arxiv.org/abs/2508.02512)
*Sheng Wu,Fei Teng,Hao Shi,Qi Jiang,Kai Luo,Kaiwei Wang,Kailun Yang*

Main category: cs.RO

TL;DR: QuaDreamer是一个专为四足机器人设计的全景数据生成引擎，通过模拟四足机器人运动生成可控、逼真的全景视频，解决了高质量训练数据稀缺的问题。

- Motivation: 由于四足机器人的运动特性和传感器校准复杂性，高质量全景训练数据稀缺，限制了其感知系统的发展。
- Method: 提出Vertical Jitter Encoding（VJE）捕捉垂直振动特征，Scene-Object Controller（SOC）管理对象运动和背景抖动，Panoramic Enhancer（PE）解决全景失真问题。
- Result: 生成的视频序列可用于训练四足机器人的全景视觉感知模型，提升360度场景中的多目标跟踪性能。
- Conclusion: QuaDreamer为四足机器人提供了高质量的全景数据生成方案，推动了其感知系统的进步。
## cs.AI

### [247] [SURE-Med: Systematic Uncertainty Reduction for Enhanced Reliability in Medical Report Generation](https://arxiv.org/abs/2508.01693)
*Yuhang Gu,Xingyu Hu,Yuyu Fan,Xulin Yan,Longhuan Xu,Peng peng*

Main category: cs.AI

TL;DR: SURE-Med是一个统一框架，通过视觉、分布和上下文三个维度减少不确定性，提升医学报告生成的可靠性。

- Motivation: 解决医学报告生成中的视觉、标签分布和上下文不确定性，提高临床可信度。
- Method: 使用Frontal-Aware View Repair Resampling模块纠正视觉错误，Token Sensitive Learning目标优化标签分布，Contextual Evidence Filter验证上下文信息。
- Result: 在MIMIC-CXR和IU-Xray基准测试中表现优异，达到最新技术水平。
- Conclusion: SURE-Med通过多维度减少不确定性，为医学报告生成设定了新的可靠性标准。
## physics.comp-ph

### [248] [Point-wise Diffusion Models for Physical Systems with Shape Variations: Application to Spatio-temporal and Large-scale system](https://arxiv.org/abs/2508.01230)
*Jiyong Kim,Sunwoong Yang,Namwoo Kang*

Main category: physics.comp-ph

TL;DR: 提出了一种新颖的点级扩散模型，通过独立处理时空点来高效预测复杂物理系统，显著提升了计算效率和预测精度。

- Motivation: 传统基于图像的扩散模型在处理非结构化数据（如网格和点云）时效率低下，且难以保持几何保真度。本研究旨在解决这一问题。
- Method: 采用点级扩散过程，结合点级扩散变压器架构进行去噪，并使用DDIM实现高效确定性采样，仅需5-10步。
- Result: 模型在训练时间减少94.4%、参数减少89.0%的同时，预测精度提升28%，计算速度提升100-200倍。
- Conclusion: 点级扩散模型在复杂物理系统中表现出优越性能，为实时预测提供了高效解决方案。
## cs.SD

### [249] [Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling](https://arxiv.org/abs/2508.02000)
*Xuanjun Chen,Shih-Peng Cheng,Jiawei Du,Lin Zhang,Xiaoxiao Miao,Chung-Che Wang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 论文提出了一种名为HBMNet的分层边界建模网络，用于解决音频-视觉时间深度伪造定位问题，通过多模块协作提升定位性能。

- Motivation: 在内容驱动的部分操纵下，音频-视觉时间深度伪造定位极具挑战性，因为伪造区域通常仅跨越少数帧，而其余大部分保持原始内容。
- Method: HBMNet包含三个模块：音频-视觉特征编码器提取帧级特征，粗粒度提案生成器预测候选边界区域，细粒度概率生成器通过双向边界-内容概率优化提案。
- Result: 实验表明，编码和融合主要提高精度，帧级监督提升召回率，各模块互补增强定位性能，HBMNet优于BA-TFD和UMMAFormer。
- Conclusion: HBMNet在音频-视觉时间深度伪造定位任务中表现出色，具有潜在的可扩展性。


### [250] [Towards Reliable Audio Deepfake Attribution and Model Recognition: A Multi-Level Autoencoder-Based Framework](https://arxiv.org/abs/2508.02521)
*Andrea Di Pierno,Luca Guarnera,Dario Allegra,Sebastiano Battiato*

Main category: cs.SD

TL;DR: LAVA框架通过分层架构和注意力增强的潜在表示，实现了音频深度伪造检测和模型识别，性能优异且鲁棒性强。

- Motivation: 音频深度伪造的泛滥威胁数字通信信任，现有方法在溯源方面研究不足。
- Method: 使用卷积自编码器提取注意力增强的潜在表示，结合两个分类器（ADA和ADMR）进行技术识别和模型实例识别，并引入置信度阈值提高鲁棒性。
- Result: ADA分类器F1分数超过95%，ADMR模块在六类中达到96.31%宏F1，且在未见攻击中表现稳健。
- Conclusion: LAVA框架在公开基准上验证了其有效性，为深度伪造溯源和模型识别提供了新方法。
## cs.CR

### [251] [CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception](https://arxiv.org/abs/2508.01062)
*Chenyi Wang,Ruoyu Song,Raymond Muller,Jean-Philippe Monteuuis,Z. Berkay Celik,Jonathan Petit,Ryan Gerdes,Ming Li*

Main category: cs.CR

TL;DR: CP-FREEZER是一种针对协同感知系统的延迟攻击，通过V2V消息注入对抗性扰动，显著增加计算延迟，威胁系统可用性。

- Motivation: 研究协同感知系统在对抗性延迟攻击下的鲁棒性，填补了现有研究在攻击时效性方面的空白。
- Method: 提出CP-FREEZER攻击方法，解决点云预处理不可微、传输延迟导致的输入异步等挑战，并使用新型损失函数最大化CP管道执行时间。
- Result: 实验表明，CP-FREEZER能将端到端延迟提升90倍以上，单帧处理时间超过3秒，成功率100%。
- Conclusion: 该攻击揭示了协同感知系统在可用性方面的严重威胁，亟需防御措施。
## eess.IV

### [252] [ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation](https://arxiv.org/abs/2508.01058)
*Sara Yavari,Rahul Nitin Pandya,Jacob Furst*

Main category: eess.IV

TL;DR: 提出了一种半监督的两阶段框架，用于MRI中脑肿瘤的精确分割，无需真实掩码，通过残差引导的扩散模型和轻量级U-Net实现，性能优于基线。

- Motivation: 脑肿瘤的精确分割对临床诊断和治疗规划至关重要，但现有方法在更大、更异构的数据集上表现不足。
- Method: 两阶段框架：1) 残差引导的扩散模型进行跨模态合成；2) 轻量级U-Net结合残差图和多模态输入进行分割。
- Result: 在BraTS 2021数据集上，Dice得分为93.02%，IoU为86.7%，优于基线。
- Conclusion: 该方法在真实世界多中心MRI数据上表现出更高的准确性和可扩展性。


### [253] [Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation](https://arxiv.org/abs/2508.01064)
*Fenghe Tang,Bingkun Nian,Jianrui Ding,Wenxin Ma,Quan Quan,Chengqi Dong,Jie Yang,Wei Liu,S. Kevin Zhou*

Main category: eess.IV

TL;DR: 提出Mobile U-ViT模型，针对医学图像分割任务，结合高效计算与医学图像特性，实现轻量且高性能的网络。

- Motivation: 解决现有移动模型在医学图像任务上表现不佳的问题，因自然图像与医学图像的信息密度差异大。
- Method: 采用ConvUtr作为分层补丁嵌入，引入LGL块平衡局部-全局信息交换，结合轻量级Transformer瓶颈和级联解码器。
- Result: 在八个公开2D和3D数据集上实现最优性能，包括在四个未见数据集上的零样本测试。
- Conclusion: Mobile U-ViT是一种高效、强大且通用的移动医学图像分析解决方案。


### [254] [Diagnostic Accuracy of Open-Source Vision-Language Models on Diverse Medical Imaging Tasks](https://arxiv.org/abs/2508.01016)
*Gustav Müller-Franzes,Debora Jutz,Jakob Nikolas Kather,Christiane Kuhl,Sven Nebelung,Daniel Truhn*

Main category: eess.IV

TL;DR: 该研究评估了五种VLMs在MedFMC数据集上的表现，Qwen2.5在多项任务中表现最佳，但所有模型在复杂领域（如视网膜眼底检查）表现有限。

- Motivation: 评估开源VLMs在医学图像诊断中的准确性和适用性，为临床应用的可行性提供依据。
- Method: 使用MedFMC数据集，比较五种VLMs在三种实验设置（仅视觉输入、多模态输入、思维链推理）下的诊断准确性。
- Result: Qwen2.5在胸部X光和内窥镜图像中表现最佳，但在复杂领域表现不佳；多模态输入和思维链推理未能提升准确性。
- Conclusion: 开源VLMs在部分医学诊断任务中表现良好，但在复杂领域需进一步优化才能用于临床。


### [255] [CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis](https://arxiv.org/abs/2508.01292)
*Alec Sargood,Lemuel Puglisi,James H. Cole,Neil P. Oxtoby,Daniele Ravì,Daniel C. Alexander*

Main category: eess.IV

TL;DR: 该论文提出了一种名为CoCoLIT的扩散潜在生成框架，用于从结构MRI合成淀粉样蛋白PET扫描，以低成本大规模筛查阿尔茨海默病。

- Motivation: 尽管MRI不直接检测淀粉样蛋白病理，但可能包含与淀粉样蛋白沉积相关的信息，通过高级建模可以揭示这些信息。
- Method: CoCoLIT结合了加权图像空间损失（WISL）、潜在平均稳定化（LAS）分析和ControlNet条件化，用于MRI到PET的转换。
- Result: 在公开数据集上，CoCoLIT在图像和淀粉样蛋白相关指标上显著优于现有方法，淀粉样蛋白阳性分类性能提升10.5%和23.7%。
- Conclusion: CoCoLIT是一种高效且成本低廉的阿尔茨海默病筛查工具，代码和模型已开源。


### [256] [SWAN: Synergistic Wavelet-Attention Network for Infrared Small Target Detection](https://arxiv.org/abs/2508.01322)
*Yuxin Jing,Jufeng Zhao,Tianpei Zhang,Yiming Zhu*

Main category: eess.IV

TL;DR: 提出了一种名为SWAN的新框架，结合小波变换和注意力机制，用于红外小目标检测，显著提高了复杂背景下的检测精度和鲁棒性。

- Motivation: 红外小目标检测在民用和军事应用中至关重要，但现有方法依赖传统卷积操作，难以区分目标和复杂背景的频域特征。
- Method: SWAN框架包含Haar小波卷积（HWConv）、移位空间注意力（SSA）和残差双通道注意力（RDCA）模块，实现跨域特征融合和长程空间依赖建模。
- Result: 在基准数据集上的实验表明，SWAN优于现有方法，检测精度和鲁棒性显著提升。
- Conclusion: SWAN通过结合频域和空间域特征，有效解决了复杂背景下的红外小目标检测问题。


### [257] [Classification of Brain Tumors using Hybrid Deep Learning Models](https://arxiv.org/abs/2508.01350)
*Neerav Nemchand Gala*

Main category: eess.IV

TL;DR: 研究通过迁移学习优化CNN在医学图像分类中的性能，比较EfficientNetV2、EfficientNet和ResNet50，发现EfficientNetV2表现最佳但训练时间更长。

- Motivation: 传统CNN在医学图像分类中需要大量计算资源和训练数据，本研究旨在通过迁移学习减少样本需求并提升性能。
- Method: 使用迁移学习，比较EfficientNetV2、EfficientNet和ResNet50在三种脑肿瘤分类中的表现。
- Result: EfficientNetV2表现最优，但训练时间增加。
- Conclusion: EfficientNetV2在性能上优于其他模型，但需权衡训练时间与模型复杂度。


### [258] [Predicting EGFR Mutation in LUAD from Histopathological Whole-Slide Images Using Pretrained Foundation Model and Transfer Learning: An Indian Cohort Study](https://arxiv.org/abs/2508.01352)
*Sagar Singh Gwal,Rajan,Suyash Devgan,Shraddhanjali Satapathy,Abhishek Goyal,Nuruddin Mohammad Iqbal,Vivaan Jain,Prabhat Singh Mallik,Deepali Jain,Ishaan Gupta*

Main category: eess.IV

TL;DR: 该研究提出了一种基于视觉变换器（ViT）和注意力多实例学习（ABMIL）的深度学习框架，用于从H&E染色全切片图像（WSI）预测EGFR突变状态，并在印度和TCGA数据集中表现出色。

- Motivation: 预测EGFR突变状态有助于临床决策，尤其是在东南亚人群中EGFR突变率较高的情况下。
- Method: 使用ViT和ABMIL架构构建深度学习框架，训练于印度队列（170 WSI），并在内部（30 WSI）和外部（TCGA，86 WSI）测试集上评估。
- Result: 模型在内部和外部测试集上的AUC分别为0.933和0.965，表现优于先前研究。
- Conclusion: 该框架可在资源有限的环境中高效预测EGFR突变状态，具有临床潜力。


### [259] [Deeply Supervised Multi-Task Autoencoder for Biological Brain Age estimation using three dimensional T$_1$-weighted magnetic resonance imaging](https://arxiv.org/abs/2508.01565)
*Mehreen Kanwal,Yunsik Son*

Main category: eess.IV

TL;DR: 提出了一种深度监督多任务自编码器（DSMT-AE）框架，用于从3D MRI中准确预测大脑年龄，并通过多任务学习提升性能。

- Motivation: 解决3D模型优化中的梯度消失问题，并利用性别分类等辅助任务提高预测准确性和泛化能力。
- Method: 采用深度监督和多任务学习，同时优化大脑年龄预测、性别分类和图像重建任务。
- Result: 在OpenBHB数据集上实现了最先进的性能，并在不同年龄和性别子组中表现出鲁棒性。
- Conclusion: DSMT-AE框架通过多任务学习和深度监督显著提升了大脑年龄预测的准确性和鲁棒性。


### [260] [Tractography-Guided Dual-Label Collaborative Learning for Multi-Modal Cranial Nerves Parcellation](https://arxiv.org/abs/2508.01577)
*Lei Xie,Junxiong Huang,Yuanjing Feng,Qingrun Zeng*

Main category: eess.IV

TL;DR: 提出了一种基于纤维追踪和专家标注的双标签协作学习网络（DCLNet），用于多模态颅神经分割，通过模态自适应编码模块（MEM）优化MRI和扩散MRI的信息融合，实验验证了其有效性。

- Motivation: 现有多模态融合方法对扩散MRI信息利用不足，导致分割性能较低。
- Method: 提出DCLNet，结合纤维追踪生成的粗标签和专家标注的精确标签进行协作学习，并引入MEM模块优化模态间信息交换。
- Result: 在HCP数据集上的实验表明，DCLNet性能优于单标签网络。
- Conclusion: 双标签策略能有效解决颅神经分割任务中的固有模糊性问题。


### [261] [Measuring and Predicting Where and When Pathologists Focus their Visual Attention while Grading Whole Slide Images of Cancer](https://arxiv.org/abs/2508.01668)
*Souradeep Chakraborty,Ruoyu Xue,Rajarsi Gupta,Oksana Yaskiv,Constantin Friedman,Natallia Sheuka,Dana Perez,Paul Friedman,Won-Tak Choi,Waqas Mahmud,Beatrice Knudsen,Gregory Zelinsky,Joel Saltz,Dimitris Samaras*

Main category: eess.IV

TL;DR: 开发了一种预测病理学家注意力轨迹的方法，通过两阶段模型预测扫描路径，提升病理学培训效果。

- Motivation: 预测专家病理学家的注意力有助于开发决策支持系统，改善病理学培训。
- Method: 使用两阶段模型：第一阶段基于Transformer预测静态注意力热图，第二阶段自回归预测动态扫描路径。
- Result: 模型表现优于随机和基线模型。
- Conclusion: 该模型工具可帮助病理学学员学习如何像专家一样分配注意力。


### [262] [LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation](https://arxiv.org/abs/2508.01772)
*Cristian Minoccheri,Matthew Hodgman,Haoyuan Ma,Rameez Merchant,Emily Wittrup,Craig Williamson,Kayvan Najarian*

Main category: eess.IV

TL;DR: 论文探讨了通过迁移学习和LoRA方法改进动脉瘤性蛛网膜下腔出血（SAH）的医学图像分割，提出了一种新的CP-LoRA方法，并在实验中验证了其有效性。

- Motivation: 动脉瘤性SAH是一种高死亡率的神经系统急症，现有方法在有限数据集上表现不佳，迁移学习和参数高效的LoRA方法在医学图像分割中尚未充分探索。
- Method: 采用预训练的Unet架构，基于124例创伤性脑损伤患者的CT扫描数据，并在30例SAH患者数据上进行微调。提出了CP-LoRA和多种DoRA变体，并与现有LoRA方法和标准微调策略进行比较。
- Result: LoRA方法在SAH分割中表现优于标准微调，CP-LoRA在参数效率上表现突出，且大出血体积的准确性更高。
- Conclusion: 研究表明，跨血肿类型的迁移学习可行，LoRA方法显著优于传统Unet微调，为SAH分割提供了新思路。


### [263] [Joint Lossless Compression and Steganography for Medical Images via Large Language Models](https://arxiv.org/abs/2508.01782)
*Pengcheng Zheng,Xiaorong Pu,Kecheng Chen,Jiaxin Huang,Meng Yang,Bai Feng,Yazhou Ren,Jianan Jiang*

Main category: eess.IV

TL;DR: 提出了一种结合无损压缩和隐写术的新框架，用于医学图像，优化了压缩性能与效率的权衡，并提升了安全性。

- Motivation: 现有基于LLM的医学图像压缩方法在性能与效率之间权衡不足，且忽视压缩过程的安全性。
- Method: 采用比特平面切片（BPS）启发的方法，将图像分为全局和局部模态，设计双路径无损压缩，并在局部路径中嵌入隐写算法，结合A-LoRA微调策略。
- Result: 实验表明，该方法在压缩比、效率和安全性方面表现优越。
- Conclusion: 提出的框架为医学图像无损压缩与安全传输提供了有效解决方案，代码将开源。


### [264] [Large Kernel MedNeXt for Breast Tumor Segmentation and Self-Normalizing Network for pCR Classification in Magnetic Resonance Images](https://arxiv.org/abs/2508.01831)
*Toufiq Musah*

Main category: eess.IV

TL;DR: 论文提出了一种基于大核MedNeXt架构的两阶段训练策略，用于DCE-MRI中的乳腺肿瘤分割和pCR分类，取得了较好的分割和分类性能。

- Motivation: 准确的乳腺肿瘤分割对下游任务（如pCR评估）至关重要，但现有方法在性能和泛化能力上仍有提升空间。
- Method: 采用两阶段训练策略扩展感受野（从3x3x3到5x5x5），结合大核MedNeXt架构和UpKern算法，并利用SNN进行pCR分类。
- Result: 分割任务中Dice得分为0.67，NormHD为0.24；pCR分类的平均平衡准确率为57%，部分子组达75%。
- Conclusion: 大感受野与放射组学驱动的分类结合具有潜力，未来可探索更先进的集成方法和临床变量整合以进一步提升性能。


### [265] [Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation](https://arxiv.org/abs/2508.01941)
*Andrea Dosi,Semanto Mondal,Rajib Chandra Ghosh,Massimo Brescia,Giuseppe Longo*

Main category: eess.IV

TL;DR: 将遥感领域的AMBER模型（基于Transformer）迁移到医疗领域，用于3D医学数据分割，通过AFNO替代多头自注意力机制，显著降低模型复杂度，同时保持高性能。

- Motivation: 探索如何将遥感领域的先进模型迁移到医疗领域，以提升3D医学数据分割的效率和性能。
- Method: 采用AMBER架构，用AFNO替代多头自注意力机制，通过频域混合捕捉全局上下文，减少模型参数和计算复杂度。
- Result: 在ACDC和Synapse数据集上，AMBER-AFNO在DSC和HD指标上表现优异，参数减少80%以上，训练和推理效率显著提升。
- Conclusion: AMBER-AFNO在3D医学数据分割中实现了高效且高性能的解决方案，为模型轻量化提供了新思路。


### [266] [REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation for Interpretable Medical Image Classification](https://arxiv.org/abs/2508.02104)
*Hongzhao Chen,Hexiao Ding,Yufeng Jiang,Jing Lan,Ka Chun Li,Gerald W. Y. Cheng,Sam Ng,Chi Lai Ho,Jing Cai,Liang-ting Lin,Jung Sun Yoo*

Main category: eess.IV

TL;DR: REACT-KD是一个区域感知跨模态拓扑知识蒸馏框架，通过多模态数据指导轻量级CT模型，提升肝癌分类的可靠性和可解释性。

- Motivation: 解决临床影像中肿瘤分类的可靠性差、可解释性低的问题，由于模态质量不均、标注有限和缺乏结构化解剖指导。
- Method: 采用双教师设计，分别从双示踪PET/CT和低剂量CT数据中提取知识，通过语义对齐和区域图蒸馏指导学生模型，并引入模态丢弃提升鲁棒性。
- Result: 在内部PET/CT队列中平均AUC为93.4%，在外部CT测试中AUC维持在76.6%至81.5%，决策曲线分析显示其临床效益显著。
- Conclusion: REACT-KD在肝癌分期任务中表现出色，具有实际诊断潜力，代码已开源。


### [267] [Tackling Ill-posedness of Reversible Image Conversion with Well-posed Invertible Network](https://arxiv.org/abs/2508.02111)
*Yuanfei Huang,Hua Huang*

Main category: eess.IV

TL;DR: 论文提出了一种解决可逆图像转换（RIC）中病态问题的方法，通过构建一个过定系统并设计一种新的可逆卷积（WIC），消除了对随机变量的依赖，实现了病态问题的解决。

- Motivation: 解决可逆图像转换中的病态问题，现有方法因依赖随机变量采样而无法完全解决这一问题。
- Method: 提出了一种基于过定系统的可逆1×1卷积（WIC），并设计了两种新型网络（WIN-Naïve和WIN），后者通过高级跳跃连接增强长期记忆。
- Result: 在多种RIC任务中（如图像隐藏、图像缩放和图像去色）均取得了最先进的性能。
- Conclusion: 该方法有效克服了现有RIC解决方案的瓶颈，为该领域设定了新的基准。


### [268] [GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction](https://arxiv.org/abs/2508.02408)
*Yikuang Yuluo,Yue Ma,Kuan Shen,Tongtong Jin,Wang Liao,Yangpu Ma,Fuquan Wang*

Main category: eess.IV

TL;DR: GR-Gaussian是一种基于图的3D高斯泼溅框架，通过去噪点云初始化策略和像素图感知梯度策略，有效减少稀疏视图条件下的针状伪影，提升CT重建精度。

- Motivation: 现有3D高斯泼溅方法在稀疏视图条件下容易产生针状伪影，影响重建精度，因此需要改进。
- Method: 提出GR-Gaussian框架，包含去噪点云初始化策略和像素图感知梯度策略，优化梯度计算和密度表示。
- Result: 在X-3D和真实数据集上，PSNR提升0.67 dB和0.92 dB，SSIM提升0.011和0.021。
- Conclusion: GR-Gaussian在稀疏视图条件下显著提升CT重建精度，具有实际应用价值。


### [269] [Identifying actionable driver mutations in lung cancer using an efficient Asymmetric Transformer Decoder](https://arxiv.org/abs/2508.02431)
*Biagio Brattoli,Jack Shi,Jongchan Park,Taebum Lee,Donggeun Yoo,Sergio Pereira*

Main category: eess.IV

TL;DR: 该研究利用多实例学习（MIL）技术检测非小细胞肺癌（NSCLC）的六种关键驱动突变，并提出一种非对称Transformer解码器模型，显著提高了预测准确性，尤其是对罕见突变的检测。

- Motivation: 尽管基因检测对NSCLC治疗决策至关重要，但其广泛应用受限于可用性和耗时问题。机器学习方法虽提供潜在解决方案，但现有研究多聚焦于少数常见突变，临床价值有限。
- Method: 研究评估了多种MIL技术，并引入一种非对称Transformer解码器模型，通过不同维度的查询和键值提取信息，同时结合组织类型信息以优化分析。
- Result: 该方法平均优于其他MIL模型3%，在罕见突变（如ERBB2和BRAF）预测中提升超过4%，使ML检测更接近实际替代标准基因检测。
- Conclusion: 该研究通过改进MIL技术和引入新模型，显著提升了NSCLC驱动突变的检测能力，尤其是罕见突变，为临床实践提供了更实用的替代方案。


### [270] [From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC](https://arxiv.org/abs/2508.02528)
*Jingsong Liu,Xiaofeng Deng,Han Li,Azar Kazemi,Christian Grashei,Gesa Wilkens,Xin You,Tanja Groll,Nassir Navab,Carolin Mogler,Peter J. Schüffler*

Main category: eess.IV

TL;DR: 提出了一种名为Star-Diff的结构感知染色恢复扩散模型，用于从H&E染色虚拟生成IHC图像，解决了现有方法在图像对齐和结构保持上的问题，并通过新的评价指标SFS验证了其临床诊断一致性。

- Motivation: H&E染色缺乏分子诊断信息，而IHC虽提供关键生物标志物信息但成本高且耗时。虚拟染色技术有望填补这一空白，但面临图像对齐和结构保持的挑战。
- Method: 提出Star-Diff模型，结合残差和噪声生成路径，将虚拟染色任务重新定义为图像恢复任务，并引入SFS指标评估诊断一致性。
- Result: 在BCI数据集上，Star-Diff在视觉保真度和诊断相关性上均达到SOTA性能，且推理速度快，适用于临床。
- Conclusion: Star-Diff为虚拟IHC合成提供了一种实用解决方案，尤其在术中应用场景中具有潜力。


### [271] [RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation](https://arxiv.org/abs/2508.02557)
*Jierui Qu,Jianchun Zhao*

Main category: eess.IV

TL;DR: 论文提出了一种基于强化学习的双分支U-Net架构（RL-U²Net），用于多模态3D全心脏分割，解决了现有方法的空间不一致性和静态融合策略问题。

- Motivation: 多模态心脏分割在心血管疾病诊断中至关重要，但现有方法存在空间不一致、融合策略静态且效率低的问题。
- Method: 采用双分支U-Net并行处理CT和MRI数据，引入RL-XAlign模块通过强化学习优化特征对齐，最后通过集成学习生成分割结果。
- Result: 在MM-WHS 2017数据集上，CT和MRI的Dice系数分别达到93.1%和87.0%，优于现有方法。
- Conclusion: RL-U²Net在多模态心脏分割中表现出高效性和优越性，为精确诊断提供了新工具。
## cs.LG

### [272] [FRAM: Frobenius-Regularized Assignment Matching with Mixed-Precision Computing](https://arxiv.org/abs/2508.00887)
*Binrui Shen,Yuan Liang,Shengxin Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种新的图匹配松弛框架（FRA），通过Frobenius正则化线性分配问题减少误差，并开发了SDSN算法和混合精度架构，显著提升了性能。

- Motivation: 现有方法在解决NP难的QAP问题时，通过投影松弛会引入数值尺度敏感性和几何不对齐的误差，需要一种更有效的方法来缓解这些问题。
- Method: 提出FRA框架，将投影步骤重新表述为Frobenius正则化线性分配问题，并开发SDSN算法和混合精度架构。
- Result: FRAM在CPU基准测试中优于基线方法，结合GPU混合精度架构可实现370倍加速，且精度损失可忽略。
- Conclusion: FRA框架和SDSN算法有效解决了现有松弛方法的误差问题，混合精度架构进一步提升了计算效率。


### [273] [IMU: Influence-guided Machine Unlearning](https://arxiv.org/abs/2508.01620)
*Xindi Fan,Jing Wu,Mingyi Zhou,Pengwei Liang,Dinh Phung*

Main category: cs.LG

TL;DR: 论文提出了一种名为IMU的机器遗忘方法，仅需遗忘集即可实现选择性遗忘，无需保留集数据，显著提升了遗忘效果和模型实用性。

- Motivation: 深度学习模型易受攻击且会记忆训练数据，引发隐私泄露问题，现有机器遗忘方法多需保留集数据，不切实际。
- Method: 提出IMU方法，利用梯度上升和动态分配遗忘强度，仅需遗忘集数据。
- Result: 在视觉和语言任务中，IMU表现优于现有无需保留集数据的机器遗忘方法。
- Conclusion: IMU是一种简单有效的机器遗忘方法，解决了现有方法依赖保留集数据的问题。


### [274] [Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via Adaptive Vicinity and Auxiliary Regularization](https://arxiv.org/abs/2508.01725)
*Xin Ding,Yun Chen,Yongwei Wang,Kao Zhang,Sen Zhang,Peibei Cao,Xiangxue Wang*

Main category: cs.LG

TL;DR: CcGAN-AVAR是一种改进的CcGAN框架，解决了数据不平衡和计算效率问题，通过自适应邻域机制和多任务判别器提升生成质量，同时保持高效采样。

- Motivation: 现有方法（如CcGAN和CCDM）在数据不平衡和计算效率方面存在局限性，CcGAN-AVAR旨在解决这些问题。
- Method: CcGAN-AVAR结合了GAN的一步生成优势，引入自适应邻域机制和多任务判别器（通过辅助回归和密度比估计）优化训练。
- Result: 在四个基准数据集上的实验表明，CcGAN-AVAR在生成质量和采样效率上均达到最优。
- Conclusion: CcGAN-AVAR通过创新设计解决了现有方法的不足，为条件生成建模提供了高效且高质量的解决方案。


### [275] [OccamVTS: Distilling Vision Models to 1% Parameters for Time Series Forecasting](https://arxiv.org/abs/2508.01727)
*Sisuo Lyu,Siru Zhong,Weilin Ruan,Qingxiang Liu,Qingsong Wen,Hui Xiong,Yuxuan Liang*

Main category: cs.LG

TL;DR: OccamVTS通过知识蒸馏从大型视觉模型中提取仅1%的关键参数，显著提升时间序列预测性能，同时减少过拟合。

- Motivation: 现有方法利用大型视觉模型（LVMs）进行时间序列预测，但99%的参数对任务无用，甚至可能因高维语义特征损害准确性。
- Method: 提出OccamVTS框架，通过金字塔式特征对齐和相关性与特征蒸馏，从LVMs中提取1%的关键信息到轻量网络中。
- Result: 在多个基准数据集上，OccamVTS以仅1%的参数实现最优性能，尤其在少样本和零样本场景中表现突出。
- Conclusion: OccamVTS通过极简参数设计，有效过滤语义噪声并保留关键时间模式，显著提升预测准确性。


### [276] [$ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise](https://arxiv.org/abs/2508.02387)
*Jialiang Wang,Xiong Zhou,Deming Zhai,Junjun Jiang,Xiangyang Ji,Xianming Liu*

Main category: cs.LG

TL;DR: Error

- Motivation: Error
- Method: Error
- Result: Error
- Conclusion: Error


### [277] [Clinical Expert Uncertainty Guided Generalized Label Smoothing for Medical Noisy Label Learning](https://arxiv.org/abs/2508.02495)
*Kunyu Zhang,Lin Gu,Liangchen Liu,Yingke Chen,Bingyang Wang,Jin Yan,Yingying Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种考虑临床专家不确定性的标签平滑方法，以解决医学图像分析中标签噪声问题。

- Motivation: 现有方法忽略了临床专家在诊断时的不确定性（如“可能”或“未排除”），导致标签噪声，影响数据集质量。
- Method: 首先分析临床专家不确定性对标签噪声的影响，然后提出一种不确定性感知的标签平滑方法。
- Result: 提出的方法显著优于现有技术。
- Conclusion: 通过考虑专家不确定性，新方法有效减少了标签噪声，提升了医学图像分析的性能。


### [278] [Explainable AI Methods for Neuroimaging: Systematic Failures of Common Tools, the Need for Domain-Specific Validation, and a Proposal for Safe Application](https://arxiv.org/abs/2508.02560)
*Nys Tjade Siegel,James H. Cole,Mohamad Habes,Stefan Haufe,Kerstin Ritter,Marc-André Schulz*

Main category: cs.LG

TL;DR: 论文通过大规模系统比较XAI方法在神经影像数据上的表现，揭示了常用方法（如GradCAM和Layer-wise Relevance Propagation）的局限性，并推荐了更稳健的SmoothGrad方法。

- Motivation: 验证XAI方法在神经影像领域的可靠性，避免因方法不适用导致的错误解释。
- Method: 构建包含已知信号源的预测任务，使用新颖的XAI验证框架对45,000张结构脑MRI进行系统比较。
- Result: GradCAM无法准确定位预测特征，Layer-wise Relevance Propagation产生大量虚假解释，而SmoothGrad表现稳健。
- Conclusion: XAI方法需针对神经影像数据特性进行适配和验证，现有研究结果需重新评估，推荐使用SmoothGrad。
