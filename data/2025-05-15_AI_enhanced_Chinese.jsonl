{"id": "2505.08800", "pdf": "https://arxiv.org/pdf/2505.08800", "abs": "https://arxiv.org/abs/2505.08800", "authors": ["Olivia Nocentini", "Marta Lagomarsino", "Gokhan Solak", "Younggeol Cho", "Qiyi Tong", "Marta Lorenzini", "Arash Ajoudani"], "title": "Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Driver fatigue poses a significant challenge to railway safety, with\ntraditional systems like the dead-man switch offering limited and basic\nalertness checks. This study presents an online behavior-based monitoring\nsystem utilizing a customised Directed-Graph Neural Network (DGNN) to classify\ntrain driver's states into three categories: alert, not alert, and\npathological. To optimize input representations for the model, an ablation\nstudy was performed, comparing three feature configurations: skeletal-only,\nfacial-only, and a combination of both. Experimental results show that\ncombining facial and skeletal features yields the highest accuracy (80.88%) in\nthe three-class model, outperforming models using only facial or skeletal\nfeatures. Furthermore, this combination achieves over 99% accuracy in the\nbinary alertness classification. Additionally, we introduced a novel dataset\nthat, for the first time, incorporates simulated pathological conditions into\ntrain driver monitoring, broadening the scope for assessing risks related to\nfatigue and health. This work represents a step forward in enhancing railway\nsafety through advanced online monitoring using vision-based technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u5206\u6790\u7684\u5728\u7ebf\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5229\u7528\u5b9a\u5236\u5316DGNN\u5bf9\u706b\u8f66\u53f8\u673a\u72b6\u6001\u8fdb\u884c\u5206\u7c7b\uff0c\u7ed3\u5408\u9762\u90e8\u548c\u9aa8\u9abc\u7279\u5f81\u5b9e\u73b0\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7cfb\u7edf\uff08\u5982\u6b7b\u673a\u5f00\u5173\uff09\u5728\u94c1\u8def\u5b89\u5168\u4e2d\u4ec5\u63d0\u4f9b\u6709\u9650\u8b66\u89c9\u6027\u68c0\u67e5\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u7684DGNN\u6a21\u578b\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u6bd4\u8f83\u4e09\u79cd\u7279\u5f81\u914d\u7f6e\uff08\u9aa8\u9abc\u3001\u9762\u90e8\u53ca\u4e24\u8005\u7ed3\u5408\uff09\u4f18\u5316\u8f93\u5165\u8868\u793a\u3002", "result": "\u7ed3\u5408\u9762\u90e8\u548c\u9aa8\u9abc\u7279\u5f81\u7684\u6a21\u578b\u5728\u4e09\u5206\u7c7b\u4e2d\u51c6\u786e\u7387\u8fbe80.88%\uff0c\u4e8c\u5206\u7c7b\u8b66\u89c9\u6027\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc799%\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u89c6\u89c9\u6280\u672f\u63d0\u5347\u4e86\u94c1\u8def\u5b89\u5168\u7684\u5728\u7ebf\u76d1\u6d4b\u80fd\u529b\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u6a21\u62df\u75c5\u7406\u72b6\u6001\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2505.08801", "pdf": "https://arxiv.org/pdf/2505.08801", "abs": "https://arxiv.org/abs/2505.08801", "authors": ["Md. Sakib Hassan Chowdhury", "Md. Hafiz Ahamed", "Bishowjit Paul", "Sarafat Hussain Abhi", "Abu Bakar Siddique", "Md. Robius Sany"], "title": "OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "12 pages, 17 figures", "summary": "Gait recognition, known for its ability to identify individuals from a\ndistance, has gained significant attention in recent times due to its\nnon-intrusive verification. While video-based gait identification systems\nperform well on large public datasets, their performance drops when applied to\nreal-world, unconstrained gait data due to various factors. Among these,\nuncontrolled outdoor environments, non-overlapping camera views, varying\nillumination, and computational efficiency are core challenges in gait-based\nauthentication. Currently, no dataset addresses all these challenges\nsimultaneously. In this paper, we propose an OptiGait-LGBM model capable of\nrecognizing person re-identification under these constraints using a skeletal\nmodel approach, which helps mitigate inconsistencies in a person's appearance.\nThe model constructs a dataset from landmark positions, minimizing memory usage\nby using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to\nrepresent uncontrolled gait sequences in complex outdoor environments. The\nprocess involves extracting skeletal joint landmarks, generating numerical\ndatasets, and developing an OptiGait-LGBM gait classification model. Our aim is\nto address the aforementioned challenges with minimal computational cost\ncompared to existing methods. A comparative analysis with ensemble techniques\nsuch as Random Forest and CatBoost demonstrates that the proposed approach\noutperforms them in terms of accuracy, memory usage, and training time. This\nmethod provides a novel, low-cost, and memory-efficient video-based gait\nrecognition solution for real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9aa8\u9abc\u6a21\u578b\u7684OptiGait-LGBM\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u6b65\u6001\u8bc6\u522b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u5728\u975e\u7ea6\u675f\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u7f3a\u4e4f\u540c\u65f6\u89e3\u51b3\u6237\u5916\u73af\u5883\u3001\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\u7684\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u9aa8\u9abc\u6a21\u578b\u63d0\u53d6\u5173\u952e\u70b9\uff0c\u6784\u5efa\u975e\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u5f00\u53d1OptiGait-LGBM\u5206\u7c7b\u6a21\u578b\uff0c\u5e76\u5f15\u5165RUET-GAIT\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "OptiGait-LGBM\u5728\u51c6\u786e\u6027\u3001\u5185\u5b58\u4f7f\u7528\u548c\u8bad\u7ec3\u65f6\u95f4\u4e0a\u4f18\u4e8e\u968f\u673a\u68ee\u6797\u548cCatBoost\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u5185\u5b58\u6b65\u6001\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08808", "pdf": "https://arxiv.org/pdf/2505.08808", "abs": "https://arxiv.org/abs/2505.08808", "authors": ["Anqing Jiang", "Jinhao Chai", "Yu Gao", "Yiru Wang", "Yuwen Heng", "Zhigang Sun", "Hao Sun", "Zezhong Zhao", "Li Sun", "Jian Zhou", "Lijuan Zhu", "Shugong Xu", "Hao Zhao"], "title": "SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in high-definition \\emph{HD} map construction have\ndemonstrated the effectiveness of dense representations, which heavily rely on\ncomputationally intensive bird's-eye view \\emph{BEV} features. While sparse\nrepresentations offer a more efficient alternative by avoiding dense BEV\nprocessing, existing methods often lag behind due to the lack of tailored\ndesigns. These limitations have hindered the competitiveness of sparse\nrepresentations in online HD map construction. In this work, we systematically\nrevisit and enhance sparse representation techniques, identifying key\narchitectural and algorithmic improvements that bridge the gap with--and\nultimately surpass--dense approaches. We introduce a dedicated network\narchitecture optimized for sparse map feature extraction, a sparse-dense\nsegmentation auxiliary task to better leverage geometric and semantic cues, and\na denoising module guided by physical priors to refine predictions. Through\nthese enhancements, our method achieves state-of-the-art performance on the\nnuScenes dataset, significantly advancing HD map construction and centerline\ndetection. Specifically, SparseMeXt-Tiny reaches a mean average precision\n\\emph{mAP} of 55.5% at 32 frames per second \\emph{fps}, while SparseMeXt-Base\nattains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large\nachieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for\nsparse representations in HD map construction. These results underscore the\nuntapped potential of sparse methods, challenging the conventional reliance on\ndense representations and redefining efficiency-performance trade-offs in the\nfield.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u7a00\u758f\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e13\u7528\u7f51\u7edc\u67b6\u6784\u3001\u7a00\u758f-\u5bc6\u96c6\u5206\u5272\u8f85\u52a9\u4efb\u52a1\u548c\u7269\u7406\u5148\u9a8c\u53bb\u566a\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u7a00\u758f\u8868\u793a\u5728\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u4e2d\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u9488\u5bf9\u6027\u8bbe\u8ba1\u800c\u6027\u80fd\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u6027\u6539\u8fdb\uff0c\u4f7f\u7a00\u758f\u65b9\u6cd5\u8d85\u8d8a\u5bc6\u96c6\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e13\u7528\u7f51\u7edc\u67b6\u6784\u7528\u4e8e\u7a00\u758f\u7279\u5f81\u63d0\u53d6\uff0c\u5f15\u5165\u7a00\u758f-\u5bc6\u96c6\u5206\u5272\u8f85\u52a9\u4efb\u52a1\u4ee5\u5229\u7528\u51e0\u4f55\u548c\u8bed\u4e49\u7ebf\u7d22\uff0c\u5e76\u91c7\u7528\u7269\u7406\u5148\u9a8c\u53bb\u566a\u6a21\u5757\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0cSparseMeXt-Tiny\uff0855.5% mAP, 32 fps\uff09\u3001SparseMeXt-Base\uff0865.2% mAP\uff09\u548cSparseMeXt-Large\uff0868.9% mAP, 20 fps\uff09\u3002", "conclusion": "\u7a00\u758f\u65b9\u6cd5\u5728\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u6311\u6218\u4e86\u5bc6\u96c6\u8868\u793a\u7684\u4f20\u7edf\u4f9d\u8d56\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6743\u8861\u3002"}}
{"id": "2505.08811", "pdf": "https://arxiv.org/pdf/2505.08811", "abs": "https://arxiv.org/abs/2505.08811", "authors": ["Shijie Lian", "Ziyi Zhang", "Laurence Tianruo Yang and", "Mengyu Ren", "Debin Liu", "Hua Li"], "title": "TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Underwater 3D scene reconstruction is crucial for undewater robotic\nperception and navigation. However, the task is significantly challenged by the\ncomplex interplay between light propagation, water medium, and object surfaces,\nwith existing methods unable to model their interactions accurately.\nAdditionally, expensive training and rendering costs limit their practical\napplication in underwater robotic systems. Therefore, we propose Tensorized\nUnderwater Gaussian Splatting (TUGS), which can effectively solve the modeling\nchallenges of the complex interactions between object geometries and water\nmedia while achieving significant parameter reduction. TUGS employs lightweight\ntensorized higher-order Gaussians with a physics-based underwater Adaptive\nMedium Estimation (AME) module, enabling accurate simulation of both light\nattenuation and backscatter effects in underwater environments. Compared to\nother NeRF-based and GS-based methods designed for underwater, TUGS is able to\nrender high-quality underwater images with faster rendering speeds and less\nmemory usage. Extensive experiments on real-world underwater datasets have\ndemonstrated that TUGS can efficiently achieve superior reconstruction quality\nusing a limited number of parameters, making it particularly suitable for\nmemory-constrained underwater UAV applications", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTUGS\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u89e3\u51b3\u6c34\u4e0b3D\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u590d\u6742\u5149\u4f20\u64ad\u4e0e\u7269\u4f53\u8868\u9762\u4ea4\u4e92\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u6c34\u4e0b3D\u573a\u666f\u91cd\u5efa\u5bf9\u673a\u5668\u4eba\u611f\u77e5\u548c\u5bfc\u822a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u5149\u4f20\u64ad\u3001\u6c34\u4f53\u4ecb\u8d28\u4e0e\u7269\u4f53\u8868\u9762\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "TUGS\u91c7\u7528\u8f7b\u91cf\u5316\u7684\u5f20\u91cf\u5316\u9ad8\u9636\u9ad8\u65af\u6a21\u578b\u548c\u57fa\u4e8e\u7269\u7406\u7684\u81ea\u9002\u5e94\u6c34\u4f53\u4f30\u8ba1\u6a21\u5757\uff08AME\uff09\uff0c\u6a21\u62df\u6c34\u4e0b\u5149\u8870\u51cf\u548c\u80cc\u6563\u5c04\u6548\u5e94\u3002", "result": "TUGS\u5728\u771f\u5b9e\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u4ee5\u66f4\u5feb\u7684\u6e32\u67d3\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u5360\u7528\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "conclusion": "TUGS\u901a\u8fc7\u9ad8\u6548\u53c2\u6570\u5316\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u7279\u522b\u9002\u5408\u5185\u5b58\u53d7\u9650\u7684\u6c34\u4e0b\u65e0\u4eba\u673a\u5e94\u7528\u3002"}}
{"id": "2505.08814", "pdf": "https://arxiv.org/pdf/2505.08814", "abs": "https://arxiv.org/abs/2505.08814", "authors": ["Wenkai Li", "Xiaoqi Li", "Yingjie Mao", "Yishun Wang"], "title": "Towards Understanding Deep Learning Model in Image Recognition via Coverage Test", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) play a crucial role in the field of artificial\nintelligence, and their security-related testing has been a prominent research\nfocus. By inputting test cases, the behavior of models is examined for\nanomalies, and coverage metrics are utilized to determine the extent of neurons\ncovered by these test cases. With the widespread application and advancement of\nDNNs, different types of neural behaviors have garnered attention, leading to\nthe emergence of various coverage metrics for neural networks. However, there\nis currently a lack of empirical research on these coverage metrics,\nspecifically in analyzing the relationships and patterns between model depth,\nconfiguration information, and neural network coverage. This paper aims to\ninvestigate the relationships and patterns of four coverage metrics: primary\nfunctionality, boundary, hierarchy, and structural coverage. A series of\nempirical experiments were conducted, selecting LeNet, VGG, and ResNet as\ndifferent DNN architectures, along with 10 models of varying depths ranging\nfrom 5 to 54 layers, to compare and study the relationships between different\ndepths, configuration information, and various neural network coverage metrics.\nAdditionally, an investigation was carried out on the relationships between\nmodified decision/condition coverage and dataset size. Finally, three potential\nfuture directions are proposed to further contribute to the security testing of\nDNN Models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4e2d\u56db\u79cd\u8986\u76d6\u5ea6\u6307\u6807\u7684\u5173\u7cfb\u4e0e\u6a21\u5f0f\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u4e0e\u8986\u76d6\u5ea6\u7684\u5173\u8054\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u8986\u76d6\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740DNN\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6d4b\u8bd5\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u8986\u76d6\u5ea6\u6307\u6807\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5c24\u5176\u662f\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u4e0e\u8986\u76d6\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u9009\u62e9LeNet\u3001VGG\u548cResNet\u4e09\u79cd\u67b6\u6784\u53ca10\u79cd\u4e0d\u540c\u6df1\u5ea6\u7684\u6a21\u578b\uff085\u81f354\u5c42\uff09\uff0c\u6bd4\u8f83\u5206\u6790\u4e86\u56db\u79cd\u8986\u76d6\u5ea6\u6307\u6807\uff08\u4e3b\u8981\u529f\u80fd\u3001\u8fb9\u754c\u3001\u5c42\u6b21\u548c\u7ed3\u6784\u8986\u76d6\uff09\u7684\u5173\u7cfb\uff0c\u5e76\u7814\u7a76\u4e86\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u8986\u76d6\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u6a21\u578b\u6df1\u5ea6\u3001\u914d\u7f6e\u4fe1\u606f\u4e0e\u8986\u76d6\u5ea6\u6307\u6807\u4e4b\u95f4\u7684\u5177\u4f53\u5173\u7cfb\uff0c\u5e76\u53d1\u73b0\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u8986\u76d6\u5ea6\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e3aDNN\u5b89\u5168\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.08817", "pdf": "https://arxiv.org/pdf/2505.08817", "abs": "https://arxiv.org/abs/2505.08817", "authors": ["Camilo Carvajal Reyes", "Joaqu\u00edn Fontbona", "Felipe Tobar"], "title": "Towards SFW sampling for diffusion models via external conditioning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepcted at IJCNN 2025", "summary": "Score-based generative models (SBM), also known as diffusion models, are the\nde facto state of the art for image synthesis. Despite their unparalleled\nperformance, SBMs have recently been in the spotlight for being tricked into\ncreating not-safe-for-work (NSFW) content, such as violent images and\nnon-consensual nudity. Current approaches that prevent unsafe generation are\nbased on the models' own knowledge, and the majority of them require\nfine-tuning. This article explores the use of external sources for ensuring\nsafe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional\nTrajectory Correction step that guides the samples away from undesired regions\nin the ambient space using multimodal models as the source of conditioning.\nFurthermore, using Contrastive Language Image Pre-training (CLIP), our method\nadmits user-defined NSFW classes, which can vary in different settings. Our\nexperiments on the text-to-image SBM Stable Diffusion validate that the\nproposed SFW sampler effectively reduces the generation of explicit content\nwhile being competitive with other fine-tuning-based approaches, as assessed\nvia independent NSFW detectors. Moreover, we evaluate the impact of the SFW\nsampler on image quality and show that the proposed correction scheme comes at\na minor cost with negligible effect on samples not needing correction. Our\nstudy confirms the suitability of the SFW sampler towards aligned SBM models\nand the potential of using model-agnostic conditioning for the prevention of\nunwanted images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5916\u90e8\u591a\u6a21\u6001\u6a21\u578b\u7684SFW\u91c7\u6837\u5668\uff0c\u7528\u4e8e\u9632\u6b62\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\uff08SBM\uff09\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u5982\u66b4\u529b\u6216\u975e\u81ea\u613f\u88f8\u9732\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6761\u4ef6\u8f68\u8ff9\u6821\u6b63\u6b65\u9aa4\u5f15\u5bfc\u6837\u672c\u8fdc\u79bb\u4e0d\u671f\u671b\u533a\u57df\uff0c\u5e76\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49NSFW\u7c7b\u522b\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e14\u5bf9\u56fe\u50cf\u8d28\u91cf\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u5c3d\u7ba1SBM\u5728\u56fe\u50cf\u5408\u6210\u9886\u57df\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u5b58\u5728\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\u7684\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8e\u6a21\u578b\u81ea\u8eab\u77e5\u8bc6\u6216\u5fae\u8c03\uff0c\u672c\u6587\u63a2\u7d22\u5229\u7528\u5916\u90e8\u591a\u6a21\u6001\u6a21\u578b\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faSFW\u91c7\u6837\u5668\uff0c\u7ed3\u5408\u6761\u4ef6\u8f68\u8ff9\u6821\u6b63\u6b65\u9aa4\u548c\u591a\u6a21\u6001\u6a21\u578b\uff08\u5982CLIP\uff09\u4f5c\u4e3a\u6761\u4ef6\u6e90\uff0c\u5f15\u5bfc\u6837\u672c\u8fdc\u79bb\u4e0d\u671f\u671b\u533a\u57df\uff0c\u5e76\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49NSFW\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSFW\u91c7\u6837\u5668\u6709\u6548\u51cf\u5c11\u4e0d\u5b89\u5168\u5185\u5bb9\u7684\u751f\u6210\uff0c\u4e14\u5bf9\u56fe\u50cf\u8d28\u91cf\u5f71\u54cd\u8f83\u5c0f\u3002\u5728Stable Diffusion\u4e0a\u7684\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u4e0e\u5fae\u8c03\u65b9\u6cd5\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "SFW\u91c7\u6837\u5668\u9002\u7528\u4e8e\u5bf9\u9f50\u7684SBM\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5229\u7528\u6a21\u578b\u65e0\u5173\u6761\u4ef6\u9632\u6b62\u4e0d\u826f\u56fe\u50cf\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.08833", "pdf": "https://arxiv.org/pdf/2505.08833", "abs": "https://arxiv.org/abs/2505.08833", "authors": ["Qingyi Wang", "Yuebing Liang", "Yunhan Zheng", "Kaiyuan Xu", "Jinhua Zhao", "Shenhao Wang"], "title": "Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Generative AI offers new opportunities for automating urban planning by\ncreating site-specific urban layouts and enabling flexible design exploration.\nHowever, existing approaches often struggle to produce realistic and practical\ndesigns at scale. Therefore, we adapt a state-of-the-art Stable Diffusion\nmodel, extended with ControlNet, to generate high-fidelity satellite imagery\nconditioned on land use descriptions, infrastructure, and natural environments.\nTo overcome data availability limitations, we spatially link satellite imagery\nwith structured land use and constraint information from OpenStreetMap. Using\ndata from three major U.S. cities, we demonstrate that the proposed diffusion\nmodel generates realistic and diverse urban landscapes by varying land-use\nconfigurations, road networks, and water bodies, facilitating cross-city\nlearning and design diversity. We also systematically evaluate the impacts of\nvarying language prompts and control imagery on the quality of satellite\nimagery generation. Our model achieves high FID and KID scores and demonstrates\nrobustness across diverse urban contexts. Qualitative assessments from urban\nplanners and the general public show that generated images align closely with\ndesign descriptions and constraints, and are often preferred over real images.\nThis work establishes a benchmark for controlled urban imagery generation and\nhighlights the potential of generative AI as a tool for enhancing planning\nworkflows and public engagement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStable Diffusion\u548cControlNet\u7684\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u536b\u661f\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u571f\u5730\u5229\u7528\u63cf\u8ff0\u548c\u57fa\u7840\u8bbe\u65bd\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u5927\u89c4\u6a21\u5b9e\u7528\u8bbe\u8ba1\u7684\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u5f0fAI\u4e3a\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u5e03\u5c40\u548c\u7075\u6d3b\u8bbe\u8ba1\u63a2\u7d22\u7684\u65b0\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u5927\u89c4\u6a21\u4e14\u5b9e\u7528\u7684\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528Stable Diffusion\u6a21\u578b\uff0c\u7ed3\u5408ControlNet\uff0c\u5229\u7528OpenStreetMap\u7684\u571f\u5730\u5229\u7528\u548c\u7ea6\u675f\u4fe1\u606f\u751f\u6210\u536b\u661f\u56fe\u50cf\u3002", "result": "\u6a21\u578b\u5728\u4e09\u4e2a\u7f8e\u56fd\u57ce\u5e02\u7684\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u57ce\u5e02\u666f\u89c2\uff0cFID\u548cKID\u5f97\u5206\u9ad8\uff0c\u4e14\u7528\u6237\u8bc4\u4f30\u663e\u793a\u751f\u6210\u56fe\u50cf\u4f18\u4e8e\u771f\u5b9e\u56fe\u50cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u63a7\u57ce\u5e02\u56fe\u50cf\u751f\u6210\u8bbe\u5b9a\u4e86\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u63d0\u5347\u89c4\u5212\u6d41\u7a0b\u548c\u516c\u4f17\u53c2\u4e0e\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.08834", "pdf": "https://arxiv.org/pdf/2505.08834", "abs": "https://arxiv.org/abs/2505.08834", "authors": ["Muhammad Junaid Asif"], "title": "Crowd Scene Analysis using Deep Learning Techniques", "categories": ["cs.CV", "cs.AI"], "comment": "MS Graduate Research Thesis", "summary": "Our research is focused on two main applications of crowd scene analysis\ncrowd counting and anomaly detection In recent years a large number of\nresearches have been presented in the domain of crowd counting We addressed two\nmain challenges in this domain 1 Deep learning models are datahungry paradigms\nand always need a large amount of annotated data for the training of algorithm\nIt is timeconsuming and costly task to annotate such large amount of data\nSelfsupervised training is proposed to deal with this challenge 2 MCNN consists\nof multicolumns of CNN with different sizes of filters by presenting a novel\napproach based on a combination of selfsupervised training and MultiColumn CNN\nThis enables the model to learn features at different levels and makes it\neffective in dealing with challenges of occluded scenes nonuniform density\ncomplex backgrounds and scale invariation The proposed model was evaluated on\npublicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE\nand MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly\ndetection addressing challenges like lighting environmental conditions\nunexpected objects and scalability The model extracts spatial and temporal\nfeatures allowing it to be generalized to realworld scenes Spatial features are\nlearned using CNN while temporal features are learned using LSTM blocks The\nmodel works on binary classification and can detect normal or abnormal behavior\nThe models performance is improved by replacing fully connected layers with\ndense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset\nshow our models outperform other stateoftheart approaches", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u591a\u5217CNN\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u6807\u6ce8\u6570\u636e\u9700\u6c42\u9ad8\u548c\u573a\u666f\u590d\u6742\u6027\u95ee\u9898\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVGG19\u7684\u65f6\u7a7a\u6a21\u578b\uff0c\u7528\u4e8e\u4eba\u7fa4\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u4eba\u7fa4\u8ba1\u6570\u4e2d\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u573a\u666f\u590d\u6742\u6027\uff08\u5982\u906e\u6321\u3001\u975e\u5747\u5300\u5bc6\u5ea6\u7b49\uff09\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u4eba\u7fa4\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5149\u7167\u3001\u73af\u5883\u6761\u4ef6\u7b49\u95ee\u9898\u3002", "method": "1. \u4eba\u7fa4\u8ba1\u6570\uff1a\u91c7\u7528\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u591a\u5217CNN\u7ed3\u5408\u7684\u65b9\u6cd5\uff1b2. \u5f02\u5e38\u68c0\u6d4b\uff1a\u57fa\u4e8eVGG19\u7684\u65f6\u7a7a\u6a21\u578b\uff0c\u7ed3\u5408CNN\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\u548cLSTM\u63d0\u53d6\u65f6\u95f4\u7279\u5f81\u3002", "result": "\u5728ShanghaiTech\u548cUCFQNRF\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff08MAE\u548cMSE\u6307\u6807\uff09\uff1b\u5728Hockey Fight\u548cSCVD\u6570\u636e\u96c6\u4e0a\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4eba\u7fa4\u8ba1\u6570\u548c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6807\u6ce8\u548c\u573a\u666f\u590d\u6742\u6027\u95ee\u9898\u3002"}}
{"id": "2505.08854", "pdf": "https://arxiv.org/pdf/2505.08854", "abs": "https://arxiv.org/abs/2505.08854", "authors": ["Yuping Wang", "Shuo Xing", "Cui Can", "Renjie Li", "Hongyuan Hua", "Kexin Tian", "Zhaobin Mo", "Xiangbo Gao", "Keshu Wu", "Sulong Zhou", "Hengxu You", "Juntong Peng", "Junge Zhang", "Zehao Wang", "Rui Song", "Mingxuan Yan", "Walter Zimmer", "Xingcheng Zhou", "Peiran Li", "Zhaohan Lu", "Chia-Ju Chen", "Yue Huang", "Ryan A. Rossi", "Lichao Sun", "Hongkai Yu", "Zhiwen Fan", "Frank Hao Yang", "Yuhao Kang", "Ross Greer", "Chenxi Liu", "Eun Hak Lee", "Xuan Di", "Xinyue Ye", "Liu Ren", "Alois Knoll", "Xiaopeng Li", "Shuiwang Ji", "Masayoshi Tomizuka", "Marco Pavone", "Tianbao Yang", "Jing Du", "Ming-Hsuan Yang", "Hua Wei", "Ziran Wang", "Yang Zhou", "Jiachen Li", "Zhengzhong Tu"], "title": "Generative AI for Autonomous Driving: Frontiers and Opportunities", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) constitutes a transformative\ntechnological wave that reconfigures industries through its unparalleled\ncapabilities for content creation, reasoning, planning, and multimodal\nunderstanding. This revolutionary force offers the most promising path yet\ntoward solving one of engineering's grandest challenges: achieving reliable,\nfully autonomous driving, particularly the pursuit of Level 5 autonomy. This\nsurvey delivers a comprehensive and critical synthesis of the emerging role of\nGenAI across the autonomous driving stack. We begin by distilling the\nprinciples and trade-offs of modern generative modeling, encompassing VAEs,\nGANs, Diffusion Models, and Large Language Models (LLMs). We then map their\nfrontier applications in image, LiDAR, trajectory, occupancy, video generation\nas well as LLM-guided reasoning and decision making. We categorize practical\napplications, such as synthetic data workflows, end-to-end driving strategies,\nhigh-fidelity digital twin systems, smart transportation networks, and\ncross-domain transfer to embodied AI. We identify key obstacles and\npossibilities such as comprehensive generalization across rare cases,\nevaluation and safety checks, budget-limited implementation, regulatory\ncompliance, ethical concerns, and environmental effects, while proposing\nresearch plans across theoretical assurances, trust metrics, transport\nintegration, and socio-technical influence. By unifying these threads, the\nsurvey provides a forward-looking reference for researchers, engineers, and\npolicymakers navigating the convergence of generative AI and advanced\nautonomous mobility. An actively maintained repository of cited works is\navailable at https://github.com/taco-group/GenAI4AD.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u4e86\u751f\u6210\u6a21\u578b\u539f\u7406\u3001\u524d\u6cbf\u5e94\u7528\u53ca\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "GenAI\u56e0\u5176\u5728\u5185\u5bb9\u751f\u6210\u3001\u63a8\u7406\u548c\u89c4\u5212\u65b9\u9762\u7684\u80fd\u529b\uff0c\u88ab\u89c6\u4e3a\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\uff08Level 5\uff09\u7684\u5173\u952e\u6280\u672f\u3002\u672c\u6587\u65e8\u5728\u5168\u9762\u5206\u6790GenAI\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89d2\u8272\u3002", "method": "\u6587\u7ae0\u603b\u7ed3\u4e86\u73b0\u4ee3\u751f\u6210\u6a21\u578b\uff08\u5982VAEs\u3001GANs\u3001\u6269\u6563\u6a21\u578b\u548cLLMs\uff09\u7684\u539f\u7406\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u5728\u56fe\u50cf\u3001LiDAR\u3001\u8f68\u8ff9\u7b49\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "GenAI\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u5305\u62ec\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u7aef\u5230\u7aef\u9a7e\u9a76\u7b56\u7565\u3001\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7b49\uff0c\u4f46\u4e5f\u9762\u4e34\u6cdb\u5316\u80fd\u529b\u3001\u5b89\u5168\u6027\u3001\u4f26\u7406\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u8005\u3001\u5de5\u7a0b\u5e08\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86GenAI\u4e0e\u81ea\u52a8\u9a7e\u9a76\u878d\u5408\u7684\u524d\u77bb\u6027\u53c2\u8003\uff0c\u5e76\u63d0\u51fa\u4e86\u7406\u8bba\u4fdd\u969c\u3001\u4fe1\u4efb\u6307\u6807\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.08882", "pdf": "https://arxiv.org/pdf/2505.08882", "abs": "https://arxiv.org/abs/2505.08882", "authors": ["Ali Almakhluk", "Uthman Baroudi", "Yasser El-Alfy"], "title": "Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety", "categories": ["cs.CV", "cs.SY", "eess.IV", "eess.SY"], "comment": null, "summary": "This study aims to improve transportation safety, especially traffic safety.\nRoad damage anomalies such as potholes and cracks have emerged as a significant\nand recurring cause for accidents. To tackle this problem and improve road\nsafety, a comprehensive system has been developed to detect potholes, cracks\n(e.g. alligator, transverse, longitudinal), classify their sizes, and transmit\nthis data to the cloud for appropriate action by authorities. The system also\nbroadcasts warning signals to nearby vehicles warning them if a severe anomaly\nis detected on the road. Moreover, the system can count road anomalies in\nreal-time. It is emulated through the utilization of Raspberry Pi, a camera\nmodule, deep learning model, laptop, and cloud service. Deploying this\ninnovative solution aims to proactively enhance road safety by notifying\nrelevant authorities and drivers about the presence of potholes and cracks to\ntake actions, thereby mitigating potential accidents arising from this\nprevalent road hazard leading to safer road conditions for the whole community.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6811\u8393\u6d3e\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u548c\u5206\u7c7b\u9053\u8def\u635f\u574f\uff08\u5982\u5751\u6d1e\u548c\u88c2\u7f1d\uff09\uff0c\u5e76\u901a\u8fc7\u4e91\u670d\u52a1\u901a\u77e5\u76f8\u5173\u90e8\u95e8\u548c\u8f66\u8f86\uff0c\u4ee5\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u3002", "motivation": "\u9053\u8def\u635f\u574f\uff08\u5982\u5751\u6d1e\u548c\u88c2\u7f1d\uff09\u662f\u4ea4\u901a\u4e8b\u6545\u7684\u5e38\u89c1\u539f\u56e0\uff0c\u4e9f\u9700\u4e00\u79cd\u4e3b\u52a8\u68c0\u6d4b\u548c\u9884\u8b66\u7cfb\u7edf\u6765\u63d0\u5347\u9053\u8def\u5b89\u5168\u6027\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u6811\u8393\u6d3e\u3001\u6444\u50cf\u5934\u6a21\u5757\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u4e91\u670d\u52a1\uff0c\u5b9e\u65f6\u68c0\u6d4b\u3001\u5206\u7c7b\u9053\u8def\u635f\u574f\u5e76\u4f20\u8f93\u6570\u636e\uff0c\u540c\u65f6\u5411\u9644\u8fd1\u8f66\u8f86\u53d1\u9001\u8b66\u544a\u4fe1\u53f7\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u548c\u5206\u7c7b\u9053\u8def\u635f\u574f\uff0c\u5e76\u901a\u8fc7\u4e91\u670d\u52a1\u548c\u8f66\u8f86\u8b66\u544a\u4fe1\u53f7\u5b9e\u73b0\u5feb\u901f\u54cd\u5e94\uff0c\u51cf\u5c11\u4e8b\u6545\u98ce\u9669\u3002", "conclusion": "\u8be5\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u4e3b\u52a8\u68c0\u6d4b\u548c\u9884\u8b66\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9053\u8def\u5b89\u5168\u6027\uff0c\u4e3a\u793e\u533a\u521b\u9020\u4e86\u66f4\u5b89\u5168\u7684\u4ea4\u901a\u73af\u5883\u3002"}}
{"id": "2505.08886", "pdf": "https://arxiv.org/pdf/2505.08886", "abs": "https://arxiv.org/abs/2505.08886", "authors": ["Hamideh Khaleghpour", "Brett McKinney"], "title": "Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images", "categories": ["cs.CV", "cs.LG"], "comment": "7 pages, 10 figures. Accepted at the 2nd Asia Pacific Computer\n  Systems Conference (APCS 2024), March 15-17, 2024", "summary": "The rising incidence of skin cancer, coupled with limited public awareness\nand a shortfall in clinical expertise, underscores an urgent need for advanced\ndiagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool\nin this domain, particularly for distinguishing malignant from benign skin\nlesions. Leveraging publicly available datasets of skin lesions, researchers\nhave been developing AI-based diagnostic solutions. However, the integration of\nsuch computer systems in clinical settings is still nascent. This study aims to\nbridge this gap by employing a fusion of image processing techniques and\nmachine learning algorithms, specifically neuro-fuzzy and colonial competition\napproaches. Applied to dermoscopic images from the ISIC database, our method\nachieved a notable accuracy of 94% on a dataset of 560 images. These results\nunderscore the potential of our approach in aiding clinicians in the early\ndetection of melanoma, thereby contributing significantly to skin cancer\ndiagnostics.", "AI": {"tldr": "AI\u7ed3\u5408\u56fe\u50cf\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u76ae\u80a4\u764c\u65e9\u671f\u8bca\u65ad\uff0c\u51c6\u786e\u7387\u8fbe94%\u3002", "motivation": "\u76ae\u80a4\u764c\u53d1\u75c5\u7387\u4e0a\u5347\uff0c\u516c\u4f17\u610f\u8bc6\u4e0d\u8db3\u4e14\u4e34\u5e8a\u4e13\u5bb6\u77ed\u7f3a\uff0c\u4e9f\u9700\u5148\u8fdb\u8bca\u65ad\u8f85\u52a9\u5de5\u5177\u3002", "method": "\u878d\u5408\u56fe\u50cf\u5904\u7406\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u795e\u7ecf\u6a21\u7cca\u548c\u6b96\u6c11\u7ade\u4e89\u65b9\u6cd5\uff09\uff0c\u5e94\u7528\u4e8eISIC\u6570\u636e\u5e93\u7684\u76ae\u80a4\u955c\u56fe\u50cf\u3002", "result": "\u5728560\u5f20\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fbe\u523094%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9ed1\u8272\u7d20\u7624\u65e9\u671f\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5bf9\u76ae\u80a4\u764c\u8bca\u65ad\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.08909", "pdf": "https://arxiv.org/pdf/2505.08909", "abs": "https://arxiv.org/abs/2505.08909", "authors": ["Deliang Wei", "Peng Chen", "Haobo Xu", "Jiale Yao", "Fang Li", "Tieyong Zeng"], "title": "Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems", "categories": ["cs.CV", "cs.LG", "math.FA", "math.OC", "94A08, 47H10, 47J26, 46N10, 47N10"], "comment": "31 pages", "summary": "Plug-and-play (PnP) methods with deep denoisers have shown impressive results\nin imaging problems. They typically require strong convexity or smoothness of\nthe fidelity term and a (residual) non-expansive denoiser for convergence.\nThese assumptions, however, are violated in Poisson inverse problems, and\nnon-expansiveness can hinder denoising performance. To address these\nchallenges, we propose a cocoercive conservative (CoCo) denoiser, which may be\n(residual) expansive, leading to improved denoising. By leveraging the\ngeneralized Helmholtz decomposition, we introduce a novel training strategy\nthat combines Hamiltonian regularization to promote conservativeness and\nspectral regularization to ensure cocoerciveness. We prove that CoCo denoiser\nis a proximal operator of a weakly convex function, enabling a restoration\nmodel with an implicit weakly convex prior. The global convergence of PnP\nmethods to a stationary point of this restoration model is established.\nExtensive experimental results demonstrate that our approach outperforms\nclosely related methods in both visual quality and quantitative metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684CoCo\u53bb\u566a\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPnP\u65b9\u6cd5\u5728\u6cca\u677e\u9006\u95ee\u9898\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edfPnP\u65b9\u6cd5\u5728\u6cca\u677e\u9006\u95ee\u9898\u4e2d\u56e0\u5047\u8bbe\u6761\u4ef6\u4e0d\u6ee1\u8db3\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u53bb\u566a\u5668\u3002", "method": "\u63d0\u51faCoCo\u53bb\u566a\u5668\uff0c\u7ed3\u5408\u54c8\u5bc6\u987f\u6b63\u5219\u5316\u548c\u8c31\u6b63\u5219\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u786e\u4fdd\u5176\u4fdd\u5b88\u6027\u548c\u5171\u8f6d\u6027\u3002", "result": "CoCo\u53bb\u566a\u5668\u662f\u5f31\u51f8\u51fd\u6570\u7684\u90bb\u8fd1\u7b97\u5b50\uff0cPnP\u65b9\u6cd5\u5168\u5c40\u6536\u655b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u89c6\u89c9\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u76f8\u5173\u65b9\u6cd5\u3002", "conclusion": "CoCo\u53bb\u566a\u5668\u4e3a\u6cca\u677e\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86PnP\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2505.08910", "pdf": "https://arxiv.org/pdf/2505.08910", "abs": "https://arxiv.org/abs/2505.08910", "authors": ["Nahid Alam", "Karthik Reddy Kanjula", "Surya Guthikonda", "Timothy Chung", "Bala Krishna S Vegesna", "Abhipsha Das", "Anthony Susevski", "Ryan Sze-Yin Chan", "S M Iftekhar Uddin", "Shayekh Bin Islam", "Roshan Santhosh", "Snegha A", "Drishti Sharma", "Chen Liu", "Isha Chaturvedi", "Genta Indra Winata", "Ashvanth. S", "Snehanshu Mukherjee", "Alham Fikri Aji"], "title": "Behind Maya: Building a Multilingual Vision Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at VLM4ALL CVPR 2025 Workshop", "summary": "In recent times, we have seen a rapid development of large Vision-Language\nModels (VLMs). They have shown impressive results on academic benchmarks,\nprimarily in widely spoken languages but lack performance on low-resource\nlanguages and varied cultural contexts. To address these limitations, we\nintroduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a\nmultilingual image-text pretraining dataset in eight languages, based on the\nLLaVA pretraining dataset; and 2) a multilingual image-text model supporting\nthese languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.", "AI": {"tldr": "Maya\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u8de8\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8eLLaVA\u6570\u636e\u96c6\u6784\u5efa\u4e86\u516b\u79cd\u8bed\u8a00\u7684\u56fe\u50cf-\u6587\u672c\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u652f\u6301\u8fd9\u4e9b\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u6a21\u578b\u3002", "result": "Maya\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u6587\u5316\u548c\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "Maya\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.08961", "pdf": "https://arxiv.org/pdf/2505.08961", "abs": "https://arxiv.org/abs/2505.08961", "authors": ["Yancheng Wang", "Nebojsa Jojic", "Yingzhen Yang"], "title": "Differentiable Channel Selection in Self-Attention For Person Re-Identification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we propose a novel attention module termed the Differentiable\nChannel Selection Attention module, or the DCS-Attention module. In contrast\nwith conventional self-attention, the DCS-Attention module features selection\nof informative channels in the computation of the attention weights. The\nselection of the feature channels is performed in a differentiable manner,\nenabling seamless integration with DNN training. Our DCS-Attention is\ncompatible with either fixed neural network backbones or learnable backbones\nwith Differentiable Neural Architecture Search (DNAS), leading to DCS with\nFixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our\nDCS-Attention is motivated by the principle of Information Bottleneck (IB), and\na novel variational upper bound for the IB loss, which can be optimized by SGD,\nis derived and incorporated into the training loss of the networks with the\nDCS-Attention modules. In this manner, a neural network with DCS-Attention\nmodules is capable of selecting the most informative channels for feature\nextraction so that it enjoys state-of-the-art performance for the Re-ID task.\nExtensive experiments on multiple person Re-ID benchmarks using both DCS-FB and\nDCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy\nof DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention\nin learning discriminative features critical to identifying person identities.\nThe code of our work is available at\nhttps://github.com/Statistical-Deep-Learning/DCS-Attention.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6ce8\u610f\u529b\u6a21\u5757DCS-Attention\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u901a\u9053\u9009\u62e9\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\uff08IB\uff09\u539f\u5219\uff0c\u63d0\u51fa\u4e00\u79cd\u53ef\u5fae\u5206\u901a\u9053\u9009\u62e9\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u6ce8\u610f\u529b\u6743\u91cd\u8ba1\u7b97\u4e2d\u7684\u4fe1\u606f\u901a\u9053\u9009\u62e9\u3002", "method": "\u8bbe\u8ba1\u4e86DCS-Attention\u6a21\u5757\uff0c\u652f\u6301\u56fa\u5b9a\u6216\u53ef\u5b66\u4e60\u9aa8\u5e72\u7f51\u7edc\uff08DCS-FB\u548cDCS-DNAS\uff09\uff0c\u5e76\u63a8\u5bfc\u4e86IB\u635f\u5931\u7684\u53d8\u5206\u4e0a\u754c\uff0c\u901a\u8fc7SGD\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u884c\u4eba\u91cd\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDCS-Attention\u663e\u8457\u63d0\u5347\u4e86DNN\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "DCS-Attention\u80fd\u6709\u6548\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u901a\u9053\uff0c\u5b66\u4e60\u5224\u522b\u6027\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u3002"}}
{"id": "2505.08971", "pdf": "https://arxiv.org/pdf/2505.08971", "abs": "https://arxiv.org/abs/2505.08971", "authors": ["Yangyi Chen", "Hao Peng", "Tong Zhang", "Heng Ji"], "title": "Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "The code will be available at https://github.com/Yangyi-Chen/PRIOR", "summary": "In standard large vision-language models (LVLMs) pre-training, the model\ntypically maximizes the joint probability of the caption conditioned on the\nimage via next-token prediction (NTP); however, since only a small subset of\ncaption tokens directly relates to the visual content, this naive NTP\nunintentionally fits the model to noise and increases the risk of\nhallucination. We present PRIOR, a simple vision-language pre-training approach\nthat addresses this issue by prioritizing image-related tokens through\ndifferential weighting in the NTP loss, drawing from the importance sampling\nframework. PRIOR introduces a reference model-a text-only large language model\n(LLM) trained on the captions without image inputs, to weight each token based\non its probability for LVLMs training. Intuitively, tokens that are directly\nrelated to the visual inputs are harder to predict without the image and thus\nreceive lower probabilities from the text-only reference LLM. During training,\nwe implement a token-specific re-weighting term based on the importance scores\nto adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs\nwith visual encoders and LVLMs without visual encoders. We observe 19% and 8%\naverage relative improvement, respectively, on several vision-language\nbenchmarks compared to NTP. In addition, PRIOR exhibits superior scaling\nproperties, as demonstrated by significantly higher scaling coefficients,\nindicating greater potential for performance gains compared to NTP given\nincreasing compute and data.", "AI": {"tldr": "PRIOR\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5dee\u5f02\u52a0\u6743NTP\u635f\u5931\u6765\u4f18\u5148\u5904\u7406\u4e0e\u56fe\u50cf\u76f8\u5173\u7684\u6807\u8bb0\uff0c\u51cf\u5c11\u566a\u58f0\u62df\u5408\u548c\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u6807\u51c6LVLMs\u9884\u8bad\u7ec3\u4e2d\uff0cNTP\u65b9\u6cd5\u4f1a\u62df\u5408\u566a\u58f0\u5e76\u589e\u52a0\u5e7b\u89c9\u98ce\u9669\uff0c\u56e0\u4e3a\u53ea\u6709\u5c11\u91cf\u6807\u8bb0\u4e0e\u89c6\u89c9\u5185\u5bb9\u76f4\u63a5\u76f8\u5173\u3002", "method": "PRIOR\u5229\u7528\u6587\u672c\u53c2\u8003LLM\u4e3a\u6bcf\u4e2a\u6807\u8bb0\u5206\u914d\u91cd\u8981\u6027\u5206\u6570\uff0c\u8c03\u6574NTP\u635f\u5931\u6743\u91cd\uff0c\u4f18\u5148\u5904\u7406\u56fe\u50cf\u76f8\u5173\u6807\u8bb0\u3002", "result": "\u5728\u4e24\u79cdLVLMs\u8bbe\u7f6e\u4e0b\uff0cPRIOR\u76f8\u6bd4NTP\u5206\u522b\u5b9e\u73b0\u4e8619%\u548c8%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5e76\u5c55\u793a\u4e86\u66f4\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "PRIOR\u901a\u8fc7\u5dee\u5f02\u52a0\u6743\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2505.08999", "pdf": "https://arxiv.org/pdf/2505.08999", "abs": "https://arxiv.org/abs/2505.08999", "authors": ["Wei-Long Tian", "Peng Gao", "Xiao Liu", "Long Xu", "Hamido Fujita", "Hanan Aljuai", "Mao-Li Wang"], "title": "Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, visual tracking methods based on convolutional neural\nnetworks and Transformers have achieved remarkable performance and have been\nsuccessfully applied in fields such as autonomous driving. However, the\nnumerous security issues exposed by deep learning models have gradually\naffected the reliable application of visual tracking methods in real-world\nscenarios. Therefore, how to reveal the security vulnerabilities of existing\nvisual trackers through effective adversarial attacks has become a critical\nproblem that needs to be addressed. To this end, we propose an adaptive\nmeta-gradient adversarial attack (AMGA) method for visual tracking. This method\nintegrates multi-model ensembles and meta-learning strategies, combining\nmomentum mechanisms and Gaussian smoothing, which can significantly enhance the\ntransferability and attack effectiveness of adversarial examples. AMGA randomly\nselects models from a large model repository, constructs diverse tracking\nscenarios, and iteratively performs both white- and black-box adversarial\nattacks in each scenario, optimizing the gradient directions of each model.\nThis paradigm minimizes the gap between white- and black-box adversarial\nattacks, thus achieving excellent attack performance in black-box scenarios.\nExtensive experimental results on large-scale datasets such as OTB2015, LaSOT,\nand GOT-10k demonstrate that AMGA significantly improves the attack\nperformance, transferability, and deception of adversarial examples. Codes and\ndata are available at https://github.com/pgao-lab/AMGA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5143\u68af\u5ea6\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff08AMGA\uff09\uff0c\u7528\u4e8e\u63ed\u793a\u89c6\u89c9\u8ddf\u8e2a\u5668\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u96c6\u6210\u548c\u5143\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u548c\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5b89\u5168\u95ee\u9898\u5f71\u54cd\u4e86\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u53ef\u9760\u5e94\u7528\uff0c\u9700\u901a\u8fc7\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u63ed\u793a\u5176\u6f0f\u6d1e\u3002", "method": "AMGA\u7ed3\u5408\u591a\u6a21\u578b\u96c6\u6210\u3001\u5143\u5b66\u4e60\u3001\u52a8\u91cf\u673a\u5236\u548c\u9ad8\u65af\u5e73\u6ed1\uff0c\u968f\u673a\u9009\u62e9\u6a21\u578b\u6784\u5efa\u591a\u6837\u573a\u666f\uff0c\u8fed\u4ee3\u4f18\u5316\u68af\u5ea6\u65b9\u5411\uff0c\u7f29\u5c0f\u767d\u76d2\u4e0e\u9ed1\u76d2\u653b\u51fb\u5dee\u8ddd\u3002", "result": "\u5728OTB2015\u3001LaSOT\u548cGOT-10k\u7b49\u6570\u636e\u96c6\u4e0a\uff0cAMGA\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u7684\u653b\u51fb\u6027\u80fd\u3001\u8fc1\u79fb\u6027\u548c\u6b3a\u9a97\u6027\u3002", "conclusion": "AMGA\u4e3a\u89c6\u89c9\u8ddf\u8e2a\u5668\u7684\u5b89\u5168\u6f0f\u6d1e\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.09018", "pdf": "https://arxiv.org/pdf/2505.09018", "abs": "https://arxiv.org/abs/2505.09018", "authors": ["Adarsh Kumar"], "title": "Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective dietary monitoring is critical for managing Type 2 diabetes, yet\naccurately estimating caloric intake remains a major challenge. While\ncontinuous glucose monitors (CGMs) offer valuable physiological data, they\noften fall short in capturing the full nutritional profile of meals due to\ninter-individual and meal-specific variability. In this work, we introduce a\nmultimodal deep learning framework that jointly leverages CGM time-series data,\nDemographic/Microbiome, and pre-meal food images to enhance caloric estimation.\nOur model utilizes attention based encoding and a convolutional feature\nextraction for meal imagery, multi-layer perceptrons for CGM and Microbiome\ndata followed by a late fusion strategy for joint reasoning. We evaluate our\napproach on a curated dataset of over 40 participants, incorporating\nsynchronized CGM, Demographic and Microbiome data and meal photographs with\nstandardized caloric labels. Our model achieves a Root Mean Squared Relative\nError (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These\nfindings demonstrate the potential of multimodal sensing to improve automated\ndietary assessment tools for chronic disease management.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408CGM\u6570\u636e\u3001\u4eba\u53e3\u7edf\u8ba1/\u5fae\u751f\u7269\u7ec4\u6570\u636e\u548c\u9910\u524d\u98df\u7269\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u70ed\u91cf\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u51c6\u786e\u4f30\u8ba1\u70ed\u91cf\u6444\u5165\u5bf92\u578b\u7cd6\u5c3f\u75c5\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982CGM\uff09\u56e0\u4e2a\u4f53\u548c\u9910\u98df\u5dee\u5f02\u96be\u4ee5\u5168\u9762\u6355\u6349\u8425\u517b\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u7f16\u7801\u548c\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u5904\u7406\u98df\u7269\u56fe\u50cf\uff0c\u591a\u5c42\u611f\u77e5\u673a\u5904\u7406CGM\u548c\u5fae\u751f\u7269\u7ec4\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u540e\u671f\u878d\u5408\u7b56\u7565\u8054\u5408\u63a8\u7406\u3002", "result": "\u572840\u591a\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u7684\u70ed\u91cf\u4f30\u8ba1\u8bef\u5dee\uff08RMSRE\uff09\u4e3a0.2544\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534750%\u4ee5\u4e0a\u3002", "conclusion": "\u591a\u6a21\u6001\u4f20\u611f\u6280\u672f\u6709\u671b\u63d0\u5347\u6162\u6027\u75c5\u7ba1\u7406\u4e2d\u81ea\u52a8\u5316\u996e\u98df\u8bc4\u4f30\u5de5\u5177\u7684\u6548\u679c\u3002"}}
{"id": "2505.09073", "pdf": "https://arxiv.org/pdf/2505.09073", "abs": "https://arxiv.org/abs/2505.09073", "authors": ["J. Brennan Peace", "Shuowen Hu", "Benjamin S. Riggan"], "title": "2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition", "categories": ["cs.CV"], "comment": "To appear at the IEEE International Conference on Automatic Face and\n  Gesture 2025 (FG2025)", "summary": "Despite recent advances in facial recognition, there remains a fundamental\nissue concerning degradations in performance due to substantial perspective\n(pose) differences between enrollment and query (probe) imagery. Therefore, we\npropose a novel domain adaptive framework to facilitate improved performances\nacross large discrepancies in pose by enabling image-based (2D) representations\nto infer properties of inherently pose invariant point cloud (3D)\nrepresentations. Specifically, our proposed framework achieves better pose\ninvariance by using (1) a shared (joint) attention mapping to emphasize common\npatterns that are most correlated between 2D facial images and 3D facial data\nand (2) a joint entropy regularizing loss to promote better\nconsistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D\nand 3D representations$\\unicode{x2014}$by leveraging both attention maps. This\nframework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms\ncompetitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$)\nTAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and\n1.57$\\unicode{x0025}$, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc72D\u56fe\u50cf\u63a8\u65ad3D\u70b9\u4e91\u7684\u59ff\u6001\u4e0d\u53d8\u6027\uff0c\u89e3\u51b3\u4e86\u9762\u90e8\u8bc6\u522b\u4e2d\u56e0\u59ff\u6001\u5dee\u5f02\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u9762\u90e8\u8bc6\u522b\u5728\u59ff\u6001\u5dee\u5f02\u8f83\u5927\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u63d0\u5347\u8de8\u59ff\u6001\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5171\u4eab\u6ce8\u610f\u529b\u6620\u5c04\u548c\u8054\u5408\u71b5\u6b63\u5219\u5316\u635f\u5931\uff0c\u589e\u5f3a2D\u548c3D\u8868\u793a\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u5728FaceScape\u548cARL-VTF\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u59ff\u6001\u4e0d\u53d8\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8de8\u59ff\u6001\u9762\u90e8\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09092", "pdf": "https://arxiv.org/pdf/2505.09092", "abs": "https://arxiv.org/abs/2505.09092", "authors": ["Yuhang Wang", "Abdulaziz Alhuraish", "Shengming Yuan", "Hao Zhou"], "title": "OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its\nreal-world performance remains underexplored due to proprietary systems and\nlimited data access. This paper presents OpenLKA, the first open, large-scale\ndataset for LKA evaluation and improvement. It includes 400 hours of driving\ndata from 50+ production vehicle models, collected through extensive road\ntesting in Tampa, Florida and global contributions from the Comma.ai driving\ncommunity. The dataset spans a wide range of challenging scenarios, including\ncomplex road geometries, degraded lane markings, adverse weather, lighting\nconditions and surrounding traffic. The dataset is multimodal, comprising: i)\nfull CAN bus streams, decoded using custom reverse-engineered DBC files to\nextract key LKA events (e.g., system disengagements, lane detection failures);\nii) synchronized high-resolution dash-cam video; iii) real-time outputs from\nOpenpilot, providing accurate estimates of road curvature and lane positioning;\niv) enhanced scene annotations generated by Vision Language Models, describing\nlane visibility, pavement quality, weather, lighting, and traffic conditions.\nBy integrating vehicle-internal signals with high-fidelity perception and rich\nsemantic context, OpenLKA provides a comprehensive platform for benchmarking\nthe real-world performance of production LKA systems, identifying\nsafety-critical operational scenarios, and assessing the readiness of current\nroad infrastructure for autonomous driving. The dataset is publicly available\nat: https://github.com/OpenLKA/OpenLKA.", "AI": {"tldr": "OpenLKA\u662f\u9996\u4e2a\u5f00\u653e\u7684\u5927\u89c4\u6a21LKA\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5305\u542b400\u5c0f\u65f6\u9a7e\u9a76\u6570\u636e\uff0c\u652f\u6301\u591a\u6a21\u6001\u5206\u6790\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdbLKA\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709LKA\u7cfb\u7edf\u7684\u771f\u5b9e\u6027\u80fd\u56e0\u4e13\u6709\u7cfb\u7edf\u548c\u6570\u636e\u9650\u5236\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0cOpenLKA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8def\u6d4b\u548c\u793e\u533a\u8d21\u732e\u6536\u96c6\u6570\u636e\uff0c\u6574\u5408\u8f66\u8f86\u5185\u90e8\u4fe1\u53f7\u3001\u9ad8\u6e05\u89c6\u9891\u3001\u5b9e\u65f6\u8f93\u51fa\u548c\u573a\u666f\u6807\u6ce8\u3002", "result": "\u6570\u636e\u96c6\u6db5\u76d6\u590d\u6742\u573a\u666f\uff0c\u63d0\u4f9b\u591a\u6a21\u6001\u6570\u636e\uff0c\u652f\u6301LKA\u6027\u80fd\u8bc4\u4f30\u548c\u57fa\u7840\u8bbe\u65bd\u5206\u6790\u3002", "conclusion": "OpenLKA\u4e3aLKA\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u5168\u9762\u5e73\u53f0\uff0c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.09118", "pdf": "https://arxiv.org/pdf/2505.09118", "abs": "https://arxiv.org/abs/2505.09118", "authors": ["Dayong Liang", "Changmeng Zheng", "Zhiyuan Wen", "Yi Cai", "Xiao-Yong Wei", "Qing Li"], "title": "Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Traditional scene graphs primarily focus on spatial relationships, limiting\nvision-language models' (VLMs) ability to reason about complex interactions in\nvisual scenes. This paper addresses two key challenges: (1) conventional\ndetection-to-construction methods produce unfocused, contextually irrelevant\nrelationship sets, and (2) existing approaches fail to form persistent memories\nfor generalizing interaction reasoning to new scenes. We propose\nInteraction-augmented Scene Graph Reasoning (ISGR), a framework that enhances\nVLMs' interactional reasoning through three complementary components. First,\nour dual-stream graph constructor combines SAM-powered spatial relation\nextraction with interaction-aware captioning to generate functionally salient\nscene graphs with spatial grounding. Second, we employ targeted interaction\nqueries to activate VLMs' latent knowledge of object functionalities,\nconverting passive recognition into active reasoning about how objects work\ntogether. Finally, we introduce a lone-term memory reinforcement learning\nstrategy with a specialized interaction-focused reward function that transforms\ntransient patterns into long-term reasoning heuristics. Extensive experiments\ndemonstrate that our approach significantly outperforms baseline methods on\ninteraction-heavy reasoning benchmarks, with particularly strong improvements\non complex scene understanding tasks. The source code can be accessed at\nhttps://github.com/open_upon_acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6ISGR\uff0c\u901a\u8fc7\u53cc\u6d41\u56fe\u6784\u9020\u5668\u3001\u4ea4\u4e92\u67e5\u8be2\u548c\u957f\u671f\u8bb0\u5fc6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u573a\u666f\u56fe\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u5173\u7cfb\uff0c\u9650\u5236\u4e86VLMs\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5173\u7cfb\u96c6\u4e0d\u76f8\u5173\u548c\u7f3a\u4e4f\u6301\u4e45\u8bb0\u5fc6\u7684\u95ee\u9898\u3002", "method": "ISGR\u6846\u67b6\u5305\u542b\u53cc\u6d41\u56fe\u6784\u9020\u5668\uff08\u7ed3\u5408\u7a7a\u95f4\u5173\u7cfb\u63d0\u53d6\u548c\u4ea4\u4e92\u611f\u77e5\u6807\u6ce8\uff09\u3001\u4ea4\u4e92\u67e5\u8be2\u6fc0\u6d3bVLMs\u529f\u80fd\u77e5\u8bc6\uff0c\u4ee5\u53ca\u957f\u671f\u8bb0\u5fc6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cISGR\u5728\u4ea4\u4e92\u5bc6\u96c6\u578b\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "ISGR\u901a\u8fc7\u589e\u5f3a\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u4e86VLMs\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.09123", "pdf": "https://arxiv.org/pdf/2505.09123", "abs": "https://arxiv.org/abs/2505.09123", "authors": ["Guoying Liang", "Su Yang"], "title": "Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Big model has emerged as a new research paradigm that can be applied to\nvarious down-stream tasks with only minor effort for domain adaption.\nCorrespondingly, this study tackles Camouflaged Object Detection (COD)\nleveraging the Segment Anything Model (SAM). The previous studies declared that\nSAM is not workable for COD but this study reveals that SAM works if promoted\nproperly, for which we devise a new framework to render point promotions:\nFirst, we develop the Promotion Point Targeting Network (PPT-net) to leverage\nmulti-scale features in predicting the probabilities of camouflaged objects'\npresences at given candidate points over the image. Then, we develop a key\npoint selection (KPS) algorithm to deploy both positive and negative point\npromotions contrastively to SAM to guide the segmentation. It is the first work\nto facilitate big model for COD and achieves plausible results experimentally\nover the existing methods on 3 data sets under 6 metrics. This study\ndemonstrates an off-the-shelf methodology for COD by leveraging SAM, which\ngains advantage over designing professional models from scratch, not only in\nperformance, but also in turning the problem to a less challenging task, that\nis, seeking informative but not exactly precise promotions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528Segment Anything Model (SAM)\u8fdb\u884c\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b(COD)\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u63d0\u793a\u63d0\u5347SAM\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1\u4e4b\u524d\u7684\u7814\u7a76\u8ba4\u4e3aSAM\u4e0d\u9002\u7528\u4e8eCOD\uff0c\u4f46\u672c\u7814\u7a76\u901a\u8fc7\u9002\u5f53\u7684\u63d0\u793a\u65b9\u6cd5\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\uff0c\u65e8\u5728\u5229\u7528\u5927\u6a21\u578b\u7684\u4f18\u52bf\u7b80\u5316COD\u4efb\u52a1\u3002", "method": "\u8bbe\u8ba1\u4e86Promotion Point Targeting Network (PPT-net)\u9884\u6d4b\u4f2a\u88c5\u76ee\u6807\u7684\u5b58\u5728\u6982\u7387\uff0c\u5e76\u5f00\u53d1\u4e86\u5173\u952e\u70b9\u9009\u62e9(KPS)\u7b97\u6cd5\uff0c\u901a\u8fc7\u6b63\u8d1f\u70b9\u63d0\u793a\u5bf9\u6bd4\u5f15\u5bfcSAM\u5206\u5272\u3002", "result": "\u57283\u4e2a\u6570\u636e\u96c6\u548c6\u4e2a\u6307\u6807\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86SAM\u5728COD\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528SAM\u8fdb\u884cCOD\u662f\u4e00\u79cd\u73b0\u6210\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u5c06\u95ee\u9898\u7b80\u5316\u4e3a\u5bfb\u627e\u4fe1\u606f\u6027\u800c\u975e\u7cbe\u786e\u7684\u63d0\u793a\u3002"}}
{"id": "2505.09129", "pdf": "https://arxiv.org/pdf/2505.09129", "abs": "https://arxiv.org/abs/2505.09129", "authors": ["Wei Meng"], "title": "WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes", "categories": ["cs.CV", "cs.AI", "es: 68T10, 68T05, 62H35, 68U10", "I.4.9; I.5.1; I.2.10"], "comment": "17 pages, 3 figures, 3 tables. The paper proposes a lightweight\n  weakly-supervised color intelligence model for tactical video anomaly\n  detection, tested on anonymized African surveillance data", "summary": "The deployment of traditional deep learning models in high-risk security\ntasks in an unlabeled, data-non-exploitable video intelligence environment\nfaces significant challenges. In this paper, we propose a lightweight anomaly\ndetection framework based on color features for surveillance video clips in a\nhigh sensitivity tactical mission, aiming to quickly identify and interpret\npotential threat events under resource-constrained and data-sensitive\nconditions. The method fuses unsupervised KMeans clustering with RGB channel\nhistogram modeling to achieve composite detection of structural anomalies and\ncolor mutation signals in key frames. The experiment takes an operation\nsurveillance video occurring in an African country as a research sample, and\nsuccessfully identifies multiple highly anomalous frames related to high-energy\nlight sources, target presence, and reflective interference under the condition\nof no access to the original data. The results show that this method can be\neffectively used for tactical assassination warning, suspicious object\nscreening and environmental drastic change monitoring with strong deployability\nand tactical interpretation value. The study emphasizes the importance of color\nfeatures as low semantic battlefield signal carriers, and its battlefield\nintelligent perception capability will be further extended by combining graph\nneural networks and temporal modeling in the future.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u989c\u8272\u7279\u5f81\u7684\u8f7b\u91cf\u7ea7\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u548c\u6570\u636e\u654f\u611f\u73af\u5883\u4e0b\u7684\u6218\u672f\u76d1\u63a7\u89c6\u9891\uff0c\u901a\u8fc7\u65e0\u76d1\u7763KMeans\u805a\u7c7b\u548cRGB\u901a\u9053\u76f4\u65b9\u56fe\u5efa\u6a21\u5b9e\u73b0\u5173\u952e\u5e27\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5b89\u5168\u4efb\u52a1\u4e2d\u9762\u4e34\u672a\u6807\u8bb0\u548c\u6570\u636e\u4e0d\u53ef\u5229\u7528\u7684\u6311\u6218\uff0c\u9700\u5feb\u901f\u8bc6\u522b\u6f5c\u5728\u5a01\u80c1\u4e8b\u4ef6\u3002", "method": "\u878d\u5408\u65e0\u76d1\u7763KMeans\u805a\u7c7b\u4e0eRGB\u901a\u9053\u76f4\u65b9\u56fe\u5efa\u6a21\uff0c\u68c0\u6d4b\u5173\u952e\u5e27\u4e2d\u7684\u7ed3\u6784\u5f02\u5e38\u548c\u989c\u8272\u7a81\u53d8\u4fe1\u53f7\u3002", "result": "\u6210\u529f\u8bc6\u522b\u9ad8\u80fd\u5149\u6e90\u3001\u76ee\u6807\u5b58\u5728\u548c\u53cd\u5c04\u5e72\u6270\u7b49\u5f02\u5e38\u5e27\uff0c\u9002\u7528\u4e8e\u6218\u672f\u8b66\u544a\u3001\u53ef\u7591\u7269\u4f53\u7b5b\u67e5\u548c\u73af\u5883\u7a81\u53d8\u76d1\u6d4b\u3002", "conclusion": "\u989c\u8272\u7279\u5f81\u4f5c\u4e3a\u4f4e\u8bed\u4e49\u6218\u573a\u4fe1\u53f7\u8f7d\u4f53\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u672a\u6765\u5c06\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u65f6\u95f4\u5efa\u6a21\u6269\u5c55\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2505.09139", "pdf": "https://arxiv.org/pdf/2505.09139", "abs": "https://arxiv.org/abs/2505.09139", "authors": ["Lucas Choi", "Ross Greer"], "title": "Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) offer flexible object detection through natural\nlanguage prompts but suffer from performance variability depending on prompt\nphrasing. In this paper, we introduce a method for automated prompt refinement\nusing a novel metric called the Contrastive Class Alignment Score (CCAS), which\nranks prompts based on their semantic alignment with a target object class\nwhile penalizing similarity to confounding classes. Our method generates\ndiverse prompt candidates via a large language model and filters them through\nCCAS, computed using prompt embeddings from a sentence transformer. We evaluate\nour approach on challenging object categories, demonstrating that our automatic\nselection of high-precision prompts improves object detection accuracy without\nthe need for additional model training or labeled data. This scalable and\nmodel-agnostic pipeline offers a principled alternative to manual prompt\nengineering for VLM-based detection systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u7c7b\u5bf9\u9f50\u5206\u6570\uff08CCAS\uff09\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6027\u80fd\u53d7\u63d0\u793a\u63aa\u8f9e\u5f71\u54cd\u8f83\u5927\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u4f18\u5316\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u63d0\u793a\u5019\u9009\uff0c\u901a\u8fc7CCAS\uff08\u57fa\u4e8e\u53e5\u5b50\u53d8\u6362\u5668\u7684\u5d4c\u5165\uff09\u7b5b\u9009\u8bed\u4e49\u5bf9\u9f50\u76ee\u6807\u7c7b\u4e14\u4e0e\u6df7\u6dc6\u7c7b\u5dee\u5f02\u5927\u7684\u63d0\u793a\u3002", "result": "\u5728\u6311\u6218\u6027\u76ee\u6807\u7c7b\u522b\u4e0a\u9a8c\u8bc1\uff0c\u81ea\u52a8\u9009\u62e9\u7684\u9ad8\u7cbe\u5ea6\u63d0\u793a\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6807\u6ce8\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aVLM\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6a21\u578b\u65e0\u5173\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2505.09140", "pdf": "https://arxiv.org/pdf/2505.09140", "abs": "https://arxiv.org/abs/2505.09140", "authors": ["Zechao Guan", "Feng Yan", "Shuai Du", "Lin Ma", "Qingshan Liu"], "title": "TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Diffusion Transformer (DiT) models have significantly\nimproved 3D point cloud generation. However, existing methods primarily focus\non local feature extraction while overlooking global topological information,\nsuch as voids, which are crucial for maintaining shape consistency and\ncapturing complex geometries. To address this limitation, we propose\nTopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure\nfor 3D point cloud generation. Specifically, we design the bottleneck structure\nutilizing Perceiver Resampler, which not only offers a mode to integrate\ntopological information extracted through persistent homology into feature\nlearning, but also adaptively filters out redundant local features to improve\ntraining efficiency. Experimental results demonstrate that TopoDiT-3D\noutperforms state-of-the-art models in visual quality, diversity, and training\nefficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich\ntopological information for 3D point cloud generation and its synergy with\nconventional local feature learning. Videos and code are available at\nhttps://github.com/Zechao-Guan/TopoDiT-3D.", "AI": {"tldr": "TopoDiT-3D\u662f\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u611f\u77e5\u7684\u6269\u6563Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u74f6\u9888\u7ed3\u6784\u548c\u6301\u4e45\u540c\u8c03\u63d0\u53d6\u5168\u5c40\u62d3\u6251\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff0c\u5ffd\u7565\u4e86\u5168\u5c40\u62d3\u6251\u4fe1\u606f\uff08\u5982\u7a7a\u6d1e\uff09\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u4fdd\u6301\u5f62\u72b6\u4e00\u81f4\u6027\u548c\u6355\u6349\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8ePerceiver Resampler\u7684\u74f6\u9888\u7ed3\u6784\uff0c\u5c06\u6301\u4e45\u540c\u8c03\u63d0\u53d6\u7684\u62d3\u6251\u4fe1\u606f\u878d\u5165\u7279\u5f81\u5b66\u4e60\uff0c\u5e76\u81ea\u9002\u5e94\u8fc7\u6ee4\u5197\u4f59\u5c40\u90e8\u7279\u5f81\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "result": "TopoDiT-3D\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u8bad\u7ec3\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u62d3\u6251\u4fe1\u606f\u4e0e\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u7684\u534f\u540c\u4f5c\u7528\u3002", "conclusion": "TopoDiT-3D\u8bc1\u660e\u4e86\u5168\u5c40\u62d3\u6251\u4fe1\u606f\u5bf93D\u70b9\u4e91\u751f\u6210\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.09155", "pdf": "https://arxiv.org/pdf/2505.09155", "abs": "https://arxiv.org/abs/2505.09155", "authors": ["Yichen Shi", "Zhuofu Tao", "Yuhao Gao", "Li Huang", "Hongyang Wang", "Zhiping Yu", "Ting-Jung Lin", "Lei He"], "title": "AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection", "categories": ["cs.CV"], "comment": "accepted by LAD25", "summary": "Current multimodal large language models (MLLMs) struggle to understand\ncircuit schematics due to their limited recognition capabilities. This could be\nattributed to the lack of high-quality schematic-netlist training data.\nExisting work such as AMSnet applies schematic parsing to generate netlists.\nHowever, these methods rely on hard-coded heuristics and are difficult to apply\nto complex or noisy schematics in this paper. We therefore propose a novel net\ndetection mechanism based on segmentation with high robustness. The proposed\nmethod also recovers positional information, allowing digital reconstruction of\nschematics. We then expand AMSnet dataset with schematic images from various\nsources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with\nschematic images, Spectre-formatted netlists, OpenAccess digital schematics,\nand positional information for circuit components and nets, whereas AMSnet only\nincludes 792 circuits with SPICE netlists but no digital schematics.", "AI": {"tldr": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7406\u89e3\u7535\u8def\u56fe\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u7535\u8def\u56fe-\u7f51\u8868\u8bad\u7ec3\u6570\u636e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u7684\u65b0\u578b\u7f51\u7edc\u68c0\u6d4b\u673a\u5236\uff0c\u5e76\u6269\u5c55\u4e86AMSnet\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\uff08\u5982AMSnet\uff09\u4f9d\u8d56\u786c\u7f16\u7801\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u6216\u6709\u566a\u58f0\u7684\u7535\u8def\u56fe\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u7684\u7f51\u7edc\u68c0\u6d4b\u673a\u5236\uff0c\u80fd\u591f\u6062\u590d\u4f4d\u7f6e\u4fe1\u606f\u5e76\u652f\u6301\u7535\u8def\u56fe\u7684\u6570\u5b57\u5316\u91cd\u5efa\u3002\u540c\u65f6\u6269\u5c55\u4e86AMSnet\u6570\u636e\u96c6\uff0c\u521b\u5efa\u4e86AMSnet 2.0\u3002", "result": "AMSnet 2.0\u5305\u542b2,686\u4e2a\u7535\u8def\uff0c\u63d0\u4f9b\u7535\u8def\u56fe\u56fe\u50cf\u3001Spectre\u683c\u5f0f\u7f51\u8868\u3001OpenAccess\u6570\u5b57\u7535\u8def\u56fe\u53ca\u7ec4\u4ef6\u548c\u7f51\u7edc\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u8fdc\u8d85\u539f\u59cbAMSnet\u7684792\u4e2a\u7535\u8def\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u590d\u6742\u7535\u8def\u56fe\u7684\u7406\u89e3\u80fd\u529b\uff0c\u6269\u5c55\u7684\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u8d44\u6e90\u3002"}}
{"id": "2505.09168", "pdf": "https://arxiv.org/pdf/2505.09168", "abs": "https://arxiv.org/abs/2505.09168", "authors": ["Jianlin Sun", "Xiaolin Fang", "Juwei Guan", "Dongdong Gui", "Teqi Wang", "Tongxin Zhu"], "title": "DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The core challenge in Camouflage Object Detection (COD) lies in the\nindistinguishable similarity between targets and backgrounds in terms of color,\ntexture, and shape. This causes existing methods to either lose edge details\n(such as hair-like fine structures) due to over-reliance on global semantic\ninformation or be disturbed by similar backgrounds (such as vegetation\npatterns) when relying solely on local features. We propose DRRNet, a\nfour-stage architecture characterized by a \"context-detail-fusion-refinement\"\npipeline to address these issues. Specifically, we introduce an Omni-Context\nFeature Extraction Module to capture global camouflage patterns and a Local\nDetail Extraction Module to supplement microstructural information for the\nfull-scene context module. We then design a module for forming dual\nrepresentations of scene understanding and structural awareness, which fuses\npanoramic features and local features across various scales. In the decoder, we\nalso introduce a reverse refinement module that leverages spatial edge priors\nand frequency-domain noise suppression to perform a two-stage inverse\nrefinement of the output. By applying two successive rounds of inverse\nrefinement, the model effectively suppresses background interference and\nenhances the continuity of object boundaries. Experimental results demonstrate\nthat DRRNet significantly outperforms state-of-the-art methods on benchmark\ndatasets. Our code is available at https://github.com/jerrySunning/DRRNet.", "AI": {"tldr": "DRRNet\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u9636\u6bb5\u67b6\u6784\uff0c\u901a\u8fc7\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\u878d\u5408\u53ca\u9006\u5411\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4e2d\u76ee\u6807\u4e0e\u80cc\u666f\u5728\u989c\u8272\u3001\u7eb9\u7406\u548c\u5f62\u72b6\u4e0a\u7684\u9ad8\u5ea6\u76f8\u4f3c\u6027\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\u548c\u5c40\u90e8\u7ec6\u8282\u3002", "method": "DRRNet\u91c7\u7528\u56db\u9636\u6bb5\u6d41\u7a0b\uff0c\u5305\u62ec\u5168\u5c40\u4e0a\u4e0b\u6587\u7279\u5f81\u63d0\u53d6\u3001\u5c40\u90e8\u7ec6\u8282\u8865\u5145\u3001\u53cc\u8868\u5f81\u878d\u5408\u548c\u9006\u5411\u7ec6\u5316\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRRNet\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DRRNet\u901a\u8fc7\u591a\u9636\u6bb5\u7279\u5f81\u878d\u5408\u548c\u9006\u5411\u7ec6\u5316\uff0c\u6709\u6548\u6291\u5236\u80cc\u666f\u5e72\u6270\u5e76\u63d0\u5347\u76ee\u6807\u8fb9\u754c\u8fde\u7eed\u6027\u3002"}}
{"id": "2505.09178", "pdf": "https://arxiv.org/pdf/2505.09178", "abs": "https://arxiv.org/abs/2505.09178", "authors": ["Yitao Zhu", "Yuan Yin", "Zhenrong Shen", "Zihao Zhao", "Haiyu Song", "Sheng Wang", "Dinggang Shen", "Qian Wang"], "title": "UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System", "categories": ["cs.CV"], "comment": "14 pages", "summary": "The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/.", "AI": {"tldr": "UniCAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u591a\u4efb\u52a1\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\uff08CAD\uff09\u5e73\u53f0\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u7b56\u7565\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u7684\u533b\u7597\u56fe\u50cf\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u5f71\u50cf\u9886\u57df\u7f3a\u4e4f\u5f00\u6e90CAD\u5e73\u53f0\u4ee5\u53ca\u591a\u4efb\u52a1\u6a21\u578b\u5f00\u53d1\u8d44\u6e90\u5bc6\u96c6\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\u7b56\u7565\u548c\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7ed3\u5408\u51bb\u7ed3\u7684\u57fa\u7840\u6a21\u578b\u4e0e\u53ef\u63d2\u62d4\u4e13\u5bb6\u6a21\u5757\u3002", "result": "\u572812\u4e2a\u533b\u7597\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ec5\u97000.17%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "UniCAD\u4e3a\u533b\u7597\u5f71\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5f00\u6e90\u5e73\u53f0\uff0c\u4fc3\u8fdb\u4e86\u7814\u7a76\u751f\u6001\u7684\u516c\u5e73\u4e0e\u6548\u7387\u3002"}}
{"id": "2505.09188", "pdf": "https://arxiv.org/pdf/2505.09188", "abs": "https://arxiv.org/abs/2505.09188", "authors": ["Minjun Kim", "Jaehyeon Choi", "Jongkeun Lee", "Wonjin Cho", "U Kang"], "title": "Zero-shot Quantization: A Comprehensive Survey", "categories": ["cs.CV"], "comment": "IJCAI 2025 Survey Track", "summary": "Network quantization has proven to be a powerful approach to reduce the\nmemory and computational demands of deep learning models for deployment on\nresource-constrained devices. However, traditional quantization methods often\nrely on access to training data, which is impractical in many real-world\nscenarios due to privacy, security, or regulatory constraints. Zero-shot\nQuantization (ZSQ) emerges as a promising solution, achieving quantization\nwithout requiring any real data. In this paper, we provide a comprehensive\noverview of ZSQ methods and their recent advancements. First, we provide a\nformal definition of the ZSQ problem and highlight the key challenges. Then, we\ncategorize the existing ZSQ methods into classes based on data generation\nstrategies, and analyze their motivations, core ideas, and key takeaways.\nLastly, we suggest future research directions to address the remaining\nlimitations and advance the field of ZSQ. To the best of our knowledge, this\npaper is the first in-depth survey on ZSQ.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u96f6\u6837\u672c\u91cf\u5316\uff08ZSQ\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u91cf\u5316\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u5e76\u5206\u7c7b\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u9690\u79c1\u3001\u5b89\u5168\u6216\u6cd5\u89c4\u9650\u5236\u4e0b\u4e0d\u5b9e\u7528\uff0c\u56e0\u6b64\u96f6\u6837\u672c\u91cf\u5316\uff08ZSQ\uff09\u6210\u4e3a\u65e0\u9700\u771f\u5b9e\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u9996\u5148\u5b9a\u4e49\u4e86ZSQ\u95ee\u9898\u53ca\u6311\u6218\uff0c\u7136\u540e\u6839\u636e\u6570\u636e\u751f\u6210\u7b56\u7565\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u52a8\u673a\u3001\u6838\u5fc3\u601d\u60f3\u548c\u5173\u952e\u70b9\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9ZSQ\u65b9\u6cd5\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u603b\u7ed3\u4e86\u5176\u6700\u65b0\u8fdb\u5c55\u3002", "conclusion": "\u672c\u6587\u662f\u9996\u4e2a\u5173\u4e8eZSQ\u7684\u6df1\u5ea6\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u89e3\u51b3\u73b0\u6709\u5c40\u9650\u6027\u3002"}}
{"id": "2505.09196", "pdf": "https://arxiv.org/pdf/2505.09196", "abs": "https://arxiv.org/abs/2505.09196", "authors": ["Tong Li", "Lizhi Wang", "Hansen Feng", "Lin Zhu", "Hua Huang"], "title": "PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement", "categories": ["cs.CV"], "comment": "11 pages, 9 tables, 9 figures", "summary": "Low-light image enhancement (LLIE) is a fundamental task in computational\nphotography, aiming to improve illumination, reduce noise, and enhance image\nquality. While recent advancements focus on designing increasingly complex\nneural network models, we observe a peculiar phenomenon: resetting certain\nparameters to random values unexpectedly improves enhancement performance for\nsome images. Drawing inspiration from biological genes, we term this phenomenon\nthe gene effect. The gene effect limits enhancement performance, as even random\nparameters can sometimes outperform learned ones, preventing models from fully\nutilizing their capacity. In this paper, we investigate the reason and propose\na solution. Based on our observations, we attribute the gene effect to static\nparameters, analogous to how fixed genetic configurations become maladaptive\nwhen environments change. Inspired by biological evolution, where adaptation to\nnew environments relies on gene mutation and recombination, we propose\nparameter dynamic evolution (PDE) to adapt to different images and mitigate the\ngene effect. PDE employs a parameter orthogonal generation technique and the\ncorresponding generated parameters to simulate gene recombination and gene\nmutation, separately. Experiments validate the effectiveness of our techniques.\nThe code will be released to the public.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53c2\u6570\u52a8\u6001\u6f14\u5316\uff08PDE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u57fa\u56e0\u91cd\u7ec4\u548c\u7a81\u53d8\u6765\u89e3\u51b3\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u9759\u6001\u53c2\u6570\u95ee\u9898\uff08\u57fa\u56e0\u6548\u5e94\uff09\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u968f\u673a\u53c2\u6570\u6709\u65f6\u4f18\u4e8e\u5b66\u4e60\u53c2\u6570\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\uff08\u57fa\u56e0\u6548\u5e94\uff09\uff0c\u53d7\u751f\u7269\u8fdb\u5316\u542f\u53d1\uff0c\u63d0\u51fa\u52a8\u6001\u8c03\u6574\u53c2\u6570\u4ee5\u9002\u5e94\u4e0d\u540c\u56fe\u50cf\u3002", "method": "\u91c7\u7528\u53c2\u6570\u6b63\u4ea4\u751f\u6210\u6280\u672f\u6a21\u62df\u57fa\u56e0\u91cd\u7ec4\u548c\u7a81\u53d8\uff0c\u5b9e\u73b0\u53c2\u6570\u52a8\u6001\u6f14\u5316\uff08PDE\uff09\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PDE\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u663e\u8457\u63d0\u5347\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6027\u80fd\u3002", "conclusion": "PDE\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u53c2\u6570\u8c03\u6574\u89e3\u51b3\u4e86\u57fa\u56e0\u6548\u5e94\u95ee\u9898\uff0c\u4e3a\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.09251", "pdf": "https://arxiv.org/pdf/2505.09251", "abs": "https://arxiv.org/abs/2505.09251", "authors": ["Vineetha Joy", "Aditya Anand", "Nidhi", "Anshuman Kumar", "Amit Sethi", "Hema Singh"], "title": "A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures", "categories": ["cs.CV"], "comment": null, "summary": "Metasurface-based radar absorbing structures (RAS) are highly preferred for\napplications like stealth technology, electromagnetic (EM) shielding, etc. due\nto their capability to achieve frequency selective absorption characteristics\nwith minimal thickness and reduced weight penalty. However, the conventional\napproach for the EM design and optimization of these structures relies on\nforward simulations, using full wave simulation tools, to predict the\nelectromagnetic (EM) response of candidate meta atoms. This process is\ncomputationally intensive, extremely time consuming and requires exploration of\nlarge design spaces. To overcome this challenge, we propose a surrogate model\nthat significantly accelerates the prediction of EM responses of multi-layered\nmetasurface-based RAS. A convolutional neural network (CNN) based architecture\nwith Huber loss function has been employed to estimate the reflection\ncharacteristics of the RAS model. The proposed model achieved a cosine\nsimilarity of 99.9% and a mean square error of 0.001 within 1000 epochs of\ntraining. The efficiency of the model has been established via full wave\nsimulations as well as experiment where it demonstrated significant reduction\nin computational time while maintaining high predictive accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u5feb\u901f\u9884\u6d4b\u591a\u5c42\u8d85\u8868\u9762\u96f7\u8fbe\u5438\u6536\u7ed3\u6784\uff08RAS\uff09\u7684\u7535\u78c1\u54cd\u5e94\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\u548c\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u7684\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5168\u6ce2\u4eff\u771f\u5de5\u5177\u8fdb\u884c\u7535\u78c1\u8bbe\u8ba1\u4f18\u5316\uff0c\u8ba1\u7b97\u91cf\u5927\u4e14\u8017\u65f6\u3002", "method": "\u91c7\u7528\u57fa\u4e8eHuber\u635f\u5931\u51fd\u6570\u7684CNN\u67b6\u6784\u9884\u6d4bRAS\u7684\u53cd\u5c04\u7279\u6027\u3002", "result": "\u6a21\u578b\u57281000\u6b21\u8bad\u7ec3\u5468\u671f\u5185\u8fbe\u523099.9%\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c0.001\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u8ba1\u7b97\u65f6\u95f4\u663e\u8457\u51cf\u5c11\u4e14\u9884\u6d4b\u7cbe\u5ea6\u9ad8\u3002", "conclusion": "\u8be5\u4ee3\u7406\u6a21\u578b\u4e3a\u8d85\u8868\u9762RAS\u7684\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09252", "pdf": "https://arxiv.org/pdf/2505.09252", "abs": "https://arxiv.org/abs/2505.09252", "authors": ["Yinuo Wang", "Yue Zeng", "Kai Chen", "Cai Meng", "Chao Pan", "Zhouping Tang"], "title": "Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping", "categories": ["cs.CV"], "comment": null, "summary": "Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes\non non-contrast computed tomography is critical for prognosis prediction and\ntherapeutic decision-making, yet remains challenging due to low contrast and\nblurring boundaries. This study evaluates the performance of zero-shot\nmulti-modal large language models (MLLMs) compared to traditional deep learning\nmethods in ICH binary classification and subtyping. Methods: We utilized a\ndataset provided by RSNA, comprising 192 NCCT volumes. The study compares\nvarious MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,\nwith conventional deep learning models, including ResNet50 and Vision\nTransformer. Carefully crafted prompts were used to guide MLLMs in tasks such\nas ICH presence, subtype classification, localization, and volume estimation.\nResults: The results indicate that in the ICH binary classification task,\ntraditional deep learning models outperform MLLMs comprehensively. For subtype\nclassification, MLLMs also exhibit inferior performance compared to traditional\ndeep learning models, with Gemini 2.0 Flash achieving an macro-averaged\nprecision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While\nMLLMs excel in interactive capabilities, their overall accuracy in ICH\nsubtyping is inferior to deep networks. However, MLLMs enhance interpretability\nthrough language interactions, indicating potential in medical imaging\nanalysis. Future efforts will focus on model refinement and developing more\nprecise MLLMs to improve performance in three-dimensional medical image\nprocessing.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9885\u5185\u51fa\u8840\uff08ICH\uff09\u5206\u7c7b\u548c\u4e9a\u578b\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4f20\u7edf\u6a21\u578b\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8eMLLMs\u3002", "motivation": "\u9885\u5185\u51fa\u8840\u7684\u53ca\u65f6\u8bc6\u522b\u5bf9\u9884\u540e\u548c\u6cbb\u7597\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u56e0\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u4f4e\u548c\u8fb9\u754c\u6a21\u7cca\u800c\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30MLLMs\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528RSNA\u63d0\u4f9b\u7684192\u4e2aNCCT\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86GPT-4o\u3001Gemini 2.0 Flash\u7b49MLLMs\u4e0eResNet50\u3001Vision Transformer\u7b49\u4f20\u7edf\u6a21\u578b\u5728ICH\u5206\u7c7b\u548c\u4e9a\u578b\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728ICH\u4e8c\u5206\u7c7b\u548c\u4e9a\u578b\u8bc6\u522b\u4efb\u52a1\u4e2d\u5168\u9762\u4f18\u4e8eMLLMs\uff0cGemini 2.0 Flash\u7684\u5b8f\u5e73\u5747\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u5206\u522b\u4e3a0.41\u548c0.31\u3002", "conclusion": "MLLMs\u5728\u4ea4\u4e92\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u51c6\u786e\u7387\u4e0d\u53ca\u4f20\u7edf\u6a21\u578b\u3002\u672a\u6765\u5c06\u4f18\u5316MLLMs\u4ee5\u63d0\u9ad8\u5176\u5728\u4e09\u7ef4\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09256", "pdf": "https://arxiv.org/pdf/2505.09256", "abs": "https://arxiv.org/abs/2505.09256", "authors": ["Jaemin Jung", "Youngjoon Jang", "Joon Son Chung"], "title": "Test-Time Augmentation for Pose-invariant Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "The goal of this paper is to enhance face recognition performance by\naugmenting head poses during the testing phase. Existing methods often rely on\ntraining on frontalised images or learning pose-invariant representations, yet\nboth approaches typically require re-training and testing for each dataset,\ninvolving a substantial amount of effort. In contrast, this study proposes\nPose-TTA, a novel approach that aligns faces at inference time without\nadditional training. To achieve this, we employ a portrait animator that\ntransfers the source image identity into the pose of a driving image. Instead\nof frontalising a side-profile face -- which can introduce distortion --\nPose-TTA generates matching side-profile images for comparison, thereby\nreducing identity information loss. Furthermore, we propose a weighted feature\naggregation strategy to address any distortions or biases arising from the\nsynthetic data, thus enhancing the reliability of the augmented images.\nExtensive experiments on diverse datasets and with various pre-trained face\nrecognition models demonstrate that Pose-TTA consistently improves inference\nperformance. Moreover, our method is straightforward to integrate into existing\nface recognition pipelines, as it requires no retraining or fine-tuning of the\nunderlying recognition models.", "AI": {"tldr": "\u63d0\u51faPose-TTA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u9636\u6bb5\u589e\u5f3a\u5934\u90e8\u59ff\u6001\u63d0\u5347\u4eba\u8138\u8bc6\u522b\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u91cd\u65b0\u8bad\u7ec3\uff0c\u8017\u65f6\u8017\u529b\u3002", "method": "\u4f7f\u7528\u8096\u50cf\u52a8\u753b\u5668\u5728\u63a8\u7406\u65f6\u5bf9\u9f50\u4eba\u8138\uff0c\u751f\u6210\u5339\u914d\u7684\u4fa7\u8138\u56fe\u50cf\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u7279\u5f81\u805a\u5408\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePose-TTA\u80fd\u6301\u7eed\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u6d41\u7a0b\u4e2d\u3002", "conclusion": "Pose-TTA\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u4eba\u8138\u8bc6\u522b\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2505.09263", "pdf": "https://arxiv.org/pdf/2505.09263", "abs": "https://arxiv.org/abs/2505.09263", "authors": ["Guan Gui", "Bin-Bin Gao", "Jun Liu", "Chengjie Wang", "Yunsheng Wu"], "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ECCV 2024", "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnoGen\u7684\u5c11\u6837\u672c\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684\u5f02\u5e38\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u5f02\u5e38\u4e0e\u771f\u5b9e\u5f02\u5e38\u5b58\u5728\u8bed\u4e49\u5dee\u8ddd\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a1\uff09\u57fa\u4e8e\u5c11\u91cf\u771f\u5b9e\u5f02\u5e38\u5b66\u4e60\u5f02\u5e38\u5206\u5e03\uff1b2\uff09\u5229\u7528\u5d4c\u5165\u548c\u8fb9\u754c\u6846\u6307\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u5f02\u5e38\uff1b3\uff09\u63d0\u51fa\u5f31\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728MVTec\u6570\u636e\u96c6\u4e0a\uff0c\u5f02\u5e38\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5982DRAEM\u548cDesTSeg\u7684AU-PR\u6307\u6807\u5206\u522b\u63d0\u9ad85.8%\u548c1.5%\u3002", "conclusion": "AnoGen\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u771f\u5b9e\u5f02\u5e38\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09264", "pdf": "https://arxiv.org/pdf/2505.09264", "abs": "https://arxiv.org/abs/2505.09264", "authors": ["Bin-Bin Gao"], "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ECCV 2024", "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5f20\u6b63\u5e38\u56fe\u50cf\u63d0\u793a\uff08OneNIP\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u7edf\u4e00\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u7ec6\u5316\u5668\u6539\u8fdb\u50cf\u7d20\u7ea7\u5f02\u5e38\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u81ea\u6ce8\u610f\u529b\u91cd\u5efa\u6a21\u578b\u53ef\u80fd\u56e0\u9ad8\u4e00\u81f4\u6027\u5bfc\u81f4\u5bf9\u5f02\u5e38\u548c\u6b63\u5e38\u7279\u5f81\u7684\u5b8c\u7f8e\u91cd\u5efa\uff0c\u4ece\u800c\u65e0\u6cd5\u68c0\u6d4b\u5f02\u5e38\uff0c\u4e14\u5728\u4f4e\u5206\u8fa8\u7387\u6f5c\u5728\u7a7a\u95f4\u4e2d\u91cd\u5efa\u5bfc\u81f4\u5f02\u5e38\u5206\u5272\u4e0d\u51c6\u786e\u3002", "method": "\u4f7f\u7528OneNIP\u91cd\u5efa\u6b63\u5e38\u7279\u5f81\u5e76\u6062\u590d\u5f02\u5e38\u7279\u5f81\uff0c\u540c\u65f6\u5f15\u5165\u76d1\u7763\u7ec6\u5316\u5668\u56de\u5f52\u91cd\u5efa\u8bef\u5dee\u3002", "result": "\u5728MVTec\u3001BTAD\u548cVisA\u4e09\u4e2a\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "OneNIP\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7edf\u4e00\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u5f02\u5e38\u5206\u5272\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.09265", "pdf": "https://arxiv.org/pdf/2505.09265", "abs": "https://arxiv.org/abs/2505.09265", "authors": ["Bin-Bin Gao"], "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by NeurIPS 2024", "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7eaf\u89c6\u89c9\u57fa\u7840\u6a21\u578bMetaUAS\uff0c\u7528\u4e8e\u901a\u7528\u89c6\u89c9\u5f02\u5e38\u5206\u5272\uff0c\u65e0\u9700\u4f9d\u8d56\u8bed\u8a00\u6a21\u578b\u6216\u7279\u6b8a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u3002", "motivation": "\u89c6\u89c9\u8868\u793a\u4e0e\u8bed\u8a00\u65e0\u5173\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8bed\u8a00\u6a21\u578b\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002", "method": "\u5c06\u5f02\u5e38\u5206\u5272\u7edf\u4e00\u4e3a\u53d8\u5316\u5206\u5272\uff0c\u5229\u7528\u5408\u6210\u56fe\u50cf\u5bf9\u8bad\u7ec3MetaUAS\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u8f6f\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u5904\u7406\u51e0\u4f55\u53d8\u5316\u3002", "result": "MetaUAS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5168\u6837\u672c\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "MetaUAS\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\uff0c\u4ec5\u9700\u4e00\u5f20\u6b63\u5e38\u56fe\u50cf\u63d0\u793a\u3002"}}
{"id": "2505.09274", "pdf": "https://arxiv.org/pdf/2505.09274", "abs": "https://arxiv.org/abs/2505.09274", "authors": ["Fares Bougourzi", "Abdenour Hadid"], "title": "Recent Advances in Medical Imaging Segmentation: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Medical imaging is a cornerstone of modern healthcare, driving advancements\nin diagnosis, treatment planning, and patient care. Among its various tasks,\nsegmentation remains one of the most challenging problem due to factors such as\ndata accessibility, annotation complexity, structural variability, variation in\nmedical imaging modalities, and privacy constraints. Despite recent progress,\nachieving robust generalization and domain adaptation remains a significant\nhurdle, particularly given the resource-intensive nature of some proposed\nmodels and their reliance on domain expertise. This survey explores\ncutting-edge advancements in medical image segmentation, focusing on\nmethodologies such as Generative AI, Few-Shot Learning, Foundation Models, and\nUniversal Models. These approaches offer promising solutions to longstanding\nchallenges. We provide a comprehensive overview of the theoretical foundations,\nstate-of-the-art techniques, and recent applications of these methods. Finally,\nwe discuss inherent limitations, unresolved issues, and future research\ndirections aimed at enhancing the practicality and accessibility of\nsegmentation models in medical imaging. We are maintaining a\n\\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub\nRepository} to continue tracking and updating innovations in this field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u3001\u5c11\u6837\u672c\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\u548c\u901a\u7528\u6a21\u578b\u7b49\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u3001\u6807\u6ce8\u590d\u6742\u6027\u3001\u7ed3\u6784\u53d8\u5f02\u6027\u3001\u6a21\u6001\u591a\u6837\u6027\u548c\u9690\u79c1\u7ea6\u675f\u7b49\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u5728\u6cdb\u5316\u548c\u9886\u57df\u9002\u5e94\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u7efc\u8ff0\u4e86\u751f\u6210\u5f0fAI\u3001\u5c11\u6837\u672c\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\u548c\u901a\u7528\u6a21\u578b\u7b49\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u7406\u8bba\u57fa\u7840\u548c\u5e94\u7528\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u4e3a\u89e3\u51b3\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u548c\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u81f4\u529b\u4e8e\u63d0\u5347\u5206\u5272\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u540c\u65f6\u7ef4\u62a4\u4e00\u4e2aGitHub\u4ed3\u5e93\u4ee5\u6301\u7eed\u8ddf\u8e2a\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002"}}
{"id": "2505.09306", "pdf": "https://arxiv.org/pdf/2505.09306", "abs": "https://arxiv.org/abs/2505.09306", "authors": ["Thijs L van der Plas", "Stephen Law", "Michael JO Pocock"], "title": "Predicting butterfly species presence from satellite imagery using soft contrastive regularisation", "categories": ["cs.CV", "cs.LG"], "comment": "To be published in the 2025 CVPR FGVC12 workshop", "summary": "The growing demand for scalable biodiversity monitoring methods has fuelled\ninterest in remote sensing data, due to its widespread availability and\nextensive coverage. Traditionally, the application of remote sensing to\nbiodiversity research has focused on mapping and monitoring habitats, but with\nincreasing availability of large-scale citizen-science wildlife observation\ndata, recent methods have started to explore predicting multi-species presence\ndirectly from satellite images. This paper presents a new data set for\npredicting butterfly species presence from satellite data in the United\nKingdom. We experimentally optimise a Resnet-based model to predict\nmulti-species presence from 4-band satellite images, and find that this model\nespecially outperforms the mean rate baseline for locations with high species\nbiodiversity. To improve performance, we develop a soft, supervised contrastive\nregularisation loss that is tailored to probabilistic labels (such as\nspecies-presence data), and demonstrate that this improves prediction accuracy.\nIn summary, our new data set and contrastive regularisation method contribute\nto the open challenge of accurately predicting species biodiversity from remote\nsensing data, which is key for efficient biodiversity monitoring.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u536b\u661f\u56fe\u50cf\u9884\u6d4b\u8774\u8776\u7269\u79cd\u5b58\u5728\u7684\u65b0\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316Resnet\u6a21\u578b\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\u635f\u5931\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u5bf9\u53ef\u6269\u5c55\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u65b9\u6cd5\u7684\u9700\u6c42\u589e\u52a0\uff0c\u7ed3\u5408\u9065\u611f\u6570\u636e\u548c\u516c\u6c11\u79d1\u5b66\u89c2\u6d4b\u6570\u636e\uff0c\u63a2\u7d22\u76f4\u63a5\u4ece\u536b\u661f\u56fe\u50cf\u9884\u6d4b\u591a\u7269\u79cd\u5b58\u5728\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u75284\u6ce2\u6bb5\u536b\u661f\u56fe\u50cf\uff0c\u4f18\u5316Resnet\u6a21\u578b\u9884\u6d4b\u591a\u7269\u79cd\u5b58\u5728\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u9488\u5bf9\u6982\u7387\u6807\u7b7e\u7684\u8f6f\u76d1\u7763\u5bf9\u6bd4\u6b63\u5219\u5316\u635f\u5931\u3002", "result": "\u6a21\u578b\u5728\u7269\u79cd\u591a\u6837\u6027\u9ad8\u7684\u5730\u533a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5bf9\u6bd4\u6b63\u5219\u5316\u635f\u5931\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u65b0\u6570\u636e\u96c6\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\u65b9\u6cd5\u4e3a\u4ece\u9065\u611f\u6570\u636e\u51c6\u786e\u9884\u6d4b\u751f\u7269\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u9ad8\u6548\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u3002"}}
{"id": "2505.09324", "pdf": "https://arxiv.org/pdf/2505.09324", "abs": "https://arxiv.org/abs/2505.09324", "authors": ["Lakshya Gupta", "Imran N. Junejo"], "title": "Neural Video Compression using 2D Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "9 pages, 8 figures", "summary": "The computer vision and image processing research community has been involved\nin standardizing video data communications for the past many decades, leading\nto standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent\ngroundbreaking works have focused on employing deep learning-based techniques\nto replace the traditional video codec pipeline to a greater affect. Neural\nvideo codecs (NVC) create an end-to-end ML-based solution that does not rely on\nany handcrafted features (motion or edge-based) and have the ability to learn\ncontent-aware compression strategies, offering better adaptability and higher\ncompression efficiency than traditional methods. This holds a great potential\nnot only for hardware design, but also for various video streaming platforms\nand applications, especially video conferencing applications such as MS-Teams\nor Zoom that have found extensive usage in classrooms and workplaces. However,\ntheir high computational demands currently limit their use in real-time\napplications like video conferencing. To address this, we propose a\nregion-of-interest (ROI) based neural video compression model that leverages 2D\nGaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable\nof real-time decoding and can be optimized using fewer data points, requiring\nonly thousands of Gaussians for decent quality outputs as opposed to millions\nin 3D scenes. In this work, we designed a video pipeline that speeds up the\nencoding time of the previous Gaussian splatting-based image codec by 88% by\nusing a content-aware initialization strategy paired with a novel Gaussian\ninter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be\nused for a video-codec solution, the first of its kind solution in this neural\nvideo codec space.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u57df\u5174\u8da3\uff08ROI\uff09\u7684\u795e\u7ecf\u89c6\u9891\u538b\u7f29\u6a21\u578b\uff0c\u5229\u75282D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u89e3\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u7f16\u89e3\u7801\u6807\u51c6\uff08\u5982AVC\u3001HEVC\u7b49\uff09\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\uff0c\u800c\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u5668\uff08NVC\uff09\u901a\u8fc7\u5b66\u4e60\u5185\u5bb9\u611f\u77e5\u538b\u7f29\u7b56\u7565\u63d0\u4f9b\u66f4\u9ad8\u6548\u7387\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u91c7\u75282D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u7ed3\u5408\u5185\u5bb9\u611f\u77e5\u521d\u59cb\u5316\u7b56\u7565\u548c\u5e27\u95f4\u5197\u4f59\u51cf\u5c11\u673a\u5236\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u89c6\u9891\u7f16\u89e3\u7801\u65b9\u6848\u3002", "result": "\u7f16\u7801\u901f\u5ea6\u63d0\u5347\u4e8688%\uff0c\u9996\u6b21\u5c06\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5e94\u7528\u4e8e\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u9886\u57df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u89c6\u9891\u5e94\u7528\uff08\u5982\u89c6\u9891\u4f1a\u8bae\uff09\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09329", "pdf": "https://arxiv.org/pdf/2505.09329", "abs": "https://arxiv.org/abs/2505.09329", "authors": ["Jiarun Liu", "Hong-Yu Zhou", "Weijian Huang", "Hao Yang", "Dongning Song", "Tao Tan", "Yong Liang", "Shanshan Wang"], "title": "BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "Scaling up model and data size have demonstrated impressive performance\nimprovement over a wide range of tasks. Despite extensive studies on scaling\nbehaviors for general-purpose tasks, medical images exhibit substantial\ndifferences from natural data. It remains unclear the key factors in developing\nmedical vision foundation models at scale due to the absence of an extensive\nunderstanding of scaling behavior in the medical domain. In this paper, we\nexplored the scaling behavior across model sizes, training algorithms, data\nsizes, and imaging modalities in developing scalable medical vision foundation\nmodels by self-supervised learning. To support scalable pretraining, we\nintroduce BioVFM-21M, a large-scale biomedical image dataset encompassing a\nwide range of biomedical image modalities and anatomies. We observed that\nscaling up does provide benefits but varies across tasks. Additional analysis\nreveals several factors correlated with scaling benefits. Finally, we propose\nBioVFM, a large-scale medical vision foundation model pretrained on 21 million\nbiomedical images, which outperforms the previous state-of-the-art foundation\nmodels across 12 medical benchmarks. Our results highlight that while scaling\nup is beneficial for pursuing better performance, task characteristics, data\ndiversity, pretraining methods, and computational efficiency remain critical\nconsiderations for developing scalable medical foundation models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u533b\u7597\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u89c4\u6a21\u5316\u884c\u4e3a\uff0c\u53d1\u73b0\u89c4\u6a21\u5316\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u76ca\u4f46\u6548\u679c\u56e0\u4efb\u52a1\u800c\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86BioVFM\u6a21\u578b\uff0c\u572812\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u533b\u7597\u56fe\u50cf\u4e0e\u81ea\u7136\u6570\u636e\u5dee\u5f02\u663e\u8457\uff0c\u4f46\u89c4\u6a21\u5316\u884c\u4e3a\u5728\u533b\u7597\u9886\u57df\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u660e\u786e\u5173\u952e\u56e0\u7d20\u4ee5\u5f00\u53d1\u89c4\u6a21\u5316\u533b\u7597\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u7814\u7a76\u4e86\u6a21\u578b\u5927\u5c0f\u3001\u8bad\u7ec3\u7b97\u6cd5\u3001\u6570\u636e\u89c4\u6a21\u548c\u6210\u50cf\u6a21\u6001\u7684\u89c4\u6a21\u5316\u884c\u4e3a\uff0c\u5e76\u6784\u5efa\u4e86BioVFM-21M\u6570\u636e\u96c6\u652f\u6301\u9884\u8bad\u7ec3\u3002", "result": "BioVFM\u6a21\u578b\u572812\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u6a21\u578b\uff0c\u4f46\u89c4\u6a21\u5316\u6548\u679c\u56e0\u4efb\u52a1\u800c\u5f02\uff0c\u9700\u8003\u8651\u4efb\u52a1\u7279\u6027\u3001\u6570\u636e\u591a\u6837\u6027\u7b49\u56e0\u7d20\u3002", "conclusion": "\u89c4\u6a21\u5316\u867d\u6709\u76ca\u4e8e\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u4efb\u52a1\u7279\u6027\u3001\u6570\u636e\u591a\u6837\u6027\u3001\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u8ba1\u7b97\u6548\u7387\u4ecd\u662f\u5f00\u53d1\u533b\u7597\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2505.09336", "pdf": "https://arxiv.org/pdf/2505.09336", "abs": "https://arxiv.org/abs/2505.09336", "authors": ["Muzammil Behzad"], "title": "Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce MultiviewVLM, a vision-language model designed\nfor unsupervised contrastive multiview representation learning of facial\nemotions from 3D/4D data. Our architecture integrates pseudo-labels derived\nfrom generated textual prompts to guide implicit alignment of emotional\nsemantics. To capture shared information across multi-views, we propose a joint\nembedding space that aligns multiview representations without requiring\nexplicit supervision. We further enhance the discriminability of our model\nthrough a novel multiview contrastive learning strategy that leverages stable\npositive-negative pair sampling. A gradient-friendly loss function is\nintroduced to promote smoother and more stable convergence, and the model is\noptimized for distributed training to ensure scalability. Extensive experiments\ndemonstrate that MultiviewVLM outperforms existing state-of-the-art methods and\ncan be easily adapted to various real-world applications with minimal\nmodifications.", "AI": {"tldr": "MultiviewVLM\u662f\u4e00\u79cd\u7528\u4e8e\u65e0\u76d1\u7763\u5bf9\u6bd4\u591a\u89c6\u89d2\u8868\u793a\u5b66\u4e60\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e3D/4D\u9762\u90e8\u60c5\u7eea\u6570\u636e\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u548c\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u89c6\u89d2\u9762\u90e8\u60c5\u7eea\u6570\u636e\u7684\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u548c\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\u3002", "method": "\u6a21\u578b\u7ed3\u5408\u4f2a\u6807\u7b7e\u548c\u6587\u672c\u63d0\u793a\uff0c\u63d0\u51fa\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u548c\u591a\u89c6\u89d2\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u68af\u5ea6\u53cb\u597d\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMultiviewVLM\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6613\u4e8e\u9002\u5e94\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "MultiviewVLM\u5728\u591a\u89c6\u89d2\u60c5\u7eea\u8868\u793a\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.09358", "pdf": "https://arxiv.org/pdf/2505.09358", "abs": "https://arxiv.org/abs/2505.09358", "authors": ["Bingxin Ke", "Kevin Qu", "Tianfu Wang", "Nando Metzger", "Shengyu Huang", "Bo Li", "Anton Obukhov", "Konrad Schindler"], "title": "Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis", "categories": ["cs.CV", "cs.LG"], "comment": "Journal extension of our CVPR 2024 paper, featuring new tasks,\n  improved efficiency, high-resolution capabilities, and enhanced accessibility", "summary": "The success of deep learning in computer vision over the past decade has\nhinged on large labeled datasets and strong pretrained models. In data-scarce\nsettings, the quality of these pretrained models becomes crucial for effective\ntransfer learning. Image classification and self-supervised learning have\ntraditionally been the primary methods for pretraining CNNs and\ntransformer-based architectures. Recently, the rise of text-to-image generative\nmodels, particularly those using denoising diffusion in a latent space, has\nintroduced a new class of foundational models trained on massive, captioned\nimage datasets. These models' ability to generate realistic images of unseen\ncontent suggests they possess a deep understanding of the visual world. In this\nwork, we present Marigold, a family of conditional generative models and a\nfine-tuning protocol that extracts the knowledge from pretrained latent\ndiffusion models like Stable Diffusion and adapts them for dense image analysis\ntasks, including monocular depth estimation, surface normals prediction, and\nintrinsic decomposition. Marigold requires minimal modification of the\npre-trained latent diffusion model's architecture, trains with small synthetic\ndatasets on a single GPU over a few days, and demonstrates state-of-the-art\nzero-shot generalization. Project page:\nhttps://marigoldcomputervision.github.io", "AI": {"tldr": "Marigold\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u8fdb\u884c\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff0c\u5982\u6df1\u5ea6\u4f30\u8ba1\u548c\u8868\u9762\u6cd5\u7ebf\u9884\u6d4b\uff0c\u901a\u8fc7\u5fae\u8c03\u534f\u8bae\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8d28\u91cf\u5bf9\u8fc1\u79fb\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08\u5982\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff09\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5c1a\u672a\u88ab\u5145\u5206\u5229\u7528\u4e8e\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u3002", "method": "\u63d0\u51faMarigold\uff0c\u4e00\u79cd\u6761\u4ef6\u751f\u6210\u6a21\u578b\u548c\u5fae\u8c03\u534f\u8bae\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u67b6\u6784\u4fee\u6539\uff0c\u5229\u7528\u5c0f\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u5728\u5355GPU\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "Marigold\u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u8868\u9762\u6cd5\u7ebf\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Marigold\u5c55\u793a\u4e86\u4ece\u751f\u6210\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\u7528\u4e8e\u5bc6\u96c6\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09368", "pdf": "https://arxiv.org/pdf/2505.09368", "abs": "https://arxiv.org/abs/2505.09368", "authors": ["Jenny Schmalfuss", "Victor Oei", "Lukas Mehl", "Madlen Bartsch", "Shashank Agnihotri", "Margret Keuper", "Andr\u00e9s Bruhn"], "title": "RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Standard benchmarks for optical flow, scene flow, and stereo vision\nalgorithms generally focus on model accuracy rather than robustness to image\ncorruptions like noise or rain. Hence, the resilience of models to such\nreal-world perturbations is largely unquantified. To address this, we present\nRobustSpring, a comprehensive dataset and benchmark for evaluating robustness\nto image corruptions for optical flow, scene flow, and stereo models.\nRobustSpring applies 20 different image corruptions, including noise, blur,\ncolor changes, quality degradations, and weather distortions, in a time-,\nstereo-, and depth-consistent manner to the high-resolution Spring dataset,\ncreating a suite of 20,000 corrupted images that reflect challenging\nconditions. RobustSpring enables comparisons of model robustness via a new\ncorruption robustness metric. Integration with the Spring benchmark enables\npublic two-axis evaluations of both accuracy and robustness. We benchmark a\ncurated selection of initial models, observing that accurate models are not\nnecessarily robust and that robustness varies widely by corruption type.\nRobustSpring is a new computer vision benchmark that treats robustness as a\nfirst-class citizen to foster models that combine accuracy with resilience. It\nwill be available at https://spring-benchmark.org.", "AI": {"tldr": "RobustSpring\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5149\u6d41\u3001\u573a\u666f\u6d41\u548c\u7acb\u4f53\u89c6\u89c9\u6a21\u578b\u5bf9\u56fe\u50cf\u635f\u574f\uff08\u5982\u566a\u58f0\u6216\u96e8\uff09\u9c81\u68d2\u6027\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u9700\u8981\u91cf\u5316\u6a21\u578b\u5bf9\u8fd9\u4e9b\u6270\u52a8\u7684\u62b5\u6297\u529b\u3002", "method": "\u901a\u8fc7\u5728\u9ad8\u5206\u8fa8\u7387Spring\u6570\u636e\u96c6\u4e0a\u5e94\u752820\u79cd\u4e0d\u540c\u7684\u56fe\u50cf\u635f\u574f\uff08\u5982\u566a\u58f0\u3001\u6a21\u7cca\u3001\u989c\u8272\u53d8\u5316\u7b49\uff09\uff0c\u751f\u621020,000\u5f20\u635f\u574f\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684\u9c81\u68d2\u6027\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u6d4b\u8bd5\u53d1\u73b0\u51c6\u786e\u6027\u9ad8\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u9c81\u68d2\uff0c\u4e14\u9c81\u68d2\u6027\u56e0\u635f\u574f\u7c7b\u578b\u800c\u5f02\u3002", "conclusion": "RobustSpring\u5c06\u9c81\u68d2\u6027\u4f5c\u4e3a\u9996\u8981\u76ee\u6807\uff0c\u65e8\u5728\u63a8\u52a8\u517c\u5177\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u7684\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2505.09372", "pdf": "https://arxiv.org/pdf/2505.09372", "abs": "https://arxiv.org/abs/2505.09372", "authors": ["Siyuan Yan", "Xieji Li", "Ming Hu", "Yiwen Jiang", "Zhen Yu", "Zongyuan Ge"], "title": "MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment", "categories": ["cs.CV"], "comment": "MICCAI2025 early acceptance; First two authors contribute equally", "summary": "Dermatological diagnosis represents a complex multimodal challenge that\nrequires integrating visual features with specialized clinical knowledge. While\nvision-language pretraining (VLP) has advanced medical AI, its effectiveness in\ndermatology is limited by text length constraints and the lack of structured\ntexts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced\nvision-language pretraining framework for zero-shot dermatological tasks.\nRecognizing that comprehensive dermatological descriptions require multiple\nknowledge aspects that exceed standard text constraints, our framework\nintroduces: (1) a multi-aspect contrastive learning strategy that decomposes\nclinical narratives into knowledge-enhanced sub-texts through large language\nmodels, (2) a fine-grained alignment mechanism that connects subcaptions with\ndiagnostically relevant image features, and (3) a diagnosis-guided weighting\nscheme that adaptively prioritizes different sub-captions based on clinical\nsignificance prior. Through pretraining on 403,563 dermatological image-text\npairs collected from education resources, MAKE significantly outperforms\nstate-of-the-art VLP models on eight datasets across zero-shot skin disease\nclassification, concept annotation, and cross-modal retrieval tasks. Our code\nwill be made publicly available at https: //github.com/SiyuanYan1/MAKE.", "AI": {"tldr": "MAKE\u662f\u4e00\u4e2a\u591a\u77e5\u8bc6\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u76ae\u80a4\u75c5\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u89e3\u4e34\u5e8a\u53d9\u8ff0\u3001\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u8bca\u65ad\u5f15\u5bfc\u52a0\u6743\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709VLP\u6a21\u578b\u3002", "motivation": "\u76ae\u80a4\u75c5\u8bca\u65ad\u9700\u8981\u6574\u5408\u89c6\u89c9\u7279\u5f81\u548c\u4e34\u5e8a\u77e5\u8bc6\uff0c\u4f46\u73b0\u6709VLP\u65b9\u6cd5\u56e0\u6587\u672c\u957f\u5ea6\u9650\u5236\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u800c\u6548\u679c\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u591a\u77e5\u8bc6\u589e\u5f3a\u6846\u67b6MAKE\uff0c\u5305\u62ec\u591a\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u3001\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u673a\u5236\u548c\u8bca\u65ad\u5f15\u5bfc\u52a0\u6743\u65b9\u6848\u3002", "result": "\u5728403,563\u4e2a\u76ae\u80a4\u75c5\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u9884\u8bad\u7ec3\u540e\uff0cMAKE\u5728\u96f6\u6837\u672c\u5206\u7c7b\u3001\u6982\u5ff5\u6807\u6ce8\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709VLP\u6a21\u578b\u3002", "conclusion": "MAKE\u901a\u8fc7\u591a\u77e5\u8bc6\u589e\u5f3a\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u76ae\u80a4\u75c5\u8bca\u65ad\u4e2d\u7684\u591a\u6a21\u6001\u6311\u6218\u3002"}}
{"id": "2505.09379", "pdf": "https://arxiv.org/pdf/2505.09379", "abs": "https://arxiv.org/abs/2505.09379", "authors": ["Ali Rida Sahili", "Najett Neji", "Hedi Tabia"], "title": "Text-driven Motion Generation: Overview, Challenges and Directions", "categories": ["cs.CV"], "comment": "17 pages, 5 tables", "summary": "Text-driven motion generation offers a powerful and intuitive way to create\nhuman movements directly from natural language. By removing the need for\npredefined motion inputs, it provides a flexible and accessible approach to\ncontrolling animated characters. This makes it especially useful in areas like\nvirtual reality, gaming, human-computer interaction, and robotics. In this\nreview, we first revisit the traditional perspective on motion synthesis, where\nmodels focused on predicting future poses from observed initial sequences,\noften conditioned on action labels. We then provide a comprehensive and\nstructured survey of modern text-to-motion generation approaches, categorizing\nthem from two complementary perspectives: (i) architectural, dividing methods\ninto VAE-based, diffusion-based, and hybrid models; and (ii) motion\nrepresentation, distinguishing between discrete and continuous motion\ngeneration strategies. In addition, we explore the most widely used datasets,\nevaluation methods, and recent benchmarks that have shaped progress in this\narea. With this survey, we aim to capture where the field currently stands,\nbring attention to its key challenges and limitations, and highlight promising\ndirections for future exploration. We hope this work offers a valuable starting\npoint for researchers and practitioners working to push the boundaries of\nlanguage-driven human motion synthesis.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u5206\u7c7b\u4e86\u67b6\u6784\u548c\u8fd0\u52a8\u8868\u793a\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63d0\u4f9b\u7075\u6d3b\u3001\u76f4\u89c2\u7684\u8fd0\u52a8\u751f\u6210\u65b9\u5f0f\uff0c\u9002\u7528\u4e8e\u865a\u62df\u73b0\u5b9e\u3001\u6e38\u620f\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u3002", "method": "\u4ece\u67b6\u6784\uff08VAE\u3001\u6269\u6563\u3001\u6df7\u5408\u6a21\u578b\uff09\u548c\u8fd0\u52a8\u8868\u793a\uff08\u79bb\u6563\u3001\u8fde\u7eed\uff09\u4e24\u4e2a\u89d2\u5ea6\u5206\u7c7b\u73b0\u4ee3\u65b9\u6cd5\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u9886\u57df\u7684\u8fdb\u5c55\u3001\u6311\u6218\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u4e3a\u8bed\u8a00\u9a71\u52a8\u8fd0\u52a8\u5408\u6210\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d77\u70b9\u3002"}}
{"id": "2505.09380", "pdf": "https://arxiv.org/pdf/2505.09380", "abs": "https://arxiv.org/abs/2505.09380", "authors": ["Qinghui Liu", "Jon Nesvold", "Hanna Raaum", "Elakkyen Murugesu", "Martin R\u00f8vang", "Bradley J Maclntosh", "Atle Bj\u00f8rnerud", "Karoline Skogen"], "title": "Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "19 pages, 11 figures, on submission to BMC Methods", "summary": "Background: There are many challenges and opportunities in the clinical\ndeployment of AI tools in radiology. The current study describes a radiology\nsoftware platform called NeoMedSys that can enable efficient deployment and\nrefinements of AI models. We evaluated the feasibility and effectiveness of\nrunning NeoMedSys for three months in real-world clinical settings and focused\non improvement performance of an in-house developed AI model (VIOLA-AI)\ndesigned for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI\nmodels with a web-based medical image viewer, annotation system, and\nhospital-wide radiology information systems. A pragmatic investigation was\ndeployed using clinical cases of patients presenting to the largest Emergency\nDepartment in Norway (site-1) with suspected traumatic brain injury (TBI) or\npatients with suspected stroke (site-2). We assessed ICH classification\nperformance as VIOLA-AI encountered new data and underwent pre-planned model\nretraining. Performance metrics included sensitivity, specificity, accuracy,\nand the area under the receiver operating characteristic curve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model,\nsignificantly enhancing its diagnostic accuracy. Automated bleed detection and\nsegmentation were reviewed in near real-time to facilitate re-training\nVIOLA-AI. The iterative refinement process yielded a marked improvement in\nclassification sensitivity, rising to 90.3% (from 79.2%), and specificity that\nreached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire\nsample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).\nModel refinement stages were associated with notable gains, highlighting the\nvalue of real-time radiologist feedback.", "AI": {"tldr": "NeoMedSys\u662f\u4e00\u4e2a\u653e\u5c04\u5b66\u8f6f\u4ef6\u5e73\u53f0\uff0c\u7528\u4e8e\u9ad8\u6548\u90e8\u7f72\u548c\u4f18\u5316AI\u6a21\u578b\uff0c\u901a\u8fc7\u5b9e\u9645\u4e34\u5e8a\u6d4b\u8bd5\u663e\u8457\u63d0\u5347\u4e86ICH\u68c0\u6d4b\u6a21\u578bVIOLA-AI\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3AI\u5de5\u5177\u5728\u653e\u5c04\u5b66\u4e34\u5e8a\u90e8\u7f72\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347AI\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "NeoMedSys\u6574\u5408\u4e86AI\u6a21\u578b\u90e8\u7f72\u3001\u6d4b\u8bd5\u548c\u4f18\u5316\u5de5\u5177\uff0c\u7ed3\u5408\u533b\u7597\u56fe\u50cf\u67e5\u770b\u5668\u548c\u6ce8\u91ca\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u9645\u75c5\u4f8b\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "VIOLA-AI\u7684\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u663e\u8457\u63d0\u9ad8\uff0cAUC\u8fbe\u52300.949\uff0c\u8868\u660e\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u5316\u3002", "conclusion": "NeoMedSys\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u6a21\u578b\u7684\u4e34\u5e8a\u8bca\u65ad\u80fd\u529b\u3002"}}
{"id": "2505.09385", "pdf": "https://arxiv.org/pdf/2505.09385", "abs": "https://arxiv.org/abs/2505.09385", "authors": ["Xiaoyang Yu", "Xiaoming Wu", "Xin Wang", "Dongrun Li", "Ming Yang", "Peng Cheng"], "title": "FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Federated semantic segmentation enables pixel-level classification in images\nthrough collaborative learning while maintaining data privacy. However,\nexisting research commonly overlooks the fine-grained class relationships\nwithin the semantic space when addressing heterogeneous problems, particularly\ndomain shift. This oversight results in ambiguities between class\nrepresentation. To overcome this challenge, we propose a novel federated\nsegmentation framework that strikes class consistency, termed FedSaaS.\nSpecifically, we introduce class exemplars as a criterion for both local- and\nglobal-level class representations. On the server side, the uploaded class\nexemplars are leveraged to model class prototypes, which supervise global\nbranch of clients, ensuring alignment with global-level representation. On the\nclient side, we incorporate an adversarial mechanism to harmonize contributions\nof global and local branches, leading to consistent output. Moreover,\nmultilevel contrastive losses are employed on both sides to enforce consistency\nbetween two-level representations in the same semantic space. Extensive\nexperiments on several driving scene segmentation datasets demonstrate that our\nframework outperforms state-of-the-art methods, significantly improving average\nsegmentation accuracy and effectively addressing the class-consistency\nrepresentation problem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedSaaS\u7684\u65b0\u578b\u8054\u90a6\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u6837\u672c\u548c\u5bf9\u6297\u673a\u5236\u89e3\u51b3\u5f02\u6784\u95ee\u9898\u548c\u7c7b\u4e00\u81f4\u6027\u8868\u793a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5904\u7406\u5f02\u6784\u95ee\u9898\uff08\u5982\u57df\u504f\u79fb\uff09\u65f6\u5ffd\u7565\u4e86\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7c7b\u5173\u7cfb\uff0c\u5bfc\u81f4\u7c7b\u8868\u793a\u6a21\u7cca\u3002", "method": "\u5f15\u5165\u7c7b\u6837\u672c\u4f5c\u4e3a\u672c\u5730\u548c\u5168\u5c40\u7c7b\u8868\u793a\u7684\u51c6\u5219\uff0c\u670d\u52a1\u5668\u7aef\u5efa\u6a21\u7c7b\u539f\u578b\u76d1\u7763\u5ba2\u6237\u7aef\u5168\u5c40\u5206\u652f\uff0c\u5ba2\u6237\u7aef\u901a\u8fc7\u5bf9\u6297\u673a\u5236\u534f\u8c03\u5168\u5c40\u548c\u672c\u5730\u5206\u652f\uff0c\u5e76\u4f7f\u7528\u591a\u7ea7\u5bf9\u6bd4\u635f\u5931\u786e\u4fdd\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u9a7e\u9a76\u573a\u666f\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u5206\u5272\u7cbe\u5ea6\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u4e00\u81f4\u6027\u8868\u793a\u95ee\u9898\u3002", "conclusion": "FedSaaS\u6846\u67b6\u901a\u8fc7\u7c7b\u6837\u672c\u548c\u5bf9\u6297\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u7c7b\u4e00\u81f4\u6027\u8868\u793a\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.09406", "pdf": "https://arxiv.org/pdf/2505.09406", "abs": "https://arxiv.org/abs/2505.09406", "authors": ["Yue Wen", "Liang Song", "Yijia Liu", "Siting Zhu", "Yanzi Miao", "Lijun Han", "Hesheng Wang"], "title": "FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling", "categories": ["cs.CV"], "comment": "7 pages, 9 figures, accepted by ICRA2025", "summary": "Dynamic scene reconstruction for autonomous driving enables vehicles to\nperceive and interpret complex scene changes more precisely. Dynamic Neural\nRadiance Fields (NeRFs) have recently shown promising capability in scene\nmodeling. However, many existing methods rely heavily on accurate poses inputs\nand multi-sensor data, leading to increased system complexity. To address this,\nwe propose FreeDriveRF, which reconstructs dynamic driving scenes using only\nsequential RGB images without requiring poses inputs. We innovatively decouple\ndynamic and static parts at the early sampling level using semantic\nsupervision, mitigating image blurring and artifacts. To overcome the\nchallenges posed by object motion and occlusion in monocular camera, we\nintroduce a warped ray-guided dynamic object rendering consistency loss,\nutilizing optical flow to better constrain the dynamic modeling process.\nAdditionally, we incorporate estimated dynamic flow to constrain the pose\noptimization process, improving the stability and accuracy of unbounded scene\nreconstruction. Extensive experiments conducted on the KITTI and Waymo datasets\ndemonstrate the superior performance of our method in dynamic scene modeling\nfor autonomous driving.", "AI": {"tldr": "FreeDriveRF\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u9700RGB\u56fe\u50cf\u5e8f\u5217\u7684\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u65e0\u9700\u4f4d\u59ff\u8f93\u5165\uff0c\u901a\u8fc7\u8bed\u4e49\u76d1\u7763\u89e3\u8026\u52a8\u6001\u4e0e\u9759\u6001\u90e8\u5206\uff0c\u5e76\u5f15\u5165\u5149\u6d41\u7ea6\u675f\u52a8\u6001\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u4f4d\u59ff\u8f93\u5165\u548c\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u589e\u52a0\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u8bed\u4e49\u76d1\u7763\u65e9\u671f\u89e3\u8026\u52a8\u6001\u4e0e\u9759\u6001\u90e8\u5206\uff0c\u5f15\u5165\u5149\u6d41\u7ea6\u675f\u52a8\u6001\u5efa\u6a21\uff0c\u5e76\u5229\u7528\u52a8\u6001\u6d41\u4f18\u5316\u4f4d\u59ff\u3002", "result": "\u5728KITTI\u548cWaymo\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "FreeDriveRF\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7b80\u5316\u4e86\u7cfb\u7edf\u9700\u6c42\u3002"}}
{"id": "2505.09413", "pdf": "https://arxiv.org/pdf/2505.09413", "abs": "https://arxiv.org/abs/2505.09413", "authors": ["Ma Changfeng", "Bi Ran", "Guo Jie", "Wang Chongjun", "Guo Yanwen"], "title": "Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians", "categories": ["cs.CV"], "comment": "CVPR 2025 Accepted", "summary": "Current learning-based methods predict NeRF or 3D Gaussians from point clouds\nto achieve photo-realistic rendering but still depend on categorical priors,\ndense point clouds, or additional refinements. Hence, we introduce a novel\npoint cloud rendering method by predicting 2D Gaussians from point clouds. Our\nmethod incorporates two identical modules with an entire-patch architecture\nenabling the network to be generalized to multiple datasets. The module\nnormalizes and initializes the Gaussians utilizing the point cloud information\nincluding normals, colors and distances. Then, splitting decoders are employed\nto refine the initial Gaussians by duplicating them and predicting more\naccurate results, making our methodology effectively accommodate sparse point\nclouds as well. Once trained, our approach exhibits direct generalization to\npoint clouds across different categories. The predicted Gaussians are employed\ndirectly for rendering without additional refinement on the rendered images,\nretaining the benefits of 2D Gaussians. We conduct extensive experiments on\nvarious datasets, and the results demonstrate the superiority and\ngeneralization of our method, which achieves SOTA performance. The code is\navailable at\nhttps://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u70b9\u4e91\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b2D\u9ad8\u65af\u5206\u5e03\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u70b9\u4e91\u4e14\u65e0\u9700\u989d\u5916\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7c7b\u522b\u5148\u9a8c\u3001\u5bc6\u96c6\u70b9\u4e91\u6216\u989d\u5916\u4f18\u5316\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u6a21\u5757\u67b6\u6784\uff0c\u5229\u7528\u70b9\u4e91\u4fe1\u606f\u521d\u59cb\u5316\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5206\u88c2\u89e3\u7801\u5668\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u4e14\u80fd\u76f4\u63a5\u6cdb\u5316\u5230\u4e0d\u540c\u7c7b\u522b\u7684\u70b9\u4e91\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u70b9\u4e91\u6e32\u67d3\u3002"}}
{"id": "2505.09415", "pdf": "https://arxiv.org/pdf/2505.09415", "abs": "https://arxiv.org/abs/2505.09415", "authors": ["Hongyang Wang", "Yichen Shi", "Zhuofu Tao", "Yuhao Gao", "Liepiao Zhang", "Xun Lin", "Jun Feng", "Xiaochen Yuan", "Zitong Yu", "Xiaochun Cao"], "title": "FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Face anti-spoofing (FAS) is crucial for protecting facial recognition systems\nfrom presentation attacks. Previous methods approached this task as a\nclassification problem, lacking interpretability and reasoning behind the\npredicted results. Recently, multimodal large language models (MLLMs) have\nshown strong capabilities in perception, reasoning, and decision-making in\nvisual tasks. However, there is currently no universal and comprehensive MLLM\nand dataset specifically designed for FAS task. To address this gap, we propose\nFaceShield, a MLLM for FAS, along with the corresponding pre-training and\nsupervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.\nFaceShield is capable of determining the authenticity of faces, identifying\ntypes of spoofing attacks, providing reasoning for its judgments, and detecting\nattack areas. Specifically, we employ spoof-aware vision perception (SAVP) that\nincorporates both the original image and auxiliary information based on prior\nknowledge. We then use an prompt-guided vision token masking (PVTM) strategy to\nrandom mask vision tokens, thereby improving the model's generalization\nability. We conducted extensive experiments on three benchmark datasets,\ndemonstrating that FaceShield significantly outperforms previous deep learning\nmodels and general MLLMs on four FAS tasks, i.e., coarse-grained\nclassification, fine-grained classification, reasoning, and attack\nlocalization. Our instruction datasets, protocols, and codes will be released\nsoon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFaceShield\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u7528\u4e8e\u4eba\u8138\u9632\u4f2a\uff08FAS\uff09\uff0c\u5e76\u914d\u5957\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\u3002FaceShield\u80fd\u5224\u65ad\u4eba\u8138\u771f\u5b9e\u6027\u3001\u8bc6\u522b\u653b\u51fb\u7c7b\u578b\u3001\u63d0\u4f9b\u63a8\u7406\u4f9d\u636e\u53ca\u5b9a\u4f4d\u653b\u51fb\u533a\u57df\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709FAS\u65b9\u6cd5\u591a\u4e3a\u5206\u7c7b\u95ee\u9898\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u65e0\u4e13\u7528\u4e8eFAS\u7684\u901a\u7528MLLM\u548c\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faFaceShield\u6a21\u578b\uff0c\u7ed3\u5408\u539f\u59cb\u56fe\u50cf\u548c\u5148\u9a8c\u77e5\u8bc6\u7684\u8f85\u52a9\u4fe1\u606f\uff08SAVP\uff09\uff0c\u91c7\u7528\u63d0\u793a\u5f15\u5bfc\u7684\u89c6\u89c9\u6807\u8bb0\u63a9\u7801\uff08PVTM\uff09\u7b56\u7565\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFaceShield\u5728\u7c97/\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3001\u63a8\u7406\u548c\u653b\u51fb\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u901a\u7528MLLM\u3002", "conclusion": "FaceShield\u4e3aFAS\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6570\u636e\u96c6\u3001\u534f\u8bae\u548c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.09422", "pdf": "https://arxiv.org/pdf/2505.09422", "abs": "https://arxiv.org/abs/2505.09422", "authors": ["Xiangyuan Peng", "Yu Wang", "Miao Tang", "Bierzynski Kay", "Lorenzo Servadei", "Robert Wille"], "title": "MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Reliable autonomous driving systems require accurate detection of traffic\nparticipants. To this end, multi-modal fusion has emerged as an effective\nstrategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame\nradar point clouds have demonstrated the effectiveness in bridging the point\ndensity gap. However, they often neglect radar point clouds' inter-frame\nmisalignment caused by object movement during accumulation and do not fully\nexploit the object dynamic information from 4D radar. In this paper, we propose\nMoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for\nrobust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is\ndesigned to compensate for inter-frame radar misalignment from moving objects.\nLater, a Motion Attention Gated Fusion (MAGF) module integrate radar motion\nfeatures to guide LiDAR features to focus on dynamic foreground objects.\nExtensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL\noutperforms existing methods, achieving the highest mAP of 73.30% in the entire\narea and 88.68% in the driving corridor. Notably, our method also achieves the\nbest AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in\nthe driving corridor.", "AI": {"tldr": "MoRAL\u662f\u4e00\u4e2a\u8fd0\u52a8\u611f\u77e5\u7684\u591a\u5e274D\u96f7\u8fbe\u4e0eLiDAR\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u9c81\u68d2\u76843D\u7269\u4f53\u68c0\u6d4b\uff0c\u901a\u8fc7\u8865\u507f\u8fd0\u52a8\u5f15\u8d77\u7684\u96f7\u8fbe\u70b9\u4e91\u9519\u4f4d\u5e76\u5229\u7528\u52a8\u6001\u4fe1\u606f\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u96f7\u8fbe\u70b9\u4e91\u5728\u5e27\u95f4\u7684\u8fd0\u52a8\u9519\u4f4d\u95ee\u9898\uff0c\u4e14\u672a\u5145\u5206\u5229\u75284D\u96f7\u8fbe\u7684\u52a8\u6001\u4fe1\u606f\uff0c\u5f71\u54cd\u4e86\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faMoRAL\u6846\u67b6\uff0c\u5305\u62ec\u8fd0\u52a8\u611f\u77e5\u96f7\u8fbe\u7f16\u7801\u5668\uff08MRE\uff09\u8865\u507f\u8fd0\u52a8\u9519\u4f4d\uff0c\u4ee5\u53ca\u8fd0\u52a8\u6ce8\u610f\u529b\u95e8\u63a7\u878d\u5408\uff08MAGF\uff09\u6a21\u5757\u6574\u5408\u96f7\u8fbe\u52a8\u6001\u4fe1\u606f\u6307\u5bfcLiDAR\u7279\u5f81\u805a\u7126\u52a8\u6001\u7269\u4f53\u3002", "result": "\u5728VoD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6574\u4f53\u533a\u57dfmAP\u8fbe73.30%\uff0c\u9a7e\u9a76\u8d70\u5eca\u8fbe88.68%\uff0c\u884c\u4eba\u68c0\u6d4bAP\u4e3a69.67%\uff0c\u9a91\u884c\u8005\u5728\u9a7e\u9a76\u8d70\u5ecaAP\u8fbe96.25%\u3002", "conclusion": "MoRAL\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u548c\u52a8\u6001\u4fe1\u606f\u878d\u5408\u663e\u8457\u63d0\u5347\u4e863D\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u52a8\u6001\u7269\u4f53\u68c0\u6d4b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.09433", "pdf": "https://arxiv.org/pdf/2505.09433", "abs": "https://arxiv.org/abs/2505.09433", "authors": ["Jiahao Zhu", "Kang You", "Dandan Ding", "Zhan Ma"], "title": "Efficient LiDAR Reflectance Compression via Scanning Serialization", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Reflectance attributes in LiDAR point clouds provide essential information\nfor downstream tasks but remain underexplored in neural compression methods. To\naddress this, we introduce SerLiC, a serialization-based neural compression\nframework to fully exploit the intrinsic characteristics of LiDAR reflectance.\nSerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order\nserialization, offering a device-centric perspective for reflectance analysis.\nEach point is then tokenized into a contextual representation comprising its\nsensor scanning index, radial distance, and prior reflectance, for effective\ndependencies exploration. For efficient sequential modeling, Mamba is\nincorporated with a dual parallelization scheme, enabling simultaneous\nautoregressive dependency capture and fast processing. Extensive experiments\ndemonstrate that SerLiC attains over 2x volume reduction against the original\nreflectance data, outperforming the state-of-the-art method by up to 22%\nreduction of compressed bits while using only 2% of its parameters. Moreover, a\nlightweight version of SerLiC achieves > 10 fps (frames per second) with just\n111K parameters, which is attractive for real-world applications.", "AI": {"tldr": "SerLiC\u662f\u4e00\u79cd\u57fa\u4e8e\u5e8f\u5217\u5316\u7684\u795e\u7ecf\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u538b\u7f29LiDAR\u53cd\u5c04\u7387\u6570\u636e\uff0c\u901a\u8fc7\u626b\u63cf\u987a\u5e8f\u5e8f\u5217\u5316\u548cMamba\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u4f53\u79ef\u548c\u53c2\u6570\u9700\u6c42\u3002", "motivation": "LiDAR\u70b9\u4e91\u4e2d\u7684\u53cd\u5c04\u7387\u5c5e\u6027\u5bf9\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "SerLiC\u5c063D LiDAR\u70b9\u4e91\u901a\u8fc7\u626b\u63cf\u987a\u5e8f\u5e8f\u5217\u5316\u4e3a1D\u5e8f\u5217\uff0c\u5229\u7528\u4f20\u611f\u5668\u626b\u63cf\u7d22\u5f15\u3001\u5f84\u5411\u8ddd\u79bb\u548c\u5148\u9a8c\u53cd\u5c04\u7387\u8fdb\u884c\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u7ed3\u5408Mamba\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u3002", "result": "SerLiC\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684\u6570\u636e\u4f53\u79ef\u538b\u7f29\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1122%\u7684\u538b\u7f29\u6bd4\u7279\u6570\uff0c\u4ec5\u4f7f\u75282%\u7684\u53c2\u6570\u3002\u8f7b\u91cf\u7248SerLiC\u8fbe\u5230>10 fps\uff0c\u4ec5\u9700111K\u53c2\u6570\u3002", "conclusion": "SerLiC\u5728LiDAR\u53cd\u5c04\u7387\u538b\u7f29\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.09435", "pdf": "https://arxiv.org/pdf/2505.09435", "abs": "https://arxiv.org/abs/2505.09435", "authors": ["Yili He", "Yan Zhu", "Peiyao Fu", "Ruijie Yang", "Tianyi Chen", "Zhihua Wang", "Quanlin Li", "Pinghong Zhou", "Xian Yang", "Shuo Wang"], "title": "Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records", "categories": ["cs.CV", "cs.AI"], "comment": "Early accepted to MICCAI 2025", "summary": "Pre-training on image-text colonoscopy records offers substantial potential\nfor improving endoscopic image analysis, but faces challenges including\nnon-informative background images, complex medical terminology, and ambiguous\nmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised\nframework that enhances Contrastive Language-Image Pre-training (CLIP) for this\ndomain. Endo-CLIP's three-stage framework--cleansing, attunement, and\nunification--addresses these challenges by (1) removing background frames, (2)\nleveraging large language models to extract clinical attributes for\nfine-grained contrastive learning, and (3) employing patient-level\ncross-attention to resolve multi-polyp ambiguities. Extensive experiments\ndemonstrate that Endo-CLIP significantly outperforms state-of-the-art\npre-training methods in zero-shot and few-shot polyp detection and\nclassification, paving the way for more accurate and clinically relevant\nendoscopic analysis.", "AI": {"tldr": "Endo-CLIP\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\u4f18\u5316\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u606f\u8089\u68c0\u6d4b\u548c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u9762\u4e34\u975e\u4fe1\u606f\u80cc\u666f\u3001\u590d\u6742\u533b\u5b66\u672f\u8bed\u548c\u591a\u75c5\u7076\u63cf\u8ff0\u6a21\u7cca\u7684\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "Endo-CLIP\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u6e05\u6d17\uff08\u53bb\u80cc\u666f\uff09\u3001\u8c03\u8c10\uff08\u5229\u7528\u5927\u6a21\u578b\u63d0\u53d6\u4e34\u5e8a\u5c5e\u6027\uff09\u3001\u7edf\u4e00\uff08\u60a3\u8005\u7ea7\u4ea4\u53c9\u6ce8\u610f\u529b\u89e3\u51b3\u591a\u606f\u8089\u6a21\u7cca\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEndo-CLIP\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Endo-CLIP\u4e3a\u66f4\u51c6\u786e\u548c\u4e34\u5e8a\u76f8\u5173\u7684\u5185\u7aa5\u955c\u5206\u6790\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2505.09450", "pdf": "https://arxiv.org/pdf/2505.09450", "abs": "https://arxiv.org/abs/2505.09450", "authors": ["Yuelin Zhang", "Qingpeng Ding", "Long Lei", "Yongxuan Feng", "Raymond Shing-Yan Tang", "Shing Shin Cheng"], "title": "MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy", "categories": ["cs.CV"], "comment": "Early Accepted by MICCAI 2025", "summary": "Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally\ninvasive diagnostic procedure. However, an aspiration needle tracker addressing\nrapid reciprocating motion is still missing. MrTrack, an aspiration needle\ntracker with a mamba-based register mechanism, is proposed. MrTrack leverages a\nMamba-based register extractor to sequentially distill global context from each\nhistorical search map, storing these temporal cues in a register bank. The\nMamba-based register retriever then retrieves temporal prompts from the\nregister bank to provide external cues when current vision features are\ntemporarily unusable due to rapid reciprocating motion and imaging degradation.\nA self-supervised register diversify loss is proposed to encourage feature\ndiversity and dimension independence within the learned register, mitigating\nfeature collapse. Comprehensive experiments conducted on both motorized and\nmanual aspiration datasets demonstrate that MrTrack not only outperforms\nstate-of-the-art trackers in accuracy and robustness but also achieves superior\ninference efficiency.", "AI": {"tldr": "MrTrack\u662f\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u9488\u5934\u8ffd\u8e2a\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u8d85\u58f0\u5f15\u5bfc\u4e0b\u7ec6\u9488\u7a7f\u523a\u6d3b\u68c0\u4e2d\u5feb\u901f\u5f80\u590d\u8fd0\u52a8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u4e0a\u4e0b\u6587\u63d0\u53d6\u548c\u65f6\u5e8f\u63d0\u793a\u68c0\u7d22\u63d0\u5347\u8ffd\u8e2a\u6027\u80fd\u3002", "motivation": "\u8d85\u58f0\u5f15\u5bfc\u4e0b\u7ec6\u9488\u7a7f\u523a\u6d3b\u68c0\u4e2d\u5feb\u901f\u5f80\u590d\u8fd0\u52a8\u5bfc\u81f4\u89c6\u89c9\u7279\u5f81\u6682\u65f6\u4e0d\u53ef\u7528\uff0c\u73b0\u6709\u8ffd\u8e2a\u5668\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faMrTrack\uff0c\u91c7\u7528Mamba-based register\u673a\u5236\u63d0\u53d6\u5168\u5c40\u4e0a\u4e0b\u6587\u5e76\u5b58\u50a8\u65f6\u5e8f\u7ebf\u7d22\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u635f\u5931\u4f18\u5316\u7279\u5f81\u591a\u6837\u6027\u3002", "result": "\u5728\u81ea\u52a8\u548c\u624b\u52a8\u7a7f\u523a\u6570\u636e\u96c6\u4e0a\uff0cMrTrack\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u63a8\u7406\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u8ffd\u8e2a\u5668\u3002", "conclusion": "MrTrack\u4e3a\u89e3\u51b3\u5feb\u901f\u5f80\u590d\u8fd0\u52a8\u4e0b\u7684\u9488\u5934\u8ffd\u8e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09455", "pdf": "https://arxiv.org/pdf/2505.09455", "abs": "https://arxiv.org/abs/2505.09455", "authors": ["Jeremie Ochin", "Raphael Chekroun", "Bogdan Stanciulescu", "Sotiris Manitsaris"], "title": "Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos", "categories": ["cs.CV"], "comment": "12 pages, submitted to Advanced Concepts for Intelligent Vision\n  Systems 2025", "summary": "State-of-the-art spatio-temporal action detection (STAD) methods show\npromising results for extracting soccer events from broadcast videos. However,\nwhen operated in the high-recall, low-precision regime required for exhaustive\nevent coverage in soccer analytics, their lack of contextual understanding\nbecomes apparent: many false positives could be resolved by considering a\nbroader sequence of actions and game-state information. In this work, we\naddress this limitation by reasoning at the game level and improving STAD\nthrough the addition of a denoising sequence transduction task. Sequences of\nnoisy, context-free player-centric predictions are processed alongside clean\ngame state information using a Transformer-based encoder-decoder model. By\nmodeling extended temporal context and reasoning jointly over team-level\ndynamics, our method leverages the \"language of soccer\" - its tactical\nregularities and inter-player dependencies - to generate \"denoised\" sequences\nof actions. This approach improves both precision and recall in low-confidence\nregimes, enabling more reliable event extraction from broadcast video and\ncomplementing existing pixel-based methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u6e38\u620f\u72b6\u6001\u4fe1\u606f\u548c\u5e8f\u5217\u53bb\u566a\u4efb\u52a1\uff0c\u63d0\u5347\u4e86\u65f6\u7a7a\u52a8\u4f5c\u68c0\u6d4b\uff08STAD\uff09\u5728\u8db3\u7403\u89c6\u9891\u5206\u6790\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709STAD\u65b9\u6cd5\u5728\u9ad8\u53ec\u56de\u7387\u3001\u4f4e\u7cbe\u5ea6\u573a\u666f\u4e0b\uff08\u5982\u8db3\u7403\u6bd4\u8d5b\u5206\u6790\uff09\u56e0\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7406\u89e3\u800c\u4ea7\u751f\u5927\u91cf\u8bef\u62a5\uff0c\u9700\u901a\u8fc7\u66f4\u5e7f\u6cdb\u7684\u884c\u52a8\u5e8f\u5217\u548c\u6e38\u620f\u72b6\u6001\u4fe1\u606f\u89e3\u51b3\u3002", "method": "\u4f7f\u7528Transformer\u6a21\u578b\u5904\u7406\u566a\u58f0\u8f83\u5927\u7684\u73a9\u5bb6\u4e2d\u5fc3\u9884\u6d4b\u5e8f\u5217\u548c\u6e05\u6670\u7684\u6e38\u620f\u72b6\u6001\u4fe1\u606f\uff0c\u901a\u8fc7\u5efa\u6a21\u957f\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u56e2\u961f\u52a8\u6001\uff0c\u751f\u6210\u53bb\u566a\u540e\u7684\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4f4e\u7f6e\u4fe1\u5ea6\u573a\u666f\u4e0b\u540c\u65f6\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u63d0\u9ad8\u4e86\u4ece\u5e7f\u64ad\u89c6\u9891\u4e2d\u63d0\u53d6\u4e8b\u4ef6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6e38\u620f\u72b6\u6001\u4fe1\u606f\u548c\u5e8f\u5217\u53bb\u566a\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u8db3\u7403\u7684\u6218\u672f\u89c4\u5f8b\u548c\u7403\u5458\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u5347\u4e86STAD\u6027\u80fd\u3002"}}
{"id": "2505.09466", "pdf": "https://arxiv.org/pdf/2505.09466", "abs": "https://arxiv.org/abs/2505.09466", "authors": ["Xi Chen", "Shiyang Zhou", "Muqi Huang", "Jiaxu Feng", "Yun Xiong", "Kun Zhou", "Biao Yang", "Yuhui Zhang", "Huishuai Bao", "Sijia Peng", "Chuan Li", "Feng Shi"], "title": "A 2D Semantic-Aware Position Encoding for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 4 figures, 3 tables", "summary": "Vision transformers have demonstrated significant advantages in computer\nvision tasks due to their ability to capture long-range dependencies and\ncontextual relationships through self-attention. However, existing position\nencoding techniques, which are largely borrowed from natural language\nprocessing, fail to effectively capture semantic-aware positional relationships\nbetween image patches. Traditional approaches like absolute position encoding\nand relative position encoding primarily focus on 1D linear position\nrelationship, often neglecting the semantic similarity between distant yet\ncontextually related patches. These limitations hinder model generalization,\ntranslation equivariance, and the ability to effectively handle repetitive or\nstructured patterns in images. In this paper, we propose 2-Dimensional\nSemantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding\nmethod with semantic awareness that dynamically adapts position representations\nby leveraging local content instead of fixed linear position relationship or\nspatial coordinates. Our method enhances the model's ability to generalize\nacross varying image resolutions and scales, improves translation equivariance,\nand better aggregates features for visually similar but spatially distant\npatches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the\ngap between position encoding and perceptual similarity, thereby improving\nperformance on computer vision tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76842D\u8bed\u4e49\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff08SaPE\u00b2\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8bed\u4e49\u5173\u7cfb\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff08\u5982\u7edd\u5bf9\u548c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff09\u4e3b\u8981\u5173\u6ce81D\u7ebf\u6027\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u5757\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7ffb\u8bd1\u7b49\u53d8\u6027\u3002", "method": "\u63d0\u51fa\u4e86SaPE\u00b2\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u4f4d\u7f6e\u8868\u793a\uff0c\u5229\u7528\u5c40\u90e8\u5185\u5bb9\u800c\u975e\u56fa\u5b9a\u7ebf\u6027\u5173\u7cfb\u6216\u7a7a\u95f4\u5750\u6807\uff0c\u589e\u5f3a\u8bed\u4e49\u611f\u77e5\u3002", "result": "SaPE\u00b2\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u5c3a\u5ea6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6539\u5584\u4e86\u7ffb\u8bd1\u7b49\u53d8\u6027\uff0c\u5e76\u66f4\u597d\u5730\u805a\u5408\u4e86\u89c6\u89c9\u76f8\u4f3c\u4f46\u7a7a\u95f4\u8ddd\u79bb\u8fdc\u7684\u56fe\u50cf\u5757\u7279\u5f81\u3002", "conclusion": "\u901a\u8fc7\u5c06SaPE\u00b2\u96c6\u6210\u5230\u89c6\u89c9Transformer\u4e2d\uff0c\u5f25\u5408\u4e86\u4f4d\u7f6e\u7f16\u7801\u4e0e\u611f\u77e5\u76f8\u4f3c\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.09484", "pdf": "https://arxiv.org/pdf/2505.09484", "abs": "https://arxiv.org/abs/2505.09484", "authors": ["Yingjie Ma", "Xun Lin", "Zitong Yu", "Xin Liu", "Xiaochen Yuan", "Weicheng Xie", "Linlin Shen"], "title": "Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing", "categories": ["cs.CV"], "comment": null, "summary": "Face Anti-Spoofing (FAS) is essential for the security of facial recognition\nsystems in diverse scenarios such as payment processing and surveillance.\nCurrent multimodal FAS methods often struggle with effective generalization,\nmainly due to modality-specific biases and domain shifts. To address these\nchallenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising\nand \\textbf{A}lignment (\\textbf{MMDA}) framework. By leveraging the zero-shot\ngeneralization capability of CLIP, the MMDA framework effectively suppresses\nnoise in multimodal data through denoising and alignment mechanisms, thereby\nsignificantly enhancing the generalization performance of cross-modal\nalignment. The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential\n\\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the\nimpacts of domain and modality noise by refining the attention mechanism based\non extracted common noise features. Furthermore, the \\textbf{R}epresentation\n\\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the\npre-trained CLIP model to align multi-domain multimodal data into a generalized\nrepresentation space in a flexible manner, preserving intricate representations\nand enhancing the model's adaptability to various unseen conditions. We also\ndesign a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation\n(\\textbf{U-DSA}) module to enhance the adaptability of representations while\nmaintaining generalization performance. These improvements not only enhance the\nframework's generalization capabilities but also boost its ability to represent\ncomplex representations. Our experimental results on four benchmark datasets\nunder different evaluation protocols demonstrate that the MMDA framework\noutperforms existing state-of-the-art methods in terms of cross-domain\ngeneralization and multimodal detection accuracy. The code will be released\nsoon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMDA\u7684\u591a\u6a21\u6001\u53bb\u566a\u4e0e\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408CLIP\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001FAS\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u6a21\u6001\u7279\u5b9a\u504f\u5dee\u548c\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "MMDA\u6846\u67b6\u5305\u542bMD2A\u6a21\u5757\uff08\u7528\u4e8e\u53bb\u566a\u548c\u5bf9\u9f50\uff09\u3001RS2\u5bf9\u9f50\u7b56\u7565\uff08\u5229\u7528CLIP\u6a21\u578b\u5bf9\u9f50\u6570\u636e\uff09\u548cU-DSA\u6a21\u5757\uff08\u589e\u5f3a\u8868\u793a\u9002\u5e94\u6027\uff09\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMMDA\u5728\u8de8\u57df\u6cdb\u5316\u548c\u591a\u6a21\u6001\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MMDA\u6846\u67b6\u901a\u8fc7\u53bb\u566a\u548c\u5bf9\u9f50\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001FAS\u7684\u6cdb\u5316\u80fd\u529b\u548c\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.09498", "pdf": "https://arxiv.org/pdf/2505.09498", "abs": "https://arxiv.org/abs/2505.09498", "authors": ["Bo Zhang", "Shuo Li", "Runhe Tian", "Yang Yang", "Jixin Tang", "Jinhao Zhou", "Lin Ma"], "title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications.", "AI": {"tldr": "Flash-VL 2B\u662f\u4e00\u79cd\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u9488\u5bf9\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\uff0c\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u9ad8\u6548\u8fd0\u884c\u3002", "method": "\u91c7\u7528\u67b6\u6784\u589e\u5f3a\u3001\u8ba1\u7b97\u7b56\u7565\u4f18\u5316\u3001\u4ee4\u724c\u538b\u7f29\u3001\u6570\u636e\u7b5b\u9009\u3001\u8bad\u7ec3\u65b9\u6848\u6539\u8fdb\u53ca\u9690\u5f0f\u8bed\u4e49\u62fc\u63a5\u6280\u672f\u3002", "result": "\u572811\u4e2a\u6807\u51c6VLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlash-VL 2B\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u5747\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "Flash-VL 2B\u662f\u8d44\u6e90\u53d7\u9650\u548c\u5927\u89c4\u6a21\u5b9e\u65f6\u5e94\u7528\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09528", "pdf": "https://arxiv.org/pdf/2505.09528", "abs": "https://arxiv.org/abs/2505.09528", "authors": ["Jeffrey Wen", "Rizwan Ahmad", "Philip Schniter"], "title": "Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems", "categories": ["cs.CV"], "comment": null, "summary": "In imaging inverse problems, we would like to know how close the recovered\nimage is to the true image in terms of full-reference image quality (FRIQ)\nmetrics like PSNR, SSIM, LPIPS, etc. This is especially important in\nsafety-critical applications like medical imaging, where knowing that, say, the\nSSIM was poor could potentially avoid a costly misdiagnosis. But since we don't\nknow the true image, computing FRIQ is non-trivial. In this work, we combine\nconformal prediction with approximate posterior sampling to construct bounds on\nFRIQ that are guaranteed to hold up to a user-specified error probability. We\ndemonstrate our approach on image denoising and accelerated magnetic resonance\nimaging (MRI) problems. Code is available at\nhttps://github.com/jwen307/quality_uq.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5171\u5f62\u9884\u6d4b\u548c\u8fd1\u4f3c\u540e\u9a8c\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e0d\u77e5\u9053\u771f\u5b9e\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u6784\u5efa\u5168\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\uff08FRIQ\uff09\u6307\u6807\u7684\u53ef\u9760\u8fb9\u754c\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\uff08\u5982\u533b\u5b66\u6210\u50cf\uff09\u4e2d\uff0c\u51c6\u786e\u8bc4\u4f30\u6062\u590d\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u7684\u63a5\u8fd1\u7a0b\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76f4\u63a5\u8ba1\u7b97FRIQ\u6307\u6807\u56e0\u7f3a\u4e4f\u771f\u5b9e\u56fe\u50cf\u800c\u56f0\u96be\u3002", "method": "\u7ed3\u5408\u5171\u5f62\u9884\u6d4b\u4e0e\u8fd1\u4f3c\u540e\u9a8c\u91c7\u6837\uff0c\u6784\u5efa\u5177\u6709\u7528\u6237\u6307\u5b9a\u9519\u8bef\u6982\u7387\u4fdd\u8bc1\u7684FRIQ\u8fb9\u754c\u3002", "result": "\u5728\u56fe\u50cf\u53bb\u566a\u548c\u52a0\u901f\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aFRIQ\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8fb9\u754c\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u573a\u666f\u3002"}}
{"id": "2505.09529", "pdf": "https://arxiv.org/pdf/2505.09529", "abs": "https://arxiv.org/abs/2505.09529", "authors": ["Mohamed Moustafa", "Joseph Lemley", "Peter Corcoran"], "title": "Contactless Cardiac Pulse Monitoring Using Event Cameras", "categories": ["cs.CV", "cs.ET", "cs.LG", "eess.IV"], "comment": "This paper is a preprint of a paper submitted to IEEE Access and is\n  currently under review", "summary": "Time event cameras are a novel technology for recording scene information at\nextremely low latency and with low power consumption. Event cameras output a\nstream of events that encapsulate pixel-level light intensity changes within\nthe scene, capturing information with a higher dynamic range and temporal\nresolution than traditional cameras. This study investigates the contact-free\nreconstruction of an individual's cardiac pulse signal from time event\nrecording of their face using a supervised convolutional neural network (CNN)\nmodel. An end-to-end model is trained to extract the cardiac signal from a\ntwo-dimensional representation of the event stream, with model performance\nevaluated based on the accuracy of the calculated heart rate. The experimental\nresults confirm that physiological cardiac information in the facial region is\neffectively preserved within the event stream, showcasing the potential of this\nnovel sensor for remote heart rate monitoring. The model trained on event\nframes achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)\ncompared to the RMSE of 2.92 bpm achieved by the baseline model trained on\nstandard camera frames. Furthermore, models trained on event frames generated\nat 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an\nRMSE of 2.54 and 2.13 bpm, respectively.", "AI": {"tldr": "\u4e8b\u4ef6\u76f8\u673a\u6280\u672f\u7528\u4e8e\u65e0\u63a5\u89e6\u5fc3\u7387\u76d1\u6d4b\uff0c\u901a\u8fc7CNN\u6a21\u578b\u4ece\u9762\u90e8\u4e8b\u4ef6\u6d41\u4e2d\u63d0\u53d6\u5fc3\u7387\u4fe1\u53f7\uff0c\u6027\u80fd\u63a5\u8fd1\u4f20\u7edf\u76f8\u673a\u3002", "motivation": "\u63a2\u7d22\u4e8b\u4ef6\u76f8\u673a\u5728\u751f\u7406\u4fe1\u53f7\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u65e0\u63a5\u89e6\u5fc3\u7387\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528\u76d1\u7763\u5f0fCNN\u6a21\u578b\uff0c\u4ece\u4e8c\u7ef4\u4e8b\u4ef6\u6d41\u8868\u793a\u4e2d\u63d0\u53d6\u5fc3\u7387\u4fe1\u53f7\uff0c\u8bc4\u4f30\u5fc3\u7387\u8ba1\u7b97\u51c6\u786e\u6027\u3002", "result": "\u4e8b\u4ef6\u76f8\u673a\u572860\u548c120 FPS\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf30 FPS\u76f8\u673a\uff0cRMSE\u5206\u522b\u4e3a2.54\u548c2.13 bpm\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u5728\u5fc3\u7387\u76d1\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u6027\u80fd\u63a5\u8fd1\u6216\u4f18\u4e8e\u4f20\u7edf\u76f8\u673a\u3002"}}
{"id": "2505.09562", "pdf": "https://arxiv.org/pdf/2505.09562", "abs": "https://arxiv.org/abs/2505.09562", "authors": ["Nicola Marinello", "Simen Cassiman", "Jonas Heylen", "Marc Proesmans", "Luc Van Gool"], "title": "Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 Workshop on Autonomous Driving", "summary": "Autonomous vehicles need a complete map of their surroundings to plan and\nact. This has sparked research into the tasks of 3D occupancy prediction, 3D\nscene completion, and 3D panoptic scene completion, which predict a dense map\nof the ego vehicle's surroundings as a voxel grid. Scene completion extends\noccupancy prediction by predicting occluded regions of the voxel grid, and\npanoptic scene completion further extends this task by also distinguishing\nobject instances within the same class; both aspects are crucial for path\nplanning and decision-making. However, 3D panoptic scene completion is\ncurrently underexplored. This work introduces a novel framework for 3D panoptic\nscene completion that extends existing 3D semantic scene completion models. We\npropose an Object Module and Panoptic Module that can easily be integrated with\n3D occupancy and scene completion methods presented in the literature. Our\napproach leverages the available annotations in occupancy benchmarks, allowing\nindividual object shapes to be learned as a differentiable problem. The code is\navailable at https://github.com/nicolamarinello/OffsetOcc .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e3D\u5168\u666f\u573a\u666f\u8865\u5168\u7684\u65b0\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u73b0\u67093D\u8bed\u4e49\u573a\u666f\u8865\u5168\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u5bf9\u8c61\u6a21\u5757\u548c\u5168\u666f\u6a21\u5757\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u5b8c\u6574\u7684\u73af\u5883\u5730\u56fe\u4ee5\u89c4\u5212\u548c\u884c\u52a8\uff0c\u800c3D\u5168\u666f\u573a\u666f\u8865\u5168\u76ee\u524d\u7814\u7a76\u4e0d\u8db3\uff0c\u4f46\u8be5\u4efb\u52a1\u5bf9\u8def\u5f84\u89c4\u5212\u548c\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5305\u542b\u5bf9\u8c61\u6a21\u5757\u548c\u5168\u666f\u6a21\u5757\uff0c\u53ef\u8f7b\u677e\u4e0e\u73b0\u67093D\u5360\u7528\u548c\u573a\u666f\u8865\u5168\u65b9\u6cd5\u96c6\u6210\uff0c\u5229\u7528\u5360\u7528\u57fa\u51c6\u4e2d\u7684\u6807\u6ce8\u5b66\u4e60\u5bf9\u8c61\u5f62\u72b6\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u533a\u5206\u540c\u4e00\u7c7b\u522b\u4e2d\u7684\u5bf9\u8c61\u5b9e\u4f8b\uff0c\u5e76\u9884\u6d4b\u88ab\u906e\u6321\u533a\u57df\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u73af\u5883\u5730\u56fe\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a3D\u5168\u666f\u573a\u666f\u8865\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.09564", "pdf": "https://arxiv.org/pdf/2505.09564", "abs": "https://arxiv.org/abs/2505.09564", "authors": ["Anne-Marie Rickmann", "Stephanie L. Thorn", "Shawn S. Ahn", "Supum Lee", "Selen Uman", "Taras Lysyy", "Rachel Burns", "Nicole Guerrera", "Francis G. Spinale", "Jason A. Burdick", "Albert J. Sinusas", "James S. Duncan"], "title": "Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation", "categories": ["cs.CV"], "comment": "accepted at FIMH 2025", "summary": "Cardiac image segmentation is an important step in many cardiac image\nanalysis and modeling tasks such as motion tracking or simulations of cardiac\nmechanics. While deep learning has greatly advanced segmentation in clinical\nsettings, there is limited work on pre-clinical imaging, notably in porcine\nmodels, which are often used due to their anatomical and physiological\nsimilarity to humans. However, differences between species create a domain\nshift that complicates direct model transfer from human to pig data.\n  Recently, foundation models trained on large human datasets have shown\npromise for robust medical image segmentation; yet their applicability to\nporcine data remains largely unexplored. In this work, we investigate whether\nfoundation models can generate sufficiently accurate pseudo-labels for pig\ncardiac CT and propose a simple self-training approach to iteratively refine\nthese labels. Our method requires no manually annotated pig data, relying\ninstead on iterative updates to improve segmentation quality. We demonstrate\nthat this self-training process not only enhances segmentation accuracy but\nalso smooths out temporal inconsistencies across consecutive frames. Although\nour results are encouraging, there remains room for improvement, for example by\nincorporating more sophisticated self-training strategies and by exploring\nadditional foundation models and other cardiac imaging technologies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u57fa\u7840\u6a21\u578b\u4e3a\u732a\u5fc3\u810fCT\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u81ea\u8bad\u7ec3\u65b9\u6cd5\u8fed\u4ee3\u4f18\u5316\u6807\u7b7e\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u7531\u4e8e\u732a\u4e0e\u4eba\u7c7b\u5728\u5fc3\u810f\u89e3\u5256\u548c\u751f\u7406\u4e0a\u7684\u76f8\u4f3c\u6027\uff0c\u732a\u6a21\u578b\u5e38\u7528\u4e8e\u4e34\u5e8a\u524d\u7814\u7a76\uff0c\u4f46\u7269\u79cd\u5dee\u5f02\u5bfc\u81f4\u6a21\u578b\u76f4\u63a5\u8fc1\u79fb\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\u5e76\u8fed\u4ee3\u4f18\u5316\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "result": "\u81ea\u8bad\u7ec3\u8fc7\u7a0b\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\uff0c\u5e76\u5e73\u6ed1\u4e86\u8fde\u7eed\u5e27\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u7ed3\u679c\u4ee4\u4eba\u9f13\u821e\uff0c\u4f46\u4ecd\u53ef\u901a\u8fc7\u66f4\u590d\u6742\u7684\u81ea\u8bad\u7ec3\u7b56\u7565\u548c\u63a2\u7d22\u5176\u4ed6\u57fa\u7840\u6a21\u578b\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2505.09568", "pdf": "https://arxiv.org/pdf/2505.09568", "abs": "https://arxiv.org/abs/2505.09568", "authors": ["Jiuhai Chen", "Zhiyang Xu", "Xichen Pan", "Yushi Hu", "Can Qin", "Tom Goldstein", "Lifu Huang", "Tianyi Zhou", "Saining Xie", "Silvio Savarese", "Le Xue", "Caiming Xiong", "Ran Xu"], "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578bBLIP3-o\uff0c\u7ed3\u5408\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u7edf\u4e00\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u7684\u591a\u6a21\u6001\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\u751f\u6210CLIP\u56fe\u50cf\u7279\u5f81\uff0c\u63d0\u51fa\u5206\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6BLIP3o-60k\u3002", "result": "BLIP3-o\u5728\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u6548\u7387\u548c\u8d28\u91cf\u5747\u6709\u63d0\u5347\u3002", "conclusion": "BLIP3-o\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u521b\u65b0\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f00\u6e90\u8d44\u6e90\u5c06\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2505.09571", "pdf": "https://arxiv.org/pdf/2505.09571", "abs": "https://arxiv.org/abs/2505.09571", "authors": ["Guillermo Gomez-Trenado", "Pablo Mesejo", "Oscar Cord\u00f3n", "St\u00e9phane Lathuili\u00e8re"], "title": "Don't Forget your Inverse DDIM for Image Editing", "categories": ["cs.CV", "I.2.10; I.5.0"], "comment": "12 pages, 12 figures, code available at\n  https://guillermogotre.github.io/sage/", "summary": "The field of text-to-image generation has undergone significant advancements\nwith the introduction of diffusion models. Nevertheless, the challenge of\nediting real images persists, as most methods are either computationally\nintensive or produce poor reconstructions. This paper introduces SAGE\n(Self-Attention Guidance for image Editing) - a novel technique leveraging\npre-trained diffusion models for image editing. SAGE builds upon the DDIM\nalgorithm and incorporates a novel guidance mechanism utilizing the\nself-attention layers of the diffusion U-Net. This mechanism computes a\nreconstruction objective based on attention maps generated during the inverse\nDDIM process, enabling efficient reconstruction of unedited regions without the\nneed to precisely reconstruct the entire input image. Thus, SAGE directly\naddresses the key challenges in image editing. The superiority of SAGE over\nother methods is demonstrated through quantitative and qualitative evaluations\nand confirmed by a statistically validated comprehensive user study, in which\nall 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE\nranks as the top-performing method in seven out of 10 quantitative analyses and\nsecures second and third places in the remaining three.", "AI": {"tldr": "SAGE\u662f\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u7f16\u8f91\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u91cd\u5efa\u6548\u679c\u5dee\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408DDIM\u7b97\u6cd5\u548c\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u5229\u7528\u53cd\u5411DDIM\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u56fe\u5b9e\u73b0\u9ad8\u6548\u91cd\u5efa\u3002", "result": "\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7528\u6237\u7814\u7a76\u4e2d47\u540d\u7528\u6237\u5168\u90e8\u504f\u597dSAGE\u3002", "conclusion": "SAGE\u5728\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.09591", "pdf": "https://arxiv.org/pdf/2505.09591", "abs": "https://arxiv.org/abs/2505.09591", "authors": ["Tobias Jan Wieczorek", "Nathalie Daun", "Mohammad Emtiyaz Khan", "Marcus Rohrbach"], "title": "Variational Visual Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 16 figures, under review at ICCV 2025", "summary": "Despite remarkable progress in multimodal models for Visual Question\nAnswering (VQA), there remain major reliability concerns because the models can\noften be overconfident and miscalibrated, especially in out-of-distribution\n(OOD) settings. Plenty has been done to address such issues for unimodal\nmodels, but little work exists for multimodal cases. Here, we address\nunreliability in multimodal models by proposing a Variational VQA approach.\nSpecifically, instead of fine-tuning vision-language models by using AdamW, we\nemploy a recently proposed variational algorithm called IVON, which yields a\nposterior distribution over model parameters. Through extensive experiments, we\nshow that our approach improves calibration and abstentions without sacrificing\nthe accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce\nExpected Calibration Error by more than 50% compared to the AdamW baseline and\nraise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of\ndistribution shifts, the performance gain is even higher, achieving 8% Coverage\n(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we\npresent variational learning as a viable option to enhance the reliability of\nmultimodal models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u5b66\u4e60\u7684VQA\u65b9\u6cd5\uff08IVON\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u6821\u51c6\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u5728\u5206\u5e03\u504f\u79fb\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u5728VQA\u4efb\u52a1\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u5bb9\u6613\u8fc7\u5ea6\u81ea\u4fe1\u548c\u6821\u51c6\u4e0d\u8db3\u3002\u76ee\u524d\u9488\u5bf9\u5355\u6a21\u6001\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\u8f83\u591a\uff0c\u4f46\u591a\u6a21\u6001\u9886\u57df\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u91c7\u7528\u53d8\u5206\u7b97\u6cd5IVON\u66ff\u4ee3\u4f20\u7edf\u7684AdamW\u4f18\u5316\u5668\uff0c\u751f\u6210\u6a21\u578b\u53c2\u6570\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u6821\u51c6\u6027\u548c\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u9884\u671f\u6821\u51c6\u8bef\u5dee\uff08>50%\uff09\uff0c\u5e76\u63d0\u9ad8\u4e86\u8986\u76d6\u7387\uff084% vs. SOTA\uff09\u3002\u5728\u5206\u5e03\u504f\u79fb\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u63d0\u5347\u66f4\u660e\u663e\uff08\u8986\u76d6\u7387\u63d0\u9ad88%\uff09\u3002", "conclusion": "\u53d8\u5206\u5b66\u4e60\u662f\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u53ef\u9760\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u5728OOD\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.09608", "pdf": "https://arxiv.org/pdf/2505.09608", "abs": "https://arxiv.org/abs/2505.09608", "authors": ["Nadav Magar", "Amir Hertz", "Eric Tabellion", "Yael Pritch", "Alex Rav-Acha", "Ariel Shamir", "Yedid Hoshen"], "title": "LightLab: Controlling Light Sources in Images with Diffusion Models", "categories": ["cs.CV", "cs.GR"], "comment": "Project Page: https://nadmag.github.io/LightLab/", "summary": "We present a simple, yet effective diffusion-based method for fine-grained,\nparametric control over light sources in an image. Existing relighting methods\neither rely on multiple input views to perform inverse rendering at inference\ntime, or fail to provide explicit control over light changes. Our method\nfine-tunes a diffusion model on a small set of real raw photograph pairs,\nsupplemented by synthetically rendered images at scale, to elicit its\nphotorealistic prior for relighting. We leverage the linearity of light to\nsynthesize image pairs depicting controlled light changes of either a target\nlight source or ambient illumination. Using this data and an appropriate\nfine-tuning scheme, we train a model for precise illumination changes with\nexplicit control over light intensity and color. Lastly, we show how our method\ncan achieve compelling light editing results, and outperforms existing methods\nbased on user preference.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u56fe\u50cf\u5149\u6e90\u7684\u7cbe\u7ec6\u53c2\u6570\u5316\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u89c6\u56fe\u8f93\u5165\u6216\u65e0\u6cd5\u63d0\u4f9b\u5149\u6e90\u53d8\u5316\u7684\u663e\u5f0f\u63a7\u5236\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\u6570\u636e\uff0c\u5229\u7528\u5149\u7684\u7ebf\u6027\u7279\u6027\u5408\u6210\u56fe\u50cf\u5bf9\uff0c\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u7cbe\u786e\u5149\u6e90\u63a7\u5236\u3002", "result": "\u65b9\u6cd5\u5728\u5149\u6e90\u7f16\u8f91\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7528\u6237\u504f\u597d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56fe\u50cf\u5149\u6e90\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09615", "pdf": "https://arxiv.org/pdf/2505.09615", "abs": "https://arxiv.org/abs/2505.09615", "authors": ["Yung-Hsuan Lai", "Janek Ebbers", "Yu-Chiang Frank Wang", "Fran\u00e7ois Germain", "Michael Jeffrey Jones", "Moitreya Chatterjee"], "title": "UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "CVPR 2025", "summary": "Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing\nboth uni-modal events (i.e., those occurring exclusively in either the visual\nor acoustic modality of a video) and multi-modal events (i.e., those occurring\nin both modalities concurrently). Moreover, the prohibitive cost of annotating\ntraining data with the class labels of all these events, along with their start\nand end times, imposes constraints on the scalability of AVVP techniques unless\nthey can be trained in a weakly-supervised setting, where only\nmodality-agnostic, video-level labels are available in the training data. To\nthis end, recently proposed approaches seek to generate segment-level\npseudo-labels to better guide model training. However, the absence of\ninter-segment dependencies when generating these pseudo-labels and the general\nbias towards predicting labels that are absent in a segment limit their\nperformance. This work proposes a novel approach towards overcoming these\nweaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video\nParsing (UWAV). Additionally, our innovative approach factors in the\nuncertainty associated with these estimated pseudo-labels and incorporates a\nfeature mixup based training regularization for improved training. Empirical\nresults show that UWAV outperforms state-of-the-art methods for the AVVP task\non multiple metrics, across two different datasets, attesting to its\neffectiveness and generalizability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUWAV\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\uff08AVVP\uff09\u4efb\u52a1\u4e2d\u7684\u5f31\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u548c\u7279\u5f81\u6df7\u5408\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "AVVP\u4efb\u52a1\u9700\u8981\u5b9a\u4f4d\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u4e8b\u4ef6\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u5f31\u76d1\u7763\u5b66\u4e60\u6210\u4e3a\u5fc5\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u4f2a\u6807\u7b7e\u65f6\u7f3a\u4e4f\u6bb5\u95f4\u4f9d\u8d56\u6027\u548c\u5b58\u5728\u9884\u6d4b\u504f\u5dee\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51faUWAV\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u5904\u7406\u4f2a\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u6df7\u5408\u6b63\u5219\u5316\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUWAV\u5728\u591a\u4e2a\u6307\u6807\u548c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UWAV\u901a\u8fc7\u6539\u8fdb\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86AVVP\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u5f31\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.08798", "pdf": "https://arxiv.org/pdf/2505.08798", "abs": "https://arxiv.org/abs/2505.08798", "authors": ["Mobina Shrestha", "Bishwas Mandal", "Vishal Mandal", "Asis Shrestha"], "title": "In-Context Learning for Label-Efficient Cancer Image Classification in Oncology", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The application of AI in oncology has been limited by its reliance on large,\nannotated datasets and the need for retraining models for domain-specific\ndiagnostic tasks. Taking heed of these limitations, we investigated in-context\nlearning as a pragmatic alternative to model retraining by allowing models to\nadapt to new diagnostic tasks using only a few labeled examples at inference,\nwithout the need for retraining. Using four vision-language models\n(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across\nthree oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our\nknowledge, this is the first study to compare the performance of multiple VLMs\non different oncology classification tasks. Without any parameter updates, all\nmodels showed significant gains with few-shot prompting, with GPT-4o reaching\nan F1 score of 0.81 in binary classification and 0.60 in multi-class\nclassification settings. While these results remain below the ceiling of fully\nfine-tuned systems, they highlight the potential of ICL to approximate\ntask-specific behavior using only a handful of examples, reflecting how\nclinicians often reason from prior cases. Notably, open-source models like\nPaligemma and CLIP demonstrated competitive gains despite their smaller size,\nsuggesting feasibility for deployment in computing constrained clinical\nenvironments. Overall, these findings highlight the potential of ICL as a\npractical solution in oncology, particularly for rare cancers and\nresource-limited contexts where fine-tuning is infeasible and annotated data is\ndifficult to obtain.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5728\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u4e0b\u5b9e\u73b0\u80bf\u7624\u5b66\u8bca\u65ad\u4efb\u52a1\uff0c\u5e76\u6bd4\u8f83\u4e86\u56db\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "AI\u5728\u80bf\u7624\u5b66\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22ICL\u7684\u5b9e\u7528\u6027\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Paligemma\u3001CLIP\u3001ALIGN\u3001GPT-4o\uff09\u5728\u4e09\u4e2a\u80bf\u7624\u5b66\u6570\u636e\u96c6\uff08MHIST\u3001PatchCamelyon\u3001HAM10000\uff09\u4e0a\u8bc4\u4f30ICL\u6027\u80fd\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u5c11\u91cf\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0cGPT-4o\u5728\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u5230F1\u5206\u65700.81\u548c0.60\u3002\u5f00\u6e90\u6a21\u578b\u5982Paligemma\u548cCLIP\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "ICL\u5728\u80bf\u7624\u5b66\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7f55\u89c1\u764c\u75c7\u548c\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2505.08819", "pdf": "https://arxiv.org/pdf/2505.08819", "abs": "https://arxiv.org/abs/2505.08819", "authors": ["Asahi Miyazaki", "Tsuyoshi Okita"], "title": "Thoughts on Objectives of Sparse and Hierarchical Masked Image Model", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "9 pages, 11 figures", "summary": "Masked image modeling is one of the most poplular objectives of training.\nRecently, the SparK model has been proposed with superior performance among\nself-supervised learning models. This paper proposes a new mask pattern for\nthis SparK model, proposing it as the Mesh Mask-ed SparK model. We report the\neffect of the mask pattern used for image masking in pre-training on\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a9\u7801\u6a21\u5f0fMesh Mask-ed SparK\uff0c\u7528\u4e8e\u6539\u8fdbSparK\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u63a9\u7801\u6a21\u5f0f\u5bf9\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5728SparK\u6a21\u578b\u4e2d\u5f15\u5165Mesh Mask\u63a9\u7801\u6a21\u5f0f\u3002", "result": "\u9a8c\u8bc1\u4e86\u63a9\u7801\u6a21\u5f0f\u5bf9\u9884\u8bad\u7ec3\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "Mesh Mask-ed SparK\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2505.08835", "pdf": "https://arxiv.org/pdf/2505.08835", "abs": "https://arxiv.org/abs/2505.08835", "authors": ["Hyunsik Na", "Wonho Lee", "Seungdeok Roh", "Sohee Park", "Daeseon Choi"], "title": "Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "The advent of convenient and efficient fully unmanned stores equipped with\nartificial intelligence-based automated checkout systems marks a new era in\nretail. However, these systems have inherent artificial intelligence security\nvulnerabilities, which are exploited via adversarial patch attacks,\nparticularly in physical environments. This study demonstrated that adversarial\npatches can severely disrupt object detection models used in unmanned stores,\nleading to issues such as theft, inventory discrepancies, and interference. We\ninvestigated three types of adversarial patch attacks -- Hiding, Creating, and\nAltering attacks -- and highlighted their effectiveness. We also introduce the\nnovel color histogram similarity loss function by leveraging attacker knowledge\nof the color information of a target class object. Besides the traditional\nconfusion-matrix-based attack success rate, we introduce a new\nbounding-boxes-based metric to analyze the practical impact of these attacks.\nStarting with attacks on object detection models trained on snack and fruit\ndatasets in a digital environment, we evaluated the effectiveness of\nadversarial patches in a physical testbed that mimicked a real unmanned store\nwith RGB cameras and realistic conditions. Furthermore, we assessed the\nrobustness of these attacks in black-box scenarios, demonstrating that shadow\nattacks can enhance success rates of attacks even without direct access to\nmodel parameters. Our study underscores the necessity for robust defense\nstrategies to protect unmanned stores from adversarial threats. Highlighting\nthe limitations of the current defense mechanisms in real-time detection\nsystems and discussing various proactive measures, we provide insights into\nimproving the robustness of object detection models and fortifying unmanned\nretail environments against these attacks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u4eba\u5546\u5e97\u4e2d\u57fa\u4e8eAI\u7684\u81ea\u52a8\u7ed3\u8d26\u7cfb\u7edf\u9762\u4e34\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f3a\u8c03\u4e86\u9632\u5fa1\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u65e0\u4eba\u5546\u5e97\u7684AI\u7cfb\u7edf\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u53ef\u80fd\u5bfc\u81f4\u76d7\u7a83\u548c\u5e93\u5b58\u95ee\u9898\uff0c\u4e9f\u9700\u7814\u7a76\u548c\u9632\u5fa1\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\uff08\u9690\u85cf\u3001\u521b\u5efa\u3001\u4fee\u6539\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u989c\u8272\u76f4\u65b9\u56fe\u76f8\u4f3c\u6027\u635f\u5931\u51fd\u6570\u548c\u8fb9\u754c\u6846\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5728\u6570\u5b57\u548c\u7269\u7406\u73af\u5883\u4e2d\u6d4b\u8bd5\u653b\u51fb\u6548\u679c\u3002", "result": "\u653b\u51fb\u5728\u6570\u5b57\u548c\u7269\u7406\u73af\u5883\u4e2d\u5747\u6709\u6548\uff0c\u9ed1\u76d2\u573a\u666f\u4e0b\u653b\u51fb\u6210\u529f\u7387\u66f4\u9ad8\uff0c\u5f53\u524d\u9632\u5fa1\u673a\u5236\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u7b56\u7565\u4ee5\u4fdd\u62a4\u65e0\u4eba\u5546\u5e97\u514d\u53d7\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u5e76\u6539\u8fdb\u5b9e\u65f6\u68c0\u6d4b\u7cfb\u7edf\u7684\u80fd\u529b\u3002"}}
{"id": "2505.08837", "pdf": "https://arxiv.org/pdf/2505.08837", "abs": "https://arxiv.org/abs/2505.08837", "authors": ["Muhammad Saqib", "Dipkumar Mehta", "Fnu Yashu", "Shubham Malhotra"], "title": "Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning", "categories": ["cs.CR", "cs.CV", "cs.DC", "cs.LG", "cs.NI"], "comment": "10 pages, 6 figures, 1 table", "summary": "The security of cloud environments, such as Amazon Web Services (AWS), is\ncomplex and dynamic. Static security policies have become inadequate as threats\nevolve and cloud resources exhibit elasticity [1]. This paper addresses the\nlimitations of static policies by proposing a security policy management\nframework that uses reinforcement learning (RL) to adapt dynamically.\nSpecifically, we employ deep reinforcement learning algorithms, including deep\nQ Networks and proximal policy optimization, enabling the learning and\ncontinuous adjustment of controls such as firewall rules and Identity and\nAccess Management (IAM) policies. The proposed RL based solution leverages\ncloud telemetry data (AWS Cloud Trail logs, network traffic data, threat\nintelligence feeds) to continuously refine security policies, maximizing threat\nmitigation, and compliance while minimizing resource impact. Experimental\nresults demonstrate that our adaptive RL based framework significantly\noutperforms static policies, achieving higher intrusion detection rates (92%\ncompared to 82% for static policies) and substantially reducing incident\ndetection and response times by 58%. In addition, it maintains high conformity\nwith security requirements and efficient resource usage. These findings\nvalidate the effectiveness of adaptive reinforcement learning approaches in\nimproving cloud security policy management.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u52a8\u6001\u5b89\u5168\u7b56\u7565\u7ba1\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e91\u73af\u5883\u4e2d\u9759\u6001\u5b89\u5168\u7b56\u7565\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5165\u4fb5\u68c0\u6d4b\u7387\u548c\u54cd\u5e94\u6548\u7387\u3002", "motivation": "\u4e91\u73af\u5883\uff08\u5982AWS\uff09\u7684\u5b89\u5168\u9700\u6c42\u590d\u6742\u4e14\u52a8\u6001\u53d8\u5316\uff0c\u9759\u6001\u5b89\u5168\u7b56\u7565\u5df2\u65e0\u6cd5\u5e94\u5bf9\u5a01\u80c1\u7684\u6f14\u5316\u548c\u8d44\u6e90\u7684\u5f39\u6027\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u6df1\u5ea6Q\u7f51\u7edc\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff09\uff0c\u5229\u7528\u4e91\u9065\u6d4b\u6570\u636e\u52a8\u6001\u8c03\u6574\u9632\u706b\u5899\u89c4\u5219\u548cIAM\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u7684\u5165\u4fb5\u68c0\u6d4b\u7387\u4e3a92%\uff08\u9759\u6001\u7b56\u7565\u4e3a82%\uff09\uff0c\u4e8b\u4ef6\u68c0\u6d4b\u548c\u54cd\u5e94\u65f6\u95f4\u51cf\u5c1158%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5408\u89c4\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4e91\u5b89\u5168\u7b56\u7565\u7ba1\u7406\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4f18\u4e8e\u9759\u6001\u7b56\u7565\u3002"}}
{"id": "2505.08838", "pdf": "https://arxiv.org/pdf/2505.08838", "abs": "https://arxiv.org/abs/2505.08838", "authors": ["Peixuan Ge", "Tongkun Su", "Faqin Lv", "Baoliang Zhao", "Peng Zhang", "Chi Hong Wong", "Liang Yao", "Yu Sun", "Zenan Wang", "Pak Kin Wong", "Ying Hu"], "title": "Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Ultrasound (US) report generation is a challenging task due to the\nvariability of US images, operator dependence, and the need for standardized\ntext. Unlike X-ray and CT, US imaging lacks consistent datasets, making\nautomation difficult. In this study, we propose a unified framework for\nmulti-organ and multilingual US report generation, integrating fragment-based\nmultilingual training and leveraging the standardized nature of US reports. By\naligning modular text fragments with diverse imaging data and curating a\nbilingual English-Chinese dataset, the method achieves consistent and\nclinically accurate text generation across organ sites and languages.\nFine-tuning with selective unfreezing of the vision transformer (ViT) further\nimproves text-image alignment. Compared to the previous state-of-the-art KMVE\nmethod, our approach achieves relative gains of about 2\\% in BLEU scores,\napproximately 3\\% in ROUGE-L, and about 15\\% in CIDEr, while significantly\nreducing errors such as missing or incorrect content. By unifying multi-organ\nand multi-language report generation into a single, scalable framework, this\nwork demonstrates strong potential for real-world clinical workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u5668\u5b98\u548c\u591a\u8bed\u8a00\u8d85\u58f0\u62a5\u544a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7247\u6bb5\u5316\u591a\u8bed\u8a00\u8bad\u7ec3\u548c\u6807\u51c6\u5316\u62a5\u544a\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u62a5\u544a\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u8d85\u58f0\u62a5\u544a\u751f\u6210\u56e0\u56fe\u50cf\u53d8\u5f02\u6027\u3001\u64cd\u4f5c\u4f9d\u8d56\u6027\u548c\u6807\u51c6\u5316\u9700\u6c42\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4e00\u81f4\u6027\u6570\u636e\u96c6\u3002", "method": "\u6574\u5408\u7247\u6bb5\u5316\u591a\u8bed\u8a00\u8bad\u7ec3\uff0c\u5229\u7528\u6807\u51c6\u5316\u62a5\u544a\u7279\u6027\uff0c\u7ed3\u5408\u53cc\u8bed\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u9009\u62e9\u6027\u89e3\u51bb\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u4f18\u5316\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u3002", "result": "\u76f8\u6bd4KMVE\u65b9\u6cd5\uff0cBLEU\u63d0\u53472%\uff0cROUGE-L\u63d0\u53473%\uff0cCIDEr\u63d0\u534715%\uff0c\u663e\u8457\u51cf\u5c11\u9519\u8bef\u5185\u5bb9\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u5668\u5b98\u548c\u591a\u8bed\u8a00\u62a5\u544a\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.08843", "pdf": "https://arxiv.org/pdf/2505.08843", "abs": "https://arxiv.org/abs/2505.08843", "authors": ["Marco Corrias", "Giada Franceschi", "Michele Riva", "Alberto Tampieri", "Karin F\u00f6ttinger", "Ulrike Diebold", "Thomas Pock", "Cesare Franchini"], "title": "Total Variation-Based Image Decomposition and Denoising for Microscopy Images", "categories": ["eess.IV", "cond-mat.mtrl-sci", "cs.CV"], "comment": null, "summary": "Experimentally acquired microscopy images are unavoidably affected by the\npresence of noise and other unwanted signals, which degrade their quality and\nmight hide relevant features. With the recent increase in image acquisition\nrate, modern denoising and restoration solutions become necessary. This study\nfocuses on image decomposition and denoising of microscopy images through a\nworkflow based on total variation (TV), addressing images obtained from various\nmicroscopy techniques, including atomic force microscopy (AFM), scanning\ntunneling microscopy (STM), and scanning electron microscopy (SEM). Our\napproach consists in restoring an image by extracting its unwanted signal\ncomponents and subtracting them from the raw one, or by denoising it. We\nevaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving\nthis goal in distinct study cases. Huber-ROF proved to be the most flexible\none, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a\nwider applicability of this method in microscopy, restricted not only to STM,\nAFM, and SEM images. The Python code used for this study is publicly available\nas part of AiSurf. It is designed to be integrated into experimental workflows\nfor image acquisition or can be used to denoise previously acquired images.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u603b\u53d8\u5206\uff08TV\uff09\u7684\u663e\u5fae\u955c\u56fe\u50cf\u5206\u89e3\u4e0e\u53bb\u566a\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86TV-L1\u3001Huber-ROF\u548cTGV-L1\u5728\u4e0d\u540c\u6848\u4f8b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86Huber-ROF\u7684\u7075\u6d3b\u6027\u548cTGV-L1\u7684\u53bb\u566a\u4f18\u52bf\u3002", "motivation": "\u663e\u5fae\u955c\u56fe\u50cf\u5e38\u53d7\u566a\u58f0\u548c\u5e72\u6270\u4fe1\u53f7\u5f71\u54cd\uff0c\u73b0\u4ee3\u53bb\u566a\u548c\u6062\u590d\u65b9\u6cd5\u9700\u6c42\u8feb\u5207\u3002", "method": "\u901a\u8fc7\u603b\u53d8\u5206\uff08TV\uff09\u65b9\u6cd5\u5206\u89e3\u56fe\u50cf\u5e76\u53bb\u9664\u566a\u58f0\uff0c\u8bc4\u4f30\u4e86TV-L1\u3001Huber-ROF\u548cTGV-L1\u7684\u6548\u679c\u3002", "result": "Huber-ROF\u8868\u73b0\u6700\u7075\u6d3b\uff0cTGV-L1\u6700\u9002\u5408\u53bb\u566a\uff0c\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u663e\u5fae\u955c\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u663e\u5fae\u955c\u56fe\u50cf\u5904\u7406\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u76f8\u5173Python\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.08845", "pdf": "https://arxiv.org/pdf/2505.08845", "abs": "https://arxiv.org/abs/2505.08845", "authors": ["Misgina Tsighe Hagos", "Antti Suutala", "Dmitrii Bychkov", "Hakan K\u00fcc\u00fckel", "Joar von Bahr", "Milda Poceviciute", "Johan Lundin", "Nina Linder", "Claes Lundstr\u00f6m"], "title": "Validation of Conformal Prediction in Cervical Atypia Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Deep learning based cervical cancer classification can potentially increase\naccess to screening in low-resource regions. However, deep learning models are\noften overconfident and do not reliably reflect diagnostic uncertainty.\nMoreover, they are typically optimized to generate maximum-likelihood\npredictions, which fail to convey uncertainty or ambiguity in their results.\nSuch challenges can be addressed using conformal prediction, a model-agnostic\nframework for generating prediction sets that contain likely classes for\ntrained deep-learning models. The size of these prediction sets indicates model\nuncertainty, contracting as model confidence increases. However, existing\nconformal prediction evaluation primarily focuses on whether the prediction set\nincludes or covers the true class, often overlooking the presence of extraneous\nclasses. We argue that prediction sets should be truthful and valuable to end\nusers, ensuring that the listed likely classes align with human expectations\nrather than being overly relaxed and including false positives or unlikely\nclasses. In this study, we comprehensively validate conformal prediction sets\nusing expert annotation sets collected from multiple annotators. We evaluate\nthree conformal prediction approaches applied to three deep-learning models\ntrained for cervical atypia classification. Our expert annotation-based\nanalysis reveals that conventional coverage-based evaluations overestimate\nperformance and that current conformal prediction methods often produce\nprediction sets that are not well aligned with human labels. Additionally, we\nexplore the capabilities of the conformal prediction methods in identifying\nambiguous and out-of-distribution data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5171\u5f62\u9884\u6d4b\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5bab\u9888\u764c\u5206\u7c7b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u9a8c\u8bc1\u5176\u771f\u5b9e\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5bab\u9888\u764c\u5206\u7c7b\u4e2d\u5e38\u8fc7\u4e8e\u81ea\u4fe1\u4e14\u65e0\u6cd5\u53ef\u9760\u53cd\u6620\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\uff0c\u5171\u5f62\u9884\u6d4b\u53ef\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u7565\u4e86\u9884\u6d4b\u96c6\u7684\u771f\u5b9e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u4e09\u79cd\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u591a\u6807\u6ce8\u8005\u7684\u4e13\u5bb6\u6807\u6ce8\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u7684\u8986\u76d6\u7387\u8bc4\u4f30\u9ad8\u4f30\u6027\u80fd\uff0c\u5171\u5f62\u9884\u6d4b\u751f\u6210\u7684\u9884\u6d4b\u96c6\u4e0e\u4eba\u7c7b\u6807\u6ce8\u4e0d\u4e00\u81f4\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u5176\u5728\u8bc6\u522b\u6a21\u7cca\u548c\u5206\u5e03\u5916\u6570\u636e\u7684\u80fd\u529b\u3002", "conclusion": "\u5171\u5f62\u9884\u6d4b\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u751f\u6210\u66f4\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u7684\u9884\u6d4b\u96c6\uff0c\u63d0\u9ad8\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.08889", "pdf": "https://arxiv.org/pdf/2505.08889", "abs": "https://arxiv.org/abs/2505.08889", "authors": ["Linjie Lyu", "Valentin Deschaintre", "Yannick Hold-Geoffroy", "Milo\u0161 Ha\u0161an", "Jae Shin Yoon", "Thomas Leimk\u00fchler", "Christian Theobalt", "Iliyan Georgiev"], "title": "IntrinsicEdit: Precise generative image manipulation in intrinsic space", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025 Journal track", "summary": "Generative diffusion models have advanced image editing with high-quality\nresults and intuitive interfaces such as prompts and semantic drawing. However,\nthese interfaces lack precise control, and the associated methods typically\nspecialize on a single editing task. We introduce a versatile, generative\nworkflow that operates in an intrinsic-image latent space, enabling semantic,\nlocal manipulation with pixel precision for a range of editing operations.\nBuilding atop the RGB-X diffusion framework, we address key challenges of\nidentity preservation and intrinsic-channel entanglement. By incorporating\nexact diffusion inversion and disentangled channel manipulation, we enable\nprecise, efficient editing with automatic resolution of global illumination\neffects -- all without additional data collection or model fine-tuning. We\ndemonstrate state-of-the-art performance across a variety of tasks on complex\nimages, including color and texture adjustments, object insertion and removal,\nglobal relighting, and their combinations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB-X\u6269\u6563\u6846\u67b6\u7684\u901a\u7528\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u50cf\u7d20\u7ea7\u7cbe\u786e\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u6301\u548c\u901a\u9053\u7ea0\u7f20\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u7f3a\u4e4f\u7cbe\u786e\u63a7\u5236\uff0c\u4e14\u901a\u5e38\u4ec5\u9002\u7528\u4e8e\u5355\u4e00\u4efb\u52a1\u3002", "method": "\u91c7\u7528RGB-X\u6269\u6563\u6846\u67b6\uff0c\u7ed3\u5408\u7cbe\u786e\u6269\u6563\u53cd\u6f14\u548c\u901a\u9053\u89e3\u7f20\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7cbe\u786e\u7f16\u8f91\u3002", "result": "\u5728\u590d\u6742\u56fe\u50cf\u4e0a\u5c55\u793a\u4e86\u591a\u79cd\u4efb\u52a1\u7684\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ec\u989c\u8272\u8c03\u6574\u3001\u5bf9\u8c61\u63d2\u5165\u5220\u9664\u548c\u5168\u5c40\u5149\u7167\u8c03\u6574\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u591a\u529f\u80fd\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u8d44\u6e90\u3002"}}
{"id": "2505.08919", "pdf": "https://arxiv.org/pdf/2505.08919", "abs": "https://arxiv.org/abs/2505.08919", "authors": ["Kangxian Xie", "Yufei Zhu", "Kaiming Kuang", "Li Zhang", "Hongwei Bran Li", "Mingchen Gao", "Jiancheng Yang"], "title": "Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "In revision process", "summary": "High-quality 3D reconstruction of pulmonary segments plays a crucial role in\nsegmentectomy and surgical treatment planning for lung cancer. Due to the\nresolution requirement of the target reconstruction, conventional deep\nlearning-based methods often suffer from computational resource constraints or\nlimited granularity. Conversely, implicit modeling is favored due to its\ncomputational efficiency and continuous representation at any resolution. We\npropose a neural implicit function-based method to learn a 3D surface to\nachieve anatomy-aware, precise pulmonary segment reconstruction, represented as\na shape by deforming a learnable template. Additionally, we introduce two\nclinically relevant evaluation metrics to assess the reconstruction\ncomprehensively. Further, due to the absence of publicly available shape\ndatasets to benchmark reconstruction algorithms, we developed a shape dataset\nnamed Lung3D, including the 3D models of 800 labeled pulmonary segments and the\ncorresponding airways, arteries, veins, and intersegmental veins. We\ndemonstrate that the proposed approach outperforms existing methods, providing\na new perspective for pulmonary segment reconstruction. Code and data will be\navailable at https://github.com/M3DV/ImPulSe.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u9690\u5f0f\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf3D\u80ba\u6bb5\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8ba1\u7b97\u8d44\u6e90\u548c\u5206\u8fa8\u7387\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u9ad8\u8d28\u91cf\u76843D\u80ba\u6bb5\u91cd\u5efa\u5bf9\u80ba\u6bb5\u5207\u9664\u672f\u548c\u80ba\u764c\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6216\u5206\u8fa8\u7387\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u9690\u5f0f\u51fd\u6570\u5b66\u4e603D\u8868\u9762\uff0c\u901a\u8fc7\u53d8\u5f62\u53ef\u5b66\u4e60\u6a21\u677f\u5b9e\u73b0\u89e3\u5256\u611f\u77e5\u7684\u7cbe\u786e\u91cd\u5efa\uff0c\u5e76\u5f15\u5165\u4e24\u4e2a\u4e34\u5e8a\u76f8\u5173\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86Lung3D\u6570\u636e\u96c6\uff0c\u5305\u542b800\u4e2a\u6807\u8bb0\u80ba\u6bb5\u76843D\u6a21\u578b\u53ca\u76f8\u5173\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u80ba\u6bb5\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002"}}
{"id": "2505.08932", "pdf": "https://arxiv.org/pdf/2505.08932", "abs": "https://arxiv.org/abs/2505.08932", "authors": ["Mohammad Wasil", "Ahmad Drak", "Brennan Penfold", "Ludovico Scarton", "Maximilian Johenneken", "Alexander Asteroth", "Sebastian Houben"], "title": "Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to the Novel Approaches for Precision Agriculture and\n  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and\nforest monitoring, including seed dispersal in hard-to-reach terrains. However,\na detailed understanding of the forest floor remains a challenge due to high\nnatural variability, quickly changing environmental parameters, and ambiguous\nannotations due to unclear definitions. To address this issue, we adapt the\nSegment Anything Model (SAM), a vision foundation model with strong\ngeneralization capabilities, to segment forest floor objects such as tree\nstumps, vegetation, and woody debris. To this end, we employ\nparameter-efficient fine-tuning (PEFT) to fine-tune a small subset of\nadditional model parameters while keeping the original weights fixed. We adjust\nSAM's mask decoder to generate masks corresponding to our dataset categories,\nallowing for automatic segmentation without manual prompting. Our results show\nthat the adapter-based PEFT method achieves the highest mean intersection over\nunion (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a\nlightweight alternative for resource-constrained UAV platforms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSegment Anything Model (SAM)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u68ee\u6797\u5730\u9762\u5bf9\u8c61\uff08\u5982\u6811\u6869\u3001\u690d\u88ab\u548c\u6728\u8d28\u6b8b\u9ab8\uff09\u7684\u81ea\u52a8\u5206\u5272\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u68ee\u6797\u5730\u9762\u5bf9\u8c61\u7684\u8be6\u7ec6\u7406\u89e3\u56e0\u9ad8\u81ea\u7136\u53d8\u5f02\u6027\u3001\u5feb\u901f\u53d8\u5316\u7684\u73af\u5883\u53c2\u6570\u548c\u6a21\u7cca\u7684\u6ce8\u91ca\u5b9a\u4e49\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u8c03\u6574SAM\u7684\u63a9\u7801\u89e3\u7801\u5668\u4ee5\u81ea\u52a8\u751f\u6210\u5bf9\u5e94\u6570\u636e\u7c7b\u522b\u7684\u63a9\u7801\u3002", "result": "\u57fa\u4e8e\u9002\u914d\u5668\u7684PEFT\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08mIoU\uff09\uff0c\u800c\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5219\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u68ee\u6797\u5730\u9762\u5bf9\u8c61\u5206\u5272\u7684\u6311\u6218\uff0c\u540c\u65f6\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u63d0\u4f9b\u4e86\u4f18\u5316\u9009\u62e9\u3002"}}
{"id": "2505.08949", "pdf": "https://arxiv.org/pdf/2505.08949", "abs": "https://arxiv.org/abs/2505.08949", "authors": ["Kateryna Zorina", "David Kovar", "Mederic Fourmy", "Florent Lamiraux", "Nicolas Mansard", "Justin Carpentier", "Josef Sivic", "Vladimir Petrik"], "title": "Multi-step manipulation task and motion planning guided by video demonstration", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "This work aims to leverage instructional video to solve complex multi-step\ntask-and-motion planning tasks in robotics. Towards this goal, we propose an\nextension of the well-established Rapidly-Exploring Random Tree (RRT) planner,\nwhich simultaneously grows multiple trees around grasp and release states\nextracted from the guiding video. Our key novelty lies in combining contact\nstates and 3D object poses extracted from the guiding video with a traditional\nplanning algorithm that allows us to solve tasks with sequential dependencies,\nfor example, if an object needs to be placed at a specific location to be\ngrasped later. We also investigate the generalization capabilities of our\napproach to go beyond the scene depicted in the instructional video. To\ndemonstrate the benefits of the proposed video-guided planning approach, we\ndesign a new benchmark with three challenging tasks: (I) 3D re-arrangement of\nmultiple objects between a table and a shelf, (ii) multi-step transfer of an\nobject through a tunnel, and (iii) transferring objects using a tray similar to\na waiter transfers dishes. We demonstrate the effectiveness of our planning\nalgorithm on several robots, including the Franka Emika Panda and the KUKA KMR\niiwa. For a seamless transfer of the obtained plans to the real robot, we\ndevelop a trajectory refinement approach formulated as an optimal control\nproblem (OCP).", "AI": {"tldr": "\u5229\u7528\u6559\u5b66\u89c6\u9891\u6307\u5bfc\u673a\u5668\u4eba\u5b8c\u6210\u590d\u6742\u591a\u6b65\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u51fa\u57fa\u4e8eRRT\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u9891\u63d0\u53d6\u7684\u63a5\u89e6\u72b6\u6001\u548c3D\u7269\u4f53\u4f4d\u59ff\uff0c\u89e3\u51b3\u987a\u5e8f\u4f9d\u8d56\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u590d\u6742\u591a\u6b65\u4efb\u52a1\u89c4\u5212\u95ee\u9898\uff0c\u5229\u7528\u6559\u5b66\u89c6\u9891\u63d0\u4f9b\u76f4\u89c2\u6307\u5bfc\u3002", "method": "\u6269\u5c55RRT\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u89c6\u9891\u63d0\u53d6\u7684\u63a5\u89e6\u72b6\u6001\u548c3D\u7269\u4f53\u4f4d\u59ff\uff0c\u8bbe\u8ba1\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u8bbe\u8ba1\u4e86\u65b0\u57fa\u51c6\u4efb\u52a1\u5e76\u5c55\u793a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u89c6\u9891\u5f15\u5bfc\u7684\u89c4\u5212\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.08990", "pdf": "https://arxiv.org/pdf/2505.08990", "abs": "https://arxiv.org/abs/2505.08990", "authors": ["Andrew C. Freeman"], "title": "Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ", "categories": ["cs.MM", "cs.CV", "cs.DC", "cs.NI"], "comment": "Accepted to the ICME 2025 LIVES workshop", "summary": "Live video streaming is increasingly popular on social media platforms. With\nthe growth of live streaming comes an increased need for robust content\nmoderation to remove dangerous, illegal, or otherwise objectionable content.\nWhereas video on demand distribution enables offline content analysis, live\nstreaming imposes restrictions on latency for both analysis and distribution.\nIn this paper, we present extensions to the in-progress Media Over QUIC\nTransport protocol that enable real-time content moderation in one-to-many\nvideo live streams. Importantly, our solution removes only the video segments\nthat contain objectionable content, allowing playback resumption as soon as the\nstream conforms to content policies again. Content analysis tasks may be\ntransparently distributed to arbitrary client devices. We implement and\nevaluate our system in the context of light strobe removal for photosensitive\nviewers, finding that streaming clients experience an increased latency of only\none group-of-pictures duration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMedia Over QUIC Transport\u534f\u8bae\u7684\u5b9e\u65f6\u5185\u5bb9\u5ba1\u6838\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e00\u5bf9\u591a\u89c6\u9891\u76f4\u64ad\u6d41\uff0c\u4ec5\u5220\u9664\u8fdd\u89c4\u5185\u5bb9\u7247\u6bb5\uff0c\u540c\u65f6\u652f\u6301\u5ba2\u6237\u7aef\u5206\u5e03\u5f0f\u5206\u6790\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u76f4\u64ad\u6d41\u5a92\u4f53\u7684\u666e\u53ca\uff0c\u5b9e\u65f6\u5185\u5bb9\u5ba1\u6838\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u6280\u672f\u96be\u4ee5\u5728\u4f4e\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5ba1\u6838\u3002", "method": "\u6269\u5c55Media Over QUIC Transport\u534f\u8bae\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5185\u5bb9\u5ba1\u6838\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u5ba2\u6237\u7aef\u5206\u6790\u4efb\u52a1\u3002", "result": "\u7cfb\u7edf\u5728\u5149\u654f\u6027\u89c2\u4f17\u573a\u666f\u4e0b\u6d4b\u8bd5\uff0c\u4ec5\u589e\u52a0\u4e00\u4e2aGOP\uff08\u56fe\u50cf\u7ec4\uff09\u65f6\u957f\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u76f4\u64ad\u6d41\u5b9e\u65f6\u5185\u5bb9\u5ba1\u6838\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u7075\u6d3b\u7684\u5185\u5bb9\u5206\u6790\u4efb\u52a1\u5206\u914d\u3002"}}
{"id": "2505.08998", "pdf": "https://arxiv.org/pdf/2505.08998", "abs": "https://arxiv.org/abs/2505.08998", "authors": ["Liwen Wu", "Sai Bi", "Zexiang Xu", "Hao Tan", "Kai Zhang", "Fujun Luan", "Haolin Lu", "Ravi Ramamoorthi"], "title": "Neural BRDF Importance Sampling by Reparameterization", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Neural bidirectional reflectance distribution functions (BRDFs) have emerged\nas popular material representations for enhancing realism in physically-based\nrendering. Yet their importance sampling remains a significant challenge. In\nthis paper, we introduce a reparameterization-based formulation of neural BRDF\nimportance sampling that seamlessly integrates into the standard rendering\npipeline with precise generation of BRDF samples. The reparameterization-based\nformulation transfers the distribution learning task to a problem of\nidentifying BRDF integral substitutions. In contrast to previous methods that\nrely on invertible networks and multi-step inference to reconstruct BRDF\ndistributions, our model removes these constraints, which offers greater\nflexibility and efficiency. Our variance and performance analysis demonstrates\nthat our reparameterization method achieves the best variance reduction in\nneural BRDF renderings while maintaining high inference speeds compared to\nexisting baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u53c2\u6570\u5316\u7684\u795e\u7ecfBRDF\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6e32\u67d3\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u795e\u7ecfBRDF\u5728\u7269\u7406\u6e32\u67d3\u4e2d\u63d0\u5347\u771f\u5b9e\u611f\uff0c\u4f46\u5176\u91cd\u8981\u6027\u91c7\u6837\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u5c06\u5206\u5e03\u5b66\u4e60\u4efb\u52a1\u8f6c\u5316\u4e3aBRDF\u79ef\u5206\u66ff\u6362\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u53ef\u9006\u7f51\u7edc\u548c\u591a\u6b65\u63a8\u7406\u3002", "result": "\u5728\u795e\u7ecfBRDF\u6e32\u67d3\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u65b9\u5dee\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecfBRDF\u91cd\u8981\u6027\u91c7\u6837\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09040", "pdf": "https://arxiv.org/pdf/2505.09040", "abs": "https://arxiv.org/abs/2505.09040", "authors": ["Owen Kwon", "Abraham George", "Alison Bartsch", "Amir Barati Farimani"], "title": "RT-cache: Efficient Robot Trajectory Retrieval System", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference", "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.", "AI": {"tldr": "RT-cache\u901a\u8fc7\u5b58\u50a8\u548c\u68c0\u7d22\u6210\u529f\u8f68\u8ff9\u7247\u6bb5\uff0c\u663e\u8457\u964d\u4f4e\u673a\u5668\u4eba\u63a8\u7406\u5ef6\u8fdf\uff0c\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u901f\u5ea6\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u4ee3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u591a\u6837\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5355\u6b65\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u95ee\u9898\u3002RT-cache\u65e8\u5728\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u548c\u5927\u6570\u636e\u68c0\u7d22\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RT-cache\u7ed3\u5408Memory Builder\u548cTrajectory Retrieval\uff0c\u5b58\u50a8\u5927\u89c4\u6a21\u6210\u529f\u8f68\u8ff9\uff0c\u5e76\u5728\u5339\u914d\u573a\u666f\u65f6\u5feb\u901f\u68c0\u7d22\u591a\u6b65\u8fd0\u52a8\u7247\u6bb5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRT-cache\u5728Open-X Embodiment Dataset\u7b49\u6570\u636e\u4e0a\u6bd4\u65e0\u68c0\u7d22\u57fa\u7ebf\u66f4\u5feb\u3001\u66f4\u6210\u529f\u5730\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "RT-cache\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6570\u636e\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09091", "pdf": "https://arxiv.org/pdf/2505.09091", "abs": "https://arxiv.org/abs/2505.09091", "authors": ["Zeeshan Ahmad", "Shudi Bao", "Meng Chen"], "title": "DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "comment": null, "summary": "In recent years, generative adversarial networks (GANs) have made significant\nprogress in generating audio sequences. However, these models typically rely on\nbandwidth-limited mel-spectrograms, which constrain the resolution of generated\naudio sequences, and lead to mode collapse during conditional generation. To\naddress this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),\na novel GAN architecture that incorporates a kernel-based periodic ReLU\nactivation function to induce periodic bias in audio generation. This\ninnovative approach enhances the model's ability to capture and reproduce\nintricate audio patterns. In particular, our proposed model features a DPN\nmodule for multi-resolution generation utilizing deformable convolution\noperations, allowing for adaptive receptive fields that improve the quality and\nfidelity of the synthetic audio. Additionally, we enhance the discriminator\nnetwork using deformable convolution to better distinguish between real and\ngenerated samples, further refining the audio quality. We trained two versions\nof the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M\nparameters). For evaluation, we use five different datasets, covering both\nspeech synthesis and music generation tasks, to demonstrate the efficiency of\nthe DPN-GAN. The experimental results demonstrate that DPN-GAN delivers\nsuperior performance on both out-of-distribution and noisy data, showcasing its\nrobustness and adaptability. Trained across various datasets, DPN-GAN\noutperforms state-of-the-art GAN architectures on standard evaluation metrics,\nand exhibits increased robustness in synthesized audio.", "AI": {"tldr": "DPN-GAN\u662f\u4e00\u79cd\u65b0\u578bGAN\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u6838\u7684\u5468\u671f\u6027ReLU\u6fc0\u6d3b\u51fd\u6570\u548c\u591a\u5206\u8fa8\u7387\u751f\u6210\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709GAN\u6a21\u578b\u4f9d\u8d56\u5e26\u5bbd\u53d7\u9650\u7684mel\u9891\u8c31\u56fe\uff0c\u5bfc\u81f4\u751f\u6210\u97f3\u9891\u5206\u8fa8\u7387\u53d7\u9650\u548c\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u63d0\u51faDPN-GAN\uff0c\u7ed3\u5408\u5468\u671f\u6027ReLU\u6fc0\u6d3b\u51fd\u6570\u548c\u53ef\u53d8\u5f62\u5377\u79ef\u64cd\u4f5c\uff0c\u589e\u5f3a\u97f3\u9891\u6a21\u5f0f\u6355\u6349\u80fd\u529b\uff1b\u6539\u8fdb\u5224\u522b\u5668\u7f51\u7edc\u4ee5\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDPN-GAN\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709GAN\u6a21\u578b\uff0c\u751f\u6210\u97f3\u9891\u8d28\u91cf\u66f4\u9ad8\u4e14\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "DPN-GAN\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u89e3\u51b3\u4e86\u97f3\u9891\u751f\u6210\u4e2d\u7684\u5206\u8fa8\u7387\u4e0e\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.09109", "pdf": "https://arxiv.org/pdf/2505.09109", "abs": "https://arxiv.org/abs/2505.09109", "authors": ["Yuxing Chen", "Bowen Xiao", "He Wang"], "title": "FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Due to the deformability of garments, generating a large amount of\nhigh-quality data for robotic garment manipulation tasks is highly challenging.\nIn this paper, we present a synthetic garment dataset that can be used for\nrobotic garment folding. We begin by constructing geometric garment templates\nbased on keypoints and applying generative models to generate realistic texture\npatterns. Leveraging these keypoint annotations, we generate folding\ndemonstrations in simulation and train folding policies via closed-loop\nimitation learning. To improve robustness, we propose KG-DAgger, which uses a\nkeypoint-based strategy to generate demonstration data for recovering from\nfailures. KG-DAgger significantly improves the model performance, boosting the\nreal-world success rate by 25\\%. After training with 15K trajectories (about 2M\nimage-action pairs), the model achieves a 75\\% success rate in the real world.\nExperiments in both simulation and real-world settings validate the\neffectiveness of our proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u8863\u7269\u6298\u53e0\u4efb\u52a1\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u51e0\u4f55\u6a21\u677f\u548c\u751f\u6210\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5e76\u5229\u7528\u5173\u952e\u70b9\u6807\u6ce8\u8bad\u7ec3\u6298\u53e0\u7b56\u7565\uff0c\u6700\u7ec8\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u8fbe\u523075%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u7531\u4e8e\u8863\u7269\u7684\u53ef\u53d8\u5f62\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u8863\u7269\u64cd\u4f5c\u4efb\u52a1\u751f\u6210\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6784\u5efa\u51e0\u4f55\u8863\u7269\u6a21\u677f\uff0c\u5e94\u7528\u751f\u6210\u6a21\u578b\u751f\u6210\u7eb9\u7406\uff0c\u5229\u7528\u5173\u952e\u70b9\u6807\u6ce8\u751f\u6210\u6298\u53e0\u6f14\u793a\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u95ed\u73af\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u3002\u63d0\u51faKG-DAgger\u65b9\u6cd5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "KG-DAgger\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u771f\u5b9e\u4e16\u754c\u6210\u529f\u7387\u63d0\u9ad825%\uff0c\u6700\u7ec8\u8fbe\u523075%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u8863\u7269\u6298\u53e0\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2505.09175", "pdf": "https://arxiv.org/pdf/2505.09175", "abs": "https://arxiv.org/abs/2505.09175", "authors": ["Mohammad Ganjirad", "Mahmoud Reza Delavar", "Hossein Bagheri", "Mohammad Mehdi Azizi"], "title": "Optimizing Urban Critical Green Space Development Using Machine Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This paper presents a novel framework for prioritizing urban green space\ndevelopment in Tehran using diverse socio-economic, environmental, and\nsensitivity indices. The indices were derived from various sources including\nGoogle Earth Engine, air pollution measurements, municipal reports and the\nWeather Research & Forecasting (WRF) model. The WRF model was used to estimate\nthe air temperature at a 1 km resolution due to insufficient meteorological\nstations, yielding RMSE and MAE values of 0.96{\\deg}C and 0.92{\\deg}C,\nrespectively. After data preparation, several machine learning models were used\nfor binary vegetation cover classification including XGBoost, LightGBM, Random\nForest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%\nin Overall Accuracy, Recall, and F1-score. Then, the probability of areas\nlacking vegetation cover was assessed using socio-economic, environmental and\nsensitivity indices. This resulted in the RF generating an urban green space\ndevelopment prioritization map. Feature Importance Analysis revealed that the\nmost significant indices were nightly land surface temperature (LST) and\nsensitive population. Finally, the framework performance was validated through\nmicroclimate simulation to assess the critical areas after and before the green\nspace development by green roofs. The simulation demonstrated reducing air\ntemperature by up to 0.67{\\deg}C after utilizing the green roof technology in\ncritical areas. As a result, this framework provides a valuable tool for urban\nplanners to develop green spaces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6e90\u6570\u636e\u7684\u57ce\u5e02\u7eff\u5730\u5f00\u53d1\u4f18\u5148\u7ea7\u6846\u67b6\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5fae\u6c14\u5019\u6a21\u62df\uff0c\u4e3a\u5fb7\u9ed1\u5170\u7684\u7eff\u5730\u89c4\u5212\u63d0\u4f9b\u4e86\u79d1\u5b66\u4f9d\u636e\u3002", "motivation": "\u7531\u4e8e\u5fb7\u9ed1\u5170\u7eff\u5730\u5206\u5e03\u4e0d\u5747\u4e14\u6c14\u8c61\u7ad9\u6570\u636e\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u4f18\u5148\u5f00\u53d1\u7eff\u5730\uff0c\u4ee5\u6539\u5584\u57ce\u5e02\u73af\u5883\u548c\u5c45\u6c11\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u4f7f\u7528WRF\u6a21\u578b\u4f30\u7b97\u6c14\u6e29\uff0c\u7ed3\u5408\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982XGBoost\u3001LightGBM\u3001RF\uff09\u8fdb\u884c\u690d\u88ab\u8986\u76d6\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u786e\u5b9a\u5173\u952e\u6307\u6807\u3002", "result": "RF\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u7387>94%\uff09\uff0c\u591c\u95f4\u5730\u8868\u6e29\u5ea6\u548c\u654f\u611f\u4eba\u53e3\u662f\u6700\u91cd\u8981\u6307\u6807\uff1b\u5fae\u6c14\u5019\u6a21\u62df\u663e\u793a\u7eff\u8272\u5c4b\u9876\u6280\u672f\u53ef\u964d\u6e290.67\u00b0C\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u57ce\u5e02\u7eff\u5730\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u663e\u8457\u6539\u5584\u4e86\u57ce\u5e02\u5fae\u6c14\u5019\u3002"}}
{"id": "2505.09193", "pdf": "https://arxiv.org/pdf/2505.09193", "abs": "https://arxiv.org/abs/2505.09193", "authors": ["Wei Jiang", "Junru Li", "Kai Zhang", "Li Zhang"], "title": "BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression", "categories": ["eess.IV", "cs.CV"], "comment": "The first learned video codec that surpasses VTM 13.2 RA across all\n  standard test datasets. Code will be available at\n  https://github.com/JiangWeibeta/ECVC", "summary": "Recent forward prediction-based learned video compression (LVC) methods have\nachieved impressive results, even surpassing VVC reference software VTM under\nthe Low Delay B (LDB) configuration. In contrast, learned bidirectional video\ncompression (BVC) remains underexplored and still lags behind its forward-only\ncounterparts. This performance gap is mainly due to the limited ability to\nextract diverse and accurate contexts: most existing BVCs primarily exploit\ntemporal motion while neglecting non-local correlations across frames.\nMoreover, they lack the adaptability to dynamically suppress harmful contexts\narising from fast motion or occlusion. To tackle these challenges, we propose\nBiECVC, a BVC framework that incorporates diversified local and non-local\ncontext modeling along with adaptive context gating. For local context\nenhancement, BiECVC reuses high-quality features from lower layers and aligns\nthem using decoded motion vectors without introducing extra motion overhead.To\nmodel non-local dependencies efficiently, we adopt a linear attention mechanism\nthat balances performance and complexity. To further mitigate the impact of\ninaccurate context prediction, we introduce Bidirectional Context Gating,\ninspired by data-dependent decay in recent autoregressive language models, to\ndynamically filter contextual information based on conditional coding results.\nExtensive experiments demonstrate that BiECVC achieves state-of-the-art\nperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2\nunder the Random Access (RA) configuration with intra periods of 32 and 64,\nrespectively. To our knowledge, BiECVC is the first learned video codec to\nsurpass VTM 13.2 RA across all standard test datasets. Code will be available\nat https://github.com/JiangWeibeta/ECVC.", "AI": {"tldr": "BiECVC\u662f\u4e00\u79cd\u53cc\u5411\u89c6\u9891\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86VTM 13.2\u3002", "motivation": "\u73b0\u6709\u53cc\u5411\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\u5728\u63d0\u53d6\u591a\u6837\u5316\u548c\u51c6\u786e\u4e0a\u4e0b\u6587\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u52a8\u6001\u6291\u5236\u6709\u5bb3\u4e0a\u4e0b\u6587\u7684\u673a\u5236\u3002", "method": "BiECVC\u7ed3\u5408\u5c40\u90e8\u548c\u975e\u5c40\u90e8\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u91c7\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u548c\u53cc\u5411\u4e0a\u4e0b\u6587\u95e8\u63a7\u52a8\u6001\u8fc7\u6ee4\u4fe1\u606f\u3002", "result": "BiECVC\u5728RA\u914d\u7f6e\u4e0b\u6bd4\u7279\u7387\u964d\u4f4e13.4%\u548c15.7%\uff0c\u6027\u80fd\u4f18\u4e8eVTM 13.2\u3002", "conclusion": "BiECVC\u662f\u9996\u4e2a\u5728\u6240\u6709\u6807\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aVTM 13.2\u7684\u5b66\u4e60\u578b\u89c6\u9891\u7f16\u89e3\u7801\u5668\u3002"}}
{"id": "2505.09262", "pdf": "https://arxiv.org/pdf/2505.09262", "abs": "https://arxiv.org/abs/2505.09262", "authors": ["Hongxin Xiang", "Ke Li", "Mingquan Liu", "Zhixiang Cheng", "Bin Yao", "Wenjie Du", "Jun Xia", "Li Zeng", "Xin Jin", "Xiangxiang Zeng"], "title": "EDBench: Large-Scale Electron Density Data for Molecular Modeling", "categories": ["physics.chem-ph", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Existing molecular machine learning force fields (MLFFs) generally focus on\nthe learning of atoms, molecules, and simple quantum chemical properties (such\nas energy and force), but ignore the importance of electron density (ED)\n$\\rho(r)$ in accurately understanding molecular force fields (MFFs). ED\ndescribes the probability of finding electrons at specific locations around\natoms or molecules, which uniquely determines all ground state properties (such\nas energy, molecular structure, etc.) of interactive multi-particle systems\naccording to the Hohenberg-Kohn theorem. However, the calculation of ED relies\non the time-consuming first-principles density functional theory (DFT) which\nleads to the lack of large-scale ED data and limits its application in MLFFs.\nIn this paper, we introduce EDBench, a large-scale, high-quality dataset of ED\ndesigned to advance learning-based research at the electronic scale. Built upon\nthe PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million\nmolecules. To comprehensively evaluate the ability of models to understand and\nutilize electronic information, we design a suite of ED-centric benchmark tasks\nspanning prediction, retrieval, and generation. Our evaluation on several\nstate-of-the-art methods demonstrates that learning from EDBench is not only\nfeasible but also achieves high accuracy. Moreover, we show that learning-based\nmethod can efficiently calculate ED with comparable precision while\nsignificantly reducing the computational cost relative to traditional DFT\ncalculations. All data and benchmarks from EDBench will be freely available,\nlaying a robust foundation for ED-driven drug discovery and materials science.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEDBench\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u7535\u5b50\u5bc6\u5ea6\uff08ED\uff09\u5728\u673a\u5668\u5b66\u4e60\u529b\u573a\uff08MLFFs\uff09\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\uff0c\u5e76\u5c55\u793a\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u9ad8\u6548\u8ba1\u7b97ED\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709MLFFs\u5ffd\u7565\u7535\u5b50\u5bc6\u5ea6\uff08ED\uff09\u7684\u91cd\u8981\u6027\uff0c\u800cED\u662f\u7406\u89e3\u5206\u5b50\u529b\u573a\u7684\u5173\u952e\u3002\u4f20\u7edfDFT\u8ba1\u7b97ED\u8017\u65f6\u4e14\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u57fa\u4e8ePCQM4Mv2\u6784\u5efaEDBench\u6570\u636e\u96c6\uff0c\u5305\u542b330\u4e07\u5206\u5b50\u7684\u9ad8\u8d28\u91cfED\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217ED\u76f8\u5173\u57fa\u51c6\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eEDBench\u7684\u5b66\u4e60\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u884c\uff0c\u8fd8\u80fd\u9ad8\u6548\u8ba1\u7b97ED\uff0c\u7cbe\u5ea6\u4e0e\u4f20\u7edfDFT\u76f8\u5f53\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "EDBench\u4e3aED\u9a71\u52a8\u7684\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u79d1\u5b66\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5176\u6570\u636e\u548c\u57fa\u51c6\u4efb\u52a1\u5c06\u516c\u5f00\u5171\u4eab\u3002"}}
{"id": "2505.09315", "pdf": "https://arxiv.org/pdf/2505.09315", "abs": "https://arxiv.org/abs/2505.09315", "authors": ["Xuefeng Jiang", "Yuan Ma", "Pengxiang Li", "Leimeng Xu", "Xin Wen", "Kun Zhan", "Zhongpu Xia", "Peng Jia", "XianPeng Lang", "Sheng Sun"], "title": "TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Under review", "summary": "In recent years, diffusion model has shown its potential across diverse\ndomains from vision generation to language modeling. Transferring its\ncapabilities to modern autonomous driving systems has also emerged as a\npromising direction.In this work, we propose TransDiffuser, an encoder-decoder\nbased generative trajectory planning model for end-to-end autonomous driving.\nThe encoded scene information serves as the multi-modal conditional input of\nthe denoising decoder. To tackle the mode collapse dilemma in generating\nhigh-quality diverse trajectories, we introduce a simple yet effective\nmulti-modal representation decorrelation optimization mechanism during the\ntraining process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,\nsurpassing previous state-of-the-art methods without any anchor-based prior\ntrajectories.", "AI": {"tldr": "TransDiffuser\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\u548c\u53bb\u76f8\u5173\u4f18\u5316\u673a\u5236\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u8f68\u8ff9\u3002", "motivation": "\u5c06\u6269\u6563\u6a21\u578b\u7684\u6f5c\u529b\u6269\u5c55\u5230\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u89e3\u51b3\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5229\u7528\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\u548c\u8bad\u7ec3\u4e2d\u7684\u53bb\u76f8\u5173\u4f18\u5316\u673a\u5236\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523094.85 PDMS\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TransDiffuser\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.09323", "pdf": "https://arxiv.org/pdf/2505.09323", "abs": "https://arxiv.org/abs/2505.09323", "authors": ["Pengli Zhu", "Yingji Fu", "Nanguang Chen", "Anqi Qiu"], "title": "Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025", "summary": "This study, we propose a novel Q-space Guided Collaborative Attention\nTranslation Networks (Q-CATN) for multi-shell, high-angular resolution DWI\n(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly\nacquired structural MRI data. Q-CATN employs a collaborative attention\nmechanism to effectively extract complementary information from multiple\nmodalities and dynamically adjust its internal representations based on\nflexible q-space information, eliminating the need for fixed sampling schemes.\nAdditionally, we introduce a range of task-specific constraints to preserve\nanatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic\nrelationships between directional DWI signal distributions and q-space.\nExtensive experiments on the Human Connectome Project (HCP) dataset demonstrate\nthat Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,\nand QGAN, in estimating parameter maps and fiber tracts both quantitatively and\nqualitatively, while preserving fine-grained details. Notably, its ability to\naccommodate flexible q-space sampling highlights its potential as a promising\ntoolkit for clinical and research applications. Our code is available at\nhttps://github.com/Idea89560041/Q-CATN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQ-CATN\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7075\u6d3b\u7684q\u7a7a\u95f4\u91c7\u6837\u4e2d\u5408\u6210\u591a\u58f3\u9ad8\u89d2\u5ea6\u5206\u8fa8\u7387DWI\u6570\u636e\uff0c\u5229\u7528\u7ed3\u6784MRI\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u8c03\u6574\u5185\u90e8\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7075\u6d3bq\u7a7a\u95f4\u91c7\u6837\u4e0b\u5408\u6210DWI\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u5256\u4fdd\u771f\u5ea6\u3002", "method": "\u91c7\u7528\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\u4ece\u591a\u6a21\u6001\u6570\u636e\u4e2d\u63d0\u53d6\u4e92\u8865\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f\u4ee5\u4fdd\u6301DWI\u7684\u89e3\u5256\u4fdd\u771f\u5ea6\u3002", "result": "\u5728HCP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cQ-CATN\u5728\u53c2\u6570\u56fe\u548c\u7ea4\u7ef4\u675f\u4f30\u8ba1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u7ec6\u8282\u3002", "conclusion": "Q-CATN\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u548c\u7814\u7a76\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u7075\u6d3bq\u7a7a\u95f4\u91c7\u6837\u573a\u666f\u4e0b\u3002"}}
{"id": "2505.09334", "pdf": "https://arxiv.org/pdf/2505.09334", "abs": "https://arxiv.org/abs/2505.09334", "authors": ["Sadman Sakib Alif", "Nasim Anzum Promise", "Fiaz Al Abid", "Aniqua Nusrat Zereen"], "title": "DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Lung cancer is a leading cause of cancer-related deaths globally, where early\ndetection and accurate diagnosis are critical for improving survival rates.\nWhile deep learning, particularly convolutional neural networks (CNNs), has\nrevolutionized medical image analysis by detecting subtle patterns indicative\nof early-stage lung cancer, its adoption faces challenges. These models are\noften computationally expensive and require significant resources, making them\nunsuitable for resource constrained environments. Additionally, their lack of\ntransparency hinders trust and broader adoption in sensitive fields like\nhealthcare. Knowledge distillation addresses these challenges by transferring\nknowledge from large, complex models (teachers) to smaller, lightweight models\n(students). We propose a knowledge distillation-based approach for lung cancer\ndetection, incorporating explainable AI (XAI) techniques to enhance model\ntransparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,\nand VGG16, are evaluated as teacher models. We developed and trained a\nlightweight student model, Distilled Custom Student Network (DCSNet) using\nResNet50 as the teacher. This approach not only ensures high diagnostic\nperformance in resource-constrained settings but also addresses transparency\nconcerns, facilitating the adoption of AI-driven diagnostic tools in\nhealthcare.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u8f7b\u91cf\u7ea7\u6a21\u578bDCSNet\uff0c\u7528\u4e8e\u80ba\u764c\u68c0\u6d4b\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\u63d0\u9ad8\u900f\u660e\u5ea6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "motivation": "\u80ba\u764c\u662f\u5168\u7403\u764c\u75c7\u76f8\u5173\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u548c\u51c6\u786e\u8bca\u65ad\u5bf9\u63d0\u9ad8\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u867d\u6709\u6548\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5c06\u590d\u6742\u6559\u5e08\u6a21\u578b\uff08\u5982ResNet50\uff09\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578bDCSNet\u4e2d\uff0c\u5e76\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\u3002", "result": "DCSNet\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u8bca\u65ad\u6027\u80fd\uff0c\u8fd8\u89e3\u51b3\u4e86\u900f\u660e\u5ea6\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u6709\u52a9\u4e8eAI\u9a71\u52a8\u8bca\u65ad\u5de5\u5177\u5728\u533b\u7597\u9886\u57df\u7684\u63a8\u5e7f\u3002"}}
{"id": "2505.09344", "pdf": "https://arxiv.org/pdf/2505.09344", "abs": "https://arxiv.org/abs/2505.09344", "authors": ["Gabriel Cort\u00eas", "Nuno Louren\u00e7o", "Paolo Romano", "Penousal Machado"], "title": "GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Determining the performance of a Deep Neural Network during Neural\nArchitecture Search processes is essential for identifying optimal\narchitectures and hyperparameters. Traditionally, this process requires\ntraining and evaluation of each network, which is time-consuming and\nresource-intensive. Zero-cost proxies estimate performance without training,\nserving as an alternative to traditional training. However, recent proxies\noften lack generalization across diverse scenarios and provide only relative\nrankings rather than predicted accuracies. To address these limitations, we\npropose GreenFactory, an ensemble of zero-cost proxies that leverages a random\nforest regressor to combine multiple predictors' strengths and directly predict\nmodel test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust\nresults across multiple datasets. Specifically, GreenFactory achieves high\nKendall correlations on NATS-Bench-SSS, indicating substantial agreement\nbetween its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945\nfor CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we\nachieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for\nImageNet-16-120, showcasing its reliability in both search spaces.", "AI": {"tldr": "GreenFactory\u662f\u4e00\u79cd\u96c6\u6210\u96f6\u6210\u672c\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u76f4\u63a5\u9884\u6d4b\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4ee3\u7406\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u548c\u4ec5\u63d0\u4f9b\u76f8\u5bf9\u6392\u540d\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u8bc4\u4f30\u6027\u80fd\u9700\u8981\u8bad\u7ec3\u548c\u8bc4\u4f30\u6bcf\u4e2a\u7f51\u7edc\uff0c\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\uff1b\u73b0\u6709\u96f6\u6210\u672c\u4ee3\u7406\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u4e14\u4ec5\u63d0\u4f9b\u76f8\u5bf9\u6392\u540d\u3002", "method": "\u63d0\u51faGreenFactory\uff0c\u96c6\u6210\u591a\u4e2a\u96f6\u6210\u672c\u4ee3\u7406\uff0c\u5229\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u76f4\u63a5\u9884\u6d4b\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "result": "\u5728NATS-Bench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cKendall\u76f8\u5173\u7cfb\u6570\u9ad8\uff08\u5982CIFAR-10\u4e3a0.907\uff09\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GreenFactory\u5728\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u63d0\u4f9b\u9ad8\u6548\u3001\u51c6\u786e\u7684\u6027\u80fd\u9884\u6d4b\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u573a\u666f\u3002"}}
{"id": "2505.09356", "pdf": "https://arxiv.org/pdf/2505.09356", "abs": "https://arxiv.org/abs/2505.09356", "authors": ["Srinivas Ravuri", "Yuan Xu", "Martin Ludwig Zehetner", "Ketan Motlag", "Sahin Albayrak"], "title": "APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages with 6 figures", "summary": "Precise initialization plays a critical role in the performance of\nlocalization algorithms, especially in the context of robotics, autonomous\ndriving, and computer vision. Poor localization accuracy is often a consequence\nof inaccurate initial poses, particularly noticeable in GNSS-denied\nenvironments where GPS signals are primarily relied upon for initialization.\nRecent advances in leveraging deep neural networks for pose regression have led\nto significant improvements in both accuracy and robustness, especially in\nestimating complex spatial relationships and orientations. In this paper, we\nintroduce APR-Transformer, a model architecture inspired by state-of-the-art\nmethods, which predicts absolute pose (3D position and 3D orientation) using\neither image or LiDAR data. We demonstrate that our proposed method achieves\nstate-of-the-art performance on established benchmark datasets such as the\nRadar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our\nexperiments to include our custom complex APR-BeIntelli dataset. Additionally,\nwe validate the reliability of our approach in GNSS-denied environments by\ndeploying the model in real-time on an autonomous test vehicle. This showcases\nthe practical feasibility and effectiveness of our approach. The source code is\navailable at:https://github.com/GT-ARC/APR-Transformer.", "AI": {"tldr": "APR-Transformer\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7edd\u5bf9\u59ff\u6001\uff083D\u4f4d\u7f6e\u548c\u65b9\u5411\uff09\uff0c\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u7684\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7cbe\u786e\u7684\u521d\u59cb\u5b9a\u4f4d\u5bf9\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faAPR-Transformer\u6a21\u578b\uff0c\u5229\u7528\u56fe\u50cf\u6216LiDAR\u6570\u636e\u9884\u6d4b\u7edd\u5bf9\u59ff\u6001\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u3002", "result": "\u5728Radar Oxford Robot-Car\u548cDeepLoc\u7b49\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "APR-Transformer\u5728\u590d\u6742\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u7528\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.09393", "pdf": "https://arxiv.org/pdf/2505.09393", "abs": "https://arxiv.org/abs/2505.09393", "authors": ["Huakun Liu", "Hiroki Ota", "Xin Wei", "Yutaro Hirao", "Monica Perusquia-Hernandez", "Hideaki Uchiyama", "Kiyoshi Kiyokawa"], "title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Sparse wearable inertial measurement units (IMUs) have gained popularity for\nestimating 3D human motion. However, challenges such as pose ambiguity, data\ndrift, and limited adaptability to diverse bodies persist. To address these\nissues, we propose UMotion, an uncertainty-driven, online fusing-all state\nestimation framework for 3D human shape and pose estimation, supported by six\nintegrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB\nsensors measure inter-node distances to infer spatial relationships, aiding in\nresolving pose ambiguities and body shape variations when combined with\nanthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors\nare affected by body occlusions. Consequently, we develop a tightly coupled\nUnscented Kalman Filter (UKF) framework that fuses uncertainties from sensor\ndata and estimated human motion based on individual body shape. The UKF\niteratively refines IMU and UWB measurements by aligning them with uncertain\nhuman motion constraints in real-time, producing optimal estimates for each.\nExperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of UMotion in stabilizing sensor data and the improvement over\nstate of the art in pose accuracy.", "AI": {"tldr": "UMotion\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u5728\u7ebf\u878d\u5408\u72b6\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408IMU\u548cUWB\u4f20\u611f\u5668\uff0c\u7528\u4e8e3D\u4eba\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u59ff\u6001\u6a21\u7cca\u3001\u6570\u636e\u6f02\u79fb\u548c\u8eab\u4f53\u591a\u6837\u6027\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u7a00\u758f\u53ef\u7a7f\u6234IMU\u57283D\u4eba\u4f53\u8fd0\u52a8\u4f30\u8ba1\u4e2d\u5b58\u5728\u59ff\u6001\u6a21\u7cca\u3001\u6570\u636e\u6f02\u79fb\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faUMotion\u6846\u67b6\uff0c\u7ed3\u5408IMU\u548cUWB\u4f20\u611f\u5668\uff0c\u901a\u8fc7UKF\u5b9e\u65f6\u878d\u5408\u4f20\u611f\u5668\u6570\u636e\u548c\u4eba\u4f53\u8fd0\u52a8\u7ea6\u675f\uff0c\u4f18\u5316\u4f30\u8ba1\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUMotion\u80fd\u7a33\u5b9a\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u5728\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "UMotion\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u548c\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u4eba\u4f53\u8fd0\u52a8\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.09521", "pdf": "https://arxiv.org/pdf/2505.09521", "abs": "https://arxiv.org/abs/2505.09521", "authors": ["Dongyi He", "Shiyang Li", "Bin Jiang", "He Yan"], "title": "Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High-resolution functional magnetic resonance imaging (fMRI) is essential for\nmapping human brain activity; however, it remains costly and logistically\nchallenging. If comparable volumes could be generated directly from widely\navailable scalp electroencephalography (EEG), advanced neuroimaging would\nbecome significantly more accessible. Existing EEG-to-fMRI generators rely on\nplain CNNs that fail to capture cross-channel time-frequency cues or on heavy\ntransformer/GAN decoders that strain memory and stability. We propose\nSpec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts\nthese issues via a Multi-directional Time-Frequency Convolutional Attention\nEncoder, stacking temporal, spectral and joint convolutions with\nself-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space\nblocks enable efficient long-range spatial modelling. Trained end-to-end with a\nhybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on\nthree public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball\nand 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%\nrespectively over previous best SSIM scores. Furthermore, it achieves\ncompetitive PSNR scores, particularly excelling on the CN-EPFL dataset with a\n4.6% improvement over the previous best PSNR, thus striking a better balance in\nreconstruction quality. The proposed model is lightweight and efficient, making\nit suitable for real-time applications in clinical and research settings. The\ncode is available at https://github.com/hdy6438/Spec2VolCAMU-Net.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Spec2VolCAMU-Net\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u65b9\u5411\u65f6\u95f4-\u9891\u7387\u5377\u79ef\u6ce8\u610f\u529b\u7f16\u7801\u5668\u548cVision-Mamba U-Net\u89e3\u7801\u5668\uff0c\u4eceEEG\u751f\u6210\u9ad8\u5206\u8fa8\u7387fMRI\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387fMRI\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u83b7\u53d6\uff0c\u800cEEG\u5e7f\u6cdb\u53ef\u7528\u3002\u73b0\u6709EEG-to-fMRI\u751f\u6210\u5668\u5b58\u5728\u6027\u80fd\u6216\u6548\u7387\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u591a\u65b9\u5411\u65f6\u95f4-\u9891\u7387\u5377\u79ef\u6ce8\u610f\u529b\u7f16\u7801\u5668\u548cVision-Mamba U-Net\u89e3\u7801\u5668\uff0c\u7ed3\u5408SSI-MSE\u635f\u5931\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73SSIM\u548cPSNR\u5206\u6570\uff0c\u5206\u522b\u63d0\u534714.5%\u300114.9%\u300116.9%\u548c4.6%\u3002", "conclusion": "Spec2VolCAMU-Net\u8f7b\u91cf\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u548c\u7814\u7a76\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2505.09565", "pdf": "https://arxiv.org/pdf/2505.09565", "abs": "https://arxiv.org/abs/2505.09565", "authors": ["Maik Dannecker", "Thomas Sanchez", "Meritxell Bach Cuadra", "\u00d6zg\u00fcn Turgut", "Anthony N. Price", "Lucilio Cordero-Grande", "Vanessa Kyriakopoulou", "Joseph V. Hajnal", "Daniel Rueckert"], "title": "Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 6 figures", "summary": "High-resolution slice-to-volume reconstruction (SVR) from multiple\nmotion-corrupted low-resolution 2D slices constitutes a critical step in\nimage-based diagnostics of moving subjects, such as fetal brain Magnetic\nResonance Imaging (MRI). Existing solutions struggle with image artifacts and\nsevere subject motion or require slice pre-alignment to achieve satisfying\nreconstruction performance. We propose a novel SVR method to enable fast and\naccurate MRI reconstruction even in cases of severe image and motion\ncorruption. Our approach performs motion correction, outlier handling, and\nsuper-resolution reconstruction with all operations being entirely based on\nimplicit neural representations. The model can be initialized with\ntask-specific priors through fully self-supervised meta-learning on either\nsimulated or real-world data. In extensive experiments including over 480\nreconstructions of simulated and clinical MRI brain data from different\ncenters, we prove the utility of our method in cases of severe subject motion\nand image artifacts. Our results demonstrate improvements in reconstruction\nquality, especially in the presence of severe motion, compared to\nstate-of-the-art methods, and up to 50% reduction in reconstruction time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u9ad8\u5206\u8fa8\u7387\u5207\u7247\u5230\u4f53\u79ef\u91cd\u5efa\uff08SVR\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u8fd0\u52a8\u4f2a\u5f71\u4e25\u91cd\u7684MRI\u56fe\u50cf\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8fd0\u52a8\u4f2a\u5f71\u4e25\u91cd\u6216\u9700\u8981\u5207\u7247\u9884\u5bf9\u9f50\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u5feb\u901f\u4e14\u51c6\u786e\u7684MRI\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8fdb\u884c\u8fd0\u52a8\u6821\u6b63\u3001\u5f02\u5e38\u503c\u5904\u7406\u53ca\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5143\u5b66\u4e60\u521d\u59cb\u5316\u4efb\u52a1\u7279\u5b9a\u5148\u9a8c\u3002", "result": "\u5728\u6a21\u62df\u548c\u4e34\u5e8aMRI\u8111\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u91cd\u5efa\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u91cd\u5efa\u65f6\u95f4\u51cf\u5c1150%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u4f2a\u5f71\u4e25\u91cd\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u5feb\u901f\u7684MRI\u91cd\u5efa\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
