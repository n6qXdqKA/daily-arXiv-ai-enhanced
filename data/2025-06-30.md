[[toc]]

## cs.CV

### [1] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Main category: cs.CV

TL;DR: SpatialReasoner-R1模型通过M3CTS生成高质量空间推理监督数据，结合fDPO优化，显著提升空间推理任务性能。

- Motivation: 当前视觉语言模型在细粒度空间推理和多步逻辑对齐方面表现不足，需改进。
- Method: 采用M3CTS生成多样化、逻辑一致的LongCoT推理轨迹，并提出fDPO优化方法，结合空间奖励机制。
- Result: fDPO在空间质量和数量任务上分别提升4.1%和9.0%，SpatialReasoner-R1在SPATIALRGPT-Bench上创下新SoTA。
- Conclusion: SpatialReasoner-R1通过fDPO和M3CTS显著提升空间推理能力，同时保持通用视觉语言任务竞争力。


### [2] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: TanDiT是一种生成全景图像的新方法，通过统一扩散模型生成切线平面图像，解决了现有模型在全景图像生成中的几何失真和一致性挑战。

- Motivation: 现有图像生成模型在全景图像生成中面临几何失真和一致性不足的问题，需要一种更高效的方法。
- Method: TanDiT利用统一扩散模型生成切线平面图像，并通过后处理步骤增强全局一致性。
- Result: 实验表明，TanDiT能有效泛化，处理复杂文本提示，并生成高质量全景图像。
- Conclusion: TanDiT为全景图像生成提供了一种高效、高质量的解决方案。


### [3] [FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering](https://arxiv.org/abs/2506.21710)
*Liangyu Zhong,Fabio Rosenthal,Joachim Sicking,Fabian Hüger,Thorsten Bagdonat,Hanno Gottschalk,Leo Schwinn*

Main category: cs.CV

TL;DR: FOCUS是一种无需训练的视觉裁剪方法，利用MLLM内部表示指导搜索最相关图像区域，显著提升细粒度VQA任务的性能和效率。

- Motivation: 尽管MLLM在图像-文本输入上表现出强大的感知和推理能力，但针对小图像细节的VQA任务仍具挑战性。现有视觉裁剪方法存在任务特定微调需求、低效搜索或与高效注意力实现不兼容等问题。
- Method: FOCUS通过四个步骤实现：1) 识别VQA提示中的目标对象；2) 使用KV缓存计算对象相关性图；3) 基于相关性图提出并排序相关图像区域；4) 使用排名最高的区域执行细粒度VQA任务。
- Result: FOCUS在四个细粒度VQA数据集和两种MLLM上表现优异，优于三种流行视觉裁剪方法，并在计算效率上显著提升（3-6.5倍）。
- Conclusion: FOCUS通过智能搜索策略解决了现有视觉裁剪方法的局限性，为细粒度VQA任务提供了高效且高性能的解决方案。


### [4] [CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection](https://arxiv.org/abs/2506.21711)
*Aryan Thakre,Omkar Nagwekar,Vedang Talekar,Aparna Santra Biswas*

Main category: cs.CV

TL;DR: 提出了一种基于交叉注意力的CAST模型，用于更有效地融合时空特征，提升深度伪造视频检测的性能。

- Motivation: 深度伪造技术对数字媒体真实性构成威胁，现有CNN-Transformer模型在时空特征融合上存在局限性，需要更高效的检测方法。
- Method: 提出CAST模型，利用交叉注意力动态融合时空特征，增强对时间演化伪造痕迹的检测能力。
- Result: 在FaceForensics++、Celeb-DF和DeepfakeDetection数据集上表现出色，AUC达99.49%，准确率97.57%。
- Conclusion: 交叉注意力特征融合显著提升了深度伪造检测的鲁棒性和泛化能力。


### [5] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种将扩散训练范式融入通用图像修复（IR）框架的新方法，通过系统分析时间步依赖、网络层次、噪声级关系和多任务相关性，提升了单任务和多任务IR的性能。

- Motivation: 扩散模型在图像修复任务中表现出强大的生成能力，但其复杂架构和迭代过程限制了实际应用。现有方法主要关注网络架构和扩散路径优化，忽视了扩散训练范式与通用IR框架的整合。
- Method: 通过分析时间步依赖、网络层次、噪声级关系和多任务相关性，提出新的IR框架，并引入正则化策略和对齐扩散目标与IR任务的方法。此外，开发增量训练范式和任务特定适配器以优化多任务IR。
- Result: 实验表明，该方法显著提升了单任务IR的泛化能力，并在多任务统一IR中表现出优越性能。
- Conclusion: 提出的框架可无缝集成到现有通用IR架构中，为扩散模型在IR任务中的实际应用提供了有效解决方案。


### [6] [Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning](https://arxiv.org/abs/2506.21724)
*Remco F. Leijenaar,Hamidreza Kasaei*

Main category: cs.CV

TL;DR: AsymDSD是一种自监督学习框架，通过潜在空间预测结合掩码建模和不变性学习，提升了3D点云的语义表示能力。

- Motivation: 解决无标签大规模3D点云数据中语义表示学习的挑战，避免传统掩码点建模（MPM）在高层语义捕获上的局限性。
- Method: 提出AsymDSD框架，采用非对称双自蒸馏设计，结合潜在空间预测、多掩码采样和多裁剪点云适应等技术。
- Result: 在ScanObjectNN上达到90.53%的准确率，预训练后提升至93.72%，优于现有方法。
- Conclusion: AsymDSD通过创新设计显著提升了3D点云语义表示的性能，为自监督学习提供了新思路。


### [7] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Main category: cs.CV

TL;DR: 论文提出MESP和LCH两个理论框架，探讨概率生成模型的局限性，即学习全局分布导致记忆而非生成行为。基于MESP提出BL-AE和ARVM模型，实验表现优异但反映记忆问题。LCH假设局部相关性提升生成能力。

- Motivation: 研究概率生成模型中全局分布学习导致记忆而非生成行为的潜在局限性。
- Method: 提出MESP框架，基于VAE重新思考，提出BL-AE和ARVM模型；进一步提出LCH假设，强调局部相关性。
- Result: ARVM在标准数据集上取得竞争性FID分数，但反映记忆问题；LCH假设通过实验验证。
- Conclusion: MESP和LCH框架揭示了生成模型的局限性，并提出局部相关性是提升生成能力的关键。


### [8] [Equitable Federated Learning with NCA](https://arxiv.org/abs/2506.21735)
*Nick Lemke,Mirko Konstantin,Henry John Krumb,John Kalkhof,Jonathan Stieber,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: FedNCA是一种专为医疗图像分割设计的联邦学习系统，适用于资源受限的低中收入国家，通过轻量级架构和低通信成本实现高效训练。

- Motivation: 在低中收入国家，医疗资源匮乏且基础设施薄弱，传统联邦学习难以应用。FedNCA旨在解决这些问题。
- Method: 采用轻量级Med-NCA架构，支持低成本边缘设备（如智能手机）训练，并减少通信开销，同时具备加密功能。
- Result: FedNCA在资源受限环境下高效运行，支持不稳定的网络通信，为医疗影像分析提供可行解决方案。
- Conclusion: FedNCA为资源匮乏地区提供了高效、轻量且安全的医疗影像分析工具，推动医疗公平。


### [9] [ImplicitQA: Going beyond frames towards Implicit Video Reasoning](https://arxiv.org/abs/2506.21742)
*Sirnam Swetha,Rohit Gupta,Parth Parag Kulkarni,David G Shatwell,Jeffrey A Chan Santiago,Nyle Siddiqui,Joseph Fioresi,Mubarak Shah*

Main category: cs.CV

TL;DR: 论文提出了ImplicitQA基准，用于测试视频问答模型在隐含推理能力上的表现，填补了现有基准的不足。

- Motivation: 现有视频问答基准主要关注显性视觉内容，而忽略了隐含推理（如动机、因果关系等），无法反映人类的理解能力。
- Method: 构建了包含1K QA对的ImplicitQA基准，涵盖多种推理维度（如空间、因果、社交等），并评估了主流模型的性能。
- Result: 主流模型在隐含推理任务上表现不佳，凸显其对表层视觉线索的依赖。
- Conclusion: ImplicitQA为视频问答研究提供了新方向，鼓励社区进一步探索隐含推理能力。


### [10] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Main category: cs.CV

TL;DR: 提出了一种基于EfficientNet-B0的深度学习流程，用于从视网膜眼底图像中检测青光眼，通过多数据集训练提升泛化能力。

- Motivation: 青光眼是导致不可逆失明的主要原因，早期检测可显著改善治疗效果，但传统方法通常具有侵入性且需要专业设备。
- Method: 使用EfficientNet-B0架构，通过ACRIMA、ORIGA和RIM-ONE数据集进行顺序训练和微调，最小化预处理。
- Result: 实验表明，简单预处理比复杂增强方法具有更高的AUC-ROC，模型在未见数据集上表现出强判别性能。
- Conclusion: 该流程为青光眼早期检测提供了可重复且可扩展的方法，具有潜在临床价值。


### [11] [Comparing Learning Paradigms for Egocentric Video Summarization](https://arxiv.org/abs/2506.21785)
*Daniel Wen*

Main category: cs.CV

TL;DR: 研究比较了监督学习、无监督学习和提示微调在自我中心视频理解中的表现，发现GPT-4o优于专用模型，但现有方法在适应第一人称视角上仍有局限。

- Motivation: 探索计算机视觉技术在自我中心视频中的应用，评估不同方法的有效性，推动该领域的发展。
- Method: 评估了监督学习模型Shotluck Holmes、无监督学习模型TAC-SUM和提示微调模型GPT-4o在视频摘要任务中的表现。
- Result: 现有方法在第一人称视频中表现较差，而GPT-4o优于专用模型，凸显了现有方法的局限性。
- Conclusion: 需进一步改进以适应第一人称视频的独特挑战，研究为未来技术发展提供了概念验证。


### [12] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: 该论文介绍了首个白内障手术场景图数据集（CAT-SG），用于捕捉手术工具与组织间的语义关系，并提出了一种新的场景图生成模型CatSGG，性能优于现有方法。

- Motivation: 现有数据集仅关注手术分析的孤立方面（如工具检测或阶段分割），缺乏对实体间语义关系的全面表示，因此需要更全面的数据集来建模复杂的手术工作流程。
- Method: 论文提出了CAT-SG数据集，包含工具-组织交互、程序变化和时间依赖的结构化标注，并开发了CatSGG模型用于生成结构化手术表示。
- Result: CAT-SG提供了手术工作流程的整体视图，CatSGG模型在生成结构化表示方面优于现有方法。
- Conclusion: CAT-SG数据集和CatSGG模型为AI驱动的临床实践（如手术培训和实时决策支持）提供了更智能、上下文感知的基础。


### [13] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Main category: cs.CV

TL;DR: 提出了一种基于大型视觉基础模型和参数高效微调的少样本历史地图分割方法，性能优于现有技术，并显著减少标注需求。

- Motivation: 历史地图是重要的历史资源，但其视觉多样性及标注数据有限，自动化处理面临挑战。
- Method: 结合大型视觉基础模型的语义嵌入和参数高效微调，实现少样本分割。
- Result: 在Siegfried数据集上，10-shot和5-shot场景下分别提升5%和13%的mIoU；在ICDAR 2021数据集上达到67.3%的PQ。
- Conclusion: 该方法在极低数据量下仍保持高性能，大幅减少人工标注需求，推动了历史地图的自动化处理。


### [14] [TaleForge: Interactive Multimodal System for Personalized Story Creation](https://arxiv.org/abs/2506.21832)
*Minh-Loi Nguyen,Quang-Khai Le,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: TaleForge是一个结合LLMs和文本到图像扩散技术的个性化故事生成系统，通过将用户的面部图像嵌入叙事和插图中，提升沉浸感和参与度。

- Motivation: 现有方法将用户视为被动消费者，提供有限个性化的通用情节，降低了参与度和沉浸感。
- Method: TaleForge包含三个模块：故事生成（LLMs根据用户提示创建叙事和角色描述）、个性化图像生成（将用户面部和服装融入角色插图）、背景生成（创建包含个性化角色的场景）。
- Result: 用户研究表明，当用户成为主角时，参与感和归属感显著提升。系统实时预览和直观控制受到好评，但用户希望有更精细的叙事编辑工具。
- Conclusion: TaleForge通过个性化文本和图像的结合，推动了多模态故事叙述的发展，创造了沉浸式、以用户为中心的体验。


### [15] [PrefPaint: Enhancing Image Inpainting through Expert Human Feedback](https://arxiv.org/abs/2506.21834)
*Duy-Bao Bui,Hoang-Khang Nguyen,Trung-Nghia Le*

Main category: cs.CV

TL;DR: PrefPaint通过结合人类反馈改进Stable Diffusion Inpainting，提升医学图像修复的准确性和可靠性。

- Motivation: 医学图像修复（如息肉图像）的准确性对诊断至关重要，现有模型可能生成不准确图像，需专家标注以确保可靠性。
- Method: 提出PrefPaint，将人类反馈融入训练过程，避免计算昂贵的奖励模型，并开发基于Web的交互界面简化训练和微调。
- Result: PrefPaint在多个领域优于现有方法，减少视觉不一致性，尤其在医学图像中生成更真实的息肉图像。
- Conclusion: PrefPaint通过人类反馈和交互界面，显著提升了医学图像修复的准确性和用户体验。


### [16] [ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts](https://arxiv.org/abs/2506.21835)
*Xiaoqi Wang,Clint Sebastian,Wenbin He,Liu Ren*

Main category: cs.CV

TL;DR: ProSAM提出了一种改进的视觉参考分割方法，通过变分提示编码器预测多变量提示分布，解决了现有SAM方法在边界生成提示导致的不稳定问题。

- Motivation: 现有SAM方法在视觉参考分割中因提示编码器不理想，常在物体边界生成提示，导致不稳定性和鲁棒性下降。
- Method: ProSAM通过学习变分提示编码器预测多变量提示分布，避免在不稳定区域生成提示。
- Result: ProSAM在Pascal-5^i和COCO-20^i数据集上表现优于现有方法。
- Conclusion: ProSAM为视觉参考分割提供了更稳定和鲁棒的解决方案。


### [17] [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
*Mengyi Shan,Brian Curless,Ira Kemelmacher-Shlizerman,Steve Seitz*

Main category: cs.CV

TL;DR: 提出了一种分层多智能体框架，用于生成视觉吸引人、逻辑严密且具有智力挑战性的逃脱室谜题图像。

- Motivation: 挑战文本到图像模型在生成逃脱室谜题图像时的能力，解决基础模型在空间关系和功能推理上的不足。
- Method: 采用分层多智能体框架，将任务分解为功能设计、符号场景图推理、布局合成和局部图像编辑四个阶段，通过智能体协作和迭代反馈确保场景的视觉一致性和功能性可解性。
- Result: 实验表明，智能体协作显著提高了输出质量，包括可解性、避免捷径和功能清晰度，同时保持了视觉质量。
- Conclusion: 该框架有效提升了文本到图像模型在复杂场景生成中的表现，为逃脱室谜题设计提供了新思路。


### [18] [3D-Telepathy: Reconstructing 3D Objects from EEG Signals](https://arxiv.org/abs/2506.21843)
*Yuxiang Ge,Jionghao Cheng,Ruiquan Ge,Zhaojie Fang,Gangyong Jia,Xiang Wan,Nannan Li,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: 提出了一种创新的EEG编码器架构，结合双重自注意力机制，从EEG数据中成功重建3D对象。

- Motivation: 传统方法仅将脑电活动转化为2D图像，忽略了3D空间信息的重建，限制了BCI的实际应用。
- Method: 采用混合训练策略，包括交叉注意力、对比学习和自监督学习，并结合稳定扩散和变分评分蒸馏技术。
- Result: 成功从EEG数据中生成了内容和结构相似的3D对象。
- Conclusion: 该方法为从EEG数据重建3D视觉刺激提供了新思路，具有BCI应用的潜力。


### [19] [End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model](https://arxiv.org/abs/2506.21851)
*Haofeng Wang,Fangtao Zhou,Qi Zhang,Zeyuan Chen,Enci Zhang,Zhao Wang,Xiaofeng Huang,Siwei Ma*

Main category: cs.CV

TL;DR: 提出了一种用于RGB-IR图像对的联合压缩框架，通过跨模态熵模型（CCEM）和低频上下文提取与融合模块（LCEB和LCFB）优化压缩性能。

- Motivation: 随着RGB-IR图像对在智能监控等应用中的广泛使用，数据存储和传输成本增加，因此需要高效的压缩方法。
- Method: 设计了CCEM模型，包含LCEB和LCFB模块，用于提取和融合跨模态的低频上下文信息，以优化熵参数预测。
- Result: 在LLVIP和KAIST数据集上优于现有RGB-IR和单模态压缩方法，例如在LLVIP上比特率节省23.1%。
- Conclusion: 该框架通过跨模态信息利用显著提升了RGB-IR图像对的压缩效率。


### [20] [Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation](https://arxiv.org/abs/2506.21855)
*Jiho Choi,Sang Jun Lee*

Main category: cs.CV

TL;DR: 提出了一种通过自监督学习从无标签面部视频中学习周期性信号的通用表示方法，用于远程光电容积描记术（rPPG）估计。

- Motivation: 捕捉面部视频中皮肤色调的细微变化，以提取生理信号，解决rPPG任务中的挑战。
- Method: 使用视频掩码自编码器学习时空表示，结合帧掩码和生理带宽限制约束。
- Result: 在多个数据集上表现优异，尤其在跨数据集评估中显著提升性能。
- Conclusion: 该方法为rPPG任务提供了一种有效的自监督学习框架。


### [21] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Main category: cs.CV

TL;DR: SPADE是一种基础模型，整合了组织病理学和空间转录组学数据，通过对比学习在多模态框架中学习图像表示，显著优于基线模型。

- Motivation: 数字病理学和自监督深度学习的快速发展为多模态方法提供了可能，但全切片图像与空间转录组学的全面整合仍存在空白，这对捕捉分子异质性至关重要。
- Method: SPADE采用混合数据专家技术，通过两阶段特征空间聚类和对比学习，整合共定位的WSI块和基因表达谱。
- Result: 在HEST-1k数据集上预训练后，SPADE在14个下游任务中表现出显著优于基线模型的少样本性能。
- Conclusion: SPADE展示了将形态学和分子信息整合到统一潜在空间的优势，为病理学任务提供了新方法。


### [22] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Main category: cs.CV

TL;DR: LLaVA-Scissor是一种无需训练的令牌压缩策略，用于视频多模态大语言模型，通过语义连通组件（SCC）实现全面的语义覆盖，优于现有方法。

- Motivation: 现有基于注意力分数的令牌压缩方法无法有效捕捉所有语义区域且导致冗余，需要更高效的压缩策略。
- Method: 提出基于语义连通组件（SCC）的两步时空令牌压缩策略，确保非重叠语义令牌覆盖整个视频。
- Result: 在多种视频理解基准测试中表现优异，尤其在低令牌保留率下优于其他方法。
- Conclusion: LLaVA-Scissor通过SCC实现了高效的令牌压缩，显著提升了视频多模态模型的性能。


### [23] [Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling](https://arxiv.org/abs/2506.21863)
*Sungjune Park,Yeongyun Kim,Se Yeon Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: 提出了一种针对遥感（RS）图像理解的新型大视觉与语言模型（LVLM）框架，通过语义增强的多级对齐和语义感知专家建模，解决了现有LVLM在RS领域的局限性。

- Motivation: 现有的大视觉与语言模型（LVLMs）在自然图像领域表现优异，但在遥感（RS）领域由于视觉外观、对象尺度和语义的显著差异，其应用受限。
- Method: 提出两个核心组件：1）语义增强的多级对齐模块，通过检索相关语义信息并聚合到多级视觉特征中；2）语义感知专家建模，设计不同级别的语义专家进行分层语义理解。
- Result: 在多个RS任务（如场景分类和视觉问答）中，该框架实现了跨多语义级别的一致性能提升。
- Conclusion: 该框架有效填补了通用LVLMs与RS特定需求之间的差距，展示了其在RS视觉语言理解中的能力和有效性。


### [24] [Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images](https://arxiv.org/abs/2506.21866)
*Yanguang Sun,Jiexi Yan,Jianjun Qian,Chunyan Xu,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: 提出了一种新型的双视角统一Transformer（DPU-Former），用于光学遥感图像（ORSIs）的分割任务，结合了卷积和Transformer的优势，解决了特征异质性、高复杂性和大参数问题。

- Motivation: 现有方法通常忽视卷积和Transformer特征的异质性及模型复杂性，导致分割效果不佳。
- Method: 设计了全局-局部混合注意力机制和傅里叶空间合并策略，并引入门控线性前馈网络增强表达能力，构建了DPU-Former解码器。
- Result: DPU-Former在多个数据集上优于现有最优方法。
- Conclusion: DPU-Former通过双视角融合和高效特征聚合，显著提升了ORSIs的分割性能。


### [25] [Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning](https://arxiv.org/abs/2506.21873)
*Tzu-Chun Chien,Chieh-Kai Lin,Shiang-Feng Tsai,Ruei-Chi Lai,Hung-Jen Chen,Min Sun*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在视觉定位中表现优异，但视觉标记剪枝会显著降低其定位能力。研究发现位置ID错位是主要原因，提出GAP方法无需额外训练即可恢复90%性能。

- Motivation: 视觉标记剪枝虽降低计算成本，但严重损害模型定位能力，需解决此问题以保持性能。
- Method: 提出GAP方法，通过调整位置ID恢复剪枝后的定位性能，适用于多种模型和剪枝策略。
- Result: GAP将LLaVA在RefCOCO验证集上的准确率从15.34%恢复至51.42%，接近原始性能的90%。
- Conclusion: GAP是一种简单有效的方法，显著提升剪枝后模型的定位能力，适用于多种MLLMs。


### [26] [GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification](https://arxiv.org/abs/2506.21883)
*Basudha Pal,Sharif Amit Kamran,Brendon Lutnick,Molly Lucas,Chaitanya Parmar,Asha Patel Shah,David Apfel,Steven Fakharzadeh,Lloyd Miller,Gabriela Cula,Kristopher Standish*

Main category: cs.CV

TL;DR: 提出一种基于梯度解释性的框架，自动标记训练图像中的问题样本，提升银屑病严重程度评分的模型泛化能力。

- Motivation: 银屑病严重程度评分的远程评估受限于图像质量不一致和医生标注差异，影响模型可靠性。
- Method: 使用梯度解释性方法追踪误分类验证图像的梯度，检测标注不一致或受非临床因素影响的训练样本。
- Result: 移除8.2%的问题图像后，模型AUC-ROC提升5%（85%至90%），并能高效识别标注不一致的样本。
- Conclusion: 该方法显著提升远程评估的自动化评分鲁棒性，减少对人工标注的依赖。


### [27] [Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles](https://arxiv.org/abs/2506.21885)
*Chuheng Wei,Ziye Qin,Ziyan Zhang,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: 本文系统综述了多传感器融合在自动驾驶中的重要性，分类了融合策略，并探讨了深度学习方法、数据集及未来趋势。

- Motivation: 多传感器融合能克服单一传感器限制，提升自动驾驶的环境感知能力，尤其在恶劣天气和复杂环境中。
- Method: 将多传感器融合策略分为数据级、特征级和决策级，并综述了基于深度学习的对应方法。
- Result: 提供了关键多模态数据集及其适用性分析，探讨了新兴趋势如视觉语言模型和大语言模型的融合。
- Conclusion: 多传感器融合在提升自动驾驶系统适应性和鲁棒性方面潜力巨大，本文为未来研究提供了方向。


### [28] [DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025](https://arxiv.org/abs/2506.21891)
*Umihiro Kamoto,Tatsuya Ishibashi,Noriyuki Kugo*

Main category: cs.CV

TL;DR: DIVE方法在2025年复杂视频推理与鲁棒性评估挑战赛中夺冠，通过迭代推理框架实现高精度视频问答。

- Motivation: 解决复杂视频问答问题，提升自然语言答案的准确性和鲁棒性。
- Method: 采用DIVE（深度搜索迭代视频探索）方法，通过语义分解和逐步推理解决复杂查询。
- Result: 在CVRR-ES基准测试中达到81.44%的准确率，排名第一。
- Conclusion: DIVE的迭代推理框架在视频问答中表现出高效性和鲁棒性。


### [29] [SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation](https://arxiv.org/abs/2506.21892)
*Adam Goodge,Xun Xu,Bryan Hooi,Wee Siong Ng,Jingyi Liao,Yongyi Su,Xulei Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SODA的新方法，用于检测点云数据中的分布外（OOD）对象，通过邻域评分传播方案提升性能，无需额外训练。

- Motivation: 随着点云数据应用的普及，检测OOD点云对象对模型安全性和可靠性至关重要，但现有研究对此问题探索不足。
- Method: 利用3D视觉语言模型（3D VLMs）进行OOD检测，并提出SODA方法，通过邻域评分传播解决合成到真实域偏移问题。
- Result: SODA在无需额外训练的情况下，在多个数据集和问题设置中实现了最先进的性能。
- Conclusion: SODA方法有效提升了OOD点云检测的性能，解决了合成到真实域偏移的挑战。


### [30] [Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.21895)
*Fangling Jiang,Qi Li,Weining Wang,Gang Wang,Bing Liu,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出了一种基于强化微调的人脸反欺骗方法，通过多模态大语言模型学习推理策略，而非依赖记忆模式，提升了跨域泛化能力和可解释性。

- Motivation: 现有方法容易记忆训练集模式，泛化能力差且缺乏可解释性，需解决跨域人脸反欺骗任务。
- Method: 设计了可验证的类别一致奖励和推理一致奖励，采用GRPO优化策略，引导模型从多角度探索推理策略。
- Result: 实验表明，该方法在跨域泛化性能上达到最优，能有效应对未知攻击类型并提供可解释性。
- Conclusion: 该方法通过强化学习提炼高度泛化的决策规则，显著提升了跨域人脸反欺骗任务的性能。


### [31] [Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment](https://arxiv.org/abs/2506.21903)
*Dipayan Biswas,Shishir Shah,Jaspal Subhlok*

Main category: cs.CV

TL;DR: 论文提出了一种基于迁移学习的方法，用于检测讲座视频中的视觉元素，优化了YOLO模型，并公开了标注数据集和源代码。

- Motivation: 讲座视频中的视觉元素（如图表、表格）对理解和检索至关重要，但自动检测这些元素存在挑战，缺乏标准结构和标注数据。
- Method: 采用迁移学习方法，评估了多种目标检测模型，优化YOLO并采用半监督自动标注策略。
- Result: YOLO表现最佳，优化后效果显著，并开发了通用解决方案。
- Conclusion: 论文成功解决了讲座视频中视觉元素检测问题，公开了数据集和代码以促进未来研究。


### [32] [RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network](https://arxiv.org/abs/2506.21905)
*Mingquan Liu*

Main category: cs.CV

TL;DR: 提出了一种结合Mamba特征建模、区域注意力和贝叶斯不确定性的半监督方法，用于细粒度视觉分类（FGVC），在标记数据稀缺时表现优异。

- Motivation: 细粒度视觉分类因类间差异细微且特征表示脆弱而具有挑战性，现有方法在标记数据稀缺时表现不佳。
- Method: 结合Mamba特征建模、区域注意力和贝叶斯不确定性，增强局部到全局特征建模，并利用贝叶斯推理选择高质量伪标签。
- Result: 在FGVC基准测试中表现优异，尤其在遮挡情况下，标记数据有限时仍具鲁棒性。
- Conclusion: 该方法在标记数据稀缺的细粒度分类任务中表现出色，代码已开源。


### [33] [CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability](https://arxiv.org/abs/2506.21909)
*Justin Reinman,Sunwoong Choi*

Main category: cs.CV

TL;DR: CERBERUS是一个用于训练和评估基础设施缺陷检测AI模型的合成基准，包含裂缝图像生成器和Unity构建的3D场景。测试表明，合成与真实数据结合能提升模型性能。

- Motivation: 为自动化基础设施检测提供灵活、可重复的测试工具，支持未来研究。
- Method: 使用CERBERUS生成合成数据，结合真实数据训练YOLO模型，测试其在Fly-By和Underpass场景中的表现。
- Result: 合成与真实数据结合显著提升了模型在真实图像上的性能。
- Conclusion: CERBERUS为缺陷检测系统提供了有效的测试方法，并支持进一步研究。


### [34] [Generating Attribute-Aware Human Motions from Textual Prompt](https://arxiv.org/abs/2506.21912)
*Xinghan Wang,Kun Xu,Fei Li,Cao Sheng,Jiazhong Yu,Yadong Mu*

Main category: cs.CV

TL;DR: 该论文提出了一种新框架，用于生成基于文本描述和人类属性（如年龄、性别、体重和身高）的人体动作。通过解耦动作语义和属性信息，模型能够生成更真实的、属性感知的动作。

- Motivation: 当前文本驱动的人体动作生成方法忽视了人类属性对动作模式的影响。本文旨在填补这一空白，探索如何结合文本描述和人类属性生成更真实的动作。
- Method: 提出了一种基于结构因果模型的新框架，将动作语义与人类属性解耦，实现文本到语义的预测和属性控制的生成。
- Result: 模型能够生成与用户文本和属性输入一致的、属性感知的逼真动作。HumanAttr数据集的引入为评估提供了基准。
- Conclusion: 论文验证了模型的有效性，为属性感知的文本到动作生成领域设定了首个基准。


### [35] [SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition](https://arxiv.org/abs/2506.21920)
*Nam Quan Nguyen,Xuan Phong Pham,Tuan-Anh Tran*

Main category: cs.CV

TL;DR: SepFormer是一种基于DETR架构的表格结构识别方法，通过单步分割与合并范式提升速度和鲁棒性，在多个基准数据集上表现优异。

- Motivation: 表格结构识别（TSR）是语义数据提取的基础，现有方法需要改进速度和鲁棒性。
- Method: SepFormer采用粗到细的两阶段方法，通过两个Transformer解码器预测表格分隔符，结合角度损失优化单线分割。
- Result: SepFormer在SciTSR等数据集上达到25.6 FPS的速度，性能与最先进方法相当。
- Conclusion: SepFormer通过单步分割与合并范式，显著提升了表格结构识别的效率和效果。


### [36] [ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction](https://arxiv.org/abs/2506.21923)
*Juming Xiong,Ruining Deng,Jialin Yue,Siqi Lu,Junlin Guo,Marilyn Lionts,Tianyuan Yao,Can Cui,Junchao Zhu,Chongyu Qu,Mengmeng Yin,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: ZeroReg3D是一种新型零样本配准方法，用于从连续组织切片构建精确的3D模型，解决了传统方法在组织变形和成像不一致性上的问题。

- Motivation: 传统2D组织学分析方法难以保留3D空间关系，且现有深度学习方法泛化性差或需要大量训练数据。
- Method: 结合零样本深度学习的特征点匹配与基于优化的仿射和非刚性配准技术。
- Result: 有效解决了组织变形、切片伪影、染色变异和光照不一致等问题，无需重新训练或微调。
- Conclusion: ZeroReg3D为临床和研究应用提供了一种高效且通用的3D组织学分析方法。


### [37] [SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding](https://arxiv.org/abs/2506.21924)
*Zhao Jin,Rong-Cheng Tu,Jingyi Liao,Wenhao Sun,Xiao Luo,Shunyu Liu,Dacheng Tao*

Main category: cs.CV

TL;DR: SPAZER结合3D和2D模态进行渐进式推理，显著提升零样本3D视觉定位性能。

- Motivation: 解决现有方法在3D视觉定位中仅侧重空间或语义理解的局限性。
- Method: SPAZER通过3D渲染、锚点引导筛选和3D-2D联合决策实现定位。
- Result: 在ScanRefer和Nr3D基准测试中，准确率分别提升9.0%和10.9%。
- Conclusion: SPAZER无需3D标注数据即可实现鲁棒的零样本定位。


### [38] [Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images](https://arxiv.org/abs/2506.21925)
*Liu Yang,Huiyu Duan,Jiarui Wang,Jing Liu,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Patrick Le Callet*

Main category: cs.CV

TL;DR: 该论文研究了AI生成的全景图像（AIGODIs）的质量评估和优化问题，提出了一个数据库（OHF2024）和两个基于BLIP-2的模型（BLIP2OIQA和BLIP2OISal），用于评估视觉体验和预测失真区域，并展示了优化过程。

- Motivation: 随着AIGC技术的发展，AI生成的全景图像在VR/AR中具有潜力，但其质量评估和优化研究仍不足。
- Method: 建立OHF2024数据库，包含主观评分和失真区域数据；提出BLIP2OIQA和BLIP2OISal模型；设计优化流程。
- Result: BLIP2OIQA和BLIP2OISal在视觉体验评估和失真区域预测任务中达到SOTA效果，优化流程有效。
- Conclusion: 该研究为AIGODIs的质量评估和优化提供了有效工具，数据库和代码将开源。


### [39] [SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images](https://arxiv.org/abs/2506.21945)
*Naftaly Wambugu,Ruisheng Wang,Bo Guo,Tianshu Yu,Sheng Xu,Mohammed Elhassan*

Main category: cs.CV

TL;DR: 提出了一种堆叠深度残差网络（SDRNet）用于高分辨率遥感图像的语义分割，解决了类间差异、遮挡和对象尺寸变化等问题。

- Motivation: 高分辨率遥感图像的语义分割面临类间差异、遮挡和对象尺寸变化的挑战，需要提取鲁棒特征和多上下文特征。
- Method: 采用堆叠的编码器-解码器网络和扩张残差块（DRB）来捕获长距离语义并保留空间信息。
- Result: 在ISPRS Vaihingen和Potsdam数据集上，SDRNet表现优于现有深度卷积神经网络。
- Conclusion: SDRNet能有效解决高分辨率遥感图像语义分割的挑战，性能优于现有方法。


### [40] [Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding](https://arxiv.org/abs/2506.21957)
*Yixin Zha,Chuxin Wang,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于语义掩码自动编码器的方法，通过原型语义建模和增强掩码策略，解决了随机掩码在点云理解中的语义关系捕捉不足问题。

- Motivation: 现有基于随机掩码的点云预训练方法难以捕捉合理的语义关系，影响了模型的性能。
- Method: 提出语义掩码自动编码器，包括原型语义建模模块和增强掩码策略，并结合提示调优策略提升下游任务性能。
- Result: 在ScanObjectNN、ModelNet40和ShapeNetPart等数据集上的实验验证了方法的有效性。
- Conclusion: 该方法显著提升了点云理解的语义表示能力，为下游任务提供了更优的特征。


### [41] [TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models](https://arxiv.org/abs/2506.21975)
*Meng Yu,Te Cui,Qitong Chu,Wenjie Song,Yi Yang,Yufeng Yue*

Main category: cs.CV

TL;DR: TASeg是一个基于文本感知的RGB-T语义分割框架，通过LoRA微调技术和动态特征融合模块（DFFM）解决现有模型在视觉特征相似类别分割中的不足，并结合CLIP文本嵌入提升语义理解准确性。

- Motivation: 现有RGB-T语义分割模型依赖低层次视觉特征，缺乏高层次文本信息，导致在视觉特征相似类别中分割不准确；同时，SAM在多模态融合和计算效率方面存在挑战。
- Method: 提出TASeg框架，采用LoRA微调技术适配视觉基础模型，设计DFFM模块融合多模态视觉特征，并引入CLIP文本嵌入进行语义对齐。
- Result: 在多个数据集上的实验表明，TASeg在复杂场景中表现优异，且训练参数更少。
- Conclusion: TASeg通过结合视觉和文本信息，显著提升了RGB-T语义分割的准确性和效率。


### [42] [R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning](https://arxiv.org/abs/2506.21980)
*Biao Wang,Wenwen Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于多模态大语言模型（MLLMs）的视觉单目标跟踪方法R1-Track，通过GRPO强化学习方法微调Qwen2.5-VL，在GOT-10k基准上表现优异，支持灵活初始化并保留原模型通用能力。

- Motivation: 传统视觉跟踪方法依赖显式分类和回归建模，需要大规模监督训练且缺乏灵活性。多模态大语言模型（如Qwen2.5-VL）在基础任务中表现优异，但直接应用于跟踪任务效果不佳，因此需要改进。
- Method: 采用GRPO强化学习方法在小规模数据集上微调Qwen2.5-VL，结合基于规则的奖励函数，开发了R1-Track模型。
- Result: R1-Track在GOT-10k基准上表现突出，支持通过边界框或文本描述灵活初始化，同时保留了原模型的通用能力。
- Conclusion: R1-Track展示了将MLLMs应用于视觉跟踪任务的潜力，未来仍有改进空间。


### [43] [RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation](https://arxiv.org/abs/2506.22007)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Soumajit Majumder,Ziyuan Liu,Gitta Kutyniok,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出一种新方法，通过分解任务和关键帧生成，避免自回归生成误差，实现长时程视频生成。

- Motivation: 解决现有文本到视频扩散模型在长时程机器人任务中因自回归生成导致的误差累积问题。
- Method: 1) 分解任务并生成关键帧；2) 使用扩散模型插值关键帧；3) 引入语义保持注意力模块和轻量策略模型。
- Result: 在两个基准测试中取得视频质量和一致性的最佳表现，并在长时程任务中优于现有策略模型。
- Conclusion: 提出的方法有效解决了长时程视频生成的误差问题，并在性能和一致性上表现优异。


### [44] [Towards Universal & Efficient Model Compression via Exponential Torque Pruning](https://arxiv.org/abs/2506.22015)
*Sarthak Ketanbhai Modi,Lim Zi Pong,Shourya Kuchhal,Yoshi Cao,Yupeng Cheng,Teo Yon Shin,Lin Shang-Wei,Zhiming Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为指数扭矩剪枝（ETP）的新方法，通过指数力应用方案改进现有剪枝技术的不足，显著提高了压缩率且几乎不影响准确性。

- Motivation: 现代深度神经网络（DNNs）的复杂性和规模快速增长，导致计算成本和内存使用问题日益突出，亟需高效的模型压缩技术。现有基于扭矩正则化的方法剪枝效果不理想，网络仍较密集且准确性下降明显。
- Method: 作者提出指数扭矩剪枝（ETP），采用指数力应用方案进行正则化，有效剪除冗余和远距离模块，同时保留近距离且必要的模块。
- Result: 实验结果表明，ETP在多个领域显著优于现有剪枝策略，压缩率更高且准确性下降几乎可忽略。
- Conclusion: ETP是一种简单但高效的剪枝方法，解决了现有技术的不足，为模型压缩提供了新思路。


### [45] [Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision](https://arxiv.org/abs/2506.22022)
*Zhanyi Lu,Yue Zhou*

Main category: cs.CV

TL;DR: 提出了一种结合语义保留约束和伪配对监督的面部风格化方法，解决了现有方法中的语义偏移问题，并实现了高质量的风格化效果。

- Motivation: 现有基于StyleGAN的方法在面部风格化中仍存在伪影或内容保真度不足的问题，主要原因是忽视了生成器在风格化过程中的语义偏移。
- Method: 提出了一种集成语义保留约束和伪配对监督的方法，并开发了多级伪配对数据集构建方法。此外，实现了无需复杂网络设计的多模态和参考引导风格化。
- Result: 实验结果表明，该方法生成的面部风格化结果具有高保真度和美学吸引力，优于现有方法。
- Conclusion: 该方法通过语义保留和伪配对监督有效提升了面部风格化的质量，并展示了灵活的多模态应用潜力。


### [46] [Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method](https://arxiv.org/abs/2506.22027)
*Han Wang,Shengyang Li,Jian Yang,Yuxuan Liu,Yixuan Lv,Zhuang Zhou*

Main category: cs.CV

TL;DR: 提出了一种结合光学和合成孔径雷达（SAR）的船舶重识别数据集（HOSS ReID数据集）及基线方法TransOSS，用于解决传统船舶跟踪方法的局限性。

- Motivation: 传统船舶跟踪方法依赖低分辨率或短时拍摄的卫星，无法满足全天候、高覆盖的需求。
- Method: 构建HOSS ReID数据集，提出基于Vision Transformer的TransOSS方法，优化嵌入结构和对比学习。
- Result: HOSS ReID数据集支持多模态船舶重识别，TransOSS能有效提取模态不变特征。
- Conclusion: HOSS ReID数据集和TransOSS方法为全天候船舶跟踪提供了新解决方案。


### [47] [Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation](https://arxiv.org/abs/2506.22032)
*Jialei Chen,Xu Zheng,Danda Pani Paudel,Luc Van Gool,Hiroshi Murase,Daisuke Deguchi*

Main category: cs.CV

TL;DR: 论文提出Chimera-Seg和SGD方法，解决零样本语义分割中视觉-语言对齐和特征粒度差异问题，实验显示性能提升。

- Motivation: 零样本语义分割中，现有方法在视觉-语言对齐和特征粒度差异方面存在挑战。
- Method: 提出Chimera-Seg结合分割主干和CLIP语义头，以及SGD选择性蒸馏知识，并使用SAM模块进一步对齐特征。
- Result: 在两个基准测试中，hIoU分别提升了0.9%和1.2%。
- Conclusion: Chimera-Seg和SGD有效解决了零样本语义分割中的关键问题，提升了性能。


### [48] [Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field](https://arxiv.org/abs/2506.22044)
*Hong Nie,Fuyuan Cao,Lu Chen,Fengxin Chen,Yuefeng Zou,Jun Yu*

Main category: cs.CV

TL;DR: FIAG是一种新型3D说话头合成框架，通过共享的全局高斯场和通用运动场，实现高效的身份特定适应，仅需少量训练数据即可完成。

- Motivation: 现有基于重建和渲染的说话头合成方法依赖身份特定模型，计算成本高且扩展性差。
- Method: FIAG结合全局高斯场（支持多身份共享表示）和通用运动场（捕捉跨身份运动动态），实现快速身份适应。
- Result: 实验表明，FIAG在效果和泛化性上优于现有方法。
- Conclusion: FIAG框架高效且通用，适用于多身份说话头合成。


### [49] [EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode](https://arxiv.org/abs/2506.22063)
*Durgesh K. Singh,Ahcene Boubekki,Qing Cao,Svein Arne Aase,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 提出了一种通过强制直线约束增强左心室测量准确性的新框架，利用解剖M模式图像训练地标检测器，减少误差并提高临床实用性。

- Motivation: 手动放置左心室地标耗时且易错，现有深度学习方法常导致地标错位，影响测量准确性。
- Method: 训练地标检测器于解剖M模式图像，实时从B模式视频计算并转换回B模式空间，结合半自动设计（用户仅需放置虚拟扫描线）。
- Result: 实验显示该方法比标准B模式方法更准确，且能适应不同网络架构。
- Conclusion: 提出的框架通过直线约束和半自动设计，显著提高了左心室测量的准确性和临床实用性。


### [50] [MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation](https://arxiv.org/abs/2506.22065)
*Dechao Meng,Steven Xiao,Xindi Zhang,Guangyuan Wang,Peng Zhang,Qi Wang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: MirrorMe是一个基于LTX视频模型的实时、可控框架，通过空间和时间压缩实现高效潜在空间去噪，解决了音频驱动肖像动画的高延迟和时间一致性问题。

- Motivation: 音频驱动肖像动画在实时生成高保真、时间一致的视频方面面临挑战，现有扩散方法因逐帧UNet架构导致高延迟和时间不一致。
- Method: 提出MirrorMe框架，采用LTX视频模型，引入参考身份注入机制、因果音频编码器和适配器，以及渐进式训练策略。
- Result: 在EMTD Benchmark上，MirrorMe在保真度、唇音同步准确性和时间稳定性方面表现优异。
- Conclusion: MirrorMe通过创新机制和训练策略，实现了高质量的实时音频驱动肖像动画。


### [51] [Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras](https://arxiv.org/abs/2506.22069)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出了一种新颖的方法，通过滚动快门相机中单扫描线的线投影交点估计相对位姿，无需显式建模相机运动。

- Motivation: 解决滚动快门相机在结构从运动（SfM）中的位姿估计问题，避免复杂的运动建模。
- Method: 利用线投影与单扫描线的交点进行位姿估计，支持单视图或多视图场景，并开发了最小求解器。
- Result: 在Fastec数据集上的实验验证了方法的可行性，适用于滚动快门SfM的初始化。
- Conclusion: 该方法为滚动快门SfM提供了基础模块，具有进一步开发的潜力。


### [52] [Reasoning in machine vision: learning to think fast and slow](https://arxiv.org/abs/2506.22075)
*Shaheer U. Saeed,Yipei Wang,Veeru Kasivisvanathan,Brian R. Davidson,Matthew J. Clarkson,Yipeng Hu,Daniel C. Alexander*

Main category: cs.CV

TL;DR: 论文提出了一种新的学习范式，通过增加推理时间（计算资源）提升机器在视觉任务中的推理能力，模仿人类双系统认知，在数据稀缺场景下表现优于监督学习和人类专家。

- Motivation: 机器智能在推理能力上仍受限于训练数据，无法在推理时动态优化解决方案，尤其在非语言领域（如视觉感知、医学诊断）表现不足。
- Method: 结合心理学中的双过程理论，设计了一个快速思考的System I模块和一个通过自玩强化学习迭代优化的System II模块。
- Result: 在视觉任务（包括计算机视觉基准和医学图像癌症定位）中，通过延长思考时间，表现优于大规模监督学习、基础模型和人类专家。
- Conclusion: 该范式为非语言机器推理提供了突破性潜力，尤其在数据稀缺场景下表现优异。


### [53] [Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction](https://arxiv.org/abs/2506.22078)
*Pei-Kai Huanga,Ya-Ting Chan,Kuan-Wen Chen,Yen-Chun Chou,Shih-Yu Yang,Chiou-Ting Hsu*

Main category: cs.CV

TL;DR: 提出了一种从超短2秒视频片段中准确测量心率的方法，通过周期性引导的rPPG估计和信号重建来解决挑战。

- Motivation: 现有远程心率测量方法多关注10秒视频，忽略了超短视频片段的需求。
- Method: 提出周期性引导的rPPG估计方法，并通过生成器重建更长的rPPG信号以减少频谱泄漏。
- Result: 在四个基准数据集上表现优异，优于现有技术。
- Conclusion: 方法在超短视频中心率测量上达到最先进性能。


### [54] [BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting](https://arxiv.org/abs/2506.22099)
*Zipei Ma,Junzhe Jiang,Yurui Chen,Li Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于Bézier曲线的高斯泼溅方法（BézierGS），用于动态场景重建，减少对高精度物体姿态标注的依赖，并在实验中表现优异。

- Motivation: 现有方法依赖高精度物体姿态标注，限制了大规模场景重建。BézierGS通过Bézier曲线建模动态物体运动轨迹，自动修正姿态误差。
- Method: 使用可学习的Bézier曲线表示动态物体运动轨迹，引入动态物体渲染和曲线间一致性的额外监督。
- Result: 在Waymo Open Dataset和nuPlan基准测试中，BézierGS在动态和静态场景重建及新视角合成上优于现有方法。
- Conclusion: BézierGS通过曲线建模和额外监督，实现了场景元素的合理分离和重建，为自动驾驶仿真提供了有效工具。


### [55] [Tied Prototype Model for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2506.22101)
*Hyeongji Kim,Stine Hansen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: TPM改进ADNet，通过绑定原型位置和多原型扩展，提升医学图像少样本分割的准确性。

- Motivation: 解决ADNet在医学图像分割中的局限性：单原型依赖、二分类问题和固定阈值。
- Method: 提出Tied Prototype Model (TPM)，绑定前景和背景分布的原型位置，支持多原型和多类分割。
- Result: TPM显著提升了分割准确性，并适应患者和器官的变异性。
- Conclusion: TPM为医学图像少样本分割提供了新视角，代码已开源。


### [56] [Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD](https://arxiv.org/abs/2506.22111)
*Ruthvik Bokkasam,Shankar Gangisetty,A. H. Abdul Hafez,C. V. Jawahar*

Main category: cs.CV

TL;DR: 论文介绍了一个印度驾驶行人数据集，用于解决非结构化环境中行人行为建模的复杂性，并展示了现有预测方法在该数据集上的性能下降。

- Motivation: 随着自动驾驶的快速发展，准确预测行人行为对复杂交通环境中的安全至关重要，但现有数据集未能充分捕捉非结构化环境的挑战。
- Method: 提出了一个印度驾驶行人数据集，包含高水平和低水平的详细注释，重点关注需要车辆注意的行人行为。
- Result: 在该数据集上，现有意图预测方法的性能下降高达15%，轨迹预测方法的MSE增加高达1208，表现不如标准数据集。
- Conclusion: 该数据集为行人行为研究社区提供了新的挑战，有助于开发更鲁棒的模型。


### [57] [Pipe Reconstruction from Point Cloud Data](https://arxiv.org/abs/2506.22118)
*Antje Alex,Jannis Stoppe*

Main category: cs.CV

TL;DR: 提出了一种从激光扫描数据自动重建管道的流程，通过骨架曲线估计、轴心重定位和3D平滑步骤，实现快速准确的管道建模。

- Motivation: 工业资产（如船舶和海上平台）的数字孪生需要精确重建复杂管道网络，但手动建模耗时耗力。
- Method: 采用拉普拉斯收缩估计骨架曲线，结合滚动球技术和2D圆拟合重定位轴心，最后进行3D平滑。
- Result: 能够确定管道的半径、长度和方向，并生成复杂管道网络的详细3D模型。
- Conclusion: 自动化管道重建支持数字孪生开发，降低成本并提高建模速度和准确性。


### [58] [Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization](https://arxiv.org/abs/2506.22134)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.CV

TL;DR: 提出了一种基于CP分解的低秩张量函数（CP-INR），通过神经网络参数化实现连续数据表示，并引入稀疏性和平滑性正则化，在多维数据恢复任务中表现优越。

- Motivation: 现有张量分解方法（如Tucker和CP）在灵活性和可解释性之间存在权衡，且稀疏解难以获得。CP-INR旨在结合CP分解的天然结构优势，同时解决稀疏性和平滑性问题。
- Method: 1. 提出CP-INR，利用神经网络参数化CP分解；2. 引入Schatten-p拟范数的变分形式实现稀疏性；3. 提出基于Jacobian谱范数的平滑性正则化。
- Result: 在图像修复、去噪和点云上采样等任务中，CP-INR优于现有方法，展示了其优越性和通用性。
- Conclusion: CP-INR结合了CP分解的天然结构和神经网络的非线性能力，通过稀疏性和平滑性正则化，在多维数据恢复任务中表现出色。


### [59] [Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs](https://arxiv.org/abs/2506.22139)
*Shaojie Zhang,Jiahui Yang,Jianqin Yin,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: Q-Frame是一种自适应帧选择和多分辨率缩放方法，通过CLIP等文本-图像匹配网络实现，提升视频理解任务的效果。

- Motivation: 现有视频-LLM在均匀帧采样下难以捕捉关键时空线索，需要更高效的方法。
- Method: 使用训练免费、即插即用的策略，结合Gumbel-Max技巧进行帧选择。
- Result: 在多个基准数据集上表现优异，优于现有方法。
- Conclusion: Q-Frame能有效提升视频理解任务的性能，具有广泛应用潜力。


### [60] [Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs](https://arxiv.org/abs/2506.22146)
*Amirmohammad Izadi,Mohammad Ali Banayeeanzade,Fatemeh Askari,Ali Rahimiakbar,Mohammad Mahdi Vahedi,Hosein Hasani,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 通过在视觉输入中添加低层次空间结构（如水平线）并结合文本提示，显著提升了视觉语言模型在视觉推理任务中的表现。

- Motivation: 当前视觉语言模型在视觉推理中存在绑定问题，即难以可靠地将感知特征与正确的视觉对象关联，导致在计数、视觉搜索等任务中表现不佳。
- Method: 在视觉输入中引入低层次空间结构，并配合鼓励序列化、空间感知解析的文本提示。
- Result: 在视觉搜索、计数、场景描述和空间关系任务中分别提升了25.00%、26.83%、0.32（编辑距离误差减少）和9.50%的性能。
- Conclusion: 低层次视觉结构化是提升视觉语言模型在空间任务中表现的有效且未被充分探索的方向。


### [61] [RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models](https://arxiv.org/abs/2506.22149)
*Ronald Fecso,José Morano,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: RetFiner是一种自监督学习视觉语言优化方案，通过利用文本数据的丰富监督信号，提升现有基础模型的表示能力，显著提高下游任务性能。

- Motivation: 现有OCT基础模型仅依赖图像数据，缺乏对图像的全面语义理解，导致下游任务表现不佳，且需要监督微调。
- Method: 提出RetFiner，结合多种训练目标，利用文本数据的监督信号优化基础模型表示。
- Result: 在七项OCT分类任务中，RetFiner显著提升了RETFound、UrFound和VisionFM的线性探测性能，平均分别提高5.8、3.9和2.1个百分点。
- Conclusion: RetFiner通过视觉语言优化有效提升了基础模型的适应性和下游任务表现，为医学影像分析提供了新思路。


### [62] [Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection](https://arxiv.org/abs/2506.22161)
*Taijin Zhao,Heqian Qiu,Yu Dai,Lanxiao Wang,Fanman Meng,Qingbo Wu,Hongliang Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为UOFS的优化框架，通过解耦特征空间来解决少样本目标检测中的问题，并结合HBO策略和SADA模块提升性能。

- Motivation: 现有少样本目标检测方法在共享特征空间中耦合了目标识别和分类任务，导致对新类别的样本表现不佳。
- Method: 提出UOFS框架，将特征空间解耦为幅度和角度两部分，并结合HBO策略和SADA模块优化训练过程。
- Result: 实验表明，该方法显著优于基于耦合特征空间的现有方法。
- Conclusion: 通过解耦特征空间和优化策略，UOFS框架有效提升了少样本目标检测的性能。


### [63] [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](https://arxiv.org/abs/2506.22179)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Hongfei Xue,Aidong Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于频率-语义增强的变分自编码器（FS-VAE），用于零样本骨架动作识别，通过频率分解和语义对齐提升模型性能。

- Motivation: 现有方法在零样本骨架动作识别中忽视了语义空间的细粒度动作模式，导致对视觉和语义相似动作的区分能力不足。
- Method: FS-VAE包含三个关键模块：1）基于频率的增强模块；2）多级对齐的语义动作描述；3）校准的跨对齐损失。
- Result: 在基准测试中，FS-VAE表现出色，能够有效区分视觉和语义相似的动作簇。
- Conclusion: 频率增强的语义特征显著提升了零样本动作识别的鲁棒性和准确性。


### [64] [Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints](https://arxiv.org/abs/2506.22191)
*Yuxin Cui,Rui Song,Yibin Li,Max Q. -H. Meng,Zhe Min*

Main category: cs.CV

TL;DR: 提出了一种新颖的多视图2D/3D刚性配准方法，通过两阶段设计和交叉视图约束，显著提高了配准的鲁棒性和准确性。

- Motivation: 解决单视图术中图像视野有限的问题，提升配准的准确性和鲁棒性。
- Method: 两阶段方法：第一阶段设计联合损失函数，引入交叉视图训练损失；第二阶段通过测试时优化细化估计姿态。
- Result: 在DeepFluoro数据集上实现了0.79±2.17 mm的平均目标配准误差，优于现有方法。
- Conclusion: 该方法通过多视图约束显著提升了配准性能，适用于临床导航。


### [65] [ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2506.22216)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: ReF-LLE是一种基于傅里叶频域和深度强化学习的个性化低光图像增强方法，首次将深度强化学习引入该领域。

- Motivation: 解决低光图像增强中的两个主要挑战：不同条件下的显著变化和主观偏好的影响。
- Method: 在傅里叶频域中操作，结合深度强化学习，引入零参考图像评估策略进行训练，并在推理阶段采用个性化自适应迭代策略。
- Result: 在基准数据集上表现优于现有方法，实现了更高的感知质量和个性化适应性。
- Conclusion: ReF-LLE在个性化低光图像增强中表现出色，具有优越的适应性和效果。


### [66] [Boosting Classification with Quantum-Inspired Augmentations](https://arxiv.org/abs/2506.22241)
*Matthias Tschöpe,Vitor Fortes Rey,Sogo Pierre Sanon,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: cs.CV

TL;DR: 量子门微小扰动可作为数据增强技术，提升经典机器学习性能，但强变换不增强隐私保护。

- Motivation: 研究量子门扰动在量子机器学习中的潜在优势，探索其作为数据增强技术的效果。
- Method: 使用随机Bloch球旋转作为量子启发的数据增强技术，应用于ImageNet数据集。
- Result: Top-1准确率提升3%，Top-5提升2.5%，F1分数从8%增至12%。强变换不增强隐私。
- Conclusion: 量子扰动增强经典机器学习性能，但隐私保护效果有限。


### [67] [4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration](https://arxiv.org/abs/2506.22242)
*Jiahui Zhang,Yurui Chen,Yueming Xu,Ze Huang,Yanpeng Zhou,Yu-Jie Yuan,Xinyue Cai,Guowei Huang,Xingyue Quan,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: 论文提出4D-VLA方法，通过整合4D信息解决机器人数据预训练中的坐标系混乱和状态混乱问题，显著提升性能。

- Motivation: 现有方法使用简单观测作为输入，导致动作分布分散，影响预训练效率。
- Method: 引入深度和时间信息到视觉特征中，结合RGB-D输入对齐坐标系，并提出记忆库采样策略。
- Result: 在模拟和真实实验中，模型性能显著提升，成功率高过OpenVLA。
- Conclusion: 4D-VLA方法有效解决了预训练中的混乱问题，增强了空间理解和适应性。


### [68] [EAMamba: Efficient All-Around Vision State Space Model for Image Restoration](https://arxiv.org/abs/2506.22246)
*Yu-Cheng Lin,Yu-Syuan Xu,Hao-Wei Chen,Hsien-Kai Kuo,Chun-Yi Lee*

Main category: cs.CV

TL;DR: EAMamba框架通过多向扫描机制解决了Vision Mamba在图像修复任务中的计算复杂性和局部像素遗忘问题，显著降低了计算量。

- Motivation: Vision Mamba在图像修复任务中表现出色，但仍面临计算复杂性和局部像素遗忘的挑战。
- Method: 提出EAMamba框架，引入多向选择性扫描模块（MHSSM）和全向扫描策略，以高效捕捉全局信息。
- Result: 实验表明，EAMamba在多种修复任务中显著降低FLOPs（31-89%），同时保持性能。
- Conclusion: EAMamba为图像修复任务提供了一种高效且性能优越的解决方案。


### [69] [COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication](https://arxiv.org/abs/2506.22274)
*Filippo Merlo,Ece Takmaz,Wenkai Chen,Albert Gatt*

Main category: cs.CV

TL;DR: 研究探讨视觉语言模型（VLMs）是否依赖场景上下文生成对象引用，并引入COOCO数据集测试模型在不同场景-对象一致性和噪声下的表现。

- Motivation: 探究VLMs是否像人类一样利用场景上下文进行对象识别和引用。
- Method: 使用COOCO数据集，测试模型在不同场景-对象一致性和噪声下的表现，并进行注意力分析。
- Result: 模型会根据场景-对象语义相关性和噪声水平自适应地利用上下文，尤其在高度一致或对象退化时更依赖上下文。
- Conclusion: VLMs动态平衡局部和上下文信息进行对象引用生成，数据集和代码已开源。


### [70] [Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment](https://arxiv.org/abs/2506.22283)
*Rui Xu,Yunke Wang,Yong Luo,Bo Du*

Main category: cs.CV

TL;DR: VisionDrop是一种无需训练的视觉令牌剪枝框架，通过视觉内注意力选择信息丰富的令牌，解决了跨模态不对齐问题，提升了计算效率。

- Motivation: 现有视觉令牌剪枝方法依赖文本条件，导致跨模态不对齐，影响效果。
- Method: 提出VisionDrop，基于视觉内注意力进行令牌选择，并设计渐进式剪枝流程。
- Result: 在多样化基准测试中表现优于现有方法，保持性能的同时提升效率。
- Conclusion: VisionDrop简单有效，无需额外训练即可实现高效推理。


### [71] [RoomCraft: Controllable and Complete 3D Indoor Scene Generation](https://arxiv.org/abs/2506.22291)
*Mengqi Zhou,Xipeng Wang,Yuxi Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: RoomCraft是一个多阶段管道，将真实图像、草图或文本描述转换为连贯的3D室内场景，解决了现有方法在几何一致性、空间关系和视觉真实性上的不足。

- Motivation: 现有神经生成方法因全局空间推理有限导致重复元素，而程序化方法在多约束场景中难以避免物体碰撞和布局不完整。
- Method: 结合场景生成管道和约束驱动优化框架，通过结构化信息提取、空间关系网络构建、启发式深度优先搜索算法和冲突感知定位策略（CAPS）实现优化布局。
- Result: RoomCraft在生成真实、语义连贯且视觉吸引人的房间布局方面显著优于现有方法。
- Conclusion: RoomCraft通过多阶段优化和冲突处理机制，有效解决了复杂多约束场景下的3D室内场景生成问题。


### [72] [OutDreamer: Video Outpainting with a Diffusion Transformer](https://arxiv.org/abs/2506.22298)
*Linhao Zhong,Fan Li,Yi Huang,Jianzhuang Liu,Renjing Pei,Fenglong Song*

Main category: cs.CV

TL;DR: OutDreamer是一种基于扩散变换器（DiT）的视频外绘框架，通过高效视频控制分支和条件外绘分支实现高质量内容生成，并在零样本任务中表现优异。

- Motivation: 视频外绘任务需要时空一致性，现有方法在生成质量和适应性上仍有不足，扩散变换器（DiT）因其优越性能成为潜在解决方案。
- Method: 提出OutDreamer框架，包含高效视频控制分支和条件外绘分支，引入掩码驱动自注意力层和潜在对齐损失，并采用跨视频片段细化器。
- Result: 在广泛认可的基准测试中，OutDreamer在零样本任务上优于现有方法。
- Conclusion: OutDreamer通过创新设计和优化，显著提升了视频外绘的质量和适应性。


### [73] [MatChA: Cross-Algorithm Matching with Feature Augmentation](https://arxiv.org/abs/2506.22336)
*Paula Carbó Cubero,Alberto Jaenal Gálvez,André Mateus,José Araújo,Patric Jensfelt*

Main category: cs.CV

TL;DR: 提出了一种针对跨检测器特征匹配的增强方法，显著提升了图像匹配和视觉定位的性能。

- Motivation: 现有方法在跨设备使用不同稀疏特征提取算法时表现不佳，关键在于假设使用相同检测器，而实践中很少如此。
- Method: 通过特征描述符增强和潜在空间的特征转换来解决跨检测器匹配问题。
- Result: 在多个基准测试中，该方法显著提升了图像匹配和视觉定位的性能。
- Conclusion: 该方法有效解决了跨检测器特征匹配的挑战，为视觉定位提供了新思路。


### [74] [A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake](https://arxiv.org/abs/2506.22338)
*Luigi Russo,Deodato Tapete,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 提出了一种基于单日期高分辨率SAR图像和多源地理空间数据的深度学习框架，用于快速检测建筑物损坏，无需依赖灾前数据。

- Motivation: 灾后快速识别建筑物损坏对应急响应至关重要，但传统光学卫星图像常受云层或缺乏灾前数据限制。
- Method: 结合SAR图像、OSM建筑轮廓、DSM数据和GEM属性，构建多模态深度学习模型，仅使用灾后数据。
- Result: 在土耳其2023年地震数据集上验证，显示结合地理空间特征显著提升检测性能和泛化能力。
- Conclusion: 该框架为灾后快速评估提供可靠工具，支持高效灾害管理，且具备跨区域适用性。


### [75] [Closing the Performance Gap in Biometric Cryptosystems: A Deeper Analysis on Unlinkable Fuzzy Vaults](https://arxiv.org/abs/2506.22347)
*Hans Geißner,Christian Rathgeb*

Main category: cs.CV

TL;DR: 本文提出了一种基于等频区间的特征量化方法，解决了模糊保险库生物识别系统中的性能下降问题。

- Motivation: 模糊保险库生物识别系统因特征集大小不稳定和特征类型转换导致的信息丢失而性能下降。
- Method: 提出了一种基于等频区间的特征量化方法，确保固定特征集大小，并支持无训练适应任意区间数。
- Result: 实验表明，该方法显著减少了模板保护引入的性能差距，并在主流生物识别系统中表现优异。
- Conclusion: 该方法有效解决了性能下降问题，适用于多种生物识别模态。


### [76] [From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications](https://arxiv.org/abs/2506.22360)
*Nouf Almesafri,Hector Figueiredo,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 研究比较了CNN（ResNet34）和ViT（ViT B16）在事件相机上的性能，发现ResNet34在分类准确率上略优，但ViT B16在噪声环境下表现更稳健。

- Motivation: 事件相机适用于动态环境（如无人机和自动驾驶），但传统深度学习模型在其上的性能尚不明确，因此需要比较主流架构的表现。
- Method: 在GEN1事件数据集上微调ResNet34和ViT B16，并在标准条件和模拟噪声下评估性能。
- Result: ResNet34和ViT B16在干净数据上的准确率分别为88%和86%，ViT B16在噪声下表现更稳健。
- Conclusion: 尽管ResNet34在准确率上略优，ViT B16的稳健性使其更适合动态环境，未来可扩展至无人机相关任务。


### [77] [Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation](https://arxiv.org/abs/2506.22375)
*Tiankai Chen,Yushu Li,Adam Goodge,Fei Teng,Xulei Yang,Tianrui Li,Xun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型（VLM）的无训练框架，用于3D点云数据的分布外（OOD）检测，通过图分数传播（GSP）方法提升检测效果。

- Motivation: 3D点云数据的OOD检测在安全感知应用中至关重要，但现有方法多针对2D图像，难以直接应用于3D环境。
- Method: 利用VLM构建基于类原型和测试数据的图，提出GSP方法结合提示聚类和自训练负提示，优化OOD评分。
- Result: GSP在合成和真实数据集上均优于现有方法，且适用于少样本场景。
- Conclusion: 该方法为3D点云OOD检测提供了高效且灵活的解决方案。


### [78] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Main category: cs.CV

TL;DR: 论文提出了一种新任务DVidE，旨在提升视频大模型在动态推理中的能力，通过反事实推理和ASR增强技术改进分类和生成任务。

- Motivation: 现有视频大模型在抽象和适应性推理上表现不足，DVidE任务旨在模拟现实中的动态推理过程。
- Method: 分类任务采用反事实推理框架，生成任务结合ASR和LLM；并提出了新的评估数据集和指标。
- Result: 实验结果显示方法显著提升了模型的动态推理能力。
- Conclusion: DVidE任务及相关方法有效增强了视频大模型的适应性推理能力。


### [79] [Test-Time Consistency in Vision Language Models](https://arxiv.org/abs/2506.22395)
*Shih-Han Chou,Shivam Chandhok,James J. Little,Leonid Sigal*

Main category: cs.CV

TL;DR: 提出了一种无需监督重新训练的测试时一致性框架，通过交叉熵一致性损失和伪标签一致性损失提升视觉语言模型的语义一致性。

- Motivation: 现有视觉语言模型在语义等效输入下表现不一致，影响可靠性和鲁棒性。
- Method: 提出一种后处理方法，利用交叉熵一致性损失和伪标签一致性损失，无需重新训练。
- Result: 在MM-R3基准测试中显著提升了模型的一致性。
- Conclusion: 该方法为多模态学习的推理时适应提供了新方向。


### [80] [Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy](https://arxiv.org/abs/2506.22432)
*Yuhao Liu,Tengfei Wang,Fang Liu,Zhenwei Wang,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: Shape-for-Motion是一个新颖的视频编辑框架，通过3D代理实现精确且一致的视频编辑。

- Motivation: 现有方法在满足用户对视频编辑的精细控制需求方面仍有不足。
- Method: 利用3D代理（时间一致的网格）进行编辑，采用双传播策略简化编辑流程，并通过解耦视频扩散模型生成结果。
- Result: 支持多种精确且物理一致的操作，实验证明了其优越性和有效性。
- Conclusion: Shape-for-Motion为高质量、可控的视频编辑工作流迈出了关键一步。


### [81] [WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields](https://arxiv.org/abs/2506.22433)
*Sadra Safadoust,Fabio Tosi,Fatma Güney,Matteo Poggi*

Main category: cs.CV

TL;DR: WarpRF是一种无需训练、通用的框架，用于量化辐射场的不确定性，通过跨视角的后向变形和一致性测量实现。

- Motivation: 量化辐射场的不确定性对于提高渲染精度和下游任务（如主动视图选择）至关重要。
- Method: 利用后向变形将可靠渲染投影到新视角，并测量与渲染图像的一致性。
- Result: WarpRF在不确定性量化和下游任务中表现优异，优于现有方法。
- Conclusion: WarpRF是一种简单、低成本且通用的解决方案，适用于任何辐射场实现。


### [82] [MiCo: Multi-image Contrast for Reinforcement Visual Reasoning](https://arxiv.org/abs/2506.22434)
*Xi Chen,Mingkang Zhu,Shaoteng Liu,Xiaoyang Wu,Xiaogang Xu,Yu Liu,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 论文提出一种方法，通过自监督视觉表示学习，利用图像中的固有约束作为监督信号，训练模型进行多图像推理。

- Motivation: 解决传统方法依赖人工标注问答对的局限性，尤其是在处理细粒度视觉细节和多图像复杂逻辑时的挑战。
- Method: 构建图像三元组（两个增强视图和一个相似但不同的图像），通过规则强化学习优化模型，使其生成推理过程比较图像。
- Result: 模型在视觉比较任务上训练后，能够泛化到多种问题，无需人工标注问答对，在多图像推理基准上表现优异。
- Conclusion: 该方法展示了自监督学习在多图像推理任务中的潜力，为视觉语言模型提供了新的训练思路。
## cs.AR

### [83] [Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction](https://arxiv.org/abs/2506.22156)
*Mattia Ricchi,Fabrizio Alfonsi,Camilla Marella,Marco Barbieri,Alessandra Retico,Leonardo Brizi,Alessandro Gabrielli,Claudia Testa*

Main category: cs.AR

TL;DR: FPGA加速的神经网络用于MRF数据的实时脑参数重建，训练时间显著缩短。

- Motivation: 传统神经网络训练耗时且资源密集，限制了MRF技术在实时应用中的潜力。
- Method: 采用基于FPGA的神经网络，优化训练过程，显著减少训练时间。
- Result: 训练时间从标准CPU的250倍缩短至200秒，实现实时脑分析。
- Conclusion: 该方法有望推动移动设备上的实时脑分析，革新临床决策和远程医疗。
## cs.RO

### [84] [TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions](https://arxiv.org/abs/2506.21630)
*Yixin Sun,Li Li,Wenke E,Amir Atapour-Abarghouei,Toby P. Breckon*

Main category: cs.RO

TL;DR: 论文提出了Trail-based Off-road Multimodal Dataset (TOMD)，用于解决非结构化户外环境中可通行路径检测的挑战，并提出了动态多尺度数据融合模型。

- Motivation: 现有数据集和模型主要针对城市环境或宽阔的越野路径，无法应对狭窄、小径般的复杂越野场景，因此需要专门的数据集和方法。
- Method: 引入TOMD数据集，包含多模态传感器数据，并提出动态多尺度数据融合模型，分析不同光照条件下的融合策略。
- Result: 结果表明该方法有效，且光照对分割性能有显著影响。
- Conclusion: TOMD数据集和提出的模型为小径越野导航研究提供了支持，未来可进一步探索相关应用。


### [85] [AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing](https://arxiv.org/abs/2506.21635)
*Haiping Yang,Huaxing Liu,Wei Wu,Zuohui Chen,Ning Wu*

Main category: cs.RO

TL;DR: 论文提出了一种基于视觉的无人机着陆偏差预警系统AeroLite-MDNet，通过多尺度融合模块和分割分支提高检测精度，并引入新评估指标AWD和新数据集UAVLandData，实验显示系统性能优异。

- Motivation: 无人机着陆时因GPS信号干扰等问题难以精准着陆，需一种可靠方法提升着陆安全性。
- Method: 提出AeroLite-MDNet模型，结合多尺度融合模块和分割分支，用于偏差检测和方向估计；引入AWD指标和新数据集UAVLandData。
- Result: 系统AWD为0.7秒，偏差检测准确率达98.6%，显著提升着陆可靠性。
- Conclusion: AeroLite-MDNet系统有效解决无人机着陆偏差问题，实验验证其高效性，代码将开源。


### [86] [Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation](https://arxiv.org/abs/2506.21732)
*Ameya Salvi,Venkat Krovi*

Main category: cs.RO

TL;DR: 提出了一种基于学习的视觉导航结构化方法，显著提升了性能。

- Motivation: 解决滑移转向车辆在动态操作中缺乏准确分析模型的问题。
- Method: 采用端到端学习方法（如模仿学习和深度强化学习），提出结构化学习视觉导航的方法。
- Result: 通过软件模拟、硬件评估和消融研究，性能显著优于现有方法。
- Conclusion: 结构化学习方法为滑移转向车辆的视觉导航提供了有效解决方案。


### [87] [Embodied Domain Adaptation for Object Detection](https://arxiv.org/abs/2506.21860)
*Xiangyu Shi,Yanyuan Qiao,Lingqiao Liu,Feras Dayoub*

Main category: cs.RO

TL;DR: 论文提出了一种无需源数据的领域自适应方法（SFDA），通过时间聚类和多尺度阈值融合改进伪标签，结合对比学习的Mean Teacher框架，显著提升了室内动态环境下的零样本检测性能。

- Motivation: 解决标准闭集方法和开放词汇对象检测（OVOD）在室内环境中因领域偏移和动态条件导致的性能不足问题。
- Method: 采用源数据无关的领域自适应（SFDA），通过时间聚类优化伪标签，多尺度阈值融合，以及结合对比学习的Mean Teacher框架。
- Result: 在EDAOD基准测试中，零样本检测性能显著提升，能够灵活适应动态室内条件。
- Conclusion: 提出的方法有效解决了室内环境中的领域适应问题，为移动机器人的感知任务提供了更可靠的解决方案。


### [88] [Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration](https://arxiv.org/abs/2506.22116)
*Noora Sassali,Roel Pieters*

Main category: cs.RO

TL;DR: 该研究提出了一种基于姿态估计和几何模型的方法，用于平面工作空间中指向目标的定位，并评估了其在机器人任务中的表现。

- Motivation: 指向手势是人机协作中常用的交互方式，但缺乏系统的方法来定位和评估指向目标。
- Method: 采用姿态估计和基于肩-腕伸展的几何模型，从RGB-D流中提取手势数据，并集成多模态功能（如物体检测和语音处理）。
- Result: 研究提出了一套严格的评估方法，并在概念验证机器人系统中展示了多模态集成的可行性。
- Conclusion: 该方法为多模态机器人系统中的指向手势提供了实用工具，但存在局限性，需进一步优化。


### [89] [KnotDLO: Toward Interpretable Knot Tying](https://arxiv.org/abs/2506.22176)
*Holly Dinkel,Raghavendra Navaratna,Jingyi Xiang,Brian Coltin,Trey Smith,Timothy Bretl*

Main category: cs.RO

TL;DR: KnotDLO是一种无需人工演示或训练的单手可变形线性物体（DLO）打结方法，具有抗遮挡性、可重复性和可解释性。

- Motivation: 解决DLO打结任务中因遮挡、初始配置变化和缺乏人类示范带来的挑战。
- Method: 通过当前DLO形状规划抓取和目标路径点，利用分段线性曲线跟踪和几何计算生成中间路径点，将视觉推理与控制分离。
- Result: 在16次打结试验中，KnotDLO从未见过的配置中成功打结的成功率为50%。
- Conclusion: KnotDLO展示了无需人类示范的DLO打结能力，但在成功率上有提升空间。
## cs.LG

### [90] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为非对称策略优化（APO）的方法，通过动态调整KL惩罚和抑制过度思考，提升多模态大语言模型（MLLMs）的复杂推理能力。

- Motivation: 多模态大语言模型（MLLMs）在复杂推理任务中表现不佳，而强化学习（RL）的应用常导致性能下降或过度思考问题。
- Method: 提出APO方法，将样本分为正负两组：正样本采用动态调整KL权重的DADS技术，负样本采用抑制过长响应的STCR技术。
- Result: 在Qwen2.5-VL-3B上实现的View-R1-3B模型，推理能力平均提升7%，并在多个基准测试中优于更大的MLLMs。
- Conclusion: APO方法有效提升了MLLMs的推理能力，同时保持了通用任务的性能，展示了DADS和STCR技术的广泛适用性。


### [91] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: 论文提出了一种动态控制质量-复杂度权衡的方法，通过调整时间步长和神经网络长度，优化连续归一化流和扩散模型的采样效率。

- Motivation: 现有方法主要减少采样时间步以提高效率，但忽略了神经网络长度的动态调整。本文探索了时间步长和网络长度的动态控制，以优化质量-复杂度权衡。
- Method: 通过重新连接基于Transformer架构的块，解决内部离散化的ODE，并在训练中引入时间和长度一致性项，实现任意时间步和块数的采样。
- Result: 在CelebA-HQ和ImageNet上的实验显示，最高效采样模式下延迟减少3倍，高质量采样时FID分数提升3.5分。
- Conclusion: 提出的方法在时间维度和长度维度上均具有灵活性，显著降低了延迟和内存使用，同时提升了生成质量。


### [92] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: 论文提出CitySim愿景，通过SceneDiffuser++实现端到端的城市规模交通模拟，整合场景生成、行为建模等技术。

- Motivation: 解决交通模拟中动态场景生成和环境模拟等技术研究不足的问题，实现更真实的城市规模模拟。
- Method: 提出SceneDiffuser++，基于单一损失函数的端到端生成世界模型，支持从A点到B点的城市规模模拟。
- Result: 在扩展的Waymo Open Motion Dataset上验证了SceneDiffuser++的模拟质量和长期模拟的真实性。
- Conclusion: SceneDiffuser++为城市规模交通模拟提供了高效且真实的解决方案。


### [93] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 本文提出了一种基于Koopman算子的方法，用于加速条件流匹配（CFM）并提升其动态过程的可解释性。通过将非线性流映射为线性演化，实现了闭式一步采样，显著提高了效率。

- Motivation: 传统CFM采样依赖非线性ODE的数值求解，计算成本高且难以解释。现有方法虽能加速采样，但未能揭示生成过程的结构。
- Method: 结合Koopman算子理论，提出无解码器的Koopman-CFM架构，在学习的嵌入空间中实现线性动态，支持矩阵指数的一步采样。
- Result: 在2D数据集和MNIST、F-MNIST、TFD等基准测试中，显著提升了采样速度，同时通过Koopman生成器的谱特性提供了生成行为的分析工具。
- Conclusion: Koopman增强的流匹配结合了采样效率和解析结构，为快速且可解释的生成建模提供了新方向。
## cs.IR

### [94] [Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization](https://arxiv.org/abs/2506.21601)
*Duong Bach*

Main category: cs.IR

TL;DR: HPC-ColPali通过分层压缩技术提升ColPali的效率，同时保持检索精度。

- Motivation: 解决ColPali因高维补丁嵌入和延迟交互评分带来的存储和计算成本问题。
- Method: 采用K-Means量化、注意力引导动态剪枝和可选二进制编码三种创新技术。
- Result: 在ViDoRe和SEC-Filings数据集上，HPC-ColPali降低查询延迟30-50%，同时保持高检索精度。
- Conclusion: HPC-ColPali是一种可扩展且高效的多向量文档检索解决方案。


### [95] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Main category: cs.IR

TL;DR: 提出了一种系统化、量化的基准框架，用于评估多模态生成AI的可信度，特别是在企业文档智能中的VisualRAG系统中。通过优化模态权重（文本30%、图像15%、标题25%、OCR30%），性能提升57.3%。

- Motivation: 当前的多模态生成AI评估框架难以建立可信度，阻碍了企业应用。
- Method: 引入量化基准框架，测量跨模态输入（文本、图像、标题、OCR）的可信度，建立技术指标与用户信任度之间的定量关系。
- Result: 优化模态权重后，性能提升57.3%，同时保持计算效率。
- Conclusion: 该工作为企业AI提供了量化可信度的严格框架，推动了负责任AI的部署。


### [96] [CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design](https://arxiv.org/abs/2506.21934)
*Najmeh Forouzandehmehr,Reza Yousefi Maragheh,Sriram Kollipara,Kai Zhao,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.IR

TL;DR: CAL-RAG是一个基于检索增强和代理协作的内容感知布局生成框架，结合多模态检索、大语言模型和视觉语言评分，显著提升了布局生成的性能。

- Motivation: 现有深度生成模型在布局生成中缺乏上下文设计范例的支持，难以实现语义对齐和视觉连贯性。
- Method: CAL-RAG通过检索相关布局范例，利用LLM推荐布局，视觉语言评分代理评估，反馈代理迭代优化。
- Result: 在PKU PosterLayout数据集上，CAL-RAG在多个布局指标上达到最优性能，显著优于基线方法。
- Conclusion: 结合检索增强和代理多步推理，CAL-RAG为自动化布局生成提供了可扩展、可解释且高保真的解决方案。
## physics.optics

### [97] [Inverse Design of Diffractive Metasurfaces Using Diffusion Models](https://arxiv.org/abs/2506.21748)
*Liav Hen,Erez Yosef,Dan Raviv,Raja Giryes,Jacob Scheuer*

Main category: physics.optics

TL;DR: 该论文提出了一种利用扩散模型进行超表面逆向设计的方法，解决了传统方法中计算量大、易陷入局部最优的问题。

- Motivation: 超表面的逆向设计因结构与光学响应之间的复杂非线性关系而具有挑战性，传统方法需要专家调整且计算成本高。
- Method: 通过结合扩散模型的生成能力，使用RCWA模拟器生成训练数据，训练条件扩散模型以预测目标空间功率分布对应的超表面几何结构。
- Result: 模型能够快速生成低误差的超表面设计，例如均匀强度分束器和偏振分束器，设计时间少于30分钟。
- Conclusion: 该方法为数据驱动的超表面设计提供了高效工具，并公开了代码和数据集以支持进一步研究。
## eess.IV

### [98] [PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors](https://arxiv.org/abs/2506.21680)
*Sai Sri Teja,Sreevidya Chintalapati,Vinayak Gupta,Mukund Varma T,Haejoon Lee,Aswin Sankaranarayanan,Kaushik Mitra*

Main category: eess.IV

TL;DR: 论文提出PhotonSplat框架，利用SPAD传感器的高速成像能力解决运动模糊问题，支持3D场景重建和动态场景处理。

- Motivation: 现有神经渲染技术在输入图像因运动模糊而损坏时表现不佳，SPAD传感器的高速成像能力为解决这一问题提供了可能。
- Method: 引入PhotonSplat框架，直接从SPAD二值图像重建3D场景，结合3D空间滤波降噪，支持无参考和基于参考的着色。
- Result: 提出PhotonScenes数据集，验证了方法在动态场景中的有效性，支持下游任务如分割和检测。
- Conclusion: PhotonSplat通过SPAD传感器和新型滤波技术，有效解决了运动模糊问题，扩展了神经渲染的应用场景。


### [99] [TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker](https://arxiv.org/abs/2506.21765)
*Qi Li,Shaheer U. Saeed,Yuliang Huang,Mingyuan Luo,Zhongnuo Yan,Jiongquan Chen,Xin Yang,Dong Ni,Nektarios Winter,Phuc Nguyen,Lucas Steinberger,Caelan Haney,Yuan Zhao,Mingjie Jiang,Bowen Ren,SiYeoul Lee,Seonho Kim,MinKyung Seo,MinWoo Kim,Yimeng Dou,Zhiwei Zhang,Yin Li,Tomy Varghese,Dean C. Barratt,Matthew J. Clarkson,Tom Vercauteren,Yipeng Hu*

Main category: eess.IV

TL;DR: TUS-REC2024挑战赛旨在推动无追踪器自由手超声3D重建技术的发展，通过公开数据集和评估框架，吸引了43个团队参与，展示了多种算法方法，并总结了当前技术的进展与局限。

- Motivation: 为无追踪器自由手超声3D重建提供低成本、便携且广泛适用的解决方案，解决运动估计、漂移累积和协议通用性等挑战。
- Method: 挑战赛提供了公开数据集、基线模型和评估框架，吸引了43个团队提交21个解决方案，涵盖循环模型、配准驱动体积优化、注意力机制和物理启发模型等方法。
- Result: 挑战赛展示了当前技术的进展与局限，为未来研究提供了方向。数据集、评估代码和基线模型已公开。
- Conclusion: TUS-REC2024挑战赛推动了该领域的发展，未来将持续改进并定期举办，以促进技术进步。


### [100] [Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer](https://arxiv.org/abs/2506.21880)
*Yuansheng Li,Yunhao Zou,Linwei Chen,Ying Fu*

Main category: eess.IV

TL;DR: 论文提出了一种新的干涉高光谱成像（IHI）重建方法，通过简化退化模型和设计深度学习模型IHRUT，解决了训练数据缺乏和特定退化问题。

- Motivation: IHI在遥感任务中具有优势，但受限于复杂误差和现有信号处理算法的性能。缺乏训练数据和难以消除IHI特定退化是主要挑战。
- Method: 建立简化的IHI退化模型和参数估计方法，生成训练数据；设计IHRUT模型，通过条纹增强机制和空间-光谱变换器进行重建。
- Result: 实验表明，该方法在性能和泛化能力上表现优越。
- Conclusion: 提出的方法有效解决了IHI重建中的关键问题，为深度学习在IHI中的应用提供了新思路。


### [101] [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](https://arxiv.org/abs/2506.21884)
*Fabian Perez,Sara Rojas,Carlos Hinojosa,Hoover Rueda-Chacón,Bernard Ghanem*

Main category: eess.IV

TL;DR: UnMix-NeRF将光谱解混引入NeRF，实现联合高光谱新视角合成和无监督材料分割，解决了现有方法缺乏材料属性的问题。

- Motivation: 现有基于NeRF的分割方法仅依赖RGB数据，缺乏材料属性，限制了在机器人、增强现实等应用中的准确材料感知。
- Method: 通过建模光谱反射率的漫反射和镜面反射分量，利用全局端元字典表示纯材料特征，并通过点级丰度捕获其分布，实现无监督材料聚类。
- Result: 实验表明，UnMix-NeRF在光谱重建和材料分割方面优于现有方法。
- Conclusion: UnMix-NeRF通过整合光谱解混，实现了材料感知的新视角合成和分割，为场景编辑提供了灵活性。


### [102] [StableCodec: Taming One-Step Diffusion for Extreme Image Compression](https://arxiv.org/abs/2506.21977)
*Tianyu Zhang,Xin Luo,Li Li,Dong Liu*

Main category: eess.IV

TL;DR: StableCodec提出了一种基于扩散模型的超低比特率图像压缩方法，通过一步去噪和高保真重建，显著提升了编码效率和实时性。

- Motivation: 现有扩散模型在超低比特率下需要大量去噪步骤，且难以保证像素级一致性，限制了实时应用。
- Method: 开发了高效的深度压缩潜在编码器和双分支编码结构，结合端到端优化，实现一步去噪和高保真重建。
- Result: 在CLIC 2020、DIV2K和Kodak数据集上，StableCodec在FID、KID和DISTS指标上显著优于现有方法，比特率低至0.005 bpp。
- Conclusion: StableCodec在超低比特率下实现了高真实性和高保真重建，同时具备与主流变换编码相当的推理速度。


### [103] [Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction](https://arxiv.org/abs/2506.22012)
*Qi Gao,Zhihao Chen,Dong Zeng,Junping Zhang,Jianhua Ma,Hongming Shan*

Main category: eess.IV

TL;DR: 提出了一种名为NEED的噪声启发扩散模型，用于低剂量CT重建，通过双域扩散模型和噪声特性匹配，显著提升了重建和泛化性能。

- Motivation: 解决深度学习模型在未见剂量数据上的泛化问题，并改进扩散模型在低剂量CT重建中的噪声适应性和先验信息准确性。
- Method: 提出移位泊松扩散模型用于投影数据去噪，以及双引导扩散模型用于图像重建，通过双域扩散模型和训练策略实现泛化。
- Result: 在多个数据集上，NEED在重建和泛化性能上均优于现有方法。
- Conclusion: NEED是一种高效且泛化能力强的低剂量CT重建方法，仅需正常剂量数据训练即可适应多种剂量水平。


### [104] [Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning](https://arxiv.org/abs/2506.22041)
*Julia Machnio,Sebastian Nørgaard Llambias,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的白质高信号（WMH）分割和定位框架，支持单模态和多模态MRI输入，并评估了不同输入配置的性能。

- Motivation: 现有方法在处理缺失模态和整合解剖定位方面缺乏灵活性，需要更灵活且高效的方法。
- Method: 使用深度学习框架，评估了四种输入配置（FLAIR-only、T1-only、FLAIR+T1、模态可互换），并引入多任务模型联合预测病变和解剖区域。
- Result: 多模态输入显著提升分割性能，但模态可互换设置牺牲了准确性以换取鲁棒性。多任务学习效果不如单独模型。
- Conclusion: 多模态融合对WMH分析更准确和鲁棒，联合建模在集成预测方面具有潜力。


### [105] [Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections](https://arxiv.org/abs/2506.22222)
*Hao Xu,Ruth Lim,Brian E. Chapman*

Main category: eess.IV

TL;DR: 该论文提出四种深度学习模型用于自动分割主动脉夹层的真腔、假腔和假腔血栓，显著提高了分割准确性。

- Motivation: 主动脉夹层是一种危及生命的心血管疾病，需要从CTA图像中准确分割真腔、假腔和假腔血栓以进行有效管理。手动分割耗时且存在变异性，因此需要自动化解决方案。
- Method: 研究开发了四种基于深度学习的管道（单步模型、顺序模型、顺序多任务模型和集成模型），使用3D U-Net和Swin-UnetR架构，并在100例回顾性CTA图像数据集上进行了训练和测试。
- Result: 提出的方法在分割准确性上表现优异，真腔、假腔和假腔血栓的Dice系数分别为0.91、0.88和0.47，优于现有研究。
- Conclusion: 该研究为主动脉夹层的自动分割提供了高效解决方案，有助于形态学参数的提取，支持监测和治疗规划。


### [106] [Cardiovascular disease classification using radiomics and geometric features from cardiac CT](https://arxiv.org/abs/2506.22226)
*Ajay Mittal,Raghav Mehta,Omar Todd,Philipp Seeböck,Georg Langs,Ben Glocker*

Main category: eess.IV

TL;DR: 论文提出了一种基于分割、配准和分类的三步法，用于从CT图像中自动检测和分类心血管疾病（CVD），提高了分类准确性和临床可解释性。

- Motivation: 现有深度学习方法直接从原始CT数据或结合心脏结构分割进行端到端分类，临床可解释性差。本文旨在通过分步方法解决这一问题。
- Method: 将CVD分类流程分为图像分割、图像配准和下游分类三步，利用Atlas-ISTN框架和分割基础模型生成解剖结构分割和健康图谱，提取放射组学特征和几何特征进行分类。
- Result: 在ASOCA数据集上，该方法分类准确率达87.50%，显著优于直接使用原始CT图像的模型（67.50%）。
- Conclusion: 分步方法不仅提高了CVD分类准确性，还增强了临床可解释性，为未来研究提供了新思路。


### [107] [DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model](https://arxiv.org/abs/2506.22280)
*Yuliang Huang,Imraj Singh,Thomas Joyce,Kris Thielemans,Jamie R. McClelland*

Main category: eess.IV

TL;DR: 论文提出了一种基于自由变形（FFD）的4D高斯泼溅（4DGS）方法，用于动态CBCT重建，解决了现有方法计算成本高和运动不一致的问题，实现了更高效的图像重建。

- Motivation: 动态CBCT在放疗中因呼吸运动产生运动伪影，现有方法如相位排序或隐式运动表示存在计算成本高或运动不一致的问题。
- Method: 引入FFD空间基函数和变形感知框架，统一高斯均值位置、尺度和旋转的时间演化，确保运动一致性。
- Result: 在六个CBCT数据集上验证，图像质量优于HexPlane，速度提升6倍。
- Conclusion: 变形感知4DGS为高效、运动补偿的CBCT重建提供了潜在解决方案。


### [108] [Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism](https://arxiv.org/abs/2506.22397)
*Anirban Ray,Ashesh,Florian Jug*

Main category: eess.IV

TL;DR: HazeMatching是一种新型的迭代去雾方法，用于平衡显微镜图像的去雾效果与真实性。

- Motivation: 解决现有方法在显微镜图像去雾中无法同时保证数据保真度和真实性的问题。
- Method: 基于条件流匹配框架，通过引导生成过程实现去雾。
- Result: 在5个数据集上优于7个基线方法，平衡了保真度和真实性。
- Conclusion: HazeMatching无需显式退化算子，适用于真实显微镜数据，且结果校准良好。


### [109] [Single-shot HDR using conventional image sensor shutter functions and optical randomization](https://arxiv.org/abs/2506.22426)
*Xiang Dai,Kyrollos Yanny,Kristina Monakhova,Nicholas Antipa*

Main category: eess.IV

TL;DR: 提出了一种基于全局复位释放（GRR）快门模式和光学随机排列的单次曝光高动态范围（HDR）成像方法，有效解决了传统多曝光方法的运动伪影问题。

- Motivation: 传统HDR成像依赖多曝光，导致动态场景中出现运动伪影；单次曝光方法虽能缓解此问题，但在高光区域表现不佳。
- Method: 利用GRR快门模式实现不同区域的曝光时间差异，结合光学随机排列技术，通过优化问题和总变分先验恢复HDR数据。
- Result: 仿真显示，在10%或更多像素饱和时优于其他单次曝光方法；物理原型实现了73dB的动态范围。
- Conclusion: 该方法在单次曝光HDR成像中表现优异，尤其适用于高光区域，且硬件成本低。
## cs.CL

### [110] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: 论文提出了MIME基准，用于评估视觉语言模型对哑剧动作的理解能力，发现现有模型表现远不如人类。

- Motivation: 研究哑剧动作（NVC的子集）是理解更复杂非语言交流的关键前提。
- Method: 构建MIME基准，包含86种哑剧动作，通过运动捕捉数据生成变体以评估模型鲁棒性。
- Result: 现有视觉语言模型在MIME上表现显著低于人类水平。
- Conclusion: 需加强研究以提升模型对人类手势的理解能力。


### [111] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Main category: cs.CL

TL;DR: 提出了一种基于BART架构的新手语识别方法，通过独立编码x和y坐标并保持其关联性，显著提升了效率和准确性。

- Motivation: 解决传统方法在效率和准确性之间的权衡问题，以及梯度消失和高计算成本的问题。
- Method: 使用BART架构的编码器-解码器独立编码x和y坐标，并通过交叉注意力保持其关联性。
- Result: 在LSA-64数据集上达到96.04%的准确率，参数仅749,888，优于百万参数模型。
- Conclusion: 该方法为手语识别提供了可靠且高效的解决方案，有望提升听障人士的沟通工具。


### [112] [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
*Avash Palikhe,Zhenyu Yu,Zichong Wang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）的可解释性方法，分类讨论了基于不同Transformer架构的XAI技术，并探讨了其评估、应用及未来研究方向。

- Motivation: LLMs的决策过程缺乏透明度，限制了其在高风险领域的应用，因此需要系统化的可解释性方法。
- Method: 通过分类（编码器、解码器、编码器-解码器模型）综述XAI方法，并分析其评估和应用。
- Result: 总结了现有XAI技术的优缺点，并提出了未来研究方向。
- Conclusion: 推动透明和负责任的LLMs发展需要进一步研究可解释性方法。


### [113] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段框架（感知与预测）来评估视觉语言模型（VLMs）作为世界模型（WMs）的能力，并通过大规模基准测试WM-ABench发现现有模型在基础世界建模能力上存在显著不足。

- Motivation: 现有研究对VLMs作为通用世界模型的能力缺乏系统性评估，尤其是感知与预测能力的细粒度分析。
- Method: 提出两阶段评估框架（感知与预测），设计WM-ABench基准，涵盖23个细粒度维度，并在6个模拟环境中进行660次实验。
- Result: 现有VLMs在基础世界建模能力上表现不佳，例如运动轨迹区分准确率接近随机，且存在颜色偏见等问题。
- Conclusion: VLMs与世界建模的人类水平存在显著差距，需进一步改进其感知与推理能力。
## quant-ph

### [114] [QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks](https://arxiv.org/abs/2506.22340)
*Yannick Werner,Akash Malemath,Mengxi Liu,Vitor Fortes Rey,Nikolaos Palaiodimopoulos,Paul Lukowicz,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: 论文提出了一种基于Kolmogorov Arnold Networks (KANs)的量子架构QuKAN，结合经典与量子组件，展示了其在量子机器学习中的可行性和性能。

- Motivation: 探索KANs在量子机器学习中的潜力，利用其表达复杂函数的能力，结合量子电路的优势。
- Method: 通过Quantum Circuit Born Machine (QCBM)实现KAN的混合和全量子形式，利用预训练残差函数进行参数传递。
- Result: 展示了QuKAN架构的可行性、可解释性和性能。
- Conclusion: QuKAN为量子机器学习提供了一种新的高效架构，结合了KANs和量子计算的优势。
