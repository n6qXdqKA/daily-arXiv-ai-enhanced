{"id": "2507.01099", "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd4D\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u89c6\u89d2\u70b9\u56fe\u5bf9\u9f50\u76d1\u7763\uff0c\u5b9e\u73b0\u591a\u89c6\u89d23D\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u673a\u5668\u4eba\u52a8\u6001\u573a\u666f\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u589e\u5f3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u89c4\u5212\u548c\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u89c6\u9891\u751f\u6210\u4e2d\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "method": "\u5229\u7528RGB-D\u89c2\u6d4b\u6570\u636e\uff0c\u901a\u8fc7\u8de8\u89c6\u89d2\u70b9\u56fe\u5bf9\u9f50\u76d1\u7763\u8bad\u7ec3\u6a21\u578b\uff0c\u5b66\u4e60\u5171\u4eab3D\u573a\u666f\u8868\u793a\uff0c\u65e0\u9700\u76f8\u673a\u4f4d\u59ff\u8f93\u5165\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e2d\uff0c\u751f\u6210\u66f4\u7a33\u5b9a\u4e14\u7a7a\u95f4\u5bf9\u9f50\u7684\u89c6\u9891\u9884\u6d4b\uff0c\u652f\u6301\u673a\u5668\u4eba\u8f68\u8ff9\u6062\u590d\u548c\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u65b0\u89c6\u89d2\u6cdb\u5316\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2507.01123", "pdf": "https://arxiv.org/pdf/2507.01123", "abs": "https://arxiv.org/abs/2507.01123", "authors": ["Rahul A. Burange", "Harsh K. Shinde", "Omkar Mutyalwar"], "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "20 pages, 24 figures", "summary": "Landslides pose severe threats to infrastructure, economies, and human lives,\nnecessitating accurate detection and predictive mapping across diverse\ngeographic regions. With advancements in deep learning and remote sensing,\nautomated landslide detection has become increasingly effective. This study\npresents a comprehensive approach integrating multi-source satellite imagery\nand deep learning models to enhance landslide identification and prediction. We\nleverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and\nDigital Elevation Model (DEM) layers to capture critical environmental features\ninfluencing landslide occurrences. Various geospatial analysis techniques are\nemployed to assess the impact of terra in characteristics, vegetation cover,\nand rainfall on detection accuracy. Additionally, we evaluate the performance\nof multiple stateof-the-art deep learning segmentation models, including U-Net,\nDeepLabV3+, and Res-Net, to determine their effectiveness in landslide\ndetection. The proposed framework contributes to the development of reliable\nearly warning systems, improved disaster risk management, and sustainable\nland-use planning. Our findings provide valuable insights into the potential of\ndeep learning and multi-source remote sensing in creating robust, scalable, and\ntransferable landslide prediction models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u591a\u6e90\u536b\u661f\u5f71\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u65b9\u6cd5\u6765\u63d0\u9ad8\u6ed1\u5761\u8bc6\u522b\u548c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u6ed1\u5761\u5bf9\u57fa\u7840\u8bbe\u65bd\u3001\u7ecf\u6d4e\u548c\u4eba\u7c7b\u751f\u547d\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u51c6\u786e\u68c0\u6d4b\u548c\u9884\u6d4b\u3002", "method": "\u7ed3\u5408Sentinel-2\u591a\u5149\u8c31\u6570\u636e\u548cALOS PALSAR\u884d\u751f\u7684\u5761\u5ea6\u53caDEM\u5c42\uff0c\u91c7\u7528\u591a\u79cd\u5730\u7406\u7a7a\u95f4\u5206\u6790\u6280\u672f\u548c\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\uff08\u5982U-Net\u3001DeepLabV3+\u548cRes-Net\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3001\u6539\u8fdb\u707e\u5bb3\u98ce\u9669\u7ba1\u7406\u548c\u53ef\u6301\u7eed\u571f\u5730\u5229\u7528\u89c4\u5212\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6e90\u9065\u611f\u6280\u672f\u5728\u6784\u5efa\u7a33\u5065\u3001\u53ef\u6269\u5c55\u548c\u53ef\u8fc1\u79fb\u7684\u6ed1\u5761\u9884\u6d4b\u6a21\u578b\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.01163", "pdf": "https://arxiv.org/pdf/2507.01163", "abs": "https://arxiv.org/abs/2507.01163", "authors": ["Al\u00e1n F. Mu\u00f1oz", "Tim Treis", "Alexandr A. Kalinin", "Shatavisha Dasgupta", "Fabian Theis", "Anne E. Carpenter", "Shantanu Singh"], "title": "cp_measure: API-first feature extraction for image-based profiling workflows", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM", "I.4.7"], "comment": "10 pages, 4 figures, 4 supplementary figures. CODEML Workshop paper\n  accepted (non-archival), as a part of ICML2025 events", "summary": "Biological image analysis has traditionally focused on measuring specific\nvisual properties of interest for cells or other entities. A complementary\nparadigm gaining increasing traction is image-based profiling - quantifying\nmany distinct visual features to form comprehensive profiles which may reveal\nhidden patterns in cellular states, drug responses, and disease mechanisms.\nWhile current tools like CellProfiler can generate these feature sets, they\npose significant barriers to automated and reproducible analyses, hindering\nmachine learning workflows. Here we introduce cp_measure, a Python library that\nextracts CellProfiler's core measurement capabilities into a modular, API-first\ntool designed for programmatic feature extraction. We demonstrate that\ncp_measure features retain high fidelity with CellProfiler features while\nenabling seamless integration with the scientific Python ecosystem. Through\napplications to 3D astrocyte imaging and spatial transcriptomics, we showcase\nhow cp_measure enables reproducible, automated image-based profiling pipelines\nthat scale effectively for machine learning applications in computational\nbiology.", "AI": {"tldr": "cp_measure\u662f\u4e00\u4e2aPython\u5e93\uff0c\u5c06CellProfiler\u7684\u6838\u5fc3\u6d4b\u91cf\u529f\u80fd\u6a21\u5757\u5316\uff0c\u4fbf\u4e8e\u7a0b\u5e8f\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u548c\u53ef\u91cd\u590d\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u751f\u7269\u56fe\u50cf\u5206\u6790\u5de5\u5177\u5982CellProfiler\u5728\u81ea\u52a8\u5316\u3001\u53ef\u91cd\u590d\u6027\u548c\u673a\u5668\u5b66\u4e60\u96c6\u6210\u65b9\u9762\u5b58\u5728\u969c\u788d\u3002", "method": "\u5f00\u53d1cp_measure\u5e93\uff0c\u63d0\u53d6CellProfiler\u7684\u6838\u5fc3\u6d4b\u91cf\u529f\u80fd\uff0c\u8bbe\u8ba1\u4e3a\u6a21\u5757\u5316\u3001API\u4f18\u5148\u7684\u5de5\u5177\u3002", "result": "cp_measure\u7684\u7279\u5f81\u4e0eCellProfiler\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230Python\u751f\u6001\u7cfb\u7edf\u4e2d\u3002", "conclusion": "cp_measure\u652f\u6301\u53ef\u91cd\u590d\u3001\u81ea\u52a8\u5316\u7684\u56fe\u50cf\u5206\u6790\u6d41\u7a0b\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002"}}
{"id": "2507.01182", "pdf": "https://arxiv.org/pdf/2507.01182", "abs": "https://arxiv.org/abs/2507.01182", "authors": ["Zhuo Su", "Li Liu", "Matthias M\u00fcller", "Jiehua Zhang", "Diana Wofk", "Ming-Ming Cheng", "Matti Pietik\u00e4inen"], "title": "Rapid Salient Object Detection with Difference Convolutional Neural Networks", "categories": ["cs.CV"], "comment": "16 pages, accepted in TPAMI", "summary": "This paper addresses the challenge of deploying salient object detection\n(SOD) on resource-constrained devices with real-time performance. While recent\nadvances in deep neural networks have improved SOD, existing top-leading models\nare computationally expensive. We propose an efficient network design that\ncombines traditional wisdom on SOD and the representation power of modern CNNs.\nLike biologically-inspired classical SOD methods relying on computing contrast\ncues to determine saliency of image regions, our model leverages Pixel\nDifference Convolutions (PDCs) to encode the feature contrasts. Differently,\nPDCs are incorporated in a CNN architecture so that the valuable contrast cues\nare extracted from rich feature maps. For efficiency, we introduce a difference\nconvolution reparameterization (DCR) strategy that embeds PDCs into standard\nconvolutions, eliminating computation and parameters at inference.\nAdditionally, we introduce SpatioTemporal Difference Convolution (STDC) for\nvideo SOD, enhancing the standard 3D convolution with spatiotemporal contrast\ncapture. Our models, SDNet for image SOD and STDNet for video SOD, achieve\nsignificant improvements in efficiency-accuracy trade-offs. On a Jetson Orin\ndevice, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on\nstreamed images and videos, surpassing the second-best lightweight models in\nour experiments by more than $2\\times$ and $3\\times$ in speed with superior\naccuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7f51\u7edc\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4f20\u7edf\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff08SOD\uff09\u4e0e\u73b0\u4ee3CNN\uff0c\u901a\u8fc7\u50cf\u7d20\u5dee\u5f02\u5377\u79ef\uff08PDCs\uff09\u548c\u5dee\u5f02\u5377\u79ef\u91cd\u53c2\u6570\u5316\uff08DCR\uff09\u7b56\u7565\u63d0\u5347\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SOD\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u4f20\u7edfSOD\u7684\u5bf9\u6bd4\u7ebf\u7d22\u4e0e\u73b0\u4ee3CNN\uff0c\u63d0\u51faPDCs\u548cDCR\u7b56\u7565\uff0c\u5e76\u6269\u5c55\u81f3\u89c6\u9891SOD\uff08STDC\uff09\u3002", "result": "\u6a21\u578bSDNet\u548cSTDNet\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5728Jetson Orin\u8bbe\u5907\u4e0a\u5206\u522b\u8fbe\u523046 FPS\u548c150 FPS\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6SOD\uff0c\u901f\u5ea6\u548c\u7cbe\u5ea6\u5747\u4f18\u4e8e\u73b0\u6709\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002"}}
{"id": "2507.01254", "pdf": "https://arxiv.org/pdf/2507.01254", "abs": "https://arxiv.org/abs/2507.01254", "authors": ["Runze Cheng", "Xihang Qiu", "Ming Li", "Ye Zhang", "Chun Li", "Fei Yu"], "title": "Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using H\u00f6lder Divergence and Mutual Information-Enhanced Knowledge Transfer", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal MRI provides critical complementary information for accurate brain\ntumor segmentation. However, conventional methods struggle when certain\nmodalities are missing due to issues such as image quality, protocol\ninconsistencies, patient allergies, or financial constraints. To address this,\nwe propose a robust single-modality parallel processing framework that achieves\nhigh segmentation accuracy even with incomplete modalities. Leveraging Holder\ndivergence and mutual information, our model maintains modality-specific\nfeatures while dynamically adjusting network parameters based on the available\ninputs. By using these divergence- and information-based loss functions, the\nframework effectively quantifies discrepancies between predictions and\nground-truth labels, resulting in consistently accurate segmentation. Extensive\nevaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior\nperformance over existing methods in handling missing modalities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u6a21\u6001\u5e76\u884c\u5904\u7406\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001MRI\u6570\u636e\u7f3a\u5931\u65f6\u7684\u8111\u80bf\u7624\u5206\u5272\u95ee\u9898\uff0c\u901a\u8fc7Holder\u6563\u5ea6\u548c\u4e92\u4fe1\u606f\u4fdd\u6301\u6a21\u6001\u7279\u5f81\uff0c\u5e76\u5728BraTS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u591a\u6a21\u6001MRI\u6570\u636e\u5728\u8111\u80bf\u7624\u5206\u5272\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u56e0\u5404\u79cd\u539f\u56e0\u7f3a\u5931\u67d0\u4e9b\u6a21\u6001\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u91c7\u7528\u5355\u6a21\u6001\u5e76\u884c\u5904\u7406\u6846\u67b6\uff0c\u7ed3\u5408Holder\u6563\u5ea6\u548c\u4e92\u4fe1\u606f\uff0c\u52a8\u6001\u8c03\u6574\u7f51\u7edc\u53c2\u6570\u4ee5\u5904\u7406\u7f3a\u5931\u6a21\u6001\u3002", "result": "\u5728BraTS 2018\u548c2020\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u7f3a\u5931\u6a21\u6001\u65f6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u91cf\u5316\u9884\u6d4b\u4e0e\u771f\u5b9e\u6807\u7b7e\u7684\u5dee\u5f02\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5206\u5272\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4e34\u5e8a\u573a\u666f\u3002"}}
{"id": "2507.01255", "pdf": "https://arxiv.org/pdf/2507.01255", "abs": "https://arxiv.org/abs/2507.01255", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation", "categories": ["cs.CV"], "comment": "Working in Progress", "summary": "The rapid advancement of AI-generated video models has created a pressing\nneed for robust and interpretable evaluation frameworks. Existing metrics are\nlimited to producing numerical scores without explanatory comments, resulting\nin low interpretability and human evaluation alignment. To address those\nchallenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video\nEvaluation(AIGVE), which can provide not only numerical scores but also\nmulti-aspect language comment feedback in evaluating these generated videos.\nCentral to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising\n2,500 AI-generated videos and 22,500 human-annotated detailed comments and\nnumerical scores across nine critical evaluation aspects. Leveraging\nAIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a\nnovel token-wise weighted loss and a dynamic frame sampling strategy to better\nalign with human evaluators. Comprehensive experiments across supervised and\nzero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art\nperformance in both scoring correlation and comment quality, significantly\noutperforming prior baselines including GPT-4o and VideoScore. In addition, we\nfurther showcase a multi-agent refinement framework where feedback from\nAIGVE-MACS drives iterative improvements in video generation, leading to 53.5%\nquality enhancement. This work establishes a new paradigm for comprehensive,\nhuman-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2\nand AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAIGVE-MACS\uff0c\u4e00\u79cd\u4e3aAI\u751f\u6210\u89c6\u9891\u63d0\u4f9b\u6570\u503c\u8bc4\u5206\u548c\u591a\u65b9\u9762\u8bed\u8a00\u53cd\u9988\u7684\u7edf\u4e00\u8bc4\u4f30\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u89c6\u9891\u8bc4\u4f30\u6307\u6807\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u5bf9\u9f50\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5229\u7528AIGVE-BENCH 2\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u52a0\u6743\u635f\u5931\u548c\u52a8\u6001\u5e27\u91c7\u6837\u7b56\u7565\uff0c\u5f00\u53d1AIGVE-MACS\u6a21\u578b\u3002", "result": "AIGVE-MACS\u5728\u8bc4\u5206\u76f8\u5173\u6027\u548c\u8bc4\u8bba\u8d28\u91cf\u4e0a\u5747\u8fbe\u5230\u6700\u4f18\uff0c\u5e76\u901a\u8fc7\u591a\u4ee3\u7406\u6846\u67b6\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf53.5%\u3002", "conclusion": "AIGVE-MACS\u4e3aAI\u751f\u6210\u89c6\u9891\u7684\u5168\u9762\u8bc4\u4f30\u8bbe\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002"}}
{"id": "2507.01269", "pdf": "https://arxiv.org/pdf/2507.01269", "abs": "https://arxiv.org/abs/2507.01269", "authors": ["Mohammad Jahanbakht", "Alex Olsen", "Ross Marchant", "Emilie Fillols", "Mostafa Rahimi Azghadi"], "title": "Advancements in Weed Mapping: A Systematic Review", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Weed mapping plays a critical role in precision management by providing\naccurate and timely data on weed distribution, enabling targeted control and\nreduced herbicide use. This minimizes environmental impacts, supports\nsustainable land management, and improves outcomes across agricultural and\nnatural environments. Recent advances in weed mapping leverage ground-vehicle\nRed Green Blue (RGB) cameras, satellite and drone-based remote sensing combined\nwith sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The\nresulting data are processed using advanced techniques including big data\nanalytics and machine learning, significantly improving the spatial and\ntemporal resolution of weed maps and enabling site-specific management\ndecisions. Despite a growing body of research in this domain, there is a lack\nof comprehensive literature reviews specifically focused on weed mapping. In\nparticular, the absence of a structured analysis spanning the entire mapping\npipeline, from data acquisition to processing techniques and mapping tools,\nlimits progress in the field. This review addresses these gaps by\nsystematically examining state-of-the-art methods in data acquisition (sensor\nand platform technologies), data processing (including annotation and\nmodelling), and mapping techniques (such as spatiotemporal analysis and\ndecision support tools). Following PRISMA guidelines, we critically evaluate\nand synthesize key findings from the literature to provide a holistic\nunderstanding of the weed mapping landscape. This review serves as a\nfoundational reference to guide future research and support the development of\nefficient, scalable, and sustainable weed management systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u6742\u8349\u6d4b\u7ed8\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u586b\u8865\u4e86\u4ece\u6570\u636e\u91c7\u96c6\u5230\u5904\u7406\u6280\u672f\u7684\u5168\u9762\u6587\u732e\u7a7a\u767d\uff0c\u65e8\u5728\u63a8\u52a8\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u7684\u6742\u8349\u7ba1\u7406\u7cfb\u7edf\u53d1\u5c55\u3002", "motivation": "\u6742\u8349\u6d4b\u7ed8\u5bf9\u7cbe\u51c6\u7ba1\u7406\u548c\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u6587\u732e\u7efc\u8ff0\u548c\u5206\u6790\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u91c7\u7528PRISMA\u6307\u5357\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u6570\u636e\u91c7\u96c6\uff08\u4f20\u611f\u5668\u4e0e\u5e73\u53f0\u6280\u672f\uff09\u3001\u6570\u636e\u5904\u7406\uff08\u6807\u6ce8\u4e0e\u5efa\u6a21\uff09\u548c\u6d4b\u7ed8\u6280\u672f\uff08\u65f6\u7a7a\u5206\u6790\u4e0e\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff09\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "result": "\u7efc\u8ff0\u63d0\u4f9b\u4e86\u6742\u8349\u6d4b\u7ed8\u9886\u57df\u7684\u5168\u9762\u7406\u89e3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u53ef\u6301\u7eed\u6742\u8349\u7ba1\u7406\u7cfb\u7edf\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u8bba\u6587\u586b\u8865\u4e86\u6742\u8349\u6d4b\u7ed8\u9886\u57df\u7684\u6587\u732e\u7a7a\u767d\uff0c\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6742\u8349\u7ba1\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2507.01275", "pdf": "https://arxiv.org/pdf/2507.01275", "abs": "https://arxiv.org/abs/2507.01275", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Unpaired image dehazing has attracted increasing attention due to its\nflexible data requirements during model training. Dominant methods based on\ncontrastive learning not only introduce haze-unrelated content information, but\nalso ignore haze-specific properties in the frequency domain (\\ie,~haze-related\ndegradation is mainly manifested in the amplitude spectrum). To address these\nissues, we propose a novel frequency domain-based diffusion model, named \\ours,\nfor fully exploiting the beneficial knowledge in unpaired clear data. In\nparticular, inspired by the strong generative ability shown by Diffusion Models\n(DMs), we tackle the dehazing task from the perspective of frequency domain\nreconstruction and perform the DMs to yield the amplitude spectrum consistent\nwith the distribution of clear images. To implement it, we propose an Amplitude\nResidual Encoder (ARE) to extract the amplitude residuals, which effectively\ncompensates for the amplitude gap from the hazy to clear domains, as well as\nprovide supervision for the DMs training. In addition, we propose a Phase\nCorrection Module (PCM) to eliminate artifacts by further refining the phase\nspectrum during dehazing with a simple attention mechanism. Experimental\nresults demonstrate that our \\ours outperforms other state-of-the-art methods\non both synthetic and real-world datasets.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.01290", "pdf": "https://arxiv.org/pdf/2507.01290", "abs": "https://arxiv.org/abs/2507.01290", "authors": ["Sunyong Seo", "Semin Kim", "Jongha Lee"], "title": "Learning an Ensemble Token from Task-driven Priors in Facial Analysis", "categories": ["cs.CV"], "comment": "11pages, 8figures, 4tables", "summary": "Facial analysis exhibits task-specific feature variations. While\nConvolutional Neural Networks (CNNs) have enabled the fine-grained\nrepresentation of spatial information, Vision Transformers (ViTs) have\nfacilitated the representation of semantic information at the patch level.\nAlthough the generalization of conventional methodologies has advanced visual\ninterpretability, there remains paucity of research that preserves the unified\nfeature representation on single task learning during the training process. In\nthis work, we introduce ET-Fuser, a novel methodology for learning ensemble\ntoken by leveraging attention mechanisms based on task priors derived from\npre-trained models for facial analysis. Specifically, we propose a robust prior\nunification learning method that generates a ensemble token within a\nself-attention mechanism, which shares the mutual information along the\npre-trained encoders. This ensemble token approach offers high efficiency with\nnegligible computational cost. Our results show improvements across a variety\nof facial analysis, with statistically significant enhancements observed in the\nfeature representations.", "AI": {"tldr": "ET-Fuser\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u4efb\u52a1\u5148\u9a8c\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b66\u4e60\u96c6\u6210\u4ee4\u724c\uff0c\u4ee5\u6539\u8fdb\u9762\u90e8\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u7279\u5f81\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u4efb\u52a1\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u7684\u7279\u5f81\u8868\u793a\uff0cET-Fuser\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u96c6\u6210\u4ee4\u724c\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u5171\u4eab\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cd\u9762\u90e8\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u7279\u5f81\u8868\u793a\u6539\u8fdb\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\u3002", "conclusion": "ET-Fuser\u901a\u8fc7\u96c6\u6210\u4ee4\u724c\u65b9\u6cd5\u9ad8\u6548\u5730\u63d0\u5347\u4e86\u9762\u90e8\u5206\u6790\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01305", "pdf": "https://arxiv.org/pdf/2507.01305", "abs": "https://arxiv.org/abs/2507.01305", "authors": ["Worameth Chinchuthakun", "Pakkapon Phongthawee", "Amit Raj", "Varun Jampani", "Pramook Khungurn", "Supasorn Suwajanakorn"], "title": "DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting", "categories": ["cs.CV", "cs.GR", "cs.LG", "I.3.3; I.4.8"], "comment": "arXiv admin note: substantial text overlap with arXiv:2312.09168", "summary": "We introduce a simple yet effective technique for estimating lighting from a\nsingle low-dynamic-range (LDR) image by reframing the task as a chrome ball\ninpainting problem. This approach leverages a pre-trained diffusion model,\nStable Diffusion XL, to overcome the generalization failures of existing\nmethods that rely on limited HDR panorama datasets. While conceptually simple,\nthe task remains challenging because diffusion models often insert incorrect or\ninconsistent content and cannot readily generate chrome balls in HDR format.\nOur analysis reveals that the inpainting process is highly sensitive to the\ninitial noise in the diffusion process, occasionally resulting in unrealistic\noutputs. To address this, we first introduce DiffusionLight, which uses\niterative inpainting to compute a median chrome ball from multiple outputs to\nserve as a stable, low-frequency lighting prior that guides the generation of a\nhigh-quality final result. To generate high-dynamic-range (HDR) light probes,\nan Exposure LoRA is fine-tuned to create LDR images at multiple exposure\nvalues, which are then merged. While effective, DiffusionLight is\ntime-intensive, requiring approximately 30 minutes per estimation. To reduce\nthis overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to\nabout 30 seconds with minimal quality loss. This 60x speedup is achieved by\ntraining a Turbo LoRA to directly predict the averaged chrome balls from the\niterative process. Inference is further streamlined into a single denoising\npass using a LoRA swapping technique. Experimental results that show our method\nproduces convincing light estimates across diverse settings and demonstrates\nsuperior generalization to in-the-wild scenarios. Our code is available at\nhttps://diffusionlight.github.io/turbo", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u94ec\u7403\u4fee\u590d\u95ee\u9898\uff0c\u4ece\u5355\u5f20\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u56fe\u50cf\u4f30\u8ba1\u5149\u7167\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\u3002\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578bStable Diffusion XL\uff0c\u514b\u670d\u4e86\u4f9d\u8d56\u6709\u9650HDR\u5168\u666f\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u7684\u6cdb\u5316\u95ee\u9898\u3002\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u751f\u6210\u7a33\u5b9a\u7684\u4f4e\u9891\u5149\u7167\u5148\u9a8c\uff0c\u5e76\u7ed3\u5408\u66dd\u5149LoRA\u751f\u6210HDR\u5149\u63a2\u9488\u3002\u8fdb\u4e00\u6b65\u4f18\u5316\u4e3aDiffusionLight-Turbo\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684HDR\u5168\u666f\u6570\u636e\u96c6\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u6269\u6563\u6a21\u578b\u5728\u751f\u6210HDR\u683c\u5f0f\u7684\u94ec\u7403\u65f6\u5b58\u5728\u56f0\u96be\u3002", "method": "\u4f7f\u7528Stable Diffusion XL\u8fdb\u884c\u94ec\u7403\u4fee\u590d\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u751f\u6210\u7a33\u5b9a\u7684\u5149\u7167\u5148\u9a8c\uff1b\u7ed3\u5408\u66dd\u5149LoRA\u751f\u6210HDR\u5149\u63a2\u9488\uff1b\u4f18\u5316\u4e3aDiffusionLight-Turbo\uff0c\u901a\u8fc7LoRA\u4ea4\u6362\u6280\u672f\u52a0\u901f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5149\u7167\u4f30\u8ba1\uff0c\u5e76\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiffusionLight\u53ca\u5176\u4f18\u5316\u7248\u672cTurbo\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.01340", "pdf": "https://arxiv.org/pdf/2507.01340", "abs": "https://arxiv.org/abs/2507.01340", "authors": ["Cuong Le", "Huy-Phuong Le", "Duc Le", "Minh-Thien Duong", "Van-Binh Nguyen", "My-Ha Le"], "title": "Physics-informed Ground Reaction Dynamics from Human Motion Capture", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, 4 tables, HSI 2025", "summary": "Body dynamics are crucial information for the analysis of human motions in\nimportant research fields, ranging from biomechanics, sports science to\ncomputer vision and graphics. Modern approaches collect the body dynamics,\nexternal reactive force specifically, via force plates, synchronizing with\nhuman motion capture data, and learn to estimate the dynamics from a black-box\ndeep learning model. Being specialized devices, force plates can only be\ninstalled in laboratory setups, imposing a significant limitation on the\nlearning of human dynamics. To this end, we propose a novel method for\nestimating human ground reaction dynamics directly from the more reliable\nmotion capture data with physics laws and computational simulation as\nconstrains. We introduce a highly accurate and robust method for computing\nground reaction forces from motion capture data using Euler's integration\nscheme and PD algorithm. The physics-based reactive forces are used to inform\nthe learning model about the physics-informed motion dynamics thus improving\nthe estimation accuracy. The proposed approach was tested on the GroundLink\ndataset, outperforming the baseline model on: 1) the ground reaction force\nestimation accuracy compared to the force plates measurement; and 2) our\nsimulated root trajectory precision. The implementation code is available at\nhttps://github.com/cuongle1206/Phys-GRD", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u8fd0\u52a8\u6355\u6349\u6570\u636e\u4f30\u8ba1\u5730\u9762\u53cd\u4f5c\u7528\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u9ed1\u76d2\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5b9e\u9a8c\u5ba4\u4e13\u7528\u8bbe\u5907\uff08\u5982\u529b\u677f\uff09\u83b7\u53d6\u5730\u9762\u53cd\u4f5c\u7528\u529b\uff0c\u9650\u5236\u4e86\u52a8\u6001\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u7ed3\u5408\u6b27\u62c9\u79ef\u5206\u65b9\u6848\u548cPD\u7b97\u6cd5\uff0c\u5229\u7528\u7269\u7406\u5b9a\u5f8b\u548c\u8ba1\u7b97\u6a21\u62df\u7ea6\u675f\uff0c\u4ece\u8fd0\u52a8\u6355\u6349\u6570\u636e\u8ba1\u7b97\u5730\u9762\u53cd\u4f5c\u7528\u529b\u3002", "result": "\u5728GroundLink\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u5730\u9762\u53cd\u4f5c\u7528\u529b\u4f30\u8ba1\u7cbe\u5ea6\u548c\u6a21\u62df\u6839\u8f68\u8ff9\u7cbe\u5ea6\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7269\u7406\u7ea6\u675f\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5730\u9762\u53cd\u4f5c\u7528\u529b\u7684\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u6269\u5c55\u4e86\u52a8\u6001\u5b66\u4e60\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.01342", "pdf": "https://arxiv.org/pdf/2507.01342", "abs": "https://arxiv.org/abs/2507.01342", "authors": ["Luxi Zhao", "Mahmoud Afifi", "Michael S. Brown"], "title": "Learning Camera-Agnostic White-Balance Preferences", "categories": ["cs.CV"], "comment": null, "summary": "The image signal processor (ISP) pipeline in modern cameras consists of\nseveral modules that transform raw sensor data into visually pleasing images in\na display color space. Among these, the auto white balance (AWB) module is\nessential for compensating for scene illumination. However, commercial AWB\nsystems often strive to compute aesthetic white-balance preferences rather than\naccurate neutral color correction. While learning-based methods have improved\nAWB accuracy, they typically struggle to generalize across different camera\nsensors -- an issue for smartphones with multiple cameras. Recent work has\nexplored cross-camera AWB, but most methods remain focused on achieving neutral\nwhite balance. In contrast, this paper is the first to address aesthetic\nconsistency by learning a post-illuminant-estimation mapping that transforms\nneutral illuminant corrections into aesthetically preferred corrections in a\ncamera-agnostic space. Once trained, our mapping can be applied after any\nneutral AWB module to enable consistent and stylized color rendering across\nunseen cameras. Our proposed model is lightweight -- containing only $\\sim$500\nparameters -- and runs in just 0.024 milliseconds on a typical flagship mobile\nCPU. Evaluated on a dataset of 771 smartphone images from three different\ncameras, our method achieves state-of-the-art performance while remaining fully\ncompatible with existing cross-camera AWB techniques, introducing minimal\ncomputational and memory overhead.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u540e\u5149\u7167\u4f30\u8ba1\u6620\u5c04\uff0c\u5c06\u4e2d\u6027\u767d\u5e73\u8861\u6821\u6b63\u8f6c\u6362\u4e3a\u7f8e\u5b66\u504f\u597d\u7684\u6821\u6b63\uff0c\u5b9e\u73b0\u8de8\u76f8\u673a\u7f8e\u5b66\u4e00\u81f4\u6027\u3002", "motivation": "\u5546\u4e1a\u81ea\u52a8\u767d\u5e73\u8861\uff08AWB\uff09\u7cfb\u7edf\u901a\u5e38\u8ffd\u6c42\u7f8e\u5b66\u504f\u597d\u800c\u975e\u51c6\u786e\u4e2d\u6027\u6821\u6b63\uff0c\u4e14\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u76f8\u673a\u4f20\u611f\u5668\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7f8e\u5b66\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540e\u5149\u7167\u4f30\u8ba1\u6620\u5c04\uff0c\u5c06\u4e2d\u6027\u767d\u5e73\u8861\u6821\u6b63\u8f6c\u6362\u4e3a\u7f8e\u5b66\u504f\u597d\u7684\u6821\u6b63\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u4e2d\u6027AWB\u6a21\u5757\uff0c\u4e14\u6a21\u578b\u8f7b\u91cf\uff08\u7ea6500\u53c2\u6570\uff09\u3002", "result": "\u5728771\u5f20\u667a\u80fd\u624b\u673a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u8ba1\u7b97\u901f\u5ea6\u5feb\uff080.024\u6beb\u79d2\uff09\uff0c\u517c\u5bb9\u73b0\u6709\u8de8\u76f8\u673aAWB\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u8de8\u76f8\u673a\u7f8e\u5b66\u4e00\u81f4\u6027\uff0c\u8f7b\u91cf\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u76f8\u673a\u4f20\u611f\u5668\u3002"}}
{"id": "2507.01347", "pdf": "https://arxiv.org/pdf/2507.01347", "abs": "https://arxiv.org/abs/2507.01347", "authors": ["Andrei Jelea", "Ahmed Nabil Belbachir", "Marius Leordeanu"], "title": "Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Generalized Test-Time Augmentation (GTTA), a highly effective\nmethod for improving the performance of a trained model, which unlike other\nexisting Test-Time Augmentation approaches from the literature is general\nenough to be used off-the-shelf for many vision and non-vision tasks, such as\nclassification, regression, image segmentation and object detection. By\napplying a new general data transformation, that randomly perturbs multiple\ntimes the PCA subspace projection of a test input, GTTA forms robust ensembles\nat test time in which, due to sound statistical properties, the structural and\nsystematic noises in the initial input data is filtered out and final estimator\nerrors are reduced. Different from other existing methods, we also propose a\nfinal self-supervised learning stage in which the ensemble output, acting as an\nunsupervised teacher, is used to train the initial single student model, thus\nreducing significantly the test time computational cost, at no loss in\naccuracy. Our tests and comparisons to strong TTA approaches and SoTA models on\nvarious vision and non-vision well-known datasets and tasks, such as image\nclassification and segmentation, speech recognition and house price prediction,\nvalidate the generality of the proposed GTTA. Furthermore, we also prove its\neffectiveness on the more specific real-world task of salmon segmentation and\ndetection in low-visibility underwater videos, for which we introduce\nDeepSalmon, the largest dataset of its kind in the literature.", "AI": {"tldr": "GTTA\u662f\u4e00\u79cd\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u548c\u975e\u89c6\u89c9\u4efb\u52a1\uff0c\u901a\u8fc7\u6270\u52a8PCA\u5b50\u7a7a\u95f4\u6295\u5f71\u5f62\u6210\u9c81\u68d2\u96c6\u6210\uff0c\u5e76\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0cGTTA\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "method": "GTTA\u901a\u8fc7\u968f\u673a\u6270\u52a8PCA\u5b50\u7a7a\u95f4\u6295\u5f71\u5f62\u6210\u96c6\u6210\uff0c\u5e76\u5f15\u5165\u81ea\u76d1\u7763\u5b66\u4e60\u9636\u6bb5\uff0c\u5229\u7528\u96c6\u6210\u8f93\u51fa\u8bad\u7ec3\u521d\u59cb\u6a21\u578b\u3002", "result": "GTTA\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u4f4e\u80fd\u89c1\u5ea6\u6c34\u4e0b\u89c6\u9891\u4e2d\u7684\u9c91\u9c7c\u5206\u5272\uff09\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "GTTA\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.01351", "pdf": "https://arxiv.org/pdf/2507.01351", "abs": "https://arxiv.org/abs/2507.01351", "authors": ["Chaoxiang Cai", "Longrong Yang", "Kaibing Chen", "Fan Yang", "Xi Li"], "title": "Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "The mixture-of-experts (MoE), which replaces dense models with sparse\narchitectures, has gained attention in large vision-language models (LVLMs) for\nachieving comparable performance with fewer activated parameters. Existing MoE\nframeworks for LVLMs focus on token-to-expert routing (TER), encouraging\ndifferent experts to specialize in processing distinct tokens. However, these\nframeworks often rely on the load balancing mechanism, overlooking the inherent\ndistributional differences between vision and language. To this end, we propose\na Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,\ntackling two challenges: (1) Distribution-aware router for modality-specific\nrouting. We observe that language TER follows a uniform distribution, whereas\nvision TER exhibits a long-tailed distribution. This discrepancy necessitates\ndistinct routing strategies tailored to each modality. (2) Enhancing expert\nactivation for vision tail tokens. Recognizing the importance of vision tail\ntokens, we introduce an oversampling-like strategy by increasing the number of\nactivated experts for these tokens. Experiments on extensive benchmarks\nvalidate the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u957f\u5c3e\u5206\u5e03\u611f\u77e5\u8def\u7531\u5668\uff08LTDR\uff09\uff0c\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u4e2d\u7684\u4ee4\u724c\u5230\u4e13\u5bb6\u8def\u7531\uff08TER\uff09\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u95f4\u5206\u5e03\u5dee\u5f02\u548c\u89c6\u89c9\u5c3e\u90e8\u4ee4\u724c\u6fc0\u6d3b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MoE\u6846\u67b6\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u5ffd\u89c6\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u5bfc\u81f4\u8def\u7531\u7b56\u7565\u4e0d\u5339\u914d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u8def\u7531\u548c\u589e\u5f3a\u5c3e\u90e8\u4ee4\u724c\u6fc0\u6d3b\u6765\u4f18\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51faLTDR\uff0c\u5305\u62ec\uff081\uff09\u9488\u5bf9\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u6001\u7684\u4e0d\u540c\u5206\u5e03\u8bbe\u8ba1\u8def\u7531\u7b56\u7565\uff1b\uff082\uff09\u901a\u8fc7\u7c7b\u4f3c\u8fc7\u91c7\u6837\u7684\u65b9\u6cd5\u589e\u52a0\u89c6\u89c9\u5c3e\u90e8\u4ee4\u724c\u7684\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86LTDR\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LTDR\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u8def\u7531\u548c\u5c3e\u90e8\u4ee4\u724c\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00MoE\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01367", "pdf": "https://arxiv.org/pdf/2507.01367", "abs": "https://arxiv.org/abs/2507.01367", "authors": ["Tianrui Lou", "Xiaojun Jia", "Siyuan Liang", "Jiawei Liang", "Ming Zhang", "Yanjun Xiao", "Xiaochun Cao"], "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Physical adversarial attack methods expose the vulnerabilities of deep neural\nnetworks and pose a significant threat to safety-critical scenarios such as\nautonomous driving. Camouflage-based physical attack is a more promising\napproach compared to the patch-based attack, offering stronger adversarial\neffectiveness in complex physical environments. However, most prior work relies\non mesh priors of the target object and virtual environments constructed by\nsimulators, which are time-consuming to obtain and inevitably differ from the\nreal world. Moreover, due to the limitations of the backgrounds in training\nimages, previous methods often fail to produce multi-view robust adversarial\ncamouflage and tend to fall into sub-optimal solutions. Due to these reasons,\nprior work lacks adversarial effectiveness and robustness across diverse\nviewpoints and physical environments. We propose a physical attack framework\nbased on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and\nprecise reconstruction with few images, along with photo-realistic rendering\ncapabilities. Our framework further enhances cross-view robustness and\nadversarial effectiveness by preventing mutual and self-occlusion among\nGaussians and employing a min-max optimization approach that adjusts the\nimaging background of each viewpoint, helping the algorithm filter out\nnon-robust adversarial features. Extensive experiments validate the\neffectiveness and superiority of PGA. Our code is available\nat:https://github.com/TRLou/PGA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6563\u5c04\u7684\u7269\u7406\u653b\u51fb\u6846\u67b6PGA\uff0c\u901a\u8fc7\u5feb\u901f\u91cd\u5efa\u548c\u903c\u771f\u6e32\u67d3\uff0c\u589e\u5f3a\u4e86\u8de8\u89c6\u89d2\u9c81\u68d2\u6027\u548c\u5bf9\u6297\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4f2a\u88c5\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u76ee\u6807\u5bf9\u8c61\u7684\u7f51\u683c\u5148\u9a8c\u548c\u6a21\u62df\u73af\u5883\uff0c\u8017\u65f6\u4e14\u4e0e\u73b0\u5b9e\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u7f3a\u4e4f\u591a\u89c6\u89d2\u9c81\u68d2\u6027\u3002", "method": "\u5229\u75283D\u9ad8\u65af\u6563\u5c04\u6280\u672f\u5feb\u901f\u91cd\u5efa\u76ee\u6807\uff0c\u907f\u514d\u9ad8\u65af\u95f4\u7684\u906e\u6321\uff0c\u5e76\u901a\u8fc7min-max\u4f18\u5316\u8c03\u6574\u80cc\u666f\uff0c\u8fc7\u6ee4\u975e\u9c81\u68d2\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PGA\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "PGA\u5728\u7269\u7406\u653b\u51fb\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5bf9\u6297\u6548\u679c\u548c\u8de8\u89c6\u89d2\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.01368", "pdf": "https://arxiv.org/pdf/2507.01368", "abs": "https://arxiv.org/abs/2507.01368", "authors": ["Tianning Chai", "Chancharik Mitra", "Brandon Huang", "Gautam Rajendrakumar Gare", "Zhiqiu Lin", "Assaf Arbelle", "Leonid Karlinsky", "Rogerio Feris", "Trevor Darrell", "Deva Ramanan", "Roei Herzig"], "title": "Activation Reward Models for Few-Shot Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c11\u6837\u672c\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u2014\u2014\u6fc0\u6d3b\u5956\u52b1\u6a21\u578b\uff08Activation RMs\uff09\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5bfc\u5411\u6784\u5efa\u5bf9\u9f50\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u7f13\u89e3\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5230\u4eba\u7c7b\u504f\u597d\u662f\u63d0\u5347\u751f\u6210\u8f93\u51fa\u8d28\u91cf\u7684\u5173\u952e\uff0c\u4f20\u7edf\u5956\u52b1\u5efa\u6a21\u96be\u4ee5\u9002\u5e94\u65b0\u504f\u597d\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5229\u7528\u6fc0\u6d3b\u5bfc\u5411\u6280\u672f\u6784\u5efa\u5956\u52b1\u4fe1\u53f7\uff0c\u4ec5\u9700\u5c11\u91cf\u76d1\u7763\u4e14\u65e0\u9700\u989d\u5916\u6a21\u578b\u5fae\u8c03\uff0c\u63d0\u51faPreferenceHack\u57fa\u51c6\u6d4b\u8bd5\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u3002", "result": "Activation RMs\u5728\u6807\u51c6\u5956\u52b1\u5efa\u6a21\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u5e76\u5728PreferenceHack\u57fa\u51c6\u4e0a\u8d85\u8d8aGPT-4o\u3002", "conclusion": "Activation RMs\u4e3a\u5c11\u6837\u672c\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2507.01372", "pdf": "https://arxiv.org/pdf/2507.01372", "abs": "https://arxiv.org/abs/2507.01372", "authors": ["Max Hamilton", "Jinlin Lai", "Wenlong Zhao", "Subhransu Maji", "Daniel Sheldon"], "title": "Active Measurement: Efficient Estimation at Scale", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "AI has the potential to transform scientific discovery by analyzing vast\ndatasets with little human effort. However, current workflows often do not\nprovide the accuracy or statistical guarantees that are needed. We introduce\nactive measurement, a human-in-the-loop AI framework for scientific\nmeasurement. An AI model is used to predict measurements for individual units,\nwhich are then sampled for human labeling using importance sampling. With each\nnew set of human labels, the AI model is improved and an unbiased Monte Carlo\nestimate of the total measurement is refined. Active measurement can provide\nprecise estimates even with an imperfect AI model, and requires little human\neffort when the AI model is very accurate. We derive novel estimators,\nweighting schemes, and confidence intervals, and show that active measurement\nreduces estimation error compared to alternatives in several measurement tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4e3b\u52a8\u6d4b\u91cf\u201d\u7684\u4eba\u673a\u534f\u4f5cAI\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u548c\u8fed\u4ee3\u4f18\u5316\u6a21\u578b\uff0c\u63d0\u9ad8\u79d1\u5b66\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u548c\u7edf\u8ba1\u4fdd\u969c\u3002", "motivation": "\u5f53\u524dAI\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u7f3a\u4e4f\u8db3\u591f\u7684\u51c6\u786e\u6027\u548c\u7edf\u8ba1\u4fdd\u969c\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528AI\u6a21\u578b\u9884\u6d4b\u6d4b\u91cf\u503c\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u9009\u62e9\u6837\u672c\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\uff0c\u8fed\u4ee3\u4f18\u5316\u6a21\u578b\u5e76\u751f\u6210\u65e0\u504f\u4f30\u8ba1\u3002", "result": "\u4e3b\u52a8\u6d4b\u91cf\u6846\u67b6\u5728\u591a\u4e2a\u6d4b\u91cf\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u4f30\u8ba1\u8bef\u5dee\uff0c\u4e14\u5bf9AI\u6a21\u578b\u7684\u521d\u59cb\u7cbe\u5ea6\u8981\u6c42\u8f83\u4f4e\u3002", "conclusion": "\u4e3b\u52a8\u6d4b\u91cf\u4e3a\u79d1\u5b66\u6d4b\u91cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u4eba\u673a\u534f\u4f5c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01384", "pdf": "https://arxiv.org/pdf/2507.01384", "abs": "https://arxiv.org/abs/2507.01384", "authors": ["Langyu Wang", "Bingke Zhu", "Yingying Chen", "Yiyuan Zhang", "Ming Tang", "Jinqiao Wang"], "title": "MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing", "categories": ["cs.CV"], "comment": "Accpted by ICCV 2025", "summary": "The weakly-supervised audio-visual video parsing (AVVP) aims to predict all\nmodality-specific events and locate their temporal boundaries. Despite\nsignificant progress, due to the limitations of the weakly-supervised and the\ndeficiencies of the model architecture, existing methods are lacking in\nsimultaneously improving both the segment-level prediction and the event-level\nprediction. In this work, we propose a audio-visual Mamba network with pseudo\nlabeling aUGmentation (MUG) for emphasising the uniqueness of each segment and\nexcluding the noise interference from the alternate modalities. Specifically,\nwe annotate some of the pseudo-labels based on previous work. Using unimodal\npseudo-labels, we perform cross-modal random combinations to generate new data,\nwhich can enhance the model's ability to parse various segment-level event\ncombinations. For feature processing and interaction, we employ a audio-visual\nmamba network. The AV-Mamba enhances the ability to perceive different segments\nand excludes additional modal noise while sharing similar modal information.\nOur extensive experiments demonstrate that MUG improves state-of-the-art\nresults on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of\nvisual Segment-level and audio Segment-level metrics). Our code is available at\nhttps://github.com/WangLY136/MUG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f2a\u6807\u7b7e\u589e\u5f3a\u7684\u97f3\u9891-\u89c6\u89c9Mamba\u7f51\u7edc\uff08MUG\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u4efb\u52a1\u4e2d\u7684\u6bb5\u7ea7\u548c\u4e8b\u4ef6\u7ea7\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u548c\u6a21\u578b\u67b6\u6784\u7684\u9650\u5236\u4e0b\uff0c\u96be\u4ee5\u540c\u65f6\u63d0\u5347\u6bb5\u7ea7\u548c\u4e8b\u4ef6\u7ea7\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4f2a\u6807\u7b7e\u589e\u5f3a\u751f\u6210\u65b0\u6570\u636e\uff0c\u5e76\u91c7\u7528\u97f3\u9891-\u89c6\u89c9Mamba\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u5904\u7406\u548c\u4ea4\u4e92\u3002", "result": "\u5728LLP\u6570\u636e\u96c6\u4e0a\uff0cMUG\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5982\u89c6\u89c9\u6bb5\u7ea7\u548c\u97f3\u9891\u6bb5\u7ea7\u6307\u6807\u5206\u522b\u63d0\u53472.1%\u548c1.2%\u3002", "conclusion": "MUG\u901a\u8fc7\u4f2a\u6807\u7b7e\u589e\u5f3a\u548cMamba\u7f51\u7edc\u6709\u6548\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01390", "pdf": "https://arxiv.org/pdf/2507.01390", "abs": "https://arxiv.org/abs/2507.01390", "authors": ["Shuai Tan", "Bill Gong", "Bin Ji", "Ye Pan"], "title": "FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases", "categories": ["cs.CV"], "comment": null, "summary": "Talking head generation is gaining significant importance across various\ndomains, with a growing demand for high-quality rendering. However, existing\nmethods often suffer from identity leakage (IL) and rendering artifacts (RA),\nparticularly in extreme cases. Through an in-depth analysis of previous\napproaches, we identify two key insights: (1) IL arises from identity\ninformation embedded within motion features, and (2) this identity information\ncan be leveraged to address RA. Building on these findings, this paper\nintroduces FixTalk, a novel framework designed to simultaneously resolve both\nissues for high-quality talking head generation. Firstly, we propose an\nEnhanced Motion Indicator (EMI) to effectively decouple identity information\nfrom motion features, mitigating the impact of IL on generated talking heads.\nTo address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes\nthe leaked identity information to supplement missing details, thus fixing the\nartifacts. Extensive experiments demonstrate that FixTalk effectively mitigates\nIL and RA, achieving superior performance compared to state-of-the-art methods.", "AI": {"tldr": "FixTalk\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8eab\u4efd\u4fe1\u606f\u548c\u8fd0\u52a8\u7279\u5f81\uff0c\u540c\u65f6\u89e3\u51b3\u8eab\u4efd\u6cc4\u6f0f\u548c\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bf4\u8bdd\u5934\u751f\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u5b58\u5728\u8eab\u4efd\u6cc4\u6f0f\uff08IL\uff09\u548c\u6e32\u67d3\u4f2a\u5f71\uff08RA\uff09\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u8bf4\u8bdd\u5934\u751f\u6210\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51faFixTalk\u6846\u67b6\uff0c\u5305\u62ec\u589e\u5f3a\u8fd0\u52a8\u6307\u793a\u5668\uff08EMI\uff09\u89e3\u8026\u8eab\u4efd\u4fe1\u606f\uff0c\u4ee5\u53ca\u589e\u5f3a\u7ec6\u8282\u6307\u793a\u5668\uff08EDI\uff09\u5229\u7528\u6cc4\u6f0f\u4fe1\u606f\u4fee\u590d\u4f2a\u5f71\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFixTalk\u6709\u6548\u7f13\u89e3\u4e86IL\u548cRA\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FixTalk\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u8eab\u4efd\u6cc4\u6f0f\u548c\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u8bf4\u8bdd\u5934\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01397", "pdf": "https://arxiv.org/pdf/2507.01397", "abs": "https://arxiv.org/abs/2507.01397", "authors": ["Khanh Son Pham", "Christian Witte", "Jens Behley", "Johannes Betz", "Cyrill Stachniss"], "title": "Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IROS 2025", "summary": "Most autonomous cars rely on the availability of high-definition (HD) maps.\nCurrent research aims to address this constraint by directly predicting HD map\nelements from onboard sensors and reasoning about the relationships between the\npredicted map and traffic elements. Despite recent advancements, the coherent\nonline construction of HD maps remains a challenging endeavor, as it\nnecessitates modeling the high complexity of road topologies in a unified and\nconsistent manner. To address this challenge, we propose a coherent approach to\npredict lane segments and their corresponding topology, as well as road\nboundaries, all by leveraging prior map information represented by commonly\navailable standard-definition (SD) maps. We propose a network architecture,\nwhich leverages hybrid lane segment encodings comprising prior information and\ndenoising techniques to enhance training stability and performance.\nFurthermore, we facilitate past frames for temporal consistency. Our\nexperimental evaluation demonstrates that our approach outperforms previous\nmethods by a large margin, highlighting the benefits of our modeling scheme.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6807\u51c6\u5730\u56fe\uff08SD\uff09\u4fe1\u606f\u9884\u6d4b\u8f66\u9053\u6bb5\u3001\u62d3\u6251\u548c\u9053\u8def\u8fb9\u754c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u7f16\u7801\u548c\u53bb\u566a\u6280\u672f\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u4f9d\u8d56\u9ad8\u6e05\u5730\u56fe\uff08HD\uff09\uff0c\u4f46\u5728\u7ebf\u6784\u5efaHD\u5730\u56fe\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u7edf\u4e00\u5efa\u6a21\u590d\u6742\u9053\u8def\u62d3\u6251\u3002", "method": "\u63d0\u51fa\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528SD\u5730\u56fe\u5148\u9a8c\u4fe1\u606f\u548c\u6df7\u5408\u8f66\u9053\u6bb5\u7f16\u7801\uff0c\u7ed3\u5408\u53bb\u566a\u6280\u672f\u548c\u5386\u53f2\u5e27\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5148\u9a8c\u4fe1\u606f\u548c\u4e00\u81f4\u6027\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86HD\u5730\u56fe\u5728\u7ebf\u6784\u5efa\u7684\u6311\u6218\u3002"}}
{"id": "2507.01401", "pdf": "https://arxiv.org/pdf/2507.01401", "abs": "https://arxiv.org/abs/2507.01401", "authors": ["Huanwen Liang", "Jingxian Xu", "Yuanji Zhang", "Yuhao Huang", "Yuhan Zhang", "Xin Yang", "Ran Li", "Xuedong Deng", "Yanjun Liu", "Guowei Tao", "Yun Wu", "Sheng Zhao", "Xinru Gao", "Dong Ni"], "title": "Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by MICCAI 2025", "summary": "Fetal abdominal malformations are serious congenital anomalies that require\naccurate diagnosis to guide pregnancy management and reduce mortality. Although\nAI has demonstrated significant potential in medical diagnosis, its application\nto prenatal abdominal anomalies remains limited. Most existing studies focus on\nimage-level classification and rely on standard plane localization, placing\nless emphasis on case-level diagnosis. In this paper, we develop a case-level\nmultiple instance learning (MIL)-based method, free of standard plane\nlocalization, for classifying fetal abdominal anomalies in prenatal ultrasound.\nOur contribution is three-fold. First, we adopt a mixture-of-attention-experts\nmodule (MoAE) to weight different attention heads for various planes. Secondly,\nwe propose a medical-knowledge-driven feature selection module (MFS) to align\nimage features with medical knowledge, performing self-supervised image token\nselection at the case-level. Finally, we propose a prompt-based prototype\nlearning (PPL) to enhance the MFS. Extensively validated on a large prenatal\nabdominal ultrasound dataset containing 2,419 cases, with a total of 24,748\nimages and 6 categories, our proposed method outperforms the state-of-the-art\ncompetitors. Codes are available at:https://github.com/LL-AC/AAcls.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5b9e\u4f8b\u5b66\u4e60\u7684\u6848\u4f8b\u7ea7\u65b9\u6cd5\uff0c\u7528\u4e8e\u80ce\u513f\u8179\u90e8\u5f02\u5e38\u7684\u8d85\u58f0\u5206\u7c7b\uff0c\u65e0\u9700\u6807\u51c6\u5e73\u9762\u5b9a\u4f4d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u80ce\u513f\u8179\u90e8\u7578\u5f62\u662f\u4e25\u91cd\u7684\u5148\u5929\u6027\u5f02\u5e38\uff0c\u9700\u51c6\u786e\u8bca\u65ad\u4ee5\u6307\u5bfc\u598a\u5a20\u7ba1\u7406\u5e76\u964d\u4f4e\u6b7b\u4ea1\u7387\u3002\u73b0\u6709AI\u65b9\u6cd5\u591a\u5173\u6ce8\u56fe\u50cf\u7ea7\u5206\u7c7b\uff0c\u6848\u4f8b\u7ea7\u8bca\u65ad\u8f83\u5c11\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6ce8\u610f\u529b\u4e13\u5bb6\u6a21\u5757\uff08MoAE\uff09\u5bf9\u4e0d\u540c\u5e73\u9762\u52a0\u6743\uff0c\u63d0\u51fa\u533b\u5b66\u77e5\u8bc6\u9a71\u52a8\u7684\u7279\u5f81\u9009\u62e9\u6a21\u5757\uff08MFS\uff09\u8fdb\u884c\u81ea\u76d1\u7763\u56fe\u50cf\u6807\u8bb0\u9009\u62e9\uff0c\u5e76\u7ed3\u5408\u63d0\u793a\u539f\u578b\u5b66\u4e60\uff08PPL\uff09\u589e\u5f3aMFS\u3002", "result": "\u5728\u5305\u542b2,419\u4e2a\u6848\u4f8b\u300124,748\u5f20\u56fe\u50cf\u548c6\u4e2a\u7c7b\u522b\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u80ce\u513f\u8179\u90e8\u5f02\u5e38\u8d85\u58f0\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6848\u4f8b\u7ea7\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01409", "pdf": "https://arxiv.org/pdf/2507.01409", "abs": "https://arxiv.org/abs/2507.01409", "authors": ["Kuniaki Saito", "Donghyun Kim", "Kwanyong Park", "Atsushi Hashimoto", "Yoshitaka Ushiku"], "title": "CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning", "categories": ["cs.CV"], "comment": "Accepted to ICCV2025", "summary": "An image captioning model flexibly switching its language pattern, e.g.,\ndescriptiveness and length, should be useful since it can be applied to diverse\napplications. However, despite the dramatic improvement in generative\nvision-language models, fine-grained control over the properties of generated\ncaptions is not easy due to two reasons: (i) existing models are not given the\nproperties as a condition during training and (ii) existing models cannot\nsmoothly transition its language pattern from one state to the other. Given\nthis challenge, we propose a new approach, CaptionSmiths, to acquire a single\ncaptioning model that can handle diverse language patterns. First, our approach\nquantifies three properties of each caption, length, descriptiveness, and\nuniqueness of a word, as continuous scalar values, without human annotation.\nGiven the values, we represent the conditioning via interpolation between two\nendpoint vectors corresponding to the extreme states, e.g., one for a very\nshort caption and one for a very long caption. Empirical results demonstrate\nthat the resulting model can smoothly change the properties of the output\ncaptions and show higher lexical alignment than baselines. For instance,\nCaptionSmiths reduces the error in controlling caption length by 506\\% despite\nbetter lexical alignment. Code will be available on\nhttps://github.com/omron-sinicx/captionsmiths.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCaptionSmiths\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u6807\u9898\u5c5e\u6027\u5e76\u63d2\u503c\u7aef\u70b9\u5411\u91cf\uff0c\u5b9e\u73b0\u5355\u6a21\u578b\u7075\u6d3b\u63a7\u5236\u6807\u9898\u7684\u8bed\u8a00\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6807\u9898\u6a21\u578b\u96be\u4ee5\u7cbe\u7ec6\u63a7\u5236\u751f\u6210\u6807\u9898\u7684\u5c5e\u6027\uff0c\u5982\u957f\u5ea6\u548c\u63cf\u8ff0\u6027\uff0c\u4e14\u7f3a\u4e4f\u5e73\u6ed1\u8fc7\u6e21\u80fd\u529b\u3002", "method": "\u91cf\u5316\u6807\u9898\u957f\u5ea6\u3001\u63cf\u8ff0\u6027\u548c\u8bcd\u6c47\u72ec\u7279\u6027\u4e3a\u8fde\u7eed\u6807\u91cf\uff0c\u901a\u8fc7\u63d2\u503c\u7aef\u70b9\u5411\u91cf\u5b9e\u73b0\u6761\u4ef6\u63a7\u5236\u3002", "result": "\u6a21\u578b\u80fd\u5e73\u6ed1\u8c03\u6574\u6807\u9898\u5c5e\u6027\uff0c\u8bcd\u6c47\u5bf9\u9f50\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5982\u957f\u5ea6\u63a7\u5236\u8bef\u5dee\u51cf\u5c11506%\u3002", "conclusion": "CaptionSmiths\u6210\u529f\u5b9e\u73b0\u5355\u6a21\u578b\u7075\u6d3b\u63a7\u5236\u6807\u9898\u8bed\u8a00\u6a21\u5f0f\uff0c\u63d0\u5347\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2507.01417", "pdf": "https://arxiv.org/pdf/2507.01417", "abs": "https://arxiv.org/abs/2507.01417", "authors": ["Jiawei Gu", "Ziyue Qiao", "Zechao Li"], "title": "Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "Out-of-Distribution (OOD) detection is critical for safely deploying deep\nmodels in open-world environments, where inputs may lie outside the training\ndistribution. During inference on a model trained exclusively with\nIn-Distribution (ID) data, we observe a salient gradient phenomenon: around an\nID sample, the local gradient directions for \"enhancing\" that sample's\npredicted class remain relatively consistent, whereas OOD samples--unseen in\ntraining--exhibit disorganized or conflicting gradient directions in the same\nneighborhood. Motivated by this observation, we propose an inference-stage\ntechnique to short-circuit those feature coordinates that spurious gradients\nexploit to inflate OOD confidence, while leaving ID classification largely\nintact. To circumvent the expense of recomputing the logits after this gradient\nshort-circuit, we further introduce a local first-order approximation that\naccurately captures the post-modification outputs without a second forward\npass. Experiments on standard OOD benchmarks show our approach yields\nsubstantial improvements. Moreover, the method is lightweight and requires\nminimal changes to the standard inference pipeline, offering a practical path\ntoward robust OOD detection in real-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u65b9\u5411\u4e00\u81f4\u6027\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u77ed\u8def\u865a\u5047\u68af\u5ea6\u63d0\u5347\u68c0\u6d4b\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301ID\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u5f00\u653e\u4e16\u754c\u4e2d\uff0cOOD\u68c0\u6d4b\u5bf9\u5b89\u5168\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u53d1\u73b0ID\u6837\u672c\u7684\u68af\u5ea6\u65b9\u5411\u4e00\u81f4\uff0c\u800cOOD\u6837\u672c\u5219\u6df7\u4e71\uff0c\u8fd9\u542f\u53d1\u4e86\u65b0\u65b9\u6cd5\u7684\u63d0\u51fa\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u7406\u9636\u6bb5\u7684\u6280\u672f\uff0c\u77ed\u8def\u865a\u5047\u68af\u5ea6\u5229\u7528\u7684\u7279\u5f81\u5750\u6807\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u4e00\u9636\u8fd1\u4f3c\u907f\u514d\u4e8c\u6b21\u524d\u5411\u4f20\u64ad\u3002", "result": "\u5728\u6807\u51c6OOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u65b9\u6cd5\u8f7b\u91cf\u4e14\u5bf9\u6807\u51c6\u63a8\u7406\u6d41\u7a0b\u6539\u52a8\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2OOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2507.01422", "pdf": "https://arxiv.org/pdf/2507.01422", "abs": "https://arxiv.org/abs/2507.01422", "authors": ["Wenjie Liu", "Bingshu Wang", "Ze Wang", "C. L. Philip Chen"], "title": "DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Document shadow removal is a crucial task in the field of document image\nenhancement. However, existing methods tend to remove shadows with constant\ncolor background and ignore color shadows. In this paper, we first design a\ndiffusion model in latent space for document image shadow removal, called\nDocShaDiffusion. It translates shadow images from pixel space to latent space,\nenabling the model to more easily capture essential features. To address the\nissue of color shadows, we design a shadow soft-mask generation module (SSGM).\nIt is able to produce accurate shadow mask and add noise into shadow regions\nspecially. Guided by the shadow mask, a shadow mask-aware guided diffusion\nmodule (SMGDM) is proposed to remove shadows from document images by\nsupervising the diffusion and denoising process. We also propose a\nshadow-robust perceptual feature loss to preserve details and structures in\ndocument images. Moreover, we develop a large-scale synthetic document color\nshadow removal dataset (SDCSRD). It simulates the distribution of realistic\ncolor shadows and provides powerful supports for the training of models.\nExperiments on three public datasets validate the proposed method's superiority\nover state-of-the-art. Our code and dataset will be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u7684\u6587\u6863\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u65b9\u6cd5DocShaDiffusion\uff0c\u7ed3\u5408\u9634\u5f71\u8f6f\u63a9\u6a21\u751f\u6210\u6a21\u5757\uff08SSGM\uff09\u548c\u9634\u5f71\u63a9\u6a21\u5f15\u5bfc\u6269\u6563\u6a21\u5757\uff08SMGDM\uff09\uff0c\u6709\u6548\u5904\u7406\u5f69\u8272\u9634\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5904\u7406\u6052\u5b9a\u989c\u8272\u80cc\u666f\u7684\u9634\u5f71\uff0c\u800c\u5ffd\u7565\u4e86\u5f69\u8272\u9634\u5f71\uff0c\u9650\u5236\u4e86\u6587\u6863\u56fe\u50cf\u589e\u5f3a\u7684\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6a21\u578bDocShaDiffusion\uff0c\u7ed3\u5408SSGM\u751f\u6210\u9634\u5f71\u63a9\u6a21\u5e76\u6ce8\u5165\u566a\u58f0\uff0cSMGDM\u5f15\u5bfc\u6269\u6563\u548c\u53bb\u566a\u8fc7\u7a0b\u53bb\u9664\u9634\u5f71\uff0c\u5e76\u63d0\u51fa\u9634\u5f71\u9c81\u68d2\u611f\u77e5\u7279\u5f81\u635f\u5931\u4fdd\u62a4\u7ec6\u8282\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u5408\u6210\u6587\u6863\u5f69\u8272\u9634\u5f71\u53bb\u9664\u6570\u636e\u96c6\uff08SDCSRD\uff09\u652f\u6301\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "DocShaDiffusion\u5728\u6587\u6863\u9634\u5f71\u53bb\u9664\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.01428", "pdf": "https://arxiv.org/pdf/2507.01428", "abs": "https://arxiv.org/abs/2507.01428", "authors": ["Chen Sun", "Haiyang Sun", "Zhiqing Guo", "Yunfeng Diao", "Liejun Wang", "Dan Ma", "Gaobo Yang", "Keqin Li"], "title": "DiffMark: Diffusion-based Robust Watermark Against Deepfakes", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Deepfakes pose significant security and privacy threats through malicious\nfacial manipulations. While robust watermarking can aid in authenticity\nverification and source tracking, existing methods often lack the sufficient\nrobustness against Deepfake manipulations. Diffusion models have demonstrated\nremarkable performance in image generation, enabling the seamless fusion of\nwatermark with image during generation. In this study, we propose a novel\nrobust watermarking framework based on diffusion model, called DiffMark. By\nmodifying the training and sampling scheme, we take the facial image and\nwatermark as conditions to guide the diffusion model to progressively denoise\nand generate corresponding watermarked image. In the construction of facial\ncondition, we weight the facial image by a timestep-dependent factor that\ngradually reduces the guidance intensity with the decrease of noise, thus\nbetter adapting to the sampling process of diffusion model. To achieve the\nfusion of watermark condition, we introduce a cross information fusion (CIF)\nmodule that leverages a learnable embedding table to adaptively extract\nwatermark features and integrates them with image features via cross-attention.\nTo enhance the robustness of the watermark against Deepfake manipulations, we\nintegrate a frozen autoencoder during training phase to simulate Deepfake\nmanipulations. Additionally, we introduce Deepfake-resistant guidance that\nemploys specific Deepfake model to adversarially guide the diffusion sampling\nprocess to generate more robust watermarked images. Experimental results\ndemonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.\nOur code will be available at https://github.com/vpsg-research/DiffMark.", "AI": {"tldr": "DiffMark\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9c81\u68d2\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6848\uff0c\u7ed3\u5408\u9762\u90e8\u56fe\u50cf\u548c\u6c34\u5370\u6761\u4ef6\uff0c\u751f\u6210\u6297Deepfake\u7be1\u6539\u7684\u6c34\u5370\u56fe\u50cf\u3002", "motivation": "Deepfake\u6280\u672f\u5bf9\u5b89\u5168\u548c\u9690\u79c1\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5bf9\u6297Deepfake\u7be1\u6539\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "DiffMark\u901a\u8fc7\u65f6\u95f4\u6b65\u4f9d\u8d56\u7684\u9762\u90e8\u6761\u4ef6\u5f15\u5bfc\u3001\u4ea4\u53c9\u4fe1\u606f\u878d\u5408\u6a21\u5757\uff08CIF\uff09\u548c\u5bf9\u6297\u6027Deepfake\u6a21\u62df\u8bad\u7ec3\uff0c\u589e\u5f3a\u6c34\u5370\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDiffMark\u5728\u5178\u578bDeepfake\u653b\u51fb\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DiffMark\u4e3a\u5bf9\u6297Deepfake\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01439", "pdf": "https://arxiv.org/pdf/2507.01439", "abs": "https://arxiv.org/abs/2507.01439", "authors": ["Shaocheng Yan", "Pengcheng Shi", "Zhenjun Zhao", "Kaixin Wang", "Kuang Cao", "Ji Wu", "Jiayuan Li"], "title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration", "categories": ["cs.CV"], "comment": "ICCV-2025 Accepted Paper", "summary": "Robust estimation is essential in correspondence-based Point Cloud\nRegistration (PCR). Existing methods using maximal clique search in\ncompatibility graphs achieve high recall but suffer from exponential time\ncomplexity, limiting their use in time-sensitive applications. To address this\nchallenge, we propose a fast and robust estimator, TurboReg, built upon a novel\nlightweight clique, TurboClique, and a highly parallelizable Pivot-Guided\nSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within a\nhighly-constrained compatibility graph. The lightweight nature of the 3-clique\nallows for efficient parallel searching, and the highly-constrained\ncompatibility graph ensures robust spatial consistency for stable\ntransformation estimation. Next, PGS selects matching pairs with high SC$^2$\nscores as pivots, effectively guiding the search toward TurboCliques with\nhigher inlier ratios. Moreover, the PGS algorithm has linear time complexity\nand is significantly more efficient than the maximal clique search with\nexponential time complexity. Extensive experiments show that TurboReg achieves\nstate-of-the-art performance across multiple real-world datasets, with\nsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,\nTurboReg (1K) operates $208.22\\times$ faster than 3DMAC while also achieving\nhigher recall. Our code is accessible at\n\\href{https://github.com/Laka-3DV/TurboReg}{\\texttt{TurboReg}}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u9c81\u68d2\u7684PCR\u4f30\u8ba1\u5668TurboReg\uff0c\u57fa\u4e8e\u8f7b\u91cf\u7ea7TurboClique\u548c\u5e76\u884cPGS\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6700\u5927\u56e2\u641c\u7d22\u7684\u65b9\u6cd5\u867d\u53ec\u56de\u7387\u9ad8\uff0c\u4f46\u65f6\u95f4\u590d\u6742\u5ea6\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4e0d\u9002\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u573a\u666f\u3002", "method": "\u5b9a\u4e49TurboClique\u4e3a\u9ad8\u5ea6\u7ea6\u675f\u517c\u5bb9\u56fe\u4e2d\u76843-\u56e2\uff0c\u7ed3\u5408\u5e76\u884cPGS\u7b97\u6cd5\uff0c\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u57283DMatch+FCGF\u6570\u636e\u96c6\u4e0a\uff0cTurboReg\u901f\u5ea6\u63d0\u5347208.22\u500d\u4e14\u53ec\u56de\u7387\u66f4\u9ad8\u3002", "conclusion": "TurboReg\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u5747\u8fbe\u5230SOTA\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2507.01455", "pdf": "https://arxiv.org/pdf/2507.01455", "abs": "https://arxiv.org/abs/2507.01455", "authors": ["Yuxing Liu", "Ji Zhang", "Zhou Xuchuan", "Jingzhong Xiao", "Huimin Yang", "Jiaxin Zhong"], "title": "OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous\nobjects within images. Existing pixel-wise methods typically assign anomaly\nscores individually and employ a global thresholding strategy to segment\nanomalies. Despite their effectiveness, these approaches encounter significant\nchallenges in real-world applications: (1) neglecting spatial correlations\namong pixels within the same object, resulting in fragmented segmentation; (2)\nvariabil ity in anomaly score distributions across image regions, causing\nglobal thresholds to either generate false positives in background areas or\nmiss segments of anomalous objects. In this work, we introduce OoDDINO, a novel\nmulti-level anomaly segmentation framework designed to address these\nlimitations through a coarse-to-fine anomaly detection strategy. OoDDINO\ncombines an uncertainty-guided anomaly detection model with a pixel-level\nsegmentation model within a two-stage cascade architecture. Initially, we\npropose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that\nsequentially integrates multiple uncertainty metrics with visual\nrepresentations, employing orthogonal constraints to strengthen the detection\nmodel's capacity for localizing anomalous regions accurately. Subsequently, we\ndevelop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically\ngenerates region-specific thresholds based on object-level detection outputs\nand pixel-wise anomaly scores. This approach allows for distinct thresholding\nstrategies within foreground and background areas, achieving fine-grained\nanomaly segmentation. The proposed framework is compatible with other\npixel-wise anomaly detection models, which acts as a plug-in to boost the\nperformance. Extensive experiments on two benchmark datasets validate our\nframework's superiority and compatibility over state-of-the-art methods.", "AI": {"tldr": "OoDDINO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u5c42\u6b21\u5f02\u5e38\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u68c0\u6d4b\u7b56\u7565\u89e3\u51b3\u73b0\u6709\u50cf\u7d20\u7ea7\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u68c0\u6d4b\u6a21\u578b\u548c\u50cf\u7d20\u7ea7\u5206\u5272\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5f02\u5e38\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u50cf\u7d20\u7ea7\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\u5ffd\u89c6\u50cf\u7d20\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u4e14\u5168\u5c40\u9608\u503c\u7b56\u7565\u5bfc\u81f4\u8bef\u68c0\u6216\u6f0f\u68c0\uff0cOoDDINO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7ea7\u8054\u67b6\u6784\uff1a1) \u6b63\u4ea4\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u878d\u5408\u7b56\u7565\uff08OUAFS\uff09\u6574\u5408\u591a\u6307\u6807\uff1b2) \u81ea\u9002\u5e94\u53cc\u9608\u503c\u7f51\u7edc\uff08ADT-Net\uff09\u52a8\u6001\u751f\u6210\u533a\u57df\u7279\u5b9a\u9608\u503c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cOoDDINO\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u517c\u5bb9\u6027\u5f3a\u3002", "conclusion": "OoDDINO\u901a\u8fc7\u591a\u5c42\u6b21\u7b56\u7565\u6709\u6548\u63d0\u5347\u5f02\u5e38\u5206\u5272\u7cbe\u5ea6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01463", "pdf": "https://arxiv.org/pdf/2507.01463", "abs": "https://arxiv.org/abs/2507.01463", "authors": ["Max Gandyra", "Alessandro Santonicola", "Michael Beetz"], "title": "NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation", "categories": ["cs.CV", "cs.AI", "I.2; I.4; I.5"], "comment": "10 pages, 3 figures, 3 tables, NeurIPS 2025 preprint", "summary": "Instance segmentation of novel objects instances in RGB images, given some\nexample images for each object, is a well known problem in computer vision.\nDesigning a model general enough to be employed, for all kinds of novel\nobjects, without (re-) training, has proven to be a difficult task. To handle\nthis, we propose a simple, yet powerful, framework, called: Novel Object Cyclic\nThreshold based Instance Segmentation (NOCTIS). This work stems from and\nimproves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also\nleverages on recent vision foundation models, namely: Grounded-SAM 2 and\nDINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise\nbounding boxes and their corresponding segmentation masks; while DINOv2's\nzero-shot capabilities are employed to generate the image embeddings. The\nquality of those masks, together with their embeddings, is of vital importance\nto our approach; as the proposal-object matching is realized by determining an\nobject matching score based on the similarity of the class embeddings and the\naverage maximum similarity of the patch embeddings. Differently to SAM-6D,\ncalculating the latter involves a prior patch filtering based on the distance\nbetween each patch and its corresponding cyclic/roundtrip patch in the image\ngrid. Furthermore, the average confidence of the proposals' bounding box and\nmask is used as an additional weighting factor for the object matching score.\nWe empirically show that NOCTIS, without further training/fine tuning,\noutperforms the best RGB and RGB-D methods on the seven core datasets of the\nBOP 2023 challenge for the \"Model-based 2D segmentation of unseen objects\"\ntask.", "AI": {"tldr": "NOCTIS\u662f\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5bf9\u672a\u89c1\u7269\u4f53\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86Grounded-SAM 2\u548cDINOv2\u7684\u4f18\u52bf\uff0c\u5728BOP 2023\u6311\u6218\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5bf9\u5404\u7c7b\u672a\u89c1\u7269\u4f53\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u7684\u96be\u9898\u3002", "method": "\u5229\u7528Grounded-SAM 2\u751f\u6210\u7269\u4f53\u63d0\u8bae\u548c\u5206\u5272\u63a9\u7801\uff0cDINOv2\u751f\u6210\u56fe\u50cf\u5d4c\u5165\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u8bc4\u5206\u548c\u5faa\u73af\u9608\u503c\u5339\u914d\u7269\u4f53\u3002", "result": "\u5728BOP 2023\u6311\u6218\u76847\u4e2a\u6838\u5fc3\u6570\u636e\u96c6\u4e0a\uff0cNOCTIS\u8868\u73b0\u4f18\u4e8e\u73b0\u6709RGB\u548cRGB-D\u65b9\u6cd5\u3002", "conclusion": "NOCTIS\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u5206\u5272\u672a\u89c1\u7269\u4f53\u3002"}}
{"id": "2507.01467", "pdf": "https://arxiv.org/pdf/2507.01467", "abs": "https://arxiv.org/abs/2507.01467", "authors": ["Ge Wu", "Shen Zhang", "Ruijing Shi", "Shanghua Gao", "Zhenyuan Chen", "Lei Wang", "Zhaowei Chen", "Hongcheng Gao", "Yao Tang", "Jian Yang", "Ming-Ming Cheng", "Xiang Li"], "title": "Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think", "categories": ["cs.CV"], "comment": null, "summary": "REPA and its variants effectively mitigate training challenges in diffusion\nmodels by incorporating external visual representations from pretrained models,\nthrough alignment between the noisy hidden projections of denoising networks\nand foundational clean image representations. We argue that the external\nalignment, which is absent during the entire denoising inference process, falls\nshort of fully harnessing the potential of discriminative representations. In\nthis work, we propose a straightforward method called Representation\nEntanglement for Generation (REG), which entangles low-level image latents with\na single high-level class token from pretrained foundation models for\ndenoising. REG acquires the capability to produce coherent image-class pairs\ndirectly from pure noise, substantially improving both generation quality and\ntraining efficiency. This is accomplished with negligible additional inference\noverhead, requiring only one single additional token for denoising (<0.5\\%\nincrease in FLOPs and latency). The inference process concurrently reconstructs\nboth image latents and their corresponding global semantics, where the acquired\nsemantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence\nacceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster\ntraining than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,\nSiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA\ntrained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at:\nhttps://github.com/Martinser/REG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aREG\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4f4e\u5c42\u56fe\u50cf\u6f5c\u5728\u8868\u793a\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u5c42\u7c7b\u522b\u6807\u8bb0\u7ea0\u7f20\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982REPA\uff09\u5728\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u65f6\u5229\u7528\u5916\u90e8\u89c6\u89c9\u8868\u793a\uff0c\u4f46\u672a\u5728\u53bb\u566a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5145\u5206\u5229\u7528\u5224\u522b\u6027\u8868\u793a\u3002", "method": "REG\u65b9\u6cd5\u5c06\u4f4e\u5c42\u56fe\u50cf\u6f5c\u5728\u8868\u793a\u4e0e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u5355\u4e2a\u9ad8\u5c42\u7c7b\u522b\u6807\u8bb0\u7ea0\u7f20\uff0c\u76f4\u63a5\u4ece\u7eaf\u566a\u58f0\u4e2d\u751f\u6210\u4e00\u81f4\u7684\u56fe\u50cf-\u7c7b\u522b\u5bf9\u3002", "result": "\u5728ImageNet 256\u00d7256\u4e0a\uff0cSiT-XL/2 + REG\u5b9e\u73b0\u4e8663\u500d\u548c23\u500d\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u4e14\u4ec5\u9700400K\u6b21\u8fed\u4ee3\u5373\u53ef\u8d85\u8d8a4M\u6b21\u8fed\u4ee3\u7684SiT-XL/2 + REPA\u3002", "conclusion": "REG\u901a\u8fc7\u8bed\u4e49\u77e5\u8bc6\u4e3b\u52a8\u5f15\u5bfc\u56fe\u50cf\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4e14\u63a8\u7406\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2507.01472", "pdf": "https://arxiv.org/pdf/2507.01472", "abs": "https://arxiv.org/abs/2507.01472", "authors": ["Jon\u00e1\u0161 Herec", "V\u00edt R\u016f\u017ei\u010dka", "Rado Pito\u0148\u00e1k"], "title": "Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware", "categories": ["cs.CV", "cs.LG", "cs.PF"], "comment": "This is a preprint of a paper accepted for the EDHPC 2025 Conference", "summary": "Methane is a potent greenhouse gas, and detecting its leaks early via\nhyperspectral satellite imagery can help mitigate climate change. Meanwhile,\nmany existing missions operate in manual tasking regimes only, thus missing\npotential events of interest. To overcome slow downlink rates cost-effectively,\nonboard detection is a viable solution. However, traditional methane\nenhancement methods are too computationally demanding for resource-limited\nonboard hardware. This work accelerates methane detection by focusing on\nefficient, low-power algorithms. We test fast target detection methods (ACE,\nCEM) that have not been previously used for methane detection and propose a\nMag1c-SAS - a significantly faster variant of the current state-of-the-art\nalgorithm for methane detection: Mag1c. To explore their true detection\npotential, we integrate them with a machine learning model (U-Net, LinkNet).\nOur results identify two promising candidates (Mag1c-SAS and CEM), both\nacceptably accurate for the detection of strong plumes and computationally\nefficient enough for onboard deployment: one optimized more for accuracy, the\nother more for speed, achieving up to ~100x and ~230x faster computation than\noriginal Mag1c on resource-limited hardware. Additionally, we propose and\nevaluate three band selection strategies. One of them can outperform the method\ntraditionally used in the field while using fewer channels, leading to even\nfaster processing without compromising accuracy. This research lays the\nfoundation for future advancements in onboard methane detection with minimal\nhardware requirements, improving timely data delivery. The produced code, data,\nand models are open-sourced and can be accessed from\nhttps://github.com/zaitra/methane-filters-benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u7532\u70f7\u6cc4\u6f0f\u68c0\u6d4b\u7b97\u6cd5\uff08Mag1c-SAS\u548cCEM\uff09\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u901f\u5ea6\uff08100x-230x\uff09\uff0c\u5e76\u4f18\u5316\u4e86\u6ce2\u6bb5\u9009\u62e9\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u661f\u8f7d\u786c\u4ef6\u3002", "motivation": "\u7532\u70f7\u662f\u5f3a\u6548\u6e29\u5ba4\u6c14\u4f53\uff0c\u73b0\u6709\u536b\u661f\u68c0\u6d4b\u65b9\u6cd5\u56e0\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u624b\u52a8\u4efb\u52a1\u6a21\u5f0f\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u9700\u5feb\u901f\u3001\u4f4e\u529f\u8017\u7684\u661f\u8f7d\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u6d4b\u8bd5\u4e86\u5feb\u901f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08ACE\u3001CEM\uff09\uff0c\u63d0\u51fa\u6539\u8fdb\u7b97\u6cd5Mag1c-SAS\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08U-Net\u3001LinkNet\uff09\u8bc4\u4f30\u6027\u80fd\u3002\u540c\u65f6\u63d0\u51fa\u4e09\u79cd\u6ce2\u6bb5\u9009\u62e9\u7b56\u7565\u3002", "result": "Mag1c-SAS\u548cCEM\u5728\u5f3a\u7fbd\u6d41\u68c0\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\uff08100x-230x\uff09\u3002\u4e00\u79cd\u6ce2\u6bb5\u9009\u62e9\u7b56\u7565\u5728\u51cf\u5c11\u901a\u9053\u7684\u540c\u65f6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f4e\u786c\u4ef6\u9700\u6c42\u7684\u661f\u8f7d\u7532\u70f7\u68c0\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01478", "pdf": "https://arxiv.org/pdf/2507.01478", "abs": "https://arxiv.org/abs/2507.01478", "authors": ["Chentao Shen", "Ding Pan", "Mingyu Mei", "Zaixing He", "Xinyue Zhao"], "title": "Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects", "categories": ["cs.CV"], "comment": "preprint version", "summary": "Visual pose tracking is playing an increasingly vital role in industrial\ncontexts in recent years. However, the pose tracking for industrial metal\nobjects remains a challenging task especially in the real world-environments,\ndue to the reflection characteristic of metal objects. To address this issue,\nwe propose a novel 6DoF pose tracking method based on active control points.\nThe method uses image control points to generate edge feature for optimization\nactively instead of 6DoF pose-based rendering, and serve them as optimization\nvariables. We also introduce an optimal control point regression method to\nimprove robustness. The proposed tracking method performs effectively in both\ndataset evaluation and real world tasks, providing a viable solution for\nreal-time tracking of industrial metal objects. Our source code is made\npublicly available at: https://github.com/tomatoma00/ACPTracking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a7\u5236\u70b9\u76846DoF\u59ff\u6001\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5de5\u4e1a\u91d1\u5c5e\u7269\u4f53\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u53cd\u5c04\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u91d1\u5c5e\u7269\u4f53\u7684\u59ff\u6001\u8ddf\u8e2a\u56e0\u53cd\u5c04\u7279\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u63a7\u5236\u70b9\u751f\u6210\u8fb9\u7f18\u7279\u5f81\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u6700\u4f18\u63a7\u5236\u70b9\u56de\u5f52\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6570\u636e\u96c6\u8bc4\u4f30\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u8ddf\u8e2a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u91d1\u5c5e\u7269\u4f53\u7684\u5b9e\u65f6\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01484", "pdf": "https://arxiv.org/pdf/2507.01484", "abs": "https://arxiv.org/abs/2507.01484", "authors": ["Xiaoshuai Hao", "Yuting Zhao", "Yuheng Ji", "Luanyuan Dai", "Peng Hao", "Dingzhe Li", "Shuai Cheng", "Rong Yin"], "title": "What Really Matters for Robust Multi-Sensor HD Map Construction?", "categories": ["cs.CV"], "comment": "Accepted by IROS 2025", "summary": "High-definition (HD) map construction methods are crucial for providing\nprecise and comprehensive static environmental information, which is essential\nfor autonomous driving systems. While Camera-LiDAR fusion techniques have shown\npromising results by integrating data from both modalities, existing approaches\nprimarily focus on improving model accuracy and often neglect the robustness of\nperception models, which is a critical aspect for real-world applications. In\nthis paper, we explore strategies to enhance the robustness of multi-modal\nfusion methods for HD map construction while maintaining high accuracy. We\npropose three key components: data augmentation, a novel multi-modal fusion\nmodule, and a modality dropout training strategy. These components are\nevaluated on a challenging dataset containing 10 days of NuScenes data. Our\nexperimental results demonstrate that our proposed methods significantly\nenhance the robustness of baseline methods. Furthermore, our approach achieves\nstate-of-the-art performance on the clean validation set of the NuScenes\ndataset. Our findings provide valuable insights for developing more robust and\nreliable HD map construction models, advancing their applicability in\nreal-world autonomous driving scenarios. Project website:\nhttps://robomap-123.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u9c81\u68d2\u6027\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u3001\u65b0\u578b\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u548c\u6a21\u6001\u4e22\u5f03\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u7cbe\u5ea6\uff0c\u800c\u5ffd\u7565\u4e86\u9c81\u68d2\u6027\uff0c\u8fd9\u5bf9\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u6570\u636e\u589e\u5f3a\u3001\u65b0\u578b\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u548c\u6a21\u6001\u4e22\u5f03\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u53ef\u9760\u7684\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.01492", "pdf": "https://arxiv.org/pdf/2507.01492", "abs": "https://arxiv.org/abs/2507.01492", "authors": ["Jiyang Tang", "Hengyi Li", "Yifan Du", "Wayne Xin Zhao"], "title": "AVC-DPO: Aligned Video Captioning via Direct Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Although video multimodal large language models (video MLLMs) have achieved\nsubstantial progress in video captioning tasks, it remains challenging to\nadjust the focal emphasis of video captions according to human preferences. To\naddress this limitation, we propose Aligned Video Captioning via Direct\nPreference Optimization (AVC-DPO), a post-training framework designed to\nenhance captioning capabilities in video MLLMs through preference alignment.\nOur approach designs enhanced prompts that specifically target temporal\ndynamics and spatial information-two key factors that humans care about when\nwatching a video-thereby incorporating human-centric preferences. AVC-DPO\nleverages the same foundation model's caption generation responses under varied\nprompt conditions to conduct preference-aware training and caption alignment.\nUsing this framework, we have achieved exceptional performance in the\nLOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving\nfirst place on the Video Detailed Captioning (VDC) benchmark according to the\nVDCSCORE evaluation metric.", "AI": {"tldr": "AVC-DPO\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u5bf9\u9f50\u589e\u5f3a\u89c6\u9891MLLM\u7684\u6807\u9898\u751f\u6210\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u65f6\u7a7a\u52a8\u6001\u548c\u7a7a\u95f4\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u89c6\u9891MLLM\u5728\u6807\u9898\u751f\u6210\u4efb\u52a1\u4e2d\u96be\u4ee5\u6839\u636e\u4eba\u7c7b\u504f\u597d\u8c03\u6574\u7126\u70b9\uff0c\u9700\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u589e\u5f3a\u63d0\u793a\uff0c\u9488\u5bf9\u65f6\u7a7a\u52a8\u6001\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u54cd\u5e94\u8fdb\u884c\u504f\u597d\u611f\u77e5\u8bad\u7ec3\u548c\u5bf9\u9f50\u3002", "result": "\u5728LOVE@CVPR'25 Workshop Track 1A\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cVDC\u57fa\u51c6\u4e0a\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "AVC-DPO\u901a\u8fc7\u504f\u597d\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891MLLM\u7684\u6807\u9898\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2507.01494", "pdf": "https://arxiv.org/pdf/2507.01494", "abs": "https://arxiv.org/abs/2507.01494", "authors": ["Muhammad Hassam Ejaz", "Muhammad Bilal", "Usman Habib"], "title": "Crop Pest Classification Using Deep Learning Techniques: A Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.", "AI": {"tldr": "\u7efc\u8ff0\u5206\u6790\u4e862018-2025\u5e74\u95f437\u9879\u5173\u4e8eAI\u5bb3\u866b\u5206\u7c7b\u7684\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u53ca\u6280\u672f\u6311\u6218\uff0c\u6307\u51fa\u4eceCNN\u8f6c\u5411\u6df7\u5408\u548cTransformer\u6a21\u578b\u7684\u8d8b\u52bf\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u4e3b\u8981\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u5bb3\u866b\u76d1\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u5982CNN\u3001ViT\uff09\u4e3a\u81ea\u52a8\u5316\u5bb3\u866b\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u679037\u9879\u7814\u7a76\uff0c\u6309\u4f5c\u7269\u7c7b\u578b\u3001\u5bb3\u866b\u79cd\u7c7b\u3001\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u548c\u6280\u672f\u6311\u6218\u8fdb\u884c\u5206\u7c7b\u603b\u7ed3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u65e9\u671f\u7814\u7a76\u591a\u4f9d\u8d56CNN\uff0c\u800c\u6700\u65b0\u8d8b\u52bf\u8f6c\u5411\u6df7\u5408\u548cTransformer\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u3001\u5c0f\u5bb3\u866b\u68c0\u6d4b\u96be\u7b49\u6311\u6218\u3002", "conclusion": "\u7efc\u8ff0\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u7ed3\u6784\u5316\u6982\u8ff0\uff0c\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u4e3aAI\u5bb3\u866b\u76d1\u6d4b\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2507.01496", "pdf": "https://arxiv.org/pdf/2507.01496", "abs": "https://arxiv.org/abs/2507.01496", "authors": ["Jimyeong Kim", "Jungwon Park", "Yeji Song", "Nojun Kwak", "Wonjong Rhee"], "title": "ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation", "categories": ["cs.CV"], "comment": "Published at ICCV 2025. Project page:\n  https://wlaud1001.github.io/ReFlex/", "summary": "Rectified Flow text-to-image models surpass diffusion models in image quality\nand text alignment, but adapting ReFlow for real-image editing remains\nchallenging. We propose a new real-image editing method for ReFlow by analyzing\nthe intermediate representations of multimodal transformer blocks and\nidentifying three key features. To extract these features from real images with\nsufficient structural preservation, we leverage mid-step latent, which is\ninverted only up to the mid-step. We then adapt attention during injection to\nimprove editability and enhance alignment to the target text. Our method is\ntraining-free, requires no user-provided mask, and can be applied even without\na source prompt. Extensive experiments on two benchmarks with nine baselines\ndemonstrate its superior performance over prior methods, further validated by\nhuman evaluations confirming a strong user preference for our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u9700\u7528\u6237\u63d0\u4f9b\u63a9\u7801\u7684ReFlow\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4e2d\u95f4\u8868\u793a\u548c\u5229\u7528\u4e2d\u6b65\u6f5c\u5728\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u6548\u679c\u548c\u6587\u672c\u5bf9\u9f50\u3002", "motivation": "\u5c3d\u7ba1ReFlow\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5e94\u7528\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u7528\u6237\u5e72\u9884\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u591a\u6a21\u6001Transformer\u5757\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff0c\u5e76\u5229\u7528\u4e2d\u6b65\u6f5c\u5728\u7279\u5f81\u8fdb\u884c\u7ed3\u6784\u4fdd\u7559\uff0c\u540c\u65f6\u8c03\u6574\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u63d0\u5347\u7f16\u8f91\u6027\u548c\u6587\u672c\u5bf9\u9f50\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4e5d\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u7528\u6237\u5bf9\u8be5\u65b9\u6cd5\u7684\u5f3a\u70c8\u504f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u9700\u63a9\u7801\u7684\u4f18\u52bf\uff0c\u4e14\u80fd\u663e\u8457\u63d0\u5347\u7f16\u8f91\u6548\u679c\u548c\u6587\u672c\u5bf9\u9f50\u3002"}}
{"id": "2507.01502", "pdf": "https://arxiv.org/pdf/2507.01502", "abs": "https://arxiv.org/abs/2507.01502", "authors": ["Ozan Durgut", "Beril Kallfelz-Sirmacek", "Cem Unsalan"], "title": "Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures, journal manuscript", "summary": "Global warming, loss of biodiversity, and air pollution are among the most\nsignificant problems facing Earth. One of the primary challenges in addressing\nthese issues is the lack of monitoring forests to protect them. To tackle this\nproblem, it is important to leverage remote sensing and computer vision methods\nto automate monitoring applications. Hence, automatic tree crown detection\nalgorithms emerged based on traditional and deep learning methods. In this\nstudy, we first introduce two different tree crown detection methods based on\nthese approaches. Then, we form a novel rule-based approach that integrates\nthese two methods to enhance robustness and accuracy of tree crown detection\nresults. While traditional methods are employed for feature extraction and\nsegmentation of forested areas, deep learning methods are used to detect tree\ncrowns in our method. With the proposed rule-based approach, we post-process\nthese results, aiming to increase the number of detected tree crowns through\nneighboring trees and localized operations. We compare the obtained results\nwith the proposed method in terms of the number of detected tree crowns and\nreport the advantages, disadvantages, and areas for improvement of the obtained\noutcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6811\u51a0\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c4\u5219\u540e\u5904\u7406\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5168\u7403\u53d8\u6696\u3001\u751f\u7269\u591a\u6837\u6027\u4e27\u5931\u548c\u7a7a\u6c14\u6c61\u67d3\u7b49\u95ee\u9898\u4e9f\u9700\u68ee\u6797\u76d1\u6d4b\uff0c\u4f46\u7f3a\u4e4f\u81ea\u52a8\u5316\u76d1\u6d4b\u624b\u6bb5\u3002", "method": "\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5206\u5272\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u6811\u51a0\uff0c\u5e76\u901a\u8fc7\u89c4\u5219\u540e\u5904\u7406\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u65b0\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6811\u51a0\u68c0\u6d4b\u6570\u91cf\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4f18\u7f3a\u70b9\u548c\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c4\u5219\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6811\u51a0\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u68ee\u6797\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01504", "pdf": "https://arxiv.org/pdf/2507.01504", "abs": "https://arxiv.org/abs/2507.01504", "authors": ["Robert Aufschl\u00e4ger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u6846\u67b6cRID\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u68c0\u6d4b\u884c\u4eba\u6570\u636e\u4e2d\u7684PII\u5e76\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u8857\u666f\u6570\u636e\u4f5c\u4e3a\u5f00\u653e\u6570\u636e\u5bf9\u81ea\u52a8\u9a7e\u9a76\u548cAI\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u4e2d\u5305\u542b\u7684\u4e2a\u4eba\u53ef\u8bc6\u522b\u4fe1\u606f\uff08PII\uff09\u5e26\u6765\u9690\u79c1\u98ce\u9669\u3002", "method": "\u7ed3\u5408\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u8868\u5f81\u5b66\u4e60\uff0c\u68c0\u6d4b\u6587\u672c\u53ef\u63cf\u8ff0\u7684PII\u7ebf\u7d22\uff0c\u5e76\u589e\u5f3a\u884c\u4eba\u91cd\u8bc6\u522b\u3002", "result": "\u5728Market-1501\u5230CUHK03-np\u7684\u8de8\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u6539\u8fdb\u6027\u80fd\u3002", "conclusion": "cRID\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684PII\uff0c\u5e76\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.01509", "pdf": "https://arxiv.org/pdf/2507.01509", "abs": "https://arxiv.org/abs/2507.01509", "authors": ["Tapas K. Dutta", "Snehashis Majhi", "Deepak Ranjan Nayak", "Debesh Jha"], "title": "Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 2 figures, MICCAI-2025", "summary": "Polyp segmentation in colonoscopy images is crucial for early detection and\ndiagnosis of colorectal cancer. However, this task remains a significant\nchallenge due to the substantial variations in polyp shape, size, and color, as\nwell as the high similarity between polyps and surrounding tissues, often\ncompounded by indistinct boundaries. While existing encoder-decoder CNN and\ntransformer-based approaches have shown promising results, they struggle with\nstable segmentation performance on polyps with weak or blurry boundaries. These\nmethods exhibit limited abilities to distinguish between polyps and non-polyps\nand capture essential boundary cues. Moreover, their generalizability still\nfalls short of meeting the demands of real-time clinical applications. To\naddress these limitations, we propose SAM-MaGuP, a groundbreaking approach for\nrobust polyp segmentation. By incorporating a boundary distillation module and\na 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels\nat resolving weak boundary challenges and amplifies feature learning through\nenriched global contextual interactions. Extensive evaluations across five\ndiverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,\nachieving unmatched segmentation accuracy and robustness. Our key innovations,\na Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in\nthe field, pushing the boundaries of polyp segmentation to new heights.", "AI": {"tldr": "SAM-MaGuP\u662f\u4e00\u79cd\u57fa\u4e8eSegment Anything Model\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fb9\u754c\u84b8\u998f\u6a21\u5757\u548c1D-2D Mamba\u9002\u914d\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u606f\u8089\u5206\u5272\u5728\u7ed3\u80a0\u955c\u56fe\u50cf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u606f\u8089\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u989c\u8272\u7684\u591a\u6837\u6027\u4ee5\u53ca\u8fb9\u754c\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u5206\u5272\u3002", "method": "\u63d0\u51faSAM-MaGuP\uff0c\u7ed3\u5408\u8fb9\u754c\u84b8\u998f\u6a21\u5757\u548c1D-2D Mamba\u9002\u914d\u5668\uff0c\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u4ea4\u4e92\u548c\u8fb9\u754c\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5206\u5272\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "SAM-MaGuP\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\uff0c\u4e3a\u606f\u8089\u5206\u5272\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2507.01532", "pdf": "https://arxiv.org/pdf/2507.01532", "abs": "https://arxiv.org/abs/2507.01532", "authors": ["Tomas Zelezny", "Jakub Straka", "Vaclav Javorek", "Ondrej Valach", "Marek Hruz", "Ivan Gruber"], "title": "Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights", "categories": ["cs.CV"], "comment": "8 pages, 9 figures, supplementary, SLRTP2025, CVPR2025", "summary": "Sign Language Translation (SLT) has evolved significantly, moving from\nisolated recognition approaches to complex, continuous gloss-free translation\nsystems. This paper explores the impact of pose-based data preprocessing\ntechniques - normalization, interpolation, and augmentation - on SLT\nperformance. We employ a transformer-based architecture, adapting a modified T5\nencoder-decoder model to process pose representations. Through extensive\nablation studies on YouTubeASL and How2Sign datasets, we analyze how different\npreprocessing strategies affect translation accuracy. Our results demonstrate\nthat appropriate normalization, interpolation, and augmentation techniques can\nsignificantly improve model robustness and generalization abilities.\nAdditionally, we provide a deep analysis of the model's attentions and reveal\ninteresting behavior suggesting that adding a dedicated register token can\nimprove overall model performance. We publish our code on our GitHub\nrepository, including the preprocessed YouTubeASL data.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u59ff\u6001\u7684\u6570\u636e\u9884\u5904\u7406\u6280\u672f\uff08\u5f52\u4e00\u5316\u3001\u63d2\u503c\u548c\u589e\u5f3a\uff09\u5bf9\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u6539\u8fdb\u7684T5\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u59ff\u6001\u6570\u636e\u9884\u5904\u7406\u6280\u672f\u63d0\u5347\u624b\u8bed\u7ffb\u8bd1\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8fde\u7eed\u3001\u65e0\u6ce8\u91ca\u7684\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684T5\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u5904\u7406\u59ff\u6001\u8868\u793a\uff0c\u5e76\u5728YouTubeASL\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u9884\u5904\u7406\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u9002\u5f53\u7684\u5f52\u4e00\u5316\u3001\u63d2\u503c\u548c\u589e\u5f3a\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u6dfb\u52a0\u4e13\u7528\u5bc4\u5b58\u5668\u6807\u8bb0\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u59ff\u6001\u6570\u636e\u9884\u5904\u7406\u6280\u672f\u5bf9\u624b\u8bed\u7ffb\u8bd1\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u6a21\u578b\u6539\u8fdb\u548c\u516c\u5f00\u6570\u636e\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.01535", "pdf": "https://arxiv.org/pdf/2507.01535", "abs": "https://arxiv.org/abs/2507.01535", "authors": ["Bingxi Liu", "Calvin Chen", "Junhao Li", "Guyang Yu", "Haoqian Song", "Xuchen Liu", "Jinqiang Cui", "Hong Zhang"], "title": "TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking", "categories": ["cs.CV"], "comment": "12 pages", "summary": "The Vision Transformer (ViT) model has long struggled with the challenge of\nquadratic complexity, a limitation that becomes especially critical in unmanned\naerial vehicle (UAV) tracking systems, where data must be processed in real\ntime. In this study, we explore the recently proposed State-Space Model, Mamba,\nleveraging its computational efficiency and capability for long-sequence\nmodeling to effectively process dense image sequences in tracking tasks. First,\nwe highlight the issue of temporal inconsistency in existing Mamba-based\nmethods, specifically the failure to account for temporal continuity in the\nMamba scanning mechanism. Secondly, building upon this insight,we propose\nTrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model\nfor handling image sequence of tracking problem. In our framework, the mamba\nscan is performed in a nested way while independently process temporal and\nspatial coherent patch tokens. While the template frame is encoded as query\ntoken and utilized for tracking in every scan. Extensive experiments conducted\non five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves\nstate-of-the-art precision while offering noticeable higher speed in UAV\ntracking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u6a21\u578b\u7684TrackingMiM\u67b6\u6784\uff0c\u89e3\u51b3\u4e86ViT\u5728\u65e0\u4eba\u673a\u8ddf\u8e2a\u4e2d\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u548c\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "Vision Transformer\uff08ViT\uff09\u5728\u65e0\u4eba\u673a\uff08UAV\uff09\u8ddf\u8e2a\u7cfb\u7edf\u4e2d\u56e0\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u96be\u4ee5\u5b9e\u65f6\u5904\u7406\u6570\u636e\uff0c\u800c\u73b0\u6709Mamba\u65b9\u6cd5\u5b58\u5728\u65f6\u95f4\u8fde\u7eed\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86TrackingMiM\uff0c\u4e00\u79cdMamba-in-Mamba\u67b6\u6784\uff0c\u901a\u8fc7\u5d4c\u5957\u626b\u63cf\u72ec\u7acb\u5904\u7406\u65f6\u7a7a\u4e00\u81f4\u7684\u56fe\u50cf\u5757\uff0c\u6a21\u677f\u5e27\u4f5c\u4e3a\u67e5\u8be2\u4ee4\u724c\u7528\u4e8e\u8ddf\u8e2a\u3002", "result": "\u5728\u4e94\u4e2aUAV\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTrackingMiM\u5b9e\u73b0\u4e86\u6700\u9ad8\u7cbe\u5ea6\u548c\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "TrackingMiM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u65e0\u4eba\u673a\u8ddf\u8e2a\u4efb\u52a1\u3002"}}
{"id": "2507.01539", "pdf": "https://arxiv.org/pdf/2507.01539", "abs": "https://arxiv.org/abs/2507.01539", "authors": ["Mohammadreza Amirian", "Michael Bach", "Oscar Jimenez-del-Toro", "Christoph Aberle", "Roger Schaer", "Vincent Andrearczyk", "Jean-F\u00e9lix Maestrati", "Maria Martin Asiain", "Kyriakos Flouris", "Markus Obmann", "Clarisse Dromain", "Beno\u00eet Dufour", "Pierre-Alexandre Alois Poletti", "Hendrik von Tengg-Kobligk", "Rolf H\u00fcgli", "Martin Kretzschmar", "Hatem Alkadhi", "Ender Konukoglu", "Henning M\u00fcller", "Bram Stieltjes", "Adrien Depeursinge"], "title": "A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization", "categories": ["cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) has introduced numerous opportunities for human\nassistance and task automation in medicine. However, it suffers from poor\ngeneralization in the presence of shifts in the data distribution. In the\ncontext of AI-based computed tomography (CT) analysis, significant data\ndistribution shifts can be caused by changes in scanner manufacturer,\nreconstruction technique or dose. AI harmonization techniques can address this\nproblem by reducing distribution shifts caused by various acquisition settings.\nThis paper presents an open-source benchmark dataset containing CT scans of an\nanthropomorphic phantom acquired with various scanners and settings, which\npurpose is to foster the development of AI harmonization techniques. Using a\nphantom allows fixing variations attributed to inter- and intra-patient\nvariations. The dataset includes 1378 image series acquired with 13 scanners\nfrom 4 manufacturers across 8 institutions using a harmonized protocol as well\nas several acquisition doses. Additionally, we present a methodology, baseline\nresults and open-source code to assess image- and feature-level stability and\nliver tissue classification, promoting the development of AI harmonization\nstrategies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4fc3\u8fdbAI\u5728CT\u5206\u6790\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u51cf\u5c11\u6570\u636e\u5206\u5e03\u504f\u79fb\u3002", "motivation": "\u89e3\u51b3AI\u5728\u533b\u5b66CT\u5206\u6790\u4e2d\u56e0\u6570\u636e\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5305\u542b1378\u4e2aCT\u626b\u63cf\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u626b\u63cf\u4eea\u548c\u8bbe\u7f6e\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u57fa\u7ebf\u7ed3\u679c\u548c\u5f00\u6e90\u4ee3\u7801\uff0c\u652f\u6301AI\u8c10\u8c03\u6280\u672f\u7684\u53d1\u5c55\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6709\u52a9\u4e8e\u63a8\u52a8AI\u8c10\u8c03\u6280\u672f\u7684\u5f00\u53d1\u548c\u5e94\u7528\u3002"}}
{"id": "2507.01557", "pdf": "https://arxiv.org/pdf/2507.01557", "abs": "https://arxiv.org/abs/2507.01557", "authors": ["Marcin Kowlaczyk", "Tomasz Kryjak"], "title": "Interpolation-Based Event Visual Data Filtering Algorithms", "categories": ["cs.CV"], "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Vancouver, 2023.\n  Copyright IEEE", "summary": "The field of neuromorphic vision is developing rapidly, and event cameras are\nfinding their way into more and more applications. However, the data stream\nfrom these sensors is characterised by significant noise. In this paper, we\npropose a method for event data that is capable of removing approximately 99\\%\nof noise while preserving the majority of the valid signal. We have proposed\nfour algorithms based on the matrix of infinite impulse response (IIR) filters\nmethod. We compared them on several event datasets that were further modified\nby adding artificially generated noise and noise recorded with dynamic vision\nsensor. The proposed methods use about 30KB of memory for a sensor with a\nresolution of 1280 x 720 and is therefore well suited for implementation in\nembedded devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u9650\u8109\u51b2\u54cd\u5e94\uff08IIR\uff09\u6ee4\u6ce2\u5668\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u80fd\u53bb\u9664\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e2d\u7ea699%\u7684\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u6548\u4fe1\u53f7\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u6d41\u4e2d\u5b58\u5728\u663e\u8457\u566a\u58f0\uff0c\u5f71\u54cd\u4e86\u5176\u5728\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u57fa\u4e8eIIR\u6ee4\u6ce2\u5668\u77e9\u9635\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u4e8b\u4ef6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5305\u62ec\u6dfb\u52a0\u4eba\u5de5\u566a\u58f0\u548c\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u8bb0\u5f55\u7684\u566a\u58f0\u3002", "result": "\u65b9\u6cd5\u80fd\u53bb\u9664\u7ea699%\u7684\u566a\u58f0\uff0c\u5185\u5b58\u5360\u7528\u7ea630KB\uff0c\u9002\u7528\u4e8e1280x720\u5206\u8fa8\u7387\u7684\u4f20\u611f\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u8bbe\u5907\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u7684\u566a\u58f0\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.01573", "pdf": "https://arxiv.org/pdf/2507.01573", "abs": "https://arxiv.org/abs/2507.01573", "authors": ["Hao Wang", "Keyan Hu", "Xin Guo", "Haifeng Li", "Chao Tao"], "title": "A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation", "categories": ["cs.CV"], "comment": "20 pages, 14 figures", "summary": "Remote sensing semantic segmentation must address both what the ground\nobjects are within an image and where they are located. Consequently,\nsegmentation models must ensure not only the semantic correctness of\nlarge-scale patches (low-frequency information) but also the precise\nlocalization of boundaries between patches (high-frequency information).\nHowever, most existing approaches rely heavily on discriminative learning,\nwhich excels at capturing low-frequency features, while overlooking its\ninherent limitations in learning high-frequency features for semantic\nsegmentation. Recent studies have revealed that diffusion generative models\nexcel at generating high-frequency details. Our theoretical analysis confirms\nthat the diffusion denoising process significantly enhances the model's ability\nto learn high-frequency features; however, we also observe that these models\nexhibit insufficient semantic inference for low-frequency features when guided\nsolely by the original image. Therefore, we integrate the strengths of both\ndiscriminative and generative learning, proposing the Integration of\nDiscriminative and diffusion-based Generative learning for Boundary Refinement\n(IDGBR) framework. The framework first generates a coarse segmentation map\nusing a discriminative backbone model. This map and the original image are fed\ninto a conditioning guidance network to jointly learn a guidance representation\nsubsequently leveraged by an iterative denoising diffusion process refining the\ncoarse segmentation. Extensive experiments across five remote sensing semantic\nsegmentation datasets (binary and multi-class segmentation) confirm our\nframework's capability of consistent boundary refinement for coarse results\nfrom diverse discriminative architectures. The source code will be available at\nhttps://github.com/KeyanHu-git/IDGBR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5224\u522b\u5b66\u4e60\u548c\u6269\u6563\u751f\u6210\u5b66\u4e60\u7684\u6846\u67b6\uff08IDGBR\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u9065\u611f\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u8fb9\u754c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u9ad8\u9891\u8fb9\u754c\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u6269\u6563\u751f\u6210\u6a21\u578b\u64c5\u957f\u751f\u6210\u9ad8\u9891\u7ec6\u8282\uff0c\u4f46\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u8f83\u5f31\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5224\u522b\u6a21\u578b\u751f\u6210\u7c97\u5206\u5272\u56fe\uff0c\u7ed3\u5408\u539f\u59cb\u56fe\u50cf\u8f93\u5165\u6761\u4ef6\u5f15\u5bfc\u7f51\u7edc\uff0c\u518d\u5229\u7528\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u8fed\u4ee3\u4f18\u5316\u8fb9\u754c\u3002", "result": "\u5728\u4e94\u4e2a\u9065\u611f\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIDGBR\u80fd\u663e\u8457\u6539\u8fdb\u8fb9\u754c\u7cbe\u5ea6\u3002", "conclusion": "IDGBR\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86\u5224\u522b\u548c\u751f\u6210\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u8fb9\u754c\u5206\u5272\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2507.01586", "pdf": "https://arxiv.org/pdf/2507.01586", "abs": "https://arxiv.org/abs/2507.01586", "authors": ["Bryan Constantine Sadihin", "Michael Hua Wang", "Shei Pern Chua", "Hang Su"], "title": "SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation", "categories": ["cs.CV"], "comment": "Project page and code: https://bconstantine.github.io/SketchColour", "summary": "The production of high-quality 2D animation is highly labor-intensive\nprocess, as animators are currently required to draw and color a large number\nof frames by hand. We present SketchColour, the first sketch-to-colour pipeline\nfor 2D animation built on a diffusion transformer (DiT) backbone. By replacing\nthe conventional U-Net denoiser with a DiT-style architecture and injecting\nsketch information via lightweight channel-concatenation adapters accompanied\nwith LoRA finetuning, our method natively integrates conditioning without the\nparameter and memory bloat of a duplicated ControlNet, greatly reducing\nparameter count and GPU memory usage. Evaluated on the SAKUGA dataset,\nSketchColour outperforms previous state-of-the-art video colourization methods\nacross all metrics, despite using only half the training data of competing\nmodels. Our approach produces temporally coherent animations with minimal\nartifacts such as colour bleeding or object deformation. Our code is available\nat: https://bconstantine.github.io/SketchColour .", "AI": {"tldr": "SketchColour\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u76842D\u52a8\u753b\u8349\u56fe\u5230\u8272\u5f69\u8f6c\u6362\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u548cGPU\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf2D\u52a8\u753b\u5236\u4f5c\u9700\u8981\u5927\u91cf\u624b\u5de5\u7ed8\u5236\u548c\u4e0a\u8272\uff0c\u8017\u65f6\u8017\u529b\u3002SketchColour\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u901a\u9053\u8fde\u63a5\u9002\u914d\u5668\u548cLoRA\u5fae\u8c03\u6ce8\u5165\u8349\u56fe\u4fe1\u606f\uff0c\u907f\u514dControlNet\u7684\u53c2\u6570\u81a8\u80c0\u3002", "result": "\u5728SAKUGA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u4e0a\u8272\u65b9\u6cd5\uff0c\u4ec5\u7528\u4e00\u534a\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u66f4\u9ad8\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u66f4\u5c11\u7684\u4f2a\u5f71\u3002", "conclusion": "SketchColour\u4e3a2D\u52a8\u753b\u5236\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01587", "pdf": "https://arxiv.org/pdf/2507.01587", "abs": "https://arxiv.org/abs/2507.01587", "authors": ["Youngjin Oh", "Junhyeong Kwon", "Keuntek Lee", "Nam Ik Cho"], "title": "Towards Controllable Real Image Denoising with Camera Parameters", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted for publication in ICIP 2025, IEEE International Conference\n  on Image Processing", "summary": "Recent deep learning-based image denoising methods have shown impressive\nperformance; however, many lack the flexibility to adjust the denoising\nstrength based on the noise levels, camera settings, and user preferences. In\nthis paper, we introduce a new controllable denoising framework that adaptively\nremoves noise from images by utilizing information from camera parameters.\nSpecifically, we focus on ISO, shutter speed, and F-number, which are closely\nrelated to noise levels. We convert these selected parameters into a vector to\ncontrol and enhance the performance of the denoising network. Experimental\nresults show that our method seamlessly adds controllability to standard\ndenoising neural networks and improves their performance. Code is available at\nhttps://github.com/OBAKSA/CPADNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u53ef\u63a7\u53bb\u566a\u6846\u67b6\uff0c\u901a\u8fc7ISO\u3001\u5feb\u95e8\u901f\u5ea6\u548c\u5149\u5708\u503c\u8c03\u6574\u53bb\u566a\u5f3a\u5ea6\uff0c\u63d0\u5347\u4e86\u53bb\u566a\u7f51\u7edc\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u53bb\u566a\u65f6\u7f3a\u4e4f\u6839\u636e\u566a\u58f0\u6c34\u5e73\u3001\u76f8\u673a\u8bbe\u7f6e\u548c\u7528\u6237\u504f\u597d\u8c03\u6574\u53bb\u566a\u5f3a\u5ea6\u7684\u7075\u6d3b\u6027\u3002", "method": "\u5c06ISO\u3001\u5feb\u95e8\u901f\u5ea6\u548c\u5149\u5708\u503c\u8f6c\u6362\u4e3a\u5411\u91cf\uff0c\u7528\u4e8e\u63a7\u5236\u548c\u589e\u5f3a\u53bb\u566a\u7f51\u7edc\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e3a\u6807\u51c6\u7684\u53bb\u566a\u795e\u7ecf\u7f51\u7edc\u589e\u52a0\u4e86\u53ef\u63a7\u6027\uff0c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u81ea\u9002\u5e94\u53bb\u566a\uff0c\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01590", "pdf": "https://arxiv.org/pdf/2507.01590", "abs": "https://arxiv.org/abs/2507.01590", "authors": ["Ameer Hamza", "Zuhaib Hussain But", "Umar Arif", "Samiya", "M. Abdullah Asad", "Muhammad Naeem"], "title": "Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This study presents a novel classroom surveillance system that integrates\nmultiple modalities, including drowsiness, tracking of mobile phone usage, and\nface recognition,to assess student attentiveness with enhanced precision.The\nsystem leverages the YOLOv8 model to detect both mobile phone and sleep\nusage,(Ghatge et al., 2024) while facial recognition is achieved through\nLResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These\nmodels work in synergy to provide comprehensive, real-time monitoring, offering\ninsights into student engagement and behavior.(S et al., 2023) The framework is\ntrained on specialized datasets, such as the RMFD dataset for face recognition\nand a Roboflow dataset for mobile phone detection. The extensive evaluation of\nthe system shows promising results. Sleep detection achieves 97. 42% mAP@50,\nface recognition achieves 86. 45% validation accuracy and mobile phone\ndetection reach 85. 89% mAP@50. The system is implemented within a core PHP web\napplication and utilizes ESP32-CAM hardware for seamless data capture.(Neto et\nal., 2024) This integrated approach not only enhances classroom monitoring, but\nalso ensures automatic attendance recording via face recognition as students\nremain seated in the classroom, offering scalability for diverse educational\nenvironments.(Banada,2025)", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8bfe\u5802\u76d1\u63a7\u7cfb\u7edf\uff0c\u7ed3\u5408\u7761\u610f\u68c0\u6d4b\u3001\u624b\u673a\u4f7f\u7528\u8ffd\u8e2a\u548c\u4eba\u8138\u8bc6\u522b\uff0c\u4ee5\u9ad8\u7cbe\u5ea6\u8bc4\u4f30\u5b66\u751f\u6ce8\u610f\u529b\u3002\u7cfb\u7edf\u91c7\u7528YOLOv8\u3001LResNet Occ FC\u3001YOLO\u548cMTCNN\u6a21\u578b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u76d1\u6d4b\uff0c\u5e76\u5728\u6838\u5fc3PHP\u5e94\u7528\u4e2d\u90e8\u7f72\uff0c\u4f7f\u7528ESP32-CAM\u786c\u4ef6\u3002", "motivation": "\u63d0\u5347\u8bfe\u5802\u76d1\u63a7\u7684\u7cbe\u786e\u6027\u548c\u5168\u9762\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u81ea\u52a8\u8003\u52e4\u8bb0\u5f55\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u6559\u80b2\u73af\u5883\u3002", "method": "\u96c6\u6210YOLOv8\u68c0\u6d4b\u624b\u673a\u548c\u7761\u610f\uff0cLResNet Occ FC\u548cMTCNN\u5b9e\u73b0\u4eba\u8138\u8bc6\u522b\uff0c\u4f7f\u7528RMFD\u548cRoboflow\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u7761\u610f\u68c0\u6d4bmAP@50\u4e3a97.42%\uff0c\u4eba\u8138\u8bc6\u522b\u9a8c\u8bc1\u51c6\u786e\u738786.45%\uff0c\u624b\u673a\u68c0\u6d4bmAP@50\u4e3a85.89%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u591a\u6a21\u6001\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u8bfe\u5802\u76d1\u63a7\u6548\u679c\uff0c\u5e76\u5177\u5907\u81ea\u52a8\u8003\u52e4\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u6559\u80b2\u573a\u666f\u3002"}}
{"id": "2507.01603", "pdf": "https://arxiv.org/pdf/2507.01603", "abs": "https://arxiv.org/abs/2507.01603", "authors": ["Yue-Jiang Dong", "Wang Zhao", "Jiale Xu", "Ying Shan", "Song-Hai Zhang"], "title": "DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based video depth estimation methods have achieved remarkable\nsuccess with strong generalization ability. However, predicting depth for long\nvideos remains challenging. Existing methods typically split videos into\noverlapping sliding windows, leading to accumulated scale discrepancies across\ndifferent windows, particularly as the number of windows increases.\nAdditionally, these methods rely solely on 2D diffusion priors, overlooking the\ninherent 3D geometric structure of video depths, which results in geometrically\ninconsistent predictions. In this paper, we propose DepthSync, a novel,\ntraining-free framework using diffusion guidance to achieve scale- and\ngeometry-consistent depth predictions for long videos. Specifically, we\nintroduce scale guidance to synchronize the depth scale across windows and\ngeometry guidance to enforce geometric alignment within windows based on the\ninherent 3D constraints in video depths. These two terms work synergistically,\nsteering the denoising process toward consistent depth predictions. Experiments\non various datasets validate the effectiveness of our method in producing depth\nestimates with improved scale and geometry consistency, particularly for long\nvideos.", "AI": {"tldr": "DepthSync\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u5f15\u5bfc\u5b9e\u73b0\u957f\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u7684\u5c3a\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5b58\u5728\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u548c\u51e0\u4f55\u7ed3\u6784\u5ffd\u7565\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5c3a\u5ea6\u5f15\u5bfc\u548c\u51e0\u4f55\u5f15\u5bfc\uff0c\u534f\u540c\u4f18\u5316\u53bb\u566a\u8fc7\u7a0b\u4ee5\u5b9e\u73b0\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DepthSync\u5728\u957f\u89c6\u9891\u4e2d\u63d0\u5347\u5c3a\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u6709\u6548\u6027\u3002", "conclusion": "DepthSync\u4e3a\u957f\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01607", "pdf": "https://arxiv.org/pdf/2507.01607", "abs": "https://arxiv.org/abs/2507.01607", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi", "Eric Bourbao"], "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u9488\u5bf9\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5927\u89c4\u6a21\u635f\u5931\u8bad\u7ec3\u7684\u6a21\u578b\u540c\u6837\u6613\u53d7\u653b\u51fb\u3002\u901a\u8fc720\u79cd\u7ba1\u9053\u914d\u7f6e\u548c15\u79cd\u653b\u51fb\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u5355\u4e00\u540e\u95e8\u53ef\u7ed5\u8fc7\u7cfb\u7edf\u529f\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u5e94\u5bf9\u63aa\u65bd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u8bc6\u522b\u7684\u5e7f\u6cdb\u5e94\u7528\u5e26\u6765\u4e86\u5b89\u5168\u9690\u60a3\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u771f\u5b9e\u65e0\u7ea6\u675f\u7cfb\u7edf\u4e2d\u7684\u540e\u95e8\u653b\u51fb\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u63a2\u7d22DNN\u540e\u95e8\u5728\u4eba\u8138\u8bc6\u522b\u7ba1\u9053\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4eba\u8138\u751f\u6210\u548c\u5730\u6807\u504f\u79fb\u4e24\u79cd\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5927\u89c4\u6a21\u635f\u5931\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u8106\u5f31\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5355\u4e00\u540e\u95e8\u53ef\u7ed5\u8fc7\u7cfb\u7edf\u529f\u80fd\uff0c\u5c55\u793a\u4e8620\u79cd\u7ba1\u9053\u914d\u7f6e\u548c15\u79cd\u653b\u51fb\u6848\u4f8b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u540e\u95e8\u98ce\u9669\uff0c\u5e76\u63d0\u4f9b\u4e86\u9488\u5bf9\u6027\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2507.01608", "pdf": "https://arxiv.org/pdf/2507.01608", "abs": "https://arxiv.org/abs/2507.01608", "authors": ["Xu Zhang", "Ming Lu", "Yan Chen", "Zhan Ma"], "title": "Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference", "categories": ["cs.CV", "eess.IV"], "comment": "International Conference on Multimedia and Expo (ICME), 2025", "summary": "In recent years, compressed domain semantic inference has primarily relied on\nlearned image coding models optimized for mean squared error (MSE). However,\nMSE-oriented optimization tends to yield latent spaces with limited semantic\nrichness, which hinders effective semantic inference in downstream tasks.\nMoreover, achieving high performance with these models often requires\nfine-tuning the entire vision model, which is computationally intensive,\nespecially for large models. To address these problems, we introduce\nPerception-Oriented Latent Coding (POLC), an approach that enriches the\nsemantic content of latent features for high-performance compressed domain\nsemantic inference. With the semantically rich latent space, POLC requires only\na plug-and-play adapter for fine-tuning, significantly reducing the parameter\ncount compared to previous MSE-oriented methods. Experimental results\ndemonstrate that POLC achieves rate-perception performance comparable to\nstate-of-the-art generative image coding methods while markedly enhancing\nperformance in vision tasks, with minimal fine-tuning overhead. Code is\navailable at https://github.com/NJUVISION/POLC.", "AI": {"tldr": "POLC\u63d0\u51fa\u4e86\u4e00\u79cd\u611f\u77e5\u5bfc\u5411\u7684\u6f5c\u5728\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e30\u5bcc\u6f5c\u5728\u7279\u5f81\u7684\u8bed\u4e49\u5185\u5bb9\uff0c\u63d0\u5347\u538b\u7f29\u57df\u8bed\u4e49\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u5fae\u8c03\u53c2\u6570\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eMSE\u4f18\u5316\u7684\u56fe\u50cf\u7f16\u7801\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\u8d2b\u4e4f\uff0c\u4e14\u5168\u6a21\u578b\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faPOLC\u65b9\u6cd5\uff0c\u901a\u8fc7\u611f\u77e5\u5bfc\u5411\u4f18\u5316\u4e30\u5bcc\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\uff0c\u4ec5\u9700\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5fae\u8c03\u3002", "result": "POLC\u5728\u538b\u7f29\u57df\u8bed\u4e49\u63a8\u7406\u4e2d\u6027\u80fd\u63a5\u8fd1\u751f\u6210\u5f0f\u56fe\u50cf\u7f16\u7801\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u89c6\u89c9\u4efb\u52a1\u8868\u73b0\uff0c\u5fae\u8c03\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "POLC\u4e3a\u538b\u7f29\u57df\u8bed\u4e49\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01630", "pdf": "https://arxiv.org/pdf/2507.01630", "abs": "https://arxiv.org/abs/2507.01630", "authors": ["Yuxiao Wang", "Yu Lei", "Zhenao Wei", "Weiying Xue", "Xinyu Jiang", "Nan Zhuang", "Qi Liu"], "title": "Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICCV 2025", "summary": "The task of Human-Object conTact (HOT) detection involves identifying the\nspecific areas of the human body that are touching objects. Nevertheless,\ncurrent models are restricted to just one type of image, often leading to too\nmuch segmentation in areas with little interaction, and struggling to maintain\ncategory consistency within specific regions. To tackle this issue, a HOT\nframework, termed \\textbf{P3HOT}, is proposed, which blends \\textbf{P}rompt\nguidance and human \\textbf{P}roximal \\textbf{P}erception. To begin with, we\nutilize a semantic-driven prompt mechanism to direct the network's attention\ntowards the relevant regions based on the correlation between image and text.\nThen a human proximal perception mechanism is employed to dynamically perceive\nkey depth range around the human, using learnable parameters to effectively\neliminate regions where interactions are not expected. Calculating depth\nresolves the uncertainty of the overlap between humans and objects in a 2D\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\nthe shortcomings of existing methods in addressing negative samples.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art performance in four metrics across two benchmark datasets.\nSpecifically, our model achieves an improvement of \\textbf{0.7}$\\uparrow$,\n\\textbf{2.0}$\\uparrow$, \\textbf{1.6}$\\uparrow$, and \\textbf{11.0}$\\uparrow$ in\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\ndataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.", "AI": {"tldr": "P3HOT\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u63d0\u793a\u5f15\u5bfc\u548c\u4eba\u7c7b\u8fd1\u7aef\u611f\u77e5\uff0c\u6539\u8fdbHOT\u68c0\u6d4b\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u533a\u57df\u5206\u5272\u548c\u7c7b\u522b\u4e00\u81f4\u6027\u4e0a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709HOT\u68c0\u6d4b\u6a21\u578b\u5c40\u9650\u4e8e\u5355\u4e00\u56fe\u50cf\u7c7b\u578b\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u5206\u5272\u548c\u533a\u57df\u7c7b\u522b\u4e0d\u4e00\u81f4\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faP3HOT\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u9a71\u52a8\u7684\u63d0\u793a\u673a\u5236\u548c\u4eba\u7c7b\u8fd1\u7aef\u611f\u77e5\u673a\u5236\uff0c\u52a8\u6001\u611f\u77e5\u5173\u952e\u6df1\u5ea6\u8303\u56f4\uff0c\u5e76\u5f15\u5165\u533a\u57df\u8054\u5408\u635f\u5931\uff08RJLoss\uff09\u548c\u65b0\u8bc4\u4f30\u6307\u6807AD-Acc\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cP3HOT\u5728\u56db\u4e2a\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5177\u4f53\u63d0\u5347\u4e3a0.7\u2191\u30012.0\u2191\u30011.6\u2191\u548c11.0\u2191\u3002", "conclusion": "P3HOT\u901a\u8fc7\u591a\u673a\u5236\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86HOT\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01631", "pdf": "https://arxiv.org/pdf/2507.01631", "abs": "https://arxiv.org/abs/2507.01631", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D\n  Vision Across Altitudes). Version before camera ready. Our code will be made\n  public after the conference", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.", "AI": {"tldr": "Snake-NeRF\u662f\u4e00\u79cd\u6269\u5c55\u5230\u5927\u573a\u666f\u7684NeRF\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u5904\u7406\u548c\u4f18\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u5185\u5b58\u9650\u5236\u4ec5\u9002\u7528\u4e8e\u5c0f\u573a\u666f\uff0c\u9700\u6269\u5c55\u5230\u5927\u573a\u666f\u3002", "method": "\u91c7\u7528\u5206\u5757\u5904\u7406\u3001\u56fe\u50cf\u91cd\u53e0\u88c1\u526a\u30012\u00d72 3D\u74e6\u7247\u7b56\u7565\u548c\u5206\u6bb5\u91c7\u6837\u5668\u3002", "result": "\u5355GPU\u4e0a\u7ebf\u6027\u65f6\u95f4\u5904\u7406\u5927\u536b\u661f\u56fe\u50cf\uff0c\u8d28\u91cf\u65e0\u635f\u5931\u3002", "conclusion": "Snake-NeRF\u6709\u6548\u6269\u5c55\u4e86NeRF\u7684\u5e94\u7528\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u5927\u573a\u666f\u3002"}}
{"id": "2507.01634", "pdf": "https://arxiv.org/pdf/2507.01634", "abs": "https://arxiv.org/abs/2507.01634", "authors": ["Boyuan Sun", "Modi Jin", "Bowen Yin", "Qibin Hou"], "title": "Depth Anything at Any Condition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC", "AI": {"tldr": "DepthAnything-AC\u662f\u4e00\u79cd\u57fa\u7840\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u80fd\u591f\u5728\u591a\u6837\u73af\u5883\u6761\u4ef6\u4e0b\u5de5\u4f5c\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u4e00\u81f4\u6027\u6b63\u5219\u5316\u5fae\u8c03\u548c\u5c0f\u91cf\u672a\u6807\u8bb0\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u7840MDE\u6a21\u578b\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u6076\u52a3\u5929\u6c14\u548c\u4f20\u611f\u5668\u5931\u771f\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u4f2a\u6807\u7b7e\u751f\u6210\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65e0\u76d1\u7763\u4e00\u81f4\u6027\u6b63\u5219\u5316\u5fae\u8c03\u8303\u5f0f\u548c\u7a7a\u95f4\u8ddd\u79bb\u7ea6\u675f\uff0c\u4ee5\u5b66\u4e60\u8865\u4e01\u7ea7\u76f8\u5bf9\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDepthAnything-AC\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u6076\u52a3\u5929\u6c14\u3001\u5408\u6210\u5931\u771f\u548c\u901a\u7528\u573a\u666f\uff09\u4e2d\u5177\u6709\u96f6\u6837\u672c\u80fd\u529b\u3002", "conclusion": "DepthAnything-AC\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01643", "pdf": "https://arxiv.org/pdf/2507.01643", "abs": "https://arxiv.org/abs/2507.01643", "authors": ["Weijie Yin", "Dingkang Yang", "Hongyuan Dong", "Zijian Kang", "Jiacong Wang", "Xiao Liang", "Chao Feng", "Jiao Ran"], "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement", "categories": ["cs.CV"], "comment": "We release SAILViT, a series of versatile vision foundation models", "summary": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent.", "AI": {"tldr": "SAILViT\u662f\u4e00\u79cd\u9010\u6b65\u7279\u5f81\u5b66\u4e60\u589e\u5f3a\u7684Vision Transformer\uff0c\u65e8\u5728\u89e3\u51b3ViT\u4e0eLLM\u76f4\u63a5\u8054\u5408\u8bad\u7ec3\u65f6\u7684\u53c2\u6570\u51b2\u7a81\u548c\u6a21\u6001\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u63d0\u5347MLLM\u5728\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709ViT\u901a\u8fc7\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\u6216\u81ea\u76d1\u7763\u673a\u5236\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u96be\u4ee5\u4e0eLLM\u76f4\u63a5\u8054\u5408\u8bad\u7ec3\uff0c\u5b58\u5728\u53c2\u6570\u521d\u59cb\u5316\u51b2\u7a81\u548c\u6a21\u6001\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002", "method": "\u63d0\u51faSAILViT\uff0c\u91c7\u7528\u9010\u6b65\u7279\u5f81\u7ec6\u5316\u5b9e\u73b0\u7c97\u5230\u7ec6\u7684\u7279\u5f81\u5bf9\u9f50\u548c\u4e16\u754c\u77e5\u8bc6\u6ce8\u5165\uff0c\u9002\u5e94\u76ee\u6807\u8bad\u7ec3\u9700\u6c42\u3002", "result": "SAILViT\u5728\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u3001\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u89c4\u6a21\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u663e\u8457\u63d0\u5347MLLM\u5728OpenCompass\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "SAILViT\u6709\u6548\u89e3\u51b3\u4e86ViT\u4e0eLLM\u8054\u5408\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u4e3aMLLM\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01652", "pdf": "https://arxiv.org/pdf/2507.01652", "abs": "https://arxiv.org/abs/2507.01652", "authors": ["Yuxin Mao", "Zhen Qin", "Jinxing Zhou", "Hui Deng", "Xuyang Shen", "Bin Fan", "Jing Zhang", "Yiran Zhong", "Yuchao Dai"], "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236LASAD\uff0c\u7528\u4e8e\u89e3\u51b3\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u65e0\u6cd5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u4f9d\u8d56Transformer\u67b6\u6784\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u5f00\u9500\u9ad8\uff0c\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faLinear Attention with Spatial-Aware Decay (LASAD)\uff0c\u901a\u8fc72D\u7a7a\u95f4\u4f4d\u7f6e\u8ba1\u7b97\u8870\u51cf\u56e0\u5b50\uff0c\u4fdd\u6301\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u5728ImageNet\u4e0a\uff0cLASADGen\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u751f\u6210\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "LASADGen\u6210\u529f\u5e73\u8861\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u6548\u7387\u548c\u9ad8\u8d28\u91cf\u751f\u6210\u6240\u9700\u7684\u7a7a\u95f4\u7406\u89e3\u3002"}}
{"id": "2507.01653", "pdf": "https://arxiv.org/pdf/2507.01653", "abs": "https://arxiv.org/abs/2507.01653", "authors": ["Yuran Wang", "Yingping Liang", "Yutao Hu", "Ying Fu"], "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather", "categories": ["cs.CV"], "comment": "accepted by ICCV25", "summary": "Learning-based stereo matching models struggle in adverse weather conditions\ndue to the scarcity of corresponding training data and the challenges in\nextracting discriminative features from degraded images. These limitations\nsignificantly hinder zero-shot generalization to out-of-distribution weather\nconditions. In this paper, we propose \\textbf{RobuSTereo}, a novel framework\nthat enhances the zero-shot generalization of stereo matching models under\nadverse weather by addressing both data scarcity and feature extraction\nchallenges. First, we introduce a diffusion-based simulation pipeline with a\nstereo consistency module, which generates high-quality stereo data tailored\nfor adverse conditions. By training stereo matching models on our synthetic\ndatasets, we reduce the domain gap between clean and degraded images,\nsignificantly improving the models' robustness to unseen weather conditions.\nThe stereo consistency module ensures structural alignment across synthesized\nimage pairs, preserving geometric integrity and enhancing depth estimation\naccuracy. Second, we design a robust feature encoder that combines a\nspecialized ConvNet with a denoising transformer to extract stable and reliable\nfeatures from degraded images. The ConvNet captures fine-grained local\nstructures, while the denoising transformer refines global representations,\neffectively mitigating the impact of noise, low visibility, and weather-induced\ndistortions. This enables more accurate disparity estimation even under\nchallenging visual conditions. Extensive experiments demonstrate that\n\\textbf{RobuSTereo} significantly improves the robustness and generalization of\nstereo matching models across diverse adverse weather scenarios.", "AI": {"tldr": "RobuSTereo\u6846\u67b6\u901a\u8fc7\u6269\u6563\u6a21\u62df\u548c\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u6076\u52a3\u5929\u6c14\u4e0b\u7acb\u4f53\u5339\u914d\u6a21\u578b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u4e13\u7528ConvNet\u548c\u53bb\u566aTransformer\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "motivation": "\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\uff0c\u7acb\u4f53\u5339\u914d\u6a21\u578b\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u7279\u5f81\u63d0\u53d6\u56f0\u96be\u800c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51fa\u6269\u6563\u6a21\u62df\u7ba1\u9053\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u8bbe\u8ba1\u7ed3\u5408ConvNet\u548c\u53bb\u566aTransformer\u7684\u7279\u5f81\u7f16\u7801\u5668\u3002", "result": "\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RobuSTereo\u6709\u6548\u89e3\u51b3\u4e86\u6076\u52a3\u5929\u6c14\u4e0b\u7acb\u4f53\u5339\u914d\u7684\u6311\u6218\u3002"}}
{"id": "2507.01654", "pdf": "https://arxiv.org/pdf/2507.01654", "abs": "https://arxiv.org/abs/2507.01654", "authors": ["Martine Hjelkrem-Tan", "Marius Aasan", "Gabriel Y. Arteaga", "Ad\u00edn Ram\u00edrez Rivera"], "title": "SPoT: Subpixel Placement of Tokens in Vision Transformers", "categories": ["cs.CV", "cs.LG"], "comment": "To appear in Workshop on Efficient Computing under Limited Resources:\n  Visual Computing (ICCV 2025). Code available at\n  https://github.com/dsb-ifi/SPoT", "summary": "Vision Transformers naturally accommodate sparsity, yet standard tokenization\nmethods confine features to discrete patch grids. This constraint prevents\nmodels from fully exploiting sparse regimes, forcing awkward compromises. We\npropose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that\npositions tokens continuously within images, effectively sidestepping\ngrid-based limitations. With our proposed oracle-guided search, we uncover\nsubstantial performance gains achievable with ideal subpixel token positioning,\ndrastically reducing the number of tokens necessary for accurate predictions\nduring inference. SPoT provides a new direction for flexible, efficient, and\ninterpretable ViT architectures, redefining sparsity as a strategic advantage\nrather than an imposed limitation.", "AI": {"tldr": "SPoT\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u89c9Transformer\u6807\u8bb0\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u8fde\u7eed\u653e\u7f6e\u6807\u8bb0\u907f\u514d\u7f51\u683c\u9650\u5236\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6240\u9700\u7684\u6807\u8bb0\u6570\u91cf\u3002", "motivation": "\u6807\u51c6\u6807\u8bb0\u5316\u65b9\u6cd5\u5c06\u7279\u5f81\u9650\u5236\u5728\u79bb\u6563\u7684\u7f51\u683c\u4e2d\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5728\u7a00\u758f\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faSubpixel Placement of Tokens (SPoT)\uff0c\u901a\u8fc7\u8fde\u7eed\u653e\u7f6e\u6807\u8bb0\uff0c\u5e76\u7ed3\u5408oracle-guided\u641c\u7d22\u4f18\u5316\u4f4d\u7f6e\u3002", "result": "SPoT\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u6240\u9700\u7684\u6807\u8bb0\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "SPoT\u4e3a\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684ViT\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u6218\u7565\u4f18\u52bf\u3002"}}
{"id": "2507.01667", "pdf": "https://arxiv.org/pdf/2507.01667", "abs": "https://arxiv.org/abs/2507.01667", "authors": ["Gianluca Monaci", "Philippe Weinzaepfel", "Christian Wolf"], "title": "What does really matter in image goal navigation?", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Image goal navigation requires two different skills: firstly, core navigation\nskills, including the detection of free space and obstacles, and taking\ndecisions based on an internal representation; and secondly, computing\ndirectional information by comparing visual observations to the goal image.\nCurrent state-of-the-art methods either rely on dedicated image-matching, or\npre-training of computer vision modules on relative pose estimation. In this\npaper, we study whether this task can be efficiently solved with end-to-end\ntraining of full agents with RL, as has been claimed by recent work. A positive\nanswer would have impact beyond Embodied AI and allow training of relative pose\nestimation from reward for navigation alone. In a large study we investigate\nthe effect of architectural choices like late fusion, channel stacking,\nspace-to-depth projections and cross-attention, and their role in the emergence\nof relative pose estimators from navigation training. We show that the success\nof recent methods is influenced up to a certain extent by simulator settings,\nleading to shortcuts in simulation. However, we also show that these\ncapabilities can be transferred to more realistic setting, up to some extend.\nWe also find evidence for correlations between navigation performance and\nprobed (emerging) relative pose estimation performance, an important sub skill.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5206\u6790\u4e86\u67b6\u6784\u9009\u62e9\u548c\u6a21\u62df\u5668\u8bbe\u7f6e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u90e8\u5206\u80fd\u529b\u53ef\u8fc1\u79fb\u5230\u66f4\u73b0\u5b9e\u7684\u573a\u666f\u3002", "motivation": "\u9a8c\u8bc1\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u662f\u5426\u80fd\u5728\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u9ad8\u6548\u89e3\u51b3\u6838\u5fc3\u5bfc\u822a\u548c\u65b9\u5411\u8ba1\u7b97\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u4e13\u7528\u56fe\u50cf\u5339\u914d\u6216\u9884\u8bad\u7ec3\u6a21\u5757\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u7814\u7a76\u5206\u6790\u4e0d\u540c\u67b6\u6784\u9009\u62e9\uff08\u5982\u5ef6\u8fdf\u878d\u5408\u3001\u901a\u9053\u5806\u53e0\u7b49\uff09\u5bf9\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u5668\u4ece\u5bfc\u822a\u8bad\u7ec3\u4e2d\u6d8c\u73b0\u7684\u5f71\u54cd\uff0c\u5e76\u8003\u5bdf\u6a21\u62df\u5668\u8bbe\u7f6e\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u6a21\u62df\u5668\u8bbe\u7f6e\u4f1a\u5f71\u54cd\u6027\u80fd\uff0c\u4f46\u90e8\u5206\u80fd\u529b\u53ef\u8fc1\u79fb\u5230\u66f4\u73b0\u5b9e\u573a\u666f\uff1b\u5bfc\u822a\u6027\u80fd\u4e0e\u6d8c\u73b0\u7684\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u80fd\u529b\u76f8\u5173\u3002", "conclusion": "\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6ce8\u610f\u6a21\u62df\u5668\u8bbe\u7f6e\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a2\u7d22\u80fd\u529b\u8fc1\u79fb\u548c\u6027\u80fd\u76f8\u5173\u6027\u3002"}}
{"id": "2507.01673", "pdf": "https://arxiv.org/pdf/2507.01673", "abs": "https://arxiv.org/abs/2507.01673", "authors": ["Muzammil Behzad"], "title": "Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) in 3D and 4D domains presents a\nsignificant challenge in affective computing due to the complexity of spatial\nand temporal facial dynamics. Its success is crucial for advancing applications\nin human behavior understanding, healthcare monitoring, and human-computer\ninteraction. In this work, we propose FACET-VLM, a vision-language framework\nfor 3D/4D FER that integrates multiview facial representation learning with\nsemantic guidance from natural language prompts. FACET-VLM introduces three key\ncomponents: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,\nMultiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,\nand a multiview consistency loss to enforce structural coherence across views.\nOur model achieves state-of-the-art accuracy across multiple benchmarks,\nincluding BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend\nFACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,\ndemonstrating strong performance in capturing subtle, short-lived emotional\ncues. The extensive experimental results confirm the effectiveness and\nsubstantial contributions of each individual component within the framework.\nOverall, FACET-VLM offers a robust, extensible, and high-performing solution\nfor multimodal FER in both posed and spontaneous settings.", "AI": {"tldr": "FACET-VLM\u662f\u4e00\u4e2a\u7528\u4e8e3D/4D\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8868\u793a\u5b66\u4e60\u548c\u8bed\u4e49\u5f15\u5bfc\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u3002", "motivation": "3D/4D\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u5728\u60c5\u611f\u8ba1\u7b97\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\u3001\u533b\u7597\u76d1\u6d4b\u548c\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faCVSA\u3001MTGF\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u635f\u5931\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u6574\u5408\u591a\u89c6\u89d2\u8868\u793a\u5b66\u4e60\u548c\u8bed\u4e49\u5f15\u5bfc\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u6210\u529f\u6269\u5c55\u52304D\u5fae\u8868\u60c5\u8bc6\u522b\u3002", "conclusion": "FACET-VLM\u4e3a\u591a\u6a21\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01711", "pdf": "https://arxiv.org/pdf/2507.01711", "abs": "https://arxiv.org/abs/2507.01711", "authors": ["Mingfu Yan", "Jiancheng Huang", "Yifan Liu", "Shifeng Chen"], "title": "Component Adaptive Clustering for Generalized Category Discovery", "categories": ["cs.CV"], "comment": "Accepted by IEEE ICME 2025", "summary": "Generalized Category Discovery (GCD) tackles the challenging problem of\ncategorizing unlabeled images into both known and novel classes within a\npartially labeled dataset, without prior knowledge of the number of unknown\ncategories. Traditional methods often rely on rigid assumptions, such as\npredefining the number of classes, which limits their ability to handle the\ninherent variability and complexity of real-world data. To address these\nshortcomings, we propose AdaGCD, a cluster-centric contrastive learning\nframework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD\nframework. AdaSlot dynamically determines the optimal number of slots based on\ndata complexity, removing the need for predefined slot counts. This adaptive\nmechanism facilitates the flexible clustering of unlabeled data into known and\nnovel categories by dynamically allocating representational capacity. By\nintegrating adaptive representation with dynamic slot allocation, our method\ncaptures both instance-specific and spatially clustered features, improving\nclass discovery in open-world scenarios. Extensive experiments on public and\nfine-grained datasets validate the effectiveness of our framework, emphasizing\nthe advantages of leveraging spatial local information for category discovery\nin unlabeled image datasets.", "AI": {"tldr": "AdaGCD\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u90e8\u5206\u6807\u8bb0\u6570\u636e\u96c6\u4e2d\u53d1\u73b0\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u7c7b\u522b\u6570\u91cf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7c7b\u522b\u6570\u91cf\u7684\u5047\u8bbe\uff0c\u65e0\u6cd5\u5904\u7406\u771f\u5b9e\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51faAdaGCD\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\uff08AdaSlot\uff09\uff0c\u52a8\u6001\u786e\u5b9a\u69fd\u7684\u6570\u91cf\uff0c\u5b9e\u73b0\u7075\u6d3b\u805a\u7c7b\u3002", "result": "\u5728\u516c\u5f00\u548c\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5229\u7528\u7a7a\u95f4\u5c40\u90e8\u4fe1\u606f\u5728\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u53d1\u73b0\u7c7b\u522b\u7684\u4f18\u52bf\u3002", "conclusion": "AdaGCD\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u548c\u52a8\u6001\u69fd\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u7c7b\u522b\u53d1\u73b0\u80fd\u529b\u3002"}}
{"id": "2507.01712", "pdf": "https://arxiv.org/pdf/2507.01712", "abs": "https://arxiv.org/abs/2507.01712", "authors": ["Xinle Tian", "Matthew Nunes", "Emiko Dupont", "Shaunagh Downing", "Freddie Lichtenstein", "Matt Burns"], "title": "Using Wavelet Domain Fingerprints to Improve Source Camera Identification", "categories": ["cs.CV", "eess.IV", "stat.AP"], "comment": null, "summary": "Camera fingerprint detection plays a crucial role in source identification\nand image forensics, with wavelet denoising approaches proving to be\nparticularly effective in extracting sensor pattern noise (SPN). In this\narticle, we propose a modification to wavelet-based SPN extraction. Rather than\nconstructing the fingerprint as an image, we introduce the notion of a wavelet\ndomain fingerprint. This avoids the final inversion step of the denoising\nalgorithm and allows fingerprint comparisons to be made directly in the wavelet\ndomain. As such, our modification streamlines the extraction and comparison\nprocess. Experimental results on real-world datasets demonstrate that our\nmethod not only achieves higher detection accuracy but can also significantly\nimprove processing speed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u57df\u7684\u76f8\u673a\u6307\u7eb9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u53cd\u6f14\u6b65\u9aa4\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u5c0f\u6ce2\u53bb\u566a\u65b9\u6cd5\u5728\u63d0\u53d6\u4f20\u611f\u5668\u6a21\u5f0f\u566a\u58f0\uff08SPN\uff09\u65f6\u9700\u8981\u53cd\u6f14\u6b65\u9aa4\uff0c\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u5c06\u6307\u7eb9\u6784\u9020\u4e3a\u5c0f\u6ce2\u57df\u6307\u7eb9\uff0c\u76f4\u63a5\u5728\u9891\u57df\u8fdb\u884c\u6bd4\u8f83\uff0c\u907f\u514d\u53cd\u6f14\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4e0d\u4ec5\u68c0\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u8fd8\u80fd\u663e\u8457\u63d0\u5347\u5904\u7406\u901f\u5ea6\u3002", "conclusion": "\u5c0f\u6ce2\u57df\u6307\u7eb9\u65b9\u6cd5\u7b80\u5316\u4e86\u63d0\u53d6\u548c\u6bd4\u8f83\u6d41\u7a0b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.01721", "pdf": "https://arxiv.org/pdf/2507.01721", "abs": "https://arxiv.org/abs/2507.01721", "authors": ["Zhongwen Zhang", "Yuri Boykov"], "title": "Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation", "categories": ["cs.CV"], "comment": "published at CVPR 2025", "summary": "We consider weakly supervised segmentation where only a fraction of pixels\nhave ground truth labels (scribbles) and focus on a self-labeling approach\noptimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled\npixels. While WSSS methods can directly optimize such losses via gradient\ndescent, prior work suggests that higher-order optimization can improve network\ntraining by introducing hidden pseudo-labels and powerful CRF sub-problem\nsolvers, e.g. graph cut. However, previously used hard pseudo-labels can not\nrepresent class uncertainty or errors, which motivates soft self-labeling. We\nderive a principled auxiliary loss and systematically evaluate standard and new\nCRF relaxations (convex and non-convex), neighborhood systems, and terms\nconnecting network predictions with soft pseudo-labels. We also propose a\ngeneral continuous sub-problem solver. Using only standard architectures, soft\nself-labeling consistently improves scribble-based training and outperforms\nsignificantly more complex specialized WSSS systems. It can outperform full\npixel-precise supervision. Our general ideas apply to other weakly-supervised\nproblems/systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u81ea\u6807\u8bb0\u7684\u5f31\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316CRF/Potts\u635f\u5931\u7684\u677e\u5f1b\u5f62\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6d82\u9e26\u6807\u7b7e\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u751a\u81f3\u4f18\u4e8e\u5168\u50cf\u7d20\u7ea7\u76d1\u7763\u3002", "motivation": "\u89e3\u51b3\u5f31\u76d1\u7763\u5206\u5272\u4e2d\u4ec5\u90e8\u5206\u50cf\u7d20\u6709\u6807\u7b7e\uff08\u6d82\u9e26\uff09\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8f6f\u81ea\u6807\u8bb0\u6539\u8fdb\u786c\u4f2a\u6807\u7b7e\u65e0\u6cd5\u8868\u793a\u7c7b\u522b\u4e0d\u786e\u5b9a\u6027\u548c\u9519\u8bef\u7684\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u81ea\u6807\u8bb0\u7684\u8f85\u52a9\u635f\u5931\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6807\u51c6\u548c\u65b0CRF\u677e\u5f1b\uff08\u51f8\u548c\u975e\u51f8\uff09\u3001\u90bb\u57df\u7cfb\u7edf\u4ee5\u53ca\u7f51\u7edc\u9884\u6d4b\u4e0e\u8f6f\u4f2a\u6807\u7b7e\u7684\u8fde\u63a5\u9879\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8fde\u7eed\u5b50\u95ee\u9898\u6c42\u89e3\u5668\u3002", "result": "\u8f6f\u81ea\u6807\u8bb0\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6d82\u9e26\u6807\u7b7e\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u4f18\u4e8e\u66f4\u590d\u6742\u7684\u4e13\u7528WSSS\u7cfb\u7edf\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u5168\u50cf\u7d20\u7ea7\u76d1\u7763\u3002", "conclusion": "\u8f6f\u81ea\u6807\u8bb0\u662f\u4e00\u79cd\u6709\u6548\u7684\u5f31\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u5176\u901a\u7528\u601d\u60f3\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u5f31\u76d1\u7763\u95ee\u9898/\u7cfb\u7edf\u3002"}}
{"id": "2507.01722", "pdf": "https://arxiv.org/pdf/2507.01722", "abs": "https://arxiv.org/abs/2507.01722", "authors": ["Enrico Cassano", "Riccardo Renzulli", "Andrea Bragagnolo", "Marco Grangetto"], "title": "When Does Pruning Benefit Vision Representations?", "categories": ["cs.CV"], "comment": null, "summary": "Pruning is widely used to reduce the complexity of deep learning models, but\nits effects on interpretability and representation learning remain poorly\nunderstood. This paper investigates how pruning influences vision models across\nthree key dimensions: (i) interpretability, (ii) unsupervised object discovery,\nand (iii) alignment with human perception. We first analyze different vision\nnetwork architectures to examine how varying sparsity levels affect feature\nattribution interpretability methods. Additionally, we explore whether pruning\npromotes more succinct and structured representations, potentially improving\nunsupervised object discovery by discarding redundant information while\npreserving essential features. Finally, we assess whether pruning enhances the\nalignment between model representations and human perception, investigating\nwhether sparser models focus on more discriminative features similarly to\nhumans. Our findings also reveal the presence of sweet spots, where sparse\nmodels exhibit higher interpretability, downstream generalization and human\nalignment. However, these spots highly depend on the network architectures and\ntheir size in terms of trainable parameters. Our results suggest a complex\ninterplay between these three dimensions, highlighting the importance of\ninvestigating when and how pruning benefits vision representations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u526a\u679d\u5bf9\u89c6\u89c9\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u3001\u65e0\u76d1\u7763\u76ee\u6807\u53d1\u73b0\u548c\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b58\u5728\u201c\u6700\u4f73\u70b9\u201d\u4f7f\u7a00\u758f\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u8ba8\u526a\u679d\u5982\u4f55\u5f71\u54cd\u6df1\u5ea6\u89c6\u89c9\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u8868\u793a\u5b66\u4e60\u548c\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e0d\u540c\u89c6\u89c9\u7f51\u7edc\u67b6\u6784\u5728\u4e0d\u540c\u7a00\u758f\u5ea6\u4e0b\u7684\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\uff0c\u7814\u7a76\u526a\u679d\u662f\u5426\u4fc3\u8fdb\u66f4\u7b80\u6d01\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5e76\u8bc4\u4f30\u526a\u679d\u662f\u5426\u589e\u5f3a\u6a21\u578b\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5bf9\u9f50\u3002", "result": "\u53d1\u73b0\u7a00\u758f\u6a21\u578b\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff08\u201c\u6700\u4f73\u70b9\u201d\uff09\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u3001\u4e0b\u6e38\u6cdb\u5316\u80fd\u529b\u548c\u4eba\u7c7b\u5bf9\u9f50\u6027\uff0c\u4f46\u8fd9\u4e9b\u6761\u4ef6\u9ad8\u5ea6\u4f9d\u8d56\u7f51\u7edc\u67b6\u6784\u548c\u53c2\u6570\u89c4\u6a21\u3002", "conclusion": "\u526a\u679d\u4e0e\u89c6\u89c9\u6a21\u578b\u8868\u73b0\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u5173\u7cfb\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u526a\u679d\u4f55\u65f6\u53ca\u5982\u4f55\u4f18\u5316\u89c6\u89c9\u8868\u793a\u3002"}}
{"id": "2507.01735", "pdf": "https://arxiv.org/pdf/2507.01735", "abs": "https://arxiv.org/abs/2507.01735", "authors": ["Kai Chen", "Ruiyuan Gao", "Lanqing Hong", "Hang Xu", "Xu Jia", "Holger Caesar", "Dengxin Dai", "Bingbing Liu", "Dzmitry Tsishkou", "Songcen Xu", "Chunjing Xu", "Qiang Xu", "Huchuan Lu", "Dit-Yan Yeung"], "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/", "summary": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u5c4aW-CODA\u7814\u8ba8\u4f1a\u7684\u8be6\u60c5\uff0c\u805a\u7126\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6781\u7aef\u573a\u666f\u7684\u4e0b\u4e00\u4ee3\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63a2\u7d22\u524d\u6cbf\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u7406\u89e3\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6781\u7aef\u573a\u666f\u95ee\u9898\u3002", "method": "\u9080\u8bf75\u4f4d\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u4e13\u5bb6\u5206\u4eab\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u4e3e\u529e\u53cc\u8f68\u6311\u6218\u8d5b\uff08\u6781\u7aef\u573a\u666f\u7406\u89e3\u4e0e\u751f\u6210\uff09\u3002", "result": "\u7814\u8ba8\u4f1a\u6c47\u96c6\u4e86\u7814\u7a76\u8bba\u6587\u548c\u6311\u6218\u8d5b\u6210\u679c\uff0c\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u8fdb\u6b65\u3002", "conclusion": "W-CODA\u5c06\u6301\u7eed\u5f25\u5408\u524d\u6cbf\u6280\u672f\u4e0e\u5b8c\u5168\u667a\u80fd\u3001\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.01737", "pdf": "https://arxiv.org/pdf/2507.01737", "abs": "https://arxiv.org/abs/2507.01737", "authors": ["Lin Wu", "Zhixiang Chen", "Jianglin Lan"], "title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions.", "AI": {"tldr": "HOI-Dyn\u6846\u67b6\u901a\u8fc7\u9a71\u52a8-\u54cd\u5e94\u7cfb\u7edf\u751f\u6210\u903c\u771f\u76843D\u4eba-\u7269\u4ea4\u4e92\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7Transformer\u6a21\u578b\u9884\u6d4b\u7269\u4f53\u5bf9\u4eba\u7269\u52a8\u4f5c\u7684\u53cd\u5e94\uff0c\u5e76\u901a\u8fc7\u6b8b\u5dee\u52a8\u529b\u5b66\u635f\u5931\u63d0\u5347\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u4eba\u7269\u548c\u7269\u4f53\u8fd0\u52a8\uff0c\u5bfc\u81f4\u4ea4\u4e92\u884c\u4e3a\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u4e14\u56e0\u679c\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faHOI-Dyn\u6846\u67b6\uff0c\u5c06\u4ea4\u4e92\u5efa\u6a21\u4e3a\u9a71\u52a8-\u54cd\u5e94\u7cfb\u7edf\uff0c\u4f7f\u7528Transformer\u9884\u6d4b\u7269\u4f53\u54cd\u5e94\uff0c\u5e76\u5f15\u5165\u6b8b\u5dee\u52a8\u529b\u5b66\u635f\u5931\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHOI-Dyn\u63d0\u5347\u4e86\u4ea4\u4e92\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "HOI-Dyn\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4ea4\u4e92\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u4e86\u66f4\u903c\u771f\u548c\u4e00\u81f4\u7684\u4eba-\u7269\u4ea4\u4e92\u751f\u6210\u3002"}}
{"id": "2507.01738", "pdf": "https://arxiv.org/pdf/2507.01738", "abs": "https://arxiv.org/abs/2507.01738", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-jiang Liu", "Sen Yang", "Wenxiao Cai", "Yanpeng Sun", "Wankou Yang"], "title": "DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Referring Image Segmentation (RIS) is a challenging task that aims to segment\nobjects in an image based on natural language expressions. While prior studies\nhave predominantly concentrated on improving vision-language interactions and\nachieving fine-grained localization, a systematic analysis of the fundamental\nbottlenecks in existing RIS frameworks remains underexplored. To bridge this\ngap, we propose DeRIS, a novel framework that decomposes RIS into two key\ncomponents: perception and cognition. This modular decomposition facilitates a\nsystematic analysis of the primary bottlenecks impeding RIS performance. Our\nfindings reveal that the predominant limitation lies not in perceptual\ndeficiencies, but in the insufficient multi-modal cognitive capacity of current\nmodels. To mitigate this, we propose a Loopback Synergy mechanism, which\nenhances the synergy between the perception and cognition modules, thereby\nenabling precise segmentation while simultaneously improving robust image-text\ncomprehension. Additionally, we analyze and introduce a simple non-referent\nsample conversion data augmentation to address the long-tail distribution issue\nrelated to target existence judgement in general scenarios. Notably, DeRIS\ndemonstrates inherent adaptability to both non- and multi-referents scenarios\nwithout requiring specialized architectural modifications, enhancing its\ngeneral applicability. The codes and models are available at\nhttps://github.com/Dmmm1997/DeRIS.", "AI": {"tldr": "DeRIS\u6846\u67b6\u5c06Referring Image Segmentation\u5206\u89e3\u4e3a\u611f\u77e5\u548c\u8ba4\u77e5\u4e24\u4e2a\u6a21\u5757\uff0c\u63d0\u51faLoopback Synergy\u673a\u5236\u589e\u5f3a\u6a21\u5757\u95f4\u534f\u540c\uff0c\u5e76\u5f15\u5165\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RIS\u6846\u67b6\u7f3a\u4e4f\u5bf9\u6027\u80fd\u74f6\u9888\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u5c24\u5176\u662f\u591a\u6a21\u6001\u8ba4\u77e5\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "method": "DeRIS\u5c06RIS\u5206\u89e3\u4e3a\u611f\u77e5\u548c\u8ba4\u77e5\u6a21\u5757\uff0c\u63d0\u51faLoopback Synergy\u673a\u5236\uff0c\u5e76\u5f15\u5165\u975e\u53c2\u7167\u6837\u672c\u8f6c\u6362\u6570\u636e\u589e\u5f3a\u3002", "result": "DeRIS\u5728\u975e\u548c\u591a\u53c2\u7167\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u4e13\u95e8\u67b6\u6784\u4fee\u6539\u3002", "conclusion": "DeRIS\u901a\u8fc7\u6a21\u5757\u5316\u5206\u6790\u548c\u534f\u540c\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86RIS\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.01744", "pdf": "https://arxiv.org/pdf/2507.01744", "abs": "https://arxiv.org/abs/2507.01744", "authors": ["Benjamin Jin", "Grant Mair", "Joanna M. Wardlaw", "Maria del C. Vald\u00e9s Hern\u00e1ndez"], "title": "Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) have gained significant popularity in the natural\nimage domain but have been less successful in 3D medical image segmentation.\nNevertheless, 3D ViTs are particularly interesting for large medical imaging\nvolumes due to their efficient self-supervised training within the masked\nautoencoder (MAE) framework, which enables the use of imaging data without the\nneed for expensive manual annotations. intracranial arterial calcification\n(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to\nneurovascular diseases such as stroke and dementia, and automated IAC\nquantification could enable their large-scale risk assessment. We pre-train\nViTs with MAE and fine-tune them for IAC segmentation for the first time. To\ndevelop our models, we use highly heterogeneous data from a large clinical\ntrial, the third International Stroke Trial (IST-3). We evaluate key aspects of\nMAE pre-trained ViTs in IAC segmentation, and analyse the clinical\nimplications. We show: 1) our calibrated self-supervised ViT beats a strong\nsupervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial\nfor ViTs for IAC segmentation and interpolation upsampling with regular\nconvolutions is preferable to transposed convolutions for ViT-based models, and\n3) our ViTs increase robustness to higher slice thicknesses and improve risk\ngroup classification in a clinical scenario by 46%. Our code is available\nonline.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eVision Transformers\uff08ViTs\uff09\u548c\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u6846\u67b6\u76843D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u5176\u5e94\u7528\u4e8e\u9885\u5185\u52a8\u8109\u9499\u5316\uff08IAC\uff09\u7684\u81ea\u52a8\u91cf\u5316\uff0c\u5e76\u5728\u4e34\u5e8a\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "3D ViTs\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5176\u5728MAE\u6846\u67b6\u4e0b\u7684\u9ad8\u6548\u81ea\u76d1\u7763\u8bad\u7ec3\u7279\u6027\u4f7f\u5176\u9002\u7528\u4e8e\u65e0\u9700\u6602\u8d35\u6807\u6ce8\u7684\u5927\u89c4\u6a21\u533b\u5b66\u5f71\u50cf\u6570\u636e\u3002IAC\u4f5c\u4e3a\u795e\u7ecf\u8840\u7ba1\u75be\u75c5\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5176\u81ea\u52a8\u91cf\u5316\u6709\u52a9\u4e8e\u5927\u89c4\u6a21\u98ce\u9669\u8bc4\u4f30\u3002", "method": "\u91c7\u7528MAE\u9884\u8bad\u7ec3ViTs\uff0c\u5e76\u5728IST-3\u4e34\u5e8a\u8bd5\u9a8c\u7684\u5f02\u6784\u6570\u636e\u4e0a\u5fae\u8c03\u7528\u4e8eIAC\u5206\u5272\u3002\u7814\u7a76\u4e86\u4f4epatch\u5927\u5c0f\u548c\u63d2\u503c\u4e0a\u91c7\u6837\u5bf9ViTs\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u81ea\u76d1\u7763ViT\u5728Dice\u5206\u6570\u4e0a\u6bd4\u76d1\u7763nnU-Net\u57fa\u7ebf\u9ad83.2\u5206\uff1b\u4f4epatch\u5927\u5c0f\u548c\u63d2\u503c\u4e0a\u91c7\u6837\u5bf9ViTs\u8868\u73b0\u81f3\u5173\u91cd\u8981\uff1bViTs\u5bf9\u9ad8\u5207\u7247\u539a\u5ea6\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4e34\u5e8a\u98ce\u9669\u5206\u7c7b\u63d0\u534746%\u3002", "conclusion": "MAE\u9884\u8bad\u7ec3\u7684ViTs\u5728IAC\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01747", "pdf": "https://arxiv.org/pdf/2507.01747", "abs": "https://arxiv.org/abs/2507.01747", "authors": ["Nora Gourmelon", "Marcel Dreier", "Martin Mayr", "Thorsten Seehaus", "Dakota Pyles", "Matthias Braun", "Andreas Maier", "Vincent Christlein"], "title": "SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery", "categories": ["cs.CV"], "comment": "in IEEE Transactions on Geoscience and Remote Sensing. arXiv admin\n  note: text overlap with arXiv:2501.05281", "summary": "Glaciers are losing ice mass at unprecedented rates, increasing the need for\naccurate, year-round monitoring to understand frontal ablation, particularly\nthe factors driving the calving process. Deep learning models can extract\ncalving front positions from Synthetic Aperture Radar imagery to track seasonal\nice losses at the calving fronts of marine- and lake-terminating glaciers. The\ncurrent state-of-the-art model relies on ImageNet-pretrained weights. However,\nthey are suboptimal due to the domain shift between the natural images in\nImageNet and the specialized characteristics of remote sensing imagery, in\nparticular for Synthetic Aperture Radar imagery. To address this challenge, we\npropose two novel self-supervised multimodal pretraining techniques that\nleverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14\nSentinel-2 images of Arctic glaciers, with one optical image per glacier in the\ndataset. Additionally, we introduce a novel hybrid model architecture that\ncombines a Swin Transformer encoder with a residual Convolutional Neural\nNetwork (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean\ndistance error of 293 m on the \"CAlving Fronts and where to Find thEm\" (CaFFe)\nbenchmark dataset, outperforming the prior best model by 67 m. Evaluating an\nensemble of the proposed model on a multi-annotator study of the benchmark\ndataset reveals a mean distance error of 75 m, approaching the human\nperformance of 38 m. This advancement enables precise monitoring of seasonal\nchanges in glacier calving fronts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u81ea\u76d1\u7763\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6280\u672f\u548c\u4e00\u79cd\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u4e2d\u63d0\u53d6\u51b0\u5ddd\u5d29\u89e3\u524d\u6cbf\u4f4d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76d1\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u51b0\u5ddd\u51b0\u91cf\u635f\u5931\u52a0\u5267\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5168\u5e74\u76d1\u6d4b\u65b9\u6cd5\u4ee5\u7406\u89e3\u5d29\u89e3\u8fc7\u7a0b\u3002\u73b0\u6709\u57fa\u4e8eImageNet\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u56e0\u9886\u57df\u5dee\u5f02\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u81ea\u76d1\u7763\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u5229\u7528\u65b0\u6570\u636e\u96c6SSL4SAR\uff0c\u5e76\u8bbe\u8ba1\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff08Swin Transformer\u7f16\u7801\u5668+\u6b8b\u5deeCNN\u89e3\u7801\u5668\uff09\u3002", "result": "\u5728CaFFe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5e73\u5747\u8ddd\u79bb\u8bef\u5dee\u4e3a293\u7c73\uff0c\u4f18\u4e8e\u4e4b\u524d\u6700\u4f73\u6a21\u578b67\u7c73\uff1b\u96c6\u6210\u6a21\u578b\u8bef\u5dee\u964d\u81f375\u7c73\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0838\u7c73\uff09\u3002", "conclusion": "\u8be5\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u51b0\u5ddd\u5d29\u89e3\u524d\u6cbf\u7684\u5b63\u8282\u6027\u53d8\u5316\u76d1\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2507.01756", "pdf": "https://arxiv.org/pdf/2507.01756", "abs": "https://arxiv.org/abs/2507.01756", "authors": ["Peng Zheng", "Junke Wang", "Yi Chang", "Yizhou Yu", "Rui Ma", "Zuxuan Wu"], "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis", "categories": ["cs.CV"], "comment": "accepted by iccv 2025", "summary": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin.", "AI": {"tldr": "DisCon\u6846\u67b6\u901a\u8fc7\u5c06\u79bb\u6563\u6807\u8bb0\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u800c\u975e\u751f\u6210\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u8868\u793a\u5efa\u6a21\u7684\u4f18\u5316\u6311\u6218\uff0c\u540c\u65f6\u907f\u514d\u4e86\u91cf\u5316\u5e26\u6765\u7684\u4fe1\u606f\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u5c55\u6fc0\u53d1\u4e86\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u79bb\u6563\u6807\u8bb0\u7684\u5174\u8da3\uff0c\u4f46\u57fa\u4e8e\u81ea\u56de\u5f52\uff08AR\uff09\u7684\u89c6\u89c9\u751f\u6210\u6a21\u578b\u56e0\u91cf\u5316\u8fc7\u7a0b\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\uff0c\u964d\u4f4e\u4e86\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002", "method": "DisCon\u6846\u67b6\u91cd\u65b0\u5c06\u79bb\u6563\u6807\u8bb0\u89e3\u91ca\u4e3a\u6761\u4ef6\u4fe1\u53f7\uff0c\u5efa\u6a21\u8fde\u7eed\u8868\u793a\u5728\u79bb\u6563\u6807\u8bb0\u6761\u4ef6\u4e0b\u7684\u6982\u7387\u5206\u5e03\uff0c\u4ece\u800c\u907f\u514d\u76f4\u63a5\u5efa\u6a21\u8fde\u7eed\u8868\u793a\u7684\u6311\u6218\u3002", "result": "DisCon\u5728ImageNet 256\u00d7256\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e861.38\u7684gFID\u5206\u6570\uff0c\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u3002", "conclusion": "DisCon\u901a\u8fc7\u7ed3\u5408\u79bb\u6563\u548c\u8fde\u7eed\u8868\u793a\u7684\u4f18\u52bf\uff0c\u4e3a\u89c6\u89c9\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01788", "pdf": "https://arxiv.org/pdf/2507.01788", "abs": "https://arxiv.org/abs/2507.01788", "authors": ["Montasir Shams", "Chashi Mahiul Islam", "Shaeke Salman", "Phat Tran", "Xiuwen Liu"], "title": "Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages", "summary": "Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u89c6\u89c9\u53d8\u6362\u5668\uff08ViTs\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u793a\u7f3a\u4e4f\u8bed\u4e49\u610f\u4e49\uff0c\u4e14\u5bf9\u5fae\u5c0f\u53d8\u5316\u5177\u6709\u8106\u5f31\u6027\uff0c\u5bfc\u81f4\u5206\u7c7b\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "motivation": "\u7814\u7a76ViTs\u5728\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8868\u793a\u662f\u5426\u5177\u6709\u8bed\u4e49\u610f\u4e49\uff0c\u4ee5\u53ca\u5176\u5bf9\u6297\u5fae\u5c0f\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6295\u5f71\u68af\u5ea6\u7684\u7b97\u6cd5\u5206\u6790ViTs\u7684\u8868\u793a\u3002", "result": "ViTs\u7684\u8868\u793a\u7f3a\u4e4f\u8bed\u4e49\u610f\u4e49\uff0c\u5bf9\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u53ef\u80fd\u4e0b\u964d\u8d85\u8fc760%\u3002", "conclusion": "ViTs\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u793a\u5b58\u5728\u6839\u672c\u6027\u95ee\u9898\uff0c\u53ef\u80fd\u5f71\u54cd\u5176\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2507.01791", "pdf": "https://arxiv.org/pdf/2507.01791", "abs": "https://arxiv.org/abs/2507.01791", "authors": ["Zihong Guo", "Chen Wan", "Yayin Zheng", "Hailing Kuang", "Xiaohai Lu"], "title": "Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation", "categories": ["cs.CV"], "comment": null, "summary": "The transferability of adversarial examples poses a significant security\nchallenge for deep neural networks, which can be attacked without knowing\nanything about them. In this paper, we propose a new Segmented Gaussian Pyramid\n(SGP) attack method to enhance the transferability, particularly against\ndefense models. Unlike existing methods that generally focus on single-scale\nimages, our approach employs Gaussian filtering and three types of downsampling\nto construct a series of multi-scale examples. Then, the gradients of the loss\nfunction with respect to each scale are computed, and their average is used to\ndetermine the adversarial perturbations. The proposed SGP can be considered an\ninput transformation with high extensibility that is easily integrated into\nmost existing adversarial attacks. Extensive experiments demonstrate that in\ncontrast to the state-of-the-art methods, SGP significantly enhances attack\nsuccess rates against black-box defense models, with average attack success\nrates increasing by 2.3% to 32.6%, based only on transferability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6bb5\u9ad8\u65af\u91d1\u5b57\u5854\uff08SGP\uff09\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u56fe\u50cf\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u9632\u5fa1\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u6210\u91cd\u5927\u5b89\u5168\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u5c3a\u5ea6\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u653b\u51fb\u6548\u679c\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6ee4\u6ce2\u548c\u4e09\u79cd\u4e0b\u91c7\u6837\u65b9\u6cd5\u6784\u5efa\u591a\u5c3a\u5ea6\u6837\u672c\uff0c\u8ba1\u7b97\u5404\u5c3a\u5ea6\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\u5e76\u53d6\u5e73\u5747\u4ee5\u786e\u5b9a\u5bf9\u6297\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSGP\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u9ed1\u76d2\u9632\u5fa1\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e73\u5747\u63d0\u53472.3%\u81f332.6%\u3002", "conclusion": "SGP\u662f\u4e00\u79cd\u9ad8\u6269\u5c55\u6027\u7684\u8f93\u5165\u53d8\u6362\u65b9\u6cd5\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u4e2d\uff0c\u6709\u6548\u63d0\u5347\u8fc1\u79fb\u6027\u3002"}}
{"id": "2507.01792", "pdf": "https://arxiv.org/pdf/2507.01792", "abs": "https://arxiv.org/abs/2507.01792", "authors": ["Peng Zheng", "Ye Wang", "Rui Ma", "Zuxuan Wu"], "title": "FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation plays a crucial role in applications such as\nvirtual try-on and poster design. Existing approaches typically fine-tune\npretrained generative models or apply LoRA-based adaptations for individual\nsubjects. However, these methods struggle with multi-subject personalization,\nas combining independently adapted modules often requires complex re-tuning or\njoint optimization. We present FreeLoRA, a simple and generalizable framework\nthat enables training-free fusion of subject-specific LoRA modules for\nmulti-subject personalization. Each LoRA module is adapted on a few images of a\nspecific subject using a Full Token Tuning strategy, where it is applied across\nall tokens in the prompt to encourage weakly supervised token-content\nalignment. At inference, we adopt Subject-Aware Inference, activating each\nmodule only on its corresponding subject tokens. This enables training-free\nfusion of multiple personalized subjects within a single image, while\nmitigating overfitting and mutual interference between subjects. Extensive\nexperiments show that FreeLoRA achieves strong performance in both subject\nfidelity and prompt consistency.", "AI": {"tldr": "FreeLoRA\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u7279\u5b9a\u4e3b\u9898\u7684LoRA\u6a21\u5757\u5b9e\u73b0\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u751f\u6210\u4e2d\u5b58\u5728\u590d\u6742\u8c03\u6574\u6216\u8054\u5408\u4f18\u5316\u7684\u95ee\u9898\uff0cFreeLoRA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528Full Token Tuning\u7b56\u7565\u8bad\u7ec3\u7279\u5b9a\u4e3b\u9898\u7684LoRA\u6a21\u5757\uff0c\u5e76\u901a\u8fc7Subject-Aware Inference\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b\u5bf9\u5e94\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFreeLoRA\u5728\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FreeLoRA\u4e3a\u591a\u4e3b\u9898\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01800", "pdf": "https://arxiv.org/pdf/2507.01800", "abs": "https://arxiv.org/abs/2507.01800", "authors": ["Shengli Zhou", "Jianuo Zhu", "Qilin Huang", "Fangjing Wang", "Yanfu Zhang", "Feng Zheng"], "title": "HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision", "categories": ["cs.CV", "cs.MM"], "comment": "ICANN 2025", "summary": "3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the\nphysical world and perform spatial reasoning. Answer-centric supervision is a\ncommonly used training method for 3D VQA models. Many models that utilize this\nstrategy have achieved promising results in 3D VQA tasks. However, the\nanswer-centric approach only supervises the final output of models and allows\nmodels to develop reasoning pathways freely. The absence of supervision on the\nreasoning pathway enables the potential for developing superficial shortcuts\nthrough common patterns in question-answer pairs. Moreover, although\nslow-thinking methods advance large language models, they suffer from\nunderthinking. To address these issues, we propose \\textbf{HCNQA}, a 3D VQA\nmodel leveraging a hierarchical concentration narrowing supervision method. By\nmimicking the human process of gradually focusing from a broad area to specific\nobjects while searching for answers, our method guides the model to perform\nthree phases of concentration narrowing through hierarchical supervision. By\nsupervising key checkpoints on a general reasoning pathway, our method can\nensure the development of a rational and effective reasoning pathway. Extensive\nexperimental results demonstrate that our method can effectively ensure that\nthe model develops a rational reasoning pathway and performs better. The code\nis available at https://github.com/JianuoZhu/HCNQA.", "AI": {"tldr": "HCNQA\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u9010\u6b65\u805a\u7126\u7684\u8fc7\u7a0b\uff0c\u786e\u4fdd3D VQA\u6a21\u578b\u53d1\u5c55\u51fa\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\uff0c\u907f\u514d\u6d45\u5c42\u6377\u5f84\u3002", "motivation": "\u73b0\u67093D VQA\u6a21\u578b\u4ec5\u76d1\u7763\u6700\u7ec8\u8f93\u51fa\uff0c\u53ef\u80fd\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u4e0d\u5408\u7406\u6216\u6d45\u5c42\u6377\u5f84\u3002", "method": "\u91c7\u7528\u5206\u5c42\u76d1\u7763\uff0c\u5206\u4e09\u4e2a\u9636\u6bb5\u9010\u6b65\u805a\u7126\uff0c\u76d1\u7763\u5173\u952e\u68c0\u67e5\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHCNQA\u80fd\u6709\u6548\u786e\u4fdd\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "HCNQA\u901a\u8fc7\u5206\u5c42\u76d1\u7763\u89e3\u51b3\u4e863D VQA\u4e2d\u7684\u63a8\u7406\u8def\u5f84\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.01801", "pdf": "https://arxiv.org/pdf/2507.01801", "abs": "https://arxiv.org/abs/2507.01801", "authors": ["Bin Rao", "Haicheng Liao", "Yanchen Guan", "Chengyue Wang", "Bonan Wang", "Jiaxun Zhang", "Zhenning Li"], "title": "AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Accurately predicting the future trajectories of traffic agents is essential\nin autonomous driving. However, due to the inherent imbalance in trajectory\ndistributions, tail data in natural datasets often represents more complex and\nhazardous scenarios. Existing studies typically rely solely on a base model's\nprediction error, without considering the diversity and uncertainty of\nlong-tail trajectory patterns. We propose an adaptive momentum and decoupled\ncontrastive learning framework (AMD), which integrates unsupervised and\nsupervised contrastive learning strategies. By leveraging an improved momentum\ncontrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,\nour framework enhances the model's ability to recognize rare and complex\ntrajectories. Additionally, we design four types of trajectory random\naugmentation methods and introduce an online iterative clustering strategy,\nallowing the model to dynamically update pseudo-labels and better adapt to the\ndistributional shifts in long-tail data. We propose three different criteria to\ndefine long-tail trajectories and conduct extensive comparative experiments on\nthe nuScenes and ETH$/$UCY datasets. The results show that AMD not only\nachieves optimal performance in long-tail trajectory prediction but also\ndemonstrates outstanding overall prediction accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u52a8\u91cf\u548c\u89e3\u8026\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08AMD\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u957f\u5c3e\u8f68\u8ff9\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u4f9d\u8d56\u57fa\u6a21\u578b\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u672a\u8003\u8651\u957f\u5c3e\u8f68\u8ff9\u6a21\u5f0f\u7684\u591a\u6837\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u5bf9\u590d\u6742\u5371\u9669\u573a\u666f\u7684\u9884\u6d4b\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684\u52a8\u91cf\u5bf9\u6bd4\u5b66\u4e60\uff08MoCo-DT\uff09\u548c\u89e3\u8026\u5bf9\u6bd4\u5b66\u4e60\uff08DCL\uff09\uff0c\u8bbe\u8ba1\u56db\u79cd\u8f68\u8ff9\u968f\u673a\u589e\u5f3a\u65b9\u6cd5\u548c\u5728\u7ebf\u8fed\u4ee3\u805a\u7c7b\u7b56\u7565\u3002", "result": "\u5728nuScenes\u548cETH/UCY\u6570\u636e\u96c6\u4e0a\uff0cAMD\u5728\u957f\u5c3e\u8f68\u8ff9\u9884\u6d4b\u548c\u6574\u4f53\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u5747\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "AMD\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u957f\u5c3e\u8f68\u8ff9\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2507.01835", "pdf": "https://arxiv.org/pdf/2507.01835", "abs": "https://arxiv.org/abs/2507.01835", "authors": ["Daniil Reutsky", "Daniil Vladimirov", "Yasin Mamedov", "Georgy Perevozchikov", "Nancy Mehta", "Egor Ershov", "Radu Timofte"], "title": "Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral reconstruction (HSR) from RGB images is a fundamentally\nill-posed problem due to severe spectral information loss. Existing approaches\ntypically rely on a single RGB image, limiting reconstruction accuracy. In this\nwork, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)\nframework that leverages a triple-camera smartphone system, where two lenses\nare equipped with carefully selected spectral filters. Our configuration,\ngrounded in theoretical and empirical analysis, enables richer and more diverse\nspectral observations than conventional single-camera setups. To support this\nnew paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising\naligned images from three smartphone cameras and a hyperspectral reference\ncamera across diverse scenes. We show that the proposed HSR model achieves\nconsistent improvements over existing methods on the newly proposed benchmark.\nIn a nutshell, our setup allows 30% towards more accurately estimated spectra\ncompared to an ordinary RGB camera. Our findings suggest that multi-view\nspectral filtering with commodity hardware can unlock more accurate and\npractical hyperspectral imaging solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u56fe\u50cf\u7684\u8d85\u5149\u8c31\u91cd\u5efa\u6846\u67b6\uff08MI-HSR\uff09\uff0c\u5229\u7528\u914d\u5907\u5149\u8c31\u6ee4\u955c\u7684\u4e09\u6444\u50cf\u5934\u667a\u80fd\u624b\u673a\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355RGB\u56fe\u50cf\uff0c\u5bfc\u81f4\u5149\u8c31\u4fe1\u606f\u4e22\u5931\u4e25\u91cd\uff0c\u91cd\u5efa\u7cbe\u5ea6\u53d7\u9650\u3002", "method": "\u91c7\u7528\u4e09\u6444\u50cf\u5934\u667a\u80fd\u624b\u673a\u7cfb\u7edf\uff0c\u5176\u4e2d\u4e24\u4e2a\u955c\u5934\u914d\u5907\u7279\u5b9a\u5149\u8c31\u6ee4\u955c\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u914d\u7f6e\uff0c\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u9891\u8c31\u89c2\u6d4b\u3002", "result": "\u5728\u63d0\u51fa\u7684Doomer\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8630%\u7684\u5149\u8c31\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u591a\u89c6\u89d2\u5149\u8c31\u6ee4\u6ce2\u7ed3\u5408\u6d88\u8d39\u7ea7\u786c\u4ef6\uff0c\u53ef\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u5b9e\u7528\u7684\u8d85\u5149\u8c31\u6210\u50cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01838", "pdf": "https://arxiv.org/pdf/2507.01838", "abs": "https://arxiv.org/abs/2507.01838", "authors": ["Hailong Yan", "Ao Li", "Xiangtao Zhang", "Zhe Liu", "Zenglin Shi", "Ce Zhu", "Le Zhang"], "title": "MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Recent advancements in deep neural networks have driven significant progress\nin image enhancement (IE). However, deploying deep learning models on\nresource-constrained platforms, such as mobile devices, remains challenging due\nto high computation and memory demands. To address these challenges and\nfacilitate real-time IE on mobile, we introduce an extremely lightweight\nConvolutional Neural Network (CNN) framework with around 4K parameters. Our\napproach integrates reparameterization with an Incremental Weight Optimization\nstrategy to ensure efficiency. Additionally, we enhance performance with a\nFeature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,\noptimized with a Local Variance-Weighted loss. With this efficient framework,\nwe are the first to achieve real-time IE inference at up to 1,100 frames per\nsecond (FPS) while delivering competitive image quality, achieving the best\ntrade-off between speed and performance across multiple IE tasks. The code will\nbe available at https://github.com/AVC2-UESTC/MobileIE.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7CNN\u6846\u67b6\uff0c\u4ec5\u7ea64K\u53c2\u6570\uff0c\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u548c\u589e\u91cf\u6743\u91cd\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\uff0c\u6700\u9ad8\u8fbe1,100 FPS\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\uff08\u5982\u79fb\u52a8\u8bbe\u5907\uff09\u4e0a\u90e8\u7f72\u7684\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u91cd\u53c2\u6570\u5316\u548c\u589e\u91cf\u6743\u91cd\u4f18\u5316\u7b56\u7565\uff0c\u5f15\u5165\u7279\u5f81\u81ea\u53d8\u6362\u6a21\u5757\u548c\u5c42\u6b21\u53cc\u8def\u5f84\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u7528\u5c40\u90e8\u65b9\u5dee\u52a0\u6743\u635f\u5931\u4f18\u5316\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u9ad8\u8fbe1,100 FPS\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8fbe\u5230\u901f\u5ea6\u4e0e\u6027\u80fd\u7684\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01882", "pdf": "https://arxiv.org/pdf/2507.01882", "abs": "https://arxiv.org/abs/2507.01882", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Marcel Hussing", "Edward Zhang", "Eric Eaton", "Daniel A. Hashimoto"], "title": "Future Slot Prediction for Unsupervised Object Discovery in Surgical Video", "categories": ["cs.CV"], "comment": "Accepted by MICCAI2025", "summary": "Object-centric slot attention is an emerging paradigm for unsupervised\nlearning of structured, interpretable object-centric representations (slots).\nThis enables effective reasoning about objects and events at a low\ncomputational cost and is thus applicable to critical healthcare applications,\nsuch as real-time interpretation of surgical video. The heterogeneous scenes in\nreal-world applications like surgery are, however, difficult to parse into a\nmeaningful set of slots. Current approaches with an adaptive slot count perform\nwell on images, but their performance on surgical videos is low. To address\nthis challenge, we propose a dynamic temporal slot transformer (DTST) module\nthat is trained both for temporal reasoning and for predicting the optimal\nfuture slot initialization. The model achieves state-of-the-art performance on\nmultiple surgical databases, demonstrating that unsupervised object-centric\nmethods can be applied to real-world data and become part of the common arsenal\nin healthcare applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u65f6\u5e8f\u69fd\u53d8\u6362\u5668\uff08DTST\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u672f\u89c6\u9891\u4e2d\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u5e93\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\uff08\u5982\u624b\u672f\uff09\u4e2d\u7684\u5f02\u6784\u573a\u666f\u96be\u4ee5\u89e3\u6790\u4e3a\u6709\u610f\u4e49\u7684\u4e00\u7ec4\u69fd\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u624b\u672f\u89c6\u9891\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u65f6\u5e8f\u69fd\u53d8\u6362\u5668\uff08DTST\uff09\u6a21\u5757\uff0c\u7ed3\u5408\u65f6\u5e8f\u63a8\u7406\u548c\u9884\u6d4b\u672a\u6765\u6700\u4f18\u69fd\u521d\u59cb\u5316\u3002", "result": "\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u5e93\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u65e0\u76d1\u7763\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u5e76\u6210\u4e3a\u533b\u7597\u5e94\u7528\u4e2d\u7684\u5e38\u89c1\u5de5\u5177\u3002"}}
{"id": "2507.01884", "pdf": "https://arxiv.org/pdf/2507.01884", "abs": "https://arxiv.org/abs/2507.01884", "authors": ["Kunlun Xu", "Fan Zhuo", "Jiangmeng Li", "Xu Zou", "Jiahuan Zhou"], "title": "Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Current lifelong person re-identification (LReID) methods predominantly rely\non fully labeled data streams. However, in real-world scenarios where\nannotation resources are limited, a vast amount of unlabeled data coexists with\nscarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)\nproblem where LReID methods suffer severe performance degradation. Existing\nLReID methods, even when combined with semi-supervised strategies, suffer from\nlimited long-term adaptation performance due to struggling with the noisy\nknowledge occurring during unlabeled data utilization. In this paper, we\npioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing\nPrototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key\ninnovation lies in establishing a self-reinforcing cycle between dynamic\nprototype-guided pseudo-label generation and new-old knowledge collaborative\npurification to enhance the utilization of unlabeled data. Specifically,\nlearnable identity prototypes are introduced to dynamically capture the\nidentity distributions and generate high-quality pseudo-labels. Then, the\ndual-knowledge cooperation scheme integrates current model specialization and\nhistorical model generalization, refining noisy pseudo-labels. Through this\ncyclic design, reliable pseudo-labels are progressively mined to improve\ncurrent-stage learning and ensure positive knowledge propagation over long-term\nlearning. Experiments on the established Semi-LReID benchmarks show that our\nSPRED achieves state-of-the-art performance. Our source code is available at\nhttps://github.com/zhoujiahuan1991/ICCV2025-SPRED", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPRED\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u534a\u76d1\u7763\u7ec8\u8eab\u884c\u4eba\u91cd\u8bc6\u522b\uff08Semi-LReID\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u539f\u578b\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u65b0\u65e7\u77e5\u8bc6\u534f\u540c\u51c0\u5316\uff0c\u63d0\u5347\u672a\u6807\u8bb0\u6570\u636e\u7684\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6807\u6ce8\u8d44\u6e90\u6709\u9650\uff0c\u73b0\u6709LReID\u65b9\u6cd5\u5728\u672a\u6807\u8bb0\u6570\u636e\u5229\u7528\u65f6\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u8eab\u4efd\u539f\u578b\u52a8\u6001\u6355\u6349\u8eab\u4efd\u5206\u5e03\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7\u53cc\u77e5\u8bc6\u5408\u4f5c\u65b9\u6848\u6574\u5408\u5f53\u524d\u6a21\u578b\u548c\u5386\u53f2\u6a21\u578b\u77e5\u8bc6\uff0c\u51c0\u5316\u566a\u58f0\u4f2a\u6807\u7b7e\u3002", "result": "\u5728Semi-LReID\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPRED\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SPRED\u901a\u8fc7\u81ea\u589e\u5f3a\u5faa\u73af\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u7ec8\u8eab\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.01908", "pdf": "https://arxiv.org/pdf/2507.01908", "abs": "https://arxiv.org/abs/2507.01908", "authors": ["Qingdong He", "Xueqin Chen", "Chaoyi Wang", "Yanjie Pan", "Xiaobin Hu", "Zhenye Gan", "Yabiao Wang", "Chengjie Wang", "Xiangtai Li", "Jiangning Zhang"], "title": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Instruction-based image editing (IIE) has advanced rapidly with the success\nof diffusion models. However, existing efforts primarily focus on simple and\nexplicit instructions to execute editing operations such as adding, deleting,\nmoving, or swapping objects. They struggle to handle more complex implicit\nhypothetical instructions that require deeper reasoning to infer plausible\nvisual changes and user intent. Additionally, current datasets provide limited\nsupport for training and evaluating reasoning-aware editing capabilities.\nArchitecturally, these methods also lack mechanisms for fine-grained detail\nextraction that support such reasoning. To address these limitations, we\npropose Reason50K, a large-scale dataset specifically curated for training and\nevaluating hypothetical instruction reasoning image editing, along with\nReasonBrain, a novel framework designed to reason over and execute implicit\nhypothetical instructions across diverse scenarios. Reason50K includes over 50K\nsamples spanning four key reasoning scenarios: Physical, Temporal, Causal, and\nStory reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)\nfor editing guidance generation and a diffusion model for image synthesis,\nincorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture\ndetailed visual and textual semantics essential for supporting instruction\nreasoning. To mitigate the semantic loss, we further introduce a Cross-Modal\nEnhancer (CME) that enables rich interactions between the fine-grained cues and\nMLLM-derived features. Extensive experiments demonstrate that ReasonBrain\nconsistently outperforms state-of-the-art baselines on reasoning scenarios\nwhile exhibiting strong zero-shot generalization to conventional IIE tasks. Our\ndataset and code will be released publicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Reason50K\u6570\u636e\u96c6\u548cReasonBrain\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u4e2d\u590d\u6742\u9690\u542b\u5047\u8bbe\u6307\u4ee4\u7684\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9700\u8981\u6df1\u5c42\u63a8\u7406\u7684\u590d\u6742\u9690\u542b\u5047\u8bbe\u6307\u4ee4\uff0c\u4e14\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u548c\u67b6\u6784\u652f\u6301\u3002", "method": "\u63d0\u51faReason50K\u6570\u636e\u96c6\u548cReasonBrain\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u7ec6\u7c92\u5ea6\u63a8\u7406\u7ebf\u7d22\u63d0\u53d6\u6a21\u5757\u548c\u8de8\u6a21\u6001\u589e\u5f3a\u5668\u3002", "result": "ReasonBrain\u5728\u63a8\u7406\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Reason50K\u548cReasonBrain\u4e3a\u590d\u6742\u6307\u4ee4\u63a8\u7406\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01909", "pdf": "https://arxiv.org/pdf/2507.01909", "abs": "https://arxiv.org/abs/2507.01909", "authors": ["Jorge Tapias Gomez", "Nishant Nadkarni", "Lando S. Bosma", "Jue Jiang", "Ergys D. Subashi", "William P. Segars", "James M. Balter", "Mert R Sabuncu", "Neelam Tyagi", "Harini Veeraraghavan"], "title": "Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion", "categories": ["cs.CV"], "comment": "7 Pages, 6 figures, 4 tables", "summary": "Objective: Clinical implementation of deformable image registration (DIR)\nrequires voxel-based spatial accuracy metrics such as manually identified\nlandmarks, which are challenging to implement for highly mobile\ngastrointestinal (GI) organs. To address this, patient-specific digital twins\n(DT) modeling temporally varying motion were created to assess the accuracy of\nDIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D\nsequences were generated from static 3D patient scans using published\nanalytical GI motion models through a semi-automated pipeline. Eleven datasets,\nincluding six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,\nand three contrast-enhanced CT scans. The motion amplitudes of the DTs were\nassessed against real patient stomach motion amplitudes extracted from\nindependent 4D MRI datasets. The generated DTs were then used to assess six\ndifferent DIR methods using target registration error, Dice similarity\ncoefficient, and the 95th percentile Hausdorff distance using summary metrics\nand voxel-level granular visualizations. Finally, for a subset of T2w MRI scans\nfrom patients treated with MR-guided radiation therapy, dose distributions were\nwarped and accumulated to assess dose warping errors, including evaluations of\nDIR performance in both low- and high-dose regions for patient-specific error\nestimation. Main results: Our proposed pipeline synthesized DTs modeling\nrealistic GI motion, achieving mean and maximum motion amplitudes and a mean\nlog Jacobian determinant within 0.8 mm and 0.01, respectively, similar to\npublished real-patient gastric motion data. It also enables the extraction of\ndetailed quantitative DIR performance metrics and rigorous validation of dose\nmapping accuracy. Significance: The pipeline enables rigorously testing DIR\ntools for dynamic, anatomically complex regions enabling granular spatial and\ndosimetric accuracies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60a3\u8005\u7279\u5f02\u6027\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u8bc4\u4f30\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\uff08DIR\uff09\u65b9\u6cd5\u5728\u80c3\u80a0\u9053\uff08GI\uff09\u5668\u5b98\u8fd0\u52a8\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u65bdDIR\u9700\u8981\u57fa\u4e8e\u4f53\u7d20\u7684\u7a7a\u95f4\u51c6\u786e\u6027\u5ea6\u91cf\uff0c\u5982\u624b\u52a8\u6807\u8bb0\u70b9\uff0c\u4f46\u5bf9\u4e8e\u9ad8\u5ea6\u79fb\u52a8\u7684GI\u5668\u5b98\u96be\u4ee5\u5b9e\u73b0\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7DT\u6a21\u578b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u4ece\u9759\u60013D\u60a3\u8005\u626b\u63cf\u751f\u621021\u4e2a\u6a21\u62dfGI\u8fd0\u52a8\u76844D\u5e8f\u5217\uff0c\u5e76\u8bc4\u4f30\u516d\u79cdDIR\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "result": "\u751f\u6210\u7684DT\u6a21\u578b\u8fd0\u52a8\u5e45\u5ea6\u4e0e\u771f\u5b9e\u60a3\u8005\u80c3\u8fd0\u52a8\u6570\u636e\u76f8\u4f3c\uff0c\u4e14\u80fd\u63d0\u53d6\u8be6\u7ec6\u7684DIR\u6027\u80fd\u6307\u6807\u548c\u5242\u91cf\u6620\u5c04\u51c6\u786e\u6027\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7ba1\u9053\u4e3a\u52a8\u6001\u590d\u6742\u89e3\u5256\u533a\u57df\u7684DIR\u5de5\u5177\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u652f\u6301\u7a7a\u95f4\u548c\u5242\u91cf\u51c6\u786e\u6027\u7684\u8be6\u7ec6\u8bc4\u4f30\u3002"}}
{"id": "2507.01912", "pdf": "https://arxiv.org/pdf/2507.01912", "abs": "https://arxiv.org/abs/2507.01912", "authors": ["Ranjan Sapkota", "Zhichao Meng", "Martin Churuvija", "Xiaoqiang Du", "Zenghong Ma", "Manoj Karkee"], "title": "3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP", "categories": ["cs.CV"], "comment": "17 pages, 4 tables, 11 figures", "summary": "In orchard automation, dense foliage during the canopy season severely\noccludes tree structures, minimizing visibility to various canopy parts such as\ntrunks and branches, which limits the ability of a machine vision system.\nHowever, canopy structure is more open and visible during the dormant season\nwhen trees are defoliated. In this work, we present an information fusion\nframework that integrates multi-seasonal structural data to support robotic and\nautomated crop load management during the entire growing season. The framework\ncombines high-resolution RGB-D imagery from both dormant and canopy periods\nusing YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D\nreconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for\nmodel alignment. Segmentation outputs from YOLOv9-Seg were used to extract\ndepth-informed masks, which enabled accurate 3D point cloud reconstruction via\nKinect Fusion; these reconstructed models from each season were subsequently\naligned using Fast GICP to achieve spatially coherent multi-season fusion. The\nYOLOv9-Seg model, trained on manually annotated images, achieved a mean squared\nerror (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in\ndormant season dataset. Kinect Fusion enabled accurate reconstruction of tree\ngeometry, validated with field measurements resulting in root mean square\nerrors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and\n13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal\nregistration with a minimum fitness score of 0.00197, allowing integrated,\ncomprehensive tree structure modeling despite heavy occlusions during the\ngrowing season. This fused structural representation enables robotic systems to\naccess otherwise obscured architectural information, improving the precision of\npruning, thinning, and other automated orchard operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5b63\u8282\u4fe1\u606f\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4f11\u7720\u671f\u548c\u751f\u957f\u671f\u7684RGB-D\u56fe\u50cf\uff0c\u901a\u8fc7\u5b9e\u4f8b\u5206\u5272\u30013D\u91cd\u5efa\u548c\u6a21\u578b\u5bf9\u9f50\uff0c\u5b9e\u73b0\u679c\u56ed\u81ea\u52a8\u5316\u7ba1\u7406\u3002", "motivation": "\u679c\u56ed\u751f\u957f\u671f\u6811\u51a0\u5bc6\u96c6\u906e\u6321\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u800c\u4f11\u7720\u671f\u6811\u51a0\u5f00\u653e\u53ef\u89c1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u878d\u5408\u591a\u5b63\u8282\u6570\u636e\u4ee5\u652f\u6301\u5168\u5e74\u81ea\u52a8\u5316\u7ba1\u7406\u3002", "method": "\u4f7f\u7528YOLOv9-Seg\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\uff0cKinect Fusion\u8fdb\u884c3D\u91cd\u5efa\uff0cFast GICP\u8fdb\u884c\u6a21\u578b\u5bf9\u9f50\uff0c\u878d\u5408\u4f11\u7720\u671f\u548c\u751f\u957f\u671f\u7684RGB-D\u56fe\u50cf\u3002", "result": "YOLOv9-Seg\u5728\u4f11\u7720\u671f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff08MSE 0.0047\uff0cmAP@50 0.78\uff09\uff0cKinect Fusion\u91cd\u5efa\u7cbe\u5ea6\u9ad8\uff08RMSE 5.23 mm\uff09\uff0cFast GICP\u5b9e\u73b0\u8de8\u5b63\u8282\u7cbe\u786e\u5bf9\u9f50\uff08\u6700\u5c0f\u9002\u5e94\u5206\u65700.00197\uff09\u3002", "conclusion": "\u591a\u5b63\u8282\u878d\u5408\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5bf9\u6811\u51a0\u7ed3\u6784\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u4fee\u526a\u548c\u758f\u679c\u7b49\u81ea\u52a8\u5316\u64cd\u4f5c\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2507.01926", "pdf": "https://arxiv.org/pdf/2507.01926", "abs": "https://arxiv.org/abs/2507.01926", "authors": ["Yaowei Li", "Xiaoyu Li", "Zhaoyang Zhang", "Yuxuan Bian", "Gan Liu", "Xinyuan Li", "Jiale Xu", "Wenbo Hu", "Yating Liu", "Lingen Li", "Jing Cai", "Yuexian Zou", "Yancheng He", "Ying Shan"], "title": "IC-Custom: Diverse Image Customization via In-Context Learning", "categories": ["cs.CV"], "comment": "Project page: https://liyaowei-stu.github.io/project/IC_Custom", "summary": "Image customization, a crucial technique for industrial media production,\naims to generate content that is consistent with reference images. However,\ncurrent approaches conventionally separate image customization into\nposition-aware and position-free customization paradigms and lack a universal\nframework for diverse customization, limiting their applications across various\nscenarios. To overcome these limitations, we propose IC-Custom, a unified\nframework that seamlessly integrates position-aware and position-free image\ncustomization through in-context learning. IC-Custom concatenates reference\nimages with target images to a polyptych, leveraging DiT's multi-modal\nattention mechanism for fine-grained token-level interactions. We introduce the\nIn-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented\nregister tokens and boundary-aware positional embeddings to enable the model to\ncorrectly handle different task types and distinguish various inputs in\npolyptych configurations. To bridge the data gap, we carefully curated a\nhigh-quality dataset of 12k identity-consistent samples with 8k from real-world\nsources and 4k from high-quality synthetic data, avoiding the overly glossy and\nover-saturated synthetic appearance. IC-Custom supports various industrial\napplications, including try-on, accessory placement, furniture arrangement, and\ncreative IP customization. Extensive evaluations on our proposed ProductBench\nand the publicly available DreamBench demonstrate that IC-Custom significantly\noutperforms community workflows, closed-source models, and state-of-the-art\nopen-source approaches. IC-Custom achieves approximately 73% higher human\npreference across identity consistency, harmonicity, and text alignment\nmetrics, while training only 0.4% of the original model parameters. Project\npage: https://liyaowei-stu.github.io/project/IC_Custom", "AI": {"tldr": "IC-Custom\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u5b9a\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6574\u5408\u4f4d\u7f6e\u611f\u77e5\u548c\u65e0\u4f4d\u7f6e\u5b9a\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u5b9a\u5236\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6846\u67b6\uff0c\u9650\u5236\u4e86\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u3002", "method": "\u63d0\u51faIC-Custom\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u548c\u4efb\u52a1\u5bfc\u5411\u7684\u6ce8\u518c\u4ee4\u724c\uff0c\u5229\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4eba\u7c7b\u504f\u597d\u5ea6\u63d0\u534773%\uff0c\u4ec5\u8bad\u7ec30.4%\u7684\u6a21\u578b\u53c2\u6570\u3002", "conclusion": "IC-Custom\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u56fe\u50cf\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01927", "pdf": "https://arxiv.org/pdf/2507.01927", "abs": "https://arxiv.org/abs/2507.01927", "authors": ["Zhentan Zheng"], "title": "evMLP: An Efficient Event-Driven MLP Architecture for Vision", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks have achieved remarkable results in computer vision\ntasks. In the early days, Convolutional Neural Networks (CNNs) were the\nmainstream architecture. In recent years, Vision Transformers (ViTs) have\nbecome increasingly popular. In addition, exploring applications of multi-layer\nperceptrons (MLPs) has provided new perspectives for research into vision model\narchitectures. In this paper, we present evMLP accompanied by a simple\nevent-driven local update mechanism. The proposed evMLP can independently\nprocess patches on images or feature maps via MLPs. We define changes between\nconsecutive frames as \"events\". Under the event-driven local update mechanism,\nevMLP selectively processes patches where events occur. For sequential image\ndata (e.g., video processing), this approach improves computational performance\nby avoiding redundant computations. Through ImageNet image classification\nexperiments, evMLP attains accuracy competitive with state-of-the-art models.\nMore significantly, experimental results on multiple video datasets demonstrate\nthat evMLP reduces computational cost via its event-driven local update\nmechanism while maintaining output consistency with its non-event-driven\nbaseline. The code and trained models are available at\nhttps://github.com/i-evi/evMLP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aevMLP\u7684\u65b0\u578b\u89c6\u89c9\u6a21\u578b\uff0c\u7ed3\u5408\u4e8b\u4ef6\u9a71\u52a8\u7684\u5c40\u90e8\u66f4\u65b0\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5904\u7406\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u591a\u5c42\u611f\u77e5\u673a\uff08MLPs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u89c6\u9891\u5904\u7406\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\u3002", "method": "\u63d0\u51faevMLP\u6a21\u578b\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u9009\u62e9\u6027\u5904\u7406\u56fe\u50cf\u6216\u7279\u5f81\u56fe\u4e2d\u7684\u53d8\u5316\u533a\u57df\uff08\u4e8b\u4ef6\uff09\uff0c\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5728ImageNet\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89c6\u9891\u6570\u636e\u96c6\u5b9e\u9a8c\u663e\u793a\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u4e14\u8f93\u51fa\u4e00\u81f4\u3002", "conclusion": "evMLP\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u89c6\u89c9\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01938", "pdf": "https://arxiv.org/pdf/2507.01938", "abs": "https://arxiv.org/abs/2507.01938", "authors": ["Yiming Ju", "Jijin Hu", "Zhengxiong Luo", "Haoge Deng", "hanyu Zhao", "Li Du", "Chengwei Wu", "Donglin Hao", "Xinlong Wang", "Tengfei Pan"], "title": "CI-VID: A Coherent Interleaved Text-Video Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video (T2V) generation has recently attracted considerable attention,\nresulting in the development of numerous high-quality datasets that have\npropelled progress in this area. However, existing public datasets are\nprimarily composed of isolated text-video (T-V) pairs and thus fail to support\nthe modeling of coherent multi-clip video sequences. To address this\nlimitation, we introduce CI-VID, a dataset that moves beyond isolated\ntext-to-video (T2V) generation toward text-and-video-to-video (TV2V)\ngeneration, enabling models to produce coherent, multi-scene video sequences.\nCI-VID contains over 340,000 samples, each featuring a coherent sequence of\nvideo clips with text captions that capture both the individual content of each\nclip and the transitions between them, enabling visually and textually grounded\ngeneration. To further validate the effectiveness of CI-VID, we design a\ncomprehensive, multi-dimensional benchmark incorporating human evaluation,\nVLM-based assessment, and similarity-based metrics. Experimental results\ndemonstrate that models trained on CI-VID exhibit significant improvements in\nboth accuracy and content consistency when generating video sequences. This\nfacilitates the creation of story-driven content with smooth visual transitions\nand strong temporal coherence, underscoring the quality and practical utility\nof the CI-VID dataset We release the CI-VID dataset and the accompanying code\nfor data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86CI-VID\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u652f\u6301\u8fde\u8d2f\u591a\u573a\u666f\u89c6\u9891\u5e8f\u5217\u7684\u751f\u6210\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u5b64\u7acb\u6587\u672c-\u89c6\u9891\u5bf9\u3002", "motivation": "\u73b0\u6709\u516c\u5171\u6570\u636e\u96c6\u4e3b\u8981\u5305\u542b\u5b64\u7acb\u7684\u6587\u672c-\u89c6\u9891\u5bf9\uff0c\u65e0\u6cd5\u652f\u6301\u8fde\u8d2f\u591a\u573a\u666f\u89c6\u9891\u5e8f\u5217\u7684\u5efa\u6a21\u3002", "method": "\u63d0\u51faCI-VID\u6570\u636e\u96c6\uff0c\u5305\u542b34\u4e07\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u8fde\u8d2f\u89c6\u9891\u7247\u6bb5\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u652f\u6301\u6587\u672c\u548c\u89c6\u9891\u5230\u89c6\u9891\u7684\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528CI-VID\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u751f\u6210\u89c6\u9891\u5e8f\u5217\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u5185\u5bb9\u4e00\u81f4\u6027\u3002", "conclusion": "CI-VID\u6570\u636e\u96c6\u5728\u751f\u6210\u6545\u4e8b\u9a71\u52a8\u5185\u5bb9\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u8d28\u91cf\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.01945", "pdf": "https://arxiv.org/pdf/2507.01945", "abs": "https://arxiv.org/abs/2507.01945", "authors": ["Nan Chen", "Mengqi Huang", "Yihao Meng", "Zhendong Mao"], "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory", "categories": ["cs.CV"], "comment": null, "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.", "AI": {"tldr": "\u63d0\u51faLongAnimation\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8303\u5f0f\u5b9e\u73b0\u957f\u52a8\u753b\u7740\u8272\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u957f\u52a8\u753b\u7740\u8272\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u77ed\u7247\u6bb5\uff0c\u7f3a\u4e4f\u5168\u5c40\u4e00\u81f4\u6027\u3002", "method": "\u7ed3\u5408SketchDiT\u3001\u52a8\u6001\u5168\u5c40-\u5c40\u90e8\u8bb0\u5fc6\u6a21\u5757\uff08DGLM\uff09\u548c\u989c\u8272\u4e00\u81f4\u6027\u5956\u52b1\u3002", "result": "\u5728\u77ed\u7247\u6bb5\uff0814\u5e27\uff09\u548c\u957f\u7247\u6bb5\uff08\u5e73\u5747500\u5e27\uff09\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LongAnimation\u6709\u6548\u89e3\u51b3\u4e86\u957f\u52a8\u753b\u7740\u8272\u7684\u5168\u5c40\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2507.01949", "pdf": "https://arxiv.org/pdf/2507.01949", "abs": "https://arxiv.org/abs/2507.01949", "authors": ["Kwai Keye Team", "Biao Yang", "Bin Wen", "Changyi Liu", "Chenglong Chu", "Chengru Song", "Chongling Rao", "Chuan Yi", "Da Li", "Dunju Zang", "Fan Yang", "Guorui Zhou", "Hao Peng", "Haojie Ding", "Jiaming Huang", "Jiangxia Cao", "Jiankang Chen", "Jingyun Hua", "Jin Ouyang", "Kaibing Chen", "Kaiyu Jiang", "Kaiyu Tang", "Kun Gai", "Shengnan Zhang", "Siyang Mao", "Sui Huang", "Tianke Zhang", "Tingting Gao", "Wei Chen", "Wei Yuan", "Xiangyu Wu", "Xiao Hu", "Xingyu Lu", "Yang Zhou", "Yi-Fan Zhang", "Yiping Yang", "Yulong Chen", "Zhenhua Wu", "Zhenyu Li", "Zhixin Ling", "Ziming Li", "Dehua Ma", "Di Xu", "Haixuan Gao", "Hang Li", "Jiawei Guo", "Jing Wang", "Lejian Ren", "Muhao Wei", "Qianqian Wang", "Qigen Hu", "Shiyao Wang", "Tao Yu", "Xinchen Luo", "Yan Li", "Yiming Liang", "Yuhang Hu", "Zeyi Lu", "Zhuoran Yang", "Zixing Zhang"], "title": "Kwai Keye-VL Technical Report", "categories": ["cs.CV"], "comment": "Technical Report: https://github.com/Kwai-Keye/Keye", "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce \\textbf{Kwai Keye-VL}, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the \\textbf{KC-MMBench}, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.", "AI": {"tldr": "Kwai Keye-VL\u662f\u4e00\u4e2a80\u4ebf\u53c2\u6570\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u77ed\u89c6\u9891\u7406\u89e3\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u521b\u65b0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6a21\u578b\u5728\u89c6\u9891\u548c\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u52a8\u6001\u3001\u4fe1\u606f\u5bc6\u96c6\u7684\u77ed\u89c6\u9891\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0cKwai Keye-VL\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u9884\u8bad\u7ec3\u548c\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\uff0c\u5305\u62ec\u521b\u65b0\u7684\u4e94\u6a21\u5f0f\u6570\u636e\u6df7\u5408\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "Keye-VL\u5728\u516c\u5171\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\uff0c\u5e76\u5728\u901a\u7528\u56fe\u50cf\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "Kwai Keye-VL\u6210\u529f\u89e3\u51b3\u4e86\u77ed\u89c6\u9891\u7406\u89e3\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u65b0\u57fa\u51c6KC-MMBench\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2507.01953", "pdf": "https://arxiv.org/pdf/2507.01953", "abs": "https://arxiv.org/abs/2507.01953", "authors": ["Yukang Cao", "Chenyang Si", "Jinghao Wang", "Ziwei Liu"], "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://yukangcao.github.io/FreeMorph/", "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.", "AI": {"tldr": "FreeMorph\u662f\u4e00\u79cd\u65e0\u9700\u8c03\u4f18\u7684\u56fe\u50cf\u53d8\u5f62\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8bed\u4e49\u6216\u5e03\u5c40\u7684\u8f93\u5165\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u63d2\u503c\u548c\u6ce8\u610f\u529b\u6a21\u5757\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u53d8\u5f62\uff0c\u901f\u5ea6\u63d0\u534710~50\u500d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u53d7\u9650\u4e8e\u65f6\u95f4\u548c\u8bed\u4e49/\u5e03\u5c40\u5dee\u5f02\uff0cFreeMorph\u65e8\u5728\u65e0\u9700\u5b9e\u4f8b\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u4fdd\u771f\u56fe\u50cf\u53d8\u5f62\u3002", "method": "1\uff09\u63d0\u51fa\u6307\u5bfc\u611f\u77e5\u7684\u7403\u5f62\u63d2\u503c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4fee\u6539\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u89e3\u51b3\u8eab\u4efd\u4e22\u5931\u95ee\u9898\uff1b2\uff09\u5f15\u5165\u6b65\u9aa4\u5bfc\u5411\u7684\u53d8\u5206\u8d8b\u52bf\uff0c\u6df7\u5408\u8f93\u5165\u56fe\u50cf\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u4ee5\u5b9e\u73b0\u53ef\u63a7\u8fc7\u6e21\u3002", "result": "FreeMorph\u5728\u56fe\u50cf\u53d8\u5f62\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u63d0\u534710~50\u500d\uff0c\u6210\u4e3a\u65b0\u7684\u6280\u672f\u6807\u6746\u3002", "conclusion": "FreeMorph\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u56fe\u50cf\u53d8\u5f62\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01955", "pdf": "https://arxiv.org/pdf/2507.01955", "abs": "https://arxiv.org/abs/2507.01955", "authors": ["Rahul Ramachandran", "Ali Garjani", "Roman Bachmann", "Andrei Atanov", "O\u011fuzhan Fatih Kar", "Amir Zamir"], "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page at https://fm-vision-evals.epfl.ch/", "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.", "AI": {"tldr": "\u8bba\u6587\u5bf9\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u6807\u51c6\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5176\u867d\u4e0d\u53ca\u4e13\u4e1a\u6a21\u578b\uff0c\u4f46\u4f5c\u4e3a\u901a\u7528\u6a21\u578b\u8868\u73b0\u5c1a\u53ef\uff0c\u4e14\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u51e0\u4f55\u4efb\u52a1\u3002", "motivation": "\u8bc4\u4f30\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u660e\u786e\u5176\u4e0e\u4e13\u4e1a\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u94fe\u5c06\u6807\u51c6\u89c6\u89c9\u4efb\u52a1\u8f6c\u5316\u4e3a\u6587\u672c\u53ef\u63d0\u793a\u548cAPI\u517c\u5bb9\u7684\u4efb\u52a1\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\u6846\u67b6\u3002", "result": "\u6a21\u578b\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u51e0\u4f55\u4efb\u52a1\uff0cGPT-4o\u5728\u975e\u63a8\u7406\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u63a8\u7406\u6a21\u578b\u5728\u51e0\u4f55\u4efb\u52a1\u4e0a\u6709\u6539\u8fdb\u3002", "conclusion": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u901a\u7528\u6027\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u63a5\u8fd1\u4e13\u4e1a\u6a21\u578b\u6c34\u5e73\u3002"}}
{"id": "2507.01957", "pdf": "https://arxiv.org/pdf/2507.01957", "abs": "https://arxiv.org/abs/2507.01957", "authors": ["Zhuoyang Zhang", "Luke J. Huang", "Chengyue Wu", "Shang Yang", "Kelly Peng", "Yao Lu", "Song Han"], "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "The first two authors contributed equally to this work", "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256$\\times$256 res.) and 1024 to 48 (512$\\times$512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4$\\times$ lower latency than previous parallelized\nautoregressive models.", "AI": {"tldr": "Locality-aware Parallel Decoding (LPD) \u901a\u8fc7\u7075\u6d3b\u7684\u81ea\u56de\u5f52\u5efa\u6a21\u548c\u5c40\u90e8\u611f\u77e5\u751f\u6210\u987a\u5e8f\uff0c\u663e\u8457\u52a0\u901f\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\uff0c\u51cf\u5c11\u751f\u6210\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4f9d\u8d56\u987a\u5e8f\u9884\u6d4b\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff1b\u73b0\u6709\u5e76\u884c\u5316\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u7075\u6d3b\u5e76\u884c\u81ea\u56de\u5f52\u5efa\u6a21\u548c\u5c40\u90e8\u611f\u77e5\u751f\u6210\u987a\u5e8f\u4e24\u79cd\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u5e76\u884c\u5316\u4e14\u4fdd\u6301\u8d28\u91cf\u3002", "result": "\u5728 ImageNet \u4e0a\uff0c\u751f\u6210\u6b65\u9aa4\u4ece 256 \u51cf\u5c11\u5230 20\uff08256\u00d7256\uff09\uff0c1024 \u51cf\u5c11\u5230 48\uff08512\u00d7512\uff09\uff0c\u5ef6\u8fdf\u964d\u4f4e\u81f3\u5c11 3.4 \u500d\u3002", "conclusion": "LPD \u5728\u52a0\u901f\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.01055", "pdf": "https://arxiv.org/pdf/2507.01055", "abs": "https://arxiv.org/abs/2507.01055", "authors": ["Hao Yang", "Xinlong Liang", "Zhang Li", "Yue Sun", "Zheyu Hu", "Xinghe Xie", "Behdad Dashtbozorg", "Jincheng Huang", "Shiwei Zhu", "Luyi Han", "Jiong Zhang", "Shanshan Wang", "Ritse Mann", "Qifeng Yu", "Tao Tan"], "title": "Prompt Mechanisms in Medical Imaging: A Comprehensive Survey", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning offers transformative potential in medical imaging, yet its\nclinical adoption is frequently hampered by challenges such as data scarcity,\ndistribution shifts, and the need for robust task generalization. Prompt-based\nmethodologies have emerged as a pivotal strategy to guide deep learning models,\nproviding flexible, domain-specific adaptations that significantly enhance\nmodel performance and adaptability without extensive retraining. This\nsystematic review critically examines the burgeoning landscape of prompt\nengineering in medical imaging. We dissect diverse prompt modalities, including\ntextual instructions, visual prompts, and learnable embeddings, and analyze\ntheir integration for core tasks such as image generation, segmentation, and\nclassification. Our synthesis reveals how these mechanisms improve\ntask-specific outcomes by enhancing accuracy, robustness, and data efficiency\nand reducing reliance on manual feature engineering while fostering greater\nmodel interpretability by making the model's guidance explicit. Despite\nsubstantial advancements, we identify persistent challenges, particularly in\nprompt design optimization, data heterogeneity, and ensuring scalability for\nclinical deployment. Finally, this review outlines promising future\ntrajectories, including advanced multimodal prompting and robust clinical\nintegration, underscoring the critical role of prompt-driven AI in accelerating\nthe revolution of diagnostics and personalized treatment planning in medicine.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u63d0\u793a\u5de5\u7a0b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5176\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6f5c\u529b\u53ca\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u5206\u5e03\u504f\u79fb\u548c\u4efb\u52a1\u6cdb\u5316\u7b49\u6311\u6218\uff0c\u63d0\u793a\u65b9\u6cd5\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u5173\u952e\u7b56\u7565\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u63d0\u793a\u5de5\u7a0b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u591a\u79cd\u5f62\u5f0f\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u89c6\u89c9\u63d0\u793a\u548c\u53ef\u5b66\u4e60\u5d4c\u5165\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u56fe\u50cf\u751f\u6210\u3001\u5206\u5272\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u63d0\u793a\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9\u624b\u5de5\u7279\u5f81\u5de5\u7a0b\u7684\u4f9d\u8d56\uff0c\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u5c55\uff0c\u63d0\u793a\u8bbe\u8ba1\u4f18\u5316\u3001\u6570\u636e\u5f02\u6784\u6027\u548c\u4e34\u5e8a\u90e8\u7f72\u7684\u53ef\u6269\u5c55\u6027\u4ecd\u662f\u6311\u6218\uff0c\u672a\u6765\u65b9\u5411\u5305\u62ec\u591a\u6a21\u6001\u63d0\u793a\u548c\u4e34\u5e8a\u6574\u5408\u3002"}}
{"id": "2507.01059", "pdf": "https://arxiv.org/pdf/2507.01059", "abs": "https://arxiv.org/abs/2507.01059", "authors": ["Xiangbo Gao", "Keshu Wu", "Hao Zhang", "Kexin Tian", "Yang Zhou", "Zhengzhong Tu"], "title": "Automated Vehicles Should be Connected with Natural Language", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "comment": null, "summary": "Multi-agent collaborative driving promises improvements in traffic safety and\nefficiency through collective perception and decision making. However, existing\ncommunication media -- including raw sensor data, neural network features, and\nperception results -- suffer limitations in bandwidth efficiency, information\ncompleteness, and agent interoperability. Moreover, traditional approaches have\nlargely ignored decision-level fusion, neglecting critical dimensions of\ncollaborative driving. In this paper we argue that addressing these challenges\nrequires a transition from purely perception-oriented data exchanges to\nexplicit intent and reasoning communication using natural language. Natural\nlanguage balances semantic density and communication bandwidth, adapts flexibly\nto real-time conditions, and bridges heterogeneous agent platforms. By enabling\nthe direct communication of intentions, rationales, and decisions, it\ntransforms collaborative driving from reactive perception-data sharing into\nproactive coordination, advancing safety, efficiency, and transparency in\nintelligent transportation systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u9a7e\u9a76\u4e2d\u7684\u610f\u56fe\u548c\u63a8\u7406\u901a\u4fe1\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u901a\u4fe1\u65b9\u5f0f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u534f\u4f5c\u9a7e\u9a76\u901a\u4fe1\u65b9\u5f0f\uff08\u5982\u4f20\u611f\u5668\u6570\u636e\u3001\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u7b49\uff09\u5728\u5e26\u5bbd\u6548\u7387\u3001\u4fe1\u606f\u5b8c\u6574\u6027\u548c\u667a\u80fd\u4f53\u4e92\u64cd\u4f5c\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u5ffd\u89c6\u4e86\u51b3\u7b56\u7ea7\u878d\u5408\u3002", "method": "\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u901a\u4fe1\u5a92\u4ecb\uff0c\u76f4\u63a5\u4f20\u9012\u610f\u56fe\u3001\u63a8\u7406\u548c\u51b3\u7b56\uff0c\u5b9e\u73b0\u4ece\u611f\u77e5\u6570\u636e\u5171\u4eab\u5230\u4e3b\u52a8\u534f\u8c03\u7684\u8f6c\u53d8\u3002", "result": "\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u63d0\u9ad8\u4e86\u534f\u4f5c\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u662f\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u9a7e\u9a76\u4e2d\u89e3\u51b3\u73b0\u6709\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.01066", "pdf": "https://arxiv.org/pdf/2507.01066", "abs": "https://arxiv.org/abs/2507.01066", "authors": ["Hanzhong Liang", "Jinghao Shi", "Xiang Shen", "Zixuan Wang", "Vera Wen", "Ardalan Mehrani", "Zhiqian Chen", "Yifan Wu", "Zhixin Zhang"], "title": "Embedding-based Retrieval in Multimodal Content Moderation", "categories": ["cs.IR", "cs.CV", "cs.LG"], "comment": "Camera ready for SIGIR 2025", "summary": "Video understanding plays a fundamental role for content moderation on short\nvideo platforms, enabling the detection of inappropriate content. While\nclassification remains the dominant approach for content moderation, it often\nstruggles in scenarios requiring rapid and cost-efficient responses, such as\ntrend adaptation and urgent escalations. To address this issue, we introduce an\nEmbedding-Based Retrieval (EBR) method designed to complement traditional\nclassification approaches. We first leverage a Supervised Contrastive Learning\n(SCL) framework to train a suite of foundation embedding models, including both\nsingle-modal and multi-modal architectures. Our models demonstrate superior\nperformance over established contrastive learning methods such as CLIP and\nMoCo. Building on these embedding models, we design and implement the\nembedding-based retrieval system that integrates embedding generation and video\nretrieval to enable efficient and effective trend handling. Comprehensive\noffline experiments on 25 diverse emerging trends show that EBR improves\nROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online\nexperiments reveal that EBR increases action rates by 10.32% and reduces\noperational costs by over 80%, while also enhancing interpretability and\nflexibility compared to classification-based solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\uff08EBR\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u8865\u5145\u4f20\u7edf\u7684\u5185\u5bb9\u5ba1\u6838\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u5728\u5feb\u901f\u54cd\u5e94\u548c\u6210\u672c\u6548\u7387\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u8d8b\u52bf\u9002\u5e94\u548c\u7d27\u6025\u5347\u7ea7\u7b49\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff08SCL\uff09\u6846\u67b6\u8bad\u7ec3\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5d4c\u5165\u751f\u6210\u4e0e\u89c6\u9891\u68c0\u7d22\u96c6\u6210\u7684\u7cfb\u7edf\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u4e2d\uff0cEBR\u5c06ROC-AUC\u4ece0.85\u63d0\u5347\u52300.99\uff0cPR-AUC\u4ece0.35\u63d0\u5347\u52300.95\uff1b\u5728\u7ebf\u5b9e\u9a8c\u4e2d\uff0c\u884c\u52a8\u7387\u63d0\u9ad8\u4e8610.32%\uff0c\u8fd0\u8425\u6210\u672c\u964d\u4f4e\u4e8680%\u3002", "conclusion": "EBR\u65b9\u6cd5\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u548c\u9ad8\u7075\u6d3b\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u3002"}}
{"id": "2507.01074", "pdf": "https://arxiv.org/pdf/2507.01074", "abs": "https://arxiv.org/abs/2507.01074", "authors": ["N. P. Garc\u00eda-de-la-Puente", "Roc\u00edo del Amor", "Fernando Garc\u00eda-Torres", "Niels M\u00f8ller Israelsen", "Coraline Lapre", "Christian Rosenberg Petersen", "Ole Bang", "Dominik Brouczek", "Martin Schwentenwein", "Kevin Neumann", "Niels Benson", "Valery Naranjo"], "title": "MID-INFRARED (MIR) OCT-based inspection in industry", "categories": ["eess.IV", "cs.CV"], "comment": "Paper accepted at i-ESA 2024 12th International Conference on\n  Interoperability for Enterprise Systems and Applications 6 pages, 2 figures,\n  2 tables", "summary": "This paper aims to evaluate mid-infrared (MIR) Optical Coherence Tomography\n(OCT) systems as a tool to penetrate different materials and detect sub-surface\nirregularities. This is useful for monitoring production processes, allowing\nNon-Destructive Inspection Techniques of great value to the industry. In this\nexploratory study, several acquisitions are made on composite and ceramics to\nknow the capabilities of the system. In addition, it is assessed which\npreprocessing and AI-enhanced vision algorithms can be anomaly-detection\nmethodologies capable of detecting abnormal zones in the analyzed objects.\nLimitations and criteria for the selection of optimal parameters will be\ndiscussed, as well as strengths and weaknesses will be highlighted.", "AI": {"tldr": "\u8bc4\u4f30\u4e2d\u7ea2\u5916\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\uff08MIR-OCT\uff09\u7cfb\u7edf\u5728\u7a7f\u900f\u6750\u6599\u548c\u68c0\u6d4b\u6b21\u8868\u9762\u5f02\u5e38\u7684\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u5de5\u4e1a\u65e0\u635f\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u5de5\u4e1a\u751f\u4ea7\u8fc7\u7a0b\u63d0\u4f9b\u65e0\u635f\u68c0\u6d4b\u6280\u672f\uff0c\u63d0\u9ad8\u6750\u6599\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u590d\u5408\u6750\u6599\u548c\u9676\u74f7\u7684\u591a\u6b21\u91c7\u96c6\uff0c\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u7ed3\u5408\u9884\u5904\u7406\u548cAI\u589e\u5f3a\u89c6\u89c9\u7b97\u6cd5\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u63a2\u8ba8\u4e86\u7cfb\u7edf\u53c2\u6570\u9009\u62e9\u7684\u4f18\u5316\u6807\u51c6\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "MIR-OCT\u7cfb\u7edf\u5728\u5de5\u4e1a\u65e0\u635f\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u53c2\u6570\u548c\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2507.01201", "pdf": "https://arxiv.org/pdf/2507.01201", "abs": "https://arxiv.org/abs/2507.01201", "authors": ["Hyoseo", "Yoon", "Yisong Yue", "Been Kim"], "title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u81ea\u52a8\u7f16\u7801\u5668\u8c03\u5236\u5668\uff08JAM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u8868\u793a\u5bf9\u9f50\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5b9e\u73b0\u6a21\u6001\u95f4\u5171\u4eab\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5728\u72ec\u7acb\u8bad\u7ec3\u540e\u6536\u655b\u5230\u5171\u4eab\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u5e76\u7814\u7a76\u5982\u4f55\u663e\u5f0f\u4f18\u5316\u8fd9\u79cd\u5bf9\u9f50\u3002", "method": "\u5f15\u5165JAM\u6846\u67b6\uff0c\u8054\u5408\u8bad\u7ec3\u6a21\u6001\u7279\u5b9a\u7684\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u91cd\u6784\u548c\u8de8\u6a21\u6001\u76ee\u6807\u9f13\u52b1\u5bf9\u9f50\u3002", "result": "JAM\u6846\u67b6\u5728\u591a\u79cd\u5bf9\u9f50\u76ee\u6807\u3001\u5c42\u6df1\u5ea6\u548c\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u4e0b\u5747\u80fd\u6709\u6548\u8bf1\u5bfc\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5c06\u901a\u7528\u5355\u6a21\u6001\u57fa\u7840\u6a21\u578b\u8f6c\u5316\u4e3a\u4e13\u7528\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u8def\u5f84\u3002"}}
{"id": "2507.01279", "pdf": "https://arxiv.org/pdf/2507.01279", "abs": "https://arxiv.org/abs/2507.01279", "authors": ["Ahmad Chaddad", "Jihao Peng", "Yihang Wu"], "title": "Classification based deep learning models for lung cancer and disease using medical images", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted in IEEE Transactions on Radiation and Plasma Medical\n  Sciences", "summary": "The use of deep learning (DL) in medical image analysis has significantly\nimproved the ability to predict lung cancer. In this study, we introduce a\nnovel deep convolutional neural network (CNN) model, named ResNet+, which is\nbased on the established ResNet framework. This model is specifically designed\nto improve the prediction of lung cancer and diseases using the images. To\naddress the challenge of missing feature information that occurs during the\ndownsampling process in CNNs, we integrate the ResNet-D module, a variant\ndesigned to enhance feature extraction capabilities by modifying the\ndownsampling layers, into the traditional ResNet model. Furthermore, a\nconvolutional attention module was incorporated into the bottleneck layers to\nenhance model generalization by allowing the network to focus on relevant\nregions of the input images. We evaluated the proposed model using five public\ndatasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and\nLCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT\n$n$=425024 images). To address class imbalance, we used data augmentation\ntechniques to artificially increase the representation of underrepresented\nclasses in the training dataset. The experimental results show that ResNet+\nmodel demonstrated remarkable accuracy/F1, reaching 98.14/98.14\\% on the\nLC25000 dataset and 99.25/99.13\\% on the IQ-OTH/NCCD dataset. Furthermore, the\nResNet+ model saved computational cost compared to the original ResNet series\nin predicting lung cancer images. The proposed model outperformed the baseline\nmodels on publicly available datasets, achieving better performance metrics.\nOur codes are publicly available at\nhttps://github.com/AIPMLab/Graduation-2024/tree/main/Peng.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aResNet+\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u6539\u8fdb\u80ba\u764c\u9884\u6d4b\uff0c\u901a\u8fc7\u96c6\u6210ResNet-D\u6a21\u5757\u548c\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfCNN\u5728\u964d\u91c7\u6837\u8fc7\u7a0b\u4e2d\u4e22\u5931\u7279\u5f81\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u80ba\u764c\u548c\u80ba\u90e8\u75be\u75c5\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408ResNet-D\u6a21\u5757\u6539\u8fdb\u964d\u91c7\u6837\u5c42\uff0c\u5e76\u5728\u74f6\u9888\u5c42\u52a0\u5165\u5377\u79ef\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u540c\u65f6\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5982LC2500\uff0898.14%\u51c6\u786e\u7387/F1\uff09\u548cIQ-OTH/NCCD\uff0899.25%\u51c6\u786e\u7387/F1\uff09\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "ResNet+\u6a21\u578b\u5728\u80ba\u764c\u9884\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.01284", "pdf": "https://arxiv.org/pdf/2507.01284", "abs": "https://arxiv.org/abs/2507.01284", "authors": ["Cristian Gariboldi", "Hayato Tokida", "Ken Kinjo", "Yuki Asada", "Alexander Carballo"], "title": "VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.LG"], "comment": "2025 IEEE 28th International Conference on Intelligent Transportation\n  Systems (ITSC)", "summary": "Recent advancements in open-source Visual Language Models (VLMs) such as\nLLaVA, Qwen-VL, and Llama have catalyzed extensive research on their\nintegration with diverse systems. The internet-scale general knowledge\nencapsulated within these models presents significant opportunities for\nenhancing autonomous driving perception, prediction, and planning capabilities.\nIn this paper we propose VLAD, a vision-language autonomous driving model,\nwhich integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end\nsystem. We implement a specialized fine-tuning approach using custom\nquestion-answer datasets designed specifically to improve the spatial reasoning\ncapabilities of the model. The enhanced VLM generates high-level navigational\ncommands that VAD subsequently processes to guide vehicle operation.\nAdditionally, our system produces interpretable natural language explanations\nof driving decisions, thereby increasing transparency and trustworthiness of\nthe traditionally black-box end-to-end architecture. Comprehensive evaluation\non the real-world nuScenes dataset demonstrates that our integrated system\nreduces average collision rates by 31.82% compared to baseline methodologies,\nestablishing a new benchmark for VLM-augmented autonomous driving systems.", "AI": {"tldr": "VLAD\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08VAD\uff09\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5b9a\u5236\u95ee\u7b54\u6570\u636e\u96c6\u5fae\u8c03VLM\uff0c\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u751f\u6210\u5bfc\u822a\u6307\u4ee4\u5e76\u89e3\u91ca\u9a7e\u9a76\u51b3\u7b56\uff0c\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u3002", "motivation": "\u5229\u7528\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982LLaVA\u3001Qwen-VL\uff09\u7684\u901a\u7528\u77e5\u8bc6\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u80fd\u529b\u3002", "method": "\u63d0\u51faVLAD\u6a21\u578b\uff0c\u5c06\u5fae\u8c03\u7684VLM\u4e0eVAD\u7cfb\u7edf\u7ed3\u5408\uff0c\u4f7f\u7528\u5b9a\u5236\u95ee\u7b54\u6570\u636e\u96c6\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u751f\u6210\u5bfc\u822a\u6307\u4ee4\u5e76\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u78b0\u649e\u7387\u5e73\u5747\u964d\u4f4e31.82%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VLAD\u4e3aVLM\u589e\u5f3a\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2507.01291", "pdf": "https://arxiv.org/pdf/2507.01291", "abs": "https://arxiv.org/abs/2507.01291", "authors": ["Wenxuan Li", "Xinze Zhou", "Qi Chen", "Tianyu Lin", "Pedro R. A. S. Bassi", "Szymon Plotka", "Jaroslaw B. Cwikla", "Xiaoxi Chen", "Chen Ye", "Zheren Zhu", "Kai Ding", "Heng Li", "Kang Wang", "Yang Yang", "Yucheng Tang", "Daguang Xu", "Alan L. Yuille", "Zongwei Zhou"], "title": "PanTS: The Pancreatic Tumor Segmentation Dataset", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "PanTS is a large-scale, multi-institutional dataset curated to advance\nresearch in pancreatic CT analysis. It contains 36,390 CT scans from 145\nmedical centers, with expert-validated, voxel-wise annotations of over 993,000\nanatomical structures, covering pancreatic tumors, pancreas head, body, and\ntail, and 24 surrounding anatomical structures such as vascular/skeletal\nstructures and abdominal/thoracic organs. Each scan includes metadata such as\npatient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness,\netc. AI models trained on PanTS achieve significantly better performance in\npancreatic tumor detection, localization, and segmentation compared to those\ntrained on existing public datasets. Our analysis indicates that these gains\nare directly attributable to the 16x larger-scale tumor annotations and\nindirectly supported by the 24 additional surrounding anatomical structures. As\nthe largest and most comprehensive resource of its kind, PanTS offers a new\nbenchmark for developing and evaluating AI models in pancreatic CT analysis.", "AI": {"tldr": "PanTS\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u673a\u6784\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63a8\u52a8\u80f0\u817aCT\u5206\u6790\u7814\u7a76\uff0c\u5305\u542b36,390\u4e2aCT\u626b\u63cf\u548c993,000\u591a\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u4f53\u7d20\u7ea7\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u6a21\u578b\u5728\u80f0\u817a\u80bf\u7624\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u5206\u5272\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u516c\u5171\u6570\u636e\u96c6\u5728\u80f0\u817aCT\u5206\u6790\u4e2d\u8868\u73b0\u6709\u9650\uff0cPanTS\u65e8\u5728\u901a\u8fc7\u66f4\u5927\u89c4\u6a21\u7684\u6807\u6ce8\u548c\u66f4\u5168\u9762\u7684\u89e3\u5256\u7ed3\u6784\u8986\u76d6\uff0c\u63d0\u5347AI\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "PanTS\u6570\u636e\u96c6\u5305\u542b36,390\u4e2aCT\u626b\u63cf\uff0c\u8986\u76d6\u80f0\u817a\u80bf\u7624\u53ca\u5176\u5468\u56f424\u4e2a\u89e3\u5256\u7ed3\u6784\uff0c\u6bcf\u4e2a\u626b\u63cf\u9644\u5e26\u4e30\u5bcc\u7684\u5143\u6570\u636e\u3002", "result": "\u5728PanTS\u4e0a\u8bad\u7ec3\u7684AI\u6a21\u578b\u5728\u80f0\u817a\u80bf\u7624\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\uff0c\u6027\u80fd\u63d0\u5347\u5f52\u56e0\u4e8e16\u500d\u66f4\u5927\u7684\u80bf\u7624\u6807\u6ce8\u89c4\u6a21\u548c\u989d\u5916\u7684\u89e3\u5256\u7ed3\u6784\u652f\u6301\u3002", "conclusion": "PanTS\u662f\u76ee\u524d\u6700\u5927\u3001\u6700\u5168\u9762\u7684\u80f0\u817aCT\u5206\u6790\u8d44\u6e90\uff0c\u4e3aAI\u6a21\u578b\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.01308", "pdf": "https://arxiv.org/pdf/2507.01308", "abs": "https://arxiv.org/abs/2507.01308", "authors": ["Muhammad Atta ur Rahman", "Dooseop Choi", "KyoungWook Min"], "title": "LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at the 17th IEEE International Conference on Advanced\n  Computational Intelligence (ICACI 2025)", "summary": "Accurate motion forecasting is critical for safe and efficient autonomous\ndriving, enabling vehicles to predict future trajectories and make informed\ndecisions in complex traffic scenarios. Most of the current designs of motion\nprediction models are based on the major representation of lane centerlines,\nwhich limits their capability to capture critical road environments and traffic\nrules and constraints. In this work, we propose an enhanced motion forecasting\nmodel informed by multiple vector map elements, including lane boundaries and\nroad edges, that facilitates a richer and more complete representation of\ndriving environments. An effective feature fusion strategy is developed to\nmerge information in different vector map components, where the model learns\nholistic information on road structures and their interactions with agents.\nSince encoding more information about the road environment increases memory\nusage and is computationally expensive, we developed an effective pruning\nmechanism that filters the most relevant map connections to the target agent,\nensuring computational efficiency while maintaining essential spatial and\nsemantic relationships for accurate trajectory prediction. Overcoming the\nlimitations of lane centerline-based models, our method provides a more\ninformative and efficient representation of the driving environment and\nadvances the state of the art for autonomous vehicle motion forecasting. We\nverify our approach with extensive experiments on the Argoverse 2 motion\nforecasting dataset, where our method maintains competitiveness on AV2 while\nachieving improved performance.\n  Index Terms-Autonomous driving, trajectory prediction, vector map elements,\nroad topology, connection pruning, Argoverse 2.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5411\u91cf\u5730\u56fe\u5143\u7d20\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u8f66\u9053\u8fb9\u754c\u548c\u9053\u8def\u8fb9\u7f18\u7b49\u4fe1\u606f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u526a\u679d\u673a\u5236\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8f66\u9053\u4e2d\u5fc3\u7ebf\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u6355\u6349\u9053\u8def\u73af\u5883\u548c\u4ea4\u901a\u89c4\u5219\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7279\u5f81\u878d\u5408\u7b56\u7565\u548c\u526a\u679d\u673a\u5236\uff0c\u6574\u5408\u591a\u5411\u91cf\u5730\u56fe\u5143\u7d20\u5e76\u7b5b\u9009\u76f8\u5173\u8fde\u63a5\uff0c\u4ee5\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728Argoverse 2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u7ade\u4e89\u529b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u4e14\u9ad8\u6548\u7684\u9053\u8def\u73af\u5883\u8868\u793a\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u9884\u6d4b\u7684\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2507.01323", "pdf": "https://arxiv.org/pdf/2507.01323", "abs": "https://arxiv.org/abs/2507.01323", "authors": ["Rongchang Zhao", "Huanchi Liu", "Jian Zhang"], "title": "SWinMamba: Serpentine Window State Space Model for Vascular Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Vascular segmentation in medical images is crucial for disease diagnosis and\nsurgical navigation. However, the segmented vascular structure is often\ndiscontinuous due to its slender nature and inadequate prior modeling. In this\npaper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve\naccurate vascular segmentation. The proposed SWinMamba innovatively models the\ncontinuity of slender vascular structures by incorporating serpentine window\nsequences into bidirectional state space models. The serpentine window\nsequences enable efficient feature capturing by adaptively guiding global\nvisual context modeling to the vascular structure. Specifically, the Serpentine\nWindow Tokenizer (SWToken) adaptively splits the input image using overlapping\nserpentine window sequences, enabling flexible receptive fields (RFs) for\nvascular structure modeling. The Bidirectional Aggregation Module (BAM)\nintegrates coherent local features in the RFs for vascular continuity\nrepresentation. In addition, dual-domain learning with Spatial-Frequency Fusion\nUnit (SFFU) is designed to enhance the feature representation of vascular\nstructure. Extensive experiments on three challenging datasets demonstrate that\nthe proposed SWinMamba achieves superior performance with complete and\nconnected vessels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSWinMamba\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u86c7\u5f62\u7a97\u53e3\u5e8f\u5217\u548c\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u533b\u5b66\u56fe\u50cf\u4e2d\u8840\u7ba1\u7684\u51c6\u786e\u5206\u5272\u3002", "motivation": "\u8840\u7ba1\u5206\u5272\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5e38\u56e0\u8840\u7ba1\u7ec6\u957f\u548c\u5148\u9a8c\u5efa\u6a21\u4e0d\u8db3\u5bfc\u81f4\u5206\u5272\u4e0d\u8fde\u7eed\u3002", "method": "\u7ed3\u5408\u86c7\u5f62\u7a97\u53e3\u5e8f\u5217\u548c\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86SWToken\u548cBAM\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u53cc\u57df\u5b66\u4e60\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u5b8c\u6574\u4e14\u8fde\u7eed\u7684\u8840\u7ba1\u5206\u5272\u3002", "conclusion": "SWinMamba\u901a\u8fc7\u521b\u65b0\u5efa\u6a21\u8840\u7ba1\u8fde\u7eed\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6548\u679c\u3002"}}
{"id": "2507.01326", "pdf": "https://arxiv.org/pdf/2507.01326", "abs": "https://arxiv.org/abs/2507.01326", "authors": ["Dong Liang", "Xingyu Qiu", "Yuzhen Li", "Wei Wang", "Kuanquan Wang", "Suyu Dong", "Gongning Luo"], "title": "Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 3 figures, accepted by MICCAI", "summary": "MR imaging techniques are of great benefit to disease diagnosis. However, due\nto the limitation of MR devices, significant intensity inhomogeneity often\nexists in imaging results, which impedes both qualitative and quantitative\nmedical analysis. Recently, several unsupervised deep learning-based models\nhave been proposed for MR image improvement. However, these models merely\nconcentrate on global appearance learning, and neglect constraints from image\nstructures and smoothness of bias field, leading to distorted corrected\nresults. In this paper, novel structure and smoothness constrained dual\nnetworks, named S2DNets, are proposed aiming to self-supervised bias field\ncorrection. S2DNets introduce piece-wise structural constraints and smoothness\nof bias field for network training to effectively remove non-uniform intensity\nand retain much more structural details. Extensive experiments executed on both\nclinical and simulated MR datasets show that the proposed model outperforms\nother conventional and deep learning-based models. In addition to comparison on\nvisual metrics, downstream MR image segmentation tasks are also used to\nevaluate the impact of the proposed model. The source code is available at:\nhttps://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS2DNets\u7684\u53cc\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u548c\u5e73\u6ed1\u6027\u7ea6\u675f\u81ea\u76d1\u7763\u6821\u6b63MR\u56fe\u50cf\u7684\u504f\u7f6e\u573a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u7ed3\u6784\u7ec6\u8282\u4fdd\u7559\u3002", "motivation": "MR\u56fe\u50cf\u5e38\u56e0\u8bbe\u5907\u9650\u5236\u5b58\u5728\u5f3a\u5ea6\u4e0d\u5747\u5300\u6027\uff0c\u5f71\u54cd\u8bca\u65ad\u548c\u5206\u6790\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5ffd\u89c6\u7ed3\u6784\u7ea6\u675f\u548c\u5e73\u6ed1\u6027\uff0c\u5bfc\u81f4\u6821\u6b63\u7ed3\u679c\u5931\u771f\u3002", "method": "\u63d0\u51faS2DNets\uff0c\u7ed3\u5408\u5206\u6bb5\u7ed3\u6784\u7ea6\u675f\u548c\u504f\u7f6e\u573a\u5e73\u6ed1\u6027\uff0c\u901a\u8fc7\u53cc\u7f51\u7edc\u81ea\u76d1\u7763\u6821\u6b63MR\u56fe\u50cf\u3002", "result": "\u5728\u4e34\u5e8a\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u53ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "S2DNets\u80fd\u6709\u6548\u53bb\u9664\u4e0d\u5747\u5300\u6027\u5e76\u4fdd\u7559\u7ed3\u6784\u7ec6\u8282\uff0c\u63d0\u5347MR\u56fe\u50cf\u5206\u6790\u8d28\u91cf\u3002"}}
{"id": "2507.01387", "pdf": "https://arxiv.org/pdf/2507.01387", "abs": "https://arxiv.org/abs/2507.01387", "authors": ["Ahmad Soliman", "Ron Keuth", "Marian Himstedt"], "title": "BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The limited availability of bronchoscopy images makes image synthesis\nparticularly interesting for training deep learning models. Robust image\ntranslation across different domains -- virtual bronchoscopy, phantom as well\nas in-vivo and ex-vivo image data -- is pivotal for clinical applications. This\npaper proposes BronchoGAN introducing anatomical constraints for image-to-image\ntranslation being integrated into a conditional GAN. In particular, we force\nbronchial orifices to match across input and output images. We further propose\nto use foundation model-generated depth images as intermediate representation\nensuring robustness across a variety of input domains establishing models with\nsubstantially less reliance on individual training datasets. Moreover our\nintermediate depth image representation allows to easily construct paired image\ndata for training. Our experiments showed that input images from different\ndomains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to\nimages mimicking realistic human airway appearance. We demonstrated that\nanatomical settings (i.e. bronchial orifices) can be robustly preserved with\nour approach which is shown qualitatively and quantitatively by means of\nimproved FID, SSIM and dice coefficients scores. Our anatomical constraints\nenabled an improvement in the Dice coefficient of up to 0.43 for synthetic\nimages. Through foundation models for intermediate depth representations,\nbronchial orifice segmentation integrated as anatomical constraints into\nconditional GANs we are able to robustly translate images from different\nbronchoscopy input domains. BronchoGAN allows to incorporate public CT scan\ndata (virtual bronchoscopy) in order to generate large-scale bronchoscopy image\ndatasets with realistic appearance. BronchoGAN enables to bridge the gap of\nmissing public bronchoscopy images.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBronchoGAN\uff0c\u901a\u8fc7\u5f15\u5165\u89e3\u5256\u7ea6\u675f\u548c\u4e2d\u95f4\u6df1\u5ea6\u56fe\u50cf\u8868\u793a\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u652f\u6c14\u7ba1\u955c\u56fe\u50cf\u57df\u7684\u7a33\u5065\u56fe\u50cf\u7ffb\u8bd1\uff0c\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u6c14\u9053\u56fe\u50cf\u3002", "motivation": "\u652f\u6c14\u7ba1\u955c\u56fe\u50cf\u6570\u636e\u6709\u9650\uff0c\u8de8\u57df\u56fe\u50cf\u7ffb\u8bd1\u5bf9\u4e34\u5e8a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u6761\u4ef6GAN\u96c6\u6210\u89e3\u5256\u7ea6\u675f\uff0c\u5f3a\u5236\u652f\u6c14\u7ba1\u5b54\u5339\u914d\uff0c\u5e76\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u6df1\u5ea6\u56fe\u50cf\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBronchoGAN\u80fd\u6210\u529f\u7ffb\u8bd1\u4e0d\u540c\u57df\u56fe\u50cf\uff0c\u4fdd\u7559\u89e3\u5256\u7ed3\u6784\uff0cFID\u3001SSIM\u548cDice\u7cfb\u6570\u663e\u8457\u63d0\u5347\u3002", "conclusion": "BronchoGAN\u901a\u8fc7\u89e3\u5256\u7ea6\u675f\u548c\u6df1\u5ea6\u8868\u793a\uff0c\u586b\u8865\u4e86\u516c\u5171\u652f\u6c14\u7ba1\u955c\u56fe\u50cf\u6570\u636e\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.01411", "pdf": "https://arxiv.org/pdf/2507.01411", "abs": "https://arxiv.org/abs/2507.01411", "authors": ["Yifei Sun", "Marshall A. Dalton", "Robert D. Sanders", "Yixuan Yuan", "Xiang Li", "Sharon L. Naismith", "Fernando Calamante", "Jinglei Lv"], "title": "Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Grey matter loss in the hippocampus is a hallmark of neurobiological aging,\nyet understanding the corresponding changes in its functional connectivity\nremains limited. Seed-based functional connectivity (FC) analysis enables\nvoxel-wise mapping of the hippocampus's synchronous activity with cortical\nregions, offering a window into functional reorganization during aging. In this\nstudy, we develop an interpretable deep learning framework to predict brain age\nfrom hippocampal FC using a three-dimensional convolutional neural network (3D\nCNN) combined with LayerCAM saliency mapping. This approach maps key\nhippocampal-cortical connections, particularly with the precuneus, cuneus,\nposterior cingulate cortex, parahippocampal cortex, left superior parietal\nlobule, and right superior temporal sulcus, that are highly sensitive to age.\nCritically, disaggregating anterior and posterior hippocampal FC reveals\ndistinct mapping aligned with their known functional specializations. These\nfindings provide new insights into the functional mechanisms of hippocampal\naging and demonstrate the power of explainable deep learning to uncover\nbiologically meaningful patterns in neuroimaging data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6d77\u9a6c\u529f\u80fd\u8fde\u63a5\u9884\u6d4b\u5927\u8111\u5e74\u9f84\uff0c\u63ed\u793a\u4e86\u4e0e\u5e74\u9f84\u76f8\u5173\u7684\u5173\u952e\u6d77\u9a6c-\u76ae\u5c42\u8fde\u63a5\u3002", "motivation": "\u6d77\u9a6c\u7070\u8d28\u51cf\u5c11\u662f\u795e\u7ecf\u751f\u7269\u8870\u8001\u7684\u6807\u5fd7\uff0c\u4f46\u5176\u529f\u80fd\u8fde\u63a5\u7684\u53d8\u5316\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528\u4e09\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff083D CNN\uff09\u7ed3\u5408LayerCAM\u663e\u8457\u6027\u6620\u5c04\uff0c\u5206\u6790\u6d77\u9a6c\u529f\u80fd\u8fde\u63a5\u3002", "result": "\u8bc6\u522b\u51fa\u4e0e\u5e74\u9f84\u9ad8\u5ea6\u654f\u611f\u7684\u6d77\u9a6c-\u76ae\u5c42\u8fde\u63a5\u533a\u57df\uff0c\u5e76\u533a\u5206\u4e86\u524d\u540e\u6d77\u9a6c\u529f\u80fd\u8fde\u63a5\u7684\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6d77\u9a6c\u8870\u8001\u7684\u529f\u80fd\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u5728\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.01513", "pdf": "https://arxiv.org/pdf/2507.01513", "abs": "https://arxiv.org/abs/2507.01513", "authors": ["Beitao Chen", "Xinyu Lyu", "Lianli Gao", "Jingkuan Song", "Heng Tao Shen"], "title": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "By incorporating visual inputs, Multimodal Large Language Models (MLLMs)\nextend LLMs to support visual reasoning. However, this integration also\nintroduces new vulnerabilities, making MLLMs susceptible to multimodal\njailbreak attacks and hindering their safe deployment.Existing defense methods,\nincluding Image-to-Text Translation, Safe Prompting, and Multimodal Safety\nTuning, attempt to address this by aligning multimodal inputs with LLMs'\nbuilt-in safeguards.Yet, they fall short in uncovering root causes of\nmultimodal vulnerabilities, particularly how harmful multimodal tokens trigger\njailbreak in MLLMs? Consequently, they remain vulnerable to text-driven\nmultimodal jailbreaks, often exhibiting overdefensive behaviors and imposing\nheavy training overhead.To bridge this gap, we present an comprehensive\nanalysis of where, how and which harmful multimodal tokens bypass safeguards in\nMLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers\nare responsible for inducing unsafe behaviors, highlighting the potential of\nprecisely removing a small subset of harmful tokens, without requiring safety\ntuning, can still effectively improve safety against jailbreaks. Motivated by\nthis, we propose Safe Prune-then-Restore (SafePTR), an training-free defense\nframework that selectively prunes harmful tokens at vulnerable layers while\nrestoring benign features at subsequent layers.Without incurring additional\ncomputational overhead, SafePTR significantly enhances the safety of MLLMs\nwhile preserving efficiency. Extensive evaluations across three MLLMs and five\nbenchmarks demonstrate SafePTR's state-of-the-art performance in mitigating\njailbreak risks without compromising utility.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u6f0f\u6d1e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9632\u5fa1\u6846\u67b6SafePTR\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u526a\u679d\u6709\u5bb3\u4ee4\u724c\u6765\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6269\u5c55\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u6f0f\u6d1e\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u89e3\u51b3\u5176\u6839\u672c\u539f\u56e0\u3002", "method": "\u63d0\u51faSafePTR\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u526a\u679d\u6709\u5bb3\u4ee4\u724c\u5e76\u6062\u590d\u826f\u6027\u7279\u5f81\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u5b89\u5168\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafePTR\u5728\u4e09\u79cdMLLMs\u548c\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u8d8a\u72f1\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6548\u7387\u3002", "conclusion": "SafePTR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347MLLMs\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2507.01559", "pdf": "https://arxiv.org/pdf/2507.01559", "abs": "https://arxiv.org/abs/2507.01559", "authors": ["Lapo Frati", "Neil Traft", "Jeff Clune", "Nick Cheney"], "title": "How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent work in continual learning has highlighted the beneficial effect of\nresampling weights in the last layer of a neural network (``zapping\"). Although\nempirical results demonstrate the effectiveness of this approach, the\nunderlying mechanisms that drive these improvements remain unclear. In this\nwork, we investigate in detail the pattern of learning and forgetting that take\nplace inside a convolutional neural network when trained in challenging\nsettings such as continual learning and few-shot transfer learning, with\nhandwritten characters and natural images. Our experiments show that models\nthat have undergone zapping during training more quickly recover from the shock\nof transferring to a new domain. Furthermore, to better observe the effect of\ncontinual learning in a multi-task setting we measure how each individual task\nis affected. This shows that, not only zapping, but the choice of optimizer can\nalso deeply affect the dynamics of learning and forgetting, causing complex\npatterns of synergy/interference between tasks to emerge when the model learns\nsequentially at transfer time.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u795e\u7ecf\u7f51\u7edc\u6700\u540e\u4e00\u5c42\u6743\u91cd\u91cd\u91c7\u6837\uff08\u201czapping\u201d\uff09\u5728\u6301\u7eed\u5b66\u4e60\u548c\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5176\u80fd\u52a0\u901f\u6a21\u578b\u9002\u5e94\u65b0\u9886\u57df\uff0c\u5e76\u63ed\u793a\u4e86\u4f18\u5316\u5668\u9009\u62e9\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u590d\u6742\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u201czapping\u201d\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u6709\u6548\uff0c\u4f46\u5176\u4f5c\u7528\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u548c\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u5b9e\u9a8c\uff0c\u5206\u6790\u201czapping\u201d\u548c\u4f18\u5316\u5668\u9009\u62e9\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u5f71\u54cd\u3002", "result": "\u201czapping\u201d\u80fd\u5e2e\u52a9\u6a21\u578b\u66f4\u5feb\u9002\u5e94\u65b0\u9886\u57df\uff0c\u4f18\u5316\u5668\u9009\u62e9\u4f1a\u663e\u8457\u5f71\u54cd\u4efb\u52a1\u95f4\u7684\u5b66\u4e60\u4e0e\u9057\u5fd8\u52a8\u6001\u3002", "conclusion": "\u201czapping\u201d\u548c\u4f18\u5316\u5668\u9009\u62e9\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u95f4\u590d\u6742\u7684\u534f\u540c/\u5e72\u6270\u6a21\u5f0f\u3002"}}
{"id": "2507.01564", "pdf": "https://arxiv.org/pdf/2507.01564", "abs": "https://arxiv.org/abs/2507.01564", "authors": ["Chia-Ming Lee", "Bo-Cheng Qiu", "Ting-Yao Chen", "Ming-Han Sun", "Fang-Ying Lin", "Jung-Tse Tsai", "I-An Tsai", "Yu-Fan Lin", "Chih-Chung Hsu"], "title": "Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present our solution for the Multi-Source COVID-19 Detection Challenge,\nwhich classifies chest CT scans from four distinct medical centers. To address\nmulti-source variability, we employ the Spatial-Slice Feature Learning (SSFL)\nframework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing\npipeline combines lung region extraction, quality control, and adaptive slice\nsampling to select eight representative slices per scan. We compare\nEfficientNet and Swin Transformer architectures on the validation set. The\nEfficientNet model achieves an F1-score of 94.68%, compared to the Swin\nTransformer's 93.34%. The results demonstrate the effectiveness of our\nKDS-based pipeline on multi-source data and highlight the importance of dataset\nbalance in multi-institutional medical imaging evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSSFL\u548cKDS\u7684\u591a\u6e90COVID-19\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u5904\u7406\u548c\u6a21\u578b\u6bd4\u8f83\uff0cEfficientNet\u8868\u73b0\u4f18\u4e8eSwin Transformer\u3002", "motivation": "\u89e3\u51b3\u591a\u6e90\u533b\u7597\u5f71\u50cf\u6570\u636e\u7684\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u63d0\u5347COVID-19\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528SSFL\u6846\u67b6\u548cKDS\u91c7\u6837\uff0c\u7ed3\u5408\u80ba\u90e8\u533a\u57df\u63d0\u53d6\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u9009\u62e9\u4ee3\u8868\u6027\u5207\u7247\uff0c\u6bd4\u8f83EfficientNet\u548cSwin Transformer\u3002", "result": "EfficientNet\u7684F1-score\u4e3a94.68%\uff0c\u4f18\u4e8eSwin Transformer\u768493.34%\u3002", "conclusion": "KDS\u9884\u5904\u7406\u6d41\u7a0b\u5728\u591a\u6e90\u6570\u636e\u4e2d\u6709\u6548\uff0c\u6570\u636e\u96c6\u5e73\u8861\u5bf9\u591a\u673a\u6784\u533b\u7597\u5f71\u50cf\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.01778", "pdf": "https://arxiv.org/pdf/2507.01778", "abs": "https://arxiv.org/abs/2507.01778", "authors": ["Vivek Tetarwal", "Sandeep Kumar"], "title": "A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification", "categories": ["cs.IT", "cs.CV", "math.IT"], "comment": "6 pages", "summary": "The installation of solar energy systems is on the rise, and therefore,\nappropriate maintenance techniques are required to be used in order to maintain\nmaximum performance levels. One of the major challenges is the automated\ndiscrimination between clean and dirty solar panels. This paper presents a\nnovel Dual Ensemble Neural Network (DENN) to classify solar panels using\nimage-based features. The suggested approach utilizes the advantages offered by\nvarious ensemble models by integrating them into a dual framework, aimed at\nimproving both classification accuracy and robustness. The DENN model is\nevaluated in comparison to current ensemble methods, showcasing its superior\nperformance across a range of assessment metrics. The proposed approach\nperforms the best compared to other methods and reaches state-of-the-art\naccuracy on experimental results for the Deep Solar Eye dataset, effectively\nserving predictive maintenance purposes in solar energy systems. It reveals the\npotential of hybrid ensemble learning techniques to further advance the\nprospects of automated solar panel inspections as a scalable solution to\nreal-world challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u53cc\u96c6\u6210\u795e\u7ecf\u7f51\u7edc\uff08DENN\uff09\uff0c\u7528\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7279\u5f81\u5206\u7c7b\u6e05\u6d01\u4e0e\u810f\u6c61\u7684\u592a\u9633\u80fd\u677f\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u5728Deep Solar Eye\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u968f\u7740\u592a\u9633\u80fd\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u9700\u8981\u6709\u6548\u7684\u7ef4\u62a4\u6280\u672f\u4ee5\u4fdd\u6301\u5176\u6027\u80fd\u3002\u81ea\u52a8\u533a\u5206\u6e05\u6d01\u4e0e\u810f\u6c61\u592a\u9633\u80fd\u677f\u662f\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u3002", "method": "\u63d0\u51fa\u53cc\u96c6\u6210\u795e\u7ecf\u7f51\u7edc\uff08DENN\uff09\uff0c\u6574\u5408\u591a\u79cd\u96c6\u6210\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4ee5\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "DENN\u5728Deep Solar Eye\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u6df7\u5408\u96c6\u6210\u5b66\u4e60\u6280\u672f\u5728\u81ea\u52a8\u5316\u592a\u9633\u80fd\u677f\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u6269\u5c55\u89e3\u51b3\u73b0\u5b9e\u95ee\u9898\u3002"}}
{"id": "2507.01790", "pdf": "https://arxiv.org/pdf/2507.01790", "abs": "https://arxiv.org/abs/2507.01790", "authors": ["Tianze Hua", "Tian Yun", "Ellie Pavlick"], "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing", "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u8f93\u5165\u4fe1\u606f\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u4f18\u5148\u5904\u7406\u67d0\u4e00\u6a21\u6001\uff0c\u5e76\u63ed\u793a\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528\u3002", "motivation": "\u7406\u89e3\u591a\u6a21\u6001\u6a21\u578b\u5728\u8f93\u5165\u4fe1\u606f\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\u53ca\u5176\u5185\u90e8\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5411\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e0d\u4e00\u81f4\u8f93\u5165\uff08\u5982\u56fe\u7247\u4e0e\u6807\u9898\u77db\u76fe\uff09\uff0c\u89c2\u5bdf\u6a21\u578b\u5bf9\u4e0d\u540c\u6a21\u6001\u7684\u504f\u597d\uff0c\u5e76\u5206\u6790\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u6a21\u578b\u503e\u5411\u4e8e\u4f18\u5148\u5904\u7406\u67d0\u4e00\u6a21\u6001\uff0c\u4e14\u5185\u90e8\u6ce8\u610f\u529b\u5934\u53ef\u8c03\u6574\u6a21\u6001\u504f\u597d\uff1b\u53d1\u73b0\u6a21\u6001\u65e0\u5173\u7684\u201c\u8def\u7531\u5934\u201d\u53ef\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bc6\u522b\u548c\u63a7\u5236\u591a\u6a21\u6001\u6a21\u578b\u5728\u51b2\u7a81\u4fe1\u53f7\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2507.01794", "pdf": "https://arxiv.org/pdf/2507.01794", "abs": "https://arxiv.org/abs/2507.01794", "authors": ["Carlo Alberto Barbano", "Benoit Dufumier", "Edouard Duchesnay", "Marco Grangetto", "Pietro Gori"], "title": "Robust brain age estimation from structural MRI with contrastive learning", "categories": ["eess.IV", "cs.CV", "68T07", "I.2.6"], "comment": "11 pages", "summary": "Estimating brain age from structural MRI has emerged as a powerful tool for\ncharacterizing normative and pathological aging. In this work, we explore\ncontrastive learning as a scalable and robust alternative to supervised\napproaches for brain age estimation. We introduce a novel contrastive loss\nfunction, $\\mathcal{L}^{exp}$, and evaluate it across multiple public\nneuroimaging datasets comprising over 20,000 scans. Our experiments reveal four\nkey findings. First, scaling pre-training on diverse, multi-site data\nconsistently improves generalization performance, cutting external mean\nabsolute error (MAE) nearly in half. Second, $\\mathcal{L}^{exp}$ is robust to\nsite-related confounds, maintaining low scanner-predictability as training size\nincreases. Third, contrastive models reliably capture accelerated aging in\npatients with cognitive impairment and Alzheimer's disease, as shown through\nbrain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike\nsupervised baselines, $\\mathcal{L}^{exp}$ maintains a strong correlation\nbetween brain age accuracy and downstream diagnostic performance, supporting\nits potential as a foundation model for neuroimaging. These results position\ncontrastive learning as a promising direction for building generalizable and\nclinically meaningful brain representations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.01808", "pdf": "https://arxiv.org/pdf/2507.01808", "abs": "https://arxiv.org/abs/2507.01808", "authors": ["Xiaoyu Ji", "Jessica Shorland", "Joshua Shank", "Pascal Delpe-Brice", "Latanya Sweeney", "Jan Allebach", "Ali Shakouri"], "title": "Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.ET", "68T01, 68T05, 68T45, 94A60"], "comment": "20 pages, 11 figures, 30 references", "summary": "Small- and medium-sized manufacturers need innovative data tools but, because\nof competition and privacy concerns, often do not want to share their\nproprietary data with researchers who might be interested in helping. This\npaper introduces a privacy-preserving platform by which manufacturers may\nsafely share their data with researchers through secure methods, so that those\nresearchers then create innovative tools to solve the manufacturers' real-world\nproblems, and then provide tools that execute solutions back onto the platform\nfor others to use with privacy and confidentiality guarantees. We illustrate\nthis problem through a particular use case which addresses an important problem\nin the large-scale manufacturing of food crystals, which is that quality\ncontrol relies on image analysis tools. Previous to our research, food crystals\nin the images were manually counted, which required substantial and\ntime-consuming human efforts, but we have developed and deployed a crystal\nanalysis tool which makes this process both more rapid and accurate. The tool\nenables automatic characterization of the crystal size distribution and numbers\nfrom microscope images while the natural imperfections from the sample\npreparation are automatically removed; a machine learning model to count high\nresolution translucent crystals and agglomeration of crystals was also\ndeveloped to aid in these efforts. The resulting algorithm was then packaged\nfor real-world use on the factory floor via a web-based app secured through the\noriginating privacy-preserving platform, allowing manufacturers to use it while\nkeeping their proprietary data secure. After demonstrating this full process,\nfuture directions are also explored.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u5e73\u53f0\uff0c\u5e2e\u52a9\u4e2d\u5c0f\u578b\u5236\u9020\u5546\u5b89\u5168\u5171\u4eab\u6570\u636e\uff0c\u7814\u7a76\u4eba\u5458\u57fa\u4e8e\u6b64\u5f00\u53d1\u5de5\u5177\u5e76\u53cd\u9988\u7ed9\u5236\u9020\u5546\u3002\u901a\u8fc7\u98df\u54c1\u6676\u4f53\u8d28\u91cf\u63a7\u5236\u7684\u6848\u4f8b\u5c55\u793a\u4e86\u8be5\u5e73\u53f0\u7684\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u4e2d\u5c0f\u578b\u5236\u9020\u5546\u56e0\u7ade\u4e89\u548c\u9690\u79c1\u95ee\u9898\u4e0d\u613f\u5171\u4eab\u6570\u636e\uff0c\u4f46\u9700\u8981\u521b\u65b0\u5de5\u5177\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u5e73\u53f0\uff0c\u5236\u9020\u5546\u901a\u8fc7\u5b89\u5168\u65b9\u6cd5\u5171\u4eab\u6570\u636e\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u5de5\u5177\u5e76\u53cd\u9988\u3002", "result": "\u6210\u529f\u5f00\u53d1\u5e76\u90e8\u7f72\u4e86\u98df\u54c1\u6676\u4f53\u81ea\u52a8\u5206\u6790\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u9690\u79c1\u4fdd\u62a4\u5e73\u53f0\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5171\u4eab\u95ee\u9898\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u5e94\u7528\u3002"}}
{"id": "2507.01828", "pdf": "https://arxiv.org/pdf/2507.01828", "abs": "https://arxiv.org/abs/2507.01828", "authors": ["Tyler Ward", "Meredith K. Owen", "O'Kira Coleman", "Brian Noehren", "Abdullah-Al-Zubaer Imran"], "title": "Autoadaptive Medical Segment Anything Model", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 2 figures, 3 tables", "summary": "Medical image segmentation is a key task in the imaging workflow, influencing\nmany image-based decisions. Traditional, fully-supervised segmentation models\nrely on large amounts of labeled training data, typically obtained through\nmanual annotation, which can be an expensive, time-consuming, and error-prone\nprocess. This signals a need for accurate, automatic, and annotation-efficient\nmethods of training these models. We propose ADA-SAM (automated,\ndomain-specific, and adaptive segment anything model), a novel multitask\nlearning framework for medical image segmentation that leverages class\nactivation maps from an auxiliary classifier to guide the predictions of the\nsemi-supervised segmentation branch, which is based on the Segment Anything\n(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient\nfeedback mechanism to create a learnable connection between the segmentation\nand classification branches by using the segmentation gradients to guide and\nimprove the classification predictions. We validate ADA-SAM on real-world\nclinical data collected during rehabilitation trials, and demonstrate that our\nproposed method outperforms both fully-supervised and semi-supervised baselines\nby double digits in limited label settings. Our code is available at:\nhttps://github.com/tbwa233/ADA-SAM.", "AI": {"tldr": "ADA-SAM\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\u8f85\u52a9\u5206\u7c7b\u5668\u7684\u7c7b\u6fc0\u6d3b\u56fe\u6307\u5bfc\u534a\u76d1\u7763\u5206\u5272\u5206\u652f\uff0c\u5e76\u7ed3\u5408\u68af\u5ea6\u53cd\u9988\u673a\u5236\u63d0\u5347\u5206\u7c7b\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u5168\u76d1\u7763\u5206\u5272\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eSegment Anything (SAM)\u6846\u67b6\uff0c\u5229\u7528\u8f85\u52a9\u5206\u7c7b\u5668\u7684\u7c7b\u6fc0\u6d3b\u56fe\u6307\u5bfc\u5206\u5272\u5206\u652f\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u53cd\u9988\u673a\u5236\u8fde\u63a5\u5206\u5272\u4e0e\u5206\u7c7b\u5206\u652f\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cADA-SAM\u5728\u6709\u9650\u6807\u6ce8\u6761\u4ef6\u4e0b\u4f18\u4e8e\u5168\u76d1\u7763\u548c\u534a\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u5347\u5e45\u5ea6\u8fbe\u4e24\u4f4d\u6570\u3002", "conclusion": "ADA-SAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.01881", "pdf": "https://arxiv.org/pdf/2507.01881", "abs": "https://arxiv.org/abs/2507.01881", "authors": ["Niccol\u00f2 McConnell", "Pardeep Vasudev", "Daisuke Yamada", "Daryl Cheng", "Mehran Azimbagirad", "John McCabe", "Shahab Aslani", "Ahmed H. Shahin", "Yukun Zhou", "The SUMMIT Consortium", "Andre Altmann", "Yipeng Hu", "Paul Taylor", "Sam M. Janes", "Daniel C. Alexander", "Joseph Jacob"], "title": "A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Low-dose computed tomography (LDCT) imaging employed in lung cancer screening\n(LCS) programs is increasing in uptake worldwide. LCS programs herald a\ngenerational opportunity to simultaneously detect cancer and non-cancer-related\nearly-stage lung disease. Yet these efforts are hampered by a shortage of\nradiologists to interpret scans at scale. Here, we present TANGERINE, a\ncomputationally frugal, open-source vision foundation model for volumetric LDCT\nanalysis. Designed for broad accessibility and rapid adaptation, TANGERINE can\nbe fine-tuned off the shelf for a wide range of disease-specific tasks with\nlimited computational resources and training data. Relative to models trained\nfrom scratch, TANGERINE demonstrates fast convergence during fine-tuning,\nthereby requiring significantly fewer GPU hours, and displays strong label\nefficiency, achieving comparable or superior performance with a fraction of\nfine-tuning data. Pretrained using self-supervised learning on over 98,000\nthoracic LDCTs, including the UK's largest LCS initiative to date and 27 public\ndatasets, TANGERINE achieves state-of-the-art performance across 14 disease\nclassification tasks, including lung cancer and multiple respiratory diseases,\nwhile generalising robustly across diverse clinical centres. By extending a\nmasked autoencoder framework to 3D imaging, TANGERINE offers a scalable\nsolution for LDCT analysis, departing from recent closed, resource-intensive\nmodels by combining architectural simplicity, public availability, and modest\ncomputational requirements. Its accessible, open-source lightweight design lays\nthe foundation for rapid integration into next-generation medical imaging tools\nthat could transform LCS initiatives, allowing them to pivot from a singular\nfocus on lung cancer detection to comprehensive respiratory disease management\nin high-risk populations.", "AI": {"tldr": "TANGERINE\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u4f4e\u5242\u91cfCT\uff08LDCT\uff09\u5206\u6790\uff0c\u53ef\u5feb\u901f\u9002\u5e94\u591a\u79cd\u75be\u75c5\u68c0\u6d4b\u4efb\u52a1\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3LDCT\u626b\u63cf\u5206\u6790\u4e2d\u653e\u5c04\u79d1\u533b\u751f\u77ed\u7f3a\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u8d85\u8fc798,000\u4e2a\u80f8\u90e8LDCT\u626b\u63cf\u6570\u636e\uff0c\u901a\u8fc73D\u63a9\u7801\u81ea\u7f16\u7801\u5668\u6846\u67b6\u5b9e\u73b0\u3002", "result": "\u572814\u79cd\u75be\u75c5\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ec\u80ba\u764c\u548c\u591a\u79cd\u547c\u5438\u7cfb\u7edf\u75be\u75c5\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u4e34\u5e8a\u4e2d\u5fc3\u3002", "conclusion": "TANGERINE\u4e3a\u4e0b\u4e00\u4ee3\u533b\u5b66\u5f71\u50cf\u5de5\u5177\u63d0\u4f9b\u4e86\u5feb\u901f\u96c6\u6210\u7684\u57fa\u7840\uff0c\u6709\u671b\u5c06\u80ba\u764c\u7b5b\u67e5\u6269\u5c55\u4e3a\u5168\u9762\u7684\u547c\u5438\u7cfb\u7edf\u75be\u75c5\u7ba1\u7406\u3002"}}
