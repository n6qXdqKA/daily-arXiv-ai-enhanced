[[toc]]

## cs.CV

### [1] [Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models](https://arxiv.org/abs/2505.17064)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: 该论文提出了一种评估文本到图像（TTI）扩散模型在历史背景描绘中的准确性的方法，并揭示了模型在历史风格、一致性和人口统计表示方面的系统性问题。

- Motivation: 随着TTI模型在内容创作中的影响力增加，研究其社会和文化影响变得重要，尤其是历史背景的准确描绘尚未被充分探索。
- Method: 作者使用HistVis数据集（包含30,000张合成图像），评估了三种先进扩散模型在历史风格关联、历史一致性和人口统计表示方面的表现。
- Result: 研究发现TTI模型在历史主题图像中存在系统性不准确，包括刻板风格、时代错误和人口统计偏差。
- Conclusion: 该研究为评估和改进TTI模型的历史准确性提供了方法论和基准，是迈向更准确和文化对齐模型的第一步。


### [2] [EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language](https://arxiv.org/abs/2505.17090)
*Phoebe Chua,Cathy Mengying Fang,Takehiko Ohkawa,Raja Kushalnagar,Suranga Nanayakkara,Pattie Maes*

Main category: cs.CV

TL;DR: EmoSign是首个包含200个美国手语视频的情感数据集，填补了手语情感研究的空白，并提供了基线模型。

- Motivation: 手语中情感表达的研究不足，导致关键场景中的沟通障碍。
- Method: 收集并标注200个ASL视频，由3名聋人手语专家进行情感和情绪标注，并提供基线分类模型。
- Result: 创建了EmoSign数据集，为多模态手语情感识别设立了新基准。
- Conclusion: EmoSign填补了研究空白，为手语情感识别提供了重要资源。


### [3] [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
*Yanshu Li,JianJiang Yang,Bozheng Li,Ruixiang Tang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CAMA的方法，通过校准LVLM的注意力机制来提升多模态ICL的稳定性。

- Motivation: 多模态ICL在LVLMs中表现不稳定，现有研究忽视了对注意力内部机制的优化。
- Method: 提出Context-Aware Modulated Attention (CAMA)，一种无需训练的即插即用方法，直接校准LVLM的注意力对数。
- Result: 在四个LVLMs和六个基准测试中验证了CAMA的有效性和通用性。
- Conclusion: CAMA为深入探索和利用LVLM注意力动态提供了新机会，推动了多模态推理的发展。


### [4] [Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts](https://arxiv.org/abs/2505.17127)
*Michal Golovanevsky,William Rudman,Michael Lepori,Amir Bar,Ritambhara Singh,Carsten Eickhoff*

Main category: cs.CV

TL;DR: 论文研究了多模态大语言模型（MLLMs）在视觉问答任务中依赖记忆的世界知识还是视觉输入，通过Visual CounterFact数据集和PvP方法揭示了视觉信息最终覆盖记忆先验的竞争机制。

- Motivation: 探究MLLMs在视觉任务中依赖记忆知识还是视觉输入，并开发工具控制模型行为。
- Method: 引入Visual CounterFact数据集和PvP方法，通过激活干预控制模型输出。
- Result: PvP成功将92.5%的颜色和74.6%的大小预测从记忆先验转向视觉输入。
- Conclusion: 研究提供了理解和控制多模态模型事实行为的新工具。


### [5] [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
*Tanqiu Jiang,Jiacheng Liang,Rongyi Zhu,Jiawei Zhou,Fenglong Ma,Ting Wang*

Main category: cs.CV

TL;DR: DTR是一种新型推理时防御方法，通过优化模型的KV缓存来减轻多模态越狱攻击，无需依赖特定安全数据或昂贵转换。

- Motivation: 大型视觉语言模型（VLM）易受越狱攻击，需要高效防御方法。
- Method: 动态调整视觉标记权重，最小化对抗性视觉输入的影响。
- Result: DTR在攻击鲁棒性和良性任务性能上优于现有防御方法。
- Conclusion: DTR首次成功将KV缓存优化应用于多模态基础模型的安全增强。


### [6] [A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data](https://arxiv.org/abs/2505.17201)
*Chaim Chai Elchik,Fatemeh Karimi Nejadasl,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.CV

TL;DR: 该论文提出了一种多视角框架，用于水下鱼类检测与追踪，解决了传统单视角模型在复杂3D运动和噪声环境中的不足。

- Motivation: 水下环境中鱼类的小目标追踪面临复杂3D运动和噪声的挑战，传统单视角模型表现不佳。
- Method: 结合FairMOT和YOLOv8模型，开发多视角框架，利用立体视频输入提升追踪精度和鱼类行为识别。
- Result: 框架在鱼类检测中达到47%的相对准确率，并通过立体匹配技术生成3D输出。
- Conclusion: 多视角框架显著提升了水下鱼类追踪的精度和可靠性，为生态研究提供了更全面的鱼类行为分析工具。


### [7] [REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge](https://arxiv.org/abs/2505.17223)
*Siyang Song,Micol Spitale,Xiangyu Kong,Hengde Zhu,Cheng Luo,Cristina Palmero,German Barquero,Sergio Escalera,Michel Valstar,Mohamed Daoudi,Tobias Baur,Fabien Ringeval,Andrew Howes,Elisabeth Andre,Hatice Gunes*

Main category: cs.CV

TL;DR: REACT 2025挑战赛旨在开发能生成多样、逼真且同步的人类面部反应的ML模型，并提供了大规模多模态数据集MARS。

- Motivation: 推动机器生成多样化且逼真的人类面部反应，以提升人机交互的自然性。
- Method: 提供MARS数据集，包含137对人类交互数据，并设立离线与在线两个子挑战。
- Result: 提出了挑战赛的基线和公开代码，供参与者参考和优化。
- Conclusion: REACT 2025挑战赛为ML模型在面部反应生成领域提供了新的基准和资源。


### [8] [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/abs/2505.17235)
*Omar Moured,Yufan Chen,Ruiping Liu,Simon Reiß,Philip Torr,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: CHAOS是一个用于评估多模态大语言模型（MLLMs）对图表扰动的鲁棒性基准，包含文本和视觉扰动，分为三个难度级别。

- Motivation: 现实应用中的图表常带有噪声或挑战性特征，MLLMs在解释扰动图表时表现不佳，因此需要系统评估其鲁棒性。
- Method: CHAOS包含5种文本和10种视觉扰动，分为三个难度级别，评估13种MLLMs在ChartQA和Chart-to-Text任务中的表现。
- Result: 实验和案例研究揭示了模型在不同图表扰动下的鲁棒性，为未来图表理解研究提供指导。
- Conclusion: CHAOS为评估和提升MLLMs在图表理解中的鲁棒性提供了重要基准，数据和代码已公开。


### [9] [Extending Dataset Pruning to Object Detection: A Variance-based Approach](https://arxiv.org/abs/2505.17245)
*Ryota Yagi*

Main category: cs.CV

TL;DR: 本文首次将分类剪枝技术扩展到目标检测领域，解决了三个关键挑战，并提出了一种新的评分方法VPS，实验证明其优于现有方法。

- Motivation: 目标检测领域的数据集剪枝研究较少，本文旨在填补这一空白，提升计算效率和存储效率。
- Method: 提出Variance-based Prediction Score (VPS)方法，结合IoU和置信度评分，解决目标检测中的三个关键问题。
- Result: 在PASCAL VOC和MS COCO上的实验表明，VPS方法在mAP上优于现有剪枝方法。
- Conclusion: 选择信息丰富的样本比数据集大小或平衡更重要，本文为复杂视觉任务的数据集剪枝奠定了基础。


### [10] [ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation](https://arxiv.org/abs/2505.17256)
*Liang Shi,Yun Fu*

Main category: cs.CV

TL;DR: ExpertGen提出了一种无需训练的框架，利用预训练专家模型（如人脸识别、属性识别和年龄估计）实现细粒度文本到人脸生成控制。

- Motivation: 现有方法需要额外训练模块以实现特定控制（如身份、属性或年龄），缺乏灵活性且资源消耗大。
- Method: 使用潜在一致性模型确保扩散步骤中的真实性和分布内预测，结合预训练专家模型生成精确引导信号。
- Result: 定性和定量实验表明，专家模型能高精度引导生成，多专家协作可实现多面部特征同时控制。
- Conclusion: ExpertGen通过直接集成现成专家模型，将其转化为即插即用组件，提升可控人脸生成的灵活性。


### [11] [Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models](https://arxiv.org/abs/2505.17280)
*Pushkar Shukla,Aditya Chinchure,Emily Diana,Alexander Tolbert,Kartik Hosanagar,Vineeth N Balasubramanian,Leonid Sigal,Matthew Turk*

Main category: cs.CV

TL;DR: BiasConnect和InterMit工具用于分析和量化文本到图像模型的偏见交互，并通过干预和算法减少偏见。

- Motivation: 理解偏见维度间的相互依赖关系，以设计更公平的生成模型。
- Method: 引入BiasConnect分析偏见交互，提出InterMit算法进行偏见缓解。
- Result: InterMit在减少偏见和步骤效率上优于传统方法，且图像质量更高。
- Conclusion: BiasConnect和InterMit为灵活、可扩展的偏见缓解解决方案。


### [12] [Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays](https://arxiv.org/abs/2505.17311)
*Harim Kim,Yuhan Wang,Minkyu Ahn,Heeyoul Choi,Yuyin Zhou,Charmgil Hong*

Main category: cs.CV

TL;DR: Diff3M是一种多模态扩散框架，结合胸部X光和电子健康记录（EHR）提升无监督异常检测（UAD）性能。通过图像-EHR交叉注意力模块和静态掩码策略，显著优于现有方法。

- Motivation: 现有基于扩散的UAD模型仅依赖图像特征，难以区分正常解剖变异和病理异常，需结合多模态数据提升检测能力。
- Method: 提出Diff3M框架，集成图像和EHR数据，引入图像-EHR交叉注意力模块和静态掩码策略。
- Result: 在CheXpert和MIMIC-CXR/IV数据集上表现优异，优于现有UAD方法。
- Conclusion: Diff3M通过多模态数据融合显著提升了医学图像异常检测性能。


### [13] [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/abs/2505.17316)
*Jiachen Jiang,Jinxin Zhou,Bo Peng,Xia Ning,Zhihui Zhu*

Main category: cs.CV

TL;DR: 论文研究了视觉嵌入与大型语言模型（LLM）的对齐问题，提出了一种新的训练方法（patch-aligned training）以增强对齐效果，实验表明该方法在多模态任务中表现优异。

- Motivation: 提升多模态LLM（MLLM）的能力，关键在于优化视觉嵌入与LLM的对齐。现有方法通过投影器连接视觉编码器和LLM，但其对齐机制尚不明确。
- Method: 研究了投影器在压缩视觉信息和对齐词嵌入中的作用，提出多语义对齐假设，并设计了patch-aligned training方法。
- Result: 实验显示，新方法显著提升了压缩能力和对齐效果，在多项任务中表现优于基线（如引用表达定位任务提升16%）。
- Conclusion: patch-aligned training是一种高效的多模态对齐方法，可扩展至其他模型。


### [14] [Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens](https://arxiv.org/abs/2505.17317)
*Alyson East,Elizabeth G. Campolongo,Luke Meyers,S M Rayeed,Samuel Stevens,Iuliia Zarubiieva,Isadora E. Fluck,Jennifer C. Girón,Maximiliane Jousse,Scott Lowe,Kayla I Perry,Isabelle Betancourt,Noah Charney,Evan Donoso,Nathan Fox,Kim J. Landsbergen,Ekaterina Nepovinnykh,Michelle Ramirez,Parkash Singh,Khum Thapa-Magar,Matthew Thompson,Evan Waite,Tanya Berger-Wolf,Hilmar Lapp,Paula Mabee,Graham Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 本文提出了一套优化生物标本图像以支持计算机视觉应用的框架，包括10个关键考虑因素，旨在弥补当前成像实践与自动化分析需求之间的差距。

- Motivation: 当前生物标本的成像协议主要针对人类视觉解释设计，未充分考虑计算机视觉分析的需求，限制了自动化分析的潜力。
- Method: 通过跨学科合作，提出10个关键考虑因素，包括标准化成像、数据存储和共享等，以优化生物标本图像。
- Result: 提出的框架能够支持自动化特征提取、物种识别及生态进化分析，提升计算机视觉应用的效率。
- Conclusion: 实施这些建议可以显著提升生物标本图像在计算机视觉中的应用潜力，关键在于详细记录方法选择。


### [15] [Game-invariant Features Through Contrastive and Domain-adversarial Learning](https://arxiv.org/abs/2505.17328)
*Dylan Kline*

Main category: cs.CV

TL;DR: 提出了一种结合对比学习和领域对抗训练的方法，学习游戏无关的视觉特征，提升跨游戏任务的泛化能力。

- Motivation: 基础游戏图像编码器容易过拟合特定游戏的视觉风格，影响在新游戏下游任务中的表现。
- Method: 结合对比学习和领域对抗训练，通过鼓励相似内容聚类并抑制游戏特定线索，学习游戏无关特征。
- Result: 在Bingsu游戏图像数据集上实验表明，模型特征不再按游戏聚类，显示成功的不变性，并具备改进的跨游戏迁移潜力。
- Conclusion: 该方法为更通用的游戏视觉模型铺平道路，减少对新游戏的重新训练需求。


### [16] [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/abs/2505.17330)
*Amit Agarwal,Srikant Panda,Kulbhushan Pachauri*

Main category: cs.CV

TL;DR: FS-DAG是一种高效、可扩展的模型架构，用于少样本场景下的视觉丰富文档理解（VRDU），通过模块化框架适应多样文档类型，性能优越且参数少于90M。

- Motivation: 解决视觉丰富文档理解中的少样本适应问题，同时应对OCR错误、拼写错误和领域偏移等实际挑战。
- Method: 结合领域特定和语言/视觉特定骨干网络，采用模块化框架，适应多样文档类型。
- Result: 在信息提取任务中表现出显著的收敛速度和性能提升，优于现有方法。
- Conclusion: FS-DAG展示了在保持高性能的同时开发更小、更高效模型的潜力，适用于资源受限的实际应用。


### [17] [Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis](https://arxiv.org/abs/2505.17333)
*Xin You,Minghui Zhang,Hanxiao Zhang,Jie Yang,Nassir Navab*

Main category: cs.CV

TL;DR: 提出了一种基于图像到视频（I2V）合成框架的方法，用于模拟呼吸运动，解决了现有方法需要高剂量扫描的局限性。通过时间差分扩散模型和提示注意力层，提升了生成视频的时间一致性和准确性。

- Motivation: 现有方法需要同时存在起始和结束帧的高剂量扫描，且患者轻微移动会导致动态背景偏差，影响时间建模。
- Method: 采用I2V框架，通过首帧预测未来帧；设计时间差分扩散模型生成时间差分场，结合提示注意力层和场增强层提升时间一致性。
- Result: 在ACDC心脏和4D肺部数据集上，生成的4D视频在感知相似性和时间一致性上优于其他方法。
- Conclusion: 该方法能够准确模拟呼吸运动轨迹，为临床图像引导应用提供了有效解决方案。


### [18] [Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering](https://arxiv.org/abs/2505.17338)
*Zhongpai Gao,Meng Zheng,Benjamin Planche,Anwesa Choudhuri,Terrence Chen,Ziyan Wu*

Main category: cs.CV

TL;DR: Render-FM是一种新型基础模型，用于直接实时渲染CT扫描的3D体积图像，通过大规模预训练消除逐扫描优化，显著提升临床适用性。

- Motivation: 当前高保真神经渲染技术需要逐场景优化，计算成本高且泛化性差，限制了在临床中的应用。
- Method: 采用编码器-解码器架构，直接从CT体积回归6D高斯喷洒参数，结合大规模预训练。
- Result: Render-FM在视觉保真度上媲美或优于逐扫描方法，并将准备时间从近一小时缩短至秒级。
- Conclusion: 该方法为实时手术规划和诊断工作流程提供了无缝集成的高质量3D可视化。


### [19] [Ocular Authentication: Fusion of Gaze and Periocular Modalities](https://arxiv.org/abs/2505.17343)
*Dillon Lohr,Michael J. Proulx,Mehedi Hasan Raju,Oleg V. Komogortsev*

Main category: cs.CV

TL;DR: 本文研究了将眼动和眼周图像两种眼中心认证模态融合的可行性，提出了一种多模态认证系统，并在大规模数据集上验证其性能优于单模态系统。

- Motivation: 探索眼动和眼周图像在多模态认证系统中的结合潜力，填补现有研究中大规模验证的空白。
- Method: 提出了一种基于先进机器学习架构的多模态认证系统，并利用包含9202名受试者的大规模数据集进行验证。
- Result: 多模态系统在所有场景中均优于单模态系统，性能超过FIDO基准。
- Conclusion: 多模态融合结合先进机器学习架构显著提升了认证性能，证明了其在大规模应用中的潜力。


### [20] [Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey](https://arxiv.org/abs/2505.17352)
*Preeti Lamba,Kiran Ravish,Ankita Kushwaha,Pawan Kumar*

Main category: cs.CV

TL;DR: 该论文提案研究了如何利用强化学习和奖励建模对齐扩散模型的输出与人类偏好和安全约束，总结了现有方法并提出了五个未来研究方向。

- Motivation: 扩散模型在生成图像等方面表现出色，但其输出与人类偏好和安全约束的对齐仍是一个关键挑战。
- Method: 通过强化学习和奖励建模技术（如人类反馈强化学习、直接偏好优化等）对扩散模型进行微调，并对方法进行分类和比较。
- Result: 总结了现有方法的优缺点，并提出了五个未来研究方向，包括多目标对齐、高效人类反馈使用等。
- Conclusion: 该提案旨在为更安全、更符合人类价值观的扩散模型生成AI提供新的见解和技术。


### [21] [Dual Ascent Diffusion for Inverse Problems](https://arxiv.org/abs/2505.17353)
*Minseo Kim,Axel Levy,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型先验的双上升优化框架，用于解决MAP问题，提升了图像质量、鲁棒性和速度。

- Motivation: 现有方法依赖计算近似，导致结果不准确或次优。
- Method: 采用双上升优化框架结合扩散模型先验。
- Result: 在图像恢复任务中表现更优，对高噪声更鲁棒，速度更快。
- Conclusion: 新框架在MAP问题中优于现有技术，更忠实于观测数据。


### [22] [Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues](https://arxiv.org/abs/2505.17358)
*Chinmay Talegaonkar,Nikhil Gandudi Suresh,Zachary Novack,Yash Belhe,Priyanka Nagasamudra,Nicholas Antipa*

Main category: cs.CV

TL;DR: 论文提出了一种在推理时通过引入散焦模糊线索的方法，提升了预训练扩散模型Marigold在零样本单目深度估计中的性能，使其无需训练即可预测度量深度。

- Motivation: 现有零样本单目度量深度估计方法在分布外数据集上性能显著下降，论文旨在通过引入散焦模糊线索解决这一问题。
- Method: 通过从同一视角拍摄小光圈和大光圈两张图像，利用散焦模糊图像形成模型的损失函数优化Marigold的度量深度缩放参数和噪声潜在变量。
- Result: 在自收集的真实数据集上，论文方法在定量和定性上均优于现有零样本单目度量深度估计方法。
- Conclusion: 论文方法成功将Marigold转化为无需训练的度量深度预测器，显著提升了零样本泛化性能。


### [23] [Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches](https://arxiv.org/abs/2505.17363)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.CV

TL;DR: 研究评估了四种深度学习架构在IoT僵尸网络检测中的效果，发现二元分类任务中所有模型表现优异，而多分类任务中GNN模型表现较差。

- Motivation: 由于IoT僵尸网络攻击的激增，研究者探索了多种先进技术以提升IoT安全性。
- Method: 评估了四种深度学习架构：VAE-MLP、VAE-GCN、VAE-GAT和ViT-MLP，使用N-BaIoT数据集进行二元和多分类任务测试。
- Result: 二元分类任务中所有模型表现优异（>99.93%），多分类任务中GNN模型（VAE-GCN、VAE-GAT）表现较差（86.42%、89.46%），而VAE-MLP和ViT-MLP表现更好（99.72%、98.38%）。
- Conclusion: GNN模型在多分类任务中表现不佳，而VAE-MLP和ViT-MLP更适合复杂分类任务。


### [24] [Optimizing YOLOv8 for Parking Space Detection: Comparative Analysis of Custom YOLOv8 Architecture](https://arxiv.org/abs/2505.17364)
*Apar Pokhrel,Gia Dao*

Main category: cs.CV

TL;DR: 本文对YOLOv8与不同主干网络（ResNet-18、VGG16、EfficientNetV2、Ghost）在停车位占用检测中的性能进行了比较分析。

- Motivation: 传统目标检测方法（如YOLOv8）在部分可见车辆、小型车辆（如摩托车）和光线不佳等边缘情况下表现不佳，需改进。
- Method: 通过集成不同主干网络（ResNet-18、VGG16、EfficientNetV2、Ghost）到YOLOv8中，并在PKLot数据集上评估其检测精度和计算效率。
- Result: 实验结果展示了各架构的优势与权衡，为停车位占用检测模型的选择提供了参考。
- Conclusion: 研究为智能停车管理系统提供了模型选择的依据，优化了边缘情况下的检测性能。


### [25] [EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion](https://arxiv.org/abs/2505.17367)
*Zichuan Yang*

Main category: cs.CV

TL;DR: EVM-Fusion是一种可解释的视觉Mamba架构，通过神经算法融合机制提升多器官医学图像分类的准确性、可解释性和泛化性。

- Motivation: 医学图像分类对临床决策至关重要，但准确性、可解释性和泛化性仍具挑战性。
- Method: 采用多路径设计，结合DenseNet、U-Net和Vision Mamba模块，通过两阶段融合（跨模态注意力和NAF块）动态整合特征。
- Result: 在9类多器官医学图像数据集上达到99.75%的测试准确率，并提供多方面的决策解释。
- Conclusion: EVM-Fusion在医学诊断中展现出可信赖AI的潜力。


### [26] [Dual-sensing driving detection model](https://arxiv.org/abs/2505.17392)
*Leon C. C. K,Zeng Hui*

Main category: cs.CV

TL;DR: 提出了一种结合计算机视觉和生理信号分析的新型双感知驾驶员疲劳检测方法，突破了单模态方法的限制。

- Motivation: 现有单模态方法存在局限性，需结合多模态优势以提高疲劳检测的准确性和可靠性。
- Method: 采用实时面部特征分析与生理信号处理结合的创新架构，结合高级融合策略。
- Result: 在控制和真实环境中均优于传统方法，保持高准确性，验证了实际应用潜力。
- Conclusion: 为驾驶员疲劳检测提供了更可靠、经济且人性化的解决方案。


### [27] [Wildfire Detection Using Vision Transformer with the Wildfire Dataset](https://arxiv.org/abs/2505.17395)
*Gowtham Raj Vuppari,Navarun Gupta,Ahmed El-Sayed,Xingguo Xiong*

Main category: cs.CV

TL;DR: 论文探讨了利用Vision Transformers（ViTs）提升野火早期检测的潜力，并分析了数据质量、计算成本和实时集成等挑战。

- Motivation: 美国尤其是加州野火频发，造成严重损失，亟需高效的检测和预防技术。
- Method: 使用10.74 GB高分辨率图像数据集，通过ViT模型进行训练，数据预处理包括调整尺寸、转换为张量格式和归一化。
- Result: ViT模型能够处理复杂图像数据，但面临数据质量、计算成本和实时集成等挑战。
- Conclusion: ViT在野火检测中具有潜力，但需解决数据、计算和实时性问题。


### [28] [Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention](https://arxiv.org/abs/2505.17412)
*Shuang Wu,Youtian Lin,Feihu Zhang,Yifei Zeng,Yikang Yang,Yajie Bao,Jiachen Qian,Siyu Zhu,Philip Torr,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: Direct3D S2是一个基于稀疏体积的可扩展3D生成框架，通过Spatial Sparse Attention机制显著提升计算效率，并在生成质量和训练成本上优于现有方法。

- Motivation: 解决高分辨率3D形状生成中计算和内存的高成本问题。
- Method: 采用稀疏体积和Spatial Sparse Attention机制，结合统一的稀疏体积变分自编码器设计。
- Result: 实现了3.9倍前向和9.6倍反向计算加速，支持1024分辨率训练仅需8个GPU。
- Conclusion: Direct3D S2在生成效率和质量上超越现有方法，使大规模3D生成更实用和可访问。


### [29] [VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR](https://arxiv.org/abs/2505.17423)
*Shenghui Chen,Po-han Li,Sandeep Chichali,Ufuk Topcu*

Main category: cs.CV

TL;DR: VIBE是一种无需标注的方法，通过评估视觉语言模型（VLM）输出的摘要质量（对齐视觉内容和任务实用性）来提升决策任务的效率和准确性。

- Motivation: 当前VLM生成的摘要冗长冗余，且现有评估方法依赖人工标注，忽略了摘要在下游任务中的实用性。
- Method: 提出VIBE方法，通过两个指标（grounding和utility）对VLM输出进行评分和排名，以支持高效决策。
- Result: 实验表明，VIBE选出的摘要显著提升任务准确性（最高61.23%）并减少响应时间（75.77%）。
- Conclusion: VIBE为VLM摘要提供了一种高效、无需标注的评估方法，显著优化了决策任务的性能。


### [30] [Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads](https://arxiv.org/abs/2505.17425)
*Wei Jie Yeo,Rui Mao,Moloud Abdar,Erik Cambria,Ranjan Satapathy*

Main category: cs.CV

TL;DR: LTC框架通过识别和修正CLIP中的虚假注意力头，提升模型在偏见任务中的性能。

- Motivation: CLIP模型可能学习到目标变量与干扰因素之间的虚假关联，影响性能。
- Method: 提出LTC框架，通过机制分析识别虚假注意力头，并通过正交投影整合任务相关特征。
- Result: 在偏见任务中，LTC比基线方法提升了50%以上的最差组准确率。
- Conclusion: LTC有效识别并修正虚假注意力头，同时可视化验证了其机制。


### [31] [Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision](https://arxiv.org/abs/2505.17437)
*Yuanshao Zhu,James Jianqiao Yu,Xiangyu Zhao,Xiao Han,Qidong Liu,Xuetao Wei,Yuxuan Liang*

Main category: cs.CV

TL;DR: OmniTraj是一个多模态轨迹检索框架，解决了传统方法在大规模数据、条件查询和轨迹相似性度量上的局限性。

- Motivation: 移动设备和数据收集技术的普及导致轨迹数据激增，现有轨迹检索方法在效率、灵活性和多模态支持上存在不足。
- Method: OmniTraj整合了四种模态（原始轨迹、拓扑、路段和区域），设计了专用编码器并将其嵌入共享表示空间。
- Result: 实验表明，OmniTraj能高效处理大规模数据，支持灵活多模态查询，适用于下游任务。
- Conclusion: OmniTraj通过多模态融合克服了传统方法的局限性，为轨迹检索提供了更高效的解决方案。


### [32] [VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models](https://arxiv.org/abs/2505.17440)
*Hefei Mei,Zirui Wang,Shen You,Minjing Dong,Chang Xu*

Main category: cs.CV

TL;DR: VEAttack是一种针对大型视觉语言模型（LVLM）的视觉编码器攻击方法，通过最小化干净和扰动视觉特征的余弦相似度生成对抗样本，无需访问后续大型语言模型、任务信息或标签。

- Motivation: 现有攻击方法在LVLM中局限于任务特定的白盒设置，计算成本高。VEAttack通过仅攻击视觉编码器，显著降低计算开销并消除任务和标签依赖性。
- Method: 通过优化图像令牌而非分类令牌生成对抗样本，最小化干净和扰动视觉特征的余弦相似度。
- Result: 在图像描述任务中性能下降94.5%，在视觉问答任务中下降75.7%。
- Conclusion: VEAttack简单有效，可泛化到多种任务，并揭示了LVLM攻击/防御的关键观察。


### [33] [Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds](https://arxiv.org/abs/2505.17442)
*Hao Jing,Anhong Wang,Yifan Zhang,Donghan Bu,Junhui Hou*

Main category: cs.CV

TL;DR: 提出了一种基于反射率预测知识蒸馏（RPKD）的3D物体检测框架，通过压缩点坐标并丢弃反射率，再通过几何反射率预测模块重建反射率，提升低比特率传输下的检测精度。

- Motivation: 解决现有压缩传输系统中反射率编码带来的传输负担和信息丢失导致的检测鲁棒性不足问题。
- Method: 提出RPKD框架，包括学生检测器和教师检测器，通过反射率知识蒸馏（RKD）和检测知识蒸馏（DKD）联合训练，提升学生检测器的鲁棒性。
- Result: 在KITTI和Waymo数据集上验证，RPKD在低码率下显著提升检测精度，KITTI数据集上达到73.6 mAP。
- Conclusion: RPKD框架有效解决了低比特率传输下的检测精度问题，优于现有方法。


### [34] [PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints](https://arxiv.org/abs/2505.17445)
*Inpyo Song,Hyemin Hwang,Jangwon Lee*

Main category: cs.CV

TL;DR: 论文介绍了PawPrint和PawPrint+数据集，用于猫狗脚印识别，以解决传统宠物识别方法的局限性，并探讨了深度学习和经典方法在不同条件下的表现。

- Motivation: 随着美国家庭宠物拥有率上升，传统宠物识别方法（如GPS标签或ID照片）存在易移除、信号问题等局限性，亟需更有效的非侵入式识别方法。
- Method: 通过PawPrint和PawPrint+数据集，结合现代深度神经网络（如CNN、Transformers）和经典局部特征方法进行基准测试。
- Result: 研究发现不同方法在基质复杂性和数据可用性条件下各有优劣，建议结合全局表示与局部描述符以提高可靠性。
- Conclusion: 脚印识别为非侵入式替代方案，有望在宠物管理和野生动物保护中发挥重要作用。


### [35] [Real-time Traffic Accident Anticipation with Feature Reuse](https://arxiv.org/abs/2505.17449)
*Inpyo Song,Jangwon Lee*

Main category: cs.CV

TL;DR: RARE是一个轻量级框架，通过重用预训练目标检测器的中间特征，显著降低了延迟，并引入注意力分数排名损失，提高了事故预测的准确性和可解释性。

- Motivation: 实时预测交通事故对自动驾驶安全至关重要，但现有方法依赖计算密集型模块，难以实际部署。
- Method: RARE利用预训练目标检测器的中间特征，避免额外特征提取，并引入注意力分数排名损失。
- Result: 在DAD和CCD基准测试中，RARE速度提升4-8倍，延迟降至13.6ms/帧（73.3 FPS），同时保持最高平均精度。
- Conclusion: RARE在实时性和准确性上表现优异，适用于安全关键应用。


### [36] [Graph Mamba for Efficient Whole Slide Image Understanding](https://arxiv.org/abs/2505.17457)
*Jiaxuan Lu,Junyan Shi,Yuhui Lin,Fang Yan,Yue Gao,Shaoting Zhang,Xiaosong Wang*

Main category: cs.CV

TL;DR: WSI-GMamba框架结合GNN的关系建模能力和Mamba的高效性，解决了WSI分析中的可扩展性和计算成本问题。

- Motivation: WSI的高分辨率和大尺寸对医学图像分析提出了挑战，现有MIL方法在可扩展性和计算成本上存在局限。
- Method: 提出WSI-GMamba框架，整合GNN和Mamba，通过Bi-SSM实现高效特征聚合。
- Result: 性能达到Transformer级别，计算量减少7倍。
- Conclusion: WSI-GMamba为大规模WSI分析提供了高精度和高效的计算解决方案。


### [37] [Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies](https://arxiv.org/abs/2505.17461)
*Kazuki Hayashi,Shintaro Ozaki,Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CV

TL;DR: LVLMs能解释色觉缺陷（CVDs）但无法模拟色觉缺陷者在图像任务中的感知，需更多关注感知多样性和公平性。

- Motivation: 研究LVLMs是否能处理人类视觉感知的多样性，特别是色觉缺陷（CVDs）和文化语言差异。
- Method: 使用Ishihara测试评估LVLMs在自然语言和图像任务中对CVDs的模拟能力。
- Result: LVLMs能用自然语言解释CVDs，但无法在图像任务中模拟色觉缺陷者的感知。
- Conclusion: 需开发能处理颜色感知多样性的多模态系统，推动感知包容性和公平性讨论。


### [38] [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
*Jiangning Zhu,Yuxing Zhou,Zheng Wang,Juntao Yao,Yima Gu,Yuhui Yuan,Shixia Liu*

Main category: cs.CV

TL;DR: OrionBench是一个用于提升视觉语言模型（VLMs）在图表和人类可识别对象（HROs）检测准确性的基准数据集，包含大量真实和合成信息图，并通过多种应用验证其有效性。

- Motivation: 现有VLMs在图表理解中的视觉定位能力不足，限制了其在实际应用中的表现。
- Method: 提出OrionBench基准数据集，结合模型在环和程序化方法生成大量标注数据。
- Result: OrionBench包含26,250张真实和78,750张合成信息图，标注超过690万个边界框，并通过三种应用验证其有效性。
- Conclusion: OrionBench为提升VLMs的图表理解能力提供了重要支持，并展示了在文档布局和UI元素检测中的潜在应用。


### [39] [PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation](https://arxiv.org/abs/2505.17475)
*Uyoung Jeong,Jonathan Freer,Seungryul Baek,Hyung Jin Chang,Kwang In Kim*

Main category: cs.CV

TL;DR: PoseBH是一个多数据集训练框架，通过非参数关键点原型和跨类型自监督机制解决姿态估计中的骨架异质性问题。

- Motivation: 解决多数据集训练中骨架异质性和有限监督的挑战。
- Method: 提出非参数关键点原型和跨类型自监督机制。
- Result: 在多个数据集上显著提升泛化能力，并保持标准基准性能。
- Conclusion: PoseBH有效解决了骨架异质性，且学习到的关键点嵌入可迁移至其他任务。


### [40] [The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts](https://arxiv.org/abs/2505.17476)
*Yuchen Zhang,Yaxiong Wang,Yujiao Wu,Lianwei Wu,Li Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种新方法，通过多模态大语言模型（MLLM）生成高风险虚假信息，并构建了MDSM数据集和AMD框架，以检测和应对MLLM驱动的多模态欺骗。

- Motivation: 当前方法低估了MLLM驱动的欺骗风险，且依赖不现实的语义不连贯内容。论文旨在解决这些局限性。
- Method: 提出MDSM数据集和AMD框架，包括Artifact Pre-perception Encoding和Manipulation-Oriented Reasoning。
- Result: 实验验证了AMD框架在检测MLLM驱动的多模态欺骗方面的优越性。
- Conclusion: 论文提出的方法为检测MLLM驱动的虚假信息提供了有效的解决方案。


### [41] [Research on Defect Detection Method of Motor Control Board Based on Image Processing](https://arxiv.org/abs/2505.17493)
*Jingde Huang,Zhangyu Huang,Chenyu Li,Jiantong Liu*

Main category: cs.CV

TL;DR: 论文研究了基于图像处理的电机控制板缺陷检测技术，通过噪声抑制、特征提取和优化算法，实现了99%以上的检测准确率，适用于生产线上的高效检测。

- Motivation: 电机控制板存在色差、插接位置错误、焊锡短路等缺陷，直接影响其性能和稳定性，因此研究缺陷检测技术对提升质量控制水平至关重要。
- Method: 研究了电机控制板的数字图像处理方法，分析了噪声抑制技术；建立了缺陷特征提取和色差识别模型；优化了缺陷图像搜索算法。
- Result: 实验结果表明，基于图像处理的缺陷检测模型准确率超过99%，适用于生产线上大批量电机控制板的实时处理。
- Conclusion: 该缺陷检测方法不仅可用于电机控制板的在线检测，还为集成电路板的缺陷处理提供了行业解决方案。


### [42] [RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition](https://arxiv.org/abs/2505.17501)
*Yuehan Jin,Xiaoqing Liu,Yiyuan Yang,Zhiwen Yu,Tong Zhang,Kaixiang Yang*

Main category: cs.CV

TL;DR: 提出RoHyDR框架，通过混合扩散和对抗学习解决多模态情感识别中的缺失模态问题，提升鲁棒性。

- Motivation: 解决现实世界中因噪声或传感器故障导致的多模态数据缺失或损坏问题（IMER挑战）。
- Method: 结合扩散生成器和对抗学习，在单模态和多模态层面恢复缺失信息，并采用多阶段优化策略。
- Result: 在两个多模态情感识别基准测试中表现优于现有方法，适应多种缺失模态场景。
- Conclusion: RoHyDR框架有效缓解了因缺失模态导致的性能下降，具有实际应用潜力。


### [43] [Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning](https://arxiv.org/abs/2505.17509)
*Shiji Zhao,Qihui Zhu,Shukun Xiong,Shouwei Ruan,Yize Fan,Ranjie Duan,Qing Guo,Xingxing Wei*

Main category: cs.CV

TL;DR: 论文提出了一种名为AMPT的方法，通过多提示学习和条件权重路由增强视觉语言模型对抗攻击的鲁棒性。

- Motivation: 现有对抗提示调优方法在面对多种对抗攻击时泛化能力不足，易过拟合。
- Method: 提出AMPT方法，学习混合文本提示并通过条件权重路由生成样本特定的聚合文本特征。
- Result: 在11个数据集上，AMPT优于现有方法，表现出更强的对抗鲁棒性。
- Conclusion: AMPT通过多提示学习和动态权重分配，显著提升了模型对抗攻击的泛化能力。


### [44] [Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding](https://arxiv.org/abs/2505.17529)
*Yeongjae Cho,Keonwoo Kim,Taebaek Hwang,Sungzoon Cho*

Main category: cs.CV

TL;DR: 提出了一种名为Ensemble Decoding (ED)的新方法，通过分割输入图像并结合注意力图权重分配，显著减少了大型视觉语言模型中的物体幻觉问题。

- Motivation: 大型视觉语言模型在图像描述和视觉问答任务中表现优异，但仍存在物体幻觉问题，即模型生成不准确的描述。现有方法在可扩展性和依赖外部模块方面存在不足。
- Method: 提出ED方法，将输入图像分割为子图像，通过注意力图权重分配结合logit分布，并引入ED自适应合理性约束和FastED变体。
- Result: 在多个幻觉基准测试中，ED方法取得了最先进的性能。
- Conclusion: ED方法有效解决了物体幻觉问题，并在性能和速度上表现出色。


### [45] [Co-Reinforcement Learning for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.17534)
*Jingjing Jiang,Chongjie Si,Jun Luo,Hanwang Zhang,Chao Ma*

Main category: cs.CV

TL;DR: 本文提出了一种通过群体相对策略优化的强化学习方法（CoRL），用于统一多模态大型语言模型（ULMs），显著提升了生成和理解能力。

- Motivation: 探索如何通过强化学习同时增强ULMs的生成和理解能力，以实现跨任务协同优化。
- Method: 提出CoRL框架，包括联合优化的统一RL阶段和任务特定的精细化RL阶段。
- Result: 模型ULM-R1在三个文本到图像生成数据集上平均提升7%，在九个多模态理解基准上平均提升23%。
- Conclusion: CoRL框架有效促进了ULMs的跨任务协同优化，证明了强化学习在多模态任务中的潜力。


### [46] [RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning](https://arxiv.org/abs/2505.17540)
*Mingrui Wu,Lu Wang,Pu Zhao,Fangkai Yang,Jianjin Zhang,Jianfeng Liu,Yuefeng Zhan,Weihao Han,Hao Sun,Jiayi Ji,Xiaoshuai Sun,Qingwei Lin,Weiwei Deng,Dongmei Zhang,Feng Sun,Qi Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: RePrompt是一个通过强化学习引入显式推理的提示增强框架，显著提升了文本到图像生成的空间布局保真度和组合泛化能力。

- Motivation: 现有文本到图像生成模型难以从简短且不明确的提示中准确捕捉用户意图，且现有方法生成的风格化或不现实内容缺乏视觉语义和现实组合的支撑。
- Method: 提出RePrompt框架，通过强化学习训练语言模型生成结构化、自反思的提示，优化图像级结果，利用奖励模型评估生成图像。
- Result: 在GenEval和T2I-Compbench上实验表明，RePrompt显著提升了空间布局保真度和组合泛化能力，达到最新技术水平。
- Conclusion: RePrompt通过强化学习和奖励模型实现了无需人工标注数据的端到端训练，有效提升了文本到图像生成的性能。


### [47] [T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models](https://arxiv.org/abs/2505.17550)
*Xiaoyu Ye,Songjie Cheng,Yongtao Wang,Yajiao Xiong,Yishen Li*

Main category: cs.CV

TL;DR: 论文提出了一种针对文本到视频（T2V）扩散模型的精确去学习方法，通过负引导速度预测微调和提示增强，有效消除特定概念，同时保留其他生成能力。

- Motivation: T2V模型可能生成有害内容，需防止滥用和侵权，受文本到图像（T2I）模型去学习技术启发，扩展至T2V领域。
- Method: 采用负引导速度预测微调，结合提示增强确保鲁棒性，并引入定位和保护正则化以实现精确去学习。
- Result: 实验表明，该方法能有效消除特定概念，同时保留其他生成能力，优于现有方法。
- Conclusion: 提出的方法在T2V模型中实现了高效去学习，为模型安全提供了新思路。


### [48] [Center-aware Residual Anomaly Synthesis for Multi-class Industrial Anomaly Detection](https://arxiv.org/abs/2505.17551)
*Qiyu Chen,Huiyuan Luo,Haiming Yao,Wei Luo,Zhen Qu,Chengkan Lv,Zhengtao Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CRAS的新方法，用于解决多类异常检测中的类别间干扰和类别内重叠问题，通过中心感知残差学习和距离引导异常合成，实现了高检测精度和快速推理。

- Motivation: 现有方法需要为每个类别单独部署模型，导致成本增加，且多类异常检测中存在类别间干扰和类别内重叠问题，影响检测效果。
- Method: 提出CRAS方法，利用中心感知残差学习将不同类别的样本耦合到统一中心，减少类别间干扰；通过距离引导异常合成自适应调整噪声方差，减少类别内重叠。
- Result: 在多个数据集和实际工业应用中，CRAS表现出优越的检测精度和竞争性的推理速度。
- Conclusion: CRAS是一种高效的多类异常检测方法，解决了现有问题，并公开了源代码和新数据集。


### [49] [Deeper Diffusion Models Amplify Bias](https://arxiv.org/abs/2505.17560)
*Shahin Hakemi,Naveed Akhtar,Ghulam Mubashar Hassan,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文探讨了扩散模型中的偏差-方差权衡问题，揭示了模型可能放大训练数据偏差或泄露隐私的风险，并提出了一种无需训练的方法提升生成质量。

- Motivation: 扩散模型内部机制尚不明确，可能导致偏差放大或隐私泄露问题，需系统性研究其偏差-方差权衡。
- Method: 提出一种无需训练的方法，通过在去噪过程中部分绕过中间块的贡献，临时增加生成过程的高方差。
- Result: 该方法在文本到图像和图像到图像生成中一致提升质量，理论及实验验证有效。
- Conclusion: 研究扩展了对生成模型记忆-泛化谱的理解，并提供了改进生成质量的实用方法。


### [50] [Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model](https://arxiv.org/abs/2505.17561)
*Kwanyoung Kim,Sanghyun Kim*

Main category: cs.CV

TL;DR: ANSE（主动噪声选择生成）通过注意力不确定性量化选择高质量噪声种子，提升视频扩散模型的质量和时间一致性。

- Motivation: 初始噪声选择对视频扩散模型生成质量影响显著，现有方法忽视模型内部信号，ANSE旨在解决这一问题。
- Method: 提出BANSA（贝叶斯主动噪声选择）作为获取函数，通过注意力样本熵分歧估计模型置信度，并引入Bernoulli掩码近似实现高效推理。
- Result: 在CogVideoX-2B和5B上实验显示，ANSE仅增加8%和13%推理时间，显著提升视频质量和时间一致性。
- Conclusion: ANSE为视频扩散中的噪声选择提供了一种原则性且可推广的方法。


### [51] [Enhancing Fourier-based Doppler Resolution with Diffusion Models](https://arxiv.org/abs/2505.17567)
*Denisa Qosja,Kilian Barth,Simon Wagner*

Main category: cs.CV

TL;DR: 利用人工智能提升雷达多普勒分辨率，通过扩散模型的生成神经网络优化零填充FFT，有效分离密集目标。

- Motivation: 高多普勒分辨率对检测慢速目标至关重要，但硬件和物理因素限制了分辨率，需后处理技术提升。
- Method: 基于零填充FFT，结合扩散模型的生成神经网络进行数据细化。
- Result: 方法克服传统FFT限制，成功分离密集目标。
- Conclusion: AI技术可有效提升雷达多普勒分辨率，为慢速目标检测提供新方案。


### [52] [InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO](https://arxiv.org/abs/2505.17574)
*Xueji Fang,Liyuan Ma,Zhiyang Chen,Mingyuan Zhou,Guo-jun Qi*

Main category: cs.CV

TL;DR: InfLVG是一个推理时框架，通过动态选择上下文实现长视频生成，解决了现有模型在长视频生成中的计算和一致性挑战。

- Motivation: 现有文本到视频生成模型在生成长跨场景视频时面临计算成本高和一致性下降的问题。
- Method: 引入InfLVG框架，利用可学习的上下文选择策略（GRPO优化）动态保留语义相关上下文，固定计算预算。
- Result: InfLVG能将视频长度扩展至9倍，保持跨场景一致性和语义保真度。
- Conclusion: InfLVG为长视频生成提供了一种高效且一致的解决方案。


### [53] [MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse Weather Image Recovery](https://arxiv.org/abs/2505.17581)
*Hainuo Wang,Qiming Hu,Xiaojie Guo*

Main category: cs.CV

TL;DR: MODEM提出了一种基于Morton-Order的退化估计机制，通过MOS2D和DDEM模块自适应恢复恶劣天气下的图像，取得了SOTA效果。

- Motivation: 恶劣天气导致的图像退化具有高度非均匀性和空间异质性，传统方法难以准确估计退化模式，从而限制了恢复效果。
- Method: MODEM结合了Morton-Order编码和选择性状态空间模型（MOS2D），并通过DDEM模块分离全局和局部退化先验，动态指导图像恢复。
- Result: 实验表明，MODEM在多种天气类型和基准测试中均达到最优性能。
- Conclusion: MODEM通过自适应退化估计和恢复策略，有效解决了复杂天气条件下的图像恢复问题。


### [54] [CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis](https://arxiv.org/abs/2505.17590)
*Florian Barthel,Wieland Morgenstern,Paul Hinzer,Anna Hilsmann,Peter Eisert*

Main category: cs.CV

TL;DR: CGS-GAN是一种新型3D高斯泼溅GAN框架，无需依赖视角条件即可稳定训练并生成高质量、3D一致的人头合成。

- Motivation: 现有方法通过将随机潜在向量与当前相机位置绑定来稳定训练和提升渲染质量，但牺牲了3D一致性，导致视角变化时身份显著改变。
- Method: 引入多视角正则化技术增强生成器收敛，改进条件损失函数，并设计新的生成器架构以稳定训练、高效渲染和扩展分辨率至2048²。
- Result: 在FFHQ数据集上验证，CGS-GAN实现了高渲染质量和3D一致性，FID分数表现优异。
- Conclusion: CGS-GAN在无需视角条件的情况下，实现了高质量、3D一致的合成，为3D生成模型提供了新思路。


### [55] [PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive Learning and Pathology-Informed Synthetic Embeddings](https://arxiv.org/abs/2505.17614)
*Sinchee Chin,Yinuo Ma,Xiaochen Yang,Jing-Hao Xue,Wenming Yang*

Main category: cs.CV

TL;DR: PathoSCOPE是一种少样本无监督病理检测框架，仅需少量非病理样本（最少2个样本），显著提高数据效率，通过全局-局部对比损失和病理感知嵌入生成模块实现高性能。

- Motivation: 医院数据偏向症状人群，隐私法规限制健康数据收集，现有方法需大量健康数据，PathoSCOPE旨在解决这一问题。
- Method: 提出全局-局部对比损失（GLCL）和病理感知嵌入生成模块（PiEG），减少非病理嵌入的变异性并增强病理区域的区分能力。
- Result: 在BraTS2020和ChestXray8数据集上达到无监督方法的SOTA性能，计算效率高（2.48 GFLOPs，166 FPS）。
- Conclusion: PathoSCOPE在少样本条件下高效检测病理，为无监督病理检测提供了新思路。


### [56] [Scaling Image and Video Generation via Test-Time Evolutionary Search](https://arxiv.org/abs/2505.17618)
*Haoran He,Jiajun Liang,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai,Ling Pan*

Main category: cs.CV

TL;DR: EvoSearch是一种通用的测试时扩展方法，通过进化搜索优化扩散和流模型的去噪轨迹，显著提升图像和视频生成的质量和多样性。

- Motivation: 随着模型预训练的计算成本持续增加，测试时扩展（TTS）成为提升生成模型性能的潜在方向，但现有方法在视觉任务中存在局限性，如领域受限、可扩展性差或牺牲多样性。
- Method: EvoSearch将TTS问题转化为进化搜索问题，利用生物进化原理优化去噪轨迹，通过选择和突变机制生成高质量且多样化的样本。
- Result: 实验表明，EvoSearch在扩散和流模型中均优于现有方法，生成质量更高、多样性更强，且对未见过的评估指标具有良好泛化性。
- Conclusion: EvoSearch为视觉生成任务提供了一种高效、通用的测试时扩展方法，无需额外训练或模型扩展。


### [57] [CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment](https://arxiv.org/abs/2505.17619)
*Bo Wang,De-Xing Huang,Xiao-Hu Zhou,Mei-Jiang Gui,Nu-Fang Xiao,Jian-Long Hao,Ming-Yuan Liu,Zeng-Guang Hou*

Main category: cs.CV

TL;DR: 论文提出CAS-IQA框架，利用视觉语言模型结合辅助图像信息，提升合成X光血管造影图像质量评估的准确性。

- Motivation: 现有图像质量评估方法未充分利用辅助图像参考，且缺乏临床相关的细粒度指标，导致评估效果不佳。
- Method: 提出CAS-IQA框架，结合视觉语言模型和多路径特征融合模块（MUST），利用辅助图像信息预测细粒度质量分数。
- Result: 在CAS-3K数据集上，CAS-IQA显著优于现有方法。
- Conclusion: CAS-IQA通过结合辅助信息和任务特定指标，有效提升了合成血管造影图像的质量评估效果。


### [58] [HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning](https://arxiv.org/abs/2505.17645)
*Chuhao Zhou,Jianfei Yang*

Main category: cs.CV

TL;DR: HoloLLM是一种多模态大语言模型，整合了LiDAR、红外、毫米波雷达和WiFi等传感器，解决了视觉数据依赖和异构信号表示的挑战。

- Motivation: 解决视觉语言模型在遮挡、光线不足或隐私限制下的局限性，提升多模态感知能力。
- Method: 设计了通用模态注入投影器（UMIP）和人类-VLM协作数据标注流程，以增强模态嵌入和生成配对文本注释。
- Result: 在两个新基准测试中，HoloLLM显著优于现有MLLM，语言感知准确率提升30%。
- Conclusion: 为现实世界中的多模态语言智能奠定了基础。


### [59] [Instruct2See: Learning to Remove Any Obstructions Across Distributions](https://arxiv.org/abs/2505.17649)
*Junhang Li,Yu Guo,Chuhua Xian,Shengfeng He*

Main category: cs.CV

TL;DR: Instruct2See是一个零样本框架，通过多模态提示处理遮挡问题，实现软硬掩码修复，适用于训练中未见过的障碍物。

- Motivation: 解决图像中广泛存在的遮挡问题，现有方法局限于特定障碍物，数据收集不切实际。
- Method: 将遮挡修复统一为软硬掩码问题，利用多模态提示（视觉语义和文本指令）和交叉注意力单元增强上下文理解，动态调整掩码。
- Result: 在分布内外障碍物上均表现优异，泛化能力强。
- Conclusion: Instruct2See在遮挡修复中具有强性能和泛化能力，适用于未知障碍物。


### [60] [EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy](https://arxiv.org/abs/2505.17665)
*Yichun Yu,Yuqing Lan,Zhihuan Xing,Xiaoyi Yang,Tingyue Tang,Dan Yu*

Main category: cs.CV

TL;DR: RAPNet提出了一种结合Transformer和区域感知的高分辨率遥感图像分割方法，通过CRA和GCR模块实现全局与局部特征的平衡，显著提升了分割精度。

- Motivation: 高分辨率遥感图像分割面临复杂空间布局和多样物体外观的挑战，传统CNN和Transformer各有局限。
- Method: RAPNet包含CRA模块（用Transformer捕获区域级上下文依赖）和GCR模块（学习全局类别注意力图），结合区域级分割和全局信息优化。
- Result: 在三个公开数据集上，RAPNet优于现有方法，实现了更高的多类分割准确率。
- Conclusion: RAPNet通过区域感知和全局优化，有效解决了高分辨率遥感图像分割的挑战。


### [61] [Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification](https://arxiv.org/abs/2505.17666)
*Shuxian Ma,Zihao Dong,Runmin Cong,Sam Kwong,Xiuli Shao*

Main category: cs.CV

TL;DR: 论文提出Proto-FG3D，首个基于原型的细粒度3D形状分类框架，通过原型关联、在线聚类和原型引导学习解决多视图特征聚合的局限性。

- Motivation: 细粒度3D分类因多视图特征聚合中捕获的判别信息有限（如类间细微差异、类别不平衡和参数模型可解释性不足）而研究不足。
- Method: 提出Proto-FG3D框架，包括原型关联、在线聚类和原型引导监督学习，实现从参数化softmax到非参数化原型学习的转变。
- Result: 在FG3D和ModelNet40数据集上，Proto-FG3D在准确性、透明预测和即时可解释性方面超越现有方法。
- Conclusion: Proto-FG3D为细粒度3D识别提供了新范式，挑战了传统方法，并展示了原型学习的潜力。


### [62] [SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding](https://arxiv.org/abs/2505.17674)
*Xuerui Qiu,Peixi Wu,Yaozhi Wen,Shaowei Gu,Yuqi Pan,Xinhao Luo,Bo XU,Guoqi Li*

Main category: cs.CV

TL;DR: SVL框架通过多模态预训练提升SNNs性能，在零样本3D分类等任务中超越ANNs。

- Motivation: 现有SNNs因预训练策略不足，性能不及ANNs，尤其在多模态理解和零样本任务中表现不佳。
- Method: 提出SVL框架，包含MTA（多模态对比学习）和Rep-VLI（轻量级视觉语言集成）。
- Result: SVL在零样本3D分类中达到85.4%准确率，多项下游任务表现优于ANNs和SNNs。
- Conclusion: SVL首次实现可扩展、通用且硬件友好的3D开放世界理解，缩小了SNNs与ANNs的差距。


### [63] [Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery](https://arxiv.org/abs/2505.17677)
*Ming Hu,Zhendi Yu,Feilong Tang,Kaiwen Chen,Yulong Li,Imran Razzak,Junjun He,Tolga Birdal,Kaijing Zhou,Zongyuan Ge*

Main category: cs.CV

TL;DR: OphNet-3D是一个用于眼科手术的大规模RGB-D动态3D重建数据集，包含7.1百万帧数据，并提出了自动标注流水线和两个新基准测试。

- Motivation: 缺乏真实、大规模的数据集和可靠的标注工具阻碍了基于视觉的眼科显微手术分析的进展。
- Method: 设计了多阶段自动标注流水线，结合多视角数据、运动先验和生物力学约束，并提出了H-Net和OH-Net两种架构。
- Result: 提出的模型在MPJPE和ADD-S指标上显著优于现有方法，分别提高了2mm和23%。
- Conclusion: OphNet-3D数据集和新模型为眼科手术的3D重建提供了重要工具和基准。


### [64] [5G-DIL: Domain Incremental Learning with Similarity-Aware Sampling for Dynamic 5G Indoor Localization](https://arxiv.org/abs/2505.17684)
*Nisha Lakshmana Raichur,Lucas Heublein,Christopher Mutschler,Felix Ott*

Main category: cs.CV

TL;DR: 论文提出了一种基于5G数据的室内定位方法5G-DIL，通过域增量学习（DIL）和相似性感知采样技术，快速适应环境变化，减少训练时间和资源需求。

- Motivation: 基于机器学习的5G室内定位方法在环境变化时性能下降，重新训练模型耗时耗资源。
- Method: 采用域增量学习（DIL）和基于切比雪夫距离的相似性感知采样技术，仅训练新环境的修改区域。
- Result: 在动态环境条件下，定位误差MAE为0.261米，且仅需50个样本即可适应新环境。
- Conclusion: 5G-DIL方法高效适应环境变化，显著减少训练成本，同时保持高定位精度。


### [65] [FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving](https://arxiv.org/abs/2505.17685)
*Shuang Zeng,Xinyuan Chang,Mengwei Xie,Xinran Liu,Yifan Bai,Zheng Pan,Mu Xu,Xing Wei*

Main category: cs.CV

TL;DR: 提出了一种时空链式思维（CoT）推理方法，通过视觉生成和理解增强自动驾驶的视觉推理能力。

- Motivation: 现有视觉语言模型（VLMs）通常使用离散文本CoT，可能导致时空关系模糊和细粒度信息丢失，因此探索基于真实世界模拟和想象的建模是否优于纯符号逻辑。
- Method: 提出时空CoT推理方法，利用VLM作为世界模型生成统一图像帧预测未来状态，结合感知结果和未来帧表示时空关系，并作为中间推理步骤用于轨迹规划。
- Result: 实验证明该方法有效，推动了自动驾驶向视觉推理方向发展。
- Conclusion: 通过视觉生成和理解的统一预训练范式，以及渐进式视觉CoT，显著提升了自动驾驶的视觉推理能力。


### [66] [Semi-Supervised Medical Image Segmentation via Dual Networks](https://arxiv.org/abs/2505.17690)
*Yunyao Lu,Yihang Wu,Reem Kateb,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 提出了一种创新的半监督3D医学图像分割方法，减少对大规模专家标注数据的依赖，通过双网络架构和自监督对比学习策略提升性能。

- Motivation: 传统监督模型需要大量标注数据，而半监督方法存在伪标签噪声和特征空间监督不足的问题。
- Method: 采用双网络架构优化上下文信息利用和伪标签生成，结合自监督对比学习增强网络表示。
- Result: 在临床MRI数据上表现优于现有技术。
- Conclusion: 该方法有效解决了半监督医学图像分割中的关键问题，具有实际应用潜力。


### [67] [ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection](https://arxiv.org/abs/2505.17692)
*Ziteng Yang,Jingzehua Xu,Yanshu Li,Zepeng Li,Yeqiang Wang,Xinghui Li*

Main category: cs.CV

TL;DR: ViP²-CLIP提出了一种视觉感知提示机制，通过融合全局和多尺度局部视觉上下文自适应生成细粒度文本提示，解决了零样本异常检测中手工模板和静态可学习提示的局限性。

- Motivation: 现有基于CLIP的零样本异常检测方法依赖手工或静态提示，存在语义覆盖不足和适应性差的问题，且对类别名称敏感。
- Method: ViP²-CLIP引入视觉感知提示机制（ViP-Prompt），结合全局和局部视觉上下文生成自适应文本提示，无需手工模板或类别名称先验。
- Result: 在15个工业和医学基准测试中，ViP²-CLIP实现了最先进的性能和强大的跨域泛化能力。
- Conclusion: ViP²-CLIP通过自适应提示机制显著提升了零样本异常检测的准确性和适应性，尤其在类别标签模糊或隐私受限的场景中表现突出。


### [68] [Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek](https://arxiv.org/abs/2505.17702)
*Xueyang Li,Jiahao Li,Yu Song,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: Seek-CAD是一种基于开源LLM DeepSeek-R1的训练免费方法，用于生成CAD参数模型，结合视觉和思维链反馈进行自我优化。

- Motivation: 解决闭源LLM的高成本和本地部署限制问题，探索开源LLM在CAD生成中的应用。
- Method: 利用DeepSeek-R1生成初始CAD模型，通过视觉语言模型和思维链反馈进行自我优化，并基于SSR设计范式构建数据集。
- Result: 实验验证了Seek-CAD在多种指标下的有效性。
- Conclusion: Seek-CAD为CAD生成提供了一种灵活高效的解决方案，适用于工业应用。


### [69] [SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation](https://arxiv.org/abs/2505.17721)
*Dekai Zhu,Yan Di,Stefan Gavranovic,Slobodan Ilic*

Main category: cs.CV

TL;DR: SeaLion是一种新型扩散模型，用于生成带有细粒度分割标签的高质量点云，并提出了新的评估指标p-CD。

- Motivation: 现有研究很少关注生成带有分割标签的点云及其评估方法，因此需要一种新模型填补这一空白。
- Method: 采用语义部分感知潜在点扩散技术，联合预测噪声和分割标签，并引入p-CD评估生成质量。
- Result: 在ShapeNet和IntrA数据集上，SeaLion在生成质量和多样性上显著优于DiffFacto，1-NNA（p-CD）分别提升13.33%和6.52%。
- Conclusion: SeaLion不仅支持半监督训练，还能用于生成数据增强和3D形状编辑，具有广泛应用潜力。


### [70] [Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM](https://arxiv.org/abs/2505.17726)
*Donghwan Chi,Hyomin Kim,Yoonjin Oh,Yongjin Kim,Donghoon Lee,Daejin Jo,Jongmin Kim,Junyeob Baek,Sungjin Ahn,Sungwoong Kim*

Main category: cs.CV

TL;DR: 提出了一种基于Slot Attention的面向多模态大语言模型（MLLMs）的对象中心视觉分词器，以解决现有方法在局部细节理解与生成上的不足。

- Motivation: 现有图像分词方法仅捕获全局抽象概念或均匀分割的图像块，限制了MLLMs在对象级别理解和生成详细视觉内容的能力。
- Method: 基于Q-Former编码器、扩散解码器和残差向量量化，提出离散化的slot tokens，既能编码局部视觉细节，又能保持高级语义，并与文本数据对齐。
- Result: Slot-MLLM在多种需要局部细节理解和生成的视觉语言任务中显著优于基线方法。
- Conclusion: 首次证明了在MLLMs和自然图像中实现对象中心Slot Attention的可行性。


### [71] [SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain](https://arxiv.org/abs/2505.17727)
*Jiawei Zhou,Linye Lyu,Zhuotao Tian,Cheng Zhuo,Yu Li*

Main category: cs.CV

TL;DR: SafeMVDrive是一个生成高质量、多视角安全关键驾驶视频的框架，填补了现有方法在真实世界多视角数据上的不足。

- Motivation: 现有方法无法满足端到端自动驾驶系统对真实世界多视角视频数据的需求，因此需要一种新框架来生成此类数据。
- Method: SafeMVDrive结合了安全关键轨迹生成器和多视角视频生成器，通过增强轨迹生成器的场景理解能力和引入两阶段可控轨迹生成机制，解决了现有方法的局限性。
- Result: 实验表明，生成的视频数据显著提高了端到端自动驾驶规划器的碰撞率，验证了框架的有效性。
- Conclusion: SafeMVDrive为测试自动驾驶规划模块提供了一种高效工具，代码和数据已公开。


### [72] [RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection](https://arxiv.org/abs/2505.17732)
*Ozsel Kilinc,Cem Tarhan*

Main category: cs.CV

TL;DR: 论文提出了一种新的3D物体检测方法RQR3D，通过限制四边形表示将旋转框检测转化为关键点回归任务，显著提升了性能。

- Motivation: 现有BEV-based 3D物体检测方法因角度表示导致损失函数不连续，影响性能。受航空领域启发，提出更稳定的表示方法。
- Method: RQR3D回归水平框及其与旋转框的偏移，结合无锚单阶段检测器和雷达融合骨干网络。
- Result: 在nuScenes数据集上，NDS提升4%，mAP提升2.4%，显著减少平移和方向误差。
- Conclusion: RQR3D方法稳健、精确，适用于自动驾驶的实际需求。


### [73] [R-Genie: Reasoning-Guided Generative Image Editing](https://arxiv.org/abs/2505.17768)
*Dong Zhang,Lingfeng He,Rui Yan,Fei Shen,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出了一种基于推理引导的图像编辑方法R-Genie，结合扩散模型和多模态大语言模型，能处理复杂文本查询和隐含用户意图。

- Motivation: 现有图像编辑方法受限于显式文本指令和有限操作，缺乏对隐含意图和上下文推理的深度理解。
- Method: 构建包含1000多个图像-指令-编辑三元组的数据集，提出R-Genie模型，结合扩散模型和多模态大语言模型，引入推理注意力机制。
- Result: 实验证明R-Genie能显著提升扩散模型在推理引导编辑任务中的能力。
- Conclusion: R-Genie为智能图像合成开辟了新方向，展示了推理引导编辑的潜力。


### [74] [TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving](https://arxiv.org/abs/2505.17771)
*Yanping Fu,Xinyuan Liu,Tianyu Li,Yike Ma,Yucheng Zhang,Feng Dai*

Main category: cs.CV

TL;DR: TopoPoint 是一个新颖的框架，通过显式检测车道端点和联合推理端点与车道，解决了现有方法中车道端点偏差导致拓扑构建错误的问题。

- Motivation: 现有方法在车道端点检测上存在偏差，导致拓扑推理不准确，影响自动驾驶对交叉口的理解。
- Method: TopoPoint 通过点-车道合并自注意力机制和点-车道图卷积网络，增强全局上下文共享和特征聚合。推理时使用点-车道几何匹配算法优化端点。
- Result: 在 OpenLane-V2 基准测试中，TopoPoint 在拓扑推理上达到 48.8 OLS 的领先性能，端点检测指标 DET$_p$ 上显著优于现有方法（52.6 vs. 45.2）。
- Conclusion: TopoPoint 有效解决了车道端点偏差问题，提升了拓扑推理的鲁棒性和准确性。


### [75] [TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis](https://arxiv.org/abs/2505.17778)
*Yu Xie,Jielei Zhang,Pengyu Chen,Ziyue Wang,Weihang Wang,Longwen Gao,Peiyi Li,Huyang Sun,Qiang Zhang,Qian Qiao,Jiaqing Fan,Zhouhui Lian*

Main category: cs.CV

TL;DR: TextFlux是一种基于DiT的框架，用于多语言场景文本合成，无需OCR模块，支持低资源多语言生成，训练数据需求仅为竞争方法的1%，并能灵活控制多行文本生成。

- Motivation: 现有方法依赖额外视觉条件模块和大规模标注数据，TextFlux旨在简化架构并提升多语言场景文本合成的效率和准确性。
- Method: 利用扩散模型的上下文推理能力，设计OCR-free架构，支持低资源多语言生成和精确的多行文本控制。
- Result: TextFlux在定性和定量评估中优于现有方法，尤其在低资源多语言场景下表现优异。
- Conclusion: TextFlux通过简化架构和高效训练，实现了高质量的多语言场景文本合成，具有广泛的应用潜力。


### [76] [U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding](https://arxiv.org/abs/2505.17779)
*Anjie Le,Henan Liu,Yue Wang,Zhenyu Liu,Rongkun Zhu,Taohan Weng,Jinze Yu,Boyang Wang,Yalun Wu,Kaiwen Yan,Quanlin Sun,Meirui Jiang,Jialun Pei,Siya Liu,Haoyun Zheng,Zhoujun Li,Alison Noble,Jacques Souquet,Xiaoqing Guo,Manxi Lin,Hongcheng Guo*

Main category: cs.CV

TL;DR: U2-BENCH是首个评估大型视觉语言模型（LVLMs）在超声理解任务中的综合基准，涵盖分类、检测、回归和文本生成任务，揭示了其在空间推理和临床语言生成方面的挑战。

- Motivation: 超声图像解释因操作者、噪声和解剖结构差异而具有挑战性，而LVLMs在超声领域的性能尚未充分探索。
- Method: U2-BENCH整合了7,241个病例，覆盖15个解剖区域和50个应用场景，定义了8个临床任务，评估了20种LVLMs。
- Result: LVLMs在图像分类任务中表现良好，但在空间推理和临床语言生成方面仍有困难。
- Conclusion: U2-BENCH为超声领域的LVLM研究提供了严格的测试平台，推动了多模态医学影像的发展。


### [77] [Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest Monitoring](https://arxiv.org/abs/2505.17782)
*Nikolas Papadopoulos,Nikolaos Ioannis Bountos,Maria Sdraka,Andreas Karavias,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: 本文介绍了Hephaestus Minicubes数据集，用于支持火山活动监测的深度学习研究，并提供了多模态、多时间分类和语义分割的基准测试。

- Motivation: 火山活动中的地面变形是喷发前的重要信号，但深度学习在此领域的应用因缺乏数据集而受限。本文旨在填补这一空白。
- Method: 基于Hephaestus数据集，构建了包含38个时空数据立方体的Hephaestus Minicubes，整合了InSAR、地形和大气数据，并提供专家标注。
- Result: 数据集覆盖了44座活跃火山，支持多任务基准测试，并展示了先进架构的基线性能。
- Conclusion: 该工作推动了机器学习在火山监测中的应用，促进了数据驱动方法在地球科学中的整合。


### [78] [Generative Data Augmentation for Object Point Cloud Segmentation](https://arxiv.org/abs/2505.17783)
*Dekai Zhu,Stefan Gavranovic,Flavien Boussuge,Benjamin Busam,Slobodan Ilic*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的部分感知生成方法，用于点云分割任务的数据增强，显著优于传统方法和半监督/自监督方法。

- Motivation: 传统数据增强方法在点云分割任务中数据多样性有限，而现有生成模型缺乏语义标签，限制了其应用。
- Method: 扩展了3D扩散模型Lion，使其能基于分割掩码生成高质量点云，并提出了三步生成数据增强流程和基于扩散的伪标签过滤方法。
- Result: 在两个大规模合成数据集和一个真实医学数据集上验证，生成数据增强方法优于传统方法和相关半监督/自监督方法。
- Conclusion: 该方法通过生成高质量带标签的点云数据，有效解决了数据稀缺问题，提升了点云分割模型的性能。


### [79] [DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval](https://arxiv.org/abs/2505.17796)
*Yuxin Yang,Yinan Zhou,Yuxin Chen,Ziqi Zhang,Zongyang Ma,Chunfeng Yuan,Bing Li,Lin Song,Jun Gao,Peng Li,Weiming Hu*

Main category: cs.CV

TL;DR: DetailFusion是一种新颖的双分支框架，通过协调全局和细节信息，提升组合图像检索（CIR）的性能。

- Motivation: 现有方法在全局信息融合上表现不足，难以处理细微视觉变化或复杂文本指令。
- Method: 提出DetailFusion框架，利用图像编辑数据集提取细节变化先验，并通过细节导向优化策略增强推理分支。
- Result: 在CIRR和FashionIQ数据集上实现最优性能，验证了细节增强的有效性和跨域适应性。
- Conclusion: DetailFusion通过动态融合全局和细节特征，显著提升了CIR任务的性能。


### [80] [Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition](https://arxiv.org/abs/2505.17807)
*Ping Li,Jianan Ni,Bo Pang*

Main category: cs.CV

TL;DR: 提出了一种基于背景混合和时间一致性的对抗攻击方法（BMTC），用于提升动作识别模型对抗样本的迁移性。

- Motivation: 现有对抗攻击方法依赖源模型和目标模型决策边界相似的假设，且攻击方向不确定导致梯度振荡，限制了迁移性和攻击效果。
- Method: 设计了模型无关的背景对抗混合模块，通过强化学习选择攻击能力强的背景帧进行混合，并利用背景类别指导梯度更新，结合时间梯度一致性损失确保攻击方向稳定。
- Result: 在UCF101、Kinetics-400和ImageNet数据集上验证了方法的有效性，显著提升了对抗样本的迁移性。
- Conclusion: BMTC方法通过减少模型依赖性和稳定攻击方向，显著提升了对抗攻击的迁移性和效果。


### [81] [An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma](https://arxiv.org/abs/2505.17808)
*Ramanathan Swaminathan*

Main category: cs.CV

TL;DR: 论文提出了一种结合卷积神经网络与Vision Transformer的混合深度学习模型，用于青光眼检测。

- Motivation: 探索卷积神经网络与Vision Transformer的协同效应，提升青光眼检测的准确性。
- Method: 通过交叉注意力模块将卷积神经网络与Vision Transformer结合，并在ACRIMA和Drishti数据集上进行实验。
- Result: 模型在青光眼检测任务中表现出色。
- Conclusion: 混合模型在青光眼检测中具有潜力，为未来研究提供了新方向。


### [82] [Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations](https://arxiv.org/abs/2505.17812)
*Boxu Chen,Ziwei Zheng,Le Yang,Zeyu Geng,Zhengyu Zhao,Chenhao Lin,Chao Shen*

Main category: cs.CV

TL;DR: VaLSe框架通过视觉感知潜在引导策略，解决大型视觉语言模型中的对象幻觉问题，提升模型鲁棒性。

- Motivation: 大型视觉语言模型在对象幻觉问题上表现不佳，现有方法对视觉决策机制的理解不足。
- Method: 提出VaLSe框架，采用解释后缓解策略，生成视觉贡献图并引导潜在空间，减少幻觉输出。
- Result: 实验证明VaLSe是有效的解释工具，能显著减少对象幻觉，并揭示现有评估指标的局限性。
- Conclusion: VaLSe为对象幻觉问题提供了新解决方案，并呼吁未来开发更细致的评估基准。


### [83] [ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral Object Re-Identification](https://arxiv.org/abs/2505.17821)
*Shihao Li,Chenglong Li,Aihua Zheng,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种基于身份条件文本提示学习（ICPL）的多光谱目标重识别方法，利用CLIP的跨模态对齐能力统一不同光谱的视觉特征。

- Motivation: 多光谱目标重识别在复杂光照和恶劣天气下具有优势，但现有方法缺乏对光谱信息的细粒度语义理解。
- Method: 通过在线提示学习和多光谱身份条件模块，结合低秩适应方法学习光谱特定特征。
- Result: 在5个基准测试中表现优于现有方法。
- Conclusion: ICPL框架有效解决了多光谱目标重识别中的模态差异问题。


### [84] [VLM Models and Automated Grading of Atopic Dermatitis](https://arxiv.org/abs/2505.17835)
*Marc Lalonde,Hamed Ghodrati*

Main category: cs.CV

TL;DR: 该研究探讨了七种视觉语言模型（VLM）在评估特应性皮炎（AD）严重程度方面的能力。

- Motivation: 由于特应性皮炎（AD）的评估对皮肤科医生具有挑战性，且深度学习技术不断发展，尤其是多模态模型和视觉语言模型（VLM）的出现，为医学图像（包括皮肤病学）的可解释性评估提供了新可能。
- Method: 研究通过实验评估了七种VLM在测试图像上评估AD严重程度的能力。
- Result: 未明确提及具体结果，但实验旨在验证VLM的有效性。
- Conclusion: 研究展示了VLM在AD严重程度评估中的潜在应用价值。


### [85] [Locality-Sensitive Hashing for Efficient Hard Negative Sampling in Contrastive Learning](https://arxiv.org/abs/2505.17844)
*Fabian Deuser,Philipp Hausenblas,Hannah Schieber,Daniel Roth,Martin Werner,Norbert Oswald*

Main category: cs.CV

TL;DR: 提出了一种基于GPU友好的局部敏感哈希（LSH）方案，用于高效寻找高质量硬负样本，提升对比学习性能。

- Motivation: 在大规模高维数据集中高效寻找高质量的硬负样本是计算挑战，现有方法计算成本高。
- Method: 使用LSH将实值特征向量量化为二进制表示，进行近似最近邻搜索。
- Result: 在多个文本和视觉数据集上表现优于或与现有硬负样本挖掘策略相当，计算量显著减少。
- Conclusion: 提出的LSH方案高效且性能优越，适用于对比学习中的硬负样本挖掘。


### [86] [Multi-task Learning For Joint Action and Gesture Recognition](https://arxiv.org/abs/2505.17867)
*Konstantinos Spathis,Nikolaos Kardaris,Petros Maragos*

Main category: cs.CV

TL;DR: 多任务学习用于动作和手势识别，通过共享表示提高效率和泛化能力。

- Motivation: 动作和手势识别是相关任务，但现有方法分开处理，未能利用其协同效应。
- Method: 采用多任务学习范式，联合训练单一深度神经网络以学习共享表示。
- Result: 在多个数据集上的实验表明，多任务学习优于单任务学习。
- Conclusion: 多任务学习能更高效、鲁棒且泛化地处理动作和手势识别任务。


### [87] [Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization](https://arxiv.org/abs/2505.17881)
*Wenjin Qin,Hailin Wang,Hao Shu,Feng Zhang,Jianjun Wang,Xiangyong Cao,Xi-Le Zhao,Gemine Vivone*

Main category: cs.CV

TL;DR: 提出了一种名为HAD-EUNTRFR的新型高光谱异常检测方法，通过增强的非凸张量环分解和正则化，显著提升了检测性能。

- Motivation: 现有方法未能充分利用高光谱图像背景组分的全局相关性和局部平滑性，导致检测性能不佳。
- Method: 采用张量环分解捕获背景组分的空间-光谱相关性，并引入基于张量奇异值分解的非凸正则化器，同时设计广义非凸正则化项以利用异常组分的群稀疏性。
- Result: 在多个基准数据集上的实验表明，该方法在检测精度上优于现有最先进方法。
- Conclusion: HAD-EUNTRFR通过创新的张量分解和正则化策略，显著提升了高光谱异常检测的性能。


### [88] [Track Anything Annotate: Video annotation and dataset generation of computer vision models](https://arxiv.org/abs/2505.17884)
*Nikita Ivanov,Mark Klimov,Dmitry Glukhikh,Tatiana Chernysheva,Igor Glukhikh*

Main category: cs.CV

TL;DR: 论文提出了一种基于视频跟踪和分割的标注工具原型，显著加速了训练数据集的生成过程。

- Motivation: 现代机器学习方法需要大量标注数据，但标注过程耗时且资源密集，因此需要更高效的解决方案。
- Method: 研究了从技术选择到最终实现的不同方法，开发了一个标注和生成训练数据集的工具原型。
- Result: 开发的工具原型比手动标注显著加快了数据集生成速度。
- Conclusion: 该工具原型为机器学习数据准备提供了高效解决方案，相关资源已开源。


### [89] [Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data](https://arxiv.org/abs/2505.17893)
*Shruti Atul Mali,Zohaib Salahuddin,Danial Khan,Yumeng Zhang,Henry C. Woodruff,Eduardo Ibor-Crespo,Ana Jimenez-Pastor,Luis Marti-Bonmati,Philippe Lambin*

Main category: cs.CV

TL;DR: 研究评估了多中心非小细胞肺癌（NSCLC）数据中图像特征整合和协调对生存预测的影响，结合手工放射组学、预训练基础模型特征和临床数据，提升了预测性能。

- Motivation: 探讨多中心NSCLC数据中图像特征整合和协调对生存预测的改进效果，以提供更准确的预后评估。
- Method: 分析了876名NSCLC患者的CT扫描和临床数据，提取多区域特征并采用ComBat和RKN方法协调。使用正则化Cox模型预测生存，并通过C-index、t-AUC和HR评估性能。
- Result: TNM分期显示预后价值（C-index=0.67）。临床+肿瘤放射组学模型性能最佳（C-index=0.7616）。共识模型覆盖78%测试病例，t-AUC达0.92。
- Conclusion: 特征协调和多区域整合显著提升生存预测性能，结合放射组学和共识模型可提供稳健的风险分层。


### [90] [Semantic segmentation with reward](https://arxiv.org/abs/2505.17905)
*Xie Ting,Ye Huang,Zhilin Liu,Lixin Duan*

Main category: cs.CV

TL;DR: RSS（Reward in Semantic Segmentation）首次将基于奖励的强化学习应用于纯语义分割任务，支持像素级和图像级两种奖励粒度，通过PSR和PSD等技术确保网络收敛，并在图像级奖励下优于现有弱监督方法。

- Motivation: 解决真实场景中像素级标签不足的问题，探索利用非传统标签（如解析结果质量反馈）训练语义分割网络。
- Method: 提出RSS框架，结合渐进尺度奖励（PSR）和成对空间差异（PSD）技术，支持像素级和图像级奖励。
- Result: 实验证明RSS能确保网络在两种奖励粒度下收敛，且图像级奖励性能优于现有弱监督方法。
- Conclusion: RSS为语义分割任务提供了一种灵活且高效的训练方法，尤其在标签稀缺场景下表现优异。


### [91] [DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning](https://arxiv.org/abs/2505.17910)
*Bin Wu,Wei Wang,Yahui Liu,Zixiang Li,Yao Zhao*

Main category: cs.CV

TL;DR: DiffusionReward框架首次将Reward Feedback Learning (ReFL) 引入盲脸修复任务，通过Face Reward Model (FRM) 提供反馈信号，结合梯度流优化修复网络，显著提升身份一致性和面部细节。

- Motivation: 传统扩散方法在盲脸修复中难以生成真实面部细节且身份一致性差，因此提出DiffusionReward框架以解决这些问题。
- Method: 框架核心是FRM，通过标注数据训练提供反馈信号，结合梯度流优化修复网络，梯度由FRM、正则化项和结构一致性约束共同决定。
- Result: 实验表明，DiffusionReward在合成和真实数据集上优于现有方法，显著提升身份一致性和面部细节。
- Conclusion: DiffusionReward框架通过ReFL和动态优化的FRM，有效解决了盲脸修复中的关键问题，性能优越且开源。


### [92] [Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention](https://arxiv.org/abs/2505.17911)
*Zheyang Huang,Jagannath Aryal,Saeid Nahavandi,Xuequan Lu,Chee Peng Lim,Lei Wei,Hailing Zhou*

Main category: cs.CV

TL;DR: OCGNet提出了一种对象级跨视角地理定位网络，通过高斯核传递和双嵌入机制实现高精度定位，并在CVOGL数据集上取得最优性能。

- Motivation: 传统方法仅关注图像级定位，而实际应用（如搜救、基础设施检查）需要对象级精度。OCGNet旨在解决视角、时间和成像条件变化带来的挑战。
- Method: OCGNet结合高斯核传递（GKT）保留位置信息，并通过位置增强（LE）和多头交叉注意力（MHCA）模块优化特征提取与匹配。
- Result: OCGNet在CVOGL数据集上达到最优性能，并展示少量样本学习能力。
- Conclusion: OCGNet为对象级跨视角地理定位提供了高效解决方案，适用于多样化应用场景。


### [93] [Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy](https://arxiv.org/abs/2505.17921)
*Carlos Salazar-Ruiz,Francisco Lopez-Tiro,Ivan Reyes-Amezcua,Clement Larose,Gilberto Ochoa-Ruiz,Christian Daul*

Main category: cs.CV

TL;DR: 论文提出了一种基于少样本学习的深度学习方法，用于在训练数据有限的情况下对肾结石类型进行内窥镜图像分类。

- Motivation: 当前肾结石类型识别方法耗时或依赖专家，而传统深度学习模型因数据不足表现不佳。
- Method: 采用少样本学习中的原型网络，在训练数据稀缺或存在罕见类别时生成判别性特征。
- Result: 原型网络仅需25%的训练数据即可达到或超过传统深度学习模型的性能。
- Conclusion: 该方法为肾结石分类提供了一种高效且数据需求低的解决方案。


### [94] [AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models](https://arxiv.org/abs/2505.17931)
*Xingjian Li,Qifeng Wu,Colleen Que,Yiran Ding,Adithya S. Ubaradka,Jianhua Xing,Tianyang Wang,Min Xu*

Main category: cs.CV

TL;DR: 提出一种零样本自动医学图像分割方法，结合视觉语言和分割基础模型，无需大量标注或手动提示。

- Motivation: 解决当前深度学习方法需要大量专家标注或手动提示的问题，提供高效、可扩展的零样本分割方案。
- Method: 结合视觉语言模型生成初始边界框，通过视觉提示增强模块优化提示，再由可提示分割模型生成最终掩码。引入测试时适应框架以解决领域差距和结果验证问题。
- Result: 在七个医学影像数据集上表现良好，性能接近弱提示交互式基础模型。
- Conclusion: 该方法为医学图像分割提供了一种高效、自动化的解决方案，适用于多样化任务。


### [95] [SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes](https://arxiv.org/abs/2505.17951)
*Haihong Xiao,Jianan Zou,Yuxin Zhou,Ying He,Wenxiong Kang*

Main category: cs.CV

TL;DR: SplatCo是一种用于高保真渲染复杂户外环境的结构-视图协作高斯溅射框架，通过全局与局部特征融合及多视图一致性优化，显著提升重建质量。

- Motivation: 解决复杂户外环境的高保真渲染问题，结合全局场景布局与局部细节，同时优化多视图一致性。
- Method: 提出跨结构协作模块（全局三平面与局部网格特征融合）和跨视图辅助训练策略（梯度同步、可见性感知优化）。
- Result: 在13个大规模场景中，PSNR提升1-2 dB，SSIM提升0.1-0.2，重建质量优于现有方法。
- Conclusion: SplatCo为大规模无边界场景的高保真渲染设定了新基准。


### [96] [Diffusion Classifiers Understand Compositionality, but Conditions Apply](https://arxiv.org/abs/2505.17955)
*Yujin Jeong,Arnas Uselis,Seong Joon Oh,Anna Rohrbach*

Main category: cs.CV

TL;DR: 本文研究了扩散分类器在组合任务中的判别能力，通过广泛实验揭示了其性能与数据集领域、时间步权重等因素的关系。

- Motivation: 尽管生成式文本到图像扩散模型在合成复杂场景方面表现出色，但其判别性组合能力尚未被充分研究。本文旨在填补这一空白。
- Method: 研究覆盖了三种扩散模型（SD 1.5、2.0和3-m），涉及10个数据集和30多个任务，并引入新的诊断基准Self-Bench。
- Result: 扩散分类器在组合任务中表现良好，但性能受数据集领域和时间步权重的影响，尤其是SD3-m对时间步敏感。
- Conclusion: 扩散分类器具有组合理解能力，但其表现依赖于特定条件。


### [97] [Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development](https://arxiv.org/abs/2505.17959)
*Nguyen Duc,Yan-Ling Lai,Patrick Madlindl,Xinyuan Zhu,Benedikt Schwab,Olaf Wysocki,Ludwig Hoegner,Thomas H. Kolbe*

Main category: cs.CV

TL;DR: 提出了一种新方法DoGSS-PCL，用于测量真实世界传感器观测与模拟数据之间的领域差距，支持全面的领域差距分析。

- Motivation: 解决长尾数据分布问题，确保安全关键应用（如自动驾驶）中模拟数据的可信度。
- Method: 引入新度量DoGSS-PCL，评估模拟点云的几何和语义质量。
- Result: 实验证实该方法可有效测量领域差距，且合成语义点云可用于训练深度神经网络。
- Conclusion: 该方法将推动可信数据模拟研究，并支持自动驾驶测试和数字孪生的大规模应用。


### [98] [MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings](https://arxiv.org/abs/2505.17972)
*Kazi Mahmudul Hassan,Xuyang Zhao,Hidenori Sugano,Toshihisa Tanaka*

Main category: cs.CV

TL;DR: 提出了一种名为MR-EEGWaveNet的新型端到端模型，用于区分癫痫事件与背景EEG及伪迹/噪声，通过多分辨率特征提取显著提升了性能。

- Motivation: 特征工程在广义癫痫检测模型中仍具挑战性，现有模型性能不稳定且难以区分伪迹与癫痫数据。
- Method: 模型包含卷积、特征提取和预测三个模块，采用深度和时空卷积提取特征，并通过异常分数后处理降低假阳性率。
- Result: 在Siena和Juntendo数据集上，F1分数分别从0.177提升至0.336和0.327提升至0.488，精度分别提高15.9%和20.62%。
- Conclusion: MR-EEGWaveNet在多分辨率特征提取和分类性能上显著优于传统方法，为癫痫检测提供了有效解决方案。


### [99] [To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models](https://arxiv.org/abs/2505.17973)
*Simone Gaisbauer,Prabin Gyawali,Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: 论文比较了传统与可学习特征匹配方法在语义3D建筑相机-模型匹配任务中的表现，发现可学习方法在准确性和鲁棒性上显著优于传统方法。

- Motivation: 尽管深度学习在特征匹配领域取得了进展，但针对语义3D建筑相机-模型匹配任务的全面比较仍缺乏。
- Method: 使用标准数据集（HPatches、MegaDepth-1500）和自定义数据集（立面纹理与相机图像），通过PnP算法评估绝对姿态估计的准确性。
- Result: 可学习方法在自定义数据集上表现显著优于传统方法，准确性和鲁棒性更高。
- Conclusion: 该研究将促进基于模型的视觉定位方法的发展。


### [100] [Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling](https://arxiv.org/abs/2505.17982)
*Bryan Wong,Jong Woo Kim,Huazhu Fu,Mun Yong Yi*

Main category: cs.CV

TL;DR: HiVE-MIL提出了一种分层视觉语言框架，通过统一图结构和动态过滤机制改进多尺度交互和模态对齐，显著提升了少样本病理图像分类性能。

- Motivation: 现有方法在多尺度交互和视觉-文本模态对齐方面存在不足，影响了病理图像的分类效果。
- Method: HiVE-MIL构建了统一图结构，包含跨尺度父子链接和同尺度异质边，并采用文本引导的动态过滤机制和分层对比损失。
- Result: 在TCGA数据集上，HiVE-MIL在16-shot设置下比传统和VLM-based MIL方法性能提升高达4.1%。
- Conclusion: HiVE-MIL通过联合建模分层结构和多模态对齐，为有限病理数据的高效学习提供了有效解决方案。


### [101] [Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid Pose Recovery on Limited Datasets](https://arxiv.org/abs/2505.17992)
*Fahd Alhamazani,Yu-Kun Lai,Paul L. Rosin*

Main category: cs.CV

TL;DR: 提出一种将单视角深度图像转换为规范形式的模型，用于非刚性物体的3D重建，仅需少量数据即可实现高效结果。

- Motivation: 传统方法在处理非刚性物体时需大量训练数据，难以覆盖所有变形情况，本研究旨在解决这一问题。
- Method: 通过规范姿态重建模型，将单视角深度图像对齐到规范形式，利用刚体重建技术完成形状重建，并恢复输入姿态。
- Result: 在动物和人类数据集上，模型表现优于现有方法，仅需约300个样本即可取得显著效果。
- Conclusion: 该模型为小数据集下的非刚性物体3D重建提供了高效解决方案。


### [102] [Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation](https://arxiv.org/abs/2505.17994)
*Zhihua Liu,Amrutha Saseendran,Lei Tong,Xilin He,Fariba Yousefi,Nikolay Burlutskiy,Dino Oglic,Tom Diethe,Philip Teare,Huiyu Zhou,Chen Jin*

Main category: cs.CV

TL;DR: 提出了一种无需训练的开放集图像分割方法Segment Anyword，通过冻结扩散模型的跨注意力图生成分割掩码，并结合语言引导的视觉提示正则化提升分割效果。

- Motivation: 现有方法需要大量训练或微调，且在多样化文本参考下难以一致分割对象。
- Method: 利用冻结扩散模型的跨注意力图生成初始掩码，再通过语言引导的视觉提示正则化优化掩码。
- Result: 在多个开放集分割任务中取得最优结果，如Pascal Context 59上52.5 mIoU。
- Conclusion: Segment Anyword是一种高效、通用的开放集分割方法，无需训练且性能优越。


### [103] [Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation Estimation Using Spectral Imaging](https://arxiv.org/abs/2505.18010)
*Jens De Winne,Siri Willems,Siri Luthman,Danilo Babin,Hiep Luong,Wim Ceelen*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的实时组织氧合估计方法，通过蒙特卡洛模拟光谱训练神经网络，并采用域对抗训练缩小模拟与临床数据的差距。

- Motivation: 传统线性分解方法依赖假设，实际效果受限，而深度学习能更准确地估计组织氧合。
- Method: 使用全连接神经网络（FCN）和卷积神经网络（CNN），结合域对抗训练方法，优化模型在临床数据上的表现。
- Result: 深度学习模型与术中毛细血管乳酸测量的相关性优于传统方法，域对抗训练有效减少了域差距。
- Conclusion: 深度学习结合域对抗训练为实时组织氧合监测提供了更可靠的解决方案。


### [104] [SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification](https://arxiv.org/abs/2505.18015)
*Shashank Agnihotri,David Schader,Jonas Jakubassa,Nico Sharei,Simon Kral,Mehmet Ege Kaçar,Ruben Weber,Margret Keuper*

Main category: cs.CV

TL;DR: 论文提出了SEMSEGBENCH和DETECBENCH工具，用于评估语义分割和物体检测模型的鲁棒性和泛化能力，并进行了大规模实验。

- Motivation: 研究深度学习在安全关键领域（如语义分割和物体检测）中的可靠性和泛化能力，填补现有研究主要集中在图像分类的空白。
- Method: 开发了SEMSEGBENCH和DETECBENCH工具，对76个分割模型和61个检测模型进行了6139次评估，测试其在对抗攻击和常见干扰下的表现。
- Result: 发现当前最先进模型存在系统性弱点，并揭示了架构、主干网络和模型容量对性能的影响趋势。
- Conclusion: 开源工具和数据将推动未来研究，提升模型在分类任务之外的可靠性。


### [105] [Building Floor Number Estimation from Crowdsourced Street-Level Images: Munich Dataset and Baseline Method](https://arxiv.org/abs/2505.18021)
*Yao Sun,Sining Chen,Yifan Tian,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出一种端到端深度学习框架，直接从街景图像推断建筑楼层数，无需手工特征，适用于多种建筑风格。

- Motivation: 大规模楼层数据在3D城市数据库中稀缺，但对家庭估算、风险评估等至关重要。
- Method: 使用深度学习分类-回归网络，直接从街景图像预测楼层数。
- Result: 在公开数据集上，准确率达81.2%，97.9%预测误差在±1层内。
- Conclusion: 方法和数据集为3D城市模型提供可扩展的垂直信息补充，推动城市信息学发展。


### [106] [RemoteSAM: Towards Segment Anything for Earth Observation](https://arxiv.org/abs/2505.18022)
*Liang Yao,Fan Liu,Delong Chen,Chuanyi Zhang,Yijun Wang,Ziyun Chen,Wei Xu,Shimin Di,Yuhui Zheng*

Main category: cs.CV

TL;DR: 提出了RemoteSAM，一种用于地球观测的视觉基础模型，通过自动数据引擎和任务统一范式，显著提升了语义覆盖范围和任务适应性。

- Motivation: 当前系统无法满足多样化视觉目标的识别和定位需求，且缺乏兼容性。
- Method: 引入自动数据引擎构建大规模数据集，并提出以参考表达分割为中心的任务统一范式。
- Result: RemoteSAM在多个地球观测基准测试中表现优异，效率显著高于其他模型。
- Conclusion: RemoteSAM通过数据和建模创新，成为地球观测领域的新SoTA模型。


### [107] [A Wavelet-based Stereo Matching Framework for Solving Frequency Convergence Inconsistency](https://arxiv.org/abs/2505.18024)
*Xiaobao Wei,Jiawei Liu,Dongbo Yang,Junda Cheng,Changyong Shu,Wei Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于小波的立体匹配框架（Wavelet-Stereo），通过分离处理高频和低频信息，解决了现有迭代方法在高频区域性能受限的问题。

- Motivation: 现有迭代方法在优化所有频率分量时未区分高频和低频，导致高频区域（如边缘和细物体）性能下降。
- Method: 使用离散小波变换分解图像为高频和低频分量，分别输入多尺度特征提取器，并提出基于LSTM的高频保护更新算子。
- Result: Wavelet-Stereo在KITTI 2015和KITTI 2012排行榜上表现最佳。
- Conclusion: 通过分离处理高频和低频信息，框架显著提升了立体匹配性能，适用于细节丰富的场景。


### [108] [3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation](https://arxiv.org/abs/2505.18025)
*Evangelos Sariyanidi,Claudio Ferrari,Federico Nocentini,Stefano Berretti,Andrea Cavallaro,Birkan Tunc*

Main category: cs.CV

TL;DR: 论文提出了一个模块化的3D人脸重建基准工具包（M3DFB），通过分离和可互换的组件量化误差计算的影响，并引入新的校正组件。实验表明，传统ICP方法性能最差，而非刚性对齐和校正方案显著提升精度和效率。

- Motivation: 当前3D人脸重建的基准工具是单一的，缺乏对误差计算最佳方法的共识，限制了基准测试的灵活性和准确性。
- Method: 提出模块化工具包M3DFB，分离误差计算的核心组件（如裁剪、对齐、点对应），并引入新的校正组件。通过16种误差估计器和10种重建方法在多个数据集上测试。
- Result: 传统ICP方法表现最差（相关性低至0.41），非刚性对齐显著提升性能（相关性>0.90）。校正方案与非刚性变形结合，精度媲美最佳方法且速度更快。
- Conclusion: M3DFB工具包支持灵活组件比较，提升基准测试效率，并为学习型重建方法提供更准确的误差估计。


### [109] [CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention](https://arxiv.org/abs/2505.18035)
*Naseem Khan,Tuan Nguyen,Amine Bermak,Issa Khalil*

Main category: cs.CV

TL;DR: CAMME框架通过多模态跨注意力机制提升跨域深度伪造检测性能，显著优于现有方法。

- Motivation: 解决现有深度伪造检测方法在未见生成架构上性能下降的问题。
- Method: 提出CAMME框架，动态整合视觉、文本和频域特征，通过跨注意力机制实现跨域泛化。
- Result: CAMME在自然场景和面部深度伪造上分别提升12.56%和13.25%，对抗攻击下保持高准确率。
- Conclusion: 多模态跨注意力机制能有效提升深度伪造检测的跨域鲁棒性。


### [110] [Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation](https://arxiv.org/abs/2505.18039)
*Li Zhong,Ahmed Ghazal,Jun-Jun Wan,Frederik Zilly,Patrick Mackens,Joachim E. Vollrath,Bogdan Sorin Coseriu*

Main category: cs.CV

TL;DR: Clip4Retrofit是一个高效的模型蒸馏框架，旨在将CLIP模型的知识迁移到轻量级学生模型中，以在资源受限的边缘设备上实现实时图像标注。

- Motivation: 解决CLIP等基础模型在计算复杂性和内存占用方面的限制，使其无法在资源受限的边缘设备（如车载摄像头）上部署的问题。
- Method: 通过结合EfficientNet-B3和多层感知机（MLP）投影头，将CLIP模型的知识蒸馏到轻量级学生模型中，以保持跨模态对齐并显著降低计算需求。
- Result: 实验结果表明，Clip4Retrofit能够在资源有限的边缘设备上实现实时图像标注和对象识别，平衡了效率与性能。
- Conclusion: 该工作填补了先进视觉语言模型与资源受限环境部署之间的差距，为边缘计算中基础模型的广泛应用铺平了道路。


### [111] [RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration](https://arxiv.org/abs/2505.18047)
*Sudarshan Rajagopalan,Kartik Narayan,Vishal M. Patel*

Main category: cs.CV

TL;DR: RestoreVAR是一种基于视觉自回归模型（VAR）的生成方法，显著提升了图像修复性能，同时推理速度比潜在扩散模型（LDM）快10倍以上。

- Motivation: LDM-based方法在图像修复中表现优异但推理速度慢，不适用于时间敏感场景。
- Method: 采用视觉自回归模型（VAR），结合精心设计的跨注意力机制和潜在空间细化模块。
- Result: RestoreVAR在生成式图像修复方法中达到最优性能，并具备强泛化能力。
- Conclusion: RestoreVAR在性能和效率上均优于LDM-based方法，适用于实际应用。


### [112] [SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios](https://arxiv.org/abs/2505.18048)
*Simon Malzard,Nitish Mital,Richard Walters,Victoria Nockles,Raghuveer Rao,Celso M. De Melo*

Main category: cs.CV

TL;DR: 论文提出了一个针对骨骼人体动作识别（SHAR）的数据退化基准测试，评估了五种领先模型在三种退化形式下的鲁棒性，并发现退化类型对模型准确性有显著影响。通过插值方法提升性能，并发现基于粗糙路径理论的LogSigRNN模型在低帧率下表现优异。

- Motivation: 现实世界中，计算机视觉模型常因实时或资源受限硬件而处理退化数据，但现有模型评估不足。SHAR作为关键任务，其退化鲁棒性评估缺乏一致性。
- Method: 在NTU-RGB+D-120数据集上建立退化基准，评估五种SHAR模型对三种退化形式的鲁棒性，并通过插值方法提升性能。
- Result: 退化类型对模型准确性影响显著（差异>40%），插值方法提升性能>40%。LogSigRNN模型在低帧率下优于SoTA模型6%。
- Conclusion: 研究填补了SHAR退化评估的空白，提出了有效改进方法，并发现LogSigRNN在退化数据中表现优异，为未来研究提供了重要参考。


### [113] [SpikeGen: Generative Framework for Visual Spike Stream Processing](https://arxiv.org/abs/2505.18049)
*Gaole Dai,Menghang Dong,Rongyu Zhang,Ruichuan An,Shanghang Zhang,Tiejun Huang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SpikeGen的生成处理框架，用于处理尖峰相机捕获的视觉尖峰流，解决了稀疏数据问题，并通过生成模型融合尖峰和RGB模态信息。

- Motivation: 尖峰视觉系统（如尖峰相机）在动态条件下能捕捉清晰纹理，但生成的数据空间稀疏，而传统RGB模态提供密集空间信息。生成模型能有效弥补稀疏数据的不足。
- Method: 提出SpikeGen框架，利用生成模型的潜在空间操作能力，融合尖峰和RGB模态信息，支持条件生成和任务如去模糊、帧重建和新视角合成。
- Result: 实验证明，SpikeGen能有效利用尖峰流的时间丰富性，弥补空间信息稀疏性，实现不同视觉模态的协同增强。
- Conclusion: SpikeGen框架通过生成模型解决了尖峰流数据的稀疏性问题，同时充分利用其时间特性，为多模态视觉任务提供了有效解决方案。


### [114] [LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision](https://arxiv.org/abs/2505.18051)
*Anthony Fuller,Yousef Yassin,Junfeng Wen,Daniel G. Kyrollos,Tarek Ibrahim,James R. Green,Evan Shelhamer*

Main category: cs.CV

TL;DR: LookWhere方法通过自适应计算减少Vision Transformers在高分辨率下的计算成本，联合训练选择器和提取器，显著降低FLOPs和时间消耗。

- Motivation: Vision Transformers在高分辨率下计算成本极高，需要一种经济高效的方法来减少计算负担。
- Method: 提出LookWhere方法，结合低分辨率选择器和高分辨率提取器，通过自监督教师蒸馏联合训练。
- Result: 在Traffic Signs任务中减少34x FLOPs和6x时间，ImageNet和ADE20K任务中提升精度并减少1.36x时间。
- Conclusion: LookWhere能高效选择和处理图像表示，适用于稀疏和全局识别任务。


### [115] [BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching](https://arxiv.org/abs/2505.18052)
*Zhihua Liu,Lei Tong,Xilin He,Che Liu,Rossella Arcucci,Chen Jin,Huiyu Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为BOTM的新分割框架，通过双向最优令牌匹配解决超声心动图分割中的解剖不一致性问题。

- Motivation: 现有超声心动图分割方法因形状变化、部分观察和区域模糊等问题导致解剖结构不一致，BOTM旨在提供解剖一致性保证。
- Method: BOTM通过双向最优令牌匹配和跨传输注意力代理，同时进行分割和解剖结构传输。
- Result: 实验表明BOTM在稳定性和准确性上表现优异（如CAMUS2H LV的HD降低1.917，TED的Dice提高1.9%）。
- Conclusion: BOTM能生成稳定且准确的分割结果，并提供解剖一致性保证。


### [116] [FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation](https://arxiv.org/abs/2505.18053)
*Zherui Zhang,Jiaxin Wu,Changwei Wang,Rongtao Xu,Longzhao Huang,Wenhao Xu,Wenbo Xu,Li Guo,Shibiao Xu*

Main category: cs.CV

TL;DR: 本文提出FDBPL方法，通过共享软监督上下文和加速I/O，解决了现有蒸馏提示学习方法效率低的问题，并引入区域感知提示学习范式，提升零样本性能。

- Motivation: 现有提示学习方法在泛化性和效率上存在不足，尤其是蒸馏方法牺牲了训练效率。
- Method: FDBPL通过共享软监督上下文、加速I/O和区域感知提示学习范式（双正负提示空间）提升效率与性能。
- Result: 在11个数据集上表现优异，训练速度提升2.2倍，同时保持参数高效和强泛化能力。
- Conclusion: FDBPL在效率、泛化性和零样本性能上均优于现有方法。


### [117] [Semantic Correspondence: Unified Benchmarking and a Strong Baseline](https://arxiv.org/abs/2505.18060)
*Kaiyan Zhang,Xinghui Li,Jingyi Lu,Kai Han*

Main category: cs.CV

TL;DR: 本文首次全面综述了语义匹配方法，提出了分类法，总结了现有方法，并通过实验分析了不同组件的有效性，同时提出了一个简单但高效的基线方法。

- Motivation: 语义匹配是计算机视觉中的挑战性任务，但缺乏全面综述和分析。
- Method: 提出分类法对现有方法分类，详细分析每种方法，并通过实验验证组件有效性。
- Result: 汇总了多个基准测试的结果，提出了一个性能优越的基线方法。
- Conclusion: 本文为语义匹配领域提供了全面的参考和基准，代码已公开。


### [118] [DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation](https://arxiv.org/abs/2505.18078)
*Junhao Chen,Mingjin Chen,Jianjin Xu,Xiang Li,Junting Dong,Mingze Sun,Puhua Jiang,Hongxiang Li,Yuhang Yang,Hao Zhao,Xiaoxiao Long,Ruqi Huang*

Main category: cs.CV

TL;DR: DanceTogether是一个端到端的扩散框架，通过结合参考图像和独立姿势流生成多角色交互视频，解决了现有可控视频生成系统在噪声控制信号下的身份漂移问题。

- Motivation: 当前可控视频生成系统在多角色交互时表现不佳，尤其是在噪声控制信号下。DanceTogether旨在解决这一问题，实现身份严格保留的长视频生成。
- Method: 提出MaskPoseAdapter，通过融合跟踪掩码和姿势热图，在每一步去噪过程中绑定身份和动作。训练使用了新数据集PairFS-4K和HumanRob-300，并提出了TogetherVideoBench基准。
- Result: 在TogetherVideoBench上，DanceTogether显著优于现有方法，并能通过微调快速适应人机交互任务。
- Conclusion: DanceTogether及其相关数据集和基准将可控视频生成从单角色扩展到多角色交互，为数字制作和仿真提供了新途径。


### [119] [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
*Xiaoyi Zhang,Zhaoyang Jia,Zongyu Guo,Jiahao Li,Bin Li,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Deep Video Discovery（DVD）的智能代理，用于解决长视频理解中的复杂时空问题和问答挑战。通过自主搜索策略和工具集，结合LLM的推理能力，显著提升了性能。

- Motivation: 长视频理解因时空复杂性和上下文长而困难，现有LLM在处理信息密集的长视频时仍有局限。
- Method: 提出DVD代理，利用自主搜索策略和多粒度视频数据库工具集，结合LLM推理能力进行迭代优化。
- Result: 在多个长视频理解基准测试中表现优异，尤其在LVBench数据集上大幅超越先前工作。
- Conclusion: DVD代理通过自主性和工具集设计，显著提升了长视频理解能力，为未来智能代理研究提供了方向。


### [120] [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays](https://arxiv.org/abs/2505.18087)
*Hyungyung Lee,Geon Choi,Jung-Oh Lee,Hangyul Yoon,Hyuk Gi Hong,Edward Choi*

Main category: cs.CV

TL;DR: 提出了CheXStruct和CXReasonBench，用于评估大型视觉语言模型在医学任务中的临床推理能力。

- Motivation: 现有基准主要关注最终诊断结果，缺乏对模型是否进行临床有意义推理的评估。
- Method: 基于MIMIC-CXR-JPG数据集，构建结构化流程和基准，自动生成中间推理步骤。
- Result: 评估了10个模型，发现它们在结构化推理和泛化能力上表现不佳。
- Conclusion: 新基准支持细粒度评估，揭示了模型在临床推理中的不足。


### [121] [DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations](https://arxiv.org/abs/2505.18096)
*Ziqiao Peng,Yanbo Fan,Haoyu Wu,Xuan Wang,Hongyan Liu,Jun He,Zhaoxin Fan*

Main category: cs.CV

TL;DR: 论文提出了一种新任务——多轮双说话者交互的3D说话头生成，并提出了DualTalk框架，以模拟真实对话中的角色切换和非语言反馈。

- Motivation: 现有3D说话头生成模型仅关注说话或倾听，忽略了对话中的自然动态切换，导致交互不自然。
- Method: 提出DualTalk框架，整合说话者和倾听者的动态行为，生成连续对话中的角色切换和非语言反馈。
- Result: 实验表明，该方法显著提升了双说话者对话中3D说话头的自然性和表现力。
- Conclusion: DualTalk框架有效解决了对话中角色切换的问题，提升了交互的真实感。


### [122] [F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles](https://arxiv.org/abs/2505.18106)
*Varun Ajith,Anindya Pal,Saumik Bhattacharya,Sayantari Ghosh*

Main category: cs.CV

TL;DR: F-ANcGAN是一种基于注意力机制的生成对抗网络，用于从少量数据生成高质量的纳米材料SEM图像。

- Motivation: 纳米材料研究中缺乏高质量标注数据集，阻碍了纳米尺度成像的强分割模型开发。
- Method: 采用Style U-Net生成器和带自注意力的U-Net分割网络，结合数据增强方法。
- Result: 在TiO2数据集上达到FID 17.65，经后处理降至10.39。
- Conclusion: 该方法可解决数据短缺问题，提升下游分割任务效果，适用于资源有限领域。


### [123] [Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking](https://arxiv.org/abs/2505.18111)
*Cheng-Yen Yang,Hsiang-Wei Huang,Pyong-Kun Kim,Chien-Kai Kuo,Jui-Wei Chang,Kwang-Ju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种将Segment Anything Model 2（SAM2）适配到视觉目标跟踪（VOT）任务的有效方法，通过优化SAM2并在2024 ICPR多模态目标跟踪挑战中取得89.4的AUC分数。

- Motivation: 利用SAM2的预训练能力，提升其在VOT任务中的性能。
- Method: 结合SAM2与提出的优化技术，包括特定增强和性能改进。
- Result: 在2024 ICPR挑战中取得第一名的AUC分数89.4。
- Conclusion: 该方法有效提升了SAM2在VOT任务中的表现，尤其在多模态数据集上表现优异。


### [124] [Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion](https://arxiv.org/abs/2505.18115)
*Jacob Hansen,Wei Lin,Junmo Kang,Muhammad Jehanzeb Mirza,Hongyin Luo,Rogerio Feris,Alan Ritter,James Glass,Leonid Karlinsky*

Main category: cs.CV

TL;DR: 本文提出了一种开放统一的方法（\method），用于将图像元数据转换为视觉指令调整（VisIT）数据，利用开源LLM降低成本并提升质量。

- Motivation: 现有VisIT数据集构建方法不透明、成本高且难以扩展，需要一种开放、可复现的解决方案。
- Method: 提出多阶段框架，包括元数据分组、质量控制、数据组织及对话采样，使用开源LLM（如Gemma 2 27B和LLaMa 3.1 70B）。
- Result: 方法在相同数据源下提升VisIT数据质量，平均提升3%，最高12%，且支持性能扩展。
- Conclusion: \method为VisIT数据生成提供高效、开放的解决方案，支持未来领域扩展。


### [125] [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/abs/2505.18129)
*Yan Ma,Linge Du,Xuyang Shen,Shaoxiang Chen,Pengfei Li,Qibing Ren,Lizhuang Ma,Yuchao Dai,Pengfei Liu,Junjie Yan*

Main category: cs.CV

TL;DR: V-Triune是一个视觉三重统一强化学习系统，通过联合学习视觉推理和感知任务，显著提升了视觉语言模型的能力。

- Motivation: 探索强化学习在视觉语言模型中的更广泛应用，特别是在感知密集型任务（如物体检测和定位）中。
- Method: V-Triune系统包含三个互补组件：样本级数据格式化、验证级奖励计算和源级指标监控，并引入动态IoU奖励。
- Result: Orsta模型在推理和感知任务上表现一致提升，在MEGA-Bench Core上实现了+2.1到+14.1的改进。
- Conclusion: V-Triune系统展示了统一强化学习方法的有效性和可扩展性，为视觉语言模型提供了广泛的能力提升。


### [126] [BiggerGait: Unlocking Gait Recognition with Layer-wise Representations from Large Vision Models](https://arxiv.org/abs/2505.18132)
*Dingqing Ye,Chao Fan,Zhanbo Huang,Chengwen Luo,Jianqiang Li,Shiqi Yu,Xiaoming Liu*

Main category: cs.CV

TL;DR: 论文提出BiggerGait方法，通过整合大型视觉模型（LVM）多层次的表征，显著提升了步态识别性能，无需依赖复杂的步态先验知识。

- Motivation: 现有LVM方法过度依赖步态先验，忽视了LVM多层次表征的潜力。
- Method: 分析LVM各层表征对任务的影响，提出整合多层次表征的BiggerGait方法。
- Result: 在多个数据集上验证了BiggerGait的优越性，尤其在跨域任务中表现突出。
- Conclusion: BiggerGait是一种简单实用的步态表征学习基线方法，模型和代码将开源。


### [127] [Boosting Open Set Recognition Performance through Modulated Representation Learning](https://arxiv.org/abs/2505.18137)
*Amit Kumar Kundu,Vaishnavi Patil,Joseph Jaja*

Main category: cs.CV

TL;DR: 本文提出了一种基于负余弦调度的温度调制表示学习方法，用于开放集识别（OSR），通过动态调整温度参数，提升模型的表示能力和泛化性。

- Motivation: 现有OSR方法使用固定温度参数限制了表示学习的多样性，无法同时探索实例级和语义级特征。
- Method: 采用负余弦调度方案动态调整温度参数，从粗粒度决策边界逐渐过渡到平滑边界。
- Result: 该方法在多种基线模型和损失函数上均提升了OSR和闭集性能，尤其在语义偏移基准测试中表现突出。
- Conclusion: 动态温度调制是一种高效且无需额外计算开销的OSR改进方案。


### [128] [TokBench: Evaluating Your Visual Tokenizer before Visual Generation](https://arxiv.org/abs/2505.18142)
*Junfeng Wu,Dongliang Luo,Weizhi Zhao,Zhihao Xie,Yuanhao Wang,Junyi Li,Xudong Xie,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 本文揭示了视觉分词器和VAE在保留细粒度特征上的局限性，并提出了一种评估重建性能的基准，重点关注文本和人脸。通过轻量级方法评估，发现现代视觉分词器在小尺度上仍难以保留细节，且传统指标不适用于人脸和文本重建。

- Motivation: 视觉分词器和VAE在图像压缩中不可避免地丢失信息，限制了视觉生成质量的上限。本文旨在评估这些压缩损失对文本和人脸（人类最敏感的元素）的影响。
- Method: 收集并整理文本和人脸图像数据集，使用OCR模型评估文本重建的识别准确率，并通过特征相似性量化人脸重建保真度。方法轻量，仅需2GB内存和4分钟完成评估。
- Result: 现代视觉分词器在小尺度上难以保留细粒度特征，传统指标无法准确反映人脸和文本的重建性能，而提出的指标可作为有效补充。
- Conclusion: 本文提出的基准和指标为视觉分词器和VAE的性能评估提供了新视角，尤其在文本和人脸重建方面具有重要参考价值。


### [129] [REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders](https://arxiv.org/abs/2505.18153)
*Savya Khosla,Sethuraman TV,Barnett Lee,Alexander Schwing,Derek Hoiem*

Main category: cs.CV

TL;DR: REN是一种快速高效的模型，通过点提示生成基于区域的图像表示，避免了传统方法的高计算成本，速度提升60倍，内存减少35倍，同时提升表示质量。

- Motivation: 现有方法结合分割器和图像编码器生成区域表示，但计算成本高。REN旨在绕过这一瓶颈，直接生成区域标记。
- Method: REN使用轻量级模块，通过跨注意力块将点提示作为查询，图像编码器特征作为键和值，生成区域标记。
- Result: REN在语义分割和检索任务中表现优异，速度和内存效率显著提升，在多个基准测试中达到或超过现有方法。
- Conclusion: REN是一种高效且通用的区域表示生成方法，适用于多种编码器，并在多个任务中表现出色。
## cs.HC

### [130] [CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models](https://arxiv.org/abs/2505.17202)
*Arnav Verma,Kushin Mukherjee,Christopher Potts,Elisa Kreiss,Judith E. Fan*

Main category: cs.HC

TL;DR: 论文评估了八种视觉语言模型在六项数据可视化理解任务上的表现，发现其表现普遍低于人类，且错误模式与人类不同。

- Motivation: 研究动机是探索视觉语言模型是否能模拟人类对数据可视化的理解能力。
- Method: 方法是通过比较模型和人类在六项数据可视化测试中的表现，评估模型的性能。
- Result: 结果显示模型表现普遍不如人类，且错误模式与人类显著不同。
- Conclusion: 结论是当前模型仍需进一步改进，以更好地模拟人类对数据可视化的推理能力。


### [131] [ProTAL: A Drag-and-Link Video Programming Framework for Temporal Action Localization](https://arxiv.org/abs/2505.17555)
*Yuchen He,Jianbing Lv,Liqi Cheng,Lingyu Meng,Dazhen Deng,Yingcai Wu*

Main category: cs.HC

TL;DR: ProTAL是一个拖拽链接的视频编程框架，用于简化TAL模型的训练标注过程，通过定义关键事件生成动作标签。

- Motivation: TAL模型训练需要大量人工标注数据，数据编程方法在TAL中难以定义复杂动作。
- Method: 提出ProTAL框架，用户通过拖拽节点定义关键事件并生成标签，结合半监督方法训练模型。
- Result: 通过使用场景和用户研究验证了ProTAL的有效性。
- Conclusion: ProTAL为视频编程框架设计提供了新思路。
## cs.LG

### [132] [OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning](https://arxiv.org/abs/2505.17163)
*Mingxin Huang,Yongxin Shi,Dezhi Peng,Songxuan Lai,Zecheng Xie,Lianwen Jin*

Main category: cs.LG

TL;DR: OCR-Reasoning是一个系统性评估多模态大语言模型在文本丰富图像推理任务中的基准，包含1,069个人工标注示例，覆盖6种核心推理能力和18种任务。

- Motivation: 现有研究对多模态慢思考系统在文本丰富图像推理任务中的能力缺乏系统性评估，因此提出OCR-Reasoning填补这一空白。
- Method: 构建OCR-Reasoning基准，标注推理过程和最终答案，并评估现有MLLMs的表现。
- Result: 现有MLLMs在OCR-Reasoning上表现不佳，最高准确率未超过50%，显示文本丰富图像推理任务的挑战性。
- Conclusion: OCR-Reasoning揭示了现有方法的局限性，强调了文本丰富图像推理任务的紧迫性，并提供了基准和评估脚本。


### [133] [Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction](https://arxiv.org/abs/2505.17357)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.LG

TL;DR: 论文提出了一种框架，通过降维技术处理高维IoT攻击数据后，再将其转化为图结构数据，以提升基于GAT模型的僵尸网络攻击检测性能。

- Motivation: 现有方法在处理IoT攻击数据时，忽略了实例间关系，且高维数据转化为图结构时存在计算开销大的问题。
- Method: 使用VAE-encoder、AE-encoder和PCA三种降维技术处理数据，再通过GAT模型进行检测。
- Result: 降维技术有效减少了计算开销，同时GAT模型结合降维数据提升了检测性能。
- Conclusion: 提出的框架在降低计算复杂度的同时，提高了僵尸网络攻击检测的准确性。


### [134] [Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling](https://arxiv.org/abs/2505.17384)
*Tianyu Xie,Shuchen Xue,Zijin Feng,Tianyang Hu,Jiacheng Sun,Zhenguo Li,Cheng Zhang*

Main category: cs.LG

TL;DR: VADD是一种新型离散扩散模型框架，通过隐变量建模增强离散扩散，显著提升样本质量，尤其在去噪步骤较少时表现优于传统MDM。

- Motivation: 传统MDM在去噪步骤较少时因维度间依赖关系建模不足导致性能下降，VADD通过隐变量建模解决这一问题。
- Method: VADD引入辅助识别模型，通过变分下界最大化实现稳定训练，并在训练集上进行摊销推断。
- Result: 在2D玩具数据、像素级图像生成和文本生成任务中，VADD均优于MDM基线。
- Conclusion: VADD在保持MDM效率的同时显著提升样本质量，尤其在去噪步骤较少时表现突出。


### [135] [Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning](https://arxiv.org/abs/2505.17448)
*Bhanuka Gamage,Adnan Labib,Aisha Joomun,Chern Hong Lim,KokSheik Wong*

Main category: cs.LG

TL;DR: 提出了一种名为BaitRadar的算法，通过深度学习技术结合六个推理模型来检测YouTube上的点击诱饵内容，准确率达98%。

- Motivation: YouTube上点击诱饵问题日益严重，用户常因标题和缩略图吸引而观看与宣传不符的内容。
- Method: BaitRadar算法结合六个推理模型，分别分析视频的标题、评论、缩略图、标签、统计数据和音频转录，通过平均计算得出最终分类。
- Result: 在1,400个YouTube视频上测试，平均准确率为98%，推理时间少于2秒。
- Conclusion: BaitRadar能有效检测点击诱饵，即使在数据缺失的情况下也能提供稳健的分类。


### [136] [Wildfire spread forecasting with Deep Learning](https://arxiv.org/abs/2505.17556)
*Nikolaos Anastasiou,Spyros Kondylatos,Ioannis Papoutsis*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的框架，利用点火时的数据预测野火最终燃烧范围，多日观测数据显著提升预测准确性。

- Motivation: 准确预测野火蔓延对风险管理和应急响应至关重要。
- Method: 利用2006-2022年地中海地区的时空数据集，结合多种数据源，通过消融研究评估时间上下文的影响。
- Result: 最佳模型（点火前后4-5天数据）比基线模型F1分数和IoU提升近5%。
- Conclusion: 公开数据集和模型以促进数据驱动的野火建模研究。


### [137] [MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity](https://arxiv.org/abs/2505.17591)
*Judith Vilella-Cantos,Juan José Cabrera,Luis Payá,Mónica Ballesta,David Valiente*

Main category: cs.LG

TL;DR: MinkUNeXt-SI是一种基于LiDAR点云数据的鲁棒地点识别方法，通过结合Minkowski卷积和U-net架构，实现了高精度和泛化能力。

- Motivation: 在自主导航系统中，地点识别问题需要解决场景变化（如季节和天气）带来的挑战，同时需具备跨环境的泛化能力。
- Method: 方法包括预处理LiDAR点云数据（归一化球坐标和强度值），并采用结合Minkowski卷积和U-net架构的深度学习模型。
- Result: MinkUNeXt-SI在性能上超越现有技术，并在其他数据集上表现出良好的泛化能力。
- Conclusion: 该方法不仅高效，还公开了代码和数据集，便于复现和应用。


### [138] [SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data](https://arxiv.org/abs/2505.17695)
*Dong-Hee Kim,Hyunjee Song,Donghyun Kim*

Main category: cs.LG

TL;DR: WildRES是一个新的基准测试，用于评估复杂推理能力的RES模型，而SynRES是一个自动化管道，用于生成合成训练数据，显著提升模型性能。

- Motivation: 现有RES基准测试的局限性（如短查询或单一领域）无法充分评估复杂推理能力，因此需要更全面的评估工具。
- Method: 引入WildRES基准测试，包含长查询和多目标非显著查询；提出SynRES，通过密集标注驱动合成、语义对齐机制和领域感知增强生成训练数据。
- Result: 当前RES模型在WildRES上表现显著下降；SynRES训练模型在WildRES-ID和WildRES-DS上分别提升gIoU 2.0%和3.8%。
- Conclusion: WildRES和SynRES为RES模型的复杂推理能力评估和性能提升提供了有效解决方案。


### [139] [Soft-CAM: Making black box models self-explainable for high-stakes decisions](https://arxiv.org/abs/2505.17748)
*Kerol Djoumessi,Philipp Berens*

Main category: cs.LG

TL;DR: SoftCAM是一种使CNN架构具有内在可解释性的方法，通过移除全局平均池化层并替换全连接分类层，生成显式的类别激活图。

- Motivation: 现有的事后解释方法不可靠且难以反映模型真实推理过程，限制了在高风险应用中的可信度。
- Method: 移除全局平均池化层，用基于卷积的类别证据层替换全连接分类层，保留空间信息并生成类别激活图。
- Result: 在三个医学数据集上，SoftCAM保持分类性能，同时显著提升解释的质和量。
- Conclusion: CNN可以在不牺牲性能的情况下实现内在可解释性，推动高风险决策的自解释深度学习发展。


### [140] [A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances](https://arxiv.org/abs/2505.17799)
*Brian B. Moser,Arundhati S. Shanbhag,Stanislav Frolov,Federico Raue,Joachim Folz,Andreas Dengel*

Main category: cs.LG

TL;DR: 这篇论文综述了核心集选择的研究，统一了训练无关、训练导向和无标签方法，并探讨了修剪策略对泛化和神经扩展定律的影响。

- Motivation: 解决在大数据集中选择小型代表性子集的挑战，填补现有综述的不足，提供更全面的视角。
- Method: 通过统一三种主要核心集研究方法（训练无关、训练导向、无标签）构建分类法，并分析修剪策略和伪标签技术。
- Result: 揭示了修剪策略对泛化和神经扩展定律的影响，比较了不同方法在计算、鲁棒性和性能需求下的表现。
- Conclusion: 提出了未来研究的开放性问题，如鲁棒性、异常值过滤和适应基础模型的核心集选择。


### [141] [FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks](https://arxiv.org/abs/2505.17883)
*Laines Schmalwasser,Niklas Penzel,Joachim Denzler,Julia Niebling*

Main category: cs.LG

TL;DR: FastCAV是一种加速概念激活向量（CAV）提取的新方法，比现有方法快46.4倍（最高63.6倍），同时保持性能。

- Motivation: 现有CAV计算方法在大规模、高维架构中计算成本高，限制了其应用。
- Method: 提出FastCAV方法，理论证明其与SVM方法等效，并通过实验验证其高效性和稳定性。
- Result: FastCAV提取的CAV性能与现有方法相当，但更高效稳定，适用于下游概念解释任务。
- Conclusion: FastCAV为深度模型的概念研究提供了新工具，可用于跟踪训练过程中的概念演化。


### [142] [Knot So Simple: A Minimalistic Environment for Spatial Reasoning](https://arxiv.org/abs/2505.18028)
*Zizhao Chen,Yoav Artzi*

Main category: cs.LG

TL;DR: KnotGym是一个用于复杂空间推理和操作的交互环境，专注于基于图像观察的绳结操作任务，任务难度可量化。

- Motivation: 提供一个可量化复杂度的环境，以测试和开发在纯图像观察下进行空间推理和操作的方法。
- Method: 设计基于绳结交叉数量的任务复杂度轴，评估模型强化学习、模型预测控制和链式推理等方法。
- Result: KnotGym展示了在整合感知、空间推理和操作方面的核心挑战。
- Conclusion: KnotGym为复杂空间推理和操作研究提供了可扩展且具有挑战性的测试平台。


### [143] [Mahalanobis++: Improving OOD Detection via Feature Normalization](https://arxiv.org/abs/2505.18032)
*Maximilian Mueller,Matthias Hein*

Main category: cs.LG

TL;DR: 论文提出通过简单的ℓ₂归一化特征，显著提升基于马氏距离的OOD检测方法性能，解决了特征范数不一致导致的问题。

- Motivation: 在安全关键应用中，检测分布外（OOD）样本对部署可靠的机器学习模型至关重要。现有基于马氏距离的后处理方法在ImageNet规模OOD检测中表现不一，主要因特征范数差异大，违背了高斯分布假设。
- Method: 通过ℓ₂归一化特征，使其更符合高斯分布假设，从而改进马氏距离估计。
- Result: 在44种不同架构和预训练方案的模型上实验表明，ℓ₂归一化显著且一致地提升了传统马氏距离方法的性能，优于其他近期提出的OOD检测方法。
- Conclusion: ℓ₂归一化是提升马氏距离OOD检测方法性能的有效且简单的方法。


### [144] [Towards more transferable adversarial attack in black-box manner](https://arxiv.org/abs/2505.18097)
*Chun Tong Lei,Zhongliang Guo,Hon Chung Lee,Minh Quoc Duong,Chun Pong Lau*

Main category: cs.LG

TL;DR: 论文提出了一种新的损失函数和替代模型，以减少计算开销的同时提高对抗攻击的迁移性。

- Motivation: 传统黑盒攻击依赖优化框架改进迁移性，而扩散模型虽有效但计算成本高。作者假设具有类似归纳偏置的模型结合适当损失函数可达到类似效果。
- Method: 提出了一种新的损失函数和独特的替代模型，利用分类器引导扩散模型的时间相关分类器分数。
- Result: 实验结果显示，该方法在不同模型架构中显著提高了迁移性，同时对扩散防御保持鲁棒性。
- Conclusion: 验证了假设，表明无需依赖扩散模型即可高效实现高迁移性攻击。
## cs.CL

### [145] [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
*Abdullah Abdullah,Seong Tae Kim*

Main category: cs.CL

TL;DR: 提出了一种基于多模态视觉语言模型（VLM）的放射学知识图谱生成框架，解决了现有单模态方法的局限性。

- Motivation: 放射学知识图谱生成面临专业语言和领域数据稀缺的挑战，现有方法仅依赖报告且难以处理长数据。
- Method: 采用多模态VLM框架，结合放射学报告和影像数据生成知识图谱。
- Result: 新方法优于现有方法，首次实现多模态放射学知识图谱生成。
- Conclusion: 多模态VLM框架为放射学知识图谱生成提供了更优解决方案。


### [146] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
*Xinlong Chen,Yuanxing Zhang,Qiang Liu,Junfei Wu,Fuzheng Zhang,Tieniu Tan*

Main category: cs.CL

TL;DR: 提出了一种名为MoD的新方法，通过动态调整解码策略来缓解大型视觉语言模型中的幻觉问题。

- Motivation: 大型视觉语言模型在视觉任务中表现出色，但仍受幻觉问题困扰。
- Method: MoD通过评估模型对图像标记的关注正确性，动态调整解码策略，包括互补策略和对比策略。
- Result: 实验表明，MoD在多个主流基准测试中显著优于现有解码方法，有效缓解了幻觉问题。
- Conclusion: MoD是一种有效的幻觉缓解方法，代码已开源。


### [147] [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)
*Prateek Verma,Mert Pilanci*

Main category: cs.CL

TL;DR: 通过训练自回归LLM模型，文本模型内部发展出理解和处理图像与音频的能力，无需额外微调即可实现多模态任务。

- Motivation: 探索文本LLM模型是否能够通过训练文本数据，自然发展出处理其他模态（如图像和音频）的能力，以减少从头训练多模态模型的需求。
- Method: 采用自回归LLM模型，输入图像块、音频波形或标记，输出嵌入或分类标签，验证其在音频和图像分类任务中的表现。
- Result: 在FSD-50K和GTZAN音频数据集及CIFAR-10和Fashion-MNIST图像数据集上，文本LLM模型展现出强大的分类能力。
- Conclusion: 文本LLM模型能够学习通用的内部电路，通过激活必要连接实现多模态任务，为模型复用提供了新思路。


### [148] [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
*Yanshu Li,Tian Yun,Jianjiang Yang,Pinyuan Feng,Jinfa Huang,Ruixiang Tang*

Main category: cs.CL

TL;DR: 论文提出TACO模型，通过任务映射动态配置多模态上下文序列，提升大视觉语言模型（LVLM）的推理能力。

- Motivation: 多模态上下文学习（ICL）的效果高度依赖输入序列质量，但对其工作机制的理解有限，阻碍了性能提升。
- Method: 提出任务映射视角，揭示演示序列中的局部和全局关系如何引导模型推理，并设计TACO模型，通过任务感知注意力动态配置序列。
- Result: 在五个LVLM和九个数据集上的实验表明，TACO在多样化ICL任务中均优于基线方法。
- Conclusion: 任务映射为理解和改进多模态ICL提供了新视角，TACO模型展示了其有效性。


### [149] [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
*Subrata Biswas,Mohammad Nur Hossain Khan,Bashima Islam*

Main category: cs.CL

TL;DR: RAVEN是一种多模态问答架构，通过QuART模块动态分配模态令牌的相关性分数，提升信号质量并抑制干扰。采用三阶段训练流程，并在AVS-QA数据集上验证，显著优于现有方法。

- Motivation: 解决多模态问答中模态分歧问题，如无关音频或视频干扰融合模型。
- Method: 提出QuART模块动态评分模态令牌，采用三阶段训练（单模态预训练、查询对齐融合、分歧微调）。
- Result: 在七大多模态QA基准测试中，RAVEN准确率提升最高14.5%，传感器数据额外提升16.4%，抗干扰能力优于SOTA 50.23%。
- Conclusion: RAVEN通过动态模态选择和鲁棒训练，显著提升多模态问答性能，并开源代码与数据集。


### [150] [CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation](https://arxiv.org/abs/2505.17167)
*Ibrahim Ethem Hamamci,Sezgin Er,Suprosanna Shit,Hadrien Reynaud,Bernhard Kainz,Bjoern Menze*

Main category: cs.CL

TL;DR: 提出了CRG Score，一种分布感知且可适应的度量标准，用于评估放射学报告生成中的临床相关性异常。

- Motivation: 现有的NLG指标无法捕捉临床正确性，而基于LLM的指标缺乏普适性，临床准确性指标则对类别不平衡敏感。
- Method: CRG Score通过仅评估参考报告中明确描述的临床相关异常，支持二元和结构化标签，并可搭配任何LLM进行特征提取。
- Result: CRG Score通过基于标签分布的惩罚平衡，实现了更公平、更稳健的评估，并作为临床对齐的奖励函数。
- Conclusion: CRG Score为放射学报告生成提供了一种更有效的评估方法。


### [151] [Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports](https://arxiv.org/abs/2505.17625)
*Hayato Aida,Kosuke Takahashi,Takahiro Omi*

Main category: cs.CL

TL;DR: 提出了一种增强大型视觉语言模型（LVLM）表格理解能力的方法，通过结合表格内文本内容和布局特征，显著提升了复杂文档布局的解析性能。

- Motivation: 随着大语言模型（LLM）和检索增强生成（RAG）的发展，表格结构理解在金融等领域变得至关重要。然而，表格格式多样（如HTML、图像、纯文本），现有大型视觉语言模型（LVLM）在字符和空间关系理解上仍有不足。
- Method: 通过引入表格内文本内容和布局特征，增强LVLM的表格理解能力。
- Result: 实验结果表明，辅助模态显著提升了性能，能够在不依赖显式结构化输入的情况下解析复杂文档布局。
- Conclusion: 该方法为多模态LLM在表格理解领域的应用提供了有效解决方案。
## cs.AI

### [152] [MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation](https://arxiv.org/abs/2505.17613)
*Jihan Yao,Yushi Hu,Yujie Yi,Bin Han,Shangbin Feng,Guang Yang,Bingbing Wen,Ranjay Krishna,Lucy Lu Wang,Yulia Tsvetkov,Noah A. Smith,Banghua Zhu*

Main category: cs.AI

TL;DR: MMMG是一个多模态生成评估基准，涵盖4种模态组合和49个任务，通过模型与程序结合实现可靠自动评估，与人类评估高度一致（94.3%）。

- Motivation: 自动评估多模态生成任务时，现有指标常难以与人类评估一致，尤其是复杂多模态任务。
- Method: 开发MMMG基准，包含49个任务（29个新任务）和937个指令，评估多模态生成模型的关键能力。
- Result: MMMG与人类评估高度一致（94.3%），当前最佳模型GPT Image在图像生成上达78.3%，但在多模态推理和交错生成上表现不足。音频生成仍有较大改进空间。
- Conclusion: MMMG为多模态生成评估提供了可靠基准，揭示了当前模型的不足，尤其是音频生成和多模态推理方向需进一步研究。


### [153] [ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback](https://arxiv.org/abs/2505.17908)
*Litao Guo,Xinli Xu,Luozhou Wang,Jiantao Lin,Jinsong Zhou,Zixin Zhang,Bolan Su,Ying-Cong Chen*

Main category: cs.AI

TL;DR: ComfyMind是一个基于ComfyUI平台的协作AI系统，通过语义工作流接口和搜索树规划机制，提升复杂生成任务的稳定性和灵活性，并在多个基准测试中表现优异。

- Motivation: 现有开源框架在支持复杂现实应用时表现脆弱，缺乏结构化工作流规划和执行级反馈，ComfyMind旨在解决这些问题。
- Method: ComfyMind引入语义工作流接口（SWI）和搜索树规划机制，前者将低级节点图抽象为自然语言描述的功能模块，后者通过分层决策和局部反馈实现自适应修正。
- Result: 在ComfyBench、GenEval和Reason-Edit三个基准测试中，ComfyMind表现优于现有开源基线，性能接近GPT-Image-1。
- Conclusion: ComfyMind为开源通用生成AI系统的发展提供了有前景的路径。


### [154] [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/abs/2505.18134)
*Alex L. Zhang,Thomas L. Griffiths,Karthik R. Narasimhan,Ofir Press*

Main category: cs.AI

TL;DR: VideoGameBench是一个用于评估视觉语言模型（VLMs）在实时视频游戏中表现的新基准，包含10款游戏，挑战模型仅凭视觉输入完成任务。前沿模型表现不佳，完成率极低。

- Motivation: 研究VLMs在人类自然任务（如感知、空间导航）中的能力，填补现有研究的空白。
- Method: 引入VideoGameBench基准，包含10款1990年代游戏，模型需实时交互并完成任务。部分游戏保密以测试泛化能力。
- Result: 前沿模型（如Gemini 2.5 Pro）完成率极低（0.48%），实时延迟是主要限制。
- Conclusion: VideoGameBench为研究VLMs在人类技能任务中的表现提供了新方向，需进一步改进模型能力。
## eess.IV

### [155] [TAGS: 3D Tumor-Adaptive Guidance for SAM](https://arxiv.org/abs/2505.17096)
*Sirui Li,Linkai Peng,Zheyuan Zhang,Gorkem Durak,Ulas Bagci*

Main category: eess.IV

TL;DR: TAGS框架通过多提示融合将2D基础模型（如CLIP和SAM）适配到3D医学图像分割任务，显著提升肿瘤分割性能。

- Motivation: 现有基础模型（如CLIP和SAM）在2D自然图像上表现优异，但在3D医学图像（如肿瘤分割）中因领域差异和缺乏3D解剖上下文而受限。
- Method: 提出TAGS框架，结合CLIP的语义信息和解剖学特定提示，增强SAM的空间特征提取能力，同时保留预训练权重。
- Result: 在三个开源肿瘤分割数据集上，TAGS超越现有最佳模型（如nnUNet）46.88%，并显著优于其他医学基础模型（至少提升13%）。
- Conclusion: TAGS展示了在多样化医学分割任务中的鲁棒性和适应性，为2D基础模型在3D医学领域的应用提供了有效解决方案。


### [156] [Assessing the generalization performance of SAM for ureteroscopy scene understanding](https://arxiv.org/abs/2505.17210)
*Martin Villagrana,Francisco Lopez-Tiro,Clement Larose,Gilberto Ochoa-Ruiz,Christian Daul*

Main category: eess.IV

TL;DR: 研究探讨了Segment Anything Model (SAM)在肾结石分割中的潜力，发现其在泛化能力上显著优于传统U-Net模型。

- Motivation: 肾结石分割是识别结石类型的关键步骤，但手动分割在大规模图像数据库中不切实际，因此需要自动化解决方案。
- Method: 研究比较了SAM与传统模型（U-Net、Residual U-Net、Attention U-Net）的性能，评估其在分布内和分布外数据上的表现。
- Result: SAM在分布内数据上与U-Net表现相当（准确率97.68%，Dice系数97.78%，IoU 95.76%），但在分布外数据上泛化能力显著提升，最高领先23%。
- Conclusion: SAM在肾结石分割中展现出卓越的适应性和效率，是自动化分割的有力工具。


### [157] [SUFFICIENT: A scan-specific unsupervised deep learning framework for high-resolution 3D isotropic fetal brain MRI reconstruction](https://arxiv.org/abs/2505.17472)
*Jiangjie Wu,Lixuan Chen,Zhenghao Li,Xin Li,Saban Ozturk,Lihui Wang,Rongpin Wang,Hongjiang Wei,Yuyao Zhang*

Main category: eess.IV

TL;DR: 提出了一种无监督的迭代SVR-SRR框架，用于从运动伪影的2D切片重建高质量3D胎儿脑MRI，无需大规模外部训练数据。

- Motivation: 临床胎儿MRI难以获取大规模外部训练数据，而传统深度学习方法依赖此类数据。因此，需要一种无监督方法来解决这一问题。
- Method: 通过卷积神经网络参数化SVR，将2D切片与3D目标体积对齐；在SRR中，结合深度图像先验框架和解码网络，优化高分辨率体积重建。
- Result: 在模拟数据和临床数据上，该框架优于现有胎儿脑重建方法。
- Conclusion: 提出的无监督框架在胎儿脑MRI重建中表现优异，无需依赖外部训练数据。


### [158] [Anatomy-Guided Multitask Learning for MRI-Based Classification of Placenta Accreta Spectrum and its Subtypes](https://arxiv.org/abs/2505.17484)
*Hai Jiang,Qiongting Liu,Yuanpin Zhou,Jiawei Pan,Ting Song,Yao Lu*

Main category: eess.IV

TL;DR: 提出了一种新型CNN架构，用于胎盘植入谱系障碍（PAS）及其亚型的一阶段多类诊断，通过双分支结构和多任务学习策略，显著提升了诊断性能。

- Motivation: 现有指南和方法主要关注PAS的存在，而亚型识别研究有限，且多类诊断通常依赖低效的两阶段级联分类任务。
- Method: 设计了一种双分支CNN架构，主分支采用残差块结构，次分支整合解剖特征，结合多任务学习策略。
- Result: 在真实临床数据集上实现了最先进的性能。
- Conclusion: 该模型为PAS及其亚型的高效诊断提供了新方法。


### [159] [DECT-based Space-Squeeze Method for Multi-Class Classification of Metastatic Lymph Nodes in Breast Cancer](https://arxiv.org/abs/2505.17528)
*Hai Jiang,Chushan Zheng,Jiawei Pan,Yuanpin Zhou,Qiongting Liu,Xiang Zhang,Jun Shen,Yao Lu*

Main category: eess.IV

TL;DR: 该研究利用双能CT（DECT）开发了一种非侵入性模型，通过空间挤压方法和虚拟类注入技术，提高了对乳腺癌淋巴结转移负担的多分类准确性。

- Motivation: 传统影像学方法难以区分淋巴结转移负担的不同水平，因此需要一种更准确的非侵入性评估工具来指导治疗决策。
- Method: 提出了一种结合通道注意力机制和虚拟类注入的空间挤压方法，用于压缩和重新校准DECT的光谱空间特征。
- Result: 在227例活检确认的病例中，该方法平均测试AUC为0.86，优于现有CNN模型。
- Conclusion: 该框架通过整合DECT的光谱空间数据并减少类别模糊性，为临床实践提供了一种有前景的非侵入性评估工具。


### [160] [FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image Segmentation](https://arxiv.org/abs/2505.17544)
*Ruiqi Xing*

Main category: eess.IV

TL;DR: FreqU-FNet是一种基于频率域的U形分割架构，通过频率编码器和空间可学习解码器解决医学图像分割中的类别不平衡和局部细节问题。

- Motivation: 医学图像分割中，类别不平衡和频率分布不均导致传统CNN和Transformer方法难以捕捉少数类信号和局部细节。
- Method: 提出FreqU-FNet，结合低通频率卷积和Daubechies小波下采样的频率编码器，以及自适应多分支上采样的空间可学习解码器，并设计频率感知损失函数。
- Result: 在多个医学分割基准测试中，FreqU-FNet显著优于CNN和Transformer基线，尤其在少数类处理上表现突出。
- Conclusion: FreqU-FNet通过频率域操作有效解决了医学图像分割中的关键问题，展示了频率特征的重要性。


### [161] [Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras](https://arxiv.org/abs/2505.17582)
*Masataka Kobayashi,Shintaro Shiba,Quan Kong,Norimasa Kobori,Tsukasa Shimizu,Shan Lu,Takaya Yamazato*

Main category: eess.IV

TL;DR: 提出了一种基于单目事件相机和路边LED条的距离估计方法，通过相位相关技术实现高精度三角测量，实验显示在20-60米范围内误差小于0.5米。

- Motivation: 随着自动驾驶的发展，多传感器融合虽然有效，但硬件复杂且成本高，因此开发多功能单传感器成为需求。事件相机因其高动态范围和低延迟等特性成为理想选择。
- Method: 利用单目事件相机和路边LED条，通过相位相关技术检测光源间的空间位移，实现无需立体视觉的高精度距离估计。
- Result: 户外驾驶场景实验表明，该方法在20-60米范围内的成功率超过90%，误差小于0.5米。
- Conclusion: 未来可通过扩展至完整位置估计，结合智能基础设施，提升导航精度和智能交通系统集成。


### [162] [Towards Prospective Medical Image Reconstruction via Knowledge-Informed Dynamic Optimal Transport](https://arxiv.org/abs/2505.17644)
*Taoran Zheng,Xing Li,Yan Yang,Xiang Gu,Zongben Xu,Jian Sun*

Main category: eess.IV

TL;DR: 论文提出了一种基于成像知识动态最优传输（KIDOT）的医学图像重建方法，解决了模拟数据与真实数据之间的性能差距问题。

- Motivation: 医学图像重建中，模拟数据训练模型在真实数据上表现不佳，因模拟与真实成像知识不完整。
- Method: KIDOT将重建建模为从测量到图像的动态传输路径，利用成像知识指导传输过程，支持无配对数据学习。
- Result: 实验表明，KIDOT在MRI和CT重建中表现优异。
- Conclusion: KIDOT通过动态最优传输和成像知识结合，提升了重建的鲁棒性和性能。


### [163] [Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection](https://arxiv.org/abs/2505.17683)
*Dan Yuan,Yi Feng,Ziyun Tang*

Main category: eess.IV

TL;DR: 论文提出了一种结合CBAM和SAL的增强Residual U-Net架构，用于早产儿脑超声图像中脑室出血的精确分割，取得了优异的性能。

- Motivation: 早产儿脑室出血（IVH）是一种严重的神经并发症，需要从脑超声图像中早期准确检测以改善临床结果。现有深度学习方法在捕捉局部空间细节和全局上下文依赖方面存在挑战。
- Method: 提出了一种增强的Residual U-Net架构，结合了CBAM（用于空间和通道特征细化）和SAL（稀疏注意力层，通过双分支设计抑制噪声并确保信息传播）。
- Result: 在脑超声数据集上的实验表明，该方法在脑室区域分割中达到了89.04%的Dice分数和81.84%的IoU，性能优于现有方法。
- Conclusion: 结合空间细化和注意力稀疏性的方法对脑解剖结构的鲁棒检测非常有效。


### [164] [UltraBoneUDF: Self-supervised Bone Surface Reconstruction from Ultrasound Based on Neural Unsigned Distance Functions](https://arxiv.org/abs/2505.17912)
*Luohong Wu,Matthias Seibold,Nicola A. Cavalcanti,Giuseppe Loggia,Lisa Reissner,Bastian Sigrist,Jonas Hein,Lilian Calvet,Arnd Viehöfer,Philipp Fürnstahl*

Main category: eess.IV

TL;DR: UltraBoneUDF是一种自监督框架，用于从超声数据中重建开放骨表面，通过神经网络无符号距离函数和全局特征提取器显著提升重建质量。

- Motivation: 传统超声成像仅捕获部分骨表面，现有方法难以处理不完整数据，导致重建误差和伪影。UltraBoneUDF旨在解决这一问题。
- Method: 提出自监督框架UltraBoneUDF，结合神经网络无符号距离函数和全局特征提取器，并引入基于局部切平面优化的新型损失函数。
- Result: UltraBoneUDF在四个开源数据集上显著优于现有方法，平均Chamfer距离误差降低39.6%至70.2%。
- Conclusion: UltraBoneUDF为开放骨表面重建提供了一种高效且准确的方法，优于现有技术。


### [165] [Promptable cancer segmentation using minimal expert-curated data](https://arxiv.org/abs/2505.17915)
*Lynn Karam,Yipei Wang,Veeru Kasivisvanathan,Mirabela Rusu,Yipeng Hu,Shaheer U. Saeed*

Main category: eess.IV

TL;DR: 提出了一种新的可提示分割方法，仅需少量标注数据（24张全标注和8张弱标注图像），通过结合弱监督和全监督分类器优化分割效果，显著减少标注成本。

- Motivation: 解决医学图像癌症分割中标注成本高、数据获取困难及现有方法对病理区域效果不佳的问题。
- Method: 结合弱监督和全监督分类器，通过单点提示引导搜索过程优化分割。
- Result: 在仅使用少量标注数据的情况下，性能优于现有可提示分割方法，接近全监督方法。
- Conclusion: 该方法显著降低了标注数据需求，使高质量标签的获取成为可能。


### [166] [Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment](https://arxiv.org/abs/2505.17971)
*Danial Khan,Zohaib Salahuddin,Yumeng Zhang,Sheng Kuang,Shruti Atul Mali,Henry C. Woodruff,Sina Amirrajab,Rachel Cavill,Eduardo Ibor-Crespo,Ana Jimenez-Pastor,Adrian Galiana-Bordera,Paula Jimenez Gomez,Luis Marti-Bonmati,Philippe Lambin*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的自动化前列腺癌风险分层流程，结合解剖学指导和可解释性，显著提升了诊断准确性和效率。

- Motivation: 通过结合解剖学信息和深度学习，提高前列腺癌风险分层的准确性和可解释性，支持临床决策。
- Method: 流程包括nnU-Net分割模块、UMedPT Swin Transformer分类模块和VAE-GAN生成反事实热图模块，使用1,500例PI-CAI数据和617例CHAIMELEON数据。
- Result: 分割Dice分数达0.95（腺体），分类AUC提升至0.79，临床测试中诊断准确性从0.72提升至0.77，审查时间减少40%。
- Conclusion: 解剖学感知的深度学习模型结合反事实解释，可实现准确、可解释且高效的前列腺癌风险评估，有望成为临床虚拟活检工具。


### [167] [A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer](https://arxiv.org/abs/2505.18058)
*Yumeng Zhang,Zohaib Salahuddin,Danial Khan,Shruti Atul Mali,Henry C. Woodruff,Sina Amirrajab,Eduardo Ibor-Crespo,Ana Jimenez-Pastor,Luis Marti-Bonmati,Philippe Lambin*

Main category: eess.IV

TL;DR: 该研究开发了一种基于多中心、基础模型驱动的框架，用于自动分类直肠癌MRI中的EVI和MFI，通过特征融合和频率域协调显著提升了诊断性能。

- Motivation: 由于视觉评估EVI和MFI存在主观性和机构间差异，研究旨在开发一种自动分类方法以提高准确性和一致性。
- Method: 研究使用331例直肠癌MRI数据，通过TotalSegmentator提取直肠区域，训练频率域协调管道以减少扫描仪差异，并比较了四种分类器的性能。
- Result: UMedPT_LR在EVI检测中表现最佳（AUC=0.82），UMedPT在MFI分类中表现最优（AUC=0.77），均优于现有方法。
- Conclusion: 结合基础模型特征、协调技术和多视图融合可显著提升直肠MRI的诊断性能。


### [168] [Accelerating Learned Image Compression Through Modeling Neural Training Dynamics](https://arxiv.org/abs/2505.18107)
*Yichi Zhang,Zhihao Duan,Yuning Huang,Fengqing Zhu*

Main category: eess.IV

TL;DR: 本文提出了一种加速学习图像压缩（LIC）方法训练的技术，通过建模神经训练动态，减少可训练参数数量并加速收敛。

- Motivation: 随着学习图像压缩方法计算需求增加，提高训练效率至关重要。
- Method: 提出Sensitivity-aware True and Dummy Embedding Training（STDET）机制，聚类参数为少量模式，并利用参数敏感性和稳定相关性减少可训练参数；结合Sampling-then-Moving Average（SMA）技术平滑训练动态。
- Result: 方法显著减少训练空间维度和可训练参数数量，不牺牲模型性能，加速收敛。理论分析显示其训练方差低于标准SGD。
- Conclusion: 该方法为开发高效LIC训练方法提供了新思路。
## cs.RO

### [169] [Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling](https://arxiv.org/abs/2505.17659)
*Xiaolong Tang,Meina Kan,Shiguang Shan,Xilin Chen*

Main category: cs.RO

TL;DR: Plan-R1是一个两阶段轨迹规划框架，结合了专家数据和强化学习，显著提升了自动驾驶的安全性和可行性。

- Motivation: 现有基于学习的规划方法依赖专家演示，缺乏明确的安全意识，可能继承不安全行为。
- Method: 两阶段框架：第一阶段通过专家数据训练自回归轨迹预测器；第二阶段设计规则奖励并使用GRPO强化学习微调模型。
- Result: 在nuPlan基准测试中，Plan-R1显著提升了规划安全性和可行性，达到最优性能。
- Conclusion: Plan-R1通过结合专家数据和强化学习，实现了更安全、可行的自动驾驶轨迹规划。


### [170] [Is Single-View Mesh Reconstruction Ready for Robotics?](https://arxiv.org/abs/2505.17966)
*Frederik Nolte,Bernhard Schölkopf,Ingmar Posner*

Main category: cs.RO

TL;DR: 该论文评估了单视角网格重建模型在机器人操作中创建数字孪生环境的适用性，发现现有方法虽在计算机视觉基准上表现良好，但无法满足机器人特定需求。

- Motivation: 研究单视角3D重建模型在机器人操作中的适用性，填补计算机视觉与机器人需求之间的差距。
- Method: 建立机器人场景下的3D重建基准标准，包括处理典型输入、生成无碰撞稳定重建、管理遮挡和满足计算约束。
- Result: 实证评估显示，现有方法无法满足机器人特定需求，揭示了单视角重建在机器人应用中的局限性。
- Conclusion: 研究结果强调了计算机视觉进展与机器人需求之间的关键差距，为未来研究提供了方向。
## cs.GR

### [171] [From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation](https://arxiv.org/abs/2505.17402)
*Mahmoud Chick Zaouali,Todd Charter,Homayoun Najjaran*

Main category: cs.GR

TL;DR: 提出了一种基于无人机的方法，结合Feature-3DGS和语言引导的3D分割，利用CLIP嵌入和SAM/SAM2实现语义分割，提升空中检测的语义理解能力。

- Motivation: 传统摄影测量和3D高斯溅射技术缺乏语义解释性，限制了自动化检测的效率。
- Method: 结合Feature-3DGS、LSeg特征场和CLIP嵌入，通过语言提示生成热图，再使用SAM/SAM2进行精细分割。
- Result: 展示了不同特征场主干（CLIP-LSeg、SAM、SAM2）在大规模户外环境中的表现，验证了方法的有效性。
- Conclusion: 该方法实现了语言驱动的3D重建交互，为语义空中检测和场景理解提供了新可能。


### [172] [Multi-Person Interaction Generation from Two-Person Motion Priors](https://arxiv.org/abs/2505.17860)
*Wenning Xu,Shiyu Fan,Paul Henderson,Edmond S. L. Ho*

Main category: cs.GR

TL;DR: 提出了一种基于图驱动的交互采样方法，利用现有双人运动扩散模型生成多样且真实的多人物交互动作。

- Motivation: 多人物交互建模是一个尚未充分探索的领域，现有方法难以生成高质量且多样化的交互动作。
- Method: 通过将复杂多人物交互分解为图结构中的双人交互（Pairwise Interaction Graph），并引入图相关引导项以减少生成中的伪影。
- Result: 实验表明，该方法在生成多样且高质量的多人物交互动作时，显著减少了伪影，优于现有方法。
- Conclusion: 该方法无需训练新模型，通过分解和引导实现了高效的多人物交互生成。


### [173] [WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions](https://arxiv.org/abs/2505.18151)
*Zizhang Li,Hong-Xing Yu,Wei Liu,Yin Yang,Charles Herrmann,Gordon Wetzstein,Jiajun Wu*

Main category: cs.GR

TL;DR: WonderPlay是一个结合物理模拟与视频生成的新框架，从单张图像生成动作条件的动态3D场景。

- Motivation: 现有方法局限于刚体或简单弹性动力学，而WonderPlay旨在通过混合生成模拟器合成更广泛的3D动态效果。
- Method: 采用混合生成模拟器，先用物理求解器模拟粗略3D动态，再通过视频生成器生成更精细、真实的视频，最后用视频更新模拟场景，形成闭环。
- Result: 实验表明，WonderPlay支持用户通过单张图像与多种场景（如布料、沙、雪、液体等）交互。
- Conclusion: 该框架结合了物理模拟的精确性与扩散视频生成的表现力，代码将公开。
