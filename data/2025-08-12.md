[[toc]]

## cs.CV

### [1] [Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG](https://arxiv.org/abs/2508.06496)
*Rakesh Raj Madavan,Akshat Kaimal,Hashim Faisal,Chandrakala S*

Main category: cs.CV

TL;DR: BIND和Med-GRIM模型通过密集编码和图形检索提升医疗VQA任务的精确性，减少计算成本。

- Motivation: 现有VQA模型在复杂领域（如医疗）中缺乏精确性，需改进。
- Method: BIND通过密集编码优化嵌入空间；Med-GRIM结合图形检索和提示工程，使用小型语言模型。
- Result: Med-GRIM以低计算成本实现高性能，并发布DermaGraph数据集支持研究。
- Conclusion: BIND和Med-GRIM为医疗VQA提供高效解决方案，推动零样本多模态研究。


### [2] [DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation](https://arxiv.org/abs/2508.06511)
*He Feng,Yongjia Ma,Donglin Di,Lei Fan,Tonghua Su,Xiangqian Wu*

Main category: cs.CV

TL;DR: DiTalker是一个基于DiT的统一框架，用于控制说话风格的肖像动画，解决了现有方法忽略动态风格（如头部运动）的问题，并通过模块化设计优化了同步和细节保留。

- Motivation: 现有基于扩散的肖像动画方法主要关注唇同步或静态情感转换，忽略了动态风格（如头部运动），且双U-Net架构增加了计算开销。
- Method: 提出DiTalker框架，包含Style-Emotion Encoding Module（风格和情感分支）和Audio-Style Fusion Module（解耦音频与风格），并采用优化约束提升唇同步和细节保留。
- Result: 实验证明DiTalker在唇同步和说话风格控制方面优于现有方法。
- Conclusion: DiTalker通过模块化设计和优化约束，实现了高效的动态风格控制和高质量的肖像动画。


### [3] [BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok](https://arxiv.org/abs/2508.06515)
*Minh Duc Chu,Kshitij Pawar,Zihao He,Roxanna Sharifi,Ross Sonnenblick,Magdalayna Curry,Laura D'Adamo,Lindsay Young,Stuart B Murray,Kristina Lerman*

Main category: cs.CV

TL;DR: 论文提出BigTokDetect框架，用于检测TikTok上促进肌肉变形行为的危害内容，通过多模态方法提升检测效果。

- Motivation: 社交媒体难以检测伪装成健身内容的肌肉变形行为危害内容，尤其是对青少年男性的影响。
- Method: 开发BigTokDetect框架，使用专家标注的多模态数据集BigTok，结合视觉语言模型进行检测。
- Result: 在多模态融合下，主分类准确率达82.9%，子分类达69%，视频特征表现最佳。
- Conclusion: 研究为心理健康领域的有害内容检测提供了新基准和方法论。


### [4] [Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation](https://arxiv.org/abs/2508.06517)
*Haoran Xi,Chen Liu,Xiaolin Li*

Main category: cs.CV

TL;DR: FPGM是一种基于频率先验的半监督学习方法，通过利用息肉边缘的频率特征提升跨域分割性能。

- Motivation: 解决息肉分割中标注数据不足和域适应能力差的问题。
- Method: 两阶段框架：学习域不变频率先验，并对未标注图像进行频谱扰动以对齐频率特征。
- Result: 在六个数据集上达到SOTA，零样本泛化能力显著提升（Dice分数提高10%以上）。
- Conclusion: FPGM为临床部署提供了鲁棒的息肉分割解决方案。


### [5] [Large Language Models Facilitate Vision Reflection in Image Classification](https://arxiv.org/abs/2508.06525)
*Guoyuan An,JaeYoon Kim,SungEui Yoon*

Main category: cs.CV

TL;DR: 本文揭示了大型多模态模型（LMMs）中视觉反射的可解释性新发现，包括通过提示验证提升识别准确率、视觉-语言连接器的作用，以及无需训练的连接器对细粒度任务的改进。

- Motivation: 探索LMMs在视觉任务中的表现及其可解释性，特别是视觉反射如何通过语言模型提升性能。
- Method: 通过提示验证、分析视觉-语言连接器行为，以及测试训练-free连接器在细粒度任务中的效果。
- Result: 发现视觉反射能提升识别准确率，连接器将视觉特征映射为文本概念，且少量文本标记可替代大量视觉标记。
- Conclusion: 视觉反射为LMMs提供了鲁棒且可解释的视觉识别策略。


### [6] [A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition](https://arxiv.org/abs/2508.06528)
*Xiuliang Zhang,Tadiwa Elisha Nyamasvisva,Chuntao Liu*

Main category: cs.CV

TL;DR: 提出了一种结合3D CNN和Transformer的混合框架，用于视频行为识别，解决了传统方法在长程依赖和高计算成本上的问题，并在性能上优于单独使用这两种方法。

- Motivation: 视频行为识别在公共安全、智能监控和人机交互等领域至关重要。传统3D CNN擅长捕捉局部时空特征但难以建模长程依赖，而Transformer擅长全局上下文但计算成本高。
- Method: 设计了一个混合框架，3D CNN模块提取低层时空特征，Transformer模块捕捉长程时间依赖，并通过融合机制整合两种表示。
- Result: 在基准数据集上，该模型优于传统3D CNN和单独Transformer，实现了更高的识别准确率且复杂度可控。消融实验验证了两模块的互补性。
- Conclusion: 该混合框架为视频行为识别提供了高效且可扩展的解决方案。


### [7] [RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving](https://arxiv.org/abs/2508.06529)
*Jiayuan Wang,Q. M. Jonathan Wu,Katsuya Suto,Ning Zhang*

Main category: cs.CV

TL;DR: RMT-PPAD是一种基于Transformer的多任务模型，用于实时自动驾驶感知，联合完成目标检测、可行驶区域分割和车道线分割，性能优越且速度快。

- Motivation: 自动驾驶系统需要高精度和实时性的全景驾驶感知，现有方法在多任务联合处理上存在负迁移和标签不一致问题。
- Method: 提出轻量级门控适配器模块自适应融合共享和任务特定特征，设计自适应分割解码器自动学习多尺度特征权重，解决标签不一致问题。
- Result: 在BDD100K数据集上，目标检测mAP50为84.9%，召回率95.4%；可行驶区域分割mIoU为92.6%；车道线分割IoU为56.8%，准确率84.7%；推理速度32.6 FPS。
- Conclusion: RMT-PPAD在性能和速度上均达到先进水平，实际场景中表现稳定，代码和模型已开源。


### [8] [What Makes "Good" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?](https://arxiv.org/abs/2508.06530)
*Ming-Kun Xie,Jia-Hao Xiao,Gang Niu,Lei Feng,Zhiqiang Kou,Min-Ling Zhang,Masashi Sugiyama*

Main category: cs.CV

TL;DR: HOPE基准通过生成更具误导性的干扰项，更严格评估大型视觉语言模型（LVLMs）的幻觉问题，显著优于POPE基准。

- Motivation: 现有POPE基准因采样策略简单，无法有效评估LVLMs的幻觉问题。
- Method: HOPE利用CLIP选择高预测可能性的负对象作为干扰项，并通过真实对象与虚假描述配对构建干扰项。
- Result: HOPE导致多种LVLMs的精度下降9%至23%，显著暴露其幻觉漏洞。
- Conclusion: HOPE为评估LVLMs的幻觉问题提供了更严格的基准。


### [9] [Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset](https://arxiv.org/abs/2508.06537)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: 论文分析了智能手机天体摄影数据集MobilTelesco，评估了目标检测模型在稀疏特征条件下的表现。

- Motivation: 现有目标检测数据集（如ImageNet、COCO）主要关注日常物体，缺乏非商业领域（如天体摄影）中的信号稀疏性。
- Method: 使用MobilTelesco数据集，对多种目标检测模型进行基准测试。
- Result: 揭示了模型在特征不足条件下的挑战。
- Conclusion: MobilTelesco数据集填补了稀疏信号领域的空白，为模型优化提供了新方向。


### [10] [MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing](https://arxiv.org/abs/2508.06543)
*Jinghan Yu,Zhiyuan Ma,Yue Ma,Kaiqi Liu,Yuhan Wang,Jianjun Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为MILD的新方法，用于解决复杂多实例场景下的人像擦除问题，通过分层扩散和形态学指导显著提升了性能。

- Motivation: 现有方法在复杂多实例场景（如人物遮挡、背景干扰）中表现不佳，主要由于数据集限制和空间解耦不足。
- Method: 提出Multi-Layer Diffusion（MILD）策略，将生成过程分解为语义分离的路径，并引入Human Morphology Guidance和Spatially-Modulated Attention。
- Result: MILD在挑战性的人像擦除任务中优于现有方法。
- Conclusion: MILD通过分层生成和形态学指导有效解决了复杂场景下的人像擦除问题。


### [11] [Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images](https://arxiv.org/abs/2508.06546)
*Qi Xun Yeo,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: 论文提出了一种仅使用多视角RGB图像进行3D语义场景图估计的方法，克服了伪点几何噪声和背景噪声问题，通过语义掩码和邻域信息提升特征鲁棒性，并利用统计先验优化预测结果。

- Motivation: 在缺乏3D标注数据的情况下，探索仅依赖多视角RGB图像实现准确的3D语义场景图估计。
- Method: 通过语义掩码引导特征聚合以减少背景噪声，设计邻域节点信息融合方法增强特征鲁棒性，并利用统计先验优化预测。
- Result: 实验表明，该方法在仅使用多视角图像输入的情况下优于现有方法。
- Conclusion: 该方法为无3D标注数据的场景图估计提供了有效解决方案，并展示了性能优势。


### [12] [Slice or the Whole Pie? Utility Control for AI Models](https://arxiv.org/abs/2508.06551)
*Ye Tao*

Main category: cs.CV

TL;DR: NNObfuscator是一种新型的实用控制机制，允许AI模型根据预设条件动态调整性能，避免了为不同用户需求训练多个模型的开销。

- Motivation: 深度神经网络训练资源密集，传统方法需为不同需求训练多个模型，效率低且难以维护。
- Method: 提出NNObfuscator机制，使单一模型能实时适应不同性能需求，支持分层访问控制。
- Result: 实验验证了NNObfuscator在图像分类、语义分割等任务中的有效性，单一模型可适应多任务。
- Conclusion: NNObfuscator提高了资源利用率，支持可持续的AI部署商业模式。


### [13] [Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection](https://arxiv.org/abs/2508.06552)
*Unisha Joshi*

Main category: cs.CV

TL;DR: 论文提出了一种解决深度伪造数据集中年龄偏见的方法，通过构建年龄多样化的数据集，并验证其在多种检测模型中的公平性和泛化能力。

- Motivation: 当前深度伪造检测模型存在年龄偏见问题，缺乏公平性，需要构建更全面的数据集以改进检测性能。
- Method: 结合现有数据集（Celeb-DF、FaceForensics++、UTKFace）和合成数据，构建年龄多样化的数据集，并使用XceptionNet、EfficientNet和LipForensics模型进行评估。
- Result: 新数据集显著提高了模型在不同年龄组的公平性、整体准确性和跨数据集泛化能力。
- Conclusion: 研究提供了一个可复现的、公平性更强的深度伪造数据集和模型流程，为未来研究奠定了基础。


### [14] [Static and Plugged: Make Embodied Evaluation Simple](https://arxiv.org/abs/2508.06553)
*Jiahao Xiao,Jianbo Zhang,BoWen Yan,Shengyu Guo,Tongrui Ye,Kaiwei Zhang,Zicheng Zhang,Xiaohong Liu,Zhengxue Cheng,Lei Fan,Chuyi Li,Guangtao Zhai*

Main category: cs.CV

TL;DR: StaticEmbodiedBench是一个基于静态场景表示的即插即用基准测试，解决了现有评估方法的高成本和碎片化问题。

- Motivation: 当前基于模拟或真实环境的评估方法成本高、难以扩展，需要一种统一的评估工具。
- Method: 提出StaticEmbodiedBench，覆盖42个场景和8个核心维度，支持通过简单接口进行可扩展评估。
- Result: 评估了19个VLMs和11个VLAs模型，建立了首个统一的静态排行榜。
- Conclusion: 通过发布200个样本，加速了具身智能的发展。


### [15] [StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback](https://arxiv.org/abs/2508.06555)
*Hongbo Ma,Fei Shen,Hongbin Xu,Xiaoce Wang,Gang Xu,Jinkai Zheng,Liangqiong Qu,Ming Li*

Main category: cs.CV

TL;DR: StyleTailor是一个协作代理框架，整合了服装设计、购物推荐、虚拟试穿和系统评估，通过多级负面反馈实现个性化时尚搭配。

- Motivation: 个性化时尚搭配领域尚未充分探索，StyleTailor旨在提升购物体验。
- Method: 采用迭代视觉优化范式，由设计师和顾问两个核心代理协作，通过分层视觉语言模型反馈逐步优化推荐。
- Result: 实验表明StyleTailor在个性化设计和推荐方面表现优异，优于无负面反馈的基线方法。
- Conclusion: StyleTailor为智能时尚系统设立了新基准，展示了负面反馈机制的有效性。


### [16] [From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets](https://arxiv.org/abs/2508.06556)
*Sarina Penquitt,Jonathan Klees,Rinor Cakaj,Daniel Kondermann,Matthias Rottmann,Lars Schmarje*

Main category: cs.CV

TL;DR: Error

- Motivation: Error
- Method: Error
- Result: Error
- Conclusion: Error


### [17] [On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications](https://arxiv.org/abs/2508.06558)
*Simon Baur,Alexandra Benova,Emilio Dolgener Cantú,Jackie Ma*

Main category: cs.CV

TL;DR: 提出了一种多模态特权知识蒸馏（MMPKD）方法，利用训练时额外的模态数据指导单模态视觉模型，提升注意力图的零样本能力，但效果不跨域。

- Motivation: 临床实践中部署深度学习模型常需多模态数据，但推理时并非所有模态都可用，因此研究如何利用训练时的额外模态提升单模态模型性能。
- Method: 提出MMPKD策略，使用文本和表格元数据作为教师模型，将知识蒸馏到视觉Transformer学生模型中。
- Result: MMPKD提升了注意力图在输入图像中定位ROI的零样本能力，但效果不跨域。
- Conclusion: MMPKD能有效利用训练时的额外模态提升单模态模型性能，但需注意其效果可能局限于特定领域。


### [18] [Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC](https://arxiv.org/abs/2508.06564)
*Guanyu Hu,Dimitrios Kollias,Xinyu Yang*

Main category: cs.CV

TL;DR: 论文提出了一种基于CLIP的视觉情感引导锚定（VEGA）机制，通过引入类级视觉语义提升多模态情感识别性能。

- Motivation: 现有模型在多模态情感识别中缺乏心理学先验指导，导致模态对齐不足。
- Method: 利用CLIP的图像编码器构建情感特定的视觉锚点，结合随机锚点采样和双分支自蒸馏架构。
- Result: 在IEMOCAP和MELD数据集上实现了最优性能。
- Conclusion: VEGA机制通过心理学对齐的视觉锚点显著提升了多模态情感识别的效果。


### [19] [Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06565)
*Jing Zhang,Xiaowei Yu,Minheng Chen,Lu Zhang,Tong Chen,Yan Zhuang,Chao Cao,Yanjun Lyu,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.CV

TL;DR: 提出一种新框架，将脑连接组与临床报告在共享跨模态潜在空间中对齐，提升表征学习，用于轻度认知障碍（MCI）诊断。

- Motivation: 结合脑成像数据与临床报告，利用多模态信息提升临床诊断效果，解决客观成像数据与主观文本报告的关联问题。
- Method: 将脑子网络作为成像数据的标记，与临床报告中的词语标记对齐，构建共享潜在空间，实现系统级关联分析。
- Result: 在ADNI数据集上取得领先预测性能，并识别出有临床意义的连接组-文本对，为阿尔茨海默病早期机制提供新见解。
- Conclusion: 该方法不仅性能优越，还为开发临床有用的多模态生物标志物提供了支持。


### [20] [Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features](https://arxiv.org/abs/2508.06566)
*Manish Kansana,Elias Hossain,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: Surformer v1是一种基于Transformer的架构，用于结合触觉和视觉输入进行表面材料分类，在准确性和计算效率之间取得了良好平衡。

- Motivation: 表面材料识别是机器人感知和物理交互的关键，结合触觉和视觉输入可以提升性能。
- Method: 提出Surformer v1，结合结构化触觉特征和PCA降维的视觉嵌入，使用模态特定编码器和跨模态注意力层。
- Result: Surformer v1达到99.4%准确率，推理时间0.77毫秒，优于其他模型。
- Conclusion: Surformer v1在准确性、效率和计算成本之间提供了理想平衡，适合实时应用。


### [21] [ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos](https://arxiv.org/abs/2508.06570)
*Mohammad Zia Ur Rehman,Anukriti Bhatnagar,Omkar Kabde,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频数据集ImpliHateVid，专注于隐式仇恨言论检测，并设计了一个两阶段对比学习框架用于视频仇恨言论检测。

- Motivation: 现有研究主要集中在文本和图像仇恨言论检测，视频领域的隐式仇恨言论研究不足。
- Method: 提出两阶段对比学习框架：第一阶段训练模态特定编码器（音频、文本、图像），第二阶段训练跨编码器优化多模态表示，并加入情感、情绪和字幕特征。
- Result: 在ImpliHateVid和HateMM数据集上验证了方法的有效性，证明了多模态对比学习在视频仇恨内容检测中的优势。
- Conclusion: 论文填补了视频隐式仇恨言论检测的空白，提出的数据集和方法为未来研究提供了重要基础。


### [22] [ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification](https://arxiv.org/abs/2508.06623)
*Sihan Ma,Qiming Wu,Ruotong Jiang,Frank Burns*

Main category: cs.CV

TL;DR: 论文提出了一种名为ContextGuard-LVLM的新框架，用于解决数字新闻媒体中视觉与文本信息的细粒度跨模态一致性（FCCC）问题。该框架基于先进的视觉语言大模型（LVLM），通过多阶段上下文推理机制和强化/对抗学习，显著提升了检测细微上下文不一致的能力。

- Motivation: 数字新闻媒体的普及需要更强大的方法来验证内容真实性，尤其是视觉与文本信息的深层一致性。传统方法在解决细粒度跨模态上下文一致性（FCCC）问题上表现不足。
- Method: 提出ContextGuard-LVLM框架，基于视觉语言大模型（LVLM），结合多阶段上下文推理机制和强化/对抗学习，检测细微的上下文不一致。扩展并标注了三个数据集，新增细粒度上下文注释。
- Result: ContextGuard-LVLM在几乎所有细粒度一致性任务上均优于现有零样本LVLM基线（如InstructBLIP和LLaVA 1.5），在复杂逻辑推理和上下文理解方面表现显著提升。
- Conclusion: ContextGuard-LVLM在检测复杂上下文不一致方面表现出色，对细微扰动具有更强的鲁棒性，并与人类专家判断高度一致，验证了其有效性。


### [23] [VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis](https://arxiv.org/abs/2508.06624)
*Kexin Yu,Zihan Xu,Jialei Xie,Carter Adams*

Main category: cs.CV

TL;DR: VL-MedGuide是一个基于视觉-语言大模型的新型框架，用于皮肤病的智能辅助诊断，具有高解释性和性能。

- Motivation: 解决皮肤病诊断中视觉特征复杂多样且现有模型缺乏解释性的问题。
- Method: 采用两阶段框架：多模态概念感知模块和可解释疾病推理模块，结合视觉和语言信息进行诊断。
- Result: 在Derm7pt数据集上表现优异，诊断和概念检测指标均超过现有基线。
- Conclusion: VL-MedGuide通过提供可解释的诊断结果，提升了AI在皮肤病临床实践中的实用性。


### [24] [CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation](https://arxiv.org/abs/2508.06625)
*Shilong Zou,Yuhang Huang,Renjiao Yi,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的跨域图像翻译方法，通过联合学习框架对齐扩散和翻译过程，提升全局最优性和性能。

- Motivation: 解决现有GAN方法在跨域图像翻译中的局限性，以及扩散模型与翻译过程未对齐导致的局部最优问题。
- Method: 提出联合学习框架，提取图像成分表示干净信号，并设计时间依赖的翻译网络，实现端到端学习。
- Result: 在RGB↔RGB及多种跨模态任务中表现优于现有方法，生成质量和结构一致性显著提升。
- Conclusion: 联合学习框架有效优化扩散和翻译过程，为跨域图像翻译提供了新思路。


### [25] [CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition](https://arxiv.org/abs/2508.06632)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Tiancheng Zhao,Gaolei Li,Changting Lin,Yike Guo,Meng Han*

Main category: cs.CV

TL;DR: 提出了一种基于动态系数分解的神经渲染框架，用于改进复杂视依赖外观的建模，生成更清晰的镜面反射和高光效果。

- Motivation: NeRF在复杂镜面反射和高光场景中表现不佳，现有方法存在模糊反射或优化不稳定的问题。
- Method: 通过动态系数分解将外观分解为静态神经基础和动态系数，由动态辐射积分器合成最终辐射。
- Result: 在多个挑战性基准测试中，该方法能生成更清晰、更真实的镜面高光。
- Conclusion: 这种分解范式为神经场景表示中的复杂外观建模提供了灵活有效的方向。


### [26] [Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors](https://arxiv.org/abs/2508.06640)
*Zheyuan Zhang,Weihao Tang,Hong Chen*

Main category: cs.CV

TL;DR: CausalNet是一种新的微表情识别框架，通过处理完整序列和因果学习模块，解决了关键帧索引误差问题，同时保持高识别精度。

- Motivation: 现有方法依赖准确的关键帧索引，但实际应用中获取准确索引困难且存在误差，限制了实用性。
- Method: 提出CausalNet框架，包含CMPLM模块定位肌肉运动区域和CAB模块学习因果关系，处理完整序列以减少冗余。
- Result: 在多个基准测试中，CausalNet在不同噪声水平下表现稳健，并在标准测试中超越现有方法。
- Conclusion: CausalNet通过因果学习提升了微表情识别的鲁棒性和准确性，具有实际应用潜力。


### [27] [Towards Robust Red-Green Watermarking for Autoregressive Image Generators](https://arxiv.org/abs/2508.06656)
*Denis Lukovnikov,Andreas Müller,Erwin Quiring,Asja Fischer*

Main category: cs.CV

TL;DR: 本文探讨了在自回归图像模型中应用生成时水印的方法，提出了两种基于视觉令牌聚类的新水印方案，显著提高了水印的鲁棒性和检测能力。

- Motivation: 尽管生成时水印在潜在扩散模型中表现出高鲁棒性，但其在自回归图像模型中的应用尚未被探索。本文旨在填补这一空白。
- Method: 提出了两种基于视觉令牌聚类的水印方法：一种是无训练的方法，依赖聚类查找表；另一种是微调VAE编码器，直接从扰动图像预测令牌聚类。
- Result: 实验表明，聚类级水印显著提高了对扰动和再生攻击的鲁棒性，同时保持了图像质量。聚类分类进一步提升了水印检测能力，优于基线方法。
- Conclusion: 本文提出的方法在自回归图像模型中实现了高效的水印检测和验证，为生成内容的检测和溯源提供了新思路。


### [28] [Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision](https://arxiv.org/abs/2508.06696)
*Tianqin Li,George Liu,Tai Sing Lee*

Main category: cs.CV

TL;DR: 论文提出使用线描作为结构优先的预训练模态，以生成更紧凑和可泛化的视觉表示。实验表明，线描预训练的模型在分类、检测和分割任务中表现更优，且具有更低的固有维度和更好的可压缩性。

- Motivation: 现代视觉识别系统依赖冗余的视觉输入，而人类能轻松理解稀疏的线描，表明结构而非外观是高效视觉理解的基础。
- Method: 提出线描预训练方法，包括监督和无监督（通过“学习绘制”方法）两种设置。
- Result: 线描预训练模型表现出更强的形状偏好、更集中的注意力和更高的数据效率，且表示更易压缩，适合蒸馏到轻量学生模型。
- Conclusion: 结构优先的视觉学习能提升效率、泛化能力和人类对齐的归纳偏差，为构建更鲁棒的视觉系统提供简单而强大的策略。


### [29] [MMFformer: Multimodal Fusion Transformer Network for Depression Detection](https://arxiv.org/abs/2508.06701)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: MMFformer是一种多模态抑郁症检测网络，通过分析社交媒体信息中的时空模式，显著提升了抑郁症检测的准确性。

- Motivation: 抑郁症早期诊断困难，主要依赖主观临床评估，而社交媒体内容提供了新的检测途径。
- Method: 采用Transformer网络提取视频空间特征和音频时间动态，通过多模态融合策略整合特征。
- Result: 在两个大规模数据集上表现优异，F1-Score分别提升13.92%和7.74%。
- Conclusion: MMFformer在多模态抑郁症检测中优于现有方法，代码已开源。


### [30] [Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography](https://arxiv.org/abs/2508.06703)
*Justin London*

Main category: cs.CV

TL;DR: 提出了一种基于点云和MRI数据的高效快速CGH生成框架，通过非凸傅里叶光学优化算法生成相位全息图和复全息图，并与深度学习方法HoloNet进行比较。

- Motivation: 计算机生成全息术（CGH）是一种有前景的方法，但需要高效快速的生成框架以优化性能。
- Method: 利用初始点云和MRI数据重建体积对象，通过交替投影、SGD和拟牛顿法生成相位全息图和复全息图。
- Result: 通过MSE、RMSE和PSNR评估算法性能，使用2D中值滤波优化后性能指标提升。
- Conclusion: 提出的框架在CGH生成中表现优异，2D中值滤波有效减少了噪声和伪影。


### [31] [Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video](https://arxiv.org/abs/2508.06715)
*Jixuan He,Chieh Hubert Lin,Lu Qi,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 论文提出了一种利用真实视频运动先验生成物理一致4D内容的方法Restage4D，通过视频重放训练策略和几何保持技术提升运动质量和几何一致性。

- Motivation: 现有生成模型难以捕捉物理真实性和运动动态性，而真实视频提供了物理基础的几何和运动线索，因此探索如何利用视频先验生成物理一致的4D内容。
- Method: 提出Restage4D，结合视频重放训练策略、遮挡感知刚性损失和去遮挡回溯机制，通过共享运动表示连接真实视频和合成视频。
- Result: 在DAVIS和PointOdyssey数据集上验证，显示几何一致性、运动质量和3D跟踪性能的提升。
- Conclusion: Restage4D不仅在新运动中保持可变形结构，还能自动纠正生成模型的错误，展示了视频先验在4D重演任务中的潜力。


### [32] [FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI](https://arxiv.org/abs/2508.06756)
*Somayeh Farahani,Marjaneh Hejazi,Antonio Di Ieva,Sidong Liu*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的非侵入性方法（FoundBioNet）用于预测胶质瘤IDH突变状态，通过多参数MRI数据实现高准确率。

- Motivation: 传统侵入性方法无法捕捉肿瘤空间异质性，且现有深度学习模型因标注数据稀缺而性能受限。
- Method: 采用SWIN-UNETR架构，结合Tumor-Aware Feature Encoding（TAFE）和Cross-Modality Differential（CMD）模块，从多参数MRI中提取特征。
- Result: 在多个独立测试集上AUC表现优异（最高90.58%），显著优于基线方法。
- Conclusion: FoundBioNet通过大规模预训练和任务微调，提高了胶质瘤诊断的准确性和可解释性，有望推动个性化治疗。


### [33] [VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions](https://arxiv.org/abs/2508.06757)
*Yash Garg,Saketh Bachu,Arindam Dutta,Rohit Lal,Sarosij Bose,Calvin-Khang Ta,M. Salman Asif,Amit Roy-Chowdhury*

Main category: cs.CV

TL;DR: 论文提出了一个名为VOccl3D的新数据集，用于解决现有3D人体姿态和形状估计方法在真实遮挡场景下的不足。

- Motivation: 现有方法在复杂姿态或严重遮挡场景下表现不佳，且现有数据集缺乏真实遮挡情况。
- Method: 通过计算机图形渲染技术构建VOccl3D数据集，并微调HPS方法（如CLIFF和BEDLAM-CLIFF）和YOLO11检测器。
- Result: 在多个公共数据集和自建测试集上，方法表现出显著改进。
- Conclusion: VOccl3D为未来研究提供了更真实的遮挡基准资源。


### [34] [SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding](https://arxiv.org/abs/2508.06763)
*Zihao Sheng,Zilin Huang,Yen-Jung Chen,Yansong Qu,Yuhao Luo,Yue Leng,Sikai Chen*

Main category: cs.CV

TL;DR: SafePLUG是一个多模态大语言模型框架，专注于交通事故的细粒度分析，支持像素级理解和时间定位。

- Motivation: 现有MLLM在交通事故理解中缺乏对细粒度视觉细节和局部场景的处理能力，限制了其在复杂场景中的应用。
- Method: 提出SafePLUG框架，支持任意形状的视觉提示和像素级分割，并识别时间锚定事件。
- Result: 实验表明，SafePLUG在区域问答、像素级分割、时间事件定位和事故理解等任务中表现优异。
- Conclusion: SafePLUG为复杂交通场景的细粒度理解奠定了基础，有望提升智能交通系统的安全性和情境感知。


### [35] [DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging](https://arxiv.org/abs/2508.06768)
*Noe Bertramo,Gabriel Duguey,Vivek Gopalakrishnan*

Main category: cs.CV

TL;DR: DiffUS是一种基于物理的可微分超声渲染器，通过模拟超声波的传播和反射，从MRI数据生成逼真的B超图像，用于术中引导。

- Motivation: 术中超声成像受噪声、伪影和对齐问题影响，DiffUS旨在弥合术前MRI/CT与术中超声的差距。
- Method: DiffUS将MRI 3D扫描转换为声阻抗体积，通过射线追踪模拟超声波传播，并构建稀疏线性系统捕捉多次反射，最终生成包含噪声和伪影的B超图像。
- Result: 在ReMIND数据集上验证，DiffUS能从脑部MRI数据生成解剖学准确的超声图像。
- Conclusion: DiffUS为术中引导提供了一种有效的工具，支持基于梯度的优化任务。


### [36] [Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling](https://arxiv.org/abs/2508.06805)
*Aarav Mehta,Priya Deshmukh,Vikram Singh,Siddharth Malhotra,Krishnan Menon Iyer,Tanvi Iyer*

Main category: cs.CV

TL;DR: 提出了一种针对医学图像的精确边缘检测方法，通过反向细化架构提升边界定位精度，显著改善医学成像任务。

- Motivation: 医学图像中器官边界的精确定位对分割、配准和放疗等任务至关重要，现有卷积网络在自然图像中表现良好，但在医学图像中缺乏毫米级精度。
- Method: 采用自上而下的反向细化架构，逐步融合高层语义特征与低层细节，并扩展设计以处理各向异性体积数据。
- Result: 在多个CT和MRI数据集上，边界定位精度显著优于基线方法和现有医学边缘检测技术，下游任务（如分割和配准）性能也得到提升。
- Conclusion: 该方法生成的精确器官边界对医学成像任务具有重要临床价值。


### [37] [DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation](https://arxiv.org/abs/2508.06816)
*Vikram Singh,Kabir Malhotra,Rohan Desai,Ananya Shankaracharya,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 提出了一种新型双分辨率架构，用于精确分割皮肤镜图像中的黑色素瘤，结合全分辨率流和多尺度上下文流，并通过边界感知连接和通道注意力模块优化结果。

- Motivation: 皮肤镜图像中黑色素瘤的精确分割对自动化癌症筛查和临床决策至关重要，但现有方法难以处理纹理、颜色变化及常见成像伪影。
- Method: 采用ResNet启发的双分辨率架构，结合全分辨率流和多尺度上下文流，引入边界感知残差连接和通道注意力模块，并加入轻量级伪影抑制块和多任务训练目标。
- Result: 在公开皮肤镜基准测试中显著提升了边界贴合度和临床相关分割指标，优于标准编码器-解码器基线。
- Conclusion: 该方法无需复杂后处理或预训练协议，为自动化黑色素瘤评估系统提供了实用解决方案。


### [38] [VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation](https://arxiv.org/abs/2508.06819)
*Ayaan Nooruddin Siddiqui,Mahnoor Zaidi,Ayesha Nazneen Shahbaz,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 提出了一种弱监督训练框架，用于皮下血管分割，利用稀疏标注生成密集监督，结合不确定性加权损失和拓扑感知正则化，显著减少标注负担并提升分割效果。

- Motivation: 临床图像中皮下血管分割因标注稀缺、成本高以及低对比度和噪声问题而困难，需要一种高效且低成本的解决方案。
- Method: 利用稀疏标注（如中心线、点标记或短涂鸦）通过可微随机游走标签传播模型生成密集概率监督，结合图像驱动的血管特征和管状连续性先验，同时联合训练CNN分割预测器。
- Result: 在临床皮下成像数据集上，该方法优于稀疏标签的简单训练和传统密集伪标注，生成更完整的血管图并提供更可靠的不确定性估计。
- Conclusion: 该方法显著减少了标注负担，同时保留了临床相关的血管拓扑结构，提升了分割的准确性和实用性。


### [39] [Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification](https://arxiv.org/abs/2508.06831)
*Taha Mustapha Nehdi,Nairouz Mrabah,Atif Belal,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: SAGE-reID是一种高效、无需源数据的多源域自适应方法，通过低秩适配器和轻量级门控网络实现跨域知识迁移，显著减少计算成本和内存消耗。

- Motivation: 解决多源域自适应（MSDA）方法中训练参数和计算成本高的问题，同时保持高准确性和鲁棒性。
- Method: 首先训练源特定的低秩适配器（LoRA），然后通过轻量级门控网络动态分配融合权重。
- Result: 在Market-1501、DukeMTMC-reID和MSMT17等基准测试中表现优于现有方法，且计算高效。
- Conclusion: SAGE-reID是一种高效、轻量级的MSDA方法，适用于行人重识别任务。


### [40] [Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology](https://arxiv.org/abs/2508.06845)
*Hamidreza Samadi,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.CV

TL;DR: 该研究提出了一种结合高分辨率3D扫描和混合机器学习框架的方法，用于预测制造组件的几何偏差，显著提升了预测精度。

- Motivation: 现代制造中，复杂几何形状的尺寸精度难以保持，需要更准确的预测方法。
- Method: 使用高分辨率3D扫描仪采集多角度表面数据，通过精确对齐、降噪和合并技术生成3D模型，结合卷积神经网络和梯度提升决策树进行预测建模。
- Result: 预测精度达到0.012毫米（95%置信水平），比传统方法提升73%，并揭示了制造参数与几何偏差的隐藏关联。
- Conclusion: 该方法在自动化质量控制、预测性维护和设计优化方面具有潜力，并为未来研究提供了高质量数据集。


### [41] [AGIC: Attention-Guided Image Captioning to Improve Caption Relevance](https://arxiv.org/abs/2508.06853)
*L. D. M. S. Sai Teja,Ashok Urlana,Pruthwik Mishra*

Main category: cs.CV

TL;DR: AGIC通过注意力机制增强视觉特征，结合混合解码策略提升图像描述生成效果，实验表明其性能优于现有模型且推理更快。

- Motivation: 尽管图像描述生成取得进展，但生成准确且描述性强的标题仍是挑战。
- Method: 提出AGIC，通过注意力机制增强视觉特征，并采用混合解码策略平衡流畅性与多样性。
- Result: 在Flickr8k和Flickr30k数据集上，AGIC性能优于多个先进模型，推理速度更快。
- Conclusion: AGIC为图像描述生成提供了可扩展且可解释的解决方案。


### [42] [A Joint Sparse Self-Representation Learning Method for Multiview Clustering](https://arxiv.org/abs/2508.06857)
*Mengxue Jia,Zhihua Allen-Zhao,You Zhao,Sanyang Liu*

Main category: cs.CV

TL;DR: 提出了一种新的联合稀疏自表示学习模型，用于多视图聚类，通过引入基数约束提取视图特定局部信息，并开发了具有全局收敛性的交替二次惩罚方法。

- Motivation: 多视图聚类（MC）旨在利用不同视图的一致性和互补信息对样本进行分组，而子空间聚类作为MC的基础技术受到广泛关注。现有方法通常使用图拉普拉斯正则化，但本文提出基数约束以更直接地提取局部和全局结构信息。
- Method: 提出了一种联合稀疏自表示学习模型，使用基数约束（ℓ0-范数）替代图拉普拉斯正则化，提取视图特定局部信息。低秩约束用于揭示共识亲和矩阵中的全局一致结构。为解决非凸、非光滑模型的收敛问题，开发了交替二次惩罚（AQP）方法。
- Result: 在六个标准数据集上的实验表明，所提模型和AQP方法优于八种最先进算法。
- Conclusion: 本文提出的模型和方法在多视图聚类中表现出色，基数约束和AQP方法的结合显著提升了性能。


### [43] [VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding](https://arxiv.org/abs/2508.06869)
*Jianxiang He,Shaoguang Wang,Weiyu Guo,Meisheng Hong,Jungang Li,Yijie Xu,Ziyang Chen,Hui Xiong*

Main category: cs.CV

TL;DR: 提出了一种名为VSI的多模态关键帧搜索方法，通过结合字幕和时间戳，显著提升了长视频理解任务的性能。

- Motivation: 长视频理解因数据规模庞大而具有挑战性，现有方法因多模态对齐不足和复杂时间语义信息缺失而效果不佳。
- Method: VSI通过双流搜索机制（视频搜索流和字幕匹配流）整合视觉和文本信息，提升关键帧搜索精度。
- Result: 在LongVideoBench上，VSI的关键帧定位准确率达40.00%，视频问答任务准确率达68.48%，均显著超越基线方法。
- Conclusion: VSI在长视频问答任务中达到SOTA性能，证明了其多模态搜索策略的鲁棒性和泛化能力。


### [44] [NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective](https://arxiv.org/abs/2508.06878)
*Maoxun Yuan,Duanni Meng,Ziteng Xi,Tianyi Zhao,Shiji Zhao,Yimian Dai,Xingxing Wei*

Main category: cs.CV

TL;DR: 提出了一种新型噪声抑制特征金字塔网络（NS-FPN），通过低频引导特征净化（LFP）和螺旋感知特征采样（SFS）模块，显著减少红外小目标检测中的误报问题。

- Motivation: 红外小目标检测因目标暗淡、形状模糊及背景干扰而极具挑战性，现有CNN方法虽有效但易增加误报。
- Method: 结合LFP模块抑制噪声特征，SFS模块螺旋采样融合目标相关特征，构建轻量高效的NS-FPN。
- Result: 在公开数据集上验证，NS-FPN显著降低误报率，性能优越。
- Conclusion: NS-FPN为红外小目标检测提供了一种轻量且高效的解决方案。


### [45] [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/abs/2508.06895)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: 论文提出BASIC方法，通过直接监督视觉嵌入提升多模态大语言模型的性能。

- Motivation: 当前方法仅将视觉嵌入作为上下文线索，缺乏直接视觉监督，限制了视觉嵌入的精细对齐。
- Method: BASIC利用LLM浅层中优化的视觉嵌入作为监督，从嵌入方向和语义匹配两方面指导视觉投影器。
- Result: BASIC显著提升了MLLMs在多个基准测试中的性能。
- Conclusion: 直接视觉监督是提升视觉嵌入对齐的有效方法。


### [46] [Advancements in Chinese font generation since deep learning era: A survey](https://arxiv.org/abs/2508.06900)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的汉字字体生成方法，分为多样本和少样本生成两类，并探讨了挑战与未来方向。

- Motivation: 汉字字体生成是字体设计师和排版师关注的重要课题，但如何提高生成质量仍具挑战性。
- Method: 通过文献选择与分析，分类总结了多样本和少样本字体生成方法，并讨论其优缺点。
- Result: 综述了现有方法的代表性技术和局限性。
- Conclusion: 总结了当前挑战，并提出了未来研究方向，为该领域研究者提供参考。


### [47] [eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos](https://arxiv.org/abs/2508.06902)
*Xuecheng Wu,Dingkang Yang,Danlei Huang,Xinyi Yin,Yifan Wang,Jia Zhang,Jiayu Nie,Liangyu Fu,Yang Liu,Junxiao Xue,Hadi Amirpour,Wei Zhou*

Main category: cs.CV

TL;DR: 论文提出eMotions数据集和AV-CANet网络，用于短视频情感分析，解决了数据稀缺和模态复杂性挑战。

- Motivation: 短视频情感分析面临数据稀缺和模态复杂性挑战，需新方法解决。
- Method: 提出eMotions数据集和多阶段标注流程，设计AV-CANet网络及Local-Global Fusion Module和EP-CE Loss。
- Result: AV-CANet在多个数据集上表现优异，为未来研究提供参考。
- Conclusion: AV-CANet有效解决短视频情感分析问题，数据集和代码将开源。


### [48] [A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2508.06904)
*Chao Yin,Jide Li,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 提出了一种无需训练的实例感知提示框架（IAPF），通过多模态大语言模型和实例级掩码生成，显著提升了伪装物体分割的性能。

- Motivation: 现有训练免费的伪装物体分割方法依赖语义级提示，导致在多实例场景下效果不佳。
- Method: IAPF包含三个步骤：文本提示生成、实例掩码生成和自一致性投票。
- Result: 在标准基准测试中，IAPF显著优于现有方法。
- Conclusion: IAPF为伪装物体分割提供了一种高效且无需训练的解决方案。


### [49] [MultiRef: Controllable Image Generation with Multiple Visual References](https://arxiv.org/abs/2508.06905)
*Ruoxi Chen,Dongping Chen,Siyuan Wu,Sinan Wang,Shiyun Lang,Petr Sushko,Gaoyang Jiang,Yao Wan,Ranjay Krishna*

Main category: cs.CV

TL;DR: 论文提出了一种基于多视觉参考的可控图像生成任务，并引入MultiRef-bench评估框架和MultiRef数据集，发现现有模型在多参考条件下表现不佳。

- Motivation: 当前图像生成框架主要依赖单源输入，而设计师通常需要从多视觉参考中获取灵感，因此需要开发更灵活的生成工具。
- Method: 通过RefBlend数据引擎生成990合成样本和1,000真实样本，构建MultiRef数据集（38k图像），并在多个模型上进行实验评估。
- Result: 最佳模型OmniGen在合成和真实样本上的平均表现分别为66.6%和79.0%，表明现有系统在多参考条件下仍有挑战。
- Conclusion: 研究为开发更灵活的创意工具提供了方向，MultiRef数据集将促进进一步研究。


### [50] [MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification](https://arxiv.org/abs/2508.06908)
*Jinhao Li,Zijian Chen,Lirong Deng,Changbo Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文提出首个多任务多模态行人重识别基准MMReID-Bench，利用多模态大语言模型（MLLMs）解决传统模型在多模态数据泛化能力差的问题。

- Motivation: 传统行人重识别模型在多模态数据（如RGB、热成像、红外、文本等）中泛化能力差，而现有方法未充分利用MLLMs的推理和跨模态理解能力。
- Method: 引入MMReID-Bench基准，包含20,710个多模态查询和库图像，覆盖10种行人重识别任务，评估MLLMs的性能。
- Result: 实验表明MLLMs在行人重识别中表现优异，但在处理热成像和红外数据时仍有局限性。
- Conclusion: MMReID-Bench有望推动开发更鲁棒、通用的多模态基础模型。


### [51] [Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing](https://arxiv.org/abs/2508.06916)
*Shichao Ma,Yunhe Guo,Jiahao Su,Qihe Huang,Zhengyang Zhou,Yang Wang*

Main category: cs.CV

TL;DR: Talk2Image是一个多代理系统，用于多轮对话场景中的交互式图像生成和编辑，解决了现有单代理系统的意图漂移和不连贯编辑问题。

- Motivation: 现有文本到图像生成任务多关注单轮场景，难以处理多轮迭代任务，且单代理系统易导致意图漂移和不连贯编辑。
- Method: Talk2Image整合了三个关键组件：对话历史的意图解析、任务分解与专业代理协作执行、基于多视角评估的反馈驱动优化。
- Result: 实验表明，Talk2Image在可控性、连贯性和用户满意度上优于现有基线。
- Conclusion: Talk2Image通过多代理协作和反馈机制，实现了用户意图的逐步对齐和一致的图像编辑。


### [52] [AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](https://arxiv.org/abs/2508.06924)
*Shihao Yuan,Yahui Liu,Yang Yue,Jingyuan Zhang,Wangmeng Zuo,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CV

TL;DR: 论文提出AR-GRPO方法，通过在线强化学习优化自回归图像生成模型，显著提升生成图像的质量和人类偏好。

- Motivation: 受强化学习在大型语言模型优化中的成功启发，研究如何将其应用于自回归图像生成模型，以提升生成图像的多维度质量。
- Method: 采用Group Relative Policy Optimization (GRPO)算法，设计多维度奖励函数（感知质量、真实性和语义保真度）优化自回归模型。
- Result: 在类别条件和文本条件的图像生成任务中，RL增强框架显著优于标准自回归基线，各项评估指标均有提升。
- Conclusion: 研究表明基于强化学习的优化方法适用于自回归图像生成，为可控高质量图像合成开辟了新途径。


### [53] [CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing](https://arxiv.org/abs/2508.06937)
*Weiyan Xie,Han Gao,Didan Deng,Kaican Li,April Hua Liu,Yongxiang Huang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: CannyEdit是一种无需训练的文本到图像编辑框架，通过选择性Canny控制和双提示引导，在编辑区域实现文本驱动编辑，同时保持未编辑区域的上下文完整性。

- Motivation: 现有方法难以平衡编辑区域的文本依从性、未编辑区域的上下文保真度以及编辑的无缝集成。
- Method: 采用选择性Canny控制（保留未编辑区域细节）和双提示引导（结合局部和全局提示）。
- Result: 在真实图像编辑任务中，CannyEdit在文本依从性和上下文保真度上优于现有方法，用户研究显示其编辑结果更自然。
- Conclusion: CannyEdit通过创新方法解决了现有挑战，实现了更高质量的图像编辑。


### [54] [SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work](https://arxiv.org/abs/2508.06951)
*Harry Walsh,Ed Fish,Ozge Mercanoglu Sincan,Mohamed Ilyes Lakhal,Richard Bowden,Neil Fox,Bencie Woll,Kepeng Wu,Zecheng Li,Weichao Zhao,Haodong Wang,Wengang Zhou,Houqiang Li,Shengeng Tang,Jiayi He,Xu Wang,Ruobei Zhang,Yaxiong Wang,Lechao Cheng,Meryem Tasyurek,Tugce Kiziltepe,Hacer Yalim Keles*

Main category: cs.CV

TL;DR: 论文介绍了首个手语生成挑战赛，旨在通过标准化评估指标比较不同系统的性能，并提出了获胜方法。

- Motivation: 由于缺乏标准化评估指标，手语生成领域难以进行系统间的有效比较。
- Method: 通过举办挑战赛，使用RWTH-PHOENIX-Weather-2014T数据集和自定义测试集，评估文本到姿态（T2P）翻译架构。
- Result: 33名参与者提交了231个解决方案，最佳团队BLEU-1得分为31.40，DTW-MJE为0.0574。获胜方法采用检索框架和预训练语言模型。
- Conclusion: 挑战赛为手语生成领域提供了标准化评估网络和基线，促进了未来研究的比较。


### [55] [Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification](https://arxiv.org/abs/2508.06959)
*Qin Xu,Lili Zhu,Xiaoxia Cheng,Bo Jiang*

Main category: cs.CV

TL;DR: SCOPE方法通过自适应增强空间域中的细节和语义表示，解决了频率域方法固定基函数的局限性，在细粒度视觉分类中取得了新突破。

- Motivation: 细粒度视觉分类需要捕捉细微的视觉特征，而现有频率域方法缺乏对图像内容的适应性和动态调整能力。
- Method: 提出了SCOPE方法，包含Subtle Detail Extractor（SDE）和Salient Semantic Refiner（SSR）模块，逐步结合局部细节和全局语义。
- Result: 在四个流行的细粒度图像分类基准测试中取得了新的最佳性能。
- Conclusion: SCOPE通过自适应多尺度融合和动态特征提取，显著提升了细粒度分类的灵活性及性能。


### [56] [Adversarial Video Promotion Against Text-to-Video Retrieval](https://arxiv.org/abs/2508.06964)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Qian Li,Shuai Liu,Chao Shen*

Main category: cs.CV

TL;DR: 本文提出了首个针对文本到视频检索（T2VR）的视频推广攻击（ViPro），并提出了模态细化（MoRe）方法以提高黑盒迁移性。实验表明ViPro在白盒、灰盒和黑盒设置下均优于基线方法。

- Motivation: 现有T2VR攻击主要关注降低视频排名，而提升视频排名的攻击尚未充分研究，这种攻击可能带来更大的经济利益和（错误）信息传播风险。
- Method: 提出ViPro攻击方法，通过模态细化（MoRe）捕捉视觉和文本模态间的细粒度交互，增强黑盒迁移性。
- Result: ViPro在白盒、灰盒和黑盒设置下分别超越基线方法30%、10%和4%。实验覆盖3种T2VR模型、3个数据集和3种场景。
- Conclusion: 本文揭示了T2VR的潜在漏洞，分析了攻击的上下限，并提供了防御建议。代码将公开。


### [57] [Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View](https://arxiv.org/abs/2508.06968)
*Ulas Gunes,Matias Turkulainen,Juho Kannala,Esa Rahtu*

Main category: cs.CV

TL;DR: 本文首次评估了基于鱼眼镜头的3D高斯溅射方法Fisheye-GS和3DGUT在真实图像上的表现，研究了它们在超过180度视场下的性能。

- Motivation: 研究鱼眼镜头在极端畸变下的3D重建效果，并解决传统SfM初始化在强畸变下失效的问题。
- Method: 通过不同视场（200度、160度、120度）评估性能，并提出基于UniK3D预测的深度初始化策略。
- Result: Fisheye-GS在160度视场下表现最佳，而3DGUT在200度视场下仍保持高质量；UniK3D策略在困难场景中表现优异。
- Conclusion: 鱼眼3DGS方法在稀疏且畸变严重的图像输入下具有实际应用潜力。


### [58] [WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering](https://arxiv.org/abs/2508.06982)
*Yixin Zhu,Zuoliang Zhu,Miloš Hašan,Jian Yang,Jin Xie,Beibei Wang*

Main category: cs.CV

TL;DR: WeatherDiffusion是一个基于扩散模型的框架，用于自动驾驶场景中的正向和逆向渲染，支持通过文本描述控制天气和光照编辑。

- Motivation: 复杂天气和光照条件对自动驾驶场景的理解和重建提出了挑战，现有扩散模型难以控制且缺乏鲁棒性。
- Method: 提出WeatherDiffusion框架，结合Intrinsic map-aware attention (MAA)和合成/真实数据集（WeatherSynthetic和WeatherReal）。
- Result: 在多个基准测试中优于现有方法，并提升下游任务（如目标检测和图像分割）的鲁棒性。
- Conclusion: WeatherDiffusion为自动驾驶场景的渲染和编辑提供了高效且可控的解决方案。


### [59] [TADoc: Robust Time-Aware Document Image Dewarping](https://arxiv.org/abs/2508.06988)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Yu Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种动态建模的文档图像去扭曲方法TADoc，并设计了新评估指标DLS，实验验证了其优越性。

- Motivation: 现有方法难以处理复杂文档结构和高变形度，作者认为去扭曲是一个渐进过程而非一步转换。
- Method: 将任务重新建模为动态过程，设计轻量级框架TADoc，并提出新评估指标DLS。
- Result: 实验表明TADoc在多种文档类型和变形度下表现优越。
- Conclusion: TADoc通过动态建模和新指标DLS，显著提升了文档去扭曲的效果和评估全面性。


### [60] [OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware](https://arxiv.org/abs/2508.06993)
*Nick Lemke,John Kalkhof,Niklas Babendererde,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: 论文提出OctreeNCA，通过八叉树数据结构扩展NCA的邻域定义，解决高分辨率图像和视频分割中的VRAM瓶颈问题。

- Motivation: 医学图像分割需要处理大尺寸输入，但现有模型（如UNet或Vision Transformer）因VRAM消耗高而无法一次性处理，导致全局一致性和推理速度受限。
- Method: 提出OctreeNCA，利用八叉树扩展NCA的邻域定义，实现全局知识的高效传递，并开发CUDA实现的NCA推理函数以减少VRAM占用。
- Result: OctreeNCA在高分辨率图像和视频分割中占用VRAM比UNet少90%，可一次性处理184兆像素病理切片或1分钟手术视频。
- Conclusion: OctreeNCA通过改进NCA的邻域定义和优化实现，显著降低了VRAM需求，提升了高分辨率医学图像分割的效率。


### [61] [S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision](https://arxiv.org/abs/2508.06995)
*Huihui Xu,Jin Ye,Hongqiu Wang,Changkai Ji,Jiashi Lin,Ming Hu,Ziyan Huang,Ying Chen,Chenglong Ma,Tianbin Li,Lihao Liu,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种快速伪掩码算法UniAP和自监督通用分割模型S2-UniSeg，解决了现有方法多阶段预训练和耗时伪掩码生成的问题，显著提升了性能。

- Motivation: 现有自监督图像分割模型的多阶段预训练和耗时伪掩码生成过程难以扩展且优化不连续，限制了性能。
- Method: 提出快速伪掩码算法UniAP和自监督模型S2-UniSeg，结合QuerySD任务进行连续预训练。
- Result: S2-UniSeg在多个基准测试中显著优于SOTA模型，如COCO上AP提升6.9。
- Conclusion: UniAP和S2-UniSeg有效解决了现有方法的局限性，并在大规模数据集上表现出色。


### [62] [HiMat: DiT-based Ultra-High Resolution SVBRDF Generation](https://arxiv.org/abs/2508.07011)
*Zixiong Wang,Jian Yang,Yiwei Hu,Milos Hasan,Beibei Wang*

Main category: cs.CV

TL;DR: HiMat是一个高效扩散框架，用于生成4K分辨率的SVBRDF贴图，通过轻量级CrossStitch模块解决多贴图一致性问题。

- Motivation: 高分辨率文本到图像生成模型的兴起为SVBRDF生成提供了机会，但如何高效生成多贴图并保持一致性仍具挑战性。
- Method: HiMat框架基于扩散模型，引入CrossStitch模块捕获贴图间依赖关系，保持DiT主干不变。
- Result: 实验证明HiMat能生成结构一致且高频细节丰富的4K SVBRDF贴图，并可推广至本征分解等任务。
- Conclusion: HiMat通过轻量级设计高效生成高质量SVBRDF贴图，为3D内容创作提供了新工具。


### [63] [TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders](https://arxiv.org/abs/2508.07020)
*Tanjim Bin Faruk,Abdul Matin,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: TerraMAE是一种新型的高光谱图像编码框架，通过自适应通道分组和增强的重建损失函数，有效捕捉空间-光谱相关性，适用于多种地理空间分析任务。

- Motivation: 高光谱卫星图像包含数百个连续波段，但现有自监督方法难以充分利用其复杂的空间-光谱相关性。
- Method: 提出TerraMAE框架，采用自适应通道分组策略和增强的重建损失函数，优化空间-光谱嵌入学习。
- Result: TerraMAE在高保真图像重建中表现出色，并在作物识别、土地覆盖分类和土壤质地预测等任务中验证了其有效性。
- Conclusion: TerraMAE为高光谱图像分析提供了高效且通用的解决方案。


### [64] [DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents](https://arxiv.org/abs/2508.07021)
*Kun Qian,Wenjie Li,Tianyu Sun,Wenhong Wang,Wenhan Luo*

Main category: cs.CV

TL;DR: DocRefine是一个基于多智能体系统的框架，用于科学PDF文档的智能理解、内容优化和自动摘要，显著优于现有方法。

- Motivation: 科学文献的快速增长需要高效准确的文档处理工具，传统方法和现有大模型在复杂布局和多模态内容处理上存在不足。
- Method: DocRefine采用多智能体系统，包含六个协作代理，结合高级LVLMs（如GPT-4o），实现闭环反馈架构。
- Result: 在DocEditBench数据集上，DocRefine在语义一致性、布局保真度和指令遵循率上分别达到86.7%、93.9%和85.0%。
- Conclusion: DocRefine在复杂多模态文档编辑中表现出色，是科学文档自动化处理的重要进展。


### [65] [MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering](https://arxiv.org/abs/2508.07023)
*Jingwei Peng,Jiehao Chen,Mateo Alejandro Rojas,Meilin Zhang*

Main category: cs.CV

TL;DR: MV-CoRe模型通过深度融合多模态视觉和语言信息，显著提升了复杂视觉问答任务的性能。

- Motivation: 现有大型视觉语言模型依赖高层全局特征，难以应对复杂视觉问答任务的多模态推理和外部知识整合需求。
- Method: MV-CoRe整合了预训练视觉和语言模型的全局嵌入与细粒度语义视觉特征，并通过多模态融合Transformer实现跨模态注意力。
- Result: 在GQA、A-OKVQA和OKVQA等基准测试中，MV-CoRe表现优于现有模型，GQA准确率达77.5%。
- Conclusion: MV-CoRe通过对象和场景图特征的深度融合，展现出强大的视觉和概念理解能力。


### [66] [Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation](https://arxiv.org/abs/2508.07028)
*Juntong Fan,Shuyi Fan,Debesh Jha,Changsheng Fang,Tieyong Zeng,Hengyong Yu,Dayang Wang*

Main category: cs.CV

TL;DR: FOCUS-Med提出了一种结合空间和结构图的注意力感知内窥镜息肉分割方法，通过双图卷积网络和自注意力机制提升分割效果，并在公开基准测试中表现优异。

- Motivation: 内窥镜图像中息肉分割对早期结直肠癌检测至关重要，但低对比度、高光和模糊边界使其具有挑战性。
- Method: FOCUS-Med融合双图卷积网络（Dual-GCN）捕捉空间和拓扑结构依赖，结合自注意力机制增强全局上下文，并采用加权快速归一化融合策略。
- Result: 在公开基准测试中，FOCUS-Med在五项关键指标上达到最先进性能。
- Conclusion: FOCUS-Med展示了AI辅助结肠镜检查的临床潜力，尤其在复杂形状和边界的分割上表现突出。


### [67] [TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree](https://arxiv.org/abs/2508.07083)
*Yueyu Hu,Ran Gong,Tingyu Fan,Yao Wang*

Main category: cs.CV

TL;DR: 提出了一种名为TeSO的新型3D表示方法，结合了八叉树结构和纹理贴图，解决了现有3D表示在渲染质量、表面定义和压缩性上的局限性。

- Motivation: 现有3D表示（如点云、网格和3D高斯）在渲染质量、表面定义和压缩性方面存在不足，需要一种更高效的解决方案。
- Method: 基于点云构建TeSO，将场景表示为八叉树组织的立方体边界曲面元，每个曲面元关联纹理贴图，并通过压缩方案高效编码几何和纹理。
- Result: TeSO在低比特率下实现了比点云和3D高斯基线更高的渲染质量。
- Conclusion: TeSO是一种高效且高质量的3D表示方法，适用于3D流媒体和AR/VR应用。


### [68] [ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting](https://arxiv.org/abs/2508.07089)
*Sandro Papais,Letian Wang,Brian Cheong,Steven L. Waslander*

Main category: cs.CV

TL;DR: ForeSight是一种新型的联合检测与预测框架，用于自动驾驶中的3D感知，通过多任务流式学习和双向学习提升性能。

- Motivation: 传统方法将检测与预测作为独立任务，无法充分利用时间线索，ForeSight旨在解决这一问题。
- Method: 采用多任务流式学习和双向学习，结合检测与预测的查询记忆，使用预测感知检测变换器和流式预测变换器提升性能。
- Result: 在nuScenes数据集上，EPA达到54.9%，优于之前方法9.3%，同时在多视图检测与预测模型中表现最佳。
- Conclusion: ForeSight通过联合检测与预测，无需显式目标关联，显著提升了性能，适用于多帧序列。


### [69] [Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration](https://arxiv.org/abs/2508.07092)
*Yue Hu,Juntong Peng,Yunqiao Yang,Siheng Chen*

Main category: cs.CV

TL;DR: 提出了一种名为HyComm的通信高效LiDAR协作3D检测系统，通过自适应整合两种通信消息（感知输出和原始观测）来优化性能与带宽的权衡。

- Motivation: 协作3D检测通过信息交换提升性能，但面临性能与通信带宽的权衡问题。
- Method: 提出HyComm系统，自适应整合紧凑的感知输出和丰富的原始观测，并优先处理关键数据。
- Result: 在DAIR-V2X和OPV2V数据集上表现优异，通信量降低2006倍以上，性能优于Where2comm。
- Conclusion: HyComm通过自适应消息选择和标准化格式，实现了高效的性能与带宽权衡，适用于多种通信场景。


### [70] [AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation](https://arxiv.org/abs/2508.07112)
*Nikolai Warner,Wenjin Zhang,Irfan Essa,Apaar Sadhwani*

Main category: cs.CV

TL;DR: AugLift通过增强2D关键点输入（加入置信度和深度估计）提升3D人体姿态估计的泛化能力，无需额外数据或传感器。

- Motivation: 现有基于提升的方法在跨数据集和真实场景中泛化能力差，需改进。
- Method: AugLift在标准2D关键点坐标基础上加入置信度和深度估计，利用预训练模型计算这些信号。
- Result: 实验显示AugLift在未见数据集上平均提升10.1%性能，内部数据集提升4.0%。
- Conclusion: AugLift是一种模块化改进方法，显著提升提升式姿态估计模型的泛化能力。


### [71] [Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays](https://arxiv.org/abs/2508.07128)
*Gregory Schuit,Denis Parra,Cecilia Besa*

Main category: cs.CV

TL;DR: 论文评估了生成对抗网络（GANs）和扩散模型（DMs）在合成胸部X光片中的表现，发现DMs整体更真实，而GANs在某些条件下更准确。

- Motivation: 解决医学影像数据稀缺问题，尤其是低发病率异常，以提高AI诊断工具的性能。
- Method: 使用MIMIC-CXR数据集中的真实图像和GANs、DMs生成的合成图像，进行放射科医生的读者研究。
- Result: DMs生成的图像整体更真实，但GANs在特定条件下（如无ECS）更准确。
- Conclusion: GANs和DMs各有优势，需进一步优化以可靠地增强AI诊断系统的训练数据。


### [72] [CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance](https://arxiv.org/abs/2508.07140)
*Yingtie Lei,Fanghai Yi,Yihang Dong,Weihuang Liu,Xiaofeng Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: 论文提出CMAMRNet，一种基于上下文掩码感知的壁画修复网络，通过多尺度特征提取和掩码引导解决现有方法在修复质量上的不足。

- Motivation: 壁画作为文化遗产易受环境和人为因素破坏，现有学习方法在修复时难以保持掩码一致性，导致修复质量不佳。
- Method: 提出CMAMRNet，包含掩码感知上下采样器（MAUDS）和共特征聚合器（CFA），分别用于多尺度掩码引导和特征提取。
- Result: 实验表明CMAMRNet优于现有方法，能有效保留壁画的结构和艺术细节。
- Conclusion: CMAMRNet通过掩码感知和多尺度特征提取，显著提升了壁画修复的质量和真实性。


### [73] [Dynamic Pattern Alignment Learning for Pretraining Lightweight Human-Centric Vision Models](https://arxiv.org/abs/2508.07144)
*Xuanhan Wang,Huimin Deng,Ke Liu,Jun Wang,Lianli Gao,Jingkuan Song*

Main category: cs.CV

TL;DR: 论文提出了一种名为DPAL的蒸馏预训练框架，通过动态模式对齐学习，使轻量级HVMs从大型HVMs中学习典型视觉模式，从而提升泛化能力。

- Motivation: 现有HVMs依赖大型神经网络和受限的预训练数据，限制了实际应用。
- Method: 设计了动态模式解码器（D-PaDe）和三层次对齐目标，以全局、局部和实例关系级别对齐轻量级与大型HVMs。
- Result: 实验表明，DPAL在15个数据集上表现优异，轻量级模型（5M参数）达到与大型模型（84M/307M）相似的泛化能力。
- Conclusion: DPAL框架有效提升了轻量级HVMs的泛化能力，适用于多种人本视觉任务。


### [74] [Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.07146)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的行人轨迹预测框架，结合短期和长期运动意图，通过自适应指导和残差噪声预测器提升预测精度。

- Motivation: 现有扩散模型缺乏对行人意图的显式语义建模，可能导致行为误解和预测准确性下降。
- Method: 使用残差极坐标表示建模短期意图，通过可学习的基于令牌的终点预测器估计长期意图，并结合自适应指导和残差噪声预测器优化扩散过程。
- Result: 在ETH、UCY和SDD基准测试中表现优异，优于现有方法。
- Conclusion: 该框架通过显式建模意图和优化扩散过程，显著提升了行人轨迹预测的准确性。


### [75] [SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.07149)
*Ruolin Yang,Da Li,Honggang Zhang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 提出了一种名为SketchAnimator的草图动画模型，通过三个阶段（外观学习、运动学习和视频先验蒸馏）将静态草图转化为动态视频，保留草图外观并模仿参考视频的动态运动。

- Motivation: 草图动画通常需要专业技能且耗时，对业余用户不友好，因此需要一种简化此过程的方法。
- Method: 分为三个阶段：1. 外观学习（LoRA整合草图外观信息）；2. 运动学习（从参考视频提取动态）；3. 视频先验蒸馏（SDS更新贝塞尔曲线参数）。
- Result: 模型生成的草图视频既保留了原始外观，又模仿了参考视频的动态运动，优于其他方法。
- Conclusion: SketchAnimator在一次性运动定制挑战中表现优异，为草图动画提供了高效解决方案。


### [76] [CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion](https://arxiv.org/abs/2508.07162)
*Xiaotong Lin,Tianming Liang,Jian-Fang Hu,Kun-Yu Lin,Yulei Kang,Chunwei Tian,Jianhuang Lai,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: CoopDiff提出了一种解耦扩散框架，分别建模人和物体的运动，通过接触点实现一致性，显著提升了3D人-物交互预测性能。

- Motivation: 现有方法通常用一个模型同时预测人和物体的运动，忽略了二者运动模式的差异。
- Method: 采用两个分支分别建模人和物体的运动，通过共享接触点实现一致性，并引入人驱动的交互模块。
- Result: 在BEHAVE和Human-object Interaction数据集上表现优于现有方法。
- Conclusion: 解耦建模和接触点一致性约束有效提升了预测的准确性和一致性。


### [77] [Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection](https://arxiv.org/abs/2508.07170)
*Yunpeng Shi,Lei Chen,Xiaolu Shen,Yanju Guo*

Main category: cs.CV

TL;DR: 提出了一种轻量级多尺度特征提取层（LMF层），并基于此构建了LMFNet网络，显著减少了参数量，同时在显著目标检测任务中保持了竞争力。

- Motivation: 解决轻量级网络中多尺度特征提取的效率与性能之间的权衡问题。
- Method: 采用深度可分离扩张卷积构建LMF层，并集成多个LMF层形成LMFNet。
- Result: 在五个基准数据集上达到或接近最优性能，仅需0.81M参数，优于传统和轻量级模型。
- Conclusion: LMFNet不仅解决了轻量级网络的多尺度学习问题，还展示了在图像处理任务中的广泛应用潜力。


### [78] [EventRR: Event Referential Reasoning for Referring Video Object Segmentation](https://arxiv.org/abs/2508.07171)
*Huihui Xu,Jiashi Lin,Haoyu Chen,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: 论文提出EventRR框架，通过解耦视频对象分割为对象总结和引用推理两部分，利用语义事件图（REG）提升视频引用表达的结构化推理能力。

- Motivation: 现有RVOS方法忽视引用表达的语义结构，而视频引用表达比图像引用更复杂，包含事件属性和时间关系，传统方法难以处理。
- Method: EventRR框架分为对象总结和引用推理两部分：总结阶段通过瓶颈令牌聚合全局跨模态时间上下文；推理阶段构建REG图，通过TCRR方法从叶节点到根节点累积引用分数。
- Result: 在四个基准数据集上，EventRR在定量和定性上均优于现有RVOS方法。
- Conclusion: EventRR通过结构化语义事件图和分步推理，显著提升了视频引用对象分割的性能。


### [79] [Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset](https://arxiv.org/abs/2508.07211)
*Junyi He,Liuling Chen,Hongyang Zhou,Zhang xiaoxing,Xiaobin Zhu,Shengxiang Yu,Jingyan Qin,Xu-Cheng Yin*

Main category: cs.CV

TL;DR: 论文提出了一种深度引导网络（DGN）用于图像恢复，结合新的大规模高分辨率数据集，解决了现有方法忽视深度信息的问题。

- Motivation: 现有图像恢复方法常忽略深度信息，导致相似性匹配不佳、浅景深场景注意力分散及深景深场景背景过度增强。
- Method: DGN包含两个交互分支：深度估计分支提供结构引导，图像恢复分支通过渐进窗口自注意力和稀疏非局部注意力实现恢复。
- Result: 实验表明，该方法在多个标准基准上达到最优性能，并能泛化到未见过的植物图像。
- Conclusion: DGN通过深度引导和联合训练提升了图像恢复质量，同时新数据集支持了方法的有效性和鲁棒性。


### [80] [Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling](https://arxiv.org/abs/2508.07214)
*Hongyang Zhou,Xiaobin Zhu,Liuling Chen,Junyi He,Jingyan Qin,Xu-Cheng Yin,Zhang xiaoxing*

Main category: cs.CV

TL;DR: 提出了一种基于校正流的无监督真实世界超分辨率方法，通过建模连续可逆的退化轨迹和傅里叶先验指导，生成更真实的低分辨率图像，显著提升现有超分辨率方法的性能。

- Motivation: 真实世界超分辨率面临复杂未知的退化分布，现有方法因域差距难以泛化。
- Method: 提出Rectified Flow Degradation Module（RFDM）和Fourier Prior Guided Degradation Module（FGDM），建模退化轨迹并利用傅里叶相位信息生成真实退化图像。
- Result: 在真实世界数据集上显著提升现有超分辨率方法的性能。
- Conclusion: 该方法有效解决了真实世界超分辨率中的退化建模问题，提升了生成图像的真实性。


### [81] [Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization](https://arxiv.org/abs/2508.07216)
*Songlin Li,Zhiqing Guo,Yuanman Li,Zeyu Li,Yunfeng Diao,Gaobo Yang,Liejun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于认知启发的多模态边界保持网络（CMB-Net），通过结合大语言模型（LLMs）分析图像篡改区域，并生成基于提示的文本信息以补充视觉信息中缺失的语义关系。

- Motivation: 现有图像篡改定位（IML）模型主要依赖视觉线索，忽略了内容特征间的语义逻辑关系。篡改技术通常会破坏内容特征的内部关系，从而留下语义线索。
- Method: CMB-Net利用LLMs生成文本信息，并通过图像-文本中心模糊模块（ITCAM）量化文本与图像特征的模糊性以分配权重。此外，图像-文本交互模块（ITIM）对齐视觉和文本特征，而恢复边缘解码器（RED）则保留篡改区域的边界信息。
- Result: 实验表明，CMB-Net在性能上优于大多数现有IML模型。
- Conclusion: 通过结合语义分析和多模态交互，CMB-Net有效提升了篡改定位的准确性。


### [82] [Generic Calibration: Pose Ambiguity/Linear Solution and Parametric-hybrid Pipeline](https://arxiv.org/abs/2508.07217)
*Yuqi Han,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: 提出了一种混合标定方法，结合通用和参数化模型，解决了通用标定中的姿态模糊问题，提升了标定精度。

- Motivation: 离线相机标定中，参数化模型依赖经验，而通用方法复杂且无法提供传统内参，且存在姿态模糊问题。
- Method: 提出线性求解器和非线性优化解决姿态模糊，并引入混合标定方法结合两种模型。
- Result: 仿真和实验表明，混合方法在各种镜头类型和噪声下表现优异。
- Conclusion: 混合标定方法为复杂场景提供了可靠且精确的解决方案。


### [83] [Landmark Guided Visual Feature Extractor for Visual Speech Recognition with Limited Resource](https://arxiv.org/abs/2508.07233)
*Lei Yang,Junshan Jin,Mingyuan Zhang,Yi He,Bofan Chen,Shilin Wang*

Main category: cs.CV

TL;DR: 提出了一种基于面部标志物的视觉特征提取器，结合时空多图卷积网络和多级唇动态融合框架，以提升有限数据下的视觉语音识别性能。

- Motivation: 现有深度学习方法易受视觉干扰影响且需大量数据，本文旨在减少用户特定特征的影响并提升小数据下的性能。
- Method: 使用面部标志物辅助训练视觉特征提取器，设计时空多图卷积网络，并结合多级唇动态融合框架。
- Result: 实验表明该方法在小数据下表现良好，并提高了对未见说话者的识别准确率。
- Conclusion: 该方法有效降低了数据需求并提升了模型泛化能力。


### [84] [ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation](https://arxiv.org/abs/2508.07237)
*Bo Wang,Mengyuan Xu,Yue Yan,Yuqun Yang,Kechen Shu,Wei Ping,Xu Tang,Wei Jiang,Zheng You*

Main category: cs.CV

TL;DR: ASM-UNet是一种基于Mamba的新型架构，用于精细分割（FGS），通过自适应扫描顺序提升性能。

- Motivation: 现有粗分割方法在精细分割中表现不足，且Mamba模型依赖固定扫描顺序，无法适应个体差异。
- Method: 提出ASM-UNet，结合群体共性和个体差异生成自适应扫描分数，动态指导扫描顺序。
- Result: 在ACDC、Synapse和BTMS数据集上，ASM-UNet在粗分割和精细分割任务中均表现优异。
- Conclusion: ASM-UNet通过自适应扫描顺序解决了精细分割中的个体差异问题，性能显著提升。


### [85] [Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers](https://arxiv.org/abs/2508.07246)
*Xin Ma,Yaohui Wang,Genyun Jia,Xinyuan Chen,Tien-Tsin Wong,Cunjian Chen*

Main category: cs.CV

TL;DR: MiraMo框架通过高效线性注意力、运动残差学习和DCT噪声优化，提升图像动画的效率和一致性。

- Motivation: 解决图像动画中外观一致性和运动平滑性的挑战，同时降低计算复杂度。
- Method: 采用线性注意力替代传统自注意力，引入运动残差学习和DCT噪声优化策略。
- Result: 实验证明MiraMo在生成一致性、平滑性和可控性动画方面优于现有方法。
- Conclusion: MiraMo为图像动画提供了高效、一致的解决方案，并适用于多种任务。


### [86] [SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking](https://arxiv.org/abs/2508.07250)
*Fengchao Xiong,Zhenxing Wu,Sen Jia,Yuntao Qian*

Main category: cs.CV

TL;DR: 本文提出了一种新的高光谱视频跟踪方法，通过结合Transformer和集合论的包含-排除原理，优化了光谱交互，提升了跟踪性能。

- Motivation: 现有方法主要关注空间交互，忽略了光谱交互，导致性能不佳。本文旨在从架构和训练层面解决这一问题。
- Method: 1. 使用Transformer建立波段间的长程空间关系；2. 基于集合论建模光谱交互；3. 引入光谱损失函数以增强鲁棒性。
- Result: 实验表明，该方法实现了最先进的跟踪性能。
- Conclusion: 通过优化光谱交互，显著提升了高光谱视频跟踪的准确性和鲁棒性。


### [87] [Understanding Dynamic Scenes in Ego Centric 4D Point Clouds](https://arxiv.org/abs/2508.07251)
*Junsheng Huang,Shengyu Hao,Bocheng Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: EgoDynamic4D是一个新的QA基准，用于动态4D场景理解，包含RGB-D视频、相机姿态、实例掩码和4D边界框，支持细粒度时空推理任务。

- Motivation: 现有数据集缺乏统一的4D标注和任务驱动的评估协议，无法支持细粒度时空推理，尤其是物体和人的运动及其交互。
- Method: 提出EgoDynamic4D数据集，包含927K QA对和显式思维链（CoT），设计12种动态QA任务，并提出一种端到端时空推理框架。
- Result: 实验表明，该方法在EgoDynamic4D上优于基线，验证了多模态时间建模的有效性。
- Conclusion: EgoDynamic4D填补了动态场景理解的空白，提出的框架为时空推理任务提供了有效解决方案。


### [88] [Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM](https://arxiv.org/abs/2508.07260)
*Sihan Yang,Huitong Ji,Shaolin Lu,Jiayi Chen,Binxiao Xu,Ming Lu,Yuanxing Zhang,Wenhui Dong,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为SLC的协作框架，通过小型和大型视觉语言模型的协作，实现高效个性化，同时避免小型模型的幻觉问题。

- Motivation: 大型视觉语言模型（VLMs）虽然能力强大，但训练成本高且难以直接个性化；小型VLMs易于个性化但推理能力不足。
- Method: 设计Small-Large Collaboration（SLC）框架，小型VLM生成个性化信息，大型VLM整合信息并提供准确响应，采用测试时反思策略避免幻觉。
- Result: 实验证明SLC在多个基准测试和大型VLM中有效，且训练高效。
- Conclusion: SLC是首个支持开源和闭源大型VLM的高效个性化框架，具有广泛的实际应用潜力。


### [89] [OpenHAIV: A Framework Towards Practical Open-World Learning](https://arxiv.org/abs/2508.07270)
*Xiang Xiang,Qinhao Zhou,Zhuo Xu,Jing Ma,Jiaxin Dai,Yifan Liang,Hanlin Li*

Main category: cs.CV

TL;DR: OpenHAIV框架整合了OOD检测、新类发现和增量持续微调，以解决开放世界场景中的知识更新问题。

- Motivation: 现有方法（如OOD检测和增量学习）在开放世界场景中存在局限性，无法自主更新知识或依赖监督条件。
- Method: 提出OpenHAIV框架，将OOD检测、新类发现和增量持续微调结合为一个统一流程。
- Result: 模型能够在开放世界环境中自主获取和更新知识。
- Conclusion: OpenHAIV为开放世界识别提供了一种有效的解决方案。


### [90] [Representation Understanding via Activation Maximization](https://arxiv.org/abs/2508.07281)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.CV

TL;DR: 本文提出了一种统一的特征可视化框架，适用于CNN和ViT，扩展了中间层的可视化，并探讨了激活最大化在生成对抗样本中的应用。

- Motivation: 理解深度神经网络内部特征表示是模型可解释性的关键步骤，现有方法主要关注CNN输出层神经元，缺乏对中间层和ViT的研究。
- Method: 采用激活最大化技术，统一框架适用于CNN和ViT，扩展至中间层特征可视化，并研究其在生成对抗样本中的作用。
- Result: 实验证明该方法在CNN和ViT中均有效，揭示了模型的潜在漏洞和决策边界。
- Conclusion: 提出的框架具有通用性和解释价值，为深度神经网络的特征表示和安全性研究提供了新视角。


### [91] [SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations](https://arxiv.org/abs/2508.07298)
*Zhiqiang Shen,Peng Cao,Xiaoli Liu,Jinzhu Yang,Osmar R. Zaiane*

Main category: cs.CV

TL;DR: SynMatch通过合成图像匹配伪标签，解决了医学图像分割中标签稀缺的问题，无需额外训练参数，显著提升了性能。

- Motivation: 医学图像分割中标签稀缺是主要挑战，现有方法因伪标签与未标记图像不一致而受限。
- Method: SynMatch从生成伪标签的分割模型中提取纹理和形状特征，合成与伪标签高度一致的图像。
- Result: 在多种医学图像分割任务中表现优异，特别是在BSL设置下，显著优于现有方法。
- Conclusion: SynMatch提供了一种无需额外训练参数的高效解决方案，适用于标签稀缺的场景。


### [92] [BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2508.07300)
*Ping-Mao Huang,I-Tien Chao,Ping-Chia Huang,Jia-Wei Liao,Yung-Yu Chuang*

Main category: cs.CV

TL;DR: 论文提出了一种名为BEVANet的双边高效视觉注意力网络，通过大核注意力机制（LKA）和动态调整感受野的综合核选择（CKS）机制，实现了实时语义分割的高性能。

- Motivation: 实时语义分割需要高效架构以捕获大感受野并细化细节轮廓，但现有视觉变换器计算成本高。
- Method: 提出BEVANet，结合稀疏分解大可分核注意力（SDLSKA）、动态调整感受野的CKS机制、大核金字塔池化模块（DLKPPM）和边界引导自适应融合（BGAF）模块。
- Result: BEVANet在未预训练和ImageNet预训练下分别达到79.3%和81.0%的mIoU，实时分割速度为33 FPS。
- Conclusion: BEVANet在性能和效率上均达到先进水平，适用于实时语义分割任务。


### [93] [DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices](https://arxiv.org/abs/2508.07306)
*Md Zahurul Haquea,Yeahyea Sarker,Muhammed Farhan Sadique Mahi,Syed Jubayer Jaman,Md Robiul Islam*

Main category: cs.CV

TL;DR: 该研究提出了一种轻量级CNN模型DragonFruitQualityNet，用于火龙果的实时质量检测，准确率达93.98%，并开发了移动应用以支持实际应用。

- Motivation: 火龙果需求增长，但缺乏高效的采前采后质量检测方法，影响了生产效率和减少损失。
- Method: 研究使用13,789张火龙果图像数据集，训练轻量级CNN模型，并嵌入移动应用实现实时检测。
- Result: 模型准确率达93.98%，优于现有方法，并成功应用于移动端。
- Conclusion: 该研究为火龙果质量控制提供了高效、可扩展的AI解决方案，支持数字农业和小农户。


### [94] [MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark](https://arxiv.org/abs/2508.07307)
*Haiyang Guo,Fei Zhu,Hongbo Zhao,Fanhu Zeng,Wenzhuo Liu,Shijie Ma,Da-Han Wang,Xu-Yao Zhang*

Main category: cs.CV

TL;DR: 论文介绍了MCITlib，一个用于多模态大语言模型持续指令调优的代码库，包含8种代表性算法，并在2个基准上进行了系统评估。

- Motivation: 多模态持续学习任务需要模型不仅能缓解灾难性遗忘，还需处理跨模态交互的挑战，但目前缺乏相关工具。
- Method: 开发了MCITlib代码库，集成了8种多模态持续指令调优算法，并在2个基准上评估。
- Result: MCITlib为多模态持续学习研究提供了工具支持，并将持续更新以反映领域进展。
- Conclusion: MCITlib的发布推动了多模态持续学习的研究，未来会持续更新以支持新进展。


### [95] [MobileViCLIP: An Efficient Video-Text Model for Mobile Devices](https://arxiv.org/abs/2508.07312)
*Min Yang,Zihan Jia,Zhilin Dai,Sheng Guo,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频-文本模型MobileViCLIP，通过时间结构重参数化技术，在移动设备上实现快速推理和零样本分类与检索。

- Motivation: 现有视频预训练模型多基于高延迟的ViT架构，缺乏针对移动设备的高效模型。本文旨在填补这一空白。
- Method: 将时间结构重参数化引入高效的图像-文本模型，并在大规模高质量视频-文本数据集上训练。
- Result: MobileViCLIP-Small在移动设备上的推理速度显著优于InternVideo2-L14和InternVideo2-S14，零样本检索性能接近或优于后者。
- Conclusion: MobileViCLIP为移动设备提供了一种高效的视频-文本模型，兼具速度和性能优势。


### [96] [DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding](https://arxiv.org/abs/2508.07313)
*Junyu Xiong,Yonghui Wang,Weichao Zhao,Chenyu Liu,Bing Yin,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: DocR1是一种通过新型强化学习框架EviGRPO训练的多模态大语言模型，专注于多页文档理解，通过证据感知奖励机制实现从粗到细的推理策略。

- Motivation: 多页文档理解对多模态大语言模型（MLLMs）提出了挑战，需要细粒度视觉理解和跨页多跳推理。现有强化学习方法在此领域的应用尚未充分探索。
- Method: 提出EviGRPO框架，结合证据感知奖励机制，采用两阶段标注流程和课程学习策略，构建EviBench和ArxivFullQA数据集。
- Result: DocR1在多页任务中达到最先进性能，同时在单页任务中表现稳定。
- Conclusion: EviGRPO框架和DocR1模型在多页文档理解中表现出色，为有限监督下的高质量模型训练提供了有效方法。


### [97] [RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning](https://arxiv.org/abs/2508.07318)
*Jinjing Gu,Tianbao Qin,Yuanyuan Pu,Zhengpeng Zhao*

Main category: cs.CV

TL;DR: RORPCap提出了一种基于检索的对象和关系提示方法，用于图像描述生成，解决了传统方法中冗余检测信息、GCN构建困难和训练成本高的问题。

- Motivation: 传统图像描述方法依赖对象检测器或结合GCN，但存在冗余信息、构建复杂和高成本的问题。RORPCap通过检索提供语义信息，优化了这一过程。
- Method: RORPCap从图像中提取对象和关系词，嵌入预定义提示模板，并通过Mamba网络快速映射图像嵌入到视觉-文本嵌入，最终用GPT-2生成描述。
- Result: 在MS-COCO数据集上，RORPCap仅需2.6小时训练，CIDEr得分120.5%，SPICE得分22.0%，性能与检测器和GCN模型相当。
- Conclusion: RORPCap是一种高效且性能可比的图像描述生成替代方案。


### [98] [Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos](https://arxiv.org/abs/2508.07330)
*Tuyen Tran,Thao Minh Le,Quang-Hung Le,Truyen Tran*

Main category: cs.CV

TL;DR: Planner-Refiner框架通过语言引导迭代优化视觉元素的时空表示，减少语义鸿沟，提升视频-语言对齐任务性能。

- Motivation: 解决视频-语言对齐中语言复杂性、动态实体交互和语义鸿沟的挑战。
- Method: Planner模块分解复杂语言提示为短句链，Refiner模块通过空间-时间自注意力优化视觉表示。
- Result: 在Referring Video Object Segmentation和Temporal Grounding任务中表现优于现有方法，尤其在复杂提示下。
- Conclusion: Planner-Refiner框架有效提升视频-语言对齐性能，适用于复杂语言场景。


### [99] [CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation](https://arxiv.org/abs/2508.07341)
*Fangtai Wu,Mushui Liu,Weijie He,Wanggui He,Hao Jiang,Zhao Wang,Yunlong Yu*

Main category: cs.CV

TL;DR: CoAR是一种新颖的框架，通过冻结预训练参数，仅调整少量参数实现定制化图像生成，解决了现有方法的过拟合和计算成本问题。

- Motivation: 现有定制化生成方法依赖全微调或适配器，成本高且易过拟合或遗忘。CoAR旨在高效注入主题概念。
- Method: 采用分层多模态上下文学习策略，引入正则化防止过拟合和语言漂移，支持无训练风格定制。
- Result: CoAR在主题和风格个性化上表现优异，计算和内存效率显著提升，仅调整0.05%参数。
- Conclusion: CoAR在保持预训练模型性能的同时，实现了高效、低成本的定制化生成。


### [100] [SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal](https://arxiv.org/abs/2508.07346)
*Tingyu Yang,Jue Gong,Jinpei Guo,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: SODiff是一种新颖的语义导向一步扩散模型，用于JPEG伪影去除，通过语义对齐的图像提示提取器和质量因子感知时间预测器，显著提升了恢复效果。

- Motivation: JPEG高压缩比会引入严重的视觉伪影，现有深度学习方法难以恢复复杂纹理细节，导致输出过度平滑。
- Method: 提出SODiff模型，结合语义对齐的图像提示提取器（SAIPE）和质量因子感知时间预测器，优化扩散过程。
- Result: 实验表明，SODiff在视觉质量和定量指标上均优于现有方法。
- Conclusion: SODiff通过语义导向和自适应时间预测，有效提升了JPEG伪影去除的性能。


### [101] [GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction](https://arxiv.org/abs/2508.07355)
*Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: GS4Buildings利用语义3D建筑模型改进2D高斯泼溅（2DGS）在大规模复杂城市场景中的重建效果，显著提升完整性和几何精度。

- Motivation: 传统2DGS在复杂城市场景中因遮挡问题导致重建不完整，需结合语义建筑模型提升性能。
- Method: 直接从LoD2语义3D建筑模型初始化高斯，并引入先验深度和法线图优化重建过程。
- Result: 实验显示重建完整性提升20.5%，几何精度提升32.8%，高斯基元减少71.8%。
- Conclusion: 语义建筑模型集成可推动高斯泼溅技术在城市智能化和数字孪生等实际应用中的发展。


### [102] [Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring](https://arxiv.org/abs/2508.07369)
*Tianyu Xin,Jin-Liang Xiao,Zeyu Xia,Shan Yin,Liang-Jian Deng*

Main category: cs.CV

TL;DR: 提出一种模块化分解和特征裁剪方法，显著提升跨传感器泛化能力，同时大幅降低训练和推理时间。

- Motivation: 现有深度学习全色锐化模型在跨传感器数据上泛化能力差，传统方法耗时或需额外数据。
- Method: 通过模块化分解模型，在关键接口集成特征裁剪器，利用物理感知无监督损失高效训练，并采用分块并行推理。
- Result: 在跨传感器场景下性能显著提升，训练和推理时间极短（如512x512x8图像仅需0.2秒）。
- Conclusion: 该方法在质量和效率上均达到最优，适用于实际大规模应用。


### [103] [DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery](https://arxiv.org/abs/2508.07372)
*Rajaei Khatib,Raja Giryes*

Main category: cs.CV

TL;DR: DIP-GS是一种基于深度图像先验（DIP）的3D高斯泼溅（3DGS）方法，解决了3DGS在稀疏视图重建中的不足，无需预训练模型即可实现高性能。

- Motivation: 3DGS在多视图重建中表现优异，但在稀疏视图场景下效果不佳。本文旨在通过DIP先验改进3DGS，使其适用于稀疏视图重建。
- Method: 提出DIP-GS，结合DIP先验和3DGS，采用由粗到细的方式学习场景表示，仅依赖输入帧，无需预训练模型。
- Result: DIP-GS在稀疏视图重建任务中达到SOTA水平，展示了其有效性。
- Conclusion: DIP-GS通过DIP先验显著提升了3DGS在稀疏视图场景下的性能，且无需额外预训练模型。


### [104] [LET-US: Long Event-Text Understanding of Scenes](https://arxiv.org/abs/2508.07401)
*Rui Chen,Xingyu Chen,Shaoan Wang,Shihan Kong,Junzhi Yu*

Main category: cs.CV

TL;DR: LET-US框架通过自适应压缩机制处理长事件流，结合两阶段优化和跨模态查询，提升事件流与文本的对齐能力，在多项任务中表现优异。

- Motivation: 现有MLLM难以有效解析长事件流或保留关键视觉细节，LET-US旨在填补这一空白。
- Method: 采用自适应压缩机制、两阶段优化、文本引导的跨模态查询及分层聚类，构建大规模事件-文本对齐数据集。
- Result: LET-US在长事件流的描述准确性和语义理解上优于现有MLLM。
- Conclusion: LET-US为长事件流与文本的跨模态理解开辟了新方向，代码和数据集将公开。


### [105] [ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack](https://arxiv.org/abs/2508.07402)
*Rongxuan Peng,Shunquan Tan,Chenqi Kong,Anwei Luo,Alex C. Kot,Jiwu Huang*

Main category: cs.CV

TL;DR: 论文提出ForensicsSAM，一种对抗性攻击鲁棒的图像伪造检测与定位框架，通过注入伪造和对抗专家提升性能。

- Motivation: 现有参数高效微调方法忽视对抗攻击的脆弱性，导致性能下降。
- Method: 设计ForensicsSAM框架，包括伪造专家注入、轻量对抗检测器和对抗专家自适应激活。
- Result: 实验表明ForensicsSAM在对抗攻击下表现优异，同时达到最先进的伪造检测与定位性能。
- Conclusion: ForensicsSAM为图像伪造检测与定位提供了高效且鲁棒的解决方案。


### [106] [CharacterShot: Controllable and Consistent 4D Character Animation](https://arxiv.org/abs/2508.07409)
*Junyao Gao,Jiaxing Li,Wenran Liu,Yanhong Zeng,Fei Shen,Kai Chen,Yanan Sun,Cairong Zhao*

Main category: cs.CV

TL;DR: CharacterShot是一个可控且一致的4D角色动画框架，通过单张参考图像和2D姿态序列生成动态3D角色动画。

- Motivation: 旨在让设计师能够轻松创建动态3D角色动画，从2D到3D的转换中保持时空和视角一致性。
- Method: 基于DiT的图像到视频模型预训练2D动画模型，引入双注意力模块和相机先验生成多视角视频，并通过4D高斯溅射优化生成4D角色表示。
- Result: 在CharacterBench基准测试中表现优于现有方法，并构建了大规模数据集Character4D。
- Conclusion: CharacterShot为4D角色动画提供了高效可控的解决方案，代码和数据集将开源。


### [107] [CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](https://arxiv.org/abs/2508.07413)
*Youqi Wang,Shunquan Tan,Rongxuan Peng,Bin Li,Jiwu Huang*

Main category: cs.CV

TL;DR: CLUE利用Stable Diffusion 3和Segment Anything Model的机制，通过噪声注入和参数高效调整，实现高保真伪造定位，显著优于现有方法。

- Motivation: 图像编辑工具和生成AI的普及导致数字媒体真实性受到威胁，需要更高效的伪造检测方法。
- Method: 结合SD3的Rectified Flow机制和LoRA参数调整，注入噪声放大伪造痕迹，并利用SAM的语义特征增强定位精度。
- Result: CLUE在泛化性能和鲁棒性上显著优于现有方法，并能抵抗常见后处理攻击和社交网络干扰。
- Conclusion: CLUE为数字媒体伪造检测提供了高效且鲁棒的解决方案，代码已开源。


### [108] [Freeze and Reveal: Exposing Modality Bias in Vision-Language Models](https://arxiv.org/abs/2508.07432)
*Vivek Hruday Kavuri,Vysishtya Karanam,Venkata Jahnavi Venkamsetty,Kriti Madumadukala,Lakshmipathi Balaji Darur,Ponnurangam Kumaraguru*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型中的性别偏见，提出两种去偏方法（CDA和DAUDoS），并通过实验发现视觉和文本编码器的偏见来源不同。

- Motivation: 视觉语言模型在训练数据中继承了性别偏见，需要分析并减少这种偏见。
- Method: 使用反事实数据增强（CDA）和基于刻板印象程度的新方法（DAUDoS）进行去偏。
- Result: CDA减少性别差距6%，DAUDoS减少3%且仅需三分之一数据；两种方法均提升性别识别准确率3%。
- Conclusion: 发现CLIP视觉编码器和PaliGemma2文本编码器偏见来源不同，为未来多模态系统的去偏提供了方向。


### [109] [Levarging Learning Bias for Noisy Anomaly Detection](https://arxiv.org/abs/2508.07441)
*Yuxin Zhang,Yunkang Cao,Yuqi Cheng,Yihan Sun,Weiming Shen*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段框架，利用模型的学习偏置解决完全无监督图像异常检测（FUIAD）中训练数据可能包含未标记异常的问题。

- Motivation: 传统方法假设训练数据无异常，但实际数据可能被污染，导致模型将异常误认为正常，影响检测性能。
- Method: 提出两阶段框架：第一阶段利用学习偏置（正常样本统计优势和特征空间差异）筛选纯净数据集；第二阶段在纯净数据上训练最终检测器。
- Result: 在Real-IAD基准测试中表现出优异的异常检测和定位性能，且对噪声具有鲁棒性。
- Conclusion: 该框架兼容多种无监督骨干网络，为实际场景中不完美的训练数据提供了实用解决方案。


### [110] [Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines](https://arxiv.org/abs/2508.07450)
*Suman Kunwar,Prabesh Rai*

Main category: cs.CV

TL;DR: 该研究评估了五种医疗废物分类模型在尼泊尔的性能，发现YOLOv5-s准确率最高（95.06%），但推理速度略慢于YOLOv8-n。EfficientNet-B0表现也不错（93.22%），但推理时间最长。最终部署了YOLOv5-s模型，并建议进一步优化数据和本地化。

- Motivation: 尼泊尔医疗设施增加导致医疗废物管理挑战加剧，不当处理可能引发污染和传染病传播。
- Method: 使用分层K折技术（5折）评估ResNeXt-50、EfficientNet-B0、MobileNetV3-S、YOLOv8-n和YOLOv5-s模型，并通过重复ANOVA验证统计显著性。
- Result: YOLOv5-s准确率最高（95.06%），EfficientNet-B0次之（93.22%），但推理时间最长。YOLOv8-n推理速度最快。
- Conclusion: YOLOv5-s被部署为最佳模型，并建议未来优化数据和本地化处理。


### [111] [AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning](https://arxiv.org/abs/2508.07470)
*Siminfar Samakoush Galougah,Rishie Raj,Sanjoy Chowdhury,Sayan Nag,Ramani Duraiswami*

Main category: cs.CV

TL;DR: AURA是一个新的音频-视觉基准测试，旨在评估跨模态推理能力，弥补现有基准仅关注答案准确性的不足。

- Motivation: 现有音频-视觉基准仅关注最终答案准确性，无法区分真正理解与通过错误推理得出的正确答案。
- Method: AURA包含六个认知领域的挑战性问题，要求模型基于音频和视频构建逻辑路径，并提出新指标AuraScore评估推理过程。
- Result: 评估显示，尽管模型在某些任务上准确率高达92%，但其推理一致性和逻辑有效性得分低于45%。
- Conclusion: AURA揭示了模型推理能力的不足，为更稳健的多模态评估提供了工具。


### [112] [Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution](https://arxiv.org/abs/2508.07483)
*Pranav Chougule*

Main category: cs.CV

TL;DR: 本文比较了摄影测量和高斯泼溅技术在3D模型重建和视图合成中的表现，提出了一种改进的高斯泼溅方法，并展示了其在提升摄影测量重建质量上的潜力。

- Motivation: 研究动机在于比较两种3D重建技术的性能，并探索高斯泼溅技术在生成高质量新视图及改进摄影测量模型中的应用。
- Method: 通过创建真实场景的图像数据集，使用两种方法构建3D模型，并采用SSIM、PSNR、LPIPS和分辨率指标进行评估。改进高斯泼溅方法以支持Blender环境中的新视图渲染。
- Result: 结果表明高斯泼溅能生成高质量新视图，并可能提升摄影测量重建效果。两种方法各有优劣。
- Conclusion: 高斯泼溅技术在3D重建和视图合成中具有潜力，为扩展现实（XR）、摄影测量和自动驾驶模拟提供了有价值的信息。


### [113] [VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding](https://arxiv.org/abs/2508.07493)
*Jian Chen,Ming Li,Jihyung Kil,Chenguang Wang,Tong Yu,Ryan Rossi,Tianyi Zhou,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: VisR-Bench是一个多语言基准测试，用于长文档中的问题驱动多模态检索，覆盖16种语言和3种问题类型，评估了多种检索模型的性能。

- Motivation: 现有基准测试主要关注英文文档检索或单页图像的多语言问答，缺乏对多语言长文档多模态检索的支持。
- Method: 引入VisR-Bench，包含35K QA对和1.2K文档，支持细粒度多模态检索评估，涵盖多种语言和问题类型。
- Result: MLLMs表现优于文本和多模态编码器模型，但在结构化表格和低资源语言上仍有困难。
- Conclusion: VisR-Bench填补了多语言长文档多模态检索的空白，揭示了模型在复杂场景中的挑战。


### [114] [FormCoach: Lift Smarter, Not Harder](https://arxiv.org/abs/2508.07501)
*Xiaoye Zuo,Nikos Athanasiou,Ginger Delmas,Yiming Huang,Xingyu Fu,Lingjie Liu*

Main category: cs.CV

TL;DR: FormCoach是一个基于视觉语言模型（VLMs）的AI健身教练，通过摄像头实时检测和纠正用户动作错误，并发布了数据集和评估工具以推动研究。

- Motivation: 为居家健身爱好者提供专业反馈，解决专家指导难以获取的问题。
- Method: 利用视觉语言模型（VLMs）分析用户动作，并通过网页界面实时提供个性化纠正。
- Result: 在1,700个专家标注的视频对上测试，发现AI与人类教练水平仍有差距。
- Conclusion: FormCoach展示了AI在健身领域的潜力，为具身AI开辟了新方向。


### [115] [From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials](https://arxiv.org/abs/2508.07514)
*Artzai Picon,Itziar Eguskiza,Daniel Mugica,Javier Romero,Carlos Javier Jimenez,Eric White,Gabriel Do-Lago-Junqueira,Christian Klukas,Ramon Navarra-Mestre*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的分割模型，结合自监督视觉模型和植物分类学层次推理，显著提升了物种识别和损害分类的准确性和鲁棒性，并在跨设备和跨地域测试中表现优异。

- Motivation: 传统的手动视觉评估方法耗时、费力且主观，自动化物种和损害识别因视觉差异细微而具有挑战性，但能显著提高效率和一致性。
- Method: 结合通用自监督视觉模型和植物分类学层次推理，利用多年份、多地区的数据集进行训练，并在不同设备和地域的测试数据上评估模型鲁棒性。
- Result: 模型显著提升了物种识别（F1-score: 0.52到0.85）和损害分类（F1-score: 0.28到0.44）的性能，在跨设备测试中仍保持较强表现。
- Conclusion: 模型具有鲁棒性和实际应用价值，已被部署用于大规模自动化作物和杂草监测。


### [116] [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](https://arxiv.org/abs/2508.07519)
*Joonghyuk Shin,Alchan Hwang,Yujin Kim,Daneul Kim,Jaesik Park*

Main category: cs.CV

TL;DR: 本文分析了多模态扩散变换器（MM-DiT）的注意力机制，提出了一种基于提示的图像编辑方法，适用于MM-DiT及其变体。

- Motivation: 传统方法依赖单向跨注意力机制，而MM-DiT引入了双向信息流的统一注意力机制，这对现有编辑技术提出了挑战。
- Method: 通过将注意力矩阵分解为四个块，分析其特性，并提出一种基于提示的编辑方法。
- Result: 该方法支持从全局到局部的编辑，适用于多种MM-DiT变体。
- Conclusion: 研究填补了U-Net方法与新兴架构之间的空白，提供了对MM-DiT行为模式的深入理解。


### [117] [Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module](https://arxiv.org/abs/2508.07528)
*Xiaotong Ji,Ryoma Bise,Seiichi Uchida*

Main category: cs.CV

TL;DR: 论文提出了一种结合拒绝模块的改进方法，用于解决医疗图像处理中噪声标签和类别模糊实例对排名学习的影响。

- Motivation: 医疗图像诊断的准确性至关重要，但噪声标签和类别模糊实例会干扰排名学习的目标，导致错误诊断。
- Method: 提出了一种结合拒绝模块的排名学习方法，通过共同优化排名损失和拒绝函数来识别和减少异常值的影响。
- Result: 在医疗数据集上的实验验证表明，该方法能有效检测和减少异常值，提高诊断的可靠性和准确性。
- Conclusion: 该方法通过集成拒绝模块，显著提升了医疗图像诊断的排名学习效果。


### [118] [Enhanced Generative Structure Prior for Chinese Text Image Super-resolution](https://arxiv.org/abs/2508.07537)
*Xiaoming Li,Wangmeng Zuo,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出了一种基于结构先验的高质量中文文本图像超分辨率框架，通过结合StyleGAN和代码本机制，实现了对低分辨率中文字符的精确恢复。

- Motivation: 现有方法主要关注英文文本，对更复杂的中文脚本关注不足，因此需要一种能够恢复中文字符精确笔画的超分辨率方法。
- Method: 提出了一种结构先验，结合StyleGAN模型和代码本机制，通过代码本表示字符结构，StyleGAN控制字符样式，生成高分辨率结构先验。
- Result: 实验表明，该方法能够为低分辨率中文文本提供鲁棒的结构指导，准确恢复模糊字符的清晰笔画。
- Conclusion: 该方法在中文文本图像超分辨率任务中表现出色，尤其适用于不规则布局的真实场景低分辨率文本。


### [119] [A DICOM Image De-identification Algorithm in the MIDI-B Challenge](https://arxiv.org/abs/2508.07538)
*Hongzhu Jiang,Sihan Xie,Zhiyu Wan*

Main category: cs.CV

TL;DR: 论文探讨了医学图像去标识化的重要性，介绍了MIDI-B挑战赛及其方法，展示了算法的优异表现，并分析了当前方法的局限性和未来改进方向。

- Motivation: 医学图像共享需符合隐私法规（如HIPAA、DICOM PS3.15），去标识化对保护患者隐私和维持数据实用性至关重要。
- Method: 采用像素掩码、日期偏移、哈希、文本识别、替换和移除等方法，严格遵循标准处理数据集。
- Result: 算法在MIDI-B挑战赛中正确执行99.92%的操作，排名第二。
- Conclusion: 当前方法仍有局限，未来需进一步优化以提升去标识化效果。


### [120] [Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning](https://arxiv.org/abs/2508.07539)
*Yuki Shigeyasu,Shota Harada,Akihiko Yoshizawa,Kazuhiro Terada,Naoki Nakazima,Mariyo Kurata,Hiroyuki Abe,Tetsuo Ushiku,Ryoma Bise*

Main category: cs.CV

TL;DR: 提出一种针对病理图像中域偏移的通用方法，通过聚类非肿瘤区域的WSI特征并利用对比学习减少特征差异。

- Motivation: 传统方法依赖多医院数据，但数据收集困难，因此提出利用医院内域偏移的方法。
- Method: 采用两阶段对比学习（WSI级和patch级）以减少不同聚类间的特征差异。
- Result: 有效减少了病理图像中的域偏移问题。
- Conclusion: 该方法为病理图像域偏移问题提供了一种实用解决方案。


### [121] [CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts](https://arxiv.org/abs/2508.07540)
*Junuk Cha,Jihyeon Kim*

Main category: cs.CV

TL;DR: 论文提出了一种结合链式思维（CoT）推理的3D人体姿态生成框架，解决了现有模型依赖低层次提示的问题，能够从高层次抽象语言生成准确的姿态。

- Motivation: 现有文本到姿态生成模型依赖低层次详细提示，而人类通常使用高层次抽象语言描述动作，导致实际应用中的挑战。
- Method: 提出CoT-Pose框架，将CoT推理融入姿态生成过程，并设计数据合成流程生成抽象提示、详细提示和对应3D姿态的三元组用于训练。
- Result: 实验表明，CoT-Pose能从抽象文本输入生成语义对齐的合理姿态。
- Conclusion: 该工作强调了高层次理解在姿态生成中的重要性，为推理增强的姿态生成方法开辟了新方向。


### [122] [Commentary Generation for Soccer Highlights](https://arxiv.org/abs/2508.07543)
*Chidaksh Ravuru*

Main category: cs.CV

TL;DR: 论文探讨了基于MatchVoice模型的足球评论生成技术，通过改进对齐方法提升实时同步性，并在GOAL数据集上验证其性能。

- Motivation: 现有系统如SoccerNet-Caption在视频内容与评论的细粒度对齐上表现不足，MatchVoice通过粗粒度和细粒度对齐技术解决了这一问题。
- Method: 扩展MatchVoice模型，应用于足球集锦评论生成，使用GOAL数据集，并测试不同训练配置和硬件限制的影响。
- Result: 实验验证了MatchVoice的泛化能力，但需结合更广泛的视频-语言技术以进一步提升性能。
- Conclusion: MatchVoice在足球评论生成中表现良好，但未来需整合更多技术以优化效果。


### [123] [Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning](https://arxiv.org/abs/2508.07548)
*Takehiro Yamane,Itaru Tsuge,Susumu Saito,Ryoma Bise*

Main category: cs.CV

TL;DR: 提出了一种基于PU学习的医学图像分割伪标签方法，通过单张图像学习选择有效伪标签。

- Motivation: 解决医学图像分割中伪标签选择的问题，尤其是在背景区域多样化的场景下。
- Method: 采用PU学习（仅使用正样本和未标记数据）来区分前景和背景区域，并选择有效的伪标签。
- Result: 实验结果表明该方法在医学图像分割中有效。
- Conclusion: 提出的PU学习方法能够有效选择伪标签，适用于多样化的背景区域。


### [124] [Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring](https://arxiv.org/abs/2508.07552)
*Ludan Zhang,Sihan Wang,Yuqi Dai,Shuofei Qiao,Lei He*

Main category: cs.CV

TL;DR: 该论文提出了一种基于特征图收敛分数（FMCS）的独立评估方法，用于解决端到端自动驾驶模型中中间功能模块缺乏显式监督信号的问题，并通过实验验证了其有效性。

- Motivation: 端到端自动驾驶模型的中间功能模块缺乏显式监督信号，导致机制不透明且可解释性有限，传统方法难以独立评估和训练这些模块。
- Method: 提出了基于特征图-真值表示相似性的评估框架，构建了双粒度动态加权评分系统（DG-DWSS），并开发了基于CLIP的特征图质量评估网络（CLIP-FMQE-Net）。
- Result: 在NuScenes数据集上的实验表明，该方法提高了3D目标检测性能，NDS提升了3.89%。
- Conclusion: 该方法能有效提升特征表示质量和整体模型性能。


### [125] [Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation](https://arxiv.org/abs/2508.07557)
*Minghao Yin,Yukang Cao,Songyou Peng,Kai Han*

Main category: cs.CV

TL;DR: Splat4D是一个从单目视频生成高质量4D内容的新框架，通过多视角渲染、不一致性识别、视频扩散模型和非对称U-Net优化，实现了时空一致性和高保真度。

- Motivation: 解决从单目视频生成4D内容时面临的时空一致性、细节保留和用户指导有效性的挑战。
- Method: 采用多视角渲染、不一致性识别、视频扩散模型和非对称U-Net进行优化。
- Result: 在公开基准测试中表现优异，支持文本/图像条件4D生成、4D人体生成和文本引导内容编辑。
- Conclusion: Splat4D在多种应用中展现了高效性和多功能性，能够生成符合用户指令的连贯结果。


### [126] [Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2508.07570)
*Khanh-Binh Nguyen,Phuoc-Nguyen Bui,Hyunseung Choo,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: 论文提出了一种自适应缓存增强（ACE）框架，通过动态优化缓存和决策边界，解决了视觉语言模型在分布偏移下的性能下降问题。

- Motivation: 视觉语言模型（VLMs）在零样本泛化方面表现优异，但在下游任务中面临分布偏移导致的性能下降问题，尤其是在缺乏标注数据时。测试时适应（TTA）方法通过在线优化模型来解决这一问题，但现有缓存方法存在置信度不可靠和决策边界僵化的挑战。
- Method: ACE框架通过动态选择高置信度或低熵的图像嵌入，并基于类别特定阈值和指数移动平均优化缓存，实现自适应决策边界。
- Result: 在15个基准数据集上的实验表明，ACE在分布偏移场景下优于现有TTA方法，表现出更高的鲁棒性和泛化能力。
- Conclusion: ACE通过动态缓存和自适应决策边界，显著提升了视觉语言模型在分布偏移下的性能，为TTA方法提供了新的研究方向。


### [127] [Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification](https://arxiv.org/abs/2508.07577)
*Zhaorui Tan,Tan Pan,Kaizhu Huang,Weimiao Yu,Kai Yao,Chen Jiang,Qiufeng Wang,Anh Nguyen,Xin Guo,Yuan Cheng,Xi Yang*

Main category: cs.CV

TL;DR: 本文研究了LayerNorm在Vision Transformers（ViTs）中的微调动态，特别是在数据稀缺和领域转移下的表现，并提出了一种基于Fine-tuning Shift Ratio（FSR）的简单有效的重新缩放机制。

- Motivation: LayerNorm在ViTs中至关重要，但其在数据稀缺和领域转移下的微调动态尚未充分研究。本文旨在填补这一空白。
- Method: 提出了一种基于FSR的重新缩放机制，使用标量λ与FSR负相关，并结合循环框架优化LayerNorm微调。
- Result: 实验表明，OOD任务通常具有较低的FSR和较高的λ，而病理数据微调更接近ID设置。
- Conclusion: 研究揭示了LayerNorm在迁移学习中的动态特性，并提供了实用的微调策略。


### [128] [GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm](https://arxiv.org/abs/2508.07585)
*Yu-Huan Wu,Wei Liu,Zi-Xuan Zhu,Zizhou Wang,Yong Liu,Liangli Zhen*

Main category: cs.CV

TL;DR: GAPNet是一种基于粒度感知范式的轻量级网络，用于图像和视频显著目标检测（SOD），通过多尺度解码器输出和高效特征融合模块实现高性能。

- Motivation: 现有SOD模型依赖重型主干网络，计算成本高，限制了在边缘设备上的应用。
- Method: 采用粒度感知连接、粒度金字塔卷积（GPC）和跨尺度注意力（CSA）模块，结合自注意力模块优化特征利用和语义解释。
- Result: 在轻量级图像和视频SOD模型中达到新的最优性能。
- Conclusion: GAPNet通过高效特征融合和粒度感知监督，显著提升了轻量级SOD模型的性能。


### [129] [Voice Pathology Detection Using Phonation](https://arxiv.org/abs/2508.07587)
*Sri Raksha Siva,Nived Suthahar,Prakash Boominathan,Uma Ranjan*

Main category: cs.CV

TL;DR: 该研究提出了一种基于机器学习的非侵入性框架，利用语音数据检测声音病理，通过声学特征和RNN分类器实现自动化诊断。

- Motivation: 声音障碍严重影响交流和生活质量，传统诊断方法侵入性强且主观，需要一种非侵入性、准确的早期检测方法。
- Method: 使用Saarbrücken语音数据库的语音数据，提取MFCC、chroma特征和Mel频谱图，结合RNN（LSTM和注意力机制）进行分类，并通过数据增强和预处理提升模型性能。
- Result: 提出的框架能够非侵入性地自动诊断声音病理，支持AI驱动的医疗保健，改善患者预后。
- Conclusion: 该研究为声音病理的早期检测提供了一种有效的非侵入性工具，具有临床应用潜力。


### [130] [From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users](https://arxiv.org/abs/2508.07596)
*Shahroz Tariq,Simon S. Woo,Priyanka Singh,Irena Irmalasari,Saakshi Gupta,Dev Gupta*

Main category: cs.CV

TL;DR: DF-P2E是一个多模态框架，通过视觉、语义和叙述层解释，使深度伪造检测更透明和易理解。

- Motivation: 解决现有深度伪造检测系统缺乏透明度和可解释性的问题，特别是在非专家用户中。
- Method: 结合了基于Grad-CAM的显著图可视化、视觉字幕生成模块和细调的大型语言模型（LLM）的叙述优化模块。
- Result: 在DF40基准测试中表现优异，检测性能与高质量解释一致。
- Conclusion: DF-P2E为可解释的深度伪造检测提供了可扩展的解决方案，推动了可信赖和透明AI系统的发展。


### [131] [ShoulderShot: Generating Over-the-Shoulder Dialogue Videos](https://arxiv.org/abs/2508.07597)
*Yuang Zhang,Junqi Cheng,Haoyu Zhao,Jiaxi Gu,Fangyuan Zou,Zenghui Lu,Peng Shu*

Main category: cs.CV

TL;DR: ShoulderShot框架通过双镜头生成和循环视频技术，解决了对话视频生成中的角色一致性和空间连续性挑战，优于现有方法。

- Motivation: 对话视频在影视和广告中具有重要情感连接作用，但相关研究较少，存在角色一致性、空间连续性和计算预算限制等挑战。
- Method: 结合双镜头生成与循环视频技术，生成长对话视频并保持角色一致性。
- Result: 在镜头切换布局、空间连续性和对话长度灵活性方面优于现有方法。
- Conclusion: ShoulderShot为实际对话视频生成提供了新可能性。


### [132] [LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation](https://arxiv.org/abs/2508.07603)
*Wenhui Song,Hanhui Li,Jiehui Huang,Panwen Hu,Yuhao Cheng,Long Chen,Yiqiang Yan,Xiaodan Liang*

Main category: cs.CV

TL;DR: LaVieID是一种新的局部自回归视频扩散框架，旨在解决身份保持的文本到视频任务，通过空间和时间视角优化身份信息保留。

- Motivation: 现有扩散变换器（DiTs）在全局生成过程中容易丢失身份信息，LaVieID旨在解决这一问题。
- Method: 引入局部路由器显式表示面部潜在状态，并集成时间自回归模块以增强帧间身份一致性。
- Result: LaVieID能生成高保真个性化视频，并达到最先进性能。
- Conclusion: LaVieID通过局部和时间的优化，显著提升了身份保持视频生成的效果。


### [133] [X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning](https://arxiv.org/abs/2508.07607)
*Jian Ma,Xujie Zhu,Zihao Pan,Qirong Peng,Xu Guo,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: 论文介绍了X2Edit数据集和任务感知的MoE-LoRA训练方法，用于图像编辑任务，性能优于现有开源数据集。

- Motivation: 现有开源数据集和编辑模块在图像编辑任务中表现不足，需要更高质量的数据集和兼容性强的编辑方法。
- Method: 构建X2Edit数据集（3.7百万高质量数据），设计任务感知的MoE-LoRA训练方法，并引入对比学习。
- Result: 模型编辑性能优异，数据集优于现有开源数据集。
- Conclusion: X2Edit数据集和方法为图像编辑任务提供了高质量解决方案，代码和数据集已开源。


### [134] [An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View](https://arxiv.org/abs/2508.07618)
*Hyoung Suk Park,Kiwan Jeon*

Main category: cs.CV

TL;DR: 提出了一种两阶段方法，通过隐式神经表示（INR）生成先验图像并校正投影数据，以减少牙科CBCT中的截断伪影。

- Motivation: 小型探测器导致视野截断，影响迭代重建图像质量，需解决截断伪影问题。
- Method: 第一阶段用INR生成先验图像并估计截断误差；第二阶段用校正后的数据迭代重建。
- Result: 两阶段方法有效抑制截断伪影，提升图像质量。
- Conclusion: 该方法为牙科CBCT提供了一种有效的截断伪影抑制方案。


### [135] [SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation](https://arxiv.org/abs/2508.07621)
*Yunsung Chung,Chanho Lim,Ghassan Bidaoui,Christian Massad,Nassir Marrouche,Jihun Hamm*

Main category: cs.CV

TL;DR: SOFA是一种深度学习框架，通过模拟消融手术效果、预测房颤复发风险并优化手术参数，个性化房颤消融治疗。

- Motivation: 房颤消融手术效果差异大，现有方法难以评估和优化手术效果。SOFA旨在通过模拟和优化手术参数，提高消融效果。
- Method: SOFA利用深度学习生成消融后图像，预测复发风险，并通过优化手术参数（如位置、时长、温度等）降低风险。
- Result: SOFA能准确合成消融后图像，优化方案使模型预测的复发风险降低22.18%。
- Conclusion: SOFA首次整合了手术效果模拟、复发预测和参数优化，为个性化房颤消融提供了新工具。


### [136] [Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction](https://arxiv.org/abs/2508.07624)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出了一种基于图神经网络的后期处理流程，利用物体间的空间关系修正检测异常，显著提升了检测性能。

- Motivation: 现有物体检测模型未能充分利用静态环境中物体空间布局的一致性，导致检测结果不一致或错误。
- Method: 使用图神经网络（GNN）建模物体间的空间关系，修正检测异常。
- Result: 实验表明，该方法使mAP@50提升高达4%。
- Conclusion: 利用环境空间结构可显著提升物体检测系统的可靠性。


### [137] [A Trustworthy Method for Multimodal Emotion Recognition](https://arxiv.org/abs/2508.07625)
*Junxiao Xue,Xiaozhen Liu,Jie Wang,Xuecheng Wu,Bin Wu*

Main category: cs.CV

TL;DR: 提出了一种基于不确定性估计的可信情绪识别方法（TER），通过多模态置信度结合输出可信预测，并引入新评估标准衡量可靠性。

- Motivation: 现有情绪识别方法虽有效但模型复杂，且对噪声和异常数据可靠性不足，需提升预测可信度。
- Method: TER利用不确定性估计计算预测置信值，结合多模态结果输出可信预测，并提出可信精度、召回率等新评估标准。
- Result: TER在Music-video上准确率达82.40%，在IEMOCAP和Music-video上的可信F1分数分别为0.7511和0.9035。
- Conclusion: TER通过置信度模块提升了模型的可靠性和鲁棒性，实验验证了其有效性。


### [138] [AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning](https://arxiv.org/abs/2508.07626)
*Dejie Yang,Zijing Zhao,Yang Liu*

Main category: cs.CV

TL;DR: 论文提出AR-VRM方法，通过模仿人类手部关键点从大规模人类动作视频中学习，提升视觉机器人操作的泛化能力。

- Motivation: 现有方法因依赖与机器人任务差异大的网络数据或隐式训练，泛化能力有限。
- Method: 提出基于关键点的视觉语言模型预训练，并通过类比推理将人类动作映射到机器人组件。
- Result: 在CALVIN基准测试和真实实验中表现领先，尤其在少样本场景下优势显著。
- Conclusion: 显式模仿人类动作在数据稀缺时更有效，为视觉机器人操作提供了新思路。


### [139] [LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering](https://arxiv.org/abs/2508.07647)
*Xiaohang Zhan,Dingming Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的图像生成算法，通过体积渲染原理精确控制图像中物体的遮挡关系。

- Motivation: 现有方法依赖提示或布局控制遮挡，但缺乏精确性。本文旨在解决这一问题。
- Method: 利用预训练扩散模型和体积渲染原理，在潜在空间中根据遮挡关系和透射率“渲染”场景。
- Result: 在遮挡精度上显著优于现有方法，并能调整物体透明度、密度等效果。
- Conclusion: 该方法无需重新训练模型，实现了基于物理的精确遮挡控制，扩展了图像生成的应用范围。


### [140] [Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels](https://arxiv.org/abs/2508.07656)
*Yimin Fu,Zhunga Liu,Dongxiu Guo,Longfei Wang*

Main category: cs.CV

TL;DR: 提出了一种名为CLSDF的方法，通过融合散射和深度特征，结合半监督学习，有效解决了SAR ATR中噪声标签的问题。

- Motivation: 由于SAR数据标注需要专业知识，噪声标签不可避免，导致性能下降。现有方法主要针对图像数据，SAR的非直观特性使其难以直接应用。
- Method: 设计多模型特征融合框架，将散射特征与深度特征结合；利用GMM建模损失分布区分干净和噪声标签；通过半监督学习和联合分布对齐策略提升性能。
- Result: 在MSTAR数据集上验证，CLSDF在不同噪声条件下均达到最优性能。
- Conclusion: CLSDF方法有效提升了SAR ATR在噪声标签下的鲁棒性和性能。


### [141] [Undress to Redress: A Training-Free Framework for Virtual Try-On](https://arxiv.org/abs/2508.07680)
*Zhiying Li,Junhao Wu,Yeying Jin,Daiheng Gao,Yun Ji,Kaichuan Kong,Lei Yu,Hao Xu,Kai Chen,Bruce Gu,Nana Wang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: UR-VTON提出了一种无需训练的新框架，通过‘脱衣-穿衣’机制解决长袖转短袖虚拟试衣中的皮肤恢复问题，结合动态调度和结构细化器提升效果。

- Motivation: 现有虚拟试衣方法在长袖转短袖场景中因皮肤恢复不准确导致效果不佳，需改进。
- Method: 提出UR-VTON框架，采用‘脱衣-穿衣’两步机制，结合动态调度和结构细化器优化细节。
- Result: 实验表明UR-VTON在细节保留和图像质量上优于现有方法。
- Conclusion: UR-VTON为长袖转短袖虚拟试衣提供了有效解决方案，并提出了新基准LS-TON。


### [142] [TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding](https://arxiv.org/abs/2508.07683)
*Chaohong Guo,Xun Mo,Yongwei Nie,Xuemiao Xu,Chao Xu,Fei Yu,Chengjiang Long*

Main category: cs.CV

TL;DR: 论文提出TAR-TVG框架，通过时间戳锚点约束推理过程，提升视频时序定位的准确性，并采用自蒸馏训练策略优化模型性能。

- Motivation: 现有强化学习方法未能显式约束推理过程，影响最终时序预测质量，需改进。
- Method: 引入时间戳锚点作为中间验证点，要求推理步骤逐步精确化；采用三阶段自蒸馏训练策略（GRPO、SFT、GRPO）。
- Result: 模型在实验中表现最优，生成可解释、可验证的推理链，并逐步细化时间估计。
- Conclusion: TAR-TVG框架通过显式监督和自蒸馏策略，显著提升了视频时序定位的性能和可解释性。


### [143] [Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing](https://arxiv.org/abs/2508.07700)
*Weitao Wang,Haoran Xu,Jun Meng,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出了一种无需调优、即插即用的方案，用于在单次推理中对编辑后的3D内容进行几何对齐，提升多视角一致性和网格质量。

- Motivation: 随着3D生成技术的发展，用户对个性化内容的需求增加，但现有编辑工具多为2D领域，直接应用于3D生成会导致信息丢失和质量下降。
- Method: 提出几何保留模块和注入切换器，通过原始法线潜在变量指导编辑后的多视角生成，并控制原始法线的监督程度。
- Result: 实验表明，该方法在多视角扩散模型和编辑方法的多种组合中，显著提升了编辑后3D资产的多视角一致性和网格质量。
- Conclusion: 该方法为3D内容编辑提供了一种高效且高质量的解决方案，无需额外调优即可实现几何对齐。


### [144] [Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2508.07701)
*Bo Jia,Yanan Guo,Ying Chang,Benkui Zhang,Ying Xie,Kangning Du,Lin Cao*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）在多视角场景中通过距离和法线约束优化表面重建，解决了单视角投影中的几何偏差问题。

- Motivation: 解决3DGS在单视角投影中几何法线对齐导致的邻近视角偏差问题，提升多视角场景下的几何深度一致性和重建精度。
- Method: 设计了多视角法线和距离引导的高斯泼溅方法，包括距离重投影正则化模块和法线增强模块，通过约束邻近深度图和法线对齐实现几何统一。
- Result: 实验结果表明，该方法在定量和定性评估中均优于基线，显著提升了3DGS的表面重建能力。
- Conclusion: 通过多视角约束优化，有效解决了3DGS在多视角场景中的几何偏差问题，实现了高精度重建。


### [145] [DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models](https://arxiv.org/abs/2508.07714)
*Licheng Zhang,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 提出了一种结合深度目标检测模型和大语言模型（LLM）的半自动化流程，用于构建多类别门检测数据集，显著降低标注成本。

- Motivation: 由于公开可用的细粒度多类别门检测数据集稀缺，而门检测在建筑合规检查和室内场景理解中至关重要，因此需要一种高效的数据集构建方法。
- Method: 使用深度目标检测模型统一检测门，再用LLM基于视觉和上下文特征分类，最后通过人工验证确保标签和边界框质量。
- Result: 该方法显著减少了标注成本，生成了适合用于基准测试的数据集。
- Conclusion: 结合深度学习和多模态推理的方法在复杂现实领域的数据集构建中具有潜力。


### [146] [A Registration-Based Star-Shape Segmentation Model and Fast Algorithms](https://arxiv.org/abs/2508.07721)
*Daoping Zhang,Xue-Cheng Tai,Lok Ming Lui*

Main category: cs.CV

TL;DR: 提出了一种基于配准框架的星形分割模型，结合水平集表示和约束，支持单中心或多中心的完整或部分星形分割，并通过实验验证了其有效性。

- Motivation: 图像分割在提取目标物体和识别边界时至关重要，但在遮挡、模糊或噪声情况下准确性受限。星形先验信息为解决这一问题提供了新思路。
- Method: 结合水平集表示与配准框架，对变形水平集函数施加约束，支持单中心或多中心的星形分割，并可指定边界通过特定地标位置。
- Result: 通过合成和真实图像的数值实验，证明了该方法在实现准确星形分割方面的有效性。
- Conclusion: 提出的星形分割模型在复杂图像条件下表现优异，为图像分割任务提供了新的解决方案。


### [147] [Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting](https://arxiv.org/abs/2508.07723)
*Ting Xiang,Changjian Chen,Zhuo Tang,Qifeng Zhang,Fei Lyu,Li Yang,Jiapeng Zhang,Kenli Li*

Main category: cs.CV

TL;DR: 论文提出TriReWeight方法，通过三重连接样本重加权增强生成数据增强，理论证明其性能不降且泛化接近最优，实验验证优于现有方法。

- Motivation: 解决生成模型在数据增强中因不可控生成和自然语言歧义导致的噪声图像问题。
- Method: 提出TriReWeight方法，基于理论分析的三重连接样本重加权。
- Result: 实验显示TriReWeight在六种自然图像数据集上平均提升7.9%，在三种医学数据集上平均提升3.4%。
- Conclusion: TriReWeight能有效提升生成数据增强性能，适用于多种生成方法。


### [148] [Grouped Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/abs/2508.07747)
*Junhyuk So,Juncheol Shin,Hyunho Kook,Eunhyeok Park*

Main category: cs.CV

TL;DR: 提出了一种无需训练的加速方法GSD，通过动态分组评估图像令牌，显著提升自回归图像模型的推理速度。

- Motivation: 自回归图像模型推理速度慢，传统推测解码方法未能充分利用图像令牌的冗余性和多样性。
- Method: 提出动态分组推测解码（GSD），评估视觉有效令牌簇而非单一目标令牌。
- Result: GSD平均加速3.7倍，且保持图像质量。
- Conclusion: GSD为自回归图像模型提供了一种高效、无需训练的加速方案。


### [149] [Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion](https://arxiv.org/abs/2508.07755)
*Minseo Kim,Minchan Kwon,Dongyeun Lee,Yunho Jeon,Junmo Kim*

Main category: cs.CV

TL;DR: 提出了一种无需额外信息的对比反演方法，用于从少量图像中提取共同概念，优于现有技术。

- Motivation: 现有方法依赖额外指导（如文本提示或空间掩码）提取共同概念，可能导致辅助特征分离不完全，影响生成质量。
- Method: 通过对比学习训练目标标记和图像辅助文本标记，提取解耦的真实语义，并应用解耦交叉注意力微调以提高概念保真度。
- Result: 实验表明，该方法在概念表示和编辑方面均表现优异。
- Conclusion: 对比反演方法在无需额外信息的情况下，实现了高质量的共同概念提取和编辑。


### [150] [Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild](https://arxiv.org/abs/2508.07759)
*Haoran Wang,Zekun Li,Jian Zhang,Lei Qi,Yinghuan Shi*

Main category: cs.CV

TL;DR: CAV-SAM通过将参考-目标图像对的对应关系表示为伪视频，利用SAM2的iVOS能力，以轻量级方式适应下游任务，性能提升超过5%。

- Motivation: 现有参考分割方法依赖元学习，需要大量数据和计算成本，CAV-SAM旨在以更轻量的方式适应下游任务。
- Method: 将参考-目标图像对的对应关系表示为伪视频，利用SAM2的iVOS能力，结合DBST模块（扩散模型构建语义转换序列）和TTGA模块（测试时几何对齐）。
- Result: 在广泛使用的数据集上，分割性能提升超过5%，优于现有SOTA方法。
- Conclusion: CAV-SAM是一种高效轻量的参考分割方法，显著提升了分割性能。


### [151] [UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models](https://arxiv.org/abs/2508.07766)
*Jinke Li,Jiarui Yu,Chenxing Wei,Hande Dong,Qiang Lin,Liangjing Yang,Zhicai Wang,Yanbin Hao*

Main category: cs.CV

TL;DR: 论文提出UniSVG数据集，用于训练多模态大语言模型（MLLM）以解决SVG的理解与生成任务，并展示了其性能超越现有模型。

- Motivation: AI在SVG的理解与生成（U&G）中面临高精度和多模态处理的挑战，需要统一的数据集和模型支持。
- Method: 构建了包含525k数据项的UniSVG数据集，支持从文本提示和图像生成SVG，并理解SVG的属性。
- Result: 使用该数据集训练的MLLM在SVG U&G任务上表现优异，超越GPT-4V等闭源模型。
- Conclusion: UniSVG数据集为SVG U&G任务提供了统一解决方案，推动了多模态模型在该领域的发展。


### [152] [Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation](https://arxiv.org/abs/2508.07769)
*Xiaoyan Liu,Kangrui Li,Jiaxin Liu*

Main category: cs.CV

TL;DR: Dream4D框架通过结合可控视频生成和神经4D重建，解决了4D内容合成中的时空一致性问题，显著提升了生成质量。

- Motivation: 当前方法在保持视角一致性和处理复杂场景动态方面存在困难，尤其是在大规模多元素交互环境中。
- Method: 采用两阶段架构：先通过少样本学习预测相机轨迹，再通过姿态条件扩散过程生成几何一致的多视角序列，最终转换为持久4D表示。
- Result: Dream4D在质量指标（如mPSNR、mSSIM）上优于现有方法。
- Conclusion: 该框架首次结合了视频扩散模型的时间先验和重建模型的几何感知能力，显著推动了4D生成技术的发展。


### [153] [Prototype-Guided Curriculum Learning for Zero-Shot Learning](https://arxiv.org/abs/2508.07771)
*Lei Wang,Shiming Chen,Guo-Sen Xie,Ziming Hong,Chaojian Yu,Qinmu Peng,Xinge You*

Main category: cs.CV

TL;DR: 提出了一种原型引导的课程学习框架（CLZSL），通过PCL模块解决实例级不匹配问题，通过PUP模块动态更新原型以减少类级不精确性，提升了零样本学习的效果。

- Motivation: 现有零样本学习中的语义原型通常是人工定义的，可能因实例级不匹配和类级不精确性导致噪声监督，影响知识迁移效果。
- Method: CLZSL框架包含PCL模块（优先学习高相似样本）和PUP模块（动态更新原型），以减少噪声监督。
- Result: 在AWA2、SUN和CUB数据集上验证了方法的有效性。
- Conclusion: CLZSL通过课程学习和动态原型更新，显著提升了零样本学习的性能。


### [154] [Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)](https://arxiv.org/abs/2508.07775)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经控制微分方程和$SO(3)$ Savitzky-Golay路径的3D旋转轨迹建模方法，解决了非保守力和噪声观测下的鲁棒性问题。

- Motivation: 解决$SO(3)$外推中的挑战，如未知物理量、非保守力及噪声观测下的状态估计。
- Method: 利用神经控制微分方程和$SO(3)$ Savitzky-Golay路径建模旋转轨迹，不依赖能量守恒假设。
- Result: 模型在仿真和实际场景中表现出鲁棒的外推能力，适用于复杂非惯性系统。
- Conclusion: 该方法为旋转轨迹建模提供了一种物理和几何意义明确且鲁棒的解决方案。


### [155] [GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences](https://arxiv.org/abs/2508.07782)
*Saihui Hou,Chenye Wang,Wenpeng Lang,Zhengxiang Lan,Yongzhen Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于片段（snippet）的步态识别方法，通过多尺度时间上下文学习更全面的步态特征，解决了传统集合和序列方法的局限性。

- Motivation: 传统步态识别方法（集合或序列）在短范围和长范围时间依赖上存在不足，本文受人类识别启发，提出将步态视为个体化动作的组合。
- Method: 将步态表示为随机选取的连续片段（snippet），提出片段采样和片段建模的关键组件，结合2D卷积网络进行识别。
- Result: 在四个常用数据集上验证有效性，例如在Gait3D和GREW上分别达到77.5%和81.7%的rank-1准确率。
- Conclusion: 片段方法展示了多尺度时间上下文学习的潜力，为步态识别提供了新视角。


### [156] [Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake](https://arxiv.org/abs/2508.07795)
*Hongrui Zheng,Yuezun Li,Liejun Wang,Yunfeng Diao,Zhiqing Guo*

Main category: cs.CV

TL;DR: 论文提出了一种双阶段防御框架（TSDF），通过双功能对抗扰动来持久防御深度伪造技术，防止攻击者通过重新训练模型绕过防御。

- Motivation: 现有主动防御策略缺乏持久性，攻击者可通过重新训练模型绕过防御，限制了实际应用。
- Method: 提出TSDF框架，利用强度分离机制设计双功能对抗扰动，既直接扭曲伪造结果，又通过毒化数据源阻止攻击者模型适应防御。
- Result: 实验表明，传统防御方法在对抗重新训练时性能急剧下降，而TSDF展现出强大的双重防御能力，提高了防御的持久性。
- Conclusion: TSDF通过双功能对抗扰动实现了持久防御，解决了现有防御策略的短效问题。


### [157] [Power Battery Detection](https://arxiv.org/abs/2508.07797)
*Xiaoqi Zhao,Peiqian Cao,Lihe Zhang,Zonglei Feng,Hanqi Liu,Jiaming Zuo,Youwei Pang,Weisi Lin,Georges El Fakhri,Huchuan Lu,Xiaofeng Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的任务——动力电池检测（PBD），旨在通过工业X射线图像定位阴极和阳极板的密集端点以进行质量检查。为解决传统方法的不足，作者提出了PBD5K基准数据集和MDCNeXt模型。

- Motivation: 动力电池内部结构缺陷可能引发严重安全问题，而传统的人工检测效率低且易出错，视觉算法难以应对密集排列、低对比度等问题。
- Method: 作者开发了PBD5K数据集，包含5000张X射线图像，并提出MDCNeXt模型，结合点、线、计数等多维结构信息，并引入两个状态空间模块以提高性能。
- Result: MDCNeXt模型通过任务特定提示和密度感知模块，显著提升了检测精度，并适应不同空间分布的电极位置。
- Conclusion: 该研究为动力电池检测提供了首个大规模基准数据集和高效模型，推动了该领域的发展。


### [158] [MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks](https://arxiv.org/abs/2508.07803)
*Yushen Xu,Xiaosong Li,Zhenyu Kuang,Xiaoqi Cheng,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: 本文提出MambaTrans，一种多模态融合图像模态翻译器，旨在解决可见图像与多模态融合图像之间的像素分布差异问题，提升下游任务性能。

- Motivation: 现有下游预训练模型通常基于可见图像训练，而多模态融合图像与可见图像的像素分布差异会降低下游任务性能，甚至不如仅使用可见图像。
- Method: 提出MambaTrans，利用多模态大语言模型的描述和语义分割模型的掩码作为输入，结合Multi-Model State Space Block（掩码-图像-文本交叉注意力和3D-Selective Scan模块）增强视觉能力。
- Result: 在公开数据集上的实验表明，MambaTrans有效提升了多模态图像在下游任务中的性能。
- Conclusion: MambaTrans通过结合多模态信息和先验知识，在不调整预训练模型参数的情况下，显著提升了多模态融合图像在下游任务中的表现。


### [159] [Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.07804)
*Bao Li,Xiaomei Zhang,Miao Xu,Zhaoxin Fan,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: Pose-RFT提出了一种基于强化学习的微调框架，用于从多模态输入生成3D人体姿态，通过混合动作强化学习优化语言预测和姿态生成。

- Motivation: 现有基于监督学习的多模态大语言模型在3D姿态生成任务中难以建模固有模糊性和任务对齐性。
- Method: 提出Pose-RFT框架，采用HyGRPO算法进行混合动作强化学习，结合任务特定奖励函数优化空间对齐和语义一致性。
- Result: 在多个姿态生成基准测试中显著优于现有方法。
- Conclusion: 混合动作强化微调对3D姿态生成任务有效。


### [160] [DiTVR: Zero-Shot Diffusion Transformer for Video Restoration](https://arxiv.org/abs/2508.07811)
*Sicheng Gao,Nancy Mehta,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: DiTVR是一种零样本视频修复框架，结合扩散变换器和轨迹感知注意力，通过光流轨迹对齐令牌，提升时间一致性。

- Motivation: 传统回归方法生成不真实细节且需大量配对数据，生成扩散模型难以保证时间一致性。
- Method: 引入扩散变换器与轨迹感知注意力，结合光流轨迹对齐令牌，动态选择相关令牌，流引导采样器注入数据一致性。
- Result: 在视频修复基准测试中达到零样本最优，表现出更好的时间一致性和细节保留。
- Conclusion: DiTVR在零样本视频修复中表现优越，对光流噪声和遮挡具有鲁棒性。


### [161] [Semi-supervised Multiscale Matching for SAR-Optical Image](https://arxiv.org/abs/2508.07812)
*Jingze Gai,Changchun Li*

Main category: cs.CV

TL;DR: 提出了一种半监督的SAR-光学图像匹配方法（S2M2-SAR），利用少量标注数据和大量未标注数据，通过伪标签和跨模态特征增强模块提升匹配效果。

- Motivation: 现有SAR-光学图像匹配方法依赖像素级标注数据，标注成本高且难以获取足够数据。
- Method: 结合深度和浅层匹配结果生成伪标签，利用跨模态特征增强模块分离模态共享和特定特征。
- Result: S2M2-SAR在基准数据集上表现优于现有半监督方法，与全监督方法竞争。
- Conclusion: S2M2-SAR高效且实用，解决了标注数据不足的问题。


### [162] [Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models](https://arxiv.org/abs/2508.07818)
*Chenyue Song,Chen Hui,Haiqi Zhu,Feng Jiang,Yachun Mi,Wei Zhang,Shaohui Liu*

Main category: cs.CV

TL;DR: 提出了一种名为RSFIQA的无参考图像质量评估模型，通过动态分割语义区域并结合多模态大语言模型，提升对局部质量变化的感知能力。

- Motivation: 现有NR-IQA方法在全局表征或区域特征权重分配上存在不足，无法充分捕捉语义显著区域或局部质量变化。
- Method: 利用SAM动态分割图像为语义区域，结合MLLM提取内容和感知失真，引入RSA机制聚合局部表征。
- Result: 在多个基准数据集上表现出鲁棒性和有效性，质量预测性能具有竞争力。
- Conclusion: RSFIQA通过细粒度区域级信息整合，显著提升了图像质量评估的准确性。


### [163] [Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP](https://arxiv.org/abs/2508.07819)
*Ke Ma,Jun Long,Hongxiao Fei,Liujie Hua,Yueyi Luo*

Main category: cs.CV

TL;DR: 论文提出了一种架构协同设计框架，通过卷积低秩适配器和动态融合网关改进视觉语言模型在零样本异常检测中的表现。

- Motivation: 预训练的视觉语言模型在零样本异常检测中存在适应性不足的问题，主要源于缺乏局部归纳偏置和特征融合方式的局限性。
- Method: 采用卷积低秩适配器（Conv-LoRA）注入局部归纳偏置，并引入动态融合网关（DFG）自适应调节文本提示，实现双向融合。
- Result: 在工业和医学基准测试中表现出更高的准确性和鲁棒性。
- Conclusion: 协同设计框架能有效改进基础模型在密集感知任务中的适应性。


### [164] [MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization](https://arxiv.org/abs/2508.07833)
*Animesh Jain,Alexandros Stergiou*

Main category: cs.CV

TL;DR: 提出MIMIC框架，通过可视化VLM内部表示来增强透明度和信任度。

- Motivation: VLMs的复杂架构难以解释，限制了透明度和信任。
- Method: 使用联合VLM反演和特征对齐目标，结合空间对齐、图像平滑和语义真实性的正则化。
- Result: 通过定量和定性评估，展示了MIMIC在视觉质量和语义度量上的表现。
- Conclusion: MIMIC是首个针对VLM视觉概念解释的反演方法。


### [165] [Effortless Vision-Language Model Specialization in Histopathology without Annotation](https://arxiv.org/abs/2508.07835)
*Jingna Qiu,Nishanth Jain,Jonas Ammeling,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: 本文研究了通过无标注的领域和任务相关图像-文本对继续预训练视觉语言模型（VLM），以提升其在组织病理学任务中的零样本和少样本性能。

- Motivation: 尽管通用设计的VLM在组织病理学中表现出色，但在特定任务中可能表现不佳，而监督微调需要人工标注数据。本文旨在探索无需标注的适应方法。
- Method: 通过从现有数据库中提取领域和任务相关的图像-文本对，对VLM（如CONCH和QuiltNet）进行继续预训练。
- Result: 实验表明，这种无标注的继续预训练显著提升了零样本和少样本性能，且在大规模训练时表现与少样本方法相当。
- Conclusion: 无标注的继续预训练是一种有效的、任务无关的VLM适应方法，适用于组织病理学任务。


### [166] [CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving](https://arxiv.org/abs/2508.07838)
*Qi Xiang,Kunsong Shi,Zhigui Lin,Lei He*

Main category: cs.CV

TL;DR: 提出了一种基于功能模块级别的分层解耦Mixture-of-Experts架构（CBDES MoE），用于提升多模态BEV感知系统的输入适应性、建模能力和泛化性能。

- Motivation: 现有多模态BEV方法存在输入适应性有限、建模能力受限和泛化性能不足的问题。
- Method: 采用异构专家网络和轻量级自注意力路由（SAR）门控机制，实现动态专家路径选择和稀疏高效推理。
- Result: 在nuScenes数据集上，CBDES MoE在3D目标检测中优于单专家基准模型，mAP提升1.6点，NDS提升4.1点。
- Conclusion: CBDES MoE是一种有效的模块化专家混合框架，为自动驾驶领域提供了实用优势。


### [167] [Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images](https://arxiv.org/abs/2508.07847)
*Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出了一种基于深度状态空间模型的Deep SWM方法，用于太阳耀斑预测，结合稀疏掩码自编码器和新预训练策略，性能优于基线方法和人类专家。

- Motivation: 太阳耀斑预测对关键基础设施保护至关重要，但现有方法在特征表示和长时序依赖建模上存在不足。
- Method: 采用多深度状态空间模型处理十通道太阳图像和长时空依赖，结合稀疏掩码自编码器和两阶段掩码预训练策略。
- Result: 在FlareBench基准测试中，性能优于基线方法和人类专家。
- Conclusion: Deep SWM为太阳耀斑预测提供了高效可靠的新方法，并开源了FlareBench基准。


### [168] [Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs](https://arxiv.org/abs/2508.07850)
*Noriko Nitta,Rei Miyata,Naoto Oishi*

Main category: cs.CV

TL;DR: 通过电子显微镜图像提取Ge表面的拓扑特征，使用图卷积网络嵌入，并通过PCA分析发现辐照角度对形态的影响大于辐照通量。

- Motivation: 研究离子束辐照对Ge表面微观结构形态的影响，特别是辐照角度和通量的差异。
- Method: 处理电子显微镜图像提取拓扑特征（骨架图），使用图卷积网络嵌入，并通过PCA和Davies-Bouldin指数分析聚类可分性。
- Result: 辐照角度对Ge表面形态的影响显著大于辐照通量。
- Conclusion: 辐照角度是调控Ge表面微观结构形态的关键因素。


### [169] [Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images](https://arxiv.org/abs/2508.07851)
*Konrad Reuter,Suresh Guttikonda,Sarah Latus,Lennart Maack,Christian Betz,Tobias Maurer,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 提出了一种基于2D TAP网络的无标记3D组织追踪方法，结合两个CoTracker模型，用于从立体内窥镜图像中估计3D运动。

- Motivation: 微创手术中动态组织运动和有限视野带来挑战，准确追踪组织可提升手术安全性并支持机器人辅助。
- Method: 结合两个CoTracker模型（时间追踪和立体匹配），利用立体内窥镜图像估计3D运动。
- Result: 在鸡组织模型上追踪误差低至1.1 mm（速度10 mm/s），效果优于合成模型。
- Conclusion: 基于TAP的模型在复杂手术场景中具有高精度无标记3D追踪潜力。


### [170] [Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model](https://arxiv.org/abs/2508.07863)
*Bin Cao,Sipeng Zheng,Ye Wang,Lujie Xia,Qianshan Wei,Qin Jin,Jing Liu,Zongqing Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Being-M0.5的实时可控视觉语言运动模型（VLMM），解决了现有模型在可控性方面的五大瓶颈，并基于HuMo100M数据集实现了卓越性能。

- Motivation: 现有视觉语言运动模型（VLMMs）在可控性方面存在显著不足，包括对多样化指令的响应不足、姿态初始化能力有限、长序列生成效果差、对未见场景处理不佳以及缺乏对身体部位的细粒度控制。
- Method: 基于HuMo100M数据集（包含500万自采集运动序列和1亿多任务指令实例），提出了一种新型部分感知残差量化技术，用于运动标记化，以实现对身体部位的精确控制。
- Result: Being-M0.5在多个运动生成任务中实现了最先进的性能，并通过效率分析验证了其实时能力。
- Conclusion: HuMo100M数据集和Being-M0.5模型为运动生成技术的实际应用提供了重要推动，未来研究可基于此进一步优化。


### [171] [CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/abs/2508.07871)
*Yanshu Li,Jianjiang Yang,Zhennan Shen,Ligong Han,Haoyan Xu,Ruixiang Tang*

Main category: cs.CV

TL;DR: 论文提出了一种针对多模态上下文学习的图像令牌剪枝方法CATP，通过两阶段渐进式剪枝减少冗余令牌，提升效率且性能略有提升。

- Motivation: 现有图像令牌剪枝方法仅适用于单图像任务，忽视了多模态上下文学习中更大的冗余和效率需求，导致性能下降。
- Method: 提出CATP方法，通过两阶段渐进式剪枝，充分处理输入序列中的跨模态交互。
- Result: 剪除77.8%的图像令牌后，CATP在多个模型和基准测试中平均性能提升0.6%，推理延迟降低10.78%。
- Conclusion: CATP提升了多模态上下文学习的实用性，为未来图像-文本交错场景的研究奠定了基础。


### [172] [Selective Contrastive Learning for Weakly Supervised Affordance Grounding](https://arxiv.org/abs/2508.07877)
*WonJun Moon,Hyun Seok Seong,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 论文提出了一种选择性原型和像素对比学习的方法，用于弱监督功能部件定位（WSAG），通过结合部分和对象级别的线索，提高功能部件的识别准确性。

- Motivation: 现有方法主要依赖分类器，容易关注与功能无关的类特定模式，而忽略了功能相关的部件。
- Method: 利用CLIP模型从不同视角（第一人称和第三人称）发现动作相关对象，并通过交叉参考挖掘功能部件线索，结合选择性原型和像素对比学习。
- Result: 实验证明，该方法能有效将激活从无关区域转移到功能相关区域，提高了功能部件的识别准确性。
- Conclusion: 提出的方法在弱监督功能部件定位任务中表现优异，代码已开源。


### [173] [TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal](https://arxiv.org/abs/2508.07878)
*Hanting Wang,Shengpeng Ji,Shulei Wang,Hai Huang,Xiao Jin,Qifei Zhang,Tao Jin*

Main category: cs.CV

TL;DR: 提出了一种参数高效的All-in-One图像修复框架，通过任务感知增强提示处理多种恶劣天气退化，显著减少参数开销。

- Motivation: 现有方法通常为每种退化类型设计专用模块或参数，导致参数开销大且忽略任务间相关性。
- Method: 采用两阶段训练范式（预训练和提示调优），利用任务感知增强提示和低秩分解捕捉任务通用与特定特征。
- Result: 实验表明，该方法仅用2.75M参数即实现优越性能。
- Conclusion: 任务感知增强提示提升了参数效率和任务建模准确性。


### [174] [NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction](https://arxiv.org/abs/2508.07897)
*Tianle Zeng,Junlei Hu,Gerardo Loza Galindo,Sharib Ali,Duygu Sarikaya,Pietro Valdastri,Dominic Jones*

Main category: cs.CV

TL;DR: 提出一种动态高斯点渲染技术，解决手术图像数据稀缺问题，生成高质量合成图像，提升模型性能。

- Motivation: 当前数据驱动方法依赖大量高质量标注数据，限制了手术数据科学的应用。
- Method: 采用动态高斯模型表示手术场景，结合动态训练调整策略和自动标注方法。
- Result: 生成照片级真实感合成图像（PSNR 29.87），模型性能提升15%。
- Conclusion: 动态高斯点渲染技术有效解决数据稀缺问题，显著提升手术自动化性能。


### [175] [Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation](https://arxiv.org/abs/2508.07901)
*Bowen Xue,Qixin Yan,Wenjing Wang,Hao Liu,Chen Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级、即插即用的视频生成框架Stand-In，用于身份保留，仅需少量参数和训练数据即可实现高质量视频生成。

- Motivation: 现有方法依赖过多训练参数且与其他AIGC工具兼容性差，需要一种更高效的身份保留视频生成方案。
- Method: 在预训练视频生成模型中引入条件图像分支，通过受限自注意力与条件位置映射实现身份控制，仅需2000对数据快速学习。
- Result: 仅增加约1%参数，视频质量和身份保留效果优于全参数训练方法，且可无缝集成到其他任务中。
- Conclusion: Stand-In框架高效、轻量，适用于多种视频生成任务，具有广泛的应用潜力。


### [176] [CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality](https://arxiv.org/abs/2508.07904)
*Marco Peer,Anna Scius-Bertrand,Andreas Fischer*

Main category: cs.CV

TL;DR: 本文提出了一种基于CTC对齐算法的自训练方法，用于解决历史文献手写文本识别中的标注错误问题，特别是在Bullinger信件集中的连字符问题。

- Motivation: 历史文献手写文本识别因书写变异性、文档退化及缺乏布局感知标注而具有挑战性。本文旨在解决Bullinger信件集中的标注错误问题。
- Method: 采用基于CTC对齐算法的自训练方法，利用动态规划和模型输出概率匹配全文转录与文本行图像。
- Result: 方法提升了性能（如CER提高1.1个百分点）并提高了对齐准确性，发现较弱模型反而能生成更准确的对齐。
- Conclusion: 该方法可迭代应用以进一步提升CER和对齐质量，同时发布了修正后的Bullinger数据集子集和代码。


### [177] [Generative Video Matting](https://arxiv.org/abs/2508.07905)
*Yongtao Ge,Kangyang Xie,Guangkai Xu,Mingyu Liu,Li Ke,Longtao Huang,Hui Xue,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 论文提出了一种解决视频抠图问题的新方法，通过大规模预训练和合成数据生成，结合视频扩散模型的先验知识，显著提升了模型的泛化能力和时间一致性。

- Motivation: 传统视频抠图方法因缺乏高质量真实数据而受限，现有数据集仅提供不完美的人工标注，导致模型在真实场景中泛化能力差。
- Method: 1. 利用大规模合成和伪标签分割数据集进行预训练；2. 开发可扩展的合成数据生成流程；3. 提出基于视频扩散模型的新架构，增强时间一致性。
- Result: 在三个基准数据集上表现优异，定性结果显示在多样真实场景中具有强泛化能力。
- Conclusion: 该方法通过预训练和新型架构设计，显著提升了视频抠图的性能和时间一致性，代码已开源。


### [178] [Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07908)
*Xudong Cai,Shuo Wang,Peng Wang,Yongcai Wang,Zhaoxin Fan,Wanting Li,Tianbao Zhang,Jianrong Tao,Yeying Jin,Deying Li*

Main category: cs.CV

TL;DR: Mem4D提出了一种双内存架构，分别处理静态结构和动态运动，解决了单目视频动态场景重建中的内存需求矛盾。

- Motivation: 现有方法在静态结构长期稳定性和动态运动高保真细节保留之间存在矛盾，导致重建效果不佳。
- Method: 设计了双内存架构：TDM捕获动态细节，PSM保留静态结构全局一致性。
- Result: 在基准测试中表现优异，同时保持高效性。
- Conclusion: Mem4D有效解决了动态场景重建中的内存需求问题，实现了高保真和全局一致性的平衡。


### [179] [RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering](https://arxiv.org/abs/2508.07918)
*Xing Zi,Jinghao Xiao,Yunxiao Shi,Xian Tao,Jun Li,Ali Braytee,Mukesh Prasad*

Main category: cs.CV

TL;DR: 论文介绍了RSVLM-QA数据集，一个用于遥感视觉问答（VQA）的大规模、内容丰富的数据集，旨在解决现有数据集的局限性。

- Motivation: 现有遥感VQA数据集在标注丰富性、问题多样性和特定推理能力评估方面存在不足，需要更全面的数据集推动研究。
- Method: 通过整合多个遥感数据集，采用双轨标注生成流程：利用GPT-4.1生成详细标注和复杂问题，并开发专门流程处理对象计数问题。
- Result: RSVLM-QA包含13,820张图像和162,373个VQA对，标注丰富且问题多样，实验表明其能有效评估主流视觉语言模型。
- Conclusion: RSVLM-QA将成为遥感VQA和视觉语言模型研究的重要资源，推动领域发展。


### [180] [Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection](https://arxiv.org/abs/2508.07923)
*Jakub Binda,Valentina Paneta,Vasileios Eleftheriadis,Hongkyou Chung,Panagiotis Papadimitroulas,Neo Christopher Chung*

Main category: cs.CV

TL;DR: 论文提出了一种混合异常检测框架，用于增强生成式AI在核医学数据合成中的可靠性和安全性，并展示了两个应用案例。

- Motivation: 生成式AI在核医学中具有巨大潜力，但高风险的生物医学影像需要可靠的异常检测机制以确保模型行为的正确性。
- Method: 开发并实施了一种混合异常检测框架，应用于Pose2Xray和DosimetrEYE两个系统，分别用于生成合成X射线和估计3D辐射剂量分布。
- Result: 异常检测提高了系统的可靠性，减少了人工监督需求，并支持实时质量控制。
- Conclusion: 该方法增强了生成式AI在临床前环境中的工业可行性，提高了鲁棒性、可扩展性和法规合规性。


### [181] [TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding](https://arxiv.org/abs/2508.07925)
*Jin-Seop Lee,SungJoon Lee,Jaehan Ahn,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 论文提出了一种零样本视频时间定位方法TAG，通过时间池化、时间相干聚类和相似性调整，解决了现有方法的语义碎片化和相似性分布偏差问题，无需训练且不依赖LLMs。

- Motivation: 现有零样本视频时间定位方法存在语义碎片化和相似性分布偏差问题，且依赖昂贵的LLMs推理。
- Method: 提出TAG方法，结合时间池化、时间相干聚类和相似性调整，捕捉视频时间上下文并修正相似性分布。
- Result: 在Charades-STA和ActivityNet Captions数据集上达到最优性能。
- Conclusion: TAG是一种简单有效的零样本视频时间定位方法，解决了现有方法的局限性。


### [182] [VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security](https://arxiv.org/abs/2508.07960)
*Ajnas Muhammed,Iurri Medvedev,Nuno Gonçalves*

Main category: cs.CV

TL;DR: VOIDFace是一个新型面部识别框架，通过视觉秘密共享消除数据复制，提升数据控制，同时提出基于补丁的多训练网络，实现隐私保护的面部识别。

- Motivation: 现代面部识别系统依赖大规模数据集，但数据复制和管理问题导致隐私和伦理问题。
- Method: 使用视觉秘密共享存储训练数据，提出基于补丁的多训练网络。
- Result: 在VGGFace2数据集上验证，VOIDFace在保持性能的同时提供数据控制、安全和隐私。
- Conclusion: VOIDFace通过技术改进解决了数据隐私和控制问题，为面部识别系统提供了更安全的解决方案。


### [183] [TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking](https://arxiv.org/abs/2508.07968)
*Tony Danjun Wang,Christian Heiliger,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: TrackOR框架通过3D几何特征实现手术室中多人员长期跟踪与重识别，提升关联准确性11%，并支持离线轨迹恢复，为个性化智能系统提供基础。

- Motivation: 为手术团队提供智能支持，改善患者结果，需解决手术室中人员长期身份跟踪的计算挑战。
- Method: 提出TrackOR框架，利用3D几何特征进行在线跟踪与离线轨迹恢复。
- Result: 关联准确性提升11%，实现持久身份跟踪，支持个性化智能系统。
- Conclusion: TrackOR为手术室个性化智能支持开辟新途径，如团队效率与安全分析。


### [184] [Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation](https://arxiv.org/abs/2508.07981)
*Fangyuan Mao,Aiming Hao,Jintao Chen,Dongxia Liu,Xiaokun Feng,Jiashu Zhu,Meiqi Wu,Chubin Chen,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: Omni-Effects提出了一种统一框架，支持提示引导的效果生成和空间可控的复合效果，解决了当前方法在多效果生成中的干扰和空间不可控问题。

- Motivation: 当前视频生成模型在VFX生产中受限于单效果生成，无法满足多效果空间可控的需求。
- Method: 提出了LoRA-MoE和SAP两项创新，结合IIF模块，实现了多效果的统一生成和精确空间控制。
- Result: 实验表明，Omni-Effects能精确控制效果类别和位置，支持多样化的复合效果生成。
- Conclusion: Omni-Effects为VFX生产提供了高效、可控的解决方案，推动了多效果生成技术的发展。


### [185] [The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility](https://arxiv.org/abs/2508.07989)
*Xiantao Zhang*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在盲人和视障人士（BVI）辅助技术中潜力巨大，但存在‘电梯问题’——无法感知电梯运行方向，揭示了‘隐式运动盲区’的深层限制。

- Motivation: 揭示当前MLLMs在动态环境中的局限性，尤其是对连续低信号运动的感知不足，影响用户信任。
- Method: 通过‘电梯问题’作为典型案例，分析现有视频理解框架（基于离散帧采样）的不足。
- Result: 指出隐式运动盲区对用户信任的负面影响，并呼吁开发更注重物理感知的模型和评测标准。
- Conclusion: 呼吁从语义识别转向物理感知的范式转变，强调以用户需求为中心的安全性和可靠性。


### [186] [Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models](https://arxiv.org/abs/2508.07996)
*Thinesh Thiyakesan Ponbagavathi,Chengzheng Yang,Alina Roitberg*

Main category: cs.CV

TL;DR: ProGraD是一种基于Vision Foundation Models（VFMs）的群体活动检测方法，通过可学习的群体提示和轻量级GroupContext Transformer提升性能，尤其在多群体场景中表现优异。

- Motivation: 现有VFMs在群体动态建模方面未充分探索，直接替换CNN骨干网络效果有限，需要结构化、群体感知的推理方法。
- Method: ProGraD结合可学习的群体提示和两层的GroupContext Transformer，指导VFM注意力并推断演员-群体关联及集体行为。
- Result: 在Cafe和Social-CAD基准测试中超越现有方法，多群体场景下提升6.5%（Group mAP@1.0）和8.2%（Group mAP@0.5）。
- Conclusion: ProGraD不仅性能优越，还能生成可解释的注意力图，为群体推理提供洞察。


### [187] [Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition](https://arxiv.org/abs/2508.08004)
*Anqi Xiao,Weichen Yu,Hongyuan Yu*

Main category: cs.CV

TL;DR: 提出了一种名为SRA的搜索自由自动数据增强方法，通过动态调整增强策略和启发式评分模块，显著提升了性能。

- Motivation: 主流AutoDA方法存在搜索耗时或性能不足的问题，SRA旨在解决这些挑战。
- Method: SRA采用不对称、搜索自由的方法，结合启发式评分模块和动态调整策略。
- Result: 在ImageNet上达到78.31%的Top-1准确率，并展示出良好的兼容性和泛化能力。
- Conclusion: SRA为更简单、有效且实用的AutoDA设计提供了新方向。


### [188] [Mitigating Biases in Surgical Operating Rooms with Geometry](https://arxiv.org/abs/2508.08028)
*Tony Danjun Wang,Tobias Czempiel,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: 论文探讨了深度学习模型在手术室（OR）中因学习虚假相关性而导致的偏差问题，提出通过3D点云序列编码人员信息，以分离身份相关的形状和运动模式，从而避免外观混淆。

- Motivation: 手术室中标准化服装（如手术服）掩盖了识别特征，导致模型依赖无关视觉线索（如鞋子或眼镜），影响任务准确性。需要开发更鲁棒的方法以准确建模人员特征。
- Method: 采用梯度显著性分析揭示CNN模型的偏差，并提出基于3D点云序列的几何表示方法，分离形状和运动特征。
- Result: 实验显示，在现实临床环境中，RGB模型性能下降12%，而几何方法表现更稳定，表明几何特征能捕捉更有意义的生物特征。
- Conclusion: 几何表示能有效避免外观混淆，为手术室中人员建模提供了更鲁棒的方法。


### [189] [TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation](https://arxiv.org/abs/2508.08038)
*Huawei Sun,Zixu Wang,Hao Feng,Julius Ott,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 论文提出TRIDE算法，结合雷达-相机融合和文本特征提取，通过天气感知模块优化深度估计性能，在KITTI和nuScenes数据集上表现优于现有方法。

- Motivation: 现有雷达-相机融合算法未考虑天气条件对传感器性能的影响，且未充分利用语言描述辅助深度估计。
- Method: 提出文本生成策略和特征融合技术，结合雷达点信息优化文本特征提取，并设计天气感知融合模块动态调整雷达权重。
- Result: 在nuScenes数据集上，MAE和RMSE分别提升12.87%和9.08%。
- Conclusion: TRIDE算法通过多模态融合和天气适应机制，显著提升了深度估计的准确性和鲁棒性。


### [190] [S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix](https://arxiv.org/abs/2508.08048)
*Peng Dai,Feitong Tan,Qiangeng Xu,Yihua Huang,David Futschik,Ruofei Du,Sean Fanello,Yinda Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出了一种无需姿态估计和额外训练的方法，利用现成的单目视频生成模型生成沉浸式3D视频。

- Motivation: 现有视频生成模型在生成高质量单目视频方面表现优异，但生成3D立体和空间视频的研究较少。
- Method: 通过估计深度信息将单目视频映射到预定义视角，并采用帧矩阵修复框架合成缺失内容，同时提出双更新方案提升修复质量。
- Result: 实验表明，该方法在多种生成模型（如Sora、Lumiere等）上显著优于现有方法。
- Conclusion: 该方法为生成沉浸式3D视频提供了一种高效且无需额外训练的解决方案。


### [191] [PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI](https://arxiv.org/abs/2508.08058)
*Ziad Al-Haj Hemidi,Eytan Kats,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: PrIINeR是一种结合预训练深度学习模型和隐式神经表示（INR）的MRI重建方法，显著提升高加速因子下的图像质量。

- Motivation: 加速MRI扫描会降低图像质量，现有INR方法在高加速因子下表现不佳，导致结构丢失和伪影。
- Method: PrIINeR将预训练模型的先验知识融入INR框架，结合实例优化和双重数据一致性，确保与k空间数据和先验重建对齐。
- Result: 在NYU fastMRI数据集上，PrIINeR优于现有INR和部分学习型方法，显著提升结构保留和保真度，有效消除伪影。
- Conclusion: PrIINeR结合深度学习和INR技术，为高质量加速MRI重建提供了更可靠的解决方案。


### [192] [Investigating the Design Space of Visual Grounding in Multimodal Large Language Model](https://arxiv.org/abs/2508.08066)
*Weitai Kang,Weiming Zhuang,Zhizhong Li,Yan Yan,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 本文研究了多模态大语言模型（MLLMs）在视觉定位（VG）任务中的设计选择，通过系统验证优化了性能。

- Motivation: 现有方法在MLLMs的视觉定位任务中设计选择不一致且缺乏系统验证，本文旨在填补这一空白。
- Method: 使用LLaVA-1.5模型，分析不同视觉定位范式和数据设计对VG性能的影响。
- Result: 优化后的MLLM在RefCOCO/+/g数据集上性能提升了5.6%/6.9%/7.0%。
- Conclusion: 研究为MLLMs的视觉定位任务提供了有效的设计指导，显著提升了性能。


### [193] [Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition](https://arxiv.org/abs/2508.08069)
*Xiaoxiao Cui,Yiran Li,Kai He,Shanzhi Jiang,Mengli Xue,Wentao Li,Junhong Leng,Zhi Liu,Lizhen Cui,Shuo Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于信息瓶颈的因果注意力方法（IBCA），用于医学图像的多标签分类，通过高斯混合多标签空间注意力过滤无关信息，并通过对比增强因果干预减少虚假注意力。

- Motivation: 现有方法在医学图像多标签分类中难以有效学习类别特异性特征，且容易受到无关特征的干扰。
- Method: 提出结构因果模型（SCM）将类别特异性注意力分解为因果、虚假和噪声因素，并设计IBCA方法，结合高斯混合多标签空间注意力和对比增强因果干预。
- Result: 在Endo和MuReD数据集上，IBCA在多项指标上显著优于其他方法，如CR、OR和mAP等。
- Conclusion: IBCA能有效学习类别特异性注意力，提升医学图像多标签分类的准确性和可解释性。


### [194] [ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness](https://arxiv.org/abs/2508.08082)
*Zizheng Guo,Bochao Zou,Junbao Zhuo,Huimin Ma*

Main category: cs.CV

TL;DR: 本文提出了两种基于状态空间模型的架构（ME-TST和ME-TST+），用于微表情（ME）的检测与识别，通过视频级回归和时序状态转换机制改进传统滑动窗口分类的局限性，并引入多粒度ROI建模和慢快Mamba框架提升性能。

- Motivation: 传统深度学习方法在微表情分析中存在固定窗口长度和硬分类的局限性，且将检测与识别视为独立任务，忽略了其内在联系。
- Method: 提出ME-TST和ME-TST+架构，利用时序状态转换机制实现视频级回归，引入多粒度ROI建模和慢快Mamba框架，并设计特征和结果层面的协同策略。
- Result: 实验表明，所提方法在性能上达到最优。
- Conclusion: ME-TST和ME-TST+通过改进时序动态建模和协同策略，显著提升了微表情分析的准确性和效率。


### [195] [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086)
*Zhongqi Yang,Wenhang Ge,Yuqi Li,Jiaqi Chen,Haoyuan Li,Mengyin An,Fei Kang,Hua Xue,Baixin Xu,Yuyang Yin,Eric Li,Yang Liu,Yikai Wang,Hao-Xiang Guo,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-3D框架通过全景表示和条件视频生成技术，实现高质量、几何一致的3D世界生成，结合了前馈和优化两种重建方法。

- Motivation: 现有方法在生成3D场景时范围有限，Matrix-3D旨在通过全景视频扩散模型和3D重建技术解决这一问题。
- Method: 训练轨迹引导的全景视频扩散模型，结合场景网格渲染作为条件，提出前馈和优化两种3D重建方法。
- Result: 在全景视频生成和3D世界生成任务中达到最先进性能。
- Conclusion: Matrix-3D框架通过全景表示和两种重建方法，显著提升了3D场景生成的覆盖范围和质量。


### [196] [MDD-Net: Multimodal Depression Detection through Mutual Transformer](https://arxiv.org/abs/2508.08093)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了一种基于多模态数据的抑郁症检测网络（MDD-Net），利用社交媒体的声学和视觉数据，通过互Transformer高效提取和融合特征，显著提升了检测性能。

- Motivation: 抑郁症严重影响身心健康，社交媒体数据易于收集，适合用于心理健康研究。
- Method: MDD-Net包含声学特征提取、视觉特征提取、互Transformer特征融合和检测层四个模块。
- Result: 在D-Vlog数据集上的实验表明，MDD-Net的F1-Score比现有方法高出17.37%。
- Conclusion: MDD-Net在多模态抑郁症检测中表现出色，性能优于现有技术。


### [197] [3D Plant Root Skeleton Detection and Extraction](https://arxiv.org/abs/2508.08094)
*Jiakai Lin,Jinchang Zhang,Ge Jin,Wenzhan Song,Tianming Liu,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出了一种从少量图像中高效提取植物根系3D骨架的方法，解决了根系复杂结构和缺乏纹理信息的问题，验证了模型的有效性，并展示了其在自动化育种机器人中的应用潜力。

- Motivation: 植物根系结构复杂且缺乏纹理信息，传统2D研究难以满足需求，3D根系表型信息对研究遗传性状和根系发育至关重要。
- Method: 包括侧根检测与匹配、三角测量提取侧根骨架结构，以及侧根与主根整合。
- Result: 提取的3D根系骨架与真实结构高度相似，验证了模型的有效性。
- Conclusion: 该方法可提升自动化育种机器人的效率，通过精确分析3D根系结构，帮助筛选优良种子，推动现代农业智能化发展。


### [198] [TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning](https://arxiv.org/abs/2508.08098)
*Junzhe Xu,Yuyang Yin,Xi Chen*

Main category: cs.CV

TL;DR: TBAC-UniImage是一种新型多模态理解与生成统一模型，通过深度集成预训练扩散模型与多模态大语言模型（MLLM），解决了现有方法的浅层连接和高计算成本问题。

- Motivation: 现有扩散统一模型存在两种局限：一是仅利用MLLM的最终隐藏状态作为生成条件，导致浅层连接；二是从头预训练统一架构计算成本高。本文旨在探索更高效、更深层次的统一方法。
- Method: 利用MLLM多个不同层的表征作为扩散模型的生成条件，将预训练生成器视为“梯子”，从MLLM理解过程的多层次中获取指导。
- Result: TBAC-UniImage实现了更深层次、更细粒度的理解与生成统一。
- Conclusion: 通过多层次表征的深度集成，TBAC-UniImage为多模态统一模型提供了更高效且性能优越的解决方案。


### [199] [Hyperspectral Imaging](https://arxiv.org/abs/2508.08107)
*Danfeng Hong,Chenyu Li,Naoto Yokoya,Bing Zhang,Xiuping Jia,Antonio Plaza,Paolo Gamba,Jon Atli Benediktsson,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 这篇论文概述了高光谱成像（HSI）的原理、技术、分析方法和应用，并探讨了当前挑战与未来发展方向。

- Motivation: HSI能够同时捕捉空间和光谱信息，为非侵入性、无标记的材料、化学和生物分析提供了强大工具，但其硬件、数据复杂性和应用多样性仍存在挑战。
- Method: 论文总结了HSI的物理原理、传感器架构、数据采集与校准步骤，以及经典和现代分析方法（如降维、分类、光谱解混和深度学习）。
- Result: HSI在多个领域（如地球观测、精准农业、生物医学）展示了其揭示亚视觉特征的能力，但也面临硬件限制和数据复杂性等挑战。
- Conclusion: 未来HSI的发展方向包括实时嵌入式系统、自监督学习和基础模型，有望成为跨学科通用平台，推动科学与社会变革。


### [200] [GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking](https://arxiv.org/abs/2508.08117)
*Xudong Han,Pengcheng Fang,Yueying Tian,Jianhui Yu,Xiaohao Cai,Daniel Roggen,Philip Birch*

Main category: cs.CV

TL;DR: GRASPTrack是一种新颖的深度感知多目标跟踪框架，通过结合单目深度估计和实例分割，解决了传统跟踪方法在遮挡和深度模糊问题上的不足。

- Motivation: 传统基于检测的跟踪方法缺乏几何感知能力，难以处理遮挡和深度模糊问题。
- Method: GRASPTrack将单目深度估计和实例分割集成到标准跟踪流程中，生成3D点云，并采用体素化3D IoU进行空间关联，同时引入深度感知自适应噪声补偿和深度增强的观测中心动量。
- Result: 在MOT17、MOT20和DanceTrack基准测试中表现优异，显著提升了复杂场景下的跟踪鲁棒性。
- Conclusion: GRASPTrack通过3D几何推理和深度感知技术，有效解决了遮挡和复杂运动模式下的多目标跟踪问题。


### [201] [Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control](https://arxiv.org/abs/2508.08134)
*Zeqian Long,Mingzhe Zheng,Kunyu Feng,Xinhua Zhang,Hongyu Liu,Harry Yang,Linfeng Zhang,Qifeng Chen,Yue Ma*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练和掩码的图像编辑框架Follow-Your-Shape，通过轨迹差异图（TDM）和计划KV注入机制实现精确的形状编辑。

- Motivation: 现有流式图像编辑模型在大规模形状变换任务中表现不佳，容易改变非目标区域或无法实现预期形状变化。
- Method: 通过比较反演和去噪路径的令牌速度差异计算TDM，定位可编辑区域，并采用计划KV注入机制确保稳定编辑。
- Result: 实验表明，该方法在形状替换任务中具有更高的编辑能力和视觉保真度。
- Conclusion: Follow-Your-Shape框架在保持非目标内容的同时，实现了精确可控的形状编辑。


### [202] [FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting](https://arxiv.org/abs/2508.08136)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Huajie Wang,Shuting He*

Main category: cs.CV

TL;DR: FantasyStyle是一个基于3DGS的风格迁移框架，首次完全依赖扩散模型蒸馏，解决了多视图不一致性和VGG特征依赖问题。

- Motivation: 当前3DGS风格迁移方法存在多视图不一致导致风格冲突，以及VGG特征难以分离风格和内容的问题。
- Method: 提出多视图频率一致性和可控风格蒸馏，通过3D滤波和负引导优化3D高斯分布。
- Result: 实验表明，该方法在风格质量和视觉真实感上优于现有技术。
- Conclusion: FantasyStyle通过创新方法有效提升了3D风格迁移的效果。


### [203] [Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization](https://arxiv.org/abs/2508.08141)
*Nicholas Klein,Hemlata Tak,James Fullwood,Krishna Regmi,Leonidas Spinoulas,Ganesh Sivaraman,Tianxiang Chen,Elie Khoury*

Main category: cs.CV

TL;DR: 本文提出了针对深度伪造视频分类和定位的解决方案，在ACM 1M Deepfakes Detection Challenge中表现出色。

- Motivation: 随着视觉和音频生成技术的快速发展，检测合成内容的需求日益迫切，尤其是针对细粒度修改的挑战。
- Method: 提交的方法在ACM 1M Deepfakes Detection Challenge中进行了测试。
- Result: 在时间定位任务中表现最佳，在分类任务中排名前四。
- Conclusion: 提出的解决方案在检测深度伪造视频方面具有显著效果。


### [204] [Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2508.08165)
*Yan Wang,Da-Wei Zhou,Han-Jia Ye*

Main category: cs.CV

TL;DR: 论文提出了一种结合任务特定和通用适配器（TUNA）的方法，用于解决类增量学习中的模块选择问题和共享知识利用不足问题。

- Motivation: 现有基于预训练模型的类增量学习方法通常冻结预训练网络，并使用轻量级模块（如适配器）适应增量任务，但模块选择错误和任务特定模块忽视共享知识会导致性能下降。
- Method: 训练任务特定适配器捕获关键特征，引入基于熵的选择机制选择最佳适配器，并通过适配器融合策略构建通用适配器，结合两者预测以利用专业和通用知识。
- Result: 在多个基准数据集上的实验表明，该方法达到了最先进的性能。
- Conclusion: TUNA方法有效解决了类增量学习中的模块选择和知识共享问题，显著提升了性能。


### [205] [ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction](https://arxiv.org/abs/2508.08170)
*Chaojun Ni,Guosheng Zhao,Xiaofeng Wang,Zheng Zhu,Wenkang Qin,Xinze Chen,Guanghong Jia,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: ReconDreamer-RL框架通过结合视频扩散先验和动力学模型，缩小仿真与现实的差距，提升端到端自动驾驶训练效果。

- Motivation: 现有仿真环境与真实世界差异大，且受限于训练数据分布，难以处理新轨迹或极端场景。
- Method: 提出ReconSimulator（视频扩散先验+动力学模型）和Dynamic Adversary Agent（DAA）生成极端场景，Cousin Trajectory Generator（CTG）解决数据分布偏差。
- Result: 实验显示ReconDreamer-RL优于模仿学习方法，碰撞率降低5倍。
- Conclusion: 该框架有效缩小sim2real差距，提升自动驾驶训练性能。


### [206] [CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data](https://arxiv.org/abs/2508.08173)
*Chongke Bi,Xin Gao,Jiangkang Deng,Guan*

Main category: cs.CV

TL;DR: CD-TVD结合对比学习和改进的扩散模型，仅需少量高分辨率数据即可实现3D超分辨率，显著降低对大规模数据集的依赖。

- Motivation: 大规模科学模拟生成高分辨率时变数据成本高，现有超分辨率方法依赖大量高分辨率训练数据，限制了其适用性。
- Method: CD-TVD框架通过对比学习编码器和改进的扩散模型（含局部注意力机制），利用历史模拟数据预训练，仅需一个高分辨率时间步微调。
- Result: 在流体和大气模拟数据集上，CD-TVD实现了准确且资源高效的3D超分辨率。
- Conclusion: CD-TVD为大规模科学模拟的数据增强提供了显著进步，代码已开源。


### [207] [MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision](https://arxiv.org/abs/2508.08177)
*Zhonghao Yan,Muxi Diao,Yuxuan Yang,Jiayuan Xu,Kaizhou Zhang,Ruoyan Jing,Lele Yang,Yanxi Liu,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 论文提出了一种名为UMRG的新任务，结合临床推理和像素级定位，并发布了包含14K样本的数据集U-MRG-14K。同时，提出了MedReasoner框架，通过强化学习优化推理模块，实现了在医学定位任务上的最先进性能。

- Motivation: 当前医学定位方法依赖显式空间提示，无法处理临床实践中的隐式查询，因此需要一种结合临床推理和像素级定位的新方法。
- Method: 提出了MedReasoner框架，将推理与分割分离：通过强化学习优化MLLM推理模块，使用冻结的分割专家生成掩码，并通过格式和准确性奖励实现对齐。
- Result: MedReasoner在U-MRG-14K数据集上实现了最先进的性能，并能泛化到未见过的临床查询。
- Conclusion: 强化学习在可解释的医学定位任务中具有显著潜力，MedReasoner框架为临床实践提供了有效支持。


### [208] [3D Human Mesh Estimation from Single View RGBD](https://arxiv.org/abs/2508.08178)
*Ozhan Suat,Bedirhan Uguz,Batuhan Karagoz,Muhammed Can Keles,Emre Akbas*

Main category: cs.CV

TL;DR: 提出了一种利用RGBD相机进行3D人体网格估计的方法，通过虚拟投影和掩码自编码器解决数据稀缺问题，性能优于现有方法。

- Motivation: RGBD相机虽提供深度数据，但在3D人体网格估计中未充分利用，且现有数据集规模小、多样性不足。
- Method: 利用MoCap数据集生成虚拟单视角深度数据，训练掩码自编码器完成部分网格，推理时匹配深度数据生成完整网格。
- Result: 在SURREAL、CAPE和BEHAVE数据集上表现优异，PVE误差分别为16.8 mm、22.0 mm和70.9 mm。
- Conclusion: 深度数据显著提升3D人体网格估计性能，方法优于现有技术。


### [209] [PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation](https://arxiv.org/abs/2508.08179)
*Sihan Zhao,Zixuan Wang,Tianyu Luan,Jia Jia,Wentao Zhu,Jiebo Luo,Junsong Yuan,Nan Xi*

Main category: cs.CV

TL;DR: 论文提出了一种物理标注方法PP-Motion，用于评估人体运动的物理和感知保真度，通过最小修改量计算运动与物理定律的对齐程度，并结合人类感知损失，优于现有方法。

- Motivation: 人体运动生成在多个领域有广泛应用，但现有评估方法在物理可行性与人类感知之间存在差距，且主观标注限制了数据驱动指标的鲁棒性。
- Method: 提出物理标注方法，计算运动与物理定律的最小修改量；设计PP-Motion指标，结合皮尔逊相关损失和人类感知损失。
- Result: PP-Motion在物理对齐和人类感知保真度上均优于现有方法。
- Conclusion: PP-Motion为运动保真度评估提供了更客观和细粒度的解决方案。


### [210] [THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening](https://arxiv.org/abs/2508.08183)
*Hongkun Jin,Hongcheng Jiang,Zejun Zhang,Yuan Zhang,Jia Fu,Tingfeng Li,Kai Luo*

Main category: cs.CV

TL;DR: 论文提出了一种名为THAT的Transformer框架，通过改进高频特征表示和令牌选择，解决了现有方法在冗余令牌表示和多尺度特征建模上的不足，提升了高光谱图像融合的性能。

- Motivation: 现有的Transformer方法在高光谱图像融合中存在冗余令牌表示和多尺度特征建模不足的问题，且难以保留高频成分（如边缘和纹理）。
- Method: 提出了THAT框架，包括Pivotal Token Selective Attention（PTSA）以选择关键令牌，以及Multi-level Variance-aware Feed-forward Network（MVFN）以增强高频细节学习。
- Result: 在标准基准测试中，THAT实现了最先进的性能，重建质量和效率均有所提升。
- Conclusion: THAT通过改进高频特征表示和令牌选择，显著提升了高光谱图像融合的效果。


### [211] [KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning](https://arxiv.org/abs/2508.08186)
*Md Meftahul Ferdaus,Mahdi Abdelguerfi,Elias Ioup,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: KARMA是一种高效的语义分割框架，通过一维函数组合建模复杂缺陷模式，显著减少参数数量，适用于实时基础设施检测。

- Motivation: 解决现有深度学习方法参数过多、不适用于实时检测的问题，同时应对缺陷外观多变、成像条件恶劣和类别不平衡的挑战。
- Method: 引入KARMA框架，包括TiKAN模块、优化的特征金字塔结构和静态-动态原型机制，实现高效多尺度缺陷分析。
- Result: KARMA在参数减少97%的情况下，性能与现有方法相当或更优，推理速度适合实时部署。
- Conclusion: KARMA为基础设施自动化检测提供了高效、实用的解决方案，兼顾准确性和实时性。


### [212] [Reinforcement Learning in Vision: A Survey](https://arxiv.org/abs/2508.08189)
*Weijia Wu,Chen Gao,Joya Chen,Kevin Qinghong Lin,Qingwei Meng,Yiming Zhang,Yuke Qiu,Hong Zhou,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文综述了视觉强化学习（RL）的最新进展，总结了200多篇代表性工作，并提出了未来研究方向。

- Motivation: 整合视觉智能与强化学习的最新成果，为研究者和从业者提供领域概览。
- Method: 将视觉RL问题形式化，分析策略优化方法的演变，并将相关工作分为四大主题支柱。
- Result: 总结了算法设计、奖励工程和基准测试的进展，并提出了开放挑战。
- Conclusion: 为视觉RL领域提供了清晰的路线图，并指出了未来研究的潜在方向。


### [213] [Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model](https://arxiv.org/abs/2508.08199)
*Peiqi He,Zhenhao Zhang,Yixiang Zhang,Xiongjun Zhao,Shaoliang Peng*

Main category: cs.CV

TL;DR: 提出了一种名为Spatial-ORMLLM的模型，仅使用RGB模态进行3D空间推理，解决了手术室中多模态数据难以获取的问题，并在多个临床数据集上表现优异。

- Motivation: 手术室中的精确空间建模对临床任务至关重要，但现有方法依赖多模态数据且忽略3D能力，导致数据获取困难和细节捕捉不足。
- Method: 引入Spatial-ORMLLM模型，通过Spatial-Enhanced Feature Fusion Block将2D输入与3D空间知识结合，采用端到端框架进行推理。
- Result: 在多个临床数据集上达到最优性能，并能泛化到未见的手术场景和任务。
- Conclusion: Spatial-ORMLLM为手术室3D空间推理提供了一种高效且无需额外标注或传感器的解决方案。


### [214] [SAGOnline: Segment Any Gaussians Online](https://arxiv.org/abs/2508.08219)
*Wentao Sun,Quanyun Wu,Hanqing Xu,Kyle Gao,Zhengsen Xu,Yiping Chen,Dedong Zhang,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: SAGOnline是一个轻量级、零样本的实时3D分割框架，通过解耦策略和GPU加速算法解决了3D高斯场景中的分割与跟踪问题。

- Motivation: 当前方法在3D高斯场景分割中存在计算成本高、空间推理能力有限及无法同时跟踪多对象的问题。
- Method: 结合视频基础模型（如SAM2）进行视图一致的2D掩码传播，并采用GPU加速的3D掩码生成与高斯级实例标记算法。
- Result: 在NVOS和Spin-NeRF基准测试中表现优异（92.7%和95.2% mIoU），推理速度提升15-1500倍（27毫秒/帧）。
- Conclusion: SAGOnline实现了实时渲染与3D场景理解，为AR/VR和机器人应用提供了实用解决方案。


### [215] [Learning User Preferences for Image Generation Model](https://arxiv.org/abs/2508.08220)
*Wenyi Mo,Ying Ba,Tianyu Zhang,Yalong Bai,Biye Li*

Main category: cs.CV

TL;DR: 提出了一种基于多模态大语言模型的方法，通过对比偏好损失和偏好标记学习个性化用户偏好，显著提升了偏好预测准确性。

- Motivation: 现有方法通常依赖通用偏好或静态用户画像，忽略了用户偏好的动态性和多样性，需要更全面的个性化预测方法。
- Method: 采用多模态大语言模型，引入对比偏好损失和偏好标记，区分用户喜好并捕捉共享兴趣表示。
- Result: 实验表明模型在偏好预测准确性上优于其他方法，能有效识别相似审美倾向的用户。
- Conclusion: 该方法为生成符合个人口味的图像提供了更精准的指导，解决了现有方法的局限性。


### [216] [OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.08227)
*Zhiqiang Wu,Zhaomang Sun,Tong Zhou,Bingtao Fu,Ji Cong,Yitong Dong,Huaqi Zhang,Xuan Tang,Mingsong Chen,Xian Wei*

Main category: cs.CV

TL;DR: OMGSR是一种基于DDPM/FM的一步式真实图像超分辨率框架，通过在中时间步注入LQ图像潜在分布，优化生成效果。

- Motivation: 解决LQ图像潜在分布与高斯噪声潜在分布之间的差距，以更有效地利用生成先验。
- Method: 提出OMGSR框架，包括Latent Distribution Refinement损失和Overlap-Chunked LPIPS/GAN损失，并实例化为OMGSR-S和OMGSR-F两种变体。
- Result: OMGSR-S/F在512分辨率下表现优异，OMGSR-F在所有参考指标中占据主导地位，1k和2k分辨率下也取得出色结果。
- Conclusion: OMGSR框架有效提升了一步式真实图像超分辨率的性能，尤其在细节生成方面表现突出。


### [217] [Cut2Next: Generating Next Shot via In-Context Tuning](https://arxiv.org/abs/2508.08244)
*Jingwen He,Hongbo Liu,Jiajun Li,Ziqi Huang,Yu Qiao,Wanli Ouyang,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Next Shot Generation (NSG)的方法，通过Cut2Next框架生成符合专业编辑模式和严格电影连续性的高质量镜头。

- Motivation: 现有方法多关注基本视觉一致性，忽略了推动叙事流畅的关键编辑模式，导致输出缺乏叙事复杂性和电影完整性。
- Method: 采用Diffusion Transformer (DiT)框架，结合Hierarchical Multi-Prompting策略（Relational Prompts和Individual Prompts）以及Context-Aware Condition Injection (CACI)和Hierarchical Attention Mask (HAM)技术。
- Result: 实验表明Cut2Next在视觉一致性和文本保真度上表现优异，用户研究显示其更符合编辑意图和电影连续性。
- Conclusion: Cut2Next能生成高质量、叙事表达丰富且电影连贯的后续镜头。


### [218] [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2508.08248)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAvatar是一种端到端的视频扩散变换器，能够生成无限长度的高质量视频，解决了现有模型在音频同步和身份一致性上的问题。

- Motivation: 现有音频驱动的虚拟化身视频生成模型难以生成长视频且音频同步和身份一致性差。
- Method: 引入时间步感知音频适配器和音频原生引导机制，结合动态加权滑动窗口策略。
- Result: 实验证明StableAvatar在质量和定量上均优于现有方法。
- Conclusion: StableAvatar通过创新设计解决了长视频生成中的关键问题，显著提升了性能。


### [219] [ReferSplat: Referring Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2508.08252)
*Shuting He,Guangquan Jie,Changshuo Wang,Yun Zhou,Shuming Hu,Guanbin Li,Henghui Ding*

Main category: cs.CV

TL;DR: R3DGS任务通过自然语言描述在3D高斯场景中分割目标物体，提出数据集Ref-LERF和框架ReferSplat，解决3D多模态理解和空间关系建模的挑战。

- Motivation: 推动具身AI发展，解决3D场景中基于语言描述的目标分割任务，尤其是遮挡或不可见物体的识别。
- Method: 提出ReferSplat框架，显式建模3D高斯点与自然语言表达的空间关系。
- Result: ReferSplat在R3DGS任务和3D开放词汇分割基准上达到最优性能。
- Conclusion: R3DGS任务和ReferSplat框架为3D多模态理解提供了新方向，数据集和代码已开源。


### [220] [Learning an Implicit Physics Model for Image-based Fluid Simulation](https://arxiv.org/abs/2508.08254)
*Emily Yue-Ting Jia,Jiageng Mao,Zhiyuan Gao,Yajie Zhao,Yue Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于物理原理的神经网络方法，从单张图像生成物理一致的4D场景动画。

- Motivation: 人类能够从静态图像想象4D场景（包括运动和3D几何），而现有方法生成的动画常违反物理原理，显得不真实。
- Method: 使用物理信息神经网络预测表面点运动，结合Navier-Stokes方程等物理原理，并通过3D高斯特征捕捉外观。
- Result: 实验表明，该方法能生成物理合理的动画，性能显著优于现有方法。
- Conclusion: 该方法成功实现了从单张图像生成物理一致的4D场景动画，为相关领域提供了新思路。
## q-bio.NC

### [221] [Sensory robustness through top-down feedback and neural stochasticity in recurrent vision models](https://arxiv.org/abs/2508.07115)
*Antonino Greco,Marco D'Alessandro,Karl J. Friston,Giovanni Pezzulo,Markus Siegel*

Main category: q-bio.NC

TL;DR: 研究探讨了自上而下反馈在视觉处理中的作用，发现结合随机神经变异性的卷积循环神经网络（ConvRNN）在速度和准确性、抗噪及对抗攻击方面表现更优。

- Motivation: 生物系统利用自上而下反馈进行视觉处理，但人工视觉模型通常仅使用前馈或循环架构，因此研究反馈路径的计算贡献。
- Method: 训练带有或不带自上而下反馈的ConvRNN，结合随机神经变异性（如dropout），分析反馈对网络表现的影响。
- Result: 带反馈的ConvRNN在速度和准确性、抗噪及对抗攻击方面表现更优，且反馈通过dropout优化了网络活动的低维流形表示。
- Conclusion: 反馈与随机神经变异性共同作用，实现稳健的感官编码：随机性防止单元级共适应，反馈则稳定网络活动于低维流形。
## cs.MM

### [222] [MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training](https://arxiv.org/abs/2508.07590)
*Xiongwei Xiao,Baoying Chen,Jishen Zeng,Jianquan Yang*

Main category: cs.MM

TL;DR: 提出了一种轻量级人脸质量评估网络（MSPT），通过多阶段渐进训练策略，在保持高效推理的同时实现高性能。

- Motivation: 传统人脸质量评估方法泛化性差，而学习型方法虽性能优越但计算和存储成本高，难以实际部署。
- Method: 采用三阶段渐进训练策略，逐步引入多样化数据样本并提高输入图像分辨率。
- Result: 在VQualA 2025数据集上取得第二高分，性能与最先进方法相当或更优。
- Conclusion: MSPT在轻量级网络中有效学习复杂质量特征，显著减轻灾难性遗忘，适合实际应用。


### [223] [AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition](https://arxiv.org/abs/2508.07608)
*Junxiao Xue,Xiaozhen Liu,Xuecheng Wu,Xinyi Yin,Danlei Huang,Fei Yu*

Main category: cs.MM

TL;DR: 论文提出了一种基于双向模态增强的新型音频-视觉语音识别框架AD-AVSR，通过双向信息流和噪声抑制模块提升性能。

- Motivation: 现有方法在不对称信息条件下难以捕捉音频-视觉数据的异质性和互补性关联。
- Method: 采用音频双流编码策略，结合音频感知视觉细化模块和跨模态噪声抑制掩码模块，实现双向信息流。
- Result: 在LRS2和LRS3数据集上表现优于现有方法，尤其在噪声鲁棒性方面。
- Conclusion: AD-AVSR框架有效提升了音频-视觉语音识别的性能和噪声鲁棒性。
## cs.CL

### [224] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: InterChart是一个评估视觉语言模型在多图表推理任务中表现的诊断基准，涵盖从简单事实推理到复杂语义推断的多层次挑战。

- Motivation: 现有基准主要关注孤立图表，而InterChart旨在填补多图表推理任务的空白，适用于科学、金融和政策分析等实际场景。
- Method: 基准分为三个难度层级：单图表事实推理、合成对齐图表集的综合分析，以及真实复杂图表对的语义推断。
- Result: 评估显示，随着图表复杂度增加，模型准确率显著下降，分解复杂图表可提升表现，但跨图表整合仍是挑战。
- Conclusion: InterChart揭示了模型的系统性局限，为复杂多视觉环境中的多模态推理研究提供了严格框架。
## cond-mat.mtrl-sci

### [225] [Digital generation of the 3-D pore architecture of isotropic membranes using 2-D cross-sectional scanning electron microscopy images](https://arxiv.org/abs/2508.06664)
*Sima Zeinali Danalou,Hooman Chamani,Arash Rabbani,Patrick C. Lee,Jason Hattrick Simpers,Jay R Werber*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种改进的算法，从单张2D SEM图像重建3D孔网络，解决了传统方法的局限，并在商业膜上验证了其高保真度和分辨率。

- Motivation: 传统3D重建技术昂贵且复杂，无法广泛使用，而现有方法难以复现真实膜中的多样孔几何结构。
- Method: 开发了增强的重建算法，保持统计特性并准确复现复杂孔形态。
- Result: 在商业膜上生成高保真3D重建，与X射线断层扫描数据验证一致，分辨率更高。
- Conclusion: 该方法适用于各向同性多孔膜，未来需扩展至各向异性膜。
## cs.AI

### [226] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 本文提出IRL-VLA框架，通过逆强化学习奖励世界模型解决VLA模型在自动驾驶中的闭环训练问题，并在NAVSIM v2和CVPR2025竞赛中取得优异表现。

- Motivation: 现有VLA模型在自动驾驶中存在开环模仿学习的局限性以及闭环训练对高保真模拟的依赖问题。
- Method: 采用三阶段方法：预训练VLA策略、构建轻量级奖励世界模型、通过PPO优化强化学习。
- Result: 在NAVSIM v2和CVPR2025竞赛中表现优异。
- Conclusion: IRL-VLA框架有望推动闭环自动驾驶中VLA模型的研究。


### [227] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: 论文提出了CountQA基准测试，用于评估多模态大语言模型（MLLMs）在复杂场景中的物体计数能力，发现现有模型表现不佳。

- Motivation: MLLMs在视觉场景理解中表现出色，但在物体计数方面存在明显缺陷，限制了其实际应用的可靠性。
- Method: 引入CountQA基准测试，包含1,500多个真实世界图像的问题-答案对，评估15种主流MLLMs。
- Result: 表现最佳的模型准确率仅为42.9%，且随着物体数量增加性能下降。
- Conclusion: CountQA为改进MLLMs的计数能力提供了工具，推动开发更具数值和空间感知能力的模型。


### [228] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: 论文提出了一种基于AI的端到端严重天气预测系统MP-Bench和气象多模态大模型（MMLM），解决了数据稀缺、高维气象数据与文本对齐问题，并通过实验验证了其有效性。

- Motivation: 当前严重天气预警系统依赖人工专家解读，存在主观性和操作负担。AI技术的发展为自动化预测提供了可能，但面临数据稀缺、高维数据对齐和模型能力不足的挑战。
- Method: 构建了大规模多模态数据集MP-Bench，并开发了MMLM模型，包含三个自适应融合模块，动态提取和整合4D气象数据特征。
- Result: 实验表明MMLM在多个任务中表现优异，验证了其在严重天气预测中的有效性。
- Conclusion: MP-Bench和MMLM为实现自动化AI驱动的天气预报系统迈出了关键一步，代码和数据集将公开。


### [229] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav是一个模块化框架，通过引入基于技能的结构化推理，提升了Transformer-based VLN代理的性能。

- Motivation: 当前方法在复杂空间和时间推理任务中泛化能力不足，需要更有效的解决方案。
- Method: 将导航任务分解为可解释的原子技能，每个技能由专门代理处理，并使用VLM-based路由器动态选择代理。
- Result: 在R2R和GSA-R2R基准测试中达到最新最优性能，并展示出强泛化能力。
- Conclusion: SkillNav通过模块化和技能分解，显著提升了VLN任务的性能和泛化能力。


### [230] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: FEAT是一个基于多智能体AI框架的自动化法医死因鉴定系统，通过领域适应的大语言模型提升效率和一致性。

- Motivation: 解决法医死因鉴定中的人力短缺和诊断差异问题，特别是在高负荷的法医系统中。
- Method: FEAT采用多智能体架构，包括任务分解的中央规划器、证据分析的专业本地求解器、迭代优化的记忆与反思模块，以及结论合成的全局求解器。结合工具增强推理、分层检索增强生成和法医调优的LLM。
- Result: 在多样化的中国案例中，FEAT在长篇幅尸检分析和简洁死因结论上优于现有AI系统，并在盲法验证中与专家高度一致。
- Conclusion: FEAT是首个基于LLM的法医AI系统，结合AI效率和人类监督，提升法医服务的可及性和一致性。
## cs.IT

### [231] [Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer](https://arxiv.org/abs/2402.16868)
*Peigen Ye,Yaping Sun,Shumin Yao,Hao Chen,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: 提出了一种基于高质量码本的鲁棒图像语义通信系统，通过联合构建语义编解码器和码本，并设计向量到索引的转换器，有效消除信道噪声影响，生成高质量图像。

- Motivation: 码本生成式语义通信中，码向量语义关系与索引距离无关，系统性能易受信道噪声影响，需提升鲁棒性。
- Method: 联合构建语义编解码器和码本，设计向量到索引转换器，利用高质量码本辅助生成图像。
- Result: 接收端生成的图像在视觉感知上优于对比方法（JPEG+LDPC和传统JSCC）。
- Conclusion: 所提方法在生成图像质量和抗噪性能上优于传统方法，验证了其有效性。
## eess.AS

### [232] [KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features](https://arxiv.org/abs/2508.07337)
*Ivan Kukanov,Jun Wah Ng*

Main category: eess.AS

TL;DR: 论文提出了一种多模态方法用于检测和定位深度伪造内容，结合视觉和音频特征，在性能和可解释性之间取得平衡。

- Motivation: 随着音频驱动的人脸生成和TTS技术的发展，深度伪造内容日益复杂，需要更鲁棒的检测方法。现有方法计算成本高且难以泛化到新攻击场景。
- Method: 视觉模态采用手工特征以提高可解释性和适应性；音频模态结合自监督学习骨干和图注意力网络，增强检测鲁棒性。
- Result: 在AV-Deepfake1M++数据集上，多模态系统在深度伪造分类任务中AUC达92.78%，仅用音频模态时时间定位IoU为0.3536。
- Conclusion: 该方法在性能和实际部署之间取得平衡，具有鲁棒性和潜在可解释性。
## cs.RO

### [233] [Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound](https://arxiv.org/abs/2508.06921)
*Zhongyu Chen,Chenyang Li,Xuesong Li,Dianye Huang,Zhongliang Jiang,Stefanie Speidel,Xiangyu Chu,K. W. Samuel Au*

Main category: cs.RO

TL;DR: 提出了一种通过周期性振动针头来恢复超声引导下针头对齐的方法，解决了针头在超声图像中不可见时的对齐问题。

- Motivation: 针头对齐在超声引导手术中至关重要，但噪声、伪影和低分辨率等问题导致针头检测困难，尤其是在针头不可见时。
- Method: 通过机械系统周期性振动针头，提出一种基于振动的能量度量，并开发控制策略调整超声探头位置。
- Result: 实验结果显示，平移误差为0.41±0.27 mm，旋转误差为0.51±0.19度。
- Conclusion: 该方法在针头不可见时仍能有效恢复对齐，具有较高的精确性和鲁棒性。


### [234] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 论文提出了农业场景下的视觉与语言导航基准（A2A）和基线方法AgriVLN，通过子任务分解模块提升性能。

- Motivation: 农业机器人依赖人工操作或固定轨道，移动性受限。现有VLN方法未针对农业场景设计。
- Method: 提出A2A基准和AgriVLN基线方法，基于VLM生成控制指令，并引入子任务分解模块（STL）。
- Result: AgriVLN在短指令上表现良好，长指令通过STL模块将成功率从0.33提升至0.47。
- Conclusion: AgriVLN在农业领域表现最优，为农业机器人导航提供了有效解决方案。


### [235] [Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2508.07560)
*Yan Gong,Naibang Wang,Jianli Lu,Xinyu Zhang,Yongsheng Gao,Jie Zhao,Zifan Huang,Haozhi Bai,Nanxin Zeng,Nayu Su,Lei Yang,Ziying Song,Xiaoxi Hu,Xinmin Jiang,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.RO

TL;DR: 本文综述了自动驾驶中鸟瞰图（BEV）感知的安全性挑战，分析了单模态、多模态和多代理协作感知的框架，并探讨了未来研究方向。

- Motivation: 随着自动驾驶从受控环境转向实际部署，确保BEV感知在复杂场景中的安全性和可靠性成为关键挑战。
- Method: 系统分析了BEV感知的三个阶段：单模态车辆端、多模态车辆端和多代理协作感知，并评估了相关数据集。
- Result: 识别了开放世界中的关键挑战，如开放集识别、传感器退化等，并提出了未来研究方向。
- Conclusion: BEV感知在自动驾驶中具有重要潜力，但仍需解决开放世界挑战，未来可结合端到端系统和大语言模型等方向。


### [236] [Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning](https://arxiv.org/abs/2508.07885)
*Shoaib Ahmmad,Zubayer Ahmed Aditto,Md Mehrab Hossain,Noushin Yeasmin,Shorower Hossain*

Main category: cs.RO

TL;DR: 论文提出了一种基于AI的感知系统，用于GPS缺失的室内环境中自主四轴飞行器导航，结合云计算和定制PCB，实现高效导航。

- Motivation: 解决GPS缺失环境下四轴飞行器的自主导航问题，提升在狭小空间中的感知和决策能力。
- Method: 系统整合了YOLOv11、Depth Anything V2、ToF传感器、IMU和云端LLM，采用多线程架构和虚拟安全边界技术。
- Result: 实验显示，目标检测mAP50为0.6，深度估计MAE为7.2 cm，42次试验中仅16次安全边界突破，系统延迟低于1秒。
- Conclusion: 该框架为GPS缺失环境下的无人机导航提供了高效辅助系统，补充了现有技术的不足。


### [237] [ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks](https://arxiv.org/abs/2508.08240)
*Kaijun Wang,Liqin Lu,Mingyu Liu,Jianuo Jiang,Zeju Li,Bolin Zhang,Wancai Zheng,Xinyi Yu,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: ODYSSEY是一个统一的移动操作框架，结合了高级任务规划和低级全身控制，解决了语言引导的长时程移动操作中的感知、泛化和控制问题。

- Motivation: 语言引导的长时程移动操作面临感知受限、泛化不足和平台机动性与精确控制的双重挑战。
- Method: ODYSSEY采用分层规划器（基于视觉语言模型）和全身控制策略，实现任务分解和精确执行。
- Result: 通过仿真到现实的迁移，系统在多样化的室内外场景中展示了泛化能力和鲁棒性。
- Conclusion: ODYSSEY推动了通用机器人助手在复杂动态任务中的可行性。
## cs.LG

### [238] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMFT的单阶段算法，通过动态平衡监督微调（SFT）和强化学习（RL）的奖励信号，解决了传统两阶段方法中的灾难性遗忘和探索-模仿权衡问题。

- Motivation: 传统两阶段方法（SFT+RL）存在灾难性遗忘和探索-模仿权衡问题，而现有单阶段方法缺乏动态平衡机制。
- Method: 提出AMFT算法，利用元梯度自适应权重控制器动态优化SFT和RL的奖励平衡，并通过策略熵正则化确保稳定性。
- Result: 在数学推理、抽象视觉推理和视觉语言导航等任务上，AMFT表现优异，并在分布外任务上展现出更强的泛化能力。
- Conclusion: AMFT通过动态平衡SFT和RL的奖励信号，提供了一种更高效、稳定的LLM对齐范式。


### [239] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Second-Order MeanFlow的新方法，通过引入平均加速度场扩展了MeanFlow框架，证明了其可行性和高效性，并提供了理论支持。

- Motivation: 生成建模在无模拟范式（如Flow Matching）中取得了显著进展，但需要进一步扩展以支持更丰富的动力学和高效采样。
- Method: 通过引入平均加速度场，提出Second-Order MeanFlow框架，证明其满足一致性条件，并通过电路复杂度分析和注意力计算优化实现高效采样。
- Result: 证明了Second-Order MeanFlow的可行性和高效性，支持单步采样，并展示了其在TC0类电路中的实现能力。
- Conclusion: 为高阶流匹配模型奠定了理论基础，结合了丰富的动力学和实际采样效率。


### [240] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 该研究提出了一种结合视觉定位和大型语言模型（LLM）导航的室内导航方法，实现了高精度定位和中等准确度的导航指令生成。

- Motivation: 室内导航因缺乏GPS信号和建筑复杂性而具有挑战性，研究旨在开发一种无需基础设施的解决方案。
- Method: 使用ResNet-50进行视觉定位，结合LLM生成导航指令，实验在复杂环境中测试。
- Result: 定位准确率达96%，导航指令准确率为75%，但存在零样本推理和时间限制问题。
- Conclusion: 该方法展示了利用普通设备和公开地图实现室内导航的潜力，适用于资源有限的环境。
## cs.CR

### [241] [Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems](https://arxiv.org/abs/2508.07263)
*Qingyuan Zeng,Shu Jiang,Jiajing Lin,Zhenzhong Wang,Kay Chen Tan,Min Jiang*

Main category: cs.CR

TL;DR: 本文提出了一种名为GMEA的黑盒攻击框架，用于挑战3D高斯泼溅（3DGS）中的数字水印技术，通过多目标优化平衡水印移除与视觉质量，实验证明其有效性。

- Motivation: 随着3DGS的兴起，数字水印技术用于版权保护，但其对抗攻击的鲁棒性尚未充分研究，本文旨在填补这一空白。
- Method: 提出GMEA框架，将攻击建模为大规模多目标优化问题，采用基于组的优化策略和间接目标函数，最小化卷积网络提取特征的方差。
- Result: 实验表明，GMEA能有效移除主流3DGS水印方法中的1D和2D水印，同时保持高视觉保真度。
- Conclusion: 本文揭示了现有3DGS版权保护方案的脆弱性，呼吁开发更鲁棒的水印系统。


### [242] [IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2508.08031)
*Jiayao Wang,Yang Song,Zhendong Zhao,Jiale Zhang,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: 本文提出了一种针对联邦自监督学习（FSSL）的隐蔽且有效的后门攻击方法IPBA，解决了现有方法在隐蔽性和实用性上的不足。

- Motivation: 尽管FSSL在可扩展性和隐私保护方面具有潜力，但研究表明其容易受到后门攻击，且现有方法的隐蔽性和实用性不足。
- Method: IPBA通过解耦后门样本与增强样本的特征分布，并引入Sliced-Wasserstein距离来优化触发生成过程。
- Result: 实验表明，IPBA在多个FSSL场景和数据集中显著优于现有后门攻击方法，且对各种防御机制表现出强鲁棒性。
- Conclusion: IPBA为FSSL中的后门攻击提供了更隐蔽和有效的解决方案。
## eess.IV

### [243] [Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification](https://arxiv.org/abs/2508.06535)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: 研究探讨了使用预训练卷积神经网络（CNN）和迁移学习技术，结合数据增强方法，提高急性淋巴细胞白血病（ALL）血液涂片图像分类的准确性。

- Motivation: 准确分类ALL对早期诊断和治疗计划至关重要，但现有方法在性能上仍有提升空间。
- Method: 采用ResNet50、ResNet101和EfficientNet（B0、B1、B3）等模型，通过数据增强平衡数据集（10,000张/类）。
- Result: EfficientNet-B3表现最佳，F1分数94.30%，准确率92.02%，AUC 94.79%，优于C-NMC挑战赛的现有方法。
- Conclusion: 结合数据增强和迁移学习（尤其是EfficientNet-B3）可显著提升ALL诊断工具的准确性和鲁棒性。


### [244] [LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification](https://arxiv.org/abs/2508.06874)
*Shisheng Zhang,Ramtin Gharleghi,Sonit Singh,Daniel Moses,Dona Adikari,Arcot Sowmya,Susann Beier*

Main category: eess.IV

TL;DR: 提出一种结合解剖学知识和规则拓扑约束的轻量级方法，用于自动化冠状动脉标记，解决了传统方法和深度学习的局限性。

- Motivation: 冠状动脉疾病是全球主要死因，CTCA是关键诊断工具，但手动分析耗时且劳动密集。现有自动化方法无法兼顾数据驱动和临床知识。
- Method: 整合解剖学知识和规则拓扑约束，提出轻量级方法，用于自动化冠状动脉标记。
- Result: 在基准数据集上达到最先进性能。
- Conclusion: 该方法为自动化冠状动脉标记提供了有前景的解决方案。


### [245] [Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning](https://arxiv.org/abs/2508.06891)
*Melika Filvantorkaman,Mohsen Piri,Maral Filvan Torkaman,Ashkan Zabihi,Hamidreza Moradi*

Main category: eess.IV

TL;DR: 该研究提出了一种基于MobileNetV2和DenseNet121的集成深度学习框架，结合可解释AI技术，用于MRI脑肿瘤分类，性能优于单一模型，并得到临床验证。

- Motivation: MRI脑肿瘤分类的准确性和可解释性对诊断和治疗至关重要，但现有方法在性能和临床信任度上仍有不足。
- Method: 采用MobileNetV2和DenseNet121的集成模型，结合软投票策略和可解释AI模块（Grad-CAM++和临床决策规则），在Figshare数据集上通过5折交叉验证进行评估。
- Result: 集成模型准确率达91.7%，Grad-CAM++可视化与专家标注区域高度一致，临床评估显示高解释性和实用性。
- Conclusion: 该框架为脑肿瘤分类提供了高性能且可解释的解决方案，推动了深度学习在临床神经诊断中的应用。


### [246] [Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments](https://arxiv.org/abs/2508.07006)
*Gian Mario Favero,Ge Ya Luo,Nima Fathi,Justin Szeto,Douglas L. Arnold,Brennan Nichyporuk,Chris Pal,Tal Arbel*

Main category: eess.IV

TL;DR: 提出首个治疗感知的时空扩散模型，用于预测多发性硬化症（MS）病灶的未来演变，结合多模态数据和不同治疗方案，展示临床潜力。

- Motivation: 多发性硬化症（MS）的异质性进展需要个性化医疗，图像生成模型可帮助预测病灶演变。
- Method: 基于体素空间的多模态数据（MRI和治疗信息）构建时空扩散模型，预测未来病灶。
- Result: 模型在2131例患者数据上验证，能准确预测六种治疗方案的病灶演变，并支持下游临床任务。
- Conclusion: 因果图像生成模型为MS数据驱动预后提供了强大工具，具有临床应用潜力。


### [247] [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](https://arxiv.org/abs/2508.07031)
*Anindya Bijoy Das,Shahnewaz Karim Sakib,Shibbir Ahmed*

Main category: eess.IV

TL;DR: 研究探讨了大型语言模型（LLMs）在医学影像任务中的幻觉问题，分析了图像到文本和文本到图像两种方向中的错误，并提出了改进建议。

- Motivation: LLMs在医学影像任务中常产生幻觉（自信但错误的输出），可能误导临床决策，因此需要系统研究其错误模式。
- Method: 研究分析了图像到文本（生成报告）和文本到图像（生成影像）任务中的幻觉，使用专家标准评估错误。
- Result: 发现幻觉在解释性和生成性任务中均存在常见模式，并揭示了模型架构和训练数据等因素的影响。
- Conclusion: 通过系统研究，为提升LLM驱动的医学影像系统的安全性和可信度提供了见解。


### [248] [3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression](https://arxiv.org/abs/2508.07038)
*Yuke Xing,William Gordon,Qi Yang,Kaifa Yang,Jiarui Wang,Yiling Xu*

Main category: eess.IV

TL;DR: 3DGS-VBench是一个用于评估3D高斯泼溅（3DGS）压缩算法视觉质量的大规模数据集和基准测试，包含660个压缩模型和视频序列，并提供了MOS评分。

- Motivation: 3DGS的高存储需求阻碍了实际应用，现有压缩技术缺乏系统性的质量评估研究。
- Method: 建立了包含11个场景、6种压缩算法的数据集，并通过50名参与者标注MOS评分，验证了数据集的可靠性。
- Result: 评估了6种压缩算法的存储效率和视觉质量，并测试了15种质量评估指标。
- Conclusion: 该工作为3DGS的压缩和质量评估研究提供了基础，数据集已开源。


### [249] [SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging](https://arxiv.org/abs/2508.07041)
*Junkai Liu,Nay Aung,Theodoros N. Arvanitis,Stefan K. Piechnik,Joao A C Lima,Steffen E. Petersen,Le Zhang*

Main category: eess.IV

TL;DR: 论文提出了一种名为SAGCNet的方法，用于解决MRI中缺失切片的问题，通过图结构和空间适配器提升3D空间信息的利用。

- Motivation: MRI在临床实践中常因缺失或不可用切片而影响准确性，现有方法难以建模3D数据的局部和全局关系。
- Method: SAGCNet包含两个创新模块：1) 体积切片图补全模块，建模切片间关系；2) 体积空间适配器，捕捉3D空间上下文。
- Result: 在心脏MRI数据集上，SAGCNet在定量和定性上均优于现有方法，且在数据有限时仍表现优异。
- Conclusion: SAGCNet有效解决了MRI缺失切片问题，提升了3D空间信息的利用，具有临床潜力。


### [250] [Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications](https://arxiv.org/abs/2508.07165)
*Zelin Qiu,Xi Wang,Zhuoyao Xie,Juan Zhou,Yu Wang,Lingjie Yang,Xinrui Jiang,Juyoung Bae,Moo Hyun Son,Qiang Ye,Dexuan Chen,Rui Zhang,Tao Li,Neeraj Ramesh Mahboobani,Varut Vardhanabhuti,Xiaohui Duan,Yinghua Zhao,Hao Chen*

Main category: eess.IV

TL;DR: PRISM是一个基于大规模多序列MRI预训练的基础模型，旨在解决MRI序列异质性对深度学习模型泛化能力的挑战。

- Motivation: MRI序列的异质性限制了深度学习模型的泛化能力和临床实用性。
- Method: PRISM通过解耦解剖不变特征和序列特异性变化，构建了大规模多器官多序列MRI预训练语料库，并在44个下游任务上评估性能。
- Result: PRISM在39/44的下游任务中显著优于非预训练模型和现有基础模型。
- Conclusion: PRISM提供了一个可扩展的多序列MRI分析框架，增强了AI在放射学中的转化潜力。


### [251] [HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation](https://arxiv.org/abs/2508.07225)
*Xuepeng Liu,Zheng Jiang,Pinan Zhu,Hanyu Liu,Chao Li*

Main category: eess.IV

TL;DR: HaDM-ST是一种基于H&E图像和低分辨率空间转录组数据的高分辨率ST生成框架，解决了现有方法在特征提取、多模态对齐和基因特异性建模方面的挑战。

- Motivation: 当前空间转录组技术分辨率有限，且现有方法在利用H&E图像时面临特征提取、多模态对齐和基因特异性建模的三大挑战。
- Method: HaDM-ST包含三个模块：(i) 语义蒸馏网络从H&E图像提取特征；(ii) 空间对齐模块确保像素级对应；(iii) 通道感知对抗学习器实现基因级建模。
- Result: 在200个基因的实验中，HaDM-ST在空间保真度和基因一致性上优于现有方法。
- Conclusion: HaDM-ST显著提升了高分辨率ST预测的准确性，适用于多种组织和物种。


### [252] [DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework](https://arxiv.org/abs/2508.07682)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: eess.IV

TL;DR: DiffVC-OSD是一种基于单步扩散的感知神经视频压缩框架，通过直接输入重建潜在表示到单步扩散模型中，显著提升感知质量和解码速度。

- Motivation: 传统多步扩散方法效率低，DiffVC-OSD旨在通过单步扩散和时序上下文适配器提升压缩性能和速度。
- Method: 采用单步扩散模型和时序上下文适配器，结合端到端微调策略优化压缩性能。
- Result: DiffVC-OSD在感知压缩性能上达到最优，解码速度快20倍，比特率降低86.92%。
- Conclusion: DiffVC-OSD通过单步扩散和时序上下文适配器实现了高效、高质量的神经视频压缩。


### [253] [Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning](https://arxiv.org/abs/2508.07788)
*Runze Wang,Zeli Chen,Zhiyun Song,Wei Fang,Jiajin Zhang,Danyang Tu,Yuxing Tang,Minfeng Xu,Xianghua Ye,Le Lu,Dakai Jin*

Main category: eess.IV

TL;DR: ALDEN是一种基于深度学习的低剂量CT去噪方法，通过结合预训练视觉模型的语义特征与对抗和对比学习，显著提升了去噪效果并保留了解剖结构。

- Motivation: 现有低剂量CT去噪方法常忽略解剖语义，导致去噪效果不佳。ALDEN旨在通过引入解剖感知机制解决这一问题。
- Method: ALDEN结合了预训练视觉模型的语义特征，采用解剖感知判别器和语义引导的对比学习模块，动态融合层次语义特征并保持解剖一致性。
- Result: 在多个低剂量CT去噪数据集上，ALDEN表现优于现有方法，显著减少了过度平滑问题，并在下游多器官分割任务中验证了其解剖感知能力。
- Conclusion: ALDEN通过解剖感知机制和对比学习，显著提升了低剂量CT去噪的效果，为医学影像分析提供了更可靠的解决方案。


### [254] [Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images](https://arxiv.org/abs/2508.07875)
*Shuo Han,Ahmed Karam Eldaly,Solomon Sunday Oyelere*

Main category: eess.IV

TL;DR: 提出了一种人机协作的深度学习系统，用于提高乳腺浸润性导管癌（IDC）的检测精度。

- Motivation: 早期准确诊断IDC对患者生存率至关重要，结合AI与医学专家知识可提升检测效率和精度。
- Method: 采用EfficientNetV2S模型进行初步诊断，并通过人机反馈循环优化模型性能。
- Result: 模型初始准确率达93.65%，人机协作系统进一步提升了检测精度。
- Conclusion: 人机协作方法为AI辅助医疗诊断提供了高效且高精度的方向。


### [255] [Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models](https://arxiv.org/abs/2508.07903)
*Johanna P. Müller,Anika Knupfer,Pedro Blöss,Edoardo Berardi Vittur,Bernhard Kainz,Jana Hutter*

Main category: eess.IV

TL;DR: 提出了一种基于扩散模型的子宫MRI合成框架，解决了现有模型在生成女性盆腔图像时解剖学精度不足的问题。

- Motivation: 现有扩散模型在生成女性盆腔图像时解剖学精度不足，限制了其在妇科影像中的应用，而数据稀缺和患者隐私问题加剧了这一挑战。
- Method: 结合了无条件与条件化的DDPMs和LDMs（2D和3D），生成解剖学一致、高保真的合成图像。
- Result: 通过先进的感知和分布度量评估生成质量，并在关键分类任务中显著提升了诊断准确性。专家盲评进一步验证了临床真实性。
- Conclusion: 发布的模型和合成数据集支持可重复研究，推动妇科AI的公平发展。


### [256] [A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images](https://arxiv.org/abs/2508.08123)
*Lingjing Chen,Chengxiu Zhang,Yinqiao Yi,Yida Wang,Yang Song,Xu Yan,Shengfang Xu,Dalin Zhu,Mengqiu Cao,Yan Zhou,Chenglong Wang,Guang Yang*

Main category: eess.IV

TL;DR: 提出一种基于深度学习的MRI定量图像合成方法，通过嵌入MRI序列参数提升准确性和泛化能力。

- Motivation: 传统MRI定量合成方法在准确性和泛化性上存在不足，需结合物理原理改进。
- Method: 设计物理驱动的神经网络，嵌入TR、TE、TI等参数，输入T1、T2、T2-FLAIR图像，输出T1、T2、PD定量图。
- Result: 在内部和外部测试集上表现优异，PSNR>34dB，SSIM>0.92，泛化能力优于传统模型。
- Conclusion: 参数嵌入显著提升定量MRI合成的性能和可靠性，具有加速qMRI和临床应用的潜力。


### [257] [RedDino: A foundation model for red blood cell analysis](https://arxiv.org/abs/2508.08180)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto,Carsten Marr*

Main category: eess.IV

TL;DR: RedDino是一个自监督基础模型，专为红细胞图像分析设计，性能优于现有最先进模型。

- Motivation: 红细胞形态分析对血液病诊断至关重要，但现有AI解决方案不足。
- Method: 采用DINOv2自监督学习框架的RBC特定版本，训练于125万张RBC图像数据集。
- Result: RedDino在红细胞形状分类中表现优异，特征表示和泛化能力强大。
- Conclusion: RedDino解决了计算血液学中的关键挑战，推动了可靠诊断工具的发展。


### [258] [Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping](https://arxiv.org/abs/2508.07760)
*Maximilian Kromer,Panagiotis Agrafiotis,Begüm Demir*

Main category: eess.IV

TL;DR: 论文提出Sea-Undistort数据集，用于解决浅水区图像测深中的光学失真问题，并评估了两种图像恢复方法和一种改进的扩散模型。

- Motivation: 浅水区图像测深因动态水面、水柱特性和太阳光照导致的光学失真（如波浪、散射和太阳耀斑）而具有挑战性。
- Method: 使用Blender渲染1200对512x512图像（失真和无失真），并训练改进的轻量级扩散模型，结合太阳耀斑掩码。
- Result: 改进的扩散模型在真实航空数据中表现更优，生成更完整的海底数字表面模型，减少测深误差，抑制耀斑和散射。
- Conclusion: Sea-Undistort数据集和扩散模型为浅水区图像测深提供了有效解决方案，代码和数据已公开。


### [259] [PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography](https://arxiv.org/abs/2508.07773)
*Mohammed Salah,Numan Saeed,Davor Svetinovic,Stefano Sfarra,Mohammed Omar,Yusra Abdulrahman*

Main category: eess.IV

TL;DR: 提出了一种基于PCA引导的自编码框架，用于结构化降维，以改进红外热成像数据的缺陷表征。

- Motivation: 现有AIRT自编码器的潜在空间缺乏结构，限制了其在缺陷表征任务中的效果。
- Method: 引入PCA蒸馏损失函数，引导自编码器在降维时对齐PCA结构，同时捕捉非线性特征。
- Result: 在PVC、CFRP和PLA样本上，PCA引导的自编码器在对比度、信噪比和神经网络指标上优于现有方法。
- Conclusion: PCA引导的自编码框架显著提升了AIRT数据的结构化降维效果，适用于缺陷表征。


### [260] [MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer](https://arxiv.org/abs/2508.07817)
*Tao Tang,Chengxu Yang*

Main category: eess.IV

TL;DR: 提出了一种结合多尺度卷积与Transformer架构的医学图像自适应去噪模型（MI-ND），通过噪声感知驱动特征融合，显著提升图像质量与下游诊断性能。

- Motivation: 医学图像因噪声干扰影响诊断准确性，需高效去噪方法。
- Method: 集成多尺度卷积与Transformer，引入噪声水平估计器（NLE）和噪声自适应注意力模块（NAAB），实现噪声感知驱动的特征融合。
- Result: 在PSNR、SSIM等指标上优于对比方法，提升下游任务的F1分数和ROC-AUC。
- Conclusion: 模型在结构恢复、诊断敏感性和跨模态鲁棒性方面表现突出，具有实际应用价值。


### [261] [Learned Regularization for Microwave Tomography](https://arxiv.org/abs/2508.08114)
*Bowen Tong,Hao Chen,Shaorui Guo,Dong Liu*

Main category: eess.IV

TL;DR: 提出了一种基于物理信息的混合框架SSD-Reg，结合扩散模型作为学习正则化，用于解决微波断层扫描中的非线性逆问题，无需配对数据即可恢复复杂结构。

- Motivation: 传统优化方法难以恢复精细结构，深度学习需要大量配对数据且泛化能力有限。
- Method: 提出SSD-Reg，将扩散先验嵌入迭代重建过程，结合物理模型和数据一致性。
- Result: 实验表明SSD-Reg提高了重建的准确性、稳定性和鲁棒性。
- Conclusion: SSD-Reg为功能图像重建提供了一种灵活有效的解决方案。
## stat.ML

### [262] [Membership Inference Attacks with False Discovery Rate Control](https://arxiv.org/abs/2508.07066)
*Chenxu Zhao,Wei Qian,Aobo Chen,Mengdi Huai*

Main category: stat.ML

TL;DR: 本文提出了一种新型的成员推理攻击方法，能够控制假发现率（FDR），并展示其在实际场景中的有效性。

- Motivation: 现有成员推理攻击方法无法保证假发现率（FDR），且分布未知和概率依赖性问题使其难以实现。
- Method: 设计了一种新型成员推理攻击方法，可作为包装器与现有方法无缝集成，并提供FDR控制。
- Result: 理论分析和多场景实验验证了方法的有效性，包括黑盒和终身学习设置。
- Conclusion: 该方法在提供FDR保证的同时，还能标记真实非成员数据的边际概率，具有广泛适用性。
## cs.SE

### [263] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: Mo2oM框架通过软聚类方法将单体应用分解为重叠微服务，显著提升结构模块化和通信效率。

- Motivation: 传统硬聚类方法在微服务提取中导致服务间耦合增加和内聚性降低，Mo2oM旨在解决这一问题。
- Method: 结合深度语义嵌入和结构依赖，使用图神经网络进行软聚类，生成重叠微服务。
- Result: 在四个开源基准测试中，Mo2oM在结构模块化、通信开销等方面显著优于现有方法。
- Conclusion: Mo2oM通过重叠微服务设计，有效平衡了内聚性和耦合性，提升了系统性能。
