[[toc]]

## cs.CV

### [1] [OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport](https://arxiv.org/abs/2506.20741)
*Qin Ren,Yifan Wang,Ruogu Fang,Haibin Ling,Chenyu You*

Main category: cs.CV

TL;DR: OTSurv是一种基于最优传输（OT）的多实例学习（MIL）框架，用于全切片图像（WSI）的生存预测，通过全局和局部约束显式捕捉病理异质性，显著提升了预测性能。

- Motivation: 现有MIL方法未能显式捕捉WSI中的病理异质性（全局长尾分布和局部预测不确定性），OTSurv旨在通过OT建模这种异质性。
- Method: OTSurv将生存预测建模为具有全局长尾约束和局部不确定性约束的OT问题，转化为不平衡OT问题并通过高效矩阵缩放算法求解。
- Result: 在六个基准测试中，OTSurv实现了平均C-index绝对提升3.6%，并在对数秩检验中达到统计显著性。
- Conclusion: OTSurv是一种高效、可解释的生存预测工具，适用于数字病理学。


### [2] [StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation](https://arxiv.org/abs/2506.20756)
*Haodong Li,Chen Wang,Jiahui Lei,Kostas Daniilidis,Lingjie Liu*

Main category: cs.CV

TL;DR: StereoDiff结合立体匹配和视频深度扩散，分别处理视频中的静态和动态区域，实现更一致的视频深度估计。

- Motivation: 视频深度估计并非图像深度估计的简单扩展，静态和动态区域的时序一致性需求不同。
- Method: 提出两阶段视频深度估计器StereoDiff，静态区域通过立体匹配，动态区域通过视频深度扩散。
- Result: 在零样本、真实世界动态视频深度基准测试中表现优异，达到SoTA水平。
- Conclusion: StereoDiff通过结合立体匹配和视频深度扩散，显著提升了视频深度估计的一致性和准确性。


### [3] [ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations](https://arxiv.org/abs/2506.20757)
*Zhiyuan Wu,Yongqiang Zhao,Shan Luo*

Main category: cs.CV

TL;DR: ConViTac提出了一种视觉-触觉表征学习网络，通过对比表征增强特征融合的对齐性，显著提升了材料分类和抓取预测任务的性能。

- Motivation: 视觉和触觉是机器人感知和操作任务中的互补模态，但现有方法在模态融合时特征整合效果不佳。
- Method: 提出ConViTac网络，采用对比嵌入条件（CEC）机制，通过自监督对比学习预训练的编码器生成统一潜在嵌入，并利用跨模态注意力进行特征融合。
- Result: 实验表明，ConViTac在材料分类和抓取预测任务中准确率提升高达12.0%，优于现有方法。
- Conclusion: ConViTac通过对比表征和跨模态注意力有效提升了视觉-触觉特征融合的性能。


### [4] [AI-Driven MRI-based Brain Tumour Segmentation Benchmarking](https://arxiv.org/abs/2506.20786)
*Connor Ludwig,Khashayar Namdar,Farzad Khalvati*

Main category: cs.CV

TL;DR: 论文评估了多种可提示模型（如SAM、SAM 2、MedSAM等）在医学图像分割中的零样本推理性能，并与nnU-Net进行了比较。结果显示，某些模型在高质量提示下表现优异，但nnU-Net仍占主导地位。

- Motivation: 当前缺乏对多种可提示模型在医学数据集上的统一评估与比较，尤其是在不同提示质量下的表现。
- Method: 使用SAM、SAM 2、MedSAM、SAM-Med-3D和nnU-Net在BraTS 2023数据集上进行零样本推理，并通过微调进一步评估性能。
- Result: SAM和SAM 2在高质量边界框提示下表现优异（Dice分数达0.894和0.893），但nnU-Net仍更实用。微调后点提示性能显著提升，但仍不及边界框或nnU-Net。
- Conclusion: 高质量提示对模型性能至关重要，但实际应用中nnU-Net仍是最优选择。微调展示了未来研究的潜力。


### [5] [How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?](https://arxiv.org/abs/2506.20795)
*Stephanie Käs,Anton Burenko,Louis Markert,Onur Alp Culha,Dennis Mack,Timm Linder,Bastian Leibe*

Main category: cs.CV

TL;DR: 研究探讨了基于视觉基础模型（VFM）和视觉语言模型（VLM）的动态全身手势识别，比较了V-JEPA、Gemini Flash 2.0和HD-GCN的性能，并提出了NUGGET数据集用于评估。

- Motivation: 在嘈杂环境中，手势是非人机交互的重要方式，传统深度学习方法依赖任务特定架构，而VFMs和VLMs的泛化能力可能降低系统复杂性。
- Method: 研究比较了V-JEPA（VFM）、Gemini Flash 2.0（VLM）和HD-GCN（骨架方法）在手势识别中的表现，并引入了NUGGET数据集。
- Result: HD-GCN表现最佳，但V-JEPA接近其性能，Gemini在零样本设置下表现不佳。
- Conclusion: V-JEPA有望作为共享多任务模型降低系统复杂性，但需进一步研究手势的输入表示。


### [6] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉语言模型（VLM）的自动化框架，从扩散模型生成的多幅超分辨率（SR）图像中选择最可信的样本，并通过混合指标（TWS）量化其可靠性。

- Motivation: 解决超分辨率问题中单一回归模型和扩散模型在信息保真度和多样性选择上的不足，确保生成结果的语义正确性和视觉质量。
- Method: 利用VLM（如BLIP-2、GPT-4o）进行结构化查询评估，结合CLIP嵌入、SSIM边缘图和多级小波分解，提出TWS指标量化可靠性。
- Result: TWS与人类偏好高度相关，VLM引导的选择在模糊和自然图像中均表现优异，优于传统指标（如PSNR、LPIPS）。
- Conclusion: 该方法为生成式超分辨率提供了可扩展、通用的可信度评估框架，设定了新的可信度基准。


### [7] [FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization](https://arxiv.org/abs/2506.20841)
*Ha Min Son,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: FixCLR是一种新的半监督域泛化方法，通过对比学习显式正则化域不变表示，提升性能。

- Motivation: 解决半监督域泛化中因标签稀缺导致的性能不足问题，现有方法未显式正则化域不变表示。
- Method: 结合对比学习，利用伪标签的类别信息和排斥项，显式正则化域不变表示。
- Result: FixCLR表现优异，尤其与其他半监督方法结合时，实验验证了其有效性。
- Conclusion: FixCLR是一种有效的SSDG方法，可与其他方法互补提升性能。


### [8] [Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision](https://arxiv.org/abs/2506.20850)
*Yuting He,Shuo Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为COVER的向量对比学习框架，解决了传统对比学习在像素级表示中的过分散问题，提升了医学视觉基础模型的泛化能力。

- Motivation: 传统对比学习（CL）在自监督预训练（SSP）中表现优异，但在像素级表示中存在过分散问题，破坏了像素级特征相关性。本文旨在解决这一问题。
- Method: 提出向量对比学习（vector CL），将CL重新定义为向量回归问题，并通过COVER框架实现。COVER包括向量自学习、优化流一致性和向量金字塔架构。
- Result: 在8个任务、2个维度和4种模态的实验中，COVER显著提升了像素级SSP性能。
- Conclusion: COVER框架有效解决了像素级表示中的过分散问题，为医学视觉基础模型提供了更好的泛化能力。


### [9] [Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation](https://arxiv.org/abs/2506.20867)
*Ryosuke Kawamura,Hideaki Hayashi,Shunsuke Otake,Noriko Takemura,Hajime Nagahara*

Main category: cs.CV

TL;DR: MIDAS是一种数据增强方法，通过软标签和视频帧的凸组合提升动态面部表情识别（DFER）性能，尤其在处理模糊表情时表现优异。

- Motivation: 解决动态面部表情识别中模糊表情的准确识别问题，提升在真实场景中的应用效果。
- Method: 提出MIDAS方法，通过软标签和视频帧的凸组合进行数据增强，扩展mixup技术至软标签视频数据。
- Result: 在DFEW和FERV39k-Plus数据集上，MIDAS训练模型优于现有最佳方法。
- Conclusion: MIDAS是一种简单高效的数据增强方法，显著提升了DFER任务中模糊表情的识别性能。


### [10] [THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion](https://arxiv.org/abs/2506.20877)
*Calin Teodor Ioan*

Main category: cs.CV

TL;DR: ThirdEye是一种基于显式单目线索的深度估计方法，通过预训练网络提供线索，并结合层次结构和记忆模块优化结果。

- Motivation: 传统深度估计方法忽略了人类视觉系统依赖的显式单目线索，如遮挡边界、阴影和透视。ThirdEye旨在通过显式提供这些线索改进深度估计。
- Method: ThirdEye使用预训练且冻结的专家网络提供单目线索，通过三阶段皮层层次结构（V1->V2->V3）和记忆模块融合线索，最终通过自适应分箱变换器生成高分辨率视差图。
- Result: 由于冻结了专家网络，ThirdEye继承了大量外部监督，仅需少量微调。
- Conclusion: ThirdEye通过显式线索和层次结构优化了深度估计，未来版本将提供更多实验细节和定量结果。


### [11] [MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans](https://arxiv.org/abs/2506.20879)
*Shubhankar Borse,Seokeon Choi,Sunghyun Park,Jeongho Kim,Shreya Kadambi,Risheek Garrepalli,Sungrack Yun,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了MultiHuman-Testbench，一个用于评估多人生成模型的基准，包含1800个样本和5550张人脸图像，并提出了多维度评估指标和新技术。

- Motivation: 生成包含多个人类且保留面部身份的复杂动作图像是一个挑战，缺乏专用基准是主要原因。
- Method: 引入MultiHuman-Testbench基准，提供文本提示和匹配的姿势条件图像，提出多维度评估指标和新技术（如分割和匈牙利匹配）。
- Result: 基准和新技术显著提高了ID相似性，为多人生成研究提供了标准化工具。
- Conclusion: MultiHuman-Testbench为多人生成研究提供了有价值的见解和标准化工具。


### [12] [The Role of Cyclopean-Eye in Stereo Vision](https://arxiv.org/abs/2506.20900)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 论文研究了现代立体视觉系统的几何基础，结合3D结构和人类感知改进深度重建，提出了新的几何约束，并通过理论和实验验证了其有效性。

- Motivation: 探索立体视觉系统的几何基础，结合人类感知和3D结构，以提高深度重建的准确性。
- Method: 重新审视Cyclopean Eye模型，提出新的几何约束，评估深度学习模型的立体特征匹配质量，并研究注意力机制的作用。
- Result: 理论和实证研究表明，结合几何先验和学习特征能更好地理解立体视觉系统。
- Conclusion: 几何先验与学习特征的结合为立体视觉系统提供了有效的内部抽象。


### [13] [FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing](https://arxiv.org/abs/2506.20911)
*Advait Gupta,Rishie Raj,Dang Nguyen,Tianyi Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为FaSTA$^*$的神经符号代理，通过结合大型语言模型（LLMs）的高效规划和局部A$^*$搜索，实现多轮图像编辑任务的成本优化。

- Motivation: 解决多轮图像编辑任务的高成本问题，通过结合快速规划和精确搜索，提高效率。
- Method: 结合LLMs的快速子任务规划和A$^*$搜索，提取并复用成功子程序，形成自适应快慢规划策略。
- Result: FaSTA$^*$在计算效率上显著优于现有方法，同时保持与最先进基线相当的成功率。
- Conclusion: FaSTA$^*$通过复用子程序和自适应规划，实现了高效且成本优化的图像编辑。


### [14] [M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization](https://arxiv.org/abs/2506.20922)
*Ju-Hyeon Nam,Dong-Hyun Moon,Sang-Chul Lee*

Main category: cs.CV

TL;DR: M2SFormer是一种基于Transformer的框架，通过统一多频率和多尺度注意力机制，结合全局先验图，显著提升了图像伪造定位的准确性和泛化能力。

- Motivation: 解决现有深度学习方法在图像伪造定位中计算开销大、表示能力不足的问题，尤其是对复杂或细微篡改的检测。
- Method: 提出M2SFormer框架，结合多频率和多尺度注意力机制，并引入全局先验图和难度引导注意力模块，以保留细微篡改特征。
- Result: 在多个基准数据集上，M2SFormer优于现有最先进模型，表现出更强的泛化能力。
- Conclusion: M2SFormer通过创新的注意力机制和全局先验图，显著提升了伪造检测和定位的性能。


### [15] [PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling](https://arxiv.org/abs/2506.20936)
*Hao Zhang,Haolan Xu,Chun Feng,Varun Jampani,Narendra Ahuja*

Main category: cs.CV

TL;DR: PhysRig提出了一种基于物理的皮肤绑定框架，解决了传统线性混合皮肤（LBS）的体积损失和非自然变形问题，适用于弹性材料建模。

- Motivation: 传统LBS方法在模拟弹性材料（如软组织、毛发等）时存在缺陷，PhysRig旨在通过物理模拟克服这些限制。
- Method: 将刚性骨架嵌入体积表示（如四面体网格），通过连续力学和粒子离散化实现可微分性，并引入材料原型减少学习空间。
- Result: 在合成数据集上表现优于传统LBS方法，生成更真实且物理合理的结果，适用于姿态迁移任务。
- Conclusion: PhysRig为皮肤绑定和骨架动画提供了更灵活、物理准确的解决方案。


### [16] [AIR-VIEW: The Aviation Image Repository for Visibility Estimation of Weather, A Dataset and Benchmark](https://arxiv.org/abs/2506.20939)
*Chad Mourning,Zhewei Wang,Justin Murray*

Main category: cs.CV

TL;DR: 本文介绍了一个新的航空天气数据集，填补了公开可用数据的空白，并提供了基准测试结果。

- Motivation: 航空天气的低成本替代方案需求增长，但缺乏适合监督学习的多样化、大规模数据集。
- Method: 通过一年的数据收集活动，从FAA天气摄像头网络获取图像，并测试了三种常用方法和一个通用基线。
- Result: 数据集填补了空白，基准测试结果与ASTM标准进行了对比。
- Conclusion: 新数据集和基准测试为航空天气能见度估计提供了有价值的资源。


### [17] [Hierarchical Sub-action Tree for Continuous Sign Language Recognition](https://arxiv.org/abs/2506.20947)
*Dejie Yang,Zhu Xu,Xinjie Gao,Yang Liu*

Main category: cs.CV

TL;DR: 提出了一种名为HST-CSLR的方法，通过构建层次化子动作树（HST）结合视觉和文本模态，利用大语言模型的知识提升连续手语识别性能。

- Motivation: 当前连续手语识别（CSLR）因缺乏大规模数据集和精确标注而受限，且现有方法未能充分利用文本模态的知识。
- Method: 构建层次化子动作树（HST）以结合视觉和文本模态，并引入对比对齐增强以减少模态间的差异。
- Result: 在四个数据集（PHOENIX-2014、PHOENIX-2014T、CSL-Daily和Sign Language Gesture）上验证了HST-CSLR的有效性。
- Conclusion: HST-CSLR通过层次化结构和对比对齐显著提升了连续手语识别的性能。


### [18] [OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs](https://arxiv.org/abs/2506.20960)
*Yiman Zhang,Ziheng Luo,Qiangyu Yan,Wei He,Borui Jiang,Xinghao Chen,Kai Han*

Main category: cs.CV

TL;DR: OmniEval是一个评估全模态模型（如MiniCPM-O 2.6）的基准，涵盖视觉、听觉和文本输入，具有全模态协作、视频多样性和任务多样性等特点。

- Motivation: 现有基准无法全面评估多模态模型的协作能力，因此设计了OmniEval以填补这一空白。
- Method: 设计了包含810个音频-视频同步视频和2617个问答对的任务，涵盖3大类12小类任务，并引入更细粒度的视频定位任务Grounding。
- Result: 实验在多个全模态模型上进行，展示了OmniEval的全面评估能力。
- Conclusion: OmniEval为评估多模态模型的上下文理解和协作能力提供了平台，代码和数据已公开。


### [19] [Evidence-based diagnostic reasoning with multi-agent copilot for human pathology](https://arxiv.org/abs/2506.20964)
*Chengkuan Chen,Luca L. Weishaupt,Drew F. K. Williamson,Richard J. Chen,Tong Ding,Bowen Chen,Anurag Vaidya,Long Phi Le,Guillaume Jaume,Ming Y. Lu,Faisal Mahmood*

Main category: cs.CV

TL;DR: PathChat+是一种专为病理学设计的新型多模态大语言模型，通过大量病理学指令样本训练，显著优于现有模型，并结合SlideSeek系统实现自主诊断推理。

- Motivation: 传统病理学模型缺乏自然语言指令和文本上下文的整合，现有多模态大语言模型在数据、多图像理解及自主诊断推理方面存在不足。
- Method: PathChat+基于超过100万病理学指令样本和550万问答对训练，结合SlideSeek系统进行迭代式分层诊断推理。
- Result: PathChat+在多项病理学基准测试中显著优于现有模型，SlideSeek系统在DDxBench上实现高精度诊断并生成可解释报告。
- Conclusion: PathChat+和SlideSeek系统填补了病理学AI在自主诊断和多模态理解上的空白，具有重要应用价值。


### [20] [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](https://arxiv.org/abs/2506.20967)
*Lingling Cai,Kang Zhao,Hangjie Yuan,Xiang Wang,Yingya Zhang,Kejie Huang*

Main category: cs.CV

TL;DR: DFVEdit是一种针对Video DiTs的高效零样本视频编辑方法，避免了注意力修改和微调，通过流变换直接操作潜在空间，显著提升了计算效率和编辑质量。

- Motivation: 现有视频编辑方法在Video DiTs上计算开销大，需要修改注意力或微调，DFVEdit旨在解决这一问题。
- Method: 提出Conditional Delta Flow Vector (CDFV)和结合Implicit Cross Attention (ICA)与Embedding Reinforcement (ER)，直接在潜在空间进行流变换。
- Result: DFVEdit在推理速度上提升20倍，内存减少85%，并在结构保真度、时空一致性和编辑质量上达到SOTA。
- Conclusion: DFVEdit为Video DiTs提供了一种高效、高质量的零样本编辑方案，适用于多种流行模型。


### [21] [From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging](https://arxiv.org/abs/2506.20977)
*Tao Liu,Dafeng Zhang,Gengchen Li,Shizhuo Liu,Yongqi Song,Senmao Li,Shiqi Yang,Boqian Li,Kai Wang,Yaxing Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Cradle2Cane的两阶段人脸老化框架，通过自适应噪声注入和身份嵌入技术，解决了年龄准确性与身份一致性之间的权衡问题。

- Motivation: 现有方法在实现真实且无缝的人脸老化时，难以平衡年龄准确性与身份一致性，尤其是在处理大年龄跨度或极端头部姿态时。
- Method: 采用两阶段框架：第一阶段通过自适应噪声注入（AdaNI）机制提高年龄准确性；第二阶段通过身份感知嵌入（IDEmb）增强身份一致性。
- Result: 在CelebA-HQ测试数据集上，Cradle2Cane在年龄准确性和身份一致性方面优于现有方法。
- Conclusion: Cradle2Cane通过两阶段设计有效解决了Age-ID权衡问题，为人脸老化任务提供了更优的解决方案。


### [22] [3D Scene-Camera Representation with Joint Camera Photometric Optimization](https://arxiv.org/abs/2506.20979)
*Weichen Dai,Kangcheng Ma,Jiaxin Wang,Kecen Pan,Yuhang Ming,Hua Zhang,Wanzeng Kong*

Main category: cs.CV

TL;DR: 提出一种联合相机光度优化的3D场景表示方法，通过光度模型和深度正则化提升场景表示质量。

- Motivation: 相机成像中的光度失真会降低图像质量，进而影响3D场景表示的准确性。
- Method: 引入内外光度模型，联合优化相机参数，并使用深度正则化防止场景无关信息的拟合。
- Result: 实验表明，该方法在成像退化条件下仍能生成高质量的3D场景表示。
- Conclusion: 通过联合优化相机模型和场景表示，有效分离了场景无关信息，提升了表示质量。


### [23] [Rethink Sparse Signals for Pose-guided Text-to-image Generation](https://arxiv.org/abs/2506.20983)
*Wenjie Xuan,Jing Zhang,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏信号（如OpenPose）的新型Spatial-Pose ControlNet（SP-Ctrl），用于姿态引导的图像生成，解决了密集信号（如深度、DensePose）带来的编辑困难和与文本提示不一致的问题。

- Motivation: 密集信号在姿态引导的图像生成中存在编辑困难和与文本提示不一致的问题，而稀疏信号因其简单性和形状无关性被重新审视。
- Method: 论文扩展了OpenPose为可学习的空间表示，并引入关键点概念学习，增强关键点嵌入的表达力和姿态对齐。
- Result: 实验表明，SP-Ctrl在稀疏姿态引导下优于现有方法，甚至与密集信号方法性能相当，且在跨物种生成中表现出色。
- Conclusion: SP-Ctrl证明了稀疏信号在姿态引导图像生成中的潜力，提供了高效且灵活的解决方案。


### [24] [EVA: Mixture-of-Experts Semantic Variant Alignment for Compositional Zero-Shot Learning](https://arxiv.org/abs/2506.20986)
*Xiao Zhang,Yongqiang Ma,Haodong Jing,Nanning Zheng*

Main category: cs.CV

TL;DR: 本文提出EVA框架，通过多专家域适应和语义变体对齐，提升组合零样本学习的性能。

- Motivation: 现有CZSL方法通过简单的组合-原型映射提取特征，忽略了语义子集和组合差异，导致性能受限。
- Method: 引入多专家域适应实现标记感知学习，并通过语义变体对齐选择相关表示进行图像-原语匹配。
- Result: 在三个流行基准测试中，EVA在封闭和开放世界设置下均显著优于其他最先进方法。
- Conclusion: EVA框架通过高质量原语表示和精确组合泛化，有效提升了CZSL性能。


### [25] [Segment Anything in Pathology Images with Natural Language](https://arxiv.org/abs/2506.20988)
*Zhixuan Chen,Junlin Hou,Liqi Lin,Yihui Wang,Yequan Bie,Xi Wang,Yanning Zhou,Ronald Cheong Kin Chan,Hao Chen*

Main category: cs.CV

TL;DR: PathSegmentor是一种基于文本提示的病理图像分割基础模型，解决了现有方法在临床应用中因标注数据有限和类别定义受限而面临的挑战。

- Motivation: 当前病理图像分割方法在临床应用中存在标注数据不足和类别定义受限的问题，需要更灵活、高效的解决方案。
- Method: 提出PathSegmentor模型和PathSeg数据集，支持自然语言提示的分割，无需空间输入。
- Result: PathSegmentor在Dice分数上显著优于现有模型，具有更高的准确性和泛化能力。
- Conclusion: PathSegmentor推动了精准肿瘤学中可解释AI的发展，为临床决策提供了支持。


### [26] [TSDASeg: A Two-Stage Model with Direct Alignment for Interactive Point Cloud Segmentation](https://arxiv.org/abs/2506.20991)
*Chade Li,Pengju Zhang,Yihong Wu*

Main category: cs.CV

TL;DR: 提出TSDASeg模型，通过两阶段设计和直接跨模态对齐模块，解决3D点云与文本/2D图像对齐问题，提升交互式点云分割性能。

- Motivation: 现有方法在点级任务（如分割）中表现不佳，因缺乏直接的3D-文本对齐，无法将局部3D特征与文本上下文关联。
- Method: TSDASeg模型包含两阶段设计、直接跨模态对齐模块和内存模块。内存模块通过专用存储库动态更新场景特征。
- Result: 在多个3D数据集上实验，表现优于现有方法。
- Conclusion: TSDASeg通过直接对齐和动态内存机制，显著提升了交互式点云分割的性能。


### [27] [Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](https://arxiv.org/abs/2506.20995)
*Akio Hayakawa,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出了一种逐步视频到音频生成方法，通过分步生成特定声音事件的音频轨道，模仿传统Foley工作流程。

- Motivation: 传统方法难以全面捕捉视频中的所有声音事件，因此需要一种更系统的方法来生成高质量复合音频。
- Method: 采用分步视频到音频合成任务，结合目标文本提示和先前生成的音频轨道进行引导生成，利用预训练模型避免专用数据集需求。
- Result: 实验表明，该方法能为单一视频生成多个语义不同的音频轨道，合成音频质量优于现有基线。
- Conclusion: 该方法通过分步生成和引导合成，显著提升了复合音频的质量和多样性。


### [28] [DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting](https://arxiv.org/abs/2506.20998)
*Yeon-Ji Song,Jaein Kim,Byung-Ju Kim,Byoung-Tak Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为DBMovi-GS的方法，用于从模糊的单目视频中合成动态场景的新视角，解决了现有方法在动态模糊场景中的不足。

- Motivation: 现有新视角合成方法依赖高分辨率图像或静态几何假设，无法有效处理动态模糊场景，导致视觉质量下降。
- Method: 通过稀疏控制的高斯分布方法（Gaussian Splatting）生成密集3D高斯，从模糊视频中恢复清晰度并重建动态场景的3D几何。
- Result: 模型在动态模糊场景中实现了鲁棒的新视角合成性能，并为模糊单目视频输入设定了新的基准。
- Conclusion: DBMovi-GS方法有效解决了动态模糊场景的新视角合成问题，提升了视觉保真度和稳定性。


### [29] [Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology](https://arxiv.org/abs/2506.21001)
*Qiuyi Qi,Xin Li,Ming Kong,Zikang Xu,Bingdi Chen,Qiang Zhu,S Kevin Zhou*

Main category: cs.CV

TL;DR: 提出了一种风格对齐的图像合成方法（SAIC），用于增强异常细胞检测模型的性能和鲁棒性，解决了标注质量低、数据分布长尾和染色风格不一致等问题。

- Motivation: 解决细胞病理学中异常细胞检测面临的标注质量低、数据分布长尾和染色风格不一致等挑战。
- Method: 通过属性指导从异常细胞库中选择候选细胞，利用高频特征重建实现风格对齐的高保真合成，并引入大视觉语言模型筛选高质量合成图像。
- Result: 实验表明，SAIC合成的图像显著提升了异常细胞检测的性能和鲁棒性，特别是在尾部类别和风格上。
- Conclusion: SAIC在临床应用中具有广泛的通用性和实用性，代码将开源。


### [30] [Inverse Scene Text Removal](https://arxiv.org/abs/2506.21002)
*Takumi Yoshimatsu,Shumpei Takezaki,Seiichi Uchida*

Main category: cs.CV

TL;DR: 本文提出逆向场景文本移除（ISTR），用于检测图像是否经过文本移除处理，并定位移除区域，实验证明其高准确性，同时尝试恢复移除文本内容。

- Motivation: 场景文本移除（STR）技术可能被滥用，因此需要逆向分析技术（ISTR）来检测和定位移除区域，以防止潜在滥用并改进STR技术。
- Method: ISTR通过分析STR处理后的图像，进行二元分类（检测是否经过STR处理）和文本移除区域定位，同时训练文本识别器尝试恢复移除内容。
- Result: 实验证明ISTR在检测和定位任务中具有高准确性，但恢复移除文本内容较为困难。
- Conclusion: ISTR为STR技术的滥用提供了检测手段，同时揭示了恢复移除文本的挑战。


### [31] [VisionGuard: Synergistic Framework for Helmet Violation Detection](https://arxiv.org/abs/2506.21005)
*Lam-Huy Nguyen,Thinh-Phuc Nguyen,Thanh-Hai Nguyen,Gia-Huy Dinh,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: VisionGuard是一个多阶段框架，通过自适应标记和上下文扩展模块提升头盔违规检测的准确性和一致性。

- Motivation: 解决头盔违规检测中因环境变化、摄像头角度和数据不一致导致的分类和检测问题。
- Method: 结合自适应标记模块（基于跟踪的标签修正）和上下文扩展模块（生成虚拟边界框以解决数据不平衡）。
- Result: 相比基线检测器，整体mAP提高了3.1%。
- Conclusion: VisionGuard在交通监控系统中具有实际应用潜力，可提升安全性和法规遵从性。


### [32] [Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning](https://arxiv.org/abs/2506.21006)
*Tyler Ward,Xiaoqin Wang,Braxton McFarland,Md Atik Ahamed,Sahar Nozad,Talal Arshad,Hafsa Nebbache,Jin Chen,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出了一种结合Segment Anything Model (SAM)和Forward-Forward Contrastive Learning (FFCL)的深度学习框架，用于提高乳腺癌术中标本边缘评估的准确性和速度。

- Motivation: 当前使用的2D标本放射成像（SR）方法在评估术中标本边缘状态时准确性有限，导致近四分之一的患者需要额外手术。
- Method: 通过标注SR图像中的恶性肿瘤、非恶性组织和病理确认边缘区域，使用FFCL预训练ResNet-18进行分类，并重构粗糙二值掩码以提示SAM进行精细边缘分割。
- Result: AUC为0.8455，Dice相似度比基线模型提高27.4%，推理时间缩短至每张图像47毫秒。
- Conclusion: FFCL-SAM显著提高了术中边缘评估的速度和准确性，有望降低再切除率并改善乳腺癌治疗效果。


### [33] [The Aging Multiverse: Generating Condition-Aware Facial Aging Tree via Training-Free Diffusion](https://arxiv.org/abs/2506.21008)
*Bang Gong,Luchao Qi,Jiaye Wu,Zhicheng Fu,Chunbo Song,David W. Jacobs,John Nicholson,Roni Sengupta*

Main category: cs.CV

TL;DR: 提出了一种名为Aging Multiverse的框架，用于从单张图像生成多种可能的面部老化轨迹，每种轨迹受环境、健康和生活方式等外部因素影响。

- Motivation: 现有方法通常将老化建模为单一确定性路径，无法反映多样化的未来。本文旨在通过多维度、可控和可解释的老化过程，拓展数字叙事、健康教育和个性化可视化等应用。
- Method: 提出了一种无需训练的基于扩散的方法，结合注意力混合和模拟老化正则化策略，以平衡身份保留、年龄准确性和条件控制。
- Result: 实验和用户研究表明，该方法在身份保留、老化真实性和条件对齐方面表现优异，优于现有编辑和老化模型。
- Conclusion: 通过将老化转化为多维度、可控和可解释的过程，该方法为数字叙事、健康教育和个性化可视化开辟了新途径。


### [34] [User-in-the-Loop View Sampling with Error Peaking Visualization](https://arxiv.org/abs/2506.21009)
*Ayaka Yasunaga,Hideo Saito,Shohei Mori*

Main category: cs.CV

TL;DR: 提出了一种基于局部重建光场和可视化误差的方法，用于新视角合成，减少用户负担并扩展场景探索范围。

- Motivation: 现有方法依赖3D标注且限制场景探索范围，用户负担重。
- Method: 使用局部重建光场和可视化误差，指导用户插入新视角。
- Result: 误差峰值可视化侵入性低，减少失望感，且所需视角样本更少。
- Conclusion: 该方法可扩展至更大场景的辐射场重建，如3D高斯泼溅。


### [35] [Bridging Video Quality Scoring and Justification via Large Multimodal Models](https://arxiv.org/abs/2506.21011)
*Qizhi Xie,Kun Yuan,Yunpeng Qu,Jiachao Gong,Mingda Wu,Ming Sun,Chao Zhou,Jihong Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种基于评分的指令生成（SIG）管道，用于视频质量评估（VQA），通过自动化生成指令数据，解决了传统方法依赖人工标注的问题，并提升了视频大型多模态模型（LMMs）的质量评分和解释能力。

- Motivation: 传统VQA方法仅提供数值评分，无法描述视频质量的复杂维度，限制了其应用。通过利用语言输出和指令调优，视频LMMs有望解决这一问题。
- Method: 提出SIG管道，自动为未标记视频评分并映射到文本级别，结合层次化思维链（CoT）建模质量维度与整体质量的关系，生成Score2Instruct（S2I）数据集。采用渐进式调优策略提升视频LMMs能力。
- Result: 实验表明，S2I数据集包含32万多样化的指令-响应对，显著提升了视频LMMs的质量评分和解释能力。
- Conclusion: SIG管道和S2I数据集为视频质量评估提供了高效、可扩展的解决方案，推动了视频LMMs在质量评估领域的应用。


### [36] [FedSC: Federated Learning with Semantic-Aware Collaboration](https://arxiv.org/abs/2506.21012)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.CV

TL;DR: 论文提出了一种名为FedSC的联邦学习方法，通过语义感知协作解决数据异构性问题，利用关系原型和一致原型捕获客户端特定的类相关知识。

- Motivation: 联邦学习中的数据异构性问题（如客户端标签偏好偏差）限制了模型性能，现有方法常忽略客户端内部的语义信息。
- Method: FedSC通过构建关系原型和一致原型，结合对比学习和差异聚合策略，优化本地模型。
- Result: 实验证明FedSC在多种挑战性场景中有效，关键组件提升了效率。
- Conclusion: FedSC通过语义级协作显著提升了联邦学习在数据异构环境中的性能。


### [37] [HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation](https://arxiv.org/abs/2506.21015)
*Qingyue Jiao,Kangyu Zheng,Yiyu Shi,Zhiding Liang*

Main category: cs.CV

TL;DR: 提出了一种结合经典与量子计算的生成对抗网络（GAN），首次实现彩色医学图像生成，性能优于现有方法。

- Motivation: 解决皮肤疾病数据集的类别不平衡、隐私问题和对象偏差，同时克服量子计算生成低质量灰度图像的局限。
- Method: 采用经典-量子潜在空间融合技术，构建混合经典-量子GAN。
- Result: 模型在图像生成质量和分类性能提升上优于现有方法，且参数和训练时间大幅减少。
- Conclusion: 量子图像生成在量子硬件进步下前景广阔，模型在真实量子机器上表现稳健。


### [38] [Multimodal Prompt Alignment for Facial Expression Recognition](https://arxiv.org/abs/2506.21017)
*Fuyan Ma,Yiran He,Bin Sun,Shutao Li*

Main category: cs.CV

TL;DR: 提出了一种多模态提示对齐框架（MPA-FER），通过细粒度语义指导和跨模态对齐，提升视觉语言模型在面部表情识别中的性能。

- Motivation: 现有视觉语言模型在面部表情识别中难以捕捉细粒度的文本-视觉关系，导致对细微表情差异的区分能力不足。
- Method: 采用多粒度硬提示生成策略（利用ChatGPT生成详细描述），并通过特征差异最小化将外部知识注入软提示；结合原型引导的视觉特征对齐和跨模态全局-局部对齐模块。
- Result: 在三个FER基准数据集上表现优于现有方法，同时保持了预训练模型的泛化能力并降低了计算成本。
- Conclusion: MPA-FER框架通过细粒度语义指导和跨模态对齐，显著提升了面部表情识别的性能，且计算高效。


### [39] [LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection](https://arxiv.org/abs/2506.21018)
*Lei Hao,Lina Xu,Chang Liu,Yanni Dong*

Main category: cs.CV

TL;DR: 提出了一种轻量级注意力引导自调制特征融合网络（LASFNet），通过单特征级融合单元简化训练过程，显著降低计算成本，同时提升检测精度。

- Motivation: 解决多模态目标检测中复杂特征融合方法带来的高计算开销问题。
- Method: 引入注意力引导自调制特征融合（ASFF）模块和轻量级特征注意力变换模块（FATM），自适应调整融合特征响应。
- Result: 在三个数据集上，参数和计算成本分别减少90%和85%，检测精度提升1%-3%。
- Conclusion: LASFNet在效率和精度之间取得了良好平衡，为多模态目标检测提供了高效解决方案。


### [40] [Instella-T2I: Pushing the Limits of 1D Discrete Latent Space Image Generation](https://arxiv.org/abs/2506.21022)
*Ze Wang,Hao Chen,Benran Hu,Jiang Liu,Ximeng Sun,Jialian Wu,Yusheng Su,Xiaodong Yu,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: 提出了一种1D二进制图像潜在表示方法，显著减少了图像建模的token数量，提升了效率和性能。

- Motivation: 减少高分辨率图像建模的计算需求，提升图像和多模态理解的效率。
- Method: 引入1D二进制图像潜在空间，将图像表示为二进制向量序列，替代传统one-hot编码。
- Result: 在1024x1024图像上仅需128个离散token，比标准VQ-VAEs减少32倍token数量，训练和推理速度显著提升。
- Conclusion: 该方法为图像生成提供了一种高效且可扩展的替代方案，无需私有数据或后训练优化。


### [41] [DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic Perception and Manipulation](https://arxiv.org/abs/2506.21034)
*Wenzhou Lyu,Jialing Lin,Wenqi Ren,Ruihao Xia,Feng Qian,Yang Tang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的深度补全框架DidSee，用于解决非朗伯物体深度图噪声和不完整的问题，通过改进噪声调度器和训练策略，结合语义增强器，实现了高性能和鲁棒性。

- Motivation: 商业RGB-D相机在非朗伯物体上产生的深度图通常噪声大且不完整，传统方法因训练数据有限而泛化能力差。扩散模型虽有潜力，但存在训练-推理不匹配和特征不足的问题。
- Method: 提出DidSee框架：1) 使用重新缩放的噪声调度器消除信号泄漏偏差；2) 设计噪声无关的单步训练策略减少误差累积；3) 结合语义增强器联合优化深度补全和语义分割。
- Result: 在多个基准测试中达到最先进性能，展示了强大的现实世界泛化能力，并有效提升了下游任务（如姿态估计和机器人抓取）的表现。
- Conclusion: DidSee通过改进扩散框架和引入语义增强，显著提升了非朗伯物体的深度补全性能，具有广泛的应用潜力。


### [42] [Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability](https://arxiv.org/abs/2506.21042)
*Boyong He,Yuxiang Ji,Zhuoyue Tan,Liaoni Wu*

Main category: cs.CV

TL;DR: 提出一种利用扩散模型中间特征的方法，显著减少推理时间并提升性能，同时通过辅助分支和一致性损失增强泛化能力。

- Motivation: 解决检测器因训练与测试数据域差异导致的性能下降问题，并优化扩散模型在领域泛化和适应任务中的应用。
- Method: 提取单步扩散过程的中间特征，构建对象中心辅助分支，应用一致性损失，并在统一框架中实现特征和目标对齐。
- Result: 在3个DA和5个DG基准测试中取得竞争性结果，并在COCO泛化基准中表现优异。
- Conclusion: 展示了扩散模型在领域泛化和适应检测任务中的优越性，为跨领域视觉感知任务提供了新思路。


### [43] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Main category: cs.CV

TL;DR: 提出FGS方法，通过忠实度指导和调度策略，在图像编辑中平衡可编辑性和忠实度。

- Motivation: 解决文本引导扩散模型在图像编辑中可编辑性与忠实度之间的固有权衡问题。
- Method: 引入忠实度指导以增强输入图像信息的保留，并提出调度策略解决可编辑性与忠实度的不对齐问题。
- Result: 实验表明FGS在保持可编辑性的同时显著提升了忠实度，且兼容多种编辑方法。
- Conclusion: FGS是一种高效的方法，能够在多样化任务中实现高质量图像编辑。


### [44] [Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features](https://arxiv.org/abs/2506.21046)
*Shangbo Wu,Yu-an Tan,Ruinan Ma,Wencong Ma,Dehua Zhu,Yuanzhang Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于自监督Vision Transformer（ViT）的对抗攻击方法dSVA，通过利用对比学习（CL）和掩码图像建模（MIM）的全局与局部特征，显著提升了对抗样本的黑盒迁移性。

- Motivation: 传统对抗攻击依赖监督学习的中间特征，而自监督学习与ViT的结合可能提供更优的特征表示，从而提升对抗样本的泛化能力。
- Method: 设计了dSVA方法，结合CL和MIM的全局与局部特征，利用生成器框架和注意力机制生成对抗样本。
- Result: 实验表明，CL和MIM使ViT关注不同特征倾向，联合利用时显著提升了对抗样本的黑盒迁移性，优于现有方法。
- Conclusion: 自监督ViT特征的双重利用为对抗攻击提供了新思路，展示了其在黑盒迁移中的潜力。


### [45] [Class-Agnostic Region-of-Interest Matching in Document Images](https://arxiv.org/abs/2506.21055)
*Demin Zhang,Jiahao Lyu,Zhijie Shen,Yu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为“RoI-Matching”的新任务，旨在灵活、高效地匹配用户自定义的文档区域，并构建了基准数据集RoI-Matching-Bench和框架RoI-Matcher。

- Motivation: 现有文档分析解决方案无法满足用户灵活定制需求，因此需要一种新的方法来实现多粒度、开放式的区域匹配。
- Method: 提出RoI-Matcher框架，采用孪生网络提取多级特征，并通过交叉注意力层对齐不同域的语义。
- Result: 实验表明，该方法在RoI-Matching-Bench上表现有效，为后续研究提供了基线。
- Conclusion: RoI-Matching任务和RoI-Matcher框架为文档分析提供了灵活、高效的解决方案，并推动了进一步研究。


### [46] [SAMURAI: Shape-Aware Multimodal Retrieval for 3D Object Identification](https://arxiv.org/abs/2506.21056)
*Dinh-Khoi Vo,Van-Loc Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: SAMURAI结合CLIP语义匹配和形状引导重排，解决了仅用2D掩码图像和自然语言描述检索3D对象的挑战。

- Motivation: 在复杂室内环境中，仅用掩码2D图像和自然语言描述检索3D对象存在困难，如视角扭曲、模糊语言提示等。
- Method: 提出SAMURAI框架，结合CLIP语义匹配、形状引导重排和多数投票策略，并优化掩码预处理。
- Result: 在ROOMELSA测试集上表现优异，验证了形状先验与语言理解结合的重要性。
- Conclusion: 形状先验与语言理解的结合对开放世界3D对象检索至关重要。


### [47] [PoseMaster: Generating 3D Characters in Arbitrary Poses from a Single Image](https://arxiv.org/abs/2506.21076)
*Hongyu Yan,Kunming Luo,Weiyu Li,Yixun Liang,Shengming Li,Jingwei Huang,Chunchao Guo,Ping Tan*

Main category: cs.CV

TL;DR: PoseMaster是一个端到端的可控3D角色生成框架，统一了姿势变换和3D角色生成，利用3D骨骼作为姿势条件，并通过随机空置条件提升控制效果。

- Motivation: 现有方法在姿势标准化阶段因自遮挡和视角问题易生成失真图像，影响后续3D重建质量。
- Method: 提出基于流的3D原生生成框架，利用3D骨骼作为姿势条件，训练中随机空置条件以提升泛化性。
- Result: 实验表明PoseMaster在A-pose角色生成上优于现有技术，并能精确控制任意姿势。
- Conclusion: PoseMaster通过统一框架和优化条件控制，显著提升了3D角色生成的效率和质量。


### [48] [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](https://arxiv.org/abs/2506.21080)
*Sanjoy Chowdhury,Subrata Biswas,Sayan Nag,Tushar Nagarajan,Calvin Murdock,Ishwarya Ananthabhotla,Yijun Qian,Vamsi Krishna Ithapu,Dinesh Manocha,Ruohan Gao*

Main category: cs.CV

TL;DR: EgoAdapt框架通过跨模态蒸馏和策略学习，显著提升了多感知自我中心任务的计算效率，同时保持或超越现有模型的性能。

- Motivation: 解决现代感知模型在资源受限环境中的高计算成本问题，实现高效推理。
- Method: 提出EgoAdapt框架，结合跨模态蒸馏和策略学习，适应不同任务的动作空间。
- Result: 在三个数据集上，计算量减少89.09%，参数减少82.02%，能耗降低9.6倍，性能与或优于现有模型。
- Conclusion: EgoAdapt为资源受限环境中的高效感知任务提供了可行解决方案。


### [49] [ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and Accurate Stereo Matching](https://arxiv.org/abs/2506.21091)
*Mahmoud Tahmasebi,Saif Huq,Kevin Meehan,Marion McAfee*

Main category: cs.CV

TL;DR: 论文提出了一种增强型Shuffle Mixer（ESM）方法，用于解决基于小规模成本体积的立体匹配中信息丢失的问题，实现了高精度和实时性能。

- Motivation: 立体匹配在自动驾驶系统中至关重要，但现有深度学习方法难以同时实现高精度和实时性能。大尺度成本体积计算量大，小尺度成本体积则信息不足。
- Method: 提出ESM方法，通过将主要特征整合到视差上采样单元中，快速提取初始视差估计的特征并与图像特征融合，利用轻量级特征引导的沙漏网络恢复细节。
- Result: ESM在高端GPU上达到116 FPS，AGX Orin上达到91 FPS，同时保持高精度视差图重建。
- Conclusion: ESM通过优化局部上下文连接和大感受野，实现了实时高精度立体匹配，为自动驾驶系统提供了有效解决方案。


### [50] [OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography](https://arxiv.org/abs/2506.21101)
*Caoshuo Li,Zengmao Ding,Xiaobin Hu,Bang Li,Donghao Luo,AndyPian Wu,Chaoyang Wang,Chengjie Wang,Taisong Jin,SevenShu,Yunsheng Wu,Yongge Liu,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了一种名为OracleFusion的两阶段语义排版框架，用于解读甲骨文未破译字符，结合多模态大语言模型和空间感知推理，生成语义丰富的矢量字体，显著提升可读性和美观性。

- Motivation: 甲骨文作为早期古代语言，记录了丰富的文化和智慧，但仍有大量字符未破译，其复杂结构和抽象图像为解读带来挑战。
- Method: 采用两阶段框架：第一阶段利用增强空间感知推理的多模态大语言模型分析字形结构；第二阶段引入甲骨结构矢量融合（OSVF），确保生成语义丰富的矢量字体。
- Result: 实验表明，OracleFusion在语义、视觉吸引力和字形维护方面优于现有基线模型，显著提升可读性和美观性。
- Conclusion: OracleFusion为甲骨文破译提供了专家级见解，是推动甲骨文解读的有价值工具。


### [51] [Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection](https://arxiv.org/abs/2506.21109)
*Luosheng Xu,Dalin Zhang,Zhaohui Song*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级遥感变化检测模型FlickCD，通过增强差异模块和多尺度特征融合，在减少计算资源的同时保持高精度。

- Motivation: 现有深度学习模型在遥感变化检测中计算复杂且资源消耗大，但精度提升有限。研究旨在开发高效轻量级模型，适用于卫星端处理。
- Method: 提出FlickCD模型，包含增强差异模块（EDM）抑制无关变化，以及局部-全局融合块（SWSA和EGSA）捕捉多尺度语义信息。
- Result: 在四个基准数据集上，FlickCD显著降低计算和存储开销（减少一个数量级），同时达到或接近SOTA性能（F1损失<1%）。
- Conclusion: FlickCD在性能和资源消耗之间取得了优异平衡，为遥感变化检测提供了一种高效解决方案。


### [52] [IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes](https://arxiv.org/abs/2506.21116)
*Yujia Liang,Jile Jiao,Zhicheng Wang,Xuetao Feng,Zixuan Ye,Yuan Wang,Hao Lu*

Main category: cs.CV

TL;DR: 论文提出MultiClip-Bench数据集和IPFormer-VideoLLM模型，解决VideoLLMs在多镜头场景中的性能问题。

- Motivation: 现有VideoLLMs在多镜头场景中表现不佳，缺乏相关数据集和模型优化。
- Method: 引入MultiClip-Bench数据集，并提出IPFormer-VideoLLM模型，通过实例提示增强特征聚合。
- Result: 新数据集和模型显著提升多场景视频理解能力，并在多个基准测试中表现优异。
- Conclusion: MultiClip-Bench和IPFormer-VideoLLM有效解决了多镜头场景的挑战，提升了模型性能。


### [53] [CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization](https://arxiv.org/abs/2506.21117)
*Jan Ackermann,Jonas Kulhanek,Shengqu Cai,Haofei Xu,Marc Pollefeys,Gordon Wetzstein,Leonidas Guibas,Songyou Peng*

Main category: cs.CV

TL;DR: CL-Splats是一种动态3D场景表示更新方法，通过增量更新高斯泼溅表示，结合变化检测模块，实现高效局部优化，提升重建质量。

- Motivation: 在动态3D环境中，实时更新场景表示对机器人、混合现实和嵌入式AI至关重要，但传统方法计算开销大，需要高效解决方案。
- Method: CL-Splats引入增量更新机制和高斯泼溅表示，结合变化检测模块，实现局部优化和场景状态存储与恢复。
- Result: 实验表明，CL-Splats在更新效率和重建质量上优于现有技术。
- Conclusion: CL-Splats为实时3D场景重建任务提供了高效解决方案，奠定了未来研究基础。


### [54] [GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction](https://arxiv.org/abs/2506.21121)
*Muleilan Pei,Shaoshuai Shi,Lu Zhang,Peiliang Li,Shaojie Shen*

Main category: cs.CV

TL;DR: 提出了一种基于图导向逆强化学习（GoIRL）的轨迹预测框架，通过向量化上下文表示和最大熵IRL范式，结合分层轨迹生成器，显著提升了预测性能。

- Motivation: 自动驾驶中周围代理的轨迹预测具有高度不确定性和多模态性，现有数据驱动方法依赖监督学习，存在局限性。
- Method: 采用GoIRL框架，结合特征适配器和最大熵IRL推断奖励分布，通过分层参数化轨迹生成器和概率融合策略优化预测。
- Result: 在Argoverse和nuScenes基准测试中达到最优性能，并展现出优于现有监督模型的泛化能力。
- Conclusion: GoIRL框架为轨迹预测提供了一种高效且泛化性强的解决方案。


### [55] [Learning to See in the Extremely Dark](https://arxiv.org/abs/2506.21132)
*Hai Jiang,Binhao Guan,Zhen Liu,Xiaohong Liu,Jian Yu,Zheng Liu,Songchen Han,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 论文提出了一种用于极低光RAW图像增强的扩散模型框架，并创建了一个名为SIED的大规模数据集。

- Motivation: 现有方法在极低光环境（0.0001 lux）下的表现尚未探索，缺乏相关数据集。
- Method: 提出了一种数据合成管道生成极低光RAW图像，并设计了一个扩散模型框架，包含自适应光照校正模块和颜色一致性损失。
- Result: 在SIED数据集和公开基准测试中验证了方法的有效性。
- Conclusion: 该方法在极低光条件下表现出色，代码和数据集已开源。


### [56] [YOLO-FDA: Integrating Hierarchical Attention and Detail Enhancement for Surface Defect Detection](https://arxiv.org/abs/2506.21135)
*Jiawei Hu*

Main category: cs.CV

TL;DR: YOLO-FDA是一种基于YOLO的检测框架，通过细粒度细节增强和注意力引导特征融合，解决了工业表面缺陷检测中的冗余特征、细节敏感度不足和多尺度鲁棒性弱的问题。

- Motivation: 工业表面缺陷检测因缺陷类型多样、形状不规则、细粒度要求和复杂纹理而具有挑战性，现有方法存在冗余特征、细节敏感度不足和多尺度鲁棒性弱的问题。
- Method: YOLO-FDA采用BiFPN架构增强多级特征聚合，引入DDFM模块捕捉细粒度结构变化，并提出AC和CAF两种注意力融合策略提升上下文表示和减少特征噪声。
- Result: 在多个基准数据集上，YOLO-FDA在准确性和鲁棒性上均优于现有最先进方法。
- Conclusion: YOLO-FDA通过创新的特征增强和融合策略，显著提升了工业表面缺陷检测的性能。


### [57] [Tree-based Semantic Losses: Application to Sparsely-supervised Large Multi-class Hyperspectral Segmentation](https://arxiv.org/abs/2506.21150)
*Junwen Wang,Oscar Maccormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: 提出两种基于树结构的语义损失函数，用于改进高光谱图像（HSI）的生物医学分割任务，利用标签层次结构提升性能，并在稀疏标注数据集上达到SOTA。

- Motivation: 现有学习方法对错误惩罚无差别，未能利用标签空间的语义层次结构，限制了HSI在手术应用中的潜力。
- Method: 引入两种树基语义损失函数，结合稀疏标注训练方法，利用标签层次结构优化分割。
- Result: 在包含107类的稀疏标注HSI数据集上达到SOTA性能，同时有效检测OOD像素且不影响ID像素的分割。
- Conclusion: 树基语义损失函数显著提升HSI分割性能，为手术应用提供更精确的工具。


### [58] [Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels](https://arxiv.org/abs/2506.21151)
*Aida Moafi,Danial Moafi,Evgeny M. Mirkes,Gerry P. McCann,Abbas S. Alatrany,Jayanth R. Arnold,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的自动化心肌瘢痕分割方法，通过改进损失函数和数据增强解决了标签噪声和数据异质性等问题，性能优于现有模型。

- Motivation: 心肌瘢痕的准确分割对临床评估和治疗规划至关重要，但现有方法面临标签噪声、数据异质性和类别不平衡等挑战。
- Method: 通过微调前沿模型，使用Kullback-Leibler损失函数和大量数据增强，解决了标签噪声和数据异质性问题。
- Result: 在急性和慢性病例中表现优异，生成的分割结果准确且平滑，优于nnU-Net等模型，并在分布外测试集上表现出强泛化能力。
- Conclusion: 该方法为自动化心肌瘢痕量化提供了可靠基础，支持深度学习在心脏影像中的广泛应用。


### [59] [Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image](https://arxiv.org/abs/2506.21152)
*Pufan Li,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: 提出一种新方法，通过整合几何和感知先验，从单张图像重建高保真3D物体，无需额外训练。

- Motivation: 现有方法在多视角一致性和几何细节上表现不佳，需要改进。
- Method: 训练三个高斯分支（几何先验、感知先验和高斯噪声），通过交互和重投影策略优化3D高斯分支。
- Result: 实验显示，该方法在新视角合成和3D重建上优于现有方法，生成结果更一致且细节丰富。
- Conclusion: 该方法有效解决了多视角一致性和几何细节问题，实现了高质量的3D物体生成。


### [60] [Topology-Aware Modeling for Unsupervised Simulation-to-Reality Point Cloud Recognition](https://arxiv.org/abs/2506.21165)
*Longkun Zou,Kangjun Liu,Ke Chen,Kailing Guo,Kui Jia,Yaowei Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为TAM的框架，用于解决3D点云数据中Sim2Real的域适应问题，通过全局拓扑建模和自监督学习提升分类器的泛化能力。

- Motivation: 3D点云数据因采集方法不同导致几何变化显著，现有UDA方法因缺乏域不敏感描述符而难以应对Sim2Real的域差距。
- Method: 提出TAM框架，利用全局空间拓扑和局部几何特征的拓扑关系，结合自监督任务和自训练策略。
- Result: 在三个公开Sim2Real基准测试中表现优于现有方法。
- Conclusion: TAM框架有效缩小了Sim2Real的域差距，提升了点云分类器的性能。


### [61] [Task-Aware KV Compression For Cost-Effective Long Video Understanding](https://arxiv.org/abs/2506.21184)
*Minghao Qin,Yan Shu,Peitian Zhang,Kun Lun,Huaying Yuan,Juenjie Zhou,Shitao Xiao,Bo Zhao,Zheng Liu*

Main category: cs.CV

TL;DR: Video-X^2L通过双级KV压缩和选择性KV重加载，高效解决长视频理解中的计算成本问题，无需额外训练且兼容现有MLLMs。

- Motivation: 长视频理解（LVU）因计算成本高而具有挑战性，现有KV压缩方法在高压缩比下信息损失严重。
- Method: 提出Video-X^2L，包含双级KV压缩（生成L-KVs和H-KVs）和选择性KV重加载（解码阶段动态选择KV）。
- Result: 在多个LVU基准测试中表现优异，显著优于现有KV压缩方法，同时大幅节省计算成本。
- Conclusion: Video-X^2L是一种简单高效的方法，无需额外训练即可提升长视频理解性能。


### [62] [Out-of-Distribution Semantic Occupancy Prediction](https://arxiv.org/abs/2506.21185)
*Yuheng Zhang,Mengfei Duan,Kunyu Peng,Yuhang Wang,Ruiping Liu,Fei Teng,Kai Luo,Zhiyong Li,Kailun Yang*

Main category: cs.CV

TL;DR: 论文提出了一种用于3D语义占据预测的OoD检测方法，通过合成异常数据增强数据集，并开发了OccOoD框架，实现了先进的OoD检测性能。

- Motivation: 现有方法在分布外（OoD）场景和长尾分布中表现不佳，导致安全隐患，需改进。
- Method: 提出合成异常集成管道生成数据集（VAA-KITTI和VAA-KITTI-360），并开发OccOoD框架，结合Voxel-BEV渐进融合（VBPF）和RWKV分支。
- Result: OccOoD在1.2m区域内OoD检测AuROC达67.34%，AuPRCr达29.21%，同时保持竞争力占据预测性能。
- Conclusion: OccOoD框架和数据集填补了OoD检测的空白，为自动驾驶安全提供了新工具。


### [63] [GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding](https://arxiv.org/abs/2506.21188)
*Zijun Lin,Shuting He,Cheston Tan,Bihan Wen*

Main category: cs.CV

TL;DR: 论文提出GroundFlow模块，用于提升3D点云序列定位任务（SG3D）中的时间推理能力，显著提高现有3D视觉定位方法的性能。

- Motivation: 现有3D视觉定位方法未充分利用文本指令中的时间信息，且无法处理代词依赖的上下文，导致在SG3D任务中表现不佳。
- Method: 提出GroundFlow模块，选择性提取短期和长期步骤信息，增强时间推理能力。
- Result: 在SG3D基准测试中，GroundFlow显著提升基线方法性能（+7.5%和+10.2%），超越预训练3D大语言模型。
- Conclusion: GroundFlow为3D视觉定位模型引入时间推理能力，在SG3D任务中实现最先进性能。


### [64] [Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation](https://arxiv.org/abs/2506.21198)
*Yihong Cao,Jiaming Zhang,Xu Zheng,Hao Shi,Kunyu Peng,Hang Liu,Kailun Yang,Hui Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种无需源数据的全景图像分割方法UNLOCK，解决了传统方法依赖源数据的问题，并在实验中表现出色。

- Motivation: 全景图像处理面临失真、遮挡和标注不足等问题，传统方法依赖源数据，限制了实用性。
- Method: 提出UNLOCK框架，包含Omni Pseudo-Labeling Learning和Amodal-Driven Context Learning模块，无需源数据或目标标签。
- Result: 在实验中，UNLOCK性能接近依赖源数据的方法，mAAP和mAP得分分别为10.9和11.6，mAPQ提升+4.3。
- Conclusion: UNLOCK为全景图像分割提供了一种更实用的解决方案，性能优异且开源。


### [65] [MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image Segmentation and Classification](https://arxiv.org/abs/2506.21199)
*Shadman Sobhan,Kazi Abrar Mahmud,Abduz Zami*

Main category: cs.CV

TL;DR: MedPrompt是一个统一的医学图像分析框架，结合了大型语言模型（LLM）和模块化卷积神经网络（CNN），支持用户自定义任务流程，无需重新训练整个系统。

- Motivation: 解决现有医学图像分析系统任务单一、灵活性不足的问题。
- Method: 使用LLM（Llama-4-17B）进行任务规划和动态路由任务特定的预训练权重，结合模块化CNN（DeepFusionLab）进行图像处理。
- Result: 在19个公共数据集上测试，覆盖12个任务和5种成像模式，系统端到端正确率为97%，推理延迟2.5秒，分割和分类性能优异。
- Conclusion: MedPrompt通过结合LLM的解释性和模块化CNN的效率，实现了可扩展的、基于提示的医学图像分析。


### [66] [BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models](https://arxiv.org/abs/2506.21209)
*Louis Kerner,Michel Meintz,Bihe Zhao,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: BitMark是一种针对Infinity文本到图像模型的比特级水印框架，旨在防止模型崩溃，通过嵌入不可察觉但可检测的信号来识别生成内容。

- Motivation: 随着生成模型输出的广泛传播，其可能被重新用作训练数据，导致模型性能逐渐退化（模型崩溃）。水印技术被视为缓解这一问题的有效策略。
- Method: BitMark在Infinity图像生成过程中，直接在比特级别嵌入多尺度水印，保持视觉保真度和生成速度，同时对多种去除技术具有鲁棒性。
- Result: BitMark不仅能在生成图像中嵌入水印，还能在后续用这些图像训练的模型中传递水印（放射性），即使经过微调也能检测到。
- Conclusion: BitMark为预防图像生成模型的崩溃提供了可靠的方法，通过水印技术确保生成内容的可检测性。


### [67] [ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation](https://arxiv.org/abs/2506.21233)
*Xiwei Xuan,Ziquan Deng,Kwan-Liu Ma*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的开集词汇语义分割（OVS）方法，通过优化数据质量显著提升性能。

- Motivation: 现有方法依赖预训练模型或合成数据，性能受限于模型能力或数据质量。本文关注数据质量问题，发现高质量参考集对OVS至关重要。
- Method: 提出一个数据质量导向的框架，包括构建高质量参考集的数据流程和基于相似性的检索方法。
- Result: 在十个基准数据集上的评估表明，该方法优于所有现有无需训练的OVS方法。
- Conclusion: 数据为中心的设计对无需训练的OVS任务至关重要，该方法为未来研究提供了新方向。


### [68] [Real-Time ESFP: Estimating, Smoothing, Filtering, and Pose-Mapping](https://arxiv.org/abs/2506.21234)
*Qifei Cui,Yuang Zhou,Ruichen Deng*

Main category: cs.CV

TL;DR: ESFP是一个端到端流水线，将单目RGB视频转换为低成本4自由度桌面机械臂的可执行关节轨迹。

- Motivation: 解决从单目视频中提取并优化机械臂关节轨迹的问题，以适应低成本机械臂的需求。
- Method: 包含四个模块：1) 通过ROMP估计3D骨架；2) 使用HPSTM Transformer平滑轨迹；3) 根据不确定性过滤噪声；4) 几何重定向到机械臂工作空间。
- Result: 实现了从视频到机械臂轨迹的高效转换，并优化了轨迹的平滑性和准确性。
- Conclusion: ESFP提供了一种高效且低成本的方法，将视频动作转化为机械臂的可执行轨迹。


### [69] [DiMPLe -- Disentangled Multi-Modal Prompt Learning: Enhancing Out-Of-Distribution Alignment with Invariant and Spurious Feature Separation](https://arxiv.org/abs/2506.21237)
*Umaima Rahman,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: DiMPLe通过解耦多模态学习中的不变和虚假特征，提升分布外性能，优于现有方法。

- Motivation: 解决视觉数据中虚假相关性对分布外性能的影响。
- Method: 结合互信息最小化、虚假特征正则化和不变特征的对比学习。
- Result: 在11个数据集上表现优于CoOp-OOD，基础类和新类准确率分别提升15.27和44.31。
- Conclusion: DiMPLe在多模态学习中有效提升泛化能力和鲁棒性。


### [70] [Temporal Rate Reduction Clustering for Human Motion Segmentation](https://arxiv.org/abs/2506.21249)
*Xianghan Meng,Zhengyu Tong,Zhiyuan Huang,Chun-Guang Li*

Main category: cs.CV

TL;DR: 提出了一种名为Temporal Rate Reduction Clustering（TR²C）的新方法，用于人类运动分割（HMS），通过联合学习结构化表示和亲和力来分割视频帧序列，并在多个基准数据集上取得了最先进的性能。

- Motivation: 现有HMS方法主要基于子空间聚类，假设高维时间数据符合子空间联合分布，但复杂运动视频帧可能不符合此假设。
- Method: 提出TR²C方法，联合学习结构化表示和亲和力，确保表示具有时间一致性并符合子空间联合结构。
- Result: 在五个基准HMS数据集上进行了广泛实验，使用不同特征提取器均取得了最先进的性能。
- Conclusion: TR²C方法通过优化表示和亲和力学习，显著提升了HMS任务的性能。


### [71] [DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic](https://arxiv.org/abs/2506.21260)
*Munish Monga,Vishal Chudasama,Pankaj Wasnik,Biplab Banerjee*

Main category: cs.CV

TL;DR: 论文提出了一种名为DuIOD的双增量目标检测方法，通过DuET框架解决了类别和域增量学习的问题，显著提升了性能。

- Motivation: 现实中的目标检测系统需要同时处理新类别学习和环境变化适应，现有方法（CIOD和DIOD）仅解决单一方面问题，限制了实际应用。
- Method: 提出DuET框架，基于任务算术模型合并，引入方向一致性损失以减少符号冲突，支持多种检测器。
- Result: 在Pascal Series和Diverse Weather Series上，DuET分别提升了13.12%和11.39%的RAI，同时保持高Avg RI。
- Conclusion: DuIOD和DuET框架有效解决了类别和域增量学习的双重挑战，具有实际应用潜力。


### [72] [Video Virtual Try-on with Conditional Diffusion Transformer Inpainter](https://arxiv.org/abs/2506.21270)
*Cheng Zou,Senlin Cheng,Bolei Xu,Dandan Zheng,Xiaobo Li,Jingdong Chen,Ming Yang*

Main category: cs.CV

TL;DR: ViTI提出了一种基于条件视频修复的视频虚拟试穿方法，通过3D时空注意力扩散变换器实现更好的时空一致性。

- Motivation: 解决现有视频试穿方法在时空一致性和细节保留上的不足。
- Method: 将视频试穿建模为条件视频修复任务，采用3D时空注意力扩散变换器，结合多阶段训练和掩码策略。
- Result: ViTI在定量和定性实验上优于现有方法。
- Conclusion: ViTI通过视频生成框架显著提升了视频试穿的时空一致性和细节保留能力。


### [73] [WordCon: Word-level Typography Control in Scene Text Rendering](https://arxiv.org/abs/2506.21276)
*Wenda Shi,Yiren Song,Zihan Rao,Dengming Zhang,Jiaming Liu,Xingxing Zou*

Main category: cs.CV

TL;DR: 论文提出了一种名为TIA的框架和WordCon方法，用于在生成图像中实现精确的单词级排版控制，通过跨模态对齐和混合参数高效微调提升效果。

- Motivation: 解决生成图像中单词级排版控制的挑战。
- Method: 构建单词级控制数据集，提出TIA框架和WordCon方法，利用跨模态对齐和参数高效微调。
- Result: 定性和定量结果均显示方法优于现有技术。
- Conclusion: 该方法在艺术文本渲染、文本编辑等任务中表现优异，数据集和代码将公开。


### [74] [HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context](https://arxiv.org/abs/2506.21277)
*Qize Yang,Shimin Yao,Weixuan Chen,Shenghao Fu,Detao Bai,Jiaxing Zhao,Boyuan Sun,Bowen Yin,Xihan Wei,Jingren Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种通过强化学习增强多模态大语言模型推理能力的方法，解决了全局上下文理解不足和捷径问题，并引入了IntentBench基准。

- Motivation: 多模态大语言模型在理解人类意图时存在全局上下文理解不足和捷径问题，需要改进推理能力。
- Method: 采用强化学习，结合上下文、格式、准确性和逻辑奖励，提升模型对多模态信息的理解和推理能力。
- Result: 提出的方法在多个全模态基准测试中表现优于其他开源模型。
- Conclusion: 通过强化学习优化多模态推理能力，显著提升了模型性能，为复杂意图理解提供了有效解决方案。


### [75] [HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation](https://arxiv.org/abs/2506.21287)
*Diego Biagini,Nassir Navab,Azade Farshad*

Main category: cs.CV

TL;DR: HieraSurg是一个层次感知的手术视频生成框架，通过两阶段扩散模型解决现有方法在手术视频合成中缺乏一致性和细粒度指导的问题。

- Motivation: 现有手术视频生成方法多为无条件生成，缺乏对手术动作和阶段的语义一致性，无法实现事实模拟。
- Method: HieraSurg采用两阶段模型：第一阶段预测粗粒度语义变化，第二阶段结合细粒度视觉特征生成最终视频。
- Result: 在胆囊切除术视频生成任务中，HieraSurg在定量和定性上均优于现有方法，并能生成更高帧率的视频。
- Conclusion: HieraSurg通过多层次手术信息整合，展示了在手术应用中的潜力，特别是在提供现有分割图时表现优异。


### [76] [Continual Self-Supervised Learning with Masked Autoencoders in Remote Sensing](https://arxiv.org/abs/2506.21312)
*Lars Möllenbrok,Behnood Rasti,Begüm Demir*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoSMAE的持续自监督学习方法，通过数据混合和模型混合知识蒸馏来减少灾难性遗忘，并在实验中优于现有方法。

- Motivation: 解决遥感领域持续学习中需要大量标注数据的问题，提出一种无需大量标注的自监督学习方法。
- Method: 结合数据混合（保留先前任务信息）和模型混合知识蒸馏（同时利用过去和当前模型知识），通过掩码自编码器实现。
- Result: CoSMAE在实验中比现有持续学习方法性能提升高达4.94%。
- Conclusion: CoSMAE通过数据与模型层面的混合策略，有效减少了灾难性遗忘，提升了持续学习性能。


### [77] [DrishtiKon: Multi-Granular Visual Grounding for Text-Rich Document Images](https://arxiv.org/abs/2506.21316)
*Badri Vishal Kasuba,Parag Chaudhuri,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 提出了一种多粒度视觉定位框架Drishtikon，用于提升文档图像中视觉问答系统的可解释性和准确性。

- Motivation: 解决文本丰富的文档图像中视觉定位的挑战，提升文档智能和视觉问答系统的信任度。
- Method: 结合多语言OCR、大语言模型和新型区域匹配算法，实现块、行、词和点级别的精确定位。
- Result: 在多个粒度上实现了最先进的定位准确率，行级粒度在精度和召回率之间取得了最佳平衡。
- Conclusion: 该方法为现实世界中文本密集场景下的文档理解系统提供了更鲁棒和可解释的解决方案。


### [78] [LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning](https://arxiv.org/abs/2506.21317)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: 论文提出了一种通过整合人体关键点与传统视觉特征（如标题和边界框）生成专用视觉-语言指令数据的方法，以提升视觉-语言模型在人体姿态和动作任务中的表现。

- Motivation: 现有视觉-语言模型在复杂人体姿态和动作任务中表现不佳，主要缺乏专用数据。
- Method: 通过整合人体关键点与传统视觉特征生成专用数据集（200,328样本），并微调LLaVA-1.5-7B模型。
- Result: 微调后的LLaVA-Pose模型在E-HPAUB基准上表现提升33.2%。
- Conclusion: 整合关键点数据显著提升了多模态模型在人体姿态和动作理解任务中的性能。


### [79] [Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models](https://arxiv.org/abs/2506.21330)
*Haoyang Wu,Tsun-Hsuan Wang,Mathias Lechner,Ramin Hasani,Jennifer A. Eckhoff,Paul Pak,Ozanan R. Meireles,Guy Rosman,Yutong Ban,Daniela Rus*

Main category: cs.CV

TL;DR: 提出了一种基于层次化状态空间模型的新方法，用于高效分析长时间的手术视频，显著优于现有技术。

- Motivation: 手术视频分析因时长问题难以高效处理，现有Transformer模型因二次注意力机制受限。
- Method: 采用层次化状态空间模型，结合局部和全局动态捕捉，使用混合离散-连续监督策略训练。
- Result: 在多个数据集上表现优异（Cholec80 +2.8%，MICCAI2016 +4.3%，Heichole +12.9%）。
- Conclusion: 该方法在手术视频分析中实现了高效和准确性，代码将公开。


### [80] [PanSt3R: Multi-view Consistent Panoptic Segmentation](https://arxiv.org/abs/2506.21348)
*Lojze Zust,Yohann Cabon,Juliette Marrie,Leonid Antsfeld,Boris Chidlovskii,Jerome Revaud,Gabriela Csurka*

Main category: cs.CV

TL;DR: PanSt3R提出了一种联合预测3D几何和多视角全景分割的方法，避免了现有方法依赖2D分割和测试时优化的缺点，实现了快速且高性能的3D场景分割。

- Motivation: 现有方法依赖2D全景分割和测试时优化，无法充分利用多视角空间关系且计算成本高。
- Method: 基于MUSt3R，PanSt3R联合预测3D几何和多视角全景分割，并改进了掩码合并方法和新视角预测。
- Result: PanSt3R在多个基准测试中达到SOTA性能，且速度显著快于现有方法。
- Conclusion: PanSt3R是一种简单、快速且可扩展的方法，为3D场景全景分割提供了高效解决方案。


### [81] [Generalizable Neural Electromagnetic Inverse Scattering](https://arxiv.org/abs/2506.21349)
*Yizhe Cheng,Chunxun Tian,Haoru Wang,Wentao Zhu,Xiaoxuan Ma,Yizhou Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于物理驱动的通用框架，用于解决电磁逆散射问题（EISP），通过两阶段逆传输-散射过程，显著提高了重建精度、泛化能力和鲁棒性。

- Motivation: 电磁逆散射问题在医学成像等应用中至关重要，但传统方法（如Img-Interiors）存在泛化能力差、对稀疏发射器设置不鲁棒等问题。
- Method: 提出了一种两阶段框架，包括电流估计器和介电常数求解器，通过显式学习诱导电流作为中间表示，实现端到端训练和预测。
- Result: 实验表明，该方法在重建精度、泛化能力和鲁棒性上优于现有方法。
- Conclusion: 该研究为电磁逆散射问题提供了新的视角，为低成本电磁成像解决方案迈出了重要一步。


### [82] [ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models](https://arxiv.org/abs/2506.21356)
*Hongbo Liu,Jingwen He,Yi Jin,Dian Zheng,Yuhao Dong,Fan Zhang,Ziqi Huang,Yinan He,Yangguang Li,Weichao Chen,Yu Qiao,Wanli Ouyang,Shengjie Zhao,Ziwei Liu*

Main category: cs.CV

TL;DR: ShotBench是一个专门用于评估电影语言理解的基准测试，揭示了现有视觉语言模型在理解电影语法方面的不足，并提出了ShotQA数据集和ShotVL模型以推动该领域的发展。

- Motivation: 电影摄影是电影的核心视觉语言，但现有视觉语言模型在理解电影语法方面表现不足，限制了AI在视频生成中的精确性。
- Method: 提出了ShotBench基准测试和ShotQA数据集，并通过监督微调和Group Relative Policy Optimization开发了ShotVL模型。
- Result: 现有模型在ShotBench上表现不佳（最高准确率低于60%），而ShotVL显著优于所有现有模型。
- Conclusion: ShotBench和ShotQA为AI驱动的电影理解和生成提供了重要工具，ShotVL成为该领域的新标杆。


### [83] [CoPa-SG: Dense Scene Graphs with Parametric and Proto-Relations](https://arxiv.org/abs/2506.21357)
*Julian Lorenz,Mrunmai Phatak,Robin Schön,Katja Ludwig,Nico Hörmann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CV

TL;DR: CoPa-SG是一个高精度合成场景图数据集，引入参数化和原型关系，提升场景图生成模型的性能。

- Motivation: 解决现有场景图数据不足的问题，提供更精细的关系表示。
- Method: 提出CoPa-SG数据集，包含参数化关系和原型关系，用于模型比较和下游应用。
- Result: 展示了新关系类型如何增强规划和推理能力。
- Conclusion: CoPa-SG为场景理解提供了更丰富的数据和关系表示，推动了相关应用的发展。


### [84] [ToosiCubix: Monocular 3D Cuboid Labeling via Vehicle Part Annotations](https://arxiv.org/abs/2506.21358)
*Behrooz Nasihatkon,Hossein Resani,Amirreza Mehrzadian*

Main category: cs.CV

TL;DR: ToosiCubix提出了一种仅需单目图像和相机内参的3D立方体标注方法，通过用户点击和几何优化实现高效标注。

- Motivation: 现有方法依赖昂贵设备，限制了大规模数据标注的可行性，ToosiCubix旨在提供低成本、易操作的解决方案。
- Method: 通过用户点击车辆特征点，结合PnP和最小二乘优化，利用概率尺寸先验解决尺度模糊问题。
- Result: 在KITTI和Cityscapes3D数据集上验证，标注效果与专业设备相当。
- Conclusion: ToosiCubix为3D标注提供了一种经济高效且可扩展的方法。


### [85] [CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection](https://arxiv.org/abs/2506.21364)
*Zhixin Cheng,Jiacheng Deng,Xinjun Li,Xiaotian Yin,Bohao Liao,Baoqun Yin,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 提出了一种改进图像到点云配准的方法，通过通道自适应调整模块（CAA）和全局最优选择模块（GOS）解决特征通道注意力差异和冗余对应问题。

- Motivation: 现有检测自由方法在图像和点云特征匹配中，因特征通道注意力差异和场景结构相似性导致匹配结果退化。
- Method: 引入CAA模块增强模态内特征并抑制跨模态敏感性，GOS模块用全局优化替代局部选择。
- Result: 在RGB-D Scenes V2和7-Scenes数据集上实现最先进的配准性能。
- Conclusion: CAA和GOS模块有效提升了图像到点云的配准精度。


### [86] [GenFlow: Interactive Modular System for Image Generation](https://arxiv.org/abs/2506.21369)
*Duc-Hung Nguyen,Huu-Phuc Huynh,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: GenFlow是一个模块化框架，旨在降低生成艺术的复杂性，通过节点编辑器和自然语言处理助手，使不同技能水平的用户都能轻松生成图像。

- Motivation: 生成艺术需要高级技术知识，限制了其广泛应用。GenFlow旨在通过简化工作流程和降低技术门槛，让更多人能够使用生成艺术工具。
- Method: GenFlow采用模块化设计，提供节点编辑器和工作流自动化功能，结合自然语言处理助手，实现直观的用户体验。
- Result: 用户研究表明，GenFlow能优化工作流程，减少任务完成时间，并通过直观界面提升用户理解。
- Conclusion: GenFlow通过提升可访问性和效率，重新定义了生成艺术的工具标准。


### [87] [FastRef:Fast Prototype Refinement for Few-Shot Industrial Anomaly Detection](https://arxiv.org/abs/2506.21398)
*Long Tian,Yufei Li,Yuyang Dai,Wenchao Chen,Xiyang Liu,Bo Chen*

Main category: cs.CV

TL;DR: FastRef是一种高效的少样本工业异常检测（FS-IAD）原型优化框架，通过特征转移和异常抑制提升原型代表性。

- Motivation: 现有方法在数据稀缺环境下未能充分利用查询图像统计信息，导致原型代表性不足。
- Method: FastRef采用两阶段迭代过程：特征转移（通过可优化矩阵）和异常抑制（通过原型对齐和最优传输）。
- Result: 在多个基准数据集上，FastRef与现有方法结合表现出高效性和有效性。
- Conclusion: FastRef显著提升了少样本工业异常检测的性能和效率。


### [88] [Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction](https://arxiv.org/abs/2506.21401)
*Zhirui Gao. Renjiao Yi,Yaqiao Dai,Xuening Zhu,Wei Chen,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: 提出了一种端到端框架，直接从多视角边缘图重建3D参数曲线，避免了传统两阶段方法的误差累积。

- Motivation: 现有两阶段方法（先重建边缘点云再拟合参数曲线）存在优化间隙导致的误差累积问题，需要一种更直接的方法。
- Method: 提出了一种基于参数曲线与边缘导向高斯组件的双向耦合机制（CurveGaussian），支持可微渲染，并通过动态自适应拓扑优化框架优化曲线结构。
- Result: 在ABC数据集和真实场景测试中表现优于两阶段方法，重建更干净、鲁棒，且训练参数更少，效率更高。
- Conclusion: 直接优化参数曲线的一阶段方法显著提升了重建质量和效率，为3D曲线重建提供了新思路。


### [89] [XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation](https://arxiv.org/abs/2506.21416)
*Bowen Chen,Mengyi Zhao,Haomiao Sun,Li Chen,Xu Wang,Kang Du,Xinglong Wu*

Main category: cs.CV

TL;DR: XVerse模型通过将参考图像转换为特定令牌的偏移量，实现了对多主题图像生成的精确控制，避免了属性纠缠和编辑性问题。

- Motivation: 解决文本到图像生成中多主题控制的编辑性和一致性不足问题，以及避免属性纠缠和伪影。
- Method: 将参考图像转换为偏移量，用于特定令牌的文本流调制，实现对单个主题的独立控制。
- Result: XVerse实现了高保真、可编辑的多主题图像合成，并能精确控制每个主题的特征和语义属性。
- Conclusion: XVerse显著提升了复杂场景和个性化图像生成的能力。


### [90] [EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting](https://arxiv.org/abs/2506.21420)
*Taoyu Wu,Yiyi Miao,Zhuoxiao Li,Haocheng Zhao,Kang Dang,Jionglong Su,Limin Yu,Haoang Li*

Main category: cs.CV

TL;DR: 该论文提出了一种结合光流损失和深度正则化的3D高斯溅射（3DGS）SLAM方法，用于解决内窥镜场景中的光度不一致性和动态运动问题，并在静态和动态手术场景中表现出色。

- Motivation: 内窥镜手术场景中存在光度不一致性和动态运动（如呼吸）的挑战，传统基于外观约束的3DGS SLAM方法性能受限。
- Method: 引入光流损失作为几何约束，提出深度正则化策略，并改进3DGS细化策略以优化关键帧的渲染质量。
- Result: 在C3VD静态数据集和StereoMIS动态数据集上，新方法在视图合成和姿态估计方面优于现有方法。
- Conclusion: 结合几何约束和深度正则化的3DGS SLAM方法显著提升了内窥镜场景中的重建和渲染性能。


### [91] [HyperSORT: Self-Organising Robust Training with hyper-networks](https://arxiv.org/abs/2506.21430)
*Samuel Joutard,Marijn Stollenga,Marc Balle Sanchez,Mohammad Farid Azampour,Raphael Prevost*

Main category: cs.CV

TL;DR: HyperSORT是一个框架，通过超网络预测UNet参数，以处理医学影像数据中的异质性偏差，并识别系统性偏差和错误样本。

- Motivation: 医学影像数据常包含异质性偏差（如错误标签或不一致的标注风格），这些偏差会降低深度分割网络的性能。识别和表征这些偏差是一项繁琐且具有挑战性的任务。
- Method: HyperSORT使用超网络从潜在向量预测UNet参数，这些向量表示图像和标注的变异性。超网络参数和潜在向量集合通过联合学习优化。
- Result: 在3D腹部CT数据集上的实验表明，HyperSORT能够结构化映射数据集，识别系统性偏差和错误样本，并生成符合潜在偏差的分割UNet参数。
- Conclusion: HyperSORT有效解决了医学影像数据中的偏差问题，为数据集分析和分割任务提供了新方法。


### [92] [Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation](https://arxiv.org/abs/2506.21444)
*Sweta Banerjee,Viktoria Weiss,Taryn A. Donovan,Rutger A. Fick,Thomas Conrad,Jonas Ammeling,Nils Porsche,Robert Klopfleisch,Christopher Kaltenecker,Katharina Breininger,Marc Aubreville,Christof A. Bertram*

Main category: cs.CV

TL;DR: 该研究通过深度学习模型（包括基线模型、基础模型和LoRA微调模型）对乳腺癌中的非典型有丝分裂（AMF）进行分类，并在多个数据集上验证了其有效性。

- Motivation: 非典型有丝分裂是肿瘤恶性程度的独立预后标志物，但其识别存在挑战，如低发生率、形态学差异小、病理学家间一致性低以及数据集类别不平衡。
- Method: 研究使用了AMi-Br数据集，并引入了两个新的数据集AtNorM-Br和AtNorM-MD，比较了多种深度学习方法，包括LoRA微调的基础模型。
- Result: 在AMi-Br、AtNorM-Br和AtNorM-MD数据集上，模型的平衡准确率分别达到0.8135、0.7696和0.7705，其中LoRA微调的Virchow基础模型表现最佳。
- Conclusion: 研究表明，尽管非典型有丝分裂分类具有挑战性，但通过迁移学习和模型微调技术可以有效解决。所有代码和数据已开源。


### [93] [Controllable 3D Placement of Objects with Scene-Aware Diffusion Models](https://arxiv.org/abs/2506.21446)
*Mohamed Omran,Dimitris Kalatzis,Jens Petersen,Amirhossein Habibian,Auke Wiggers*

Main category: cs.CV

TL;DR: 提出了一种基于视觉地图和粗略对象掩码的高质量对象放置方法，解决了精确位置和方向编辑的挑战。

- Motivation: 现有方法在精确放置对象时依赖复杂的掩码或提示，缺乏灵活性。
- Method: 设计了一种新的条件信号，结合视觉地图和粗略掩码，保留背景完整性。
- Result: 在汽车场景中验证了方法的有效性，支持形状和方向变化，并实现精确位置控制。
- Conclusion: 该方法在对象放置任务中表现出色，兼具灵活性和精确性。


### [94] [A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario](https://arxiv.org/abs/2506.21451)
*Cyrus Addy,Ajay Kumar Gurumadaiah,Yixiang Gao,Kwame Awuah-Offei*

Main category: cs.CV

TL;DR: 本文提出了一种专门用于地下采矿环境的热成像数据集，以支持矿工检测系统的开发和验证，并评估了多种先进的目标检测算法。

- Motivation: 地下采矿作业面临重大安全挑战，机器人辅助搜救需要可靠的矿工检测能力，但目前缺乏针对此类环境的数据集。
- Method: 通过系统采集各种采矿活动和场景的热成像数据，构建数据集，并评估了YOLOv8、YOLOv10、YOLO11和RT-DETR等算法的性能。
- Result: 研究证明了热成像用于矿工检测的可行性，并为未来研究奠定了基础。
- Conclusion: 该数据集是开发可靠的矿工检测系统的重要第一步，未来可应用于实际紧急情况。


### [95] [Rethinking Oversaturation in Classifier-Free Guidance via Low Frequency](https://arxiv.org/abs/2506.21452)
*Kaiyu Song,Hanjiang Lai*

Main category: cs.CV

TL;DR: 论文提出了一种基于低频信号改进的无分类器引导（LF-CFG）方法，通过自适应阈值减少冗余信息，缓解过饱和和不真实伪影问题。

- Motivation: 高引导尺度在增强条件扩散模型性能时会导致过饱和和不真实伪影，低频信号中的冗余信息是主要原因。
- Method: 引入自适应阈值测量冗余信息位置，分析低频信息变化率确定合理阈值，并采用降权策略减少冗余信息影响。
- Result: 实验表明，LF-CFG在多种扩散模型（如Stable Diffusion-XL等）中有效缓解了过饱和和不真实伪影。
- Conclusion: LF-CFG通过低频信号改进，显著提升了条件扩散模型的生成质量。


### [96] [Evaluation of Traffic Signals for Daily Traffic Pattern](https://arxiv.org/abs/2506.21469)
*Mohammad Shokrolah Shirazi,Hung-Fu Chang*

Main category: cs.CV

TL;DR: 该论文提出了三种基于转向运动计数（TMC）的交通信号配置方法（动态、静态和混合），并通过仿真实验验证了其在不同交通条件下的性能。

- Motivation: 转向运动计数数据对交通信号设计、交叉口规划和拥堵分析至关重要，但现有方法可能无法适应多样化的交通模式。
- Method: 开发了基于视觉的跟踪系统估计TMC，结合仿真工具SUMO评估信号配置性能，并提出了动态、静态和混合信号方法。
- Result: 实验表明，90秒和120秒的信号周期效果最佳，动态配置在四个交叉口表现更好，混合方法适用于高峰和非高峰交通。
- Conclusion: 混合信号方法能适应多样化的交通模式，尤其适用于交通流量分布不均的交叉口。


### [97] [Logios : An open source Greek Polytonic Optical Character Recognition system](https://arxiv.org/abs/2506.21474)
*Perifanos Konstantinos,Goutsos Dionisis*

Main category: cs.CV

TL;DR: 提出了一种针对希腊多调文本的OCR系统，结合卷积层和循环层，显著提升了识别准确率和效率。

- Motivation: 解决传统OCR方法在处理希腊多调文本时的局限性。
- Method: 结合卷积层进行特征提取和循环层进行序列学习。
- Result: 显著提高了希腊多调文本的识别准确率和效率。
- Conclusion: 该系统作为开源库发布，供学术使用。


### [98] [Global and Local Entailment Learning for Natural World Imagery](https://arxiv.org/abs/2506.21476)
*Srikumar Sastry,Aayush Dhakal,Eric Xing,Subash Khanal,Nathan Jacobs*

Main category: cs.CV

TL;DR: 提出了一种名为RCME的框架，通过显式建模传递性强制蕴含，优化视觉-语言模型中概念的偏序关系，提升了层次分类和检索任务的性能。

- Motivation: 现有方法未能显式建模蕴含的传递性，限制了视觉-语言模型中数据层次结构的学习。
- Method: 引入Radial Cross-Modal Embeddings (RCME)框架，显式建模传递性强制蕴含，优化概念的偏序关系。
- Result: 在层次物种分类和层次检索任务中，RCME框架显著优于现有方法。
- Conclusion: RCME框架成功提升了视觉-语言模型中层次结构的表示能力，为相关任务提供了更优的解决方案。


### [99] [TITAN: Query-Token based Domain Adaptive Adversarial Learning](https://arxiv.org/abs/2506.21484)
*Tajamul Ashraf,Janibul Bashir*

Main category: cs.CV

TL;DR: 论文提出了一种名为TITAN的方法，用于解决源数据不可用的无源域自适应目标检测问题，通过分离目标图像为易/难子集并结合查询-令牌对抗模块，显著提升了性能。

- Motivation: 解决源数据不可用时，现有自监督方法因伪标签噪声导致的性能下降问题。
- Method: 提出TITAN方法，将目标图像分为易/难子集，并引入查询-令牌对抗模块以减少域间差异。
- Result: 在多个数据集上验证，TITAN性能显著优于现有方法，mAP提升最高达22.7%。
- Conclusion: TITAN通过优化伪标签生成和域间对齐，有效提升了无源域自适应目标检测的性能。


### [100] [Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection](https://arxiv.org/abs/2506.21486)
*Tobias J. Riedlinger,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 该论文提出了一种基于空间统计学的目标检测模型，解决了现有深度神经网络在目标检测和语义分割中置信度估计不准确的问题，特别是在未检测到物体的区域。

- Motivation: 现有目标检测模型的置信度估计不准确，且无法量化未检测区域的障碍物不确定性，这在自动驾驶等应用中存在安全隐患。
- Method: 提出了一种基于标记点过程的空间统计学框架，将边界框数据建模为空间点事件的实现，并通过似然训练提供明确的置信度估计。
- Result: 通过校准评估和性能测试，证明了该方法的有效性。
- Conclusion: 该统计框架为区域是否可行驶（无障碍物）提供了明确的置信度估计，提升了目标检测的可靠性。


### [101] [Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration](https://arxiv.org/abs/2506.21509)
*Jiahe Chen,Jiaying He,Qian Shao,Qiyuan Chen,Jiahe Ying,Hongxia Xu,Jintai Chen,Jianwei Zheng,Jian Wu*

Main category: cs.CV

TL;DR: 论文提出了一种名为动态对数校准（DLC）的新解码框架，旨在动态对齐文本生成与视觉证据，减少大视觉语言模型（LVLM）中的幻觉问题。

- Motivation: 大视觉语言模型在多模态理解方面取得显著进展，但常因生成与视觉输入矛盾的文本（幻觉）而受限。现有解码策略存在静态约束、效率低下和细节损失等问题。
- Method: DLC在解码阶段逐步使用CLIP评估图像与生成文本的语义对齐，并通过相对视觉优势（RVA）动态调整输出对数，偏好视觉基础标记。自适应加权机制平衡视觉引导与文本质量。
- Result: 实验表明，DLC显著减少幻觉，优于现有方法，同时保持高效推理。
- Conclusion: DLC是一种高效解码时解决方案，提升LVLM的可靠性。


### [102] [GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation](https://arxiv.org/abs/2506.21513)
*Wentao Hu,Shunkai Li,Ziqiao Peng,Haoxian Zhang,Fan Shi,Xiaoqiang Liu,Pengfei Wan,Di Zhang,Hui Tian*

Main category: cs.CV

TL;DR: GGTalker提出了一种结合通用先验和身份特定适应的两阶段训练策略，用于生成高质量、通用的语音驱动3D说话头部。

- Motivation: 现有方法在固定视角和小规模音频变化下表现良好，但在大角度头部旋转和分布外音频下表现不佳，且需要耗时的身份特定训练。
- Method: 采用两阶段Prior-Adaptation训练策略，学习高斯头部先验并适应个体特征，包括音频-表情和表情-视觉先验，以及颜色MLP和身体修复器。
- Result: GGTalker在渲染质量、3D一致性、唇同步准确性和训练效率方面达到最先进水平。
- Conclusion: GGTalker通过通用先验和身份适应，显著提升了语音驱动3D说话头部的生成质量和泛化能力。


### [103] [G$^{2}$D: Boosting Multimodal Learning with Gradient-Guided Distillation](https://arxiv.org/abs/2506.21514)
*Mohammed Rakib,Arunkumar Bagavathi*

Main category: cs.CV

TL;DR: 提出了一种名为Gradient-Guided Distillation (G²D)的知识蒸馏框架，通过动态模态优先级技术解决多模态学习中的模态不平衡问题。

- Motivation: 传统多模态模型存在模态不平衡问题，导致弱模态特征表示不足。
- Method: G²D结合单模态和多模态目标的自定义损失函数，并引入动态顺序模态优先级（SMP）技术。
- Result: 在多个真实数据集上验证，G²D显著提升弱模态的重要性，并在分类和回归任务中优于现有方法。
- Conclusion: G²D有效解决了模态不平衡问题，提升了多模态学习的性能。


### [104] [MADrive: Memory-Augmented Driving Scene Modeling](https://arxiv.org/abs/2506.21520)
*Polina Karpikova,Daniil Selikhanovych,Kirill Struminsky,Ruslan Musaev,Maria Golitsyna,Dmitry Baranchuk*

Main category: cs.CV

TL;DR: MADrive是一个记忆增强的重建框架，通过替换观测车辆为外部记忆库中的3D资产，扩展了现有场景重建方法的能力。

- Motivation: 现有自动驾驶环境的重建方法难以支持显著改变或新颖驾驶场景的逼真合成。
- Method: MADrive使用MAD-Cars数据集和检索模块，从记忆库中找到相似车辆实例，重建3D资产并集成到目标场景中。
- Result: 实验表明，该方法能够提供完整的多视角车辆表示，支持显著改变配置的逼真合成。
- Conclusion: MADrive通过记忆增强技术，显著提升了场景重建的灵活性和逼真度。


### [105] [WAFT: Warping-Alone Field Transforms for Optical Flow](https://arxiv.org/abs/2506.21526)
*Yihan Wang,Jia Deng*

Main category: cs.CV

TL;DR: WAFT是一种简单高效的基于高分辨率变形的光流方法，替代了传统的成本体积构建，性能更好且内存成本更低。

- Motivation: 挑战传统观念，即构建成本体积是实现高性能的必要条件，提出一种更简单灵活的方法。
- Method: 使用高分辨率变形替代成本体积，设计简单且依赖最小归纳偏置和定制设计。
- Result: 在Spring和KITTI基准测试中排名第一，零样本泛化性能最佳，速度提升4.1倍。
- Conclusion: WAFT证明了无需成本体积也能实现高性能光流，为未来研究提供了新思路。


### [106] [Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval](https://arxiv.org/abs/2506.21538)
*Hani Alomari,Anushka Sivakumar,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 提出了一种新的跨模态图像-文本检索方法，通过优化嵌入集的一对一匹配和引入两种损失函数，解决了稀疏监督和集合崩溃问题，并在MS-COCO和Flickr30k上实现了最优性能。

- Motivation: 传统单向量嵌入方法难以捕捉跨模态的多样关联，而基于集合的方法虽能捕获更丰富的关系，但仍面临稀疏监督和集合崩溃的挑战。
- Method: 提出Maximal Pair Assignment Similarity优化嵌入集的一对一匹配，并引入Global Discriminative Loss和Intra-Set Divergence Loss增强表示。
- Result: 在MS-COCO和Flickr30k上实现了最优性能，无需依赖外部数据。
- Conclusion: 新方法有效解决了集合表示中的问题，显著提升了跨模态检索性能。


### [107] [StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning](https://arxiv.org/abs/2506.21541)
*Chuxin Wang,Yixin Zha,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: StruMamba3D是一种自监督点云表示学习新方法，通过保留空间依赖性和改进状态更新策略，解决了Mamba方法在点云处理中的局限性。

- Motivation: Mamba方法在点云表示学习中表现优异，但存在破坏点邻接性和长序列记忆丢失问题。
- Method: 设计空间状态代理保留空间依赖，增强SSM的状态更新策略，并引入轻量卷积和序列长度自适应策略。
- Result: 在四个下游任务中表现优异，ModelNet40准确率达95.1%，ScanObjectNN达92.75%。
- Conclusion: StruMamba3D有效解决了Mamba方法的局限性，提升了点云表示学习性能。


### [108] [DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion](https://arxiv.org/abs/2506.21544)
*Yansong Qu,Shaohui Dai,Xinyang Li,Yuze Wang,You Shen,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出了一种端到端框架，用于从部分遮挡的单张图像生成多视角视图，以改进3D重建质量。

- Motivation: 现有基于扩散的视图合成模型在输入完全可见时表现良好，但在遮挡情况下会导致视图不一致和重建质量下降。
- Method: 通过自监督训练管道，利用遮挡-未遮挡图像对和伪真实视图，联合学习完成和多视角生成。
- Result: 方法能够从遮挡图像生成六个结构一致的新视图，无需先验修复或手动标注。
- Conclusion: 提出的框架和基准为遮挡感知的3D重建提供了标准化评估协议。


### [109] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: HalluSegBench是首个通过反事实视觉推理评估视觉基础中幻觉的基准，包含1340个反事实实例对和新的量化指标，揭示了视觉驱动幻觉的普遍性。

- Motivation: 现有分割幻觉评估协议主要关注标签或文本幻觉，缺乏对视觉上下文的操控，无法诊断关键失败。
- Method: 提出HalluSegBench基准，包含新数据集和量化指标，用于评估视觉基础中的幻觉。
- Result: 实验表明，视觉驱动幻觉比标签驱动更普遍，模型常持续错误分割。
- Conclusion: 反事实推理对诊断基础保真度至关重要，HalluSegBench填补了评估空白。


### [110] [SAM4D: Segment Anything in Camera and LiDAR Streams](https://arxiv.org/abs/2506.21547)
*Jianyun Xu,Song Wang,Ziqian Ni,Chunyong Hu,Sheng Yang,Jianke Zhu,Qiang Li*

Main category: cs.CV

TL;DR: SAM4D是一个多模态时序基础模型，用于相机和LiDAR流的可提示分割，通过统一多模态位置编码和运动感知跨模态记忆注意力提升性能，并开发了自动化数据引擎加速标注。

- Motivation: 解决自动驾驶场景中相机和LiDAR数据的分割问题，提升跨模态交互和时序一致性，同时避免人工标注瓶颈。
- Method: 引入统一多模态位置编码（UMPE）对齐特征，提出运动感知跨模态记忆注意力（MCMA）增强时序一致性，开发多模态自动化数据引擎生成伪标签。
- Result: 在Waymo-4DSeg数据集上验证了SAM4D的强大跨模态分割能力和高效数据标注潜力。
- Conclusion: SAM4D在跨模态分割和自动化标注方面表现出色，为自动驾驶场景提供了高效解决方案。


### [111] [SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark](https://arxiv.org/abs/2506.21549)
*Alex Costanzino,Pierluigi Zama Ramirez,Luigi Lella,Matteo Ragaglia,Alessandro Oliva,Giuseppe Lisanti,Luigi Di Stefano*

Main category: cs.CV

TL;DR: SiM3D是首个结合多视角和多模态信息的3D异常检测与分割（ADS）基准，专注于单实例异常检测，并首次解决从合成训练数据泛化到真实测试数据的挑战。

- Motivation: 制造业中单实例异常检测的需求，以及现有方法在多视角和多模态信息整合上的不足。
- Method: 提出SiM3D基准，包含多模态多视角数据集，并采用工业传感器和机器人采集数据。
- Result: 数据集包含333个实例的高分辨率图像和点云数据，并提供手动标注的3D分割GT。
- Conclusion: SiM3D为多视角3D ADS任务提供了首个基准，并评估了单视角方法的性能。


### [112] [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552)
*Yutong Bai,Danny Tran,Amir Bar,Yann LeCun,Trevor Darrell,Jitendra Malik*

Main category: cs.CV

TL;DR: 论文提出了一种通过人体动作预测第一人称视角视频的方法（PEVA），利用3D身体姿态轨迹训练自回归条件扩散变换器，并在大规模数据集Nymeria上进行验证。

- Motivation: 研究旨在探索如何通过人体动作模拟第一人称视角下的环境变化，以解决复杂现实环境和具身行为的建模挑战。
- Method: 采用基于运动学姿态轨迹的自回归条件扩散变换器，结合人体关节层次结构，训练模型预测视频。
- Result: 设计了分层评估协议，全面分析模型的预测和控制能力，验证了方法的有效性。
- Conclusion: 该研究为从人类视角预测视频和建模具身行为提供了初步尝试，展示了在复杂环境中的潜力。
## cs.AI

### [113] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Main category: cs.AI

TL;DR: MindCube基准测试揭示现有视觉语言模型（VLMs）在构建空间心理模型方面的不足，提出“先映射后推理”方法显著提升性能。

- Motivation: 探索VLMs是否能像人类一样从少量视角构建完整场景的空间心理模型，以推理布局、视角和动态。
- Method: 通过MindCube基准测试评估VLMs的空间心理模型能力，并提出三种改进方法，最终采用“先映射后推理”的协同方法。
- Result: “先映射后推理”方法将准确率从37.8%提升至60.8%，结合强化学习后达到70.7%。
- Conclusion: 通过构建和利用结构化空间表征，VLMs对不可观察空间的理解能力显著提升。
## eess.AS

### [114] [ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing](https://arxiv.org/abs/2506.21448)
*Huadai Liu,Jialei Wang,Kaicheng Luo,Wen Wang,Qian Chen,Zhou Zhao,Wei Xue*

Main category: eess.AS

TL;DR: ThinkSound是一个基于Chain-of-Thought推理的框架，用于视频到音频的生成和编辑，通过多阶段分解和交互式优化实现高保真音频生成。

- Motivation: 当前端到端视频到音频生成方法难以捕捉视觉内容的细微差别，需要更复杂的推理能力来模拟专业创作过程。
- Method: ThinkSound将过程分解为三个阶段：基础音效生成、交互式对象中心优化和自然语言指导的编辑，利用多模态大语言模型生成上下文对齐的推理。
- Result: ThinkSound在音频指标和CoT指标上均达到最先进水平，并在Movie Gen Audio基准测试中表现优异。
- Conclusion: ThinkSound通过分阶段推理和交互式优化，显著提升了视频到音频生成的质量和灵活性。
## cs.DL

### [115] [Automatic Reviewers Assignment to a Research Paper Based on Allied References and Publications Weight](https://arxiv.org/abs/2506.21331)
*Tamim Al Mahmud,B M Mainul Hossain,Dilshad Ara*

Main category: cs.DL

TL;DR: 论文提出了一种自动选择最佳审稿人的新策略，通过分析论文的参考文献和作者指标来优化审稿人选择。

- Motivation: 随着研究领域的扩展和论文数量的激增，传统审稿人选择方法效率低下，难以确保审稿质量。
- Method: 通过收集论文参考文献，提取关键词，搜索领域内顶尖研究者，并基于h-index等指标排名，最终筛选出最佳审稿人。
- Result: 实现了自动化选择审稿人的程序，能够高效匹配论文与领域专家。
- Conclusion: 该方法显著提升了审稿人选择的效率和准确性，适用于大规模学术出版需求。
## cs.HC

### [116] [Multimodal LLMs for Visualization Reconstruction and Understanding](https://arxiv.org/abs/2506.21319)
*Can Liu,Chunlin Da,Xiaoxiao Long,Yuxiao Yang,Yu Zhang,Yong Wang*

Main category: cs.HC

TL;DR: 提出了一种针对可视化理解的多模态数据集和模型，通过结合图表图像及其向量化表示，显著提升了数据提取和图表重建的准确性。

- Motivation: 当前多模态大模型在自然图像理解上表现良好，但在可视化理解上存在不足，无法解析数据到视觉的映射规则或提取结构化信息。
- Method: 结合图表图像及其向量化表示、编码方案和数据特征，训练专门的多模态可视化大模型。
- Result: 实验结果显示，数据提取准确性和图表重建质量均有显著提升。
- Conclusion: 提出的方法有效解决了可视化理解中的挑战，为数据通信提供了更高效的工具。
## eess.IV

### [117] [Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG](https://arxiv.org/abs/2506.20683)
*Alexander Selivanov,Philip Müller,Özgün Turgut,Nil Stolt-Ansó,Daniel Rückert*

Main category: eess.IV

TL;DR: PTACL是一种多模态对比学习框架，通过整合CMR的时空信息增强ECG表示，提升心脏功能评估能力。

- Motivation: ECG无法直接测量心脏功能参数，而CMR虽为金标准但昂贵且不易获取，PTACL旨在弥补这一差距。
- Method: PTACL结合全局患者级和局部时间级对比损失，对齐ECG和CMR表示，无需额外可学习参数。
- Result: 在UK Biobank数据上，PTACL在患者检索和心脏功能参数预测任务中优于基线方法。
- Conclusion: PTACL展示了利用ECG增强非侵入性心脏诊断的潜力。


### [118] [U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs](https://arxiv.org/abs/2506.20689)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TL;DR: 论文提出了一种名为U-R-Veda的深度学习模型，结合卷积变换、视觉变换器、残差链接、通道注意力和空间注意力，用于心脏磁共振图像的自动语义分割，显著提高了分割精度。

- Motivation: 自动化准确的心脏图像分割是心脏疾病诊断和管理的关键步骤，现有方法在信息保留和特征提取方面存在不足。
- Method: U-R-Veda模型整合了卷积变换、视觉变换器、残差链接、通道和空间注意力，以及基于边缘检测的跳跃连接，以减少信息损失并提升特征提取能力。
- Result: 模型在DSC指标上达到95.2%的平均准确率，优于其他模型，尤其在右心室和左心室心肌的分割上表现突出。
- Conclusion: U-R-Veda模型通过创新的注意力机制和特征提取方法，显著提升了心脏磁共振图像的语义分割效果，为医疗图像分析提供了更可靠的自动化工具。


### [119] [Development of MR spectral analysis method robust against static magnetic field inhomogeneity](https://arxiv.org/abs/2506.20897)
*Shuki Maruyama,Hidenori Takeshima*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的谱分析方法，通过模拟B0不均匀性诱导的谱变化，提高了谱分析的准确性。

- Motivation: 解决静态磁场B0不均匀性对谱分析准确性的影响。
- Method: 使用深度学习模型，训练基于B0图和健康人脑代谢物比生成的模拟谱，评估模型性能。
- Result: 模拟谱与实测谱接近，模型性能显著优于传统方法（如LCModel），MSE和MAPE均显著降低。
- Conclusion: 该方法通过增加训练样本，有望提高谱分析的准确性。


### [120] [Uncover Treasures in DCT: Advancing JPEG Quality Enhancement by Exploiting Latent Correlations](https://arxiv.org/abs/2506.21171)
*Jing Yang,Qunliang Xing,Mai Xu,Minglang Qiao*

Main category: eess.IV

TL;DR: 提出了一种基于DCT域的JPEG质量增强方法（AJQE），通过利用DCT系数中的相关性，显著提升了性能并降低了计算复杂度。

- Motivation: 现有JPEG质量增强方法多在像素域操作，计算成本高；而DCT域方法性能有限。
- Method: 识别JPEG图像DCT系数中的两种关键相关性，提出AJQE方法，将像素域模型适配到DCT域。
- Result: 与像素域方法相比，PSNR提升0.35 dB，增强吞吐量提高60.5%。
- Conclusion: AJQE方法在DCT域实现了更高效的JPEG质量增强，性能优于现有方法。


### [121] [GANet-Seg: Adversarial Learning for Brain Tumor Segmentation with Hybrid Generative Models](https://arxiv.org/abs/2506.21245)
*Qifei Cui,Xinyu Lu*

Main category: eess.IV

TL;DR: 提出了一种结合预训练GAN和Unet架构的脑肿瘤分割框架，通过全局异常检测和精细化掩模生成网络提高分割精度，利用多模态MRI数据和合成图像增强鲁棒性。

- Motivation: 解决脑肿瘤分割中标注数据有限的问题，并提高分割精度。
- Method: 结合预训练GAN和Unet架构，引入全局异常检测模块和精细化掩模生成网络，使用对抗损失约束和多模态MRI数据增强。
- Result: 在BraTS数据集上表现优异，Dice和HD95指标优于基线，减少了对全标注数据的依赖。
- Conclusion: 该方法具有可扩展性，适用于临床实际应用。


### [122] [Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising](https://arxiv.org/abs/2506.21499)
*Hojat Asgariandehkordi,Mostafa Sharifzadeh,Hassan Rivaz*

Main category: eess.IV

TL;DR: 提出一种零样本去噪框架，用于低角度CPWC成像，通过自监督学习提升对比度，无需额外训练数据。

- Motivation: CPWC成像中增加角度数会降低帧率并引入模糊伪影，且图像易受噪声影响，尤其在传输角度有限时。
- Method: 将传输角度分为两个子集生成含噪声的复合图像，通过自监督残差学习训练轻量级网络，分离噪声与组织信号。
- Result: 在仿真、体模和活体数据上，相比传统和深度学习去噪方法，表现出更优的对比度增强和结构保留。
- Conclusion: 该方法无需领域特定微调或配对数据，适应性强，计算成本低，适用于多种解剖区域和采集设置。


### [123] [Exploring the Design Space of 3D MLLMs for CT Report Generation](https://arxiv.org/abs/2506.21535)
*Mohammed Baharoon,Jun Ma,Congyu Fang,Augustin Toma,Bo Wang*

Main category: eess.IV

TL;DR: 本文系统研究了3D MLLMs在放射学报告生成（RRG）中的设计空间，包括视觉输入表示、投影器、LLMs和微调技术，并提出了两种知识增强方法，性能提升达10%。

- Motivation: 探索3D MLLMs在自动化放射学报告生成中的潜力，优化设计空间以提高性能。
- Method: 研究了视觉输入表示、投影器、LLMs和微调技术，并引入两种知识增强方法。
- Result: 在AMOS-MM数据集上，性能提升10%，达到MICCAI 2024挑战赛第二名；发现RRG性能与LLM大小无关，且分割掩码能提升性能。
- Conclusion: 3D MLLMs在RRG中具有潜力，设计选择和知识增强方法能显著提升性能。
## cs.GR

### [124] [Generative Blocks World: Moving Things Around in Pictures](https://arxiv.org/abs/2506.20703)
*Vaibhav Vavilala,Seemandhar Jain,Rahul Vasanth,D. A. Forsyth,Anand Bhattad*

Main category: cs.GR

TL;DR: 提出了一种通过操纵简单几何抽象来编辑生成图像场景的方法，利用3D基元表示场景，并通过流式生成方法实现高保真度编辑。

- Motivation: 现有方法在编辑生成图像场景时难以同时保持纹理一致性和对象身份，需要一种更灵活且高保真的编辑方法。
- Method: 将场景表示为3D凸基元的组合，支持不同基元数量的编辑；采用基于流的方法生成图像，结合深度和纹理提示。
- Result: 在视觉保真度、可编辑性和组合泛化性上优于现有方法。
- Conclusion: 该方法通过3D基元和纹理提示实现了高效且高保真的场景编辑。


### [125] [3DGH: 3D Head Generation with Composable Hair and Face](https://arxiv.org/abs/2506.20875)
*Chengan He,Junxuan Li,Tobias Kirschstein,Artem Sevastopolsky,Shunsuke Saito,Qingyang Tan,Javier Romero,Chen Cao,Holly Rushmeier,Giljoo Nam*

Main category: cs.GR

TL;DR: 3DGH是一种无条件生成3D人头模型的方法，通过分离头发和面部建模，使用基于模板的3D高斯泼溅和可变形头发几何，结合双生成器和交叉注意力机制，实现高质量合成和可编辑性。

- Motivation: 现有方法在头发和面部建模上存在纠缠问题，限制了生成和编辑的灵活性。3DGH旨在通过分离建模解决这一问题。
- Method: 提出基于模板的3D高斯泼溅数据表示，引入可变形头发几何；设计双生成器架构和交叉注意力机制；使用合成渲染和优化目标训练模型。
- Result: 实验验证了3DGH的设计有效性，在无条件全头图像合成和可编辑3D发型方面优于现有方法。
- Conclusion: 3DGH通过分离头发和面部建模，实现了高质量的生成和编辑能力，为3D人头建模提供了新思路。


### [126] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: VideoTex利用视频生成模型解决3D纹理的空间和时间不一致问题，通过几何感知条件和结构UV扩散策略，生成更平滑、一致的纹理。

- Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解，导致不一致性。视频生成模型的成功启发利用其解决纹理合成的时空问题。
- Method: 结合几何感知条件和结构UV扩散策略，利用3D网格结构生成纹理，增强遮挡区域生成。
- Result: VideoTex在纹理保真度、接缝融合和稳定性上优于现有方法，实现高质量、时间稳定的纹理。
- Conclusion: VideoTex为动态实时应用提供了视觉质量和时间一致性的解决方案。


### [127] [FairyGen: Storied Cartoon Video from a Single Child-Drawn Character](https://arxiv.org/abs/2506.21272)
*Jiayi Zheng,Xiaodong Cun*

Main category: cs.GR

TL;DR: FairyGen是一个自动系统，能从单张儿童绘画生成故事驱动的卡通视频，保留其独特艺术风格。

- Motivation: 现有方法主要关注角色一致性和基本动作，缺乏对风格化背景生成和电影化镜头设计的支持。
- Method: 结合MLLM生成结构化故事板，风格传播适配器确保视觉一致性，镜头设计模块增强多样性，3D代理和MMDiT模型实现动画。
- Result: 系统能生成风格一致、叙事结构清晰的动画视频。
- Conclusion: FairyGen在个性化故事动画中具有潜力，代码将开源。
## cs.LG

### [128] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 提出了一种轻量级回归模型，通过分析DNN不同层对攻击的响应差异来检测对抗样本，具有高效性和通用性。

- Motivation: 现有对抗样本检测方法要么效果不佳，要么计算效率低，无法满足实时处理需求。
- Method: 训练一个轻量级回归模型，预测深层特征与浅层特征的差异，利用预测误差检测对抗样本。
- Result: 该方法在多种DNN架构和领域（如图像、视频、音频）中均表现出高效性和有效性。
- Conclusion: 该方法为对抗样本检测提供了一种实用且高效的解决方案。


### [129] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为SharpZO的混合优化方法，用于在无需反向传播的情况下微调视觉语言模型，显著提升了性能和收敛速度。

- Motivation: 解决传统微调方法在内存受限的边缘设备上不可行的问题，同时避免高方差的进化策略或零阶优化方法的性能不足。
- Method: 采用两阶段优化：先通过锐度感知的进化策略全局探索和平滑损失函数，再通过稀疏零阶优化进行局部搜索。
- Result: 在CLIP模型上实验表明，SharpZO比现有方法平均提升7%的准确率，并加快收敛速度。
- Conclusion: SharpZO为无需反向传播的模型微调提供了一种高效且性能优越的解决方案。


### [130] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang,Peijia Li,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的数据选择方法RL-Selector，通过量化样本冗余性（epsilon-sample cover）动态优化选择策略，显著提升训练效率和模型性能。

- Motivation: 现实数据集通常存在大量冗余，传统方法依赖静态评分或预训练模型，忽略了样本选择和训练动态的交互作用。
- Method: 引入epsilon-sample cover量化冗余性，将数据选择建模为强化学习问题，RL-Selector通过动态奖励信号优化选择策略。
- Result: 在多个基准数据集和架构上，RL-Selector优于现有方法，训练效率和泛化性能均有提升。
- Conclusion: RL-Selector通过动态数据选择有效减少冗余，为高效训练提供了新范式。


### [131] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang,Kuangpu Guo,Zhihe Lu,Yunbo Wang,Jian Liang*

Main category: cs.LG

TL;DR: 提出了一种基于双提示学习和交叉融合的个性化联邦学习框架pFedDC，解决了数据、计算和通信异构性问题。

- Motivation: 联邦学习在异构环境下表现不佳，现有方法仅依赖文本提示且忽略标签-域分布变化。
- Method: 采用全局和局部提示的双提示学习，结合交叉融合模块自适应整合提示。
- Result: 在九种异构数据集上表现优于现有方法。
- Conclusion: pFedDC有效解决了联邦学习中的异构性问题，提升了模型性能。
## quant-ph

### [132] [ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers](https://arxiv.org/abs/2506.21537)
*Nicholas S. DiBrita,Jason Han,Tirthak Patel*

Main category: quant-ph

TL;DR: 量子计算在机器学习中的应用潜力推动了量子机器学习研究的发展。本文探讨了神经ODE（神经常微分方程）在残差神经网络（ResNets）中的应用，并提出了ResQ框架，利用模拟Rydberg原子量子计算机优化分类问题。

- Motivation: 探索量子计算在神经ODE和ResNets中的潜在优势，尤其是模拟Rydberg原子量子计算机的适用性。
- Method: 提出ResQ框架，通过模拟Rydberg原子量子计算机优化神经ODE的动态特性，用于机器学习分类问题。
- Result: 展示了模拟Rydberg原子量子计算机在ResNets中的高效性，并通过ResQ框架验证了其分类能力。
- Conclusion: 模拟Rydberg原子量子计算机在神经ODE和ResNets中具有显著潜力，ResQ框架为量子机器学习提供了新思路。
## cs.RO

### [133] [Model-Based Real-Time Pose and Sag Estimation of Overhead Power Lines Using LiDAR for Drone Inspection](https://arxiv.org/abs/2506.20812)
*Alexandre Girard,Steven A. Parkison,Philippe Hamelin*

Main category: cs.RO

TL;DR: 论文提出了一种基于LiDAR的无人机电力线检测方法，通过最小化LiDAR测量值与整体几何模型的误差，解决了导体点稀疏、检测不一致和干扰物区分的问题。

- Motivation: 无人机检测电力线时，LiDAR传感器面临导体点稀疏、检测不稳定和干扰物区分困难等挑战，需要一种更高效的定位方法。
- Method: 提出一种估计方法，通过最小化LiDAR测量值与整体导体阵列几何模型的误差，而非单独跟踪每根导体。
- Result: 实验表明，该方法在部分观测、噪声和异常值情况下仍能实现精确跟踪，求解器每帧收敛时间低于50毫秒，且对异常值的容忍度是有效导体测量值的两倍。
- Conclusion: 该方法在复杂环境下表现出高效性和鲁棒性，为无人机电力线检测提供了实用解决方案。


### [134] [ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation](https://arxiv.org/abs/2506.20969)
*Shruti Bansal,Wenshan Wang,Yifei Liu,Parv Maheshwari*

Main category: cs.RO

TL;DR: 论文提出了一种利用条件扩散模型将RGB图像转换为热图像的方法，以解决热成像数据不足的问题，促进热相机在机器人领域的应用。

- Motivation: 热相机在夜间或恶劣环境中能提供有价值的信息，但缺乏足够的数据限制了其在机器人领域的应用。
- Method: 使用条件扩散模型和自注意力机制，将现有的RGB图像转换为热图像，学习真实物体的热特性。
- Result: 通过合成热数据增强了现有数据集，为热相机的广泛应用提供了支持。
- Conclusion: 该方法为热成像数据的快速生成和热相机的普及提供了有效解决方案。


### [135] [V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling](https://arxiv.org/abs/2506.21041)
*Junwei You,Pei Li,Zhuoyu Jiang,Zilin Huang,Rui Gan,Haotian Shi,Bin Ran*

Main category: cs.RO

TL;DR: V2X-REALM是一个基于视觉语言模型（VLM）的框架，通过自适应多模态学习解决自动驾驶在长尾场景下的鲁棒性问题。

- Motivation: 解决自动驾驶在罕见、多样且视觉退化的长尾场景中的规划和决策问题，尤其是在协同环境中。
- Method: 提出三个核心创新：1）基于提示的长尾场景生成与评估；2）门控多场景自适应注意力模块；3）多任务场景感知对比学习目标。
- Result: 在复杂驾驶条件下，V2X-REALM在鲁棒性、语义推理、安全性和规划准确性上显著优于现有基线。
- Conclusion: V2X-REALM提升了端到端协同自动驾驶的可扩展性。
