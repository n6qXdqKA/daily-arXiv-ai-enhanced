{"id": "2507.13359", "pdf": "https://arxiv.org/pdf/2507.13359", "abs": "https://arxiv.org/abs/2507.13359", "authors": ["Yang Zhou", "Junjie Li", "CongYang Ou", "Dawei Yan", "Haokui Zhang", "Xizhe Xue"], "title": "Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives", "categories": ["cs.CV"], "comment": "27 pages, 5 figures", "summary": "Due to its extensive applications, aerial image object detection has long\nbeen a hot topic in computer vision. In recent years, advancements in Unmanned\nAerial Vehicles (UAV) technology have further propelled this field to new\nheights, giving rise to a broader range of application requirements. However,\ntraditional UAV aerial object detection methods primarily focus on detecting\npredefined categories, which significantly limits their applicability. The\nadvent of cross-modal text-image alignment (e.g., CLIP) has overcome this\nlimitation, enabling open-vocabulary object detection (OVOD), which can\nidentify previously unseen objects through natural language descriptions. This\nbreakthrough significantly enhances the intelligence and autonomy of UAVs in\naerial scene understanding. This paper presents a comprehensive survey of OVOD\nin the context of UAV aerial scenes. We begin by aligning the core principles\nof OVOD with the unique characteristics of UAV vision, setting the stage for a\nspecialized discussion. Building on this foundation, we construct a systematic\ntaxonomy that categorizes existing OVOD methods for aerial imagery and provides\na comprehensive overview of the relevant datasets. This structured review\nenables us to critically dissect the key challenges and open problems at the\nintersection of these fields. Finally, based on this analysis, we outline\npromising future research directions and application prospects. This survey\naims to provide a clear road map and a valuable reference for both newcomers\nand seasoned researchers, fostering innovation in this rapidly evolving domain.\nWe keep tracing related works at\nhttps://github.com/zhouyang2002/OVOD-in-UVA-imagery", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u65e0\u4eba\u673a\u822a\u62cd\u573a\u666f\u4e2d\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\uff08OVOD\uff09\uff0c\u63a2\u8ba8\u4e86\u5176\u6838\u5fc3\u539f\u7406\u3001\u65b9\u6cd5\u5206\u7c7b\u3001\u6570\u636e\u96c6\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u800c\u8de8\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6280\u672f\uff08\u5982CLIP\uff09\u63a8\u52a8\u4e86\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u7684\u53d1\u5c55\uff0c\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u822a\u62cd\u573a\u666f\u4e2d\u7684\u667a\u80fd\u6027\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709OVOD\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u65e0\u4eba\u673a\u89c6\u89c9\u7279\u70b9\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u7efc\u8ff0\u6846\u67b6\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86OVOD\u5728\u65e0\u4eba\u673a\u822a\u62cd\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\u3001\u5173\u952e\u6311\u6218\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u548c\u53c2\u8003\uff0c\u4fc3\u8fdb\u4e86\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u7684\u521b\u65b0\u3002"}}
{"id": "2507.13360", "pdf": "https://arxiv.org/pdf/2507.13360", "abs": "https://arxiv.org/abs/2507.13360", "authors": ["Le-Anh Tran", "Chung Nguyen Tran", "Ngoc-Luu Nguyen", "Nhan Cach Dang", "Jordi Carrabina", "David Castells-Rufas", "Minh Son Nguyen"], "title": "Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, ICCCE 2025", "summary": "This paper introduces a novel deep learning framework for low-light image\nenhancement, named the Encoder-Decoder Network with Illumination Guidance\n(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination\nmap, derived from Bright Channel Prior (BCP), as a guidance input. This\nillumination guidance helps the network focus on underexposed regions,\neffectively steering the enhancement process. To further improve the model's\nrepresentational power, a Spatial Pyramid Pooling (SPP) module is incorporated\nto extract multi-scale contextual features, enabling better handling of diverse\nlighting conditions. Additionally, the Swish activation function is employed to\nensure smoother gradient propagation during training. EDNIG is optimized within\na Generative Adversarial Network (GAN) framework using a composite loss\nfunction that combines adversarial loss, pixel-wise mean squared error (MSE),\nand perceptual loss. Experimental results show that EDNIG achieves competitive\nperformance compared to state-of-the-art methods in quantitative metrics and\nvisual quality, while maintaining lower model complexity, demonstrating its\nsuitability for real-world applications. The source code for this work is\navailable at https://github.com/tranleanh/ednig.", "AI": {"tldr": "EDNIG\u662f\u4e00\u79cd\u57fa\u4e8eU-Net\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u4eae\u5ea6\u5f15\u5bfc\u548cSPP\u6a21\u5757\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728GAN\u6846\u67b6\u4e2d\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u95ee\u9898\uff0c\u901a\u8fc7\u4eae\u5ea6\u5f15\u5bfc\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u63d0\u53d6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u7ed3\u5408U-Net\u67b6\u6784\uff0c\u5f15\u5165\u4eae\u5ea6\u56fe\u548cSPP\u6a21\u5757\uff0c\u4f7f\u7528Swish\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5728GAN\u6846\u67b6\u4e2d\u4f18\u5316\u3002", "result": "\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u6a21\u578b\u590d\u6742\u5ea6\u8f83\u4f4e\u3002", "conclusion": "EDNIG\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u9ad8\u6548\u3002"}}
{"id": "2507.13361", "pdf": "https://arxiv.org/pdf/2507.13361", "abs": "https://arxiv.org/abs/2507.13361", "authors": ["Shmuel Berman", "Jia Deng"], "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual Language Models (VLMs) excel at complex visual tasks such as VQA and\nchart understanding, yet recent work suggests they struggle with simple\nperceptual tests. We present an evaluation that tests vision-language models'\ncapacity for nonlocal visual reasoning -- reasoning that requires chaining\nevidence collected from multiple, possibly distant, regions of an image. We\nisolate three distinct forms of non-local vision: comparative perception, which\ndemands holding two images in working memory and comparing them; saccadic\nsearch, which requires making discrete, evidence-driven jumps to locate\nsuccessive targets; and smooth visual search, which involves searching smoothly\nalong a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude\nVision 3.7, GPT-o4-mini), even those that perform well on prior\nprimitive-vision benchmarks, fail these tests and barely exceed random accuracy\non two variants of our tasks that are trivial for humans. Our structured\nevaluation suite allows us to test if VLMs can perform similar visual\nalgorithms to humans. Our findings show that despite gains in raw visual\nacuity, current models lack core visual reasoning capabilities.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u975e\u5c40\u90e8\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u63a5\u8fd1\u968f\u673a\u51c6\u786e\u7387\u3002", "motivation": "\u8bc4\u4f30VLMs\u5728\u975e\u5c40\u90e8\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u4ee5\u63ed\u793a\u5176\u6838\u5fc3\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u79cd\u975e\u5c40\u90e8\u89c6\u89c9\u4efb\u52a1\uff08\u6bd4\u8f83\u611f\u77e5\u3001\u626b\u89c6\u641c\u7d22\u548c\u5e73\u6ed1\u89c6\u89c9\u641c\u7d22\uff09\uff0c\u6d4b\u8bd5\u4e3b\u6d41\u6a21\u578b\uff08\u5982Gemini 2.5 Pro\u3001Claude Vision 3.7\u3001GPT-o4-mini\uff09\u7684\u8868\u73b0\u3002", "result": "\u5373\u4f7f\u5728\u8fd9\u4e9b\u5bf9\u4eba\u7c7b\u6765\u8bf4\u7b80\u5355\u7684\u4efb\u52a1\u4e0a\uff0c\u4e3b\u6d41\u6a21\u578b\u7684\u8868\u73b0\u4e5f\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\u51c6\u786e\u7387\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u539f\u59cb\u89c6\u89c9\u654f\u9510\u5ea6\u4e0a\u6709\u8fdb\u6b65\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u6838\u5fc3\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.13362", "pdf": "https://arxiv.org/pdf/2507.13362", "abs": "https://arxiv.org/abs/2507.13362", "authors": ["Binbin Ji", "Siddharth Agrawal", "Qiance Tang", "Yvonne Wu"], "title": "Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.CL", "I.2.10; I.4.8; I.2.6; I.2.7; I.5.4; I.5.1"], "comment": "10 pages, 5 figures, submitted to a conference (IEEE formate).\n  Authored by students from the Courant Institute, NYU", "summary": "This study investigates the spatial reasoning capabilities of vision-language\nmodels (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement\nlearning. We begin by evaluating the impact of different prompting strategies\nand find that simple CoT formats, where the model generates a reasoning step\nbefore the answer, not only fail to help, but can even harm the model's\noriginal performance. In contrast, structured multi-stage prompting based on\nscene graphs (SceneGraph CoT) significantly improves spatial reasoning\naccuracy. Furthermore, to improve spatial reasoning ability, we fine-tune\nmodels using Group Relative Policy Optimization (GRPO) on the SAT dataset and\nevaluate their performance on CVBench. Compared to supervised fine-tuning\n(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates\nsuperior robustness under out-of-distribution (OOD) conditions. In particular,\nwe find that SFT overfits to surface-level linguistic patterns and may degrade\nperformance when test-time phrasing changes (e.g., from \"closer to\" to \"farther\nfrom\"). GRPO, on the other hand, generalizes more reliably and maintains stable\nperformance under such shifts. Our findings provide insights into how\nreinforcement learning and structured prompting improve the spatial reasoning\ncapabilities and generalization behavior of modern VLMs. All code is open\nsource at: https://github.com/Yvonne511/spatial-vlm-investigator", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7Chain-of-Thought\uff08CoT\uff09\u63d0\u793a\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6027\u80fd\u3002\u53d1\u73b0\u7ed3\u6784\u5316\u591a\u9636\u6bb5\u63d0\u793a\uff08SceneGraph CoT\uff09\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u800c\u7b80\u5355CoT\u53ef\u80fd\u635f\u5bb3\u6027\u80fd\u3002\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728SAT\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6a21\u578b\uff0c\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u5c24\u5176\u5728OOD\u6761\u4ef6\u4e0b\u66f4\u7a33\u5065\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u63d0\u793a\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347VLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\uff08\u5982SFT\uff09\u5728\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "1. \u8bc4\u4f30\u4e0d\u540cCoT\u63d0\u793a\u7b56\u7565\u7684\u6548\u679c\uff1b2. \u63d0\u51faSceneGraph CoT\u7ed3\u6784\u5316\u63d0\u793a\u65b9\u6cd5\uff1b3. \u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728SAT\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6a21\u578b\uff0c\u5e76\u5728CVBench\u4e0a\u8bc4\u4f30\u3002", "result": "SceneGraph CoT\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u51c6\u786e\u6027\uff1bGRPO\u5728Pass@1\u8bc4\u4f30\u4e2d\u4f18\u4e8eSFT\uff0c\u4e14\u5728OOD\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u7a33\u5065\uff0c\u907f\u514d\u4e86SFT\u56e0\u8bed\u8a00\u6a21\u5f0f\u8fc7\u62df\u5408\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u548c\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u80fd\u6709\u6548\u63d0\u5347VLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.13363", "pdf": "https://arxiv.org/pdf/2507.13363", "abs": "https://arxiv.org/abs/2507.13363", "authors": ["Atharv Goel", "Mehar Khurana"], "title": "Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern 3D object detection datasets are constrained by narrow class\ntaxonomies and costly manual annotations, limiting their ability to scale to\nopen-world settings. In contrast, 2D vision-language models trained on\nweb-scale image-text pairs exhibit rich semantic understanding and support\nopen-vocabulary detection via natural language prompts. In this work, we\nleverage the maturity and category diversity of 2D foundation models to perform\nopen-vocabulary 3D object detection without any human-annotated 3D labels.\n  Our pipeline uses a 2D vision-language detector to generate text-conditioned\nproposals, which are segmented with SAM and back-projected into 3D using camera\ngeometry and either LiDAR or monocular pseudo-depth. We introduce a geometric\ninflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D\nbounding boxes without training. To simulate adverse real-world conditions, we\nconstruct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes\ndataset.\n  Experiments demonstrate that our method achieves competitive localization\nperformance across multiple settings, including LiDAR-based and purely RGB-D\ninputs, all while remaining training-free and open-vocabulary. Our results\nhighlight the untapped potential of 2D foundation models for scalable 3D\nperception. We open-source our code and resources at\nhttps://github.com/atharv0goel/open-world-3D-det.", "AI": {"tldr": "\u5229\u75282D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f00\u653e\u8bcd\u6c473D\u7269\u4f53\u68c0\u6d4b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce83D\u6807\u7b7e\uff0c\u901a\u8fc7\u51e0\u4f55\u7b56\u7565\u63a8\u65ad3D\u8fb9\u754c\u6846\uff0c\u5e76\u5728\u591a\u79cd\u8f93\u5165\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u7269\u4f53\u68c0\u6d4b\u6570\u636e\u96c6\u53d7\u9650\u4e8e\u72ed\u7a84\u7684\u7c7b\u522b\u5206\u7c7b\u548c\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u96be\u4ee5\u9002\u5e94\u5f00\u653e\u4e16\u754c\u573a\u666f\u30022D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u80fd\u529b\uff0c\u53ef\u7528\u4e8e\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u75282D\u89c6\u89c9\u8bed\u8a00\u68c0\u6d4b\u5668\u751f\u6210\u6587\u672c\u6761\u4ef6\u63d0\u6848\uff0c\u901a\u8fc7SAM\u5206\u5272\u5e76\u5229\u7528\u76f8\u673a\u51e0\u4f55\u548cLiDAR\u6216\u5355\u76ee\u4f2a\u6df1\u5ea6\u53cd\u6295\u5f71\u52303D\u7a7a\u95f4\u3002\u5f15\u5165\u57fa\u4e8eDBSCAN\u805a\u7c7b\u548cRotating Calipers\u7684\u51e0\u4f55\u81a8\u80c0\u7b56\u7565\u63a8\u65ad3D\u8fb9\u754c\u6846\u3002", "result": "\u5728LiDAR\u548cRGB-D\u8f93\u5165\u7b49\u591a\u79cd\u8bbe\u7f6e\u4e0b\uff0c\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u5e76\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u3002", "conclusion": "\u5c55\u793a\u4e862D\u57fa\u7840\u6a21\u578b\u5728\u53ef\u6269\u5c553D\u611f\u77e5\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.13364", "pdf": "https://arxiv.org/pdf/2507.13364", "abs": "https://arxiv.org/abs/2507.13364", "authors": ["Siddharth Srivastava", "Gaurav Sharma"], "title": "OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present a novel multimodal multitask network and associated training\nalgorithm. The method is capable of ingesting data from approximately 12\ndifferent modalities namely image, video, audio, text, depth, point cloud, time\nseries, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed\napproach utilizes modality specialized tokenizers, a shared transformer\narchitecture, and cross-attention mechanisms to project the data from different\nmodalities into a unified embedding space. It addresses multimodal and\nmultitask scenarios by incorporating modality-specific task heads for different\ntasks in respective modalities. We propose a novel pretraining strategy with\niterative modality switching to initialize the network, and a training\nalgorithm which trades off fully joint training over all modalities, with\ntraining on pairs of modalities at a time. We provide comprehensive evaluation\nacross 25 datasets from 12 modalities and show state of the art performances,\ndemonstrating the effectiveness of the proposed architecture, pretraining\nstrategy and adapted multitask training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u591a\u4efb\u52a1\u7f51\u7edc\u53ca\u8bad\u7ec3\u7b97\u6cd5\uff0c\u652f\u630112\u79cd\u6a21\u6001\u6570\u636e\u8f93\u5165\uff0c\u901a\u8fc7\u5171\u4eabTransformer\u67b6\u6784\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7edf\u4e00\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u572825\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u548c\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u6570\u636e\u878d\u5408\u4e0e\u4efb\u52a1\u534f\u540c\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u591a\u6837\u5316\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u6a21\u6001\u4e13\u7528\u5206\u8bcd\u5668\u3001\u5171\u4eabTransformer\u67b6\u6784\u548c\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u51fa\u8fed\u4ee3\u6a21\u6001\u5207\u6362\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u6001\u5bf9\u8bad\u7ec3\u7b97\u6cd5\u3002", "result": "\u572812\u79cd\u6a21\u6001\u768425\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u591a\u4efb\u52a1\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u590d\u6742\u6570\u636e\u878d\u5408\u548c\u4efb\u52a1\u534f\u540c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13371", "pdf": "https://arxiv.org/pdf/2507.13371", "abs": "https://arxiv.org/abs/2507.13371", "authors": ["Yeming Cai", "Yang Wang", "Zhenglin Li"], "title": "Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper proposes an end-to-end deep learning framework integrating optical\nmotion capture with a Transformer-based model to enhance medical\nrehabilitation. It tackles data noise and missing data caused by occlusion and\nenvironmental factors, while detecting abnormal movements in real time to\nensure patient safety. Utilizing temporal sequence modeling, our framework\ndenoises and completes motion capture data, improving robustness. Evaluations\non stroke and orthopedic rehabilitation datasets show superior performance in\ndata reconstruction and anomaly detection, providing a scalable, cost-effective\nsolution for remote rehabilitation with reduced on-site supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u533b\u7597\u5eb7\u590d\uff0c\u901a\u8fc7\u5904\u7406\u566a\u58f0\u548c\u7f3a\u5931\u6570\u636e\uff0c\u5b9e\u65f6\u68c0\u6d4b\u5f02\u5e38\u52a8\u4f5c\u3002", "motivation": "\u89e3\u51b3\u56e0\u906e\u6321\u548c\u73af\u5883\u56e0\u7d20\u5bfc\u81f4\u7684\u6570\u636e\u566a\u58f0\u548c\u7f3a\u5931\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u60a3\u8005\u5b89\u5168\u3002", "method": "\u7ed3\u5408\u5149\u5b66\u8fd0\u52a8\u6355\u6349\u548cTransformer\u6a21\u578b\uff0c\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u53bb\u566a\u548c\u8865\u5168\u6570\u636e\u3002", "result": "\u5728\u5352\u4e2d\u548c\u9aa8\u79d1\u5eb7\u590d\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6570\u636e\u91cd\u5efa\u548c\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u8fdc\u7a0b\u5eb7\u590d\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u73b0\u573a\u76d1\u7763\u9700\u6c42\u3002"}}
{"id": "2507.13372", "pdf": "https://arxiv.org/pdf/2507.13372", "abs": "https://arxiv.org/abs/2507.13372", "authors": ["Yeming Cai", "Zhenglin Li", "Yang Wang"], "title": "Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Breast cancer is a leading cause of death among women globally, and early\ndetection is critical for improving survival rates. This paper introduces an\ninnovative framework that integrates Vision Transformers (ViT) and Graph Neural\nNetworks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.\nOur framework leverages ViT's ability to capture global image features and\nGNN's strength in modeling structural relationships, achieving an accuracy of\n84.2%, outperforming traditional methods. Additionally, interpretable attention\nheatmaps provide insights into the model's decision-making process, aiding\nradiologists in clinical settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Vision Transformers\u548cGraph Neural Networks\u7684\u521b\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u4e73\u817a\u764c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5728CBIS-DDSM\u6570\u636e\u96c6\u4e0a\u8fbe\u523084.2%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e73\u817a\u764c\u662f\u5168\u7403\u5973\u6027\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u63d0\u9ad8\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6574\u5408Vision Transformers\uff08ViT\uff09\u548cGraph Neural Networks\uff08GNN\uff09\uff0c\u5229\u7528ViT\u6355\u6349\u5168\u5c40\u56fe\u50cf\u7279\u5f81\u548cGNN\u5efa\u6a21\u7ed3\u6784\u5173\u7cfb\u7684\u80fd\u529b\u3002", "result": "\u5728CBIS-DDSM\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8684.2%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u70ed\u56fe\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2507.13373", "pdf": "https://arxiv.org/pdf/2507.13373", "abs": "https://arxiv.org/abs/2507.13373", "authors": ["Xiaojian Lin", "Wenxin Zhang", "Yuchu Jiang", "Wangyu Wu", "Yiran Guo", "Kangxu Wang", "Zongzheng Zhang", "Guijin Wang", "Lei Jin", "Hao Zhao"], "title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection", "categories": ["cs.CV", "I.4.8; I.2.10; H.5.1; I.2.6"], "comment": "10 pages, 6 figures. Supplementary material: 8 pages, 7 figures.\n  Accepted at ACM Multimedia 2025", "summary": "Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity.", "AI": {"tldr": "Butter\u662f\u4e00\u79cd\u65b0\u578b\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u5206\u5c42\u7279\u5f81\u8868\u793a\u63d0\u5347\u68c0\u6d4b\u9c81\u68d2\u6027\uff0c\u5f15\u5165FAFCE\u548cPHFFNet\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u67b6\u6784\uff08\u5982YOLO\u548cDETR\uff09\u5728\u52a8\u6001\u73af\u5883\u4e2d\u96be\u4ee5\u4fdd\u6301\u591a\u5c3a\u5ea6\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u4e14\u96be\u4ee5\u5e73\u8861\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "method": "Butter\u63d0\u51faFAFCE\u7ec4\u4ef6\uff08\u81ea\u9002\u5e94\u9891\u7387\u6ee4\u6ce2\u589e\u5f3a\u7279\u5f81\u4e00\u81f4\u6027\uff09\u548cPHFFNet\u6a21\u5757\uff08\u6e10\u8fdb\u5f0f\u5206\u5c42\u7279\u5f81\u878d\u5408\uff09\uff0c\u4f18\u5316\u591a\u5c3a\u5ea6\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728BDD100K\u3001KITTI\u548cCityscapes\u6570\u636e\u96c6\u4e0a\uff0cButter\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "conclusion": "Butter\u901a\u8fc7\u5206\u5c42\u7279\u5f81\u4f18\u5316\u4e0e\u878d\u5408\uff0c\u5728\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u3001\u53ef\u90e8\u7f72\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2507.13374", "pdf": "https://arxiv.org/pdf/2507.13374", "abs": "https://arxiv.org/abs/2507.13374", "authors": ["Kevin Dela Rosa"], "title": "Smart Routing for Multimodal Video Retrieval: When to Search What", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": "Accepted to ICCV 2025 Multimodal Representation and Retrieval\n  Workshop", "summary": "We introduce ModaRoute, an LLM-based intelligent routing system that\ndynamically selects optimal modalities for multimodal video retrieval. While\ndense text captions can achieve 75.9% Recall@5, they require expensive offline\nprocessing and miss critical visual information present in 34% of clips with\nscene text not captured by ASR. By analyzing query intent and predicting\ninformation needs, ModaRoute reduces computational overhead by 41% while\nachieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR\n(speech), OCR (text), and visual indices, averaging 1.78 modalities per query\nversus exhaustive 3.0 modality search. Evaluation on 1.8M video clips\ndemonstrates that intelligent routing provides a practical solution for scaling\nmultimodal retrieval systems, reducing infrastructure costs while maintaining\ncompetitive effectiveness for real-world deployment.", "AI": {"tldr": "ModaRoute\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u8def\u7531\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u591a\u6a21\u6001\u89c6\u9891\u68c0\u7d22\u7684\u6700\u4f18\u6a21\u6001\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u5e76\u4fdd\u6301\u9ad8\u6548\u68c0\u7d22\u3002", "motivation": "\u4f20\u7edf\u5bc6\u96c6\u6587\u672c\u6807\u6ce8\u9700\u8981\u6602\u8d35\u7684\u79bb\u7ebf\u5904\u7406\u4e14\u4f1a\u9057\u6f0f\u5173\u952e\u89c6\u89c9\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528GPT-4.1\u5206\u6790\u67e5\u8be2\u610f\u56fe\u5e76\u9884\u6d4b\u4fe1\u606f\u9700\u6c42\uff0c\u52a8\u6001\u8def\u7531\u67e5\u8be2\u5230ASR\u3001OCR\u548c\u89c6\u89c9\u7d22\u5f15\u3002", "result": "\u8ba1\u7b97\u5f00\u9500\u51cf\u5c1141%\uff0cRecall@5\u8fbe\u523060.9%\uff0c\u5e73\u5747\u6bcf\u67e5\u8be2\u4f7f\u75281.78\u79cd\u6a21\u6001\u3002", "conclusion": "\u667a\u80fd\u8def\u7531\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13378", "pdf": "https://arxiv.org/pdf/2507.13378", "abs": "https://arxiv.org/abs/2507.13378", "authors": ["Yuqi Cheng", "Yunkang Cao", "Haiming Yao", "Wei Luo", "Cheng Jiang", "Hui Zhang", "Weiming Shen"], "title": "A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects", "categories": ["cs.CV"], "comment": "27 pages, 7 figures", "summary": "Industrial defect detection is vital for upholding product quality across\ncontemporary manufacturing systems. As the expectations for precision,\nautomation, and scalability intensify, conventional inspection approaches are\nincreasingly found wanting in addressing real-world demands. Notable progress\nin computer vision and deep learning has substantially bolstered defect\ndetection capabilities across both 2D and 3D modalities. A significant\ndevelopment has been the pivot from closed-set to open-set defect detection\nframeworks, which diminishes the necessity for extensive defect annotations and\nfacilitates the recognition of novel anomalies. Despite such strides, a\ncohesive and contemporary understanding of industrial defect detection remains\nelusive. Consequently, this survey delivers an in-depth analysis of both\nclosed-set and open-set defect detection strategies within 2D and 3D\nmodalities, charting their evolution in recent years and underscoring the\nrising prominence of open-set techniques. We distill critical challenges\ninherent in practical detection environments and illuminate emerging trends,\nthereby providing a current and comprehensive vista of this swiftly progressing\nfield.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u7684\u73b0\u72b6\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u4ece\u5c01\u95ed\u96c6\u5230\u5f00\u653e\u96c6\u68c0\u6d4b\u65b9\u6cd5\u7684\u8f6c\u53d8\uff0c\u5e76\u63a2\u8ba8\u4e862D\u548c3D\u6a21\u6001\u4e0b\u7684\u6311\u6218\u4e0e\u8d8b\u52bf\u3002", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u5236\u9020\u4e1a\u5bf9\u7cbe\u5ea6\u3001\u81ea\u52a8\u5316\u548c\u53ef\u6269\u5c55\u6027\u7684\u9700\u6c42\uff0c\u800c\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdb\u6b65\u4e3a\u7f3a\u9677\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u5206\u6790\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u7f3a\u9677\u68c0\u6d4b\u7b56\u7565\uff0c\u7ed3\u54082D\u548c3D\u6a21\u6001\uff0c\u68b3\u7406\u5176\u8fd1\u5e74\u6765\u7684\u53d1\u5c55\u3002", "result": "\u5f00\u653e\u96c6\u68c0\u6d4b\u65b9\u6cd5\u9010\u6e10\u6210\u4e3a\u4e3b\u6d41\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u7f3a\u9677\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u5e76\u80fd\u8bc6\u522b\u65b0\u578b\u5f02\u5e38\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u9886\u57df\u7684\u5168\u9762\u89c6\u89d2\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.13385", "pdf": "https://arxiv.org/pdf/2507.13385", "abs": "https://arxiv.org/abs/2507.13385", "authors": ["Arjun Rao", "Esther Rolf"], "title": "Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 9 figures, 7 tables. Accepted to TerraBytes@ICML 2025", "summary": "A large variety of geospatial data layers is available around the world\nranging from remotely-sensed raster data like satellite imagery, digital\nelevation models, predicted land cover maps, and human-annotated data, to data\nderived from environmental sensors such as air temperature or wind speed data.\nA large majority of machine learning models trained on satellite imagery\n(SatML), however, are designed primarily for optical input modalities such as\nmulti-spectral satellite imagery. To better understand the value of using other\ninput modalities alongside optical imagery in supervised learning settings, we\ngenerate augmented versions of SatML benchmark tasks by appending additional\ngeographic data layers to datasets spanning classification, regression, and\nsegmentation. Using these augmented datasets, we find that fusing additional\ngeographic inputs with optical imagery can significantly improve SatML model\nperformance. Benefits are largest in settings where labeled data are limited\nand in geographic out-of-sample settings, suggesting that multi-modal inputs\nmay be especially valuable for data-efficiency and out-of-sample performance of\nSatML models. Surprisingly, we find that hard-coded fusion strategies\noutperform learned variants, with interesting implications for future work.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u536b\u661f\u56fe\u50cf\u673a\u5668\u5b66\u4e60\uff08SatML\uff09\u4e2d\u7ed3\u5408\u5176\u4ed6\u5730\u7406\u6570\u636e\u5c42\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u8f93\u5165\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u548c\u8de8\u533a\u57df\u573a\u666f\u4e0b\u3002", "motivation": "\u73b0\u6709SatML\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5149\u5b66\u56fe\u50cf\u8f93\u5165\uff0c\u4f46\u5176\u4ed6\u5730\u7406\u6570\u636e\u5c42\uff08\u5982\u9ad8\u7a0b\u6a21\u578b\u3001\u4f20\u611f\u5668\u6570\u636e\uff09\u53ef\u80fd\u63d0\u4f9b\u989d\u5916\u4ef7\u503c\uff0c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u70b9\u3002", "method": "\u901a\u8fc7\u4e3aSatML\u57fa\u51c6\u4efb\u52a1\u751f\u6210\u589e\u5f3a\u6570\u636e\u96c6\uff08\u6dfb\u52a0\u5176\u4ed6\u5730\u7406\u6570\u636e\u5c42\uff09\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u8f93\u5165\u5728\u5206\u7c7b\u3001\u56de\u5f52\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "result": "\u591a\u6a21\u6001\u8f93\u5165\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u548c\u8de8\u533a\u57df\u6d4b\u8bd5\u4e2d\uff1b\u786c\u7f16\u7801\u878d\u5408\u7b56\u7565\u4f18\u4e8e\u5b66\u4e60\u878d\u5408\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u8f93\u5165\u53ef\u63d0\u9ad8SatML\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u786c\u7f16\u7801\u878d\u5408\u7b56\u7565\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.13386", "pdf": "https://arxiv.org/pdf/2507.13386", "abs": "https://arxiv.org/abs/2507.13386", "authors": ["Yang Zhang", "Er Jin", "Yanfei Dong", "Yixuan Wu", "Philip Torr", "Ashkan Khakzar", "Johannes Stegmaier", "Kenji Kawaguchi"], "title": "Minimalist Concept Erasure in Generative Models", "categories": ["cs.CV", "cs.LG"], "comment": "ICML2025", "summary": "Recent advances in generative models have demonstrated remarkable\ncapabilities in producing high-quality images, but their reliance on\nlarge-scale unlabeled data has raised significant safety and copyright\nconcerns. Efforts to address these issues by erasing unwanted concepts have\nshown promise. However, many existing erasure methods involve excessive\nmodifications that compromise the overall utility of the model. In this work,\nwe address these issues by formulating a novel minimalist concept erasure\nobjective based \\emph{only} on the distributional distance of final generation\noutputs. Building on our formulation, we derive a tractable loss for\ndifferentiable optimization that leverages backpropagation through all\ngeneration steps in an end-to-end manner. We also conduct extensive analysis to\nshow theoretical connections with other models and methods. To improve the\nrobustness of the erasure, we incorporate neuron masking as an alternative to\nmodel fine-tuning. Empirical evaluations on state-of-the-art flow-matching\nmodels demonstrate that our method robustly erases concepts without degrading\noverall model performance, paving the way for safer and more responsible\ngenerative models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u8f93\u51fa\u5206\u5e03\u8ddd\u79bb\u7684\u6700\u5c0f\u5316\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u548c\u795e\u7ecf\u5143\u63a9\u7801\u63d0\u5347\u64e6\u9664\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u6570\u636e\u5f15\u53d1\u5b89\u5168\u548c\u7248\u6743\u95ee\u9898\uff0c\u73b0\u6709\u64e6\u9664\u65b9\u6cd5\u8fc7\u5ea6\u4fee\u6539\u6a21\u578b\u5f71\u54cd\u5176\u6548\u7528\u3002", "method": "\u57fa\u4e8e\u751f\u6210\u8f93\u51fa\u5206\u5e03\u8ddd\u79bb\u8bbe\u8ba1\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u53cd\u5411\u4f20\u64ad\u4f18\u5316\uff0c\u5f15\u5165\u795e\u7ecf\u5143\u63a9\u7801\u66ff\u4ee3\u5fae\u8c03\u3002", "result": "\u5728\u6d41\u5339\u914d\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u80fd\u9c81\u68d2\u64e6\u9664\u6982\u5ff5\u4e14\u4e0d\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u4e3a\u66f4\u5b89\u5168\u3001\u8d1f\u8d23\u4efb\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.13387", "pdf": "https://arxiv.org/pdf/2507.13387", "abs": "https://arxiv.org/abs/2507.13387", "authors": ["Chihiro Noguchi", "Takaki Yamamoto"], "title": "From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to ICCV Workshop 2025", "summary": "Accurate perception of the surrounding environment is essential for safe\nautonomous driving. 3D occupancy prediction, which estimates detailed 3D\nstructures of roads, buildings, and other objects, is particularly important\nfor vision-centric autonomous driving systems that do not rely on LiDAR\nsensors. However, in 3D semantic occupancy prediction -- where each voxel is\nassigned a semantic label -- annotated LiDAR point clouds are required, making\ndata acquisition costly. In contrast, large-scale binary occupancy data, which\nonly indicate occupied or free space without semantic labels, can be collected\nat a lower cost. Despite their availability, the potential of leveraging such\ndata remains unexplored. In this study, we investigate the utilization of\nlarge-scale binary occupancy data from two perspectives: (1) pre-training and\n(2) learning-based auto-labeling. We propose a novel binary occupancy-based\nframework that decomposes the prediction process into binary and semantic\noccupancy modules, enabling effective use of binary occupancy data. Our\nexperimental results demonstrate that the proposed framework outperforms\nexisting methods in both pre-training and auto-labeling tasks, highlighting its\neffectiveness in enhancing 3D semantic occupancy prediction. The code is\navailable at https://github.com/ToyotaInfoTech/b2s-occupancy", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u89c4\u6a21\u4e8c\u8fdb\u5236\u5360\u7528\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u9884\u6d4b\u8fc7\u7a0b\u4e3a\u4e8c\u8fdb\u5236\u548c\u8bed\u4e49\u5360\u7528\u6a21\u5757\uff0c\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\uff0c3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u9700\u8981\u6602\u8d35\u7684LiDAR\u6807\u6ce8\u6570\u636e\uff0c\u800c\u4e8c\u8fdb\u5236\u5360\u7528\u6570\u636e\u6210\u672c\u8f83\u4f4e\u4f46\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u8fdb\u5236\u5360\u7528\u7684\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e8c\u8fdb\u5236\u548c\u8bed\u4e49\u5360\u7528\u6a21\u5757\uff0c\u5229\u7528\u4e8c\u8fdb\u5236\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5b66\u4e60\u81ea\u52a8\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u548c\u81ea\u52a8\u6807\u6ce8\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5229\u7528\u4e86\u4f4e\u6210\u672c\u4e8c\u8fdb\u5236\u5360\u7528\u6570\u636e\uff0c\u4e3a3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13397", "pdf": "https://arxiv.org/pdf/2507.13397", "abs": "https://arxiv.org/abs/2507.13397", "authors": ["Kaiyuan Zhai", "Juan Chen", "Chao Wang", "Zeyi Xu"], "title": "InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Accurate pedestrian trajectory prediction is crucial for intelligent\napplications, yet it remains highly challenging due to the complexity of\ninteractions among pedestrians. Previous methods have primarily relied on\nrelative positions to model pedestrian interactions; however, they tend to\noverlook specific interaction patterns such as paired walking or conflicting\nbehaviors, limiting the prediction accuracy in crowded scenarios. To address\nthis issue, we propose InSyn (Interaction-Synchronization Network), a novel\nTransformer-based model that explicitly captures diverse interaction patterns\n(e.g., walking in sync or conflicting) while effectively modeling\ndirection-sensitive social behaviors. Additionally, we introduce a training\nstrategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue\nof initial-step divergence in numerical time-series prediction. Experiments on\nthe ETH and UCY datasets demonstrate that our model outperforms recent\nbaselines significantly, especially in high-density scenarios. Furthermore, the\nSSOS strategy proves effective in improving sequential prediction performance,\nreducing the initial-step prediction error by approximately 6.58%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578bInSyn\uff0c\u7528\u4e8e\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff0c\u901a\u8fc7\u663e\u5f0f\u6355\u6349\u591a\u6837\u5316\u7684\u4ea4\u4e92\u6a21\u5f0f\uff08\u5982\u540c\u6b65\u6216\u51b2\u7a81\uff09\u548c\u6539\u8fdb\u521d\u59cb\u6b65\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76f8\u5bf9\u4f4d\u7f6e\u5efa\u6a21\u884c\u4eba\u4ea4\u4e92\uff0c\u4f46\u5ffd\u7565\u4e86\u7279\u5b9a\u4ea4\u4e92\u6a21\u5f0f\uff08\u5982\u914d\u5bf9\u884c\u8d70\u6216\u51b2\u7a81\u884c\u4e3a\uff09\uff0c\u5bfc\u81f4\u5728\u62e5\u6324\u573a\u666f\u4e2d\u9884\u6d4b\u7cbe\u5ea6\u53d7\u9650\u3002", "method": "\u63d0\u51faInSyn\u6a21\u578b\uff0c\u7ed3\u5408Transformer\u67b6\u6784\u663e\u5f0f\u6355\u6349\u591a\u6837\u5316\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5e76\u5f15\u5165SSOS\u8bad\u7ec3\u7b56\u7565\u4ee5\u51cf\u5c11\u521d\u59cb\u6b65\u9884\u6d4b\u8bef\u5dee\u3002", "result": "\u5728ETH\u548cUCY\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e2d\uff0cSSOS\u7b56\u7565\u5c06\u521d\u59cb\u6b65\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u7ea66.58%\u3002", "conclusion": "InSyn\u6a21\u578b\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4ea4\u4e92\u6a21\u5f0f\u548cSSOS\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.13401", "pdf": "https://arxiv.org/pdf/2507.13401", "abs": "https://arxiv.org/abs/2507.13401", "authors": ["Shreya Kadambi", "Risheek Garrepalli", "Shubhankar Borse", "Munawar Hyatt", "Fatih Porikli"], "title": "MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing", "categories": ["cs.CV", "cs.LG"], "comment": "26 pages", "summary": "Despite the remarkable success of diffusion models in text-to-image\ngeneration, their effectiveness in grounded visual editing and compositional\ncontrol remains challenging. Motivated by advances in self-supervised learning\nand in-context generative modeling, we propose a series of simple yet powerful\ndesign choices that significantly enhance diffusion model capacity for\nstructured, controllable generation and editing. We introduce Masking-Augmented\nDiffusion with Inference-Time Scaling (MADI), a framework that improves the\neditability, compositionality and controllability of diffusion models through\ntwo core innovations. First, we introduce Masking-Augmented gaussian Diffusion\n(MAgD), a novel training strategy with dual corruption process which combines\nstandard denoising score matching and masked reconstruction by masking noisy\ninput from forward process. MAgD encourages the model to learn discriminative\nand compositional visual representations, thus enabling localized and\nstructure-aware editing. Second, we introduce an inference-time capacity\nscaling mechanism based on Pause Tokens, which act as special placeholders\ninserted into the prompt for increasing computational capacity at inference\ntime. Our findings show that adopting expressive and dense prompts during\ntraining further enhances performance, particularly for MAgD. Together, these\ncontributions in MADI substantially enhance the editability of diffusion\nmodels, paving the way toward their integration into more general-purpose,\nin-context generative diffusion architectures.", "AI": {"tldr": "MADI\u6846\u67b6\u901a\u8fc7Masking-Augmented gaussian Diffusion\uff08MAgD\uff09\u548c\u63a8\u7406\u65f6\u5bb9\u91cf\u6269\u5c55\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u53ef\u7f16\u8f91\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u89c6\u89c9\u7f16\u8f91\u548c\u7ec4\u5408\u63a7\u5236\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faMADI\u6846\u67b6\uff0c\u5305\u62ecMAgD\u8bad\u7ec3\u7b56\u7565\uff08\u7ed3\u5408\u53bb\u566a\u548c\u63a9\u7801\u91cd\u5efa\uff09\u548c\u63a8\u7406\u65f6Pause Tokens\u673a\u5236\u3002", "result": "MADI\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u7f16\u8f91\u80fd\u529b\u548c\u7ec4\u5408\u6027\uff0c\u5c24\u5176\u5728\u5bc6\u96c6\u63d0\u793a\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MADI\u4e3a\u6269\u6563\u6a21\u578b\u5728\u901a\u7528\u751f\u6210\u67b6\u6784\u4e2d\u7684\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13403", "pdf": "https://arxiv.org/pdf/2507.13403", "abs": "https://arxiv.org/abs/2507.13403", "authors": ["Morteza Bodaghi", "Majid Hosseini", "Raju Gottumukkala", "Ravi Teja Bhupatiraju", "Iftikhar Ahmad", "Moncef Gabbouj"], "title": "UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this study, we present a comprehensive public dataset for driver\ndrowsiness detection, integrating multimodal signals of facial, behavioral, and\nbiometric indicators. Our dataset includes 3D facial video using a depth\ncamera, IR camera footage, posterior videos, and biometric signals such as\nheart rate, electrodermal activity, blood oxygen saturation, skin temperature,\nand accelerometer data. This data set provides grip sensor data from the\nsteering wheel and telemetry data from the American truck simulator game to\nprovide more information about drivers' behavior while they are alert and\ndrowsy. Drowsiness levels were self-reported every four minutes using the\nKarolinska Sleepiness Scale (KSS). The simulation environment consists of three\nmonitor setups, and the driving condition is completely like a car. Data were\ncollected from 19 subjects (15 M, 4 F) in two conditions: when they were fully\nalert and when they exhibited signs of sleepiness. Unlike other datasets, our\nmultimodal dataset has a continuous duration of 40 minutes for each data\ncollection session per subject, contributing to a total length of 1,400\nminutes, and we recorded gradual changes in the driver state rather than\ndiscrete alert/drowsy labels. This study aims to create a comprehensive\nmultimodal dataset of driver drowsiness that captures a wider range of\nphysiological, behavioral, and driving-related signals. The dataset will be\navailable upon request to the corresponding author.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u7684\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5305\u542b\u9762\u90e8\u3001\u884c\u4e3a\u548c\u751f\u7269\u7279\u5f81\u4fe1\u53f7\uff0c\u65e8\u5728\u66f4\u5168\u9762\u5730\u6355\u6349\u75b2\u52b3\u72b6\u6001\u7684\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u53ea\u5173\u6ce8\u79bb\u6563\u7684\u75b2\u52b3\u6807\u7b7e\uff0c\u7f3a\u4e4f\u8fde\u7eed\u72b6\u6001\u53d8\u5316\u7684\u8bb0\u5f55\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u591a\u6a21\u6001\u6570\u636e\u3002", "method": "\u6570\u636e\u96c6\u6574\u5408\u4e863D\u9762\u90e8\u89c6\u9891\u3001\u7ea2\u5916\u6444\u50cf\u3001\u540e\u89c6\u89c6\u9891\u3001\u751f\u7269\u7279\u5f81\u4fe1\u53f7\uff08\u5982\u5fc3\u7387\u3001\u76ae\u80a4\u7535\u6d3b\u52a8\u7b49\uff09\u4ee5\u53ca\u65b9\u5411\u76d8\u63e1\u529b\u6570\u636e\u548c\u6a21\u62df\u5668\u9065\u6d4b\u6570\u636e\u3002\u6570\u636e\u91c7\u96c6\u81ea19\u540d\u53d7\u8bd5\u8005\uff0c\u6bcf\u6b21\u6301\u7eed40\u5206\u949f\u3002", "result": "\u6570\u636e\u96c6\u603b\u65f6\u957f1,400\u5206\u949f\uff0c\u8bb0\u5f55\u4e86\u4ece\u8b66\u89c9\u5230\u75b2\u52b3\u7684\u8fde\u7eed\u72b6\u6001\u53d8\u5316\uff0c\u800c\u975e\u79bb\u6563\u6807\u7b7e\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u75b2\u52b3\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u591a\u6a21\u6001\u8d44\u6e90\uff0c\u672a\u6765\u53ef\u652f\u6301\u66f4\u7cbe\u786e\u7684\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2507.13404", "pdf": "https://arxiv.org/pdf/2507.13404", "abs": "https://arxiv.org/abs/2507.13404", "authors": ["Delin An", "Pan Du", "Jian-Xun Wang", "Chaoli Wang"], "title": "AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D aortic construction is crucial for clinical diagnosis,\npreoperative planning, and computational fluid dynamics (CFD) simulations, as\nit enables the estimation of critical hemodynamic parameters such as blood flow\nvelocity, pressure distribution, and wall shear stress. Existing construction\nmethods often rely on large annotated training datasets and extensive manual\nintervention. While the resulting meshes can serve for visualization purposes,\nthey struggle to produce geometrically consistent, well-constructed surfaces\nsuitable for downstream CFD analysis. To address these challenges, we introduce\nAortaDiff, a diffusion-based framework that generates smooth aortic surfaces\ndirectly from CT/MRI volumes. AortaDiff first employs a volume-guided\nconditional diffusion model (CDM) to iteratively generate aortic centerlines\nconditioned on volumetric medical images. Each centerline point is then\nautomatically used as a prompt to extract the corresponding vessel contour,\nensuring accurate boundary delineation. Finally, the extracted contours are\nfitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh\nrepresentation. AortaDiff offers distinct advantages over existing methods,\nincluding an end-to-end workflow, minimal dependency on large labeled datasets,\nand the ability to generate CFD-compatible aorta meshes with high geometric\nfidelity. Experimental results demonstrate that AortaDiff performs effectively\neven with limited training data, successfully constructing both normal and\npathologically altered aorta meshes, including cases with aneurysms or\ncoarctation. This capability enables the generation of high-quality\nvisualizations and positions AortaDiff as a practical solution for\ncardiovascular research.", "AI": {"tldr": "AortaDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4eceCT/MRI\u4f53\u79ef\u751f\u6210\u5e73\u6ed1\u7684\u4e3b\u52a8\u8109\u8868\u9762\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u548c\u624b\u52a8\u5e72\u9884\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8eCFD\u5206\u6790\u3002", "motivation": "\u51c6\u786e\u76843D\u4e3b\u52a8\u8109\u6784\u5efa\u5bf9\u4e34\u5e8a\u8bca\u65ad\u548cCFD\u6a21\u62df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u548c\u624b\u52a8\u5e72\u9884\uff0c\u751f\u6210\u7684\u7f51\u683c\u51e0\u4f55\u4e00\u81f4\u6027\u5dee\u3002", "method": "AortaDiff\u4f7f\u7528\u4f53\u79ef\u5f15\u5bfc\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u4e3b\u52a8\u8109\u4e2d\u5fc3\u7ebf\uff0c\u81ea\u52a8\u63d0\u53d6\u8840\u7ba1\u8f6e\u5ed3\u5e76\u62df\u5408\u4e3a\u5e73\u6ed13D\u8868\u9762\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAortaDiff\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u6709\u6548\uff0c\u80fd\u6784\u5efa\u6b63\u5e38\u548c\u75c5\u7406\u4e3b\u52a8\u8109\u7f51\u683c\uff0c\u51e0\u4f55\u4fdd\u771f\u5ea6\u9ad8\u3002", "conclusion": "AortaDiff\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u3001\u4f4e\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5fc3\u8840\u7ba1\u7814\u7a76\u3002"}}
{"id": "2507.13405", "pdf": "https://arxiv.org/pdf/2507.13405", "abs": "https://arxiv.org/abs/2507.13405", "authors": ["Ishant Chintapatla", "Kazuma Choji", "Naaisha Agarwal", "Andrew Lin", "Hannah You", "Charles Duong", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recently, many benchmarks and datasets have been developed to evaluate\nVision-Language Models (VLMs) using visual question answering (VQA) pairs, and\nmodels have shown significant accuracy improvements. However, these benchmarks\nrarely test the model's ability to accurately complete visual entailment, for\ninstance, accepting or refuting a hypothesis based on the image. To address\nthis, we propose COREVQA (Crowd Observations and Reasoning Entailment), a\nbenchmark of 5608 image and synthetically generated true/false statement pairs,\nwith images derived from the CrowdHuman dataset, to provoke visual entailment\nreasoning on challenging crowded images. Our results show that even the\ntop-performing VLMs achieve accuracy below 80%, with other models performing\nsubstantially worse (39.98%-69.95%). This significant performance gap reveals\nkey limitations in VLMs' ability to reason over certain types of image-question\npairs in crowded scenes.", "AI": {"tldr": "COREVQA\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9\u8574\u542b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u62e5\u6324\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u5f88\u5c11\u8bc4\u4f30\u6a21\u578b\u5728\u89c6\u89c9\u8574\u542b\u4efb\u52a1\uff08\u5982\u57fa\u4e8e\u56fe\u50cf\u63a5\u53d7\u6216\u53cd\u9a73\u5047\u8bbe\uff09\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86COREVQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5608\u5f20\u56fe\u50cf\u548c\u5408\u6210\u7684\u771f/\u5047\u9648\u8ff0\u5bf9\uff0c\u56fe\u50cf\u6765\u81eaCrowdHuman\u6570\u636e\u96c6\u3002", "result": "\u5373\u4f7f\u8868\u73b0\u6700\u597d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u51c6\u786e\u7387\u4e5f\u4f4e\u4e8e80%\uff0c\u5176\u4ed6\u6a21\u578b\u8868\u73b0\u66f4\u5dee\uff0839.98%-69.95%\uff09\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u62e5\u6324\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u8574\u542b\u63a8\u7406\u80fd\u529b\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002"}}
{"id": "2507.13407", "pdf": "https://arxiv.org/pdf/2507.13407", "abs": "https://arxiv.org/abs/2507.13407", "authors": ["Vinu Sankar Sadasivan", "Mehrdad Saberi", "Soheil Feizi"], "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Accepted at ICLR 2025 Workshop on GenAI Watermarking (WMARK)", "summary": "With the rapid rise of generative AI and synthetic media, distinguishing\nAI-generated images from real ones has become crucial in safeguarding against\nmisinformation and ensuring digital authenticity. Traditional watermarking\ntechniques have shown vulnerabilities to adversarial attacks, undermining their\neffectiveness in the presence of attackers. We propose IConMark, a novel\nin-generation robust semantic watermarking method that embeds interpretable\nconcepts into AI-generated images, as a first step toward interpretable\nwatermarking. Unlike traditional methods, which rely on adding noise or\nperturbations to AI-generated images, IConMark incorporates meaningful semantic\nattributes, making it interpretable to humans and hence, resilient to\nadversarial manipulation. This method is not only robust against various image\naugmentations but also human-readable, enabling manual verification of\nwatermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,\ndemonstrating its superiority in terms of detection accuracy and maintaining\nimage quality. Moreover, IConMark can be combined with existing watermarking\ntechniques to further enhance and complement its robustness. We introduce\nIConMark+SS and IConMark+TM, hybrid approaches combining IConMark with\nStegaStamp and TrustMark, respectively, to further bolster robustness against\nmultiple types of image manipulations. Our base watermarking technique\n(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%\nhigher mean area under the receiver operating characteristic curve (AUROC)\nscores for watermark detection, respectively, compared to the best baseline on\nvarious datasets.", "AI": {"tldr": "IConMark\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8bed\u4e49\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u5230AI\u751f\u6210\u7684\u56fe\u50cf\u4e2d\uff0c\u63d0\u9ad8\u6c34\u5370\u7684\u9c81\u68d2\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u548c\u5408\u6210\u5a92\u4f53\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u533a\u5206AI\u751f\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u5bf9\u9632\u6b62\u9519\u8bef\u4fe1\u606f\u548c\u786e\u4fdd\u6570\u5b57\u771f\u5b9e\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u6c34\u5370\u6280\u672f\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "IConMark\u901a\u8fc7\u5728AI\u751f\u6210\u56fe\u50cf\u4e2d\u5d4c\u5165\u6709\u610f\u4e49\u7684\u8bed\u4e49\u5c5e\u6027\uff0c\u800c\u975e\u566a\u58f0\u6216\u6270\u52a8\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u6c34\u5370\u3002\u8be5\u65b9\u6cd5\u8fd8\u7ed3\u5408\u4e86StegaStamp\u548cTrustMark\uff0c\u5f62\u6210\u6df7\u5408\u65b9\u6cd5IConMark+SS\u548cIConMark+TM\u3002", "result": "IConMark\u53ca\u5176\u53d8\u4f53\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u4fdd\u6301\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0cAUROC\u5206\u6570\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u9ad8\u51fa10.8%\u300114.5%\u548c15.9%\u3002", "conclusion": "IConMark\u4e3a\u53ef\u89e3\u91ca\u6c34\u5370\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5176\u9c81\u68d2\u6027\u548c\u53ef\u8bfb\u6027\u4f7f\u5176\u6210\u4e3a\u5bf9\u6297\u56fe\u50cf\u64cd\u7eb5\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.13408", "pdf": "https://arxiv.org/pdf/2507.13408", "abs": "https://arxiv.org/abs/2507.13408", "authors": ["Hemanth Kumar M", "Karthika M", "Saianiruth M", "Vasanthakumar Venugopal", "Anandakumar D", "Revathi Ezhumalai", "Charulatha K", "Kishore Kumar J", "Dayana G", "Kalyan Sivasailam", "Bargava Subramanian"], "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10"], "comment": "12 pages, 2 figures", "summary": "Background: Shoulder fractures are often underdiagnosed, especially in\nemergency and high-volume clinical settings. Studies report up to 10% of such\nfractures may be missed by radiologists. AI-driven tools offer a scalable way\nto assist early detection and reduce diagnostic delays. We address this gap\nthrough a dedicated AI system for shoulder radiographs. Methods: We developed a\nmulti-model deep learning system using 10,000 annotated shoulder X-rays.\nArchitectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and\nRF-DETR. To enhance detection, we applied bounding box and classification-level\nensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW\nensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming\nindividual models across all key metrics. It demonstrated strong recall and\nlocalization precision, confirming its effectiveness for clinical fracture\ndetection in shoulder X-rays. Conclusion: The results show ensemble-based AI\ncan reliably detect shoulder fractures in radiographs with high clinical\nrelevance. The model's accuracy and deployment readiness position it well for\nintegration into real-time diagnostic workflows. The current model is limited\nto binary fracture detection, reflecting its design for rapid screening and\ntriage support rather than detailed orthopedic classification.", "AI": {"tldr": "AI\u9a71\u52a8\u7684\u591a\u6a21\u578b\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u901a\u8fc7\u96c6\u6210\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u80a9\u90e8\u9aa8\u6298\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u8fbe\u523095.5%\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5feb\u901f\u7b5b\u67e5\u3002", "motivation": "\u89e3\u51b3\u80a9\u90e8\u9aa8\u6298\u5728\u6025\u8bca\u548c\u9ad8\u6d41\u91cf\u4e34\u5e8a\u73af\u5883\u4e2d\u6f0f\u8bca\u7387\u9ad8\u7684\u95ee\u9898\uff0c\u5229\u7528AI\u6280\u672f\u63d0\u4f9b\u65e9\u671f\u68c0\u6d4b\u652f\u6301\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e10,000\u5f20\u6807\u6ce8\u80a9\u90e8X\u5149\u7247\u7684\u591a\u6a21\u578b\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u91c7\u7528Faster R-CNN\u3001EfficientDet\u548cRF-DETR\u67b6\u6784\uff0c\u5e76\u5e94\u7528Soft-NMS\u3001WBF\u548cNMW\u878d\u5408\u7b49\u96c6\u6210\u6280\u672f\u3002", "result": "NMW\u96c6\u6210\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe95.5%\uff0cF1\u5206\u6570\u4e3a0.9610\uff0c\u53ec\u56de\u7387\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u5747\u8f83\u9ad8\u3002", "conclusion": "\u96c6\u6210AI\u6a21\u578b\u80fd\u53ef\u9760\u68c0\u6d4b\u80a9\u90e8\u9aa8\u6298\uff0c\u9002\u5408\u5b9e\u65f6\u8bca\u65ad\u5de5\u4f5c\u6d41\uff0c\u4f46\u4ec5\u9650\u4e8e\u4e8c\u5143\u9aa8\u6298\u68c0\u6d4b\uff0c\u8bbe\u8ba1\u7528\u4e8e\u5feb\u901f\u7b5b\u67e5\u800c\u975e\u8be6\u7ec6\u5206\u7c7b\u3002"}}
{"id": "2507.13420", "pdf": "https://arxiv.org/pdf/2507.13420", "abs": "https://arxiv.org/abs/2507.13420", "authors": ["Alessandro Pistola", "Valentina Orru'", "Nicolo' Marchetti", "Marco Roccetti"], "title": "AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "25 pages, 9 Figures", "summary": "By upgrading an existing deep learning model with the knowledge provided by\none of the oldest sets of grayscale satellite imagery, known as CORONA, we\nimproved the AI model attitude towards the automatic identification of\narchaeological sites in an environment which has been completely transformed in\nthe last five decades, including the complete destruction of many of those same\nsites. The initial Bing based convolutional network model was retrained using\nCORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,\ncentral Mesopotamian floodplain. The results were twofold and surprising.\nFirst, the detection precision obtained on the area of interest increased\nsensibly: in particular, the Intersection over Union (IoU) values, at the image\nsegmentation level, surpassed 85 percent, while the general accuracy in\ndetecting archeological sites reached 90 percent. Second, our retrained model\nallowed the identification of four new sites of archaeological interest\n(confirmed through field verification), previously not identified by\narchaeologists with traditional techniques. This has confirmed the efficacy of\nusing AI techniques and the CORONA imagery from the 1960 to discover\narchaeological sites currently no longer visible, a concrete breakthrough with\nsignificant consequences for the study of landscapes with vanishing\narchaeological evidence induced by anthropization", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u53e4\u8001\u7684CORONA\u536b\u661f\u5f71\u50cf\u5347\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8003\u53e4\u9057\u5740\u81ea\u52a8\u8bc6\u522b\u7684\u7cbe\u5ea6\uff0c\u5e76\u53d1\u73b0\u4e86\u56db\u4e2a\u65b0\u9057\u5740\u3002", "motivation": "\u5229\u7528CORONA\u536b\u661f\u5f71\u50cf\u6539\u8fdbAI\u6a21\u578b\uff0c\u4ee5\u8bc6\u522b\u56e0\u4eba\u4e3a\u6d3b\u52a8\u800c\u6d88\u5931\u7684\u8003\u53e4\u9057\u5740\u3002", "method": "\u5728Bing\u5377\u79ef\u7f51\u7edc\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u4f7f\u7528CORONA\u5f71\u50cf\u5bf9\u4f0a\u62c9\u514b\u963f\u5e03\u683c\u83b1\u5e03\u5730\u533a\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u68c0\u6d4b\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff08IoU\u8d85\u8fc785%\uff0c\u603b\u4f53\u51c6\u786e\u7387\u8fbe90%\uff09\uff0c\u5e76\u53d1\u73b0\u56db\u4e2a\u65b0\u9057\u5740\u3002", "conclusion": "AI\u6280\u672f\u4e0eCORONA\u5f71\u50cf\u7ed3\u5408\uff0c\u4e3a\u8003\u53e4\u9057\u5740\u8bc6\u522b\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002"}}
{"id": "2507.13425", "pdf": "https://arxiv.org/pdf/2507.13425", "abs": "https://arxiv.org/abs/2507.13425", "authors": ["Sirui Wang", "Zhou Guan", "Bingxi Zhao", "Tongjia Gu"], "title": "CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate prediction of driving intention is key to enhancing the safety and\ninteractive efficiency of human-machine co-driving systems. It serves as a\ncornerstone for achieving high-level autonomous driving. However, current\napproaches remain inadequate for accurately modeling the complex\nspatio-temporal interdependencies and the unpredictable variability of human\ndriving behavior. To address these challenges, we propose CaSTFormer, a Causal\nSpatio-Temporal Transformer to explicitly model causal interactions between\ndriver behavior and environmental context for robust intention prediction.\nSpecifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)\nmechanism for precise temporal alignment of internal and external feature\nstreams, a Causal Pattern Extraction (CPE) module that systematically\neliminates spurious correlations to reveal authentic causal dependencies, and\nan innovative Feature Synthesis Network (FSN) that adaptively synthesizes these\npurified representations into coherent spatio-temporal inferences. We evaluate\nthe proposed CaSTFormer on the public Brain4Cars dataset, and it achieves\nstate-of-the-art performance. It effectively captures complex causal\nspatio-temporal dependencies and enhances both the accuracy and transparency of\ndriving intention prediction.", "AI": {"tldr": "CaSTFormer\u662f\u4e00\u79cd\u56e0\u679c\u65f6\u7a7a\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u5efa\u6a21\u9a7e\u9a76\u5458\u884c\u4e3a\u4e0e\u73af\u5883\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u63d0\u5347\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u590d\u6742\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\u548c\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u63d0\u51faCaSTFormer\uff0c\u5305\u542bReciprocal Shift Fusion\uff08RSF\uff09\u673a\u5236\u3001Causal Pattern Extraction\uff08CPE\uff09\u6a21\u5757\u548cFeature Synthesis Network\uff08FSN\uff09\u3002", "result": "\u5728Brain4Cars\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6709\u6548\u6355\u6349\u590d\u6742\u56e0\u679c\u65f6\u7a7a\u4f9d\u8d56\u3002", "conclusion": "CaSTFormer\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2507.13428", "pdf": "https://arxiv.org/pdf/2507.13428", "abs": "https://arxiv.org/abs/2507.13428", "authors": ["Jing Gu", "Xian Liu", "Yu Zeng", "Ashwin Nagarajan", "Fangrui Zhu", "Daniel Hong", "Yue Fan", "Qianqi Yan", "Kaiwen Zhou", "Ming-Yu Liu", "Xin Eric Wang"], "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models", "categories": ["cs.CV", "cs.AI"], "comment": "31 pages, 21 figures", "summary": "Video generation models have achieved remarkable progress in creating\nhigh-quality, photorealistic content. However, their ability to accurately\nsimulate physical phenomena remains a critical and unresolved challenge. This\npaper presents PhyWorldBench, a comprehensive benchmark designed to evaluate\nvideo generation models based on their adherence to the laws of physics. The\nbenchmark covers multiple levels of physical phenomena, ranging from\nfundamental principles like object motion and energy conservation to more\ncomplex scenarios involving rigid body interactions and human or animal motion.\nAdditionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts\nintentionally violate real-world physics, enabling the assessment of whether\nmodels can follow such instructions while maintaining logical consistency.\nBesides large-scale human evaluation, we also design a simple yet effective\nmethod that could utilize current MLLM to evaluate the physics realism in a\nzero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation\nmodels, including five open-source and five proprietary models, with a detailed\ncomparison and analysis. we identify pivotal challenges models face in adhering\nto real-world physics. Through systematic testing of their outputs across 1,050\ncurated prompts-spanning fundamental, composite, and anti-physics scenarios-we\nidentify pivotal challenges these models face in adhering to real-world\nphysics. We then rigorously examine their performance on diverse physical\nphenomena with varying prompt types, deriving targeted recommendations for\ncrafting prompts that enhance fidelity to physical principles.", "AI": {"tldr": "PhyWorldBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u7269\u7406\u6a21\u62df\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u4ece\u57fa\u7840\u7269\u7406\u73b0\u8c61\u5230\u590d\u6742\u573a\u666f\uff0c\u5e76\u5f15\u5165\u201c\u53cd\u7269\u7406\u201d\u7c7b\u522b\u3002\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u548cMLLM\u65b9\u6cd5\u6d4b\u8bd512\u4e2a\u5148\u8fdb\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5728\u9075\u5faa\u7269\u7406\u89c4\u5f8b\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u73b0\u8c61\u6a21\u62df\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "\u8bbe\u8ba1PhyWorldBench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u7ea7\u7269\u7406\u73b0\u8c61\u548c\u201c\u53cd\u7269\u7406\u201d\u7c7b\u522b\uff0c\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u548cMLLM\u65b9\u6cd5\u6d4b\u8bd512\u4e2a\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u7269\u7406\u6a21\u62df\u4e0a\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u548c\u53cd\u7269\u7406\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u7269\u7406\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u5efa\u8bae\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2507.13486", "pdf": "https://arxiv.org/pdf/2507.13486", "abs": "https://arxiv.org/abs/2507.13486", "authors": ["Debao Huang", "Rongjun Qin"], "title": "Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation", "categories": ["cs.CV"], "comment": "16 pages, 9 figures, this manuscript has been submitted to ISPRS\n  Journal of Photogrammetry and Remote Sensing for consideration", "summary": "Uncertainty quantification of the photogrammetry process is essential for\nproviding per-point accuracy credentials of the point clouds. Unlike airborne\nLiDAR, which typically delivers consistent accuracy across various scenes, the\naccuracy of photogrammetric point clouds is highly scene-dependent, since it\nrelies on algorithm-generated measurements (i.e., stereo or multi-view stereo).\nGenerally, errors of the photogrammetric point clouds propagate through a\ntwo-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),\nfollowed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM\nstage has been well studied using the first-order statistics of the\nreprojection error function, that in the MVS stage remains largely unsolved and\nnon-standardized, primarily due to its non-differentiable and multi-modal\nnature (i.e., from pixel values to geometry). In this paper, we present an\nuncertainty quantification framework closing this gap by associating an error\ncovariance matrix per point accounting for this two-step photogrammetry\nprocess. Specifically, to estimate the uncertainty in the MVS stage, we propose\na novel, self-calibrating method by taking reliable n-view points (n>=6)\nper-view to regress the disparity uncertainty using highly relevant cues (such\nas matching cost values) from the MVS stage. Compared to existing approaches,\nour method uses self-contained, reliable 3D points extracted directly from the\nMVS process, with the benefit of being self-supervised and naturally adhering\nto error propagation path of the photogrammetry process, thereby providing a\nrobust and certifiable uncertainty quantification across diverse scenes. We\nevaluate the framework using a variety of publicly available airborne and UAV\nimagery datasets. Results demonstrate that our method outperforms existing\napproaches by achieving high bounding rates without overestimating uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6444\u5f71\u6d4b\u91cf\u70b9\u4e91\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bef\u5dee\u534f\u65b9\u5dee\u77e9\u9635\u4e3a\u6bcf\u4e2a\u70b9\u63d0\u4f9b\u7cbe\u5ea6\u8ba4\u8bc1\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u7acb\u4f53\uff08MVS\uff09\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u6444\u5f71\u6d4b\u91cf\u70b9\u4e91\u7684\u7cbe\u5ea6\u9ad8\u5ea6\u4f9d\u8d56\u573a\u666f\uff0c\u4e14MVS\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5c1a\u672a\u6807\u51c6\u5316\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u76d1\u7763\u4e14\u7b26\u5408\u8bef\u5dee\u4f20\u64ad\u8def\u5f84\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6821\u51c6\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u9760\u7684n\u89c6\u70b9\uff08n\u22656\uff09\u548cMVS\u9636\u6bb5\u7684\u76f8\u5173\u7ebf\u7d22\uff08\u5982\u5339\u914d\u6210\u672c\u503c\uff09\u56de\u5f52\u89c6\u5dee\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u591a\u79cd\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fb9\u754c\u7387\u4e14\u672a\u9ad8\u4f30\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6444\u5f71\u6d4b\u91cf\u70b9\u4e91\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u8ba4\u8bc1\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u573a\u666f\u3002"}}
{"id": "2507.13514", "pdf": "https://arxiv.org/pdf/2507.13514", "abs": "https://arxiv.org/abs/2507.13514", "authors": ["Bhumika Laxman Sadbhave", "Philipp Vaeth", "Denise Dejon", "Gunther Schorcht", "Magda Gregorov\u00e1"], "title": "Sugar-Beet Stress Detection using Satellite Image Time Series", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Satellite Image Time Series (SITS) data has proven effective for agricultural\ntasks due to its rich spectral and temporal nature. In this study, we tackle\nthe task of stress detection in sugar-beet fields using a fully unsupervised\napproach. We propose a 3D convolutional autoencoder model to extract meaningful\nfeatures from Sentinel-2 image sequences, combined with\nacquisition-date-specific temporal encodings to better capture the growth\ndynamics of sugar-beets. The learned representations are used in a downstream\nclustering task to separate stressed from healthy fields. The resulting stress\ndetection system can be directly applied to data from different years, offering\na practical and accessible tool for stress detection in sugar-beets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eceSentinel-2\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u68c0\u6d4b\u751c\u83dc\u7530\u7684\u80c1\u8feb\u60c5\u51b5\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\uff08SITS\uff09\u6570\u636e\u56e0\u5176\u4e30\u5bcc\u7684\u9891\u8c31\u548c\u65f6\u95f4\u7279\u6027\uff0c\u5728\u519c\u4e1a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u89e3\u51b3\u751c\u83dc\u7530\u80c1\u8feb\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u91c7\u75283D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u6a21\u578b\u63d0\u53d6Sentinel-2\u56fe\u50cf\u5e8f\u5217\u7684\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u7279\u5b9a\u91c7\u96c6\u65e5\u671f\u7684\u65f6\u95f4\u7f16\u7801\u4ee5\u6355\u6349\u751c\u83dc\u751f\u957f\u52a8\u6001\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7279\u5f81\u7528\u4e8e\u4e0b\u6e38\u805a\u7c7b\u4efb\u52a1\uff0c\u533a\u5206\u80c1\u8feb\u4e0e\u5065\u5eb7\u7530\u5757\u3002\u8be5\u7cfb\u7edf\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0d\u540c\u5e74\u4efd\u7684\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751c\u83dc\u7530\u80c1\u8feb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6613\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2507.13527", "pdf": "https://arxiv.org/pdf/2507.13527", "abs": "https://arxiv.org/abs/2507.13527", "authors": ["Levi Harris", "Md Jayed Hossain", "Mufan Qiu", "Ruichen Zhang", "Pingchuan Ma", "Tianlong Chen", "Jiaqi Gu", "Seth Ariel Tongay", "Umberto Celano"], "title": "SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "comment": null, "summary": "The increasing use of two-dimensional (2D) materials in nanoelectronics\ndemands robust metrology techniques for electrical characterization, especially\nfor large-scale production. While atomic force microscopy (AFM) techniques like\nconductive AFM (C-AFM) offer high accuracy, they suffer from slow data\nacquisition speeds due to the raster scanning process. To address this, we\nintroduce SparseC-AFM, a deep learning model that rapidly and accurately\nreconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM\nscans. Our approach is robust across various scanning modes, substrates, and\nexperimental conditions. We report a comparison between (a) classic flow\nimplementation, where a high pixel density C-AFM image (e.g., 15 minutes to\ncollect) is manually parsed to extract relevant material parameters, and (b)\nour SparseC-AFM method, which achieves the same operation using data that\nrequires substantially less acquisition time (e.g., under 5 minutes).\nSparseC-AFM enables efficient extraction of critical material parameters in\nMoS$_2$, including film coverage, defect density, and identification of\ncrystalline island boundaries, edges, and cracks. We achieve over 11x reduction\nin acquisition time compared to manual extraction from a full-resolution C-AFM\nimage. Moreover, we demonstrate that our model-predicted samples exhibit\nremarkably similar electrical properties to full-resolution data gathered using\nclassic-flow scanning. This work represents a significant step toward\ntranslating AI-assisted 2D material characterization from laboratory research\nto industrial fabrication. Code and model weights are available at\ngithub.com/UNITES-Lab/sparse-cafm.", "AI": {"tldr": "SparseC-AFM\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u7a00\u758fC-AFM\u626b\u63cf\u5feb\u901f\u91cd\u5efa2D\u6750\u6599\u7684\u5bfc\u7535\u6027\u56fe\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u91c7\u96c6\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfC-AFM\u6280\u672f\u56e0\u626b\u63cf\u901f\u5ea6\u6162\u800c\u96be\u4ee5\u6ee1\u8db3\u5927\u89c4\u6a21\u751f\u4ea7\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bSparseC-AFM\u4ece\u7a00\u758f\u626b\u63cf\u6570\u636e\u4e2d\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u5bfc\u7535\u6027\u56fe\u3002", "result": "\u5b9e\u73b011\u500d\u4ee5\u4e0a\u7684\u91c7\u96c6\u65f6\u95f4\u51cf\u5c11\uff0c\u4e14\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e0e\u4f20\u7edf\u9ad8\u5206\u8fa8\u7387\u626b\u63cf\u6570\u636e\u76f8\u4f3c\u3002", "conclusion": "SparseC-AFM\u4e3a2D\u6750\u6599\u7684\u5de5\u4e1a\u7ea7\u8868\u5f81\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13530", "pdf": "https://arxiv.org/pdf/2507.13530", "abs": "https://arxiv.org/abs/2507.13530", "authors": ["Lukas Baumg\u00e4rtner", "Ronny Bergmann", "Roland Herzog", "Stephan Schmidt", "Manuel Wei\u00df"], "title": "Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising", "categories": ["cs.CV", "math.DG", "math.OC"], "comment": null, "summary": "We propose a novel formulation for the second-order total generalized\nvariation (TGV) of the normal vector on an oriented, triangular mesh embedded\nin $\\mathbb{R}^3$. The normal vector is considered as a manifold-valued\nfunction, taking values on the unit sphere. Our formulation extends previous\ndiscrete TGV models for piecewise constant scalar data that utilize a\nRaviart-Thomas function space. To exctend this formulation to the manifold\nsetting, a tailor-made tangential Raviart-Thomas type finite element space is\nconstructed in this work. The new regularizer is compared to existing methods\nin mesh denoising experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e8c\u9636\u603b\u5e7f\u4e49\u53d8\u5206\uff08TGV\uff09\u516c\u5f0f\uff0c\u7528\u4e8e\u5904\u7406\u5d4c\u5165\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e09\u89d2\u5f62\u7f51\u683c\u4e0a\u7684\u6cd5\u5411\u91cf\u3002", "motivation": "\u6269\u5c55\u79bb\u6563TGV\u6a21\u578b\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u6d41\u5f62\u503c\u51fd\u6570\uff08\u5982\u5355\u4f4d\u7403\u9762\u4e0a\u7684\u6cd5\u5411\u91cf\uff09\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684\u5207\u5411Raviart-Thomas\u578b\u6709\u9650\u5143\u7a7a\u95f4\uff0c\u4ee5\u6269\u5c55TGV\u516c\u5f0f\u5230\u6d41\u5f62\u8bbe\u7f6e\u3002", "result": "\u5728\u7f51\u683c\u53bb\u566a\u5b9e\u9a8c\u4e2d\uff0c\u65b0\u6b63\u5219\u5316\u65b9\u6cd5\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e3a\u5904\u7406\u6d41\u5f62\u503c\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6b63\u5219\u5316\u5de5\u5177\u3002"}}
{"id": "2507.13546", "pdf": "https://arxiv.org/pdf/2507.13546", "abs": "https://arxiv.org/abs/2507.13546", "authors": ["Dmitrii Mikhailov", "Aleksey Letunovskiy", "Maria Kovaleva", "Vladimir Arkhipkin", "Vladimir Korviakov", "Vladimir Polovnikov", "Viacheslav Vasilev", "Evelina Sidorova", "Denis Dimitrov"], "title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNABLA\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u4e2d\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5168\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u6210\u4e3a\u9ad8\u5206\u8fa8\u7387\u548c\u957f\u89c6\u9891\u5e8f\u5217\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86NABLA\uff08Neighborhood Adaptive Block-Level Attention\uff09\uff0c\u5229\u7528\u5757\u7ea7\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94\u7a00\u758f\u9a71\u52a8\u9608\u503c\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u65e0\u9700\u5b9a\u5236\u4f4e\u5c42\u7b97\u5b50\u8bbe\u8ba1\uff0c\u53ef\u4e0ePyTorch\u7684Flex Attention\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNABLA\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u6bd4\u57fa\u7ebf\u5feb2.7\u500d\uff0c\u51e0\u4e4e\u4e0d\u5f71\u54cd\u5b9a\u91cf\u6307\u6807\uff08\u5982CLIP\u5206\u6570\u3001VBench\u5206\u6570\u548c\u4eba\u7c7b\u8bc4\u4f30\u5206\u6570\uff09\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "NABLA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13568", "pdf": "https://arxiv.org/pdf/2507.13568", "abs": "https://arxiv.org/abs/2507.13568", "authors": ["Kaihong Wang", "Donghyun Kim", "Margrit Betke"], "title": "LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning", "categories": ["cs.CV"], "comment": null, "summary": "Continual learning for vision-language models has achieved remarkable\nperformance through synthetic replay, where samples are generated using Stable\nDiffusion to regularize during finetuning and retain knowledge. However,\nreal-world downstream applications often exhibit domain-specific nuances and\nfine-grained semantics not captured by generators, causing synthetic-replay\nmethods to produce misaligned samples that misguide finetuning and undermine\nretention of prior knowledge. In this work, we propose a LoRA-enhanced\nsynthetic-replay framework that injects task-specific low-rank adapters into a\nfrozen Stable Diffusion model, efficiently capturing each new task's unique\nvisual and semantic patterns. Specifically, we introduce a two-stage,\nconfidence-based sample selection: we first rank real task data by\npost-finetuning VLM confidence to focus LoRA finetuning on the most\nrepresentative examples, then generate synthetic samples and again select them\nby confidence for distillation. Our approach integrates seamlessly with\nexisting replay pipelines-simply swap in the adapted generator to boost replay\nfidelity. Extensive experiments on the Multi-domain Task Incremental Learning\n(MTIL) benchmark show that our method outperforms previous synthetic-replay\ntechniques, achieving an optimal balance among plasticity, stability, and\nzero-shot capability. These results demonstrate the effectiveness of generator\nadaptation via LoRA for robust continual learning in VLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLoRA\u589e\u5f3a\u7684\u5408\u6210\u91cd\u653e\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u4f4e\u79e9\u9002\u914d\u5668\u6539\u8fdbStable Diffusion\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u5bf9\u9f50\u548c\u77e5\u8bc6\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u91cd\u653e\u65b9\u6cd5\u751f\u6210\u7684\u6837\u672c\u53ef\u80fd\u56e0\u672a\u6355\u6349\u9886\u57df\u7279\u5b9a\u7ec6\u8282\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u800c\u8bef\u5bfc\u5fae\u8c03\uff0c\u5f71\u54cd\u77e5\u8bc6\u4fdd\u7559\u3002", "method": "\u91c7\u7528LoRA\u589e\u5f3a\u7684\u5408\u6210\u91cd\u653e\u6846\u67b6\uff0c\u5206\u4e24\u9636\u6bb5\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u9009\u62e9\u6837\u672c\uff1a\u5148\u9009\u62e9\u771f\u5b9e\u4efb\u52a1\u6570\u636e\u5fae\u8c03LoRA\uff0c\u518d\u751f\u6210\u5e76\u7b5b\u9009\u5408\u6210\u6837\u672c\u7528\u4e8e\u84b8\u998f\u3002", "result": "\u5728MTIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5408\u6210\u91cd\u653e\u6280\u672f\uff0c\u5e73\u8861\u4e86\u53ef\u5851\u6027\u3001\u7a33\u5b9a\u6027\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7LoRA\u9002\u914d\u751f\u6210\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.13595", "pdf": "https://arxiv.org/pdf/2507.13595", "abs": "https://arxiv.org/abs/2507.13595", "authors": ["Tengkai Wang", "Weihao Li", "Ruikai Cui", "Shi Qiu", "Nick Barnes"], "title": "NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision", "categories": ["cs.CV"], "comment": "14 pages, 4 figures", "summary": "Reconstructing accurate implicit surface representations from point clouds\nremains a challenging task, particularly when data is captured using\nlow-quality scanning devices. These point clouds often contain substantial\nnoise, leading to inaccurate surface reconstructions. Inspired by the\nNoise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel\nmethod designed to extend this concept to 3D neural fields. Our approach\nenables learning clean neural SDFs directly from noisy point clouds through\nnoisy supervision by minimizing the MSE loss between noisy SDF representations,\nallowing the network to implicitly denoise and refine surface estimations. We\nevaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the\nShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that\nour framework significantly improves surface reconstruction quality from noisy\ninputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNoiseSDF2NoiseSDF\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u76d1\u7763\u4ece\u566a\u58f0\u70b9\u4e91\u4e2d\u5b66\u4e60\u5e72\u51c0\u7684\u795e\u7ecfSDF\uff0c\u663e\u8457\u63d0\u5347\u566a\u58f0\u8f93\u5165\u4e0b\u7684\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f4e\u8d28\u91cf\u626b\u63cf\u8bbe\u5907\u6355\u83b7\u7684\u70b9\u4e91\u5e38\u542b\u5927\u91cf\u566a\u58f0\uff0c\u5bfc\u81f4\u8868\u9762\u91cd\u5efa\u4e0d\u51c6\u786e\u3002", "method": "\u6269\u5c55Noise2Noise\u8303\u5f0f\u52303D\u795e\u7ecf\u573a\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u566a\u58f0SDF\u8868\u793a\u95f4\u7684MSE\u635f\u5931\uff0c\u9690\u5f0f\u53bb\u566a\u5e76\u4f18\u5316\u8868\u9762\u4f30\u8ba1\u3002", "result": "\u5728ShapeNet\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u8f93\u5165\u4e0b\u7684\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "NoiseSDF2NoiseSDF\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u70b9\u4e91\u4e0b\u7684\u8868\u9762\u91cd\u5efa\u95ee\u9898\u3002"}}
{"id": "2507.13599", "pdf": "https://arxiv.org/pdf/2507.13599", "abs": "https://arxiv.org/abs/2507.13599", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Since acquiring large amounts of realistic blurry-sharp image pairs is\ndifficult and expensive, learning blind image deblurring from unpaired data is\na more practical and promising solution. Unfortunately, dominant approaches\nrely heavily on adversarial learning to bridge the gap from blurry domains to\nsharp domains, ignoring the complex and unpredictable nature of real-world blur\npatterns. In this paper, we propose a novel diffusion model (DM)-based\nframework, dubbed \\ours, for image deblurring by learning spatially varying\ntexture prior from unpaired data. In particular, \\ours performs DM to generate\nthe prior knowledge that aids in recovering the textures of blurry images. To\nimplement this, we propose a Texture Prior Encoder (TPE) that introduces a\nmemory mechanism to represent the image textures and provides supervision for\nDM training. To fully exploit the generated texture priors, we present the\nTexture Transfer Transformer layer (TTformer), in which a novel\nFilter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes\nspatially varying blurring through adaptive filtering. Furthermore, we\nimplement a wavelet-based adversarial loss to preserve high-frequency texture\ndetails. Extensive evaluations show that \\ours provides a promising\nunsupervised deblurring solution and outperforms SOTA methods in widely-used\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff08\\ours\uff09\uff0c\u7528\u4e8e\u4ece\u672a\u914d\u5bf9\u6570\u636e\u4e2d\u5b66\u4e60\u56fe\u50cf\u53bb\u6a21\u7cca\uff0c\u901a\u8fc7\u751f\u6210\u7eb9\u7406\u5148\u9a8c\u77e5\u8bc6\u6765\u6062\u590d\u6a21\u7cca\u56fe\u50cf\u7684\u7eb9\u7406\u3002", "motivation": "\u7531\u4e8e\u83b7\u53d6\u5927\u91cf\u771f\u5b9e\u7684\u6a21\u7cca-\u6e05\u6670\u56fe\u50cf\u5bf9\u56f0\u96be\u4e14\u6602\u8d35\uff0c\u4ece\u672a\u914d\u5bf9\u6570\u636e\u4e2d\u5b66\u4e60\u76f2\u56fe\u50cf\u53bb\u6a21\u7cca\u66f4\u5177\u5b9e\u7528\u6027\u548c\u524d\u666f\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u6297\u5b66\u4e60\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u4e16\u754c\u6a21\u7cca\u6a21\u5f0f\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faTexture Prior Encoder\uff08TPE\uff09\u548cTexture Transfer Transformer\u5c42\uff08TTformer\uff09\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u7eb9\u7406\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u6ee4\u6ce2\u53bb\u9664\u7a7a\u95f4\u53d8\u5316\u7684\u6a21\u7cca\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\\ours\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65e0\u76d1\u7763\u53bb\u6a21\u7cca\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\\ours\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u7eb9\u7406\u5148\u9a8c\u5b66\u4e60\uff0c\u4e3a\u56fe\u50cf\u53bb\u6a21\u7cca\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6a21\u7cca\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.13607", "pdf": "https://arxiv.org/pdf/2507.13607", "abs": "https://arxiv.org/abs/2507.13607", "authors": ["Kento Kawai", "Takeru Oba", "Kyotaro Tokoro", "Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Burst Super-Resolution with One-step Diffusion", "categories": ["cs.CV"], "comment": "NTIRE2025", "summary": "While burst Low-Resolution (LR) images are useful for improving their Super\nResolution (SR) image compared to a single LR image, prior burst SR methods are\ntrained in a deterministic manner, which produces a blurry SR image. Since such\nblurry images are perceptually degraded, we aim to reconstruct sharp and\nhigh-fidelity SR images by a diffusion model. Our method improves the\nefficiency of the diffusion model with a stochastic sampler with a high-order\nODE as well as one-step diffusion using knowledge distillation. Our\nexperimental results demonstrate that our method can reduce the runtime to 1.6\n% of its baseline while maintaining the SR quality measured based on image\ndistortion and perceptual quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u968f\u673a\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u56fe\u50cf\u5e8f\u5217\u4e2d\u751f\u6210\u9ad8\u4fdd\u771f\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u56fe\u50cf\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u95f4\u5e76\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u786e\u5b9a\u6027\u8bad\u7ec3\u65b9\u6cd5\u751f\u6210\u7684SR\u56fe\u50cf\u6a21\u7cca\u4e14\u611f\u77e5\u8d28\u91cf\u5dee\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u6e05\u6670\u4e14\u9ad8\u4fdd\u771f\u7684SR\u56fe\u50cf\u3002", "method": "\u4f7f\u7528\u968f\u673a\u91c7\u6837\u5668\u7ed3\u5408\u9ad8\u9636ODE\u4ee5\u53ca\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u7684\u4e00\u6b65\u6269\u6563\uff0c\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u81f3\u57fa\u7ebf\u76841.6%\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u5931\u771f\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u4fdd\u6301\u4e86SR\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9ad8\u6548\u7684\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684SR\u56fe\u50cf\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u884c\u6548\u7387\u3002"}}
{"id": "2507.13609", "pdf": "https://arxiv.org/pdf/2507.13609", "abs": "https://arxiv.org/abs/2507.13609", "authors": ["Yanan Wang", "Julio Vizcarra", "Zhi Li", "Hao Niu", "Mori Kurokawa"], "title": "CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Despite recent progress in video large language models (VideoLLMs), a key\nopen challenge remains: how to equip models with chain-of-thought (CoT)\nreasoning abilities grounded in fine-grained object-level video understanding.\nExisting instruction-tuned models, such as the Qwen and LLaVA series, are\ntrained on high-level video-text pairs, often lacking structured annotations\nnecessary for compositional, step-by-step reasoning. We propose CoTasks:\nChain-of-Thought based Video Instruction Tuning Tasks, a new framework that\ndecomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)\ninto four entity-level foundational tasks: frame localization, entity tracking,\nspatial and temporal relation extraction. By embedding these intermediate\nCoT-style reasoning steps into the input, CoTasks enables models to explicitly\nperform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA\nbenchmark show that CoTasks significantly enhance inference performance:\nLLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and\nQwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal\n(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the\neffectiveness of CoTasks as a structured CoT-style supervision framework for\nimproving compositional video reasoning.", "AI": {"tldr": "CoTasks\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u7684\u89c6\u9891\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u89c6\u9891\u95ee\u9898\u4e3a\u56db\u4e2a\u5b9e\u4f53\u7ea7\u57fa\u7840\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u7ea7\u89c6\u9891\u7406\u89e3\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u7ed3\u6784\u5316\u6ce8\u91ca\u652f\u6301\u9010\u6b65\u63a8\u7406\u3002", "method": "CoTasks\u5c06\u590d\u6742\u89c6\u9891\u95ee\u9898\u5206\u89e3\u4e3a\u5e27\u5b9a\u4f4d\u3001\u5b9e\u4f53\u8ddf\u8e2a\u3001\u65f6\u7a7a\u5173\u7cfb\u63d0\u53d6\u56db\u4e2a\u57fa\u7840\u4efb\u52a1\uff0c\u5e76\u5c06\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u5d4c\u5165\u8f93\u5165\u3002", "result": "\u5728NeXT-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLaVA-video-7B\u548cQwen2.5-VL-3B\u5206\u522b\u63d0\u5347\u4e863.3\u548c17.4\u5206\uff0c\u5c24\u5176\u5728\u56e0\u679c\u3001\u65f6\u5e8f\u548c\u63cf\u8ff0\u6027\u5b50\u7c7b\u522b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CoTasks\u4f5c\u4e3a\u4e00\u79cd\u7ed3\u6784\u5316CoT\u76d1\u7763\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.13628", "pdf": "https://arxiv.org/pdf/2507.13628", "abs": "https://arxiv.org/abs/2507.13628", "authors": ["Masahiro Ogawa", "Qi An", "Atsushi Yamashita"], "title": "Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation", "categories": ["cs.CV"], "comment": "8 pages, 15 figures, RA-L submission", "summary": "Separating moving and static objects from a moving camera viewpoint is\nessential for 3D reconstruction, autonomous navigation, and scene understanding\nin robotics. Existing approaches often rely primarily on optical flow, which\nstruggles to detect moving objects in complex, structured scenes involving\ncamera motion. To address this limitation, we propose Focus of Expansion\nLikelihood and Segmentation (FoELS), a method based on the core idea of\nintegrating both optical flow and texture information. FoELS computes the focus\nof expansion (FoE) from optical flow and derives an initial motion likelihood\nfrom the outliers of the FoE computation. This likelihood is then fused with a\nsegmentation-based prior to estimate the final moving probability. The method\neffectively handles challenges including complex structured scenes, rotational\ncamera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016\ndataset and real-world traffic videos demonstrate its effectiveness and\nstate-of-the-art performance.", "AI": {"tldr": "FoELS\u65b9\u6cd5\u7ed3\u5408\u5149\u6d41\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u6709\u6548\u5206\u79bb\u79fb\u52a8\u548c\u9759\u6001\u7269\u4f53\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u548c\u76f8\u673a\u8fd0\u52a8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5149\u6d41\uff0c\u96be\u4ee5\u5728\u590d\u6742\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u68c0\u6d4b\u79fb\u52a8\u7269\u4f53\u3002", "method": "FoELS\u901a\u8fc7\u8ba1\u7b97\u5149\u6d41\u4e2d\u7684\u6269\u5c55\u7126\u70b9\uff08FoE\uff09\u5e76\u878d\u5408\u7eb9\u7406\u4fe1\u606f\uff0c\u751f\u6210\u8fd0\u52a8\u6982\u7387\u3002", "result": "\u5728DAVIS 2016\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "FoELS\u5728\u590d\u6742\u573a\u666f\u548c\u76f8\u673a\u8fd0\u52a8\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a3D\u91cd\u5efa\u548c\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13648", "pdf": "https://arxiv.org/pdf/2507.13648", "abs": "https://arxiv.org/abs/2507.13648", "authors": ["Seungjun Moon", "Sangjoon Yu", "Gyeong-Moon Park"], "title": "EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of neural radiance fields (NeRF) has paved the way to\ngenerate animatable human avatars from a monocular video. However, the sole\nusage of NeRF suffers from a lack of details, which results in the emergence of\nhybrid representation that utilizes SMPL-based mesh together with NeRF\nrepresentation. While hybrid-based models show photo-realistic human avatar\ngeneration qualities, they suffer from extremely slow inference due to their\ndeformation scheme: to be aligned with the mesh, hybrid-based models use the\ndeformation based on SMPL skinning weights, which needs high computational\ncosts on each sampled point. We observe that since most of the sampled points\nare located in empty space, they do not affect the generation quality but\nresult in inference latency with deformation. In light of this observation, we\npropose EPSilon, a hybrid-based 3D avatar generation scheme with novel\nefficient point sampling strategies that boost both training and inference. In\nEPSilon, we propose two methods to omit empty points at rendering; empty ray\nomission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that\nprogress through the empty space. Then, EIO narrows down the sampling interval\non the ray, which wipes out the region not occupied by either clothes or mesh.\nThe delicate sampling scheme of EPSilon enables not only great computational\ncost reduction during deformation but also the designation of the important\nregions to be sampled, which enables a single-stage NeRF structure without\nhierarchical sampling. Compared to existing methods, EPSilon maintains the\ngeneration quality while using only 3.9% of sampled points and achieves around\n20 times faster inference, together with 4 times faster training convergence.\nWe provide video results on https://github.com/seungjun-moon/epsilon.", "AI": {"tldr": "EPSilon\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6df7\u54083D\u5934\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u70b9\u91c7\u6837\u7b56\u7565\uff08ERO\u548cEIO\uff09\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eNeRF\u548cSMPL\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u5934\u50cf\u65f6\u56e0\u53d8\u5f62\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\u3002", "method": "EPSilon\u91c7\u7528\u7a7a\u5c04\u7ebf\u5ffd\u7565\uff08ERO\uff09\u548c\u7a7a\u533a\u95f4\u5ffd\u7565\uff08EIO\uff09\u7b56\u7565\uff0c\u51cf\u5c11\u65e0\u6548\u91c7\u6837\u70b9\uff0c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "EPSilon\u4ec5\u4f7f\u75283.9%\u7684\u91c7\u6837\u70b9\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea620\u500d\uff0c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u63d0\u53474\u500d\u3002", "conclusion": "EPSilon\u901a\u8fc7\u9ad8\u6548\u91c7\u6837\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.13659", "pdf": "https://arxiv.org/pdf/2507.13659", "abs": "https://arxiv.org/abs/2507.13659", "authors": ["Xiao Wang", "Qian Zhu", "Shujuan Wu", "Bo Jiang", "Shiliang Zhang", "Yaowei Wang", "Yonghong Tian", "Bin Luo"], "title": "When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Recent researchers have proposed using event cameras for person\nre-identification (ReID) due to their promising performance and better balance\nin terms of privacy protection, event camera-based person ReID has attracted\nsignificant attention. Currently, mainstream event-based person ReID algorithms\nprimarily focus on fusing visible light and event stream, as well as preserving\nprivacy. Although significant progress has been made, these methods are\ntypically trained and evaluated on small-scale or simulated event camera\ndatasets, making it difficult to assess their real identification performance\nand generalization ability. To address the issue of data scarcity, this paper\nintroduces a large-scale RGB-event based person ReID dataset, called EvReID.\nThe dataset contains 118,988 image pairs and covers 1200 pedestrian identities,\nwith data collected across multiple seasons, scenes, and lighting conditions.\nWe also evaluate 15 state-of-the-art person ReID algorithms, laying a solid\nfoundation for future research in terms of both data and benchmarking. Based on\nour newly constructed dataset, this paper further proposes a pedestrian\nattribute-guided contrastive learning framework to enhance feature learning for\nperson re-identification, termed TriPro-ReID. This framework not only\neffectively explores the visual features from both RGB frames and event\nstreams, but also fully utilizes pedestrian attributes as mid-level semantic\nfeatures. Extensive experiments on the EvReID dataset and MARS datasets fully\nvalidated the effectiveness of our proposed RGB-Event person ReID framework.\nThe benchmark dataset and source code will be released on\nhttps://github.com/Event-AHU/Neuromorphic_ReID", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5927\u89c4\u6a21RGB-\u4e8b\u4ef6\u6570\u636e\u96c6EvReID\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86TriPro-ReID\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u7279\u5f81\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u6216\u6a21\u62df\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u8bc4\u4f30\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u6784\u5efaEvReID\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faTriPro-ReID\u6846\u67b6\uff0c\u7ed3\u5408RGB\u548c\u4e8b\u4ef6\u6d41\u6570\u636e\u53ca\u884c\u4eba\u5c5e\u6027\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5728EvReID\u548cMARS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "EvReID\u6570\u636e\u96c6\u548cTriPro-ReID\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u548c\u57fa\u51c6\u652f\u6301\u3002"}}
{"id": "2507.13663", "pdf": "https://arxiv.org/pdf/2507.13663", "abs": "https://arxiv.org/abs/2507.13663", "authors": ["Xingyu Jiang", "Ning Gao", "Hongkun Dou", "Xiuhui Zhang", "Xiaoqing Zhong", "Yue Deng", "Hongjue Li"], "title": "Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Natural image quality is often degraded by adverse weather conditions,\nsignificantly impairing the performance of downstream tasks. Image restoration\nhas emerged as a core solution to this challenge and has been widely discussed\nin the literature. Although recent transformer-based approaches have made\nremarkable progress in image restoration, their increasing system complexity\nposes significant challenges for real-time processing, particularly in\nreal-world deployment scenarios. To this end, most existing methods attempt to\nsimplify the self-attention mechanism, such as by channel self-attention or\nstate space model. However, these methods primarily focus on network\narchitecture while neglecting the inherent characteristics of image restoration\nitself. In this context, we explore a pyramid Wavelet-Fourier iterative\npipeline to demonstrate the potential of Wavelet-Fourier processing for image\nrestoration. Inspired by the above findings, we propose a novel and efficient\nrestoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).\nSpecifically, PW-FNet features two key design principles: 1) at the inter-block\nlevel, integrates a pyramid wavelet-based multi-input multi-output structure to\nachieve multi-scale and multi-frequency bands decomposition; and 2) at the\nintra-block level, incorporates Fourier transforms as an efficient alternative\nto self-attention mechanisms, effectively reducing computational complexity\nwhile preserving global modeling capability. Extensive experiments on tasks\nsuch as image deraining, raindrop removal, image super-resolution, motion\ndeblurring, image dehazing, image desnowing and underwater/low-light\nenhancement demonstrate that PW-FNet not only surpasses state-of-the-art\nmethods in restoration quality but also achieves superior efficiency, with\nsignificantly reduced parameter size, computational cost and inference time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPW-FNet\u7684\u9ad8\u6548\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5c0f\u6ce2\u548c\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u4e86\u4fee\u590d\u8d28\u91cf\u3002", "motivation": "\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u81ea\u7136\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u8ba1\u7b97\u590d\u6742\uff0c\u96be\u4ee5\u5b9e\u65f6\u5904\u7406\u3002", "method": "PW-FNet\u91c7\u7528\u91d1\u5b57\u5854\u5c0f\u6ce2\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7ed3\u6784\u5b9e\u73b0\u591a\u5c3a\u5ea6\u5206\u89e3\uff0c\u5e76\u5728\u5757\u5185\u7528\u5085\u91cc\u53f6\u53d8\u6362\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPW-FNet\u5728\u591a\u79cd\u4fee\u590d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "PW-FNet\u901a\u8fc7\u5c0f\u6ce2\u548c\u5085\u91cc\u53f6\u53d8\u6362\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u4fee\u590d\u3002"}}
{"id": "2507.13673", "pdf": "https://arxiv.org/pdf/2507.13673", "abs": "https://arxiv.org/abs/2507.13673", "authors": ["Yuechen Xie", "Haobo Jiang", "Jian Yang", "Yigong Zhang", "Jin Xie"], "title": "MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training", "categories": ["cs.CV"], "comment": "10 pages, 8 figures, 6 tables", "summary": "In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of\nhands and objects from monocular RGB input remains highly challenging due to\nthe inherent geometric ambiguity of RGB images and the severe mutual occlusions\nthat occur during interaction.To address these challenges, we propose MaskHOI,\na novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI\npose estimation. Our core idea is to leverage the masking-then-reconstruction\nstrategy of MAE to encourage the feature encoder to infer missing spatial and\nstructural information, thereby facilitating geometric-aware and\nocclusion-robust representation learning. Specifically, based on our\nobservation that human hands exhibit far greater geometric complexity than\nrigid objects, conventional uniform masking fails to effectively guide the\nreconstruction of fine-grained hand structures. To overcome this limitation, we\nintroduce a Region-specific Mask Ratio Allocation, primarily comprising the\nregion-specific masking assignment and the skeleton-driven hand masking\nguidance. The former adaptively assigns lower masking ratios to hand regions\nthan to rigid objects, balancing their feature learning difficulty, while the\nlatter prioritizes masking critical hand parts (e.g., fingertips or entire\nfingers) to realistically simulate occlusion patterns in real-world\ninteractions. Furthermore, to enhance the geometric awareness of the pretrained\nencoder, we introduce a novel Masked Signed Distance Field (SDF)-driven\nmultimodal learning mechanism. Through the self-masking 3D SDF prediction, the\nlearned encoder is able to perceive the global geometric structure of hands and\nobjects beyond the 2D image plane, overcoming the inherent limitations of\nmonocular input and alleviating self-occlusion issues. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches.", "AI": {"tldr": "MaskHOI\u662f\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u53473D\u624b-\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u59ff\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\uff0c\u901a\u8fc7\u533a\u57df\u7279\u5b9a\u63a9\u7801\u5206\u914d\u548c\u9aa8\u67b6\u9a71\u52a8\u63a9\u7801\u5f15\u5bfc\u89e3\u51b3\u51e0\u4f55\u6a21\u7cca\u548c\u906e\u6321\u95ee\u9898\u3002", "motivation": "\u7531\u4e8eRGB\u56fe\u50cf\u7684\u51e0\u4f55\u6a21\u7cca\u6027\u548c\u4ea4\u4e92\u4e2d\u7684\u4e25\u91cd\u906e\u6321\uff0c\u5355\u76eeRGB\u8f93\u5165\u4e0b\u7684\u624b-\u7269\u4f53\u4ea4\u4e92\u59ff\u6001\u4f30\u8ba1\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faMaskHOI\u6846\u67b6\uff0c\u91c7\u7528\u533a\u57df\u7279\u5b9a\u63a9\u7801\u5206\u914d\u548c\u9aa8\u67b6\u9a71\u52a8\u63a9\u7801\u5f15\u5bfc\uff0c\u7ed3\u5408\u63a9\u7801\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u9a71\u52a8\u7684\u591a\u6a21\u6001\u5b66\u4e60\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MaskHOI\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u548c\u906e\u6321\u9c81\u68d2\u7684\u7279\u5f81\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u624b-\u7269\u4f53\u4ea4\u4e92\u59ff\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2507.13677", "pdf": "https://arxiv.org/pdf/2507.13677", "abs": "https://arxiv.org/abs/2507.13677", "authors": ["Chuheng Wei", "Ziye Qin", "Walter Zimmer", "Guoyuan Wu", "Matthew J. Barth"], "title": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Ranked first in CVPR DriveX workshop TUM-Traf V2X challenge. Accepted\n  by ITSC2025", "summary": "Real-world Vehicle-to-Everything (V2X) cooperative perception systems often\noperate under heterogeneous sensor configurations due to cost constraints and\ndeployment variability across vehicles and infrastructure. This heterogeneity\nposes significant challenges for feature fusion and perception reliability. To\naddress these issues, we propose HeCoFuse, a unified framework designed for\ncooperative perception across mixed sensor setups where nodes may carry Cameras\n(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that\nadaptively weights features through a combination of channel-wise and spatial\nattention, HeCoFuse can tackle critical challenges such as cross-modality\nfeature misalignment and imbalanced representation quality. In addition, an\nadaptive spatial resolution adjustment module is employed to balance\ncomputational cost and fusion effectiveness. To enhance robustness across\ndifferent configurations, we further implement a cooperative learning strategy\nthat dynamically adjusts fusion type based on available modalities. Experiments\non the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%\n3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D\nbaseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC\nscenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine\nheterogeneous sensor configurations. These results, validated by our\nfirst-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the\ncurrent state-of-the-art on TUM-Traf V2X dataset while demonstrating robust\nperformance across diverse sensor deployments.", "AI": {"tldr": "HeCoFuse\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6784\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u7684V2X\u534f\u540c\u611f\u77e5\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5c42\u878d\u5408\u673a\u5236\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684V2X\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u56e0\u4f20\u611f\u5668\u914d\u7f6e\u7684\u5f02\u6784\u6027\u5bfc\u81f4\u7279\u5f81\u878d\u5408\u548c\u611f\u77e5\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faHeCoFuse\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u878d\u5408\u673a\u5236\uff08\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\uff09\u548c\u81ea\u9002\u5e94\u7a7a\u95f4\u5206\u8fa8\u7387\u8c03\u6574\u6a21\u5757\uff0c\u7ed3\u5408\u52a8\u6001\u878d\u5408\u7c7b\u578b\u8c03\u6574\u7b56\u7565\u3002", "result": "\u5728TUMTraf-V2X\u6570\u636e\u96c6\u4e0a\uff0cHeCoFuse\u5728\u591a\u79cd\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c3D mAP\u6700\u9ad8\u8fbe43.38%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HeCoFuse\u5728\u5f02\u6784\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u6210\u4e3a\u5f53\u524dTUM-Traf V2X\u6570\u636e\u96c6\u4e0a\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.13693", "pdf": "https://arxiv.org/pdf/2507.13693", "abs": "https://arxiv.org/abs/2507.13693", "authors": ["Hongyi Liu", "Haifeng Wang"], "title": "Gaussian kernel-based motion measurement", "categories": ["cs.CV"], "comment": null, "summary": "The growing demand for structural health monitoring has driven increasing\ninterest in high-precision motion measurement, as structural information\nderived from extracted motions can effectively reflect the current condition of\nthe structure. Among various motion measurement techniques, vision-based\nmethods stand out due to their low cost, easy installation, and large-scale\nmeasurement. However, when it comes to sub-pixel-level motion measurement,\ncurrent vision-based methods either lack sufficient accuracy or require\nextensive manual parameter tuning (e.g., pyramid layers, target pixels, and\nfilter parameters) to reach good precision. To address this issue, we developed\na novel Gaussian kernel-based motion measurement method, which can extract the\nmotion between different frames via tracking the location of Gaussian kernels.\nThe motion consistency, which fits practical structural conditions, and a\nsuper-resolution constraint, are introduced to increase accuracy and robustness\nof our method. Numerical and experimental validations show that it can\nconsistently reach high accuracy without customized parameter setup for\ndifferent test samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6838\u7684\u8fd0\u52a8\u6d4b\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u5728\u4e9a\u50cf\u7d20\u7ea7\u8fd0\u52a8\u6d4b\u91cf\u4e2d\u7cbe\u5ea6\u4e0d\u8db3\u6216\u53c2\u6570\u8c03\u4f18\u590d\u6742\u7684\u95ee\u9898\u3002", "motivation": "\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u5bf9\u9ad8\u7cbe\u5ea6\u8fd0\u52a8\u6d4b\u91cf\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u5728\u4e9a\u50cf\u7d20\u7ea7\u6d4b\u91cf\u4e2d\u7cbe\u5ea6\u4e0d\u8db3\u6216\u9700\u590d\u6742\u53c2\u6570\u8c03\u4f18\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6838\u7684\u8fd0\u52a8\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u9ad8\u65af\u6838\u4f4d\u7f6e\u63d0\u53d6\u5e27\u95f4\u8fd0\u52a8\uff0c\u5f15\u5165\u8fd0\u52a8\u4e00\u81f4\u6027\u548c\u8d85\u5206\u8fa8\u7387\u7ea6\u675f\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "result": "\u6570\u503c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u6837\u672c\u5b9a\u5236\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u3001\u65e0\u9700\u590d\u6742\u53c2\u6570\u8c03\u4f18\u7684\u8fd0\u52a8\u6d4b\u91cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13706", "pdf": "https://arxiv.org/pdf/2507.13706", "abs": "https://arxiv.org/abs/2507.13706", "authors": ["\u00c1ngel F. Garc\u00eda-Fern\u00e1ndez", "Jinhao Gu", "Lennart Svensson", "Yuxuan Xia", "Jan Krej\u010d\u00ed", "Oliver Kost", "Ond\u0159ej Straka"], "title": "GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms", "categories": ["cs.CV", "math.ST", "stat.TH"], "comment": null, "summary": "This paper introduces two quasi-metrics for performance assessment of\nmulti-object tracking (MOT) algorithms. In particular, one quasi-metric is an\nextension of the generalised optimal subpattern assignment (GOSPA) metric and\nmeasures the discrepancy between sets of objects. The other quasi-metric is an\nextension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy\nbetween sets of trajectories. Similar to the GOSPA-based metrics, these\nquasi-metrics include costs for localisation error for properly detected\nobjects, the number of false objects and the number of missed objects. The\nT-GOSPA quasi-metric also includes a track switching cost. Differently from the\nGOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of\npenalising missed and false objects with different costs, and the localisation\ncosts are not required to be symmetric. These properties can be useful in MOT\nevaluation in certain applications. The performance of several Bayesian MOT\nalgorithms is assessed with the T-GOSPA quasi-metric via simulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7528\u4e8e\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u7b97\u6cd5\u6027\u80fd\u8bc4\u4f30\u7684\u51c6\u5ea6\u91cf\uff0c\u5206\u522b\u6269\u5c55\u4e86GOSPA\u548cT-GOSPA\u5ea6\u91cf\uff0c\u5177\u6709\u7075\u6d3b\u7684\u975e\u5bf9\u79f0\u60e9\u7f5a\u6210\u672c\u3002", "motivation": "\u73b0\u6709GOSPA\u548cT-GOSPA\u5ea6\u91cf\u5728\u8bc4\u4f30MOT\u7b97\u6cd5\u65f6\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6839\u636e\u5e94\u7528\u9700\u6c42\u8c03\u6574\u60e9\u7f5a\u6210\u672c\u3002", "method": "\u6269\u5c55GOSPA\u548cT-GOSPA\u5ea6\u91cf\uff0c\u5f15\u5165\u975e\u5bf9\u79f0\u60e9\u7f5a\u6210\u672c\u548c\u5c40\u90e8\u5316\u8bef\u5dee\u6210\u672c\uff0c\u5e76\u589e\u52a0\u8f68\u8ff9\u5207\u6362\u6210\u672c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86T-GOSPA\u51c6\u5ea6\u91cf\u5728\u8bc4\u4f30\u8d1d\u53f6\u65afMOT\u7b97\u6cd5\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u51c6\u5ea6\u91cf\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684MOT\u8bc4\u4f30\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.13708", "pdf": "https://arxiv.org/pdf/2507.13708", "abs": "https://arxiv.org/abs/2507.13708", "authors": ["Sofia Jamil", "Bollampalli Areen Reddy", "Raghvendra Kumar", "Sriparna Saha", "Koustava Goswami", "K. J. Joseph"], "title": "PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement", "categories": ["cs.CV"], "comment": "ECAI 2025", "summary": "Recent advancements in text-to-image diffusion models have achieved\nremarkable success in generating realistic and diverse visual content. A\ncritical factor in this process is the model's ability to accurately interpret\ntextual prompts. However, these models often struggle with creative\nexpressions, particularly those involving complex, abstract, or highly\ndescriptive language. In this work, we introduce a novel training-free approach\ntailored to improve image generation for a unique form of creative language:\npoetic verse, which frequently features layered, abstract, and dual meanings.\nOur proposed PoemTale Diffusion approach aims to minimise the information that\nis lost during poetic text-to-image conversion by integrating a multi stage\nprompt refinement loop into Language Models to enhance the interpretability of\npoetic texts. To support this, we adapt existing state-of-the-art diffusion\nmodels by modifying their self-attention mechanisms with a consistent\nself-attention technique to generate multiple consistent images, which are then\ncollectively used to convey the poem's meaning. Moreover, to encourage research\nin the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting\nof 1111 poems sourced from multiple online and offline resources. We engaged a\npanel of poetry experts for qualitative assessments. The results from both\nhuman and quantitative evaluations validate the efficacy of our method and\ncontribute a novel perspective to poem-to-image generation with enhanced\ninformation capture in the generated images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5PoemTale Diffusion\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63d0\u793a\u4f18\u5316\u5faa\u73af\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u8bd7\u6b4c\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u3001\u62bd\u8c61\u7684\u8bd7\u6b4c\u8bed\u8a00\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u7ed3\u5408\u591a\u9636\u6bb5\u63d0\u793a\u4f18\u5316\u5faa\u73af\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u751f\u6210\u591a\u5f20\u4e00\u81f4\u56fe\u50cf\u4ee5\u4f20\u8fbe\u8bd7\u6b4c\u542b\u4e49\uff0c\u5e76\u521b\u5efaP4I\u6570\u636e\u96c6\u652f\u6301\u7814\u7a76\u3002", "result": "\u4eba\u7c7b\u548c\u5b9a\u91cf\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u56fe\u50cf\u80fd\u66f4\u597d\u5730\u6355\u6349\u8bd7\u6b4c\u4fe1\u606f\u3002", "conclusion": "PoemTale Diffusion\u4e3a\u8bd7\u6b4c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u4fdd\u7559\u80fd\u529b\u3002"}}
{"id": "2507.13719", "pdf": "https://arxiv.org/pdf/2507.13719", "abs": "https://arxiv.org/abs/2507.13719", "authors": ["Daniele Pannone", "Alessia Castronovo", "Maurizio Mancini", "Gian Luca Foresti", "Claudio Piciarelli", "Rossana Gabrieli", "Muhammad Yasir Bilal", "Danilo Avola"], "title": "Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an innovative augmented reality pipeline tailored for\nmuseum environments, aimed at recognizing artworks and generating accurate 3D\nmodels from single images. By integrating two complementary pre-trained depth\nestimation models, i.e., GLPN for capturing global scene structure and\nDepth-Anything for detailed local reconstruction, the proposed approach\nproduces optimized depth maps that effectively represent complex artistic\nfeatures. These maps are then converted into high-quality point clouds and\nmeshes, enabling the creation of immersive AR experiences. The methodology\nleverages state-of-the-art neural network architectures and advanced computer\nvision techniques to overcome challenges posed by irregular contours and\nvariable textures in artworks. Experimental results demonstrate significant\nimprovements in reconstruction accuracy and visual realism, making the system a\nhighly robust tool for museums seeking to enhance visitor engagement through\ninteractive digital content.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u535a\u7269\u9986\u73af\u5883\u7684\u589e\u5f3a\u73b0\u5b9e\u6d41\u7a0b\uff0c\u901a\u8fc7\u5355\u5f20\u56fe\u50cf\u8bc6\u522b\u827a\u672f\u54c1\u5e76\u751f\u6210\u7cbe\u786e3D\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u827a\u672f\u54c1\u590d\u6742\u8f6e\u5ed3\u548c\u7eb9\u7406\u5e26\u6765\u7684\u91cd\u5efa\u6311\u6218\uff0c\u63d0\u5347\u535a\u7269\u9986\u7684\u4e92\u52a8\u4f53\u9a8c\u3002", "method": "\u7ed3\u5408GLPN\u548cDepth-Anything\u4e24\u79cd\u9884\u8bad\u7ec3\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u751f\u6210\u4f18\u5316\u7684\u6df1\u5ea6\u56fe\u5e76\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u70b9\u4e91\u548c\u7f51\u683c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u535a\u7269\u9986\u63d0\u4f9b\u4e86\u4e00\u79cd\u589e\u5f3a\u8bbf\u5ba2\u4e92\u52a8\u4f53\u9a8c\u7684\u5f3a\u5065\u5de5\u5177\u3002"}}
{"id": "2507.13722", "pdf": "https://arxiv.org/pdf/2507.13722", "abs": "https://arxiv.org/abs/2507.13722", "authors": ["Julia Laubmann", "Johannes Reschke"], "title": "Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "In today's digital age, concerns about the dangers of AI-generated images are\nincreasingly common. One powerful tool in this domain is StyleGAN (style-based\ngenerative adversarial networks), a generative adversarial network capable of\nproducing highly realistic synthetic faces. To gain a deeper understanding of\nhow such a model operates, this work focuses on analyzing the inner workings of\nStyleGAN's generator component. Key architectural elements and techniques, such\nas the Equalized Learning Rate, are explored in detail to shed light on the\nmodel's behavior. A StyleGAN model is trained using the PyTorch framework,\nenabling direct inspection of its learned weights. Through pruning, it is\nrevealed that a significant number of these weights can be removed without\ndrastically affecting the output, leading to reduced computational\nrequirements. Moreover, the role of the latent vector -- which heavily\ninfluences the appearance of the generated faces -- is closely examined. Global\nalterations to this vector primarily affect aspects like color tones, while\ntargeted changes to individual dimensions allow for precise manipulation of\nspecific facial features. This ability to finetune visual traits is not only of\nacademic interest but also highlights a serious ethical concern: the potential\nmisuse of such technology. Malicious actors could exploit this capability to\nfabricate convincing fake identities, posing significant risks in the context\nof digital deception and cybercrime.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86StyleGAN\u751f\u6210\u5668\u7684\u5185\u90e8\u673a\u5236\uff0c\u63a2\u8ba8\u4e86\u5176\u5173\u952e\u6280\u672f\u548c\u6743\u91cd\u4fee\u526a\u7684\u6548\u679c\uff0c\u63ed\u793a\u4e86\u6f5c\u5728\u5411\u91cf\u7684\u4f5c\u7528\u53ca\u5176\u4f26\u7406\u98ce\u9669\u3002", "motivation": "\u7814\u7a76StyleGAN\u751f\u6210\u5668\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u4ee5\u7406\u89e3\u5176\u5982\u4f55\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u7684\u5408\u6210\u4eba\u8138\uff0c\u5e76\u63a2\u7d22\u5176\u6f5c\u5728\u7684\u6280\u672f\u548c\u4f26\u7406\u95ee\u9898\u3002", "method": "\u4f7f\u7528PyTorch\u6846\u67b6\u8bad\u7ec3StyleGAN\u6a21\u578b\uff0c\u901a\u8fc7\u6743\u91cd\u4fee\u526a\u548c\u6f5c\u5728\u5411\u91cf\u5206\u6790\uff0c\u7814\u7a76\u5176\u5185\u90e8\u673a\u5236\u3002", "result": "\u53d1\u73b0\u5927\u91cf\u6743\u91cd\u53ef\u4fee\u526a\u800c\u4e0d\u663e\u8457\u5f71\u54cd\u8f93\u51fa\uff0c\u6f5c\u5728\u5411\u91cf\u80fd\u7cbe\u786e\u63a7\u5236\u9762\u90e8\u7279\u5f81\uff0c\u4f46\u4e5f\u5b58\u5728\u88ab\u6ee5\u7528\u7684\u98ce\u9669\u3002", "conclusion": "StyleGAN\u7684\u6280\u672f\u80fd\u529b\u5177\u6709\u5b66\u672f\u4ef7\u503c\uff0c\u4f46\u5176\u6f5c\u5728\u7684\u6ee5\u7528\u98ce\u9669\u9700\u5f15\u8d77\u4f26\u7406\u5173\u6ce8\u3002"}}
{"id": "2507.13739", "pdf": "https://arxiv.org/pdf/2507.13739", "abs": "https://arxiv.org/abs/2507.13739", "authors": ["Junsu Kim", "Yunhoe Ku", "Seungryul Baek"], "title": "Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning", "categories": ["cs.CV", "cs.AI"], "comment": "6th CLVISION ICCV Workshop accepted", "summary": "Few-shot class-incremental learning (FSCIL) is challenging due to extremely\nlimited training data; while aiming to reduce catastrophic forgetting and learn\nnew information. We propose Diffusion-FSCIL, a novel approach that employs a\ntext-to-image diffusion model as a frozen backbone. Our conjecture is that\nFSCIL can be tackled using a large generative model's capabilities benefiting\nfrom 1) generation ability via large-scale pre-training; 2) multi-scale\nrepresentation; 3) representational flexibility through the text encoder. To\nmaximize the representation capability, we propose to extract multiple\ncomplementary diffusion features to play roles as latent replay with slight\nsupport from feature distillation for preventing generative biases. Our\nframework realizes efficiency through 1) using a frozen backbone; 2) minimal\ntrainable components; 3) batch processing of multiple feature extractions.\nExtensive experiments on CUB-200, \\emph{mini}ImageNet, and CIFAR-100 show that\nDiffusion-FSCIL surpasses state-of-the-art methods, preserving performance on\npreviously learned classes and adapting effectively to new ones.", "AI": {"tldr": "Diffusion-FSCIL\u5229\u7528\u51bb\u7ed3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u6f5c\u5728\u91cd\u653e\u89e3\u51b3\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u6311\u6218\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u9762\u4e34\u6570\u636e\u6709\u9650\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5b66\u4e60\u65b0\u4fe1\u606f\u5e76\u4fdd\u7559\u65e7\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u51bb\u7ed3\u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u4f5c\u4e3a\u6f5c\u5728\u91cd\u653e\uff0c\u8f85\u4ee5\u7279\u5f81\u84b8\u998f\u51cf\u5c11\u751f\u6210\u504f\u5dee\uff0c\u4ec5\u9700\u5c11\u91cf\u53ef\u8bad\u7ec3\u7ec4\u4ef6\u3002", "result": "\u5728CUB-200\u3001miniImageNet\u548cCIFAR-100\u4e0a\uff0cDiffusion-FSCIL\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u4fdd\u7559\u65e7\u7c7b\u6027\u80fd\u5e76\u9002\u5e94\u65b0\u7c7b\u3002", "conclusion": "Diffusion-FSCIL\u901a\u8fc7\u51bb\u7ed3\u9aa8\u5e72\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u3002"}}
{"id": "2507.13753", "pdf": "https://arxiv.org/pdf/2507.13753", "abs": "https://arxiv.org/abs/2507.13753", "authors": ["Tongtong Su", "Chengyu Wang", "Bingyan Liu", "Jun Huang", "Dongming Lu"], "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, large text-to-video (T2V) synthesis models have garnered\nconsiderable attention for their abilities to generate videos from textual\ndescriptions. However, achieving both high imaging quality and effective motion\nrepresentation remains a significant challenge for these T2V models. Existing\napproaches often adapt pre-trained text-to-image (T2I) models to refine video\nframes, leading to issues such as flickering and artifacts due to\ninconsistencies across frames. In this paper, we introduce EVS, a training-free\nEncapsulated Video Synthesizer that composes T2I and T2V models to enhance both\nvisual fidelity and motion smoothness of generated videos. Our approach\nutilizes a well-trained diffusion-based T2I model to refine low-quality video\nframes by treating them as out-of-distribution samples, effectively optimizing\nthem with noising and denoising steps. Meanwhile, we employ T2V backbones to\nensure consistent motion dynamics. By encapsulating the T2V temporal-only prior\ninto the T2I generation process, EVS successfully leverages the strengths of\nboth types of models, resulting in videos of improved imaging and motion\nquality. Experimental results validate the effectiveness of our approach\ncompared to previous approaches. Our composition process also leads to a\nsignificant improvement of 1.6x-4.5x speedup in inference time. Source codes:\nhttps://github.com/Tonniia/EVS.", "AI": {"tldr": "EVS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5c01\u88c5\u89c6\u9891\u5408\u6210\u5668\uff0c\u7ed3\u5408T2I\u548cT2V\u6a21\u578b\uff0c\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\u3002", "motivation": "\u73b0\u6709T2V\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u65f6\u5b58\u5728\u753b\u9762\u95ea\u70c1\u548c\u4f2a\u5f71\u95ee\u9898\uff0cEVS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684T2I\u6a21\u578b\u4f18\u5316\u4f4e\u8d28\u91cf\u89c6\u9891\u5e27\uff0c\u540c\u65f6\u4f7f\u7528T2V\u6a21\u578b\u786e\u4fdd\u8fd0\u52a8\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eEVS\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.6-4.5\u500d\u3002", "conclusion": "EVS\u901a\u8fc7\u7ed3\u5408T2I\u548cT2V\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.13769", "pdf": "https://arxiv.org/pdf/2507.13769", "abs": "https://arxiv.org/abs/2507.13769", "authors": ["Mingyang Yu", "Zhijian Wu", "Dingjiang Huang"], "title": "Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its\ndegraded 2D measurements. Recently great progress has been made in deep\nlearning-based methods, however, these methods often struggle to accurately\ncapture high-frequency details of the HSI. To address this issue, this paper\nproposes a Spectral Diffusion Prior (SDP) that is implicitly learned from\nhyperspectral images using a diffusion model. Leveraging the powerful ability\nof the diffusion model to reconstruct details, this learned prior can\nsignificantly improve the performance when injected into the HSI model. To\nfurther improve the effectiveness of the learned prior, we also propose the\nSpectral Prior Injector Module (SPIM) to dynamically guide the model to recover\nthe HSI details. We evaluate our method on two representative HSI methods: MST\nand BISRNet. Experimental results show that our method outperforms existing\nnetworks by about 0.5 dB, effectively improving the performance of HSI\nreconstruction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5149\u8c31\u6269\u6563\u5148\u9a8c\uff08SDP\uff09\u548c\u5149\u8c31\u5148\u9a8c\u6ce8\u5165\u6a21\u5757\uff08SPIM\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u91cd\u5efa\u7684\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728HSI\u91cd\u5efa\u4e2d\u96be\u4ee5\u51c6\u786e\u6355\u6349\u9ad8\u9891\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u6269\u6563\u6a21\u578b\u9690\u5f0f\u5b66\u4e60HSI\u7684\u5149\u8c31\u6269\u6563\u5148\u9a8c\uff08SDP\uff09\uff0c\u5e76\u8bbe\u8ba1\u5149\u8c31\u5148\u9a8c\u6ce8\u5165\u6a21\u5757\uff08SPIM\uff09\u52a8\u6001\u6307\u5bfc\u6a21\u578b\u6062\u590d\u7ec6\u8282\u3002", "result": "\u5728MST\u548cBISRNet\u4e24\u79cdHSI\u65b9\u6cd5\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7f51\u7edc\u7ea60.5 dB\u3002", "conclusion": "\u63d0\u51fa\u7684SDP\u548cSPIM\u663e\u8457\u63d0\u5347\u4e86HSI\u91cd\u5efa\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u65b9\u9762\u3002"}}
{"id": "2507.13772", "pdf": "https://arxiv.org/pdf/2507.13772", "abs": "https://arxiv.org/abs/2507.13772", "authors": ["Abhijit Sen", "Giridas Maiti", "Bikram K. Parida", "Bhanu P. Mishra", "Mahima Arya", "Denys I. Bondar"], "title": "Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Feature engineering continues to play a critical role in image\nclassification, particularly when interpretability and computational efficiency\nare prioritized over deep learning models with millions of parameters. In this\nstudy, we revisit classical machine learning based image classification through\na novel approach centered on Permutation Entropy (PE), a robust and\ncomputationally lightweight measure traditionally used in time series analysis\nbut rarely applied to image data. We extend PE to two-dimensional images and\npropose a multiscale, multi-orientation entropy-based feature extraction\napproach that characterizes spatial order and complexity along rows, columns,\ndiagonals, anti-diagonals, and local patches of the image. To enhance the\ndiscriminatory power of the entropy features, we integrate two classic image\ndescriptors: the Histogram of Oriented Gradients (HOG) to capture shape and\nedge structure, and Local Binary Patterns (LBP) to encode micro-texture of an\nimage. The resulting hand-crafted feature set, comprising of 780 dimensions, is\nused to train Support Vector Machine (SVM) classifiers optimized through grid\nsearch. The proposed approach is evaluated on multiple benchmark datasets,\nincluding Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers\ncompetitive classification performance without relying on deep architectures.\nOur results demonstrate that the fusion of PE with HOG and LBP provides a\ncompact, interpretable, and effective alternative to computationally expensive\nand limited interpretable deep learning models. This shows a potential of\nentropy-based descriptors in image classification and contributes a lightweight\nand generalizable solution to interpretable machine learning in image\nclassification and computer vision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6392\u5217\u71b5\uff08PE\uff09\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408HOG\u548cLBP\u7279\u5f81\uff0c\u8bad\u7ec3SVM\u5206\u7c7b\u5668\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u7279\u5f81\u5de5\u7a0b\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4ecd\u5177\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\u3002", "method": "\u6269\u5c55PE\u81f3\u4e8c\u7ef4\u56fe\u50cf\uff0c\u63d0\u51fa\u591a\u5c3a\u5ea6\u3001\u591a\u65b9\u5411\u7684\u71b5\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u7ed3\u5408HOG\u548cLBP\u7279\u5f81\uff0c\u8bad\u7ec3SVM\u5206\u7c7b\u5668\u3002", "result": "\u5728Fashion-MNIST\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u71b5\u7279\u5f81\u4e0e\u7ecf\u5178\u63cf\u8ff0\u7b26\u7ed3\u5408\uff0c\u4e3a\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13773", "pdf": "https://arxiv.org/pdf/2507.13773", "abs": "https://arxiv.org/abs/2507.13773", "authors": ["Pu Jian", "Donglei Yu", "Wen Yang", "Shuo Ren", "Jiajun Zhang"], "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions", "categories": ["cs.CV", "cs.CL"], "comment": "ACL2025 Main", "summary": "In visual question answering (VQA) context, users often pose ambiguous\nquestions to visual language models (VLMs) due to varying expression habits.\nExisting research addresses such ambiguities primarily by rephrasing questions.\nThese approaches neglect the inherently interactive nature of user interactions\nwith VLMs, where ambiguities can be clarified through user feedback. However,\nresearch on interactive clarification faces two major challenges: (1)\nBenchmarks are absent to assess VLMs' capacity for resolving ambiguities\nthrough interaction; (2) VLMs are trained to prefer answering rather than\nasking, preventing them from seeking clarification. To overcome these\nchallenges, we introduce \\textbf{ClearVQA} benchmark, which targets three\ncommon categories of ambiguity in VQA context, and encompasses various VQA\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faClearVQA\u57fa\u51c6\uff0c\u89e3\u51b3\u89c6\u89c9\u95ee\u7b54\u4e2d\u7528\u6237\u6a21\u7cca\u95ee\u9898\u7684\u4ea4\u4e92\u5f0f\u6f84\u6e05\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u8fc7\u91cd\u8ff0\u95ee\u9898\u89e3\u51b3\u6a21\u7cca\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u7528\u6237\u53cd\u9988\u7684\u4ea4\u4e92\u6027\u3002\u7f3a\u4e4f\u8bc4\u4f30\u4ea4\u4e92\u6f84\u6e05\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u4e14\u6a21\u578b\u503e\u5411\u4e8e\u56de\u7b54\u800c\u975e\u63d0\u95ee\u3002", "method": "\u5f15\u5165ClearVQA\u57fa\u51c6\uff0c\u9488\u5bf9\u4e09\u7c7b\u5e38\u89c1\u6a21\u7cca\u95ee\u9898\uff0c\u6db5\u76d6\u591a\u79cdVQA\u573a\u666f\u3002", "result": "ClearVQA\u57fa\u51c6\u586b\u8865\u4e86\u4ea4\u4e92\u5f0f\u6f84\u6e05\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "conclusion": "ClearVQA\u4e3a\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u4ea4\u4e92\u5f0f\u6f84\u6e05\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2507.13779", "pdf": "https://arxiv.org/pdf/2507.13779", "abs": "https://arxiv.org/abs/2507.13779", "authors": ["Durgesh Singh", "Ahc\u00e8ne Boubekki", "Robert Jenssen", "Michael Kampffmeyer"], "title": "SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering", "categories": ["cs.CV"], "comment": null, "summary": "Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA)\nenhance the model performance by exploiting information from labeled and\nunlabeled data. The clustering assumption has proven advantageous for learning\nwith limited supervision and states that data points belonging to the same\ncluster in a high-dimensional space should be assigned to the same category.\nRecent works have utilized different training mechanisms to implicitly enforce\nthis assumption for the SSL and UDA. In this work, we take a different approach\nby explicitly involving a differentiable clustering module which is extended to\nleverage the supervised data to compute its centroids. We demonstrate the\neffectiveness of our straightforward end-to-end training strategy for SSL and\nUDA over extensive experiments and highlight its benefits, especially in low\nsupervision regimes, both as a standalone model and as a regularizer for\nexisting approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\uff0c\u7528\u4e8e\u534a\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff0c\u901a\u8fc7\u5229\u7528\u76d1\u7763\u6570\u636e\u8ba1\u7b97\u805a\u7c7b\u4e2d\u5fc3\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9690\u5f0f\u5730\u5229\u7528\u805a\u7c7b\u5047\u8bbe\uff0c\u800c\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u663e\u5f0f\u5f15\u5165\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\uff0c\u66f4\u76f4\u63a5\u5730\u5229\u7528\u76d1\u7763\u6570\u636e\u4f18\u5316\u805a\u7c7b\u4e2d\u5fc3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\uff0c\u663e\u5f0f\u5229\u7528\u76d1\u7763\u6570\u636e\u8ba1\u7b97\u805a\u7c7b\u4e2d\u5fc3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u534a\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u4f4e\u76d1\u7763\u6761\u4ef6\u4e0b\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u663e\u5f0f\u5f15\u5165\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u7b56\u7565\uff0c\u53ef\u4f5c\u4e3a\u72ec\u7acb\u6a21\u578b\u6216\u73b0\u6709\u65b9\u6cd5\u7684\u6b63\u5219\u5316\u5668\u3002"}}
{"id": "2507.13789", "pdf": "https://arxiv.org/pdf/2507.13789", "abs": "https://arxiv.org/abs/2507.13789", "authors": ["Kyriakos Flouris", "Moritz Halter", "Yolanne Y. R. Lee", "Samuel Castonguay", "Luuk Jacobs", "Pietro Dirix", "Jonathan Nestmann", "Sebastian Kozerke", "Ender Konukoglu"], "title": "Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI", "categories": ["cs.CV", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "Hemodynamic analysis is essential for predicting aneurysm rupture and guiding\ntreatment. While magnetic resonance flow imaging enables time-resolved\nvolumetric blood velocity measurements, its low spatiotemporal resolution and\nsignal-to-noise ratio limit its diagnostic utility. To address this, we propose\nthe Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that\nenhances both spatial and temporal resolution with the ability to predict wall\nshear stress (WSS) directly from clinical imaging data. LoFNO integrates\nLaplacian eigenvectors as geometric priors for improved structural awareness on\nirregular, unseen geometries and employs an Enhanced Deep Super-Resolution\nNetwork (EDSR) layer for robust upsampling. By combining geometric priors with\nneural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow\ndata, achieving superior velocity and WSS predictions compared to interpolation\nand alternative deep learning methods, enabling more precise cerebrovascular\ndiagnostics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoFNO\u76843D\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u63d0\u5347\u8840\u6d41\u6570\u636e\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\uff0c\u76f4\u63a5\u9884\u6d4b\u58c1\u9762\u526a\u5207\u5e94\u529b\uff08WSS\uff09\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u78c1\u5171\u632f\u8840\u6d41\u6210\u50cf\u7684\u4f4e\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u4fe1\u566a\u6bd4\u9650\u5236\u4e86\u5176\u8bca\u65ad\u4ef7\u503c\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u5206\u8fa8\u7387\u548c\u9884\u6d4b\u80fd\u529b\u3002", "method": "LoFNO\u7ed3\u5408\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u91c7\u7528EDSR\u5c42\u8fdb\u884c\u4e0a\u91c7\u6837\uff0c\u76f4\u63a5\u4ece\u4e34\u5e8a\u5f71\u50cf\u6570\u636e\u9884\u6d4bWSS\u3002", "result": "LoFNO\u5728\u901f\u5ea6\u548cWSS\u9884\u6d4b\u4e0a\u4f18\u4e8e\u63d2\u503c\u548c\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u8111\u8840\u7ba1\u8bca\u65ad\u7684\u7cbe\u786e\u6027\u3002", "conclusion": "LoFNO\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8840\u6d41\u6570\u636e\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u8bca\u65ad\u80fd\u529b\u3002"}}
{"id": "2507.13797", "pdf": "https://arxiv.org/pdf/2507.13797", "abs": "https://arxiv.org/abs/2507.13797", "authors": ["Huu-Phu Do", "Yu-Wei Chen", "Yi-Cheng Liao", "Chi-Wei Hsiao", "Han-Yang Wang", "Wei-Chen Chiu", "Ching-Chun Huang"], "title": "DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Blind Face Restoration aims to recover high-fidelity, detail-rich facial\nimages from unknown degraded inputs, presenting significant challenges in\npreserving both identity and detail. Pre-trained diffusion models have been\nincreasingly used as image priors to generate fine details. Still, existing\nmethods often use fixed diffusion sampling timesteps and a global guidance\nscale, assuming uniform degradation. This limitation and potentially imperfect\ndegradation kernel estimation frequently lead to under- or over-diffusion,\nresulting in an imbalance between fidelity and quality. We propose\nDynFaceRestore, a novel blind face restoration approach that learns to map any\nblindly degraded input to Gaussian blurry images. By leveraging these blurry\nimages and their respective Gaussian kernels, we dynamically select the\nstarting timesteps for each blurry image and apply closed-form guidance during\nthe diffusion sampling process to maintain fidelity. Additionally, we introduce\na dynamic guidance scaling adjuster that modulates the guidance strength across\nlocal regions, enhancing detail generation in complex areas while preserving\nstructural fidelity in contours. This strategy effectively balances the\ntrade-off between fidelity and quality. DynFaceRestore achieves\nstate-of-the-art performance in both quantitative and qualitative evaluations,\ndemonstrating robustness and effectiveness in blind face restoration.", "AI": {"tldr": "DynFaceRestore\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u6269\u6563\u91c7\u6837\u8d77\u59cb\u65f6\u95f4\u548c\u5c40\u90e8\u5f15\u5bfc\u5f3a\u5ea6\u7684\u76f2\u8138\u6062\u590d\u65b9\u6cd5\uff0c\u6709\u6548\u5e73\u8861\u4e86\u4fdd\u771f\u5ea6\u548c\u7ec6\u8282\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u76f2\u8138\u6062\u590d\u65b9\u6cd5\u56e0\u56fa\u5b9a\u6269\u6563\u91c7\u6837\u65f6\u95f4\u548c\u5168\u5c40\u5f15\u5bfc\u5c3a\u5ea6\uff0c\u5bfc\u81f4\u4fdd\u771f\u5ea6\u548c\u7ec6\u8282\u8d28\u91cf\u4e0d\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u5c06\u9000\u5316\u8f93\u5165\u6620\u5c04\u5230\u9ad8\u65af\u6a21\u7cca\u56fe\u50cf\uff0c\u52a8\u6001\u9009\u62e9\u8d77\u59cb\u65f6\u95f4\u6b65\uff0c\u5e76\u5e94\u7528\u5c40\u90e8\u52a8\u6001\u5f15\u5bfc\u8c03\u6574\u3002", "result": "DynFaceRestore\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u76f2\u8138\u6062\u590d\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5e73\u8861\u4e86\u4fdd\u771f\u5ea6\u4e0e\u7ec6\u8282\u8d28\u91cf\u3002"}}
{"id": "2507.13801", "pdf": "https://arxiv.org/pdf/2507.13801", "abs": "https://arxiv.org/abs/2507.13801", "authors": ["Haoang Lu", "Yuanqi Su", "Xiaoning Zhang", "Hao Hu"], "title": "One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a\ncritical perception task for autonomous driving due to its ability to infer\ncomplete 3D scene layouts and semantics from single 2D images. However, in\nreal-world traffic scenarios, a significant portion of the scene remains\noccluded or outside the camera's field of view -- a fundamental challenge that\nexisting monocular SSC methods fail to address adequately. To overcome these\nlimitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC\nframework that leverages pseudo-future frame prediction to expand the model's\neffective perceptual range. Our approach combines poses and depths to establish\naccurate 3D correspondences, enabling geometrically-consistent fusion of past,\npresent, and predicted future frames in 3D space. Unlike conventional methods\nthat rely on simple feature stacking, our 3D-aware architecture achieves more\nrobust scene completion by explicitly modeling spatial-temporal relationships.\nComprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks\ndemonstrate state-of-the-art performance, validating the effectiveness of our\napproach, highlighting our method's ability to improve occlusion reasoning and\n3D scene completion accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f43D\u8bed\u4e49\u573a\u666f\u8865\u5168\u6846\u67b6CF-SSC\uff0c\u901a\u8fc7\u9884\u6d4b\u4f2a\u672a\u6765\u5e27\u6269\u5c55\u611f\u77e5\u8303\u56f4\uff0c\u63d0\u5347\u906e\u6321\u63a8\u7406\u548c\u573a\u666f\u8865\u5168\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5355\u76eeSSC\u65b9\u6cd5\u5728\u771f\u5b9e\u4ea4\u901a\u573a\u666f\u4e2d\u96be\u4ee5\u5904\u7406\u906e\u6321\u548c\u89c6\u91ce\u5916\u533a\u57df\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u7ed3\u5408\u4f4d\u59ff\u548c\u6df1\u5ea6\u5efa\u7acb3D\u5bf9\u5e94\u5173\u7cfb\uff0c\u878d\u5408\u8fc7\u53bb\u3001\u5f53\u524d\u548c\u9884\u6d4b\u7684\u672a\u6765\u5e27\uff0c\u663e\u5f0f\u5efa\u6a21\u65f6\u7a7a\u5173\u7cfb\u3002", "result": "\u5728SemanticKITTI\u548cSSCBench-KITTI-360\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "CF-SSC\u901a\u8fc7\u65f6\u7a7a\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u8865\u5168\u6027\u80fd\u3002"}}
{"id": "2507.13803", "pdf": "https://arxiv.org/pdf/2507.13803", "abs": "https://arxiv.org/abs/2507.13803", "authors": ["Weiqi Yang", "Xu Zhou", "Jingfu Guan", "Hao Du", "Tianyu Bai"], "title": "GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely\ndeployed in smart homes, intelligent transport, industrial automation, and\nhealthcare. However, existing systems often face challenges: high model\ncomplexity hinders deployment in resource-constrained environments,\nunidirectional modal alignment neglects inter-modal relationships, and\nrobustness suffers when sensor data is missing. These issues impede efficient\nand robust multimodal perception in real-world IoT settings. To overcome these\nlimitations, we propose GRAM-MAMBA. This framework utilizes the\nlinear-complexity Mamba model for efficient sensor time-series processing,\ncombined with an optimized GRAM matrix strategy for pairwise alignment among\nmodalities, addressing the shortcomings of traditional single-modality\nalignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive\nlow-rank layer compensation strategy to handle missing modalities\npost-training. This strategy freezes the pre-trained model core and irrelevant\nadaptive layers, fine-tuning only those related to available modalities and the\nfusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On\nthe SPAWC2021 indoor positioning dataset, the pre-trained model shows lower\nerror than baselines; adapting to missing modalities yields a 24.5% performance\nboost by training less than 0.2% of parameters. On the USC-HAD human activity\nrecognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),\noutperforming prior work; the update strategy increases F1 by 23% while\ntraining less than 0.3% of parameters. These results highlight GRAM-MAMBA's\npotential for achieving efficient and robust multimodal perception in\nresource-constrained environments.", "AI": {"tldr": "GRAM-MAMBA\u6846\u67b6\u901a\u8fc7\u7ebf\u6027\u590d\u6742\u5ea6\u7684Mamba\u6a21\u578b\u548c\u4f18\u5316\u7684GRAM\u77e9\u9635\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u6548\u7387\u3001\u6a21\u6001\u5bf9\u9f50\u548c\u7f3a\u5931\u6570\u636e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u878d\u5408\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u9762\u4e34\u6a21\u578b\u590d\u6742\u3001\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u548c\u7f3a\u5931\u6570\u636e\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408Mamba\u6a21\u578b\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u4f7f\u7528GRAM\u77e9\u9635\u4f18\u5316\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4f4e\u79e9\u81ea\u9002\u5e94\u5c42\u8865\u507f\u7f3a\u5931\u6a21\u6001\u3002", "result": "\u5728SPAWC2021\u548cUSC-HAD\u6570\u636e\u96c6\u4e0a\uff0cGRAM-MAMBA\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u4e14\u53c2\u6570\u8bad\u7ec3\u91cf\u6781\u5c11\u3002", "conclusion": "GRAM-MAMBA\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u9c81\u68d2\u591a\u6a21\u6001\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13812", "pdf": "https://arxiv.org/pdf/2507.13812", "abs": "https://arxiv.org/abs/2507.13812", "authors": ["Yingying Zhang", "Lixiang Ru", "Kang Wu", "Lei Yu", "Lei Liang", "Yansheng Li", "Jingdong Chen"], "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing", "categories": ["cs.CV"], "comment": "Accepted by ICCV25", "summary": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.", "AI": {"tldr": "SkySense V2\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u9065\u611f\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u4e00Transformer\u4e3b\u5e72\u548c\u81ea\u9002\u5e94SSL\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5197\u4f59\u548c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u57fa\u7840\u6a21\u578b\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u79cd\u6570\u636e\u6a21\u6001\u8bad\u7ec3\u5355\u72ec\u7684\u4e3b\u5e72\u7f51\u7edc\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u53c2\u6570\u5229\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u9884\u8bad\u7ec3\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u9002\u5e94\u9065\u611f\u56fe\u50cf\u7684\u7279\u70b9\u3002", "method": "SkySense V2\u91c7\u7528\u5355\u4e00Transformer\u4e3b\u5e72\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\uff0c\u7ed3\u5408\u81ea\u9002\u5e94SSL\u7b56\u7565\u3001\u81ea\u9002\u5e94\u5757\u5408\u5e76\u6a21\u5757\u3001\u53ef\u5b66\u4e60\u6a21\u6001\u63d0\u793a\u4ee4\u724c\u548cMoE\u6a21\u5757\u3002", "result": "\u57287\u4e2a\u4efb\u52a1\u768416\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cSkySense V2\u5e73\u5747\u6bd4SkySense\u63d0\u53471.8\u5206\u3002", "conclusion": "SkySense V2\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u9065\u611f\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13820", "pdf": "https://arxiv.org/pdf/2507.13820", "abs": "https://arxiv.org/abs/2507.13820", "authors": ["Jun Xie", "Zhaoran Zhao", "Xiongjun Guan", "Yingjian Zhu", "Hongzhu Yi", "Xinming Wang", "Feng Chen", "Zhepeng Wang"], "title": "Team of One: Cracking Complex Video QA with Model Synergy", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f00\u653e\u89c6\u9891\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u534f\u4f5c\u63d0\u5347\u63a8\u7406\u6df1\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u5982\u4e0a\u4e0b\u6587\u7406\u89e3\u6709\u9650\u3001\u65f6\u5e8f\u5efa\u6a21\u5f31\u3001\u5bf9\u6a21\u7cca\u6216\u7ec4\u5408\u67e5\u8be2\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u5f15\u5165\u63d0\u793a-\u54cd\u5e94\u96c6\u6210\u673a\u5236\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u534f\u8c03\u591a\u4e2a\u5f02\u6784\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5229\u7528\u5916\u90e8\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u548c\u96c6\u6210\u5668\u3002", "result": "\u5728CVRR-ES\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u7b56\u7565\uff0c\u4e3a\u672a\u6765\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.13852", "pdf": "https://arxiv.org/pdf/2507.13852", "abs": "https://arxiv.org/abs/2507.13852", "authors": ["Luigi Russo", "Francesco Mauro", "Babak Memar", "Alessandro Sebastianelli", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted at IEEE Joint Urban Remote Sensing Event (JURSE) 2025", "summary": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528Quanvolution\u9884\u5904\u7406\u589e\u5f3aAttention U-Net\u6a21\u578b\u5728\u57ce\u5e02\u5efa\u7b51\u5206\u5272\u4e2d\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u3002", "motivation": "\u57ce\u5e02\u5efa\u7b51\u5206\u5272\u5728\u89c4\u5212\u3001\u707e\u5bb3\u54cd\u5e94\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u7684\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408Quanvolution\u9884\u5904\u7406\u548cAttention U-Net\u6a21\u578b\uff0c\u5229\u7528Sentinel-1 SAR\u56fe\u50cf\u8fdb\u884c\u5efa\u7b51\u5206\u5272\u3002", "result": "\u65b9\u6cd5\u5728\u6d4b\u8bd5\u7cbe\u5ea6\u4e0a\u4e0e\u6807\u51c6\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u7f51\u7edc\u53c2\u6570\u3002", "conclusion": "\u91cf\u5b50\u8f85\u52a9\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u5efa\u7b51\u5206\u5272\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.13857", "pdf": "https://arxiv.org/pdf/2507.13857", "abs": "https://arxiv.org/abs/2507.13857", "authors": ["Max van den Hoven", "Kishaan Jeeveswaran", "Pieter Piscaer", "Thijs Wensveen", "Elahe Arani", "Bahram Zonooz"], "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.", "AI": {"tldr": "Depth3DLane\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u6df1\u5ea6\u4f30\u8ba1\uff0c\u65e0\u9700\u6602\u8d35\u4f20\u611f\u5668\u6216\u6df1\u5ea6\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u5355\u76ee3D\u8f66\u9053\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5355\u76ee3D\u8f66\u9053\u68c0\u6d4b\u4e2d\u7f3a\u4e4f\u663e\u5f0f\u7a7a\u95f4\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u6602\u8d35\u4f20\u611f\u5668\u6216\u5927\u89c4\u6a21\u6df1\u5ea6\u6570\u636e\u3002", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u6df1\u5ea6\u7f51\u7edc\u751f\u6210\u70b9\u4e91\uff0c\u7ed3\u5408\u9e1f\u77b0\u56fe\u548c\u524d\u89c6\u56fe\u8def\u5f84\u63d0\u53d6\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u4f7f\u75283D\u8f66\u9053\u951a\u70b9\u91c7\u6837\u7279\u5f81\u3002", "result": "\u5728OpenLane\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u65e0\u9700\u76f8\u673a\u6807\u5b9a\u5373\u53ef\u5e94\u7528\u3002", "conclusion": "Depth3DLane\u5728\u65e0\u6807\u5b9a\u573a\u666f\u4e0b\u5177\u6709\u7ade\u4e89\u529b\uff0c\u6269\u5c55\u4e86\u5355\u76ee3D\u8f66\u9053\u68c0\u6d4b\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2507.13861", "pdf": "https://arxiv.org/pdf/2507.13861", "abs": "https://arxiv.org/abs/2507.13861", "authors": ["Junjie Hu", "Tianyang Han", "Kai Ma", "Jialin Gao", "Hao Dou", "Song Yang", "Xianhua He", "Jianhui Zhang", "Junfeng Luo", "Xiaoming Wei", "Wenqiang Zhang"], "title": "PositionIC: Unified Position and Identity Consistency for Image Customization", "categories": ["cs.CV"], "comment": null, "summary": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.", "AI": {"tldr": "PositionIC\u6846\u67b6\u901a\u8fc7\u4f4d\u7f6e\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u5b9e\u73b0\u591a\u4e3b\u4f53\u56fe\u50cf\u5b9a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a7\u5236\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5b9a\u5236\u65b9\u6cd5\u5728\u5b9e\u4f53\u7ea7\u7a7a\u95f4\u63a7\u5236\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u7f3a\u4e4f\u7ed1\u5b9a\u8eab\u4efd\u4e0e\u7cbe\u786e\u4f4d\u7f6e\u7684\u53ef\u6269\u5c55\u6570\u636e\u96c6\u662f\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u63d0\u51faPositionIC\u6846\u67b6\uff0c\u6784\u5efa\u53cc\u5411\u751f\u6210\u8303\u5f0f\u5408\u6210\u6570\u636e\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u4f4d\u7f6e\u8c03\u5236\u5c42\u89e3\u8026\u7a7a\u95f4\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u7cbe\u786e\u7a7a\u95f4\u63a7\u5236\u5e76\u4fdd\u6301\u56fe\u50cf\u5b9a\u5236\u7684\u9ad8\u4e00\u81f4\u6027\u3002", "conclusion": "PositionIC\u4e3a\u5f00\u653e\u4e16\u754c\u591a\u5b9e\u4f53\u573a\u666f\u4e0b\u7684\u53ef\u63a7\u9ad8\u4fdd\u771f\u56fe\u50cf\u5b9a\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c06\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2507.13868", "pdf": "https://arxiv.org/pdf/2507.13868", "abs": "https://arxiv.org/abs/2507.13868", "authors": ["Francesco Ortu", "Zhijing Jin", "Diego Doimo", "Alberto Cazzaniga"], "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5982\u4f55\u5904\u7406\u5185\u90e8\u77e5\u8bc6\u4e0e\u5916\u90e8\u4fe1\u606f\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u6a21\u6001\u53cd\u4e8b\u5b9e\u67e5\u8be2\u6570\u636e\u96c6\uff0c\u5b9a\u4f4d\u5e76\u4fee\u6539\u63a7\u5236\u51b2\u7a81\u7684\u6ce8\u610f\u529b\u5934\uff0c\u4ece\u800c\u5f15\u5bfc\u6a21\u578b\u884c\u4e3a\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u4e0e\u5916\u90e8\u4fe1\u606f\u4e4b\u95f4\u53ef\u80fd\u4ea7\u751f\u51b2\u7a81\uff0c\u5bfc\u81f4\u5e7b\u89c9\u548c\u4e0d\u53ef\u9760\u54cd\u5e94\uff0c\u4f46\u5176\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f15\u5165\u591a\u6a21\u6001\u53cd\u4e8b\u5b9e\u67e5\u8be2\u6570\u636e\u96c6\uff0c\u5b9a\u4f4d\u63a7\u5236\u51b2\u7a81\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u901a\u8fc7\u4fee\u6539\u8fd9\u4e9b\u5934\u6765\u5f15\u5bfc\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5c11\u91cf\u6ce8\u610f\u529b\u5934\u63a7\u5236\u51b2\u7a81\uff0c\u4fee\u6539\u8fd9\u4e9b\u5934\u53ef\u5f15\u5bfc\u6a21\u578b\u504f\u5411\u5185\u90e8\u77e5\u8bc6\u6216\u89c6\u89c9\u8f93\u5165\uff1b\u8fd9\u4e9b\u5934\u7684\u6ce8\u610f\u529b\u5b9a\u4f4d\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u4f4d\u548c\u4fee\u6539\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff0c\u53ef\u4ee5\u6709\u6548\u63a7\u5236VLMs\u5728\u77e5\u8bc6\u51b2\u7a81\u4e2d\u7684\u884c\u4e3a\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002"}}
{"id": "2507.13880", "pdf": "https://arxiv.org/pdf/2507.13880", "abs": "https://arxiv.org/abs/2507.13880", "authors": ["Marten Kreis", "Benjamin Kiefer"], "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u878d\u5408\u5b9e\u65f6\u89c6\u89c9\u6570\u636e\u548c\u6d77\u56fe\u4fe1\u606f\u6765\u589e\u5f3a\u6d77\u6d0b\u89c6\u89c9\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u5bfc\u822a\u6807\u5fd7\u7684\u7cbe\u51c6\u5339\u914d\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u548c\u6311\u6218\u6027\u6d77\u6d0b\u73af\u5883\u4e2d\u5bfc\u822a\u6807\u5fd7\u5b9a\u4f4d\u548c\u5173\u8054\u7684\u51c6\u786e\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165\u57fa\u4e8eTransformer\u7684\u7aef\u5230\u7aef\u795e\u7ecf\u7f51\u7edc\uff0c\u9884\u6d4b\u6d6e\u6807\u67e5\u8be2\u7684\u8fb9\u754c\u6846\u548c\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u76f4\u63a5\u5339\u914d\u56fe\u50cf\u57df\u68c0\u6d4b\u4e0e\u4e16\u754c\u7a7a\u95f4\u6d77\u56fe\u6807\u8bb0\u3002", "result": "\u5728\u771f\u5b9e\u6d77\u6d0b\u573a\u666f\u6570\u636e\u96c6\u4e0a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u5b9a\u4f4d\u548c\u5173\u8054\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u548c\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u6d77\u6d0b\u89c6\u89c9\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13891", "pdf": "https://arxiv.org/pdf/2507.13891", "abs": "https://arxiv.org/abs/2507.13891", "authors": ["Yu Wei", "Jiahui Zhang", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.", "AI": {"tldr": "PCR-GS\u662f\u4e00\u79cd\u65e0\u9700COLMAP\u76843D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u901a\u8fc7\u76f8\u673a\u59ff\u6001\u5171\u6b63\u5219\u5316\u6539\u8fdb\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u4e0b\u76843D\u573a\u666f\u5efa\u6a21\u548c\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u73b0\u67093D-GS\u65b9\u6cd5\u5728\u590d\u6742\u76f8\u673a\u8f68\u8ff9\uff08\u5982\u5267\u70c8\u65cb\u8f6c\u548c\u5e73\u79fb\uff09\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u8054\u5408\u4f18\u5316\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "method": "PCR-GS\u901a\u8fc7\u7279\u5f81\u91cd\u6295\u5f71\u6b63\u5219\u5316\uff08\u5bf9\u9f50\u76f8\u90bb\u89c6\u56fe\u7684DINO\u7279\u5f81\uff09\u548c\u5c0f\u6ce2\u9891\u7387\u6b63\u5219\u5316\uff08\u4f18\u5316\u65cb\u8f6c\u77e9\u9635\uff09\u5b9e\u73b0\u76f8\u673a\u59ff\u6001\u5171\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPCR-GS\u5728\u5267\u70c8\u53d8\u5316\u7684\u76f8\u673a\u8f68\u8ff9\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u65e0\u59ff\u60013D-GS\u573a\u666f\u5efa\u6a21\u3002", "conclusion": "PCR-GS\u901a\u8fc7\u53cc\u91cd\u6b63\u5219\u5316\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u76843D\u5efa\u6a21\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2507.13899", "pdf": "https://arxiv.org/pdf/2507.13899", "abs": "https://arxiv.org/abs/2507.13899", "authors": ["Yujian Mo", "Yan Wu", "Junqiao Zhao", "Jijun Wang", "Yinghao Hu", "Jun Yan"], "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528DepthAnything\u751f\u6210\u6df1\u5ea6\u5148\u9a8c\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3aLiDAR\u70b9\u4e91\u7279\u5f81\uff0c\u63d0\u53473D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LiDAR\u70b9\u4e91\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u53cd\u5c04\u7387\u5c5e\u6027\u7684\u5f31\u533a\u5206\u80fd\u529b\u3002", "method": "\u878d\u5408DepthAnything\u9884\u6d4b\u7684\u6df1\u5ea6\u5148\u9a8c\u4e0e\u539f\u59cbLiDAR\u5c5e\u6027\uff0c\u63d0\u51fa\u70b9\u7ea7\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u53cc\u8def\u5f84RoI\u7279\u5f81\u63d0\u53d6\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u53cc\u5411\u95e8\u63a7RoI\u7279\u5f81\u878d\u5408\u6a21\u5757\u3002", "result": "\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u53ef\u6709\u6548\u63d0\u5347LiDAR-based 3D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.13929", "pdf": "https://arxiv.org/pdf/2507.13929", "abs": "https://arxiv.org/abs/2507.13929", "authors": ["Hsiang-Hui Hung", "Huu-Phu Do", "Yung-Hui Li", "Ching-Chun Huang"], "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by MM 2024", "summary": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.", "AI": {"tldr": "TimeNeRF\u662f\u4e00\u79cd\u901a\u7528\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4efb\u610f\u89c6\u89d2\u548c\u65f6\u95f4\u4e0b\u6e32\u67d3\u65b0\u89c6\u56fe\uff0c\u5373\u4f7f\u8f93\u5165\u89c6\u56fe\u8f83\u5c11\u3002\u5b83\u7ed3\u5408\u4e86\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u89e3\u8026\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u6784\u5efa\u4efb\u610f\u65f6\u95f4\u7684\u795e\u7ecf\u8f90\u5c04\u573a\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTimeNeRF\u65e0\u9700\u9010\u573a\u666f\u4f18\u5316\u5373\u53ef\u751f\u6210\u5e73\u6ed1\u7684\u65f6\u95f4\u8fc7\u6e21\u89c6\u56fe\u3002", "motivation": "\u5f53\u524dNeRF\u6280\u672f\u5728\u5408\u6210\u65b0\u89c6\u56fe\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u65f6\u95f4\u7ef4\u5ea6\u76843D\u573a\u666f\u5efa\u6a21\u65b9\u9762\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\uff0c\u4e14\u7f3a\u4e4f\u4e13\u7528\u6570\u636e\u96c6\u3002\u6570\u5b57\u9886\u57df\uff08\u5982\u5143\u5b87\u5b99\uff09\u5bf9\u663c\u591c\u81ea\u7136\u8fc7\u6e21\u76843D\u73af\u5883\u5efa\u6a21\u9700\u6c42\u8feb\u5207\u3002", "method": "\u7ed3\u5408\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u89e3\u8026\u7b56\u7565\uff0c\u6784\u5efa\u9690\u5f0f\u5185\u5bb9\u8f90\u5c04\u573a\u8868\u793a\u573a\u666f\uff0c\u5e76\u652f\u6301\u4efb\u610f\u65f6\u95f4\u7684\u795e\u7ecf\u8f90\u5c04\u573a\u6784\u5efa\u3002\u901a\u8fc7\u4f53\u79ef\u6e32\u67d3\u5408\u6210\u65b0\u89c6\u56fe\u3002", "result": "TimeNeRF\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u65e0\u9700\u9010\u573a\u666f\u4f18\u5316\u5373\u53ef\u6e32\u67d3\u65b0\u89c6\u56fe\uff0c\u5e76\u80fd\u751f\u6210\u5e73\u6ed1\u7684\u65f6\u95f4\u8fc7\u6e21\u89c6\u56fe\uff0c\u6355\u6349\u4ece\u9ece\u660e\u5230\u9ec4\u660f\u7684\u81ea\u7136\u573a\u666f\u53d8\u5316\u3002", "conclusion": "TimeNeRF\u4e3a\u65f6\u95f4\u7ef4\u5ea6\u76843D\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5143\u5b87\u5b99\u7b49\u9700\u8981\u52a8\u6001\u573a\u666f\u7684\u5e94\u7528\u3002"}}
{"id": "2507.13934", "pdf": "https://arxiv.org/pdf/2507.13934", "abs": "https://arxiv.org/abs/2507.13934", "authors": ["Marzieh Gheisari", "Auguste Genovesio"], "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.", "AI": {"tldr": "DiViD\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u663e\u5f0f\u5206\u79bb\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\uff0c\u901a\u8fc7\u5168\u5c40\u9759\u6001\u6807\u8bb0\u548c\u5e27\u7279\u5b9a\u52a8\u6001\u6807\u8bb0\u5b9e\u73b0\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eVAE\u548cGAN\u7684\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u6cc4\u6f0f\u548c\u6a21\u7cca\u91cd\u5efa\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89c6\u9891\u9759\u6001-\u52a8\u6001\u5206\u79bb\u65b9\u6cd5\u3002", "method": "DiViD\u91c7\u7528\u5e8f\u5217\u7f16\u7801\u5668\u63d0\u53d6\u5168\u5c40\u9759\u6001\u6807\u8bb0\u548c\u5e27\u7279\u5b9a\u52a8\u6001\u6807\u8bb0\uff0c\u7ed3\u5408\u6761\u4ef6DDPM\u89e3\u7801\u5668\uff0c\u5f15\u5165\u5171\u4eab\u566a\u58f0\u8ba1\u5212\u3001\u65f6\u95f4\u53d8\u5316KL\u74f6\u9888\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u7b49\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "DiViD\u5728\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u6700\u9ad8\u7684\u4ea4\u6362\u8054\u5408\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9759\u6001\u4fdd\u771f\u5ea6\u548c\u52a8\u6001\u4f20\u9012\u6027\uff0c\u51cf\u5c11\u4ea4\u53c9\u6cc4\u6f0f\u3002", "conclusion": "DiViD\u662f\u9996\u4e2a\u5b9e\u73b0\u663e\u5f0f\u9759\u6001-\u52a8\u6001\u5206\u89e3\u7684\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.13942", "pdf": "https://arxiv.org/pdf/2507.13942", "abs": "https://arxiv.org/abs/2507.13942", "authors": ["Jacob C Walker", "Pedro V\u00e9lez", "Luisa Polania Cabrera", "Guangyao Zhou", "Rishabh Kabra", "Carl Doersch", "Maks Ovsjanikov", "Jo\u00e3o Carreira", "Shiry Ginosar"], "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u77ed\u671f\u9884\u6d4b\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u6a21\u578b\u9884\u6d4b\u672a\u6765\u7279\u5f81\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u9884\u6d4b\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u4ee5\u63d0\u5347\u901a\u7528\u7cfb\u7edf\u5728\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u4e0a\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u901a\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u51bb\u7ed3\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u4e2d\u9884\u6d4b\u672a\u6765\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u5b9e\u73b0\u3002", "result": "\u5728\u4e5d\u4e2a\u6a21\u578b\u548c\u56db\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u611f\u77e5\u80fd\u529b\u4e0e\u9884\u6d4b\u6027\u80fd\u5f3a\u76f8\u5173\u3002", "conclusion": "\u7ed3\u5408\u8868\u5f81\u5b66\u4e60\u548c\u751f\u6210\u6a21\u578b\u5bf9\u89c6\u9891\u7406\u89e3\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.13981", "pdf": "https://arxiv.org/pdf/2507.13981", "abs": "https://arxiv.org/abs/2507.13981", "authors": ["Sara Abdulaziz", "Giacomo D'Amicantonio", "Egor Bondarev"], "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset", "categories": ["cs.CV"], "comment": "accepted at ICCV'25 workshop CV4BIOM", "summary": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u89c6\u89c9\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u7684\u6846\u67b6\uff0c\u5e76\u53d1\u5e03\u4e86HR-VISPR\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u53ef\u89e3\u91ca\u7684\u9690\u79c1\u5ea6\u91cf\u3002", "motivation": "AI\u9a71\u52a8\u7684\u76d1\u63a7\u6280\u672f\u5f15\u53d1\u4e86\u5bf9\u654f\u611f\u6570\u636e\u5904\u7406\u7684\u9690\u79c1\u62c5\u5fe7\uff0c\u9700\u8981\u5ba2\u89c2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u7ef4\uff08\u9690\u79c1\u3001\u5b9e\u7528\u6027\u548c\u5b9e\u7528\u6027\uff09\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5229\u7528HR-VISPR\u6570\u636e\u96c6\u8bc4\u4f30\u4e8611\u79cd\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002", "result": "\u6846\u67b6\u80fd\u591f\u533a\u5206\u9690\u79c1\u7ea7\u522b\uff0c\u5e76\u63ed\u793a\u9690\u79c1\u3001\u5b9e\u7528\u6027\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u548c\u6570\u636e\u96c6\u4e3a\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2507.13984", "pdf": "https://arxiv.org/pdf/2507.13984", "abs": "https://arxiv.org/abs/2507.13984", "authors": ["Quang-Binh Nguyen", "Minh Luu", "Quang Nguyen", "Anh Tran", "Khoi Nguyen"], "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025", "summary": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.", "AI": {"tldr": "CSD-VAR\u662f\u4e00\u79cd\u57fa\u4e8eVAR\u7684\u5185\u5bb9-\u98ce\u683c\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c3a\u5ea6\u611f\u77e5\u4f18\u5316\u3001SVD\u6821\u6b63\u548c\u589e\u5f3aK-V\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5bb9\u4fdd\u6301\u548c\u98ce\u683c\u5316\u6548\u679c\u3002", "motivation": "\u63a2\u7d22VAR\u4f5c\u4e3a\u751f\u6210\u6846\u67b6\u7528\u4e8e\u5185\u5bb9-\u98ce\u683c\u5206\u89e3\uff0c\u5229\u7528\u5176\u5c3a\u5ea6\u751f\u6210\u8fc7\u7a0b\u6539\u8fdb\u89e3\u8026\u6548\u679c\u3002", "method": "\u63d0\u51faCSD-VAR\u65b9\u6cd5\uff0c\u5305\u62ec\u5c3a\u5ea6\u611f\u77e5\u4ea4\u66ff\u4f18\u5316\u3001SVD\u6821\u6b63\u548c\u589e\u5f3aK-V\u8bb0\u5fc6\u3002", "result": "CSD-VAR\u5728\u5185\u5bb9\u4fdd\u6301\u548c\u98ce\u683c\u5316\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CSD-VAR\u4e3a\u5185\u5bb9-\u98ce\u683c\u5206\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u6846\u67b6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.13985", "pdf": "https://arxiv.org/pdf/2507.13985", "abs": "https://arxiv.org/abs/2507.13985", "authors": ["Haoran Li", "Yuli Tian", "Kun Lan", "Yong Liao", "Lin Wang", "Pan Hui", "Peng Yuan Zhou"], "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation", "categories": ["cs.CV"], "comment": "Extended version of ECCV 2024 paper \"DreamScene\"", "summary": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.", "AI": {"tldr": "DreamScene\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u6216\u5bf9\u8bdd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u7f16\u8f91\u76843D\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u30013D\u4e00\u81f4\u6027\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u62103D\u573a\u666f\u5728\u6e38\u620f\u3001\u7535\u5f71\u548c\u8bbe\u8ba1\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u3001\u4e00\u81f4\u6027\u548c\u63a7\u5236\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "DreamScene\u7ed3\u5408\u573a\u666f\u89c4\u5212\u6a21\u5757\uff08GPT-4\u4ee3\u7406\u63a8\u65ad\u8bed\u4e49\u548c\u7a7a\u95f4\u7ea6\u675f\uff09\u3001\u57fa\u4e8e\u56fe\u7684\u5e03\u5c40\u7b97\u6cd5\u3001\u51e0\u4f55\u751f\u6210\uff08FPS\uff09\u548c\u6e10\u8fdb\u76f8\u673a\u91c7\u6837\u7b56\u7565\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDreamScene\u5728\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u548c\u7075\u6d3b\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u5f00\u653e\u57df3D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "DreamScene\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u7f16\u8f91\u76843D\u573a\u666f\u751f\u6210\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.14010", "pdf": "https://arxiv.org/pdf/2507.14010", "abs": "https://arxiv.org/abs/2507.14010", "authors": ["Yong Feng", "Xiaolei Zhang", "Shijin Feng", "Yong Zhao", "Yihan Chen"], "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations", "categories": ["cs.CV"], "comment": "8 pages, 10 figures, 3 tables", "summary": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e24\u6b65\u6cd5\uff0c\u7528\u4e8e\u96a7\u9053\u88c2\u7f1d\u7684\u5206\u7c7b\u548c\u5206\u5272\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u96a7\u9053\u88c2\u7f1d\u662f\u96a7\u9053\u5b89\u5168\u72b6\u6001\u7684\u91cd\u8981\u6307\u6807\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5206\u7c7b\u548c\u5206\u5272\u88c2\u7f1d\u3002", "method": "\u7b2c\u4e00\u6b65\u4f7f\u7528DenseNet-169\u8fdb\u884c\u96a7\u9053\u56fe\u50cf\u5206\u7c7b\uff0c\u7b2c\u4e8c\u6b65\u57fa\u4e8eDeepLabV3+\u8fdb\u884c\u88c2\u7f1d\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u6280\u672f\u8bc4\u4f30\u6a21\u578b\u903b\u8f91\u3002", "result": "\u5206\u7c7b\u6a21\u578b\u51c6\u786e\u7387\u4e3a92.23%\uff0cFPS\u4e3a39.80\uff1b\u5206\u5272\u6a21\u578b\u7684IoU\u4e3a57.01%\uff0cF1\u5f97\u5206\u4e3a67.44%\uff0c\u5747\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5206\u7c7b\u548c\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u89e3\u91ca\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u7406\u89e3\u6027\uff0c\u4e3a\u96a7\u9053\u5065\u5eb7\u72b6\u51b5\u7684\u5feb\u901f\u51c6\u786e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14013", "pdf": "https://arxiv.org/pdf/2507.14013", "abs": "https://arxiv.org/abs/2507.14013", "authors": ["Ji-Yan Wu", "Zheng Yong Poh", "Anoop C. Patil", "Bongsoo Park", "Giovanni Volpe", "Daisuke Urano"], "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model", "categories": ["cs.CV"], "comment": null, "summary": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5149\u8c31\u6210\u50cf\u548c\u589e\u5f3aYOLOv5\u6a21\u578b\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u690d\u7269\u53f6\u7247\u5f02\u5e38\u5206\u5272\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u7cbe\u51c6\u519c\u4e1a\u9700\u8981\u51c6\u786e\u68c0\u6d4b\u690d\u7269\u53f6\u7247\u8425\u517b\u7f3a\u4e4f\uff0c\u4ee5\u5b9e\u73b0\u65e9\u671f\u5e72\u9884\u65bd\u80a5\u3001\u75be\u75c5\u548c\u538b\u529b\u7ba1\u7406\u3002", "method": "\u91c7\u7528\u591a\u5149\u8c31\u6210\u50cf\u548c\u589e\u5f3aYOLOv5\u6a21\u578b\uff0c\u7ed3\u5408\u57fa\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5904\u7406\u4e5d\u901a\u9053\u591a\u5149\u8c31\u8f93\u5165\u3002", "result": "\u6a21\u578b\u5728Dice\u5206\u6570\u548cIoU\u4e0a\u5e73\u5747\u63d0\u534712%\uff0c\u5c24\u5176\u5728\u68c0\u6d4b\u9ec4\u5316\u548c\u8272\u7d20\u79ef\u7d2f\u7b49\u6311\u6218\u6027\u75c7\u72b6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7ed3\u5408\u591a\u5149\u8c31\u6210\u50cf\u548c\u5149\u8c31-\u7a7a\u95f4\u7279\u5f81\u5b66\u4e60\uff0c\u4e3a\u690d\u7269\u8868\u578b\u548c\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14024", "pdf": "https://arxiv.org/pdf/2507.14024", "abs": "https://arxiv.org/abs/2507.14024", "authors": ["Jiarong Ye", "Sharon X. Huang"], "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u60c5\u7eea\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u96c6MoodArchive\u3001\u6a21\u578bMoodifyCLIP\u548c\u7f16\u8f91\u5de5\u5177Moodifier\uff0c\u7528\u4e8e\u7cbe\u786e\u8c03\u6574\u56fe\u50cf\u60c5\u7eea\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "motivation": "\u60c5\u7eea\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u5728\u521b\u610f\u4ea7\u4e1a\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u56e0\u60c5\u7eea\u62bd\u8c61\u4e14\u8868\u73b0\u591a\u6837\uff0c\u7cbe\u786e\u64cd\u4f5c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "1. \u6784\u5efaMoodArchive\u6570\u636e\u96c6\uff088M+\u56fe\u50cf\uff0c\u5e26\u5c42\u7ea7\u60c5\u7eea\u6807\u6ce8\uff09\uff1b2. \u5f00\u53d1MoodifyCLIP\u6a21\u578b\uff0c\u5c06\u60c5\u7eea\u6620\u5c04\u4e3a\u89c6\u89c9\u5c5e\u6027\uff1b3. \u63d0\u51faMoodifier\u7f16\u8f91\u6a21\u578b\uff0c\u7ed3\u5408MLLMs\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7684\u60c5\u7eea\u8f6c\u6362\u3002", "result": "Moodifier\u5728\u60c5\u7eea\u51c6\u786e\u6027\u548c\u5185\u5bb9\u4fdd\u7559\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\uff08\u5982\u89d2\u8272\u8868\u60c5\u3001\u65f6\u5c1a\u8bbe\u8ba1\u7b49\uff09\u3002", "conclusion": "\u901a\u8fc7\u5c06\u62bd\u8c61\u60c5\u7eea\u4e0e\u5177\u4f53\u89c6\u89c9\u53d8\u5316\u5173\u8054\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u60c5\u7eea\u5185\u5bb9\u521b\u4f5c\u5f00\u8f9f\u65b0\u53ef\u80fd\u3002"}}
{"id": "2507.14031", "pdf": "https://arxiv.org/pdf/2507.14031", "abs": "https://arxiv.org/abs/2507.14031", "authors": ["Hao Fang", "Sihao Teng", "Hao Yu", "Siyi Yuan", "Huaiwu He", "Zhe Liu", "Yunjie Yang"], "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography", "categories": ["cs.CV", "cs.ET", "cs.LG"], "comment": "10 pages, 12 figures", "summary": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u8f85\u52a9\u7f51\u7edc\u7684\u8d85\u8f7b\u91cf\u7ea7EIT\u56fe\u50cf\u91cd\u5efa\u6846\u67b6QuantEIT\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u65e0\u76d1\u7763\u3001\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u3002", "motivation": "EIT\u56fe\u50cf\u91cd\u5efa\u7684\u9006\u95ee\u9898\u5177\u6709\u75c5\u6001\u6027\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7f51\u7edc\u7ed3\u6784\uff0c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "QuantEIT\u7ed3\u5408\u5e76\u884c2\u91cf\u5b50\u6bd4\u7279\u7535\u8def\u751f\u6210\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5355\u5c42\u7ebf\u6027\u7f51\u7edc\u91cd\u5efa\u7535\u5bfc\u7387\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e2D/3D\u80ba\u90e8EIT\u6570\u636e\u4e0a\uff0cQuantEIT\u4ec5\u75280.2%\u53c2\u6570\u5373\u8fbe\u5230\u6216\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u7684\u7cbe\u5ea6\uff0c\u4e14\u6297\u566a\u6027\u66f4\u5f3a\u3002", "conclusion": "QuantEIT\u9996\u6b21\u5c06\u91cf\u5b50\u7535\u8def\u5f15\u5165EIT\u91cd\u5efa\uff0c\u4e3a\u9ad8\u6548\u3001\u8f7b\u91cf\u5316\u7684\u533b\u5b66\u6210\u50cf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.14042", "pdf": "https://arxiv.org/pdf/2507.14042", "abs": "https://arxiv.org/abs/2507.14042", "authors": ["Qiankun Ma", "Ziyao Zhang", "Chi Su", "Jie Chen", "Zhen Song", "Hairong Zheng", "Wen Gao"], "title": "Training-free Token Reduction for Vision Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.", "AI": {"tldr": "Vision Mamba\uff08\u89c6\u89c9Mamba\uff09\u4f5c\u4e3aVision Transformers\uff08ViTs\uff09\u7684\u7ade\u4e89\u8005\uff0c\u56e0\u5176\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u9ad8\u6548\u6027\u800c\u5907\u53d7\u5173\u6ce8\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684Mamba Token Reduction\uff08MTR\uff09\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u91cf\u4e14\u6027\u80fd\u5f71\u54cd\u5c0f\u3002", "motivation": "\u63a2\u7d22Vision Mamba\u7684\u6548\u7387\u4ee5\u6269\u5c55\u5176\u5e94\u7528\uff0c\u4f46\u73b0\u6709ViTs\u7684token\u7f29\u51cf\u6280\u672f\u76f4\u63a5\u5e94\u7528\u4e8eMamba\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faMamba\u7ed3\u6784\u611f\u77e5\u7684\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u65e0\u9700\u8bad\u7ec3\u7684MTR\u6846\u67b6\u3002", "result": "MTR\u5728Vim-B\u9aa8\u5e72\u4e0a\u51cf\u5c11\u7ea640%\u7684FLOPs\uff0cImageNet\u6027\u80fd\u4ec5\u4e0b\u964d1.6%\u3002", "conclusion": "MTR\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684token\u7f29\u51cf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cdMamba\u6a21\u578b\u3002"}}
{"id": "2507.14050", "pdf": "https://arxiv.org/pdf/2507.14050", "abs": "https://arxiv.org/abs/2507.14050", "authors": ["Mohamed Elkhayat", "Mohamed Mahmoud", "Jamil Fayyad", "Nourhan Bayasi"], "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification", "categories": ["cs.CV"], "comment": "Accepted at the MICCAI EMERGE 2025 workshop", "summary": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u76ae\u80a4\u75c5\u5206\u7c7b\u4e2d\u5229\u7528\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u8fdb\u884c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7MLP\u589e\u91cf\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u63a2\u7d22\u4e86\u96f6\u8bad\u7ec3\u573a\u666f\u4e0b\u7684\u539f\u578b\u5206\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u5206\u7c7b\u4e2d\u7684\u589e\u91cf\u5b66\u4e60\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u51bb\u7ed3FM\u4e3b\u5e72\uff0c\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u589e\u91cf\u8bad\u7ec3\u8f7b\u91cf\u7ea7MLP\uff1b\u540c\u65f6\u6d4b\u8bd5\u96f6\u8bad\u7ec3\u573a\u666f\u4e0b\u7684\u539f\u578b\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728CIL\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff1b\u539f\u578b\u5206\u7c7b\u65b9\u6cd5\u4e5f\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "conclusion": "\u51bb\u7ed3FM\u5728\u76ae\u80a4\u75c5\u6301\u7eed\u5b66\u4e60\u4e2d\u8868\u73b0\u5f3a\u5927\uff0c\u652f\u6301\u5176\u5728\u771f\u5b9e\u533b\u7597\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\u3002"}}
{"id": "2507.14067", "pdf": "https://arxiv.org/pdf/2507.14067", "abs": "https://arxiv.org/abs/2507.14067", "authors": ["Shuliang Liu", "Qi Zheng", "Jesse Jiaxi Xu", "Yibo Yan", "He Geng", "Aiwei Liu", "Peijie Jiang", "Jia Liu", "Yik-Cheung Tam", "Xuming Hu"], "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking", "AI": {"tldr": "VLA-Mark\u662f\u4e00\u79cd\u89c6\u89c9\u5bf9\u9f50\u7684\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u534f\u8c03\u5d4c\u5165\u53ef\u68c0\u6d4b\u6c34\u5370\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u6c34\u5370\u65b9\u6cd5\u7834\u574f\u4e86\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\uff0c\u9700\u8981\u4e00\u79cd\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\u4e14\u4e0d\u5f71\u54cd\u591a\u6a21\u6001\u4e00\u81f4\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u591a\u5c3a\u5ea6\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u6307\u6807\uff08\u5c40\u90e8\u8865\u4e01\u4eb2\u548c\u6027\u3001\u5168\u5c40\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u6a21\u5f0f\uff09\u52a8\u6001\u5e73\u8861\u6c34\u5370\u5f3a\u5ea6\u548c\u8bed\u4e49\u4fdd\u7559\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cVLA-Mark\u5728PPL\u548cBLEU\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe98.8% AUC\uff0c\u6297\u653b\u51fb\u80fd\u529b\u4e3a96.1%\u3002", "conclusion": "VLA-Mark\u4e3a\u8d28\u91cf\u4fdd\u6301\u7684\u591a\u6a21\u6001\u6c34\u5370\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2507.14083", "pdf": "https://arxiv.org/pdf/2507.14083", "abs": "https://arxiv.org/abs/2507.14083", "authors": ["Sara Abdulaziz", "Egor Bondarev"], "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection", "categories": ["cs.CV"], "comment": "ACIVS 2025", "summary": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u56db\u79cd\u533f\u540d\u5316\u6280\u672f\u5bf9\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7b97\u6cd5\u8bbe\u8ba1\u5bf9\u533f\u540d\u5316\u6570\u636e\u7684\u54cd\u5e94\u662f\u5173\u952e\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u68c0\u6d4b\u6548\u7528\u7684\u6743\u8861\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u76d1\u63a7\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u6d89\u53ca\u654f\u611f\u6570\u636e\u6536\u96c6\u5f15\u53d1\u9690\u79c1\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u533f\u540d\u5316\u6280\u672f\u5bf9\u5f02\u5e38\u68c0\u6d4b\u7684\u5f71\u54cd\u3002", "method": "\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u56db\u79cd\u533f\u540d\u5316\u6280\u672f\uff08\u6a21\u7cca\u3001\u63a9\u7801\u3001\u52a0\u5bc6\u3001\u865a\u62df\u66ff\u6362\uff09\uff0c\u5e76\u8bc4\u4f30\u56db\u79cd\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff08MGFN\u3001UR-DMU\u3001BN-WVAD\u3001PEL4VAD\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u533f\u540d\u5316\u6570\u636e\u4e0b\u5f02\u5e38\u68c0\u6d4b\u4ecd\u53ef\u884c\uff0c\u4e14\u67d0\u4e9b\u533f\u540d\u5316\u6280\u672f\uff08\u5982\u52a0\u5bc6\u548c\u63a9\u7801\uff09\u751a\u81f3\u80fd\u63d0\u5347\u90e8\u5206\u6a21\u578b\u7684AUC\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7b97\u6cd5\u5bf9\u533f\u540d\u5316\u7684\u654f\u611f\u6027\uff0c\u5f3a\u8c03\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u68c0\u6d4b\u6548\u7528\u7684\u6743\u8861\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4f20\u7edf\u533f\u540d\u5316\u4e0e\u65b0\u5174\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u7684\u4f18\u52a3\u3002"}}
{"id": "2507.14093", "pdf": "https://arxiv.org/pdf/2507.14093", "abs": "https://arxiv.org/abs/2507.14093", "authors": ["\u0160imon Kubov", "Simon Kl\u00ed\u010dn\u00edk", "Jakub Dand\u00e1r", "Zden\u011bk Straka", "Karol\u00edna Kvakov\u00e1", "Daniel Kvak"], "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u8f6f\u4ef6\uff08Carebot AI Bones\uff09\uff0c\u7528\u4e8e\u6d4b\u91cf\u810a\u67f1\u4fa7\u5f2f\u7684Cobb\u89d2\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u7684\u6d4b\u91cf\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u53ef\u7528\u4e8e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u3002", "motivation": "\u810a\u67f1\u4fa7\u5f2f\u5f71\u54cd2-4%\u7684\u9752\u5c11\u5e74\uff0c\u4f20\u7edf\u624b\u52a8\u6d4b\u91cfCobb\u89d2\u8017\u65f6\u4e14\u5b58\u5728\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u81ea\u52a8\u5316\u8f6f\u4ef6\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u7814\u7a76\u56de\u987e\u6027\u8bc4\u4f30\u4e86103\u5f20\u810a\u67f1X\u5149\u7247\uff0c\u4f7f\u7528Carebot AI Bones\u8f6f\u4ef6\u8fdb\u884c\u81ea\u52a8\u6d4b\u91cf\uff0c\u5e76\u4e0e\u4e24\u4f4d\u653e\u5c04\u79d1\u533b\u751f\u7684\u72ec\u7acb\u6d4b\u91cf\u7ed3\u679c\u5bf9\u6bd4\u3002", "result": "AI\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u7684\u6d4b\u91cf\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff08MAE\u7ea63.9\u5ea6\uff0cPearson\u76f8\u5173\u7cfb\u65700.88-0.91\uff09\uff0c\u4e14\u5206\u7c7b\u4e00\u81f4\u6027\uff08Cohen kappa\uff09\u8fbe\u52300.51-0.64\u3002", "conclusion": "\u8be5\u8f6f\u4ef6\u80fd\u590d\u73b0\u4e13\u5bb6\u6c34\u5e73\u7684Cobb\u89d2\u6d4b\u91cf\u548c\u5206\u7c7b\uff0c\u53ef\u4f18\u5316\u810a\u67f1\u4fa7\u5f2f\u7684\u4e34\u5e8a\u62a5\u544a\u548c\u5206\u8bca\u6d41\u7a0b\u3002"}}
{"id": "2507.14095", "pdf": "https://arxiv.org/pdf/2507.14095", "abs": "https://arxiv.org/abs/2507.14095", "authors": ["Yung-Hong Sun", "Ting-Hung Lin", "Jiangang Chen", "Hongrui Jiang", "Yu Hen Hu"], "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.", "AI": {"tldr": "C-DOG\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8fde\u63a5delta-overlap\u56fe\u5efa\u6a21\u548c\u6781\u7ebf\u51e0\u4f55\uff0c\u5728\u591a\u89c6\u56fe\u4e2d\u9c81\u68d2\u5730\u5173\u8054\u76ee\u6807\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u566a\u58f0\u548c\u89c6\u89c9\u4e0d\u53ef\u533a\u5206\u7684\u60c5\u51b5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u76ee\u6807\u89c6\u89c9\u4e0d\u53ef\u533a\u5206\u6216\u89c2\u6d4b\u566a\u58f0\u65f6\u5931\u6548\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8fde\u63a5delta-overlap\u56fe\u5efa\u6a21\u548c\u6781\u7ebf\u51e0\u4f55\uff0c\u7ed3\u5408IQR\u8fc7\u6ee4\u548c3D\u53cd\u6295\u5f71\u8bef\u5dee\u6807\u51c6\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u51e0\u4f55\u57fa\u7ebf\uff0c\u5e76\u5728\u9ad8\u76ee\u6807\u5bc6\u5ea6\u3001\u65e0\u89c6\u89c9\u7279\u5f81\u548c\u6709\u9650\u76f8\u673a\u91cd\u53e0\u7b49\u6311\u6218\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "C-DOG\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u6269\u5c55\u76843D\u91cd\u5efa\u3002"}}
{"id": "2507.14119", "pdf": "https://arxiv.org/pdf/2507.14119", "abs": "https://arxiv.org/abs/2507.14119", "authors": ["Maksim Kuprashevich", "Grigorii Alekseenko", "Irina Tolstykh", "Georgii Fedorov", "Bulat Suleimanov", "Vladimir Dokholyan", "Aleksandr Gordeev"], "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u6a21\u5757\u5316\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\u4e09\u5143\u7ec4\uff08\u539f\u59cb\u56fe\u50cf\u3001\u6307\u4ee4\u3001\u7f16\u8f91\u540e\u56fe\u50cf\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\u4e09\u5143\u7ec4\u6570\u636e\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6ee1\u8db3\u50cf\u7d20\u7ea7\u51c6\u786e\u6027\u8981\u6c42\u3002", "method": "\u5229\u7528\u516c\u5171\u751f\u6210\u6a21\u578b\u548c\u4efb\u52a1\u8c03\u4f18\u7684Gemini\u9a8c\u8bc1\u5668\uff0c\u81ea\u52a8\u5316\u751f\u6210\u548c\u9a8c\u8bc1\u4e09\u5143\u7ec4\uff0c\u5e76\u901a\u8fc7\u53cd\u8f6c\u548c\u7ec4\u5408\u81ea\u4e3e\u6269\u5927\u6570\u636e\u96c6\u3002", "result": "\u53d1\u5e03\u4e86NHR-Edit\u6570\u636e\u96c6\uff08358k\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\uff09\u548cBagel-NHR-Edit\u6a21\u578b\uff0c\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u81ea\u52a8\u5316\u751f\u6210\uff0c\u63a8\u52a8\u4e86\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u7684\u7814\u7a76\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002"}}
{"id": "2507.14137", "pdf": "https://arxiv.org/pdf/2507.14137", "abs": "https://arxiv.org/abs/2507.14137", "authors": ["Shashanka Venkataramanan", "Valentinos Pariza", "Mohammadreza Salehi", "Lukas Knobel", "Spyros Gidaris", "Elias Ramzi", "Andrei Bursuc", "Yuki M. Asano"], "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.", "AI": {"tldr": "Franca\u662f\u9996\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u6027\u80fd\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u4e13\u6709\u6a21\u578b\uff0c\u89e3\u51b3\u4e86SSL\u805a\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u5934\u805a\u7c7b\u6295\u5f71\u5668\u548c\u4f4d\u7f6e\u89e3\u8026\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u4e13\u6709\u6a21\u578b\u7684\u4e0d\u900f\u660e\u6027\u548cSSL\u805a\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u5f00\u6e90\u9ad8\u6027\u80fd\u89c6\u89c9\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u900f\u660e\u8bad\u7ec3\u6d41\u7a0b\u548c\u516c\u5f00\u6570\u636e\uff0c\u63d0\u51fa\u591a\u5934\u805a\u7c7b\u6295\u5f71\u5668\u548c\u4f4d\u7f6e\u89e3\u8026\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u4e0e\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "Franca\u4e3a\u900f\u660e\u9ad8\u6027\u80fd\u89c6\u89c9\u6a21\u578b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u63a8\u52a8\u4e86\u53ef\u590d\u73b0\u548c\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.12898", "pdf": "https://arxiv.org/pdf/2507.12898", "abs": "https://arxiv.org/abs/2507.12898", "authors": ["Yao Feng", "Hengkai Tan", "Xinyi Mao", "Guodong Liu", "Shuhe Huang", "Chendong Xiang", "Hang Su", "Jun Zhu"], "title": "Generalist Bimanual Manipulation via Foundation Video Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Bimanual robotic manipulation, which involves the coordinated control of two\nrobotic arms, is foundational for solving challenging tasks. Despite recent\nprogress in general-purpose manipulation, data scarcity and embodiment\nheterogeneity remain serious obstacles to further scaling up in bimanual\nsettings. In this paper, we introduce VIdeo Diffusion for Action Reasoning\n(VIDAR), a two-stage framework that leverages large-scale, diffusion-based\nvideo pre-training and a novel masked inverse dynamics model for action\nprediction. We pre-train the video diffusion model on 750K multi-view videos\nfrom three real-world bimanual robot platforms, utilizing a unified observation\nspace that encodes robot, camera, task, and scene contexts. Our masked inverse\ndynamics model learns masks to extract action-relevant information from\ngenerated trajectories without requiring pixel-level labels, and the masks can\neffectively generalize to unseen backgrounds. Our experiments demonstrate that\nwith only 20 minutes of human demonstrations on an unseen robot platform (only\n1% of typical data requirements), VIDAR generalizes to unseen tasks and\nbackgrounds with strong semantic understanding, surpassing state-of-the-art\nmethods. Our findings highlight the potential of video foundation models,\ncoupled with masked action prediction, to enable scalable and generalizable\nrobotic manipulation in diverse real-world settings.", "AI": {"tldr": "VIDAR\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u63a9\u7801\u9006\u5411\u52a8\u529b\u5b66\u6a21\u578b\u89e3\u51b3\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u6784\u6027\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faVIDAR\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u89c6\u9891\u9884\u8bad\u7ec3\u548c\u63a9\u7801\u9006\u5411\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002", "result": "\u4ec5\u970020\u5206\u949f\u4eba\u7c7b\u6f14\u793a\uff0cVIDAR\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u548c\u80cc\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u89c6\u9891\u57fa\u7840\u6a21\u578b\u4e0e\u63a9\u7801\u52a8\u4f5c\u9884\u6d4b\u7ed3\u5408\uff0c\u6709\u671b\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2507.13366", "pdf": "https://arxiv.org/pdf/2507.13366", "abs": "https://arxiv.org/abs/2507.13366", "authors": ["Baoshen Guo", "Zhiqing Hong", "Junyi Li", "Shenhao Wang", "Jinhua Zhao"], "title": "Leveraging the Spatial Hierarchy: Coarse-to-fine Trajectory Generation via Cascaded Hybrid Diffusion", "categories": ["cs.SI", "cs.CV"], "comment": null, "summary": "Urban mobility data has significant connections with economic growth and\nplays an essential role in various smart-city applications. However, due to\nprivacy concerns and substantial data collection costs, fine-grained human\nmobility trajectories are difficult to become publicly available on a large\nscale. A promising solution to address this issue is trajectory synthesizing.\nHowever, existing works often ignore the inherent structural complexity of\ntrajectories, unable to handle complicated high-dimensional distributions and\ngenerate realistic fine-grained trajectories. In this paper, we propose\nCardiff, a coarse-to-fine Cascaded hybrid diffusion-based trajectory\nsynthesizing framework for fine-grained and privacy-preserving mobility\ngeneration. By leveraging the hierarchical nature of urban mobility, Cardiff\ndecomposes the generation process into two distinct levels, i.e., discrete road\nsegment-level and continuous fine-grained GPS-level: (i) In the segment-level,\nto reduce computational costs and redundancy in raw trajectories, we first\nencode the discrete road segments into low-dimensional latent embeddings and\ndesign a diffusion transformer-based latent denoising network for segment-level\ntrajectory synthesis. (ii) Taking the first stage of generation as conditions,\nwe then design a fine-grained GPS-level conditional denoising network with a\nnoise augmentation mechanism to achieve robust and high-fidelity generation.\nAdditionally, the Cardiff framework not only progressively generates\nhigh-fidelity trajectories through cascaded denoising but also flexibly enables\na tunable balance between privacy preservation and utility. Experimental\nresults on three large real-world trajectory datasets demonstrate that our\nmethod outperforms state-of-the-art baselines in various metrics.", "AI": {"tldr": "Cardiff\u662f\u4e00\u79cd\u57fa\u4e8e\u7ea7\u8054\u6df7\u5408\u6269\u6563\u7684\u8f68\u8ff9\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u79fb\u52a8\u6027\u751f\u6210\uff0c\u901a\u8fc7\u5206\u5c42\u6b21\u751f\u6210\u8f68\u8ff9\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u95ee\u9898\u548c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\uff0c\u7ec6\u7c92\u5ea6\u7684\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\u96be\u4ee5\u5927\u89c4\u6a21\u516c\u5f00\u3002\u73b0\u6709\u8f68\u8ff9\u5408\u6210\u65b9\u6cd5\u5e38\u5ffd\u7565\u8f68\u8ff9\u7684\u7ed3\u6784\u590d\u6742\u6027\uff0c\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u5206\u5e03\u6216\u751f\u6210\u771f\u5b9e\u7684\u7ec6\u7c92\u5ea6\u8f68\u8ff9\u3002", "method": "Cardiff\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u7ea7\u8054\u65b9\u6cd5\uff0c\u5206\u4e24\u9636\u6bb5\u751f\u6210\u8f68\u8ff9\uff1a\u79bb\u6563\u8def\u6bb5\u7ea7\u548c\u8fde\u7eedGPS\u7ea7\u3002\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6269\u6563\u53d8\u6362\u5668\u8fdb\u884c\u6f5c\u5728\u53bb\u566a\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u566a\u58f0\u589e\u5f3a\u673a\u5236\u5b9e\u73b0\u9ad8\u4fdd\u771f\u751f\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCardiff\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Cardiff\u4e0d\u4ec5\u901a\u8fc7\u7ea7\u8054\u53bb\u566a\u9010\u6b65\u751f\u6210\u9ad8\u4fdd\u771f\u8f68\u8ff9\uff0c\u8fd8\u80fd\u7075\u6d3b\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.13367", "pdf": "https://arxiv.org/pdf/2507.13367", "abs": "https://arxiv.org/abs/2507.13367", "authors": ["Mehrab Hosain", "Rajiv Kapoor"], "title": "A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security", "categories": ["cs.CR", "cs.CV", "cs.MM", "eess.IV", "68Q80", "I.4.2"], "comment": "Accepted COMITCON 2023. Lecture Notes in Electrical Engineering, vol\n  1191. Springer", "summary": "Steganography is the process of embedding secret information discreetly\nwithin a carrier, ensuring secure exchange of confidential data. The Adaptive\nPixel Value Differencing (APVD) steganography method, while effective,\nencounters certain challenges like the \"unused blocks\" issue. This problem can\ncause a decrease in security, compromise the embedding capacity, and lead to\nlower visual quality. This research presents a novel steganographic strategy\nthat integrates APVD with pseudorandom pixel selection to effectively mitigate\nthese issues. The results indicate that the new method outperforms existing\ntechniques in aspects of security, data hiding capacity, and the preservation\nof image quality. Empirical results reveal that the combination of APVD with\npseudorandom pixel selection significantly enhances key image quality metrics\nsuch as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ),\nand Structural Similarity Index (SSIM), surpassing other contemporary methods\nin performance. The newly proposed method is versatile, able to handle a\nvariety of cover and secret images in both color and grayscale, thereby\nensuring secure data transmission without compromising the aesthetic quality of\nthe image.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408APVD\u548c\u4f2a\u968f\u673a\u50cf\u7d20\u9009\u62e9\u7684\u65b0\u578b\u9690\u5199\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfAPVD\u4e2d\u7684\u201c\u672a\u4f7f\u7528\u5757\u201d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfAPVD\u65b9\u6cd5\u5b58\u5728\u201c\u672a\u4f7f\u7528\u5757\u201d\u95ee\u9898\uff0c\u5bfc\u81f4\u5b89\u5168\u6027\u964d\u4f4e\u3001\u5d4c\u5165\u5bb9\u91cf\u53d7\u9650\u548c\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u7ed3\u5408APVD\u4e0e\u4f2a\u968f\u673a\u50cf\u7d20\u9009\u62e9\uff0c\u4f18\u5316\u50cf\u7d20\u5d4c\u5165\u7b56\u7565\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u56fe\u50cf\u8d28\u91cf\uff08PSNR\u3001UIQ\u3001SSIM\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u7c7b\u578b\uff0c\u786e\u4fdd\u5b89\u5168\u4f20\u8f93\u4e14\u4e0d\u635f\u5bb3\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2507.13377", "pdf": "https://arxiv.org/pdf/2507.13377", "abs": "https://arxiv.org/abs/2507.13377", "authors": ["Zhenglin Pan", "Haoran Xie"], "title": "StructInbet: Integrating Explicit Structural Guidance into Inbetween Frame Generation", "categories": ["cs.GR", "cs.CV"], "comment": "3 pages, 3 figures. SIGGRAPH 2025 Poster", "summary": "In this paper, we propose StructInbet, an inbetweening system designed to\ngenerate controllable transitions over explicit structural guidance.\nStructInbet introduces two key contributions. First, we propose explicit\nstructural guidance to the inbetweening problem to reduce the ambiguity\ninherent in pixel trajectories. Second, we adopt a temporal attention mechanism\nthat incorporates visual identity from both the preceding and succeeding\nkeyframes, ensuring consistency in character appearance.", "AI": {"tldr": "StructInbet\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u5f0f\u7ed3\u6784\u5f15\u5bfc\u7684\u4e2d\u95f4\u5e27\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u6307\u5bfc\u548c\u65f6\u5e8f\u6ce8\u610f\u529b\u673a\u5236\u63d0\u9ad8\u8fc7\u6e21\u5e27\u7684\u53ef\u63a7\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u4e2d\u95f4\u5e27\u751f\u6210\u4e2d\u50cf\u7d20\u8f68\u8ff9\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5e76\u786e\u4fdd\u89d2\u8272\u5916\u89c2\u7684\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165\u663e\u5f0f\u7ed3\u6784\u6307\u5bfc\u548c\u65f6\u5e8f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u524d\u540e\u5173\u952e\u5e27\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u751f\u6210\u53ef\u63a7\u4e14\u4e00\u81f4\u7684\u8fc7\u6e21\u5e27\uff0c\u51cf\u5c11\u4e86\u6a21\u7cca\u6027\u3002", "conclusion": "StructInbet\u901a\u8fc7\u7ed3\u6784\u6307\u5bfc\u548c\u65f6\u5e8f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4e2d\u95f4\u5e27\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2507.13383", "pdf": "https://arxiv.org/pdf/2507.13383", "abs": "https://arxiv.org/abs/2507.13383", "authors": ["Charvi Rastogi", "Tian Huey Teh", "Pushkar Mishra", "Roma Patel", "Ding Wang", "Mark D\u00edaz", "Alicia Parrish", "Aida Mostafazadeh Davani", "Zoe Ashwood", "Michela Paganini", "Vinodkumar Prabhakaran", "Verena Rieser", "Lora Aroyo"], "title": "Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "28 pages, 16 figures", "summary": "Current text-to-image (T2I) models often fail to account for diverse human\nexperiences, leading to misaligned systems. We advocate for pluralistic\nalignment, where an AI understands and is steerable towards diverse, and often\nconflicting, human values. Our work provides three core contributions to\nachieve this in T2I models. First, we introduce a novel dataset for Diverse\nIntersectional Visual Evaluation (DIVE) -- the first multimodal dataset for\npluralistic alignment. It enable deep alignment to diverse safety perspectives\nthrough a large pool of demographically intersectional human raters who\nprovided extensive feedback across 1000 prompts, with high replication,\ncapturing nuanced safety perceptions. Second, we empirically confirm\ndemographics as a crucial proxy for diverse viewpoints in this domain,\nrevealing significant, context-dependent differences in harm perception that\ndiverge from conventional evaluations. Finally, we discuss implications for\nbuilding aligned T2I models, including efficient data collection strategies,\nLLM judgment capabilities, and model steerability towards diverse perspectives.\nThis research offers foundational tools for more equitable and aligned T2I\nsystems. Content Warning: The paper includes sensitive content that may be\nharmful.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5143\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7DIVE\u6570\u636e\u96c6\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u89c6\u89d2\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u591a\u6837\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524dT2I\u6a21\u578b\u672a\u80fd\u5145\u5206\u8003\u8651\u4eba\u7c7b\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e0d\u4e00\u81f4\u3002", "method": "\u5f15\u5165DIVE\u6570\u636e\u96c6\uff0c\u5229\u7528\u591a\u5143\u4eba\u53e3\u7edf\u8ba1\u5b66\u89c6\u89d2\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u6570\u636e\u6536\u96c6\u7b56\u7565\u548c\u6a21\u578b\u53ef\u64cd\u63a7\u6027\u3002", "result": "\u8bc1\u5b9e\u4eba\u53e3\u7edf\u8ba1\u5b66\u662f\u591a\u6837\u89c2\u70b9\u7684\u91cd\u8981\u4ee3\u7406\uff0c\u63ed\u793a\u4e86\u4e0e\u4f20\u7edf\u8bc4\u4f30\u4e0d\u540c\u7684\u5371\u5bb3\u611f\u77e5\u5dee\u5f02\u3002", "conclusion": "\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u548c\u5bf9\u9f50\u7684T2I\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u5de5\u5177\u3002"}}
{"id": "2507.13384", "pdf": "https://arxiv.org/pdf/2507.13384", "abs": "https://arxiv.org/abs/2507.13384", "authors": ["Osama Hardan", "Omar Elshenhabi", "Tamer Khattab", "Mohamed Mabrok"], "title": "Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Submitted to the 2025 IEEE International Conference on Future Machine\n  Learning and Data Science (FMLDS)", "summary": "Vision Mamba models promise transformer-level performance at linear\ncomputational cost, but their reliance on serializing 2D images into 1D\nsequences introduces a critical, yet overlooked, design choice: the patch scan\norder. In medical imaging, where modalities like brain MRI contain strong\nanatomical priors, this choice is non-trivial. This paper presents the first\nsystematic study of how scan order impacts MRI segmentation. We introduce\nMulti-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures\nthat facilitates exploring diverse scan paths without additional computational\ncost. We conduct a large-scale benchmark of 21 scan strategies on three public\ndatasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our\nanalysis shows conclusively that scan order is a statistically significant\nfactor (Friedman test: $\\chi^{2}_{20}=43.9, p=0.0016$), with performance\nvarying by as much as 27 Dice points. Spatially contiguous paths -- simple\nhorizontal and vertical rasters -- consistently outperform disjointed diagonal\nscans. We conclude that scan order is a powerful, cost-free hyperparameter, and\nprovide an evidence-based shortlist of optimal paths to maximize the\nperformance of Mamba models in medical imaging.", "AI": {"tldr": "\u7814\u7a76\u4e86Vision Mamba\u6a21\u578b\u4e2d\u56fe\u50cf\u626b\u63cf\u987a\u5e8f\u5bf9MRI\u5206\u5272\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u65e0\u5173\u7684\u591a\u626b\u63cf\u6a21\u5757MS2D\uff0c\u53d1\u73b0\u626b\u63cf\u987a\u5e8f\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "Vision Mamba\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u4f9d\u8d561D\u5e8f\u5217\u53162D\u56fe\u50cf\uff0c\u4f46\u626b\u63cf\u987a\u5e8f\u7684\u8bbe\u8ba1\u9009\u62e9\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u5f3a\u89e3\u5256\u5148\u9a8c\u7684MRI\u4e2d\u3002", "method": "\u63d0\u51faMS2D\u6a21\u5757\uff0c\u8bc4\u4f3021\u79cd\u626b\u63cf\u7b56\u7565\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u4f7f\u7528Friedman\u68c0\u9a8c\u5206\u6790\u7edf\u8ba1\u663e\u8457\u6027\u3002", "result": "\u626b\u63cf\u987a\u5e8f\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff08Dice\u5206\u6570\u5dee\u5f02\u8fbe27\u70b9\uff09\uff0c\u7a7a\u95f4\u8fde\u7eed\u7684\u626b\u63cf\u8def\u5f84\uff08\u5982\u6c34\u5e73\u6216\u5782\u76f4\uff09\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u626b\u63cf\u987a\u5e8f\u662fVision Mamba\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u8d85\u53c2\u6570\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u4f18\u5316\u8def\u5f84\u7684\u5b9e\u8bc1\u5efa\u8bae\u3002"}}
{"id": "2507.13394", "pdf": "https://arxiv.org/pdf/2507.13394", "abs": "https://arxiv.org/abs/2507.13394", "authors": ["Akhil John Thomas", "Christiaan Boerkamp"], "title": "Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Nerve segmentation is crucial in medical imaging for precise identification\nof nerve structures. This study presents an optimized DeepLabV3-based\nsegmentation pipeline that incorporates automated threshold fine-tuning to\nimprove segmentation accuracy. By refining preprocessing steps and implementing\nparameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a\nPixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate\nsignificant improvements over baseline models and highlight the importance of\ntailored parameter selection in automated nerve detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDeepLabV3\u7684\u4f18\u5316\u795e\u7ecf\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u9608\u503c\u5fae\u8c03\u548c\u53c2\u6570\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u58f0\u795e\u7ecf\u56fe\u50cf\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u795e\u7ecf\u5206\u5272\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5bf9\u7cbe\u786e\u8bc6\u522b\u795e\u7ecf\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5272\u7cbe\u5ea6\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u91c7\u7528DeepLabV3\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u52a8\u9608\u503c\u5fae\u8c03\u548c\u53c2\u6570\u4f18\u5316\uff0c\u6539\u8fdb\u4e86\u9884\u5904\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u8d85\u58f0\u795e\u7ecf\u56fe\u50cf\u4e0a\u53d6\u5f97\u4e86Dice Score 0.78\u3001IoU 0.70\u548cPixel Accuracy 0.95\u7684\u4f18\u5f02\u7ed3\u679c\u3002", "conclusion": "\u4f18\u5316\u540e\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5f3a\u8c03\u4e86\u53c2\u6570\u9009\u62e9\u5728\u81ea\u52a8\u5316\u795e\u7ecf\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.13458", "pdf": "https://arxiv.org/pdf/2507.13458", "abs": "https://arxiv.org/abs/2507.13458", "authors": ["Malte Hoffmann"], "title": "Domain-randomized deep learning for neuroimage analysis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "12 pages, 6 figures, 2 tables, deep learning, domain generalization,\n  domain randomization, neuroimaging, medical image analysis, accepted for\n  publication in IEEE Signal Processing Magazine", "summary": "Deep learning has revolutionized neuroimage analysis by delivering\nunprecedented speed and accuracy. However, the narrow scope of many training\ndatasets constrains model robustness and generalizability. This challenge is\nparticularly acute in magnetic resonance imaging (MRI), where image appearance\nvaries widely across pulse sequences and scanner hardware. A recent\ndomain-randomization strategy addresses the generalization problem by training\ndeep neural networks on synthetic images with randomized intensities and\nanatomical content. By generating diverse data from anatomical segmentation\nmaps, the approach enables models to accurately process image types unseen\nduring training, without retraining or fine-tuning. It has demonstrated\neffectiveness across modalities including MRI, computed tomography, positron\nemission tomography, and optical coherence tomography, as well as beyond\nneuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray\nmicrotomography. This tutorial paper reviews the principles, implementation,\nand potential of the synthesis-driven training paradigm. It highlights key\nbenefits, such as improved generalization and resistance to overfitting, while\ndiscussing trade-offs such as increased computational demands. Finally, the\narticle explores practical considerations for adopting the technique, aiming to\naccelerate the development of generalizable tools that make deep learning more\naccessible to domain experts without extensive computational resources or\nmachine learning knowledge.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u901a\u8fc7\u5408\u6210\u591a\u6837\u5316\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6210\u50cf\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u4e2d\u56e0\u8bad\u7ec3\u6570\u636e\u8303\u56f4\u72ed\u7a84\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57df\u968f\u673a\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5408\u6210\u968f\u673a\u5f3a\u5ea6\u548c\u5185\u5bb9\u7684\u56fe\u50cf\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5904\u7406\u672a\u89c1\u8fc7\u7684\u56fe\u50cf\u7c7b\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6210\u50cf\u6280\u672f\u3002", "conclusion": "\u5408\u6210\u9a71\u52a8\u8bad\u7ec3\u8303\u5f0f\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u9700\u6743\u8861\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u6709\u671b\u52a0\u901f\u901a\u7528\u5de5\u5177\u7684\u5f00\u53d1\u3002"}}
{"id": "2507.13480", "pdf": "https://arxiv.org/pdf/2507.13480", "abs": "https://arxiv.org/abs/2507.13480", "authors": ["Sara Avesani", "Gianluca Giacchi", "Michael Multerer"], "title": "Multiresolution local smoothness detection in non-uniformly sampled multivariate signals", "categories": ["math.NA", "cs.CV", "cs.LG", "cs.NA"], "comment": null, "summary": "Inspired by edge detection based on the decay behavior of wavelet\ncoefficients, we introduce a (near) linear-time algorithm for detecting the\nlocal regularity in non-uniformly sampled multivariate signals. Our approach\nquantifies regularity within the framework of microlocal spaces introduced by\nJaffard. The central tool in our analysis is the fast samplet transform, a\ndistributional wavelet transform tailored to scattered data. We establish a\nconnection between the decay of samplet coefficients and the pointwise\nregularity of multivariate signals. As a by product, we derive decay estimates\nfor functions belonging to classical H\\\"older spaces and Sobolev-Slobodeckij\nspaces. While traditional wavelets are effective for regularity detection in\nlow-dimensional structured data, samplets demonstrate robust performance even\nfor higher dimensional and scattered data. To illustrate our theoretical\nfindings, we present extensive numerical studies detecting local regularity of\none-, two- and three-dimensional signals, ranging from non-uniformly sampled\ntime series over image segmentation to edge detection in point clouds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7cfb\u6570\u8870\u51cf\u884c\u4e3a\u7684\u8fb9\u7f18\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u5747\u5300\u91c7\u6837\u591a\u5143\u4fe1\u53f7\u7684\u5c40\u90e8\u89c4\u5f8b\u6027\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5feb\u901f\u6837\u672c\u53d8\u6362\uff08samplet transform\uff09\u5206\u6790\u4fe1\u53f7\u7684\u70b9\u6001\u89c4\u5f8b\u6027\uff0c\u5e76\u5728\u591a\u7ef4\u548c\u6563\u4e71\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u5c0f\u6ce2\u65b9\u6cd5\u5728\u4f4e\u7ef4\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9ad8\u7ef4\u548c\u975e\u5747\u5300\u91c7\u6837\u6570\u636e\u4e2d\u6548\u679c\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u9002\u7528\u4e8e\u591a\u7ef4\u548c\u6563\u4e71\u6570\u636e\u7684\u89c4\u5f8b\u6027\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5feb\u901f\u6837\u672c\u53d8\u6362\uff08samplet transform\uff09\u5206\u6790\u4fe1\u53f7\u7684\u70b9\u6001\u89c4\u5f8b\u6027\uff0c\u5efa\u7acb\u6837\u672c\u7cfb\u6570\u8870\u51cf\u4e0e\u4fe1\u53f7\u89c4\u5f8b\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u7ef4\u548c\u975e\u5747\u5300\u91c7\u6837\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e00\u7ef4\u3001\u4e8c\u7ef4\u548c\u4e09\u7ef4\u4fe1\u53f7\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6837\u672c\u53d8\u6362\u4e3a\u9ad8\u7ef4\u548c\u6563\u4e71\u6570\u636e\u7684\u89c4\u5f8b\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u5c0f\u6ce2\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.13482", "pdf": "https://arxiv.org/pdf/2507.13482", "abs": "https://arxiv.org/abs/2507.13482", "authors": ["Seyyed Saeid Cheshmi", "Buyao Lyu", "Thomas Lisko", "Rajesh Rajamani", "Robert A. McGovern", "Yogatheesan Varatharajah"], "title": "Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Human Activity Recognition (HAR) based on wearable inertial sensors plays a\ncritical role in remote health monitoring. In patients with movement disorders,\nthe ability to detect abnormal patient movements in their home environments can\nenable continuous optimization of treatments and help alert caretakers as\nneeded. Machine learning approaches have been proposed for HAR tasks using\nInertial Measurement Unit (IMU) data; however, most rely on\napplication-specific labels and lack generalizability to data collected in\ndifferent environments or populations. To address this limitation, we propose a\nnew cross-modal self-supervised pretraining approach to learn representations\nfrom large-sale unlabeled IMU-video data and demonstrate improved\ngeneralizability in HAR tasks on out of distribution (OOD) IMU datasets,\nincluding a dataset collected from patients with Parkinson's disease.\nSpecifically, our results indicate that the proposed cross-modal pretraining\napproach outperforms the current state-of-the-art IMU-video pretraining\napproach and IMU-only pretraining under zero-shot and few-shot evaluations.\nBroadly, our study provides evidence that in highly dynamic data modalities,\nsuch as IMU signals, cross-modal pretraining may be a useful tool to learn\ngeneralizable data representations. Our software is available at\nhttps://github.com/scheshmi/IMU-Video-OOD-HAR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u7684IMU-\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u5728\u5206\u5e03\u5916IMU\u6570\u636e\u96c6\u4e0a\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u60ef\u6027\u4f20\u611f\u5668\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u5e94\u7528\u6807\u7b7e\u3001\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u6807\u8bb0\u7684IMU-\u89c6\u9891\u6570\u636e\u5b66\u4e60\u901a\u7528\u8868\u793a\u3002", "result": "\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684IMU-\u89c6\u9891\u9884\u8bad\u7ec3\u548c\u4ec5IMU\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u662f\u5b66\u4e60\u901a\u7528\u6570\u636e\u8868\u793a\u7684\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u6570\u636e\u6a21\u6001\uff08\u5982IMU\u4fe1\u53f7\uff09\u4e2d\u3002"}}
{"id": "2507.13485", "pdf": "https://arxiv.org/pdf/2507.13485", "abs": "https://arxiv.org/abs/2507.13485", "authors": ["Imane Hamzaoui", "Riyadh Baghdadi"], "title": "Neural Architecture Search with Mixed Bio-inspired Learning Rules", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "comment": "ECAI 2025", "summary": "Bio-inspired neural networks are attractive for their adversarial robustness,\nenergy frugality, and closer alignment with cortical physiology, yet they often\nlag behind back-propagation (BP) based models in accuracy and ability to scale.\nWe show that allowing the use of different bio-inspired learning rules in\ndifferent layers, discovered automatically by a tailored\nneural-architecture-search (NAS) procedure, bridges this gap. Starting from\nstandard NAS baselines, we enlarge the search space to include bio-inspired\nlearning rules and use NAS to find the best architecture and learning rule to\nuse in each layer. We show that neural networks that use different bio-inspired\nlearning rules for different layers have better accuracy than those that use a\nsingle rule across all the layers. The resulting NN that uses a mix of\nbio-inspired learning rules sets new records for bio-inspired models: 95.16% on\nCIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on\nImageNet. In some regimes, they even surpass comparable BP-based networks while\nretaining their robustness advantages. Our results suggest that layer-wise\ndiversity in learning rules allows better scalability and accuracy, and\nmotivates further research on mixing multiple bio-inspired learning rules in\nthe same network.", "AI": {"tldr": "\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u81ea\u52a8\u53d1\u73b0\u5e76\u6df7\u5408\u4f7f\u7528\u4e0d\u540c\u751f\u7269\u542f\u53d1\u5b66\u4e60\u89c4\u5219\uff0c\u63d0\u5347\u751f\u7269\u542f\u53d1\u795e\u7ecf\u7f51\u7edc\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u751f\u7269\u542f\u53d1\u795e\u7ecf\u7f51\u7edc\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u80fd\u91cf\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u843d\u540e\u4e8e\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u7684\u6a21\u578b\u3002", "method": "\u6269\u5c55NAS\u641c\u7d22\u7a7a\u95f4\u4ee5\u5305\u542b\u751f\u7269\u542f\u53d1\u5b66\u4e60\u89c4\u5219\uff0c\u81ea\u52a8\u4e3a\u6bcf\u5c42\u9009\u62e9\u6700\u4f73\u67b6\u6784\u548c\u5b66\u4e60\u89c4\u5219\u3002", "result": "\u6df7\u5408\u4f7f\u7528\u4e0d\u540c\u5b66\u4e60\u89c4\u5219\u7684\u7f51\u7edc\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u521b\u4e0b\u751f\u7269\u542f\u53d1\u6a21\u578b\u7684\u65b0\u8bb0\u5f55\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u751a\u81f3\u8d85\u8d8aBP\u7f51\u7edc\u3002", "conclusion": "\u5c42\u95f4\u5b66\u4e60\u89c4\u5219\u7684\u591a\u6837\u6027\u6709\u52a9\u4e8e\u63d0\u5347\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u6df7\u5408\u5b66\u4e60\u89c4\u5219\u7684\u5e94\u7528\u3002"}}
{"id": "2507.13586", "pdf": "https://arxiv.org/pdf/2507.13586", "abs": "https://arxiv.org/abs/2507.13586", "authors": ["Kaiyuan Tang", "Kuangshi Ai", "Jun Han", "Chaoli Wang"], "title": "TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting", "categories": ["cs.GR", "cs.CL", "cs.CV"], "comment": "Accepted by IEEE VIS 2025", "summary": "Advancements in volume visualization (VolVis) focus on extracting insights\nfrom 3D volumetric data by generating visually compelling renderings that\nreveal complex internal structures. Existing VolVis approaches have explored\nnon-photorealistic rendering techniques to enhance the clarity, expressiveness,\nand informativeness of visual communication. While effective, these methods\noften rely on complex predefined rules and are limited to transferring a single\nstyle, restricting their flexibility. To overcome these limitations, we\nadvocate the representation of VolVis scenes using differentiable Gaussian\nprimitives combined with pretrained large models to enable arbitrary style\ntransfer and real-time rendering. However, conventional 3D Gaussian primitives\ntightly couple geometry and appearance, leading to suboptimal stylization\nresults. To address this, we introduce TexGS-VolVis, a textured Gaussian\nsplatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives,\nextending each Gaussian with additional texture and shading attributes,\nresulting in higher-quality, geometry-consistent stylization and enhanced\nlighting control during inference. Despite these improvements, achieving\nflexible and controllable scene editing remains challenging. To further enhance\nstylization, we develop image- and text-driven non-photorealistic scene editing\ntailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing\nwith fine-grained control. We evaluate TexGS-VolVis both qualitatively and\nquantitatively across various volume rendering scenes, demonstrating its\nsuperiority over existing methods in terms of efficiency, visual quality, and\nediting flexibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTexGS-VolVis\u6846\u67b6\uff0c\u901a\u8fc7\u7eb9\u7406\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u6539\u8fdb\u4f53\u79ef\u53ef\u89c6\u5316\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u51e0\u4f55\u4e00\u81f4\u7684\u98ce\u683c\u5316\u548c\u7075\u6d3b\u7684\u573a\u666f\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u4f53\u79ef\u53ef\u89c6\u5316\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u9884\u5b9a\u4e49\u89c4\u5219\u4e14\u98ce\u683c\u5355\u4e00\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u6548\u679c\u3002", "method": "\u91c7\u7528\u7eb9\u7406\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff08TexGS-VolVis\uff09\uff0c\u7ed3\u54082D\u9ad8\u65af\u57fa\u5143\u548c\u989d\u5916\u7eb9\u7406\u5c5e\u6027\uff0c\u652f\u6301\u56fe\u50cf\u548c\u6587\u672c\u9a71\u52a8\u7684\u975e\u771f\u5b9e\u611f\u7f16\u8f91\u3002", "result": "TexGS-VolVis\u5728\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u7075\u6d3b\u7684\u573a\u666f\u7f16\u8f91\u3002", "conclusion": "TexGS-VolVis\u4e3a\u4f53\u79ef\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13598", "pdf": "https://arxiv.org/pdf/2507.13598", "abs": "https://arxiv.org/abs/2507.13598", "authors": ["Amro Abdalla", "Ismail Shaheen", "Dan DeGenaro", "Rupayan Mallick", "Bogdan Raita", "Sarah Adel Bargal"], "title": "GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Warning: This paper contains NSFW content. Reader discretion is\n  advised", "summary": "We present GIFT: a {G}radient-aware {I}mmunization technique to defend\ndiffusion models against malicious {F}ine-{T}uning while preserving their\nability to generate safe content. Existing safety mechanisms like safety\ncheckers are easily bypassed, and concept erasure methods fail under\nadversarial fine-tuning. GIFT addresses this by framing immunization as a\nbi-level optimization problem: the upper-level objective degrades the model's\nability to represent harmful concepts using representation noising and\nmaximization, while the lower-level objective preserves performance on safe\ndata. GIFT achieves robust resistance to malicious fine-tuning while\nmaintaining safe generative quality. Experimental results show that our method\nsignificantly impairs the model's ability to re-learn harmful concepts while\nmaintaining performance on safe content, offering a promising direction for\ncreating inherently safer generative models resistant to adversarial\nfine-tuning attacks.", "AI": {"tldr": "GIFT\u662f\u4e00\u79cd\u68af\u5ea6\u611f\u77e5\u514d\u75ab\u6280\u672f\uff0c\u7528\u4e8e\u9632\u5fa1\u6269\u6563\u6a21\u578b\u514d\u53d7\u6076\u610f\u5fae\u8c03\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u751f\u6210\u5b89\u5168\u5185\u5bb9\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u673a\u5236\uff08\u5982\u5b89\u5168\u68c0\u67e5\u5668\uff09\u6613\u88ab\u7ed5\u8fc7\uff0c\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5728\u5bf9\u6297\u6027\u5fae\u8c03\u4e0b\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "GIFT\u5c06\u514d\u75ab\u95ee\u9898\u5efa\u6a21\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff1a\u4e0a\u5c42\u76ee\u6807\u901a\u8fc7\u8868\u793a\u566a\u58f0\u548c\u6700\u5927\u5316\u964d\u4f4e\u6709\u5bb3\u6982\u5ff5\u7684\u8868\u793a\u80fd\u529b\uff0c\u4e0b\u5c42\u76ee\u6807\u4fdd\u7559\u5b89\u5168\u6570\u636e\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGIFT\u663e\u8457\u524a\u5f31\u6a21\u578b\u91cd\u65b0\u5b66\u4e60\u6709\u5bb3\u6982\u5ff5\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u5185\u5bb9\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "GIFT\u4e3a\u6784\u5efa\u6297\u5bf9\u6297\u6027\u5fae\u8c03\u653b\u51fb\u7684\u5b89\u5168\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2507.13604", "pdf": "https://arxiv.org/pdf/2507.13604", "abs": "https://arxiv.org/abs/2507.13604", "authors": ["Qihang Li", "Jichen Yang", "Yaqian Chen", "Yuwen Chen", "Hanxue Gu", "Lars J. Grimm", "Maciej A. Mazurowski"], "title": "BreastSegNet: Multi-label Segmentation of Breast MRI", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Breast MRI provides high-resolution imaging critical for breast cancer\nscreening and preoperative staging. However, existing segmentation methods for\nbreast MRI remain limited in scope, often focusing on only a few anatomical\nstructures, such as fibroglandular tissue or tumors, and do not cover the full\nrange of tissues seen in scans. This narrows their utility for quantitative\nanalysis. In this study, we present BreastSegNet, a multi-label segmentation\nalgorithm for breast MRI that covers nine anatomical labels: fibroglandular\ntissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and\nimplant. We manually annotated a large set of 1123 MRI slices capturing these\nstructures with detailed review and correction from an expert radiologist.\nAdditionally, we benchmark nine segmentation models, including U-Net, SwinUNet,\nUNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among\nthem, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across\nall labels. It performs especially well on heart, liver, muscle, FGT, and bone,\nwith Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All\nmodel code and weights are publicly available, and we plan to release the data\nat a later date.", "AI": {"tldr": "BreastSegNet\u662f\u4e00\u79cd\u591a\u6807\u7b7e\u5206\u5272\u7b97\u6cd5\uff0c\u7528\u4e8e\u4e73\u817aMRI\uff0c\u8986\u76d69\u79cd\u89e3\u5256\u7ed3\u6784\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u4e73\u817aMRI\u5206\u5272\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5c11\u6570\u89e3\u5256\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5b9a\u91cf\u5206\u6790\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faBreastSegNet\uff0c\u624b\u52a8\u6807\u6ce81123\u5f20MRI\u5207\u7247\uff0c\u5e76\u5bf9\u6bd49\u79cd\u5206\u5272\u6a21\u578b\u3002", "result": "nnU-Net ResEncM\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747Dice\u5206\u65700.694\uff0c\u90e8\u5206\u7ed3\u6784\u5206\u6570\u63a5\u8fd10.90\u3002", "conclusion": "BreastSegNet\u6269\u5c55\u4e86\u4e73\u817aMRI\u5206\u5272\u7684\u8303\u56f4\uff0c\u6a21\u578b\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.13782", "pdf": "https://arxiv.org/pdf/2507.13782", "abs": "https://arxiv.org/abs/2507.13782", "authors": ["Malo Gicquel", "Ruoyi Zhao", "Anika Wuestefeld", "Nicola Spotorno", "Olof Strandberg", "Kalle \u00c5str\u00f6m", "Yu Xiao", "Laura EM Wisse", "Danielle van Westen", "Rik Ossenkoppele", "Niklas Mattsson-Carlgren", "David Berron", "Oskar Hansson", "Gabrielle Flood", "Jacob Vogel"], "title": "Converting T1-weighted MRI from 3T to 7T quality using deep learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides\ndetailed anatomical views, offering better signal-to-noise ratio, resolution\nand tissue contrast than 3T MRI, though at the cost of accessibility. We\npresent an advanced deep learning model for synthesizing 7T brain MRI from 3T\nbrain MRI. Paired 7T and 3T T1-weighted images were acquired from 172\nparticipants (124 cognitively unimpaired, 48 impaired) from the Swedish\nBioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models:\na specialized U-Net, and a U-Net integrated with a generative adversarial\nnetwork (GAN U-Net). Our models outperformed two additional state-of-the-art\n3T-to-7T models in image-based evaluation metrics. Four blinded MRI\nprofessionals judged our synthetic 7T images as comparable in detail to real 7T\nimages, and superior in subjective visual quality to 7T images, apparently due\nto the reduction of artifacts. Importantly, automated segmentations of the\namygdalae of synthetic GAN U-Net 7T images were more similar to manually\nsegmented amygdalae (n=20), than automated segmentations from the 3T images\nthat were used to synthesize the 7T images. Finally, synthetic 7T images showed\nsimilar performance to real 3T images in downstream prediction of cognitive\nstatus using MRI derivatives (n=3,168). In all, we show that synthetic\nT1-weighted brain images approaching 7T quality can be generated from 3T\nimages, which may improve image quality and segmentation, without compromising\nperformance in downstream tasks. Future directions, possible clinical use\ncases, and limitations are discussed.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece3T MRI\u5408\u62107T MRI\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u5272\u6548\u679c\uff0c\u4e0d\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "7T MRI\u63d0\u4f9b\u66f4\u9ad8\u5206\u8fa8\u7387\u548c\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\uff0c\u4f46\u666e\u53ca\u6027\u53d7\u9650\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc73T MRI\u5408\u6210\u63a5\u8fd17T\u8d28\u91cf\u7684\u56fe\u50cf\u3002", "method": "\u8bad\u7ec3\u4e86\u4e24\u79cd\u6a21\u578b\uff1a\u4e13\u7528U-Net\u548c\u7ed3\u5408GAN\u7684U-Net\uff0c\u4f7f\u7528172\u540d\u53c2\u4e0e\u8005\u7684\u914d\u5bf93T\u548c7T T1\u52a0\u6743\u56fe\u50cf\u3002", "result": "\u5408\u62107T\u56fe\u50cf\u5728\u7ec6\u8282\u4e0a\u4e0e\u771f\u5b9e7T\u56fe\u50cf\u76f8\u5f53\uff0c\u4e3b\u89c2\u89c6\u89c9\u8d28\u91cf\u66f4\u4f18\uff0c\u4e14\u81ea\u52a8\u5206\u5272\u6548\u679c\u66f4\u63a5\u8fd1\u4eba\u5de5\u5206\u5272\u3002", "conclusion": "\u5408\u62107T\u56fe\u50cf\u53ef\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u5272\u6548\u679c\uff0c\u672a\u6765\u9700\u63a2\u8ba8\u4e34\u5e8a\u5e94\u7528\u7684\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2507.13802", "pdf": "https://arxiv.org/pdf/2507.13802", "abs": "https://arxiv.org/abs/2507.13802", "authors": ["Nehir Kizililsoley", "Floor van Meer", "Osman Mutlu", "Wouter F Hoenderdaal", "Rosan G. Hob\u00e9", "Wenjuan Mu", "Arjen Gerssen", "H. J. van der Fels-Klerx", "\u00c1kos J\u00f3\u017awiak", "Ioannis Manikas", "Ali H\u00fcrriyeto\u01e7lu", "Bas H. M. van der Velden"], "title": "Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database", "categories": ["cs.CY", "cs.AI", "cs.CV"], "comment": null, "summary": "In the European Union, official food safety monitoring data collected by\nmember states are submitted to the European Food Safety Authority (EFSA) and\npublished on Zenodo. This data includes 392 million analytical results derived\nfrom over 15.2 million samples covering more than 4,000 different types of food\nproducts, offering great opportunities for artificial intelligence to analyze\ntrends, predict hazards, and support early warning systems. However, the\ncurrent format with data distributed across approximately 1000 files totaling\nseveral hundred gigabytes hinders accessibility and analysis. To address this,\nwe introduce the CompreHensive European Food Safety (CHEFS) database, which\nconsolidates EFSA monitoring data on pesticide residues, veterinary medicinal\nproduct residues, and chemical contaminants into a unified and structured\ndataset. We describe the creation and structure of the CHEFS database and\ndemonstrate its potential by analyzing trends in European food safety\nmonitoring data from 2000 to 2024. Our analyses explore changes in monitoring\nactivities, the most frequently tested products, which products were most often\nnon-compliant and which contaminants were most often found, and differences\nacross countries. These findings highlight the CHEFS database as both a\ncentralized data source and a strategic tool for guiding food safety policy,\nresearch, and regulation.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86CHEFS\u6570\u636e\u5e93\uff0c\u6574\u5408\u4e86EFSA\u7684\u98df\u54c1\u5b89\u5168\u76d1\u6d4b\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5206\u6563\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5206\u6790\u6b27\u6d32\u98df\u54c1\u5b89\u5168\u8d8b\u52bf\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "EFSA\u7684\u98df\u54c1\u5b89\u5168\u76d1\u6d4b\u6570\u636e\u5206\u6563\u4e14\u96be\u4ee5\u8bbf\u95ee\uff0c\u963b\u788d\u4e86\u4eba\u5de5\u667a\u80fd\u5206\u6790\u8d8b\u52bf\u548c\u9884\u6d4b\u98ce\u9669\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efaCHEFS\u6570\u636e\u5e93\uff0c\u7edf\u4e00\u548c\u7ed3\u6784\u5316EFSA\u7684\u76d1\u6d4b\u6570\u636e\uff0c\u6db5\u76d6\u519c\u836f\u6b8b\u7559\u3001\u517d\u836f\u6b8b\u7559\u548c\u5316\u5b66\u6c61\u67d3\u7269\u3002", "result": "CHEFS\u6570\u636e\u5e93\u6210\u529f\u6574\u5408\u6570\u636e\uff0c\u5e76\u7528\u4e8e\u5206\u67902000-2024\u5e74\u7684\u6b27\u6d32\u98df\u54c1\u5b89\u5168\u8d8b\u52bf\uff0c\u63ed\u793a\u4e86\u76d1\u6d4b\u6d3b\u52a8\u53d8\u5316\u3001\u5e38\u89c1\u4e0d\u5408\u683c\u4ea7\u54c1\u548c\u6c61\u67d3\u7269\u3002", "conclusion": "CHEFS\u6570\u636e\u5e93\u662f\u4e00\u4e2a\u96c6\u4e2d\u5316\u7684\u6570\u636e\u6e90\u548c\u6218\u7565\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u98df\u54c1\u5b89\u5168\u653f\u7b56\u3001\u7814\u7a76\u548c\u76d1\u7ba1\u3002"}}
{"id": "2507.13830", "pdf": "https://arxiv.org/pdf/2507.13830", "abs": "https://arxiv.org/abs/2507.13830", "authors": ["Maximilian Rokuss", "Benjamin Hamm", "Yannick Kirchhoff", "Klaus Maier-Hein"], "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at MICCAI 2025 WOMEN", "summary": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider", "AI": {"tldr": "\u516c\u5f00\u9996\u4e2a\u5e26\u6709\u5de6\u53f3\u4e73\u623f\u5206\u5272\u6807\u7b7e\u7684\u4e73\u817aMRI\u6570\u636e\u96c6\uff0c\u5305\u542b13,000\u591a\u4e2a\u6807\u6ce8\u6848\u4f8b\uff0c\u5e76\u63d0\u4f9b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4e73\u817aMRI\u5206\u6790\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u5973\u6027\u5065\u5eb7\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u8d44\u6e90\u3002", "method": "\u63d0\u4f9b\u516c\u5f00\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u597d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u6570\u636e\u96c6\u548c\u6a21\u578b\u5df2\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "\u4e3a\u4e73\u817aMRI\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u8d44\u6e90\u3002"}}
{"id": "2507.13871", "pdf": "https://arxiv.org/pdf/2507.13871", "abs": "https://arxiv.org/abs/2507.13871", "authors": ["Mehul Anand", "Shishir Kolathaya"], "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "6 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2409.12616", "summary": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u63a7\u5236\u5c4f\u969c\u8bc1\u4e66\uff08CBCs\uff09\u6765\u5408\u6210\u5b89\u5168\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "motivation": "\u4ece\u89c6\u89c9\u6570\u636e\u5408\u6210\u5b89\u5168\u63a7\u5236\u5668\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u8bb0\u5b89\u5168\u5173\u952e\u6570\u636e\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u8054\u5408\u5b66\u4e60\u795e\u7ecf\u5c4f\u969c\u51fd\u6570\u548c\u5b89\u5168\u63a7\u5236\u5668\uff0c\u5229\u7528\u73b0\u4ee3\u89c6\u89c9\u53d8\u6362\u5668\u7684\u9884\u6d4b\u80fd\u529b\u8fdb\u884c\u6f5c\u5728\u52a8\u529b\u5b66\u5efa\u6a21\u3002", "result": "\u901a\u8fc7\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\u548c\u6f5c\u5728\u7a7a\u95f4\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u63a7\u5236\u5668\u7684\u5408\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.13901", "pdf": "https://arxiv.org/pdf/2507.13901", "abs": "https://arxiv.org/abs/2507.13901", "authors": ["Lei Xu", "Torkel B Brismar"], "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive", "categories": ["eess.IV", "cs.CV", "62H35, 68U10", "I.4.10; I.4.7; J.3"], "comment": "24 pages, 7 figures", "summary": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.", "AI": {"tldr": "AnatomyArchive\u662f\u4e00\u4e2a\u57fa\u4e8eTotalSegmentator\u7684CT\u56fe\u50cf\u5206\u6790\u5de5\u5177\u5305\uff0c\u63d0\u4f9b\u81ea\u52a8\u76ee\u6807\u4f53\u79ef\u9009\u62e9\u3001\u89e3\u5256\u7ed3\u6784\u7ba1\u7406\u3001\u4f53\u7d20\u7279\u5f81\u63d0\u53d6\u7b49\u529f\u80fd\uff0c\u652f\u63012D\u548c3D\u5206\u6790\uff0c\u5e76\u5305\u542bGPU\u52a0\u901f\u7684\u6e32\u67d3\u5de5\u5177\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684CT\u56fe\u50cf\u5206\u6790\u5de5\u5177\uff0c\u4ee5\u652f\u6301\u7cbe\u786e\u7684\u8eab\u4f53\u6210\u5206\u5206\u6790\u548c\u533b\u5b66\u56fe\u50cf\u6570\u636e\u5e93\u7ba1\u7406\u3002", "method": "\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u548cTotalSegmentator\u6a21\u578b\uff0c\u5b9e\u73b0\u81ea\u52a8\u4f53\u79ef\u9009\u62e9\u3001\u89e3\u5256\u7ed3\u6784\u7ba1\u7406\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u63d0\u4f9bGPU\u52a0\u901f\u7684\u6e32\u67d3\u5de5\u5177\u3002", "result": "\u5b9e\u73b0\u4e86\u81ea\u52a8\u76ee\u6807\u4f53\u79ef\u9009\u62e9\u3001\u89e3\u5256\u7ed3\u6784\u7ba1\u7406\u3001\u4f53\u7d20\u7279\u5f81\u63d0\u53d6\u7b49\u529f\u80fd\uff0c\u5e76\u652f\u63012D\u548c3D\u5206\u6790\u3002", "conclusion": "AnatomyArchive\u662f\u4e00\u4e2a\u529f\u80fd\u5f3a\u5927\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2507.13915", "pdf": "https://arxiv.org/pdf/2507.13915", "abs": "https://arxiv.org/abs/2507.13915", "authors": ["Huu-Phu Do", "Po-Chih Hu", "Hao-Chien Hsueh", "Che-Kai Liu", "Vu-Hoang Tran", "Ching-Chun Huang"], "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by ACCV 2024", "summary": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u901a\u8fc7HR\u53c2\u8003\u56fe\u50cf\u751f\u6210\u5c3a\u5ea6\u611f\u77e5\u7684\u9000\u5316\u6838\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76f2\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u76f4\u63a5\u4f30\u8ba1\u9000\u5316\u6838\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0d\u540c\u8d85\u5206\u8fa8\u7387\u5c3a\u5ea6\u4e0b\u7684\u9002\u7528\u6027\u95ee\u9898\uff0c\u9700\u540c\u65f6\u8003\u8651\u9000\u5316\u8fc7\u7a0b\u548c\u7f29\u653e\u56e0\u5b50\u3002", "method": "\u5229\u7528HR\u53c2\u8003\u56fe\u50cf\u4e0e\u76ee\u6807LR\u56fe\u50cf\uff0c\u81ea\u9002\u5e94\u5730\u5b66\u4e60\u9000\u5316\u8fc7\u7a0b\uff0c\u751f\u6210\u989d\u5916\u7684LR-HR\u5bf9\u4ee5\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002", "result": "\u5728\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u6a21\u578b\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u4e2d\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u6a21\u7cca\u6838\u548c\u7f29\u653e\u56e0\u5b50\uff0c\u5e76\u5229\u7528\u53c2\u8003\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6548\u679c\u3002"}}
{"id": "2507.13941", "pdf": "https://arxiv.org/pdf/2507.13941", "abs": "https://arxiv.org/abs/2507.13941", "authors": ["Pablo Marcos-Manch\u00f3n", "Llu\u00eds Fuentemilla"], "title": "Convergent transformations of visual representation in brains and models", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "eess.IV", "I.2.10"], "comment": "for associate code, see\n  https://github.com/memory-formation/convergent-transformations", "summary": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u89c6\u89c9\u611f\u77e5\u662f\u7531\u5916\u90e8\u4e16\u754c\u7ed3\u6784\u8fd8\u662f\u5927\u8111\u5185\u90e8\u67b6\u6784\u5851\u9020\uff0c\u53d1\u73b0\u4eba\u7c7b\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u89c6\u89c9\u7f16\u7801\u4e2d\u5b58\u5728\u5171\u540c\u7684\u8868\u5f81\u8f68\u8ff9\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u611f\u77e5\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u9a8c\u8bc1\u4eba\u7c7b\u548c\u4eba\u5de5\u89c6\u89c9\u7cfb\u7edf\u662f\u5426\u9075\u5faa\u76f8\u4f3c\u7684\u7f16\u7801\u539f\u5219\u3002", "method": "\u7ed3\u5408\u8de8\u88ab\u8bd5\u76f8\u4f3c\u6027\u548c\u6a21\u578b\u5c42\u6b21\u5bf9\u9f50\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5206\u6790\u4e09\u4e2a\u72ec\u7acbfMRI\u6570\u636e\u96c6\u3002", "result": "\u53d1\u73b0\u4e00\u4e2a\u8de8\u4e2a\u4f53\u4fdd\u5b88\u7684\u53cc\u901a\u8def\u7f51\u7edc\uff0c\u4e0e\u89c6\u89c9DNN\u5c42\u6b21\u4e00\u81f4\uff0c\u8bed\u8a00\u6a21\u578b\u5219\u4e0d\u7b26\u3002", "conclusion": "\u4eba\u7c7b\u548c\u4eba\u5de5\u89c6\u89c9\u7cfb\u7edf\u5bf9\u5916\u90e8\u4e16\u754c\u7ed3\u6784\u7684\u7f16\u7801\u5b58\u5728\u6536\u655b\u7684\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13956", "pdf": "https://arxiv.org/pdf/2507.13956", "abs": "https://arxiv.org/abs/2507.13956", "authors": ["Yutao Jin", "Haowen Xiao", "Jielei Chu", "Fengmao Lv", "Yuxiao Li", "Tianrui Li"], "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction", "categories": ["cs.AI", "cs.CV", "cs.MM"], "comment": null, "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADPC\u7684\u89c6\u89c9-\u8bed\u8a00\u56e0\u679c\u5e72\u9884\u6846\u67b6\uff0c\u7528\u4e8e\u8f85\u52a9\u8bca\u65ad\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\uff0c\u901a\u8fc7\u6d88\u9664\u6df7\u6742\u56e0\u7d20\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u65e9\u671f\u8bc6\u522b\u548c\u5e72\u9884\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u53ef\u4ee5\u5ef6\u7f13AD\u8fdb\u5c55\uff0c\u4f46\u8bca\u65adAD\u4ecd\u9762\u4e34\u591a\u6a21\u6001\u6570\u636e\u9009\u62e9\u504f\u5dee\u548c\u53d8\u91cf\u590d\u6742\u5173\u7cfb\u7684\u6311\u6218\u3002", "method": "ADPC\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u603b\u7ed3\u4e34\u5e8a\u6570\u636e\uff0c\u7ed3\u5408MRI\u548cfMRI\u56fe\u50cf\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u6d88\u9664\u6df7\u6742\u56e0\u7d20\uff0c\u5206\u7c7bCN/MCI/AD\u3002", "result": "\u5b9e\u9a8c\u8868\u660eADPC\u5728\u533a\u5206CN/MCI/AD\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230SOTA\u6307\u6807\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u56e0\u679c\u63a8\u7406\u4e0e\u591a\u6a21\u6001\u5b66\u4e60\u7ed3\u5408\u5728\u795e\u7ecf\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.13974", "pdf": "https://arxiv.org/pdf/2507.13974", "abs": "https://arxiv.org/abs/2507.13974", "authors": ["Jiaqi Lv", "Yijie Zhu", "Carmen Guadalupe Colin Tenorio", "Brinder Singh Chohan", "Mark Eastwood", "Shan E Ahmed Raza"], "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "comment": "Accepted by MIUA 2025", "summary": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b0\u578b\u7f51\u7edc\uff0c\u7528\u4e8e\u9ed1\u8272\u7d20\u7624H&E\u56fe\u50cf\u4e2d\u4e94\u79cd\u7ec4\u7ec7\u7c7b\u522b\u7684\u5206\u5272\uff0c\u5229\u7528\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578bVirchow2\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408Efficient-UNet\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u5272\u3002", "motivation": "\u9ed1\u8272\u7d20\u7624\u7ec4\u7ec7\u5f62\u6001\u7684\u51c6\u786e\u8868\u5f81\u5bf9\u9884\u540e\u548c\u6cbb\u7597\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u5206\u5272\u8017\u65f6\u4e14\u6613\u53d7\u89c2\u5bdf\u8005\u5dee\u5f02\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u9760\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Virchow2\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u4e0e\u539f\u59cbRGB\u56fe\u50cf\u878d\u5408\u540e\uff0c\u901a\u8fc7Efficient-UNet\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u751f\u6210\u5206\u5272\u56fe\u3002", "result": "\u6a21\u578b\u5728PUMA Grand Challenge\u7684\u7ec4\u7ec7\u5206\u5272\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u878d\u5165\u5206\u5272\u7f51\u7edc\u53ef\u52a0\u901f\u8ba1\u7b97\u75c5\u7406\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5177\u6709\u6f5c\u5728\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.13993", "pdf": "https://arxiv.org/pdf/2507.13993", "abs": "https://arxiv.org/abs/2507.13993", "authors": ["Ningyong Wu", "Jinzhi Wang", "Wenhong Zhao", "Chenzhan Yu", "Zhigang Xiu", "Duwei Dai"], "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.", "AI": {"tldr": "OrthoInsight\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u808b\u9aa8\u9aa8\u6298\u8bca\u65ad\u548c\u62a5\u544a\u751f\u6210\uff0c\u7ed3\u5408\u4e86YOLOv9\u3001\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u548cLLaVA\u8bed\u8a00\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8eGPT-4\u548cClaude-3\u3002", "motivation": "\u533b\u7597\u5f71\u50cf\u6570\u636e\u589e\u957f\u8fc5\u901f\uff0c\u624b\u52a8\u8bca\u65ad\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faOrthoInsight\u6846\u67b6\uff0c\u6574\u5408YOLOv9\u68c0\u6d4b\u9aa8\u6298\u3001\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u4e34\u5e8a\u80cc\u666f\u3001LLaVA\u751f\u6210\u62a5\u544a\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u3002", "result": "\u572828,675\u5f20CT\u56fe\u50cf\u4e0a\u8bc4\u4f30\uff0cOrthoInsight\u5728\u8bca\u65ad\u51c6\u786e\u6027\u3001\u5185\u5bb9\u5b8c\u6574\u6027\u3001\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u4e34\u5e8a\u6307\u5bfc\u4ef7\u503c\u4e0a\u5e73\u5747\u5f97\u52064.28\uff0c\u4f18\u4e8eGPT-4\u548cClaude-3\u3002", "conclusion": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u53ef\u4e3a\u653e\u5c04\u79d1\u533b\u751f\u63d0\u4f9b\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2507.14046", "pdf": "https://arxiv.org/pdf/2507.14046", "abs": "https://arxiv.org/abs/2507.14046", "authors": ["Hao Fang", "Hao Yu", "Sihao Teng", "Tao Zhang", "Siyi Yuan", "Huaiwu He", "Zhe Liu", "Yunjie Yang"], "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "11 pages, 9 figures", "summary": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aD2IP\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e3D\u65f6\u95f4\u5e8f\u5217\u6210\u50cf\uff0c\u901a\u8fc7\u4e09\u79cd\u7b56\u7565\u52a0\u901f\u6536\u655b\u3001\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08\u5982DIP\uff09\u5728\u65ad\u5c42\u6210\u50cf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u67423D\u6216\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "D2IP\u5f15\u5165\u4e86\u4e09\u79cd\u7b56\u7565\uff1a\u65e0\u76d1\u7763\u53c2\u6570\u9884\u70ed\u542f\u52a8\uff08UPWS\uff09\u3001\u65f6\u95f4\u53c2\u6570\u4f20\u64ad\uff08TPP\uff09\u548c\u8f7b\u91cf\u7ea7\u91cd\u5efa\u9aa8\u5e72\u7f51\u7edc3D-FastResUNet\u3002", "result": "\u5728\u6a21\u62df\u548c\u4e34\u5e8a\u80ba\u90e8\u6570\u636e\u96c6\u4e0a\uff0cD2IP\u5b9e\u73b0\u4e86\u5feb\u901f\u51c6\u786e\u76843D\u65f6\u95f4\u5e8f\u5217\u7535\u963b\u6297\u65ad\u5c42\u6210\u50cf\uff08tsEIT\uff09\u91cd\u5efa\uff0c\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\uff08MSSIM\u589e\u52a024.8%\uff0cERR\u964d\u4f4e8.1%\uff09\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c117.1\u500d\u3002", "conclusion": "D2IP\u5728\u52a8\u6001\u80ba\u90e8\u6210\u50cf\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14097", "pdf": "https://arxiv.org/pdf/2507.14097", "abs": "https://arxiv.org/abs/2507.14097", "authors": ["Hari Iyer", "Neel Macwan", "Atharva Jitendra Hude", "Heejin Jeong", "Shenghan Guo"], "title": "Generative AI-Driven High-Fidelity Human Motion Simulation", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u4eba\u4f53\u8fd0\u52a8\u6a21\u62df\u65b9\u6cd5\uff08G-AI-HMS\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u8fd0\u52a8\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5de5\u4e1a\u4efb\u52a1\u4e2d\u8fd0\u52a8\u6a21\u62df\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u8fd0\u52a8\u6a21\u62df\u65b9\u6cd5\u5728\u8fd0\u52a8\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u5de5\u4e1a\u4efb\u52a1\u4e2d\u5de5\u4eba\u884c\u4e3a\u3001\u5b89\u5168\u548c\u751f\u4ea7\u6548\u7387\u7684\u8bc4\u4f30\u3002", "method": "G-AI-HMS\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u4efb\u52a1\u63cf\u8ff0\u8f6c\u5316\u4e3a\u8fd0\u52a8\u611f\u77e5\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u9a8c\u8bc1AI\u751f\u6210\u7684\u8fd0\u52a8\u4e0e\u771f\u5b9e\u4eba\u7c7b\u52a8\u4f5c\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u5728\u516b\u9879\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cAI\u589e\u5f3a\u7684\u8fd0\u52a8\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u5de5\u63cf\u8ff0\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5173\u8282\u8bef\u5dee\u548c\u65f6\u95f4\u9519\u4f4d\u3002", "conclusion": "G-AI-HMS\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u6a21\u62df\u7684\u8d28\u91cf\uff0c\u4e3a\u5de5\u4e1a\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2507.14102", "pdf": "https://arxiv.org/pdf/2507.14102", "abs": "https://arxiv.org/abs/2507.14102", "authors": ["Shravan Venkatraman", "Pavan Kumar S", "Rakesh Raj Madavan", "Chandrakala S"], "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "18 pages, 10 figures, 5 tables, 2025 ICCV Workshops", "summary": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL", "AI": {"tldr": "UGPL\u662f\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6e10\u8fdb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u5230\u5c40\u90e8\u5206\u6790\u63d0\u5347CT\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CT\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u75c5\u7406\u7279\u5f81\u7684\u7ec6\u5fae\u548c\u7a7a\u95f4\u591a\u6837\u6027\uff0cUGPL\u65e8\u5728\u901a\u8fc7\u805a\u7126\u5173\u952e\u533a\u57df\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "UGPL\u7ed3\u5408\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u975e\u6781\u5927\u503c\u6291\u5236\u673a\u5236\u63d0\u53d6\u4fe1\u606f\u4e30\u5bcc\u7684\u533a\u57df\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u6574\u5408\u4e0a\u4e0b\u6587\u548c\u7ec6\u8282\u3002", "result": "\u5728\u4e09\u4e2aCT\u6570\u636e\u96c6\u4e0a\uff0cUGPL\u5728\u80be\u810f\u5f02\u5e38\u3001\u80ba\u764c\u548cCOVID-19\u68c0\u6d4b\u4e2d\u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e863.29%\u30012.46%\u548c8.08%\u3002", "conclusion": "UGPL\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u548c\u6e10\u8fdb\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86CT\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
