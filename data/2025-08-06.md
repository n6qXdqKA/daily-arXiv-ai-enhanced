[[toc]]

## cs.CV

### [1] [PyCAT4: A Hierarchical Vision Transformer-based Framework for 3D Human Pose Estimation](https://arxiv.org/abs/2508.02806)
*Zongyou Yang,Jonathan Loo*

Main category: cs.CV

TL;DR: 本文通过结合Transformer特征提取层、时序融合技术和空间金字塔结构，优化了Pymaf网络架构，提出PyCAT4模型，显著提升了3D人体姿态估计的准确性。

- Motivation: 结合CNN与金字塔网格对齐反馈环的现有方法在3D人体姿态估计中取得显著进展，但仍有优化空间。本文旨在通过引入Transformer和时序分析技术进一步提升性能。
- Method: 1. 引入基于自注意力机制的Transformer特征提取层；2. 使用时序融合技术增强视频序列信号理解；3. 采用空间金字塔结构实现多尺度特征融合。
- Result: 在COCO和3DPW数据集上的实验表明，PyCAT4模型显著提升了人体姿态估计的检测能力。
- Conclusion: 本文提出的改进策略有效推动了人体姿态估计技术的发展，为未来研究提供了新方向。


### [2] [DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework](https://arxiv.org/abs/2508.02807)
*Tongchun Zuo,Zaiyu Huang,Shuliang Ning,Ente Lin,Chao Liang,Zerong Zheng,Jianwen Jiang,Yuan Zhang,Mingyuan Gao,Xin Dong*

Main category: cs.CV

TL;DR: DreamVVT是一个基于扩散变换器的两阶段框架，利用未配对数据提升视频虚拟试穿的适应性和细节保留能力。

- Motivation: 解决现有方法依赖配对数据、无法有效利用先验知识和测试输入的问题，以提升细节保留和时间一致性。
- Method: 第一阶段使用多帧试穿模型和视觉语言模型生成高质量关键帧；第二阶段结合骨架图和动态描述，通过预训练视频生成模型增强时间一致性。
- Result: 实验表明DreamVVT在细节保留和时间稳定性上优于现有方法。
- Conclusion: DreamVVT通过两阶段设计有效提升了视频虚拟试穿的性能，适用于真实场景。


### [3] [Elucidating the Role of Feature Normalization in IJEPA](https://arxiv.org/abs/2508.02829)
*Adam Colton*

Main category: cs.CV

TL;DR: 论文提出在IJEPA架构中用DynTanh激活替代层归一化（LN），以保留视觉标记的自然能量层级，提升模型性能。

- Motivation: 层归一化（LN）破坏了视觉标记的自然能量层级，导致语义重要区域无法被优先处理，并引入棋盘状伪影。
- Method: 用DynTanh激活替代LN，以保留标记能量并允许高能量标记对预测损失贡献更大。
- Result: 改进后的IJEPA模型在ImageNet线性探测准确率从38%提升至42.7%，并在NYU Depth V2上降低RMSE 0.08。
- Conclusion: 保留自然标记能量对自监督视觉表示学习至关重要。


### [4] [GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing](https://arxiv.org/abs/2508.02831)
*Mikołaj Zieliński,Krzysztof Byrski,Tomasz Szczepanik,Przemysław Spurek*

Main category: cs.CV

TL;DR: GENIE结合NeRF的高保真渲染和GS的可编辑性，通过高斯特征嵌入和RT-GPS实现实时编辑。

- Motivation: NeRF的隐式编码难以编辑，而GS的显式表示适合交互，但缺乏NeRF的渲染质量。GENIE旨在结合两者优势。
- Method: 使用高斯特征嵌入和RT-GPS，结合多分辨率哈希网格，实现高效的高斯邻近搜索和实时编辑。
- Result: GENIE支持实时、局部感知的编辑，动态交互，并与物理模拟兼容。
- Conclusion: GENIE通过混合隐式和显式表示，填补了几何编辑与神经渲染之间的空白。


### [5] [RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation](https://arxiv.org/abs/2508.02844)
*Anghong Du,Nay Aung,Theodoros N. Arvanitis,Stefan K. Piechnik,Joao A C Lima,Steffen E. Petersen,Le Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于粗标注的医学图像分割框架，通过引入转移矩阵建模噪声，逐步优化分割结果，性能接近全监督方法。

- Motivation: 医学图像的高质量像素级标注成本高且需要专业知识，粗标注虽易获取但存在噪声。
- Method: 提出粗到细分割框架，利用转移矩阵建模粗标注的噪声，通过联合训练多组粗标注逐步优化分割结果。
- Result: 在两个公开心脏影像数据集上验证，性能优于弱监督方法，接近全监督方法。
- Conclusion: 该方法有效利用粗标注，无需精确标注即可实现高质量分割。


### [6] [MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model](https://arxiv.org/abs/2508.02858)
*Tianheng Zhu,Yiheng Feng*

Main category: cs.CV

TL;DR: MIDAR是一个模拟LiDAR检测的模型，用于在微观交通模拟器中生成逼真的LiDAR检测结果，弥补了现有模拟器的不足。

- Motivation: 由于大规模真实自动驾驶车辆部署不现实，现有模拟器在感知建模或扩展性方面存在局限，需要一种方法结合两者的优势。
- Method: 提出MIDAR模型，通过车辆级特征预测LiDAR检测结果，构建RM-LoS图编码遮挡关系，并使用GRU增强的APPNP架构进行特征传播。
- Result: 在nuScenes数据集上，MIDAR的AUC达到0.909，验证了其模拟主流LiDAR检测模型的能力。
- Conclusion: MIDAR能无缝集成到交通模拟器和轨迹数据集中，为需要精确车辆观测的任务提供支持，并将开源。


### [7] [Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets](https://arxiv.org/abs/2508.02871)
*J. Alex Hurt,Trevor M. Bajkowski,Grant J. Scott,Curt H. Davis*

Main category: cs.CV

TL;DR: 论文探讨了基于Transformer的神经网络在高分辨率卫星图像中的物体检测性能，并与卷积网络进行了比较。

- Motivation: 随着Transformer在CV领域的兴起，需要评估其在遥感数据上的表现。
- Method: 比较了11种检测算法（5种Transformer架构和6种卷积网络）在3个公开数据集上的性能。
- Result: 展示了Transformer架构在遥感图像上的先进性能。
- Conclusion: Transformer在遥感图像检测中表现出色，为未来研究提供了方向。


### [8] [VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction](https://arxiv.org/abs/2508.02890)
*Rongxin Jiang,Robert Long,Chenghao Gu,Mingrui Yan*

Main category: cs.CV

TL;DR: VisuCraft是一个新框架，通过多模态结构化信息提取器和动态提示生成模块，显著提升大型视觉语言模型在复杂视觉引导创意内容生成中的表现。

- Motivation: 现有大型视觉语言模型在生成长文本时，难以保持高视觉保真度、真正创造性和精确遵循用户指令。VisuCraft旨在解决这些问题。
- Method: VisuCraft整合了多模态结构化信息提取器（E）和动态提示生成模块（G），提取图像细粒度属性并生成优化提示。
- Result: 在ImageStoryGen-500K数据集上，VisuCraft在故事生成和诗歌创作等任务中表现优于基线模型，尤其在创造性和指令遵循方面。
- Conclusion: VisuCraft为大型视觉语言模型在复杂创意AI应用中开辟了新潜力。


### [9] [RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation](https://arxiv.org/abs/2508.02903)
*Mehrdad Moradi,Kamran Paynabar*

Main category: cs.CV

TL;DR: 提出了一种新的鲁棒去噪扩散模型，适用于仅含污染数据的无监督异常分割任务，性能优于现有方法。

- Motivation: 传统扩散模型依赖纯净正常数据训练，限制了实际应用；本文针对仅含污染数据的情况提出解决方案。
- Method: 通过非线性回归视角重新解释去噪扩散概率模型，结合鲁棒回归推导出鲁棒版本。
- Result: 在MVTec数据集上，AUROC提升8.08%，AUPRC提升10.37%。
- Conclusion: 新框架在污染数据场景下显著优于现有扩散模型，具有广泛适用性。


### [10] [How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes](https://arxiv.org/abs/2508.02905)
*Mahnoor Fatima Saad,Ziad Al-Halah*

Main category: cs.CV

TL;DR: 论文提出了一种基于材料配置的声学特性生成方法，通过编码器-解码器模型生成目标房间脉冲响应（RIR），并在新数据集上验证了其优越性。

- Motivation: 研究如何在特定音频视觉特性的室内场景中，根据用户定义的材料配置生成目标声学特性。
- Method: 采用编码器-解码器方法，编码场景的关键属性，并根据用户提供的材料配置生成目标RIR。
- Result: 模型能够动态生成多样化的RIR，并在新数据集上表现优于基线方法和现有技术。
- Conclusion: 提出的方法能有效编码材料信息并生成高质量的RIR，为声学特性生成提供了新思路。


### [11] [Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces](https://arxiv.org/abs/2508.02917)
*Vebjørn Haug Kåsene,Pierre Lison*

Main category: cs.CV

TL;DR: 研究探讨了现成的大型视觉语言模型（LVLMs）在视觉与语言导航（VLN）任务中的表现，发现其虽能完成任务，但效果不及专门设计的模型。

- Motivation: 探索现成LVLMs在VLN任务中的潜力，并验证其是否支持低级和全景动作范式。
- Method: 在Room-to-Room数据集上微调开源模型Qwen2.5-VL-3B-Instruct，评估其在低级和全景动作空间的表现。
- Result: 最佳模型在R2R测试集上达到41%的成功率，表明现成LVLMs虽能学习VLN，但效果不如专用模型。
- Conclusion: 现成LVLMs可用于VLN任务，但性能仍有提升空间。


### [12] [How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution](https://arxiv.org/abs/2508.02923)
*Minh-Hai Nguyen,Edouard Pauwels,Pierre Weiss*

Main category: cs.CV

TL;DR: MAP估计在盲去卷积中倾向于模糊解，但扩散先验揭示局部极小值对应自然图像，需良好初始化以克服MAP限制。

- Motivation: 研究MAP估计在盲去卷积中的局限性，探索扩散先验如何揭示更优解。
- Method: 通过分析扩散先验的似然景观，结合理论分析和数值实验验证。
- Result: MAP估计倾向于模糊解，而局部极小值对应自然图像，需良好初始化。
- Conclusion: 克服MAP限制需关注局部极小值，为设计更好先验和优化技术提供指导。


### [13] [Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?](https://arxiv.org/abs/2508.02927)
*Srikanth Muralidharan,Heitor R. Medeiros,Masih Aminbeidokhti,Eric Granger,Marco Pedersoli*

Main category: cs.CV

TL;DR: 研究探讨了ImageNet预训练对超小型模型（参数<1M）在红外视觉模态下游目标检测任务中鲁棒性的影响，发现预训练仍有帮助，但模型过小时收益递减。

- Motivation: 探索预训练对小型模型在嵌入式设备上鲁棒性的影响，尤其是在红外视觉模态下的目标检测任务。
- Method: 通过标准目标识别架构的缩放定律构建两个超小型主干家族，并系统研究其性能。
- Result: 实验表明，ImageNet预训练对小型模型仍有益，但模型过小时鲁棒性收益递减。
- Conclusion: 建议使用预训练，并避免模型过小，以确保在不同工作条件下的鲁棒性。


### [14] [X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio](https://arxiv.org/abs/2508.02944)
*Chenxu Zhang,Zenan Li,Hongyi Xu,You Xie,Xiaochen Zhao,Tianpei Gu,Guoxian Song,Xin Chen,Chao Liang,Jianwen Jiang,Linjie Luo*

Main category: cs.CV

TL;DR: X-Actor是一个音频驱动的肖像动画框架，通过单张参考图像和音频生成逼真、情感丰富的说话头部视频。其核心是一个两阶段解耦生成管道，结合自回归扩散模型和视频合成模块，实现长时情感连贯的动画。

- Motivation: 现有方法多关注唇同步和短时视觉保真度，而X-Actor旨在生成演员级的长时情感表达动画，与语音内容和节奏动态一致。
- Method: 采用两阶段解耦生成管道：1）音频条件自回归扩散模型预测面部运动潜在标记；2）扩散视频合成模块生成高保真视频。通过扩散强制训练范式捕捉长时音频与面部动态关联。
- Result: 实验表明，X-Actor能生成超越标准说话头部动画的电影级表演，在长时音频驱动情感肖像表演中达到最先进水平。
- Conclusion: X-Actor通过解耦设计和扩散模型，实现了长时情感连贯的高质量肖像动画，为音频驱动表演提供了新方向。


### [15] [Towards Robust Image Denoising with Scale Equivariance](https://arxiv.org/abs/2508.02967)
*Dawei Zhang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 论文提出了一种基于尺度等变性的盲去噪框架，通过异构归一化模块（HNM）和交互门控模块（IGM）提升模型对空间异质噪声的鲁棒性。

- Motivation: 现有图像去噪模型在空间异质噪声（OOD条件）下泛化能力不足，尺度等变性被视为提升鲁棒性的核心归纳偏置。
- Method: 提出包含HNM和IGM的盲去噪框架：HNM稳定特征分布并动态校正噪声强度变化下的特征；IGM通过门控交互调节信号与特征路径的信息流。
- Result: 模型在合成和真实数据集上均优于现有方法，尤其在空间异质噪声条件下表现突出。
- Conclusion: 尺度等变性是提升去噪模型OOD鲁棒性的有效方向，提出的框架具有实际应用潜力。


### [16] [Diffusion Models with Adaptive Negative Sampling Without External Resources](https://arxiv.org/abs/2508.02973)
*Alakh Desai,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 论文提出了一种名为ANSWER的训练无关技术，通过结合负提示与无分类器引导（CFG），提升扩散模型生成图像与文本提示的匹配度。

- Motivation: 扩散模型在生成高质量图像时，对文本提示的遵循程度和生成质量存在显著差异。负提示虽能改善提示遵循性，但现有方法依赖外部资源且不完整。
- Method: 提出ANSWER采样方法，利用扩散模型内部对否定的理解，无需显式负提示即可实现负条件生成。
- Result: ANSWER在多个基准测试中优于基线方法，且人类评估显示其偏好度是其他方法的两倍。
- Conclusion: ANSWER是一种无需训练、适用于任何支持CFG的模型的技术，有效提升了生成图像的提示忠实度。


### [17] [Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning](https://arxiv.org/abs/2508.02978)
*Yusaku Takama,Ning Ding,Tatsuya Yokota,Toru Tamaki*

Main category: cs.CV

TL;DR: 论文提出一种方法，确保共享和领域特定的LoRA存在于不同的子空间（列空间和左零空间），并在动作识别任务中验证其有效性。

- Motivation: 现有多领域学习架构中，共享LoRA和领域特定LoRA是否有效捕捉领域特定信息尚不明确。
- Method: 提出一种方法，将共享和领域特定LoRA分别置于预训练权重的列空间和左零空间。
- Result: 在UCF101、Kinetics400和HMDB51数据集上的动作识别任务中验证了方法的有效性。
- Conclusion: 该方法能有效分离共享和领域特定信息，提升多领域学习性能。


### [18] [MoExDA: Domain Adaptation for Edge-based Action Recognition](https://arxiv.org/abs/2508.02981)
*Takuya Sugimoto,Ning Ding,Toru Tamaki*

Main category: cs.CV

TL;DR: 提出MoExDA方法，通过结合RGB和边缘信息减轻静态偏差，提升动作识别的鲁棒性。

- Motivation: 现代动作识别模型存在静态偏差问题，影响泛化性能。
- Method: 使用RGB帧和边缘帧进行轻量级域适应（MoExDA）。
- Result: 实验表明该方法有效抑制静态偏差，计算成本更低。
- Conclusion: MoExDA比现有方法更鲁棒且高效。


### [19] [Adversarial Attention Perturbations for Large Object Detection Transformers](https://arxiv.org/abs/2508.02987)
*Zachary Yahn,Selim Furkan Tekin,Fatih Ilhan,Sihao Hu,Tiansheng Huang,Yichang Xu,Margaret Loper,Ling Liu*

Main category: cs.CV

TL;DR: AFOG是一种针对目标检测变换器的对抗性攻击方法，通过注意力机制聚焦于脆弱区域，提升攻击效果。

- Motivation: 现有对抗性扰动方法在攻击变换器检测器时效果有限，AFOG旨在填补这一空白。
- Method: AFOG利用可学习的注意力机制，结合特征损失和迭代扰动注入，生成高效且隐蔽的对抗性扰动。
- Result: AFOG在COCO数据集上的实验表明，其攻击效果优于现有方法，提升达83%。
- Conclusion: AFOG是一种高效、隐蔽且通用的对抗性攻击方法，适用于多种目标检测器。


### [20] [Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models](https://arxiv.org/abs/2508.03006)
*Fan Yang,Yihao Huang,Jiayi Zhu,Ling Shi,Geguang Pu,Jin Song Dong,Kailong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种在扩散模型生成过程中检测NSFW内容的新方法（IGD），利用预测噪声作为内部信号，实验显示其检测准确率高达91.32%，优于基线方法。

- Motivation: 现有方法主要关注生成前提示过滤或生成后图像审核，而扩散模型生成过程中的NSFW检测尚未充分探索。
- Method: 提出In-Generation Detection（IGD），利用扩散过程中预测的噪声作为内部信号来识别NSFW内容。
- Result: 在七个NSFW类别上的实验表明，IGD对普通和对抗性NSFW提示的平均检测准确率为91.32%，优于七种基线方法。
- Conclusion: IGD是一种简单有效的方法，填补了扩散模型生成过程中NSFW检测的空白。


### [21] [Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.03007)
*Xinhui Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: 提出了一种多粒度特征校准（MGFC）框架，通过从粗到细的特征对齐提升域泛化语义分割（DGSS）的性能。

- Motivation: 现有方法主要关注全局特征微调，忽略了特征层次的分层适应，而这对密集预测任务至关重要。
- Method: MGFC框架分三步校准特征：粗粒度特征捕获全局语义，中粒度特征提升类别区分度，细粒度特征增强空间细节。
- Result: 在基准数据集上的实验表明，MGFC优于现有DGSS方法。
- Conclusion: 多粒度特征校准能有效提升语义分割任务在域泛化中的性能。


### [22] [Enhancing Long Video Question Answering with Scene-Localized Frame Grouping](https://arxiv.org/abs/2508.03009)
*Xuyi Yang,Wenhao Zhang,Hongbo Jin,Lin Liu,Hongbo Xu,Yongwei Nie,Fei Yu,Fei Ma*

Main category: cs.CV

TL;DR: 论文提出了一种针对长视频理解的新任务SceneQA和数据集LVSQA，并开发了无需修改模型架构的SLFG方法，通过语义连贯的场景帧提升MLLMs的性能。

- Motivation: 现有MLLMs在长视频理解中表现不佳，主要因资源限制无法处理所有帧信息，且现有任务与实际需求不符。
- Method: 提出SLFG方法，将单帧组合为语义连贯的场景帧，利用场景定位和动态帧重组机制。
- Result: SLFG在多个长视频基准测试中表现优异。
- Conclusion: SLFG方法有效提升了MLLMs的长视频理解能力，具有即插即用性。


### [23] [SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting](https://arxiv.org/abs/2508.03017)
*Liheng Zhang,Weihao Yu,Zubo Lu,Haozhi Gu,Jin Huang*

Main category: cs.CV

TL;DR: SA-3DGS通过重要性评分和聚类压缩，显著减少3D高斯模型的存储需求，同时保持渲染质量。

- Motivation: 现有方法在压缩高斯模型时难以识别无关高斯点，导致存储和性能问题。
- Method: SA-3DGS通过学习重要性评分、聚类压缩和代码本修复，优化高斯模型。
- Result: 实验显示，该方法实现66倍压缩，且渲染质量不降反升。
- Conclusion: SA-3DGS在压缩和性能上优于现有方法，具有广泛适用性。


### [24] [MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention](https://arxiv.org/abs/2508.03034)
*Qi Xie,Yongjia Ma,Donglin Di,Xuehao Gao,Xun Yang*

Main category: cs.CV

TL;DR: MoCA是一种基于扩散变换器（DiT）的视频扩散模型，通过混合交叉注意力机制和分层时间池化提升身份一致性，在CelebIPVid数据集上表现优于现有方法。

- Motivation: 解决现有文本到视频（T2V）生成方法在细粒度面部动态和时序身份一致性上的不足。
- Method: 提出MoCA模型，结合混合交叉注意力机制、分层时间池化和潜在视频感知损失，利用CelebIPVid数据集训练。
- Result: 在CelebIPVid数据集上，MoCA在面部相似度上比现有方法高出5%以上。
- Conclusion: MoCA通过改进的机制显著提升了T2V生成中的身份一致性和细节表现。


### [25] [VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering](https://arxiv.org/abs/2508.03039)
*Yiran Meng,Junhong Ye,Wei Zhou,Guanghui Yue,Xudong Mao,Ruomei Wang,Baoquan Zhao*

Main category: cs.CV

TL;DR: VideoForest是一个通过人物锚定分层推理解决跨视频问答挑战的新框架，显著提升了跨视频理解性能。

- Motivation: 跨视频问答面临多源信息检索和视频间关联建立的挑战，传统单视频理解方法难以应对。
- Method: 采用人物锚定特征提取、多粒度树结构和多智能体推理框架，实现高效跨视频分析。
- Result: 在人物识别、行为分析和推理任务中分别达到71.93%、83.75%和51.67%的准确率，优于现有方法。
- Conclusion: VideoForest通过人物级特征统一多视频流，为跨视频理解提供了高效新范式。


### [26] [Multi-human Interactive Talking Dataset](https://arxiv.org/abs/2508.03050)
*Zeyu Zhu,Weijia Wu,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 论文提出了MIT数据集和CovOG模型，用于多人物对话视频生成，填补了现有研究在多人交互场景中的空白。

- Motivation: 现有研究主要关注单人独白或孤立的面部动画，缺乏对多人真实交互的适用性。
- Method: 开发了自动收集和标注多人对话视频的流程，构建了MIT数据集，并提出CovOG模型，整合了多人体姿态编码器和交互式音频驱动模块。
- Result: 生成了12小时的高分辨率多人对话视频数据集，并展示了CovOG模型在多人对话视频生成中的可行性。
- Conclusion: MIT数据集和CovOG模型为多人对话视频生成提供了有价值的基准，推动了未来研究的发展。


### [27] [Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation](https://arxiv.org/abs/2508.03055)
*Hyebin Cho,Jaehyup Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为FaceMat的无trimap、不确定性感知框架，用于在复杂遮挡下预测高质量的人脸alpha遮罩，并通过两阶段训练和知识蒸馏提升性能。

- Motivation: 现有的人脸滤镜在遮挡情况下性能下降，需要一种能够分离遮挡物与人脸区域的方法。
- Method: 提出FaceMat框架，采用两阶段训练：教师模型通过NLL损失预测alpha遮罩和像素级不确定性，学生模型利用不确定性进行自适应知识蒸馏。
- Result: FaceMat在多个基准测试中优于现有方法，提升了人脸滤镜的视觉质量和鲁棒性。
- Conclusion: FaceMat无需辅助输入，适用于实时应用，并通过新构建的CelebAMat数据集支持遮挡感知的人脸遮罩任务。


### [28] [CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation](https://arxiv.org/abs/2508.03060)
*Lekang Wen,Jing Xiao,Liang Liao,Jiajun Chen,Mi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CHARM的互补学习框架，旨在通过隐式对齐和双路径优化策略实现模态间的协同和谐，而非同质化。

- Motivation: 现有方法通过显式特征对齐实现模态同质化，但削弱了各模态的独特优势和互补性。CHARM旨在解决这一问题。
- Method: CHARM包含两个组件：1）Mutual Perception Unit（MPU），通过窗口式跨模态交互实现隐式对齐；2）双路径优化策略，包括互补融合学习（CoL）和模态特异性优化（InE）。
- Result: 实验表明，CHARM在多个数据集和骨干网络上表现优于基线，尤其在脆弱模态上提升显著。
- Conclusion: CHARM将研究重点从同质化转向和谐化，实现了跨模态互补性，为多样性中的真正和谐提供了可能。


### [29] [CORE-ReID: Comprehensive Optimization and Refinement through Ensemble fusion in Domain Adaptation for person re-identification](https://arxiv.org/abs/2508.03064)
*Trinh Quoc Nguyen,Oky Dicky Ardiansyah Prima,Katsuyoshi Hotta*

Main category: cs.CV

TL;DR: 提出了一种名为CORE-ReID的新框架，用于解决无监督域适应（UDA）在行人重识别（ReID）中的问题。通过CycleGAN生成多样化数据，结合多视图特征和集成融合组件，显著提升了性能。

- Motivation: 解决不同摄像头源图像特征差异导致的UDA在ReID中的挑战，提升模型在跨域场景下的表现。
- Method: 使用CycleGAN生成数据，结合师生网络和多级聚类生成伪标签，引入集成融合组件优化特征学习。
- Result: 在三个常见UDA任务中表现优于现有方法，提高了Mean Average Precision和Top-N准确率。
- Conclusion: CORE-ReID框架通过优化特征融合和避免伪标签模糊性，成为UDA在ReID中的高效解决方案。


### [30] [SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D Medical Image Segmentation](https://arxiv.org/abs/2508.03069)
*Bo Zhang,Yifan Zhang,Shuo Yan,Yu Bai,Zheng Zhang,Wu Liu,Xiuzhuang Zhou,Wendong Wang*

Main category: cs.CV

TL;DR: SSFMamba是一种基于Mamba的对称驱动空间-频率特征融合网络，用于3D医学图像分割，通过双分支架构和Mamba块融合空间与频率域特征，提升全局上下文建模能力。

- Motivation: 现有方法在3D医学图像分割中未能充分利用频率域信息的独特性质（如共轭对称性）及与空间域的数据分布差异，导致频率域优势未被充分挖掘。
- Method: 提出SSFMamba，采用双分支架构分别提取空间和频率域特征，利用Mamba块融合异构特征，并设计3D多方向扫描机制增强局部与全局线索融合。
- Result: 在BraTS2020和BraTS2023数据集上的实验表明，SSFMamba在多项评估指标上均优于现有方法。
- Conclusion: SSFMamba通过有效融合空间与频率域特征，显著提升了3D医学图像分割的性能，验证了频率域信息在全局建模中的重要性。


### [31] [RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions](https://arxiv.org/abs/2508.03077)
*Anran Wu,Long Peng,Xin Di,Xueyuan Dai,Chen Wu,Yang Wang,Xueyang Fu,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出RobustGS模块，提升3D高斯泼溅在恶劣成像条件下的鲁棒性，实现高质量3D重建。

- Motivation: 现有前馈3D高斯泼溅方法假设输入多视图图像干净高质量，但实际场景中图像常受噪声、低光或雨等影响，导致重建质量下降。
- Method: 引入通用退化学习器和语义感知状态空间模型，增强退化感知能力并聚合语义相似信息。
- Result: 实验表明，RobustGS在多种退化条件下均实现最先进的重建质量。
- Conclusion: RobustGS模块可无缝集成现有方法，显著提升3D重建的鲁棒性和质量。


### [32] [Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models](https://arxiv.org/abs/2508.03079)
*Zaiying Zhao,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 研究探讨了大型视觉语言模型（LVLMs）的公平性，发现其输出存在广泛偏见，且文化、环境和行为因素比传统人口属性影响更大。

- Motivation: 现有研究主要关注人口属性（如种族和性别）的公平性，但更广泛的属性公平性尚未充分探索。
- Method: 利用大型语言模型（LLMs）构建开放集偏见属性知识库，评估LVLMs在细粒度属性上的公平性。
- Result: 实验结果显示LVLMs在多种属性上存在偏见，且文化、环境和行为因素对决策的影响更显著。
- Conclusion: 研究呼吁关注LVLMs在更广泛属性上的公平性问题，并强调非人口属性因素的重要性。


### [33] [Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification](https://arxiv.org/abs/2508.03081)
*Bo Zhang,Xu Xinan,Shuo Yan,Yu Bai,Zheng Zhang,Wufan Wang,Wendong Wang*

Main category: cs.CV

TL;DR: 提出了一种名为$C^2Aug$的对比跨包增强方法，通过从所有同类包中采样实例以增加伪包的多样性，并结合对比学习框架提升模型性能。

- Motivation: 现有伪包增强方法因采样范围有限导致多样性不足，且引入新实例可能减少关键实例的稀疏分布，影响模型在小肿瘤区域测试中的表现。
- Method: 提出$C^2Aug$方法，从所有同类包中采样实例；设计包级和组级对比学习框架以增强特征区分能力。
- Result: 实验表明$C^2Aug$在多项评估指标上优于现有方法。
- Conclusion: $C^2Aug$通过增加伪包多样性和对比学习框架，显著提升了模型性能，尤其在处理小肿瘤区域时表现更优。


### [34] [Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts](https://arxiv.org/abs/2508.03094)
*Jiantao Tan,Peixian Ma,Kanghao Chen,Zhiming Dai,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出了一种利用大语言模型生成的视觉概念作为语义指导的持续学习框架，通过跨模态注意力模块提升分类性能。

- Motivation: 现有方法仅依赖简单的文本模板，忽略了丰富的语义信息，限制了持续学习的效果。
- Method: 动态构建视觉概念池，使用相似性过滤机制避免冗余，并通过跨模态图像-概念注意力模块和注意力损失整合概念。
- Result: 在医学和自然图像数据集上实现了最先进的性能。
- Conclusion: 该方法有效且优越，代码将公开。


### [35] [AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video](https://arxiv.org/abs/2508.03100)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: AVATAR框架通过离线训练和时序优势塑造解决了多模态长视频推理中的数据效率、优势消失和信用分配问题，显著提升了性能。

- Motivation: 解决现有方法（如GRPO）在数据效率、优势消失和信用分配方面的局限性。
- Method: 采用离线训练架构提高样本效率，并通过时序优势塑造（TAS）优化关键推理阶段的信用分配。
- Result: 在多个基准测试中表现优异，性能提升显著，样本效率提高35%以上。
- Conclusion: AVATAR通过创新方法有效解决了多模态长视频推理中的关键问题，具有实际应用潜力。


### [36] [Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning](https://arxiv.org/abs/2508.03102)
*Tianjiao Jiang,Zhen Zhang,Yuhang Liu,Javen Qinfeng Shi*

Main category: cs.CV

TL;DR: 提出Causal CLIP Adapter (CCA)，通过无监督ICA显式解耦CLIP提取的视觉特征，减少可训练参数并避免过拟合，同时利用跨模态对齐提升分类性能。

- Motivation: 现有少样本学习方法依赖纠缠表示，需隐式恢复解混过程，限制了有效适应。
- Method: 结合ICA解耦CLIP视觉特征，并通过单向（微调文本分类器）和双向（跨注意力机制）增强跨模态对齐。
- Result: 在11个基准数据集上表现优于现有方法，分类准确率和鲁棒性显著提升。
- Conclusion: CCA在少样本学习和分布偏移下表现优异，计算高效。


### [37] [H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction](https://arxiv.org/abs/2508.03118)
*Heng Jia,Linchao Zhu,Na Zhao*

Main category: cs.CV

TL;DR: H3R提出了一种混合框架，结合体积潜在融合和基于注意力的特征聚合，解决了3D重建中几何精度与鲁棒性的权衡问题，实现了更快的收敛和更好的泛化能力。

- Motivation: 现有方法在3D重建中存在几何精度与鲁棒性的权衡问题，显式方法难以处理模糊区域，隐式方法收敛慢。H3R旨在通过混合框架解决这一问题。
- Method: H3R结合了体积潜在融合和基于注意力的特征聚合，包括几何一致性约束的潜在体积和相机感知Transformer。
- Result: H3R在多个数据集上表现优异，PSNR显著提升（RealEstate10K: +0.59 dB, ACID: +1.06 dB, DTU: +0.22 dB），收敛速度提高2倍。
- Conclusion: H3R通过混合框架实现了高效、泛化能力强的3D重建，支持多视图输入，并在多个基准测试中达到最优性能。


### [38] [Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery](https://arxiv.org/abs/2508.03127)
*Sai Ma,Zhuang Li,John A Taylor*

Main category: cs.CV

TL;DR: 论文介绍了Landsat30-AU数据集，用于提升视觉语言模型在卫星图像理解上的能力，并展示了微调模型的效果。

- Motivation: 现有数据集主要关注高分辨率、短期的卫星图像，忽视了低分辨率、多卫星、长期的存档数据（如Landsat），这些数据对全球监测至关重要。
- Method: 通过构建Landsat30-AU数据集，包含图像-标题对和视觉问答样本，利用通用视觉语言模型进行迭代优化和人工验证。
- Result: 现有模型在卫星图像理解上表现不佳，但轻量级微调显著提升了性能（如SPIDEr从0.11提升到0.31，VQA准确率从0.74提升到0.87）。
- Conclusion: Landsat30-AU填补了数据空白，展示了微调模型在卫星图像理解上的潜力，为未来研究提供了基准。


### [39] [COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks](https://arxiv.org/abs/2508.03132)
*Arion Zimmermann,Soon-Jo Chung,Fred Hadaegh*

Main category: cs.CV

TL;DR: COFFEE是一种实时姿态估计框架，用于小行星，通过结合太阳相位角信息和阴影不变特征，提供无偏差、高精度且高效的结果。

- Motivation: 解决现有方法在空间未知物体姿态估计中的不准确性和计算资源需求高的问题，尤其是对自投射阴影的鲁棒性不足。
- Method: 利用太阳相位角信息，结合稀疏神经网络和基于注意力的图神经网络，检测阴影不变的特征并匹配连续帧。
- Result: 在合成数据和小行星Apophis的渲染数据上，COFFEE比传统方法更准确，比深度学习方法快一个数量级。
- Conclusion: COFFEE提供了一种高效、无偏差的姿态估计解决方案，适用于空间任务中的实时需求。


### [40] [Uint: Building Uint Detection Dataset](https://arxiv.org/abs/2508.03139)
*Haozhou Zhai,Yanzhe Gao,Tianjiang Hu*

Main category: cs.CV

TL;DR: 介绍了一个通过无人机拍摄并增强的建筑单元火灾数据集，用于提升火灾检测模型的泛化能力。

- Motivation: 现有火灾数据中缺乏针对建筑单元的标注数据，限制了火灾预警和救援任务的模型训练。
- Method: 利用真实多层场景构建背景，结合运动模糊和亮度调整增强图像真实性，模拟无人机拍摄条件，并使用大模型生成不同位置的火灾效果。
- Result: 生成了包含1,978张图像的合成数据集，涵盖多种建筑场景。
- Conclusion: 该数据集能有效提升火灾单元检测的泛化能力，同时降低真实火灾数据收集的风险和成本。


### [41] [UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying](https://arxiv.org/abs/2508.03142)
*Chengyu Bai,Jintao Chen,Xiang Bai,Yilong Chen,Qi She,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的图像编辑框架UniEdit-I，通过理解、编辑和验证三个迭代步骤，实现统一视觉语言模型（VLM）的图像编辑能力。

- Motivation: 尽管现有的统一视觉语言模型在视觉理解和生成任务上表现优异，但如何轻松实现图像编辑能力仍未解决。
- Method: UniEdit-I通过三步迭代：1）理解步骤分析源图像并生成目标提示；2）编辑步骤引入时间自适应偏移实现连贯编辑；3）验证步骤检查目标提示与中间图像的匹配并提供反馈。
- Result: 基于BLIP3-o实现的方法在GEdit-Bench基准测试中达到了最先进的性能。
- Conclusion: UniEdit-I为统一视觉语言模型提供了一种无需训练的高保真图像编辑解决方案。


### [42] [SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance](https://arxiv.org/abs/2508.03143)
*Yanshu Wang,Xichen Xu,Xiaoning Lei,Guoyang Xie*

Main category: cs.CV

TL;DR: SARD是一种基于扩散的异常合成框架，通过区域约束扩散和判别性掩码引导，提升了空间可控性和区域保真度，显著优于现有方法。

- Motivation: 现有扩散方法在空间可控性和区域保真度上表现不足，限制了工业异常检测系统的鲁棒性提升。
- Method: 提出SARD框架，结合区域约束扩散（RCD）和判别性掩码引导（DMG），选择性更新异常区域并评估全局与局部保真度。
- Result: 在MVTec-AD和BTAD数据集上，SARD在分割精度和视觉质量上超越现有方法。
- Conclusion: SARD为像素级异常合成设定了新的技术标准。


### [43] [LORE: Latent Optimization for Precise Semantic Control in Rectified Flow-based Image Editing](https://arxiv.org/abs/2508.03144)
*Liangyang Ouyang,Jiafeng Mao*

Main category: cs.CV

TL;DR: LORE是一种无需训练的高效图像编辑方法，通过优化反转噪声解决现有方法在泛化和可控性上的核心限制，实现稳定、可控的通用概念替换。

- Motivation: 现有基于反转流的图像编辑方法存在语义偏差问题，导致编辑失败或非目标区域意外修改，尤其在源和目标语义差异大时更为严重。
- Method: 提出LORE方法，直接优化反转噪声，无需架构修改或模型微调。
- Result: 在PIEBench、SmartEdit和GapEdit三个基准测试中，LORE在语义对齐、图像质量和背景保真度上显著优于基线方法。
- Conclusion: LORE通过潜在空间优化实现了高效、通用的图像编辑，展示了其有效性和可扩展性。


### [44] [ChartCap: Mitigating Hallucination of Dense Chart Captioning](https://arxiv.org/abs/2508.03164)
*Junyoung Lim,Jaewoo Ahn,Gunhee Kim*

Main category: cs.CV

TL;DR: ChartCap是一个包含56.5万张真实图表图像的数据集，配有针对图表类型的密集标注，旨在解决现有数据集中无关信息和结构元素不足的问题。通过四阶段流程生成标注，并提出新的视觉一致性评分指标，实验证明基于ChartCap训练的模型能生成更准确、信息丰富且减少幻觉的标注。

- Motivation: 现有真实图表数据集存在无关信息过多和结构元素及关键洞察不足的问题，导致视觉语言模型生成准确且无幻觉的图表标注具有挑战性。
- Method: 设计四阶段流程生成仅基于图表可辨识数据的标注，并采用循环一致性人工验证加速质量控制。提出视觉一致性评分指标，评估标注质量。
- Result: 实验表明，基于ChartCap训练的模型生成的标注更准确、信息丰富且减少幻觉，优于开源、专有模型及人工标注。
- Conclusion: ChartCap通过高质量数据集和新指标，显著提升了图表标注的生成质量。


### [45] [SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision](https://arxiv.org/abs/2508.03177)
*Zhaoxu Li,Chenqi Kong,Yi Yu,Qiangqiang Wu,Xinghao Jiang,Ngai-Man Cheung,Bihan Wen,Alex Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: 论文研究了大型视觉语言模型（LVLMs）在处理风格化图像时的幻觉问题，并提出了一种新机制SAVER来缓解这一问题。

- Motivation: 尽管LVLMs在视觉-文本理解方面取得了突破，但幻觉问题限制了其实际应用，尤其是在风格化图像中。
- Method: 构建了一个包含摄影图像和风格化图像的数据集，并提出了SAVER机制，通过动态调整输出以减少幻觉。
- Result: 实验表明，风格化图像比摄影图像更容易引发幻觉，而SAVER在多种任务中表现优异。
- Conclusion: SAVER能有效减少风格化图像引起的幻觉，提升了LVLMs的实用性。


### [46] [Advancing Precision in Multi-Point Cloud Fusion Environments](https://arxiv.org/abs/2508.03179)
*Ulugbek Alibekov,Vanessa Staderini,Philipp Schneider,Doris Antensteiner*

Main category: cs.CV

TL;DR: 研究聚焦于通过点云和多点云匹配方法进行视觉工业检测，提出合成数据集和距离度量，并开发了新的CloudCompare插件以提高检测效率和准确性。

- Motivation: 提升工业视觉检测的自动化水平和精度。
- Method: 评估点云匹配方法，引入合成数据集和距离度量，开发CloudCompare插件。
- Result: 提高了点云注册和表面缺陷检测的效率和准确性。
- Conclusion: 提出的方法和工具显著改进了工业视觉检测系统的性能。


### [47] [Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting](https://arxiv.org/abs/2508.03180)
*Weihang Liu,Yuke Li,Yuxuan Li,Jingyi Yu,Xin Lou*

Main category: cs.CV

TL;DR: Duplex-GS提出了一种双层次框架，结合代理高斯表示和顺序无关渲染技术，显著提升了3D高斯溅射的渲染效率和质量。

- Motivation: 现有3D高斯溅射方法依赖计算密集的顺序alpha混合操作，导致资源受限平台上的性能瓶颈。
- Method: 通过代理高斯表示和单元搜索光栅化优化视图自适应基数排序，结合顺序无关透明度（OIT）技术，提出加权和渲染方法。
- Result: 实验表明，该方法在多种场景下表现稳健，速度提升1.5至4倍，基数排序开销减少52.2%至86.9%。
- Conclusion: Duplex-GS框架验证了OIT在高斯溅射中的优势，实现了高效高质量的渲染。


### [48] [Monocular Depth Estimation with Global-Aware Discretization and Local Context Modeling](https://arxiv.org/abs/2508.03186)
*Heng Wu,Qian Zhang,Guixu Zhang*

Main category: cs.CV

TL;DR: 提出了一种结合局部和全局线索的单目深度估计方法，通过Gated Large Kernel Attention Module（GLKAM）和Global Bin Prediction Module（GBPM）提升精度。

- Motivation: 单目深度估计因从单视图恢复3D结构的病态性存在固有模糊性，需结合局部和全局线索以提高准确性。
- Method: 提出GLKAM模块利用大核卷积和门控机制捕捉多尺度局部结构信息；GBPM模块预测全局深度分布以指导深度回归。
- Result: 在NYU-V2和KITTI数据集上表现优异，超越现有方法。
- Conclusion: GLKAM和GBPM模块有效提升了单目深度估计的精度。


### [49] [Unifying Locality of KANs and Feature Drift Compensation for Data-free Continual Face Forgery Detection](https://arxiv.org/abs/2508.03189)
*Tianshuo Zhang,Siran Peng,Li Gao,Haoyuan Zhang,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了一种基于KAN的持续人脸伪造检测框架（KAN-CFD），通过改进KAN的局部可塑性来解决持续学习中的灾难性遗忘问题。

- Motivation: 人脸伪造技术的快速发展要求检测器持续适应新方法，但现有方法在学习新类型时会导致对旧类型的性能快速下降（灾难性遗忘）。KAN因其局部可塑性适合解决此问题，但存在对高维图像建模无效和特征重叠时映射崩溃的局限性。
- Method: 提出KAN-CFD框架，包括域组KAN检测器（DG-KD）和无数据回放的特征分离策略（FS-KDCP）。DG-KD使KAN适应高维图像输入并保留局部可塑性；FS-KDCP避免输入空间重叠。
- Result: 实验结果表明，该方法在减少遗忘的同时实现了优越性能。
- Conclusion: KAN-CFD通过改进KAN的局部可塑性和避免输入空间重叠，有效解决了持续学习中的灾难性遗忘问题。


### [50] [Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network](https://arxiv.org/abs/2508.03197)
*Tao Chen,Dan Zhang,Da Chen,Huazhu Fu,Kai Jin,Shanshan Wang,Laurent D. Cohen,Yitian Zhao,Quanyong Yi,Jiong Zhang*

Main category: cs.CV

TL;DR: 该论文提出了MTG-Net网络，用于OCTA图像中CNV区域和血管的精确分割，并构建了首个公开的CNV数据集CNVSeg。

- Motivation: CNV是湿性AMD的主要特征，但其分割面临形状不规则和成像限制等挑战，且缺乏公开数据集。
- Method: MTG-Net结合区域和血管形态信息，通过多任务框架和图推理模块（MIGR和MRGR）实现分割优化。
- Result: 实验显示MTG-Net在区域和血管分割上的Dice分数分别为87.21%和88.12%，优于现有方法。
- Conclusion: MTG-Net和CNVSeg为CNV分析提供了有效工具，解决了现有挑战。


### [51] [AlignCAT: Visual-Linguistic Alignment of Category and Attributefor Weakly Supervised Visual Grounding](https://arxiv.org/abs/2508.03201)
*Yidan Wang,Chenyi Zhuang,Wutao Liu,Pan Gao,Nicu Sebe*

Main category: cs.CV

TL;DR: AlignCAT是一种基于查询的语义匹配框架，用于弱监督视觉定位（VG），通过粗粒度对齐和细粒度对齐模块提升视觉-语言对齐能力。

- Motivation: 现有方法在区分文本表达中的细微语义差异时缺乏强跨模态推理能力，尤其是面对类别和属性模糊性时。
- Method: 提出AlignCAT框架，包含粗粒度对齐模块（利用类别信息和全局上下文）和细粒度对齐模块（利用描述性信息和词级文本特征）。
- Result: 在RefCOCO、RefCOCO+和RefCOCOg三个基准测试中表现优于现有弱监督方法。
- Conclusion: AlignCAT通过充分利用语言线索，逐步过滤未对齐的视觉查询，提升了对比学习效率。


### [52] [Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration](https://arxiv.org/abs/2508.03207)
*Ting Lei,Shaofeng Yin,Qingchao Chen,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为INP-CC的开集词汇人-物交互检测方法，通过交互感知提示和概念校准提升检测性能。

- Motivation: 现有方法依赖视觉语言模型，但图像级预训练与细粒度区域级交互检测不匹配，且文本描述编码效果不佳。
- Method: 提出交互感知提示生成器和语言模型引导的概念校准，结合负采样策略优化跨模态相似性建模。
- Result: 在SWIG-HOI和HICO-DET数据集上显著优于现有方法。
- Conclusion: INP-CC通过动态提示和概念校准有效提升了开集词汇人-物交互检测的性能。


### [53] [GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations](https://arxiv.org/abs/2508.03209)
*Xinwei Liu,Xiaojun Jia,Yuan Xun,Simeng Qin,Xiaochun Cao*

Main category: cs.CV

TL;DR: GeoShield是一种新型对抗框架，旨在保护地理隐私，通过特征解耦、暴露元素识别和多尺度优化模块，有效抵御高级视觉语言模型的地理位置推断。

- Motivation: 高级视觉语言模型（如GPT-4o）能从公开图像推断用户位置，威胁地理隐私，现有对抗扰动方法在高分辨率图像和低扰动预算下表现不佳。
- Method: GeoShield包含三个模块：特征解耦模块分离地理与非地理信息，暴露元素识别模块定位图像中的地理揭示区域，多尺度增强模块联合优化全局和局部扰动。
- Result: 实验表明，GeoShield在黑盒设置下优于现有方法，提供强隐私保护且对图像质量影响最小。
- Conclusion: GeoShield是首个针对高级视觉语言模型地理位置推断的对抗扰动防御方案，为隐私保护提供了实用有效的解决方案。


### [54] [The Power of Many: Synergistic Unification of Diverse Augmentations for Efficient Adversarial Robustness](https://arxiv.org/abs/2508.03213)
*Wang Yu-Hang,Shiwei Li,Jianxiang Liao,Li Bohan,Jian Liu,Wenfei Yin*

Main category: cs.CV

TL;DR: 论文提出了一种名为UAA的高效对抗防御框架，通过预计算通用变换离线生成对抗扰动，显著提升了训练效率和模型鲁棒性。

- Motivation: 对抗扰动对深度学习模型构成严重威胁，现有防御方法如对抗训练（AT）计算成本高且影响标准性能，数据增强技术则要么鲁棒性提升有限，要么训练开销大。因此，开发高效且鲁棒的防御机制至关重要。
- Method: 提出Universal Adversarial Augmenter（UAA）框架，通过离线预计算通用变换，解耦昂贵的扰动生成过程与模型训练，从而高效生成独特对抗扰动。
- Result: 在多个基准测试中，UAA表现出色，成为数据增强对抗防御的新SOTA，且无需在线生成对抗样本。
- Conclusion: UAA为构建鲁棒模型提供了一种实用高效的方法，其代码已公开。


### [55] [ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow](https://arxiv.org/abs/2508.03218)
*Shanshan Guo,Xiwen Liang,Junfan Lin,Yuzheng Zhuang,Liang Lin,Xiaodan Liang*

Main category: cs.CV

TL;DR: 论文提出了一种名为ActionSink的新型机器人操作框架，通过自监督方式将机器人动作重新定义为视频中的光流（称为“动作流”），并通过检索和整合动作流来提升动作估计的精度。

- Motivation: 低级别动作估计的低精度是学习型机器人操作性能的主要限制因素，因此需要一种新方法来提高动作估计的精确性。
- Method: ActionSink框架包含两个主要模块：1）粗到细的动作流匹配器，通过迭代检索和去噪过程提高动作流精度；2）动态动作流整合器，利用工作记忆池动态管理历史动作流，并通过多层融合模块整合当前和历史的动作流。
- Result: 在LIBERO基准测试中，ActionSink比之前的最优方法提升了7.9%的成功率，在长时视觉任务LIBERO-Long上获得了近8%的准确率提升。
- Conclusion: ActionSink通过动作流的概念和动态整合机制，显著提高了机器人动作估计的精度，为学习型机器人操作领域提供了新的解决方案。


### [56] [Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing](https://arxiv.org/abs/2508.03227)
*Hongyu Shen,Junfeng Ni,Yixin Chen,Weishuo Li,Mingtao Pei,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出Gaussian Instance Tracing (GIT)方法，通过增强高斯表示和自适应密度控制，解决2D到3D分割中的不一致性问题。

- Motivation: 现有方法在2D到3D分割中存在视角间不一致和边界噪声问题，忽视语义线索。
- Method: 引入GIT，通过实例权重矩阵校正2D分割不一致性，并采用自适应密度控制优化高斯分布。
- Result: 实验表明，GIT能提取干净的3D资产，提升分割一致性，适用于多种应用场景。
- Conclusion: GIT有效解决了2D到3D分割中的一致性问题，为场景编辑等应用提供了支持。


### [57] [Zero-shot Shape Classification of Nanoparticles in SEM Images using Vision Foundation Models](https://arxiv.org/abs/2508.03235)
*Freida Barnatan,Emunah Goldstein,Einav Kalimian,Orchen Madar,Avi Huri,David Zitoun,Ya'akov Mandelbaum,Moshe Amitay*

Main category: cs.CV

TL;DR: 提出了一种基于视觉基础模型的零样本分类流程，用于SEM图像中纳米颗粒形态的高效表征，无需大量标注数据或复杂训练。

- Motivation: 传统深度学习方法需要大量标注数据和计算资源，限制了纳米颗粒研究的可及性。
- Method: 结合Segment Anything Model (SAM)和DINOv2进行对象分割和特征嵌入，使用轻量级分类器实现高精度分类。
- Result: 方法在三个数据集上表现优于微调的YOLOv11和ChatGPT o4-mini-high基线，对小数据集和形态变化具有鲁棒性。
- Conclusion: 基础模型为自动化显微镜图像分析提供了高效且易用的替代方案。


### [58] [FFHQ-Makeup: Paired Synthetic Makeup Dataset with Facial Consistency Across Multiple Styles](https://arxiv.org/abs/2508.03241)
*Xingchao Yang,Shiori Ueda,Yuantian Huang,Tomoya Akiyama,Takafumi Taketomi*

Main category: cs.CV

TL;DR: 论文提出FFHQ-Makeup数据集，通过改进的化妆迁移方法生成高质量配对化妆图像，填补了现有数据集的不足。

- Motivation: 现有化妆数据集难以收集高质量配对图像，合成方法存在失真或一致性差的问题。
- Method: 基于FFHQ数据集，改进化妆迁移方法，生成18K身份的90K配对图像。
- Result: 构建了首个高质量化妆数据集FFHQ-Makeup，每身份配5种化妆风格。
- Conclusion: FFHQ-Makeup填补了化妆数据集的空白，为美容相关任务提供资源。


### [59] [MVTOP: Multi-View Transformer-based Object Pose-Estimation](https://arxiv.org/abs/2508.03243)
*Lukas Ranftl,Felix Brendel,Bertram Drost,Carsten Steger*

Main category: cs.CV

TL;DR: MVTOP是一种基于Transformer的多视角刚性物体姿态估计方法，通过早期融合视角特征解决单视角无法处理的姿态模糊问题。

- Motivation: 解决单视角或后处理单视角姿态无法解决的姿态模糊问题，提出一种更可靠的多视角方法。
- Method: 通过视线建模多视角几何，融合多视角信息，无需额外数据（如深度），端到端可训练。
- Result: 在合成数据集上优于单视角和现有多视角方法，在YCB-V数据集上表现竞争性。
- Conclusion: MVTOP是一种高效、可靠的多视角姿态估计方法，填补了现有技术的空白。


### [60] [Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution](https://arxiv.org/abs/2508.03244)
*Chuanzhi Xu,Haoxian Zhou,Langyi Chen,Yuk Ying Chung,Qiang Qu*

Main category: cs.CV

TL;DR: 提出了一种基于脉冲神经网络的超轻量级事件超分辨率方法，适用于资源受限设备。

- Motivation: 事件相机的高时间分辨率和低延迟优势受限于空间分辨率，影响精细感知任务。
- Method: 采用双向前极性分离事件编码策略和可学习的时空极性感知损失函数。
- Result: 在多个数据集上实现了竞争性的超分辨率性能，同时显著减小模型规模和推理时间。
- Conclusion: 轻量化设计使其可嵌入事件相机或作为下游视觉任务的高效前端预处理。


### [61] [Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion](https://arxiv.org/abs/2508.03252)
*Wentao Qu,Guofeng Mei,Jing Wang,Yujiao Wu,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: RSDNet提出了一种基于DDPM的单阶段稀疏3D目标检测方法，通过轻量级去噪网络和可分离潜在框架（DLF）提升效率和鲁棒性。

- Motivation: 现有方法依赖多步迭代推理，效率低；RSDNet旨在通过单步推理和潜在空间去噪解决这一问题。
- Method: RSDNet使用多级去噪自编码器（DAEs）在潜在特征空间学习去噪过程，并引入语义-几何条件引导增强稀疏表示。
- Result: 在公开基准测试中，RSDNet表现优于现有方法，达到最先进的检测性能。
- Conclusion: RSDNet通过单步推理和潜在空间去噪，实现了高效且鲁棒的3D目标检测。


### [62] [V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models](https://arxiv.org/abs/2508.03254)
*Jisoo Kim,Wooseok Seo,Junwan Kim,Seungho Park,Sooyeon Park,Youngjae Yu*

Main category: cs.CV

TL;DR: 提出了一种结合DPO和SFT的蒸馏方法ReDPO，用于降低文本到视频模型的算力需求，同时保持性能。

- Motivation: 解决现有蒸馏方法因监督微调导致的模式崩溃问题，提升剪枝模型的性能。
- Method: 结合DPO和SFT，提出ReDPO方法，并引入V.I.P.框架筛选高质量数据集。
- Result: 在VideoCrafter2和AnimateDiff上分别减少36.2%和67.5%参数，性能不降反升。
- Conclusion: ReDPO和V.I.P.框架能高效生成高质量视频，适用于资源受限环境。


### [63] [Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation](https://arxiv.org/abs/2508.03256)
*Gang Dai,Yifan Zhang,Yutao Qin,Qiangya Guo,Shuangping Huang,Shuicheng Yan*

Main category: cs.CV

TL;DR: DiffBrush是一种基于扩散的模型，专注于生成手写文本行，通过解耦内容和风格学习，以及多尺度内容学习，实现了高质量的文本行生成。

- Motivation: 现有方法主要关注孤立单词，而真实手写文本需要处理单词间关系（如垂直对齐和水平间距），因此生成整行文本更具挑战性和实用性。
- Method: DiffBrush采用内容解耦风格学习（通过列和行掩码分离风格和内容）和多尺度内容学习（使用行和单词鉴别器确保全局一致性和局部准确性）。
- Result: 实验表明，DiffBrush在风格再现和内容保留方面表现优异，能生成高质量文本行。
- Conclusion: DiffBrush通过创新方法解决了手写文本行生成的复杂问题，为相关领域提供了有效工具。


### [64] [EgoPrompt: Prompt Pool Learning for Egocentric Action Recognition](https://arxiv.org/abs/2508.03266)
*Huaihai Lyu,Chaofan Chen,Yuheng Ji,Changsheng Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于提示学习的框架EgoPrompt，用于解决第一人称视角动作识别任务，通过统一提示池空间和注意力机制增强动词与名词组件间的交互，显著提升了性能。

- Motivation: 现有方法将动词和名词组件视为独立分类任务，忽略了它们的语义和上下文关系，导致表示碎片化和泛化能力不足。
- Method: 构建统一提示池空间，分解组件表示并通过注意力机制融合，引入多样化池标准（Prompt Selection Frequency Regularization和Prompt Knowledge Orthogonalization）优化提示池。
- Result: 在Ego4D、EPIC-Kitchens和EGTEA数据集上，EgoPrompt在跨数据集和基础到新泛化基准测试中均达到最优性能。
- Conclusion: EgoPrompt通过组件间交互和多样化提示池优化，显著提升了第一人称动作识别的表现。


### [65] [Efficient Multi-Slide Visual-Language Feature Fusion for Placental Disease Classification](https://arxiv.org/abs/2508.03277)
*Hang Guo,Qing Zhang,Zixuan Gao,Siyuan Yang,Shulin Peng,Xiang Tao,Ting Yu,Yan Wang,Qingli Li*

Main category: cs.CV

TL;DR: 提出了一种名为EmmPD的高效多模态框架，用于通过全切片图像（WSI）预测胎盘疾病，解决了现有方法在补丁选择和全局上下文保留上的不足。

- Motivation: 胎盘疾病的准确预测对预防母婴严重并发症至关重要，但现有WSI分析方法在补丁选择和全局上下文保留上存在显著局限性。
- Method: 提出两阶段补丁选择模块和多模态融合模块，结合无参数和可学习压缩策略，并通过自适应图学习和文本医疗报告增强特征表示。
- Result: 在自建和公开数据集上验证，EmmPD达到了最先进的诊断性能。
- Conclusion: EmmPD框架有效平衡了计算效率和特征保留，提升了胎盘疾病的诊断准确性。


### [66] [Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation](https://arxiv.org/abs/2508.03300)
*Jun Luo,Zijing Zhao,Yang Liu*

Main category: cs.CV

TL;DR: SDGPA提出了一种零样本域自适应语义分割方法，通过合成数据生成和渐进适应策略解决训练与测试数据分布偏移问题。

- Motivation: 解决深度学习语义分割模型在训练与测试数据分布偏移时的性能限制问题，尤其是在零样本域自适应场景下。
- Method: 利用预训练的文本到图像扩散模型生成目标风格图像，通过裁剪和编辑小区域提高空间精度，并构建中间域和渐进适应策略。
- Result: 实验表明，SDGPA在零样本语义分割任务中达到了最先进的性能。
- Conclusion: SDGPA通过合成数据和渐进适应策略，有效解决了零样本域自适应语义分割的挑战。


### [67] [BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices](https://arxiv.org/abs/2508.03313)
*Libo Zhang,Xinyu Yi,Feng Xu*

Main category: cs.CV

TL;DR: BaroPoser结合IMU和气压计数据，实时估计人体姿态和全局位移，尤其在非平坦地形上表现优于现有方法。

- Motivation: 现有方法因传感器数据稀疏和缺乏非平坦地形数据集，在姿态估计和全局位移预测上表现不佳。
- Method: 利用智能手机和智能手表的气压计数据估计高度变化，提出局部大腿坐标系以分离局部和全局运动。
- Result: 在公开数据集和实际记录中，BaroPoser优于仅使用IMU的现有方法。
- Conclusion: BaroPoser通过结合气压计数据显著提升了非平坦地形下的人体姿态估计和全局位移预测能力。


### [68] [Architectural Insights into Knowledge Distillation for Object Detection: A Comprehensive Review](https://arxiv.org/abs/2508.03317)
*Mahdi Golizadeh,Nassibeh Golizadeh,Mohammad Ali Keyvanrad,Hossein Shirazi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于架构的知识蒸馏（KD）分类法，用于解决目标检测中的计算成本问题，并评估了不同方法在MS COCO和PASCAL VOC数据集上的表现。

- Motivation: 目标检测在深度学习中的改进通常伴随计算成本的增加，限制了在资源受限设备上的部署。知识蒸馏（KD）通过让学生模型从教师模型中学习，提供了一种有效的解决方案。
- Method: 论文提出了一种新颖的架构中心分类法，区分了基于CNN和Transformer的检测器的KD方法，并评估了代表性方法在MS COCO和PASCAL VOC数据集上的表现。
- Result: 通过mAP@0.5作为性能指标，对不同KD方法进行了比较分析，展示了其有效性。
- Conclusion: 提出的分类法和分析旨在澄清KD在目标检测中的发展现状，突出当前挑战，并指导未来研究以实现高效和可扩展的检测系统。


### [69] [Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation](https://arxiv.org/abs/2508.03320)
*Peiyu Wang,Yi Peng,Yimeng Gan,Liang Hu,Tianyidan Xie,Xiaokun Wang,Yichen Wei,Chuanxin Tang,Bo Zhu,Changshi Li,Hongyang Wei,Eric Li,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork UniPic是一个15亿参数的自回归模型，统一了图像理解、文本到图像生成和图像编辑，无需任务特定适配器，在消费级硬件上实现最先进性能。

- Motivation: 旨在证明紧凑的多模态系统可以在资源有限的情况下实现高性能，消除任务特定模块的需求。
- Method: 采用解耦编码策略、渐进式分辨率训练计划和精心策划的数据集，结合任务特定奖励模型。
- Result: 在多个基准测试中表现优异，如GenEval得分0.86，DPG-Bench复杂生成记录85.5，并高效生成高分辨率图像。
- Conclusion: Skywork UniPic为可部署的高保真多模态AI提供了实用范例，代码和权重已公开。


### [70] [Live Demonstration: Neuromorphic Radar for Gesture Recognition](https://arxiv.org/abs/2508.03324)
*Satyapreet Singh Yadav,Chandra Sekhar Seelamantula,Chetan Singh Thakur*

Main category: cs.CV

TL;DR: 提出了一种基于神经形态雷达的低功耗实时手势识别框架，采用事件驱动架构，显著降低了计算和功耗开销。

- Motivation: 传统雷达手势识别系统需要连续采样和处理数据，导致高功耗和计算负担。本文旨在通过生物启发的异步编码和事件驱动处理，实现高效低功耗的手势识别。
- Method: 系统包括24 GHz多普勒雷达前端和神经形态采样器，将中频信号通过异步sigma-delta编码转换为稀疏脉冲表示，并由轻量级神经网络在Cortex-M0微控制器上处理。
- Result: 在七名用户的五种手势数据集上，系统实现了>85%的实时准确率。
- Conclusion: 这是首个将生物启发的异步sigma-delta编码和事件驱动处理框架应用于雷达手势识别的研究，显著提升了效率和功耗表现。


### [71] [LRDDv2: Enhanced Long-Range Drone Detection Dataset with Range Information and Comprehensive Real-World Challenges](https://arxiv.org/abs/2508.03331)
*Amirreza Rouhi,Sneh Patel,Noah McCarthy,Siddiqa Khan,Hadi Khorsand,Kaleb Lefkowitz,David K. Han*

Main category: cs.CV

TL;DR: 介绍了LRDDv2数据集，用于长距离无人机检测，包含39,516张标注图像，新增了目标距离信息。

- Motivation: 无人机使用激增，需要远距离检测以确保安全，但现有数据集不足。
- Method: 扩展了LRDDv1数据集，增加了图像多样性和目标距离信息。
- Result: LRDDv2数据集更全面，支持无人机距离估计算法开发。
- Conclusion: LRDDv2为长距离无人机检测研究提供了更丰富的资源。


### [72] [Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation](https://arxiv.org/abs/2508.03334)
*Xunzhi Xiang,Yabo Chen,Guiyu Zhang,Zhongyu Wang,Zhe Gao,Quanming Xiang,Gonghu Shang,Junqi Liu,Haibin Huang,Yang Gao,Chi Zhang,Qi Fan,Xuelong Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为MMPL的框架，通过分层规划（Micro和Macro Planning）解决自回归扩散模型在长视频生成中的时间漂移问题，并实现并行化生成。

- Motivation: 自回归扩散模型在短视频生成中表现优异，但在长视频生成中因时间漂移和并行化限制而效果不佳。
- Method: 采用分层规划框架MMPL，分为Micro Planning（预测关键帧）和Macro Planning（全局一致性规划），并通过并行填充中间帧。
- Result: 实验表明，该方法在长视频生成的质量和稳定性上优于现有模型。
- Conclusion: MMPL框架有效解决了长视频生成中的时间漂移问题，并实现了高效的并行化生成。


### [73] [Beyond Illumination: Fine-Grained Detail Preservation in Extreme Dark Image Restoration](https://arxiv.org/abs/2508.03336)
*Tongshun Zhang,Pingping Liu,Zixuan Zhong,Zijian Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: 提出了一种双阶段方法，通过残差傅里叶引导模块（RFGM）和Mamba模块，有效恢复极暗图像中的细节和边缘。

- Motivation: 现有方法在恢复极暗图像细节和边缘方面效果有限，影响了下游应用（如文本和边缘检测）的性能。
- Method: 第一阶段使用RFGM在频域恢复全局光照；第二阶段通过Patch Mamba和Grad Mamba模块细化纹理结构。
- Result: 在多个基准数据集和下游应用中显著提升了细节恢复性能，且模块轻量高效。
- Conclusion: 该方法在细节恢复和边缘重建方面表现优异，并可无缝集成到现有傅里叶框架中。


### [74] [Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration](https://arxiv.org/abs/2508.03337)
*Shaoguang Wang,Jianxiang He,Yijie Xu,Ziyang Chen,Weiyu Guo,Hui Xiong*

Main category: cs.CV

TL;DR: 论文提出了一种名为自适应帧剪枝（AFP）的新方法，通过智能剪枝减少视频问答任务中的帧数和令牌成本，同时提升性能。

- Motivation: 多模态大语言模型（MLLMs）在视频问答（Video-QA）中因高令牌成本而受限，且过多的帧数会导致性能下降。现有关键帧选择方法仍存在冗余问题。
- Method: AFP采用自适应层次聚类算法在融合的ResNet-50和CLIP特征空间上识别并合并冗余帧，同时引入轻量级文本语义图补充信息。
- Result: 实验表明，AFP在多个基准测试中减少了86.9%的帧数和83.2%的令牌成本，且性能优于基线。
- Conclusion: AFP不仅提高了效率，还提升了准确性，为视频问答任务提供了高效解决方案。


### [75] [CIVQLLIE: Causal Intervention with Vector Quantization for Low-Light Image Enhancement](https://arxiv.org/abs/2508.03338)
*Tongshun Zhang,Pingping Liu,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: 提出CIVQLLIE框架，通过离散表示学习和因果干预解决低光图像增强问题。

- Motivation: 夜间图像可见性差，现有方法缺乏解释性或依赖不可靠先验，物理方法假设简化。
- Method: 使用向量量化（VQ）学习视觉标记，结合多级因果干预（PCI、FCI、LSAG）和细节重建模块（HDRM）。
- Result: 通过离散表示和因果干预，有效纠正低光图像分布偏移，提升增强效果。
- Conclusion: CIVQLLIE框架在低光图像增强中表现出色，结合了可靠先验和灵活干预。


### [76] [WaMo: Wavelet-Enhanced Multi-Frequency Trajectory Analysis for Fine-Grained Text-Motion Retrieval](https://arxiv.org/abs/2508.03343)
*Junlong Ren,Gangjian Zhang,Honghao Fu,Pengcheng Wu,Hao Wang*

Main category: cs.CV

TL;DR: WaMo是一种基于小波的多频率特征提取框架，用于改进文本-运动检索（TMR），通过捕捉身体关节的多分辨率细节实现细粒度对齐。

- Motivation: 现有方法忽视了人体结构的复杂性和时空动态性，导致语义对齐不精确。
- Method: WaMo包含轨迹小波分解、轨迹小波重构和乱序运动序列预测三个关键组件。
- Result: 在HumanML3D和KIT-ML数据集上，Rsum分别提升17.0%和18.2%，优于现有方法。
- Conclusion: WaMo通过多频率特征提取和时空信息保留，显著提升了文本-运动检索的性能。


### [77] [VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation](https://arxiv.org/abs/2508.03351)
*Yufei Xue,Yushi Huang,Jiawei Shao,Jun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉语言模型（VLMs）的量化方法VLMQ，解决了现有方法因模态差异导致的性能下降问题，并在低比特量化下表现出色。

- Motivation: 现有基于Hessian的LLM量化方法在VLMs中因忽略视觉和文本令牌的差异导致性能下降，因此需要一种针对VLMs的量化框架。
- Method: VLMQ通过优化重要性感知目标，生成增强的Hessian矩阵，并利用轻量级块级反向传播计算令牌级重要性因子。
- Result: 在8个基准测试中，VLMQ在0.5B∼32B的VLMs上表现出色，尤其在低比特量化下（如2比特量化下MME-RealWorld提升16.45%）。
- Conclusion: VLMQ为VLMs提供了一种高效且有效的量化方法，解决了模态差异问题，并在低比特量化下实现了SOTA性能。


### [78] [FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models](https://arxiv.org/abs/2508.03356)
*Matteo Caligiuri,Francesco Barbato,Donald Shenaj,Umberto Michieli,Pietro Zanuttigh*

Main category: cs.CV

TL;DR: FedPromo是一种高效适应大规模基础模型的联邦学习框架，通过轻量级代理模型减少计算开销，同时保护隐私。

- Motivation: 传统联邦学习在大模型上计算资源需求高，FedPromo旨在解决这一问题。
- Method: 采用两阶段方法：服务器端知识蒸馏和对齐表示，客户端训练轻量级分类器并聚合。
- Result: 在五个图像分类基准测试中表现优于现有方法。
- Conclusion: FedPromo在资源受限的客户端上实现了高效、隐私保护的模型适应。


### [79] [Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One Image Restoration](https://arxiv.org/abs/2508.03373)
*Ni Tang,Xiaotong Luo,Zihan Cheng,Liangtai Zhou,Dongxiao Zhang,Yanyun Qu*

Main category: cs.CV

TL;DR: 提出了一种高效的图像修复方法DOD，通过单步采样实现高性能修复，解决了现有方法的高计算成本和适应性不足问题。

- Motivation: 现有基于扩散模型的图像修复方法计算成本高且适应性有限，需要改进。
- Method: 引入多退化特征调制和参数高效的低秩适应，结合高保真细节增强模块。
- Result: 在视觉质量和推理效率上优于现有方法。
- Conclusion: DOD方法在图像修复中表现出色，具有高效性和适应性。


### [80] [GRASPing Anatomy to Improve Pathology Segmentation](https://arxiv.org/abs/2508.03374)
*Keyi Li,Alexander Jaus,Jens Kleesiek,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: GRASP是一个模块化框架，通过伪标签整合和特征对齐，利用解剖学分割模型增强病理分割模型，无需重新训练解剖学组件。

- Motivation: 当前深度学习方法忽略解剖学背景，而放射科医生依赖解剖学知识准确识别病理。GRASP旨在填补这一差距。
- Method: GRASP通过伪标签输入通道和基于变压器的解剖学特征融合，将解剖学知识整合到病理分割模型中。
- Result: 在多个评估指标和架构中，GRASP表现优异，其双解剖学注入策略有效结合了解剖学背景。
- Conclusion: GRASP成功地将解剖学背景整合到病理分割中，提升了模型性能。


### [81] [GaitAdapt: Continual Learning for Evolving Gait Recognition](https://arxiv.org/abs/2508.03375)
*Jingjie Wang,Shunli Zhang,Xiang Wei,Senmao Tian*

Main category: cs.CV

TL;DR: 提出了一种名为GaitAdapter的持续步态识别方法，通过GPAK模块和EDSM方法，解决了传统方法在新数据集上性能下降的问题。

- Motivation: 传统步态识别方法在新数据集上需要重新训练，且难以保留旧数据集的知识，导致性能下降。
- Method: 提出GaitAdapter方法，结合GPAK模块（图神经网络）和EDSM（欧几里得距离稳定性方法），逐步增强步态识别能力。
- Result: 实验表明，GaitAdapter能有效保留多任务知识，显著优于其他方法。
- Conclusion: GaitAdapter解决了持续学习中的知识保留问题，提升了步态识别的性能。


### [82] [Neutralizing Token Aggregation via Information Augmentation for Efficient Test-Time Adaptation](https://arxiv.org/abs/2508.03388)
*Yizhe Xiong,Zihan Zhou,Yiwen Liang,Hui Chen,Zijia Lin,Tianxiang Hao,Fan Zhang,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 论文提出了NAVIA方法，通过信息增强中和令牌聚合的影响，在保持测试时适应能力的同时降低计算开销。

- Motivation: 现有测试时适应（TTA）方法计算开销大，令牌聚合虽高效但会导致性能下降，需解决高效测试时适应（ETTA）问题。
- Method: 提出NAVIA方法，通过增强[CLS]令牌嵌入和引入自适应偏置来恢复信息损失，优化熵最小化。
- Result: NAVIA在多个分布偏移基准上性能提升2.5%，推理延迟降低20%。
- Conclusion: NAVIA有效解决了ETTA问题，平衡了适应能力和计算效率。


### [83] [DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition](https://arxiv.org/abs/2508.03397)
*Xinzhu Li,Juepeng Zheng,Yikun Chen,Xudong Mao,Guanghui Yue,Wei Zhou,Chenlei Lv,Ruomei Wang,Fan Zhou,Baoquan Zhao*

Main category: cs.CV

TL;DR: DepthGait框架通过结合RGB深度图和剪影，提升了步态识别的性能，采用多尺度跨层次融合方案，实现了最先进的识别准确率。

- Motivation: 现有2D剪影和骨架表示无法充分处理视角变化和捕捉步态细节，需要更丰富的输入模态。
- Method: 引入RGB深度图作为新模态，结合剪影，开发多尺度跨层次融合方案。
- Result: 在标准基准测试中达到最先进性能，挑战性数据集上获得高排名1准确率。
- Conclusion: DepthGait通过多模态融合显著提升了步态识别的鲁棒性和准确性。


### [84] [SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models](https://arxiv.org/abs/2508.03402)
*Pingchuan Ma,Xiaopei Yang,Yusong Li,Ming Gui,Felix Krause,Johannes Schusterbauer,Björn Ommer*

Main category: cs.CV

TL;DR: SCFlow提出了一种通过可逆合并风格与内容的方法，避免显式解耦的挑战，并在零样本设置中表现出色。

- Motivation: 现有方法难以明确解耦风格与内容，因为它们在语义上重叠且人类感知主观性强。
- Method: SCFlow采用流匹配框架，学习风格与内容之间的双向映射，通过合成数据集训练。
- Result: SCFlow在ImageNet-1k和WikiArt上零样本表现优异，解耦能力自然显现。
- Conclusion: 可逆合并风格与内容的方法能自然实现解耦，且无需显式监督。


### [85] [Sparsity and Total Variation Constrained Multilayer Linear Unmixing for Hyperspectral Imagery](https://arxiv.org/abs/2508.03403)
*Gang Yang*

Main category: cs.CV

TL;DR: 提出了一种结合稀疏性和全变分约束的多层线性解混方法（STVMLU），用于高光谱图像解混，通过ADMM优化，性能优于其他算法。

- Motivation: 高光谱解混是预处理关键步骤，现有方法在精度和空间相似性考虑上不足，需改进。
- Method: 基于多层矩阵分解模型，引入全变分约束考虑空间相似性，采用L1/2范数稀疏约束表征丰度矩阵稀疏性，使用ADMM优化。
- Result: 实验表明STVMLU性能优于其他算法。
- Conclusion: STVMLU通过结合空间相似性和稀疏性约束，显著提升了解混精度。


### [86] [Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling](https://arxiv.org/abs/2508.03404)
*Xinlei Yu,Zhangquan Chen,Yudong Zhang,Shilin Lu,Ruolin Shen,Jiangning Zhang,Xiaobin Hu,Yanwei Fu,Shuicheng Yan*

Main category: cs.CV

TL;DR: MACT是一个多智能体协作框架，通过测试时扩展和混合奖励建模，显著提升了视觉文档理解和视觉问答任务的性能。

- Motivation: 现有视觉语言模型在参数规模、自校正能力和复杂推理任务上表现不足，特别是在长视觉上下文和文档任务中。
- Method: MACT由四个明确分工的智能体（规划、执行、判断和回答）组成，采用混合奖励建模和智能体级测试时扩展策略。
- Result: MACT在15个基准测试中的13个领先，且在长视觉上下文和复杂推理任务中表现突出。
- Conclusion: MACT以较小参数规模实现了高性能，为视觉语言任务提供了有效解决方案。


### [87] [SlotMatch: Distilling Temporally Consistent Object-Centric Representations for Unsupervised Video Segmentation](https://arxiv.org/abs/2508.03411)
*Diana-Nicoleta Grigore,Neelu Madan,Andreas Mogelmose,Thomas B. Moeslund,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 论文提出了一种名为SlotMatch的知识蒸馏框架，用于无监督视频分割任务，通过简单的余弦相似度对齐教师和学生模型，无需额外损失或监督。实验表明，该方法在性能和效率上均优于现有方法。

- Motivation: 无监督视频分割任务缺乏监督信号且场景复杂，现有方法通常依赖计算昂贵的大型神经网络。本文旨在通过知识蒸馏将对象中心表示高效传递给轻量级学生模型。
- Method: 提出SlotMatch框架，通过余弦相似度对齐教师和学生模型的槽位表示，无需额外损失或监督。
- Result: 实验表明，基于SlotMatch的学生模型在性能和效率上优于教师模型（SlotContrast），参数减少3.6倍，速度提升1.9倍，且超越其他无监督视频分割模型。
- Conclusion: SlotMatch通过简单的知识蒸馏框架，实现了高效的无监督视频分割，证明了额外损失是冗余的，为轻量级模型设计提供了新思路。


### [88] [Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN](https://arxiv.org/abs/2508.03415)
*Shivangi Nigam,Adarsh Prasad Behera,Shekhar Verma,P. Nagabhushan*

Main category: cs.CV

TL;DR: Fd-CycleGAN通过结合局部邻域编码和频率感知监督，改进了图像到图像的翻译任务，提升了生成图像的感知质量和多样性。

- Motivation: 改进CycleGAN在图像翻译任务中的表现，尤其是在低数据量情况下，通过更好地捕捉局部和全局分布特征。
- Method: 结合局部邻域编码（LNE）和频率感知监督，使用KL/JS散度和基于对数的相似性度量来量化真实与生成图像的分布对齐。
- Result: 在多个数据集上表现优于基线CycleGAN和其他先进方法，生成图像更连贯、语义一致，且收敛更快。
- Conclusion: 频率引导的潜在学习显著提升了图像翻译任务的泛化能力，适用于文档修复、艺术风格转换和医学图像合成等领域。


### [89] [R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation](https://arxiv.org/abs/2508.03426)
*Futian Wang,Yuhan Qiao,Xiao Wang,Fuling Wang,Yuxiang Zhang,Dengdi Sun*

Main category: cs.CV

TL;DR: 论文提出了一种基于多模态医学知识图谱（M3KG）的X光报告生成框架，通过结合视觉特征和知识图谱，显著提升了报告质量。

- Motivation: 解决现有X光报告生成中的幻觉问题和疾病诊断能力不足的挑战。
- Method: 构建M3KG知识图谱，结合Swin-Transformer提取视觉特征，并通过交叉注意力与知识交互，最终利用大语言模型生成报告。
- Result: 在多个数据集上的实验验证了知识图谱和报告生成框架的有效性。
- Conclusion: 提出的方法显著提升了X光报告生成的准确性和疾病诊断能力。


### [90] [Spatial Imputation Drives Cross-Domain Alignment for EEG Classification](https://arxiv.org/abs/2508.03437)
*Hongjun Liu,Chao Yao,Yalan Zhang,Xiaokun wang,Xiaojuan Ban*

Main category: cs.CV

TL;DR: IMAC是一种自监督框架，通过通道依赖掩码和插值任务解决跨域EEG信号分类中的数据分布偏移问题，显著提升分类准确性和鲁棒性。

- Motivation: EEG信号分类面临因电极配置、采集协议和硬件差异导致的数据分布偏移问题，需要一种能统一处理跨域数据的方法。
- Method: IMAC采用3D到2D位置统一映射标准化电极布局，提出通道依赖掩码和重建任务作为空间时间序列插值问题，并分离时空信息建模。
- Result: 在10个公开EEG数据集上，IMAC在跨主体和跨中心验证中达到最优分类准确率，鲁棒性显著优于基线方法。
- Conclusion: IMAC通过时空信号对齐和分离建模，有效解决了跨域EEG数据偏移问题，具有高分类准确性和鲁棒性。


### [91] [MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis](https://arxiv.org/abs/2508.03441)
*Ning Zhu,Xiaochuan Ma,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: MedCAL-Bench是首个基于Foundation Models（FMs）的冷启动主动学习（CSAL）基准测试，用于医学图像分析，评估了14种FMs和7种CSAL策略，揭示了FMs在特征提取和样本选择中的表现差异。

- Motivation: 冷启动主动学习（CSAL）在医学图像分析中缺乏先验知识时选择信息样本，现有方法依赖自监督学习（SSL），效率低且特征表示不足。预训练的Foundation Models（FMs）具有强大特征提取能力，但缺乏相关研究和基准测试。
- Method: 提出MedCAL-Bench基准测试，评估14种FMs和7种CSAL策略在7个数据集上的表现，涵盖分类和分割任务，并首次同时评估特征提取和样本选择阶段。
- Result: 1）大多数FMs在CSAL中是有效的特征提取器，DINO家族在分割任务中表现最佳；2）FMs在分割任务中性能差异大，分类任务中差异小；3）不同数据集需采用不同样本选择策略，ALPS在分割中最佳，RepDiv在分类中领先。
- Conclusion: MedCAL-Bench为FMs在CSAL中的应用提供了系统评估，揭示了FMs和策略的适用性，为未来研究提供了基准和指导。


### [92] [RAAG: Ratio Aware Adaptive Guidance](https://arxiv.org/abs/2508.03442)
*Shangwen Zhu,Qianyu Peng,Yuting Hu,Zhantao Yang,Han Zhang,Zhao Pu,Ruili Feng,Fan Cheng*

Main category: cs.CV

TL;DR: 研究发现流式生成模型在早期采样阶段对引导尺度敏感，提出了一种基于RATIO的自适应引导调度方法，显著提升了生成速度和效果。

- Motivation: 探索引导机制在流式生成模型不同采样阶段的作用，尤其是在快速低步长采样中的不稳定性问题。
- Method: 通过理论分析和实证验证，提出了一种基于RATIO的自适应引导调度方法，自动调整早期步骤的引导尺度。
- Result: 实验表明，该方法在图像和视频生成模型中实现了3倍加速，同时保持或提升了生成质量、鲁棒性和语义对齐。
- Conclusion: 自适应引导调度是提升流式生成模型性能的关键，尤其在快速采样场景中。


### [93] [CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection](https://arxiv.org/abs/2508.03447)
*Qiyu Chen,Zhen Qu,Wei Luo,Haiming Yao,Yunkang Cao,Yuxin Jiang,Yinan Duan,Huiyuan Luo,Chengkan Lv,Zhengtao Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoPS的新框架，通过动态生成条件提示来提升零样本异常检测性能，解决了静态提示和稀疏标签的局限性。

- Motivation: 现有的大规模预训练视觉语言模型在零样本异常检测中表现优异，但静态提示和固定标签限制了其泛化能力和性能。
- Method: 提出Conditional Prompt Synthesis (CoPS)，利用视觉特征动态生成提示，并结合变分自编码器建模语义特征。
- Result: 在13个工业和医学数据集上，CoPS在分类和分割任务中AUROC指标超越现有方法2.5%。
- Conclusion: CoPS通过动态提示和语义特征融合，显著提升了零样本异常检测的性能和泛化能力。


### [94] [Video Demoireing using Focused-Defocused Dual-Camera System](https://arxiv.org/abs/2508.03449)
*Xuan Dong,Xiangyuan Sun,Xia Wang,Jian Song,Ya Li,Weixin Li*

Main category: cs.CV

TL;DR: 提出了一种双摄像头框架，通过同步拍摄聚焦和散焦视频，利用散焦视频指导去摩尔纹处理，显著提升了效果。

- Motivation: 解决现有单摄像头去摩尔纹方法难以区分摩尔纹与真实纹理，以及保持色调和时间一致性的问题。
- Method: 使用双摄像头同步拍摄，通过光流对齐和多尺度CNN结合多维训练损失进行去摩尔纹处理，最后用联合双边滤波保持一致性。
- Result: 实验表明，该方法显著优于现有图像和视频去摩尔纹技术。
- Conclusion: 双摄像头框架有效解决了摩尔纹问题，同时保持了纹理和色调的一致性。


### [95] [AVPDN: Learning Motion-Robust and Scale-Adaptive Representations for Video-Based Polyp Detection](https://arxiv.org/abs/2508.03458)
*Zilin Chen,Shengnan Lu*

Main category: cs.CV

TL;DR: 提出了一种自适应视频息肉检测网络（AVPDN），用于结肠镜视频中的多尺度息肉检测，通过AFIA和SACI模块提升特征表示和上下文整合能力。

- Motivation: 结肠镜视频的动态性和快速相机移动导致背景噪声增加，影响息肉检测的准确性，需要一种鲁棒的方法来解决这些问题。
- Method: AVPDN包含AFIA模块（三支架构增强特征表示）和SACI模块（多尺度上下文整合），分别通过自注意力和膨胀卷积提升性能。
- Result: 在多个公开基准测试中表现优异，展示了方法的有效性和泛化能力。
- Conclusion: AVPDN在视频息肉检测任务中具有竞争力，解决了动态视频中的噪声和多尺度问题。


### [96] [IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models](https://arxiv.org/abs/2508.03469)
*Jiabing Yang,Chenhang Cui,Yiyang Zhou,Yixiang Chen,Peng Xia,Ying Wei,Tao Yu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为IKOD的解码策略，通过合并短序列的高图像注意力logits来缓解LVLMs中的幻觉问题，无需额外训练或工具。

- Motivation: 大型视觉语言模型（LVLMs）在长序列生成中表现出注意力下降和幻觉增加的问题，但其原因尚不明确。
- Method: 分析了视觉注意力与长序列偏差的关系，提出IKOD策略，通过合并短序列的高图像注意力logits来抑制幻觉。
- Result: 实验证明IKOD能有效减少幻觉并提升模型综合能力，且无需额外成本。
- Conclusion: IKOD是一种轻量高效的解码策略，适用于多种LVLMs，显著改善了幻觉问题。


### [97] [VideoGuard: Protecting Video Content from Unauthorized Editing](https://arxiv.org/abs/2508.03480)
*Junjie Cao,Kaizhou Li,Xinchun Yu,Hongxiang Li,Xiaoping Zhang*

Main category: cs.CV

TL;DR: VideoGuard是一种保护视频免受未经授权恶意编辑的方法，通过引入微小扰动干扰生成扩散模型，优于现有基线方法。

- Motivation: 生成技术的快速发展可能导致恶意滥用，现有方法对视频内容的保护不足，因此需要一种有效的视频保护方法。
- Method: 采用联合帧优化，将视频帧作为整体优化实体，融合视频运动信息到优化目标中，干扰生成模型输出。
- Result: 实验表明，VideoGuard的保护性能优于所有基线方法。
- Conclusion: VideoGuard能有效保护视频免受未经授权的编辑，填补了视频保护领域的空白。


### [98] [Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.03481)
*Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.CV

TL;DR: DrUM是一种新方法，通过条件级建模在潜在空间中集成用户画像，解决了T2I扩散模型中个性化生成的局限性。

- Motivation: 现有研究主要依赖提示级建模和大规模模型，导致个性化不准确，因为T2I扩散模型的输入标记容量有限。
- Method: 提出DrUM，结合用户画像和基于变压器的适配器，在潜在空间中进行条件级建模。
- Result: DrUM在大规模数据集上表现优异，并能无缝集成开源文本编码器。
- Conclusion: DrUM兼容广泛使用的基础T2I模型，无需额外微调。


### [99] [When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models](https://arxiv.org/abs/2508.03483)
*Dasol Choi Jihwan Lee,Minjae Lee,Minsuk Kahng*

Main category: cs.CV

TL;DR: 论文研究了文本到图像生成中对象（如汽车）的隐性人口统计偏见，提出了SODA框架来测量这种偏见，并揭示了模型中的刻板印象和视觉差异。

- Motivation: 探索文本到图像生成模型中对象层面的人口统计偏见，填补了此前研究主要集中在人类描绘偏见的空白。
- Method: 引入SODA框架，通过比较中性提示和人口统计提示生成的2700张图像，分析视觉属性差异。
- Result: 发现特定人口统计群体与视觉属性（如颜色模式）的强关联，某些模型输出多样性较低，加剧了视觉差异。
- Conclusion: SODA框架为揭示生成模型中的刻板印象提供了实用方法，是迈向更系统、负责任AI开发的重要一步。


### [100] [LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation](https://arxiv.org/abs/2508.03485)
*Lianwei Yang,Haokun Lin,Tianchen Zhao,Yichen Wu,Hongyu Zhu,Ruiqi Xie,Zhenan Sun,Yu Wang,Qingyi Gu*

Main category: cs.CV

TL;DR: LRQ-DiT提出了一种高效的低比特后训练量化框架，通过Twin-Log量化和自适应旋转方案解决了DiT模型量化中的权重分布和激活异常问题。

- Motivation: Diffusion Transformers（DiTs）在文本到图像生成中表现出色，但高计算成本和参数量限制了其在资源受限场景的应用。现有后训练量化方法在极低比特设置下性能下降严重。
- Method: 提出LRQ-DiT框架，包括Twin-Log量化（TLQ）和自适应旋转方案（ARS），分别优化权重分布和激活异常问题。
- Result: 在PixArt、FLUX等数据集上验证，LRQ-DiT在低比特量化下保持图像质量，优于现有PTQ基线。
- Conclusion: LRQ-DiT有效解决了DiT模型量化中的关键问题，为资源受限场景提供了可行的解决方案。


### [101] [ParticleSAM: Small Particle Segmentation for Material Quality Monitoring in Recycling Processes](https://arxiv.org/abs/2508.03490)
*Yu Zhou,Pelle Thielmann,Ayush Chamoli,Bruno Mirbach,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 提出ParticleSAM方法，改进分割基础模型以处理建筑颗粒图像，并创建新数据集验证其效果。

- Motivation: 建筑行业资源消耗大，回收材料质量监测仍依赖人工，需高效自动化解决方案。
- Method: 改进分割基础模型（ParticleSAM），并构建多颗粒数据集用于验证。
- Result: 实验证明ParticleSAM优于原始SAM方法。
- Conclusion: ParticleSAM在建筑颗粒分割中表现优异，可扩展至其他小颗粒分割领域。


### [102] [Quality Versus Sparsity in Image Recovery by Dictionary Learning Using Iterative Shrinkage](https://arxiv.org/abs/2508.03492)
*Mohammadsadegh Khoshghiaferezaee,Moritz Krauth,Shima Shabani,Michael Breuß*

Main category: cs.CV

TL;DR: 论文研究了稀疏字典学习（SDL）中稀疏性对图像恢复质量的影响，发现不同优化方法会导致不同的稀疏性模式，且高稀疏性通常不会损害恢复质量。

- Motivation: 稀疏性是SDL中解决方案的重要属性，但需要明确稀疏性对恢复质量的影响程度。
- Method: 通过多种优化方法分析SDL中解决方案的稀疏性。
- Result: 不同优化方法导致不同的稀疏性模式，高稀疏性通常不会损害恢复质量。
- Conclusion: 高稀疏性在SDL中是可取的，即使恢复图像与学习数据库差异较大。


### [103] [Prototype-Enhanced Confidence Modeling for Cross-Modal Medical Image-Report Retrieval](https://arxiv.org/abs/2508.03494)
*Shreyank N Gowda,Xiaobo Jin,Christian Wagner*

Main category: cs.CV

TL;DR: 提出了一种名为PECM的框架，通过多级原型和双流置信度估计，提升了医学图像与文本报告的跨模态检索性能。

- Motivation: 医学数据固有的模糊性和变异性使得图像与文本的准确对齐具有挑战性，现有模型难以捕捉多层次的语义关系。
- Method: PECM框架引入多级原型和双流置信度估计，利用原型相似性分布和自适应权重机制减少高不确定性数据的影响。
- Result: 在多个数据集和任务中，PECM显著提升了检索精度和一致性，性能提升最高达10.17%。
- Conclusion: PECM有效解决了医学数据模糊性问题，提升了复杂临床场景下的检索可靠性。


### [104] [EditGarment: An Instruction-Based Garment Editing Dataset Constructed with Automated MLLM Synthesis and Semantic-Aware Evaluation](https://arxiv.org/abs/2508.03497)
*Deqiang Yin,Junyi Guo,Huanda Lu,Fangyu Wu,Dongming Lu*

Main category: cs.CV

TL;DR: 论文提出了一种自动化构建服装编辑数据集的方法，以解决高质量指令-图像对稀缺的问题，并引入了语义感知的评估指标Fashion Edit Score。

- Motivation: 服装编辑需要理解特定语义和属性依赖，但高质量数据稀缺且人工标注成本高，限制了进展。
- Method: 定义六种编辑指令类别，生成平衡多样的指令-图像三元组，并引入Fashion Edit Score作为评估指标。
- Result: 构建了52,257个候选三元组，最终保留20,596个高质量数据，形成EditGarment数据集。
- Conclusion: 提出的自动化管道和评估指标为服装编辑任务提供了可靠的数据支持。


### [105] [MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation](https://arxiv.org/abs/2508.03511)
*Yazhou Zhu,Haofeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的跨域少样本医学图像分割方法MAUP，通过自适应提示策略优化预训练模型SAM，在三个医学数据集上表现优异。

- Motivation: 现有跨域少样本医学图像分割模型依赖大量训练，限制了通用性和部署便捷性。利用自然图像预训练的大模型（如SAM）可以解决这一问题。
- Method: 采用多中心自适应不确定性感知提示策略（MAUP），包括多中心提示生成、不确定性感知提示选择和自适应提示优化。
- Result: 在三个医学数据集上实现了精确分割，无需额外训练，优于传统方法和无训练模型。
- Conclusion: MAUP通过自适应提示策略有效利用预训练模型，为跨域少样本医学图像分割提供了高效解决方案。


### [106] [Distribution-aware Knowledge Unification and Association for Non-exemplar Lifelong Person Re-identification](https://arxiv.org/abs/2508.03516)
*Shiben Liu,Mingyue Xu,Huijie Fan,Qiang Wang,Yandong Tang,Zhi Han*

Main category: cs.CV

TL;DR: 论文提出了一种名为DKUA的新框架，通过分布感知知识统一与关联，解决了终身行人重识别中旧知识保留与新信息适应的平衡问题。

- Motivation: 终身行人重识别（LReID）面临的主要挑战是如何在保留旧知识的同时适应新信息。现有方法通常使用知识蒸馏，但忽略了特定分布感知和跨域统一知识学习。
- Method: 提出DKUA框架，包括分布感知模型、自适应知识巩固（AKC）、统一知识关联（UKA）和基于分布的知识转移（DKT）。
- Result: 实验结果显示，DKUA在抗遗忘和泛化能力上分别比现有方法平均提升了7.6%和5.3%。
- Conclusion: DKUA通过分布感知和跨域知识统一，显著提升了终身行人重识别的性能。


### [107] [Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models](https://arxiv.org/abs/2508.03524)
*Stefan Brandstätter,Maximilian Köller,Philipp Seeböck,Alissa Blessing,Felicitas Oberndorfer,Svitlana Pochepnia,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 提出了一种基于视觉病理学基础模型的自动拼接方法SemanticStitcher，用于解决组织样本拼接中的挑战，并在实验中表现优于现有技术。

- Motivation: 组织样本常因尺寸过大需拼接，但现有方法受限于组织损失、形态变形等问题，难以实现高效拼接。
- Method: 利用视觉病理学基础模型的潜在特征表示识别相邻区域，并通过大量语义匹配候选进行鲁棒姿态估计，形成全切片图像。
- Result: 在三个病理学数据集上的实验表明，SemanticStitcher在边界匹配上优于现有技术。
- Conclusion: SemanticStitcher为组织样本拼接提供了一种高效且鲁棒的解决方案。


### [108] [CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image Content Generation](https://arxiv.org/abs/2508.03535)
*Kaishen Yuan,Yuting Zhang,Shang Gao,Yijie Zhu,Wenshuo Chen,Yutao Yue*

Main category: cs.CV

TL;DR: CoEmoGen提出了一种新的情感图像生成方法，通过多模态大语言模型和心理学启发的模块设计，解决了现有方法在语义一致性和可扩展性上的不足。

- Motivation: 现有文本到图像扩散模型在生成抽象情感内容时表现不佳，且现有EICG方法过度依赖词级属性标签，导致语义不一致和可扩展性受限。
- Method: 利用多模态大语言模型构建高质量情感触发内容描述，并设计分层低秩适应模块（HiLoRA）建模情感特征。
- Result: 实验表明CoEmoGen在情感忠实度和语义一致性上表现优越，并通过大规模数据集EmoArt展示了其可扩展性。
- Conclusion: CoEmoGen为情感图像生成提供了语义一致且可扩展的解决方案，具有广泛应用前景。


### [109] [Retinal Lipidomics Associations as Candidate Biomarkers for Cardiovascular Health](https://arxiv.org/abs/2508.03538)
*Inamullah,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TL;DR: 研究探讨了血清脂质亚类与视网膜微血管特征的关系，发现不同脂质亚类与特定血管特征相关，支持视网膜血管作为非侵入性代谢健康标志物的潜力。

- Motivation: 视网膜微血管成像被广泛用于评估全身血管和代谢健康，但脂质组学与视网膜血管的关联研究不足。
- Method: 通过Spearman相关分析和Benjamini-Hochberg校正，研究了脂质亚类与视网膜微血管特征的关系。
- Result: FA与血管扭曲度相关，CE与动静脉平均宽度相关，DAG和TAG与血管宽度和复杂性呈负相关。
- Conclusion: 视网膜血管结构反映了循环脂质特征，支持其作为非侵入性代谢健康标志物的作用。


### [110] [Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection](https://arxiv.org/abs/2508.03539)
*Long Qian,Bingke Zhu,Yingying Chen,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: ARAS是一种语言条件自回归异常合成方法，通过令牌锚定的潜在编辑精确注入局部文本指定缺陷，结合QARAD框架，显著提升异常检测性能。

- Motivation: 现有异常合成方法存在结构缺陷，如微观结构不连续、语义控制有限和生成效率低。
- Method: 采用语言条件自回归方法（ARAS），结合硬门控自回归操作符和无训练上下文保留掩码采样核，通过QARAD框架动态加权高质量合成样本。
- Result: 在MVTec AD、VisA和BTAD数据集上，QARAD在图像和像素级异常检测任务中优于现有方法，准确性和鲁棒性提升，合成速度提高5倍。
- Conclusion: ARAS和QARAD框架显著提升了异常合成的质量和效率，为异常检测提供了更优解决方案。


### [111] [Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences](https://arxiv.org/abs/2508.03542)
*Dmitrii Korzh,Dmitrii Tarasov,Artyom Iudin,Elvir Karimov,Matvey Skripkin,Nikita Kuzmin,Andrey Kuznetsov,Oleg Y. Rogov,Ivan Oseledets*

Main category: cs.CV

TL;DR: 论文提出了一种开源数据集和方法，用于将口语数学表达式转换为LaTeX，解决了现有方法的局限性，并在多个基准测试中取得了显著改进。

- Motivation: 解决口语数学表达式转换为LaTeX的挑战，填补现有方法在数据集规模、多语言支持和应用场景上的不足。
- Method: 基于ASR后校正模型和音频语言模型，结合少样本提示技术，构建了一个包含66,000个标注样本的多语言数据集。
- Result: 在MathSpeech和S2L-equations基准测试中分别取得28%和27%的CER，显著优于现有方法。
- Conclusion: 该工作为多模态AI在数学内容识别领域的未来发展奠定了基础。


### [112] [Advancing Wildlife Monitoring: Drone-Based Sampling for Roe Deer Density Estimation](https://arxiv.org/abs/2508.03545)
*Stephanie Wohlfahrt,Christoph Praschl,Horst Leitner,Wolfram Jantsch,Julia Konic,Silvio Schueler,Andreas Stöckl,David C. Schedl*

Main category: cs.CV

TL;DR: 使用无人机和红外/RGB影像估算奥地利东南部野生动物密度，并与相机陷阱数据对比，结果显示无人机方法高效且可扩展。

- Motivation: 传统野生动物密度估算方法（如捕获-再捕获、距离采样或相机陷阱）劳动密集或空间受限，无人机提供了一种非侵入性、高效的替代方案。
- Method: 在无叶期使用多架无人机沿系统随机化航线飞行，手动标注影像中的动物，并采用三种外推方法估算密度，同时用相机陷阱数据对比。
- Result: 无人机方法估算的密度与相机陷阱结果相似，但通常更高，反映了白天开放和森林区域的活动。
- Conclusion: 无人机为野生动物密度估算提供了一种有前景的可扩展方法，与传统方法互补。


### [113] [Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching](https://arxiv.org/abs/2508.03562)
*Muzhaffar Hazman,Susan McKeever,Josephine Griffith*

Main category: cs.CV

TL;DR: 论文提出了一种更广泛的模因匹配方法，超越了传统的基于模板的匹配，并探讨了不同相似度测量方法在模因匹配中的表现。

- Motivation: 现有模因匹配方法主要基于模板匹配，忽略了非模板模因，限制了自动模因分析的效率和与网络模因词典的关联。
- Method: 引入了一种更广泛的模因匹配方法，测试了传统相似度测量（包括分段计算）和基于预训练多模态大语言模型的提示方法。
- Result: 传统方法在模板模因上表现良好，但在非模板模因上效果有限；分段计算优于全图测量；多模态大语言模型提示方法展示了潜力。
- Conclusion: 准确匹配模因的共享视觉元素（不仅是背景模板）仍是一个开放挑战，需要更复杂的匹配技术。


### [114] [A Scalable Machine Learning Pipeline for Building Footprint Detection in Historical Maps](https://arxiv.org/abs/2508.03564)
*Annemarie McCarthy*

Main category: cs.CV

TL;DR: 提出了一种针对农村历史地图的可扩展高效建筑提取方法，通过分层机器学习减少计算量。

- Motivation: 解决现有方法在分析广泛农村地区时的计算密集问题，支持历史人口普查验证和废弃聚落定位。
- Method: 采用分层机器学习方法：先用CNN分类器过滤无建筑区域，再用CNN分割算法提取建筑特征。
- Result: 在爱尔兰历史地图上验证，性能高效，发现1839年地图中一个可能因大饥荒废弃的聚落。
- Conclusion: 该方法为历史和考古研究提供了高效工具，尤其适用于稀疏建筑分布的地图。


### [115] [SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks](https://arxiv.org/abs/2508.03566)
*Xinyu Xiong,Zihuang Wu,Lei Zhang,Lei Lu,Ming Li,Guanbin Li*

Main category: cs.CV

TL;DR: SAM2-UNeXT是一个基于SAM2-UNet的改进框架，通过集成DINOv2编码器和双分辨率策略，提升了分割性能，简化了架构。

- Motivation: 现有方法在增强SAM模型的通用性和性能方面存在挑战，需要更强大的编码器。
- Method: 提出SAM2-UNeXT框架，结合DINOv2编码器和双分辨率策略，使用密集粘合层简化架构。
- Result: 在四个基准测试中表现优异，包括二值图像分割、伪装目标检测等。
- Conclusion: SAM2-UNeXT通过简单架构实现了更准确的分割，性能优于现有方法。


### [116] [RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data](https://arxiv.org/abs/2508.03578)
*Jonas Leo Mueller,Lukas Engel,Eva Dorschky,Daniel Krauss,Ingrid Ullmann,Martin Vossiek,Bjoern M. Eskofier*

Main category: cs.CV

TL;DR: RadProPoser是一种基于雷达的隐私保护人体姿态估计系统，通过概率编码器-解码器架构处理复杂雷达数据，预测三维关节位置及不确定性，并在新数据集上表现优异。

- Motivation: 雷达姿态估计具有隐私保护和光照不变性优势，但受噪声和多路径效应影响，需解决这些问题以提高可靠性。
- Method: 采用概率编码器-解码器架构，结合变分推断和关键点回归，预测关节位置及不确定性，支持高斯和拉普拉斯分布建模。
- Result: 在光学动捕数据集上，平均关节位置误差为6.425 cm，不确定性校准误差低至0.021，数据增强后F1分数达0.870。
- Conclusion: RadProPoser是首个端到端雷达姿态估计系统，为可靠和可解释的雷达运动分析奠定了基础。


### [117] [MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy](https://arxiv.org/abs/2508.03596)
*Wuyang Li,Wentao Pan,Xiaoyuan Liu,Zhendong Luo,Chenxin Li,Hengyu Liu,Din Ping Tsai,Mu Ku Chen,Yixuan Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种基于金属透镜内窥镜的新型光学驱动神经网络MetaScope，解决了金属透镜数据与算法研究的空白，并在实验中表现出色。

- Motivation: 传统内窥镜的物理限制和金属透镜的数据与算法研究空白，促使研究者探索金属透镜内窥镜的解决方案。
- Method: 建立金属透镜内窥镜数据集，提出MetaScope网络，包含光学驱动的强度调整（OIA）和色差校正（OCC），并采用梯度引导蒸馏增强学习。
- Result: MetaScope在金属透镜分割和恢复任务中优于现有方法，并在真实生物医学场景中表现出强大的泛化能力。
- Conclusion: MetaScope为金属透镜内窥镜提供了有效的解决方案，填补了研究空白，具有实际应用潜力。


### [118] [DyCAF-Net: Dynamic Class-Aware Fusion Network](https://arxiv.org/abs/2508.03598)
*Md Abrar Jahin,Shahriar Soudeep,M. F. Mridha,Nafiz Fahad,Md. Jakir Hossen*

Main category: cs.CV

TL;DR: DyCAF-Net通过动态类别感知融合网络解决了静态融合和类别无关注意力的局限性，显著提升了目标检测性能。

- Motivation: 静态融合启发式方法和类别无关注意力限制了动态场景中的性能表现。
- Method: 提出了动态类别感知融合网络（DyCAF-Net），包括输入条件平衡颈部、双动态注意力机制和类别感知特征适应。
- Result: 在13个多样化基准测试中，DyCAF-Net在精度、mAP@50和mAP@50-95上显著提升，同时保持计算效率和推理速度。
- Conclusion: DyCAF-Net因其适应性强，成为医学影像、监控和自主系统等实际检测任务的稳健解决方案。


### [119] [CloudBreaker: Breaking the Cloud Covers of Sentinel-2 Images using Multi-Stage Trained Conditional Flow Matching on Sentinel-1](https://arxiv.org/abs/2508.03608)
*Saleh Sakib Ahmed,Sara Nowreen,M. Sohel Rahman*

Main category: cs.CV

TL;DR: CloudBreaker是一个新框架，通过Sentinel-1雷达数据生成高质量的多光谱Sentinel-2信号，解决了云层和夜间条件对卫星遥感数据的限制。

- Motivation: 云层和夜间条件限制了多光谱卫星图像的可用性和可靠性，而Sentinel-1雷达数据不受这些限制。
- Method: 采用基于条件潜在流匹配的多阶段训练方法，并首次将余弦调度与流匹配结合。
- Result: CloudBreaker表现出色，FID得分为0.7432，NDWI和NDVI的SSIM分别为0.6156和0.6874。
- Conclusion: CloudBreaker为多光谱数据不可靠或不可用的遥感应用提供了有前景的解决方案。


### [120] [evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition](https://arxiv.org/abs/2508.03609)
*Rodrigo Verschae,Ignacio Bugueno-Cordova*

Main category: cs.CV

TL;DR: 提出了一种基于迁移学习的框架evTransFER，用于事件相机的人脸表情识别，通过预训练的编码器和LSTM结构显著提升了识别准确率。

- Motivation: 事件相机具有高动态范围和微秒级延迟的特性，但现有方法在表情识别任务上表现不佳，需要更高效的框架。
- Method: 使用对抗生成方法预训练编码器，迁移至表情识别系统，并结合LSTM和新的TIE事件表示方法。
- Result: 在e-CK+数据库上达到93.6%的识别率，比现有方法提升25.9%以上。
- Conclusion: evTransFER框架通过迁移学习和时空动态捕捉显著提升了事件相机的人脸表情识别性能。


### [121] [FPG-NAS: FLOPs-Aware Gated Differentiable Neural Architecture Search for Efficient 6DoF Pose Estimation](https://arxiv.org/abs/2508.03618)
*Nassim Ali Ousalah,Peyman Rostami,Anis Kacem,Enjie Ghorbel,Emmanuel Koumandakis,Djamila Aouada*

Main category: cs.CV

TL;DR: FPG-NAS是一种基于FLOPs感知的门控可微分神经架构搜索框架，专为高效的6DoF物体姿态估计设计。

- Motivation: 现有的6DoF姿态估计方法计算量大，限制了其在资源受限场景中的应用。FPG-NAS旨在解决这一问题。
- Method: 提出了一种任务特定的搜索空间和可微分门控机制，支持离散多候选算子选择，并通过FLOPs正则化平衡精度与效率。
- Result: 在LINEMOD和SPEED+数据集上，FPG-NAS生成的模型在严格FLOPs限制下优于现有方法。
- Conclusion: FPG-NAS是首个专为6DoF姿态估计设计的可微分NAS框架，显著提升了效率和多样性。


### [122] [AttZoom: Attention Zoom for Better Visual Features](https://arxiv.org/abs/2508.03625)
*Daniel DeAlcala,Aythami Morales,Julian Fierrez,Ruben Tolosana*

Main category: cs.CV

TL;DR: Attention Zoom是一种模块化且模型无关的空间注意力机制，旨在提升CNN的特征提取能力，无需特定架构集成。

- Motivation: 传统注意力方法需要特定架构集成，限制了其通用性。Attention Zoom旨在提供一种独立的空间注意力层，强调输入中的高重要性区域。
- Method: 提出了一种独立的空间注意力层，通过空间强调高重要性区域来改进特征提取。在CIFAR-100和TinyImageNet上评估了该方法。
- Result: 在多个CNN骨干网络上，Top-1和Top-5分类准确率均有提升。可视化分析显示，该方法能促进细粒度和多样化的注意力模式。
- Conclusion: Attention Zoom是一种有效且通用的方法，能以最小的架构开销提升CNN性能。


### [123] [Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images](https://arxiv.org/abs/2508.03643)
*Xiangyu Sun,Haoyi jiang,Liu Liu,Seungtae Nam,Gyeongjin Kang,Xinjie wang,Wei Sui,Zhizhong Su,Wenyu Liu,Xinggang Wang,Eunbyung Park*

Main category: cs.CV

TL;DR: Uni3R是一个新颖的前馈框架，直接从无位姿的多视角图像中联合重建带有开放词汇语义的统一3D场景表示。

- Motivation: 传统方法通常将语义理解与重建解耦或需要昂贵的每场景优化，限制了其扩展性和泛化性。
- Method: 利用跨视角Transformer整合多视角信息，回归一组带有语义特征场的3D高斯基元。
- Result: 在多个基准测试中达到新SOTA，如RE10K上的25.07 PSNR和ScanNet上的55.84 mIoU。
- Conclusion: Uni3R为可泛化的、统一的3D场景重建和理解提供了新范式。


### [124] [OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World](https://arxiv.org/abs/2508.03669)
*Katherine Liu,Sergey Zakharov,Dian Chen,Takuya Ikeda,Greg Shakhnarovich,Adrien Gaidon,Rares Ambrus*

Main category: cs.CV

TL;DR: OmniShape是一种新方法，通过解耦形状补全为多模态分布，实现从单次观测中估计物体的姿态和完整形状。

- Motivation: 解决从单次观测中估计物体姿态和完整形状的问题，无需已知3D模型或类别。
- Method: 将形状补全解耦为两个多模态分布，分别训练条件扩散模型，实现姿态和形状的联合分布采样。
- Result: 在真实世界数据集上表现优异。
- Conclusion: OmniShape是首个实现概率姿态和形状估计的方法，具有潜力。


### [125] [Veila: Panoramic LiDAR Generation from a Monocular RGB Image](https://arxiv.org/abs/2508.03690)
*Youquan Liu,Lingdong Kong,Weidong Yang,Ao Liang,Jianxiong Gao,Yang Wu,Xiang Xu,Xin Li,Linfeng Li,Runnan Chen,Ben Fei*

Main category: cs.CV

TL;DR: Veila是一个新颖的条件扩散框架，用于生成可控的全景LiDAR数据，通过自适应平衡语义和深度线索、跨模态对齐和全景特征一致性，解决了RGB图像与LiDAR数据之间的模态差异和结构一致性问题。

- Motivation: 现有方法在生成全景LiDAR数据时缺乏可控性或精细空间控制，而基于RGB图像的生成方法面临语义和深度线索的空间变化、模态差异以及结构一致性等挑战。
- Method: Veila框架包括置信感知条件机制（CACM）、几何跨模态对齐（GCMA）和全景特征一致性（PFC），并引入了跨模态语义一致性和深度一致性评估指标。
- Result: 在nuScenes、SemanticKITTI和KITTI-Weather基准测试中，Veila实现了最先进的生成保真度和跨模态一致性，并提升了LiDAR语义分割的性能。
- Conclusion: Veila通过创新的条件扩散框架，显著提升了全景LiDAR数据的生成质量和可控性，为自动驾驶和机器人领域的3D感知提供了可扩展的解决方案。


### [126] [La La LiDAR: Large-Scale Layout Generation from LiDAR Data](https://arxiv.org/abs/2508.03691)
*Youquan Liu,Lingdong Kong,Weidong Yang,Xin Li,Ao Liang,Runnan Chen,Ben Fei,Tongliang Liu*

Main category: cs.CV

TL;DR: La La LiDAR 是一种布局引导的生成框架，通过语义增强的场景图扩散和关系感知的上下文条件，实现可控的 LiDAR 场景生成，解决了现有模型缺乏对前景对象和空间关系显式控制的问题。

- Motivation: 现有基于扩散的 LiDAR 生成模型缺乏对前景对象和空间关系的显式控制，限制了其在场景模拟和安全验证中的应用。
- Method: 提出 La La LiDAR 框架，结合语义增强的场景图扩散和关系感知的上下文条件，实现结构化 LiDAR 布局生成，并通过前景感知控制注入完成场景生成。
- Result: 实验表明，La La LiDAR 在 LiDAR 生成和下游感知任务中均达到最先进性能，并引入了新的评估指标。
- Conclusion: La La LiDAR 为可控 3D 场景生成设立了新基准，解决了现有模型的局限性。


### [127] [LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences](https://arxiv.org/abs/2508.03692)
*Ao Liang,Youquan Liu,Yu Yang,Dongyue Lu,Linfeng Li,Lingdong Kong,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: LiDARCrafter是一个统一的4D LiDAR生成和编辑框架，通过自然语言输入生成动态场景，并在nuScenes数据集上实现了最先进的性能。

- Motivation: 现有生成模型多关注视频或占用网格，忽略了LiDAR的特性，动态4D LiDAR生成在可控性、时间一致性和评估标准化方面存在挑战。
- Method: LiDARCrafter将自然语言指令解析为场景图，通过三分支扩散网络生成对象结构、运动轨迹和几何形状，并结合自回归模块生成时间一致的4D LiDAR序列。
- Result: 在nuScenes数据集上，LiDARCrafter在保真度、可控性和时间一致性方面表现最佳。
- Conclusion: LiDARCrafter为数据增强和仿真提供了新途径，代码和基准已开源。


### [128] [LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation](https://arxiv.org/abs/2508.03694)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Jianfeng Feng,Chenyang Si,Yanwei Fu,Yu Qiao,Ziwei Liu*

Main category: cs.CV

TL;DR: LongVie提出了一种端到端的自回归框架，用于可控超长视频生成，解决了现有方法在时间一致性和视觉退化方面的挑战。

- Motivation: 现有方法在短片段生成中有效，但在扩展到长视频时面临时间不一致和视觉退化的问题。
- Method: LongVie采用统一的噪声初始化策略、全局控制信号归一化、多模态控制框架和退化感知训练策略。
- Result: 实验表明，LongVie在长范围可控性、一致性和质量上达到最先进水平。
- Conclusion: LongVie通过创新设计解决了长视频生成的关键问题，并展示了卓越性能。


### [129] [Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition](https://arxiv.org/abs/2508.03695)
*Pulkit Kumar,Shuaiyi Huang,Matthew Walmer,Sai Saketh Rambhatla,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: Trokens是一种将轨迹点转化为语义感知关系标记的新方法，用于改进少样本动作识别，通过语义感知采样和运动建模框架，结合轨迹标记与语义特征，在多个基准测试中达到最先进性能。

- Motivation: 视频理解需要有效建模运动和外观信息，特别是在少样本动作识别中。现有方法在点跟踪方面有进展，但仍面临选择信息点和建模运动模式的挑战。
- Method: 提出语义感知采样策略和运动建模框架（HoD），结合轨迹标记与语义特征，增强外观特征的运动信息。
- Result: 在六个少样本动作识别基准测试中（如Something-Something-V2、Kinetics等）达到最先进性能。
- Conclusion: Trokens通过语义感知和运动建模，显著提升了少样本动作识别的性能。
## cs.GR

### [130] [READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation](https://arxiv.org/abs/2508.03457)
*Haotian Wang,Yuzhe Weng,Jun Du,Haoran Xu,Xiaoyan Wu,Shan He,Bing Yin,Cong Liu,Jianqing Gao,Qingfeng Liu*

Main category: cs.GR

TL;DR: READ是一个基于扩散-Transformer的实时说话头生成框架，通过压缩视频和语音潜在空间以及异步噪声调度器，显著提高了生成速度和质量。

- Motivation: 扩散模型在说话头生成中取得了显著进展，但推理速度过慢限制了其实际应用。
- Method: READ使用时间VAE压缩视频潜在空间，结合预训练的SpeechAE生成语音潜在代码，并通过A2V-DiT建模，同时采用异步噪声调度器（ANS）加速推理。
- Result: 实验表明，READ在生成质量和速度上优于现有方法，并在长时间生成中保持稳定性。
- Conclusion: READ在说话头生成中实现了速度与质量的平衡，为实时应用提供了可行方案。
## cs.AI

### [131] [T2UE: Generating Unlearnable Examples from Text Descriptions](https://arxiv.org/abs/2508.03091)
*Xingjun Ma,Hanxun Huang,Tianwei Song,Ye Sun,Yifeng Gao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: T2UE框架通过文本描述生成不可学习样本，避免直接暴露原始图像数据，解决了隐私保护中的矛盾问题。

- Motivation: 当前不可学习样本生成方法需要依赖第三方服务，导致隐私泄露，亟需一种无需原始数据的解决方案。
- Method: 利用文本到图像模型将文本描述映射到噪声空间，结合误差最小化框架生成有效的不可学习噪声。
- Result: T2UE显著降低了下游任务的性能，且保护效果适用于多种架构和监督学习场景。
- Conclusion: T2UE实现了零接触数据保护，仅需文本描述即可保护数据隐私。
## cs.CL

### [132] [Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?](https://arxiv.org/abs/2508.03644)
*Wenxuan Shen,Mingjia Wang,Yaochen Wang,Dongping Chen,Junjie Yang,Yao Wan,Weiwei Lin*

Main category: cs.CL

TL;DR: Double-Bench是一个新的大规模、多语言、多模态评估系统，用于全面评估文档RAG系统，填补了现有基准的不足。

- Motivation: 当前文档RAG系统的评估基准存在局限性，无法反映真实世界的挑战，因此需要更全面的评估工具。
- Method: 开发了Double-Bench，包含3,276份文档和5,168个查询，支持动态更新，并由人类专家验证。
- Result: 实验显示文本和视觉嵌入模型的差距缩小，但现有RAG框架存在过度自信问题。
- Conclusion: Double-Bench为未来文档RAG研究提供了严格的基础，并计划定期更新。


### [133] [Can Large Vision-Language Models Understand Multimodal Sarcasm?](https://arxiv.org/abs/2508.03654)
*Xinyu Wang,Yue Zhang,Liqiang Jing*

Main category: cs.CL

TL;DR: 本文探讨了大型视觉语言模型（LVLMs）在多模态讽刺分析（MSA）中的应用，发现其存在视觉理解不足和概念知识缺乏的问题，并提出了一种无需训练的新框架来解决这些问题。

- Motivation: 讽刺是一种复杂的语言现象，传统方法主要依赖文本，而多模态信息的应用尚未充分探索。本文旨在评估LVLMs在MSA任务中的表现，并解决其局限性。
- Method: 提出了一种无需训练的框架，结合深度对象提取和外部概念知识，以提升模型在多模态讽刺检测和解释中的能力。
- Result: 实验结果表明，该框架在多个模型中有效提升了性能。
- Conclusion: 本文提出的框架为多模态讽刺分析提供了新的解决方案，并展示了LVLMs在此领域的潜力。
## cs.CY

### [134] [The Architecture of Trust: A Framework for AI-Augmented Real Estate Valuation in the Era of Structured Data](https://arxiv.org/abs/2508.02765)
*Petteri Teikari,Mike Jarrell,Maryam Azh,Harri Pesola*

Main category: cs.CY

TL;DR: 论文分析了UAD 3.6强制实施对房产估值的变革，结合AI技术提出三层框架，强调人机协作以解决市场偏见和信息不对称。

- Motivation: 研究UAD 3.6的强制实施和AI技术进步如何共同改变房产估值行业，解决现有估值中的系统性问题。
- Method: 开发了一个三层框架，涵盖数据采集、语义理解和认知推理，结合AI技术并保持专业监督。
- Result: 揭示了标准化与AI结合如何重构市场，提出了满足金融应用信任需求的解决方案。
- Conclusion: 成功的转型需要技术与专业知识的结合，AI应增强而非取代人工，同时解决市场历史问题。
## eess.IV

### [135] [Evaluation of 3D Counterfactual Brain MRI Generation](https://arxiv.org/abs/2508.02880)
*Pengwei Sun,Wei Peng,Lun Yu Li,Yixin Wang,Kilian M. Pohl*

Main category: eess.IV

TL;DR: 该论文提出了一种基于因果图的解剖引导框架，将六种生成模型转化为3D反事实方法，用于生成结构复杂的3D脑部MRI图像，并评估了其效果。

- Motivation: 解决因数据稀缺、结构复杂和缺乏标准化评估协议而导致的生成真实3D脑部MRI图像的挑战。
- Method: 通过解剖引导框架将六种生成模型转化为3D反事实方法，使用区域脑体积作为直接条件输入。
- Result: 解剖引导的条件化成功修改了目标解剖区域，但在保留非目标结构方面存在局限性。
- Conclusion: 研究为更可解释和临床相关的脑部MRI生成建模奠定了基础，同时指出需要更准确捕捉解剖依赖关系的新架构。


### [136] [REFLECT: Rectified Flows for Efficient Brain Anomaly Correction Transport](https://arxiv.org/abs/2508.02889)
*Farzad Beizaee,Sina Hajimiri,Ismail Ben Ayed,Gregory Lodygensky,Christian Desrosiers,Jose Dolz*

Main category: eess.IV

TL;DR: REFLECT是一种基于修正流的无监督异常检测框架，用于脑成像中的异常定位和校正，显著优于现有方法。

- Motivation: 脑成像中无监督异常检测的挑战在于复杂解剖结构和异常样本稀缺，需要更高效的方法。
- Method: 利用修正流建立线性轨迹，一步校正异常MR图像，通过检测输入与校正图像的差异定位异常。
- Result: 在多个基准测试中，REFLECT表现优于现有无监督异常检测方法。
- Conclusion: REFLECT提供了一种高效、直接的异常检测和定位方法，优于基于扩散的模型。


### [137] [AMD-Mamba: A Phenotype-Aware Multi-Modal Framework for Robust AMD Prognosis](https://arxiv.org/abs/2508.02957)
*Puzhen Wu,Mingquan Lin,Qingyu Chen,Emily Y. Chew,Zhiyong Lu,Yifan Peng,Hexin Dong*

Main category: eess.IV

TL;DR: AMD-Mamba是一种新型多模态框架，用于AMD预后，并提出新的生物标志物，整合眼底图像、遗传变异和社会人口变量，通过度量学习和多尺度融合提升预测性能。

- Motivation: AMD是导致不可逆视力丧失的主要原因，需要有效预后方法以进行及时干预。
- Method: 提出AMD-Mamba框架，结合眼底图像、遗传变异和社会人口变量，采用度量学习和Vision Mamba融合局部与全局信息，并通过多尺度融合提升性能。
- Result: 在AREDS数据集上验证，新生物标志物对AMD进展具有显著意义，结合现有变量可早期识别高风险患者。
- Conclusion: AMD-Mamba框架为AMD的精准和主动管理提供了潜力。


### [138] [ClinicalFMamba: Advancing Clinical Assessment using Mamba-based Multimodal Neuroimaging Fusion](https://arxiv.org/abs/2508.03008)
*Meng Zhou,Farzad Khalvati*

Main category: eess.IV

TL;DR: 论文提出了一种名为ClinicalFMamba的新型混合架构，结合CNN和Mamba模型，用于高效的多模态医学图像融合，并在2D/3D图像上验证了其优越性能。

- Motivation: 现有方法在全局上下文建模和计算效率上存在不足，限制了临床部署。
- Method: 采用CNN-Mamba混合架构，结合局部和全局特征建模，并设计了针对3D图像的三平面扫描策略。
- Result: 在多个数据集上表现出优越的融合性能，并在实时融合和下游分类任务中优于基线方法。
- Conclusion: ClinicalFMamba为高效的多模态医学图像融合提供了新范式，适合临床实时部署。


### [139] [A Survey of Medical Point Cloud Shape Learning: Registration, Reconstruction and Variation](https://arxiv.org/abs/2508.03057)
*Tongxu Zhang,Zhiming Liang,Bei Wang*

Main category: eess.IV

TL;DR: 本文综述了基于学习的医学点云形状分析方法，涵盖配准、重建和变异建模三大任务，总结了近年文献、方法、数据集及临床挑战。

- Motivation: 点云在3D医学成像中日益重要，深度学习为其形状分析提供了新方法。本文旨在系统梳理相关进展。
- Method: 回顾2021-2025年文献，总结代表性方法、数据集和评估指标，分析临床应用与挑战。
- Result: 关键趋势包括混合表示、大规模自监督模型和生成技术，但存在数据稀缺、患者间变异性等限制。
- Conclusion: 未来需推进点云形状学习在医学成像中的应用，解决可解释性和鲁棒性等临床需求。


### [140] [Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical Image Super-Resolution](https://arxiv.org/abs/2508.03073)
*Bo Zhang,JianFei Huo,Zheng Zhang,Wufan Wang,Hui Gao,Xiangyang Gong,Wendong Wang*

Main category: eess.IV

TL;DR: Nexus-INR是一种基于多样化知识的ARSR框架，通过双分支编码器、知识蒸馏和分割模块，实现了高质量的医学图像超分辨率。

- Motivation: 传统CNN方法不适合任意分辨率超分辨率（ARSR），而INR方法虽能解决此问题，但在处理多模态图像时仍有局限。
- Method: 提出Nexus-INR框架，包含双分支编码器、知识蒸馏模块和集成分割模块，结合多模态信息和下游任务。
- Result: 在BraTS2020数据集上，Nexus-INR在超分辨率和分割任务中均优于现有方法。
- Conclusion: Nexus-INR通过多样化知识引导，显著提升了医学图像ARSR的质量和下游任务性能。


### [141] [GL-LCM: Global-Local Latent Consistency Models for Fast High-Resolution Bone Suppression in Chest X-Ray Images](https://arxiv.org/abs/2508.03357)
*Yifei Sun,Zhanghao Chen,Hao Zheng,Yuqing Lu,Lixin Duan,Fenglei Fan,Ahmed Elazab,Xiang Wan,Changmiao Wang,Ruiquan Ge*

Main category: eess.IV

TL;DR: 论文提出了一种名为GL-LCM的模型，用于快速高效地抑制胸部X光片中的骨骼结构，提升诊断准确性。

- Motivation: 骨骼结构在胸部X光片中会掩盖关键细节，影响诊断准确性。现有方法在骨骼抑制和细节保留之间难以平衡，且计算成本高。
- Method: 采用Global-Local Latent Consistency Model（GL-LCM）架构，结合肺部分割、双路径采样和全局-局部融合，并引入Local-Enhanced Guidance以减少边界伪影。
- Result: 在自建数据集SZCH-X-Rays和公开数据集JSRT上，GL-LCM表现出卓越的骨骼抑制效果和计算效率，优于其他方法。
- Conclusion: GL-LCM是一种高效且实用的骨骼抑制方法，适用于临床环境。


### [142] [Evaluating the Predictive Value of Preoperative MRI for Erectile Dysfunction Following Radical Prostatectomy](https://arxiv.org/abs/2508.03461)
*Gideon N. L. Rouwendaal,Daniël Boeke,Inge L. Cox,Henk G. van der Poel,Margriet C. van Dijk-de Haan,Regina G. H. Beets-Tan,Thierry N. Boellaard,Wilson Silva*

Main category: eess.IV

TL;DR: 研究探讨术前MRI是否能提升前列腺切除术后12个月勃起功能障碍（ED）的预测能力，发现MRI模型表现略优于手工解剖特征，但未超越临床基线。

- Motivation: 术前准确预测ED对患者咨询至关重要，但MRI的附加价值尚未充分研究。
- Method: 评估四种建模策略：临床基线、手工解剖特征、深度学习MRI切片和多模态融合。
- Result: MRI模型（AUC 0.569）略优于手工特征（AUC 0.554），但不及临床基线（AUC 0.663）。融合模型提升有限（AUC 0.586）。
- Conclusion: MRI模型未超越临床特征，但可能通过捕捉相关解剖结构模式补充未来多模态方法。


### [143] [CADD: Context aware disease deviations via restoration of brain images using normative conditional diffusion models](https://arxiv.org/abs/2508.03594)
*Ana Lawry Aguila,Ayodeji Ijishakin,Juan Eugenio Iglesias,Tomomi Takenaga,Yukihiro Nomura,Takeharu Yoshikawa,Osamu Abe,Shouhei Hanaoka*

Main category: eess.IV

TL;DR: CADD是一种基于条件扩散模型的3D图像规范建模方法，通过结合临床信息优化异常检测性能。

- Motivation: 利用机器学习分析医疗数据（如医院档案）可革新脑部疾病检测，但异质性数据中的病理检测是一大挑战。规范建模和扩散模型为潜在解决方案，但现有方法缺乏临床信息整合且重建效果不佳。
- Method: 提出CADD，首个用于3D图像规范建模的条件扩散模型，采用新型推理修复策略平衡异常去除与保留个体特征。
- Result: 在三个具有挑战性的数据集（包括低对比度、厚切片和运动伪影的临床扫描）上，CADD实现了神经异常检测的最优性能。
- Conclusion: CADD通过结合临床信息和优化重建策略，显著提升了异质性队列中的疾病检测能力。
## astro-ph.IM

### [144] [Investigation on deep learning-based galaxy image translation models](https://arxiv.org/abs/2508.03291)
*Hengxin Ruan,Qiufan Lin,Shupei Chen,Yang Wang,Wei Zhang*

Main category: astro-ph.IM

TL;DR: 研究了生成模型在星系图像翻译中保留高阶物理信息（如光谱红移）的能力，发现现有模型在保留红移信息方面存在不足，但仍对下游应用有价值。

- Motivation: 探讨生成模型在星系图像翻译中保留复杂高阶物理信息的能力，填补现有研究在物理信息保留方面的空白。
- Method: 测试了四种代表性模型（Swin Transformer、SRGAN、胶囊网络和扩散模型）在SDSS和CFHTLS星系图像上的表现，评估其对红移信息的保留能力。
- Result: 模型在保留红移信息方面表现不一，跨波段峰值通量包含重要红移信息，但翻译过程中存在不确定性。
- Conclusion: 尽管翻译图像不完美，但仍对下游应用有价值，研究为科学用途的图像翻译模型开发提供了启示。
## cs.CR

### [145] [BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.03221)
*Yu Pan,Jiahao Chen,Lin Wang,Bingrong Dai,Yi Du*

Main category: cs.CR

TL;DR: 本文提出了一种名为BadBlocks的新型轻量级后门攻击方法，针对扩散模型，仅需30%计算资源和20% GPU时间，成功绕过现有防御框架。

- Motivation: 扩散模型在图像生成领域取得显著进展，但易受后门攻击。现有防御方法能识别大多数攻击，但新型轻量级攻击仍未被解决。
- Method: BadBlocks通过选择性污染UNet架构中的特定块，保持其余部分正常功能，减少计算资源需求。
- Result: 实验显示，BadBlocks攻击成功率高，感知质量损失低，并能绕过现有防御框架。
- Conclusion: BadBlocks显著降低了后门攻击的门槛，使攻击者能在消费级GPU上对大规模扩散模型实施攻击。
## cs.NE

### [146] [VCNet: Recreating High-Level Visual Cortex Principles for Robust Artificial Vision](https://arxiv.org/abs/2508.02995)
*Brennen A. Hill,Zhang Xinyu,Timothy Putra Prasetio*

Main category: cs.NE

TL;DR: VCNet是一种受灵长类视觉皮层启发的神经网络架构，通过模拟生物机制，在图像分类任务中表现出高效性和鲁棒性。

- Motivation: 现代卷积神经网络（CNN）在数据效率、分布外泛化和对抗扰动方面存在局限性，而灵长类视觉系统表现出更高的效率和鲁棒性，因此研究其架构原理可能为人工视觉系统提供改进方向。
- Method: VCNet模拟了灵长类视觉皮层的宏观组织，包括分层处理、双流信息分离和自上而下的预测反馈机制。
- Result: VCNet在Spots-10数据集上达到92.1%的分类准确率，在光场图像分类任务中达到74.4%，优于同类规模的现代模型。
- Conclusion: 研究表明，将神经科学原理融入网络设计可以提升模型的效率和鲁棒性，为解决机器学习中的长期挑战提供了新方向。
## cs.RO

### [147] [UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands](https://arxiv.org/abs/2508.03339)
*Haoran Lin,Wenrui Chen,Xianchi Chen,Fan Yang,Qiang Diao,Wenxin Xie,Sijie Wu,Kailun Yang,Maojun Li,Yaonan Wang*

Main category: cs.RO

TL;DR: 论文提出了一种通用的功能性抓取标注策略和数据集UniFucGrasp，解决了现有抓取数据集忽略功能性抓取的问题，并支持低成本高效收集高质量抓取数据。

- Motivation: 现有灵巧抓取数据集过于关注稳定性，忽略了功能性抓取（如开瓶盖、握杯柄），且依赖昂贵难控的高自由度Shadow Hands。
- Method: 基于仿生学，将人类自然动作映射到多种手部结构，利用几何力闭合确保功能性、稳定性和类人抓取。
- Result: 实验表明，该方法提高了功能性操作精度和抓取稳定性，支持跨多种机械手的高效泛化，并克服了标注成本和泛化挑战。
- Conclusion: UniFucGrasp为灵巧抓取提供了首个多手功能性抓取数据集和合成模型，验证了其有效性。


### [148] [DiWA: Diffusion Policy Adaptation with World Models](https://arxiv.org/abs/2508.03645)
*Akshay L Chandra,Iman Nematollahi,Chenguang Huang,Tim Welschehold,Wolfram Burgard,Abhinav Valada*

Main category: cs.RO

TL;DR: DiWA框架通过离线世界模型优化扩散策略，显著提升样本效率，减少实际交互需求。

- Motivation: 扩散策略的强化学习调优面临奖励传播困难和实际交互需求高的挑战。
- Method: 提出DiWA框架，利用离线世界模型进行扩散策略的强化学习调优。
- Result: 在CALVIN基准测试中，DiWA仅通过离线调优即提升八项任务性能，样本效率显著优于无模型基线。
- Conclusion: DiWA首次实现基于离线世界模型的扩散策略调优，为实际机器人学习提供更高效、安全的解决方案。
