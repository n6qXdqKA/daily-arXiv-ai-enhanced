{"id": "2504.17039", "pdf": "https://arxiv.org/pdf/2504.17039", "abs": "https://arxiv.org/abs/2504.17039", "authors": ["Ruben Gonzalez Avil\u00e9s", "Linus Scheibenreif", "Damian Borth"], "title": "Dense Air Pollution Estimation from Sparse in-situ Measurements and Satellite Data", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the critical environmental challenge of estimating\nambient Nitrogen Dioxide (NO$_2$) concentrations, a key issue in public health\nand environmental policy. Existing methods for satellite-based air pollution\nestimation model the relationship between satellite and in-situ measurements at\nselect point locations. While these approaches have advanced our ability to\nprovide air quality estimations on a global scale, they come with inherent\nlimitations. The most notable limitation is the computational intensity\nrequired for generating comprehensive estimates over extensive areas. Motivated\nby these limitations, this study introduces a novel dense estimation technique.\nOur approach seeks to balance the accuracy of high-resolution estimates with\nthe practicality of computational constraints, thereby enabling efficient and\nscalable global environmental assessment. By utilizing a uniformly random\noffset sampling strategy, our method disperses the ground truth data pixel\nlocation evenly across a larger patch. At inference, the dense estimation\nmethod can then generate a grid of estimates in a single step, significantly\nreducing the computational resources required to provide estimates for larger\nareas. Notably, our approach also surpasses the results of existing point-wise\nmethods by a significant margin of $9.45\\%$, achieving a Mean Absolute Error\n(MAE) of $4.98\\ \\mu\\text{g}/\\text{m}^3$. This demonstrates both high accuracy\nand computational efficiency, highlighting the applicability of our method for\nglobal environmental assessment. Furthermore, we showcase the method's\nadaptability and robustness by applying it to diverse geographic regions. Our\nmethod offers a viable solution to the computational challenges of large-scale\nenvironmental monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bc6\u96c6\u4f30\u8ba1\u6280\u672f\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u7b97\u5168\u7403\u73af\u5883\u4e2d\u7684\u6c2e\u6c27\u5316\u7269\uff08NO\u2082\uff09\u6d53\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f3a\u5ea6\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u536b\u661f\u548c\u73b0\u573a\u6d4b\u91cf\u65b9\u6cd5\u5728\u4f30\u7b97\u5168\u7403\u7a7a\u6c14\u8d28\u91cf\u65f6\u5b58\u5728\u8ba1\u7b97\u5f3a\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u73af\u5883\u8bc4\u4f30\u7684\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u5747\u5300\u968f\u673a\u504f\u79fb\u91c7\u6837\u7b56\u7565\uff0c\u5c06\u5730\u9762\u771f\u5b9e\u6570\u636e\u5747\u5300\u5206\u6563\u5230\u66f4\u5927\u533a\u57df\uff0c\u901a\u8fc7\u5bc6\u96c6\u4f30\u8ba1\u65b9\u6cd5\u4e00\u6b65\u751f\u6210\u7f51\u683c\u4f30\u7b97\u503c\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e0a\u6bd4\u73b0\u6709\u70b9\u72b6\u65b9\u6cd5\u63d0\u9ad8\u4e869.45%\uff0c\u8fbe\u52304.98 \u03bcg/m\u00b3\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5168\u7403\u73af\u5883\u8bc4\u4f30\u3002"}}
{"id": "2504.17040", "pdf": "https://arxiv.org/pdf/2504.17040", "abs": "https://arxiv.org/abs/2504.17040", "authors": ["Zhenhailong Wang", "Senthil Purushwalkam", "Caiming Xiong", "Silvio Savarese", "Heng Ji", "Ran Xu"], "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.", "AI": {"tldr": "DyMU\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u52a8\u6001\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u53d8\u6362\u5668\u4e2d\u56fa\u5b9a\u957f\u5ea6\u8f93\u51fa\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u52a8\u6001\u9002\u5e94\u56fe\u50cf\u5185\u5bb9\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5305\u62ec\u52a8\u6001\u4ee4\u724c\u5408\u5e76\uff08DToMe\uff09\u548c\u865a\u62df\u4ee4\u724c\u89e3\u5408\u5e76\uff08VTU\uff09\uff0c\u524d\u8005\u6839\u636e\u56fe\u50cf\u590d\u6742\u5ea6\u5408\u5e76\u76f8\u4f3c\u4ee4\u724c\uff0c\u540e\u8005\u6a21\u62df\u5b8c\u6574\u5e8f\u5217\u7684\u6ce8\u610f\u529b\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cDyMU\u80fd\u51cf\u5c1132%-85%\u7684\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\uff0c\u6027\u80fd\u4e0e\u5b8c\u6574\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "DyMU\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u52a8\u6001\u9002\u5e94\u56fe\u50cf\u5185\u5bb9\uff0c\u9002\u7528\u4e8e\u591a\u79cdVLM\u67b6\u6784\uff0c\u5e76\u63d0\u4f9b\u7528\u6237\u5bf9\u8ba1\u7b97\u6210\u672c\u7684\u63a7\u5236\u3002"}}
{"id": "2504.17067", "pdf": "https://arxiv.org/pdf/2504.17067", "abs": "https://arxiv.org/abs/2504.17067", "authors": ["Xinqi Xiong", "Andrea Dunn Beltran", "Jun Myeong Choi", "Marc Niethammer", "Roni Sengupta"], "title": "PPS-Ctrl: Controllable Sim-to-Real Translation for Colonoscopy Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate depth estimation enhances endoscopy navigation and diagnostics, but\nobtaining ground-truth depth in clinical settings is challenging. Synthetic\ndatasets are often used for training, yet the domain gap limits generalization\nto real data. We propose a novel image-to-image translation framework that\npreserves structure while generating realistic textures from clinical data. Our\nkey innovation integrates Stable Diffusion with ControlNet, conditioned on a\nlatent representation extracted from a Per-Pixel Shading (PPS) map. PPS\ncaptures surface lighting effects, providing a stronger structural constraint\nthan depth maps. Experiments show our approach produces more realistic\ntranslations and improves depth estimation over GAN-based MI-CycleGAN. Our code\nis publicly accessible at https://github.com/anaxqx/PPS-Ctrl.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Stable Diffusion\u548cControlNet\u7684\u56fe\u50cf\u8f6c\u6362\u6846\u67b6\uff0c\u5229\u7528Per-Pixel Shading\uff08PPS\uff09\u56fe\u751f\u6210\u66f4\u771f\u5b9e\u7684\u7eb9\u7406\uff0c\u63d0\u5347\u5185\u7aa5\u955c\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e34\u5e8a\u73af\u5883\u4e2d\u83b7\u53d6\u771f\u5b9e\u6df1\u5ea6\u6570\u636e\u56f0\u96be\uff0c\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u9886\u57df\u5dee\u8ddd\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8ePPS\u7684\u6f5c\u5728\u8868\u793a\uff0c\u7ed3\u5408Stable Diffusion\u548cControlNet\uff0c\u751f\u6210\u4fdd\u7559\u7ed3\u6784\u7684\u771f\u5b9e\u7eb9\u7406\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u50cf\u66f4\u771f\u5b9e\uff0c\u6df1\u5ea6\u4f30\u8ba1\u6548\u679c\u4f18\u4e8e\u57fa\u4e8eGAN\u7684MI-CycleGAN\u3002", "conclusion": "\u901a\u8fc7PPS\u548cControlNet\u7684\u7ed3\u5408\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2504.17069", "pdf": "https://arxiv.org/pdf/2504.17069", "abs": "https://arxiv.org/abs/2504.17069", "authors": ["Rishav Pramanik", "Antoine Poupon", "Juan A. Rodriguez", "Masih Aminbeidokhti", "David Vazquez", "Christopher Pal", "Zhaozheng Yin", "Marco Pedersoli"], "title": "Distilling semantically aware orders for autoregressive image generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u4ee5\u4efb\u610f\u987a\u5e8f\u751f\u6210\u56fe\u50cf\u5757\uff0c\u5e76\u4f18\u5316\u751f\u6210\u987a\u5e8f\uff0c\u4ece\u800c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u91c7\u7528\u56fa\u5b9a\u7684\u5149\u6805\u626b\u63cf\u987a\u5e8f\uff08\u5de6\u4e0a\u5230\u53f3\u4e0b\uff09\uff0c\u8fd9\u79cd\u987a\u5e8f\u5ffd\u7565\u4e86\u56fe\u50cf\u5185\u5bb9\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0d\u4f73\u3002", "method": "\u9996\u5148\u8bad\u7ec3\u6a21\u578b\u4ee5\u4efb\u610f\u987a\u5e8f\u751f\u6210\u56fe\u50cf\u5757\uff0c\u63a8\u65ad\u5185\u5bb9\u548c\u4f4d\u7f6e\uff1b\u7136\u540e\u5229\u7528\u63d0\u53d6\u7684\u987a\u5e8f\u5fae\u8c03\u6a21\u578b\uff0c\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u4e24\u79cd\u6570\u636e\u96c6\u4e0a\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u4f18\u4e8e\u4f20\u7edf\u5149\u6805\u626b\u63cf\u987a\u5e8f\uff0c\u4e14\u8bad\u7ec3\u6210\u672c\u548c\u989d\u5916\u6807\u6ce8\u9700\u6c42\u76f8\u540c\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u751f\u6210\u987a\u5e8f\uff0c\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002"}}
{"id": "2504.16937", "pdf": "https://arxiv.org/pdf/2504.16937", "abs": "https://arxiv.org/abs/2504.16937", "authors": ["Ariel S. Kapusta", "David Jin", "Peter M. Teague", "Robert A. Houston", "Jonathan B. Elliott", "Grace Y. Park", "Shelby S. Holdren"], "title": "A Framework for the Assurance of AI-Enabled Systems", "categories": ["cs.AI"], "comment": "12 pages, 2 figures, published in conference proceedings of SPIE\n  Defense and Commercial Sensing conference on Assurance and Security for\n  AI-enabled Systems 2025", "summary": "The United States Department of Defense (DOD) looks to accelerate the\ndevelopment and deployment of AI capabilities across a wide spectrum of defense\napplications to maintain strategic advantages. However, many common features of\nAI algorithms that make them powerful, such as capacity for learning,\nlarge-scale data ingestion, and problem-solving, raise new technical, security,\nand ethical challenges. These challenges may hinder adoption due to uncertainty\nin development, testing, assurance, processes, and requirements.\nTrustworthiness through assurance is essential to achieve the expected value\nfrom AI.\n  This paper proposes a claims-based framework for risk management and\nassurance of AI systems that addresses the competing needs for faster\ndeployment, successful adoption, and rigorous evaluation. This framework\nsupports programs across all acquisition pathways provide grounds for\nsufficient confidence that an AI-enabled system (AIES) meets its intended\nmission goals without introducing unacceptable risks throughout its lifecycle.\nThe paper's contributions are a framework process for AI assurance, a set of\nrelevant definitions to enable constructive conversations on the topic of AI\nassurance, and a discussion of important considerations in AI assurance. The\nframework aims to provide the DOD a robust yet efficient mechanism for swiftly\nfielding effective AI capabilities without overlooking critical risks or\nundermining stakeholder trust.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u660e\u7684\u6846\u67b6\uff0c\u7528\u4e8eAI\u7cfb\u7edf\u7684\u98ce\u9669\u7ba1\u7406\u548c\u4fdd\u8bc1\uff0c\u4ee5\u5e73\u8861\u5feb\u901f\u90e8\u7f72\u3001\u6210\u529f\u91c7\u7528\u548c\u4e25\u683c\u8bc4\u4f30\u7684\u9700\u6c42\u3002", "motivation": "\u7f8e\u56fd\u56fd\u9632\u90e8\u5e0c\u671b\u52a0\u901fAI\u80fd\u529b\u7684\u5f00\u53d1\u548c\u90e8\u7f72\uff0c\u4f46AI\u7b97\u6cd5\u7684\u5f3a\u5927\u7279\u6027\u5e26\u6765\u4e86\u6280\u672f\u3001\u5b89\u5168\u548c\u4f26\u7406\u6311\u6218\uff0c\u53ef\u80fd\u963b\u788d\u5176\u91c7\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u58f0\u660e\u7684\u6846\u67b6\uff0c\u652f\u6301\u6240\u6709\u91c7\u529e\u9014\u5f84\u7684\u9879\u76ee\uff0c\u786e\u4fddAI\u7cfb\u7edf\u6ee1\u8db3\u4efb\u52a1\u76ee\u6807\u4e14\u4e0d\u5f15\u5165\u4e0d\u53ef\u63a5\u53d7\u7684\u98ce\u9669\u3002", "result": "\u8d21\u732e\u5305\u62ecAI\u4fdd\u8bc1\u7684\u6846\u67b6\u6d41\u7a0b\u3001\u76f8\u5173\u5b9a\u4e49\u4ee5\u53caAI\u4fdd\u8bc1\u7684\u91cd\u8981\u8003\u8651\u56e0\u7d20\u8ba8\u8bba\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e8\u5728\u4e3a\u56fd\u9632\u90e8\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u673a\u5236\uff0c\u5feb\u901f\u90e8\u7f72\u6709\u6548\u7684AI\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u5ffd\u89c6\u5173\u952e\u98ce\u9669\u6216\u635f\u5bb3\u5229\u76ca\u76f8\u5173\u8005\u7684\u4fe1\u4efb\u3002"}}
{"id": "2504.17076", "pdf": "https://arxiv.org/pdf/2504.17076", "abs": "https://arxiv.org/abs/2504.17076", "authors": ["Jens Petersen", "Davide Abati", "Amirhossein Habibian", "Auke Wiggers"], "title": "Scene-Aware Location Modeling for Data Augmentation in Automotive Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Generative image models are increasingly being used for training data\naugmentation in vision tasks. In the context of automotive object detection,\nmethods usually focus on producing augmented frames that look as realistic as\npossible, for example by replacing real objects with generated ones. Others try\nto maximize the diversity of augmented frames, for example by pasting lots of\ngenerated objects onto existing backgrounds. Both perspectives pay little\nattention to the locations of objects in the scene. Frame layouts are either\nreused with little or no modification, or they are random and disregard realism\nentirely. In this work, we argue that optimal data augmentation should also\ninclude realistic augmentation of layouts. We introduce a scene-aware\nprobabilistic location model that predicts where new objects can realistically\nbe placed in an existing scene. By then inpainting objects in these locations\nwith a generative model, we obtain much stronger augmentation performance than\nexisting approaches. We set a new state of the art for generative data\naugmentation on two automotive object detection tasks, achieving up to\n$2.8\\times$ higher gains than the best competing approach ($+1.4$ vs. $+0.5$\nmAP boost). We also demonstrate significant improvements for instance\nsegmentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u573a\u666f\u611f\u77e5\u7684\u6982\u7387\u4f4d\u7f6e\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u65b0\u7269\u4f53\u5728\u73b0\u6709\u573a\u666f\u4e2d\u7684\u5408\u7406\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u6a21\u578b\u5728\u8fd9\u4e9b\u4f4d\u7f6e\u586b\u5145\u7269\u4f53\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6570\u636e\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u56fe\u50cf\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u4e2d\uff0c\u901a\u5e38\u5ffd\u89c6\u7269\u4f53\u5728\u573a\u666f\u4e2d\u7684\u5408\u7406\u5e03\u5c40\uff0c\u5bfc\u81f4\u589e\u5f3a\u6548\u679c\u53d7\u9650\u3002", "method": "\u5f15\u5165\u573a\u666f\u611f\u77e5\u6982\u7387\u4f4d\u7f6e\u6a21\u578b\uff0c\u9884\u6d4b\u65b0\u7269\u4f53\u7684\u5408\u7406\u4f4d\u7f6e\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u6a21\u578b\u5728\u8fd9\u4e9b\u4f4d\u7f6e\u586b\u5145\u7269\u4f53\u3002", "result": "\u5728\u4e24\u4e2a\u6c7d\u8f66\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u8fbe2.8\u500d\u7684\u6027\u80fd\u63d0\u5347\uff08+1.4 vs. +0.5 mAP\uff09\uff0c\u5e76\u5728\u5b9e\u4f8b\u5206\u5272\u4e2d\u8868\u73b0\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u5173\u6ce8\u573a\u666f\u5e03\u5c40\u7684\u5408\u7406\u6027\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u751f\u6210\u6570\u636e\u589e\u5f3a\u7684\u6548\u679c\uff0c\u4e3a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u66f4\u4f18\u7684\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2504.16938", "pdf": "https://arxiv.org/pdf/2504.16938", "abs": "https://arxiv.org/abs/2504.16938", "authors": ["Lucas Carr", "Nicholas Leisegang", "Thomas Meyer", "Sergei Obiedkov"], "title": "Rational Inference in Formal Concept Analysis", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Defeasible conditionals are a form of non-monotonic inference which enable\nthe expression of statements like \"if $\\phi$ then normally $\\psi$\". The KLM\nframework defines a semantics for the propositional case of defeasible\nconditionals by construction of a preference ordering over possible worlds. The\npattern of reasoning induced by these semantics is characterised by consequence\nrelations satisfying certain desirable properties of non-monotonic reasoning.\nIn FCA, implications are used to describe dependencies between attributes.\nHowever, these implications are unsuitable to reason with erroneous data or\ndata prone to exceptions. Until recently, the topic of non-monotonic inference\nin FCA has remained largely uninvestigated. In this paper, we provide a\nconstruction of the KLM framework for defeasible reasoning in FCA and show that\nthis construction remains faithful to the principle of non-monotonic inference\ndescribed in the original framework. We present an additional argument that,\nwhile remaining consistent with the original ideas around non-monotonic\nreasoning, the defeasible reasoning we propose in FCA offers a more contextual\nview on inference, providing the ability for more relevant conclusions to be\ndrawn when compared to the propositional case.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u4e2d\u5e94\u7528KLM\u6846\u67b6\u7684\u53ef\u5e9f\u6b62\u63a8\u7406\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfFCA\u4e2d\u4f9d\u8d56\u5173\u7cfb\u65e0\u6cd5\u5904\u7406\u5f02\u5e38\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfFCA\u4e2d\u7684\u4f9d\u8d56\u5173\u7cfb\u65e0\u6cd5\u5904\u7406\u5f02\u5e38\u6570\u636e\u6216\u4f8b\u5916\u60c5\u51b5\uff0c\u800c\u53ef\u5e9f\u6b62\u63a8\u7406\uff08\u975e\u5355\u8c03\u63a8\u7406\uff09\u5728FCA\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6784\u5efaKLM\u6846\u67b6\u7684\u504f\u597d\u6392\u5e8f\uff0c\u5c06\u5176\u5e94\u7528\u4e8eFCA\u4e2d\uff0c\u4ee5\u652f\u6301\u53ef\u5e9f\u6b62\u63a8\u7406\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728FCA\u4e2d\u4fdd\u6301\u4e86KLM\u6846\u67b6\u7684\u975e\u5355\u8c03\u63a8\u7406\u539f\u5219\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u5177\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4e0eKLM\u6846\u67b6\u4e00\u81f4\uff0c\u8fd8\u5728FCA\u4e2d\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u66f4\u590d\u6742\u7684\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2504.17111", "pdf": "https://arxiv.org/pdf/2504.17111", "abs": "https://arxiv.org/abs/2504.17111", "authors": ["Tekin Gunasar", "Virginia de Sa"], "title": "Transferring Spatial Filters via Tangent Space Alignment in Motor Imagery BCIs", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "We propose a method to improve subject transfer in motor imagery BCIs by\naligning covariance matrices on a Riemannian manifold, followed by computing a\nnew common spatial patterns (CSP) based spatial filter. We explore various ways\nto integrate information from multiple subjects and show improved performance\ncompared to standard CSP. Across three datasets, our method shows marginal\nimprovements over standard CSP; however, when training data are limited, the\nimprovements become more significant.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u5bf9\u9f50\u534f\u65b9\u5dee\u77e9\u9635\u5e76\u8ba1\u7b97\u65b0\u7684CSP\u7a7a\u95f4\u6ee4\u6ce2\u5668\u7684\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u8fd0\u52a8\u60f3\u8c61BCI\u4e2d\u7684\u4e3b\u9898\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u8fd0\u52a8\u60f3\u8c61BCI\u4e2d\u4e3b\u9898\u8fc1\u79fb\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u5bf9\u9f50\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u5e76\u57fa\u4e8e\u6b64\u8ba1\u7b97\u65b0\u7684CSP\u7a7a\u95f4\u6ee4\u6ce2\u5668\uff0c\u540c\u65f6\u63a2\u7d22\u591a\u4e3b\u9898\u4fe1\u606f\u6574\u5408\u65b9\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6807\u51c6CSP\u65b9\u6cd5\u6709\u8fb9\u9645\u6539\u8fdb\uff1b\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u65f6\u6539\u8fdb\u66f4\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u65f6\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u8fd0\u52a8\u60f3\u8c61BCI\u7684\u4e3b\u9898\u8fc1\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16939", "pdf": "https://arxiv.org/pdf/2504.16939", "abs": "https://arxiv.org/abs/2504.16939", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Hongru Wang", "Vardhan Dongre", "Xiusi Chen", "Heng Ji", "Dilek Hakkani-T\u00fcr", "Gokhan Tur"], "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational\nAI from traditional dialogue systems into sophisticated agents capable of\nautonomous actions, contextual awareness, and multi-turn interactions with\nusers. Yet, fundamental questions about their capabilities, limitations, and\npaths forward remain open. This survey paper presents a desideratum for\nnext-generation Conversational Agents - what has been achieved, what challenges\npersist, and what must be done for more scalable systems that approach\nhuman-level intelligence. To that end, we systematically analyze LLM-driven\nConversational Agents by organizing their capabilities into three primary\ndimensions: (i) Reasoning - logical, systematic thinking inspired by human\nintelligence for decision making, (ii) Monitor - encompassing self-awareness\nand user interaction monitoring, and (iii) Control - focusing on tool\nutilization and policy following. Building upon this, we introduce a novel\ntaxonomy by classifying recent work on Conversational Agents around our\nproposed desideratum. We identify critical research gaps and outline key\ndirections, including realistic evaluations, long-term multi-turn reasoning\nskills, self-evolution capabilities, collaborative and multi-agent task\ncompletion, personalization, and proactivity. This work aims to provide a\nstructured foundation, highlight existing limitations, and offer insights into\npotential future research directions for Conversational Agents, ultimately\nadvancing progress toward Artificial General Intelligence (AGI). We maintain a\ncurated repository of papers at:\nhttps://github.com/emrecanacikgoz/awesome-conversational-agents.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u8bdd\u4ee3\u7406\u7684\u73b0\u72b6\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u5173\u952e\u7814\u7a76\u7f3a\u53e3\u3002", "motivation": "\u63a2\u8ba8LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u4ee3\u7406\u7684\u80fd\u529b\u3001\u5c40\u9650\u6027\u548c\u672a\u6765\u53d1\u5c55\u8def\u5f84\uff0c\u4ee5\u63a8\u52a8\u66f4\u63a5\u8fd1\u4eba\u7c7b\u667a\u80fd\u7684\u53ef\u6269\u5c55\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5c06\u5bf9\u8bdd\u4ee3\u7406\u7684\u80fd\u529b\u5206\u4e3a\u4e09\u4e2a\u7ef4\u5ea6\uff08\u63a8\u7406\u3001\u76d1\u63a7\u3001\u63a7\u5236\uff09\uff0c\u5e76\u56f4\u7ed5\u8fd9\u4e9b\u7ef4\u5ea6\u5206\u7c7b\u73b0\u6709\u7814\u7a76\uff0c\u63d0\u51fa\u65b0\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u7814\u7a76\u7f3a\u53e3\uff0c\u5982\u957f\u671f\u591a\u8f6e\u63a8\u7406\u3001\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u3001\u534f\u4f5c\u4efb\u52a1\u5b8c\u6210\u7b49\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u5bf9\u8bdd\u4ee3\u7406\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u6307\u51fa\u4e86\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.17132", "pdf": "https://arxiv.org/pdf/2504.17132", "abs": "https://arxiv.org/abs/2504.17132", "authors": ["Ning Li", "Antai Andy Liu", "Jingran Zhang", "Justin Cui"], "title": "Latent Video Dataset Distillation", "categories": ["cs.CV"], "comment": "https://openreview.net/forum?id=i665TIHv92", "summary": "Dataset distillation has demonstrated remarkable effectiveness in\nhigh-compression scenarios for image datasets. While video datasets inherently\ncontain greater redundancy, existing video dataset distillation methods\nprimarily focus on compression in the pixel space, overlooking advances in the\nlatent space that have been widely adopted in modern text-to-image and\ntext-to-video models. In this work, we bridge this gap by introducing a novel\nvideo dataset distillation approach that operates in the latent space using a\nstate-of-the-art variational encoder. Furthermore, we employ a diversity-aware\ndata selection strategy to select both representative and diverse samples.\nAdditionally, we introduce a simple, training-free method to further compress\nthe distilled latent dataset. By combining these techniques, our approach\nachieves a new state-of-the-art performance in dataset distillation,\noutperforming prior methods on all datasets, e.g. on HMDB51 IPC 1, we achieve a\n2.6% performance increase; on MiniUCF IPC 5, we achieve a 7.8% performance\nincrease.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u548c\u591a\u6837\u6027\u611f\u77e5\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u50cf\u7d20\u7a7a\u95f4\u538b\u7f29\uff0c\u5ffd\u7565\u4e86\u6f5c\u5728\u7a7a\u95f4\u7684\u8fdb\u5c55\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u7684\u53d8\u5206\u7f16\u7801\u5668\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u84b8\u998f\uff0c\u7ed3\u5408\u591a\u6837\u6027\u611f\u77e5\u6570\u636e\u9009\u62e9\u7b56\u7565\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "result": "\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728HMDB51 IPC 1\u4e0a\u6027\u80fd\u63d0\u53472.6%\uff0c\u5728MiniUCF IPC 5\u4e0a\u63d0\u53477.8%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u9886\u57df\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2504.17006", "pdf": "https://arxiv.org/pdf/2504.17006", "abs": "https://arxiv.org/abs/2504.17006", "authors": ["Jalal Arabneydi", "Saiful Islam", "Srijita Das", "Sai Krishna Gottipati", "William Duguay", "Cloderic Mars", "Matthew E. Taylor", "Matthew Guzdial", "Antoine Fagette", "Younes Zerouali"], "title": "A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "This is a result of the collaboration by JACOBB, AMII(Alberta Machine\n  Intelligence Institute), Thales and AI Redefined (AIR) in 2021-2023", "summary": "With the growing popularity of deep reinforcement learning (DRL),\nhuman-in-the-loop (HITL) approach has the potential to revolutionize the way we\napproach decision-making problems and create new opportunities for human-AI\ncollaboration. In this article, we introduce a novel multi-layered hierarchical\nHITL DRL algorithm that comprises three types of learning: self learning,\nimitation learning and transfer learning. In addition, we consider three forms\nof human inputs: reward, action and demonstration. Furthermore, we discuss main\nchallenges, trade-offs and advantages of HITL in solving complex problems and\nhow human information can be integrated in the AI solution systematically. To\nverify our technical results, we present a real-world unmanned aerial vehicles\n(UAV) problem wherein a number of enemy drones attack a restricted area. The\nobjective is to design a scalable HITL DRL algorithm for ally drones to\nneutralize the enemy drones before they reach the area. To this end, we first\nimplement our solution using an award-winning open-source HITL software called\nCogment. We then demonstrate several interesting results such as (a) HITL leads\nto faster training and higher performance, (b) advice acts as a guiding\ndirection for gradient methods and lowers variance, and (c) the amount of\nadvice should neither be too large nor too small to avoid over-training and\nunder-training. Finally, we illustrate the role of human-AI cooperation in\nsolving two real-world complex scenarios, i.e., overloaded and decoy attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u5c42\u6b21HITL DRL\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u4e09\u79cd\u5b66\u4e60\u65b9\u5f0f\u548c\u4e09\u79cd\u4eba\u7c7b\u8f93\u5165\u5f62\u5f0f\uff0c\u5e76\u901a\u8fc7\u65e0\u4eba\u673a\u5b9e\u6218\u95ee\u9898\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u666e\u53ca\uff0c\u4eba\u673a\u534f\u540c\uff08HITL\uff09\u65b9\u6cd5\u6709\u671b\u9769\u65b0\u51b3\u7b56\u95ee\u9898\u89e3\u51b3\u65b9\u5f0f\uff0c\u521b\u9020\u4eba\u673a\u534f\u4f5c\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u591a\u5c42\u6b21HITL DRL\u7b97\u6cd5\uff0c\u5305\u542b\u81ea\u4e3b\u5b66\u4e60\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5e76\u6574\u5408\u5956\u52b1\u3001\u52a8\u4f5c\u548c\u793a\u8303\u4e09\u79cd\u4eba\u7c7b\u8f93\u5165\u5f62\u5f0f\u3002", "result": "\u5728\u65e0\u4eba\u673a\u5b9e\u6218\u4e2d\u9a8c\u8bc1\u4e86HITL\u80fd\u52a0\u901f\u8bad\u7ec3\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u4eba\u7c7b\u5efa\u8bae\u80fd\u964d\u4f4e\u68af\u5ea6\u65b9\u6cd5\u7684\u65b9\u5dee\uff0c\u4e14\u5efa\u8bae\u91cf\u9700\u9002\u4e2d\u4ee5\u907f\u514d\u8fc7\u8bad\u7ec3\u6216\u6b20\u8bad\u7ec3\u3002", "conclusion": "HITL DRL\u7b97\u6cd5\u5728\u590d\u6742\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u89e3\u51b3\u73b0\u5b9e\u590d\u6742\u573a\u666f\uff08\u5982\u8fc7\u8f7d\u548c\u8bf1\u9975\u653b\u51fb\uff09\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2504.17162", "pdf": "https://arxiv.org/pdf/2504.17162", "abs": "https://arxiv.org/abs/2504.17162", "authors": ["Cece Zhang", "Xuehuan Zhu", "Nick Peterson", "Jieqiong Wang", "Shibiao Wan"], "title": "A Comprehensive Review on RNA Subcellular Localization Prediction", "categories": ["cs.CV", "cs.AI", "q-bio.GN", "q-bio.SC"], "comment": null, "summary": "The subcellular localization of RNAs, including long non-coding RNAs\n(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,\nplays a critical role in determining their biological functions. For instance,\nlncRNAs are predominantly associated with chromatin and act as regulators of\ngene transcription and chromatin structure, while mRNAs are distributed across\nthe nucleus and cytoplasm, facilitating the transport of genetic information\nfor protein synthesis. Understanding RNA localization sheds light on processes\nlike gene expression regulation with spatial and temporal precision. However,\ntraditional wet lab methods for determining RNA localization, such as in situ\nhybridization, are often time-consuming, resource-demanding, and costly. To\novercome these challenges, computational methods leveraging artificial\nintelligence (AI) and machine learning (ML) have emerged as powerful\nalternatives, enabling large-scale prediction of RNA subcellular localization.\nThis paper provides a comprehensive review of the latest advancements in\nAI-based approaches for RNA subcellular localization prediction, covering\nvarious RNA types and focusing on sequence-based, image-based, and hybrid\nmethodologies that combine both data types. We highlight the potential of these\nmethods to accelerate RNA research, uncover molecular pathways, and guide\ntargeted disease treatments. Furthermore, we critically discuss the challenges\nin AI/ML approaches for RNA subcellular localization, such as data scarcity and\nlack of benchmarks, and opportunities to address them. This review aims to\nserve as a valuable resource for researchers seeking to develop innovative\nsolutions in the field of RNA subcellular localization and beyond.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI/ML\u5728RNA\u4e9a\u7ec6\u80de\u5b9a\u4f4d\u9884\u6d4b\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63a2\u8ba8\u4e86\u5e8f\u5217\u3001\u56fe\u50cf\u53ca\u6df7\u5408\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6311\u6218\u4e0e\u673a\u9047\u3002", "motivation": "\u4f20\u7edf\u6e7f\u5b9e\u9a8c\u65b9\u6cd5\u8017\u65f6\u8017\u529b\uff0cAI/ML\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21RNA\u5b9a\u4f4d\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5e8f\u5217\u3001\u56fe\u50cf\u53ca\u6df7\u5408\u65b9\u6cd5\u7684AI/ML\u6280\u672f\uff0c\u7528\u4e8e\u9884\u6d4bRNA\u4e9a\u7ec6\u80de\u5b9a\u4f4d\u3002", "result": "AI/ML\u65b9\u6cd5\u5728RNA\u5b9a\u4f4d\u9884\u6d4b\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u53ef\u52a0\u901f\u7814\u7a76\u5e76\u6307\u5bfc\u75be\u75c5\u6cbb\u7597\u3002", "conclusion": "\u672c\u6587\u4e3aRNA\u4e9a\u7ec6\u80de\u5b9a\u4f4d\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u5e76\u6307\u51fa\u4e86\u6570\u636e\u7a00\u7f3a\u7b49\u6311\u6218\u53ca\u89e3\u51b3\u65b9\u5411\u3002"}}
{"id": "2504.17017", "pdf": "https://arxiv.org/pdf/2504.17017", "abs": "https://arxiv.org/abs/2504.17017", "authors": ["Balaji Rao", "William Eiers", "Carlo Lipizzi"], "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.LO"], "comment": "Accepted to the Proceedings of the 19th Conference on Neurosymbolic\n  Learning and Reasoning (NeSy 2025)", "summary": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5f62\u5f0f\u5316\u8bed\u8a00\u7684\u5b8c\u6574\u8bc1\u660e\uff0c\u7ed3\u5408LLM\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5728miniF2F\u548cIsabelle\u4e0a\u9a8c\u8bc1\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u4ee3\u7801\u7684\u5174\u8d77\uff0c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4ee3\u7801\u5c5e\u6027\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u4f46\u901a\u7528\u5b9a\u7406\u8bc1\u660e\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001LLM\u751f\u6210\u5f62\u5f0f\u5316\u8bc1\u660e\u3001\u542f\u53d1\u5f0f\u6a21\u5757\u6784\u5efa\u6700\u7ec8\u8bc1\u660e\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\uff08SFT\u548cRL\uff09\u8bad\u7ec3LLM\u3002", "result": "\u5728miniF2F\u6d4b\u8bd5\u57fa\u51c6\u548cIsabelle\u8bc1\u660e\u52a9\u624b\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86AWS S3\u6876\u8bbf\u95ee\u7b56\u7565\u4ee3\u7801\u7684\u9a8c\u8bc1\u7528\u4f8b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u5b9a\u7406\u8bc1\u660e\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u521b\u5efa\u4e86\u57fa\u4e8eFVEL\u6570\u636e\u96c6\u7684\u65b0\u8bad\u7ec3\u4efb\u52a1\u6570\u636e\u96c6\u3002"}}
{"id": "2504.17163", "pdf": "https://arxiv.org/pdf/2504.17163", "abs": "https://arxiv.org/abs/2504.17163", "authors": ["Kai Cui", "Jia Li", "Yu Liu", "Xuesong Zhang", "Zhenzhen Hu", "Meng Wang"], "title": "PhysioSync: Temporal and Cross-Modal Contrastive Learning Inspired by Physiological Synchronization for EEG-Based Emotion Recognition", "categories": ["cs.CV"], "comment": "The source code will be publicly available at\n  https://github.com/MSA-LMC/PhysioSync", "summary": "Electroencephalography (EEG) signals provide a promising and involuntary\nreflection of brain activity related to emotional states, offering significant\nadvantages over behavioral cues like facial expressions. However, EEG signals\nare often noisy, affected by artifacts, and vary across individuals,\ncomplicating emotion recognition. While multimodal approaches have used\nPeripheral Physiological Signals (PPS) like GSR to complement EEG, they often\noverlook the dynamic synchronization and consistent semantics between the\nmodalities. Additionally, the temporal dynamics of emotional fluctuations\nacross different time resolutions in PPS remain underexplored. To address these\nchallenges, we propose PhysioSync, a novel pre-training framework leveraging\ntemporal and cross-modal contrastive learning, inspired by physiological\nsynchronization phenomena. PhysioSync incorporates Cross-Modal Consistency\nAlignment (CM-CA) to model dynamic relationships between EEG and complementary\nPPS, enabling emotion-related synchronizations across modalities. Besides, it\nintroduces Long- and Short-Term Temporal Contrastive Learning (LS-TCL) to\ncapture emotional synchronization at different temporal resolutions within\nmodalities. After pre-training, cross-resolution and cross-modal features are\nhierarchically fused and fine-tuned to enhance emotion recognition. Experiments\non DEAP and DREAMER datasets demonstrate PhysioSync's advanced performance\nunder uni-modal and cross-modal conditions, highlighting its effectiveness for\nEEG-centered emotion recognition.", "AI": {"tldr": "PhysioSync\u662f\u4e00\u4e2a\u65b0\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u65f6\u95f4\u548c\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u540c\u6b65EEG\u548cPPS\u4fe1\u53f7\u6765\u63d0\u5347\u60c5\u7eea\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "EEG\u4fe1\u53f7\u867d\u7136\u80fd\u53cd\u6620\u60c5\u7eea\u72b6\u6001\uff0c\u4f46\u566a\u58f0\u5927\u4e14\u4e2a\u4f53\u5dee\u5f02\u663e\u8457\uff0c\u800c\u73b0\u6709\u8de8\u6a21\u6001\u65b9\u6cd5\u5ffd\u7565\u4e86\u52a8\u6001\u540c\u6b65\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faPhysioSync\u6846\u67b6\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5bf9\u9f50\uff08CM-CA\uff09\u548c\u957f\u77ed\u65f6\u5bf9\u6bd4\u5b66\u4e60\uff08LS-TCL\uff09\uff0c\u9884\u8bad\u7ec3\u540e\u901a\u8fc7\u7279\u5f81\u878d\u5408\u4f18\u5316\u60c5\u7eea\u8bc6\u522b\u3002", "result": "\u5728DEAP\u548cDREAMER\u6570\u636e\u96c6\u4e0a\uff0cPhysioSync\u5728\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PhysioSync\u901a\u8fc7\u52a8\u6001\u540c\u6b65\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u4e3a\u4e2d\u5fc3\u7684\u60c5\u7eea\u8bc6\u522b\u6548\u679c\u3002"}}
{"id": "2504.17087", "pdf": "https://arxiv.org/pdf/2504.17087", "abs": "https://arxiv.org/abs/2504.17087", "authors": ["Yuran Li", "Jama Hussein Mohamud", "Chongren Sun", "Di Wu", "Benoit Boulet"], "title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments", "categories": ["cs.AI"], "comment": "12 pages, 5 figures, 6 tables", "summary": "Large language models (LLMs) are being widely applied across various fields,\nbut as tasks become more complex, evaluating their responses is increasingly\nchallenging. Compared to human evaluators, the use of LLMs to support\nperformance evaluation offers a more efficient alternative. However, most\nstudies focus mainly on aligning LLMs' judgments with human preferences,\noverlooking the existence of biases and mistakes in human judgment.\nFurthermore, how to select suitable LLM judgments given multiple potential LLM\nresponses remains underexplored. To address these two aforementioned issues, we\npropose a three-stage meta-judge selection pipeline: 1) developing a\ncomprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM\nagents to score judgments, and 3) applying a threshold to filter out\nlow-scoring judgments. Compared to methods using a single LLM as both judge and\nmeta-judge, our pipeline introduces multi-agent collaboration and a more\ncomprehensive rubric. Experimental results on the JudgeBench dataset show about\n15.55\\% improvement compared to raw judgments and about 8.37\\% improvement over\nthe single-agent baseline. Our work demonstrates the potential of LLMs as\nmeta-judges and lays the foundation for future research on constructing\npreference datasets for LLM-as-a-judge reinforcement learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u5143\u5224\u65ad\u9009\u62e9\u6d41\u7a0b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u7efc\u5408\u8bc4\u5206\u6807\u51c6\uff0c\u63d0\u9ad8\u4e86LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u590d\u6742\u5316\uff0c\u8bc4\u4f30LLM\u54cd\u5e94\u53d8\u5f97\u56f0\u96be\uff0c\u800c\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5224\u65ad\u7684\u504f\u89c1\u548c\u9519\u8bef\uff0c\u4e14\u672a\u5145\u5206\u63a2\u7d22\u5982\u4f55\u4ece\u591a\u4e2aLLM\u54cd\u5e94\u4e2d\u9009\u62e9\u5408\u9002\u5224\u65ad\u3002", "method": "1) \u4e0eGPT-4\u548c\u4eba\u7c7b\u4e13\u5bb6\u5171\u540c\u5236\u5b9a\u8bc4\u5206\u6807\u51c6\uff1b2) \u4f7f\u7528\u4e09\u4e2a\u9ad8\u7ea7LLM\u667a\u80fd\u4f53\u8bc4\u5206\uff1b3) \u901a\u8fc7\u9608\u503c\u8fc7\u6ee4\u4f4e\u5206\u5224\u65ad\u3002", "result": "\u5728JudgeBench\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u539f\u59cb\u5224\u65ad\u548c\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e8615.55%\u548c8.37%\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLM\u4f5c\u4e3a\u5143\u5224\u65ad\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u6784\u5efaLLM\u4f5c\u4e3a\u5224\u65ad\u8005\u7684\u5f3a\u5316\u5b66\u4e60\u504f\u597d\u6570\u636e\u96c6\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.17177", "pdf": "https://arxiv.org/pdf/2504.17177", "abs": "https://arxiv.org/abs/2504.17177", "authors": ["Kevin Lane", "Morteza Karimzadeh"], "title": "A Genealogy of Multi-Sensor Foundation Models in Remote Sensing", "categories": ["cs.CV", "cs.LG", "I.4.7; I.4.8"], "comment": "20 pages, submitted to ACM SigSpatial, currently under peer review", "summary": "Foundation models have garnered increasing attention for representation\nlearning in remote sensing, primarily adopting approaches that have\ndemonstrated success in computer vision with minimal domain-specific\nmodification. However, the development and application of foundation models in\nthis field are still burgeoning, as there are a variety of competing approaches\nthat each come with significant benefits and drawbacks. This paper examines\nthese approaches along with their roots in the computer vision field in order\nto characterize potential advantages and pitfalls while outlining future\ndirections to further improve remote sensing-specific foundation models. We\ndiscuss the quality of the learned representations and methods to alleviate the\nneed for massive compute resources. We place emphasis on the multi-sensor\naspect of Earth observations, and the extent to which existing approaches\nleverage multiple sensors in training foundation models in relation to\nmulti-modal foundation models. Finally, we identify opportunities for further\nharnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote\nsensing observations.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9065\u611f\u9886\u57df\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u4e0e\u5e94\u7528\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\uff0c\u5305\u62ec\u591a\u4f20\u611f\u5668\u5229\u7528\u548c\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u9065\u611f\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u65b9\u6cd5\u591a\u6837\u4e14\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u548c\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u9065\u611f\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u4f18\u52bf\u548c\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5f3a\u8c03\u4e86\u591a\u4f20\u611f\u5668\u5229\u7528\u548c\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672a\u6765\u5e94\u8fdb\u4e00\u6b65\u5229\u7528\u672a\u6807\u8bb0\u3001\u5b63\u8282\u6027\u548c\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u4f18\u5316\u9065\u611f\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.17179", "pdf": "https://arxiv.org/pdf/2504.17179", "abs": "https://arxiv.org/abs/2504.17179", "authors": ["Mohammad Zarei", "Melanie A Jutras", "Eliana Evans", "Mike Tan", "Omid Aaramoon"], "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO", "68T45, 68T05 68T45, 68T05 68T45, 68T05", "I.2.6; I.2.10; I.4.8"], "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025", "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u751f\u6210\u548c\u53ef\u89e3\u91caAI\u6280\u672f\u6765\u7406\u89e3\u548c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u7f55\u89c1\u6545\u969c\u6a21\u5f0f\uff08RFMs\uff09\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u5728\u68c0\u6d4b\u7f55\u89c1\u6545\u969c\u6a21\u5f0f\uff08RFMs\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u88ab\u79f0\u4e3a\u201c\u957f\u5c3e\u6311\u6218\u201d\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6027\u6837\u672c\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6765\u589e\u5f3aAVs\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u5bf9\u8c61\u5206\u5272\u63a9\u7801\u5e76\u53cd\u8f6c\u751f\u6210\u73af\u5883\u63a9\u7801\uff0c\u7ed3\u5408\u6587\u672c\u63d0\u793a\u8f93\u5165\u5b9a\u5236\u6269\u6563\u6a21\u578b\uff08Stable Diffusion\uff09\uff0c\u5229\u7528\u5bf9\u6297\u6027\u566a\u58f0\u4f18\u5316\u751f\u6210\u591a\u6837\u5316\u7684\u73af\u5883\u56fe\u50cf\u4ee5\u66b4\u9732AI\u7cfb\u7edf\u7684\u6f0f\u6d1e\u3002", "result": "\u751f\u6210\u5305\u542bRFMs\u7684\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u6539\u8fdbAV\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u548c\u89e3\u51b3AVs\u4e2d\u7684\u7f55\u89c1\u6545\u969c\u6a21\u5f0f\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2504.17180", "pdf": "https://arxiv.org/pdf/2504.17180", "abs": "https://arxiv.org/abs/2504.17180", "authors": ["Minkyu Choi", "S P Sharan", "Harsh Goel", "Sahil Shah", "Sandeep Chinchali"], "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce \\(\\projectname\\), a novel zero-training video\nrefinement pipeline that leverages neuro-symbolic feedback to automatically\nenhance video generation, achieving superior alignment with the prompts. Our\napproach first derives the neuro-symbolic feedback by analyzing a formal video\nrepresentation and pinpoints semantically inconsistent events, objects, and\ntheir corresponding frames. This feedback then guides targeted edits to the\noriginal video. Extensive empirical evaluations on both open-source and\nproprietary T2V models demonstrate that \\(\\projectname\\) significantly enhances\ntemporal and logical alignment across diverse prompts by almost $40\\%$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u8bad\u7ec3\u7684\u89c6\u9891\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u53cd\u9988\u63d0\u5347\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u63d0\u793a\u65f6\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u8bad\u7ec3\u7684\u89c6\u9891\u4f18\u5316\u6d41\u7a0b\uff0c\u5229\u7528\u795e\u7ecf\u7b26\u53f7\u53cd\u9988\u5206\u6790\u89c6\u9891\u8868\u793a\u5e76\u6307\u5bfc\u9488\u5bf9\u6027\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e0e\u903b\u8f91\u5bf9\u9f50\uff0c\u6548\u679c\u63d0\u5347\u8fd140%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u63d0\u793a\u4e0b\u89c6\u9891\u751f\u6210\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2504.17282", "pdf": "https://arxiv.org/pdf/2504.17282", "abs": "https://arxiv.org/abs/2504.17282", "authors": ["Lynn Cherif", "Flemming Kondrup", "David Venuto", "Ankit Anand", "Doina Precup", "Khimya Khetarpal"], "title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Agents that can autonomously navigate the web through a graphical user\ninterface (GUI) using a unified action space (e.g., mouse and keyboard actions)\ncan require very large amounts of domain-specific expert demonstrations to\nachieve good performance. Low sample efficiency is often exacerbated in\nsparse-reward and large-action-space environments, such as a web GUI, where\nonly a few actions are relevant in any given situation. In this work, we\nconsider the low-data regime, with limited or no access to expert behavior. To\nenable sample-efficient learning, we explore the effect of constraining the\naction space through $\\textit{intent-based affordances}$ -- i.e., considering\nin any situation only the subset of actions that achieve a desired outcome. We\npropose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$,\na method that leverages pre-trained vision-language models (VLMs) to generate\ncode that determines affordable actions through implicit intent-completion\nfunctions and using a fully-automated program generation and verification\npipeline. These programs are then used in-the-loop of a reinforcement learning\nagent to return a set of affordances given a pixel observation. By greatly\nreducing the number of actions that an agent must consider, we demonstrate on a\nwide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$\n$\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,\n$\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of\ntasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared\nwith behavior cloning when a small number of expert demonstrations is\navailable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoGA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\uff0c\u9650\u5236\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728MiniWob++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u7a00\u758f\u5956\u52b1\u548c\u5927\u52a8\u4f5c\u7a7a\u95f4\u73af\u5883\u4e2d\uff08\u5982\u7f51\u9875GUI\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6f14\u793a\u624d\u80fd\u8fbe\u5230\u826f\u597d\u6027\u80fd\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u3002", "method": "CoGA\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\uff0c\u901a\u8fc7\u610f\u56fe\u9a71\u52a8\u7684\u52a8\u4f5c\u7a7a\u95f4\u9650\u5236\uff0c\u7ed3\u5408\u81ea\u52a8\u7a0b\u5e8f\u751f\u6210\u548c\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u63d0\u4f9b\u53ef\u6267\u884c\u7684\u52a8\u4f5c\u5b50\u96c6\u3002", "result": "\u5728MiniWob++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoGA\u6bd4\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u6837\u672c\u6548\u7387\u9ad8\u5f97\u591a\uff0c\u5176\u7a0b\u5e8f\u80fd\u6cdb\u5316\u5230\u540c\u7c7b\u4efb\u52a1\uff0c\u4e14\u5728\u5c11\u91cf\u4e13\u5bb6\u6f14\u793a\u4e0b\u8868\u73b0\u4f18\u4e8e\u6216\u63a5\u8fd1\u884c\u4e3a\u514b\u9686\u3002", "conclusion": "CoGA\u901a\u8fc7\u9650\u5236\u52a8\u4f5c\u7a7a\u95f4\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.17207", "pdf": "https://arxiv.org/pdf/2504.17207", "abs": "https://arxiv.org/abs/2504.17207", "authors": ["Phillip Y. Lee", "Jihyeon Je", "Chanho Park", "Mikaela Angelina Uy", "Leonidas Guibas", "Minhyuk Sung"], "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation", "categories": ["cs.CV"], "comment": "Project Page: https://apc-vlm.github.io/", "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5fc3\u7406\u610f\u8c61\u6a21\u62df\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u89c6\u89d2\u611f\u77e5\u63a8\u7406\u7684\u6846\u67b6Abstract Perspective Change\uff08APC\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86VLM\u7684\u89c6\u89d2\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u89c6\u89d2\u611f\u77e5\u662f\u4eba\u7c7b\u89c6\u89c9\u7406\u89e3\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u73b0\u6709VLM\u5728\u6b64\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u5b58\u5728\u5f3a\u70c8\u7684\u81ea\u6211\u4e2d\u5fc3\u504f\u5dee\u3002", "method": "\u63d0\u51faAPC\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u5206\u5272\u548c\u65b9\u5411\u4f30\u8ba1\uff09\u6784\u5efa\u573a\u666f\u62bd\u8c61\u5e76\u5b9e\u73b0\u89c6\u89d2\u53d8\u6362\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAPC\u663e\u8457\u4f18\u4e8e\u73b0\u6709VLM\u53ca\u57fa\u4e8e\u7a7a\u95f4\u63a8\u7406\u548c\u65b0\u89c6\u89d2\u5408\u6210\u7684\u65b9\u6cd5\u3002", "conclusion": "APC\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86VLM\u7684\u89c6\u89d2\u611f\u77e5\u80fd\u529b\uff0c\u7f29\u5c0f\u4e86\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5dee\u8ddd\u3002"}}
{"id": "2504.17295", "pdf": "https://arxiv.org/pdf/2504.17295", "abs": "https://arxiv.org/abs/2504.17295", "authors": ["Shahrzad Khayatbashi", "Viktor Sj\u00f6lind", "Anders Gran\u00e5ker", "Amin Jalali"], "title": "AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\uff08\u5c24\u5176\u662fLLMs\uff09\u5982\u4f55\u901a\u8fc7\u81ea\u52a8\u5316\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u63a8\u52a8\u4e1a\u52a1\u6d41\u7a0b\u91cd\u6784\uff0c\u5e76\u4ee5\u4fdd\u9669\u4e1a\u6848\u4f8b\u5c55\u793a\u4e86OCPM\u65b9\u6cd5\u5728\u8bc4\u4f30AI\u81ea\u52a8\u5316\u5f71\u54cd\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8bc4\u4f30AI\u81ea\u52a8\u5316\u5bf9\u4e1a\u52a1\u6d41\u7a0b\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u4f20\u7edf\u4e0eAI\u589e\u5f3a\u6d41\u7a0b\u5171\u5b58\u7684\u8fc7\u6e21\u9636\u6bb5\u3002", "method": "\u91c7\u7528\u5bf9\u8c61\u4e2d\u5fc3\u6d41\u7a0b\u6316\u6398\uff08OCPM\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u4fdd\u9669\u4e1a\u6848\u4f8b\uff0c\u5206\u6790LLM\u81ea\u52a8\u5316\u5bf9\u6d41\u7a0b\u53ef\u6269\u5c55\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u65b0\u6d41\u7a0b\u52a8\u6001\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86OCPM\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2504.17213", "pdf": "https://arxiv.org/pdf/2504.17213", "abs": "https://arxiv.org/abs/2504.17213", "authors": ["Shiwen Cao", "Zhaoxing Zhang", "Junming Jiao", "Juyi Qiao", "Guowen Song", "Rong Shen"], "title": "MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Even in the era of rapid advances in large models, video understanding,\nparticularly long videos, remains highly challenging. Compared with textual or\nimage-based information, videos commonly contain more information with\nredundancy, requiring large models to strategically allocate attention at a\nglobal level for accurate comprehension. To address this, we propose MCAF, an\nagent-based, training-free framework perform video understanding through\nMultimodal Coarse-to-fine Attention Focusing. The key innovation lies in its\nability to sense and prioritize segments of the video that are highly relevant\nto the understanding task. First, MCAF hierarchically concentrates on highly\nrelevant frames through multimodal information, enhancing the correlation\nbetween the acquired contextual information and the query. Second, it employs a\ndilated temporal expansion mechanism to mitigate the risk of missing crucial\ndetails when extracting information from these concentrated frames. In\naddition, our framework incorporates a self-reflection mechanism utilizing the\nconfidence level of the model's responses as feedback. By iteratively applying\nthese two creative focusing strategies, it adaptively adjusts attention to\ncapture highly query-connected context and thus improves response accuracy.\nMCAF outperforms comparable state-of-the-art methods on average. On the\nEgoSchema dataset, it achieves a remarkable 5% performance gain over the\nleading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms\nthe current state-of-the-art standard by 0.2% and 0.3% respectively. On the\nVideo-MME dataset, which features videos averaging nearly an hour in length,\nMCAF also outperforms other agent-based methods.", "AI": {"tldr": "MCAF\u662f\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7c97\u5230\u7ec6\u6ce8\u610f\u529b\u805a\u7126\u5b9e\u73b0\u89c6\u9891\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u7406\u89e3\uff08\u5c24\u5176\u662f\u957f\u89c6\u9891\uff09\u56e0\u4fe1\u606f\u5197\u4f59\u548c\u590d\u6742\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u6a21\u578b\u5168\u5c40\u5206\u914d\u6ce8\u610f\u529b\u3002", "method": "MCAF\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u606f\u5206\u5c42\u805a\u7126\u76f8\u5173\u5e27\uff0c\u5e76\u91c7\u7528\u6269\u5f20\u65f6\u95f4\u6269\u5c55\u673a\u5236\u907f\u514d\u9057\u6f0f\u5173\u952e\u7ec6\u8282\uff0c\u7ed3\u5408\u81ea\u53cd\u9988\u673a\u5236\u8fed\u4ee3\u4f18\u5316\u6ce8\u610f\u529b\u5206\u914d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5982EgoSchema\u63d0\u53475%\uff0cNext-QA\u548cIntentQA\u5206\u522b\u63d0\u53470.2%\u548c0.3%\u3002", "conclusion": "MCAF\u901a\u8fc7\u521b\u65b0\u6ce8\u610f\u529b\u805a\u7126\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.17356", "pdf": "https://arxiv.org/pdf/2504.17356", "abs": "https://arxiv.org/abs/2504.17356", "authors": ["Weiliang Zhang", "Xiaohan Huang", "Yi Du", "Ziyue Qiao", "Qingqing Long", "Zhen Meng", "Yuanchun Zhou", "Meng Xiao"], "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection", "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHRLFS\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u5316\u7279\u5f81\u9009\u62e9\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e0b\u6e38\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u96c6\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u6bcf\u4e2a\u7279\u5f81\u4f7f\u7528\u4e00\u4e2a\u4ee3\u7406\u7684\u6a21\u5f0f\u3002", "method": "HRLFS\u5229\u7528LLM\u63d0\u53d6\u7279\u5f81\u7684\u6570\u5b66\u548c\u8bed\u4e49\u7279\u6027\uff0c\u5bf9\u7279\u5f81\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u548c\u5b50\u805a\u7c7b\u6784\u5efa\u5206\u5c42\u4ee3\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHRLFS\u5728\u6027\u80fd\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f81\u5b50\u7a7a\u95f4\u63a2\u7d22\u7684\u6548\u7387\u3002", "conclusion": "HRLFS\u901a\u8fc7\u5206\u5c42\u4ee3\u7406\u548cLLM\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u6570\u636e\u96c6\u7279\u5f81\u9009\u62e9\u7684\u6311\u6218\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2504.17223", "pdf": "https://arxiv.org/pdf/2504.17223", "abs": "https://arxiv.org/abs/2504.17223", "authors": ["Mengyu Qiao", "Runze Tian", "Yang Wang"], "title": "Towards Generalizable Deepfake Detection with Spatial-Frequency Collaborative Learning and Hierarchical Cross-Modal Fusion", "categories": ["cs.CV"], "comment": null, "summary": "The rapid evolution of deep generative models poses a critical challenge to\ndeepfake detection, as detectors trained on forgery-specific artifacts often\nsuffer significant performance degradation when encountering unseen forgeries.\nWhile existing methods predominantly rely on spatial domain analysis, frequency\ndomain operations are primarily limited to feature-level augmentation, leaving\nfrequency-native artifacts and spatial-frequency interactions insufficiently\nexploited. To address this limitation, we propose a novel detection framework\nthat integrates multi-scale spatial-frequency analysis for universal deepfake\ndetection. Our framework comprises three key components: (1) a local spectral\nfeature extraction pipeline that combines block-wise discrete cosine transform\nwith cascaded multi-scale convolutions to capture subtle spectral artifacts;\n(2) a global spectral feature extraction pipeline utilizing scale-invariant\ndifferential accumulation to identify holistic forgery distribution patterns;\nand (3) a multi-stage cross-modal fusion mechanism that incorporates\nshallow-layer attention enhancement and deep-layer dynamic modulation to model\nspatial-frequency interactions. Extensive evaluations on widely adopted\nbenchmarks demonstrate that our method outperforms state-of-the-art deepfake\ndetection methods in both accuracy and generalizability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c3a\u5ea6\u7a7a\u95f4-\u9891\u7387\u5206\u6790\u7684\u65b0\u578b\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7a7a\u95f4\u57df\u5206\u6790\uff0c\u9891\u7387\u57df\u64cd\u4f5c\u4ec5\u7528\u4e8e\u7279\u5f81\u589e\u5f3a\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u9891\u7387\u539f\u751f\u4f2a\u5f71\u548c\u7a7a\u95f4-\u9891\u7387\u4ea4\u4e92\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u5c40\u90e8\u9891\u8c31\u7279\u5f81\u63d0\u53d6\uff08\u5757\u7ea7\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u4e0e\u591a\u5c3a\u5ea6\u5377\u79ef\uff09\u3001\u5168\u5c40\u9891\u8c31\u7279\u5f81\u63d0\u53d6\uff08\u5c3a\u5ea6\u4e0d\u53d8\u5dee\u5206\u7d2f\u79ef\uff09\u4ee5\u53ca\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u878d\u5408\u673a\u5236\uff08\u6d45\u5c42\u6ce8\u610f\u529b\u589e\u5f3a\u4e0e\u6df1\u5c42\u52a8\u6001\u8c03\u5236\uff09\u3002", "result": "\u5728\u5e7f\u6cdb\u91c7\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u5c3a\u5ea6\u7a7a\u95f4-\u9891\u7387\u5206\u6790\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.17402", "pdf": "https://arxiv.org/pdf/2504.17402", "abs": "https://arxiv.org/abs/2504.17402", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskisarkka", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9886\u57df\u7279\u5b9a\u672c\u4f53\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u4e86DeepSeek\u548co1-preview\u4e24\u79cd\u6a21\u578b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5177\u6709\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u9886\u57df\u7279\u5b9a\u672c\u4f53\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u53ca\u5176\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684LLMs\uff08DeepSeek\u548co1-preview\uff09\uff0c\u901a\u8fc7\u80fd\u529b\u95ee\u9898\uff08CQs\uff09\u548c\u76f8\u5173\u7528\u6237\u6545\u4e8b\u751f\u6210\u672c\u4f53\uff0c\u5e76\u5728\u516d\u4e2a\u4e0d\u540c\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e24\u79cdLLMs\u5728\u6240\u6709\u9886\u57df\u4e2d\u7684\u8868\u73b0\u5747\u4e00\u81f4\uff0c\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u6cdb\u5316\u672c\u4f53\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "LLM\u4e3a\u57fa\u7840\u7684\u65b9\u6cd5\u5728\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u9886\u57df\u65e0\u5173\u7684\u672c\u4f53\u6784\u5efa\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u589e\u5f3a\u81ea\u52a8\u63a8\u7406\u548c\u77e5\u8bc6\u8868\u793a\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.17224", "pdf": "https://arxiv.org/pdf/2504.17224", "abs": "https://arxiv.org/abs/2504.17224", "authors": ["Zhifeng Wang", "Qixuan Zhang", "Peter Zhang", "Wenjia Niu", "Kaihao Zhang", "Ramesh Sankaranarayana", "Sabrina Caldwell", "Tom Gedeon"], "title": "Visual and textual prompts for enhancing emotion recognition in video", "categories": ["cs.CV"], "comment": "12 pages, 10 figures", "summary": "Vision Large Language Models (VLLMs) exhibit promising potential for\nmulti-modal understanding, yet their application to video-based emotion\nrecognition remains limited by insufficient spatial and contextual awareness.\nTraditional approaches, which prioritize isolated facial features, often\nneglect critical non-verbal cues such as body language, environmental context,\nand social interactions, leading to reduced robustness in real-world scenarios.\nTo address this gap, we propose Set-of-Vision-Text Prompting (SoVTP), a novel\nframework that enhances zero-shot emotion recognition by integrating spatial\nannotations (e.g., bounding boxes, facial landmarks), physiological signals\n(facial action units), and contextual cues (body posture, scene dynamics,\nothers' emotions) into a unified prompting strategy. SoVTP preserves holistic\nscene information while enabling fine-grained analysis of facial muscle\nmovements and interpersonal dynamics. Extensive experiments show that SoVTP\nachieves substantial improvements over existing visual prompting methods,\ndemonstrating its effectiveness in enhancing VLLMs' video emotion recognition\ncapabilities.", "AI": {"tldr": "SoVTP\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u6807\u6ce8\u3001\u751f\u7406\u4fe1\u53f7\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86VLLMs\u5728\u89c6\u9891\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLLMs\u5728\u89c6\u9891\u60c5\u611f\u8bc6\u522b\u4e2d\u56e0\u7f3a\u4e4f\u7a7a\u95f4\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u800c\u53d7\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u975e\u8bed\u8a00\u7ebf\u7d22\u5bfc\u81f4\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSoVTP\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4\u6807\u6ce8\uff08\u5982\u8fb9\u754c\u6846\u3001\u9762\u90e8\u5173\u952e\u70b9\uff09\u3001\u751f\u7406\u4fe1\u53f7\uff08\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff09\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff08\u8eab\u4f53\u59ff\u6001\u3001\u573a\u666f\u52a8\u6001\u7b49\uff09\uff0c\u5f62\u6210\u7edf\u4e00\u63d0\u793a\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSoVTP\u5728\u96f6\u6837\u672c\u60c5\u611f\u8bc6\u522b\u4e2d\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u63d0\u793a\u65b9\u6cd5\u3002", "conclusion": "SoVTP\u6709\u6548\u589e\u5f3a\u4e86VLLMs\u7684\u89c6\u9891\u60c5\u611f\u8bc6\u522b\u80fd\u529b\uff0c\u4fdd\u7559\u4e86\u573a\u666f\u6574\u4f53\u4fe1\u606f\u5e76\u652f\u6301\u7ec6\u7c92\u5ea6\u5206\u6790\u3002"}}
{"id": "2504.17404", "pdf": "https://arxiv.org/pdf/2504.17404", "abs": "https://arxiv.org/abs/2504.17404", "authors": ["Feifei Zhao", "Yuwei Wang", "Enmeng Lu", "Dongcheng Zhao", "Bing Han", "Haibo Tong", "Yao Liang", "Dongqi Liang", "Kang Sun", "Lei Wang", "Yitao Liang", "Chao Liu", "Yaodong Yang", "Yi Zeng"], "title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society", "categories": ["cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8d85\u7ea7\u5bf9\u9f50\uff08superalignment\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5916\u90e8\u76d1\u7763\u548c\u5185\u5728\u4e3b\u52a8\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u8d85\u7ea7\u667a\u80fdAI\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e00\u81f4\u3002", "motivation": "\u968f\u7740AI\u5411\u8d85\u7ea7\u667a\u80fd\uff08ASI\uff09\u53d1\u5c55\uff0c\u5176\u53ef\u80fd\u8d85\u51fa\u4eba\u7c7b\u63a7\u5236\u5e76\u5f15\u53d1\u707e\u96be\u6027\u540e\u679c\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u8d85\u7ea7\u5bf9\u9f50\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5e94\u5bf9ASI\u7684\u6311\u6218\u3002", "method": "\u8bba\u6587\u91cd\u65b0\u5b9a\u4e49\u4e86\u8d85\u7ea7\u5bf9\u9f50\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5916\u90e8\u76d1\u7763\uff08\u57fa\u4e8e\u4eba\u7c7b\u51b3\u7b56\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\uff09\u548c\u5185\u5728\u4e3b\u52a8\u5bf9\u9f50\uff08\u57fa\u4e8e\u81ea\u6211\u610f\u8bc6\u3001\u540c\u7406\u5fc3\u7b49\uff09\u7684\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u6574\u5408\u5916\u90e8\u76d1\u7763\u548c\u5185\u5728\u4e3b\u52a8\u5bf9\u9f50\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6301\u7eed\u7684\u5171\u751f\u793e\u4f1a\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u4eba\u7c7b\u4e0eAI\u7684\u5171\u540c\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u73b0\u5b89\u5168\u4e14\u6709\u76ca\u7684AGI\u548cASI\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u4fc3\u8fdb\u4e86\u4eba\u7c7b\u4e0eAI\u7684\u5171\u751f\u5173\u7cfb\u3002"}}
{"id": "2504.17229", "pdf": "https://arxiv.org/pdf/2504.17229", "abs": "https://arxiv.org/abs/2504.17229", "authors": ["Akihiro Kuwabara", "Sorachi Kato", "Takuya Fujihashi", "Toshiaki Koike-Akino", "Takashi Watanabe"], "title": "Range Image-Based Implicit Neural Compression for LiDAR Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel scheme to efficiently compress Light Detection\nand Ranging~(LiDAR) point clouds, enabling high-precision 3D scene archives,\nand such archives pave the way for a detailed understanding of the\ncorresponding 3D scenes. We focus on 2D range images~(RIs) as a lightweight\nformat for representing 3D LiDAR observations. Although conventional image\ncompression techniques can be adapted to improve compression efficiency for\nRIs, their practical performance is expected to be limited due to differences\nin bit precision and the distinct pixel value distribution characteristics\nbetween natural images and RIs. We propose a novel implicit neural\nrepresentation~(INR)--based RI compression method that effectively handles\nfloating-point valued pixels. The proposed method divides RIs into depth and\nmask images and compresses them using patch-wise and pixel-wise INR\narchitectures with model pruning and quantization, respectively. Experiments on\nthe KITTI dataset show that the proposed method outperforms existing image,\npoint cloud, RI, and INR-based compression methods in terms of 3D\nreconstruction and detection quality at low bitrates and decoding latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684LiDAR\u70b9\u4e91\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u548c\u63a9\u7801\u56fe\u50cf\u7684\u5206\u5272\u4e0e\u538b\u7f29\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6bd4\u7279\u7387\u4e0b\u76843D\u91cd\u5efa\u548c\u68c0\u6d4b\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u538b\u7f29\u6280\u672f\u5728\u5904\u7406LiDAR\u76842D\u8303\u56f4\u56fe\u50cf\uff08RIs\uff09\u65f6\u6548\u7387\u6709\u9650\uff0c\u56e0\u5176\u4e0e\u81ea\u7136\u56fe\u50cf\u5728\u6bd4\u7279\u7cbe\u5ea6\u548c\u50cf\u7d20\u503c\u5206\u5e03\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "method": "\u5c06RIs\u5206\u5272\u4e3a\u6df1\u5ea6\u548c\u63a9\u7801\u56fe\u50cf\uff0c\u5206\u522b\u91c7\u7528\u57fa\u4e8eINR\u7684\u5757\u7ea7\u548c\u50cf\u7d20\u7ea7\u67b6\u6784\uff0c\u7ed3\u5408\u6a21\u578b\u526a\u679d\u548c\u91cf\u5316\u8fdb\u884c\u538b\u7f29\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u548c\u89e3\u7801\u5ef6\u8fdf\u4e0b\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u3001\u70b9\u4e91\u3001RI\u548cINR\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684INR-based RI\u538b\u7f29\u65b9\u6cd5\u4e3a\u9ad8\u65483D\u573a\u666f\u5b58\u6863\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6027\u80fd\u3002"}}
{"id": "2504.17531", "pdf": "https://arxiv.org/pdf/2504.17531", "abs": "https://arxiv.org/abs/2504.17531", "authors": ["Justus Flerlage", "Ilja Behnke", "Odej Kao"], "title": "Towards Machine-Generated Code for the Resolution of User Intentions", "categories": ["cs.AI"], "comment": null, "summary": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528AI\uff08\u7279\u522b\u662fLLMs\uff09\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u5b9e\u73b0\u7528\u6237\u610f\u56fe\u89e3\u6790\u7684\u53ef\u884c\u6027\uff0c\u5c55\u793a\u4e86GPT-4o-mini\u5728\u751f\u6210\u5de5\u4f5c\u6d41\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\uff08\u5c24\u5176\u662fLLMs\uff09\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u4e0e\u8bbe\u5907\u7684\u4ea4\u4e92\u65b9\u5f0f\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u3002\u4f20\u7edf\u7684\u9ad8\u5c42\u5e94\u7528\u53ef\u80fd\u88abAI\u751f\u6210\u7684\u4ee3\u7801\u5de5\u4f5c\u6d41\u53d6\u4ee3\uff0c\u5b9e\u73b0\u66f4\u76f4\u63a5\u7684\u610f\u56fe\u89e3\u6790\u3002", "method": "\u901a\u8fc7\u5411LLM\uff08\u5982GPT-4o-mini\uff09\u63d0\u4f9b\u5177\u4f53\u7528\u6237\u610f\u56fe\u548c\u7b80\u5316\u7684API\uff0c\u751f\u6210\u5e76\u6267\u884c\u4ee3\u7801\u5de5\u4f5c\u6d41\uff0c\u5206\u6790\u5176\u53ef\u884c\u6027\u548c\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8be5\u65b9\u6cd5\u603b\u4f53\u53ef\u884c\uff0c\u4e14GPT-4o-mini\u5728\u751f\u6210\u7b26\u5408\u7528\u6237\u610f\u56fe\u7684\u4ee3\u7801\u5de5\u4f5c\u6d41\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "AI\u751f\u6210\u7684\u4ee3\u7801\u5de5\u4f5c\u6d41\u6709\u671b\u6210\u4e3a\u7528\u6237\u4e0e\u8bbe\u5907\u4ea4\u4e92\u7684\u65b0\u8303\u5f0f\uff0c\u7ed3\u5408\u4eba\u7c7b\u610f\u56fe\u5b9a\u4e49\u4e0eAI\u5b9e\u73b0\uff0c\u63a8\u52a8\u6df7\u5408\u5de5\u4f5c\u6d41\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.17234", "pdf": "https://arxiv.org/pdf/2504.17234", "abs": "https://arxiv.org/abs/2504.17234", "authors": ["Zhiqiang Lao", "Heather Yu"], "title": "Scene Perceived Image Perceptual Score (SPIPS): combining global and local perception for image quality assessment", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of artificial intelligence and widespread use of\nsmartphones have resulted in an exponential growth of image data, both real\n(camera-captured) and virtual (AI-generated). This surge underscores the\ncritical need for robust image quality assessment (IQA) methods that accurately\nreflect human visual perception. Traditional IQA techniques primarily rely on\nspatial features - such as signal-to-noise ratio, local structural distortions,\nand texture inconsistencies - to identify artifacts. While effective for\nunprocessed or conventionally altered images, these methods fall short in the\ncontext of modern image post-processing powered by deep neural networks (DNNs).\nThe rise of DNN-based models for image generation, enhancement, and restoration\nhas significantly improved visual quality, yet made accurate assessment\nincreasingly complex. To address this, we propose a novel IQA approach that\nbridges the gap between deep learning methods and human perception. Our model\ndisentangles deep features into high-level semantic information and low-level\nperceptual details, treating each stream separately. These features are then\ncombined with conventional IQA metrics to provide a more comprehensive\nevaluation framework. This hybrid design enables the model to assess both\nglobal context and intricate image details, better reflecting the human visual\nprocess, which first interprets overall structure before attending to\nfine-grained elements. The final stage employs a multilayer perceptron (MLP) to\nmap the integrated features into a concise quality score. Experimental results\ndemonstrate that our method achieves improved consistency with human perceptual\njudgments compared to existing IQA models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u4f20\u7edf\u65b9\u6cd5\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u9ad8\u3001\u4f4e\u5c42\u7279\u5f81\u5e76\u6574\u5408\u4f20\u7edf\u6307\u6807\uff0c\u66f4\u8d34\u5408\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u3002", "motivation": "\u968f\u7740AI\u548c\u667a\u80fd\u624b\u673a\u7684\u666e\u53ca\uff0c\u56fe\u50cf\u6570\u636e\u6fc0\u589e\uff0c\u4f20\u7edfIQA\u65b9\u6cd5\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5904\u7406\u7684\u56fe\u50cf\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u66f4\u8d34\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5c06\u6df1\u5ea6\u7279\u5f81\u89e3\u8026\u4e3a\u9ad8\u5c42\u8bed\u4e49\u548c\u4f4e\u5c42\u611f\u77e5\u7ec6\u8282\uff0c\u5206\u522b\u5904\u7406\u5e76\u4e0e\u4f20\u7edfIQA\u6307\u6807\u7ed3\u5408\uff0c\u6700\u540e\u901a\u8fc7\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u751f\u6210\u8d28\u91cf\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709IQA\u6a21\u578b\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u5224\u65ad\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e0e\u4f20\u7edfIQA\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\u3002"}}
{"id": "2504.17544", "pdf": "https://arxiv.org/pdf/2504.17544", "abs": "https://arxiv.org/abs/2504.17544", "authors": ["W. Russell Neuman", "Chad Coleman", "Ali Dasdan", "Safinah Ali", "Manan Shah"], "title": "Auditing the Ethical Logic of Generative AI Models", "categories": ["cs.AI"], "comment": null, "summary": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e94\u7ef4\u5ba1\u8ba1\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4f26\u7406\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4f26\u7406\u51b3\u7b56\u4e0a\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u5728\u89e3\u91ca\u4e25\u8c28\u6027\u548c\u9053\u5fb7\u4f18\u5148\u7ea7\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4f30\u5176\u4f26\u7406\u63a8\u7406\u80fd\u529b\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002", "method": "\u91c7\u7528\u4e94\u7ef4\u5ba1\u8ba1\u6a21\u578b\uff08\u5206\u6790\u8d28\u91cf\u3001\u4f26\u7406\u8003\u8651\u5e7f\u5ea6\u3001\u89e3\u91ca\u6df1\u5ea6\u3001\u4e00\u81f4\u6027\u548c\u51b3\u65ad\u529b\uff09\uff0c\u7ed3\u5408\u591a\u7ec4\u63d0\u793a\uff08\u5305\u62ec\u65b0\u9896\u7684\u4f26\u7406\u56f0\u5883\uff09\u6765\u8bc4\u4f30LLMs\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u6a21\u578b\u5728\u4f26\u7406\u51b3\u7b56\u4e0a\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u89e3\u91ca\u4e25\u8c28\u6027\u548c\u9053\u5fb7\u4f18\u5148\u7ea7\u5b58\u5728\u5dee\u5f02\uff1b\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u548c\u63a8\u7406\u4f18\u5316\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aAI\u7cfb\u7edf\u7684\u4f26\u7406\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86AI\u5728\u590d\u6742\u51b3\u7b56\u4e2d\u8f85\u52a9\u4eba\u7c7b\u9053\u5fb7\u63a8\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.17253", "pdf": "https://arxiv.org/pdf/2504.17253", "abs": "https://arxiv.org/abs/2504.17253", "authors": ["Yinqi Li", "Hong Chang", "Ruibing Hou", "Shiguang Shan", "Xilin Chen"], "title": "DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "Diffusion models have shown remarkable progress in various generative tasks\nsuch as image and video generation. This paper studies the problem of\nleveraging pretrained diffusion models for performing discriminative tasks.\nSpecifically, we extend the discriminative capability of pretrained frozen\ngenerative diffusion models from the classification task to the more complex\nobject detection task, by \"inverting\" a pretrained layout-to-image diffusion\nmodel. To this end, a gradient-based discrete optimization approach for\nreplacing the heavy prediction enumeration process, and a prior distribution\nmodel for making more accurate use of the Bayes' rule, are proposed\nrespectively. Empirical results show that this method is on par with basic\ndiscriminative object detection baselines on COCO dataset. In addition, our\nmethod can greatly speed up the previous diffusion-based method for\nclassification without sacrificing accuracy. Code and models are available at\nhttps://github.com/LiYinqi/DIVE .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5224\u522b\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u4ece\u5206\u7c7b\u4efb\u52a1\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u7269\u4f53\u68c0\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u4f18\u5316\u548c\u8d1d\u53f6\u65af\u89c4\u5219\u6539\u8fdb\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u6267\u884c\u5224\u522b\u4efb\u52a1\uff0c\u7279\u522b\u662f\u7269\u4f53\u68c0\u6d4b\uff0c\u4ee5\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u91c7\u7528\u68af\u5ea6\u79bb\u6563\u4f18\u5316\u65b9\u6cd5\u66ff\u4ee3\u7e41\u91cd\u7684\u9884\u6d4b\u679a\u4e3e\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u5148\u9a8c\u5206\u5e03\u6a21\u578b\u4ee5\u66f4\u51c6\u786e\u5730\u5e94\u7528\u8d1d\u53f6\u65af\u89c4\u5219\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4e0e\u57fa\u7840\u5224\u522b\u7269\u4f53\u68c0\u6d4b\u57fa\u7ebf\u76f8\u5f53\uff0c\u4e14\u663e\u8457\u52a0\u901f\u4e86\u4e4b\u524d\u57fa\u4e8e\u6269\u6563\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u7269\u4f53\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2504.16940", "pdf": "https://arxiv.org/pdf/2504.16940", "abs": "https://arxiv.org/abs/2504.16940", "authors": ["Drew Linsley", "Pinyuan Feng", "Thomas Serre"], "title": "Can deep neural networks learn biological vision?", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) once showed increasing alignment with primate\nneural responses as they improved on computer vision benchmarks. This trend\nraised the exciting possibility that better models of biological vision would\ncome as a byproduct of the deep learning revolution in artificial intelligence.\nHowever, the trend has reversed over recent years as DNNs have scaled to human\nor superhuman recognition accuracy, a divergence that may stem from modern DNNs\nlearning to rely on different visual features than primates to solve tasks.\nWhere will better computational models of biological vision come from? We\npropose that vision science must break from artificial intelligence to develop\nalgorithms that are designed with biological visual systems in mind instead of\ninternet data benchmarks. We predict that the next generation of deep learning\nmodels of biological vision will be trained with data diets, training routines,\nand objectives that are closer to those that shape human vision than those that\nare in use today.", "AI": {"tldr": "DNNs\u66fe\u4e0e\u7075\u957f\u7c7b\u795e\u7ecf\u53cd\u5e94\u66f4\u4e00\u81f4\uff0c\u4f46\u8fd1\u5e74\u8d8b\u52bf\u9006\u8f6c\uff0c\u53ef\u80fd\u56e0\u73b0\u4ee3DNN\u4f9d\u8d56\u4e0d\u540c\u89c6\u89c9\u7279\u5f81\u3002\u672a\u6765\u751f\u7269\u89c6\u89c9\u6a21\u578b\u9700\u8131\u79bbAI\uff0c\u8bbe\u8ba1\u66f4\u63a5\u8fd1\u4eba\u7c7b\u89c6\u89c9\u7684\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8DNNs\u4e0e\u7075\u957f\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u672a\u6765\u751f\u7269\u89c6\u89c9\u6a21\u578b\u9700\u66f4\u8d34\u8fd1\u4eba\u7c7b\u89c6\u89c9\u7684\u6784\u5efa\u65b9\u5f0f\u3002", "method": "\u5206\u6790DNNs\u4e0e\u7075\u957f\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5bf9\u6bd4\uff0c\u63d0\u51fa\u57fa\u4e8e\u751f\u7269\u89c6\u89c9\u7684\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u73b0\u4ee3DNNs\u4f9d\u8d56\u4e0e\u7075\u957f\u7c7b\u4e0d\u540c\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5bfc\u81f4\u6a21\u578b\u4e0e\u751f\u7269\u89c6\u89c9\u7684\u5dee\u5f02\u3002", "conclusion": "\u672a\u6765\u751f\u7269\u89c6\u89c9\u6a21\u578b\u9700\u8131\u79bbAI\u6846\u67b6\uff0c\u91c7\u7528\u66f4\u63a5\u8fd1\u4eba\u7c7b\u89c6\u89c9\u7684\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2504.17263", "pdf": "https://arxiv.org/pdf/2504.17263", "abs": "https://arxiv.org/abs/2504.17263", "authors": ["Wenqiang Zhou", "Zhendong Yu", "Xinyu Liu", "Jiaming Yang", "Rong Xiao", "Tao Wang", "Chenwei Tang", "Jiancheng Lv"], "title": "Precision Neural Network Quantization via Learnable Adaptive Modules", "categories": ["cs.CV", "cs.CC"], "comment": null, "summary": "Quantization Aware Training (QAT) is a neural network quantization technique\nthat compresses model size and improves operational efficiency while\neffectively maintaining model performance. The paradigm of QAT is to introduce\nfake quantization operators during the training process, allowing the model to\nautonomously compensate for information loss caused by quantization. Making\nquantization parameters trainable can significantly improve the performance of\nQAT, but at the cost of compromising the flexibility during inference,\nespecially when dealing with activation values with substantially different\ndistributions. In this paper, we propose an effective learnable adaptive neural\nnetwork quantization method, called Adaptive Step Size Quantization (ASQ), to\nresolve this conflict. Specifically, the proposed ASQ method first dynamically\nadjusts quantization scaling factors through a trained module capable of\naccommodating different activations. Then, to address the rigid resolution\nissue inherent in Power of Two (POT) quantization, we propose an efficient\nnon-uniform quantization scheme. We utilize the Power Of Square root of Two\n(POST) as the basis for exponential quantization, effectively handling the\nbell-shaped distribution of neural network weights across various bit-widths\nwhile maintaining computational efficiency through a Look-Up Table method\n(LUT). Extensive experimental results demonstrate that the proposed ASQ method\nis superior to the state-of-the-art QAT approaches. Notably that the ASQ is\neven competitive compared to full precision baselines, with its 4-bit quantized\nResNet34 model improving accuracy by 1.2\\% on ImageNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6b65\u957f\u91cf\u5316\u65b9\u6cd5\uff08ASQ\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91cf\u5316\u53c2\u6570\u548c\u975e\u5747\u5300\u91cf\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfQAT\u65b9\u6cd5\u5728\u91cf\u5316\u53c2\u6570\u53ef\u8bad\u7ec3\u65f6\u727a\u7272\u63a8\u7406\u7075\u6d3b\u6027\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5904\u7406\u5206\u5e03\u5dee\u5f02\u5927\u7684\u6fc0\u6d3b\u503c\u65f6\u3002", "method": "1. \u52a8\u6001\u8c03\u6574\u91cf\u5316\u7f29\u653e\u56e0\u5b50\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u5e73\u65b9\u6839\u4e8c\u7684\u6307\u6570\u91cf\u5316\u65b9\u6848\uff08POST\uff09\uff0c\u7ed3\u5408\u67e5\u627e\u8868\uff08LUT\uff09\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "result": "ASQ\u57284\u4f4d\u91cf\u5316ResNet34\u4e0a\u6bd4\u5168\u7cbe\u5ea6\u57fa\u7ebf\u63d0\u53471.2%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709QAT\u65b9\u6cd5\u3002", "conclusion": "ASQ\u901a\u8fc7\u81ea\u9002\u5e94\u91cf\u5316\u7b56\u7565\u548c\u975e\u5747\u5300\u91cf\u5316\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.16942", "pdf": "https://arxiv.org/pdf/2504.16942", "abs": "https://arxiv.org/abs/2504.16942", "authors": ["Shushman Choudhury", "Elad Aharoni", "Chandrakumari Suvarna", "Iveel Tsogsuren", "Abdul Rahman Kreidieh", "Chun-Ta Lu", "Neha Arora"], "title": "S2Vec: Self-Supervised Geospatial Embeddings", "categories": ["cs.SI", "cs.AI", "cs.CV"], "comment": "To be submitted to ACM Transactions on Spatial Algorithms and Systems", "summary": "Scalable general-purpose representations of the built environment are crucial\nfor geospatial artificial intelligence applications. This paper introduces\nS2Vec, a novel self-supervised framework for learning such geospatial\nembeddings. S2Vec uses the S2 Geometry library to partition large areas into\ndiscrete S2 cells, rasterizes built environment feature vectors within cells as\nimages, and applies masked autoencoding on these rasterized images to encode\nthe feature vectors. This approach yields task-agnostic embeddings that capture\nlocal feature characteristics and broader spatial relationships. We evaluate\nS2Vec on three large-scale socioeconomic prediction tasks, showing its\ncompetitive performance against state-of-the-art image-based embeddings. We\nalso explore the benefits of combining S2Vec embeddings with image-based\nembeddings downstream, showing that such multimodal fusion can often improve\nperformance. Our results highlight how S2Vec can learn effective\ngeneral-purpose geospatial representations and how it can complement other data\nmodalities in geospatial artificial intelligence.", "AI": {"tldr": "S2Vec\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u901a\u7528\u7684\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\uff0c\u901a\u8fc7S2\u51e0\u4f55\u5e93\u5206\u533a\u5e76\u6805\u683c\u5316\u7279\u5f81\u5411\u91cf\uff0c\u751f\u6210\u4efb\u52a1\u65e0\u5173\u7684\u5d4c\u5165\uff0c\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6784\u5efa\u53ef\u6269\u5c55\u7684\u901a\u7528\u5730\u7406\u7a7a\u95f4\u8868\u793a\u5bf9\u5730\u7406\u7a7a\u95f4\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528S2 Geometry\u5e93\u5206\u533a\uff0c\u6805\u683c\u5316\u7279\u5f81\u5411\u91cf\u4e3a\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u81ea\u7f16\u7801\u751f\u6210\u5d4c\u5165\u3002", "result": "\u5728\u4e09\u4e2a\u793e\u4f1a\u7ecf\u6d4e\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u56fe\u50cf\u5d4c\u5165\u7ed3\u5408\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "S2Vec\u80fd\u751f\u6210\u6709\u6548\u7684\u5730\u7406\u7a7a\u95f4\u8868\u793a\uff0c\u5e76\u4e0e\u5176\u4ed6\u6570\u636e\u6a21\u6001\u4e92\u8865\u3002"}}
{"id": "2504.17269", "pdf": "https://arxiv.org/pdf/2504.17269", "abs": "https://arxiv.org/abs/2504.17269", "authors": ["Yu Hong", "Xiao Cai", "Pengpeng Zeng", "Shuai Zhang", "Jingkuan Song", "Lianli Gao", "Heng Tao Shen"], "title": "Towards Generalized and Training-Free Text-Guided Semantic Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Text-guided semantic manipulation refers to semantically editing an image\ngenerated from a source prompt to match a target prompt, enabling the desired\nsemantic changes (e.g., addition, removal, and style transfer) while preserving\nirrelevant contents. With the powerful generative capabilities of the diffusion\nmodel, the task has shown the potential to generate high-fidelity visual\ncontent. Nevertheless, existing methods either typically require time-consuming\nfine-tuning (inefficient), fail to accomplish multiple semantic manipulations\n(poorly extensible), and/or lack support for different modality tasks (limited\ngeneralizability). Upon further investigation, we find that the geometric\nproperties of noises in the diffusion model are strongly correlated with the\nsemantic changes. Motivated by this, we propose a novel $\\textit{GTF}$ for\ntext-guided semantic manipulation, which has the following attractive\ncapabilities: 1) $\\textbf{Generalized}$: our $\\textit{GTF}$ supports multiple\nsemantic manipulations (e.g., addition, removal, and style transfer) and can be\nseamlessly integrated into all diffusion-based methods (i.e., Plug-and-play)\nacross different modalities (i.e., modality-agnostic); and 2)\n$\\textbf{Training-free}$: $\\textit{GTF}$ produces high-fidelity results via\nsimply controlling the geometric relationship between noises without tuning or\noptimization. Our extensive experiments demonstrate the efficacy of our\napproach, highlighting its potential to advance the state-of-the-art in\nsemantics manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTF\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u5f15\u5bfc\u7684\u8bed\u4e49\u56fe\u50cf\u7f16\u8f91\uff0c\u652f\u6301\u591a\u79cd\u8bed\u4e49\u64cd\u4f5c\u4e14\u65e0\u9700\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u3001\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u6269\u6563\u6a21\u578b\u4e2d\u566a\u58f0\u7684\u51e0\u4f55\u7279\u6027\u4e0e\u8bed\u4e49\u53d8\u5316\u5f3a\u76f8\u5173\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u566a\u58f0\u7684\u51e0\u4f55\u5173\u7cfb\u5b9e\u73b0\u8bed\u4e49\u7f16\u8f91\uff0c\u65e0\u9700\u8c03\u4f18\u6216\u4f18\u5316\u3002", "result": "GTF\u652f\u6301\u591a\u79cd\u8bed\u4e49\u64cd\u4f5c\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u4e0d\u540c\u6a21\u6001\u7684\u6269\u6563\u65b9\u6cd5\u4e2d\u3002", "conclusion": "GTF\u5728\u8bed\u4e49\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.16946", "pdf": "https://arxiv.org/pdf/2504.16946", "abs": "https://arxiv.org/abs/2504.16946", "authors": ["Xiaotong Ye", "Nicolas Bougie", "Toshihiko Yamasaki", "Narimasa Watanabe"], "title": "MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Generative agents offer promising capabilities for simulating realistic urban\nbehaviors. However, existing methods oversimplify transportation choices in\nmodern cities, and require prohibitive computational resources for large-scale\npopulation simulation. To address these limitations, we first present a virtual\ncity that features multiple functional buildings and transportation modes.\nThen, we conduct extensive surveys to model behavioral choices and mobility\npreferences among population groups. Building on these insights, we introduce a\nsimulation framework that captures the complexity of urban mobility while\nremaining scalable, enabling the simulation of over 4,000 agents. To assess the\nrealism of the generated behaviors, we perform a series of micro and\nmacro-level analyses. Beyond mere performance comparison, we explore insightful\nexperiments, such as predicting crowd density from movement patterns and\nidentifying trends in vehicle preferences across agent demographics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u57ce\u5e02\u79fb\u52a8\u6a21\u62df\u6846\u67b6\uff0c\u652f\u6301\u5927\u89c4\u6a21\u4eba\u53e3\u884c\u4e3a\u4eff\u771f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u7b80\u5316\u4e86\u73b0\u4ee3\u57ce\u5e02\u4e2d\u7684\u4ea4\u901a\u9009\u62e9\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u3002", "method": "\u6784\u5efa\u865a\u62df\u57ce\u5e02\u5e76\u8c03\u67e5\u884c\u4e3a\u9009\u62e9\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u4eff\u771f\u6846\u67b6\u3002", "result": "\u5b9e\u73b0\u4e864000\u591a\u4e2a\u4ee3\u7406\u7684\u4eff\u771f\uff0c\u5e76\u901a\u8fc7\u5fae\u89c2\u548c\u5b8f\u89c2\u5206\u6790\u9a8c\u8bc1\u4e86\u884c\u4e3a\u771f\u5b9e\u6027\u3002", "conclusion": "\u6846\u67b6\u80fd\u6709\u6548\u6a21\u62df\u590d\u6742\u57ce\u5e02\u79fb\u52a8\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2504.17280", "pdf": "https://arxiv.org/pdf/2504.17280", "abs": "https://arxiv.org/abs/2504.17280", "authors": ["Haodi Yao", "Fenghua He", "Ning Hao", "Chen Xie"], "title": "EdgePoint2: Compact Descriptors for Superior Efficiency and Accuracy", "categories": ["cs.CV"], "comment": null, "summary": "The field of keypoint extraction, which is essential for vision applications\nlike Structure from Motion (SfM) and Simultaneous Localization and Mapping\n(SLAM), has evolved from relying on handcrafted methods to leveraging deep\nlearning techniques. While deep learning approaches have significantly improved\nperformance, they often incur substantial computational costs, limiting their\ndeployment in real-time edge applications. Efforts to create lightweight neural\nnetworks have seen some success, yet they often result in trade-offs between\nefficiency and accuracy. Additionally, the high-dimensional descriptors\ngenerated by these networks poses challenges for distributed applications\nrequiring efficient communication and coordination, highlighting the need for\ncompact yet competitively accurate descriptors. In this paper, we present\nEdgePoint2, a series of lightweight keypoint detection and description neural\nnetworks specifically tailored for edge computing applications on embedded\nsystem. The network architecture is optimized for efficiency without\nsacrificing accuracy. To train compact descriptors, we introduce a combination\nof Orthogonal Procrustes loss and similarity loss, which can serve as a general\napproach for hypersphere embedding distillation tasks. Additionally, we offer\n14 sub-models to satisfy diverse application requirements. Our experiments\ndemonstrate that EdgePoint2 consistently achieves state-of-the-art (SOTA)\naccuracy and efficiency across various challenging scenarios while employing\nlower-dimensional descriptors (32/48/64). Beyond its accuracy, EdgePoint2\noffers significant advantages in flexibility, robustness, and versatility.\nConsequently, EdgePoint2 emerges as a highly competitive option for visual\ntasks, especially in contexts demanding adaptability to diverse computational\nand communication constraints.", "AI": {"tldr": "EdgePoint2\u662f\u4e00\u7cfb\u5217\u8f7b\u91cf\u7ea7\u5173\u952e\u70b9\u68c0\u6d4b\u548c\u63cf\u8ff0\u795e\u7ecf\u7f51\u7edc\uff0c\u4e13\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5173\u952e\u70b9\u63d0\u53d6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u9ad8\u7ef4\u63cf\u8ff0\u7b26\u5728\u5206\u5e03\u5f0f\u5e94\u7528\u4e2d\u6548\u7387\u4f4e\uff0c\u9700\u8981\u7d27\u51d1\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faEdgePoint2\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6b63\u4ea4Procrustes\u635f\u5931\u548c\u76f8\u4f3c\u6027\u635f\u5931\u8bad\u7ec3\u7d27\u51d1\u63cf\u8ff0\u7b26\uff0c\u5e76\u63d0\u4f9b14\u4e2a\u5b50\u6a21\u578b\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793aEdgePoint2\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5747\u8fbe\u5230SOTA\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u4e14\u4f7f\u7528\u4f4e\u7ef4\u63cf\u8ff0\u7b26\uff0832/48/64\uff09\u3002", "conclusion": "EdgePoint2\u5728\u7075\u6d3b\u6027\u3001\u9c81\u68d2\u6027\u548c\u591a\u529f\u80fd\u6027\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u662f\u9002\u5e94\u591a\u6837\u5316\u8ba1\u7b97\u548c\u901a\u4fe1\u7ea6\u675f\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2504.16947", "pdf": "https://arxiv.org/pdf/2504.16947", "abs": "https://arxiv.org/abs/2504.16947", "authors": ["Dachun Sun", "You Lyu", "Jinning Li", "Yizhuo Chen", "Tianshi Wang", "Tomoyoshi Kimura", "Tarek Abdelzaher"], "title": "SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "This paper introduces SCRAG, a prediction framework inspired by social\ncomputing, designed to forecast community responses to real or hypothetical\nsocial media posts. SCRAG can be used by public relations specialists (e.g., to\ncraft messaging in ways that avoid unintended misinterpretations) or public\nfigures and influencers (e.g., to anticipate social responses), among other\napplications related to public sentiment prediction, crisis management, and\nsocial what-if analysis. While large language models (LLMs) have achieved\nremarkable success in generating coherent and contextually rich text, their\nreliance on static training data and susceptibility to hallucinations limit\ntheir effectiveness at response forecasting in dynamic social media\nenvironments. SCRAG overcomes these challenges by integrating LLMs with a\nRetrieval-Augmented Generation (RAG) technique rooted in social computing.\nSpecifically, our framework retrieves (i) historical responses from the target\ncommunity to capture their ideological, semantic, and emotional makeup, and\n(ii) external knowledge from sources such as news articles to inject\ntime-sensitive context. This information is then jointly used to forecast the\nresponses of the target community to new posts or narratives. Extensive\nexperiments across six scenarios on the X platform (formerly Twitter), tested\nwith various embedding models and LLMs, demonstrate over 10% improvements on\naverage in key evaluation metrics. A concrete example further shows its\neffectiveness in capturing diverse ideologies and nuances. Our work provides a\nsocial computing tool for applications where accurate and concrete insights\ninto community responses are crucial.", "AI": {"tldr": "SCRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u793e\u4ea4\u8ba1\u7b97\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e0a\u793e\u533a\u5bf9\u5e16\u5b50\u7684\u53cd\u5e94\uff0c\u7ed3\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f9d\u8d56\u9759\u6001\u6570\u636e\u4e14\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u793e\u4ea4\u5a92\u4f53\u73af\u5883\u3002SCRAG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u793e\u533a\u53cd\u5e94\u9884\u6d4b\u3002", "method": "SCRAG\u6574\u5408LLMs\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u68c0\u7d22\u76ee\u6807\u793e\u533a\u7684\u5386\u53f2\u53cd\u5e94\u548c\u5916\u90e8\u77e5\u8bc6\uff08\u5982\u65b0\u95fb\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u65b0\u5e16\u5b50\u7684\u793e\u533a\u53cd\u5e94\u3002", "result": "\u5728X\u5e73\u53f0\uff08\u539fTwitter\uff09\u7684\u516d\u79cd\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0cSCRAG\u5728\u5173\u952e\u8bc4\u4f30\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u5347\u8d85\u8fc710%\uff0c\u5e76\u80fd\u6355\u6349\u591a\u6837\u5316\u7684\u610f\u8bc6\u5f62\u6001\u548c\u7ec6\u5fae\u5dee\u522b\u3002", "conclusion": "SCRAG\u4e3a\u9700\u8981\u51c6\u786e\u9884\u6d4b\u793e\u533a\u53cd\u5e94\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u793e\u4ea4\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2504.17306", "pdf": "https://arxiv.org/pdf/2504.17306", "abs": "https://arxiv.org/abs/2504.17306", "authors": ["Meher Boulaabi", "Takwa Ben A\u00efcha Gader", "Afef Kacem Echi", "Sameh Mbarek"], "title": "Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+", "categories": ["cs.CV", "cs.AI"], "comment": "This work was accepted at the ACS/IEEE International Conference on\n  Computer Systems and Applications (AICCSA) 2024", "summary": "To improve the segmentation of diabetic retinopathy lesions (microaneurysms,\nhemorrhages, exudates, and soft exudates), we implemented a binary segmentation\nmethod specific to each type of lesion. As post-segmentation, we combined the\nindividual model outputs into a single image to better analyze the lesion\ntypes. This approach facilitated parameter optimization and improved accuracy,\neffectively overcoming challenges related to dataset limitations and annotation\ncomplexity. Specific preprocessing steps included cropping and applying\ncontrast-limited adaptive histogram equalization to the L channel of the LAB\nimage. Additionally, we employed targeted data augmentation techniques to\nfurther refine the model's efficacy. Our methodology utilized the DeepLabv3+\nmodel, achieving a segmentation accuracy of 99%. These findings highlight the\nefficacy of innovative strategies in advancing medical image analysis,\nparticularly in the precise segmentation of diabetic retinopathy lesions. The\nIDRID dataset was utilized to validate and demonstrate the robustness of our\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u75c5\u53d8\u7684\u4e8c\u5143\u5206\u5272\u65b9\u6cd5\uff0c\u7ed3\u5408\u540e\u5904\u7406\u6b65\u9aa4\u63d0\u5347\u51c6\u786e\u6027\uff0c\u6700\u7ec8\u8fbe\u523099%\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u6539\u5584\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u75c5\u53d8\uff08\u5982\u5fae\u52a8\u8109\u7624\u3001\u51fa\u8840\u3001\u6e17\u51fa\u7269\u7b49\uff09\u7684\u5206\u5272\u6548\u679c\uff0c\u514b\u670d\u6570\u636e\u96c6\u9650\u5236\u548c\u6807\u6ce8\u590d\u6742\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528DeepLabv3+\u6a21\u578b\uff0c\u7ed3\u5408\u7279\u5b9a\u9884\u5904\u7406\uff08\u88c1\u526a\u548cCLAHE\uff09\u53ca\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5bf9\u6bcf\u79cd\u75c5\u53d8\u7c7b\u578b\u8fdb\u884c\u4e8c\u5143\u5206\u5272\uff0c\u540e\u5904\u7406\u5408\u5e76\u7ed3\u679c\u3002", "result": "\u5728IDRID\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5206\u5272\u7cbe\u5ea6\u8fbe\u523099%\u3002", "conclusion": "\u521b\u65b0\u7b56\u7565\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u75c5\u53d8\u7684\u7cbe\u786e\u5206\u5272\u4e0a\u3002"}}
{"id": "2504.16948", "pdf": "https://arxiv.org/pdf/2504.16948", "abs": "https://arxiv.org/abs/2504.16948", "authors": ["Zhen Tan", "Huan Liu"], "title": "Intrinsic Barriers to Explaining Deep Foundation Models", "categories": ["cs.CY", "cs.AI", "cs.ET"], "comment": null, "summary": "Deep Foundation Models (DFMs) offer unprecedented capabilities but their\nincreasing complexity presents profound challenges to understanding their\ninternal workings-a critical need for ensuring trust, safety, and\naccountability. As we grapple with explaining these systems, a fundamental\nquestion emerges: Are the difficulties we face merely temporary hurdles,\nawaiting more sophisticated analytical techniques, or do they stem from\n\\emph{intrinsic barriers} deeply rooted in the nature of these large-scale\nmodels themselves? This paper delves into this critical question by examining\nthe fundamental characteristics of DFMs and scrutinizing the limitations\nencountered by current explainability methods when confronted with this\ninherent challenge. We probe the feasibility of achieving satisfactory\nexplanations and consider the implications for how we must approach the\nverification and governance of these powerful technologies.", "AI": {"tldr": "\u63a2\u8ba8\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\uff08DFMs\uff09\u662f\u5426\u56e0\u5185\u5728\u969c\u788d\u800c\u96be\u4ee5\u89e3\u91ca\uff0c\u5e76\u5206\u6790\u5f53\u524d\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u53ca\u5176\u5bf9\u6280\u672f\u9a8c\u8bc1\u548c\u6cbb\u7406\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740DFMs\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u7406\u89e3\u5176\u5185\u90e8\u8fd0\u4f5c\u6210\u4e3a\u786e\u4fdd\u4fe1\u4efb\u3001\u5b89\u5168\u548c\u8d23\u4efb\u7684\u5173\u952e\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u89e3\u91ca\u8fd9\u4e9b\u6a21\u578b\u7684\u56f0\u96be\u662f\u6682\u65f6\u7684\u8fd8\u662f\u5185\u5728\u7684\u3002", "method": "\u901a\u8fc7\u5206\u6790DFMs\u7684\u57fa\u672c\u7279\u6027\u548c\u5f53\u524d\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63a2\u8ba8\u5176\u89e3\u91ca\u7684\u53ef\u884c\u6027\u3002", "result": "\u6307\u51faDFMs\u7684\u89e3\u91ca\u56f0\u96be\u53ef\u80fd\u6e90\u4e8e\u5176\u5185\u5728\u7279\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9DFMs\u7684\u89e3\u91ca\u6311\u6218\uff0c\u4ee5\u786e\u4fdd\u5176\u53ef\u4fe1\u5ea6\u548c\u6cbb\u7406\u3002"}}
{"id": "2504.17315", "pdf": "https://arxiv.org/pdf/2504.17315", "abs": "https://arxiv.org/abs/2504.17315", "authors": ["Zhanglin Wu", "Tengfei Song", "Ning Xie", "Weidong Zhang", "Pengfei Li", "Shuang Wu", "Chong Li", "Junhao Zhu", "Hao Yang"], "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 1 figures, 2 tables", "summary": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation.", "AI": {"tldr": "\u534e\u4e3a\u7ffb\u8bd1\u670d\u52a1\u4e2d\u5fc3\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u611f\u77e5\u94fe\u5f0f\u601d\u7ef4\u7684\u7efc\u5408\u7aef\u5230\u7aef\u6587\u6863\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u7ed3\u5408\u6700\u5c0f\u8d1d\u53f6\u65af\u89e3\u7801\u548c\u540e\u5904\u7406\u7b56\u7565\u63d0\u5347\u7ffb\u8bd1\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u5e03\u5c40\u6587\u6863\u56fe\u50cf\u7684\u7aef\u5230\u7aef\u673a\u5668\u7ffb\u8bd1\u95ee\u9898\uff0c\u7edf\u4e00\u5904\u7406OCR\u548c\u65e0OCR\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u611f\u77e5\u94fe\u5f0f\u601d\u7ef4\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u7528\u6700\u5c0f\u8d1d\u53f6\u65af\u89e3\u7801\u548c\u540e\u5904\u7406\u7b56\u7565\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5c55\u793a\u4e86\u6709\u6548\u7684\u6587\u6863\u56fe\u50cf\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u5728\u590d\u6742\u5e03\u5c40\u6587\u6863\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16968", "pdf": "https://arxiv.org/pdf/2504.16968", "abs": "https://arxiv.org/abs/2504.16968", "authors": ["Jun Wu", "Jiangtao Wen", "Yuxing Han"], "title": "Backslash: Rate Constrained Optimized Training of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (Backslash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). Backslash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that Backslash can\nreduce memory usage by 60\\% - 90\\% without accuracy loss and provides\nsignificant compression gain compared to compression after training. Moreover,\nBackslash proves to be highly versatile: it enhances generalization with small\nLagrange multipliers, improves model robustness to pruning (maintaining\naccuracy even at 80\\% pruning rates), and enables network simplification for\naccelerated inference on edge devices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBackslash\u7684\u8bad\u7ec3\u65f6\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u7387\u5931\u771f\u4f18\u5316\u5b9e\u73b0\u6a21\u578b\u7cbe\u5ea6\u4e0e\u590d\u6742\u5ea6\u7684\u7075\u6d3b\u6743\u8861\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u5197\u4f59\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u5728\u4e0d\u635f\u5931\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u51cf\u5c1160%-90%\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53c2\u6570\u538b\u7f29\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8bad\u7ec3\u540e\u9636\u6bb5\uff0c\u800c\u8bad\u7ec3\u9636\u6bb5\u7684\u538b\u7f29\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faRate-Constrained Training\uff08Backslash\uff09\uff0c\u57fa\u4e8e\u7387\u5931\u771f\u4f18\u5316\uff08RDO\uff09\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u6a21\u578b\u538b\u7f29\u3002", "result": "Backslash\u5728\u591a\u79cd\u67b6\u6784\u548c\u4efb\u52a1\u4e2d\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0860%-90%\uff09\uff0c\u4e14\u4e0d\u635f\u5931\u7cbe\u5ea6\uff1b\u540c\u65f6\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3001\u6a21\u578b\u9c81\u68d2\u6027\u548c\u63a8\u7406\u6548\u7387\u3002", "conclusion": "Backslash\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u591a\u529f\u80fd\u7684\u8bad\u7ec3\u65f6\u538b\u7f29\u65b9\u6cd5\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.17343", "pdf": "https://arxiv.org/pdf/2504.17343", "abs": "https://arxiv.org/abs/2504.17343", "authors": ["Linli Yao", "Yicheng Li", "Yuancheng Wei", "Lei Li", "Shuhuai Ren", "Yuanxin Liu", "Kun Ouyang", "Lean Wang", "Shicheng Li", "Sida Li", "Lingpeng Kong", "Qi Liu", "Yuanxing Zhang", "Xu Sun"], "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.", "AI": {"tldr": "TimeChat-Online\u662f\u4e00\u79cd\u65b0\u578b\u5728\u7ebf\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5dee\u5206\u4ee4\u724c\u4e22\u5f03\uff08DTD\uff09\u6a21\u5757\u663e\u8457\u51cf\u5c11\u89c6\u9891\u4ee4\u724c\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5b9e\u65f6\u89c6\u9891\u6d41\u5904\u7406\u7684\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u5728\u7ebf\u89c6\u9891\u5e73\u53f0\u7684\u5feb\u901f\u589e\u957f\uff0c\u7279\u522b\u662f\u76f4\u64ad\u670d\u52a1\uff0c\u5bf9\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u7cfb\u7edf\u63d0\u51fa\u4e86\u8feb\u5207\u9700\u6c42\uff0c\u800c\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6d41\u5a92\u4f53\u65f6\u5b58\u5728\u5197\u4f59\u5e27\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "TimeChat-Online\u91c7\u7528\u5dee\u5206\u4ee4\u724c\u4e22\u5f03\uff08DTD\uff09\u6a21\u5757\uff0c\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u7684\u53d8\u5316\u76f2\u89c6\u73b0\u8c61\uff0c\u4fdd\u7559\u6709\u610f\u4e49\u7684\u65f6\u95f4\u53d8\u5316\uff0c\u8fc7\u6ee4\u9759\u6001\u5197\u4f59\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDTD\u51cf\u5c11\u4e8682.8%\u7684\u89c6\u9891\u4ee4\u724c\uff0c\u540c\u65f6\u4fdd\u630198%\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u6d41\u5a92\u4f53\u89c6\u9891\u4e2d80%\u4ee5\u4e0a\u7684\u5185\u5bb9\u662f\u5197\u4f59\u7684\u3002", "conclusion": "TimeChat-Online\u5728\u6d41\u5a92\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u957f\u89c6\u9891\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u65f6\u89c6\u9891\u4ea4\u4e92\u4e2d\u7684\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2504.16972", "pdf": "https://arxiv.org/pdf/2504.16972", "abs": "https://arxiv.org/abs/2504.16972", "authors": ["Hossein Ahmadi", "Sajjad Emdadi Mahdimahalleh", "Arman Farahat", "Banafsheh Saffari"], "title": "Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "The rapid growth of unlabeled time-series data in domains such as wireless\ncommunications, radar, biomedical engineering, and the Internet of Things (IoT)\nhas driven advancements in unsupervised learning. This review synthesizes\nrecent progress in applying autoencoders and vision transformers for\nunsupervised signal analysis, focusing on their architectures, applications,\nand emerging trends. We explore how these models enable feature extraction,\nanomaly detection, and classification across diverse signal types, including\nelectrocardiograms, radar waveforms, and IoT sensor data. The review highlights\nthe strengths of hybrid architectures and self-supervised learning, while\nidentifying challenges in interpretability, scalability, and domain\ngeneralization. By bridging methodological innovations and practical\napplications, this work offers a roadmap for developing robust, adaptive models\nfor signal intelligence.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u7f16\u7801\u5668\u548c\u89c6\u89c9\u53d8\u6362\u5668\u5728\u65e0\u76d1\u7763\u4fe1\u53f7\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u67b6\u6784\u3001\u5e94\u7528\u53ca\u8d8b\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9886\u57df\u6cdb\u5316\u7b49\u6311\u6218\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u901a\u4fe1\u3001\u96f7\u8fbe\u3001\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u548c\u7269\u8054\u7f51\u7b49\u9886\u57df\u4e2d\u672a\u6807\u8bb0\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\uff0c\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u76f8\u5173\u6280\u672f\u7684\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u5206\u6790\u81ea\u7f16\u7801\u5668\u548c\u89c6\u89c9\u53d8\u6362\u5668\u7684\u67b6\u6784\u53ca\u5176\u5728\u7279\u5f81\u63d0\u53d6\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408\u6df7\u5408\u67b6\u6784\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u4f18\u52bf\u3002", "result": "\u7efc\u8ff0\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u79cd\u4fe1\u53f7\u7c7b\u578b\uff08\u5982\u5fc3\u7535\u56fe\u3001\u96f7\u8fbe\u6ce2\u5f62\u548c\u7269\u8054\u7f51\u4f20\u611f\u5668\u6570\u636e\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f00\u53d1\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u7684\u4fe1\u53f7\u667a\u80fd\u6a21\u578b\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u6280\u672f\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2504.17349", "pdf": "https://arxiv.org/pdf/2504.17349", "abs": "https://arxiv.org/abs/2504.17349", "authors": ["Yiyan Xu", "Wuqiang Zheng", "Wenjie Wang", "Fengbin Zhu", "Xinting Hu", "Yang Zhang", "Fuli Feng", "Tat-Seng Chua"], "title": "DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Personalized image generation has emerged as a promising direction in\nmultimodal content creation. It aims to synthesize images tailored to\nindividual style preferences (e.g., color schemes, character appearances,\nlayout) and semantic intentions (e.g., emotion, action, scene contexts) by\nleveraging user-interacted history images and multimodal instructions. Despite\nnotable progress, existing methods -- whether based on diffusion models, large\nlanguage models, or Large Multimodal Models (LMMs) -- struggle to accurately\ncapture and fuse user style preferences and semantic intentions. In particular,\nthe state-of-the-art LMM-based method suffers from the entanglement of visual\nfeatures, leading to Guidance Collapse, where the generated images fail to\npreserve user-preferred styles or reflect the specified semantics.\n  To address these limitations, we introduce DRC, a novel personalized image\ngeneration framework that enhances LMMs through Disentangled Representation\nComposition. DRC explicitly extracts user style preferences and semantic\nintentions from history images and the reference image, respectively, to form\nuser-specific latent instructions that guide image generation within LMMs.\nSpecifically, it involves two critical learning stages: 1) Disentanglement\nlearning, which employs a dual-tower disentangler to explicitly separate style\nand semantic features, optimized via a reconstruction-driven paradigm with\ndifficulty-aware importance sampling; and 2) Personalized modeling, which\napplies semantic-preserving augmentations to effectively adapt the disentangled\nrepresentations for robust personalized generation. Extensive experiments on\ntwo benchmarks demonstrate that DRC shows competitive performance while\neffectively mitigating the guidance collapse issue, underscoring the importance\nof disentangled representation learning for controllable and effective\npersonalized image generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRC\u7684\u65b0\u578b\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u793a\u7ec4\u5408\u589e\u5f3aLMMs\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u7528\u6237\u98ce\u683c\u504f\u597d\u548c\u8bed\u4e49\u610f\u56fe\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6269\u6563\u6a21\u578b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u6216LMMs\uff09\u96be\u4ee5\u51c6\u786e\u6355\u6349\u548c\u878d\u5408\u7528\u6237\u7684\u98ce\u683c\u504f\u597d\u4e0e\u8bed\u4e49\u610f\u56fe\uff0c\u5c24\u5176\u662fLMMs\u5b58\u5728\u89c6\u89c9\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u56fe\u50cf\u65e0\u6cd5\u4fdd\u7559\u7528\u6237\u504f\u597d\u6216\u53cd\u6620\u6307\u5b9a\u8bed\u4e49\u3002", "method": "DRC\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8868\u793a\u7ec4\u5408\uff0c\u660e\u786e\u4ece\u5386\u53f2\u56fe\u50cf\u548c\u53c2\u8003\u56fe\u50cf\u4e2d\u63d0\u53d6\u7528\u6237\u98ce\u683c\u504f\u597d\u548c\u8bed\u4e49\u610f\u56fe\uff0c\u5f62\u6210\u7528\u6237\u7279\u5b9a\u7684\u6f5c\u5728\u6307\u4ee4\u3002\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u5b66\u4e60\u9636\u6bb5\uff1a\u89e3\u8026\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u5efa\u6a21\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDRC\u5728\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u6709\u6548\u7f13\u89e3\u4e86\u6307\u5bfc\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "\u89e3\u8026\u8868\u793a\u5b66\u4e60\u5bf9\u4e8e\u53ef\u63a7\u4e14\u6709\u6548\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u81f3\u5173\u91cd\u8981\uff0cDRC\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977", "abs": "https://arxiv.org/abs/2504.16977", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications.", "AI": {"tldr": "\u6bd4\u8f83BPE\u3001SentencePiece\u548c\u5b57\u7b26\u7ea7\u5206\u8bcd\u5728\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00NER\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0SentencePiece\u5728\u8de8\u8bed\u8a00\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76BPE\u5728\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00NER\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\uff0c\u63a2\u7d22\u66f4\u4f18\u7684\u5206\u8bcd\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u5f62\u6001\u590d\u6742\u6027\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83BPE\u3001SentencePiece\u548c\u5b57\u7b26\u7ea7\u5206\u8bcd\u7b56\u7565\uff0c\u8bc4\u4f30\u5176\u5185\u5728\u8bed\u8a00\u7279\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "result": "SentencePiece\u5728\u8de8\u8bed\u8a00\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8eBPE\uff0c\u5c24\u5176\u5728\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u4e2d\u3002", "conclusion": "SentencePiece\u662f\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00NER\u4efb\u52a1\u4e2d\u66f4\u6709\u6548\u7684\u5206\u8bcd\u7b56\u7565\u3002"}}
{"id": "2504.17364", "pdf": "https://arxiv.org/pdf/2504.17364", "abs": "https://arxiv.org/abs/2504.17364", "authors": ["Ali Haider", "Muhammad Salman Ali", "Maryam Qamar", "Tahir Khalil", "Soo Ye Kim", "Jihyong Oh", "Enzo Tartaglione", "Sung-Ho Bae"], "title": "I-INR: Iterative Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) have revolutionized signal processing\nand computer vision by modeling signals as continuous, differentiable functions\nparameterized by neural networks. However, their inherent formulation as a\nregression problem makes them prone to regression to the mean, limiting their\nability to capture fine details, retain high-frequency information, and handle\nnoise effectively. To address these challenges, we propose Iterative Implicit\nNeural Representations (I-INRs) a novel plug-and-play framework that enhances\nsignal reconstruction through an iterative refinement process. I-INRs\neffectively recover high-frequency details, improve robustness to noise, and\nachieve superior reconstruction quality. Our framework seamlessly integrates\nwith existing INR architectures, delivering substantial performance gains\nacross various tasks. Extensive experiments show that I-INRs outperform\nbaseline methods, including WIRE, SIREN, and Gauss, in diverse computer vision\napplications such as image restoration, image denoising, and object occupancy\nprediction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08I-INRs\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u4fe1\u53f7\u91cd\u5efa\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfINRs\u5728\u7ec6\u8282\u4fdd\u7559\u548c\u9ad8\u9891\u4fe1\u606f\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u7531\u4e8e\u56de\u5f52\u95ee\u9898\u7684\u56fa\u6709\u7279\u6027\uff0c\u5bb9\u6613\u56de\u5f52\u5230\u5747\u503c\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u6355\u6349\u7ec6\u8282\u3001\u4fdd\u7559\u9ad8\u9891\u4fe1\u606f\u6216\u5904\u7406\u566a\u58f0\u3002", "method": "\u63d0\u51faI-INRs\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\u8fc7\u7a0b\u589e\u5f3a\u4fe1\u53f7\u91cd\u5efa\u80fd\u529b\uff0c\u5e76\u4e0e\u73b0\u6709INRs\u67b6\u6784\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cI-INRs\u5728\u56fe\u50cf\u6062\u590d\u3001\u53bb\u566a\u548c\u7269\u4f53\u5360\u636e\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982WIRE\u3001SIREN\u548cGauss\uff09\u3002", "conclusion": "I-INRs\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u53f7\u91cd\u5efa\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u7ec6\u8282\u4fdd\u7559\u548c\u566a\u58f0\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.16979", "pdf": "https://arxiv.org/pdf/2504.16979", "abs": "https://arxiv.org/abs/2504.16979", "authors": ["Masoud Tafavvoghi", "Lars Ailo Bongo", "Andr\u00e9 Berli Delgado", "Nikita Shvetsov", "Anders Sildnes", "Line Moi", "Lill-Tove Rasmussen Busund", "Kajsa M\u00f8llersen"], "title": "Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline", "categories": ["q-bio.QM", "cs.AI", "cs.CV"], "comment": "16 Pages, 9 Figures, 3 tables", "summary": "In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)\nassessment pipeline within QuPath, demonstrating the potential of easily\naccessible tools to perform complex tasks in a fully automatic fashion. First,\nwe trained a pixel classifier to segment tumor, tumor-associated stroma, and\nother tissue compartments in breast cancer H&E-stained whole-slide images (WSI)\nto isolate tumor-associated stroma for subsequent analysis. Next, we applied a\npre-trained StarDist deep learning model in QuPath for cell detection and used\nthe extracted cell features to train a binary classifier distinguishing TILs\nfrom other cells. To evaluate our TILs assessment pipeline, we calculated the\nTIL density in each WSI and categorized them as low, medium, or high TIL\nlevels. Our pipeline was evaluated against pathologist-assigned TIL scores,\nachieving a Cohen's kappa of 0.71 on the external test set, corroborating\nprevious research findings. These results confirm that existing software can\noffer a practical solution for the assessment of TILs in H&E-stained WSIs of\nbreast cancer.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u80bf\u7624\u6d78\u6da6\u6dcb\u5df4\u7ec6\u80de\uff08TILs\uff09\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5229\u7528QuPath\u5b9e\u73b0\u81ea\u52a8\u5316\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u6613\u83b7\u53d6\u7684\u5de5\u5177\uff08\u5982QuPath\uff09\u81ea\u52a8\u5316\u5b8c\u6210\u590d\u6742\u7684TILs\u8bc4\u4f30\u4efb\u52a1\u3002", "method": "1. \u8bad\u7ec3\u50cf\u7d20\u5206\u7c7b\u5668\u5206\u5272\u80bf\u7624\u548c\u80bf\u7624\u76f8\u5173\u57fa\u8d28\uff1b2. \u4f7f\u7528\u9884\u8bad\u7ec3\u7684StarDist\u6a21\u578b\u68c0\u6d4b\u7ec6\u80de\u5e76\u8bad\u7ec3\u4e8c\u5206\u7c7b\u5668\u533a\u5206TILs\uff1b3. \u8ba1\u7b97TIL\u5bc6\u5ea6\u5e76\u5206\u7c7b\u3002", "result": "\u4e0e\u75c5\u7406\u5b66\u5bb6\u8bc4\u5206\u76f8\u6bd4\uff0cCohen's kappa\u4e3a0.71\uff0c\u9a8c\u8bc1\u4e86\u6d41\u7a0b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u73b0\u6709\u8f6f\u4ef6\u53ef\u4e3a\u4e73\u817a\u764cH&E\u67d3\u8272\u5207\u7247\u4e2d\u7684TILs\u8bc4\u4f30\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365", "abs": "https://arxiv.org/abs/2504.17365", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance.", "AI": {"tldr": "TimeSoccer\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u8db3\u7403\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5168\u573a\u6bd4\u8d5b\u7684\u5355\u951a\u70b9\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u751f\u6210\uff0c\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u65f6\u95f4\u6233\u548c\u751f\u6210\u5b57\u5e55\uff0c\u5b9e\u73b0\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u8db3\u7403\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u65f6\u95f4\u5148\u9a8c\uff0c\u65e0\u6cd5\u7aef\u5230\u7aef\u5904\u7406\u89c6\u9891\uff0c\u4f20\u7edf\u65b9\u6cd5\u590d\u6742\u4e14\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faTimeSoccer\uff0c\u7ed3\u5408MoFA-Select\u8bad\u7ec3\u65e0\u5173\u7684\u8fd0\u52a8\u611f\u77e5\u5e27\u538b\u7f29\u6a21\u5757\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7b56\u7565\u81ea\u9002\u5e94\u9009\u62e9\u4ee3\u8868\u6027\u5e27\uff0c\u5e76\u91c7\u7528\u4e92\u8865\u8bad\u7ec3\u8303\u5f0f\u589e\u5f3a\u957f\u65f6\u5e8f\u5904\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTimeSoccer\u5728\u5355\u951a\u70b9\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u8bc4\u8bba\u4e14\u65f6\u95f4\u5bf9\u9f50\u51c6\u786e\u3001\u8bed\u4e49\u76f8\u5173\u6027\u5f3a\u3002", "conclusion": "TimeSoccer\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8db3\u7403\u89c6\u9891\u7684\u7aef\u5230\u7aef\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e7b\u89c9\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u5e76\u8bc1\u660e\u5176\u4e0e\u8bed\u8a00\u8bc6\u522b\u7684\u7b49\u4ef7\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u4f7f\u7528\u6b63\u786e\u6837\u672c\u8bad\u7ec3\u65f6\u68c0\u6d4b\u4e0d\u53ef\u884c\uff0c\u4f46\u52a0\u5165\u4e13\u5bb6\u6807\u6ce8\u7684\u8d1f\u6837\u672c\u540e\u68c0\u6d4b\u53d8\u4e3a\u53ef\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5206\u6790\u81ea\u52a8\u68c0\u6d4bLLM\u5e7b\u89c9\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u91c7\u7528\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5e7b\u89c9\u68c0\u6d4b\u4e0e\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u7b49\u4ef7\u5316\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u6570\u636e\uff08\u4ec5\u6b63\u6837\u672c vs. \u6b63\u8d1f\u6837\u672c\uff09\u5bf9\u68c0\u6d4b\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u4ec5\u7528\u6b63\u786e\u6837\u672c\u8bad\u7ec3\u65f6\uff0c\u5e7b\u89c9\u68c0\u6d4b\u4e0d\u53ef\u884c\uff1b\u52a0\u5165\u4e13\u5bb6\u6807\u6ce8\u7684\u8d1f\u6837\u672c\u540e\uff0c\u68c0\u6d4b\u53d8\u4e3a\u53ef\u80fd\u3002", "conclusion": "\u4e13\u5bb6\u6807\u6ce8\u7684\u8d1f\u6837\u672c\u5bf9\u5e7b\u89c9\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u652f\u6301\u57fa\u4e8e\u53cd\u9988\u7684\u65b9\u6cd5\uff08\u5982RLHF\uff09\u5728\u5b9e\u9645\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.17371", "pdf": "https://arxiv.org/pdf/2504.17371", "abs": "https://arxiv.org/abs/2504.17371", "authors": ["Oussema Dhaouadi", "Johannes Meier", "Luca Wahl", "Jacques Kaiser", "Luca Scalerandi", "Nick Wandelburg", "Zhuolun Zhou", "Nijanthan Berinpanathan", "Holger Banzhaf", "Daniel Cremers"], "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,\ntraditional datasets are usually captured by fixed sensors mounted on a car and\nare susceptible to occlusion. Additionally, such an approach can precisely\nreconstruct the dynamic environment in the close vicinity of the measurement\nvehicle only, while neglecting objects that are further away. In this paper, we\nintroduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,\nocclusion-free dataset of 6 degrees of freedom bounding box trajectories\nacquired through a novel monocular camera drone tracking pipeline. Our dataset\nincludes more than 175,000 trajectories of 14 types of traffic participants and\nsignificantly exceeds existing datasets in terms of diversity and scale,\ncontaining many unprecedented scenarios such as complex vehicle-pedestrian\ninteraction on highly populated urban streets and comprehensive parking\nmaneuvers from entry to exit. DSC3D dataset was captured in five various\nlocations in Europe and the United States and include: a parking lot, a crowded\ninner-city, a steep urban intersection, a federal highway, and a suburban\nintersection. Our 3D trajectory dataset aims to enhance autonomous driving\nsystems by providing detailed environmental 3D representations, which could\nlead to improved obstacle interactions and safety. We demonstrate its utility\nacross multiple applications including motion prediction, motion planning,\nscenario mining, and generative reactive traffic agents. Our interactive online\nvisualization platform and the complete dataset are publicly available at\napp.deepscenario.com, facilitating research in motion prediction, behavior\nmodeling, and safety validation.", "AI": {"tldr": "DSC3D\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u65e0\u906e\u6321\u76843D\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u6355\u6349\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u96c6\u56e0\u56fa\u5b9a\u4f20\u611f\u5668\u548c\u906e\u6321\u95ee\u9898\u9650\u5236\u4e86\u73af\u5883\u91cd\u5efa\u7684\u51c6\u786e\u6027\uff0cDSC3D\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5355\u76ee\u76f8\u673a\u65e0\u4eba\u673a\u8ddf\u8e2a\u6d41\u7a0b\uff0c\u6355\u634914\u7c7b\u4ea4\u901a\u53c2\u4e0e\u8005\u7684175,000\u591a\u6761\u8f68\u8ff9\uff0c\u8986\u76d6\u591a\u79cd\u590d\u6742\u573a\u666f\u3002", "result": "\u6570\u636e\u96c6\u5728\u591a\u6837\u6027\u548c\u89c4\u6a21\u4e0a\u8d85\u8d8a\u73b0\u6709\u6570\u636e\u96c6\uff0c\u652f\u6301\u8fd0\u52a8\u9884\u6d4b\u3001\u884c\u4e3a\u5efa\u6a21\u7b49\u5e94\u7528\u3002", "conclusion": "DSC3D\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u73af\u58833D\u8868\u793a\uff0c\u6709\u671b\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2504.17020", "pdf": "https://arxiv.org/pdf/2504.17020", "abs": "https://arxiv.org/abs/2504.17020", "authors": ["Kasper Engelen", "Guillermo A. P\u00e9rez", "Shrisha Rao"], "title": "Analyzing Value Functions of States in Parametric Markov Chains", "categories": ["cs.LO", "cs.AI"], "comment": "Published as part of the book \"Principles of Verification: Cycling\n  the Probabilistic Landscape: Essays Dedicated to Joost-Pieter Katoen on the\n  Occasion of His 60th Birthday, Part II\"", "summary": "Parametric Markov chains (pMC) are used to model probabilistic systems with\nunknown or partially known probabilities. Although (universal) pMC verification\nfor reachability properties is known to be coETR-complete, there have been\nefforts to approach it using potentially easier-to-check properties such as\nasking whether the pMC is monotonic in certain parameters. In this paper, we\nfirst reduce monotonicity to asking whether the reachability probability from a\ngiven state is never less than that of another given state. Recent results for\nthe latter property imply an efficient algorithm to collapse same-value\nequivalence classes, which in turn preserves verification results and\nmonotonicity. We implement our algorithm to collapse \"trivial\" equivalence\nclasses in the pMC and show empirical evidence for the following: First, the\ncollapse gives reductions in size for some existing benchmarks and significant\nreductions on some custom benchmarks; Second, the collapse speeds up existing\nalgorithms to check monotonicity and parameter lifting, and hence can be used\nas a fast pre-processing step in practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u53c2\u6570\u9a6c\u5c14\u53ef\u592b\u94fe\uff08pMC\uff09\u5355\u8c03\u6027\u95ee\u9898\u7b80\u5316\u4e3a\u72b6\u6001\u53ef\u8fbe\u6982\u7387\u6bd4\u8f83\u7684\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u7b49\u4ef7\u7c7b\u6298\u53e0\u7b97\u6cd5\u4f18\u5316\u9a8c\u8bc1\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6a21\u578b\u89c4\u6a21\u5e76\u52a0\u901f\u5355\u8c03\u6027\u68c0\u67e5\u3002", "motivation": "\u53c2\u6570\u9a6c\u5c14\u53ef\u592b\u94fe\uff08pMC\uff09\u9a8c\u8bc1\u7684\u590d\u6742\u6027\u8f83\u9ad8\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7b80\u5316\u95ee\u9898\uff08\u5982\u5355\u8c03\u6027\u68c0\u67e5\uff09\u6765\u4f18\u5316\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "method": "\u5c06\u5355\u8c03\u6027\u95ee\u9898\u8f6c\u5316\u4e3a\u72b6\u6001\u53ef\u8fbe\u6982\u7387\u6bd4\u8f83\uff0c\u5229\u7528\u7b49\u4ef7\u7c7b\u6298\u53e0\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u6a21\u578b\u89c4\u6a21\u5e76\u52a0\u901f\u5355\u8c03\u6027\u68c0\u67e5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u5feb\u901f\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u63d0\u5347pMC\u9a8c\u8bc1\u6548\u7387\u3002"}}
{"id": "2504.17395", "pdf": "https://arxiv.org/pdf/2504.17395", "abs": "https://arxiv.org/abs/2504.17395", "authors": ["Yiming Zhao", "Guorong Li", "Laiyun Qing", "Amin Beheshti", "Jian Yang", "Michael Sheng", "Yuankai Qi", "Qingming Huang"], "title": "SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object Counting", "categories": ["cs.CV"], "comment": null, "summary": "Open-world object counting leverages the robust text-image alignment of\npre-trained vision-language models (VLMs) to enable counting of arbitrary\ncategories in images specified by textual queries. However, widely adopted\nnaive fine-tuning strategies concentrate exclusively on text-image consistency\nfor categories contained in training, which leads to limited generalizability\nfor unseen categories. In this work, we propose a plug-and-play Semantic-Driven\nVisual Prompt Tuning framework (SDVPT) that transfers knowledge from the\ntraining set to unseen categories with minimal overhead in parameters and\ninference time. First, we introduce a two-stage visual prompt learning strategy\ncomposed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided\nPrompt Refinement (TGPR). The CSPI generates category-specific visual prompts,\nand then TGPR distills latent structural patterns from the VLM's text encoder\nto refine these prompts. During inference, we dynamically synthesize the visual\nprompts for unseen categories based on the semantic correlation between unseen\nand training categories, facilitating robust text-image alignment for unseen\ncategories. Extensive experiments integrating SDVPT with all available\nopen-world object counting models demonstrate its effectiveness and\nadaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDVPT\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u63d0\u5347\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u96c6\u4e2d\u672a\u89c1\u7c7b\u522b\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0cSDVPT\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u9a71\u52a8\u7684\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SDVPT\u91c7\u7528\u4e24\u9636\u6bb5\u89c6\u89c9\u63d0\u793a\u5b66\u4e60\u7b56\u7565\uff1a\u7c7b\u522b\u7279\u5b9a\u63d0\u793a\u521d\u59cb\u5316\u548c\u62d3\u6251\u5f15\u5bfc\u63d0\u793a\u7ec6\u5316\uff0c\u52a8\u6001\u5408\u6210\u672a\u89c1\u7c7b\u522b\u7684\u89c6\u89c9\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDVPT\u5728FSC-147\u3001CARPK\u548cPUCPR+\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "SDVPT\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u8ba1\u6570\u80fd\u529b\u3002"}}
{"id": "2504.17023", "pdf": "https://arxiv.org/pdf/2504.17023", "abs": "https://arxiv.org/abs/2504.17023", "authors": ["Felix Kares", "Timo Speith", "Hanwei Zhang", "Markus Langer"], "title": "What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)", "categories": ["cs.HC", "cs.AI"], "comment": "27 pages, 7 figures, 4 tables", "summary": "Saliency maps are a popular approach for explaining classifications of\n(convolutional) neural networks. However, it remains an open question as to how\nbest to evaluate salience maps, with three families of evaluation methods\ncommonly being used: subjective user measures, objective user measures, and\nmathematical metrics. We examine three of the most popular saliency map\napproaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between\nsubject study (N=166) across these families of evaluation methods. We test 1)\nfor subjective measures, if the maps differ with respect to user trust and\nsatisfaction; 2) for objective measures, if the maps increase users' abilities\nand thus understanding of a model; 3) for mathematical metrics, which map\nachieves the best ratings across metrics; and 4) whether the mathematical\nmetrics can be associated with objective user measures. To our knowledge, our\nstudy is the first to compare several salience maps across all these evaluation\nmethods$-$with the finding that they do not agree in their assessment (i.e.,\nthere was no difference concerning trust and satisfaction, Grad-CAM improved\nusers' abilities best, and Guided Backpropagation had the most favorable\nmathematical metrics). Additionally, we show that some mathematical metrics\nwere associated with user understanding, although this relationship was often\ncounterintuitive. We discuss these findings in light of general debates\nconcerning the complementary use of user studies and mathematical metrics in\nthe evaluation of explainable AI (XAI) approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u663e\u8457\u6027\u56fe\u65b9\u6cd5\uff08LIME\u3001Grad-CAM\u548cGuided Backpropagation\uff09\u5728\u4e0d\u540c\u8bc4\u4f30\u65b9\u6cd5\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8bc4\u4f30\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u5b66\u6307\u6807\u4e0e\u7528\u6237\u7406\u89e3\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u663e\u8457\u6027\u56fe\u662f\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u4f46\u5982\u4f55\u8bc4\u4f30\u5176\u6548\u679c\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u8bc4\u4f30\u65b9\u6cd5\uff08\u4e3b\u89c2\u7528\u6237\u6d4b\u91cf\u3001\u5ba2\u89c2\u7528\u6237\u6d4b\u91cf\u548c\u6570\u5b66\u6307\u6807\uff09\u4e0b\u663e\u8457\u6027\u56fe\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u88ab\u8bd5\u95f4\u5b9e\u9a8c\uff08N=166\uff09\u6d4b\u8bd5\u4e86\u4e09\u79cd\u663e\u8457\u6027\u56fe\u65b9\u6cd5\u5728\u4e3b\u89c2\u4fe1\u4efb\u4e0e\u6ee1\u610f\u5ea6\u3001\u7528\u6237\u80fd\u529b\u63d0\u5347\u53ca\u6570\u5b66\u6307\u6807\u8bc4\u5206\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u5206\u6790\u4e86\u6570\u5b66\u6307\u6807\u4e0e\u7528\u6237\u7406\u89e3\u7684\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u4e3b\u89c2\u8bc4\u4f30\u65e0\u663e\u8457\u5dee\u5f02\uff1bGrad-CAM\u6700\u80fd\u63d0\u5347\u7528\u6237\u80fd\u529b\uff1bGuided Backpropagation\u6570\u5b66\u6307\u6807\u6700\u4f18\uff1b\u90e8\u5206\u6570\u5b66\u6307\u6807\u4e0e\u7528\u6237\u7406\u89e3\u76f8\u5173\u4f46\u5173\u7cfb\u53cd\u76f4\u89c9\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u4e0d\u540c\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u663e\u8457\u6027\u56fe\u7684\u8bc4\u4ef7\u4e0d\u4e00\u81f4\uff0c\u6570\u5b66\u6307\u6807\u4e0e\u7528\u6237\u7406\u89e3\u7684\u5173\u7cfb\u590d\u6742\uff0c\u5f3a\u8c03\u4e86\u7528\u6237\u7814\u7a76\u4e0e\u6570\u5b66\u6307\u6807\u5728\u53ef\u89e3\u91caAI\u8bc4\u4f30\u4e2d\u7684\u4e92\u8865\u6027\u3002"}}
{"id": "2504.17397", "pdf": "https://arxiv.org/pdf/2504.17397", "abs": "https://arxiv.org/abs/2504.17397", "authors": ["Francesc Marti-Escofet", "Benedikt Blumenstiel", "Linus Scheibenreif", "Paolo Fraccaro", "Konrad Schindler"], "title": "Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models", "categories": ["cs.CV"], "comment": "Code available at https://github.com/IBM/peft-geofm", "summary": "Earth observation (EO) is crucial for monitoring environmental changes,\nresponding to disasters, and managing natural resources. In this context,\nfoundation models facilitate remote sensing image analysis to retrieve relevant\ngeoinformation accurately and efficiently. However, as these models grow in\nsize, fine-tuning becomes increasingly challenging due to the associated\ncomputational resources and costs, limiting their accessibility and\nscalability. Furthermore, full fine-tuning can lead to forgetting pre-trained\nfeatures and even degrade model generalization. To address this,\nParameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.\nIn this paper, we conduct extensive experiments with various foundation model\narchitectures and PEFT techniques to evaluate their effectiveness on five\ndifferent EO datasets. Our results provide a comprehensive comparison, offering\ninsights into when and how PEFT methods support the adaptation of pre-trained\ngeospatial models. We demonstrate that PEFT techniques match or even exceed\nfull fine-tuning performance and enhance model generalisation to unseen\ngeographic regions, while reducing training time and memory requirements.\nAdditional experiments investigate the effect of architecture choices such as\nthe decoder type or the use of metadata, suggesting UNet decoders and\nfine-tuning without metadata as the recommended configuration. We have\nintegrated all evaluated foundation models and techniques into the open-source\npackage TerraTorch to support quick, scalable, and cost-effective model\nadaptation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\u5728\u5730\u7403\u89c2\u6d4b\uff08EO\uff09\u9886\u57df\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u5168\u5fae\u8c03\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u4e86TerraTorch\u5de5\u5177\u5305\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u5168\u5fae\u8c03\u7684\u8ba1\u7b97\u6210\u672c\u548c\u7279\u5f81\u9057\u5fd8\u95ee\u9898\u9650\u5236\u4e86\u5176\u53ef\u8bbf\u95ee\u6027\u548c\u6269\u5c55\u6027\uff0cPEFT\u6280\u672f\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u901a\u8fc7\u591a\u79cd\u57fa\u7840\u6a21\u578b\u67b6\u6784\u548cPEFT\u6280\u672f\u5728\u4e94\u4e2aEO\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u5176\u6027\u80fd\uff0c\u5e76\u5206\u6790\u67b6\u6784\u9009\u62e9\uff08\u5982\u89e3\u7801\u5668\u7c7b\u578b\u548c\u5143\u6570\u636e\u4f7f\u7528\uff09\u7684\u5f71\u54cd\u3002", "result": "PEFT\u6280\u672f\u5728\u6027\u80fd\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u5168\u5fae\u8c03\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u5730\u7406\u533a\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u9700\u6c42\u3002UNet\u89e3\u7801\u5668\u548c\u4e0d\u4f7f\u7528\u5143\u6570\u636e\u7684\u914d\u7f6e\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "PEFT\u6280\u672f\u662f\u9ad8\u6548\u9002\u5e94\u9884\u8bad\u7ec3\u5730\u7406\u7a7a\u95f4\u6a21\u578b\u7684\u53ef\u884c\u65b9\u6848\uff0cTerraTorch\u5de5\u5177\u5305\u7684\u5f00\u6e90\u652f\u6301\u4e86\u5feb\u901f\u3001\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u6a21\u578b\u9002\u914d\u3002"}}
{"id": "2504.17028", "pdf": "https://arxiv.org/pdf/2504.17028", "abs": "https://arxiv.org/abs/2504.17028", "authors": ["Iman Khadir", "Shane Stevenson", "Henry Li", "Kyle Krick", "Abram Burrows", "David Hall", "Stan Posey", "Samuel S. P. Shen"], "title": "Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": "12 pages, 8 figures", "summary": "This paper demonstrates the feasibility of democratizing AI-driven global\nweather forecasting models among university research groups by leveraging\nGraphics Processing Units (GPUs) and freely available AI models, such as\nNVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network\nfor weather prediction and is trained on a 73-channel subset of the European\nCentre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset\nat single levels and different pressure levels. Although the training\nspecifications for FourCastNetv2 are not released to the public, the training\ndocumentation of the model's first generation, FourCastNet, is available to all\nusers. The training had 64 A100 GPUs and took 16 hours to complete. Although\nNVIDIA's models offer significant reductions in both time and cost compared to\ntraditional Numerical Weather Prediction (NWP), reproducing published\nforecasting results presents ongoing challenges for resource-constrained\nuniversity research groups with limited GPU availability. We demonstrate both\n(i) leveraging FourCastNetv2 to create predictions through the designated\napplication programming interface (API) and (ii) utilizing NVIDIA hardware to\ntrain the original FourCastNet model. Further, this paper demonstrates the\ncapabilities and limitations of NVIDIA A100's for resource-limited research\ngroups in universities. We also explore data management, training efficiency,\nand model validation, highlighting the advantages and challenges of using\nlimited high-performance computing resources. Consequently, this paper and its\ncorresponding GitHub materials may serve as an initial guide for other\nuniversity research groups and courses related to machine learning, climate\nscience, and data science to develop research and education programs on AI\nweather forecasting, and hence help democratize the AI NWP in the digital\neconomy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528GPU\u548c\u514d\u8d39AI\u6a21\u578b\uff08\u5982FourCastNetv2\uff09\u5728\u5927\u5b66\u7814\u7a76\u5c0f\u7ec4\u4e2d\u666e\u53caAI\u9a71\u52a8\u7684\u5168\u7403\u5929\u6c14\u9884\u62a5\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u8d44\u6e90\u9650\u5236\u4e0b\u7684\u6311\u6218\u4e0e\u53ef\u80fd\u6027\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\uff08NWP\uff09\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u800cAI\u6a21\u578b\uff08\u5982FourCastNetv2\uff09\u80fd\u663e\u8457\u964d\u4f4e\u6210\u672c\u548c\u65f6\u95f4\u3002\u7136\u800c\uff0c\u8d44\u6e90\u6709\u9650\u7684\u5927\u5b66\u7814\u7a76\u5c0f\u7ec4\u5728\u590d\u73b0\u7ed3\u679c\u65f6\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u73b0\u6709\u6280\u672f\u5b9e\u73b0AI\u5929\u6c14\u9884\u62a5\u7684\u6c11\u4e3b\u5316\u3002", "method": "\u901a\u8fc7FourCastNetv2\u7684API\u751f\u6210\u9884\u6d4b\uff0c\u5e76\u5229\u7528NVIDIA\u786c\u4ef6\u8bad\u7ec3\u539f\u59cbFourCastNet\u6a21\u578b\u3002\u540c\u65f6\uff0c\u5206\u6790\u4e86A100 GPU\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e0b\u7684\u80fd\u529b\u4e0e\u9650\u5236\uff0c\u63a2\u8ba8\u4e86\u6570\u636e\u7ba1\u7406\u3001\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u5c55\u793a\u4e86\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u4f7f\u7528FourCastNetv2\u548cA100 GPU\u7684\u53ef\u884c\u6027\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u8bad\u7ec3\u548c\u590d\u73b0\u7ed3\u679c\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u5927\u5b66\u7814\u7a76\u5c0f\u7ec4\u548c\u8bfe\u7a0b\u63d0\u4f9b\u4e86AI\u5929\u6c14\u9884\u62a5\u7814\u7a76\u548c\u6559\u80b2\u7684\u521d\u6b65\u6307\u5357\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AI NWP\u5728\u6570\u5b57\u7ecf\u6d4e\u4e2d\u7684\u6c11\u4e3b\u5316\u3002"}}
{"id": "2504.17399", "pdf": "https://arxiv.org/pdf/2504.17399", "abs": "https://arxiv.org/abs/2504.17399", "authors": ["Sven Teufel", "J\u00f6rg Gamerdinger", "Oliver Bringmann"], "title": "S2S-Net: Addressing the Domain Gap of Heterogeneous Sensor Systems in LiDAR-Based Collective Perception", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Collective Perception (CP) has emerged as a promising approach to overcome\nthe limitations of individual perception in the context of autonomous driving.\nVarious approaches have been proposed to realize collective perception;\nhowever, the Sensor2Sensor domain gap that arises from the utilization of\ndifferent sensor systems in Connected and Automated Vehicles (CAVs) remains\nmostly unaddressed. This is primarily due to the paucity of datasets containing\nheterogeneous sensor setups among the CAVs. The recently released SCOPE\ndatasets address this issue by providing data from three different LiDAR\nsensors for each CAV. This study is the first to tackle the Sensor2Sensor\ndomain gap in vehicle to vehicle (V2V) collective perception. First, we present\nour sensor-domain robust architecture S2S-Net. Then an in-depth analysis of the\nSensor2Sensor domain adaptation capabilities of S2S-Net on the SCOPE dataset is\nconducted. S2S-Net demonstrates the capability to maintain very high\nperformance in unseen sensor domains and achieved state-of-the-art results on\nthe SCOPE dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faS2S-Net\u67b6\u6784\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u96c6\u4f53\u611f\u77e5\u7684Sensor2Sensor\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u5e76\u5728SCOPE\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u56e0\u4e0d\u540c\u4f20\u611f\u5668\u7cfb\u7edf\u5bfc\u81f4\u7684Sensor2Sensor\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4f20\u611f\u5668\u57df\u9c81\u68d2\u67b6\u6784S2S-Net\uff0c\u5e76\u5728SCOPE\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57df\u9002\u5e94\u80fd\u529b\u5206\u6790\u3002", "result": "S2S-Net\u5728\u672a\u89c1\u8fc7\u7684\u4f20\u611f\u5668\u57df\u4e2d\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u5e76\u5728SCOPE\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "S2S-Net\u6709\u6548\u89e3\u51b3\u4e86\u96c6\u4f53\u611f\u77e5\u4e2d\u7684Sensor2Sensor\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.17029", "pdf": "https://arxiv.org/pdf/2504.17029", "abs": "https://arxiv.org/abs/2504.17029", "authors": ["Jeffrey Smith", "Taisei Fujii", "Jesse Cranney", "Charles Gretton"], "title": "Fried Parameter Estimation from Single Wavefront Sensor Image with Artificial Neural Networks", "categories": ["astro-ph.IM", "cs.AI"], "comment": null, "summary": "Atmospheric turbulence degrades the quality of astronomical observations in\nground-based telescopes, leading to distorted and blurry images. Adaptive\nOptics (AO) systems are designed to counteract these effects, using atmospheric\nmeasurements captured by a wavefront sensor to make real-time corrections to\nthe incoming wavefront. The Fried parameter, r0, characterises the strength of\natmospheric turbulence and is an essential control parameter for optimising the\nperformance of AO systems and more recently sky profiling for Free Space\nOptical (FSO) communication channels. In this paper, we develop a novel\ndata-driven approach, adapting machine learning methods from computer vision\nfor Fried parameter estimation from a single Shack-Hartmann or pyramid\nwavefront sensor image. Using these data-driven methods, we present a detailed\nsimulation-based evaluation of our approach using the open-source COMPASS AO\nsimulation tool to evaluate both the Shack-Hartmann and pyramid wavefront\nsensors. Our evaluation is over a range of guide star magnitudes, and realistic\nnoise, atmospheric and instrument conditions. Remarkably, we are able to\ndevelop a single network-based estimator that is accurate in both open and\nclosed-loop AO configurations. Our method accurately estimates the Fried\nparameter from a single WFS image directly from AO telemetry to a few\nmillimetres. Our approach is suitable for real time control, exhibiting 0.83ms\nr0 inference times on retail NVIDIA RTX 3090 GPU hardware, and thereby\ndemonstrating a compelling economic solution for use in real-time instrument\ncontrol.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u4e2a\u6ce2\u524d\u4f20\u611f\u5668\u56fe\u50cf\u4e2d\u4f30\u8ba1Fried\u53c2\u6570\uff08r0\uff09\uff0c\u4ee5\u4f18\u5316\u81ea\u9002\u5e94\u5149\u5b66\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u6c14\u6e4d\u6d41\u4f1a\u964d\u4f4e\u5730\u9762\u671b\u8fdc\u955c\u89c2\u6d4b\u7684\u8d28\u91cf\uff0c\u81ea\u9002\u5e94\u5149\u5b66\u7cfb\u7edf\u9700\u8981\u5b9e\u65f6\u6821\u6b63\u6ce2\u524d\uff0c\u800cFried\u53c2\u6570\u662f\u5173\u952e\u63a7\u5236\u53c2\u6570\u3002\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u4e0d\u591f\u9ad8\u6548\u6216\u51c6\u786e\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4eceShack-Hartmann\u6216\u91d1\u5b57\u5854\u6ce2\u524d\u4f20\u611f\u5668\u56fe\u50cf\u4e2d\u4f30\u8ba1r0\uff0c\u5e76\u901a\u8fc7COMPASS AO\u4eff\u771f\u5de5\u5177\u8fdb\u884c\u8be6\u7ec6\u8bc4\u4f30\u3002", "result": "\u65b9\u6cd5\u5728\u5f00\u73af\u548c\u95ed\u73afAO\u914d\u7f6e\u4e2d\u5747\u80fd\u51c6\u786e\u4f30\u8ba1r0\uff0c\u8bef\u5dee\u5728\u6beb\u7c73\u7ea7\uff0c\u4e14\u63a8\u7406\u65f6\u95f4\u4ec50.83\u6beb\u79d2\uff0c\u9002\u5408\u5b9e\u65f6\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u4eea\u5668\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17401", "pdf": "https://arxiv.org/pdf/2504.17401", "abs": "https://arxiv.org/abs/2504.17401", "authors": ["Xu Wang", "Jialang Xu", "Shuai Zhang", "Baoru Huang", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets.", "AI": {"tldr": "StereoMamba\u67b6\u6784\u901a\u8fc7FE-Mamba\u548cMFF\u6a21\u5757\u63d0\u5347RAMIS\u4e2d\u7684\u7acb\u4f53\u89c6\u5dee\u4f30\u8ba1\u6027\u80fd\uff0c\u5728\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u901f\u5ea6\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u4e2d\u7acb\u4f53\u89c6\u5dee\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u63a8\u7406\u901f\u5ea6\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faStereoMamba\u67b6\u6784\uff0c\u5305\u62ecFE-Mamba\u6a21\u5757\u589e\u5f3a\u957f\u7a0b\u7a7a\u95f4\u4f9d\u8d56\u6027\u548cMFF\u6a21\u5757\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "result": "\u5728SCARED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff08EPE 2.64 px\uff0c\u6df1\u5ea6MAE 2.55 mm\uff09\uff0c\u63a8\u7406\u901f\u5ea621.28 FPS\uff0c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7a81\u51fa\u3002", "conclusion": "StereoMamba\u5728RAMIS\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u7684\u5e73\u8861\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17414", "pdf": "https://arxiv.org/pdf/2504.17414", "abs": "https://arxiv.org/abs/2504.17414", "authors": ["Min Wei", "Chaohui Yu", "Jingkai Zhou", "Fan Wang"], "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://2y7c3.github.io/3DV-TON/", "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/", "AI": {"tldr": "3DV-TON\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u8bd5\u7a7f\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u52a8\u753b\u76843D\u7f51\u683c\u4f5c\u4e3a\u663e\u5f0f\u5e27\u7ea7\u6307\u5bfc\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u670d\u88c5\u548c\u591a\u6837\u59ff\u52bf\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u7ed3\u679c\u7684\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bd5\u7a7f\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u670d\u88c5\u56fe\u6848\u548c\u591a\u6837\u8eab\u4f53\u59ff\u52bf\u65f6\uff0c\u96be\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "method": "\u91c7\u7528\u751f\u6210\u7684\u53ef\u52a8\u753b\u7eb9\u74063D\u7f51\u683c\u4f5c\u4e3a\u5e27\u7ea7\u6307\u5bfc\uff0c\u5305\u62ec\u5173\u952e\u5e27\u9009\u62e9\u30012D\u56fe\u50cf\u8bd5\u7a7f\u30013D\u7f51\u683c\u91cd\u5efa\u4e0e\u52a8\u753b\u540c\u6b65\uff0c\u5e76\u5f15\u5165\u77e9\u5f62\u63a9\u7801\u7b56\u7565\u51cf\u5c11\u4f2a\u5f71\u4f20\u64ad\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728HR-VVT\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u8bd5\u7a7f\u7ed3\u679c\u3002", "conclusion": "3DV-TON\u901a\u8fc73D\u7f51\u683c\u6307\u5bfc\u548c\u81ea\u9002\u5e94\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8bd5\u7a7f\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2504.17044", "pdf": "https://arxiv.org/pdf/2504.17044", "abs": "https://arxiv.org/abs/2504.17044", "authors": ["Dhari Gandhi", "Himanshu Joshi", "Lucas Hartman", "Shabnam Hassani"], "title": "Approaches to Responsible Governance of GenAI in Organizations", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of Generative AI (GenAI) has introduced unprecedented\nopportunities while presenting complex challenges around ethics,\naccountability, and societal impact. This paper draws on a literature review,\nestablished governance frameworks, and industry roundtable discussions to\nidentify core principles for integrating responsible GenAI governance into\ndiverse organizational structures. Our objective is to provide actionable\nrecommendations for a balanced, risk-based governance approach that enables\nboth innovation and oversight. Findings emphasize the need for adaptable risk\nassessment tools, continuous monitoring practices, and cross-sector\ncollaboration to establish trustworthy GenAI. These insights provide a\nstructured foundation and Responsible GenAI Guide (ResAI) for organizations to\nalign GenAI initiatives with ethical, legal, and operational best practices.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u98ce\u9669\u7684\u8d23\u4efb\u6cbb\u7406\u6846\u67b6\uff08ResAI\uff09\uff0c\u4ee5\u5e73\u8861\u521b\u65b0\u4e0e\u76d1\u7ba1\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u4f26\u7406\u3001\u95ee\u8d23\u548c\u793e\u4f1a\u5f71\u54cd\u7b49\u590d\u6742\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u8d23\u4efb\u6cbb\u7406\u6846\u67b6\u6765\u6307\u5bfc\u7ec4\u7ec7\u5b9e\u8df5\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u3001\u73b0\u6709\u6cbb\u7406\u6846\u67b6\u548c\u884c\u4e1a\u5706\u684c\u8ba8\u8bba\uff0c\u8bc6\u522b\u6838\u5fc3\u539f\u5219\uff0c\u5e76\u63d0\u51fa\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\u3002", "result": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u9002\u5e94\u6027\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u3001\u6301\u7eed\u76d1\u63a7\u5b9e\u8df5\u548c\u8de8\u90e8\u95e8\u5408\u4f5c\uff0c\u4ee5\u5efa\u7acb\u53ef\u4fe1\u7684GenAI\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86ResAI\u6307\u5357\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u5c06GenAI\u4e0e\u4f26\u7406\u3001\u6cd5\u5f8b\u548c\u8fd0\u8425\u6700\u4f73\u5b9e\u8df5\u5bf9\u9f50\u3002"}}
{"id": "2504.17432", "pdf": "https://arxiv.org/pdf/2504.17432", "abs": "https://arxiv.org/abs/2504.17432", "authors": ["Tiancheng Gu", "Kaicheng Yang", "Ziyong Feng", "Xingjun Wang", "Yanzhao Zhang", "Dingkun Long", "Yingda Chen", "Weidong Cai", "Jiankang Deng"], "title": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs", "categories": ["cs.CV"], "comment": "13 pages, 8 figures, Project page: https://garygutc.github.io/UniME", "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.", "AI": {"tldr": "UniME\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528MLLMs\u5b66\u4e60\u591a\u6a21\u6001\u8868\u793a\uff0c\u89e3\u51b3\u4e86CLIP\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "CLIP\u5728\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\u5b58\u5728\u6587\u672c\u622a\u65ad\u3001\u5b64\u7acb\u7f16\u7801\u548c\u7ec4\u5408\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u800cMLLMs\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002", "method": "UniME\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4eceLLM\u6559\u5e08\u6a21\u578b\u84b8\u998f\u6587\u672c\u77e5\u8bc6\uff1b2) \u5f15\u5165\u786c\u8d1f\u6837\u672c\u589e\u5f3a\u7684\u6307\u4ee4\u8c03\u6574\u3002", "result": "\u5728MMEB\u57fa\u51c6\u548c\u591a\u4e2a\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cUniME\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u5224\u522b\u6027\u548c\u7ec4\u5408\u80fd\u529b\u3002", "conclusion": "UniME\u901a\u8fc7\u4e24\u9636\u6bb5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2504.17055", "pdf": "https://arxiv.org/pdf/2504.17055", "abs": "https://arxiv.org/abs/2504.17055", "authors": ["Ayushi Agrawal", "Aditya Kondai", "Kavita Vemuri"], "title": "Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "AI-powered facial assessment tools are reshaping how individuals evaluate\nappearance and internalize social judgments. This study examines the\npsychological impact of such tools on self-objectification, self-esteem, and\nemotional responses, with attention to gender differences. Two samples used\ndistinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9\nyears), and another more neutral (N=51; M=19.9 years). Participants completed\nvalidated self-objectification and self-esteem scales and custom items\nmeasuring emotion, digital/physical appearance enhancement (DAE, PAEE), and\nperceived social emotion (PSE). Results revealed consistent links between high\nself-objectification, low self-esteem, and increased appearance enhancement\nbehaviors across both versions. Despite softer framing, the newer tool still\nevoked negative emotional responses (U=1466.5, p=0.013), indicating implicit\nfeedback may reinforce appearance-related insecurities. Gender differences\nemerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital\nenhancement and less likely to perceive emotional impact in others. These\nfindings reveal how AI tools may unintentionally reinforce and amplify existing\nsocial biases and underscore the critical need for responsible AI design and\ndevelopment. Future research will investigate how human ideologies embedded in\nthe training data of such tools shape their evaluative outputs, and how these,\nin turn, influence user attitudes and decisions.", "AI": {"tldr": "AI\u9762\u90e8\u8bc4\u4f30\u5de5\u5177\u5bf9\u81ea\u6211\u7269\u5316\u3001\u81ea\u5c0a\u548c\u60c5\u7eea\u53cd\u5e94\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u6027\u522b\u5dee\u5f02\u65b9\u9762\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u4e2d\u6027\u7248\u672c\u7684\u5de5\u5177\u4e5f\u53ef\u80fd\u5f15\u53d1\u8d1f\u9762\u60c5\u7eea\uff0c\u5e76\u5f3a\u5316\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u63a2\u8ba8AI\u9762\u90e8\u8bc4\u4f30\u5de5\u5177\u5bf9\u5fc3\u7406\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u81ea\u6211\u7269\u5316\u548c\u81ea\u5c0a\uff0c\u4ee5\u53ca\u6027\u522b\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u7248\u672c\u7684\u9762\u90e8\u5206\u6790\u5de5\u5177\uff08\u6279\u8bc4\u6027\u548c\u4e2d\u6027\uff09\uff0c\u6d4b\u91cf\u53c2\u4e0e\u8005\u7684\u81ea\u6211\u7269\u5316\u3001\u81ea\u5c0a\u3001\u60c5\u7eea\u53cd\u5e94\u7b49\u3002", "result": "\u9ad8\u81ea\u6211\u7269\u5316\u548c\u4f4e\u81ea\u5c0a\u4e0e\u5916\u89c2\u589e\u5f3a\u884c\u4e3a\u76f8\u5173\uff1b\u4e2d\u6027\u5de5\u5177\u4ecd\u5f15\u53d1\u8d1f\u9762\u60c5\u7eea\uff1b\u5973\u6027\u66f4\u503e\u5411\u4e8e\u6570\u5b57\u589e\u5f3a\u4e14\u5bf9\u4ed6\u4eba\u60c5\u7eea\u611f\u77e5\u8f83\u5f31\u3002", "conclusion": "AI\u5de5\u5177\u53ef\u80fd\u65e0\u610f\u4e2d\u5f3a\u5316\u793e\u4f1a\u504f\u89c1\uff0c\u9700\u8d1f\u8d23\u4efb\u7684\u8bbe\u8ba1\u3002\u672a\u6765\u7814\u7a76\u5c06\u5173\u6ce8\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u610f\u8bc6\u5f62\u6001\u5982\u4f55\u5f71\u54cd\u5de5\u5177\u8f93\u51fa\u53ca\u7528\u6237\u6001\u5ea6\u3002"}}
{"id": "2504.17441", "pdf": "https://arxiv.org/pdf/2504.17441", "abs": "https://arxiv.org/abs/2504.17441", "authors": ["Mingxuan Wu", "Huang Huang", "Justin Kerr", "Chung Min Kim", "Anthony Zhang", "Brent Yi", "Angjoo Kanazawa"], "title": "Predict-Optimize-Distill: A Self-Improving Cycle for 4D Object Understanding", "categories": ["cs.CV"], "comment": "See our website at:\n  https://predict-optimize-distill.github.io/pod.github.io First two authors\n  contributed equally", "summary": "Humans can resort to long-form inspection to build intuition on predicting\nthe 3D configurations of unseen objects. The more we observe the object motion,\nthe better we get at predicting its 3D state immediately. Existing systems\neither optimize underlying representations from multi-view observations or\ntrain a feed-forward predictor from supervised datasets. We introduce\nPredict-Optimize-Distill (POD), a self-improving framework that interleaves\nprediction and optimization in a mutually reinforcing cycle to achieve better\n4D object understanding with increasing observation time. Given a multi-view\nobject scan and a long-form monocular video of human-object interaction, POD\niteratively trains a neural network to predict local part poses from RGB\nframes, uses this predictor to initialize a global optimization which refines\noutput poses through inverse rendering, then finally distills the results of\noptimization back into the model by generating synthetic self-labeled training\ndata from novel viewpoints. Each iteration improves both the predictive model\nand the optimized motion trajectory, creating a virtuous cycle that bootstraps\nits own training data to learn about the pose configurations of an object. We\nalso introduce a quasi-multiview mining strategy for reducing depth ambiguity\nby leveraging long video. We evaluate POD on 14 real-world and 5 synthetic\nobjects with various joint types, including revolute and prismatic joints as\nwell as multi-body configurations where parts detach or reattach independently.\nPOD demonstrates significant improvement over a pure optimization baseline\nwhich gets stuck in local minima, particularly for longer videos. We also find\nthat POD's performance improves with both video length and successive\niterations of the self-improving cycle, highlighting its ability to scale\nperformance with additional observations and looped refinement.", "AI": {"tldr": "POD\u6846\u67b6\u901a\u8fc7\u9884\u6d4b-\u4f18\u5316-\u84b8\u998f\u7684\u5faa\u73af\u81ea\u6211\u63d0\u5347\u673a\u5236\uff0c\u7ed3\u5408\u957f\u89c6\u9891\u548c\u591a\u89c6\u89d2\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u7269\u4f534D\u72b6\u6001\u7684\u7406\u89e3\u548c\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u957f\u65f6\u95f4\u89c2\u5bdf\u7269\u4f53\u8fd0\u52a8\u6765\u9884\u6d4b\u51763D\u72b6\u6001\uff0c\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u591a\u89c6\u89d2\u6570\u636e\u6216\u76d1\u7763\u5b66\u4e60\uff0c\u7f3a\u4e4f\u81ea\u6211\u63d0\u5347\u80fd\u529b\u3002", "method": "\u63d0\u51faPredict-Optimize-Distill (POD)\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u3001\u4f18\u5316\u548c\u84b8\u998f\u7684\u5faa\u73af\u8fed\u4ee3\uff0c\u5229\u7528\u957f\u89c6\u9891\u548c\u591a\u89c6\u89d2\u6570\u636e\u81ea\u6211\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u572814\u4e2a\u771f\u5b9e\u548c5\u4e2a\u5408\u6210\u7269\u4f53\u4e0a\u9a8c\u8bc1\uff0cPOD\u663e\u8457\u4f18\u4e8e\u7eaf\u4f18\u5316\u57fa\u7ebf\uff0c\u6027\u80fd\u968f\u89c6\u9891\u957f\u5ea6\u548c\u8fed\u4ee3\u6b21\u6570\u63d0\u5347\u3002", "conclusion": "POD\u5c55\u793a\u4e86\u901a\u8fc7\u5faa\u73af\u81ea\u6211\u63d0\u5347\u548c\u957f\u89c6\u9891\u5229\u7528\uff0c\u5b9e\u73b0\u5bf9\u7269\u4f534D\u72b6\u6001\u7406\u89e3\u7684\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2504.17058", "pdf": "https://arxiv.org/pdf/2504.17058", "abs": "https://arxiv.org/abs/2504.17058", "authors": ["Rahul Vishwakarma"], "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5171\u5f62\u9884\u6d4b\u4e0eGAN\u7684\u65b0\u6846\u67b6\uff08cGAN\uff09\uff0c\u4e3a\u751f\u6210\u6570\u636e\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u5bf9\u6570\u636e\u5206\u5e03\u7684\u4e25\u683c\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u5c06\u591a\u79cd\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff08\u5982ICP\u3001Mondrian\u7b49\uff09\u96c6\u6210\u5230GAN\u4e2d\uff0c\u5b9e\u73b0\u5206\u5e03\u65e0\u5173\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "cGAN\u5728\u4fdd\u6301\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u7edf\u8ba1\u4fdd\u8bc1\u548c\u6821\u51c6\u7279\u6027\u3002", "conclusion": "cGAN\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5177\u6709\u6570\u5b66\u8bc1\u660e\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.17447", "pdf": "https://arxiv.org/pdf/2504.17447", "abs": "https://arxiv.org/abs/2504.17447", "authors": ["De-An Huang", "Subhashree Radhakrishnan", "Zhiding Yu", "Jan Kautz"], "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG", "AI": {"tldr": "FRAG\u901a\u8fc7\u72ec\u7acb\u8bc4\u5206\u9009\u62e9\u8f93\u5165\u4e2d\u7684\u5173\u952e\u5e27\uff0c\u907f\u514d\u957f\u4e0a\u4e0b\u6587\u5904\u7406\uff0c\u63d0\u5347\u957f\u89c6\u9891\u548c\u591a\u9875\u6587\u6863\u7684\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u53d7\u9650\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u65e0\u9700\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFRAG\u6846\u67b6\uff0c\u72ec\u7acb\u8bc4\u5206\u9009\u62e9\u5173\u952e\u5e27\uff0c\u4ec5\u57fa\u4e8e\u9009\u5b9a\u5e27\u751f\u6210\u8f93\u51fa\u3002", "result": "\u5728\u957f\u89c6\u9891\u548c\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8fbe\u5230SOTA\u3002", "conclusion": "FRAG\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u73b0\u6709LMMs\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u957f\u8f93\u5165\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2504.17457", "pdf": "https://arxiv.org/pdf/2504.17457", "abs": "https://arxiv.org/abs/2504.17457", "authors": ["Zhiying Li", "Yeying Jin", "Fan Shen", "Zhi Liu", "Weibin Chen", "Pengju Zhang", "Xiaomei Zhang", "Boyu Chen", "Michael Shen", "Kejian Wu", "Zhaoxin Fan", "Jin Dong"], "title": "Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks", "categories": ["cs.CV"], "comment": "14 pages, 7 figures", "summary": "Expressive human pose and shape estimation (EHPS) is crucial for digital\nhuman generation, especially in applications like live streaming. While\nexisting research primarily focuses on reducing estimation errors, it largely\nneglects robustness and security aspects, leaving these systems vulnerable to\nadversarial attacks. To address this significant challenge, we propose the\n\\textbf{Tangible Attack (TBA)}, a novel framework designed to generate\nadversarial examples capable of effectively compromising any digital human\ngeneration model. Our approach introduces a \\textbf{Dual Heterogeneous Noise\nGenerator (DHNG)}, which leverages Variational Autoencoders (VAE) and\nControlNet to produce diverse, targeted noise tailored to the original image\nfeatures. Additionally, we design a custom \\textbf{adversarial loss function}\nto optimize the noise, ensuring both high controllability and potent\ndisruption. By iteratively refining the adversarial sample through\nmulti-gradient signals from both the noise and the state-of-the-art EHPS model,\nTBA substantially improves the effectiveness of adversarial attacks. Extensive\nexperiments demonstrate TBA's superiority, achieving a remarkable 41.0\\%\nincrease in estimation error, with an average improvement of approximately\n17.0\\%. These findings expose significant security vulnerabilities in current\nEHPS models and highlight the need for stronger defenses in digital human\ngeneration systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTangible Attack (TBA)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7Dual Heterogeneous Noise Generator (DHNG)\u548c\u5b9a\u5236\u5bf9\u6297\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6570\u5b57\u4eba\u751f\u6210\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u51cf\u5c11\u4f30\u8ba1\u8bef\u5dee\uff0c\u4f46\u5ffd\u89c6\u4e86\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u3002", "method": "\u63d0\u51faTBA\u6846\u67b6\uff0c\u7ed3\u5408DHNG\uff08\u5229\u7528VAE\u548cControlNet\u751f\u6210\u591a\u6837\u5316\u566a\u58f0\uff09\u548c\u5b9a\u5236\u5bf9\u6297\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u591a\u68af\u5ea6\u4fe1\u53f7\u8fed\u4ee3\u4f18\u5316\u5bf9\u6297\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u663e\u793aTBA\u5c06\u4f30\u8ba1\u8bef\u5dee\u63d0\u9ad8\u4e8641.0%\uff0c\u5e73\u5747\u63d0\u5347\u7ea617.0%\u3002", "conclusion": "TBA\u63ed\u793a\u4e86\u5f53\u524dEHPS\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u6570\u5b57\u4eba\u751f\u6210\u7cfb\u7edf\u9700\u8981\u66f4\u5f3a\u7684\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2504.17070", "pdf": "https://arxiv.org/pdf/2504.17070", "abs": "https://arxiv.org/abs/2504.17070", "authors": ["Mohaiminul Al Nahian", "Zainab Altaweel", "David Reitano", "Sabbir Ahmed", "Saumitra Lohokare", "Shiqi Zhang", "Adnan Siraj Rakin"], "title": "Robo-Troj: Attacking LLM-based Task Planners", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robots need task planning methods to achieve goals that require more than\nindividual actions. Recently, large language models (LLMs) have demonstrated\nimpressive performance in task planning. LLMs can generate a step-by-step\nsolution using a description of actions and the goal. Despite the successes in\nLLM-based task planning, there is limited research studying the security\naspects of those systems. In this paper, we develop Robo-Troj, the first\nmulti-trigger backdoor attack for LLM-based task planners, which is the main\ncontribution of this work. As a multi-trigger attack, Robo-Troj is trained to\naccommodate the diversity of robot application domains. For instance, one can\nuse unique trigger words, e.g., \"herical\", to activate a specific malicious\nbehavior, e.g., cutting hand on a kitchen robot. In addition, we develop an\noptimization method for selecting the trigger words that are most effective.\nThrough demonstrating the vulnerability of LLM-based planners, we aim to\npromote the development of secured robot systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Robo-Troj\uff0c\u4e00\u79cd\u9488\u5bf9\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u89c4\u5212\u5668\u7684\u591a\u89e6\u53d1\u5668\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u65e8\u5728\u63ed\u793a\u5176\u5b89\u5168\u6f0f\u6d1e\u5e76\u4fc3\u8fdb\u5b89\u5168\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u89c4\u5212\u5668\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5b89\u5168\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63ed\u793a\u6f5c\u5728\u5a01\u80c1\u3002", "method": "\u5f00\u53d1Robo-Troj\uff0c\u4e00\u79cd\u591a\u89e6\u53d1\u5668\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u89e6\u53d1\u5668\u9009\u62e9\u6765\u6fc0\u6d3b\u7279\u5b9a\u6076\u610f\u884c\u4e3a\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u89c4\u5212\u5668\u7684\u8106\u5f31\u6027\uff0c\u9a8c\u8bc1\u4e86Robo-Troj\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u63ed\u793a\u5b89\u5168\u6f0f\u6d1e\uff0c\u672c\u6587\u547c\u5401\u52a0\u5f3aLLM\u4efb\u52a1\u89c4\u5212\u5668\u7684\u5b89\u5168\u6027\u7814\u7a76\u3002"}}
{"id": "2504.17474", "pdf": "https://arxiv.org/pdf/2504.17474", "abs": "https://arxiv.org/abs/2504.17474", "authors": ["Weiran Pan", "Wei Wei", "Feida Zhu", "Yong Deng"], "title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6837\u672c\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u6a21\u578b\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u8d8b\u52bf\u800c\u975e\u4ec5\u4f9d\u8d56\u635f\u5931\u503c\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u533a\u5206\u6b63\u786e\u6807\u6ce8\u4f46\u96be\u5b66\u4e60\u7684\u6837\u672c\u548c\u9519\u8bef\u6807\u6ce8\u7684\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5c0f\u635f\u5931\u6837\u672c\u89c6\u4e3a\u6b63\u786e\u6807\u6ce8\uff0c\u4f46\u4e00\u4e9b\u6b63\u786e\u6807\u6ce8\u7684\u6837\u672c\u53ef\u80fd\u56e0\u96be\u4ee5\u5b66\u4e60\u800c\u5728\u8bad\u7ec3\u65e9\u671f\u8868\u73b0\u51fa\u9ad8\u635f\u5931\uff0c\u5bfc\u81f4\u6837\u672c\u9009\u62e9\u65f6\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u901a\u8fc7\u8ddf\u8e2a\u6807\u6ce8\u6807\u7b7e\u4e0e\u5176\u4ed6\u7c7b\u522b\u4e4b\u95f4\u7684\u7f6e\u4fe1\u5ea6\u5dee\u8ddd\uff0c\u5e76\u4f7f\u7528Mann-Kendall Test\u8bc4\u4f30\u5176\u8d8b\u52bf\uff0c\u5224\u65ad\u6837\u672c\u662f\u5426\u4e3a\u6b63\u786e\u6807\u6ce8\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u73b0\u6709\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u6837\u672c\u9009\u62e9\u4e2d\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u5347\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.17077", "pdf": "https://arxiv.org/pdf/2504.17077", "abs": "https://arxiv.org/abs/2504.17077", "authors": ["Dongjin Seo", "Soobin Um", "Sangbin Lee", "Jong Chul Ye", "Haejun Chung"], "title": "Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models", "categories": ["physics.optics", "cs.AI", "physics.comp-ph"], "comment": "25 pages, 7 Figures", "summary": "Designing free-form photonic devices is fundamentally challenging due to the\nvast number of possible geometries and the complex requirements of fabrication\nconstraints. Traditional inverse-design approaches--whether driven by human\nintuition, global optimization, or adjoint-based gradient methods--often\ninvolve intricate binarization and filtering steps, while recent deep learning\nstrategies demand prohibitively large numbers of simulations (10^5 to 10^6). To\novercome these limitations, we present AdjointDiffusion, a physics-guided\nframework that integrates adjoint sensitivity gradients into the sampling\nprocess of diffusion models. AdjointDiffusion begins by training a diffusion\nnetwork on a synthetic, fabrication-aware dataset of binary masks. During\ninference, we compute the adjoint gradient of a candidate structure and inject\nthis physics-based guidance at each denoising step, steering the generative\nprocess toward high figure-of-merit (FoM) solutions without additional\npost-processing. We demonstrate our method on two canonical photonic design\nproblems--a bent waveguide and a CMOS image sensor color router--and show that\nour method consistently outperforms state-of-the-art nonlinear optimizers (such\nas MMA and SLSQP) in both efficiency and manufacturability, while using orders\nof magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning\napproaches (approximately 10^5 to 10^6). By eliminating complex binarization\nschedules and minimizing simulation overhead, AdjointDiffusion offers a\nstreamlined, simulation-efficient, and fabrication-aware pipeline for\nnext-generation photonic device design. Our open-source implementation is\navailable at https://github.com/dongjin-seo2020/AdjointDiffusion.", "AI": {"tldr": "AdjointDiffusion\u662f\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u5c06\u4f34\u968f\u654f\u611f\u5ea6\u68af\u5ea6\u878d\u5165\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eff\u771f\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\uff08\u5982\u5168\u5c40\u4f18\u5316\u6216\u4f34\u968f\u68af\u5ea6\u6cd5\uff09\u9700\u8981\u590d\u6742\u7684\u4e8c\u503c\u5316\u548c\u8fc7\u6ee4\u6b65\u9aa4\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u7b56\u7565\u9700\u8981\u5927\u91cf\u4eff\u771f\u3002AdjointDiffusion\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u8bad\u7ec3\u6269\u6563\u7f51\u7edc\u4e8e\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u4f34\u968f\u68af\u5ea6\uff0c\u5f15\u5bfc\u751f\u6210\u9ad8\u4f18\u503c\u89e3\u3002", "result": "\u5728\u6ce2\u5bfc\u548cCMOS\u56fe\u50cf\u4f20\u611f\u5668\u989c\u8272\u8def\u7531\u5668\u8bbe\u8ba1\u4e2d\uff0cAdjointDiffusion\u5728\u6548\u7387\u548c\u53ef\u5236\u9020\u6027\u4e0a\u4f18\u4e8e\u975e\u7ebf\u6027\u4f18\u5316\u5668\uff0c\u4e14\u4eff\u771f\u9700\u6c42\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "AdjointDiffusion\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4eff\u771f\u9700\u6c42\u4f4e\u4e14\u5236\u9020\u53cb\u597d\u7684\u5149\u5b50\u5668\u4ef6\u8bbe\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2504.17502", "pdf": "https://arxiv.org/pdf/2504.17502", "abs": "https://arxiv.org/abs/2504.17502", "authors": ["Aviv Slobodkin", "Hagai Taitelbaum", "Yonatan Bitton", "Brian Gordon", "Michal Sokolik", "Nitzan Bitton Guetta", "Almog Gueta", "Royi Rassin", "Itay Laish", "Dani Lischinski", "Idan Szpektor"], "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\n\\emph{Animal}, \\emph{Object}), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.", "AI": {"tldr": "RefVNLI\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e3b\u9898\u9a71\u52a8\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6587\u672c\u5bf9\u9f50\u548c\u4e3b\u9898\u4fdd\u7559\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u53ef\u9760\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4ec5\u8bc4\u4f30\u5355\u4e00\u4efb\u52a1\u65b9\u9762\uff0c\u8981\u4e48\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e0d\u4e00\u81f4\uff0c\u6216\u4f9d\u8d56\u6602\u8d35\u7684API\u8bc4\u4f30\u3002", "method": "\u5f15\u5165RefVNLI\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u89c6\u9891\u63a8\u7406\u57fa\u51c6\u548c\u56fe\u50cf\u6270\u52a8\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u8bc4\u4f30\u6587\u672c\u5bf9\u9f50\u548c\u4e3b\u9898\u4fdd\u7559\u3002", "result": "RefVNLI\u5728\u591a\u4e2a\u57fa\u51c6\u548c\u4e3b\u9898\u7c7b\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6587\u672c\u5bf9\u9f50\u63d0\u53476.4\u5206\uff0c\u4e3b\u9898\u4e00\u81f4\u6027\u63d0\u53478.5\u5206\uff0c\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u6027\u8fbe87%\u3002", "conclusion": "RefVNLI\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6210\u672c\u4f4e\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u9a71\u52a8T2I\u751f\u6210\u7684\u8bc4\u4f30\u6027\u80fd\u3002"}}
{"id": "2504.17114", "pdf": "https://arxiv.org/pdf/2504.17114", "abs": "https://arxiv.org/abs/2504.17114", "authors": ["Valentin Langer", "Kartikay Tehlan", "Thomas Wendler"], "title": "Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "The code is available under\n  https://github.com/tinolan/curve_fit_multi_idif", "summary": "Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron\nemission tomography (PET) requires anatomically constrained modelling of\nimage-derived input functions (IDIFs). Traditionally, IDIFs are obtained from\nthe aorta, neglecting anatomical variations and complex vascular contributions.\nThis study proposes a multi-organ segmentation-based approach that integrates\nIDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using\nhigh-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we\nincorporate organ-specific blood supply sources to improve kinetic modelling.\nOur method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,\nresulting in a mean squared error (MSE) reduction of $13.39\\%$ for the liver\nand $10.42\\%$ for the lungs. These initial results highlight the potential of\nmultiple IDIFs in improving anatomical modelling and fully leveraging dynamic\nPET imaging. This approach could facilitate the integration of tracer kinetic\nmodelling into clinical routine.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5668\u5b98\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u6574\u5408\u4e3b\u52a8\u8109\u3001\u95e8\u9759\u8109\u3001\u80ba\u52a8\u8109\u548c\u8f93\u5c3f\u7ba1\u7684\u56fe\u50cf\u884d\u751f\u8f93\u5165\u51fd\u6570\uff08IDIFs\uff09\uff0c\u4ee5\u63d0\u9ad8\u52a8\u6001PET\u4e2d[$^{18}$F]FDG\u5206\u5e03\u7684\u52a8\u529b\u5b66\u5206\u6790\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfIDIFs\u4ec5\u4ece\u4e3b\u52a8\u8109\u83b7\u53d6\uff0c\u5ffd\u7565\u4e86\u89e3\u5256\u53d8\u5f02\u548c\u590d\u6742\u8840\u7ba1\u8d21\u732e\uff0c\u9650\u5236\u4e86\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u9ad8\u5206\u8fa8\u7387CT\u5206\u5272\u809d\u810f\u3001\u80ba\u3001\u80be\u810f\u548c\u8180\u80f1\uff0c\u6574\u5408\u5668\u5b98\u7279\u5f02\u6027\u8840\u6db2\u4f9b\u5e94\u6e90\uff0c\u6539\u8fdb\u52a8\u529b\u5b66\u5efa\u6a21\u3002", "result": "\u5728\u4e5d\u540d\u60a3\u8005\u7684\u52a8\u6001[$^{18}$F]FDG PET\u6570\u636e\u4e2d\uff0c\u809d\u810f\u548c\u80ba\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u5206\u522b\u964d\u4f4e\u4e8613.39%\u548c10.42%\u3002", "conclusion": "\u591aIDIFs\u65b9\u6cd5\u6709\u671b\u6539\u5584\u89e3\u5256\u5efa\u6a21\uff0c\u63a8\u52a8\u52a8\u6001PET\u6210\u50cf\u7684\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2504.17515", "pdf": "https://arxiv.org/pdf/2504.17515", "abs": "https://arxiv.org/abs/2504.17515", "authors": ["Zihan Cheng", "Jintao Guo", "Jian Zhang", "Lei Qi", "Luping Zhou", "Yinghuan Shi", "Yang Gao"], "title": "Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted by IEEE TMI 2025. The code is available at\n  https://github.com/orange-czh/Mamba-Sea", "summary": "To segment medical images with distribution shifts, domain generalization\n(DG) has emerged as a promising setting to train models on source domains that\ncan generalize to unseen target domains. Existing DG methods are mainly based\non CNN or ViT architectures. Recently, advanced state space models, represented\nby Mamba, have shown promising results in various supervised medical image\nsegmentation. The success of Mamba is primarily owing to its ability to capture\nlong-range dependencies while keeping linear complexity with input sequence\nlength, making it a promising alternative to CNNs and ViTs. Inspired by the\nsuccess, in the paper, we explore the potential of the Mamba architecture to\naddress distribution shifts in DG for medical image segmentation. Specifically,\nwe propose a novel Mamba-based framework, Mamba-Sea, incorporating\nglobal-to-local sequence augmentation to improve the model's generalizability\nunder domain shift issues. Our Mamba-Sea introduces a global augmentation\nmechanism designed to simulate potential variations in appearance across\ndifferent sites, aiming to suppress the model's learning of domain-specific\ninformation. At the local level, we propose a sequence-wise augmentation along\ninput sequences, which perturbs the style of tokens within random continuous\nsub-sequences by modeling and resampling style statistics associated with\ndomain shifts. To our best knowledge, Mamba-Sea is the first work to explore\nthe generalization of Mamba for medical image segmentation, providing an\nadvanced and promising Mamba-based architecture with strong robustness to\ndomain shifts. Remarkably, our proposed method is the first to surpass a Dice\ncoefficient of 90% on the Prostate dataset, which exceeds previous SOTA of\n88.61%. The code is available at https://github.com/orange-czh/Mamba-Sea.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u67b6\u6784\u7684\u65b0\u6846\u67b6Mamba-Sea\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u5230\u5c40\u90e8\u7684\u5e8f\u5217\u589e\u5f3a\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u5206\u5e03\u504f\u79fb\u95ee\u9898\u5bfc\u81f4\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u57df\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eCNN\u6216ViT\u67b6\u6784\uff0c\u800cMamba\u56e0\u5176\u957f\u8ddd\u79bb\u4f9d\u8d56\u6355\u6349\u80fd\u529b\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u663e\u793a\u51fa\u6f5c\u529b\u3002", "method": "\u63d0\u51faMamba-Sea\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u5e8f\u5217\u589e\u5f3a\uff1a\u5168\u5c40\u673a\u5236\u6a21\u62df\u4e0d\u540c\u7ad9\u70b9\u95f4\u7684\u5916\u89c2\u53d8\u5316\uff0c\u6291\u5236\u57df\u7279\u5b9a\u4fe1\u606f\u5b66\u4e60\uff1b\u5c40\u90e8\u673a\u5236\u901a\u8fc7\u6270\u52a8\u8fde\u7eed\u5b50\u5e8f\u5217\u7684\u6837\u5f0f\u7edf\u8ba1\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728Prostate\u6570\u636e\u96c6\u4e0a\uff0cMamba-Sea\u9996\u6b21\u8d85\u8fc790%\u7684Dice\u7cfb\u6570\uff0c\u4f18\u4e8e\u4e4b\u524d88.61%\u7684SOTA\u7ed3\u679c\u3002", "conclusion": "Mamba-Sea\u662f\u9996\u4e2a\u63a2\u7d22Mamba\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6cdb\u5316\u80fd\u529b\u7684\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u57df\u504f\u79fb\u95ee\u9898\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119", "abs": "https://arxiv.org/abs/2504.17119", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\uff0c\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u548c\u4fe1\u606f\u5b66\u5bb6\u8bc6\u522b\u548c\u4f18\u5316SLMs\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u9690\u79c1\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6570\u636e\u9690\u79c1\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\u4fc3\u4f7f\u7814\u7a76\u8f6c\u5411\u66f4\u9ad8\u6548\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6846\u67b6\u5206\u6790SLMs\u5728\u4e09\u4e2a\u7ef4\u5ea6\uff08NLP\u4efb\u52a1\u3001\u5229\u76ca\u76f8\u5173\u8005\u89d2\u8272\u548c\u62a4\u7406\u8fde\u7eed\u6027\uff09\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u6784\u5efa\u3001\u4f18\u5316\u548c\u538b\u7f29\u6280\u672f\u3002", "result": "\u5c55\u793a\u4e86SLMs\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u7ed3\u679c\u7684\u5168\u9762\u6c47\u7f16\u3002", "conclusion": "SLMs\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u53d8\u9769\u533b\u7597\u4fdd\u5065\u4fe1\u606f\u5b66\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.17522", "pdf": "https://arxiv.org/pdf/2504.17522", "abs": "https://arxiv.org/abs/2504.17522", "authors": ["Anyi Xiao", "Cihui Yang"], "title": "Towards One-Stage End-to-End Table Structure Recognition with Parallel Regression for Diverse Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Table structure recognition aims to parse tables in unstructured data into\nmachine-understandable formats. Recent methods address this problem through a\ntwo-stage process or optimized one-stage approaches. However, these methods\neither require multiple networks to be serially trained and perform more\ntime-consuming sequential decoding, or rely on complex post-processing\nalgorithms to parse the logical structure of tables. They struggle to balance\ncross-scenario adaptability, robustness, and computational efficiency. In this\npaper, we propose a one-stage end-to-end table structure parsing network called\nTableCenterNet. This network unifies the prediction of table spatial and\nlogical structure into a parallel regression task for the first time, and\nimplicitly learns the spatial-logical location mapping laws of cells through a\nsynergistic architecture of shared feature extraction layers and task-specific\ndecoding. Compared with two-stage methods, our method is easier to train and\nfaster to infer. Experiments on benchmark datasets show that TableCenterNet can\neffectively parse table structures in diverse scenarios and achieve\nstate-of-the-art performance on the TableGraph-24k dataset. Code is available\nat https://github.com/dreamy-xay/TableCenterNet.", "AI": {"tldr": "TableCenterNet\u662f\u4e00\u79cd\u5355\u9636\u6bb5\u7aef\u5230\u7aef\u8868\u683c\u7ed3\u6784\u89e3\u6790\u7f51\u7edc\uff0c\u7edf\u4e00\u4e86\u8868\u683c\u7a7a\u95f4\u548c\u903b\u8f91\u7ed3\u6784\u7684\u9884\u6d4b\uff0c\u901a\u8fc7\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u5c42\u548c\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u7684\u534f\u540c\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u548c\u8de8\u573a\u666f\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5e73\u8861\u8de8\u573a\u666f\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0cTableCenterNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TableCenterNet\u901a\u8fc7\u5e76\u884c\u56de\u5f52\u4efb\u52a1\u7edf\u4e00\u9884\u6d4b\u8868\u683c\u7684\u7a7a\u95f4\u548c\u903b\u8f91\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u5c42\u548c\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u7684\u534f\u540c\u67b6\u6784\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cTableCenterNet\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728TableGraph-24k\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TableCenterNet\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u6613\u8bad\u7ec3\u4e14\u63a8\u7406\u5feb\u901f\u7684\u8868\u683c\u7ed3\u6784\u89e3\u6790\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u573a\u666f\u3002"}}
{"id": "2504.17122", "pdf": "https://arxiv.org/pdf/2504.17122", "abs": "https://arxiv.org/abs/2504.17122", "authors": ["Kartikay Tehlan", "Thomas Wendler"], "title": "Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "The code is available at: https://github.com/tkartikay/PhysNRPET", "summary": "Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables\nnon-invasive quantification of glucose metabolism through kinetic analysis,\noften modelled by the two-tissue compartment model (TCKM). However, voxel-wise\nkinetic parameter estimation using conventional methods is computationally\nintensive and limited by spatial resolution. Deep neural networks (DNNs) offer\nan alternative but require large training datasets and significant\ncomputational resources. To address these limitations, we propose a\nphysiological neural representation based on implicit neural representations\n(INRs) for personalized kinetic parameter estimation. INRs, which learn\ncontinuous functions, allow for efficient, high-resolution parametric imaging\nwith reduced data requirements. Our method also integrates anatomical priors\nfrom a 3D CT foundation model to enhance robustness and precision in kinetic\nmodelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset\nand compare it to state-of-the-art DNNs. Results demonstrate superior spatial\nresolution, lower mean-squared error, and improved anatomical consistency,\nparticularly in tumour and highly vascularized regions. Our findings highlight\nthe potential of INRs for personalized, data-efficient tracer kinetic\nmodelling, enabling applications in tumour characterization, segmentation, and\nprognostic assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u7684\u4e2a\u6027\u5316\u52a8\u529b\u5b66\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u54083D CT\u57fa\u7840\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u6001PET\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\uff0c\u6df1\u5ea6\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u6570\u636e\u9700\u6c42\u66f4\u5c11\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528INRs\u5b66\u4e60\u8fde\u7eed\u51fd\u6570\uff0c\u7ed3\u54083D CT\u89e3\u5256\u5148\u9a8c\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u53c2\u6570\u6210\u50cf\u3002", "result": "\u5728[$^{18}$F]FDG\u52a8\u6001PET/CT\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u793a\u66f4\u9ad8\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u66f4\u4f4e\u7684\u5747\u65b9\u8bef\u5dee\u548c\u66f4\u597d\u7684\u89e3\u5256\u4e00\u81f4\u6027\u3002", "conclusion": "INRs\u5728\u4e2a\u6027\u5316\u3001\u6570\u636e\u9ad8\u6548\u7684\u793a\u8e2a\u52a8\u529b\u5b66\u5efa\u6a21\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u80bf\u7624\u7279\u5f81\u5206\u6790\u3001\u5206\u5272\u548c\u9884\u540e\u8bc4\u4f30\u3002"}}
{"id": "2504.17524", "pdf": "https://arxiv.org/pdf/2504.17524", "abs": "https://arxiv.org/abs/2504.17524", "authors": ["Junyan Zhang", "Yan Li", "Mengxiao Geng", "Liu Shi", "Qiegen Liu"], "title": "ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot Learning for Color Image Inpainting", "categories": ["cs.CV"], "comment": "11 pages,10 figures,Submit to tcsvt", "summary": "Image inpainting is a technique used to restore missing or damaged regions of\nan image. Traditional methods primarily utilize information from adjacent\npixels for reconstructing missing areas, while they struggle to preserve\ncomplex details and structures. Simultaneously, models based on deep learning\nnecessitate substantial amounts of training data. To address this challenge, an\nencoding strategy-inspired diffusion model with few-shot learning for color\nimage inpainting is proposed in this paper. The main idea of this novel\nencoding strategy is the deployment of a \"virtual mask\" to construct\nhigh-dimensional objects through mutual perturbations between channels. This\napproach enables the diffusion model to capture diverse image representations\nand detailed features from limited training samples. Moreover, the encoding\nstrategy leverages redundancy between channels, integrates with low-rank\nmethods during iterative inpainting, and incorporates the diffusion model to\nachieve accurate information output. Experimental results indicate that our\nmethod exceeds current techniques in quantitative metrics, and the\nreconstructed images quality has been improved in aspects of texture and\nstructural integrity, leading to more precise and coherent results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f16\u7801\u7b56\u7565\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u5f69\u8272\u56fe\u50cf\u4fee\u590d\uff0c\u901a\u8fc7\u865a\u62df\u63a9\u7801\u548c\u9ad8\u7ef4\u5bf9\u8c61\u6784\u5efa\uff0c\u63d0\u5347\u7ec6\u8282\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u590d\u6742\u7ec6\u8282\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u5c0f\u6837\u672c\u5b66\u4e60\u65b9\u6848\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7f16\u7801\u7b56\u7565\uff0c\u5229\u7528\u865a\u62df\u63a9\u7801\u6784\u5efa\u9ad8\u7ef4\u5bf9\u8c61\uff0c\u7ed3\u5408\u4f4e\u79e9\u65b9\u6cd5\u548c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u7cbe\u786e\u4fee\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4fee\u590d\u56fe\u50cf\u7684\u7eb9\u7406\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c0f\u6837\u672c\u5b66\u4e60\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u4fee\u590d\uff0c\u7ec6\u8282\u548c\u7ed3\u6784\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2504.17124", "pdf": "https://arxiv.org/pdf/2504.17124", "abs": "https://arxiv.org/abs/2504.17124", "authors": ["Ming Du", "Mark Wolfman", "Chengjun Sun", "Shelly D. Kelly", "Mathew J. Cherukara"], "title": "Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy", "categories": ["physics.app-ph", "cs.AI", "cs.CE", "cs.SY", "eess.SY"], "comment": null, "summary": "X-ray absorption near edge structure (XANES) spectroscopy is a powerful\ntechnique for characterizing the chemical state and symmetry of individual\nelements within materials, but requires collecting data at many energy points\nwhich can be time-consuming. While adaptive sampling methods exist for\nefficiently collecting spectroscopic data, they often lack domain-specific\nknowledge about XANES spectra structure. Here we demonstrate a\nknowledge-injected Bayesian optimization approach for adaptive XANES data\ncollection that incorporates understanding of spectral features like absorption\nedges and pre-edge peaks. We show this method accurately reconstructs the\nabsorption edge of XANES spectra using only 15-20% of the measurement points\ntypically needed for conventional sampling, while maintaining the ability to\ndetermine the x-ray energy of the sharp peak after absorption edge with errors\nless than 0.03 eV, the absorption edge with errors less than 0.1 eV; and\noverall root-mean-square errors less than 0.005 compared to compared to\ntraditionally sampled spectra. Our experiments on battery materials and\ncatalysts demonstrate the method's effectiveness for both static and dynamic\nXANES measurements, improving data collection efficiency and enabling better\ntime resolution for tracking chemical changes. This approach advances the\ndegree of automation in XANES experiments reducing the common errors of under-\nor over-sampling points in near the absorption edge and enabling dynamic\nexperiments that require high temporal resolution or limited measurement time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u9002\u5e94XANES\u6570\u636e\u91c7\u96c6\uff0c\u663e\u8457\u51cf\u5c11\u6d4b\u91cf\u70b9\u6570\u91cf\uff08\u4ec5\u970015-20%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfXANES\u5149\u8c31\u6570\u636e\u91c7\u96c6\u8017\u65f6\uff0c\u4e14\u73b0\u6709\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6cd5\u7f3a\u4e4f\u5bf9XANES\u5149\u8c31\u7ed3\u6784\u7684\u9886\u57df\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u6ce8\u5165\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u5438\u6536\u8fb9\u548c\u524d\u8fb9\u5cf0\u7b49\u5149\u8c31\u7279\u5f81\u77e5\u8bc6\u3002", "result": "\u4ec5\u970015-20%\u7684\u6d4b\u91cf\u70b9\u5373\u53ef\u51c6\u786e\u91cd\u5efa\u5438\u6536\u8fb9\uff0c\u5cf0\u503c\u8bef\u5dee\u5c0f\u4e8e0.03 eV\uff0c\u5438\u6536\u8fb9\u8bef\u5dee\u5c0f\u4e8e0.1 eV\uff0c\u5747\u65b9\u6839\u8bef\u5dee\u5c0f\u4e8e0.005\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86XANES\u6570\u636e\u91c7\u96c6\u6548\u7387\uff0c\u652f\u6301\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u52a8\u6001\u5b9e\u9a8c\uff0c\u51cf\u5c11\u4e86\u91c7\u6837\u8bef\u5dee\u3002"}}
{"id": "2504.17525", "pdf": "https://arxiv.org/pdf/2504.17525", "abs": "https://arxiv.org/abs/2504.17525", "authors": ["Paul Grimal", "Herv\u00e9 Le Borgne", "Olivier Ferret"], "title": "Text-to-Image Alignment in Denoising-Based Models through Step Selection", "categories": ["cs.CV"], "comment": null, "summary": "Visual generative AI models often encounter challenges related to text-image\nalignment and reasoning limitations. This paper presents a novel method for\nselectively enhancing the signal at critical denoising steps, optimizing image\ngeneration based on input semantics. Our approach addresses the shortcomings of\nearly-stage signal modifications, demonstrating that adjustments made at later\nstages yield superior results. We conduct extensive experiments to validate the\neffectiveness of our method in producing semantically aligned images on\nDiffusion and Flow Matching model, achieving state-of-the-art performance. Our\nresults highlight the importance of a judicious choice of sampling stage to\nimprove performance and overall image alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u589e\u5f3a\u5173\u952e\u53bb\u566a\u6b65\u9aa4\u7684\u4fe1\u53f7\uff0c\u4f18\u5316\u56fe\u50cf\u751f\u6210\u4e0e\u8f93\u5165\u8bed\u4e49\u7684\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u751f\u6210AI\u6a21\u578b\u4e2d\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u63a8\u7406\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u5728\u540e\u671f\u53bb\u566a\u9636\u6bb5\u9009\u62e9\u6027\u589e\u5f3a\u4fe1\u53f7\uff0c\u907f\u514d\u65e9\u671f\u4fe1\u53f7\u4fee\u6539\u7684\u4e0d\u8db3\u3002", "result": "\u5728Diffusion\u548cFlow Matching\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u5408\u7406\u9009\u62e9\u91c7\u6837\u9636\u6bb5\u5bf9\u63d0\u5347\u6027\u80fd\u548c\u56fe\u50cf\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.17129", "pdf": "https://arxiv.org/pdf/2504.17129", "abs": "https://arxiv.org/abs/2504.17129", "authors": ["Seyed Yousef Soltanian", "Wenlong Zhang"], "title": "Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference", "categories": ["eess.SY", "cs.AI", "cs.GT", "cs.RO", "cs.SY", "93C41, 49N70, 49N90, 91A27"], "comment": null, "summary": "Human-robot interactions can be modeled as incomplete-information general-sum\ndynamic games since the objective functions of both agents are not explicitly\nknown to each other. However, solving for equilibrium policies for such games\npresents a major challenge, especially if the games involve nonlinear\nunderlying dynamics. To simplify the problem, existing work often assumes that\none agent is an expert with complete information about its peer, which can lead\nto biased estimates and failures in coordination. To address this challenge, we\npropose a nonlinear peer-aware cost estimation (N-PACE) algorithm for\ngeneral-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ)\napproximation of the nonlinear general-sum game, each agent explicitly models\nthe learning dynamics of its peer agent while inferring their objective\nfunctions, leading to unbiased fast learning in inferring the unknown objective\nfunction of the peer agent, which is critical for task completion and safety\nassurance. Additionally, we demonstrate how N-PACE enables \\textbf{intent\ncommunication} in such multi-agent systems by explicitly modeling the peer's\nlearning dynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027\u540c\u4f34\u611f\u77e5\u6210\u672c\u4f30\u8ba1\u7b97\u6cd5\uff08N-PACE\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u5b8c\u5168\u4fe1\u606f\u52a8\u6001\u535a\u5f08\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ebf\u6027\u4e8c\u6b21\u903c\u8fd1\u548c\u5efa\u6a21\u540c\u4f34\u5b66\u4e60\u52a8\u6001\uff0c\u5b9e\u73b0\u5feb\u901f\u65e0\u504f\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u4e00\u4e2a\u4ee3\u7406\u5b8c\u5168\u4e86\u89e3\u540c\u4f34\uff0c\u5bfc\u81f4\u4f30\u8ba1\u504f\u5dee\u548c\u534f\u8c03\u5931\u8d25\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "N-PACE\u901a\u8fc7\u8fed\u4ee3\u7ebf\u6027\u4e8c\u6b21\u903c\u8fd1\u975e\u7ebf\u6027\u535a\u5f08\uff0c\u5efa\u6a21\u540c\u4f34\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u63a8\u65ad\u5176\u76ee\u6807\u51fd\u6570\u3002", "result": "N-PACE\u5b9e\u73b0\u4e86\u5feb\u901f\u65e0\u504f\u5b66\u4e60\uff0c\u5e76\u652f\u6301\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u610f\u56fe\u901a\u4fe1\u3002", "conclusion": "N-PACE\u4e3a\u89e3\u51b3\u4e0d\u5b8c\u5168\u4fe1\u606f\u52a8\u6001\u535a\u5f08\u4e2d\u7684\u534f\u8c03\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u589e\u5f3a\u4e86\u4efb\u52a1\u5b8c\u6210\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2504.17540", "pdf": "https://arxiv.org/pdf/2504.17540", "abs": "https://arxiv.org/abs/2504.17540", "authors": ["Ahmadreza Shateri", "Negar Nourani", "Morteza Dorrigiv", "Hamid Nasiri"], "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u76ae\u80a4\u75c5\u53d8\u56fe\u50cf\u4e2d\u81ea\u52a8\u68c0\u6d4b\u7334\u75d8\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u3001\u964d\u7ef4\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u8bca\u65ad\u3002", "motivation": "\u7334\u75d8\u5168\u7403\u4f20\u64ad\u5f15\u53d1\u516c\u5171\u536b\u751f\u62c5\u5fe7\uff0c\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u5bf9\u75be\u75c5\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528Xception\u67b6\u6784\u63d0\u53d6\u7279\u5f81\uff0cPCA\u964d\u7ef4\uff0cNGBoost\u5206\u7c7b\uff0cAVOA\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u6a21\u578b\u51c6\u786e\u738797.53%\uff0cF1-score 97.72%\uff0cAUC 97.47%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u6709\u9650\u73af\u5883\u63d0\u4f9b\u9ad8\u6548\u8bca\u65ad\u5de5\u5177\uff0c\u652f\u6301\u65e9\u671f\u68c0\u6d4b\u3002"}}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137", "abs": "https://arxiv.org/abs/2504.17137", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE.", "AI": {"tldr": "MIRAGE\u662f\u4e00\u4e2a\u4e13\u4e3aRAG\u8bc4\u4f30\u8bbe\u8ba1\u7684\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b7,560\u4e2a\u5b9e\u4f8b\u548c37,800\u4e2a\u68c0\u7d22\u6761\u76ee\uff0c\u63d0\u4f9b\u9ad8\u6548\u8bc4\u4f30\u68c0\u7d22\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u65b0\u6307\u6807\u8861\u91cfRAG\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u8bc4\u4f30\u56f0\u96be\uff0c\u7f3a\u4e4f\u9488\u5bf9\u68c0\u7d22\u4e0e\u751f\u6210\u7ec4\u4ef6\u7684\u8be6\u7ec6\u57fa\u51c6\uff0cMIRAGE\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaMIRAGE\u6570\u636e\u96c6\uff0c\u5305\u542b7,560\u4e2a\u5b9e\u4f8b\u548c37,800\u4e2a\u68c0\u7d22\u6761\u76ee\uff0c\u5e76\u8bbe\u8ba1\u65b0\u8bc4\u4f30\u6307\u6807\uff08\u5982\u566a\u58f0\u8106\u5f31\u6027\u3001\u4e0a\u4e0b\u6587\u63a5\u53d7\u5ea6\u7b49\uff09\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86RAG\u7cfb\u7edf\u4e2d\u6a21\u578b\u5bf9\u7684\u6700\u4f18\u5bf9\u9f50\u65b9\u5f0f\u53ca\u5185\u90e8\u52a8\u6001\u5173\u7cfb\u3002", "conclusion": "MIRAGE\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u4fbf\u4e8e\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2504.17545", "pdf": "https://arxiv.org/pdf/2504.17545", "abs": "https://arxiv.org/abs/2504.17545", "authors": ["Keyang Ye", "Tianjia Shao", "Kun Zhou"], "title": "When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for\nradiance field rendering, wherein a set of 2D opaque surfels with\nview-dependent colors represent the coarse-scale geometry and appearance of\nscenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale\nappearance details. The rendering with GESs consists of two passes -- surfels\nare first rasterized through a standard graphics pipeline to produce depth and\ncolor maps, and then Gaussians are splatted with depth testing and color\naccumulation on each pixel order independently. The optimization of GESs from\nmulti-view images is performed through an elaborate coarse-to-fine procedure,\nfaithfully capturing rich scene appearance. The entirely sorting-free rendering\nof GESs not only achieves very fast rates, but also produces view-consistent\nimages, successfully avoiding popping artifacts under view changes. The basic\nGES representation can be easily extended to achieve anti-aliasing in rendering\n(Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage\n(Compact-GES), and reconstruct better scene geometries by replacing 3D\nGaussians with 2D Gaussians (2D-GES). Experimental results show that GESs\nadvance the state-of-the-arts as a compelling representation for ultra-fast\nhigh-fidelity radiance field rendering.", "AI": {"tldr": "Gaussian-enhanced Surfels (GESs) \u662f\u4e00\u79cd\u53cc\u5c3a\u5ea6\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u8f90\u5c04\u573a\u6e32\u67d3\uff0c\u7ed3\u5408\u4e862D\u4e0d\u900f\u660e\u9762\u5143\u548c3D\u9ad8\u65af\u5206\u5e03\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u4fdd\u771f\u7684\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709\u8f90\u5c04\u573a\u6e32\u67d3\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0cGESs\u65e8\u5728\u901a\u8fc7\u53cc\u5c3a\u5ea6\u8868\u793a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GESs\u901a\u8fc7\u4e24\u9636\u6bb5\u6e32\u67d3\uff08\u9762\u5143\u5149\u6805\u5316\u548c\u9ad8\u65af\u5206\u5e03\u53e0\u52a0\uff09\u548c\u7c97\u5230\u7ec6\u4f18\u5316\u8fc7\u7a0b\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\u3002", "result": "GESs\u5728\u5feb\u901f\u6e32\u67d3\u7684\u540c\u65f6\u907f\u514d\u4e86\u89c6\u89c9\u4f2a\u5f71\uff0c\u5e76\u652f\u6301\u591a\u79cd\u6269\u5c55\uff08\u5982\u6297\u952f\u9f7f\u3001\u52a0\u901f\u6e32\u67d3\u7b49\uff09\u3002", "conclusion": "GESs\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u7684\u8f90\u5c04\u573a\u8868\u793a\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2504.17140", "pdf": "https://arxiv.org/pdf/2504.17140", "abs": "https://arxiv.org/abs/2504.17140", "authors": ["Ashish Ranjan", "Ayush Agarwal", "Shalin Barot", "Sushant Kumar"], "title": "Scalable Permutation-Aware Modeling for Temporal Set Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal set prediction involves forecasting the elements that will appear in\nthe next set, given a sequence of prior sets, each containing a variable number\nof elements. Existing methods often rely on intricate architectures with\nsubstantial computational overhead, which hampers their scalability. In this\nwork, we introduce a novel and scalable framework that leverages\npermutation-equivariant and permutation-invariant transformations to\nefficiently model set dynamics. Our approach significantly reduces both\ntraining and inference time while maintaining competitive performance.\nExtensive experiments on multiple public benchmarks show that our method\nachieves results on par with or superior to state-of-the-art models across\nseveral evaluation metrics. These results underscore the effectiveness of our\nmodel in enabling efficient and scalable temporal set prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65f6\u5e8f\u96c6\u5408\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7f6e\u6362\u7b49\u53d8\u548c\u7f6e\u6362\u4e0d\u53d8\u53d8\u6362\u5efa\u6a21\u96c6\u5408\u52a8\u6001\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u67b6\u6784\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5229\u7528\u7f6e\u6362\u7b49\u53d8\u548c\u7f6e\u6362\u4e0d\u53d8\u53d8\u6362\u9ad8\u6548\u5efa\u6a21\u96c6\u5408\u52a8\u6001\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4e0e\u6216\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65f6\u5e8f\u96c6\u5408\u9884\u6d4b\u3002"}}
{"id": "2504.17547", "pdf": "https://arxiv.org/pdf/2504.17547", "abs": "https://arxiv.org/abs/2504.17547", "authors": ["Jiaqi Deng", "Zonghan Wu", "Huan Huo", "Guandong Xu"], "title": "A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems: The Lifecycle of Knowledge in Visual Reasoning Task", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "20 pages, 5 figures, 4 tables", "summary": "Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u77e5\u8bc6\u9a71\u52a8\u7684\u89c6\u89c9\u95ee\u7b54\uff08KB-VQA\uff09\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u6574\u7406\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "KB-VQA\u7ed3\u5408\u89c6\u89c9\u3001\u6587\u672c\u548c\u5916\u90e8\u77e5\u8bc6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u7684\u7efc\u8ff0\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u5206\u7c7b\u6846\u67b6\uff0c\u5c06KB-VQA\u65b9\u6cd5\u5206\u4e3a\u77e5\u8bc6\u8868\u793a\u3001\u77e5\u8bc6\u68c0\u7d22\u548c\u77e5\u8bc6\u63a8\u7406\u4e09\u4e2a\u9636\u6bb5\uff0c\u5e76\u5206\u6790\u5404\u9636\u6bb5\u7684\u6280\u672f\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u73b0\u6709KB-VQA\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u6280\u672f\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3aKB-VQA\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u53c2\u8003\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.17160", "pdf": "https://arxiv.org/pdf/2504.17160", "abs": "https://arxiv.org/abs/2504.17160", "authors": ["Alberto Fern\u00e1ndez-Hern\u00e1ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ort\u00ed"], "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "10 pages, 3 figures", "summary": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOUI\u7684\u65b0\u5de5\u5177\uff0c\u7528\u4e8e\u76d1\u6d4b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u5e2e\u52a9\u9009\u62e9\u6700\u4f73\u6b63\u5219\u5316\u8d85\u53c2\u6570\uff08\u5982Weight Decay\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0cOUI\u80fd\u5feb\u901f\u6307\u793a\u6a21\u578b\u662f\u5426\u8fc7\u62df\u5408\u6216\u6b20\u62df\u5408\uff0c\u65e0\u9700\u9a8c\u8bc1\u6570\u636e\uff0c\u4e14\u6bd4\u4f20\u7edf\u6307\u6807\u66f4\u5feb\u6536\u655b\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\uff0c\u9009\u62e9\u5408\u9002\u7684\u6b63\u5219\u5316\u8d85\u53c2\u6570\uff08\u5982Weight Decay\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9a8c\u8bc1\u6570\u636e\u4e14\u8017\u65f6\uff0cOUI\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faOUI\u4f5c\u4e3a\u8bad\u7ec3\u52a8\u6001\u76d1\u6d4b\u5de5\u5177\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5728\u591a\u79cd\u6570\u636e\u96c6\uff08CIFAR-100\u3001TinyImageNet\u3001ImageNet-1K\uff09\u548c\u6a21\u578b\uff08DenseNet-BC-100\u3001EfficientNet-B0\u3001ResNet-34\uff09\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "OUI\u80fd\u5feb\u901f\u6307\u793a\u6a21\u578b\u662f\u5426\u8fc7\u62df\u5408\u6216\u6b20\u62df\u5408\uff0c\u5e2e\u52a9\u5728\u8bad\u7ec3\u65e9\u671f\u786e\u5b9a\u6700\u4f73Weight Decay\u503c\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u9a8c\u8bc1\u5206\u6570\u3002", "conclusion": "OUI\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u53ef\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u66f4\u7cbe\u786e\u5730\u8c03\u6574\u6b63\u5219\u5316\u8d85\u53c2\u6570\uff0c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.17551", "pdf": "https://arxiv.org/pdf/2504.17551", "abs": "https://arxiv.org/abs/2504.17551", "authors": ["Lin Che", "Yizi Chen", "Tanhua Jin", "Martin Raubal", "Konrad Schindler", "Peter Kiefer"], "title": "Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures, preprint version", "summary": "Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8857\u666f\u56fe\u50cf\u7684\u65e0\u76d1\u7763\u5bf9\u6bd4\u805a\u7c7b\u6a21\u578b\uff0c\u7ed3\u5408\u5730\u7406\u5148\u9a8c\uff0c\u7528\u4e8e\u590d\u6742\u57ce\u5e02\u573a\u666f\u7684\u571f\u5730\u5229\u7528\u5206\u7c7b\u4e0e\u5236\u56fe\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u6280\u672f\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7f3a\u4e4f\u7cbe\u5ea6\uff0c\u800c\u8857\u666f\u56fe\u50cf\u80fd\u6355\u6349\u5730\u9762\u7ec6\u8282\u548c\u4eba\u7c7b\u6d3b\u52a8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u5206\u7c7b\uff0c\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u6cdb\u5316\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u5bf9\u6bd4\u805a\u7c7b\u6a21\u578b\uff0c\u7ed3\u5408\u5730\u7406\u5148\u9a8c\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u89c6\u89c9\u5206\u914d\u5b9e\u73b0\u571f\u5730\u5229\u7528\u5236\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ece\u4e24\u4e2a\u57ce\u5e02\u7684\u8857\u666f\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u751f\u6210\u571f\u5730\u5229\u7528\u5730\u56fe\uff0c\u5177\u6709\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5730\u7406\u7a7a\u95f4\u6570\u636e\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u8857\u666f\u56fe\u50cf\u53ef\u7528\u7684\u591a\u79cd\u573a\u666f\uff0c\u652f\u6301\u65e0\u76d1\u7763\u571f\u5730\u5229\u7528\u5236\u56fe\u4e0e\u66f4\u65b0\u3002"}}
{"id": "2504.17582", "pdf": "https://arxiv.org/pdf/2504.17582", "abs": "https://arxiv.org/abs/2504.17582", "authors": ["Zebo Huang", "Yinghui Wang"], "title": "Occlusion-Aware Self-Supervised Monocular Depth Estimation for Weak-Texture Endoscopic Images", "categories": ["cs.CV"], "comment": null, "summary": "We propose a self-supervised monocular depth estimation network tailored for\nendoscopic scenes, aiming to infer depth within the gastrointestinal tract from\nmonocular images. Existing methods, though accurate, typically assume\nconsistent illumination, which is often violated due to dynamic lighting and\nocclusions caused by GI motility. These variations lead to incorrect geometric\ninterpretations and unreliable self-supervised signals, degrading depth\nreconstruction quality. To address this, we introduce an occlusion-aware\nself-supervised framework. First, we incorporate an occlusion mask for data\naugmentation, generating pseudo-labels by simulating viewpoint-dependent\nocclusion scenarios. This enhances the model's ability to learn robust depth\nfeatures under partial visibility. Second, we leverage semantic segmentation\nguided by non-negative matrix factorization, clustering convolutional\nactivations to generate pseudo-labels in texture-deprived regions, thereby\nimproving segmentation accuracy and mitigating information loss from lighting\nchanges. Experimental results on the SCARED dataset show that our method\nachieves state-of-the-art performance in self-supervised depth estimation.\nAdditionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate\nstrong generalization across diverse endoscopic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5185\u7aa5\u955c\u573a\u666f\u7684\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u5149\u7167\u548c\u906e\u6321\u5bfc\u81f4\u7684\u6df1\u5ea6\u91cd\u5efa\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u5149\u7167\u4e00\u81f4\uff0c\u4f46\u5185\u7aa5\u955c\u573a\u666f\u4e2d\u52a8\u6001\u5149\u7167\u548c\u80c3\u80a0\u9053\u8fd0\u52a8\u5bfc\u81f4\u7684\u906e\u6321\u4f1a\u7834\u574f\u8fd9\u4e00\u5047\u8bbe\uff0c\u5bfc\u81f4\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002", "method": "\u5f15\u5165\u906e\u6321\u611f\u77e5\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u7684\u906e\u6321\u63a9\u7801\u548c\u57fa\u4e8e\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u7684\u8bed\u4e49\u5206\u5272\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u90e8\u5206\u53ef\u89c1\u6027\u548c\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728SCARED\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728Endo-SLAM\u548cSERV-CT\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5185\u7aa5\u955c\u573a\u666f\u4e0b\u7684\u81ea\u76d1\u7763\u6df1\u5ea6\u4f30\u8ba1\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2504.17170", "pdf": "https://arxiv.org/pdf/2504.17170", "abs": "https://arxiv.org/abs/2504.17170", "authors": ["Robert Kaufman"], "title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "PhD Dissertation from University of California, San Diego; 175 pages", "summary": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u5982\u4f55\u6ee1\u8db3\u4e58\u5ba2\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u5f3a\u8c03\u4e2a\u6027\u5316\u3001\u60c5\u5883\u654f\u611f\u7684\u901a\u4fe1\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u4e09\u9879\u5b9e\u8bc1\u7814\u7a76\u652f\u6301\u8fd9\u4e00\u89c2\u70b9\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u666e\u53ca\u53d7\u9650\u4e8e\u5982\u4f55\u6ee1\u8db3\u4e0d\u540c\u4e58\u5ba2\u548c\u60c5\u5883\u4e0b\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5ffd\u89c6\u4e86\u4e2a\u4f53\u5dee\u5f02\u548c\u60c5\u5883\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u4e09\u9879\u5b9e\u8bc1\u7814\u7a76\uff1a1\uff09\u6781\u7aef\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u901a\u4fe1\u7b56\u7565\u4f18\u5316\uff1b2\uff09\u9519\u8bef\u901a\u4fe1\u7cfb\u7edf\u7684\u540e\u679c\u5206\u6790\uff1b3\uff09\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u4e58\u5ba2\u5bf9AV\u7684\u4fe1\u4efb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4efb\u52a1\u654f\u611f\u3001\u60c5\u5883\u654f\u611f\u7684\u901a\u4fe1\u7b56\u7565\u80fd\u63d0\u5347\u9a7e\u9a76\u8868\u73b0\u548c\u4fe1\u4efb\uff0c\u4e2a\u6027\u5316\u8bbe\u8ba1\u5bf9\u6ee1\u8db3\u4e2a\u4f53\u9700\u6c42\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8bba\u6587\u4e3b\u5f20\u900f\u660e\u3001\u53ef\u9002\u5e94\u4e14\u4e2a\u6027\u5316\u7684AV\u7cfb\u7edf\uff0c\u4e3a\u8bbe\u8ba1\u8005\u3001\u7814\u7a76\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2504.17594", "pdf": "https://arxiv.org/pdf/2504.17594", "abs": "https://arxiv.org/abs/2504.17594", "authors": ["Zhaofeng Si", "Siwei Lyu"], "title": "Tamper-evident Image using JPEG Fixed Points", "categories": ["cs.CV", "I.4.7"], "comment": "6 pages, 6 figures", "summary": "An intriguing phenomenon about JPEG compression has been observed since two\ndecades ago- after repeating JPEG compression and decompression, it leads to a\nstable image that does not change anymore, which is a fixed point. In this\nwork, we prove the existence of fixed points in the essential JPEG procedures.\nWe analyze JPEG compression and decompression processes, revealing the\nexistence of fixed points that can be reached within a few iterations. These\nfixed points are diverse and preserve the image's visual quality, ensuring\nminimal distortion. This result is used to develop a method to create a\ntamper-evident image from the original authentic image, which can expose\ntampering operations by showing deviations from the fixed point image.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86JPEG\u538b\u7f29\u8fc7\u7a0b\u4e2d\u5b58\u5728\u56fa\u5b9a\u70b9\uff0c\u5e76\u5229\u7528\u8fd9\u4e00\u7279\u6027\u5f00\u53d1\u4e86\u4e00\u79cd\u9632\u7be1\u6539\u56fe\u50cf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76JPEG\u538b\u7f29\u91cd\u590d\u64cd\u4f5c\u540e\u56fe\u50cf\u7a33\u5b9a\u6027\u7684\u73b0\u8c61\uff0c\u63a2\u7d22\u5176\u6f5c\u5728\u5e94\u7528\u3002", "method": "\u5206\u6790JPEG\u538b\u7f29\u4e0e\u89e3\u538b\u7f29\u8fc7\u7a0b\uff0c\u8bc1\u660e\u56fa\u5b9a\u70b9\u7684\u5b58\u5728\u53ca\u5176\u5feb\u901f\u6536\u655b\u6027\u3002", "result": "\u56fa\u5b9a\u70b9\u591a\u6837\u4e14\u89c6\u89c9\u8d28\u91cf\u9ad8\uff0c\u53ef\u7528\u4e8e\u751f\u6210\u9632\u7be1\u6539\u56fe\u50cf\u3002", "conclusion": "\u56fa\u5b9a\u70b9\u7684\u53d1\u73b0\u4e3a\u56fe\u50cf\u9632\u7be1\u6539\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.17595", "pdf": "https://arxiv.org/pdf/2504.17595", "abs": "https://arxiv.org/abs/2504.17595", "authors": ["Boyue Xu", "Yi Xu", "Ruichao Hou", "Jia Bei", "Tongwei Ren", "Gangshan Wu"], "title": "RGB-D Tracking via Hierarchical Modality Aggregation and Distribution Network", "categories": ["cs.CV"], "comment": null, "summary": "The integration of dual-modal features has been pivotal in advancing\nRGB-Depth (RGB-D) tracking. However, current trackers are less efficient and\nfocus solely on single-level features, resulting in weaker robustness in fusion\nand slower speeds that fail to meet the demands of real-world applications. In\nthis paper, we introduce a novel network, denoted as HMAD (Hierarchical\nModality Aggregation and Distribution), which addresses these challenges. HMAD\nleverages the distinct feature representation strengths of RGB and depth\nmodalities, giving prominence to a hierarchical approach for feature\ndistribution and fusion, thereby enhancing the robustness of RGB-D tracking.\nExperimental results on various RGB-D datasets demonstrate that HMAD achieves\nstate-of-the-art performance. Moreover, real-world experiments further validate\nHMAD's capacity to effectively handle a spectrum of tracking challenges in\nreal-time scenarios.", "AI": {"tldr": "HMAD\u662f\u4e00\u79cd\u65b0\u578bRGB-D\u8ddf\u8e2a\u7f51\u7edc\uff0c\u901a\u8fc7\u5206\u5c42\u6a21\u6001\u805a\u5408\u4e0e\u5206\u5e03\u63d0\u5347\u7279\u5f81\u878d\u5408\u7684\u9c81\u68d2\u6027\u548c\u901f\u5ea6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u4e14\u9002\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\u3002", "motivation": "\u5f53\u524dRGB-D\u8ddf\u8e2a\u5668\u6548\u7387\u4f4e\u4e14\u4ec5\u5173\u6ce8\u5355\u5c42\u7279\u5f81\uff0c\u5bfc\u81f4\u878d\u5408\u9c81\u68d2\u6027\u5dee\u3001\u901f\u5ea6\u6162\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51faHMAD\u7f51\u7edc\uff0c\u5229\u7528RGB\u548c\u6df1\u5ea6\u6a21\u6001\u7684\u72ec\u7279\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\u8fdb\u884c\u7279\u5f81\u5206\u5e03\u4e0e\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2aRGB-D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u6709\u6548\u5e94\u5bf9\u591a\u79cd\u8ddf\u8e2a\u6311\u6218\u3002", "conclusion": "HMAD\u901a\u8fc7\u5206\u5c42\u6a21\u6001\u805a\u5408\u4e0e\u5206\u5e03\u663e\u8457\u63d0\u5347\u4e86RGB-D\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027\u548c\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.17198", "pdf": "https://arxiv.org/pdf/2504.17198", "abs": "https://arxiv.org/abs/2504.17198", "authors": ["XiangRui Zhang", "HaoYu Chen", "Yongzhong He", "Wenjia Niu", "Qiang Li"], "title": "Automatically Generating Rules of Malicious Software Packages via Large Language Model", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": "14 pages, 11 figures", "summary": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories.", "AI": {"tldr": "RuleLLM\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u89c4\u5219\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u653b\u51fb\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5de5\u5177\u4f9d\u8d56\u4e13\u5bb6\u9884\u5b9a\u4e49\u89c4\u5219\uff0c\u96be\u4ee5\u5e94\u5bf9\u65b0\u5174\u7684\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u653b\u51fb\u3002", "method": "RuleLLM\u4ece\u6076\u610f\u8f6f\u4ef6\u4e2d\u63d0\u53d6\u5143\u6570\u636e\u548c\u4ee3\u7801\u7247\u6bb5\uff0c\u751f\u6210\u53ef\u76f4\u63a5\u90e8\u7f72\u7684YARA\u548cSemgrep\u89c4\u5219\uff0c\u5305\u62ec\u89c4\u5219\u751f\u6210\u3001\u4f18\u5316\u548c\u5bf9\u9f50\u4e09\u4e2a\u5b50\u4efb\u52a1\u3002", "result": "\u57281,633\u4e2a\u6076\u610f\u5305\u6570\u636e\u96c6\u4e0a\uff0cRuleLLM\u751f\u6210\u4e86763\u6761\u89c4\u5219\uff08452 YARA\u548c311 Semgrep\uff09\uff0c\u51c6\u786e\u738785.2%\uff0c\u53ec\u56de\u738791.8%\uff0c\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "conclusion": "RuleLLM\u901a\u8fc7\u81ea\u52a8\u5316\u89c4\u5219\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u5de5\u5177\u7684\u9002\u5e94\u6027\u548c\u68c0\u6d4b\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u4e8611\u7c7b38\u5b50\u7c7b\u7684\u89c4\u5219\u5206\u7c7b\u6cd5\u3002"}}
{"id": "2504.17609", "pdf": "https://arxiv.org/pdf/2504.17609", "abs": "https://arxiv.org/abs/2504.17609", "authors": ["Fengchun Liu", "Tong Zhang", "Chunying Zhang"], "title": "STCL:Curriculum learning Strategies for deep learning image steganography models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u56fe\u50cf\u9690\u5199\u8bad\u7ec3\u7b56\u7565\uff08STCL\uff09\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u8bad\u7ec3\u96be\u5ea6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u9690\u5199\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u5dee\u548c\u7f51\u7edc\u6536\u655b\u6162\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "method": "1. \u57fa\u4e8e\u6559\u5e08\u6a21\u578b\u7684\u96be\u5ea6\u8bc4\u4f30\u7b56\u7565\uff1b2. \u57fa\u4e8e\u62d0\u70b9\u7684\u8bad\u7ec3\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86STCL\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86PSNR\u3001SSIM\u548c\u89e3\u7801\u51c6\u786e\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u9690\u5199\u5206\u6790\u5f97\u5206\u3002", "conclusion": "STCL\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u5347\u56fe\u50cf\u9690\u5199\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7b97\u6cd5\u6846\u67b6\u3002"}}
{"id": "2504.17210", "pdf": "https://arxiv.org/pdf/2504.17210", "abs": "https://arxiv.org/abs/2504.17210", "authors": ["Junfei Wang", "Darshana Upadhyay", "Marzia Zaman", "Pirathayini Srikantha"], "title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to IEEE SmartGridComm Conference 2025", "summary": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDDPM\u7684\u7269\u7406\u4fe1\u606f\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u53ef\u884c\u7684\u7535\u529b\u6f6e\u6d41\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u6570\u636e\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u667a\u80fd\u7535\u7f51\u4e2d\u8bb8\u591a\u6570\u636e\u9a71\u52a8\u6a21\u5757\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u7535\u529b\u6f6e\u6d41\u6570\u636e\uff0c\u4f46\u5b9e\u9645\u6570\u636e\u5e38\u56e0\u9690\u79c1\u548c\u64cd\u4f5c\u9650\u5236\u800c\u4e0d\u8db3\u3002", "method": "\u91c7\u7528DDPM\u6846\u67b6\uff0c\u7ed3\u5408\u8f85\u52a9\u8bad\u7ec3\u548c\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fdd\u751f\u6210\u6570\u636e\u65e2\u5177\u7edf\u8ba1\u4fdd\u771f\u5ea6\u53c8\u7b26\u5408\u7535\u529b\u7cfb\u7edf\u53ef\u884c\u6027\u3002", "result": "\u5728IEEE 14-bus\u548c30-bus\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u5728\u53ef\u884c\u6027\u3001\u591a\u6837\u6027\u548c\u7edf\u8ba1\u7279\u5f81\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u6570\u636e\u9a71\u52a8\u7535\u529b\u7cfb\u7edf\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.17619", "pdf": "https://arxiv.org/pdf/2504.17619", "abs": "https://arxiv.org/abs/2504.17619", "authors": ["Catarina P. Coutinho", "Aneeqa Merhab", "Janko Petkovic", "Ferdinando Zanchetta", "Rita Fioresi"], "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to the 7th International Conference on Geometric Science of\n  Information", "summary": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images.", "AI": {"tldr": "\u5229\u7528\u89c6\u89c9\u76ae\u5c42\u8fb9\u754c\u8865\u5168\u673a\u5236\u6539\u8fdbCNN\u6ee4\u6ce2\u5668\uff0c\u63d0\u5347\u906e\u6321MNIST\u56fe\u50cf\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u76ae\u5c42\u673a\u5236\u5728CNN\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5bf9\u906e\u6321\u56fe\u50cf\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9\u76ae\u5c42\u8fb9\u754c\u8865\u5168\u7684\u6570\u5b66\u6a21\u578b\uff0c\u8bbe\u8ba1\u5b9a\u5236\u5316\u6ee4\u6ce2\u5668\uff0c\u6539\u8fdbLeNet 5\u3002", "result": "\u5728\u906e\u6321MNIST\u56fe\u50cf\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u89c6\u89c9\u76ae\u5c42\u673a\u5236\u7684\u6570\u5b66\u5efa\u6a21\u53ef\u6709\u6548\u6539\u8fdbCNN\u6027\u80fd\u3002"}}
{"id": "2504.17626", "pdf": "https://arxiv.org/pdf/2504.17626", "abs": "https://arxiv.org/abs/2504.17626", "authors": ["Ashish Singh", "Michael J. Jones", "Kuan-Chuan Peng", "Anoop Cherian", "Moitreya Chatterjee", "Erik Learned-Miller"], "title": "Improving Open-World Object Localization by Discovering Background", "categories": ["cs.CV"], "comment": null, "summary": "Our work addresses the problem of learning to localize objects in an\nopen-world setting, i.e., given the bounding box information of a limited\nnumber of object classes during training, the goal is to localize all objects,\nbelonging to both the training and unseen classes in an image, during\ninference. Towards this end, recent work in this area has focused on improving\nthe characterization of objects either explicitly by proposing new objective\nfunctions (localization quality) or implicitly using object-centric\nauxiliary-information, such as depth information, pixel/region affinity map\netc. In this work, we address this problem by incorporating background\ninformation to guide the learning of the notion of objectness. Specifically, we\npropose a novel framework to discover background regions in an image and train\nan object proposal network to not detect any objects in these regions. We\nformulate the background discovery task as that of identifying image regions\nthat are not discriminative, i.e., those that are redundant and constitute low\ninformation content. We conduct experiments on standard benchmarks to showcase\nthe effectiveness of our proposed approach and observe significant improvements\nover the previous state-of-the-art approaches for this task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u80cc\u666f\u4fe1\u606f\u6307\u5bfc\u76ee\u6807\u5b9a\u4f4d\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u975e\u5224\u522b\u6027\u533a\u57df\u6765\u63d0\u5347\u5f00\u653e\u4e16\u754c\u76ee\u6807\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u76ee\u6807\u5b9a\u4f4d\u95ee\u9898\uff0c\u5373\u5728\u8bad\u7ec3\u65f6\u4ec5\u4f7f\u7528\u6709\u9650\u7c7b\u522b\u8fb9\u754c\u6846\u4fe1\u606f\uff0c\u5728\u63a8\u7406\u65f6\u5b9a\u4f4d\u6240\u6709\u7c7b\u522b\uff08\u5305\u62ec\u672a\u89c1\u7c7b\u522b\uff09\u7684\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u53d1\u73b0\u56fe\u50cf\u4e2d\u7684\u80cc\u666f\u533a\u57df\uff08\u975e\u5224\u522b\u6027\u533a\u57df\uff09\uff0c\u8bad\u7ec3\u76ee\u6807\u63d0\u8bae\u7f51\u7edc\u907f\u514d\u5728\u8fd9\u4e9b\u533a\u57df\u68c0\u6d4b\u76ee\u6807\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5229\u7528\u80cc\u666f\u4fe1\u606f\u80fd\u6709\u6548\u63d0\u5347\u5f00\u653e\u4e16\u754c\u76ee\u6807\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2504.17219", "pdf": "https://arxiv.org/pdf/2504.17219", "abs": "https://arxiv.org/abs/2504.17219", "authors": ["Hyomin Lee", "Minseon Kim", "Sangwon Jang", "Jongheon Jeong", "Sung Ju Hwang"], "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness.", "AI": {"tldr": "SRL-VAE\u662f\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u6ed1\u6f5c\u5728\u7a7a\u95f4\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u4fdd\u771f\u5ea6\u3002", "motivation": "\u63a2\u7d22\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAEs\uff09\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u6311\u6218\u5bf9\u6297\u8bad\u7ec3\u4f1a\u964d\u4f4e\u751f\u6210\u6a21\u578b\u6027\u80fd\u7684\u5047\u8bbe\u3002", "method": "\u63d0\u51faSRL-VAE\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6270\u52a8\u5e73\u6ed1\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u539f\u59cb\u8868\u793a\u6b63\u5219\u5316\u4ee5\u7ef4\u6301\u4fdd\u771f\u5ea6\u3002", "result": "SRL-VAE\u5728\u56fe\u50cf\u91cd\u5efa\u3001\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u53ca\u5bf9\u6297\u653b\u51fb\uff08\u5982Nightshade\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u5bf9\u6297\u8bad\u7ec3\u53ef\u540c\u65f6\u589e\u5f3a\u751f\u6210\u6a21\u578b\u7684\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2504.17636", "pdf": "https://arxiv.org/pdf/2504.17636", "abs": "https://arxiv.org/abs/2504.17636", "authors": ["Vojtech Panek", "Qunjie Zhou", "Yaqing Ding", "S\u00e9rgio Agostinho", "Zuzana Kukelova", "Torsten Sattler", "Laura Leal-Taix\u00e9"], "title": "A Guide to Structureless Visual Localization", "categories": ["cs.CV", "I.2.10; I.4.8; I.4.9"], "comment": null, "summary": "Visual localization algorithms, i.e., methods that estimate the camera pose\nof a query image in a known scene, are core components of many applications,\nincluding self-driving cars and augmented / mixed reality systems.\nState-of-the-art visual localization algorithms are structure-based, i.e., they\nstore a 3D model of the scene and use 2D-3D correspondences between the query\nimage and 3D points in the model for camera pose estimation. While such\napproaches are highly accurate, they are also rather inflexible when it comes\nto adjusting the underlying 3D model after changes in the scene. Structureless\nlocalization approaches represent the scene as a database of images with known\nposes and thus offer a much more flexible representation that can be easily\nupdated by adding or removing images. Although there is a large amount of\nliterature on structure-based approaches, there is significantly less work on\nstructureless methods. Hence, this paper is dedicated to providing the, to the\nbest of our knowledge, first comprehensive discussion and comparison of\nstructureless methods. Extensive experiments show that approaches that use a\nhigher degree of classical geometric reasoning generally achieve higher pose\naccuracy. In particular, approaches based on classical absolute or\nsemi-generalized relative pose estimation outperform very recent methods based\non pose regression by a wide margin. Compared with state-of-the-art\nstructure-based approaches, the flexibility of structureless methods comes at\nthe cost of (slightly) lower pose accuracy, indicating an interesting direction\nfor future work.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u8ba8\u8bba\u548c\u6bd4\u8f83\u4e86\u65e0\u7ed3\u6784\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u7ecf\u5178\u51e0\u4f55\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u59ff\u6001\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u59ff\u6001\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u4f46\u7075\u6d3b\u6027\u66f4\u9ad8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ed3\u6784\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u9002\u5e94\u573a\u666f\u53d8\u5316\u3002\u65e0\u7ed3\u6784\u65b9\u6cd5\u66f4\u7075\u6d3b\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u8f83\u5c11\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u548c\u5206\u6790\u4e0d\u540c\u65e0\u7ed3\u6784\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u7ecf\u5178\u7edd\u5bf9\u6216\u534a\u5e7f\u4e49\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u59ff\u6001\u56de\u5f52\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u7ecf\u5178\u51e0\u4f55\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u59ff\u6001\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u59ff\u6001\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u4f46\u4e0e\u57fa\u4e8e\u7ed3\u6784\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7cbe\u5ea6\u7a0d\u4f4e\u3002", "conclusion": "\u65e0\u7ed3\u6784\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u548c\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u8da3\u7684\u65b9\u5411\u3002"}}
{"id": "2504.17243", "pdf": "https://arxiv.org/pdf/2504.17243", "abs": "https://arxiv.org/abs/2504.17243", "authors": ["Xinyu Zhou", "Simin Fan", "Martin Jaggi", "Jie Fu"], "title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint, 16 pages", "summary": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability.", "AI": {"tldr": "NeuralGrok\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u68af\u5ea6\u5206\u91cf\u52a0\u901fTransformer\u5728\u7b97\u672f\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u3002", "motivation": "\u7814\u7a76Grokking\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5728\u957f\u65f6\u95f4\u8fc7\u62df\u5408\u540e\u7a81\u7136\u6cdb\u5316\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "NeuralGrok\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u8f85\u52a9\u6a21\u5757\uff08\u5982MLP\u5757\uff09\u52a8\u6001\u8c03\u6574\u68af\u5ea6\u5206\u91cf\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u7b97\u6cd5\u6307\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8868\u660eNeuralGrok\u663e\u8457\u52a0\u901f\u6cdb\u5316\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7AGE\u6307\u6807\u9a8c\u8bc1\u5176\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u6548\u679c\u3002", "conclusion": "NeuralGrok\u4e3a\u7406\u89e3Transformer\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u7b97\u672f\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.17643", "pdf": "https://arxiv.org/pdf/2504.17643", "abs": "https://arxiv.org/abs/2504.17643", "authors": ["Steve G\u00f6ring"], "title": "CLIPSE -- a minimalistic CLIP-based image search engine for research", "categories": ["cs.CV"], "comment": null, "summary": "A brief overview of CLIPSE, a self-hosted image search engine with the main\napplication of research, is provided. In general, CLIPSE uses CLIP embeddings\nto process the images and also the text queries. The overall framework is\ndesigned with simplicity to enable easy extension and usage. Two benchmark\nscenarios are described and evaluated, covering indexing and querying time. It\nis shown that CLIPSE is capable of handling smaller datasets; for larger\ndatasets, a distributed approach with several instances should be considered.", "AI": {"tldr": "CLIPSE\u662f\u4e00\u4e2a\u57fa\u4e8eCLIP\u5d4c\u5165\u7684\u81ea\u6258\u7ba1\u56fe\u50cf\u641c\u7d22\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u7814\u7a76\u573a\u666f\uff0c\u652f\u6301\u7b80\u5355\u6269\u5c55\u548c\u4f7f\u7528\uff0c\u4f46\u5728\u5927\u6570\u636e\u96c6\u4e0a\u9700\u5206\u5e03\u5f0f\u5904\u7406\u3002", "motivation": "\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u56fe\u50cf\u641c\u7d22\u5f15\u64ce\u3002", "method": "\u4f7f\u7528CLIP\u5d4c\u5165\u5904\u7406\u56fe\u50cf\u548c\u6587\u672c\u67e5\u8be2\uff0c\u8bbe\u8ba1\u7b80\u5355\u6846\u67b6\u3002", "result": "\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5927\u6570\u636e\u96c6\u9700\u5206\u5e03\u5f0f\u5904\u7406\u3002", "conclusion": "CLIPSE\u9002\u5408\u5c0f\u89c4\u6a21\u7814\u7a76\uff0c\u5927\u89c4\u6a21\u5e94\u7528\u9700\u5206\u5e03\u5f0f\u65b9\u6848\u3002"}}
{"id": "2504.17247", "pdf": "https://arxiv.org/pdf/2504.17247", "abs": "https://arxiv.org/abs/2504.17247", "authors": ["Diogo Soares", "Leon Hetzel", "Paulina Szymczak", "Fabian Theis", "Stephan G\u00fcnnemann", "Ewa Szczurek"], "title": "Targeted AMP generation through controlled diffusion with efficient embeddings", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance.", "AI": {"tldr": "OmegAMP\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u5177\u6709\u7279\u5b9a\u6027\u8d28\u7684\u6297\u83cc\u80bd\uff08AMP\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9e\u9a8c\u547d\u4e2d\u7387\u548c\u591a\u6837\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728AMP\u53d1\u73b0\u4e2d\u5b9e\u9a8c\u547d\u4e2d\u7387\u4f4e\u3001\u53ef\u63a7\u6027\u4e0d\u8db3\u548c\u80bd\u6027\u8d28\u5efa\u6a21\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6269\u6563\u751f\u6210\u6a21\u578b\u7ed3\u5408\u4f4e\u7ef4\u5d4c\u5165\u3001\u7cbe\u786e\u53ef\u63a7\u673a\u5236\u548c\u65b0\u578b\u5206\u7c7b\u5668\uff0c\u51cf\u5c11\u5047\u9633\u6027\u7387\u3002", "result": "OmegAMP\u5728AMP\u53d1\u73b0\u6d41\u7a0b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6846\u67b6\u5bf9\u6297\u83cc\u8010\u836f\u6027\u7684\u6f5c\u529b\u3002", "conclusion": "OmegAMP\u4e3aAMP\u53d1\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u63a7\u4e14\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u6846\u67b6\u7684\u5e94\u7528\u3002"}}
{"id": "2504.17670", "pdf": "https://arxiv.org/pdf/2504.17670", "abs": "https://arxiv.org/abs/2504.17670", "authors": ["Lutao Jiang", "Jiantao Lin", "Kanghao Chen", "Wenhang Ge", "Xin Yang", "Yifan Jiang", "Yuanhuiyi Lyu", "Xu Zheng", "Yingcong Chen"], "title": "DiMeR: Disentangled Mesh Reconstruction Model", "categories": ["cs.CV"], "comment": "Project Page: https://lutao2021.github.io/DiMeR_page/", "summary": "With the advent of large-scale 3D datasets, feed-forward 3D generative\nmodels, such as the Large Reconstruction Model (LRM), have gained significant\nattention and achieved remarkable success. However, we observe that RGB images\noften lead to conflicting training objectives and lack the necessary clarity\nfor geometry reconstruction. In this paper, we revisit the inductive biases\nassociated with mesh reconstruction and introduce DiMeR, a novel disentangled\ndual-stream feed-forward model for sparse-view mesh reconstruction. The key\nidea is to disentangle both the input and framework into geometry and texture\nparts, thereby reducing the training difficulty for each part according to the\nPrinciple of Occam's Razor. Given that normal maps are strictly consistent with\ngeometry and accurately capture surface variations, we utilize normal maps as\nexclusive input for the geometry branch to reduce the complexity between the\nnetwork's input and output. Moreover, we improve the mesh extraction algorithm\nto introduce 3D ground truth supervision. As for texture branch, we use RGB\nimages as input to obtain the textured mesh. Overall, DiMeR demonstrates robust\ncapabilities across various tasks, including sparse-view reconstruction,\nsingle-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR\nsignificantly outperforms previous methods, achieving over 30% improvement in\nChamfer Distance on the GSO and OmniObject3D dataset.", "AI": {"tldr": "DiMeR\u662f\u4e00\u79cd\u89e3\u8026\u7684\u53cc\u6d41\u524d\u9988\u6a21\u578b\uff0c\u7528\u4e8e\u7a00\u758f\u89c6\u56fe\u7f51\u683c\u91cd\u5efa\uff0c\u901a\u8fc7\u5206\u79bb\u51e0\u4f55\u548c\u7eb9\u7406\u8f93\u5165\u4e0e\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "RGB\u56fe\u50cf\u5728\u51e0\u4f55\u91cd\u5efa\u4e2d\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u76ee\u6807\u51b2\u7a81\u4e14\u7f3a\u4e4f\u6e05\u6670\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "DiMeR\u5c06\u8f93\u5165\u548c\u6846\u67b6\u89e3\u8026\u4e3a\u51e0\u4f55\u548c\u7eb9\u7406\u90e8\u5206\uff0c\u51e0\u4f55\u5206\u652f\u4f7f\u7528\u6cd5\u7ebf\u56fe\uff0c\u7eb9\u7406\u5206\u652f\u4f7f\u7528RGB\u56fe\u50cf\uff0c\u5e76\u6539\u8fdb\u4e86\u7f51\u683c\u63d0\u53d6\u7b97\u6cd5\u3002", "result": "DiMeR\u5728\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u3001\u5355\u56fe\u50cf\u52303D\u548c\u6587\u672c\u52303D\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cChamfer Distance\u5728GSO\u548cOmniObject3D\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u8d85\u8fc730%\u3002", "conclusion": "DiMeR\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u548c\u6cd5\u7ebf\u56fe\u8f93\u5165\uff0c\u663e\u8457\u63d0\u9ad8\u4e863D\u91cd\u5efa\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2504.17255", "pdf": "https://arxiv.org/pdf/2504.17255", "abs": "https://arxiv.org/abs/2504.17255", "authors": ["Shaoyu Pei", "Renxiong Wu", "Hao Zheng", "Lang Qin", "Shuaichen Lin", "Yuxing Gan", "Wenjing Huang", "Zhixuan Wang", "Mohan Qin", "Yong Liu", "Guangming Ni"], "title": "3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations", "categories": ["eess.IV", "cs.AI", "physics.optics"], "comment": null, "summary": "Skin, the primary regulator of heat exchange, relies on sweat glands for\nthermoregulation. Alterations in sweat gland morphology play a crucial role in\nvarious pathological conditions and clinical diagnoses. Current methods for\nobserving sweat gland morphology are limited by their two-dimensional, in\nvitro, and destructive nature, underscoring the urgent need for real-time,\nnon-invasive, quantifiable technologies. We proposed a novel three-dimensional\n(3D) transformer-based multi-object segmentation framework, integrating a\nsliding window approach, joint spatial-channel attention mechanism, and\narchitectural heterogeneity between shallow and deep layers. Our proposed\nnetwork enables precise 3D sweat gland segmentation from skin volume data\ncaptured by optical coherence tomography (OCT). For the first time, subtle\nvariations of sweat gland 3D morphology in response to temperature changes,\nhave been visualized and quantified. Our approach establishes a benchmark for\nnormal sweat gland morphology and provides a real-time, non-invasive tool for\nquantifying 3D structural parameters. This enables the study of individual\nvariability and pathological changes in sweat gland structure, advancing\ndermatological research and clinical applications, including thermoregulation\nand bromhidrosis treatment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u53d8\u6362\u5668\u7684\u591a\u76ee\u6807\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u4fb5\u5165\u6027\u3001\u5b9e\u65f6\u91cf\u5316\u6c57\u817a\u5f62\u6001\uff0c\u9996\u6b21\u53ef\u89c6\u5316\u5e76\u91cf\u5316\u4e86\u6c57\u817a3D\u5f62\u6001\u968f\u6e29\u5ea6\u53d8\u5316\u7684\u7ec6\u5fae\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u6c57\u817a\u5f62\u6001\u89c2\u5bdf\u65b9\u6cd5\u591a\u4e3a\u4e8c\u7ef4\u3001\u4f53\u5916\u4e14\u7834\u574f\u6027\uff0c\u4e9f\u9700\u5b9e\u65f6\u3001\u975e\u4fb5\u5165\u6027\u3001\u53ef\u91cf\u5316\u7684\u6280\u672f\u3002", "method": "\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u3001\u8054\u5408\u7a7a\u95f4-\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u53ca\u6d45\u6df1\u5c42\u67b6\u6784\u5f02\u8d28\u6027\uff0c\u63d0\u51fa3D\u53d8\u6362\u5668\u5206\u5272\u6846\u67b6\uff0c\u5229\u7528OCT\u76ae\u80a4\u4f53\u79ef\u6570\u636e\u5b9e\u73b0\u7cbe\u786e\u5206\u5272\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u6c57\u817a3D\u5f62\u6001\u968f\u6e29\u5ea6\u53d8\u5316\u7684\u53ef\u89c6\u5316\u4e0e\u91cf\u5316\uff0c\u4e3a\u6b63\u5e38\u6c57\u817a\u5f62\u6001\u5efa\u7acb\u4e86\u57fa\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6c57\u817a\u7ed3\u6784\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u65f6\u3001\u975e\u4fb5\u5165\u6027\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u76ae\u80a4\u75c5\u5b66\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2504.17695", "pdf": "https://arxiv.org/pdf/2504.17695", "abs": "https://arxiv.org/abs/2504.17695", "authors": ["Alp\u00e1r Cseke", "Shashank Tripathi", "Sai Kumar Dwivedi", "Arjun Lakshmipathy", "Agniv Chatterjee", "Michael J. Black", "Dimitrios Tzionas"], "title": "PICO: Reconstructing 3D People In Contact with Objects", "categories": ["cs.CV"], "comment": "Accepted in CVPR'25. Project Page: https://pico.is.tue.mpg.de", "summary": "Recovering 3D Human-Object Interaction (HOI) from single color images is\nchallenging due to depth ambiguities, occlusions, and the huge variation in\nobject shape and appearance. Thus, past work requires controlled settings such\nas known object shapes and contacts, and tackles only limited object classes.\nInstead, we need methods that generalize to natural images and novel object\nclasses. We tackle this in two main ways: (1) We collect PICO-db, a new dataset\nof natural images uniquely paired with dense 3D contact on both body and object\nmeshes. To this end, we use images from the recent DAMON dataset that are\npaired with contacts, but these contacts are only annotated on a canonical 3D\nbody. In contrast, we seek contact labels on both the body and the object. To\ninfer these given an image, we retrieve an appropriate 3D object mesh from a\ndatabase by leveraging vision foundation models. Then, we project DAMON's body\ncontact patches onto the object via a novel method needing only 2 clicks per\npatch. This minimal human input establishes rich contact correspondences\nbetween bodies and objects. (2) We exploit our new dataset of contact\ncorrespondences in a novel render-and-compare fitting method, called PICO-fit,\nto recover 3D body and object meshes in interaction. PICO-fit infers contact\nfor the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db\nfor that object, and uses the contact to iteratively fit the 3D body and object\nmeshes to image evidence via optimization. Uniquely, PICO-fit works well for\nmany object categories that no existing method can tackle. This is crucial to\nenable HOI understanding to scale in the wild. Our data and code are available\nat https://pico.is.tue.mpg.de.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u5f69\u8272\u56fe\u50cf\u4e2d\u6062\u590d3D\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u65b0\u6570\u636e\u96c6PICO-db\u548c\u5f00\u53d1\u4f18\u5316\u62df\u5408\u65b9\u6cd5PICO-fit\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u6a21\u7cca\u3001\u906e\u6321\u548c\u7269\u4f53\u591a\u6837\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u5df2\u77e5\u7269\u4f53\u5f62\u72b6\u548c\u63a5\u89e6\u6761\u4ef6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u81ea\u7136\u56fe\u50cf\u548c\u65b0\u7269\u4f53\u7c7b\u522b\u3002\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9002\u7528\u4e8e\u81ea\u7136\u573a\u666f\u548c\u591a\u6837\u7269\u4f53\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u6784\u5efaPICO-db\u6570\u636e\u96c6\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u68c0\u7d223D\u7269\u4f53\u7f51\u683c\uff0c\u5e76\u901a\u8fc7\u65b0\u65b9\u6cd5\u6807\u6ce8\u5bc6\u96c6\u63a5\u89e6\u30022. \u63d0\u51faPICO-fit\u65b9\u6cd5\uff0c\u7ed3\u5408\u6e32\u67d3\u4e0e\u6bd4\u8f83\u4f18\u5316\uff0c\u62df\u54083D\u4eba\u4f53\u548c\u7269\u4f53\u7f51\u683c\u3002", "result": "PICO-fit\u80fd\u591f\u5904\u7406\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u7684\u7269\u4f53\u7c7b\u522b\uff0c\u663e\u8457\u63d0\u5347\u4e86HOI\u7406\u89e3\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u81ea\u7136\u56fe\u50cf\u4e2d\u591a\u68373D\u4eba-\u7269\u4ea4\u4e92\u7684\u6062\u590d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2504.17261", "pdf": "https://arxiv.org/pdf/2504.17261", "abs": "https://arxiv.org/abs/2504.17261", "authors": ["Jiaqi Chen", "Xiaoye Zhu", "Yue Wang", "Tianyang Liu", "Xinhui Chen", "Ying Chen", "Chak Tou Leong", "Yifei Ke", "Joseph Liu", "Yiwen Yuan", "Julian McAuley", "Li-jia Li"], "title": "Symbolic Representation for Any-to-Any Generative Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b26\u53f7\u5316\u751f\u6210\u4efb\u52a1\u63cf\u8ff0\u8bed\u8a00\u53ca\u63a8\u7406\u5f15\u64ce\uff0c\u80fd\u8868\u793a\u4efb\u610f\u591a\u6a21\u6001\u4efb\u52a1\u4e3a\u7ed3\u6784\u5316\u7b26\u53f7\u6d41\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7075\u6d3b\u6027\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u7b26\u53f7\u8868\u793a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e09\u79cd\u6838\u5fc3\u7b26\u53f7\u539f\u8bed\uff08\u51fd\u6570\u3001\u53c2\u6570\u3001\u62d3\u6251\u903b\u8f91\uff09\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u76f4\u63a5\u6620\u5c04\u4e3a\u7b26\u53f7\u5de5\u4f5c\u6d41\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u3002", "result": "\u572812\u79cd\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5185\u5bb9\u8d28\u91cf\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u7edf\u4e00\u6a21\u578b\uff0c\u540c\u65f6\u5177\u5907\u66f4\u9ad8\u6548\u7387\u3001\u53ef\u7f16\u8f91\u6027\u548c\u53ef\u4e2d\u65ad\u6027\u3002", "conclusion": "\u7b26\u53f7\u5316\u4efb\u52a1\u8868\u793a\u4e3a\u751f\u6210AI\u63d0\u4f9b\u4e86\u6210\u672c\u4f4e\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u5176\u80fd\u529b\u53d1\u5c55\u3002"}}
{"id": "2504.17696", "pdf": "https://arxiv.org/pdf/2504.17696", "abs": "https://arxiv.org/abs/2504.17696", "authors": ["Ghazal Kaviani", "Yavuz Yarici", "Seulgi Kim", "Mohit Prabhushankar", "Ghassan AlRegib", "Mashhour Solh", "Ameya Patil"], "title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/", "AI": {"tldr": "DARai\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u5206\u5c42\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u6d3b\u52a8\uff0c\u5305\u542b50\u540d\u53c2\u4e0e\u8005\u572810\u79cd\u73af\u5883\u4e2d\u7684200\u5c0f\u65f6\u6570\u636e\uff0c\u6db5\u76d620\u79cd\u4f20\u611f\u5668\u3002\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u548c\u9886\u57df\u53d8\u4f53\u5b9e\u9a8c\u4e2d\u7684\u4ef7\u503c\u3002", "motivation": "\u7406\u89e3\u771f\u5b9e\u73af\u5883\u4e2d\u4eba\u7c7b\u6d3b\u52a8\u7684\u590d\u6742\u6027\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u5206\u5c42\u6807\u6ce8\u7684\u6570\u636e\u96c6\u4ee5\u652f\u6301\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5305\u542b\u811a\u672c\u548c\u975e\u811a\u672c\u8bb0\u5f55\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e09\u5c42\u5c42\u6b21\u6807\u6ce8\uff08L1\u6d3b\u52a8\u3001L2\u52a8\u4f5c\u3001L3\u6b65\u9aa4\uff09\uff0c\u5e76\u8fdb\u884c\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DARai\u5728\u6d3b\u52a8\u8bc6\u522b\u3001\u65f6\u95f4\u5b9a\u4f4d\u548c\u672a\u6765\u52a8\u4f5c\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5355\u4e2a\u4f20\u611f\u5668\u7684\u5c40\u9650\u6027\u3002", "conclusion": "DARai\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u57fa\u51c6\uff0c\u652f\u6301\u591a\u6a21\u6001\u548c\u9886\u57df\u53d8\u4f53\u7814\u7a76\u3002"}}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264", "abs": "https://arxiv.org/abs/2504.17264", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively.", "AI": {"tldr": "JurisCTC\u662f\u4e00\u79cd\u65b0\u578b\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\uff08LJP\uff09\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u8de8\u6cd5\u5f8b\u9886\u57df\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u6cd5\u5f8b\u6587\u672c\u590d\u6742\u4e14\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u65e0\u76d1\u7763\u9886\u57df\u9002\u5e94\uff08UDA\uff09\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faJurisCTC\u6a21\u578b\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u533a\u5206\u4e0d\u540c\u6cd5\u5f8b\u9886\u57df\u7684\u6837\u672c\uff0c\u5b9e\u73b0\u6c11\u4e8b\u4e0e\u5211\u4e8b\u6cd5\u5f8b\u9886\u57df\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "JurisCTC\u5728LJP\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523076.59%\u548c78.83%\u3002", "conclusion": "JurisCTC\u4e3a\u8de8\u6cd5\u5f8b\u9886\u57df\u7684\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86LJP\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.17712", "pdf": "https://arxiv.org/pdf/2504.17712", "abs": "https://arxiv.org/abs/2504.17712", "authors": ["Zhuo He", "Paul Henderson", "Nicolas Pugeault"], "title": "Generative Fields: Uncovering Hierarchical Feature Control for StyleGAN via Inverted Receptive Fields", "categories": ["cs.CV"], "comment": null, "summary": "StyleGAN has demonstrated the ability of GANs to synthesize highly-realistic\nfaces of imaginary people from random noise. One limitation of GAN-based image\ngeneration is the difficulty of controlling the features of the generated\nimage, due to the strong entanglement of the low-dimensional latent space.\nPrevious work that aimed to control StyleGAN with image or text prompts\nmodulated sampling in W latent space, which is more expressive than Z latent\nspace. However, W space still has restricted expressivity since it does not\ncontrol the feature synthesis directly; also the feature embedding in W space\nrequires a pre-training process to reconstruct the style signal, limiting its\napplication. This paper introduces the concept of \"generative fields\" to\nexplain the hierarchical feature synthesis in StyleGAN, inspired by the\nreceptive fields of convolution neural networks (CNNs). Additionally, we\npropose a new image editing pipeline for StyleGAN using generative field theory\nand the channel-wise style latent space S, utilizing the intrinsic structural\nfeature of CNNs to achieve disentangled control of feature synthesis at\nsynthesis time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u573a\u7406\u8bba\u548c\u901a\u9053\u98ce\u683c\u6f5c\u5728\u7a7a\u95f4S\u7684\u65b0\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86StyleGAN\u4e2d\u7279\u5f81\u63a7\u5236\u7684\u5c40\u9650\u6027\u3002", "motivation": "StyleGAN\u751f\u6210\u7684\u56fe\u50cf\u7279\u5f81\u96be\u4ee5\u63a7\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u5728W\u7a7a\u95f4\u4e2d\u7684\u8868\u8fbe\u53d7\u9650\u4e14\u9700\u8981\u9884\u8bad\u7ec3\u3002", "method": "\u5f15\u5165\u751f\u6210\u573a\u7406\u8bba\u89e3\u91caStyleGAN\u7684\u5206\u5c42\u7279\u5f81\u5408\u6210\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u901a\u9053\u98ce\u683c\u6f5c\u5728\u7a7a\u95f4S\u7684\u56fe\u50cf\u7f16\u8f91\u6d41\u7a0b\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u7279\u5f81\u5408\u6210\u7684\u89e3\u8026\u63a7\u5236\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u7075\u6d3b\u6027\u548c\u6548\u679c\u3002", "conclusion": "\u751f\u6210\u573a\u7406\u8bba\u548cS\u7a7a\u95f4\u4e3aStyleGAN\u7684\u7279\u5f81\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17277", "pdf": "https://arxiv.org/pdf/2504.17277", "abs": "https://arxiv.org/abs/2504.17277", "authors": ["Zongliang Ji", "Andre Carlos Kajdacsy-Balla Amaral", "Anna Goldenberg", "Rahul G. Krishnan"], "title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the Conference on Health, Inference, and Learning (CHIL)\n  2025", "summary": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u7b56\u7565\u5b66\u4e60\u548c\u7279\u6743\u4fe1\u606f\u7684\u65b0\u65b9\u6cd5\uff08ExOSITO\uff09\uff0c\u7528\u4e8e\u4f18\u5316ICU\u5b9e\u9a8c\u5ba4\u6d4b\u8bd5\u7684\u8ba2\u8d2d\uff0c\u65e8\u5728\u51cf\u5c11\u8fc7\u5ea6\u8ba2\u8d2d\u7684\u8d1f\u62c5\u3002", "motivation": "ICU\u4e2d\u5b9e\u9a8c\u5ba4\u6d4b\u8bd5\u7684\u8fc7\u5ea6\u8ba2\u8d2d\u589e\u52a0\u4e86\u4e34\u5e8a\u8d1f\u62c5\u548c\u6210\u672c\uff0c\u9700\u8981\u4e00\u79cd\u5e73\u8861\u4fe1\u606f\u83b7\u53d6\u4e0e\u8d44\u6e90\u4f18\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u79bb\u7b56\u7565\u5b66\u4e60\u548c\u7279\u6743\u4fe1\u606f\uff0c\u7ed3\u5408\u4e34\u5e8a\u77e5\u8bc6\u548c\u89c2\u5bdf\u6570\u636e\uff0c\u63d0\u51faExOSITO\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u5f3a\u76d7\u6a21\u578b\u8bad\u7ec3\u3002", "result": "ExOSITO\u4f18\u4e8e\u533b\u751f\u7b56\u7565\u548c\u73b0\u6709\u65b9\u6cd5\uff0c\u51cf\u5c11\u6210\u672c\u4e14\u4e0d\u9057\u6f0f\u5173\u952e\u6d4b\u8bd5\u3002", "conclusion": "ExOSITO\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u4f18\u5316\u4e86\u5b9e\u9a8c\u5ba4\u6d4b\u8bd5\u7684\u8ba2\u8d2d\u3002"}}
{"id": "2504.17732", "pdf": "https://arxiv.org/pdf/2504.17732", "abs": "https://arxiv.org/abs/2504.17732", "authors": ["Zhanwen Liu", "Sai Zhou", "Yuchao Dai", "Yang Wang", "Yisheng An", "Xiangmo Zhao"], "title": "DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt State Space Model", "categories": ["cs.CV", "I.4.4"], "comment": null, "summary": "All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, significantly reducing training costs and\ndeployment complexity compared to traditional methods that design dedicated\nmodels for each degradation type. Existing approaches typically rely on\nDegradation-specific models or coarse-grained degradation prompts to guide\nimage restoration. However, they lack fine-grained modeling of degradation\ninformation and face limitations in balancing multi-task conflicts. To overcome\nthese limitations, we propose DPMambaIR, a novel All-in-One image restoration\nframework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)\nand a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained\nmodeling of complex degradation information and efficient global integration,\nwhile mitigating the loss of high-frequency details caused by task competition.\nSpecifically, the DP-SSM utilizes a pre-trained degradation extractor to\ncapture fine-grained degradation features and dynamically incorporates them\ninto the state space modeling process, enhancing the model's adaptability to\ndiverse degradation types. Concurrently, the HEB supplements high-frequency\ninformation, effectively addressing the loss of critical details, such as edges\nand textures, in multi-task image restoration scenarios. Extensive experiments\non a mixed dataset containing seven degradation types show that DPMambaIR\nachieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,\nrespectively. These results highlight the potential and superiority of\nDPMambaIR as a unified solution for All-in-One image restoration.", "AI": {"tldr": "DPMambaIR\u662f\u4e00\u79cd\u65b0\u578bAll-in-One\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5efa\u6a21\u548c\u9ad8\u6548\u5168\u5c40\u6574\u5408\u89e3\u51b3\u591a\u4efb\u52a1\u51b2\u7a81\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u4e3a\u6bcf\u79cd\u9000\u5316\u7c7b\u578b\u8bbe\u8ba1\u4e13\u7528\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5efa\u6a21\u4e14\u96be\u4ee5\u5e73\u8861\u591a\u4efb\u52a1\u51b2\u7a81\u3002", "method": "\u7ed3\u5408Degradation-Aware Prompt State Space Model (DP-SSM)\u548cHigh-Frequency Enhancement Block (HEB)\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5efa\u6a21\u548c\u9ad8\u9891\u7ec6\u8282\u8865\u5145\u3002", "result": "\u5728\u5305\u542b\u4e03\u79cd\u9000\u5316\u7c7b\u578b\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e0a\uff0cDPMambaIR\u4ee5PSNR 27.69dB\u548cSSIM 0.893\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "DPMambaIR\u5c55\u793a\u4e86\u4f5c\u4e3a\u7edf\u4e00All-in-One\u56fe\u50cf\u4fee\u590d\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.17304", "pdf": "https://arxiv.org/pdf/2504.17304", "abs": "https://arxiv.org/abs/2504.17304", "authors": ["Yimin Shi", "Yang Fei", "Shiqi Zhang", "Haixun Wang", "Xiaokui Xiao"], "title": "You Are What You Bought: Generating Customer Personas for E-commerce Applications", "categories": ["cs.IR", "cs.AI"], "comment": "SIGIR 2025", "summary": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5ba2\u6237\u753b\u50cf\uff08persona\uff09\u7684\u7528\u6237\u8868\u793a\u65b9\u6cd5GPLR\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3LLM\u548c\u968f\u673a\u6e38\u8d70\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u548c\u5ba2\u6237\u5206\u7fa4\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u751f\u6210\u7684\u7528\u6237\u5d4c\u5165\u96be\u4ee5\u7406\u89e3\u548c\u6574\u5408\u5916\u90e8\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5176\u5728\u5ba2\u6237\u5206\u7fa4\u3001\u641c\u7d22\u5bfc\u822a\u548c\u4ea7\u54c1\u63a8\u8350\u7b49\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faGPLR\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3LLM\u63a8\u65ad\u5ba2\u6237\u753b\u50cf\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u6e38\u8d70\u6280\u672f\u6269\u5c55\u8986\u76d6\u8303\u56f4\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faRevAff\u7b97\u6cd5\uff0c\u4f18\u5316\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u8bef\u5dee\u4fdd\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u7535\u5546\u6570\u636e\u96c6\u4e0a\uff0c\u5ba2\u6237\u753b\u50cf\u8868\u793a\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6a21\u578b\u7684\u6027\u80fd\uff08NDCG@K\u548cF1-Score@K\u63d0\u5347\u9ad8\u8fbe12%\uff09\u3002", "conclusion": "\u5ba2\u6237\u753b\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8bfb\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u7528\u6237\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7535\u5546\u5e94\u7528\u7684\u6027\u80fd\u3002"}}
{"id": "2504.17735", "pdf": "https://arxiv.org/pdf/2504.17735", "abs": "https://arxiv.org/abs/2504.17735", "authors": ["Akhil Padmanabha", "Saravanan Govindarajan", "Hwanmun Kim", "Sergio Ortiz", "Rahul Rajan", "Doruk Senkal", "Sneha Kadetotad"], "title": "EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Human activity recognition (HAR) on smartglasses has various use cases,\nincluding health/fitness tracking and input for context-aware AI assistants.\nHowever, current approaches for egocentric activity recognition suffer from low\nperformance or are resource-intensive. In this work, we introduce a resource\n(memory, compute, power, sample) efficient machine learning algorithm,\nEgoCHARM, for recognizing both high level and low level activities using a\nsingle egocentric (head-mounted) Inertial Measurement Unit (IMU). Our\nhierarchical algorithm employs a semi-supervised learning strategy, requiring\nprimarily high level activity labels for training, to learn generalizable low\nlevel motion embeddings that can be effectively utilized for low level activity\nrecognition. We evaluate our method on 9 high level and 3 low level activities\nachieving 0.826 and 0.855 F1 scores on high level and low level activity\nrecognition respectively, with just 63k high level and 22k low level model\nparameters, allowing the low level encoder to be deployed directly on current\nIMU chips with compute. Lastly, we present results and insights from a\nsensitivity analysis and highlight the opportunities and limitations of\nactivity recognition using egocentric IMUs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5EgoCHARM\uff0c\u7528\u4e8e\u901a\u8fc7\u5355\u4e00\u5934\u6234\u5f0fIMU\u8bc6\u522b\u9ad8\u4f4e\u5c42\u6b21\u6d3b\u52a8\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5934\u6234\u5f0f\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u6027\u80fd\u4f4e\u6216\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u7684\u5206\u5c42\u7b97\u6cd5\uff0c\u4e3b\u8981\u7528\u9ad8\u5c42\u6b21\u6d3b\u52a8\u6807\u7b7e\u8bad\u7ec3\uff0c\u5b66\u4e60\u901a\u7528\u4f4e\u5c42\u6b21\u8fd0\u52a8\u5d4c\u5165\u3002", "result": "\u57289\u79cd\u9ad8\u5c42\u6b21\u548c3\u79cd\u4f4e\u5c42\u6b21\u6d3b\u52a8\u4e0a\uff0cF1\u5206\u6570\u5206\u522b\u4e3a0.826\u548c0.855\uff0c\u6a21\u578b\u53c2\u6570\u4ec563k\u548c22k\u3002", "conclusion": "EgoCHARM\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\uff0c\u540c\u65f6\u5206\u6790\u4e86\u5934\u6234\u5f0fIMU\u7684\u6f5c\u529b\u4e0e\u9650\u5236\u3002"}}
{"id": "2504.17761", "pdf": "https://arxiv.org/pdf/2504.17761", "abs": "https://arxiv.org/abs/2504.17761", "authors": ["Shiyu Liu", "Yucheng Han", "Peng Xing", "Fukun Yin", "Rui Wang", "Wei Cheng", "Jiaqi Liao", "Yingming Wang", "Honghao Fu", "Chunrui Han", "Guopeng Li", "Yuang Peng", "Quan Sun", "Jingwei Wu", "Yan Cai", "Zheng Ge", "Ranchen Ming", "Lei Xia", "Xianfang Zeng", "Yibo Zhu", "Binxing Jiao", "Xiangyu Zhang", "Gang Yu", "Daxin Jiang"], "title": "Step1X-Edit: A Practical Framework for General Image Editing", "categories": ["cs.CV"], "comment": "code: https://github.com/stepfun-ai/Step1X-Edit", "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStep1X-Edit\u7684\u5f00\u6e90\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u65e8\u5728\u7f29\u5c0f\u4e0e\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\u548cGemini2 Flash\uff09\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u7b97\u6cd5\u4e0e\u95ed\u6e90\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u6027\u80fd\u63a5\u8fd1\u95ed\u6e90\u6a21\u578b\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001LLM\u5904\u7406\u53c2\u8003\u56fe\u50cf\u548c\u7528\u6237\u7f16\u8f91\u6307\u4ee4\uff0c\u63d0\u53d6\u6f5c\u5728\u5d4c\u5165\u5e76\u4e0e\u6269\u6563\u56fe\u50cf\u89e3\u7801\u5668\u7ed3\u5408\u751f\u6210\u76ee\u6807\u56fe\u50cf\uff1b\u6784\u5efa\u6570\u636e\u751f\u6210\u7ba1\u9053\u8bad\u7ec3\u6a21\u578b\u3002", "result": "Step1X-Edit\u5728GEdit-Bench\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u57fa\u7ebf\uff0c\u63a5\u8fd1\u9886\u5148\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "Step1X-Edit\u4e3a\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u7684\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\uff0c\u7f29\u5c0f\u4e86\u4e0e\u95ed\u6e90\u6a21\u578b\u7684\u5dee\u8ddd\u3002"}}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311", "abs": "https://arxiv.org/abs/2504.17311", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors.", "AI": {"tldr": "FLUKE\u662f\u4e00\u4e2a\u4efb\u52a1\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6700\u5c0f\u6d4b\u8bd5\u6570\u636e\u53d8\u5316\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u6a21\u578b\u5bf9\u4e0d\u540c\u8bed\u8a00\u53d8\u5316\u7684\u654f\u611f\u6027\u548c\u8106\u5f31\u6027\u3002", "motivation": "\u8bc4\u4f30\u6a21\u578b\u5728\u9762\u5bf9\u8bed\u8a00\u53d8\u5316\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u7406\u89e3\u5176\u884c\u4e3a\u7279\u70b9\u3002", "method": "FLUKE\u5f15\u5165\u8de8\u8bed\u8a00\u5c42\u6b21\u7684\u63a7\u5236\u53d8\u5316\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u5de5\u9a8c\u8bc1\u751f\u6210\u4fee\u6539\uff0c\u5e76\u5728\u56db\u4e2aNLP\u4efb\u52a1\u4e2d\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u8bed\u8a00\u53d8\u5316\u7684\u5f71\u54cd\u9ad8\u5ea6\u4f9d\u8d56\u4efb\u52a1\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u6574\u4f53\u9c81\u68d2\u6027\u66f4\u5f3a\u4f46\u4ecd\u8106\u5f31\uff0c\u6240\u6709\u6a21\u578b\u5bf9\u5426\u5b9a\u4fee\u6539\u666e\u904d\u8106\u5f31\u3002", "conclusion": "\u7cfb\u7edf\u9c81\u68d2\u6027\u6d4b\u8bd5\u5bf9\u7406\u89e3\u6a21\u578b\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.17787", "pdf": "https://arxiv.org/pdf/2504.17787", "abs": "https://arxiv.org/abs/2504.17787", "authors": ["Anton Obukhov", "Matteo Poggi", "Fabio Tosi", "Ripudaman Singh Arora", "Jaime Spencer", "Chris Russell", "Simon Hadfield", "Richard Bowden", "Shuaihang Wang", "Zhenxin Ma", "Weijie Chen", "Baobei Xu", "Fengyu Sun", "Di Xie", "Jiang Zhu", "Mykola Lavreniuk", "Haining Guan", "Qun Wu", "Yupei Zeng", "Chao Lu", "Huanran Wang", "Guangyuan Zhou", "Haotian Zhang", "Jianxiong Wang", "Qiang Rao", "Chunjie Wang", "Xiao Liu", "Zhiqiang Lou", "Hualie Jiang", "Yihao Chen", "Rui Xu", "Minglang Tan", "Zihan Qin", "Yifan Mao", "Jiayang Liu", "Jialei Xu", "Yifan Yang", "Wenbo Zhao", "Junjun Jiang", "Xianming Liu", "Mingshuai Zhao", "Anlong Ming", "Wu Chen", "Feng Xue", "Mengying Yu", "Shida Gao", "Xiangfeng Wang", "Gbenga Omotara", "Ramy Farag", "Jacket Demby", "Seyed Mohamad Ali Tousi", "Guilherme N DeSouza", "Tuan-Anh Yang", "Minh-Quang Nguyen", "Thien-Phuc Tran", "Albert Luginov", "Muhammad Shahzad"], "title": "The Fourth Monocular Depth Estimation Challenge", "categories": ["cs.CV"], "comment": "To appear in CVPRW2025", "summary": "This paper presents the results of the fourth edition of the Monocular Depth\nEstimation Challenge (MDEC), which focuses on zero-shot generalization to the\nSYNS-Patches benchmark, a dataset featuring challenging environments in both\nnatural and indoor settings. In this edition, we revised the evaluation\nprotocol to use least-squares alignment with two degrees of freedom to support\ndisparity and affine-invariant predictions. We also revised the baselines and\nincluded popular off-the-shelf methods: Depth Anything v2 and Marigold. The\nchallenge received a total of 24 submissions that outperformed the baselines on\nthe test set; 10 of these included a report describing their approach, with\nmost leading methods relying on affine-invariant predictions. The challenge\nwinners improved the 3D F-Score over the previous edition's best result,\nraising it from 22.58% to 23.05%.", "AI": {"tldr": "\u7b2c\u56db\u7248\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6311\u6218\u8d5b\uff08MDEC\uff09\u7684\u7ed3\u679c\uff0c\u91cd\u70b9\u5173\u6ce8\u96f6\u6837\u672c\u6cdb\u5316\u5230SYNS-Patches\u57fa\u51c6\uff0c\u6539\u8fdb\u4e86\u8bc4\u4f30\u534f\u8bae\u548c\u57fa\u7ebf\u65b9\u6cd5\uff0c24\u4e2a\u63d0\u4ea4\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u83b7\u80dc\u8005\u5c063D F-Score\u4ece22.58%\u63d0\u5347\u81f323.05%\u3002", "motivation": "\u6539\u8fdb\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u81ea\u7136\u548c\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u6311\u6218\u6027\u573a\u666f\u3002", "method": "\u4fee\u8ba2\u8bc4\u4f30\u534f\u8bae\uff08\u6700\u5c0f\u4e8c\u4e58\u5bf9\u9f50\u548c\u4e24\u81ea\u7531\u5ea6\u652f\u6301\uff09\uff0c\u5f15\u5165\u65b0\u57fa\u7ebf\u65b9\u6cd5\uff08Depth Anything v2\u548cMarigold\uff09\uff0c\u63a5\u653624\u4e2a\u63d0\u4ea4\u5e76\u5206\u6790\u5176\u65b9\u6cd5\u3002", "result": "24\u4e2a\u63d0\u4ea4\u4f18\u4e8e\u57fa\u7ebf\uff0c10\u4e2a\u63d0\u4ea4\u63d0\u4f9b\u65b9\u6cd5\u63cf\u8ff0\uff0c\u83b7\u80dc\u8005\u5c063D F-Score\u4ece22.58%\u63d0\u5347\u81f323.05%\u3002", "conclusion": "\u6311\u6218\u8d5b\u6210\u529f\u63a8\u52a8\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u5c24\u5176\u662f\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.17788", "pdf": "https://arxiv.org/pdf/2504.17788", "abs": "https://arxiv.org/abs/2504.17788", "authors": ["Chris Rockwell", "Joseph Tung", "Tsung-Yi Lin", "Ming-Yu Liu", "David F. Fouhey", "Chen-Hsuan Lin"], "title": "Dynamic Camera Poses and Where to Find Them", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://research.nvidia.com/labs/dir/dynpose-100k", "summary": "Annotating camera poses on dynamic Internet videos at scale is critical for\nadvancing fields like realistic video generation and simulation. However,\ncollecting such a dataset is difficult, as most Internet videos are unsuitable\nfor pose estimation. Furthermore, annotating dynamic Internet videos present\nsignificant challenges even for state-of-theart methods. In this paper, we\nintroduce DynPose-100K, a large-scale dataset of dynamic Internet videos\nannotated with camera poses. Our collection pipeline addresses filtering using\na carefully combined set of task-specific and generalist models. For pose\nestimation, we combine the latest techniques of point tracking, dynamic\nmasking, and structure-from-motion to achieve improvements over the\nstate-of-the-art approaches. Our analysis and experiments demonstrate that\nDynPose-100K is both large-scale and diverse across several key attributes,\nopening up avenues for advancements in various downstream applications.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DynPose-100K\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u52a8\u6001\u4e92\u8054\u7f51\u89c6\u9891\u7684\u76f8\u673a\u59ff\u6001\u6807\u6ce8\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u65b9\u6cd5\u5e76\u5c55\u793a\u4e86\u5176\u591a\u6837\u6027\u548c\u89c4\u6a21\u3002", "motivation": "\u52a8\u6001\u4e92\u8054\u7f51\u89c6\u9891\u7684\u76f8\u673a\u59ff\u6001\u6807\u6ce8\u5bf9\u89c6\u9891\u751f\u6210\u548c\u6a21\u62df\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u548c\u901a\u7528\u6a21\u578b\u7b5b\u9009\u89c6\u9891\uff0c\u7ed3\u5408\u70b9\u8ddf\u8e2a\u3001\u52a8\u6001\u63a9\u7801\u548c\u8fd0\u52a8\u7ed3\u6784\u6280\u672f\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u3002", "result": "DynPose-100K\u6570\u636e\u96c6\u89c4\u6a21\u5927\u4e14\u591a\u6837\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2504.17331", "pdf": "https://arxiv.org/pdf/2504.17331", "abs": "https://arxiv.org/abs/2504.17331", "authors": ["S\u00fcleyman \u00d6zdel", "Kadir Burak Buldu", "Enkelejda Kasneci", "Efe Bozkir"], "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality", "categories": ["cs.HC", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u81ea\u7136\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\uff0c\u4e0e\u4f20\u7edf\u63a7\u5236\u5668\u548c\u8bed\u97f3\u5bfc\u822a\u76f8\u6bd4\uff0c\u5177\u6709\u76f8\u4f3c\u7684\u53ef\u7528\u6027\u548c\u6c89\u6d78\u611f\uff0c\u540c\u65f6\u589e\u5f3a\u7528\u6237\u6ce8\u610f\u529b\u3002", "motivation": "\u4f20\u7edf\u8bed\u97f3\u5bfc\u822a\u4f9d\u8d56\u56fa\u5b9a\u6307\u4ee4\u96c6\uff0c\u9650\u5236\u4e86\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u7075\u6d3b\u6027\uff0c\u800cLLM\u9a71\u52a8\u7684\u5bfc\u822a\u80fd\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u8bed\u8a00\u4ea4\u4e92\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u5bfc\u822a\u65b9\u6cd5\uff1a\u63a7\u5236\u5668\u4f20\u9001\u3001\u8bed\u97f3\u8f6c\u5411\u548cLLM\u9a71\u52a8\u7684\u81ea\u7136\u8bed\u8a00\u5bfc\u822a\uff0c\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u548c\u6807\u51c6\u5316\u95ee\u5377\u5206\u6790\u7528\u6237\u4f53\u9a8c\u3002", "result": "LLM\u5bfc\u822a\u5728\u53ef\u7528\u6027\u3001\u6c89\u6d78\u611f\u548c\u6655\u52a8\u75c7\u65b9\u9762\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u80fd\u589e\u5f3a\u7528\u6237\u6ce8\u610f\u529b\u3002SHAP\u5206\u6790\u663e\u793a\u89c6\u89c9\u6ce8\u610f\u529b\u548c\u8ba4\u77e5\u5904\u7406\u6a21\u5f0f\u4e0d\u540c\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u5bfc\u822a\u4e3a\u865a\u62df\u7a7a\u95f4\u4e2d\u7684\u514d\u624b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u8212\u9002\u3001\u81ea\u7136\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u5408\u63d0\u5347\u65e0\u969c\u788d\u6027\u3002"}}
{"id": "2504.17789", "pdf": "https://arxiv.org/pdf/2504.17789", "abs": "https://arxiv.org/abs/2504.17789", "authors": ["Xu Ma", "Peize Sun", "Haoyu Ma", "Hao Tang", "Chih-Yao Ma", "Jialiang Wang", "Kunpeng Li", "Xiaoliang Dai", "Yujun Shi", "Xuan Ju", "Yushi Hu", "Artsiom Sanakoyeu", "Felix Juefei-Xu", "Ji Hou", "Junjiao Tian", "Tao Xu", "Tingbo Hou", "Yen-Cheng Liu", "Zecheng He", "Zijian He", "Matt Feiszli", "Peizhao Zhang", "Peter Vajda", "Sam Tsai", "Yun Fu"], "title": "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faToken-Shuffle\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11Transformer\u4e2d\u7684\u56fe\u50cf\u4ee4\u724c\u6570\u91cf\uff0c\u63d0\u5347\u81ea\u56de\u5f52\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u5408\u6210\u4e2d\u56e0\u9700\u8981\u5927\u91cf\u56fe\u50cf\u4ee4\u724c\u800c\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u5206\u8fa8\u7387\u548c\u6027\u80fd\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51faToken-Shuffle\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bcd\u6c47\u7684\u7ef4\u5ea6\u5197\u4f59\u6027\uff0c\u901a\u8fc7token-shuffle\u5408\u5e76\u5c40\u90e8\u4ee4\u724c\u4ee5\u51cf\u5c11\u8f93\u5165\u4ee4\u724c\u6570\u91cf\uff0c\u5e76\u901a\u8fc7token-unshuffle\u6062\u590d\u7a7a\u95f4\u6392\u5217\u3002", "result": "\u57282048x2048\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u9ad8\u6548\u56fe\u50cf\u5408\u6210\uff0c2.7B\u6a21\u578b\u5728GenAI-benchmark\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6AR\u548c\u6269\u6563\u6a21\u578b\u3002", "conclusion": "Token-Shuffle\u4e3aMLLMs\u4e2d\u7684\u9ad8\u6548\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u8ba1\u3002"}}
{"id": "2504.17346", "pdf": "https://arxiv.org/pdf/2504.17346", "abs": "https://arxiv.org/abs/2504.17346", "authors": ["Tran Thuy Nga Truong", "Jooyong Kim"], "title": "Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper introduces an enhanced Genetic Algorithm technique called\nDual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural\nnetworks for binary image classification tasks, such as cat vs. non-cat\nclassification. The proposed method employs only two individuals for crossover,\nrepresented by two parameter sets: Leader and Follower. The Leader focuses on\nexploitation, representing the primary optimal solution at even-indexed\npositions (0, 2, 4, ...), while the Follower promotes exploration by preserving\ndiversity and avoiding premature convergence, operating at odd-indexed\npositions (1, 3, 5, ...). Leader and Follower are modeled as two phases or\nroles. The key contributions of this work are threefold: (1) a self-adaptive\nlayer dimension mechanism that eliminates the need for manual tuning of layer\narchitectures; (2) generates two parameter sets, leader and follower parameter\nsets, with 10 layer architecture configurations (5 for each set), ranked by\nPareto dominance and cost. post-optimization; and (3) demonstrated superior\nperformance compared to traditional gradient-based methods. Experimental\nresults show that the Dual-Individual GA achieves 99.04% training accuracy and\n80% testing accuracy (cost = 0.034) on a three-layer network with architecture\n[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%\ntraining accuracy and 80% testing accuracy (cost = 0.092) on a four-layer\nnetwork with architecture [12288, 20, 7, 5, 1]. These findings highlight the\nefficiency and effectiveness of the proposed method in optimizing neural\nnetworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDual-Individual GA\u7684\u589e\u5f3a\u9057\u4f20\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u4e8c\u5143\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u795e\u7ecf\u7f51\u7edc\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7Leader\u548cFollower\u4e24\u4e2a\u4e2a\u4f53\u5b9e\u73b0\u63a2\u7d22\u4e0e\u5f00\u53d1\u7684\u5e73\u8861\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u68af\u5ea6\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u68af\u5ea6\u65b9\u6cd5\u5728\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u9700\u8981\u624b\u52a8\u8c03\u6574\u67b6\u6784\u548c\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002Dual-Individual GA\u65e8\u5728\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u81ea\u52a8\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u5e76\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Dual-Individual GA\u4ec5\u4f7f\u7528\u4e24\u4e2a\u4e2a\u4f53\uff08Leader\u548cFollower\uff09\u8fdb\u884c\u4ea4\u53c9\uff0cLeader\u4e13\u6ce8\u4e8e\u5f00\u53d1\u6700\u4f18\u89e3\uff0cFollower\u5219\u4fc3\u8fdb\u591a\u6837\u6027\u63a2\u7d22\u3002\u8be5\u65b9\u6cd5\u8fd8\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u5c42\u7ef4\u5ea6\u673a\u5236\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDual-Individual GA\u5728\u4e09\u5c42\u7f51\u7edc\u4e0a\u8fbe\u523099.04%\u8bad\u7ec3\u7cbe\u5ea6\u548c80%\u6d4b\u8bd5\u7cbe\u5ea6\uff08\u6210\u672c0.034\uff09\uff0c\u4f18\u4e8e\u4f20\u7edf\u68af\u5ea6\u65b9\u6cd5\uff0898%\u8bad\u7ec3\u7cbe\u5ea6\u548c80%\u6d4b\u8bd5\u7cbe\u5ea6\uff0c\u6210\u672c0.092\uff09\u3002", "conclusion": "Dual-Individual GA\u5728\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u81ea\u52a8\u8c03\u6574\u67b6\u6784\u548c\u5e73\u8861\u63a2\u7d22\u4e0e\u5f00\u53d1\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2504.17791", "pdf": "https://arxiv.org/pdf/2504.17791", "abs": "https://arxiv.org/abs/2504.17791", "authors": ["Tetiana Martyniuk", "Gilles Puy", "Alexandre Boulch", "Renaud Marlet", "Raoul de Charette"], "title": "LiDPM: Rethinking Point Diffusion for Lidar Scene Completion", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to IEEE IV 2025", "summary": "Training diffusion models that work directly on lidar points at the scale of\noutdoor scenes is challenging due to the difficulty of generating fine-grained\ndetails from white noise over a broad field of view. The latest works\naddressing scene completion with diffusion models tackle this problem by\nreformulating the original DDPM as a local diffusion process. It contrasts with\nthe common practice of operating at the level of objects, where vanilla DDPMs\nare currently used. In this work, we close the gap between these two lines of\nwork. We identify approximations in the local diffusion formulation, show that\nthey are not required to operate at the scene level, and that a vanilla DDPM\nwith a well-chosen starting point is enough for completion. Finally, we\ndemonstrate that our method, LiDPM, leads to better results in scene completion\non SemanticKITTI. The project page is https://astra-vision.github.io/LiDPM .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLiDPM\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u70b9\u9009\u62e9\uff0c\u8bc1\u660e\u4f20\u7edfDDPM\u5728\u573a\u666f\u7ea7\u522b\u5b8c\u6210\u4efb\u52a1\u65f6\u65e0\u9700\u5c40\u90e8\u6269\u6563\u8fd1\u4f3c\uff0c\u5e76\u5728SemanticKITTI\u4e0a\u53d6\u5f97\u66f4\u597d\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u6237\u5916\u573a\u666f\u70b9\u4e91\u6570\u636e\u4e0a\u96be\u4ee5\u751f\u6210\u7ec6\u7c92\u5ea6\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u5e76\u5f25\u5408\u5c40\u90e8\u6269\u6563\u4e0e\u5bf9\u8c61\u7ea7\u522b\u6269\u6563\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u4f20\u7edfDDPM\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u521d\u59cb\u70b9\uff0c\u907f\u514d\u5c40\u90e8\u6269\u6563\u8fd1\u4f3c\uff0c\u76f4\u63a5\u5728\u573a\u666f\u7ea7\u522b\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u5728SemanticKITTI\u6570\u636e\u96c6\u4e0a\uff0cLiDPM\u65b9\u6cd5\u5728\u573a\u666f\u8865\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4f20\u7edfDDPM\u5728\u573a\u666f\u7ea7\u522b\u4efb\u52a1\u4e2d\u65e0\u9700\u5c40\u90e8\u6269\u6563\u8fd1\u4f3c\uff0cLiDPM\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u70b9\u9009\u62e9\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2504.17354", "pdf": "https://arxiv.org/pdf/2504.17354", "abs": "https://arxiv.org/abs/2504.17354", "authors": ["Tarik Sahin", "Jacopo Bonari", "Sebastian Brandstaeter", "Alexander Popp"], "title": "Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "The effective contact area in rough surface contact plays a critical role in\nmulti-physics phenomena such as wear, sealing, and thermal or electrical\nconduction. Although accurate numerical methods, like the Boundary Element\nMethod (BEM), are available to compute this quantity, their high computational\ncost limits their applicability in multi-query contexts, such as uncertainty\nquantification, parameter identification, and multi-scale algorithms, where\nmany repeated evaluations are required. This study proposes a surrogate\nmodeling framework for predicting the effective contact area using\nfast-to-evaluate data-driven techniques. Various machine learning algorithms\nare trained on a precomputed dataset, where the inputs are the imposed load and\nstatistical roughness parameters, and the output is the corresponding effective\ncontact area. All models undergo hyperparameter optimization to enable fair\ncomparisons in terms of predictive accuracy and computational efficiency,\nevaluated using established quantitative metrics. Among the models, the Kernel\nRidge Regressor demonstrates the best trade-off between accuracy and\nefficiency, achieving high predictive accuracy, low prediction time, and\nminimal training overhead-making it a strong candidate for general-purpose\nsurrogate modeling. The Gaussian Process Regressor provides an attractive\nalternative when uncertainty quantification is required, although it incurs\nadditional computational cost due to variance estimation. The generalization\ncapability of the Kernel Ridge model is validated on an unseen simulation\nscenario, confirming its ability to transfer to new configurations. Database\ngeneration constitutes the dominant cost in the surrogate modeling process.\nNevertheless, the approach proves practical and efficient for multi-query\ntasks, even when accounting for this initial expense.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u66ff\u4ee3\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u9884\u6d4b\u7c97\u7cd9\u8868\u9762\u63a5\u89e6\u7684\u6709\u6548\u63a5\u89e6\u9762\u79ef\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u7c97\u7cd9\u8868\u9762\u63a5\u89e6\u7684\u6709\u6548\u63a5\u89e6\u9762\u79ef\u5bf9\u591a\u7269\u7406\u73b0\u8c61\uff08\u5982\u78e8\u635f\u3001\u5bc6\u5c01\u3001\u70ed\u6216\u7535\u4f20\u5bfc\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\uff08\u5982\u8fb9\u754c\u5143\u6cd5\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u67e5\u8be2\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8bad\u7ec3\u9884\u8ba1\u7b97\u6570\u636e\u96c6\uff0c\u8f93\u5165\u4e3a\u65bd\u52a0\u7684\u8f7d\u8377\u548c\u7edf\u8ba1\u7c97\u7cd9\u5ea6\u53c2\u6570\uff0c\u8f93\u51fa\u4e3a\u6709\u6548\u63a5\u89e6\u9762\u79ef\uff0c\u5e76\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\u4ee5\u6bd4\u8f83\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u6838\u5cad\u56de\u5f52\u5668\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u8868\u73b0\u51fa\u6700\u4f73\u5e73\u8861\uff0c\u800c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5668\u5728\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65f6\u662f\u66f4\u597d\u7684\u9009\u62e9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u67e5\u8be2\u4efb\u52a1\u4e2d\u5b9e\u7528\u4e14\u9ad8\u6548\uff0c\u6838\u5cad\u56de\u5f52\u6a21\u578b\u5728\u65b0\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.16936", "pdf": "https://arxiv.org/pdf/2504.16936", "abs": "https://arxiv.org/abs/2504.16936", "authors": ["Yusheng Zhao", "Junyu Luo", "Xiao Luo", "Weizhi Zhang", "Zhiping Xiao", "Wei Ju", "Philip S. Yu", "Ming Zhang"], "title": "Multifaceted Evaluation of Audio-Visual Capability for MLLMs: Effectiveness, Efficiency, Generalizability and Robustness", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Multi-modal large language models (MLLMs) have recently achieved great\nsuccess in processing and understanding information from diverse modalities\n(e.g., text, audio, and visual signals). Despite their growing popularity,\nthere remains a lack of comprehensive evaluation measuring the audio-visual\ncapabilities of these models, especially in diverse scenarios (e.g.,\ndistribution shifts and adversarial attacks). In this paper, we present a\nmultifaceted evaluation of the audio-visual capability of MLLMs, focusing on\nfour key dimensions: effectiveness, efficiency, generalizability, and\nrobustness. Through extensive experiments, we find that MLLMs exhibit strong\nzero-shot and few-shot generalization abilities, enabling them to achieve great\nperformance with limited data. However, their success relies heavily on the\nvision modality, which impairs performance when visual input is corrupted or\nmissing. Additionally, while MLLMs are susceptible to adversarial samples, they\ndemonstrate greater robustness compared to traditional models. The experimental\nresults and our findings provide insights into the audio-visual capabilities of\nMLLMs, highlighting areas for improvement and offering guidance for future\nresearch.", "AI": {"tldr": "\u672c\u6587\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u97f3\u9891-\u89c6\u89c9\u80fd\u529b\u8fdb\u884c\u4e86\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u53d1\u73b0\u5176\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u89c6\u89c9\u6a21\u6001\u4f9d\u8d56\u6027\u5f3a\uff0c\u4e14\u5728\u5bf9\u6297\u6837\u672c\u4e0b\u8868\u73b0\u8106\u5f31\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9MLLMs\u97f3\u9891-\u89c6\u89c9\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u504f\u79fb\u548c\u5bf9\u6297\u653b\u51fb\u7b49\u591a\u6837\u5316\u573a\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff08\u6709\u6548\u6027\u3001\u6548\u7387\u3001\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff09\u5bf9MLLMs\u8fdb\u884c\u591a\u89d2\u5ea6\u8bc4\u4f30\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "MLLMs\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u89c6\u89c9\u6a21\u6001\u4f9d\u8d56\u6027\u5f3a\uff1b\u5728\u5bf9\u6297\u6837\u672c\u4e0b\u867d\u8106\u5f31\uff0c\u4f46\u6bd4\u4f20\u7edf\u6a21\u578b\u66f4\u9c81\u68d2\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86MLLMs\u97f3\u9891-\u89c6\u89c9\u80fd\u529b\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u548c\u6307\u5bfc\u3002"}}
{"id": "2504.17355", "pdf": "https://arxiv.org/pdf/2504.17355", "abs": "https://arxiv.org/abs/2504.17355", "authors": ["Xiaohan Huang", "Dongjie Wang", "Zhiyuan Ning", "Ziyue Qiao", "Qingqing Long", "Haowei Zhu", "Yi Du", "Min Wu", "Yuanchun Zhou", "Meng Xiao"], "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, Keywords: Automated Feature Transformation, Tabular\n  Dataset, Reinforcement Learning", "summary": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets.", "AI": {"tldr": "TCTO\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u9a71\u52a8\u7684\u8def\u5f84\u4f18\u5316\u81ea\u52a8\u5316\u7279\u5f81\u5de5\u7a0b\uff0c\u52a8\u6001\u5efa\u6a21\u7279\u5f81\u548c\u8f6c\u6362\u5173\u7cfb\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u8f6c\u6362\u65b9\u6cd5\u5e38\u5ffd\u7565\u8f6c\u6362\u6b65\u9aa4\u95f4\u7684\u52a8\u6001\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u4f4e\u6548\u3002", "method": "\u63d0\u51faTCTO\u6846\u67b6\uff0c\u5229\u7528\u4ea4\u4e92\u56fe\u5efa\u6a21\u7279\u5f81\u548c\u8f6c\u6362\uff0c\u901a\u8fc7\u56fe\u526a\u679d\u548c\u56de\u6eaf\u4f18\u5316\u8def\u5f84\uff0c\u5b9e\u73b0\u52a8\u6001\u4f9d\u8d56\u7ba1\u7406\u548c\u5386\u53f2\u5b50\u56fe\u590d\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTCTO\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51cf\u5c11\u5197\u4f59\u64cd\u4f5c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "TCTO\u901a\u8fc7\u52a8\u6001\u56fe\u4f18\u5316\u548c\u8def\u5f84\u56de\u6eaf\uff0c\u663e\u8457\u63d0\u5347\u7279\u5f81\u5de5\u7a0b\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366", "abs": "https://arxiv.org/abs/2504.17366", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.", "AI": {"tldr": "\u8bba\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u57fa\u4e8e\u76f4\u64ad\u7684\u5197\u4f59\u4e30\u5bcc\u7684\u957f\u6587\u672c\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u957f\u6587\u672c\u6570\u636e\u96c6\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u5bf9\u8bdd\u7684\u5197\u4f59\u548c\u4fe1\u606f\u5bc6\u5ea6\u4e0d\u5747\u7279\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8e\u76f4\u64ad\u7684\u957f\u6587\u672c\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u68c0\u7d22\u4f9d\u8d56\u3001\u63a8\u7406\u4f9d\u8d56\u548c\u6df7\u5408\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u6d41\u884cLLM\u548c\u4e13\u7528\u65b9\u6cd5\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u5197\u4f59\u8f93\u5165\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u65b0\u57fa\u7ebf\u65b9\u6cd5\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdb\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.17384", "pdf": "https://arxiv.org/pdf/2504.17384", "abs": "https://arxiv.org/abs/2504.17384", "authors": ["Hanlin Sheng", "Xinming Wu", "Hang Gao", "Haibin Di", "Sergey Fomel", "Jintao Li", "Xu Si"], "title": "On the workflow, opportunities and challenges of developing foundation model in geophysics", "categories": ["physics.geo-ph", "cs.AI"], "comment": null, "summary": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u6846\u67b6\uff0c\u7cfb\u7edf\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\u4e0e\u5730\u7403\u7269\u7406\u6570\u636e\u7ed3\u5408\u7684\u5168\u6d41\u7a0b\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u5168\u9762\u7efc\u8ff0\u7684\u7a7a\u767d\u3002", "motivation": "\u8fd1\u5e74\u6765\u57fa\u7840\u6a21\u578b\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u5730\u7403\u7269\u7406\u5b66\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u6709\u5168\u9762\u7efc\u8ff0\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u5168\u6d41\u7a0b\u7684\u6280\u672f\u548c\u65b9\u6cd5\u5206\u6790\u3002", "method": "\u4ece\u6570\u636e\u6536\u96c6\u3001\u9884\u5904\u7406\u5230\u6a21\u578b\u67b6\u6784\u9009\u62e9\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u90e8\u7f72\uff0c\u8be6\u7ec6\u5206\u6790\u4e86\u5404\u9636\u6bb5\u5173\u952e\u6280\u672f\uff0c\u5e76\u9488\u5bf9\u5730\u7403\u7269\u7406\u6570\u636e\u7684\u591a\u6837\u6027\u3001\u590d\u6742\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u7ea6\u675f\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5c06\u7269\u7406\u7ea6\u675f\u878d\u5165\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u7269\u7406\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u586b\u8865\u4e86\u5730\u7403\u7269\u7406\u5b66\u9886\u57df\u57fa\u7840\u6a21\u578b\u5168\u6d41\u7a0b\u7efc\u8ff0\u7684\u7a7a\u767d\uff0c\u8fd8\u4e3a\u5730\u7403\u7269\u7406\u6570\u636e\u5206\u6790\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u521b\u65b0\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2504.17393", "pdf": "https://arxiv.org/pdf/2504.17393", "abs": "https://arxiv.org/abs/2504.17393", "authors": ["Vesna Nowack", "Dalal Alrajeh", "Carolina Gutierrez Mu\u00f1oz", "Katie Thomas", "William Hobson", "Catherine Hamilton-Giachritsis", "Patrick Benjamin", "Tim Grant", "Juliane A. Kloess", "Jessica Woodhams"], "title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "10 pages, 1 figure", "summary": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6267\u6cd5\u9886\u57df\u4e2dAI\u8f85\u52a9\u7cfb\u7edf\u7684\u7528\u6237\u9700\u6c42\uff0c\u5f3a\u8c03\u9ad8\u6548\u6570\u636e\u5904\u7406\u3001\u53ef\u6269\u5c55\u6027\u3001\u51c6\u786e\u6027\u7b49\u5173\u952e\u8981\u6c42\uff0c\u5e76\u6307\u51fa\u5b8c\u5168\u81ea\u52a8\u5316\u96be\u4ee5\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u586b\u8865\u6267\u6cd5\u9886\u57dfAI\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u7528\u6237\u9700\u6c42\u7a7a\u767d\uff0c\u4e86\u89e3\u4eba\u7c7b\u5728\u7cfb\u7edf\u4e2d\u7684\u8d23\u4efb\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u7814\u7a76\u5206\u6790\u6267\u6cd5\u673a\u6784\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bc6\u522b\u73b0\u6709\u5b9e\u8df5\u7684\u5c40\u9650\u6027\u5e76\u63a2\u7d22\u7528\u6237\u9700\u6c42\u3002", "result": "\u53c2\u4e0e\u8005\u63d0\u51fa\u7cfb\u7edf\u9700\u9ad8\u6548\u5904\u7406\u6570\u636e\u3001\u6ee1\u8db3\u53ef\u6269\u5c55\u6027\u7b49\u8981\u6c42\uff0c\u5e76\u5f3a\u8c03\u4eba\u5de5\u5ba1\u6838\u548c\u53cd\u9988\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7531\u4e8e\u6267\u6cd5\u9886\u57df\u7684\u590d\u6742\u6027\uff0c\u7cfb\u7edf\u96be\u4ee5\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u9700\u7ed3\u5408\u4eba\u5de5\u76d1\u7763\u548c\u53cd\u9988\u3002"}}
{"id": "2504.16974", "pdf": "https://arxiv.org/pdf/2504.16974", "abs": "https://arxiv.org/abs/2504.16974", "authors": ["Hidde Makimei", "Shuai Wang", "Willem van Peursen"], "title": "Seeing The Words: Evaluating AI-generated Biblical Art", "categories": ["cs.CY", "cs.CV", "cs.MM", "I.4.8; I.4.0; I.3.3; I.3.0"], "comment": null, "summary": "The past years witnessed a significant amount of Artificial Intelligence (AI)\ntools that can generate images from texts. This triggers the discussion of\nwhether AI can generate accurate images using text from the Bible with respect\nto the corresponding biblical contexts and backgrounds. Despite some existing\nattempts at a small scale, little work has been done to systematically evaluate\nthese generated images. In this work, we provide a large dataset of over 7K\nimages using biblical text as prompts. These images were evaluated with\nmultiple neural network-based tools on various aspects. We provide an\nassessment of accuracy and some analysis from the perspective of religion and\naesthetics. Finally, we discuss the use of the generated images and reflect on\nthe performance of the AI generators.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u751f\u6210\u5723\u7ecf\u6587\u672c\u76f8\u5173\u56fe\u50cf\u7684\u51c6\u786e\u6027\uff0c\u63d0\u4f9b\u4e867K\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5de5\u5177\u8bc4\u4f30\u4e86\u5176\u5b97\u6559\u548c\u7f8e\u5b66\u8868\u73b0\u3002", "motivation": "\u63a2\u8ba8AI\u662f\u5426\u80fd\u6839\u636e\u5723\u7ecf\u6587\u672c\u751f\u6210\u7b26\u5408\u80cc\u666f\u7684\u51c6\u786e\u56fe\u50cf\uff0c\u586b\u8865\u7cfb\u7edf\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5927\u578b\u6570\u636e\u96c6\uff087K\u56fe\u50cf\uff09\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5de5\u5177\u591a\u89d2\u5ea6\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u3002", "result": "\u63d0\u4f9b\u4e86\u51c6\u786e\u6027\u8bc4\u4f30\uff0c\u5e76\u4ece\u5b97\u6559\u548c\u7f8e\u5b66\u89d2\u5ea6\u5206\u6790\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8868\u73b0\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u751f\u6210\u56fe\u50cf\u7684\u7528\u9014\uff0c\u5e76\u53cd\u601d\u4e86AI\u751f\u6210\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2504.17421", "pdf": "https://arxiv.org/pdf/2504.17421", "abs": "https://arxiv.org/abs/2504.17421", "authors": ["Yang Liu", "Bingjie Yan", "Tianyuan Zou", "Jianqing Zhang", "Zixuan Gu", "Jianbing Ding", "Xidong Wang", "Jingyi Li", "Xiaozhou Ye", "Ye Ouyang", "Qiang Yang", "Ya-Qin Zhang"], "title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications.", "AI": {"tldr": "\u63d0\u51fa\u5927\u6a21\u578b\u4e0e\u5c0f\u6a21\u578b\u534f\u540c\u5408\u4f5c\u7684\u65b9\u6cd5\uff0c\u4ee5\u52a0\u901f\u5927\u6a21\u578b\u5728\u79c1\u6709\u9886\u57df\u7684\u9002\u5e94\u5e76\u91ca\u653eAI\u65b0\u6f5c\u529b\u3002", "motivation": "\u5927\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u5c0f\u6a21\u578b\u867d\u80fd\u529b\u8f83\u5f31\u4f46\u66f4\u9ad8\u6548\u4e14\u53ef\u5b9a\u5236\u5316\u3002\u901a\u8fc7\u534f\u540c\u5408\u4f5c\uff0c\u53ef\u4ee5\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63a2\u8ba8\u5927\u6a21\u578b\u4e0e\u5c0f\u6a21\u578b\u534f\u540c\u7684\u7b56\u7565\uff0c\u5206\u6790\u6311\u6218\u4e0e\u673a\u9047\uff0c\u5e76\u5021\u5bfc\u884c\u4e1a\u9a71\u52a8\u7684\u591a\u76ee\u6807\u57fa\u51c6\u7814\u7a76\u3002", "result": "\u63d0\u51fa\u534f\u540c\u5408\u4f5c\u80fd\u52a0\u901f\u5927\u6a21\u578b\u5728\u79c1\u6709\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u91ca\u653eAI\u65b0\u6f5c\u529b\u3002", "conclusion": "\u5efa\u8bae\u884c\u4e1a\u4f18\u5148\u7814\u7a76\u591a\u76ee\u6807\u57fa\u51c6\uff0c\u63a8\u52a8\u5927\u6a21\u578b\u4e0e\u5c0f\u6a21\u578b\u5728\u79c1\u6709\u9886\u57df\u7684\u534f\u540c\u5e94\u7528\u3002"}}
{"id": "2504.17062", "pdf": "https://arxiv.org/pdf/2504.17062", "abs": "https://arxiv.org/abs/2504.17062", "authors": ["Yu Guo", "Zhiqiang Lao", "Xiyun Song", "Yubin Zhou", "Zongfang Lin", "Heather Yu"], "title": "ePBR: Extended PBR Materials in Image Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "8 pages without references, 7 figures, accepted in CVPRW 2025", "summary": "Realistic indoor or outdoor image synthesis is a core challenge in computer\nvision and graphics. The learning-based approach is easy to use but lacks\nphysical consistency, while traditional Physically Based Rendering (PBR) offers\nhigh realism but is computationally expensive. Intrinsic image representation\noffers a well-balanced trade-off, decomposing images into fundamental\ncomponents (intrinsic channels) such as geometry, materials, and illumination\nfor controllable synthesis. However, existing PBR materials struggle with\ncomplex surface models, particularly high-specular and transparent surfaces. In\nthis work, we extend intrinsic image representations to incorporate both\nreflection and transmission properties, enabling the synthesis of transparent\nmaterials such as glass and windows. We propose an explicit intrinsic\ncompositing framework that provides deterministic, interpretable image\nsynthesis. With the Extended PBR (ePBR) Materials, we can effectively edit the\nmaterials with precise controls.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u7269\u7406\u6e32\u67d3\uff08ePBR\uff09\u6750\u6599\uff0c\u7ed3\u5408\u53cd\u5c04\u548c\u900f\u5c04\u7279\u6027\uff0c\u7528\u4e8e\u5408\u6210\u900f\u660e\u6750\u6599\uff08\u5982\u73bb\u7483\u548c\u7a97\u6237\uff09\uff0c\u63d0\u4f9b\u53ef\u63a7\u7684\u56fe\u50cf\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\uff08PBR\uff09\u5728\u590d\u6742\u8868\u9762\uff08\u5982\u9ad8\u5149\u548c\u900f\u660e\u8868\u9762\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u7f3a\u4e4f\u7269\u7406\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5e73\u8861\u7684\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u4e86\u56fa\u6709\u56fe\u50cf\u8868\u793a\uff0c\u7ed3\u5408\u53cd\u5c04\u548c\u900f\u5c04\u7279\u6027\uff0c\u63d0\u51fa\u663e\u5f0f\u56fa\u6709\u5408\u6210\u6846\u67b6\u3002", "result": "\u901a\u8fc7ePBR\u6750\u6599\uff0c\u5b9e\u73b0\u4e86\u5bf9\u900f\u660e\u6750\u6599\u7684\u6709\u6548\u7f16\u8f91\u548c\u7cbe\u786e\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u900f\u660e\u6750\u6599\u5408\u6210\u4e2d\u63d0\u4f9b\u4e86\u786e\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f18\u4e8e\u4f20\u7edfPBR\u3002"}}
{"id": "2504.17424", "pdf": "https://arxiv.org/pdf/2504.17424", "abs": "https://arxiv.org/abs/2504.17424", "authors": ["Tomoki Mizuno", "Kazuya Yabashi", "Tsuyoshi Tasaki"], "title": "Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We have developed a new method to estimate a Next Viewpoint (NV) which is\neffective for pose estimation of simple-shaped products for product display\nrobots in retail stores. Pose estimation methods using Neural Networks (NN)\nbased on an RGBD camera are highly accurate, but their accuracy significantly\ndecreases when the camera acquires few texture and shape features at a current\nview point. However, it is difficult for previous mathematical model-based\nmethods to estimate effective NV which is because the simple shaped objects\nhave few shape features. Therefore, we focus on the relationship between the\npose estimation and NV estimation. When the pose estimation is more accurate,\nthe NV estimation is more accurate. Therefore, we develop a new pose estimation\nNN that estimates NV simultaneously. Experimental results showed that our NV\nestimation realized a pose estimation success rate 77.3\\%, which was 7.4pt\nhigher than the mathematical model-based NV calculation did. Moreover, we\nverified that the robot using our method displayed 84.2\\% of products.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u4f30\u8ba1\u4e0b\u4e00\u4e2a\u89c6\u89d2\uff08NV\uff09\u6765\u63d0\u9ad8\u96f6\u552e\u673a\u5668\u4eba\u5bf9\u7b80\u5355\u5f62\u72b6\u4ea7\u54c1\u7684\u59ff\u6001\u4f30\u8ba1\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684RGBD\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u7279\u5f81\u8f83\u5c11\u65f6\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u800c\u4f20\u7edf\u6570\u5b66\u6a21\u578b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u4f30\u8ba1NV\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u59ff\u6001\u4f30\u8ba1\u795e\u7ecf\u7f51\u7edc\uff0c\u540c\u65f6\u4f30\u8ba1NV\uff0c\u5229\u7528\u59ff\u6001\u4f30\u8ba1\u4e0eNV\u4f30\u8ba1\u7684\u5173\u7cfb\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cNV\u4f30\u8ba1\u4f7f\u59ff\u6001\u4f30\u8ba1\u6210\u529f\u7387\u63d0\u9ad87.4\u4e2a\u767e\u5206\u70b9\u81f377.3%\uff0c\u673a\u5668\u4eba\u6210\u529f\u5c55\u793a84.2%\u7684\u4ea7\u54c1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7b80\u5355\u5f62\u72b6\u4ea7\u54c1\u7684\u59ff\u6001\u4f30\u8ba1\u548c\u5c55\u793a\u6548\u7387\u3002"}}
{"id": "2504.17426", "pdf": "https://arxiv.org/pdf/2504.17426", "abs": "https://arxiv.org/abs/2504.17426", "authors": ["Michele Carissimi", "Martina Saletta", "Claudio Ferretti"], "title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories.", "AI": {"tldr": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4e3b\u9898\u5efa\u6a21\u6280\u672f\uff0c\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u8bc6\u522bPython\u4ee3\u7801\u4e3b\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u4ee3\u7801\u8868\u793a\u3002", "motivation": "\u7406\u89e3\u6e90\u4ee3\u7801\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff08\u5982\u7ef4\u62a4\u548c\u91cd\u7528\uff09\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u6280\u672f\uff08\u5982LLM\u548c\u4e3b\u9898\u5efa\u6a21\uff09\u7684\u7ed3\u5408\u53ef\u80fd\u63d0\u4f9b\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LLM\u751f\u6210\u4ee3\u7801\u6458\u8981\uff0c\u518d\u5bf9\u5176\u5e94\u7528\u4e3b\u9898\u5efa\u6a21\uff0c\u5e76\u4e0e\u57fa\u4e8e\u51fd\u6570\u540d\u548c\u73b0\u6709\u6587\u6863\u5b57\u7b26\u4e32\u7684\u4e3b\u9898\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cLLM\u751f\u6210\u7684\u6458\u8981\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u4ee3\u7801\u7ed3\u6784\u8868\u793a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u6587\u6863\u751f\u6210\u3001\u4ee3\u7801\u641c\u7d22\u7b49\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17428", "pdf": "https://arxiv.org/pdf/2504.17428", "abs": "https://arxiv.org/abs/2504.17428", "authors": ["Murali Sridharan", "Mika M\u00e4ntyl\u00e4", "Leevi Rantala"], "title": "Detection, Classification and Prevalence of Self-Admitted Aging Debt", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.GL", "D.2.7; D.2.9"], "comment": "Draft", "summary": "Context: Previous research on software aging is limited with focus on dynamic\nruntime indicators like memory and performance, often neglecting evolutionary\nindicators like source code comments and narrowly examining legacy issues\nwithin the TD context. Objective: We introduce the concept of Aging Debt (AD),\nrepresenting the increased maintenance efforts and costs needed to keep\nsoftware updated. We study AD through Self-Admitted Aging Debt (SAAD) observed\nin source code comments left by software developers. Method: We employ a\nmixed-methods approach, combining qualitative and quantitative analyses to\ndetect and measure AD in software. This includes framing SAAD patterns from the\nsource code comments after analysing the source code context, then utilizing\nthe SAAD patterns to detect SAAD comments. In the process, we develop a\ntaxonomy for SAAD that reflects the temporal aging of software and its\nassociated debt. Then we utilize the taxonomy to quantify the different types\nof AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes\ntemporal software aging into Active and Dormant types. Our extensive analysis\nof over 9,000+ Open Source Software (OSS) repositories reveals that more than\n21% repositories exhibit signs of SAAD as observed from our gold standard SAAD\ndataset. Notably, Dormant AD emerges as the predominant category, highlighting\na critical but often overlooked aspect of software maintenance. Conclusion: As\nsoftware volume grows annually, so do evolutionary aging and maintenance\nchallenges; our proposed taxonomy can aid researchers in detailed software\naging studies and help practitioners develop improved and proactive maintenance\nstrategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u201c\u8001\u5316\u503a\u52a1\u201d\uff08AD\uff09\u6982\u5ff5\uff0c\u901a\u8fc7\u6e90\u4ee3\u7801\u6ce8\u91ca\u4e2d\u7684\u201c\u81ea\u8ba4\u8001\u5316\u503a\u52a1\u201d\uff08SAAD\uff09\u7814\u7a76\u8f6f\u4ef6\u8001\u5316\uff0c\u5e76\u63d0\u51fa\u5206\u7c7b\u6cd5\u91cf\u5316\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u7684AD\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8fd0\u884c\u65f6\u6307\u6807\uff0c\u5ffd\u7565\u6f14\u5316\u6307\u6807\uff08\u5982\u6e90\u4ee3\u7801\u6ce8\u91ca\uff09\uff0c\u4e14\u5bf9\u6280\u672f\u503a\u52a1\uff08TD\uff09\u80cc\u666f\u4e0b\u7684\u8001\u5316\u95ee\u9898\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff08\u5b9a\u6027\u4e0e\u5b9a\u91cf\u5206\u6790\uff09\uff0c\u4ece\u6e90\u4ee3\u7801\u6ce8\u91ca\u4e2d\u63d0\u53d6SAAD\u6a21\u5f0f\u5e76\u5efa\u7acb\u5206\u7c7b\u6cd5\uff0c\u91cf\u5316\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u7684AD\u3002", "result": "\u5206\u7c7b\u6cd5\u5c06\u8f6f\u4ef6\u8001\u5316\u5206\u4e3a\u6d3b\u8dc3\u548c\u4f11\u7720\u4e24\u7c7b\uff0c\u5206\u67909000+\u5f00\u6e90\u4ed3\u5e93\u53d1\u73b021%\u5b58\u5728SAAD\uff0c\u4f11\u7720AD\u4e3a\u4e3b\u8981\u7c7b\u578b\u3002", "conclusion": "\u8f6f\u4ef6\u89c4\u6a21\u6269\u5927\u5e26\u6765\u8001\u5316\u6311\u6218\uff0c\u5206\u7c7b\u6cd5\u53ef\u52a9\u529b\u7814\u7a76\u548c\u5b9e\u8df5\uff0c\u4f18\u5316\u7ef4\u62a4\u7b56\u7565\u3002"}}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy.", "AI": {"tldr": "HMI\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u77e5\u8bc6\u7ba1\u7406\u7684\u591a\u79df\u6237\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5c42\u7ba1\u7406PLM\u77e5\u8bc6\uff0c\u663e\u8457\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\uff0c\u652f\u6301\u9ad8\u6548\u670d\u52a1\u5927\u91cf\u79df\u6237\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u5728\u591a\u79df\u6237\u73af\u5883\u4e2d\u96be\u4ee5\u9ad8\u6548\u670d\u52a1\uff0cHMI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u5c06PLM\u77e5\u8bc6\u5206\u4e3a\u901a\u7528\u3001\u9886\u57df\u7279\u5b9a\u548c\u4efb\u52a1\u7279\u5b9a\u4e09\u7c7b\uff0c\u6784\u5efa\u5206\u5c42PLM\uff08hPLM\uff09\uff1b2. \u901a\u8fc7\u9891\u7387\u66f4\u65b0\u9886\u57df\u77e5\u8bc6\u6811\u548c\u53c2\u6570\u4ea4\u6362\u7ba1\u7406\u4efb\u52a1\u77e5\u8bc6\uff1b3. \u7cfb\u7edf\u4f18\u5316\u5305\u62ec\u5206\u5c42\u77e5\u8bc6\u9884\u53d6\u548c\u6279\u5904\u7406\u77e9\u9635\u4e58\u6cd5\u3002", "result": "HMI\u5728\u5355\u4e2aGPU\u4e0a\u53ef\u9ad8\u6548\u670d\u52a110,000\u4e2ahPLM\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "HMI\u901a\u8fc7\u5206\u5c42\u77e5\u8bc6\u7ba1\u7406\u548c\u7cfb\u7edf\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u79df\u6237\u73af\u5883\u4e2dPLM\u7684\u8d44\u6e90\u6548\u7387\u548c\u63a8\u7406\u541e\u5410\u91cf\u3002"}}
{"id": "2504.17258", "pdf": "https://arxiv.org/pdf/2504.17258", "abs": "https://arxiv.org/abs/2504.17258", "authors": ["Md Ashiqur Rahman", "Raymond A. Yeh"], "title": "Group Downsampling with Equivariant Anti-aliasing", "categories": ["cs.LG", "cs.CV", "math.GR"], "comment": null, "summary": "Downsampling layers are crucial building blocks in CNN architectures, which\nhelp to increase the receptive field for learning high-level features and\nreduce the amount of memory/computation in the model. In this work, we study\nthe generalization of the uniform downsampling layer for group equivariant\narchitectures, e.g., G-CNNs. That is, we aim to downsample signals (feature\nmaps) on general finite groups with anti-aliasing. This involves the following:\n(a) Given a finite group and a downsampling rate, we present an algorithm to\nform a suitable choice of subgroup. (b) Given a group and a subgroup, we study\nthe notion of bandlimited-ness and propose how to perform anti-aliasing.\nNotably, our method generalizes the notion of downsampling based on classical\nsampling theory. When the signal is on a cyclic group, i.e., periodic, our\nmethod recovers the standard downsampling of an ideal low-pass filter followed\nby a subsampling operation. Finally, we conducted experiments on image\nclassification tasks demonstrating that the proposed downsampling operation\nimproves accuracy, better preserves equivariance, and reduces model size when\nincorporated into G-equivariant networks", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7fa4\u7b49\u53d8\u67b6\u6784\uff08\u5982G-CNNs\uff09\u4e2d\u63a8\u5e7f\u5747\u5300\u4e0b\u91c7\u6837\u5c42\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u7fa4\u548c\u6297\u6df7\u53e0\u7684\u4e0b\u91c7\u6837\u7b97\u6cd5\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e0b\u91c7\u6837\u5c42\u662fCNN\u67b6\u6784\u4e2d\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7fa4\u7b49\u53d8\u67b6\u6784\u4e2d\u7684\u901a\u7528\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u9002\u7528\u4e8e\u6709\u9650\u7fa4\u7684\u4e0b\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u6839\u636e\u6709\u9650\u7fa4\u548c\u4e0b\u91c7\u6837\u7387\u9009\u62e9\u5b50\u7fa4\uff0c\u5e76\u7814\u7a76\u4e86\u5e26\u9650\u6027\u548c\u6297\u6df7\u53e0\u65b9\u6cd5\uff0c\u63a8\u5e7f\u4e86\u7ecf\u5178\u91c7\u6837\u7406\u8bba\u4e2d\u7684\u4e0b\u91c7\u6837\u6982\u5ff5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u66f4\u597d\u5730\u4fdd\u6301\u4e86\u7b49\u53d8\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7fa4\u7b49\u53d8\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e0b\u91c7\u6837\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2504.17461", "pdf": "https://arxiv.org/pdf/2504.17461", "abs": "https://arxiv.org/abs/2504.17461", "authors": ["Vipin Singh", "Tianheng Ling", "Teodor Chiaburu", "Felix Biessmann"], "title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation", "summary": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u534f\u8bae\uff0c\u7528\u4e8e\u57ce\u5e02\u6c61\u6c34\u7cfb\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u91cd\u70b9\u8003\u5bdf\u9884\u6d4b\u6027\u80fd\u3001\u6a21\u578b\u590d\u6742\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u5168\u5c40\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u66f4\u9ad8\uff0c\u800c\u5c40\u90e8\u6a21\u578b\u5728\u5206\u6563\u573a\u666f\u4e2d\u66f4\u5177\u97e7\u6027\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u6781\u7aef\u964d\u96e8\u9891\u53d1\uff0c\u5bf9\u57ce\u5e02\u6c61\u6c34\u7cfb\u7edf\u9020\u6210\u538b\u529b\uff0c\u4f20\u7edf\u7269\u7406\u6a21\u578b\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\uff0c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u8bc4\u4f30\u534f\u8bae\uff0c\u6bd4\u8f83\u5168\u5c40\u6a21\u578b\u548c\u5c40\u90e8\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u8bef\u5dee\u6a21\u578b\u8bc4\u4f30\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u548c\u6297\u5e72\u6270\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5168\u5c40\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u66f4\u4f18\uff0c\u5c40\u90e8\u6a21\u578b\u5728\u5206\u6563\u573a\u666f\u4e2d\u66f4\u5177\u97e7\u6027\uff1b\u957f\u9884\u6d4b\u8303\u56f4\u7684\u6a21\u578b\u5bf9\u6570\u636e\u5e72\u6270\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u6301\u7eed\u57ce\u5e02\u6c61\u6c34\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u5173\u5b9e\u73b0\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2504.17314", "pdf": "https://arxiv.org/pdf/2504.17314", "abs": "https://arxiv.org/abs/2504.17314", "authors": ["Miaoyun Zhao", "Qiang Zhang", "Chenrong Li"], "title": "Class-Conditional Distribution Balancing for Group Robust Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u504f\u5dee\u6807\u6ce8\u6216\u9884\u6d4b\u7684\u9c81\u68d2\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u6837\u672c\u5e73\u8861\u7c7b\u6761\u4ef6\u5206\u5e03\uff0c\u6709\u6548\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u504f\u5dee\u6807\u6ce8\u6216\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u96be\u4ee5\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u7f55\u89c1\u9886\u57df\u3002", "method": "\u901a\u8fc7\u51cf\u5c11\u865a\u5047\u56e0\u7d20\u4e0e\u6807\u7b7e\u4fe1\u606f\u7684\u4e92\u4fe1\u606f\uff0c\u91c7\u7528\u6837\u672c\u91cd\u65b0\u52a0\u6743\u7b56\u7565\u5b9e\u73b0\u7c7b\u6761\u4ef6\u5206\u5e03\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u5f02\uff0c\u5ab2\u7f8e\u4f9d\u8d56\u504f\u5dee\u76d1\u7763\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u65e0\u9700\u504f\u5dee\u6807\u6ce8\u6216\u9884\u6d4b\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u573a\u666f\u3002"}}
{"id": "2504.17471", "pdf": "https://arxiv.org/pdf/2504.17471", "abs": "https://arxiv.org/abs/2504.17471", "authors": ["Yacine Belal", "Mohamed Maouche", "Sonia Ben Mokhtar", "Anthony Simonet-Boulogne"], "title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory.", "AI": {"tldr": "GRANITE\u6846\u67b6\u901a\u8fc7\u5386\u53f2\u611f\u77e5\u7684\u62dc\u5360\u5ead\u6297\u6027\u5bf9\u7b49\u91c7\u6837\u534f\u8bae\uff08HaPS\uff09\u548c\u81ea\u9002\u5e94\u6982\u7387\u9608\u503c\uff08APT\uff09\uff0c\u5728\u7a00\u758f\u52a8\u6001\u56fe\u4e0a\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u8fbe30%\u62dc\u5360\u5ead\u8282\u70b9\u7684\u9c81\u68d2\u6027\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3Gossip Learning\uff08GL\uff09\u5728\u52a8\u6001\u901a\u4fe1\u56fe\u4e2d\u5bf9\u62dc\u5360\u5ead\u653b\u51fb\uff08\u6a21\u578b\u6295\u6bd2\uff09\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5f53\u62dc\u5360\u5ead\u8282\u70b9\u653b\u51fbRPS\u534f\u8bae\u65f6\u3002", "method": "\u7ed3\u5408HaPS\u534f\u8bae\uff08\u51cf\u5c11\u5bf9\u6297\u6027\u5f71\u54cd\uff09\u548cAPT\uff08\u6839\u636e\u62dc\u5360\u5ead\u8282\u70b9\u6bd4\u4f8b\u81ea\u9002\u5e94\u8bbe\u7f6e\u805a\u5408\u9608\u503c\uff09\u3002", "result": "GRANITE\u5728\u9ad8\u8fbe30%\u62dc\u5360\u5ead\u8282\u70b9\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6536\u655b\uff0c\u5b66\u4e60\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u9002\u7528\u4e8e\u6bd4\u73b0\u6709\u7406\u8bba\u7a00\u758f9\u500d\u7684\u56fe\u3002", "conclusion": "GRANITE\u4e3a\u7a00\u758f\u52a8\u6001\u56fe\u4e0a\u7684\u9c81\u68d2\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17353", "pdf": "https://arxiv.org/pdf/2504.17353", "abs": "https://arxiv.org/abs/2504.17353", "authors": ["Chengguang Gan", "Sunbowen Lee", "Zhixi Cai", "Yanbin Wei", "Lei Zheng", "Yunhao Liang", "Shiwen Ni", "Tatsunori Mori"], "title": "M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": null, "summary": "Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\nof information extraction and model interpretability. MRE aims to leverage the\nmutual understanding between tasks of different granularities, enhancing the\nperformance of both coarse-grained and fine-grained tasks through joint\nmodeling. While MRE has been explored and validated in the textual domain, its\napplicability to visual and multimodal domains remains unexplored. In this\nwork, we extend MRE to the multimodal information extraction domain for the\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\nthis task. To address the challenges posed by M-MRE, we further propose a\nPrompt Format Adapter (PFA) that is fully compatible with various Large\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\nalso be observed in the M-MRE task, a multimodal text-image understanding\nscenario. This provides strong evidence that MRE facilitates mutual gains\nacross three interrelated tasks, confirming its generalizability beyond the\ntextual domain.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5c06\u4e92\u589e\u5f3a\u6548\u5e94\uff08MRE\uff09\u6269\u5c55\u5230\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u9886\u57df\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u4e92\u589e\u5f3a\u6548\u5e94\uff08M-MRE\uff09\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u6570\u636e\u96c6\u3002\u901a\u8fc7Prompt Format Adapter\uff08PFA\uff09\u89e3\u51b3\u6311\u6218\uff0c\u5b9e\u9a8c\u8bc1\u660eMRE\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\u540c\u6837\u6709\u6548\u3002", "motivation": "\u63a2\u7d22MRE\u5728\u89c6\u89c9\u548c\u591a\u6a21\u6001\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faM-MRE\u4efb\u52a1\uff0c\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1PFA\u9002\u914d\u5668\u4ee5\u517c\u5bb9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eMRE\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\u6709\u6548\uff0c\u652f\u6301\u4efb\u52a1\u95f4\u7684\u4e92\u589e\u5f3a\u3002", "conclusion": "MRE\u5177\u6709\u8de8\u9886\u57df\u7684\u901a\u7528\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.17379", "pdf": "https://arxiv.org/pdf/2504.17379", "abs": "https://arxiv.org/abs/2504.17379", "authors": ["Hassan Keshvarikhojasteh", "Mihail Tifrea", "Sibylle Hess", "Josien P. W. Pluim", "Mitko Veta"], "title": "A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Multiple instance learning (MIL) is a promising approach for weakly\nsupervised classification in pathology using whole slide images (WSIs).\nHowever, conventional MIL methods such as Attention-Based Deep Multiple\nInstance Learning (ABMIL) typically disregard spatial interactions among\npatches that are crucial to pathological diagnosis. Recent advancements, such\nas Transformer based MIL (TransMIL), have incorporated spatial context and\ninter-patch relationships. However, it remains unclear whether explicitly\nmodeling patch relationships yields similar performance gains in ABMIL, which\nrelies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs\nTransformer-based layers, introducing a fundamental architectural shift at the\ncost of substantially increased computational complexity. In this work, we\nenhance the ABMIL framework by integrating interaction-aware representations to\naddress this question. Our proposed model, Global ABMIL (GABMIL), explicitly\ncaptures inter-instance dependencies while preserving computational efficiency.\nExperimental results on two publicly available datasets for tumor subtyping in\nbreast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage\npoint improvement in AUPRC and a 5 percentage point increase in the Kappa score\nover ABMIL, with minimal or no additional computational overhead. These\nfindings underscore the importance of incorporating patch interactions within\nMIL frameworks.", "AI": {"tldr": "GABMIL\u6539\u8fdbABMIL\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u6355\u83b7\u5b9e\u4f8b\u95f4\u4f9d\u8d56\u5173\u7cfb\u63d0\u5347\u6027\u80fd\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8eABMIL\u3002", "motivation": "\u4f20\u7edfMIL\u65b9\u6cd5\uff08\u5982ABMIL\uff09\u5ffd\u89c6\u7a7a\u95f4\u4ea4\u4e92\uff0c\u800cTransMIL\u867d\u5f15\u5165\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u5728ABMIL\u4e2d\u663e\u5f0f\u5efa\u6a21\u8865\u4e01\u5173\u7cfb\u662f\u5426\u80fd\u5e26\u6765\u7c7b\u4f3c\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faGABMIL\uff0c\u5728ABMIL\u6846\u67b6\u4e2d\u96c6\u6210\u4ea4\u4e92\u611f\u77e5\u8868\u793a\uff0c\u663e\u5f0f\u6355\u83b7\u5b9e\u4f8b\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e73\u817a\u764c\u548c\u80ba\u764c\u4e9a\u578b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGABMIL\u7684AUPRC\u63d0\u53477%\uff0cKappa\u5206\u6570\u63d0\u53475%\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u51e0\u4e4e\u4e0d\u53d8\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u8865\u4e01\u4ea4\u4e92\u5bf9MIL\u6846\u67b6\u81f3\u5173\u91cd\u8981\uff0cGABMIL\u5728\u6027\u80fd\u548c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2504.17490", "pdf": "https://arxiv.org/pdf/2504.17490", "abs": "https://arxiv.org/abs/2504.17490", "authors": ["Mingqi Yuan", "Qi Wang", "Guozheng Ma", "Bo Li", "Xin Jin", "Yunbo Wang", "Xiaokang Yang", "Wenjun Zeng", "Dacheng Tao"], "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages", "summary": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine.", "AI": {"tldr": "Plasticine\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u5851\u6027\u4f18\u5316\uff0c\u63d0\u4f9b\u591a\u79cd\u65b9\u6cd5\u548c\u6307\u6807\u3002", "motivation": "\u5f00\u53d1\u7ec8\u8eab\u5b66\u4e60\u4ee3\u7406\u5bf9\u901a\u7528\u4eba\u5de5\u667a\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5e38\u56e0\u53ef\u5851\u6027\u635f\u5931\u800c\u96be\u4ee5\u6301\u7eed\u9002\u5e94\u3002", "method": "Plasticine\u6846\u67b6\u96c6\u6210\u4e8613\u79cd\u7f13\u89e3\u65b9\u6cd5\u300110\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5728\u4e0d\u540c\u975e\u5e73\u7a33\u6027\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "Plasticine\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u91cf\u5316\u53ef\u5851\u6027\u635f\u5931\u3001\u8bc4\u4f30\u7f13\u89e3\u7b56\u7565\u548c\u5206\u6790\u52a8\u6001\u7684\u5de5\u5177\u3002", "conclusion": "Plasticine\u586b\u8865\u4e86\u53ef\u5851\u6027\u4f18\u5316\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u548c\u534f\u8bae\u3002"}}
{"id": "2504.17618", "pdf": "https://arxiv.org/pdf/2504.17618", "abs": "https://arxiv.org/abs/2504.17618", "authors": ["Nikita Gabdullin"], "title": "The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages, 10 figures, 4 tables, 4 equations", "summary": "Hessians of neural network (NN) contain essential information about the\ncurvature of NN loss landscapes which can be used to estimate NN generalization\ncapabilities. We have previously proposed generalization criteria that rely on\nthe observation that Hessian eigenvalue spectral density (HESD) behaves\nsimilarly for a wide class of NNs. This paper further studies their\napplicability by investigating factors that can result in different types of\nHESD. We conduct a wide range of experiments showing that HESD mainly has\npositive eigenvalues (MP-HESD) for NN training and fine-tuning with various\noptimizers on different datasets with different preprocessing and augmentation\nprocedures. We also show that mainly negative HESD (MN-HESD) is a consequence\nof external gradient manipulation, indicating that the previously proposed\nHessian analysis methodology cannot be applied in such cases. We also propose\ncriteria and corresponding conditions to determine HESD type and estimate NN\ngeneralization potential. These HESD types and previously proposed\ngeneralization criteria are combined into a unified HESD analysis methodology.\nFinally, we discuss how HESD changes during training, and show the occurrence\nof quasi-singular (QS) HESD and its influence on the proposed methodology and\non the conventional assumptions about the relation between Hessian eigenvalues\nand NN loss landscape curvature.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edcHessian\u77e9\u9635\u7279\u5f81\u503c\u8c31\u5bc6\u5ea6\uff08HESD\uff09\u7684\u7c7b\u578b\u53ca\u5176\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684HESD\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2dHESD\u7684\u53d8\u5316\u3002", "motivation": "\u7814\u7a76HESD\u7c7b\u578b\u53ca\u5176\u5bf9\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u4ee5\u6539\u8fdb\u73b0\u6709\u7684Hessian\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u4f18\u5316\u5668\u3001\u6570\u636e\u96c6\u548c\u9884\u5904\u7406\u65b9\u6cd5\u4e0b\u7684HESD\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u5224\u65adHESD\u7c7b\u578b\u7684\u6761\u4ef6\u548c\u6cdb\u5316\u6f5c\u529b\u4f30\u8ba1\u6807\u51c6\u3002", "result": "\u53d1\u73b0HESD\u4e3b\u8981\u5206\u4e3a\u6b63\u7279\u5f81\u503c\uff08MP-HESD\uff09\u548c\u8d1f\u7279\u5f81\u503c\uff08MN-HESD\uff09\uff0c\u5e76\u8868\u660eMN-HESD\u4e0e\u5916\u90e8\u68af\u5ea6\u64cd\u4f5c\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684HESD\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2dHESD\u7684\u53d8\u5316\u53ca\u5176\u5bf9\u4f20\u7edf\u5047\u8bbe\u7684\u5f71\u54cd\u3002"}}
{"id": "2504.17493", "pdf": "https://arxiv.org/pdf/2504.17493", "abs": "https://arxiv.org/abs/2504.17493", "authors": ["Luca-Andrei Fechete", "Mohamed Sana", "Fadhel Ayed", "Nicola Piovesan", "Wenjie Li", "Antonio De Domenico", "Tareq Si Salem"], "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u91cd\u70b9\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u5177\u4f53\u9700\u6c42\u3002", "method": "\u5c06\u6574\u4e2a\u4fe1\u53f7\u8303\u56f4\u5206\u89e3\u4e3a\u5c0f\u6bb5\uff0c\u52a8\u6001\u52a0\u6743\u7ec4\u5408\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u548c\u65b0\u65e0\u7ebf\u901a\u4fe1\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u5e94\u7528\u6027\u80fd\u3002", "conclusion": "\u4e3a\u9884\u6d4b\u7cfb\u7edf\u4e0e\u5b9e\u9645\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8fde\u63a5\u57fa\u7840\u3002"}}
{"id": "2504.17628", "pdf": "https://arxiv.org/pdf/2504.17628", "abs": "https://arxiv.org/abs/2504.17628", "authors": ["Abderrachid Hamrani", "Daniela Leizaola", "Renato Sousa", "Jose P. Ponce", "Stanley Mathis", "David G. Armstrong", "Anuradha Godavarty"], "title": "Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text-Guided Customization", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 8 figures, journal article", "summary": "Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,\nrequiring precise and efficient wound assessment to enhance patient outcomes.\nThis study introduces the Attention Diffusion Zero-shot Unsupervised System\n(ADZUS), a novel text-guided diffusion model that performs wound segmentation\nwithout relying on labeled training data. Unlike conventional deep learning\nmodels, which require extensive annotation, ADZUS leverages zero-shot learning\nto dynamically adapt segmentation based on descriptive prompts, offering\nenhanced flexibility and adaptability in clinical applications. Experimental\nevaluations demonstrate that ADZUS surpasses traditional and state-of-the-art\nsegmentation models, achieving an IoU of 86.68\\% and the highest precision of\n94.69\\% on the chronic wound dataset, outperforming supervised approaches such\nas FUSegNet. Further validation on a custom-curated DFU dataset reinforces its\nrobustness, with ADZUS achieving a median DSC of 75\\%, significantly surpassing\nFUSegNet's 45\\%. The model's text-guided segmentation capability enables\nreal-time customization of segmentation outputs, allowing targeted analysis of\nwound characteristics based on clinical descriptions. Despite its competitive\nperformance, the computational cost of diffusion-based inference and the need\nfor potential fine-tuning remain areas for future improvement. ADZUS represents\na transformative step in wound segmentation, providing a scalable, efficient,\nand adaptable AI-driven solution for medical imaging.", "AI": {"tldr": "ADZUS\u662f\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\u7684\u65e0\u76d1\u7763\u5206\u5272\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\u7684\u7cbe\u786e\u8bc4\u4f30\u5bf9\u6539\u5584\u60a3\u8005\u7ed3\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0cADZUS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ADZUS\u5229\u7528\u96f6\u6837\u672c\u5b66\u4e60\u548c\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u52a8\u6001\u9002\u5e94\u5206\u5272\u4efb\u52a1\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728\u6162\u6027\u4f24\u53e3\u6570\u636e\u96c6\u4e0a\uff0cADZUS\u7684IoU\u8fbe86.68%\uff0c\u7cbe\u5ea694.69%\uff0c\u663e\u8457\u4f18\u4e8eFUSegNet\u7b49\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "ADZUS\u4e3a\u533b\u5b66\u5f71\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u65e0\u76d1\u7763\u5206\u5272\u65b9\u6848\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u548c\u5fae\u8c03\u9700\u6c42\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2504.17497", "pdf": "https://arxiv.org/pdf/2504.17497", "abs": "https://arxiv.org/abs/2504.17497", "authors": ["Radia Berreziga", "Mohammed Brahimi", "Khairedine Kraim", "Hamid Azzoune"], "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u865a\u62df\u7b5b\u9009\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u5b50\u8868\u793a\u4e0a\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u6f5c\u5728\u504f\u5dee\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GCN\uff09\u548cLLM\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5c06LLM\u5d4c\u5165\u4e0eGCN\u9010\u5c42\u7ed3\u5408\uff0c\u5b9e\u73b0\u5c40\u90e8\u7ed3\u6784\u5b66\u4e60\u548c\u5168\u5c40\u5316\u5b66\u77e5\u8bc6\u7684\u878d\u5408\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u6df7\u5408\u6a21\u578bF1-score\u8fbe88.8%\uff0c\u4f18\u4e8eGCN\uff0887.9%\uff09\u3001XGBoost\uff0885.5%\uff09\u548cSVM\uff0885.4%\uff09\u3002", "conclusion": "\u8be5\u6df7\u5408\u67b6\u6784\u5728\u865a\u62df\u7b5b\u9009\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17655", "pdf": "https://arxiv.org/pdf/2504.17655", "abs": "https://arxiv.org/abs/2504.17655", "authors": ["Farhad Pourkamali-Anaraki"], "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "17 pages, 5 figures, and 2 tables", "summary": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\uff0c\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u6821\u51c6\u9009\u62e9\u548c\u6a21\u578b\u538b\u7f29\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5171\u5f62\u9884\u6d4b\u5728\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u5ea6\u53d8\u5316\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff08MobileNet\u3001DenseNet\u3001ResNet\uff09\u5fae\u8c03\u6709\u9650\u6807\u6ce8\u6570\u636e\uff0c\u6bd4\u8f83\u6709\u65e0\u6e29\u5ea6\u7f29\u653e\u7684\u6821\u51c6\u7ba1\u9053\uff0c\u8bc4\u4f30\u8986\u76d6\u7387\u548c\u9884\u6d4b\u96c6\u5927\u5c0f\u3002", "result": "\u5171\u5f62\u9884\u6d4b\u5373\u4f7f\u5728\u5c0f\u6837\u672c\u548c\u7b80\u5355\u975e\u5171\u5f62\u5206\u6570\u4e0b\u4e5f\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff1b\u6e29\u5ea6\u7f29\u653e\u4e0d\u4e00\u5b9a\u51cf\u5c0f\u9884\u6d4b\u96c6\u5927\u5c0f\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u566a\u58f0\u6807\u7b7e\u5bf9\u5171\u5f62\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u6a21\u578b\u538b\u7f29\u7b56\u7565\u3002"}}
{"id": "2504.17528", "pdf": "https://arxiv.org/pdf/2504.17528", "abs": "https://arxiv.org/abs/2504.17528", "authors": ["Weijie Liu", "Ziwei Zhan", "Carlee Joe-Wong", "Edith Ngai", "Jingpu Duan", "Deke Guo", "Xu Chen", "Xiaoxi Zhang"], "title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "11 pages, 7 figures, accepted by ICDCS 2025", "summary": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTACO\u7b97\u6cd5\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u7684\u8fc7\u6821\u6b63\u95ee\u9898\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u68af\u5ea6\u6821\u6b63\u548c\u6a21\u578b\u805a\u5408\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u91c7\u7528\u7edf\u4e00\u7684\u6a21\u578b\u6821\u6b63\u7cfb\u6570\uff0c\u53ef\u80fd\u5bfc\u81f4\u8fc7\u6821\u6b63\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u3002", "method": "\u63d0\u51faTACO\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5ba2\u6237\u7279\u5b9a\u7684\u68af\u5ea6\u6821\u6b63\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u805a\u5408\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1TACO\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u8d8a\u4e14\u7a33\u5b9a\uff0c\u5e76\u9996\u6b21\u63ed\u793a\u4e86\u8fc7\u6821\u6b63\u7684\u6839\u672c\u539f\u56e0\u3002", "conclusion": "TACO\u6709\u6548\u89e3\u51b3\u4e86\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.17693", "pdf": "https://arxiv.org/pdf/2504.17693", "abs": "https://arxiv.org/abs/2504.17693", "authors": ["Asier Bikandi", "Muhammad Shaheer", "Hriday Bavle", "Jayan Jevanesan", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "BIM-Constrained Optimization for Accurate Localization and Deviation Correction in Construction Monitoring", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Augmented reality (AR) applications for construction monitoring rely on\nreal-time environmental tracking to visualize architectural elements. However,\nconstruction sites present significant challenges for traditional tracking\nmethods due to featureless surfaces, dynamic changes, and drift accumulation,\nleading to misalignment between digital models and the physical world. This\npaper proposes a BIM-aware drift correction method to address these challenges.\nInstead of relying solely on SLAM-based localization, we align ``as-built\"\ndetected planes from the real-world environment with ``as-planned\"\narchitectural planes in BIM. Our method performs robust plane matching and\ncomputes a transformation (TF) between SLAM (S) and BIM (B) origin frames using\noptimization techniques, minimizing drift over time. By incorporating BIM as\nprior structural knowledge, we can achieve improved long-term localization and\nenhanced AR visualization accuracy in noisy construction environments. The\nmethod is evaluated through real-world experiments, showing significant\nreductions in drift-induced errors and optimized alignment consistency. On\naverage, our system achieves a reduction of 52.24% in angular deviations and a\nreduction of 60.8% in the distance error of the matched walls compared to the\ninitial manual alignment by the user.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBIM\u7684\u6f02\u79fb\u6821\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5e73\u9762\u4e0eBIM\u6a21\u578b\u4e2d\u7684\u5e73\u9762\u5bf9\u9f50\uff0c\u4f18\u5316SLAM\u4e0eBIM\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5efa\u7b51\u76d1\u63a7\u4e2dAR\u5e94\u7528\u7684\u6f02\u79fb\u8bef\u5dee\u3002", "motivation": "\u5efa\u7b51\u5de5\u5730\u73af\u5883\u590d\u6742\uff0c\u4f20\u7edf\u8ddf\u8e2a\u65b9\u6cd5\u56e0\u7279\u5f81\u7f3a\u5931\u548c\u52a8\u6001\u53d8\u5316\u5bfc\u81f4AR\u53ef\u89c6\u5316\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u6f02\u79fb\u6821\u6b63\u65b9\u6cd5\u3002", "method": "\u5229\u7528BIM\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u4f18\u5316\u6280\u672f\u8ba1\u7b97SLAM\u4e0eBIM\u5750\u6807\u7cfb\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u5b9e\u73b0\u5e73\u9762\u5339\u914d\u548c\u6f02\u79fb\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u51cf\u5c11\u4e8652.24%\u7684\u89d2\u5ea6\u504f\u5dee\u548c60.8%\u7684\u8ddd\u79bb\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86AR\u5bf9\u9f50\u7cbe\u5ea6\u3002", "conclusion": "\u7ed3\u5408BIM\u7684\u6f02\u79fb\u6821\u6b63\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5efa\u7b51\u5de5\u5730AR\u5e94\u7528\u4e2d\u7684\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u957f\u671f\u5b9a\u4f4d\u548c\u53ef\u89c6\u5316\u51c6\u786e\u6027\u3002"}}
{"id": "2504.17534", "pdf": "https://arxiv.org/pdf/2504.17534", "abs": "https://arxiv.org/abs/2504.17534", "authors": ["Juan Carlos Climent Pardo"], "title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SC"], "comment": null, "summary": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u8868\u793a\u548c\u591a\u7ef4\u5c3a\u5ea6\u5206\u6790\uff08MDS\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5b66\u4e60\u578b\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u578b\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u9053\u8def\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u6db5\u76d6\u591a\u6837\u5316\u9053\u8def\u7ed3\u6784\u548c\u52a8\u6001\u73af\u5883\u7684\u7279\u5f81\u7a7a\u95f4\u3002", "method": "\u5229\u7528\u56fe\u8868\u793a\u9053\u8def\u7f51\u7edc\uff0c\u5e76\u5e94\u7528\u591a\u7ef4\u5c3a\u5ea6\u5206\u6790\uff08MDS\uff09\u6280\u672f\u6784\u5efa\u7279\u5f81\u7a7a\u95f4\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u56fe\u8282\u70b9\u5d4c\u5165\u4ee5\u7b80\u5316\u5b66\u4e60\u548c\u964d\u7ef4\u3002", "result": "\u5206\u6790\u4e86\u6700\u5148\u8fdb\u7684\u56fe\u8868\u793a\u548cMDS\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u4efb\u52a1\u3002"}}
{"id": "2504.17710", "pdf": "https://arxiv.org/pdf/2504.17710", "abs": "https://arxiv.org/abs/2504.17710", "authors": ["Yoeri Poels", "Alessandro Pau", "Christian Donner", "Giulio Romanelli", "Olivier Sauter", "Cristina Venturini", "Vlado Menkovski", "the TCV team", "the WPTE team"], "title": "Plasma State Monitoring and Disruption Characterization using Multimodal VAEs", "categories": ["physics.plasm-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "When a plasma disrupts in a tokamak, significant heat and electromagnetic\nloads are deposited onto the surrounding device components. These forces scale\nwith plasma current and magnetic field strength, making disruptions one of the\nkey challenges for future devices. Unfortunately, disruptions are not fully\nunderstood, with many different underlying causes that are difficult to\nanticipate. Data-driven models have shown success in predicting them, but they\nonly provide limited interpretability. On the other hand, large-scale\nstatistical analyses have been a great asset to understanding disruptive\npatterns. In this paper, we leverage data-driven methods to find an\ninterpretable representation of the plasma state for disruption\ncharacterization. Specifically, we use a latent variable model to represent\ndiagnostic measurements as a low-dimensional, latent representation. We build\nupon the Variational Autoencoder (VAE) framework, and extend it for (1)\ncontinuous projections of plasma trajectories; (2) a multimodal structure to\nseparate operating regimes; and (3) separation with respect to disruptive\nregimes. Subsequently, we can identify continuous indicators for the disruption\nrate and the disruptivity based on statistical properties of measurement data.\nThe proposed method is demonstrated using a dataset of approximately 1600 TCV\ndischarges, selecting for flat-top disruptions or regular terminations. We\nevaluate the method with respect to (1) the identified disruption risk and its\ncorrelation with other plasma properties; (2) the ability to distinguish\ndifferent types of disruptions; and (3) downstream analyses. For the latter, we\nconduct a demonstrative study on identifying parameters connected to\ndisruptions using counterfactual-like analysis. Overall, the method can\nadequately identify distinct operating regimes characterized by varying\nproximity to disruptions in an interpretable manner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u5f81\u6258\u5361\u9a6c\u514b\u4e2d\u7b49\u79bb\u5b50\u4f53\u72b6\u6001\u7684\u53ef\u89e3\u91ca\u8868\u793a\uff0c\u4ee5\u9884\u6d4b\u548c\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u7b49\u79bb\u5b50\u4f53\u7834\u88c2\u3002", "motivation": "\u7b49\u79bb\u5b50\u4f53\u7834\u88c2\u662f\u672a\u6765\u6258\u5361\u9a6c\u514b\u8bbe\u5907\u7684\u5173\u952e\u6311\u6218\u4e4b\u4e00\uff0c\u4f46\u76ee\u524d\u5bf9\u5176\u7406\u89e3\u6709\u9650\uff0c\u4e14\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7684\u89e3\u91ca\u6027\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u4f4e\u7ef4\u8868\u793a\u6765\u8868\u5f81\u7b49\u79bb\u5b50\u4f53\u72b6\u6001\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u9884\u6d4b\u7834\u88c2\u3002", "method": "\u6269\u5c55\u4e86VAE\u6846\u67b6\uff0c\u5305\u62ec\u8fde\u7eed\u6295\u5f71\u7b49\u79bb\u5b50\u4f53\u8f68\u8ff9\u3001\u591a\u6a21\u6001\u7ed3\u6784\u5206\u79bb\u64cd\u4f5c\u533a\u57df\uff0c\u4ee5\u53ca\u9488\u5bf9\u7834\u88c2\u533a\u57df\u7684\u5206\u79bb\u3002\u901a\u8fc7\u7edf\u8ba1\u7279\u6027\u8bc6\u522b\u7834\u88c2\u7387\u548c\u7834\u88c2\u6027\u7684\u8fde\u7eed\u6307\u6807\u3002", "result": "\u5728\u7ea61600\u6b21TCV\u653e\u7535\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u533a\u5206\u4e0d\u540c\u7834\u88c2\u7c7b\u578b\uff0c\u5e76\u8bc6\u522b\u4e0e\u7834\u88c2\u76f8\u5173\u7684\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u8bc6\u522b\u4e0d\u540c\u64cd\u4f5c\u533a\u57df\u53ca\u5176\u4e0e\u7834\u88c2\u7684\u63a5\u8fd1\u7a0b\u5ea6\uff0c\u4e3a\u7834\u88c2\u9884\u6d4b\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.17539", "pdf": "https://arxiv.org/pdf/2504.17539", "abs": "https://arxiv.org/abs/2504.17539", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "title": "Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Blockchain technology enables secure, transparent data management in\ndecentralized systems, supporting applications from cryptocurrencies like\nBitcoin to tokenizing real-world assets like property. Its scalability and\nsustainability hinge on consensus mechanisms balancing security and efficiency.\nProof of Work (PoW), used by Bitcoin, ensures security through energy-intensive\ncomputations but demands significant resources. Proof of Stake (PoS), as in\nEthereum post-Merge, selects validators based on staked cryptocurrency,\noffering energy efficiency but risking centralization from wealth\nconcentration. With AI models straining computational resources, we propose\nProof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,\nworkers perform AI tasks like language processing or image analysis to earn\ncoins, which are staked to secure the network, blending security with practical\nutility. Decentralized nodes--job posters, market coordinators, workers, and\nvalidators --collaborate via smart contracts to manage tasks and rewards.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cProof of Useful Intelligence (PoUI)\u201d\u7684\u6df7\u5408\u5171\u8bc6\u673a\u5236\uff0c\u7ed3\u5408AI\u4efb\u52a1\u4e0e\u533a\u5757\u94fe\u5b89\u5168\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u5171\u8bc6\u673a\u5236\u7684\u8d44\u6e90\u6d6a\u8d39\u6216\u4e2d\u5fc3\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5171\u8bc6\u673a\u5236\u5982PoW\u8d44\u6e90\u6d88\u8017\u5927\uff0cPoS\u53ef\u80fd\u5bfc\u81f4\u4e2d\u5fc3\u5316\uff0c\u800cAI\u6a21\u578b\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u969c\u5b89\u5168\u53c8\u5177\u5b9e\u7528\u6027\u7684\u65b0\u673a\u5236\u3002", "method": "\u63d0\u51faPoUI\u673a\u5236\uff0c\u901a\u8fc7\u8ba9\u5de5\u4f5c\u8005\u5b8c\u6210AI\u4efb\u52a1\uff08\u5982\u8bed\u8a00\u5904\u7406\u6216\u56fe\u50cf\u5206\u6790\uff09\u6765\u83b7\u5f97\u4ee3\u5e01\uff0c\u5e76\u5c06\u4ee3\u5e01\u8d28\u62bc\u4ee5\u4fdd\u969c\u7f51\u7edc\u5b89\u5168\uff0c\u7ed3\u5408\u667a\u80fd\u5408\u7ea6\u534f\u8c03\u8282\u70b9\u534f\u4f5c\u3002", "result": "PoUI\u673a\u5236\u5728\u4fdd\u969c\u533a\u5757\u94fe\u5b89\u5168\u7684\u540c\u65f6\uff0c\u4e3aAI\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4ef7\u503c\uff0c\u5b9e\u73b0\u4e86\u8d44\u6e90\u7684\u9ad8\u6548\u5229\u7528\u3002", "conclusion": "PoUI\u662f\u4e00\u79cd\u521b\u65b0\u7684\u6df7\u5408\u5171\u8bc6\u673a\u5236\uff0c\u517c\u5177\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\uff0c\u6709\u671b\u63a8\u52a8\u533a\u5757\u94fe\u4e0eAI\u7684\u534f\u540c\u53d1\u5c55\u3002"}}
{"id": "2504.17728", "pdf": "https://arxiv.org/pdf/2504.17728", "abs": "https://arxiv.org/abs/2504.17728", "authors": ["Shucheng Gong", "Lingzhe Zhao", "Wenpu Li", "Hong Xie", "Yin Zhang", "Shiyu Zhao", "Peidong Liu"], "title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Source Code: https://github.com/WU-CVGL/CasualHDRSplat", "summary": "Recently, photo-realistic novel view synthesis from multi-view images, such\nas neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered\nwidespread attention due to their superior performance. However, most works\nrely on low dynamic range (LDR) images, which limits the capturing of richer\nscene details. Some prior works have focused on high dynamic range (HDR) scene\nreconstruction, typically require capturing of multi-view sharp images with\ndifferent exposure times at fixed camera positions during exposure times, which\nis time-consuming and challenging in practice. For a more flexible data\nacquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily\nand robustly reconstruct the 3D HDR scene from casually captured videos with\nauto-exposure enabled, even in the presence of severe motion blur and varying\nunknown exposure time. \\textbf{CasualHDRSplat} contains a unified\ndifferentiable physical imaging model which first applies continuous-time\ntrajectory constraint to imaging process so that we can jointly optimize\nexposure time, camera response function (CRF), camera poses, and sharp 3D HDR\nscene. Extensive experiments demonstrate that our approach outperforms existing\nmethods in terms of robustness and rendering quality. Our source code will be\navailable at https://github.com/WU-CVGL/CasualHDRSplat", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCasualHDRSplat\u7684\u5355\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u81ea\u52a8\u66dd\u5149\u7684\u89c6\u9891\u4e2d\u91cd\u5efa3D HDR\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u66dd\u5149\u65f6\u95f4\u548c\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u89c6\u89d2\u56fe\u50cf\u7684HDR\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u56fa\u5b9a\u76f8\u673a\u4f4d\u7f6e\u548c\u4e0d\u540c\u66dd\u5149\u65f6\u95f4\u7684\u56fe\u50cf\uff0c\u64cd\u4f5c\u590d\u6742\u4e14\u8017\u65f6\u3002", "method": "CasualHDRSplat\u91c7\u7528\u7edf\u4e00\u7684\u7269\u7406\u6210\u50cf\u6a21\u578b\uff0c\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4\u8f68\u8ff9\u7ea6\u675f\uff0c\u8054\u5408\u4f18\u5316\u66dd\u5149\u65f6\u95f4\u3001\u76f8\u673a\u54cd\u5e94\u51fd\u6570\u3001\u76f8\u673a\u4f4d\u59ff\u548c3D HDR\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CasualHDRSplat\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u4e14\u9ad8\u6548\u7684HDR\u573a\u666f\u91cd\u5efa\u65b9\u6848\u3002"}}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550", "abs": "https://arxiv.org/abs/2504.17550", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "title": "HalluLens: LLM Hallucination Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5e7b\u89c9\u57fa\u51c6\uff0c\u533a\u5206\u4e86\u5916\u5728\u548c\u5185\u5728\u5e7b\u89c9\uff0c\u5e76\u5f15\u5165\u4e86\u52a8\u6001\u6d4b\u8bd5\u96c6\u751f\u6210\u4ee5\u9632\u6b62\u6570\u636e\u6cc4\u6f0f\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u5185\u5bb9\u4e0e\u7528\u6237\u8f93\u5165\u6216\u8bad\u7ec3\u6570\u636e\u4e0d\u4e00\u81f4\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u4fe1\u4efb\u548c\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u91c7\u7528\u3002", "method": "\u63d0\u51fa\u6e05\u6670\u7684\u5e7b\u89c9\u5206\u7c7b\u6cd5\uff0c\u6784\u5efa\u5305\u542b\u65b0\u5916\u5728\u548c\u73b0\u6709\u5185\u5728\u8bc4\u4f30\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u5e76\u52a8\u6001\u751f\u6210\u6d4b\u8bd5\u96c6\u4ee5\u9632\u6b62\u6570\u636e\u6cc4\u6f0f\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u5e7b\u89c9\u5206\u7c7b\u6846\u67b6\uff0c\u5206\u6790\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u52a8\u6001\u6d4b\u8bd5\u96c6\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u7c7b\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2504.17617", "pdf": "https://arxiv.org/pdf/2504.17617", "abs": "https://arxiv.org/abs/2504.17617", "authors": ["Bruno Casella", "Matthias Jakobs", "Marco Aldinucci", "Sebastian Buschj\u00e4ger"], "title": "Decentralized Time Series Classification with ROCKET Features", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.11; I.2.6"], "comment": "Submitted to Workshop on Federated Learning Advancements 2025, in\n  conjunction with ECML-PKDD, WAFL25", "summary": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md.", "AI": {"tldr": "DROCKS\u662f\u4e00\u4e2a\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff08TSC\uff09\uff0c\u901a\u8fc7ROCKET\u7279\u5f81\u548c\u8282\u70b9\u95f4\u7684\u7ed3\u6784\u5316\u8def\u5f84\u8bad\u7ec3\u5168\u5c40\u6a21\u578b\uff0c\u4f18\u4e8e\u73b0\u6709\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\u7684\u9c81\u68d2\u6027\u548c\u9690\u79c1\u95ee\u9898\uff0c\u907f\u514d\u5355\u70b9\u6545\u969c\u548c\u670d\u52a1\u5668\u5bf9\u5ba2\u6237\u6570\u636e\u7684\u89c2\u5bdf\u3002", "method": "\u5229\u7528ROCKET\u7279\u5f81\uff0c\u901a\u8fc7\u8282\u70b9\u95f4\u7684\u7ed3\u6784\u5316\u8def\u5f84\u8bad\u7ec3\u5168\u5c40\u6a21\u578b\uff0c\u6bcf\u4e2a\u8282\u70b9\u4f18\u5316\u6a21\u578b\u5e76\u9009\u62e9\u6700\u6709\u6548\u7684\u672c\u5730\u6838\u4f20\u9012\u7ed9\u540e\u7ee7\u8282\u70b9\u3002", "result": "\u5728UCR\u5b58\u6863\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDROCKS\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e14\u5bf9\u8282\u70b9\u6545\u969c\u548c\u6076\u610f\u653b\u51fb\u66f4\u5177\u5f39\u6027\u3002", "conclusion": "DROCKS\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u66f4\u9c81\u68d2\u7684\u8054\u90a6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2504.17624", "pdf": "https://arxiv.org/pdf/2504.17624", "abs": "https://arxiv.org/abs/2504.17624", "authors": ["Jigang Fan", "Chunhao Zhu", "Xiaobing Lan", "Haiming Zhuang", "Mingyu Li", "Jian Zhang", "Shaoyong Lu"], "title": "Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled\nreceptor superfamily, plays an important role in modulating dopaminergic\nneuronal activity and eliciting opioid-independent analgesia. Recent studies\nsuggest that promoting \\{beta}-arrestin-biased signaling in NTSR1 may diminish\ndrugs of abuse, such as psychostimulants, thereby offering a potential avenue\nfor treating human addiction-related disorders. In this study, we utilized a\nnovel computational and experimental approach that combined nudged elastic\nband-based molecular dynamics simulations, Markov state models, temporal\ncommunication network analysis, site-directed mutagenesis, and conformational\nbiosensors, to explore the intricate mechanisms underlying NTSR1 activation and\nbiased signaling. Our study reveals a dynamic stepwise transition mechanism and\nactivated transmission network associated with NTSR1 activation. It also yields\nvaluable insights into the complex interplay between the unique polar network,\nnon-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we\nidentified a cryptic allosteric site located in the intracellular region of the\nreceptor that exists in an intermediate state within the activation pathway.\nCollectively, these findings contribute to a more profound understanding of\nNTSR1 activation and biased signaling at the atomic level, thereby providing a\npotential strategy for the development of NTSR1 allosteric modulators in the\nrealm of G protein-coupled receptor biology, biophysics, and medicine.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e86NTSR1\u7684\u52a8\u6001\u9010\u6b65\u6fc0\u6d3b\u673a\u5236\u548c\u4fe1\u53f7\u504f\u8f6c\u7f51\u7edc\uff0c\u53d1\u73b0\u4e86\u4e00\u4e2a\u9690\u79d8\u7684\u53d8\u6784\u4f4d\u70b9\uff0c\u4e3a\u5f00\u53d1NTSR1\u53d8\u6784\u8c03\u8282\u5242\u63d0\u4f9b\u4e86\u65b0\u7b56\u7565\u3002", "motivation": "\u63a2\u7d22NTSR1\u7684\u6fc0\u6d3b\u673a\u5236\u548c\u4fe1\u53f7\u504f\u8f6c\uff0c\u4ee5\u5f00\u53d1\u6cbb\u7597\u6210\u763e\u76f8\u5173\u75be\u75c5\u7684\u6f5c\u5728\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u8ba1\u7b97\u548c\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u5305\u62ec\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u3001\u9a6c\u5c14\u53ef\u592b\u72b6\u6001\u6a21\u578b\u3001\u65f6\u95f4\u901a\u4fe1\u7f51\u7edc\u5206\u6790\u3001\u5b9a\u70b9\u7a81\u53d8\u548c\u6784\u8c61\u751f\u7269\u4f20\u611f\u5668\u3002", "result": "\u63ed\u793a\u4e86NTSR1\u7684\u52a8\u6001\u6fc0\u6d3b\u673a\u5236\u3001\u4fe1\u53f7\u7f51\u7edc\u548c\u9690\u79d8\u53d8\u6784\u4f4d\u70b9\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3NTSR1\u7684\u539f\u5b50\u7ea7\u6fc0\u6d3b\u673a\u5236\u548c\u5f00\u53d1\u53d8\u6784\u8c03\u8282\u5242\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2504.17641", "pdf": "https://arxiv.org/pdf/2504.17641", "abs": "https://arxiv.org/abs/2504.17641", "authors": ["Shengtao Zhang", "Haokai Zhang", "Shiqi Lou", "Zicheng Wang", "Zinan Zeng", "Yilin Wang", "Minnan Luo"], "title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPTCL\u65b9\u6cd5\uff0c\u89e3\u51b3\u52a8\u6001\u8282\u70b9\u5206\u7c7b\u4e2d\u4ec5\u80fd\u83b7\u53d6\u6700\u7ec8\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u548c\u65f6\u95f4\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u52a8\u6001\u8282\u70b9\u5206\u7c7b\u4e2d\uff0c\u83b7\u53d6\u6240\u6709\u65f6\u95f4\u6233\u6807\u7b7e\u6210\u672c\u9ad8\u4e14\u56f0\u96be\uff0c\u800c\u6700\u7ec8\u6807\u7b7e\u66f4\u6613\u83b7\u5f97\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5229\u7528\u6700\u7ec8\u6807\u7b7e\u8fdb\u884c\u52a8\u6001\u5206\u7c7b\u3002", "method": "PTCL\u91c7\u7528\u65f6\u95f4\u89e3\u8026\u67b6\u6784\uff08\u5206\u79bb\u4e3b\u5e72\u7f51\u7edc\u548c\u89e3\u7801\u5668\uff09\u548c\u65f6\u95f4\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff08\u4e3a\u63a5\u8fd1\u6700\u7ec8\u65f6\u95f4\u6233\u7684\u4f2a\u6807\u7b7e\u5206\u914d\u66f4\u9ad8\u6743\u91cd\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePTCL\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u8d21\u732e\u4e86\u65b0\u6570\u636e\u96c6CoOAG\u3002", "conclusion": "PTCL\u548c\u7edf\u4e00\u6846\u67b6FLiD\u4e3a\u6807\u7b7e\u6709\u9650\u7684\u52a8\u6001\u8282\u70b9\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17663", "pdf": "https://arxiv.org/pdf/2504.17663", "abs": "https://arxiv.org/abs/2504.17663", "authors": ["Michelle L. Ding", "Harini Suresh"], "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps.", "AI": {"tldr": "\u672c\u6587\u91c7\u7528\u4ee5\u5e78\u5b58\u8005\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u793e\u4f1a\u6280\u672fAI\u6cbb\u7406\u5728\u9632\u6b62AI\u751f\u6210\u975e\u81ea\u613f\u4eb2\u5bc6\u56fe\u50cf\uff08AIG-NCII\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u4e00\u4e2a\u6076\u610f\u6280\u672f\u751f\u6001\u7cfb\u7edf\uff08MTE\uff09\u53ca\u5176\u6cbb\u7406\u7f3a\u9677\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u5f53\u524dAI\u6cbb\u7406\u65b9\u6cd5\u5728\u9632\u6b62\u6210\u4ebaAIG-NCII\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u6076\u610f\u6280\u672f\u751f\u6001\u7cfb\u7edf\u7684\u76d1\u7ba1\u6f0f\u6d1e\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bc6\u522bMTE\uff08\u5f00\u6e90\u6362\u8138\u6a21\u578b\u548c\u201c\u8131\u8863\u201d\u8f6f\u4ef6\uff09\uff0c\u5e76\u57fa\u4e8eNIST AI 100-4\u62a5\u544a\u5206\u6790\u5f53\u524d\u6cbb\u7406\u5b9e\u8df5\u7684\u7f3a\u9677\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6cbb\u7406\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u76d1\u7ba1MTE\uff0c\u4e14\u5b58\u5728\u9519\u8bef\u7684\u5047\u8bbe\u5bfc\u81f4\u76d1\u7ba1\u6f0f\u6d1e\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u9700\u8981\u6539\u8fdbAI\u6cbb\u7406\u65b9\u6cd5\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u9632\u6b62AIG-NCII\u7684\u751f\u6210\u548c\u4f20\u64ad\u3002"}}
{"id": "2504.17669", "pdf": "https://arxiv.org/pdf/2504.17669", "abs": "https://arxiv.org/abs/2504.17669", "authors": ["Subash Neupane", "Shaswata Mitra", "Sudip Mittal", "Shahram Rahimi"], "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare", "categories": ["cs.MA", "cs.AI", "cs.ET"], "comment": null, "summary": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7b26\u5408HIPAA\u6807\u51c6\u7684Agentic AI\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7b56\u7565\u6267\u884c\u786e\u4fdd\u5408\u89c4\u6027\uff0c\u6574\u5408\u4e86ABAC\u3001\u6df7\u5408PHI\u6e05\u7406\u7ba1\u9053\u548c\u4e0d\u53ef\u53d8\u5ba1\u8ba1\u8ddf\u8e2a\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684Agentic AI\u7cfb\u7edf\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5904\u7406\u654f\u611f\u533b\u7597\u6570\u636e\u65f6\u9700\u8981\u4e25\u683c\u9075\u5b88HIPAA\u7b49\u6cd5\u89c4\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5408\u89c4\u6846\u67b6\u3002", "method": "\u6846\u67b6\u6574\u5408\u4e86\u4e09\u79cd\u6838\u5fc3\u673a\u5236\uff1aABAC\u7528\u4e8e\u7ec6\u7c92\u5ea6PHI\u6cbb\u7406\uff0c\u6df7\u5408PHI\u6e05\u7406\u7ba1\u9053\uff08\u7ed3\u5408\u6b63\u5219\u8868\u8fbe\u5f0f\u548cBERT\u6a21\u578b\uff09\u4ee5\u51cf\u5c11\u6cc4\u6f0f\uff0c\u4ee5\u53ca\u4e0d\u53ef\u53d8\u5ba1\u8ba1\u8ddf\u8e2a\u7528\u4e8e\u5408\u89c4\u9a8c\u8bc1\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u52a8\u6001\u6267\u884c\u5408\u89c4\u7b56\u7565\uff0c\u6709\u6548\u7ba1\u7406PHI\u5e76\u51cf\u5c11\u6570\u636e\u6cc4\u6f0f\u98ce\u9669\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3aAgentic AI\u7cfb\u7edf\u5728\u533b\u7597\u9886\u57df\u7684\u5408\u89c4\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7Split Conformal Prediction\uff08SCP\uff09\u6846\u67b6\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "LVLM\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8f93\u51fa\u5e38\u4f34\u968f\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u6784\u6210\u98ce\u9669\u3002", "method": "\u91c7\u7528\u52a8\u6001\u9608\u503c\u6821\u51c6\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u9a8c\u8bc1\uff0c\u901a\u8fc7\u6570\u636e\u5206\u533a\u8ba1\u7b97\u975e\u4e00\u81f4\u6027\u5206\u6570\uff0c\u6784\u5efa\u5177\u6709\u7edf\u8ba1\u4fdd\u8bc1\u7684\u9884\u6d4b\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSCP\u6846\u67b6\u5b9e\u73b0\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u4e0d\u540c\u6570\u636e\u5206\u533a\u6bd4\u4f8b\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5e7b\u89c9\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u65b9\u6848\uff0c\u586b\u8865\u4e86\u7406\u8bba\u53ef\u9760\u6027\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2504.17675", "pdf": "https://arxiv.org/pdf/2504.17675", "abs": "https://arxiv.org/abs/2504.17675", "authors": ["Caroline Panggabean", "Devaraj Verma C", "Bhagyashree Gogoi", "Ranju Limbu", "Rhythm Sarker"], "title": "Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance", "categories": ["cs.DC", "cs.AI"], "comment": "7 pages, 5 figures, accepted for publication (not yet published)", "summary": "Cloud computing environments demand dynamic and efficient resource management\nto ensure optimal performance, reduced energy consumption, and adherence to\nService Level Agreements (SLAs). This paper presents a Genetic Algorithm\n(GA)-based approach for Virtual Machine (VM) placement and consolidation,\naiming to minimize power usage while maintaining QoS constraints. The proposed\nmethod dynamically adjusts VM allocation based on real-time workload\nvariations, outperforming traditional heuristics such as First Fit Decreasing\n(FFD) and Best Fit Decreasing (BFD). Experimental results show notable\nreductions in energy consumption, VM migrations, SLA violation rates, and\nexecution time. A correlation heatmap further illustrates strong relationships\namong these key performance indicators, confirming the effectiveness of our\napproach in optimizing cloud resource utilization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u865a\u62df\u673a\u653e\u7f6e\u4e0e\u6574\u5408\u65b9\u6cd5\uff0c\u65e8\u5728\u964d\u4f4e\u80fd\u8017\u5e76\u6ee1\u8db3\u670d\u52a1\u8d28\u91cf\u7ea6\u675f\uff0c\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u4e91\u8ba1\u7b97\u73af\u5883\u9700\u8981\u52a8\u6001\u9ad8\u6548\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u4ee5\u786e\u4fdd\u6027\u80fd\u4f18\u5316\u3001\u80fd\u8017\u964d\u4f4e\u548c\u670d\u52a1\u6c34\u5e73\u534f\u8bae\uff08SLA\uff09\u7684\u9075\u5b88\u3002", "method": "\u91c7\u7528\u9057\u4f20\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u865a\u62df\u673a\u5206\u914d\uff0c\u6839\u636e\u5b9e\u65f6\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u80fd\u8017\u3001\u865a\u62df\u673a\u8fc1\u79fb\u6b21\u6570\u3001SLA\u8fdd\u89c4\u7387\u548c\u6267\u884c\u65f6\u95f4\u663e\u8457\u964d\u4f4e\uff0c\u76f8\u5173\u70ed\u56fe\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u4e91\u8d44\u6e90\u5229\u7528\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u5b9e\u4e86\u5176\u9ad8\u6548\u6027\u3002"}}
{"id": "2504.17677", "pdf": "https://arxiv.org/pdf/2504.17677", "abs": "https://arxiv.org/abs/2504.17677", "authors": ["Jarne Thys", "Sebe Vanbrabant", "Davy Vanacken", "Gustavo Rovelo Ruiz"], "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.", "AI": {"tldr": "INSIGHT\u662f\u4e00\u4e2a\u6a21\u5757\u5316AI\u5de5\u5177\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790\u5b66\u751f\u95ee\u9898\u52a8\u6001\u6784\u5efaFAQ\uff0c\u5e2e\u52a9\u6559\u5e08\u63d0\u4f9b\u4e2a\u6027\u5316\u652f\u6301\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86AI\u5728\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8AI\uff08\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5982\u4e2a\u6027\u5316\u6559\u5b66\u4e0e\u5b66\u751f\u9690\u79c1\u95ee\u9898\u3002", "method": "\u63d0\u51faINSIGHT\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u751f\u95ee\u9898\u63d0\u53d6\u5173\u952e\u8bcd\uff0c\u52a8\u6001\u6784\u5efaFAQ\uff0c\u4e3a\u6559\u5e08\u63d0\u4f9b\u4e2a\u6027\u5316\u652f\u6301\u3002", "result": "INSIGHT\u6a21\u5757\u5316\u8bbe\u8ba1\u53ef\u6574\u5408\u5230\u9ad8\u7b49\u6559\u80b2\u8bfe\u7a0b\u4e2d\uff0c\u63d0\u5347\u6559\u5b66\u4e92\u52a8\u6027\u3002", "conclusion": "\u672a\u6765\u53ef\u901a\u8fc7\u5b66\u751f\u6570\u636e\u4f18\u5316\u81ea\u9002\u5e94\u5b66\u4e60\uff0c\u6253\u9020\u66f4\u5177\u4e92\u52a8\u6027\u548c\u5305\u5bb9\u6027\u7684\u5b66\u4e60\u4f53\u9a8c\u3002"}}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685", "abs": "https://arxiv.org/abs/2504.17685", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.", "AI": {"tldr": "\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u96c6\u6210\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\uff08EBI\uff09\u8fbe\u5230\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u6784\u5efa\u9ad8\u6027\u80fdAI\u7cfb\u7edf\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u6709\u6548\u5229\u7528\u6027\u80fd\u8f83\u4f4e\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faEnsemble Bayesian Inference\uff08EBI\uff09\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f30\u8ba1\u7ed3\u5408\u591a\u4e2aSLM\u7684\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eEBI\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u5305\u62ec\u6574\u5408\u6027\u80fd\u8f83\u5dee\u7684\u6a21\u578b\u4e5f\u80fd\u63d0\u5347\u6574\u4f53\u8868\u73b0\u3002", "conclusion": "EBI\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u9ad8\u6027\u80fdAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5c55\u793a\u4e86\u4f4e\u6027\u80fd\u6a21\u578b\u7684\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2504.17703", "pdf": "https://arxiv.org/pdf/2504.17703", "abs": "https://arxiv.org/abs/2504.17703", "authors": ["Edward Collins", "Michel Wang"], "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u5141\u8bb8\u591a\u4e2a\u5ba2\u6237\u7aef\u534f\u4f5c\u8bad\u7ec3\u5171\u4eab\u6a21\u578b\u800c\u65e0\u9700\u96c6\u4e2d\u654f\u611f\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\u3002\u672c\u6587\u7efc\u8ff0\u4e86FL\u7684\u6838\u5fc3\u67b6\u6784\u3001\u6280\u672f\u6311\u6218\u3001\u65b0\u5174\u8d8b\u52bf\u53ca\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u9690\u79c1\u3001\u5b89\u5168\u548c\u5408\u89c4\u6027\u95ee\u9898\uff0c\u63a8\u52a8\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u3001\u91d1\u878d\u548c\u7269\u8054\u7f51\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u4ecb\u7ecd\u4e86FL\u7684\u6838\u5fc3\u67b6\u6784\u3001\u901a\u4fe1\u534f\u8bae\u3001\u751f\u547d\u5468\u671f\uff08\u672c\u5730\u8bad\u7ec3\u3001\u6a21\u578b\u805a\u5408\u3001\u5168\u5c40\u66f4\u65b0\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u5904\u7406\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u3001\u7cfb\u7edf\u5f02\u6784\u6027\u3001\u901a\u4fe1\u5f00\u9500\u548c\u9690\u79c1\u4fdd\u62a4\u7b49\u6280\u672f\u6311\u6218\u3002", "result": "\u603b\u7ed3\u4e86FL\u7684\u5b9e\u9645\u5e94\u7528\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u4e2a\u6027\u5316FL\u3001\u8de8\u8bbe\u5907\u4e0e\u8de8\u7ec4\u7ec7\u8bbe\u7f6e\u7b49\u65b0\u5174\u8d8b\u52bf\u3002", "conclusion": "FL\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u4fe1\u4efb\u95ee\u9898\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u4e0e\u5176\u4ed6\u6280\u672f\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u3001\u91cf\u5b50\u8ba1\u7b97\uff09\u7684\u96c6\u6210\u3002"}}
{"id": "2504.17717", "pdf": "https://arxiv.org/pdf/2504.17717", "abs": "https://arxiv.org/abs/2504.17717", "authors": ["\u00d3scar Escudero-Arnanz", "Antonio G. Marques", "Inmaculada Mora-Jim\u00e9nez", "Joaqu\u00edn \u00c1lvarez-Rodr\u00edguez", "Cristina Soguero-Ruiz"], "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u591a\u836f\u8010\u836f\u6027\uff08MDR\uff09\uff0c\u7ed3\u5408\u60a3\u8005\u76f8\u4f3c\u6027\u7f51\u7edc\u548c\u56fe\u5f62\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u591a\u836f\u8010\u836f\u6027\uff08MDR\uff09\u662f\u5168\u7403\u5065\u5eb7\u7684\u91cd\u8981\u95ee\u9898\uff0c\u5bfc\u81f4\u4f4f\u9662\u65f6\u95f4\u5ef6\u957f\u3001\u533b\u7597\u6210\u672c\u589e\u52a0\u548c\u6b7b\u4ea1\u7387\u4e0a\u5347\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u6d1e\u5bdf\u529b\u3002", "method": "\u5c06\u60a3\u8005\u5efa\u6a21\u4e3a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff08MTS\uff09\uff0c\u4f7f\u7528\u63cf\u8ff0\u6027\u7edf\u8ba1\u3001\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u548c\u65f6\u95f4\u805a\u7c7b\u6838\u91cf\u5316\u60a3\u8005\u76f8\u4f3c\u6027\uff0c\u8f93\u5165\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548c\u652f\u6301\u5411\u91cf\u673a\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u8c31\u805a\u7c7b\u548ct-SNE\u53ef\u89c6\u5316\u9ad8\u98ce\u9669\u7fa4\u7ec4\u3002", "result": "\u5728ICU\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e0a\u9a8c\u8bc1\uff0cAUC\u8fbe81%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc6\u522b\u51fa\u6297\u751f\u7d20\u4f7f\u7528\u3001\u4fb5\u5165\u6027\u64cd\u4f5c\u7b49\u5173\u952e\u98ce\u9669\u56e0\u7d20\uff0c\u5e76\u63ed\u793a\u4e86\u4e34\u5e8a\u76f8\u5173\u7fa4\u7ec4\u3002", "conclusion": "\u60a3\u8005\u76f8\u4f3c\u6027\u8868\u793a\u4e0e\u56fe\u5f62\u5206\u6790\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u51c6\u786e\u7684MDR\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u89c1\u89e3\uff0c\u652f\u6301\u65e9\u671f\u68c0\u6d4b\u548c\u98ce\u9669\u5206\u5c42\uff0c\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u5728\u91cd\u75c7\u76d1\u62a4\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720", "abs": "https://arxiv.org/abs/2504.17720", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "Vil\u00e9m Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "title": "Multilingual Performance Biases of Large Language Models in Education", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u975e\u82f1\u8bed\u6559\u80b2\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4e0e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8bed\u8a00\u8d44\u6e90\u91cf\u76f8\u5173\uff0c\u5efa\u8bae\u90e8\u7f72\u524d\u9a8c\u8bc1\u76ee\u6807\u8bed\u8a00\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dLLMs\u4e3b\u8981\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u7814\u7a76\u5176\u5728\u975e\u82f1\u8bed\u6559\u80b2\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u8bc4\u4f30\u4e86\u6d41\u884cLLMs\u5728\u516d\u79cd\u975e\u82f1\u8bed\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\uff09\u4e2d\u7684\u56db\u9879\u6559\u80b2\u4efb\u52a1\u8868\u73b0\u3002", "result": "\u6a21\u578b\u6027\u80fd\u4e0e\u8bed\u8a00\u8d44\u6e90\u91cf\u76f8\u5173\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u8f83\u5dee\uff0c\u4e14\u4e0e\u82f1\u8bed\u76f8\u6bd4\u6709\u660e\u663e\u4e0b\u964d\u3002", "conclusion": "\u5efa\u8bae\u5728\u6559\u80b2\u4efb\u52a1\u90e8\u7f72\u524d\u9a8c\u8bc1LLMs\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2504.17721", "pdf": "https://arxiv.org/pdf/2504.17721", "abs": "https://arxiv.org/abs/2504.17721", "authors": ["Cheng Shen", "Yuewei Liu"], "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u6821\u51c6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u94a2\u94c1\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u786e\u4fdd\u6d4b\u8bd5\u96c6\u4e0a\u7684\u9884\u671f\u9519\u8bef\u7387\u4e25\u683c\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u98ce\u9669\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u68c0\u6d4b\u6548\u7387\u4f4e\u4e14\u6210\u672c\u9ad8\uff0c\u800c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982Mask R-CNN\uff09\u56e0\u6570\u636e\u6807\u6ce8\u4e0d\u786e\u5b9a\u6027\u548c\u8fc7\u62df\u5408\u95ee\u9898\u5bfc\u81f4\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6ee1\u8db3\u72ec\u7acb\u540c\u5206\u5e03\u6761\u4ef6\u7684\u6821\u51c6\u6570\u636e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u91cf\u5316\u68c0\u6d4b\u9519\u8bef\u7387\uff0c\u5e76\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u98ce\u9669\u6c34\u5e73\u63a8\u5bfc\u7edf\u8ba1\u4e25\u683c\u7684\u9608\u503c\uff0c\u6784\u5efa\u9884\u6d4b\u96c6\uff08\u5982\u7f3a\u9677\u533a\u57df\uff09\u3002", "result": "\u65b9\u6cd5\u80fd\u4e25\u683c\u9650\u5236\u6d4b\u8bd5\u96c6\u7684\u9884\u671f\u9519\u8bef\u7387\uff0c\u5e76\u89c2\u5bdf\u5230\u9884\u6d4b\u96c6\u5927\u5c0f\u4e0e\u98ce\u9669\u6c34\u5e73\u4e4b\u95f4\u7684\u8d1f\u76f8\u5173\u6027\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u7edf\u8ba1\u4e25\u683c\u5ea6\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6821\u51c6-\u6d4b\u8bd5\u5212\u5206\u6bd4\u4f8b\u4e0b\u5747\u80fd\u7a33\u5065\u9ad8\u6548\u5730\u63a7\u5236\u9884\u671f\u9519\u8bef\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u9002\u5e94\u6027\u548c\u64cd\u4f5c\u6709\u6548\u6027\u3002"}}
{"id": "2504.17751", "pdf": "https://arxiv.org/pdf/2504.17751", "abs": "https://arxiv.org/abs/2504.17751", "authors": ["Enqi Zhang"], "title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps.To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u89c6\u4e3a\u4e8c\u5143\u6fc0\u6d3b\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u7528\u4e8e\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u7684\u89c2\u70b9\uff0c\u5e76\u5206\u6790\u4e86\u5f53\u524dSNN\u67b6\u6784\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u6311\u6218\u3002\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u91cd\u7f6e\u64cd\u4f5c\u548c\u4e0d\u89c4\u5219\u671f\u7684\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u56fa\u5b9a\u4e0d\u89c4\u5219\u671fSNN\u67b6\u6784\u3002", "motivation": "\u4f20\u7edfSNN\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u5b58\u5728\u7f3a\u4e4f\u6709\u6548\u8bb0\u5fc6\u673a\u5236\u3001\u751f\u7269\u542f\u53d1\u7ec4\u4ef6\u7406\u8bba\u4e0d\u8db3\u4ee5\u53ca\u65e0\u6cd5\u5e76\u884c\u8bad\u7ec3\u7b49\u95ee\u9898\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u5176\u673a\u5236\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u91cd\u7f6e\u64cd\u4f5c\u548c\u4e0d\u89c4\u5219\u671f\u7684\u673a\u5236\uff0c\u91cd\u65b0\u8bc4\u4f30\u5176\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u56fa\u5b9a\u4e0d\u89c4\u5219\u671fSNN\u67b6\u6784\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u751f\u7269\u673a\u5236\u5e76\u975e\u5b8c\u5168\u5fc5\u8981\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7406\u8bba\u89e3\u91ca\u548c\u56fa\u5b9a\u4e0d\u89c4\u5219\u671fSNN\u67b6\u6784\u3002", "conclusion": "\u56fa\u5b9a\u4e0d\u89c4\u5219\u671fSNN\u67b6\u6784\u4e3a\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6311\u6218\u4e86\u4f20\u7edf\u751f\u7269\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2504.17771", "pdf": "https://arxiv.org/pdf/2504.17771", "abs": "https://arxiv.org/abs/2504.17771", "authors": ["Haochen Wang", "Zhiwei Shi", "Chengxi Zhu", "Yafei Qiao", "Cheng Zhang", "Fan Yang", "Pengjie Ren", "Lan Lu", "Dong Xuan"], "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to ICRA 2025. Project page:\n  https://dreamstarring.github.io/HAMLET/", "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce \\ourmethod, a novel hybrid control system for agile\nbadminton robots. Specifically, we propose a model-based strategy for chassis\nlocomotion which provides a base for arm policy. We introduce a\nphysics-informed ``IL+RL'' training framework for learning-based arm policy. In\nthis train framework, a model-based strategy with privileged information is\nused to guide arm policy training during both IL and RL phases. In addition, we\ntrain the critic model during IL phase to alleviate the performance drop issue\nwhen transitioning from IL to RL. We present results on our self-engineered\nbadminton robot, achieving 94.5% success rate against the serving machine and\n90.7% success rate against human players. Our system can be easily generalized\nto other agile mobile manipulation tasks such as agile catching and table\ntennis. Our project website: https://dreamstarring.github.io/HAMLET/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u63a7\u5236\u7cfb\u7edfHAMLET\uff0c\u7ed3\u5408\u6a21\u578b\u65b9\u6cd5\u548c\u5b66\u4e60\u7b56\u7565\uff08\u6a21\u4eff\u5b66\u4e60+\u5f3a\u5316\u5b66\u4e60\uff09\u7528\u4e8e\u654f\u6377\u7fbd\u6bdb\u7403\u673a\u5668\u4eba\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u7b56\u7565\uff08\u5982\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u654f\u6377\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u6a21\u578b\u65b9\u6cd5\u7684\u7ed3\u5408\u4ee5\u964d\u4f4e\u8bad\u7ec3\u590d\u6742\u5ea6\u5e76\u786e\u4fdd\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u5e95\u76d8\u8fd0\u52a8\u7b56\u7565\u4f5c\u4e3a\u57fa\u7840\uff0c\u5e76\u8bbe\u8ba1\u7269\u7406\u542f\u53d1\u7684\u201cIL+RL\u201d\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u7279\u6743\u4fe1\u606f\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u5728IL\u9636\u6bb5\u8bad\u7ec3\u8bc4\u8bba\u5bb6\u6a21\u578b\u4ee5\u51cf\u5c11\u6027\u80fd\u4e0b\u964d\u3002", "result": "\u5728\u81ea\u7814\u7fbd\u6bdb\u7403\u673a\u5668\u4eba\u4e0a\u5b9e\u73b094.5%\u5bf9\u53d1\u7403\u673a\u548c90.7%\u5bf9\u4eba\u7c7b\u73a9\u5bb6\u7684\u6210\u529f\u7387\u3002", "conclusion": "HAMLET\u7cfb\u7edf\u5728\u654f\u6377\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u654f\u6377\u64cd\u4f5c\u4efb\u52a1\u3002"}}
