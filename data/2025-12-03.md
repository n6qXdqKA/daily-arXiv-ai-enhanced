[[toc]]

## cs.CV

### [1] [Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale](https://arxiv.org/abs/2512.02055)
*Mirela G. Tulbure,Julio Caineta,Mark Broich,Mollie D. Gaines,Philippe Rufin,Leon-Friedrich Thomas,Hamed Alemohammad,Jan Hemmerling,Patrick Hostert*

Main category: cs.CV

TL;DR: 本研究通过微调TerraMind地理空间基础模型，使用全球85个洪水事件的FloodsNet多模态数据集，评估了不同配置在洪水范围制图上的性能，发现基础模型解冻配置在精度和计算成本间取得最佳平衡。

- Motivation: 洪水是最具破坏性的天气相关灾害之一，2024年极端洪水事件影响五大洲。虽然地球观测卫星提供了关键的洪水监测覆盖，但操作精度严重依赖标注数据集和模型泛化能力。尽管ESA-IBM的TerraMind等地理空间基础模型通过大规模自监督预训练提高了泛化性，但其在全球多样化洪水事件上的性能仍不清楚。
- Method: 使用包含全球85个洪水事件的FloodsNet多模态数据集（包含Sentinel-1 SAR和Sentinel-2光学协同定位影像）对TerraMind进行微调。测试了四种配置（基础vs大型模型；冻结vs解冻骨干网络），并与TerraMind Sen1Floods11示例以及在FloodsNet和Sen1Floods11上训练的U-Net进行比较。
- Result: 基础模型解冻配置在准确率、精确度和召回率方面提供了最佳平衡，且计算成本远低于大型模型。大型解冻模型实现了最高召回率。在FloodsNet上训练的模型在召回率上优于Sen1Floods11训练的示例，同时保持相似的总体准确率。U-Net比所有GFM配置获得更高召回率，但准确率和精确度略低。
- Conclusion: 整合多模态光学和SAR数据并微调地理空间基础模型可以增强近实时洪水制图能力。本研究首次在全球范围内评估GFM用于洪水分割，既突显了其在气候适应和灾害恢复方面的潜力，也指出了当前局限性。


### [2] [Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework](https://arxiv.org/abs/2512.02152)
*Haojin Deng,Yimin Yang*

Main category: cs.CV

TL;DR: 提出一种上下文增强的对比损失函数，通过两个收敛目标同时提高学习效果并解决信息失真问题

- Motivation: 传统对比学习中的对比损失函数可能导致信息失真，模型过度依赖相同标签样本的信息，而忽视来自同一原始图像的正样本对，特别是在大规模数据集中
- Method: 提出上下文增强对比损失函数，包含两个组件：1) 对标签对比敏感的组件，区分相同和不同类别的特征；2) 拉近来自同一源图像的增强样本，同时推远所有其他样本
- Result: 在8个大规模基准数据集上评估，相比16种最先进的对比学习方法，在泛化性能和学习收敛速度方面均有提升。在BiasedMNIST数据集上比原始对比损失函数提升22.9%
- Conclusion: 该方法能有效解决对比学习中的信息失真问题，提高训练效率和公平性，特别在处理系统性失真任务方面表现突出


### [3] [FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges](https://arxiv.org/abs/2512.02161)
*Kevin David Hayes,Micah Goldblum,Vikash Sehwag,Gowthami Somepalli,Ashwinee Panda,Tom Goldstein*

Main category: cs.CV

TL;DR: 本文提出一个联合评估文本到图像(T2I)模型和视觉语言模型(VLM)的结构化方法，通过测试VLM能否识别T2I模型生成的图像中的27种特定失败模式，并创建了包含5个T2I模型生成图像和3个VLM标注的数据集。

- Motivation: 当前T2I模型虽然能生成视觉上令人印象深刻的图像，但经常无法准确捕捉用户提示中的特定属性（如正确数量的对象和指定颜色）。同时，VLM的基准测试未能跟上VLM用于标注的复杂场景的需求。需要分层评估框架来比较不同图像生成模型的提示遵循能力。
- Method: 提出结构化方法联合评估T2I模型和VLM：1）测试VLM能否识别T2I模型基于挑战性提示生成的图像中的27种特定失败模式；2）创建数据集：包含5个T2I模型（Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large）生成的图像，以及3个VLM（Molmo, InternVL3, Pixtral）通过LLM（Llama3）标注的对应注释。
- Result: 通过分析精心策划的提示集上的失败模式，揭示了属性保真度和对象表示方面的系统性错误。发现当前指标不足以捕捉这些细微错误，强调了针对性基准测试对于推进生成模型可靠性和可解释性的重要性。
- Conclusion: 本文提出的联合评估框架和数据集揭示了T2I模型在属性保真度和对象表示方面的系统性错误，同时表明当前VLM基准测试存在不足。这强调了需要更精细的评估方法来提高生成模型的可靠性和可解释性。


### [4] [Mapping of Lesion Images to Somatic Mutations](https://arxiv.org/abs/2512.02162)
*Rahul Mehta*

Main category: cs.CV

TL;DR: LLOST模型使用双变分自编码器，通过共享潜在空间将医学图像点云表示与体细胞突变计数关联，实现从影像预测癌症基因突变

- Motivation: 癌症治疗依赖于早期诊断，医学影像和基因信息在诊断中都很重要。作者希望建立从医学图像预测体细胞突变谱的模型，以加速诊断和治疗决策
- Method: 1) 引入病灶图像的点云表示以获得模态不变性；2) 提出LLOST模型：双变分自编码器通过共享潜在空间耦合图像点云和突变计数特征；3) 使用条件归一化流先验学习三个潜在空间以适应各领域分布多样性
- Result: 在TCIA医学影像和TCGA Pan Cancer体细胞突变数据上验证，模型能预测特定突变计数和突变发生，并发现影像与突变域之间的共享模式反映癌症类型
- Conclusion: 模型展示了从医学影像预测基因突变的潜力，未来可改进模型并扩展到其他遗传领域


### [5] [SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting](https://arxiv.org/abs/2512.02172)
*Pranav Asthana,Alex Hanson,Allen Tu,Tom Goldstein,Matthias Zwicker,Amitabh Varshney*

Main category: cs.CV

TL;DR: SplatSuRe：一种选择性超分辨率方法，仅对缺乏高频监督的欠采样区域应用超分辨率内容，从而在3D高斯泼溅中实现更锐利、更一致的高分辨率渲染。

- Motivation: 现有方法对所有图像统一应用超分辨率，会导致多视图不一致和模糊渲染。关键洞察是：近距离低分辨率视图可能包含远处视图也捕获区域的高频信息，可以利用相机姿态相对于场景几何的信息来决定在何处添加超分辨率内容。
- Method: 提出SplatSuRe方法，选择性应用超分辨率内容，仅对缺乏高频监督的欠采样区域进行增强。通过相机姿态与场景几何的相对关系来识别哪些区域需要超分辨率处理。
- Result: 在Tanks & Temples、Deep Blending和Mip-NeRF 360数据集上，该方法在保真度和感知质量方面均超越基线方法。在需要更高细节的局部前景区域，改进最为显著。
- Conclusion: 选择性超分辨率策略比统一应用超分辨率更有效，能够利用多视图信息来指导超分辨率内容的应用，实现更锐利、更一致的3D高斯泼溅渲染。


### [6] [RobustSurg: Tackling domain generalisation for out-of-distribution surgical scene segmentation](https://arxiv.org/abs/2512.02188)
*Mansoor Ali,Maksim Richards,Gilberto Ochoa-Ruiz,Sharib Ali*

Main category: cs.CV

TL;DR: 提出RobustSurg方法，通过利用手术场景的风格和内容信息来提升跨中心和跨模态的泛化能力，在未见数据集上相比基线有显著改进。

- Motivation: 现有深度学习手术场景分割方法在单中心和单模态数据上表现良好，但难以泛化到未见分布（其他中心）和未见模态。自然场景中的域泛化方法不能直接应用于手术场景，因为手术场景视觉线索有限且场景极其多样。
- Method: 通过实例归一化和特征协方差映射技术来利用手术场景的风格和内容信息，减少外观变化的影响。在ResNet骨干网络中引入恢复模块，保留与任务相关的有用特征表示。同时提供了新的多类多中心数据集。
- Result: 在未见中心HeiCholSeg数据集上，相比基线DeepLabv3+提升近23%，相比SOTA方法提升10-32%（mIoU）。在EndoUDA息肉数据集的目标集上，相比基线提升近22%，相比最新SOTA方法提升近11%。
- Conclusion: RobustSurg方法通过利用风格和内容信息以及特征恢复机制，显著提升了手术场景分割的跨域泛化能力，为手术场景分割的泛化问题提供了有效解决方案。


### [7] [Multifractal Recalibration of Neural Networks for Medical Imaging Segmentation](https://arxiv.org/abs/2512.02198)
*Miguel L. Martins,Miguel T. Coimbra,Francesco Renna*

Main category: cs.CV

TL;DR: 该论文提出了两种归纳先验方法：单分形和多分形重新校准，通过利用指数概率质量与多分形谱之间的关系形成编码器嵌入的统计描述，在卷积网络中实现为通道注意力函数，并在医学图像分割任务上取得显著性能提升。

- Motivation: 现有端到端多分形方法依赖大量池化或强特征空间降采样，这限制了语义分割等任务的应用。多分形分析在捕捉病理规律方面已证明有效，但在现代深度学习中的应用仍然有限。
- Method: 引入两种归纳先验：单分形和多分形重新校准。这些方法利用指数概率质量与多分形谱之间的关系形成编码器嵌入的统计描述，在卷积网络中实现为通道注意力函数。使用U-Net框架，将多分形重新校准与基线和其他使用高阶统计的通道注意力机制进行比较。
- Result: 多分形重新校准在使用其他通道注意力机制的基线上取得了显著提升。在三个公开医学影像数据集（ISIC18皮肤镜、Kvasir-SEG内窥镜、BUSI超声）上验证了方法的有效性。分析还发现，在U-Net架构中，由于跳跃连接的存在，激励响应不会随编码器深度增加而变得过度专门化，其有效性可能与实例变异性的全局统计有关。
- Conclusion: 多分形重新校准为医学图像分割提供了一种有效的通道注意力机制，能够捕捉病理规律并取得性能提升。该方法揭示了注意力层在U-Net架构中的特殊行为，为理解深度学习中的多分形分析提供了新见解。


### [8] [Towards Unified Video Quality Assessment](https://arxiv.org/abs/2512.02224)
*Chen Feng,Tianhao Peng,Fan Zhang,David Bull*

Main category: cs.CV

TL;DR: 提出Unified-VQA框架，通过诊断混合专家模型解决通用视频质量评估问题，支持多种视频格式和失真类型，提供可解释的质量评分和伪影向量。

- Motivation: 现有视频质量评估方法存在三个主要问题：1) 单模型只能预测单一质量分数，缺乏诊断性和可解释性；2) 大多是特定格式的专用指标，而非真正的通用解决方案；3) 从不同感知域学习妥协表示，无法提供针对性分析。
- Method: 提出Unified-VQA框架，将通用VQA重构为诊断混合专家问题。采用多个"感知专家"分别处理不同感知域，设计多代理专家训练策略，使用排名启发损失优化每个专家。集成诊断多任务头，生成全局质量分数和可解释的多维伪影向量，采用弱监督学习策略优化。
- Result: 在17个包含HD、UHD、HDR和HFR格式的多样化流媒体伪影数据库中，Unified-VQA在静态模型参数下（无需重新训练或微调），相比18个基准方法，在通用VQA和诊断伪影检测任务上均表现出持续且优越的性能。
- Conclusion: Unified-VQA代表了向实用、可操作、可解释的视频质量评估迈出的重要一步，通过统一框架解决了现有方法的局限性，提供了诊断性和通用性。


### [9] [See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2512.02231)
*Le Thien Phuc Nguyen,Zhuoran Yu,Samuel Low Yu Hang,Subin An,Jeongik Lee,Yohan Ban,SeungEun Chung,Thanh-Huy Nguyen,JuWan Maeng,Soochahn Lee,Yong Jae Lee*

Main category: cs.CV

TL;DR: AV-SpeakerBench是一个专注于说话人中心视听推理的基准测试，包含3212个多选题，评估多模态大语言模型在真实视频中识别谁在说话、说了什么以及何时说话的能力。

- Motivation: 现有视频基准测试很少评估对人类语音的细粒度推理，许多任务要么可以通过视觉解决，要么只粗略评估语音，无法深入了解模型是否能对齐说话人、说话内容和时间信息。
- Method: 创建了AV-SpeakerBench基准测试，具有三个特点：1) 以说话人为核心推理单元而非场景；2) 融合基础的问题设计，将视听依赖嵌入问题语义；3) 专家标注确保时间精度和跨模态有效性。
- Result: Gemini系列模型表现最佳，Gemini 2.5 Pro达到最好结果。开源模型中Qwen3-Omni-30B接近Gemini 2.0 Flash，但仍远落后于Gemini 2.5 Pro，主要原因是视听融合能力较弱而非视觉感知。
- Conclusion: AV-SpeakerBench为推进未来多模态系统的细粒度视听推理建立了严格的基础，有助于评估模型在真实视频中理解说话人、内容和时间关系的能力。


### [10] [Exploring the Potentials of Spiking Neural Networks for Image Deraining](https://arxiv.org/abs/2512.02258)
*Shuang Chen,Tomas Krajnik,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: 该论文提出了一种用于图像去雨任务的视觉LIF神经元(VLIF)和脉冲神经网络框架，在显著降低能耗的同时取得了优于现有SNN方法的性能。

- Motivation: 脉冲神经网络(SNN)作为生物合理且节能的框架，在低层视觉任务中尚未得到充分探索。传统脉冲神经元缺乏空间上下文理解能力，且存在频域饱和限制，这阻碍了SNN在图像去雨等任务中的应用。
- Method: 1) 提出视觉LIF(VLIF)神经元，克服传统脉冲神经元缺乏空间上下文理解的障碍；2) 利用VLIF设计脉冲分解增强模块和轻量级脉冲多尺度单元，实现分层多尺度表示学习；3) 针对传统脉冲神经元的频域饱和问题提出解决方案。
- Result: 在五个基准去雨数据集上的实验表明，该方法显著优于最先进的SNN去雨方法，且仅消耗其13%的能量。在性能和能效方面都取得了突破性进展。
- Conclusion: 该研究为在高性能、节能的低层视觉任务中部署SNN奠定了坚实基础，展示了SNN在图像去雨等任务中的巨大潜力。


### [11] [Spatiotemporal Pyramid Flow Matching for Climate Emulation](https://arxiv.org/abs/2512.02268)
*Jeremy Andrew Irvin,Jiaqi Han,Zikui Wang,Abdulaziz Alharbi,Yufei Zhao,Nomin-Erdene Bayarsaikhan,Daniele Visioni,Andrew Y. Ng,Duncan Watson-Parris*

Main category: cs.CV

TL;DR: 提出SPF（时空金字塔流）方法，通过分层建模时空尺度实现高效并行气候模拟，优于现有基线模型

- Motivation: 现有生成模型依赖天气尺度自回归，对长期气候模拟速度慢且无法在非平稳强迫下稳定展开，需要更高效的气候模拟方法
- Method: SPF将生成轨迹划分为时空金字塔，逐步提高空间分辨率减少计算，每个阶段耦合特定时间尺度，可直接采样任意时间层级，并基于物理强迫条件进行模拟
- Result: 在ClimateBench上，SPF在年和月尺度上优于强流匹配基线和预训练模型，采样速度快；构建的ClimateSuite包含33,000+模拟年，SPF在跨气候模型场景中表现出良好泛化能力
- Conclusion: SPF和ClimateSuite为跨时间尺度和现实未来场景的准确、高效、概率性气候模拟提供了基础，代码和数据已公开


### [12] [Progressive Image Restoration via Text-Conditioned Video Generation](https://arxiv.org/abs/2512.02273)
*Peng Kang,Xijun Wang,Yu Yuan*

Main category: cs.CV

TL;DR: 将CogVideo文本到视频模型重新用于渐进式视觉修复任务，通过微调生成修复轨迹而非自然视频运动，在超分辨率、去模糊和低光增强等任务上取得良好效果

- Motivation: 现有的文本到视频模型在时间生成方面表现出色，但在图像修复方面的潜力尚未充分探索。研究人员希望利用这些模型的时间生成能力来实现渐进式的视觉修复
- Method: 1. 微调CogVideo模型，使其生成修复轨迹而非自然视频运动；2. 构建合成数据集（超分辨率、去模糊、低光增强），每个样本包含从退化到干净帧的渐进过渡；3. 比较两种提示策略：统一文本提示和基于LLaVA多模态LLM生成、ChatGPT优化的场景特定提示
- Result: 微调后的模型能够将时间进展与修复质量关联，生成的序列在PSNR、SSIM和LPIPS等感知指标上逐帧提升。模型有效恢复了空间细节和光照一致性，同时保持时间连贯性。在ReLoBlur数据集上无需额外训练即可泛化到真实场景，展示了强大的零样本鲁棒性和通过时间修复的可解释性
- Conclusion: 文本到视频模型可以成功重新用于渐进式视觉修复任务，通过将时间维度与修复质量关联，模型能够生成逐步改善的修复序列，并在真实场景中表现出良好的泛化能力和可解释性


### [13] [Enhancing Cross Domain SAR Oil Spill Segmentation via Morphological Region Perturbation and Synthetic Label-to-SAR Generation](https://arxiv.org/abs/2512.02290)
*Andre Juarez,Luis Salsavilca,Frida Coaquira,Celso Gonzales*

Main category: cs.CV

TL;DR: 提出MORP-Synth兩階段合成增強框架，解決SAR油汙分割模型從地中海轉移到秘魯海岸時泛化能力下降的問題，提升模型在少標註數據區域的表現。

- Motivation: 現有SAR油汙分割模型在不同區域間泛化能力差，特別是秘魯海岸標註數據稀缺，導致模型從地中海轉移到秘魯時性能大幅下降。
- Method: MORP-Synth兩階段框架：第一階段使用形態學區域擾動（曲率引導標籤空間方法）生成油汙和相似區域的幾何變體；第二階段使用條件生成INADE模型從編輯後的遮罩渲染SAR樣紋理。
- Result: 模型從地中海預訓練轉移到秘魯時，mIoU從67.8%降至51.8%；使用MORP-Synth後提升達+6 mIoU，少數類IoU顯著提升（油汙+10.8，相似區域+14.6）。
- Conclusion: MORP-Synth合成增強框架能有效提升SAR油汙分割模型的跨區域泛化能力，特別是在標註數據稀缺的區域，為實際監測應用提供實用解決方案。


### [14] [Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision](https://arxiv.org/abs/2512.02339)
*Chenshuang Zhang,Kang Zhang,Joon Son Chung,In So Kweon,Junmo Kim,Chengzhi Mao*

Main category: cs.CV

TL;DR: 利用预训练视频扩散模型中的运动表示进行自监督跟踪，无需任务特定训练，显著提升视觉相似物体的区分能力

- Motivation: 现有自监督跟踪器在视觉线索模糊时表现不佳，需要大量标注数据，限制了其可扩展性和泛化能力。研究发现预训练视频扩散模型天生学习到适合跟踪的运动表示
- Method: 利用视频扩散模型的去噪过程：早期高噪声阶段分离出运动信息，后期阶段细化外观。基于此发现开发自监督跟踪器，无需任务特定训练
- Result: 在现有基准和新设计的视觉相似物体跟踪测试中，比近期自监督方法提升高达6个百分点。可视化证实扩散模型提取的运动表示能鲁棒跟踪相同物体，应对视角变化和形变
- Conclusion: 预训练视频扩散模型蕴含的运动表示能有效解决视觉相似物体跟踪难题，为自监督跟踪提供了新方向，无需大量标注数据


### [15] [TALO: Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction](https://arxiv.org/abs/2512.02341)
*Fengyi Zhang,Tianjun Zhang,Kasra Khosoussi,Zheng Zhang,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: 提出了TALO框架，基于薄板样条实现高自由度、长期对齐，解决3D视觉基础模型在时序预测中的一致性问题。

- Motivation: 现有3D视觉基础模型在单帧预测上表现良好，但在在线场景（如驾驶）中，时序预测难以保持一致性。现有方法通过求解全局变换对齐连续预测，但在假设有效性、局部对齐范围和噪声几何鲁棒性方面存在根本局限。
- Method: 提出基于薄板样条的高自由度长期对齐框架，利用全局传播的控制点校正空间变化的不一致性；采用点无关的子图配准设计，对噪声几何预测具有固有鲁棒性；完全即插即用，兼容多种3D基础模型和相机配置。
- Result: 在多个数据集、骨干模型和相机设置上的实验表明，该方法始终产生更一致的几何结构和更低的轨迹误差，突显其鲁棒性和通用性。
- Conclusion: TALO框架有效解决了3D基础模型在时序预测中的一致性问题，通过高自由度对齐和鲁棒配准设计，在各种场景下都能实现更好的几何一致性和轨迹精度。


### [16] [A multi-weight self-matching visual explanation for cnns on sar images](https://arxiv.org/abs/2512.02344)
*Siyuan Sun,Yongping Zhang,Hongcheng Zeng,Yamin Wang,Wei Yang,Wanting Yang,Jie Chen*

Main category: cs.CV

TL;DR: 提出MS-CAM方法，通过多权重自匹配类激活映射提升SAR图像中CNN的可解释性，能更准确突出网络关注区域并验证了在弱监督目标定位中的应用可行性。

- Motivation: 卷积神经网络在SAR任务中表现出色，但其内部机制的复杂性和不透明性阻碍了高可靠性要求的满足，限制了在SAR中的应用。提升CNN的可解释性对SAR领域的发展和应用部署至关重要。
- Method: 提出多权重自匹配类激活映射（MS-CAM）方法，将SAR图像与CNN提取的特征图及对应梯度进行匹配，结合通道权重和元素权重来可视化模型在SAR图像中的决策依据。
- Result: 在自建的SAR目标分类数据集上的实验表明，MS-CAM能更准确地突出网络的关注区域，捕获详细的目标特征信息，从而增强网络可解释性。同时验证了MS-CAM在弱监督目标定位中的可行性，并深入分析了像素阈值等影响定位精度的关键因素。
- Conclusion: MS-CAM方法有效提升了SAR图像中CNN的可解释性，为网络决策提供了可视化依据，并在弱监督目标定位中展现出应用潜力，为未来工作提供了重要参考。


### [17] [Understanding and Harnessing Sparsity in Unified Multimodal Models](https://arxiv.org/abs/2512.02351)
*Shwai He,Chaorui Deng,Ang Li,Shen Yan*

Main category: cs.CV

TL;DR: 本文系统分析了统一多模态模型的推理效率问题，发现理解组件具有显著压缩性而生成组件对压缩敏感，提出基于混合专家（MoE）的稀疏激活方法，在仅激活约一半参数的情况下实现与完整模型相当的性能。

- Motivation: 统一多模态模型虽然整合了理解和生成能力，但带来了推理效率问题——特定任务或样本可能不需要完整模型的知识或容量。目前对这些效率问题在不同组件中的表现缺乏系统理解。
- Method: 首先使用训练无关的剪枝作为探测方法，系统分析统一多模态模型组件（深度剪枝和宽度缩减）。然后提出混合专家（MoE）适应方法，将生成模块划分为多个专家并启用稀疏激活以恢复生成质量。通过专家冻结调优验证稀疏激活有效性，并展示完全可训练的适应能带来额外增益。
- Result: 研究发现理解组件在理解和生成任务中都表现出显著压缩性，在生成任务中更为明显；而生成组件对压缩高度敏感，即使中等压缩比也会导致性能急剧下降。提出的MoE适应方法使BAGEL模型在仅激活约一半参数的情况下，实现了与完整模型相当的性能。
- Conclusion: 统一多模态模型存在组件间效率不平衡问题，理解组件可压缩而生成组件敏感。通过MoE稀疏激活方法可以有效解决这一问题，在保持性能的同时显著提升推理效率，为高效多模态模型设计提供了新思路。


### [18] [WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting](https://arxiv.org/abs/2512.02359)
*Bin Li,Daijie Chen,Qi Zhang*

Main category: cs.CV

TL;DR: 提出WSCF-MVCC方法，仅使用人群计数作为监督，无需相机标定和密度图标注，通过自监督排序损失和多尺度先验提升感知能力，利用语义信息改进视图匹配，在弱监督设置下优于现有方法。

- Motivation: 现有多视角人群计数方法需要昂贵的相机标定和人群标注，即使免标定方法仍需要图像级人群标注。需要开发更实用的弱监督方法，减少标注成本。
- Method: 提出弱监督免标定多视角人群计数方法(WSCF-MVCC)：1) 仅使用人群计数而非密度图作为单视角计数模块监督；2) 利用多尺度先验的自监督排序损失增强模型感知能力；3) 利用语义信息实现更准确的视图匹配。
- Result: 在三个广泛使用的多视角计数数据集上，该方法在弱监督设置下优于现有最先进方法，表明比标定方法更适合实际部署。
- Conclusion: 提出的WSCF-MVCC方法通过弱监督和自监督技术，显著减少了多视角人群计数对昂贵标注的依赖，在保持高性能的同时提高了实际部署的可行性。


### [19] [VACoT: Rethinking Visual Data Augmentation with VLMs](https://arxiv.org/abs/2512.02361)
*Zhengzhuo Xu,Chong Sun,SiNan Du,Chen Li,Jing Lyu,Chun Yuan*

Main category: cs.CV

TL;DR: VACoT框架在视觉语言模型推理时动态调用图像增强，通过后处理变换提升对抗性OCR等场景的鲁棒性，无需重新训练模型。

- Motivation: 视觉语言模型主要依赖大规模真实数据或合成多样性，在基础感知任务上表现不佳，而重新训练成本高昂且收益有限，需要一种无需重新训练就能提升鲁棒性的方法。
- Method: 提出VACoT框架，在模型推理时动态调用图像增强（如去噪），采用结构化视觉增强集合，通过高效的智能体强化学习和条件奖励机制来平衡增强效果与响应简洁性。
- Result: 在13个感知基准测试中表现优异，特别是在OCR相关的对抗场景中显著提升鲁棒性，并引入AdvOCR基准来展示后处理视觉增强的泛化优势。
- Conclusion: VACoT通过推理时动态图像增强有效提升视觉语言模型的鲁棒性，特别是在对抗性场景中，避免了昂贵的重新训练成本，为视觉语言模型感知任务提供了新思路。


### [20] [Tackling Tuberculosis: A Comparative Dive into Machine Learning for Tuberculosis Detection](https://arxiv.org/abs/2512.02364)
*Daanish Hindustani,Sanober Hindustani,Preston Nguyen*

Main category: cs.CV

TL;DR: 本研究比较了ResNet-50和SqueezeNet两种深度学习模型在胸部X光图像中诊断结核病的性能，发现SqueezeNet表现更优，准确率达89%，F1分数87%。

- Motivation: 结核病诊断在资源有限地区面临挑战，传统方法如痰涂片镜检和培养效率低下，需要探索深度学习等先进技术来改善诊断效率和准确性。
- Method: 使用Kaggle的4200张胸部X光数据集，比较预训练的ResNet-50模型和通用的SqueezeNet模型。预处理包括数据分割、增强和调整大小，使用准确率、精确率、召回率和混淆矩阵等指标评估性能。
- Result: SqueezeNet表现显著优于ResNet-50：损失32% vs 54%，准确率89% vs 73%，精确率98% vs 88%，召回率80% vs 52%，F1分数87% vs 65%。
- Conclusion: 机器学习在结核病检测中具有巨大潜力，SqueezeNet等轻量级模型适合集成到移动设备中，在资源匮乏地区具有应用前景，但仍需开发更快、更小、更准确的模型来支持全球结核病防治工作。


### [21] [Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention](https://arxiv.org/abs/2512.02368)
*Wenyi Xiong,Jian Chen*

Main category: cs.CV

TL;DR: 提出一种基于混合专家机制和选择性注意力的无地图轨迹预测算法，在时域、空域和频域进行多模态轨迹预测

- Motivation: 现有方法在复杂交互场景中难以从冗余数据中高效提取有价值的场景信息，导致计算效率低和预测精度下降，特别是在处理复杂智能体交互时
- Method: 1) 使用时域混合专家机制自适应选择关键频率分量并集成多尺度时域特征；2) 提出选择性注意力模块过滤时域序列和空间交互中的冗余信息；3) 设计多模态解码器，在补丁级和点级损失监督下生成合理轨迹
- Result: 在Nuscences数据集上的实验证明了算法的优越性，验证了其在处理复杂交互场景中的有效性
- Conclusion: 提出的无地图轨迹预测算法通过时域、空域和频域的多域处理，结合混合专家机制和选择性注意力，能够有效处理复杂交互场景，提高预测精度和计算效率


### [22] [SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains](https://arxiv.org/abs/2512.02369)
*Qingmei Li,Yang Zhang,Peifeng Zhang,Haohuan Fu,Juepeng Zheng*

Main category: cs.CV

TL;DR: SAGE框架通过合成视觉提示来提升冻结模型在隐私约束下的语义分割领域泛化能力，无需修改模型权重。

- Motivation: 在现实场景中，由于隐私和安全限制，无法访问模型参数和架构细节，传统的微调方法受限，需要不修改模型权重的输入级策略来提升泛化能力。
- Method: SAGE框架学习合成视觉提示，通过风格转移构建源域多样风格表示，然后根据每个输入的视觉上下文自适应融合这些风格线索，形成动态提示来协调图像外观。
- Result: 在五个基准数据集上的实验表明，SAGE在隐私约束下达到或优于最先进方法，在所有设置中都优于完全微调基线。
- Conclusion: SAGE通过闭环设计有效弥合了冻结模型不变性与未见域多样性之间的差距，为隐私约束下的领域泛化提供了有效解决方案。


### [23] [On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning](https://arxiv.org/abs/2512.02375)
*Liyuan Lou,Wanyun Li,Wentian Gan,Yifei Yu,Tengfei Wang,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: 提出On-the-fly Feedback SfM框架，实现无人机实时摄影测量，通过探索-利用策略在飞行中迭代优化3D重建质量，提供在线反馈指导图像采集。

- Motivation: 传统离线无人机摄影测量无法满足灾害响应等时间敏感应用需求，现有实时方法缺乏对重建质量的在线评估和采集指导反馈。
- Method: 基于SfM on-the-fly，集成三个模块：1)在线增量粗网格生成扩展稀疏点云；2)在线网格质量评估与可操作指标；3)预测性路径规划进行轨迹优化。
- Result: 方法实现近实时原位重建与评估，提供可操作反馈显著减少覆盖缺口和重复飞行成本，代码已开源。
- Conclusion: 通过数据采集、处理、3D重建评估和在线反馈的集成，该方法为从传统被动工作模式向智能自适应探索工作流转变提供了替代方案。


### [24] [From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking](https://arxiv.org/abs/2512.02392)
*Yuqing Shao,Yuchen Yang,Rui Yu,Weilong Li,Xu Guo,Huaicheng Yan,Wei Wang,Xiao Sun*

Main category: cs.CV

TL;DR: FDTA提出一个显式特征精炼框架，通过空间、时间和身份三个适配器增强目标嵌入的判别性，解决端到端多目标跟踪中关联准确率低的问题。

- Motivation: 当前端到端多目标跟踪方法虽然检测性能强，但关联准确率相对较低。研究发现共享DETR架构生成的目标嵌入具有过高的目标间相似性，因为它只强调单帧内的类别级区分，而跟踪需要跨帧的实例级区分以及时空连续性。
- Method: 提出FDTA框架，包含三个互补的特征精炼模块：空间适配器(SA)集成深度感知线索增强空间连续性；时间适配器(TA)聚合历史信息捕捉时间依赖；身份适配器(IA)利用质量感知对比学习提升实例级可分离性。
- Result: 在多个具有挑战性的MOT基准测试（包括DanceTrack、SportsMOT和BFT）上实现了最先进的性能，证明了所提出的判别性嵌入增强策略的有效性。
- Conclusion: FDTA通过显式特征精炼框架有效增强了目标嵌入的判别性，解决了端到端多目标跟踪中关联准确率不足的问题，为提升跟踪性能提供了有效解决方案。


### [25] [Reproducing and Extending RaDelft 4D Radar with Camera-Assisted Labels](https://arxiv.org/abs/2512.02394)
*Kejia Hu,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 提出一个无需人工标注的相机引导雷达标注框架，通过将雷达点云投影到相机语义分割结果并应用空间聚类，为4D雷达生成准确标签，解决了雷达语义分割数据稀缺问题。

- Motivation: 4D雷达在恶劣条件下具有鲁棒感知潜力，但雷达语义分割进展受限于开源数据集和标签的稀缺。现有RaDelft数据集仅提供LiDAR标注且无公开代码生成雷达标签，限制了可重复性和下游研究。
- Method: 开发相机引导的雷达标注流程：将雷达点云投影到基于相机的语义分割结果，应用空间聚类生成雷达标签，无需依赖人工标注。还研究了不同雾度水平对雷达标注性能的影响。
- Result: 成功复现了RaDelft组的数值结果，证明相机引导标注流程能生成准确的雷达点云标签，显著提高雷达标签准确性。建立了可重复框架供研究社区训练和评估标注的4D雷达数据。
- Conclusion: 提出的相机引导雷达标注框架解决了雷达语义分割的数据标注瓶颈，为研究社区提供了可重复的标注工具，并量化分析了雾度对雷达标注性能的影响，推动了4D雷达感知技术的发展。


### [26] [Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch](https://arxiv.org/abs/2512.02395)
*Yifan Zhang,Liang Hu,Haofeng Sun,Peiyu Wang,Yichen Wei,Shukang Yin,Jiangbo Pei,Wei Shen,Peng Xia,Yi Peng,Tianyidan Xie,Eric Li,Yang Liu,Xuchen Song,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork-R1V4是一个300亿参数的多模态智能体模型，通过监督微调实现图像操作、网络搜索和规划的统一，在多项基准测试中超越Gemini 2.5 Flash，无需强化学习。

- Motivation: 现有方法将图像操作和网络搜索视为分离能力，过度依赖昂贵的强化学习，且缺乏基于真实工具执行轨迹的规划。需要统一的多模态智能体系统。
- Method: 开发30B参数的多模态智能体模型，统一多模态规划、主动图像操作（"用图像思考"）、深度多模态搜索和交错推理。仅通过监督微调在少于30,000个高质量、规划-执行一致的轨迹上进行训练，并通过逐步一致性过滤验证。
- Result: 在MMSearch上得分为66.1，在FVQA上得分为67.2，在所有11个指标上超越Gemini 2.5 Flash。展现出推理时的涌现长视野推理能力，能够成功协调超过10个工具调用来解决复杂多步骤任务。
- Conclusion: 精心策划的监督学习足以实现复杂的多模态智能体智能，无需依赖强化学习。Skywork-R1V4展示了统一多模态能力的重要性。


### [27] [Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation](https://arxiv.org/abs/2512.02400)
*Wentao Xiang,Haokang Zhang,Tianhang Yang,Zedong Chu,Ruihang Chu,Shichao Xie,Yujian Yuan,Jian Sun,Zhining Gu,Junjie Wang,Xiaolong Wu,Mu Xu,Yujiu Yang*

Main category: cs.CV

TL;DR: Nav-R²框架通过结构化思维链推理和相似性感知记忆，在开放词汇对象导航任务中实现了对未见物体的高效定位，避免了过拟合并保持实时推理能力。

- Motivation: 现有开放词汇对象导航方法存在决策过程不透明和对未见物体定位成功率低的问题，需要更清晰的关系建模和推理机制。
- Method: 提出Nav-R²框架，通过结构化思维链推理建模目标-环境关系和环境-动作规划关系，结合相似性感知记忆从时间和语义角度压缩融合历史观测特征。
- Result: 在定位未见物体方面达到最先进性能，避免了过拟合到已见物体类别，同时保持2Hz的实时推理速度。
- Conclusion: Nav-R²通过显式关系建模和高效记忆机制，为开放词汇对象导航提供了透明、高效的解决方案，显著提升了未见物体的定位能力。


### [28] [WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate](https://arxiv.org/abs/2512.02405)
*Anoop Cherian,River Doyle,Eyal Ben-Dov,Suhas Lohit,Kuan-Chuan Peng*

Main category: cs.CV

TL;DR: WISE是一个多智能体辩论框架，通过将智能体分为求解者和反思者，结合加权聚合和反馈机制，在多模态推理任务上比现有方法提升2-7%准确率。

- Motivation: 当前多智能体辩论主要应用于纯语言任务，而在多模态问题上的效果尚未充分探索。不同大语言模型在多模态能力上各有优势，需要一种能整合这些互补优势的辩论框架。
- Method: 提出WISE框架：1) 将智能体分为求解者（生成解决方案）和反思者（验证正确性、分配权重、提供自然语言反馈）；2) 使用改进的Dawid-Skene算法进行后处理，整合两阶段辩论模型并考虑响应方差和反馈权重。
- Result: 在SMART-840、VisualPuzzles、EvoChart-QA和新的SMART-840++数据集上评估，WISE在各种多模态任务和LLM配置中，比最先进的多智能体辩论设置和聚合方法持续提升2-7%的准确率。
- Conclusion: WISE框架成功地将多智能体辩论扩展到多模态推理问题，通过异构专家和加权聚合机制有效整合不同模型的互补优势，在多模态任务上取得了显著性能提升。


### [29] [MitUNet: Enhancing Floor Plan Recognition using a Hybrid Mix-Transformer and U-Net Architecture](https://arxiv.org/abs/2512.02413)
*Dmitriy Parashchuk,Alexey Kapshitskiy,Yuriy Karyakin*

Main category: cs.CV

TL;DR: MitUNet：用于室内平面图墙分割的混合神经网络，结合Mix-Transformer编码器和U-Net解码器，通过Tversky损失优化边界精度，在3D重建中生成几何精确的掩码。

- Motivation: 现有方法在标准指标上表现良好，但难以检测薄壁结构且边界不规则，缺乏后续矢量化所需的几何精度，影响3D重建质量。
- Method: 提出MitUNet混合架构：使用分层Mix-Transformer编码器捕获全局上下文，U-Net解码器增强scSE注意力块进行精确边界恢复，采用Tversky损失函数优化策略平衡精度和召回率。
- Result: 在公开CubiCasa5k数据集和专有区域数据集上验证，方法能生成结构正确且边界精度高的掩码，优于标准单任务模型。
- Conclusion: MitUNet为自动化3D重建管道提供了鲁棒的数据准备工具，解决了薄壁检测和边界精度问题，提升了室内空间重建质量。


### [30] [Generalizing Vision-Language Models with Dedicated Prompt Guidance](https://arxiv.org/abs/2512.02421)
*Xinyao Li,Yinjie Min,Hongbo Chen,Zhekai Du,Fengling Li,Jingjing Li*

Main category: cs.CV

TL;DR: 本文提出GuiDG框架，通过训练多个参数高效的专家模型来改善视觉语言模型微调中的领域泛化能力，在标准基准测试中优于现有方法。

- Motivation: 当前视觉语言模型微调面临领域特异性和领域泛化能力之间的权衡，传统方法在整个数据集上微调单一模型可能损害对未见领域的泛化能力。
- Method: 提出两阶段GuiDG框架：1) 使用提示调优获得源领域专家模型；2) 引入跨模态注意力模块，通过自适应专家集成指导视觉编码器微调。
- Result: 在标准领域泛化基准和新建的ImageNet-DG数据集上，GuiDG优于最先进的微调方法，同时保持高效性。
- Conclusion: 通过理论分析和实验验证，训练多个参数高效的专家模型比微调单一通用模型能获得更好的领域泛化能力，GuiDG框架为此提供了有效实现。


### [31] [GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.02423)
*Haolong Yan,Yeqing Shen,Xin Huang,Jia Wang,Kaijun Tan,Zhixuan Liang,Hongxin Li,Zheng Ge,Osamu Yoshie,Si Li,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CV

TL;DR: 本文介绍了GUI Exploration Lab，一个用于GUI智能体导航研究的模拟环境引擎，通过监督微调、单轮强化学习和多轮强化学习的组合方法，显著提升了GUI导航性能。

- Motivation: 随着大型视觉语言模型的发展，GUI智能体任务从单屏任务转向复杂屏幕导航挑战。然而，真实GUI环境（如PC软件和移动应用）通常复杂且专有，难以获取全面的环境信息用于智能体训练和评估，这限制了系统研究和基准测试。
- Method: 1. 引入GUI Exploration Lab模拟环境引擎，支持灵活定义和组合屏幕、图标和导航图，提供完整环境信息用于训练和评估。
2. 采用三阶段训练方法：监督微调用于记忆基础知识，单轮强化学习增强对未见场景的泛化能力，多轮强化学习通过交互试错发展探索策略。
- Result: 在静态和交互式基准测试中验证了方法的有效性，证明监督微调为后续训练奠定基础，单轮强化学习提升泛化能力，多轮强化学习通过探索策略进一步提高屏幕导航性能，这些发现在真实场景中也能有效泛化。
- Conclusion: 强化学习方法在GUI导航中具有优势，为构建更强大、更可泛化的GUI智能体提供了实用指导。GUI Exploration Lab解决了环境信息获取的难题，促进了GUI智能体导航能力的系统研究和基准测试。


### [32] [WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning](https://arxiv.org/abs/2512.02425)
*Woongyeong Yeo,Kangsan Kim,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: WorldMM是一个多模态记忆代理，通过构建文本和视觉互补记忆来解决长视频理解问题，在五个长视频问答基准上平均性能提升8.4%

- Motivation: 现有视频大语言模型在处理小时或天级别的长视频时面临挑战：上下文容量有限，抽象过程中丢失关键视觉细节。现有基于记忆增强的方法主要依赖文本摘要，无法利用视觉证据进行复杂场景推理，且固定时间尺度的检索限制了捕捉可变时长事件的能力。
- Method: 引入WorldMM多模态记忆代理，构建三种互补记忆：1) 情节记忆：跨多个时间尺度索引事实事件；2) 语义记忆：持续更新高层概念知识；3) 视觉记忆：保留场景的详细信息。推理时，自适应检索代理迭代选择最相关记忆源，基于查询利用多个时间粒度，直到收集到足够信息。
- Result: WorldMM在五个长视频问答基准上显著优于现有基线，平均性能比先前最先进方法提升8.4%，证明了其在长视频推理上的有效性。
- Conclusion: WorldMM通过多模态记忆架构成功解决了长视频理解中的关键挑战，特别是通过文本和视觉互补记忆以及自适应多时间尺度检索，为长视频推理提供了有效解决方案。


### [33] [LightHCG: a Lightweight yet powerful HSIC Disentanglement based Causal Glaucoma Detection Model framework](https://arxiv.org/abs/2512.02437)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: 提出LightHCG，一种基于因果表征的轻量级青光眼检测模型，使用卷积VAE和因果表征学习，参数减少93-99%，性能优于现有视觉模型。

- Motivation: 青光眼作为不可逆的致盲疾病，传统诊断依赖视野检查、视盘观察和眼压测量。现有AI检测方法在可靠性、参数效率、虚假相关性以及临床应用（如干预分析）方面仍有改进空间。
- Method: 提出LightHCG模型：基于卷积VAE的轻量级潜在表征模型，采用HSIC（希尔伯特-施密特独立性准则）进行潜在空间解耦，结合图自编码器进行无监督因果表征学习，考虑青光眼相关物理因素间的真实因果关系。
- Result: 模型在青光眼分类任务中表现优异，相比InceptionV3、MobileNetV2、VGG16等先进视觉模型，参数量减少93-99%，同时提升了AI驱动干预分析的可能性。
- Conclusion: LightHCG通过因果表征学习实现了高效、轻量的青光眼检测，不仅分类性能优越，还为临床干预分析和模拟提供了更好的基础，解决了现有AI方法在可靠性和应用范围方面的局限性。


### [34] [Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources](https://arxiv.org/abs/2512.02438)
*Phuc Pham,Nhu Pham,Ngoc Quoc Ly*

Main category: cs.CV

TL;DR: 提出一种结合动量自蒸馏和梯度累积的医疗视觉语言模型训练方法，在单GPU上实现高效训练，在零样本分类和少样本适应任务中达到SOTA性能。

- Motivation: 医疗领域获取详细标注困难，需要鲁棒的视觉语言模型。对比学习需要大批量训练，计算成本高，限制了资源有限机构的使用。同时，医疗数据有限，需要从数据和模型中提取更多知识。
- Method: 结合动量方法和蒸馏技术：1）利用动量自蒸馏增强多模态学习；2）将动量机制与梯度累积结合，在不增加资源消耗的情况下扩大有效批量大小。
- Result: 在零样本分类中达到SOTA竞争性能，少样本适应获得超过90%的AUC-ROC，检索任务提升2-3%。单GPU实现高效训练，保持合理训练时间。
- Conclusion: 该方法通过减少资源需求同时提升性能，推进了高效多模态学习，特别适合医疗领域资源受限的环境。


### [35] [Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation](https://arxiv.org/abs/2512.02441)
*Junghwan Park,Woojin Cho,Junhyuk Heo,Darongsae Kwon,Kookjin Lee*

Main category: cs.CV

TL;DR: BOLT：通过提取正交任务谱基，在低秩子空间中进行参数高效迁移学习，无需元训练即可实现新任务快速适应

- Motivation: 现有元学习方法需要大量元训练任务且计算成本高，而大量任务特定预训练模型如何高效迁移到新任务尚未充分探索。需要一种无需额外元训练、能重用现有微调模型的方法。
- Method: BOLT框架：离线阶段从多个任务向量提取主导奇异方向并正交化形成可重用基；在线阶段冻结这些基，仅训练每层的小型对角系数，实现秩可控的参数高效更新。
- Result: BOLT提供了强大的免训练初始化（通过池化源任务系数）和轻量级重缩放，在参数高效微调方面表现优于常见PEFT基线和代表性元学习初始化方法。
- Conclusion: 将适应约束在任务信息正交子空间为未见任务迁移提供了有效替代方案，无需元训练即可实现高效参数迁移学习。


### [36] [Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors](https://arxiv.org/abs/2512.02447)
*Fan Luo,Zeyu Gao,Xinhao Luo,Kai Zhao,Yanfeng Lu*

Main category: cs.CV

TL;DR: 提出TDE增强SNN的时序建模能力，包含生成多样化刺激的SE模块和基于时序依赖指导生成的AGM模块，并通过SDA降低注意力能耗，在目标检测任务中取得SOTA性能。

- Motivation: 现有SNN通常直接复制输入或按固定间隔聚合为帧，导致神经元在不同时间步接收几乎相同的刺激，严重限制了模型表达能力，特别是在复杂任务如目标检测中。
- Method: 提出TDE（时序动态增强器），包含两个模块：1）SE（脉冲编码器）生成跨时间步的多样化输入刺激；2）AGM（注意力门控模块）基于时序间依赖关系指导SE生成。此外，提出SDA（脉冲驱动注意力）消除AGM引入的高能耗乘法操作。
- Result: TDE可无缝集成到现有SNN检测器中，在静态PASCAL VOC数据集上达到57.7% mAP50-95，在神经形态EvDET200K数据集上达到47.6%。SDA能耗仅为传统注意力模块的0.240倍。
- Conclusion: TDE通过增强SNN的时序信息建模能力，在保持低能耗的同时显著提升了目标检测性能，为SNN在复杂视觉任务中的应用提供了有效解决方案。


### [37] [nuScenes Revisited: Progress and Challenges in Autonomous Driving](https://arxiv.org/abs/2512.02448)
*Whye Kit Fong,Venice Erin Liong,Kok Seang Tan,Holger Caesar*

Main category: cs.CV

TL;DR: 本文回顾了nuScenes数据集在自动驾驶领域的重要地位，详细介绍了其创建过程、技术细节、对后续数据集的影响，以及基于该数据集的各种任务和方法发展。

- Motivation: 自动驾驶和高级驾驶辅助系统依赖深度学习，而深度学习需要大量标注数据。数据集与硬件、算法一样是自动驾驶发展的基础。nuScenes作为最广泛使用的自动驾驶数据集之一，体现了AV发展的关键趋势，但许多技术细节在学术出版物中尚未完全公开。
- Method: 本文深入探讨nuScenes数据集的创建过程，包括其扩展版本nuImages和Panoptic nuScenes，揭示了许多未公开的技术细节。同时追踪nuScenes对后续数据集的影响以及它定义的行业标准，并综述基于该数据集的各种任务和方法。
- Result: nuScenes是首个包含雷达数据、涵盖两大洲城市驾驶场景、使用完全自动驾驶车辆在公共道路上收集的数据集。它推动了多模态传感器融合、标准化基准测试，并支持感知、定位与建图、预测和规划等多种任务。该数据集对后续数据集产生了深远影响，定义了社区至今仍在使用的众多标准。
- Conclusion: nuScenes数据集在自动驾驶领域具有里程碑意义，不仅提供了高质量的多模态数据，还推动了行业标准的发展。本文全面回顾了nuScenes的创建、影响和应用，为自动驾驶文献提供了以nuScenes为重点的全面综述。


### [38] [HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild](https://arxiv.org/abs/2512.02450)
*Valentin Bieri,Marie-Julie Rakotosaona,Keisuke Tateno,Francis Engelmann,Leonidas Guibas*

Main category: cs.CV

TL;DR: 提出了HouseLayout3D真实世界基准数据集和MultiFloor3D训练免费基线方法，用于解决多楼层建筑布局估计问题

- Motivation: 现有3D布局估计模型主要在合成数据集上训练，只能处理单房间或单楼层环境，无法处理大型多楼层建筑，需要将场景分割为单个楼层处理，这移除了全局空间上下文，而全局上下文对于理解连接多个楼层的结构（如楼梯）至关重要
- Method: 提出了MultiFloor3D方法，这是一个简单的训练免费基线，利用最近的场景理解方法，无需训练即可处理多楼层布局估计
- Result: MultiFloor3D在提出的HouseLayout3D基准和现有数据集上都优于现有的3D布局估计模型，显示了该方法在多楼层布局估计方面的有效性
- Conclusion: 需要进一步研究全建筑规模的布局估计，HouseLayout3D基准和MultiFloor3D基线为这一方向提供了重要基础，数据和代码已公开


### [39] [ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation](https://arxiv.org/abs/2512.02453)
*Kerui Chen,Jianrong Zhang,Ming Li,Zhonglong Zheng,Hehe Fan*

Main category: cs.CV

TL;DR: 提出ClusterStyle框架，通过聚类原型建模风格多样性，解决现有风格化运动生成模型难以捕捉同一风格内多样性的问题

- Motivation: 现有风格化运动生成模型虽然能理解特定风格信息并将其插入内容运动，但难以捕捉同一风格类别内的多样性变化（intra-style diversity）
- Method: 1) 使用原型集代替非结构化嵌入来建模多样风格模式；2) 考虑全局级（同一类别风格运动间）和局部级（运动序列时间动态内）两种风格多样性；3) 构建全局和局部两个结构化风格嵌入空间，通过与非学习原型锚点对齐优化；4) 使用风格调制适配器（SMA）增强预训练文本到运动生成模型
- Result: 在风格化运动生成和运动风格迁移任务上，该方法超越了现有最先进模型
- Conclusion: ClusterStyle框架通过聚类原型有效建模风格多样性，显著提升了风格化运动生成的质量和多样性


### [40] [See, Think, Learn: A Self-Taught Multimodal Reasoner](https://arxiv.org/abs/2512.02456)
*Sourabh Sharma,Sonam Gupta,Sadbhawna*

Main category: cs.CV

TL;DR: STL是一个自训练框架，通过结构化推理模板（先提取视觉属性再推理）和负样本理性增强，提升视觉语言模型的多模态推理能力。

- Motivation: 现有方法增强视觉语言模型推理能力时，要么依赖人工标注的高质量思维链数据（成本高），要么使用忽略感知的自训练方法。需要一种成本效益高且能同时提升感知和推理能力的方法。
- Method: 提出See-Think-Learn自训练框架：1）使用结构化推理模板，要求模型先"看"（提取视觉属性文本），再"思考"（基于属性推理）；2）通过自训练循环让模型从自己生成的结构化理性中学习；3）加入负样本理性（解释为什么某些答案错误）来增强区分能力。
- Result: 在多个领域实验中，STL始终优于仅基于答案训练或仅使用自生成推理的基线方法。定性分析确认其生成的理性质量很高。
- Conclusion: STL提供了一种成本效益高的解决方案，能有效增强视觉语言模型的多模态推理能力，同时提升感知和推理两方面性能。


### [41] [Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation](https://arxiv.org/abs/2512.02457)
*Jianzong Wu,Hao Lian,Dachao Hao,Ye Tian,Qingyu Shi,Biaolong Chen,Hao Jiang*

Main category: cs.CV

TL;DR: 音频-视频联合去噪训练能提升纯视频生成质量，即使只关注视频模态

- Motivation: 探究音频-视频联合去噪训练是否能改善视频生成质量，即使我们只关心视频质量本身
- Method: 提出参数高效的AVFullDiT架构，利用预训练的文本到视频和文本到音频模块进行联合去噪训练，并与纯视频模型在相同设置下对比
- Result: 首次系统证明音频-视频联合去噪能带来超越同步性的好处，在具有大幅运动和物体接触的挑战性子集上观察到一致改进
- Conclusion: 音频预测作为特权信号，鼓励模型内化视觉事件与其声学后果之间的因果关系，从而正则化视频动态，跨模态协同训练是开发更强、更物理基础的世界模型的有前景方法


### [42] [Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration](https://arxiv.org/abs/2512.02458)
*Zhongyi Cai,Yi Du,Chen Wang,Yu Kong*

Main category: cs.CV

TL;DR: SEER-Bench是首个序列化具身探索与推理基准，包含EQA和EMN任务，提出3DSPMR方法将几何信息融入MLLM以提升序列任务性能

- Motivation: 现有具身AI研究主要关注单任务场景，但实际应用中智能体常面临序列任务，需要复用先前探索积累的空间知识来支持后续推理和探索，这一挑战尚未得到充分研究
- Method: 提出3DSPMR方法，利用已探索区域的关系、视觉和几何线索来增强多模态大语言模型，首次将几何信息显式融入MLLM的空间理解和推理中
- Result: 在SEER-Bench基准上的大量实验表明，3DSPMR在序列化EQA和EMN任务上取得了显著的性能提升
- Conclusion: 该工作填补了序列化具身AI任务的空白，提出的3DSPMR方法通过整合几何信息有效提升了MLLM在序列任务中的空间理解和推理能力


### [43] [TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution](https://arxiv.org/abs/2512.02469)
*Fengli Ran,Xiao Pu,Bo Liu,Xiuli Bi,Bin Xiao*

Main category: cs.CV

TL;DR: TGDD提出了一种基于训练轨迹引导的数据集蒸馏方法，通过动态对齐特征分布来提升合成数据的表达能力，在保持高效的同时显著提升下游任务性能。

- Motivation: 现有基于分布匹配的数据集蒸馏方法忽视了训练过程中特征表示的演化，限制了合成数据的表达能力，从而影响了下游任务性能。
- Method: TGDD将分布匹配重新定义为沿模型训练轨迹的动态对齐过程，在每个训练阶段对齐合成数据集与原始数据集的特征分布，并引入分布约束正则化来减少类别重叠。
- Result: 在十个数据集上的实验表明，TGDD达到了最先进的性能，特别是在高分辨率基准上获得了5.0%的准确率提升，且无需额外的优化开销。
- Conclusion: TGDD通过动态对齐训练轨迹中的特征分布，有效提升了数据集蒸馏的性能，在保持高效的同时实现了语义多样性和代表性的平衡。


### [44] [WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling](https://arxiv.org/abs/2512.02473)
*Yuta Oshima,Yusuke Iwasawa,Masahiro Suzuki,Yutaka Matsuo,Hiroki Furuta*

Main category: cs.CV

TL;DR: WorldPack是一个具有高效压缩内存的视频世界模型，通过轨迹打包和内存检索技术，在较短上下文长度下显著提升了长期生成的空间一致性、保真度和质量。

- Motivation: 视频世界模型在长期时空一致性建模方面存在挑战，现有最先进模型由于长上下文输入的计算成本过高而无法解决这一问题。
- Method: 提出WorldPack模型，包含压缩内存机制：1) 轨迹打包实现高上下文效率；2) 内存检索保持生成一致性并辅助需要空间推理的长期生成。
- Result: 在专门评估长期一致性的Minecraft基准测试LoopNav上，WorldPack显著优于现有最先进模型。
- Conclusion: WorldPack通过高效的压缩内存设计，解决了视频世界模型长期一致性和高质量生成的难题，在较短上下文长度下实现了优越性能。


### [45] [G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline](https://arxiv.org/abs/2512.02482)
*Vishwesh Nath,Javier G. Tejero,Ruilong Li,Filippo Filicori,Mahdi Azizian,Sean D. Huver*

Main category: cs.CV

TL;DR: G-SHARP是一个商业兼容的实时手术场景重建框架，专为需要快速准确3D建模的可变形组织微创手术设计，基于Apache-2.0许可的GSplat构建，提供实时手术可视化。

- Motivation: 现有基于高斯泼溅的实时内窥镜重建方法通常依赖非商业衍生工具，限制了实际部署能力。需要开发商业兼容、可部署的手术场景重建框架。
- Method: 基于GSplat（Apache-2.0）可微分高斯光栅化器构建原生手术管道，实现原理性变形建模、鲁棒遮挡处理和高保真重建。提供Holoscan SDK应用，可在NVIDIA IGX Orin和Thor边缘硬件上部署。
- Result: 在EndoNeRF pulling基准测试中达到最先进的重建质量，具有适合术中使用的强大速度-精度权衡。能够在实际手术室环境中实现实时手术可视化。
- Conclusion: G-SHARP是首个基于商业兼容高斯泼溅框架构建的手术重建系统，克服了现有方法的部署限制，为实时手术导航和可视化提供了实用解决方案。


### [46] [UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making](https://arxiv.org/abs/2512.02485)
*Qianhan Feng,Zhongzhen Huang,Yakun Zhu,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: UCAgents是一个分层多智能体框架，通过结构化证据审计实现单向收敛，解决医学VLM中推理脱离问题，在提高诊断准确性的同时大幅降低计算成本。

- Motivation: 当前视觉语言模型在医学诊断中存在推理脱离问题，即语言流畅的解释与可验证的图像证据脱节，损害临床信任。现有的多智能体框架虽然能减轻单一模型偏见，但开放式讨论会放大文本噪声和计算成本，且未能将推理锚定在视觉证据上。
- Method: 提出UCAgents分层多智能体框架，受临床工作流程启发，禁止立场改变并将智能体交互限制在有针对性的证据验证中。引入一轮询问讨论以发现视觉-文本不对齐的潜在风险，通过信息论形式化双重噪声瓶颈，共同约束视觉模糊性和文本噪声。
- Result: 在四个医学VQA基准测试中，UCAgents在PathVQA上达到71.3%的准确率（比最先进技术提高6.0%），同时降低87.7%的token成本。评估结果证实UCAgents在发现更多视觉证据和避免混淆性文本干扰之间取得了平衡。
- Conclusion: UCAgents展示了诊断可靠性和计算效率的平衡，这对于实际临床部署至关重要。该框架通过结构化证据审计和单向收敛机制，有效解决了医学VLM中的推理脱离问题。


### [47] [Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding](https://arxiv.org/abs/2512.02487)
*Yerim Jeon,Miso Lee,WonJun Moon,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出3D-SLIM，一种针对3D场景语言理解的注意力掩码策略，通过几何自适应掩码和指令感知掩码替代传统因果掩码，解决顺序偏见和受限注意力问题，无需架构修改即可提升性能。

- Motivation: 现有3D场景语言理解方法使用标准语言模型的因果注意力掩码，导致两个问题：1）对无序3D对象引入顺序偏见；2）对象与指令间注意力受限，阻碍任务特定推理。
- Method: 提出3D-SLIM（3D空间语言指令掩码），包含两个核心组件：1）几何自适应掩码，基于空间密度而非令牌顺序约束注意力；2）指令感知掩码，使对象令牌能直接访问指令上下文。
- Result: 在多个3D场景语言任务基准测试中，3D-SLIM带来显著性能提升，验证了其有效性，并突显了解码器设计在3D多模态推理中的关键作用。
- Conclusion: 3D-SLIM通过简单的掩码策略解决了3D场景理解中的注意力限制问题，无需架构修改或额外参数，为3D多模态推理提供了有效的解决方案。


### [48] [YingVideo-MV: Music-Driven Multi-Stage Video Generation](https://arxiv.org/abs/2512.02492)
*Jiahui Chen,Weida Wang,Runhua Shi,Huan Yang,Chaofan Ding,Zihao Chen*

Main category: cs.CV

TL;DR: YingVideo-MV是首个用于音乐驱动长视频生成的级联框架，通过音频语义分析、可解释镜头规划、时序感知扩散Transformer和长序列一致性建模，实现从音频信号自动合成高质量音乐表演视频。

- Motivation: 当前扩散模型在音频驱动虚拟人视频生成方面取得了进展，但音乐表演视频的生成（特别是包含摄像机运动）仍未被充分探索。现有长视频生成方法缺乏显式的摄像机运动控制，且需要更好的连续性处理。
- Method: 1. 构建大规模Music-in-the-Wild数据集；2. 提出级联框架：音频语义分析、可解释镜头规划模块(MV-Director)、时序感知扩散Transformer架构；3. 引入摄像机适配器模块，将摄像机姿态嵌入潜在噪声；4. 提出时间感知动态窗口范围策略，基于音频嵌入自适应调整去噪范围。
- Result: 综合基准测试表明，YingVideo-MV在生成连贯、富有表现力的音乐视频方面表现优异，实现了精确的音乐-动作-摄像机同步。项目页面展示了更多视频结果。
- Conclusion: YingVideo-MV是首个音乐驱动长视频生成框架，通过创新的级联架构和摄像机控制模块，成功解决了音乐表演视频生成中的摄像机运动控制和长序列一致性问题。


### [49] [Attention-guided reference point shifting for Gaussian-mixture-based partial point set registration](https://arxiv.org/abs/2512.02496)
*Mizuki Kikkawa,Tatsuya Yatagawa,Yutaka Ohtake,Hiromasa Suzuki*

Main category: cs.CV

TL;DR: 该研究揭示了基于深度学习和高斯混合模型的部分到部分点集配准方法中特征向量在平移和旋转下的不变性问题，提出了注意力参考点偏移层来解决这一问题，显著提升了DeepGMR和UGMMReg的性能。

- Motivation: 研究动机是探索基于深度学习和高斯混合模型的部分到部分点集配准方法中特征向量在输入点集平移和旋转下的不变性问题，特别关注DeepGMR等方法的局限性，旨在揭示问题根源并提出可理解的解决方案。
- Method: 提出了注意力参考点偏移（ARPS）层，该层利用经过充分研究的注意力模块来识别两个部分点集的共同参考点，而不是重叠区域，从而获得变换不变的特征。该方法扩展了DeepGMR及其变体UGMMReg。
- Result: ARPS层显著提升了DeepGMR和UGMMReg的性能，这些扩展模型甚至超越了先前使用注意力块和Transformer提取重叠区域或共同参考点的深度学习方法。
- Conclusion: 该研究为使用深度学习和高斯混合模型的配准方法提供了更深入的见解，提出的ARPS层通过识别共同参考点解决了特征不变性问题，为部分到部分点集配准提供了有效的解决方案。


### [50] [A Large Scale Benchmark for Test Time Adaptation Methods in Medical Image Segmentation](https://arxiv.org/abs/2512.02497)
*Wenjing Yu,Shuo Jiang,Yifei Chen,Shuo Chang,Yuanhan Wang,Beining Wu,Jie Dong,Mingxuan Liu,Shenghao Zhu,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: MedSeg-TTA是一个全面的医学图像分割测试时适应基准，系统评估了20种代表性方法在7种成像模态下的性能，揭示了不同适应范式的优缺点和临床部署的注意事项。

- Motivation: 当前医学图像分割的测试时适应评估存在局限性：模态覆盖不足、任务多样性不够、方法评估不一致。需要建立一个全面、标准化的基准来系统比较不同适应方法在不同医学成像模态下的性能。
- Method: 创建MedSeg-TTA基准，统一数据预处理、骨干网络配置和测试时协议。评估20种代表性适应方法，涵盖四种主要适应范式：输入级变换、特征级对齐、输出级正则化和先验估计。覆盖7种医学成像模态（MRI、CT、超声、病理、皮肤镜、OCT、胸片）。
- Result: 没有单一范式在所有条件下表现最佳。输入级方法在轻微外观变化下更稳定；特征级和输出级方法在边界相关指标上优势更大；先验方法表现出强烈的模态依赖性。许多方法在大规模跨中心、跨设备变化下性能显著下降。
- Conclusion: MedSeg-TTA为测试时适应研究提供了标准化数据集、验证实现和公开排行榜，为开发鲁棒、临床可靠的测试时适应方法建立了严格基础。强调了根据临床部署需求进行原则性方法选择的重要性。


### [51] [dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model](https://arxiv.org/abs/2512.02498)
*Yumeng Li,Guang Yang,Hao Liu,Bowen Wang,Colin Zhang*

Main category: cs.CV

TL;DR: dots.ocr是一个统一的视觉语言模型，首次将文档布局解析的三个核心任务（布局检测、文本识别和关系理解）在端到端框架中联合学习，在多语言文档理解方面实现了最先进的性能。

- Motivation: 当前文档布局解析方法依赖碎片化的多阶段流水线，存在错误传播问题，且无法利用联合训练的优势。文档布局解析是AI访问和理解海量结构化知识的关键，对下一代视觉语言模型至关重要。
- Method: 提出dots.ocr单一视觉语言模型，在统一端到端框架中联合学习三个核心任务。通过高度可扩展的数据引擎合成大规模多语言语料库，使模型能够在多样化的语言、布局和领域中实现鲁棒性能。
- Result: 在综合基准OmniDocBench上实现了最先进的性能。在新提出的跨126种语言的挑战性基准XDocParse上，dots.ocr建立了强大的新基线，以+7.4分的显著优势超越次优竞争者，证明了其无与伦比的多语言能力。
- Conclusion: dots.ocr展示了统一端到端框架在文档布局解析中的优势，通过联合学习和大规模多语言数据实现了卓越性能，为全球文档智能研究提供了新的强大基准。


### [52] [GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding](https://arxiv.org/abs/2512.02505)
*Jiaqi Liu,Ronghao Fu,Haoran Liu,Lang Sun,Bo Yang*

Main category: cs.CV

TL;DR: GeoDiT：首个面向地理空间领域的扩散式视觉语言模型，通过并行细化过程实现整体粗到细的合成，在结构化、面向对象的任务上超越自回归模型。

- Motivation: 自回归模型与地理空间理解的并行本质存在结构错配，强制将场景转换为刚性序列叙事，这从根本上阻碍了结构化、连贯输出的生成。
- Method: 将地理空间生成重新定义为并行细化过程，实现整体、粗到细的合成，同时解析所有语义元素。为此引入了GeoDiT——首个面向地理空间领域的扩散式视觉语言模型。
- Result: GeoDiT在需要结构化、面向对象输出的基准测试中建立了新的最先进水平。在图像描述、视觉定位和多目标检测任务上取得显著提升，这些正是自回归模型表现不佳的任务。
- Conclusion: 将生成过程与数据内在结构对齐是解锁复杂地理空间分析中卓越性能的关键。并行细化方法比序列自回归方法更适合地理空间理解。


### [53] [Two-Stage Vision Transformer for Image Restoration: Colorization Pretraining + Residual Upsampling](https://arxiv.org/abs/2512.02512)
*Aditya Chaudhary,Prachet Dev Singh,Ankit Jha*

Main category: cs.CV

TL;DR: ViT-SR：基于Vision Transformer的两阶段训练单图像超分辨率方法，通过着色任务自监督预训练学习通用视觉表示，然后微调进行4倍超分辨率，在DIV2K上取得优异结果。

- Motivation: 单图像超分辨率（SISR）在计算机视觉中仍然是一个难题，现有方法性能有限。作者希望通过结合Vision Transformer的强大表示能力和自监督预训练策略来提升超分辨率性能。
- Method: 提出ViT-SR方法，采用两阶段训练策略：1）自监督预训练阶段，在着色任务上训练Vision Transformer学习通用视觉表示；2）微调阶段，将预训练模型调整为4倍超分辨率任务，通过预测高频残差图像添加到初始双三次插值结果中来简化残差学习。
- Result: 在DIV2K基准数据集上，ViT-SR取得了SSIM 0.712和PSNR 22.90 dB的优异结果，证明了该方法在单图像超分辨率任务上的有效性。
- Conclusion: 两阶段训练方法和自监督预训练策略对复杂图像恢复任务具有显著效果，未来可通过更大的ViT架构或不同的预训练任务进一步提升性能。


### [54] [SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts](https://arxiv.org/abs/2512.02517)
*Jiaqi Liu,Ronghao Fu,Lang Sun,Haoran Liu,Xiao Yang,Weipeng Zhang,Xu Na,Zhuoran Duan,Bo Yang*

Main category: cs.CV

TL;DR: SkyMoE：基于专家混合的视觉语言模型，通过任务感知路由和上下文解耦增强，专门用于遥感多任务多粒度解释，在21个数据集上达到SOTA性能。

- Motivation: 现有通用视觉语言模型在遥感任务上表现不佳，现有地理空间VLM采用统一建模策略，难以区分任务类型和解释粒度，限制了局部细节感知和全局上下文理解的平衡能力。
- Method: 提出SkyMoE模型：1）采用自适应路由器生成任务和粒度感知的路由指令，让专门的LLM专家处理不同子任务；2）引入上下文解耦增强策略，创建局部和全局特征的对比对，引导专家进行层级特定的表示学习；3）构建MGRS-Bench基准，涵盖多个RS解释任务和粒度级别。
- Result: 在21个公共数据集上的广泛实验表明，SkyMoE在各项任务上实现了最先进的性能，验证了其在复杂场景中的适应性、可扩展性和卓越的多粒度理解能力。
- Conclusion: SkyMoE通过专家混合架构和粒度敏感设计，有效解决了遥感多任务多粒度解释的挑战，为地理空间AI提供了更灵活和强大的解决方案。


### [55] [On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection](https://arxiv.org/abs/2512.02520)
*Tai Le-Gia*

Main category: cs.CV

TL;DR: 该论文研究了零样本异常分类与分割的核心挑战，提出了基于图论的CoDeGraph框架来过滤一致性异常，并扩展到3D医学影像和视觉语言模型应用。

- Motivation: 工业检测和医学影像中需要无需训练数据的异常检测能力，但现有距离方法在一致性异常（重复出现的相似异常）场景下会失效，需要理论分析和解决方案。
- Method: 1. 分析预训练Vision Transformers中补丁表示的统计和几何行为，识别相似性缩放和邻居烧毁现象；2. 提出CoDeGraph图框架，通过多阶段图构建、社区检测和结构化精炼来过滤一致性异常；3. 扩展到3D医学影像，提出无需训练的体素化策略；4. 将CoDeGraph伪掩码用于监督提示驱动的视觉语言模型。
- Result: 1. 理论解释了距离方法在一致性异常下的失效机制；2. CoDeGraph能有效抑制一致性异常的影响；3. 实现了真正的零样本3D异常检测，无需3D训练样本；4. 建立了批量方法和文本方法之间的桥梁。
- Conclusion: 该论文为零样本异常分类与分割问题提供了理论理解和实用解决方案，通过图论框架解决了核心挑战，并展示了在工业检测和医学影像中的广泛应用潜力。


### [56] [WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens](https://arxiv.org/abs/2512.02536)
*Jian Yang,Dacheng Yin,Xiaoxuan He,Yong Li,Fengyun Rao,Jing Lyu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出Noisy Query Tokens方法，通过端到端优化学习VLM与扩散模型间的分布式表示空间，解决多模态大语言模型任务泛化崩溃问题，并引入VAE分支恢复图像细节。

- Motivation: 当前多模态大语言模型面临预训练视觉语言模型与扩散模型高效桥接的挑战。现有方法使用固定数量的可学习查询令牌虽然计算高效，但存在任务泛化崩溃问题，无法适应与预训练任务差异较大的新任务。
- Method: 1. 提出Noisy Query Tokens方法，通过端到端优化学习视觉语言模型与扩散模型之间的分布式表示空间，增强持续学习能力。2. 引入带有线性投影的VAE分支，用于恢复细粒度图像细节。
- Result: 实验结果表明，该方法有效缓解了泛化崩溃问题，能够在多样化任务上实现稳定的持续学习。
- Conclusion: Noisy Query Tokens方法通过分布式表示学习和VAE细节恢复，成功解决了多模态大语言模型中任务泛化崩溃的挑战，为高效的视觉语言-扩散模型桥接提供了有效解决方案。


### [57] [AVGGT: Rethinking Global Attention for Accelerating VGGT](https://arxiv.org/abs/2512.02541)
*Xianbing Sun,Zhikai Zhu,Zhengyu Lou,Bo Yang,Jinyang Tang,Liqing Zhang,He Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 提出一种无需训练的两步加速方案，通过将早期全局层转换为帧注意力，以及对KV进行子采样，在保持精度的同时实现8-10倍推理加速

- Motivation: 现有多视图3D模型（如VGGT和π³）依赖全局自注意力导致计算成本高，现有稀疏注意力变体缺乏对全局注意力在多视图推理中作用的系统分析
- Method: 首先深入分析VGGT和π³中全局注意力模块的作用，发现交替全局-帧架构中的角色分工；基于此提出训练免费的两步加速方案：1) 将早期全局层转换为帧注意力，2) 通过子采样KV进行全局注意力子采样，保留对角线并加入均值填充组件
- Result: 在标准姿态和点云基准测试中，方法实现8-10倍推理加速，同时匹配或略微提升原始模型精度，在极端密集多视图设置下仍保持鲁棒性
- Conclusion: 通过系统分析全局注意力在多视图推理中的作用，提出有效的训练免费加速方案，显著提升效率同时保持性能，为多视图3D模型的实际应用提供了实用解决方案


### [58] [OmniPerson: Unified Identity-Preserving Pedestrian Generation](https://arxiv.org/abs/2512.02554)
*Changxiao Ma,Chao Yuan,Xincheng Shi,Yuzhuo Ma,Yongfei Zhang,Longkun Zhou,Yujia Zhang,Shangze Li,Yifan Xu*

Main category: cs.CV

TL;DR: OmniPerson是一个统一的身份保持行人生成框架，支持可见光/红外图像/视频ReID任务，通过多参考图像融合实现高保真身份一致性，并创建了PersonSyn数据集用于数据增强。

- Motivation: 行人重识别任务面临数据隐私和标注成本问题，现有数据增强方法存在身份一致性差和可控性不足的缺陷，需要更好的行人生成方案来提升ReID性能。
- Method: 提出OmniPerson统一生成模型，支持RGB/IR模态图像/视频生成，具备多参考融合器实现身份保持，创建PersonSyn数据集并提供自动化标注流程。
- Result: OmniPerson在行人生成任务中达到SOTA水平，在视觉保真度和身份一致性方面表现优异，生成的数据能有效提升ReID模型性能。
- Conclusion: OmniPerson为行人重识别提供了有效的身份保持数据增强方案，解决了现有方法的局限性，将开源代码、模型和数据集促进社区发展。


### [59] [From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature](https://arxiv.org/abs/2512.02566)
*Kun Yuan,Min Woo Sun,Zhen Chen,Alejandro Lozano,Xiangteng He,Shi Li,Nassir Navab,Xiaoxiao Sun,Nicolas Padoy,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: Panel2Patch：一种从生物医学文献中挖掘层次化视觉语言监督的新数据管道，通过解析多面板、标记丰富的图表及其文本，构建从图表到面板再到局部块的多粒度对齐数据。

- Motivation: 当前生物医学视觉语言预训练通常将丰富的科学图表和文本压缩为粗略的图表级配对，丢弃了临床医生实际依赖的细粒度对应关系。需要一种能够保留局部语义层次结构的方法。
- Method: Panel2Patch数据管道：解析科学图表和标题的布局、面板和视觉标记，构建图表级、面板级和块级三个层次的视觉语言对齐对。基于此层次化语料库，开发粒度感知的预训练策略，统一从粗粒度教学描述到细粒度区域聚焦短语的异构目标。
- Result: 仅使用少量文献图表，Panel2Pipeline就能提取比先前方法更有效的监督信号，用更少的预训练数据实现显著更好的性能。
- Conclusion: Panel2Patch通过挖掘生物医学文献中的层次结构，构建多粒度监督，解决了现有方法忽略细粒度对应关系的问题，为开发更强大的生物医学视觉语言模型提供了有效的数据管道。


### [60] [Co-speech Gesture Video Generation via Motion-Based Graph Retrieval](https://arxiv.org/abs/2512.02576)
*Yafei Song,Peng Zhang,Bang Zhang*

Main category: cs.CV

TL;DR: 提出新框架，结合扩散模型生成手势动作和基于运动的检索算法，改善语音同步手势视频的生成质量。

- Motivation: 现有方法使用运动图检索手势轨迹，但基于音频特征距离或共享特征空间的方法无法处理音频与手势之间的多对多映射关系，导致生成效果不理想。
- Method: 1) 使用扩散模型生成手势动作，该模型隐式学习音频和动作的联合分布；2) 从输入音频提取低层和高层特征丰富训练；3) 设计基于运动的检索算法，评估全局和局部相似性在图中寻找最佳路径；4) 对检索到的非连续节点段进行无缝拼接。
- Result: 实验结果表明，该方法在同步准确性和生成手势自然度方面显著优于先前方法。
- Conclusion: 提出的结合扩散模型生成和运动检索的框架能有效解决音频-手势多对多映射问题，生成更同步自然的手势视频。


### [61] [Content-Aware Texturing for Gaussian Splatting](https://arxiv.org/abs/2512.02621)
*Panagiotis Papantonakis,Georgios Kopanas,Fredo Durand,George Drettakis*

Main category: cs.CV

TL;DR: 提出纹理化高斯基元方法，通过自适应纹理分辨率减少基元数量，提高3D重建效率

- Motivation: 传统高斯泼溅方法需要大量小基元来表示细节外观，当几何和外观具有不同频率特性时效率低下。受纹理映射传统启发，希望用纹理表示细节外观
- Method: 提出带纹理的2D高斯基元外观表示，根据图像采样频率限制纹理像素大小，在优化过程中自适应调整纹理分辨率（上采样或下采样），并基于纹理分辨率控制基元数量
- Result: 在图像质量和总参数数量方面优于其他纹理化高斯基元解决方案
- Conclusion: 纹理化高斯基元方法能有效减少基元数量，提高3D重建和实时渲染的效率


### [62] [RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence](https://arxiv.org/abs/2512.02622)
*Xuming He,Zehao Fan,Hengjia Li,Fan Zhuo,Hankun Xu,Senlin Cheng,Di Weng,Haifeng Liu,Can Ye,Boxi Wu*

Main category: cs.CV

TL;DR: RULER-Bench：首个专门评估视频生成模型规则推理能力的基准，涵盖6类规则40个任务，发现当前SOTA模型仅达到48.87%的规则一致性，显示视频模型推理能力仍有巨大提升空间。

- Motivation: 现有视频生成评估主要关注视觉感知和理解（如美学质量、指令遵循、时序一致性），但模型基于规则的推理能力尚未得到系统评估。虽然近期研究初步探索视频模型作为零样本学习者的潜力，但仍缺乏细粒度推理能力分解和全面评估方案。
- Method: 构建RULER-Bench基准，基于文本到视频和图像到视频两种范式，涵盖6个规则类别的40个代表性任务，包含622个高质量标注实例。采用包含4个指标的检查表评估生成视频，并利用GPT-4o进行评分（与人工判断一致性达85%）。
- Result: 实验表明，当前最先进的视频生成模型在规则一致性指标上仅达到48.87%，远未达到理想水平。这揭示了下一代视频模型在推理能力方面仍有显著改进空间。
- Conclusion: RULER-Bench填补了视频生成模型规则推理能力评估的空白，为开发具备推理意识的视频生成技术提供了重要见解，将推动视频生成模型向视觉基础智能方向发展。


### [63] [PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding](https://arxiv.org/abs/2512.02624)
*Zheng Huang,Xukai Liu,Tianyu Hu,Kai Zhang,Ye Liu*

Main category: cs.CV

TL;DR: PPTBench是一个用于评估多模态大语言模型在PowerPoint相关任务上的综合基准，包含4个类别4,439个样本，揭示了当前模型在视觉布局推理方面的显著不足。

- Motivation: PowerPoint演示文稿结合了丰富的文本内容和结构化视觉布局，是评估现代多模态大语言模型多模态推理和布局理解能力的理想测试平台。现有基准仅关注狭窄的子任务，忽视了布局中心挑战，而这些挑战对于真实世界的幻灯片创建和编辑至关重要。
- Method: 利用958个PPTX文件的多样化来源，构建了PPTBench基准，包含检测、理解、修改和生成四个类别，共4,439个样本。通过实验评估模型在多模态任务上的表现，并进行消融分析和案例研究。
- Result: 实验揭示了当前多模态大语言模型在语义理解和视觉布局推理之间存在显著差距：模型能够解释幻灯片内容，但无法产生连贯的空间排列。模型难以将视觉线索与基于JSON的布局结构相结合，也无法将视觉信息整合到API规划能力中。案例研究可视化地暴露了系统性布局错误，如错位和元素重叠。
- Conclusion: PPTBench为评估视觉语言模型在PPT场景中的表现提供了新视角，突出了视觉结构推理和连贯幻灯片生成方面的挑战和未来研究方向。所有数据集和代码已完全发布以支持可重复性和未来研究。


### [64] [Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening](https://arxiv.org/abs/2512.02643)
*Yongchuan Cui,Peng Liu,Yi Zeng*

Main category: cs.CV

TL;DR: 提出基于基础模型的新预训练策略，利用大规模模拟数据学习稳健的空间-光谱先验，显著提升遥感图像融合的跨传感器泛化能力

- Motivation: 现有深度学习方法在遥感图像融合中面临泛化能力差的问题，主要原因是真实训练数据有限且不同卫星传感器之间存在领域差距
- Method: 构建多样化模拟数据集（对ImageNet和SkyScript图像应用退化操作和增强），预训练融合模型学习通用空间-光谱表示，采用零样本和单样本评估范式
- Result: 预训练策略显著提升不同网络架构（CNN、Transformer、Mamba）在6个数据集上的泛化性能，零样本场景表现优异，单样本设置适应能力强
- Conclusion: 为跨域全色锐化提供实用解决方案，建立遥感图像融合泛化新基准，通过先进训练策略利用基础模型开辟新途径


### [65] [PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking](https://arxiv.org/abs/2512.02648)
*Dong Li,Jiahao Xiong,Yingda Huang,Le Chang*

Main category: cs.CV

TL;DR: PoreTrack3D是首个用于毛孔尺度非刚性3D面部轨迹跟踪的动态3D高斯泼溅基准数据集，包含超过44万条面部轨迹，建立了该领域首个性能基线。

- Motivation: 当前缺乏能够同时捕获传统面部标志点和毛孔尺度关键点轨迹的基准数据集，这限制了通过分析细微皮肤表面运动来研究精细面部表情的能力。
- Method: 创建了包含超过44万条面部轨迹的数据集，其中5.2万条超过10帧，68条手动审核的轨迹跨越150帧。系统评估了最先进的动态3D高斯泼溅方法，建立了该领域首个性能基线。
- Result: PoreTrack3D是首个同时捕获传统面部标志点和毛孔尺度关键点轨迹的基准数据集，为动态3D高斯泼溅方法提供了首个性能评估基准。
- Conclusion: 该数据集创建流程为高保真面部运动捕捉和动态3D重建建立了新框架，推动了通过分析细微皮肤表面运动来研究精细面部表情的进展。


### [66] [Hear What Matters! Text-conditioned Selective Video-to-Audio Generation](https://arxiv.org/abs/2512.02650)
*Junwon Lee,Juhan Nam,Jiyoung Lee*

Main category: cs.CV

TL;DR: 提出SelVA模型，通过文本提示从多对象视频中选择性生成特定声音源，解决了现有方法生成混合声音的问题

- Motivation: 在多媒体制作中，需要对每个声音源单独处理音频轨道以实现精确编辑和创意控制，但现有方法一次性生成混合声音，因为视觉特征纠缠且区域提示难以指定特定声源
- Method: 提出SelVA模型，将文本提示作为目标声源的显式选择器，调制视频编码器提取提示相关特征；使用补充令牌通过抑制文本无关激活来促进跨注意力；采用自增强方案解决单声道音频监督数据缺乏问题
- Result: 在VGG-MONOAUDIO基准测试中，SelVA在音频质量、语义对齐和时间同步方面表现优异，实验验证了其有效性
- Conclusion: SelVA成功实现了文本条件下的选择性视频到音频生成，为多媒体制作提供了精确的声音源控制能力


### [67] [Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation](https://arxiv.org/abs/2512.02660)
*Agathoklis Georgiou*

Main category: cs.CV

TL;DR: 提出混合架构Snappy，结合ColPali的视觉语义相似度和OCR空间坐标，实现文档区域级检索，提升RAG精确性

- Motivation: 现有视觉语言模型（如ColPali）只能返回整个页面而非具体区域，限制了检索增强生成（RAG）的精确性；而OCR系统虽有空间坐标但缺乏语义相关性判断能力
- Method: 提出混合架构：使用ColPali的patch级相似度分数作为空间相关性过滤器，应用于OCR提取的区域；形式化视觉transformer patch网格与OCR边界框的坐标映射，引入交集度量进行相关性传播
- Result: 建立了检索精度的理论界限，无需额外训练即可在推理时运行；发布了开源实现Snappy，正在进行实证评估
- Conclusion: 统一了视觉语义检索和OCR空间提取两种范式，为RAG提供了更精确的文档区域检索解决方案


### [68] [PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes](https://arxiv.org/abs/2512.02664)
*Derui Shan,Qian Qiao,Hao Lu,Tao Du,Peng Lu*

Main category: cs.CV

TL;DR: PolarGuide-GSDR：首个将偏振先验直接嵌入3D高斯泼溅优化的框架，实现高保真反射分离和实时渲染，无需环境贴图或材料假设

- Motivation: 现有基于NeRF的偏振感知方法存在训练慢、渲染效率低、对材料/视角假设依赖强的问题；而3DGS虽能实时渲染，但难以准确分离反射几何纠缠，且添加延迟反射模块会引入环境贴图依赖
- Method: 提出偏振前向引导范式，建立偏振与3DGS的双向耦合机制：首先利用3DGS的几何先验解决偏振歧义，然后用精炼的偏振信息引导3DGS的法线和球谐表示，实现高保真反射分离
- Result: 在公开和自采集数据集上，PolarGuide-GSDR在镜面反射重建、法线估计和新视角合成方面达到SOTA性能，同时保持实时渲染能力
- Conclusion: 这是首个将偏振先验直接嵌入3DGS优化的框架，为复杂反射场景建模提供了优越的可解释性和实时性能


### [69] [UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking](https://arxiv.org/abs/2512.02668)
*Qionglin Ren,Dawei Zhang,Chunxu Tian,Dan Zhang*

Main category: cs.CV

TL;DR: UAUTrack是一个统一的反无人机单目标跟踪框架，采用单流、单阶段、端到端架构，通过文本先验提示策略有效整合多模态信息，在多个反无人机数据集上实现了最先进的性能。

- Motivation: 当前反无人机跟踪研究缺乏统一的跨模态协作框架，现有方法主要关注独立任务的独立模型，忽视了跨模态信息共享的潜力，且反无人机跟踪技术仍处于起步阶段，现有解决方案难以实现有效的多模态数据融合。
- Method: 提出UAUTrack统一单目标跟踪框架，基于单流、单阶段、端到端架构，关键创新是引入文本先验提示策略，引导模型在不同场景下专注于无人机目标。
- Result: 在Anti-UAV和DUT Anti-UAV数据集上实现了最先进的性能，在Anti-UAV410数据集上保持了准确性和速度之间的良好平衡，展示了在各种反无人机场景下的高准确性和实际效率。
- Conclusion: UAUTrack通过统一的跨模态协作框架和文本先验提示策略，成功解决了反无人机跟踪中的多模态融合挑战，为反无人机跟踪领域提供了高效实用的解决方案。


### [70] [PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution](https://arxiv.org/abs/2512.02681)
*Zhongbao Yang,Jiangxin Dong,Yazhou Yao,Jinhui Tang,Jinshan Pan*

Main category: cs.CV

TL;DR: PGP-DiffSR：一种轻量级扩散超分辨率方法，通过渐进式剪枝和相位交换适配器减少计算和内存开销，同时保持恢复质量。

- Motivation: 现有的扩散模型（如SDXL、DiT）在图像超分辨率中计算和内存成本过高，需要开发轻量级方法来解决冗余问题。
- Method: 1. 识别扩散主干中的块内冗余，提出渐进式剪枝方法去除冗余块但保留恢复能力；2. 提出相位交换适配器模块，利用输入的相位信息引导剪枝后的扩散模型提升恢复性能；3. 将渐进式剪枝和相位交换适配器整合为统一模型。
- Result: 实验表明该方法在显著降低计算负载和内存消耗的同时，实现了有竞争力的恢复质量。
- Conclusion: PGP-DiffSR通过去除冗余信息和利用相位指导，为扩散超分辨率提供了一种高效轻量的解决方案。


### [71] [Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance](https://arxiv.org/abs/2512.02685)
*Huankun Sheng,Ming Li,Yixiang Wei,Yeying Fan,Yu-Hui Wen,Tieliang Gong,Yong-Jin Liu*

Main category: cs.CV

TL;DR: FASA提出了一种两阶段的前景感知槽注意力框架，通过显式分离前景和背景来改进无监督物体发现性能。

- Motivation: 现有基于槽注意力的方法在处理前景和背景区域时通常不加区分，导致背景干扰和真实世界数据上的实例发现性能不佳。
- Method: 两阶段框架：第一阶段通过双槽竞争机制进行粗粒度场景分解区分前景和背景；第二阶段引入掩码槽注意力机制，第一个槽捕获背景，其余槽竞争表示单个前景物体。还使用基于自监督图像特征构建的补丁亲和图的伪掩码指导来引导前景槽学习。
- Result: 在合成和真实世界数据集上的广泛实验表明，FASA始终优于最先进的方法，验证了显式前景建模和伪掩码指导对鲁棒场景分解和物体一致性表示的有效性。
- Conclusion: FASA通过显式前景建模和伪掩码指导，有效解决了现有槽注意力方法中的背景干扰问题，实现了更精确的无监督物体发现。


### [72] [ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data](https://arxiv.org/abs/2512.02686)
*Yuxing Liu,Yong Liu*

Main category: cs.CV

TL;DR: ClimaDrive是一个语义引导的图像到图像框架，用于合成语义连贯、天气多样且物理合理的异常驾驶数据，通过构建ClimaOoD基准显著提升了异常分割模型的性能。

- Motivation: 异常分割对于安全自动驾驶至关重要，但异常数据的稀缺性和有限多样性严重限制了模型在开放世界环境中的泛化能力。现有合成数据方法缺乏上下文连贯性和物理真实性，导致合成数据与真实数据之间存在领域差距。
- Method: 提出ClimaDrive框架，统一了结构引导的多天气生成和提示驱动的异常修复，能够创建视觉逼真的训练数据。基于此框架构建了ClimaOoD大规模基准，涵盖六种代表性驾驶场景和不同天气条件。
- Result: 在四种最先进方法上的实验表明，使用ClimaOoD训练显著提升了异常分割性能。所有方法的AUROC、AP和FPR95都有明显改善，例如RbA在Fishyscapes LAF上的FPR95从3.97降至3.52。
- Conclusion: ClimaOoD增强了模型鲁棒性，为开放世界异常检测提供了有价值的训练数据，能够实现更好的泛化能力。


### [73] [ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection](https://arxiv.org/abs/2512.02696)
*Omid Reza Heidari,Yang Wang,Xinxin Zuo*

Main category: cs.CV

TL;DR: ALDI++ 域适应框架在安全X射线图像检测中超越现有方法，结合自蒸馏、特征对齐和增强训练策略，在多个适应场景下取得最佳性能。

- Motivation: 安全X射线成像中扫描设备和环境条件的变化导致显著的域差异，这会降低目标检测模型的性能，需要有效的域适应方法来解决这一问题。
- Method: 应用ALDI++域适应框架，整合自蒸馏、特征对齐和增强训练策略，并使用Vision Transformer for Detection (ViTDet)作为骨干网络。
- Result: 在EDS数据集上的实验表明，ALDI++在多个适应场景下超越现有SOTA方法，特别是ViTDet骨干网络实现了最高的mAP，类别分析显示检测精度一致提升。
- Conclusion: ALDI++是域适应目标检测的高效解决方案，为安全X射线图像的性能稳定性和跨域泛化能力设定了新基准。


### [74] [GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization](https://arxiv.org/abs/2512.02697)
*Zixuan Song,Jing Zhang,Di Wang,Zidie Zhou,Wenbin Liu,Haonan Guo,En Wang,Bo Du*

Main category: cs.CV

TL;DR: GeoBridge是一个跨视图地理定位基础模型，通过新颖的语义锚机制实现多视图双向匹配，并支持语言到图像检索，解决了传统卫星中心方法的局限性。

- Motivation: 传统卫星中心的地理定位方法在高分辨率或最新卫星图像不可用时鲁棒性有限，且未能充分利用跨视图（无人机、卫星、街景）和跨模态（语言和图像）的互补线索。
- Method: 提出GeoBridge基础模型，基于语义锚机制通过文本描述桥接多视图特征；构建了首个大规模跨模态多视图对齐数据集GeoLoc（包含5万多对图像和文本描述）。
- Result: GeoLoc预训练显著提高了GeoBridge的地理定位精度，同时促进了跨域泛化和跨模态知识迁移；模型在多个任务上进行了广泛评估。
- Conclusion: GeoBridge通过语义锚机制实现了鲁棒灵活的地理定位，超越了传统卫星中心方法；GeoLoc数据集为跨视图跨模态地理定位研究提供了重要资源。


### [75] [VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm](https://arxiv.org/abs/2512.02700)
*Zhenkai Wu,Xiaowen Ma,Zhenliang Ni,Dengming Zhang,Han Shu,Xin Jiang,Xinghao Chen*

Main category: cs.CV

TL;DR: VLM-Pruner是一种无需训练的视觉语言模型token剪枝算法，通过平衡冗余性和空间稀疏性，在保持88.9%剪枝率的同时提升推理速度。

- Motivation: 现有视觉语言模型的视觉token数量庞大导致计算成本高，阻碍移动端部署。现有剪枝方法要么只关注token重要性而忽略冗余性，要么考虑冗余性但忽略空间关系，导致保留的token过于稀疏无法充分覆盖目标对象区域。
- Method: 提出VLM-Pruner算法：1) 采用离心式token剪枝范式，实现从近到远的选择同时优先保留细粒度物体细节；2) 设计空间稀疏性缓冲(BSS)准则，延迟选择空间距离较远的token；3) 采用并行贪心策略高效进行token选择；4) 选择性融合被丢弃token中的显著信息到保留的token中。
- Result: 在五种VLMs上，VLM-Pruner在88.9%的剪枝率下始终优于强基线方法，同时实现了端到端推理加速。
- Conclusion: VLM-Pruner通过显式平衡冗余性和空间稀疏性，解决了现有视觉语言模型token剪枝方法的局限性，在保持高性能的同时显著降低了计算成本。


### [76] [Tissue-mask supported inter-subject whole-body image registration in the UK Biobank -- A method benchmarking study](https://arxiv.org/abs/2512.02702)
*Yasemin Utkueri,Elin Lundström,Håkan Ahlström,Johan Öfverstedt,Joel Kullberg*

Main category: cs.CV

TL;DR: 提出一种基于性别分层的全身MRI图像配准方法，使用脂肪和肌肉掩膜增强图割配准，在UK Biobank数据上表现优于现有方法。

- Motivation: UK Biobank收集了大量全身MRI图像和非成像健康数据，需要稳健准确的跨被试图像配准来实现全身空间标准化，并分析非成像数据与图像衍生参数（如组织体积或脂肪含量）的相关性。
- Method: 提出性别分层的跨被试全身MRI图像配准方法，使用VIBESegmentator方法生成的皮下脂肪组织和肌肉掩膜来增强基于强度的图割配准。
- Result: 在4000名受试者子集上评估，与仅使用强度的方法相比，Dice分数提高了6个百分点；与uniGradICON和MIRTK相比分别提高了8-9和12-13个百分点。年龄相关性图谱更清晰且解剖对齐更好。
- Conclusion: 使用两种组织掩膜的图像配准方法改善了UK Biobank图像的全身配准，为医学研究提供了更准确的分析工具。


### [77] [GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding](https://arxiv.org/abs/2512.02715)
*Peirong Zhang,Yidan Zhang,Luxiao Xu,Jinliang Lin,Zonghao Guo,Fengxiang Wang,Xue Yang,Kaiwen Wei,Lei Wang*

Main category: cs.CV

TL;DR: GeoViS：一个基于地理空间奖励的视觉搜索框架，将遥感视觉定位重新定义为渐进式搜索推理过程，通过树状视觉线索序列主动探索全局图像，显著提升小目标检测和复杂空间关系理解能力。

- Motivation: 现有多模态大语言模型在视觉定位方面取得显著进展，但将其能力迁移到遥感图像仍面临挑战：目标在千米级场景中极其微小，查询通常涉及复杂的空间关系（相对位置、空间层次、跨距离对象的上下文依赖）。
- Method: 提出GeoViS框架，将遥感视觉定位重新定义为渐进式搜索推理过程。通过树状结构的视觉线索序列主动探索全局图像，整合多模态感知、空间推理和奖励引导探索，迭代优化地理空间假设，而非单步直接预测目标位置。
- Result: 在五个遥感定位基准测试上的广泛实验表明，GeoViS实现了精确的地理空间理解，在关键视觉定位指标上持续超越现有方法，展现出强大的跨域泛化能力和可解释性。
- Conclusion: GeoViS通过渐进式搜索推理框架有效解决了遥感图像中小目标检测和复杂空间关系理解的挑战，为遥感视觉定位提供了新的解决方案，具有实际应用价值。


### [78] [DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions](https://arxiv.org/abs/2512.02727)
*Yifan Zhou,Takehiko Ohkawa,Guwenxiao Zhou,Kanoko Goto,Takumi Hirose,Yusuke Sekikawa,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 提出DF-Mamba框架，通过状态空间建模和可变形状态扫描来改进3D手部姿态估计中的特征提取，解决手部遮挡问题，在多个数据集上达到SOTA性能。

- Motivation: 日常手部交互建模常面临严重遮挡问题（如双手重叠），需要鲁棒的特征学习。现有3D手部姿态估计方法多依赖ResNet，但其CNN归纳偏置可能不适合3D HPE，因为建模全局上下文能力有限。
- Method: 提出Deformable Mamba (DF-Mamba)框架，结合状态空间建模（Mamba）和可变形状态扫描。通过Mamba的选择性状态建模和可变形扫描，在卷积后的局部特征中聚合图像特征，同时选择性地保留代表全局上下文的线索。
- Result: 在五个不同数据集（单手/双手、手部/手物交互、RGB/深度）上广泛评估，DF-Mamba超越了最新图像骨干网络（包括VMamba和Spatial-Mamba），在所有数据集上达到最先进性能，推理速度与ResNet-50相当。
- Conclusion: DF-Mamba通过状态空间建模和可变形扫描有效捕捉全局上下文，显著提升结构化3D手部姿态估计的准确性，为解决遮挡问题提供了有效方案。


### [79] [Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone](https://arxiv.org/abs/2512.02737)
*Tristan Amadei,Enric Meinhardt-Llopis,Benedicte Bascle,Corentin Abgrall,Gabriele Facciolo*

Main category: cs.CV

TL;DR: CAEVL：一种无需无人机训练图像的跨视角定位方法，通过卫星图像增强模拟无人机视角，在GNSS拒止环境中实现高效图像定位

- Motivation: 在GNSS拒止环境中，基于图像的无人机定位至关重要。现有方法需要大量配对的无人机-卫星图像数据集进行训练，但这些数据获取成本高且往往不可用，限制了方法的适用性。
- Method: 采用无需无人机图像的训练范式，直接从卫星参考图像学习。通过专门的增强策略模拟卫星视图与真实无人机视图之间的视觉域偏移。提出CAEVL模型利用这一范式，并在新发布的ViLD真实无人机图像数据集上进行验证。
- Result: 该方法在无需配对数据训练的情况下，达到了与使用配对数据训练方法相竞争的性能，证明了其有效性和强大的泛化能力。
- Conclusion: CAEVL通过创新的训练范式和增强策略，解决了无人机定位中数据获取困难的问题，为GNSS拒止环境中的无人机自主导航提供了实用解决方案。


### [80] [Reasoning-Aware Multimodal Fusion for Hateful Video Detection](https://arxiv.org/abs/2512.02743)
*Shuonan Yang,Tailin Chen,Jiangbei Yue,Guangliang Cheng,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出RAMF框架，通过局部-全局上下文融合和语义交叉注意力解决多模态融合问题，引入对抗推理增强对微妙仇恨内容的理解，在两个真实仇恨视频数据集上超越SOTA方法。

- Motivation: 在线视频中的仇恨言论日益严重，现有方法难以有效融合多模态间的复杂语义关系，且缺乏对微妙仇恨内容的理解能力。
- Method: 提出RAMF框架：1) 局部-全局上下文融合(LGCF)捕捉局部显著线索和全局时序结构；2) 语义交叉注意力(SCA)实现细粒度多模态语义交互；3) 对抗推理过程，通过视觉语言模型生成客观描述、仇恨假设推理和非仇恨假设推理，提供互补语义视角。
- Result: 在两个真实仇恨视频数据集上，相比现有最佳方法，Macro-F1提升3%，仇恨类别召回率提升7%，展现出强大的泛化性能。
- Conclusion: RAMF框架通过创新的多模态融合和对抗推理机制，有效解决了在线视频仇恨检测中的语义融合和微妙内容理解问题，显著提升了检测性能。


### [81] [AttMetNet: Attention-Enhanced Deep Neural Network for Methane Plume Detection in Sentinel-2 Satellite Imagery](https://arxiv.org/abs/2512.02751)
*Rakib Ahsan,MD Sadik Hossain Shanto,Md Sultanul Arifin,Tanzima Hashem*

Main category: cs.CV

TL;DR: AttMetNet：一种基于注意力增强U-Net和NDMI的新型深度学习框架，用于Sentinel-2卫星图像中的甲烷羽流检测，通过注意力机制和焦点损失函数解决背景噪声和类别不平衡问题。

- Motivation: 甲烷是强效温室气体，准确检测甲烷排放对应对气候变化至关重要。传统方法依赖波段差异或比值，但会产生大量误报且需要专家验证；现有深度学习方法缺乏针对甲烷特征的优先级机制。
- Method: 提出AttMetNet框架：1）融合归一化甲烷差异指数（NDMI）与注意力增强U-Net架构；2）通过注意力机制选择性放大甲烷吸收特征并抑制背景噪声；3）使用焦点损失函数解决类别不平衡问题；4）在真实甲烷羽流数据集上训练。
- Result: 实验表明AttMetNet优于现有方法：误报率更低、精确率-召回率平衡更好、IoU更高，在真实卫星图像中实现了更稳健的甲烷羽流检测。
- Conclusion: AttMetNet通过结合NDMI的羽流敏感线索和注意力驱动的特征选择，创建了首个针对真实卫星图像甲烷羽流检测的专用架构，显著提升了检测性能并减少了误报。


### [82] [Rethinking Surgical Smoke: A Smoke-Type-Aware Laparoscopic Video Desmoking Method and Dataset](https://arxiv.org/abs/2512.02780)
*Qifan Liang,Junlin Li,Zhen Han,Xihao Wang,Zhongyuan Wang,Bin Mei*

Main category: cs.CV

TL;DR: 提出首个烟雾类型感知的腹腔镜视频去烟网络STANet，通过区分扩散烟雾和环境烟雾两种类型，结合烟雾掩码分割和视频重建子网络，在合成数据集上验证了优越性能。

- Motivation: 腹腔镜手术中电刀或激光产生的烟雾会阻碍视觉引导，现有去烟方法未能考虑不同烟雾类型（扩散烟雾和环境烟雾）在运动模式和时空特征上的差异。
- Method: 提出STANet网络：1）烟雾掩码分割子网络通过注意力加权掩码聚合联合预测烟雾掩码和类型；2）无烟视频重建子网络基于两种烟雾掩码指导进行针对性去烟；3）粗到细解耦模块处理两种烟雾类型的纠缠问题；4）构建首个带烟雾类型标注的大规模合成视频去烟数据集。
- Result: 实验表明该方法在质量评估上优于现有方法，并在多个下游手术任务中展现出优越的泛化能力。
- Conclusion: 通过区分烟雾类型并设计相应的解耦机制，STANet能够更有效地去除腹腔镜视频中的烟雾，提升手术视觉质量并支持下游任务。


### [83] [LumiX: Structured and Coherent Text-to-Intrinsic Generation](https://arxiv.org/abs/2512.02781)
*Xu Han,Biao Zhang,Xiangjun Tang,Xianzhi Li,Peter Wonka*

Main category: cs.CV

TL;DR: LumiX是一个结构化扩散框架，用于从文本生成内在属性图（如反照率、法线、深度等），通过查询广播注意力和张量LoRA确保物理一致性和高效联合训练。

- Motivation: 现有方法在从文本生成内在属性图时缺乏结构一致性和物理合理性，难以同时生成多个内在属性并保持它们之间的物理一致性。
- Method: 提出结构化扩散框架LumiX，包含两个关键技术：1) 查询广播注意力机制，通过在所有图中共享查询确保结构一致性；2) 张量LoRA，参数高效地建模跨图关系以实现高效联合训练。
- Result: LumiX能生成连贯且物理合理的结果，相比现有技术实现了23%更高的对齐度和更好的偏好分数（0.19 vs -0.41），并能在同一框架内执行图像条件的内在分解。
- Conclusion: LumiX通过结构化扩散框架成功解决了文本到内在属性生成的挑战，实现了多个内在属性的物理一致生成，并在质量和一致性方面超越了现有技术。


### [84] [TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking](https://arxiv.org/abs/2512.02789)
*Tang Haonan,Chen Yanjun,Jiang Lezhi*

Main category: cs.CV

TL;DR: TrackNetV5通过引入运动方向解耦模块和残差驱动的时空细化头，解决了前代版本在遮挡处理和运动方向模糊性方面的问题，在保持实时推理能力的同时实现了最先进的跟踪性能。

- Motivation: TrackNet系列在快速移动小物体跟踪方面建立了强基线，但现有版本存在显著限制：V1-V3因依赖纯视觉线索而难以处理遮挡；TrackNetV4虽然引入了运动输入，但其绝对差分方法丢弃了运动极性，导致方向模糊性。需要克服这些瓶颈以实现更鲁棒的跟踪。
- Method: 提出TrackNetV5，包含两个新颖机制：1) 运动方向解耦模块：将时间动态分解为带符号的极性场，显式编码运动发生和轨迹方向；2) 残差驱动的时空细化头：基于粗到细的范式，利用分解的时空上下文估计校正残差，有效恢复被遮挡目标。
- Result: 在TrackNetV2数据集上的广泛实验表明，TrackNetV5实现了新的最先进性能：F1分数0.9859，准确率0.9733，显著优于先前版本。性能提升仅伴随3.7%的FLOPs增加，保持了实时推理能力。
- Conclusion: TrackNetV5通过整合运动方向解耦和残差驱动的时空细化，成功解决了前代版本的方向模糊性和遮挡问题，在计算效率几乎不变的情况下实现了显著的性能提升，为快速移动小物体跟踪提供了更鲁棒的解决方案。


### [85] [UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits](https://arxiv.org/abs/2512.02790)
*Keming Ye,Zhipeng Huang,Canmiao Fu,Qingyang Liu,Jiani Cai,Zheqi Lv,Chen Li,Jing Lyu,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: 该论文提出了UnicEdit-10M数据集和UnicBench基准，通过轻量级数据管道解决多模态图像编辑模型训练数据稀缺和质量问题，并引入新评估指标诊断模型弱点。

- Motivation: 当前闭源与开源多模态图像编辑模型性能差距扩大，主要由于缺乏大规模高质量训练数据和能够诊断多样化编辑行为的综合基准。现有数据构建方法面临规模与质量的权衡：人工标注质量高但不可扩展，自动化流程存在错误传播和噪声问题。
- Method: 提出轻量级数据管道，用端到端模型替代多工具链，并加入统一后验证阶段。训练7B双任务专家模型Qwen-Verify进行高效失败检测和指令重写。构建10M规模的UnicEdit-10M数据集，涵盖基础与复杂编辑任务。提出UnicBench基准，扩展评估范围至空间和知识驱动推理，引入非编辑一致性和推理准确性等新指标。
- Result: 成功构建了UnicEdit-10M数据集（10M规模）和UnicBench基准。在UnicBench上分析主流模型，揭示了它们的局限性，为未来研究提供了明确方向。
- Conclusion: 该工作通过创新的数据管道和综合基准，解决了多模态图像编辑领域的数据稀缺和评估不足问题，为开源模型发展提供了重要基础设施，并指出了未来改进方向。


### [86] [HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval](https://arxiv.org/abs/2512.02792)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Haokun Wen,Weili Guan*

Main category: cs.CV

TL;DR: 提出HUD框架解决组合视频检索中多模态查询的信息密度差异问题，通过分层不确定性感知消歧网络提升目标视频检索精度。

- Motivation: 组合视频检索中视频模态比文本模态包含更丰富的语义信息，但现有方法忽视了这种信息密度差异，导致修改主体指代模糊和细节语义关注有限的问题。
- Method: 提出分层不确定性感知消歧网络(HUD)，包含三个关键组件：整体代词消歧、原子不确定性建模、整体到原子对齐，通过跨模态交互实现有效的对象消歧和细节语义关注。
- Result: 在三个基准数据集上，HUD在组合视频检索和组合图像检索任务中都取得了最先进的性能。
- Conclusion: HUD是首个利用视频和文本信息密度差异来增强多模态查询理解的框架，通过分层不确定性建模有效解决了组合检索中的指代模糊和细节关注问题。


### [87] [IC-World: In-Context Generation for Shared World Modeling](https://arxiv.org/abs/2512.02793)
*Fan Wu,Jiacheng Wei,Ruibo Li,Yi Xu,Junyou Li,Deheng Ye,Guosheng Lin*

Main category: cs.CV

TL;DR: IC-World是一个用于共享世界建模的视频生成框架，通过激活大型视频模型的内在上下文生成能力，从一组输入图像并行生成多个视频，并通过强化学习优化几何和运动一致性。

- Motivation: 视频世界模型在合成多样动态视觉环境方面受到关注，但现有方法在共享世界建模方面存在不足，即从同一底层世界在不同相机姿态下生成多个视频时，难以保持场景几何和物体运动的一致性。
- Method: 提出IC-World框架：1）利用大型视频模型的内在上下文生成能力并行处理所有输入图像；2）通过强化学习（Group Relative Policy Optimization）微调模型；3）引入两个新的奖励模型来强制场景级几何一致性和对象级运动一致性。
- Result: 大量实验表明，IC-World在几何一致性和运动一致性方面显著优于现有最先进方法，是首个系统探索基于视频模型的共享世界建模问题的工作。
- Conclusion: IC-World成功解决了共享世界建模中的一致性挑战，通过激活大型视频模型的上下文生成能力和强化学习优化，实现了从同一世界不同视角生成一致视频的能力。


### [88] [PhyCustom: Towards Realistic Physical Customization in Text-to-Image Generation](https://arxiv.org/abs/2512.02794)
*Fan Wu,Cheng Chen,Zhoujie Fu,Jiacheng Wei,Yi Xu,Deheng Ye,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出PhyCustom框架，通过两种正则化损失激活扩散模型进行物理概念定制，解决现有方法在物理属性生成上的不足

- Motivation: 现有基于扩散的文本到图像定制方法在具体概念（如风格和形状）上取得了成功，但缺乏对物理概念的定制能力。当前方法在训练过程中没有明确引入物理知识，即使输入提示中包含物理相关词汇，也无法准确反映相应的物理属性。
- Method: 提出PhyCustom微调框架，包含两种新颖的正则化损失：1）等距损失（isometric loss）旨在激活扩散模型学习物理概念；2）解耦损失（decouple loss）帮助消除独立概念的混合学习。
- Result: 在多样化数据集上的实验表明，PhyCustom在物理定制方面在定量和定性上都优于先前的最先进方法和流行方法。
- Conclusion: PhyCustom框架通过引入物理知识激活扩散模型，成功解决了物理概念定制这一现实而具有挑战性的问题，在物理属性生成方面取得了显著改进。


### [89] [Defense That Attacks: How Robust Models Become Better Attackers](https://arxiv.org/abs/2512.02830)
*Mohamed Awad,Mahmoud Akrm,Walid Gomaa*

Main category: cs.CV

TL;DR: 对抗训练虽然提高模型鲁棒性，却意外增加了对抗样本的可迁移性，带来新的生态系统风险

- Motivation: 尽管对抗训练是提高模型鲁棒性的主要防御方法，但其对对抗样本可迁移性的影响尚未充分研究。本文旨在探究对抗训练是否会无意中增加对抗样本的可迁移性。
- Method: 训练了包含CNN和ViT在内的36个多样化模型动物园，进行了全面的可迁移性实验，分析对抗训练模型与标准模型生成的对抗样本的转移效果。
- Result: 发现了一个明显的悖论：对抗训练模型产生的扰动比标准模型产生的扰动具有更强的可迁移性，这引入了新的生态系统风险。作者发布了所有模型、代码和实验脚本以确保可复现性。
- Conclusion: 鲁棒性评估不仅应评估模型对迁移攻击的抵抗能力，还应评估其产生可迁移对抗样本的倾向性。对抗训练在提高鲁棒性的同时可能带来新的安全风险。


### [90] [ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning](https://arxiv.org/abs/2512.02835)
*Yifan Li,Yingda Yin,Lingting Zhu,Weikai Chen,Shengju Qian,Xin Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: ReVSeg：通过显式分解推理过程，将视频目标分割任务分解为语义解释、时序证据选择和空间定位三个步骤，利用预训练视觉语言模型进行多步推理，并通过强化学习优化推理链。

- Motivation: 现有视频目标分割方法通常将复杂的推理过程（涉及动态、因果关系和时序交互）压缩为简化的潜在嵌入推理，导致推理链不透明且难以追踪。需要一种更透明、可解释的推理方法。
- Method: 提出ReVSeg框架，将推理过程显式分解为三个步骤：1)语义解释，2)时序证据选择，3)空间定位。利用预训练视觉语言模型作为原生接口执行这些操作，并通过强化学习优化多步推理链，使模型能够根据结果驱动的信号自我优化决策质量。
- Result: 在标准视频目标分割基准测试中达到最先进性能，同时产生可解释的推理轨迹，使推理过程更加透明。
- Conclusion: 通过显式分解推理过程并利用预训练视觉语言模型的能力，ReVSeg实现了更透明、可解释的视频目标分割推理，在保持高性能的同时提供了更好的可解释性。


### [91] [Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?](https://arxiv.org/abs/2512.02846)
*Manuel Benavent-Lledo,Konstantinos Bacharidis,Victoria Manousaki,Konstantinos Papoutsakis,Antonis Argyros,Jose Garcia-Rodriguez*

Main category: cs.CV

TL;DR: AAG方法通过单帧RGB特征、深度线索和先验动作信息实现动作预测，无需视频时序聚合，在多个指令活动数据集上达到与视频基准和SOTA方法竞争的性能。

- Motivation: 人类观察单个场景瞬间就能预测即将发生的动作，但传统方法依赖视频时序信息提取和聚合。本研究探索能否用替代模态替代视频聚合，实现单帧动作预测。
- Method: 提出AAG方法：结合单帧RGB特征和深度线索增强空间推理，融入先验动作信息提供长期上下文（通过视觉语言模型文本摘要或单帧动作识别器预测获得）。
- Result: 在IKEA-ASM、Meccano和Assembly101三个指令活动数据集上，AAG的单帧多模态动作预测性能与基于时序聚合的视频基准方法和SOTA方法相当。
- Conclusion: 通过适当的模态组合和上下文信息，单帧动作预测可以替代视频时序聚合，在复杂任务中实现竞争性性能，为动作理解研究提供了新方向。


### [92] [Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study](https://arxiv.org/abs/2512.02850)
*Vishal Dubey,Pallavi Tyagi*

Main category: cs.CV

TL;DR: 该论文首次系统研究了印度和南亚面孔的身份保持AIGC检测，发现现有检测器在该场景下泛化能力差，微调会导致对训练生成器线索的过拟合。

- Motivation: 现代图像编辑器可以生成身份保持的AIGC（IP-AIGC），但现有检测器在这种情况下的鲁棒性和公平性尚不清楚，特别是对于代表性不足的人群。论文旨在填补这一研究空白。
- Method: 从FairFD和HAV-DF数据集中构建印度焦点训练集，使用Gemini和ChatGPT等商业生成器创建两个IP-AIGC测试集（HIDF-img-ip-genai和HIDF-vid-ip-genai）。评估AIDE和Effort两种先进检测器在预训练和微调两种模式下的性能。
- Result: 微调在域内表现提升显著，但在印度人群的IP-AIGC测试集上性能大幅下降，表明检测器过拟合于训练生成器的特定线索。在非IP图像上，预训练模型性能保持良好，说明问题特定于身份保持编辑而非一般分布偏移。
- Conclusion: IP-AIGC-Indian是一个具有挑战性且实际相关的场景，需要开发保持表征能力的适应方法和印度感知的基准数据集，以缩小AIGC检测中的泛化差距。


### [93] [RFOP: Rethinking Fusion and Orthogonal Projection for Face-Voice Association](https://arxiv.org/abs/2512.02860)
*Abdul Hannan,Furqan Malik,Hina Jabbar,Syed Suleman Sadiq,Mubashir Noman*

Main category: cs.CV

TL;DR: 该论文针对多语言环境下的面部-语音关联任务，提出了一种融合和正交投影方法，在FAME 2026挑战赛中取得第三名，EER为33.1%

- Motivation: 多语言环境下的面部-语音关联任务具有挑战性，特别是在英语-德语跨语言场景中，需要有效提取两种模态间的相关语义信息
- Method: 重新审视融合和正交投影方法，有效聚焦于两种模态内的相关语义信息，通过改进的融合策略和正交投影技术来处理面部和语音数据的关联
- Result: 在英语-德语数据分割上表现良好，在FAME 2026挑战赛中排名第三，实现了33.1%的等错误率(EER)
- Conclusion: 提出的融合和正交投影方法在多语言面部-语音关联任务中有效，能够处理跨语言场景下的模态关联问题


### [94] [MICCAI STSR 2025 Challenge: Semi-Supervised Teeth and Pulp Segmentation and CBCT-IOS Registration](https://arxiv.org/abs/2512.02867)
*Yaqi Wang,Zhi Li,Chengyu Wu,Jun Liu,Yifan Zhang,Jialuo Chen,Jiaxue Ni,Qian Luo,Jin Liu,Can Han,Changkai Ji,Zhi Qin Tan,Ajo Babu George,Liangyu Chen,Qianni Zhang,Dahong Qian,Shuai Wang,Huiyu Zhou*

Main category: cs.CV

TL;DR: STSR 2025挑战赛在MICCAI 2025举办，旨在评估半监督学习在CBCT牙齿/根管分割和CBCT-IOS跨模态配准中的应用，提供标注/未标注数据集，吸引了开源深度学习解决方案，取得了优异性能。

- Motivation: 数字牙科中CBCT和IOS数据标注稀缺，限制了自动化解决方案的发展，需要建立基准来评估半监督学习在这些任务上的表现。
- Method: 组织STSR 2025挑战赛，包含两个任务：1) CBCT中牙齿和根管的半监督分割；2) CBCT和IOS的半监督刚性配准。提供60个标注和640个未标注IOS样本，30个标注和250个未标注CBCT扫描。
- Result: 分割任务中，领先方法使用nnU-Net和Mamba-like状态空间模型，结合伪标签和一致性正则化，在隐藏测试集上获得Dice分数0.967和实例亲和度0.738。配准任务中，有效方法结合PointNetLK与可微SVD和几何增强，混合神经-经典细化实现了准确对齐。
- Conclusion: 挑战赛成功展示了半监督学习在数字牙科中的潜力，所有数据和代码已公开以确保可重复性，为社区提供了有价值的基准和开源解决方案。


### [95] [Taming Camera-Controlled Video Generation with Verifiable Geometry Reward](https://arxiv.org/abs/2512.02870)
*Zhaoqing Wang,Xiaobo Xia,Zhuolin Bie,Jinlin Liu,Dongdong Yu,Jia-Wang Bian,Changhu Wang*

Main category: cs.CV

TL;DR: 提出在线强化学习后训练框架，优化预训练视频生成器以实现精确相机控制，通过几何奖励函数提供密集片段级反馈

- Motivation: 当前视频扩散模型主要依赖监督微调，在线强化学习后训练在相机控制视频生成领域尚未充分探索，需要更有效的优化方法
- Method: 设计可验证的几何奖励函数，估计生成视频和参考视频的3D相机轨迹，分割为短片段并计算片段级相对位姿，通过对比生成-参考片段对提供密集奖励信号
- Result: 在线RL后训练明显优于监督微调基线，在相机控制精度、几何一致性和视觉质量等多个方面表现更优
- Conclusion: 在线强化学习后训练框架能有效提升相机控制视频生成性能，通过密集几何奖励信号解决了奖励稀疏性问题


### [96] [MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm](https://arxiv.org/abs/2512.02895)
*Wei Chen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Zide Liu,Xuhao Pan,Chang Ren,Xudong Rao,Chenfeng Wang,Tao Wei,Chengjun Yu,Pengfei Yu,Yufei Zheng,Chunpeng Zhou,Pan Zhou,Xuhan Zhu*

Main category: cs.CV

TL;DR: MindGPT-4ov提出了一种通用的多模态大语言模型后训练范式，通过数据生成、模型训练和高效部署三个方面的创新，在低成本下实现SOTA性能，并支持从学术研究到工业部署的无缝过渡。

- Motivation: 现有MLLMs在跨领域泛化能力、推理能力和部署效率方面存在不足，需要一种通用的后训练范式来提升基础能力，同时降低领域适应的成本。
- Method: 提出了三个关键创新：1) 基于信息密度的数据生成方案和双维度树状标签系统；2) 协作课程监督微调方法；3) 混合强化学习范式。同时实施了5D并行训练、算子优化和推理量化等基础设施优化。
- Result: 在MMBench、MMStar、MathVision、MathVista等多个基准测试中超越了现有最先进模型，在垂直领域任务中表现出卓越的用户体验。
- Conclusion: MindGPT-4ov提供了一个适用于广泛MLLMs的通用后训练范式，其模型权重、数据集和代码将开源，支持社区MLLMs的发展。


### [97] [Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models](https://arxiv.org/abs/2512.02897)
*Pierpaolo Serio,Giulio Pisaneschi,Andrea Dan Ryals,Vincenzo Infantino,Lorenzo Gentilini,Valentina Donzella,Lorenzo Pollini*

Main category: cs.CV

TL;DR: 研究不同LiDAR到图像投影方式如何影响度量地点识别性能，发现精心设计的投影可作为端到端3D学习的有效替代方案

- Motivation: 探索LiDAR数据的不同2D投影方式对地点识别性能的影响，特别是在结合先进视觉基础模型时，了解投影特性如何决定识别能力
- Method: 构建模块化检索流程，控制主干网络、聚合方法和评估协议，隔离2D投影本身的影响；使用一致的几何和结构通道，在多个数据集和部署场景中测试不同投影
- Result: 识别出最能决定判别能力、环境变化鲁棒性和实时自主适用性的投影特性；实验验证了这些发现在实际地点识别策略中的相关性
- Conclusion: 精心设计的LiDAR到图像投影可以作为端到端3D学习的有效替代方案，在LiDAR地点识别中具有实际应用价值


### [98] [Glance: Accelerating Diffusion Models with 1 Sample](https://arxiv.org/abs/2512.02899)
*Zhuobai Dong,Rui Zhao,Songjie Wu,Junchao Yi,Linjie Li,Zhengyuan Yang,Lijuan Wang,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 提出一种基于LoRA适配器的智能加速方法，通过慢速和快速两个专家分别处理扩散模型的不同去噪阶段，实现5倍加速且保持视觉质量

- Motivation: 扩散模型虽然图像生成效果出色，但计算成本高、推理步骤多限制了实际部署。现有少步蒸馏方法需要大量重训练且泛化能力下降
- Method: 采用阶段感知策略：早期语义阶段应用较小加速，后期冗余阶段应用较大加速。使用两个轻量级LoRA适配器（Slow-LoRA和Fast-LoRA）作为专家，分别处理慢速和快速去噪阶段
- Result: 在基础模型上实现5倍加速，同时在多样化基准测试中保持可比较的视觉质量。LoRA专家仅需1个样本在单V100上1小时训练，就能在未见过的提示上表现出强泛化能力
- Conclusion: 通过轻量级LoRA适配器实现智能阶段感知加速，避免了传统蒸馏方法的重训练成本和泛化下降问题，为扩散模型高效部署提供了新思路


### [99] [MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding](https://arxiv.org/abs/2512.02906)
*Fan Yang,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出MRD框架解决高分辨率图像理解问题，通过多分辨率语义融合和开放词汇目标检测来避免裁剪导致的语义碎片化问题。

- Motivation: 现有方法通过裁剪图像并计算语义相似度来定位目标对象，但裁剪会将完整对象分割到多个片段中，破坏语义相似度计算的完整性，影响高分辨率图像理解效果。
- Method: 提出MRD框架：1) 多分辨率语义融合方法，整合不同分辨率下的语义相似度图，获得更准确的语义信息并保持目标对象完整性；2) 引入开放词汇目标检测模型，通过滑动窗口方法在全局尺度上直接定位目标对象区域。
- Result: 在不同多模态大语言模型上进行的高分辨率图像理解基准测试表明，该方法有效提升了性能。
- Conclusion: MRD框架无需训练即可有效解决高分辨率图像理解中的语义碎片化问题，通过多分辨率处理和开放词汇检测实现了更好的目标对象定位和语义理解。


### [100] [DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation](https://arxiv.org/abs/2512.02931)
*Ying Yang,Zhengyao Lv,Tianlin Pan,Haofan Wang,Binxin Yang,Hubery Yin,Chen Li,Chenyang Si*

Main category: cs.CV

TL;DR: DiverseAR通过自适应logits分布缩放和基于能量的生成路径搜索，解决了比特自回归模型中样本多样性不足的问题，在保持视觉质量的同时显著提升了多样性。

- Motivation: 研究比特自回归生成模型中样本多样性不足的问题。现有比特视觉分词器的自回归模型存在多样性受限，主要由于比特建模的二元分类特性限制了预测空间，以及过于尖锐的logits分布导致采样崩溃和多样性降低。
- Method: 提出DiverseAR方法：1) 自适应logits分布缩放机制，在采样过程中动态调整二元输出分布的尖锐度，实现更平滑的预测和更大的多样性；2) 基于能量的生成路径搜索算法，避免采样低置信度token，以保持高视觉质量。
- Result: 大量实验表明，DiverseAR在比特自回归图像生成中显著提升了样本多样性，同时保持了良好的视觉质量。
- Conclusion: DiverseAR通过解决比特自回归模型中限制多样性的两个关键问题，提供了一种有效提升样本多样性的方法，在保持视觉质量的同时实现了更好的生成效果。


### [101] [EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis](https://arxiv.org/abs/2512.02932)
*Yancheng Zhang,Guangyu Sun,Chen Chen*

Main category: cs.CV

TL;DR: EGGS提出了一种可交换的高斯溅射方法，通过整合2D和3D高斯表示来平衡渲染的外观质量和几何精度，解决了现有方法在纹理细节和多视图一致性之间的权衡问题。

- Motivation: 现有方法存在局限性：3D高斯溅射(3DGS)虽然能实现实时渲染和高保真外观，但存在多视图不一致问题，限制了几何精度；而2D高斯溅射(2DGS)虽然保证了多视图一致性，却牺牲了纹理细节。需要一种能平衡外观和几何质量的新方法。
- Method: 提出Exchangeable Gaussian Splatting (EGGS)，包含三个关键技术：1) 混合高斯光栅化实现统一渲染；2) 自适应类型交换机制，动态调整2D和3D高斯之间的转换；3) 频率解耦优化策略，有效利用不同类型高斯表示的优势。
- Result: 实验表明EGGS在渲染质量、几何精度和效率方面均优于现有方法，为高质量新视角合成提供了实用解决方案。CUDA加速实现确保了高效的训练和推理。
- Conclusion: EGGS通过整合2D和3D高斯表示，成功解决了新视角合成中外观保真度和几何精度之间的权衡问题，实现了高质量的实时渲染，在AR、VR和自动驾驶等领域具有广泛应用前景。


### [102] [LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization](https://arxiv.org/abs/2512.02933)
*Zhihan Xiao,Lin Liu,Yixin Gao,Xiaopeng Zhang,Haoxuan Che,Songping Mai,Qi Tian*

Main category: cs.CV

TL;DR: LoVoRA：一种无需掩码的视频对象移除与添加框架，通过可学习的对象感知定位机制实现时空一致的视频编辑

- Motivation: 现有文本引导视频编辑方法通常依赖辅助掩码或参考图像进行编辑指导，这限制了方法的可扩展性和泛化能力。特别是在对象移除和添加任务中，需要精确的空间和时间一致性，而现有方法难以满足这些要求。
- Method: 提出LoVoRA框架，包含独特的数据集构建流程（图像到视频转换、基于光流的掩码传播、视频修复）和核心创新：可学习的对象感知定位机制，为对象插入和移除任务提供密集的时空监督。通过扩散掩码预测器实现端到端视频编辑，推理时无需外部控制信号。
- Result: 广泛的实验和人工评估表明，LoVoRA在视频对象移除和添加任务上表现出有效性和高质量性能，实现了时空一致的编辑效果。
- Conclusion: LoVoRA通过创新的对象感知定位机制，成功解决了无需掩码的视频对象编辑问题，在保持时空一致性的同时提高了方法的可扩展性和泛化能力，为文本引导视频编辑提供了新的解决方案。


### [103] [Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench](https://arxiv.org/abs/2512.02942)
*Lanxiang Hu,Abhilash Shankarampeta,Yixin Huang,Zilin Dai,Haoyang Yu,Yujie Zhao,Haoqiang Kang,Daniel Zhao,Tajana Rosing,Hao Zhang*

Main category: cs.CV

TL;DR: VideoScience-Bench：首个评估视频模型科学推理能力的基准，包含200个物理化学场景，要求模型理解复合科学概念生成正确现象。

- Motivation: 现有视频基准主要基于物理常识，无法评估模型对科学定律的理解和推理能力，而这对零样本推理至关重要。
- Method: 构建包含200个提示的基准，涵盖14个主题、103个物理化学概念，使用专家标注评估7个SOTA视频模型在T2V和I2V设置下的五个维度表现。
- Result: 使用VLM-as-a-Judge评估视频生成，发现与人类评估有强相关性，揭示了当前模型在科学推理方面的局限性。
- Conclusion: VideoScience-Bench是首个将视频模型视为推理者而不仅仅是生成器的基准，为评估科学理解能力提供了新工具。


### [104] [Layout Anything: One Transformer for Universal Room Layout Estimation](https://arxiv.org/abs/2512.02952)
*Md Sohag Mia,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: Layout Anything是一个基于transformer的室内布局估计框架，采用OneFormer通用分割架构进行几何结构预测，通过布局退化策略和可微几何损失实现高效精确的布局估计。

- Motivation: 为了解决室内布局估计中复杂的后处理流程和计算效率问题，同时保持几何一致性，作者提出了一个端到端的框架，旨在消除复杂的后处理管道，实现高速推理，特别适用于增强现实和大规模3D场景重建任务。
- Method: 采用OneFormer的通用分割架构，结合任务条件查询和对比学习，并引入两个关键模块：(1) 布局退化策略，通过拓扑感知变换增强训练数据同时保持曼哈顿世界约束；(2) 可微几何损失，在训练中直接强制平面一致性和锐利边界预测。
- Result: 在标准基准测试中取得最先进性能：LSUN数据集上像素误差5.43%、角点误差4.02%；Hedau数据集上像素误差7.04%、角点误差5.17%；Matterport3D-Layout数据集上像素误差4.03%、角点误差3.15%。推理速度达到114ms。
- Conclusion: Layout Anything框架通过几何感知和计算效率的结合，成功实现了高效精确的室内布局估计，消除了复杂的后处理流程，特别适合增强现实应用和大规模3D场景重建任务。


### [105] [A Lightweight Real-Time Low-Light Enhancement Network for Embedded Automotive Vision Systems](https://arxiv.org/abs/2512.02965)
*Yuhan Chen,Yicui Shi,Guofa Li,Guangrui Bai,Jinyuan Shao,Xiangfei Huang,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 提出UltraFast-LieNET轻量级网络，用于车载摄像头实时低光图像增强，仅需36-180个参数，在LOLI-Street数据集上PSNR达26.51dB，比现有方法提升4.6dB

- Motivation: 夜间驾驶等低光环境下图像质量严重退化，影响车载摄像头安全，现有增强算法计算量过大，不适合车载实时应用
- Method: 提出动态移位卷积(DSConv)核，仅12个可学习参数；构建多尺度移位残差块(MSRB)扩展感受野；采用残差结构和多级梯度感知损失函数增强稳定性
- Result: 在LOLI-Street数据集上PSNR达到26.51dB，比现有方法提升4.6dB，仅使用180个参数；最小配置仅需36个参数；在四个基准数据集上验证了实时性能与增强质量的优越平衡
- Conclusion: UltraFast-LieNET在有限资源下实现了实时低光图像增强的优越性能，为车载安全应用提供了轻量高效的解决方案


### [106] [BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection](https://arxiv.org/abs/2512.02972)
*Guowen Zhang,Chenhang He,Liyi Chen,Lei Zhang*

Main category: cs.CV

TL;DR: BEVDilation：一种以LiDAR为中心的BEV融合框架，通过将图像特征作为隐式引导而非简单拼接，解决传感器几何精度差异导致的性能下降问题，并利用图像先验缓解点云稀疏性和语义限制。

- Motivation: 现有方法在BEV表示中不加区分地融合LiDAR和相机信息，但由于两种传感器在几何精度上的根本差异，这种简单融合往往导致性能下降。需要一种更智能的融合策略来缓解空间错位问题。
- Method: 提出BEVDilation框架：1）以LiDAR信息为中心，将图像BEV特征作为隐式引导而非简单拼接；2）稀疏体素扩张模块利用图像先验对前景体素进行稠密化；3）语义引导BEV扩张模块结合图像语义引导和长距离上下文捕获来增强LiDAR特征扩散。
- Result: 在nuScenes基准测试中，BEVDilation优于现有最先进方法，同时保持计算效率竞争力。更重要的是，该LiDAR中心策略对深度噪声表现出更强的鲁棒性。
- Conclusion: BEVDilation通过以LiDAR为中心的隐式引导融合策略，有效解决了多传感器融合中的几何精度差异问题，在提升3D检测性能的同时增强了系统鲁棒性。


### [107] [Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities](https://arxiv.org/abs/2512.02973)
*Yuan Xiong,Ziqi Miao,Lijun Li,Chen Qian,Jie Li,Jing Shao*

Main category: cs.CV

TL;DR: 提出了一种新的图像中心化攻击方法CIA，通过多智能体系统将有害查询嵌入看似良性的视觉上下文中，显著提升了针对多模态大语言模型的越狱攻击成功率。

- Motivation: 现有攻击方法主要关注文本-图像交互，将视觉模态视为次要提示，未能充分利用图像承载复杂上下文信息的独特潜力。需要探索更有效的图像中心化攻击方法。
- Method: 提出上下文图像攻击(CIA)方法，采用多智能体系统，通过四种不同的可视化策略将有害查询嵌入看似良性的视觉上下文中，并结合上下文元素增强和自动毒性混淆技术提升攻击效果。
- Result: 在MMSafetyBench-tiny数据集上，CIA对GPT-4o和Qwen2.5-VL-72B模型的毒性评分分别达到4.73和4.83，攻击成功率分别达到86.31%和91.07%，显著优于现有方法。
- Conclusion: 视觉模态本身是越狱先进多模态大语言模型的有效向量，图像中心化攻击方法能够显著提升攻击效果，揭示了当前MLLM安全对齐的脆弱性。


### [108] [InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration](https://arxiv.org/abs/2512.02981)
*Zhongyu Yang,Yingfang Yuan,Xuanming Jiang,Baoyi An,Wei Pang*

Main category: cs.CV

TL;DR: InEx：基于认知范式的免训练多智能体框架，通过内省推理和外部多模态验证自主缓解大语言模型幻觉问题

- Motivation: 现有方法依赖人工干预或未充分利用智能体自主缓解幻觉的能力。受人类真实世界决策过程启发：先通过内省推理减少不确定性形成初步判断，再通过外部多视角验证做出最终决策。
- Method: 提出免训练多智能体框架InEx：1）基于熵的不确定性估计指导内省推理，提升决策智能体可靠性；2）通过外部跨模态多智能体协作（编辑智能体和自反思智能体）迭代验证和精炼响应。
- Result: 在通用和幻觉基准测试中持续优于现有方法，获得4%-27%的性能提升，并展现出强大的鲁棒性。
- Conclusion: InEx框架通过模拟人类认知决策过程，有效自主缓解多模态大语言模型中的幻觉问题，为构建可靠AI系统提供了新思路。


### [109] [U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences](https://arxiv.org/abs/2512.02982)
*Xiang Xu,Ao Liang,Youquan Liu,Linfeng Li,Lingdong Kong,Ziwei Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: U4D：一个不确定性感知的4D LiDAR世界建模框架，通过空间不确定性估计和"难到易"生成策略，实现几何保真和时间一致的LiDAR序列生成。

- Motivation: 现有生成框架通常均匀处理所有空间区域，忽略了真实场景中不同区域的不确定性差异，导致复杂或模糊区域出现伪影，限制了真实性和时间稳定性。
- Method: 1) 使用预训练分割模型估计空间不确定性图，定位语义挑战区域；2) "难到易"两阶段生成：不确定性区域建模（高熵区域精细几何重建）和不确定性条件补全（剩余区域在结构先验下合成）；3) 引入时空混合块自适应融合时空表示。
- Result: U4D能够生成几何保真且时间一致的LiDAR序列，提升了4D世界建模在自动驾驶感知和仿真中的可靠性。
- Conclusion: U4D通过不确定性感知的生成方法和时空融合机制，有效解决了现有4D LiDAR世界建模中的均匀处理问题，提高了复杂场景下的生成质量和时间稳定性。


### [110] [GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection](https://arxiv.org/abs/2512.02991)
*Md Sohag Mia,Md Nahid Hasan,Tawhid Ahmed,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: GraphFusion3D：结合多模态融合与先进特征学习的3D目标检测统一框架，通过自适应跨模态变换器和图推理模块提升性能

- Motivation: 点云数据存在稀疏、结构不完整和语义信息有限的问题，同时捕捉远距离物体间的上下文关系也很困难，需要更好的方法来融合多模态信息并建模物体间关系
- Method: 提出GraphFusion3D框架：1) 自适应跨模态变换器(ACMT)将图像特征自适应集成到点表示中；2) 图推理模块(GRM)通过多尺度图注意力建模邻域关系；3) 级联解码器通过多阶段预测逐步优化检测结果
- Result: 在SUN RGB-D数据集上达到70.6% AP25和51.2% AP50，在ScanNetV2数据集上达到75.1% AP25和60.8% AP50，显著优于现有方法
- Conclusion: GraphFusion3D通过有效的多模态融合和图推理机制，成功解决了点云数据稀疏性和语义信息有限的问题，在3D目标检测任务上取得了显著性能提升


### [111] [TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond](https://arxiv.org/abs/2512.02993)
*Yifei Zeng,Yajie Bao,Jiachen Qian,Shuang Wu,Youtian Lin,Hao Zhu,Buyu Li,Feihu Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: TEXTRIX是一个原生3D属性生成框架，通过构建潜在3D属性网格并利用稀疏注意力扩散Transformer，直接在体积空间中为3D模型着色，避免了多视图融合的限制，实现了高保真纹理合成和精确的3D部件分割。

- Motivation: 现有3D纹理生成方法主要依赖多视图融合，但存在视图间不一致性和复杂表面覆盖不完整的问题，限制了生成内容的保真度和完整性。
- Method: 构建潜在3D属性网格，采用配备稀疏注意力的扩散Transformer，直接在体积空间中对3D模型进行着色，避免了多视图融合的限制。基于这种原生表示，框架可扩展到高精度3D分割，通过训练相同架构来预测网格上的语义属性。
- Result: 在纹理生成和3D分割任务上都达到了最先进的性能，生成了无缝、高保真的纹理和具有精确边界的准确3D部件分割。
- Conclusion: TEXTRIX通过原生3D表示方法有效解决了多视图融合的局限性，为高保真纹理合成和精确3D分割提供了统一的解决方案，在两项任务上都表现出色。


### [112] [DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling](https://arxiv.org/abs/2512.03000)
*Kairun Wen,Yuzhi Huang,Runyu Chen,Hui Zheng,Yunlong Lin,Panwang Pan,Chenxin Li,Wenyan Cong,Jian Zhang,Junbin Lu,Chenguo Lin,Dilin Wang,Zhicheng Yan,Hongyu Xu,Justin Theiss,Yue Huang,Xinghao Ding,Rakesh Ranjan,Zhiwen Fan*

Main category: cs.CV

TL;DR: DynamicVerse是一个物理尺度、多模态的4D世界建模框架，用于处理动态真实世界视频，通过大规模视觉、几何和多模态模型生成包含度量尺度静态几何、真实世界动态运动、实例级掩码和整体描述性标注的综合4D多模态数据。

- Motivation: 现有数据集通常来自有限的模拟器或使用传统的Structure-from-Motion方法，标注尺度有限且描述性标注不足，这限制了基础模型从单目视频中准确解释真实世界动态的能力。需要填补这一空白，使智能体能够像人类一样感知和理解动态物理世界。
- Method: 采用大规模视觉、几何和多模态模型来解释度量尺度静态几何、真实世界动态运动、实例级掩码和整体描述性标注。通过将基于窗口的Bundle Adjustment与全局优化相结合，将长真实世界视频序列转换为全面的4D多模态格式。
- Result: 构建了包含10万+视频、80万+标注掩码和1000万+帧的大规模数据集。在视频深度估计、相机姿态估计和相机内参估计三个基准任务上的实验评估表明，该方法在捕捉物理尺度测量方面优于现有方法，具有更高的全局准确性。
- Conclusion: DynamicVerse提供了一个物理尺度、多模态的4D世界建模框架，能够更好地理解和解释动态真实世界视频，为智能体提供更准确的环境感知能力，推动基础模型在真实世界动态理解方面的发展。


### [113] [DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images](https://arxiv.org/abs/2512.03004)
*Xiaoxue Chen,Ziyi Xiong,Yuantao Chen,Gen Li,Nan Wang,Hongcheng Luo,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Hongyang Li,Ya-Qin Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: DGGT：一个前馈式姿态自由的动态场景重建框架，通过联合预测3D高斯地图和相机参数，实现从稀疏无姿态图像直接重建，支持任意视角和长序列，在自动驾驶数据集上达到SOTA性能和速度。

- Motivation: 自动驾驶需要快速、可扩展的4D重建和重模拟用于训练评估，但现有动态驾驶场景方法依赖逐场景优化、已知相机标定或短帧窗口，导致速度慢且不实用。现有方法将相机姿态作为必需输入限制了灵活性和可扩展性。
- Method: 提出Driving Gaussian Grounded Transformer (DGGT)统一框架，将姿态重新定义为模型输出而非输入。联合预测每帧3D高斯地图和相机参数，通过轻量级动态头解耦动态，通过寿命头保持时间一致性，基于扩散的渲染细化减少运动/插值伪影。
- Result: 在Waymo、nuScenes、Argoverse2等大规模驾驶基准上，方法在单数据集训练和跨数据集零样本迁移中均优于先前工作，且随着输入帧数增加具有良好的可扩展性，实现单次前向、姿态自由的最先进性能和速度。
- Conclusion: DGGT通过重新定义姿态为输出而非输入，实现了从稀疏无姿态图像直接重建的灵活可扩展框架，为自动驾驶的动态场景重建提供了快速实用的解决方案，在性能和速度上都达到最先进水平。


### [114] [SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting](https://arxiv.org/abs/2512.03010)
*Svenja Strobel,Matthias Innmann,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.CV

TL;DR: SurfFill：一种基于高斯面元的LiDAR点云补全方法，通过结合LiDAR和相机优势，识别模糊区域并优化补全

- Motivation: LiDAR在平坦区域精度高但容易遗漏细小几何结构，而摄影测量能捕捉细节但精度不足。需要结合两者优势，利用LiDAR的准确性和相机对细节的捕捉能力来补全点云
- Method: 1) 分析LiDAR光束发散导致的伪影，提出模糊区域启发式检测方法；2) 在模糊区域约束高斯面元重建进行优化和密集化；3) 提取高斯基元并采样补全点云；4) 针对大规模重建采用分治策略
- Result: 在合成和真实场景的LiDAR点云补全任务中，该方法优于先前重建方法
- Conclusion: SurfFill成功结合了LiDAR和相机优势，通过高斯面元重建有效补全了点云中的模糊区域，特别适用于大规模场景重建


### [115] [In-Context Sync-LoRA for Portrait Video Editing](https://arxiv.org/abs/2512.03013)
*Sagi Polaczek,Or Patashnik,Ali Mahdavi-Amiri,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: Sync-LoRA：一种基于图像到视频扩散模型的肖像视频编辑方法，通过训练上下文LoRA实现高质量视觉修改，同时保持帧级同步和身份一致性。

- Motivation: 肖像视频编辑需要灵活而精确的控制各种修改（如外观变化、表情编辑、添加对象），同时保持原始时间行为，确保每一帧编辑都与源帧精确同步。
- Method: 使用图像到视频扩散模型，通过修改第一帧定义编辑并传播到整个序列。训练上下文LoRA使用成对视频（相同运动轨迹但外观不同），通过同步过滤自动生成和筛选训练数据。
- Result: 在紧凑、高度筛选的同步人类肖像数据集上训练后，Sync-LoRA能够泛化到未见身份和多样化编辑，在姿态和表情变化中保持高视觉保真度和强时间一致性。
- Conclusion: Sync-LoRA实现了编辑保真度和精确运动保持之间的稳健平衡，展示了高视觉质量和强时间连贯性。


### [116] [Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks](https://arxiv.org/abs/2512.03014)
*Matthew Dutson,Nathan Labiosa,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: 提出一种通用方法，通过插入稳定性适配器和高效训练流程，使帧级模型在视频处理中实现时间一致性和抗干扰性，同时保持预测质量。

- Motivation: 帧级网络在视频序列处理中存在时间不一致性问题（如帧间闪烁），当输入包含时变干扰时问题更加严重，需要一种通用方法来提升视频推理的稳定性和鲁棒性。
- Method: 提出稳定性适配器（可插入任何架构）和资源高效训练流程（基础网络冻结），引入统一概念框架和精度-稳定性-鲁棒性损失函数，分析其理论特性以确保稳定器训练的良好行为。
- Result: 在去噪（NAFNet）、图像增强（HDRNet）、单目深度估计（Depth Anything v2）和语义分割（DeepLabv3+）等任务上验证，方法显著提升时间稳定性和对多种图像干扰（压缩伪影、噪声、恶劣天气）的鲁棒性，同时保持或改进预测质量。
- Conclusion: 提出了一种通用有效的视频推理稳定化方法，通过理论分析和多任务实验验证，能够同时提升时间一致性、干扰鲁棒性和预测质量，适用于各种计算机视觉任务。


### [117] [AutoBrep: Autoregressive B-Rep Generation with Unified Topology and Geometry](https://arxiv.org/abs/2512.03018)
*Xiang Xu,Pradeep Kumar Jayaraman,Joseph G. Lambourne,Yilin Liu,Durvesh Malpure,Pete Meltzer*

Main category: cs.CV

TL;DR: AutoBrep：基于Transformer的自回归B-Rep生成模型，通过统一token化方案编码几何与拓扑特征，实现高质量、水密的CAD模型生成

- Motivation: B-Rep是CAD中定义实体模型的标准数据结构，但现有方法难以直接端到端生成具有精确几何和水密拓扑的B-Rep模型
- Method: 提出统一的token化方案，将几何基元编码为潜在几何token，结构关系定义为拓扑引用token；采用自回归Transformer模型，按B-Rep面邻接图的广度优先遍历顺序生成序列
- Result: AutoBrep在B-Rep生成方面优于基线方法，具有更好的质量和水密性；可扩展到复杂实体，具有良好的保真度和推理速度；支持通过统一token化实现B-Rep自动补全
- Conclusion: AutoBrep通过统一的token表示与自回归生成，实现了高质量、水密的B-Rep生成，为可控CAD生成提供了有效解决方案


### [118] [Unrolled Networks are Conditional Probability Flows in MRI Reconstruction](https://arxiv.org/abs/2512.03020)
*Kehan Qi,Saumya Gupta,Qingqiao Hu,Weimin Lyu,Chao Chen*

Main category: cs.CV

TL;DR: 该论文提出Flow-Aligned Training (FLAT)方法，通过将展开网络与条件概率流ODE联系起来，改进MRI重建的稳定性和收敛速度

- Motivation: MRI采集时间长限制了临床应用，现有展开网络重建方法存在参数自由学习导致的不稳定演化问题，而扩散模型虽然理论稳定但计算成本高
- Method: 理论证明展开网络是条件概率流ODE的离散实现，提出FLAT方法：从ODE离散化推导展开参数，并将中间重建与理想ODE轨迹对齐
- Result: 在三个MRI数据集上的实验表明，FLAT相比扩散生成模型减少3倍迭代次数，相比展开网络显著提高稳定性，实现高质量重建
- Conclusion: 通过建立展开网络与流ODE的理论联系，FLAT方法在保持计算效率的同时提高了MRI重建的稳定性和收敛性


### [119] [MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation](https://arxiv.org/abs/2512.03034)
*Youxin Pang,Jiajun Liu,Lingfeng Tan,Yong Zhang,Feng Gao,Xiang Deng,Zhuoliang Kang,Xiaoming Wei,Yebin Liu*

Main category: cs.CV

TL;DR: MAViD是一个新颖的多模态音频-视觉对话理解和生成框架，采用Conductor-Creator架构，结合自回归和扩散模型，能够生成生动、上下文连贯的长时长对话交互。

- Motivation: 现有方法主要关注非交互式系统，生成受限且不自然的人类语音。主要挑战在于有效整合理解和生成能力，以及实现无缝的多模态音频-视频融合。
- Method: 提出Conductor-Creator架构：Conductor负责理解、推理和生成指令，将其分解为动作和语音组件；Creator基于指令生成交互响应。采用自回归模型生成音频，扩散模型生成高质量视频，并提出新颖的融合模块增强连续片段和多模态之间的连接。
- Result: 大量实验表明，该框架能够生成生动且上下文连贯的长时长对话交互，并能准确解释用户的多模态查询。
- Conclusion: MAViD通过创新的架构设计解决了多模态对话理解和生成的挑战，实现了高质量、同步的音频-视觉内容生成，为交互式多模态系统提供了有效解决方案。


### [120] [ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation](https://arxiv.org/abs/2512.03036)
*Mengchen Zhang,Qi Chen,Tong Wu,Zihan Liu,Dahua Lin*

Main category: cs.CV

TL;DR: ViSAudio：首个端到端双耳空间音频生成框架，直接从静音视频生成沉浸式空间音频，解决现有方法两阶段流程导致的时空不一致问题。

- Motivation: 现有视频到音频生成方法主要关注单声道输出，缺乏空间沉浸感。现有的双耳方法采用两阶段流程（先生成单声道音频再空间化），导致错误累积和时空不一致问题。
- Method: 提出ViSAudio端到端框架，采用条件流匹配和双分支音频生成架构，两个专用分支分别建模音频潜在流。结合条件时空模块，平衡声道间一致性同时保留独特空间特征。
- Result: ViSAudio在客观指标和主观评估上均优于现有最先进方法，能生成高质量双耳音频，有效适应视角变化、声源运动和不同声学环境。
- Conclusion: 该研究首次提出端到端双耳空间音频生成任务，通过ViSAudio框架和BiAudio数据集实现了直接从视频生成沉浸式空间音频，解决了现有方法的局限性。


### [121] [Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation](https://arxiv.org/abs/2512.03040)
*Zeqi Xiao,Yiwei Zhao,Lingxiao Li,Yushi Lan,Yu Ning,Rahul Garg,Roshni Cooper,Mohammad H. Taghavi,Xingang Pan*

Main category: cs.CV

TL;DR: Video4Spatial框架证明仅使用视频数据的视频生成模型能够展现视觉空间智能，完成场景导航和物体定位等复杂空间任务

- Motivation: 探索视频生成模型是否能够仅凭视觉数据展现人类认知核心的视觉空间智能，而不依赖深度、姿态等辅助模态
- Method: 提出Video4Spatial框架，使用仅基于视频的场景上下文条件化的视频扩散模型，通过简单的框架设计和数据整理策略
- Result: 模型能够端到端规划导航和定位目标物体，遵循相机姿态指令同时保持空间一致性，并能泛化到长上下文和域外环境
- Conclusion: 这些结果将视频生成模型推向通用视觉空间推理，展示了仅从视频上下文中获得强大空间理解能力的可能性


### [122] [MultiShotMaster: A Controllable Multi-Shot Video Generation Framework](https://arxiv.org/abs/2512.03041)
*Qinghe Wang,Xiaoyu Shi,Baolu Li,Weikang Bian,Quande Liu,Huchuan Lu,Xintao Wang,Pengfei Wan,Kun Gai,Xu Jia*

Main category: cs.CV

TL;DR: MultiShotMaster：基于改进RoPE的框架，用于生成高度可控的多镜头叙事视频，支持灵活的镜头安排、时空定位和参考注入。

- Motivation: 当前视频生成技术擅长生成单镜头片段，但在生成叙事性多镜头视频时面临挑战，需要灵活的镜头安排、连贯的叙事以及超越文本提示的可控性。
- Method: 扩展预训练单镜头模型，引入两种新型RoPE变体：1）多镜头叙事RoPE，在镜头转换时应用显式相位偏移；2）时空位置感知RoPE，融入参考标记和定位信号。建立自动化数据标注流程提取多镜头视频数据。
- Result: 实验表明该框架在性能和控制能力方面表现优异，支持文本驱动的镜头间一致性、自定义主体运动控制和背景驱动的自定义场景，镜头数量和时长均可灵活配置。
- Conclusion: MultiShotMaster通过创新的RoPE变体和自动化数据标注，成功解决了多镜头视频生成的挑战，实现了高度可控的叙事视频生成。


### [123] [PPTArena: A Benchmark for Agentic PowerPoint Editing](https://arxiv.org/abs/2512.03042)
*Michael Ofengenden,Yunze Man,Ziqi Pang,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: PPTArena是一个专注于PowerPoint编辑的基准测试，包含100个演示文稿、2125张幻灯片和800多个编辑任务，使用VLM作为评判标准。PPTPilot是一个结构感知的幻灯片编辑代理，通过规划-编辑-检查循环实现精确控制，在实验中表现优于现有系统。

- Motivation: 现有PowerPoint编辑基准主要关注图像PDF渲染或文本到幻灯片生成，缺乏对真实幻灯片进行自然语言指令下的可靠编辑评估。需要建立一个全面的基准来测量PPT编辑能力，特别是针对文本、图表、表格、动画和样式等复杂编辑任务。
- Method: 1) 创建PPTArena基准：包含100个演示文稿、2125张幻灯片和800多个目标编辑任务，每个案例包含原始演示文稿、目标结果和双VLM评判管道；2) 开发PPTPilot代理：采用结构感知的幻灯片编辑方法，规划语义编辑序列，在高级程序化工具和确定性XML操作之间路由，通过迭代的规划-编辑-检查循环验证输出。
- Result: PPTPilot在复合编辑、布局敏感编辑和跨幻灯片编辑方面优于强大的专有代理和前沿VLM系统超过10个百分点，在视觉保真度和整个演示文稿一致性方面表现尤为突出。然而，现有代理在PPTArena中的长视野、文档规模任务上仍然表现不佳。
- Conclusion: PPTArena为PowerPoint编辑提供了一个全面的基准测试，PPTPilot展示了结构感知编辑方法的有效性，但可靠PPT编辑仍面临长视野、文档规模任务的挑战，需要进一步研究。


### [124] [OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043)
*Kaituo Feng,Manyuan Zhang,Hongyu Li,Kaixuan Fan,Shuang Chen,Yilei Jiang,Dian Zheng,Peiwen Sun,Yiyuan Zhang,Haoze Sun,Yan Feng,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.CV

TL;DR: OneThinker是一个统一的多模态推理模型，能够处理图像和视频理解中的多种基础视觉任务，通过EMA-GRPO方法解决多任务强化学习中的奖励异质性问题。

- Motivation: 现有方法通常为不同任务训练独立模型，将图像和视频推理视为分离领域，限制了向多模态推理通用模型的扩展性，阻碍了跨任务和跨模态的知识共享。
- Method: 构建OneThinker-600k训练语料库覆盖所有任务，使用商业模型进行CoT标注得到OneThinker-SFT-340k用于SFT冷启动，提出EMA-GRPO方法通过跟踪任务级奖励标准差移动平均来处理多任务RL中的奖励异质性。
- Result: 在31个基准测试中表现出色，涵盖10个基础视觉理解任务，展示了任务间的有效知识转移和初步的零样本泛化能力。
- Conclusion: OneThinker向统一的多模态推理通用模型迈出了一步，所有代码、模型和数据均已开源。


### [125] [CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models](https://arxiv.org/abs/2512.03045)
*Minkyung Kwon,Jinhyeok Choi,Jiho Park,Seonghu Jeon,Jinhyuk Jang,Junyoung Seo,Minseop Kwak,Jin-Hwa Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: CAMEO：通过几何对应监督注意力图来提升多视角扩散模型的训练效率和生成质量

- Motivation: 多视角扩散模型虽然能生成视角一致的图像，但其工作机制不明确。研究发现这些模型的注意力图在训练过程中获得了几何对应关系，但这种对应信号不完整，在大视角变化下准确性下降。
- Method: 提出CAMEO训练技术，直接使用几何对应关系监督注意力图。仅监督单个注意力层就足以引导模型学习精确对应关系，从而保持参考图像的几何结构。
- Result: CAMEO将收敛所需的训练迭代次数减少一半，同时在相同迭代次数下获得更优性能。该方法与模型无关，可应用于任何多视角扩散模型。
- Conclusion: 通过几何对应监督注意力图是提升多视角扩散模型训练效率和生成质量的有效方法，揭示了注意力机制在保持视角一致性中的关键作用。


### [126] [MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues](https://arxiv.org/abs/2512.03046)
*Zichen Liu,Yue Yu,Hao Ouyang,Qiuyu Wang,Shuailei Ma,Ka Leong Cheng,Wen Wang,Qingyan Bai,Yuxuan Zhang,Yanhong Zeng,Yixuan Li,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: MagicQuill V2提出分层组合范式，通过将创意意图分解为内容、空间、结构和颜色层，解决扩散模型在细粒度控制上的不足，实现更精确的图像编辑。

- Motivation: 当前扩散模型虽然语义生成能力强，但使用单一提示词无法区分用户对内容、位置和外观的不同意图，导致缺乏传统图形软件的细粒度控制能力。
- Method: 1) 将创意意图分解为可控视觉线索堆栈：内容层、空间层、结构层、颜色层；2) 专门的数据生成管道用于上下文感知内容集成；3) 统一控制模块处理所有视觉线索；4) 微调空间分支实现精确局部编辑（包括对象移除）。
- Result: 实验验证分层方法有效解决了用户意图差距，为创作者提供了直接、直观的生成过程控制。
- Conclusion: MagicQuill V2的分层组合范式成功弥合了扩散模型的语义能力与传统图形软件细粒度控制之间的差距，实现了更可控的生成式图像编辑。
## cs.CL

### [127] [Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs](https://arxiv.org/abs/2512.02719)
*Julian Ma,Jun Wang,Zafeirios Fountas*

Main category: cs.CL

TL;DR: LLMs在显式推理方面表现出色，但其隐式计算策略尚未充分探索。研究通过心理物理学范式评估LLMs是否表现出类似人类的最优贝叶斯多模态整合能力，发现准确率与稳健的贝叶斯策略之间存在分离。

- Motivation: 人类在感知任务中能够直觉地使用接近最优的贝叶斯策略处理噪声信号，但LLMs的隐式计算策略是否表现出类似的贝叶斯整合行为尚未被探索。研究者想知道LLMs是否能在没有显式训练或指令的情况下进行最优多模态整合。
- Method: 采用心理物理学范式，通过系统行为研究推断LLMs的计算原理。引入行为基准BayesBench：包含四个基于文本和图像的幅度估计任务（长度、位置、距离、持续时间），评估9个不同的LLM并与人类判断进行校准。通过控制噪声、上下文和指令提示的消融实验，测量多模态线索整合的性能、行为和效率。除了准确率和效率指标外，还引入了贝叶斯一致性分数，即使在准确率饱和时也能检测贝叶斯一致的行为变化。
- Result: 研究结果显示，虽然能力强的模型通常以贝叶斯一致的方式适应，但准确率并不能保证鲁棒性。值得注意的是，GPT-5 Mini在文本任务上达到完美准确率，但未能有效整合视觉线索。这揭示了能力与策略之间的关键分离，表明以准确率为中心的基准可能过度关注性能而忽略了脆弱的确定性处理。这些发现揭示了LLMs在处理不确定性方面出现了原则性方法，并显示了准确率与贝叶斯倾向之间的相关性。
- Conclusion: LLMs确实表现出类似人类的贝叶斯整合行为，但准确率与稳健的贝叶斯策略之间存在重要分离。研究强调了评估LLMs时需要超越简单的准确率指标，考虑其不确定性处理能力。发布的心理物理学基准和一致性指标可作为评估工具，并为未来多模态架构设计提供信息。
## cs.CR

### [128] [Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas](https://arxiv.org/abs/2512.02062)
*Issa Oe,Keiichiro Yamamura,Hiroki Ishikura,Ryo Hamahira,Katsuki Fujisawa*

Main category: cs.CR

TL;DR: 提出基于超像素的黑盒对抗攻击方法Superpixel Attack，通过超像素分割和多样化搜索策略，相比现有方法平均提升2.10%的攻击成功率

- Motivation: 深度学习模型在安全关键任务中应用广泛，但存在对抗攻击脆弱性。现有黑盒对抗攻击使用简单矩形区域进行扰动，效果有限，需要更强大的攻击方法来开发更有效的防御机制
- Method: 提出Superpixel Attack方法：1) 使用超像素代替简单矩形区域，平衡颜色方差和紧凑性；2) 提出多样化搜索策略；3) 结合超像素分割和多样化搜索进行对抗攻击
- Result: Superpixel Attack相比现有攻击方法平均提升2.10%的攻击成功率，在对抗鲁棒性较强的模型上表现显著
- Conclusion: 超像素分割和多样化搜索策略能有效提升黑盒对抗攻击的成功率，为开发更强大的防御机制提供了重要工具，代码已开源


### [129] [PhishSnap: Image-Based Phishing Detection Using Perceptual Hashing](https://arxiv.org/abs/2512.02243)
*Md Abdul Ahad Minhaz,Zannatul Zahan Meem,Md. Shohrab Hossain*

Main category: cs.CR

TL;DR: PhishSnap：基于感知哈希的隐私保护钓鱼检测系统，通过网页截图视觉相似性识别钓鱼网站，准确率79%

- Motivation: 现有基于URL和HTML的钓鱼检测系统难以应对混淆和视觉欺骗攻击，需要更有效的检测方法
- Method: 开发浏览器扩展，捕获网页截图，计算感知哈希(pHash)，与合法网站模板进行视觉相似性比对
- Result: 在10,000个URL数据集上达到0.79准确率、0.76精确率和0.78召回率，证明视觉相似性是可行的反钓鱼措施
- Conclusion: PhishSnap展示了基于视觉相似性的本地化钓鱼检测的可行性，在保护用户隐私的同时提供有效防护
## cs.RO

### [130] [VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM](https://arxiv.org/abs/2512.02293)
*Zihan Zhu,Wei Zhang,Norbert Haala,Marc Pollefeys,Daniel Barath*

Main category: cs.RO

TL;DR: VIGS-SLAM是一个视觉-惯性3D高斯溅射SLAM系统，实现了鲁棒的实时跟踪和高保真重建，通过紧密耦合视觉和惯性线索解决纯视觉方法在运动模糊、低纹理和曝光变化下的退化问题。

- Motivation: 现有的3DGS-based SLAM方法虽然能实现密集和照片级真实感的地图构建，但纯视觉设计在运动模糊、低纹理和曝光变化等挑战性场景下性能会显著下降，需要引入惯性测量单元(IMU)来增强系统的鲁棒性。
- Method: 提出一个统一的优化框架，紧密耦合视觉和惯性线索，联合优化相机位姿、深度和IMU状态。系统包含鲁棒的IMU初始化、时变偏差建模，以及带有一致高斯更新的闭环检测机制。
- Result: 在四个具有挑战性的数据集上进行实验，结果表明VIGS-SLAM在鲁棒性和重建质量方面优于现有的最先进方法。
- Conclusion: VIGS-SLAM通过视觉-惯性紧密耦合的优化框架，成功解决了纯视觉SLAM在挑战性场景下的退化问题，实现了鲁棒的实时跟踪和高保真重建，为3D高斯溅射SLAM系统提供了新的解决方案。


### [131] [SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction](https://arxiv.org/abs/2512.02609)
*Shengkai Wu,Jinrong Yang,Wenqiu Luo,Linfeng Gao,Chaohui Shang,Meiyu Zhi,Mingshan Sun,Fangping Yang,Liangliang Ren,Yong Zhao*

Main category: cs.RO

TL;DR: SAM2Grasp：利用SAM2的视觉时序跟踪能力，通过提示条件化解决多目标抓取中的多模态问题，实现单模态、无歧义的抓取轨迹预测

- Motivation: 机器人抓取模仿学习面临多模态问题：当场景中存在多个有效目标时，对不同物体的抓取演示会产生冲突的训练信号，导致标准模仿学习策略平均这些不同动作而产生无效动作
- Method: 提出SAM2Grasp框架，将任务重新表述为单模态、提示条件化的预测问题。利用冻结的SAM2模型获取强大的视觉时序跟踪能力，引入轻量级可训练的动作头与原生分割头并行工作。仅在小动作头上使用SAM2预计算的时序视觉特征进行训练
- Result: 通过大量实验证明，SAM2Grasp在杂乱多目标抓取任务中实现了最先进的性能
- Conclusion: 提出的时序提示方法有效消除了视觉运动策略中的歧义，通过初始提示指定特定抓取对象，利用SAM2的时序跟踪能力持续预测抓取轨迹，无需额外外部指导


### [132] [Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols](https://arxiv.org/abs/2512.02787)
*Xianchao Zeng,Xinyu Zhou,Youcheng Li,Jiayou Shi,Tianle Li,Liangming Chen,Lei Ren,Yong-Lu Li*

Main category: cs.RO

TL;DR: ViFailback是一个用于机器人操作失败诊断和纠正的框架，包含大规模真实世界数据集、评估基准和VLM模型，能够提供文本和视觉纠正指导。

- Motivation: 当前VLA模型在机器人操作中表现优异，但在失败诊断和从失败中学习方面存在局限；现有失败数据集大多在仿真中程序化生成，难以泛化到真实世界。
- Method: 提出ViFailback框架，使用显式视觉符号提高标注效率；发布包含58,126个VQA对和5,202个真实世界操作轨迹的数据集；建立包含11个细粒度VQA任务的评估基准；构建ViFailback-8B VLM模型。
- Result: ViFailback-8B在评估基准上取得显著性能提升，能生成视觉符号用于纠正指导；与VLA模型集成后，在真实世界机器人实验中成功帮助模型从失败中恢复。
- Conclusion: ViFailback框架有效解决了机器人操作失败诊断和纠正的问题，通过真实世界数据集、评估基准和VLM模型，显著提升了VLA模型从失败中学习和恢复的能力。
## cs.AI

### [133] [Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence](https://arxiv.org/abs/2512.02280)
*Noorbakhsh Amiri Golilarz,Sindhuja Penchala,Shahram Rahimi*

Main category: cs.AI

TL;DR: 论文指出当前AI系统缺乏自我监控、元认知、自适应学习等七项核心能力，主张向基于认知科学的AI范式转变以实现真正的自主性。

- Motivation: 尽管AI在感知、语言、推理和多模态领域取得快速进展，但现代AI系统在自我监控、自我纠正和动态环境中的自主行为调节方面存在根本性局限。这些限制阻碍了AI实现稳健泛化、终身适应性和真实世界自主性。
- Method: 通过识别和分析当代AI模型的七项核心缺陷，结合人工系统与生物认知的比较分析，整合AI研究、认知科学和神经科学的见解，论证为什么单纯扩展模型规模无法解决这些问题。
- Result: 识别出七项关键缺陷：缺乏内在自我监控、缺乏元认知意识、固定非自适应学习机制、无法重构目标、缺乏表征维护、不足的具身反馈、缺乏内在能动性。这些缺陷限制了当前架构（包括深度学习和基于Transformer的系统）的能力。
- Conclusion: 主张向基于认知科学的AI范式转变（认知自主性），开发能够自我导向适应、动态表征管理和有意图目标导向行为的系统，同时配备改革性监督机制以确保自主系统保持可解释性、可治理性和与人类价值观对齐。


### [134] [OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning](https://arxiv.org/abs/2512.02306)
*Boyu Zhu,Xiaofei Wen,Wenjie Jacky Mo,Tinghui Zhu,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: OmniGuard是首个面向全模态大语言模型的安全护栏系统，通过结构化安全标签和专家模型蒸馏，在超过210K多样化样本上训练，在15个基准测试中展现出强大的安全防护能力和泛化性。

- Motivation: 全模态大语言模型（OLLMs）能够处理文本、图像、视频和音频，这为人机交互带来了新的安全和价值观护栏挑战。现有护栏研究主要针对单模态设置，通常将安全防护视为二元分类问题，限制了在不同模态和任务间的鲁棒性。
- Method: 提出了OmniGuard，首个具有深思熟虑推理能力的全模态护栏系统。构建了包含超过210K多样化样本的大型全模态安全数据集，涵盖所有模态的单模态和跨模态样本。每个样本都标注了结构化安全标签，并通过有针对性的蒸馏从专家模型中精心策划安全批判。
- Result: 在15个基准测试上的广泛实验表明，OmniGuard在广泛的多模态安全场景中实现了强大的有效性和泛化能力。
- Conclusion: OmniGuard提供了一个统一的框架，能够在全模态中执行策略并减轻风险，为构建更强大和更稳健的全模态安全防护系统铺平了道路。


### [135] [Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective](https://arxiv.org/abs/2512.02340)
*Qiyao Xue,Weichen Liu,Shiqi Wang,Haoming Wang,Yuyang Wu,Wei Gao*

Main category: cs.AI

TL;DR: 该论文提出了ReMindView-Bench基准测试，用于评估视觉语言模型在多视角空间推理中的表现，发现现有模型在跨视角对齐和视角转换方面存在系统性失败。

- Motivation: 当前视觉语言模型在多视角空间推理中难以保持几何一致性和跨视角一致性，缺乏能够隔离多视角推理与单视角感知和时间因素的细粒度基准测试。
- Method: 提出了ReMindView-Bench基准测试，系统性地变化视角空间模式和查询类型来探测空间认知的关键因素。使用显性分阶段分析（LLM-as-a-judge和自洽性提示）和隐性分析（线性探测和熵动态）来诊断推理过程。
- Result: 评估15个当前VLMs显示，模型在跨视角对齐和视角转换方面存在一致失败。模型在帧内感知表现良好，但在跨视角信息整合时性能急剧下降。隐性分析显示任务相关信息逐渐丢失，正确与错误轨迹之间的不确定性分离。
- Conclusion: 该研究提供了对VLM空间推理的认知基础诊断，揭示了多视角空间心理模型在推理阶段如何形成、退化和失稳，为改进VLMs的空间推理能力提供了重要见解。
## cs.HC

### [136] [Real-Time Multimodal Data Collection Using Smartwatches and Its Visualization in Education](https://arxiv.org/abs/2512.02651)
*Alvaro Becerra,Pablo Villegas,Ruth Cobos*

Main category: cs.HC

TL;DR: 论文提出了两个互补工具：Watch-DMLT（用于Fitbit Sense 2智能手表的数据采集应用）和ViSeDOPS（基于仪表板的可视化系统），用于解决教育环境中多模态学习分析的数据采集和同步可视化问题。

- Motivation: 可穿戴传感器在教育领域提供了研究认知和情感过程的新机会，但缺乏可扩展、同步和高分辨率的多模态数据采集工具，阻碍了多模态学习分析在实际教育环境中的广泛应用。
- Method: 开发了两个互补工具：1) Watch-DMLT：用于Fitbit Sense 2智能手表的数据采集应用，支持实时多用户生理和运动信号监测；2) ViSeDOPS：基于仪表板的可视化系统，用于分析同步的多模态数据。在65名学生和最多16个智能手表的课堂环境中进行了部署。
- Result: 在课堂部署中成功采集了心率、运动、注视、视频和上下文注释等多种数据流。结果表明，所提出的系统在实际学习环境中支持细粒度、可扩展和可解释的多模态学习分析是可行且有效的。
- Conclusion: 该研究开发了一套完整的系统来解决多模态学习分析中的数据采集和可视化挑战，为在实际教育环境中进行可扩展、同步的高分辨率多模态数据分析提供了实用工具。
## cs.GR

### [137] [CoatFusion: Controllable Material Coating in Images](https://arxiv.org/abs/2512.02143)
*Sagie Levy,Elad Aharoni,Matan Levy,Ariel Shamir,Dani Lischinski*

Main category: cs.GR

TL;DR: 提出Material Coating新任务：在物体表面模拟薄材料涂层，保留原始几何细节。不同于现有材料替换方法，该方法添加涂层而不覆盖细节。

- Motivation: 现有"材料转移"方法会替换物体固有材料，常常覆盖精细几何细节。需要一种新方法来模拟在物体表面添加薄材料涂层，同时保留原始几何结构。
- Method: 构建大规模合成数据集DataCoat110K（11万张图像），包含带物理涂层变化的3D物体。提出CoatFusion架构，基于扩散模型，同时以2D反照率纹理和PBR风格参数控制（粗糙度、金属度、透射率、厚度）为条件。
- Result: 实验和用户研究表明，CoatFusion能生成逼真、可控的涂层效果，在新任务上显著优于现有材料编辑和转移方法。
- Conclusion: Material Coating是一个新颖的图像编辑任务，CoatFusion方法通过结合纹理和物理参数控制，成功实现了在保留物体几何细节的同时添加材料涂层的效果。


### [138] [SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control](https://arxiv.org/abs/2512.03028)
*Yuxuan Mu,Ziyu Zhang,Yi Shi,Minami Matsumoto,Kotaro Imamura,Guy Tevet,Chuan Guo,Michael Taylor,Chang Shu,Pengcheng Xi,Xue Bin Peng*

Main category: cs.GR

TL;DR: 提出Score-Matching Motion Priors (SMP)，利用预训练运动扩散模型和分数蒸馏采样创建可重用、任务无关的运动先验，避免对抗模仿学习中需要为每个新控制器重新训练的问题。

- Motivation: 对抗模仿学习虽然能有效从参考运动数据学习运动先验，但通常需要为每个新控制器重新训练，限制了重用性，且在下游任务训练时需要保留原始参考数据。
- Method: SMP基于预训练运动扩散模型和分数蒸馏采样，可独立于控制策略或任务在运动数据集上预训练。训练后保持冻结，作为通用奖励函数训练下游任务的策略。
- Result: SMP能在大规模数据集上训练通用运动先验，可转化为多种风格特定先验，并能组合不同风格合成原始数据集中不存在的新风格，生成的运动质量与最先进对抗模仿学习方法相当。
- Conclusion: SMP通过可重用和模块化的运动先验，在物理模拟人形角色的多样化控制任务中表现出色，解决了对抗模仿学习中先验重用性的限制问题。
## eess.IV

### [139] [Comparing Baseline and Day-1 Diffusion MRI Using Multimodal Deep Embeddings for Stroke Outcome Prediction](https://arxiv.org/abs/2512.02088)
*Sina Raeisadigh,Myles Joshua Toledo Tan,Henning Müller,Abderrahmane Hedjoudje*

Main category: eess.IV

TL;DR: 比较急性缺血性卒中患者基线(J0)和24小时(J1)扩散MRI对3个月功能预后的预测价值，发现J1多模态模型性能最优

- Motivation: 探索急性缺血性卒中后早期扩散MRI（特别是治疗后24小时）是否比基线MRI能更好地预测患者3个月的功能预后，以改善临床预后评估
- Method: 纳入74例急性缺血性卒中患者，配对分析ADC扫描和临床数据；使用3D ResNet-50提取影像特征，与结构化临床变量融合；通过主成分分析降维（≤12个成分）；采用线性支持向量机分类，使用8折分层分组交叉验证
- Result: J1多模态模型预测性能最高（AUC = 0.923 ± 0.085），优于J0配置（AUC ≤ 0.86）；加入病灶体积特征进一步提高了模型稳定性和可解释性
- Conclusion: 治疗后早期扩散MRI比治疗前影像具有更好的预后价值；结合MRI、临床和病灶体积特征可构建稳健且可解释的急性缺血性卒中患者3个月功能预后预测框架
## cs.LG

### [140] [Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models](https://arxiv.org/abs/2512.02636)
*Xinyue Ai,Yutong He,Albert Gu,Ruslan Salakhutdinov,J Zico Kolter,Nicholas Matthew Boffi,Max Simchowitz*

Main category: cs.LG

TL;DR: F2D2是一个同时加速流模型采样和似然评估的蒸馏框架，将所需神经函数评估次数减少两个数量级，通过共享速度场联合蒸馏采样轨迹和累积散度。

- Motivation: 当前最好的生成模型（扩散和流模型）需要数百到数千次神经函数评估来计算单个似然值，这成为计算瓶颈。虽然现有蒸馏方法可以加速采样，但要么完全放弃似然计算，要么仍需要昂贵的全轨迹积分。
- Method: 提出快速流联合蒸馏（F2D2）框架，利用连续归一化流中采样和似然的耦合ODE共享底层速度场的特点，通过单个模型联合蒸馏采样轨迹和累积散度，仅需添加散度预测头。
- Result: F2D2能够在少量步骤评估下实现准确的log-likelihood，同时保持高样本质量。应用该方法，仅需2步的MeanFlow模型就能超越1024步教师模型，仅需额外一次反向NFE。
- Conclusion: F2D2解决了流基生成模型中长期存在的计算瓶颈，实现了采样和似然评估的同时加速，为下游应用提供了高效的似然计算能力。


### [141] [Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation](https://arxiv.org/abs/2512.02920)
*Ziniu Zhang,Minxuan Duan,Haris N. Koutsopoulos,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 该研究构建了一个包含900万起交通事故记录和100万张高分辨率卫星图像的多模态数据集，通过整合视觉和网络嵌入，将事故预测的AUROC提升至90.1%，比仅使用图结构的模型提高了3.7%。

- Motivation: 现有交通事故预测研究主要依赖道路网络结构特征，忽略了道路表面及其周围环境的物理和环境信息。为了更全面地分析事故模式，需要整合多模态数据。
- Method: 构建了一个覆盖美国六个州的大型多模态数据集，包含900万起交通事故记录、100万张道路网络节点对应的高分辨率卫星图像，以及天气统计、道路类型、交通流量等特征。使用多模态学习方法整合视觉和网络嵌入进行预测。
- Result: 多模态整合将预测准确率提升至平均AUROC 90.1%，比仅使用图神经网络的模型提高了3.7%。因果分析显示：降水量增加导致事故率上升24%，高速公路等高速度道路事故率上升22%，季节性模式导致事故率上升29%。
- Conclusion: 整合卫星图像和道路网络数据的多模态学习方法显著提高了交通事故预测准确性。卫星图像特征对准确预测至关重要，且多模态数据支持对事故关键影响因素进行因果分析。
