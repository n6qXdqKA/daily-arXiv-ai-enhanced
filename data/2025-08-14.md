[[toc]]

## cs.CV

### [1] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出了一种多模态框架，用于检测针对女性的冒犯性内容，包括三个模块：多模态注意力模块、基于图的特征重构模块和内容特定特征学习模块。在MAMI和MMHS150K数据集上表现优于现有方法。

- Motivation: 社交媒体上针对女性的冒犯性内容较多，现有方法难以有效检测，需要针对性解决方案。
- Method: 提出多模态框架，结合注意力机制、图特征重构和内容特定特征学习，并利用词典计算特定分数。
- Result: 在MAMI和MMHS150K数据集上，平均宏F1分别提高了10.17%和8.88%。
- Conclusion: 该方法有效提升了针对女性的冒犯性内容检测性能。


### [2] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: IAD-R1是一个通用的后训练框架，显著提升视觉语言模型（VLMs）在工业异常检测中的性能，通过两阶段训练策略实现从异常感知到异常解释的能力飞跃。

- Motivation: 工业异常检测中缺陷样本稀缺，传统方法泛化能力有限，VLMs虽有潜力但性能不足。
- Method: 采用两阶段训练策略：PA-SFT阶段使用高质量Chain-of-Thought数据集增强异常感知；SC-GRPO阶段通过奖励函数实现能力飞跃。
- Result: 在7种VLMs上平均准确率提升43.3%，0.5B参数模型在零样本设置下超越GPT-4.1等商业模型。
- Conclusion: IAD-R1框架有效且优越，数据集和代码将开源。


### [3] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: CADAR是一种新型的神经符号方法，用于检测AR中的认知攻击，结合了视觉语言模型和粒子滤波技术，显著提升了检测准确性和可解释性。

- Motivation: 现有方法在检测AR认知攻击时，要么缺乏语义推理能力，要么依赖黑盒模型，可解释性不足。
- Method: CADAR通过融合多模态视觉语言输入生成符号感知图，并利用粒子滤波进行统计推理。
- Result: 在扩展的AR认知攻击数据集上，CADAR的准确率比基线方法提高了10.7%。
- Conclusion: 神经符号方法在有效且可解释的认知攻击检测中具有潜力。


### [4] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: 论文提出RL-MoE框架，将敏感视觉数据转化为隐私保护文本描述，解决智能交通系统中隐私与数据效用之间的冲突。

- Motivation: 智能交通系统中AI摄像头的普及导致隐私与数据需求之间的冲突，现有隐私保护方法（如模糊或加密）效果不足。
- Method: 结合Mixture-of-Experts（MoE）架构和强化学习（RL）代理，将视觉数据转化为隐私保护文本描述。
- Result: RL-MoE在CFP-FP数据集上成功抵御重放攻击（成功率仅9.4%），同时生成比基线方法更丰富的文本内容。
- Conclusion: RL-MoE为隐私敏感领域提供实用且可扩展的解决方案，推动更安全的智能城市和自动驾驶网络发展。


### [5] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: 提出了一种基于优化GAN和知识蒸馏的合成深度人脸生成框架，结合遗传算法提升多样性和质量，在情感分类任务中表现优异。

- Motivation: 解决情感计算中高质量、多样化深度面部数据集的缺乏问题。
- Method: 使用优化GAN和知识蒸馏（EMA教师模型）稳定训练，结合遗传算法优化潜在向量，提取多种特征进行分类。
- Result: 在多样性和质量上优于GAN、VAE、GMM和KDE，分类准确率达94%和96%。
- Conclusion: 该方法在生成质量和分类性能上均优于现有技术。


### [6] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种名为Δ-AttnMask的数据高效框架，通过注意力引导的隐藏状态掩码量化样本质量，显著减少数据需求并提升性能。

- Motivation: 视觉指令微调（VIF）需要多模态数据以支持视觉和文本的联合理解，但数据选择问题尚未充分研究。
- Method: Δ-AttnMask通过计算原始状态与高注意力区域掩码状态之间的损失差异（Δ）来评估样本质量，无需额外标签或训练。
- Result: 实验表明，Δ-AttnMask仅需20%数据即可实现最佳性能，训练速度提升5倍，准确率超过基线10.1%。
- Conclusion: Δ-AttnMask的模型无关和数据无关设计使其在多模态和架构中具有广泛适用性。


### [7] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的个性化特征翻译（PFT）方法，用于源自由域适应（SFDA），以解决面部表情识别（FER）模型在仅含中性表情的无标签目标数据上的适应问题。

- Motivation: 由于现有SFDA方法通常不适用于仅含单类目标数据的情况，且基于图像生成的方法不稳定且计算量大，本文旨在提出一种更高效的方法。
- Method: 通过预训练翻译器在源域数据上转换主体特定风格特征，并在中性目标数据上适应翻译器，无需源数据或图像合成。
- Result: PFT方法避免了面部表情生成的复杂性，生成了优化的分类嵌入，同时减少了计算开销。
- Conclusion: PFT是一种高效且轻量级的SFDA方法，适用于仅含中性表情的目标数据场景。


### [8] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: 研究比较了多种图像转换模型（如Neural Style Transfer、C-GAN和CycleGAN）在动漫角色与草图之间的转换效果，发现C-GAN能生成最接近人工绘制的高质量图像。

- Motivation: 解决动漫行业中从草图生成全彩绘图的耗时高成本问题。
- Method: 评估了Neural Style Transfer、C-GAN和CycleGAN等模型的图像转换能力。
- Result: C-GAN表现最佳，能生成高质量、高分辨率的图像。
- Conclusion: C-GAN是动漫行业草图转彩绘的高效解决方案。


### [9] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 论文介绍了MME-Emotion基准，用于评估多模态大语言模型（MLLMs）的情感知觉和推理能力，揭示了当前模型的局限性。

- Motivation: 当前情感计算领域缺乏全面评估MLLMs泛化能力和情感推理能力的基准。
- Method: 提出MME-Emotion基准，包含6,000多个视频片段和任务特定的问答对，涵盖八种情感任务，并采用混合指标和多代理系统框架进行评估。
- Result: 评估20个先进MLLMs显示，其情感知觉和推理能力不足，最佳模型识别得分仅为39.3%，推理得分为56.0%。
- Conclusion: MME-Emotion为未来提升MLLMs情感智能提供了基础。


### [10] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种四轴评估框架来更准确地衡量多模态大语言模型（MLLMs）的对抗性攻击效果，并开发了一种递归重写策略（BSD）以提高攻击成功率。

- Motivation: 现有评估标准可能高估对抗性攻击的有效性，且安全机制未能有效阻止有害输出。
- Method: 引入四轴评估框架（输入相关性、输入分布外强度、输出有害性、输出拒绝率），并开发BSD策略以优化对抗性提示。
- Result: BSD在13个MLLMs中显著提高了攻击成功率和有害性输出，同时减少了拒绝率。
- Conclusion: BSD揭示了当前多模态安全系统的潜在弱点，为未来安全改进提供了方向。


### [11] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: 论文提出了一种结合手写公式与LaTeX渲染公式的新方法，构建了最大的公式数据集Tex80M，并开发了首个大规模训练的HMER模型TexTeller，实现了SOTA性能。

- Motivation: 手写数学表达式识别（HMER）领域因数据稀缺而受限，主要原因是人工标注成本高昂。
- Method: 开发了一个可扩展的数据引擎，结合有限的手写公式和大规模LaTeX渲染公式，构建了Tex80M数据集，并提出了TexTeller模型进行混合训练。
- Result: TexTeller在几乎所有基准测试中实现了SOTA性能。
- Conclusion: 论文将公开模型、数据集和代码，以推动HMER领域的进一步研究。


### [12] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: GDAGS提出了一种基于梯度方向感知的自适应密度控制框架，解决了3D高斯泼溅中的过重建和过密集问题，显著提升了渲染质量并减少了内存消耗。

- Motivation: 现有3D高斯泼溅方法在复杂场景中存在过重建和过密集问题，导致内存浪费和渲染质量下降。
- Method: GDAGS通过梯度一致性比（GCR）和非线性动态加权机制，实现了梯度方向感知的密度控制，优化高斯分布。
- Result: GDAGS在多种真实场景基准测试中表现出色，渲染质量提升，内存消耗减少50%。
- Conclusion: GDAGS有效解决了3D高斯泼溅的关键问题，为实时逼真渲染提供了高效解决方案。


### [13] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

TL;DR: FineState-Bench是一个新的评估标准，专注于GUI代理的细粒度控制能力，填补了现有基准的不足。

- Motivation: 现有GUI代理评估框架过于关注粗粒度任务完成，忽略了细粒度控制能力的重要性。
- Method: 开发了FineState-Bench，一个多平台框架，包含2257个任务基准，并引入四阶段指标和视觉诊断助手（VDA）进行定量分析。
- Result: 实验显示当前最先进模型的细粒度交互准确率仅为32.8%，理想视觉定位可将成功率提升14.9%。
- Conclusion: 当前GUI代理的主要瓶颈是基础视觉定位能力，FineState-Bench为未来研究提供了开源工具和诊断框架。


### [14] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

TL;DR: FiGPriv是一种细粒度隐私保护框架，通过选择性屏蔽高风险隐私信息，提升视觉语言模型的可用性。

- Motivation: 针对视觉助手系统中盲人和低视力用户可能无意中泄露隐私的问题，现有方法因粗粒度分割导致可用性降低。
- Method: 结合细粒度分割与数据驱动的风险评分机制。
- Result: 在BIV-Priv-Seg数据集上，FiGPriv保留26%的图像内容，提升VLM响应能力11%和内容识别45%。
- Conclusion: FiGPriv在保护隐私的同时显著提升了系统的可用性和功能性。


### [15] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 提出了一种新型输入自适应导航方法，通过三种算法提升视觉与语言导航（VLN）模型的效率，显著减少计算量。

- Motivation: 现有输入自适应机制在减少计算量时会导致性能显著下降，因此需要更高效的方法。
- Method: 1. 选择性处理全景视图；2. 基于重要性的自适应阈值提前退出；3. 缓存机制避免重复处理。
- Result: 在七个VLN基准测试中，计算量减少超过2倍，且性能未显著下降。
- Conclusion: 提出的方法有效提升了VLN模型的效率，适用于计算资源有限的实际场景。


### [16] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: SegDAC是一种基于分割的视觉强化学习方法，通过结合SAM和YOLO-World实现对象中心分解和语义标注，显著提升了视觉泛化能力和样本效率。

- Motivation: 视觉强化学习面临高维输入和噪声奖励的挑战，现有感知模型与RL的结合效果不明，需改进视觉泛化和样本效率。
- Method: SegDAC利用SAM进行对象中心分解，YOLO-World通过文本提示语义标注，采用动态Transformer架构，无需人工标注。
- Result: 在Maniskill3基准测试中，SegDAC视觉泛化能力显著提升，最难任务性能翻倍，样本效率优于或匹配现有方法。
- Conclusion: SegDAC通过对象中心分割和语义标注，有效解决了视觉强化学习的泛化和效率问题，性能显著优于现有方法。


### [17] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: 论文提出了一种改进的生成模型Lung-DDPM+，用于高效生成高保真度的肺部CT图像，解决了现有模型效率低和解剖学不精确的问题。

- Motivation: 现有生成模型在肺部癌症诊断中存在效率低和解剖学不精确的局限性，限制了其临床应用。
- Method: 提出Lung-DDPM+，一种基于结节语义布局的去噪扩散概率模型（DDPM），并通过肺部DPM-solver加速，专注于病变区域。
- Result: 在LIDC-IDRI数据集上，Lung-DDPM+实现了8倍更少的FLOPs、6.8倍更低的GPU内存消耗和14倍更快的采样速度，同时保持与SOTA模型相当的样本质量。
- Conclusion: Lung-DDPM+能高效生成高质量的肺部CT图像，具有广泛的临床应用潜力。


### [18] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 论文提出了一种基于状态空间模型（SSM）的Ultralight Med-Vision Mamba，用于结肠镜检查中癌前息肉的精确分类和分层，提升风险评估准确性并优化个性化监测方案。

- Motivation: 通过深度学习算法提高结肠镜检查中癌前息肉的识别精度，降低结直肠癌风险。
- Method: 采用Ultralight Med-Vision Mamba模型，利用其长短期依赖建模和图像泛化能力分析全切片图像。
- Result: 该模型在计算速度和可扩展性上表现优异，适合实时临床部署。
- Conclusion: Ultralight Med-Vision Mamba是一种高效工具，有望在临床中实现癌前息肉的精准识别和个性化监测。


### [19] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

TL;DR: 提出一种实时系统，将眨眼动作转换为摩尔斯电码，帮助严重运动障碍者进行交流。

- Motivation: 为严重运动障碍者提供低成本、易用的辅助交流工具。
- Method: 使用标准摄像头和计算机视觉技术检测眨眼动作，将其分类为短（点）或长（划），并解码为字母数字字符。
- Result: 实验显示解码准确率为62%，响应时间为18-20秒。
- Conclusion: 该系统是一种可行且低成本的辅助交流方法。


### [20] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了一种名为FusionEnsemble-Net的新型注意力集成时空网络，通过动态融合视觉和运动数据提升手语识别准确率，在意大利手语数据集上达到99.44%的测试准确率。

- Motivation: 医疗沟通中手语识别的准确性是一个重要挑战，需要能解释复杂多模态手势的框架。
- Method: 采用四种时空网络同步处理RGB视频和雷达数据，通过注意力融合模块动态融合特征，最终通过集成分类器结合输出。
- Result: 在MultiMeDaLIS数据集上，FusionEnsemble-Net以99.44%的准确率优于现有方法。
- Conclusion: 注意力融合的多样化时空网络集成能够为复杂多模态手势识别提供鲁棒且准确的框架。


### [21] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 论文提出了一种双架构框架，分别针对手语识别中的独立用户和未见句子问题，通过新型网络结构显著提升了性能。

- Motivation: 解决连续手语识别中用户差异大和句子结构泛化能力差的问题。
- Method: 使用Signer-Invariant Conformer处理用户差异，Multi-Scale Fusion Transformer处理未见句子问题。
- Result: 在Isharah-1000数据集上，SI任务WER降低13.53%，US任务WER达47.78%。
- Conclusion: 任务专用网络设计显著提升性能，为后续研究设定了新基准。


### [22] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: 论文研究了医学图像分割中标注者间差异与皮肤病变恶性程度的关系，提出了一种利用标注一致性作为软特征的多任务学习方法，显著提升了分割性能。

- Motivation: 医学图像分割中存在标注者间差异，尤其是边界模糊的病变（如恶性皮肤病变），本研究旨在探索这种差异与恶性程度的关系，并利用其提升分割模型性能。
- Method: 研究者构建了IMA++数据集，分析了标注者、恶性程度、工具和技能对标注一致性的影响，并提出了一种基于多任务学习的方法，利用标注一致性作为软特征。
- Result: 研究发现标注一致性（IAA）与恶性程度显著相关（p<0.001），并证明IAA可从图像中预测（MAE=0.108）。多任务学习方法在多个数据集上平均提升了4.2%的平衡准确率。
- Conclusion: 标注一致性可作为临床特征提升分割模型性能，IMA++数据集为相关研究提供了重要资源。


### [23] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: X-UniMotion提出了一种统一的隐式潜在表示方法，用于全身人体运动（包括面部表情、身体姿势和手势），通过自监督框架实现高保真跨身份运动转移。

- Motivation: 现有方法依赖显式骨骼姿势和启发式跨身份调整，无法实现高保真、详细的跨身份运动转移。
- Method: 采用自监督端到端框架，学习运动编码器和潜在表示，结合DiT视频生成模型，并通过空间、颜色增强和3D渲染实现运动-身份解耦。
- Result: X-UniMotion在实验中表现优于现有方法，生成高表达性动画，运动保真度和身份保留效果显著。
- Conclusion: X-UniMotion为全身运动提供了一种高效、统一的潜在表示方法，具有广泛的应用潜力。


### [24] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: DenoDet V2提出了一种新的变换域特征解构与调制方法，通过精心设计的注意力架构，利用振幅和相位信息的互补性，显著提升了SAR目标检测性能。

- Motivation: 解决SAR目标检测中相干噪声的普遍影响，现有方法多基于空间域特征分析或增强，DenoDet V2探索了变换域的新视角。
- Method: 通过波段互调制机制，利用振幅和相位信息的互补性，实现相位与振幅谱的相互增强。
- Result: 在多个SAR数据集上表现优异，SARDet-100K数据集上比DenoDet V1提升0.8%，模型复杂度减半。
- Conclusion: DenoDet V2通过变换域特征调制，显著提升了SAR目标检测性能，同时降低了模型复杂度。


### [25] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: SkyShield是一个事件驱动的端到端框架，用于检测无人机在复杂环境中遇到的亚毫米级障碍物，如钢丝和风筝线。

- Motivation: 传统传感器（如RGB摄像头、LiDAR和深度相机）难以检测亚毫米级障碍物，这对无人机操作构成重大威胁。
- Method: 采用轻量级U-Net架构和创新的Dice-Contour正则化损失，利用事件流中薄障碍物的独特特征进行精确检测。
- Result: 实验结果显示，该方法平均F1分数为0.7088，延迟低至21.2毫秒，适合边缘和移动平台部署。
- Conclusion: SkyShield为无人机在复杂环境中的安全操作提供了高效、低延迟的解决方案。


### [26] [Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring](https://arxiv.org/abs/2508.09398)
*El Mustapha Mansouri*

Main category: cs.CV

TL;DR: 低成本、本地化的比利时城市花园鸟类监测系统，使用运动触发摄像头和本地服务器进行鸟类检测与分类，无需云服务或独立GPU。

- Motivation: 为公民科学提供低成本、隐私保护的鸟类监测方案，避免云费用和隐私问题。
- Method: 使用运动触发IP摄像头上传视频至本地服务器，Detectron2检测鸟类，EfficientNet-B3分类40种比利时鸟类。
- Result: 分类器在验证集上达到99.5%准确率，实际应用中为88%，适用于家庭生物多样性记录。
- Conclusion: 该系统证明了低成本本地化鸟类监测的可行性，适合公民科学项目。


### [27] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: Waymo-3DSkelMo是一个大规模高质量3D运动数据集，专注于多人交互，用于自动驾驶中细粒度行人交互理解。

- Motivation: 现有数据集依赖单目RGB视频帧估计3D姿态，存在遮挡和时间不连续问题，导致运动质量低。
- Method: 利用3D人体形状和运动先验，从LiDAR点云提取高质量3D姿态序列。
- Result: 数据集覆盖800多个真实驾驶场景，包含丰富的交互行为，并建立了3D姿态预测基准。
- Conclusion: 该数据集为复杂城市环境中细粒度人类行为理解提供了基础资源。


### [28] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: 论文提出了一种名为RampNet的两阶段管道，用于扩展路缘坡道检测数据集并提升模型性能，通过自动标注和政府数据生成大规模高质量数据集，并训练出性能优越的检测模型。

- Motivation: 路缘坡道对城市无障碍至关重要，但现有数据集在质量和规模上不足，限制了检测模型的性能。
- Method: 采用两阶段方法：1) 利用政府数据自动标注21万张Google街景全景图；2) 基于生成的数据集训练改进的ConvNeXt V2模型。
- Result: 生成的数据集精度达94.0%，召回率92.5%；检测模型AP为0.9236，远超现有工作。
- Conclusion: 研究贡献了首个大规模高质量路缘坡道检测数据集、基准和模型，显著提升了检测性能。


### [29] [Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation](https://arxiv.org/abs/2508.09423)
*Badi Li,Ren-jie Lu,Yu Zhou,Jingke Meng,Wei-shi Zheng*

Main category: cs.CV

TL;DR: GOAL是一种基于生成流的框架，通过结合LLM增强的全场景语义地图，建模室内环境的语义分布，显著提升了ObjectNav任务的性能。

- Motivation: 现有方法依赖确定性模型完成语义地图，忽视了室内布局的不确定性，限制了泛化能力。GOAL旨在解决这一问题。
- Method: 利用LLM推断空间先验，编码为二维高斯场并注入目标地图，将丰富的上下文知识融入流模型。
- Result: 在MP3D和Gibson上达到SOTA性能，并在HM3D上表现出强泛化能力。
- Conclusion: GOAL通过生成流模型和LLM先验的结合，显著提升了ObjectNav任务的泛化性和性能。


### [30] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新视觉任务，同时预测高级动作语义和细粒度身体接触区域，并提出了PaIR-Net框架和PaIR数据集。

- Motivation: 现有方法未能同时建模动作语义和空间上下文，需填补这一空白。
- Method: PaIR-Net框架包含三个模块：CPAM识别接触相关身体部位，PGCS进行像素级接触分割，IIM整合全局交互关系。
- Result: 实验表明PaIR-Net显著优于基线方法，消融研究验证了各模块的有效性。
- Conclusion: PaIR-Net和PaIR数据集为动作理解和空间上下文建模提供了新思路。


### [31] [MPT: Motion Prompt Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2508.09446)
*Jiateng Liu,Hengcan Shi,Feng Chen,Zhiwen Shao,Yaonan Wang,Jianfei Cai,Wenming Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Motion Prompt Tuning（MPT）的新方法，通过微调大型预训练模型（LMs）来改进微表情识别（MER），解决了现有方法难以捕捉细微面部动作的问题。

- Motivation: 微表情识别在医疗诊断、测谎等领域有广泛应用，但由于标注困难，数据集样本稀缺，限制了模型学习。现有大型预训练模型无法直接用于MER，因其难以捕捉短暂且细微的面部动作。
- Method: 提出MPT方法，包括运动提示生成（运动放大和高斯标记化）和组适配器设计，以增强LMs在MER领域的表现。
- Result: 在三个广泛使用的MER数据集上的实验表明，MPT方法优于现有最先进方法。
- Conclusion: MPT是一种有效的MER方法，能够显著提升模型性能，为微表情识别领域提供了新的解决方案。


### [32] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的参考超分辨率方法RASR，通过自动检索相关高分辨率参考图像，解决了现有方法依赖手动配对的局限性，并构建了首个相关数据集RASR-Flickr30。

- Motivation: 现有参考超分辨率方法依赖手动配对的图像，限制了实际应用。
- Method: 提出RASR框架，结合语义检索器和扩散模型生成器，自动检索并利用相关参考图像。
- Result: 实验显示RASRNet在PSNR和LPIPS指标上优于传统方法，生成更真实的纹理。
- Conclusion: 检索增强是连接学术研究与实际应用的有效方向。


### [33] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: HyperKD是一种新颖的知识蒸馏框架，通过逆向知识转移解决高光谱遥感中基础模型应用的挑战。

- Motivation: 基础模型在高光谱遥感中直接应用存在光谱差异和数据稀缺的问题。
- Method: HyperKD采用特征对齐、空间特征掩码和增强损失函数，从简单教师模型向学生模型转移知识。
- Result: 实验表明HyperKD显著提升了表示学习能力，改善了重建保真度和下游任务性能。
- Conclusion: HyperKD展示了知识蒸馏在高光谱遥感分析中的潜力。


### [34] [Animate-X++: Universal Character Image Animation with Dynamic Backgrounds](https://arxiv.org/abs/2508.09454)
*Shuai Tan,Biao Gong,Zhuoxin Liu,Yan Wang,Xi Chen,Yifan Feng,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Animate-X++ 是一个基于 DiT 的通用动画框架，适用于包括拟人化角色在内的多种角色类型，解决了现有方法在拟人化角色和动态背景上的局限性。

- Motivation: 现有方法主要适用于人类角色，且只能生成静态背景视频，限制了其在游戏和娱乐等行业中的实用性。
- Method: 提出 Pose Indicator 增强运动表示，结合 CLIP 视觉特征和模拟输入；采用多任务训练策略联合训练动画和 TI2V 任务。
- Result: Animate-X++ 在拟人化角色和动态背景生成上表现优异，并通过 A2Bench 基准验证了其有效性。
- Conclusion: Animate-X++ 是一个通用且高效的动画框架，显著提升了角色动画的逼真度和适用性。


### [35] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型输入感知后门攻击方法IAG，用于操纵视觉语言模型（VLMs）的视觉定位行为，使其忽略用户查询而定位特定目标对象。

- Motivation: 尽管VLMs在视觉定位任务中表现优异，但其在安全方面的研究不足，尤其是后门攻击。本文旨在填补这一空白。
- Method: 提出自适应触发器生成器，利用文本条件U-Net将攻击目标的语义信息嵌入原始图像，并通过重构损失确保攻击隐蔽性。
- Result: IAG在InternVL-2.5-8B上的ASR@0.5超过65%，且在Ferret-7B和LlaVA-1.5-7B上表现出色，对干净样本的准确性影响极小。
- Conclusion: IAG展示了强大的攻击效果和可转移性，为VLMs的安全研究提供了重要参考。


### [36] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: RelayFormer是一个统一的模块化架构，用于图像和视频中的视觉篡改定位（VML），通过灵活的局部单元和全局-局部中继注意力机制（GLoRA）实现高效处理。

- Motivation: 现有方法缺乏跨模态泛化能力，且难以高效处理高分辨率或长时间输入。
- Method: 采用灵活的局部单元和GLoRA机制，结合轻量级适配模块与现有Transformer架构（如ViT和SegFormer）无缝集成。
- Result: 在多个基准测试中达到最先进的定位性能，为可扩展和模态无关的VML设定了新基准。
- Conclusion: RelayFormer通过模块化设计和高效注意力机制，显著提升了VML任务的性能和泛化能力。


### [37] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: GEN-AFFECT是一个新框架，用于生成个性化且表情丰富的2D虚拟形象，解决了现有方法在细粒度表情捕捉和身份一致性上的不足。

- Motivation: 现有方法在生成虚拟形象时难以捕捉细粒度表情并保持身份一致性，限制了应用效果。
- Method: 通过多模态扩散变换器提取身份-表情表示，并采用一致性注意力机制在推理过程中共享信息。
- Result: GEN-AFFECT在生成表情准确性、身份保持及一致性上优于现有方法。
- Conclusion: GEN-AFFECT为虚拟形象生成提供了更高效且一致的解决方案。


### [38] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经形态计算范式的新型脉冲神经网络，用于在Intel Loihi 2硬件上实现高效能的鲁棒拟合，能耗仅为传统CPU方法的15%。

- Motivation: 鲁棒拟合在计算机视觉中至关重要，但现有方法忽略了能源效率问题。随着AI能耗问题日益突出，研究能源高效的鲁棒拟合方法变得尤为重要。
- Method: 设计了新型脉冲神经网络，并提出了事件驱动的模型估计方法，以适应Loihi 2硬件的独特架构，同时采用算法策略解决硬件精度和指令集的限制。
- Result: 实验结果表明，该神经形态鲁棒拟合方法的能耗仅为传统CPU方法的15%，且达到相同精度。
- Conclusion: 神经形态计算为鲁棒拟合提供了能源高效的解决方案，展示了其在AI应用中的潜力。


### [39] [CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios](https://arxiv.org/abs/2508.09470)
*Jialei Xu,Zizhuang Wei,Weikang You,Linyun Li,Weijian Sun*

Main category: cs.CV

TL;DR: CitySeg是一个用于城市规模点云语义分割的基础模型，通过引入文本模态实现开放词汇分割和零样本推理，解决了数据分布不均和语义标签差异问题。

- Motivation: 现有模型受限于3D数据规模小和数据集间的领域差距，导致泛化能力不足。
- Method: 提出局部-全局交叉注意力网络增强感知能力，采用分层分类策略解决标签差异，并使用两阶段训练策略和铰链损失。
- Result: 在九个封闭集基准测试中达到SOTA性能，首次实现不依赖视觉信息的零样本泛化。
- Conclusion: CitySeg通过创新方法显著提升了城市规模点云分割的性能和泛化能力。


### [40] [Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection](https://arxiv.org/abs/2508.09475)
*Shibo Yao,Renshuai Tao,Xiaolong Zheng,Chao Liang,Chunjie Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的少样本深度伪造检测方法FTNet，利用少量样本显著提升检测性能。

- Motivation: 现有深度伪造检测方法在未知样本上表现不佳，但现实场景中这些样本仍可用于分析，因此需将其视为少样本任务。
- Method: 提出FTNet，仅需一个伪造样本作为参考，无需训练或参数更新，通过比较测试样本与已知样本的距离进行分类。
- Result: 在29种生成模型的测试中，FTNet平均性能提升8.7%，达到新SoTA。
- Conclusion: FTNet为现实场景中的深度伪造检测提供了新思路，利用失败样本可显著提升性能。


### [41] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 论文提出了一种动态混合面部专家（MoFE）方法和LFA数据集，解决了视频生成中身份保持和大角度面部的问题。

- Motivation: 当前视频生成模型在大角度面部时难以保持身份特征，主要因缺乏有效机制和针对性数据集。
- Method: 引入MoFE动态整合三种专家（身份、语义、细节），并设计数据处理流程（面部约束和身份一致性）构建LFA数据集。
- Result: 实验表明，该方法在LFA数据集上显著优于现有方法，提升了面部相似度、FID和CLIP语义对齐。
- Conclusion: MoFE和LFA数据集有效解决了大角度面部身份保持问题，代码和数据集将开源。


### [42] [CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection](https://arxiv.org/abs/2508.09477)
*Zhipeng Yuan,Kai Wang,Weize Quan,Dong-Ming Yan,Tieru Wu*

Main category: cs.CV

TL;DR: 提出了一种基于异常检测的通用AI生成图像检测器，无需接触AI生成图像，通过无监督学习实现泛化检测。

- Motivation: 随着AI生成图像质量接近自然图像，传统检测方法对未见过的生成模型性能有限，需改进。
- Method: 使用预训练CLIP编码器提取特征，设计类似归一化流的无监督模型，利用代理图像（如频谱修改的自然图像）训练。
- Result: 实验证明该方法对多种图像生成器生成的AI图像检测有效。
- Conclusion: 该方法为AI生成图像检测提供了一种通用且高效的解决方案。


### [43] [GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs](https://arxiv.org/abs/2508.09478)
*Moinak Bhattacharya,Gagandeep Singh,Shubham Jain,Prateek Prasanna*

Main category: cs.CV

TL;DR: GazeLT是一种结合人类视觉注意力机制的长尾疾病分类方法，通过整合和解构放射科医生的眼动模式，显著提升了分类性能。

- Motivation: 放射科医生的眼动模式包含细粒度和粗粒度疾病信息，且注意力随时间变化。将这些信息融入深度学习框架可提升自动化图像解读能力，尤其是对长尾类别的识别。
- Method: GazeLT利用视觉搜索过程的时序特性，通过整合和解构机制，结合放射科医生的眼动数据，优化长尾疾病分类。
- Result: 在NIH-CXR-LT和MIMIC-CXR-LT数据集上，GazeLT比最佳长尾损失方法提升4.1%，比基于视觉注意力的基线方法提升21.7%的平均准确率。
- Conclusion: GazeLT通过有效利用人类视觉注意力机制，显著提升了长尾疾病分类的性能，为自动化医学图像分析提供了新思路。


### [44] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: SkySplat是一种自监督框架，通过整合RPC模型到通用3DGS流程中，改进了稀疏卫星图像的三维重建效果。

- Motivation: 现有方法因与RPC模型不兼容及泛化能力有限，无法有效处理稀疏卫星图像。
- Method: 结合RPC模型，利用RGB图像和辐射鲁棒的相对高度监督，引入CSCM模块和多视角一致性聚合策略。
- Result: 比EOGS快86倍且精度更高，在DFC19数据集上MAE从13.18米降至1.80米，在MVS3D基准上表现优异。
- Conclusion: SkySplat显著提升了稀疏卫星图像的三维重建效率和精度，具有强泛化能力。


### [45] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM框架通过模拟人类情景记忆，解决了Video-LLMs在长视频理解中的关键帧冗余和时空关系缺失问题，显著提升了性能。

- Motivation: 现有方法将长视频简化为静态关键帧匹配，忽略了时空关系和上下文连续性，导致关键帧冗余和信息稀释。
- Method: Video-EM将关键帧建模为时序性情景事件，结合空间关系和时序动态，并利用LLMs的链式思维迭代筛选最小但信息量最大的记忆子集。
- Result: 在多个基准测试中，Video-EM性能提升4-9%，且使用更少帧数。
- Conclusion: Video-EM通过情景记忆和链式思维，显著提升了Video-LLMs在长视频理解中的准确性和效率。


### [46] [SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection](https://arxiv.org/abs/2508.09487)
*Ju Yeon Kang,Jaehong Park,Semin Kim,Ji Won Yoon,Nam Soo Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为SARE的新方法，通过测量图像与其标题引导重建之间的语义差异，来检测扩散模型生成的假图像。该方法在跨模型检测中表现优异。

- Motivation: 现有检测方法在面对未见过的生成模型时性能下降，因为它们依赖模型特定的伪影。论文探索了假图像与标题相似性更高的特性。
- Method: 提出Semantic-Aware Reconstruction Error (SARE)，量化图像与标题引导重建之间的语义差异。假图像语义变化小，而真实图像语义变化大。
- Result: SARE在GenImage和CommunityForensics基准测试中表现优于现有基线方法，具有强泛化能力。
- Conclusion: SARE通过语义差异检测假图像，解决了跨模型检测的局限性，表现出优越的泛化性能。


### [47] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: CWFBind是一种基于局部曲率特征的加权、快速且准确的分子对接方法，通过整合几何信息提升预测性能。

- Motivation: 传统深度学习方法依赖图表示和语言模型编码器，忽略了关键的几何信息，导致口袋定位和结合构象不准确。
- Method: CWFBind在特征提取阶段整合局部曲率描述符，增强蛋白质和配体的几何表示，并引入度感知加权机制优化消息传递。
- Result: CWFBind在多个对接基准测试中表现出色，实现了准确性与效率的平衡。
- Conclusion: CWFBind通过几何特征和动态策略显著提升了分子对接的准确性，为药物设计提供了有力工具。


### [48] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: 论文提出了一种结合ProGAN和SAGAN的GAN变体，用于生成高质量、高分辨率的印度手语图像，并在IS和FID指标上优于传统ProGAN。

- Motivation: 手语是听障人士的重要交流媒介，但目前手语生成技术仍需探索。结合ProGAN和SAGAN的优势，可以生成更高质量的手语图像。
- Method: 开发了一种结合ProGAN和SAGAN的GAN变体，生成高分辨率且特征丰富的手语图像。
- Result: 新模型在IS和FID指标上分别提升了3.2和30.12，并发布了包含印度手语字母、数字和129个单词的大规模数据集。
- Conclusion: 该模型在手语图像生成中表现出色，为听障人士的交流提供了更好的技术支持。


### [49] [SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking](https://arxiv.org/abs/2508.09524)
*Yipei Wang,Shiyu Hu,Shukun Jia,Panxi Xu,Hongfei Ma,Yiping Ma,Jing Zhang,Xiaobo Lu,Xin Zhao*

Main category: cs.CV

TL;DR: 本文首次系统研究并量化了单目标跟踪（SOT）中的相似物体干扰（SOI），提出了一种基于自然语言的语义认知引导方法，并构建了SOIBench基准测试。实验表明，现有视觉语言跟踪方法效果有限，而基于大规模视觉语言模型的新方法表现显著提升。

- Motivation: 相似物体干扰（SOI）是单目标跟踪（SOT）中长期被忽视的关键瓶颈，本文旨在量化其影响并提出解决方案。
- Method: 通过在线干扰掩蔽（OIM）实验量化SOI影响，构建SOIBench基准测试，并提出基于大规模视觉语言模型（VLM）的新方法。
- Result: 消除干扰源显著提升跟踪性能（AUC增益达4.35），新方法在语义认知引导下表现更优（AUC增益达0.93）。
- Conclusion: SOIBench为语义认知跟踪研究提供了标准化平台，新方法显著优于现有技术，为跟踪领域带来新见解。


### [50] [Learning Spatial Decay for Vision Transformers](https://arxiv.org/abs/2508.09525)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 论文提出了一种名为SDT的新型视觉Transformer，通过动态数据依赖的空间衰减机制改进了传统ViT在空间结构任务上的性能。

- Motivation: 传统ViT的自注意力机制缺乏显式的空间归纳偏置，导致在空间结构任务上表现不佳。现有方法使用固定距离度量的空间衰减，无法适应多样化的视觉场景。
- Method: 提出了SDT，引入了一种上下文感知门控（CAG）机制，动态生成数据依赖的空间衰减，结合内容相关性和空间邻近性调节注意力。
- Result: 在ImageNet-1K分类和生成任务上，SDT表现优于基线模型。
- Conclusion: 数据依赖的空间衰减为视觉Transformer的空间注意力增强提供了新范式。


### [51] [Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing](https://arxiv.org/abs/2508.09528)
*Gang Qu,Ping Wang,Siming Zheng,Xin Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种新型的非对称Kronecker压缩感知（AKCS）模型和测量感知交叉注意力（MACA）机制，结合展开网络架构，显著提升了图像压缩感知的性能。

- Motivation: 现有方法在测量阶段的非相干性和重建阶段的隐式测量表示方面存在不足，限制了整体性能。
- Method: 提出AKCS模型提高测量非相干性，并通过MACA机制学习隐式测量表示，结合展开网络架构构建MEUNet。
- Result: MEUNet在重建精度和推理速度上达到最先进水平。
- Conclusion: AKCS和MACA的结合显著提升了压缩感知任务的性能。


### [52] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: COXNet是一种用于RGBT微小目标检测的新框架，通过跨层融合、动态对齐和优化标签分配策略，显著提升了检测性能。

- Motivation: 在无人机场景中，RGBT图像的小目标检测面临空间错位、低光照和复杂背景等挑战，现有方法难以有效利用多模态信息。
- Method: 提出COXNet框架，包括跨层融合模块、动态对齐与尺度细化模块，以及基于GeoShape相似度的优化标签分配策略。
- Result: 在RGBTDronePerson数据集上，COXNet比现有方法提升了3.32%的mAP$_{50}$。
- Conclusion: COXNet在复杂环境中表现出鲁棒性，为RGBT小目标检测提供了有效解决方案。


### [53] [Iterative Volume Fusion for Asymmetric Stereo Matching](https://arxiv.org/abs/2508.09543)
*Yuanting Gao,Linghao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种针对非对称立体视觉匹配问题的两阶段迭代体积融合网络（IVF-AStereo），通过融合两种成本体积计算方法提升匹配精度。

- Motivation: 传统立体匹配算法假设双目视觉对称，但非对称多相机系统（如广角-长焦相机）破坏了这一假设，导致匹配困难。
- Method: 提出两阶段方法：首先通过聚合拼接体积优化相关体积，随后融合两种体积以增强细节。
- Result: 在非对称场景下表现优异，对分辨率和颜色退化具有鲁棒性。
- Conclusion: 实验验证了该方法在非对称立体匹配中的有效性。


### [54] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: GoViG是一种新任务，仅通过初始和目标状态的视觉观察生成导航指令，无需结构化输入。方法包括视觉预测和指令生成，结合多模态大语言模型和推理策略，实验表现优于现有方法。

- Motivation: 传统方法依赖结构化输入（如语义标注或环境地图），限制了在未知和非结构化环境中的适应性。GoViG旨在仅通过原始视觉数据生成精确导航指令。
- Method: 任务分解为视觉预测（预测中间视觉状态）和指令生成（合成语言指令），结合多模态大语言模型和两种推理策略（单次和交替推理）。
- Result: 在R2R-Goal数据集上，GoViG在BLEU-4和CIDEr分数上显著优于现有方法，并表现出强大的跨域泛化能力。
- Conclusion: GoViG展示了仅通过视觉数据生成高质量导航指令的潜力，为未知环境中的导航提供了新思路。


### [55] [Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification](https://arxiv.org/abs/2508.09550)
*Haowen Wang,Guowei Zhang,Xiang Zhang,Zeyuan Chen,Haiyang Xu,Dou Hoon Kwark,Zhuowen Tu*

Main category: cs.CV

TL;DR: 论文探讨了在图像分类任务中，使用生成模型生成的合成数据增强训练集的效果，并量化了合成数据与真实数据在性能上的等价关系。

- Motivation: 解决机器学习中的一个关键问题：如何利用生成模型生成的合成数据增强训练集，以提升分类性能。
- Method: 通过实验比较真实图像与生成模型生成的合成图像的异同，系统分析合成数据增强的有效性，并量化其等价规模。
- Result: 实证确定了合成数据增强的等价规模，并展示了合成数据与真实数据在性能上的定量等价关系。
- Conclusion: 合成数据增强可以提升分类性能，但需要更大的规模才能与真实数据效果相当，且效果受训练集规模和合成数据量的影响。


### [56] [Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning](https://arxiv.org/abs/2508.09555)
*Ahmet Öztel,İsmet Karaca*

Main category: cs.CV

TL;DR: 该研究提出了一种基于2D虹膜图像拓扑不变量的生物识别方法，通过数字同调理论表示虹膜纹理，并评估分类性能。逻辑回归表现最佳，准确率达97.78%。

- Motivation: 探索一种基于拓扑不变量的虹膜识别方法，提供一种可解释且高效的替代深度学习的方案。
- Method: 将归一化虹膜图像分块，计算每块的Betti0、Betti1及其比值，形成特征矩阵，结合逻辑回归、KNN和SVM进行分类，并与CNN对比。
- Result: 逻辑回归准确率最高（97.78%），优于CNN（96.44%）和其他特征模型，拓扑特征表现出高准确性和低方差。
- Conclusion: 首次将数字同调理论用于虹膜识别，方法紧凑、可解释且高效，适用于安全关键领域及其他应用场景。


### [57] [WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization](https://arxiv.org/abs/2508.09560)
*Jiahao Wen,Hang Yu,Zhedong Zheng*

Main category: cs.CV

TL;DR: WeatherPrompt通过多模态学习范式，结合图像嵌入与文本上下文，解决了无人机视觉地理定位在天气扰动下的性能下降问题。

- Motivation: 现有方法在天气扰动（如雨、雾）下性能显著下降，主要受限于对有限天气类别的依赖以及场景-天气特征解耦不足。
- Method: 提出了Training-free Weather Reasoning机制和多模态动态门控框架，通过文本嵌入自适应调整视觉特征，并结合跨模态目标优化。
- Result: 在多种天气条件下，WeatherPrompt的召回率优于现有方法，夜间条件下Recall@1提升13.37%，雾雪条件下提升18.69%。
- Conclusion: WeatherPrompt通过多模态融合和动态门控机制，显著提升了无人机在复杂天气条件下的地理定位性能。


### [58] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波的曝光校正方法（WEC-DG），通过退化描述符和小波变换解决现有方法在复杂光照条件下的适应性不足问题。

- Motivation: 现有多曝光校正方法在处理单一曝光图像时，因光照、环境和天气多样性导致类内变异性问题，难以准确校正曝光异常。
- Method: 提出WEC-DG方法，结合曝光一致性对齐模块（ECAM）和曝光恢复与细节重建模块（EDRM），利用小波变换分离光与细节信息。
- Result: 在多个公开数据集上的实验表明，该方法显著优于现有算法，性能提升明显。
- Conclusion: WEC-DG方法有效解决了复杂光照条件下的曝光校正问题，具有实际应用价值。


### [59] [A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation](https://arxiv.org/abs/2508.09566)
*Haibo Jin,Haoxuan Che,Sunan He,Hao Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为诊断链（CoD）的框架，旨在解决放射学报告生成（RRG）中临床效果不佳和生成文本缺乏可解释性的问题。通过生成问答对和诊断基础模块，CoD提高了报告的准确性和可解释性。

- Motivation: 现有RRG方法在临床效果（尤其是病变属性描述）和可解释性方面表现不佳，导致放射科医生难以信任结果。
- Method: 提出CoD框架，通过诊断对话生成问答对提取关键发现，并利用大语言模型生成准确报告。设计了诊断基础和病变基础模块以提高可解释性和定位准确性。采用全监督学习策略利用多种标注数据。
- Result: CoD在两项RRG基准测试中表现优于专家和通用模型，并能准确将生成的句子与问答诊断和图像关联。
- Conclusion: CoD框架显著提升了RRG的临床准确性和可解释性，为放射科医生提供了高效且可信的工具。


### [60] [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](https://arxiv.org/abs/2508.09575)
*Jiwon Kim,Pureum Kim,SeonHwa Kim,Soobin Park,Eunju Cha,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出了一种无需训练的双递归反馈（DRF）系统，用于改进可控文本到图像（T2I）扩散模型的空间和外观控制能力。

- Motivation: 现有模型（如Ctrl-X和FreeControl）在保留空间结构和捕捉细粒度条件（如物体姿态和场景布局）方面表现不佳。
- Method: 通过外观反馈和生成反馈的双递归机制，优化中间潜在表示，以更好地反映外观信息和用户意图。
- Result: 实验表明，该方法能生成高质量、语义一致且结构一致的图像，支持跨类别的结构-外观融合（如将人类动作转移到老虎形态）。
- Conclusion: DRF系统显著提升了可控T2I模型的性能，无需额外训练，代码已开源。


### [61] [SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](https://arxiv.org/abs/2508.09584)
*Bei Yan,Zhiyuan Chen,Yuecong Min,Jie Zhang,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: 论文提出SHALE基准，用于细粒度评估大型视觉语言模型的幻觉问题，包括忠实性和事实性幻觉，并通过自动化数据构建和分层诱导框架解决现有方法的局限性。

- Motivation: 现有评估方法在细粒度分析、数据可扩展性和数据泄露方面存在不足，需要一种更高效、可控的评估基准。
- Method: 提出自动化数据构建流程和分层幻觉诱导框架，构建包含30K图像-指令对的SHALE基准，覆盖12个视觉感知方面和6个知识领域。
- Result: 实验表明主流LVLMs存在显著的事实性幻觉，并对语义扰动高度敏感。
- Conclusion: SHALE为幻觉问题提供了细粒度评估工具，揭示了模型的局限性，为未来改进提供了方向。


### [62] [Offline Auto Labeling: BAAS](https://arxiv.org/abs/2508.09585)
*Stefan Haag,Bharanidhar Duraisamy,Felix Govaers,Wolfgang Koch,Martin Fritzsche,Juergen Dickmann*

Main category: cs.CV

TL;DR: BAAS是一个基于雷达检测的扩展目标跟踪和标签标注框架，用于自动驾驶，结合贝叶斯方法和融合技术，提供精确的目标轨迹和形状估计。

- Motivation: 解决自动驾驶中雷达检测的精确跟踪和标签标注问题，支持不同监督级别下的闭环改进。
- Method: 采用贝叶斯跟踪、平滑和融合方法，结合手动标注数据实现模块化分析和优化。
- Result: 在复杂城市场景中验证了框架的跟踪性能和标签标注准确性，适用于多种动态目标和类别。
- Conclusion: BAAS框架为自动驾驶提供了高效、精确的雷达检测跟踪和标签标注解决方案。


### [63] [Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma](https://arxiv.org/abs/2508.09593)
*Haotian Tang,Jianwei Chen,Xinrui Tang,Yunjia Wu,Zhengyang Miao,Chao Li*

Main category: cs.CV

TL;DR: Hi-SMGNN是一种分层框架，结合结构和形态连接组，通过多模态交互和多尺度特征融合，提升IDH突变预测的鲁棒性和效果。

- Motivation: 当前基于功能MRI的IDH突变预测方法受限于数据可用性和噪声问题，而结构和形态连接组提供了非侵入性替代方案，但现有方法忽略了大脑的层次结构和多尺度交互。
- Method: 提出Hi-SMGNN框架，包括多模态交互模块（Siamese网络和跨模态注意力）、多尺度特征融合机制和个性化模块分区策略。
- Result: 在UCSF-PDGM数据集上，Hi-SMGNN优于基线和最新模型，表现出更高的鲁棒性和预测效果。
- Conclusion: Hi-SMGNN通过分层和多尺度方法，显著提升了IDH突变预测的性能和可解释性。


### [64] [SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/abs/2508.09597)
*Heyi Sun,Cong Wang,Tian-Xing Xu,Jingwei Huang,Di Kang,Chunchao Guo,Song-Hai Zhang*

Main category: cs.CV

TL;DR: SVG-Head提出了一种混合表示方法，结合3D高斯和FLAME网格，实现高保真且可实时编辑的头像。

- Motivation: 解决头像编辑中几何与外观纠缠的问题，提升AR/VR应用中的实时编辑能力。
- Method: 使用表面高斯和体积高斯分别建模外观和非朗伯区域，结合FLAME网格优化UV映射。
- Result: 在NeRSemble数据集上实现高保真渲染，并首次支持实时外观编辑。
- Conclusion: SVG-Head为头像编辑提供了高效且灵活的解决方案。


### [65] [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](https://arxiv.org/abs/2508.09598)
*Jie Shao,Ke Zhu,Minghao Fu,Guo-hua Wang,Jianxin Wu*

Main category: cs.CV

TL;DR: 论文提出FaME方法，通过评估图像质量并利用失败样本作为负引导，提升生成图像的感知质量，同时保持FID分数不变。

- Motivation: 现有扩散模型在类到图像生成中虽FID分数高，但某些类别图像质量低或失真，因FID忽略单样本感知质量。CFG技术虽提升指标，但可能引入分布偏移和视觉伪影。
- Method: 提出FaME，一种无需训练、推理高效的方法，利用图像质量评估模型识别低质量生成，存储其采样轨迹作为负引导，避免未来采样进入低质量区域。
- Result: 在ImageNet上实验显示，FaME能一致提升视觉质量且不影响FID，并有望扩展至文本到图像生成。
- Conclusion: FaME通过负引导优化生成质量，解决了现有模型在感知质量上的不足，同时保持了全局分布对齐的优势。


### [66] [BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation](https://arxiv.org/abs/2508.09599)
*Beomjun Kim,Suhan Woo,Sejong Heo,Euntai Kim*

Main category: cs.CV

TL;DR: 论文提出BridgeTA框架，通过轻量级教师助理（TA）网络缩小LiDAR-Camera融合与纯相机模型之间的表示差距，同时保持学生模型架构和推理成本不变。

- Motivation: 纯相机方法在自动驾驶的BEV地图分割任务中表现不如LiDAR-Camera融合方法，现有知识蒸馏方法因模仿教师架构导致推理成本增加。
- Method: 引入BridgeTA框架，通过TA网络结合教师和学生的BEV表示，创建共享潜在空间，并基于Young不等式推导蒸馏损失。
- Result: 在nuScenes数据集上，方法比纯相机基线提升4.2% mIoU，优于其他最先进蒸馏方法。
- Conclusion: BridgeTA是一种高效且成本可控的蒸馏框架，显著提升了纯相机模型的性能。


### [67] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: MInDI-3D是一种基于3D条件扩散的模型，用于稀疏视图CBCT伪影去除，显著降低辐射暴露。

- Motivation: 减少医疗成像中的辐射暴露，提升稀疏视图CBCT图像质量。
- Method: 将InDI概念从2D扩展到3D，通过迭代去噪直接从稀疏输入生成CBCT体积。生成大规模伪CBCT数据集用于训练。
- Result: 在CT-RATE测试集上PSNR提升12.96 dB，辐射暴露降低8倍，性能与3D U-Net相当，且泛化能力强。
- Conclusion: MInDI-3D在临床评估中表现优异，适用于患者定位和肿瘤边界保留。


### [68] [Plane Detection and Ranking via Model Information Optimization](https://arxiv.org/abs/2508.09625)
*Daoxin Zhong,Jun Li,Meng Yee Michael Chuah*

Main category: cs.CV

TL;DR: 提出了一种基于模型信息优化的平面检测框架，解决了RANSAC方法在复杂场景中易产生误检的问题。

- Motivation: RANSAC方法在复杂场景中因阈值模糊易产生误检，需一种更客观的机制来确定真实平面数量。
- Method: 将深度数据视为离散随机变量，通过随机子采样生成候选平面模型，结合传感器物理和噪声模型计算信息量，选择信息量最小的模型作为真实平面。
- Result: 实验表明，该方法比Open3D RANSAC更准确地估计平面参数，并通过神经网络分割加速算法。
- Conclusion: 提出的信息优化框架能有效减少误检，提升平面检测的准确性和实用性。


### [69] [Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation](https://arxiv.org/abs/2508.09626)
*Xu Tang,Junan Jia,Yijing Wang,Jingjing Ma,Xiangrong Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为SAD-Splat的新方法，通过高斯点丢弃模块和伪标签生成管道，解决了3D航空场景语义分割中的语义模糊问题。

- Motivation: 传统方法在3D航空场景语义分割中因尺度变化和结构遮挡导致语义模糊，限制了分割精度和一致性。
- Method: 引入高斯点丢弃模块（结合语义置信度估计和可学习的稀疏机制）和高置信度伪标签生成管道。
- Result: SAD-Splat在分割精度和表示紧凑性之间取得了良好平衡，实验效果优异。
- Conclusion: SAD-Splat为3D航空场景理解提供了高效且可扩展的解决方案。


### [70] [Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors](https://arxiv.org/abs/2508.09629)
*Giorgos Karvounas,Nikolaos Kyriazis,Iason Oikonomidis,Georgios Pavlakos,Antonis A. Argyros*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级纹理模块，通过纹理对齐提升单目3D手部重建的精度和真实感。

- Motivation: 现有高精度模型在手部几何与图像外观的对齐上仍不完美，纹理对齐可能是一个未被充分利用的监督信号。
- Method: 提出一个轻量级纹理模块，将像素观测嵌入UV纹理空间，并引入密集对齐损失。该方法基于可微分渲染和已知拓扑的3D手部网格模型。
- Result: 通过增强HaMeR模型，系统在精度和真实感上均有提升。
- Conclusion: 纹理对齐在手部重建中具有重要价值，能显著提升性能。


### [71] [Preacher: Paper-to-Video Agentic System](https://arxiv.org/abs/2508.09632)
*Jingwei Liu,Ling Yang,Hao Luo,Fan Wang Hongyan Li,Mengdi Wang*

Main category: cs.CV

TL;DR: 论文提出Preacher系统，将研究论文转化为结构化视频摘要，解决现有视频生成模型的局限性。

- Motivation: 现有视频生成模型存在上下文窗口限制、视频时长固定、风格单一及无法表达领域知识的问题。
- Method: 采用自上而下的分解、总结与重构，结合自下而上的视频生成，引入关键场景和渐进式思维链（P-CoT）对齐跨模态表示。
- Result: 在五个研究领域成功生成高质量视频摘要，表现优于现有模型。
- Conclusion: Preacher系统为论文到视频任务提供了高效解决方案，代码将开源。


### [72] [Multi-Contrast Fusion Module: An attention mechanism integrating multi-contrast features for fetal torso plane classification](https://arxiv.org/abs/2508.09644)
*Shengjun Zhu,Siyu Liu,Runqing Xiong,Liping Zheng,Duo Ma,Rongshang Chen,Jiaxin Cai*

Main category: cs.CV

TL;DR: 提出了一种多对比融合模块（MCFM）用于提升超声图像中胎儿躯干平面的识别精度，通过多对比注意力机制增强特征建模，实验显示性能显著提升且模型复杂度增加极小。

- Motivation: 超声图像的低对比度和纹理细节不清晰限制了胎儿躯干平面的准确识别，影响产前筛查的可靠性。
- Method: 设计了MCFM模块，专注于神经网络底层处理原始超声数据，通过多对比条件下的注意力权重分配增强特征提取。
- Result: 在胎儿躯干平面超声图像数据集上验证，MCFM显著提升识别性能，同时模型复杂度仅轻微增加。
- Conclusion: MCFM通过多对比融合增强特征表示，为产前筛查提供了更准确可靠的诊断支持，具有临床推广潜力。


### [73] [Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model](https://arxiv.org/abs/2508.09645)
*Zhongyuan Wu,Chuan-Xian Ren,Yu Wang,Xiaohua Ban,Jianning Xiao,Xiaohui Duan*

Main category: cs.CV

TL;DR: PG-SAM结合专家诊断文本引导的SAM模型，通过自动生成提示信息和跨序列注意力模块，显著提升了腮腺病变分割的准确性。

- Motivation: 腮腺病变分割因病变大小和边界复杂而困难，现有方法依赖精确提示且忽略专家知识。
- Method: 提出PG-SAM，包括专家诊断报告引导的提示生成模块和跨序列注意力模块，结合多模态信息。
- Result: 在三个独立临床中心验证中，PG-SAM达到最先进性能。
- Conclusion: PG-SAM展示了临床适用性，并验证了诊断文本对图像分割的增强效果。


### [74] [The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge](https://arxiv.org/abs/2508.09649)
*Reuben Dorent,Laura Rigolo,Colin P. Galvin,Junyu Chen,Mattias P. Heinrich,Aaron Carass,Olivier Colliot,Demian Wassermann,Alexandra Golby,Tina Kapur,William Wells*

Main category: cs.CV

TL;DR: ReMIND2Reg 2025挑战赛旨在通过多模态配准算法解决脑肿瘤手术中脑移位导致的导航不准确问题，提供标准化评估框架。

- Motivation: 脑肿瘤手术中，术前MRI导航因脑移位失去准确性，需通过配准术后超声与术前MRI恢复空间精度。
- Method: 利用ReMIND数据集，包含99例训练、5例验证和10例测试病例，评估配准算法的目标配准误差（TRE）等指标。
- Result: 挑战赛提供无标注训练数据和手动标注验证/测试数据，推动算法开发。
- Conclusion: ReMIND2Reg旨在加速开发临床可用的多模态配准算法，提升手术导航精度。


### [75] [TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos](https://arxiv.org/abs/2508.09650)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazely,Sunil Aryal*

Main category: cs.CV

TL;DR: TOTNet是一种用于体育视频分析中遮挡情况下球体跟踪的网络，通过3D卷积、可见性加权损失和遮挡增强技术显著提升了性能。

- Motivation: 解决体育视频分析中因遮挡导致的球体跟踪问题，提升事件检测和裁判工作的准确性。
- Method: 提出TOTNet，结合3D卷积、可见性加权损失和遮挡增强技术，并使用新数据集TTA进行评估。
- Result: 在四个数据集上表现优异，RMSE从37.30降至7.19，完全遮挡帧的准确率从0.63提升至0.80。
- Conclusion: TOTNet在快速体育场景中的离线分析中表现出色，代码和数据已开源。


### [76] [Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging](https://arxiv.org/abs/2508.09655)
*Lianfang Wang,Kuilin Qin,Xueying Liu,Huibin Chang,Yong Wang,Yuping Duan*

Main category: cs.CV

TL;DR: 该论文提出了一种参数化逆问题框架，用于大规模3D成像重建，结合噪声估计模块和参数化神经算子，实现快速端到端重建，并通过全局和局部时空数据特征融合提升精度和鲁棒性。

- Motivation: 非视距（NLOS）成像中，间接光信号弱且易受噪声影响，需结合物理过程实现准确重建。本文旨在解决大规模线性问题在3D重建中的挑战。
- Method: 1. 使用噪声估计模块自适应评估瞬态数据中的噪声水平；2. 开发参数化神经算子近似逆映射；3. 通过深度算法展开构建基于算子学习的重建框架；4. 提出全局与局部时空数据特征融合方法。
- Result: 在仿真和真实数据集上的实验验证了方法的有效性，尤其在快速扫描和稀疏照明点数据中表现优异。
- Conclusion: 该方法为复杂场景下的NLOS成像提供了可行解决方案，兼具快速重建、高精度和鲁棒性。


### [77] [NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation](https://arxiv.org/abs/2508.09661)
*Eduarda Caldeira,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 提出NegFaceDiff方法，通过负条件采样增强身份分离，提升人脸识别性能。

- Motivation: 解决现有身份条件扩散模型生成数据中身份重叠问题，提高人脸识别模型的性能。
- Method: 引入负条件采样机制，在身份条件扩散过程中显式引导模型远离不需要的特征。
- Result: 身份分离性显著提升（FDR从2.427增至5.687），生成数据训练的人脸识别模型性能优于基准。
- Conclusion: NegFaceDiff有效提升生成数据的身份一致性和分离性，为人脸识别训练提供更优数据源。


### [78] [GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors](https://arxiv.org/abs/2508.09667)
*Xingyilang Yin,Qi Zhang,Jiahao Chang,Ying Feng,Qingnan Fan,Xi Yang,Chi-Man Pun,Huaqi Zhang,Xiaodong Cun*

Main category: cs.CV

TL;DR: GSFixer是一种新框架，通过参考引导的视频修复模型提升稀疏视图下3D高斯泼溅（3DGS）重建的质量。

- Motivation: 稀疏视图下3DGS重建因信息不足导致明显伪影，现有生成先验方法难以保持与输入观察的一致性。
- Method: 采用基于DiT的视频扩散模型，结合2D语义和3D几何特征，增强修复视图的语义一致性和3D一致性。
- Result: 实验表明GSFixer在3DGS伪影修复和稀疏视图3D重建上优于现有方法。
- Conclusion: GSFixer通过多模态特征整合有效解决了稀疏视图3DGS重建的伪影问题。


### [79] [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)
*Gerardo Loza,Junlei Hu,Dominic Jones,Sharib Ali,Pietro Valdastri*

Main category: cs.CV

TL;DR: 提出了一种基于NeRF的新型测试时优化（TTO）方法，用于长期3D点跟踪，解决了现有方法在一致运动或3D跟踪上的局限性。

- Motivation: 当前点跟踪方法在一致运动或3D跟踪方面表现不佳，需要一种更高效的解决方案。
- Method: 采用可逆神经辐射场（InvNeRF）架构，结合多尺度HexPlanes和高效像素采样算法，实现双向可变形-规范映射。
- Result: 在2D点跟踪中，精度和准确率超过现有TTO方法近50%；在3D点跟踪中首次实现TTO方法，优于前馈方法。
- Conclusion: 该方法在2D和3D点跟踪中均表现出色，为长期跟踪提供了高效解决方案。


### [80] [PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training](https://arxiv.org/abs/2508.09691)
*Yin Xie,Zhichao Chen,Xiaoze Yu,Yongle Zhao,Xiang An,Kaicheng Yang,Zimin Ran,Jia Guo,Ziyong Feng,Jiankang Deng*

Main category: cs.CV

TL;DR: PaCo-FR是一种无监督框架，结合掩码图像建模和像素对齐，解决了现有方法在面部特征提取、空间结构利用和数据效率上的不足。

- Motivation: 现有方法在面部特征提取、空间结构利用和数据效率上存在不足，需要一种更高效、更准确的解决方案。
- Method: PaCo-FR结合结构化掩码策略、基于补丁的代码本和空间一致性约束，通过无监督学习提升面部表示。
- Result: PaCo-FR在多种面部分析任务中达到最先进性能，仅需200万无标签图像进行预训练。
- Conclusion: 该方法显著提升了面部表示学习，减少了对昂贵标注数据的依赖，推动了更高效的面部分析系统。


### [81] [Slot Attention-based Feature Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.09699)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: 论文提出SAFF方法，利用Slot Attention机制过滤无关特征，提升小样本学习性能。

- Motivation: 无关特征会显著降低小样本学习性能，需解决背景等非相关特征导致的混淆问题。
- Method: 结合Slot Attention与patch embeddings，通过相似性矩阵量化特征相关性，过滤无关特征。
- Result: SAFF在多个数据集（CIFAR-FS等）上优于现有方法，Slot Attention表现优于其他注意力机制。
- Conclusion: SAFF通过Slot Attention有效过滤无关特征，提升了小样本分类性能。


### [82] [MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers](https://arxiv.org/abs/2508.09709)
*Qianru Qiu,Jiafeng Mao,Kento Masui,Xueting Wang*

Main category: cs.CV

TL;DR: MangaDiT利用扩散变换器和分层注意力机制改进线稿着色，解决了区域级颜色一致性问题。

- Motivation: 现有方法在参考图像和目标图像姿态不同时难以保持颜色一致性，需改进。
- Method: 提出MangaDiT模型，结合分层注意力机制和动态权重策略，增强语义对应和颜色对齐。
- Result: 在基准数据集上表现优于现有方法，定性和定量评估均显著提升。
- Conclusion: MangaDiT通过内部注意力机制有效解决了区域级颜色一致性问题，性能优越。


### [83] [NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation](https://arxiv.org/abs/2508.09715)
*Devvrat Joshi,Islem Rekik*

Main category: cs.CV

TL;DR: NEURAL框架通过语义引导的数据压缩技术，显著减少多模态医学影像数据的存储和传输需求，同时保持高诊断性能。

- Motivation: 解决资源受限临床环境中多模态医学影像数据的存储和传输问题。
- Method: 利用生成式视觉语言模型的交叉注意力分数对胸部X光进行结构修剪，生成高度压缩的图表示，并与临床报告的知识图融合。
- Result: 在MIMIC-CXR和CheXpert Plus数据集上，NEURAL实现了93.4-97.7%的图像数据压缩，诊断性能AUC为0.88-0.95。
- Conclusion: NEURAL解决了数据大小与临床效用之间的权衡，支持高效工作流程和远程放射学。


### [84] [Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction](https://arxiv.org/abs/2508.09717)
*Shekhnaz Idrissova,Islem Rekik*

Main category: cs.CV

TL;DR: 提出了一种基于sheaf的框架，用于融合MRI和组织病理学数据，解决了现有方法在多模态数据融合中的局限性。

- Motivation: 胶质母细胞瘤分子亚型分类需要侵入性组织提取，现有多模态方法缺乏保留共享结构信息的机制。
- Method: 采用sheaf-based框架，实现MRI和组织病理学数据的结构感知和一致性融合。
- Result: 模型在基线方法上表现优越，对不完整或缺失数据具有鲁棒性。
- Conclusion: 该框架为快速诊断的虚拟活检工具开发提供了支持。


### [85] [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](https://arxiv.org/abs/2508.09732)
*Romeo Valentin,Sydney M. Katz,Artur B. Carneiro,Don Walker,Mykel J. Kochenderfer*

Main category: cs.CV

TL;DR: 提出了一种基于视觉的飞机姿态估计方法，通过创新的神经网络架构、校准的预测不确定性和故障检测机制，提高了精度和安全性。

- Motivation: 解决数据驱动的计算机视觉在航空应用中满足安全性和鲁棒性要求的挑战。
- Method: 采用基于空间Soft Argmax算子的神经网络架构、校准的预测不确定性损失函数，以及改进的RAIM方法进行故障检测。
- Result: 模型在跑道图像数据集上表现优于基线架构，提供亚像素级精度的校准不确定性估计。
- Conclusion: 该方法为安全关键航空应用中的视觉系统认证提供了可行方案。


### [86] [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)
*Lin Long,Yichen He,Wentao Ye,Yiyuan Pan,Yuan Lin,Hang Li,Junbo Zhao,Wei Li*

Main category: cs.CV

TL;DR: M3-Agent是一种新型多模态代理框架，具备长期记忆能力，能够处理实时视觉和听觉输入，并通过强化学习在基准测试中表现优异。

- Motivation: 研究旨在开发一种具备人类类似长期记忆能力的多模态代理，以提升其在复杂任务中的表现。
- Method: M3-Agent采用实体中心的多模态记忆组织方式，结合强化学习进行训练，并通过M3-Bench基准测试评估其性能。
- Result: M3-Agent在M3-Bench-robot、M3-Bench-web和VideoMME-long基准测试中分别比基线模型高出6.7%、7.7%和5.3%的准确率。
- Conclusion: M3-Agent在多模态代理的长期记忆和推理能力方面取得了显著进展，为实际应用提供了设计参考。


### [87] [Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection](https://arxiv.org/abs/2508.09746)
*Zhiqiu Zhang,Dongqi Fan,Mingjie Wang,Qiang Tang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: 本文提出了一种基于区域到区域变换（R2R）的图像协调方法，通过Clear-VAE和Harmony Controller提升细节保留和协调能力，并构建了新的合成数据集RPHarmony。

- Motivation: 现有基于LDM的图像协调方法在细节保留和协调能力上存在不足，且合成数据集缺乏真实光照变化。
- Method: 提出R2R模型，结合Clear-VAE和MACA模块，设计Random Poisson Blending生成多样化合成数据。
- Result: 实验表明，该方法在定量指标和视觉协调性上优于其他方法，且新数据集提升了模型在真实场景中的表现。
- Conclusion: R2R方法显著提升了图像协调效果，新数据集为未来研究提供了更真实的训练数据。


### [88] [MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models](https://arxiv.org/abs/2508.09779)
*Dianyi Wang,Siyuan Wang,Zejun Li,Yikun Wang,Yitong Li,Duyu Tang,Xiaoyu Shen,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CV

TL;DR: 提出了一种稀疏混合专家架构MoIIE，用于提升大型视觉语言模型的效率和性能，同时减少计算成本。

- Motivation: 大型视觉语言模型的计算成本高，稀疏混合专家架构（MoE）虽能提升参数效率，但在多模态任务中的应用仍具挑战性。
- Method: 提出MoIIE架构，通过模态引导的路由机制，将令牌分配到模态内专家和共享的模态间专家池中，并结合两阶段训练策略。
- Result: 实验表明，MoIIE模型在5.5B和11.3B激活参数下，性能匹配或超越现有开源多模态MoE-LLMs。
- Conclusion: MoIIE是一种高效且通用的方法，能够同时学习模态内特征和跨模态交互。


### [89] [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/abs/2508.09780)
*Nahyuk Lee,Juhong Min,Junhong Lee,Chunghyun Park,Minsu Cho*

Main category: cs.CV

TL;DR: 提出了一种新的形状匹配方法“组合匹配”，用于几何形状装配中的互锁部件组合。

- Motivation: 传统方法依赖表面形状匹配，而新方法明确建模互锁形状的两个特性：相同表面形状和相反体积占用。
- Method: 通过等变神经网络学习表面形状相同但体积占用相反的区域对应关系，并估计形状方向以减少匹配模糊性。
- Result: 在几何装配基准测试中表现优异，显著优于现有方法。
- Conclusion: 该方法有效减少了匹配模糊性，提升了部件装配的鲁棒性。


### [90] [DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.09785)
*Linpu He,Yanan Li,Bingze Li,Elvis Han Cui,Donghui Wang*

Main category: cs.CV

TL;DR: DSS-Prompt是一种简单有效的方法，通过提示微调预训练的视觉Transformer，用于少样本类增量学习（FSCIL）。

- Motivation: 探索大规模预训练模型在少样本类增量学习任务中的应用，解决持续学习新概念时遗忘旧知识的问题。
- Method: 结合静态提示和动态提示：静态提示用于缩小预训练与下游任务的领域差距，动态提示通过多模态模型提取输入相关语义并自适应调整重要性。
- Result: 在四个基准测试中，DSS-Prompt性能优于现有方法，并能缓解灾难性遗忘问题。
- Conclusion: DSS-Prompt通过提示机制有效提升了FSCIL任务的性能，无需增量任务训练即可超越现有方法。


### [91] [MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking](https://arxiv.org/abs/2508.09796)
*Yingjie Wang,Zhixing Wang,Le Zheng,Tianxiao Liu,Roujing Li,Xueyao Hu*

Main category: cs.CV

TL;DR: MeMoSORT是一种在线实时多目标跟踪方法，通过改进的卡尔曼滤波器和自适应IoU匹配，解决了复杂运动和遮挡问题。

- Motivation: 传统跟踪方法依赖卡尔曼滤波和固定IoU匹配，难以处理复杂运动和遮挡，导致跟踪错误或丢失目标。
- Method: 提出MeKF（记忆辅助卡尔曼滤波器）补偿运动模型误差，Mo-IoU（运动自适应IoU）扩展匹配空间并引入高度相似性。
- Result: 在DanceTrack和SportsMOT数据集上，HOTA分数分别达到67.9%和82.1%，表现优异。
- Conclusion: MeMoSORT通过创新设计显著提升了多目标跟踪性能，适用于复杂场景。


### [92] [MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention](https://arxiv.org/abs/2508.09802)
*Xin Du,Maoyuan Xu,Zhi Ying*

Main category: cs.CV

TL;DR: MUJICA是一种基于跨图注意力的多模态上采样适配器，用于提升PBR材质超分辨率性能，解决了现有方法的跨图不一致性和模态特征建模不足问题。

- Motivation: 现有单图像超分辨率方法在PBR材质上采样中存在跨图不一致、模态特征建模不足和数据分布偏移导致的泛化能力差问题。
- Method: 提出MUJICA适配器，结合预训练的Swin-transformer SISR模型，通过跨图注意力融合特征，保持重建能力。
- Result: 在SwinIR、DRCT和HMANet等模型上，MUJICA提升了PSNR、SSIM和LPIPS分数，同时保持跨图一致性。
- Conclusion: MUJICA在有限资源下高效训练，并在PBR材质数据集上实现领先性能。


### [93] [Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology](https://arxiv.org/abs/2508.09805)
*Jonathan Williams Ramirez,Dina Zemlyanker,Lucas Deden-Binder,Rogeny Herisse,Erendira Garcia Pallares,Karthik Gopinath,Harshvardhan Gazula,Christopher Mount,Liana N. Kozanno,Michael S. Marshall,Theresa R. Connors,Matthew P. Frosch,Mark Montine,Derek H. Oakley,Christine L. Mac Donald,C. Dirk Keene,Bradley T. Hyman,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出了一种基于U-Net架构的深度学习模型，用于自动化分割脑组织照片，性能接近人工标注水平。

- Motivation: 传统方法需要昂贵的人工干预分割脑组织照片，限制了其在大规模研究中的应用。
- Method: 使用1,414张手动标注的固定和新鲜组织照片，以及2,000张合成的随机对比度图像训练U-Net模型。
- Result: 模型在未见过的照片上表现优异，Dice分数中位数超过0.98，接近人工标注的一致性水平。
- Conclusion: 该工具公开可用，为脑组织照片的自动化分割提供了高效解决方案。


### [94] [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/abs/2508.09811)
*Jinxi Li,Ziyang Song,Bo Yang*

Main category: cs.CV

TL;DR: TRACE框架通过将3D点建模为刚性粒子，直接学习其平移旋转动力学系统，无需人工标签即可建模复杂动态3D场景的运动物理。

- Motivation: 现有方法难以学习复杂运动物理或需要额外标签，TRACE旨在解决这一问题。
- Method: 将每个3D点建模为刚性粒子，学习其平移旋转动力学系统，并估计物理参数。
- Result: 在多个数据集上表现优异，尤其在未来帧外推任务中，且能通过聚类物理参数轻松分割对象。
- Conclusion: TRACE无需人工标签即可高效建模复杂动态3D场景的物理运动。


### [95] [Poaching Hotspot Identification Using Satellite Imagery](https://arxiv.org/abs/2508.09812)
*Aryan Pandhi,Shrey Baid,Sanjali Jha*

Main category: cs.CV

TL;DR: 非洲象偷猎问题严重，计算机视觉模型被提出用于动态识别偷猎热点区域。

- Motivation: 非洲象因象牙偷猎濒临灭绝，偷猎热点区域动态变化，传统反偷猎方法效率低。
- Method: 利用计算机视觉模型结合卫星图像，自动识别偷猎热点区域的地理指标。
- Result: 模型可高效覆盖大面积区域，避免干扰当地物种和跨境飞行限制。
- Conclusion: 计算机视觉模型为动态反偷猎提供了高效、非侵入性的解决方案。


### [96] [Evolution of Low-Level and Texture Human-CLIP Alignment](https://arxiv.org/abs/2508.09814)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: CLIP模型训练早期与低层次人类图像质量评估相关性高，随后逐渐下降。研究发现这与形状-纹理偏差对齐和噪声下分类准确性下降有关。

- Motivation: 探究CLIP模型训练中与低层次人类感知相关性先升后降的现象及其原因。
- Method: 通过分析形状-纹理偏差对齐和噪声下的分类准确性，研究CLIP的学习机制。
- Result: CLIP早期学习低层次视觉特征，后期转向抽象形状表征，影响感知对齐与噪声鲁棒性。
- Conclusion: 研究揭示了CLIP学习机制，为优化视觉语言模型的感知对齐与鲁棒性提供了新思路。


### [97] [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/abs/2508.09818)
*Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Abir Ahmed,Liew Tze Hui*

Main category: cs.CV

TL;DR: ViMoNet是一个结合视频和运动数据的框架，用于理解和推断人类行为，优于现有方法。

- Motivation: 现有模型仅关注运动数据或视频，无法全面捕捉人类行为的细微差别。
- Method: 提出ViMoNet框架，联合训练视频-文本和运动-文本数据，并创建新数据集VIMOS和评估基准ViMoNet-Bench。
- Result: ViMoNet在字幕生成、运动理解和行为解释方面表现优于现有方法。
- Conclusion: 结合视频和运动数据能更全面地理解人类行为，ViMoNet展示了其有效性。


### [98] [Physical Autoregressive Model for Robotic Manipulation without Action Pretraining](https://arxiv.org/abs/2508.09822)
*Zijian Song,Sihan Qin,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: PAR模型利用视频预训练的世界知识，通过物理令牌结合帧和动作，实现机器人及其环境的联合演化，无需动作预训练即可准确预测视频和一致动作轨迹。

- Motivation: 机器人操作数据的稀缺性促使研究者利用其他模态的预训练大模型。
- Method: 提出物理自回归模型（PAR），结合DiT去令牌化器、因果掩码、逆运动学、并行训练和KV缓存机制。
- Result: 在ManiSkill基准测试中，PAR在PushCube任务上达到100%成功率，其他任务与动作预训练基线相当，并能准确预测未来视频。
- Conclusion: PAR展示了通过视频预训练转移世界知识在机器人操作中的潜力。


### [99] [KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.09823)
*Valentin Boussot,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: KonfAI是一个模块化、可扩展且完全可配置的深度学习框架，专为医学影像任务设计，通过YAML配置文件实现工作流定义，提升可重复性和开发效率。

- Motivation: 解决医学影像任务中深度学习工作流的可配置性、可重复性和开发效率问题。
- Method: 使用结构化YAML配置文件定义训练、推理和评估工作流，支持高级策略如分块学习、测试时增强和模型集成。
- Result: 成功应用于分割、配准和图像合成任务，并在国际医学影像挑战中取得优异表现。
- Conclusion: KonfAI通过其模块化和可扩展性，为医学影像任务提供了高效、透明的深度学习解决方案。


### [100] [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/abs/2508.09824)
*Xuhong Huang,Shiqi Liu,Kai Zhang,Ying Tai,Jian Yang,Hui Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新颖的深度反向卷积算子，旨在有效反转深度卷积，并通过构建反向卷积块改进现有架构。

- Motivation: 由于转置卷积并非卷积的严格逆操作，缺乏标准的反向卷积算子，作者试图填补这一空白。
- Method: 通过正则化最小二乘优化问题设计深度反向卷积算子，并结合层归一化、1×1卷积和GELU激活构建反向卷积块。
- Result: 实验证明该算子作为基础模块的有效性，并在图像恢复任务中优于传统方法。
- Conclusion: 该工作为深度学习模型设计中的新算子开发提供了方向。


### [101] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 提出了一种名为RayletDF的新方法，用于从点云或3D高斯数据中重建3D表面，通过射线距离场直接预测表面点，具有高效性和泛化能力。

- Motivation: 现有基于坐标的方法在渲染显式表面时计算量大，需要一种更高效且泛化能力强的3D表面重建方法。
- Method: 提出RayletDF方法，包含射线特征提取器、射线距离场预测器和多射线混合器三个模块，用于提取几何特征、预测距离并聚合结果。
- Result: 在多个公开数据集上表现优异，能够单次前向通过未见数据集成功重建3D表面。
- Conclusion: RayletDF在3D表面重建中具有高效性和出色的泛化能力，适用于多种数据源。


### [102] [Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment](https://arxiv.org/abs/2508.09843)
*Hao Yang,Xu Zhang,Jiaqi Ma,Linwei Zhu,Yun Zhang,Huan Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于图神经网络的OIQA框架，通过建模视口间的结构关系来提升对空间失真非均匀性的感知能力。

- Motivation: 现有OIQA方法难以评估局部非均匀失真，因缺乏对质量空间变化的建模及有效的局部细节与全局上下文特征表示。
- Method: 使用斐波那契球采样生成视口，将其表示为图节点；结合多阶段特征提取网络、图注意力网络（GAT）和图变换器，捕捉局部和长程质量依赖关系。
- Result: 在两个大规模OIQA数据库上的实验表明，该方法显著优于现有方法，验证了其有效性和强泛化能力。
- Conclusion: 所提框架通过图神经网络建模视口间关系，成功解决了局部非均匀失真的评估问题，具有广泛应用潜力。


### [103] [Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance](https://arxiv.org/abs/2508.09847)
*Dhruvraj Singh Rawat,Enggen Sherpa,Rishikesan Kirupanantha,Tin Hoang*

Main category: cs.CV

TL;DR: 论文提出了一个小规模CelebAMask-HQ数据集上的人脸生成扩散模型基准，评估了无条件与有条件生成流程，比较了UNet和DiT架构，并探索了LoRA微调预训练Stable Diffusion模型。

- Motivation: 研究旨在改进属性引导的人脸生成，尤其是在数据有限的情况下，通过增强语义对齐和可控性。
- Method: 采用多条件方法（属性向量和分割掩码），引入InfoNCE损失优化属性嵌入，并使用SegFormer分割编码器。
- Result: 结果表明对比嵌入学习和高级分割编码在有限数据下对可控人脸生成有效。
- Conclusion: 通过对比嵌入和SegFormer编码器的改进，提升了属性引导合成的语义对齐和可控性。


### [104] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: ARI3D是一款用于交互式分析X射线CT图像区域的软件工具，旨在改进相位识别、考虑部分体积效应、提高检测限和准确性，并统一3D定量分析。

- Motivation: X射线CT成像技术存在多种固有成像伪影（如束硬化和部分体积效应），导致用户需基于体素灰度值对微结构进行分割和分类，分析过程复杂且需多次决策。
- Method: 提出ARI3D软件工具，通过交互式分析3D X射线CT图像区域，辅助用户完成分类和量化协议中的各个步骤。
- Result: ARI3D能够改进相位识别、处理部分体积效应、提高检测限和量化准确性，并实现跨学科的统一3D定量分析。
- Conclusion: ARI3D为X射线CT图像分析提供了一种高效、准确的交互式工具，适用于多学科研究。


### [105] [Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment](https://arxiv.org/abs/2508.09850)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Valero Laparra,Jesus Malo*

Main category: cs.CV

TL;DR: 研究发现，更大的ViT模型与人类感知对齐度更低，数据增强和正则化会进一步降低对齐度，尤其是在重复训练周期中。

- Motivation: 探索ViT模型在图像识别任务中与人类感知的对齐情况，分析模型大小、数据集大小、数据增强和正则化的影响。
- Method: 在TID2013数据集上系统分析ViT模型与人类感知的对齐度，考察模型大小、数据集多样性、数据增强和正则化的作用。
- Result: 大模型对齐度更低；增加数据集多样性影响小，但重复训练会降低对齐度；强数据增强和正则化进一步降低对齐度。
- Conclusion: 模型复杂性、训练策略与人类感知对齐度之间存在权衡，需在需要类人视觉理解的应用中谨慎选择。


### [106] [OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better](https://arxiv.org/abs/2508.09857)
*Yupeng Zhou,Zhen Li,Ziheng Ouyang,Yuming Chen,Ruoyi Du,Daquan Zhou,Bin Fu,Yihao Liu,Peng Gao,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: 论文提出了一种结合离散和连续视频VAE的方法（OneVAE），通过利用连续VAE的先验知识，显著提升了离散视频VAE的训练效率和性能。

- Motivation: 离散视频VAE在训练稳定性、时间和重建质量方面存在问题，而连续VAE表现更优。因此，研究如何利用连续VAE的优势来改进离散视频VAE。
- Method: 1. 使用FSQ方法保留连续VAE的先验知识；2. 引入多令牌量化机制提升重建质量；3. 加强首帧重建以优化高压缩视频VAE；4. 提出联合离散-连续优化方案。
- Result: 方法收敛速度更快，性能更优，PSNR提升近1 dB，并在单一网络中实现了离散和连续表示的竞争性表现。
- Conclusion: OneVAE成功结合了离散和连续视频VAE的优势，为多模态LLM提供了更高效的视频表示方法。


### [107] [HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics](https://arxiv.org/abs/2508.09858)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: HumanGenesis框架通过四个协作代理解决合成人类动态中的几何不一致和运动泛化问题，实现了高质量的3D重建和视频合成。

- Motivation: 当前方法在3D建模和生成能力上存在不足，导致几何不一致和运动泛化受限。
- Method: 结合几何与生成建模，通过四个代理（重建器、批评代理、姿态引导器和视频协调器）协作完成。
- Result: 在文本引导合成、视频重演和新姿态泛化等任务中表现优异。
- Conclusion: HumanGenesis显著提升了表达的丰富性、几何保真度和场景融合效果。


### [108] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为COME的通用协作异构源特定专家混合框架，用于解决超声图像分析中多数据集训练时的干扰问题，并提升泛化能力。

- Motivation: 传统单数据集训练在新数据分布下表现不佳，尤其是在超声图像分析中，由于数据有限、声学阴影和斑点噪声等问题。因此，构建一个适用于多异构超声数据集的通用框架至关重要。
- Method: COME通过建立双结构-语义共享专家，创建通用表示空间，并与源特定专家协作提取判别性特征。该方法利用跨数据集的经验分布，为小批量或未见数据提供通用超声先验。
- Result: 在三种评估模式（单数据集、器官内和器官间集成数据集）下的实验表明，COME显著优于现有方法，平均AP有显著提升。
- Conclusion: COME通过协作异构源特定专家，有效解决了多数据集训练中的干扰问题，提升了模型的泛化能力和鲁棒性。


### [109] [E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras](https://arxiv.org/abs/2508.09912)
*Chaoran Feng,Zhenyu Tang,Wangbo Yu,Yatian Pang,Yian Zhao,Jianbin Zhao,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: E-4DGS是一种基于事件相机的动态高斯泼溅方法，用于快速运动相机下的多视角事件流新视角合成。

- Motivation: 传统RGB相机在光照不足、运动模糊和动态范围有限等方面存在局限性，而事件相机具有低功耗、高时间分辨率和高动态范围的优势，适用于高速运动和低光场景的重建。
- Method: 提出事件驱动的初始化方案和事件自适应切片泼溅技术，结合强度重要性修剪和自适应对比度阈值优化。
- Result: E-4DGS在合成多视角事件流数据集上表现优于仅事件和事件-RGB融合基线方法。
- Conclusion: 该方法为多视角事件流重建提供了新思路，适用于快速场景捕捉。


### [110] [SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection](https://arxiv.org/abs/2508.09913)
*Yachao Liang,Min Yu,Gang Li,Jianguo Jiang,Boquan Li,Feng Yu,Ning Zhang,Xiang Meng,Weiqing Huang*

Main category: cs.CV

TL;DR: 论文提出了一种利用音频-视觉语音表示学习的新方法，用于检测伪造视频，无需使用伪造视频训练模型即可实现跨数据集泛化和鲁棒性。

- Motivation: 音频信号能精确反映面部运动，为伪造视频检测提供了新思路。
- Method: 通过自监督掩码预测任务学习真实视频的音频-视觉语音表示，并将其直接迁移到伪造检测任务。
- Result: 实验表明，该方法在跨数据集泛化和鲁棒性上优于现有技术。
- Conclusion: 音频-视觉语音表示学习是检测伪造视频的有效方法。


### [111] [Towards Comprehensive Cellular Characterisation of H&E slides](https://arxiv.org/abs/2508.09926)
*Benjamin Adjadj,Pierre-Antoine Bannier,Guillaume Horent,Sebastien Mandela,Aurore Lyon,Kathryn Schutte,Ulysse Marteau,Valentin Gaury,Laura Dumont,Thomas Mathieu,Reda Belbahri,Benoît Schmauch,Eric Durand,Katharina Von Loga,Lucie Gillet*

Main category: cs.CV

TL;DR: HistoPLUS是一种先进的细胞分析模型，针对肿瘤微环境中的细胞检测、分割和分类问题，解决了现有方法在罕见细胞类型和跨域泛化上的不足。

- Motivation: 现有方法在罕见细胞类型和跨域泛化上表现不佳，限制了肿瘤微环境分析的准确性。
- Method: 基于一个包含108,722个细胞核的新数据集，训练了HistoPLUS模型，覆盖13种细胞类型。
- Result: 在外部验证中，HistoPLUS在检测质量和分类F1分数上分别提升5.2%和23.7%，且参数减少5倍。
- Conclusion: HistoPLUS显著提升了罕见细胞类型的研究能力，并展示了强大的跨域泛化性能，为肿瘤微环境研究提供了重要工具。


### [112] [Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?](https://arxiv.org/abs/2508.09936)
*Vittorio Pippi,Konstantina Nikolaidou,Silvia Cascianelli,George Retsinas,Giorgos Sfikas,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: 本文系统比较了三种先进的HTG模型，评估其对HTR微调的影响，并提供了选择最有效HTG模型的定量指南。

- Motivation: 解决小规模、特定作者手写文本识别（HTR）中的挑战，尤其是当数据分布与训练数据不一致时。
- Method: 比较三种HTG模型（生成对抗、扩散和自回归范式），分析合成数据的视觉和语言特征对HTR微调的影响。
- Result: 提供了HTG模型选择的定量指南，并揭示了当前HTG方法的能力及改进方向。
- Conclusion: HTG方法在低资源HTR中有潜力，但仍需进一步改进。


### [113] [AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models](https://arxiv.org/abs/2508.09943)
*Tomás de la Sotta,José M. Saavedra,Héctor Henríquez,Violeta Chang,Aline Xavier*

Main category: cs.CV

TL;DR: AST-n框架通过从中间噪声水平启动反向扩散并集成高阶ODE求解器，显著加速低剂量CT图像去噪，同时保持图像质量。

- Motivation: 低剂量CT（LDCT）协议减少辐射但增加图像噪声，影响诊断信心。扩散生成模型在LDCT去噪中表现潜力，但传统方法计算成本高。
- Method: 提出AST-n框架，结合中间噪声启动和高阶ODE求解器，减少采样步骤。评估两种加速范式：AST-n采样和标准调度。
- Result: AST-25仅用25步即达到PSNR>38 dB和SSIM>0.95，接近基线，推理时间从16秒降至1秒/切片。无条件采样质量显著下降。
- Conclusion: AST-n与高阶求解器结合，实现快速LDCT重建且不损失图像质量，推动扩散模型在临床中的可行性。


### [114] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: 利用现成的Stable Diffusion模型实现视觉上下文学习（V-ICL），无需额外微调即可适应多种视觉任务。

- Motivation: 探索如何简化视觉上下文学习过程，避免复杂的训练和额外数据需求，提高通用性。
- Method: 在Stable Diffusion的自注意力层中重新计算注意力，显式结合查询和示例提示的上下文。
- Result: 在六个视觉任务中表现优异，例如在Pascal-5i数据集上，前景分割任务的mIoU分别比Visual Prompting和IMProv提高了8.9%和3.2%。
- Conclusion: 证明了现成Stable Diffusion模型在视觉上下文学习中的潜力，并通过集成多提示进一步提升了性能。


### [115] [LIA-X: Interpretable Latent Portrait Animator](https://arxiv.org/abs/2508.09959)
*Yaohui Wang,Di Yang,Xinyuan Chen,Francois Bremond,Yu Qiao,Antitza Dantcheva*

Main category: cs.CV

TL;DR: LIA-X是一种新型可解释的肖像动画生成器，通过稀疏运动字典实现面部动态的精细控制，优于现有方法。

- Motivation: 解决现有方法在面部动态转移中缺乏精细控制和可解释性的问题。
- Method: 采用自编码器模型，通过稀疏运动字典将面部动态分解为可解释因素，支持“编辑-变形-渲染”策略。
- Result: 在自重现和跨重现任务中表现优于现有方法，支持用户引导的精细图像和视频编辑。
- Conclusion: LIA-X通过可解释性和可控性，为肖像动画和编辑提供了高效解决方案。


### [116] [January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis](https://arxiv.org/abs/2508.09966)
*Amir Hosseinian,Ashkan Dehghani Zahedani,Umer Mansoor,Noosheen Hashemi,Mark Woodward*

Main category: cs.CV

TL;DR: 论文提出了一个标准化的评估方法和高质量数据集（JFB），并开发了一个综合评估框架和专用模型，显著提升了自动营养分析的性能。

- Motivation: AI在自动营养分析领域的进展因缺乏标准化评估方法和高质量数据集而受限。
- Method: 提出了JFB数据集、综合评估框架，并比较了通用VLMs与专用模型的性能。
- Result: 专用模型的总体得分为86.2，比最佳通用配置提高了12.1分。
- Conclusion: 该研究为自动营养分析提供了新的评估数据集和框架，推动了未来研究的发展。


### [117] [MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2508.09967)
*Tianqi Xiang,Yi Li,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种元优化分类器（MOC），通过结合元学习器和多样化的分类器库，显著提升了少样本学习下的WSI分类性能。

- Motivation: 现有的少样本学习方法在数据稀缺时表现不佳，需要一种更鲁棒的分类器设计来应对这一挑战。
- Method: MOC包含一个元学习器和一个分类器库，前者自动优化分类器配置，后者提供多样化的候选分类器。
- Result: 在TCGA-NSCLC基准测试中，MOC的AUC比现有方法提高了10.4%，在1-shot条件下提升高达26.25%。
- Conclusion: MOC为临床部署中数据稀缺问题提供了重要解决方案，显著提升了少样本学习的性能。


### [118] [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/abs/2508.09973)
*Geonhee Sim,Gyeongsik Moon*

Main category: cs.CV

TL;DR: PERSONA框架结合3D和扩散方法，通过单张图像生成个性化3D人体化身，解决身份保持和姿态驱动变形问题。

- Motivation: 现有3D方法需要大量姿态丰富的视频，而扩散方法难以保持身份一致性。PERSONA旨在结合两者优势，从单张图像实现高质量化身生成。
- Method: 利用扩散模型从输入图像生成姿态丰富的视频，再优化3D化身。引入平衡采样和几何加权优化以提升渲染质量。
- Result: PERSONA能够从单张图像生成具有姿态驱动变形的高保真3D化身。
- Conclusion: PERSONA通过结合3D和扩散方法，解决了身份保持和姿态变形的挑战，为单图像化身生成提供了高效方案。


### [119] [A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation](https://arxiv.org/abs/2508.09977)
*Shuting He,Peilin Ji,Yitong Yang,Changshuo Wang,Jiayi Ji,Yinglin Wang,Henghui Ding*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）作为NeRF的替代方案，提供高保真实时渲染，并支持多种下游应用。本文综述了3DGS的最新进展，包括语义理解、分割、编辑和生成等任务。

- Motivation: 探索3DGS在3D场景表示中的潜力，超越新颖视图合成，支持几何和语义理解的应用。
- Method: 综述了3DGS的应用，包括2D基础模型、NeRF方法、任务分类（分割、编辑、生成等），并总结了代表性方法、监督策略和学习范式。
- Result: 总结了常用数据集、评估协议，并在公开基准上进行了比较分析。
- Conclusion: 3DGS在多种应用中表现出色，未来研究可通过持续更新的资源库进一步推动发展。


### [120] [LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](https://arxiv.org/abs/2508.09981)
*Chengtao Lv,Bilang Zhang,Yang Yong,Ruihao Gong,Yushi Huang,Shiqiao Gu,Jiajun Wu,Yumeng Shi,Jinyang Guo,Wenya Wang*

Main category: cs.CV

TL;DR: LLMC+是一个全面的VLM压缩基准工具包，支持20多种算法，揭示了空间和时间冗余需要不同策略，并展示了联合压缩的潜力。

- Motivation: 现有VLM压缩方法存在模块不可比、评估任务单一和技术孤立的问题，LLMC+旨在填补这些空白。
- Method: LLMC+提供插件式工具包，支持多种算法，系统研究token级和模型级压缩。
- Result: 发现空间和时间冗余需不同策略，多任务中token压缩效果下降，联合压缩可实现高效压缩。
- Conclusion: LLMC+促进公平评估，启发高效VLM研究。


### [121] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: Story2Board是一个无需训练的框架，用于从自然语言生成富有表现力的故事板，通过轻量级一致性框架提升视觉多样性和连贯性。

- Motivation: 现有方法过于关注主体身份，忽略了视觉叙事的关键方面（如空间构图、背景演变和叙事节奏），因此需要一种新方法来提升故事板的连贯性和多样性。
- Method: 采用轻量级一致性框架，包括潜在面板锚定（保留角色参考）和互惠注意力值混合（软融合视觉特征），并结合现成语言模型生成面板级提示。
- Result: 在Rich Storyboard Benchmark上，Story2Board在布局多样性、背景叙事和一致性方面表现优异，并通过用户研究验证其动态性和叙事吸引力。
- Conclusion: Story2Board能够生成更具动态性、连贯性和叙事吸引力的故事板，优于现有基线方法。


### [122] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 论文探讨了GPT-4o生成的合成图像数据在补充真实数据集不足和提供更干净监督信号方面的优势，并提出了Echo-4o-Image数据集和两个新评估基准。

- Motivation: 研究旨在解决真实图像数据集在罕见场景和文本-图像对齐方面的局限性，利用合成数据的优势提升开源模型性能。
- Method: 通过GPT-4o生成180K规模的合成数据集Echo-4o-Image，并基于此微调Bagel模型得到Echo-4o。同时提出GenEval++和Imagine-Bench两个新评估基准。
- Result: Echo-4o在标准基准测试中表现优异，且Echo-4o-Image数据集对其他基础模型也有显著性能提升。
- Conclusion: 合成图像数据在补充真实数据集和提升模型性能方面具有重要价值，新提出的数据集和评估基准为未来研究提供了有效工具。
## cs.RO

### [123] [DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation](https://arxiv.org/abs/2508.09444)
*Haoxiang Shi,Xiang Deng,Zaijing Li,Gongwei Chen,Yaowei Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: VLN-CE任务中，传统两阶段导航框架存在全局次优和性能瓶颈问题。DifNav提出了一种端到端的扩散策略，统一了路径点生成和规划，并通过DAgger增强训练，显著提升了导航性能。

- Motivation: 解决VLN-CE任务中两阶段导航框架的全局次优化和性能瓶颈问题。
- Method: 提出DifNav，一种端到端的扩散策略，统一路径点生成和规划，使用条件扩散策略直接建模连续导航空间中的多模态动作分布，并通过DAgger进行在线训练和专家轨迹增强。
- Result: 在基准数据集上，DifNav显著优于现有两阶段模型，无需路径点预测器即可实现更好的导航性能。
- Conclusion: DifNav通过端到端优化和DAgger增强训练，解决了传统两阶段框架的局限性，提升了VLN-CE任务的导航性能。


### [124] [Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes](https://arxiv.org/abs/2508.09855)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 提出了一种仅从RGB图像训练人机协作策略的方法，无需真实机器人数据，专注于人机交接任务。

- Motivation: 解决从仿真到真实环境的视觉域差距问题，减少对大规模真实机器人数据的需求。
- Method: 利用稀疏视角高斯泼溅重建人机交接场景，生成图像-动作对演示，直接映射到机器人夹爪姿态变化。
- Result: 在重建场景和真实人机交接实验中验证了方法的有效性和鲁棒性。
- Conclusion: 该方法为人机交接任务提供了新的高效表示，促进了更无缝和鲁棒的人机协作。
## cs.LG

### [125] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN框架通过模态感知的动态噪声编辑，解决了多模态情感分析中冗余和噪声信息的问题，同时保留关键信息。MoLAN+在此基础上实现了最先进的性能。

- Motivation: 多模态情感分析常因无关或误导性的视觉和听觉信息而表现不佳，现有方法往往在去噪时丢失关键信息。
- Method: MoLAN将每个模态的特征分块，根据噪声水平和语义相关性动态分配去噪强度，实现细粒度噪声抑制。
- Result: 在五个模型和四个数据集上的实验表明，MoLAN框架广泛有效，MoLAN+达到最先进性能。
- Conclusion: MoLAN是一个统一且灵活的框架，可无缝集成到多模态模型中，显著提升了多模态情感分析的性能。


### [126] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG框架通过自适应块缺失表示学习解决不同ECG布局的异步问题，显著提升心律失常识别性能。

- Motivation: 不同医院的ECG布局差异导致信号异步和部分缺失，现有模型难以应对。
- Method: 提出PatchECG框架，基于掩码训练策略自适应学习关键块，并利用导联间协作依赖关系。
- Result: 在PTB-XL数据集和真实医院数据上表现稳健，AUROC达0.835，外部验证中房颤诊断AUROC为0.778。
- Conclusion: PatchECG优于经典方法和当前最优模型，显著提升识别性能且不受布局变化影响。


### [127] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: SVG-1M数据集和SVGen模型解决了从自然语言生成精确SVG代码的挑战，提升了效率和准确性。

- Motivation: 将创意转化为精确的矢量图形耗时且复杂，需要一种更高效的方法。
- Method: 通过SVG-1M数据集和SVGen模型，结合数据增强、课程学习和强化学习优化，实现文本到SVG的生成。
- Result: SVGen在效果和效率上优于通用大模型和传统渲染方法。
- Conclusion: SVGen为前端开发和设计提供了一种高效的解决方案，代码和数据集已开源。


### [128] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级、无需训练的方法，利用检索增强生成（RAG）跨模态线性映射，解决多模态模型中的模态间隙问题。

- Motivation: 预训练大型多模态模型（LMMs）存在模态间隙，即文本和视觉表示在共享嵌入空间中的不对齐问题，而微调成本高昂且不实用。
- Method: 采用检索增强生成（RAG）技术，通过线性映射高效计算，利用训练集中的文本描述生成新的文本描述，并通过迭代技术优化映射。
- Result: 在两个基准多模态数据集上的实验结果显示显著改进。
- Conclusion: 该方法提供了一种低成本、高效的解决方案，有效缓解了模态间隙问题。


### [129] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: MoQE是一种基于MoE架构的量化推理框架，通过动态路由输入数据到最合适的量化专家模型，缓解单量化模型的性能下降问题。

- Motivation: 量化方法在提高模型效率和降低部署成本方面至关重要，但量化过程会导致精度下降。MoQE旨在通过结合多个量化变体作为专家模型，共同提升量化模型的性能。
- Method: MoQE结合一个全精度模型的多个量化变体作为专门的“量化专家”，并基于输入数据的特性动态路由到最合适的专家。设计了轻量级、结构感知的路由器模型，适用于CV和NLP任务。
- Result: 在ResNet、LLaMA和Qwen模型家族上的实验表明，MoQE性能接近SOTA量化模型，且未显著增加推理延迟。
- Conclusion: MoQE通过动态路由和多量化专家模型，有效缓解了量化模型的性能下降问题，适用于资源受限设备上的深度学习应用。


### [130] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: 提出一种动态连接掩码（DCM）机制，通过稀疏正则化增强模型对噪声标签的鲁棒性，适用于MLPs和KANs，并在实验中优于现有方法。

- Motivation: 现实场景中噪声标签不可避免，现有研究多关注鲁棒损失函数和样本选择，而对模型架构正则化的探索较少。
- Method: 提出动态连接掩码（DCM）机制，自适应掩码训练中不重要的边，减少梯度误差。
- Result: 实验证明DCM在合成和真实数据集上均优于现有方法，且KANs在噪声场景中表现优于MLPs。
- Conclusion: DCM机制有效提升模型对噪声标签的鲁棒性，并可与其他噪声鲁棒方法结合使用。


### [131] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: 论文提出了一种通过噪声超网络替代奖励引导的测试时噪声优化的方法，以减少推理计算开销，同时保持模型性能。

- Motivation: 测试时扩展虽然提升了模型性能，但计算时间大幅增加，限制了实际应用。研究旨在保留其优势的同时减少推理开销。
- Method: 提出噪声超网络，用于调制初始输入噪声，并通过可处理的噪声空间目标学习奖励倾斜分布。
- Result: 方法在显著降低计算成本的同时，恢复了大部分测试时优化的质量提升。
- Conclusion: 噪声超网络是一种有效的解决方案，能够在减少计算开销的同时保持模型性能。
## q-bio.NC

### [132] [Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions](https://arxiv.org/abs/2508.09852)
*Baihan Lin*

Main category: q-bio.NC

TL;DR: 论文提出了一种名为Perceptual Reality Transformer的框架，通过六种神经网络架构模拟八种神经感知障碍，帮助他人体验这些病症的视觉感知状态。

- Motivation: 神经感知障碍导致患者与周围人之间存在深刻的体验鸿沟，亟需一种科学方法模拟这些感知状态以促进理解和共情。
- Method: 采用六种神经网络架构，学习从自然图像到特定病症感知状态的映射，并在ImageNet和CIFAR-10数据集上评估性能。
- Result: Vision Transformer架构表现最优，超越传统CNN和生成方法，建立了首个神经感知模拟的系统基准。
- Conclusion: 该框架在医学教育、共情训练和辅助技术开发中有直接应用价值，同时推进了神经网络模拟非典型人类感知的基础研究。
## eess.IV

### [133] [Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation](https://arxiv.org/abs/2508.09177)
*Xuanru Zhou,Cheng Li,Shuqiang Wang,Ye Li,Tao Tan,Hairong Zheng,Shanshan Wang*

Main category: eess.IV

TL;DR: 本文综述了生成式AI在医学影像中的进展，包括GANs、VAEs、扩散模型等，并探讨了其在临床工作流中的应用与挑战。

- Motivation: 生成式AI在医学影像中的潜力巨大，可解决数据稀缺、标准化和多模态整合等问题，推动临床应用的进步。
- Method: 通过系统分析生成模型（如GANs、VAEs、扩散模型）在影像工作流各阶段的应用，提出三层次评估框架（像素级、特征级、任务级）。
- Result: 生成式AI在影像合成、增强、诊断支持等方面表现优异，但仍面临泛化性、隐私和监管等挑战。
- Conclusion: 生成式AI与大规模基础模型的结合有望推动下一代临床影像系统的发展，需加强跨学科合作。


### [134] [HiFi-Mamba: Dual-Stream W-Laplacian Enhanced Mamba for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2508.09179)
*Hongli Chen,Pengcheng Fang,Yuxia Chen,Yingxuan Ren,Jing Hao,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: eess.IV

TL;DR: HiFi-Mamba是一种新型双流Mamba架构，用于高保真MRI重建，解决了现有方法对高频细节不敏感和扫描冗余的问题。

- Motivation: 现有Mamba变体在MRI重建中存在对高频解剖细节不敏感和依赖冗余多向扫描的局限性。
- Method: 提出HiFi-Mamba，结合W-Laplacian块进行频谱解耦，并通过自适应状态空间调制选择性整合高频特征。
- Result: 在标准MRI重建基准测试中，HiFi-Mamba优于CNN、Transformer和其他Mamba模型。
- Conclusion: HiFi-Mamba在保持高效设计的同时，显著提升了重建精度。


### [135] [MedPatch: Confidence-Guided Multi-Stage Fusion for Multimodal Clinical Data](https://arxiv.org/abs/2508.09182)
*Baraa Al Jorf,Farah Shamout*

Main category: eess.IV

TL;DR: MedPatch是一种多阶段多模态融合架构，通过置信度引导的补丁技术整合临床数据，显著提升临床预测任务的性能。

- Motivation: 解决医疗数据异构性、规模小和模态缺失导致的模型性能限制问题。
- Method: 提出MedPatch，包含多阶段融合策略、缺失感知模块和联合融合模块。
- Result: 在院内死亡预测和临床条件分类任务中达到最优性能。
- Conclusion: 置信度引导的多阶段融合能有效处理多模态数据的异构性，为临床预测任务设定了新基准。


### [136] [Hybrid(Transformer+CNN)-based Polyp Segmentation](https://arxiv.org/abs/2508.09189)
*Madan Baduwal*

Main category: eess.IV

TL;DR: 提出了一种结合Transformer和CNN的混合模型，用于提高结肠息肉分割的鲁棒性，解决了边界模糊和内窥镜伪影的挑战。

- Motivation: 结肠息肉分割因息肉大小、形状、内窥镜类型和成像协议的多样性以及边界模糊而极具挑战性。
- Method: 采用混合（Transformer + CNN）架构，通过边界感知注意力机制和鲁棒特征提取提升性能。
- Result: 在分割准确率（召回率提升1.76%，准确率提升0.07%）和伪影鲁棒性上显著优于现有方法。
- Conclusion: 混合模型在息肉分割任务中表现出色，尤其适用于复杂场景。


### [137] [impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction](https://arxiv.org/abs/2508.09195)
*Maria Boyko,Aleksandra Beliaeva,Dmitriy Kornilov,Alexander Bernstein,Maxim Sharaev*

Main category: eess.IV

TL;DR: 提出了一种基于Transformer的多模态预训练方法impuTMAE，用于处理医学数据中的缺失模态，并在胶质瘤生存预测中取得最优性能。

- Motivation: 医学数据复杂且常缺失模态，现有方法难以有效处理，需开发新方法提升多模态模型的性能。
- Method: impuTMAE通过重建掩码补丁学习模态间和模态内交互，同时填补缺失模态，预训练后微调用于胶质瘤生存预测。
- Result: 在TCGA-GBM/LGG和BraTS数据集上，impuTMAE超越现有方法，达到最优性能。
- Conclusion: impuTMAE能有效处理缺失数据，提升多模态模型的预测能力，为医学研究提供新工具。


### [138] [FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation](https://arxiv.org/abs/2508.09196)
*Asim Ukaye,Numan Saeed,Karthik Nandakumar*

Main category: eess.IV

TL;DR: 提出了一种新颖的联邦学习方法，利用模型不确定性进行聚合和预测不确定性进行推理，实现跨多样腹部CT数据集的通用分割。

- Motivation: 解决不同CT数据集因扫描器和设置差异导致的器官标签不统一问题，同时保护患者隐私。
- Method: 通过随机小批量梯度下降估计模型权重分布，利用贝叶斯逆方差聚合方案在服务器端聚合参数，并量化预测不确定性。
- Result: 实验证明该方法在联邦聚合质量和不确定性加权推理方面优于现有基线。
- Conclusion: 该方法有效提升了跨数据集分割的通用性和临床决策的可靠性。


### [139] [Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction](https://arxiv.org/abs/2508.09200)
*Jinho Kim,Marcel Dominik Nickel,Florian Knoll*

Main category: eess.IV

TL;DR: 研究探讨了零样本自监督学习重建在减少磁共振胰胆管造影（MRCP）屏气时间中的可行性，结果显示其显著提升图像质量并缩短训练时间。

- Motivation: 减少MRCP检查中的屏气时间，提高患者舒适度和临床效率。
- Method: 采用零样本重建方法，结合预训练网络减少训练时间，对比并行成像和压缩感知重建。
- Result: 零样本重建显著提升图像质量，训练时间从271分钟缩短至11分钟。
- Conclusion: 零样本学习为临床提供了高效且高质量的MRCP重建方案。


### [140] [From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations](https://arxiv.org/abs/2508.09205)
*Yoni Schirris,Eric Marcus,Jonas Teuwen,Hugo Horlings,Efstratios Gavves*

Main category: eess.IV

TL;DR: 论文提出了一种人机交互系统，用于解释计算病理学中的分类器，结合视觉语言模型验证解释的有效性。

- Motivation: 解释深度学习模型对医学图像分析系统的临床整合至关重要，以避免模型依赖虚假特征或发现新的生物学见解。
- Method: 提出了一种人机-VLM交互系统，包括AI集成的幻灯片查看器和通用视觉语言模型的量化方法。
- Result: 系统能够定性测试解释的合理性，并量化区分不同解释。
- Conclusion: 为从可解释AI到已解释AI提供了实用路径，适用于数字病理学等领域。


### [141] [AMRG: Extend Vision Language Models for Automatic Mammography Report Generation](https://arxiv.org/abs/2508.09225)
*Nak-Jun Sung,Donghyun Lee,Bo Hwa Choi,Chae Jung Park*

Main category: eess.IV

TL;DR: AMRG是首个基于大型视觉语言模型（VLM）的端到端框架，用于生成乳腺X光检查报告，通过参数高效微调（PEFT）策略实现高性能。

- Motivation: 乳腺X光检查报告生成在医学AI中是一个关键但未充分探索的任务，面临多视图图像推理、高分辨率视觉线索和非结构化放射学语言等挑战。
- Method: 基于MedGemma-4B-it模型，采用低秩适应（LoRA）进行参数高效微调，训练和评估在公开数据集DMID上进行。
- Result: 在语言生成和临床指标上表现优异，ROUGE-L为0.5691，METEOR为0.6152，CIDEr为0.5818，BI-RADS准确率为0.5582。
- Conclusion: AMRG为放射学报告生成提供了可扩展和适应性强的解决方案，推动了多模态医学AI的未来研究。


### [142] [Dynamic Survival Prediction using Longitudinal Images based on Transformer](https://arxiv.org/abs/2508.09328)
*Bingfan Liu,Haolun Shi,Jiguo Cao*

Main category: eess.IV

TL;DR: SurLonFormer是一种基于Transformer的神经网络，结合纵向医学影像和结构化数据用于生存预测，解决了现有方法在利用截尾数据、时间相关性及可解释性上的不足。

- Motivation: 现有生存分析方法未能充分利用截尾数据，忽视纵向图像间的相关性，且缺乏可解释性。
- Method: 提出SurLonFormer，包含视觉编码器（提取空间特征）、序列编码器（聚合时间信息）和基于Cox比例风险模型的生存编码器。
- Result: 在模拟和阿尔茨海默病实际应用中，SurLonFormer表现出优越的预测性能，并成功识别疾病相关影像生物标志物。
- Conclusion: SurLonFormer有效整合了纵向医学影像和结构化数据，提升了生存预测的准确性和可解释性。


### [143] [T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis](https://arxiv.org/abs/2508.09919)
*Xiaojiao Xiao,Jianfeng Zhao,Qinmin Vivian Hu,Guanghui Wang*

Main category: eess.IV

TL;DR: 提出了一种名为T-CACE的框架，用于从非对比MRI合成多相增强MRI，解决了传统MRI的局限性。

- Motivation: 传统MRI存在对比剂风险、耗时的手动评估和有限标注数据的问题，T-CACE旨在提供更安全、高效的替代方案。
- Method: T-CACE采用条件令牌编码（CTE）和动态时间感知注意力掩码（DTAM），结合时间分类一致性约束（TCC），实现多相MRI合成。
- Result: 在两个独立肝脏MRI数据集上的实验表明，T-CACE在图像合成、分割和病变分类上优于现有方法。
- Conclusion: T-CACE为肝脏病变评估提供了临床相关的高效替代方案，提升了安全性、诊断效率和可靠性。
## astro-ph.IM

### [144] [Robustness analysis of Deep Sky Objects detection models on HPC](https://arxiv.org/abs/2508.09831)
*Olivier Parisot,Diogo Ramalho Fernandes*

Main category: astro-ph.IM

TL;DR: 论文提出利用计算机视觉和深度学习技术，结合高性能计算（HPC），自动化检测深空天体（如星系、星云和星团），并比较了不同检测模型（YOLO、RET-DETR）的性能。

- Motivation: 随着天文观测和业余天文学家的参与增加，天空图像数量激增，需要准确且鲁棒的自动化处理方法。深空天体因信号微弱和背景复杂，检测仍具挑战性。
- Method: 使用高性能计算（HPC）并行化计算，训练并比较了YOLO和RET-DETR等检测模型在智能望远镜图像上的表现，重点测试鲁棒性。
- Result: 论文展示了不同模型在深空天体检测任务中的性能表现，并验证了HPC在提升计算效率方面的作用。
- Conclusion: 计算机视觉和深度学习结合HPC技术，能够有效提升深空天体检测的自动化水平和鲁棒性。
## cs.IR

### [145] [Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations](https://arxiv.org/abs/2508.09789)
*Marco De Nadai,Andreas Damianou,Mounia Lalmas*

Main category: cs.IR

TL;DR: 论文提出了一种利用多模态大语言模型（MLLM）生成视频自然语言描述的方法，以增强视频推荐系统的语义理解能力。

- Motivation: 现有视频推荐系统依赖低层次特征或用户定义的元数据，缺乏对视频深层语义（如意图、幽默等）的捕捉。
- Method: 提出一个无需微调的框架，通过MLLM生成视频的丰富自然语言描述，并结合文本编码器输入推荐系统。
- Result: 在MicroLens-100K数据集上，该方法在五种代表性模型中优于传统特征。
- Conclusion: MLLM可作为动态知识提取器，提升视频推荐系统的意图感知能力。
## cs.CL

### [146] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本文综述了针对传统Transformer架构计算量大、难以大规模训练的问题，探讨了高效的LLM架构创新，包括线性与稀疏序列建模、注意力变体、混合模型等，并展望了未来高效AI系统的发展方向。

- Motivation: 传统Transformer架构在计算资源和训练规模上存在显著限制，亟需创新的高效LLM架构以提升性能和实用性。
- Method: 系统梳理了线性与稀疏序列建模、高效注意力变体、稀疏专家混合、混合架构及扩散LLM等技术。
- Result: 提出了现代高效LLM架构的分类框架，为未来研究提供了技术蓝图。
- Conclusion: 高效LLM架构的研究将推动更高效、多功能的AI系统发展。


### [147] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: VisCodex是一个统一框架，通过融合视觉和编码语言模型，提升多模态大语言模型（MLLMs）的多模态代码生成能力。

- Motivation: 当前MLLMs在多模态输入生成代码方面能力有限，需要更强大的解决方案。
- Method: 采用任务向量模型融合技术，将先进编码LLM集成到视觉语言骨干中，并引入Multimodal Coding Dataset（MCD）和InfiBench-V基准。
- Result: VisCodex在开源MLLMs中表现最佳，接近GPT-4o等专有模型。
- Conclusion: 模型融合策略和新数据集有效提升了多模态代码生成能力。
## q-bio.QM

### [148] [Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics](https://arxiv.org/abs/2508.09215)
*Kerem Delikoyun,Qianyu Chen,Liu Wei,Si Ko Myo,Johannes Krell,Martin Schlegel,Win Sen Kuan,John Tshon Yit Soong,Gerhard Schneider,Clarissa Prazeres da Costa,Percy A. Knolle,Laurent Renia,Matthew Edward Cove,Hwee Kuan Lee,Klaus Diepold,Oliver Hayden*

Main category: q-bio.QM

TL;DR: RT-HAD是一种基于深度学习的端到端图像和数据处理框架，用于离轴数字全息显微镜（DHM），能够实时处理大量图像数据，快速识别血液细胞聚集体。

- Motivation: 传统流式细胞仪无法有效识别血液细胞聚集体，而定量相位成像流式细胞术虽能捕捉聚集体形态，但数据存储和处理问题限制了其临床应用。RT-HAD旨在解决这些问题，提升无标记功能诊断的效率。
- Method: RT-HAD结合物理一致的全息重建和检测，将每个血细胞表示为图以识别聚集体，实时处理超过30 GB的图像数据，耗时少于1.5分钟。
- Result: RT-HAD在血小板聚集体检测中的错误率为8.9%，符合血液学生物标志物的实验室可接受误差范围，解决了即时诊断中的大数据挑战。
- Conclusion: RT-HAD为血液细胞聚集体的实时检测提供了一种高效解决方案，显著提升了无标记诊断的潜力。
## cs.CR

### [149] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为LoD的无监督框架，通过异常检测方法解决大型视觉语言模型（LVLM）的越狱攻击检测问题，显著提升了性能。

- Motivation: 尽管已有大量对齐努力，大型视觉语言模型仍易受越狱攻击，现有检测方法依赖启发式规则，性能不佳。
- Method: LoD框架包含多模态安全概念激活向量（MSCAV）和安全模式自动编码器，通过建模安全输入的分布来检测异常。
- Result: 在三个LVLM和五个基准测试中，LoD的平均AUROC达到0.9951，性能提升高达38.89%。
- Conclusion: LoD通过无监督异常检测方法，实现了对越狱攻击的高效统一检测，性能优于现有方法。
