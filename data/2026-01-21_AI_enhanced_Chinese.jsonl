{"id": "2601.11612", "pdf": "https://arxiv.org/pdf/2601.11612", "abs": "https://arxiv.org/abs/2601.11612", "authors": ["Arnav S. Sonavane"], "title": "Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 4 figures, 9 tables", "summary": "We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT", "AI": {"tldr": "\u5728\u519c\u4e1a\u75c5\u5bb3\u5206\u7c7b\u4e2d\uff0c\u9886\u57df\u7279\u5b9a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08SimCLR\uff09\u6bd4\u5c42\u6b21\u67b6\u6784\u8bbe\u8ba1\u5e26\u6765\u66f4\u5927\u7684\u7cbe\u5ea6\u63d0\u5347\uff0c\u4e14\u8be5\u4f18\u52bf\u662f\u67b6\u6784\u65e0\u5173\u7684\u3002", "motivation": "\u7814\u7a76\u9886\u57df\u7279\u5b9a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5bf9\u519c\u4e1a\u75c5\u5bb3\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u9884\u8bad\u7ec3\u4e0e\u67b6\u6784\u8bbe\u8ba1\u5bf9\u6027\u80fd\u7684\u76f8\u5bf9\u8d21\u732e\u3002", "method": "\u4f7f\u7528\u5c42\u6b21\u89c6\u89c9\u53d8\u6362\u5668\uff08HierarchicalViT\uff0cHVT\uff09\uff0c\u5728\u4e09\u4e2a\u519c\u4e1a\u75c5\u5bb3\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30SimCLR\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u5e76\u4e0eSwin-Base\u3001ViT-Base\u7b49\u67b6\u6784\u5bf9\u6bd4\u3002", "result": "SimCLR\u9884\u8bad\u7ec3\u4ec5\u75283000\u5f20\u65e0\u6807\u7b7e\u519c\u4e1a\u56fe\u50cf\u5373\u53ef\u5e26\u6765+4.57%\u7684\u7cbe\u5ea6\u63d0\u5347\uff0c\u8d85\u8fc7\u5c42\u6b21\u67b6\u6784\u8bbe\u8ba1\u7684+3.70%\u589e\u76ca\u3002\u8be5\u9884\u8bad\u7ec3\u4f18\u52bf\u5728Swin-Base\uff08+4.08%\uff09\u548cViT-Base\uff08+4.20%\uff09\u4e0a\u540c\u6837\u5b58\u5728\uff0c\u8bc1\u660e\u5176\u67b6\u6784\u65e0\u5173\u6027\u3002HVT-Base\u5728\u53c2\u6570\u91cf\u76f8\u8fd1\u65f6\u6bd4Swin-Base\u9ad8+1.68%\u7cbe\u5ea6\u3002", "conclusion": "\u5728\u519c\u4e1a\u75c5\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u9886\u57df\u7279\u5b9a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6bd4\u67b6\u6784\u9009\u62e9\u66f4\u91cd\u8981\uff0c\u5b9e\u8df5\u8005\u5e94\u4f18\u5148\u6536\u96c6\u9886\u57df\u6570\u636e\u800c\u975e\u7ea0\u7ed3\u4e8e\u67b6\u6784\u8bbe\u8ba1\u3002"}}
{"id": "2601.11614", "pdf": "https://arxiv.org/pdf/2601.11614", "abs": "https://arxiv.org/abs/2601.11614", "authors": ["Jason Qiu"], "title": "Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning", "categories": ["cs.CV", "cs.LG", "q-bio.NC"], "comment": "19 pages, 10 figures", "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.", "AI": {"tldr": "\u63d0\u51fa3D TransUNet\u6846\u67b6\uff0c\u4ece\u5e38\u89c4T1\u52a0\u6743MRI\u9884\u6d4b\u6269\u6563MRI\u7684FA\u548cMD\u56fe\uff0c\u63d0\u5347\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u8bca\u65ad\u51c6\u786e\u7387", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e38\u89c4T1\u52a0\u6743MRI\u53ea\u80fd\u68c0\u6d4b\u665a\u671f\u5b8f\u89c2\u53d8\u5316\uff0c\u800c\u6269\u6563MRI\u80fd\u68c0\u6d4b\u65e9\u671f\u5fae\u89c2\u5f02\u5e38\u4f46\u626b\u63cf\u65f6\u95f4\u957f\u4e14\u6613\u53d7\u8fd0\u52a8\u4f2a\u5f71\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528", "method": "\u4f7f\u75283D TransUNet\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4eceT1\u52a0\u6743MRI\u9884\u6d4b\u6269\u6563MRI\u7684\u5206\u6570\u5404\u5411\u5f02\u6027\uff08FA\uff09\u548c\u5e73\u5747\u6269\u6563\u7387\uff08MD\uff09\u56fe", "result": "\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cfFA\u548cMD\u56fe\uff0c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u8d85\u8fc70.93\uff0c\u4e0e\u771f\u5b9e\u6269\u6563MRI\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570>0.94\uff1b\u5408\u6210\u7279\u5f81\u4f7fAD\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u53475%\uff0878.75%\u219283.75%\uff09\uff0c\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u63d0\u534712.5%", "conclusion": "\u4ece\u5e38\u89c4T1\u52a0\u6743MRI\u53ef\u4ee5\u63a8\u65ad\u9ad8\u8d28\u91cf\u7684\u6269\u6563\u5fae\u89c2\u7ed3\u6784\u4fe1\u606f\uff0c\u5c06\u591a\u6a21\u6001\u6210\u50cf\u4f18\u52bf\u6269\u5c55\u5230\u65e0\u6cd5\u83b7\u53d6\u6269\u6563\u6570\u636e\u7684\u573a\u666f\uff0c\u6709\u671b\u63d0\u9ad8AD\u8bca\u65ad\u7684\u53ef\u53ca\u6027\u3001\u6548\u7387\u548c\u51c6\u786e\u6027"}}
{"id": "2601.11617", "pdf": "https://arxiv.org/pdf/2601.11617", "abs": "https://arxiv.org/abs/2601.11617", "authors": ["Xu Wang", "Boyao Han", "Xiaojun Chen", "Ying Liu", "Ruihui Li"], "title": "PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": null, "summary": "Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.", "AI": {"tldr": "PointSLAM++\uff1a\u57fa\u4e8e\u5c42\u6b21\u7ea6\u675f\u795e\u7ecf\u9ad8\u65af\u8868\u793a\u7684RGB-D SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u6e10\u8fdb\u59ff\u6001\u4f18\u5316\u548c\u52a8\u6001\u795e\u7ecf\u8868\u793a\u56fe\uff0c\u5728\u6df1\u5ea6\u566a\u58f0\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e09\u7ef4\u91cd\u5efa\u548c\u903c\u771f\u6e32\u67d3\u3002", "motivation": "\u5f53\u524dSLAM\u65b9\u6cd5\u5728\u6df1\u5ea6\u566a\u58f0\u5b58\u5728\u65f6\u96be\u4ee5\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u800c\u5b9e\u65f6\u4e09\u7ef4\u91cd\u5efa\u5bf9\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u4f7f\u7528\u5c42\u6b21\u7ea6\u675f\u795e\u7ecf\u9ad8\u65af\u8868\u793a\u4fdd\u6301\u7ed3\u6784\u5173\u7cfb\u5e76\u751f\u6210\u9ad8\u65af\u57fa\u5143\uff1b2. \u91c7\u7528\u6e10\u8fdb\u59ff\u6001\u4f18\u5316\u51cf\u8f7b\u6df1\u5ea6\u4f20\u611f\u5668\u566a\u58f0\uff1b3. \u5229\u7528\u52a8\u6001\u795e\u7ecf\u8868\u793a\u56fe\u6839\u636e\u5c40\u90e8\u51e0\u4f55\u590d\u6742\u5ea6\u8c03\u6574\u9ad8\u65af\u8282\u70b9\u5206\u5e03\u3002", "result": "PointSLAM++\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e3DGS\u7684SLAM\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5927\u89c4\u6a21AR\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "PointSLAM++\u901a\u8fc7\u521b\u65b0\u7684\u795e\u7ecf\u8868\u793a\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u4e09\u7ef4\u5efa\u56fe\u548c\u903c\u771f\u7684\u573a\u666f\u6e32\u67d3\uff0c\u4e3a\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u65f6\u4e09\u7ef4\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11627", "pdf": "https://arxiv.org/pdf/2601.11627", "abs": "https://arxiv.org/abs/2601.11627", "authors": ["Hassan Ugail", "Jan Ritch-Frel", "Irina Matuzava"], "title": "Handcrafted Feature-Assisted One-Class Learning for Artist Authentication in Historical Drawings", "categories": ["cs.CV"], "comment": null, "summary": "Authentication and attribution of works on paper remain persistent challenges in cultural heritage, particularly when the available reference corpus is small and stylistic cues are primarily expressed through line and limited tonal variation. We present a verification-based computational framework for historical drawing authentication using one-class autoencoders trained on a compact set of interpretable handcrafted features. Ten artist-specific verifiers are trained using authenticated sketches from the Metropolitan Museum of Art open-access collection, the Ashmolean Collections Catalogue, the Morgan Library and Museum, the Royal Collection Trust (UK), the Victoria and Albert Museum Collections, and an online catalogue of the Casa Buonarroti collection and evaluated under a biometric-style protocol with genuine and impostor trials. Feature vectors comprise Fourier-domain energy, Shannon entropy, global contrast, GLCM-based homogeneity, and a box-counting estimate of fractal complexity. Across 900 verification decisions (90 genuine and 810 impostor trials), the pooled system achieves a True Acceptance Rate of 83.3% with a False Acceptance Rate of 9.5% at the chosen operating point. Performance varies substantially by artist, with near-zero false acceptance for some verifiers and elevated confusability for others. A pairwise attribution of false accepts indicates structured error pathways consistent with stylistic proximity and shared drawing conventions, whilst also motivating tighter control of digitisation artefacts and threshold calibration. The proposed methodology is designed to complement, rather than replace, connoisseurship by providing reproducible, quantitative evidence suitable for data-scarce settings common in historical sketch attribution.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5355\u7c7b\u81ea\u7f16\u7801\u5668\u7684\u5386\u53f2\u7ed8\u753b\u8ba4\u8bc1\u6846\u67b6\uff0c\u4f7f\u7528\u624b\u5de5\u7279\u5f81\u5728\u5c0f\u578b\u53c2\u8003\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728900\u6b21\u9a8c\u8bc1\u4e2d\u8fbe\u523083.3%\u771f\u63a5\u53d7\u7387\u548c9.5%\u5047\u63a5\u53d7\u7387", "motivation": "\u7eb8\u8d28\u4f5c\u54c1\u7684\u8eab\u4efd\u9a8c\u8bc1\u548c\u5f52\u5c5e\u9274\u5b9a\u5728\u6587\u5316\u9057\u4ea7\u9886\u57df\u9762\u4e34\u6301\u7eed\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u53c2\u8003\u8bed\u6599\u5e93\u8f83\u5c0f\u4e14\u98ce\u683c\u7ebf\u7d22\u4e3b\u8981\u901a\u8fc7\u7ebf\u6761\u548c\u6709\u9650\u7684\u8272\u8c03\u53d8\u5316\u8868\u8fbe\u65f6", "method": "\u4f7f\u7528\u5355\u7c7b\u81ea\u7f16\u7801\u5668\u8bad\u7ec3\u5386\u53f2\u7ed8\u753b\u8ba4\u8bc1\u9a8c\u8bc1\u5668\uff0c\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\uff08\u5085\u91cc\u53f6\u57df\u80fd\u91cf\u3001\u9999\u519c\u71b5\u3001\u5168\u5c40\u5bf9\u6bd4\u5ea6\u3001GLCM\u540c\u8d28\u6027\u3001\u5206\u5f62\u590d\u6742\u5ea6\uff09\uff0c\u5728\u591a\u4e2a\u535a\u7269\u9986\u7684\u771f\u5b9e\u7d20\u63cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec310\u4e2a\u827a\u672f\u5bb6\u7279\u5b9a\u9a8c\u8bc1\u5668", "result": "\u5728900\u6b21\u9a8c\u8bc1\u51b3\u7b56\u4e2d\uff0c\u7cfb\u7edf\u5728\u9009\u5b9a\u64cd\u4f5c\u70b9\u8fbe\u523083.3%\u771f\u63a5\u53d7\u7387\u548c9.5%\u5047\u63a5\u53d7\u7387\u3002\u6027\u80fd\u56e0\u827a\u672f\u5bb6\u800c\u5f02\uff0c\u90e8\u5206\u9a8c\u8bc1\u5668\u5047\u63a5\u53d7\u7387\u63a5\u8fd1\u96f6\uff0c\u90e8\u5206\u5219\u5b58\u5728\u6df7\u6dc6", "conclusion": "\u8be5\u65b9\u6cd5\u65e8\u5728\u8865\u5145\u800c\u975e\u53d6\u4ee3\u9274\u8d4f\u5bb6\u5224\u65ad\uff0c\u4e3a\u5386\u53f2\u7d20\u63cf\u5f52\u5c5e\u9274\u5b9a\u4e2d\u5e38\u89c1\u7684\u6570\u636e\u7a00\u7f3a\u573a\u666f\u63d0\u4f9b\u53ef\u91cd\u590d\u7684\u5b9a\u91cf\u8bc1\u636e\uff0c\u5047\u63a5\u53d7\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u98ce\u683c\u63a5\u8fd1\u6027\u548c\u5171\u4eab\u7ed8\u753b\u60ef\u4f8b\u4e00\u81f4\u7684\u7ed3\u6784\u5316\u9519\u8bef\u8def\u5f84"}}
{"id": "2601.11630", "pdf": "https://arxiv.org/pdf/2601.11630", "abs": "https://arxiv.org/abs/2601.11630", "authors": ["Haonan Wei", "Linyuan Wang", "Nuolin Sun", "Zhizhong Zheng", "Lei Li", "Bin Yan"], "title": "A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow", "categories": ["cs.CV"], "comment": null, "summary": "Currently, Flow matching methods aim to compress the iterative generation process of diffusion models into a few or even a single step, with MeanFlow and FreeFlow being representative achievements of one-step generation based on Ordinary Differential Equations (ODEs). We observe that the 28-layer Transformer architecture of FreeFlow can be characterized as an Euler discretization scheme for an ODE along the depth axis, where the layer index serves as the discrete time step. Therefore, we distill the number of layers of the FreeFlow model, following the same derivation logic as FreeFlow, and propose SLT (Single-Layer Transformer), which uses a single shared DiT block to approximate the depth-wise feature evolution of the 28-layer teacher. During training, it matches the teacher's intermediate features at several depth patches, fuses those patch-level representations, and simultaneously aligns the teacher's final velocity prediction. Through distillation training, we compress the 28 independent Transformer Blocks of the teacher model DiT-XL/2 into a single Transformer Block, reducing the parameter count from 675M to 4.3M. Furthermore, leveraging its minimal parameters and rapid sampling speed, SLT can screen more candidate points in the noise space within the same timeframe, thereby selecting higher-quality initial points for the teacher model FreeFlow and ultimately enhancing the quality of generated images. Experimental results demonstrate that within a time budget comparable to two random samplings of the teacher model, our method performs over 100 noise screenings and produces a high-quality sample through the teacher model using the selected points. Quality fluctuations caused by low-quality initial noise under a limited number of FreeFlow sampling calls are effectively avoided, substantially improving the stability and average generation quality of one-step generation.", "AI": {"tldr": "\u63d0\u51faSLT\uff08\u5355\u5c42Transformer\uff09\uff0c\u901a\u8fc7\u84b8\u998f\u5c06FreeFlow\u768428\u5c42Transformer\u538b\u7f29\u4e3a\u5355\u4e2a\u5171\u4eabDiT\u5757\uff0c\u53c2\u6570\u91cf\u4ece675M\u964d\u81f34.3M\uff0c\u5e76\u5229\u7528\u5176\u5feb\u901f\u91c7\u6837\u80fd\u529b\u7b5b\u9009\u9ad8\u8d28\u91cf\u521d\u59cb\u566a\u58f0\u70b9\uff0c\u63d0\u5347\u4e00\u6b65\u751f\u6210\u7684\u7a33\u5b9a\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u6d41\u5339\u914d\u65b9\u6cd5\u65e8\u5728\u5c06\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\u538b\u7f29\u5230\u5c11\u6570\u751a\u81f3\u5355\u6b65\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982FreeFlow\u4ecd\u4f7f\u752828\u5c42Transformer\u67b6\u6784\u3002\u89c2\u5bdf\u5230\u8be5\u67b6\u6784\u53ef\u89c6\u4e3aODE\u6cbf\u6df1\u5ea6\u8f74\u7684\u6b27\u62c9\u79bb\u6563\u5316\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7\u84b8\u998f\u51cf\u5c11\u5c42\u6570\uff0c\u540c\u65f6\u5229\u7528\u8f7b\u91cf\u6a21\u578b\u5feb\u901f\u7b5b\u9009\u9ad8\u8d28\u91cf\u521d\u59cb\u566a\u58f0\u70b9\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "\u5c06FreeFlow\u768428\u5c42Transformer\u89c6\u4e3aODE\u7684\u6df1\u5ea6\u79bb\u6563\u5316\uff0c\u84b8\u998f\u51faSLT\uff08\u5355\u5c42Transformer\uff09\uff1a1\uff09\u4f7f\u7528\u5355\u4e2a\u5171\u4eabDiT\u5757\u8fd1\u4f3c28\u5c42\u6559\u5e08\u6a21\u578b\u7684\u6df1\u5ea6\u7279\u5f81\u6f14\u5316\uff1b2\uff09\u8bad\u7ec3\u65f6\u5339\u914d\u6559\u5e08\u4e2d\u95f4\u7279\u5f81\u5728\u591a\u4e2a\u6df1\u5ea6\u8865\u4e01\u7684\u8868\u793a\uff1b3\uff09\u878d\u5408\u8865\u4e01\u7ea7\u8868\u793a\u5e76\u5bf9\u9f50\u6559\u5e08\u7684\u6700\u7ec8\u901f\u5ea6\u9884\u6d4b\uff1b4\uff09\u5229\u7528SLT\u5feb\u901f\u91c7\u6837\u80fd\u529b\u5728\u566a\u58f0\u7a7a\u95f4\u7b5b\u9009\u5019\u9009\u70b9\uff0c\u4e3a\u6559\u5e08\u6a21\u578b\u9009\u62e9\u9ad8\u8d28\u91cf\u521d\u59cb\u70b9\u3002", "result": "\u6210\u529f\u5c06\u53c2\u6570\u91cf\u4ece675M\u538b\u7f29\u81f34.3M\uff08\u51cf\u5c1199.4%\uff09\u3002\u5728\u76f8\u5f53\u4e8e\u6559\u5e08\u6a21\u578b\u4e24\u6b21\u968f\u673a\u91c7\u6837\u7684\u65f6\u95f4\u9884\u7b97\u5185\uff0c\u80fd\u8fdb\u884c\u8d85\u8fc7100\u6b21\u566a\u58f0\u7b5b\u9009\uff0c\u5e76\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u4f7f\u7528\u9009\u5b9a\u70b9\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u3002\u6709\u6548\u907f\u514d\u4e86\u6709\u9650\u91c7\u6837\u6b21\u6570\u4e0b\u4f4e\u8d28\u91cf\u521d\u59cb\u566a\u58f0\u5f15\u8d77\u7684\u8d28\u91cf\u6ce2\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e00\u6b65\u751f\u6210\u7684\u7a33\u5b9a\u6027\u548c\u5e73\u5747\u8d28\u91cf\u3002", "conclusion": "SLT\u901a\u8fc7\u84b8\u998f\u5b9e\u73b0\u4e86\u6781\u81f4\u7684\u6a21\u578b\u538b\u7f29\uff0c\u540c\u65f6\u5229\u7528\u8f7b\u91cf\u6a21\u578b\u7684\u5feb\u901f\u91c7\u6837\u80fd\u529b\u4f18\u5316\u521d\u59cb\u566a\u58f0\u9009\u62e9\uff0c\u4e3a\u4e00\u6b65\u751f\u6210\u63d0\u4f9b\u4e86\u7a33\u5b9a\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2601.11631", "pdf": "https://arxiv.org/pdf/2601.11631", "abs": "https://arxiv.org/abs/2601.11631", "authors": ["Yurun Song", "Jiong Yin", "Rongjunchen Zhang", "Ian G. Harris"], "title": "Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents", "categories": ["cs.CV"], "comment": null, "summary": "Multi-turn GUI agents enable complex task completion through sequential decision-making, but suffer from severe context inflation as interaction history accumulates. Existing strategies either sacrifice long-term context via truncation or compromise spatial structure through token pruning. In this paper, we propose Coordinate Compression Policy Optimization (CCPO), an efficient policy optimization framework that couples visual compression with policy optimization for multi-turn GUI agents. CCPO introduces Coordinate-Aware Spatial Compression (CASC), which aggregates coordinates from multiple rollouts to capture target-relevant regions and progressively narrow historical attention around key visual areas. From interactions across rollouts, CASC adaptively constructs attention boundaries that concentrate computation on the most informative regions of the scene. We further design a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness, improving both grounding accuracy and compression quality. Extensive experiments demonstrate that CCPO achieves SOTA performance across four benchmarks with up to 55% token compression and 3.8$\\times$ training speedup.", "AI": {"tldr": "CCPO\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u8f6eGUI\u4ee3\u7406\u7684\u9ad8\u6548\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5750\u6807\u611f\u77e5\u7a7a\u95f4\u538b\u7f29\u548c\u57fa\u4e8e\u8ddd\u79bb\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe55%\u7684token\u538b\u7f29\u548c3.8\u500d\u8bad\u7ec3\u52a0\u901f\u3002", "motivation": "\u591a\u8f6eGUI\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u4f1a\u79ef\u7d2f\u5927\u91cf\u4ea4\u4e92\u5386\u53f2\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u901a\u8fc7\u622a\u65ad\u727a\u7272\u957f\u671f\u4e0a\u4e0b\u6587\uff0c\u8981\u4e48\u901a\u8fc7token\u526a\u679d\u7834\u574f\u7a7a\u95f4\u7ed3\u6784\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u53c8\u80fd\u9ad8\u6548\u5904\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u5750\u6807\u538b\u7f29\u7b56\u7565\u4f18\u5316\uff08CCPO\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u5750\u6807\u611f\u77e5\u7a7a\u95f4\u538b\u7f29\uff08CASC\uff09\uff0c\u901a\u8fc7\u805a\u5408\u591a\u4e2arollout\u7684\u5750\u6807\u4fe1\u606f\u6765\u6355\u83b7\u76ee\u6807\u76f8\u5173\u533a\u57df\uff0c\u5e76\u9010\u6b65\u7f29\u5c0f\u5386\u53f2\u6ce8\u610f\u529b\u8303\u56f4\uff1b2\uff09\u57fa\u4e8e\u8ddd\u79bb\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u63d0\u4f9b\u57fa\u4e8e\u8ddd\u79bb\u800c\u975e\u4e8c\u5143\u6b63\u786e\u6027\u7684\u7ec6\u7c92\u5ea6\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b9e\u73b0\u9ad8\u8fbe55%\u7684token\u538b\u7f29\u548c3.8\u500d\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u63a5\u5730\u51c6\u786e\u6027\u548c\u538b\u7f29\u8d28\u91cf\u3002", "conclusion": "CCPO\u901a\u8fc7\u5c06\u89c6\u89c9\u538b\u7f29\u4e0e\u7b56\u7565\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6eGUI\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.11632", "pdf": "https://arxiv.org/pdf/2601.11632", "abs": "https://arxiv.org/abs/2601.11632", "authors": ["Zhiyang Li", "Ao Ke", "Yukun Cao", "Xike Xie"], "title": "KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.", "AI": {"tldr": "KG-ViP\u901a\u8fc7\u878d\u5408\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\u6765\u89e3\u51b3MLLMs\u5728VQA\u4e2d\u7684\u77e5\u8bc6\u5e7b\u89c9\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86VQA\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u77e5\u8bc6\u5e7b\u89c9\uff08\u751f\u6210\u4e0e\u89c6\u89c9\u5185\u5bb9\u65e0\u5173\u7684\u77e5\u8bc6\uff09\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\u5206\u522b\u80fd\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5c06\u5b83\u4eec\u5b64\u7acb\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u7684\u534f\u540c\u6f5c\u529b\u3002", "method": "\u63d0\u51faKG-ViP\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22-\u878d\u5408\u6d41\u7a0b\u5c06\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\u878d\u5408\u3002\u4f7f\u7528\u67e5\u8be2\u4f5c\u4e3a\u8bed\u4e49\u6865\u6881\uff0c\u9010\u6b65\u6574\u5408\u4e24\u79cd\u56fe\u7ed3\u6784\uff0c\u751f\u6210\u7edf\u4e00\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u652f\u6301\u53ef\u9760\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002", "result": "\u5728FVQA 2.0+\u548cMVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKG-ViP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684VQA\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u878d\u5408\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\u7684\u878d\u5408\u80fd\u591f\u6709\u6548\u89e3\u51b3MLLMs\u5728VQA\u4e2d\u7684\u77e5\u8bc6\u5e7b\u89c9\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u95ee\u9898\uff0cKG-ViP\u6846\u67b6\u5c55\u793a\u4e86\u8fd9\u79cd\u878d\u5408\u7b56\u7565\u7684\u534f\u540c\u4f18\u52bf\u3002"}}
{"id": "2601.11633", "pdf": "https://arxiv.org/pdf/2601.11633", "abs": "https://arxiv.org/abs/2601.11633", "authors": ["Xuchen Li", "Xuzhao Li", "Renjie Pi", "Shiyu Hu", "Jian Zhao", "Jiahui Gao"], "title": "Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images", "categories": ["cs.CV"], "comment": "Preprint, Under review", "summary": "Despite the remarkable progress of Vision-Language Models (VLMs) in adopting \"Thinking-with-Images\" capabilities, accurately evaluating the authenticity of their reasoning process remains a critical challenge. Existing benchmarks mainly rely on outcome-oriented accuracy, lacking the capability to assess whether models can accurately leverage fine-grained visual cues for multi-step reasoning. To address these limitations, we propose ViEBench, a process-verifiable benchmark designed to evaluate faithful visual reasoning. Comprising 200 multi-scenario high-resolution images with expert-annotated visual evidence, ViEBench uniquely categorizes tasks by difficulty into perception and reasoning dimensions, where reasoning tasks require utilizing localized visual details with prior knowledge. To establish comprehensive evaluation criteria, we introduce a dual-axis matrix that provides fine-grained metrics through four diagnostic quadrants, enabling transparent diagnosis of model behavior across varying task complexities. Our experiments yield several interesting observations: (1) VLMs can sometimes produce correct final answers despite grounding on irrelevant regions, and (2) they may successfully locate the correct evidence but still fail to utilize it to reach accurate conclusions. Our findings demonstrate that ViEBench can serve as a more explainable and practical benchmark for comprehensively evaluating the effectiveness agentic VLMs. The codes will be released at: https://github.com/Xuchen-Li/ViEBench.", "AI": {"tldr": "ViEBench\u662f\u4e00\u4e2a\u8fc7\u7a0b\u53ef\u9a8c\u8bc1\u7684\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\uff0c\u5305\u542b200\u5f20\u591a\u573a\u666f\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u4e13\u5bb6\u6807\u6ce8\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u901a\u8fc7\u53cc\u8f74\u77e9\u9635\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bca\u65ad\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u7ed3\u679c\u5bfc\u5411\u7684\u51c6\u786e\u7387\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u591f\u51c6\u786e\u5229\u7528\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u9a8c\u8bc1\u63a8\u7406\u8fc7\u7a0b\u7684\u771f\u5b9e\u6027\u3002", "method": "\u63d0\u51faViEBench\u57fa\u51c6\uff0c\u5305\u542b200\u5f20\u591a\u573a\u666f\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u4e13\u5bb6\u6807\u6ce8\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u5c06\u4efb\u52a1\u6309\u96be\u5ea6\u5206\u4e3a\u611f\u77e5\u548c\u63a8\u7406\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5f15\u5165\u53cc\u8f74\u77e9\u9635\u63d0\u4f9b\u56db\u4e2a\u8bca\u65ad\u8c61\u9650\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a(1) VLMs\u6709\u65f6\u80fd\u57fa\u4e8e\u65e0\u5173\u533a\u57df\u4ea7\u751f\u6b63\u786e\u7b54\u6848\uff1b(2) \u6a21\u578b\u53ef\u80fd\u6210\u529f\u5b9a\u4f4d\u6b63\u786e\u8bc1\u636e\u4f46\u4ecd\u65e0\u6cd5\u5229\u7528\u5b83\u5f97\u51fa\u51c6\u786e\u7ed3\u8bba\u3002ViEBench\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30VLMs\u7684\u6709\u6548\u6027\u3002", "conclusion": "ViEBench\u4f5c\u4e3a\u4e00\u4e2a\u66f4\u53ef\u89e3\u91ca\u548c\u5b9e\u7528\u7684\u57fa\u51c6\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u771f\u5b9e\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.11634", "pdf": "https://arxiv.org/pdf/2601.11634", "abs": "https://arxiv.org/abs/2601.11634", "authors": ["Chenghui Yu", "Hongwei Wang", "Junwen Chen", "Zixuan Wang", "Bingfeng Deng", "Zhuolin Hao", "Hongyu Xiong", "Yang Song"], "title": "When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms", "categories": ["cs.CV"], "comment": null, "summary": "Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u6a21\u6001LLM\u4ee3\u7406\u7684\u81ea\u52a8\u95ee\u9898\u53d1\u73b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u77ed\u89c6\u9891\u5e73\u53f0\u65b0\u5174\u5185\u5bb9\u95ee\u9898\u7684\u5feb\u901f\u53d1\u73b0\u548c\u6807\u6ce8\u7b56\u7565\u66f4\u65b0", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u5185\u5bb9\u8d8b\u52bf\u5feb\u901f\u6f14\u53d8\uff0c\u4f20\u7edf\u4eba\u5de5\u53d1\u73b0\u95ee\u9898\u901f\u5ea6\u592a\u6162\uff0c\u5bfc\u81f4\u6807\u6ce8\u7b56\u7565\u66f4\u65b0\u5ef6\u8fdf\uff0c\u5f71\u54cd\u5185\u5bb9\u6cbb\u7406\u6548\u679c", "method": "\u4f7f\u7528\u591a\u6a21\u6001LLM\u4ee3\u7406\u81ea\u52a8\u53ec\u56de\u6f5c\u5728\u65b0\u95ee\u9898\u7684\u77ed\u89c6\u9891\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u805a\u7c7b\u7b56\u7565\u5206\u7ec4\uff0c\u6bcf\u4e2a\u805a\u7c7b\u5bf9\u5e94\u4e00\u4e2a\u65b0\u53d1\u73b0\u7684\u95ee\u9898\uff0c\u4ee3\u7406\u4ece\u805a\u7c7b\u751f\u6210\u66f4\u65b0\u7684\u6807\u6ce8\u7b56\u7565", "result": "\u90e8\u7f72\u5230\u771f\u5b9e\u7cfb\u7edf\uff0c\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u663e\u793a\uff1a\u65b0\u5174\u95ee\u9898\u53d1\u73b0\u6548\u679c\u663e\u8457\u63d0\u5347\uff08F1\u5206\u6570\u63d0\u9ad8\u8d8520%\uff09\uff0c\u540e\u7eed\u95ee\u9898\u6cbb\u7406\u6027\u80fd\u589e\u5f3a\uff08\u95ee\u9898\u89c6\u9891\u89c2\u770b\u91cf\u51cf\u5c11\u7ea615%\uff09\uff0c\u5927\u5e45\u964d\u4f4e\u65f6\u95f4\u6210\u672c\u5e76\u52a0\u901f\u6807\u6ce8\u7b56\u7565\u8fed\u4ee3", "conclusion": "\u57fa\u4e8e\u591a\u6a21\u6001LLM\u4ee3\u7406\u7684\u81ea\u52a8\u95ee\u9898\u53d1\u73b0\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u77ed\u89c6\u9891\u5e73\u53f0\u65b0\u5174\u5185\u5bb9\u95ee\u9898\u7684\u5feb\u901f\u53d1\u73b0\u548c\u6cbb\u7406\u95ee\u9898\uff0c\u76f8\u6bd4\u4eba\u5de5\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf"}}
{"id": "2601.11635", "pdf": "https://arxiv.org/pdf/2601.11635", "abs": "https://arxiv.org/abs/2601.11635", "authors": ["Anil Egin", "Andrea Tangherloni", "Antitza Dantcheva"], "title": "Now You See Me, Now You Don't: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos", "categories": ["cs.CV"], "comment": null, "summary": "Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.", "AI": {"tldr": "\u63d0\u51faAnon-NET\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u751f\u6210\u6a21\u578b\u548c\u89c6\u9891\u9a71\u52a8\u52a8\u753b\u5b9e\u73b0\u9762\u90e8\u89c6\u9891\u53bb\u8eab\u4efd\u5316\uff0c\u540c\u65f6\u4fdd\u7559\u5e74\u9f84\u3001\u6027\u522b\u3001\u79cd\u65cf\u3001\u59ff\u6001\u548c\u8868\u60c5\u7b49\u5c5e\u6027\u3002", "motivation": "\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5141\u8bb8\u89c6\u9891\u5206\u6790\u7528\u4e8e\u8868\u60c5\u8bc6\u522b\u3001\u4eba\u5458\u8ddf\u8e2a\u548c\u52a8\u4f5c\u8bc6\u522b\u7b49\u4e0b\u6e38\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u53bb\u8eab\u4efd\u5316\u53c8\u80fd\u4fdd\u7559\u91cd\u8981\u89c6\u89c9\u5c5e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6269\u6563\u751f\u6210\u6a21\u578b\u8fdb\u884c\u9762\u90e8\u4fee\u590d\uff0c\u901a\u8fc7\u9ad8\u7ea7\u5c5e\u6027\u8bc6\u522b\u548c\u8fd0\u52a8\u611f\u77e5\u8868\u60c5\u8f6c\u79fb\u5f15\u5bfc\uff0c\u7136\u540e\u901a\u8fc7\u89c6\u9891\u9a71\u52a8\u52a8\u753b\u5bf9\u53bb\u8eab\u4efd\u5316\u9762\u90e8\u8fdb\u884c\u52a8\u753b\u5904\u7406\u3002", "result": "\u5728VoxCeleb2\u3001CelebV-HQ\u548cHDTF\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAnon-NET\u80fd\u6709\u6548\u6df7\u6dc6\u8eab\u4efd\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u771f\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "Anon-NET\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u7559\u91cd\u8981\u7684\u9762\u90e8\u5c5e\u6027\uff0c\u4e3a\u89c6\u9891\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11637", "pdf": "https://arxiv.org/pdf/2601.11637", "abs": "https://arxiv.org/abs/2601.11637", "authors": ["Aradhya Dixit"], "title": "Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in multimodal foundation models has enabled Vision-Language Agents (VLAs) to decompose complex visual tasks into executable tool-based plans. While recent benchmarks have begun to evaluate iterative self-correction, its quantitative limits and dominant reasoning bottlenecks remain poorly characterized. This work introduces a Diagnostic Micro-Benchmark. Our analysis decouples Task Success Rate (TSR = 62 percent) from Correction Success Rate (CSR = 25 to 33 percent), revealing that initial competence does not predict repair ability. We explicitly quantify the diminishing returns of correction, which saturates after three retries. Our Failure Taxonomy reveals a frequent factor is Semantic Drift (about 28 percent of failures), a loss of contextual state. By isolating this reasoning bottleneck, this benchmark defines a reproducible framework toward stateful, trustworthy multimodal agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u5fae\u57fa\u51c6\u6765\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u521d\u59cb\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u4fee\u6b63\u6210\u529f\u7387\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u8bc6\u522b\u4e86\u8bed\u4e49\u6f02\u79fb\u4f5c\u4e3a\u4e3b\u8981\u5931\u8d25\u539f\u56e0\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4f7f\u5f97\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u80fd\u591f\u5c06\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u7684\u5de5\u5177\u8ba1\u5212\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u5bf9\u8fed\u4ee3\u81ea\u6211\u4fee\u6b63\u7684\u8bc4\u4f30\u6709\u9650\uff0c\u5176\u5b9a\u91cf\u9650\u5236\u548c\u4e3b\u8981\u63a8\u7406\u74f6\u9888\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8868\u5f81\u3002", "method": "\u5f15\u5165\u8bca\u65ad\u5fae\u57fa\u51c6\uff0c\u5c06\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u4fee\u6b63\u6210\u529f\u7387\u89e3\u8026\u5206\u6790\uff0c\u91cf\u5316\u4fee\u6b63\u7684\u9012\u51cf\u56de\u62a5\uff08\u4e09\u6b21\u91cd\u8bd5\u540e\u9971\u548c\uff09\uff0c\u5e76\u901a\u8fc7\u5931\u8d25\u5206\u7c7b\u6cd5\u8bc6\u522b\u4e3b\u8981\u5931\u8d25\u539f\u56e0\u3002", "result": "\u4efb\u52a1\u6210\u529f\u7387\u4e3a62%\uff0c\u4f46\u4fee\u6b63\u6210\u529f\u7387\u4ec5\u4e3a25-33%\uff0c\u8868\u660e\u521d\u59cb\u80fd\u529b\u65e0\u6cd5\u9884\u6d4b\u4fee\u590d\u80fd\u529b\u3002\u4fee\u6b63\u6548\u679c\u5728\u4e09\u6b21\u91cd\u8bd5\u540e\u9971\u548c\uff0c\u8bed\u4e49\u6f02\u79fb\uff08\u7ea628%\u7684\u5931\u8d25\uff09\u662f\u4e3b\u8981\u7684\u63a8\u7406\u74f6\u9888\u3002", "conclusion": "\u8be5\u57fa\u51c6\u901a\u8fc7\u9694\u79bb\u8bed\u4e49\u6f02\u79fb\u8fd9\u4e00\u63a8\u7406\u74f6\u9888\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u72b6\u6001\u4fdd\u6301\u80fd\u529b\u3001\u53ef\u4fe1\u8d56\u7684\u591a\u6a21\u6001\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.11640", "pdf": "https://arxiv.org/pdf/2601.11640", "abs": "https://arxiv.org/abs/2601.11640", "authors": ["Yingda Yu", "Jiaqi Xuan", "Shuhui Shi", "Xuanyu Teng", "Shuyang Xu", "Guanchao Tong"], "title": "Confident Learning for Object Detection under Model Constraints", "categories": ["cs.CV"], "comment": "Submitted to ICPR 2026, currently under review", "summary": "Agricultural weed detection on edge devices is subject to strict constraints on model capacity, computational resources, and real-time inference latency, which prevent performance improvements through model scaling or ensembling. This paper proposes Model-Driven Data Correction (MDDC), a data-centric framework that enhances detection performance by iteratively diagnosing and correcting data quality deficiencies. An automated error analysis procedure categorizes detection failures into four types: false negatives, false positives, class confusion, and localization errors. These error patterns are systematically addressed through a structured train-fix-retrain pipeline with version-controlled data management. Experimental results on multiple weed detection datasets demonstrate consistent improvements of 5-25 percent in mAP at 0.5 using a fixed lightweight detector (YOLOv8n), indicating that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.", "AI": {"tldr": "\u63d0\u51faMDDC\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u8d28\u91cf\u8bca\u65ad\u4e0e\u4fee\u6b63\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u6742\u8349\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u56fa\u5b9a\u8f7b\u91cf\u68c0\u6d4b\u5668\u4e0b\u5b9e\u73b05-25%\u7684mAP\u63d0\u5347", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u519c\u4e1a\u6742\u8349\u68c0\u6d4b\u9762\u4e34\u4e25\u683c\u7684\u8ba1\u7b97\u8d44\u6e90\u3001\u6a21\u578b\u5bb9\u91cf\u548c\u5b9e\u65f6\u63a8\u7406\u5ef6\u8fdf\u9650\u5236\uff0c\u65e0\u6cd5\u901a\u8fc7\u6a21\u578b\u7f29\u653e\u6216\u96c6\u6210\u6765\u63d0\u5347\u6027\u80fd\uff0c\u9700\u8981\u6570\u636e\u5c42\u9762\u7684\u4f18\u5316\u65b9\u6848", "method": "\u63d0\u51fa\u6a21\u578b\u9a71\u52a8\u6570\u636e\u4fee\u6b63\uff08MDDC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u9519\u8bef\u5206\u6790\u5c06\u68c0\u6d4b\u5931\u8d25\u5206\u4e3a\u56db\u7c7b\uff08\u5047\u9634\u6027\u3001\u5047\u9633\u6027\u3001\u7c7b\u522b\u6df7\u6dc6\u3001\u5b9a\u4f4d\u9519\u8bef\uff09\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u8bad\u7ec3-\u4fee\u590d-\u518d\u8bad\u7ec3\u6d41\u7a0b\u548c\u7248\u672c\u63a7\u5236\u6570\u636e\u7ba1\u7406", "result": "\u5728\u591a\u4e2a\u6742\u8349\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u56fa\u5b9a\u8f7b\u91cf\u68c0\u6d4b\u5668\uff08YOLOv8n\uff09\u5b9e\u73b0\u4e865-25%\u7684mAP@0.5\u4e00\u81f4\u63d0\u5347", "conclusion": "\u7cfb\u7edf\u5316\u7684\u6570\u636e\u8d28\u91cf\u4f18\u5316\u80fd\u6709\u6548\u7f13\u89e3\u56fa\u5b9a\u6a21\u578b\u5bb9\u91cf\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u519c\u4e1a\u6742\u8349\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6027\u80fd\u63d0\u5347\u65b9\u6848"}}
{"id": "2601.11641", "pdf": "https://arxiv.org/pdf/2601.11641", "abs": "https://arxiv.org/abs/2601.11641", "authors": ["Yuxi Liu", "Yipeng Hu", "Zekun Zhang", "Kunze Jiang", "Kun Yuan"], "title": "Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \\underline{\\textbf{M}}ixtrue-\\underline{\\textbf{O}}f-\\underline{\\textbf{D}}istribution \\textbf{DiT} (\\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.", "AI": {"tldr": "MOD-DiT\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91c7\u6837\u7684\u52a8\u6001\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u51c6\u786e\u5efa\u6a21\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\u5728\u89c6\u9891\u751f\u6210\u4e2d\u9762\u4e34\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8fc7\u4e8e\u7b80\u5316\u7684\u9759\u6001\u6a21\u5f0f\uff0c\u8981\u4e48\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u91c7\u6837\u64cd\u4f5c\u6765\u5b9e\u73b0\u52a8\u6001\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u6a21\u5f0f\u9884\u6d4b\u4e0d\u51c6\u786e\u548c\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faMOD-DiT\uff08\u6df7\u5408\u5206\u5e03DiT\uff09\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a1\uff09\u5229\u7528\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u6df7\u5408\u65b9\u6cd5\u5efa\u6a21\u9ad8\u6548\u7684\u7ebf\u6027\u8fd1\u4f3c\u6a21\u578b\uff0c\u9884\u6d4b\u7279\u5b9a\u53bb\u566a\u533a\u95f4\u7684\u63a9\u7801\u6a21\u5f0f\uff1b2\uff09\u5728\u7ebf\u5757\u63a9\u7801\u7b56\u7565\u52a8\u6001\u5e94\u7528\u8fd9\u4e9b\u9884\u6d4b\u7684\u63a9\u7801\uff0c\u540c\u65f6\u4fdd\u6301\u5386\u53f2\u7a00\u758f\u4fe1\u606f\uff0c\u65e0\u9700\u91cd\u590d\u91c7\u6837\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u52a0\u901f\u548c\u8d28\u91cf\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86MOD-DiT\u5728\u9ad8\u6548\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u8ba1\u7b97\u9650\u5236\u3002", "conclusion": "MOD-DiT\u901a\u8fc7\u91c7\u6837\u81ea\u7531\u7684\u52a8\u6001\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u51c6\u786e\u5efa\u6a21\u6f14\u5316\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65e2\u9ad8\u6548\u53c8\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.11642", "pdf": "https://arxiv.org/pdf/2601.11642", "abs": "https://arxiv.org/abs/2601.11642", "authors": ["Abbas Alzubaidi", "Ali Al-Bayaty"], "title": "PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "16 pages, 6 figures", "summary": "Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like \"0\" vs. \"2\") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u7684\u5408\u6210\u6a21\u62df\u6846\u67b6(PSSF)\u751f\u6210\u53ef\u63a7\u7684\u819d\u5173\u8282X\u5149\u7247\uff0c\u7528\u4e8e\u89e3\u51b3OA\u8bc4\u4f30\u4e2d\u6570\u636e\u9690\u79c1\u548c\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884cKL\u5206\u7ea7\u9884\u6d4b\u3002", "motivation": "\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\u662f\u5168\u7403\u4e3b\u8981\u81f4\u6b8b\u539f\u56e0\uff0c\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u7684KL\u5206\u7ea7\u8bc4\u4f30\u3002AI\u548c\u5f71\u50cf\u7ec4\u5b66\u9700\u8981\u5927\u91cf\u6807\u6ce8\u7684X\u5149\u6570\u636e\uff0c\u4f46\u53d7\u9690\u79c1\u3001\u7ba1\u7406\u548c\u8d44\u6e90\u9650\u5236\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u5f00\u53d1\u7269\u7406\u57fa\u7840\u7684\u5408\u6210\u6a21\u62df\u6846\u67b6(PSSF)\uff0c\u4ece\u53c2\u6570\u5316\u89e3\u5256\u6a21\u578b\u751f\u6210\u819d\u5173\u8282\u524d\u540e\u4f4dX\u5149\u7247\u3002\u521b\u5efa180\u540d\u53d7\u8bd5\u8005(260\u4e2a\u819d\u76d6)\u7684\u865a\u62df\u961f\u5217\uff0c\u91c7\u7528\u4e09\u79cd\u6210\u50cf\u534f\u8bae\u3002\u4f7f\u7528IBSI\u6807\u51c6\u5904\u7406\u5185\u4fa7\u5173\u8282\u533a\u57df\uff0c\u91c7\u7528\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548c\u68af\u5ea6\u63d0\u5347\u4e09\u79cdML\u6a21\u578b\u8fdb\u884cKL\u5206\u7ea7\u9884\u6d4b\u3002", "result": "\u5728IBSI\u534f\u8bae\u5185\u3001\u8de8\u534f\u8bae\u548c\u591a\u534f\u8bae\u573a\u666f\u4e0b\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u7c7b\u5185\u76f8\u5173\u7cfb\u6570\u8bc4\u4f30\u4e86\u7279\u5f81\u5728\u4e0d\u540c\u91c7\u96c6\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "PSSF\u6846\u67b6\u80fd\u591f\u751f\u6210\u53ef\u63a7\u7684\u5408\u6210X\u5149\u7247\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u83b7\u53d6\u7684\u9690\u79c1\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u4e3aOA\u7684\u5b9a\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2601.11644", "pdf": "https://arxiv.org/pdf/2601.11644", "abs": "https://arxiv.org/abs/2601.11644", "authors": ["Muhammad Imran", "Yugyung Lee"], "title": "Predicting When to Trust Vision-Language Models for Spatial Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 5 figures, 6 tables", "summary": "Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.", "AI": {"tldr": "\u63d0\u51fa\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u9a8c\u8bc1\u63d0\u5347\u53ef\u9760\u6027\uff0c\u5728BLIP-2\u4e0aAUROC\u63d0\u534734%\uff0c\u9009\u62e9\u6027\u9884\u6d4b\u8986\u76d6\u7387\u63d0\u53472.2\u500d", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff08\u51c6\u786e\u7387\u4ec549-54%\uff09\uff0c\u4f46\u5728\u673a\u5668\u4eba\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u9700\u8981\u53ef\u9760\u9884\u6d4b\u3002\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u7684\u81ea\u8bc4\u4f30\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u5916\u90e8\u51e0\u4f55\u9a8c\u8bc1\u6765\u8bc4\u4f30\u9884\u6d4b\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u68c0\u6d4b\u8fdb\u884c\u72ec\u7acb\u51e0\u4f55\u9a8c\u8bc1\u3002\u878d\u5408\u56db\u4e2a\u4fe1\u53f7\uff1aVLM\u9884\u6d4b\u4e0e\u5750\u6807\u7684\u51e0\u4f55\u5bf9\u9f50\u5ea6\u3001\u7a7a\u95f4\u91cd\u53e0\u7684\u6a21\u7cca\u6027\u3001\u68c0\u6d4b\u8d28\u91cf\u3001VLM\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u8fdb\u884c\u878d\u5408\u3002", "result": "\u5728BLIP-2\u4e0aAUROC\u8fbe\u52300.674\uff08\u63d0\u534734%\uff09\uff0cCLIP\u4e0a0.583\uff08\u63d0\u534716.1%\uff09\u3002\u572860%\u76ee\u6807\u51c6\u786e\u7387\u4e0b\uff0cBLIP-2\u8986\u76d6\u738761.9% vs \u57fa\u7ebf27.6%\uff082.2\u500d\u63d0\u5347\uff09\u3002\u7279\u5f81\u5206\u6790\u663e\u793a\u89c6\u89c9\u4fe1\u53f7\u8d21\u732e87.4%\u91cd\u8981\u6027\u3002", "conclusion": "\u5916\u90e8\u51e0\u4f55\u9a8c\u8bc1\u6bd4\u81ea\u8bc4\u4f30\u66f4\u6709\u6548\uff0c\u80fd\u663e\u8457\u63d0\u5347VLM\u7a7a\u95f4\u9884\u6d4b\u53ef\u9760\u6027\u3002\u8be5\u6846\u67b6\u53ef\u5b9e\u73b0\u9009\u62e9\u6027\u9884\u6d4b\uff0c\u5728\u573a\u666f\u56fe\u6784\u5efa\u4e2d\u5c06\u7cbe\u5ea6\u4ece52.1%\u63d0\u5347\u81f378.3%\uff0c\u540c\u65f6\u4fdd\u755968.2%\u8fb9\u3002"}}
{"id": "2601.11645", "pdf": "https://arxiv.org/pdf/2601.11645", "abs": "https://arxiv.org/abs/2601.11645", "authors": ["Ujjwal Jain", "Oshin Misra", "Roshni Chakraborty", "Mahua Bhattacharya"], "title": "IMSAHLO: Integrating Multi-Scale Attention and Hybrid Loss Optimization Framework for Robust Neuronal Brain Cell Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of neuronal cells in fluorescence microscopy is a fundamental task for quantitative analysis in computational neuroscience. However, it is significantly impeded by challenges such as the coexistence of densely packed and sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models often fail to preserve fine topological details or accurately delineate boundaries under these conditions. To address these limitations, we propose a novel deep learning framework, IMSAHLO (Integrating Multi-Scale Attention and Hybrid Loss Optimization), for robust and adaptive neuronal segmentation. The core of our model features Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, effectively handling variations in cell density, and a Hierarchical Attention (HA) mechanism that adaptively focuses on salient morphological features to preserve Region of Interest (ROI) boundary details. Furthermore, we introduce a novel hybrid loss function synergistically combining Tversky and Focal loss to combat class imbalance, alongside a topology-aware Centerline Dice (clDice) loss and a Contour-Weighted Boundary loss to ensure topological continuity and precise separation of adjacent cells. Large-scale experiments on the public Fluorescent Neuronal Cells (FNC) dataset demonstrate that our framework outperforms state-of-the-art architectures, achieving precision of 81.4%, macro F1 score of 82.7%, micro F1 score of 83.3%, and balanced accuracy of 99.5% on difficult dense and sparse cases. Ablation studies validate the synergistic benefits of multi-scale attention and hybrid loss terms. This work establishes a foundation for generalizable segmentation models applicable to a wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.", "AI": {"tldr": "\u63d0\u51faIMSAHLO\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u4e0e\u6df7\u5408\u635f\u5931\u4f18\u5316\uff0c\u7528\u4e8e\u8367\u5149\u663e\u5fae\u955c\u795e\u7ecf\u5143\u7ec6\u80de\u5206\u5272\uff0c\u89e3\u51b3\u5bc6\u96c6/\u7a00\u758f\u7ec6\u80de\u5171\u5b58\u3001\u5f62\u6001\u590d\u6742\u91cd\u53e0\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u8367\u5149\u663e\u5fae\u955c\u795e\u7ecf\u5143\u5206\u5272\u9762\u4e34\u5bc6\u96c6\u4e0e\u7a00\u758f\u7ec6\u80de\u5171\u5b58\u3001\u590d\u6742\u91cd\u53e0\u5f62\u6001\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u7cbe\u7ec6\u62d3\u6251\u7ec6\u8282\u548c\u51c6\u786e\u8fb9\u754c\u5212\u5206\u3002", "method": "\u63d0\u51faIMSAHLO\u6846\u67b6\uff0c\u5305\u542b\u591a\u5c3a\u5ea6\u5bc6\u96c6\u5757(MSDBs)\u6355\u83b7\u4e0d\u540c\u611f\u53d7\u91ce\u7279\u5f81\uff0c\u5206\u5c42\u6ce8\u610f\u529b(HA)\u673a\u5236\u805a\u7126\u5f62\u6001\u7279\u5f81\uff0c\u4ee5\u53ca\u7ed3\u5408Tversky\u635f\u5931\u3001Focal\u635f\u5931\u3001\u4e2d\u5fc3\u7ebfDice\u635f\u5931\u548c\u8f6e\u5ed3\u52a0\u6743\u8fb9\u754c\u635f\u5931\u7684\u6df7\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u516c\u5f00FNC\u6570\u636e\u96c6\u4e0a\uff0cIMSAHLO\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u6848\u4f8b\u4e2d\u8fbe\u523081.4%\u7cbe\u5ea6\u300182.7%\u5b8fF1\u5206\u6570\u300183.3%\u5faeF1\u5206\u6570\u548c99.5%\u5e73\u8861\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "IMSAHLO\u6846\u67b6\u901a\u8fc7\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u4e0e\u6df7\u5408\u635f\u5931\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u81ea\u9002\u5e94\u7684\u795e\u7ecf\u5143\u5206\u5272\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u5efa\u7acb\u4e86\u53ef\u63a8\u5e7f\u7684\u57fa\u7840\uff0c\u63a8\u52a8AI\u8f85\u52a9\u5206\u6790\u5411\u9ad8\u901a\u91cf\u795e\u7ecf\u751f\u7269\u5b66\u6d41\u7a0b\u53d1\u5c55\u3002"}}
{"id": "2601.11651", "pdf": "https://arxiv.org/pdf/2601.11651", "abs": "https://arxiv.org/abs/2601.11651", "authors": ["Miriam Doh", "Aditya Gulati", "Corina Canali", "Nuria Oliver"], "title": "Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification", "categories": ["cs.CV", "cs.AI", "cs.CY"], "comment": "22 pages, 15 figures", "summary": "This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women's faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men's; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.\n  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors' views.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210AI\u5b58\u5728\u7cfb\u7edf\u6027\"\u7b97\u6cd5\u5916\u8c8c\u4e3b\u4e49\"\u504f\u89c1\uff0c\u5c06\u9762\u90e8\u5438\u5f15\u529b\u4e0e\u6b63\u9762\u5c5e\u6027\u5173\u8054\uff0c\u5e76\u5728\u6027\u522b\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6027\u522b\u504f\u89c1\uff0c\u5973\u6027\u9762\u5b54\u8bef\u5206\u7c7b\u7387\u66f4\u9ad8\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793a\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u5916\u8c8c\u504f\u89c1\uff08\u7b97\u6cd5\u5916\u8c8c\u4e3b\u4e49\uff09\uff0c\u8fd9\u79cd\u504f\u89c1\u5c06\u7269\u7406\u5916\u8c8c\u4e0e\u793e\u4f1a\u5efa\u6784\u5c5e\u6027\uff08\u5982\u5438\u5f15\u529b\u4e0e\u6b63\u9762\u7279\u8d28\uff09\u9519\u8bef\u5173\u8054\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u52a0\u5267\u6027\u522b\u4e0d\u5e73\u7b49\u3002", "method": "\u4f7f\u7528Stable Diffusion 2.1\u548c3.5 Medium\u751f\u621026,400\u5f20\u5408\u6210\u4eba\u8138\uff0c\u5206\u6790\u751f\u6210AI\u6a21\u578b\u5982\u4f55\u7cfb\u7edf\u5730\u5c06\u9762\u90e8\u5438\u5f15\u529b\u4e0e\u6b63\u9762\u5c5e\u6027\u5173\u8054\uff0c\u5e76\u8bc4\u4f30\u4e09\u4e2a\u6027\u522b\u5206\u7c7b\u7b97\u6cd5\u5728\u4e0d\u540c\u5c5e\u6027\u8f93\u5165\u9762\u5b54\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) T2I\u6a21\u578b\u7cfb\u7edf\u7f16\u7801\u5438\u5f15\u529b-\u6b63\u9762\u5c5e\u6027\u5173\u8054\uff1b(2) \u6027\u522b\u5206\u7c7b\u7cfb\u7edf\u4e2d\u5973\u6027\u9762\u5b54\uff08\u7279\u522b\u662f\u5e26\u6709\u8d1f\u9762\u5c5e\u6027\u7684\uff09\u8bef\u5206\u7c7b\u7387\u663e\u8457\u9ad8\u4e8e\u7537\u6027\uff1b(3) \u65b0\u6a21\u578b\u901a\u8fc7\u5e74\u9f84\u540c\u8d28\u5316\u3001\u6027\u522b\u5316\u66b4\u9732\u6a21\u5f0f\u548c\u5730\u7406\u7b80\u5316\u52a0\u5267\u5ba1\u7f8e\u7ea6\u675f\u3002", "conclusion": "\u7b97\u6cd5\u5916\u8c8c\u4e3b\u4e49\u662f\u8de8AI\u89c6\u89c9\u7cfb\u7edf\u7684\u7cfb\u7edf\u6027\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u8868\u5f81\u548c\u8bc6\u522b\u4e24\u65b9\u9762\u52a0\u5267\u73b0\u6709\u4e0d\u5e73\u7b49\u3002\u7814\u7a76\u63ed\u793a\u4e86\u751f\u6210AI\u5982\u4f55\u53cd\u6620\u548c\u5f3a\u5316\u793e\u4f1a\u5efa\u6784\u7684\u504f\u89c1\u800c\u975e\u7ecf\u9a8c\u4e8b\u5b9e\u3002"}}
{"id": "2601.11654", "pdf": "https://arxiv.org/pdf/2601.11654", "abs": "https://arxiv.org/abs/2601.11654", "authors": ["Kaustubh Shivshankar Shejole", "Gaurav Mishra"], "title": "PSSI-MaxST: An Efficient Pixel-Segment Similarity Index Using Intensity and Smoothness Features for Maximum Spanning Tree Based Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Interactive graph-based segmentation methods partition an image into foreground and background regions with the aid of user inputs. However, existing approaches often suffer from high computational costs, sensitivity to user interactions, and degraded performance when the foreground and background share similar color distributions. A key factor influencing segmentation performance is the similarity measure used for assigning edge weights in the graph. To address these challenges, we propose a novel Pixel Segment Similarity Index (PSSI), which leverages the harmonic mean of inter-channel similarities by incorporating both pixel intensity and spatial smoothness features. The harmonic mean effectively penalizes dissimilarities in any individual channel, enhancing robustness. The computational complexity of PSSI is $\\mathcal{O}(B)$, where $B$ denotes the number of histogram bins. Our segmentation framework begins with low-level segmentation using MeanShift, which effectively captures color, texture, and segment shape. Based on the resulting pixel segments, we construct a pixel-segment graph with edge weights determined by PSSI. For partitioning, we employ the Maximum Spanning Tree (MaxST), which captures strongly connected local neighborhoods beneficial for precise segmentation. The integration of the proposed PSSI, MeanShift, and MaxST allows our method to jointly capture color similarity, smoothness, texture, shape, and strong local connectivity. Experimental evaluations on the GrabCut and Images250 datasets demonstrate that our method consistently outperforms current graph-based interactive segmentation methods such as AMOE, OneCut, and SSNCut in terms of segmentation quality, as measured by Jaccard Index (IoU), $F_1$ score, execution time and Mean Error (ME). Code is publicly available at: https://github.com/KaustubhShejole/PSSI-MaxST.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u50cf\u7d20\u6bb5\u76f8\u4f3c\u6027\u6307\u6570(PSSI)\u548c\u6700\u5927\u751f\u6210\u6811(MaxST)\u7684\u4ea4\u4e92\u5f0f\u56fe\u5206\u5272\u65b9\u6cd5\uff0c\u5728GrabCut\u548cImages250\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u56fe\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5bf9\u7528\u6237\u4ea4\u4e92\u654f\u611f\u3001\u524d\u666f\u80cc\u666f\u989c\u8272\u76f8\u4f3c\u65f6\u6027\u80fd\u4e0b\u964d\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf", "method": "\u63d0\u51faPSSI\u76f8\u4f3c\u6027\u5ea6\u91cf\uff08\u5229\u7528\u50cf\u7d20\u5f3a\u5ea6\u548c\u7a7a\u95f4\u5e73\u6ed1\u7279\u5f81\u7684\u901a\u9053\u95f4\u76f8\u4f3c\u6027\u8c03\u548c\u5e73\u5747\uff09\uff0c\u7ed3\u5408MeanShift\u4f4e\u5c42\u5206\u5272\u6784\u5efa\u50cf\u7d20-\u6bb5\u56fe\uff0c\u4f7f\u7528MaxST\u8fdb\u884c\u5206\u5272", "result": "\u5728GrabCut\u548cImages250\u6570\u636e\u96c6\u4e0a\uff0c\u5728IoU\u3001F1\u5206\u6570\u3001\u6267\u884c\u65f6\u95f4\u548c\u5e73\u5747\u8bef\u5dee\u7b49\u6307\u6807\u4e0a\u4f18\u4e8eAMOE\u3001OneCut\u3001SSNCut\u7b49\u73b0\u6709\u65b9\u6cd5", "conclusion": "PSSI\u76f8\u4f3c\u6027\u5ea6\u91cf\u7ed3\u5408MeanShift\u548cMaxST\u80fd\u6709\u6548\u6355\u6349\u989c\u8272\u76f8\u4f3c\u6027\u3001\u5e73\u6ed1\u6027\u3001\u7eb9\u7406\u3001\u5f62\u72b6\u548c\u5f3a\u5c40\u90e8\u8fde\u63a5\u6027\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u4e92\u5f0f\u5206\u5272\u6027\u80fd"}}
{"id": "2601.11660", "pdf": "https://arxiv.org/pdf/2601.11660", "abs": "https://arxiv.org/abs/2601.11660", "authors": ["Chunshu Wu", "Ruibing Song", "Sushant Kondguli", "Tong Geng", "Ang Li"], "title": "Zeros can be Informative: Masked Binary U-Net for Image Segmentation on Tensor Cores", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Real-time image segmentation is a key enabler for AR/VR, robotics, drones, and autonomous systems, where tight accuracy, latency, and energy budgets must be met on resource-constrained edge devices. While U-Net offers a favorable balance of accuracy and efficiency compared to large transformer-based models, achieving real-time performance on high-resolution input remains challenging due to compute, memory, and power limits. Extreme quantization, particularly binary networks, is appealing for its hardware-friendly operations. However, two obstacles limit practicality: (1) severe accuracy degradation, and (2) a lack of end-to-end implementations that deliver efficiency on general-purpose GPUs.\n  We make two empirical observations that guide our design. (1) An explicit zero state is essential: training with zero masking to binary U-Net weights yields noticeable sparsity. (2) Quantization sensitivity is uniform across layers. Motivated by these findings, we introduce Masked Binary U-Net (MBU-Net), obtained through a cost-aware masking strategy that prioritizes masking where it yields the highest accuracy-per-cost, reconciling accuracy with near-binary efficiency.\n  To realize these gains in practice, we develop a GPU execution framework that maps MBU-Net to Tensor Cores via a subtractive bit-encoding scheme, efficiently implementing masked binary weights with binary activations. This design leverages native binary Tensor Core BMMA instructions, enabling high throughput and energy savings on widely available GPUs. Across 3 segmentation benchmarks, MBU-Net attains near full-precision accuracy (3% average drop) while delivering 2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.", "AI": {"tldr": "MBU-Net\uff1a\u4e00\u79cd\u901a\u8fc7\u63a9\u7801\u4e8c\u8fdb\u5236\u6743\u91cd\u548c\u4e8c\u8fdb\u5236\u6fc0\u6d3b\u7684U-Net\u53d8\u4f53\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u5168\u7cbe\u5ea6\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5728GPU\u4e0a\u5b9e\u73b02.04\u500d\u52a0\u901f\u548c3.54\u500d\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u5b9e\u65f6\u56fe\u50cf\u5206\u5272\u5728AR/VR\u3001\u673a\u5668\u4eba\u3001\u65e0\u4eba\u673a\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u51c6\u786e\u7387\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u8981\u6c42\u3002\u867d\u7136U-Net\u76f8\u6bd4\u5927\u578bTransformer\u6a21\u578b\u5728\u51c6\u786e\u7387\u548c\u6548\u7387\u4e0a\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u4ecd\u9762\u4e34\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u529f\u8017\u9650\u5236\u3002\u6781\u7aef\u91cf\u5316\uff08\u7279\u522b\u662f\u4e8c\u8fdb\u5236\u7f51\u7edc\uff09\u56e0\u5176\u786c\u4ef6\u53cb\u597d\u7684\u64cd\u4f5c\u800c\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u51c6\u786e\u7387\u4e0b\u964d\u548c\u7f3a\u4e4f\u7aef\u5230\u7aefGPU\u5b9e\u73b0\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMasked Binary U-Net (MBU-Net)\uff0c\u57fa\u4e8e\u4e24\u4e2a\u7ecf\u9a8c\u89c2\u5bdf\uff1a1\uff09\u663e\u5f0f\u96f6\u72b6\u6001\u81f3\u5173\u91cd\u8981\uff0c\u901a\u8fc7\u96f6\u63a9\u7801\u8bad\u7ec3\u4e8c\u8fdb\u5236U-Net\u6743\u91cd\u4f1a\u4ea7\u751f\u663e\u8457\u7a00\u758f\u6027\uff1b2\uff09\u5404\u5c42\u91cf\u5316\u654f\u611f\u6027\u5747\u5300\u3002\u91c7\u7528\u6210\u672c\u611f\u77e5\u63a9\u7801\u7b56\u7565\uff0c\u4f18\u5148\u5728\u51c6\u786e\u7387-\u6210\u672c\u6bd4\u6700\u9ad8\u7684\u5730\u65b9\u8fdb\u884c\u63a9\u7801\uff0c\u5e73\u8861\u51c6\u786e\u7387\u4e0e\u63a5\u8fd1\u4e8c\u8fdb\u5236\u7684\u6548\u7387\u3002\u5f00\u53d1GPU\u6267\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u6cd5\u4f4d\u7f16\u7801\u65b9\u6848\u5c06MBU-Net\u6620\u5c04\u5230Tensor Core\uff0c\u5229\u7528\u539f\u751f\u4e8c\u8fdb\u5236Tensor Core BMMA\u6307\u4ee4\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u57283\u4e2a\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMBU-Net\u8fbe\u5230\u63a5\u8fd1\u5168\u7cbe\u5ea6\u51c6\u786e\u7387\uff08\u5e73\u5747\u4e0b\u964d3%\uff09\uff0c\u76f8\u6bd416\u4f4d\u6d6e\u70b9U-Net\u5b9e\u73b02.04\u500d\u52a0\u901f\u548c3.54\u500d\u80fd\u8017\u964d\u4f4e\u3002", "conclusion": "MBU-Net\u901a\u8fc7\u63a9\u7801\u4e8c\u8fdb\u5236\u6743\u91cd\u548c\u4e8c\u8fdb\u5236\u6fc0\u6d3b\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7GPU\u6846\u67b6\u5b9e\u73b0\u4e86\u7406\u8bba\u4f18\u52bf\u7684\u5b9e\u9645\u8f6c\u5316\u3002"}}
{"id": "2601.11662", "pdf": "https://arxiv.org/pdf/2601.11662", "abs": "https://arxiv.org/abs/2601.11662", "authors": ["Abdullah Jirjees", "Ryan Myers", "Muhammad Haris Ikram", "Mohamed H. Zaki"], "title": "LTV-YOLO: A Lightweight Thermal Object Detector for Young Pedestrians in Adverse Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Detecting vulnerable road users (VRUs), particularly children and adolescents, in low light and adverse weather conditions remains a critical challenge in computer vision, surveillance, and autonomous vehicle systems. This paper presents a purpose-built lightweight object detection model designed to identify young pedestrians in various environmental scenarios. To address these challenges, our approach leverages thermal imaging from long-wave infrared (LWIR) cameras, which enhances detection reliability in conditions where traditional RGB cameras operating in the visible spectrum fail. Based on the YOLO11 architecture and customized for thermal detection, our model, termed LTV-YOLO (Lightweight Thermal Vision YOLO), is optimized for computational efficiency, accuracy and real-time performance on edge devices. By integrating separable convolutions in depth and a feature pyramid network (FPN), LTV-YOLO achieves strong performance in detecting small-scale, partially occluded, and thermally distinct VRUs while maintaining a compact architecture. This work contributes a practical and scalable solution to improve pedestrian safety in intelligent transportation systems, particularly in school zones, autonomous navigation, and smart city infrastructure. Unlike prior thermal detectors, our contribution is task-specific: a thermally only edge-capable design designed for young and small VRUs (children and distant adults). Although FPN and depthwise separable convolutions are standard components, their integration into a thermal-only pipeline optimized for short/occluded VRUs under adverse conditions is, to the best of our knowledge, novel.", "AI": {"tldr": "\u63d0\u51faLTV-YOLO\u8f7b\u91cf\u7ea7\u70ed\u6210\u50cf\u68c0\u6d4b\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u548c\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u68c0\u6d4b\u513f\u7ae5\u7b49\u6613\u53d7\u4f24\u5bb3\u9053\u8def\u4f7f\u7528\u8005\uff0c\u57fa\u4e8eYOLO11\u67b6\u6784\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5b9e\u65f6\u8fd0\u884c\u3002", "motivation": "\u5728\u4f4e\u5149\u7167\u548c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\uff0c\u4f20\u7edfRGB\u6444\u50cf\u5934\u96be\u4ee5\u53ef\u9760\u68c0\u6d4b\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u7b49\u6613\u53d7\u4f24\u5bb3\u9053\u8def\u4f7f\u7528\u8005\uff0c\u8fd9\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u76d1\u63a7\u548c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u884c\u4eba\u5b89\u5168\u6784\u6210\u91cd\u5927\u6311\u6218\u3002", "method": "\u57fa\u4e8eYOLO11\u67b6\u6784\u5f00\u53d1LTV-YOLO\u6a21\u578b\uff0c\u91c7\u7528\u957f\u6ce2\u7ea2\u5916\u70ed\u6210\u50cf\u6280\u672f\uff0c\u96c6\u6210\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u548c\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff0c\u4e13\u95e8\u9488\u5bf9\u70ed\u6210\u50cf\u68c0\u6d4b\u8fdb\u884c\u4f18\u5316\uff0c\u4e13\u6ce8\u4e8e\u5c0f\u5c3a\u5ea6\u3001\u90e8\u5206\u906e\u6321\u548c\u70ed\u7279\u5f81\u660e\u663e\u7684VRU\u68c0\u6d4b\u3002", "result": "LTV-YOLO\u5728\u8ba1\u7b97\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5f3a\u6027\u80fd\u68c0\u6d4b\uff0c\u7279\u522b\u662f\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u68c0\u6d4b\u5c0f\u5c3a\u5ea6\u3001\u90e8\u5206\u906e\u6321\u7684\u6613\u53d7\u4f24\u5bb3\u9053\u8def\u4f7f\u7528\u8005\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b66\u6821\u533a\u57df\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u667a\u6167\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u70ed\u6210\u50cf\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u884c\u4eba\u5b89\u5168\u3002"}}
{"id": "2601.11665", "pdf": "https://arxiv.org/pdf/2601.11665", "abs": "https://arxiv.org/abs/2601.11665", "authors": ["Amir Farzin Nikkhah", "Dong Chen", "Bradford Campbell", "Somayeh Asadi", "Arsalan Heydarian"], "title": "UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted for publication at the International Conference on Construction Engineering and Management (I3CE 2025)", "summary": "Unmanned Aerial Vehicles (UAVs) are transforming infrastructure inspections in the Architecture, Engineering, Construction, and Facility Management (AEC+FM) domain. By synthesizing insights from over 150 studies, this review paper highlights UAV-based methodologies for data acquisition, photogrammetric modeling, defect detection, and decision-making support. Key innovations include path optimization, thermal integration, and advanced machine learning (ML) models such as YOLO and Faster R-CNN for anomaly detection. UAVs have demonstrated value in structural health monitoring (SHM), disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. Despite these advancements, challenges in real-time processing, multimodal data fusion, and generalizability remain. A proposed workflow framework, informed by literature and a case study, integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures to improve accuracy and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies. The proposed framework ensures precise and actionable insights by fusing multimodal data and dynamically adapting path planning for complex environments, presented as a comprehensive step-by-step guide to address these challenges effectively. This paper concludes with future research directions emphasizing lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u65e0\u4eba\u673a\u5728AEC+FM\u9886\u57df\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u6570\u636e\u91c7\u96c6\u3001\u5efa\u6a21\u3001\u7f3a\u9677\u68c0\u6d4b\u548c\u51b3\u7b56\u652f\u6301\uff0c\u63d0\u51fa\u4e86\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u81ea\u9002\u5e94\u8def\u5f84\u89c4\u5212\u7684\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u65e0\u4eba\u673a\u6b63\u5728\u6539\u53d8\u5efa\u7b51\u3001\u5de5\u7a0b\u3001\u65bd\u5de5\u548c\u8bbe\u65bd\u7ba1\u7406\u9886\u57df\u7684\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u65b9\u5f0f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u5904\u7406\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u6765\u6574\u5408\u5148\u8fdb\u6280\u672f\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u5206\u6790150\u591a\u9879\u7814\u7a76\uff0c\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210RGB\u56fe\u50cf\u3001LiDAR\u548c\u70ed\u4f20\u611f\u6570\u636e\u7684\u5de5\u4f5c\u6d41\u7a0b\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8etransformer\u7684\u67b6\u6784\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u81ea\u9002\u5e94\u8def\u5f84\u89c4\u5212\u6765\u5e94\u5bf9\u590d\u6742\u73af\u5883\u3002", "result": "\u65e0\u4eba\u673a\u5df2\u5728\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u3001\u707e\u5bb3\u54cd\u5e94\u3001\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u3001\u80fd\u6e90\u6548\u7387\u8bc4\u4f30\u548c\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u4e2d\u8bc1\u660e\u4ef7\u503c\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u63d0\u9ad8\u7ed3\u6784\u7f3a\u9677\u3001\u70ed\u5f02\u5e38\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u65e0\u4eba\u673a\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55\u8f7b\u91cf\u7ea7AI\u6a21\u578b\u3001\u81ea\u9002\u5e94\u98de\u884c\u89c4\u5212\u3001\u5408\u6210\u6570\u636e\u96c6\u548c\u66f4\u4e30\u5bcc\u7684\u6a21\u6001\u878d\u5408\u6280\u672f\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u65f6\u5904\u7406\u3001\u6570\u636e\u878d\u5408\u548c\u6cdb\u5316\u80fd\u529b\u7b49\u6311\u6218\u3002"}}
{"id": "2601.11666", "pdf": "https://arxiv.org/pdf/2601.11666", "abs": "https://arxiv.org/abs/2601.11666", "authors": ["Muhammad Imran", "Chi Lee", "Yugyung Lee"], "title": "MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 3 figures, 1 table", "summary": "We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX's potential to enhance trust and transparency in radiological AI applications.", "AI": {"tldr": "MATEX\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u3001\u6587\u672c\u5f15\u5bfc\u7a7a\u95f4\u5148\u9a8c\u548c\u5c42\u4e00\u81f4\u6027\u5206\u6790\uff0c\u751f\u6210\u7cbe\u786e\u3001\u7a33\u5b9a\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u68af\u5ea6\u5f52\u56e0\u56fe\uff0c\u5728MS-CXR\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u4e0d\u7cbe\u786e\u3001\u7f3a\u4e4f\u89e3\u5256\u5b66\u57fa\u7840\u3001\u6ce8\u610f\u529b\u7c92\u5ea6\u6709\u9650\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u653e\u5c04\u5b66AI\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u900f\u660e\u5ea6\u3002", "method": "MATEX\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u5c42\u6ce8\u610f\u529b\u5c55\u5f00\u3001\u6587\u672c\u5f15\u5bfc\u7a7a\u95f4\u5148\u9a8c\u548c\u5c42\u4e00\u81f4\u6027\u5206\u6790\uff0c\u901a\u8fc7\u89e3\u5256\u5b66\u4fe1\u606f\u589e\u5f3a\u7684\u7a7a\u95f4\u63a8\u7406\u6765\u751f\u6210\u7cbe\u786e\u7684\u68af\u5ea6\u5f52\u56e0\u56fe\u3002", "result": "\u5728MS-CXR\u6570\u636e\u96c6\u4e0a\uff0cMATEX\u5728\u7a7a\u95f4\u7cbe\u5ea6\u548c\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7ed3\u679c\u7684\u5bf9\u9f50\u5ea6\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684M2IB\u65b9\u6cd5\u3002", "conclusion": "MATEX\u901a\u8fc7\u63d0\u4f9b\u66f4\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u89e3\u91ca\uff0c\u6709\u671b\u589e\u5f3a\u653e\u5c04\u5b66AI\u5e94\u7528\u4e2d\u7684\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u66f4\u6709\u4ef7\u503c\u7684\u652f\u6301\u3002"}}
{"id": "2601.11675", "pdf": "https://arxiv.org/pdf/2601.11675", "abs": "https://arxiv.org/abs/2601.11675", "authors": ["Ritik Raina", "Abe Leite", "Alexandros Graikos", "Seoyoung Ahn", "Dimitris Samaras", "Gregory J. Zelinsky"], "title": "Generating metamers of human scene understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human vision combines low-resolution \"gist\" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. \"foveated\") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a \"same\" or \"different\" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.", "AI": {"tldr": "MetamerGen\uff1a\u57fa\u4e8e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u673a\u5236\uff08\u4e2d\u592e\u51f9\u9ad8\u5206\u8fa8\u7387+\u5468\u8fb9\u4f4e\u5206\u8fa8\u7387\u4fe1\u606f\uff09\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u80fd\u751f\u6210\u4e0e\u4eba\u7c7b\u573a\u666f\u7406\u89e3\u5bf9\u9f50\u7684\u56fe\u50cf\u5143\u5339\u914d", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u901a\u8fc7\u7ed3\u5408\u5468\u8fb9\u4f4e\u5206\u8fa8\u7387\"\u8981\u70b9\"\u4fe1\u606f\u548c\u6ce8\u89c6\u70b9\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\u6765\u7406\u89e3\u573a\u666f\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u672a\u80fd\u5145\u5206\u6a21\u62df\u8fd9\u79cd\u611f\u77e5\u673a\u5236\u3002\u9700\u8981\u5f00\u53d1\u80fd\u751f\u6210\u4e0e\u4eba\u7c7b\u6f5c\u5728\u573a\u666f\u8868\u5f81\u5bf9\u9f50\u7684\u56fe\u50cf\u5de5\u5177\u3002", "method": "\u63d0\u51faMetamerGen\u2014\u2014\u53cc\u6d41\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff1a\u4f7f\u7528DINOv2 tokens\u878d\u5408\u6ce8\u89c6\u70b9\u7684\u8be6\u7ec6\u7279\u5f81\u548c\u5468\u8fb9\u964d\u7ea7\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u5904\u7406\u4e2d\u592e\u51f9\u5316\u573a\u666f\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u5408\u6210\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u884c\u4e3a\u5b9e\u9a8c\uff08\u76f8\u540c/\u4e0d\u540c\u5224\u65ad\uff09\u9a8c\u8bc1\u751f\u6210\u56fe\u50cf\u4e0e\u4eba\u7c7b\u6f5c\u5728\u573a\u666f\u8868\u5f81\u7684\u5bf9\u9f50\u6027\uff0c\u53d1\u73b0\u57fa\u4e8e\u89c2\u770b\u8005\u81ea\u8eab\u6ce8\u89c6\u533a\u57df\u751f\u6210\u7684\u573a\u666f\u5728\u9ad8\u5c42\u8bed\u4e49\u5bf9\u9f50\u4e0a\u6700\u80fd\u9884\u6d4b\u5143\u5339\u914d\u6027\u3002", "conclusion": "MetamerGen\u662f\u7406\u89e3\u573a\u666f\u7406\u89e3\u7684\u6709\u529b\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u591a\u7ea7\u89c6\u89c9\u5904\u7406\u7279\u5f81\u5bf9\u4eba\u7c7b\u5224\u65ad\u7684\u8d21\u732e\uff0c\u4e3a\u7814\u7a76\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.11679", "pdf": "https://arxiv.org/pdf/2601.11679", "abs": "https://arxiv.org/abs/2601.11679", "authors": ["Richard Hartley"], "title": "Conformal Point and the Calibrated Conic", "categories": ["cs.CV"], "comment": null, "summary": "This gives some information about the conformal point and the calibrating conic, and their relationship one to the other. These concepts are useful for visualizing image geometry, and lead to intuitive ways to compute geometry, such as angles and directions in an image.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u7684\u6982\u5ff5\u53ca\u5176\u76f8\u4e92\u5173\u7cfb\uff0c\u8fd9\u4e9b\u6982\u5ff5\u6709\u52a9\u4e8e\u56fe\u50cf\u51e0\u4f55\u53ef\u89c6\u5316\uff0c\u5e76\u4e3a\u8ba1\u7b97\u56fe\u50cf\u4e2d\u7684\u89d2\u5ea6\u548c\u65b9\u5411\u7b49\u51e0\u4f55\u5c5e\u6027\u63d0\u4f9b\u4e86\u76f4\u89c2\u65b9\u6cd5\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u66f4\u76f4\u89c2\u7684\u56fe\u50cf\u51e0\u4f55\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u901a\u8fc7\u5f15\u5165\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u7684\u6982\u5ff5\uff0c\u7b80\u5316\u56fe\u50cf\u4e2d\u51e0\u4f55\u5c5e\u6027\uff08\u5982\u89d2\u5ea6\u548c\u65b9\u5411\uff09\u7684\u8ba1\u7b97\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5e76\u5efa\u7acb\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u5229\u7528\u8fd9\u4e9b\u6982\u5ff5\u6765\u53ef\u89c6\u5316\u56fe\u50cf\u51e0\u4f55\u5e76\u8ba1\u7b97\u51e0\u4f55\u5c5e\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u5171\u5f62\u70b9\u4e0e\u6821\u51c6\u5706\u9525\u4e4b\u95f4\u7684\u660e\u786e\u5173\u7cfb\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u6982\u5ff5\u5982\u4f55\u63d0\u4f9b\u76f4\u89c2\u7684\u56fe\u50cf\u51e0\u4f55\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u5e76\u7b80\u5316\u4e86\u89d2\u5ea6\u548c\u65b9\u5411\u7b49\u51e0\u4f55\u5c5e\u6027\u7684\u8ba1\u7b97\u3002", "conclusion": "\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u4e3a\u56fe\u50cf\u51e0\u4f55\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u5b66\u5de5\u5177\uff0c\u80fd\u591f\u5b9e\u73b0\u76f4\u89c2\u7684\u53ef\u89c6\u5316\u5e76\u7b80\u5316\u51e0\u4f55\u8ba1\u7b97\uff0c\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u50cf\u5904\u7406\u9886\u57df\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.11700", "pdf": "https://arxiv.org/pdf/2601.11700", "abs": "https://arxiv.org/abs/2601.11700", "authors": ["Luis A. Leiva", "Moises Diaz", "Nuwan T. Attygalle", "Miguel A. Ferrer", "Rejean Plamondon"], "title": "Telling Human and Machine Handwriting Apart", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u624b\u5199\u8fd0\u52a8\u4f5c\u4e3a\u884c\u4e3a\u751f\u7269\u7279\u5f81\u6765\u68c0\u6d4b\u8f93\u5165\u662f\u5426\u7531\u4eba\u7c7b\u751f\u6210\uff0c\u901a\u8fc7\u8bad\u7ec3\u6d45\u5c42\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u5408\u6210\u5668\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u4eba\u7c7b/\u5408\u6210\u624b\u5199\u5206\u7c7b\u3002", "motivation": "\u624b\u5199\u8fd0\u52a8\u53ef\u4f5c\u4e3a\u72ec\u7279\u7684\u751f\u7269\u7279\u5f81\u6765\u9a8c\u8bc1\u8bbe\u5907\u6216\u5e94\u7528\u662f\u5426\u7531\u771f\u5b9e\u7528\u6237\u64cd\u4f5c\uff0c\u8fd9\u76f8\u5f53\u4e8e\u4e00\u79cd\u53cd\u5411\u56fe\u7075\u6d4b\u8bd5\uff0c\u8ba1\u7b97\u673a\u9700\u8981\u68c0\u6d4b\u8f93\u5165\u662f\u7531\u4eba\u7c7b\u8fd8\u662f\u4eba\u5de5\u751f\u6210\u7684\u3002\u8be5\u7814\u7a76\u65e8\u5728\u4e3a\u9700\u8981\u9a8c\u8bc1\u4eba\u7c7b\u5b58\u5728\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u63d0\u4f9b\u989d\u5916\u7684\u5b89\u5168\u5c42\u3002", "method": "\u7814\u7a76\u4f7f\u752810\u4e2a\u516c\u5f00\u624b\u5199\u7b26\u53f7\u6570\u636e\u96c6\uff08\u5305\u62ec\u5b64\u7acb\u5b57\u7b26\u3001\u6570\u5b57\u3001\u624b\u52bf\u3001\u6307\u5411\u8f68\u8ff9\u548c\u7b7e\u540d\uff09\uff0c\u901a\u8fc77\u79cd\u4e0d\u540c\u7684\u5408\u6210\u5668\uff08\u5305\u62ec\u8fd0\u52a8\u5b66\u7406\u8bba\u3001\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u3001Transformers\u548c\u6269\u6563\u6a21\u578b\uff09\u4eba\u5de5\u590d\u5236\u3002\u8bad\u7ec3\u4e00\u4e2a\u6d45\u5c42\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u975e\u7279\u5f81\u5316\u7684\u8f68\u8ff9\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u5728\u6240\u6709\u5408\u6210\u5668\u548c\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8fbe\u523098.3%\u7684ROC\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUC\uff09\u548c1.4%\u7684\u7b49\u9519\u8bef\u7387\u3002\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\uff0c\u4ec5\u4f7f\u752810%\u6570\u636e\u8bad\u7ec3\u5c31\u80fd\u5728\u5269\u4f5990%\u6d4b\u8bd5\u6570\u636e\u4e0a\u4fdd\u6301\u4f18\u5f02\u6027\u80fd\u3002\u5728\u57df\u5916\u8bbe\u7f6e\u4e2d\u4e5f\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u624b\u5199\u8fd0\u52a8\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u751f\u7269\u7279\u5f81\u6765\u533a\u5206\u4eba\u7c7b\u548c\u5408\u6210\u8f93\u5165\uff0c\u4e3a\u8ba1\u7b97\u673a\u7cfb\u7edf\u9a8c\u8bc1\u4eba\u7c7b\u5b58\u5728\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u5b89\u5168\u6027\u4ee5\u62b5\u5fa1\u653b\u51fb\u8005\u3002"}}
{"id": "2601.11724", "pdf": "https://arxiv.org/pdf/2601.11724", "abs": "https://arxiv.org/abs/2601.11724", "authors": ["Muditha Fernando", "Kajhanan Kailainathan", "Krishnakanth Nagaratnam", "Isuranga Udaravi Bandara Senavirathne", "Ranga Rodrigo"], "title": "SemAlign: Language Guided Semi-supervised Domain Generalization", "categories": ["cs.CV"], "comment": "15 pages, 6 figures", "summary": "Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u4e30\u5bcc\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\u6765\u63d0\u5347\u6027\u80fd\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u7b56\u7565\u5b9e\u73b0SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709SSDG\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u4f2a\u6807\u7b7e\u7cbe\u5ea6\u800c\u5ffd\u89c6\u8bad\u7ec3\u671f\u95f4\u7684\u6570\u636e\u6700\u5927\u5316\u5229\u7528\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u6f5c\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u6709\u9650\u6807\u6ce8\u6570\u636e\u53c8\u80fd\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u57df\u7684\u65b9\u6cd5\u3002", "method": "1) \u5c06\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u8bed\u4e49\u4e30\u5bcc\u4e14\u6cdb\u5316\u7684\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4ee5\u4fc3\u8fdb\u57df\u4e0d\u53d8\u6027\uff1b2) \u7ed3\u5408\u6709\u6548\u7684\u56fe\u50cf\u7ea7\u589e\u5f3a\u548c\u8f93\u51fa\u7ea7\u6b63\u5219\u5316\u7b56\u7565\uff0c\u63d0\u9ad8\u6570\u636e\u5229\u7528\u5e76\u51cf\u5c11\u8fc7\u62df\u5408\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e0e\u73b0\u6709SSDG\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb(SOTA)\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6a21\u578b\u7279\u5f81\u4e0eVLM\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u589e\u5f3a\u548c\u6b63\u5219\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3SSDG\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2601.11729", "pdf": "https://arxiv.org/pdf/2601.11729", "abs": "https://arxiv.org/abs/2601.11729", "authors": ["Turhan Can Kargin", "Wojciech Jasi\u0144ski", "Adam Pardyl", "Bartosz Zieli\u0144ski", "Marcin Przewi\u0119\u017alikowski"], "title": "SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project page is available at https://sparrta.gmum.net/", "summary": "Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SpaRRTa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7a7a\u95f4\u5173\u7cfb\u8bc6\u522b\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DINO\u548cCLIP\uff09\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5177\u8eab\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002\u867d\u7136\u8fd1\u671f\u7814\u7a76\u5c1d\u8bd5\u5c063D\u4efb\u52a1\uff08\u5982\u6df1\u5ea6\u4f30\u8ba1\uff09\u878d\u5165VFM\u8bad\u7ec3\uff0c\u4f46\u6a21\u578b\u5728\u4e0d\u540c\u7a7a\u95f4\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5f15\u53d1\u4e86\u5bf9\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u771f\u6b63\u5177\u5907\u7a7a\u95f4\u610f\u8bc6\u8fd8\u662f\u4ec5\u8fc7\u62df\u5408\u7279\u5b9a3D\u76ee\u6807\u7684\u7591\u95ee\u3002", "method": "\u63d0\u51fa\u4e86\u7a7a\u95f4\u5173\u7cfb\u8bc6\u522b\u4efb\u52a1\uff08SpaRRTa\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30VFM\u8bc6\u522b\u56fe\u50cf\u4e2d\u7269\u4f53\u76f8\u5bf9\u4f4d\u7f6e\u7684\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u751f\u6210\u4efb\u610f\u6570\u91cf\u7684\u903c\u771f\u56fe\u50cf\uff0c\u5305\u542b\u591a\u6837\u5316\u573a\u666f\u548c\u5b8c\u5168\u53ef\u63a7\u7684\u7269\u4f53\u6392\u5217\uff0c\u5e76\u63d0\u4f9b\u53ef\u81ea\u7531\u8bbf\u95ee\u7684\u7a7a\u95f4\u6807\u6ce8\u3002", "result": "\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u6700\u5148\u8fdb\u7684VFM\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u663e\u8457\u5dee\u5f02\u3002\u901a\u8fc7\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u5bf9\u73b0\u4ee3VFM\u4e2d\u652f\u6301\u6216\u963b\u788d\u7a7a\u95f4\u610f\u8bc6\u673a\u5236\u7684\u89c1\u89e3\u3002", "conclusion": "SpaRRTa\u57fa\u51c6\u6d4b\u8bd5\u53ef\u4f5c\u4e3a\u6307\u5bfc\u672a\u6765\u7a7a\u95f4\u611f\u77e5\u89c6\u89c9\u6a21\u578b\u5f00\u53d1\u7684\u6709\u7528\u5de5\u5177\uff0c\u5e2e\u52a9\u63d0\u5347\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2601.11769", "pdf": "https://arxiv.org/pdf/2601.11769", "abs": "https://arxiv.org/abs/2601.11769", "authors": ["Cheng Lyu", "Jingyue Zhang", "Ryan Maunu", "Mengwei Li", "Vinny DeGenova", "Yuanli Pei"], "title": "From Pixels to Purchase: Building and Evaluating a Taxonomy-Decoupled Visual Search Engine for Home Goods E-commerce", "categories": ["cs.CV"], "comment": null, "summary": "Visual search is critical for e-commerce, especially in style-driven domains where user intent is subjective and open-ended. Existing industrial systems typically couple object detection with taxonomy-based classification and rely on catalog data for evaluation, which is prone to noise that limits robustness and scalability. We propose a taxonomy-decoupled architecture that uses classification-free region proposals and unified embeddings for similarity retrieval, enabling a more flexible and generalizable visual search. To overcome the evaluation bottleneck, we propose an LLM-as-a-Judge framework that assesses nuanced visual similarity and category relevance for query-result pairs in a zero-shot manner, removing dependence on human annotations or noise-prone catalog data. Deployed at scale on a global home goods platform, our system improves retrieval quality and yields a measurable uplift in customer engagement, while our offline evaluation metrics strongly correlate with real-world outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u89e3\u8026\u5206\u7c7b\u6cd5\u7684\u89c6\u89c9\u641c\u7d22\u67b6\u6784\u548c\u57fa\u4e8eLLM\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u7535\u5546\u5bb6\u5c45\u5e73\u53f0\u90e8\u7f72\u540e\u63d0\u5347\u4e86\u68c0\u7d22\u8d28\u91cf\u548c\u7528\u6237\u53c2\u4e0e\u5ea6", "motivation": "\u73b0\u6709\u7535\u5546\u89c6\u89c9\u641c\u7d22\u7cfb\u7edf\u901a\u5e38\u5c06\u76ee\u6807\u68c0\u6d4b\u4e0e\u57fa\u4e8e\u5206\u7c7b\u6cd5\u7684\u5206\u7c7b\u8026\u5408\uff0c\u5e76\u4f9d\u8d56\u76ee\u5f55\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5c24\u5176\u662f\u5728\u98ce\u683c\u9a71\u52a8\u7684\u9886\u57df\u4e2d\u7528\u6237\u610f\u56fe\u4e3b\u89c2\u4e14\u5f00\u653e\u7684\u60c5\u51b5\u4e0b", "method": "1) \u63d0\u51fa\u5206\u7c7b\u6cd5\u89e3\u8026\u67b6\u6784\uff1a\u4f7f\u7528\u514d\u5206\u7c7b\u7684\u533a\u57df\u63d0\u8bae\u548c\u7edf\u4e00\u5d4c\u5165\u8fdb\u884c\u76f8\u4f3c\u6027\u68c0\u7d22\uff1b2) \u63d0\u51faLLM-as-a-Judge\u6846\u67b6\uff1a\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u8bc4\u4f30\u67e5\u8be2-\u7ed3\u679c\u5bf9\u7684\u7ec6\u5fae\u89c6\u89c9\u76f8\u4f3c\u6027\u548c\u7c7b\u522b\u76f8\u5173\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u6613\u566a\u58f0\u7684\u76ee\u5f55\u6570\u636e", "result": "\u5728\u5168\u7403\u5bb6\u5c45\u7528\u54c1\u5e73\u53f0\u5927\u89c4\u6a21\u90e8\u7f72\u540e\uff0c\u7cfb\u7edf\u63d0\u9ad8\u4e86\u68c0\u7d22\u8d28\u91cf\uff0c\u5e26\u6765\u4e86\u53ef\u8861\u91cf\u7684\u5ba2\u6237\u53c2\u4e0e\u5ea6\u63d0\u5347\uff0c\u79bb\u7ebf\u8bc4\u4f30\u6307\u6807\u4e0e\u5b9e\u9645\u4e1a\u52a1\u7ed3\u679c\u5f3a\u76f8\u5173", "conclusion": "\u63d0\u51fa\u7684\u89e3\u8026\u5206\u7c7b\u6cd5\u67b6\u6784\u548c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u6846\u67b6\u80fd\u591f\u521b\u5efa\u66f4\u7075\u6d3b\u3001\u53ef\u6cdb\u5316\u7684\u89c6\u89c9\u641c\u7d22\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u73b0\u6709\u5de5\u4e1a\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c"}}
{"id": "2601.11772", "pdf": "https://arxiv.org/pdf/2601.11772", "abs": "https://arxiv.org/abs/2601.11772", "authors": ["Yimu Pan", "Hongda Mao", "Qingshuang Chen", "Yelin Kim"], "title": "studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present \\textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.", "AI": {"tldr": "studentSplat\uff1a\u4e00\u79cd\u7528\u4e8e\u5355\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u89e3\u51b3\u5c3a\u5ea6\u6a21\u7cca\u548c\u5916\u63a8\u95ee\u9898\uff0c\u5728\u5355\u89c6\u56fe\u65b0\u89c6\u89d2\u91cd\u5efa\u65b9\u9762\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u524d\u99883D\u9ad8\u65af\u6cfc\u6e85\u5728\u591a\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\u548c\u5355\u89c6\u56fe3D\u7269\u4f53\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5355\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\u7531\u4e8e\u5355\u89c6\u56fe\u56fa\u6709\u7684\u6a21\u7cca\u6027\u95ee\u9898\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u89e3\u51b3\u5c3a\u5ea6\u6a21\u7cca\u548c\u5916\u63a8\u95ee\u9898\u6765\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5355\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\u3002", "method": "\u63d0\u51fastudentSplat\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u6559\u5e08-\u5b66\u751f\u67b6\u6784\uff0c\u5176\u4e2d\u591a\u89c6\u56fe\u6559\u5e08\u6a21\u578b\u5728\u8bad\u7ec3\u671f\u95f4\u4e3a\u5355\u89c6\u56fe\u5b66\u751f\u63d0\u4f9b\u51e0\u4f55\u76d1\u7763\uff0c\u89e3\u51b3\u5c3a\u5ea6\u6a21\u7cca\u5e76\u9f13\u52b1\u51e0\u4f55\u6709\u6548\u6027\uff1b2\uff09\u5916\u63a8\u7f51\u7edc\uff0c\u7528\u4e8e\u8865\u5168\u7f3a\u5931\u7684\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5916\u63a8\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cstudentSplat\u5728\u5355\u89c6\u56fe\u65b0\u89c6\u89d2\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u573a\u666f\u7ea7\u522b\u4e0a\u6027\u80fd\u4e0e\u591a\u89c6\u56fe\u65b9\u6cd5\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u4f5c\u4e3a\u81ea\u76d1\u7763\u5355\u89c6\u56fe\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0cstudentSplat\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u663e\u793a\u4e86\u5176\u5728\u901a\u7528\u5355\u89c6\u56fe3D\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "studentSplat\u6210\u529f\u89e3\u51b3\u4e86\u5355\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u5c3a\u5ea6\u6a21\u7cca\u548c\u5916\u63a8\u95ee\u9898\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u548c\u5916\u63a8\u7f51\u7edc\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5355\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\uff0c\u4e3a\u5355\u89c6\u56fe3D\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11779", "pdf": "https://arxiv.org/pdf/2601.11779", "abs": "https://arxiv.org/abs/2601.11779", "authors": ["Vinicius F. Arruda", "Rodrigo F. Berriel", "Thiago M. Paix\u00e3o", "Claudine Badue", "Alberto F. De Souza", "Nicu Sebe", "Thiago Oliveira-Santos"], "title": "Cross-Domain Object Detection Using Unsupervised Image Translation", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised domain adaptation for object detection addresses the adaption of detectors trained in a source domain to work accurately in an unseen target domain. Recently, methods approaching the alignment of the intermediate features proven to be promising, achieving state-of-the-art results. However, these methods are laborious to implement and hard to interpret. Although promising, there is still room for improvements to close the performance gap toward the upper-bound (when training with the target data). In this work, we propose a method to generate an artificial dataset in the target domain to train an object detector. We employed two unsupervised image translators (CycleGAN and an AdaIN-based model) using only annotated data from the source domain and non-annotated data from the target domain. Our key contributions are the proposal of a less complex yet more effective method that also has an improved interpretability. Results on real-world scenarios for autonomous driving show significant improvements, outperforming state-of-the-art methods in most cases, further closing the gap toward the upper-bound.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u76ee\u6807\u57df\u4eba\u5de5\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528CycleGAN\u548cAdaIN\u8fdb\u884c\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u63a5\u8fd1\u4e0a\u9650", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u4e2d\u95f4\u7279\u5f81\u5bf9\u9f50\u53d6\u5f97\u4e86\u4e0d\u9519\u6548\u679c\uff0c\u4f46\u5b9e\u73b0\u590d\u6742\u3001\u96be\u4ee5\u89e3\u91ca\uff0c\u4e14\u4e0e\u4f7f\u7528\u76ee\u6807\u57df\u6570\u636e\u8bad\u7ec3\u7684\u4e0a\u9650\u4ecd\u6709\u5dee\u8ddd", "method": "\u4f7f\u7528CycleGAN\u548cAdaIN\u4e24\u79cd\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\u6a21\u578b\uff0c\u4ec5\u5229\u7528\u6e90\u57df\u6807\u6ce8\u6570\u636e\u548c\u76ee\u6807\u57df\u975e\u6807\u6ce8\u6570\u636e\uff0c\u751f\u6210\u76ee\u6807\u57df\u7684\u4eba\u5de5\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u771f\u5b9e\u573a\u666f\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4e86\u4e0e\u4e0a\u9650\u7684\u5dee\u8ddd", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u76ee\u6807\u57df\u4eba\u5de5\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2601.11896", "pdf": "https://arxiv.org/pdf/2601.11896", "abs": "https://arxiv.org/abs/2601.11896", "authors": ["Ngoc-Khai Hoang", "Thi-Nhu-Mai Nguyen", "Huy-Hieu Pham"], "title": "Digital FAST: An AI-Driven Multimodal Framework for Rapid and Early Stroke Screening", "categories": ["cs.CV"], "comment": null, "summary": "Early identification of stroke symptoms is essential for enabling timely intervention and improving patient outcomes, particularly in prehospital settings. This study presents a fast, non-invasive multimodal deep learning framework for automatic binary stroke screening based on data collected during the F.A.S.T. assessment. The proposed approach integrates complementary information from facial expressions, speech signals, and upper-body movements to enhance diagnostic robustness. Facial dynamics are represented using landmark based features and modeled with a Transformer architecture to capture temporal dependencies. Speech signals are converted into mel spectrograms and processed using an Audio Spectrogram Transformer, while upper-body pose sequences are analyzed with an MLP-Mixer network to model spatiotemporal motion patterns. The extracted modality specific representations are combined through an attention-based fusion mechanism to effectively learn cross modal interactions. Experiments conducted on a self-collected dataset of 222 videos from 37 subjects demonstrate that the proposed multimodal model consistently outperforms unimodal baselines, achieving 95.83% accuracy and a 96.00% F1-score. The model attains a strong balance between sensitivity and specificity and successfully detects all stroke cases in the test set. These results highlight the potential of multimodal learning and transfer learning for early stroke screening, while emphasizing the need for larger, clinically representative datasets to support reliable real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eF.A.S.T.\u8bc4\u4f30\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9762\u90e8\u8868\u60c5\u3001\u8bed\u97f3\u4fe1\u53f7\u548c\u4e0a\u534a\u8eab\u8fd0\u52a8\u7684\u878d\u5408\u5b9e\u73b0\u5feb\u901f\u3001\u975e\u4fb5\u5165\u6027\u7684\u5352\u4e2d\u7b5b\u67e5\uff0c\u5728\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8fbe\u523095.83%\u51c6\u786e\u7387\u3002", "motivation": "\u5352\u4e2d\u65e9\u671f\u8bc6\u522b\u5bf9\u53ca\u65f6\u5e72\u9884\u548c\u6539\u5584\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u9662\u524d\u73af\u5883\u4e2d\u3002\u9700\u8981\u5f00\u53d1\u5feb\u901f\u3001\u975e\u4fb5\u5165\u6027\u7684\u81ea\u52a8\u7b5b\u67e5\u65b9\u6cd5\u6765\u8f85\u52a9F.A.S.T.\u8bc4\u4f30\u3002", "method": "\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff1a1) \u9762\u90e8\u52a8\u6001\u4f7f\u7528\u57fa\u4e8elandmark\u7684\u7279\u5f81\u548cTransformer\u5efa\u6a21\uff1b2) \u8bed\u97f3\u4fe1\u53f7\u8f6c\u6362\u4e3amel\u9891\u8c31\u56fe\u5e76\u7528Audio Spectrogram Transformer\u5904\u7406\uff1b3) \u4e0a\u534a\u8eab\u59ff\u6001\u5e8f\u5217\u7528MLP-Mixer\u7f51\u7edc\u5206\u6790\u65f6\u7a7a\u8fd0\u52a8\u6a21\u5f0f\uff1b4) \u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "\u572837\u540d\u53d7\u8bd5\u8005\u7684222\u4e2a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u6a21\u6001\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u8fbe\u523095.83%\u51c6\u786e\u7387\u548c96.00% F1\u5206\u6570\uff0c\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u5e73\u8861\u826f\u597d\uff0c\u6d4b\u8bd5\u96c6\u4e2d\u6240\u6709\u5352\u4e2d\u75c5\u4f8b\u5747\u88ab\u68c0\u6d4b\u51fa\u3002", "conclusion": "\u591a\u6a21\u6001\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u5728\u65e9\u671f\u5352\u4e2d\u7b5b\u67e5\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u66f4\u5927\u3001\u66f4\u5177\u4e34\u5e8a\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u53ef\u9760\u7684\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u3002"}}
{"id": "2601.11898", "pdf": "https://arxiv.org/pdf/2601.11898", "abs": "https://arxiv.org/abs/2601.11898", "authors": ["Yilmaz Korkmaz", "Vishal M. Patel"], "title": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available \\href{https://github.com/yilmazkorkmaz1/RemoteVAR}{\\underline{here}}.", "AI": {"tldr": "RemoteVAR\uff1a\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u878d\u5408\u53cc\u65f6\u76f8\u7279\u5f81\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u548cTransformer\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u5bf9\u4e8e\u73af\u5883\u76d1\u6d4b\u548c\u707e\u5bb3\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u5728\u50cf\u7d20\u7ea7\u5224\u522b\u4efb\u52a1\u4e2d\u5b58\u5728\u53ef\u63a7\u6027\u5f31\u3001\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u4e0d\u4f73\u548c\u66dd\u5149\u504f\u5dee\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faRemoteVAR\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u81ea\u56de\u5f52\u9884\u6d4b\u6761\u4ef6\u5316\u4e8e\u591a\u5206\u8fa8\u7387\u878d\u5408\u7684\u53cc\u65f6\u76f8\u7279\u5f81\uff1b2\uff09\u4e13\u95e8\u8bbe\u8ba1\u4e86\u9488\u5bf9\u53d8\u5316\u56fe\u9884\u6d4b\u7684\u81ea\u56de\u5f52\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u6807\u51c6\u53d8\u5316\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRemoteVAR\u76f8\u6bd4\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u548cTransformer\u7684\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u7ade\u4e89\u529b\u7684\u81ea\u56de\u5f52\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "RemoteVAR\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u5728\u50cf\u7d20\u7ea7\u5224\u522b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2601.11907", "pdf": "https://arxiv.org/pdf/2601.11907", "abs": "https://arxiv.org/abs/2601.11907", "authors": ["Prosenjit Chatterjee", "ANK Zaman"], "title": "Towards Airborne Object Detection: A Deep Learning Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eEfficientNetB4\u7684\u53cc\u4efb\u52a1\u6a21\u578b\uff0c\u540c\u65f6\u8fdb\u884c\u7a7a\u4e2d\u7269\u4f53\u5206\u7c7b\u548c\u5a01\u80c1\u7b49\u7ea7\u9884\u6d4b\uff0c\u5728AODTA\u6570\u636e\u96c6\u4e0a\u8fbe\u523096%\u5206\u7c7b\u51c6\u786e\u7387\u548c90%\u5a01\u80c1\u9884\u6d4b\u51c6\u786e\u7387", "motivation": "\u968f\u7740\u7a7a\u4e2d\u5e73\u53f0\uff08\u5546\u7528\u98de\u673a\u3001\u65e0\u4eba\u673a\u3001UAV\uff09\u7684\u5feb\u901f\u589e\u52a0\uff0c\u9700\u8981\u5b9e\u65f6\u81ea\u52a8\u5316\u7684\u5a01\u80c1\u8bc4\u4f30\u7cfb\u7edf\u3002\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u76d1\u63a7\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u6709\u9650\u548c\u64cd\u4f5c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u6784\u5efaAODTA\u6570\u636e\u96c6\uff08\u805a\u5408\u548c\u7cbe\u70bc\u591a\u4e2a\u516c\u5f00\u6570\u636e\u6e90\uff09\uff0c\u5f00\u53d1\u57fa\u4e8eEfficientNetB4\u7684\u53cc\u4efb\u52a1\u6a21\u578b\uff0c\u540c\u65f6\u6267\u884c\u7a7a\u4e2d\u7269\u4f53\u5206\u7c7b\u548c\u5a01\u80c1\u7b49\u7ea7\u9884\u6d4b\uff0c\u5e76\u4e0eResNet-50\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "EfficientNetB4\u6a21\u578b\u5728\u7269\u4f53\u5206\u7c7b\u4e0a\u8fbe\u523096%\u51c6\u786e\u7387\uff0c\u5728\u5a01\u80c1\u7b49\u7ea7\u9884\u6d4b\u4e0a\u8fbe\u523090%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eResNet-50\u57fa\u7ebf\u3002\u5728AVD\u6570\u636e\u96c6\u548c\u65b0\u5f00\u53d1\u7684AODTA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u53cc\u4efb\u52a1\u6a21\u578b\u5728\u76d1\u89c6\u3001\u9632\u5fa1\u548c\u7a7a\u57df\u7ba1\u7406\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002\u867d\u7136\u6807\u9898\u63d0\u5230\u68c0\u6d4b\uff0c\u4f46\u672c\u7814\u7a76\u4e13\u6ce8\u4e8e\u4f7f\u7528\u73b0\u6709\u6570\u636e\u96c6\u63d0\u4f9b\u7684\u9884\u5b9a\u4f4d\u7a7a\u4e2d\u7269\u4f53\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u548c\u5a01\u80c1\u7b49\u7ea7\u63a8\u65ad\u3002"}}
{"id": "2601.11909", "pdf": "https://arxiv.org/pdf/2601.11909", "abs": "https://arxiv.org/abs/2601.11909", "authors": ["Io Yamada", "Hirotsugu Okuno"], "title": "Effects of the retina-inspired light intensity encoding on color discrimination performance", "categories": ["cs.CV"], "comment": "8 pages, 14 figures, 4 tables", "summary": "Color is an important source of information for visual functions such as object recognition, but it is greatly affected by the color of illumination. The ability to perceive the color of a visual target independent of illumination color is called color constancy (CC), and is an important feature for vision systems that use color information. In this study, we investigated the effects of the light intensity encoding function on the performance of CC of the center/surround (C/S) retinex model, which is a well-known model inspired by CC of the visual nervous system. The functions used to encode light intensity are the logarithmic function used in the original C/S retinex model and the Naka-Rushton (N-R) function, which is a model of retinal photoreceptor response. Color-variable LEDs were used to illuminate visual targets with various lighting colors, and color information computed by each model was used to evaluate the degree to which the color of visual targets illuminated with different lighting colors could be discriminated. Color information was represented using the HSV color space and a color plane based on the classical opponent color theory. The results showed that the combination of the N-R function and the double opponent color plane representation provided superior discrimination performance.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e2d\u5fc3/\u5468\u8fb9Retinex\u6a21\u578b\u4e2d\u4e0d\u540c\u5149\u5f3a\u7f16\u7801\u51fd\u6570\u5bf9\u989c\u8272\u6052\u5e38\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0Naka-Rushton\u51fd\u6570\u7ed3\u5408\u53cc\u62ee\u6297\u989c\u8272\u5e73\u9762\u8868\u793a\u80fd\u63d0\u4f9b\u6700\u4f73\u7684\u989c\u8272\u8fa8\u522b\u6027\u80fd\u3002", "motivation": "\u989c\u8272\u662f\u89c6\u89c9\u529f\u80fd\u7684\u91cd\u8981\u4fe1\u606f\u6765\u6e90\uff0c\u4f46\u53d7\u5149\u7167\u989c\u8272\u5f71\u54cd\u5f88\u5927\u3002\u989c\u8272\u6052\u5e38\u6027\uff08CC\uff09\u662f\u89c6\u89c9\u7cfb\u7edf\u4f7f\u7528\u989c\u8272\u4fe1\u606f\u7684\u91cd\u8981\u7279\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5149\u5f3a\u7f16\u7801\u51fd\u6570\u5bf9\u4e2d\u5fc3/\u5468\u8fb9Retinex\u6a21\u578b\u989c\u8272\u6052\u5e38\u6027\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u5149\u5f3a\u7f16\u7801\u51fd\u6570\uff1a\u539f\u59cbC/S Retinex\u6a21\u578b\u4f7f\u7528\u7684\u5bf9\u6570\u51fd\u6570\u548c\u89c6\u7f51\u819c\u5149\u611f\u53d7\u5668\u54cd\u5e94\u6a21\u578bNaka-Rushton\u51fd\u6570\u3002\u4f7f\u7528\u53ef\u53d8\u989c\u8272LED\u4ee5\u4e0d\u540c\u5149\u7167\u989c\u8272\u7167\u5c04\u89c6\u89c9\u76ee\u6807\uff0c\u901a\u8fc7\u6bcf\u4e2a\u6a21\u578b\u8ba1\u7b97\u7684\u989c\u8272\u4fe1\u606f\u6765\u8bc4\u4f30\u4e0d\u540c\u5149\u7167\u4e0b\u989c\u8272\u76ee\u6807\u7684\u8fa8\u522b\u7a0b\u5ea6\u3002\u989c\u8272\u4fe1\u606f\u4f7f\u7528HSV\u989c\u8272\u7a7a\u95f4\u548c\u57fa\u4e8e\u7ecf\u5178\u62ee\u6297\u989c\u8272\u7406\u8bba\u7684\u989c\u8272\u5e73\u9762\u8868\u793a\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cNaka-Rushton\u51fd\u6570\u4e0e\u53cc\u62ee\u6297\u989c\u8272\u5e73\u9762\u8868\u793a\u7684\u7ec4\u5408\u63d0\u4f9b\u4e86\u6700\u4f18\u7684\u8fa8\u522b\u6027\u80fd\u3002", "conclusion": "\u5728\u4e2d\u5fc3/\u5468\u8fb9Retinex\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528Naka-Rushton\u51fd\u6570\u4f5c\u4e3a\u5149\u5f3a\u7f16\u7801\u51fd\u6570\u5e76\u7ed3\u5408\u53cc\u62ee\u6297\u989c\u8272\u5e73\u9762\u8868\u793a\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u989c\u8272\u6052\u5e38\u6027\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u7cfb\u7edf\u989c\u8272\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6a21\u578b\u3002"}}
{"id": "2601.11910", "pdf": "https://arxiv.org/pdf/2601.11910", "abs": "https://arxiv.org/abs/2601.11910", "authors": ["Guiying Zhu", "Bowen Yang", "Yin Zhuang", "Tong Zhang", "Guanqun Wang", "Zhihao Che", "He Chen", "Lianlin Li"], "title": "A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of \"guess what\". Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.", "AI": {"tldr": "\u63d0\u51faGW-VLM\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u6982\u5ff5\u63d0\u793a\uff0c\u8ba9\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u73a9\"\u731c\u731c\u662f\u4ec0\u4e48\"\u6e38\u620f\u6765\u5b9e\u73b0\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u6839\u636e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5efa\u7acb\u901a\u7528\u76ee\u6807\u8ba4\u77e5\u7684\u5fc5\u8981\u6027\u3002\u867d\u7136\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5df2\u7ecf\u6784\u5efa\u4e86\u5177\u6709\u96f6\u6837\u672c\u80fd\u529b\u7684\u591a\u529f\u80fd\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u5f62\u6210\u5bf9\u4efb\u4f55\u5bf9\u8c61\u7684\u901a\u7528\u7406\u89e3\u8303\u5f0f\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faGW-VLM\u6846\u67b6\uff1a1) \u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u641c\u7d22(MS-VLS)\uff1a\u5229\u7528\u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u8f6f\u5bf9\u9f50\uff0c\u4ece\u7c7b\u522b\u65e0\u5173\u76ee\u6807\u68c0\u6d4b\u7ed3\u679c\u4e2d\u751f\u6210\u7247\u6bb5\uff1b2) \u4e0a\u4e0b\u6587\u6982\u5ff5\u63d0\u793a(CCP)\uff1a\u57fa\u4e8eMS-VLS\u5f62\u6210\u6982\u5ff5\u6d41\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u7247\u6bb5\u4ee5\u5b9e\u73b0\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u3002\u6574\u4e2a\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6(COCO val, Pascal VOC)\u548c\u9065\u611f\u6570\u636e\u96c6(DIOR, NWPU-10)\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGW-VLM\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u6b65\u9aa4\u5c31\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "GW-VLM\u901a\u8fc7\u521b\u65b0\u7684\"\u731c\u731c\u662f\u4ec0\u4e48\"\u6e38\u620f\u8303\u5f0f\uff0c\u6210\u529f\u6784\u5efa\u4e86\u57fa\u4e8e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u76ee\u6807\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11911", "pdf": "https://arxiv.org/pdf/2601.11911", "abs": "https://arxiv.org/abs/2601.11911", "authors": ["Muhammad Ibrahim", "Alfe Suny", "MD Sakib Ul Islam", "Md. Imran Hossain"], "title": "Reliable Deep Learning for Small-Scale Classifications: Experiments on Real-World Image Datasets from Bangladesh", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural networks (CNNs) have achieved state-of-the-art performance in image recognition tasks but often involve complex architectures that may overfit on small datasets. In this study, we evaluate a compact CNN across five publicly available, real-world image datasets from Bangladesh, including urban encroachment, vehicle detection, road damage, and agricultural crops. The network demonstrates high classification accuracy, efficient convergence, and low computational overhead. Quantitative metrics and saliency analyses indicate that the model effectively captures discriminative features and generalizes robustly across diverse scenarios, highlighting the suitability of streamlined CNN architectures for small-class image classification tasks.", "AI": {"tldr": "\u7d27\u51d1\u578bCNN\u5728\u5b5f\u52a0\u62c9\u56fd\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u7b80\u5316\u67b6\u6784\u5728\u5c0f\u7c7b\u522b\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027", "motivation": "\u4f20\u7edfCNN\u5728\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u590d\u6742\u67b6\u6784\u5bb9\u6613\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8fc7\u62df\u5408\u3002\u9700\u8981\u9a8c\u8bc1\u7b80\u5316CNN\u67b6\u6784\u5728\u5c0f\u7c7b\u522b\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u7d27\u51d1\u578b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u5b5f\u52a0\u62c9\u56fd\u4e94\u4e2a\u516c\u5f00\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ec\u57ce\u5e02\u4fb5\u5360\u3001\u8f66\u8f86\u68c0\u6d4b\u3001\u9053\u8def\u635f\u574f\u548c\u519c\u4f5c\u7269\u7b49\u573a\u666f\u3002", "result": "\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3001\u9ad8\u6548\u6536\u655b\u548c\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002\u5b9a\u91cf\u6307\u6807\u548c\u663e\u8457\u6027\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u5224\u522b\u6027\u7279\u5f81\uff0c\u5e76\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7b80\u5316CNN\u67b6\u6784\u5728\u5c0f\u7c7b\u522b\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u9002\u7528\u6027\uff0c\u80fd\u591f\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.11915", "pdf": "https://arxiv.org/pdf/2601.11915", "abs": "https://arxiv.org/abs/2601.11915", "authors": ["Chi Wang", "Xinjue Hu", "Boyu Wang", "Ziwen He", "Zhangjie Fu"], "title": "From Spurious to Causal: Low-rank Orthogonal Subspace Intervention for Generalizable Face Forgery Detection", "categories": ["cs.CV"], "comment": null, "summary": "The generalization problem remains a critical challenge in face forgery detection. Some researches have discovered that ``a backdoor path\" in the representations from forgery-irrelevant information to labels induces biased learning, thereby hindering the generalization. In this paper, these forgery-irrelevant information are collectively termed spurious correlations factors. Previous methods predominantly focused on identifying concrete, specific spurious correlation and designing corresponding solutions to address them. However, spurious correlations arise from unobservable confounding factors, making it impractical to identify and address each one individually. To address this, we propose an intervention paradigm for representation space. Instead of tracking and blocking various instance-level spurious correlation one by one, we uniformly model them as a low-rank subspace and intervene in them. Specifically, we decompose spurious correlation features into a low-rank subspace via orthogonal low-rank projection, subsequently removing this subspace from the original representation and training its orthogonal complement to capture forgery-related features. This low-rank projection removal effectively eliminates spurious correlation factors, ensuring that classification decision is based on authentic forgery cues. With only 0.43M trainable parameters, our method achieves state-of-the-art performance across several benchmarks, demonstrating excellent robustness and generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u4f4e\u79e9\u6295\u5f71\u53bb\u9664\u8868\u793a\u7a7a\u95f4\u4e2d\u865a\u5047\u76f8\u5173\u6027\u7684\u5e72\u9884\u8303\u5f0f\uff0c\u4ee5\u63d0\u5347\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u6e90\u4e8e\u865a\u5047\u76f8\u5173\u6027\u56e0\u7d20\uff08\u4f2a\u9020\u65e0\u5173\u4fe1\u606f\uff09\u901a\u8fc7\"\u540e\u95e8\u8def\u5f84\"\u5bfc\u81f4\u6709\u504f\u5b66\u4e60\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8bc6\u522b\u5177\u4f53\u7684\u865a\u5047\u76f8\u5173\u6027\u5e76\u9010\u4e00\u89e3\u51b3\uff0c\u4f46\u865a\u5047\u76f8\u5173\u6027\u6e90\u4e8e\u4e0d\u53ef\u89c2\u6d4b\u7684\u6df7\u6742\u56e0\u7d20\uff0c\u96be\u4ee5\u9010\u4e00\u8bc6\u522b\u548c\u5904\u7406\u3002", "method": "\u63d0\u51fa\u8868\u793a\u7a7a\u95f4\u5e72\u9884\u8303\u5f0f\uff1a\u5c06\u5404\u79cd\u5b9e\u4f8b\u7ea7\u865a\u5047\u76f8\u5173\u6027\u7edf\u4e00\u5efa\u6a21\u4e3a\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u6b63\u4ea4\u4f4e\u79e9\u6295\u5f71\u5c06\u5176\u5206\u89e3\u51fa\u6765\uff0c\u7136\u540e\u4ece\u539f\u59cb\u8868\u793a\u4e2d\u79fb\u9664\u8be5\u5b50\u7a7a\u95f4\uff0c\u8bad\u7ec3\u5176\u6b63\u4ea4\u8865\u7a7a\u95f4\u6765\u6355\u83b7\u4f2a\u9020\u76f8\u5173\u7279\u5f81\u3002", "result": "\u4ec5\u4f7f\u75280.43M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u865a\u5047\u76f8\u5173\u6027\u7edf\u4e00\u5efa\u6a21\u4e3a\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5e76\u8fdb\u884c\u5e72\u9884\uff0c\u80fd\u591f\u6709\u6548\u6d88\u9664\u865a\u5047\u76f8\u5173\u56e0\u7d20\uff0c\u786e\u4fdd\u5206\u7c7b\u51b3\u7b56\u57fa\u4e8e\u771f\u5b9e\u7684\u4f2a\u9020\u7ebf\u7d22\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0d\u53ef\u89c2\u6d4b\u6df7\u6742\u56e0\u7d20\u7684\u95ee\u9898\u3002"}}
{"id": "2601.11918", "pdf": "https://arxiv.org/pdf/2601.11918", "abs": "https://arxiv.org/abs/2601.11918", "authors": ["Akito Morita", "Hirotsugu Okuno"], "title": "Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions", "categories": ["cs.CV", "cs.LG"], "comment": "5 pages, 4 figures, 4 tables", "summary": "In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.", "AI": {"tldr": "\u4f7f\u7528Gabor\u6ee4\u6ce2\u5668\u4f5c\u4e3aCNN\u9884\u5904\u7406\uff0c\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0a\u673a\u5668\u4eba\u89c6\u89c9\u5e94\u7528\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684CNN\u9700\u8981\u5c0f\u578b\u67b6\u6784\uff0c\u673a\u5668\u4eba\u89c6\u89c9\u5e94\u7528\u9700\u8981\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u9ad8\u6548\u8bad\u7ec3\u3002\u89c6\u89c9\u795e\u7ecf\u7cfb\u7edf(VNS)\u80fd\u4ece\u5c11\u91cf\u89c6\u89c9\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u6a21\u578bGabor\u6ee4\u6ce2\u5668\u4f5c\u4e3aCNN\u9884\u5904\u7406\u7684\u6548\u679c", "method": "\u4f7f\u7528Gabor\u6ee4\u6ce2\u5668\uff08\u89c6\u89c9\u795e\u7ecf\u7cfb\u7edf\u7279\u5f81\u63d0\u53d6\u5668\u6a21\u578b\uff09\u4f5c\u4e3aCNN\u7684\u9884\u5904\u7406\u5c42\uff0c\u5728\u4e0d\u540cCNN\u67b6\u6784\u4e0a\u6bd4\u8f83\u6709\u65e0Gabor\u9884\u5904\u7406\u7684\u6548\u679c\u3002\u521b\u5efa\u5305\u542b\u4e0d\u540c\u76f8\u673a\u4f4d\u7f6e\u56fe\u50cf\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5728\u6709\u9650\u6761\u4ef6\u4e0b\u8bad\u7ec3\u7684CNN\u7684\u6cdb\u5316\u80fd\u529b", "result": "Gabor\u6ee4\u6ce2\u5668\u9884\u5904\u7406\u80fd\u63d0\u9ad8CNN\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u6709\u52a9\u4e8e\u51cf\u5c0fCNN\u7684\u6a21\u578b\u5c3a\u5bf8", "conclusion": "Gabor\u6ee4\u6ce2\u5668\u4f5c\u4e3a\u9884\u5904\u7406\u80fd\u6709\u6548\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0aCNN\u673a\u5668\u4eba\u89c6\u89c9\u5e94\u7528\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u65e2\u80fd\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u53c8\u80fd\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f"}}
{"id": "2601.11930", "pdf": "https://arxiv.org/pdf/2601.11930", "abs": "https://arxiv.org/abs/2601.11930", "authors": ["Xulei Shi", "Maoyu Wang", "Yuning Peng", "Guanbo Wang", "Xin Wang", "Qi Chen", "Pengjie Tao"], "title": "SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM", "categories": ["cs.CV"], "comment": null, "summary": "Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at https://anonymous.4open.science/r/SupScene-5B73.", "AI": {"tldr": "SupScene\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eSfM\u56fe\u50cf\u68c0\u7d22\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u56fe\u8bad\u7ec3\u7b56\u7565\u548cDiVLAD\u805a\u5408\u5668\u5b66\u4e60\u66f4\u9002\u5408\u51e0\u4f55\u91cd\u53e0\u914d\u5bf9\u7684\u5168\u5c40\u63cf\u8ff0\u7b26\uff0c\u5728GL3D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u800cSfM\u4e2d\u7684\u56fe\u50cf\u68c0\u7d22\u66f4\u9700\u8981\u5173\u6ce8\u51e0\u4f55\u5339\u914d\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u6279\u91cf\u4e8c\u5143\u6807\u7b7e\uff08\u91cd\u53e0vs\u975e\u91cd\u53e0\uff09\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u7ec6\u5fae\u5dee\u522b\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9SfM\u91cd\u53e0\u56fe\u50cf\u5bf9\u4f18\u5316\u7684\u63cf\u8ff0\u7b26\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u5b50\u56fe\u8bad\u7ec3\u7b56\u7565\uff1a\u8d85\u8d8a\u5b64\u7acb\u56fe\u50cf\u5bf9\uff0c\u5229\u7528\u5730\u9762\u771f\u5b9e\u51e0\u4f55\u91cd\u53e0\u5173\u7cfb\u53ca\u5176\u4e0d\u540c\u6743\u91cd\uff0c\u901a\u8fc7\u8f6f\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff1b2. \u63d0\u51faDiVLAD\u805a\u5408\u5668\uff1a\u57fa\u4e8eDINO\u542f\u53d1\uff0c\u5229\u7528ViT\u6700\u540e\u4e00\u5c42\u7684\u591a\u5934\u6ce8\u610f\u529b\u56fe\uff1b3. \u8bbe\u8ba1\u53ef\u5b66\u4e60\u95e8\u63a7\u673a\u5236\uff1a\u81ea\u9002\u5e94\u5730\u7ed3\u5408\u8bed\u4e49\u663e\u8457\u7ebf\u7d22\u4e0e\u89c6\u89c9\u7279\u5f81\uff0c\u751f\u6210\u66f4\u5177\u533a\u5206\u6027\u7684\u5168\u5c40\u63cf\u8ff0\u7b26\u3002", "result": "\u5728GL3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eNetVLAD\uff0c\u540c\u65f6\u4ec5\u5f15\u5165\u53ef\u5ffd\u7565\u7684\u989d\u5916\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u4e0d\u540c\u805a\u5408\u6280\u672f\u4e0a\u90fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SupScene\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u805a\u5408\u5668\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86SfM\u4e2d\u56fe\u50cf\u68c0\u7d22\u5bf9\u51e0\u4f55\u5339\u914d\u6027\u7684\u7279\u6b8a\u9700\u6c42\uff0c\u4e3a\u65e0\u7ea6\u675fSfM\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.11931", "pdf": "https://arxiv.org/pdf/2601.11931", "abs": "https://arxiv.org/abs/2601.11931", "authors": ["Zhengxian Wu", "Chuanrui Zhang", "Shenao Jiang", "Hangrui Xu", "Zirui Liao", "Luyuan Zhang", "Huaqiu Li", "Peng Jiao", "Haoqian Wang"], "title": "Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition is emerging as a promising technology and an innovative field within computer vision. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions.To address the above challenges, we present a Language guided and Motion-aware gait recognition framework, named LMGait.In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences.", "AI": {"tldr": "LMGait\uff1a\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u6b65\u6001\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u6b65\u6001\u76f8\u5173\u8bed\u8a00\u7ebf\u7d22\u6355\u6349\u5173\u952e\u8fd0\u52a8\u7279\u5f81\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u62df\u5408\u9759\u6001\u566a\u58f0\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u76f4\u63a5\u4ece\u56fe\u50cf\u63d0\u53d6\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u6c60\u5316\u64cd\u4f5c\u83b7\u5f97\u5e8f\u5217\u7ea7\u8868\u793a\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5bb9\u6613\u8fc7\u5ea6\u62df\u5408\u9759\u6001\u566a\u58f0\uff08\u5982\u670d\u88c5\uff09\uff0c\u540c\u65f6\u65e0\u6cd5\u6709\u6548\u6355\u6349\u52a8\u6001\u8fd0\u52a8\u533a\u57df", "method": "\u63d0\u51faLMGait\u6846\u67b6\uff0c\u5229\u7528\u8bbe\u8ba1\u7684\u6b65\u6001\u76f8\u5173\u8bed\u8a00\u7ebf\u7d22\u6765\u6355\u6349\u6b65\u6001\u5e8f\u5217\u4e2d\u7684\u5173\u952e\u8fd0\u52a8\u7279\u5f81\uff0c\u5b9e\u73b0\u8bed\u8a00\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u6b65\u6001\u8bc6\u522b", "result": "\u4ece\u6458\u8981\u4e2d\u65e0\u6cd5\u83b7\u53d6\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "conclusion": "LMGait\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u6355\u6349\u6b65\u6001\u4e2d\u7684\u52a8\u6001\u8fd0\u52a8\u7279\u5f81\uff0c\u51cf\u5c11\u5bf9\u9759\u6001\u566a\u58f0\u7684\u8fc7\u5ea6\u62df\u5408"}}
{"id": "2601.11944", "pdf": "https://arxiv.org/pdf/2601.11944", "abs": "https://arxiv.org/abs/2601.11944", "authors": ["Lexin Ren", "Jiamiao Lu", "Weichuan Zhang", "Benqing Wu", "Tuo Wang", "Yi Liao", "Jiapan Guo", "Changming Sun", "Liang Guo"], "title": "Deep learning-based neurodevelopmental assessment in preterm infants", "categories": ["cs.CV"], "comment": "27 pages, 8 figures", "summary": "Preterm infants (born between 28 and 37 weeks of gestation) face elevated risks of neurodevelopmental delays, making early identification crucial for timely intervention. While deep learning-based volumetric segmentation of brain MRI scans offers a promising avenue for assessing neonatal neurodevelopment, achieving accurate segmentation of white matter (WM) and gray matter (GM) in preterm infants remains challenging due to their comparable signal intensities (isointense appearance) on MRI during early brain development. To address this, we propose a novel segmentation neural network, named Hierarchical Dense Attention Network. Our architecture incorporates a 3D spatial-channel attention mechanism combined with an attention-guided dense upsampling strategy to enhance feature discrimination in low-contrast volumetric data. Quantitative experiments demonstrate that our method achieves superior segmentation performance compared to state-of-the-art baselines, effectively tackling the challenge of isointense tissue differentiation. Furthermore, application of our algorithm confirms that WM and GM volumes in preterm infants are significantly lower than those in term infants, providing additional imaging evidence of the neurodevelopmental delays associated with preterm birth. The code is available at: https://github.com/ICL-SUST/HDAN.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u65e9\u4ea7\u513f\u8111MRI\u767d\u8d28\u548c\u7070\u8d28\u5206\u5272\u7684\u5c42\u6b21\u5bc6\u96c6\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u89e3\u51b3\u7b49\u4fe1\u53f7\u5f3a\u5ea6\u7ec4\u7ec7\u533a\u5206\u96be\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u65e9\u4ea7\u513f\u795e\u7ecf\u53d1\u80b2\u5ef6\u8fdf\u98ce\u9669\u9ad8\uff0c\u9700\u8981\u65e9\u671f\u8bc6\u522b\u3002\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u8111MRI\u4f53\u79ef\u5206\u5272\u6709\u6f5c\u529b\uff0c\u4f46\u65e9\u4ea7\u513f\u767d\u8d28\u548c\u7070\u8d28\u5728MRI\u4e0a\u4fe1\u53f7\u5f3a\u5ea6\u76f8\u4f3c\uff08\u7b49\u4fe1\u53f7\u5916\u89c2\uff09\uff0c\u5bfc\u81f4\u51c6\u786e\u5206\u5272\u56f0\u96be", "method": "\u63d0\u51fa\u5c42\u6b21\u5bc6\u96c6\u6ce8\u610f\u529b\u7f51\u7edc\uff08HDAN\uff09\uff0c\u7ed3\u54083D\u7a7a\u95f4\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5bc6\u96c6\u4e0a\u91c7\u6837\u7b56\u7565\uff0c\u589e\u5f3a\u4f4e\u5bf9\u6bd4\u5ea6\u4f53\u79ef\u6570\u636e\u7684\u7279\u5f81\u533a\u5206\u80fd\u529b", "result": "\u5b9a\u91cf\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5206\u5272\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7b49\u4fe1\u53f7\u7ec4\u7ec7\u533a\u5206\u6311\u6218\u3002\u5e94\u7528\u7b97\u6cd5\u8bc1\u5b9e\u65e9\u4ea7\u513f\u767d\u8d28\u548c\u7070\u8d28\u4f53\u79ef\u663e\u8457\u4f4e\u4e8e\u8db3\u6708\u513f", "conclusion": "\u63d0\u51fa\u7684HDAN\u7f51\u7edc\u80fd\u6709\u6548\u5206\u5272\u65e9\u4ea7\u513f\u8111MRI\u4e2d\u7684\u767d\u8d28\u548c\u7070\u8d28\uff0c\u4e3a\u65e9\u4ea7\u76f8\u5173\u795e\u7ecf\u53d1\u80b2\u5ef6\u8fdf\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u5f71\u50cf\u5b66\u8bc1\u636e\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2601.11952", "pdf": "https://arxiv.org/pdf/2601.11952", "abs": "https://arxiv.org/abs/2601.11952", "authors": ["Haonan An", "Guang Hua", "Wei Du", "Hangcheng Cao", "Yihang Tao", "Guowen Xu", "Susanto Rahardja", "Yuguang Fang"], "title": "Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal", "categories": ["cs.CV"], "comment": null, "summary": "Box-free model watermarking has gained significant attention in deep neural network (DNN) intellectual property protection due to its model-agnostic nature and its ability to flexibly manage high-entropy image outputs from generative models. Typically operating in a black-box manner, it employs an encoder-decoder framework for watermark embedding and extraction. While existing research has focused primarily on the encoders for the robustness to resist various attacks, the decoders have been largely overlooked, leading to attacks against the watermark. In this paper, we identify one such attack against the decoder, where query responses are utilized to obtain backpropagated gradients to train a watermark remover. To address this issue, we propose Decoder Gradient Shields (DGSs), a family of defense mechanisms, including DGS at the output (DGS-O), at the input (DGS-I), and in the layers (DGS-L) of the decoder, with a closed-form solution for DGS-O and provable performance for all DGS. Leveraging the joint design of reorienting and rescaling of the gradients from watermark channel gradient leaking queries, the proposed DGSs effectively prevent the watermark remover from achieving training convergence to the desired low-loss value, while preserving image quality of the decoder output. We demonstrate the effectiveness of our proposed DGSs in diverse application scenarios. Our experimental results on deraining and image generation tasks with the state-of-the-art box-free watermarking show that our DGSs achieve a defense success rate of 100% under all settings.", "AI": {"tldr": "\u63d0\u51faDecoder Gradient Shields (DGS)\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u91cd\u5b9a\u5411\u548c\u7f29\u653e\u89e3\u7801\u5668\u68af\u5ea6\u6765\u9632\u6b62\u57fa\u4e8e\u68af\u5ea6\u6cc4\u9732\u67e5\u8be2\u7684\u6c34\u5370\u79fb\u9664\u653b\u51fb\uff0c\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u5b9e\u73b0100%\u9632\u5fa1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u800c\u89e3\u7801\u5668\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u5b58\u5728\u9488\u5bf9\u89e3\u7801\u5668\u7684\u653b\u51fb\u3002\u653b\u51fb\u8005\u53ef\u4ee5\u5229\u7528\u67e5\u8be2\u54cd\u5e94\u83b7\u53d6\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u6765\u8bad\u7ec3\u6c34\u5370\u79fb\u9664\u5668\uff0c\u5a01\u80c1\u6c34\u5370\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faDGS\u9632\u5fa1\u673a\u5236\u5bb6\u65cf\uff1aDGS-O\uff08\u8f93\u51fa\u5c42\uff09\u3001DGS-I\uff08\u8f93\u5165\u5c42\uff09\u548cDGS-L\uff08\u5c42\u95f4\uff09\uff0c\u901a\u8fc7\u91cd\u5b9a\u5411\u548c\u7f29\u653e\u6765\u81ea\u6c34\u5370\u901a\u9053\u68af\u5ea6\u6cc4\u9732\u67e5\u8be2\u7684\u68af\u5ea6\uff0c\u9632\u6b62\u6c34\u5370\u79fb\u9664\u5668\u8fbe\u5230\u4f4e\u635f\u5931\u503c\u7684\u8bad\u7ec3\u6536\u655b\u3002DGS-O\u6709\u95ed\u5f0f\u89e3\uff0c\u6240\u6709DGS\u90fd\u6709\u53ef\u8bc1\u660e\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "result": "\u5728\u53bb\u96e8\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u65e0\u76d2\u6c34\u5370\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0cDGS\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u5747\u5b9e\u73b0100%\u7684\u9632\u5fa1\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u7801\u5668\u8f93\u51fa\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "DGS\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u9488\u5bf9\u89e3\u7801\u5668\u7684\u68af\u5ea6\u6cc4\u9732\u653b\u51fb\u95ee\u9898\uff0c\u4e3a\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u5728\u4fdd\u62a4DNN\u77e5\u8bc6\u4ea7\u6743\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.11970", "pdf": "https://arxiv.org/pdf/2601.11970", "abs": "https://arxiv.org/abs/2601.11970", "authors": ["S. M. Khalid Bin Zahid", "Md. Rakibul Hasan Nishat", "Abdul Hasib", "Md. Rakibul Hasan", "Md. Ashiqussalehin", "Md. Sahadat Hossen Sajib", "A. S. M. Ahsanul Sarkar Akib"], "title": "Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms", "categories": ["cs.CV"], "comment": null, "summary": "Intelligent surveillance systems often handle perceptual tasks such as object detection, facial recognition, and emotion analysis independently, but they lack a unified, adaptive runtime scheduler that dynamically allocates computational resources based on contextual triggers. This limits their holistic understanding and efficiency on low-power edge devices. To address this, we present a real-time multi-modal vision framework that integrates object detection, owner-specific face recognition, and emotion detection into a unified pipeline deployed on a Raspberry Pi 5 edge platform. The core of our system is an adaptive scheduling mechanism that reduces computational load by 65\\% compared to continuous processing by selectively activating modules such as, YOLOv8n for object detection, a custom FaceNet-based embedding system for facial recognition, and DeepFace's CNN for emotion classification. Experimental results demonstrate the system's efficacy, with the object detection module achieving an Average Precision (AP) of 0.861, facial recognition attaining 88\\% accuracy, and emotion detection showing strong discriminatory power (AUC up to 0.97 for specific emotions), while operating at 5.6 frames per second. Our work demonstrates that context-aware scheduling is the key to unlocking complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5728\u6811\u8393\u6d3e5\u8fb9\u7f18\u5e73\u53f0\u4e0a\u90e8\u7f72\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u89c6\u89c9\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\u5c06\u7269\u4f53\u68c0\u6d4b\u3001\u4eba\u8138\u8bc6\u522b\u548c\u60c5\u7eea\u68c0\u6d4b\u96c6\u6210\u5230\u7edf\u4e00\u6d41\u7a0b\u4e2d\uff0c\u76f8\u6bd4\u8fde\u7eed\u5904\u7406\u51cf\u5c1165%\u8ba1\u7b97\u8d1f\u8f7d\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\u901a\u5e38\u72ec\u7acb\u5904\u7406\u7269\u4f53\u68c0\u6d4b\u3001\u4eba\u8138\u8bc6\u522b\u548c\u60c5\u7eea\u5206\u6790\u7b49\u611f\u77e5\u4efb\u52a1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u3001\u81ea\u9002\u5e94\u7684\u8fd0\u884c\u65f6\u8c03\u5ea6\u5668\u6765\u6839\u636e\u4e0a\u4e0b\u6587\u89e6\u53d1\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6574\u4f53\u7406\u89e3\u548c\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u65f6\u591a\u6a21\u6001\u89c6\u89c9\u6846\u67b6\uff0c\u96c6\u6210\u4e86YOLOv8n\u7269\u4f53\u68c0\u6d4b\u3001\u57fa\u4e8eFaceNet\u7684\u81ea\u5b9a\u4e49\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u548cDeepFace CNN\u60c5\u7eea\u5206\u7c7b\u3002\u6838\u5fc3\u662f\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6fc0\u6d3b\u6a21\u5757\u6765\u51cf\u5c11\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "\u7269\u4f53\u68c0\u6d4b\u6a21\u5757\u5e73\u5747\u7cbe\u5ea6(AP)\u8fbe\u52300.861\uff0c\u4eba\u8138\u8bc6\u522b\u51c6\u786e\u7387\u4e3a88%\uff0c\u60c5\u7eea\u68c0\u6d4b\u5bf9\u7279\u5b9a\u60c5\u7eea\u7684AUC\u9ad8\u8fbe0.97\uff0c\u7cfb\u7edf\u4ee55.6\u5e27/\u79d2\u7684\u901f\u5ea6\u8fd0\u884c\uff0c\u76f8\u6bd4\u8fde\u7eed\u5904\u7406\u51cf\u5c1165%\u8ba1\u7b97\u8d1f\u8f7d\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5ea6\u662f\u5b9e\u73b0\u4f4e\u6210\u672c\u8fb9\u7f18\u786c\u4ef6\u4e0a\u590d\u6742\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7684\u5173\u952e\uff0c\u4f7f\u667a\u80fd\u611f\u77e5\u66f4\u52a0\u53ef\u8bbf\u95ee\u548c\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2601.11976", "pdf": "https://arxiv.org/pdf/2601.11976", "abs": "https://arxiv.org/abs/2601.11976", "authors": ["Zongmin Li", "Yachuan Li", "Lei Kang", "Dimosthenis Karatzas", "Wenkang Ma"], "title": "AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering", "categories": ["cs.CV"], "comment": "7 pages, 3 figures", "summary": "Multi-page Document Visual Question Answering (MP-DocVQA) remains challenging because long documents not only strain computational resources but also reduce the effectiveness of the attention mechanism in large vision-language models (LVLMs). We tackle these issues with an Adaptive Visual In-document Retrieval (AVIR) framework. A lightweight retrieval model first scores each page for question relevance. Pages are then clustered according to the score distribution to adaptively select relevant content. The clustered pages are screened again by Top-K to keep the context compact. However, for short documents, clustering reliability decreases, so we use a relevance probability threshold to select pages. The selected pages alone are fed to a frozen LVLM for answer generation, eliminating the need for model fine-tuning. The proposed AVIR framework reduces the average page count required for question answering by 70%, while achieving an ANLS of 84.58% on the MP-DocVQA dataset-surpassing previous methods with significantly lower computational cost. The effectiveness of the proposed AVIR is also verified on the SlideVQA and DUDE benchmarks. The code is available at https://github.com/Li-yachuan/AVIR.", "AI": {"tldr": "AVIR\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u89c6\u89c9\u6587\u6863\u68c0\u7d22\uff0c\u5148\u8bc4\u5206\u9875\u9762\u76f8\u5173\u6027\uff0c\u518d\u805a\u7c7b\u7b5b\u9009\uff0c\u4ec5\u5c06\u76f8\u5173\u9875\u9762\u8f93\u5165\u51bb\u7ed3\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u63d0\u5347MP-DocVQA\u6027\u80fd", "motivation": "\u591a\u9875\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u7d27\u5f20\u548c\u6ce8\u610f\u529b\u673a\u5236\u6548\u7387\u964d\u4f4e\u7684\u6311\u6218\uff0c\u957f\u6587\u6863\u4f1a\u964d\u4f4e\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u6548\u679c", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u6846\u67b6\uff1a1) \u8f7b\u91cf\u68c0\u7d22\u6a21\u578b\u8bc4\u5206\u9875\u9762\u76f8\u5173\u6027\uff1b2) \u6839\u636e\u5206\u6570\u5206\u5e03\u805a\u7c7b\u9875\u9762\u81ea\u9002\u5e94\u9009\u62e9\u76f8\u5173\u5185\u5bb9\uff1b3) \u5bf9\u805a\u7c7b\u9875\u9762\u8fdb\u884cTop-K\u7b5b\u9009\u4fdd\u6301\u4e0a\u4e0b\u6587\u7d27\u51d1\uff1b4) \u77ed\u6587\u6863\u4f7f\u7528\u76f8\u5173\u6027\u6982\u7387\u9608\u503c\u9009\u62e9\u9875\u9762\uff1b5) \u4ec5\u5c06\u9009\u5b9a\u9875\u9762\u8f93\u5165\u51bb\u7ed3\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848", "result": "\u5728MP-DocVQA\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u6240\u9700\u9875\u9762\u6570\u51cf\u5c1170%\uff0cANLS\u8fbe\u523084.58%\uff0c\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\uff1b\u5728SlideVQA\u548cDUDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "conclusion": "AVIR\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u9875\u9762\u68c0\u7d22\u548c\u7b5b\u9009\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u9875\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u7684\u8ba1\u7b97\u548c\u6ce8\u610f\u529b\u6548\u7387\u95ee\u9898\uff0c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd"}}
{"id": "2601.11981", "pdf": "https://arxiv.org/pdf/2601.11981", "abs": "https://arxiv.org/abs/2601.11981", "authors": ["Jian Lang", "Rongpei Hong", "Ting Zhong", "Yong Wang", "Fan Zhou"], "title": "Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection", "categories": ["cs.CV"], "comment": "13 pages. Accepted by KDD 2026 research track. Codes are released at https://github.com/Jian-Lang/RADAR", "summary": "Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances' representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.", "AI": {"tldr": "RADAR\u662f\u4e00\u4e2a\u7528\u4e8e\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u8303\u5f0f\uff0c\u5229\u7528\u76ee\u6807\u57df\u4e2d\u7684\u7a33\u5b9a\u89c6\u9891\u6765\u6307\u5bfc\u8bed\u4e49\u76f8\u5173\u4f46\u4e0d\u7a33\u5b9a\u5b9e\u4f8b\u7684\u9c81\u68d2\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u65b0\u95fb\u4e3b\u9898\u5206\u5e03\u4e00\u81f4\uff0c\u65e0\u6cd5\u68c0\u6d4b\u4e0e\u65b0\u5174\u4e8b\u4ef6\u548c\u672a\u89c1\u4e3b\u9898\u76f8\u5173\u7684\u5047\u65b0\u95fb\u89c6\u9891\u3002\u9700\u8981\u80fd\u591f\u9002\u5e94\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRADAR\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u57fa\u4e8e\u71b5\u9009\u62e9\u7684\u68c0\u7d22\u673a\u5236\uff0c\u4e3a\u89c6\u9891\u63d0\u4f9b\u7a33\u5b9a\u3001\u76f8\u5173\u7684\u53c2\u8003\u8fdb\u884c\u9002\u5e94\uff1b2) \u7a33\u5b9a\u951a\u70b9\u5f15\u5bfc\u7684\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u5206\u5e03\u7ea7\u5339\u914d\u5c06\u4e0d\u7a33\u5b9a\u5b9e\u4f8b\u8868\u793a\u4e0e\u6e90\u57df\u5bf9\u9f50\uff1b3) \u76ee\u6807\u57df\u611f\u77e5\u7684\u81ea\u8bad\u7ec3\u8303\u5f0f\uff0c\u751f\u6210\u7531\u7a33\u5b9a\u53c2\u8003\u589e\u5f3a\u7684\u4fe1\u606f\u6027\u4f2a\u6807\u7b7e\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eRADAR\u5728\u6d4b\u8bd5\u65f6\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u5bf9\u672a\u89c1\u5047\u65b0\u95fb\u89c6\u9891\u4e3b\u9898\u8fdb\u884c\u5f3a\u5927\u7684\u5373\u65f6\u9002\u5e94\u3002", "conclusion": "RADAR\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u68c0\u7d22\u5f15\u5bfc\u81ea\u9002\u5e94\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u65b0\u5174\u4e8b\u4ef6\u548c\u672a\u89c1\u4e3b\u9898\u5047\u65b0\u95fb\u89c6\u9891\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.11983", "pdf": "https://arxiv.org/pdf/2601.11983", "abs": "https://arxiv.org/abs/2601.11983", "authors": ["Md. Asiful Islam", "Abdul Hasib", "Tousif Mahmud Emon", "Khandaker Tabin Hasan", "A. S. M. Ahsanul Sarkar Akib"], "title": "An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System", "categories": ["cs.CV"], "comment": null, "summary": "The growing number of differently-abled and elderly individuals demands affordable, intelligent wheelchairs that combine safe navigation with health monitoring. Traditional wheelchairs lack dynamic features, and many smart alternatives remain costly, single-modality, and limited in health integration. Motivated by the pressing demand for advanced, personalized, and affordable assistive technologies, we propose a comprehensive AI-IoT based smart wheelchair system that incorporates glove-based gesture control for hands-free navigation, real-time object detection using YOLOv8 with auditory feedback for obstacle avoidance, and ultrasonic for immediate collision avoidance. Vital signs (heart rate, SpO$_2$, ECG, temperature) are continuously monitored, uploaded to ThingSpeak, and trigger email alerts for critical conditions. Built on a modular and low-cost architecture, the gesture control achieved a 95.5\\% success rate, ultrasonic obstacle detection reached 94\\% accuracy, and YOLOv8-based object detection delivered 91.5\\% Precision, 90.2\\% Recall, and a 90.8\\% F1-score. This integrated, multi-modal approach offers a practical, scalable, and affordable solution, significantly enhancing user autonomy, safety, and independence by bridging the gap between innovative research and real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eAI-IoT\u7684\u667a\u80fd\u8f6e\u6905\u7cfb\u7edf\uff0c\u6574\u5408\u624b\u52bf\u63a7\u5236\u3001\u7269\u4f53\u68c0\u6d4b\u548c\u5065\u5eb7\u76d1\u6d4b\uff0c\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u591a\u6a21\u6001\u7684\u8f85\u52a9\u89e3\u51b3\u65b9\u6848", "motivation": "\u9488\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u6b8b\u969c\u4eba\u58eb\u548c\u8001\u5e74\u4eba\u9700\u6c42\uff0c\u4f20\u7edf\u8f6e\u6905\u7f3a\u4e4f\u52a8\u6001\u529f\u80fd\uff0c\u73b0\u6709\u667a\u80fd\u8f6e\u6905\u6210\u672c\u9ad8\u3001\u529f\u80fd\u5355\u4e00\u4e14\u5065\u5eb7\u76d1\u6d4b\u96c6\u6210\u4e0d\u8db3\uff0c\u9700\u8981\u5148\u8fdb\u3001\u4e2a\u6027\u5316\u4e14\u8d1f\u62c5\u5f97\u8d77\u7684\u8f85\u52a9\u6280\u672f", "method": "\u91c7\u7528\u57fa\u4e8eAI-IoT\u7684\u7efc\u5408\u7cfb\u7edf\uff1a\u624b\u5957\u624b\u52bf\u63a7\u5236\u5b9e\u73b0\u514d\u624b\u5bfc\u822a\uff0cYOLOv8\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\u914d\u5408\u542c\u89c9\u53cd\u9988\u907f\u969c\uff0c\u8d85\u58f0\u6ce2\u7528\u4e8e\u5373\u65f6\u78b0\u649e\u907f\u514d\uff0c\u6301\u7eed\u76d1\u6d4b\u5fc3\u7387\u3001\u8840\u6c27\u3001\u5fc3\u7535\u56fe\u3001\u4f53\u6e29\u7b49\u751f\u547d\u4f53\u5f81\u5e76\u4e0a\u4f20ThingSpeak\u5e73\u53f0", "result": "\u624b\u52bf\u63a7\u5236\u6210\u529f\u738795.5%\uff0c\u8d85\u58f0\u6ce2\u969c\u788d\u7269\u68c0\u6d4b\u51c6\u786e\u738794%\uff0cYOLOv8\u7269\u4f53\u68c0\u6d4b\u7cbe\u5ea691.5%\u3001\u53ec\u56de\u738790.2%\u3001F1\u5206\u657090.8%\uff0c\u7cfb\u7edf\u63d0\u4f9b\u6a21\u5757\u5316\u4f4e\u6210\u672c\u67b6\u6784", "conclusion": "\u8fd9\u79cd\u96c6\u6210\u591a\u6a21\u6001\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u5b9e\u60e0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f25\u5408\u521b\u65b0\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7528\u6237\u7684\u81ea\u4e3b\u6027\u3001\u5b89\u5168\u6027\u548c\u72ec\u7acb\u6027"}}
{"id": "2601.11987", "pdf": "https://arxiv.org/pdf/2601.11987", "abs": "https://arxiv.org/abs/2601.11987", "authors": ["Khaled Berkani"], "title": "Structural Graph Neural Networks with Anatomical Priors for Explainable Chest X-ray Diagnosis", "categories": ["cs.CV"], "comment": "15 pages, 3 figures, 3 tables", "summary": "We present a structural graph reasoning framework that incorporates explicit anatomical priors for explainable vision-based diagnosis. Convolutional feature maps are reinterpreted as patch-level graphs, where nodes encode both appearance and spatial coordinates, and edges reflect local structural adjacency. Unlike conventional graph neural networks that rely on generic message passing, we introduce a custom structural propagation mechanism that explicitly models relative spatial relations as part of the reasoning process. This design enables the graph to act as an inductive bias for structured inference rather than a passive relational representation. The proposed model jointly supports node-level lesion-aware predictions and graph-level diagnostic reasoning, yielding intrinsic explainability through learned node importance scores without relying on post-hoc visualization techniques. We demonstrate the approach through a chest X-ray case study, illustrating how structural priors guide relational reasoning and improve interpretability. While evaluated in a medical imaging context, the framework is domain-agnostic and aligns with the broader vision of graph-based reasoning across artificial intelligence systems. This work contributes to the growing body of research exploring graphs as computational substrates for structure-aware and explainable learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u89e3\u5256\u5148\u9a8c\u7684\u7ed3\u6784\u56fe\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u8bca\u65ad\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u7684\u7ed3\u6784\u4f20\u64ad\u673a\u5236\u5efa\u6a21\u7a7a\u95f4\u5173\u7cfb\uff0c\u652f\u6301\u8282\u70b9\u7ea7\u75c5\u53d8\u611f\u77e5\u548c\u56fe\u7ea7\u8bca\u65ad\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u4f9d\u8d56\u901a\u7528\u6d88\u606f\u4f20\u9012\uff0c\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u7ed3\u6784\u5173\u7cfb\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u9700\u8981\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u5377\u79ef\u7279\u5f81\u56fe\u91cd\u65b0\u89e3\u91ca\u4e3a\u8865\u4e01\u7ea7\u56fe\uff0c\u8282\u70b9\u7f16\u7801\u5916\u89c2\u548c\u7a7a\u95f4\u5750\u6807\uff0c\u8fb9\u53cd\u6620\u5c40\u90e8\u7ed3\u6784\u90bb\u63a5\uff1b\u5f15\u5165\u5b9a\u5236\u5316\u7ed3\u6784\u4f20\u64ad\u673a\u5236\uff0c\u663e\u5f0f\u5efa\u6a21\u76f8\u5bf9\u7a7a\u95f4\u5173\u7cfb\u4f5c\u4e3a\u63a8\u7406\u8fc7\u7a0b\u7684\u4e00\u90e8\u5206\u3002", "result": "\u5728\u80f8\u90e8X\u5149\u6848\u4f8b\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u7ed3\u6784\u5148\u9a8c\u5982\u4f55\u6307\u5bfc\u5173\u7cfb\u63a8\u7406\u5e76\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u6a21\u578b\u652f\u6301\u8282\u70b9\u7ea7\u75c5\u53d8\u611f\u77e5\u9884\u6d4b\u548c\u56fe\u7ea7\u8bca\u65ad\u63a8\u7406\uff0c\u901a\u8fc7\u5b66\u4e60\u8282\u70b9\u91cd\u8981\u6027\u5206\u6570\u63d0\u4f9b\u5185\u5728\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u9886\u57df\u65e0\u5173\u7684\uff0c\u4e3a\u7ed3\u6784\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u5b66\u4e60\u63d0\u4f9b\u4e86\u57fa\u4e8e\u56fe\u7684\u8ba1\u7b97\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u56fe\u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7ed3\u6784\u5316\u63a8\u7406\u5de5\u5177\u7684\u7814\u7a76\u3002"}}
{"id": "2601.11990", "pdf": "https://arxiv.org/pdf/2601.11990", "abs": "https://arxiv.org/abs/2601.11990", "authors": ["Yiming Li", "Chen Cai", "Tianyi Liu", "Dan Lin", "Wenqian Wang", "Wenfei Liang", "Bingbing Li", "Kim-Hui Yap"], "title": "DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset", "categories": ["cs.CV"], "comment": null, "summary": "In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.", "AI": {"tldr": "\u63d0\u51faDAOS\u6570\u636e\u96c6\u548cAOR-Net\u6a21\u578b\uff0c\u901a\u8fc7\u5efa\u6a21\u4eba-\u7269\u5173\u7cfb\u6765\u63d0\u5347\u9a7e\u9a76\u5458\u52a8\u4f5c\u8bc6\u522b\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u9a7e\u9a76\u5458\u76d1\u63a7\u6570\u636e\u96c6\u7f3a\u4e4f\u7cbe\u786e\u7684\u7269\u4f53\u4f4d\u7f6e\u6807\u6ce8\u6216\u672a\u5c06\u7269\u4f53\u4e0e\u76f8\u5173\u52a8\u4f5c\u5173\u8054\uff0c\u5bfc\u81f4\u76f8\u4f3c\u7684\u4e0a\u534a\u8eab\u52a8\u4f5c\u96be\u4ee5\u533a\u5206", "method": "1) \u521b\u5efaDAOS\u6570\u636e\u96c6\uff1a\u5305\u542b9,787\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u6807\u6ce836\u79cd\u7ec6\u7c92\u5ea6\u9a7e\u9a76\u5458\u52a8\u4f5c\u548c15\u79cd\u7269\u4f53\u7c7b\u522b\uff1b2) \u63d0\u51faAOR-Net\u6a21\u578b\uff1a\u901a\u8fc7\u591a\u7ea7\u63a8\u7406\u548c\u52a8\u4f5c\u94fe\u63d0\u793a\u673a\u5236\u5efa\u6a21\u52a8\u4f5c\u3001\u7269\u4f53\u53ca\u5176\u5173\u7cfb\uff0c\u5f15\u5165\u601d\u7ef4\u6df7\u5408\u6a21\u5757\u52a8\u6001\u9009\u62e9\u5173\u952e\u77e5\u8bc6", "result": "AOR-Net\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u7269\u4f53\u4e30\u5bcc\u548c\u7a00\u7f3a\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027", "conclusion": "\u901a\u8fc7\u5efa\u6a21\u9a7e\u9a76\u5458\u52a8\u4f5c\u4e0e\u7269\u4f53\u4e4b\u95f4\u7684\u903b\u8f91\u5173\u7cfb\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u9a7e\u9a76\u5458\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\uff0cDAOS\u6570\u636e\u96c6\u548cAOR-Net\u4e3a\u9a7e\u9a76\u5458\u76d1\u63a7\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12010", "pdf": "https://arxiv.org/pdf/2601.12010", "abs": "https://arxiv.org/abs/2601.12010", "authors": ["Yifei Chen", "Ross Greer"], "title": "SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine", "categories": ["cs.CV"], "comment": null, "summary": "The safety validation of autonomous robotic vehicles hinges on systematically testing their planning and control stacks against rare, safety-critical scenarios. Mining these long-tail events from massive real-world driving logs is therefore a critical step in the robotic development lifecycle. The goal of the Scenario Mining task is to retrieve useful information to enable targeted re-simulation, regression testing, and failure analysis of the robot's decision-making algorithms. RefAV, introduced by the Argoverse team, is an end-to-end framework that uses large language models (LLMs) to spatially and temporally localize scenarios described in natural language. However, this process performs retrieval on trajectory labels, ignoring the direct connection between natural language and raw RGB images, which runs counter to the intuition of video retrieval; it also depends on the quality of upstream 3D object detection and tracking. Further, inaccuracies in trajectory data lead to inaccuracies in downstream spatial and temporal localization. To address these issues, we propose Robust Scenario Mining for Robotic Autonomy from Coarse to Fine (SMc2f), a coarse-to-fine pipeline that employs vision-language models (VLMs) for coarse image-text filtering, builds a database of successful mining cases on top of RefAV and automatically retrieves exemplars to few-shot condition the LLM for more robust retrieval, and introduces text-trajectory contrastive learning to pull matched pairs together and push mismatched pairs apart in a shared embedding space, yielding a fine-grained matcher that refines the LLM's candidate trajectories. Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency.", "AI": {"tldr": "\u63d0\u51faSMc2f\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u6d41\u7a0b\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u6316\u6398\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c-\u8f68\u8ff9\u5bf9\u6bd4\u5b66\u4e60\uff0c\u63d0\u5347\u68c0\u7d22\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709RefAV\u6846\u67b6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4ec5\u57fa\u4e8e\u8f68\u8ff9\u6807\u7b7e\u8fdb\u884c\u68c0\u7d22\uff0c\u5ffd\u7565\u4e86\u81ea\u7136\u8bed\u8a00\u4e0e\u539f\u59cbRGB\u56fe\u50cf\u7684\u76f4\u63a5\u8054\u7cfb\uff1b2) \u4f9d\u8d56\u4e0a\u6e383D\u76ee\u6807\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7684\u8d28\u91cf\uff0c\u8f68\u8ff9\u6570\u636e\u4e0d\u51c6\u786e\u4f1a\u5bfc\u81f4\u4e0b\u6e38\u65f6\u7a7a\u5b9a\u4f4d\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51fa\u7c97\u5230\u7ec6\u7684SMc2f\u6846\u67b6\uff1a1) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7c97\u7c92\u5ea6\u56fe\u50cf-\u6587\u672c\u8fc7\u6ee4\uff1b2) \u5728RefAV\u57fa\u7840\u4e0a\u6784\u5efa\u6210\u529f\u6316\u6398\u6848\u4f8b\u6570\u636e\u5e93\uff0c\u81ea\u52a8\u68c0\u7d22\u793a\u4f8b\u8fdb\u884c\u5c11\u6837\u672c\u5b66\u4e60\uff1b3) \u5f15\u5165\u6587\u672c-\u8f68\u8ff9\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u62c9\u8fd1\u5339\u914d\u5bf9\u3001\u63a8\u5f00\u4e0d\u5339\u914d\u5bf9\uff0c\u83b7\u5f97\u7ec6\u7c92\u5ea6\u5339\u914d\u5668\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SMc2f\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u573a\u666f\u6316\u6398\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12015", "pdf": "https://arxiv.org/pdf/2601.12015", "abs": "https://arxiv.org/abs/2601.12015", "authors": ["Pavan Kumar Yata", "Pediredla Pradeep", "Goli Himanish", "Swathi M"], "title": "SAR-Based Marine Oil Spill Detection Using the DeepSegFusion Architecture", "categories": ["cs.CV"], "comment": "12 pages, 6 figures. Submitted to arXiv. Code and dataset details included in the paper", "summary": "Detection of oil spills from satellite images is essential for both environmental surveillance and maritime safety. Traditional threshold-based methods frequently encounter performance degradation due to very high false alarm rates caused by look-alike phenomena such as wind slicks and ship wakes. Here, a hybrid deep learning model, DeepSegFusion, is presented for oil spill segmentation in Synthetic Aperture Radar (SAR) images. The model uses SegNet and DeepLabV3+ integrated with an attention-based feature fusion mechanism to achieve better boundary precision as well as improved contextual understanding. Results obtained on SAR oil spill datasets, including ALOS PALSAR imagery, confirm that the proposed DeepSegFusion model achieves an accuracy of 94.85%, an Intersection over Union (IoU) of 0.5685, and a ROC-AUC score of 0.9330. The proposed method delivers more than three times fewer false detections compared to individual baseline models and traditional non-segmentation methods, achieving a reduction of 64.4%. These results indicate that DeepSegFusion is a stable model under various marine conditions and can therefore be used in near real-time oil spill monitoring scenarios.", "AI": {"tldr": "\u63d0\u51faDeepSegFusion\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8eSAR\u56fe\u50cf\u4e2d\u7684\u6ea2\u6cb9\u5206\u5272\uff0c\u7ed3\u5408SegNet\u548cDeepLabV3+\u5e76\u52a0\u5165\u6ce8\u610f\u529b\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u7387\uff0c\u9002\u7528\u4e8e\u8fd1\u5b9e\u65f6\u6ea2\u6cb9\u76d1\u6d4b\u3002", "motivation": "\u4f20\u7edf\u9608\u503c\u65b9\u6cd5\u5728\u536b\u661f\u56fe\u50cf\u6ea2\u6cb9\u68c0\u6d4b\u4e2d\u56e0\u98ce\u6d6a\u6761\u7eb9\u3001\u8239\u8236\u5c3e\u6d41\u7b49\u7c7b\u4f3c\u73b0\u8c61\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDeepSegFusion\u6df7\u5408\u6a21\u578b\uff0c\u96c6\u6210SegNet\u548cDeepLabV3+\u4e24\u79cd\u5206\u5272\u7f51\u7edc\uff0c\u5e76\u52a0\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u63d0\u5347\u8fb9\u754c\u7cbe\u5ea6\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5728SAR\u6ea2\u6cb9\u6570\u636e\u96c6\uff08\u5305\u62ecALOS PALSAR\u56fe\u50cf\uff09\u4e0a\u8fbe\u523094.85%\u51c6\u786e\u7387\u30010.5685 IoU\u548c0.9330 ROC-AUC\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u51cf\u5c1164.4%\u8bef\u68c0\uff0c\u8bef\u62a5\u7387\u964d\u4f4e\u8d85\u8fc73\u500d\u3002", "conclusion": "DeepSegFusion\u5728\u4e0d\u540c\u6d77\u6d0b\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5b9a\uff0c\u80fd\u591f\u7528\u4e8e\u8fd1\u5b9e\u65f6\u6ea2\u6cb9\u76d1\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u5927\u5e45\u964d\u4f4e\u8bef\u62a5\u7387\u3002"}}
{"id": "2601.12020", "pdf": "https://arxiv.org/pdf/2601.12020", "abs": "https://arxiv.org/abs/2601.12020", "authors": ["Guillermo Figueroa-Araneda", "Iris Diana Jimenez", "Florian Hofherr", "Manny Ko", "Hector Andrade-Loarca", "Daniel Cremers"], "title": "DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering", "categories": ["cs.CV"], "comment": null, "summary": "Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).\n  We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.\n  Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.", "AI": {"tldr": "DIAMOND-SSS\uff1a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u589e\u5f3a\u7a00\u758f\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u534a\u900f\u660e\u6750\u8d28\u91cd\u5efa\uff0c\u4ec5\u970010\u5f20\u56fe\u50cf\u5373\u53ef\u5b8c\u6210\uff0c\u51cf\u5c1190%\u771f\u5b9e\u91c7\u96c6\u9700\u6c42", "motivation": "\u534a\u900f\u660e\u6750\u8d28\uff08\u5982\u8721\u3001\u7389\u3001\u5927\u7406\u77f3\u3001\u76ae\u80a4\uff09\u7684\u6b21\u8868\u9762\u6563\u5c04\u6548\u679c\u5728\u795e\u7ecf\u6e32\u67d3\u4e2d\u5efa\u6a21\u56f0\u96be\uff0c\u9700\u8981\u5bc6\u96c6\u7684\u591a\u89c6\u89d2\u591a\u5149\u7167\u6570\u636e\u96c6\uff08\u901a\u5e38\u8d85\u8fc7100\u4e2a\u89c6\u89d2\u548c112\u4e2aOLAT\uff09\uff0c\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u6602", "method": "1. \u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u65b0\u89c6\u89d2\u5408\u6210\u548c\u91cd\u5149\u7167\uff0c\u57fa\u4e8e\u4f30\u8ba1\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u4ec5\u9700\u4e0d\u52307%\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff1b2. \u5f15\u5165\u5149\u7167\u65e0\u5173\u7684\u51e0\u4f55\u5148\u9a8c\uff1a\u591a\u89c6\u89d2\u8f6e\u5ed3\u4e00\u81f4\u6027\u635f\u5931\u548c\u591a\u89c6\u89d2\u6df1\u5ea6\u4e00\u81f4\u6027\u635f\u5931\uff0c\u7a33\u5b9a\u7a00\u758f\u6216\u5408\u6210\u76d1\u7763\u4e0b\u7684\u91cd\u5efa", "result": "\u5728\u6240\u6709\u7a00\u758f\u5ea6\u6761\u4ef6\u4e0b\uff0cDIAMOND-SSS\u5728\u53ef\u91cd\u5149\u7167\u9ad8\u65af\u6e32\u67d3\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u8d28\u91cf\uff0c\u76f8\u6bd4SSS-3DGS\u51cf\u5c11\u9ad8\u8fbe90%\u7684\u771f\u5b9e\u91c7\u96c6\u9700\u6c42\uff0c\u4ec5\u970010\u5f20\u56fe\u50cf\u5373\u53ef\u5b8c\u6210\u9ad8\u8d28\u91cf\u91cd\u5efa", "conclusion": "DIAMOND-SSS\u901a\u8fc7\u6269\u6563\u6a21\u578b\u6570\u636e\u589e\u5f3a\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u4ece\u6781\u7a00\u758f\u76d1\u7763\uff08\u4ec510\u5f20\u56fe\u50cf\uff09\u7684\u9ad8\u4fdd\u771f\u534a\u900f\u660e\u6750\u8d28\u91cd\u5efa\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u6570\u636e\u91c7\u96c6\u6210\u672c"}}
{"id": "2601.12049", "pdf": "https://arxiv.org/pdf/2601.12049", "abs": "https://arxiv.org/abs/2601.12049", "authors": ["Chenchen Zhao", "Muxi Chen", "Qiang Xu"], "title": "\\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 13 figures", "summary": "Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.", "AI": {"tldr": "FocaLogic\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u903b\u8f91\u8868\u793a\u6765\u91cf\u5316\u548c\u89e3\u91ca\u6a21\u578b\u51b3\u7b56\uff0c\u8bc6\u522b\u5173\u952e\u89c6\u89c9\u533a\u57df\u5e76\u8f6c\u5316\u4e3a\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u63d0\u4f9b\u5b9a\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u8981\u4e48\u4f9d\u8d56\u767d\u76d2\u6a21\u578b\u8bbf\u95ee\u6743\u9650\uff0c\u8981\u4e48\u7f3a\u4e4f\u5b9a\u91cf\u4e25\u8c28\u6027\u3002\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u3001\u53ef\u6269\u5c55\u7684\u5b9a\u91cf\u89e3\u51b3\u65b9\u6848\u3002", "method": "FocaLogic\u6846\u67b6\u8bc6\u522b\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u7684\u6700\u5c0f\u53ef\u89e3\u91ca\u89c6\u89c9\u533a\u57df\u5b50\u96c6\uff08\u79f0\u4e3a\u89c6\u89c9\u7126\u70b9\uff09\uff0c\u5c06\u8fd9\u4e9b\u89c6\u89c9\u7126\u70b9\u8f6c\u5316\u4e3a\u7cbe\u786e\u7d27\u51d1\u7684\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u51fa\u7126\u70b9\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u6563\u5ea6\u7b49\u5b9a\u91cf\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u663e\u793aFocaLogic\u80fd\u591f\u63ed\u793a\u91cd\u8981\u6d1e\u5bdf\uff1a\u8bad\u7ec3\u5bfc\u81f4\u7684\u6ce8\u610f\u529b\u96c6\u4e2d\u3001\u901a\u8fc7\u6cdb\u5316\u63d0\u9ad8\u7126\u70b9\u51c6\u786e\u6027\u3001\u4ee5\u53ca\u5728\u504f\u89c1\u548c\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u5f02\u5e38\u7126\u70b9\u3002\u6846\u67b6\u63d0\u4f9b\u4e86\u7cfb\u7edf\u3001\u53ef\u6269\u5c55\u7684\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u65b9\u6848\u3002", "conclusion": "FocaLogic\u4e3a\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u4e14\u5b9a\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u903b\u8f91\u8868\u793a\u548c\u5b9a\u91cf\u6307\u6807\u5b9e\u73b0\u4e86\u900f\u660e\u3001\u7ed3\u6784\u5316\u7684\u6a21\u578b\u51b3\u7b56\u89e3\u91ca\u3002"}}
{"id": "2601.12051", "pdf": "https://arxiv.org/pdf/2601.12051", "abs": "https://arxiv.org/abs/2601.12051", "authors": ["Weixin Ye", "Wei Wang", "Yahui Liu", "Yue Song", "Bin Ren", "Wei Bi", "Rita Cucchiara", "Nicu Sebe"], "title": "A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models", "categories": ["cs.CV"], "comment": "9 figures, 12 tables", "summary": "In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \\textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack", "AI": {"tldr": "\u63d0\u51faMJP\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u6253\u4e71token\u987a\u5e8f\u5e76\u7528\u53ef\u5b66\u4e60\u7684\u672a\u77e5\u4f4d\u7f6e\u5d4c\u5165\u8fdb\u884c\u63a9\u7801\uff0c\u7834\u574fTransformer\u4e2d\u4f4d\u7f6e\u5d4c\u5165\u7684\u5c40\u90e8\u7a7a\u95f4\u4fe1\u606f\uff0c\u65e2\u80fd\u9632\u5fa1\u68af\u5ea6\u653b\u51fb\u53c8\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2dTransformer\u9762\u4e34\u68af\u5ea6\u653b\u51fb\u7684\u4e25\u91cd\u5a01\u80c1\uff0c\u7814\u7a76\u53d1\u73b0\u4f4d\u7f6e\u5d4c\u5165\u7684\u68af\u5ea6\u5305\u542b\u8db3\u591f\u4fe1\u606f\u53ef\u7528\u4e8e\u91cd\u6784\u8f93\u5165\u6570\u636e\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5b89\u5168\u95ee\u9898", "method": "\u63d0\u51faMasked Jigsaw Puzzle\u6846\u67b6\uff1a1) \u968f\u673a\u6253\u4e71token\u987a\u5e8f\u7834\u574ftoken\u987a\u5e8f\uff1b2) \u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u672a\u77e5\u4f4d\u7f6e\u5d4c\u5165\u63a9\u7801\u88ab\u6253\u4e71token\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u7834\u574f\u5c40\u90e8\u7a7a\u95f4\u4fe1\u606f", "result": "MJP\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u6a21\u578b\u5bf9\u68af\u5ea6\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u80fd\u63d0\u5347\u5728\u56fe\u50cf\u5206\u7c7b\uff08\u5982ImageNet-1K\uff09\u548c\u6587\u672c\u60c5\u611f\u5206\u6790\uff08\u5982Yelp\u548cAmazon\uff09\u7b49\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "conclusion": "MJP\u662f\u9002\u7528\u4e8e\u4e0d\u540cTransformer\u6a21\u578b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u540c\u65f6\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u548c\u6027\u80fd\u63d0\u5347\u9700\u6c42\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2601.12052", "pdf": "https://arxiv.org/pdf/2601.12052", "abs": "https://arxiv.org/abs/2601.12052", "authors": ["Zaiyan Zhang", "Jie Li", "Shaowei Shi", "Qiangqiang Yuan"], "title": "Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation", "categories": ["cs.CV"], "comment": "Submitted to IGARSS 2026 Conference", "summary": "Optical remote sensing imagery is indispensable for Earth observation, yet persistent cloud occlusion limits its downstream utility. Most cloud removal (CR) methods are optimized for low-level fidelity and can over-smooth textures and boundaries that are critical for analysis-ready data (ARD), leading to a mismatch between visually plausible restoration and semantic utility. To bridge this gap, we propose TDP-CR, a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation. Central to our approach is a Prompt-Guided Fusion (PGF) mechanism, which utilizes a learnable degradation prompt to encode cloud thickness and spatial uncertainty. By combining global channel context with local prompt-conditioned spatial bias, PGF adaptively integrates Synthetic Aperture Radar (SAR) information only where optical data is corrupted. We further introduce a parameter-efficient two-phase training strategy that decouples reconstruction and semantic representation learning. Experiments on the LuojiaSET-OSFCR dataset demonstrate the superiority of our framework: TDP-CR surpasses heavy state-of-the-art baselines by 0.18 dB in PSNR while using only 15\\% of the parameters, and achieves a 1.4\\% improvement in mIoU consistently against multi-task competitors, effectively delivering analysis-ready data.", "AI": {"tldr": "TDP-CR\uff1a\u4e00\u4e2a\u4efb\u52a1\u9a71\u52a8\u7684\u591a\u6a21\u6001\u4e91\u53bb\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u9000\u5316\u63d0\u793a\u878d\u5408SAR\u4fe1\u606f\uff0c\u540c\u65f6\u8fdb\u884c\u4e91\u53bb\u9664\u548c\u571f\u5730\u8986\u76d6\u5206\u5272\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u4e91\u53bb\u9664\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u4f4e\u5c42\u4fdd\u771f\u5ea6\uff0c\u4f1a\u8fc7\u5ea6\u5e73\u6ed1\u7eb9\u7406\u548c\u8fb9\u754c\uff0c\u5bfc\u81f4\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u4fee\u590d\u4e0e\u8bed\u4e49\u5b9e\u7528\u6027\u4e0d\u5339\u914d\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5206\u6790\u5c31\u7eea\u6570\u636e\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u9a71\u52a8\u7684\u591a\u6a21\u6001\u6846\u67b6TDP-CR\uff0c\u5305\u542b\u63d0\u793a\u5f15\u5bfc\u878d\u5408\u673a\u5236\uff08PGF\uff09\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u9000\u5316\u63d0\u793a\u7f16\u7801\u4e91\u539a\u5ea6\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408\u5168\u5c40\u901a\u9053\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u63d0\u793a\u6761\u4ef6\u7a7a\u95f4\u504f\u7f6e\uff0c\u81ea\u9002\u5e94\u878d\u5408SAR\u4fe1\u606f\u3002\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u8026\u91cd\u5efa\u548c\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728LuojiaSET-OSFCR\u6570\u636e\u96c6\u4e0a\uff0cTDP-CR\u5728PSNR\u4e0a\u8d85\u8fc7\u6700\u5148\u8fdb\u57fa\u7ebf0.18dB\uff0c\u4ec5\u4f7f\u752815%\u53c2\u6570\uff1b\u5728mIoU\u4e0a\u6bd4\u591a\u4efb\u52a1\u7ade\u4e89\u8005\u63d0\u53471.4%\uff0c\u6709\u6548\u63d0\u4f9b\u5206\u6790\u5c31\u7eea\u6570\u636e\u3002", "conclusion": "TDP-CR\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u7684\u591a\u6a21\u6001\u8bbe\u8ba1\u548c\u63d0\u793a\u5f15\u5bfc\u878d\u5408\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e91\u53bb\u9664\u4e2d\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0e\u8bed\u4e49\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u5730\u7403\u89c2\u6d4b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u5206\u6790\u5c31\u7eea\u6570\u636e\u3002"}}
{"id": "2601.12055", "pdf": "https://arxiv.org/pdf/2601.12055", "abs": "https://arxiv.org/abs/2601.12055", "authors": ["Lina Meyer", "Felix Wissel", "Tobias Knopp", "Susanne Pfefferle", "Ralf Fliegert", "Maximilian Sandmann", "Liana Uebler", "Franziska M\u00f6ckl", "Bj\u00f6rn-Philipp Diercks", "David Lohr", "Ren\u00e9 Werner"], "title": "Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP.", "AI": {"tldr": "AUTO-DIP\uff1a\u57fa\u4e8e\u56fe\u50cf\u5143\u6570\u636e\u76f8\u4f3c\u6027\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u53c2\u6570\u81ea\u52a8\u8fc1\u79fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u8367\u5149\u663e\u5fae\u955c\u56fe\u50cf\u53bb\u566a\uff0c\u65e0\u9700\u4e3a\u6bcf\u5f20\u56fe\u50cf\u91cd\u65b0\u4f18\u5316\u53c2\u6570\u3002", "motivation": "\u4f20\u7edf\u65e0\u76d1\u7763\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\uff08DIP\uff09\u9700\u8981\u4e3a\u6bcf\u5f20\u65b0\u56fe\u50cf\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u548c\u505c\u6b62\u70b9\uff0c\u8017\u65f6\u4e14\u9650\u5236\u4e86\u5728\u591a\u56fe\u50cf\u5904\u7406\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u4f5c\u8005\u5047\u8bbe\u76f8\u4f3c\u56fe\u50cf\u5728DIP\u53bb\u566a\u4e2d\u5177\u6709\u53ef\u6bd4\u7684\u6700\u4f18\u53c2\u6570\u914d\u7f6e\uff0c\u4ece\u800c\u53ef\u5b9e\u73b0\u514d\u4f18\u5316\u7684DIP\u5e94\u7528\u3002", "method": "1. \u4ece\u5f00\u6e90\u6570\u636e\u96c6\u751f\u6210\u6821\u51c6\u96c6\uff08110\u5f20\uff09\u548c\u9a8c\u8bc1\u96c6\uff0855\u5f20\uff09\u8bed\u4e49\u4e0d\u540c\u7684\u56fe\u50cf\uff1b2. \u9488\u5bf9\u7406\u60f3U-net\u67b6\u6784\u548c\u505c\u6b62\u70b9\u8fdb\u884c\u7f51\u7edc\u67b6\u6784\u641c\u7d22\uff1b3. \u57fa\u4e8e\u56fe\u50cf\u5143\u6570\u636e\u76f8\u4f3c\u6027\uff08\u663e\u5fae\u955c\u7c7b\u578b\u3001\u6210\u50cf\u6837\u672c\u7b49\uff09\u800c\u975e\u5b9a\u91cf\u56fe\u50cf\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u8fdb\u884c\u53c2\u6570\u8fc1\u79fb\uff1b4. \u5b9e\u73b0AUTO-DIP\u81ea\u52a8\u53c2\u6570\u8fc1\u79fb\u7ba1\u9053\u3002", "result": "1. \u57fa\u4e8e\u56fe\u50cf\u5143\u6570\u636e\u76f8\u4f3c\u6027\u7684\u53c2\u6570\u8fc1\u79fb\u6548\u679c\u4f18\u4e8e\u57fa\u4e8e\u5b9a\u91cf\u56fe\u50cf\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u7684\u65b9\u6cd5\uff1b2. AUTO-DIP\u5728\u591a\u4e2a\u590d\u6742\u5ea6\u4e0d\u540c\u7684\u5f00\u6e90\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebfDIP\uff08\u539f\u59cbDIP\u53c2\u6570\uff09\u548c\u53d8\u5206\u53bb\u566a\u65b9\u6cd5\uff0c\u5c24\u5176\u5bf9\u9ad8\u566a\u58f0\u8f93\u5165\u6548\u679c\u663e\u8457\uff1b3. \u5728\u672c\u5730\u83b7\u53d6\u7684\u8367\u5149\u663e\u5fae\u955c\u56fe\u50cf\u4e0a\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86AUTO-DIP\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "AUTO-DIP\u901a\u8fc7\u57fa\u4e8e\u56fe\u50cf\u5143\u6570\u636e\u76f8\u4f3c\u6027\u7684\u53c2\u6570\u81ea\u52a8\u8fc1\u79fb\uff0c\u5b9e\u73b0\u4e86\u514d\u4f18\u5316\u7684DIP\u53bb\u566a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5904\u7406\u6548\u7387\uff0c\u5728\u8367\u5149\u663e\u5fae\u955c\u56fe\u50cf\u53bb\u566a\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u9ad8\u566a\u58f0\u56fe\u50cf\u3002"}}
{"id": "2601.12062", "pdf": "https://arxiv.org/pdf/2601.12062", "abs": "https://arxiv.org/abs/2601.12062", "authors": ["Xiaomei Yang", "Xizhan Gao", "Antai Liu", "Kang Wei", "Fa Zhu", "Guang Feng", "Xiaofeng Qu", "Sijie Niu"], "title": "Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance. To address these issues, we propose the language-driven sequence-level modal-invariant representation learning (LSMRL) method, which includes spatial-temporal feature learning (STFL) module, semantic diffusion (SD) module and cross-modal interaction (CMI) module. To enable parameter- and computation-efficient spatial-temporal modeling, the STFL module is built upon CLIP with minimal modifications. To achieve sufficient cross-modal interaction and enhance the learning of modal-invariant features, the SD module is proposed to diffuse modality-shared language prompts into visible and infrared features to establish preliminary modal consistency. The CMI module is further developed to leverage bidirectional cross-modal self-attention to eliminate residual modality gaps and refine modal-invariant representations. To explicitly enhance the learning of modal-invariant representations, two modality-level losses are introduced to improve the features' discriminative ability and their generalization to unseen categories. Extensive experiments on large-scale VVI-ReID datasets demonstrate the superiority of LSMRL over AOTA methods.", "AI": {"tldr": "\u63d0\u51faLSMRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u7684\u5e8f\u5217\u7ea7\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\uff0c\u89e3\u51b3VVI-ReID\u4e2d\u7a7a\u95f4-\u65f6\u95f4\u5efa\u6a21\u3001\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u663e\u5f0f\u6a21\u6001\u7ea7\u635f\u5931\u6307\u5bfc\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u8bed\u8a00\u63d0\u793a\u7684VVI-ReID\u65b9\u6cd5\u5728\u7a7a\u95f4-\u65f6\u95f4\u5efa\u6a21\u6548\u7387\u3001\u8de8\u6a21\u6001\u4ea4\u4e92\u5145\u5206\u6027\u548c\u663e\u5f0f\u6a21\u6001\u7ea7\u635f\u5931\u6307\u5bfc\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faLSMRL\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aSTFL\u6a21\u5757\u7528\u4e8e\u53c2\u6570\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u7a7a\u95f4-\u65f6\u95f4\u5efa\u6a21\uff1bSD\u6a21\u5757\u901a\u8fc7\u6269\u6563\u6a21\u6001\u5171\u4eab\u8bed\u8a00\u63d0\u793a\u5efa\u7acb\u521d\u6b65\u6a21\u6001\u4e00\u81f4\u6027\uff1bCMI\u6a21\u5757\u5229\u7528\u53cc\u5411\u8de8\u6a21\u6001\u81ea\u6ce8\u610f\u529b\u6d88\u9664\u5269\u4f59\u6a21\u6001\u5dee\u5f02\u3002\u540c\u65f6\u5f15\u5165\u4e24\u79cd\u6a21\u6001\u7ea7\u635f\u5931\u3002", "result": "\u5728\u5927\u89c4\u6a21VVI-ReID\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLSMRL\u65b9\u6cd5\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u65b9\u6cd5\uff08AOTA\uff09\u3002", "conclusion": "LSMRL\u65b9\u6cd5\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u7684\u5e8f\u5217\u7ea7\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86VVI-ReID\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2601.12066", "pdf": "https://arxiv.org/pdf/2601.12066", "abs": "https://arxiv.org/abs/2601.12066", "authors": ["Zijie Lou", "Xiangwei Feng", "Jiaxin Wang", "Xiaochao Qu", "Luoqi Liu", "Ting Liu"], "title": "Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation", "categories": ["cs.CV"], "comment": null, "summary": "Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene's physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u6865\u6a21\u578b\u7684\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\uff0c\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u5230\u89c6\u9891\u7684\u8f6c\u6362\uff0c\u5229\u7528\u6e90\u89c6\u9891\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a9\u7801\u8c03\u5236\u5e73\u8861\u80cc\u666f\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\u4ece\u9ad8\u65af\u566a\u58f0\u5f00\u59cb\u751f\u6210\uff0c\u4e22\u5f03\u4e86\u539f\u59cb\u89c6\u9891\u4e2d\u7684\u4e30\u5bcc\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u5148\u9a8c\uff0c\u5bfc\u81f4\u5bf9\u8c61\u64e6\u9664\u4e0d\u5b8c\u6574\u6216\u751f\u6210\u4e0e\u573a\u666f\u7269\u7406\u903b\u8f91\u51b2\u7a81\u7684\u5185\u5bb9\u3002", "method": "\u5c06\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u968f\u673a\u6865\u6a21\u578b\u8fdb\u884c\u7684\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u4efb\u52a1\uff0c\u5efa\u7acb\u4ece\u6e90\u89c6\u9891\uff08\u542b\u5bf9\u8c61\uff09\u5230\u76ee\u6807\u89c6\u9891\uff08\u5bf9\u8c61\u79fb\u9664\uff09\u7684\u76f4\u63a5\u968f\u673a\u8def\u5f84\u3002\u63d0\u51fa\u81ea\u9002\u5e94\u63a9\u7801\u8c03\u5236\u7b56\u7565\uff0c\u6839\u636e\u63a9\u7801\u7279\u5f81\u52a8\u6001\u8c03\u5236\u8f93\u5165\u5d4c\u5165\uff0c\u5e73\u8861\u80cc\u666f\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u7075\u6d3b\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u5230\u89c6\u9891\u7684\u6865\u63a5\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u81ea\u9002\u5e94\u63a9\u7801\u8c03\u5236\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u8f93\u5165\u89c6\u9891\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u5bf9\u8c61\u79fb\u9664\uff0c\u540c\u65f6\u786e\u4fdd\u586b\u5145\u533a\u57df\u4e0e\u5468\u56f4\u73af\u5883\u903b\u8f91\u4e00\u81f4\u3002"}}
{"id": "2601.12067", "pdf": "https://arxiv.org/pdf/2601.12067", "abs": "https://arxiv.org/abs/2601.12067", "authors": ["VSS Tejaswi Abburi", "Ananya Singhal", "Saurabh J. Shigwan", "Nitin Kumar"], "title": "ARMARecon: An ARMA Convolutional Filter based Graph Neural Network for Neurodegenerative Dementias Classification", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Early detection of neurodegenerative diseases such as Alzheimer's Disease (AD) and Frontotemporal Dementia (FTD) is essential for reducing the risk of progression to severe disease stages. As AD and FTD propagate along white-matter regions in a global, graph-dependent manner, graph-based neural networks are well suited to capture these patterns. Hence, we introduce ARMARecon, a unified graph learning framework that integrates Autoregressive Moving Average (ARMA) graph filtering with a reconstruction-driven objective to enhance feature representation and improve classification accuracy. ARMARecon effectively models both local and global connectivity by leveraging 20-bin Fractional Anisotropy (FA) histogram features extracted from white-matter regions, while mitigating over-smoothing. Overall, ARMARecon achieves superior performance compared to state-of-the-art methods on the multi-site dMRI datasets ADNI and NIFD.", "AI": {"tldr": "ARMARecon\uff1a\u4e00\u79cd\u7ed3\u5408ARMA\u56fe\u6ee4\u6ce2\u548c\u91cd\u6784\u76ee\u6807\u7684\u7edf\u4e00\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u989d\u989e\u53f6\u75f4\u5446\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u5728ADNI\u548cNIFD\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u548c\u989d\u989e\u53f6\u75f4\u5446\uff08FTD\uff09\u7b49\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u5bf9\u4e8e\u964d\u4f4e\u8fdb\u5c55\u4e3a\u4e25\u91cd\u75be\u75c5\u9636\u6bb5\u7684\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8eAD\u548cFTD\u6cbf\u7740\u767d\u8d28\u533a\u57df\u4ee5\u5168\u5c40\u3001\u56fe\u4f9d\u8d56\u7684\u65b9\u5f0f\u4f20\u64ad\uff0c\u57fa\u4e8e\u56fe\u7684\u795e\u7ecf\u7f51\u7edc\u975e\u5e38\u9002\u5408\u6355\u6349\u8fd9\u4e9b\u6a21\u5f0f\u3002", "method": "\u63d0\u51faARMARecon\u6846\u67b6\uff0c\u6574\u5408\u81ea\u56de\u5f52\u79fb\u52a8\u5e73\u5747\uff08ARMA\uff09\u56fe\u6ee4\u6ce2\u4e0e\u91cd\u6784\u9a71\u52a8\u76ee\u6807\uff0c\u589e\u5f3a\u7279\u5f81\u8868\u793a\u5e76\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002\u5229\u7528\u4ece\u767d\u8d28\u533a\u57df\u63d0\u53d6\u768420-bin\u5206\u6570\u5404\u5411\u5f02\u6027\uff08FA\uff09\u76f4\u65b9\u56fe\u7279\u5f81\uff0c\u6709\u6548\u5efa\u6a21\u5c40\u90e8\u548c\u5168\u5c40\u8fde\u63a5\u6027\uff0c\u540c\u65f6\u7f13\u89e3\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "result": "ARMARecon\u5728ADNI\u548cNIFD\u591a\u7ad9\u70b9dMRI\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "ARMARecon\u662f\u4e00\u4e2a\u6709\u6548\u7684\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u6574\u5408ARMA\u56fe\u6ee4\u6ce2\u548c\u91cd\u6784\u76ee\u6807\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u5728\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u65e9\u671f\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.12076", "pdf": "https://arxiv.org/pdf/2601.12076", "abs": "https://arxiv.org/abs/2601.12076", "authors": ["H. Jiang", "Y. Sun", "Z. Dong", "T. Liu", "Y. Gu"], "title": "CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u9065\u611f\u89c6\u9891\u6307\u4ee3\u5206\u5272\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6RS-RVOS Bench\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8bb0\u5fc6\u8d28\u91cf\u63a7\u5236\u7684MQC-SAM\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u5e8f\u8fd0\u52a8\u4e00\u81f4\u6027\u6a21\u5757\u548c\u89e3\u8026\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u76ee\u6807\u8868\u793a\u4fdd\u6301\u548c\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "motivation": "\u9065\u611f\u89c6\u9891\u6307\u4ee3\u5206\u5272\u9762\u4e34\u76ee\u6807\u663e\u8457\u6027\u5f31\u3001\u89c6\u89c9\u4fe1\u606f\u622a\u65ad\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e13\u7528\u57fa\u51c6\uff0c\u4e14\u73b0\u6709\u6a21\u578b\u5b58\u5728\u521d\u59cb\u8bb0\u5fc6\u6784\u5efa\u504f\u5dee\u548c\u566a\u58f0\u79ef\u7d2f\u5bfc\u81f4\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "method": "1) \u6784\u5efaRS-RVOS Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b111\u4e2a\u89c6\u9891\u5e8f\u5217\u3001\u7ea625,000\u5e27\u548c213,000\u4e2a\u65f6\u5e8f\u6307\u4ee3\u6807\u6ce8\uff0c\u91c7\u7528\u4e25\u683c\u7684\u56e0\u679c\u611f\u77e5\u6807\u6ce8\u7b56\u7565\uff1b2) \u63d0\u51faMQC-SAM\u6846\u67b6\uff0c\u5305\u542b\u65f6\u5e8f\u8fd0\u52a8\u4e00\u81f4\u6027\u6a21\u5757\u7528\u4e8e\u521d\u59cb\u8bb0\u5fc6\u6821\u51c6\uff0c\u4ee5\u53ca\u89e3\u8026\u6ce8\u610f\u529b\u8bb0\u5fc6\u96c6\u6210\u673a\u5236\u8fdb\u884c\u52a8\u6001\u8d28\u91cf\u8bc4\u4f30\u548c\u9009\u62e9\u6027\u66f4\u65b0\u3002", "result": "\u5728RS-RVOS Bench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMQC-SAM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6570\u636e\u96c6\u6784\u5efa\u548c\u65b9\u6cd5\u521b\u65b0\u63a8\u52a8\u4e86\u9065\u611f\u89c6\u9891\u6307\u4ee3\u5206\u5272\u9886\u57df\u7684\u53d1\u5c55\uff0cMQC-SAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u76ee\u6807\u8868\u793a\u4fdd\u6301\u548c\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u573a\u666f\u4e0b\u7684\u6307\u4ee3\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12079", "pdf": "https://arxiv.org/pdf/2601.12079", "abs": "https://arxiv.org/abs/2601.12079", "authors": ["Jing Zhang", "Bingjie Fan", "Jixiang Zhu", "Zhe Wang"], "title": "EmoLat: Text-driven Image Sentiment Transfer via Emotion Latent Space", "categories": ["cs.CV"], "comment": "10 pages, 5 figures", "summary": "We propose EmoLat, a novel emotion latent space that enables fine-grained, text-driven image sentiment transfer by modeling cross-modal correlations between textual semantics and visual emotion features. Within EmoLat, an emotion semantic graph is constructed to capture the relational structure among emotions, objects, and visual attributes. To enhance the discriminability and transferability of emotion representations, we employ adversarial regularization, aligning the latent emotion distributions across modalities. Building upon EmoLat, a cross-modal sentiment transfer framework is proposed to manipulate image sentiment via joint embedding of text and EmoLat features. The network is optimized using a multi-objective loss incorporating semantic consistency, emotion alignment, and adversarial regularization. To support effective modeling, we construct EmoSpace Set, a large-scale benchmark dataset comprising images with dense annotations on emotions, object semantics, and visual attributes. Extensive experiments on EmoSpace Set demonstrate that our approach significantly outperforms existing state-of-the-art methods in both quantitative metrics and qualitative transfer fidelity, establishing a new paradigm for controllable image sentiment editing guided by textual input. The EmoSpace Set and all the code are available at http://github.com/JingVIPLab/EmoLat.", "AI": {"tldr": "EmoLat\u662f\u4e00\u79cd\u65b0\u9896\u7684\u60c5\u611f\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u5efa\u6a21\u6587\u672c\u8bed\u4e49\u4e0e\u89c6\u89c9\u60c5\u611f\u7279\u5f81\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u76f8\u5173\u6027\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u60c5\u611f\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u60c5\u611f\u8fc1\u79fb\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u6587\u672c\u5f15\u5bfc\u80fd\u529b\uff0c\u96be\u4ee5\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u8bed\u4e49\u7684\u7cbe\u786e\u60c5\u611f\u7f16\u8f91\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5efa\u6a21\u8de8\u6a21\u6001\u60c5\u611f\u5173\u8054\u5e76\u652f\u6301\u6587\u672c\u9a71\u52a8\u60c5\u611f\u8fc1\u79fb\u7684\u6846\u67b6\u3002", "method": "\u6784\u5efa\u60c5\u611f\u8bed\u4e49\u56fe\u6355\u6349\u60c5\u611f\u3001\u7269\u4f53\u548c\u89c6\u89c9\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u7ed3\u6784\uff1b\u91c7\u7528\u5bf9\u6297\u6b63\u5219\u5316\u589e\u5f3a\u60c5\u611f\u8868\u793a\u7684\u53ef\u533a\u5206\u6027\u548c\u53ef\u8fc1\u79fb\u6027\uff1b\u63d0\u51fa\u8de8\u6a21\u6001\u60c5\u611f\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u548cEmoLat\u7279\u5f81\u7684\u8054\u5408\u5d4c\u5165\u6765\u64cd\u7eb5\u56fe\u50cf\u60c5\u611f\uff1b\u4f7f\u7528\u5305\u542b\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u60c5\u611f\u5bf9\u9f50\u548c\u5bf9\u6297\u6b63\u5219\u5316\u7684\u591a\u76ee\u6807\u635f\u5931\u4f18\u5316\u7f51\u7edc\u3002", "result": "\u5728EmoSpace Set\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u8fc1\u79fb\u4fdd\u771f\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u6587\u672c\u5f15\u5bfc\u7684\u53ef\u63a7\u56fe\u50cf\u60c5\u611f\u7f16\u8f91\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "conclusion": "EmoLat\u6210\u529f\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u3001\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u60c5\u611f\u8fc1\u79fb\uff0c\u901a\u8fc7\u5efa\u6a21\u8de8\u6a21\u6001\u60c5\u611f\u5173\u8054\u548c\u6784\u5efa\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4e3a\u53ef\u63a7\u56fe\u50cf\u60c5\u611f\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12080", "pdf": "https://arxiv.org/pdf/2601.12080", "abs": "https://arxiv.org/abs/2601.12080", "authors": ["Haipeng Zhou", "Zhaohu Xing", "Hongqiu Wang", "Jun Ma", "Ping Li", "Lei Zhu"], "title": "Toward Real-World High-Precision Image Matting and Segmentation", "categories": ["cs.CV"], "comment": "Accepted by AAAI2026, Poster", "summary": "High-precision scene parsing tasks, including image matting and dichotomous segmentation, aim to accurately predict masks with extremely fine details (such as hair). Most existing methods focus on salient, single foreground objects. While interactive methods allow for target adjustment, their class-agnostic design restricts generalization across different categories. Furthermore, the scarcity of high-quality annotation has led to a reliance on inharmonious synthetic data, resulting in poor generalization to real-world scenarios. To this end, we propose a Foreground Consistent Learning model, dubbed as FCLM, to address the aforementioned issues. Specifically, we first introduce a Depth-Aware Distillation strategy where we transfer the depth-related knowledge for better foreground representation. Considering the data dilemma, we term the processing of synthetic data as domain adaptation problem where we propose a domain-invariant learning strategy to focus on foreground learning. To support interactive prediction, we contribute an Object-Oriented Decoder that can receive both visual and language prompts to predict the referring target. Experimental results show that our method quantitatively and qualitatively outperforms SOTA methods.", "AI": {"tldr": "\u63d0\u51faFCLM\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u84b8\u998f\u548c\u57df\u4e0d\u53d8\u5b66\u4e60\u89e3\u51b3\u9ad8\u7cbe\u5ea6\u573a\u666f\u89e3\u6790\u4e2d\u7684\u524d\u666f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u652f\u6301\u89c6\u89c9\u548c\u8bed\u8a00\u63d0\u793a\u7684\u4ea4\u4e92\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u9ad8\u7cbe\u5ea6\u573a\u666f\u89e3\u6790\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u663e\u8457\u5355\u4e00\u524d\u666f\u5bf9\u8c61\uff0c\u4ea4\u4e92\u65b9\u6cd5\u7c7b\u522b\u4e0d\u53ef\u77e5\u9650\u5236\u4e86\u8de8\u7c7b\u522b\u6cdb\u5316\uff0c\u4e14\u9ad8\u8d28\u91cf\u6807\u6ce8\u7a00\u7f3a\u5bfc\u81f4\u4f9d\u8d56\u4e0d\u548c\u8c10\u7684\u5408\u6210\u6570\u636e\uff0c\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\u6548\u679c\u5dee\u3002", "method": "\u63d0\u51fa\u524d\u666f\u4e00\u81f4\u5b66\u4e60\u6a21\u578bFCLM\uff1a1) \u6df1\u5ea6\u611f\u77e5\u84b8\u998f\u7b56\u7565\uff0c\u8f6c\u79fb\u6df1\u5ea6\u76f8\u5173\u77e5\u8bc6\u4ee5\u6539\u5584\u524d\u666f\u8868\u793a\uff1b2) \u5c06\u5408\u6210\u6570\u636e\u5904\u7406\u89c6\u4e3a\u57df\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u51fa\u57df\u4e0d\u53d8\u5b66\u4e60\u7b56\u7565\u4e13\u6ce8\u4e8e\u524d\u666f\u5b66\u4e60\uff1b3) \u9762\u5411\u5bf9\u8c61\u89e3\u7801\u5668\uff0c\u53ef\u63a5\u6536\u89c6\u89c9\u548c\u8bed\u8a00\u63d0\u793a\u6765\u9884\u6d4b\u53c2\u8003\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FCLM\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u84b8\u998f\u548c\u57df\u4e0d\u53d8\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7cbe\u5ea6\u573a\u666f\u89e3\u6790\u4e2d\u7684\u524d\u666f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u652f\u6301\u591a\u6a21\u6001\u4ea4\u4e92\u9884\u6d4b\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.12082", "pdf": "https://arxiv.org/pdf/2601.12082", "abs": "https://arxiv.org/abs/2601.12082", "authors": ["Tiffanie Godelaine", "Maxime Zanella", "Karim El Khoury", "Sa\u00efd Mahmoudi", "Beno\u00eet Macq", "Christophe De Vleeschouwer"], "title": "Conditional Random Fields for Interactive Refinement of Histopathological Predictions", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.", "AI": {"tldr": "\u63d0\u51faHistoCRF\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u968f\u673a\u573a\u4f18\u5316\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u6790\u5bf9\u764c\u75c7\u68c0\u6d4b\u548c\u5206\u671f\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u4ef7\u503c\uff0c\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u63d0\u4f9b\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u4f46\u7ed3\u679c\u4e0d\u5b8c\u7f8e\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316", "method": "\u63d0\u51faHistoCRF\u6846\u67b6\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u6761\u4ef6\u968f\u673a\u573a\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7684\u6210\u5bf9\u52bf\u51fd\u6570\u6765\u4fc3\u8fdb\u6807\u7b7e\u591a\u6837\u6027\u5e76\u5229\u7528\u4e13\u5bb6\u6807\u6ce8\uff0c\u652f\u6301\u65e0\u6807\u6ce8\u3001\u6709\u6807\u6ce8\u548c\u8fed\u4ee3\u4eba\u673a\u4ea4\u4e92\u4e09\u79cd\u6a21\u5f0f", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u5668\u5b98\u548c\u75be\u75c5\u7684patch\u7ea7\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u65e0\u6807\u6ce8\u65f6\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534716.0%\uff0c\u4ec5\u7528100\u4e2a\u6807\u6ce8\u63d0\u534727.5%\uff0c\u4eba\u673a\u4ea4\u4e92\u6a21\u5f0f\u4e0b\u8fdb\u4e00\u6b65\u63d0\u5347\u81f332.6%", "conclusion": "HistoCRF\u80fd\u6709\u6548\u4f18\u5316\u7ec4\u7ec7\u75c5\u7406\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5177\u6709\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c"}}
{"id": "2601.12090", "pdf": "https://arxiv.org/pdf/2601.12090", "abs": "https://arxiv.org/abs/2601.12090", "authors": ["Matej Mok", "Luk\u00e1\u0161 Gajdo\u0161ech", "Michal Mes\u00e1ro\u0161", "Martin Madaras", "Viktor Kocur"], "title": "Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data", "categories": ["cs.CV"], "comment": "8 pages", "summary": "The task of 6DoF object pose estimation is one of the fundamental problems of 3D vision with many practical applications such as industrial automation. Traditional deep learning approaches for this task often require extensive training data or CAD models, limiting their application in real-world industrial settings where data is scarce and object instances vary. We propose a novel method for 6DoF pose estimation focused specifically on bins used in industrial settings. We exploit the cuboid geometry of bins by first detecting intermediate 3D line segments corresponding to their top edges. Our approach extends the 2D line segment detection network LeTR to operate on structured point cloud data. The detected 3D line segments are then processed using a simple geometric procedure to robustly determine the bin's 6DoF pose. To evaluate our method, we extend an existing dataset with a newly collected and annotated dataset, which we make publicly available. We show that incorporating synthetic training data significantly improves pose estimation accuracy on real scans. Moreover, we show that our method significantly outperforms current state-of-the-art 6DoF pose estimation methods in terms of the pose accuracy (3 cm translation error, 8.2$^\\circ$ rotation error) while not requiring instance-specific CAD models during inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5de5\u4e1a\u6599\u7bb1\u76846DoF\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u6599\u7bb1\u7684\u7acb\u65b9\u4f53\u51e0\u4f55\u7279\u6027\uff0c\u901a\u8fc7\u68c0\u6d4b3D\u7ebf\u6bb5\u5e76\u51e0\u4f55\u5904\u7406\u6765\u4f30\u8ba1\u4f4d\u59ff\uff0c\u65e0\u9700\u5b9e\u4f8b\u7279\u5b9a\u7684CAD\u6a21\u578b", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216CAD\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u3001\u5bf9\u8c61\u591a\u53d8\u7684\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5e94\u7528", "method": "\u5229\u7528\u6599\u7bb1\u7684\u7acb\u65b9\u4f53\u51e0\u4f55\u7279\u6027\uff0c\u9996\u5148\u68c0\u6d4b\u5bf9\u5e94\u9876\u90e8\u8fb9\u7f18\u76843D\u7ebf\u6bb5\uff0c\u5c062D\u7ebf\u6bb5\u68c0\u6d4b\u7f51\u7edcLeTR\u6269\u5c55\u5230\u7ed3\u6784\u5316\u70b9\u4e91\u6570\u636e\uff0c\u7136\u540e\u901a\u8fc7\u7b80\u5355\u51e0\u4f55\u5904\u7406\u7a33\u5065\u786e\u5b9a6DoF\u4f4d\u59ff", "result": "\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u76846DoF\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff083cm\u5e73\u79fb\u8bef\u5dee\uff0c8.2\u00b0\u65cb\u8f6c\u8bef\u5dee\uff09\uff0c\u4e14\u4e0d\u9700\u8981\u5b9e\u4f8b\u7279\u5b9a\u7684CAD\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u6599\u7bb1\u4f4d\u59ff\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u51e0\u4f55\u7279\u6027\u548c\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u5728\u771f\u5b9e\u626b\u63cf\u4e2d\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6"}}
{"id": "2601.12109", "pdf": "https://arxiv.org/pdf/2601.12109", "abs": "https://arxiv.org/abs/2601.12109", "authors": ["Larissa Ferreira Rodrigues Moreira", "Rodrigo Moreira", "Leonardo Gabriel Ferreira Rodrigues"], "title": "Energy-Aware Ensemble Learning for Coffee Leaf Disease Classification", "categories": ["cs.CV"], "comment": null, "summary": "Coffee yields are contingent on the timely and accurate diagnosis of diseases; however, assessing leaf diseases in the field presents significant challenges. Although Artificial Intelligence (AI) vision models achieve high accuracy, their adoption is hindered by the limitations of constrained devices and intermittent connectivity. This study aims to facilitate sustainable on-device diagnosis through knowledge distillation: high-capacity Convolutional Neural Networks (CNNs) trained in data centers transfer knowledge to compact CNNs through Ensemble Learning (EL). Furthermore, dense tiny pairs were integrated through simple and optimized ensembling to enhance accuracy while adhering to strict computational and energy constraints. On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive with prior work with significantly reduced energy consumption and carbon footprint. This indicates that lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for Internet of Things (IoT) applications.", "AI": {"tldr": "\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u96c6\u6210\u5b66\u4e60\uff0c\u5c06\u9ad8\u5bb9\u91cfCNN\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u7d27\u51d1CNN\u4e0a\uff0c\u5b9e\u73b0\u5496\u5561\u53f6\u75c5\u5bb3\u7684\u53ef\u6301\u7eed\u8bbe\u5907\u7aef\u8bca\u65ad\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\u548c\u78b3\u8db3\u8ff9\u3002", "motivation": "\u5496\u5561\u4ea7\u91cf\u4f9d\u8d56\u53ca\u65f6\u51c6\u786e\u7684\u75c5\u5bb3\u8bca\u65ad\uff0c\u4f46\u7530\u95f4\u53f6\u7247\u75c5\u5bb3\u8bc4\u4f30\u9762\u4e34\u6311\u6218\u3002\u867d\u7136AI\u89c6\u89c9\u6a21\u578b\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u53d7\u9650\u4e8e\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u548c\u95f4\u6b47\u6027\u8fde\u63a5\uff0c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u7269\u8054\u7f51\u8bbe\u5907\u7684\u8f7b\u91cf\u7ea7\u8bca\u65ad\u65b9\u6848\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff1a\u5728\u6570\u636e\u4e2d\u5fc3\u8bad\u7ec3\u7684\u9ad8\u5bb9\u91cfCNN\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u5c06\u77e5\u8bc6\u8f6c\u79fb\u5230\u7d27\u51d1CNN\u3002\u901a\u8fc7\u7b80\u5355\u4f18\u5316\u7684\u96c6\u6210\u65b9\u6cd5\u6574\u5408\u5bc6\u96c6\u7684\u5c0f\u578b\u6a21\u578b\u5bf9\uff0c\u5728\u4e25\u683c\u7684\u8ba1\u7b97\u548c\u80fd\u8017\u7ea6\u675f\u4e0b\u63d0\u5347\u51c6\u786e\u6027\u3002", "result": "\u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u5496\u5561\u53f6\u6570\u636e\u96c6\u4e0a\uff0c\u84b8\u998f\u540e\u7684\u5c0f\u578b\u96c6\u6210\u6a21\u578b\u8fbe\u5230\u4e86\u4e0e\u5148\u524d\u5de5\u4f5c\u76f8\u5f53\u7684\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u78b3\u8db3\u8ff9\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u7ecf\u8fc7\u9002\u5f53\u7684\u84b8\u998f\u548c\u96c6\u6210\u540e\uff0c\u53ef\u4ee5\u4e3a\u7269\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u5b9e\u7528\u7684\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u8bbe\u5907\u7aef\u75c5\u5bb3\u8bca\u65ad\u3002"}}
{"id": "2601.12111", "pdf": "https://arxiv.org/pdf/2601.12111", "abs": "https://arxiv.org/abs/2601.12111", "authors": ["Wyatt McCurdy", "Xin Zhang", "Yuqi Song", "Min Gao"], "title": "RCDN: Real-Centered Detection Network for Robust Face Forgery Identification", "categories": ["cs.CV"], "comment": null, "summary": "Image forgery has become a critical threat with the rapid proliferation of AI-based generation tools, which make it increasingly easy to synthesize realistic but fraudulent facial content. Existing detection methods achieve near-perfect performance when training and testing are conducted within the same domain, yet their effectiveness deteriorates substantially in crossdomain scenarios. This limitation is problematic, as new forgery techniques continuously emerge and detectors must remain reliable against unseen manipulations. To address this challenge, we propose the Real-Centered Detection Network (RCDN), a frequency spatial convolutional neural networks(CNN) framework with an Xception backbone that anchors its representation space around authentic facial images. Instead of modeling the diverse and evolving patterns of forgeries, RCDN emphasizes the consistency of real images, leveraging a dual-branch architecture and a real centered loss design to enhance robustness under distribution shifts. Extensive experiments on the DiFF dataset, focusing on three representative forgery types (FE, I2I, T2I), demonstrate that RCDN achieves both state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization. Notably, RCDN reduces the generalization gap compared to leading baselines and achieves the highest cross/in-domain stability ratio, highlighting its potential as a practical solution for defending against evolving and unseen image forgery techniques.", "AI": {"tldr": "RCDN\u662f\u4e00\u4e2a\u57fa\u4e8e\u9891\u7387\u7a7a\u95f4CNN\u7684\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4ee5\u771f\u5b9e\u56fe\u50cf\u4e3a\u4e2d\u5fc3\u7684\u8868\u5f81\u7a7a\u95f4\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u79cd\u4f2a\u9020\u7c7b\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u5de5\u5177\u7684\u666e\u53ca\uff0c\u56fe\u50cf\u4f2a\u9020\u5df2\u6210\u4e3a\u4e25\u91cd\u5a01\u80c1\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u540c\u57df\u573a\u666f\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8de8\u57df\u573a\u666f\u4e0b\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u800c\u65b0\u7684\u4f2a\u9020\u6280\u672f\u4e0d\u65ad\u6d8c\u73b0\uff0c\u9700\u8981\u68c0\u6d4b\u5668\u5bf9\u672a\u89c1\u8fc7\u7684\u4f2a\u9020\u7c7b\u578b\u4fdd\u6301\u53ef\u9760\u3002", "method": "\u63d0\u51faReal-Centered Detection Network (RCDN)\uff0c\u91c7\u7528\u9891\u7387\u7a7a\u95f4CNN\u6846\u67b6\u548cXception\u9aa8\u5e72\u7f51\u7edc\uff0c\u5c06\u8868\u5f81\u7a7a\u95f4\u951a\u5b9a\u5728\u771f\u5b9e\u56fe\u50cf\u5468\u56f4\u3002\u4f7f\u7528\u53cc\u5206\u652f\u67b6\u6784\u548c\u771f\u5b9e\u4e2d\u5fc3\u635f\u5931\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u771f\u5b9e\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u800c\u975e\u5efa\u6a21\u591a\u6837\u5316\u7684\u4f2a\u9020\u6a21\u5f0f\u3002", "result": "\u5728DiFF\u6570\u636e\u96c6\u4e0a\u5bf9\u4e09\u79cd\u4ee3\u8868\u6027\u4f2a\u9020\u7c7b\u578b(FE, I2I, T2I)\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRCDN\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u57df\u5185\u51c6\u786e\u7387\uff0c\u5e76\u663e\u8457\u589e\u5f3a\u4e86\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u76f8\u6bd4\u9886\u5148\u57fa\u7ebf\uff0cRCDN\u51cf\u5c11\u4e86\u6cdb\u5316\u5dee\u8ddd\uff0c\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u8de8\u57df/\u57df\u5185\u7a33\u5b9a\u6027\u6bd4\u7387\u3002", "conclusion": "RCDN\u901a\u8fc7\u4ee5\u771f\u5b9e\u56fe\u50cf\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u8de8\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u9632\u5fa1\u4e0d\u65ad\u6f14\u5316\u548c\u672a\u89c1\u8fc7\u7684\u4f2a\u9020\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12119", "pdf": "https://arxiv.org/pdf/2601.12119", "abs": "https://arxiv.org/abs/2601.12119", "authors": ["Xiaotong Zhou", "Zhenhui Yuan", "Yi Han", "Tianhua Xu", "Laurence T. Yang"], "title": "CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at https://github.com/Rebecca689/CARLA-Round.", "AI": {"tldr": "\u63d0\u51faCARLA-Round\u4eff\u771f\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u73af\u5c9b\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\uff0c\u901a\u8fc7\u7cfb\u7edf\u63a7\u5236\u5929\u6c14\u548c\u4ea4\u901a\u5bc6\u5ea6\u6761\u4ef6\uff0c\u91cf\u5316\u5206\u6790\u5404\u56e0\u7d20\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73af\u5c9b\u573a\u666f\u7684\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u5bf9\u4ea4\u901a\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u73af\u5f62\u51e0\u4f55\u7ed3\u6784\u3001\u8fde\u7eed\u6c47\u5165\u548c\u8ba9\u884c\u4ea4\u4e92\u3001\u7f3a\u4e4f\u4ea4\u901a\u4fe1\u53f7\u7b49\u7279\u70b9\uff0c\u9884\u6d4b\u6781\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u5b58\u5728\u89c2\u6d4b\u4e0d\u5b8c\u6574\u3001\u56e0\u7d20\u6df7\u6742\u96be\u4ee5\u5206\u79bb\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaCARLA-Round\u4eff\u771f\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bbe\u8ba15\u79cd\u5929\u6c14\u6761\u4ef6\u548c5\u4e2a\u670d\u52a1\u6c34\u5e73\uff08A-E\uff09\u7684\u4ea4\u901a\u5bc6\u5ea6\uff0c\u5f62\u621025\u4e2a\u53d7\u63a7\u573a\u666f\u3002\u6bcf\u4e2a\u573a\u666f\u5305\u542b\u771f\u5b9e\u7684\u9a7e\u9a76\u884c\u4e3a\u6df7\u5408\uff0c\u5e76\u63d0\u4f9b\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u7684\u663e\u5f0f\u6807\u6ce8\u3002\u4f7f\u7528\u6807\u51c6\u57fa\u7ebf\u6a21\u578b\uff08LSTM\u3001GCN\u3001GRU+GCN\uff09\u8fdb\u884c\u9a8c\u8bc1\u5b9e\u9a8c\u3002", "result": "\u9a8c\u8bc1\u5b9e\u9a8c\u663e\u793a\u4ea4\u901a\u5bc6\u5ea6\u5bf9\u9884\u6d4b\u96be\u5ea6\u5177\u6709\u4e3b\u5bfc\u6027\u7684\u5355\u8c03\u6548\u5e94\uff0c\u800c\u5929\u6c14\u6761\u4ef6\u5448\u73b0\u975e\u7ebf\u6027\u5f71\u54cd\u3002\u6700\u4f73\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754crounD\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.312m ADE\uff0c\u8bc1\u660e\u4e86\u6709\u6548\u7684\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "CARLA-Round\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u91cf\u5316\u4e86\u5728\u6df7\u6742\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u65e0\u6cd5\u5206\u79bb\u7684\u56e0\u7d20\u5f71\u54cd\uff0c\u4e3a\u73af\u5c9b\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u591a\u6a21\u6001\u3001\u73b0\u5b9e\u7684\u4eff\u771f\u6570\u636e\u96c6\u3002"}}
{"id": "2601.12147", "pdf": "https://arxiv.org/pdf/2601.12147", "abs": "https://arxiv.org/abs/2601.12147", "authors": ["Zezhong Fan", "Xiaohan Li", "Topojoy Biswas", "Kaushiki Nag", "Kannan Achan"], "title": "Segment and Matte Anything in a Unified Model", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI 2026", "summary": "Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.", "AI": {"tldr": "SAMA\u662fSAM\u7684\u8f7b\u91cf\u7ea7\u6269\u5c55\uff0c\u5728\u4fdd\u6301\u5c11\u91cf\u989d\u5916\u53c2\u6570\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u5f0f\u56fe\u50cf\u5206\u5272\u548c\u62a0\u56fe\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u5b9a\u4f4d\u7f16\u7801\u5668\u548c\u5c40\u90e8\u9002\u914d\u5668\u63d0\u5347\u8fb9\u754c\u7ec6\u8282\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1SAM\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u7075\u6d3b\u63d0\u793a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u63a9\u7801\u9884\u6d4b\u7cbe\u5ea6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u663e\u4e0d\u8db3\u3002\u73b0\u6709\u7ec6\u5316\u6a21\u5757\u96be\u4ee5\u5728\u7edf\u4e00\u6846\u67b6\u5185\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5bf9\u8c61\u63cf\u7ed8\uff0c\u4e14SAM\u5c1a\u672a\u63a2\u7d22\u4ea4\u4e92\u5f0f\u56fe\u50cf\u62a0\u56fe\u4efb\u52a1\u3002\u8003\u8651\u5230\u5206\u5272\u4e0e\u62a0\u56fe\u4e4b\u95f4\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u540c\u65f6\u5904\u7406\u8fd9\u4e24\u9879\u4efb\u52a1\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u63d0\u51faSAMA\u6a21\u578b\uff0c\u4f5c\u4e3aSAM\u7684\u8f7b\u91cf\u7ea7\u6269\u5c55\uff1a1) \u591a\u89c6\u56fe\u5b9a\u4f4d\u7f16\u7801\u5668(MVLE)\u4ece\u5c40\u90e8\u89c6\u56fe\u6355\u83b7\u8be6\u7ec6\u7279\u5f81\uff1b2) \u5c40\u90e8\u9002\u914d\u5668(Local-Adapter)\u901a\u8fc7\u6062\u590d\u7ec6\u5fae\u8fb9\u754c\u7ec6\u8282\u6765\u7ec6\u5316\u63a9\u7801\u8f93\u51fa\uff1b3) \u4e3a\u5206\u5272\u548c\u62a0\u56fe\u4efb\u52a1\u5206\u522b\u8bbe\u8ba1\u9884\u6d4b\u5934\uff0c\u540c\u65f6\u751f\u6210\u4e24\u79cd\u63a9\u7801\u3002\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "SAMA\u5728\u591a\u4e2a\u5206\u5272\u548c\u62a0\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5e7f\u6cdb\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002\u6a21\u578b\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u53c2\u6570\u5c31\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u548c\u62a0\u56fe\u7ed3\u679c\u3002", "conclusion": "SAMA\u6210\u529f\u5730\u5c06\u5206\u5272\u548c\u62a0\u56fe\u4efb\u52a1\u7edf\u4e00\u5230\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u521b\u65b0\u7684MVLE\u548cLocal-Adapter\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u754c\u7ec6\u8282\u7684\u6062\u590d\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5206\u5272\u548c\u62a0\u56fe\u89e3\u51b3\u65b9\u6848\u3002"}}
