[[toc]]

## cs.CV

### [1] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 论文提出了V2-VLNCE（带有多视角变化的VLNCE）场景和VIL（视角不变学习）方法，通过对比学习和师生框架增强导航策略对视角变化的鲁棒性，显著提升了性能。

- Motivation: 现有导航策略对视角变化敏感，限制了其在真实环境中的适用性。
- Method: 采用对比学习框架提取稀疏且视角不变的特征，并设计师生框架优化Waypoint Predictor模块，通过端到端训练联合优化。
- Result: 在V2-VLNCE上性能提升8-15%，在标准VLNCE设置下表现也有所提升，RxR-CE数据集上达到最优。
- Conclusion: VIL是一种即插即用的后训练方法，能显著提升导航策略的鲁棒性，且不影响标准视角性能。


### [2] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 提出一种基于面部生物特征异常模式的深度学习技术，用于检测深度伪造视频。

- Motivation: 深度伪造技术容易被用于欺诈、诈骗和政治虚假信息，亟需有效的检测方法。
- Method: 利用面部生物特征中的异常模式，开发了一种新的法医机器学习技术。
- Result: 在大规模深度伪造数据集上评估了技术的有效性，并测试了其对视频篡改的鲁棒性和泛化能力。
- Conclusion: 该技术能有效检测深度伪造视频，具有较高的可靠性和泛化性。


### [3] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM是一种无需数据和任务无关的方法，用于减少视觉语言模型（如CLIP）中的隐含偏差。它通过两阶段方法生成场景描述并学习投影以减少偏差。

- Motivation: 视觉语言模型（VLMs）常因训练数据中的偏差而产生预测偏差，PRISM旨在无需额外数据或预定义偏差类别的情况下解决这一问题。
- Method: PRISM分两阶段：1）用LLM生成包含虚假相关性的场景描述；2）使用对比式去偏差损失学习投影以减少偏差。
- Result: PRISM在Waterbirds和CelebA数据集上优于现有去偏差方法。
- Conclusion: PRISM提供了一种有效且通用的去偏差解决方案，代码已公开。


### [4] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: 提出了一种结合时空和运动学信息的人体网格恢复方法HMR-ViT，通过构建时空-运动学特征图像并使用Vision Transformer进行编码，显著提升了性能。

- Motivation: 现有的人体网格恢复方法仅利用时空或运动学信息，未能结合两者，限制了性能提升。
- Method: 构建时空-运动学特征图像，使用通道重排矩阵优化特征排列，通过Vision Transformer和回归网络推断SMPL参数。
- Result: 在3DPW和Human3.6M数据集上表现优异。
- Conclusion: HMR-ViT通过结合时空和运动学信息，显著提升了人体网格恢复的准确性。


### [5] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 提出了一种结合NeRF和MPM的新框架，通过视觉观测推断颗粒材料特性，摩擦角估计误差在2度以内。

- Motivation: 解决在无法直接测量的情况下，通过视觉观测表征颗粒材料特性的问题。
- Method: 生成合成实验数据，利用NeRF重建3D几何，MPM模拟初始化材料点位置，通过贝叶斯优化最小化图像损失估计摩擦角。
- Result: 摩擦角估计误差在2度以内，验证了方法的有效性。
- Conclusion: 该方法为颗粒材料特性表征提供了一种可行的视觉解决方案。


### [6] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA是一个视觉分析框架，旨在提升多模态基础模型生成标签的质量，通过结合多阶段数据验证策略和人类专家知识，解决现有方法在数据质量验证上的不足。

- Motivation: 现有方法多关注数据数量而非质量，且缺乏全面的验证手段，导致多模态模型生成的标签质量难以保证。
- Method: 提出VISTA框架，结合多阶段数据验证和人类专家知识，用于识别和修正标签中的隐藏问题。
- Result: 在开放词汇图像分割领域的两个基准数据集上，VISTA在定量和定性方面均表现出有效性。
- Conclusion: VISTA通过提升数据质量，显著增强了多模态模型在下游任务中的性能。


### [7] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是一个用于构建模块化脑部病变图像分析流程的Python工具包，旨在简化开发流程并提供高效的工作流。

- Motivation: 为临床和科研实践提供一种简化复杂工作流的方法，减少认知负担，提升脑部病变图像分析的效率。
- Method: 基于Pythonic原则设计，包含可适应的预处理模块，支持多模态图像处理，并利用BraTS挑战赛算法进行模态合成、病变修复和肿瘤分割。
- Result: 提供了一套完整的工具包，支持脑部病变（如胶质瘤、转移瘤和多发性硬化症）的图像分析，并可扩展到其他生物医学图像分析应用。
- Conclusion: BrainLesion Suite是一个功能强大且灵活的工具，适用于脑部病变图像分析，并可通过GitHub获取其组件和教程。


### [8] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 论文提出两种对比损失函数，用于解决类别不平衡数据下扩散模型生成的尾部类别图像多样性不足的问题。

- Motivation: 类别不平衡数据导致尾部类别图像生成时模式崩溃和多样性降低，需在不影响头部类别性能的情况下提升尾部类别的多样性。
- Method: 引入无监督InfoNCE损失和MSE损失，通过对比学习和条件-无条件对齐增强尾部类别多样性。
- Result: 方法在多个数据集（如CIFAR10/100-LT等）上优于标准DDPM和其他替代方法。
- Conclusion: 对比学习框架简单有效，首次将条件-无条件对齐应用于扩散模型，显著提升了尾部类别的生成多样性。


### [9] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 论文探讨了大型语言模型（LLM）和多模态扩展（MLLM）在视频理解中的进展，提出无限视频理解（Infinite Video Understanding）作为未来研究方向，以解决长视频处理中的计算、内存和时序一致性等挑战。

- Motivation: 当前视频理解模型在处理长时间视频时面临计算、内存和时序一致性等限制，需要探索更高效的方法以实现无限视频理解。
- Method: 提出将无限视频理解作为研究目标，推动流式架构、持久内存机制、分层自适应表示、事件中心推理等创新。
- Result: 论文未提供具体实验结果，但提出了无限视频理解的核心挑战和关键研究方向。
- Conclusion: 无限视频理解是多媒体和AI研究的重要前沿，需在架构、表示和评估等方面进行创新。


### [10] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: BlindSight通过利用视觉语言模型中的注意力稀疏性，提出了一种无需训练的推理优化方法，显著减少计算量且保持准确性。

- Motivation: 视觉数据的加入导致提示长度增加，注意力计算的二次复杂度进一步延长预填充时间，需优化。
- Method: 分析注意力模式，提出基于输入模板的稀疏注意力掩码BlindSight，分类注意力头并应用稀疏掩码。
- Result: 在Qwen2-VL等模型上，FLOPs平均减少32%-41%，准确性变化在-2%到+2%之间。
- Conclusion: BlindSight有效优化视觉语言模型推理，减少计算负担且几乎不影响性能。


### [11] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: 综述了定量遥感反演方法从物理模型到机器学习再到基础模型的演变，比较了各范式的假设、应用场景和限制，并展望了下一代基础模型的发展方向。

- Motivation: 定量遥感反演在生态系统监测、碳核算和土地管理中具有重要应用，随着遥感系统和人工智能的发展，传统物理模型逐渐被数据驱动和基础模型取代。
- Method: 系统回顾了反演技术的演变，从物理模型（如PROSPECT、SCOPE、DART）到机器学习（如深度学习、多模态融合），再到基础模型（如SatMAE、GFM、mmEarth）。
- Result: 比较了各范式的建模假设、应用场景和限制，并重点讨论了基础模型在自监督预训练、多模态集成和跨任务适应方面的进展。
- Conclusion: 展望了下一代基础模型的发展，强调统一建模能力、跨域泛化和物理可解释性。


### [12] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: 论文提出了一种无需微调的零样本方法，通过扰动生成视频模型提取光流，优于现有方法。

- Motivation: 探索是否可以通过冻结的自监督视频模型（仅用于未来帧预测）直接提取光流，避免因标签稀缺和合成数据差距导致的微调问题。
- Method: 基于CWM范式，提出KL-tracing方法，通过局部扰动和KL散度计算光流，利用LRAS架构的优势。
- Result: 在TAP-Vid DAVIS和Kubric数据集上分别取得16.6%和4.7%的相对改进。
- Conclusion: 表明基于生成视频模型的对抗提示是一种高效且可扩展的光流提取替代方案。


### [13] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: 本文提出了一种名为MI CAM的新型后验视觉解释方法，通过激活映射和互信息加权生成显著性可视化，优于现有方法。

- Motivation: 随着机器视觉在医疗和自动化电厂等关键领域的应用，理解卷积神经网络的内部机制和推理原因变得重要。
- Method: MI CAM通过互信息加权特征图，并与激活图线性组合生成显著性可视化，同时通过反事实分析验证因果解释。
- Result: MI CAM在定性和定量指标上优于部分现有方法，表现与最先进方法相当。
- Conclusion: MI CAM提供了无偏见的模型推理视觉解释，其实现已公开。


### [14] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: RadEyeVideo利用放射科医生的眼动视频序列提升大型视觉语言模型（LVLM）在胸部X光分析中的性能，显著提高了报告生成和疾病诊断任务的表现。

- Motivation: 现有方法通常忽略眼动的顺序信息，而RadEyeVideo旨在通过整合时空动态的眼动数据来增强模型性能。
- Method: 提出RadEyeVideo方法，将眼动数据作为视频序列输入到支持视频的LVLM中，评估其在胸部X光报告生成和疾病诊断中的效果。
- Result: 使用眼动视频提示后，模型性能在报告生成任务中提升24.6%，在两项任务中平均提升15.2%，甚至超越专业医学LVLM。
- Conclusion: RadEyeVideo展示了专家知识（眼动信息）与LVLM的有效结合，能显著提升通用模型在临床任务中的能力，为医疗图像分析提供了一种可扩展的人本方法。


### [15] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: PointSD利用Stable Diffusion模型增强3D点云的自监督学习，通过点云引导图像去噪并提取特征，提升下游任务性能。

- Motivation: 现有3D扩散模型受限于小规模数据集，而文本到图像扩散模型（如Stable Diffusion）在大规模数据上训练，可能弥补这一不足。
- Method: 提出PointSD框架，将Stable Diffusion的文本编码器替换为3D编码器，训练点云到图像的扩散模型，并提取特征对齐3D主干网络。
- Result: 实验表明，Stable Diffusion能有效提升点云自监督学习性能。
- Conclusion: PointSD通过利用大规模预训练模型，显著改善了3D点云表示学习。


### [16] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 论文提出了一种结合自回归和扩散模型的混合方法，用于手语生成（SLP），通过多尺度姿态表示和置信感知因果注意力机制提升生成质量和实时性。

- Motivation: 传统自回归方法在推理阶段存在误差累积问题，而扩散模型因迭代特性难以实时应用。本文旨在结合两者优势，解决SLP中的实时生成问题。
- Method: 采用混合自回归和扩散模型的方法，设计多尺度姿态表示模块和置信感知因果注意力机制，以优化生成过程。
- Result: 在PHOENIX14T和How2Sign数据集上的实验表明，该方法在生成质量和实时性上均表现优异。
- Conclusion: 混合模型结合了自回归和扩散模型的优势，显著提升了手语生成的实时性和准确性。


### [17] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 论文提出了首个针对人-物交互（HOI）检测的鲁棒性基准测试RoHOI，并提出了语义感知掩码渐进学习（SAMPL）策略，以提升模型在复杂环境下的性能。

- Motivation: 现有HOI检测模型在真实世界中因环境变化、遮挡和噪声等问题性能下降，缺乏鲁棒性评估标准。
- Method: 引入RoHOI基准测试，包含20种基于HICO-DET和V-COCO数据集的干扰类型，并提出SAMPL策略，通过动态调整优化增强鲁棒性特征学习。
- Result: 实验表明，SAMPL策略优于现有方法，显著提升了模型在干扰条件下的性能。
- Conclusion: RoHOI为HOI检测的鲁棒性评估提供了新标准，SAMPL策略为提升模型鲁棒性提供了有效解决方案。


### [18] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为MG-CLIP的方法，通过分析视觉-语言预训练模型中的模态间隙变化，改进CLIP在类增量学习中的性能。

- Motivation: 利用CLIP模型在持续学习中的潜力，但现有方法忽视了其模态间隙对泛化和适应性的关键影响。
- Method: 提出MG-CLIP方法，通过模态间隙保持和补偿来减少遗忘并增强新数据学习能力。
- Result: 在多个基准测试中表现优于现有方法，且无需额外回放数据。
- Conclusion: 模态间隙为持续学习提供了新视角，MG-CLIP方法简单有效。


### [19] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: 论文介绍了SnapMoGen数据集和MoMask++模型，用于提升文本到动作生成的精细控制和泛化能力。

- Motivation: 当前文本到动作生成方法受限于短文本提示和数据集约束，缺乏精细控制和泛化能力。
- Method: 提出SnapMoGen数据集（20K动作片段，44小时，122K详细文本描述）和改进的MoMask++模型，利用多尺度标记序列和生成掩码变换器。
- Result: MoMask++在HumanML3D和SnapMoGen基准测试中达到最优性能，并能处理用户输入的随意提示。
- Conclusion: SnapMoGen和MoMask++显著提升了文本到动作生成的精细控制和泛化能力。


### [20] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM提出了一种基于大型语言模型（LLM）的姿态估计框架，通过非线性MLP视觉语言连接器取代线性投影器，显著提升了定位精度和零样本泛化能力。

- Motivation: 传统姿态估计方法依赖关键点先验，泛化能力有限；现有语言引导方法（如LocLLM）的线性投影器无法捕捉复杂的空间-文本交互。
- Method: PoseLLM采用两层的非线性MLP（带GELU激活）作为视觉语言连接器，实现跨模态特征分层融合。
- Result: 在COCO验证集上达到77.8 AP，优于LocLLM（+0.4 AP），并在Human-Art和MPII上保持强零样本泛化能力。
- Conclusion: 非线性连接器在不牺牲泛化能力的情况下显著提升定位精度，推动了语言引导姿态估计的先进水平。


### [21] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: 提出了一种名为$I^{2}$-World的高效4D占用预测框架，通过解耦场景标记化并采用编码器-解码器架构，显著提升了性能。

- Motivation: 解决复杂3D场景标记化的挑战，以支持自动驾驶系统中的极端情况预测和场景生成。
- Method: 采用双标记器设计（场景内和场景间），结合多尺度残差量化和残差聚合，以及编码器-解码器架构。
- Result: 在4D占用预测中，性能优于现有方法25.1%（mIoU）和36.9%（IoU），计算效率高（2.9GB内存，37.0 FPS）。
- Conclusion: $I^{2}$-World在性能和效率上均表现出色，为3D场景预测提供了高效解决方案。


### [22] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: 论文提出了一种名为Stable Score Distillation（SSD）的新方法，用于改进基于扩散模型的文本引导图像和3D编辑，解决了现有方法在稳定性、空间控制和编辑强度上的不足。

- Motivation: 当前方法如Delta Denoising Score在稳定性、空间控制和编辑强度上存在局限性，主要依赖复杂的辅助结构，导致优化信号冲突和局部编辑不精确。
- Method: SSD通过锚定单一分类器到源提示，利用Classifier-Free Guidance（CFG）方程实现跨提示对齐，并引入空文本分支稳定优化过程，同时加入提示增强分支提升编辑强度。
- Result: SSD在2D和3D编辑任务（如NeRF和文本驱动风格编辑）中取得了最先进的结果，收敛更快且复杂度更低。
- Conclusion: SSD为文本引导编辑提供了一种高效且鲁棒的解决方案，能够保持内容结构并实现平滑、提示特定的修改。


### [23] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于视觉Transformer的视觉主干网络，融合RGB和深度模态以增强泛化能力，结合对比学习和课程学习优化训练。

- Motivation: 深度信息对场景外观变化具有鲁棒性且携带3D空间细节，但如何有效融合RGB和深度模态以提升泛化能力仍需探索。
- Method: 分别通过CNN处理不同模态，将卷积特征输入可扩展视觉Transformer；设计对比学习方案和课程学习计划。
- Result: 通过融合RGB和深度模态，结合对比学习和课程学习，提升了模型的泛化能力和训练效率。
- Conclusion: 该方法有效融合多模态信息，并通过优化学习策略提升了模型在sim2real任务中的表现。


### [24] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 论文研究了Few-Shot Class-Incremental Learning (FSCIL)中的提示池方法，发现性能下降问题并提出新方法LGSP-Prompt。

- Motivation: FSCIL面临数据稀缺和增量学习的双重挑战，现有提示池方法在FSCIL中的有效性未得到验证。
- Method: 提出LGSP-Prompt，将提示学习从token维度转移到空间维度，结合局部和全局特征生成动态空间提示。
- Result: 实验表明LGSP-Prompt在多个FSCIL基准测试中达到最优性能。
- Conclusion: LGSP-Prompt通过空间提示解决了token维度饱和问题，显著提升了FSCIL的性能。


### [25] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: 论文揭示了Rotary Position Encoding（RoPE）在大型视觉语言模型（LVLMs）中的长期衰减对多模态对齐的负面影响，并提出基于曼哈顿距离的MCA-LLaVA方法以缓解图像对齐偏差。

- Motivation: 解决LVLMs中因多模态特征不对齐导致的幻觉问题，特别是RoPE长期衰减引起的图像对齐偏差。
- Method: 提出MCA-LLaVA，基于曼哈顿距离扩展RoPE的长期衰减为二维多方向空间衰减，整合一维序列顺序和二维空间位置。
- Result: MCA-LLaVA在多种幻觉和通用基准测试中表现出有效性和通用性。
- Conclusion: MCA-LLaVA通过缓解图像对齐偏差，显著提升了多模态对齐效果，减少了幻觉现象。


### [26] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种名为THYME的动态场景图生成方法，结合分层特征聚合和循环时间细化，解决了现有方法在空间细节和时间依赖性上的不足，并在新数据集AeroEye-v1.0上验证了其优越性。

- Motivation: 动态场景理解在自动驾驶、监控等领域需求迫切，但现有方法在空间细节和时间依赖性上表现不佳。
- Method: 提出THYME方法，通过分层特征聚合和循环时间细化，同时建模多尺度空间上下文和时间一致性。
- Result: 在ASPIRe和AeroEye-v1.0数据集上，THYME优于现有方法，提升了场景理解的准确性。
- Conclusion: THYME方法有效解决了动态场景图生成中的关键问题，为复杂场景理解提供了新工具。


### [27] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: 通过视频分析表面波的传播特性，推断材料厚度和刚度的方法。

- Motivation: 表面波的传播特性可以反映材料内部的物理性质，为健康监测和人机交互提供新方法。
- Method: 从视频中提取波的色散关系，通过物理优化问题求解最佳厚度和刚度参数。
- Result: 在模拟和真实数据中验证，结果与真实测量高度一致。
- Conclusion: 该方法为家庭健康监测和人机交互领域提供了概念验证。


### [28] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了一种名为Expert-CFG的专家参与框架，无需额外训练即可将医学视觉语言模型与临床专业知识对齐，显著提升了性能。

- Motivation: 当前医学视觉语言模型存在概率不确定性，可能导致错误或未经验证的响应，这在医学应用中具有严重的影响。现有方法依赖训练且成本高昂，且与临床专业知识对齐不足。
- Method: 提出Expert-CFG框架，通过不确定性估计识别不可靠输出，检索相关参考资料辅助专家标记关键术语，并应用无分类器引导调整模型输出。
- Result: 在三个医学视觉问答基准测试中，Expert-CFG以4.2B参数和有限专家标注优于13B参数的现有模型。
- Conclusion: Expert-CFG展示了在资源有限环境中部署临床应用的可行性。


### [29] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于立体视觉的3D异常物体检测算法（S3AD），通过解耦2D和3D训练策略提升模型对任意形状目标的泛化能力，并设计了基于前景置信度预测的异常评分算法。同时，合成数据集KITTI-AR用于验证算法性能。

- Motivation: 解决3D检测模型在开放道路中对罕见异常物体的误检或漏检问题，提升模型的泛化能力和异常检测能力。
- Method: 提出S3AD算法，解耦2D和3D训练策略，设计异常评分算法；合成KITTI-AR数据集，包含扩展类别以验证算法。
- Result: 实验验证了算法和数据集的有效性，提升了3D异常检测的泛化性能。
- Conclusion: S3AD算法和KITTI-AR数据集为3D异常检测提供了有效解决方案，适用于开放道路场景。


### [30] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: 提出了一种新颖的球形采样方法，用于全景图像，可直接利用现有的二维预训练模型，有效减少失真并提升性能。

- Motivation: 由于缺乏大规模全景图像数据集，现有任务依赖二维预训练模型，但这些模型无法处理全景图像的失真和不连续性，影响性能。
- Method: 采用基于预训练模型权重的球形离散采样方法，减少失真并获得良好的初始训练值；将该方法应用于全景图像分割，利用球形模型特征作为通道注意力掩码。
- Result: 在常用室内数据集Stanford2D3D上取得了良好效果。
- Conclusion: 提出的球形采样方法有效解决了全景图像处理中的失真问题，提升了模型性能。


### [31] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: 该论文提出了一种在线长期点跟踪方法Track-On，基于Transformer架构，无需未来帧信息，在七个公开基准测试中达到新SOTA。

- Motivation: 现实场景中需要在线预测（仅使用当前和过去帧），而现有方法多为离线处理，无法满足需求。
- Method: 评估视觉基础模型的适用性，提出Track-On模型，将每个跟踪点作为查询，逐帧处理视频。
- Result: Track-On在七个公开基准测试中表现最优，证明了无需未来信息的长期跟踪可行性。
- Conclusion: 视觉基础模型可作为初始化工具，但需专用设计（如Track-On）以实现在线长期跟踪。


### [32] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: StaRFM框架通过Fisher信息惩罚（FIP）和置信度对齐惩罚（CMP）解决了基础模型在视觉语言分类和医学分割任务中的分布偏移和置信度不对齐问题，显著提升了性能。

- Motivation: 基础模型（如CLIP和SAM）在低样本迁移学习中表现出色，但面临训练与测试数据分布偏移和置信度不对齐的挑战，导致过自信的错误预测。
- Method: StaRFM引入FIP减少嵌入中的协变量偏移，并通过CMP校准分割任务中的不确定性。理论分析表明FIP通过Fisher-Rao范数控制泛化，CMP通过Brier分数优化最小化校准误差。
- Result: 在19个视觉数据集上，StaRFM提升了3.5%的准确率和28%的ECE；在医学分割任务中达到84.7%的DSC和4.8mm HD95；跨域性能差距降低40%。
- Conclusion: StaRFM是一个即插即用的统一框架，无需大幅修改模型结构即可与基础模型无缝集成，显著提升了性能。


### [33] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成先验的方法，从第一人称视角重建可动画化虚拟形象，利用Stable Diffusion减少训练负担并提高泛化能力。

- Motivation: 理想数字远程呈现需要准确复制人体、服装和动作，但第一人称视角存在遮挡和比例失真问题。现有方法多依赖多视角数据训练，本文旨在通过生成模型解决这一问题。
- Method: 基于Stable Diffusion和ControlNet，提出一种从遮挡的俯视角图像生成真实正面视图的流程，并将其输入图像到动作模型生成虚拟形象动作。
- Result: 该方法首次利用生成模型从第一人称视角重建可动画化虚拟形象，减少了训练负担并提高了泛化能力。
- Conclusion: 通过单张俯视角图像生成真实正面视图和动作，为更易用和通用的远程呈现系统铺平了道路。


### [34] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 提出了一种新的绘画过程评估框架，包括数据集PPAD和模型PPJudge，优于现有方法。

- Motivation: 现有方法仅关注静态图像，忽略了绘画过程的动态性和多阶段性。
- Method: 引入PPAD数据集和基于Transformer的PPJudge模型，采用时间感知位置编码和混合专家架构。
- Result: 实验表明，该方法在准确性、鲁棒性和与人类判断的一致性上优于基线。
- Conclusion: 为计算创造力和艺术教育提供了新见解。


### [35] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: AGCD-Net提出了一种基于注意力引导的上下文去偏模型，通过混合ConvNeXt编码器和因果干预模块，有效解决了情感识别中的上下文偏差问题。

- Motivation: 传统情感识别方法存在上下文偏差问题（如背景与情感标签的虚假关联），影响了真实场景中的情感计算效果。
- Method: AGCD-Net结合了混合ConvNeXt编码器（集成空间变换网络和SE层）和注意力引导的因果干预模块（AG-CIM），通过扰动上下文特征和注意力驱动校正来消除偏差。
- Result: 在CAER-S数据集上，AGCD-Net实现了最先进的性能，验证了因果去偏对复杂场景中情感识别的重要性。
- Conclusion: AGCD-Net通过注意力引导和因果干预，显著提升了情感识别的鲁棒性，为真实场景中的情感计算提供了有效解决方案。


### [36] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: AAHR框架通过动态聚类原型对比学习和全局/局部特征提取机制，解决了图像-文本匹配中的语义模糊和高阶关联问题，显著提升了匹配性能。

- Motivation: 现有方法在处理高阶关联和语义模糊（如软正负样本问题）时表现不佳，且未能充分利用训练批次中的邻域关系。
- Method: AAHR采用动态聚类原型对比学习、全局/局部特征提取、自适应聚合网络、GNN增强语义交互，以及动量对比学习扩展负样本集。
- Result: AAHR在Flickr30K、MSCOCO和ECCV Caption数据集上优于现有方法，显著提升了匹配准确性和效率。
- Conclusion: AAHR通过综合策略有效解决了语义模糊和高阶关联问题，为图像-文本匹配提供了更优的解决方案。


### [37] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于视觉标记化的无注释手语翻译方法，通过分段减少输入序列长度，提高计算效率，并在PHOENIX14T基准测试中表现优异。

- Motivation: 当前无注释手语翻译方法虽性能提升，但模型复杂且计算需求高，难以扩展到大尺度数据集。
- Method: 采用分段感知的视觉标记化框架，将连续视频转换为离散标记，并引入标记间对比对齐目标和双重监督机制。
- Result: 在PHOENIX14T基准测试中超越现有方法，序列长度减少50%，内存使用降低2.67倍。
- Conclusion: 该方法在减少计算需求的同时提升了性能，验证了视觉标记化和对齐策略的潜力。


### [38] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 提出跨知识蒸馏（CKD）方法，通过语义相似性和滑动替换解决跨模态问题，间接分阶段知识蒸馏解决跨架构问题，提升SNN在DVS数据上的性能。

- Motivation: SNN在计算机视觉领域潜力巨大，但受限于标注数据和架构不成熟，性能不及ANN。希望通过知识蒸馏提升SNN在DVS数据上的表现。
- Method: 提出CKD方法，结合语义相似性和滑动替换解决跨模态问题，间接分阶段知识蒸馏解决跨架构问题。
- Result: 在主流神经形态数据集（如N-Caltech101和CEP-DVS）上验证，性能优于当前最优方法。
- Conclusion: CKD方法有效解决了跨模态和跨架构挑战，显著提升了SNN的性能。


### [39] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: Prompt4Trust是一个针对多模态大语言模型（MLLMs）的强化学习框架，旨在通过上下文感知的辅助提示改善模型置信度校准，提升医疗领域中的安全性和可信度。

- Motivation: MLLMs在医疗等安全关键领域的应用受到两个主要限制：对提示设计的敏感性和高置信度下生成错误答案的倾向。
- Method: 提出Prompt4Trust框架，通过训练轻量级LLM生成上下文感知的辅助提示，优化下游任务MLLM的置信度校准和准确性。
- Result: 在PMC-VQA基准测试中取得最佳性能，并展示了零样本泛化能力。
- Conclusion: Prompt4Trust展示了自动化提示工程在提升MLLMs可信度方面的潜力，尤其在安全关键领域。


### [40] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 提出了一种基于深度生成模型的盲运动去模糊（BMD）新框架，通过预训练的GAN生成模糊核先验分布和高质量初始化，解决了传统方法对初始模糊核高度敏感的问题。

- Motivation: 传统深度先验方法在盲运动去模糊中因优化过程的高度非凸性对初始模糊核极为敏感，限制了性能。
- Method: 预训练基于GAN的模糊核生成器和初始化器，约束解在紧凑的潜在核流形内，并与现有BMD方法即插即用结合。
- Result: 在挑战性基准数据集上实现了最先进的性能，并扩展到盲非均匀运动去模糊。
- Conclusion: 提出的框架显著缓解了模糊核初始化的敏感性，提升了BMD方法的整体性能。


### [41] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 提出了一种语义感知的定位框架，通过联合估计深度和语义光线，结合结构-语义概率体积，显著提升了楼层平面图定位的性能。

- Motivation: 现有楼层平面图定位技术主要依赖深度结构线索，忽略了平面图中的丰富语义信息。
- Method: 采用从粗到细的方式构建概率体积，先采样少量光线生成低分辨率概率体积，再在高概率区域密集采样以细化预测。
- Result: 在两个标准基准测试中，该方法显著优于现有技术，召回率显著提升，并能轻松整合额外元数据（如房间标签）以进一步提高精度和效率。
- Conclusion: 该框架通过结合语义信息，显著提升了楼层平面图定位的准确性和效率。


### [42] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: Geo-RepNet利用深度信息增强手术阶段识别，通过几何感知框架提升性能。

- Motivation: 手术阶段识别在智能辅助系统中至关重要，但RGB图像的视觉相似性和缺乏结构信息带来挑战。深度信息能提供几何线索，弥补RGB的不足。
- Method: 提出Geo-RepNet框架，结合RGB和深度信息，包含DGPG模块提取几何先验，GEMA模块通过几何感知注意力增强多尺度聚合。
- Result: 在真实ESD数据集上，Geo-RepNet实现了最先进的性能，并在复杂手术环境中保持高效计算。
- Conclusion: 深度信息显著提升了手术阶段识别的性能，Geo-RepNet为复杂手术场景提供了高效解决方案。


### [43] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: ViT-ProtoNet结合ViT-Small和原型网络，在少样本图像分类中表现优异，优于CNN原型网络和部分Transformer方法。

- Motivation: Vision Transformers（ViTs）在少样本图像分类中的潜力未充分挖掘，因此提出ViT-ProtoNet以提升性能。
- Method: 将ViT-Small作为主干网络，结合原型网络框架，通过平均支持样本的token嵌入构建鲁棒原型。
- Result: 在Mini-ImageNet等四个基准测试中，ViT-ProtoNet在5-shot设置下表现最佳，准确率提升3.2%，且特征可分性更强。
- Conclusion: ViT-ProtoNet是一种强大且灵活的少样本分类方法，为基于Transformer的元学习设定了新基准。


### [44] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: 提出了一种名为DAA*的新方法，通过引入路径角度自由（PAF）改进路径平滑性，显著提升了路径相似性和最优性。

- Motivation: 路径平滑性在专家演示的路径模仿学习中常被忽视，本文旨在通过自适应路径平滑性提升路径相似性。
- Method: 提出DAA*方法，结合PAF探索移动角度对路径节点扩展的影响，通过路径缩短和平滑的联合优化改进路径最优性。
- Result: 在7个数据集上评估，DAA*在路径相似性和长度上显著优于现有方法，如SPR提升9.0%。
- Conclusion: DAA*在路径最优性和搜索效率之间存在微小权衡，但整体表现优于现有技术。


### [45] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: ALPHAVAE 是一种新型的 RGBA VAE，通过扩展预训练的 RGB VAE 并引入专用 alpha 通道，显著提升了透明图像的重建和生成质量。

- Motivation: 当前潜在扩散模型在高保真 RGB 图像合成方面取得了显著成果，但透明或分层内容（RGBA 图像）的生成仍缺乏大规模基准和研究。
- Method: 提出了 ALPHA 基准，将标准 RGB 指标扩展到四通道图像，并开发了 ALPHAVAE，通过复合目标训练（包括 alpha 混合像素重建、感知一致性等）优化 RGBA 表示。
- Result: ALPHAVAE 在仅使用 8K 图像训练的情况下，PSNR 提高了 4.9 dB，SSIM 提高了 3.2%，且在潜在扩散框架中表现出色。
- Conclusion: ALPHAVAE 为透明图像生成提供了高效解决方案，代码和数据已开源。


### [46] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 论文提出了ProactiveBench基准和PAUC指标，用于评估多模态对话系统的主动交互能力，PAUC优于传统指标。

- Motivation: 随着多模态对话系统研究的深入，用户期望系统能更主动交互，例如在视频播放时实时决定多轮响应时机。
- Method: 引入ProactiveBench基准和PAUC指标，考虑响应时间动态性，通过基准测试和用户研究验证。
- Result: PAUC与传统指标相比更符合人类偏好，能更准确评估主动交互场景的用户体验。
- Conclusion: PAUC为主动交互场景提供了更可靠的评估方法，推动了多模态对话系统的发展。


### [47] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: 论文提出了一种动态类间混淆感知编码器（DICCAE），通过细粒度的音频-视频对齐和动态调整混淆损失，提升模型对相似活动的区分能力。

- Motivation: 现有音频-视频预训练范式仅关注整体模态对齐，忽略了通过认知归纳和对比强化易混淆类别的区分能力。
- Method: 提出DICCAE编码器，动态调整类间混淆损失；引入包含音频、视频及其融合的新型训练框架；采用聚类引导的自监督预训练策略解决数据稀缺问题。
- Result: 在VGGSound数据集上达到65.5%的top-1准确率，接近最先进水平。
- Conclusion: DICCAE通过细粒度对齐和动态混淆损失，显著提升模型性能，并通过消融实验验证了各模块的必要性。


### [48] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: Fast3D是一个用于3D多模态大语言模型（MLLMs）的视觉令牌剪枝框架，通过全局注意力预测和样本自适应剪枝技术提高计算效率。

- Motivation: 3D MLLMs在场景理解方面表现出色，但计算效率低，主要瓶颈在于处理过多的对象中心视觉令牌。现有2D令牌剪枝方法在3D领域适用性有限。
- Method: 提出Fast3D框架，包含全局注意力预测（GAP）和样本自适应剪枝（SAP）技术，无需修改目标模型参数。
- Result: 在五个基准测试中验证了Fast3D的有效性，尤其在高剪枝率下表现优异。
- Conclusion: Fast3D显著提升了3D MLLMs的计算效率，为实际部署提供了可行方案。


### [49] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: 本文探讨了使用简单的Video ViTs架构和预训练技术实现高效的交通异常检测（TAD），发现预训练是关键，且自监督方法表现最佳。

- Motivation: 现有TAD方法依赖复杂架构，但预训练基础模型可能提供更简单高效的解决方案。
- Method: 采用简单的编码器架构（Video ViTs），研究不同预训练方法（弱监督、全监督、自监督MVM和DAPT）对TAD的影响。
- Result: 强预训练的简单模型性能优于复杂方法；自监督MVM表现最佳；DAPT进一步提升性能。
- Conclusion: 预训练是关键，简单架构结合有效预训练可实现高效、可扩展的TAD。


### [50] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: 该研究开发了一个基于CNN的图像分类系统，用于自动检测和分类八种常见作物病害，并结合治疗建议模块，部署在移动平台上，为农民提供实时诊断。

- Motivation: 作物病害对农业生产力构成重大威胁，尤其是在大规模农业中，早期识别往往延迟或不准确。
- Method: 采用CNN架构，包括卷积层、池化层和全连接层，使用TensorFlow和Keras进行训练，并结合图像预处理和增强技术。
- Result: 系统训练准确率约90%，验证准确率约60%，表明存在轻微过拟合，但整体性能可靠。
- Conclusion: 该研究为精准农业提供了可扩展且易用的工具，结合深度学习与农业实践支持，展示了CNN在全球作物健康监测中的潜力。


### [51] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: 本文提出了一种低资源处理相机陷阱数据的流程，结合ML/AI技术，适用于资源有限的小型研究团队。

- Motivation: 相机陷阱数据量大且处理复杂，现有工具难以满足资源有限团队的需求，因此需要一种实用的解决方案。
- Method: 开发了一种低资源流程，包括数据传输、推断和评估，结合ML/AI技术，适用于本地部署。
- Result: 该流程为小型研究团队提供了高效处理相机陷阱数据的方法，帮助发现有意义的研究结果。
- Conclusion: 通过实用化的低资源流程，研究者能够更好地管理和分析日益增长的相机陷阱数据。


### [52] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级神经网络方法，用于实时检测和描述天体表面地形特征，解决了传统方法计算量大和数据稀缺的问题。

- Motivation: 传统的地形特征检测方法依赖离线处理，计算成本高且泛化能力有限，而现有学习技术又难以在航天器硬件上实时运行。本文旨在解决这些问题。
- Method: 采用轻量级神经网络架构，结合改进的域适应方法和注意力对齐技术，实现实时地形特征检测与描述。
- Result: 提出的系统在性能上优于现有技术，适用于实时操作。
- Conclusion: 该方法为航天器自主导航提供了高效、实时的解决方案。


### [53] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: 提出了一种计算高效的多人体运动预测模型，通过简化时空交互，显著降低计算成本并实现最佳性能。

- Motivation: 解决多人体运动预测中复杂的个体和交互依赖性及高计算成本问题。
- Method: 设计轻量级双分支学习局部和全局表示，引入跨级别交互块整合时空表示，并显式嵌入空间人际距离。
- Result: 在CMU-Mocap、MuPoTS-3D和3DPW数据集上实现最佳性能，同时显著降低计算成本。
- Conclusion: 提出的高效时空设计在多人体运动预测中表现出色，兼具高性能和低计算成本。


### [54] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D是一个新颖的3D点云实例分割框架，结合了注意力机制、嵌入学习和跨模态对齐，支持无监督实例分割和零样本检索。

- Motivation: 解决3D点云实例分割中几何结构建模和多模态理解的统一问题，减少监督需求并提升实用性。
- Method: 构建分层特征提取器，结合对比聚类实现无监督实例分割，并在共享语义空间中对齐3D数据与自然语言查询。
- Result: 在实例分割和多模态理解方面优于Mask3D和ULIP等方法，具有更少的监督需求和更高的实用性。
- Conclusion: SegVec3D成功统一了实例分割和多模态理解，展示了在低监督和实际部署中的潜力。


### [55] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: CKAA框架通过双级知识对齐和任务置信度引导的适配器混合，提升了持续学习模型对误导任务ID的鲁棒性。

- Motivation: 解决参数高效微调（PEFT）方法中因独立训练子模块导致特征子空间不对齐，从而在误导任务ID下产生模糊决策的问题。
- Method: 提出CKAA框架，包括双级知识对齐（DKA）和任务置信度引导的适配器混合（TC-MoA）。
- Result: 实验表明CKAA优于现有PEFT-based持续学习方法。
- Conclusion: CKAA通过知识对齐和自适应聚合任务知识，显著提升了模型鲁棒性。


### [56] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: 提出了一种名为HMID-Net的新方法，结合掩码图像建模（MIM）和知识蒸馏技术于双曲空间，显著提升了模型效率与性能。

- Motivation: 现有方法MERU虽成功将多模态学习技术从欧几里得空间扩展到双曲空间，但如何更高效地训练模型以捕获和利用视觉-语义层次结构仍是一个关键问题。
- Method: 提出HMID-Net，首次在双曲空间中结合MIM和知识蒸馏技术，并设计了一种专门的双曲空间知识蒸馏损失函数。
- Result: 实验表明，双曲空间中的MIM和知识蒸馏技术能取得与欧几里得空间相同的显著成功，在图像分类和检索任务中显著优于MERU和CLIP等现有模型。
- Conclusion: HMID-Net通过双曲空间中的MIM和知识蒸馏技术，高效地捕获了视觉-语义层次结构，并在多个下游任务中表现出色。


### [57] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: GLIMPSE是一个新的视频理解基准测试，旨在评估大型视觉语言模型（LVLMs）是否能真正进行视频推理，而非仅依赖关键帧分析。

- Motivation: 现有视频基准测试多基于静态图像问题，无法评估模型是否具备深度时间推理能力。
- Method: GLIMPSE包含3,269个视频和4,342个视觉中心问题，覆盖11个类别，要求完整观看视频并进行上下文推理。
- Result: 人类评估准确率达94.82%，但最佳LVLM模型（GPT-o3）仅达66.43%，显示模型在视频推理上仍有困难。
- Conclusion: GLIMPSE揭示了LVLMs在视频理解上的局限性，需进一步改进以实现真正的视频推理。


### [58] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: 论文提出了一种自适应的张量正则化网络（SDTN）和轻量级网络（TRN），用于高光谱图像分类，解决了高维数据、光谱-空间冗余和标记样本稀缺的问题，显著提升了分类精度并降低了计算复杂度。

- Motivation: 高光谱图像分类在精准农业中至关重要，但传统方法在高维数据、光谱-空间冗余和标记样本稀缺的情况下表现不佳。
- Method: 结合张量分解和正则化机制的自适应网络（SDTN）动态调整张量秩，优化特征表示；TRN则利用SDTN提取的特征，构建轻量级网络捕获多尺度光谱-空间特征。
- Result: 在PaviaU数据集上的实验表明，该方法在分类精度和模型参数减少方面优于现有技术。
- Conclusion: 提出的SDTN和TRN框架不仅提高了分类精度，还降低了计算复杂度，适用于资源受限的实时部署环境。


### [59] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 论文提出了一种可靠的测试时适应方法（ReTA），通过一致性感知熵重加权（CER）和多样性驱动的分布校准（DDC）解决现有缓存方法在分布偏移下的不可靠性问题。

- Motivation: 视觉语言模型（VLMs）在零样本任务中表现优异，但在无标注数据的情况下面对分布偏移时性能下降，因此需要改进测试时适应（TTA）方法。
- Method: ReTA结合了CER（通过一致性约束加权熵以优化缓存）和DDC（通过高斯分布建模类级文本嵌入以调整决策边界）。
- Result: 实验表明，ReTA在真实世界分布偏移下优于现有方法。
- Conclusion: ReTA通过提升缓存质量和决策边界的灵活性，显著提高了VLMs在分布偏移下的适应性和可靠性。


### [60] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: HFUT-VUT团队提出了一种用于微手势在线识别的新方法，结合数据增强和时空注意力机制，显著提升了性能，在IJCAI 2025 MiGA挑战赛中排名第一。

- Motivation: 微手势在线识别任务具有挑战性，需精确定位时间位置并识别类别，且微手势与其他人类动作差异较大。
- Method: 采用手工数据增强和时空注意力机制，提升模型对微手势的分类和定位能力。
- Result: F1分数达到38.03，比之前的最佳方法提升了37.9%，在挑战赛中排名第一。
- Conclusion: 所提方法在微手势在线识别任务中表现出色，显著优于现有技术。


### [61] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap是一种后训练激活剪枝方法，通过去除冗余空间激活并恢复维度来提升VMamba等SSM模型的吞吐量，且无需重新训练。

- Motivation: 解决VMamba等基于状态空间模型的视觉骨干网络中空间冗余问题，提升效率。
- Method: 提出QuarterMap方法，通过剪枝冗余空间激活并使用最近邻上采样恢复维度。
- Result: 在ImageNet-1K上实现11%的速度提升且精度下降小于0.9%，在ADE20K和MedMamba上也表现良好。
- Conclusion: QuarterMap为SSM模型提供了一种即插即用的部署效率工具，无需牺牲迁移性。


### [62] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: 提出DehazeSB，一种基于Schrödinger Bridge的无配对去雾框架，利用最优传输理论直接桥接雾图和清晰图的分布，生成高质量结果。

- Motivation: 现有基于GAN的无配对去雾方法因生成器的传输映射能力有限，效果受限。
- Method: 结合最优传输理论和细节保留正则化，引入提示学习以利用预训练CLIP模型区分雾图和清晰图。
- Result: 在多个真实数据集上表现优越。
- Conclusion: DehazeSB通过更优的传输映射和细节保留，显著提升无配对去雾效果。


### [63] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: VDInstruct是一种多模态大语言模型，通过分离空间区域检测与语义特征提取，采用内容感知的标记化策略，显著提升了密集文档的理解效率与准确性。

- Motivation: 现有MLLMs在密集文档上表现不佳，且视觉标记化方法效率低下，导致计算冗余和内存浪费。
- Method: 提出VDInstruct模型，采用内容感知的标记化策略和三阶段训练范式，优化文档理解。
- Result: 在KIE基准测试中达到SOTA，零样本评估中F1分数提升5.5点，图像标记数量减少3.6倍。
- Conclusion: 内容感知标记化与显式布局建模为文档理解提供了有效方向。


### [64] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: 提出了一种基于动态RPCA的深度学习网络DRPCA-Net，通过整合稀疏先验和动态展开机制，显著提升了红外小目标检测的准确性和泛化能力。

- Motivation: 现有深度学习方法在红外小目标检测中过于依赖复杂架构，忽视了目标稀疏性的先验知识，导致模型可解释性、参数效率和泛化能力不足。
- Method: 结合模型驱动的RPCA和深度学习，提出动态RPCA网络（DRPCA-Net），通过轻量级超网络动态生成迭代参数，并设计动态残差组（DRG）模块优化背景建模。
- Result: 在多个公开红外数据集上，DRPCA-Net的检测精度显著优于现有方法。
- Conclusion: DRPCA-Net通过动态展开机制和稀疏先验的整合，实现了高效且鲁棒的红外小目标检测。


### [65] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: 论文提出了一种新任务——序列化紧密间隔红外小目标（CSIST）分离，并贡献了一个开源生态系统，包括数据集SeqCSIST和工具包。作者还提出了DeRefNet模型，通过TDFA模块提升性能，实验显示其优于现有方法。

- Motivation: 由于光学镜头焦距和红外探测器分辨率的限制，远距离CSIST群在红外图像中表现为混合斑点，现有方法难以精确检测，且缺乏高质量公开数据集。
- Method: 提出了DeRefNet，一种模型驱动的深度学习框架，包含TDFA模块用于自适应帧间信息聚合。
- Result: 在SeqCSIST数据集上，DeRefNet的mAP指标比现有方法提高了5.3%。
- Conclusion: 该研究首次在多帧范式中解决CSIST分离任务，并提供了开源数据集和工具包，推动了相关研究。


### [66] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: 提出了一种分段架构EHPE，通过局部提取TIP和手腕关节，减少误差累积，提升手部姿态估计精度。

- Motivation: 现有方法忽视TIP和手腕的重要性，导致远端关节误差累积，影响整体姿态估计质量。
- Method: EHPE分两阶段：TW-stage提取TIP和手腕关节，PG-stage通过双分支交互网络优化其余关节位置。
- Result: 在两个基准测试中表现优异，达到最先进水平。
- Conclusion: EHPE有效解决了误差累积问题，提升了手部姿态估计的整体质量。


### [67] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 本文首次全面调查了SAM及其变体中的提示工程技术，揭示了其从简单几何输入到多模态方法的发展，并探讨了关键挑战与未来方向。

- Motivation: 尽管SAM通过基于提示的方法革新了图像分割，但提示工程的关键作用尚未充分研究。本文旨在填补这一空白。
- Method: 系统整理并分析了快速发展的提示工程技术，涵盖基础方法、实际应用和关键挑战。
- Result: 揭示了提示工程从简单几何输入到多模态方法的演变，并识别了优化中的独特挑战。
- Conclusion: 本文为理解和推进分割基础模型中的提示工程提供了结构化框架。


### [68] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft是一个交互式艺术字体系统，通过扩散模型实现多区域生成和连续优化，结合大语言模型解析用户提示，显著提升了艺术字体合成的交互性和灵活性。

- Motivation: 传统艺术字体设计依赖手工，现有生成模型在交互性、局部编辑和多字符组合方面存在局限。
- Method: WordCraft采用无训练的区域注意力机制和噪声混合技术，结合大语言模型解析用户提示。
- Result: 系统支持高质量、多语言的艺术字体生成，适用于单字符和多字符输入。
- Conclusion: WordCraft为艺术家和设计师提供了更灵活的创作工具，扩展了艺术字体的创意可能性。


### [69] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR是一个新型自回归框架，通过两阶段训练实现多模态输入与图像输出的细粒度对齐，提升生成控制性和效率。

- Motivation: 解决现有文本到图像模型在多模态输入平衡、精确视觉控制和复杂多模态图像生成训练需求方面的局限性。
- Method: 结合自回归图像生成器和两阶段训练范式：多模态对齐阶段和指令调优阶段，无需额外适配器或跨注意力模块。
- Result: 在DreamBench++基准测试中表现优异，优于基线模型，同时提供更高的图像重建保真度和训练效率。
- Conclusion: MENTOR在多模态图像生成中实现了高效、可控和高质量的生成效果。


### [70] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: MA-SAM2是一种无需训练的视频对象分割策略，通过上下文感知和抗遮挡内存模型，显著提升了SAM2在复杂手术视频中的分割性能。

- Motivation: 手术视频分割对提升手术质量和患者结果至关重要，但SAM2的贪婪选择内存设计在复杂手术视频中表现不佳。
- Method: 提出MA-SAM2，采用上下文感知和抗遮挡内存模型，支持多目标单循环单提示推理。
- Result: 在EndoVis2017和EndoVis2018数据集上，MA-SAM2性能分别提升4.36%和6.1%。
- Conclusion: MA-SAM2在复杂手术视频中表现出色，具有实际应用潜力。


### [71] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1是一种基于扩散的文本到图像生成模型，性能优于主流模型，但缺乏官方技术文档。本报告通过逆向工程解析其架构。

- Motivation: FLUX.1在文本到图像生成领域表现卓越，但缺乏官方技术细节，阻碍了进一步研究和应用。
- Method: 通过逆向工程从源代码中解析FLUX.1的架构和训练设置。
- Result: 报告提供了FLUX.1的非官方技术细节，支持其作为未来研究的骨干模型。
- Conclusion: FLUX.1的逆向工程解析有助于推动其在研究和开发中的应用，但需注意非官方性质。


### [72] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: Inter2Former通过优化密集令牌处理的计算分配，提出四种关键改进，实现了在CPU设备上的高效高精度交互式分割。

- Motivation: 当前交互式分割方法在密集令牌处理上存在速度与精度的权衡，Inter2Former旨在解决这一问题。
- Method: 提出动态提示嵌入（DPE）、动态混合注意力（DHA）、混合专家（HMoE）和动态局部上采样（DLU）四种改进。
- Result: 在高精度交互式分割基准测试中，Inter2Former实现了最先进的性能，并在CPU设备上高效运行。
- Conclusion: Inter2Former通过计算优化，成功平衡了交互式分割的速度与精度，适用于实际应用场景。


### [73] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: FAIR方法通过动态对齐图像和文本特征，改进了无监督适应中的伪标签生成，显著提升了细粒度分类性能。

- Motivation: 现有方法在细粒度分类中无法捕捉动态的类别差异或计算成本高，FAIR旨在解决这些问题。
- Method: FAIR通过类描述锚点（CDA）和学习对齐分数（LAS）动态对齐图像和文本特征，并提出自训练加权机制优化伪标签。
- Result: FAIR在13个细粒度数据集上比现有方法平均提升2.78%的性能。
- Conclusion: FAIR通过动态跨模态交互和伪标签优化，显著提升了细粒度无监督适应的性能。


### [74] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: GAA是一个基于区域引导的少样本异常图像-掩码对生成框架，利用预训练的潜在扩散模型生成真实、多样且语义对齐的异常样本，解决了现有方法在异常合成中的低真实性和对齐问题。

- Motivation: 工业制造中异常样本稀缺限制了现有方法在定位和分类任务中的效果，现有异常合成方法存在低真实性、掩码对齐不准确和泛化能力差的问题。
- Method: GAA通过局部概念分解联合建模异常的语义特征和空间信息，利用自适应多轮异常聚类增强异常表示一致性，并通过区域引导的掩码生成策略确保异常与掩码的精确对齐。
- Result: 在MVTec AD和LOCO数据集上的实验表明，GAA在异常合成质量和下游任务（如定位和分类）中表现优异。
- Conclusion: GAA框架通过生成高质量、语义对齐的异常样本，显著提升了异常检测任务的性能。


### [75] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 该研究提出了一种基于MaxViT的人工智能框架，用于多类别中风分类（缺血性、出血性和无中风），在CT扫描图像上达到98%的准确率和F1分数，并通过可解释AI增强模型透明度。

- Motivation: 中风是全球主要死因之一，早期准确诊断对改善患者预后至关重要，尤其是在紧急情况下。CT扫描因其快速、可及性和成本效益成为关键成像方式。
- Method: 采用MaxViT作为主要深度学习模型，结合其他Transformer变体（如Vision Transformer和ConvNext），并通过数据增强（包括合成图像生成）解决类别不平衡问题。
- Result: MaxViT模型在增强数据训练下表现最佳，准确率和F1分数达98%，优于其他模型和基线方法。
- Conclusion: 该研究开发了一种可信赖的AI辅助诊断工具，结合可解释AI（如Grad-CAM++），为临床实践提供了准确、可解释的中风早期检测方案。


### [76] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: 该研究评估了AI模型在糖尿病视网膜病变（DR）预测中的公平性和性能，探讨了解缠技术对减少偏见的影响。

- Motivation: 糖尿病视网膜病变是导致工作年龄成年人视力丧失的主要原因，传统筛查方法成本高且难以普及，AI算法提供了一种可扩展的解决方案，但公平性和泛化性问题仍需解决。
- Method: 使用mBRSET眼底数据集，训练了三种模型（ConvNeXt V2、DINOv2、Swin V2）预测DR和敏感属性（如年龄、性别），并评估公平性，应用解缠技术减少偏见。
- Result: 所有模型在DR预测中表现优异（最高94% AUROC），但对敏感属性的预测存在差异（如DINOv2在年龄组间存在10% AUROC差距）。解缠技术对模型性能影响不一。
- Conclusion: 研究强调了医学影像AI中公平性的重要性，解缠技术的效果因模型而异，需进一步探索以确保公平可靠的医疗解决方案。


### [77] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: EyeSeg是一个用于AR/VR中眼部分割的新型框架，通过贝叶斯不确定性学习解决运动模糊、眼睑遮挡和域差距问题，提升了分割和视线估计的鲁棒性。

- Motivation: 现有方法在运动模糊、眼睑遮挡和域差距下表现不佳，需要更鲁棒的眼部分割技术以提升AR/VR交互体验。
- Method: 设计了一个不确定性感知的眼部分割框架，通过贝叶斯后验学习量化不确定性，并融合多个视线估计结果以提高鲁棒性。
- Result: 在MIoU、E1、F1和ACC等指标上优于现有方法，尤其在运动模糊、眼睑遮挡和跨域场景中表现突出。
- Conclusion: EyeSeg通过不确定性建模显著提升了眼部分割和视线估计的性能，适用于AR/VR中的复杂场景。


### [78] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: VST-Pose是一种基于WiFi信道状态信息的深度学习框架，通过双流架构和速度建模分支实现高精度连续姿态估计，在智能家居场景中表现优异。

- Motivation: 利用WiFi的穿透性和隐私优势，提供非视觉的姿态估计方案，解决传统视觉方法的局限性。
- Method: 提出ViSTA-Former双流时空注意力架构，分别捕捉时间依赖和关节结构关系，并集成速度建模分支增强细微动作敏感性。
- Result: 在自建数据集上PCK@50准确率达92.2%，优于现有方法8.3%；在公开MMFi数据集上验证了3D姿态估计的鲁棒性。
- Conclusion: VST-Pose为室内环境提供了一种可靠且隐私友好的连续运动分析解决方案。


### [79] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: 提出了一种基于提示的单目深度估计框架，用于高分辨率DEM生成，实现了100倍分辨率提升，并在多种地形中表现出色。

- Motivation: 高分辨率高程数据对水文、城市形态和生态系统研究至关重要，但现有方法存在局限性。
- Method: 结合低分辨率SRTM数据和高分辨率RGB图像，利用视觉变换器编码器和LiDAR数据微调，实现DEM估计、填补和更新。
- Result: 在三种不同地形中，分辨率提升至30厘米，误差低于5米，优于SRTM 18%。
- Conclusion: 该框架具有广泛适用性，适用于灾害和环境研究，代码和模型已开源。


### [80] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: 论文提出自动生成多学科科学实验评论的任务，并构建了首个相关数据集ExpInstruct，同时提出了检索增强模型ExpStar，显著优于现有大型多模态模型。

- Motivation: 解决人工教师依赖专业知识和时间成本高的问题，探索大型多模态模型在生成细粒度实验评论方面的潜力。
- Method: 构建ExpInstruct数据集，提出检索增强模型ExpStar，结合外部知识生成评论。
- Result: ExpStar在实验中显著优于14种领先的大型多模态模型。
- Conclusion: ExpStar在AI辅助科学实验教学方面具有巨大潜力。


### [81] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 本文系统分类并比较了视觉Transformer（ViT）中的令牌压缩技术，揭示了其在紧凑型ViT上的局限性。

- Motivation: 当前缺乏对令牌压缩方法的统一分类和比较，且其在紧凑型ViT上的有效性尚未验证。
- Method: 提出了一种系统分类法，并在标准和紧凑型ViT架构上评估了代表性令牌压缩技术。
- Result: 令牌压缩方法在通用ViT上有效，但在紧凑型设计上表现不佳。
- Conclusion: 研究为未来在边缘AI中优化紧凑型Transformer提供了方向。


### [82] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: 论文提出了一种改进的变分分数蒸馏方法（$L^2$-VSD），通过线性化模型和前瞻优化顺序解决了传统VSD的收敛问题和训练不稳定性。

- Motivation: 传统变分分数蒸馏（VSD）在实践中存在收敛慢和训练不稳定的问题，论文旨在通过优化模型交互和引入线性化方法提升生成质量。
- Method: 提出线性化前瞻变分分数蒸馏（$L^2$-VSD），通过调整优化顺序和使用线性化模型改进梯度校正。
- Result: 实验证明$L^2$-VSD在生成质量和稳定性上优于现有方法，并能无缝集成到其他VSD框架中。
- Conclusion: $L^2$-VSD是一种高效且稳定的文本到3D生成方法，显著提升了VSD的性能。


### [83] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 提出了一种高效的混合（几何和图像）方法，用于计算碎片对的最优对齐，无需假设其形状、尺寸或图像内容。

- Motivation: 现有方法难以处理真实拼图中碎片的几何特性，且通常依赖于碎片的限制形状。
- Method: 结合几何和图像信息，提出新的兼容性计算方法，并引入新的碎片数据集和侵蚀模型。
- Result: 在RePAIR 2D数据集上实现了最先进的邻域级精度和召回率。
- Conclusion: 该方法显著提升了碎片兼容性计算的性能，适用于考古拼图等实际应用。


### [84] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: NegRefine提出了一种改进的负标签细化框架，用于零样本OOD检测，通过过滤子类别标签和专有名词，并引入多匹配感知评分函数，提升了检测的鲁棒性。

- Motivation: 现有基于负标签的方法（如NegLabel和CSP）在区分OOD样本时存在误判问题，尤其是当负标签是分布内标签的子类别或专有名词时。此外，这些方法难以处理匹配多个标签的图像。
- Method: NegRefine通过过滤机制排除负标签集中的子类别标签和专有名词，并引入动态调整多标签匹配贡献的评分函数。
- Result: 在ImageNet-1K等大规模基准测试中，NegRefine表现优于现有方法，实现了更鲁棒的OOD检测。
- Conclusion: NegRefine通过负标签细化和多匹配感知评分，显著提升了零样本OOD检测的性能。


### [85] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: 论文提出了VRU-Accident基准，用于评估多模态大语言模型（MLLMs）在涉及弱势道路使用者（VRUs）的高风险交通场景中的推理能力。

- Motivation: 现有研究缺乏标准化的基准来定量评估MLLMs在复杂、安全关键的VRU场景中的表现。
- Method: 构建了包含1K真实事故视频、6K多选题对和1K密集场景描述的VRU-Accident基准，并评估了17种先进MLLMs。
- Result: MLLMs在视觉基础属性上表现良好，但在推理事故原因、类型和可预防性方面存在显著挑战。
- Conclusion: VRU-Accident填补了评估MLLMs在安全关键场景中的空白，揭示了其推理能力的不足。


### [86] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: 论文探讨了人类和深度学习模型在识别3D形状时的表现差异，发现视觉变换器模型更接近人类表现。

- Motivation: 研究深度学习模型是否形成与人类相似的3D形状表征。
- Method: 通过实验系统操纵点密度、物体方向和局部几何结构，比较人类与两种深度学习模型（DGCNN和点变换器）的表现。
- Result: 点变换器模型比卷积模型更接近人类表现，因其支持3D形状的层次抽象。
- Conclusion: 点变换器模型在3D形状表征上更接近人类视觉机制。


### [87] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: 综述了多模态大语言模型（MLLMs）在视觉丰富文档理解（VRDU）中的最新进展，包括特征编码与融合方法、训练范式及数据集，并讨论了未来方向。

- Motivation: VRDU因需要自动处理包含复杂视觉、文本和布局信息的文档而成为关键领域，MLLMs在此领域展现出巨大潜力。
- Method: 分析了MLLMs在VRDU中的三大核心组件：特征编码与融合方法、训练范式（如预训练策略和指令响应调优）及数据集使用。
- Result: 总结了MLLMs在VRDU中的应用现状，提出了未来研究方向。
- Conclusion: VRDU领域仍有挑战，未来需提升系统效率、泛化性和鲁棒性。


### [88] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: 论文介绍了SpeakerVid-5M数据集，这是一个用于音频-视觉双模态交互虚拟人生成的大规模高质量数据集，包含超过5.2百万视频片段，并提供了基准模型和测试数据。

- Motivation: 随着大规模模型的发展，音频-视觉双模态交互虚拟人成为新的研究热点，但缺乏相关数据集。论文旨在填补这一空白。
- Method: 构建了SpeakerVid-5M数据集，按交互类型和数据质量分类，并提供了基于自回归的视频聊天基准模型。
- Result: 数据集包含8,743小时视频，覆盖多种交互场景，并提供了基准测试VidChatBench。
- Conclusion: SpeakerVid-5M为音频-视觉双模态交互虚拟人研究提供了重要资源，未来工作可基于此展开。


### [89] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频推理范式ViTCoT，结合视觉和文本信息，显著提升了视频理解性能。

- Motivation: 现有方法主要依赖文本信息进行推理，忽视了视觉模态的重要性，而人类在推理时会自然重新审视视觉内容。
- Method: 构建了Video-Text Interleaved Benchmark (ViTIB)，并探索了ViTCoT范式在视频理解中的潜力。
- Result: 实验表明，ViTCoT显著优于传统仅文本的CoT范式，并能更有效地激活MLLMs中的神经元。
- Conclusion: ViTCoT为视频推理提供了一种更直观且认知对齐的方法，推动了视频理解领域的发展。


### [90] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出首个4D人体解析框架，通过减少推理时间和引入开放词汇能力，解决现有方法依赖封闭数据集和推理时间长的问题。

- Motivation: 动态3D人体表示在虚拟和扩展现实应用中日益重要，但现有方法受限于封闭数据集和长推理时间。
- Method: 基于开放词汇3D人体解析技术，提出三项创新：1) 采用基于掩码的视频对象跟踪；2) 设计掩码验证模块；3) 提出4D掩码融合模块。
- Result: 实验证明方法有效且灵活，推理速度提升93.3%。
- Conclusion: 该框架显著提升了4D人体解析的效率和适用性。


### [91] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: 提出了一种新框架CECAS，通过因果引导的对抗方法生成反事实解释，避免虚假相关性干扰，提升解释质量。

- Motivation: 现有反事实视觉解释方法忽视因果关系和虚假相关性，导致解释质量受限。
- Method: 利用因果引导的对抗方法生成反事实解释，避免对虚假因素的干扰。
- Result: 在多个基准数据集上优于现有方法，实现有效性、稀疏性、邻近性和真实性的平衡。
- Conclusion: CECAS框架显著提升了反事实解释的质量和实用性。


### [92] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: MCGA提出了一种两阶段方法，通过学习光谱模式再估计RGB到HSI的映射，结合灰度感知注意力和量化自注意力，实现了高效的高光谱图像重建。

- Motivation: 现有方法直接学习RGB到HSI的映射，忽略了从低维到高维信息转换的固有挑战，MCGA旨在解决这一问题。
- Method: MCGA采用两阶段方法：1) 多尺度VQ-VAE学习光谱模式，提取混合码本；2) 通过查询码本特征优化RGB到HSI的映射，并结合灰度感知注意力和量化自注意力。
- Result: 实验表明MCGA在重建性能上达到最先进水平。
- Conclusion: MCGA通过两阶段设计和物理启发的注意力机制，实现了高效且鲁棒的高光谱图像重建。


### [93] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: 论文提出了一种严格旋转等变的单阶段检测器MessDet，通过改进网络结构和多分支头设计，在低参数量的情况下实现了高性能。

- Motivation: 航空图像中物体方向任意，旋转等变性对检测器至关重要，但目前研究较少，且现有方法多为近似旋转等变。
- Method: 实现严格旋转等变的骨干和颈部网络，并提出多分支头网络以减少参数并提高精度。
- Result: 在DOTA-v1.0、DOTA-v1.5和DIOR-R数据集上达到最优性能，且参数量极低。
- Conclusion: 严格旋转等变对航空图像检测器性能有显著提升，多分支头设计有效平衡参数与精度。


### [94] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: IGD是一种基于自然语言指令的多模态图形设计工具，通过参数化渲染和图像资产生成，实现可编辑的图形设计，解决了传统方法缺乏创造力和非可编辑性的问题。

- Motivation: 传统图形设计方法依赖布局生成，缺乏智能化和创造力，且现有扩散模型生成的文件不可编辑，视觉文本渲染效果差。
- Method: IGD采用参数化渲染和图像资产生成的新范式，利用多模态理解和推理能力预测属性、排序和布局，并通过扩散模型生成图像内容。
- Result: 实验结果表明，IGD在复杂图形设计任务中具有可扩展性和扩展性，提供了新的解决方案。
- Conclusion: IGD通过自然语言指令快速生成可编辑的多模态图层，为自动化图形设计提供了创新且实用的方法。


### [95] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: Crucial-Diff是一个领域无关的框架，通过生成关键样本解决数据稀缺问题，提升检测和分割性能。

- Motivation: 数据稀缺导致模型过拟合和数据集不平衡，现有生成模型生成的样本重复或简单，无法针对下游模型的弱点提供关键信息。
- Method: Crucial-Diff包含两个模块：SAFE（场景无关特征提取器）和WASM（弱点感知样本挖掘器），通过反馈生成高质量样本。
- Result: 在MVTec上达到83.63%的像素级AP和78.12%的F1-MAX；在息肉数据集上达到81.64%的mIoU和87.69%的mDice。
- Conclusion: Crucial-Diff能生成多样且高质量的样本，显著提升下游任务性能。


### [96] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 论文评估了GPT-4o-mini和Gemini 2.0 Flash在时尚产品属性识别任务中的零样本性能，发现Gemini 2.0 Flash表现更优。

- Motivation: 探索大型语言模型在细粒度时尚属性识别中的表现，以提升电商产品目录的组织和客户发现体验。
- Method: 使用DeepFashion-MultiModal数据集，仅以图像为输入，评估模型在18个时尚属性类别中的表现。
- Result: Gemini 2.0 Flash的宏F1得分为56.79%，优于GPT-4o-mini的43.28%。
- Conclusion: 研究为电商产品属性任务提供了实用见解，并指出领域特定微调的必要性，为时尚AI的未来研究奠定了基础。


### [97] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: 提出了一种基于多图像超分辨率（MISR）和卷积神经网络（CNN）的方法，用于在超低剂量条件下实现原子级分辨率，适用于对辐射敏感的样品。

- Motivation: 电子显微镜在原子分辨率下对辐射敏感材料（如蛋白质和二维材料）的应用受到辐射损伤的限制。
- Method: 结合多图像超分辨率（MISR）和卷积神经网络（CNN），通过融合多个低分辨率、亚像素偏移的图像，并利用合成多角度观测增强重建。
- Result: 在超低剂量条件下，实现了与传统ptychography相当的空间分辨率，适用于非晶、半晶和晶体样品。
- Conclusion: 该方法扩展了4D-STEM的能力，为辐射敏感材料的结构分析提供了通用解决方案。


### [98] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: KPHD-Net提出了一种基于Hölder散度的多视图分类和聚类方法，通过结合Dempster-Shafer证据理论和Kalman滤波器，提高了多视图学习的准确性和可靠性。

- Motivation: 现有方法通常使用KL散度估计不确定性，但忽略了模态间的领域差异，导致多视图集成和决策的可靠性不足。
- Method: KPHD-Net利用变分Dirichlet分布表示类别概率分布，结合Hölder散度和Dempster-Shafer证据理论，并通过Kalman滤波器增强融合结果的可靠性。
- Result: 实验表明，KPHD-Net在分类和聚类任务中优于现有方法，具有更高的准确性、鲁棒性和可靠性。
- Conclusion: KPHD-Net通过改进的不确定性估计和多视图融合方法，显著提升了多视图学习的性能。


### [99] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: 论文分析了潜在扩散模型（LDMs）中自编码器的作用，提出了一种新的变分掩码自编码器（VMAEs），并将其整合为LDMAEs，显著提升了图像生成质量和计算效率。

- Motivation: 尽管潜在扩散模型在图像生成方面潜力巨大，但自编码器的理想特性和最优设计尚未充分探索。
- Method: 通过分析自编码器的三个关键特性（潜在平滑性、感知压缩质量和重建质量），提出VMAEs，并将其整合到LDM框架中形成LDMAEs。
- Result: 实验表明，LDMAEs显著提升了图像生成质量和计算效率。
- Conclusion: VMAEs和LDMAEs为潜在扩散模型中的自编码器设计提供了新的方向，并取得了显著改进。


### [100] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 3DGAA是一种基于3D高斯散射的对抗攻击框架，通过联合优化几何和外观属性，生成物理上可实现的对抗对象，显著降低检测性能。

- Motivation: 现有2D和3D物理攻击在平衡物理真实性和攻击鲁棒性方面存在困难，3DGAA旨在解决这一问题。
- Method: 利用3D高斯散射的14维参数化，联合优化几何和外观属性，并引入物理过滤和增强模块。
- Result: 在虚拟和物理实验中，3DGAA将检测mAP从87.21%降至7.38%，优于现有方法。
- Conclusion: 3DGAA是一种实用的攻击框架，可用于评估自动驾驶感知系统的安全性。


### [101] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: 该研究利用多壳层扩散MRI数据和视觉Transformer框架，支持阿尔茨海默病的早期诊断和淀粉样蛋白积累检测，取得了较高的分类准确率。

- Motivation: 通过多壳层扩散MRI数据的微结构信息，结合深度学习技术，实现阿尔茨海默病和淀粉样蛋白的早期诊断。
- Method: 采用Swin Transformer模型，结合DTI和NODDI指标，通过低秩适应技术优化模型，用于阿尔茨海默病和淀粉样蛋白状态的分类。
- Result: 在阿尔茨海默病诊断中达到95.2%的平衡准确率，淀粉样蛋白检测中最高为77.2%，并识别出关键脑区。
- Conclusion: 研究表明扩散MRI和Transformer架构在阿尔茨海默病早期检测中具有潜力，支持数据有限的生物医学诊断。


### [102] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: 本文综述了反无人机跟踪技术的现状、挑战及未来方向，重点介绍了基于视觉和多传感器融合的算法，并提供了公开数据集链接。

- Motivation: 随着无人机技术的快速发展和广泛应用，高效准确的反无人机跟踪在公共安全、边境巡逻等复杂场景中变得至关重要。
- Method: 回顾了反无人机检测与跟踪技术的特点和挑战，整理了公开数据集，并分析了近年来的视觉和视觉融合算法。
- Result: 总结了当前主流技术及其局限性，为研究者提供了数据支持和算法参考。
- Conclusion: 提出了未来研究方向，旨在推动反无人机跟踪领域的进一步发展。


### [103] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于图像序列的二项式自补偿方法（I-BSC），解决了相位移动轮廓术（PSP）在动态测量中的运动误差问题，显著降低了计算复杂度。

- Motivation: PSP在动态测量中因物体运动而产生误差，现有方法（P-BSC）计算复杂且误差累积。
- Method: I-BSC通过加权求和同质条纹图像而非相位帧，仅需一次反正切计算，降低了计算复杂度。
- Result: I-BSC在减少运动误差的同时实现了准单帧速率，计算帧速率提升数倍至数十倍。
- Conclusion: I-BSC显著优于现有方法，适用于高分辨率3D重建。


### [104] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: 本文重新评估了两种CLIP变体（ResNet和ViT）在bouba-kiki效应中的表现，发现它们未能像人类一样一致地关联形状与伪词，揭示了多模态模型的局限性。

- Motivation: 探讨视觉-语言模型（VLMs）是否像人类一样整合跨模态信息，尤其是bouba-kiki效应。
- Method: 使用基于提示的概率评估和Grad-CAM视觉注意力分析，对ResNet和ViT进行测试。
- Result: 模型未表现出一致的bouba-kiki效应，且表现远不如人类。
- Conclusion: VLMs在跨模态概念理解上存在局限，与人类认知不一致。


### [105] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: Hyma提出了一种基于超网络的多模态模型对齐方法，显著降低了模型选择和连接器训练的计算成本。

- Motivation: 多模态基础模型通常通过拼接多个预训练的单模态模型构建，但选择和训练连接器模块的计算成本高昂。
- Method: 利用超网络的参数预测能力，为多种单模态模型组合联合训练连接器模块。
- Result: Hyma将最优单模态模型对的搜索成本降低了10倍，同时性能与网格搜索相当。
- Conclusion: Hyma是一种高效的多模态模型对齐解决方案，显著提升了计算效率。


### [106] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种选择性优化框架，结合低分辨率反向传播（BP-low）和高分辨率零阶优化（ZO-high），实现内存高效且高质量的文本到图像扩散模型个性化。

- Motivation: 解决在边缘设备上高效个性化文本到图像扩散模型时的内存限制和隐私保护问题。
- Method: 通过动态选择BP-low和ZO-high优化策略，结合时间步感知的概率函数，平衡效率和效果。
- Result: 实验表明，该方法在显著降低内存消耗的同时保持了高质量性能。
- Conclusion: 该框架实现了高效、高质量的设备端个性化，且不增加推理延迟。


### [107] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为LifelongPR的持续学习框架，用于解决点云地点识别中的灾难性遗忘问题，通过动态样本选择和提示学习提升性能。

- Motivation: 点云地点识别在自动驾驶等领域至关重要，但现有模型在持续学习新环境时会出现性能下降，导致实用性受限。
- Method: 提出动态样本选择方法和基于提示学习的框架，结合两阶段训练策略，以减少知识遗忘并适应领域变化。
- Result: 在大规模数据集上验证，性能提升显著（mIR@1提高6.50%，mR@1提高7.96%，F减少8.95%）。
- Conclusion: LifelongPR框架有效解决了点云地点识别中的持续学习问题，提升了模型的实用性和性能。


### [108] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: CoSMo是一种用于漫画书页面流分割的新型多模态Transformer，显著优于传统方法和通用视觉语言模型。

- Motivation: 漫画书页面流分割（PSS）是自动化内容理解的关键任务，为下游任务（如角色分析、故事索引）提供基础。
- Method: 开发了视觉和多模态版本的CoSMo，并在20,800页的标注数据集上进行了测试。
- Result: CoSMo在F1-Macro、全景质量和流级指标上表现优异，视觉特征主导宏观结构，多模态有助于解决模糊问题。
- Conclusion: CoSMo为漫画书分析设立了新标准，支持可扩展的内容理解。


### [109] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: 该研究提出了一种基于轻量级机器学习的家禽疾病检测方法，通过分析粪便图像实现高准确率（95.85%）且低资源消耗。

- Motivation: 家禽养殖易受传染病威胁，现有方法资源消耗高，需一种低成本、高效且可扩展的解决方案。
- Method: 采用多颜色空间特征提取（RGB、HSV、LAB）及多种描述符（颜色直方图、LBP、小波变换等），结合PCA和XGBoost降维，最终用ANN分类器训练。
- Result: 模型在Google Colab上仅需638秒且无需GPU，准确率达95.85%，优于Xception和MobileNetV3等深度学习模型。
- Conclusion: 该方法为低资源农业环境提供了一种高效、可解释且可扩展的疾病检测替代方案。


### [110] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS是一个新颖的前馈模型，能够在一秒内从单目视频合成4D动态新视角。它通过像素对齐的高斯基元网格表示动态3D场景，并显式监督其时间变化运动，首次实现了外观、几何和运动的统一建模。

- Motivation: 传统的动态场景建模方法通常需要任务特定的监督，且难以统一处理外观、几何和运动。MoVieS旨在解决这些问题，通过一个学习框架实现多任务支持。
- Method: MoVieS使用像素对齐的高斯基元网格表示动态3D场景，显式监督时间变化运动，实现了外观、几何和运动的统一建模。
- Result: 实验表明，MoVieS在多个任务中表现出色，性能竞争性强，同时实现了数量级的加速。
- Conclusion: MoVieS通过统一的动态场景建模框架，支持多任务应用，并在效率和性能上取得了显著成果。


### [111] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: 论文提出了一种基于频率域调控的方法，通过小波变换分别调整低频和高频子带，显著改善了扩散模型的生成质量，并解决了曝光偏差问题。

- Motivation: 扩散模型在生成能力上表现出色，但受到曝光偏差的显著影响。论文通过观察预测噪声图像能量变化，发现能量减少在低频和高频子带中呈现不同模式，并导致重建数据与真实数据间的幅度差异。
- Method: 引入基于小波变换的频率域调控机制，分别调整低频和高频子带，并更准确地分析曝光偏差。方法无需训练，即插即用。
- Result: 显著提升了多种扩散模型的生成质量，为不同模型架构的曝光偏差问题提供了鲁棒解决方案。
- Conclusion: 论文提出的方法有效解决了扩散模型中的曝光偏差问题，通过频率域调控机制显著提升了生成质量，且具有广泛适用性。


### [112] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: 提出了一种基于SegFormer的两阶段迁移学习策略，显著提升了遥感图像水体分割的精度，解决了域偏移和小样本问题。

- Motivation: 解决遥感图像水体分割中域偏移和小样本的挑战，特别是在地形和光谱特征复杂的地区。
- Method: 采用两阶段迁移学习策略：先在多样源域训练基础分割模型，再在目标域微调。
- Result: IoU从直接迁移的25.50%提升至64.84%，显著改善了模型性能。
- Conclusion: 该策略有效解决了域差异导致的性能下降，为数据稀缺且环境独特的遥感场景提供了高精度信息提取的技术范例。


### [113] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP通过双分支训练、区域提示和分层特征对齐模块解决了CLIP在长文本任务中的限制，并在长短文本检索任务中取得了最佳性能。

- Motivation: CLIP在长文本输入（>77 tokens）任务中表现不佳，限制了其应用范围。
- Method: 提出FIX-CLIP，包括双分支训练、多区域提示和分层特征对齐模块，并利用合成数据训练。
- Result: FIX-CLIP在长短文本检索任务中表现最佳，且适用于扩散模型的长文本输入。
- Conclusion: FIX-CLIP有效解决了CLIP的长文本限制，具有广泛的应用潜力。


### [114] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: 提出了一种多摄像头多目标（MCMT）跟踪框架，通过轨迹和外观特征实现跨视图的全局身份一致性分配。

- Motivation: 解决多摄像头场景下目标跟踪中的身份一致性分配问题。
- Method: 采用BoT-SORT单摄像头跟踪，通过轨迹特征匹配初始化全局ID，后续帧使用优先全局匹配策略。
- Result: 实现了跨视图的全局身份一致性分配，并通过3D位置估计进行空间验证。
- Conclusion: 该框架在多摄像头多目标跟踪中表现出色，确保了身份一致性。


### [115] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 提出了一种新型半监督全景分割方法DEARLi，通过结合两个专用基础模型，显著提升了识别和定位能力，在标注数据稀缺的情况下表现优异。

- Motivation: 像素级标注成本高昂且耗时，半监督分割方法通过利用少量标注图像和大量未标注图像学习模型。基础模型的潜力尚未充分挖掘。
- Method: 结合无监督掩码变换一致性和CLIP特征的零样本分类增强识别；通过类无关解码器预热和SAM伪标签增强定位。
- Result: 在ADE20K数据集上仅用158张标注图像，取得了29.9 PQ和38.9 mIoU的优异性能，且GPU内存需求降低8倍。
- Conclusion: DEARLi在标注稀缺和大分类任务中表现突出，显著优于现有半监督语义分割方法。


### [116] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: 本文研究了先进点跟踪方法在超声心动图中的应用，发现现有方法因方向性运动偏差而受限，提出改进训练策略和轻量级网络，显著提升了性能。

- Motivation: 精确跟踪超声心动图中的可变形组织对心脏功能测量至关重要，但传统方法和现代点跟踪方法在此领域表现有限。
- Method: 通过分析心脏运动偏差，改进训练策略并引入定制化增强，提出轻量级网络利用多尺度成本体积。
- Result: 实验显示改进策略显著提升性能，EchoTracker位置精度提高60.7%，轨迹误差减少61.5%。
- Conclusion: 改进方法在临床评估中表现更优，与半自动化工具一致，展示了更好的实际应用价值。


### [117] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 论文提出了一种受预测编码启发的反馈机制，通过循环优化内部状态，显著提升了模型在噪声条件下的性能和数据效率。

- Motivation: 生物视觉系统依赖反馈连接优化感知，而人工神经网络多为前馈结构。研究旨在探索反馈机制如何提升模型的鲁棒性和适应性。
- Method: 在U-Net架构中引入反馈循环，结合软最大投影和指数衰减操作确保稳定性，并在合成分割任务中验证。
- Result: 反馈模型在噪声条件下表现显著优于前馈模型，且仅需两个训练样本即可达到随机性能，而前馈模型需至少四个。
- Conclusion: 反馈机制增强了模型的鲁棒性和数据效率，为更适应性和生物启发的神经网络架构提供了方向。


### [118] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: SlumpGuard是一种基于AI的视频系统，用于实时自动评估混凝土的工作性，取代传统的手工坍落度测试，提高质量和效率。

- Motivation: 传统坍落度测试手工操作耗时且不一致，无法满足实时监控需求，因此需要自动化解决方案。
- Method: 开发了SlumpGuard系统，通过分析混凝土从卡车溜槽流动的视频，自动评估工作性，无需人工干预。
- Result: 系统在实际部署中表现出色，提高了质量控制的准确性和效率。
- Conclusion: SlumpGuard是混凝土质量保证的实用解决方案，适用于现代施工需求。


### [119] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本的人物检索方法，通过图像和区域级别的域适应技术，解决了合成预训练数据与真实数据之间的域差距问题。

- Motivation: 由于隐私问题和手动标注成本高，合成数据成为预训练模型的选择，但合成数据与真实数据之间的域差距（如光照、颜色和视角差异）限制了预训练-微调范式的效果。
- Method: 提出了一个统一的文本人物检索流程，包含两个主要组件：图像级域适应的Domain-aware Diffusion（DaD）和区域级对齐的Multi-granularity Relation Alignment（MRA）。
- Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上取得了最先进的结果。
- Conclusion: 双级别域适应方法有效解决了域差距问题，显著提升了文本人物检索的性能。


### [120] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: 提出ECP框架，解决多模态大语言模型在高分辨率图像任务中的性能问题，通过两阶段方法保留细节并提升表现。

- Motivation: MLLMs在高分辨率图像任务中表现不佳，因固定分辨率训练导致泛化能力差，ECP旨在解决这一问题。
- Method: ECP框架分两阶段：1) 通过降采样图像提取候选区域；2) 基于候选区域预测最终结果，无需额外训练。
- Result: 在4K GUI grounding和4K、8K MLLM感知任务中，分别提升21.3%、5.8%、5.2%。
- Conclusion: ECP有效提升MLLMs在高分辨率任务中的性能，保留细节且无需额外训练。


### [121] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文提出了一种非对称表示学习（ARL）策略，通过不平衡优化提升多模态学习性能，证明模态依赖比例与方差比例成反比时性能最优。

- Motivation: 解决多模态学习中因梯度平衡导致的性能不佳问题，证明不平衡学习可能是更优设置。
- Method: 提出ARL策略，通过辅助正则器计算模态预测方差，重新加权优化，并联合优化预测偏差和多模态损失。
- Result: 实验验证了ARL的有效性和通用性，不引入额外参数且独立于模型结构。
- Conclusion: ARL通过不平衡优化显著提升多模态学习性能，为多模态学习提供了新思路。


### [122] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: 研究探讨了种族背景对情绪表达的影响，挑战了情绪普遍性假设，并提出了一种考虑种族差异的微表情识别框架。

- Motivation: 情绪普遍性假设认为情绪表达在不同文化和社会背景下是相同的，但研究认为这是过度概括，需考虑种族背景的影响。
- Method: 构建跨文化微表情数据库，算法标注种族标签，进行单一种族与混合种族的比较实验，并提出种族感知的微表情识别框架。
- Result: 实验揭示了种族偏见对微表情分析的影响，提出的框架能更好地识别种族差异。
- Conclusion: 种族背景对情绪表达有显著影响，未来的微表情研究应纳入种族因素以提高准确性。


### [123] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文提出了一种解耦梯度学习（DGL）框架，解决多模态学习中模态编码器与融合模块的优化冲突问题，提升性能。

- Motivation: 多模态学习中，模态间的优化冲突导致性能低于单模态学习，现有方法未能解决主导模态性能下降的问题。
- Method: DGL通过截断多模态损失对模态编码器的梯度，并用单模态损失梯度替代，同时移除单模态损失对融合模块的梯度干扰。
- Result: 实验表明，DGL在多种模态、任务和框架中均有效且通用。
- Conclusion: DGL成功解决了多模态学习中的优化冲突问题，显著提升了性能。


### [124] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: 提出了一种名为Wardrobe Polyptych LoRA的新方法，通过训练LoRA层实现高效个性化人体图像生成，解决了现有方法计算成本高的问题。

- Motivation: 个性化人体图像生成需要精确保留属性（如身份、服装细节），现有方法要么需要推理时微调，要么需要大规模数据集训练，计算成本高且不实用。
- Method: 训练LoRA层，利用空间参考减少信息丢失，并引入选择性主题区域损失以优化生成结果。
- Result: 在保真度和一致性上显著优于现有技术，实现了高保真且身份保留的全身合成。
- Conclusion: Wardrobe Polyptych LoRA无需额外参数，仅需少量训练样本即可实现高效个性化图像生成。


### [125] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: VRFNO是一种改进Reflow的新方法，通过噪声优化和历史速度项提升图像生成质量。

- Motivation: Reflow在单步或少步生成中存在分布差距问题，导致图像质量受限。
- Method: VRFNO结合编码器和神经速度场，引入历史速度项和噪声优化技术。
- Result: VRFNO在合成和真实数据上表现优异，显著优于Reflow。
- Conclusion: VRFNO有效解决了Reflow的局限性，成为单步和少步生成任务的先进方法。


### [126] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: Spatial Lifting (SL) 是一种新颖的密集预测方法，通过将输入提升到高维空间并使用高维网络处理，显著减少模型参数和推理成本，同时保持性能。

- Motivation: 传统密集预测方法通常计算成本高且参数多，SL旨在通过维度提升解决这些问题。
- Method: SL将2D图像等输入提升到高维空间（如3D），并使用高维网络（如3D U-Net）处理，生成结构化输出。
- Result: 在19个基准数据集上验证，SL在减少98%参数和降低推理成本的同时，性能与传统方法相当。
- Conclusion: SL为密集预测任务提供了一种高效、准确且可靠的新范式。


### [127] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: 论文介绍了名为ProGait的多用途数据集，用于支持基于视觉的机器学习任务，如步态分析，并展示了其在假肢特定任务中的优越性能。

- Motivation: 假肢步态分析对康复至关重要，但现有视觉方法在假肢检测和分析上存在挑战。
- Method: 提出ProGait数据集，包含412个视频片段，支持多种视觉任务，并提供基准模型。
- Result: 基准模型在假肢特定任务中表现优于预训练模型，展示了数据集的实用性。
- Conclusion: ProGait数据集填补了假肢步态分析的空白，为未来研究提供了重要资源。


### [128] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: SynOOD利用基础模型生成合成OOD数据，通过迭代修复和噪声调整增强CLIP模型的边界区分能力，显著提升OOD检测性能。

- Motivation: 解决现有预训练视觉语言模型在检测接近InD数据的OOD样本时容易误分类的问题。
- Method: 利用扩散模型和多模态大语言模型生成边界对齐的合成OOD数据，通过迭代修复和噪声调整优化样本，并微调CLIP模型。
- Result: 在ImageNet基准测试中，AUROC提升2.80%，FPR95降低11.13%，性能显著优于现有方法。
- Conclusion: SynOOD通过合成边界OOD数据有效增强了模型的OOD检测能力，且计算开销小。


### [129] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 论文探讨了AI生成图像检测（AID）在真实世界中的挑战，提出了ITW-SM数据集，并分析了影响检测性能的四个关键因素，最终将AUC提升了26.87%。

- Motivation: 随着生成技术的快速发展，AI生成图像的逼真度已足以欺骗人类，但现有AID模型在真实世界中的表现不佳，亟需改进。
- Method: 引入ITW-SM数据集，系统评估了骨干架构、训练数据组成、预处理策略和数据增强组合对AID性能的影响。
- Result: 通过优化上述因素，AID模型在真实世界中的AUC平均提升了26.87%。
- Conclusion: 研究揭示了AID模型在真实世界中的关键改进方向，为未来技术发展提供了重要参考。


### [130] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: 研究探讨了风格迁移在语义分割中减少纹理偏置和增强鲁棒性的效果。

- Motivation: 探索风格迁移是否能在语义分割中减少纹理偏置并提高鲁棒性。
- Method: 通过Voronoi细胞生成随机区域进行风格迁移，并用其训练语义分割DNN。
- Result: 风格迁移显著减少纹理偏置，增强了对图像损坏和对抗攻击的鲁棒性。
- Conclusion: 该方法在多种架构和数据集上均有效，具有普适性。


### [131] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: 论文提出了一种针对相机姿态估计模型的攻击方法（KBA），通过使用多对称性背景纹理显著降低模型准确性。

- Motivation: 在稀疏输入的对象中心场景中，背景纹理可能显著影响相机姿态估计的准确性，因此需要研究针对性的攻击方法。
- Method: 引入Kaleidoscopic Background Attack（KBA），利用多对称性纹理生成对抗性背景，并提出投影方向一致性损失优化纹理。
- Result: 实验表明，优化的对抗性背景能有效攻击多种相机姿态估计模型。
- Conclusion: KBA方法在攻击相机姿态估计模型方面表现出高效性，揭示了模型对背景纹理的敏感性。


### [132] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: 提出FTCFormer，通过语义聚类动态生成视觉标记，优化特征表示，提升图像分类性能。

- Motivation: 传统Transformer基于网格标记，忽略语义信息，导致特征表示不理想。
- Method: 引入聚类下采样模块、DPC-FKNN机制、SCS评分和Cmerge策略。
- Result: 在32个数据集上验证，分类性能显著提升，最高提升1.43%。
- Conclusion: FTCFormer通过语义聚类优化标记生成，有效提升图像分类效果。


### [133] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR是一种新方法，通过参考图像提供身份条件，结合解耦交叉注意力和反馈学习，解决人脸视频恢复中的身份漂移问题。

- Motivation: 传统方法在严重退化时难以保留细粒度的身份特征，导致结果缺乏个体特性。
- Method: IP-FVR利用参考图像作为视觉提示，采用解耦交叉注意力机制和反馈学习方法，结合指数混合策略和多流负提示。
- Result: 实验表明IP-FVR在质量和身份保留上优于现有方法。
- Conclusion: IP-FVR在解决身份漂移和提升恢复质量方面具有显著潜力。


### [134] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: FaceLLM是一种专为面部图像理解设计的MLLM，通过ChatGPT生成的弱监督数据训练，显著提升了面部相关任务的性能。

- Motivation: 现有MLLM在通用数据集上训练，缺乏对领域特定视觉线索（如面部图像）的推理能力。
- Method: 提出弱监督流程，利用ChatGPT生成高质量问答对，构建FairFaceGPT数据集，训练FaceLLM。
- Result: FaceLLM在面部中心任务中表现优异，达到SOTA性能。
- Conclusion: 展示了语言模型合成监督在构建领域专用MLLM中的潜力，为可信赖的多模态AI系统树立了先例。


### [135] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DisCo是一种新的视觉封装方法，通过视觉概念判别器和时间焦点校准器，解决了视频MLLM中的语义模糊和时间不连贯问题，显著提升了性能。

- Motivation: 线性投影器在视频MLLM中导致语义模糊和时间不连贯，而现有重采样器结构尚未有效解决这些问题。
- Method: DisCo包含视觉概念判别器（VCD）和时间焦点校准器（TFC），分别确保语义清晰和时间连贯。
- Result: 在多个视频MLLM框架上，DisCo显著优于现有方法，并在视频理解基准测试中表现优异，同时提高了令牌效率。
- Conclusion: DisCo为视频MLLM提供了一种高效且性能优越的视觉封装解决方案。


### [136] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于双视觉编码器的无注释手语翻译框架，通过对比视觉-语言预训练提升性能。

- Motivation: 传统方法依赖昂贵且不完整的注释，无法捕捉连续手语的复杂性。
- Method: 采用双视觉编码器框架，通过对比目标对齐视觉和文本嵌入，下游任务中融合视觉特征输入编码器-解码器模型。
- Result: 在Phoenix-2014T基准测试中，性能优于单流变体，并在无注释方法中取得最高BLEU-4分数。
- Conclusion: 双编码器框架有效解决了注释依赖问题，提升了手语翻译性能。


### [137] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: 论文提出了一种名为IMD的框架，利用预训练扩散模型解决图像特征匹配中的对齐问题，显著提升了多实例场景下的性能。

- Motivation: 现有方法在引入基础模型进行特征匹配时忽略了单图像理解与跨图像理解需求之间的不对齐问题，导致性能受限。
- Method: IMD框架包含两部分：1）使用生成式扩散模型捕捉实例级细节；2）提出跨图像交互提示模块促进双向信息交互。
- Result: IMD在常见基准测试中达到新SOTA，并在新提出的IMIM基准上提升12%。
- Conclusion: IMD有效解决了基础模型在特征匹配中的不对齐问题，显著提升了多实例场景的性能。


### [138] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: QLIP是一种新的量化方法，利用文本提示指导扩散模型每层和每时间步的比特精度选择，降低计算复杂度并提升生成图像质量。

- Motivation: 扩散模型在图像生成任务中表现出色，但计算复杂度高，限制了其在资源受限环境中的应用。现有量化方法未充分利用输入条件（如文本提示）的信息。
- Method: 提出QLIP方法，通过文本提示动态选择每层和每时间步的比特精度，并可无缝集成到现有量化方法中。
- Result: 实验表明，QLIP能有效降低计算复杂度，并在多个数据集上提升生成图像质量。
- Conclusion: QLIP为扩散模型量化提供了一种高效且灵活的方法，显著提升了资源受限环境中的实用性。


### [139] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: FGSSNet是一种多头部特征引导的语义分割架构，旨在提升平面图中墙壁分割的泛化能力。

- Motivation: 传统方法在平面图墙壁分割任务中泛化能力不足，FGSSNet通过引入多头部特征提取器来解决这一问题。
- Method: FGSSNet采用U-Net作为分割主干，结合多头部特征提取器提取域特定特征图，并将其注入U-Net的潜在空间以引导分割过程。特征提取器通过编码-解码结构训练，专注于墙壁纹理和宽度特征。
- Result: 实验表明，FGSSNet通过注入特征显著提升了性能，优于传统U-Net。
- Conclusion: FGSSNet通过特征引导机制有效提升了墙壁分割的泛化能力和准确性。


### [140] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种基于随机图模型的文本适配器（VRGAdapter），通过顶点随机知识图（VRKG）建模类别多样性和类间关系，并结合不确定性引导的多分支融合（UMF）提升下游任务性能。

- Motivation: 现有确定性文本适配器无法充分捕捉类别描述的多样性和类间关系，限制了视觉语言模型（VLM）在下游任务中的潜力。
- Method: 利用顶点随机知识图（VRKG）建模类别多样性和类间关系，通过概率消息传播学习上下文感知的分布表示，并结合UMF动态集成多个预训练模型。
- Result: 在多个基准数据集上的实验验证了VRGAdapter和UMF的有效性。
- Conclusion: VRGAdapter提供了一种更通用的适配器解决方案，显著提升了视觉语言模型在下游任务中的表现。


### [141] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文提出并解决了细粒度零样本目标检测（FG-ZSD）问题，开发了基于改进两阶段检测器的MSHC方法，并构建了首个FG-ZSD基准数据集FGZSD-Birds。

- Motivation: 现有零样本目标检测（ZSD）方法主要针对粗粒度对象，而现实场景中常需区分细节差异小的细粒度对象（如不同鸟类、鱼类和花卉）。
- Method: 提出MSHC方法，基于改进的两阶段检测器，采用多级语义感知嵌入对齐损失，确保视觉与语义空间的紧密耦合。
- Result: 在构建的FGZSD-Birds数据集上，MSHC方法优于现有ZSD模型。
- Conclusion: FG-ZSD是一个重要且具有挑战性的新问题，MSHC方法在细粒度检测中表现优异。


### [142] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: FOCAL是一种测试时、数据驱动的框架，利用基础模型的互联网规模视觉先验，通过生成和优化候选变换，实现鲁棒的视觉感知，无需重新训练或架构更改。

- Motivation: 当前方法依赖专用架构或预定义增强训练，限制了泛化能力，需要一种更通用的方法实现视觉感知的鲁棒性。
- Method: FOCAL框架通过生成和优化候选变换，将其导向视觉上典型的“规范”视图，从而增强鲁棒性。
- Result: 实验表明，FOCAL显著提升了CLIP和SAM在2D/3D旋转、光照变化（对比度和颜色）及昼夜变化等挑战性变换中的鲁棒性。
- Conclusion: FOCAL挑战了变换特定训练的必要性，提供了一种可扩展的鲁棒性实现路径，并展示了在主动视觉中的潜在应用。


### [143] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: 论文提出了一种将拓扑数据分析（TDA）特征与深度学习模型结合的方法，显著提升了遥感图像分类的性能。

- Motivation: 卷积神经网络（CNN）在图像分类中偏向于纹理特征，而TDA能捕捉几何信息，因此结合两者可以弥补CNN的局限性。
- Method: 设计了一个TDA特征工程流程，并将拓扑特征与ResNet18模型结合，应用于EuroSAT和RESISC45数据集。
- Result: 在EuroSAT数据集上，模型准确率达到99.33%，超过所有已知单模型性能；在RESISC45数据集上，比基线模型提升1.82%。
- Conclusion: TDA特征可以与深度学习模型有效结合，即使在没有显式拓扑结构的数据集上也能提升性能，扩展了TDA的适用性。


### [144] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度卷积神经网络的手写Devanagari字符识别方法，取得了高准确率。

- Motivation: 由于Devanagari脚本缺乏数字化工具，研究旨在通过自动化方法识别手写印地语字符，节省时间并避免数据过时。
- Method: 使用两层深度卷积神经网络，结合Devanagari手写字符数据集（DHCD）进行训练和测试。
- Result: 测试准确率为96.36%，训练准确率为99.55%。
- Conclusion: 该方法在手写Devanagari字符识别中表现优异，具有实际应用潜力。


### [145] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 论文提出CrisisLandMark数据集和CLOSP框架，通过文本对齐光学与SAR图像，提升遥感图像检索性能。

- Motivation: 现有文本到图像检索系统多限于RGB数据，未能充分利用多传感器（如SAR和多光谱）的物理信息。
- Method: 引入CrisisLandMark数据集，提出CLOSP框架，通过对比学习对齐光学与SAR图像到统一嵌入空间。
- Result: CLOSP在检索性能上提升54%，GeoCLOSP进一步优化地理位置相关任务。
- Conclusion: 多传感器数据与地理信息的结合对遥感档案的潜力释放至关重要。


### [146] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: 讨论了代数、数值计算和计算机视觉中一个看似不太可能的交汇点，重点是解决参数化代数方程系统的多实例问题。

- Motivation: 该问题的动机源于计算机视觉中常用的鲁棒模型拟合方法（如RanSaC），需要解决参数化代数方程系统的多实例问题。
- Method: 概述了过去5年多的工作，旨在衡量解决此类参数化系统的内在难度，并探索实用解决方案。
- Result: 研究取得了一定进展，提出了解决此类问题的实用方法。
- Conclusion: 该研究为参数化代数方程系统的多实例问题提供了新的见解和实用工具。


### [147] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SC-AGIQA的统一框架，通过文本-视觉语义约束改进AI生成图像的质量评估，解决了语义对齐和细节感知缺失的问题。

- Motivation: 现有方法在评估AI生成图像时存在语义对齐和细节感知的不足，需要更全面的评估框架。
- Method: SC-AGIQA结合了文本辅助语义对齐模块（TSAM）和频域细粒度退化感知模块（FFDPM），利用多模态大语言模型和频域分析提升评估效果。
- Result: 实验表明，SC-AGIQA在多个基准数据集上优于现有方法。
- Conclusion: SC-AGIQA通过语义约束和频域分析显著提升了AI生成图像的质量评估能力。


### [148] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal是一种无需稀疏关键点注释即可从视频重建可动画3D动物的新框架，通过密集特征网络和分层对齐策略提高效率和稳定性。

- Motivation: 现有方法依赖稀疏语义关键点，获取困难且不可靠，因此提出无需关键点注释的解决方案。
- Method: 引入密集特征网络将2D表示映射到SMAL参数，结合分层对齐策略（轮廓、部分、像素和时间线索）优化重建。
- Result: 实验表明4D-Animal优于基于模型和无模型的基线方法，生成高质量3D资产。
- Conclusion: 该方法高效稳定，适用于大规模应用，代码已开源。


### [149] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: CoralVQA是首个用于珊瑚礁分析的大规模VQA数据集，包含12,805张真实珊瑚图像和277,653个问答对，解决了领域特定注释和多维问题的挑战。

- Motivation: 珊瑚礁是重要但脆弱的生态系统，需要持续监测以支持保护。传统珊瑚图像解读依赖专业知识，而VQA技术有望提供更友好的交互方式。
- Method: 开发了半自动数据构建流程，结合海洋生物学家确保数据质量和可扩展性，构建了CoralVQA数据集。
- Result: 评估了多种先进LVLM模型，揭示了关键局限性和机会，为未来模型开发提供了基础。
- Conclusion: CoralVQA为珊瑚礁图像中的视觉-语言推理提供了全面基准，支持珊瑚保护工作的未来研究。


### [150] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: RAPNet提出了一种基于内容自适应卷积的架构，用于提高遥感图像融合的精度。

- Motivation: 传统CNN在遥感图像融合中因卷积核的均匀应用而忽略了局部内容变化，限制了性能。
- Method: RAPNet采用Receptive-field Adaptive Pansharpening Convolution (RAPConv)和Pansharpening Dynamic Feature Fusion (PAN-DFF)模块，实现空间自适应核和注意力机制。
- Result: 在公开数据集上，RAPNet在定量和定性评估中均优于现有方法。
- Conclusion: RAPNet通过自适应组件显著提升了遥感图像融合的性能。


### [151] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种新的盲人脸图像修复方法RefSTAR，通过参考图像的选择、转移和重建，解决了身份保留问题。

- Motivation: 盲人脸图像修复因未知的复杂退化和人类对脸部的敏感性而极具挑战性，现有方法在身份保留上存在问题。
- Method: 构建了参考选择模块（RefSel），设计了特征融合范式，并提出了参考图像重建机制。
- Result: 在各种骨干模型上的实验表明，该方法在身份保留和参考特征转移质量上表现优异。
- Conclusion: RefSTAR方法有效解决了盲人脸图像修复中的身份保留问题，并展示了优越的性能。


### [152] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: GT-Loc是一种新颖的检索方法，联合预测图像的拍摄时间（小时和月份）和地理位置（GPS坐标），通过共享高维特征空间对齐嵌入，并采用时间度量学习目标，优于现有时间预测方法。

- Motivation: 解决时间戳预测与地理定位的相互依赖问题，支持元数据校正、检索和数字取证等应用。
- Method: 使用图像、时间和位置的独立编码器，在共享特征空间中对齐嵌入，提出基于循环环形表面的时间度量学习目标。
- Result: GT-Loc在时间预测上优于现有方法，即使输入真实地理位置；同时在标准地理定位任务中表现优异，支持组合和基于文本的图像检索。
- Conclusion: GT-Loc通过联合优化时间与地理定位，显著提升了性能，并展示了统一嵌入空间的多功能应用潜力。


### [153] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: 提出了一种结合半监督联邦学习、室内定位导航和视觉识别的跌倒检测框架，整体准确率达99.99%，兼顾隐私保护。

- Motivation: 老龄化加剧导致跌倒风险增加，及时检测可降低医疗成本和恢复时间，但需兼顾隐私和可靠性。
- Method: 结合SF2D（半监督联邦学习）、室内定位导航系统和视觉识别系统，通过可穿戴设备和边缘设备实现多系统协同检测。
- Result: SF2D准确率99.19%，视觉识别准确率96.3%，导航系统成功率95%，综合准确率达99.99%。
- Conclusion: 该框架高效可靠且保护隐私，适用于老年人跌倒检测。


### [154] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 提出了一种基于置信度的自蒸馏方法，用于改进结肠镜息肉分割任务，减少资源消耗并提升性能。

- Motivation: 现有深度学习模型在息肉分割中表现优异但参数过多，易过拟合且泛化能力差，传统蒸馏方法资源消耗大。
- Method: 采用动态置信系数计算批次内前后迭代的损失，仅需存储前次迭代数据，无需额外计算或内存。
- Result: 在息肉分割任务中表现优于现有最优模型，并在多临床中心数据集上泛化良好。
- Conclusion: 提出的方法高效且性能优越，代码将公开。


### [155] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: 论文提出了一个全面的视网膜异常检测基准，解决了现有方法在数据、算法和评估上的局限性，并提出了NFM-DRA方法，结合了监督学习和记忆机制，实现了新的最优性能。

- Motivation: 现有的视网膜异常检测方法因缺乏公开基准而受限，导致实验设置不充分、泛化能力不足。论文旨在填补这一空白，提供更全面的数据和算法评估。
- Method: 通过分类和基准测试现有方法，发现基于异常解耦表示（DRA）的监督学习方法表现最佳，但面对未见异常时性能下降。提出NFM-DRA，结合DRA和正常特征记忆机制，提升性能。
- Result: NFM-DRA方法在基准测试中表现最优，显著减少了性能下降，成为新的SOTA。
- Conclusion: 论文提出的基准和方法为视网膜异常检测提供了更全面的评估和改进方向，推动了该领域的发展。


### [156] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 本文比较了多种在多视图Transformer中利用相机几何信息的方法，并提出了一种新的相对位置编码（PRoPE），实验表明其在多个任务中均优于其他方法。

- Motivation: 多视图计算机视觉任务中，相机几何关系对3D感知至关重要，但现有方法未能充分利用这些关系。
- Method: 比较了三种相机几何编码方法：token级光线映射编码、attention级相对位姿编码，以及新提出的PRoPE（包含完整相机参数）。
- Result: PRoPE在多个任务（新视图合成、立体深度估计、空间认知）中表现最佳，且能泛化到不同相机参数和序列长度。
- Conclusion: PRoPE是一种有效的相机几何编码方法，适用于多视图Transformer任务。


### [157] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: 该研究利用高分辨率地球观测数据和深度学习技术，首次绘制了莫桑比克全国范围内的2100万块农田边界，准确率达93%，揭示了小农户农业系统的复杂性及其与经济社会环境的关系。

- Motivation: 设计科学政策以提升小农户农业可持续性时，缺乏对农田分布和地块大小等基本系统属性的理解。
- Method: 整合高分辨率（1.5米）地球观测数据和深度迁移学习，最小化参考数据需求并提升可迁移性。
- Result: 绘制了莫桑比克全国2100万块农田的边界，准确率93%；发现农田面积普遍较小（83%小于0.5公顷），且与人口密度、森林覆盖率等密切相关。
- Conclusion: 农田面积是农业社会经济和环境结果（如粮食生产、生计、森林砍伐）的关键指标，研究为政策制定提供了重要数据支持。


### [158] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: ReVQ框架通过预训练的VAE快速训练VQ-VAE，显著降低计算成本，同时保持高质量图像重建。

- Motivation: 解决高压缩率VQ-VAE训练计算成本高的问题。
- Method: 利用预训练的VAE，结合通道多组量化和后矫正器，减少量化误差。
- Result: 在ImageNet上仅用512个token压缩图像，rFID为1.06，训练成本降低两个数量级。
- Conclusion: ReVQ在效率和重建质量间实现了优越的平衡。


### [159] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 论文介绍了EmRACE-3K数据集，用于评估和改进视觉语言模型在具身环境中的推理能力，展示了现有模型的局限性及其改进潜力。

- Motivation: 现有视觉语言模型在离线图像和视频理解任务中表现优异，但在需要在线交互和主动场景理解的具身环境中表现有限。
- Method: 通过构建包含3,000多个任务的EmRACE-3K数据集，并在Qwen2.5-VL-7B模型上进行监督学习和强化学习的微调。
- Result: 零样本设置下所有模型成功率低于20%，但微调后模型在探索、动态空间语义推理和多阶段目标执行方面显著提升。
- Conclusion: EmRACE-3K数据集为提升视觉语言模型在具身环境中的推理能力提供了有效工具，并揭示了当前模型的局限性。


### [160] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出一种自监督学习方法，利用DINOv2框架从无标签相机陷阱视频中学习黑猩猩面部嵌入，无需身份标签即可实现高精度识别。

- Motivation: 解决野生动物监测中手动识别个体的瓶颈问题，推动非侵入性种群研究的可扩展性。
- Method: 基于DINOv2框架，利用自动提取的面部图像训练Vision Transformers，完全自监督学习。
- Result: 在Bossou等挑战性基准测试中，表现优于有监督基线方法。
- Conclusion: 自监督学习在生物多样性监测中潜力巨大，为大规模种群研究提供了新途径。
## eess.IV

### [161] [Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network](https://arxiv.org/abs/2507.08855)
*Yang Ming,Jiang Shi Zhong,Zhou Su Juan*

Main category: eess.IV

TL;DR: 本文提出了一种新型深度学习算法框架，通过融合多模态医学数据（如PET、MRI、遗传数据和临床数据），利用不对称跨模态交叉注意力机制，有效提升阿尔茨海默病（AD）诊断的准确性。

- Motivation: 传统卷积神经网络和简单的特征拼接方法在多模态数据融合中效果不佳，无法充分利用互补信息，且容易丢失关键信息。
- Method: 采用不对称跨模态交叉注意力机制，融合PET、MRI、遗传和临床数据，捕捉不同模态间的关键交互特征。
- Result: 算法在测试集上达到94.88%的准确率。
- Conclusion: 不对称跨模态交叉注意力机制显著提升了AD诊断的准确性，为多模态数据融合提供了新思路。


### [162] [Interpretable Artificial Intelligence for Detecting Acute Heart Failure on Acute Chest CT Scans](https://arxiv.org/abs/2507.08952)
*Silas Nyboe Ørting,Kristina Miger,Anne Sophie Overgaard Olesen,Mikael Ploug Boesen,Michael Brun Andersen,Jens Petersen,Olav W. Nielsen,Marleen de Bruijne*

Main category: eess.IV

TL;DR: 开发了一种可解释的AI模型，用于检测胸部CT中的急性心力衰竭（AHF）放射学征象，准确性接近胸科放射科医生。

- Motivation: 由于放射科医生短缺，胸部CT扫描的解读常被延迟，而AI可作为辅助工具提高诊断精度。
- Method: 采用Boosted Trees模型，基于急性胸部CT扫描中分割的心脏和肺部结构测量数据预测AHF，并使用Shapley Additive explanations解释预测。
- Result: 模型在独立测试集上的ROC曲线下面积为0.87，误分类分析显示部分错误源于初始放射学报告的不准确。
- Conclusion: 开发的AI模型具有强判别性能，且其透明预测可辅助决策。


### [163] [VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models](https://arxiv.org/abs/2507.08982)
*Hanene F. Z. Brachemi Meftah,Wassim Hamidouche,Sid Ahmed Fezza,Olivier Déforges*

Main category: eess.IV

TL;DR: 论文提出一种针对视觉语言模型（VLMs）的隐私保护方法，通过选择性隐藏图像中的敏感区域（ROIs），防止模型泄露隐私信息，同时保持图像其余部分的语义完整性。

- Motivation: 随着视觉语言模型的广泛应用，用户隐私问题日益突出，需要一种方法防止模型处理或泄露敏感视觉信息。
- Method: 将隐私保护问题建模为对抗攻击问题，提出一种新型攻击策略，选择性隐藏图像中的ROIs，避免全局图像破坏。
- Result: 在三种先进VLMs（LLaVA、Instruct-BLIP、BLIP2-T5）上实验表明，目标ROIs的检测率降低高达98%，同时保持图像语义完整性。
- Conclusion: 该方法为多模态模型的隐私保护提供了实用工具，推动了隐私意识的应用发展。


### [164] [Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture](https://arxiv.org/abs/2507.09158)
*Sunil Munthumoduku Krishna Murthy,Kumar Rajamani,Srividya Tirunellai Rajamani,Yupei Li,Qiyang Sun,Bjoern W. Schuller*

Main category: eess.IV

TL;DR: 提出了一种改进的U-Net模型，用于自动分割X射线图像中的胸椎，提高了分割精度和效率。

- Motivation: 在脊柱活动性疾病中，准确分割和轮廓提取椎体对评估活动障碍和手术规划至关重要，但传统手动方法耗时且易出错。
- Method: 采用了一种新型的“三明治”U-Net结构，结合双激活函数，优化了椎体的分割效果。
- Result: 与基线U-Net模型相比，Dice分数提高了4.1%，显著提升了分割精度。
- Conclusion: 该自动化方法为脊柱活动性疾病的诊断和手术规划提供了更高效、准确的解决方案。


### [165] [PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution](https://arxiv.org/abs/2507.09227)
*Sanyam Jain,Bruna Neves de Freitas,Andreas Basse-OConnor,Alexandros Iosifidis,Ruben Pauwels*

Main category: eess.IV

TL;DR: 提出了一种结合扩散生成（PanoDiff）和超分辨率（SR）的方法，用于生成高质量合成牙科全景X光片。实验表明合成图像质量接近真实图像。

- Motivation: 解决医学图像数据稀缺问题，同时为AI研究和教育提供合成数据集。
- Method: 使用PanoDiff生成低分辨率种子图像，再通过SR模型提升分辨率，采用新型Transformer学习局部-全局关系。
- Result: 合成图像与真实图像的FID得分为40.69，临床专家区分准确率为68.5%。
- Conclusion: 该方法能生成高质量的合成医学图像，具有实际应用潜力。


### [166] [prNet: Data-Driven Phase Retrieval via Stochastic Refinement](https://arxiv.org/abs/2507.09608)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: 提出了一种基于Langevin动力学的新型相位检索框架，通过高效后验采样平衡失真与感知质量。

- Motivation: 传统方法过于关注像素级精度，而忽略了感知质量与失真之间的权衡。
- Method: 结合随机采样、学习去噪和基于模型的更新，提出三种复杂度递增的变体，包括Langevin推断、自适应噪声调度学习和并行重建采样。
- Result: 在多个基准测试中实现了最先进的性能，兼顾保真度和感知质量。
- Conclusion: 该框架通过理论支持的Langevin动力学有效解决了相位检索中的感知-失真权衡问题。


### [167] [I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.09609)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: 提出了一种基于图像到图像扩散框架的新相位检索方法，结合混合迭代技术和加速机制，显著提升了训练效率和重建质量。

- Motivation: 传统相位检索算法对初始化和测量噪声敏感，扩散模型在图像重建任务中表现出色，因此探索其在相位检索中的应用。
- Method: 结合混合输入-输出和误差减少方法，利用加速机制生成初始估计，再通过图像到图像扩散框架迭代优化。
- Result: 在训练效率和重建质量上均有显著提升，优于传统和现代方法。
- Conclusion: 该方法在多种应用中具有高效和有效的相位检索潜力。


### [168] [Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging](https://arxiv.org/abs/2507.09731)
*Robby Hoover,Nelly Elsayed,Zag ElSayed,Chengcheng Li*

Main category: eess.IV

TL;DR: 研究了预训练深度学习模型在X射线图像中分类骨骨折的鲁棒性，通过模拟不同设备质量条件，比较了ResNet50、VGG16和EfficientNetv2的性能。

- Motivation: 解决全球医疗资源不均问题，通过技术手段评估AI模型在不同质量X射线图像中的表现。
- Method: 使用ResNet50、VGG16和EfficientNetv2三种预训练模型，通过逐步添加噪声模拟图像质量退化，评估其对骨骨折分类的影响。
- Result: 建立了评估AI模型性能退化的方法框架，提供了不同预训练模型在不同噪声条件下的鲁棒性和泛化性数据。
- Conclusion: 为医疗影像技术在不同环境中的应用提供了实用见解，帮助应对现实世界中的图像质量挑战。


### [169] [AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)](https://arxiv.org/abs/2507.09759)
*Abdul Manaf,Nimra Mughal*

Main category: eess.IV

TL;DR: 该研究提出了一种基于机器学习的儿科胸部肺炎分类系统，通过CNN模型结合数据增强和GAN生成图像，提高了肺炎诊断的准确性和效率。

- Motivation: 肺炎是五岁以下儿童死亡的主要原因，需要准确的胸部X光诊断。研究旨在帮助医疗专业人员通过深度学习技术提高诊断效率。
- Method: 使用5,863张标记的儿童胸部X光图像训练CNN模型，采用数据增强和GAN生成合成图像以解决数据不足和类别不平衡问题。
- Result: 系统通过结合原始、增强和GAN生成的数据实现了最佳性能，并通过Flask应用实现了实时分类。
- Conclusion: 研究表明深度学习和GAN在儿科肺炎分类中具有潜力，尤其适用于资源有限的临床环境。


### [170] [Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction](https://arxiv.org/abs/2507.09872)
*Shengjie Liu,Lu Zhang,Siqin Wang*

Main category: eess.IV

TL;DR: 提出了一种物理引导的深度学习框架，用于整合高时间分辨率和低空间分辨率数据，以重建高时空分辨率的温度数据。

- Motivation: 地球观测中空间和时间分辨率的权衡是关键问题，尤其是温度数据的需求。现有技术无法同时满足高时空分辨率，且云层覆盖加剧了数据缺口。
- Method: 使用卷积神经网络，结合年度温度周期和线性项，将粗分辨率的地球系统模型输出放大为卫星观测的细尺度温度值。
- Result: 通过GOES-16和Landsat卫星数据验证，框架在四种数据集上实现了有效的温度重建。
- Conclusion: 该框架为生成高分辨率温度数据提供了新方法，适用于全球范围和全天候条件。


### [171] [Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images](https://arxiv.org/abs/2507.09898)
*Alireza Golkarieha,Kiana Kiashemshakib,Sajjad Rezvani Boroujenic,Nasibeh Asadi Isakand*

Main category: eess.IV

TL;DR: 研究探讨了结合不同CNN骨干网络的U-Net架构在胸部CT图像中自动检测和分割肺癌的效果，结果显示U-Net与ResNet50结合在癌症分割中表现最佳，而U-Net与Xception结合的分类模型准确率高达99.1%。

- Motivation: 解决临床环境中对准确诊断工具的需求，提升肺癌检测和分割的自动化水平。
- Method: 使用CLAHE预处理832张胸部CT图像，构建U-Net模型（ResNet50、VGG16、Xception骨干网络），并评估CNN分类器及混合模型（结合传统机器学习）。
- Result: U-Net+ResNet50在癌症分割中表现最佳（Dice:0.9495，准确率:0.9735），U-Net+Xception分类模型准确率达99.1%。
- Conclusion: 结合U-Net与先进CNN骨干网络为肺癌CT扫描的分割和分类提供了高效方法，支持早期诊断和临床决策。


### [172] [IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution](https://arxiv.org/abs/2507.09923)
*Sejin Park,Sangmin Lee,Kyong Hwan Jin,Seung-Won Jung*

Main category: eess.IV

TL;DR: 提出了一种名为IM-LUT的新型框架，通过混合多种插值函数实现任意尺度超分辨率（ASISR），兼顾效率和性能。

- Motivation: 现有基于查找表（LUT）的方法仅适用于固定尺度超分辨率，而现有ASISR方法计算成本高、内存需求大。
- Method: 提出IM-LUT框架，通过IM-Net预测插值函数的混合权重，并利用LUT替换计算密集型操作。
- Result: 在多个基准数据集上，IM-LUT在图像质量和效率之间取得了优越的平衡。
- Conclusion: IM-LUT是一种适用于资源受限应用的高效解决方案。


### [173] [A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion](https://arxiv.org/abs/2507.09966)
*Mingda Zhang*

Main category: eess.IV

TL;DR: 提出一种多级融合架构，结合像素、特征和语义信息，提升脑肿瘤MRI分割精度。

- Motivation: 脑肿瘤形态异质性和复杂空间关系使自动分割具有挑战性，现有方法未充分利用医学报告中的语义知识。
- Method: 集成3D U-Net和CLIP模型，通过3D-2D语义桥接、跨模态语义引导和语义注意力机制实现多级融合。
- Result: 在BraTS 2020数据集上，Dice系数达0.8567，比传统3D U-Net提升4.8%，增强肿瘤区域提升7.3%。
- Conclusion: 多级融合架构有效整合语义信息，显著提升脑肿瘤分割性能。


### [174] [Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)](https://arxiv.org/abs/2507.09995)
*Guohao Huo,Ruiting Dai,Hao Tang*

Main category: eess.IV

TL;DR: 提出EdgeIMLocSys系统，结合GMLN-BTS网络，通过人类反馈持续学习优化脑肿瘤分割模型，显著提升模型泛化能力和边界精度。

- Motivation: 解决MRI扫描仪成像质量差异对模型泛化性的挑战，提升脑肿瘤分割的临床适用性。
- Method: 集成GMLN-BTS网络，包括M2AE编码器、G2MCIM模块和VRUM模块，通过轻量化和多模态交互优化分割性能。
- Result: 在BraTS2017数据集上Dice分数达85.1%，参数量仅4.58M，显著优于主流轻量方法。
- Conclusion: 实现了高精度、低资源消耗的脑肿瘤分割，适合资源受限的临床环境部署。


### [175] [DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology](https://arxiv.org/abs/2507.10250)
*Ashkan Shakarami,Lorenzo Nicole,Rocco Cappellesso,Angelo Paolo Dei Tos,Stefano Ghidoni*

Main category: eess.IV

TL;DR: DepViT-CAD是一种基于MAViT（多注意力视觉变换器）的可部署AI系统，用于多类癌症诊断，通过大规模真实世界验证，诊断敏感性达94.11%和92%。

- Motivation: 提高癌症诊断的准确性和及时性，支持临床决策。
- Method: 使用MAViT（多注意力视觉变换器）从1008张全切片图像中提取细粒度形态学特征，涵盖11种诊断类别。
- Result: 在两个独立队列中验证，诊断敏感性分别为94.11%和92%。
- Conclusion: DepViT-CAD提供了一种稳健且可扩展的AI辅助癌症诊断方法，代码将公开以支持透明性和可重复性。
## q-bio.NC

### [176] [CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience](https://arxiv.org/abs/2507.09024)
*Marie St-Laurent,Basile Pinsard,Oliver Contier,Elizabeth DuPre,Katja Seeliger,Valentina Borghesani,Julie A. Boyle,Lune Bellec,Martin N. Hebart*

Main category: q-bio.NC

TL;DR: CNeuroMod-THINGS整合了THINGS和CNeuroMod项目，提供了一个大规模、密集采样的fMRI数据集，用于建模人类视觉体验。

- Motivation: 满足神经AI建模对大规模神经影像数据的需求，通过结合现有资源扩展数据采集能力。
- Method: 利用THINGS的注释图像集和CNeuroMod的fMRI数据采集方法，四名参与者完成了33-36次连续识别任务。
- Result: 报告了行为学和神经影像学指标，展示了数据的高质量。
- Conclusion: CNeuroMod-THINGS通过整合资源，增强了建模人类视觉体验的能力。


### [177] [Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding](https://arxiv.org/abs/2507.09513)
*Yanchen Wang,Han Yu,Ari Blau,Yizi Zhang,The International Brain Laboratory,Liam Paninski,Cole Hurwitz,Matt Whiteway*

Main category: q-bio.NC

TL;DR: BEAST是一种新型的自监督预训练框架，通过结合掩码自编码和时间对比学习，利用未标记视频数据提升神经行为分析性能。

- Motivation: 现代神经科学研究需要通过行为理解大脑，但现有视频分析方法依赖大量标记数据和专用模型，限制了其应用。
- Method: BEAST结合掩码自编码和时间对比学习，预训练实验专用的视觉变换器，以利用未标记视频数据。
- Result: 在多种物种中，BEAST在行为特征提取、姿态估计和动作分割任务中表现优异，尤其在标记数据稀缺的场景下。
- Conclusion: BEAST为神经行为分析提供了一个强大且通用的骨干模型，显著提升了行为分析的效率和适用范围。
## cs.IT

### [178] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Main category: cs.IT

TL;DR: 论文提出了一种轻量级深度学习框架，用于XL-MIMO系统中的高效级联信道估计，旨在降低计算复杂度并适用于资源受限的边缘设备。

- Motivation: 6G等下一代无线技术对超高数据速率、低延迟和增强连接性提出了严格要求，但XL-MIMO和RIS的潜力依赖于准确的信道状态信息（CSI）。现有估计模型在XL-MIMO系统中的可扩展性和实际部署受限。
- Method: 通过利用信道中的空间相关性，提出了一种基于分块的训练机制，将输入降维为分块级表示，同时保留关键信息，实现大规模系统的可扩展训练。
- Result: 仿真结果表明，该框架显著提高了估计精度并降低了计算复杂度，且不受XL-MIMO系统中天线和RIS元件数量增加的影响。
- Conclusion: 该轻量级框架为XL-MIMO系统中的高效信道估计提供了可行方案，适合资源受限的边缘设备部署。
## eess.AS

### [179] [Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction](https://arxiv.org/abs/2507.09834)
*Shu-wen Yang,Byeonggeun Kim,Kuan-Po Huang,Qingming Tang,Huy Phan,Bo-Ru Lu,Harsha Sundar,Shalini Ghosh,Hung-yi Lee,Chieh-Chi Kao,Chao Wang*

Main category: eess.AS

TL;DR: 论文提出了一种基于因果语言模型（LM）的音频生成方法，利用逐标记扩散建模连续值标记的分布，显著优于之前的离散方法，并在参数更少的情况下达到与SOTA扩散模型相当的性能。

- Motivation: 扩展自回归Transformer解码器范式到音频生成领域，解决音频连续性的独特挑战。
- Method: 采用逐标记扩散建模连续值标记分布，并提出新颖的掩码下一标记预测任务。
- Result: 在AudioCaps上，FAD和KL散度分别提升20%和40%，掩码任务进一步带来41%和33%的FAD提升，参数更少。
- Conclusion: 该方法在音频生成中高效且性能优越，为连续数据建模提供了新思路。
## cs.LG

### [180] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 提出了一种名为WRCor的无训练代理方法，用于加速神经架构搜索（NAS），通过响应相关性矩阵评估架构的表达性和泛化性。

- Motivation: 解决现有零样本NAS方法在有效性、稳定性和通用性方面的不足。
- Method: 使用加权响应相关性（WRCor）作为训练免费代理，通过计算不同输入样本的响应相关性矩阵来评估架构。
- Result: WRCor及其投票代理在代理评估中表现更高效，NAS算法在不同搜索空间中优于现有方法。
- Conclusion: WRCor是一种高效且通用的零样本NAS代理方法，能在短时间内发现高性能架构。


### [181] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 本文提出了一种系统框架，通过表示引导改进扩散模型，引入两种新策略并展示了在图像、蛋白质序列和分子生成任务中的优越性能。

- Motivation: 通过将扩散模型的内部表示与预训练模型对齐，提高生成质量。
- Method: 1. 使用目标表示对示例进行配对学习；2. 设计平衡表示学习和数据生成的最优训练课程。
- Result: 在ImageNet 256×256基准测试中，训练速度比原始SiT-XL快23.3倍，比REPA快4倍。
- Conclusion: 表示引导显著提升了扩散模型的性能和训练效率。


### [182] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: 论文提出了一种递归MDN（R-MDN）层，用于在持续学习中消除混杂变量的影响，确保特征表示不变性。

- Motivation: 混杂变量会导致虚假相关性和预测偏差，现有方法（如MDN）在持续学习中难以有效处理。
- Method: 引入R-MDN层，通过递归最小二乘法动态调整特征分布，适应数据和混杂变量的变化。
- Result: 实验表明，R-MDN在静态学习和持续学习中均能减少混杂变量影响，提升预测公平性。
- Conclusion: R-MDN是一种通用且有效的方法，适用于多种深度学习架构，能持续适应数据变化。


### [183] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出了一种名为“warm-start model”的确定性模型，通过提供更好的初始点加速条件生成，显著减少生成过程所需的步骤。

- Motivation: 传统迭代生成模型（如扩散模型和流匹配）生成高保真样本需要大量计算步骤，效率低下。
- Method: 使用条件化的高斯分布N(mu, sigma)作为初始点，而非传统的N(0, I)先验，并通过条件归一化技巧使其兼容标准生成模型。
- Result: 在图像修复等任务中，仅需11次函数评估即可达到与1000步DDPM基线竞争的结果。
- Conclusion: warm-start model显著提升了生成效率，且无需修改现有模型即可与其他高效采样技术结合。


### [184] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ是一种结合低秩近似和混合精度量化的新方法，用于在资源受限的边缘设备上部署Transformer网络，通过两阶段优化实现性能提升。

- Motivation: 在边缘设备上部署Transformer网络面临资源限制的挑战，需要高效的压缩技术。
- Method: MLoRQ采用两阶段优化：层内优化筛选压缩方案，层间优化分配位宽和秩；可选步骤通过自适应舍入减少误差。
- Result: 在视觉Transformer任务中，MLoRQ性能提升高达15%。
- Conclusion: MLoRQ是一种高效且兼容性强的压缩方法，适用于边缘设备部署。


### [185] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: 提出了一种基于边界条件数据学习物理定律的通用物理模拟基础AI模型，无需预先编码方程。

- Motivation: 传统方法如PINNs和有限差分法需要显式数学方程，限制了通用性和发现潜力。本文旨在通过条件生成问题重新构想计算物理。
- Method: 采用草图引导的扩散变换器方法，通过空间边界条件合成稳态解，利用增强的扩散变换器架构实现边界到平衡的直接映射。
- Result: 模型直接生成稳态解，SSIM > 0.8，边界精度达亚像素级，支持通过LRP分析发现物理关系。
- Conclusion: 该工作实现了从AI加速物理到AI发现物理的范式转变，建立了首个真正通用的物理模拟框架。


### [186] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: 提出了一种对抗性表示学习方法，通过引入焦点熵来减少信息泄露，同时保持高预测能力。

- Motivation: 在保护用户隐私的同时，学习具有高预测能力的表示。
- Method: 使用对抗性表示学习方法和焦点熵（focal entropy）来减少敏感信息泄露。
- Result: 在多个基准测试中验证了方法的可行性，结果显示在适度隐私泄露的情况下仍保持高目标效用。
- Conclusion: 该方法在隐私保护和预测能力之间取得了平衡，具有实际应用潜力。


### [187] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: CLA是一种自监督学习策略，用于在线持续学习，通过对齐当前与过去的表征来减少遗忘，提升训练收敛速度，并在相同计算预算下优于现有方法。

- Motivation: 解决在线持续学习中数据以小批量到达、计算预算固定且无任务边界时，自监督学习技术不足的问题。
- Method: 提出Continual Latent Alignment (CLA)，通过对齐当前模型与过去学习的表征来缓解遗忘。
- Result: CLA加速了在线场景下的训练收敛，优于现有方法；作为预训练协议时，早期使用CLA比完整i.i.d.预训练表现更好。
- Conclusion: CLA是一种有效的在线持续学习自监督策略，兼具高效性和性能优势。
## cs.RO

### [188] [Multimodal HD Mapping for Intersections by Intelligent Roadside Units](https://arxiv.org/abs/2507.08903)
*Zhongzhang Chen,Miao Fan,Shengtong Xu,Mengmeng Yang,Kun Jiang,Xiangzeng Liu,Haoyi Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种基于摄像头-LiDAR融合的新框架，利用智能路边单元（IRU）生成高清语义地图，并发布了RS-seq数据集，用于多模态互补性研究。

- Motivation: 传统车辆方法在复杂交叉路口的高清语义映射中存在遮挡和视角限制问题，需通过多模态融合提升性能。
- Method: 采用两阶段融合框架，结合摄像头高分辨率纹理和LiDAR精确几何数据，进行特征提取和语义整合。
- Result: 多模态方法在RS-seq数据集上表现优于单模态，语义分割mIoU分别比图像和点云方法提升4%和18%。
- Conclusion: 该研究为基于IRU的高清语义映射提供了基准方法和数据集，支持基础设施辅助自动驾驶系统的未来研究。


### [189] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: 首次在真实世界中实现了一种基于蘑菇体（MB）架构的视觉归巢系统，应用于小型自动驾驶汽车机器人，验证了其在不同实验中的有效性。

- Motivation: 受蚂蚁在少量感官输入和学习行走后实现稳健视觉归巢的启发，探索蘑菇体模型在视觉归巢中的应用。
- Method: 采用侧向化MB架构，通过角路径积分信号分类全景视图，分四个实验验证：模拟巢动态、解耦学习行走后的归巢、随机行走归巢、以及通过第五MB输出神经元实现精确停止。
- Result: 系统在自然户外环境中实现了稳健的视觉归巢，资源占用低（内存<9 kB），运行频率为8 Hz。
- Conclusion: 该系统为自主视觉归巢提供了一种生物启发的资源高效解决方案，功能上类似于机器人中的基于路点的位置控制。


### [190] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER是一个概率框架，通过双阶段（导航和操作）估计人类意图，提升人机协作效率。

- Motivation: 准确推断人类意图可以避免人机协作中的冲突，同时不限制人类控制。
- Method: GUIDER采用双阶段框架：导航阶段结合控制器速度和占用网格；操作阶段结合显著性检测和几何抓取可行性测试。
- Result: 在25次试验中，导航稳定性达93-100%，操作稳定性达94-100%，优于基线方法。
- Conclusion: GUIDER在移动操作任务中显著提升了意图推断的准确性和效率。


### [191] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: SC-ADAS是一个结合生成式AI的模块化框架，通过自然语言交互提升ADAS的灵活性和适应性。

- Motivation: 当前ADAS缺乏场景理解和自然语言交互能力，限制了其在动态环境中的适应性。
- Method: 集成大型语言模型、视觉到文本解释和结构化函数调用，实现实时、可解释的驾驶员辅助。
- Result: 在CARLA模拟器中验证了SC-ADAS的可行性，展示了场景感知和多轮对话的能力。
- Conclusion: SC-ADAS为下一代智能驾驶辅助系统提供了可行的解决方案。
## cs.DB

### [192] [TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing](https://arxiv.org/abs/2507.09448)
*Pramod Chunduri,Yao Lu,Joy Arulraj*

Main category: cs.DB

TL;DR: Tracer是一种新型VDBMS，通过自适应查询处理框架高效处理Re-ID查询，解决了现有系统Spactula的局限性，并在性能上显著优于现有技术。

- Motivation: 现有系统Spactula在大型摄像头网络中时空过滤精度有限，且不支持自适应查询处理，无法满足高召回率需求的关键视频分析应用。
- Method: Tracer采用循环网络建模长期历史相关性，选择最优摄像头处理；引入概率自适应搜索模型，动态更新采样概率以加速查询。
- Result: Tracer在多样化数据集上平均性能优于现有最佳系统3.9倍。
- Conclusion: Tracer通过自适应框架和合成基准解决了Re-ID任务中的关键问题，显著提升了性能。
## cs.MM

### [193] [ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual Event Localization](https://arxiv.org/abs/2507.09945)
*Huilai Li,Yonghao Dang,Ying Xing,Yiming Wang,Jianqin Yin*

Main category: cs.MM

TL;DR: 论文提出了一种名为ESG-Net的方法，通过多阶段语义引导和多事件关系建模，解决了密集视听事件定位中的模态语义鸿沟和事件相关性不足的问题。

- Motivation: 现有方法在中间层缺乏跨模态语义桥接，导致模态语义鸿沟，且未考虑事件间的相关性，限制了模型在复杂场景中的表现。
- Method: ESG-Net包含早期语义交互模块（ESI）和依赖专家混合模块（MoDE），分别实现多阶段语义引导和多事件依赖关系建模。
- Result: 实验表明，该方法显著优于现有技术，同时大幅减少了参数和计算负担。
- Conclusion: ESG-Net通过分层语义理解和自适应事件依赖提取，有效提升了密集视听事件定位的性能。


### [194] [LayLens: Improving Deepfake Understanding through Simplified Explanations](https://arxiv.org/abs/2507.10066)
*Abhijeet Narang,Parul Gupta,Liuyijia Su,Abhinav Dhall*

Main category: cs.MM

TL;DR: LayLens是一个帮助用户理解深度伪造的工具，通过三阶段流程（检测、简化解释、视觉重建）提供技术与非技术解释，提升用户识别深度伪造的能力。

- Motivation: 解决现有深度伪造检测工具依赖技术术语的问题，降低用户理解门槛。
- Method: 采用三阶段流程：1) 可解释的深度伪造检测；2) 自然语言简化技术解释；3) 视觉重建原始图像。
- Result: 用户研究表明，简化解释显著提升清晰度并降低认知负担，用户识别深度伪造的信心增强。
- Conclusion: LayLens推动了透明、可信且用户友好的深度伪造取证工具的发展。
## cs.GR

### [195] [RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling](https://arxiv.org/abs/2507.09441)
*Ankit Sanjyal*

Main category: cs.GR

TL;DR: 论文提出了一种自适应分类器自由引导（CFG）调度方法，解决扩散模型在高分辨率图像合成中的能量不稳定性和引导伪影问题，显著提升了图像质量。

- Motivation: 高分辨率图像合成中，扩散模型常因能量不稳定和引导伪影导致视觉质量下降，需要一种更稳定的引导方法。
- Method: 通过分析采样过程中的潜在能量景观，提出自适应CFG调度策略，动态调整引导强度，并引入能量感知调度方法。
- Result: 采用线性递减CFG调度的DPM++ 2M模型表现最佳，稳定性得分（0.9998）和一致性指标（0.9873）优于固定引导方法，生成图像更清晰且伪影更少。
- Conclusion: 能量分析框架为理解和改进扩散模型行为提供了有力工具，自适应CFG调度显著提升了图像合成的稳定性和质量。


### [196] [CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design](https://arxiv.org/abs/2507.09792)
*Prashant Govindarajan,Davide Baldelli,Jay Pathak,Quentin Fournier,Sarath Chandar*

Main category: cs.GR

TL;DR: 论文提出了一种利用大型语言模型（LLMs）自动化CAD设计的方法，通过微调代码-LLMs生成基于自然语言描述的CAD序列，并引入新的几何和拓扑指标评估生成质量。

- Motivation: CAD建模目前仍依赖耗时的手动操作，现有方法未能充分利用LLMs的潜力。本文旨在探索LLMs在CAD设计中的应用。
- Method: 构建了一个包含17万多个CAD模型的数据集，使用GPT-4.1生成高质量描述，并微调代码-LLMs以从自然语言生成JSON格式的CAD序列。
- Result: 实验表明，该方法能有效自动化CAD设计，显著提升设计效率，并通过新指标验证了生成模型的结构质量。
- Conclusion: CADmium方法展示了LLMs在CAD设计中的潜力，为自动化设计提供了新思路。


### [197] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Nießner,Derek Bradley*

Main category: cs.GR

TL;DR: 提出了一种基于局部面部表情和3D高斯抛光的超高清3D头部化身生成方法，实现了实时高保真动画。

- Motivation: 解决在近距离渲染数字化身时表现面部微特征和表情的挑战，捕捉人类头部的细节和动态。
- Method: 结合局部表情特征与3D高斯抛光，利用基于块的几何3D面部模型提取表情，并通过Scaffold-GS的锚点动态合成3D高斯。
- Result: 实现了视觉自然的实时动画，涵盖多样面部表情和风格，性能达到最新水平。
- Conclusion: 通过局部表情与3D高斯的结合，ScaffoldAvatar在高质量和实时性上表现优异。
