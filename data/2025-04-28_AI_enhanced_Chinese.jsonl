{"id": "2504.17804", "pdf": "https://arxiv.org/pdf/2504.17804", "abs": "https://arxiv.org/abs/2504.17804", "authors": ["Andrew Kiruluta"], "title": "Spectral Dictionary Learning for Generative Image Modeling", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel spectral generative model for image synthesis that departs\nradically from the common variational, adversarial, and diffusion paradigms. In\nour approach, images, after being flattened into one-dimensional signals, are\nreconstructed as linear combinations of a set of learned spectral basis\nfunctions, where each basis is explicitly parameterized in terms of frequency,\nphase, and amplitude. The model jointly learns a global spectral dictionary\nwith time-varying modulations and per-image mixing coefficients that quantify\nthe contributions of each spectral component. Subsequently, a simple\nprobabilistic model is fitted to these mixing coefficients, enabling the\ndeterministic generation of new images by sampling from the latent space. This\nframework leverages deterministic dictionary learning, offering a highly\ninterpretable and physically meaningful representation compared to methods\nrelying on stochastic inference or adversarial training. Moreover, the\nincorporation of frequency-domain loss functions, computed via the short-time\nFourier transform (STFT), ensures that the synthesized images capture both\nglobal structure and fine-grained spectral details, such as texture and edge\ninformation. Experimental evaluations on the CIFAR-10 benchmark demonstrate\nthat our approach not only achieves competitive performance in terms of\nreconstruction quality and perceptual fidelity but also offers improved\ntraining stability and computational efficiency. This new type of generative\nmodel opens up promising avenues for controlled synthesis, as the learned\nspectral dictionary affords a direct handle on the intrinsic frequency content\nof the images, thus providing enhanced interpretability and potential for novel\napplications in image manipulation and analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5149\u8c31\u751f\u6210\u6a21\u578b\u7684\u65b0\u578b\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u5b66\u4e60\u7684\u5149\u8c31\u57fa\u51fd\u6570\u91cd\u6784\u56fe\u50cf\uff0c\u5177\u6709\u9ad8\u89e3\u91ca\u6027\u548c\u7269\u7406\u610f\u4e49\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u6a21\u578b\uff08\u5982\u53d8\u5206\u3001\u5bf9\u6297\u548c\u6269\u6563\u6a21\u578b\uff09\u4f9d\u8d56\u968f\u673a\u63a8\u7406\u6216\u5bf9\u6297\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u786e\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5c06\u56fe\u50cf\u5c55\u5e73\u4e3a\u4e00\u7ef4\u4fe1\u53f7\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u5149\u8c31\u57fa\u51fd\u6570\uff08\u53c2\u6570\u5316\u4e3a\u9891\u7387\u3001\u76f8\u4f4d\u548c\u632f\u5e45\uff09\u91cd\u6784\u56fe\u50cf\uff0c\u5e76\u62df\u5408\u6df7\u5408\u7cfb\u6570\u7684\u6982\u7387\u6a21\u578b\u3002\u4f7f\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff08STFT\uff09\u8ba1\u7b97\u9891\u57df\u635f\u5931\u3002", "result": "\u5728CIFAR-10\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5728\u91cd\u5efa\u8d28\u91cf\u3001\u611f\u77e5\u4fdd\u771f\u5ea6\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u63a7\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5149\u8c31\u5b57\u5178\u7684\u76f4\u63a5\u64cd\u4f5c\u589e\u5f3a\u4e86\u56fe\u50cf\u9891\u7387\u5185\u5bb9\u7684\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u548c\u5206\u6790\u7684\u65b0\u5e94\u7528\u3002"}}
{"id": "2504.17810", "pdf": "https://arxiv.org/pdf/2504.17810", "abs": "https://arxiv.org/abs/2504.17810", "authors": ["Yuxin Yao", "Yan Zhang", "Zhening Huang", "Joan Lasenby"], "title": "SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 4 figures, Accepted by CVPR workshop", "summary": "Dynamic videos with small baseline motions are ubiquitous in daily life,\nespecially on social media. However, these videos present a challenge to\nexisting pose estimation frameworks due to ambiguous features, drift\naccumulation, and insufficient triangulation constraints. Gaussian splatting,\nwhich maintains an explicit representation for scenes, provides a reliable\nnovel view rasterization when the viewpoint change is small. Inspired by this,\nwe propose SmallGS, a camera pose estimation framework that is specifically\ndesigned for small-baseline videos. SmallGS optimizes sequential camera poses\nusing Gaussian splatting, which reconstructs the scene from the first frame in\neach video segment to provide a stable reference for the rest. The temporal\nconsistency of Gaussian splatting within limited viewpoint differences reduced\nthe requirement of sufficient depth variations in traditional camera pose\nestimation. We further incorporate pretrained robust visual features, e.g.\nDINOv2, into Gaussian splatting, where high-dimensional feature map rendering\nenhances the robustness of camera pose estimation. By freezing the Gaussian\nsplatting and optimizing camera viewpoints based on rasterized features,\nSmallGS effectively learns camera poses without requiring explicit feature\ncorrespondences or strong parallax motion. We verify the effectiveness of\nSmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves\nimpressive accuracy in camera pose estimation compared to MonST3R and\nDORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at:\nhttps://yuxinyao620.github.io/SmallGS", "AI": {"tldr": "SmallGS\u662f\u4e00\u4e2a\u9488\u5bf9\u5c0f\u57fa\u7ebf\u89c6\u9891\u8bbe\u8ba1\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\u76f8\u673a\u59ff\u6001\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c0f\u57fa\u7ebf\u89c6\u9891\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u5e38\u89c1\uff0c\u4f46\u73b0\u6709\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u56e0\u7279\u5f81\u6a21\u7cca\u3001\u6f02\u79fb\u7d2f\u79ef\u548c\u4e09\u89d2\u7ea6\u675f\u4e0d\u8db3\u800c\u96be\u4ee5\u5904\u7406\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u573a\u666f\u4f5c\u4e3a\u53c2\u8003\uff0c\u7ed3\u5408DINOv2\u7b49\u9884\u8bad\u7ec3\u7279\u5f81\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u51bb\u7ed3\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\u76f8\u673a\u89c6\u89d2\u3002", "result": "\u5728TUM-Dynamics\u5e8f\u5217\u4e2d\uff0cSmallGS\u5728\u5c0f\u57fa\u7ebf\u89c6\u9891\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u4e0a\u8868\u73b0\u4f18\u4e8eMonST3R\u548cDORID-SLAM\u3002", "conclusion": "SmallGS\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u548c\u9884\u8bad\u7ec3\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u57fa\u7ebf\u89c6\u9891\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\u3002"}}
{"id": "2504.17812", "pdf": "https://arxiv.org/pdf/2504.17812", "abs": "https://arxiv.org/abs/2504.17812", "authors": ["Sara Sabour"], "title": "Object Learning and Robust 3D Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "PhD Thesis", "summary": "In this thesis we discuss architectural designs and training methods for a\nneural network to have the ability of dissecting an image into objects of\ninterest without supervision. The main challenge in 2D unsupervised object\nsegmentation is distinguishing between foreground objects of interest and\nbackground. FlowCapsules uses motion as a cue for the objects of interest in 2D\nscenarios. The last part of this thesis focuses on 3D applications where the\ngoal is detecting and removal of the object of interest from the input images.\nIn these tasks, we leverage the geometric consistency of scenes in 3D to detect\nthe inconsistent dynamic objects. Our transient object masks are then used for\ndesigning robust optimization kernels to improve 3D modelling in a casual\ncapture setup. One of our goals in this thesis is to show the merits of\nunsupervised object based approaches in computer vision. Furthermore, we\nsuggest possible directions for defining objects of interest or foreground\nobjects without requiring supervision. Our hope is to motivate and excite the\ncommunity into further exploring explicit object representations in image\nunderstanding tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u65e0\u76d1\u7763\u795e\u7ecf\u7f51\u7edc\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u56fe\u50cf\u4e2d\u76ee\u6807\u5206\u5272\uff0c\u5e76\u6269\u5c55\u52303D\u573a\u666f\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u4e0e\u79fb\u9664\u3002", "motivation": "\u7814\u7a76\u65e0\u76d1\u7763\u76ee\u6807\u5206\u5272\u65b9\u6cd5\uff0c\u89e3\u51b32D\u548c3D\u573a\u666f\u4e2d\u524d\u666f\u4e0e\u80cc\u666f\u7684\u533a\u5206\u95ee\u9898\uff0c\u63a8\u52a8\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u65e0\u76d1\u7763\u76ee\u6807\u8868\u793a\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528FlowCapsules\u4ee5\u8fd0\u52a8\u4e3a\u7ebf\u7d22\u5206\u52722D\u76ee\u6807\uff1b\u57283D\u4e2d\u5229\u7528\u51e0\u4f55\u4e00\u81f4\u6027\u68c0\u6d4b\u52a8\u6001\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6838\u6539\u8fdb\u5efa\u6a21\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u76d1\u77632D\u76ee\u6807\u5206\u5272\u548c3D\u52a8\u6001\u76ee\u6807\u68c0\u6d4b\u4e0e\u79fb\u9664\uff0c\u5c55\u793a\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "conclusion": "\u65e0\u76d1\u7763\u76ee\u6807\u8868\u793a\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u66f4\u591a\u65e0\u76d1\u7763\u5b9a\u4e49\u76ee\u6807\u7684\u65b9\u6cd5\u3002"}}
{"id": "2504.17813", "pdf": "https://arxiv.org/pdf/2504.17813", "abs": "https://arxiv.org/abs/2504.17813", "authors": ["Dileepa Pitawela", "Gustavo Carneiro", "Hsiang-Ting Chen"], "title": "CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025", "summary": "In ordinal classification, misclassifying neighboring ranks is common, yet\nthe consequences of these errors are not the same. For example, misclassifying\nbenign tumor categories is less consequential, compared to an error at the\npre-cancerous to cancerous threshold, which could profoundly influence\ntreatment choices. Despite this, existing ordinal classification methods do not\naccount for the varying importance of these margins, treating all neighboring\nclasses as equally significant. To address this limitation, we propose CLOC, a\nnew margin-based contrastive learning method for ordinal classification that\nlearns an ordered representation based on the optimization of multiple margins\nwith a novel multi-margin n-pair loss (MMNP). CLOC enables flexible decision\nboundaries across key adjacent categories, facilitating smooth transitions\nbetween classes and reducing the risk of overfitting to biases present in the\ntraining data. We provide empirical discussion regarding the properties of MMNP\nand show experimental results on five real-world image datasets (Adience,\nHistorical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic\nRetinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset\nsimulating clinical decision bias. Our results demonstrate that CLOC\noutperforms existing ordinal classification methods and show the\ninterpretability and controllability of CLOC in learning meaningful, ordered\nrepresentations that align with clinical and practical needs.", "AI": {"tldr": "CLOC\u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8fb9\u754c\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6709\u5e8f\u5206\u7c7b\uff0c\u901a\u8fc7\u4f18\u5316\u591a\u4e2a\u8fb9\u754c\u6765\u5b66\u4e60\u6709\u5e8f\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u672a\u8003\u8651\u4e0d\u540c\u8fb9\u754c\u91cd\u8981\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u6709\u5e8f\u5206\u7c7b\u4e2d\uff0c\u76f8\u90bb\u7c7b\u522b\u7684\u9519\u8bef\u5206\u7c7b\u540e\u679c\u4e0d\u540c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u8003\u8651\u8fd9\u4e00\u70b9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u7075\u6d3b\u5904\u7406\u5173\u952e\u8fb9\u754c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCLOC\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u8fb9\u754cn\u5bf9\u635f\u5931\uff08MMNP\uff09\u5b66\u4e60\u6709\u5e8f\u8868\u793a\uff0c\u4f18\u5316\u591a\u4e2a\u8fb9\u754c\u4ee5\u5b9e\u73b0\u7075\u6d3b\u51b3\u7b56\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0cCLOC\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "CLOC\u901a\u8fc7\u4f18\u5316\u5173\u952e\u8fb9\u754c\uff0c\u5b66\u4e60\u5230\u7b26\u5408\u4e34\u5e8a\u9700\u6c42\u7684\u6709\u5e8f\u8868\u793a\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2504.17929", "pdf": "https://arxiv.org/pdf/2504.17929", "abs": "https://arxiv.org/abs/2504.17929", "authors": ["Ayesha Siddique", "Khurram Khalil", "Khaza Anuarul Hoque"], "title": "ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing", "categories": ["cs.AI", "cs.AR"], "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), June 30th - July 5th, 2025 in Rome, Italy", "summary": "Explainable artificial intelligence (XAI) enhances AI system transparency by\nframing interpretability as an optimization problem. However, this approach\noften necessitates numerous iterations of computationally intensive operations,\nlimiting its applicability in real-time scenarios. While recent research has\nfocused on XAI hardware acceleration on FPGAs and TPU, these methods do not\nfully address energy efficiency in real-time settings. To address this\nlimitation, we propose XAIedge, a novel framework that leverages approximate\ncomputing techniques into XAI algorithms, including integrated gradients, model\ndistillation, and Shapley analysis. XAIedge translates these algorithms into\napproximate matrix computations and exploits the synergy between convolution,\nFourier transform, and approximate computing paradigms. This approach enables\nefficient hardware acceleration on TPU-based edge devices, facilitating faster\nreal-time outcome interpretations. Our comprehensive evaluation demonstrates\nthat XAIedge achieves a $2\\times$ improvement in energy efficiency compared to\nexisting accurate XAI hardware acceleration techniques while maintaining\ncomparable accuracy. These results highlight the potential of XAIedge to\nsignificantly advance the deployment of explainable AI in energy-constrained\nreal-time applications.", "AI": {"tldr": "XAIedge\u6846\u67b6\u901a\u8fc7\u8fd1\u4f3c\u8ba1\u7b97\u6280\u672f\u63d0\u5347XAI\u7b97\u6cd5\u7684\u80fd\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\u3002", "motivation": "\u73b0\u6709XAI\u786c\u4ef6\u52a0\u901f\u65b9\u6cd5\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u80fd\u6548\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51faXAIedge\u6846\u67b6\uff0c\u7ed3\u5408\u8fd1\u4f3c\u8ba1\u7b97\u6280\u672f\u4f18\u5316XAI\u7b97\u6cd5\uff0c\u5e76\u5728TPU\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u786c\u4ef6\u52a0\u901f\u3002", "result": "XAIedge\u80fd\u6548\u63d0\u53472\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "XAIedge\u6709\u671b\u63a8\u52a8\u53ef\u89e3\u91caAI\u5728\u80fd\u6e90\u53d7\u9650\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2504.17815", "pdf": "https://arxiv.org/pdf/2504.17815", "abs": "https://arxiv.org/abs/2504.17815", "authors": ["Mingxuan Cui", "Qing Guo", "Yuyi Wang", "Hongkai Yu", "Di Lin", "Qin Zou", "Ming-Ming Cheng", "Xi Li"], "title": "Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning", "categories": ["cs.CV"], "comment": "14 pages, 12 figures, ICCV", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D\nrepresentation for novel view synthesis. This paper extends 3DGS capabilities\nto inpainting, where masked objects in a scene are replaced with new contents\nthat blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D\nGaussian inpainting (3DGI) is challenging in effectively leveraging\ncomplementary visual and semantic cues from multiple input views, as occluded\nareas in one view may be visible in others. To address this, we propose a\nmethod that measures the visibility uncertainties of 3D points across different\ninput views and uses them to guide 3DGI in utilizing complementary visual cues.\nWe also employ uncertainties to learn a semantic concept of scene without the\nmasked object and use a diffusion model to fill masked objects in input images\nbased on the learned concept. Finally, we build a novel 3DGI framework, VISTA,\nby integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl\nlearning. VISTA generates high-quality 3DGS models capable of synthesizing\nartifact-free and naturally inpainted novel views. Furthermore, our approach\nextends to handling dynamic distractors arising from temporal object changes,\nenhancing its versatility in diverse scene reconstruction scenarios. We\ndemonstrate the superior performance of our method over state-of-the-art\ntechniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10\ndiverse static 3D inpainting scenes, and an underwater 3D inpainting dataset\nderived from UTB180, including fast-moving fish as inpainting targets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u76843D\u4fee\u590d\u65b9\u6cd5\uff083DGI\uff09\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u7684\u53ef\u89c1\u6027\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u548c\u573a\u666f\u6982\u5ff5\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u65e0\u7f1d\u4fee\u590d\u3002", "motivation": "3D\u4fee\u590d\u5728\u5229\u7528\u591a\u89c6\u89d2\u4e92\u8865\u89c6\u89c9\u548c\u8bed\u4e49\u7ebf\u7d22\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u906e\u6321\u533a\u57df\u7684\u5904\u7406\u4e0a\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faVISTA\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u89c1\u6027\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u76843D\u4fee\u590d\u548c\u573a\u666f\u6982\u5ff5\u5b66\u4e60\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u586b\u5145\u906e\u6321\u533a\u57df\u3002", "result": "VISTA\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65e0\u4f2a\u5f71\u7684\u4fee\u590d\u7ed3\u679c\uff0c\u5e76\u652f\u6301\u52a8\u6001\u5e72\u6270\u7269\u7684\u5904\u7406\u3002\u5728SPIn-NeRF\u548cUTB180\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VISTA\u4e3a3D\u4fee\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u3002"}}
{"id": "2504.17967", "pdf": "https://arxiv.org/pdf/2504.17967", "abs": "https://arxiv.org/abs/2504.17967", "authors": ["Kevin Song", "Andrew Trotter", "Jake Y. Chen"], "title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery", "categories": ["cs.AI"], "comment": "15 pages, 3 figures", "summary": "Drug discovery remains a formidable challenge: more than 90 percent of\ncandidate molecules fail in clinical evaluation, and development costs often\nexceed one billion dollars per approved therapy. Disparate data streams, from\ngenomics and transcriptomics to chemical libraries and clinical records, hinder\ncoherent mechanistic insight and slow progress. Meanwhile, large language\nmodels excel at reasoning and tool integration but lack the modular\nspecialization and iterative memory required for regulated, hypothesis-driven\nworkflows. We introduce PharmaSwarm, a unified multi-agent framework that\norchestrates specialized LLM \"agents\" to propose, validate, and refine\nhypotheses for novel drug targets and lead compounds. Each agent accesses\ndedicated functionality--automated genomic and expression analysis; a curated\nbiomedical knowledge graph; pathway enrichment and network simulation;\ninterpretable binding affinity prediction--while a central Evaluator LLM\ncontinuously ranks proposals by biological plausibility, novelty, in silico\nefficacy, and safety. A shared memory layer captures validated insights and\nfine-tunes underlying submodels over time, yielding a self-improving system.\nDeployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm\nsupports literature-driven discovery, omics-guided target identification, and\nmarket-informed repurposing. We also describe a rigorous four-tier validation\npipeline spanning retrospective benchmarking, independent computational assays,\nexperimental testing, and expert user studies to ensure transparency,\nreproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm\ncan accelerate translational research and deliver high-confidence hypotheses\nmore efficiently than traditional pipelines.", "AI": {"tldr": "PharmaSwarm\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u4e13\u4e1aLLM\u4ee3\u7406\u63d0\u51fa\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\u836f\u7269\u9776\u70b9\u548c\u5148\u5bfc\u5316\u5408\u7269\u7684\u5047\u8bbe\uff0c\u4ee5\u52a0\u901f\u836f\u7269\u53d1\u73b0\u3002", "motivation": "\u836f\u7269\u53d1\u73b0\u6210\u672c\u9ad8\u3001\u5931\u8d25\u7387\u9ad8\uff0c\u4e14\u6570\u636e\u5206\u6563\u963b\u788d\u8fdb\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6574\u5408\u6570\u636e\u3001\u652f\u6301\u5047\u8bbe\u9a71\u52a8\u5de5\u4f5c\u6d41\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "PharmaSwarm\u901a\u8fc7\u591a\u4e2a\u4e13\u4e1aLLM\u4ee3\u7406\uff08\u5982\u57fa\u56e0\u7ec4\u5206\u6790\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u901a\u8def\u5bcc\u96c6\u7b49\uff09\u534f\u4f5c\uff0c\u4e2d\u592e\u8bc4\u4f30\u5668LLM\u5bf9\u63d0\u6848\u8fdb\u884c\u6392\u540d\uff0c\u5171\u4eab\u8bb0\u5fc6\u5c42\u6301\u7eed\u4f18\u5316\u7cfb\u7edf\u3002", "result": "PharmaSwarm\u652f\u6301\u591a\u79cd\u836f\u7269\u53d1\u73b0\u573a\u666f\uff08\u5982\u6587\u732e\u9a71\u52a8\u3001\u7ec4\u5b66\u6307\u5bfc\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u56db\u5c42\u9a8c\u8bc1\u7ba1\u9053\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "PharmaSwarm\u4f5c\u4e3aAI\u52a9\u624b\uff0c\u80fd\u6bd4\u4f20\u7edf\u6d41\u7a0b\u66f4\u9ad8\u6548\u5730\u751f\u6210\u9ad8\u53ef\u4fe1\u5047\u8bbe\uff0c\u52a0\u901f\u8f6c\u5316\u7814\u7a76\u3002"}}
{"id": "2504.17816", "pdf": "https://arxiv.org/pdf/2504.17816", "abs": "https://arxiv.org/abs/2504.17816", "authors": ["Daneul Kim", "Jingxu Zhang", "Wonjoon Jin", "Sunghyun Cho", "Qi Dai", "Jaesik Park", "Chong Luo"], "title": "Subject-driven Video Generation via Disentangled Identity and Motion", "categories": ["cs.CV", "eess.IV"], "comment": "Project Page :\n  https://carpedkm.github.io/projects/disentangled_sub/index.html", "summary": "We propose to train a subject-driven customized video generation model\nthrough decoupling the subject-specific learning from temporal dynamics in\nzero-shot without additional tuning. A traditional method for video\ncustomization that is tuning-free often relies on large, annotated video\ndatasets, which are computationally expensive and require extensive annotation.\nIn contrast to the previous approach, we introduce the use of an image\ncustomization dataset directly on training video customization models,\nfactorizing the video customization into two folds: (1) identity injection\nthrough image customization dataset and (2) temporal modeling preservation with\na small set of unannotated videos through the image-to-video training method.\nAdditionally, we employ random image token dropping with randomized image\ninitialization during image-to-video fine-tuning to mitigate the copy-and-paste\nissue. To further enhance learning, we introduce stochastic switching during\njoint optimization of subject-specific and temporal features, mitigating\ncatastrophic forgetting. Our method achieves strong subject consistency and\nscalability, outperforming existing video customization models in zero-shot\nsettings, demonstrating the effectiveness of our framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u3001\u65e0\u9700\u8c03\u4f18\u7684\u4e3b\u9898\u9a71\u52a8\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u79bb\u4e3b\u9898\u5b66\u4e60\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u5229\u7528\u56fe\u50cf\u5b9a\u5236\u6570\u636e\u96c6\u548c\u5c0f\u89c4\u6a21\u672a\u6807\u6ce8\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5b9a\u5236\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u89c6\u9891\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9700\u5927\u91cf\u6807\u6ce8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u56fe\u50cf\u6570\u636e\u96c6\u76f4\u63a5\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\uff0c\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "1. \u901a\u8fc7\u56fe\u50cf\u5b9a\u5236\u6570\u636e\u96c6\u6ce8\u5165\u4e3b\u9898\u7279\u5f81\uff1b2. \u5229\u7528\u5c11\u91cf\u672a\u6807\u6ce8\u89c6\u9891\u8fdb\u884c\u56fe\u50cf\u5230\u89c6\u9891\u8bad\u7ec3\uff0c\u4fdd\u7559\u65f6\u95f4\u52a8\u6001\uff1b3. \u5f15\u5165\u968f\u673a\u56fe\u50cf\u6807\u8bb0\u4e22\u5f03\u548c\u968f\u673a\u521d\u59cb\u5316\u4ee5\u89e3\u51b3\u590d\u5236\u7c98\u8d34\u95ee\u9898\uff1b4. \u4f7f\u7528\u968f\u673a\u5207\u6362\u4f18\u5316\u4e3b\u9898\u548c\u65f6\u95f4\u7279\u5f81\u7684\u8054\u5408\u5b66\u4e60\u3002", "result": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u53ef\u6269\u5c55\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u5b9a\u5236\u4e2d\u7684\u8ba1\u7b97\u548c\u6807\u6ce8\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.18007", "pdf": "https://arxiv.org/pdf/2504.18007", "abs": "https://arxiv.org/abs/2504.18007", "authors": ["Yazan Otoum", "Amiya Nayak"], "title": "Differential Privacy-Driven Framework for Enhancing Heart Disease Prediction", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": "\\c{opyright} 2025 IEEE. Accepted to IEEE International Conference on\n  Communications ICC 2025. Final version to appear in IEEE Xplore", "summary": "With the rapid digitalization of healthcare systems, there has been a\nsubstantial increase in the generation and sharing of private health data.\nSafeguarding patient information is essential for maintaining consumer trust\nand ensuring compliance with legal data protection regulations. Machine\nlearning is critical in healthcare, supporting personalized treatment, early\ndisease detection, predictive analytics, image interpretation, drug discovery,\nefficient operations, and patient monitoring. It enhances decision-making,\naccelerates research, reduces errors, and improves patient outcomes. In this\npaper, we utilize machine learning methodologies, including differential\nprivacy and federated learning, to develop privacy-preserving models that\nenable healthcare stakeholders to extract insights without compromising\nindividual privacy. Differential privacy introduces noise to data to guarantee\nstatistical privacy, while federated learning enables collaborative model\ntraining across decentralized datasets. We explore applying these technologies\nto Heart Disease Data, demonstrating how they preserve privacy while delivering\nvaluable insights and comprehensive analysis. Our results show that using a\nfederated learning model with differential privacy achieved a test accuracy of\n85%, ensuring patient data remained secure and private throughout the process.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u533b\u7597\u6570\u636e\u5feb\u901f\u6570\u5b57\u5316\u7684\u80cc\u666f\u4e0b\uff0c\u5982\u4f55\u5229\u7528\u5dee\u5206\u9690\u79c1\u548c\u8054\u90a6\u5b66\u4e60\u6280\u672f\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u533b\u7597\u7cfb\u7edf\u7684\u5feb\u901f\u6570\u5b57\u5316\uff0c\u79c1\u4eba\u5065\u5eb7\u6570\u636e\u7684\u751f\u6210\u548c\u5171\u4eab\u5927\u5e45\u589e\u52a0\uff0c\u4fdd\u62a4\u60a3\u8005\u4fe1\u606f\u5bf9\u7ef4\u62a4\u6d88\u8d39\u8005\u4fe1\u4efb\u548c\u9075\u5b88\u6cd5\u5f8b\u6570\u636e\u4fdd\u62a4\u6cd5\u89c4\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u5dee\u5206\u9690\u79c1\u548c\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5dee\u5206\u9690\u79c1\u901a\u8fc7\u6dfb\u52a0\u566a\u58f0\u4fdd\u8bc1\u6570\u636e\u9690\u79c1\uff0c\u8054\u90a6\u5b66\u4e60\u652f\u6301\u5728\u5206\u6563\u6570\u636e\u96c6\u4e0a\u534f\u4f5c\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u5fc3\u810f\u75c5\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u7684\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523085%\uff0c\u540c\u65f6\u786e\u4fdd\u6570\u636e\u9690\u79c1\u3002", "conclusion": "\u5dee\u5206\u9690\u79c1\u548c\u8054\u90a6\u5b66\u4e60\u7684\u7ed3\u5408\u4e3a\u533b\u7597\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002"}}
{"id": "2504.17817", "pdf": "https://arxiv.org/pdf/2504.17817", "abs": "https://arxiv.org/abs/2504.17817", "authors": ["Alexandre Cardaillac", "Donald G. Dansereau"], "title": "Learning Underwater Active Perception in Simulation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "When employing underwater vehicles for the autonomous inspection of assets,\nit is crucial to consider and assess the water conditions. Indeed, they have a\nsignificant impact on the visibility, which also affects robotic operations.\nTurbidity can jeopardise the whole mission as it may prevent correct visual\ndocumentation of the inspected structures. Previous works have introduced\nmethods to adapt to turbidity and backscattering, however, they also include\nmanoeuvring and setup constraints. We propose a simple yet efficient approach\nto enable high-quality image acquisition of assets in a broad range of water\nconditions. This active perception framework includes a multi-layer perceptron\n(MLP) trained to predict image quality given a distance to a target and\nartificial light intensity. We generated a large synthetic dataset including\nten water types with different levels of turbidity and backscattering. For\nthis, we modified the modelling software Blender to better account for the\nunderwater light propagation properties. We validated the approach in\nsimulation and showed significant improvements in visual coverage and quality\nof imagery compared to traditional approaches. The project code is available on\nour project page at https://roboticimaging.org/Projects/ActiveUW/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u7684\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6c34\u4e0b\u4e0d\u540c\u6761\u4ef6\u4e0b\u83b7\u53d6\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u6d51\u6d4a\u5ea6\u548c\u80cc\u6563\u5c04\u4f1a\u5f71\u54cd\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u89c1\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u673a\u52a8\u6027\u548c\u8bbe\u7f6e\u9650\u5236\u3002", "method": "\u4f7f\u7528MLP\u9884\u6d4b\u56fe\u50cf\u8d28\u91cf\uff0c\u751f\u6210\u5305\u542b\u4e0d\u540c\u6d51\u6d4a\u5ea6\u548c\u80cc\u6563\u5c04\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5728Blender\u4e2d\u6539\u8fdb\u6c34\u4e0b\u5149\u4f20\u64ad\u6a21\u578b\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u8986\u76d6\u8303\u56f4\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6c34\u4e0b\u6761\u4ef6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.18039", "pdf": "https://arxiv.org/pdf/2504.18039", "abs": "https://arxiv.org/abs/2504.18039", "authors": ["Zheng Zhang", "Nuoqian Xiao", "Qi Chai", "Deheng Ye", "Hao Wang"], "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.", "AI": {"tldr": "MultiMind\u6846\u67b6\u9996\u6b21\u5c06\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5982\u9762\u90e8\u8868\u60c5\u548c\u8bed\u97f3\u8bed\u8c03\uff09\u6574\u5408\u5230\u793e\u4ea4\u63a8\u7406\u6e38\u620f\uff08SDG\uff09\u4ee3\u7406\u4e2d\uff0c\u7ed3\u5408\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5728\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dSDG\u4ee3\u7406\u4ec5\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u793e\u4ea4\u4e2d\u5173\u952e\u7684\u591a\u6a21\u6001\u7ebf\u7d22\uff08\u5982\u9762\u90e8\u8868\u60c5\u548c\u8bed\u8c03\uff09\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u5176\u4ed6\u73a9\u5bb6\u5fc3\u7406\u72b6\u6001\u7684\u5efa\u6a21\u3002", "method": "\u4f7f\u7528One Night Ultimate Werewolf\uff08ONUW\uff09\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0cMultiMind\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff08\u9762\u90e8\u8868\u60c5\u3001\u8bed\u97f3\u8bed\u8c03\uff09\u548cToM\u6a21\u578b\uff0c\u7ed3\u5408MCTS\u4f18\u5316\u6c9f\u901a\u7b56\u7565\u3002", "result": "\u5728\u4ee3\u7406\u95f4\u6a21\u62df\u548c\u4eba\u7c7b\u73a9\u5bb6\u5b9e\u9a8c\u4e2d\uff0cMultiMind\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6e38\u620f\u6027\u80fd\u3002", "conclusion": "MultiMind\u4e3aLLM\u4ee3\u7406\u5728\u591a\u6a21\u6001\u9886\u57df\u4e2d\u5b9e\u73b0\u7c7b\u4eba\u793e\u4ea4\u63a8\u7406\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2504.17821", "pdf": "https://arxiv.org/pdf/2504.17821", "abs": "https://arxiv.org/abs/2504.17821", "authors": ["Xinyu Chen", "Yunxin Li", "Haoyuan Shi", "Baotian Hu", "Wenhan Luo", "Yaowei Wang", "Min Zhang"], "title": "VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.", "AI": {"tldr": "VideoVista-CulturalLingo\u662f\u9996\u4e2a\u8de8\u6587\u5316\u3001\u8bed\u8a00\u548c\u9886\u57df\u7684\u89c6\u9891\u7406\u89e3\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5305\u542b1,389\u4e2a\u89c6\u9891\u548c3,134\u4e2aQA\u5bf9\uff0c\u8bc4\u4f30\u4e8624\u4e2a\u4e3b\u6d41\u89c6\u9891\u5927\u6a21\u578b\u3002\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u4e2d\u6587\u95ee\u9898\u3001\u65f6\u95f4\u7406\u89e3\u548c\u6570\u5b66\u9886\u57df\u8868\u73b0\u8f83\u5f31\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bc4\u6d4b\u57fa\u51c6\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u8bed\u8a00\uff08\u82f1\u8bed\uff09\u548c\u897f\u65b9\u6587\u5316\u80cc\u666f\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u8de8\u6587\u5316\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u4e2d\u3001\u7f8e\u3001\u6b27\u6587\u5316\u7684\u591a\u8bed\u8a00\uff08\u4e2d\u82f1\uff09\u89c6\u9891\u8bc4\u6d4b\u57fa\u51c6\uff0c\u6db5\u76d6\u5e7f\u6cdb\u9886\u57df\uff0c\u5e76\u8bc4\u4f30\u4e8624\u4e2a\u89c6\u9891\u5927\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u4e2d\u6587\u95ee\u9898\uff08\u5c24\u5176\u662f\u4e2d\u56fd\u5386\u53f2\uff09\u8868\u73b0\u8f83\u5dee\uff1b\u5f00\u6e90\u6a21\u578b\u5728\u65f6\u95f4\u7406\u89e3\u4efb\u52a1\u4e2d\u6700\u9ad8\u5f97\u5206\u4ec545.2%\uff1b\u4e3b\u6d41\u6a21\u578b\u5728\u79d1\u5b66\u95ee\u9898\u8868\u73b0\u5f3a\uff0c\u5f00\u6e90\u6a21\u578b\u5728\u6570\u5b66\u9886\u57df\u5f31\u3002", "conclusion": "VideoVista-CulturalLingo\u586b\u8865\u4e86\u8de8\u6587\u5316\u89c6\u9891\u8bc4\u6d4b\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.18096", "pdf": "https://arxiv.org/pdf/2504.18096", "abs": "https://arxiv.org/abs/2504.18096", "authors": ["Xiang Li", "Haixu Ma", "Guanyong Wu", "Shi Mu", "Chen Li", "Shunpan Liang"], "title": "Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation", "categories": ["cs.AI", "cs.LG"], "comment": "18 pages, 5 figures", "summary": "Medication recommendation is crucial in healthcare, offering effective\ntreatments based on patient's electronic health records (EHR). Previous studies\nshow that integrating more medication-related knowledge improves medication\nrepresentation accuracy. However, not all medications encompass multiple types\nof knowledge data simultaneously. For instance, some medications provide only\ntextual descriptions without structured data. This imbalance in data\navailability limits the performance of existing models, a challenge we term the\n\"bucket effect\" in medication recommendation. Our data analysis uncovers the\nseverity of the \"bucket effect\" in medication recommendation. To fill this gap,\nwe introduce a cross-modal medication encoder capable of seamlessly aligning\ndata from different modalities and propose a medication recommendation\nframework to integrate Multiple types of Knowledge, named MKMed. Specifically,\nwe first pre-train a cross-modal encoder with contrastive learning on five\nknowledge modalities, aligning them into a unified space. Then, we combine the\nmulti-knowledge medication representations with patient records for\nrecommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets\ndemonstrate that MKMed mitigates the \"bucket effect\" in data, and significantly\noutperforms state-of-the-art baselines in recommendation accuracy and safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u836f\u7269\u7f16\u7801\u5668MKMed\uff0c\u7528\u4e8e\u89e3\u51b3\u836f\u7269\u63a8\u8350\u4e2d\u7684\u201c\u6876\u6548\u5e94\u201d\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u77e5\u8bc6\u6a21\u6001\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u836f\u7269\u63a8\u8350\u4e2d\u4e0d\u540c\u836f\u7269\u77e5\u8bc6\u6a21\u6001\u6570\u636e\u4e0d\u5e73\u8861\uff08\u5982\u90e8\u5206\u836f\u7269\u4ec5\u6709\u6587\u672c\u63cf\u8ff0\u800c\u65e0\u7ed3\u6784\u5316\u6570\u636e\uff09\uff0c\u5bfc\u81f4\u73b0\u6709\u6a21\u578b\u6027\u80fd\u53d7\u9650\uff0c\u5373\u201c\u6876\u6548\u5e94\u201d\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u836f\u7269\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u4e94\u79cd\u77e5\u8bc6\u6a21\u6001\uff0c\u5c06\u5176\u5bf9\u9f50\u5230\u7edf\u4e00\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u60a3\u8005\u8bb0\u5f55\u8fdb\u884c\u63a8\u8350\u3002", "result": "\u5728MIMIC-III\u548cMIMIC-IV\u6570\u636e\u96c6\u4e0a\uff0cMKMed\u663e\u8457\u7f13\u89e3\u201c\u6876\u6548\u5e94\u201d\uff0c\u63a8\u8350\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "MKMed\u901a\u8fc7\u591a\u77e5\u8bc6\u6a21\u6001\u6574\u5408\u6709\u6548\u89e3\u51b3\u4e86\u836f\u7269\u63a8\u8350\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.17822", "pdf": "https://arxiv.org/pdf/2504.17822", "abs": "https://arxiv.org/abs/2504.17822", "authors": ["Wenwen Li", "Chia-Yu Hsu", "Sizhe Wang", "Zhining Gu", "Yili Yang", "Brendan M. Rogers", "Anna Liljedahl"], "title": "A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost\nlandforms with significant environmental impacts. Mapping these RTS is crucial\nbecause their appearance serves as a clear indication of permafrost thaw.\nHowever, their small scale compared to other landform features, vague\nboundaries, and spatiotemporal variation pose significant challenges for\naccurate detection. In this paper, we employed a state-of-the-art deep learning\nmodel, the Cascade Mask R-CNN with a multi-scale vision transformer-based\nbackbone, to delineate RTS features across the Arctic. Two new strategies were\nintroduced to optimize multimodal learning and enhance the model's predictive\nperformance: (1) a feature-level, residual cross-modality attention fusion\nstrategy, which effectively integrates feature maps from multiple modalities to\ncapture complementary information and improve the model's ability to understand\ncomplex patterns and relationships within the data; (2) pre-trained unimodal\nlearning followed by multimodal fine-tuning to alleviate high computing demand\nwhile achieving strong model performance. Experimental results demonstrated\nthat our approach outperformed existing models adopting data-level fusion,\nfeature-level convolutional fusion, and various attention fusion strategies,\nproviding valuable insights into the efficient utilization of multimodal data\nfor RTS mapping. This research contributes to our understanding of permafrost\nlandforms and their environmental implications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u68c0\u6d4b\u5317\u6781\u5730\u533a\u7684Retrogressive Thaw Slumps (RTS)\uff0c\u89e3\u51b3\u4e86\u5176\u5c0f\u5c3a\u5ea6\u3001\u6a21\u7cca\u8fb9\u754c\u548c\u65f6\u7a7a\u53d8\u5316\u7684\u6311\u6218\u3002", "motivation": "RTS\u662f\u5317\u6781\u5730\u533a\u91cd\u8981\u7684\u51bb\u571f\u9000\u5316\u6807\u5fd7\uff0c\u4f46\u5176\u5c0f\u5c3a\u5ea6\u548c\u590d\u6742\u7279\u5f81\u4f7f\u5f97\u51c6\u786e\u68c0\u6d4b\u56f0\u96be\u3002", "method": "\u91c7\u7528Cascade Mask R-CNN\u7ed3\u5408\u591a\u5c3a\u5ea6\u89c6\u89c9Transformer\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u65b0\u7b56\u7565\uff1a\u7279\u5f81\u7ea7\u6b8b\u5dee\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u548c\u9884\u8bad\u7ec3\u5355\u6a21\u6001\u5b66\u4e60\u540e\u591a\u6a21\u6001\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728RTS\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u7684\u9ad8\u6548\u5229\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63d0\u5347\u4e86RTS\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u6df1\u5316\u4e86\u5bf9\u51bb\u571f\u5730\u8c8c\u53ca\u5176\u73af\u5883\u5f71\u54cd\u7684\u7406\u89e3\u3002"}}
{"id": "2504.18443", "pdf": "https://arxiv.org/pdf/2504.18443", "abs": "https://arxiv.org/abs/2504.18443", "authors": ["Simon Dold", "Malte Helmert", "Jakob Nordstr\u00f6m", "Gabriele R\u00f6ger", "Tanja Schindler"], "title": "Pseudo-Boolean Proof Logging for Optimal Classical Planning", "categories": ["cs.AI"], "comment": "35th International Conference on Automated Planning and Scheduling\n  (ICAPS'2025)", "summary": "We introduce lower-bound certificates for classical planning tasks, which can\nbe used to prove the unsolvability of a task or the optimality of a plan in a\nway that can be verified by an independent third party. We describe a general\nframework for generating lower-bound certificates based on pseudo-Boolean\nconstraints, which is agnostic to the planning algorithm used.\n  As a case study, we show how to modify the $A^{*}$ algorithm to produce\nproofs of optimality with modest overhead, using pattern database heuristics\nand $h^\\textit{max}$ as concrete examples. The same proof logging approach\nworks for any heuristic whose inferences can be efficiently expressed as\nreasoning over pseudo-Boolean constraints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ecf\u5178\u89c4\u5212\u4efb\u52a1\u7684\u4f4e\u754c\u8bc1\u4e66\uff0c\u53ef\u8bc1\u660e\u4efb\u52a1\u65e0\u89e3\u6216\u8ba1\u5212\u6700\u4f18\uff0c\u4e14\u53ef\u7531\u7b2c\u4e09\u65b9\u9a8c\u8bc1\u3002\u57fa\u4e8e\u4f2a\u5e03\u5c14\u7ea6\u675f\u7684\u901a\u7528\u6846\u67b6\u751f\u6210\u8bc1\u4e66\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u89c4\u5212\u7b97\u6cd5\u3002\u4ee5A*\u7b97\u6cd5\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u751f\u6210\u6700\u4f18\u6027\u8bc1\u660e\u3002", "motivation": "\u89e3\u51b3\u89c4\u5212\u4efb\u52a1\u4e2d\u65e0\u89e3\u6216\u6700\u4f18\u6027\u7684\u9a8c\u8bc1\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u72ec\u7acb\u9a8c\u8bc1\u7684\u8bc1\u660e\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4f2a\u5e03\u5c14\u7ea6\u675f\u7684\u901a\u7528\u6846\u67b6\u751f\u6210\u4f4e\u754c\u8bc1\u4e66\uff0c\u4fee\u6539A*\u7b97\u6cd5\u4ee5\u751f\u6210\u6700\u4f18\u6027\u8bc1\u660e\uff0c\u4f7f\u7528\u6a21\u5f0f\u6570\u636e\u5e93\u542f\u53d1\u5f0f\u548ch^max\u4f5c\u4e3a\u5177\u4f53\u793a\u4f8b\u3002", "result": "\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4f2a\u5e03\u5c14\u7ea6\u675f\u9ad8\u6548\u8868\u8fbe\u542f\u53d1\u5f0f\u63a8\u7406\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f4e\u754c\u8bc1\u4e66\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c4\u5212\u7b97\u6cd5\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2504.17825", "pdf": "https://arxiv.org/pdf/2504.17825", "abs": "https://arxiv.org/abs/2504.17825", "authors": ["Dehong Kong", "Fan Li", "Zhixin Wang", "Jiaqi Xu", "Renjing Pei", "Wenbo Li", "WenQi Ren"], "title": "Dual Prompting Image Restoration with Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR2025", "summary": "Recent state-of-the-art image restoration methods mostly adopt latent\ndiffusion models with U-Net backbones, yet still facing challenges in achieving\nhigh-quality restoration due to their limited capabilities. Diffusion\ntransformers (DiTs), like SD3, are emerging as a promising alternative because\nof their better quality with scalability. In this paper, we introduce DPIR\n(Dual Prompting Image Restoration), a novel image restoration method that\neffectivly extracts conditional information of low-quality images from multiple\nperspectives. Specifically, DPIR consits of two branches: a low-quality image\nconditioning branch and a dual prompting control branch. The first branch\nutilizes a lightweight module to incorporate image priors into the DiT with\nhigh efficiency. More importantly, we believe that in image restoration,\ntextual description alone cannot fully capture its rich visual characteristics.\nTherefore, a dual prompting module is designed to provide DiT with additional\nvisual cues, capturing both global context and local appearance. The extracted\nglobal-local visual prompts as extra conditional control, alongside textual\nprompts to form dual prompts, greatly enhance the quality of the restoration.\nExtensive experimental results demonstrate that DPIR delivers superior image\nrestoration performance.", "AI": {"tldr": "DPIR\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u63d0\u793a\u63a7\u5236\u5206\u652f\u548c\u591a\u89c6\u89d2\u6761\u4ef6\u4fe1\u606f\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8eU-Net\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff09\u5728\u56fe\u50cf\u4fee\u590d\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u800c\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u56e0\u5176\u66f4\u597d\u7684\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u6210\u4e3a\u6709\u6f5c\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "DPIR\u5305\u542b\u4e24\u4e2a\u5206\u652f\uff1a\u4f4e\u8d28\u91cf\u56fe\u50cf\u6761\u4ef6\u5206\u652f\u548c\u53cc\u63d0\u793a\u63a7\u5236\u5206\u652f\u3002\u524d\u8005\u9ad8\u6548\u6574\u5408\u56fe\u50cf\u5148\u9a8c\uff0c\u540e\u8005\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u89c6\u89c9\u63d0\u793a\u589e\u5f3a\u4fee\u590d\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDPIR\u5728\u56fe\u50cf\u4fee\u590d\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DPIR\u901a\u8fc7\u53cc\u63d0\u793a\u548c\u591a\u89c6\u89d2\u6761\u4ef6\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u8d28\u91cf\u3002"}}
{"id": "2504.18453", "pdf": "https://arxiv.org/pdf/2504.18453", "abs": "https://arxiv.org/abs/2504.18453", "authors": ["Peiyuan Jing", "Kinhei Lee", "Zhenxuan Zhang", "Huichi Zhou", "Zhengqing Yuan", "Zhifan Gao", "Lei Zhu", "Giorgos Papanastasiou", "Yingying Fang", "Guang Yang"], "title": "Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Radiology report generation is critical for efficiency but current models\nlack the structured reasoning of experts, hindering clinical trust and\nexplainability by failing to link visual findings to precise anatomical\nlocations. This paper introduces BoxMed-RL, a groundbreaking unified training\nframework for generating spatially verifiable and explainable radiology\nreports. Built on a large vision-language model, BoxMed-RL revolutionizes\nreport generation through two integrated phases: (1) In the Pretraining Phase,\nwe refine the model via medical concept learning, using Chain-of-Thought\nsupervision to internalize the radiologist-like workflow, followed by spatially\nverifiable reinforcement, which applies reinforcement learning to align medical\nfindings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze\nthe pretrained weights and train a downstream adapter to ensure fluent and\nclinically credible reports. This framework precisely mimics radiologists'\nworkflow, compelling the model to connect high-level medical concepts with\ndefinitive anatomical evidence. Extensive experiments on public datasets\ndemonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR\nand ROUGE-L metrics compared to state-of-the-art methods. An average 5%\nimprovement in large language model-based metrics further underscores\nBoxMed-RL's robustness in generating high-quality radiology reports.", "AI": {"tldr": "BoxMed-RL\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u751f\u6210\u53ef\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u7684\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u62a5\u544a\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u4e13\u5bb6\u7ea7\u7684\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5bfc\u81f4\u4e34\u5e8a\u4fe1\u4efb\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5c06\u89c6\u89c9\u53d1\u73b0\u4e0e\u7cbe\u786e\u89e3\u5256\u4f4d\u7f6e\u5173\u8054\u3002", "method": "BoxMed-RL\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9884\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u533b\u5b66\u6982\u5ff5\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u533b\u5b66\u53d1\u73b0\u4e0e\u8fb9\u754c\u6846\uff1b\u4e0b\u6e38\u9002\u914d\u5668\u9636\u6bb5\u51bb\u7ed3\u9884\u8bad\u7ec3\u6743\u91cd\u5e76\u8bad\u7ec3\u9002\u914d\u5668\u4ee5\u751f\u6210\u6d41\u7545\u53ef\u4fe1\u7684\u62a5\u544a\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cBoxMed-RL\u5728METEOR\u548cROUGE-L\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u53477%\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6307\u6807\u4e0a\u63d0\u53475%\u3002", "conclusion": "BoxMed-RL\u901a\u8fc7\u6a21\u4eff\u653e\u5c04\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u7684\u8d28\u91cf\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.17826", "pdf": "https://arxiv.org/pdf/2504.17826", "abs": "https://arxiv.org/abs/2504.17826", "authors": ["Kaicheng Pang", "Xingxing Zou", "Waikeung Wong"], "title": "FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fashion styling and personalized recommendations are pivotal in modern\nretail, contributing substantial economic value in the fashion industry. With\nthe advent of vision-language models (VLM), new opportunities have emerged to\nenhance retailing through natural language and visual interactions. This work\nproposes FashionM3, a multimodal, multitask, and multiround fashion assistant,\nbuilt upon a VLM fine-tuned for fashion-specific tasks. It helps users discover\nsatisfying outfits by offering multiple capabilities including personalized\nrecommendation, alternative suggestion, product image generation, and virtual\ntry-on simulation. Fine-tuned on the novel FashionRec dataset, comprising\n331,124 multimodal dialogue samples across basic, personalized, and alternative\nrecommendation tasks, FashionM3 delivers contextually personalized suggestions\nwith iterative refinement through multiround interactions. Quantitative and\nqualitative evaluations, alongside user studies, demonstrate FashionM3's\nsuperior performance in recommendation effectiveness and practical value as a\nfashion assistant.", "AI": {"tldr": "FashionM3\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u3001\u591a\u8f6e\u65f6\u5c1a\u52a9\u624b\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u63a8\u8350\u3001\u66ff\u4ee3\u5efa\u8bae\u3001\u4ea7\u54c1\u56fe\u50cf\u751f\u6210\u548c\u865a\u62df\u8bd5\u7a7f\u7b49\u529f\u80fd\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u73b0\u4ee3\u96f6\u552e\u4e2d\u65f6\u5c1a\u642d\u914d\u548c\u4e2a\u6027\u5316\u63a8\u8350\u5177\u6709\u91cd\u8981\u7ecf\u6d4e\u4ef7\u503c\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u96f6\u552e\u4e1a\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u57fa\u4e8eVLM\u5fae\u8c03\uff0c\u6784\u5efa\u591a\u4efb\u52a1\u3001\u591a\u8f6e\u65f6\u5c1a\u52a9\u624bFashionM3\uff0c\u5e76\u5728FashionRec\u6570\u636e\u96c6\uff08331,124\u4e2a\u591a\u6a21\u6001\u5bf9\u8bdd\u6837\u672c\uff09\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u53ca\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cFashionM3\u5728\u63a8\u8350\u6548\u679c\u548c\u5b9e\u7528\u4ef7\u503c\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FashionM3\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u63d0\u4f9b\u4e2a\u6027\u5316\u5efa\u8bae\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65f6\u5c1a\u63a8\u8350\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.18530", "pdf": "https://arxiv.org/pdf/2504.18530", "abs": "https://arxiv.org/abs/2504.18530", "authors": ["Joshua Engels", "David D. Baek", "Subhash Kantamneni", "Max Tegmark"], "title": "Scaling Laws For Scalable Oversight", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "34 pages, 17 figures", "summary": "Scalable oversight, the process by which weaker AI systems supervise stronger\nones, has been proposed as a key strategy to control future superintelligent\nsystems. However, it is still unclear how scalable oversight itself scales. To\naddress this gap, we propose a framework that quantifies the probability of\nsuccessful oversight as a function of the capabilities of the overseer and the\nsystem being overseen. Specifically, our framework models oversight as a game\nbetween capability-mismatched players; the players have oversight-specific and\ndeception-specific Elo scores that are a piecewise-linear function of their\ngeneral intelligence, with two plateaus corresponding to task incompetence and\ntask saturation. We validate our framework with a modified version of the game\nNim and then apply it to four oversight games: \"Mafia\", \"Debate\", \"Backdoor\nCode\" and \"Wargames\". For each game, we find scaling laws that approximate how\ndomain performance depends on general AI system capability (using Chatbot Arena\nElo as a proxy for general capability). We then build on our findings in a\ntheoretical study of Nested Scalable Oversight (NSO), a process in which\ntrusted models oversee untrusted stronger models, which then become the trusted\nmodels in the next step. We identify conditions under which NSO succeeds and\nderive numerically (and in some cases analytically) the optimal number of\noversight levels to maximize the probability of oversight success. In our\nnumerical examples, the NSO success rate is below 52% when overseeing systems\nthat are 400 Elo points stronger than the baseline overseer, and it declines\nfurther for overseeing even stronger systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u76d1\u7763\u6210\u529f\u6982\u7387\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6e38\u620f\u6a21\u578b\u9a8c\u8bc1\u4e86\u76d1\u7763\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u5d4c\u5957\u53ef\u6269\u5c55\u76d1\u7763\uff08NSO\uff09\u7684\u6210\u529f\u6761\u4ef6\u3002", "motivation": "\u89e3\u51b3\u53ef\u6269\u5c55\u76d1\u7763\uff08scalable oversight\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5982\u4f55\u91cf\u5316\u7684\u95ee\u9898\uff0c\u4ee5\u63a7\u5236\u672a\u6765\u8d85\u7ea7\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u76d1\u7763\u5efa\u6a21\u4e3a\u80fd\u529b\u4e0d\u5339\u914d\u73a9\u5bb6\u4e4b\u95f4\u7684\u6e38\u620f\uff0c\u4f7f\u7528Elo\u5206\u6570\u91cf\u5316\u76d1\u7763\u548c\u6b3a\u9a97\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4fee\u6539\u7248Nim\u6e38\u620f\u548c\u56db\u79cd\u76d1\u7763\u6e38\u620f\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u76d1\u7763\u6210\u529f\u7387\u5728\u76d1\u7763\u6bd4\u57fa\u7ebf\u76d1\u7763\u8005\u5f3a400 Elo\u70b9\u7684\u7cfb\u7edf\u65f6\u4f4e\u4e8e52%\uff0c\u4e14\u968f\u7cfb\u7edf\u80fd\u529b\u589e\u5f3a\u8fdb\u4e00\u6b65\u4e0b\u964d\u3002", "conclusion": "\u5d4c\u5957\u53ef\u6269\u5c55\u76d1\u7763\uff08NSO\uff09\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u884c\uff0c\u4f46\u76d1\u7763\u6210\u529f\u7387\u968f\u88ab\u76d1\u7763\u7cfb\u7edf\u80fd\u529b\u589e\u5f3a\u800c\u663e\u8457\u964d\u4f4e\u3002"}}
{"id": "2504.17828", "pdf": "https://arxiv.org/pdf/2504.17828", "abs": "https://arxiv.org/abs/2504.17828", "authors": ["Bozheng Li", "Yongliang Wu", "Yi Lu", "Jiashuo Yu", "Licheng Tang", "Jiawang Cao", "Wenqing Zhu", "Yuyang Sun", "Jay Wu", "Wenbo Zhu"], "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR2025", "summary": "Widely shared videos on the internet are often edited. Recently, although\nVideo Large Language Models (Vid-LLMs) have made great progress in general\nvideo understanding tasks, their capabilities in video editing understanding\n(VEU) tasks remain unexplored. To address this gap, in this paper, we introduce\nVEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark\nthat categorizes video editing components across various dimensions, from\nintra-frame features like shot size to inter-shot attributes such as cut types\nand transitions. Unlike previous video editing understanding benchmarks that\nfocus mainly on editing element classification, VEU-Bench encompasses 19\nfine-grained tasks across three stages: recognition, reasoning, and judging. To\nenhance the annotation of VEU automatically, we built an annotation pipeline\nintegrated with an ontology-based knowledge base. Through extensive experiments\nwith 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs\nface significant challenges in VEU tasks, with some performing worse than\nrandom choice. To alleviate this issue, we develop Oscars, a VEU expert model\nfine-tuned on the curated VEU-Bench dataset. It outperforms existing\nopen-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves\nperformance comparable to commercial models like GPT-4o. We also demonstrate\nthat incorporating VEU data significantly enhances the performance of Vid-LLMs\non general video understanding benchmarks, with an average improvement of 8.3%\nacross nine reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86VEU-Bench\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u89c6\u9891\u7f16\u8f91\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b19\u4e2a\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86Oscars\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Vid-LLMs\uff09\u5728\u89c6\u9891\u7f16\u8f91\u7406\u89e3\uff08VEU\uff09\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaVEU-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d619\u4e2a\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u6807\u6ce8\u7ba1\u9053\u548cOscars\u6a21\u578b\u3002", "result": "\u5f53\u524dVid-LLMs\u5728VEU\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cOscars\u6a21\u578b\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u534728.3%\uff0c\u5e76\u63a5\u8fd1GPT-4o\u6027\u80fd\u3002", "conclusion": "VEU\u6570\u636e\u663e\u8457\u63d0\u5347Vid-LLMs\u5728\u901a\u7528\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e73\u5747\u63d0\u53478.3%\u3002"}}
{"id": "2504.18536", "pdf": "https://arxiv.org/pdf/2504.18536", "abs": "https://arxiv.org/abs/2504.18536", "authors": ["Anna Katariina Wisakanto", "Joe Rogero", "Avyay M. Casheekar", "Richard Mallah"], "title": "Adapting Probabilistic Risk Assessment for AI", "categories": ["cs.AI", "cs.CY", "cs.LG", "cs.SY", "eess.SY", "stat.AP"], "comment": "for project website, see https://pra-for-ai.github.io/pra/", "summary": "Modern general-purpose artificial intelligence (AI) systems present an urgent\nrisk management challenge, as their rapidly evolving capabilities and potential\nfor catastrophic harm outpace our ability to reliably assess their risks.\nCurrent methods often rely on selective testing and undocumented assumptions\nabout risk priorities, frequently failing to make a serious attempt at\nassessing the set of pathways through which Al systems pose direct or indirect\nrisks to society and the biosphere. This paper introduces the probabilistic\nrisk assessment (PRA) for AI framework, adapting established PRA techniques\nfrom high-reliability industries (e.g., nuclear power, aerospace) for the new\nchallenges of advanced AI. The framework guides assessors in identifying\npotential risks, estimating likelihood and severity, and explicitly documenting\nevidence, underlying assumptions, and analyses at appropriate granularities.\nThe framework's implementation tool synthesizes the results into a risk report\ncard with aggregated risk estimates from all assessed risks. This systematic\napproach integrates three advances: (1) Aspect-oriented hazard analysis\nprovides systematic hazard coverage guided by a first-principles taxonomy of AI\nsystem aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk\npathway modeling analyzes causal chains from system aspects to societal impacts\nusing bidirectional analysis and incorporating prospective techniques; and (3)\nUncertainty management employs scenario decomposition, reference scales, and\nexplicit tracing protocols to structure credible projections with novelty or\nlimited data. Additionally, the framework harmonizes diverse assessment methods\nby integrating evidence into comparable, quantified absolute risk estimates for\ncritical decisions. We have implemented this as a workbook tool for AI\ndevelopers, evaluators, and regulators, available on the project website.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u73b0\u4ee3\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u7684\u6982\u7387\u98ce\u9669\u8bc4\u4f30\uff08PRA\uff09\u6846\u67b6\uff0c\u501f\u9274\u9ad8\u53ef\u9760\u6027\u884c\u4e1a\u7684PRA\u6280\u672f\uff0c\u4ee5\u7cfb\u7edf\u5316\u8bc6\u522b\u3001\u8bc4\u4f30\u548c\u7ba1\u7406AI\u98ce\u9669\u3002", "motivation": "\u73b0\u4ee3\u901a\u7528AI\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\u548c\u6f5c\u5728\u707e\u96be\u6027\u98ce\u9669\u8d85\u51fa\u4e86\u73b0\u6709\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u7cfb\u7edf\u3001\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u91c7\u7528\u6982\u7387\u98ce\u9669\u8bc4\u4f30\uff08PRA\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u9762\u5411\u65b9\u9762\u7684\u5371\u5bb3\u5206\u6790\u3001\u98ce\u9669\u8def\u5f84\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\uff0c\u751f\u6210\u53ef\u91cf\u5316\u7684\u98ce\u9669\u62a5\u544a\u5361\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u5f00\u53d1\u8005\u548c\u76d1\u7ba1\u8005\u53ef\u7528\u7684\u5de5\u5177\uff0c\u7cfb\u7edf\u5316\u8bc6\u522b\u98ce\u9669\u8def\u5f84\u3001\u4f30\u8ba1\u98ce\u9669\u6982\u7387\u548c\u4e25\u91cd\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u6bd4\u7684\u98ce\u9669\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u66f4\u53ef\u9760\u7684\u98ce\u9669\u7ba1\u7406\u548c\u51b3\u7b56\u3002"}}
{"id": "2504.17829", "pdf": "https://arxiv.org/pdf/2504.17829", "abs": "https://arxiv.org/abs/2504.17829", "authors": ["Vlad Vasilescu", "Ana Neacsu", "Daniela Faur"], "title": "Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Single-image dehazing is an important topic in remote sensing applications,\nenhancing the quality of acquired images and increasing object detection\nprecision. However, the reliability of such structures has not been\nsufficiently analyzed, which poses them to the risk of imperceptible\nperturbations that can significantly hinder their performance. In this work, we\nshow that state-of-the-art image-to-image dehazing transformers are susceptible\nto adversarial noise, with even 1 pixel change being able to decrease the PSNR\nby as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies\naimed at increasing the robustness of pre-trained transformers. Our methods\nresults in comparable clean performance, while significantly increasing the\nprotection against adversarial data. We further present their applicability in\ntwo remote sensing scenarios, showcasing their robust behavior for\nout-of-distribution data. The source code for adversarial fine-tuning and\nattack algorithms can be found at github.com/Vladimirescu/RobustDehazing.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5355\u56fe\u50cf\u53bb\u96fe\u6a21\u578b\u7684\u5bf9\u6297\u566a\u58f0\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7b56\u7565\u4ee5\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u5355\u56fe\u50cf\u53bb\u96fe\u5728\u9065\u611f\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u7684\u53ef\u9760\u6027\u672a\u5145\u5206\u5206\u6790\uff0c\u6613\u53d7\u5bf9\u6297\u566a\u58f0\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7b56\u7565\uff0c\u589e\u5f3a\u9884\u8bad\u7ec3Transformer\u7684\u9c81\u68d2\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u4fdd\u6301\u5e72\u51c0\u6570\u636e\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u6570\u636e\u7684\u9632\u62a4\u80fd\u529b\uff0c\u5e76\u5728\u9065\u611f\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u53bb\u96fe\u6a21\u578b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5916\u6570\u636e\u3002"}}
{"id": "2504.17792", "pdf": "https://arxiv.org/pdf/2504.17792", "abs": "https://arxiv.org/abs/2504.17792", "authors": ["Hauke Sandhaus", "Angel Hsing-Chi Hwang", "Wendy Ju", "Qian Yang"], "title": "My Precious Crash Data: Barriers and Opportunities in Encouraging Autonomous Driving Companies to Share Safety-Critical Data", "categories": ["cs.HC", "cs.AI", "cs.DB", "E.m; H.2.8; J.1"], "comment": "To appear in Proc. ACM Hum.-Comput. Interact., Computer-Supported\n  Cooperative Work & Social Computing (CSCW), 2025", "summary": "Safety-critical data, such as crash and near-crash records, are crucial to\nimproving autonomous vehicle (AV) design and development. Sharing such data\nacross AV companies, academic researchers, regulators, and the public can help\nmake all AVs safer. However, AV companies rarely share safety-critical data\nexternally. This paper aims to pinpoint why AV companies are reluctant to share\nsafety-critical data, with an eye on how these barriers can inform new\napproaches to promote sharing. We interviewed twelve AV company employees who\nactively work with such data in their day-to-day work. Findings suggest two\nkey, previously unknown barriers to data sharing: (1) Datasets inherently embed\nsalient knowledge that is key to improving AV safety and are\nresource-intensive. Therefore, data sharing, even within a company, is fraught\nwith politics. (2) Interviewees believed AV safety knowledge is private\nknowledge that brings competitive edges to their companies, rather than public\nknowledge for social good. We discuss the implications of these findings for\nincentivizing and enabling safety-critical AV data sharing, specifically,\nimplications for new approaches to (1) debating and stratifying public and\nprivate AV safety knowledge, (2) innovating data tools and data sharing\npipelines that enable easier sharing of public AV safety data and knowledge;\n(3) offsetting costs of curating safety-critical data and incentivizing data\nsharing.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u9a7e\u9a76\u516c\u53f8\u4e0d\u613f\u5171\u4eab\u5b89\u5168\u5173\u952e\u6570\u636e\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4fc3\u8fdb\u6570\u636e\u5171\u4eab\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5171\u4eab\u5b89\u5168\u5173\u952e\u6570\u636e\u53ef\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\uff0c\u4f46\u516c\u53f8\u56e0\u7ade\u4e89\u548c\u8d44\u6e90\u95ee\u9898\u4e0d\u613f\u5171\u4eab\u3002", "method": "\u901a\u8fc7\u8bbf\u8c0812\u540d\u81ea\u52a8\u9a7e\u9a76\u516c\u53f8\u5458\u5de5\uff0c\u5206\u6790\u6570\u636e\u5171\u4eab\u7684\u969c\u788d\u3002", "result": "\u53d1\u73b0\u4e24\u5927\u969c\u788d\uff1a\u6570\u636e\u8574\u542b\u5173\u952e\u77e5\u8bc6\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u4ee5\u53ca\u516c\u53f8\u89c6\u5176\u4e3a\u7ade\u4e89\u4f18\u52bf\u800c\u975e\u516c\u5171\u77e5\u8bc6\u3002", "conclusion": "\u63d0\u51fa\u6fc0\u52b1\u6570\u636e\u5171\u4eab\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u533a\u5206\u516c\u5171\u4e0e\u79c1\u4eba\u77e5\u8bc6\u3001\u521b\u65b0\u6570\u636e\u5de5\u5177\u53ca\u6210\u672c\u8865\u507f\u3002"}}
{"id": "2504.17892", "pdf": "https://arxiv.org/pdf/2504.17892", "abs": "https://arxiv.org/abs/2504.17892", "authors": ["Yasmine Omri", "Parth Shroff", "Thierry Tambe"], "title": "Token Sequence Compression for Efficient Multimodal Computing", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The exponential growth of Large Multimodal Models (LMMs) has driven\nadvancements in cross-modal reasoning but at significant computational costs.\nIn this work, we focus on visual language models. We highlight the redundancy\nand inefficiency in current vision encoders, and seek to construct an adaptive\ncompression method for multimodal data. In this work, we characterize a panoply\nof visual token selection and merging approaches through both benchmarking and\nqualitative analysis. In particular, we demonstrate that simple cluster-level\ntoken aggregation outperforms prior state-of-the-art works in token selection\nand merging, including merging at the vision encoder level and attention-based\napproaches. We underline the redundancy in current vision encoders, and shed\nlight on several puzzling trends regarding principles of visual token selection\nthrough cross-modal attention visualizations. This work is a first effort\ntowards more effective encoding and processing of high-dimensional data, and\npaves the way for more scalable and sustainable multimodal systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u538b\u7f29\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5197\u4f59\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u901a\u8fc7\u805a\u7c7b\u7ea7\u6807\u8bb0\u805a\u5408\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u7f16\u7801\u5668\u5b58\u5728\u5197\u4f59\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9a\u6027\u5206\u6790\uff0c\u7814\u7a76\u4e86\u591a\u79cd\u89c6\u89c9\u6807\u8bb0\u9009\u62e9\u548c\u5408\u5e76\u65b9\u6cd5\uff0c\u91cd\u70b9\u63d0\u51fa\u805a\u7c7b\u7ea7\u6807\u8bb0\u805a\u5408\u3002", "result": "\u805a\u7c7b\u7ea7\u6807\u8bb0\u805a\u5408\u5728\u6807\u8bb0\u9009\u62e9\u548c\u5408\u5e76\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u6807\u8bb0\u9009\u62e9\u4e2d\u7684\u5197\u4f59\u548c\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u6548\u7f16\u7801\u548c\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u6301\u7eed\u6027\u53d1\u5c55\u3002"}}
{"id": "2504.17799", "pdf": "https://arxiv.org/pdf/2504.17799", "abs": "https://arxiv.org/abs/2504.17799", "authors": ["S. L. Thomson", "M. W. Przewozniczek"], "title": "Subfunction Structure Matters: A New Perspective on Local Optima Networks", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Local optima networks (LONs) capture fitness landscape information. They are\ntypically constructed in a black-box manner; information about the problem\nstructure is not utilised. This also applies to the analysis of LONs: knowledge\nabout the problem, such as interaction between variables, is not considered. We\nchallenge this status-quo with an alternative approach: we consider how LON\nanalysis can be improved by incorporating subfunction-based information - this\ncan either be known a-priori or learned during search. To this end, LONs are\nconstructed for several benchmark pseudo-boolean problems using three\napproaches: firstly, the standard algorithm; a second algorithm which uses\ndeterministic grey-box crossover; and a third algorithm which selects\nperturbations based on learned information about variable interactions. Metrics\nrelated to subfunction changes in a LON are proposed and compared with metrics\nfrom previous literature which capture other aspects of a LON. Incorporating\nproblem structure in LON construction and analysing it can bring enriched\ninsight into optimisation dynamics. Such information may be crucial to\nunderstanding the difficulty of solving a given problem with state-of-the-art\nlinkage learning optimisers. In light of the results, we suggest incorporation\nof problem structure as an alternative paradigm in landscape analysis for\nproblems with known or suspected subfunction structure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u5c40\u90e8\u6700\u4f18\u7f51\u7edc\uff08LON\uff09\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5b50\u51fd\u6570\u4fe1\u606f\uff08\u5df2\u77e5\u6216\u5b66\u4e60\u5f97\u5230\uff09\u6765\u4e30\u5bcc\u4f18\u5316\u52a8\u6001\u7684\u7406\u89e3\u3002", "motivation": "\u4f20\u7edfLON\u6784\u5efa\u548c\u5206\u6790\u672a\u5229\u7528\u95ee\u9898\u7ed3\u6784\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u4f18\u5316\u52a8\u6001\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u65b9\u6cd5\u6784\u5efaLON\uff1a\u6807\u51c6\u7b97\u6cd5\u3001\u57fa\u4e8e\u786e\u5b9a\u6027\u7070\u76d2\u4ea4\u53c9\u7684\u7b97\u6cd5\u3001\u57fa\u4e8e\u5b66\u4e60\u53d8\u91cf\u4ea4\u4e92\u7684\u6270\u52a8\u9009\u62e9\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e0e\u5b50\u51fd\u6570\u53d8\u5316\u76f8\u5173\u7684\u5ea6\u91cf\u3002", "result": "\u7ed3\u5408\u95ee\u9898\u7ed3\u6784\u7684LON\u5206\u6790\u80fd\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4f18\u5316\u52a8\u6001\u4fe1\u606f\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u95ee\u9898\u6c42\u89e3\u96be\u5ea6\u3002", "conclusion": "\u5efa\u8bae\u5728\u5df2\u77e5\u6216\u7591\u4f3c\u5b50\u51fd\u6570\u7ed3\u6784\u7684\u95ee\u9898\u4e2d\uff0c\u5c06\u95ee\u9898\u7ed3\u6784\u7eb3\u5165\u666f\u89c2\u5206\u6790\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2504.17894", "pdf": "https://arxiv.org/pdf/2504.17894", "abs": "https://arxiv.org/abs/2504.17894", "authors": ["Aniruddha Bala", "Rohit Chowdhury", "Rohan Jaiswal", "Siddharth Roheda"], "title": "DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Advancements in diffusion models have enabled effortless image editing via\ntext prompts, raising concerns about image security. Attackers with access to\nuser images can exploit these tools for malicious edits. Recent defenses\nattempt to protect images by adding a limited noise in the pixel space to\ndisrupt the functioning of diffusion-based editing models. However, the\nadversarial noise added by previous methods is easily noticeable to the human\neye. Moreover, most of these methods are not robust to purification techniques\nlike JPEG compression under a feasible pixel budget. We propose a novel\noptimization approach that introduces adversarial perturbations directly in the\nfrequency domain by modifying the Discrete Cosine Transform (DCT) coefficients\nof the input image. By leveraging the JPEG pipeline, our method generates\nadversarial images that effectively prevent malicious image editing. Extensive\nexperiments across a variety of tasks and datasets demonstrate that our\napproach introduces fewer visual artifacts while maintaining similar levels of\nedit protection and robustness to noise purification techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u9891\u57df\uff08DCT\u7cfb\u6570\uff09\u6dfb\u52a0\u5bf9\u6297\u6027\u6270\u52a8\u7684\u65b9\u6cd5\uff0c\u4ee5\u4fdd\u62a4\u56fe\u50cf\u514d\u53d7\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6076\u610f\u7f16\u8f91\uff0c\u540c\u65f6\u51cf\u5c11\u89c6\u89c9\u4f2a\u5f71\u5e76\u589e\u5f3a\u5bf9JPEG\u538b\u7f29\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u8fdb\u6b65\u4f7f\u5f97\u901a\u8fc7\u6587\u672c\u63d0\u793a\u8f7b\u677e\u7f16\u8f91\u56fe\u50cf\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u56fe\u50cf\u5b89\u5168\u95ee\u9898\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u50cf\u7d20\u7a7a\u95f4\u6dfb\u52a0\u7684\u566a\u58f0\u5bb9\u6613\u88ab\u5bdf\u89c9\u4e14\u5bf9JPEG\u538b\u7f29\u4e0d\u9c81\u68d2\u3002", "method": "\u901a\u8fc7\u5728\u9891\u57df\uff08DCT\u7cfb\u6570\uff09\u4f18\u5316\u6dfb\u52a0\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u5229\u7528JPEG\u6d41\u7a0b\u751f\u6210\u5bf9\u6297\u6027\u56fe\u50cf\uff0c\u6709\u6548\u9632\u6b62\u6076\u610f\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u51cf\u5c11\u4e86\u89c6\u89c9\u4f2a\u5f71\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7f16\u8f91\u4fdd\u62a4\u6548\u679c\u548c\u5bf9\u566a\u58f0\u51c0\u5316\u6280\u672f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u9891\u57df\u4f18\u5316\u65b9\u6cd5\u5728\u4fdd\u62a4\u56fe\u50cf\u5b89\u5168\u65b9\u9762\u66f4\u6709\u6548\u4e14\u89c6\u89c9\u4e0a\u66f4\u9690\u853d\u3002"}}
{"id": "2504.17801", "pdf": "https://arxiv.org/pdf/2504.17801", "abs": "https://arxiv.org/abs/2504.17801", "authors": ["Xufeng Yao", "Jiaxi Jiang", "Yuxuan Zhao", "Peiyu Liao", "Yibo Lin", "Bei Yu"], "title": "Evolution of Optimization Algorithms for Global Placement via Large Language Models", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Optimization algorithms are widely employed to tackle complex problems, but\ndesigning them manually is often labor-intensive and requires significant\nexpertise. Global placement is a fundamental step in electronic design\nautomation (EDA). While analytical approaches represent the state-of-the-art\n(SOTA) in global placement, their core optimization algorithms remain heavily\ndependent on heuristics and customized components, such as initialization\nstrategies, preconditioning methods, and line search techniques. This paper\npresents an automated framework that leverages large language models (LLM) to\nevolve optimization algorithms for global placement. We first generate diverse\ncandidate algorithms using LLM through carefully crafted prompts. Then we\nintroduce an LLM-based genetic flow to evolve selected candidate algorithms.\nThe discovered optimization algorithms exhibit substantial performance\nimprovements across many benchmarks. Specifically, Our design-case-specific\ndiscovered algorithms achieve average HPWL improvements of \\textbf{5.05\\%},\n\\text{5.29\\%} and \\textbf{8.30\\%} on MMS, ISPD2005 and ISPD2019 benchmarks, and\nup to \\textbf{17\\%} improvements on individual cases. Additionally, the\ndiscovered algorithms demonstrate good generalization ability and are\ncomplementary to existing parameter-tuning methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u4e2d\u7684\u5168\u5c40\u5e03\u5c40\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5168\u5c40\u5e03\u5c40\u7b97\u6cd5\u7684\u8bbe\u8ba1\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u7ecf\u9a8c\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u901a\u7528\u5316\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u751f\u6210\u591a\u6837\u5316\u5019\u9009\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eLLM\u7684\u9057\u4f20\u6d41\u7a0b\u8fdb\u884c\u7b97\u6cd5\u8fdb\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u53d1\u73b0\u7684\u7b97\u6cd5\u5e73\u5747HPWL\u63d0\u53475.05%\u81f38.30%\uff0c\u4e2a\u522b\u6848\u4f8b\u63d0\u5347\u9ad8\u8fbe17%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd8\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u4e92\u8865\u3002"}}
{"id": "2504.17902", "pdf": "https://arxiv.org/pdf/2504.17902", "abs": "https://arxiv.org/abs/2504.17902", "authors": ["Girish A. Koushik", "Diptesh Kanojia", "Helen Treharne", "Aditya Joshi"], "title": "CAMU: Context Augmentation for Meme Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "Under review at ACM MM 2025", "summary": "Social media memes are a challenging domain for hate detection because they\nintertwine visual and textual cues into culturally nuanced messages. We\nintroduce a novel framework, CAMU, which leverages large vision-language models\nto generate more descriptive captions, a caption-scoring neural network to\nemphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's\ntext encoder for an improved multimodal understanding of memes. Experiments on\npublicly available hateful meme datasets show that simple projection layer\nfine-tuning yields modest gains, whereas selectively tuning deeper text encoder\nlayers significantly boosts performance on all evaluation metrics. Moreover,\nour approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful\nMemes dataset, at par with the existing SoTA framework while being much more\nefficient, offering practical advantages in real-world scenarios that rely on\nfixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the\nMultiOFF dataset for offensive meme identification, demonstrating its\ngeneralisability. Additional analyses on benign confounders reveal that robust\nvisual grounding and nuanced text representations are crucial for reliable hate\nand offence detection. We will publicly release CAMU along with the resultant\nmodels for further research.\n  Disclaimer: This paper includes references to potentially disturbing,\nhateful, or offensive content due to the nature of the task.", "AI": {"tldr": "CAMU\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u6539\u8fdb\u7684CLIP\u6587\u672c\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u793e\u4ea4\u5a92\u4f53\u6a21\u56e0\u4e2d\u4ec7\u6068\u5185\u5bb9\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u5728\u6548\u7387\u548c\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u6a21\u56e0\u56e0\u5176\u89c6\u89c9\u4e0e\u6587\u672c\u7ed3\u5408\u7684\u590d\u6742\u6027\uff0c\u6210\u4e3a\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u7684\u6311\u6218\u3002CAMU\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u7406\u89e3\u548c\u9ad8\u6548\u5fae\u8c03\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CAMU\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63cf\u8ff0\u6027\u6807\u9898\uff0c\u901a\u8fc7\u6807\u9898\u8bc4\u5206\u7f51\u7edc\u7a81\u51fa\u4ec7\u6068\u76f8\u5173\u5185\u5bb9\uff0c\u5e76\u9ad8\u6548\u5fae\u8c03CLIP\u6587\u672c\u7f16\u7801\u5668\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u7406\u89e3\u3002", "result": "CAMU\u5728Hateful Memes\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.807\u51c6\u786e\u7387\u548c0.806 F1\u5206\u6570\uff0c\u5728MultiOFF\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u4e3a0.673\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u66f4\u9ad8\u6548\u3002", "conclusion": "CAMU\u5c55\u793a\u4e86\u5728\u4ec7\u6068\u548c\u5192\u72af\u5185\u5bb9\u68c0\u6d4b\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5f3a\u8c03\u89c6\u89c9\u57fa\u7840\u548c\u6587\u672c\u8868\u793a\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.17805", "pdf": "https://arxiv.org/pdf/2504.17805", "abs": "https://arxiv.org/abs/2504.17805", "authors": ["Tri Nguyen", "Kelly Cohen"], "title": "Fuzzy Logic -- Based Scheduling System for Part-Time Workforce", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper explores the application of genetic fuzzy systems to efficiently\ngenerate schedules for a team of part-time student workers at a university.\nGiven the preferred number of working hours and availability of employees, our\nmodel generates feasible solutions considering various factors, such as maximum\nweekly hours, required number of workers on duty, and the preferred number of\nworking hours. The algorithm is trained and tested with availability data\ncollected from students at the University of Cincinnati. The results\ndemonstrate the algorithm's efficiency in producing schedules that meet\noperational criteria and its robustness in understaffed conditions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u9057\u4f20\u6a21\u7cca\u7cfb\u7edf\u4e3a\u5927\u5b66\u517c\u804c\u5b66\u751f\u751f\u6210\u9ad8\u6548\u6392\u73ed\u8868\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5927\u5b66\u517c\u804c\u5b66\u751f\u6392\u73ed\u95ee\u9898\uff0c\u8003\u8651\u5458\u5de5\u504f\u597d\u548c\u53ef\u7528\u6027\uff0c\u540c\u65f6\u6ee1\u8db3\u8fd0\u8425\u9700\u6c42\u3002", "method": "\u91c7\u7528\u9057\u4f20\u6a21\u7cca\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b66\u751f\u53ef\u7528\u6027\u6570\u636e\uff0c\u751f\u6210\u6ee1\u8db3\u591a\u79cd\u7ea6\u675f\u6761\u4ef6\u7684\u6392\u73ed\u65b9\u6848\u3002", "result": "\u7b97\u6cd5\u5728\u8f9b\u8f9b\u90a3\u63d0\u5927\u5b66\u6570\u636e\u4e0a\u8868\u73b0\u9ad8\u6548\uff0c\u80fd\u751f\u6210\u7b26\u5408\u8fd0\u8425\u6807\u51c6\u7684\u6392\u73ed\u8868\uff0c\u4e14\u5728\u4eba\u5458\u4e0d\u8db3\u65f6\u4ecd\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u9057\u4f20\u6a21\u7cca\u7cfb\u7edf\u662f\u89e3\u51b3\u590d\u6742\u6392\u73ed\u95ee\u9898\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.17935", "pdf": "https://arxiv.org/pdf/2504.17935", "abs": "https://arxiv.org/abs/2504.17935", "authors": ["H. Martin Gillis", "Ming Hill", "Paul Hollensen", "Alan Fine", "Thomas Trappenberg"], "title": "Masked strategies for images with small objects", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The hematology analytics used for detection and classification of small blood\ncomponents is a significant challenge. In particular, when objects exists as\nsmall pixel-sized entities in a large context of similar objects. Deep learning\napproaches using supervised models with pre-trained weights, such as residual\nnetworks and vision transformers have demonstrated success for many\napplications. Unfortunately, when applied to images outside the domain of\nlearned representations, these methods often result with less than acceptable\nperformance. A strategy to overcome this can be achieved by using\nself-supervised models, where representations are learned and weights are then\napplied for downstream applications. Recently, masked autoencoders have proven\nto be effective to obtain representations that captures global context\ninformation. By masking regions of an image and having the model learn to\nreconstruct both the masked and non-masked regions, weights can be used for\nvarious applications. However, if the sizes of the objects in images are less\nthan the size of the mask, the global context information is lost, making it\nalmost impossible to reconstruct the image. In this study, we investigated the\neffect of mask ratios and patch sizes for blood components using a MAE to\nobtain learned ViT encoder representations. We then applied the encoder weights\nto train a U-Net Transformer for semantic segmentation to obtain both local and\nglobal contextual information. Our experimental results demonstrates that both\nsmaller mask ratios and patch sizes improve the reconstruction of images using\na MAE. We also show the results of semantic segmentation with and without\npre-trained weights, where smaller-sized blood components benefited with\npre-training. Overall, our proposed method offers an efficient and effective\nstrategy for the segmentation and classification of small objects.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8840\u6db2\u6210\u5206\u5206\u6790\u4e2d\uff0c\u4f7f\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u548cViT\u7f16\u7801\u5668\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u63a9\u7801\u6bd4\u4f8b\u548c\u8865\u4e01\u5c3a\u5bf8\u4f18\u5316\u56fe\u50cf\u91cd\u5efa\uff0c\u5e76\u5e94\u7528\u4e8eU-Net Transformer\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u3002", "motivation": "\u8840\u6db2\u6210\u5206\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\uff0c\u5c0f\u50cf\u7d20\u5c3a\u5bf8\u5bf9\u8c61\u5728\u76f8\u4f3c\u80cc\u666f\u4e2d\u7684\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u57df\u5916\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u63a2\u7d22\u81ea\u76d1\u7763\u6a21\u578b\u4ee5\u6539\u5584\u6027\u80fd\u3002", "method": "\u91c7\u7528MAE\u5b66\u4e60ViT\u7f16\u7801\u5668\u8868\u793a\uff0c\u8c03\u6574\u63a9\u7801\u6bd4\u4f8b\u548c\u8865\u4e01\u5c3a\u5bf8\u4f18\u5316\u91cd\u5efa\u6548\u679c\uff0c\u5e76\u5c06\u7f16\u7801\u5668\u6743\u91cd\u7528\u4e8e\u8bad\u7ec3U-Net Transformer\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8f83\u5c0f\u7684\u63a9\u7801\u6bd4\u4f8b\u548c\u8865\u4e01\u5c3a\u5bf8\u80fd\u6539\u5584MAE\u7684\u56fe\u50cf\u91cd\u5efa\u6548\u679c\uff0c\u9884\u8bad\u7ec3\u6743\u91cd\u5bf9\u5c0f\u5c3a\u5bf8\u8840\u6db2\u6210\u5206\u7684\u5206\u5272\u6709\u76ca\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5c0f\u5bf9\u8c61\u7684\u5206\u5272\u548c\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7b56\u7565\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u5c3a\u5bf8\u8840\u6db2\u6210\u5206\u7684\u5206\u6790\u3002"}}
{"id": "2504.17807", "pdf": "https://arxiv.org/pdf/2504.17807", "abs": "https://arxiv.org/abs/2504.17807", "authors": ["Ze Yang", "Yihong Jin", "Juntian Liu", "Xinhe Xu", "Yihan Zhang", "Shuyang Ji"], "title": "Research on Cloud Platform Network Traffic Monitoring and Anomaly Detection System based on Large Language Models", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "Proceedings of 2025 IEEE 7th International Conference on\n  Communications, Information System and Computer Engineering (CISCE 2025)", "summary": "The rapidly evolving cloud platforms and the escalating complexity of network\ntraffic demand proper network traffic monitoring and anomaly detection to\nensure network security and performance. This paper introduces a large language\nmodel (LLM)-based network traffic monitoring and anomaly detection system. In\naddition to existing models such as autoencoders and decision trees, we harness\nthe power of large language models for processing sequence data from network\ntraffic, which allows us a better capture of underlying complex patterns, as\nwell as slight fluctuations in the dataset. We show for a given detection task,\nthe need for a hybrid model that incorporates the attention mechanism of the\ntransformer architecture into a supervised learning framework in order to\nachieve better accuracy. A pre-trained large language model analyzes and\npredicts the probable network traffic, and an anomaly detection layer that\nconsiders temporality and context is added. Moreover, we present a novel\ntransfer learning-based methodology to enhance the model's effectiveness to\nquickly adapt to unknown network structures and adversarial conditions without\nrequiring extensive labeled datasets. Actual results show that the designed\nmodel outperforms traditional methods in detection accuracy and computational\nefficiency, effectively identify various network anomalies such as zero-day\nattacks and traffic congestion pattern, and significantly reduce the false\npositive rate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7f51\u7edc\u6d41\u91cf\u76d1\u63a7\u4e0e\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u81ea\u7f16\u7801\u5668\u548c\u51b3\u7b56\u6811\uff09\u548cLLM\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u968f\u7740\u4e91\u5e73\u53f0\u7684\u5feb\u901f\u53d1\u5c55\u548c\u7f51\u7edc\u6d41\u91cf\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u7f51\u7edc\u76d1\u63a7\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u6a21\u5f0f\u548c\u7ec6\u5fae\u6ce2\u52a8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408Transformer\u67b6\u6784\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684LLM\u5206\u6790\u6d41\u91cf\u6570\u636e\uff0c\u5e76\u6dfb\u52a0\u8003\u8651\u65f6\u5e8f\u548c\u4e0a\u4e0b\u6587\u7684\u5f02\u5e38\u68c0\u6d4b\u5c42\u3002\u5f15\u5165\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u4ee5\u5feb\u901f\u9002\u5e94\u672a\u77e5\u7f51\u7edc\u7ed3\u6784\u548c\u5bf9\u6297\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u96f6\u65e5\u653b\u51fb\u548c\u6d41\u91cf\u62e5\u585e\u6a21\u5f0f\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u7387\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u6df7\u5408\u6a21\u578b\u4e3a\u7f51\u7edc\u6d41\u91cf\u76d1\u63a7\u548c\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u6a21\u5f0f\u548c\u672a\u77e5\u7f51\u7edc\u6761\u4ef6\u65f6\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.17990", "pdf": "https://arxiv.org/pdf/2504.17990", "abs": "https://arxiv.org/abs/2504.17990", "authors": ["Yabing Wang", "Zhuotao Tian", "Qingpei Guo", "Zheng Qin", "Sanping Zhou", "Ming Yang", "Le Wang"], "title": "From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Composed Image Retrieval (CIR) is a challenging multimodal task that\nretrieves a target image based on a reference image and accompanying\nmodification text. Due to the high cost of annotating CIR triplet datasets,\nzero-shot (ZS) CIR has gained traction as a promising alternative. Existing\nstudies mainly focus on projection-based methods, which map an image to a\nsingle pseudo-word token. However, these methods face three critical\nchallenges: (1) insufficient pseudo-word token representation capacity, (2)\ndiscrepancies between training and inference phases, and (3) reliance on\nlarge-scale synthetic data. To address these issues, we propose a two-stage\nframework where the training is accomplished from mapping to composing. In the\nfirst stage, we enhance image-to-pseudo-word token learning by introducing a\nvisual semantic injection module and a soft text alignment objective, enabling\nthe token to capture richer and fine-grained image information. In the second\nstage, we optimize the text encoder using a small amount of synthetic triplet\ndata, enabling it to effectively extract compositional semantics by combining\npseudo-word tokens with modification text for accurate target image retrieval.\nThe strong visual-to-pseudo mapping established in the first stage provides a\nsolid foundation for the second stage, making our approach compatible with both\nhigh- and low-quality synthetic data, and capable of achieving significant\nperformance gains with only a small amount of synthetic data. Extensive\nexperiments were conducted on three public datasets, achieving superior\nperformance compared to existing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u89e3\u51b3\u96f6\u6837\u672c\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u4f2a\u8bcd\u8868\u793a\u4e0d\u8db3\u3001\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e0d\u4e00\u81f4\u53ca\u4f9d\u8d56\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u56e0\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u96f6\u6837\u672c\u65b9\u6cd5\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u6295\u5f71\u65b9\u6cd5\u5b58\u5728\u4f2a\u8bcd\u8868\u793a\u4e0d\u8db3\u3001\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e0d\u4e00\u81f4\u53ca\u4f9d\u8d56\u5408\u6210\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u89c6\u89c9\u8bed\u4e49\u6ce8\u5165\u6a21\u5757\u548c\u8f6f\u6587\u672c\u5bf9\u9f50\u76ee\u6807\u589e\u5f3a\u56fe\u50cf\u5230\u4f2a\u8bcd\u7684\u6620\u5c04\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f18\u5316\u6587\u672c\u7f16\u7801\u5668\uff0c\u5229\u7528\u5c11\u91cf\u5408\u6210\u6570\u636e\u63d0\u53d6\u7ec4\u5408\u8bed\u4e49\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e14\u5bf9\u5408\u6210\u6570\u636e\u8d28\u91cf\u8981\u6c42\u4f4e\u3002", "conclusion": "\u4e24\u9636\u6bb5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6837\u672cCIR\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2504.17991", "pdf": "https://arxiv.org/pdf/2504.17991", "abs": "https://arxiv.org/abs/2504.17991", "authors": ["Zheng Qin", "Le Wang", "Yabing Wang", "Sanping Zhou", "Gang Hua", "Wei Tang"], "title": "RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recent image-goal navigation (ImageNav) methods learn a perception-action\npolicy by separately capturing semantic features of the goal and egocentric\nimages, then passing them to a policy network. However, challenges remain: (1)\nSemantic features often fail to provide accurate directional information,\nleading to superfluous actions, and (2) performance drops significantly when\nviewpoint inconsistencies arise between training and application. To address\nthese challenges, we propose RSRNav, a simple yet effective method that reasons\nspatial relationships between the goal and current observations as navigation\nguidance. Specifically, we model the spatial relationship by constructing\ncorrelations between the goal and current observations, which are then passed\nto the policy network for action prediction. These correlations are\nprogressively refined using fine-grained cross-correlation and direction-aware\ncorrelation for more precise navigation. Extensive evaluation of RSRNav on\nthree benchmark datasets demonstrates superior navigation performance,\nparticularly in the \"user-matched goal\" setting, highlighting its potential for\nreal-world applications.", "AI": {"tldr": "RSRNav\u901a\u8fc7\u5efa\u6a21\u76ee\u6807\u4e0e\u5f53\u524d\u89c2\u6d4b\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u63d0\u5347\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u7279\u5f81\u65b9\u5411\u4fe1\u606f\u4e0d\u8db3\u548c\u89c6\u89d2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u8bed\u4e49\u7279\u5f81\u7f3a\u4e4f\u51c6\u786e\u65b9\u5411\u4fe1\u606f\u4e14\u5bf9\u89c6\u89d2\u53d8\u5316\u654f\u611f\uff0c\u5bfc\u81f4\u5bfc\u822a\u6548\u7387\u4f4e\u3002", "method": "RSRNav\u901a\u8fc7\u6784\u5efa\u76ee\u6807\u4e0e\u5f53\u524d\u89c2\u6d4b\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u5229\u7528\u7ec6\u7c92\u5ea6\u4e92\u76f8\u5173\u548c\u65b9\u5411\u611f\u77e5\u76f8\u5173\u6027\u9010\u6b65\u4f18\u5316\u5bfc\u822a\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cRSRNav\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u7528\u6237\u5339\u914d\u76ee\u6807\u573a\u666f\u4e2d\u3002", "conclusion": "RSRNav\u901a\u8fc7\u7a7a\u95f4\u5173\u7cfb\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17823", "pdf": "https://arxiv.org/pdf/2504.17823", "abs": "https://arxiv.org/abs/2504.17823", "authors": ["Darcy Kim", "Aida Kalender", "Sennay Ghebreab", "Giovanni Sileno"], "title": "The Cloud Weaving Model for AI development", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "presented at alt.CHI 2025, Yokohama", "summary": "While analysing challenges in pilot projects developing AI with marginalized\ncommunities, we found it difficult to express them within commonly used\nparadigms. We therefore constructed an alternative conceptual framework to\nground AI development in the social fabric -- the Cloud Weaving Model --\ninspired (amongst others) by indigenous knowledge, motifs from nature, and\nEastern traditions. This paper introduces and elaborates on the fundamental\nelements of the model (clouds, spiders, threads, spiderwebs, and weather) and\ntheir interpretation in an AI context. The framework is then applied to\ncomprehend patterns observed in co-creation pilots approaching marginalized\ncommunities, highlighting neglected yet relevant dimensions for responsible AI\ndevelopment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cCloud Weaving Model\u201d\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u7528\u4e8e\u5c06AI\u5f00\u53d1\u4e0e\u793e\u4f1a\u80cc\u666f\u7ed3\u5408\uff0c\u7279\u522b\u5173\u6ce8\u8fb9\u7f18\u5316\u793e\u533a\u3002", "motivation": "\u73b0\u6709\u8303\u5f0f\u96be\u4ee5\u8868\u8fbe\u8fb9\u7f18\u5316\u793e\u533a\u5728AI\u5f00\u53d1\u4e2d\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u571f\u8457\u77e5\u8bc6\u3001\u81ea\u7136\u56fe\u6848\u548c\u4e1c\u65b9\u4f20\u7edf\uff0c\u6784\u5efa\u4e86\u201cCloud Weaving Model\u201d\uff0c\u5e76\u8be6\u7ec6\u89e3\u91ca\u5176\u6838\u5fc3\u5143\u7d20\uff08\u4e91\u3001\u8718\u86db\u3001\u7ebf\u3001\u8718\u86db\u7f51\u548c\u5929\u6c14\uff09\u5728AI\u4e2d\u7684\u542b\u4e49\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5e94\u7528\u4e8e\u5206\u6790\u8fb9\u7f18\u5316\u793e\u533a\u7684\u5171\u540c\u521b\u9020\u8bd5\u70b9\u9879\u76ee\uff0c\u63ed\u793a\u4e86\u8d1f\u8d23\u4efbAI\u5f00\u53d1\u4e2d\u88ab\u5ffd\u89c6\u7684\u7ef4\u5ea6\u3002", "conclusion": "\u201cCloud Weaving Model\u201d\u4e3aAI\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0e\u793e\u4f1a\u80cc\u666f\u7d27\u5bc6\u7ed3\u5408\u7684\u65b0\u89c6\u89d2\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fb9\u7f18\u5316\u793e\u533a\u3002"}}
{"id": "2504.17996", "pdf": "https://arxiv.org/pdf/2504.17996", "abs": "https://arxiv.org/abs/2504.17996", "authors": ["Yuanbing Ouyang", "Yizhuo Liang", "Qingpeng Li", "Xinfei Guo", "Yiming Luo", "Di Wu", "Hao Wang", "Yushan Pan"], "title": "Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) excel in semantic segmentation but demand\nsignificant computation, posing challenges for deployment on\nresource-constrained devices. Existing token pruning methods often overlook\nfundamental visual data characteristics. This study introduces 'LVTP', a\nprogressive token pruning framework guided by multi-scale Tsallis entropy and\nlow-level visual features with twice clustering. It integrates high-level\nsemantics and basic visual attributes for precise segmentation. A novel dynamic\nscoring mechanism using multi-scale Tsallis entropy weighting overcomes\nlimitations of traditional single-parameter entropy. The framework also\nincorporates low-level feature analysis to preserve critical edge information\nwhile optimizing computational cost. As a plug-and-play module, it requires no\narchitectural changes or additional training. Evaluations across multiple\ndatasets show 20%-45% computational reductions with negligible performance\nloss, outperforming existing methods in balancing cost and accuracy, especially\nin complex edge regions.", "AI": {"tldr": "LVTP\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6Tsallis\u71b5\u548c\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\u7684\u6e10\u8fdb\u5f0ftoken\u526a\u679d\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u4e14\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "Vision Transformers\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u73b0\u6709token\u526a\u679d\u65b9\u6cd5\u5e38\u5ffd\u7565\u89c6\u89c9\u6570\u636e\u7684\u57fa\u672c\u7279\u5f81\u3002", "method": "\u63d0\u51faLVTP\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6Tsallis\u71b5\u548c\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u4e24\u6b21\u805a\u7c7b\uff0c\u52a8\u6001\u8bc4\u5206\u673a\u5236\u4f18\u5316\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b020%-45%\u8ba1\u7b97\u91cf\u51cf\u5c11\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u5c24\u5176\u5728\u590d\u6742\u8fb9\u7f18\u533a\u57df\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LVTP\u5728\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u6216\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2504.17824", "pdf": "https://arxiv.org/pdf/2504.17824", "abs": "https://arxiv.org/abs/2504.17824", "authors": ["Yibin Wang", "Jiaxi Xie", "Lakshminarayanan Subramanian"], "title": "EduBot -- Can LLMs Solve Personalized Learning and Programming Assignments?", "categories": ["cs.SE", "cs.AI"], "comment": "Published at AAAI 2025 AI4EDU Workshop", "summary": "The prevalence of Large Language Models (LLMs) is revolutionizing the process\nof writing code. General and code LLMs have shown impressive performance in\ngenerating standalone functions and code-completion tasks with one-shot\nqueries. However, the ability to solve comprehensive programming tasks with\nrecursive requests and bug fixes remains questionable. In this paper, we\npropose EduBot, an intelligent automated assistant system that combines\nconceptual knowledge teaching, end-to-end code development, personalized\nprogramming through recursive prompt-driven methods, and debugging with limited\nhuman interventions powered by LLMs. We show that EduBot can solve complicated\nprogramming tasks consisting of sub-tasks with increasing difficulties ranging\nfrom conceptual to coding questions by recursive automatic prompt-driven\nsystems without finetuning on LLMs themselves. To further evaluate EduBot's\nperformance, we design and conduct a benchmark suite consisting of 20 scenarios\nin algorithms, machine learning, and real-world problems. The result shows that\nEduBot can complete most scenarios in less than 20 minutes. Based on the\nbenchmark suites, we perform a comparative study to take different LLMs as the\nbackbone and to verify EduBot's compatibility and robustness across LLMs with\nvarying capabilities. We believe that EduBot is an exploratory approach to\nexplore the potential of pre-trained LLMs in multi-step reasoning and code\ngeneration for solving personalized assignments with knowledge learning and\ncode generation.", "AI": {"tldr": "EduBot\u662f\u4e00\u4e2a\u57fa\u4e8eLLMs\u7684\u667a\u80fd\u7f16\u7a0b\u52a9\u624b\uff0c\u901a\u8fc7\u9012\u5f52\u63d0\u793a\u9a71\u52a8\u65b9\u6cd5\u89e3\u51b3\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\uff0c\u5305\u62ec\u6559\u5b66\u3001\u4ee3\u7801\u751f\u6210\u548c\u8c03\u8bd5\uff0c\u65e0\u9700\u5fae\u8c03LLMs\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3LLMs\u5728\u591a\u6b65\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u9012\u5f52\u9700\u6c42\u548c\u8c03\u8bd5\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6982\u5ff5\u6559\u5b66\u3001\u7aef\u5230\u7aef\u4ee3\u7801\u5f00\u53d1\u3001\u4e2a\u6027\u5316\u7f16\u7a0b\u548c\u8c03\u8bd5\uff0c\u901a\u8fc7\u9012\u5f52\u63d0\u793a\u9a71\u52a8\u65b9\u6cd5\u5b9e\u73b0\u81ea\u52a8\u5316\u3002", "result": "\u572820\u4e2a\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEduBot\u80fd\u572820\u5206\u949f\u5185\u5b8c\u6210\u5927\u90e8\u5206\u4efb\u52a1\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540cLLMs\u4e0a\u7684\u517c\u5bb9\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "EduBot\u5c55\u793a\u4e86\u9884\u8bad\u7ec3LLMs\u5728\u89e3\u51b3\u4e2a\u6027\u5316\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u591a\u6b65\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18020", "pdf": "https://arxiv.org/pdf/2504.18020", "abs": "https://arxiv.org/abs/2504.18020", "authors": ["Guyue Hu", "Siyuan Song", "Yukun Kang", "Zhu Yin", "Gangming Zhao", "Chenglong Li", "Jin Tang"], "title": "Federated Client-tailored Adapter for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation in X-ray images is beneficial for computer-aided\ndiagnosis and lesion localization. Existing methods mainly fall into a\ncentralized learning paradigm, which is inapplicable in the practical medical\nscenario that only has access to distributed data islands. Federated Learning\nhas the potential to offer a distributed solution but struggles with heavy\ntraining instability due to client-wise domain heterogeneity (including\ndistribution diversity and class imbalance). In this paper, we propose a novel\nFederated Client-tailored Adapter (FCA) framework for medical image\nsegmentation, which achieves stable and client-tailored adaptive segmentation\nwithout sharing sensitive local data. Specifically, the federated adapter stirs\nuniversal knowledge in off-the-shelf medical foundation models to stabilize the\nfederated training process. In addition, we develop two client-tailored\nfederated updating strategies that adaptively decompose the adapter into common\nand individual components, then globally and independently update the parameter\ngroups associated with common client-invariant and individual client-specific\nunits, respectively. They further stabilize the heterogeneous federated\nlearning process and realize optimal client-tailored instead of sub-optimal\nglobal-compromised segmentation models. Extensive experiments on three\nlarge-scale datasets demonstrate the effectiveness and superiority of the\nproposed FCA framework for federated medical segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5ba2\u6237\u7aef\u5b9a\u5236\u9002\u914d\u5668\uff08FCA\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u6570\u636e\u5c9b\u5c7f\u548c\u5ba2\u6237\u7aef\u57df\u5f02\u8d28\u6027\u5e26\u6765\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u5b66\u4e60\uff0c\u4e0d\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u6570\u636e\u5c9b\u5c7f\u7684\u5b9e\u9645\u533b\u7597\u573a\u666f\uff0c\u4e14\u8054\u90a6\u5b66\u4e60\u56e0\u5ba2\u6237\u7aef\u57df\u5f02\u8d28\u6027\uff08\u5982\u5206\u5e03\u591a\u6837\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\uff09\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "FCA\u6846\u67b6\u5229\u7528\u73b0\u6210\u533b\u5b66\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u901a\u7528\u77e5\u8bc6\u7a33\u5b9a\u8054\u90a6\u8bad\u7ec3\uff0c\u5e76\u5f00\u53d1\u4e24\u79cd\u5ba2\u6237\u7aef\u5b9a\u5236\u8054\u90a6\u66f4\u65b0\u7b56\u7565\uff0c\u5c06\u9002\u914d\u5668\u5206\u89e3\u4e3a\u516c\u5171\u548c\u4e2a\u4f53\u7ec4\u4ef6\uff0c\u5206\u522b\u66f4\u65b0\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFCA\u6846\u67b6\u5728\u8054\u90a6\u533b\u5b66\u5206\u5272\u4e2d\u5177\u6709\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "FCA\u6846\u67b6\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u5ba2\u6237\u7aef\u5b9a\u5236\u7684\u81ea\u9002\u5e94\u5206\u5272\uff0c\u65e0\u9700\u5171\u4eab\u654f\u611f\u672c\u5730\u6570\u636e\u3002"}}
{"id": "2504.18025", "pdf": "https://arxiv.org/pdf/2504.18025", "abs": "https://arxiv.org/abs/2504.18025", "authors": ["Shuanglin Yan", "Neng Dong", "Shuang Li", "Rui Yan", "Hao Tang", "Jing Qin"], "title": "ShapeSpeak: Body Shape-Aware Textual Alignment for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Visible-Infrared Person Re-identification (VIReID) aims to match visible and\ninfrared pedestrian images, but the modality differences and the complexity of\nidentity features make it challenging. Existing methods rely solely on identity\nlabel supervision, which makes it difficult to fully extract high-level\nsemantic information. Recently, vision-language pre-trained models have been\nintroduced to VIReID, enhancing semantic information modeling by generating\ntextual descriptions. However, such methods do not explicitly model body shape\nfeatures, which are crucial for cross-modal matching. To address this, we\npropose an effective Body Shape-aware Textual Alignment (BSaTa) framework that\nexplicitly models and utilizes body shape information to improve VIReID\nperformance. Specifically, we design a Body Shape Textual Alignment (BSTA)\nmodule that extracts body shape information using a human parsing model and\nconverts it into structured text representations via CLIP. We also design a\nText-Visual Consistency Regularizer (TVCR) to ensure alignment between body\nshape textual representations and visual body shape features. Furthermore, we\nintroduce a Shape-aware Representation Learning (SRL) mechanism that combines\nMulti-text Supervision and Distribution Consistency Constraints to guide the\nvisual encoder to learn modality-invariant and discriminative identity\nfeatures, thus enhancing modality invariance. Experimental results demonstrate\nthat our method achieves superior performance on the SYSU-MM01 and RegDB\ndatasets, validating its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBSaTa\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8eab\u4f53\u5f62\u72b6\u4fe1\u606f\u6765\u63d0\u5347\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\uff08VIReID\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8eab\u4efd\u6807\u7b7e\u76d1\u7763\uff0c\u96be\u4ee5\u5145\u5206\u63d0\u53d6\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\uff0c\u4e14\u672a\u663e\u5f0f\u5efa\u6a21\u8eab\u4f53\u5f62\u72b6\u7279\u5f81\u3002", "method": "\u8bbe\u8ba1\u4e86BSTA\u6a21\u5757\u63d0\u53d6\u8eab\u4f53\u5f62\u72b6\u4fe1\u606f\u5e76\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\uff0c\u5f15\u5165TVCR\u786e\u4fdd\u6587\u672c\u4e0e\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\uff0c\u4ee5\u53caSRL\u673a\u5236\u7ed3\u5408\u591a\u6587\u672c\u76d1\u7763\u548c\u5206\u5e03\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "result": "\u5728SYSU-MM01\u548cRegDB\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "BSaTa\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8eab\u4f53\u5f62\u72b6\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86VIReID\u6027\u80fd\u3002"}}
{"id": "2504.18027", "pdf": "https://arxiv.org/pdf/2504.18027", "abs": "https://arxiv.org/abs/2504.18027", "authors": ["Zezhou Chen", "Zhaoxiang Liu", "Kai Wang", "Kohou Wang", "Shiguo Lian"], "title": "A Large Vision-Language Model based Environment Perception System for Visually Impaired People", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted by IROS2024(9 pages, 8 figures)", "summary": "It is a challenging task for visually impaired people to perceive their\nsurrounding environment due to the complexity of the natural scenes. Their\npersonal and social activities are thus highly limited. This paper introduces a\nLarge Vision-Language Model(LVLM) based environment perception system which\nhelps them to better understand the surrounding environment, by capturing the\ncurrent scene they face with a wearable device, and then letting them retrieve\nthe analysis results through the device. The visually impaired people could\nacquire a global description of the scene by long pressing the screen to\nactivate the LVLM output, retrieve the categories of the objects in the scene\nresulting from a segmentation model by tapping or swiping the screen, and get a\ndetailed description of the objects they are interested in by double-tapping\nthe screen. To help visually impaired people more accurately perceive the\nworld, this paper proposes incorporating the segmentation result of the RGB\nimage as external knowledge into the input of LVLM to reduce the LVLM's\nhallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the\nsystem could provide a more accurate description of the scene compared to\nQwen-VL-Chat, exploratory experiments show that the system helps visually\nimpaired people to perceive the surrounding environment effectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u73af\u5883\u611f\u77e5\u7cfb\u7edf\uff0c\u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u901a\u8fc7\u53ef\u7a7f\u6234\u8bbe\u5907\u6355\u6349\u548c\u5206\u6790\u5468\u56f4\u73af\u5883\uff0c\u63d0\u4f9b\u573a\u666f\u63cf\u8ff0\u3001\u7269\u4f53\u5206\u7c7b\u548c\u8be6\u7ec6\u63cf\u8ff0\u529f\u80fd\u3002", "motivation": "\u89c6\u969c\u4eba\u58eb\u56e0\u81ea\u7136\u573a\u666f\u7684\u590d\u6742\u6027\u96be\u4ee5\u611f\u77e5\u73af\u5883\uff0c\u9650\u5236\u4e86\u5176\u4e2a\u4eba\u548c\u793e\u4ea4\u6d3b\u52a8\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408LVLM\u548c\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7RGB\u56fe\u50cf\u5206\u5272\u7ed3\u679c\u51cf\u5c11LVLM\u7684\u5e7b\u89c9\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u573a\u666f\u63cf\u8ff0\u3002", "result": "\u5728POPE\u3001MME\u548cLLaVA-QA90\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u6bd4Qwen-VL-Chat\u66f4\u51c6\u786e\uff0c\u4e14\u80fd\u6709\u6548\u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u611f\u77e5\u73af\u5883\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u6280\u672f\u6539\u8fdb\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u73af\u5883\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17827", "pdf": "https://arxiv.org/pdf/2504.17827", "abs": "https://arxiv.org/abs/2504.17827", "authors": ["Bingye Zhou", "Caiyang Yu"], "title": "Evolution Meets Diffusion: Efficient Neural Architecture Generation", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEDNAG\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u8fdb\u5316\u7b97\u6cd5\u548c\u6269\u6563\u6a21\u578b\uff0c\u9ad8\u6548\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e0e\u901f\u5ea6\u3002", "motivation": "\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u8ba1\u7b97\u548c\u65f6\u95f4\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u6269\u6563\u6a21\u578b\u5728\u5168\u5c40\u641c\u7d22\u80fd\u529b\u548c\u6548\u7387\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "EDNAG\u5229\u7528\u8fdb\u5316\u7b97\u6cd5\u6a21\u62df\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u901a\u8fc7\u9002\u5e94\u5ea6\u5f15\u5bfc\u4ece\u968f\u673a\u5206\u5e03\u751f\u6210\u6700\u4f18\u67b6\u6784\u3002", "result": "EDNAG\u5728\u67b6\u6784\u4f18\u5316\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u51c6\u786e\u7387\u63d0\u534710.45%\uff0c\u63a8\u7406\u901f\u5ea6\u5e73\u5747\u63d0\u534750\u500d\u3002", "conclusion": "EDNAG\u7ed3\u5408\u8fdb\u5316\u7b56\u7565\u4e0e\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u67b6\u6784\u751f\u6210\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2504.18032", "pdf": "https://arxiv.org/pdf/2504.18032", "abs": "https://arxiv.org/abs/2504.18032", "authors": ["Chen Chen", "Daochang Liu", "Mubarak Shah", "Chang Xu"], "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. Project page:\n  https://chenchen-usyd.github.io/PRSS-Project-Page/", "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ncreating images highly aligned with user prompts, yet their proclivity for\nmemorizing training set images has sparked concerns about the originality of\nthe generated images and privacy issues, potentially leading to legal\ncomplications for both model owners and users, particularly when the memorized\nimages contain proprietary content. Although methods to mitigate these issues\nhave been suggested, enhancing privacy often results in a significant decrease\nin the utility of the outputs, as indicated by text-alignment scores. To bridge\nthe research gap, we introduce a novel method, PRSS, which refines the\nclassifier-free guidance approach in diffusion models by integrating prompt\nre-anchoring (PR) to improve privacy and incorporating semantic prompt search\n(SS) to enhance utility. Extensive experiments across various privacy levels\ndemonstrate that our approach consistently improves the privacy-utility\ntrade-off, establishing a new state-of-the-art.", "AI": {"tldr": "PRSS\u65b9\u6cd5\u901a\u8fc7\u6539\u8fdb\u6269\u6563\u6a21\u578b\u7684\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff0c\u7ed3\u5408\u63d0\u793a\u91cd\u65b0\u951a\u5b9a\uff08PR\uff09\u548c\u8bed\u4e49\u63d0\u793a\u641c\u7d22\uff08SS\uff09\uff0c\u5728\u9690\u79c1\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4e0e\u7528\u6237\u63d0\u793a\u9ad8\u5ea6\u4e00\u81f4\u7684\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u56fe\u50cf\u8bb0\u5fc6\u95ee\u9898\uff0c\u53ef\u80fd\u5f15\u53d1\u9690\u79c1\u548c\u6cd5\u5f8b\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u5347\u9690\u79c1\u7684\u540c\u65f6\u5f80\u5f80\u727a\u7272\u5b9e\u7528\u6027\u3002", "method": "PRSS\u65b9\u6cd5\u7ed3\u5408\u63d0\u793a\u91cd\u65b0\u951a\u5b9a\uff08PR\uff09\u548c\u8bed\u4e49\u63d0\u793a\u641c\u7d22\uff08SS\uff09\uff0c\u4f18\u5316\u9690\u79c1\u4e0e\u5b9e\u7528\u6027\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPRSS\u5728\u4e0d\u540c\u9690\u79c1\u7ea7\u522b\u4e0b\u5747\u80fd\u663e\u8457\u6539\u5584\u9690\u79c1-\u5b9e\u7528\u6027\u5e73\u8861\uff0c\u8fbe\u5230\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "PRSS\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5e73\u8861\u9690\u79c1\u4e0e\u5b9e\u7528\u6027\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.18040", "pdf": "https://arxiv.org/pdf/2504.18040", "abs": "https://arxiv.org/abs/2504.18040", "authors": ["Xiaoyi Liu", "Hao Tang"], "title": "Cabbage: A Differential Growth Framework for Open Surfaces", "categories": ["cs.CV"], "comment": null, "summary": "We propose Cabbage, a differential growth framework to model buckling\nbehavior in 3D open surfaces found in nature-like the curling of flower petals.\nCabbage creates high-quality triangular meshes free of self-intersection.\nCabbage-Shell is driven by edge subdivision which differentially increases\ndiscretization resolution. Shell forces expands the surface, generating\nbuckling over time. Feature-aware smoothing and remeshing ensures mesh quality.\nCorrective collision effectively prevents self-collision even in tight spaces.\nWe additionally provide Cabbage-Collision, and approximate alternative,\nfollowed by CAD-ready surface generation. Cabbage is the first open-source\neffort with this calibre and robustness, outperforming SOTA methods in its\nmorphological expressiveness, mesh quality, and stably generates large, complex\npatterns over hundreds of simulation steps. It is a source not only of\ncomputational modeling, digital fabrication, education, but also high-quality,\nannotated data for geometry processing and shape analysis.", "AI": {"tldr": "Cabbage\u662f\u4e00\u4e2a\u7528\u4e8e\u6a21\u62df3D\u5f00\u653e\u8868\u9762\uff08\u5982\u82b1\u74e3\u5377\u66f2\uff09\u7684\u5fae\u5206\u751f\u957f\u6846\u67b6\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u65e0\u81ea\u4ea4\u7684\u4e09\u89d2\u7f51\u683c\uff0c\u652f\u6301CAD-ready\u8868\u9762\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6a21\u62df\u81ea\u7136\u754c\u4e2d3D\u5f00\u653e\u8868\u9762\u7684\u5c48\u66f2\u884c\u4e3a\uff08\u5982\u82b1\u74e3\u5377\u66f2\uff09\uff0c\u5e76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u65e0\u81ea\u4ea4\u7684\u7f51\u683c\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u8fb9\u7f18\u7ec6\u5206\u9a71\u52a8\u5fae\u5206\u589e\u957f\uff08Cabbage-Shell\uff09\uff0c\u7ed3\u5408\u58f3\u529b\u3001\u7279\u5f81\u611f\u77e5\u5e73\u6ed1\u548c\u91cd\u7f51\u683c\u5316\uff0c\u4ee5\u53ca\u7ea0\u6b63\u78b0\u649e\u9632\u6b62\u81ea\u4ea4\u3002", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u7f51\u683c\uff0c\u5f62\u6001\u8868\u73b0\u529b\u5f3a\uff0c\u652f\u6301\u590d\u6742\u6a21\u5f0f\u6a21\u62df\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Cabbage\u662f\u9996\u4e2a\u5f00\u6e90\u7684\u9ad8\u6027\u80fd\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u5efa\u6a21\u3001\u6570\u5b57\u5236\u9020\u548c\u6559\u80b2\uff0c\u540c\u65f6\u4e3a\u51e0\u4f55\u5904\u7406\u548c\u5f62\u72b6\u5206\u6790\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6570\u636e\u3002"}}
{"id": "2504.18046", "pdf": "https://arxiv.org/pdf/2504.18046", "abs": "https://arxiv.org/abs/2504.18046", "authors": ["Guohao Huo", "Zibo Lin", "Zitong Wang", "Ruiting Dai", "Hao Tang"], "title": "DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ophthalmic diseases pose a significant global health challenge, yet\ntraditional diagnosis methods and existing single-eye deep learning approaches\noften fail to account for binocular pathological correlations. To address this,\nwe propose DMS-Net, a dual-modal multi-scale Siamese network for binocular\nfundus image classification. Our framework leverages weight-shared Siamese\nResNet-152 backbones to extract deep semantic features from paired fundus\nimages. To tackle challenges such as lesion boundary ambiguity and scattered\npathological distributions, we introduce a Multi-Scale Context-Aware Module\n(MSCAM) that integrates adaptive pooling and attention mechanisms for\nmulti-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion\n(DMFF) module enhances cross-modal interaction through spatial-semantic\nrecalibration and bidirectional attention, effectively combining global context\nand local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves\nstate-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8%\nCohen's kappa, demonstrating superior capability in detecting symmetric\npathologies and advancing clinical decision-making for ocular diseases.", "AI": {"tldr": "DMS-Net\u662f\u4e00\u79cd\u53cc\u6a21\u6001\u591a\u5c3a\u5ea6Siamese\u7f51\u7edc\uff0c\u7528\u4e8e\u53cc\u773c\u773c\u5e95\u56fe\u50cf\u5206\u7c7b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\u548c\u53cc\u6a21\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u548c\u5355\u773c\u6df1\u5ea6\u5b66\u4e60\u672a\u80fd\u8003\u8651\u53cc\u773c\u75c5\u7406\u76f8\u5173\u6027\uff0cDMS-Net\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6743\u91cd\u5171\u4eab\u7684Siamese ResNet-152\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\u548c\u53cc\u6a21\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\u3002", "result": "\u5728ODIR-5K\u6570\u636e\u96c6\u4e0a\u8fbe\u523080.5%\u51c6\u786e\u7387\u300186.1%\u53ec\u56de\u7387\u548c83.8% Cohen's kappa\u3002", "conclusion": "DMS-Net\u5728\u68c0\u6d4b\u5bf9\u79f0\u75c5\u7406\u548c\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.17833", "pdf": "https://arxiv.org/pdf/2504.17833", "abs": "https://arxiv.org/abs/2504.17833", "authors": ["Xiao Huang", "Zhengzhong Tu", "Xinyue Ye", "Michael Goodchild"], "title": "The Role of Open-Source LLMs in Shaping the Future of GeoAI", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are transforming geospatial artificial\nintelligence (GeoAI), offering new capabilities in data processing, spatial\nanalysis, and decision support. This paper examines the open-source paradigm's\npivotal role in this transformation. While proprietary LLMs offer\naccessibility, they often limit the customization, interoperability, and\ntransparency vital for specialized geospatial tasks. Conversely, open-source\nalternatives significantly advance Geographic Information Science (GIScience)\nby fostering greater adaptability, reproducibility, and community-driven\ninnovation. Open frameworks empower researchers to tailor solutions, integrate\ncutting-edge methodologies (e.g., reinforcement learning, advanced spatial\nindexing), and align with FAIR principles. However, the growing reliance on any\nLLM necessitates careful consideration of security vulnerabilities, ethical\nrisks, and robust governance for AI-generated geospatial outputs. Ongoing\ndebates on accessibility, regulation, and misuse underscore the critical need\nfor responsible AI development strategies. This paper argues that GIScience\nadvances best not through a single model type, but by cultivating a diverse,\ninteroperable ecosystem combining open-source foundations for innovation,\nbespoke geospatial models, and interdisciplinary collaboration. By critically\nevaluating the opportunities and challenges of open-source LLMs within the\nbroader GeoAI landscape, this work contributes to a nuanced discourse on\nleveraging AI to effectively advance spatial research, policy, and\ndecision-making in an equitable, sustainable, and scientifically rigorous\nmanner.", "AI": {"tldr": "\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728GeoAI\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5f3a\u8c03\u5176\u5b9a\u5236\u5316\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u900f\u660e\u6027\u4f18\u52bf\uff0c\u4f46\u4e5f\u9700\u5173\u6ce8\u5b89\u5168\u3001\u4f26\u7406\u548c\u6cbb\u7406\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8\u5f00\u6e90LLMs\u5982\u4f55\u63a8\u52a8\u5730\u7406\u4fe1\u606f\u79d1\u5b66\uff08GIScience\uff09\u7684\u521b\u65b0\uff0c\u540c\u65f6\u89e3\u51b3\u5176\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u5206\u6790\u5f00\u6e90\u4e0e\u4e13\u6709LLMs\u7684\u5bf9\u6bd4\uff0c\u5f3a\u8c03\u5f00\u6e90\u5728\u9002\u5e94\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u793e\u533a\u9a71\u52a8\u521b\u65b0\u65b9\u9762\u7684\u4f18\u52bf\u3002", "result": "\u5f00\u6e90LLMs\u4e3aGeoAI\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u4fc3\u8fdb\u7a7a\u95f4\u7814\u7a76\u548c\u51b3\u7b56\u652f\u6301\u3002", "conclusion": "GIScience\u5e94\u901a\u8fc7\u5f00\u6e90\u4e0e\u4e13\u6709\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u6784\u5efa\u8d1f\u8d23\u4efb\u3001\u53ef\u6301\u7eed\u7684AI\u53d1\u5c55\u7b56\u7565\u3002"}}
{"id": "2504.18049", "pdf": "https://arxiv.org/pdf/2504.18049", "abs": "https://arxiv.org/abs/2504.18049", "authors": ["Xin Li", "Wenhui Zhu", "Peijie Qiu", "Oana M. Dumitrascu", "Amal Youssef", "Yalin Wang"], "title": "A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the field of medical imaging, the advent of deep learning, especially the\napplication of convolutional neural networks (CNNs) has revolutionized the\nanalysis and interpretation of medical images. Nevertheless, deep learning\nmethods usually rely on large amounts of labeled data. In medical imaging\nresearch, the acquisition of high-quality labels is both expensive and\ndifficult. The introduction of Vision Transformers (ViT) and self-supervised\nlearning provides a pre-training strategy that utilizes abundant unlabeled\ndata, effectively alleviating the label acquisition challenge while broadening\nthe breadth of data utilization. However, ViT's high computational density and\nsubstantial demand for computing power, coupled with the lack of localization\ncharacteristics of its operations on image patches, limit its efficiency and\napplicability in many application scenarios. In this study, we employ\nnn-MobileNet, a lightweight CNN framework, to implement a BERT-style\nself-supervised learning approach. We pre-train the network on the unlabeled\nretinal fundus images from the UK Biobank to improve downstream application\nperformance. We validate the results of the pre-trained model on Alzheimer's\ndisease (AD), Parkinson's disease (PD), and various retinal diseases\nidentification. The results show that our approach can significantly improve\nperformance in the downstream tasks. In summary, this study combines the\nbenefits of CNNs with the capabilities of advanced self-supervised learning in\nhandling large-scale unlabeled data, demonstrating the potential of CNNs in the\npresence of label scarcity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f7b\u91cf\u7ea7CNN\uff08nn-MobileNet\uff09\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u6807\u8bb0\u7684\u89c6\u7f51\u819c\u56fe\u50cf\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u56f0\u96be\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\uff08\u5982ViT\uff09\u8ba1\u7b97\u9700\u6c42\u5927\u4e14\u7f3a\u4e4f\u5c40\u90e8\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8f7b\u91cf\u7ea7CNN\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528nn-MobileNet\u6846\u67b6\uff0c\u91c7\u7528BERT\u98ce\u683c\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u672a\u6807\u8bb0\u7684\u89c6\u7f51\u819c\u56fe\u50cf\uff08UK Biobank\uff09\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u5e15\u91d1\u68ee\u75c5\u53ca\u591a\u79cd\u89c6\u7f51\u819c\u75be\u75c5\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86CNN\u5728\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17838", "pdf": "https://arxiv.org/pdf/2504.17838", "abs": "https://arxiv.org/abs/2504.17838", "authors": ["Bernhard Jaeger", "Daniel Dauner", "Jens Bei\u00dfwenger", "Simon Gerstenecker", "Kashyap Chitta", "Andreas Geiger"], "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "We investigate reinforcement learning (RL) for privileged planning in\nautonomous driving. State-of-the-art approaches for this task are rule-based,\nbut these methods do not scale to the long tail. RL, on the other hand, is\nscalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum\nmultiple individual rewards, \\eg~progress, position, or orientation rewards. We\nshow that PPO fails to optimize a popular version of these rewards when the\nmini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single\nintuitive reward term: route completion. Infractions are penalized by\nterminating the episode or multiplicatively reducing route completion. We find\nthat PPO scales well with higher mini-batch sizes when trained with our simple\nreward, even improving performance. Training with large mini-batch sizes\nenables efficient scaling via distributed data parallelism. We scale PPO to\n300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The\nresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,\noutperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is\nthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and\n90.6 in reactive traffic on the Val14 benchmark while being an order of\nmagnitude faster than prior work.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u7279\u6743\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u7ebf\u5b8c\u6210\u5ea6\u7684\u7b80\u5355\u5956\u52b1\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86PPO\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6cd5\u591a\u4e3a\u57fa\u4e8e\u89c4\u5219\u7684\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u573a\u666f\uff1b\u800c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u56e0\u590d\u6742\u7684\u5956\u52b1\u8bbe\u8ba1\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8def\u7ebf\u5b8c\u6210\u5ea6\u4e3a\u6838\u5fc3\u7684\u7b80\u5355\u5956\u52b1\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\u6280\u672f\u6269\u5c55PPO\u7b97\u6cd5\u3002", "result": "\u5728CARLA\u548cnuPlan\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u660e\u663e\u3002", "conclusion": "\u7b80\u5355\u5956\u52b1\u8bbe\u8ba1\u7ed3\u5408\u5927\u89c4\u6a21\u5e76\u884c\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2504.18059", "pdf": "https://arxiv.org/pdf/2504.18059", "abs": "https://arxiv.org/abs/2504.18059", "authors": ["Prachi Garg", "Joseph K J", "Vineeth N Balasubramanian", "Necati Cihan Camgoz", "Chengde Wan", "Kenrick Kin", "Weiguang Si", "Shugao Ma", "Fernando De La Torre"], "title": "POET: Prompt Offset Tuning for Continual Human Action Adaptation", "categories": ["cs.CV"], "comment": "ECCV 2024 (Oral), webpage\n  https://humansensinglab.github.io/POET-continual-action-recognition/", "summary": "As extended reality (XR) is redefining how users interact with computing\ndevices, research in human action recognition is gaining prominence. Typically,\nmodels deployed on immersive computing devices are static and limited to their\ndefault set of classes. The goal of our research is to provide users and\ndevelopers with the capability to personalize their experience by adding new\naction classes to their device models continually. Importantly, a user should\nbe able to add new classes in a low-shot and efficient manner, while this\nprocess should not require storing or replaying any of user's sensitive\ntraining data. We formalize this problem as privacy-aware few-shot continual\naction recognition. Towards this end, we propose POET: Prompt-Offset Tuning.\nWhile existing prompt tuning approaches have shown great promise for continual\nlearning of image, text, and video modalities; they demand access to\nextensively pretrained transformers. Breaking away from this assumption, POET\ndemonstrates the efficacy of prompt tuning a significantly lightweight\nbackbone, pretrained exclusively on the base class data. We propose a novel\nspatio-temporal learnable prompt offset tuning approach, and are the first to\napply such prompt tuning to Graph Neural Networks. We contribute two new\nbenchmarks for our new problem setting in human action recognition: (i) NTU\nRGB+D dataset for activity recognition, and (ii) SHREC-2017 dataset for hand\ngesture recognition. We find that POET consistently outperforms comprehensive\nbenchmarks. Source code at\nhttps://github.com/humansensinglab/POET-continual-action-recognition.", "AI": {"tldr": "POET\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u611f\u77e5\u7684\u5c11\u6837\u672c\u6301\u7eed\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\u548c\u65f6\u7a7a\u53ef\u5b66\u4e60\u63d0\u793a\u504f\u79fb\u8c03\u4f18\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u8bbe\u5907\u9700\u8981\u4e2a\u6027\u5316\u52a8\u4f5c\u8bc6\u522b\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u9759\u6001\u4e14\u7f3a\u4e4f\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u63d0\u51faPOET\uff08Prompt-Offset Tuning\uff09\uff0c\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\u548c\u65f6\u7a7a\u63d0\u793a\u504f\u79fb\u8c03\u4f18\uff0c\u9002\u7528\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728NTU RGB+D\u548cSHREC-2017\u6570\u636e\u96c6\u4e0a\uff0cPOET\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "POET\u4e3a\u9690\u79c1\u611f\u77e5\u7684\u6301\u7eed\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8eXR\u8bbe\u5907\u3002"}}
{"id": "2504.17872", "pdf": "https://arxiv.org/pdf/2504.17872", "abs": "https://arxiv.org/abs/2504.17872", "authors": ["Max Muchen Sun", "Allison Pinosky", "Todd Murphey"], "title": "Flow Matching Ergodic Coverage", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "15 pages, 15 figures. Accepted to Robotics: Science and Systems (RSS)\n  2025. Project website: https://murpheylab.github.io/lqr-flow-matching/", "summary": "Ergodic coverage effectively generates exploratory behaviors for embodied\nagents by aligning the spatial distribution of the agent's trajectory with a\ntarget distribution, where the difference between these two distributions is\nmeasured by the ergodic metric. However, existing ergodic coverage methods are\nconstrained by the limited set of ergodic metrics available for control\nsynthesis, fundamentally limiting their performance. In this work, we propose\nan alternative approach to ergodic coverage based on flow matching, a technique\nwidely used in generative inference for efficient and scalable sampling. We\nformally derive the flow matching problem for ergodic coverage and show that it\nis equivalent to a linear quadratic regulator problem with a closed-form\nsolution. Our formulation enables alternative ergodic metrics from generative\ninference that overcome the limitations of existing ones. These metrics were\npreviously infeasible for control synthesis but can now be supported with no\ncomputational overhead. Specifically, flow matching with the Stein variational\ngradient flow enables control synthesis directly over the score function of the\ntarget distribution, improving robustness to the unnormalized distributions; on\nthe other hand, flow matching with the Sinkhorn divergence flow enables an\noptimal transport-based ergodic metric, improving coverage performance on\nnon-smooth distributions with irregular supports. We validate the improved\nperformance and competitive computational efficiency of our method through\ncomprehensive numerical benchmarks and across different nonlinear dynamics. We\nfurther demonstrate the practicality of our method through a series of drawing\nand erasing tasks on a Franka robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u904d\u5386\u8986\u76d6\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u904d\u5386\u5ea6\u91cf\u6709\u9650\u800c\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u904d\u5386\u8986\u76d6\u65b9\u6cd5\u53d7\u9650\u4e8e\u53ef\u7528\u7684\u904d\u5386\u5ea6\u91cf\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6d41\u5339\u914d\u6280\u672f\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u91c7\u7528\u6d41\u5339\u914d\u6280\u672f\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\u95ee\u9898\uff0c\u5e76\u5229\u7528\u751f\u6210\u63a8\u7406\u4e2d\u7684\u66ff\u4ee3\u904d\u5386\u5ea6\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u3002", "conclusion": "\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u904d\u5386\u8986\u76d6\u65b9\u6cd5\u6269\u5c55\u4e86\u53ef\u7528\u5ea6\u91cf\uff0c\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.18068", "pdf": "https://arxiv.org/pdf/2504.18068", "abs": "https://arxiv.org/abs/2504.18068", "authors": ["Zhuohao Yan", "Shaoquan Feng", "Xingxing Li", "Yuxuan Zhou", "Chunxi Xia", "Shengyu Li"], "title": "S3MOT: Monocular 3D Object Tracking with Selective State Space Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and reliable multi-object tracking (MOT) in 3D space is essential\nfor advancing robotics and computer vision applications. However, it remains a\nsignificant challenge in monocular setups due to the difficulty of mining 3D\nspatiotemporal associations from 2D video streams. In this work, we present\nthree innovative techniques to enhance the fusion and exploitation of\nheterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State\nSpace Model (HSSM), a novel data association mechanism that compresses\ncontextual tracking cues across multiple paths, enabling efficient and\ncomprehensive assignment decisions with linear complexity. HSSM features a\nglobal receptive field and dynamic weights, in contrast to traditional linear\nassignment algorithms that rely on hand-crafted association costs. (2) We\npropose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI\npooling by directly using dense feature maps for contrastive learning, thus\nimproving object re-identification accuracy under challenging conditions such\nas varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation\nthrough VeloSSM, an encoder-decoder architecture that models temporal\ndependencies in velocity to capture motion dynamics, overcoming the limitations\nof frame-based 3D inference. Experiments on the KITTI public test benchmark\ndemonstrate the effectiveness of our method, achieving a new state-of-the-art\nperformance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best\nby significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness\nand efficiency for monocular 3D MOT tasks. The code and models are available at\nhttps://github.com/bytepioneerX/s3mot.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u79cd\u521b\u65b0\u6280\u672f\uff08HSSM\u3001FCOE\u3001VeloSSM\uff09\u63d0\u5347\u5355\u76ee3D\u591a\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523076.86 HOTA\uff0c\u4f18\u4e8e\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u5355\u76ee3D\u591a\u76ee\u6807\u8ddf\u8e2a\u5728\u673a\u5668\u4eba\u5b66\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece2D\u89c6\u9891\u6d41\u4e2d\u6316\u63983D\u65f6\u7a7a\u5173\u8054\u3002", "method": "1. HSSM\uff1a\u9ad8\u6548\u6570\u636e\u5173\u8054\u673a\u5236\uff1b2. FCOE\uff1a\u6539\u8fdb\u76ee\u6807\u91cd\u8bc6\u522b\uff1b3. VeloSSM\uff1a\u589e\u5f3a6-DoF\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5728KITTI\u6d4b\u8bd5\u4e2d\u8fbe\u523076.86 HOTA\uff0c31 FPS\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5355\u76ee3D\u591a\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.17878", "pdf": "https://arxiv.org/pdf/2504.17878", "abs": "https://arxiv.org/abs/2504.17878", "authors": ["Xu Wang", "Yiquan Wang", "Tin-yeh Huang"], "title": "Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted at the AI4NA workshop at ICLR 2025. 18pages, 4figures", "summary": "In the looming post-quantum era, traditional cryptographic systems are\nincreasingly vulnerable to quantum computing attacks that can compromise their\nmathematical foundations. To address this critical challenge, we propose\ncrypto-ncRNA-a bio-convergent cryptographic framework that leverages the\ndynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy,\nquantum-resistant keys and produce unpredictable ciphertexts. The framework\nemploys a novel, multi-stage process: encoding plaintext into RNA sequences,\npredicting and manipulating RNA secondary structures using advanced algorithms,\nand deriving cryptographic keys through the intrinsic physical unclonability of\nRNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's\nencryption speed is marginally lower than that of AES, it significantly\noutperforms RSA in terms of efficiency and scalability while achieving a 100%\npass rate on the NIST SP 800-22 randomness tests. These results demonstrate\nthat crypto-ncRNA offers a promising and robust approach for securing digital\ninfrastructures against the evolving threats posed by quantum computing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7f16\u7801RNA\uff08ncRNA\uff09\u7684\u751f\u7269\u5bc6\u7801\u5b66\u6846\u67b6crypto-ncRNA\uff0c\u7528\u4e8e\u751f\u6210\u6297\u91cf\u5b50\u8ba1\u7b97\u7684\u9ad8\u71b5\u5bc6\u94a5\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u5bc6\u6587\u3002", "motivation": "\u4f20\u7edf\u5bc6\u7801\u7cfb\u7edf\u5728\u91cf\u5b50\u8ba1\u7b97\u65f6\u4ee3\u9762\u4e34\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u578b\u6297\u91cf\u5b50\u653b\u51fb\u7684\u52a0\u5bc6\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u660e\u6587\u7f16\u7801\u4e3aRNA\u5e8f\u5217\uff0c\u5229\u7528RNA\u7684\u52a8\u6001\u6298\u53e0\u7279\u6027\u751f\u6210\u5bc6\u94a5\uff0c\u7ed3\u5408\u7269\u7406\u4e0d\u53ef\u514b\u9686\u6027\u5b9e\u73b0\u52a0\u5bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0ccrypto-ncRNA\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8eRSA\uff0c\u5e76\u901a\u8fc7NIST\u968f\u673a\u6027\u6d4b\u8bd5\u3002", "conclusion": "crypto-ncRNA\u4e3a\u62b5\u5fa1\u91cf\u5b50\u8ba1\u7b97\u5a01\u80c1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18087", "pdf": "https://arxiv.org/pdf/2504.18087", "abs": "https://arxiv.org/abs/2504.18087", "authors": ["Weipeng Tan", "Chuming Lin", "Chengming Xu", "FeiFan Xu", "Xiaobin Hu", "Xiaozhong Ji", "Junwei Zhu", "Chengjie Wang", "Yanwei Fu"], "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2409.03270", "summary": "Recent advances in Talking Head Generation (THG) have achieved impressive lip\nsynchronization and visual quality through diffusion models; yet existing\nmethods struggle to generate emotionally expressive portraits while preserving\nspeaker identity. We identify three critical limitations in current emotional\ntalking head generation: insufficient utilization of audio's inherent emotional\ncues, identity leakage in emotion representations, and isolated learning of\nemotion correlations. To address these challenges, we propose a novel framework\ndubbed as DICE-Talk, following the idea of disentangling identity with emotion,\nand then cooperating emotions with similar characteristics. First, we develop a\ndisentangled emotion embedder that jointly models audio-visual emotional cues\nthrough cross-modal attention, representing emotions as identity-agnostic\nGaussian distributions. Second, we introduce a correlation-enhanced emotion\nconditioning module with learnable Emotion Banks that explicitly capture\ninter-emotion relationships through vector quantization and attention-based\nfeature aggregation. Third, we design an emotion discrimination objective that\nenforces affective consistency during the diffusion process through\nlatent-space classification. Extensive experiments on MEAD and HDTF datasets\ndemonstrate our method's superiority, outperforming state-of-the-art approaches\nin emotion accuracy while maintaining competitive lip-sync performance.\nQualitative results and user studies further confirm our method's ability to\ngenerate identity-preserving portraits with rich, correlated emotional\nexpressions that naturally adapt to unseen identities.", "AI": {"tldr": "DICE-Talk \u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8eab\u4efd\u4e0e\u60c5\u611f\u5e76\u5408\u4f5c\u76f8\u4f3c\u60c5\u611f\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u60c5\u611f\u8bf4\u8bdd\u5934\u751f\u6210\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u60c5\u611f\u8868\u8fbe\u4e30\u5bcc\u7684\u8096\u50cf\u65f6\u5b58\u5728\u8eab\u4efd\u6cc4\u6f0f\u3001\u60c5\u611f\u7ebf\u7d22\u5229\u7528\u4e0d\u8db3\u4ee5\u53ca\u60c5\u611f\u5173\u8054\u5b64\u7acb\u5b66\u4e60\u7684\u95ee\u9898\u3002", "method": "1. \u5f00\u53d1\u89e3\u8026\u60c5\u611f\u5d4c\u5165\u5668\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5efa\u6a21\u97f3\u9891-\u89c6\u89c9\u60c5\u611f\u7ebf\u7d22\uff1b2. \u5f15\u5165\u76f8\u5173\u6027\u589e\u5f3a\u7684\u60c5\u611f\u6761\u4ef6\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u60c5\u611f\u94f6\u884c\u6355\u83b7\u60c5\u611f\u5173\u7cfb\uff1b3. \u8bbe\u8ba1\u60c5\u611f\u5224\u522b\u76ee\u6807\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5206\u7c7b\u5f3a\u5236\u60c5\u611f\u4e00\u81f4\u6027\u3002", "result": "\u5728 MEAD \u548c HDTF \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u60c5\u611f\u51c6\u786e\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5507\u540c\u6b65\u6027\u80fd\u3002", "conclusion": "DICE-Talk \u80fd\u591f\u751f\u6210\u8eab\u4efd\u4fdd\u7559\u4e14\u60c5\u611f\u4e30\u5bcc\u7684\u8096\u50cf\uff0c\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u8eab\u4efd\u3002"}}
{"id": "2504.18112", "pdf": "https://arxiv.org/pdf/2504.18112", "abs": "https://arxiv.org/abs/2504.18112", "authors": ["Deepak Ghimire", "Byoungjun Kim", "Donghoon Kim", "SungHwan Jeong"], "title": "Study on Real-Time Road Surface Reconstruction Using Stereo Vision", "categories": ["cs.CV"], "comment": "Stereo Vision, Efficient CNN, Pruning, Optimization. 2025 Intelligent\n  Information and Control Conference (IICC 2025), Jeonju, Korea", "summary": "Road surface reconstruction plays a crucial role in autonomous driving,\nproviding essential information for safe and smooth navigation. This paper\nenhances the RoadBEV [1] framework for real-time inference on edge devices by\noptimizing both efficiency and accuracy. To achieve this, we proposed to apply\nIsomorphic Global Structured Pruning to the stereo feature extraction backbone,\nreducing network complexity while maintaining performance. Additionally, the\nhead network is redesigned with an optimized hourglass structure, dynamic\nattention heads, reduced feature channels, mixed precision inference, and\nefficient probability volume computation. Our approach improves inference speed\nwhile achieving lower reconstruction error, making it well-suited for real-time\nroad surface reconstruction in autonomous driving.", "AI": {"tldr": "\u672c\u6587\u4f18\u5316\u4e86RoadBEV\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u5168\u5c40\u7ed3\u6784\u5316\u526a\u679d\u548c\u91cd\u65b0\u8bbe\u8ba1\u7684\u5934\u90e8\u7f51\u7edc\uff0c\u63d0\u5347\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u63a8\u7406\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u8def\u8868\u91cd\u5efa\u5bf9\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u548c\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5f02\u6784\u5168\u5c40\u7ed3\u6784\u5316\u526a\u679d\u4f18\u5316\u7acb\u4f53\u7279\u5f81\u63d0\u53d6\u4e3b\u5e72\u7f51\u7edc\uff0c\u5e76\u91cd\u65b0\u8bbe\u8ba1\u5934\u90e8\u7f51\u7edc\uff0c\u5305\u62ec\u4f18\u5316\u6c99\u6f0f\u7ed3\u6784\u3001\u52a8\u6001\u6ce8\u610f\u529b\u5934\u3001\u51cf\u5c11\u7279\u5f81\u901a\u9053\u3001\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u548c\u9ad8\u6548\u6982\u7387\u4f53\u79ef\u8ba1\u7b97\u3002", "result": "\u65b9\u6cd5\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u91cd\u5efa\u8bef\u5dee\u3002", "conclusion": "\u4f18\u5316\u540e\u7684\u6846\u67b6\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b9e\u65f6\u8def\u8868\u91cd\u5efa\u3002"}}
{"id": "2504.17901", "pdf": "https://arxiv.org/pdf/2504.17901", "abs": "https://arxiv.org/abs/2504.17901", "authors": ["Benned Hedegaard", "Ziyi Yang", "Yichen Wei", "Ahmed Jaafar", "Stefanie Tellex", "George Konidaris", "Naman Shah"], "title": "Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Task and motion planning is a well-established approach for solving\nlong-horizon robot planning problems. However, traditional methods assume that\neach task-level robot action, or skill, can be reduced to kinematic motion\nplanning. In this work, we address the challenge of planning with both\nkinematic skills and closed-loop motor controllers that go beyond kinematic\nconsiderations. We propose a novel method that integrates these controllers\ninto motion planning using Composable Interaction Primitives (CIPs), enabling\nthe use of diverse, non-composable pre-learned skills in hierarchical robot\nplanning. Toward validating our Task and Skill Planning (TASP) approach, we\ndescribe ongoing robot experiments in real-world scenarios designed to\ndemonstrate how CIPs can allow a mobile manipulator robot to effectively\ncombine motion planning with general-purpose skills to accomplish complex\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fd0\u52a8\u89c4\u5212\u548c\u95ed\u73af\u7535\u673a\u63a7\u5236\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7CIPs\u5b9e\u73b0\u975e\u7ec4\u5408\u9884\u5b66\u4e60\u6280\u80fd\u7684\u5206\u5c42\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u4efb\u52a1\u7ea7\u673a\u5668\u4eba\u52a8\u4f5c\u53ef\u7b80\u5316\u4e3a\u8fd0\u52a8\u89c4\u5212\uff0c\u4f46\u5b9e\u9645\u9700\u5904\u7406\u8d85\u8d8a\u8fd0\u52a8\u5b66\u8003\u8651\u7684\u95ed\u73af\u63a7\u5236\u5668\u3002", "method": "\u4f7f\u7528Composable Interaction Primitives (CIPs)\u5c06\u95ed\u73af\u63a7\u5236\u5668\u96c6\u6210\u5230\u8fd0\u52a8\u89c4\u5212\u4e2d\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u573a\u666f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TASP\u65b9\u6cd5\uff0c\u5c55\u793a\u79fb\u52a8\u673a\u68b0\u81c2\u5982\u4f55\u7ed3\u5408\u8fd0\u52a8\u89c4\u5212\u548c\u901a\u7528\u6280\u80fd\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "CIPs\u4e3a\u673a\u5668\u4eba\u5206\u5c42\u89c4\u5212\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u548c\u591a\u6837\u6027\uff0c\u652f\u6301\u590d\u6742\u4efb\u52a1\u7684\u5b8c\u6210\u3002"}}
{"id": "2504.18127", "pdf": "https://arxiv.org/pdf/2504.18127", "abs": "https://arxiv.org/abs/2504.18127", "authors": ["Jingfan Yang", "Hu Gao", "Ying Zhang", "Depeng Dang"], "title": "Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network", "categories": ["cs.CV"], "comment": null, "summary": "Spacecraft image super-resolution seeks to enhance low-resolution spacecraft\nimages into high-resolution ones. Although existing arbitrary-scale\nsuper-resolution methods perform well on general images, they tend to overlook\nthe difference in features between the spacecraft core region and the large\nblack space background, introducing irrelevant noise. In this paper, we propose\na salient region-guided spacecraft image arbitrary-scale super-resolution\nnetwork (SGSASR), which uses features from the spacecraft core salient regions\nto guide latent modulation and achieve arbitrary-scale super-resolution.\nSpecifically, we design a spacecraft core region recognition block (SCRRB) that\nidentifies the core salient regions in spacecraft images using a pre-trained\nsaliency detection model. Furthermore, we present an adaptive-weighted feature\nfusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft\ncore region features with general image features by dynamic weight parameter to\nenhance the response of the core salient regions. Experimental results\ndemonstrate that the proposed SGSASR outperforms state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u533a\u57df\u5f15\u5bfc\u7684\u822a\u5929\u5668\u56fe\u50cf\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\uff08SGSASR\uff09\uff0c\u901a\u8fc7\u8bc6\u522b\u822a\u5929\u5668\u6838\u5fc3\u533a\u57df\u5e76\u9009\u62e9\u6027\u878d\u5408\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u822a\u5929\u5668\u56fe\u50cf\u4e2d\u5ffd\u7565\u4e86\u6838\u5fc3\u533a\u57df\u4e0e\u9ed1\u8272\u80cc\u666f\u7684\u7279\u5f81\u5dee\u5f02\uff0c\u5bfc\u81f4\u566a\u58f0\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u822a\u5929\u5668\u6838\u5fc3\u533a\u57df\u8bc6\u522b\u5757\uff08SCRRB\uff09\u548c\u81ea\u9002\u5e94\u52a0\u6743\u7279\u5f81\u878d\u5408\u589e\u5f3a\u673a\u5236\uff08AFFEM\uff09\uff0c\u901a\u8fc7\u663e\u8457\u533a\u57df\u5f15\u5bfc\u8c03\u5236\u5b9e\u73b0\u8d85\u5206\u8fa8\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSGSASR\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SGSASR\u901a\u8fc7\u663e\u8457\u533a\u57df\u5f15\u5bfc\u548c\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u822a\u5929\u5668\u56fe\u50cf\u7684\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\u3002"}}
{"id": "2504.17921", "pdf": "https://arxiv.org/pdf/2504.17921", "abs": "https://arxiv.org/abs/2504.17921", "authors": ["Mateo Espinosa Zarlenga", "Gabriele Dominici", "Pietro Barbiero", "Zohreh Shams", "Mateja Jamnik"], "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.HC"], "comment": null, "summary": "In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions.", "AI": {"tldr": "\u7814\u7a76\u6982\u5ff5\u6a21\u578b\uff08CMs\uff09\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u8f93\u5165\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709CMs\u5b58\u5728\u6cc4\u6f0f\u4e2d\u6bd2\u95ee\u9898\uff0c\u5e76\u63d0\u51faMixCEM\u6a21\u578b\u4ee5\u52a8\u6001\u5229\u7528\u6cc4\u6f0f\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8\u6982\u5ff5\u6a21\u578b\u5728OOD\u8f93\u5165\u4e0b\u7684\u8868\u73b0\u53ca\u6982\u5ff5\u5e72\u9884\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faMixCEM\u6a21\u578b\uff0c\u52a8\u6001\u5229\u7528\u6cc4\u6f0f\u4fe1\u606f\uff0c\u6539\u8fdb\u6982\u5ff5\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "MixCEM\u5728\u6709\u65e0\u6982\u5ff5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u63d0\u5347\u5206\u5e03\u5185\u548cOOD\u6837\u672c\u7684\u51c6\u786e\u6027\u3002", "conclusion": "MixCEM\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709CMs\u7684\u6cc4\u6f0f\u4e2d\u6bd2\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728OOD\u8f93\u5165\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.18136", "pdf": "https://arxiv.org/pdf/2504.18136", "abs": "https://arxiv.org/abs/2504.18136", "authors": ["Liugang Lu", "Dabin He", "Congxiang Liu", "Zhixiang Deng"], "title": "MASF-YOLO: An Improved YOLOv11 Network for Small Object Detection on Drone View", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of Unmanned Aerial Vehicle (UAV) and computer\nvision technologies, object detection from UAV perspectives has emerged as a\nprominent research area. However, challenges for detection brought by the\nextremely small proportion of target pixels, significant scale variations of\nobjects, and complex background information in UAV images have greatly limited\nthe practical applications of UAV. To address these challenges, we propose a\nnovel object detection network Multi-scale Context Aggregation and\nScale-adaptive Fusion YOLO (MASF-YOLO), which is developed based on YOLOv11.\nFirstly, to tackle the difficulty of detecting small objects in UAV images, we\ndesign a Multi-scale Feature Aggregation Module (MFAM), which significantly\nimproves the detection accuracy of small objects through parallel multi-scale\nconvolutions and feature fusion. Secondly, to mitigate the interference of\nbackground noise, we propose an Improved Efficient Multi-scale Attention Module\n(IEMA), which enhances the focus on target regions through feature grouping,\nparallel sub-networks, and cross-spatial learning. Thirdly, we introduce a\nDimension-Aware Selective Integration Module (DASI), which further enhances\nmulti-scale feature fusion capabilities by adaptively weighting and fusing\nlow-dimensional features and high-dimensional features. Finally, we conducted\nextensive performance evaluations of our proposed method on the VisDrone2019\ndataset. Compared to YOLOv11-s, MASFYOLO-s achieves improvements of 4.6% in\nmAP@0.5 and 3.5% in mAP@0.5:0.95 on the VisDrone2019 validation set.\nRemarkably, MASF-YOLO-s outperforms YOLOv11-m while requiring only\napproximately 60% of its parameters and 65% of its computational cost.\nFurthermore, comparative experiments with state-of-the-art detectors confirm\nthat MASF-YOLO-s maintains a clear competitive advantage in both detection\naccuracy and model efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMASF-YOLO\u7684\u65b0\u578b\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u9488\u5bf9\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u3001\u80cc\u666f\u566a\u58f0\u548c\u5c3a\u5ea6\u53d8\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u5c0f\u76ee\u6807\u50cf\u7d20\u6bd4\u4f8b\u6781\u4f4e\u3001\u7269\u4f53\u5c3a\u5ea6\u53d8\u5316\u5927\u548c\u80cc\u666f\u590d\u6742\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\u6a21\u5757\uff08MFAM\uff09\u3001\u6539\u8fdb\u7684\u9ad8\u6548\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\uff08IEMA\uff09\u548c\u7ef4\u5ea6\u611f\u77e5\u9009\u62e9\u6027\u96c6\u6210\u6a21\u5757\uff08DASI\uff09\uff0c\u4ee5\u63d0\u5347\u5c0f\u76ee\u6807\u68c0\u6d4b\u548c\u80cc\u666f\u566a\u58f0\u6291\u5236\u80fd\u529b\u3002", "result": "\u5728VisDrone2019\u6570\u636e\u96c6\u4e0a\uff0cMASF-YOLO-s\u76f8\u6bd4YOLOv11-s\u5728mAP@0.5\u548cmAP@0.5:0.95\u4e0a\u5206\u522b\u63d0\u5347\u4e864.6%\u548c3.5%\uff0c\u4e14\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "MASF-YOLO\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6a21\u578b\u6548\u7387\u4e0a\u5747\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u65e0\u4eba\u673a\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17964", "pdf": "https://arxiv.org/pdf/2504.17964", "abs": "https://arxiv.org/abs/2504.17964", "authors": ["Celia Chen", "Alex Leitch"], "title": "Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content", "categories": ["cs.HC", "cs.AI"], "comment": "Under review at ACM Web Science Conference 2025's Human-GenAI\n  Interactions Workshop, 4 pages", "summary": "This paper examines how graduate students develop frameworks for evaluating\nmachine-generated expertise in web-based interactions with large language\nmodels (LLMs). Through a qualitative study combining surveys, LLM interaction\ntranscripts, and in-depth interviews with 14 graduate students, we identify\npatterns in how these emerging professionals assess and engage with\nAI-generated content. Our findings reveal that students construct evaluation\nframeworks shaped by three main factors: professional identity, verification\ncapabilities, and system navigation experience. Rather than uniformly accepting\nor rejecting LLM outputs, students protect domains central to their\nprofessional identities while delegating others--with managers preserving\nconceptual work, designers safeguarding creative processes, and programmers\nmaintaining control over core technical expertise. These evaluation frameworks\nare further influenced by students' ability to verify different types of\ncontent and their experience navigating complex systems. This research\ncontributes to web science by highlighting emerging human-genAI interaction\npatterns and suggesting how platforms might better support users in developing\neffective frameworks for evaluating machine-generated expertise signals in\nAI-mediated web environments.", "AI": {"tldr": "\u7814\u7a76\u751f\u5982\u4f55\u8bc4\u4f30\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ea4\u4e92\u4e2d\u7684\u673a\u5668\u751f\u6210\u5185\u5bb9\uff0c\u7814\u7a76\u53d1\u73b0\u5176\u8bc4\u4f30\u6846\u67b6\u53d7\u4e13\u4e1a\u8eab\u4efd\u3001\u9a8c\u8bc1\u80fd\u529b\u548c\u7cfb\u7edf\u5bfc\u822a\u7ecf\u9a8c\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u7814\u7a76\u751f\u5728AI\u751f\u6210\u5185\u5bb9\u4e2d\u5982\u4f55\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u7406\u89e3\u4eba\u673a\u4ea4\u4e92\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u7814\u7a76\uff08\u8c03\u67e5\u3001LLM\u4ea4\u4e92\u8bb0\u5f55\u548c14\u540d\u7814\u7a76\u751f\u7684\u6df1\u5ea6\u8bbf\u8c08\uff09\u5206\u6790\u8bc4\u4f30\u6a21\u5f0f\u3002", "result": "\u5b66\u751f\u8bc4\u4f30\u6846\u67b6\u53d7\u4e13\u4e1a\u8eab\u4efd\u3001\u9a8c\u8bc1\u80fd\u529b\u548c\u7cfb\u7edf\u5bfc\u822a\u7ecf\u9a8c\u5f71\u54cd\uff0c\u4e0d\u540c\u9886\u57df\u5b66\u751f\u4fdd\u62a4\u5176\u6838\u5fc3\u4e13\u4e1a\u9886\u57df\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u65b0\u6a21\u5f0f\uff0c\u5efa\u8bae\u5e73\u53f0\u652f\u6301\u7528\u6237\u6784\u5efa\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2504.18152", "pdf": "https://arxiv.org/pdf/2504.18152", "abs": "https://arxiv.org/abs/2504.18152", "authors": ["Yi-Xing Peng", "Qize Yang", "Yu-Ming Tang", "Shenghao Fu", "Kun-Yu Lin", "Xihan Wei", "Wei-Shi Zheng"], "title": "ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Fine-grained understanding of human actions and poses in videos is essential\nfor human-centric AI applications. In this work, we introduce ActionArt, a\nfine-grained video-caption dataset designed to advance research in\nhuman-centric multimodal understanding. Our dataset comprises thousands of\nvideos capturing a broad spectrum of human actions, human-object interactions,\nand diverse scenarios, each accompanied by detailed annotations that\nmeticulously label every limb movement. We develop eight sub-tasks to evaluate\nthe fine-grained understanding capabilities of existing large multimodal models\nacross different dimensions. Experimental results indicate that, while current\nlarge multimodal models perform commendably on various tasks, they often fall\nshort in achieving fine-grained understanding. We attribute this limitation to\nthe scarcity of meticulously annotated data, which is both costly and difficult\nto scale manually. Since manual annotations are costly and hard to scale, we\npropose proxy tasks to enhance the model perception ability in both spatial and\ntemporal dimensions. These proxy tasks are carefully crafted to be driven by\ndata automatically generated from existing MLLMs, thereby reducing the reliance\non costly manual labels. Experimental results show that the proposed proxy\ntasks significantly narrow the gap toward the performance achieved with\nmanually annotated fine-grained data.", "AI": {"tldr": "ActionArt\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u89c6\u9891\u63cf\u8ff0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u4eba\u7c7b\u52a8\u4f5c\u7684\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7684\u4ee3\u7406\u4efb\u52a1\u51cf\u5c11\u5bf9\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "motivation": "\u7ec6\u7c92\u5ea6\u7406\u89e3\u4eba\u7c7b\u52a8\u4f5c\u548c\u59ff\u6001\u5bf9AI\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u56e0\u7f3a\u4e4f\u7cbe\u7ec6\u6807\u6ce8\u6570\u636e\u800c\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1ActionArt\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u5316\u4eba\u7c7b\u52a8\u4f5c\u89c6\u9891\u53ca\u8be6\u7ec6\u6807\u6ce8\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u81ea\u52a8\u751f\u6210\u6570\u636e\u7684\u4ee3\u7406\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4ee3\u7406\u4efb\u52a1\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u4ee3\u7406\u4efb\u52a1\u4e3a\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u4f9d\u8d56\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u7406\u89e3\u7684\u7814\u7a76\u3002"}}
{"id": "2504.17979", "pdf": "https://arxiv.org/pdf/2504.17979", "abs": "https://arxiv.org/abs/2504.17979", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Bharadwaj Dogga", "Nick Ernest", "Tim Arnett", "Kelly Cohen"], "title": "Fuzzy-RRT for Obstacle Avoidance in a 2-DOF Semi-Autonomous Surgical Robotic Arm", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 5 figures. Submitted to NAFIPS 2025 Conference (North\n  American Fuzzy Information Processing Society). Includes results on Fuzzy-RRT\n  performance in surgical robotics path planning", "summary": "AI-driven semi-autonomous robotic surgery is essential for addressing the\nmedical challenges of long-duration interplanetary missions, where limited crew\nsizes and communication delays restrict traditional surgical approaches.\nCurrent robotic surgery systems require full surgeon control, demanding\nextensive expertise and limiting feasibility in space. We propose a novel\nadaptation of the Fuzzy Rapidly-exploring Random Tree algorithm for obstacle\navoidance and collaborative control in a two-degree-of-freedom robotic arm\nmodeled on the Miniaturized Robotic-Assisted surgical system. It was found that\nthe Fuzzy Rapidly-exploring Random Tree algorithm resulted in an 743 percent\nimprovement to path search time and 43 percent improvement to path cost.", "AI": {"tldr": "AI\u9a71\u52a8\u7684\u534a\u81ea\u4e3b\u673a\u5668\u4eba\u624b\u672f\u901a\u8fc7\u6539\u8fdb\u7684Fuzzy Rapidly-exploring Random Tree\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u5f84\u641c\u7d22\u65f6\u95f4\u548c\u6210\u672c\uff0c\u9002\u7528\u4e8e\u592a\u7a7a\u4efb\u52a1\u4e2d\u7684\u624b\u672f\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u957f\u671f\u661f\u9645\u4efb\u52a1\u4e2d\u7684\u533b\u7597\u6311\u6218\uff0c\u4f20\u7edf\u624b\u672f\u65b9\u6cd5\u53d7\u9650\u4e8e\u901a\u4fe1\u5ef6\u8fdf\u548c\u4eba\u5458\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eFuzzy Rapidly-exploring Random Tree\u7b97\u6cd5\u7684\u4e24\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u81c2\u534f\u4f5c\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u8def\u5f84\u641c\u7d22\u65f6\u95f4\u63d0\u5347743%\uff0c\u8def\u5f84\u6210\u672c\u964d\u4f4e43%\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u624b\u672f\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u592a\u7a7a\u4efb\u52a1\u3002"}}
{"id": "2504.18158", "pdf": "https://arxiv.org/pdf/2504.18158", "abs": "https://arxiv.org/abs/2504.18158", "authors": ["Jiahao Zhang", "Bowen Wang", "Hong Liu", "Liangzhi Li", "Yuta Nakashima", "Hajime Nagahara"], "title": "E-InMeMo: Enhanced Prompting for Visual In-Context Learning", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Large-scale models trained on extensive datasets have become the standard due\nto their strong generalizability across diverse tasks. In-context learning\n(ICL), widely used in natural language processing, leverages these models by\nproviding task-specific prompts without modifying their parameters. This\nparadigm is increasingly being adapted for computer vision, where models\nreceive an input-output image pair, known as an in-context pair, alongside a\nquery image to illustrate the desired output. However, the success of visual\nICL largely hinges on the quality of these prompts. To address this, we propose\nEnhanced Instruct Me More (E-InMeMo), a novel approach that incorporates\nlearnable perturbations into in-context pairs to optimize prompting. Through\nextensive experiments on standard vision tasks, E-InMeMo demonstrates superior\nperformance over existing state-of-the-art methods. Notably, it improves mIoU\nscores by 7.99 for foreground segmentation and by 17.04 for single object\ndetection when compared to the baseline without learnable prompts. These\nresults highlight E-InMeMo as a lightweight yet effective strategy for\nenhancing visual ICL. Code is publicly available at:\nhttps://github.com/Jackieam/E-InMeMo", "AI": {"tldr": "E-InMeMo\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6270\u52a8\u4f18\u5316\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9ICL\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u63d0\u793a\u8d28\u91cf\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u53ef\u5b66\u4e60\u63d0\u793a\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faE-InMeMo\uff0c\u5728\u4e0a\u4e0b\u6587\u5bf9\u4e2d\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6270\u52a8\u4ee5\u4f18\u5316\u63d0\u793a\u3002", "result": "\u5728\u6807\u51c6\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0cE-InMeMo\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cmIoU\u63d0\u53477.99\uff08\u524d\u666f\u5206\u5272\uff09\u548c17.04\uff08\u5355\u76ee\u6807\u68c0\u6d4b\uff09\u3002", "conclusion": "E-InMeMo\u662f\u4e00\u79cd\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u89c6\u89c9ICL\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2504.18010", "pdf": "https://arxiv.org/pdf/2504.18010", "abs": "https://arxiv.org/abs/2504.18010", "authors": ["Zilin Huang", "Zihao Sheng", "Zhengyang Wan", "Yansong Qu", "Yuhao Luo", "Boyue Wang", "Pei Li", "Yen-Jung Chen", "Jiancong Chen", "Keke Long", "Jiayi Meng", "Yue Leng", "Sikai Chen"], "title": "Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "15 pages, 7 figures", "summary": "Recent advances in autonomous system simulation platforms have significantly\nenhanced the safe and scalable testing of driving policies. However, existing\nsimulators do not yet fully meet the needs of future transportation research,\nparticularly in modeling socially-aware driving agents and enabling effective\nhuman-AI collaboration. This paper introduces Sky-Drive, a novel distributed\nmulti-agent simulation platform that addresses these limitations through four\nkey innovations: (a) a distributed architecture for synchronized simulation\nacross multiple terminals; (b) a multi-modal human-in-the-loop framework\nintegrating diverse sensors to collect rich behavioral data; (c) a human-AI\ncollaboration mechanism supporting continuous and adaptive knowledge exchange;\nand (d) a digital twin (DT) framework for constructing high-fidelity virtual\nreplicas of real-world transportation environments. Sky-Drive supports diverse\napplications such as autonomous vehicle (AV)-vulnerable road user (VRU)\ninteraction modeling, human-in-the-loop training, socially-aware reinforcement\nlearning, personalized driving policy, and customized scenario generation.\nFuture extensions will incorporate foundation models for context-aware decision\nsupport and hardware-in-the-loop (HIL) testing for real-world validation. By\nbridging scenario generation, data collection, algorithm training, and hardware\nintegration, Sky-Drive has the potential to become a foundational platform for\nthe next generation of socially-aware and human-centered autonomous\ntransportation research. The demo video and code are available\nat:https://sky-lab-uw.github.io/Sky-Drive-website/", "AI": {"tldr": "Sky-Drive\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u4eff\u771f\u5e73\u53f0\uff0c\u901a\u8fc7\u56db\u9879\u521b\u65b0\u6280\u672f\u89e3\u51b3\u73b0\u6709\u6a21\u62df\u5668\u5728\u793e\u4f1a\u611f\u77e5\u9a7e\u9a76\u548c\u4eba\u7c7b-AI\u534f\u4f5c\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u5e73\u53f0\u672a\u80fd\u6ee1\u8db3\u672a\u6765\u4ea4\u901a\u7814\u7a76\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u793e\u4f1a\u611f\u77e5\u9a7e\u9a76\u548c\u4eba\u7c7b-AI\u534f\u4f5c\u65b9\u9762\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\u3001\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\u3001\u4eba\u7c7b-AI\u534f\u4f5c\u673a\u5236\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u3002", "result": "Sky-Drive\u652f\u6301\u591a\u79cd\u5e94\u7528\uff0c\u5982\u81ea\u52a8\u9a7e\u9a76\u4e0e\u5f31\u52bf\u9053\u8def\u7528\u6237\u4ea4\u4e92\u5efa\u6a21\u3001\u793e\u4f1a\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u7b49\u3002", "conclusion": "Sky-Drive\u6709\u671b\u6210\u4e3a\u4e0b\u4e00\u4ee3\u793e\u4f1a\u611f\u77e5\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u7684\u57fa\u7840\u5e73\u53f0\u3002"}}
{"id": "2504.18165", "pdf": "https://arxiv.org/pdf/2504.18165", "abs": "https://arxiv.org/abs/2504.18165", "authors": ["Michel Gokan Khan", "Renan Guarese", "Fabian Johnson", "Xi Vincent Wang", "Anders Bergman", "Benjamin Edvinsson", "Mario Romero", "J\u00e9r\u00e9my Vachier", "Jan Kronqvist"], "title": "PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning\nframework that combines camera and sensory data with 3D Gaussian Splatting and\ncomputer vision models for digital twinning, object tracking, and Key\nPerformance Indicators (KPIs) extraction in industrial production lines. By\nutilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam\noffers a semi-automated approach to object tracking and spatial mapping,\nenabling digital twins that capture real-time KPIs such as availability,\nperformance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts\nin the production line. We validate the effectiveness of PerfCam through a\npractical deployment within realistic test production lines in the\npharmaceutical industry and contribute an openly published dataset to support\nfurther research and development in the field. The results demonstrate\nPerfCam's ability to deliver actionable insights through its precise digital\ntwin capabilities, underscoring its value as an effective tool for developing\nusable digital twins in smart manufacturing environments and extracting\noperational analytics.", "AI": {"tldr": "PerfCam\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u7ed3\u5408\u76f8\u673a\u548c\u4f20\u611f\u5668\u6570\u636e\u30013D\u9ad8\u65af\u6cfc\u6e85\u53ca\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff0c\u7528\u4e8e\u5de5\u4e1a\u751f\u4ea7\u7ebf\u4e2d\u7684\u6570\u5b57\u5b6a\u751f\u3001\u5bf9\u8c61\u8ddf\u8e2a\u548cKPI\u63d0\u53d6\u3002", "motivation": "\u4e3a\u5de5\u4e1a\u751f\u4ea7\u7ebf\u63d0\u4f9b\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u548cKPI\u63d0\u53d6\u5de5\u5177\uff0c\u4ee5\u63d0\u5347\u667a\u80fd\u5236\u9020\u7684\u6548\u7387\u548c\u53ef\u64cd\u4f5c\u6027\u3002", "method": "\u5229\u75283D\u91cd\u5efa\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u5b9e\u73b0\u534a\u81ea\u52a8\u5316\u7684\u5bf9\u8c61\u8ddf\u8e2a\u548c\u7a7a\u95f4\u6620\u5c04\u3002", "result": "\u5728\u5236\u836f\u884c\u4e1a\u7684\u5b9e\u9645\u751f\u4ea7\u7ebf\u4e2d\u9a8c\u8bc1\u4e86PerfCam\u7684\u6709\u6548\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u3002\u7ed3\u679c\u8868\u660e\u5176\u80fd\u63d0\u4f9b\u7cbe\u786e\u7684\u6570\u5b57\u5b6a\u751f\u548c\u64cd\u4f5c\u5206\u6790\u3002", "conclusion": "PerfCam\u662f\u667a\u80fd\u5236\u9020\u73af\u5883\u4e2d\u5f00\u53d1\u53ef\u7528\u6570\u5b57\u5b6a\u751f\u548c\u63d0\u53d6\u64cd\u4f5c\u5206\u6790\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.18012", "pdf": "https://arxiv.org/pdf/2504.18012", "abs": "https://arxiv.org/abs/2504.18012", "authors": ["Zhuang Yu", "Shiliang Sun", "Jing Zhao", "Tengfei Song", "Hao Yang"], "title": "Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Machine Translation (MMT) aims to improve translation quality by\nleveraging auxiliary modalities such as images alongside textual input. While\nrecent advances in large-scale pre-trained language and vision models have\nsignificantly benefited unimodal natural language processing tasks, their\neffectiveness and role in MMT remain underexplored. In this work, we conduct a\nsystematic study on the impact of pre-trained encoders and decoders in\nmultimodal translation models. Specifically, we analyze how different training\nstrategies, from training from scratch to using pre-trained and partially\nfrozen components, affect translation performance under a unified MMT\nframework. Experiments are carried out on the Multi30K and CoMMuTE dataset\nacross English-German and English-French translation tasks. Our results reveal\nthat pre-training plays a crucial yet asymmetrical role in multimodal settings:\npre-trained decoders consistently yield more fluent and accurate outputs, while\npre-trained encoders show varied effects depending on the quality of\nvisual-text alignment. Furthermore, we provide insights into the interplay\nbetween modality fusion and pre-trained components, offering guidance for\nfuture architecture design in multimodal translation systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5728\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\uff08MMT\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u800c\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u6548\u679c\u5219\u53d6\u51b3\u4e8e\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u7684\u8d28\u91cf\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\u5728\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6709\u6548\u6027\u548c\u4f5c\u7528\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\uff08\u4ece\u96f6\u8bad\u7ec3\u5230\u4f7f\u7528\u9884\u8bad\u7ec3\u53ca\u90e8\u5206\u51bb\u7ed3\u7ec4\u4ef6\uff09\u5bf9\u7ffb\u8bd1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5728Multi30K\u548cCoMMuTE\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u9884\u8bad\u7ec3\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u8d77\u5173\u952e\u4f46\u4e0d\u5bf9\u79f0\u4f5c\u7528\uff1a\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u80fd\u751f\u6210\u66f4\u6d41\u7545\u51c6\u786e\u7684\u8f93\u51fa\uff0c\u800c\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u6548\u679c\u56e0\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u8d28\u91cf\u800c\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u7ec4\u4ef6\u4e0e\u6a21\u6001\u878d\u5408\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u7ffb\u8bd1\u7cfb\u7edf\u7684\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2504.18179", "pdf": "https://arxiv.org/pdf/2504.18179", "abs": "https://arxiv.org/abs/2504.18179", "authors": ["Lovro Sindicic", "Ivica Kopriva"], "title": "Label-independent hyperparameter-free self-supervised single-view deep subspace clustering", "categories": ["cs.CV", "cs.LG", "68", "I.5.3; I.4.6; I.4.10"], "comment": "35 pages; 1 figure; 10 Tables", "summary": "Deep subspace clustering (DSC) algorithms face several challenges that hinder\ntheir widespread adoption across variois application domains. First, clustering\nquality is typically assessed using only the encoder's output layer,\ndisregarding valuable information present in the intermediate layers. Second,\nmost DSC approaches treat representation learning and subspace clustering as\nindependent tasks, limiting their effectiveness. Third, they assume the\navailability of a held-out dataset for hyperparameter tuning, which is often\nimpractical in real-world scenarios. Fourth, learning termination is commonly\nbased on clustering error monitoring, requiring external labels. Finally, their\nperformance often depends on post-processing techniques that rely on labeled\ndata. To address this limitations, we introduce a novel single-view DSC\napproach that: (i) minimizes a layer-wise self expression loss using a joint\nrepresentation matrix; (ii) optimizes a subspace-structured norm to enhance\nclustering quality; (iii) employs a multi-stage sequential learning framework,\nconsisting of pre-training and fine-tuning, enabling the use of multiple\nregularization terms without hyperparameter tuning; (iv) incorporates a\nrelative error-based self-stopping mechanism to terminate training without\nlabels; and (v) retains a fixed number of leading coefficients in the learned\nrepresentation matrix based on prior knowledge. We evaluate the proposed method\non six datasets representing faces, digits, and objects. The results show that\nour method outperforms most linear SC algorithms with careffulyl tuned\nhyperparameters while maintaining competitive performance with the best\nperforming linear appoaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u89c6\u56fe\u6df1\u5ea6\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fe1\u606f\u5229\u7528\u3001\u4efb\u52a1\u72ec\u7acb\u6027\u3001\u8d85\u53c2\u6570\u8c03\u4f18\u3001\u5b66\u4e60\u7ec8\u6b62\u548c\u6570\u636e\u4f9d\u8d56\u7b49\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b50\u7a7a\u95f4\u805a\u7c7b\u7b97\u6cd5\u5b58\u5728\u591a\u4e2a\u95ee\u9898\uff0c\u5982\u4ec5\u4f7f\u7528\u7f16\u7801\u5668\u8f93\u51fa\u5c42\u8bc4\u4f30\u805a\u7c7b\u8d28\u91cf\u3001\u5c06\u8868\u793a\u5b66\u4e60\u4e0e\u5b50\u7a7a\u95f4\u805a\u7c7b\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\u3001\u4f9d\u8d56\u5916\u90e8\u6807\u7b7e\u7b49\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u8868\u793a\u77e9\u9635\u7684\u5c42\u95f4\u81ea\u8868\u8fbe\u635f\u5931\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u4f18\u5316\u5b50\u7a7a\u95f4\u7ed3\u6784\u8303\u6570\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u5f15\u5165\u81ea\u505c\u6b62\u673a\u5236\uff0c\u5e76\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u4fdd\u7559\u56fa\u5b9a\u6570\u91cf\u7684\u7cfb\u6570\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u591a\u6570\u7ebf\u6027\u5b50\u7a7a\u95f4\u805a\u7c7b\u7b97\u6cd5\uff0c\u5e76\u4e0e\u6027\u80fd\u6700\u4f73\u7684\u7ebf\u6027\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u6539\u8fdb\u4fe1\u606f\u5229\u7528\u548c\u4efb\u52a1\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b50\u7a7a\u95f4\u805a\u7c7b\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.18026", "pdf": "https://arxiv.org/pdf/2504.18026", "abs": "https://arxiv.org/abs/2504.18026", "authors": ["Emiliano Penaloza", "Tianyue H. Zhan", "Laurent Charlin", "Mateo Espinosa Zarlenga"], "title": "Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI\nsystems by constraining their decisions on a set of human understandable\nconcepts. However, CBMs typically assume that datasets contains accurate\nconcept labels an assumption often violated in practice, which we show can\nsignificantly degrade performance (by 25% in some cases). To address this, we\nintroduce the Concept Preference Optimization (CPO) objective, a new loss\nfunction based on Direct Preference Optimization, which effectively mitigates\nthe negative impact of concept mislabeling on CBM performance. We provide an\nanalysis on some key properties of the CPO objective showing it directly\noptimizes for the concept's posterior distribution, and contrast it against\nBinary Cross Entropy (BCE) where we show CPO is inherently less sensitive to\nconcept noise. We empirically confirm our analysis finding that CPO\nconsistently outperforms BCE in three real world datasets with and without\nadded label noise.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570CPO\uff0c\u7528\u4e8e\u89e3\u51b3\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u4e2d\u6982\u5ff5\u6807\u7b7e\u9519\u8bef\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "CBMs\u4f9d\u8d56\u51c6\u786e\u7684\u6982\u5ff5\u6807\u7b7e\uff0c\u4f46\u5b9e\u9645\u6570\u636e\u4e2d\u6807\u7b7e\u9519\u8bef\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff08\u53ef\u8fbe25%\uff09\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684CPO\u76ee\u6807\u51fd\u6570\uff0c\u4f18\u5316\u6982\u5ff5\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5bf9\u6807\u7b7e\u566a\u58f0\u66f4\u9c81\u68d2\u3002", "result": "CPO\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfBCE\u635f\u5931\u51fd\u6570\uff0c\u5c24\u5176\u5728\u6807\u7b7e\u566a\u58f0\u5b58\u5728\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "CPO\u6709\u6548\u89e3\u51b3\u4e86CBMs\u4e2d\u6982\u5ff5\u6807\u7b7e\u9519\u8bef\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2504.18190", "pdf": "https://arxiv.org/pdf/2504.18190", "abs": "https://arxiv.org/abs/2504.18190", "authors": ["Brun\u00f3 B. Englert", "Tommie Kerssies", "Gijs Dubbelman"], "title": "What is the Added Value of UDA in the VFM Era?", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) can improve a perception model's\ngeneralization to an unlabeled target domain starting from a labeled source\ndomain. UDA using Vision Foundation Models (VFMs) with synthetic source data\ncan achieve generalization performance comparable to fully-supervised learning\nwith real target data. However, because VFMs have strong generalization from\ntheir pre-training, more straightforward, source-only fine-tuning can also\nperform well on the target. As data scenarios used in academic research are not\nnecessarily representative for real-world applications, it is currently unclear\n(a) how UDA behaves with more representative and diverse data and (b) if\nsource-only fine-tuning of VFMs can perform equally well in these scenarios.\nOur research aims to close these gaps and, similar to previous studies, we\nfocus on semantic segmentation as a representative perception task. We assess\nUDA for synth-to-real and real-to-real use cases with different source and\ntarget data combinations. We also investigate the effect of using a small\namount of labeled target data in UDA. We clarify that while these scenarios are\nmore realistic, they are not necessarily more challenging. Our results show\nthat, when using stronger synthetic source data, UDA's improvement over\nsource-only fine-tuning of VFMs reduces from +8 mIoU to +2 mIoU, and when using\nmore diverse real source data, UDA has no added value. However, UDA\ngeneralization is always higher in all synthetic data scenarios than\nsource-only fine-tuning and, when including only 1/16 of Cityscapes labels,\nsynthetic UDA obtains the same state-of-the-art segmentation quality of 85 mIoU\nas a fully-supervised model using all labels. Considering the mixed results, we\ndiscuss how UDA can best support robust autonomous driving at scale.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u5728\u66f4\u771f\u5b9e\u548c\u591a\u6837\u5316\u6570\u636e\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0UDA\u5728\u5408\u6210\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4ec5\u6e90\u57df\u5fae\u8c03\uff0c\u4f46\u5728\u591a\u6837\u5316\u771f\u5b9e\u6570\u636e\u573a\u666f\u4e2d\u65e0\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u63a2\u8ba8UDA\u5728\u66f4\u5177\u4ee3\u8868\u6027\u548c\u591a\u6837\u5316\u6570\u636e\u4e2d\u7684\u884c\u4e3a\uff0c\u4ee5\u53ca\u6e90\u57df\u5fae\u8c03\u662f\u5426\u8db3\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u573a\u666f\u3002", "method": "\u8bc4\u4f30UDA\u5728\u5408\u6210\u5230\u771f\u5b9e\u548c\u771f\u5b9e\u5230\u771f\u5b9e\u7528\u4f8b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u5c11\u91cf\u76ee\u6807\u57df\u6807\u8bb0\u6570\u636e\u7684\u5f71\u54cd\u3002", "result": "UDA\u5728\u5408\u6210\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u6e90\u57df\u5fae\u8c03\uff08+2 mIoU\uff09\uff0c\u4f46\u5728\u591a\u6837\u5316\u771f\u5b9e\u6570\u636e\u4e2d\u65e0\u4f18\u52bf\uff1b\u4f7f\u7528\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u65f6\uff0cUDA\u8fbe\u5230\u4e0e\u5168\u76d1\u7763\u6a21\u578b\u76f8\u540c\u7684\u6027\u80fd\uff0885 mIoU\uff09\u3002", "conclusion": "UDA\u5728\u5408\u6210\u6570\u636e\u573a\u666f\u4e2d\u4ecd\u6709\u4ef7\u503c\uff0c\u4f46\u5728\u591a\u6837\u5316\u771f\u5b9e\u6570\u636e\u4e2d\u9700\u8c28\u614e\u4f7f\u7528\uff1b\u8ba8\u8bba\u4e86\u5982\u4f55\u4f18\u5316UDA\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u81ea\u52a8\u9a7e\u9a76\u3002"}}
{"id": "2504.18201", "pdf": "https://arxiv.org/pdf/2504.18201", "abs": "https://arxiv.org/abs/2504.18201", "authors": ["Yin Tang", "Jiankai Li", "Hongyu Yang", "Xuan Dong", "Lifeng Fan", "Weixin Li"], "title": "Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In an era where social media platforms abound, individuals frequently share\nimages that offer insights into their intents and interests, impacting\nindividual life quality and societal stability. Traditional computer vision\ntasks, such as object detection and semantic segmentation, focus on concrete\nvisual representations, while intent recognition relies more on implicit visual\nclues. This poses challenges due to the wide variation and subjectivity of such\nclues, compounded by the problem of intra-class variety in conveying abstract\nconcepts, e.g. \"enjoy life\". Existing methods seek to solve the problem by\nmanually designing representative features or building prototypes for each\nclass from global features. However, these methods still struggle to deal with\nthe large visual diversity of each intent category. In this paper, we introduce\na novel approach named Multi-grained Compositional visual Clue Learning (MCCL)\nto address these challenges for image intent recognition. Our method leverages\nthe systematic compositionality of human cognition by breaking down intent\nrecognition into visual clue composition and integrating multi-grained\nfeatures. We adopt class-specific prototypes to alleviate data imbalance. We\ntreat intent recognition as a multi-label classification problem, using a graph\nconvolutional network to infuse prior knowledge through label embedding\ncorrelations. Demonstrated by a state-of-the-art performance on the Intentonomy\nand MDID datasets, our approach advances the accuracy of existing methods while\nalso possessing good interpretability. Our work provides an attempt for future\nexplorations in understanding complex and miscellaneous forms of human\nexpression.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCCL\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u89c6\u89c9\u7ebf\u7d22\u5b66\u4e60\u548c\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u610f\u56fe\u8bc6\u522b\u4e2d\u7684\u591a\u6837\u6027\u548c\u4e3b\u89c2\u6027\u95ee\u9898\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u56fe\u50cf\u610f\u56fe\u8bc6\u522b\u5bf9\u4e2a\u4eba\u548c\u793e\u4f1a\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u89c6\u89c9\u7ebf\u7d22\u7684\u591a\u6837\u6027\u548c\u4e3b\u89c2\u6027\u3002", "method": "\u91c7\u7528\u591a\u7c92\u5ea6\u89c6\u89c9\u7ebf\u7d22\u7ec4\u5408\u548c\u7c7b\u7279\u5b9a\u539f\u578b\uff0c\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "result": "\u5728Intentonomy\u548cMDID\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5177\u6709\u826f\u597d\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u590d\u6742\u4eba\u7c7b\u8868\u8fbe\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18041", "pdf": "https://arxiv.org/pdf/2504.18041", "abs": "https://arxiv.org/abs/2504.18041", "authors": ["Bang An", "Shiyue Zhang", "Mark Dredze"], "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025", "summary": "Efforts to ensure the safety of large language models (LLMs) include safety\nfine-tuning, evaluation, and red teaming. However, despite the widespread use\nof the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses\non standard LLMs, which means we know little about how RAG use cases change a\nmodel's safety profile. We conduct a detailed comparative analysis of RAG and\nnon-RAG frameworks with eleven LLMs. We find that RAG can make models less safe\nand change their safety profile. We explore the causes of this change and find\nthat even combinations of safe models with safe documents can cause unsafe\ngenerations. In addition, we evaluate some existing red teaming methods for RAG\nsettings and show that they are less effective than when used for non-RAG\nsettings. Our work highlights the need for safety research and red-teaming\nmethods specifically tailored for RAG LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cRAG\u6846\u67b6\u53ef\u80fd\u964d\u4f4eLLMs\u7684\u5b89\u5168\u6027\uff0c\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u5bf9RAG\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u9488\u5bf9\u6027\u7814\u7a76\u3002", "motivation": "\u63a2\u8ba8RAG\u6846\u67b6\u5bf9LLMs\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u5b89\u5168\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5bf911\u79cdLLMs\u8fdb\u884cRAG\u4e0e\u975eRAG\u6846\u67b6\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u8bc4\u4f30\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "RAG\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u5b89\u5168\u6027\uff0c\u5b89\u5168\u6a21\u578b\u4e0e\u5b89\u5168\u6587\u6863\u7ec4\u5408\u4ecd\u53ef\u80fd\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u5bf9RAG\u6548\u679c\u8f83\u5dee\u3002", "conclusion": "\u9700\u9488\u5bf9RAG LLMs\u5f00\u53d1\u4e13\u95e8\u7684\u5b89\u5168\u7814\u7a76\u548c\u7ea2\u961f\u65b9\u6cd5\u3002"}}
{"id": "2504.18203", "pdf": "https://arxiv.org/pdf/2504.18203", "abs": "https://arxiv.org/abs/2504.18203", "authors": ["Raul David Dominguez Sanchez", "Xavier Diaz Ortiz", "Xingcheng Zhou", "Max Peter Ronecker", "Michael Karner", "Daniel Watzenig", "Alois Knoll"], "title": "LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for the Data-Driven Learning for Intelligent Vehicle\n  Applications Workshop at the 36th IEEE Intelligent Vehicles Symposium (IV)\n  2025", "summary": "Railway systems, particularly in Germany, require high levels of automation\nto address legacy infrastructure challenges and increase train traffic safely.\nA key component of automation is robust long-range perception, essential for\nearly hazard detection, such as obstacles at level crossings or pedestrians on\ntracks. Unlike automotive systems with braking distances of ~70 meters, trains\nrequire perception ranges exceeding 1 km. This paper presents an\ndeep-learning-based approach for long-range 3D object detection tailored for\nautonomous trains. The method relies solely on monocular images, inspired by\nthe Faraway-Frustum approach, and incorporates LiDAR data during training to\nimprove depth estimation. The proposed pipeline consists of four key modules:\n(1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation\nnetwork, and (3-4) dedicated short- and long-range 3D detection heads.\nEvaluations on the OSDaR23 dataset demonstrate the effectiveness of the\napproach in detecting objects up to 250 meters. Results highlight its potential\nfor railway automation and outline areas for future improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5355\u76ee\u56fe\u50cf\u957f\u8ddd\u79bb3D\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e13\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5217\u8f66\u8bbe\u8ba1\uff0c\u7ed3\u5408LiDAR\u6570\u636e\u63d0\u5347\u6df1\u5ea6\u4f30\u8ba1\uff0c\u6709\u6548\u68c0\u6d4b250\u7c73\u5185\u7269\u4f53\u3002", "motivation": "\u5fb7\u56fd\u94c1\u8def\u7cfb\u7edf\u9700\u8981\u9ad8\u81ea\u52a8\u5316\u4ee5\u5e94\u5bf9\u8001\u65e7\u57fa\u7840\u8bbe\u65bd\u6311\u6218\u5e76\u5b89\u5168\u589e\u52a0\u5217\u8f66\u6d41\u91cf\uff0c\u957f\u8ddd\u79bb\u611f\u77e5\u662f\u5173\u952e\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684YOLOv9\u8fdb\u884c2.5D\u76ee\u6807\u68c0\u6d4b\uff0c\u7ed3\u5408\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\u548c\u77ed/\u957f\u8ddd\u79bb3D\u68c0\u6d4b\u5934\uff0c\u8bad\u7ec3\u65f6\u5f15\u5165LiDAR\u6570\u636e\u3002", "result": "\u5728OSDaR23\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u80fd\u68c0\u6d4b250\u7c73\u5185\u7269\u4f53\uff0c\u5c55\u793a\u4e86\u94c1\u8def\u81ea\u52a8\u5316\u7684\u6f5c\u529b\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\uff0c\u4f46\u4ecd\u9700\u672a\u6765\u6539\u8fdb\u3002"}}
{"id": "2504.18044", "pdf": "https://arxiv.org/pdf/2504.18044", "abs": "https://arxiv.org/abs/2504.18044", "authors": ["Omid Veisi", "Sasan Bahrami", "Roman Englert", "Claudia M\u00fcller"], "title": "AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.IT", "math.IT"], "comment": "Accepted for presentation at the ACM Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW) 2025. To appear in Proceedings\n  of the ACM on Human-Computer Interaction (PACM HCI)", "summary": "Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social\nComputing requires the examination of ethical and social norms to ensure safe\nincorporation into human life. We conducted a mixed-method study, including an\nonline survey with 111 participants and an interview study with 38 experts, to\ninvestigate the AI ethics and social norms in ChatGPT as everyday life tools.\nThis study aims to evaluate whether ChatGPT in an empirical context operates\nfollowing ethics and social norms, which is critical for understanding actions\nin industrial and academic research and achieving machine ethics. The findings\nof this study provide initial insights into six important aspects of AI ethics,\nincluding bias, trustworthiness, security, toxicology, social norms, and\nethical data. Significant obstacles related to transparency and bias in\nunsupervised data collection methods are identified as ChatGPT's ethical\nconcerns.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86ChatGPT\u5728\u533b\u7597\u3001\u534f\u4f5c\u5de5\u4f5c\u548c\u793e\u4f1a\u8ba1\u7b97\u4e2d\u7684\u4f26\u7406\u4e0e\u793e\u4f1a\u89c4\u8303\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u53d1\u73b0\u5176\u5b58\u5728\u900f\u660e\u5ea6\u548c\u504f\u89c1\u7b49\u4f26\u7406\u969c\u788d\u3002", "motivation": "\u786e\u4fddAI\u5de5\u5177\u5982ChatGPT\u5728\u4eba\u7c7b\u751f\u6d3b\u4e2d\u7684\u5b89\u5168\u878d\u5165\uff0c\u9700\u8bc4\u4f30\u5176\u662f\u5426\u7b26\u5408\u4f26\u7406\u548c\u793e\u4f1a\u89c4\u8303\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u5305\u62ec111\u4eba\u7684\u5728\u7ebf\u8c03\u67e5\u548c38\u4f4d\u4e13\u5bb6\u7684\u8bbf\u8c08\u3002", "result": "\u7814\u7a76\u53d1\u73b0ChatGPT\u5728\u4f26\u7406\u548c\u793e\u4f1a\u89c4\u8303\u65b9\u9762\u5b58\u5728\u900f\u660e\u5ea6\u548c\u504f\u89c1\u95ee\u9898\uff0c\u6d89\u53ca\u516d\u4e2a\u5173\u952e\u4f26\u7406\u9886\u57df\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u4f26\u7406\u63d0\u4f9b\u4e86\u521d\u6b65\u89c1\u89e3\uff0c\u5f3a\u8c03\u900f\u660e\u5ea6\u548c\u504f\u89c1\u662fChatGPT\u7684\u4e3b\u8981\u4f26\u7406\u6311\u6218\u3002"}}
{"id": "2504.18204", "pdf": "https://arxiv.org/pdf/2504.18204", "abs": "https://arxiv.org/abs/2504.18204", "authors": ["Kun Li", "Jianhui Wang", "Yangfan He", "Xinyuan Song", "Ruoyu Wang", "Hongyang He", "Wenxin Zhang", "Jiaqi Chen", "Keqin Li", "Sida Li", "Miao Zhang", "Tianyu Shi", "Xueqian Wang"], "title": "Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2503.17660", "summary": "Generative AI has significantly changed industries by enabling text-driven\nimage generation, yet challenges remain in achieving high-resolution outputs\nthat align with fine-grained user preferences. Consequently, multi-round\ninteractions are necessary to ensure the generated images meet expectations.\nPrevious methods enhanced prompts via reward feedback but did not optimize over\na multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation\n(VCA) framework incorporating human-in-the-loop feedback, leveraging a\nwell-trained reward model aligned with human preferences. Using a diverse\nmulti-turn dialogue dataset, our framework applies multiple reward functions,\nsuch as diversity, consistency, and preference feedback, while fine-tuning the\ndiffusion model through LoRA, thus optimizing image generation based on user\ninput. We also construct multi-round dialogue datasets of prompts and image\npairs aligned with user intent. Experiments demonstrate that our method\noutperforms state-of-the-art baselines, significantly improving image\nconsistency and alignment with user intent. Our approach consistently surpasses\ncompeting models in user satisfaction, especially in multi-turn dialogue\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u534f\u540c\u9002\u5e94\uff08VCA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u548c\u4eba\u7c7b\u53cd\u9988\u4f18\u5316\u751f\u6210\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u548c\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u9ad8\u5206\u8fa8\u7387\u8f93\u51fa\u4e0e\u7528\u6237\u7ec6\u7c92\u5ea6\u504f\u597d\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u9700\u591a\u8f6e\u4ea4\u4e92\u4f18\u5316\u3002", "method": "\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u591a\u6837\u6027\u3001\u4e00\u81f4\u6027\u548c\u504f\u597d\u53cd\u9988\u7b49\u591a\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVCA\u5728\u56fe\u50cf\u4e00\u81f4\u6027\u548c\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "VCA\u6846\u67b6\u5728\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2504.18213", "pdf": "https://arxiv.org/pdf/2504.18213", "abs": "https://arxiv.org/abs/2504.18213", "authors": ["Nicolas M\u00fcnger", "Max Peter Ronecker", "Xavier Diaz", "Michael Karner", "Daniel Watzenig", "Jan Skaloud"], "title": "A Data-Centric Approach to 3D Semantic Segmentation of Railway Scenes", "categories": ["cs.CV"], "comment": "Accepted at the 28th Computer Vision Winter Workshop 2025", "summary": "LiDAR-based semantic segmentation is critical for autonomous trains,\nrequiring accurate predictions across varying distances. This paper introduces\ntwo targeted data augmentation methods designed to improve segmentation\nperformance on the railway-specific OSDaR23 dataset. The person instance\npasting method enhances segmentation of pedestrians at distant ranges by\ninjecting realistic variations into the dataset. The track sparsification\nmethod redistributes point density in LiDAR scans, improving track segmentation\nat far distances with minimal impact on close-range accuracy. Both methods are\nevaluated using a state-of-the-art 3D semantic segmentation network,\ndemonstrating significant improvements in distant-range performance while\nmaintaining robustness in close-range predictions. We establish the first 3D\nsemantic segmentation benchmark for OSDaR23, demonstrating the potential of\ndata-centric approaches to address railway-specific challenges in autonomous\ntrain perception.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u9488\u5bf9\u94c1\u8def\u573a\u666f\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u63d0\u5347LiDAR\u8bed\u4e49\u5206\u5272\u5728\u8fdc\u8ddd\u79bb\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u94c1\u8def\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u7cbe\u786e\u7684\u8bed\u4e49\u5206\u5272\uff0c\u5c24\u5176\u662f\u5728\u8fdc\u8ddd\u79bb\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u884c\u4eba\u5b9e\u4f8b\u7c98\u8d34\u548c\u8f68\u9053\u7a00\u758f\u5316\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u663e\u8457\u63d0\u5347\u8fdc\u8ddd\u79bb\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8fd1\u8ddd\u79bb\u51c6\u786e\u6027\u3002", "conclusion": "\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u94c1\u8def\u81ea\u52a8\u9a7e\u9a76\u7684\u611f\u77e5\u6311\u6218\u3002"}}
{"id": "2504.18215", "pdf": "https://arxiv.org/pdf/2504.18215", "abs": "https://arxiv.org/abs/2504.18215", "authors": ["Nanjie Yao", "Gangjian Zhang", "Wenhao Shen", "Jian Shu", "Hao Wang"], "title": "Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D clothed human reconstruction aims to create a complete 3D avatar\nfrom a single image. To tackle the human geometry lacking in one RGB image,\ncurrent methods typically resort to a preceding model for an explicit geometric\nrepresentation. For the reconstruction itself, focus is on modeling both it and\nthe input image. This routine is constrained by the preceding model, and\noverlooks the integrity of the reconstruction task. To address this, this paper\nintroduces a novel paradigm that treats human reconstruction as a holistic\nprocess, utilizing an end-to-end network for direct prediction from 2D image to\n3D avatar, eliminating any explicit intermediate geometry display. Based on\nthis, we further propose a novel reconstruction framework consisting of two\ncore components: the Anatomy Shaping Extraction module, which captures implicit\nshape features taking into account the specialty of human anatomy, and the\nTwins Negotiating Reconstruction U-Net, which enhances reconstruction through\nfeature interaction between two U-Nets of different modalities. Moreover, we\npropose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to\nbolster model performance in more complex case input. Extensive experiments on\ntwo test sets and many in-the-wild cases show the superiority of our method\nover SOTA methods. Our demos can be found in :\nhttps://e2e3dgsrecon.github.io/e2e3dgsrecon/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5355\u76ee3D\u7a7f\u8863\u4eba\u4f53\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u9884\u6d4b\u4ece2D\u56fe\u50cf\u52303D\u865a\u62df\u5f62\u8c61\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u4e2d\u95f4\u51e0\u4f55\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\u548c\u4e00\u4e2a\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u524d\u7f6e\u6a21\u578b\u751f\u6210\u663e\u5f0f\u51e0\u4f55\u8868\u793a\uff0c\u9650\u5236\u4e86\u91cd\u5efa\u4efb\u52a1\u7684\u5b8c\u6574\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7aef\u5230\u7aef\u7f51\u7edc\u76f4\u63a5\u9884\u6d4b3D\u865a\u62df\u5f62\u8c61\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7f51\u7edc\u6846\u67b6\uff0c\u5305\u542b\u89e3\u5256\u5f62\u72b6\u63d0\u53d6\u6a21\u5757\u548c\u53cc\u6a21\u6001U-Net\u7279\u5f81\u4ea4\u4e92\u6a21\u5757\uff0c\u5e76\u91c7\u7528\u6f2b\u753b\u6570\u636e\u589e\u5f3a\u7b56\u7565\u548c\u6784\u5efa\u5927\u89c4\u6a213D\u4eba\u4f53\u626b\u63cf\u6570\u636e\u96c6\u3002", "result": "\u5728\u4e24\u4e2a\u6d4b\u8bd5\u96c6\u548c\u5b9e\u9645\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7aef\u5230\u7aef\u8bbe\u8ba1\u548c\u6838\u5fc3\u6a21\u5757\u7684\u5f15\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u7a7f\u8863\u4eba\u4f53\u91cd\u5efa\u7684\u6027\u80fd\u3002"}}
{"id": "2504.18050", "pdf": "https://arxiv.org/pdf/2504.18050", "abs": "https://arxiv.org/abs/2504.18050", "authors": ["Mingwei Zheng", "Danning Xie", "Qingkai Shi", "Chengpeng Wang", "Xiangyu Zhang"], "title": "Validating Network Protocol Parsers with Traceable RFC Document Interpretation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Validating the correctness of network protocol implementations is highly\nchallenging due to the oracle and traceability problems. The former determines\nwhen a protocol implementation can be considered buggy, especially when the\nbugs do not cause any observable symptoms. The latter allows developers to\nunderstand how an implementation violates the protocol specification, thereby\nfacilitating bug fixes. Unlike existing works that rarely take both problems\ninto account, this work considers both and provides an effective solution using\nrecent advances in large language models (LLMs). Our key observation is that\nnetwork protocols are often released with structured specification documents,\na.k.a. RFC documents, which can be systematically translated to formal protocol\nmessage specifications via LLMs. Such specifications, which may contain errors\ndue to the hallucination of LLMs, are used as a quasi-oracle to validate\nprotocol parsers, while the validation results in return gradually refine the\noracle. Since the oracle is derived from the document, any bugs we find in a\nprotocol implementation can be traced back to the document, thus addressing the\ntraceability problem. We have extensively evaluated our approach using nine\nnetwork protocols and their implementations written in C, Python, and Go. The\nresults show that our approach outperforms the state-of-the-art and has\ndetected 69 bugs, with 36 confirmed. The project also demonstrates the\npotential for fully automating software validation based on natural language\nspecifications, a process previously considered predominantly manual due to the\nneed to understand specification documents and derive expected outputs for test\ninputs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u51b3\u7f51\u7edc\u534f\u8bae\u5b9e\u73b0\u9a8c\u8bc1\u4e2d\u7684oracle\u548ctraceability\u95ee\u9898\uff0c\u901a\u8fc7\u5c06RFC\u6587\u6863\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u534f\u8bae\u6d88\u606f\u89c4\u8303\uff0c\u5e76\u9010\u6b65\u4f18\u5316oracle\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u534f\u8bae\u9a8c\u8bc1\u548c\u9519\u8bef\u8ffd\u8e2a\u3002", "motivation": "\u7f51\u7edc\u534f\u8bae\u5b9e\u73b0\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\u9762\u4e34oracle\u548ctraceability\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5f88\u5c11\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002", "method": "\u5229\u7528LLMs\u5c06RFC\u6587\u6863\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u534f\u8bae\u6d88\u606f\u89c4\u8303\uff0c\u4f5c\u4e3aquasi-oracle\u9a8c\u8bc1\u534f\u8bae\u89e3\u6790\u5668\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u7ed3\u679c\u9010\u6b65\u4f18\u5316oracle\u3002", "result": "\u57289\u4e2a\u7f51\u7edc\u534f\u8bae\u53ca\u5176C\u3001Python\u3001Go\u5b9e\u73b0\u4e2d\u68c0\u6d4b\u523069\u4e2a\u9519\u8bef\uff0c\u5176\u4e2d36\u4e2a\u5df2\u786e\u8ba4\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u5316\u8f6f\u4ef6\u9a8c\u8bc1\u7684\u6f5c\u529b\uff0c\u51cf\u5c11\u4e86\u4f20\u7edf\u624b\u52a8\u9a8c\u8bc1\u7684\u9700\u6c42\u3002"}}
{"id": "2504.18233", "pdf": "https://arxiv.org/pdf/2504.18233", "abs": "https://arxiv.org/abs/2504.18233", "authors": ["Wenxiang Gua", "Lin Qia"], "title": "Dense Geometry Supervision for Underwater Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "The field of monocular depth estimation is continually evolving with the\nadvent of numerous innovative models and extensions. However, research on\nmonocular depth estimation methods specifically for underwater scenes remains\nlimited, compounded by a scarcity of relevant data and methodological support.\nThis paper proposes a novel approach to address the existing challenges in\ncurrent monocular depth estimation methods for underwater environments. We\nconstruct an economically efficient dataset suitable for underwater scenarios\nby employing multi-view depth estimation to generate supervisory signals and\ncorresponding enhanced underwater images. we introduces a texture-depth fusion\nmodule, designed according to the underwater optical imaging principles, which\naims to effectively exploit and integrate depth information from texture cues.\nExperimental results on the FLSea dataset demonstrate that our approach\nsignificantly improves the accuracy and adaptability of models in underwater\nsettings. This work offers a cost-effective solution for monocular underwater\ndepth estimation and holds considerable promise for practical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6c34\u4e0b\u573a\u666f\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7ecf\u6d4e\u9ad8\u6548\u7684\u6570\u636e\u96c6\u548c\u7eb9\u7406-\u6df1\u5ea6\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u6c34\u4e0b\u573a\u666f\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7814\u7a76\u8f83\u5c11\uff0c\u4e14\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u548c\u65b9\u6cd5\u652f\u6301\u3002", "method": "\u6784\u5efa\u7ecf\u6d4e\u9ad8\u6548\u7684\u6c34\u4e0b\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u7eb9\u7406-\u6df1\u5ea6\u878d\u5408\u6a21\u5757\uff0c\u7ed3\u5408\u6c34\u4e0b\u5149\u5b66\u6210\u50cf\u539f\u7406\u3002", "result": "\u5728FLSea\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6c34\u4e0b\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.18057", "pdf": "https://arxiv.org/pdf/2504.18057", "abs": "https://arxiv.org/abs/2504.18057", "authors": ["Jiayi Chen", "Shuai Wang", "Guoliang Li", "Wei Xu", "Guangxu Zhu", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Opportunistic Collaborative Planning with Large Vision Model Guided Control and Joint Query-Service Optimization", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Navigating autonomous vehicles in open scenarios is a challenge due to the\ndifficulties in handling unseen objects. Existing solutions either rely on\nsmall models that struggle with generalization or large models that are\nresource-intensive. While collaboration between the two offers a promising\nsolution, the key challenge is deciding when and how to engage the large model.\nTo address this issue, this paper proposes opportunistic collaborative planning\n(OCP), which seamlessly integrates efficient local models with powerful cloud\nmodels through two key innovations. First, we propose large vision model guided\nmodel predictive control (LVM-MPC), which leverages the cloud for LVM\nperception and decision making. The cloud output serves as a global guidance\nfor a local MPC, thereby forming a closed-loop perception-to-control system.\nSecond, to determine the best timing for large model query and service, we\npropose collaboration timing optimization (CTO), including object detection\nconfidence thresholding (ODCT) and cloud forward simulation (CFS), to decide\nwhen to seek cloud assistance and when to offer cloud service. Extensive\nexperiments show that the proposed OCP outperforms existing methods in terms of\nboth navigation time and success rate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u4f1a\u534f\u4f5c\u89c4\u5212\uff08OCP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u672c\u5730\u6a21\u578b\u548c\u4e91\u7aef\u5927\u6a21\u578b\uff0c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5f00\u653e\u573a\u666f\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5f00\u653e\u573a\u666f\u4e2d\u5904\u7406\u672a\u89c1\u7269\u4f53\u65f6\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u8981\u4e48\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\u3002", "method": "1. \u63d0\u51faLVM-MPC\uff0c\u5229\u7528\u4e91\u7aef\u5927\u6a21\u578b\u8fdb\u884c\u611f\u77e5\u548c\u51b3\u7b56\uff0c\u4e3a\u672c\u5730MPC\u63d0\u4f9b\u5168\u5c40\u6307\u5bfc\uff1b2. \u63d0\u51faCTO\uff08\u5305\u62ecODCT\u548cCFS\uff09\uff0c\u4f18\u5316\u5927\u6a21\u578b\u67e5\u8be2\u548c\u670d\u52a1\u7684\u65f6\u673a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOCP\u5728\u5bfc\u822a\u65f6\u95f4\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OCP\u901a\u8fc7\u672c\u5730\u4e0e\u4e91\u7aef\u6a21\u578b\u7684\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2504.18235", "pdf": "https://arxiv.org/pdf/2504.18235", "abs": "https://arxiv.org/abs/2504.18235", "authors": ["Andreas Ziegler", "David Joseph", "Thomas Gossard", "Emil Moldovan", "Andreas Zell"], "title": "BiasBench: A reproducible benchmark for tuning the biases of event cameras", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to CVPR 2025 Workshop on Event-based Vision", "summary": "Event-based cameras are bio-inspired sensors that detect light changes\nasynchronously for each pixel. They are increasingly used in fields like\ncomputer vision and robotics because of several advantages over traditional\nframe-based cameras, such as high temporal resolution, low latency, and high\ndynamic range. As with any camera, the output's quality depends on how well the\ncamera's settings, called biases for event-based cameras, are configured. While\nframe-based cameras have advanced automatic configuration algorithms, there are\nvery few such tools for tuning these biases. A systematic testing framework\nwould require observing the same scene with different biases, which is tricky\nsince event cameras only generate events when there is movement. Event\nsimulators exist, but since biases heavily depend on the electrical circuit and\nthe pixel design, available simulators are not well suited for bias tuning. To\nallow reproducibility, we present BiasBench, a novel event dataset containing\nmultiple scenes with settings sampled in a grid-like pattern. We present three\ndifferent scenes, each with a quality metric of the downstream application.\nAdditionally, we present a novel, RL-based method to facilitate online bias\nadjustments.", "AI": {"tldr": "BiasBench\u662f\u4e00\u4e2a\u65b0\u7684\u4e8b\u4ef6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5316\u6d4b\u8bd5\u548c\u8c03\u6574\u4e8b\u4ef6\u76f8\u673a\u7684\u504f\u7f6e\u8bbe\u7f6e\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5728\u7ebf\u504f\u7f6e\u8c03\u6574\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u504f\u7f6e\u8bbe\u7f6e\u7f3a\u4e4f\u81ea\u52a8\u914d\u7f6e\u5de5\u5177\uff0c\u73b0\u6709\u6a21\u62df\u5668\u4e0d\u9002\u5408\u504f\u7f6e\u8c03\u6574\u3002", "method": "\u63d0\u51fa\u4e86BiasBench\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u4e2a\u573a\u666f\u548c\u504f\u7f6e\u8bbe\u7f6e\u7684\u7f51\u683c\u91c7\u6837\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5728\u7ebf\u504f\u7f6e\u8c03\u6574\u65b9\u6cd5\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u7cfb\u7edf\u5316\u6d4b\u8bd5\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5728\u7ebf\u504f\u7f6e\u8c03\u6574\u3002", "conclusion": "BiasBench\u4e3a\u4e8b\u4ef6\u76f8\u673a\u7684\u504f\u7f6e\u8c03\u6574\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2504.18058", "pdf": "https://arxiv.org/pdf/2504.18058", "abs": "https://arxiv.org/abs/2504.18058", "authors": ["Sijia Cheng", "Wen-Yu Chang", "Yun-Nung Chen"], "title": "Exploring Personality-Aware Interactions in Salesperson Dialogue Agents", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IWSDS 2025", "summary": "The integration of dialogue agents into the sales domain requires a deep\nunderstanding of how these systems interact with users possessing diverse\npersonas. This study explores the influence of user personas, defined using the\nMyers-Briggs Type Indicator (MBTI), on the interaction quality and performance\nof sales-oriented dialogue agents. Through large-scale testing and analysis, we\nassess the pre-trained agent's effectiveness, adaptability, and personalization\ncapabilities across a wide range of MBTI-defined user types. Our findings\nreveal significant patterns in interaction dynamics, task completion rates, and\ndialogue naturalness, underscoring the future potential for dialogue agents to\nrefine their strategies to better align with varying personality traits. This\nwork not only provides actionable insights for building more adaptive and\nuser-centric conversational systems in the sales domain but also contributes\nbroadly to the field by releasing persona-defined user simulators. These\nsimulators, unconstrained by domain, offer valuable tools for future research\nand demonstrate the potential for scaling personalized dialogue systems across\ndiverse applications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86MBTI\u5b9a\u4e49\u7684\u7528\u6237\u4eba\u683c\u5bf9\u9500\u552e\u5bf9\u8bdd\u4ee3\u7406\u4ea4\u4e92\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4ea4\u4e92\u52a8\u6001\u3001\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u5bf9\u8bdd\u81ea\u7136\u6027\u5b58\u5728\u663e\u8457\u6a21\u5f0f\uff0c\u5e76\u53d1\u5e03\u4e86\u8de8\u9886\u57df\u7528\u6237\u6a21\u62df\u5668\u3002", "motivation": "\u7406\u89e3\u7528\u6237\u4eba\u683c\u591a\u6837\u6027\u5bf9\u9500\u552e\u5bf9\u8bdd\u4ee3\u7406\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347\u5176\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u6d4b\u8bd5\u548c\u5206\u6790\uff0c\u8bc4\u4f30\u9884\u8bad\u7ec3\u4ee3\u7406\u5728\u4e0d\u540cMBTI\u7528\u6237\u7c7b\u578b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u63ed\u793a\u4e86\u4ea4\u4e92\u52a8\u6001\u3001\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u5bf9\u8bdd\u81ea\u7136\u6027\u7684\u663e\u8457\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86\u8de8\u9886\u57df\u7528\u6237\u6a21\u62df\u5668\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u9500\u552e\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u63a8\u52a8\u4e86\u8de8\u9886\u57df\u4e2a\u6027\u5316\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.18249", "pdf": "https://arxiv.org/pdf/2504.18249", "abs": "https://arxiv.org/abs/2504.18249", "authors": ["Qinyu Chen", "Chang Gao", "Min Liu", "Daniele Perrone", "Yan Ru Pei", "Zuowen Wang", "Zhuo Zou", "Shihang Tan", "Tao Han", "Guorui Lu", "Zhen Xu", "Junyuan Ding", "Ziteng Wang", "Zongwei Wu", "Han Han", "Yuliang Wu", "Jinze Chen", "Wei Zhai", "Yang Cao", "Zheng-jun Zha", "Nuwan Bandara", "Thivya Kandappu", "Archan Misra", "Xiaopeng Lin", "Hongxiang Huang", "Hongwei Ren", "Bojun Cheng", "Hoang M. Truong", "Vinh-Thuan Ly", "Huy G. Tran", "Thuan-Phat Nguyen", "Tram T. Doan"], "title": "Event-Based Eye Tracking. 2025 Event-based Vision Workshop", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This survey serves as a review for the 2025 Event-Based Eye Tracking\nChallenge organized as part of the 2025 CVPR event-based vision workshop. This\nchallenge focuses on the task of predicting the pupil center by processing\nevent camera recorded eye movement. We review and summarize the innovative\nmethods from teams rank the top in the challenge to advance future event-based\neye tracking research. In each method, accuracy, model size, and number of\noperations are reported. In this survey, we also discuss event-based eye\ntracking from the perspective of hardware design.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e862025\u5e74\u57fa\u4e8e\u4e8b\u4ef6\u7684\u773c\u52a8\u8ffd\u8e2a\u6311\u6218\u8d5b\u7684\u9876\u5c16\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u786c\u4ef6\u8bbe\u8ba1\u89c6\u89d2\u3002", "motivation": "\u63a8\u52a8\u57fa\u4e8e\u4e8b\u4ef6\u7684\u773c\u52a8\u8ffd\u8e2a\u7814\u7a76\uff0c\u603b\u7ed3\u6311\u6218\u8d5b\u4e2d\u521b\u65b0\u65b9\u6cd5\u3002", "method": "\u56de\u987e\u5e76\u5206\u6790\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u9760\u524d\u56e2\u961f\u7684\u65b9\u6cd5\uff0c\u62a5\u544a\u51c6\u786e\u6027\u3001\u6a21\u578b\u5927\u5c0f\u548c\u64cd\u4f5c\u6570\u3002", "result": "\u603b\u7ed3\u4e86\u5404\u65b9\u6cd5\u7684\u6027\u80fd\u6307\u6807\uff0c\u5e76\u8ba8\u8bba\u4e86\u786c\u4ef6\u8bbe\u8ba1\u7684\u5f71\u54cd\u3002", "conclusion": "\u4e3a\u672a\u6765\u57fa\u4e8e\u4e8b\u4ef6\u7684\u773c\u52a8\u8ffd\u8e2a\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u548c\u65b9\u5411\u3002"}}
{"id": "2504.18062", "pdf": "https://arxiv.org/pdf/2504.18062", "abs": "https://arxiv.org/abs/2504.18062", "authors": ["Lingyan Bao", "Sinwoong Yun", "Jemin Lee", "Tony Q. S. Quek"], "title": "LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have led to a significant\ninterest in deploying LLMempowered algorithms for wireless communication\nnetworks. Meanwhile, open radio access network (O-RAN) techniques offer\nunprecedented flexibility, with the non-real-time (non-RT) radio access network\n(RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT)\nRIC (near-RT RIC) components enabling intelligent resource management across\ndifferent time scales. In this paper, we propose the LLM empowered hierarchical\nRIC (LLM-hRIC) framework to improve the collaboration between RICs. This\nframework integrates LLMs with reinforcement learning (RL) for efficient\nnetwork resource management. In this framework, LLMs-empowered non-RT RICs\nprovide strategic guidance and high-level policies based on environmental\ncontext. Concurrently, RL-empowered near-RT RICs perform low-latency tasks\nbased on strategic guidance and local near-RT observation. We evaluate the\nLLM-hRIC framework in an integrated access and backhaul (IAB) network setting.\nSimulation results demonstrate that the proposed framework achieves superior\nperformance. Finally, we discuss the key future challenges in applying LLMs to\nO-RAN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u5206\u5c42RIC\u6846\u67b6\uff08LLM-hRIC\uff09\uff0c\u7528\u4e8e\u65e0\u7ebf\u901a\u4fe1\u7f51\u7edc\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u5e76\u5728IAB\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5229\u7528LLM\u548cO-RAN\u6280\u672f\u7684\u7075\u6d3b\u6027\uff0c\u63d0\u5347\u65e0\u7ebf\u901a\u4fe1\u7f51\u7edc\u4e2dRIC\u7ec4\u4ef6\u95f4\u7684\u534f\u4f5c\u6548\u7387\u3002", "method": "\u63d0\u51faLLM-hRIC\u6846\u67b6\uff0cLLM\u9a71\u52a8\u7684\u975e\u5b9e\u65f6RIC\u63d0\u4f9b\u6218\u7565\u6307\u5bfc\uff0cRL\u9a71\u52a8\u7684\u8fd1\u5b9e\u65f6RIC\u6267\u884c\u4f4e\u5ef6\u8fdf\u4efb\u52a1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728IAB\u7f51\u7edc\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8bba\u6587\u8ba8\u8bba\u4e86LLM\u5728O-RAN\u4e2d\u5e94\u7528\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2504.18256", "pdf": "https://arxiv.org/pdf/2504.18256", "abs": "https://arxiv.org/abs/2504.18256", "authors": ["Elena Plekhanova", "Damien Robert", "Johannes Dollinger", "Emilia Arens", "Philipp Brun", "Jan Dirk Wegner", "Niklaus Zimmermann"], "title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology", "categories": ["cs.CV"], "comment": "CVPR 2025, EarthVision workshop", "summary": "With the exacerbation of the biodiversity and climate crises, macroecological\npursuits such as global biodiversity mapping become more urgent. Remote sensing\noffers a wealth of Earth observation data for ecological studies, but the\nscarcity of labeled datasets remains a major challenge. Recently,\nself-supervised learning has enabled learning representations from unlabeled\ndata, triggering the development of pretrained geospatial models with\ngeneralizable features. However, these models are often trained on datasets\nbiased toward areas of high human activity, leaving entire ecological regions\nunderrepresented. Additionally, while some datasets attempt to address\nseasonality through multi-date imagery, they typically follow calendar seasons\nrather than local phenological cycles. To better capture vegetation seasonality\nat a global scale, we propose a simple phenology-informed sampling strategy and\nintroduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we\ntrain an existing model with a season-contrastive objective. We compare\nrepresentations learned from SSL4Eco against other datasets on diverse\necological downstream tasks and demonstrate that our straightforward sampling\nmethod consistently improves representation quality, highlighting the\nimportance of dataset construction. The model pretrained on SSL4Eco reaches\nstate of the art performance on 7 out of 8 downstream tasks spanning\n(multi-label) classification and regression. We release our code, data, and\nmodel weights to support macroecological and computer vision research at\nhttps://github.com/PlekhanovaElena/ssl4eco.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u5019\u5b66\u7684\u91c7\u6837\u7b56\u7565\u548cSSL4Eco\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u5168\u7403\u751f\u6001\u533a\u57df\u8868\u5f81\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u751f\u7269\u591a\u6837\u6027\u548c\u6c14\u5019\u5371\u673a\u52a0\u5267\uff0c\u5168\u7403\u751f\u7269\u591a\u6837\u6027\u5236\u56fe\u9700\u6c42\u8feb\u5207\u3002\u73b0\u6709\u9065\u611f\u6570\u636e\u5b58\u5728\u6807\u6ce8\u7a00\u7f3a\u548c\u533a\u57df\u504f\u5dee\u95ee\u9898\uff0c\u4e14\u5b63\u8282\u6027\u5904\u7406\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u7269\u5019\u5b66\u91c7\u6837\u7b56\u7565\uff0c\u6784\u5efaSSL4Eco\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5b63\u8282\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u6a21\u578b\u3002", "result": "SSL4Eco\u9884\u8bad\u7ec3\u6a21\u578b\u57288\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d7\u4e2a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u6570\u636e\u96c6\u6784\u5efa\u5bf9\u8868\u5f81\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0cSSL4Eco\u4e3a\u5b8f\u89c2\u751f\u6001\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.18283", "pdf": "https://arxiv.org/pdf/2504.18283", "abs": "https://arxiv.org/abs/2504.18283", "authors": ["Minjae Kang", "Martim Brand\u00e3o"], "title": "Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "comment": "Originally submitted to CVPR 2025 on 2024-11-15 with paper ID 15808", "summary": "Recent audio-visual generative models have made substantial progress in\ngenerating images from audio. However, existing approaches focus on generating\nimages from single-class audio and fail to generate images from mixed audio. To\naddress this, we propose an Audio-Visual Generation and Separation model\n(AV-GAS) for generating images from soundscapes (mixed audio containing\nmultiple classes). Our contribution is threefold: First, we propose a new\nchallenge in the audio-visual generation task, which is to generate an image\ngiven a multi-class audio input, and we propose a method that solves this task\nusing an audio-visual separator. Second, we introduce a new audio-visual\nseparation task, which involves generating separate images for each class\npresent in a mixed audio input. Lastly, we propose new evaluation metrics for\nthe audio-visual generation task: Class Representation Score (CRS) and a\nmodified R@K. Our model is trained and evaluated on the VGGSound dataset. We\nshow that our method outperforms the state-of-the-art, achieving 7% higher CRS\nand 4% higher R@2* in generating plausible images with mixed audio.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAV-GAS\u6a21\u578b\uff0c\u89e3\u51b3\u4ece\u6df7\u5408\u97f3\u9891\u751f\u6210\u56fe\u50cf\u7684\u6311\u6218\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u751f\u6210\u6a21\u578b\u4ec5\u652f\u6301\u5355\u7c7b\u97f3\u9891\u751f\u6210\u56fe\u50cf\uff0c\u65e0\u6cd5\u5904\u7406\u6df7\u5408\u97f3\u9891\u8f93\u5165\u3002", "method": "\u63d0\u51faAV-GAS\u6a21\u578b\uff0c\u7ed3\u5408\u97f3\u9891-\u89c6\u89c9\u5206\u79bb\u5668\uff0c\u652f\u6301\u4ece\u591a\u7c7b\u97f3\u9891\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u5206\u79bb\u4efb\u52a1\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728VGGSound\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cCRS\u63d0\u9ad87%\uff0cR@2*\u63d0\u9ad84%\u3002", "conclusion": "AV-GAS\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u97f3\u9891\u751f\u6210\u56fe\u50cf\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u4efb\u52a1\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2504.18070", "pdf": "https://arxiv.org/pdf/2504.18070", "abs": "https://arxiv.org/abs/2504.18070", "authors": ["Jingjin Wang"], "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths", "categories": ["cs.CL", "cs.AI"], "comment": "Code and data to be released at:\n  https://github.com/ReLink-Inc/PropRAG", "summary": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning.", "AI": {"tldr": "PropRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u547d\u9898\u8def\u5f84\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u56fe\u904d\u5386\u548c\u9884\u8ba1\u7b97\u5d4c\u5165\uff0c\u907f\u514d\u4e86\u5728\u7ebfLLM\u63a8\u7406\u6210\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u6807\u51c6RAG\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u8bb0\u5fc6\u7684\u5173\u8054\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316RAG\u65b9\u6cd5\u5b58\u5728\u4e0a\u4e0b\u6587\u4e22\u5931\u95ee\u9898\u3002PropRAG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "PropRAG\u5229\u7528\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u547d\u9898\u548c\u65b0\u578b\u675f\u641c\u7d22\u7b97\u6cd5\uff0c\u663e\u5f0f\u53d1\u73b0\u591a\u6b65\u63a8\u7406\u94fe\uff0c\u5b8c\u5168\u907f\u514d\u5728\u7ebfLLM\u63a8\u7406\uff0c\u4f9d\u8d56\u9ad8\u6548\u7684\u56fe\u904d\u5386\u548c\u9884\u8ba1\u7b97\u5d4c\u5165\u3002", "result": "PropRAG\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u53ec\u56de\u7387\u548cF1\u5206\u6570\uff0c\u5982PopQA\uff0855.3%\uff09\u30012Wiki\uff0893.7%\uff09\u3001HotpotQA\uff0897.0%\uff09\u548cMuSiQue\uff0877.3%\uff09\u3002", "conclusion": "PropRAG\u901a\u8fc7\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u548c\u663e\u5f0f\u7684\u5728\u7ebf\u8def\u5f84\u53d1\u73b0\uff0c\u63a8\u52a8\u4e86\u975e\u53c2\u6570\u6301\u7eed\u5b66\u4e60\u7684\u8fdb\u5c55\u3002"}}
{"id": "2504.18286", "pdf": "https://arxiv.org/pdf/2504.18286", "abs": "https://arxiv.org/abs/2504.18286", "authors": ["Christian Pionzewski", "Rebecca Rademacher", "J\u00e9r\u00f4me Rutinowski", "Antonia Ponikarov", "Stephan Matzke", "Tim Chilla", "Pia Schreynemackers", "Alice Kirchheim"], "title": "Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.10; I.4.9"], "comment": "Published in: 2024 International Conference on Machine Learning and\n  Applications (ICMLA), IEEE. 6 pages, 3 figures", "summary": "This contribution explores the impact of synthetic training data usage and\nthe prediction of material wear and aging in the context of re-identification.\nDifferent experimental setups and gallery set expanding strategies are tested,\nanalyzing their impact on performance over time for aging re-identification\nsubjects. Using a continuously updating gallery, we were able to increase our\nmean Rank-1 accuracy by 24%, as material aging was taken into account step by\nstep. In addition, using models trained with 10% artificial training data,\nRank-1 accuracy could be increased by up to 13%, in comparison to a model\ntrained on only real-world data, significantly boosting generalized performance\non hold-out data. Finally, this work introduces a novel, open-source\nre-identification dataset, pallet-block-2696. This dataset contains 2,696\nimages of Euro pallets, taken over a period of 4 months. During this time,\nnatural aging processes occurred and some of the pallets were damaged during\ntheir usage. These wear and tear processes significantly changed the appearance\nof the pallets, providing a dataset that can be used to generate synthetically\naged pallets or other wooden materials.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5bf9\u6750\u6599\u78e8\u635f\u548c\u8001\u5316\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u6269\u5c55\u7b56\u7565\u63d0\u5347\u4e86\u8001\u5316\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u3002", "motivation": "\u7814\u7a76\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5728\u6750\u6599\u8001\u5316\u91cd\u8bc6\u522b\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u5b9e\u9a8c\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "method": "\u6d4b\u8bd5\u4e0d\u540c\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u6269\u5c55\u7b56\u7565\uff0c\u4f7f\u7528\u6301\u7eed\u66f4\u65b0\u7684\u56fe\u5e93\u548c\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u4f7f\u7528\u5408\u6210\u8bad\u7ec3\u6570\u636e\u63d0\u5347Rank-1\u51c6\u786e\u738713%\uff0c\u52a8\u6001\u56fe\u5e93\u7b56\u7565\u63d0\u534724%\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u548c\u52a8\u6001\u56fe\u5e93\u7b56\u7565\u663e\u8457\u63d0\u5347\u8001\u5316\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2504.18078", "pdf": "https://arxiv.org/pdf/2504.18078", "abs": "https://arxiv.org/abs/2504.18078", "authors": ["Xiaolu Chen", "Chenghao Huang", "Yanru Zhang", "Hao Wang"], "title": "Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages", "summary": "The rapid expansion of distributed photovoltaic (PV) installations worldwide,\nmany being behind-the-meter systems, has significantly challenged energy\nmanagement and grid operations, as unobservable PV generation further\ncomplicates the supply-demand balance. Therefore, estimating this generation\nfrom net load, known as PV disaggregation, is critical. Given privacy concerns\nand the need for large training datasets, federated learning becomes a\npromising approach, but statistical heterogeneity, arising from geographical\nand behavioral variations among prosumers, poses new challenges to PV\ndisaggregation. To overcome these challenges, a privacy-preserving distributed\nPV disaggregation framework is proposed using Personalized Federated Learning\n(PFL). The proposed method employs a two-level framework that combines local\nand global modeling. At the local level, a transformer-based PV disaggregation\nmodel is designed to generate solar irradiance embeddings for representing\nlocal PV conditions. A novel adaptive local aggregation mechanism is adopted to\nmitigate the impact of statistical heterogeneity on the local model, extracting\na portion of global information that benefits the local model. At the global\nlevel, a central server aggregates information uploaded from multiple data\ncenters, preserving privacy while enabling cross-center knowledge sharing.\nExperiments on real-world data demonstrate the effectiveness of this proposed\nframework, showing improved accuracy and robustness compared to benchmark\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\uff08PFL\uff09\u7684\u5206\u5e03\u5f0f\u5149\u4f0f\u5206\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u672c\u5730\u548c\u5168\u5c40\u5efa\u6a21\u89e3\u51b3\u7edf\u8ba1\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5149\u4f0f\u53d1\u7535\u91cf\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u5149\u4f0f\uff08PV\uff09\u7684\u5feb\u901f\u589e\u957f\u5bf9\u80fd\u6e90\u7ba1\u7406\u548c\u7535\u7f51\u8fd0\u884c\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5c24\u5176\u662f\u65e0\u6cd5\u76f4\u63a5\u89c2\u6d4b\u7684PV\u53d1\u7535\u91cf\u3002\u9690\u79c1\u95ee\u9898\u548c\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u9700\u6c42\u4f7f\u5f97\u8054\u90a6\u5b66\u4e60\u6210\u4e3a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7edf\u8ba1\u5f02\u8d28\u6027\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u5c42\u6b21\u6846\u67b6\uff1a\u672c\u5730\u5c42\u9762\u8bbe\u8ba1\u57fa\u4e8eTransformer\u7684\u5149\u4f0f\u5206\u89e3\u6a21\u578b\uff0c\u751f\u6210\u592a\u9633\u8f90\u7167\u5ea6\u5d4c\u5165\uff1b\u5168\u5c40\u5c42\u9762\u901a\u8fc7\u4e2d\u592e\u670d\u52a1\u5668\u805a\u5408\u591a\u6570\u636e\u4e2d\u5fc3\u4fe1\u606f\u3002\u5f15\u5165\u81ea\u9002\u5e94\u672c\u5730\u805a\u5408\u673a\u5236\u4ee5\u51cf\u5c11\u7edf\u8ba1\u5f02\u8d28\u6027\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u9690\u79c1\u4fdd\u62a4\u5206\u5e03\u5f0f\u5149\u4f0f\u5206\u89e3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7edf\u8ba1\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u4e3a\u5149\u4f0f\u53d1\u7535\u91cf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18317", "pdf": "https://arxiv.org/pdf/2504.18317", "abs": "https://arxiv.org/abs/2504.18317", "authors": ["Zhengru Fang", "Zhenghao Liu", "Jingjing Wang", "Senkang Hu", "Yu Guo", "Yiqin Deng", "Yuguang Fang"], "title": "Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy", "categories": ["cs.CV", "cs.NI"], "comment": "Code and dataset will be made publicly available:\n  https://github.com/fangzr/TOC-Edge-Aerial", "summary": "To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles\n(UAVs) localization in urban areas where global positioning system (GPS)\nsignals are unavailable. Vision-based methods offer a viable alternative but\nface severe bandwidth, memory and processing constraints on lightweight UAVs.\nInspired by mammalian spatial cognition, we propose a task-oriented\ncommunication framework, where UAVs equipped with multi-camera systems extract\ncompact multi-view features and offload localization tasks to edge servers. We\nintroduce the Orthogonally-constrained Variational Information Bottleneck\nencoder (O-VIB), which incorporates automatic relevance determination (ARD) to\nprune non-informative features while enforcing orthogonality to minimize\nredundancy. This enables efficient and accurate localization with minimal\ntransmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows\nthat O-VIB achieves high-precision localization under stringent bandwidth\nbudgets. Code and dataset will be made publicly available:\ngithub.com/fangzr/TOC-Edge-Aerial.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u5bfc\u5411\u7684\u901a\u4fe1\u6846\u67b6O-VIB\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u65e0GPS\u4fe1\u53f7\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u9ad8\u6548\u7cbe\u51c6\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u65e0GPS\u4fe1\u53f7\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9\u8f7b\u91cf\u7ea7\u65e0\u4eba\u673a\u5728\u5e26\u5bbd\u3001\u5185\u5b58\u548c\u5904\u7406\u80fd\u529b\u4e0a\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u63d0\u53d6\u7d27\u51d1\u7684\u591a\u89c6\u89d2\u7279\u5f81\uff0c\u901a\u8fc7O-VIB\u7f16\u7801\u5668\u7ed3\u5408\u81ea\u52a8\u76f8\u5173\u6027\u786e\u5b9a\uff08ARD\uff09\u548c\u6b63\u4ea4\u7ea6\u675f\uff0c\u51cf\u5c11\u5197\u4f59\u7279\u5f81\u5e76\u5378\u8f7d\u5b9a\u4f4d\u4efb\u52a1\u81f3\u8fb9\u7f18\u670d\u52a1\u5668\u3002", "result": "\u5728\u4e13\u7528LAE\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\uff0cO-VIB\u5728\u4e25\u683c\u5e26\u5bbd\u9650\u5236\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "conclusion": "O-VIB\u6846\u67b6\u4e3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2504.18080", "pdf": "https://arxiv.org/pdf/2504.18080", "abs": "https://arxiv.org/abs/2504.18080", "authors": ["Wataru Kawakami", "Keita Suzuki", "Junichiro Iwasawa"], "title": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) show potential in medicine, yet clinical\nadoption is hindered by concerns over factual accuracy, language-specific\nlimitations (e.g., Japanese), and critically, their reliability when required\nto generate reasoning explanations -- a prerequisite for trust. This paper\nintroduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the\nJapanese medical domain to achieve both high accuracy and stable reasoning. We\nemploy a two-stage fine-tuning process on the Qwen2.5-72B base model: first,\nContinued Pretraining (CPT) on a comprehensive Japanese medical corpus instills\ndeep domain knowledge. Second, Reasoning Preference Optimization (RPO), a\npreference-based method, enhances the generation of reliable reasoning pathways\nwhile preserving high answer accuracy. Evaluations on the Japanese Medical\nLicensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves\nstate-of-the-art performance (0.868 accuracy), surpassing strong proprietary\nmodels like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which\nexhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively\non IgakuQA) when prompted for explanations, our model maintains its high\naccuracy (0.868) under such conditions. This highlights RPO's effectiveness in\nstabilizing reasoning generation. This work underscores the importance of\noptimizing for reliable explanations alongside accuracy. We release the\nPreferred-MedLLM-Qwen-72B model weights to foster research into trustworthy\nLLMs for specialized, high-stakes applications.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Preferred-MedLLM-Qwen-72B\uff0c\u4e00\u4e2a\u9488\u5bf9\u65e5\u672c\u533b\u5b66\u9886\u57df\u4f18\u5316\u768472B\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u5b9e\u73b0\u9ad8\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u5e94\u7528\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u8bed\u8a00\u9650\u5236\u53ca\u63a8\u7406\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u4e34\u5e8a\u4fe1\u4efb\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\uff1a1) \u5728\u65e5\u672c\u533b\u5b66\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff1b2) \u901a\u8fc7\u63a8\u7406\u504f\u597d\u4f18\u5316\u589e\u5f3a\u53ef\u9760\u63a8\u7406\u8def\u5f84\u751f\u6210\u3002", "result": "\u5728IgakuQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.868\u51c6\u786e\u7387\uff0c\u8d85\u8d8aGPT-4o\uff0c\u4e14\u5728\u751f\u6210\u89e3\u91ca\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u4f18\u5316\u53ef\u9760\u89e3\u91ca\u4e0e\u51c6\u786e\u6027\u540c\u7b49\u91cd\u8981\uff0c\u6a21\u578b\u6743\u91cd\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u53ef\u4fe1\u8d56LLM\u7814\u7a76\u3002"}}
{"id": "2504.18318", "pdf": "https://arxiv.org/pdf/2504.18318", "abs": "https://arxiv.org/abs/2504.18318", "authors": ["Yunze Deng", "Haijun Xiong", "Bin Feng", "Xinggang Wang", "Wenyu Liu"], "title": "STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-4D generation is rapidly developing and widely applied in various\nscenarios. However, existing methods often fail to incorporate adequate\nspatio-temporal modeling and prompt alignment within a unified framework,\nresulting in temporal inconsistencies, geometric distortions, or low-quality 4D\ncontent that deviates from the provided texts. Therefore, we propose STP4D, a\nnovel approach that aims to integrate comprehensive spatio-temporal-prompt\nconsistency modeling for high-quality text-to-4D generation. Specifically,\nSTP4D employs three carefully designed modules: Time-varying Prompt Embedding,\nGeometric Information Enhancement, and Temporal Extension Deformation, which\ncollaborate to accomplish this goal. Furthermore, STP4D is among the first\nmethods to exploit the Diffusion model to generate 4D Gaussians, combining the\nfine-grained modeling capabilities and the real-time rendering process of 4DGS\nwith the rapid inference speed of the Diffusion model. Extensive experiments\ndemonstrate that STP4D excels in generating high-fidelity 4D content with\nexceptional efficiency (approximately 4.6s per asset), surpassing existing\nmethods in both quality and speed.", "AI": {"tldr": "STP4D\u662f\u4e00\u79cd\u65b0\u7684\u6587\u672c\u52304D\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u7a7a\u548c\u63d0\u793a\u4e00\u81f4\u6027\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u65f6\u7a7a\u4e0d\u4e00\u81f4\u548c\u51e0\u4f55\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u52304D\u751f\u6210\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u7684\u65f6\u7a7a\u548c\u63d0\u793a\u5bf9\u9f50\u6846\u67b6\uff0c\u5bfc\u81f4\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u4f4e\u4e14\u4e0e\u6587\u672c\u4e0d\u7b26\u3002", "method": "STP4D\u91c7\u7528\u4e09\u4e2a\u6a21\u5757\uff1a\u65f6\u53d8\u63d0\u793a\u5d4c\u5165\u3001\u51e0\u4f55\u4fe1\u606f\u589e\u5f3a\u548c\u65f6\u95f4\u6269\u5c55\u53d8\u5f62\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u62104D\u9ad8\u65af\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTP4D\u5728\u751f\u6210\u9ad8\u4fdd\u771f4D\u5185\u5bb9\u65f6\u6548\u7387\u6781\u9ad8\uff08\u7ea64.6\u79d2/\u8d44\u4ea7\uff09\uff0c\u8d28\u91cf\u548c\u901f\u5ea6\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STP4D\u901a\u8fc7\u7edf\u4e00\u5efa\u6a21\u65f6\u7a7a\u548c\u63d0\u793a\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u52304D\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2504.18082", "pdf": "https://arxiv.org/pdf/2504.18082", "abs": "https://arxiv.org/abs/2504.18082", "authors": ["Vignesh Balaji", "Christos Kozyrakis", "Gal Chechik", "Haggai Maron"], "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.", "AI": {"tldr": "COMM-RAND\u662f\u4e00\u79cd\u7ed3\u5408\u793e\u533a\u7ed3\u6784\u611f\u77e5\u548c\u968f\u673a\u6027\u7684GNN\u5c0f\u6279\u91cf\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u4e14\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709GNN\u5c0f\u6279\u91cf\u8bad\u7ec3\u65b9\u6cd5\u5728\u6548\u7387\u548c\u7ed3\u6784\u611f\u77e5\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u968f\u673a\u5316\u65b9\u6cd5\u5ffd\u7565\u56fe\u7ed3\u6784\uff0c\u800c\u786e\u5b9a\u6027\u65b9\u6cd5\u727a\u7272\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faCOMM-RAND\uff0c\u5728\u968f\u673a\u6027\u548c\u56fe\u7ed3\u6784\u611f\u77e5\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u4f18\u5316GPU\u7f13\u5b58\u4f7f\u7528\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOMM-RAND\u5e73\u5747\u63d0\u901f1.8\u500d\uff0c\u7cbe\u5ea6\u635f\u5931\u4ec50.42%\u3002", "conclusion": "COMM-RAND\u4e3aGNN\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18325", "pdf": "https://arxiv.org/pdf/2504.18325", "abs": "https://arxiv.org/abs/2504.18325", "authors": ["Dongxin Lyu", "Han Huang", "Cheng Tan", "Zimu Li"], "title": "Depth3DLane: Monocular 3D Lane Detection via Depth Prior Distillation", "categories": ["cs.CV"], "comment": "Submitting to ICCV2025", "summary": "Monocular 3D lane detection is challenging due to the difficulty in capturing\ndepth information from single-camera images. A common strategy involves\ntransforming front-view (FV) images into bird's-eye-view (BEV) space through\ninverse perspective mapping (IPM), facilitating lane detection using BEV\nfeatures. However, IPM's flat-ground assumption and loss of contextual\ninformation lead to inaccuracies in reconstructing 3D information, especially\nheight. In this paper, we introduce a BEV-based framework to address these\nlimitations and improve 3D lane detection accuracy. Our approach incorporates a\nHierarchical Depth-Aware Head that provides multi-scale depth features,\nmitigating the flat-ground assumption by enhancing spatial awareness across\nvarying depths. Additionally, we leverage Depth Prior Distillation to transfer\nsemantic depth knowledge from a teacher model, capturing richer structural and\ncontextual information for complex lane structures. To further refine lane\ncontinuity and ensure smooth lane reconstruction, we introduce a Conditional\nRandom Field module that enforces spatial coherence in lane predictions.\nExtensive experiments validate that our method achieves state-of-the-art\nperformance in terms of z-axis error and outperforms other methods in the field\nin overall performance. The code is released at:\nhttps://anonymous.4open.science/r/Depth3DLane-DCDD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBEV\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6df1\u5ea6\u7279\u5f81\u548c\u6df1\u5ea6\u5148\u9a8c\u84b8\u998f\uff0c\u6539\u8fdb\u5355\u76ee3D\u8f66\u9053\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5355\u76ee3D\u8f66\u9053\u68c0\u6d4b\u56e0\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\u800c\u56f0\u96be\uff0c\u4f20\u7edfIPM\u65b9\u6cd5\u56e0\u5047\u8bbe\u5730\u9762\u5e73\u5766\u548c\u4e22\u5931\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bfc\u81f4\u4e0d\u51c6\u786e\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6df1\u5ea6\u611f\u77e5\u5934\u63d0\u4f9b\u591a\u5c3a\u5ea6\u6df1\u5ea6\u7279\u5f81\uff0c\u5229\u7528\u6df1\u5ea6\u5148\u9a8c\u84b8\u998f\u4ece\u6559\u5e08\u6a21\u578b\u4f20\u9012\u8bed\u4e49\u6df1\u5ea6\u77e5\u8bc6\uff0c\u5e76\u5f15\u5165\u6761\u4ef6\u968f\u673a\u573a\u6a21\u5757\u4f18\u5316\u8f66\u9053\u8fde\u7eed\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728z\u8f74\u8bef\u5dee\u548c\u6574\u4f53\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u591a\u5c3a\u5ea6\u6df1\u5ea6\u7279\u5f81\u548c\u6df1\u5ea6\u5148\u9a8c\u84b8\u998f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8f66\u9053\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.18085", "pdf": "https://arxiv.org/pdf/2504.18085", "abs": "https://arxiv.org/abs/2504.18085", "authors": ["Muhammad Mubashar", "Shireen Kudukkil Manchingal", "Fabio Cuzzolin"], "title": "Random-Set Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "16 pages, 6 figures", "summary": "Large Language Models (LLMs) are known to produce very high-quality tests and\nresponses to our queries. But how much can we trust this generated text? In\nthis paper, we study the problem of uncertainty quantification in LLMs. We\npropose a novel Random-Set Large Language Model (RSLLM) approach which predicts\nfinite random sets (belief functions) over the token space, rather than\nprobability vectors as in classical LLMs. In order to allow so efficiently, we\nalso present a methodology based on hierarchical clustering to extract and use\na budget of \"focal\" subsets of tokens upon which the belief prediction is\ndefined, rather than using all possible collections of tokens, making the\nmethod scalable yet effective. RS-LLMs encode the epistemic uncertainty induced\nin their generation process by the size and diversity of its training set via\nthe size of the credal sets associated with the predicted belief functions. The\nproposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,\nMistral-7b and Phi-2 models and is shown to outperform the standard model in\nboth datasets in terms of correctness of answer while also showing potential in\nestimating the second level uncertainty in its predictions and providing the\ncapability to detect when its hallucinating.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u96c6\u5927\u8bed\u8a00\u6a21\u578b\uff08RSLLM\uff09\uff0c\u7528\u4e8e\u91cf\u5316LLM\u751f\u6210\u6587\u672c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u9884\u6d4b\u6709\u9650\u968f\u673a\u96c6\uff08\u4fe1\u5ff5\u51fd\u6570\uff09\u800c\u975e\u4f20\u7edf\u6982\u7387\u5411\u91cf\uff0c\u5e76\u7ed3\u5408\u5206\u5c42\u805a\u7c7b\u65b9\u6cd5\u63d0\u9ad8\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660eRSLLM\u5728CoQA\u548cOBQA\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6807\u51c6\u6a21\u578b\uff0c\u5e76\u80fd\u6709\u6548\u4f30\u8ba1\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76LLM\u751f\u6210\u6587\u672c\u7684\u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfLLM\u4ec5\u8f93\u51fa\u6982\u7387\u5411\u91cf\u800c\u65e0\u6cd5\u5145\u5206\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u7684\u5c40\u9650\u3002", "method": "\u63d0\u51faRSLLM\u6a21\u578b\uff0c\u9884\u6d4b\u6709\u9650\u968f\u673a\u96c6\uff08\u4fe1\u5ff5\u51fd\u6570\uff09\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u805a\u7c7b\u63d0\u53d6\u5173\u952e\u5b50\u96c6\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u6a21\u578b\u901a\u8fc7\u8bad\u7ec3\u96c6\u7684\u5927\u5c0f\u548c\u591a\u6837\u6027\u7f16\u7801\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728CoQA\u548cOBQA\u6570\u636e\u96c6\u4e0a\uff0cRSLLM\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u7b54\u6848\u6b63\u786e\u6027\uff0c\u5e76\u80fd\u4f30\u8ba1\u7b2c\u4e8c\u5c42\u4e0d\u786e\u5b9a\u6027\u53ca\u68c0\u6d4b\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "RSLLM\u4e3aLLM\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u751f\u6210\u6587\u672c\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.18332", "pdf": "https://arxiv.org/pdf/2504.18332", "abs": "https://arxiv.org/abs/2504.18332", "authors": ["Shuting Zhao", "Linxin Bai", "Liangjing Shao", "Ye Zhang", "Xinrong Chen"], "title": "SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations", "categories": ["cs.CV", "cs.HC", "68U05"], "comment": "9 pages, 6 figures, conference ICMR 2025", "summary": "The growing applications of AR/VR increase the demand for real-time full-body\npose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint\nsignals from the head and hands, reconstructing a full-body pose remains\nchallenging due to the unconstrained lower body. Recent advancements often rely\non conventional neural networks and generative models to improve performance in\nthis task, such as Transformers and diffusion models. However, these approaches\nstruggle to strike a balance between achieving precise pose reconstruction and\nmaintaining fast inference speed. To overcome these challenges, a lightweight\nand efficient model, SSD-Poser, is designed for robust full-body motion\nestimation from sparse observations. SSD-Poser incorporates a well-designed\nhybrid encoder, State Space Attention Encoders, to adapt the state space\nduality to complex motion poses and enable real-time realistic pose\nreconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate\njitter caused by variable-frequency motion signals, remarkably enhancing the\nmotion smoothness. Comprehensive experiments on the AMASS dataset demonstrate\nthat SSD-Poser achieves exceptional accuracy and computational efficiency,\nshowing outstanding inference efficiency compared to state-of-the-art methods.", "AI": {"tldr": "SSD-Poser\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9ad8\u6548\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u89c2\u6d4b\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5168\u8eab\u59ff\u6001\u4f30\u8ba1\uff0c\u7ed3\u5408\u6df7\u5408\u7f16\u7801\u5668\u548c\u9891\u7387\u611f\u77e5\u89e3\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "AR/VR\u5e94\u7528\u5bf9\u5b9e\u65f6\u5168\u8eab\u59ff\u6001\u4f30\u8ba1\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u8bbe\u8ba1SSD-Poser\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u7f16\u7801\u5668\uff08State Space Attention Encoders\uff09\u548c\u9891\u7387\u611f\u77e5\u89e3\u7801\u5668\uff08Frequency-Aware Decoder\uff09\u3002", "result": "\u5728AMASS\u6570\u636e\u96c6\u4e0a\uff0cSSD-Poser\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u63a8\u7406\u901f\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SSD-Poser\u5728\u5b9e\u65f6\u5168\u8eab\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e0e\u7cbe\u5ea6\u7684\u5e73\u8861\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.18104", "pdf": "https://arxiv.org/pdf/2504.18104", "abs": "https://arxiv.org/abs/2504.18104", "authors": ["Yinglong Yu", "Hao Shen", "Zhengyi Lyu", "Qi He"], "title": "Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In response to the growing problem of misinformation in the context of\nglobalization and informatization, this paper proposes a classification method\nfor fact-check-worthiness estimation based on prompt tuning. We construct a\nmodel for fact-check-worthiness estimation at the methodological level using\nprompt tuning. By applying designed prompt templates to large language models,\nwe establish in-context learning and leverage prompt tuning technology to\nimprove the accuracy of determining whether claims have fact-check-worthiness,\nparticularly when dealing with limited or unlabeled data. Through extensive\nexperiments on public datasets, we demonstrate that the proposed method\nsurpasses or matches multiple baseline methods in the classification task of\nfact-check-worthiness estimation assessment, including classical pre-trained\nmodels such as BERT, as well as recent popular large models like GPT-3.5 and\nGPT-4. Experiments show that the prompt tuning-based method proposed in this\nstudy exhibits certain advantages in evaluation metrics such as F1 score and\naccuracy, thereby effectively validating its effectiveness and advancement in\nthe task of fact-check-worthiness estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u8c03\u4f18\u7684\u4e8b\u5b9e\u6838\u67e5\u4ef7\u503c\u4f30\u8ba1\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u63d0\u793a\u6a21\u677f\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5728\u6709\u9650\u6216\u65e0\u6807\u7b7e\u6570\u636e\u4e0b\u5224\u65ad\u4e8b\u5b9e\u6838\u67e5\u4ef7\u503c\u7684\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728F1\u5206\u6570\u548c\u51c6\u786e\u7387\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u6216\u5339\u914dBERT\u3001GPT-3.5\u548cGPT-4\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5168\u7403\u5316\u548c\u4fe1\u606f\u5316\u80cc\u666f\u4e0b\uff0c\u9519\u8bef\u4fe1\u606f\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4e8b\u5b9e\u6838\u67e5\u7684\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u63d0\u793a\u8c03\u4f18\u6280\u672f\uff0c\u8bbe\u8ba1\u63d0\u793a\u6a21\u677f\u5e76\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4f18\u5316\u4e8b\u5b9e\u6838\u67e5\u4ef7\u503c\u4f30\u8ba1\u7684\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728F1\u5206\u6570\u548c\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u6216\u5339\u914dBERT\u3001GPT-3.5\u548cGPT-4\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u8c03\u4f18\u7684\u65b9\u6cd5\u5728\u4e8b\u5b9e\u6838\u67e5\u4ef7\u503c\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u5148\u8fdb\u6027\u3002"}}
{"id": "2504.18348", "pdf": "https://arxiv.org/pdf/2504.18348", "abs": "https://arxiv.org/abs/2504.18348", "authors": ["Fengchun Liu. Tong Zhang", "Chunying Zhang"], "title": "TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "For deep learning-based image steganography frameworks, in order to ensure\nthe invisibility and recoverability of the information embedding, the loss\nfunction usually contains several losses such as embedding loss, recovery loss\nand steganalysis loss. In previous research works, fixed loss weights are\nusually chosen for training optimization, and this setting is not linked to the\nimportance of the steganography task itself and the training process. In this\npaper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for\nbalancing multinomial losses in deep learning image steganography algorithms.\nTSCL consists of two phases: a priori curriculum control and loss dynamics\ncontrol. The first phase firstly focuses the model on learning the information\nembedding of the original image by controlling the loss weights in the\nmulti-party adversarial training; secondly, it makes the model shift its\nlearning focus to improving the decoding accuracy; and finally, it makes the\nmodel learn to generate a steganographic image that is resistant to\nsteganalysis. In the second stage, the learning speed of each training task is\nevaluated by calculating the loss drop of the before and after iteration rounds\nto balance the learning of each task. Experimental results on three large\npublic datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL\nstrategy improves the quality of steganography, decoding accuracy and security.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u635f\u5931\u8c03\u5ea6\u5668\uff08TSCL\uff09\uff0c\u7528\u4e8e\u5e73\u8861\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u9690\u5199\u7b97\u6cd5\u4e2d\u7684\u591a\u9879\u635f\u5931\uff0c\u63d0\u5347\u9690\u5199\u8d28\u91cf\u3001\u89e3\u7801\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56fa\u5b9a\u635f\u5931\u6743\u91cd\u65e0\u6cd5\u9002\u5e94\u9690\u5199\u4efb\u52a1\u7684\u91cd\u8981\u6027\u548c\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5f71\u54cd\u4e86\u9690\u5199\u6548\u679c\u3002", "method": "TSCL\u5206\u4e3a\u5148\u9a8c\u8bfe\u7a0b\u63a7\u5236\u548c\u635f\u5931\u52a8\u6001\u63a7\u5236\u4e24\u9636\u6bb5\uff0c\u5206\u522b\u8c03\u6574\u6a21\u578b\u5b66\u4e60\u91cd\u70b9\u548c\u4efb\u52a1\u5b66\u4e60\u901f\u5ea6\u3002", "result": "\u5728ALASKA2\u3001VOC2012\u548cImageNet\u6570\u636e\u96c6\u4e0a\uff0cTSCL\u663e\u8457\u63d0\u5347\u4e86\u9690\u5199\u8d28\u91cf\u3001\u89e3\u7801\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "conclusion": "TSCL\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u6709\u6548\u4f18\u5316\u4e86\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u9690\u5199\u7b97\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2504.18113", "pdf": "https://arxiv.org/pdf/2504.18113", "abs": "https://arxiv.org/abs/2504.18113", "authors": ["Aniket Dixit", "Muhammad Ibrahim Khan", "Faizan Ahmed", "James Brusey"], "title": "Learning from Less: SINDy Surrogates in RL", "categories": ["cs.LG", "cs.AI"], "comment": "World Models @ ICLR 2025", "summary": "This paper introduces an approach for developing surrogate environments in\nreinforcement learning (RL) using the Sparse Identification of Nonlinear\nDynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach\nthrough extensive experiments in OpenAI Gym environments, particularly Mountain\nCar and Lunar Lander. Our results show that SINDy-based surrogate models can\naccurately capture the underlying dynamics of these environments while reducing\ncomputational costs by 20-35%. With only 75 interactions for Mountain Car and\n1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997, with\nmean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06\nfor LunarLander position. RL agents trained in these surrogate environments\nrequire fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs.\n1,000,000 for Lunar Lander) while achieving comparable performance to those\ntrained in the original environments, exhibiting similar convergence patterns\nand final performance metrics. This work contributes to the field of\nmodel-based RL by providing an efficient method for generating accurate,\ninterpretable surrogate environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528SINDy\u7b97\u6cd5\u4e3a\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u66ff\u4ee3\u73af\u5883\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u73af\u5883\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u4f7f\u7528SINDy\u7b97\u6cd5\u6784\u5efa\u66ff\u4ee3\u73af\u5883\uff0c\u5e76\u5728OpenAI Gym\u7684Mountain Car\u548cLunar Lander\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u66ff\u4ee3\u6a21\u578b\u80fd\u51c6\u786e\u6355\u6349\u73af\u5883\u52a8\u6001\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e20-35%\uff0cRL\u4ee3\u7406\u8bad\u7ec3\u6b65\u6570\u51cf\u5c11\u4e14\u6027\u80fd\u4e0e\u539f\u73af\u5883\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u66ff\u4ee3\u73af\u5883\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2504.18349", "pdf": "https://arxiv.org/pdf/2504.18349", "abs": "https://arxiv.org/abs/2504.18349", "authors": ["Hongyu Zhu", "Sichu Liang", "Wenwen Wang", "Boheng Li", "Tongxin Yuan", "Fangqi Li", "ShiLin Wang", "Zhuosheng Zhang"], "title": "Revisiting Data Auditing in Large Vision-Language Models", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "With the surge of large language models (LLMs), Large Vision-Language Models\n(VLMs)--which integrate vision encoders with LLMs for accurate visual\ngrounding--have shown great potential in tasks like generalist agents and\nrobotic control. However, VLMs are typically trained on massive web-scraped\nimages, raising concerns over copyright infringement and privacy violations,\nand making data auditing increasingly urgent. Membership inference (MI), which\ndetermines whether a sample was used in training, has emerged as a key auditing\ntechnique, with promising results on open-source VLMs like LLaVA (AUC > 80%).\nIn this work, we revisit these advances and uncover a critical issue: current\nMI benchmarks suffer from distribution shifts between member and non-member\nimages, introducing shortcut cues that inflate MI performance. We further\nanalyze the nature of these shifts and propose a principled metric based on\noptimal transport to quantify the distribution discrepancy. To evaluate MI in\nrealistic settings, we construct new benchmarks with i.i.d. member and\nnon-member images. Existing MI methods fail under these unbiased conditions,\nperforming only marginally better than chance. Further, we explore the\ntheoretical upper bound of MI by probing the Bayes Optimality within the VLM's\nembedding space and find the irreducible error rate remains high. Despite this\npessimistic outlook, we analyze why MI for VLMs is particularly challenging and\nidentify three practical scenarios--fine-tuning, access to ground-truth texts,\nand set-based inference--where auditing becomes feasible. Our study presents a\nsystematic view of the limits and opportunities of MI for VLMs, providing\nguidance for future efforts in trustworthy data auditing.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6210\u5458\u63a8\u7406\uff08MI\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b58\u5728\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709MI\u65b9\u6cd5\u5728\u65e0\u504f\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u6307\u51fa\u4e86\u4e09\u79cd\u53ef\u884c\u5ba1\u8ba1\u573a\u666f\u3002", "motivation": "\u968f\u7740VLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6570\u636e\u5ba1\u8ba1\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\uff0c\u4f46\u73b0\u6709MI\u65b9\u6cd5\u56e0\u5206\u5e03\u504f\u79fb\u95ee\u9898\u5bfc\u81f4\u6027\u80fd\u865a\u9ad8\uff0c\u9700\u91cd\u65b0\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002", "method": "\u5206\u6790\u4e86\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u65e0\u504f\u57fa\u51c6\u6d4b\u8bd5\u3002\u540c\u65f6\u63a2\u8ba8\u4e86MI\u7684\u7406\u8bba\u4e0a\u9650\u548c\u53ef\u884c\u573a\u666f\u3002", "result": "\u73b0\u6709MI\u65b9\u6cd5\u5728\u65e0\u504f\u6761\u4ef6\u4e0b\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\uff0c\u4f46\u53d1\u73b0\u4e86\u4e09\u79cd\u5b9e\u9645\u53ef\u884c\u7684\u5ba1\u8ba1\u573a\u666f\u3002", "conclusion": "\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86VLMs\u4e2dMI\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u53ef\u4fe1\u6570\u636e\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2504.18114", "pdf": "https://arxiv.org/pdf/2504.18114", "abs": "https://arxiv.org/abs/2504.18114", "authors": ["Atharva Kulkarni", "Yuan Zhang", "Joel Ruben Antony Moniz", "Xiou Ge", "Bo-Hsiang Tseng", "Dhivya Piraviperumal", "Swabha Swayamdipta", "Hong Yu"], "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc4\u4f30\u63ed\u793a\u4e86\u5f53\u524d\u5e7b\u89c9\u68c0\u6d4b\u6307\u6807\u7684\u4e0d\u8db3\uff0c\u53d1\u73b0LLM\uff08\u5982GPT-4\uff09\u8bc4\u4f30\u6548\u679c\u6700\u4f73\uff0c\u5e76\u63d0\u51fa\u9700\u8981\u66f4\u9c81\u68d2\u7684\u6307\u6807\u548c\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\u963b\u788d\u5176\u53ef\u9760\u6027\u548c\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u9a8c\u8bc1\u3002", "method": "\u5bf96\u7ec4\u5e7b\u89c9\u68c0\u6d4b\u6307\u6807\u57284\u4e2a\u6570\u636e\u96c6\u300137\u4e2a\u8bed\u8a00\u6a21\u578b\u548c5\u79cd\u89e3\u7801\u65b9\u6cd5\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e0d\u4e00\u81f4\u3001\u89c6\u91ce\u72ed\u7a84\u4e14\u53c2\u6570\u6269\u5c55\u6548\u679c\u4e0d\u4e00\u81f4\uff0c\u4f46LLM\uff08\u5982GPT-4\uff09\u8bc4\u4f30\u6548\u679c\u6700\u4f73\uff0c\u6a21\u5f0f\u641c\u7d22\u89e3\u7801\u65b9\u6cd5\u80fd\u51cf\u5c11\u5e7b\u89c9\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u5e7b\u89c9\u8bc4\u4f30\u6307\u6807\u548c\u7f13\u89e3\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2504.18355", "pdf": "https://arxiv.org/pdf/2504.18355", "abs": "https://arxiv.org/abs/2504.18355", "authors": ["Maximilian Xiling Li", "Korbinian Rudolf", "Nils Blank", "Rudolf Lioutikov"], "title": "Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Robotic agents need to understand how to interact with objects in their\nenvironment, both autonomously and during human-robot interactions. Affordance\ndetection on 3D point clouds, which identifies object regions that allow\nspecific interactions, has traditionally relied on deep learning models like\nPointNet++, DGCNN, or PointTransformerV3. However, these models operate as\nblack boxes, offering no insight into their decision-making processes.\nPrototypical Learning methods, such as ProtoPNet, provide an interpretable\nalternative to black-box models by employing a \"this looks like that\"\ncase-based reasoning approach. However, they have been primarily applied to\nimage-based tasks. In this work, we apply prototypical learning to models for\naffordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet\nbenchmark dataset show that prototypical models achieve competitive performance\nwith state-of-the-art black-box models and offer inherent interpretability.\nThis makes prototypical models a promising candidate for human-robot\ninteraction scenarios that require increased trust and safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u5b66\u4e60\u7684\u65b9\u6cd5\u7528\u4e8e3D\u70b9\u4e91\u7684\u53ef\u64cd\u4f5c\u6027\u68c0\u6d4b\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u9ed1\u76d2\u6a21\u578b\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u4e14\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u3002", "motivation": "\u4f20\u7edf3D\u70b9\u4e91\u53ef\u64cd\u4f5c\u6027\u68c0\u6d4b\u6a21\u578b\uff08\u5982PointNet++\u3001DGCNN\uff09\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u539f\u578b\u5b66\u4e60\uff08\u5982ProtoPNet\uff09\u5728\u56fe\u50cf\u4efb\u52a1\u4e2d\u5df2\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u672a\u5e94\u7528\u4e8e3D\u70b9\u4e91\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06\u539f\u578b\u5b66\u4e60\u65b9\u6cd5\u5e94\u7528\u4e8e3D\u70b9\u4e91\u53ef\u64cd\u4f5c\u6027\u68c0\u6d4b\uff0c\u901a\u8fc7\u201c\u7c7b\u4f3c\u6848\u4f8b\u63a8\u7406\u201d\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u57283D-AffordanceNet\u6570\u636e\u96c6\u4e0a\uff0c\u539f\u578b\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u9ed1\u76d2\u6a21\u578b\uff0c\u540c\u65f6\u5177\u5907\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u539f\u578b\u6a21\u578b\u5728\u53ef\u64cd\u4f5c\u6027\u68c0\u6d4b\u4e2d\u517c\u5177\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u5408\u9700\u8981\u4fe1\u4efb\u548c\u5b89\u5168\u7684\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u3002"}}
{"id": "2504.18142", "pdf": "https://arxiv.org/pdf/2504.18142", "abs": "https://arxiv.org/abs/2504.18142", "authors": ["Fida Ullah", "Muhammad Ahmad", "Muhammad Tayyab Zamir", "Muhammad Arif", "Grigori sidorov", "Edgardo Manuel Felipe River\u00f3n", "Alexander Gelbukh"], "title": "EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named Entity Recognition (NER) plays a pivotal role in various Natural\nLanguage Processing (NLP) tasks by identifying and classifying named entities\n(NEs) from unstructured data into predefined categories such as person,\norganization, location, date, and time. While extensive research exists for\nhigh-resource languages and general domains, NER in Urdu particularly within\ndomain-specific contexts like education remains significantly underexplored.\nThis is Due to lack of annotated datasets for educational content which limits\nthe ability of existing models to accurately identify entities such as academic\nroles, course names, and institutional terms, underscoring the urgent need for\ntargeted resources in this domain. To the best of our knowledge, no dataset\nexists in the domain of the Urdu language for this purpose. To achieve this\nobjective this study makes three key contributions. Firstly, we created a\nmanually annotated dataset in the education domain, named EDU-NER-2025, which\ncontains 13 unique most important entities related to education domain. Second,\nwe describe our annotation process and guidelines in detail and discuss the\nchallenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed\nkey linguistic challenges, such as morphological complexity and ambiguity,\nwhich are prevalent in formal Urdu texts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u6559\u80b2\u9886\u57df\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u7684\u4e0d\u8db3\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aEDU-NER-2025\u7684\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u6807\u6ce8\u8fc7\u7a0b\u4e2d\u7684\u6311\u6218\u548c\u4e4c\u5c14\u90fd\u8bed\u7684\u8bed\u8a00\u590d\u6742\u6027\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u5728\u6559\u80b2\u9886\u57df\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7814\u7a76\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u5b66\u672f\u89d2\u8272\u3001\u8bfe\u7a0b\u540d\u79f0\u7b49\u5b9e\u4f53\u3002", "method": "\u521b\u5efa\u4e86EDU-NER-2025\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5305\u542b13\u4e2a\u6559\u80b2\u9886\u57df\u5b9e\u4f53\uff1b\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6807\u6ce8\u8fc7\u7a0b\u548c\u6307\u5357\uff1b\u5206\u6790\u4e86\u4e4c\u5c14\u90fd\u8bed\u7684\u8bed\u8a00\u6311\u6218\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u9996\u4e2a\u4e4c\u5c14\u90fd\u8bed\u6559\u80b2\u9886\u57dfNER\u6570\u636e\u96c6\uff0c\u5e76\u603b\u7ed3\u4e86\u6807\u6ce8\u8fc7\u7a0b\u4e2d\u7684\u6311\u6218\u548c\u8bed\u8a00\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4e4c\u5c14\u90fd\u8bed\u6559\u80b2\u9886\u57dfNER\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\u548c\u65b9\u6cd5\u53c2\u8003\u3002"}}
{"id": "2504.18361", "pdf": "https://arxiv.org/pdf/2504.18361", "abs": "https://arxiv.org/abs/2504.18361", "authors": ["Haozhen Yan", "Yan Hong", "Jiahui Zhan", "Yikun Ji", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Recent advancements in image manipulation have achieved unprecedented\nprogress in generating photorealistic content, but also simultaneously\neliminating barriers to arbitrary manipulation and editing, raising concerns\nabout multimedia authenticity and cybersecurity. However, existing Image\nManipulation Detection and Localization (IMDL) methodologies predominantly\nfocus on splicing or copy-move forgeries, lacking dedicated benchmarks for\ninpainting-based manipulations. To bridge this gap, we present COCOInpaint, a\ncomprehensive benchmark specifically designed for inpainting detection, with\nthree key contributions: 1) High-quality inpainting samples generated by six\nstate-of-the-art inpainting models, 2) Diverse generation scenarios enabled by\nfour mask generation strategies with optional text guidance, and 3) Large-scale\ncoverage with 258,266 inpainted images with rich semantic diversity. Our\nbenchmark is constructed to emphasize intrinsic inconsistencies between\ninpainted and authentic regions, rather than superficial semantic artifacts\nsuch as object shapes. We establish a rigorous evaluation protocol using three\nstandard metrics to assess existing IMDL approaches. The dataset will be made\npublicly available to facilitate future research in this area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86COCOInpaint\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u57fa\u4e8e\u4fee\u590d\u7684\u56fe\u50cf\u7be1\u6539\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u62fc\u63a5\u6216\u590d\u5236\u79fb\u52a8\u4f2a\u9020\uff0c\u7f3a\u4e4f\u9488\u5bf9\u4fee\u590d\u7be1\u6539\u7684\u4e13\u7528\u57fa\u51c6\u3002", "method": "\u6784\u5efaCOCOInpaint\u57fa\u51c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u4fee\u590d\u6837\u672c\u3001\u591a\u6837\u5316\u751f\u6210\u573a\u666f\u548c\u5927\u89c4\u6a21\u6570\u636e\u8986\u76d6\u3002", "result": "\u63d0\u4f9b\u4e86258,266\u5f20\u4fee\u590d\u56fe\u50cf\uff0c\u5e76\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\u534f\u8bae\u3002", "conclusion": "COCOInpaint\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\uff0c\u5f3a\u8c03\u4fee\u590d\u533a\u57df\u4e0e\u771f\u5b9e\u533a\u57df\u7684\u5185\u5728\u4e0d\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.18160", "pdf": "https://arxiv.org/pdf/2504.18160", "abs": "https://arxiv.org/abs/2504.18160", "authors": ["Mathieu Petitbois", "R\u00e9my Portelas", "Sylvain Lamprier", "Ludovic Denoyer"], "title": "Offline Learning of Controllable Diverse Behaviors", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Generative Models for Robot Learning Workshop at ICLR 2025", "summary": "Imitation Learning (IL) techniques aim to replicate human behaviors in\nspecific tasks. While IL has gained prominence due to its effectiveness and\nefficiency, traditional methods often focus on datasets collected from experts\nto produce a single efficient policy. Recently, extensions have been proposed\nto handle datasets of diverse behaviors by mainly focusing on learning\ntransition-level diverse policies or on performing entropy maximization at the\ntrajectory level. While these methods may lead to diverse behaviors, they may\nnot be sufficient to reproduce the actual diversity of demonstrations or to\nallow controlled trajectory generation. To overcome these drawbacks, we propose\na different method based on two key features: a) Temporal Consistency that\nensures consistent behaviors across entire episodes and not just at the\ntransition level as well as b) Controllability obtained by constructing a\nlatent space of behaviors that allows users to selectively activate specific\nbehaviors based on their requirements. We compare our approach to\nstate-of-the-art methods over a diverse set of tasks and environments. Project\npage: https://mathieu-petitbois.github.io/projects/swr/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u884c\u4e3a\u591a\u6837\u6027\u548c\u53ef\u63a7\u8f68\u8ff9\u751f\u6210\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4e13\u6ce8\u4e8e\u4ece\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u5355\u4e00\u7b56\u7565\uff0c\u96be\u4ee5\u5904\u7406\u884c\u4e3a\u591a\u6837\u6027\u6216\u5b9e\u73b0\u53ef\u63a7\u8f68\u8ff9\u751f\u6210\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u884c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u884c\u4e3a\u5728\u6574\u6bb5\u8f68\u8ff9\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u9009\u62e9\u6027\u6fc0\u6d3b\u7279\u5b9a\u884c\u4e3a\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u4e0e\u73b0\u6709\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u884c\u4e3a\u591a\u6837\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3a\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18391", "pdf": "https://arxiv.org/pdf/2504.18391", "abs": "https://arxiv.org/abs/2504.18391", "authors": ["Tiankai Hang", "Jianmin Bao", "Fangyun Wei", "Dong Chen"], "title": "Fast Autoregressive Models for Continuous Latent Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Autoregressive models have demonstrated remarkable success in sequential data\ngeneration, particularly in NLP, but their extension to continuous-domain image\ngeneration presents significant challenges. Recent work, the masked\nautoregressive model (MAR), bypasses quantization by modeling per-token\ndistributions in continuous spaces using a diffusion head but suffers from slow\ninference due to the high computational cost of the iterative denoising\nprocess. To address this, we propose the Fast AutoRegressive model (FAR), a\nnovel framework that replaces MAR's diffusion head with a lightweight shortcut\nhead, enabling efficient few-step sampling while preserving autoregressive\nprinciples. Additionally, FAR seamlessly integrates with causal Transformers,\nextending them from discrete to continuous token generation without requiring\narchitectural modifications. Experiments demonstrate that FAR achieves\n$2.3\\times$ faster inference than MAR while maintaining competitive FID and IS\nscores. This work establishes the first efficient autoregressive paradigm for\nhigh-fidelity continuous-space image generation, bridging the critical gap\nbetween quality and scalability in visual autoregressive modeling.", "AI": {"tldr": "FAR\u6a21\u578b\u901a\u8fc7\u8f7b\u91cf\u7ea7shortcut head\u66ff\u4ee3MAR\u7684diffusion head\uff0c\u5b9e\u73b0\u9ad8\u6548\u8fde\u7eed\u7a7a\u95f4\u56fe\u50cf\u751f\u6210\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3MAR\u6a21\u578b\u5728\u8fde\u7eed\u7a7a\u95f4\u56fe\u50cf\u751f\u6210\u4e2d\u56e0\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFAR\u6846\u67b6\uff0c\u7528\u8f7b\u91cf\u7ea7shortcut head\u66ff\u6362MAR\u7684diffusion head\uff0c\u652f\u6301\u9ad8\u6548\u5c11\u6b65\u91c7\u6837\uff0c\u5e76\u4e0e\u56e0\u679cTransformer\u65e0\u7f1d\u96c6\u6210\u3002", "result": "FAR\u63a8\u7406\u901f\u5ea6\u6bd4MAR\u5feb2.3\u500d\uff0cFID\u548cIS\u5206\u6570\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "FAR\u9996\u6b21\u5efa\u7acb\u4e86\u9ad8\u6548\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u586b\u8865\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u5efa\u6a21\u4e2d\u8d28\u91cf\u4e0e\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2504.18397", "pdf": "https://arxiv.org/pdf/2504.18397", "abs": "https://arxiv.org/abs/2504.18397", "authors": ["Kesen Zhao", "Beier Zhu", "Qianru Sun", "Hanwang Zhang"], "title": "Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning greatly improves the interpretability and\nproblem-solving abilities of multimodal large language models (MLLMs). However,\nexisting approaches are focused on text CoT, limiting their ability to leverage\nvisual cues. Visual CoT remains underexplored, and the only work is based on\nsupervised fine-tuning (SFT) that relies on extensive labeled bounding-box data\nand is hard to generalize to unseen cases. In this paper, we introduce\nUnsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT\nreasoning via preference optimization. UV-CoT performs preference comparisons\nbetween model-generated bounding boxes (one is preferred and the other is\ndis-preferred), eliminating the need for bounding-box annotations. We get such\npreference data by introducing an automatic data generation pipeline. Given an\nimage, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using\na template prompt and then answers the question using each bounded region as\ninput. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these\nrankings serve as supervision to train the target MLLM with UV-CoT by\nminimizing negative log-likelihood losses. By emulating human\nperception--identifying key regions and reasoning based on them--UV-CoT can\nimprove visual comprehension, particularly in spatial reasoning tasks where\ntextual descriptions alone fall short. Our experiments on six datasets\ndemonstrate the superiority of UV-CoT, compared to the state-of-the-art textual\nand visual CoT methods. Our zero-shot testing on four unseen datasets shows the\nstrong generalization of UV-CoT. The code is available in\nhttps://github.com/kesenzhao/UV-CoT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u89c6\u89c9\u94fe\u5f0f\u601d\u7ef4\uff08UV-CoT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5b9e\u73b0\u56fe\u50cf\u7ea7\u63a8\u7406\uff0c\u65e0\u9700\u8fb9\u754c\u6846\u6807\u6ce8\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u94fe\u5f0f\u601d\u7ef4\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u7ebf\u7d22\u7684\u5229\u7528\uff0c\u4e14\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002UV-CoT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u5f0f\u63d0\u5347\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "method": "UV-CoT\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u504f\u597d\u6570\u636e\uff08\u6a21\u578b\u751f\u6210\u7684\u8fb9\u754c\u6846\u53ca\u5176\u54cd\u5e94\u6392\u540d\uff09\uff0c\u5229\u7528\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u76ee\u6807\u6a21\u578b\uff0c\u65e0\u9700\u6807\u6ce8\u8fb9\u754c\u6846\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cUV-CoT\u4f18\u4e8e\u73b0\u6709\u6587\u672c\u548c\u89c6\u89c9\u94fe\u5f0f\u601d\u7ef4\u65b9\u6cd5\uff0c\u4e14\u5728\u56db\u4e2a\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UV-CoT\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.18180", "pdf": "https://arxiv.org/pdf/2504.18180", "abs": "https://arxiv.org/abs/2504.18180", "authors": ["\u00de\u00f3rir Hrafn Har\u00f0arson", "Hrafn Loftsson", "Stef\u00e1n \u00d3lafsson"], "title": "Aligning Language Models for Icelandic Legal Text Summarization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at NoDaLiDa 2025", "summary": "The integration of language models in the legal domain holds considerable\npromise for streamlining processes and improving efficiency in managing\nextensive workloads. However, the specialized terminology, nuanced language,\nand formal style of legal texts can present substantial challenges. This study\nexamines whether preference-based training techniques, specifically\nReinforcement Learning from Human Feedback and Direct Preference Optimization,\ncan enhance models' performance in generating Icelandic legal summaries that\nalign with domain-specific language standards and user preferences. We compare\nmodels fine-tuned with preference training to those using conventional\nsupervised learning. Results indicate that preference training improves the\nlegal accuracy of generated summaries over standard fine-tuning but does not\nsignificantly enhance the overall quality of Icelandic language usage.\nDiscrepancies between automated metrics and human evaluations further\nunderscore the importance of qualitative assessment in developing language\nmodels for the legal domain.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u57fa\u4e8e\u504f\u597d\u7684\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982RLHF\u548cDPO\uff09\u662f\u5426\u80fd\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u51b0\u5c9b\u6cd5\u5f8b\u6458\u8981\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u504f\u597d\u8bad\u7ec3\u63d0\u9ad8\u4e86\u6cd5\u5f8b\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u8bed\u8a00\u8d28\u91cf\u6539\u8fdb\u4e0d\u660e\u663e\u3002", "motivation": "\u6cd5\u5f8b\u9886\u57df\u7684\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u6f5c\u529b\u5927\uff0c\u4f46\u4e13\u4e1a\u672f\u8bed\u548c\u6b63\u5f0f\u8bed\u8a00\u98ce\u683c\u5e26\u6765\u6311\u6218\uff0c\u9700\u63a2\u7d22\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u6bd4\u8f83\u504f\u597d\u8bad\u7ec3\uff08RLHF\u548cDPO\uff09\u4e0e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u5728\u51b0\u5c9b\u6cd5\u5f8b\u6458\u8981\u751f\u6210\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u504f\u597d\u8bad\u7ec3\u63d0\u9ad8\u4e86\u6cd5\u5f8b\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u51b0\u5c9b\u8bed\u8a00\u8d28\u91cf\u7684\u63d0\u5347\u4e0d\u663e\u8457\uff1b\u81ea\u52a8\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u504f\u597d\u8bad\u7ec3\u5728\u6cd5\u5f8b\u9886\u57df\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u7ed3\u5408\u5b9a\u6027\u8bc4\u4f30\u4ee5\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u3002"}}
{"id": "2504.18419", "pdf": "https://arxiv.org/pdf/2504.18419", "abs": "https://arxiv.org/abs/2504.18419", "authors": ["Carlo Sgaravatti", "Roberto Basla", "Riccardo Pieroni", "Matteo Corno", "Sergio M. Savaresi", "Luca Magri", "Giacomo Boracchi"], "title": "A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "We present a new way to detect 3D objects from multimodal inputs, leveraging\nboth LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an\nRGB detection network and a 3D LiDAR detector. We exploit late fusion\nprinciples to reduce LiDAR False Positives, matching LiDAR detections with RGB\nones by projecting the LiDAR bounding boxes on the image. We rely on cascade\nfusion principles to recover LiDAR False Negatives leveraging epipolar\nconstraints and frustums generated by RGB detections of separate views. Our\nsolution can be plugged on top of any underlying single-modal detectors,\nenabling a flexible training process that can take advantage of pre-trained\nLiDAR and RGB detectors, or train the two branches separately. We evaluate our\nresults on the KITTI object detection benchmark, showing significant\nperformance improvements, especially for the detection of Pedestrians and\nCyclists.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8f93\u5165\u76843D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408LiDAR\u548cRGB\u76f8\u673a\uff0c\u901a\u8fc7\u540e\u671f\u7ea7\u8054\u878d\u5408\u51cf\u5c11\u8bef\u62a5\u548c\u6f0f\u62a5\u3002", "motivation": "\u89e3\u51b3LiDAR\u68c0\u6d4b\u4e2d\u7684\u8bef\u62a5\u548c\u6f0f\u62a5\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u76843D\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u540e\u671f\u7ea7\u8054\u878d\u5408\u65b9\u6848\uff0c\u5c06LiDAR\u68c0\u6d4b\u7ed3\u679c\u4e0eRGB\u68c0\u6d4b\u7ed3\u679c\u5339\u914d\uff0c\u5229\u7528\u6781\u7ebf\u7ea6\u675f\u548c\u89c6\u9525\u6062\u590d\u6f0f\u68c0\u76ee\u6807\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u884c\u4eba\u548c\u9a91\u884c\u8005\u7684\u68c0\u6d4b\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7075\u6d3b\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u5355\u6a21\u6001\u68c0\u6d4b\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u60013D\u7269\u4f53\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.18424", "pdf": "https://arxiv.org/pdf/2504.18424", "abs": "https://arxiv.org/abs/2504.18424", "authors": ["Rui Li", "Biao Zhang", "Zhenyu Li", "Federico Tombari", "Peter Wonka"], "title": "LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning", "categories": ["cs.CV"], "comment": "Project page: https://ruili3.github.io/lari", "summary": "We present layered ray intersections (LaRI), a new method for unseen geometry\nreasoning from a single image. Unlike conventional depth estimation that is\nlimited to the visible surface, LaRI models multiple surfaces intersected by\nthe camera rays using layered point maps. Benefiting from the compact and\nlayered representation, LaRI enables complete, efficient, and view-aligned\ngeometric reasoning to unify object- and scene-level tasks. We further propose\nto predict the ray stopping index, which identifies valid intersecting pixels\nand layers from LaRI's output. We build a complete training data generation\npipeline for synthetic and real-world data, including 3D objects and scenes,\nwith necessary data cleaning steps and coordination between rendering engines.\nAs a generic method, LaRI's performance is validated in two scenarios: It\nyields comparable object-level results to the recent large generative model\nusing 4% of its training data and 17% of its parameters. Meanwhile, it achieves\nscene-level occluded geometry reasoning in only one feed-forward.", "AI": {"tldr": "LaRI\u662f\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u8fdb\u884c\u672a\u89c1\u51e0\u4f55\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u70b9\u56fe\u5efa\u6a21\u76f8\u673a\u5149\u7ebf\u76f8\u4ea4\u7684\u591a\u4e2a\u8868\u9762\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u5b8c\u6574\u7684\u51e0\u4f55\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f30\u8ba1\u4ec5\u5c40\u9650\u4e8e\u53ef\u89c1\u8868\u9762\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u8868\u9762\u6216\u906e\u6321\u60c5\u51b5\uff0cLaRI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u70b9\u56fe\u5efa\u6a21\u5149\u7ebf\u76f8\u4ea4\u7684\u591a\u8868\u9762\uff0c\u9884\u6d4b\u5149\u7ebf\u505c\u6b62\u7d22\u5f15\u4ee5\u8bc6\u522b\u6709\u6548\u50cf\u7d20\u548c\u5c42\uff0c\u5e76\u6784\u5efa\u5b8c\u6574\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5728\u5bf9\u8c61\u7ea7\u522b\uff0cLaRI\u4ec5\u75284%\u7684\u8bad\u7ec3\u6570\u636e\u548c17%\u7684\u53c2\u6570\u8fbe\u5230\u4e0e\u5927\u578b\u751f\u6210\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff1b\u5728\u573a\u666f\u7ea7\u522b\uff0c\u4ec5\u9700\u4e00\u6b21\u524d\u5411\u63a8\u7406\u5373\u53ef\u5b8c\u6210\u906e\u6321\u51e0\u4f55\u63a8\u7406\u3002", "conclusion": "LaRI\u662f\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u7edf\u4e00\u5bf9\u8c61\u548c\u573a\u666f\u7ea7\u522b\u7684\u51e0\u4f55\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2504.18230", "pdf": "https://arxiv.org/pdf/2504.18230", "abs": "https://arxiv.org/abs/2504.18230", "authors": ["He Shanxuan", "Lin Zuhong", "Yu Bolun", "Gao Xu", "Long Biao", "Yao Jingjing"], "title": "Learning to fuse: dynamic integration of multi-source data for accurate battery lifespan prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate prediction of lithium-ion battery lifespan is vital for ensuring\noperational reliability and reducing maintenance costs in applications like\nelectric vehicles and smart grids. This study presents a hybrid learning\nframework for precise battery lifespan prediction, integrating dynamic\nmulti-source data fusion with a stacked ensemble (SE) modeling approach. By\nleveraging heterogeneous datasets from the National Aeronautics and Space\nAdministration (NASA), Center for Advanced Life Cycle Engineering (CALCE),\nMIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA)\nchemistries, an entropy-based dynamic weighting mechanism mitigates variability\nacross heterogeneous datasets. The SE model combines Ridge regression, long\nshort-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost),\neffectively capturing temporal dependencies and nonlinear degradation patterns.\nIt achieves a mean absolute error (MAE) of 0.0058, root mean square error\n(RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839,\noutperforming established baseline models with a 46.2% improvement in R2 and an\n83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis\nidentifies differential discharge capacity (Qdlin) and temperature of\nmeasurement (Temp_m) as critical aging indicators. This scalable, interpretable\nframework enhances battery health management, supporting optimized maintenance\nand safety across diverse energy storage systems, thereby contributing to\nimproved battery health management in energy storage systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u591a\u6e90\u6570\u636e\u878d\u5408\u548c\u5806\u53e0\u96c6\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u7cbe\u786e\u9884\u6d4b\u9502\u79bb\u5b50\u7535\u6c60\u5bff\u547d\u3002", "motivation": "\u9502\u79bb\u5b50\u7535\u6c60\u5bff\u547d\u9884\u6d4b\u5bf9\u7535\u52a8\u6c7d\u8f66\u548c\u667a\u80fd\u7535\u7f51\u7b49\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u7ef4\u62a4\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6574\u5408NASA\u3001CALCE\u3001TRC\u548cNCA\u6570\u636e\u96c6\uff0c\u91c7\u7528\u57fa\u4e8e\u71b5\u7684\u52a8\u6001\u52a0\u6743\u673a\u5236\uff0c\u7ed3\u5408Ridge\u56de\u5f52\u3001LSTM\u548cXGBoost\u7684\u5806\u53e0\u96c6\u6210\u6a21\u578b\u3002", "result": "MAE\u4e3a0.0058\uff0cRMSE\u4e3a0.0092\uff0cR2\u4e3a0.9839\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u7535\u6c60\u5065\u5eb7\u7ba1\u7406\u3002"}}
{"id": "2504.18447", "pdf": "https://arxiv.org/pdf/2504.18447", "abs": "https://arxiv.org/abs/2504.18447", "authors": ["Ryo Yamaki", "Shintaro Shiba", "Guillermo Gallego", "Yoshimitsu Aoki"], "title": "Iterative Event-based Motion Segmentation by Variational Contrast Maximization", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "11 pages, 9 figures, 3 tables, CVPR Workshop 2025", "summary": "Event cameras provide rich signals that are suitable for motion estimation\nsince they respond to changes in the scene. As any visual changes in the scene\nproduce event data, it is paramount to classify the data into different motions\n(i.e., motion segmentation), which is useful for various tasks such as object\ndetection and visual servoing. We propose an iterative motion segmentation\nmethod, by classifying events into background (e.g., dominant motion\nhypothesis) and foreground (independent motion residuals), thus extending the\nContrast Maximization framework. Experimental results demonstrate that the\nproposed method successfully classifies event clusters both for public and\nself-recorded datasets, producing sharp, motion-compensated edge-like images.\nThe proposed method achieves state-of-the-art accuracy on moving object\ndetection benchmarks with an improvement of over 30%, and demonstrates its\npossibility of applying to more complex and noisy real-world scenes. We hope\nthis work broadens the sensitivity of Contrast Maximization with respect to\nboth motion parameters and input events, thus contributing to theoretical\nadvancements in event-based motion segmentation estimation.\nhttps://github.com/aoki-media-lab/event_based_segmentation_vcmax", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u8fed\u4ee3\u8fd0\u52a8\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e8b\u4ef6\u5206\u7c7b\u4e3a\u80cc\u666f\u548c\u524d\u666f\uff0c\u6269\u5c55\u4e86\u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u7269\u4f53\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u80fd\u591f\u6355\u6349\u573a\u666f\u53d8\u5316\uff0c\u4f46\u9700\u8981\u5c06\u4e8b\u4ef6\u6570\u636e\u5206\u7c7b\u4e3a\u4e0d\u540c\u8fd0\u52a8\u4ee5\u5b9e\u73b0\u8fd0\u52a8\u5206\u5272\uff0c\u8fd9\u5bf9\u76ee\u6807\u68c0\u6d4b\u548c\u89c6\u89c9\u4f3a\u670d\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u8fd0\u52a8\u5206\u5272\u65b9\u6cd5\uff0c\u5c06\u4e8b\u4ef6\u5206\u4e3a\u80cc\u666f\uff08\u4e3b\u5bfc\u8fd0\u52a8\u5047\u8bbe\uff09\u548c\u524d\u666f\uff08\u72ec\u7acb\u8fd0\u52a8\u6b8b\u5dee\uff09\uff0c\u6269\u5c55\u4e86\u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u6846\u67b6\u3002", "result": "\u5728\u516c\u5f00\u548c\u81ea\u5f55\u6570\u636e\u96c6\u4e0a\u6210\u529f\u5206\u7c7b\u4e8b\u4ef6\u7c07\uff0c\u751f\u6210\u6e05\u6670\u7684\u8fd0\u52a8\u8865\u507f\u8fb9\u7f18\u56fe\u50cf\uff0c\u8fd0\u52a8\u7269\u4f53\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u534730%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u6846\u67b6\u7684\u654f\u611f\u6027\uff0c\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u8fd0\u52a8\u5206\u5272\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18231", "pdf": "https://arxiv.org/pdf/2504.18231", "abs": "https://arxiv.org/abs/2504.18231", "authors": ["Petar Labura", "Tomislav Antic", "Tomislav Capuder"], "title": "Time and Frequency Domain-based Anomaly Detection in Smart Meter Data for Distribution Network Studies", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "The widespread integration of new technologies in low-voltage distribution\nnetworks on the consumer side creates the need for distribution system\noperators to perform advanced real-time calculations to estimate network\nconditions. In recent years, data-driven models based on machine learning and\nbig data analysis have emerged for calculation purposes, leveraging the\ninformation available in large datasets obtained from smart meters and other\nadvanced measurement infrastructure. However, existing data-driven algorithms\ndo not take into account the quality of data collected from smart meters. They\nlack built-in anomaly detection mechanisms and fail to differentiate anomalies\nbased on whether the value or context of anomalous data instances deviates from\nthe norm. This paper focuses on methods for detecting and mitigating the impact\nof anomalies on the consumption of active and reactive power datasets. It\nproposes an anomaly detection framework based on the Isolation Forest machine\nlearning algorithm and Fast Fourier Transform filtering that works in both the\ntime and frequency domain and is unaffected by point anomalies or contextual\nanomalies of the power consumption data. The importance of integrating anomaly\ndetection methods is demonstrated in the analysis important for distribution\nnetworks with a high share of smart meters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9694\u79bb\u68ee\u6797\u7b97\u6cd5\u548c\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u6ee4\u6ce2\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u667a\u80fd\u7535\u8868\u6570\u636e\u4e2d\u7684\u5f02\u5e38\uff0c\u4ee5\u63d0\u5347\u4f4e\u538b\u914d\u7535\u7f51\u7edc\u7684\u5b9e\u65f6\u8ba1\u7b97\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u4f4e\u538b\u914d\u7535\u7f51\u7edc\u4e2d\u667a\u80fd\u7535\u8868\u7b49\u6280\u672f\u7684\u666e\u53ca\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\u672a\u8003\u8651\u6570\u636e\u8d28\u91cf\uff0c\u7f3a\u4e4f\u5f02\u5e38\u68c0\u6d4b\u673a\u5236\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u7ed3\u5408\u9694\u79bb\u68ee\u6797\u7b97\u6cd5\u548c\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u6ee4\u6ce2\uff0c\u5728\u65f6\u57df\u548c\u9891\u57df\u4e2d\u68c0\u6d4b\u5e76\u7f13\u89e3\u5f02\u5e38\u6570\u636e\u5bf9\u6709\u529f\u548c\u65e0\u529f\u529f\u7387\u6570\u636e\u96c6\u7684\u5f71\u54cd\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u533a\u5206\u70b9\u5f02\u5e38\u548c\u4e0a\u4e0b\u6587\u5f02\u5e38\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u7535\u8868\u9ad8\u5360\u6bd4\u7684\u914d\u7535\u7f51\u7edc\u3002", "conclusion": "\u96c6\u6210\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u63d0\u5347\u914d\u7535\u7f51\u7edc\u6570\u636e\u9a71\u52a8\u7684\u5b9e\u65f6\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.18448", "pdf": "https://arxiv.org/pdf/2504.18448", "abs": "https://arxiv.org/abs/2504.18448", "authors": ["Haotian Dong", "Xin Wang", "Di Lin", "Yipeng Wu", "Qin Chen", "Ruonan Liu", "Kairui Yang", "Ping Li", "Qing Guo"], "title": "NoiseController: Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration", "categories": ["cs.CV"], "comment": null, "summary": "High-quality video generation is crucial for many fields, including the film\nindustry and autonomous driving. However, generating videos with spatiotemporal\nconsistencies remains challenging. Current methods typically utilize attention\nmechanisms or modify noise to achieve consistent videos, neglecting global\nspatiotemporal information that could help ensure spatial and temporal\nconsistency during video generation. In this paper, we propose the\nNoiseController, consisting of Multi-Level Noise Decomposition, Multi-Frame\nNoise Collaboration, and Joint Denoising, to enhance spatiotemporal\nconsistencies in video generation. In multi-level noise decomposition, we first\ndecompose initial noises into scene-level foreground/background noises,\ncapturing distinct motion properties to model multi-view foreground/background\nvariations. Furthermore, each scene-level noise is further decomposed into\nindividual-level shared and residual components. The shared noise preserves\nconsistency, while the residual component maintains diversity. In multi-frame\nnoise collaboration, we introduce an inter-view spatiotemporal collaboration\nmatrix and an intra-view impact collaboration matrix , which captures mutual\ncross-view effects and historical cross-frame impacts to enhance video quality.\nThe joint denoising contains two parallel denoising U-Nets to remove each\nscene-level noise, mutually enhancing video generation. We evaluate our\nNoiseController on public datasets focusing on video generation and downstream\ntasks, demonstrating its state-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNoiseController\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ea7\u566a\u58f0\u5206\u89e3\u3001\u591a\u5e27\u566a\u58f0\u534f\u4f5c\u548c\u8054\u5408\u53bb\u566a\uff0c\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "motivation": "\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u5bf9\u7535\u5f71\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u89c6\u5168\u5c40\u65f6\u7a7a\u4fe1\u606f\uff0c\u5bfc\u81f4\u65f6\u7a7a\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "NoiseController\u5305\u542b\u591a\u7ea7\u566a\u58f0\u5206\u89e3\uff08\u573a\u666f\u7ea7\u548c\u4e2a\u4f53\u7ea7\u566a\u58f0\uff09\u3001\u591a\u5e27\u566a\u58f0\u534f\u4f5c\uff08\u8de8\u89c6\u56fe\u548c\u8de8\u5e27\u77e9\u9635\uff09\u548c\u8054\u5408\u53bb\u566a\uff08\u5e76\u884cU-Net\uff09\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cNoiseController\u5728\u89c6\u9891\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "NoiseController\u901a\u8fc7\u5168\u5c40\u65f6\u7a7a\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.18246", "pdf": "https://arxiv.org/pdf/2504.18246", "abs": "https://arxiv.org/abs/2504.18246", "authors": ["Ritesh Goru", "Shanay Mehta", "Prateek Jain"], "title": "Efficient Single-Pass Training for Multi-Turn Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u54cd\u5e94\u4ee4\u724c\u590d\u5236\u548c\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u63a9\u7801\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u591a\u8f6e\u63a8\u7406\u6570\u636e\u4e0a\u5fae\u8c03LLMs\u65f6\u65e0\u6cd5\u5355\u6b21\u524d\u5411\u5904\u7406\u7684\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5728\u591a\u8f6e\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cLLMs\u751f\u6210\u7684\u63a8\u7406\u4ee4\u724c\u88ab\u6392\u9664\u5728\u540e\u7eed\u8f93\u5165\u4e4b\u5916\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5355\u6b21\u524d\u5411\u5904\u7406\u6574\u4e2a\u5bf9\u8bdd\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u91c7\u7528\u54cd\u5e94\u4ee4\u724c\u590d\u5236\u548c\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u786e\u4fdd\u9002\u5f53\u7684\u53ef\u89c1\u6027\u7ea6\u675f\uff0c\u4ece\u800c\u652f\u6301\u5355\u6b21\u524d\u5411\u5904\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u8f6e\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u9ad8\u6548\u5fae\u8c03\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u63a8\u7406\u6570\u636e\u5fae\u8c03\u4e2d\u7684\u9650\u5236\uff0c\u4e3aLLMs\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18468", "pdf": "https://arxiv.org/pdf/2504.18468", "abs": "https://arxiv.org/abs/2504.18468", "authors": ["Georgios Kouros", "Minye Wu", "Tinne Tuytelaars"], "title": "RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects", "categories": ["cs.CV"], "comment": null, "summary": "We introduce RGS-DR, a novel inverse rendering method for reconstructing and\nrendering glossy and reflective objects with support for flexible relighting\nand scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian\nSplatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D\nGaussian surfel representation to accurately estimate geometry and surface\nnormals, an essential property for high-quality inverse rendering. Our approach\nexplicitly models geometric and material properties through learnable\nprimitives rasterized into a deferred shading pipeline, effectively reducing\nrendering artifacts and preserving sharp reflections. By employing a\nmulti-level cube mipmap, RGS-DR accurately approximates environment lighting\nintegrals, facilitating high-quality reconstruction and relighting. A residual\npass with spherical-mipmap-based directional encoding further refines the\nappearance modeling. Experiments demonstrate that RGS-DR achieves high-quality\nreconstruction and rendering quality for shiny objects, often outperforming\nreconstruction-exclusive state-of-the-art methods incapable of relighting.", "AI": {"tldr": "RGS-DR\u662f\u4e00\u79cd\u65b0\u7684\u9006\u6e32\u67d3\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u91cd\u5efa\u548c\u6e32\u67d3\u5177\u6709\u5149\u6cfd\u548c\u53cd\u5c04\u7279\u6027\u7684\u7269\u4f53\uff0c\u652f\u6301\u7075\u6d3b\u7684\u91cd\u65b0\u5149\u7167\u548c\u573a\u666f\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982NeRF\u548c3D\u9ad8\u65af\u6cfc\u6e85\uff09\u5728\u5904\u7406\u89c6\u89d2\u4f9d\u8d56\u6548\u5e94\u65f6\u8868\u73b0\u4e0d\u4f73\uff0cRGS-DR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RGS-DR\u4f7f\u75282D\u9ad8\u65af\u9762\u5143\u8868\u793a\u51c6\u786e\u4f30\u8ba1\u51e0\u4f55\u548c\u8868\u9762\u6cd5\u7ebf\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u57fa\u5143\u5efa\u6a21\u51e0\u4f55\u548c\u6750\u8d28\u5c5e\u6027\uff0c\u5e76\u91c7\u7528\u591a\u7ea7\u7acb\u65b9\u4f53mipmap\u8fd1\u4f3c\u73af\u5883\u5149\u7167\u79ef\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRGS-DR\u5728\u5149\u6cfd\u7269\u4f53\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u65e0\u6cd5\u91cd\u65b0\u5149\u7167\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RGS-DR\u901a\u8fc7\u521b\u65b0\u7684\u8868\u793a\u548c\u6e32\u67d3\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5149\u6cfd\u7269\u4f53\u7684\u91cd\u5efa\u548c\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2504.18490", "pdf": "https://arxiv.org/pdf/2504.18490", "abs": "https://arxiv.org/abs/2504.18490", "authors": ["Andrews Danyo", "Anthony Dontoh", "Armstrong Aboah"], "title": "An Improved ResNet50 Model for Predicting Pavement Condition Index (PCI) Directly from Pavement Images", "categories": ["cs.CV"], "comment": null, "summary": "Accurately predicting the Pavement Condition Index (PCI), a measure of\nroadway conditions, from pavement images is crucial for infrastructure\nmaintenance. This study proposes an enhanced version of the Residual Network\n(ResNet50) architecture, integrated with a Convolutional Block Attention Module\n(CBAM), to predict PCI directly from pavement images without additional\nannotations. By incorporating CBAM, the model autonomously prioritizes critical\nfeatures within the images, improving prediction accuracy. Compared to the\noriginal baseline ResNet50 and DenseNet161 architectures, the enhanced\nResNet50-CBAM model achieved a significantly lower mean absolute percentage\nerror (MAPE) of 58.16%, compared to the baseline models that achieved 70.76%\nand 65.48% respectively. These results highlight the potential of using\nattention mechanisms to refine feature extraction, ultimately enabling more\naccurate and efficient assessments of pavement conditions. This study\nemphasizes the importance of targeted feature refinement in advancing automated\npavement analysis through attention mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CBAM\u7684\u6539\u8fdbResNet50\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u8def\u9762\u56fe\u50cf\u76f4\u63a5\u9884\u6d4bPCI\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u8def\u9762\u72b6\u51b5\u6307\u6570\uff08PCI\uff09\u5bf9\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9884\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u5728ResNet50\u67b6\u6784\u4e2d\u96c6\u6210CBAM\u6a21\u5757\uff0c\u81ea\u4e3b\u4f18\u5148\u63d0\u53d6\u5173\u952e\u7279\u5f81\u3002", "result": "\u6539\u8fdb\u540e\u7684ResNet50-CBAM\u6a21\u578bMAPE\u4e3a58.16%\uff0c\u4f18\u4e8e\u539f\u59cbResNet50\uff0870.76%\uff09\u548cDenseNet161\uff0865.48%\uff09\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u80fd\u4f18\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u63d0\u5347\u8def\u9762\u72b6\u51b5\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.18253", "pdf": "https://arxiv.org/pdf/2504.18253", "abs": "https://arxiv.org/abs/2504.18253", "authors": ["Amirhossein Zhalehmehrabi", "Daniele Meli", "Francesco Dal Santo", "Francesco Trotti", "Alessandro Farinelli"], "title": "Depth-Constrained ASV Navigation with Deep RL and Limited Sensing", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 8 figures", "summary": "Autonomous Surface Vehicles (ASVs) play a crucial role in maritime\noperations, yet their navigation in shallow-water environments remains\nchallenging due to dynamic disturbances and depth constraints. Traditional\nnavigation strategies struggle with limited sensor information, making safe and\nefficient operation difficult. In this paper, we propose a reinforcement\nlearning (RL) framework for ASV navigation under depth constraints, where the\nvehicle must reach a target while avoiding unsafe areas with only a single\ndepth measurement per timestep from a downward-facing Single Beam Echosounder\n(SBES). To enhance environmental awareness, we integrate Gaussian Process (GP)\nregression into the RL framework, enabling the agent to progressively estimate\na bathymetric depth map from sparse sonar readings. This approach improves\ndecision-making by providing a richer representation of the environment.\nFurthermore, we demonstrate effective sim-to-real transfer, ensuring that\ntrained policies generalize well to real-world aquatic conditions. Experimental\nresults validate our method's capability to improve ASV navigation performance\nwhile maintaining safety in challenging shallow-water environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u9ad8\u65af\u8fc7\u7a0b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\u5728\u6d45\u6c34\u73af\u5883\u4e2d\u7684\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u4f20\u611f\u5668\u4fe1\u606f\u6709\u9650\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u6d45\u6c34\u73af\u5883\u4e2d\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\u7684\u5bfc\u822a\u9762\u4e34\u52a8\u6001\u5e72\u6270\u548c\u6df1\u5ea6\u9650\u5236\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u4f20\u611f\u5668\u4fe1\u606f\u6709\u9650\u96be\u4ee5\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u64cd\u4f5c\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff0c\u5229\u7528\u5355\u6ce2\u675f\u6d4b\u6df1\u4eea\u7a00\u758f\u6570\u636e\u9010\u6b65\u4f30\u8ba1\u6c34\u6df1\u56fe\uff0c\u63d0\u5347\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u5bfc\u822a\u6027\u80fd\uff0c\u5e76\u5728\u6311\u6218\u6027\u6d45\u6c34\u73af\u5883\u4e2d\u4fdd\u6301\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6d45\u6c34\u5bfc\u822a\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.18509", "pdf": "https://arxiv.org/pdf/2504.18509", "abs": "https://arxiv.org/abs/2504.18509", "authors": ["Shivam Duggal", "Yushi Hu", "Oscar Michel", "Aniruddha Kembhavi", "William T. Freeman", "Noah A. Smith", "Ranjay Krishna", "Antonio Torralba", "Ali Farhadi", "Wei-Chiu Ma"], "title": "Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation", "categories": ["cs.CV"], "comment": "CVPR 2025. Project page and codes: https://eval3d.github.io/", "summary": "Despite the unprecedented progress in the field of 3D generation, current\nsystems still often fail to produce high-quality 3D assets that are visually\nappealing and geometrically and semantically consistent across multiple\nviewpoints. To effectively assess the quality of the generated 3D data, there\nis a need for a reliable 3D evaluation tool. Unfortunately, existing 3D\nevaluation metrics often overlook the geometric quality of generated assets or\nmerely rely on black-box multimodal large language models for coarse\nassessment. In this paper, we introduce Eval3D, a fine-grained, interpretable\nevaluation tool that can faithfully evaluate the quality of generated 3D assets\nbased on various distinct yet complementary criteria. Our key observation is\nthat many desired properties of 3D generation, such as semantic and geometric\nconsistency, can be effectively captured by measuring the consistency among\nvarious foundation models and tools. We thus leverage a diverse set of models\nand tools as probes to evaluate the inconsistency of generated 3D assets across\ndifferent aspects. Compared to prior work, Eval3D provides pixel-wise\nmeasurement, enables accurate 3D spatial feedback, and aligns more closely with\nhuman judgments. We comprehensively evaluate existing 3D generation models\nusing Eval3D and highlight the limitations and challenges of current models.", "AI": {"tldr": "Eval3D\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u76843D\u751f\u6210\u8bc4\u4f30\u5de5\u5177\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u4e00\u81f4\u6027\u8bc4\u4f303D\u8d44\u4ea7\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d3D\u751f\u6210\u7cfb\u7edf\u5728\u89c6\u89c9\u5438\u5f15\u529b\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5229\u7528\u591a\u79cd\u57fa\u7840\u6a21\u578b\u548c\u5de5\u5177\u4f5c\u4e3a\u63a2\u9488\uff0c\u6d4b\u91cf3D\u8d44\u4ea7\u5728\u4e0d\u540c\u65b9\u9762\u7684\u4e00\u81f4\u6027\u3002", "result": "Eval3D\u63d0\u4f9b\u50cf\u7d20\u7ea7\u6d4b\u91cf\u3001\u51c6\u786e\u7684\u7a7a\u95f4\u53cd\u9988\uff0c\u5e76\u66f4\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u3002", "conclusion": "Eval3D\u63ed\u793a\u4e86\u5f53\u524d3D\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.18267", "pdf": "https://arxiv.org/pdf/2504.18267", "abs": "https://arxiv.org/abs/2504.18267", "authors": ["Prajwal Chauhan", "Salah Eddine Choutri", "Mohamed Ghattassi", "Nader Masmoudi", "Saif Eddin Jabari"], "title": "Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study", "categories": ["cs.LG", "cs.AI"], "comment": "26 pages, 15 figures, 6 tables, under review at Artificial\n  Intelligence for Transportation | Journal", "summary": "This paper investigates the limitations of neural operators in learning\nsolutions for a Hughes model, a first-order hyperbolic conservation law system\nfor crowd dynamics. The model couples a Fokker-Planck equation representing\npedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes\nmodel belongs to the class of nonlinear hyperbolic systems that often exhibit\ncomplex solution structures, including shocks and discontinuities. In this\nstudy, we assess the performance of three state-of-the-art neural operators\n(Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural\nOperator) in various challenging scenarios. Specifically, we consider (1)\ndiscontinuous and Gaussian initial conditions and (2) diverse boundary\nconditions, while also examining the impact of different numerical schemes.\n  Our results show that these neural operators perform well in easy scenarios\nwith fewer discontinuities in the initial condition, yet they struggle in\ncomplex scenarios with multiple initial discontinuities and dynamic boundary\nconditions, even when trained specifically on such complex samples. The\npredicted solutions often appear smoother, resulting in a reduction in total\nvariation and a loss of important physical features. This smoothing behavior is\nsimilar to issues discussed by Daganzo (1995), where models that introduce\nartificial diffusion were shown to miss essential features such as shock waves\nin hyperbolic systems. These results suggest that current neural operator\narchitectures may introduce unintended regularization effects that limit their\nability to capture transport dynamics governed by discontinuities. They also\nraise concerns about generalizing these methods to traffic applications where\nshock preservation is essential.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7b97\u5b50\u5728\u89e3\u51b3Hughes\u6a21\u578b\uff08\u4e00\u79cd\u7528\u4e8e\u4eba\u7fa4\u52a8\u529b\u5b66\u7684\u53cc\u66f2\u5b88\u6052\u5f8b\u7cfb\u7edf\uff09\u65f6\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5176\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u591a\u521d\u59cb\u95f4\u65ad\u548c\u52a8\u6001\u8fb9\u754c\u6761\u4ef6\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9884\u6d4b\u7ed3\u679c\u8fc7\u4e8e\u5e73\u6ed1\uff0c\u4e22\u5931\u4e86\u91cd\u8981\u7269\u7406\u7279\u5f81\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7b97\u5b50\u5728\u5904\u7406\u975e\u7ebf\u6027\u53cc\u66f2\u7cfb\u7edf\uff08\u5982Hughes\u6a21\u578b\uff09\u65f6\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u95f4\u65ad\u548c\u590d\u6742\u8fb9\u754c\u6761\u4ef6\u7684\u573a\u666f\u4e2d\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u5148\u8fdb\u7684\u795e\u7ecf\u7b97\u5b50\uff08\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u3001\u5c0f\u6ce2\u795e\u7ecf\u7b97\u5b50\u548c\u591a\u5c0f\u6ce2\u795e\u7ecf\u7b97\u5b50\uff09\u5728\u4e0d\u540c\u6311\u6218\u6027\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u5305\u62ec\u95f4\u65ad\u548c\u9ad8\u65af\u521d\u59cb\u6761\u4ef6\u4ee5\u53ca\u591a\u6837\u8fb9\u754c\u6761\u4ef6\u3002", "result": "\u795e\u7ecf\u7b97\u5b50\u5728\u7b80\u5355\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u9884\u6d4b\u7ed3\u679c\u8fc7\u4e8e\u5e73\u6ed1\uff0c\u4e22\u5931\u4e86\u95f4\u65ad\u7279\u5f81\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u5de5\u6269\u6563\u6a21\u578b\u7684\u95ee\u9898\u3002", "conclusion": "\u5f53\u524d\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\u53ef\u80fd\u5f15\u5165\u4e0d\u5fc5\u8981\u7684\u6b63\u5219\u5316\u6548\u5e94\uff0c\u9650\u5236\u4e86\u5176\u6355\u6349\u95f4\u65ad\u4e3b\u5bfc\u7684\u8f93\u8fd0\u52a8\u6001\u7684\u80fd\u529b\uff0c\u5bf9\u4ea4\u901a\u5e94\u7528\u4e2d\u7684\u51b2\u51fb\u6ce2\u4fdd\u6301\u63d0\u51fa\u4e86\u6311\u6218\u3002"}}
{"id": "2504.18510", "pdf": "https://arxiv.org/pdf/2504.18510", "abs": "https://arxiv.org/abs/2504.18510", "authors": ["Patrick M\u00fcller", "Alexander Braun", "Margret Keuper"], "title": "Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models", "categories": ["cs.CV"], "comment": "v1.0", "summary": "Deep neural networks (DNNs) have proven to be successful in various computer\nvision applications such that models even infer in safety-critical situations.\nTherefore, vision models have to behave in a robust way to disturbances such as\nnoise or blur. While seminal benchmarks exist to evaluate model robustness to\ndiverse corruptions, blur is often approximated in an overly simplistic way to\nmodel defocus, while ignoring the different blur kernel shapes that result from\noptical systems. To study model robustness against realistic optical blur\neffects, this paper proposes two datasets of blur corruptions, which we denote\nOpticsBench and LensCorruptions. OpticsBench examines primary aberrations such\nas coma, defocus, and astigmatism, i.e. aberrations that can be represented by\nvarying a single parameter of Zernike polynomials. To go beyond the principled\nbut synthetic setting of primary aberrations, LensCorruptions samples linear\ncombinations in the vector space spanned by Zernike polynomials, corresponding\nto 100 real lenses. Evaluations for image classification and object detection\non ImageNet and MSCOCO show that for a variety of different pre-trained models,\nthe performance on OpticsBench and LensCorruptions varies significantly,\nindicating the need to consider realistic image corruptions to evaluate a\nmodel's robustness against blur.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\uff08OpticsBench\u548cLensCorruptions\uff09\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9\u771f\u5b9e\u5149\u5b66\u6a21\u7cca\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u6a21\u7cca\u9c81\u68d2\u6027\u8bc4\u4f30\u8fc7\u4e8e\u7b80\u5316\uff0c\u5ffd\u7565\u4e86\u5149\u5b66\u7cfb\u7edf\u7684\u590d\u6742\u6a21\u7cca\u6548\u5e94\uff0c\u9700\u66f4\u771f\u5b9e\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7Zernike\u591a\u9879\u5f0f\u751f\u6210\u4e24\u79cd\u6570\u636e\u96c6\uff1aOpticsBench\uff08\u5355\u4e00\u53c2\u6570\u6a21\u62df\u4e3b\u50cf\u5dee\uff09\u548cLensCorruptions\uff08\u6a21\u62df100\u79cd\u771f\u5b9e\u955c\u5934\u6a21\u7cca\uff09\u3002", "result": "\u5728ImageNet\u548cMSCOCO\u4e0a\u6d4b\u8bd5\u591a\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u9700\u5f15\u5165\u771f\u5b9e\u6a21\u7cca\u6570\u636e\u96c6\u4ee5\u66f4\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.18521", "pdf": "https://arxiv.org/pdf/2504.18521", "abs": "https://arxiv.org/abs/2504.18521", "authors": ["Shintaro Shiba", "Quan Kong", "Norimasa Kobori"], "title": "E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization", "categories": ["cs.CV", "cs.RO", "eess.SP"], "comment": "10 pages, 9 figures, 5 tables, CVPRW on EventVision 2025", "summary": "Optical communication using modulated LEDs (e.g., visible light\ncommunication) is an emerging application for event cameras, thanks to their\nhigh spatio-temporal resolutions. Event cameras can be used simply to decode\nthe LED signals and also to localize the camera relative to the LED marker\npositions. However, there is no public dataset to benchmark the decoding and\nlocalization in various real-world settings. We present, to the best of our\nknowledge, the first public dataset that consists of an event camera, a frame\ncamera, and ground-truth poses that are precisely synchronized with hardware\ntriggers. It provides various camera motions with various sensitivities in\ndifferent scene brightness settings, both indoor and outdoor. Furthermore, we\npropose a novel method of localization that leverages the Contrast Maximization\nframework for motion estimation and compensation. The detailed analysis and\nexperimental results demonstrate the advantages of LED-based localization with\nevents over the conventional AR-marker--based one with frames, as well as the\nefficacy of the proposed method in localization. We hope that the proposed\ndataset serves as a future benchmark for both motion-related classical computer\nvision tasks and LED marker decoding tasks simultaneously, paving the way to\nbroadening applications of event cameras on mobile devices.\nhttps://woven-visionai.github.io/evlc-dataset", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e8b\u4ef6\u76f8\u673a\u5728LED\u4fe1\u53f7\u89e3\u7801\u548c\u5b9a\u4f4d\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u7684\u65b0\u578b\u5b9a\u4f4d\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u4e8b\u4ef6\u76f8\u673a\u5728LED\u4fe1\u53f7\u89e3\u7801\u548c\u5b9a\u4f4d\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u6846\u67b6\u7684\u8fd0\u52a8\u4f30\u8ba1\u548c\u8865\u507f\u65b9\u6cd5\uff0c\u7528\u4e8eLED\u6807\u8bb0\u7684\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u4e8b\u4ef6\u7684LED\u5b9a\u4f4d\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u5e27\u7684AR\u6807\u8bb0\u5b9a\u4f4d\uff0c\u4e14\u65b0\u65b9\u6cd5\u5728\u5b9a\u4f4d\u4e2d\u8868\u73b0\u9ad8\u6548\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u6709\u671b\u6210\u4e3a\u672a\u6765\u8fd0\u52a8\u76f8\u5173\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u548cLED\u6807\u8bb0\u89e3\u7801\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u63a8\u52a8\u4e8b\u4ef6\u76f8\u673a\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2504.18524", "pdf": "https://arxiv.org/pdf/2504.18524", "abs": "https://arxiv.org/abs/2504.18524", "authors": ["Fengjia Zhang", "Samrudhdhi B. Rangrej", "Tristan Aumentado-Armstrong", "Afsaneh Fazly", "Alex Levinshtein"], "title": "Augmenting Perceptual Super-Resolution via Image Quality Predictors", "categories": ["cs.CV"], "comment": null, "summary": "Super-resolution (SR), a classical inverse problem in computer vision, is\ninherently ill-posed, inducing a distribution of plausible solutions for every\ninput. However, the desired result is not simply the expectation of this\ndistribution, which is the blurry image obtained by minimizing pixelwise error,\nbut rather the sample with the highest image quality. A variety of techniques,\nfrom perceptual metrics to adversarial losses, are employed to this end. In\nthis work, we explore an alternative: utilizing powerful non-reference image\nquality assessment (NR-IQA) models in the SR context. We begin with a\ncomprehensive analysis of NR-IQA metrics on human-derived SR data, identifying\nboth the accuracy (human alignment) and complementarity of different metrics.\nThen, we explore two methods of applying NR-IQA models to SR learning: (i)\naltering data sampling, by building on an existing multi-ground-truth SR\nframework, and (ii) directly optimizing a differentiable quality score. Our\nresults demonstrate a more human-centric perception-distortion tradeoff,\nfocusing less on non-perceptual pixel-wise distortion, instead improving the\nbalance between perceptual fidelity and human-tuned NR-IQA measures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u4efb\u52a1\u4e2d\u5229\u7528\u975e\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08NR-IQA\uff09\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u56fe\u50cf\u8d28\u91cf\u800c\u975e\u7b80\u5355\u7684\u50cf\u7d20\u7ea7\u8bef\u5dee\u6700\u5c0f\u5316\u3002", "motivation": "\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u5b58\u5728\u591a\u89e3\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u503e\u5411\u4e8e\u751f\u6210\u6a21\u7cca\u56fe\u50cf\uff0c\u800c\u5b9e\u9645\u9700\u6c42\u662f\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7NR-IQA\u6a21\u578b\u5b9e\u73b0\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u8d28\u91cf\u4f18\u5316\u3002", "method": "\u5206\u6790\u4e86NR-IQA\u6307\u6807\u5728\u4eba\u7c7b\u751f\u6210SR\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u4e86\u4e24\u79cd\u5e94\u7528\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u591a\u771f\u5b9e\u503c\u6846\u67b6\u6539\u53d8\u6570\u636e\u91c7\u6837\uff1b2\uff09\u76f4\u63a5\u4f18\u5316\u53ef\u5fae\u5206\u8d28\u91cf\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u5931\u771f\u6743\u8861\u4e0a\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\uff0c\u51cf\u5c11\u4e86\u975e\u611f\u77e5\u50cf\u7d20\u7ea7\u5931\u771f\u7684\u5f71\u54cd\u3002", "conclusion": "NR-IQA\u6a21\u578b\u5728SR\u4efb\u52a1\u4e2d\u80fd\u591f\u6709\u6548\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u5b9e\u73b0\u66f4\u4eba\u7c7b\u4e2d\u5fc3\u7684\u611f\u77e5\u4f18\u5316\u3002"}}
{"id": "2504.18310", "pdf": "https://arxiv.org/pdf/2504.18310", "abs": "https://arxiv.org/abs/2504.18310", "authors": ["Prashant Garg", "Thiemo Fetzer"], "title": "Artificial Intelligence health advice accuracy varies across languages and contexts", "categories": ["econ.GN", "cs.AI", "cs.CY", "cs.HC", "cs.LG", "q-fin.EC"], "comment": "10 pages, 2 figures. All data, code and materials used is freely\n  available in the Zenodo (DOI: 10.5281/zenodo.15281282)", "summary": "Using basic health statements authorized by UK and EU registers and 9,100\njournalist-vetted public-health assertions on topics such as abortion, COVID-19\nand politics from sources ranging from peer-reviewed journals and government\nadvisories to social media and news across the political spectrum, we benchmark\nsix leading large language models from in 21 languages, finding that, despite\nhigh accuracy on English-centric textbook claims, performance falls in multiple\nnon-European languages and fluctuates by topic and source, highlighting the\nurgency of comprehensive multilingual, domain-aware validation before deploying\nAI in global health communication.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u516d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u572821\u79cd\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5728\u82f1\u8bed\u6559\u79d1\u4e66\u7c7b\u95ee\u9898\u4e0a\u51c6\u786e\u6027\u9ad8\uff0c\u4f46\u5728\u975e\u6b27\u6d32\u8bed\u8a00\u548c\u4e0d\u540c\u4e3b\u9898\u53ca\u6765\u6e90\u4e0a\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u5f3a\u8c03\u4e86\u5168\u7403\u5065\u5eb7\u4f20\u64ad\u4e2d\u591a\u8bed\u8a00\u548c\u9886\u57df\u611f\u77e5\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u5065\u5eb7\u4f20\u64ad\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u591a\u6837\u5316\u4e3b\u9898\u80cc\u666f\u4e0b\u7684\u51c6\u786e\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u6765\u81ea\u82f1\u56fd\u548c\u6b27\u76df\u6ce8\u518c\u7684\u57fa\u672c\u5065\u5eb7\u58f0\u660e\uff0c\u4ee5\u53ca9,100\u6761\u7ecf\u8fc7\u8bb0\u8005\u5ba1\u67e5\u7684\u516c\u5171\u536b\u751f\u65ad\u8a00\uff0c\u6db5\u76d6\u5815\u80ce\u3001COVID-19\u548c\u653f\u6cbb\u7b49\u4e3b\u9898\uff0c\u6765\u6e90\u4ece\u540c\u884c\u8bc4\u5ba1\u671f\u520a\u5230\u793e\u4ea4\u5a92\u4f53\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u82f1\u8bed\u6559\u79d1\u4e66\u7c7b\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u975e\u6b27\u6d32\u8bed\u8a00\u548c\u67d0\u4e9b\u4e3b\u9898\u53ca\u6765\u6e90\u4e0a\u51c6\u786e\u6027\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u5728\u5168\u7403\u5065\u5eb7\u4f20\u64ad\u4e2d\u90e8\u7f72AI\u524d\uff0c\u9700\u8fdb\u884c\u5168\u9762\u7684\u591a\u8bed\u8a00\u548c\u9886\u57df\u611f\u77e5\u9a8c\u8bc1\u3002"}}
{"id": "2504.17819", "pdf": "https://arxiv.org/pdf/2504.17819", "abs": "https://arxiv.org/abs/2504.17819", "authors": ["Mohaddeseh Chegini", "Ali Mahloojifar"], "title": "A Deep Bayesian Convolutional Spiking Neural Network-based CAD system with Uncertainty Quantification for Medical Images Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "The Computer_Aided Diagnosis (CAD) systems facilitate accurate diagnosis of\ndiseases. The development of CADs by leveraging third generation neural\nnetwork, namely, Spiking Neural Network (SNN), is essential to utilize of the\nbenefits of SNNs, such as their event_driven processing, parallelism, low power\nconsumption, and the ability to process sparse temporal_spatial information.\nHowever, Deep SNN as a deep learning model faces challenges with unreliability.\nTo deal with unreliability challenges due to inability to quantify the\nuncertainty of the predictions, we proposed a deep Bayesian Convolutional\nSpiking Neural Network based_CADs with uncertainty_aware module. In this study,\nthe Monte Carlo Dropout method as Bayesian approximation is used as an\nuncertainty quantification method. This method was applied to several medical\nimage classification tasks. Our experimental results demonstrate that our\nproposed model is accurate and reliable and will be a proper alternative to\nconventional deep learning for medical image classification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u8d1d\u53f6\u65af\u5377\u79ef\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684CAD\u7cfb\u7edf\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1bDropout\u65b9\u6cd5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6SNN\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u5b58\u5728\u4e0d\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u65e0\u6cd5\u91cf\u5316\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u8d1d\u53f6\u65af\u5377\u79ef\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1bDropout\u65b9\u6cd5\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u624b\u6bb5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51c6\u786e\u4e14\u53ef\u9760\u3002", "conclusion": "\u8be5\u6a21\u578b\u662f\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u3002"}}
{"id": "2504.18316", "pdf": "https://arxiv.org/pdf/2504.18316", "abs": "https://arxiv.org/abs/2504.18316", "authors": ["Yacine Majdoub", "Eya Ben Charrada", "Haifa Touati"], "title": "Towards Adaptive Software Agents for Debugging", "categories": ["cs.SE", "cs.AI"], "comment": "5 pages, 3 figures, FSE2025", "summary": "Using multiple agents was found to improve the debugging capabilities of\nLarge Language Models. However, increasing the number of LLM-agents has several\ndrawbacks such as increasing the running costs and rising the risk for the\nagents to lose focus. In this work, we propose an adaptive agentic design,\nwhere the number of agents and their roles are determined dynamically based on\nthe characteristics of the task to be achieved. In this design, the agents\nroles are not predefined, but are generated after analyzing the problem to be\nsolved. Our initial evaluation shows that, with the adaptive design, the number\nof agents that are generated depends on the complexity of the buggy code. In\nfact, for simple code with mere syntax issues, the problem was usually fixed\nusing one agent only. However, for more complex problems, we noticed the\ncreation of a higher number of agents. Regarding the effectiveness of the fix,\nwe noticed an average improvement of 11% compared to the one-shot prompting.\nGiven these promising results, we outline future research directions to improve\nour design for adaptive software agents that can autonomously plan and conduct\ntheir software goals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u52a8\u6001\u8c03\u6574\u667a\u80fd\u4f53\u6570\u91cf\u548c\u89d2\u8272\u4ee5\u4f18\u5316\u8c03\u8bd5\u6548\u679c\uff0c\u76f8\u6bd4\u5355\u6b21\u63d0\u793a\u5e73\u5747\u63d0\u534711%\u7684\u4fee\u590d\u6548\u679c\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u867d\u80fd\u63d0\u5347LLM\u8c03\u8bd5\u80fd\u529b\uff0c\u4f46\u56fa\u5b9a\u6570\u91cf\u4f1a\u589e\u52a0\u6210\u672c\u548c\u5206\u6563\u6ce8\u610f\u529b\uff0c\u9700\u52a8\u6001\u8c03\u6574\u4ee5\u9002\u5e94\u4efb\u52a1\u7279\u6027\u3002", "method": "\u8bbe\u8ba1\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6839\u636e\u4efb\u52a1\u7279\u6027\u52a8\u6001\u751f\u6210\u667a\u80fd\u4f53\u6570\u91cf\u548c\u89d2\u8272\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u3002", "result": "\u667a\u80fd\u4f53\u6570\u91cf\u968f\u4ee3\u7801\u590d\u6742\u5ea6\u53d8\u5316\uff0c\u7b80\u5355\u95ee\u9898\u4ec5\u9700\u4e00\u4e2a\u667a\u80fd\u4f53\uff0c\u590d\u6742\u95ee\u9898\u751f\u6210\u66f4\u591a\uff1b\u4fee\u590d\u6548\u679c\u5e73\u5747\u63d0\u534711%\u3002", "conclusion": "\u81ea\u9002\u5e94\u8bbe\u8ba1\u6548\u679c\u663e\u8457\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5b9e\u73b0\u667a\u80fd\u4f53\u81ea\u4e3b\u89c4\u5212\u548c\u6267\u884c\u8f6f\u4ef6\u76ee\u6807\u3002"}}
{"id": "2504.17865", "pdf": "https://arxiv.org/pdf/2504.17865", "abs": "https://arxiv.org/abs/2504.17865", "authors": ["Charles J. Carver", "Hadleigh Schwartz", "Toma Itagaki", "Zachary Englhardt", "Kechen Liu", "Megan Graciela Nauli Manik", "Chun-Cheng Chang", "Vikram Iyer", "Brian Plancher", "Xia Zhou"], "title": "Set Phasers to Stun: Beaming Power and Control to Mobile Robots with Laser Light", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 7 figures, submitted to IROS 2025", "summary": "We present Phaser, a flexible system that directs narrow-beam laser light to\nmoving robots for concurrent wireless power delivery and communication. We\ndesign a semi-automatic calibration procedure to enable fusion of\nstereo-vision-based 3D robot tracking with high-power beam steering, and a\nlow-power optical communication scheme that reuses the laser light as a data\nchannel. We fabricate a Phaser prototype using off-the-shelf hardware and\nevaluate its performance with battery-free autonomous robots. Phaser delivers\noptical power densities of over 110 mW/cm$^2$ and error-free data to mobile\nrobots at multi-meter ranges, with on-board decoding drawing 0.3 mA (97\\% less\ncurrent than Bluetooth Low Energy). We demonstrate Phaser fully powering\ngram-scale battery-free robots to nearly 2x higher speeds than prior work while\nsimultaneously controlling them to navigate around obstacles and along paths.\nCode, an open-source design guide, and a demonstration video of Phaser is\navailable at https://mobilex.cs.columbia.edu/phaser.", "AI": {"tldr": "Phaser\u662f\u4e00\u4e2a\u5229\u7528\u7a84\u5149\u675f\u6fc0\u5149\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u65e0\u7ebf\u4f9b\u7535\u548c\u901a\u4fe1\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u8ddf\u8e2a\u548c\u5149\u675f\u63a7\u5236\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u80fd\u91cf\u4f20\u8f93\u548c\u6570\u636e\u901a\u4fe1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u65e0\u7ebf\u4f9b\u7535\u548c\u901a\u4fe1\u7684\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u80fd\u91cf\u4f20\u8f93\u6548\u7387\u548c\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u534a\u81ea\u52a8\u6821\u51c6\u7a0b\u5e8f\uff0c\u7ed3\u5408\u7acb\u4f53\u89c6\u89c93D\u8ddf\u8e2a\u548c\u9ad8\u529f\u7387\u5149\u675f\u63a7\u5236\uff0c\u5e76\u5229\u7528\u6fc0\u5149\u4f5c\u4e3a\u6570\u636e\u901a\u9053\u8fdb\u884c\u4f4e\u529f\u8017\u901a\u4fe1\u3002", "result": "\u539f\u578b\u673a\u5b9e\u73b0\u4e86\u8d85\u8fc7110 mW/cm\u00b2\u7684\u5149\u529f\u7387\u5bc6\u5ea6\u548c\u591a\u7c73\u8303\u56f4\u5185\u7684\u65e0\u5dee\u9519\u6570\u636e\u4f20\u8f93\uff0c\u529f\u8017\u6bd4\u84dd\u7259\u4f4e97%\u3002", "conclusion": "Phaser\u6210\u529f\u4e3a\u65e0\u7535\u6c60\u673a\u5668\u4eba\u63d0\u4f9b\u9ad8\u6548\u4f9b\u7535\u548c\u901a\u4fe1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u6027\u80fd\u548c\u529f\u80fd\u3002"}}
{"id": "2504.18329", "pdf": "https://arxiv.org/pdf/2504.18329", "abs": "https://arxiv.org/abs/2504.18329", "authors": ["Anh-Duy Pham", "Olivier Basole Kashongwe", "Martin Atzmueller", "Tim R\u00f6mer"], "title": "PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Balancing performance and interpretability in multivariate time series\nclassification is a significant challenge due to data complexity and high\ndimensionality. This paper introduces PHeatPruner, a method integrating\npersistent homology and sheaf theory to address these challenges. Persistent\nhomology facilitates the pruning of up to 45% of the applied variables while\nmaintaining or enhancing the accuracy of models such as Random Forest,\nCatBoost, XGBoost, and LightGBM, all without depending on posterior\nprobabilities or supervised optimization algorithms. Concurrently, sheaf theory\ncontributes explanatory vectors that provide deeper insights into the data's\nstructural nuances. The approach was validated using the UEA Archive and a\nmastitis detection dataset for dairy cows. The results demonstrate that\nPHeatPruner effectively preserves model accuracy. Furthermore, our results\nhighlight PHeatPruner's key features, i.e. simplifying complex data and\noffering actionable insights without increasing processing time or complexity.\nThis method bridges the gap between complexity reduction and interpretability,\nsuggesting promising applications in various fields.", "AI": {"tldr": "PHeatPruner\u7ed3\u5408\u6301\u4e45\u540c\u8c03\u548c\u5c42\u7406\u8bba\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u53d8\u91cf\u6570\u91cf\uff0c\u5e76\u63d0\u4f9b\u6570\u636e\u7ed3\u6784\u7684\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u5e73\u8861\u7684\u6311\u6218\u3002", "method": "\u96c6\u6210\u6301\u4e45\u540c\u8c03\uff08\u51cf\u5c11\u53d8\u91cf\uff09\u548c\u5c42\u7406\u8bba\uff08\u63d0\u4f9b\u89e3\u91ca\u6027\uff09\uff0c\u65e0\u9700\u540e\u9a8c\u6982\u7387\u6216\u76d1\u7763\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728UEA Archive\u548c\u5976\u725b\u4e73\u817a\u708e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u51cf\u5c1145%\u53d8\u91cf\u4e14\u4fdd\u6301\u7cbe\u5ea6\u3002", "conclusion": "PHeatPruner\u5728\u7b80\u5316\u6570\u636e\u4e0e\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17898", "pdf": "https://arxiv.org/pdf/2504.17898", "abs": "https://arxiv.org/abs/2504.17898", "authors": ["David Wang", "Derek Goh", "Jiale Zhang"], "title": "Material Identification Via RFID For Smart Shopping", "categories": ["eess.SP", "cs.CV", "J.0; J.7; B.0"], "comment": "5 pages, 7 figures", "summary": "Cashierless stores rely on computer vision and RFID tags to associate\nshoppers with items, but concealed items placed in backpacks, pockets, or bags\ncreate challenges for theft prevention. We introduce a system that turns\nexisting RFID tagged items into material sensors by exploiting how different\ncontainers attenuate and scatter RF signals. Using RSSI and phase angle, we\ntrained a neural network to classify seven common containers. In a simulated\nretail environment, the model achieves 89% accuracy with one second samples and\n74% accuracy from single reads. Incorporating distance measurements, our system\nachieves 82% accuracy across 0.3-2m tag to reader separations. When deployed at\naisle or doorway choke points, the system can flag suspicious events in real\ntime, prompting camera screening or staff intervention. By combining material\nidentification with computer vision tracking, our system provides proactive\nloss prevention for cashierless retail while utilizing existing infrastructure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528RFID\u6807\u7b7e\u4fe1\u53f7\u8870\u51cf\u548c\u6563\u5c04\u7279\u6027\u68c0\u6d4b\u9690\u85cf\u7269\u54c1\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u548c\u8ddd\u79bb\u6d4b\u91cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u9632\u76d7\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u5546\u5e97\u4e2d\u9690\u85cf\u7269\u54c1\uff08\u5982\u80cc\u5305\u3001\u53e3\u888b\u4e2d\u7684\u5546\u54c1\uff09\u5bfc\u81f4\u7684\u76d7\u7a83\u95ee\u9898\u3002", "method": "\u5229\u7528RFID\u6807\u7b7e\u7684RSSI\u548c\u76f8\u4f4d\u89d2\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u5206\u7c7b\u4e03\u79cd\u5e38\u89c1\u5bb9\u5668\uff0c\u5e76\u7ed3\u5408\u8ddd\u79bb\u6d4b\u91cf\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5728\u6a21\u62df\u96f6\u552e\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u5bf9\u4e00\u79d2\u6837\u672c\u7684\u5206\u7c7b\u51c6\u786e\u7387\u8fbe89%\uff0c\u5355\u6b21\u8bfb\u53d6\u4e3a74%\uff1b\u7ed3\u5408\u8ddd\u79bb\u6d4b\u91cf\u540e\uff0c0.3-2\u7c73\u8303\u56f4\u5185\u51c6\u786e\u7387\u4e3a82%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408RFID\u4fe1\u53f7\u5206\u6790\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9690\u85cf\u7269\u54c1\u7684\u5b9e\u65f6\u68c0\u6d4b\uff0c\u63d0\u5347\u4e86\u65e0\u4eba\u5546\u5e97\u7684\u9632\u76d7\u80fd\u529b\u3002"}}
{"id": "2504.18346", "pdf": "https://arxiv.org/pdf/2504.18346", "abs": "https://arxiv.org/abs/2504.18346", "authors": ["Toghrul Abbasli", "Kentaroh Toyoda", "Yuan Wang", "Leon Witt", "Muhammad Asif Ali", "Yukai Miao", "Dan Li", "Qingsong Wei"], "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u548c\u6821\u51c6\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u6587\u732e\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "LLMs\u7684\u5e7b\u89c9\u95ee\u9898\uff08\u8f93\u51fa\u9519\u8bef\u4fe1\u606f\uff09\u662f\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\uff0c\u4f46\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u5bf9\u5176UQ\u548c\u6821\u51c6\u65b9\u6cd5\u7684\u6df1\u5165\u5206\u6790\u548c\u7efc\u5408\u6bd4\u8f83\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u4ee3\u8868\u6027\u6587\u732e\uff0c\u5f15\u5165\u4e00\u4e2a\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528\u4e24\u4e2a\u53ef\u9760\u6027\u6570\u636e\u96c6\u5bf9\u516d\u79cd\u76f8\u5173\u65b9\u6cd5\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u7efc\u8ff0\u7684\u91cd\u8981\u53d1\u73b0\uff0c\u5e76\u4e3aLLMs\u7684UQ\u548c\u6821\u51c6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86LLMs\u9886\u57dfUQ\u548c\u6821\u51c6\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2504.17943", "pdf": "https://arxiv.org/pdf/2504.17943", "abs": "https://arxiv.org/abs/2504.17943", "authors": ["Mingsi Liao", "Gota Morota", "Ye Bi", "Rebecca R. Cockrum"], "title": "Predicting Dairy Calf Body Weight from Depth Images Using Deep Learning (YOLOv8) and Threshold Segmentation with Cross-Validation and Longitudinal Analysis", "categories": ["eess.IV", "cs.CV"], "comment": "Published on Animals, 18 March 2025", "summary": "Monitoring calf body weight (BW) before weaning is essential for assessing\ngrowth, feed efficiency, health, and weaning readiness. However, labor, time,\nand facility constraints limit BW collection. Additionally, Holstein calf coat\npatterns complicate image-based BW estimation, and few studies have explored\nnon-contact measurements taken at early time points for predicting later BW.\nThe objectives of this study were to (1) develop deep learning-based\nsegmentation models for extracting calf body metrics, (2) compare deep learning\nsegmentation with threshold-based methods, and (3) evaluate BW prediction using\nsingle-time-point cross-validation with linear regression (LR) and extreme\ngradient boosting (XGBoost) and multiple-time-point cross-validation with LR,\nXGBoost, and a linear mixed model (LMM). Depth images from Holstein (n = 63)\nand Jersey (n = 5) pre-weaning calves were collected, with 20 Holstein calves\nbeing weighed manually. Results showed that You Only Look Once version 8\n(YOLOv8) deep learning segmentation (intersection over union = 0.98)\noutperformed threshold-based methods (0.89). In single-time-point\ncross-validation, XGBoost achieved the best BW prediction (R^2 = 0.91, mean\nabsolute percentage error (MAPE) = 4.37%), while LMM provided the most accurate\nlongitudinal BW prediction (R^2 = 0.99, MAPE = 2.39%). These findings highlight\nthe potential of deep learning for automated BW prediction, enhancing farm\nmanagement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u725b\u728a\u4f53\u91cd\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u50cf\u5206\u5272\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u4f53\u91cd\u9884\u6d4b\uff0c\u4e3a\u519c\u573a\u7ba1\u7406\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u725b\u728a\u65ad\u5976\u524d\u7684\u4f53\u91cd\u76d1\u6d4b\u5bf9\u8bc4\u4f30\u751f\u957f\u3001\u9972\u6599\u6548\u7387\u3001\u5065\u5eb7\u548c\u65ad\u5976\u51c6\u5907\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u4eba\u529b\u3001\u65f6\u95f4\u548c\u8bbe\u65bd\u3002\u6b64\u5916\uff0c\u8377\u65af\u5766\u725b\u728a\u7684\u6bdb\u8272\u56fe\u6848\u589e\u52a0\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u4f53\u91cd\u4f30\u8ba1\u96be\u5ea6\uff0c\u4e14\u5c11\u6709\u7814\u7a76\u63a2\u7d22\u65e9\u671f\u975e\u63a5\u89e6\u6d4b\u91cf\u5bf9\u540e\u671f\u4f53\u91cd\u7684\u9884\u6d4b\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\uff08YOLOv8\uff09\uff0c\u5e76\u4e0e\u9608\u503c\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff1b\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\uff08LR\uff09\u3001\u6781\u7aef\u68af\u5ea6\u63d0\u5347\uff08XGBoost\uff09\u548c\u7ebf\u6027\u6df7\u5408\u6a21\u578b\uff08LMM\uff09\u8fdb\u884c\u5355\u70b9\u548c\u591a\u70b9\u4ea4\u53c9\u9a8c\u8bc1\u3002", "result": "YOLOv8\u5206\u5272\u6548\u679c\u4f18\u4e8e\u9608\u503c\u65b9\u6cd5\uff08IoU=0.98 vs. 0.89\uff09\u3002XGBoost\u5728\u5355\u70b9\u9a8c\u8bc1\u4e2d\u8868\u73b0\u6700\u4f73\uff08R\u00b2=0.91\uff0cMAPE=4.37%\uff09\uff0cLMM\u5728\u7eb5\u5411\u9884\u6d4b\u4e2d\u6700\u51c6\u786e\uff08R\u00b2=0.99\uff0cMAPE=2.39%\uff09\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u725b\u728a\u4f53\u91cd\u81ea\u52a8\u5316\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u663e\u8457\u63d0\u5347\u519c\u573a\u7ba1\u7406\u6548\u7387\u3002"}}
{"id": "2504.17945", "pdf": "https://arxiv.org/pdf/2504.17945", "abs": "https://arxiv.org/abs/2504.17945", "authors": ["Bastien C. Baluyot", "Marta Varela", "Chen Qin"], "title": "Spectral Bias Correction in PINNs for Myocardial Image Registration of Pathological Data", "categories": ["eess.IV", "cs.CV"], "comment": "6 pages, 3 figures, 3 tables", "summary": "Accurate myocardial image registration is essential for cardiac strain\nanalysis and disease diagnosis. However, spectral bias in neural networks\nimpedes modeling high-frequency deformations, producing inaccurate,\nbiomechanically implausible results, particularly in pathological data. This\npaper addresses spectral bias in physics-informed neural networks (PINNs) by\nintegrating Fourier Feature mappings and introducing modulation strategies into\na PINN framework. Experiments on two distinct datasets demonstrate that the\nproposed methods enhance the PINN's ability to capture complex, high-frequency\ndeformations in cardiomyopathies, achieving superior registration accuracy\nwhile maintaining biomechanical plausibility - thus providing a foundation for\nscalable cardiac image registration and generalization across multiple patients\nand pathologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u7279\u5f81\u6620\u5c04\u548c\u8c03\u5236\u7b56\u7565\u89e3\u51b3\u8c31\u504f\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5fc3\u808c\u56fe\u50cf\u914d\u51c6\u7684\u51c6\u786e\u6027\u548c\u751f\u7269\u529b\u5b66\u5408\u7406\u6027\u3002", "motivation": "\u5fc3\u808c\u56fe\u50cf\u914d\u51c6\u5bf9\u5fc3\u810f\u5e94\u53d8\u5206\u6790\u548c\u75be\u75c5\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u795e\u7ecf\u7f51\u7edc\u7684\u8c31\u504f\u95ee\u9898\u5bfc\u81f4\u9ad8\u9891\u53d8\u5f62\u5efa\u6a21\u4e0d\u51c6\u786e\uff0c\u5c24\u5176\u5728\u75c5\u7406\u6570\u636e\u4e2d\u3002", "method": "\u5728PINN\u6846\u67b6\u4e2d\u96c6\u6210\u5085\u91cc\u53f6\u7279\u5f81\u6620\u5c04\u5e76\u5f15\u5165\u8c03\u5236\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u8c31\u504f\u95ee\u9898\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6355\u6349\u5fc3\u808c\u75c5\u4e2d\u7684\u9ad8\u9891\u53d8\u5f62\uff0c\u914d\u51c6\u51c6\u786e\u6027\u66f4\u9ad8\u4e14\u4fdd\u6301\u751f\u7269\u529b\u5b66\u5408\u7406\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u6269\u5c55\u7684\u5fc3\u810f\u56fe\u50cf\u914d\u51c6\u53ca\u8de8\u60a3\u8005\u548c\u75c5\u7406\u7684\u6cdb\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.18353", "pdf": "https://arxiv.org/pdf/2504.18353", "abs": "https://arxiv.org/abs/2504.18353", "authors": ["Roya Nasiri"], "title": "Testing Individual Fairness in Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages", "summary": "The biases in artificial intelligence (AI) models can lead to automated\ndecision-making processes that discriminate against groups and/or individuals\nbased on sensitive properties such as gender and race. While there are many\nstudies on diagnosing and mitigating biases in various AI models, there is\nlittle research on individual fairness in Graph Neural Networks (GNNs). Unlike\ntraditional models, which treat data features independently and overlook their\ninter-relationships, GNNs are designed to capture graph-based structure where\nnodes are interconnected. This relational approach enables GNNs to model\ncomplex dependencies, but it also means that biases can propagate through these\nconnections, complicating the detection and mitigation of individual fairness\nviolations. This PhD project aims to develop a testing framework to assess and\nensure individual fairness in GNNs. It first systematically reviews the\nliterature on individual fairness, categorizing existing approaches to define,\nmeasure, test, and mitigate model biases, creating a taxonomy of individual\nfairness. Next, the project will develop a framework for testing and ensuring\nfairness in GNNs by adapting and extending current fairness testing and\nmitigation techniques. The framework will be evaluated through industrial case\nstudies, focusing on graph-based large language models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u786e\u4fdd\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e2d\u7684\u4e2a\u4f53\u516c\u5e73\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u5173\u4e8eGNNs\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u7a7a\u767d\u3002", "motivation": "AI\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u53ef\u80fd\u5bfc\u81f4\u57fa\u4e8e\u6027\u522b\u3001\u79cd\u65cf\u7b49\u654f\u611f\u5c5e\u6027\u7684\u6b67\u89c6\u6027\u51b3\u7b56\u3002\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u7814\u7a76\u5173\u6ce8AI\u6a21\u578b\u7684\u504f\u89c1\u8bca\u65ad\u548c\u7f13\u89e3\uff0c\u4f46GNNs\u4e2d\u7684\u4e2a\u4f53\u516c\u5e73\u6027\u7814\u7a76\u8f83\u5c11\u3002GNNs\u7684\u56fe\u7ed3\u6784\u7279\u6027\u4f7f\u5f97\u504f\u89c1\u53ef\u80fd\u901a\u8fc7\u8282\u70b9\u8fde\u63a5\u4f20\u64ad\uff0c\u589e\u52a0\u4e86\u516c\u5e73\u6027\u95ee\u9898\u7684\u590d\u6742\u6027\u3002", "method": "1. \u7cfb\u7edf\u56de\u987e\u4e2a\u4f53\u516c\u5e73\u6027\u6587\u732e\uff0c\u5efa\u7acb\u5206\u7c7b\u6cd5\uff1b2. \u5f00\u53d1\u9002\u7528\u4e8eGNNs\u7684\u516c\u5e73\u6027\u6d4b\u8bd5\u6846\u67b6\uff0c\u5e76\u6269\u5c55\u73b0\u6709\u6280\u672f\uff1b3. \u901a\u8fc7\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\uff08\u5982\u56fe\u57fa\u5927\u8bed\u8a00\u6a21\u578b\uff09\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u9884\u671f\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u6d4b\u8bd5\u548c\u786e\u4fddGNNs\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u4e3aGNNs\u4e2d\u7684\u4e2a\u4f53\u516c\u5e73\u6027\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63a8\u52a8\u516c\u5e73AI\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.17954", "pdf": "https://arxiv.org/pdf/2504.17954", "abs": "https://arxiv.org/abs/2504.17954", "authors": ["Kaiyuan Tang", "Siyuan Yao", "Chaoli Wang"], "title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)", "summary": "In volume visualization, users can interactively explore the\nthree-dimensional data by specifying color and opacity mappings in the transfer\nfunction (TF) or adjusting lighting parameters, facilitating meaningful\ninterpretation of the underlying structure. However, rendering large-scale\nvolumes demands powerful GPUs and high-speed memory access for real-time\nperformance. While existing novel view synthesis (NVS) methods offer faster\nrendering speeds with lower hardware requirements, the visible parts of a\nreconstructed scene are fixed and constrained by preset TF settings,\nsignificantly limiting user exploration. This paper introduces inverse volume\nrendering via Gaussian splatting (iVR-GS), an innovative NVS method that\nreduces the rendering cost while enabling scene editing for interactive volume\nexploration. Specifically, we compose multiple iVR-GS models associated with\nbasic TFs covering disjoint visible parts to make the entire volumetric scene\nvisible. Each basic model contains a collection of 3D editable Gaussians, where\neach Gaussian is a 3D spatial point that supports real-time scene rendering and\nediting. We demonstrate the superior reconstruction quality and composability\nof iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on\nvarious volume datasets. The code is available at\nhttps://github.com/TouKaienn/iVR-GS.", "AI": {"tldr": "iVR-GS\u662f\u4e00\u79cd\u65b0\u578b\u7684\u9006\u4f53\u79ef\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u964d\u4f4e\u6e32\u67d3\u6210\u672c\uff0c\u540c\u65f6\u652f\u6301\u573a\u666f\u7f16\u8f91\uff0c\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u4f53\u79ef\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709NVS\u65b9\u6cd5\u5728\u6e32\u67d3\u901f\u5ea6\u548c\u786c\u4ef6\u9700\u6c42\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9884\u8bbe\u7684TF\u8bbe\u7f6e\u9650\u5236\u4e86\u7528\u6237\u5bf9\u573a\u666f\u7684\u63a2\u7d22\u3002iVR-GS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "iVR-GS\u901a\u8fc7\u7ec4\u5408\u591a\u4e2a\u4e0e\u57fa\u7840TF\u5173\u8054\u7684\u6a21\u578b\uff0c\u8986\u76d6\u573a\u666f\u7684\u4e0d\u540c\u53ef\u89c1\u90e8\u5206\uff0c\u6bcf\u4e2a\u6a21\u578b\u5305\u542b\u53ef\u7f16\u8f91\u76843D\u9ad8\u65af\u70b9\uff0c\u652f\u6301\u5b9e\u65f6\u6e32\u67d3\u548c\u7f16\u8f91\u3002", "result": "iVR-GS\u5728\u591a\u4e2a\u4f53\u79ef\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u4e8e\u5176\u4ed6NVS\u65b9\u6cd5\uff08\u5982Plenoxels\u3001CCNeRF\u548c3DGS\uff09\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u53ef\u7ec4\u5408\u6027\u3002", "conclusion": "iVR-GS\u4e3a\u4ea4\u4e92\u5f0f\u4f53\u79ef\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18015", "pdf": "https://arxiv.org/pdf/2504.18015", "abs": "https://arxiv.org/abs/2504.18015", "authors": ["Hanrui Wang", "Shuo Wang", "Chun-Shien Lu", "Isao Echizen"], "title": "Diffusion-Driven Universal Model Inversion Attack for Face Recognition", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": null, "summary": "Facial recognition technology poses significant privacy risks, as it relies\non biometric data that is inherently sensitive and immutable if compromised. To\nmitigate these concerns, face recognition systems convert raw images into\nembeddings, traditionally considered privacy-preserving. However, model\ninversion attacks pose a significant privacy threat by reconstructing these\nprivate facial images, making them a crucial tool for evaluating the privacy\nrisks of face recognition systems. Existing methods usually require training\nindividual generators for each target model, a computationally expensive\nprocess. In this paper, we propose DiffUMI, a training-free diffusion-driven\nuniversal model inversion attack for face recognition systems. DiffUMI is the\nfirst approach to apply a diffusion model for unconditional image generation in\nmodel inversion. Unlike other methods, DiffUMI is universal, eliminating the\nneed for training target-specific generators. It operates within a fixed\nframework and pretrained diffusion model while seamlessly adapting to diverse\ntarget identities and models. DiffUMI breaches privacy-preserving face\nrecognition systems with state-of-the-art success, demonstrating that an\nunconditional diffusion model, coupled with optimized adversarial search,\nenables efficient and high-fidelity facial reconstruction. Additionally, we\nintroduce a novel application of out-of-domain detection (OODD), marking the\nfirst use of model inversion to distinguish non-face inputs from face inputs\nbased solely on embeddings.", "AI": {"tldr": "DiffUMI\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u6a21\u578b\u53cd\u6f14\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u9ad8\u6548\u91cd\u6784\u9762\u90e8\u56fe\u50cf\uff0c\u63ed\u793a\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u7684\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u9762\u90e8\u8bc6\u522b\u6280\u672f\u4f9d\u8d56\u654f\u611f\u7684\u751f\u7269\u7279\u5f81\u6570\u636e\uff0c\u4f20\u7edf\u5d4c\u5165\u65b9\u6cd5\u88ab\u8ba4\u4e3a\u9690\u79c1\u4fdd\u62a4\uff0c\u4f46\u6a21\u578b\u53cd\u6f14\u653b\u51fb\u4ecd\u80fd\u91cd\u6784\u56fe\u50cf\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u4e3a\u76ee\u6807\u6a21\u578b\u5355\u72ec\u8bad\u7ec3\u751f\u6210\u5668\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faDiffUMI\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u65e0\u9700\u8bad\u7ec3\u76ee\u6807\u7279\u5b9a\u751f\u6210\u5668\uff0c\u901a\u8fc7\u4f18\u5316\u5bf9\u6297\u641c\u7d22\u5b9e\u73b0\u9ad8\u6548\u9ad8\u4fdd\u771f\u9762\u90e8\u91cd\u6784\u3002", "result": "DiffUMI\u5728\u9690\u79c1\u4fdd\u62a4\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u4e2d\u53d6\u5f97\u5148\u8fdb\u6210\u679c\uff0c\u5e76\u9996\u6b21\u5229\u7528\u6a21\u578b\u53cd\u6f14\u533a\u5206\u975e\u9762\u90e8\u8f93\u5165\u3002", "conclusion": "DiffUMI\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u6a21\u578b\u53cd\u6f14\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u8bc4\u4f30\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u9690\u79c1\u98ce\u9669\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.18376", "pdf": "https://arxiv.org/pdf/2504.18376", "abs": "https://arxiv.org/abs/2504.18376", "authors": ["Pablo Miralles-Gonz\u00e1lez", "Javier Huertas-Tato", "Alejandro Mart\u00edn", "David Camacho"], "title": "Pushing the boundary on Natural Language Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684NLI\u65b9\u6cd5\uff0c\u4f7f\u7528GRPO\u548cCoT\u5b66\u4e60\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524dNLI\u7cfb\u7edf\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u5b58\u5728\u6570\u636e\u96c6\u504f\u5dee\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528GRPO\u548cCoT\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408LoRA\u548cQLoRA\u53c2\u6570\u9ad8\u6548\u6280\u672f\u5fae\u8c03\u6a21\u578b\u3002", "result": "32B\u91cf\u5316\u6a21\u578b\u5728\u591a\u9879\u5bf9\u6297\u6027\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u5185\u5b58\u5360\u7528\u4ec522GB\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u9c81\u68d2NLI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2504.18053", "pdf": "https://arxiv.org/pdf/2504.18053", "abs": "https://arxiv.org/abs/2504.18053", "authors": ["Jianyu Liu", "Hangyu Guo", "Ranjie Duan", "Xingyuan Bu", "Yancheng He", "Shilong Li", "Hui Huang", "Jiaheng Liu", "Yucheng Wang", "Chenchen Jing", "Xingwei Qu", "Xiao Zhang", "Yingshui Tan", "Yanan Wu", "Jihao Gu", "Yangguang Li", "Jianke Zhu"], "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models", "categories": ["cs.CL", "cs.CV"], "comment": "[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM", "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDREAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u98ce\u9669\u89e3\u8026\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347MLLMs\u7684\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6548\u679c\u4f18\u4e8eGPT-4V\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u56e0\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u800c\u9762\u4e34\u72ec\u7279\u7684\u5b89\u5168\u6311\u6218\uff0c\u9700\u89e3\u51b3\u6f5c\u5728\u653b\u51fb\u548c\u590d\u6742\u98ce\u9669\u7ec4\u5408\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u98ce\u9669\u89e3\u8026\u5206\u6790\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\uff08RLAIF\uff09\uff0c\u63d0\u51faDREAM\u65b9\u6cd5\u3002", "result": "DREAM\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff08SIUO\u5f97\u5206\u63d0\u9ad816.17%\uff09\uff0c\u4e14\u4e0d\u5f71\u54cd\u6b63\u5e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "DREAM\u901a\u8fc7\u98ce\u9669\u89e3\u8026\u548c\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u589e\u5f3aMLLMs\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.18380", "pdf": "https://arxiv.org/pdf/2504.18380", "abs": "https://arxiv.org/abs/2504.18380", "authors": ["Steven H\u00e4sler", "Philipp Ackermann"], "title": "Spatial Reasoner: A 3D Inference Pipeline for XR Applications", "categories": ["cs.SE", "cs.AI", "cs.GR", "cs.HC", "spatial computing, extended reality, knowledge representation,\n  spatial reasoning"], "comment": "11 pages, preprint of ICVARS 2025 paper", "summary": "Modern extended reality XR systems provide rich analysis of image data and\nfusion of sensor input and demand AR/VR applications that can reason about 3D\nscenes in a semantic manner. We present a spatial reasoning framework that\nbridges geometric facts with symbolic predicates and relations to handle key\ntasks such as determining how 3D objects are arranged among each other ('on',\n'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box\nrepresentations, enhanced by a comprehensive set of spatial predicates, ranging\nfrom topology and connectivity to directionality and orientation, expressed in\na formalism related to natural language. The derived predicates form a spatial\nknowledge graph and, in combination with a pipeline-based inference model,\nenable spatial queries and dynamic rule evaluation. Implementations for client-\nand server-side processing demonstrate the framework's capability to\nefficiently translate geometric data into actionable knowledge, ensuring\nscalable and technology-independent spatial reasoning in complex 3D\nenvironments. The Spatial Reasoner framework is fostering the creation of\nspatial ontologies, and seamlessly integrates with and therefore enriches\nmachine learning, natural language processing, and rule systems in XR\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u51e0\u4f55\u4e8b\u5b9e\u4e0e\u7b26\u53f7\u8c13\u8bcd\u548c\u5173\u7cfb\u7ed3\u5408\uff0c\u7528\u4e8e\u5904\u74063D\u573a\u666f\u4e2d\u7684\u8bed\u4e49\u63a8\u7406\u4efb\u52a1\u3002", "motivation": "\u73b0\u4ee3XR\u7cfb\u7edf\u9700\u8981\u80fd\u591f\u4ee5\u8bed\u4e49\u65b9\u5f0f\u63a8\u74063D\u573a\u666f\u7684AR/VR\u5e94\u7528\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u7a7a\u95f4\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5b9a\u54113D\u8fb9\u754c\u6846\u8868\u793a\uff0c\u7ed3\u5408\u7a7a\u95f4\u8c13\u8bcd\uff08\u5982\u62d3\u6251\u3001\u65b9\u5411\u6027\u7b49\uff09\uff0c\u6784\u5efa\u7a7a\u95f4\u77e5\u8bc6\u56fe\uff0c\u5e76\u901a\u8fc7\u7ba1\u9053\u63a8\u7406\u6a21\u578b\u652f\u6301\u52a8\u6001\u89c4\u5219\u8bc4\u4f30\u3002", "result": "\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5730\u5c06\u51e0\u4f55\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u77e5\u8bc6\uff0c\u652f\u6301\u590d\u67423D\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u67e5\u8be2\u548c\u63a8\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aXR\u5e94\u7528\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6280\u672f\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e30\u5bcc\u4e86\u673a\u5668\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u89c4\u5219\u7cfb\u7edf\u3002"}}
{"id": "2504.18067", "pdf": "https://arxiv.org/pdf/2504.18067", "abs": "https://arxiv.org/abs/2504.18067", "authors": ["Chuyu Wang", "Huiting Deng", "Dong Liu"], "title": "Physics-Driven Neural Compensation For Electrical Impedance Tomography", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Electrical Impedance Tomography (EIT) provides a non-invasive, portable\nimaging modality with significant potential in medical and industrial\napplications. Despite its advantages, EIT encounters two primary challenges:\nthe ill-posed nature of its inverse problem and the spatially variable,\nlocation-dependent sensitivity distribution. Traditional model-based methods\nmitigate ill-posedness through regularization but overlook sensitivity\nvariability, while supervised deep learning approaches require extensive\ntraining data and lack generalization. Recent developments in neural fields\nhave introduced implicit regularization techniques for image reconstruction,\nbut these methods typically neglect the physical principles underlying EIT,\nthus limiting their effectiveness. In this study, we propose PhyNC\n(Physics-driven Neural Compensation), an unsupervised deep learning framework\nthat incorporates the physical principles of EIT. PhyNC addresses both the\nill-posed inverse problem and the sensitivity distribution by dynamically\nallocating neural representational capacity to regions with lower sensitivity,\nensuring accurate and balanced conductivity reconstructions. Extensive\nevaluations on both simulated and experimental data demonstrate that PhyNC\noutperforms existing methods in terms of detail preservation and artifact\nresistance, particularly in low-sensitivity regions. Our approach enhances the\nrobustness of EIT reconstructions and provides a flexible framework that can be\nadapted to other imaging modalities with similar challenges.", "AI": {"tldr": "PhyNC\u662f\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u539f\u7406\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3EIT\u4e2d\u7684\u9006\u95ee\u9898\u548c\u7075\u654f\u5ea6\u5206\u5e03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "EIT\u5728\u533b\u5b66\u548c\u5de5\u4e1a\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u5176\u9006\u95ee\u9898\u7684\u4e0d\u9002\u5b9a\u6027\u548c\u7075\u654f\u5ea6\u5206\u5e03\u7684\u7a7a\u95f4\u53d8\u5f02\u6027\u662f\u4e3b\u8981\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u672a\u80fd\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u7269\u7406\u539f\u7406\u7684\u652f\u6301\u3002", "method": "\u63d0\u51faPhyNC\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u795e\u7ecf\u8868\u793a\u80fd\u529b\u5230\u4f4e\u7075\u654f\u5ea6\u533a\u57df\uff0c\u7ed3\u5408EIT\u7684\u7269\u7406\u539f\u7406\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u6570\u636e\u4e0a\uff0cPhyNC\u5728\u7ec6\u8282\u4fdd\u7559\u548c\u6297\u4f2a\u5f71\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4f4e\u7075\u654f\u5ea6\u533a\u57df\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "PhyNC\u4e0d\u4ec5\u63d0\u5347\u4e86EIT\u91cd\u5efa\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u4e3a\u5176\u4ed6\u7c7b\u4f3c\u6210\u50cf\u6a21\u6001\u63d0\u4f9b\u4e86\u7075\u6d3b\u6846\u67b6\u3002"}}
{"id": "2504.18383", "pdf": "https://arxiv.org/pdf/2504.18383", "abs": "https://arxiv.org/abs/2504.18383", "authors": ["Qidong Liu", "Xiangyu Zhao", "Yejing Wang", "Zijian Zhang", "Howard Zhong", "Chong Chen", "Xiang Li", "Wei Huang", "Feng Tian"], "title": "Bridge the Domains: Large Language Models Enhanced Cross-domain Sequential Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "accepted by SIGIR'25", "summary": "Cross-domain Sequential Recommendation (CDSR) aims to extract the preference\nfrom the user's historical interactions across various domains. Despite some\nprogress in CDSR, two problems set the barrier for further advancements, i.e.,\noverlap dilemma and transition complexity. The former means existing CDSR\nmethods severely rely on users who own interactions on all domains to learn\ncross-domain item relationships, compromising the practicability. The latter\nrefers to the difficulties in learning the complex transition patterns from the\nmixed behavior sequences. With powerful representation and reasoning abilities,\nLarge Language Models (LLMs) are promising to address these two problems by\nbridging the items and capturing the user's preferences from a semantic view.\nTherefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation\nmodel (LLM4CDSR). To obtain the semantic item relationships, we first propose\nan LLM-based unified representation module to represent items. Then, a\ntrainable adapter with contrastive regularization is designed to adapt the CDSR\ntask. Besides, a hierarchical LLMs profiling module is designed to summarize\nuser cross-domain preferences. Finally, these two modules are integrated into\nthe proposed tri-thread framework to derive recommendations. We have conducted\nextensive experiments on three public cross-domain datasets, validating the\neffectiveness of LLM4CDSR. We have released the code online.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLLM4CDSR\u6a21\u578b\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u51b3\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u4e2d\u7684\u91cd\u53e0\u56f0\u5883\u548c\u8f6c\u79fb\u590d\u6742\u6027\uff0c\u901a\u8fc7\u8bed\u4e49\u8868\u793a\u548c\u5c42\u6b21\u5316\u7528\u6237\u504f\u597d\u5efa\u6a21\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u65b9\u6cd5\u4f9d\u8d56\u7528\u6237\u5728\u6240\u6709\u57df\u7684\u884c\u4e3a\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u590d\u6742\u8f6c\u79fb\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002LLMs\u7684\u8bed\u4e49\u8868\u793a\u548c\u63a8\u7406\u80fd\u529b\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faLLM4CDSR\u6a21\u578b\uff0c\u5305\u62ec\u57fa\u4e8eLLM\u7684\u7edf\u4e00\u8868\u793a\u6a21\u5757\u3001\u53ef\u8bad\u7ec3\u9002\u914d\u5668\u3001\u5c42\u6b21\u5316\u7528\u6237\u504f\u597d\u6a21\u5757\uff0c\u5e76\u6574\u5408\u4e3a\u4e09\u7ebf\u7a0b\u6846\u67b6\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LLM4CDSR\u7684\u6709\u6548\u6027\u3002", "conclusion": "LLM4CDSR\u901a\u8fc7LLMs\u7684\u8bed\u4e49\u80fd\u529b\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.18207", "pdf": "https://arxiv.org/pdf/2504.18207", "abs": "https://arxiv.org/abs/2504.18207", "authors": ["Simon Lucey"], "title": "Gradient Descent as a Shrinkage Operator for Spectral Bias", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We generalize the connection between activation function and spline\nregression/smoothing and characterize how this choice may influence spectral\nbias within a 1D shallow network. We then demonstrate how gradient descent (GD)\ncan be reinterpreted as a shrinkage operator that masks the singular values of\na neural network's Jacobian. Viewed this way, GD implicitly selects the number\nof frequency components to retain, thereby controlling the spectral bias. An\nexplicit relationship is proposed between the choice of GD hyperparameters\n(learning rate & number of iterations) and bandwidth (the number of active\ncomponents). GD regularization is shown to be effective only with monotonic\nactivation functions. Finally, we highlight the utility of non-monotonic\nactivation functions (sinc, Gaussian) as iteration-efficient surrogates for\nspectral bias.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6fc0\u6d3b\u51fd\u6570\u4e0e\u6837\u6761\u56de\u5f52/\u5e73\u6ed1\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf91D\u6d45\u5c42\u7f51\u7edc\u9891\u8c31\u504f\u7f6e\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5c06\u68af\u5ea6\u4e0b\u964d\uff08GD\uff09\u91cd\u65b0\u89e3\u91ca\u4e3a\u4e00\u79cd\u6536\u7f29\u7b97\u5b50\uff0c\u63ed\u793a\u4e86GD\u5982\u4f55\u901a\u8fc7\u9690\u5f0f\u9009\u62e9\u9891\u7387\u5206\u91cf\u6765\u63a7\u5236\u9891\u8c31\u504f\u7f6e\u3002", "motivation": "\u7814\u7a76\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u795e\u7ecf\u7f51\u7edc\u7684\u9891\u8c31\u504f\u7f6e\uff0c\u5e76\u63a2\u7d22\u68af\u5ea6\u4e0b\u964d\u5728\u5176\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u5c06\u68af\u5ea6\u4e0b\u964d\u91cd\u65b0\u89e3\u91ca\u4e3a\u6536\u7f29\u7b97\u5b50\uff0c\u5206\u6790\u5176\u5bf9\u795e\u7ecf\u7f51\u7edcJacobian\u77e9\u9635\u5947\u5f02\u503c\u7684\u63a9\u853d\u4f5c\u7528\uff0c\u5e76\u63d0\u51faGD\u8d85\u53c2\u6570\u4e0e\u5e26\u5bbd\u7684\u663e\u5f0f\u5173\u7cfb\u3002", "result": "GD\u6b63\u5219\u5316\u4ec5\u5bf9\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\u6709\u6548\uff0c\u975e\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\uff08\u5982sinc\u3001\u9ad8\u65af\uff09\u53ef\u4f5c\u4e3a\u9891\u8c31\u504f\u7f6e\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u6fc0\u6d3b\u51fd\u6570\u7684\u9009\u62e9\u548cGD\u8d85\u53c2\u6570\u7684\u8c03\u6574\u5bf9\u63a7\u5236\u9891\u8c31\u504f\u7f6e\u81f3\u5173\u91cd\u8981\uff0c\u975e\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\u5728\u8fed\u4ee3\u6548\u7387\u4e0a\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2504.18400", "pdf": "https://arxiv.org/pdf/2504.18400", "abs": "https://arxiv.org/abs/2504.18400", "authors": ["Yui Lo", "Yuqian Chen", "Dongnan Liu", "Leo Zekelman", "Jarrett Rushmore", "Yogesh Rathi", "Nikos Makris", "Alexandra J. Golby", "Fan Zhang", "Weidong Cai", "Lauren J. O'Donnell"], "title": "A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "21 pages, 3 figures, 6 tables", "summary": "Shape measures have emerged as promising descriptors of white matter\ntractography, offering complementary insights into anatomical variability and\nassociations with cognitive and clinical phenotypes. However, conventional\nmethods for computing shape measures are computationally expensive and\ntime-consuming for large-scale datasets due to reliance on voxel-based\nrepresentations. We propose Tract2Shape, a novel multimodal deep learning\nframework that leverages geometric (point cloud) and scalar (tabular) features\nto predict ten white matter tractography shape measures. To enhance model\nefficiency, we utilize a dimensionality reduction algorithm for the model to\npredict five primary shape components. The model is trained and evaluated on\ntwo independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.\nWe evaluate the performance of Tract2Shape by training and testing it on the\nHCP-YA dataset and comparing the results with state-of-the-art models. To\nfurther assess its robustness and generalization ability, we also test\nTract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep\nlearning models across all ten shape measures, achieving the highest average\nPearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows\nthat both multimodal input and PCA contribute to performance gains. On the\nunseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low\nnMSE, demonstrating strong generalizability in cross-dataset evaluation.\nTract2Shape enables fast, accurate, and generalizable prediction of white\nmatter shape measures from tractography data, supporting scalable analysis\nacross datasets. This framework lays a promising foundation for future\nlarge-scale white matter shape analysis.", "AI": {"tldr": "Tract2Shape\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u9884\u6d4b\u767d\u8d28\u7ea4\u7ef4\u675f\u7684\u5f62\u72b6\u6d4b\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u767d\u8d28\u7ea4\u7ef4\u675f\u5f62\u72b6\u6d4b\u91cf\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5206\u6790\u3002", "method": "\u63d0\u51faTract2Shape\uff0c\u7ed3\u5408\u51e0\u4f55\uff08\u70b9\u4e91\uff09\u548c\u6807\u91cf\uff08\u8868\u683c\uff09\u7279\u5f81\uff0c\u5229\u7528\u964d\u7ef4\u7b97\u6cd5\u9884\u6d4b\u4e94\u79cd\u4e3b\u8981\u5f62\u72b6\u6210\u5206\u3002", "result": "\u5728HCP-YA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728PPMI\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Tract2Shape\u4e3a\u5927\u89c4\u6a21\u767d\u8d28\u5f62\u72b6\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18268", "pdf": "https://arxiv.org/pdf/2504.18268", "abs": "https://arxiv.org/abs/2504.18268", "authors": ["Ana Matoso", "Catarina Passarinho", "Marta P. Loureiro", "Jos\u00e9 Maria Moreira", "Patr\u00edcia Figueiredo", "Rita G. Nunes"], "title": "Towards a deep learning approach for classifying treatment response in glioblastomas", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Glioblastomas are the most aggressive type of glioma, having a 5-year\nsurvival rate of 6.9%. Treatment typically involves surgery, followed by\nradiotherapy and chemotherapy, and frequent magnetic resonance imaging (MRI)\nscans to monitor disease progression. To assess treatment response,\nradiologists use the Response Assessment in Neuro-Oncology (RANO) criteria to\ncategorize the tumor into one of four labels based on imaging and clinical\nfeatures: complete response, partial response, stable disease, and progressive\ndisease. This assessment is very complex and time-consuming. Since deep\nlearning (DL) has been widely used to tackle classification problems, this work\naimed to implement the first DL pipeline for the classification of RANO\ncriteria based on two consecutive MRI acquisitions. The models were trained and\ntested on the open dataset LUMIERE. Five approaches were tested: 1) subtraction\nof input images, 2) different combinations of modalities, 3) different model\narchitectures, 4) different pretraining tasks, and 5) adding clinical data. The\npipeline that achieved the best performance used a Densenet264 considering only\nT1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR)\nimages as input without any pretraining. A median Balanced Accuracy of 50.96%\nwas achieved. Additionally, explainability methods were applied. Using Saliency\nMaps, the tumor region was often successfully highlighted. In contrast,\nGrad-CAM typically failed to highlight the tumor region, with some exceptions\nobserved in the Complete Response and Progressive Disease classes, where it\neffectively identified the tumor region. These results set a benchmark for\nfuture studies on glioblastoma treatment response assessment based on the RANO\ncriteria while emphasizing the heterogeneity of factors that might play a role\nwhen assessing the tumor's response to treatment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u6839\u636e\u8fde\u7eed\u4e24\u6b21MRI\u626b\u63cf\u5bf9\u795e\u7ecf\u80bf\u7624\u5b66\u53cd\u5e94\u8bc4\u4f30\uff08RANO\uff09\u6807\u51c6\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5728LUMIERE\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u6700\u7ec8\u4f7f\u7528Densenet264\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u7684\u6cbb\u7597\u53cd\u5e94\u8bc4\u4f30\u590d\u6742\u4e14\u8017\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u5206\u7c7b\u95ee\u9898\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u56e0\u6b64\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u9996\u4e2a\u57fa\u4e8eRANO\u6807\u51c6\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u7ba1\u9053\u3002", "method": "\u7814\u7a76\u6d4b\u8bd5\u4e86\u4e94\u79cd\u65b9\u6cd5\uff1a\u8f93\u5165\u56fe\u50cf\u51cf\u6cd5\u3001\u4e0d\u540c\u6a21\u6001\u7ec4\u5408\u3001\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u3001\u4e0d\u540c\u9884\u8bad\u7ec3\u4efb\u52a1\u53ca\u6dfb\u52a0\u4e34\u5e8a\u6570\u636e\u3002\u6700\u4f73\u6027\u80fd\u7ba1\u9053\u4f7f\u7528Densenet264\u6a21\u578b\uff0c\u4ec5\u8f93\u5165T1\u3001T2\u548cFLAIR\u56fe\u50cf\u3002", "result": "\u6700\u4f73\u6a21\u578b\u7684\u5e73\u8861\u51c6\u786e\u7387\u4e3a50.96%\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08\u5982Saliency Maps\uff09\u6210\u529f\u7a81\u51fa\u80bf\u7624\u533a\u57df\uff0c\u4f46Grad-CAM\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u57fa\u4e8eRANO\u6807\u51c6\u7684\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u6cbb\u7597\u53cd\u5e94\u8bc4\u4f30\u8bbe\u5b9a\u4e86\u57fa\u51c6\uff0c\u5e76\u5f3a\u8c03\u4e86\u8bc4\u4f30\u80bf\u7624\u6cbb\u7597\u53cd\u5e94\u65f6\u53ef\u80fd\u6d89\u53ca\u7684\u5f02\u8d28\u6027\u56e0\u7d20\u3002"}}
{"id": "2504.18404", "pdf": "https://arxiv.org/pdf/2504.18404", "abs": "https://arxiv.org/abs/2504.18404", "authors": ["Liang Yu"], "title": "Paradigm shift on Coding Productivity Using GenAI", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Generative AI (GenAI) applications are transforming software engineering by\nenabling automated code co-creation. However, empirical evidence on GenAI's\nproductivity effects in industrial settings remains limited. This paper\ninvestigates the adoption of GenAI coding assistants (e.g., Codeium, Amazon Q)\nwithin telecommunications and FinTech domains. Through surveys and interviews\nwith industrial domain-experts, we identify primary productivity-influencing\nfactors, including task complexity, coding skills, domain knowledge, and GenAI\nintegration. Our findings indicate that GenAI tools enhance productivity in\nroutine coding tasks (e.g., refactoring and Javadoc generation) but face\nchallenges in complex, domain-specific activities due to limited\ncontext-awareness of codebases and insufficient support for customized design\nrules. We highlight new paradigms for coding transfer, emphasizing iterative\nprompt refinement, immersive development environment, and automated code\nevaluation as essential for effective GenAI usage.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u7f16\u7801\u52a9\u624b\u5728\u7535\u4fe1\u548c\u91d1\u878d\u79d1\u6280\u9886\u57df\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5bf9\u5e38\u89c4\u7f16\u7801\u4efb\u52a1\u6709\u751f\u4ea7\u529b\u63d0\u5347\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\u3002", "motivation": "\u63a2\u8ba8GenAI\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5bf9\u751f\u4ea7\u529b\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u586b\u8865\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u548c\u8bbf\u8c08\u7535\u4fe1\u4e0e\u91d1\u878d\u79d1\u6280\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u5206\u6790\u751f\u4ea7\u529b\u5f71\u54cd\u56e0\u7d20\u3002", "result": "GenAI\u5728\u5e38\u89c4\u4efb\u52a1\uff08\u5982\u91cd\u6784\uff09\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u56e0\u4e0a\u4e0b\u6587\u611f\u77e5\u4e0d\u8db3\u800c\u53d7\u9650\u3002", "conclusion": "\u63d0\u51fa\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u548c\u6c89\u6d78\u5f0f\u5f00\u53d1\u73af\u5883\u7b49\u65b0\u8303\u5f0f\uff0c\u4ee5\u63d0\u5347GenAI\u7684\u4f7f\u7528\u6548\u679c\u3002"}}
{"id": "2504.18269", "pdf": "https://arxiv.org/pdf/2504.18269", "abs": "https://arxiv.org/abs/2504.18269", "authors": ["Shintaro Ozaki", "Kazuki Hayashi", "Yusuke Sakai", "Jingun Kwon", "Hidetaka Kamigaito", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation", "categories": ["cs.CL", "cs.CV"], "comment": "Under review", "summary": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance.", "AI": {"tldr": "TextTIGER\u901a\u8fc7\u589e\u5f3a\u548c\u603b\u7ed3\u5b9e\u4f53\u63cf\u8ff0\u6765\u63d0\u5347\u56fe\u50cf\u751f\u6210\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4ec5\u4f7f\u7528\u6807\u9898\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u4e2d\u5b9e\u4f53\u77e5\u8bc6\u8bb0\u5fc6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56e0\u5b9e\u4f53\u6570\u91cf\u5e9e\u5927\u4e14\u6301\u7eed\u589e\u52a0\u3002", "method": "\u63d0\u51faTextTIGER\uff0c\u5229\u7528LLM\u589e\u5f3a\u548c\u603b\u7ed3\u5b9e\u4f53\u63cf\u8ff0\uff0c\u51cf\u5c11\u957f\u8f93\u5165\u7684\u6027\u80fd\u4e0b\u964d\u3002", "result": "\u5728IS\u3001FID\u548cCLIPScore\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u4ec5\u7528\u6807\u9898\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u4e14\u603b\u7ed3\u7684\u63cf\u8ff0\u66f4\u4e30\u5bcc\u3002", "conclusion": "\u589e\u5f3a\u548c\u603b\u7ed3\u5b9e\u4f53\u63cf\u8ff0\u80fd\u6709\u6548\u63d0\u5347\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2504.18323", "pdf": "https://arxiv.org/pdf/2504.18323", "abs": "https://arxiv.org/abs/2504.18323", "authors": ["Yangyang Xu", "Kexin Li", "Li Yang", "You-Wei Wen"], "title": "Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation", "categories": ["math.NA", "cs.CV", "cs.LG", "cs.NA", "65K10, 15A69", "I.4.5; G.1.6"], "comment": "12 pages, 6 figures, 3 tables", "summary": "Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique\nfor decomposing multi-dimensional data into a low-rank tensor and an outlier\ntensor, yet existing methods relying on sparse outlier assumptions often fail\nunder structured corruptions. In this paper, we propose a self-guided data\naugmentation approach that employs adaptive weighting to suppress outlier\ninfluence, reformulating the original TRPCA problem into a standard Tensor\nPrincipal Component Analysis (TPCA) problem. The proposed model involves an\noptimization-driven weighting scheme that dynamically identifies and\ndownweights outlier contributions during tensor augmentation. We develop an\nefficient proximal block coordinate descent algorithm with closed-form updates\nto solve the resulting optimization problem, ensuring computational efficiency.\nTheoretical convergence is guaranteed through a framework combining block\ncoordinate descent with majorization-minimization principles. Numerical\nexperiments on synthetic and real-world datasets, including face recovery,\nbackground subtraction, and hyperspectral denoising, demonstrate that our\nmethod effectively handles various corruption patterns. The results show the\nimprovements in both accuracy and computational efficiency compared to\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u5f15\u5bfc\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u6291\u5236\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u5c06TRPCA\u95ee\u9898\u8f6c\u5316\u4e3a\u6807\u51c6TPCA\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5904\u7406\u7ed3\u6784\u5316\u566a\u58f0\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684TRPCA\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u5f02\u5e38\u5047\u8bbe\uff0c\u5728\u7ed3\u6784\u5316\u566a\u58f0\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u52a0\u6743\u7b56\u7565\u52a8\u6001\u8bc6\u522b\u548c\u6291\u5236\u5f02\u5e38\u503c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fd1\u7aef\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\uff0c\u5e76\u4fdd\u8bc1\u7406\u8bba\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982\u4eba\u8138\u6062\u590d\u3001\u80cc\u666f\u51cf\u9664\u548c\u9ad8\u5149\u8c31\u53bb\u566a\uff09\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u566a\u58f0\u6a21\u5f0f\u3002"}}
{"id": "2504.18423", "pdf": "https://arxiv.org/pdf/2504.18423", "abs": "https://arxiv.org/abs/2504.18423", "authors": ["Rajesh Yarra"], "title": "LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Despite the transformative impact of Artificial Intelligence (AI) across\nvarious sectors, cyber security continues to rely on traditional static and\ndynamic analysis tools, hampered by high false positive rates and superficial\ncode comprehension. While generative AI offers promising automation\ncapabilities for software development, leveraging Large Language Models (LLMs)\nfor vulnerability detection presents unique challenges. This paper explores the\npotential and limitations of LLMs in identifying vulnerabilities, acknowledging\ninherent weaknesses such as hallucinations, limited context length, and\nknowledge cut-offs. Previous attempts employing machine learning models for\nvulnerability detection have proven ineffective due to limited real-world\napplicability, feature engineering challenges, lack of contextual\nunderstanding, and the complexities of training models to keep pace with the\nevolving threat landscape. Therefore, we propose a robust AI-driven approach\nfocused on mitigating these limitations and ensuring the quality and\nreliability of LLM based vulnerability detection. Through innovative\nmethodologies combining Retrieval-Augmented Generation (RAG) and\nMixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs\nwhile addressing their weaknesses, ultimately paving the way for dependable and\nefficient AI-powered solutions in securing the ever-evolving software\nlandscape.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u6f0f\u6d1e\u68c0\u6d4b\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\uff0c\u63d0\u51fa\u7ed3\u5408RAG\u548cMoA\u7684\u521b\u65b0\u65b9\u6cd5\u4ee5\u63d0\u5347\u68c0\u6d4b\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7f51\u7edc\u5b89\u5168\u5de5\u5177\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u548c\u6d45\u5c42\u4ee3\u7801\u7406\u89e3\u95ee\u9898\uff0c\u800c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u56e0\u9002\u7528\u6027\u4e0d\u8db3\u548c\u7279\u5f81\u5de5\u7a0b\u6311\u6218\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u6df7\u5408\u4ee3\u7406\uff08MoA\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u7684\u4f18\u52bf\u5e76\u5f25\u8865\u5176\u5f31\u70b9\u3002", "result": "\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u66f4\u53ef\u9760\u3001\u9ad8\u6548\u7684AI\u9a71\u52a8\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\uff0c\u8bba\u6587\u4e3aAI\u5728\u8f6f\u4ef6\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2504.18344", "pdf": "https://arxiv.org/pdf/2504.18344", "abs": "https://arxiv.org/abs/2504.18344", "authors": ["Kristine S\u00f8rensen", "Oscar Camara", "Ole de Backer", "Klaus Kofoed", "Rasmus Paulsen"], "title": "NUDF: Neural Unsigned Distance Fields for high resolution 3D medical image segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation is often considered as the task of labelling each\npixel or voxel as being inside or outside a given anatomy. Processing the\nimages at their original size and resolution often result in insuperable memory\nrequirements, but downsampling the images leads to a loss of important details.\nInstead of aiming to represent a smooth and continuous surface in a binary\nvoxel-grid, we propose to learn a Neural Unsigned Distance Field (NUDF)\ndirectly from the image. The small memory requirements of NUDF allow for high\nresolution processing, while the continuous nature of the distance field allows\nus to create high resolution 3D mesh models of shapes of any topology (i.e.\nopen surfaces). We evaluate our method on the task of left atrial appendage\n(LAA) segmentation from Computed Tomography (CT) images. The LAA is a complex\nand highly variable shape, being thus difficult to represent with traditional\nsegmentation methods using discrete labelmaps. With our proposed method, we are\nable to predict 3D mesh models that capture the details of the LAA and achieve\naccuracy in the order of the voxel spacing in the CT images.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\uff08NUDF\uff09\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u5206\u8fa8\u7387\u5904\u7406\u4e2d\u7684\u5185\u5b58\u95ee\u9898\uff0c\u5e76\u80fd\u591f\u751f\u6210\u9ad8\u7cbe\u5ea6\u76843D\u7f51\u683c\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u9762\u4e34\u5185\u5b58\u4e0d\u8db3\u6216\u7ec6\u8282\u4e22\u5931\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u590d\u6742\u89e3\u5256\u7ed3\u6784\uff08\u5982\u5de6\u5fc3\u8033\uff09\u3002", "method": "\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u795e\u7ecf\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\uff08NUDF\uff09\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4e8c\u503c\u4f53\u7d20\u7f51\u683c\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u5904\u7406\u548c\u8fde\u7eed\u8868\u9762\u5efa\u6a21\u3002", "result": "\u5728\u5de6\u5fc3\u8033\uff08LAA\uff09\u5206\u5272\u4efb\u52a1\u4e2d\uff0cNUDF\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u7cbe\u5ea6\u76843D\u7f51\u683c\u6a21\u578b\uff0c\u7cbe\u5ea6\u8fbe\u5230CT\u56fe\u50cf\u4f53\u7d20\u95f4\u8ddd\u7ea7\u522b\u3002", "conclusion": "NUDF\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u89e3\u5256\u7ed3\u6784\u7684\u9ad8\u5206\u8fa8\u7387\u5efa\u6a21\u3002"}}
{"id": "2504.18425", "pdf": "https://arxiv.org/pdf/2504.18425", "abs": "https://arxiv.org/abs/2504.18425", "authors": ["KimiTeam", "Ding Ding", "Zeqian Ju", "Yichong Leng", "Songxiang Liu", "Tong Liu", "Zeyu Shang", "Kai Shen", "Wei Song", "Xu Tan", "Heyi Tang", "Zhengtao Wang", "Chu Wei", "Yifei Xin", "Xinran Xu", "Jianwei Yu", "Yutao Zhang", "Xinyu Zhou", "Y. Charles", "Jun Chen", "Yanru Chen", "Yulun Du", "Weiran He", "Zhenxing Hu", "Guokun Lai", "Qingcheng Li", "Yangyang Liu", "Weidong Sun", "Jianzhou Wang", "Yuzhi Wang", "Yuefeng Wu", "Yuxin Wu", "Dongchao Yang", "Hao Yang", "Ying Yang", "Zhilin Yang", "Aoxiong Yin", "Ruibin Yuan", "Yutong Zhang", "Zaida Zhou"], "title": "Kimi-Audio Technical Report", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.", "AI": {"tldr": "Kimi-Audio\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff0c\u64c5\u957f\u97f3\u9891\u7406\u89e3\u3001\u751f\u6210\u548c\u5bf9\u8bdd\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u97f3\u9891\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u591a\u529f\u80fd\u3001\u9ad8\u6027\u80fd\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u5e7f\u6cdb\u7684\u97f3\u9891\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002", "method": "\u91c7\u752812.5Hz\u97f3\u9891\u5206\u8bcd\u5668\uff0c\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u65b0\u67b6\u6784\uff0c\u7ed3\u5408\u6d41\u5339\u914d\u7684\u5206\u5757\u6d41\u5f0f\u89e3\u7801\u5668\uff0c\u5229\u7528\u8d85\u8fc71300\u4e07\u5c0f\u65f6\u7684\u9884\u8bad\u7ec3\u6570\u636e\u548c\u9ad8\u8d28\u91cf\u540e\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u8bed\u97f3\u8bc6\u522b\u3001\u97f3\u9891\u7406\u89e3\u3001\u95ee\u7b54\u548c\u5bf9\u8bdd\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "Kimi-Audio\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u548c\u5de5\u5177\u5305\u63a8\u52a8\u97f3\u9891\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2504.18398", "pdf": "https://arxiv.org/pdf/2504.18398", "abs": "https://arxiv.org/abs/2504.18398", "authors": ["Xinmin Feng", "Zhuoyuan Li", "Li Li", "Dong Liu", "Feng Wu"], "title": "Partition Map-Based Fast Block Partitioning for VVC Inter Coding", "categories": ["eess.IV", "cs.CV"], "comment": "23 pages, 26 figures. Project page:\n  https://github.com/ustc-ivclab/IPM", "summary": "Among the new techniques of Versatile Video Coding (VVC), the quadtree with\nnested multi-type tree (QT+MTT) block structure yields significant coding gains\nby providing more flexible block partitioning patterns. However, the recursive\npartition search in the VVC encoder increases the encoder complexity\nsubstantially. To address this issue, we propose a partition map-based\nalgorithm to pursue fast block partitioning in inter coding. Based on our\nprevious work on partition map-based methods for intra coding, we analyze the\ncharacteristics of VVC inter coding, and thus improve the partition map by\nincorporating an MTT mask for early termination. Next, we develop a neural\nnetwork that uses both spatial and temporal features to predict the partition\nmap. It consists of several special designs including stacked top-down and\nbottom-up processing, quantization parameter modulation layers, and\npartitioning-adaptive warping. Furthermore, we present a dual-threshold\ndecision scheme to achieve a fine-grained trade-off between complexity\nreduction and rate-distortion (RD) performance loss. The experimental results\ndemonstrate that the proposed method achieves an average 51.30% encoding time\nsaving with a 2.12% Bjontegaard Delta Bit Rate (BDBR) under the random access\nconfiguration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u533a\u56fe\u7684\u5feb\u901f\u5757\u5212\u5206\u7b97\u6cd5\uff0c\u7528\u4e8eVVC\u7f16\u7801\u4e2d\u7684\u5e27\u95f4\u7f16\u7801\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u53cc\u9608\u503c\u51b3\u7b56\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u7f16\u7801\u590d\u6742\u5ea6\u3002", "motivation": "VVC\u7f16\u7801\u4e2d\u7684QT+MTT\u5757\u7ed3\u6784\u867d\u63d0\u9ad8\u7f16\u7801\u6548\u7387\uff0c\u4f46\u9012\u5f52\u5206\u533a\u641c\u7d22\u589e\u52a0\u4e86\u7f16\u7801\u590d\u6742\u5ea6\uff0c\u9700\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u6539\u8fdb\u5206\u533a\u56fe\uff0c\u52a0\u5165MTT\u63a9\u7801\u63d0\u524d\u7ec8\u6b62\uff1b\u8bbe\u8ba1\u795e\u7ecf\u7f51\u7edc\u5229\u7528\u65f6\u7a7a\u7279\u5f81\u9884\u6d4b\u5206\u533a\u56fe\uff0c\u5e76\u91c7\u7528\u53cc\u9608\u503c\u51b3\u7b56\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5e73\u5747\u8282\u770151.30%\u7f16\u7801\u65f6\u95f4\uff0cBDBR\u4ec5\u589e\u52a02.12%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u7387\u5931\u771f\u6027\u80fd\u3002"}}
{"id": "2504.18437", "pdf": "https://arxiv.org/pdf/2504.18437", "abs": "https://arxiv.org/abs/2504.18437", "authors": ["Kun He", "Zijian Song", "Shuoxi Zhang", "John E. Hopcroft"], "title": "Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural Collapse", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Class-Incremental Learning (CIL) is a critical capability for real-world\napplications, enabling learning systems to adapt to new tasks while retaining\nknowledge from previous ones. Recent advancements in pre-trained models (PTMs)\nhave significantly advanced the field of CIL, demonstrating superior\nperformance over traditional methods. However, understanding how features\nevolve and are distributed across incremental tasks remains an open challenge.\nIn this paper, we propose a novel approach to modeling feature evolution in\nPTM-based CIL through the lens of neural collapse (NC), a striking phenomenon\nobserved in the final phase of training, which leads to a well-separated,\nequiangular feature space. We explore the connection between NC and CIL\neffectiveness, showing that aligning feature distributions with the NC geometry\nenhances the ability to capture the dynamic behavior of continual learning.\nBased on this insight, we introduce Neural Collapse-inspired Pre-Trained\nModel-based CIL (NCPTM-CIL), a method that dynamically adjusts the feature\nspace to conform to the elegant NC structure, thereby enhancing the continual\nlearning process. Extensive experiments demonstrate that NCPTM-CIL outperforms\nstate-of-the-art methods across four benchmark datasets. Notably, when\ninitialized with ViT-B/16-IN1K, NCPTM-CIL surpasses the runner-up method by\n6.73% on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5d29\u6e83\uff08NC\uff09\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff08NCPTM-CIL\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7279\u5f81\u7a7a\u95f4\u4ee5\u7b26\u5408NC\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u7406\u89e3\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u4e2d\u7279\u5f81\u5982\u4f55\u6f14\u5316\u548c\u5206\u5e03\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u5d29\u6e83\u73b0\u8c61\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faNCPTM-CIL\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u5d29\u6e83\u7684\u51e0\u4f55\u7279\u6027\u52a8\u6001\u8c03\u6574\u7279\u5f81\u7a7a\u95f4\uff0c\u4ee5\u4f18\u5316\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cNCPTM-CIL\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728ViT-B/16-IN1K\u521d\u59cb\u5316\u4e0b\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u901a\u8fc7\u795e\u7ecf\u5d29\u6e83\u73b0\u8c61\u4f18\u5316\u7279\u5f81\u7a7a\u95f4\u5206\u5e03\uff0cNCPTM-CIL\u4e3a\u7c7b\u589e\u91cf\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18405", "pdf": "https://arxiv.org/pdf/2504.18405", "abs": "https://arxiv.org/abs/2504.18405", "authors": ["Jens Hooge", "Gerard Sanroma-Guell", "Faidra Stavropoulou", "Alexander Ullmann", "Gesine Knobloch", "Mark Klemens", "Carola Schmidt", "Sabine Weckbach", "Andreas Bolz"], "title": "HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and Adversarial Models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a\ncrucial role in the detection and characterization of focal liver lesions, with\nthe hepatobiliary phase (HBP) providing essential diagnostic information.\nHowever, acquiring HBP images requires prolonged scan times, which may\ncompromise patient comfort and scanner throughput. In this study, we propose a\ndeep learning based approach for synthesizing HBP images from earlier contrast\nphases (precontrast and transitional) and compare three generative models: a\nperceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion\nprobabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from\ndiverse clinical settings and introduced a contrast evolution score (CES) to\nassess training data quality, enhancing model performance. Quantitative\nevaluation using pixel-wise and perceptual metrics, combined with qualitative\nassessment through blinded radiologist reviews, showed that pGAN achieved the\nbest quantitative performance but introduced heterogeneous contrast in\nout-of-distribution cases. In contrast, the U-Net produced consistent liver\nenhancement with fewer artifacts, while DDPM underperformed due to limited\npreservation of fine structural details. These findings demonstrate the\nfeasibility of synthetic HBP image generation as a means to reduce scan time\nwithout compromising diagnostic utility, highlighting the clinical potential of\ndeep learning for dynamic contrast enhancement in liver MRI. A project demo is\navailable at: https://jhooge.github.io/hepatogen", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5408\u6210HBP\u56fe\u50cf\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u751f\u6210\u6a21\u578b\uff08U-Net\u3001pGAN\u3001DDPM\uff09\uff0c\u65e8\u5728\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u5e76\u4fdd\u6301\u8bca\u65ad\u6548\u679c\u3002", "motivation": "HBP\u56fe\u50cf\u7684\u83b7\u53d6\u65f6\u95f4\u8f83\u957f\uff0c\u5f71\u54cd\u60a3\u8005\u8212\u9002\u5ea6\u548c\u626b\u63cf\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u751f\u6210\u6a21\u578b\uff08U-Net\u3001pGAN\u3001DDPM\uff09\u4ece\u65e9\u671f\u5bf9\u6bd4\u9636\u6bb5\u5408\u6210HBP\u56fe\u50cf\uff0c\u5e76\u5f15\u5165CES\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\u3002", "result": "pGAN\u5b9a\u91cf\u8868\u73b0\u6700\u4f73\u4f46\u5b58\u5728\u5f02\u8d28\u6027\uff0cU-Net\u4e00\u81f4\u6027\u66f4\u597d\uff0cDDPM\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5408\u6210HBP\u56fe\u50cf\u53ef\u884c\uff0c\u80fd\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u4e14\u4e0d\u635f\u5bb3\u8bca\u65ad\u6548\u679c\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u809d\u810fMRI\u4e2d\u7684\u4e34\u5e8a\u6f5c\u529b\u3002"}}
{"id": "2504.18458", "pdf": "https://arxiv.org/pdf/2504.18458", "abs": "https://arxiv.org/abs/2504.18458", "authors": ["Wenyi Xiao", "Leilei Gan", "Weilong Dai", "Wanggui He", "Ziwei Huang", "Haoyuan Li", "Fangxun Shu", "Zhelun Yu", "Peng Zhang", "Hao Jiang", "Fei Wu"], "title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "16 pages, 5 figures, and 12 tables", "summary": "Recent advances in large vision-language models (LVLMs) have revealed an\n\\textit{overthinking} phenomenon, where models generate verbose reasoning\nacross all tasks regardless of questions. To address this issue, we present\n\\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework\nthat dynamically adapts reasoning depth based on question characteristics.\nThrough empirical analysis, we establish the feasibility of fast-slow thinking\nin LVLMs by investigating how response length and data distribution affect\nperformance. We develop FAST-GRPO with three components: model-based metrics\nfor question characterization, an adaptive thinking reward mechanism, and\ndifficulty-aware KL regularization. Experiments across seven reasoning\nbenchmarks demonstrate that FAST achieves state-of-the-art accuracy with over\n10\\% relative improvement compared to the base model, while reducing token\nusage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively\nbalancing reasoning length and accuracy.", "AI": {"tldr": "FAST\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5b58\u5728\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u73b0\u8c61\uff0c\u5373\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u751f\u6210\u5197\u957f\u7684\u63a8\u7406\uff0c\u65e0\u8bba\u95ee\u9898\u662f\u5426\u9700\u8981\u3002", "method": "\u63d0\u51fa\u4e86FAST\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u6a21\u578b\u7684\u6307\u6807\u7528\u4e8e\u95ee\u9898\u7279\u5f81\u5316\u3001\u81ea\u9002\u5e94\u63a8\u7406\u5956\u52b1\u673a\u5236\u548c\u96be\u5ea6\u611f\u77e5\u7684KL\u6b63\u5219\u5316\u3002", "result": "\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFAST\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u76f8\u5bf9\u57fa\u7840\u6a21\u578b\u63d0\u5347\u4e8610%\u4ee5\u4e0a\uff0c\u540c\u65f6\u51cf\u5c11\u4e8632.7-67.3%\u7684token\u4f7f\u7528\u3002", "conclusion": "FAST\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u63a8\u7406\u957f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u4e3aLVLM\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18442", "pdf": "https://arxiv.org/pdf/2504.18442", "abs": "https://arxiv.org/abs/2504.18442", "authors": ["Yue Li", "Pulkit Khandelwal", "Long Xie", "Laura E. M. Wisse", "Nidhi Mundada", "Christopher A. Brown", "Emily McGrew", "Amanda Denning", "Sandhitsu R. Das", "David A. Wolk", "Paul A. Yushkevich"], "title": "Nearly isotropic segmentation for medial temporal lobe subregions in multi-modality MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Morphometry of medial temporal lobe (MTL) subregions in brain MRI is\nsensitive biomarker to Alzheimers Disease and other related conditions. While\nT2-weighted (T2w) MRI with high in-plane resolution is widely used to segment\nhippocampal subfields due to its higher contrast in hippocampus, its lower\nout-of-plane resolution reduces the accuracy of subregion thickness\nmeasurements. To address this issue, we developed a nearly isotropic\nsegmentation pipeline that incorporates image and label upsampling and\nhigh-resolution segmentation in T2w MRI. First, a high-resolution atlas was\ncreated based on an existing anisotropic atlas derived from 29 individuals.\nBoth T1-weighted and T2w images in the atlas were upsampled from their original\nresolution to a nearly isotropic resolution 0.4x0.4x0.52mm3 using a non-local\nmeans approach. Manual segmentations within the atlas were also upsampled to\nmatch this resolution using a UNet-based neural network, which was trained on a\ncohort consisting of both high-resolution ex vivo and low-resolution\nanisotropic in vivo MRI with manual segmentations. Second, a multi-modality\ndeep learning-based segmentation model was trained within this nearly isotropic\natlas. Finally, experiments showed the nearly isotropic subregion segmentation\nimproved the accuracy of cortical thickness as an imaging biomarker for\nneurodegeneration in T2w MRI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4e4e\u5404\u5411\u540c\u6027\u7684\u5206\u5272\u6d41\u7a0b\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u6807\u7b7e\u4e0a\u91c7\u6837\uff0c\u63d0\u9ad8\u4e86T2\u52a0\u6743MRI\u4e2dMTL\u4e9a\u533a\u539a\u5ea6\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3T2\u52a0\u6743MRI\u56e0\u4f4e\u5e73\u9762\u5916\u5206\u8fa8\u7387\u5bfc\u81f4\u7684\u4e9a\u533a\u539a\u5ea6\u6d4b\u91cf\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "method": "\u521b\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u8c31\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u6807\u7b7e\u4e0a\u91c7\u6837\uff0c\u8bad\u7ec3\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\u3002", "result": "\u8fd1\u4e4e\u5404\u5411\u540c\u6027\u7684\u4e9a\u533a\u5206\u5272\u63d0\u9ad8\u4e86T2\u52a0\u6743MRI\u4e2d\u76ae\u5c42\u539a\u5ea6\u4f5c\u4e3a\u795e\u7ecf\u9000\u884c\u6027\u751f\u7269\u6807\u5fd7\u7269\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86T2\u52a0\u6743MRI\u5728\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.18471", "pdf": "https://arxiv.org/pdf/2504.18471", "abs": "https://arxiv.org/abs/2504.18471", "authors": ["Alejandro Murillo-Gonzalez", "Lantao Liu"], "title": "Action Flow Matching for Continual Robot Learning", "categories": ["cs.RO", "cs.AI"], "comment": "Robotics: Science and Systems 2025", "summary": "Continual learning in robotics seeks systems that can constantly adapt to\nchanging environments and tasks, mirroring human adaptability. A key challenge\nis refining dynamics models, essential for planning and control, while\naddressing issues such as safe adaptation, catastrophic forgetting, outlier\nmanagement, data efficiency, and balancing exploration with exploitation -- all\nwithin task and onboard resource constraints. Towards this goal, we introduce a\ngenerative framework leveraging flow matching for online robot dynamics model\nalignment. Rather than executing actions based on a misaligned model, our\napproach refines planned actions to better match with those the robot would\ntake if its model was well aligned. We find that by transforming the actions\nthemselves rather than exploring with a misaligned model -- as is traditionally\ndone -- the robot collects informative data more efficiently, thereby\naccelerating learning. Moreover, we validate that the method can handle an\nevolving and possibly imperfect model while reducing, if desired, the\ndependency on replay buffers or legacy model snapshots. We validate our\napproach using two platforms: an unmanned ground vehicle and a quadrotor. The\nresults highlight the method's adaptability and efficiency, with a record\n34.2\\% higher task success rate, demonstrating its potential towards enabling\ncontinual robot learning. Code:\nhttps://github.com/AlejandroMllo/action_flow_matching.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u5bf9\u9f50\uff0c\u901a\u8fc7\u4f18\u5316\u52a8\u4f5c\u800c\u975e\u63a2\u7d22\uff0c\u63d0\u9ad8\u6570\u636e\u6536\u96c6\u6548\u7387\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u52a8\u529b\u5b66\u6a21\u578b\u5bf9\u9f50\u95ee\u9898\uff0c\u5e94\u5bf9\u5b89\u5168\u9002\u5e94\u3001\u707e\u96be\u6027\u9057\u5fd8\u3001\u5f02\u5e38\u7ba1\u7406\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u6d41\u5339\u914d\u751f\u6210\u6846\u67b6\uff0c\u4f18\u5316\u52a8\u4f5c\u4ee5\u5339\u914d\u5bf9\u9f50\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u51cf\u5c11\u5bf9\u91cd\u653e\u7f13\u51b2\u533a\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u548c\u56db\u65cb\u7ffc\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad834.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6301\u7eed\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\uff0c\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.18497", "pdf": "https://arxiv.org/pdf/2504.18497", "abs": "https://arxiv.org/abs/2504.18497", "authors": ["Yifeng Mao", "Bozhidar Stevanoski", "Yves-Alexandre de Montjoye"], "title": "DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Empirical inference attacks are a popular approach for evaluating the privacy\nrisk of data release mechanisms in practice. While an active attack literature\nexists to evaluate machine learning models or synthetic data release, we\ncurrently lack comparable methods for fixed aggregate statistics, in particular\nwhen only a limited number of statistics are released. We here propose an\ninference attack framework against fixed aggregate statistics and an attribute\ninference attack called DeSIA. We instantiate DeSIA against the U.S. Census\nPPMF dataset and show it to strongly outperform reconstruction-based attacks.\nIn particular, we show DeSIA to be highly effective at identifying vulnerable\nusers, achieving a true positive rate of 0.14 at a false positive rate of\n$10^{-3}$. We then show DeSIA to perform well against users whose attributes\ncannot be verified and when varying the number of aggregate statistics and\nlevel of noise addition. We also perform an extensive ablation study of DeSIA\nand show how DeSIA can be successfully adapted to the membership inference\ntask. Overall, our results show that aggregation alone is not sufficient to\nprotect privacy, even when a relatively small number of aggregates are being\nreleased, and emphasize the need for formal privacy mechanisms and testing\nbefore aggregate statistics are released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fa\u5b9a\u805a\u5408\u7edf\u8ba1\u6570\u636e\u7684\u63a8\u65ad\u653b\u51fb\u6846\u67b6DeSIA\uff0c\u5e76\u5728\u7f8e\u56fd\u4eba\u53e3\u666e\u67e5PPMF\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4f18\u4e8e\u57fa\u4e8e\u91cd\u6784\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u56fa\u5b9a\u805a\u5408\u7edf\u8ba1\u6570\u636e\u7684\u63a8\u65ad\u653b\u51fb\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u53d1\u5e03\u5c11\u91cf\u7edf\u8ba1\u6570\u636e\u65f6\u3002", "method": "\u63d0\u51faDeSIA\u653b\u51fb\u6846\u67b6\uff0c\u5e76\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\uff08\u5982\u566a\u58f0\u6dfb\u52a0\u3001\u7edf\u8ba1\u91cf\u6570\u91cf\uff09\u6d4b\u8bd5\u5176\u6027\u80fd\u3002", "result": "DeSIA\u5728\u8bc6\u522b\u6613\u53d7\u653b\u51fb\u7528\u6237\u65f6\u8868\u73b0\u51fa\u8272\uff08\u771f\u9633\u6027\u73870.14\uff0c\u5047\u9633\u6027\u738710^-3\uff09\uff0c\u4e14\u9002\u5e94\u6027\u5f3a\u3002", "conclusion": "\u4ec5\u9760\u805a\u5408\u4e0d\u8db3\u4ee5\u4fdd\u62a4\u9690\u79c1\uff0c\u9700\u7ed3\u5408\u6b63\u5f0f\u9690\u79c1\u673a\u5236\u548c\u6d4b\u8bd5\u3002"}}
{"id": "2504.18520", "pdf": "https://arxiv.org/pdf/2504.18520", "abs": "https://arxiv.org/abs/2504.18520", "authors": ["Jiahao Huang", "Fanwen Wang", "Pedro F. Ferreira", "Haosen Zhang", "Yinzhe Wu", "Zhifan Gao", "Lei Zhu", "Angelica I. Aviles-Rivero", "Carola-Bibiane Schonlieb", "Andrew D. Scott", "Zohya Khalique", "Maria Dwornik", "Ramyah Rajakulasingam", "Ranil De Silva", "Dudley J. Pennell", "Guang Yang", "Sonia Nielles-Vallespin"], "title": "RSFR: A Coarse-to-Fine Reconstruction Framework for Diffusion Tensor Cardiac MRI with Semantic-Aware Refinement", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cardiac diffusion tensor imaging (DTI) offers unique insights into\ncardiomyocyte arrangements, bridging the gap between microscopic and\nmacroscopic cardiac function. However, its clinical utility is limited by\ntechnical challenges, including a low signal-to-noise ratio, aliasing\nartefacts, and the need for accurate quantitative fidelity. To address these\nlimitations, we introduce RSFR (Reconstruction, Segmentation, Fusion &\nRefinement), a novel framework for cardiac diffusion-weighted image\nreconstruction. RSFR employs a coarse-to-fine strategy, leveraging zero-shot\nsemantic priors via the Segment Anything Model and a robust Vision Mamba-based\nreconstruction backbone. Our framework integrates semantic features effectively\nto mitigate artefacts and enhance fidelity, achieving state-of-the-art\nreconstruction quality and accurate DT parameter estimation under high\nundersampling rates. Extensive experiments and ablation studies demonstrate the\nsuperior performance of RSFR compared to existing methods, highlighting its\nrobustness, scalability, and potential for clinical translation in quantitative\ncardiac DTI.", "AI": {"tldr": "RSFR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5148\u9a8c\u548c\u89c6\u89c9Mamba\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u5fc3\u810fDTI\u7684\u6280\u672f\u6311\u6218\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u53c2\u6570\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "motivation": "\u5fc3\u810fDTI\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u53d7\u9650\u4e8e\u4f4e\u4fe1\u566a\u6bd4\u3001\u4f2a\u5f71\u548c\u5b9a\u91cf\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRSFR\u6846\u67b6\uff0c\u91c7\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u7b56\u7565\uff0c\u7ed3\u5408Segment Anything Model\u7684\u96f6\u6837\u672c\u8bed\u4e49\u5148\u9a8c\u548cVision Mamba\u91cd\u5efa\u9aa8\u5e72\u3002", "result": "RSFR\u5728\u9ad8\u6b20\u91c7\u6837\u7387\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u51c6\u786e\u7684DT\u53c2\u6570\u4f30\u8ba1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RSFR\u5177\u6709\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u4e34\u5e8a\u8f6c\u5316\u6f5c\u529b\uff0c\u4e3a\u5fc3\u810fDTI\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18538", "pdf": "https://arxiv.org/pdf/2504.18538", "abs": "https://arxiv.org/abs/2504.18538", "authors": ["Yixiao Wang"], "title": "Generalization Capability for Imitation Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Imitation learning holds the promise of equipping robots with versatile\nskills by learning from expert demonstrations. However, policies trained on\nfinite datasets often struggle to generalize beyond the training distribution.\nIn this work, we present a unified perspective on the generalization capability\nof imitation learning, grounded in both information theorey and data\ndistribution property. We first show that the generalization gap can be upper\nbounded by (i) the conditional information bottleneck on intermediate\nrepresentations and (ii) the mutual information between the model parameters\nand the training dataset. This characterization provides theoretical guidance\nfor designing effective training strategies in imitation learning, particularly\nin determining whether to freeze, fine-tune, or train large pretrained encoders\n(e.g., vision-language models or vision foundation models) from scratch to\nachieve better generalization. Furthermore, we demonstrate that high\nconditional entropy from input to output induces a flatter likelihood\nlandscape, thereby reducing the upper bound on the generalization gap. In\naddition, it shortens the stochastic gradient descent (SGD) escape time from\nsharp local minima, which may increase the likelihood of reaching global optima\nunder fixed optimization budgets. These insights explain why imitation learning\noften exhibits limited generalization and underscore the importance of not only\nscaling the diversity of input data but also enriching the variability of\noutput labels conditioned on the same input.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u4fe1\u606f\u8bba\u548c\u6570\u636e\u5206\u5e03\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u6cdb\u5316\u5dee\u8ddd\u7684\u4e0a\u754c\u6761\u4ef6\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bad\u7ec3\u7b56\u7565\u7684\u8bbe\u8ba1\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u7406\u8bba\u6307\u5bfc\u4ee5\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u548c\u6a21\u578b\u53c2\u6570\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u4e92\u4fe1\u606f\u6765\u4e0a\u754c\u6cdb\u5316\u5dee\u8ddd\uff0c\u5e76\u5206\u6790\u6761\u4ef6\u71b5\u5bf9\u4f18\u5316\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u6761\u4ef6\u71b5\u80fd\u5e73\u5766\u5316\u4f3c\u7136\u666f\u89c2\uff0c\u51cf\u5c11\u6cdb\u5316\u5dee\u8ddd\u4e0a\u754c\uff0c\u5e76\u7f29\u77edSGD\u9003\u79bb\u5c40\u90e8\u6781\u5c0f\u503c\u7684\u65f6\u95f4\u3002", "conclusion": "\u63d0\u5347\u8f93\u5165\u6570\u636e\u591a\u6837\u6027\u548c\u8f93\u51fa\u6807\u7b7e\u53d8\u5f02\u6027\u5bf9\u6a21\u4eff\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
