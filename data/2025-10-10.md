[[toc]]

## cs.CV

### [1] [Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation](https://arxiv.org/abs/2510.07346)
*Nader Nemati*

Main category: cs.CV

TL;DR: 基于RT-DETR的实时海上目标检测系统，通过合成图像增强和多尺度特征融合，解决了小目标和真实数据不足的问题，在保持实时性能的同时提升了检测精度。

- Motivation: 海上目标检测面临小目标尺寸和真实RGB标注数据有限的挑战，需要开发能够处理这些问题的实时检测系统。
- Method: 采用RT-DETR架构，结合多尺度特征融合、不确定性最小化查询选择和合成-真实数据权重平衡策略，并使用数据增强技术平衡数据集类别。
- Result: 开发了完整的Python鲁棒海上检测流水线，在保持实时性能的同时验证了各模块的贡献，并测试了系统在极端光照和海况下的表现。
- Conclusion: 该系统在保持DETR端到端集合预测优势的同时，允许用户在推理时调整速度与精度平衡，并通过组件分析量化了各架构模块的贡献。


### [2] [DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis](https://arxiv.org/abs/2510.07441)
*Nithin C. Babu,Aniruddha Mahapatra,Harsh Rangwani,Rajiv Soundararajan,Kuldeep Kulkarni*

Main category: cs.CV

TL;DR: 提出了DynamicEval基准，专门评估文本到视频生成模型在动态相机运动下的表现，包含4.5万个人工标注和新的背景场景一致性与前景对象一致性指标。

- Motivation: 现有T2V评估基准存在两个局限：忽视动态相机运动的重要性，以及仅关注模型级评分而忽略视频级评估。
- Method: 构建包含动态相机运动提示的基准，获得4.5万个人工标注，提出基于对象错误图的新背景一致性指标和基于点跟踪的前景一致性指标。
- Result: 新指标在视频级和模型级与人类偏好相关性提升超过2个百分点，能有效处理遮挡/去遮挡等失败情况。
- Conclusion: DynamicEval为动态相机运动下的T2V模型评估提供了更全面的基准，新指标显著提升了与人类判断的一致性。


### [3] [Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors](https://arxiv.org/abs/2510.07470)
*Marien Renaud,Julien Hermant,Deliang Wei,Yu Sun*

Main category: cs.CV

TL;DR: 提出RISP方法，结合重启惯性和基于分数的图像先验，在解决病态成像逆问题时实现快速收敛和高质量重建

- Motivation: 现有方法如RED主要关注设计复杂图像先验来提高重建质量，但收敛加速依赖启发式方法。需要弥合这一差距，在保持高质量重建的同时实现快速收敛。
- Method: RISP方法在RED基础上引入重启惯性机制，同时允许使用基于分数的图像先验。该方法不需要图像先验的凸性假设，并建立了与重球ODE的连续时间动力学系统联系。
- Result: 理论证明RISP比RED具有更快的驻点收敛速度。在多种成像逆问题实验中，RISP实现了快速收敛和高质量重建。
- Conclusion: RISP通过重启惯性和分数先验的结合，为病态成像逆问题提供了既快速又高质量的解决方案，建立了收敛理论与实际性能之间的联系。


### [4] [A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy](https://arxiv.org/abs/2510.07492)
*Guoliang Gong,Man Yu*

Main category: cs.CV

TL;DR: 提出基于图像纯化策略的超低剂量CT去噪框架，通过生成结构对齐的uLDCT-NDCT图像对解决空间错位问题，结合频域流匹配模型实现解剖结构完整性保持。

- Motivation: 超低剂量CT显著减少辐射暴露但引入严重噪声和伪影，导致uLDCT与正常剂量CT图像对之间存在显著空间错位，现有去噪网络难以直接应用。
- Method: 构建真实临床uLDCT肺部数据集；提出图像纯化策略生成结构对齐的图像对；设计频域流匹配模型与IP策略协同工作。
- Result: 在真实临床数据集上，IP策略显著提升多个主流去噪模型性能；FFM模型结合IP策略在解剖结构保持方面达到最先进水平。
- Conclusion: 本研究为真实世界uLDCT去噪中的数据不匹配问题提供了有效解决方案。


### [5] [D2RA: Dual Domain Regeneration Attack](https://arxiv.org/abs/2510.07538)
*Pragati Shuddhodhan Meshram,Varun Chandrasekaran*

Main category: cs.CV

TL;DR: D2RA是一种无需训练的单图像攻击方法，能够在资源受限的对抗环境下有效移除或削弱多种语义水印方案，揭示了当前水印设计的基本弱点。

- Motivation: 随着生成模型的广泛使用，确保内容归属和溯源的水印方法需求日益增长。尽管最近的语义水印方案通过在潜在或频率表示中嵌入信号提高了鲁棒性，但作者发现它们在资源受限的对抗环境下仍然脆弱。
- Method: D2RA是一种无需训练、无需访问底层模型的单图像攻击方法。通过将带水印图像投影到互补表示的自然先验上，D2RA能够抑制水印信号同时保持视觉保真度。
- Result: 在多种水印方案上的实验表明，D2RA方法能够一致地降低水印可检测性，揭示当前水印设计的基本弱点。
- Conclusion: 当前的水印方案在面对资源受限的对抗攻击时仍然存在脆弱性，D2RA攻击方法暴露了这些方案的基本设计缺陷。


### [6] [PickStyle: Video-to-Video Style Transfer with Context-Style Adapters](https://arxiv.org/abs/2510.07546)
*Soroush Mehraban,Vida Adeli,Jacob Rommann,Babak Taati,Kyryl Truskovskyi*

Main category: cs.CV

TL;DR: PickStyle是一个视频风格转换框架，通过插入低秩适配器和利用成对静态图像数据，实现高效的运动风格转换，同时保持视频内容与风格的对齐。

- Motivation: 解决视频风格转换任务中缺乏配对视频数据监督的挑战，目标是保持输入视频内容的同时根据文本提示渲染目标风格。
- Method: 在预训练视频扩散骨干网络中插入低秩适配器，利用成对静态图像数据训练，通过共享增强模拟相机运动构建合成训练片段，并引入上下文-风格分类器自由引导(CS-CFG)。
- Result: 在多个基准测试中实现了时间一致、风格忠实且内容保留的视频转换，在质量和数量上都优于现有基线方法。
- Conclusion: PickStyle框架通过有效的适配器设计和创新的引导机制，成功解决了视频风格转换中的数据稀缺问题，实现了高质量的视频风格迁移。


### [7] [TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility](https://arxiv.org/abs/2510.07550)
*Saman Motamed,Minghao Chen,Luc Van Gool,Iro Laina*

Main category: cs.CV

TL;DR: 提出TRAVL微调方法和ImplausiBench基准，用于评估和改进视频生成模型的物理合理性，解决现有视频语言模型在物理推理方面的局限性。

- Motivation: 现代视频生成模型经常产生违反物理规律的序列，但缺乏定量评估物理真实性的方法。现有视频语言模型难以识别物理违规，暴露了其时序和因果推理的根本局限性。
- Method: 引入TRAVL微调配方，结合平衡训练数据集和轨迹感知注意力模块来改进视频语言模型中的运动编码和判别能力。同时提出ImplausiBench基准，包含300个视频（150个真实，150个生成），消除语言偏见并隔离视觉时序理解。
- Result: 性能通过黄金标准人类判断和更严格的LLM-as-judge指标报告，为多模态模型中的物理合理性提供了统一的评估框架。
- Conclusion: TRAVL和ImplausiBench为探索和改进多模态模型中的物理合理性提供了统一框架，揭示了视觉时序理解中具有挑战性且未被充分探索的方面。


### [8] [Label Semantics for Robust Hyperspectral Image Classification](https://arxiv.org/abs/2510.07556)
*Rafin Hassan,Zarin Tasnim Roshni,Rafiqul Bari,Alimul Islam,Nabeel Mohammed,Moshiur Farazi,Shafin Rahman*

Main category: cs.CV

TL;DR: 提出S3FN网络，通过结合文本语义描述来增强高光谱图像分类性能，解决传统单模态模型过拟合和特征-标签对齐问题

- Motivation: 高光谱图像分类面临训练样本有限、数据维度高导致的过拟合问题，且传统模型仅依赖光谱-空间数据，缺乏语义信息
- Method: 使用LLM生成类别文本描述，通过BERT等文本编码器提取语义向量，与光谱-空间特征融合进行多模态分类
- Result: 在三个高光谱基准数据集上取得显著性能提升，验证了文本语义与光谱-空间数据的协同作用
- Conclusion: 语义增强的高光谱分类模型具有广阔前景，文本语义能有效提升分类性能


### [9] [Cross-Modal Attention Guided Unlearning in Vision-Language Models](https://arxiv.org/abs/2510.07567)
*Karuna Bhaila,Aneesh Komanduri,Minh-Hao Van,Xintao Wu*

Main category: cs.CV

TL;DR: 提出了CAGUL框架，通过跨模态注意力引导的遗忘方法，解决视觉语言模型中敏感信息泄露问题，无需修改预训练模型参数。

- Motivation: 视觉语言模型在训练过程中可能记忆隐私和敏感信息，并在推理时泄露。现有遗忘方法主要针对文本模型，而视觉语言模型增加了视觉上下文中的敏感信息复杂性。
- Method: 利用跨模态注意力分析视觉token在输出生成中的作用，提出CAGUL框架，通过外部模块在低重要性视觉token中编码遗忘信息。
- Result: 实验结果显示该方法性能优于或等同于基于微调的基线方法，能有效防止信息泄露并保持参考模型行为。
- Conclusion: CAGUL是一种轻量级、高效的视觉语言模型遗忘解决方案，无需重新训练成本，具有实际应用价值。


### [10] [MaizeStandCounting (MaSC): Automated and Accurate Maize Stand Counting from UAV Imagery Using Image Processing and Deep Learning](https://arxiv.org/abs/2510.07580)
*Dewi Endah Kharismawati,Toni Kazic*

Main category: cs.CV

TL;DR: 提出MaSC算法，使用轻量级YOLOv9模型从RGB图像自动计数玉米幼苗，支持两种处理模式：拼接图像分块和原始视频帧对齐，能区分玉米与杂草并实现行级精确计数。

- Motivation: 手动计数玉米株数劳动密集、效率低且易出错，特别是在大面积或变化大的田地中，需要自动化解决方案来支持作物管理和研究。
- Method: 使用轻量级YOLOv9模型检测V2-V10生长阶段的玉米幼苗，通过两种模式处理图像：拼接图像分块和原始视频帧使用单应性矩阵对齐，然后基于检测结果的空间分布进行行和范围分割。
- Result: 与2024年夏季苗圃实地人工计数相比，拼接图像模式R²=0.616，原始帧模式R²=0.906；处理83张全分辨率帧仅需60.63秒，包括推理和后处理。
- Conclusion: MaSC是一个可扩展、低成本且准确的自动化玉米株数计数工具，适用于研究和生产环境，具有实时操作潜力。


### [11] [Quick-CapsNet (QCN): A fast alternative to Capsule Networks](https://arxiv.org/abs/2510.07600)
*Pouya Shiri,Ramin Sharifi,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 提出了Quick-CapsNet（QCN）作为Capsule Network的快速替代方案，通过减少胶囊数量实现5倍推理速度提升，但牺牲了少量准确率。

- Motivation: CapsNet在检测重叠数字和对抗仿射变换方面优于传统CNN，但训练和推理速度慢，限制了其在需要快速网络的实时应用中的使用。
- Method: 通过产生更少的胶囊来构建更快的网络，并采用更强大的解码器来进一步提升性能。
- Result: 在MNIST、F-MNIST、SVHN和Cifar-10数据集上实现5倍推理速度提升，但准确率略有下降。
- Conclusion: QCN是开发快速实时CapsNet应用的起点，在速度和准确性之间取得了良好平衡。


### [12] [Rectified-CFG++ for Flow Based Models](https://arxiv.org/abs/2510.07631)
*Shreshth Saini,Shashank Gupta,Alan C. Bovik*

Main category: cs.CV

TL;DR: Rectified-CFG++是一种针对整流流模型的改进引导方法，通过自适应预测器-校正器机制解决标准CFG在整流流模型中引起的离流形漂移问题，在保持确定性的同时实现几何感知的条件化。

- Motivation: 标准CFG在整流流模型上应用时会产生严重的离流形漂移，导致视觉伪影、文本对齐问题和脆弱行为，需要一种更稳定的引导方法。
- Method: 提出自适应预测器-校正器引导：首先执行条件RF更新将样本锚定在学习到的传输路径附近，然后应用加权条件校正，在条件和无条件速度场之间进行插值。
- Result: 在Flux、Stable Diffusion 3/3.5、Lumina等大规模文生图模型上的实验表明，Rectified-CFG++在MS-COCO、LAION-Aesthetic和T2I-CompBench等基准数据集上持续优于标准CFG。
- Conclusion: Rectified-CFG++能够确保速度场的边际一致性，其轨迹保持在数据流形的有界管状邻域内，在广泛的引导强度范围内保持稳定性。


### [13] [PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment](https://arxiv.org/abs/2510.07636)
*Shashank Gupta,Gregoire Phillips,Alan C. Bovik*

Main category: cs.CV

TL;DR: 提出了PIT-QMM，一种能够端到端处理文本、图像和点云的新型大型多模态模型，用于无参考点云质量评估，在多个基准测试中显著优于现有方法。

- Motivation: 大型多模态模型在图像和视频质量评估方面取得了显著进展，但在3D资产领域的应用尚未充分探索。不同模态数据（文本描述、2D投影和3D点云视图）能够提供关于点云质量的互补信息。
- Method: 构建PIT-QMM模型，能够端到端消费文本、图像和点云来预测质量分数，利用多模态信息的互补性。
- Result: 在流行基准测试中显著优于现有最先进方法，且训练迭代次数更少。该框架还支持失真定位和识别，为模型可解释性和交互性开辟了新途径。
- Conclusion: PIT-QMM为无参考点云质量评估提供了有效的多模态解决方案，在性能和效率方面均优于现有方法，同时增强了模型的可解释性。


### [14] [Dual-Stream Alignment for Action Segmentation](https://arxiv.org/abs/2510.07652)
*Harshala Gammulle,Clinton Fookes,Sridha Sridharan,Simon Denman*

Main category: cs.CV

TL;DR: 提出了Dual-Stream Alignment Network (DSA Net)，这是一个用于动作分割的双流混合量子-经典机器学习框架，通过特征对齐学习共享特征空间，在多个基准数据集上达到最先进性能。

- Motivation: 现有动作分割方法主要关注单流时空建模，但最近研究转向双流方法学习动作特征以提升性能。本文旨在探索引入第二流动作特征来指导分割，捕捉动作和动作转换线索。
- Method: 提出DSA Net双流对齐网络，包含帧级流和动作级流，通过Temporal Context块进行跨流通信，使用交叉注意力和量子动作引导调制融合互补信息。采用双流对齐损失（关系一致性、跨级对比和循环一致性重建损失）鼓励特征对齐。
- Result: 在GTEA、Breakfast、50Salads和EgoProcel等多个基准数据集上评估，DSA Net实现了最先进的性能，显著优于现有方法。通过消融研究验证了各组件有效性。
- Conclusion: 这是首个将混合量子-经典机器学习框架引入动作分割的研究，提出的双流对齐网络通过特征对齐学习共享特征空间，在动作分割任务中表现出卓越性能。


### [15] [Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection](https://arxiv.org/abs/2510.07654)
*Yanjie Pan,Qingdong He,Lidong Wang,Bo Peng,Mingmin Chi*

Main category: cs.CV

TL;DR: 提出OIE方法，基于首帧服装替换的视频虚拟试穿策略，通过图像模型替换首帧服装，然后利用姿态和掩码信息引导视频生成模型合成后续帧，在保持领先性能的同时实现参数和计算效率的提升

- Motivation: 解决现有双分支架构在基于Diffusion Transformer的扩散模型中面临的挑战：引入服装参考分支的潜在空间特征需要修改骨干网络导致参数过多，且服装潜在特征缺乏时间特性需要额外学习
- Method: 使用图像服装转移模型替换首帧服装，在编辑后的首帧内容控制下，利用姿态和掩码信息引导视频生成模型的时间先验，顺序合成剩余帧
- Result: 实验表明该方法在参数效率和计算效率方面表现优异，同时在约束条件下仍保持领先性能
- Conclusion: OIE方法通过首帧服装替换策略有效解决了现有方法的参数复杂性和时间特性学习问题，实现了高效且高性能的视频虚拟试穿


### [16] [MONKEY: Masking ON KEY-Value Activation Adapter for Personalization](https://arxiv.org/abs/2510.07656)
*James Baker*

Main category: cs.CV

TL;DR: 提出了一种基于IP-Adapter的个性化扩散模型改进方法，通过自动生成的掩码在第二次处理中限制图像标记到主体区域，让文本提示能够更好地控制背景生成。

- Motivation: 现有的个性化扩散模型在生成图像时往往过度关注主体而忽略文本提示，导致生成的图像只是简单复制主体而无法准确响应文本描述。
- Method: 利用IP-Adapter自动生成的掩码，在第二次处理过程中将图像标记限制在主体区域，使文本提示能够专注于控制背景内容的生成。
- Result: 该方法在描述地点和场景的文本提示下，能够生成准确描绘主体同时完全匹配提示的图像，相比其他个性化方法显示出更高的提示和源图像对齐度。
- Conclusion: 通过掩码限制图像标记到主体区域的方法有效解决了个性化扩散模型中主体过度主导的问题，显著提升了文本提示的控制能力。


### [17] [Automatic Text Box Placement for Supporting Typographic Design](https://arxiv.org/abs/2510.07665)
*Jun Muraoka,Daichi Haraguchi,Naoto Inoue,Wataru Shimoda,Kota Yamaguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: 比较了Transformer模型和视觉语言模型在自动文本框布局中的性能，发现Transformer方法通常优于VLM方法，特别是在包含更丰富外观信息时。

- Motivation: 在广告和网页布局设计中，平衡视觉吸引力和沟通效率至关重要，需要研究自动文本框布局方法。
- Method: 比较了标准Transformer方法、小型视觉语言模型(Phi3.5-vision)、大型预训练VLM(Gemini)以及处理多图像的扩展Transformer在Crello数据集上的表现。
- Result: 标准Transformer模型通常优于VLM方法，特别是在包含更丰富外观信息时。所有方法在处理非常小的文本或密集布局时都面临挑战。
- Conclusion: 任务特定架构具有优势，为自动布局设计的进一步改进提供了方向。


### [18] [TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration](https://arxiv.org/abs/2510.07666)
*Heming Wu,Di Wang,Tai Ma,Peng Zhao,Yubin Xiao,Zhongke Wu,Xing-Ce Wang,Chuang Li,Xuan Wu,You Zhou*

Main category: cs.CV

TL;DR: 提出TCIP模型，通过特征增强残差模块(FERM)解决金字塔网络中解剖结构错位累积问题，并采用双阶段阈值控制迭代(TCI)策略自适应确定迭代次数，在多个医学影像数据集上实现优于现有方法的配准精度。

- Motivation: 金字塔网络在可变形医学图像配准中表现出色，但其解码器架构容易传播和累积解剖结构错位，且现有模型无法根据图像变形需求自适应确定迭代次数，导致过早终止或过度迭代影响配准精度。
- Method: 提出特征增强残差模块(FERM)作为金字塔网络各解码层的核心组件，包含三个顺序块：提取解剖语义特征、抑制无关特征、估计最终变形场。同时提出双阶段阈值控制迭代(TCI)策略，第一阶段评估配准稳定性，第二阶段评估收敛性。
- Result: 在三个公共脑部MRI数据集和一个腹部CT数据集上的实验表明，TCIP在精度上优于最先进的配准网络，同时保持相当的推理速度和紧凑的模型参数规模。
- Conclusion: TCIP模型通过FERM和TCI的有效整合，显著提升了医学图像配准的精度和适应性，具有良好的泛化能力。


### [19] [Controllable Video Synthesis via Variational Inference](https://arxiv.org/abs/2510.07670)
*Haoyi Duan,Yunzhi Zhang,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种视频合成方法，通过变分推断近似组合分布，利用多个视频生成主干网络共同处理所有任务约束，实现高可控性和多样性。

- Motivation: 现有视频生成模型通常针对固定输入格式训练，而实际视频工作流需要从精确的4D对象轨迹到粗略文本提示等多种粒度的用户控制。
- Method: 将任务建模为变分推断问题，采用逐步KL散度最小化策略，并提出上下文条件分解技术以减少解空间中的模式，避免局部最优。
- Result: 实验表明，该方法在可控性、多样性和3D一致性方面优于先前工作。
- Conclusion: 该方法能够为指定元素提供高可控性，同时为未指定元素保持多样性，解决了视频合成中的多粒度控制需求。


### [20] [Hybrid CNN-BYOL Approach for Fault Detection in Induction Motors Using Thermal Images](https://arxiv.org/abs/2510.07692)
*Tangin Amir Smrity,MD Zahin Muntaqim Hasan Muhammad Kafi,Abu Saleh Musa Miah,Najmul Hassan,Yuichi Okuyama,Nobuyoshi Asai,Taro Suzuki,Jungpil Shin*

Main category: cs.CV

TL;DR: 提出了一种结合BYOL和CNN的混合方法，用于通过热图像分类检测感应电机故障，新设计的BYOL-IMNet模型在测试中达到99.89%的准确率。

- Motivation: 感应电机在工业和日常生活中不可或缺，但容易发生各种故障，导致过热、能耗浪费和服务中断。早期故障检测对保护电机和延长其使用寿命至关重要。
- Method: 集成BYOL与CNN的混合方法，使用多种深度学习模型（ResNet-50、DenseNet-121等）进行BYOL技术，并提出了新的轻量级CNN模型BYOL-IMNet，包含四个定制设计的块专门用于热图像故障分类。
- Result: 提出的BYOL-IMNet模型达到99.89%的测试准确率，每张图像的推理时间为5.7毫秒，优于现有最先进模型。
- Conclusion: CNN-BYOL混合方法在提高感应电机故障检测准确性方面表现出色，为工业环境中的在线监测提供了稳健的方法论。


### [21] [Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision](https://arxiv.org/abs/2510.07703)
*Xiaoxu Ma,Runhao Li,Zhenyu Weng*

Main category: cs.CV

TL;DR: 提出了一种名为MLH的弱到强哈希学习框架，通过中心哈希分支和成对哈希分支的相互学习，结合混合哈希专家模块，在保持全局结构建模优势的同时有效利用局部相似性信息。

- Motivation: 中心哈希方法在建模全局数据结构方面表现出色，但往往未能充分利用重要的局部相似性信息。为了解决这一局限性，需要一种能够同时兼顾全局结构和局部相似性的哈希学习方法。
- Method: 采用弱到强学习框架，包含一个强大的中心哈希分支和一个较弱的成对哈希分支。通过迭代式相互学习过程，中心分支从成对分支学习局部相似性线索。此外，引入了混合哈希专家模块来实现有效的跨分支交互。
- Result: 在多个基准数据集上的广泛实验表明，MLH始终优于最先进的哈希方法，证明了该方法的有效性。
- Conclusion: MLH框架成功地将中心哈希方法的全局建模优势与成对哈希方法的局部相似性保持能力相结合，通过相互学习和专家混合机制实现了更优的哈希学习性能。


### [22] [RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning](https://arxiv.org/abs/2510.07721)
*Zipeng Guo,Lichen Ma,Xiaolong Fu,Gaojing Zhou,Lan Yang,Yuchen Zhou,Linkai Liu,Yu He,Ximan Liu,Shiping Dong,Jingling Fu,Zhen Chen,Yu Shi,Junshi Huang,Jason Li,Chao Gou*

Main category: cs.CV

TL;DR: 提出了Repainter强化学习框架，结合空间抠图轨迹优化和GRPO，用于电商产品图像去水印和促销文本，显著优于现有方法。

- Motivation: 电商平台中产品图像的水印和促销文本等侵入元素阻碍了清晰吸引人的产品视觉展示，现有扩散修复方法在商业场景中面临对象移除不可靠和领域适应性有限的问题。
- Method: 使用强化学习框架，集成空间抠图轨迹优化和组相对策略优化(GRPO)，通过调制注意力机制强调背景上下文，引入复合奖励机制平衡全局、局部和语义约束。
- Result: 在EcomPaint-Bench基准测试中显著优于最先进方法，特别是在复杂构图场景中表现优异，减少了视觉伪影和奖励欺骗问题。
- Conclusion: Repainter框架有效解决了电商图像修复中的关键挑战，同时贡献了EcomPaint-100K数据集和标准化基准，为领域内研究提供了重要资源。


### [23] [SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction](https://arxiv.org/abs/2510.07723)
*Wenyue Chen,Peng Li,Wangguandong Zheng,Chengfeng Zhao,Mengfei Li,Yaolong Zhu,Zhiyang Dou,Ronggang Wang,Yuan Liu*

Main category: cs.CV

TL;DR: SyncHuman提出了一种结合2D多视角生成模型和3D原生生成模型的新框架，用于从单视角图像重建高质量着衣人体网格，即使在挑战性姿态下也能实现。

- Motivation: 现有方法依赖SMPL估计和条件图像生成模型，但存在3D先验不准确、难以处理复杂人体姿态和重建精细细节的问题。
- Method: 联合微调多视角生成模型和3D原生生成模型，使用像素对齐的2D-3D同步注意力机制，并引入特征注入机制将2D多视角图像的精细细节提升到对齐的3D形状上。
- Result: 实验表明SyncHuman实现了稳健且逼真的3D人体重建，在几何精度和视觉保真度上优于基线方法。
- Conclusion: 该方法为未来3D生成模型提供了一个有前景的方向，通过整合2D和3D生成模型的互补优势实现了高质量重建。


### [24] [ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes](https://arxiv.org/abs/2510.07729)
*Jian Gao,Mengqi Yuan,Yifei Zeng,Chang Zeng,Zhihao Li,Zhenyu Chen,Weichao Qiu,Xiao-Xiao Long,Hao Zhu,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 提出了ComGS框架，通过表面八面体探针(SOPs)实现可重光照的物体重建和基于扩散模型的场景光照估计，解决了高斯泼溅中物体-场景组合的不一致性问题，实现28FPS的实时渲染。

- Motivation: 高斯泼溅(GS)虽然能实现沉浸式渲染，但物体与场景组合时存在外观和阴影不一致的问题，需要可重光照的物体重建和场景光照估计。现有方法效率低或无法处理复杂场景。
- Method: 1. 使用表面八面体探针(SOPs)存储光照和遮挡信息，通过插值实现高效3D查询，避免昂贵的光线追踪；2. 简化场景光照估计，聚焦物体放置位置的环境光照，使用扩散模型补全光照。
- Result: SOPs在重建中提供至少2倍加速，支持高斯场景中的实时阴影计算。ComGS框架实现28FPS的高质量实时渲染，产生视觉和谐的生动阴影效果，编辑仅需36秒。
- Conclusion: ComGS通过创新的SOPs和光照估计方法，有效解决了3D物体-场景组合的挑战，实现了高效、高质量的实时渲染，为沉浸式3D内容创作提供了实用解决方案。


### [25] [UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes](https://arxiv.org/abs/2510.07741)
*Yuang Meng,Xin Jin,Lina Lei,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: 本文提出UltraLED框架，仅使用单帧短曝光RAW图像进行超高清动态范围重建，通过曝光校正和亮度感知RAW去噪来恢复暗区细节，避免传统多帧方法中的重影和运动模糊问题。

- Motivation: 超高清动态范围场景中明暗区域曝光差异显著，传统RGB包围曝光方法易产生重影和对齐问题。研究发现短曝光图像已保留足够高光细节，主要挑战在于暗区去噪和信息恢复。
- Method: 提出两阶段框架UltraLED：首先通过比率图进行曝光校正以平衡动态范围，然后使用亮度感知RAW去噪器增强暗区细节恢复。构建9档包围曝光流程合成真实UHDR图像数据集。
- Result: 大量实验表明，UltraLED在单帧方法中表现显著优于现有方法，能够有效恢复暗区细节同时避免重影和运动模糊。
- Conclusion: 仅使用单帧短曝光RAW图像即可实现超高清动态范围重建，该方法在动态场景中具有鲁棒性，为UHDR图像处理提供了新的解决方案。


### [26] [DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream](https://arxiv.org/abs/2510.07752)
*Junhao He,Jiaxu Wang,Jia Li,Mingyuan Sun,Qiang Zhang,Jiahang Cao,Ziyi Zhang,Yi Gu,Jingkai Sun,Renjing Xu*

Main category: cs.CV

TL;DR: 提出了一种结合低帧率RGB视频和高帧率事件流来重建动态3D高斯泼溅的方法，通过事件运动先验指导变形场优化

- Motivation: 从低帧率RGB视频重建动态3DGS具有挑战性，因为大帧间运动会增加解空间的不确定性。事件相机能异步捕捉快速视觉变化且对运动模糊鲁棒，但缺乏颜色信息。结合两种模态可以解决这一挑战
- Method: 采用事件运动先验指导变形场优化：1) 使用LoCM无监督微调框架提取事件流中的运动先验；2) 提出几何感知数据关联方法建立事件-高斯运动对应关系；3) 采用运动分解和帧间伪标签策略
- Result: 在合成和真实场景上的广泛实验表明，该方法优于现有的图像和事件方法，证明事件数据能有效优化动态3DGS
- Conclusion: 通过结合RGB和事件模态，利用事件运动先验指导变形场优化，能够有效解决低帧率视频重建动态3DGS的挑战


### [27] [Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis](https://arxiv.org/abs/2510.07785)
*Ming Jie Ong,Sze Yinn Ung,Sim Kuan Goh,Jimmy Y. Zhong*

Main category: cs.CV

TL;DR: 本研究使用可解释人工智能(XAI)技术提升MRI图像中脑肿瘤分割的准确性，评估了UNet、ResUNet和AttUNet三种深度学习模型，发现ResUNet在脑肿瘤分割任务中表现最佳。

- Motivation: 通过XAI技术提高脑肿瘤分割模型的准确性，增强医生对AI模型的信任度，辅助临床决策。
- Method: 使用BraTS2020公开数据集，评估UNet、ResUNet和AttUNet三种模型，采用Grad-CAM和注意力可视化等XAI技术分析模型决策过程。
- Result: ResUNet在最终测试阶段表现最优，在Dice和Jaccard相似度分数以及准确率、召回率和F1分数上均优于其他模型。
- Conclusion: 推荐使用ResUNet进行自动化脑肿瘤分割，XAI技术成功提供了模型决策的视觉化解释，增强了模型的可信度。


### [28] [GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.07791)
*Qinghongbing Xie,Zhaoyuan Xia,Feng Zhu,Lijun Gong,Ziyue Li,Rui Zhao,Long Zeng*

Main category: cs.CV

TL;DR: 提出了GTR-Bench基准测试，用于评估视觉语言模型在地理时空推理方面的能力，发现现有模型在结合地图和视频进行多视角时空推理方面表现远低于人类水平。

- Motivation: 现有时空基准测试主要关注以自我为中心的图像/视频推理或以地图为中心的图形推理，缺乏评估视觉语言模型在地理时空推理中同时利用图像/视频和图形上下文的能力，这对交通管理和应急响应等领域很重要。
- Method: 构建了GTR-Bench基准测试，要求在大规模摄像头网络中推理移动目标的地理时空关系，需要在地图和视频之间进行多视角切换、跨多个非重叠视野视频的联合推理，以及对视频未观测区域的时空推理。
- Result: 评估了10多个流行视觉语言模型，发现最佳专有模型Gemini-2.5-Pro准确率仅为34.9%，远低于人类表现的78.61%。模型存在三个主要缺陷：时空上下文利用不平衡、时间预测能力弱、地图与多视角视频输入理解对齐能力不足。
- Conclusion: GTR-Bench为时空智能研究提供了有价值的见解和新机遇，揭示了当前模型在地理时空推理方面的显著局限性。


### [29] [FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition](https://arxiv.org/abs/2510.07810)
*Luu Tu Nguyen,Vu Tram Anh Khuong,Thi Bich Phuong Man,Thi Duyen Ngo,Thanh Ha Le*

Main category: cs.CV

TL;DR: 提出了一种新的微表情识别方法，通过整合起始-顶点和顶点-偏移两个阶段的运动信息，使用幅度调制组合光流(MM-COF)表示和FMANet网络架构，在多个标准数据集上取得了优于现有方法的性能。

- Motivation: 现有微表情识别方法通常只计算起始帧到顶点帧之间的光流，忽略了顶点到偏移阶段的运动信息，这限制了识别性能的提升。
- Method: 提出了幅度调制组合光流(MM-COF)表示，整合两个微表情阶段的运动动态；设计了FMANet端到端神经网络，将双阶段分析和幅度调制内化为可学习模块，自适应融合运动线索并关注重要面部区域。
- Result: 在MMEW、SMIC、CASME-II和SAMM等标准基准数据集上的实验评估表明，提出的MM-COF表示和FMANet优于现有方法。
- Conclusion: 可学习的双阶段框架在推进微表情识别方面具有潜力，能够更全面地捕捉微表情的完整动态过程。


### [30] [An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images](https://arxiv.org/abs/2510.07817)
*Kanglin Ning,Ruzhao Chen,Penghong Wang,Xingtao Wang,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 提出了一种基于房间几何约束的360度室内全景图深度估计框架，通过布局预测提取房间几何信息，并通过背景分割机制将这些信息整合到深度估计过程中。

- Motivation: 现有方法主要关注像素级精度，导致房间角落过度平滑和对噪声敏感。为了解决这些问题，需要引入房间几何约束来改进深度估计。
- Method: 框架包含共享特征编码器和任务特定解码器（布局估计、深度估计、背景分割）。采用两种策略：基于房间几何的背景深度解析策略和背景分割引导的融合机制。
- Result: 在Stanford2D3D、Matterport3D和Structured3D数据集上的实验结果表明，该方法相比现有开源方法取得了显著优越的性能。
- Conclusion: 通过整合房间几何约束和背景分割机制，提出的框架能够有效改善360度室内全景图的深度估计质量，特别是在保持房间结构细节方面。


### [31] [Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation](https://arxiv.org/abs/2510.07823)
*Shohei Enomoto*

Main category: cs.CV

TL;DR: 提出ACAVP方法，通过引入仿射变换和颜色变换增强视觉提示的表达能力，同时使用TrivialAugment数据增强解决过拟合问题，在12个图像分类数据集上取得SOTA效果。

- Motivation: 传统视觉提示方法存在表达力受限和参数增加时容易过拟合的问题，限制了其性能表现。
- Method: ACAVP结合了仿射变换（创建任务特定提示区域）、颜色变换（强调任务相关特征）和加法变换，并采用TrivialAugment数据增强来缓解过拟合。
- Result: 在12个数据集上达到视觉提示方法的SOTA准确率，超越线性探测的平均准确率，对分布偏移具有更强鲁棒性，同时保持推理时的最小计算开销。
- Conclusion: ACAVP通过增强变换操作和有效数据增强，显著提升了视觉提示方法的性能，证明了适当的数据增强对视觉提示训练的普遍益处。


### [32] [MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions](https://arxiv.org/abs/2510.07828)
*Kaen Kogashi,Anoop Cherian,Meng-Yu Jennifer Kuo*

Main category: cs.CV

TL;DR: 提出了MMHOI数据集和MMHOI-Net模型，用于多人物多物体的3D交互建模，在复杂交互场景中实现了最先进的性能。

- Motivation: 现实场景中多人多物体的复杂交互（因果性、目标导向、协作性）在现有3D人-物交互基准中未被充分研究，需要更全面的数据集和模型。
- Method: 构建了MMHOI大规模数据集，包含12个日常场景的完整3D形状和姿态标注；提出了MMHOI-Net端到端transformer网络，采用结构化双补丁表示建模物体及其交互，结合动作识别增强交互预测。
- Result: 在MMHOI和CORE4D数据集上的实验表明，该方法在多HOI建模中达到最先进性能，在准确性和重建质量方面表现出色。
- Conclusion: MMHOI数据集和MMHOI-Net模型为下一代人-物交互研究提供了全面测试平台，能够有效处理复杂的多人多物体交互场景。


### [33] [PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting](https://arxiv.org/abs/2510.07830)
*Houqiang Zhong,Zhenglong Wu,Sihua Fu,Zihan Zheng,Xin Jin,Xiaoyun Zhang,Li Song,Qiang Hu*

Main category: cs.CV

TL;DR: PrismGS是一个基于物理正则化的框架，通过金字塔多尺度监督和显式尺寸正则化，解决了3D高斯溅射在大规模城市场景中的混叠伪影和优化不稳定问题。

- Motivation: 3D高斯溅射在大型城市场景中会出现严重的混叠伪影（闪烁纹理和锯齿边缘）和优化不稳定问题，特别是在高分辨率渲染下。现有方法虽然解决了可扩展性问题，但未能解决这种保真度差距。
- Method: 提出两种协同正则化器：1）金字塔多尺度监督，通过监督渲染与预滤波图像金字塔的一致性，强制模型学习抗混叠表示；2）显式尺寸正则化，对3D高斯尺寸施加物理基础的下界约束，防止形成退化的视图相关基元。
- Result: 在MatrixCity、Mill-19和UrbanScene3D数据集上的实验表明，PrismGS实现了最先进的性能，相比CityGaussian获得了约1.5 dB的PSNR增益，在4K渲染下保持了优越的质量和鲁棒性。
- Conclusion: PrismGS是一个即插即用的框架，与现有管道兼容，能够显著改善3D高斯的渲染行为，在大规模城市场景中实现高质量、稳定的实时渲染。


### [34] [IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text Intermediaries](https://arxiv.org/abs/2510.07837)
*Harsh Kavediya,Vighnesh Nayak,Bheeshm Sharma,Balamurugan Palaniappan*

Main category: cs.CV

TL;DR: 提出IsoSignVid2Aud端到端框架，直接将孤立手语视频序列翻译为语音，无需中间文本表示，避免多阶段系统的延迟和级联错误。

- Motivation: 连接听力和语言障碍人群与他人交流，特别是在教育应用和手语提示界面中处理孤立手语序列而非连续语法手语。
- Method: 结合I3D特征提取模块、专用特征转换网络和音频生成流水线，使用新颖的NMS算法进行非语法连续序列中手语的时间检测。
- Result: 在ASL-Citizen-1500和WLASL-100数据集上分别达到72.01%和78.67%的Top-1准确率，音频质量指标（PESQ: 2.67, STOI: 0.73）表明可理解的语音输出。
- Conclusion: IsoSignVid2Aud框架能够有效实现手语视频到语音的直接翻译，在孤立手语序列翻译任务中表现出竞争力。


### [35] [AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views](https://arxiv.org/abs/2510.07839)
*Yijie Gao,Houqiang Zhong,Tianchi Zhu,Zhengxue Cheng,Qiang Hu,Li Song*

Main category: cs.CV

TL;DR: AlignGS是一个新颖的框架，通过协同优化几何和语义，利用2D基础模型的丰富先验知识直接正则化3D表示，在稀疏视图重建中实现最先进的性能。

- Motivation: 室内场景语义丰富的3D模型需求快速增长，但稀疏视图重建面临几何模糊性挑战。现有方法将语义视为被动特征，作者认为语义理解应作为主动引导力量来提升重建鲁棒性。
- Method: 提出AlignGS框架，通过深度一致性和多面法向正则化等语义到几何的引导机制，从2D基础模型提取丰富先验，端到端协同优化几何和语义。
- Result: 在标准基准测试中，该方法在新视角合成方面达到最先进水平，并产生具有优越几何精度的重建结果。
- Conclusion: 利用语义先验作为几何正则化器，能够从有限输入视图中生成更连贯和完整的3D模型。


### [36] [Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials](https://arxiv.org/abs/2510.07853)
*Thomas Lautenschlager,Nils Friederich,Angelo Jovin Yamachui Sitcheu,Katja Nau,Gaëlle Hayot,Thomas Dickmeis,Ralf Mikut*

Main category: cs.CV

TL;DR: 本文展示了自监督学习在斑马鱼胚胎毒性测试中的应用，能够有效区分不同化合物的作用模式，为高通量毒性测试系统提供自动化评估方案。

- Motivation: 解决高通量毒性测试中自动化评估的关键挑战，利用机器学习模型识别毒物引起的变化，提高测试效率和准确性。
- Method: 使用自监督学习方法学习表征，基于公开的EmbryoNet数据集（包含10种斑马鱼胚胎表型），分析不同化学化合物引起的变化。
- Result: 自监督学习学到的表征能够有效区分不同化合物的作用模式，为毒性测试提供了可靠的自动化评估方法。
- Conclusion: 自监督学习在毒性测试中具有应用潜力，可集成到物理毒性测试设备中，为TOXBOX项目提供技术支持。


### [37] [XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method](https://arxiv.org/abs/2510.07856)
*Haochen Yu,Qiankun Liu,Hongyuan Liu,Jianfei Jiang,Juntao Lyu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: XYZCylinder是一个基于统一圆柱体提升方法的feedforward模型，通过相机建模和特征提升来解决驾驶场景重建中的泛化能力和精度问题。

- Motivation: 现有的feedforward重建方法在驾驶场景重建中存在两个主要问题：(1)固定视角变换在相机配置改变时失效，限制了跨不同驾驶场景的泛化能力；(2)稀疏视图间重叠区域小且驾驶场景复杂，增加了学习难度，降低了重建精度。
- Method: 提出统一圆柱体相机建模(UCCM)策略避免学习视角依赖的空间对应关系，并使用可调参数统一不同相机配置；基于新设计的圆柱平面特征组(CPFG)提出混合表示和专用模块，将2D图像特征提升到3D空间。
- Result: 实验结果表明XYZCylinder在不同评估设置下实现了最先进的性能，并且能够以零样本方式泛化到其他驾驶场景。
- Conclusion: XYZCylinder通过统一的圆柱体提升方法有效解决了驾驶场景重建中的泛化能力和精度问题，在跨场景应用中表现出色。


### [38] [MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.07915)
*Peiran Wu,Zhuorui Yu,Yunze Liu,Chi-Hao Wu,Enmin Zhou,Junxiao Shen*

Main category: cs.CV

TL;DR: 提出MARC方法，通过检索增强的强化学习进行token压缩，在视频理解任务中仅使用1帧token即可达到接近基线的准确率，大幅降低计算成本。

- Motivation: 视觉语言模型扩展到视频时面临高计算成本问题，现有token压缩方法会导致信息丢失和性能下降。
- Method: 采用检索-压缩策略：视觉记忆检索器选择关键片段，压缩组相对策略优化框架将教师模型的推理能力蒸馏到学生模型。
- Result: 在6个视频基准测试中，仅使用1帧token达到接近基线准确率，视觉token减少95%，GPU内存减少72%，延迟降低23.9%。
- Conclusion: MARC在资源受限场景下具有高效实时视频理解的潜力，适用于视频问答、监控和自动驾驶等应用。


### [39] [ASBench: Image Anomalies Synthesis Benchmark for Anomaly Detection](https://arxiv.org/abs/2510.07927)
*Qunyi Zhang,Songan Zhang,Jinbao Wang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: 提出了ASBench，首个专门评估异常合成方法的综合基准框架，包含四个关键评估维度：跨数据集和管道的泛化性能、合成与真实数据比例、合成图像内在指标与异常检测性能指标的相关性，以及混合异常合成策略。

- Motivation: 异常检测在制造业质量控制中至关重要，但受限于异常样本稀少和人工标注成本高。现有研究主要将异常合成作为异常检测框架的辅助组件，缺乏对异常合成算法的系统评估，且忽视了异常合成特有的关键因素。
- Method: 开发ASBench基准框架，通过四个维度系统评估异常合成方法：1）跨数据集和检测管道的泛化能力；2）合成与真实数据比例的影响；3）合成图像内在指标与检测性能的相关性分析；4）混合异常合成策略研究。
- Result: 通过大量实验，ASBench不仅揭示了当前异常合成方法的局限性，还为异常合成领域的未来研究方向提供了可操作的见解。
- Conclusion: ASBench填补了异常合成领域系统性评估的空白，为开发更有效的异常合成方法提供了重要基准和指导方向。


### [40] [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)
*Leigang Qu,Ziyang Wang,Na Zheng,Wenjie Wang,Liqiang Nie,Tat-Seng Chua*

Main category: cs.CV

TL;DR: TTOM是一个无需训练的视频生成框架，通过测试时优化和记忆机制来改善视频基础模型在组合场景中的表现，实现更好的文本-图像对齐。

- Motivation: 视频基础模型在视觉生成方面表现出色，但在组合场景（如运动、数字和空间关系）中存在困难，需要改进文本-图像对齐。
- Method: 提出TTOM框架，在推理时通过优化新参数来对齐时空布局，使用通用布局-注意力目标，并引入参数化记忆机制来维护历史优化上下文。
- Result: 实验结果表明TTOM在T2V-CompBench和Vbench基准测试中表现优异，能够实现组合视频生成的跨模态对齐。
- Conclusion: TTOM是一个有效、实用、可扩展且高效的框架，能够在线实现组合视频生成的跨模态对齐，并展现出强大的可迁移性和泛化能力。


### [41] [CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving](https://arxiv.org/abs/2510.07944)
*Tianrui Zhang,Yichen Liu,Zilin Guo,Yuxin Guo,Jingcheng Ni,Chenjing Ding,Dan Xu,Lewei Lu,Zehuan Wu*

Main category: cs.CV

TL;DR: CVD-STORM是一个基于交叉视角视频扩散的模型，通过空间-时间重建VAE生成多视角长视频，具备4D重建能力，在自动驾驶场景下显著提升视频生成质量。

- Motivation: 随着自动驾驶技术的发展，不仅需要高保真度的可控视频生成，还需要产生多样且有意义的深度估计等几何信息。
- Method: 首先通过辅助4D重建任务微调VAE，增强其3D结构和时间动态编码能力，然后将该VAE集成到视频扩散过程中以提升生成质量。
- Result: 实验结果显示模型在FID和FVD指标上取得显著提升，联合训练的高斯溅射解码器能有效重建动态场景。
- Conclusion: 该方法能够生成高质量的多视角视频，并提供有价值的几何信息，有助于全面场景理解。


### [42] [A Large-scale Dataset for Robust Complex Anime Scene Text Detection](https://arxiv.org/abs/2510.07951)
*Ziyi Dong,Yurui Zhang,Changmao Li,Naomi Rue Golding,Qing Long*

Main category: cs.CV

TL;DR: 提出了AnimeText数据集，专门针对动漫场景的文本检测，包含73.5万张图像和420万个标注文本块，解决了现有数据集在动漫场景中的不足。

- Motivation: 现有文本检测数据集主要针对自然或文档场景，而动漫场景中的文本具有风格多样、排列不规则、易与复杂视觉元素混淆等特点，现有数据集无法有效处理这些特性。
- Method: 构建大规模动漫文本数据集AnimeText，包含分层标注和针对动漫场景的困难负样本，通过交叉数据集基准测试验证其有效性。
- Result: 实验表明，在AnimeText上训练的模型在动漫文本检测任务中优于基于现有数据集训练的模型。
- Conclusion: AnimeText填补了动漫场景文本检测的数据空白，为动漫文本检测提供了专门的训练资源。


### [43] [SimCast: Enhancing Precipitation Nowcasting with Short-to-Long Term Knowledge Distillation](https://arxiv.org/abs/2510.07953)
*Yifang Yin,Shengkai Chen,Yiyao Li,Lu Wang,Ruibing Jin,Wei Cui,Shili Xiang*

Main category: cs.CV

TL;DR: 提出SimCast和CasCast框架，通过短长期知识蒸馏和加权MSE损失改进降水临近预报，结合扩散模型解决确定性预测的模糊和分布偏移问题，在多个数据集上显著优于现有方法。

- Motivation: 降水临近预报对灾害管理、农业、交通和能源优化等社会需求至关重要，现有非自回归方法存在局限性，需要改进预测精度。
- Method: 提出SimCast训练管道，采用短长期知识蒸馏技术和加权MSE损失重点关注强降雨区域；进一步将SimCast集成到基于扩散的CasCast框架中，结合概率模型优势。
- Result: 在三个基准数据集上验证有效性：SEVIR平均CSI得分0.452，HKO-7得分0.474，MeteoNet得分0.361，显著优于现有方法。
- Conclusion: SimCast和CasCast框架能够在不增加推理开销的情况下改善临近预报预测，结合确定性预测和概率模型的优势，在多个数据集上取得优异性能。


### [44] [Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement](https://arxiv.org/abs/2510.07961)
*Yidi Liu,Xueyang Fu,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Latent Harmony是一个两阶段UHD图像修复框架，通过联合正则化潜在空间和强制高频感知重建来重新定义VAE，在效率和细节保留之间取得平衡

- Motivation: 解决UHD图像修复中计算效率与高频细节保留之间的权衡问题，传统VAE的高斯约束会丢弃退化特定高频信息，影响重建保真度
- Method: 两阶段框架：第一阶段LH-VAE通过视觉语义约束和渐进退化扰动增强语义鲁棒性，潜在等变性强化高频重建；第二阶段使用HF-LoRA联合训练，包含编码器LoRA（保真度导向的高频对齐损失）和解码器LoRA（感知导向的损失），通过交替优化和选择性梯度传播训练
- Result: 在UHD和标准分辨率任务上达到最先进性能，有效平衡效率、感知质量和重建精度
- Conclusion: Latent Harmony通过重新定义VAE和HF-LoRA机制，成功解决了UHD图像修复中的效率-保真度权衡问题


### [45] [The impact of abstract and object tags on image privacy classification](https://arxiv.org/abs/2510.07976)
*Darya Baranouskaya,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 该论文探讨了在图像隐私分类任务中，对象标签和抽象标签的有效性比较。研究发现当标签预算有限时，抽象标签更有效；而当每个图像可获得更多标签时，对象相关信息同样有用。

- Motivation: 研究哪种类型的标签更适合上下文依赖且具有主观性的图像隐私任务。对象标签表示具体实体，而抽象标签捕捉更高层次的信息，对需要上下文理解的任务更相关。
- Method: 比较对象标签和抽象标签在图像隐私分类中的效果，分析不同标签数量下的性能差异。
- Result: 当标签预算有限时，抽象标签比对象标签更有效；当每个图像可获得更多标签时，对象相关信息与抽象标签同样有用。
- Conclusion: 这些发现将指导未来研究开发更准确的图像隐私分类器，考虑标签类型和数量的作用。


### [46] [Is Architectural Complexity Always the Answer? A Case Study on SwinIR vs. an Efficient CNN](https://arxiv.org/abs/2510.07984)
*Chandresh Sutariya,Nitin Singh*

Main category: cs.CV

TL;DR: 该论文比较了Transformer模型SwinIR与轻量级CNN在低光图像恢复任务中的性能与效率权衡，发现CNN在仅需10个训练周期和55倍更小模型尺寸的情况下，能达到接近SwinIR的性能水平。

- Motivation: 解决低光图像恢复中高频细节恢复和严重噪声抑制的挑战，同时探索在保持性能的前提下降低计算成本的方法，以满足实际应用中的资源限制需求。
- Method: 通过实验对比最先进的SwinIR Transformer模型与标准轻量级卷积神经网络(CNN)在低光图像恢复任务上的表现，分析两者的性能差异和效率差异。
- Result: SwinIR达到39.03 dB的峰值信噪比，而轻量级CNN达到37.4 dB，性能接近但CNN仅需10个训练周期（SwinIR需132个周期），且模型尺寸比SwinIR小55倍以上。
- Conclusion: 标准CNN能以显著降低的计算开销提供接近最先进水平的性能，在资源受限的实际应用场景中具有明显优势。


### [47] [GraphEnet: Event-driven Human Pose Estimation with a Graph Neural Network](https://arxiv.org/abs/2510.07990)
*Gaurvi Goyal,Pham Cong Thuong,Arren Glover,Masayoshi Mizuno,Chiara Bartolozzi*

Main category: cs.CV

TL;DR: 提出GraphEnet，一种基于图神经网络的2D人体姿态估计方法，专门用于处理事件相机数据，实现高频率的单人姿态估计。

- Motivation: 事件相机具有低延迟和低能耗优势，适合便携设备和移动机器人应用，但现有方法未能充分利用其稀疏特性进行人体姿态估计。
- Method: 使用图神经网络处理事件相机的稀疏输出，采用基于线的中间事件表示，结合新颖的偏移向量学习范式和基于置信度的池化方法。
- Result: 这是首个将图神经网络应用于事件数据的人体姿态估计工作，实现了高频率的姿态估计。
- Conclusion: GraphEnet成功展示了图神经网络在事件相机人体姿态估计中的有效性，为资源受限环境下的实时应用提供了新解决方案。


### [48] [CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.08003)
*Weihuang Lin,Yiwei Ma,Jiayi Ji,Xiaoshuai Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: CIR-CoT是首个集成显式思维链推理的端到端检索导向多模态大语言模型，通过生成可解释的推理链来提升跨模态交互理解，实现更准确和透明的图像检索。

- Motivation: 当前基于视觉语言模型和多模态大语言模型的组合图像检索方法存在不透明性，用户无法理解检索原理，且模型难以遵循复杂细粒度指令。
- Method: 采用三阶段流程创建结构化思维链标注（描述、推理、结论），然后微调模型生成结构化输出，最后将检索意图编码到专用嵌入中。
- Result: 在领域内数据集（FashionIQ、CIRR）上表现优异，在领域外数据集CIRCO上展现出卓越的泛化能力。
- Conclusion: CIR-CoT为构建更有效和可信赖的检索系统开辟了新路径，通过显式推理提高了检索准确性和透明度。


### [49] [RayFusion: Ray Fusion Enhanced Collaborative Visual Perception](https://arxiv.org/abs/2510.08017)
*Shaohong Wang,Bin Lu,Xinyu Xiao,Hanzhi Zhong,Bowen Pang,Tong Wang,Zhiyu Xiang,Hangguan Shan,Eryun Liu*

Main category: cs.CV

TL;DR: RayFusion是一种基于光线的融合方法，通过利用协作车辆的射线占用信息来减少冗余和误检，提升纯摄像头协作感知系统的3D目标检测性能。

- Motivation: 解决摄像头感知系统因缺乏显式深度信息而难以生成准确3D检测预测的问题，缓解深度估计的模糊性。
- Method: 提出基于光线的融合方法，利用协作车辆的射线占用信息来过滤冗余和误检预测。
- Result: 综合实验表明该方法持续优于现有最先进模型，显著提升了协作视觉感知性能。
- Conclusion: RayFusion方法有效提升了纯摄像头协作感知系统的检测性能，为自动驾驶视觉感知提供了新的解决方案。


### [50] [RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans](https://arxiv.org/abs/2510.08052)
*Bheeshm Sharma,Karthikeyan Jaganathan,Balamurugan Palaniappan*

Main category: cs.CV

TL;DR: 提出RASALoRE框架，通过两阶段弱监督学习方法实现脑部MRI异常检测，使用区域感知空间注意力机制和基于位置的随机嵌入，在多个数据集上达到SOTA性能且参数少于800万。

- Motivation: 解决脑部MRI扫描中弱监督异常检测的挑战，当缺乏精确像素级标注而只有切片级弱标签可用时，需要快速准确地检测脑部异常。
- Method: 两阶段框架：第一阶段使用判别式双提示调优机制生成高质量伪弱掩码；第二阶段采用带有区域感知空间注意力机制的分割网络，依赖基于位置的固定随机嵌入来聚焦异常区域。
- Result: 在BraTS20、BraTS21、BraTS23和MSD数据集上的广泛评估显示性能显著提升，同时计算复杂度大幅降低，参数少于800万。
- Conclusion: RASALoRE框架在弱监督脑部MRI异常检测中实现了最先进的性能，显著优于现有方法，同时保持了较低的计算复杂度。


### [51] [RetouchLLM: Training-free White-box Image Retouching](https://arxiv.org/abs/2510.08054)
*Moon Ye-Bin,Roy Miles,Tae-Hyun Oh,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

TL;DR: RetouchLLM是一个无需训练的白盒图像润色系统，通过代码生成实现可解释的高分辨率图像润色，模拟人类多步骤润色过程。

- Motivation: 现有基于学习的方法需要大规模配对数据且作为黑盒运行，使得润色过程不透明，难以适应用户或图像特定的多样化调整需求。
- Method: 包含两个主要模块：视觉评论器识别输入图像与参考图像之间的差异，代码生成器产生可执行代码。系统以渐进方式增强图像，支持探索不同的调整路径。
- Result: 实验表明该方法能够很好地泛化到不同的润色风格，基于自然语言的用户交互实现了可解释和可控的调整，适应用户意图。
- Conclusion: RetouchLLM提供了一个无需训练数据、可解释的代码驱动图像润色框架，通过白盒方法实现了多样化和用户定制的图像调整。


### [52] [A class-driven hierarchical ResNet for classification of multispectral remote sensing images](https://arxiv.org/abs/2510.08060)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种多时序类别驱动的分层ResNet网络，用于多光谱图像时间序列的多层次语义分类，通过引入分层惩罚映射和模块化架构来提高分类准确性和适应性。

- Motivation: 为了解决多光谱图像时间序列分类中不同语义层次类别的区分问题，特别是在训练样本有限的情况下，需要一种能够有效利用类别层次结构并提高分类一致性的方法。
- Method: 修改ResNet架构，引入额外分支进行不同层次分类，使用分层惩罚映射避免不一致的层次转换，利用类别层次标签分层训练网络，先训练通用类别再训练具体类别。
- Result: 在亚马逊森林的Sentinel 2图像数据集上实验表明，该方法能够有效泛化到不同层次级别，在微类别级别实现准确分类，并更好地表示少数类别。
- Conclusion: 提出的分层模块化网络具有内在适应能力，通过微调可以适应新任务，在多层次语义分类中表现出色，特别是在处理有限训练样本的情况下。


### [53] [Towards Real-World Deepfake Detection: A Diverse In-the-wild Dataset of Forgery Faces](https://arxiv.org/abs/2510.08067)
*Junyu Shi,Minghui Li,Junguo Zuo,Zhifei Yu,Yipeng Lin,Shengshan Hu,Ziqi Zhou,Yechao Zhang,Wei Wan,Yinzhe Xu,Leo Yu Zhang*

Main category: cs.CV

TL;DR: RedFace是一个面向真实世界的深度伪造人脸数据集，包含6万张伪造图像和1000个篡改视频，使用9个商业在线平台集成最新深度伪造技术，填补了学术评估与现实需求之间的差距。

- Motivation: 现有深度伪造检测的学术评估和基准测试在特异性、深度伪造多样性和操作技术方面存在不足，无法有效应用于现实世界场景。
- Method: 利用9个商业在线平台生成深度伪造内容，采用定制算法合成多样化的深度伪造，模拟真实世界黑盒场景。
- Result: 在RedFace上的广泛实验验证了现有深度伪造检测方案在现实应用中的实用性有限，相比传统数据集对检测性能产生显著影响。
- Conclusion: RedFace数据集通过模拟真实世界深度伪造技术，揭示了现有检测方法的局限性，为改进深度伪造检测提供了重要基准。


### [54] [Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection](https://arxiv.org/abs/2510.08073)
*Shuhai Zhang,ZiHao Lian,Jiahao Yang,Daiyuan Li,Guoxuan Pang,Feng Liu,Bo Han,Shutao Li,Mingkui Tan*

Main category: cs.CV

TL;DR: 提出基于概率流守恒原理的物理驱动AI生成视频检测方法NSG-VD，通过归一化时空梯度特征和最大均值差异计算来识别违反物理规律的AI视频

- Motivation: AI生成视频已达到近乎完美的视觉真实感，迫切需要可靠的检测机制，但现有方法面临高维时空动态建模和识别违反物理规律的细微异常的挑战
- Method: 提出归一化时空梯度统计量来量化空间概率梯度与时间密度变化的比例，利用预训练扩散模型开发NSG估计器，通过空间梯度近似和运动感知时间建模，最后基于NSG特征的最大均值差异进行检测
- Result: 在广泛实验中，NSG-VD在召回率上优于最先进基线16.00%，在F1分数上优于10.75%，验证了其优越性能
- Conclusion: NSG-VD通过物理驱动的检测范式有效识别AI生成视频，证明了生成视频由于分布偏移而表现出放大的差异


### [55] [DarkHash: A Data-Free Backdoor Attack Against Deep Hashing](https://arxiv.org/abs/2510.08094)
*Ziqi Zhou,Menghao Deng,Yufei Song,Hangtao Zhang,Wei Wan,Shengshan Hu,Minghui Li,Leo Yu Zhang,Dezhong Yao*

Main category: cs.CV

TL;DR: DarkHash是首个针对深度哈希的无数据后门攻击方法，通过双语义指导的阴影后门攻击框架，在不访问训练数据的情况下植入后门，同时保持原始检索精度。

- Motivation: 现实世界中由于隐私保护和知识产权问题，获取训练数据往往被禁止，因此需要在无法访问训练数据的情况下对深度哈希模型进行后门攻击，同时保持原始任务的检索准确性。
- Method: 设计双语义指导的阴影后门攻击框架，仅使用替代数据集微调受害者模型的特定层来嵌入后门功能；利用样本与其邻居的关系，通过拓扑对齐损失优化个体和邻近中毒样本朝向目标样本。
- Result: 在四个图像数据集、五种模型架构和两种哈希方法上的实验结果表明，DarkHash具有高有效性，优于现有最先进的后门攻击方法；防御实验显示DarkHash能够抵御现有主流后门防御方法。
- Conclusion: DarkHash成功实现了无需训练数据的深度哈希后门攻击，在保持原始检索精度的同时增强了攻击能力，且能抵御现有防御方法。


### [56] [Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting](https://arxiv.org/abs/2510.08096)
*Ankit Gahlawat,Anirban Mukherjee,Dinesh Babu Jayagopi*

Main category: cs.CV

TL;DR: 提出了一种利用3D高斯溅射从多视角预测中生成精确分割掩码的标签细化流程，显著提升了极端视角下的人脸解析精度

- Motivation: 极端视角下的人脸解析因缺乏标注数据而面临挑战，手动标注成本高昂且难以规模化
- Method: 联合拟合两个3DGS模型（RGB图像和初始分割图），通过共享几何实现多视角一致性，合成姿态多样的训练数据
- Result: 在挑战性头部姿态上显著提升了解析精度，同时在标准视角上保持强性能，无需真实3D标注且仅需少量初始图像
- Conclusion: 该方法为提升真实场景中人脸解析鲁棒性提供了可扩展且有效的解决方案


### [57] [Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation](https://arxiv.org/abs/2510.08116)
*Eirik A. Østmo,Kristoffer K. Wickstrøm,Keyur Radiya,Michael C. Kampffmeyer,Karl Øyvind Mikalsen,Robert Jenssen*

Main category: cs.CV

TL;DR: 本文提出了一种针对CT图像的特定增强技术——随机窗宽调整，通过利用CT图像中Hounsfield单位的强度分布，解决了传统自然图像增强方法在CT图像中导致伪影和泛化能力差的问题。

- Motivation: 在医学CT图像分割中，深度学习方法需要图像增强来提升泛化能力，但传统的自然图像增强方法忽视了CT图像中Hounsfield单位的物理意义，导致伪影和泛化性能下降。
- Method: 提出了随机窗宽调整增强技术，该方法利用CT图像中Hounsfield单位的强度分布特性，增强模型对对比度变化的鲁棒性。
- Result: 该方法在多个数据集上进行了消融实验和分析，在具有较差对比度或时相的挑战性图像上显著提高了模型性能，并优于现有的最先进方法。
- Conclusion: 随机窗宽调整是一种有效的CT特定增强技术，能够显著提升肝脏肿瘤分割任务的模型性能，特别是在对比度不佳的情况下。


### [58] [Real-Time Motion-Controllable Autoregressive Video Diffusion](https://arxiv.org/abs/2510.08131)
*Kesen Zhao,Jiaxin Shi,Beier Zhu,Junbao Zhou,Xiaolong Shen,Yuan Zhou,Qianru Sun,Hanwang Zhang*

Main category: cs.CV

TL;DR: AR-Drag是一个基于强化学习的少步自回归视频扩散模型，用于实时图像到视频生成，支持多样化运动控制，显著降低了延迟并提高了视觉质量。

- Motivation: 解决实时运动可控视频生成的挑战，包括双向扩散模型的固有延迟问题、现有自回归方法在少步生成中的质量下降和运动伪影问题。
- Method: 首先微调基础图像到视频模型以支持基本运动控制，然后通过基于轨迹的奖励模型进行强化学习改进。通过自展开机制保持马尔可夫性质，并在去噪步骤中选择性引入随机性来加速训练。
- Result: AR-Drag在视觉保真度和运动对齐精度方面表现出色，与最先进的运动可控视频扩散模型相比显著降低了延迟，仅使用13亿参数。
- Conclusion: AR-Drag是首个RL增强的少步自回归视频扩散模型，成功实现了实时图像到视频生成与多样化运动控制，在质量和效率方面都有显著提升。


### [59] [Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement](https://arxiv.org/abs/2510.08138)
*Chengzhi Li,Heyan Huang,Ping Jian,Zhen Yang,Yaning Tian*

Main category: cs.CV

TL;DR: 提出TCAS方法增强视频语言模型的时序逻辑一致性，通过改进跨模态注意力头的时间分辨能力来解决模型对重述问题产生自相矛盾回答的问题。

- Motivation: 视频语言模型在处理重述问题时会产生自相矛盾的回答，严重影响可靠性，但这种现象的根本原因尚未得到充分探索。
- Method: 采用可解释性驱动方法分析问题，提出TCAS方法构建基于注意力差异的增强目标，提升模型的时间分辨能力。
- Result: 实验表明TCAS显著提升了视频语言模型的时序逻辑一致性，同时提高了通用视频时序定位任务的性能。
- Conclusion: 时序逻辑一致性是视频时序理解的瓶颈，通过增强一致性可以显著推动视频时序理解的进展。


### [60] [UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution](https://arxiv.org/abs/2510.08143)
*Shian Du,Menghan Xia,Chang Liu,Quande Liu,Xintao Wang,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: UniMMVSR是首个统一的多模态视频超分辨率框架，能够整合文本、图像和视频等多种生成条件，显著提升视频生成质量和对多模态条件的符合度。

- Motivation: 现有级联视频超分辨率方法主要局限于文本到视频任务，未能充分利用文本之外的多模态生成条件，而这些条件对于确保多模态视频生成的保真度至关重要。
- Method: 在潜在视频扩散模型中探索条件注入策略、训练方案和数据混合技术，设计不同的数据构建和条件利用方法，使模型能够精确利用所有条件类型。
- Result: UniMMVSR显著优于现有方法，生成具有更优细节和更高多模态条件符合度的视频，并能与基础模型结合实现4K视频的多模态引导生成。
- Conclusion: 该框架成功解决了多模态条件利用的挑战，为高质量视频生成开辟了新途径，实现了之前无法达到的4K多模态视频生成能力。


### [61] [Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing](https://arxiv.org/abs/2510.08157)
*Zhentao Zou,Zhengrong Yue,Kunpeng Du,Binlei Bao,Hanting Li,Haizhen Xie,Guozheng Xu,Yue Zhou,Yali Wang,Jie Hu,Xue Jiang,Xinghao Chen*

Main category: cs.CV

TL;DR: 提出了MURE框架，通过多模态链式推理将图像编辑从纯文本推理转向文本-视觉交替推理，结合MMDC机制减少幻觉，在三个基准测试中显著提升编辑效果。

- Motivation: 现有方法在处理复杂对象交叉和细粒度空间关系时缺乏显式推理过程，纯文本CoT或坐标增强CoT无法充分表示复杂视觉布局和引导像素级细节生成。
- Method: 使用原生多模态交替文本-图像CoT，生成包含文本描述和对应视觉提示（如位置掩码）的逐步推理链；引入MMDC推理范式，通过奖励模型的深度置信度修剪低质量分支。
- Result: 在三个图像编辑基准测试中取得显著改进，能够将复杂编辑任务分解为相互依赖的子任务，在每个阶段实现更高精度。
- Conclusion: MURE框架通过多模态链式推理有效解决了复杂图像编辑中的推理不足问题，生成的编辑结果具有高保真度。


### [62] [Robust Canonicalization through Bootstrapped Data Re-Alignment](https://arxiv.org/abs/2510.08178)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: 提出一种自举算法，通过迭代重新对齐训练样本来改进细粒度视觉分类中的几何偏差处理，无需依赖对齐假设或限制模型表达能力。

- Motivation: 细粒度视觉分类任务需要处理几何偏差和噪声，现有方法依赖数据增强或等变架构，但前者需要强大模型，后者限制表达能力。规范化方法假设训练数据对齐，但现实数据集无法满足这一假设。
- Method: 提出自举算法，通过迭代重新对齐训练样本，逐步减少方差并恢复对齐假设。该算法适用于任意紧致群，并建立了收敛保证。
- Result: 在四个细粒度视觉分类基准测试中，该方法始终优于等变和规范化基线方法，与数据增强方法性能相当。
- Conclusion: 自举算法提供了一种有效的几何偏差处理方法，无需依赖对齐假设或限制模型表达能力，在细粒度视觉分类任务中表现出色。


### [63] [InstructUDrag: Joint Text Instructions and Object Dragging for Interactive Image Editing](https://arxiv.org/abs/2510.08181)
*Haoran Yu,Yi Shi*

Main category: cs.CV

TL;DR: 提出InstructUDrag框架，结合文本指令和物体拖拽，实现同时进行物体拖拽和基于文本的图像编辑，解决了现有方法在精确定位和静态重定位方面的局限性。

- Motivation: 文本到图像扩散模型在图像编辑方面潜力巨大，但现有方法各有局限：基于文本的方法难以精确定位物体，物体拖拽方法仅限于静态重定位。需要一种能同时实现精确物体定位和语义控制的编辑方法。
- Method: 将物体拖拽视为图像重建过程，分为两个协同分支：移动重建分支使用基于能量的梯度引导精确移动物体，细化交叉注意力图提高重定位精度；文本驱动编辑分支与重建分支共享梯度信号，确保一致变换并允许对物体属性进行细粒度控制。还采用DDPM反演并在噪声图中注入先验信息以保持移动物体的结构。
- Result: 大量实验证明InstructUDrag能够实现灵活、高保真度的图像编辑，在物体重定位精度和图像内容语义控制方面都表现出色。
- Conclusion: InstructUDrag成功结合了文本指令和物体拖拽的优势，为图像编辑提供了既精确又灵活的解决方案，实现了同时的物体重定位和语义属性控制。


### [64] [Fine-grained text-driven dual-human motion generation via dynamic hierarchical interaction](https://arxiv.org/abs/2510.08260)
*Mu Li,Yin Wang,Zhiying Leng,Jiapeng Liu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: FineDual是一个三阶段的双人运动生成方法，通过建模动态层次化交互来生成细粒度的双人运动，在个体、个体间和整体三个层次上分别处理运动特征。

- Motivation: 现有方法在建模人类交互时通常忽略距离和层次结构，而人类交互本质上是动态和层次化的。需要一种能够捕捉动态距离变化和从个体到整体层次结构的方法。
- Method: 三阶段方法：1) 自学习阶段：使用大语言模型将双人整体文本分解为个体文本，在个体层面对齐文本和运动特征；2) 自适应调整阶段：预测交互距离，通过交互感知图网络在个体间层面动态建模交互；3) 教师引导精炼阶段：使用整体文本特征作为指导，在整体层面精炼运动特征。
- Result: 在双人运动数据集上的广泛定量和定性评估表明，FineDual优于现有方法，能够有效建模动态层次化的人类交互。
- Conclusion: FineDual通过建模动态层次化交互，能够生成细粒度和高质量的双人运动，解决了现有方法忽略距离和层次结构的问题。


### [65] [Adaptive Gradient Calibration for Single-Positive Multi-Label Learning in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2510.08269)
*Chenying Liu,Gianmarco Perantoni,Lorenzo Bruzzone,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了一种用于遥感图像单正多标签学习（SPML）的自适应梯度校准（AdaGC）框架，通过梯度校准机制、Mixup和双EMA模块来缓解监督模糊性问题，在多个基准数据集上达到最先进性能。

- Motivation: 遥感图像的多标签分类比单标签分类能提供更全面的语义理解，但获取完整标注成本高昂。单正多标签学习（SPML）作为一种实用替代方案，每个图像只标注一个相关标签，但存在显著的监督模糊性问题。
- Method: 提出AdaGC框架，包含梯度校准（GC）机制、Mixup技术和双指数移动平均（EMA）模块，用于生成鲁棒的伪标签。通过基于训练动态的自适应触发机制，在初始预热阶段后启动GC，有效缓解对标签噪声的过拟合。
- Result: 在两个基准遥感数据集上的广泛实验表明，AdaGC在两种不同的标签噪声类型下均达到了最先进的性能，并在多样化设置中保持了强大的鲁棒性。
- Conclusion: AdaGC为遥感图像的单正多标签学习提供了一个有效且通用的解决方案，成功解决了监督模糊性问题，在多个数据集上表现出优越性能。


### [66] [One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting](https://arxiv.org/abs/2510.08273)
*Haipeng Liu,Yang Wang,Meng Wang*

Main category: cs.CV

TL;DR: 提出NTN-Diff方法，通过频率感知的扩散模型解决文本引导图像修复中的语义一致性和未掩码区域保护问题，将去噪过程分为早期和晚期阶段，分别处理中低频带。

- Motivation: 现有方法无法同时解决文本引导图像修复中的两个关键挑战：保护未掩码区域和实现掩码与未掩码区域的语义一致性，这源于不同频率带在去噪过程中对文本提示的不同鲁棒性。
- Method: 提出空文本空频率感知扩散模型(NTN-Diff)，将去噪过程分为早期(高级噪声)和晚期(低级噪声)阶段，在早期阶段通过文本引导去噪稳定中频带，然后引导空文本去噪处理低频带，最后在晚期阶段进行文本引导去噪。
- Result: 大量实验验证了NTN-Diff在文本引导扩散模型中的优越性，能够同时实现跨掩码和未掩码区域的语义一致性，并保护未掩码区域。
- Conclusion: NTN-Diff通过频率分解和分阶段去噪策略，有效解决了文本引导图像修复中的核心挑战，在保持未掩码区域的同时实现了语义一致性。


### [67] [A Multimodal Depth-Aware Method For Embodied Reference Understanding](https://arxiv.org/abs/2510.08278)
*Fevziye Irem Eyiokur,Dogucan Yaman,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 提出了一种新的具身参考理解框架，通过结合LLM数据增强、深度图模态和深度感知决策模块，在复杂环境中更准确地识别目标物体。

- Motivation: 解决现有开放词汇目标检测方法在存在多个候选物体的模糊场景中容易失败的问题。
- Method: 联合利用LLM数据增强、深度图模态和深度感知决策模块，实现语言和具身线索的鲁棒集成。
- Result: 在两个数据集上的实验结果表明，该方法显著优于现有基线，实现了更准确可靠的参考检测。
- Conclusion: 提出的ERU框架能够有效处理复杂或杂乱环境中的歧义问题，提升具身参考理解的性能。


### [68] [Learning Neural Exposure Fields for View Synthesis](https://arxiv.org/abs/2510.08279)
*Michael Niemeyer,Fabian Manhardt,Marie-Julie Rakotosaona,Michael Oechsle,Christina Tsalicoglou,Keisuke Tateno,Jonathan T. Barron,Federico Tombari*

Main category: cs.CV

TL;DR: 提出Neural Exposure Fields (NExF)，一种通过3D点级曝光预测来鲁棒重建高动态范围场景的新方法，在挑战性真实数据上实现高质量视图合成。

- Motivation: 现有神经场景表示在处理包含强曝光变化的数据时性能下降，特别是在室内外混合场景或带窗户的房间中。
- Method: 学习预测每个3D点最优曝光值的神经场，通过新型神经条件机制联合优化场景表示和曝光场。
- Result: 训练速度比先前工作更快，在多个基准测试中达到最先进性能，比最佳基线提升超过55%。
- Conclusion: 该方法能够准确合成高动态范围场景的视图，无需后处理步骤或多曝光捕获。


### [69] [LTCA: Long-range Temporal Context Attention for Referring Video Object Segmentation](https://arxiv.org/abs/2510.08305)
*Cilin Yan,Jingyun Wang,Guoliang Kang*

Main category: cs.CV

TL;DR: 提出了一种有效的长程时序上下文注意力机制（LTCA），通过稀疏局部注意力堆叠和全局查询设计来平衡局部性和全局性，在四个参考视频分割基准上实现了新的最先进性能。

- Motivation: 解决参考视频分割中长程时序上下文信息提取的挑战，现有方法在局部性和全局性之间平衡不佳，且计算复杂度随视频长度增加而显著增加。
- Method: 提出LTCA机制：1）堆叠稀疏局部注意力（扩张窗口注意力）平衡局部性和全局性；2）每个查询从全局池中随机选择少量键进行注意力；3）设计全局查询与其他所有查询交互以直接编码全局上下文信息。
- Result: 在四个参考视频分割基准上实现了新的最先进性能，在MeViS数据集上分别提升了11.3%和8.3%。
- Conclusion: LTCA机制有效平衡了局部性和全局性，显著提升了参考视频分割性能，同时控制了计算复杂度。


### [70] [Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge](https://arxiv.org/abs/2510.08316)
*Yu Huang,Zelin Peng,Changsong Wen,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: 提出了一种基于语义基础学习的3D功能分割框架，通过跨模态亲和力转移(CMAT)将2D视觉基础模型的丰富语义知识迁移到3D领域，并设计了跨模态功能分割变换器(CAST)来生成精确的提示感知分割图。

- Motivation: 现有的3D功能分割方法通常使用点云编码器作为通用特征提取器，但忽略了3D数据固有的挑战（如稀疏性、噪声和几何模糊性），导致学习到的3D特征缺乏清晰且语义一致的功能边界。
- Method: 1. 跨模态亲和力转移(CMAT)：一种预训练策略，将3D编码器与提升的2D语义对齐，联合优化重建、亲和力和多样性；2. 跨模态功能分割变换器(CAST)：集成多模态提示与CMAT预训练特征，生成精确的提示感知分割图。
- Result: 在标准基准测试上的广泛实验表明，该框架在3D功能分割任务上取得了新的最先进结果。
- Conclusion: 提出的语义基础学习范式通过将2D视觉基础模型的丰富语义知识转移到3D领域，有效解决了3D功能分割中的语义边界模糊问题，显著提升了分割性能。


### [71] [LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation](https://arxiv.org/abs/2510.08318)
*Yushi Huang,Xingtong Ge,Ruihao Gong,Chengtao Lv,Jun Zhang*

Main category: cs.CV

TL;DR: LinVideo是一个高效的数据无关后训练框架，通过将自注意力模块替换为线性注意力来加速视频扩散模型，同时保持生成质量。

- Motivation: 视频扩散模型的计算成本随序列长度呈二次方增长，而线性注意力的表达能力有限且需要昂贵的预训练，因此需要一种无需数据且能保持性能的后训练加速方法。
- Method: 提出选择性转移方法，将层选择视为二分类问题，自动渐进地将层转换为线性注意力；引入任意时间分布匹配目标，在采样轨迹的任何时间步对齐样本分布。
- Result: 实验显示该方法实现了1.25-2.00倍加速，同时保持生成质量；4步蒸馏模型进一步实现了15.92倍延迟降低，视觉质量下降最小。
- Conclusion: LinVideo框架有效解决了视频扩散模型的计算效率问题，通过选择性层转换和分布匹配目标实现了显著的加速效果。


### [72] [Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception](https://arxiv.org/abs/2510.08352)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: 提出了首个专注于交通场景感知的视觉问答基准DTPQA，包含距离标注，评估小型视觉语言模型在自动驾驶感知任务中的表现。

- Motivation: 自动驾驶系统需要可靠的感知能力，特别是对远距离物体的感知，但现有视觉语言模型在交通场景中的感知性能尚未得到充分评估。
- Method: 创建DTPQA基准，仅包含基于感知的问题并排除推理问题，评估多个最先进的小型视觉语言模型在该基准上的表现。
- Result: 最佳小型视觉语言模型平均准确率约60%，远低于人类约85%的表现，且在左右区分等特定感知任务上表现较差。
- Conclusion: 当前小型视觉语言模型在交通场景感知任务中表现不足，需要进一步改进才能满足自动驾驶系统的安全要求。


### [73] [SPICE: Simple and Practical Image Clarification and Enhancement](https://arxiv.org/abs/2510.08358)
*Alexander Belyaev,Pierre-Alain Fayolle,Michael Cohen*

Main category: cs.CV

TL;DR: 提出一种简单高效的图像增强方法，用于低光照图像增强和雾霾图像（包括雾天、沙尘、水下图像）的清晰化处理。

- Motivation: 解决低光照和雾霾条件下图像质量下降的问题，提供一种简单实用的图像增强方案。
- Method: 构建图像滤波器模拟低光照或雾霾条件，并推导近似反向滤波器以最小化增强图像中的失真。
- Result: 实验结果表明该方法在处理极暗图像和增强雾霾图像方面具有高度竞争力，甚至优于现有最先进技术。
- Conclusion: 该方法的主要优势在于其简单性，仅需几行MATLAB代码即可实现。


### [74] [Hyperspectral data augmentation with transformer-based diffusion models](https://arxiv.org/abs/2510.08363)
*Mattia Ferrari,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种基于引导扩散模型的数据增强技术，结合轻量级Transformer网络和优化的损失函数，用于解决高光谱图像小样本分类中的过拟合问题。

- Motivation: 新一代高光谱卫星传感器和深度学习方法虽然提升了土地覆盖分类能力，但在小样本数据集上训练时容易过拟合，需要有效的数据增强技术。
- Method: 使用引导扩散模型进行数据增强，结合轻量级Transformer网络、改进的加权损失函数和优化的余弦方差调度器，实现在小数据集上的快速有效训练。
- Result: 在PRISMA卫星获取的10种森林类型高光谱图像分类任务中，该方法在平均准确率和加权平均准确率上均优于其他数据增强技术，且训练过程稳定。
- Conclusion: 该方法有效解决了深度生成模型在实际应用中训练不稳定的常见问题，为小样本高光谱图像分类提供了可靠的数据增强解决方案。


### [75] [UniVideo: Unified Understanding, Generation, and Editing for Videos](https://arxiv.org/abs/2510.08377)
*Cong Wei,Quande Liu,Zixuan Ye,Qiulin Wang,Xintao Wang,Pengfei Wan,Kun Gai,Wenhu Chen*

Main category: cs.CV

TL;DR: UniVideo是一个统一的多模态视频生成和编辑框架，采用双流设计结合MLLM和MMDiT，支持多种视频任务并在单一多模态指令范式下联合训练。

- Motivation: 现有的统一多模态模型主要局限于图像领域，需要扩展到视频领域以实现更广泛的多模态内容生成和编辑能力。
- Method: 采用双流设计：多模态大语言模型(MLLM)用于理解指令，多模态DiT(MMDiT)用于视频生成，通过联合训练统一多种视频任务。
- Result: 在文本/图像到视频生成、上下文视频生成和编辑等任务上达到或超越最先进的特定任务基线，支持任务组合和跨域泛化。
- Conclusion: UniVideo展示了统一视频建模的可行性，支持任务组合和零样本泛化，为未来视频生成研究提供了新方向。


### [76] [Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning](https://arxiv.org/abs/2510.08385)
*Sofia Kirsanova,Yao-Yi Chiang,Weiwei Duan*

Main category: cs.CV

TL;DR: 提出一种结合LayoutLMv3和GPT-4o的方法，通过边界框预测检测和链接历史地图图例中的符号与描述，在结构化JSON提示下达到88% F-1和85% IoU。

- Motivation: 历史地图图例对于解释制图符号至关重要，但其不一致的布局和非结构化格式使得自动提取具有挑战性。现有方法主要关注分割或通用OCR，很少有方法能有效以结构化方式匹配图例符号与对应描述。
- Method: 结合LayoutLMv3进行布局检测，使用GPT-4o通过上下文学习来检测和链接图例项及其描述，基于边界框预测。
- Result: 实验显示，使用结构化JSON提示的GPT-4优于基线方法，达到88% F-1分数和85% IoU，并揭示了提示设计、示例数量和布局对齐对性能的影响。
- Conclusion: 该方法支持可扩展的、布局感知的图例解析，并提高了各种视觉风格历史地图的索引和可搜索性。


### [77] [Robust Source-Free Domain Adaptation for Medical Image Segmentation based on Curriculum Learning](https://arxiv.org/abs/2510.08393)
*Ziqi Zhang,Yuexiang Li,Yawen Huang,Nanjun He,Tao Xu,Liwei Lin,Yefeng Zheng,Shaoxin Li,Feiyue Huang*

Main category: cs.CV

TL;DR: 提出了一种基于课程学习的源自由域自适应框架LFC，通过从易到难和从源到目标的课程设计，在无需源数据的情况下实现医学图像的域自适应。

- Motivation: 解决医学图像数据隐私和安全问题，当前源自由域自适应方法主要关注伪标签细化而忽略了学习过程，渐进式学习过程有助于知识迁移。
- Method: 设计了两种课程：从易到难课程让框架从简单样本开始学习，逐步增加样本难度；从源到目标课程确保模型从源域到目标域的平稳迁移。
- Result: 在眼底分割和息肉分割的跨域数据集上评估，实验结果表明该方法超越了现有方法，达到了新的最先进水平。
- Conclusion: 提出的LFC框架通过课程学习策略有效解决了源自由域自适应问题，在医学图像分割任务中表现出色。


### [78] [VideoVerse: How Far is Your T2V Generator from a World Model?](https://arxiv.org/abs/2510.08398)
*Zeqing Wang,Xinyu Wei,Bairui Li,Zhen Guo,Jinrui Zhang,Hongyang Wei,Keze Wang,Lei Zhang*

Main category: cs.CV

TL;DR: VideoVerse是一个新的文本到视频生成基准测试，专注于评估T2V模型对现实世界中复杂时间因果关系和世界知识的理解能力，包含300个精心策划的提示和793个二元评估问题。

- Motivation: 现有T2V基准测试不足以评估最先进模型，缺乏事件级时间因果关系评估和系统性的世界知识评估，而这些对于构建世界模型至关重要。
- Method: 收集跨领域代表性视频，提取具有时间因果关系的事件级描述，由独立标注者重写为文本到视频提示，设计包含10个维度的二元评估问题，使用现代视觉语言模型开发人类偏好对齐的QA评估流程。
- Result: 构建了包含300个提示、815个事件和793个评估问题的VideoVerse基准，对开源和闭源T2V模型进行了系统评估。
- Conclusion: VideoVerse为评估T2V模型是否能够理解复杂时间因果关系和世界知识提供了全面基准，有助于分析当前T2V生成器与世界模型的距离。


### [79] [Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency](https://arxiv.org/abs/2510.08431)
*Kaiwen Zheng,Yuji Wang,Qianli Ma,Huayu Chen,Jintao Zhang,Yogesh Balaji,Jianfei Chen,Ming-Yu Liu,Jun Zhu,Qinsheng Zhang*

Main category: cs.CV

TL;DR: 本文首次将连续时间一致性蒸馏扩展到通用应用级图像和视频扩散模型，提出了得分正则化连续时间一致性模型(rCM)，在保持高生成多样性的同时显著提升视觉质量，可在1-4步内生成高保真样本，加速扩散采样15-50倍。

- Motivation: 尽管连续时间一致性模型(sCM)在学术规模扩散加速方面具有理论优势和实证效果，但其在大规模文本到图像和视频任务中的适用性仍不明确，主要面临Jacobian-vector product计算的基础设施挑战和标准评估基准的局限性。
- Method: 开发并行兼容的FlashAttention-2 JVP内核，支持超过100亿参数模型和高维视频任务训练；提出得分正则化连续时间一致性模型(rCM)，将得分蒸馏作为长跳跃正则器，结合sCM的前向散度和反向散度的模式寻求特性。
- Result: 在高达140亿参数的Cosmos-Predict2、Wan2.1等大规模模型和5秒视频任务上验证，rCM在质量指标上匹配或超越最先进的蒸馏方法DMD2，同时在多样性方面具有显著优势，无需GAN调优或大量超参数搜索。
- Conclusion: rCM作为一个实用且理论基础的框架，为推进大规模扩散蒸馏提供了有效解决方案，可在仅1-4步内生成高保真样本，实现15-50倍的扩散采样加速。


### [80] [Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning](https://arxiv.org/abs/2510.08442)
*Andrew Lee,Ian Chuang,Dechen Gao,Kai Fukazawa,Iman Soltani*

Main category: cs.CV

TL;DR: 提出了Gaze on the Prize框架，通过可学习的中央凹注意力机制引导视觉强化学习，利用回报差异识别任务相关特征，显著提高样本效率。

- Motivation: 视觉强化学习需要处理高维图像数据，但只有少量像素与任务相关，导致探索和计算资源浪费，学习效率低下且不稳定。
- Method: 引入可学习的中央凹注意力机制，通过基于回报差异的自监督信号进行训练。使用回报引导的对比学习，将相似视觉表示根据回报差异分组为正负样本，构建对比三元组训练注意力机制。
- Result: 在ManiSkill3基准测试的多个操作任务中，样本效率提升最高达2.4倍，能够解决基线方法无法学习的任务，且无需修改底层算法或超参数。
- Conclusion: 基于回报差异的注意力机制能够有效识别任务相关特征，显著提高视觉强化学习的样本效率和稳定性。


### [81] [Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction](https://arxiv.org/abs/2510.08449)
*Noor Islam S. Mohammad*

Main category: cs.CV

TL;DR: 提出了一个模块化空间图像处理框架，包含灰度量化、色彩增强、图像锐化、双向变换管道和几何特征提取等功能，在多个数据集上表现出稳健性能。

- Motivation: 开发一个综合性的图像处理框架，能够处理从基础灰度量化到复杂几何特征提取的多种任务，满足实时图像分析和计算机视觉应用的需求。
- Method: 采用模块化设计，包括灰度量化（8级离散化）、色彩增强（RGB和YCrCb空间的直方图均衡）、亮度调整（HSV值通道操作）、图像锐化（3×3卷积核）、双向变换管道（包含反锐化掩模、伽马校正和噪声放大）以及几何特征提取（Canny边缘检测、Hough线估计、Harris角点检测等）。
- Result: 双向变换管道在正向和反向过程中分别达到76.10%和74.80%的准确率；Hough线估计成功检测出51.50度的台球杆对齐角度；球杆隔离与真实图像的相似度达到81.87%；在多样化数据集上表现出稳健和确定性性能。
- Conclusion: 该模块化框架在图像处理任务中表现出色，具有实时图像分析和计算机视觉应用的潜力，为复杂视觉任务提供了有效的解决方案。


### [82] [Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools](https://arxiv.org/abs/2510.08480)
*Zhenlong Yuan,Xiangyan Qu,Chengxuan Qian,Rui Chen,Jing Tang,Lei Sun,Xiangxiang Chu,Dapeng Zhang,Yiwei Wang,Yujun Cai,Shuo Li*

Main category: cs.CV

TL;DR: Video-STAR是一个用于开放词汇动作识别的框架，通过子动作分解和工具增强强化学习来减少多模态大语言模型中的跨模态幻觉问题。

- Motivation: 多模态大语言模型在视觉-文本推理方面表现出色，但依赖文本中心先验限制了其在开放词汇场景中区分语义相似动作的能力。
- Method: 将动作分解为可区分的子动作进行细粒度匹配，同时动态调用领域特定工具进行跨模态交织，通过分层奖励平衡工具使用效率、子动作相关性和推理结构一致性。
- Result: 在HMDB-51、UCF-101、SSv2、Kinetics-400和Kinetics-600数据集上的广泛评估表明，该方法在区分细粒度动作和处理跨模态幻觉方面优于现有方法。
- Conclusion: Video-STAR框架通过子动作分解和工具增强强化学习，成功提升了开放词汇动作识别的性能，验证了其优秀的鲁棒性和泛化能力。


### [83] [The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping](https://arxiv.org/abs/2510.08482)
*Onur Keleş,Aslı Özyürek,Gerardo Ortega,Kadir Gökgö,Esam Ghaleb*

Main category: cs.CV

TL;DR: 提出了视觉象似性挑战基准，评估视觉语言模型在手语象似性任务上的表现，发现模型在音系形式预测、透明度和象似性评分方面均低于人类水平，但模型表现与人类判断存在相关性。

- Motivation: 手语中的象似性（语言形式与意义的相似性）为视觉基础提供了自然测试平台，旨在评估视觉语言模型从动态人体动作中恢复这种基本映射的能力。
- Method: 引入视觉象似性挑战基准，基于视频评估视觉语言模型在三个任务上的表现：音系手语形式预测、透明度和分级象似性评分，在零样本和少样本设置下测试了13个最先进的视觉语言模型。
- Result: 模型在音系形式预测中能恢复部分手形和位置细节但仍低于人类表现；在透明度任务上远低于人类基线；只有顶级模型与人类象似性评分呈中等相关。有趣的是，音系形式预测更强的模型与人类象似性判断相关性更好。
- Conclusion: 验证了这些诊断任务的有效性，表明需要以人为中心的信号和具身学习方法建模象似性，改进多模态模型中的视觉基础。


### [84] [InstructX: Towards Unified Visual Editing with MLLM Guidance](https://arxiv.org/abs/2510.08485)
*Chong Mou,Qichao Sun,Yanze Wu,Pengze Zhang,Xinghui Li,Fulong Ye,Songtao Zhao,Qian He*

Main category: cs.CV

TL;DR: InstructX是一个统一的图像和视频编辑框架，通过多模态大语言模型提升扩散模型的编辑性能，实现了图像训练数据到视频编辑能力的迁移，并在单一模型中统一处理多种编辑任务。

- Motivation: 当前多模态大语言模型在视觉理解方面表现出色，但如何将其与扩散模型有效整合以提升编辑性能，特别是在视频编辑等困难任务中，仍是一个开放挑战。
- Method: 提出了InstructX统一框架，通过全面研究MLLM与扩散模型的整合策略，分析图像和视频在统一建模中的协作与差异，利用图像数据训练实现无显式监督的视频编辑能力，并整合模态特定的MLLM特征。
- Result: 实验表明该方法能够处理广泛的图像和视频编辑任务，并达到最先进的性能水平。
- Conclusion: 图像数据训练可以产生视频编辑的涌现能力，缓解视频训练数据稀缺的限制；通过模态特定的MLLM特征，可以在单一模型中有效统一图像和视频编辑任务。


### [85] [MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration](https://arxiv.org/abs/2510.08508)
*Lu Liu,Chunlei Cai,Shaocheng Shen,Jianfeng Liang,Weimin Ouyang,Tianxiao Ye,Jian Mao,Huiyu Duan,Jiangchao Yao,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: MoA-VR是一个基于多智能体协作的视频修复系统，通过三个协调的智能体（退化识别、路由修复、修复质量评估）来模拟人类专家的推理和处理流程，有效处理复杂多样的视频退化问题。

- Motivation: 现实世界中的视频经常遭受复杂的退化问题（如噪声、压缩伪影、低光失真），现有方法需要专业手动选择专用模型或采用单一架构，难以泛化处理各种退化类型。
- Method: 构建大规模高分辨率视频退化识别基准，使用视觉语言模型(VLM)驱动退化识别器；引入基于大语言模型(LLM)的自适应路由器，通过观察工具使用模式自主学习有效修复策略；构建Res-VQ数据集并设计专用的VLM视频质量评估模型。
- Result: 大量实验表明，MoA-VR能有效处理多样化和复合退化问题，在客观指标和感知质量方面均优于现有基线方法。
- Conclusion: 这些结果突显了在多模态智能和模块化推理集成在通用视频修复系统中的潜力。


### [86] [To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models](https://arxiv.org/abs/2510.08510)
*Jiayun Luo,Wan-Cyuan Fan,Lyuyang Wang,Xiangteng He,Tanzila Rahman,Purang Abolmaesumi,Leonid Sigal*

Main category: cs.CV

TL;DR: 本文发现视觉变换器(ViT)中存在高范数视觉标记(ViT attention sinks)，这些标记包含高级语义信息但常被忽视。通过利用这些标记，可以显著提升大视觉语言模型的视觉推理能力。

- Motivation: 现有研究主要关注LLM中的注意力汇聚点，但很少研究ViT中的高范数视觉标记。这些标记对视觉理解和推理很重要，但在现有LVLM架构中常被忽略。
- Method: 提出定性和定量分析ViT汇聚标记中的信息，并开发了无需训练和基于训练的方法来更好地利用这些标记信息。
- Result: 通过显式利用ViT汇聚标记，在多种LVLM和视觉推理任务上实现了显著改进。
- Conclusion: ViT注意力汇聚点在增强视觉推理方面具有未开发的潜力，值得进一步研究和利用。


### [87] [Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression](https://arxiv.org/abs/2510.08512)
*Nikolaos Stathoulopoulos,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.CV

TL;DR: 提出基于语义场景图的点云深度压缩框架，在保持结构和语义保真度的同时实现高达98%的数据压缩率，支持多机器人位姿图优化和地图融合等下游应用

- Motivation: 解决3D点云数据在带宽受限和间歇性连接环境下传输效率低的问题，提升多智能体机器人系统的感知性能
- Method: 将点云分解为语义一致的分块，使用基于FiLM的语义感知编码器编码为紧凑潜在表示，通过基于折叠的解码器结合潜在特征和图节点属性进行重构
- Result: 在SemanticKITTI和nuScenes数据集上达到最先进的压缩率，数据量减少高达98%，同时保持结构和语义保真度
- Conclusion: 该框架为带宽受限环境下的点云传输提供了高效解决方案，支持下游机器人应用并保持与原始LiDAR扫描相当的精度


### [88] [SliceFine: The Universal Winning-Slice Hypothesis for Pretrained Networks](https://arxiv.org/abs/2510.08513)
*Md Kowsher,Ali O. Polat,Ehsan Mohammady Ardehaly,Mehrdad Salehi,Zia Ghiasi,Prasanth Murali,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出了一个理论框架解释为什么在预训练模型中对随机选择的小子网络进行微调足以适应下游任务，并基于此提出了SliceFine参数高效微调方法。

- Motivation: 为参数高效微调（PEFT）提供理论基础，解释为什么仅更新模型的一小部分参数就能达到良好性能，同时避免像适配器方法那样引入新参数。
- Method: 提出Universal Winning Slice Hypothesis理论框架，基于谱平衡和任务能量两个现象。开发SliceFine方法，仅更新原始权重中的选定切片，不引入新参数。
- Result: SliceFine在语言和视觉任务上达到最先进PEFT方法的性能，同时显著提升训练速度、内存效率和模型紧凑性。
- Conclusion: 该工作连接了理论与实践，为现有PEFT技术提供了理论依据的替代方案，证明了利用模型内在冗余性的有效性。


### [89] [FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control](https://arxiv.org/abs/2510.08527)
*Zhiyuan Zhang,Can Wang,Dongdong Chen,Jing Liao*

Main category: cs.CV

TL;DR: FlexTraj是一个用于图像到视频生成的框架，通过统一的基于点的运动表示实现灵活轨迹控制，支持密集和稀疏轨迹控制，具有快速收敛和高效推理的特点。

- Motivation: 现有的图像到视频生成方法在轨迹控制方面存在局限性，需要更灵活、多粒度的轨迹控制能力，同时保持时间一致性和鲁棒性。
- Method: 采用统一的点基运动表示（分割ID、轨迹ID、颜色通道），使用序列连接方案而非token连接或ControlNet，并采用退火训练策略逐步减少对完整监督和对齐条件的依赖。
- Result: 实验结果表明FlexTraj支持多粒度、对齐无关的轨迹控制，能够实现运动克隆、拖拽式图像到视频、运动插值、相机重定向、灵活动作控制和网格动画等多种应用。
- Conclusion: FlexTraj提供了一个高效且灵活的轨迹控制框架，在图像到视频生成中实现了强大的可控性，同时保持了训练和推理的效率。


### [90] [SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.08531)
*Hongxing Li,Dingming Li,Zixuan Wang,Yuchen Yan,Hang Wu,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 提出了一种渐进式训练框架SpatialLadder，通过三阶段训练（空间感知、空间理解、复杂推理）显著提升了视觉语言模型的空间推理能力，在多个基准测试中超越GPT-4o和Gemini-2.0-Flash。

- Motivation: 现有视觉语言模型在空间推理方面存在明显不足，主要原因是缺乏从感知到理解的层次化基础。需要建立渐进式的空间智能构建方法。
- Method: 构建SpatialLadder-26k多模态数据集，包含26,610个样本；设计三阶段渐进训练框架：1)通过物体定位建立空间感知，2)通过多维空间任务发展空间理解，3)通过可验证奖励的强化学习加强复杂推理。
- Result: SpatialLadder模型（3B参数）在空间推理基准测试中达到最先进性能，相比基础模型平均提升23.4%，超越GPT-4o 20.8%，超越Gemini-2.0-Flash 10.1%，在域外基准测试中提升7.2%。
- Conclusion: 从感知到推理的渐进式训练对于构建鲁棒的空间智能至关重要，该方法显著提升了模型的空间推理能力和泛化性能。


### [91] [Kontinuous Kontext: Continuous Strength Control for Instruction-based Image Editing](https://arxiv.org/abs/2510.08532)
*Rishubh Parihar,Or Patashnik,Daniil Ostashev,R. Venkatesh Babu,Daniel Cohen-Or,Kuan-Chieh Wang*

Main category: cs.CV

TL;DR: Kontinuous Kontext是一个指令驱动的图像编辑模型，通过引入标量编辑强度参数，实现了从无变化到完全实现的平滑连续编辑控制。

- Motivation: 传统的基于文本指令的图像编辑方法缺乏对编辑程度的精细控制，限制了用户对编辑效果的精确调整。
- Method: 扩展了最先进的图像编辑模型，添加标量编辑强度输入，训练轻量级投影网络将标量和编辑指令映射到模型的调制空间系数，使用生成模型合成多样化训练数据。
- Result: 模型能够在风格化、属性、材质、背景和形状变化等多种操作中提供从细微到强烈的精细编辑控制，无需特定属性训练。
- Conclusion: Kontinuous Kontext为指令驱动的图像编辑提供了统一的精细控制方法，实现了平滑连续的编辑强度调节。


### [92] [MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization](https://arxiv.org/abs/2510.08540)
*Xiangyu Zhao,Junming Lin,Tianhao Liang,Yifan Zhou,Wenhao Chai,Yuzhe Gu,Weiyun Wang,Kai Chen,Gen Luo,Wenwei Zhang,Junchi Yan,Hua Yang,Haodong Duan,Xue Yang*

Main category: cs.CV

TL;DR: 该论文提出了MM-HELIX基准测试来评估多模态大语言模型的长链反思推理能力，并开发了AHPO训练策略来提升模型在此类复杂推理任务上的表现。

- Motivation: 当前多模态大语言模型在数学和逻辑推理任务上表现出色，但其长链反思推理能力（解决复杂现实问题所需的关键能力）尚未得到充分探索。
- Method: 1) 构建MM-HELIX基准测试（1,260个样本，42个挑战性任务）；2) 开发Step-Elicited Response Generation管道创建MM-HELIX-100K数据集；3) 提出自适应混合策略优化（AHPO）训练策略，动态结合离线监督和在线优化。
- Result: 在Qwen2.5-VL-7B基线上，该方法在MM-HELIX基准上实现了+18.6%的准确率提升，在一般数学和逻辑任务上平均性能提升+5.7%。
- Conclusion: 研究表明多模态大语言模型的反思推理能力可以有效学习和泛化，为开发更强大的多模态模型铺平了道路。


### [93] [VideoNorms: Benchmarking Cultural Awareness of Video Language Models](https://arxiv.org/abs/2510.08543)
*Nikhil Reddy Varimalla,Yunfei Xu,Arkadiy Saakyan,Meng Fan Wang,Smaranda Muresan*

Main category: cs.CV

TL;DR: VideoNorms是一个包含1000多个视频片段与文化规范对的数据集，用于评估视频大语言模型的文化意识，涵盖中美文化背景。

- Motivation: 随着视频大语言模型在全球部署，需要评估其对相关文化背景的理解和基础认知能力。
- Method: 采用人机协作框架构建数据集，使用基于理论的提示生成候选标注，由训练有素的人类专家验证和修正。
- Result: 测试发现模型在规范违反识别、中国文化理解、非语言证据提供等方面表现较差，且在正式非幽默语境中表现不如人类。
- Conclusion: 研究强调了文化基础视频语言模型训练的必要性，该基准和框架开始解决这一差距。


### [94] [ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation](https://arxiv.org/abs/2510.08551)
*Guanghao Li,Kerui Ren,Linning Xu,Zhewen Zheng,Changjian Jiang,Xin Gao,Bo Dai,Jian Pu,Mulin Yu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: ARTDECO是一个统一的3D重建框架，结合了前馈模型的效率和SLAM管道的可靠性，通过3D基础模型进行位姿估计和点预测，使用高斯解码器将多尺度特征转换为结构化3D高斯，并采用分层高斯表示和LoD感知渲染策略。

- Motivation: 解决单目图像序列实时3D重建的挑战，现有方法存在权衡：逐场景优化精度高但计算昂贵，前馈基础模型实时但精度和鲁棒性不足。
- Method: 使用3D基础模型进行位姿估计和点预测，结合高斯解码器将多尺度特征转换为结构化3D高斯，设计分层高斯表示和LoD感知渲染策略。
- Result: 在8个不同的室内外基准测试中，ARTDECO实现了与SLAM相当的交互性能，与前馈系统相似的鲁棒性，以及接近逐场景优化的重建质量。
- Conclusion: ARTDECO为实时数字化真实世界环境提供了实用路径，兼具准确几何和高视觉保真度。


### [95] [Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation](https://arxiv.org/abs/2510.08553)
*Yunzhe Xu,Yiyuan Pan,Zhe Liu*

Main category: cs.CV

TL;DR: Memoir提出了一种基于想象力的记忆检索方法，用于视觉语言导航任务，通过世界模型想象未来状态作为查询，选择性检索环境观察和行为历史，显著提升了导航性能。

- Motivation: 现有记忆持久化视觉语言导航方法存在关键限制：缺乏有效的记忆访问机制，依赖整个记忆整合或固定范围查找，且主要存储环境观察而忽视编码决策策略的导航行为模式。
- Method: 1) 语言条件化世界模型想象未来状态，用于编码经验和生成检索查询；2) 混合视角级记忆，将观察和行为模式锚定到视角，实现混合检索；3) 经验增强导航模型，通过专用编码器整合检索知识。
- Result: 在10个不同测试场景的多样化记忆持久化VLN基准上，Memoir在所有场景中均取得显著改进：在IR2R上比最佳记忆持久化基线提升5.4% SPL，同时实现8.3倍训练加速和74%推理内存减少。
- Conclusion: 预测性检索环境和行为记忆能够实现更有效的导航，分析表明这种想象力引导范式具有巨大的改进空间（73.3% vs 93.4%上限）。


### [96] [VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning](https://arxiv.org/abs/2510.08555)
*Minghong Cai,Qiulin Wang,Zongli Ye,Wenze Liu,Quande Liu,Weicai Ye,Xintao Wang,Pengfei Wan,Kun Gai,Xiangyu Yue*

Main category: cs.CV

TL;DR: VideoCanvas是一个新颖的视频生成框架，通过混合条件策略解决因果VAE的时间模糊性问题，实现任意时空视频补全，统一了多种可控视频生成任务。

- Motivation: 现有视频生成任务（如图像到视频、修复、扩展和插值）各自独立，缺乏统一的控制范式。因果VAE将多帧压缩到单个潜在表示中，导致精确的帧级控制结构上困难。
- Method: 采用零参数增加的上下文条件范式，提出混合条件策略：空间控制通过零填充处理，时间对齐通过时间RoPE插值实现，为每个条件分配连续分数位置。
- Result: 在VideoCanvasBench基准测试中，VideoCanvas显著优于现有条件范式，在帧内保真度和帧间创造力方面都表现出色。
- Conclusion: VideoCanvas解决了因果VAE的时间模糊性问题，实现了像素帧感知控制，为灵活统一的视频生成建立了新的技术标准。


### [97] [SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models](https://arxiv.org/abs/2510.08559)
*Andong Deng,Taojiannan Yang,Shoubin Yu,Lincoln Spencer,Mohit Bansal,Chen Chen,Serena Yeung-Levy,Xiaohan Wang*

Main category: cs.CV

TL;DR: SciVideoBench是一个专门评估科学视频推理能力的基准测试，包含1000个来自前沿科学实验视频的多选题，覆盖25+学科领域，挑战模型的高阶认知能力。

- Motivation: 当前视频基准主要针对通用场景，依赖感知/识别且推理任务简单，无法有效评估先进的多模态认知技能，特别是在科学领域。
- Method: 构建包含1000个精心设计多选题的基准，源自前沿科学实验视频，覆盖25+专业学科，采用半自动验证系统确保质量。
- Result: 评估显示最先进的专有和开源LMMs（包括Gemini 2.5 Pro和Qwen2.5-VL）存在显著性能缺陷，表明视频推理能力有大幅提升空间。
- Conclusion: SciVideoBench填补了科学视频推理评估的关键空白，为LMMs未来发展提供了宝贵见解和明确方向，推动真正有能力多模态AI的发展。


### [98] [MultiCOIN: Multi-Modal COntrollable Video INbetweening](https://arxiv.org/abs/2510.08561)
*Maham Tanveer,Yang Zhou,Simon Niklaus,Ali Mahdavi Amiri,Hao Zhang,Krishna Kumar Singh,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 提出了一种支持多模态控制的视频插帧框架，通过将各种运动控制映射为统一的基于点的表示，并分离内容和运动控制分支，实现了灵活、易用且精确的视频插值。

- Motivation: 现有视频插帧方法无法生成大型、复杂或精细的运动，难以适应多样化的用户意图，缺乏对中间帧细节的精细控制，导致与创意想法不一致。
- Method: 采用Diffusion Transformer架构作为视频生成模型，将所有运动控制映射为统一的基于点的表示，分离内容和运动控制为两个分支，并提出分阶段训练策略。
- Result: 广泛的定性和定量实验表明，多模态控制能够实现更动态、可定制和上下文准确的视觉叙事。
- Conclusion: 该框架通过多模态控制实现了灵活、易用且精确的视频插值，填补了现有方法的空白。


### [99] [ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving](https://arxiv.org/abs/2510.08562)
*Zhiyu Zheng,Shaoyu Chen,Haoran Yin,Xinbang Zhang,Jialv Zou,Xinggang Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 提出ResAD框架解决端到端自动驾驶中的轨迹数据时空不平衡问题，通过预测相对于惯性参考的残差偏差和点归一化来改善优化过程。

- Motivation: 端到端自动驾驶系统面临轨迹数据固有的时空不平衡问题，这导致模型学习虚假相关性而非因果推理，同时优先考虑不确定的远距离预测，从而危及即时安全。
- Method: 提出ResAD框架，将学习任务重新定义为预测相对于确定性惯性参考的残差偏差。惯性参考作为反事实，迫使模型识别导致偏离默认惯性路径的因果因素。进一步采用点归一化来重新加权优化目标，防止与远距离不确定路径点相关的大幅度误差主导学习信号。
- Result: 在NAVSIM基准测试中，ResAD使用仅有两个去噪步骤的普通扩散策略实现了88.6的最优PDMS，表明该方法显著简化了学习任务并提高了模型性能。
- Conclusion: ResAD框架通过残差轨迹建模和点归一化，有效解决了端到端自动驾驶中的时空不平衡问题，提高了模型性能和安全性。


### [100] [NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints](https://arxiv.org/abs/2510.08565)
*Changyao Tian,Hao Li,Gen Luo,Xizhou Zhu,Weijie Su,Hanming Deng,Jinguo Zhu,Jie Shao,Ziran Zhu,Yunpeng Liu,Lewei Lu,Wenhai Wang,Hongsheng Li,Jifeng Dai*

Main category: cs.CV

TL;DR: 本文研究了多模态大语言模型（MLLMs）的端到端原生训练方法，提出了最佳元架构NaViL，并揭示了视觉编码器与LLMs之间的正相关缩放关系。

- Motivation: 现有MLLMs通常采用组合式训练范式，但难以探索其多模态缩放特性。本文旨在研究端到端原生训练的设计空间和缩放特性。
- Method: 在数据约束的实际设置下，系统研究MLLMs的各种设计选择，找到性能与训练成本最佳平衡的元架构，并探索视觉编码器与LLMs的缩放关系。
- Result: 提出了NaViL模型，在14个多模态基准测试中表现出与现有MLLMs竞争的性能，验证了视觉编码器与LLMs的正相关缩放关系。
- Conclusion: NaViL模型及其简单有效的训练方法为未来原生MLLMs研究提供了深入见解，证明了端到端原生训练的有效性。


### [101] [D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction](https://arxiv.org/abs/2510.08566)
*Meixi Song,Xin Lin,Dizhe Zhang,Haodong Li,Xiangtai Li,Bo Du,Lu Qi*

Main category: cs.CV

TL;DR: 提出D²GS框架解决稀疏视图下3D高斯泼溅的性能下降问题，包含密度深度引导的dropout策略和距离感知保真度增强模块

- Motivation: 稀疏视图条件下3D高斯泼溅存在性能下降和不稳定性问题，主要由于相机附近高斯密度过高导致过拟合，以及远距离区域高斯覆盖不足导致欠拟合
- Method: 1) 深度密度引导dropout策略：基于密度和深度自适应掩码冗余高斯；2) 距离感知保真度增强模块：通过针对性监督改善远场区域重建质量；3) 提出新的高斯分布稳定性评估指标
- Result: 在多个数据集上的广泛实验表明，该方法显著提升了稀疏视图条件下的视觉质量和鲁棒性
- Conclusion: D²GS框架有效解决了稀疏视图下3D高斯泼溅的过拟合和欠拟合问题，提高了重建质量和稳定性


### [102] [MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning](https://arxiv.org/abs/2510.08567)
*Tajamul Ashraf,Umair Nawaz,Abdelrahman M. Shaker,Rao Anwer,Philip Torr,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 提出了一个视觉中心的多模态智能体调优框架MATRIX，通过自动合成多模态轨迹和生成偏好对，训练VLM控制器实现鲁棒的工具使用推理。

- Motivation: 解决VLM作为控制器时高质量多模态轨迹稀缺和人工标注成本高的问题。
- Method: 构建M-TRACE数据集（28.5K任务，177K轨迹），开发MATRIX Agent控制器，通过Pref-X偏好对进行步进式偏好学习。
- Result: 在Agent-X、GTA和GAIA三个基准测试中，MATRIX持续超越开源和闭源VLM。
- Conclusion: MATRIX展示了可扩展且有效的多模态工具使用能力。


### [103] [ReSplat: Learning Recurrent Gaussian Splats](https://arxiv.org/abs/2510.08575)
*Haofei Xu,Daniel Barath,Andreas Geiger,Marc Pollefeys*

Main category: cs.CV

TL;DR: ReSplat是一种前馈循环高斯溅射模型，通过迭代优化3D高斯分布来提升渲染质量，无需显式计算梯度，显著减少了高斯数量并提高了渲染速度。

- Motivation: 传统前馈高斯溅射模型依赖单次前向传播，性能受限。本文旨在通过循环反馈机制改进高斯溅射，利用渲染误差作为反馈信号来指导高斯更新。
- Method: 提出循环高斯溅射模型，使用渲染误差作为反馈信号指导高斯更新；引入紧凑重建模型在16倍下采样空间初始化高斯，减少高斯数量；支持不同输入视图、分辨率和数据集。
- Result: 在DL3DV和RealEstate10K数据集上，使用2、8、16个输入视图和256×256到540×960分辨率，实现了最先进的性能，显著减少了高斯数量并提升了渲染速度。
- Conclusion: ReSplat通过循环反馈机制有效提升了高斯溅射的性能，在减少计算开销的同时实现了更好的泛化能力和渲染质量。
## astro-ph.IM

### [104] [FlowLensing: Simulating Gravitational Lensing with Flow Matching](https://arxiv.org/abs/2510.07878)
*Hamees Sayed,Pranath Reddy,Michael W. Toomey,Sergei Gleyzer*

Main category: astro-ph.IM

TL;DR: FlowLensing是一个基于扩散变换器的紧凑高效流匹配模型，用于强引力透镜模拟，相比传统模拟器实现200倍加速，支持离散和连续参数处理。

- Motivation: 现有引力透镜模拟工具依赖光线追踪或前向建模管道，虽然精确但速度过慢，成为暗物质研究的瓶颈。
- Method: 采用扩散变换器架构的流匹配模型，在离散和连续两种机制下运行，处理不同暗物质模型类别和连续模型参数，确保物理一致性。
- Result: 模型在密集暗物质模型上相比经典模拟器实现200倍加速，具有高保真度和低推理延迟。
- Conclusion: FlowLensing能够实现快速、可扩展且物理一致的图像合成，为传统前向建模管道提供了实用替代方案，可推进暗物质亚结构探测研究。
## cs.RO

### [105] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: IntentionVLA是一个新的视觉-语言-动作模型框架，通过课程训练范式解决当前VLA模型在隐含人类意图推理方面的不足，显著提升了在复杂人机交互任务中的表现。

- Motivation: 当前最先进的VLA模型主要在通用多模态任务上预训练，缺乏针对具身场景的推理密集型预训练和推理引导的操控能力，无法处理复杂现实交互中所需的隐含人类意图推理。
- Method: 提出IntentionVLA框架，首先使用精心设计的推理数据（包含意图推断、空间定位和紧凑具身推理）进行预训练，赋予模型推理和感知能力；然后在微调阶段使用紧凑推理输出作为动作生成的上下文指导，实现间接指令下的快速推理。
- Result: IntentionVLA显著优于π0基准模型，在直接指令下成功率提高18%，在意图指令下比ECoT提高28%。在分布外意图任务中，成功率是所有基准模型的两倍以上，并在零样本人机交互中达到40%的成功率。
- Conclusion: IntentionVLA代表了下一代人机交互系统的有前景范式，通过推理引导的操控显著提升了模型在复杂现实交互任务中的表现。


### [106] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文提出了一种基于Falcon模型的主动风险感知模块，用于提升机器人在动态人流量环境中的社交导航性能，在IROS 2025 RoboSense挑战赛中获得第二名。

- Motivation: 解决自主机器人在动态人流量室内环境中安全、高效且符合社交规范的导航问题，特别是在仅使用机载传感器且无全局地图的情况下。
- Method: 在Falcon模型基础上引入主动风险感知模块，通过学习预测周围行人的基于距离的碰撞风险分数，增强空间感知和主动避障能力。
- Result: 在Social-HM3D基准测试中，该方法提高了机器人在拥挤室内场景中保持个人空间合规性的能力，在16支参赛队伍中获得第二名。
- Conclusion: 提出的主动风险感知模块有效提升了社交导航性能，使机器人能够在动态人流量环境中更安全地导航。


### [107] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: 提出了NavSpace基准测试，包含6个任务类别和1,228个轨迹-指令对，用于评估导航代理的空间智能。评估了22个导航代理，并提出了新的空间智能导航模型SNav，在NavSpace和真实机器人测试中表现优异。

- Motivation: 现有基准测试主要关注语义理解，但忽视了系统评估导航代理的空间感知和推理能力。
- Method: 构建NavSpace基准测试，包含6个任务类别和1,228个轨迹-指令对，用于评估空间智能。提出了新的空间智能导航模型SNav。
- Result: 评估了22个导航代理，揭示了空间智能在具身导航中的重要性。SNav在NavSpace和真实机器人测试中优于现有导航代理。
- Conclusion: NavSpace基准测试为评估导航代理的空间智能提供了系统框架，SNav模型为未来工作建立了强基线。


### [108] [DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos](https://arxiv.org/abs/2510.08475)
*Jhen Hsieh,Kuan-Hsun Tu,Kuo-Han Hung,Tsung-Wei Ke*

Main category: cs.RO

TL;DR: DexMan是一个自动化框架，可将人类视觉演示转换为仿真中类人机器人的双手灵巧操作技能，无需相机标定、深度传感器或3D对象资产。

- Motivation: 解决现有方法仅考虑简化浮动手部、需要大量手动数据收集和昂贵运动捕捉的问题，实现从野外视频直接学习灵巧操作技能。
- Method: 基于第三人称人类操作视频，使用接触奖励改进策略学习，直接从噪声手-物体姿态估计中学习，控制完整类人机器人。
- Result: 在TACO基准测试中物体姿态估计达到SOTA性能（ADD-S和VSD分别提升0.08和0.12），在OakInk-v2上成功率比先前方法提高19%。
- Conclusion: DexMan能从真实和合成视频生成技能，无需手动数据收集和昂贵运动捕捉，为训练通用灵巧操作创建大规模多样化数据集。


### [109] [R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation](https://arxiv.org/abs/2510.08547)
*Xiuwei Xu,Angyuan Ma,Hankun Li,Bingyao Yu,Zheng Zhu,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出了R2RGen框架，直接从真实世界的点云观测-动作对进行数据增强，无需模拟器和渲染，实现了高效即插即用的真实到真实3D数据生成。

- Motivation: 解决机器人操作中的空间泛化问题，传统方法需要大量人工演示数据覆盖不同空间配置，而现有数据生成方法存在显著的模拟到真实差距，且受限于固定基座场景和预定义相机视角。
- Method: 基于单个源演示，通过细粒度场景和轨迹解析的标注机制，提出分组增强策略处理复杂多对象组合和多样化任务约束，并采用相机感知处理使生成数据分布与真实世界3D传感器对齐。
- Result: R2RGen在大量实验中显著提升了数据效率，并展示了在移动操作中的扩展和应用潜力。
- Conclusion: R2RGen框架通过真实到真实的3D数据生成，有效解决了机器人操作中的空间泛化问题，具有高效、即插即用的特点，为移动操作提供了强有力的数据增强解决方案。


### [110] [DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model](https://arxiv.org/abs/2510.08556)
*Xueyi Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 提出了一种新颖的sim-to-real框架，通过联合动力学模型有效弥合现实差距，使单一策略能在真实世界中泛化处理各种物体的旋转任务

- Motivation: 解决机器人手内物体旋转任务中从仿真到现实的迁移难题，克服复杂接触动力学带来的现实差距，突破现有方法在物体几何、尺寸、手腕姿态等方面的限制
- Method: 使用联合动力学模型学习真实世界数据来弥合现实差距，通过因子化关节动力学、压缩系统影响为低维变量，并配合自主数据收集策略
- Result: 单一策略成功旋转复杂形状物体（如动物模型）、高宽比物体（达5.33）和小尺寸物体，处理多样手腕方向和旋转轴，验证了方法的有效性和鲁棒性
- Conclusion: 该方法实现了前所未有的泛化能力，为灵巧操作提供了有效的sim-to-real解决方案


### [111] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本机器人操作框架，通过视频生成和3D物体流分析将任务描述转换为可执行计划，无需演示或特定平台训练。

- Motivation: 现有机器人操作方法通常需要分布内任务或依赖特定平台数据的微调，限制了跨平台迁移能力。
- Method: 使用视频生成模型合成任务视频，通过感知模块提取3D物体流，对刚性物体计算相对位姿，对可变形物体使用基于粒子的动力学模型进行规划。
- Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、关节和可变形物体操作任务的有效零样本执行。
- Conclusion: 通过将任务理解与底层控制解耦，NovaFlow实现了跨平台的零样本操作能力。


### [112] [Scalable Offline Metrics for Autonomous Driving](https://arxiv.org/abs/2510.08571)
*Animikh Aich,Adwait Kulkarni,Eshed Ohn-Bar*

Main category: cs.RO

TL;DR: 该研究发现自动驾驶系统中离线评估与在线性能之间的相关性比之前报告的要差，提出了基于认知不确定性的离线指标，将相关性提高了13%以上，并在真实世界中验证了该方法的有效性。

- Motivation: 当前自动驾驶系统的离线评估难以准确预测在线性能，微小的错误可能在测试时累积导致事故，这种离线与在线评估之间的关系在复杂城市机动中研究不足。
- Method: 通过大量实验分析离线与在线评估的相关性，提出基于认知不确定性的离线指标来捕捉可能导致闭环设置中错误的事件。
- Result: 发现离线与在线评估的相关性比先前研究报道的更差，提出的新离线指标将相关性提高了13%以上，在真实世界环境中观察到更大的增益。
- Conclusion: 当前驾驶策略评估实践和指标的有效性值得怀疑，基于认知不确定性的离线指标能更好地桥接离线与在线评估之间的差距。
## eess.IV

### [113] [Curriculum Learning with Synthetic Data for Enhanced Pulmonary Nodule Detection in Chest Radiographs](https://arxiv.org/abs/2510.07681)
*Pranav Sambhu,Om Guin,Madhav Sambhu,Jinho Cha*

Main category: eess.IV

TL;DR: 该研究评估了将课程学习与基于扩散的合成增强相结合，是否能提高胸部X光片中困难肺结节的检测能力，特别是那些尺寸小、亮度低、对比度差的结节。

- Motivation: 解决传统AI模型在检测困难肺结节时面临的数据不平衡和标注有限的问题，这些结节由于尺寸小、亮度低、对比度差而难以检测。
- Method: 使用带有特征金字塔网络(FPN)的Faster R-CNN，在混合数据集上训练，包括专家标注的NODE21、VinDr-CXR、CheXpert和11,206张DDPM生成的合成图像。基于尺寸、亮度和对比度的难度分数指导课程学习。
- Result: 课程学习模型的平均AUC为0.95，而基线模型为0.89(p < 0.001)，敏感性(70% vs 48%)和准确率(82% vs 70%)均有显著提升。分层分析显示在所有难度级别上均有一致的改进。
- Conclusion: 课程引导的合成增强能够提高肺结节检测模型的鲁棒性和泛化能力。


### [114] [SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion](https://arxiv.org/abs/2510.07905)
*Yufei Tong,Guanjie Cheng,Peihan Wu,Yicheng Zhu,Kexu Lu,Feiyi Chen,Meng Xi,Junqin Huang,Shuiguang Deng*

Main category: eess.IV

TL;DR: SatFusion是一个统一的卫星物联网图像增强框架，通过多时相和多源数据融合来解决现有方法的局限性。

- Motivation: 现有方法未能充分利用时相和源维度的互补信息。MISR方法受限于输入图像的细粒度纹理细节，而全色锐化方法对噪声和配准误差敏感。
- Method: SatFusion包含三个模块：多时相图像融合模块实现与全色图像的深度特征对齐；多源图像融合模块注入全色数据的细粒度纹理信息；融合组合模块自适应整合两种模态的优势并动态优化光谱一致性。
- Result: 在WorldStrat、WV3、QB和GF2数据集上的广泛实验表明，SatFusion显著提高了融合质量、在挑战性条件下的鲁棒性以及在实际Sat-IoT场景中的泛化能力。
- Conclusion: SatFusion通过统一的多时相和多源数据融合框架，有效解决了卫星物联网图像增强中的关键问题，具有优异的性能和实用性。


### [115] [AI-Driven Radiology Report Generation for Traumatic Brain Injuries](https://arxiv.org/abs/2510.08498)
*Riadh Bouslimi,Houda Trabelsi,Wahiba Ben Abdssalem Karaa,Hana Hedhli*

Main category: eess.IV

TL;DR: 提出了一种基于AI的自动放射学报告生成方法，专门针对颅脑创伤病例，结合AC-BiFPN和Transformer架构，在RSNA颅内出血检测数据集上表现优于传统CNN模型。

- Motivation: 创伤性脑损伤在急诊医学中诊断困难，及时解读医学影像对患者预后至关重要。需要开发AI系统来辅助放射科医生在高压环境下进行诊断。
- Method: 集成AC-BiFPN与Transformer架构，AC-BiFPN提取多尺度特征检测颅内出血等复杂异常，Transformer生成连贯的诊断报告。
- Result: 在RSNA颅内出血检测数据集上，模型在诊断准确性和报告生成方面均优于传统CNN模型。
- Conclusion: 结合先进特征提取和基于Transformer的文本生成，有望改善创伤性脑损伤诊断中的临床决策制定。
## cs.AI

### [116] [Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models](https://arxiv.org/abs/2510.07632)
*Yinglun Zhu,Jiancheng Zhang,Fuzhi Tang*

Main category: cs.AI

TL;DR: 本文发现现有评估指标系统性地低估了AI模型的组合推理能力，提出了组匹配分数和测试时匹配(TTM)算法，显著提升了模型性能，在多个基准测试中达到新SOTA。

- Motivation: 现有研究表明前沿AI模型在组合推理任务上表现不佳，但作者认为这是评估指标的问题而非模型能力问题。
- Method: 引入组匹配分数来更好地利用组结构，并提出测试时匹配(TTM)算法，这是一种无需外部监督的迭代自改进方法。
- Result: SigLIP-B16超越所有先前结果，GPT-4.1首次超越人类在Winoground上的表现；TTM使SigLIP-B16在MMVP-VLM上超越GPT-4.1，在16个数据集上均获得持续改进。
- Conclusion: 通过改进评估方法和测试时优化，AI模型在组合推理任务上的能力被显著低估，TTM算法能有效提升模型性能并推动组合推理前沿。


### [117] [How to Teach Large Multimodal Models New Skills](https://arxiv.org/abs/2510.08564)
*Zhen Zhu,Yiming Gong,Yao Xiao,Yaoyao Liu,Derek Hoiem*

Main category: cs.AI

TL;DR: 研究大模型多模态模型在连续微调中的遗忘问题，发现遗忘可部分恢复，并提出两种有效的微调方法以减少性能下降。

- Motivation: 研究如何在教大模型多模态模型新技能时避免遗忘已有能力，解决连续微调中的灾难性遗忘问题。
- Method: 在五个目标技能上连续微调，监控八个保留基准的性能，分析输出token分布变化，并提出两种微调方法：仅更新自注意力投影层，或仅更新MLP Gate&Up层。
- Result: 发现窄域微调后的明显遗忘可在后期部分恢复，通过简单的计数偏差探针可测量输出分布变化，两种提出的微调方法在获得强目标增益的同时能保持保留性能。
- Conclusion: 通过选择性更新特定层，可以有效平衡大模型多模态模型学习新技能和保持已有能力的需求，减少灾难性遗忘。
## cs.GR

### [118] [SViM3D: Stable Video Material Diffusion for Single Image 3D Generation](https://arxiv.org/abs/2510.08271)
*Andreas Engelhardt,Mark Boss,Vikram Voletti,Chun-Han Yao,Hendrik P. A. Lensch,Varun Jampani*

Main category: cs.GR

TL;DR: SViM3D是一个从单张图像预测多视角一致的基于物理渲染(PBR)材质的框架，通过扩展潜在视频扩散模型来联合输出PBR参数和法线，实现可重光照的3D资产生成。

- Motivation: 现有方法要么使用简单材质模型，要么需要额外步骤来估计反射率，限制了重光照和外观编辑能力。
- Method: 扩展潜在视频扩散模型，在显式相机控制下联合输出空间变化的PBR参数和表面法线，并引入多种机制改进质量。
- Result: 在多个以对象为中心的数据集上实现了最先进的重光照和新视角合成性能，能泛化到多样化输入。
- Conclusion: 该方法能够生成可用于AR/VR、电影、游戏等视觉媒体的可重光照3D资产。


### [119] [Spectral Prefiltering of Neural Fields](https://arxiv.org/abs/2510.08394)
*Mustafa B. Yaldiz,Ishit Mehta,Nithin Raghavan,Andreas Meuleman,Tzu-Mao Li,Ravi Ramamoorthi*

Main category: cs.GR

TL;DR: 提出了一种优化神经场的方法，使其能够在单次前向传播中进行预滤波，支持多种参数化滤波器。

- Motivation: 神经场擅长表示连续视觉信号，但通常只能在单一固定分辨率下工作，需要一种能够进行预滤波的优化方法。
- Method: 在输入域进行卷积滤波，通过分析性地缩放傅里叶特征嵌入与滤波器的频率响应；使用单样本蒙特卡洛估计训练神经场。
- Result: 在训练和推理时都很快，对网络架构没有额外约束，在神经场滤波方面比现有方法有定量和定性改进。
- Conclusion: 该方法简单而强大，能够支持训练时未见过的参数化滤波器，实现了高效的神经场预滤波。


### [120] [Splat the Net: Radiance Fields with Splattable Neural Primitives](https://arxiv.org/abs/2510.08491)
*Xilong Zhou,Bao-Huy Nguyen,Loïc Magne,Vladislav Golyanik,Thomas Leimkühler,Christian Theobalt*

Main category: cs.GR

TL;DR: 提出了一种可splat的神经基元表示，结合了神经辐射场的高表达能力和基元splatting的高效性，使用更少的基元和参数达到与3D高斯splatting相当的质量和速度。

- Motivation: 神经辐射场（如NeRF）表达能力强但渲染成本高，而基元方法（如3D高斯splatting）渲染高效但表达能力有限。需要一种能兼顾表达能力和渲染效率的表示方法。
- Method: 使用有界神经密度场的基元表示，每个基元由浅层神经网络参数化。通过解析求解线积分，实现透视准确的splatting核计算，无需昂贵的光线行进。
- Result: 在新视角合成基准测试中，与3D高斯splatting相比，使用10倍更少的基元和6倍更少的参数，达到相当的质量和速度。
- Conclusion: 该表示方法成功结合了神经模型的高表达能力和基元splatting的高效性，无需复杂控制框架即可实现优越性能。


### [121] [X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering](https://arxiv.org/abs/2510.08530)
*Zhitong Huang,Mohan Zhang,Renhan Wang,Rui Tang,Hao Zhu,Jing Liao*

Main category: cs.GR

TL;DR: X2Video是首个基于内在通道（反照率、法线、粗糙度、金属度、辐照度）指导的扩散模型，能够生成逼真视频，并支持参考图像和文本提示的多模态控制。

- Motivation: 现有视频生成模型缺乏对颜色、材质、几何和光照的精确控制能力，需要开发能够同时支持内在通道指导和直观多模态控制的视频生成方法。
- Method: 扩展XRGB图像生成模型到视频生成，采用混合自注意力确保时间一致性，开发掩码交叉注意力分离全局和局部文本提示，使用递归采样方法生成长视频。
- Result: X2Video能够生成长时间、时间一致且逼真的视频，有效支持多模态控制和参数化编辑。
- Conclusion: X2Video通过内在通道指导和多模态控制，实现了对视频颜色、材质、几何和光照的精确操控，为视频生成提供了新的可能性。
## cs.LG

### [122] [DUA-D2C: Dynamic Uncertainty Aware Method for Overfitting Remediation in Deep Learning](https://arxiv.org/abs/2411.15876)
*Md. Saiful Bari Siddiqui,Md Mohaiminul Islam,Md. Golam Rabiul Alam*

Main category: cs.LG

TL;DR: 提出了DUA-D2C方法，通过动态不确定性感知的聚合机制改进Divide2Conquer方法，有效对抗深度学习中的过拟合问题。

- Motivation: 标准D2C方法在聚合子集模型时平等对待所有模型或使用固定启发式方法，未能充分利用不同子集模型泛化能力的差异信息。
- Method: 在D2C基础上引入动态不确定性感知聚合，基于共享验证集上的性能和预测不确定性动态加权子集模型的贡献。
- Result: 在多个领域的基准数据集上，DUA-D2C显著提高了泛化性能，即使在其他正则化方法之上应用也能进一步改善性能。
- Conclusion: DUA-D2C是一个理论基础扎实且有效的方法，能够有效对抗现代深度学习中的过拟合问题。


### [123] [Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children](https://arxiv.org/abs/2510.07320)
*Nelaka K. A. R,Peiris M. K. V,Liyanage R. P. B*

Main category: cs.LG

TL;DR: 该研究探讨自闭症谱系障碍儿童的行为模式和情绪识别，提出通过纵向监测建立基线理解，并开发针对性技术辅助框架来改善学习环境。

- Motivation: 自闭症谱系障碍显著影响个体的沟通、学习和社交能力，但现有研究在理解自闭症儿童微妙行为模式和情绪识别方面存在关键空白，特别是在技能发展之前阶段。
- Method: 采用纵向方法监测情绪和行为，分析行为趋势，建立基线理解，并基于此提出针对性的应用和技术辅助框架。
- Result: 研究强调了顺序性和基于证据的干预方法的重要性，优先深入理解每个儿童的行为和情绪状况作为有效技能发展的基础。
- Conclusion: 通过将重点转向早期行为模式识别，旨在营造更具包容性和支持性的学习环境，显著改善自闭症儿童的教育和发展轨迹。


### [124] [MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation](https://arxiv.org/abs/2510.07328)
*Md Zubair,Hao Zheng,Nussdorf Jonathan,Grayson W. Armstrong,Lucy Q. Shen,Gabriela Wilson,Yu Tian,Xingquan Zhu,Min Shi*

Main category: cs.LG

TL;DR: 提出MultiFair方法解决多模态医疗分类中的两个关键挑战：模态学习不平衡和群体公平性问题，通过双层级梯度调制实现公平的多模态学习

- Motivation: 现有多模态学习模型存在两个关键问题：不同数据模态学习不均衡导致模型偏向某些模态，以及模型可能偏向特定人口群体导致不公平性能。这两个问题相互影响，需要同时解决
- Method: 提出MultiFair方法，采用双层级梯度调制过程，在数据模态和群体两个层面动态调节训练梯度的优化方向和幅度
- Result: 在两个多模态医疗数据集上的实验表明，MultiFair在多个人口群体上优于当前最先进的多模态学习和公平性学习方法
- Conclusion: MultiFair通过双层级梯度调制有效解决了多模态医疗分类中的模态不平衡和群体公平性问题，实现了更公平和平衡的多模态学习


### [125] [ConCuR: Conciseness Makes State-of-the-Art Kernel Generation](https://arxiv.org/abs/2510.07356)
*Lingcheng Kong,Jiateng Wei,Hanzhang Shen,Huan Wang*

Main category: cs.LG

TL;DR: 为了解决GPU内核生成中高质量数据稀缺的问题，作者开发了一个生成和筛选高质量CUDA内核及推理轨迹的流程，构建了ConCuR数据集并训练了KernelCoder模型，在KernelBench上表现优于现有最佳模型。

- Motivation: GPU内核生成面临高质量数据稀缺的挑战，大多数高质量内核是专有且不开源的，这阻碍了使用监督微调将LLM与内核生成任务对齐。
- Method: 开发了一个生成和筛选高质量CUDA内核及推理轨迹的流程，构建ConCuR数据集，并训练KernelCoder模型，该模型是首个在包含PyTorch、推理和CUDA内核对的精选数据集上训练的模型。
- Result: 在KernelBench设置中，模型显著优于现有表现最佳的模型QwQ-32B，超越了所有为内核生成微调的开源模型以及前沿模型如DeepSeek-V3.1-Think和Claude-4-sonnet。
- Conclusion: 平均推理长度可以作为评估内核生成任务难度的指标，观察结果、指标以及数据收集和筛选流程有助于未来在内核生成任务中获得更好的数据。


### [126] [MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis](https://arxiv.org/abs/2510.07513)
*Qinghua Liu,Sam Heshmati,Zheda Mai,Zubin Abraham,John Paparrizos,Liu Ren*

Main category: cs.LG

TL;DR: MLLM4TS是一个利用多模态大语言模型进行时间序列分析的新框架，通过视觉表示增强分析能力，在分类、异常检测和预测等任务中表现出色。

- Motivation: 时间序列数据具有复杂的时间依赖性和跨通道交互，传统分析方法难以有效处理。受人类视觉检查模式的启发，探索视觉表示是否能增强自动化时间序列分析。
- Method: 提出MLLM4TS框架，将每个时间序列通道渲染为水平堆叠的彩色线图，使用时间感知的视觉块对齐策略，融合数值数据的细粒度时间细节和视觉表示的全局上下文信息。
- Result: 在标准基准测试上的广泛实验表明，MLLM4TS在预测任务（如分类）和生成任务（如异常检测和预测）中均表现出有效性。
- Conclusion: 将视觉模态与预训练语言模型相结合，有望实现鲁棒且可泛化的时间序列分析。


### [127] [MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation](https://arxiv.org/abs/2510.07910)
*Chongmyung Kwon,Yujin Kim,Seoeun Park,Yunji Lee,Charmgil Hong*

Main category: cs.LG

TL;DR: 提出MMM框架，通过整合三维量子化学信息（电子定位函数ELF）来改进药物相互作用预测，相比传统GNN方法在多个指标上取得显著提升

- Motivation: 现有基于图神经网络的药物推荐方法使用简化的离散形式表示药物结构，无法充分捕捉分子结合亲和力和反应性，药物相互作用风险仍是重大挑战
- Method: 提出MMM框架，生成3D电子密度图（使用ELF），结合ELF衍生的全局电子特性特征和二分图编码器建模局部子结构相互作用，学习药物分子的互补特征
- Result: 在MIMIC-III数据集上评估，与SafeDrug等基线模型相比，在F1分数、Jaccard系数和DDI率上均取得统计显著改进（p值分别为0.0387、0.0112、0.0386）
- Conclusion: 基于ELF的3D表示有潜力提高预测准确性，支持临床实践中更安全的组合药物处方


### [128] [Dual-granularity Sinkhorn Distillation for Enhanced Learning from Long-tailed Noisy Data](https://arxiv.org/abs/2510.08179)
*Feng Hong,Yu Huang,Zihua Zhao,Zhihan Zhou,Jiangchao Yao,Dongsheng Li,Ya Zhang,Yanfeng Wang*

Main category: cs.LG

TL;DR: 提出D-SINK框架，通过知识蒸馏整合专门处理类别不平衡和标签噪声的辅助模型，增强深度学习模型在长尾噪声数据上的鲁棒性。

- Motivation: 现实数据集中常同时存在类别不平衡和标签噪声问题，单独处理方法难以协同，且区分真实尾部样本与噪声数据困难。论文发现这两个问题在不同粒度上运作，可以互补解决。
- Method: 提出双粒度Sinkhorn蒸馏(D-SINK)框架，使用最优传输优化的代理标签分配，将目标模型的样本级预测与噪声鲁棒辅助模型对齐，将类别分布与不平衡鲁棒辅助模型对齐。
- Result: 在基准数据集上的广泛实验表明，D-SINK显著提高了鲁棒性，在长尾噪声数据学习中取得了强大的实证性能。
- Conclusion: 通过协同利用专门但'弱'的辅助模型，D-SINK框架成功解决了类别不平衡和标签噪声的联合挑战，为处理现实世界复杂数据提供了有效方案。


### [129] [Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin](https://arxiv.org/abs/2510.08407)
*Lauren Anderson,Lucas Chatelain,Nicolas Tremblay,Kathryn Grandfield,David Rousseau,Aurélien Gourrier*

Main category: cs.LG

TL;DR: 本研究测试了多种深度学习超分辨率模型，用于提升牙本质孔隙网络成像的分辨率，并通过生物学驱动的评估方法验证模型性能。

- Motivation: 当前牙本质孔隙成像受限于共聚焦显微镜的分辨率，只能观察小区域。需要开发方法在保持图像质量的同时扩大视野范围。
- Method: 应用三种监督式2D超分辨率模型（RCAN、pix2pix、FSRCNN）和一种无监督模型（CycleGAN），在不同放大倍数（×2、×4、×8）下处理配对的低分辨率和高分辨率共聚焦图像。
- Result: 传统图像质量评估指标与视觉感知不一致，而基于孔隙网络结构和连通性的生物学驱动评估能更好地解释超分辨率模型的性能差异。
- Conclusion: 针对特定生物结构（如牙本质孔隙）的超分辨率评估需要采用生物学驱动的分析方法，而非依赖通用的图像质量评估指标。


### [130] [Reinforcing Diffusion Models by Direct Group Preference Optimization](https://arxiv.org/abs/2510.08425)
*Yihong Luo,Tianyang Hu,Jing Tang*

Main category: cs.LG

TL;DR: 提出了Direct Group Preference Optimization (DGPO)算法，解决了扩散模型中强化学习效率低下的问题，通过直接利用组级偏好信息，无需依赖低效的随机策略，实现了20倍训练加速和更优性能。

- Motivation: 现有强化学习方法如GRPO在扩散模型中面临挑战，因为高效的扩散采样器基于确定性ODE，而GRPO需要随机策略。现有方法使用低效的SDE采样器引入随机性，导致收敛缓慢。
- Method: 提出DGPO算法，完全摆脱策略梯度框架，直接学习组级偏好信息，利用组内样本的相对信息，无需随机策略，从而可以使用高效的确定性ODE采样器。
- Result: 实验结果显示DGPO比现有最先进方法训练速度快约20倍，在领域内和领域外奖励指标上均取得更优性能。
- Conclusion: DGPO通过直接学习组级偏好，解决了扩散模型中强化学习的效率瓶颈，实现了快速训练和卓越性能。


### [131] [Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models](https://arxiv.org/abs/2510.08492)
*Sharut Gupta,Shobhita Sundaram,Chenyu Wang,Stefanie Jegelka,Phillip Isola*

Main category: cs.LG

TL;DR: UML：一种模态无关的训练范式，通过交替处理不同模态的输入并共享参数，利用未配对的辅助多模态数据增强目标模态的表征学习。

- Motivation: 传统多模态学习依赖配对数据集，但能否利用未配对的辅助多模态数据直接增强目标模态的表征学习是一个被忽视但具有潜力的问题。
- Method: 提出UML训练范式：单一模型交替处理不同模态输入，跨模态共享参数，利用不同模态是共享现实投影的假设，无需显式配对即可受益于跨模态结构。
- Result: 理论上在线性数据生成假设下，未配对辅助数据能产生比单模态训练更丰富的信息表征；实证表明使用文本、音频或图像等未配对辅助数据能持续提升图像和音频等单模态目标的下游性能。
- Conclusion: UML证明了未配对多模态数据可以有效增强单模态表征学习，为利用丰富但未配对的跨模态数据提供了新途径。
