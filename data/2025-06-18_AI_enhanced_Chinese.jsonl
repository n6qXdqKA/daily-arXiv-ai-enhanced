{"id": "2506.13769", "pdf": "https://arxiv.org/pdf/2506.13769", "abs": "https://arxiv.org/abs/2506.13769", "authors": ["Filippo Leveni"], "title": "Non-planar Object Detection and Identification by Features Matching and Triangulation Growth", "categories": ["cs.CV", "cs.AI"], "comment": "Master's thesis at Politecnico di Milano", "summary": "Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u5339\u914d\u56fe\u50cf\u4e0e\u6a21\u677f\u7684\u7279\u5f81\u6765\u68c0\u6d4b\u548c\u8bc6\u522b\u573a\u666f\u4e2d\u7684\u53d8\u5f62\u5bf9\u8c61\uff0c\u5229\u7528Delaunay\u4e09\u89d2\u5256\u5206\u4f5c\u4e3a\u6307\u5bfc\u5de5\u5177\u3002", "motivation": "\u5bf9\u8c61\u68c0\u6d4b\u548c\u8bc6\u522b\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u76ee\u6807\u8ddf\u8e2a\u3001\u5de5\u4e1a\u673a\u5668\u4eba\u63a7\u5236\u7b49\u9886\u57df\u3002\u4f20\u7edf\u51e0\u4f55\u6a21\u578b\uff08\u5982\u5355\u5e94\u6027\uff09\u5728\u975e\u5e73\u9762\u6216\u53d8\u5f62\u5bf9\u8c61\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u6a21\u677f\u7279\u5f81\u7684Delaunay\u4e09\u89d2\u5256\u5206\u89c6\u4e3a\u56fe\uff0c\u4ece\u5355\u4e2a\u4e09\u89d2\u5f62\u5f00\u59cb\u9010\u6b65\u8bc4\u4f30\u90bb\u8fd1\u8282\u70b9\u7684\u7279\u5f81\u5339\u914d\uff0c\u57fa\u4e8e\u51e0\u4f55\u548c\u5149\u5ea6\u4e00\u81f4\u6027\u6807\u51c6\u8fdb\u884c\u5206\u7ec4\u3002", "result": "\u5728\u53d8\u5f62\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u5355\u5e94\u6027\u7684RANSAC\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d\uff1b\u5728\u53d8\u5f62\u663e\u8457\u65f6\uff0c\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u975e\u5e73\u9762\u6216\u53d8\u5f62\u5bf9\u8c61\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u51e0\u4f55\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2506.13770", "pdf": "https://arxiv.org/pdf/2506.13770", "abs": "https://arxiv.org/abs/2506.13770", "authors": ["Shiwen Zhang", "Zhuowei Chen", "Lang Chen", "Yanze Wu"], "title": "CDST: Color Disentangled Style Transfer for Universal Style Reference Customization", "categories": ["cs.CV"], "comment": "codes and models will be released if the paper is accepted", "summary": "We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks.", "AI": {"tldr": "CDST\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u6d41\u98ce\u683c\u8fc1\u79fb\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5b8c\u5168\u5206\u79bb\u989c\u8272\u4e0e\u98ce\u683c\uff0c\u4f7f\u98ce\u683c\u6d41\u5bf9\u989c\u8272\u4e0d\u654f\u611f\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8c03\u4f18\u7684\u901a\u7528\u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u4e2d\u989c\u8272\u4e0e\u98ce\u683c\u8026\u5408\u7684\u95ee\u9898\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u8c03\u4f18\u7684\u7279\u5f81\u4fdd\u7559\u98ce\u683c\u8fc1\u79fb\u3002", "method": "\u91c7\u7528\u4e24\u6d41\u8bad\u7ec3\u8303\u5f0f\uff0c\u5206\u79bb\u989c\u8272\u4e0e\u98ce\u683c\uff0c\u7ed3\u5408\u591a\u7279\u5f81\u56fe\u50cf\u5d4c\u5165\u538b\u7f29\u548c\u57fa\u4e8eDiffusion UNet\u89e3\u8026\u5b9a\u5f8b\u7684\u65b0\u98ce\u683c\u5b9a\u4e49\u3002", "result": "\u5728\u591a\u79cd\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6548\u679c\uff0c\u98ce\u683c\u76f8\u4f3c\u6027\u663e\u8457\u63d0\u5347\u4e14\u4fdd\u7559\u5f3a\u7f16\u8f91\u80fd\u529b\u3002", "conclusion": "CDST\u901a\u8fc7\u989c\u8272\u4e0e\u98ce\u683c\u89e3\u8026\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u65e0\u9700\u8c03\u4f18\u7684\u901a\u7528\u98ce\u683c\u8fc1\u79fb\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.13780", "pdf": "https://arxiv.org/pdf/2506.13780", "abs": "https://arxiv.org/abs/2506.13780", "authors": ["Sedat Porikli", "Vedat Porikli"], "title": "Hidden Bias in the Machine: Stereotypes in Text-to-Image Models", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "Equal contribution by both authors, Published at CVPR 2025 Workshop on Experimental Model Auditing via Controllable Synthesis (EMACS) and Workshop on Demographic Diversity in Computer Vision (DemoDiv)", "summary": "Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\u53ef\u80fd\u590d\u5236\u548c\u653e\u5927\u793e\u4f1a\u504f\u89c1\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u63d0\u793a\u8bcd\u751f\u6210\u5e76\u5206\u6790\u4e86\u5927\u91cf\u56fe\u50cf\uff0c\u63ed\u793a\u4e86\u6027\u522b\u3001\u79cd\u65cf\u7b49\u65b9\u9762\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8T2I\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\u662f\u5426\u590d\u5236\u548c\u653e\u5927\u793e\u4f1a\u504f\u89c1\uff0c\u4ee5\u4fc3\u8fdb\u751f\u6210\u89c6\u89c9\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u3002", "method": "\u4f7f\u7528Stable Diffusion 1.5\u548cFlux-1\u6a21\u578b\u751f\u621016,000\u591a\u5f20\u56fe\u50cf\uff0c\u5e76\u4e0eGoogle Image Search\u76848,000\u5f20\u56fe\u50cf\u5bf9\u6bd4\uff0c\u5206\u6790\u4eba\u7c7b\u4e2d\u5fc3\u56e0\u7d20\u7684\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u751f\u6210\u56fe\u50cf\u4e2d\u5b58\u5728\u6027\u522b\u3001\u79cd\u65cf\u3001\u5e74\u9f84\u7b49\u65b9\u9762\u7684\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u5f80\u5f80\u5f3a\u5316\u4e86\u793e\u4f1a\u53d9\u4e8b\u4e2d\u7684\u6709\u5bb3\u523b\u677f\u5370\u8c61\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u66f4\u5305\u5bb9\u7684\u6570\u636e\u96c6\u548c\u5f00\u53d1\u5b9e\u8df5\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u89c6\u89c9\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2506.13846", "pdf": "https://arxiv.org/pdf/2506.13846", "abs": "https://arxiv.org/abs/2506.13846", "authors": ["Runtao Liu", "Jiahao Zhan", "Yingqing He", "Chen Wei", "Alan Yuille", "Qifeng Chen"], "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGAN-RM\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u7b80\u5316\u5956\u52b1\u6a21\u578b\u6784\u5efa\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u6570\u636e\u6216\u590d\u6742\u8d28\u91cf\u7ef4\u5ea6\u8bbe\u8ba1\uff0c\u4ec5\u9700\u5c11\u91cf\u76ee\u6807\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u6570\u636e\u6216\u590d\u6742\u8d28\u91cf\u7ef4\u5ea6\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u590d\u6742\u4e14\u4e0d\u5b8c\u6574\uff0c\u4e9f\u9700\u7b80\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u601d\u60f3\uff0c\u901a\u8fc7\u533a\u5206\u76ee\u6807\u6837\u672c\u4e0e\u6a21\u578b\u751f\u6210\u6837\u672c\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u4ec5\u9700\u5c11\u91cf\u76ee\u6807\u6837\u672c\uff08Preference Proxy Data\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGAN-RM\u5728\u591a\u9879\u5e94\u7528\u4e2d\u6709\u6548\uff0c\u5305\u62ec\u6d4b\u8bd5\u65f6\u6837\u672c\u7b5b\u9009\uff08Best-of-N\uff09\u53ca\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08SFT\u3001DPO\uff09\u3002", "conclusion": "GAN-RM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7b80\u5316\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u5b9e\u73b0\u590d\u6742\u5ea6\u3002"}}
{"id": "2506.13897", "pdf": "https://arxiv.org/pdf/2506.13897", "abs": "https://arxiv.org/abs/2506.13897", "authors": ["Thomas Kreutz", "Max M\u00fchlh\u00e4user", "Alejandro Sanchez Guinea"], "title": "DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding", "categories": ["cs.CV"], "comment": "This work is currently under review at ICCV 2025", "summary": "Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding (e.g., human activity recognition (HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model, which effectively learns a joint embedding space across these four modalities through noise contrastive estimation. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDeSPITE\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u6bd4\u9884\u8bad\u7ec3\uff08LiDAR\u70b9\u4e91\u3001\u4eba\u4f53\u9aa8\u67b6\u3001IMU\u6570\u636e\u548c\u6587\u672c\uff09\u5b66\u4e60\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u586b\u8865\u4e86LiDAR\u5728\u4eba\u7c7b\u6d3b\u52a8\u7406\u89e3\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "LiDAR\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684RGB\u76f8\u673a\u66ff\u4ee3\u54c1\uff0c\u5728\u591a\u6a21\u6001\u5bf9\u6bd4\u9884\u8bad\u7ec3\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u4eba\u7c7b\u6d3b\u52a8\u7406\u89e3\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51faDeSPITE\u6a21\u578b\uff0c\u5229\u7528\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\u5b66\u4e60\u56db\u79cd\u6a21\u6001\u7684\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u7ed3\u5408LIPD\u548cBabel\u6570\u636e\u96c6\u5b9e\u73b0\u6570\u636e\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDeSPITE\u652f\u6301\u9aa8\u67b6-\u70b9\u4e91-IMU\u5339\u914d\u3001\u68c0\u7d22\u548c\u65f6\u95f4\u7247\u6bb5\u68c0\u7d22\u7b49\u65b0\u4efb\u52a1\uff0c\u5e76\u5728\u70b9\u4e91HAR\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9884\u8bad\u7ec3\u6709\u6548\u6027\u3002", "conclusion": "DeSPITE\u586b\u8865\u4e86LiDAR\u5728\u591a\u6a21\u6001\u4eba\u7c7b\u6d3b\u52a8\u7406\u89e3\u4e2d\u7684\u7a7a\u767d\uff0c\u4e3a\u70b9\u4e91\u5e8f\u5217\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.13902", "pdf": "https://arxiv.org/pdf/2506.13902", "abs": "https://arxiv.org/abs/2506.13902", "authors": ["Raymond Yu", "Paul Han", "Josh Myers-Dean", "Piper Wolters", "Favyen Bastani"], "title": "OPTIMUS: Observing Persistent Transformations in Multi-temporal Unlabeled Satellite-data", "categories": ["cs.CV"], "comment": "WACV 2025", "summary": "In the face of pressing environmental issues in the 21st century, monitoring surface changes on Earth is more important than ever. Large-scale remote sensing, such as satellite imagery, is an important tool for this task. However, using supervised methods to detect changes is difficult because of the lack of satellite data annotated with change labels, especially for rare categories of change. Annotation proves challenging due to the sparse occurrence of changes in satellite images. Even within a vast collection of images, only a small fraction may exhibit persistent changes of interest. To address this challenge, we introduce OPTIMUS, a self-supervised learning method based on an intuitive principle: if a model can recover information about the relative order of images in the time series, then that implies that there are long-lasting changes in the images. OPTIMUS demonstrates this principle by using change point detection methods on model outputs in a time series. We demonstrate that OPTIMUS can directly detect interesting changes in satellite images, achieving an improvement in AUROC score from 56.3% to 87.6% at distinguishing changed time series from unchanged ones compared to baselines. Our code and dataset are available at https://huggingface.co/datasets/optimus-change/optimus-dataset/.", "AI": {"tldr": "OPTIMUS\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u957f\u671f\u53d8\u5316\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u536b\u661f\u56fe\u50cf\u4e2d\u53d8\u5316\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u5c24\u5176\u662f\u7f55\u89c1\u7c7b\u522b\uff0c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "method": "OPTIMUS\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u4e2d\u56fe\u50cf\u76f8\u5bf9\u987a\u5e8f\u7684\u4fe1\u606f\u6062\u590d\u539f\u7406\uff0c\u7ed3\u5408\u53d8\u5316\u70b9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u76f4\u63a5\u68c0\u6d4b\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u53d8\u5316\u3002", "result": "OPTIMUS\u5728\u533a\u5206\u53d8\u5316\u4e0e\u672a\u53d8\u5316\u65f6\u95f4\u5e8f\u5217\u7684AUROC\u5f97\u5206\u4e0a\u4ece56.3%\u63d0\u5347\u81f387.6%\u3002", "conclusion": "OPTIMUS\u4e3a\u536b\u661f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.13910", "pdf": "https://arxiv.org/pdf/2506.13910", "abs": "https://arxiv.org/abs/2506.13910", "authors": ["Aritra Dutta", "Pushpita Boral", "G Suseela"], "title": "Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u66b4\u529b\u68c0\u6d4b\u4e0e\u5206\u7c7b\u6846\u67b6\uff0c\u7ed3\u54083D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u53cc\u5411LSTM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5168\u7403\u72af\u7f6a\u7387\u4e0a\u5347\u53ca\u4f20\u7edf\u76d1\u63a7\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u5f00\u53d1\u81ea\u52a8\u66b4\u529b\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u4f7f\u75283D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u66b4\u529b\u68c0\u6d4b\uff0c\u7ed3\u5408\u53ef\u5206\u79bb\u5377\u79ef3D\u6a21\u578b\u548c\u53cc\u5411LSTM\u8fdb\u884c\u5206\u7c7b\uff0c\u8bad\u7ec3\u6570\u636e\u6765\u81ea\u591a\u6837\u5316\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u5728\u8ba1\u7b97\u8d44\u6e90\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u66b4\u529b\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.13925", "pdf": "https://arxiv.org/pdf/2506.13925", "abs": "https://arxiv.org/abs/2506.13925", "authors": ["Numair Nadeem", "Saeed Anwar", "Muhammad Hamza Asad", "Abdul Bais"], "title": "HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.", "AI": {"tldr": "HierVL\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u63a9\u7801\u53d8\u6362\u5668\u67b6\u6784\u7684\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u67e5\u8be2\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5728\u6807\u7b7e\u7a00\u7f3a\u548c\u9886\u57df\u53d8\u5316\u4e0b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u89c6\u89c9\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u8fb9\u754c\u5b9a\u4f4d\u4e0a\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHierVL\u6846\u67b6\uff0c\u5305\u62ec\u5206\u5c42\u8bed\u4e49\u67e5\u8be2\u751f\u6210\u5668\u3001\u8de8\u6a21\u6001\u7a7a\u95f4\u5bf9\u9f50\u6a21\u5757\u548c\u53cc\u67e5\u8be2\u53d8\u6362\u5668\u89e3\u7801\u5668\uff0c\u5e76\u7ed3\u5408\u76ee\u6807\u6b63\u5219\u5316\u635f\u5931\u3002", "result": "\u5728COCO\u3001Pascal VOC\u3001ADE20\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e864.4%\u30013.1%\u30015.9%\u548c1.8%\u7684\u5e73\u5747IoU\u63d0\u5347\u3002", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684\u5206\u5272\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6807\u7b7e\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u548c\u5b9e\u4f8b\u611f\u77e5\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.13993", "pdf": "https://arxiv.org/pdf/2506.13993", "abs": "https://arxiv.org/abs/2506.13993", "authors": ["Michelangelo Conserva", "Alex Wilson", "Charlotte Stanton", "Vishal Batchu", "Varun Gulshan"], "title": "Mapping Farmed Landscapes from Remote Sensing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\\%) and farmed land (95\\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.", "AI": {"tldr": "Farmscapes\u662f\u9996\u4e2a\u8986\u76d6\u82f1\u683c\u5170\u5927\u90e8\u5206\u5730\u533a\u7684\u9ad8\u5206\u8fa8\u7387\uff0825\u5398\u7c73\uff09\u519c\u6751\u666f\u89c2\u7279\u5f81\u5730\u56fe\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u751f\u6210\uff0c\u4e3a\u751f\u6001\u5b66\u5bb6\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u5f00\u653e\u5de5\u5177\u3002", "motivation": "\u519c\u4e1a\u666f\u89c2\u7684\u6709\u6548\u7ba1\u7406\u5bf9\u5168\u7403\u751f\u7269\u591a\u6837\u6027\u76ee\u6807\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u8be6\u7ec6\u7684\u5927\u89c4\u6a21\u751f\u6001\u5730\u56fe\u963b\u788d\u4e86\u76f8\u5173\u52aa\u529b\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\uff0c\u57fa\u4e8e942\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u822a\u62cd\u56fe\u50cf\u5757\u8bad\u7ec3\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5730\u56fe\u3002", "result": "\u6a21\u578b\u51c6\u786e\u8bc6\u522b\u5173\u952e\u6816\u606f\u5730\uff0c\u6797\u5730\uff0896%\uff09\u548c\u519c\u7530\uff0895%\uff09\u7684F1\u5206\u6570\u9ad8\uff0c\u7ebf\u6027\u7279\u5f81\uff08\u5982\u6811\u7bf1\uff09\u7684F1\u5206\u6570\u4e3a72%\u3002", "conclusion": "\u8be5\u5730\u56fe\u4e3a\u6816\u606f\u5730\u6062\u590d\u7684\u6570\u636e\u9a71\u52a8\u89c4\u5212\u63d0\u4f9b\u652f\u6301\uff0c\u5e76\u4e3a\u666f\u89c2\u8fde\u901a\u6027\u5206\u6790\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2506.14008", "pdf": "https://arxiv.org/pdf/2506.14008", "abs": "https://arxiv.org/abs/2506.14008", "authors": ["Daniel Montoya", "Aymen Bouguerra", "Alexandra Gomez-Villa", "Fabio Arnez"], "title": "FindMeIfYouCan: Bringing Open Set metrics to $\\textit{near} $, $ \\textit{far} $ and $\\textit{farther}$ Out-of-Distribution Object Detection", "categories": ["cs.CV"], "comment": "Preprint", "summary": "State-of-the-art Object Detection (OD) methods predominantly operate under a closed-world assumption, where test-time categories match those encountered during training. However, detecting and localizing unknown objects is crucial for safety-critical applications in domains such as autonomous driving and medical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a vital research direction for OD, focusing on identifying incorrect predictions typically associated with unknown objects. This paper shows that the current evaluation protocol for OOD-OD violates the assumption of non-overlapping objects with respect to the In-Distribution (ID) datasets, and obscures crucial situations such as ignoring unknown objects, potentially leading to overconfidence in deployment scenarios where truly novel objects might be encountered. To address these limitations, we manually curate, and enrich the existing benchmark by exploiting semantic similarity to create new evaluation splits categorized as $\\textit{near}$, $\\textit{far}$, and $\\textit{farther}$ from ID distributions. Additionally, we incorporate established metrics from the Open Set community, providing deeper insights into how effectively methods detect unknowns, when they ignore them, and when they mistakenly classify OOD objects as ID. Our comprehensive evaluation demonstrates that semantically and visually close OOD objects are easier to localize than far ones, but are also more easily confounded with ID objects. $\\textit{Far}$ and $\\textit{farther}$ objects are harder to localize but less prone to be taken for an ID object.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524dOOD-OD\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u7f3a\u9677\uff0c\u63d0\u51fa\u65b0\u57fa\u51c6\u548c\u6307\u6807\u4ee5\u66f4\u5168\u9762\u8bc4\u4f30\u672a\u77e5\u7269\u4f53\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709OOD-OD\u8bc4\u4f30\u534f\u8bae\u4e2d\u5ffd\u7565\u672a\u77e5\u7269\u4f53\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u68c0\u6d4b\u53ef\u9760\u6027\u3002", "method": "\u624b\u52a8\u6784\u5efa\u65b0\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5229\u7528\u8bed\u4e49\u76f8\u4f3c\u6027\u5212\u5206near\u3001far\u3001farther\u7c7b\u522b\uff0c\u5e76\u5f15\u5165\u5f00\u653e\u96c6\u6307\u6807\u3002", "result": "\u8bed\u4e49\u548c\u89c6\u89c9\u63a5\u8fd1\u7684OOD\u7269\u4f53\u66f4\u6613\u5b9a\u4f4d\u4f46\u6613\u4e0eID\u6df7\u6dc6\uff0cfar\u548cfarther\u7269\u4f53\u5b9a\u4f4d\u96be\u4f46\u4e0d\u6613\u8bef\u5224\u3002", "conclusion": "\u65b0\u8bc4\u4f30\u534f\u8bae\u548c\u6307\u6807\u80fd\u66f4\u5168\u9762\u53cd\u6620OOD-OD\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u66f4\u53ef\u9760\u4f9d\u636e\u3002"}}
{"id": "2506.14015", "pdf": "https://arxiv.org/pdf/2506.14015", "abs": "https://arxiv.org/abs/2506.14015", "authors": ["Nick Yiwen Huang", "Akin Caliskan", "Berkay Kicanaoglu", "James Tompkin", "Hyeongwoo Kim"], "title": "Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation", "categories": ["cs.CV"], "comment": null, "summary": "We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e2d\u89e3\u80263D\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u63a7\u76843D\u8096\u50cf\uff0c\u652f\u6301\u6587\u672c\u548c3D\u51e0\u4f55\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u4ece\u9884\u8bad\u7ec3\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89e3\u80263D\u4fe1\u606f\u7684\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u8096\u50cf\u5916\u89c2\u548c\u51e0\u4f55\u7684\u81ea\u7531\u63a7\u5236\u3002", "method": "\u4f7f\u7528\u89c4\u8303\u5316\u65b9\u6cd5\u5c063D\u4fe1\u606f\u89e3\u8026\u52302D\u53c2\u8003\u6846\u67b6\uff0c\u5e76\u901a\u8fc7Jacobian\u6b63\u5219\u5316\u5904\u7406LVLM\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u566a\u58f0\u3002", "result": "\u751f\u6210\u7684\u8096\u50cf\u5728\u6587\u672c\u548c3D\u63a7\u5236\u4e0b\u4fdd\u6301\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u521b\u4f5c\u8005\u80fd\u591f\u5229\u7528\u81ea\u5df1\u76842D\u6570\u636e\u63a7\u52363D\u751f\u6210\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6807\u6ce8\u6216\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u3002"}}
{"id": "2506.14035", "pdf": "https://arxiv.org/pdf/2506.14035", "abs": "https://arxiv.org/abs/2506.14035", "authors": ["Chelsi Jain", "Yiran Wu", "Yifan Zeng", "Jiale Liu", "S hengyu Dai", "Zhenwen Shao", "Qingyun Wu", "Huazheng Wang"], "title": "SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.", "AI": {"tldr": "SimpleDoc\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u6863\u89c6\u89c9\u95ee\u7b54\uff08DocVQA\uff09\uff0c\u901a\u8fc7\u53cc\u7ebf\u7d22\u68c0\u7d22\u5668\u548c\u8fed\u4ee3\u5de5\u4f5c\u8bb0\u5fc6\u673a\u5236\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "DocVQA\u4efb\u52a1\u9700\u8981\u5904\u7406\u591a\u9875\u548c\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u91c7\u7528RAG\u6d41\u7a0b\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "SimpleDoc\u901a\u8fc7\u5d4c\u5165\u76f8\u4f3c\u6027\u68c0\u7d22\u5019\u9009\u9875\u9762\uff0c\u518d\u57fa\u4e8e\u9875\u9762\u6458\u8981\u8fc7\u6ee4\u548c\u91cd\u6392\u5e8f\uff0c\u5229\u7528VLM\u63a8\u7406\u4ee3\u7406\u8fed\u4ee3\u68c0\u7d22\u65b0\u9875\u9762\u3002", "result": "SimpleDoc\u57284\u4e2aDocVQA\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u53473.2%\uff0c\u4e14\u68c0\u7d22\u9875\u9762\u66f4\u5c11\u3002", "conclusion": "SimpleDoc\u901a\u8fc7\u9ad8\u6548\u7684\u53cc\u7ebf\u7d22\u68c0\u7d22\u548c\u8fed\u4ee3\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86DocVQA\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.14096", "pdf": "https://arxiv.org/pdf/2506.14096", "abs": "https://arxiv.org/abs/2506.14096", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u7ed3\u5408\u5728\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u4e2d\u7684\u6f5c\u529b\u3001\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u9700\u8981\u9ad8\u7cbe\u5ea6\u7684\u573a\u666f\u7406\u89e3\u4ee5\u786e\u4fdd\u5b89\u5168\u548c\u6548\u7387\uff0cLLMs\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u7ed3\u5408\u4e3a\u6b64\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86LLM\u589e\u5f3a\u56fe\u50cf\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u63d0\u793a\u673a\u5236\u548c\u6838\u5fc3\u67b6\u6784\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "\u8fd9\u4e9b\u521b\u65b0\u6280\u672f\u53ef\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u3001\u4ea4\u901a\u76d1\u63a7\u548c\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u4e2d\u7684\u9053\u8def\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u672a\u6765\u9700\u89e3\u51b3\u5b9e\u65f6\u6027\u548c\u5b89\u5168\u6027\u7b49\u5173\u952e\u6311\u6218\uff0c\u5e76\u53d1\u5c55\u53ef\u89e3\u91ca\u3001\u4ee5\u4eba\u4e3a\u672c\u7684AI\u6280\u672f\uff0c\u4ee5\u63a8\u52a8\u5176\u5728\u4e0b\u4e00\u4ee3\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002"}}
{"id": "2506.14121", "pdf": "https://arxiv.org/pdf/2506.14121", "abs": "https://arxiv.org/abs/2506.14121", "authors": ["Siyu Xu", "Wenjie Li", "Guangwei Gao", "Jian Yang", "Guo-Jun Qi", "Chia-Wen Lin"], "title": "FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution", "categories": ["cs.CV"], "comment": "12 pages, 11 figures, 6 tales", "summary": "Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387\u611f\u77e5\u53cc\u8def\u5f84\u7f51\u7edc\uff08FADPNet\uff09\uff0c\u901a\u8fc7\u5206\u89e3\u9ad8\u4f4e\u9891\u7279\u5f81\u5e76\u5206\u522b\u5904\u7406\uff0c\u4f18\u5316\u4e86\u4eba\u8138\u8d85\u5206\u8fa8\u7387\uff08FSR\uff09\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u6240\u6709\u9762\u90e8\u50cf\u7d20\u4e00\u89c6\u540c\u4ec1\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u4e0d\u4f18\u548c\u6027\u80fd\u4e0b\u964d\u3002CNN\u5bf9\u9ad8\u9891\u7279\u5f81\u654f\u611f\uff0c\u800cMamba\u64c5\u957f\u4f4e\u9891\u7279\u5f81\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "method": "FADPNet\u5c06\u7279\u5f81\u5206\u89e3\u4e3a\u9ad8\u4f4e\u9891\uff0c\u5206\u522b\u7528Mamba\u548cCNN\u5904\u7406\u3002\u4f4e\u9891\u5206\u652f\u91c7\u7528LFEB\u6a21\u5757\uff0c\u9ad8\u9891\u5206\u652f\u91c7\u7528DPA\u548cHFR\u6a21\u5757\u3002", "result": "\u65b9\u6cd5\u5728FSR\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FADPNet\u901a\u8fc7\u9891\u7387\u611f\u77e5\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86FSR\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.14130", "pdf": "https://arxiv.org/pdf/2506.14130", "abs": "https://arxiv.org/abs/2506.14130", "authors": ["Chunyu Cao", "Jintao Cheng", "Zeyu Chen", "Linfan Zhan", "Rui Fan", "Zhijian He", "Xiaoyu Tang"], "title": "KDMOS:Knowledge Distillation for Motion Segmentation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8elogits\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff08KDMOS\uff09\uff0c\u7528\u4e8e\u8fd0\u52a8\u76ee\u6807\u5206\u5272\uff08MOS\uff09\uff0c\u901a\u8fc7BEV\u6295\u5f71\u6a21\u578b\uff08\u5b66\u751f\uff09\u548c\u975e\u6295\u5f71\u6a21\u578b\uff08\u6559\u5e08\uff09\u7ed3\u5408\uff0c\u4f18\u5316\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u4e14\u8fd0\u52a8\u4e0e\u975e\u8fd0\u52a8\u7c7b\u522b\u4e25\u91cd\u4e0d\u5e73\u8861\u3002", "method": "\u91c7\u7528BEV\u6295\u5f71\u6a21\u578b\u4f5c\u4e3a\u5b66\u751f\uff0c\u975e\u6295\u5f71\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\uff0c\u89e3\u8026\u8fd0\u52a8\u4e0e\u975e\u8fd0\u52a8\u7c7b\u522b\u5e76\u5e94\u7528\u5b9a\u5236\u84b8\u998f\u7b56\u7565\uff0c\u5f15\u5165\u52a8\u6001\u4e0a\u91c7\u6837\u548c\u7f51\u7edc\u67b6\u6784\u4f18\u5316\u3002", "result": "\u5728SemanticKITTI-MOS\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523078.8%\u7684IoU\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c117.69%\uff0c\u5728Apollo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "KDMOS\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86MOS\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.14136", "pdf": "https://arxiv.org/pdf/2506.14136", "abs": "https://arxiv.org/abs/2506.14136", "authors": ["Nafiz Sadman", "Farhana Zulkernine", "Benjamin Kwan"], "title": "Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology", "categories": ["cs.CV"], "comment": "GitHub: https://github.com/Nafiz95/BioVLM_Eval_CXR", "summary": "In this paper, we construct two research objectives: i) explore the learned embedding space of BiomedCLIP, an open-source large vision language model, to analyse meaningful class separations, and ii) quantify the limitations of BiomedCLIP when applied to a highly imbalanced, out-of-distribution multi-label medical dataset. We experiment on IU-xray dataset, which exhibits the aforementioned criteria, and evaluate BiomedCLIP in classifying images (radiographs) in three contexts: zero-shot inference, full finetuning, and linear probing. The results show that the model under zero-shot settings over-predicts all labels, leading to poor precision and inter-class separability. Full fine-tuning improves classification of distinct diseases, while linear probing detects overlapping features. We demonstrate visual understanding of the model using Grad-CAM heatmaps and compare with 15 annotations by a radiologist. We highlight the need for careful adaptations of the models to foster reliability and applicability in a real-world setting. The code for the experiments in this work is available and maintained on GitHub.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86BiomedCLIP\u5728\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u4e86\u5176\u5d4c\u5165\u7a7a\u95f4\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76BiomedCLIP\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u3001\u5206\u5e03\u5916\u591a\u6807\u7b7e\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4ee5\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u5728IU-xray\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u96f6\u6837\u672c\u63a8\u7406\u3001\u5168\u5fae\u8c03\u548c\u7ebf\u6027\u63a2\u6d4b\u4e09\u79cd\u5206\u7c7b\u65b9\u5f0f\uff0c\u5e76\u4f7f\u7528Grad-CAM\u70ed\u56fe\u53ef\u89c6\u5316\u6a21\u578b\u7406\u89e3\u3002", "result": "\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6a21\u578b\u9884\u6d4b\u8fc7\u5ea6\uff0c\u7cbe\u5ea6\u4f4e\uff1b\u5168\u5fae\u8c03\u6539\u5584\u75be\u75c5\u5206\u7c7b\uff1b\u7ebf\u6027\u63a2\u6d4b\u68c0\u6d4b\u91cd\u53e0\u7279\u5f81\u3002", "conclusion": "\u9700\u8c28\u614e\u8c03\u6574\u6a21\u578b\u4ee5\u63d0\u9ad8\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2506.14142", "pdf": "https://arxiv.org/pdf/2506.14142", "abs": "https://arxiv.org/abs/2506.14142", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "categories": ["cs.CV", "cs.CL"], "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.", "AI": {"tldr": "RadFabric\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u591a\u4ee3\u7406\u7684CXR\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u548c\u6587\u672c\u63a8\u7406\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316CXR\u7cfb\u7edf\u5728\u75c5\u7406\u8986\u76d6\u3001\u8bca\u65ad\u51c6\u786e\u6027\u53ca\u89c6\u89c9\u4e0e\u6587\u672c\u63a8\u7406\u6574\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8eModel Context Protocol (MCP)\uff0c\u7ed3\u5408\u75c5\u7406\u68c0\u6d4b\u4ee3\u7406\u3001\u89e3\u5256\u89e3\u91ca\u4ee3\u7406\u548c\u63a8\u7406\u4ee3\u7406\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u3002", "result": "\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6027\u80fd\uff0c\u5982\u9aa8\u6298\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe1.000\uff0c\u6574\u4f53\u8bca\u65ad\u51c6\u786e\u73870.799\uff0c\u8fdc\u8d85\u4f20\u7edf\u7cfb\u7edf\u3002", "conclusion": "RadFabric\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u504f\u597d\u9a71\u52a8\u63a8\u7406\uff0c\u63a8\u52a8\u4e86\u900f\u660e\u3001\u7cbe\u51c6\u4e14\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684CXR\u5206\u6790\u3002"}}
{"id": "2506.14144", "pdf": "https://arxiv.org/pdf/2506.14144", "abs": "https://arxiv.org/abs/2506.14144", "authors": ["Juho Bai", "Inwook Shim"], "title": "SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.", "AI": {"tldr": "SceneAware\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u573a\u666f\u7406\u89e3\u548c\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728ETH/UCY\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u884c\u4eba\u95f4\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u4f46\u5ffd\u7565\u4e86\u73af\u5883\u80cc\u666f\u5bf9\u884c\u4eba\u8f68\u8ff9\u7684\u91cd\u8981\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Vision Transformer\uff08ViT\uff09\u7f16\u7801\u573a\u666f\u4fe1\u606f\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u751f\u6210\u53ef\u901a\u884c\u533a\u57df\u63a9\u7801\uff0c\u5e76\u901a\u8fc7Transformer\u7f16\u7801\u8f68\u8ff9\uff0c\u540c\u65f6\u5f15\u5165\u78b0\u649e\u60e9\u7f5a\u673a\u5236\u3002", "result": "\u5728ETH/UCY\u6570\u636e\u96c6\u4e0a\uff0cSceneAware\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u534750%\u4ee5\u4e0a\uff0c\u4e14\u5728\u4e0d\u540c\u7c7b\u578b\u884c\u4eba\u8fd0\u52a8\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u663e\u5f0f\u5229\u7528\u573a\u666f\u4fe1\u606f\u80fd\u6709\u6548\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\uff0cSceneAware\u6846\u67b6\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.14168", "pdf": "https://arxiv.org/pdf/2506.14168", "abs": "https://arxiv.org/abs/2506.14168", "authors": ["Hu Yu", "Biao Gong", "Hangjie Yuan", "DanDan Zheng", "Weilong Chai", "Jingdong Chen", "Kecheng Zheng", "Feng Zhao"], "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to NeurIPS 2025", "summary": "Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \\textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\\%$), training data ($0.5\\%$), and GPU resources ($0.2\\%$).", "AI": {"tldr": "VideoMAR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u56e0\u679c\u6027\u548c\u7a7a\u95f4\u53cc\u5411\u6027\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u548c\u6e10\u8fdb\u7b56\u7565\u4f18\u5316\u957f\u5e8f\u5217\u5efa\u6a21\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u9ad8\u6210\u672c\u548c\u96be\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51faVideoMAR\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u95f4\u56e0\u679c\u6027\u548c\u7a7a\u95f4\u53cc\u5411\u6027\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u3001\u6e10\u8fdb\u5206\u8fa8\u7387\u548c\u6e29\u5ea6\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728VBench-I2V\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideoMAR\u6027\u80fd\u4f18\u4e8eCosmos I2V\uff0c\u4e14\u53c2\u6570\u3001\u8bad\u7ec3\u6570\u636e\u548cGPU\u8d44\u6e90\u9700\u6c42\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "VideoMAR\u5c55\u793a\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.14170", "pdf": "https://arxiv.org/pdf/2506.14170", "abs": "https://arxiv.org/abs/2506.14170", "authors": ["Shulong Zhang", "Mingyuan Yao", "Jiayin Zhao", "Xiao Liu", "Haihua Wang"], "title": "A multi-stage augmented multimodal interaction network for fish feeding intensity quantification", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u589e\u5f3a\u591a\u6a21\u6001\u4ea4\u4e92\u7f51\u7edc\uff08MAINet\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u3001\u4ea4\u4e92\u589e\u5f3a\u548c\u8bc1\u636e\u63a8\u7406\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5728\u591a\u6a21\u6001\u9009\u62e9\u3001\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u4ee5\u53ca\u51b3\u7b56\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u7684\u51c6\u786e\u6027\u3001\u9002\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faMAINet\uff0c\u5305\u62ec\u901a\u7528\u7279\u5f81\u63d0\u53d6\u6846\u67b6\u3001\u8f85\u52a9\u6a21\u6001\u589e\u5f3a\u4e3b\u6a21\u6001\u673a\u5236\uff08ARPM\uff09\u548c\u8bc1\u636e\u63a8\u7406\uff08ER\uff09\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMAINet\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u8d85\u8fc796.7%\uff0c\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u6a21\u578b\u3002", "conclusion": "MAINet\u901a\u8fc7\u6539\u8fdb\u7b56\u7565\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u7279\u5f81\u5229\u7528\u7387\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\u91cf\u5316\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.14176", "pdf": "https://arxiv.org/pdf/2506.14176", "abs": "https://arxiv.org/abs/2506.14176", "authors": ["Renao Yan"], "title": "One-Shot Neural Architecture Search with Network Similarity Directed Initialization for Pathological Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based pathological image analysis presents unique challenges due to the practical constraints of network design. Most existing methods apply computer vision models directly to medical tasks, neglecting the distinct characteristics of pathological images. This mismatch often leads to computational inefficiencies, particularly in edge-computing scenarios. To address this, we propose a novel Network Similarity Directed Initialization (NSDI) strategy to improve the stability of neural architecture search (NAS). Furthermore, we introduce domain adaptation into one-shot NAS to better handle variations in staining and semantic scale across pathology datasets. Experiments on the BRACS dataset demonstrate that our method outperforms existing approaches, delivering both superior classification performance and clinically relevant feature localization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u76f8\u4f3c\u6027\u5f15\u5bfc\u521d\u59cb\u5316\uff08NSDI\uff09\u7684\u7b56\u7565\uff0c\u7ed3\u5408\u9886\u57df\u81ea\u9002\u5e94\u7684\u4e00\u952e\u5f0f\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\uff0c\u4ee5\u63d0\u5347\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5230\u533b\u5b66\u4efb\u52a1\u4e2d\uff0c\u5ffd\u89c6\u4e86\u75c5\u7406\u56fe\u50cf\u7684\u72ec\u7279\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f15\u5165NSDI\u7b56\u7565\u63d0\u5347NAS\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u7ed3\u5408\u9886\u57df\u81ea\u9002\u5e94\u5904\u7406\u75c5\u7406\u6570\u636e\u96c6\u4e2d\u7684\u67d3\u8272\u548c\u8bed\u4e49\u5c3a\u5ea6\u53d8\u5316\u3002", "result": "\u5728BRACS\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u6027\u80fd\u548c\u4e34\u5e8a\u76f8\u5173\u7279\u5f81\u5b9a\u4f4d\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u7406\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.14181", "pdf": "https://arxiv.org/pdf/2506.14181", "abs": "https://arxiv.org/abs/2506.14181", "authors": ["Yufei Li", "Jirui Wu", "Long Tian", "Liming Wang", "Xiaonan Liu", "Zijun Liu", "Xiyang Liu"], "title": "Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition", "categories": ["cs.CV"], "comment": "15 pages, 5 figures", "summary": "Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical phases.We establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u4f18\u5316\u7684\u5206\u7c7b\u6269\u6563\u6a21\u578b\uff08Meta-SurDiff\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5305\u62ec\u89c6\u9891\u5e27\u6a21\u7cca\u548c\u624b\u672f\u9636\u6bb5\u5206\u5e03\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u5bf9\u751f\u547d\u5065\u5eb7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u672a\u5145\u5206\u63a2\u7d22\u624b\u672f\u89c6\u9891\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5f71\u54cd\u8bc6\u522b\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6269\u6563\u6a21\u578b\u8bc4\u4f30\u6a21\u7cca\u89c6\u9891\u5e27\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u5229\u7528\u5143\u5b66\u4e60\u4f18\u5316\u6269\u6563\u6a21\u578b\u4ee5\u589e\u5f3a\u5206\u7c7b\u8fb9\u754c\u5bf9\u4e0d\u540c\u624b\u672f\u9636\u6bb5\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\uff08Cholec80\u3001AutoLaparo\u3001M2Cai16\u3001OphNet\u3001NurViD\uff09\u4e0a\u9a8c\u8bc1\u4e86Meta-SurDiff\u7684\u6709\u6548\u6027\u3002", "conclusion": "Meta-SurDiff\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u5143\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.14189", "pdf": "https://arxiv.org/pdf/2506.14189", "abs": "https://arxiv.org/abs/2506.14189", "authors": ["Kunyuan Deng", "Yi Wang", "Lap-Pui Chau"], "title": "Egocentric Human-Object Interaction Detection: A New Benchmark and Method", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: https://dengkunyuan.github.io/EgoHOIBench/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6Ego-HOIBench\uff0c\u7528\u4e8e\u63a8\u52a8\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u89c6\u89d2\u7684\u4eba-\u7269\u4ea4\u4e92\uff08Ego-HOI\uff09\u68c0\u6d4b\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684HGIR\u65b9\u6848\u6765\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\uff0c\u5ffd\u7565\u4e86\u66f4\u76f4\u89c2\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\uff08Ego-HOI\uff09\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aEgo-HOIBench\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5305\u542b27K+\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u4e86HGIR\u65b9\u6848\uff0c\u5229\u7528\u624b\u90e8\u51e0\u4f55\u548c\u4ea4\u4e92\u4fe1\u606f\u4f18\u5316\u4ea4\u4e92\u7279\u5f81\u3002", "result": "HGIR\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86Ego-HOI\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5728Ego-HOIBench\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "Ego-HOIBench\u548cHGIR\u65b9\u6848\u4e3aEgo-HOI\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.14229", "pdf": "https://arxiv.org/pdf/2506.14229", "abs": "https://arxiv.org/abs/2506.14229", "authors": ["Changbai Li", "Haodong Zhu", "Hanlin Chen", "Juan Zhang", "Tongfei Chen", "Shuo Yang", "Shuwei Shao", "Wenhao Dong", "Baochang Zhang"], "title": "HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.", "AI": {"tldr": "HRGS\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u5206\u5c42\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5757\u7ea7\u4f18\u5316\u89e3\u51b33DGS\u5728\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e2d\u7684\u5185\u5b58\u6269\u5c55\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u5b9e\u65f63D\u573a\u666f\u91cd\u5efa\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e0b\u9762\u4e34\u5185\u5b58\u6269\u5c55\u95ee\u9898\u3002", "method": "\u9996\u5148\u751f\u6210\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u7684\u5168\u5c40\u7c97\u7565\u9ad8\u65af\u8868\u793a\uff0c\u7136\u540e\u5c06\u573a\u666f\u5212\u5206\u4e3a\u591a\u4e2a\u5757\uff0c\u7528\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u7ec6\u5316\u6bcf\u4e2a\u5757\uff0c\u5e76\u901a\u8fc7\u91cd\u8981\u6027\u9a71\u52a8\u7684\u9ad8\u65af\u4fee\u526a\uff08IDGP\uff09\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHRGS\u5728\u9ad8\u5206\u8fa8\u7387\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u548c\u8868\u9762\u91cd\u5efa\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HRGS\u5728\u5185\u5b58\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u76843D\u573a\u666f\u91cd\u5efa\u3002"}}
{"id": "2506.14238", "pdf": "https://arxiv.org/pdf/2506.14238", "abs": "https://arxiv.org/abs/2506.14238", "authors": ["Yinuo Zheng", "Lipeng Gu", "Honghua Chen", "Liangliang Nan", "Mingqiang Wei"], "title": "Unified Representation Space for 3D Visual Grounding", "categories": ["cs.CV"], "comment": null, "summary": "3D visual grounding (3DVG) is a critical task in scene understanding that aims to identify objects in 3D scenes based on text descriptions. However, existing methods rely on separately pre-trained vision and text encoders, resulting in a significant gap between the two modalities in terms of spatial geometry and semantic categories. This discrepancy often causes errors in object positioning and classification. The paper proposes UniSpace-3D, which innovatively introduces a unified representation space for 3DVG, effectively bridging the gap between visual and textual features. Specifically, UniSpace-3D incorporates three innovative designs: i) a unified representation encoder that leverages the pre-trained CLIP model to map visual and textual features into a unified representation space, effectively bridging the gap between the two modalities; ii) a multi-modal contrastive learning module that further reduces the modality gap; iii) a language-guided query selection module that utilizes the positional and semantic information to identify object candidate points aligned with textual descriptions. Extensive experiments demonstrate that UniSpace-3D outperforms baseline models by at least 2.24% on the ScanRefer and Nr3D/Sr3D datasets. The code will be made available upon acceptance of the paper.", "AI": {"tldr": "UniSpace-3D\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7CLIP\u6a21\u578b\u548c\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\uff0c\u5bfc\u81f4\u6a21\u6001\u95f4\u5728\u7a7a\u95f4\u51e0\u4f55\u548c\u8bed\u4e49\u7c7b\u522b\u4e0a\u7684\u5dee\u8ddd\uff0c\u5f71\u54cd\u5b9a\u4f4d\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "1) \u4f7f\u7528CLIP\u6a21\u578b\u7684\u7edf\u4e00\u8868\u793a\u7f16\u7801\u5668\uff1b2) \u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff1b3) \u8bed\u8a00\u5f15\u5bfc\u7684\u67e5\u8be2\u9009\u62e9\u6a21\u5757\u3002", "result": "\u5728ScanRefer\u548cNr3D/Sr3D\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u5347\u81f3\u5c112.24%\u3002", "conclusion": "UniSpace-3D\u901a\u8fc7\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\u548c\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\u3002"}}
{"id": "2506.14243", "pdf": "https://arxiv.org/pdf/2506.14243", "abs": "https://arxiv.org/abs/2506.14243", "authors": ["Xiaohui Jiang", "Haijiang Zhu", "Chadei Li", "Fulin Tang", "Ning An"], "title": "Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u65e0\u5173\u51e0\u4f55\u63a8\u7406\u76843D\u5730\u70b9\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5f39\u6027\u70b9\u9690\u5f0f\u8868\u793a\u89e3\u51b3\u4e86\u70b9\u4e91\u5bc6\u5ea6\u4e0d\u4e00\u81f4\u548c\u5355\u5c42\u51e0\u4f55\u62bd\u8c61\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u63d0\u53d6\uff0c\u9762\u4e34\u70b9\u4e91\u5bc6\u5ea6\u4e0d\u4e00\u81f4\u548c\u5355\u5c42\u51e0\u4f55\u62bd\u8c61\u5bfc\u81f4\u7684\u63cf\u8ff0\u7b26\u4e0d\u7a33\u5b9a\u548c\u8868\u793a\u8106\u5f31\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5f39\u6027\u70b9\u9690\u5f0f\u8868\u793a\uff0c\u751f\u6210\u5747\u5300\u5206\u5e03\u7684\u70b9\u4e91\uff0c\u5e76\u4ece\u4e2d\u63d0\u53d6\u5360\u7528\u7f51\u683c\u548c\u6cd5\u5411\u91cf\u4fe1\u606f\uff0c\u878d\u5408\u9e1f\u77b0\u56fe\u548c3D\u7247\u6bb5\u7684\u51e0\u4f55\u4fe1\u606f\u751f\u6210\u63cf\u8ff0\u7b26\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u4f18\u5316\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u5f02\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u4ee3\u7801\u3002"}}
{"id": "2506.14255", "pdf": "https://arxiv.org/pdf/2506.14255", "abs": "https://arxiv.org/abs/2506.14255", "authors": ["Johannes Flotzinger", "Fabian Deuser", "Achref Jaziri", "Heiko Neumann", "Norbert Oswald", "Visvanathan Ramesh", "Thomas Braml"], "title": "synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?", "categories": ["cs.CV"], "comment": null, "summary": "Adequate bridge inspection is increasingly challenging in many countries due to growing ailing stocks, compounded with a lack of staff and financial resources. Automating the key task of visual bridge inspection, classification of defects and building components on pixel level, improves efficiency, increases accuracy and enhances safety in the inspection process and resulting building assessment. Models overtaking this task must cope with an assortment of real-world conditions. They must be robust to variations in image quality, as well as background texture, as defects often appear on surfaces of diverse texture and degree of weathering. dacl10k is the largest and most diverse dataset for real-world concrete bridge inspections. However, the dataset exhibits class imbalance, which leads to notably poor model performance particularly when segmenting fine-grained classes such as cracks and cavities. This work introduces \"synth-dacl\", a compilation of three novel dataset extensions based on synthetic concrete textures. These extensions are designed to balance class distribution in dacl10k and enhance model performance, especially for crack and cavity segmentation. When incorporating the synth-dacl extensions, we observe substantial improvements in model robustness across 15 perturbed test sets. Notably, on the perturbed test set, a model trained on dacl10k combined with all synthetic extensions achieves a 2% increase in mean IoU, F1 score, Recall, and Precision compared to the same model trained solely on dacl10k.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u6269\u5c55\u65b9\u6cd5\uff08synth-dacl\uff09\uff0c\u7528\u4e8e\u6539\u5584\u6865\u6881\u89c6\u89c9\u68c0\u6d4b\u4e2d\u7f3a\u9677\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u88c2\u7f1d\u548c\u7a7a\u6d1e\u7b49\u7ec6\u7c92\u5ea6\u7c7b\u522b\u4e0a\u3002", "motivation": "\u6865\u6881\u68c0\u6d4b\u9762\u4e34\u8d44\u6e90\u4e0d\u8db3\u548c\u8001\u5316\u95ee\u9898\uff0c\u81ea\u52a8\u5316\u89c6\u89c9\u68c0\u6d4b\u80fd\u63d0\u5347\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\uff08dacl10k\uff09\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5408\u6210\u6df7\u51dd\u571f\u7eb9\u7406\u6570\u636e\u6269\u5c55\u6570\u636e\u96c6\uff08synth-dacl\uff09\uff0c\u5e73\u8861\u7c7b\u522b\u5206\u5e03\uff0c\u5e76\u6d4b\u8bd5\u5176\u572815\u79cd\u6270\u52a8\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u5408synth-dacl\u6269\u5c55\u7684\u6a21\u578b\u5728\u6270\u52a8\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5e73\u5747IoU\u3001F1\u5206\u6570\u3001\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u5747\u63d0\u53472%\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u6269\u5c55\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6865\u6881\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2506.14256", "pdf": "https://arxiv.org/pdf/2506.14256", "abs": "https://arxiv.org/abs/2506.14256", "authors": ["Deepak Ghimire", "Joonwhoan Lee"], "title": "Comparison of Two Methods for Stationary Incident Detection Based on Background Image", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u80cc\u666f\u51cf\u6cd5\u7684\u65b9\u6848\uff0c\u7528\u4e8e\u68c0\u6d4b\u89c6\u9891\u4e2d\u6682\u65f6\u9759\u6b62\u7684\u7269\u4f53\uff0c\u5e76\u901a\u8fc7NCC\u56fe\u50cf\u6bd4\u8f83\u8fdb\u884c\u8ddf\u8e2a\uff0c\u65b9\u6cd5\u5bf9\u90e8\u5206\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u80cc\u666f\u51cf\u6cd5\u4e3b\u8981\u7528\u4e8e\u68c0\u6d4b\u8fd0\u52a8\u7269\u4f53\uff0c\u800c\u672c\u6587\u65e8\u5728\u68c0\u6d4b\u6682\u65f6\u9759\u6b62\u7684\u7269\u4f53\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6848\uff1a\u5355\u80cc\u666f\u6cd5\u548c\u53cc\u80cc\u666f\u6cd5\uff08\u4e0d\u540c\u5b66\u4e60\u7387\u751f\u6210\uff09\uff0c\u5e76\u901a\u8fc7NCC\u56fe\u50cf\u6bd4\u8f83\u8ddf\u8e2a\u9759\u6b62\u7269\u4f53\u3002", "result": "\u65b9\u6cd5\u5bf9\u90e8\u5206\u906e\u6321\u3001\u77ed\u65f6\u5b8c\u5168\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u53cc\u80cc\u666f\u6cd5\u5728\u68c0\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.14265", "pdf": "https://arxiv.org/pdf/2506.14265", "abs": "https://arxiv.org/abs/2506.14265", "authors": ["Siran Dai", "Qianqian Xu", "Peisong Wen", "Yang Liu", "Qingming Huang"], "title": "Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling", "categories": ["cs.CV"], "comment": "CVPR 2025 Computer Vision for Drug Discovery", "summary": "Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSLProfiler\u7684\u975e\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u7ec6\u80de\u56fe\u50cf\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u7ec6\u80de\u56fe\u50cf\u4e0e\u81ea\u7136\u56fe\u50cf\u5206\u5e03\u5dee\u5f02\u5927\u4ee5\u53ca\u591a\u56fe\u50cf\u4fe1\u606f\u878d\u5408\u7684\u6311\u6218\u3002", "motivation": "\u7ec6\u80de\u56fe\u50cf\u5206\u6790\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u56e0\u7ec6\u80de\u56fe\u50cf\u4e0e\u81ea\u7136\u56fe\u50cf\u5206\u5e03\u5dee\u5f02\u5927\u4ee5\u53ca\u591a\u56fe\u50cf\u4fe1\u606f\u878d\u5408\u56f0\u96be\u800c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSSLProfiler\u6846\u67b6\uff0c\u5f15\u5165\u9488\u5bf9\u7ec6\u80de\u56fe\u50cf\u7684\u4e13\u7528\u6570\u636e\u589e\u5f3a\u548c\u8868\u793a\u540e\u5904\u7406\u65b9\u6cd5\u3002", "result": "SSLProfiler\u5728CVPR 2025\u7684Cell Line Transferability\u6311\u6218\u4e2d\u83b7\u80dc\u3002", "conclusion": "SSLProfiler\u4e3a\u7ec6\u80de\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9c81\u68d2\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002"}}
{"id": "2506.14271", "pdf": "https://arxiv.org/pdf/2506.14271", "abs": "https://arxiv.org/abs/2506.14271", "authors": ["Weiming Zhang", "Dingwen Xiao", "Aobotao Dai", "Yexin Liu", "Tianbo Pan", "Shiqi Wen", "Lei Chen", "Lin Wang"], "title": "Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment", "categories": ["cs.CV"], "comment": "23 pages, 16 figures", "summary": "360 video captures the complete surrounding scenes with the ultra-large field of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation and tracking, crucial for appications, such as autonomous driving, robotics. With the recent emergence of foundation models, the community is, however, impeded by the lack of large-scale, labelled real-world datasets. This is caused by the inherent spherical properties, eg, severe distortion in polar regions, and content discontinuities, rendering the annotation costly yet complex. This paper introduces Leader360V, the first large-scale, labeled real-world 360 video datasets for instance segmentation and tracking. Our datasets enjoy high scene diversity, ranging from indoor and urban settings to natural and dynamic outdoor scenes. To automate annotation, we design an automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors and large language models to facilitate the labeling. The pipeline operates in three novel stages. Specifically, in the Initial Annotation Phase, we introduce a Semantic- and Distortion-aware Refinement module, which combines object mask proposals from multiple 2D segmentors with LLM-verified semantic labels. These are then converted into mask prompts to guide SAM2 in generating distortion-aware masks for subsequent frames. In the Auto-Refine Annotation Phase, missing or incomplete regions are corrected either by applying the SDR again or resolving the discontinuities near the horizontal borders. The Manual Revision Phase finally incorporates LLMs and human annotators to further refine and validate the annotations. Extensive user studies and evaluations demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments confirm that Leader360V significantly enhances model performance for 360 video segmentation and tracking, paving the way for more scalable 360 scene understanding.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Leader360V\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u6807\u6ce8\u7684\u771f\u5b9e\u4e16\u754c360\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b9e\u4f8b\u5206\u5272\u548c\u8ddf\u8e2a\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\u3002", "motivation": "360\u89c6\u9891\u7684\u7403\u5f62\u7279\u6027\u5bfc\u81f4\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u590d\u6742\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u963b\u788d\u4e86\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e09\u9636\u6bb5\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff1a\u521d\u59cb\u6807\u6ce8\u9636\u6bb5\u7ed3\u54082D\u5206\u5272\u5668\u548cLLM\u751f\u6210\u8bed\u4e49\u6807\u7b7e\uff1b\u81ea\u52a8\u7ec6\u5316\u9636\u6bb5\u4fee\u6b63\u7f3a\u5931\u533a\u57df\uff1b\u4eba\u5de5\u4fee\u8ba2\u9636\u6bb5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002", "result": "\u6807\u6ce8\u6d41\u7a0b\u9ad8\u6548\uff0cLeader360V\u663e\u8457\u63d0\u5347\u4e86360\u89c6\u9891\u5206\u5272\u548c\u8ddf\u8e2a\u7684\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "Leader360V\u4e3a360\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\u548c\u6807\u6ce8\u65b9\u6cd5\u3002"}}
{"id": "2506.14322", "pdf": "https://arxiv.org/pdf/2506.14322", "abs": "https://arxiv.org/abs/2506.14322", "authors": ["Avigail Cohen Rimon", "Mirela Ben-Chen", "Or Litany"], "title": "FRIDU: Functional Map Refinement with Guided Image Diffusion", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to SGP 2025 (Symposium on Geometry Processing)", "summary": "We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u529f\u80fd\u6620\u5c04\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u76f4\u63a5\u5728\u529f\u80fd\u6620\u5c04\u7a7a\u95f4\u4e2d\u8fdb\u884c\uff0c\u9ad8\u6548\u4e14\u80fd\u751f\u6210\u7cbe\u786e\u7684\u6620\u5c04\u3002", "motivation": "\u529f\u80fd\u6620\u5c04\u901a\u5e38\u8868\u793a\u4e3a\u57fa\u53d8\u6362\u77e9\u9635\uff0c\u53ef\u4ee5\u89c6\u4e3a2D\u56fe\u50cf\uff0c\u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f18\u5316\u521d\u59cb\u4e0d\u51c6\u786e\u7684\u6620\u5c04\u3002", "method": "\u5728\u529f\u80fd\u6620\u5c04\u7a7a\u95f4\u8bad\u7ec3\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u63a8\u7406\u65f6\u5229\u7528\u70b9\u6620\u5c04\u4f5c\u4e3a\u6307\u5bfc\uff0c\u540c\u65f6\u652f\u6301\u6b63\u4ea4\u6027\u548cLaplace-Beltrami\u7b97\u5b50\u4ea4\u6362\u6027\u7b49\u529f\u80fd\u6620\u5c04\u76ee\u6807\u3002", "result": "\u65b9\u6cd5\u5728\u6620\u5c04\u4f18\u5316\u65b9\u9762\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u7ade\u4e89\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u529f\u80fd\u6620\u5c04\u5904\u7406\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u4e3a\u529f\u80fd\u6620\u5c04\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.14350", "pdf": "https://arxiv.org/pdf/2506.14350", "abs": "https://arxiv.org/abs/2506.14350", "authors": ["Zoubida Ameur", "Fr\u00e9d\u00e9ric Lefebvre", "Philippe De Lagrange", "Milo\u0161 Radosavljevi\u0107"], "title": "FGA-NN: Film Grain Analysis Neural Network", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Film grain, once a by-product of analog film, is now present in most cinematographic content for aesthetic reasons. However, when such content is compressed at medium to low bitrates, film grain is lost due to its random nature. To preserve artistic intent while compressing efficiently, film grain is analyzed and modeled before encoding and synthesized after decoding. This paper introduces FGA-NN, the first learning-based film grain analysis method to estimate conventional film grain parameters compatible with conventional synthesis. Quantitative and qualitative results demonstrate FGA-NN's superior balance between analysis accuracy and synthesis complexity, along with its robustness and applicability.", "AI": {"tldr": "FGA-NN\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u80f6\u7247\u9897\u7c92\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u538b\u7f29\u540e\u6062\u590d\u80f6\u7247\u9897\u7c92\uff0c\u5e73\u8861\u4e86\u5206\u6790\u7cbe\u5ea6\u4e0e\u5408\u6210\u590d\u6742\u5ea6\u3002", "motivation": "\u80f6\u7247\u9897\u7c92\u5728\u4f4e\u6bd4\u7279\u7387\u538b\u7f29\u65f6\u4f1a\u4e22\u5931\uff0c\u5f71\u54cd\u827a\u672f\u6548\u679c\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u538b\u7f29\u540e\u6062\u590d\u9897\u7c92\u3002", "method": "\u63d0\u51faFGA-NN\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5206\u6790\u80f6\u7247\u9897\u7c92\u53c2\u6570\uff0c\u517c\u5bb9\u4f20\u7edf\u5408\u6210\u65b9\u6cd5\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u663e\u793aFGA-NN\u5728\u5206\u6790\u7cbe\u5ea6\u4e0e\u5408\u6210\u590d\u6742\u5ea6\u95f4\u53d6\u5f97\u4f18\u8d8a\u5e73\u8861\uff0c\u4e14\u5177\u6709\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "FGA-NN\u4e3a\u80f6\u7247\u9897\u7c92\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u517c\u5bb9\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.14356", "pdf": "https://arxiv.org/pdf/2506.14356", "abs": "https://arxiv.org/abs/2506.14356", "authors": ["Xiaoqi Wang", "Yi Wang", "Lap-Pui Chau"], "title": "EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .", "AI": {"tldr": "EVA02-AT\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u9891-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u9884\u8bad\u7ec3\u3001\u7a7a\u95f4-\u65f6\u95f4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548c\u5bf9\u79f0\u591a\u76f8\u4f3c\u6027\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9884\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u65f6\u7a7a\u7f16\u7801\u65e0\u6548\u548c\u5b66\u4e60\u76ee\u6807\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u9636\u6bb5\u9884\u8bad\u7ec3\u3001\u624b\u52a8\u5206\u5272\u76843D\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548c\u8f6f\u6807\u7b7e\u591a\u5b9e\u4f8b\u68c0\u7d22\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u3001\u7279\u5f81\u4ea4\u4e92\u5dee\u548c\u8d1f\u5bf9\u76f8\u5173\u6027\u5ffd\u7565\u7684\u95ee\u9898\u3002", "method": "1) \u5355\u9636\u6bb5\u9884\u8bad\u7ec3\u5c06\u56fe\u50cfCLIP\u6a21\u578b\u8f6c\u6362\u4e3a\u89c6\u9891\u7f16\u7801\u5668\uff1b2) \u5f15\u5165\u7a7a\u95f4-\u65f6\u95f4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548c\u8054\u5408\u6ce8\u610f\u529b\uff1b3) \u63d0\u51fa\u5bf9\u79f0\u591a\u76f8\u4f3c\u6027\uff08SMS\uff09\u635f\u5931\u548c\u6539\u8fdb\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u5728Ego4D\u3001EPIC-Kitchens-100\u548cCharades-Ego\u4e0a\uff0cEVA02-AT\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "EVA02-AT\u901a\u8fc7\u9ad8\u6548\u9884\u8bad\u7ec3\u3001\u8054\u5408\u65f6\u7a7a\u7f16\u7801\u548c\u7cbe\u786e\u5b66\u4e60\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891-\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.14362", "pdf": "https://arxiv.org/pdf/2506.14362", "abs": "https://arxiv.org/abs/2506.14362", "authors": ["Daniele Rege Cambrin", "Eleonora Poeta", "Eliana Pastor", "Isaac Corley", "Tania Cerquitelli", "Elena Baralis", "Paolo Garza"], "title": "HydroChronos: Forecasting Decades of Surface Water Change", "categories": ["cs.CV"], "comment": null, "summary": "Forecasting surface water dynamics is crucial for water resource management and climate change adaptation. However, the field lacks comprehensive datasets and standardized benchmarks. In this paper, we introduce HydroChronos, a large-scale, multi-modal spatiotemporal dataset for surface water dynamics forecasting designed to address this gap. We couple the dataset with three forecasting tasks. The dataset includes over three decades of aligned Landsat 5 and Sentinel-2 imagery, climate data, and Digital Elevation Models for diverse lakes and rivers across Europe, North America, and South America. We also propose AquaClimaTempo UNet, a novel spatiotemporal architecture with a dedicated climate data branch, as a strong benchmark baseline. Our model significantly outperforms a Persistence baseline for forecasting future water dynamics by +14% and +11% F1 across change detection and direction of change classification tasks, and by +0.1 MAE on the magnitude of change regression. Finally, we conduct an Explainable AI analysis to identify the key climate variables and input channels that influence surface water change, providing insights to inform and guide future modeling efforts.", "AI": {"tldr": "HydroChronos\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5730\u8868\u6c34\u52a8\u6001\u9884\u6d4b\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u6570\u636e\u548c\u57fa\u51c6\u7a7a\u767d\u3002\u63d0\u51fa\u7684AquaClimaTempo UNet\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u5730\u8868\u6c34\u52a8\u6001\u9884\u6d4b\u5bf9\u6c34\u8d44\u6e90\u7ba1\u7406\u548c\u6c14\u5019\u53d8\u5316\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u6570\u636e\u96c6\u548c\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u5f15\u5165HydroChronos\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6e90\u6570\u636e\uff0c\u5e76\u63d0\u51faAquaClimaTempo UNet\u6a21\u578b\u4f5c\u4e3a\u57fa\u51c6\u3002", "result": "\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0cF1\u63d0\u534714%\u548c11%\uff0cMAE\u63d0\u53470.1\u3002", "conclusion": "\u901a\u8fc7\u53ef\u89e3\u91caAI\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u5f71\u54cd\u5730\u8868\u6c34\u53d8\u5316\u7684\u5173\u952e\u6c14\u5019\u53d8\u91cf\u548c\u8f93\u5165\u901a\u9053\uff0c\u4e3a\u672a\u6765\u5efa\u6a21\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2506.14367", "pdf": "https://arxiv.org/pdf/2506.14367", "abs": "https://arxiv.org/abs/2506.14367", "authors": ["Sumshun Nahar Eity", "Mahin Montasir Afif", "Tanisha Fairooz", "Md. Mortuza Ahmmed", "Md Saef Ullah Miah"], "title": "DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI", "categories": ["cs.CV"], "comment": null, "summary": "Accurate diagnosis of brain disorders such as Alzheimer's disease and brain tumors remains a critical challenge in medical imaging. Conventional methods based on manual MRI analysis are often inefficient and error-prone. To address this, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and DenseNet121 to enhance feature extraction and classification. DenseNet121 promotes feature reuse and efficient gradient flow through dense connectivity, while VGG16 contributes strong hierarchical spatial representations. Their fusion enables robust multiclass classification of neurological conditions. Grad-CAM is applied to visualize salient regions, enhancing model transparency. Trained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a test accuracy of 91.33\\%, with precision, recall, and F1-score all exceeding 91\\%. These results highlight DGG-XNet's potential as an effective and interpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and oncological brain disorders.", "AI": {"tldr": "DGG-XNet\u662f\u4e00\u79cd\u7ed3\u5408VGG16\u548cDenseNet121\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u8111\u90e8\u75be\u75c5\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfMRI\u5206\u6790\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u9700\u66f4\u51c6\u786e\u7684\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u878d\u5408VGG16\u548cDenseNet121\u7684DGG-XNet\u6a21\u578b\uff0c\u5229\u7528Grad-CAM\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728BraTS 2021\u548cKaggle\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe91.33%\uff0c\u5404\u9879\u6307\u6807\u5747\u8d85\u8fc791%\u3002", "conclusion": "DGG-XNet\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8111\u90e8\u75be\u75c5\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2506.14373", "pdf": "https://arxiv.org/pdf/2506.14373", "abs": "https://arxiv.org/abs/2506.14373", "authors": ["Junyeob Baek", "Hosung Lee", "Christopher Hoang", "Mengye Ren", "Sungjin Ahn"], "title": "Discrete JEPA: Learning Discrete Token Representations without Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiscrete-JEPA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u6807\u8bb0\u5316\u548c\u65b0\u76ee\u6807\u6539\u8fdb\u56fe\u50cf\u6807\u8bb0\u5316\uff0c\u663e\u8457\u63d0\u5347\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u6807\u8bb0\u5316\u65b9\u6cd5\u5728\u7b26\u53f7\u62bd\u8c61\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7cfb\u7edf\u6027\u63a8\u7406\u9700\u6c42\u3002", "method": "\u6269\u5c55\u6f5c\u5728\u9884\u6d4b\u7f16\u7801\u6846\u67b6\uff0c\u5f15\u5165\u8bed\u4e49\u6807\u8bb0\u5316\u548c\u65b0\u4e92\u8865\u76ee\u6807\uff0c\u6784\u5efa\u9002\u7528\u4e8e\u7b26\u53f7\u63a8\u7406\u7684\u9c81\u68d2\u6807\u8bb0\u5316\u65b9\u6cd5\u3002", "result": "Discrete-JEPA\u5728\u89c6\u89c9\u7b26\u53f7\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u6807\u8bb0\u7a7a\u95f4\u81ea\u53d1\u5f62\u6210\u7cfb\u7edf\u6027\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7b26\u53f7\u4e16\u754c\u5efa\u6a21\u548c\u89c4\u5212\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2506.14382", "pdf": "https://arxiv.org/pdf/2506.14382", "abs": "https://arxiv.org/abs/2506.14382", "authors": ["Ning Zhou", "Shanxiong Chen", "Mingting Zhou", "Haigang Sui", "Lieyun Hu", "Han Li", "Li Hua", "Qiming Zhou"], "title": "DepthSeg: Depth prompting in remote sensing semantic segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDepthSeg\u7684\u6df1\u5ea6\u63d0\u793a\u4e8c\u7ef4\u9065\u611f\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u6df1\u5ea6/\u9ad8\u5ea6\u4fe1\u606f\u6765\u89e3\u51b3\u5149\u8c31\u6df7\u6dc6\u548c\u9634\u5f71\u906e\u6321\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5149\u8c31\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u76ee\u6807\u7684\u9ad8\u5ea6\u5dee\u5f02\uff0c\u5bfc\u81f4\u590d\u6742\u573a\u666f\u4e2d\u7684\u5730\u7269\u8bef\u5206\u7c7b\u3002", "method": "DepthSeg\u6846\u67b6\u5305\u62ec\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u3001\u6df1\u5ea6\u63d0\u793a\u5668\u548c\u8bed\u4e49\u5206\u7c7b\u89e3\u7801\u5668\uff0c\u5206\u522b\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\u3001\u6df1\u5ea6\u5efa\u6a21\u548c\u8bed\u4e49\u9884\u6d4b\u3002", "result": "\u5728LiuZhou\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DepthSeg\u5728\u5730\u7269\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u6d88\u878d\u7814\u7a76\u5f3a\u8c03\u4e86\u6df1\u5ea6\u63d0\u793a\u7684\u91cd\u8981\u6027\u3002", "conclusion": "DepthSeg\u901a\u8fc7\u6574\u5408\u6df1\u5ea6\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.14384", "pdf": "https://arxiv.org/pdf/2506.14384", "abs": "https://arxiv.org/abs/2506.14384", "authors": ["Huan Kang", "Hui Li", "Xiao-Jun Wu", "Tianyang Xu", "Rui Wang", "Chunyang Cheng", "Josef Kittler"], "title": "GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion", "categories": ["cs.CV"], "comment": "16 pages, 11 figures", "summary": "In the field of image fusion, promising progress has been made by modeling data from different modalities as linear subspaces.\n  However, in practice, the source images are often located in a non-Euclidean space, where the Euclidean methods usually cannot\n  encapsulate the intrinsic topological structure. Typically, the inner product performed in the Euclidean space calculates the algebraic\n  similarity rather than the semantic similarity, which results in undesired attention output and a decrease in fusion performance.\n  While the balance of low-level details and high-level semantics should be considered in infrared and visible image fusion task. To\n  address this issue, in this paper, we propose a novel attention mechanism based on Grassmann manifold for infrared and visible\n  image fusion (GrFormer). Specifically, our method constructs a low-rank subspace mapping through projection constraints on the\n  Grassmann manifold, compressing attention features into subspaces of varying rank levels. This forces the features to decouple into\n  high-frequency details (local low-rank) and low-frequency semantics (global low-rank), thereby achieving multi-scale semantic\n  fusion. Additionally, to effectively integrate the significant information, we develop a cross-modal fusion strategy (CMS) based on\n  a covariance mask to maximise the complementary properties between different modalities and to suppress the features with high\n  correlation, which are deemed redundant. The experimental results demonstrate that our network outperforms SOTA methods both\n  qualitatively and quantitatively on multiple image fusion benchmarks. The codes are available at https://github.com/Shaoyun2023.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGrassmann\u6d41\u5f62\u7684\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff08GrFormer\uff09\uff0c\u7528\u4e8e\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8bed\u4e49\u878d\u5408\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u65e0\u6cd5\u6355\u6349\u56fe\u50cf\u7684\u56fa\u6709\u62d3\u6251\u7ed3\u6784\uff0c\u5bfc\u81f4\u878d\u5408\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5229\u7528Grassmann\u6d41\u5f62\u6784\u5efa\u4f4e\u79e9\u5b50\u7a7a\u95f4\u6620\u5c04\uff0c\u5e76\u901a\u8fc7\u534f\u65b9\u5dee\u63a9\u6a21\u7684\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\uff08CMS\uff09\u4f18\u5316\u4fe1\u606f\u6574\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "GrFormer\u901a\u8fc7\u591a\u5c3a\u5ea6\u8bed\u4e49\u878d\u5408\u548c\u8de8\u6a21\u6001\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u878d\u5408\u6548\u679c\u3002"}}
{"id": "2506.14399", "pdf": "https://arxiv.org/pdf/2506.14399", "abs": "https://arxiv.org/abs/2506.14399", "authors": ["Tian Xia", "Fabio De Sousa Ribeiro", "Rajat R Rasal", "Avinash Kori", "Raghav Mehta", "Ben Glocker"], "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08DCFG\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5c5e\u6027\u63a7\u5236\u548c\u8eab\u4efd\u4fdd\u6301\u3002", "motivation": "\u6807\u51c6\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\u5728\u5168\u5c40\u6743\u91cd\u4e0b\u53ef\u80fd\u5bfc\u81f4\u8eab\u4efd\u4fdd\u6301\u4e0d\u4f73\u548c\u865a\u5047\u5c5e\u6027\u53d8\u5316\uff08\u5c5e\u6027\u653e\u5927\uff09\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faDCFG\uff0c\u901a\u8fc7\u5c5e\u6027\u5206\u7ec4\u5d4c\u5165\u7b56\u7565\u89e3\u8026\u8bed\u4e49\u8f93\u5165\uff0c\u5bf9\u5e72\u9884\u548c\u4e0d\u53d8\u5c5e\u6027\u5206\u522b\u5e94\u7528\u4e0d\u540c\u7684\u5f15\u5bfc\u3002", "result": "\u5728CelebA-HQ\u3001MIMIC-CXR\u548cEMBED\u6570\u636e\u96c6\u4e0a\uff0cDCFG\u63d0\u9ad8\u4e86\u5e72\u9884\u4fdd\u771f\u5ea6\uff0c\u51cf\u5c11\u4e86\u610f\u5916\u53d8\u5316\uff0c\u589e\u5f3a\u4e86\u53ef\u9006\u6027\u3002", "conclusion": "DCFG\u80fd\u591f\u5b9e\u73b0\u66f4\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2506.14404", "pdf": "https://arxiv.org/pdf/2506.14404", "abs": "https://arxiv.org/abs/2506.14404", "authors": ["Nikos Spyrou", "Athanasios Vlontzos", "Paraskevas Pegios", "Thomas Melistas", "Nefeli Gkouti", "Yannis Panagakis", "Giorgos Papanastasiou", "Sotirios A. Tsaftaris"], "title": "Causally Steered Diffusion for Automated Video Counterfactual Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic \"what-if\" video scenarios in diverse areas such as healthcare and digital media.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u4f18\u5316\u751f\u6210\u53cd\u4e8b\u5b9e\u89c6\u9891\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u7f16\u8f91\u4e2d\u96be\u4ee5\u4fdd\u6301\u56e0\u679c\u5173\u7cfb\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u73b0\u5b9e\u7684\u7ed3\u679c\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6307\u5bfc\u6587\u672c\u63d0\u793a\u4f18\u5316\uff0c\u57fa\u4e8e\u5047\u8bbe\u7684\u56e0\u679c\u56fe\u751f\u6210\u53cd\u4e8b\u5b9e\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u56e0\u679c\u4e00\u81f4\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\uff0c\u4e14\u517c\u5bb9\u4efb\u4f55\u9ed1\u76d2\u89c6\u9891\u7f16\u8f91\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u533b\u7597\u548c\u6570\u5b57\u5a92\u4f53\u7b49\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u751f\u6210\u903c\u771f\u7684\u201c\u5047\u8bbe\u201d\u89c6\u9891\u573a\u666f\u3002"}}
{"id": "2506.14418", "pdf": "https://arxiv.org/pdf/2506.14418", "abs": "https://arxiv.org/abs/2506.14418", "authors": ["Jiayi Chen", "Yanbiao Ma", "Andi Zhang", "Weidong Tang", "Wei Dai", "Bowei Liu"], "title": "Compositional Attribute Imbalance in Vision Datasets", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u6846\u67b6\u6765\u89e3\u51b3\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u89c6\u89c9\u5c5e\u6027\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u8c03\u6574\u6837\u672c\u91c7\u6837\u6982\u7387\u5e76\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u89c6\u89c9\u5c5e\u6027\u4e0d\u5e73\u8861\u662f\u56fe\u50cf\u5206\u7c7b\u4e2d\u5e38\u89c1\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5b9a\u4e49\u56fe\u50cf\u7684\u4e00\u7ea7\u548c\u4e8c\u7ea7\u5c5e\u6027\uff0c\u6784\u5efa\u89c6\u89c9\u5c5e\u6027\u5b57\u5178\uff0c\u5206\u6790\u5355\u5c5e\u6027\u548c\u7ec4\u5408\u5c5e\u6027\u4e0d\u5e73\u8861\uff0c\u8c03\u6574\u91c7\u6837\u6982\u7387\u5e76\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5982CutMix\u3001Fmix\u7b49\uff09\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5c5e\u6027\u4e0d\u5e73\u8861\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5efa\u6a21\u89c6\u89c9\u5c5e\u6027\u5206\u5e03\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u957f\u5c3e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14428", "pdf": "https://arxiv.org/pdf/2506.14428", "abs": "https://arxiv.org/abs/2506.14428", "authors": ["Ruihao Xi", "Xuekuan Wang", "Yongcheng Li", "Shuhua Li", "Zichen Wang", "Yiwei Wang", "Feng Wei", "Cairong Zhao"], "title": "Toward Rich Video Human-Motion2D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u76842D\u89c6\u9891\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff08RVHM2D\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08Motion2D-Video-150K\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u89d2\u8272\u4ea4\u4e92\u52a8\u4f5c\u751f\u6210\u7684\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u4eba\u9645\u52a8\u6001\u5efa\u6a21\u7684\u590d\u6742\u6027\uff0c\u751f\u6210\u771f\u5b9e\u4e14\u53ef\u63a7\u7684\u591a\u89d2\u8272\u4ea4\u4e92\u52a8\u4f5c\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "1. \u6784\u5efa\u4e86\u5305\u542b15\u4e07\u89c6\u9891\u5e8f\u5217\u7684\u6570\u636e\u96c6Motion2D-Video-150K\uff1b2. \u63d0\u51faRVHM2D\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u6587\u672c\u7f16\u7801\u5668\uff08CLIP-L/B\u6216T5-XXL\uff09\u589e\u5f3a\u6587\u672c\u6761\u4ef6\u673a\u5236\uff1b3. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u6269\u6563\u76ee\u6807\u8bad\u7ec3\u548c\u57fa\u4e8eFID\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff09\u3002", "result": "RVHM2D\u5728Motion2D-Video-150K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u80fd\u751f\u6210\u5355\u89d2\u8272\u548c\u53cc\u89d2\u8272\u4ea4\u4e92\u52a8\u4f5c\u3002", "conclusion": "RVHM2D\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u6a21\u578b\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u89d2\u8272\u4ea4\u4e92\u52a8\u4f5c\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.14435", "pdf": "https://arxiv.org/pdf/2506.14435", "abs": "https://arxiv.org/abs/2506.14435", "authors": ["Hongyu Wang", "Jiayu Xu", "Ruiping Wang", "Yan Feng", "Yitao Zhai", "Peng Pei", "Xunliang Cai", "Xilin Chen"], "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models", "categories": ["cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.", "AI": {"tldr": "MoTE\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u7cbe\u5ea6\u4e13\u5bb6\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u56e0\u9ad8\u7cbe\u5ea6\u4e13\u5bb6\u5bfc\u81f4\u5185\u5b58\u5360\u7528\u9ad8\u3001\u96be\u4ee5\u90e8\u7f72\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684FFN\u4f5c\u4e3a\u5171\u4eab\u4e13\u5bb6\uff0c\u8bad\u7ec3\u4e09\u5143\u8def\u7531\u4e13\u5bb6\uff08\u53c2\u6570\u4e3a{-1, 0, 1}\uff09\uff0c\u7ed3\u5408\u540e\u8bad\u7ec3\u91cf\u5316\u3002", "result": "MoTE\u5728\u76f8\u540c\u5185\u5b58\u5360\u7528\u4e0b\u6027\u80fd\u4f18\u4e8e\u5168\u7cbe\u5ea6\u57fa\u7ebf\uff0c\u5185\u5b58\u53d7\u9650\u65f6\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "MoTE\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u7684\u591a\u6a21\u6001\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2506.14440", "pdf": "https://arxiv.org/pdf/2506.14440", "abs": "https://arxiv.org/abs/2506.14440", "authors": ["David E. Hernandez", "Jose Chang", "Torbj\u00f6rn E. M. Nordling"], "title": "Model compression using knowledge distillation with integrated gradients", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "49 pages, 12 figures", "summary": "Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u68af\u5ea6\uff08IG\uff09\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6a21\u578b\u538b\u7f29\u6548\u679c\uff0c\u663e\u8457\u63d0\u9ad8\u6d4b\u8bd5\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u6280\u672f\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u538b\u7f29\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u4f7f\u7528IG\u56fe\u53e0\u52a0\u5230\u8f93\u5165\u56fe\u50cf\u4e0a\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u589e\u5f3a\u5b66\u751f\u5bf9\u6559\u5e08\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\u3002", "result": "\u5728CIFAR-10\u4e0a\u8fbe\u523092.6%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u538b\u7f29\u6bd4\u4e3a4.1\u500d\uff0c\u63a8\u7406\u65f6\u95f4\u4ece140ms\u964d\u81f313ms\u3002", "conclusion": "IG\u589e\u5f3a\u7684\u77e5\u8bc6\u84b8\u998f\u5728\u591a\u79cd\u67b6\u6784\u548c\u538b\u7f29\u6bd4\u4e0b\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2506.14451", "pdf": "https://arxiv.org/pdf/2506.14451", "abs": "https://arxiv.org/abs/2506.14451", "authors": ["Aditya Shourya", "Michel Dumontier", "Chang Sun"], "title": "Adapting Lightweight Vision Language Models for Radiological Visual Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u653e\u5c04\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u7684\u6570\u636e\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5c0f\u6a21\u578b\u5728\u9002\u5f53\u8c03\u4f18\u540e\u4e5f\u80fd\u5b9e\u73b0\u7a33\u5065\u6027\u80fd\u3002", "motivation": "\u653e\u5c04\u5b66VQA\u9762\u4e34\u6570\u636e\u83b7\u53d6\u56f0\u96be\u3001\u56fe\u50cf\u6a21\u5f0f\u590d\u6742\u4ee5\u53ca\u7f3a\u4e4f\u8bc4\u4f30\u5de5\u5177\u7b49\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u548c\u9ad8\u6548\u8bad\u7ec3\u6d41\u7a0b\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5408\u6210\u95ee\u7b54\u5bf9\u751f\u6210\u5230\u591a\u9636\u6bb5\u5fae\u8c03\u7684\u6210\u672c\u6548\u76ca\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u4f7f\u7528\u4e86ROCO v2.0\u548cMedPix v2.0\u7b49\u4e13\u4e1a\u6570\u636e\u96c6\u3002", "result": "\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u8fdc\u5c0f\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff08\u5982LLaVA-Med\uff09\uff0c\u4f46\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u4ecd\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u663e\u8457\u6027\u5206\u6790\u5de5\u5177\u5e2e\u52a9\u8bc6\u522b\u6a21\u578b\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u9002\u5f53\u8c03\u4f18\u548c\u4e13\u7528\u6570\u636e\u652f\u6301\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u653e\u5c04\u5b66VQA\u7684\u590d\u6742\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u4ee5\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002"}}
{"id": "2506.14471", "pdf": "https://arxiv.org/pdf/2506.14471", "abs": "https://arxiv.org/abs/2506.14471", "authors": ["Yikang Zhou", "Tao Zhang", "Dizhe Zhang", "Shunping Ji", "Xiangtai Li", "Lu Qi"], "title": "Dense360: Dense Understanding from Omnidirectional Panoramas", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capabilities through limited field-of-view (FOV) visual inputs (e.g., 70 degree), we take the first step toward dense understanding from omnidirectional panoramas. We first introduce an omnidirectional panoramas dataset featuring a comprehensive suite of reliability-scored annotations. Specifically, our dataset contains 160K panoramas with 5M dense entity-level captions, 1M unique referring expressions, and 100K entity-grounded panoramic scene descriptions. Compared to multi-view alternatives, panoramas can provide more complete, compact, and continuous scene representations through equirectangular projections (ERP). However, the use of ERP introduces two key challenges for MLLMs: i) spatial continuity along the circle of latitude, and ii) latitude-dependent variation in information density. We address these challenges through ERP-RoPE, a position encoding scheme specifically designed for panoramic ERP. In addition, we introduce Dense360-Bench, the first benchmark for evaluating MLLMs on omnidirectional captioning and grounding, establishing a comprehensive framework for advancing dense visual-language understanding in panoramic settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u666f\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165ERP-RoPE\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u89e3\u51b3\u5168\u666f\u56fe\u50cf\u7684\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u4fe1\u606f\u5bc6\u5ea6\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b160K\u5168\u666f\u56fe\u50cf\u7684\u6570\u636e\u96c6\u548c\u9996\u4e2a\u5168\u666f\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u57fa\u51c6Dense360-Bench\u3002", "motivation": "\u73b0\u6709MLLM\u901a\u8fc7\u6709\u9650\u89c6\u573a\uff08FOV\uff09\u89c6\u89c9\u8f93\u5165\u5b9e\u73b0\u4e16\u754c\u7406\u89e3\uff0c\u4f46\u5168\u666f\u56fe\u50cf\u80fd\u63d0\u4f9b\u66f4\u5b8c\u6574\u3001\u7d27\u51d1\u548c\u8fde\u7eed\u7684\u573a\u666f\u8868\u793a\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5229\u7528\u5168\u666f\u56fe\u50cf\u5b9e\u73b0\u5bc6\u96c6\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u3002", "method": "\u63d0\u51faERP-RoPE\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\uff0c\u4e13\u95e8\u9488\u5bf9\u5168\u666f\u56fe\u50cf\u7684\u7b49\u8ddd\u67f1\u72b6\u6295\u5f71\uff08ERP\uff09\u8bbe\u8ba1\uff0c\u89e3\u51b3\u5176\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u4fe1\u606f\u5bc6\u5ea6\u95ee\u9898\u3002\u540c\u65f6\u53d1\u5e03\u5305\u542b160K\u5168\u666f\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u5bc6\u96c6\u5b9e\u4f53\u7ea7\u63cf\u8ff0\u548c\u573a\u666f\u63cf\u8ff0\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u5168\u666f\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u57fa\u51c6Dense360-Bench\uff0c\u4e3a\u5168\u666f\u73af\u5883\u4e0b\u7684\u5bc6\u96c6\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\u3002", "conclusion": "\u901a\u8fc7ERP-RoPE\u548cDense360-Bench\uff0c\u8bba\u6587\u4e3a\u5168\u666f\u56fe\u50cf\u7684\u591a\u6a21\u6001\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5bc6\u96c6\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.14473", "pdf": "https://arxiv.org/pdf/2506.14473", "abs": "https://arxiv.org/abs/2506.14473", "authors": ["Zhijing Wan", "Zhixiang Wang", "Zheng Wang", "Xin Xu", "Shin'ichi Satoh"], "title": "Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection", "categories": ["cs.CV", "cs.LG"], "comment": "18 pages, 10 figures, accepted by ICML 2025", "summary": "One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u4e00\u6b21\u6027\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u4fe1\u606f\u63d0\u53d6\u5668\uff08IEs\uff09\uff0c\u5e76\u63d0\u51faRAM-APL\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfIEs\u4f9d\u8d56\u4e8e\u76ee\u6807\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002FMs\u53ef\u80fd\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u4f46\u6027\u80fd\u5dee\u5f02\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51faRAM-APL\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u4e2aFMs\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u4f18\u5316\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u7684\u5b50\u96c6\u9009\u62e9\u3002", "result": "FMs\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfIEs\uff0c\u4f46\u5728\u7c97\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u4f18\u52bf\u51cf\u5f31\u3002RAM-APL\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "FMs\u5728\u5b50\u96c6\u9009\u62e9\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u3002RAM-APL\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.14495", "pdf": "https://arxiv.org/pdf/2506.14495", "abs": "https://arxiv.org/abs/2506.14495", "authors": ["Yu Qi", "Lipeng Gu", "Honghua Chen", "Liangliang Nan", "Mingqiang Wei"], "title": "I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs", "categories": ["cs.CV"], "comment": null, "summary": "Existing 3D visual grounding methods rely on precise text prompts to locate objects within 3D scenes. Speech, as a natural and intuitive modality, offers a promising alternative. Real-world speech inputs, however, often suffer from transcription errors due to accents, background noise, and varying speech rates, limiting the applicability of existing 3DVG methods. To address these challenges, we propose \\textbf{SpeechRefer}, a novel 3DVG framework designed to enhance performance in the presence of noisy and ambiguous speech-to-text transcriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and introduces two key innovations. First, the Speech Complementary Module captures acoustic similarities between phonetically related words and highlights subtle distinctions, generating complementary proposal scores from the speech signal. This reduces dependence on potentially erroneous transcriptions. Second, the Contrastive Complementary Module employs contrastive learning to align erroneous text features with corresponding speech features, ensuring robust performance even when transcription errors dominate. Extensive experiments on the SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves the performance of existing 3DVG methods by a large margin, which highlights SpeechRefer's potential to bridge the gap between noisy speech inputs and reliable 3DVG, enabling more intuitive and practical multimodal systems.", "AI": {"tldr": "SpeechRefer\u662f\u4e00\u4e2a\u65b0\u76843D\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u97f3\u4fe1\u53f7\u548c\u5bf9\u6bd4\u5b66\u4e60\u89e3\u51b3\u8bed\u97f3\u8f6c\u6587\u672c\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u5347\u73b0\u67093DVG\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u7684\u6587\u672c\u63d0\u793a\uff0c\u800c\u8bed\u97f3\u8f93\u5165\u56e0\u566a\u97f3\u548c\u8f6c\u5f55\u9519\u8bef\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002SpeechRefer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SpeechRefer\u5f15\u5165\u8bed\u97f3\u4e92\u8865\u6a21\u5757\u548c\u5bf9\u6bd4\u4e92\u8865\u6a21\u5757\uff0c\u5206\u522b\u5229\u7528\u8bed\u97f3\u4fe1\u53f7\u548c\u5bf9\u6bd4\u5b66\u4e60\u51cf\u5c11\u5bf9\u9519\u8bef\u8f6c\u5f55\u7684\u4f9d\u8d56\u3002", "result": "\u5728SpeechRefer\u548cSpeechNr3D\u6570\u636e\u96c6\u4e0a\uff0cSpeechRefer\u663e\u8457\u63d0\u5347\u4e86\u73b0\u67093DVG\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "SpeechRefer\u901a\u8fc7\u5904\u7406\u8bed\u97f3\u8f93\u5165\u4e2d\u7684\u566a\u97f3\u548c\u9519\u8bef\uff0c\u4e3a3D\u89c6\u89c9\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u548c\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14511", "pdf": "https://arxiv.org/pdf/2506.14511", "abs": "https://arxiv.org/abs/2506.14511", "authors": ["Zhiwen Shao", "Yifan Cheng", "Feiran Li", "Yong Zhou", "Xuequan Lu", "Yuan Xie", "Lizhuang Ma"], "title": "MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution", "categories": ["cs.CV"], "comment": "This paper has been accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence", "summary": "Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available at https://github.com/CYF-cuber/MOL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u3001\u56fe\u5377\u79ef\u548c\u666e\u901a\u5377\u79ef\u7684\u7aef\u5230\u7aef\u5fae\u8868\u60c5\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7F5C\u5757\u63d0\u53d6\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\uff0c\u65e0\u9700\u5173\u952e\u5e27\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5fae\u8868\u60c5\u8bc6\u522b\u56e0\u52a8\u4f5c\u77ed\u6682\u4e14\u7ec6\u5fae\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u6216\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51faF5C\u5757\uff08\u5168\u8fde\u63a5\u5377\u79ef\u548c\u901a\u9053\u5bf9\u5e94\u5377\u79ef\uff09\uff0c\u7ed3\u5408Transformer\u548c\u56fe\u5377\u79ef\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\uff08\u5149\u6d41\u4f30\u8ba1\u548c\u9762\u90e8\u6807\u5fd7\u68c0\u6d4b\uff09\u589e\u5f3a\u5fae\u8868\u60c5\u8bc6\u522b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u5149\u6d41\u4f30\u8ba1\u548c\u9762\u90e8\u6807\u5fd7\u68c0\u6d4b\uff0c\u5e76\u80fd\u6355\u6349\u4e0e\u5fae\u8868\u60c5\u76f8\u5173\u7684\u5c40\u90e8\u808c\u8089\u52a8\u4f5c\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u5173\u952e\u5e27\u3002"}}
{"id": "2506.14512", "pdf": "https://arxiv.org/pdf/2506.14512", "abs": "https://arxiv.org/abs/2506.14512", "authors": ["Zijian Song", "Xiaoxin Lin", "Qiuming Huang", "Guangrun Wang", "Liang Lin"], "title": "SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks", "categories": ["cs.CV"], "comment": "16 pages, 9 figures", "summary": "Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving.", "AI": {"tldr": "SIRI-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7a7a\u95f4\u667a\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u6d4b\u8bd5\u5176\u7a7a\u95f4\u7406\u89e3\u548c\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dVLMs\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u800c\u7a7a\u95f4\u667a\u80fd\u5bf9\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86SIRI-Bench\uff0c\u5305\u542b\u8fd11K\u89c6\u9891-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u7ed3\u5408\u81ea\u52a8\u573a\u666f\u751f\u6210\u5f15\u64ce\uff08\u5229\u7528LLM\u4ee3\u7406\uff09\u4ece\u6570\u5b66\u95ee\u9898\u751f\u6210\u771f\u5b9e3D\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684VLMs\u5728SIRI-Bench\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u7a7a\u95f4\u63a8\u7406\u7684\u6311\u6218\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u63a8\u52a8VLMs\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u89c6\u89c9\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u7684\u8fdb\u6b65\u3002"}}
{"id": "2506.14525", "pdf": "https://arxiv.org/pdf/2506.14525", "abs": "https://arxiv.org/abs/2506.14525", "authors": ["Zhuoyue Tan", "Boyong He", "Yuxiang Ji", "Liaoni Wu"], "title": "VisLanding: Monocular 3D Perception for UAV Safe Landing via Depth-Normal Synergy", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by IROS2025", "summary": "This paper presents VisLanding, a monocular 3D perception-based framework for safe UAV (Unmanned Aerial Vehicle) landing. Addressing the core challenge of autonomous UAV landing in complex and unknown environments, this study innovatively leverages the depth-normal synergy prediction capabilities of the Metric3D V2 model to construct an end-to-end safe landing zones (SLZ) estimation framework. By introducing a safe zone segmentation branch, we transform the landing zone estimation task into a binary semantic segmentation problem. The model is fine-tuned and annotated using the WildUAV dataset from a UAV perspective, while a cross-domain evaluation dataset is constructed to validate the model's robustness. Experimental results demonstrate that VisLanding significantly enhances the accuracy of safe zone identification through a depth-normal joint optimization mechanism, while retaining the zero-shot generalization advantages of Metric3D V2. The proposed method exhibits superior generalization and robustness in cross-domain testing compared to other approaches. Furthermore, it enables the estimation of landing zone area by integrating predicted depth and normal information, providing critical decision-making support for practical applications.", "AI": {"tldr": "VisLanding\u662f\u4e00\u4e2a\u57fa\u4e8e\u5355\u76ee3D\u611f\u77e5\u7684\u65e0\u4eba\u673a\u5b89\u5168\u7740\u9646\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6-\u6cd5\u7ebf\u534f\u540c\u9884\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u7740\u9646\u533a\u7684\u8bc6\u522b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u81ea\u4e3b\u7740\u9646\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5229\u7528Metric3D V2\u6a21\u578b\u7684\u6df1\u5ea6-\u6cd5\u7ebf\u534f\u540c\u9884\u6d4b\u80fd\u529b\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u7684\u5b89\u5168\u7740\u9646\u533a\u4f30\u8ba1\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVisLanding\u5728\u8de8\u57df\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u901a\u8fc7\u6df1\u5ea6\u548c\u6cd5\u7ebf\u4fe1\u606f\u4f30\u8ba1\u7740\u9646\u533a\u9762\u79ef\u3002", "conclusion": "VisLanding\u4e3a\u65e0\u4eba\u673a\u5b89\u5168\u7740\u9646\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.14541", "pdf": "https://arxiv.org/pdf/2506.14541", "abs": "https://arxiv.org/abs/2506.14541", "authors": ["Rongchang Lu", "Tianduo Luo", "Yunzhi Zhang", "Conghan Yue", "Pei Yang", "Guibao Liu", "Changyang Gu"], "title": "Exploring Diffusion with Test-Time Training on Efficient Image Restoration", "categories": ["cs.CV"], "comment": "Submitted to The 8th Chinese Conference on Pattern Recognition and Computer Vision (2025). Contact to nomodeset@qq.com. Source code will open in 4 months", "summary": "Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.", "AI": {"tldr": "DiffRWKVIR\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u548c\u9ad8\u6548\u6269\u6563\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u7279\u5f81\u878d\u5408\u3001\u8ba1\u7b97\u74f6\u9888\u548c\u6269\u6563\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u56fe\u50cf\u6062\u590d\u4e2d\u5b58\u5728\u7279\u5f81\u878d\u5408\u4e0d\u9ad8\u6548\u3001\u8ba1\u7b97\u74f6\u9888\u548c\u6269\u6563\u8fc7\u7a0b\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. Omni-Scale 2D State Evolution\u5b9e\u73b0\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\uff1b2. Chunk-Optimized Flash Processing\u52a0\u901f\u5e76\u884c\u5904\u7406\uff1b3. Prior-Guided Efficient Diffusion\u63d0\u53d6\u7d27\u51d1\u56fe\u50cf\u5148\u9a8c\u8868\u793a\u3002", "result": "\u5728\u8d85\u5206\u8fa8\u7387\u548c\u4fee\u590d\u4efb\u52a1\u4e2d\uff0cDiffRWKVIR\u5728PSNR\u3001SSIM\u3001LPIPS\u548c\u6548\u7387\u6307\u6807\u4e0a\u4f18\u4e8eSwinIR\u3001HAT\u548cMambaIR/v2\u3002", "conclusion": "DiffRWKVIR\u4e3a\u9ad8\u6548\u81ea\u9002\u5e94\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4f18\u5316\u4e86\u786c\u4ef6\u5229\u7528\u7387\u3002"}}
{"id": "2506.14549", "pdf": "https://arxiv.org/pdf/2506.14549", "abs": "https://arxiv.org/abs/2506.14549", "authors": ["Yong Liu", "Wenpeng Xiao", "Qianqian Wang", "Junlin Chen", "Shiyin Wang", "Yitong Wang", "Xinglong Wu", "Yansong Tang"], "title": "DreamLight: Towards Harmonious and Consistent Image Relighting", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.", "AI": {"tldr": "DreamLight\u662f\u4e00\u4e2a\u901a\u7528\u56fe\u50cf\u91cd\u5149\u7167\u6a21\u578b\uff0c\u652f\u6301\u57fa\u4e8e\u56fe\u50cf\u6216\u6587\u672c\u7684\u80cc\u666f\u91cd\u5149\u7167\uff0c\u901a\u8fc7\u7edf\u4e00\u8f93\u5165\u683c\u5f0f\u548c\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\u751f\u6210\u81ea\u7136\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u56fe\u50cf\u7684\u91cd\u5149\u7167\uff0c\u4e14\u4f9d\u8d56\u590d\u6742\u7684\u73af\u5883\u6620\u5c04\u6216\u50cf\u7d20\u7ea7\u8f6c\u6362\uff0c\u96be\u4ee5\u5b9e\u73b0\u524d\u666f\u4e0e\u80cc\u666f\u7684\u81ea\u7136\u5149\u7167\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u8f93\u5165\u683c\u5f0f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u63d0\u51fa\u4f4d\u7f6e\u5f15\u5bfc\u5149\u9002\u914d\u5668\uff08PGLA\uff09\u548c\u9891\u8c31\u524d\u666f\u4fee\u590d\u5668\uff08SFF\uff09\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cDreamLight\u5728\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DreamLight\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u91cd\u5149\u7167\u6548\u679c\u3002"}}
{"id": "2506.14560", "pdf": "https://arxiv.org/pdf/2506.14560", "abs": "https://arxiv.org/abs/2506.14560", "authors": ["David Butler", "Adrian Hilton", "Gustavo Carneiro"], "title": "Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u9884\u6d4b\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u672a\u6765\u56fe\u50cf\uff0c\u7528\u4e8e\u9884\u6d4b\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\uff08OA\uff09\u8fdb\u5c55\u98ce\u9669\uff0c\u5e76\u5b9a\u4f4d\u89e3\u5256\u6807\u5fd7\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u819d\u5173\u8282OA\u98ce\u9669\u9884\u6d4b\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u751f\u6210\u672a\u6765\u56fe\u50cf\u7684\u590d\u6742\u6027\u9ad8\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u5728\u7c7b\u522b\u6761\u4ef6\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u672a\u6765\u56fe\u50cf\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u9884\u6d4b\u6a21\u578b\u5206\u7c7b\u672a\u6765OA\u4e25\u91cd\u7a0b\u5ea6\u5e76\u9884\u6d4b\u89e3\u5256\u6807\u5fd7\u3002", "result": "\u5728Osteoarthritis Initiative\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5c06\u9884\u6d4b\u819d\u5173\u8282OA\u8fdb\u5c55\u7684AUC\u63d0\u5347\u81f30.71\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u7ea69%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u751f\u6210\u548c\u591a\u4efb\u52a1\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u819d\u5173\u8282OA\u98ce\u9669\u9884\u6d4b\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.14583", "pdf": "https://arxiv.org/pdf/2506.14583", "abs": "https://arxiv.org/abs/2506.14583", "authors": ["Krishna Sahukara", "Zineddine Bettouche", "Andreas Fischer"], "title": "Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684LaTeX\u6d41\u7a0b\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u591a\u6837\u5316\u8868\u683c\u5e03\u5c40\u7684\u5408\u6210\u6570\u636e\uff0c\u4ee5\u589e\u5f3a\u771f\u5b9e\u6570\u636e\u96c6Marmot\uff0c\u5e76\u8bc4\u4f30TableNet\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u52a8\u63d0\u53d6\u6587\u6863\u4e2d\u7684\u8868\u683c\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LaTeX\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u8868\u683c\u5e03\u5c40\u548c\u5bf9\u9f50\u7684\u771f\u5b9e\u63a9\u7801\uff0c\u7528\u4e8e\u8bad\u7ec3TableNet\u3002", "result": "\u5728\u5408\u6210\u6d4b\u8bd5\u96c6\u4e0a\uff0cTableNet\u7684\u50cf\u7d20XOR\u8bef\u5dee\u4e3a4.04%\uff08256x256\uff09\u548c4.33%\uff081024x1024\uff09\uff1b\u5728Marmot\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e3a9.18%\uff08256x256\uff09\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u6709\u6548\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\uff0c\u5e76\u63d0\u5347\u4e86\u8868\u683c\u63d0\u53d6\u7684\u81ea\u52a8\u5316\u6027\u80fd\u3002"}}
{"id": "2506.14596", "pdf": "https://arxiv.org/pdf/2506.14596", "abs": "https://arxiv.org/abs/2506.14596", "authors": ["Ming Xu", "Xu Zhang"], "title": "PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.", "AI": {"tldr": "PoseGRAF\u6846\u67b6\u901a\u8fc7\u53cc\u56fe\u5377\u79ef\u7ed3\u6784\u6355\u6349\u5173\u8282\u548c\u9aa8\u9abc\u7684\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u548c\u52a8\u6001\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5173\u8282\u4f4d\u7f6e\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u9aa8\u9abc\u5185\u5728\u7684\u65b9\u5411\u548c\u89d2\u5ea6\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u5728\u5173\u8282\u906e\u6321\u6216\u5feb\u901f\u8fd0\u52a8\u65f6\u4ea7\u751f\u4e0d\u5408\u7406\u7684\u59ff\u6001\u3002", "method": "\u6784\u5efa\u53cc\u56fe\u5377\u79ef\u7ed3\u6784\u5904\u7406\u5173\u8282\u548c\u9aa8\u9abc\u56fe\uff0c\u5f15\u5165\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u5efa\u6a21\u9aa8\u9abc\u65b9\u5411\u4e0e\u5173\u8282\u7279\u5f81\u7684\u76f8\u4e92\u4f9d\u8d56\uff0c\u8bbe\u8ba1\u52a8\u6001\u878d\u5408\u6a21\u5757\u81ea\u9002\u5e94\u6574\u5408\u7279\u5f81\uff0c\u5e76\u6539\u8fdbTransformer\u7f16\u7801\u5668\u751f\u6210\u6700\u7ec8\u8f93\u51fa\u3002", "result": "\u5728Human3.6M\u548cMPI-INF-3DHP\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u91ce\u5916\u89c6\u9891\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PoseGRAF\u901a\u8fc7\u7ed3\u5408\u5173\u8282\u548c\u9aa8\u9abc\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u76ee3D\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u6311\u6218\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2506.14603", "pdf": "https://arxiv.org/pdf/2506.14603", "abs": "https://arxiv.org/abs/2506.14603", "authors": ["Amirmojtaba Sabour", "Sanja Fidler", "Karsten Kreis"], "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/", "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAlign Your Flow\u7684\u6d41\u6620\u5c04\u6a21\u578b\uff0c\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u76ee\u6807\u548c\u65b0\u6280\u672f\u6539\u8fdb\u751f\u6210\u6027\u80fd\uff0c\u5e76\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u9886\u5148\u6548\u679c\u3002", "motivation": "\u6269\u6563\u548c\u6d41\u6a21\u578b\u867d\u7136\u5148\u8fdb\uff0c\u4f46\u9700\u8981\u591a\u6b65\u91c7\u6837\uff0c\u800c\u4e00\u81f4\u6027\u6a21\u578b\u867d\u9ad8\u6548\u4f46\u6027\u80fd\u968f\u6b65\u6570\u589e\u52a0\u4e0b\u964d\u3002\u6d41\u6620\u5c04\u901a\u8fc7\u5355\u6b65\u8fde\u63a5\u566a\u58f0\u6c34\u5e73\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8fde\u7eed\u65f6\u95f4\u76ee\u6807\u8bad\u7ec3\u6d41\u6620\u5c04\uff0c\u7ed3\u5408\u81ea\u5f15\u5bfc\u548c\u5bf9\u6297\u5fae\u8c03\u6280\u672f\uff0c\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728ImageNet 64x64\u548c512x512\u4e0a\u5b9e\u73b0\u9886\u5148\u7684\u5c11\u6b65\u751f\u6210\u6548\u679c\uff0c\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u975e\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u6d41\u6620\u5c04\u6a21\u578b\u5728\u9ad8\u6548\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5c11\u6b65\u91c7\u6837\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.14605", "pdf": "https://arxiv.org/pdf/2506.14605", "abs": "https://arxiv.org/abs/2506.14605", "authors": ["Giacomo Meanti", "Thomas Ryckeboer", "Michael Arbel", "Julien Mairal"], "title": "Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Code available at https://github.com/inria-thoth/ddm4ip", "summary": "This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9006\u95ee\u9898\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u4e2d\u524d\u5411\u6a21\u578b\u672a\u77e5\u6216\u6570\u636e\u914d\u5bf9\u56f0\u96be\u7684\u60c5\u51b5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u7684\u524d\u5411\u6a21\u578b\u6216\u914d\u5bf9\u6570\u636e\uff0c\u800c\u771f\u5b9e\u573a\u666f\u4e2d\u8fd9\u4e9b\u6761\u4ef6\u96be\u4ee5\u6ee1\u8db3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u6d41\u5339\u914d\u5efa\u6a21\u9000\u5316\u89c2\u6d4b\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5339\u914d\u635f\u5931\u5b66\u4e60\u524d\u5411\u6a21\u578b\u3002", "result": "\u5728\u53bb\u6a21\u7cca\u548c\u975e\u5747\u5300\u70b9\u6269\u6563\u51fd\u6570\u6821\u51c6\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u56fe\u50cf\u76f2\u65b9\u6cd5\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5728\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\uff08\u5982\u955c\u5934\u6821\u51c6\uff09\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u91c7\u96c6\u9700\u6c42\u3002"}}
{"id": "2506.14629", "pdf": "https://arxiv.org/pdf/2506.14629", "abs": "https://arxiv.org/abs/2506.14629", "authors": ["Md. Adnanul Islam", "Md. Faiyaz Abdullah Sayeedi", "Md. Asaduzzaman Shuvo", "Muhammad Ziaur Rahman", "Shahanur Rahman Bappy", "Raiyan Rahman", "Swakkhar Shatabda"], "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme \"Prevention is Better than Cure\", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito", "AI": {"tldr": "VisText-Mosquito\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u7528\u4e8e\u868a\u866b\u6ecb\u751f\u5730\u7684\u81ea\u52a8\u68c0\u6d4b\u3001\u5206\u5272\u548c\u63a8\u7406\u5206\u6790\u3002YOLOv9s\u548cYOLOv11n-Seg\u6a21\u578b\u5728\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cBLIP\u6a21\u578b\u5728\u63a8\u7406\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u9ad8\u5206\u3002", "motivation": "\u868a\u5a92\u75be\u75c5\u662f\u5168\u7403\u91cd\u5927\u5065\u5eb7\u5a01\u80c1\uff0c\u9700\u901a\u8fc7\u65e9\u671f\u68c0\u6d4b\u548c\u4e3b\u52a8\u63a7\u5236\u6ecb\u751f\u5730\u6765\u9884\u9632\u7206\u53d1\u3002", "method": "\u6784\u5efaVisText-Mosquito\u6570\u636e\u96c6\uff0c\u5305\u542b\u6807\u6ce8\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u6587\u672c\uff0c\u4f7f\u7528YOLO\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u68c0\u6d4b\u548c\u5206\u5272\uff0cBLIP\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u751f\u6210\u3002", "result": "YOLOv9s\u68c0\u6d4b\u7cbe\u5ea60.92926\uff0cmAP@50 0.92891\uff1bYOLOv11n-Seg\u5206\u5272\u7cbe\u5ea60.91587\uff0cmAP@50 0.79795\uff1bBLIP\u6a21\u578bBLEU 54.7\uff0cBERTScore 0.91\uff0cROUGE-L 0.87\u3002", "conclusion": "\u6570\u636e\u96c6\u548c\u6a21\u578b\u6846\u67b6\u652f\u6301\u201c\u9884\u9632\u80dc\u4e8e\u6cbb\u7597\u201d\u4e3b\u9898\uff0c\u5c55\u793a\u4e86AI\u5728\u868a\u5a92\u75be\u75c5\u98ce\u9669\u4e3b\u52a8\u9632\u63a7\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14642", "pdf": "https://arxiv.org/pdf/2506.14642", "abs": "https://arxiv.org/abs/2506.14642", "authors": ["Yuke Xing", "Jiarui Wang", "Peizhi Niu", "Wenjie Huang", "Guangtao Zhai", "Yiling Xu"], "title": "3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.", "AI": {"tldr": "3DGS-IEval-15K\u662f\u9996\u4e2a\u9488\u5bf9\u538b\u7f293D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5305\u542b15,200\u5f20\u56fe\u50cf\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u538b\u7f29\u7ea7\u522b\u4e0b\u7684\u611f\u77e5\u5f71\u54cd\u3002", "motivation": "3DGS\u5728\u5b9e\u65f6\u6e32\u67d3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9ad8\u5b58\u50a8\u9700\u6c42\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u538b\u7f29\u7b97\u6cd5\u611f\u77e5\u5f71\u54cd\u7684\u7efc\u5408\u6846\u67b6\u3002", "method": "\u901a\u8fc76\u79cd\u4ee3\u8868\u60273DGS\u7b97\u6cd5\u572810\u4e2a\u771f\u5b9e\u573a\u666f\u4e2d\u6e32\u67d3\u56fe\u50cf\uff0c\u6536\u96c660\u540d\u89c2\u4f17\u7684\u4e3b\u89c2\u8bc4\u4ef7\u6570\u636e\uff0c\u5e76\u5206\u6790\u573a\u666f\u591a\u6837\u6027\u548cMOS\u5206\u5e03\u3002", "result": "\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u538b\u7f29\u5bf93DGS\u56fe\u50cf\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u5efa\u7acb\u4e86\u5305\u542b30\u79cdIQA\u6307\u6807\u7684\u57fa\u51c6\u3002", "conclusion": "3DGS-IEval-15K\u4e3a\u5f00\u53d13DGS\u4e13\u7528IQA\u6307\u6807\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u652f\u6301\u7814\u7a763DGS\u7279\u6709\u7684\u89c6\u89d2\u4f9d\u8d56\u8d28\u91cf\u5206\u5e03\u6a21\u5f0f\u3002"}}
{"id": "2506.14667", "pdf": "https://arxiv.org/pdf/2506.14667", "abs": "https://arxiv.org/abs/2506.14667", "authors": ["Matt Poyser", "Toby P. Breckon"], "title": "DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification", "categories": ["cs.CV"], "comment": "27 single-column pages, 8 figures, to be published in Pattern Recognition", "summary": "In order to address the scalability challenge within Neural Architecture Search (NAS), we speed up NAS training via dynamic hard example mining within a curriculum learning framework. By utilizing an autoencoder that enforces an image similarity embedding in latent space, we construct an efficient kd-tree structure to order images by furthest neighbour dissimilarity in a low-dimensional embedding. From a given query image from our subsample dataset, we can identify the most dissimilar image within the global dataset in logarithmic time. Via curriculum learning, we then dynamically re-formulate an unbiased subsample dataset for NAS optimisation, upon which the current NAS solution architecture performs poorly. We show that our DDS-NAS framework speeds up gradient-based NAS strategies by up to 27x without loss in performance. By maximising the contribution of each image sample during training, we reduce the duration of a NAS training cycle and the number of iterations required for convergence.", "AI": {"tldr": "\u901a\u8fc7\u52a8\u6001\u786c\u6837\u672c\u6316\u6398\u548c\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\u52a0\u901f\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u8bad\u7ec3\uff0cDDS-NAS\u6846\u67b6\u5c06\u68af\u5ea6\u57faNAS\u7b56\u7565\u63d0\u901f27\u500d\u4e14\u65e0\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u8bad\u7ec3\u6837\u672c\u63d0\u5347\u6548\u7387\u3002", "method": "\u5229\u7528\u81ea\u7f16\u7801\u5668\u6784\u5efa\u56fe\u50cf\u76f8\u4f3c\u6027\u5d4c\u5165\uff0c\u901a\u8fc7kd\u6811\u7ed3\u6784\u5feb\u901f\u8bc6\u522b\u6700\u4e0d\u76f8\u4f3c\u56fe\u50cf\uff0c\u52a8\u6001\u91cd\u7ec4\u5b50\u6570\u636e\u96c6\u7528\u4e8eNAS\u4f18\u5316\u3002", "result": "DDS-NAS\u6846\u67b6\u5c06\u8bad\u7ec3\u901f\u5ea6\u63d0\u534727\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u4e0d\u53d8\uff0c\u51cf\u5c11\u8bad\u7ec3\u5468\u671f\u548c\u6536\u655b\u6240\u9700\u8fed\u4ee3\u6b21\u6570\u3002", "conclusion": "\u52a8\u6001\u786c\u6837\u672c\u6316\u6398\u548c\u8bfe\u7a0b\u5b66\u4e60\u663e\u8457\u63d0\u5347NAS\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14674", "pdf": "https://arxiv.org/pdf/2506.14674", "abs": "https://arxiv.org/abs/2506.14674", "authors": ["Ling Li", "Yao Zhou", "Yuxuan Liang", "Fugee Tsung", "Jiaheng Wei"], "title": "Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vision-language models (LVLMs) has enabled a rethinking of geo-localization as a reasoning-driven task grounded in visual cues. However, two major challenges persist. On the data side, existing reasoning-focused datasets are primarily based on street-view imagery, offering limited scene diversity and constrained viewpoints. On the modeling side, current approaches predominantly rely on supervised fine-tuning, which yields only marginal improvements in reasoning capabilities. To address these challenges, we propose a novel pipeline that constructs a reasoning-oriented geo-localization dataset, MP16-Reason, using diverse social media images. We introduce GLOBE, Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning, yielding Bi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE incorporates task-specific rewards that jointly enhance locatability assessment, visual clue reasoning, and geolocation accuracy. Both qualitative and quantitative results demonstrate that GLOBE outperforms state-of-the-art open-source LVLMs on geo-localization tasks, particularly in diverse visual scenes, while also generating more insightful and interpretable reasoning trajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5GLOBE\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6837\u6027\u6570\u636e\u96c6MP16-Reason\u548c\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u7406\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u591a\u4e3a\u5206\u7c7b\u6216\u68c0\u7d22\u4efb\u52a1\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff1b\u4e14\u73b0\u6709\u6570\u636e\u96c6\u548c\u6a21\u578b\u5728\u591a\u6837\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faGLOBE\u65b9\u6cd5\uff0c\u5305\u62ec\u6784\u5efa\u591a\u6837\u6027\u6570\u636e\u96c6MP16-Reason\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u8054\u5408\u4f18\u5316\u5b9a\u4f4d\u8bc4\u4f30\u548c\u89c6\u89c9\u7ebf\u7d22\u63a8\u7406\u3002", "result": "GLOBE\u5728\u591a\u6837\u89c6\u89c9\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e14\u751f\u6210\u66f4\u5177\u6d1e\u5bdf\u529b\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "GLOBE\u901a\u8fc7\u6570\u636e\u591a\u6837\u6027\u548c\u6a21\u578b\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.14686", "pdf": "https://arxiv.org/pdf/2506.14686", "abs": "https://arxiv.org/abs/2506.14686", "authors": ["Xi Chen", "Hengshuang Zhao"], "title": "FocalClick-XL: Towards Unified and High-quality Interactive Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Interactive segmentation enables users to extract binary masks of target objects through simple interactions such as clicks, scribbles, and boxes. However, existing methods often support only limited interaction forms and struggle to capture fine details. In this paper, we revisit the classical coarse-to-fine design of FocalClick and introduce significant extensions. Inspired by its multi-stage strategy, we propose a novel pipeline, FocalClick-XL, to address these challenges simultaneously. Following the emerging trend of large-scale pretraining, we decompose interactive segmentation into meta-tasks that capture different levels of information -- context, object, and detail -- assigning a dedicated subnet to each level.This decomposition allows each subnet to undergo scaled pretraining with independent data and supervision, maximizing its effectiveness. To enhance flexibility, we share context- and detail-level information across different interaction forms as common knowledge while introducing a prompting layer at the object level to encode specific interaction types. As a result, FocalClick-XL achieves state-of-the-art performance on click-based benchmarks and demonstrates remarkable adaptability to diverse interaction formats, including boxes, scribbles, and coarse masks. Beyond binary mask generation, it is also capable of predicting alpha mattes with fine-grained details, making it a versatile and powerful tool for interactive segmentation.", "AI": {"tldr": "FocalClick-XL\u901a\u8fc7\u591a\u9636\u6bb5\u7b56\u7565\u6539\u8fdb\u4ea4\u4e92\u5f0f\u5206\u5272\uff0c\u652f\u6301\u591a\u79cd\u4ea4\u4e92\u5f62\u5f0f\u5e76\u63d0\u5347\u7ec6\u8282\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u5206\u5272\u65b9\u6cd5\u652f\u6301\u6709\u9650\u4ea4\u4e92\u5f62\u5f0f\u4e14\u96be\u4ee5\u6355\u6349\u7ec6\u8282\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faFocalClick-XL\uff0c\u5206\u89e3\u4efb\u52a1\u4e3a\u5143\u4efb\u52a1\uff08\u4e0a\u4e0b\u6587\u3001\u5bf9\u8c61\u3001\u7ec6\u8282\uff09\uff0c\u6bcf\u4e2a\u5b50\u7f51\u72ec\u7acb\u9884\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u63d0\u793a\u5c42\u7f16\u7801\u4ea4\u4e92\u7c7b\u578b\u3002", "result": "\u5728\u70b9\u51fb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u652f\u6301\u591a\u79cd\u4ea4\u4e92\u5f62\u5f0f\uff0c\u5e76\u80fd\u9884\u6d4b\u7cbe\u7ec6alpha\u906e\u7f69\u3002", "conclusion": "FocalClick-XL\u662f\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2506.14696", "pdf": "https://arxiv.org/pdf/2506.14696", "abs": "https://arxiv.org/abs/2506.14696", "authors": ["Dahang Wan", "Rongsheng Lu", "Yang Fang", "Xianli Lang", "Shuangbao Shu", "Jingjing Chen", "Siyuan Shen", "Ting Xu", "Zecong Ye"], "title": "YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework", "categories": ["cs.CV"], "comment": "28 pages, 8 figures", "summary": "Multispectral object detection, which integrates information from multiple bands, can enhance detection accuracy and environmental adaptability, holding great application potential across various fields. Although existing methods have made progress in cross-modal interaction, low-light conditions, and model lightweight, there are still challenges like the lack of a unified single-stage framework, difficulty in balancing performance and fusion strategy, and unreasonable modality weight allocation. To address these, based on the YOLOv11 framework, we present YOLOv11-RGBT, a new comprehensive multimodal object detection framework. We designed six multispectral fusion modes and successfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After reevaluating the importance of the two modalities, we proposed a P3 mid-fusion strategy and multispectral controllable fine-tuning (MCF) strategy for multispectral models. These improvements optimize feature fusion, reduce redundancy and mismatches, and boost overall model performance. Experiments show our framework excels on three major open-source multispectral object detection datasets, like LLVIP and FLIR. Particularly, the multispectral controllable fine-tuning strategy significantly enhanced model adaptability and robustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP by 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and strategies' effectiveness. The code is available at: https://github.com/wandahangFY/YOLOv11-RGBT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv11\u7684\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u6846\u67b6YOLOv11-RGBT\uff0c\u901a\u8fc7\u8bbe\u8ba1\u516d\u79cd\u591a\u5149\u8c31\u878d\u5408\u6a21\u5f0f\u548c\u4f18\u5316\u6a21\u6001\u6743\u91cd\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u5728\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7edf\u4e00\u6846\u67b6\u3001\u6027\u80fd\u4e0e\u878d\u5408\u7b56\u7565\u5e73\u8861\u53ca\u6a21\u6001\u6743\u91cd\u5206\u914d\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8eYOLOv11\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u516d\u79cd\u591a\u5149\u8c31\u878d\u5408\u6a21\u5f0f\uff0c\u63d0\u51faP3\u4e2d\u878d\u5408\u7b56\u7565\u548c\u591a\u5149\u8c31\u53ef\u63a7\u5fae\u8c03\uff08MCF\uff09\u7b56\u7565\u3002", "result": "\u5728LLVIP\u548cFLIR\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cFLIR\u6570\u636e\u96c6\u4e0amAP\u63d0\u53473.41%-5.65%\uff0c\u6700\u9ad8\u8fbe47.61%\u3002", "conclusion": "YOLOv11-RGBT\u6846\u67b6\u548c\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.14706", "pdf": "https://arxiv.org/pdf/2506.14706", "abs": "https://arxiv.org/abs/2506.14706", "authors": ["Ni Ou", "Zhuo Chen", "Xinru Zhang", "Junzheng Wang"], "title": "Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion", "categories": ["cs.CV"], "comment": "7 pages, 4 figures, accepted by IROS 2025", "summary": "Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66ff\u4ee3\u6269\u6563\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u76f8\u673a\u548cLiDAR\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u539f\u65b9\u6cd5\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u6807\u5b9a\u65b9\u6cd5\u591a\u4e3a\u5355\u6b65\u9884\u6d4b\uff0c\u7f3a\u4e4f\u8fed\u4ee3\u4f18\u5316\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u66ff\u4ee3\u6269\u6563\u6846\u67b6\uff0c\u5c06\u521d\u59cb\u5916\u53c2\u901a\u8fc7\u53bb\u566a\u8fc7\u7a0b\u8fed\u4ee3\u4f18\u5316\uff0c\u539f\u6807\u5b9a\u65b9\u6cd5\u4f5c\u4e3a\u66ff\u4ee3\u53bb\u566a\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u540e\uff0c\u6240\u6709\u6807\u5b9a\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u8fed\u4ee3\u65b9\u6cd5\u548c\u5355\u6b65\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u6807\u5b9a\u6027\u80fd\uff0c\u5177\u6709\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.14709", "pdf": "https://arxiv.org/pdf/2506.14709", "abs": "https://arxiv.org/abs/2506.14709", "authors": ["Kunal Swami", "Debtanu Gupta", "Amrit Kumar Muduli", "Chirag Jaiswal", "Pankaj Kumar Bajpai"], "title": "DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted in IROS 2025", "summary": "Depth estimation is crucial for intelligent systems, enabling applications from autonomous navigation to augmented reality. While traditional stereo and active depth sensors have limitations in cost, power, and robustness, dual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling alternative. This paper introduces DiFuse-Net, a novel modality decoupled network design for disentangled RGB and DP based depth estimation. DiFuse-Net features a window bi-directional parallax attention mechanism (WBiPAM) specifically designed to capture the subtle DP disparity cues unique to smartphone cameras with small aperture. A separate encoder extracts contextual information from the RGB image, and these features are fused to enhance depth prediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to utilize large-scale RGB-D datasets in the literature to cope with the limitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and comparison of the proposed method demonstrates its superiority over the DP and stereo-based baseline methods. Additionally, we contribute a new, high-quality, real-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP) dataset, created using our novel symmetric stereo camera hardware setup, stereo calibration and rectification protocol, and AI stereo disparity estimation method.", "AI": {"tldr": "DiFuse-Net\u662f\u4e00\u79cd\u65b0\u578b\u6a21\u6001\u89e3\u8026\u7f51\u7edc\uff0c\u7528\u4e8eRGB\u548c\u53cc\u50cf\u7d20\uff08DP\uff09\u6df1\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7WBiPAM\u673a\u5236\u6355\u6349\u667a\u80fd\u624b\u673a\u76f8\u673a\u7684\u5c0f\u5b54\u5f84DP\u5dee\u5f02\u7ebf\u7d22\uff0c\u5e76\u7ed3\u5408RGB\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u5347\u6df1\u5ea6\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f20\u611f\u5668\u5728\u6210\u672c\u3001\u529f\u8017\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u73b0\u4ee3\u76f8\u673a\u4e2d\u666e\u904d\u7684\u53cc\u50cf\u7d20\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faDiFuse-Net\uff0c\u91c7\u7528WBiPAM\u673a\u5236\u6355\u6349DP\u5dee\u5f02\u7ebf\u7d22\uff0c\u7ed3\u5408RGB\u7f16\u7801\u5668\u63d0\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7CmTL\u673a\u5236\u5229\u7528\u5927\u89c4\u6a21RGB-D\u6570\u636e\u96c6\u3002", "result": "DiFuse-Net\u5728DP\u548c\u7acb\u4f53\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5e76\u8d21\u732e\u4e86\u65b0\u7684\u9ad8\u8d28\u91cfRGB-DP-D\u6570\u636e\u96c6DCDP\u3002", "conclusion": "DiFuse-Net\u4e3a\u667a\u80fd\u624b\u673a\u76f8\u673a\u7684\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u786c\u4ef6\u8bbe\u7f6e\u63a8\u52a8\u4e86\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2506.14730", "pdf": "https://arxiv.org/pdf/2506.14730", "abs": "https://arxiv.org/abs/2506.14730", "authors": ["Corey Scher", "Jamon Van Den Hoek"], "title": "Active InSAR monitoring of building damage in Gaza during the Israel-Hamas War", "categories": ["cs.CV"], "comment": null, "summary": "Aerial bombardment of the Gaza Strip beginning October 7, 2023 is one of the most intense bombing campaigns of the twenty-first century, driving widespread urban damage. Characterizing damage over a geographically dynamic and protracted armed conflict requires active monitoring. Synthetic aperture radar (SAR) has precedence for mapping disaster-induced damage with bi-temporal methods but applications to active monitoring during sustained crises are limited. Using interferometric SAR data from Sentinel-1, we apply a long temporal-arc coherent change detection (LT-CCD) approach to track weekly damage trends over the first year of the 2023- Israel-Hamas War. We detect 92.5% of damage labels in reference data from the United Nations with a negligible (1.2%) false positive rate. The temporal fidelity of our approach reveals rapidly increasing damage during the first three months of the war focused in northern Gaza, a notable pause in damage during a temporary ceasefire, and surges of new damage as conflict hot-spots shift from north to south. Three-fifths (191,263) of all buildings are damaged or destroyed by the end of the study. With massive need for timely data on damage in armed conflict zones, our low-cost and low-latency approach enables rapid uptake of damage information at humanitarian and journalistic organizations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u7684\u957f\u671f\u76f8\u5e72\u53d8\u5316\u68c0\u6d4b\uff08LT-CCD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u6d4b2023\u5e74\u4ee5\u8272\u5217-\u54c8\u9a6c\u65af\u6218\u4e89\u671f\u95f4\u52a0\u6c99\u5730\u5e26\u7684\u5efa\u7b51\u7834\u574f\u60c5\u51b5\u3002", "motivation": "\u52a0\u6c99\u5730\u5e26\u57282023\u5e7410\u67087\u65e5\u5f00\u59cb\u7684\u7a7a\u88ad\u4e2d\u906d\u53d7\u4e86\u4e25\u91cd\u7684\u57ce\u5e02\u7834\u574f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u76d1\u6d4b\u6301\u7eed\u51b2\u7a81\u4e2d\u7834\u574f\u60c5\u51b5\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Sentinel-1\u7684\u5e72\u6d89SAR\u6570\u636e\uff0c\u91c7\u7528LT-CCD\u65b9\u6cd5\u6bcf\u5468\u8ddf\u8e2a\u7834\u574f\u8d8b\u52bf\u3002", "result": "\u68c0\u6d4b\u5230\u8054\u5408\u56fd\u53c2\u8003\u6570\u636e\u4e2d92.5%\u7684\u7834\u574f\u6807\u7b7e\uff0c\u8bef\u62a5\u7387\u4ec5\u4e3a1.2%\u3002\u7814\u7a76\u53d1\u73b0\u6218\u4e89\u524d\u4e09\u4e2a\u6708\u7834\u574f\u8fc5\u901f\u589e\u52a0\uff0c\u4e34\u65f6\u505c\u706b\u671f\u95f4\u7834\u574f\u6682\u505c\uff0c\u968f\u540e\u7834\u574f\u70ed\u70b9\u4ece\u5317\u90e8\u8f6c\u79fb\u5230\u5357\u90e8\u3002\u7814\u7a76\u7ed3\u675f\u65f6\uff0c\u4e94\u5206\u4e4b\u4e09\u7684\u5efa\u7b51\uff08191,263\u680b\uff09\u53d7\u635f\u6216\u88ab\u6bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u672c\u4f4e\u3001\u5ef6\u8fdf\u4f4e\uff0c\u4e3a\u4eba\u9053\u4e3b\u4e49\u548c\u65b0\u95fb\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u53ca\u65f6\u7684\u7834\u574f\u6570\u636e\u3002"}}
{"id": "2506.14742", "pdf": "https://arxiv.org/pdf/2506.14742", "abs": "https://arxiv.org/abs/2506.14742", "authors": ["Ziqiao Peng", "Wentao Hu", "Junyuan Ma", "Xiangyu Zhu", "Xiaomei Zhang", "Hao Zhao", "Hui Tian", "Jun He", "Hongyan Liu", "Zhaoxin Fan"], "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.", "AI": {"tldr": "SyncTalk++\u901a\u8fc7\u52a8\u6001\u8096\u50cf\u6e32\u67d3\u5668\u548c\u9762\u90e8\u540c\u6b65\u63a7\u5236\u5668\u89e3\u51b3\u8bed\u97f3\u9a71\u52a8\u89c6\u9891\u4e2d\u7684\u540c\u6b65\u95ee\u9898\uff0c\u63d0\u5347\u771f\u5b9e\u611f\u548c\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "\u8bed\u97f3\u9a71\u52a8\u89c6\u9891\u4e2d\u8eab\u4efd\u3001\u5507\u52a8\u3001\u8868\u60c5\u548c\u5934\u90e8\u59ff\u6001\u7684\u540c\u6b65\u662f\u521b\u9020\u771f\u5b9e\u8bf4\u8bdd\u5934\u50cf\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6563\u5c04\u7684\u52a8\u6001\u8096\u50cf\u6e32\u67d3\u5668\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u9762\u90e8\u540c\u6b65\u63a7\u5236\u5668\u5bf9\u9f50\u5507\u52a8\u4e0e\u8bed\u97f3\uff0c3D\u6df7\u5408\u5f62\u72b6\u6a21\u578b\u91cd\u5efa\u8868\u60c5\uff0c\u5934\u90e8\u540c\u6b65\u7a33\u5b9a\u5668\u4f18\u5316\u59ff\u6001\u3002", "result": "SyncTalk++\u5728\u540c\u6b65\u6027\u548c\u771f\u5b9e\u611f\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6e32\u67d3\u901f\u5ea6\u8fbe101\u5e27/\u79d2\u3002", "conclusion": "SyncTalk++\u663e\u8457\u63d0\u5347\u8bed\u97f3\u9a71\u52a8\u89c6\u9891\u7684\u540c\u6b65\u6027\u548c\u771f\u5b9e\u611f\uff0c\u9002\u7528\u4e8e\u9ad8\u8981\u6c42\u573a\u666f\u3002"}}
{"id": "2506.14753", "pdf": "https://arxiv.org/pdf/2506.14753", "abs": "https://arxiv.org/abs/2506.14753", "authors": ["Qinchan", "Li", "Kenneth Chen", "Changyue", "Su", "Wittawat Jitkrittum", "Qi Sun", "Patsorn Sangkloy"], "title": "Cost-Aware Routing for Efficient Text-To-Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6839\u636e\u63d0\u793a\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u6210\u672c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u7531\u9009\u62e9\u6700\u4f18\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u5e73\u8861\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5e0c\u671b\u6839\u636e\u63d0\u793a\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u8def\u7531\u5c06\u63d0\u793a\u5206\u914d\u5230\u6700\u9002\u5408\u7684\u751f\u6210\u6a21\u578b\uff08\u5982\u4e0d\u540c\u6b65\u6570\u7684\u6269\u6563\u6a21\u578b\u6216\u5176\u4ed6\u72ec\u7acb\u6a21\u578b\uff09\uff0c\u5b66\u4e60\u4fdd\u7559\u9ad8\u6210\u672c\u9009\u62e9\u7ed9\u590d\u6742\u63d0\u793a\u3002", "result": "\u5728COCO\u548cDiffusionDB\u4e0a\u9a8c\u8bc1\uff0c\u8def\u7531\u5230\u4e5d\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u540e\uff0c\u5e73\u5747\u8d28\u91cf\u9ad8\u4e8e\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6210\u672c\u4e0e\u751f\u6210\u8d28\u91cf\u7684\u6700\u4f18\u5e73\u8861\u3002"}}
{"id": "2506.14765", "pdf": "https://arxiv.org/pdf/2506.14765", "abs": "https://arxiv.org/abs/2506.14765", "authors": ["Nikolaos Dionelis", "Jente Bosmans", "Riccardo Musto", "Giancarlo Paoletti", "Simone Sarti", "Giacomo Cascarano", "Casper Fibaek", "Luke Camilleri", "Bertrand Le Saux", "Nicolas Long\u00e9p\u00e9"], "title": "Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset", "categories": ["cs.CV"], "comment": "6 pages, 9 figures, 1 table, 29 references", "summary": "Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures. Finally, we fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPhilEO\u7684\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u6570\u636e\u96c6\uff08MajorTOM 23TB\u548cFastTOM 2TB\uff09\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5fae\u8c03\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u536b\u661f\u751f\u6210\u5927\u91cf\u6570\u636e\uff0c\u4f46\u7f3a\u4e4f\u6807\u8bb0\u6570\u636e\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u9ad8\u6548\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\uff0c\u51cf\u5c11\u4e0b\u6e38\u4efb\u52a1\u5bf9\u6807\u8bb0\u6570\u636e\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faPhilEO Geo-Aware U-Net\u6a21\u578b\uff0c\u5e76\u5728MajorTOM\u548cFastTOM\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u7814\u7a76\u4e86\u4e0d\u540c\u53c2\u6570\u548c\u67b6\u6784\u7684\u6a21\u578b\u53d8\u4f53\uff0c\u5e76\u5728PhilEO Bench\u4e0a\u8fdb\u884c\u5fae\u8c03\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "PhilEO 44M MajorTOM 23TB\u6a21\u578b\u5728\u9053\u8def\u5bc6\u5ea6\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0cPhilEO 200M FastTOM\u5728\u9053\u8def\u5bc6\u5ea6\u4f30\u8ba1\u548c\u5efa\u7b51\u5bc6\u5ea6\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5c55\u5bf9\u63d0\u5347\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u4eceU-Net CNN\u8fc7\u6e21\u5230ViT\u67b6\u6784\u4e5f\u503c\u5f97\u7814\u7a76\u3002"}}
{"id": "2506.14766", "pdf": "https://arxiv.org/pdf/2506.14766", "abs": "https://arxiv.org/abs/2506.14766", "authors": ["Yujun Wang", "Jinhe Bi", "Yunpu Ma", "Soeren Pirk"], "title": "ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM", "categories": ["cs.CV", "cs.CL"], "comment": "15 pages, 7 figures", "summary": "Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u53ef\u63a7\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5e72\u9884\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u6709\u6548\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5e38\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u90e8\u5206\u7ebf\u7d22\u800c\u4ea7\u751f\u5e7b\u89c9\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982VCD\u548cICD\uff09\u867d\u6709\u6548\uff0c\u4f46\u5176\u4f5c\u7528\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u6539\u53d8\u6ce8\u610f\u529b\u5206\u5e03\u53d1\u6325\u4f5c\u7528\uff0c\u4ece\u800c\u63d0\u51fa\u66f4\u76f4\u63a5\u5e72\u9884\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u53ef\u63a7\u7684\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\uff0c\u76f4\u63a5\u5e72\u9884\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u800c\u975e\u4ec5\u8c03\u6574\u8868\u9762\u5c42\u7684logits\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aMLLM\u67b6\u6784\u548c\u4e0d\u540c\u89e3\u7801\u65b9\u6cd5\u4e2d\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u5728POPE\u3001CHAIR\u548cMMHal-Bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u6539\u5584\u4e86\u6807\u51c6VQA\u4efb\u52a1\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u5e72\u9884\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8be5\u65b9\u6cd5\u4e3a\u51cf\u5c11MLLM\u5e7b\u89c9\u63d0\u4f9b\u4e86\u66f4\u539f\u5219\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.14769", "pdf": "https://arxiv.org/pdf/2506.14769", "abs": "https://arxiv.org/abs/2506.14769", "authors": ["Jiahua Ma", "Yiran Qin", "Yixiong Li", "Xuanqi Liao", "Yulan Guo", "Ruimao Zhang"], "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.", "AI": {"tldr": "CDP\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u5386\u53f2\u52a8\u4f5c\u5e8f\u5217\u63d0\u5347\u52a8\u4f5c\u9884\u6d4b\u7684\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u786c\u4ef6\u9650\u5236\u548c\u6570\u636e\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u786c\u4ef6\u9650\u5236\u548c\u6570\u636e\u8d28\u91cf\u4e0b\u964d\u5bfc\u81f4\u673a\u5668\u4eba\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u6548\u679c\u964d\u4f4e\uff0c\u5f71\u54cd\u4efb\u52a1\u6267\u884c\u3002", "method": "\u63d0\u51faCausal Diffusion Policy\uff08CDP\uff09\uff0c\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u5386\u53f2\u52a8\u4f5c\u5e8f\u5217\u548c\u7f13\u5b58\u673a\u5236\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0cCDP\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u8f93\u5165\u8d28\u91cf\u4e0b\u964d\u65f6\u4ecd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "CDP\u901a\u8fc7\u65f6\u95f4\u8fde\u7eed\u6027\u63a8\u7406\uff0c\u5728\u73b0\u5b9e\u4e0d\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u3002"}}
{"id": "2506.13807", "pdf": "https://arxiv.org/pdf/2506.13807", "abs": "https://arxiv.org/abs/2506.13807", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Ujjwal Baid", "Hendrik M\u00f6ller", "Josef A. Buchner", "Felix Steinbauer", "Eva Oswald", "Ezequiel de la Rosa", "Ivan Ezhov", "Constantin von See", "Jan Kirschke", "Anton Schmick", "Sarthak Pati", "Akis Linardos", "Carla Pitarch", "Sanyukta Adap", "Jeffrey Rudie", "Maria Correia de Verdier", "Rachit Saluja", "Evan Calabrese", "Dominic LaBella", "Mariam Aboian", "Ahmed W. Moawad", "Nazanin Maleki", "Udunna Anazodo", "Maruf Adewole", "Marius George Linguraru", "Anahita Fathi Kazerooni", "Zhifan Jiang", "Gian Marco Conte", "Hongwei Li", "Juan Eugenio Iglesias", "Spyridon Bakas", "Benedikt Wiestler", "Marie Piraud", "Bjoern Menze"], "title": "BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "27p, 2figs, 3tabs", "summary": "The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.", "AI": {"tldr": "BraTS orchestrator\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5de5\u5177\u5305\uff0c\u65e8\u5728\u7b80\u5316BraTS\u6311\u6218\u8d5b\u4e2d\u5148\u8fdb\u8111\u80bf\u7624\u5206\u5272\u548c\u5408\u6210\u7b97\u6cd5\u7684\u4f7f\u7528\uff0c\u4fc3\u8fdb\u5176\u5728\u79d1\u7814\u548c\u4e34\u5e8a\u4e2d\u7684\u666e\u53ca\u3002", "motivation": "\u5c3d\u7ba1BraTS\u6311\u6218\u8d5b\u5728\u8111\u80bf\u7624\u56fe\u50cf\u5206\u6790\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u7b97\u6cd5\u548c\u6a21\u578b\u5728\u79d1\u5b66\u548c\u4e34\u5e8a\u9886\u57df\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002\u4e3a\u4e86\u52a0\u901f\u8fd9\u4e9b\u6280\u672f\u7684\u4f20\u64ad\uff0c\u5f00\u53d1\u4e86BraTS orchestrator\u3002", "method": "BraTS orchestrator\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5305\uff0c\u63d0\u4f9b\u76f4\u89c2\u7684\u6559\u7a0b\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u8f7b\u677e\u90e8\u7f72BraTS\u6311\u6218\u8d5b\u4e2d\u7684\u4f18\u80dc\u7b97\u6cd5\u3002", "result": "\u8be5\u5de5\u5177\u5305\u7b80\u5316\u4e86\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u7684\u590d\u6742\u6027\uff0c\u4f7f\u66f4\u5e7f\u6cdb\u7684\u795e\u7ecf\u653e\u5c04\u5b66\u548c\u795e\u7ecf\u80bf\u7624\u5b66\u7528\u6237\u80fd\u591f\u8f7b\u677e\u4f7f\u7528\u8fd9\u4e9b\u5148\u8fdb\u6280\u672f\u3002", "conclusion": "BraTS orchestrator\u901a\u8fc7\u5f00\u6e90\u548c\u6613\u7528\u6027\u8bbe\u8ba1\uff0c\u6210\u529f\u63a8\u52a8\u4e86BraTS\u793e\u533a\u4e13\u4e1a\u77e5\u8bc6\u5728\u66f4\u5e7f\u6cdb\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2506.13819", "pdf": "https://arxiv.org/pdf/2506.13819", "abs": "https://arxiv.org/abs/2506.13819", "authors": ["El Arbi Belfarsi", "Henry Flores", "Maria Valero"], "title": "Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Submitted to the IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI 2025)", "summary": "In this study, we present a dual-modal AI framework based on short-wave infrared (SWIR) spectroscopy. The first modality employs a multi-wavelength SWIR imaging system coupled with convolutional neural networks (CNNs) to capture spatial features linked to glucose absorption. The second modality uses a compact photodiode voltage sensor and machine learning regressors (e.g., random forest) on normalized optical signals. Both approaches were evaluated on synthetic blood phantoms and skin-mimicking materials across physiological glucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage error (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error Grid, while the photodiode system reached 86.4% Zone A accuracy. This framework constitutes a state-of-the-art solution that balances clinical accuracy, cost efficiency, and wearable integration, paving the way for reliable continuous non-invasive glucose monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77ed\u6ce2\u7ea2\u5916\u5149\u8c31\u7684\u53cc\u6a21\u6001AI\u6846\u67b6\uff0c\u7ed3\u5408CNN\u548c\u5149\u7535\u538b\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u65e0\u521b\u8840\u7cd6\u76d1\u6d4b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u4e34\u5e8a\u51c6\u786e\u3001\u6210\u672c\u9ad8\u6548\u4e14\u53ef\u7a7f\u6234\u7684\u65e0\u521b\u8840\u7cd6\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u591a\u6ce2\u957fSWIR\u6210\u50cf\u7cfb\u7edf\u4e0eCNN\u6355\u6349\u7a7a\u95f4\u7279\u5f81\uff0c\u4ee5\u53ca\u5149\u7535\u538b\u4f20\u611f\u5668\u4e0e\u673a\u5668\u5b66\u4e60\u56de\u5f52\u5668\u5206\u6790\u5149\u5b66\u4fe1\u53f7\u3002", "result": "CNN\u5728650 nm\u6ce2\u957f\u4e0bMAPE\u4e3a4.82%\uff0c\u5149\u7535\u538b\u7cfb\u7edfZone A\u51c6\u786e\u7387\u4e3a86.4%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e0\u521b\u8840\u7cd6\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u4e34\u5e8a\u51c6\u786e\u6027\u548c\u53ef\u7a7f\u6234\u6027\u3002"}}
{"id": "2506.13888", "pdf": "https://arxiv.org/pdf/2506.13888", "abs": "https://arxiv.org/abs/2506.13888", "authors": ["Jipeng Zhang", "Kehao Miao", "Renjie Pi", "Zhaowei Wang", "Runtao Liu", "Rui Pan", "Tong Zhang"], "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u4e13\u5bb6\u3001\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u62d2\u7edd\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u5956\u52b1\u6a21\u578b\uff08VL-RM\uff09\u8bad\u7ec3\u4e2d\u7684\u81ea\u4e3e\u56f0\u5883\u548c\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e7b\u89c9\u68c0\u6d4b\u548c\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u5956\u52b1\u6a21\u578b\uff08VL-RM\uff09\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9762\u4e34\u81ea\u4e3e\u56f0\u5883\u548c\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u504f\u597d\u6570\u636e\u5b58\u5728\u7f3a\u9677\uff0c\u8fdb\u4e00\u6b65\u8bef\u5bfc\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u4e13\u5bb6\u3001\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u62d2\u7edd\u91c7\u6837\uff0c\u4f18\u5316\u504f\u597d\u6570\u636e\u96c6\u5e76\u589e\u5f3a\u7ed3\u6784\u5316\u6279\u5224\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e7b\u89c9\u68c0\u6d4b\u548c\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2506.14107", "pdf": "https://arxiv.org/pdf/2506.14107", "abs": "https://arxiv.org/abs/2506.14107", "authors": ["Jinwoo Hwang", "Daeun Kim", "Sangyeop Lee", "Yoonsung Kim", "Guseul Heo", "Hojoon Kim", "Yunseok Jeong", "Tadiwos Meaza", "Eunhyeok Park", "Jeongseob Ahn", "Jongse Park"], "title": "D\u00e9j\u00e0 Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse", "categories": ["cs.DC", "cs.CV"], "comment": "Accepted to 2025 VLDB", "summary": "Recently, Video-Language Models (VideoLMs) have demonstrated remarkable capabilities, offering significant potential for flexible and powerful video query systems. These models typically rely on Vision Transformers (ViTs), which process video frames individually to extract visual embeddings. However, generating embeddings for large-scale videos requires ViT inferencing across numerous frames, posing a major hurdle to real-world deployment and necessitating solutions for integration into scalable video data management systems. This paper introduces D\u00e9j\u00e0 Vu, a video-language query engine that accelerates ViT-based VideoLMs by reusing computations across consecutive frames. At its core is ReuseViT, a modified ViT model specifically designed for VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking an effective balance between accuracy and reuse. Although ReuseViT significantly reduces computation, these savings do not directly translate into performance gains on GPUs. To overcome this, D\u00e9j\u00e0 Vu integrates memory-compute joint compaction techniques that convert the FLOP savings into tangible performance gains. Evaluations on three VideoLM tasks show that D\u00e9j\u00e0 Vu accelerates embedding generation by up to a 2.64x within a 2% error bound, dramatically enhancing the practicality of VideoLMs for large-scale video analytics.", "AI": {"tldr": "D\u00e9j\u00e0 Vu\u662f\u4e00\u4e2a\u89c6\u9891\u8bed\u8a00\u67e5\u8be2\u5f15\u64ce\uff0c\u901a\u8fc7\u91cd\u7528\u8fde\u7eed\u5e27\u7684\u8ba1\u7b97\u52a0\u901fViT-based VideoLMs\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u89c6\u9891\u5206\u6790\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709VideoLMs\u5728\u5904\u7406\u5927\u89c4\u6a21\u89c6\u9891\u65f6\u56e0\u9700\u9010\u5e27\u8ba1\u7b97ViT\u5d4c\u5165\u800c\u9762\u4e34\u6027\u80fd\u74f6\u9888\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faReuseViT\uff0c\u6539\u8fdbViT\u6a21\u578b\u4ee5\u68c0\u6d4b\u5e27\u95f4\u91cd\u7528\u673a\u4f1a\uff0c\u7ed3\u5408\u5185\u5b58-\u8ba1\u7b97\u8054\u5408\u538b\u7f29\u6280\u672f\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5728\u4e09\u4e2aVideoLM\u4efb\u52a1\u4e2d\uff0cD\u00e9j\u00e0 Vu\u5c06\u5d4c\u5165\u751f\u6210\u901f\u5ea6\u63d0\u5347\u81f32.64\u500d\uff0c\u8bef\u5dee\u63a7\u5236\u57282%\u4ee5\u5185\u3002", "conclusion": "D\u00e9j\u00e0 Vu\u901a\u8fc7\u8ba1\u7b97\u91cd\u7528\u548c\u4f18\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86VideoLMs\u5728\u5927\u89c4\u6a21\u89c6\u9891\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.14135", "pdf": "https://arxiv.org/pdf/2506.14135", "abs": "https://arxiv.org/abs/2506.14135", "authors": ["Ying Chai", "Litao Deng", "Ruizhi Shao", "Jiajun Zhang", "Liangjun Xing", "Hongwen Zhang", "Yebin Liu"], "title": "GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation", "categories": ["cs.RO", "cs.CV"], "comment": "http://chaiying1.github.io/GAF.github.io/project_page/", "summary": "Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: http://chaiying1.github.io/GAF.github.io/project_page/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e4D\u9ad8\u65af\u52a8\u4f5c\u573a\uff08GAF\uff09\u7684V-4D-A\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u52a8\u6001\u573a\u666f\u4e2d\u76f4\u63a5\u63a8\u7406\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08V-A\u6216V-3D-A\uff09\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u52a8\u4f5c\u63a8\u7406\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u52a8\u4f5c\u63a8\u7406\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u6269\u5c553D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5f15\u5165\u53ef\u5b66\u4e60\u8fd0\u52a8\u5c5e\u6027\uff0c\u6784\u5efaGAF\uff0c\u652f\u6301\u573a\u666f\u91cd\u5efa\u3001\u672a\u6765\u5e27\u9884\u6d4b\u548c\u521d\u59cb\u52a8\u4f5c\u4f30\u8ba1\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4f18\u5316\u52a8\u4f5c\u3002", "result": "GAF\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u63d0\u534711.5385 dB PSNR\u548c-0.5574 LPIPS\uff0c\u673a\u5668\u4eba\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad810.33%\u3002", "conclusion": "GAF\u6846\u67b6\u901a\u8fc74D\u52a8\u6001\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2506.14198", "pdf": "https://arxiv.org/pdf/2506.14198", "abs": "https://arxiv.org/abs/2506.14198", "authors": ["Jeremy A. Collins", "Lor\u00e1nd Cheng", "Kunal Aneja", "Albert Wilcox", "Benjamin Joffe", "Animesh Garg"], "title": "AMPLIFY: Actionless Motion Priors for Robot Learning from Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.", "AI": {"tldr": "AMPLIFY\u6846\u67b6\u5229\u7528\u5927\u89c4\u6a21\u65e0\u52a8\u4f5c\u89c6\u9891\u6570\u636e\uff0c\u901a\u8fc7\u5173\u952e\u70b9\u8f68\u8ff9\u751f\u6210\u7d27\u51d1\u7684\u8fd0\u52a8\u6807\u8bb0\uff0c\u5206\u79bb\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\u4e0e\u52a8\u4f5c\u63a8\u65ad\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u673a\u5668\u4eba\u52a8\u4f5c\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\uff0c\u800c\u65e0\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u4e30\u5bcc\u4f46\u96be\u4ee5\u8f6c\u5316\u4e3a\u6709\u6548\u7b56\u7565\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u5229\u7528\u5f02\u6784\u6570\u636e\u3002", "method": "AMPLIFY\u901a\u8fc7\u5173\u952e\u70b9\u8f68\u8ff9\u751f\u6210\u8fd0\u52a8\u6807\u8bb0\uff0c\u5206\u522b\u8bad\u7ec3\u524d\u5411\u52a8\u529b\u5b66\u6a21\u578b\uff08\u65e0\u52a8\u4f5c\u89c6\u9891\uff09\u548c\u9006\u5411\u52a8\u529b\u5b66\u6a21\u578b\uff08\u5c11\u91cf\u6807\u8bb0\u6570\u636e\uff09\uff0c\u5b9e\u73b0\u6a21\u5757\u5316\u5b66\u4e60\u3002", "result": "\u52a8\u529b\u5b66\u6a21\u578b\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff08MSE\u63d0\u9ad83.7\u500d\uff0c\u50cf\u7d20\u9884\u6d4b\u7cbe\u5ea6\u63d0\u9ad82.5\u500d\uff09\uff0c\u4e0b\u6e38\u7b56\u7565\u5b66\u4e60\u6548\u679c\u63d0\u53471.2-2.2\u500d\uff0c\u5e76\u80fd\u4ece\u96f6\u6570\u636e\u6cdb\u5316\u5230LIBERO\u4efb\u52a1\u3002", "conclusion": "AMPLIFY\u5c55\u793a\u4e86\u5229\u7528\u5f02\u6784\u6570\u636e\u6784\u5efa\u9ad8\u6548\u3001\u901a\u7528\u4e16\u754c\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u53ca\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2506.14209", "pdf": "https://arxiv.org/pdf/2506.14209", "abs": "https://arxiv.org/abs/2506.14209", "authors": ["Pengwei Wang"], "title": "Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.\n  However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.\n  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.\n  The proposed method achieves successful segmentation on both simulated and real patient data.\n  This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522bONJ\u5f71\u50cf\u4e2d\u7684\u5f02\u5e38\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u51cf\u5c11\u4e86\u624b\u52a8\u6807\u6ce8\u7684\u8d1f\u62c5\u3002", "motivation": "\u7531\u4e8eONJ\u5f71\u50cf\u4e2d\u6807\u8bb0\u6570\u636e\u7684\u7a00\u7f3a\uff0c\u76d1\u7763\u8bad\u7ec3\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3VQ-GAN\u91cd\u5efa\u6b63\u5e38\u6570\u636e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u968f\u673a\u548cONJ\u7279\u5b9a\u63a9\u7801\u8bad\u7ec3\u65b0\u7f16\u7801\u5668\u3002", "result": "\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u60a3\u8005\u6570\u636e\u4e0a\u5747\u5b9e\u73b0\u4e86\u6210\u529f\u5206\u5272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5feb\u901f\u521d\u59cb\u5206\u5272\u65b9\u6848\uff0c\u5e76\u6709\u671b\u76f4\u63a5\u7528\u4e8e3D\u6253\u5370\u3002"}}
{"id": "2506.14303", "pdf": "https://arxiv.org/pdf/2506.14303", "abs": "https://arxiv.org/abs/2506.14303", "authors": ["Niran Nataraj", "Maina Sogabe", "Kenji Kawashima"], "title": "orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "24 pages, 7figures", "summary": "Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small \"mimicking organ\" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.", "AI": {"tldr": "orGAN\u662f\u4e00\u4e2a\u57fa\u4e8eGAN\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5e26\u6ce8\u91ca\u7684\u624b\u672f\u51fa\u8840\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u3001\u4f26\u7406\u95ee\u9898\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "motivation": "\u624b\u672f\u4e2d\u51fa\u8840\u68c0\u6d4b\u548c\u5b9a\u4f4d\u56e0\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7a00\u7f3a\u800c\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u4f26\u7406\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "\u5229\u7528\u5c0f\u89c4\u6a21\u201c\u6a21\u62df\u5668\u5b98\u201d\u6570\u636e\u96c6\uff0c\u7ed3\u5408StyleGAN\u548cRelational Positional Learning\u751f\u6210\u903c\u771f\u51fa\u8840\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7LaMa\u4fee\u590d\u6a21\u5757\u63d0\u4f9b\u7cbe\u786e\u6807\u6ce8\u3002", "result": "\u5408\u6210\u4e0e\u771f\u5b9e\u56fe\u50cf\u6df7\u5408\u7684\u6570\u636e\u96c6\u5728\u624b\u672f\u573a\u666f\u4e2d\u8fbe\u523090%\u68c0\u6d4b\u51c6\u786e\u7387\u548c99%\u5e27\u7ea7\u51c6\u786e\u7387\u3002", "conclusion": "orGAN\u4e3a\u624b\u672fAI\u63d0\u4f9b\u4e86\u4f26\u7406\u5408\u89c4\u3001\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u6807\u6ce8\u6570\u636e\u96c6\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2506.14315", "pdf": "https://arxiv.org/pdf/2506.14315", "abs": "https://arxiv.org/abs/2506.14315", "authors": ["Jinyan Yuan", "Bangbang Yang", "Keke Wang", "Panwang Pan", "Lin Ma", "Xuehai Zhang", "Xiao Liu", "Zhaopeng Cui", "Yuewen Ma"], "title": "ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured Proxies", "categories": ["cs.GR", "cs.CV"], "comment": "Project webpage: https://immersegen.github.io", "summary": "Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery.This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: https://immersegen.github.io.", "AI": {"tldr": "ImmerseGen\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u5f15\u5bfc\u7684\u8f7b\u91cf\u7ea73D\u573a\u666f\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u51e0\u4f55\u4ee3\u7406\u548cRGBA\u7eb9\u7406\u5408\u6210\u5b9e\u73b0\u9ad8\u6548\u4e14\u903c\u771f\u7684VR\u573a\u666f\u751f\u6210\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u9ad8\u591a\u8fb9\u5f62\u7f51\u683c\u6216\u5927\u89c4\u6a213D\u9ad8\u65af\u6a21\u578b\uff0c\u5bfc\u81f4\u6d41\u7a0b\u590d\u6742\u6216\u89c6\u89c9\u771f\u5b9e\u611f\u4e0d\u8db3\u3002ImmerseGen\u65e8\u5728\u7b80\u5316\u5efa\u6a21\u6d41\u7a0b\u5e76\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u5206\u5c42\u51e0\u4f55\u4ee3\u7406\uff08\u7b80\u5316\u5730\u5f62\u548c\u5e7f\u544a\u724c\u7f51\u683c\uff09\u548cRGBA\u7eb9\u7406\u5408\u6210\uff0c\u7ed3\u5408\u5730\u5f62\u6761\u4ef6\u7eb9\u7406\u548cVLM\u4ee3\u7406\u5b9e\u73b0\u81ea\u52a8\u5316\u573a\u666f\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cImmerseGen\u5728\u903c\u771f\u5ea6\u3001\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u6e32\u67d3\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ImmerseGen\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4ee3\u7406\u548c\u7eb9\u7406\u5408\u6210\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u903c\u771f\u7684VR\u573a\u666f\u5efa\u6a21\uff0c\u9002\u7528\u4e8e\u79fb\u52a8VR\u8bbe\u5907\u3002"}}
{"id": "2506.14318", "pdf": "https://arxiv.org/pdf/2506.14318", "abs": "https://arxiv.org/abs/2506.14318", "authors": ["Amirreza Fateh", "Yasin Rezvani", "Sara Moayedi", "Sadjad Rezvani", "Fatemeh Fateh", "Mansoor Fateh"], "title": "BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification with Swin-HAFNet", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis, largely due to the lack of high-quality, balanced, and diverse datasets. In this work, we present a new curated MRI dataset designed specifically for brain tumor segmentation and classification tasks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified radiologists and physicians, spanning three major tumor types-glioma, meningioma, and pituitary-as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we propose a transformer-based segmentation model and benchmark it against established baselines. Our method achieves the highest weighted mean Intersection-over-Union (IoU) of 82.3%, with improvements observed across all tumor categories. Importantly, this study serves primarily as an introduction to the dataset, establishing foundational benchmarks for future research. We envision this dataset as a valuable resource for advancing machine learning applications in neuro-oncology, supporting both academic research and clinical decision-support development. datasetlink: https://www.kaggle.com/datasets/briscdataset/brisc2025/", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684MRI\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\u548c\u5206\u7c7b\u4efb\u52a1\uff0c\u5305\u542b6000\u4e2a\u6807\u6ce8\u6837\u672c\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eTransformer\u7684\u5206\u5272\u6a21\u578b\uff0c\u53d6\u5f97\u4e8682.3%\u7684\u52a0\u6743\u5e73\u5747IoU\u3002", "motivation": "\u8111\u80bf\u7624\u7684\u51c6\u786e\u5206\u5272\u548c\u5206\u7c7b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u5e73\u8861\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b6000\u4e2a\u6807\u6ce8MRI\u626b\u63cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u5206\u5272\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u52a0\u6743\u5e73\u5747IoU\u4e0a\u8fbe\u523082.3%\uff0c\u5728\u6240\u6709\u80bf\u7624\u7c7b\u522b\u4e2d\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u795e\u7ecf\u80bf\u7624\u5b66\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u652f\u6301\u5b66\u672f\u7814\u7a76\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2506.14381", "pdf": "https://arxiv.org/pdf/2506.14381", "abs": "https://arxiv.org/abs/2506.14381", "authors": ["Yuxuan Jiang", "Siyue Teng", "Qiang Zhu", "Chen Feng", "Chengxi Zeng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull"], "title": "Compressed Video Super-Resolution based on Hierarchical Encoding", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper presents a general-purpose video super-resolution (VSR) method, dubbed VSR-HE, specifically designed to enhance the perceptual quality of compressed content. Targeting scenarios characterized by heavy compression, the method upscales low-resolution videos by a ratio of four, from 180p to 720p or from 270p to 1080p. VSR-HE adopts hierarchical encoding transformer blocks and has been sophisticatedly optimized to eliminate a wide range of compression artifacts commonly introduced by H.265/HEVC encoding across various quantization parameter (QP) levels. To ensure robustness and generalization, the model is trained and evaluated under diverse compression settings, allowing it to effectively restore fine-grained details and preserve visual fidelity. The proposed VSR-HE has been officially submitted to the ICME 2025 Grand Challenge on VSR for Video Conferencing (Team BVI-VSR), under both the Track 1 (General-Purpose Real-World Video Content) and Track 2 (Talking Head Videos).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5VSR-HE\uff0c\u4e13\u6ce8\u4e8e\u63d0\u5347\u538b\u7f29\u5185\u5bb9\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u53d8\u6362\u5757\u6d88\u9664\u538b\u7f29\u4f2a\u5f71\uff0c\u5e76\u5728\u591a\u79cd\u538b\u7f29\u8bbe\u7f6e\u4e0b\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u9488\u5bf9\u9ad8\u538b\u7f29\u573a\u666f\uff0c\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u6d88\u9664H.265/HEVC\u7f16\u7801\u5f15\u5165\u7684\u538b\u7f29\u4f2a\u5f71\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7f16\u7801\u53d8\u6362\u5757\uff0c\u4f18\u5316\u6a21\u578b\u4ee5\u6d88\u9664\u591a\u79cd\u91cf\u5316\u53c2\u6570\u4e0b\u7684\u538b\u7f29\u4f2a\u5f71\uff0c\u652f\u6301\u4ece180p\u5230720p\u6216270p\u52301080p\u7684\u8d85\u5206\u8fa8\u7387\u3002", "result": "\u6a21\u578b\u5728\u591a\u79cd\u538b\u7f29\u8bbe\u7f6e\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u80fd\u6709\u6548\u6062\u590d\u7ec6\u8282\u5e76\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "VSR-HE\u65b9\u6cd5\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5df2\u63d0\u4ea4\u81f3ICME 2025\u6311\u6218\u8d5b\u3002"}}
{"id": "2506.14390", "pdf": "https://arxiv.org/pdf/2506.14390", "abs": "https://arxiv.org/abs/2506.14390", "authors": ["Conrad Orglmeister", "Erik Bochinski", "Volker Eiselein", "Elvira Fleig"], "title": "Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection", "categories": ["cs.LG", "cs.CV"], "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in Computer Safety, Reliability and Security - SAFECOMP 2024 Workshops - DECSoS, SASSUR, TOASTS, and WAISE, and is available online at https://doi.org/10.1007/978-3-031-68738-9_29", "summary": "Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u89e3\u91ca\u539f\u578b\u53d8\u5206\u6a21\u578b\u4e0e\u81ea\u7f16\u7801\u5668\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff0c\u7528\u4e8e\u5206\u7c7b\u3001OOD\u68c0\u6d4b\u548c\u91cd\u5efa\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63d0\u9ad8\u6df1\u5ea6\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u76f8\u5173\u5e94\u7528\u4e2d\u7684\u51b3\u7b56\u900f\u660e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9a\u4e49\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u4e3aID\u533a\u57df\uff0c\u5f15\u5165\u9650\u5236\u635f\u5931\u4ee5\u538b\u7f29ID\u533a\u57df\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u89e3\u91ca\u6027\u3002", "result": "\u5728OOD\u68c0\u6d4b\u57fa\u51c6\u548c\u5b9e\u9645\u94c1\u8def\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u539f\u578b\u89e3\u91ca\u6027\u548cOOD\u68c0\u6d4b\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.14432", "pdf": "https://arxiv.org/pdf/2506.14432", "abs": "https://arxiv.org/abs/2506.14432", "authors": ["Asbj\u00f8rn Munk", "Stefano Cerri", "Jakob Ambsdorf", "Julia Machnio", "Sebastian N\u00f8rgaard Llambias", "Vardan Nersesjan", "Christian Hedeager Krag", "Peirong Liu", "Pablo Rocamora Garc\u00eda", "Mostafa Mehdipour Ghazi", "Mikael Boesen", "Michael Eriksen Benros", "Juan Eugenio Iglesias", "Mads Nielsen"], "title": "A large-scale heterogeneous 3D magnetic resonance brain imaging dataset for self-supervised learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present FOMO60K, a large-scale, heterogeneous dataset of 60,529 brain Magnetic Resonance Imaging (MRI) scans from 13,900 sessions and 11,187 subjects, aggregated from 16 publicly available sources. The dataset includes both clinical- and research-grade images, multiple MRI sequences, and a wide range of anatomical and pathological variability, including scans with large brain anomalies. Minimal preprocessing was applied to preserve the original image characteristics while reducing barriers to entry for new users. Accompanying code for self-supervised pretraining and finetuning is provided. FOMO60K is intended to support the development and benchmarking of self-supervised learning methods in medical imaging at scale.", "AI": {"tldr": "FOMO60K\u662f\u4e00\u4e2a\u5305\u542b60,529\u4e2a\u8111\u90e8MRI\u626b\u63cf\u7684\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u533b\u5b66\u5f71\u50cf\u4e2d\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u8111\u90e8MRI\u6570\u636e\u96c6\uff0c\u4ee5\u964d\u4f4e\u65b0\u7528\u6237\u7684\u5165\u95e8\u95e8\u69db\uff0c\u5e76\u63a8\u52a8\u533b\u5b66\u5f71\u50cf\u4e2d\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u6570\u636e\u96c6\u6574\u5408\u4e8616\u4e2a\u516c\u5f00\u6765\u6e90\u7684\u4e34\u5e8a\u548c\u7814\u7a76\u7ea7\u56fe\u50cf\uff0c\u8fdb\u884c\u4e86\u6700\u5c0f\u9884\u5904\u7406\u4ee5\u4fdd\u7559\u539f\u59cb\u7279\u5f81\uff0c\u5e76\u63d0\u4f9b\u4e86\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u4ee3\u7801\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u591a\u79cdMRI\u5e8f\u5217\u548c\u5e7f\u6cdb\u7684\u89e3\u5256\u53ca\u75c5\u7406\u53d8\u5f02\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7814\u7a76\u3002", "conclusion": "FOMO60K\u4e3a\u533b\u5b66\u5f71\u50cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8d44\u6e90\u548c\u5de5\u5177\u3002"}}
{"id": "2506.14497", "pdf": "https://arxiv.org/pdf/2506.14497", "abs": "https://arxiv.org/abs/2506.14497", "authors": ["Franco Matzkin", "Agostina Larrazabal", "Diego H Milone", "Jose Dolz", "Enzo Ferrante"], "title": "Towards Reliable WMH Segmentation under Domain Shift: An Application Study using Maximum Entropy Regularization to Improve Uncertainty Estimation", "categories": ["eess.IV", "cs.CV"], "comment": "32 pages, 7 figures", "summary": "Accurate segmentation of white matter hyperintensities (WMH) is crucial for clinical decision-making, particularly in the context of multiple sclerosis. However, domain shifts, such as variations in MRI machine types or acquisition parameters, pose significant challenges to model calibration and uncertainty estimation. This study investigates the impact of domain shift on WMH segmentation by proposing maximum-entropy regularization techniques to enhance model calibration and uncertainty estimation, with the purpose of identifying errors post-deployment using predictive uncertainty as a proxy measure that does not require ground-truth labels. To do this, we conducted experiments using a U-Net architecture to evaluate these regularization schemes on two publicly available datasets, assessing performance with the Dice coefficient, expected calibration error, and entropy-based uncertainty estimates. Our results show that entropy-based uncertainty estimates can anticipate segmentation errors, and that maximum-entropy regularization further strengthens the correlation between uncertainty and segmentation performance while also improving model calibration under domain shift.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u6700\u5927\u71b5\u6b63\u5219\u5316\u6280\u672f\uff0c\u7528\u4e8e\u6539\u5584\u767d\u8d28\u9ad8\u4fe1\u53f7\uff08WMH\uff09\u5206\u5272\u6a21\u578b\u7684\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4ee5\u5e94\u5bf9MRI\u6570\u636e\u57df\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u767d\u8d28\u9ad8\u4fe1\u53f7\uff08WMH\uff09\u7684\u51c6\u786e\u5206\u5272\u5bf9\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46MRI\u673a\u5668\u7c7b\u578b\u6216\u91c7\u96c6\u53c2\u6570\u7684\u5dee\u5f02\uff08\u57df\u504f\u79fb\uff09\u5bf9\u6a21\u578b\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5e26\u6765\u6311\u6218\u3002", "method": "\u91c7\u7528U-Net\u67b6\u6784\uff0c\u7ed3\u5408\u6700\u5927\u71b5\u6b63\u5219\u5316\u6280\u672f\uff0c\u8bc4\u4f30\u5176\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528Dice\u7cfb\u6570\u3001\u9884\u671f\u6821\u51c6\u8bef\u5dee\u548c\u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u9884\u6d4b\u5206\u5272\u9519\u8bef\uff0c\u6700\u5927\u71b5\u6b63\u5219\u5316\u589e\u5f3a\u4e86\u4e0d\u786e\u5b9a\u6027\u4e0e\u5206\u5272\u6027\u80fd\u7684\u76f8\u5173\u6027\uff0c\u5e76\u6539\u5584\u4e86\u57df\u504f\u79fb\u4e0b\u7684\u6a21\u578b\u6821\u51c6\u3002", "conclusion": "\u6700\u5927\u71b5\u6b63\u5219\u5316\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347WMH\u5206\u5272\u6a21\u578b\u5728\u57df\u504f\u79fb\u4e0b\u7684\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\u3002"}}
{"id": "2506.14513", "pdf": "https://arxiv.org/pdf/2506.14513", "abs": "https://arxiv.org/abs/2506.14513", "authors": ["Farha Abdul Wasay", "Mohammed Abdul Rahman", "Hania Ghouse"], "title": "GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.", "AI": {"tldr": "GAMORA\u662f\u4e00\u79cd\u57fa\u4e8eVR\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u98ce\u9669\u7684\u5b9e\u9a8c\u5ba4\u73af\u5883\uff0c\u901a\u8fc7\u624b\u52bf\u63a7\u5236\u5b9e\u73b0\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u7cbe\u786e\u5ea6\u3002", "motivation": "\u968f\u7740\u751f\u7269\u5371\u5bb3\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u51cf\u5c11\u4eba\u7c7b\u76f4\u63a5\u66b4\u9732\u5e76\u4fdd\u6301\u64cd\u4f5c\u7cbe\u786e\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408Oculus Quest 2\u3001NVIDIA Jetson Nano\u548cROS\uff0c\u63d0\u4f9b\u5b9e\u65f6\u6c89\u6d78\u5f0f\u63a7\u5236\u3001\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u548c\u9006\u8fd0\u52a8\u5b66\u64cd\u4f5c\u3002", "result": "GAMORA\u5b9e\u73b0\u4e862.2\u6beb\u7c73\u7684\u5e73\u5747\u4f4d\u7f6e\u8bef\u5dee\u30010.2\u6beb\u5347\u7684\u79fb\u6db2\u7cbe\u5ea6\u548c1.2\u6beb\u7c73\u7684\u91cd\u590d\u6027\uff0c\u5e76\u964d\u4f4e\u4e8650%\u7684\u80fd\u8017\u3002", "conclusion": "GAMORA\u4e3a\u9ad8\u98ce\u9669\u5b9e\u9a8c\u5ba4\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6c89\u6d78\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u751f\u7269\u5b89\u5168\u6027\u548c\u64cd\u4f5c\u6548\u7387\u3002"}}
{"id": "2506.14515", "pdf": "https://arxiv.org/pdf/2506.14515", "abs": "https://arxiv.org/abs/2506.14515", "authors": ["Prabhav Sanga", "Jaskaran Singh", "Arun K. Dubey"], "title": "Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at ICML MUGen'25", "summary": "As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \\textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.", "AI": {"tldr": "FAMR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u5904\u7406\u9057\u5fd8\u6846\u67b6\uff0c\u7528\u4e8e\u6df1\u5ea6\u56fe\u50cf\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u5b9e\u73b0\u9009\u62e9\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u53d7\u9690\u79c1\u6cd5\u89c4\u7ea6\u675f\u7684\u6570\u636e\uff0c\u9009\u62e9\u6027\u9057\u5fd8\u7279\u5b9a\u4fe1\u606f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "FAMR\u5c06\u9057\u5fd8\u95ee\u9898\u8f6c\u5316\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u9057\u5fd8\u96c6\u4e0a\u7684\u5747\u5300\u9884\u6d4b\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u21132\u60e9\u7f5a\u951a\u5b9a\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728CIFAR-10\u548cImageNet-100\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFAMR\u5728\u4fdd\u7559\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u9057\u5fd8\u3002", "conclusion": "FAMR\u4e3a\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u9a8c\u8bc1\u7684\u9ad8\u6548\u540e\u5904\u7406\u9057\u5fd8\u65b9\u6cd5\u3002"}}
{"id": "2506.14524", "pdf": "https://arxiv.org/pdf/2506.14524", "abs": "https://arxiv.org/abs/2506.14524", "authors": ["Nadezhda Alsahanova", "Pavel Bartenev", "Maksim Sharaev", "Milos Ljubisavljevic", "Taleb Al. Mansoori", "Yauhen Statsenko"], "title": "Integrating Radiomics with Deep Learning Enhances Multiple Sclerosis Lesion Delineation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Background: Accurate lesion segmentation is critical for multiple sclerosis (MS) diagnosis, yet current deep learning approaches face robustness challenges.\n  Aim: This study improves MS lesion segmentation by combining data fusion and deep learning techniques.\n  Materials and Methods: We suggested novel radiomic features (concentration rate and R\u00e9nyi entropy) to characterize different MS lesion types and fused these with raw imaging data. The study integrated radiomic features with imaging data through a ResNeXt-UNet architecture and attention-augmented U-Net architecture. Our approach was evaluated on scans from 46 patients (1102 slices), comparing performance before and after data fusion.\n  Results: The radiomics-enhanced ResNeXt-UNet demonstrated high segmentation accuracy, achieving significant improvements in precision and sensitivity over the MRI-only baseline and a Dice score of 0.774$\\pm$0.05; p<0.001 according to Bonferroni-adjusted Wilcoxon signed-rank tests. The radiomics-enhanced attention-augmented U-Net model showed a greater model stability evidenced by reduced performance variability (SDD = 0.18 $\\pm$ 0.09 vs. 0.21 $\\pm$ 0.06; p=0.03) and smoother validation curves with radiomics integration.\n  Conclusion: These results validate our hypothesis that fusing radiomics with raw imaging data boosts segmentation performance and stability in state-of-the-art models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u548c\u539f\u59cb\u5f71\u50cf\u6570\u636e\uff0c\u63d0\u5347\u4e86\u591a\u53d1\u6027\u786c\u5316\u75c7\uff08MS\uff09\u75c5\u7076\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728MS\u75c5\u7076\u5206\u5272\u4e2d\u5b58\u5728\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u878d\u5408\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6539\u8fdb\u5206\u5272\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff08\u6d53\u5ea6\u7387\u548cR\u00e9nyi\u71b5\uff09\uff0c\u5e76\u5c06\u5176\u4e0e\u539f\u59cb\u5f71\u50cf\u6570\u636e\u878d\u5408\uff0c\u91c7\u7528ResNeXt-UNet\u548c\u6ce8\u610f\u529b\u589e\u5f3aU-Net\u67b6\u6784\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u878d\u5408\u653e\u5c04\u7ec4\u5b66\u7684ResNeXt-UNet\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u654f\u611f\u6027\uff08Dice\u5f97\u52060.774\u00b10.05\uff09\uff0c\u6ce8\u610f\u529b\u589e\u5f3aU-Net\u5219\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6a21\u578b\u7a33\u5b9a\u6027\u3002", "conclusion": "\u653e\u5c04\u7ec4\u5b66\u4e0e\u5f71\u50cf\u6570\u636e\u7684\u878d\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u548c\u6a21\u578b\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.14542", "pdf": "https://arxiv.org/pdf/2506.14542", "abs": "https://arxiv.org/abs/2506.14542", "authors": ["Xie Shuyang", "Zhou Jie", "Xu Bo", "Wang Jun", "Xu Renjing"], "title": "MobileHolo: A Lightweight Complex-Valued Deformable CNN for High-Quality Computer-Generated Hologram", "categories": ["physics.optics", "cs.CV"], "comment": "8 pages, 9 figures", "summary": "Holographic displays have significant potential in virtual reality and augmented reality owing to their ability to provide all the depth cues. Deep learning-based methods play an important role in computer-generated holograms (CGH). During the diffraction process, each pixel exerts an influence on the reconstructed image. However, previous works face challenges in capturing sufficient information to accurately model this process, primarily due to the inadequacy of their effective receptive field (ERF). Here, we designed complex-valued deformable convolution for integration into network, enabling dynamic adjustment of the convolution kernel's shape to increase flexibility of ERF for better feature extraction. This approach allows us to utilize a single model while achieving state-of-the-art performance in both simulated and optical experiment reconstructions, surpassing existing open-source models. Specifically, our method has a peak signal-to-noise ratio that is 2.04 dB, 5.31 dB, and 9.71 dB higher than that of CCNN-CGH, HoloNet, and Holo-encoder, respectively, when the resolution is 1920$\\times$1072. The number of parameters of our model is only about one-eighth of that of CCNN-CGH.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8ba1\u7b97\u673a\u751f\u6210\u5168\u606f\u56fe\uff08CGH\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u590d\u6570\u53ef\u53d8\u5f62\u5377\u79ef\u7f51\u7edc\uff0c\u52a8\u6001\u8c03\u6574\u5377\u79ef\u6838\u5f62\u72b6\u4ee5\u63d0\u5347\u6709\u6548\u611f\u53d7\u91ce\uff08ERF\uff09\u7684\u7075\u6d3b\u6027\uff0c\u4ece\u800c\u5728\u6a21\u62df\u548c\u5149\u5b66\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5168\u606f\u663e\u793a\u5728\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u6709\u6548\u611f\u53d7\u91ce\u4e0d\u8db3\u800c\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u884d\u5c04\u8fc7\u7a0b\u3002", "method": "\u8bbe\u8ba1\u590d\u6570\u53ef\u53d8\u5f62\u5377\u79ef\u7f51\u7edc\uff0c\u52a8\u6001\u8c03\u6574\u5377\u79ef\u6838\u5f62\u72b6\u4ee5\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u57281920\u00d71072\u5206\u8fa8\u7387\u4e0b\uff0c\u5cf0\u503c\u4fe1\u566a\u6bd4\u5206\u522b\u6bd4CCNN-CGH\u3001HoloNet\u548cHolo-encoder\u9ad82.04 dB\u30015.31 dB\u548c9.71 dB\uff0c\u4e14\u6a21\u578b\u53c2\u6570\u4ec5\u4e3aCCNN-CGH\u7684\u516b\u5206\u4e4b\u4e00\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u63d0\u5347ERF\u7075\u6d3b\u6027\uff0c\u5b9e\u73b0\u4e86\u5355\u6a21\u578b\u4e0b\u7684\u6700\u4f18\u6027\u80fd\uff0c\u4e3a\u5168\u606f\u663e\u793a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.14582", "pdf": "https://arxiv.org/pdf/2506.14582", "abs": "https://arxiv.org/abs/2506.14582", "authors": ["Kaleel Mahmood", "Caleb Manicke", "Ethan Rathbun", "Aayushi Verma", "Sohaib Ahmad", "Nicholas Stamatakis", "Laurent Michel", "Benjamin Fuller"], "title": "Busting the Paper Ballot: Voting Meets Adversarial Machine Learning", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "18 Pages. Author version of article to appear at CCS 2025", "summary": "We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers.\n  Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50% or greater) attack success rate is ideal. However, for certain elections, even a 5% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u7f8e\u56fd\u9009\u4e3e\u8ba1\u7968\u5668\u4e2d\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u591a\u79cd\u6a21\u578b\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u767d\u76d2\u653b\u51fb\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793a\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5728\u9009\u4e3e\u8ba1\u7968\u4e2d\u7684\u6f5c\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165\u56db\u4e2a\u65b0\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u591a\u79cd\u6a21\u578b\uff08\u5982SVM\u3001CNN\u3001ViT\uff09\u3001\u5206\u6790\u68af\u5ea6\u63a9\u853d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4f20\u7edf\u767d\u76d2\u653b\u51fb\u56e0\u68af\u5ea6\u63a9\u853d\u65e0\u6548\uff0c\u6539\u8fdb\u65b9\u6cd5\u5728\u7269\u7406\u653b\u51fb\u4e2d\u53ef\u5b9e\u73b05%\u6210\u529f\u7387\uff0c\u8db3\u4ee5\u5f71\u54cd\u9009\u4e3e\u7ed3\u679c\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u9009\u4e3e\u8ba1\u7968\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u9700\u6539\u8fdb\u5b89\u5168\u63aa\u65bd\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u5a01\u80c1\u3002"}}
{"id": "2506.14698", "pdf": "https://arxiv.org/pdf/2506.14698", "abs": "https://arxiv.org/abs/2506.14698", "authors": ["Sidney Bender", "Jan Herrmann", "Klaus-Robert M\u00fcller", "Gr\u00e9goire Montavon"], "title": "Towards Desiderata-Driven Design of Visual Counterfactual Explainers", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff08SCE\uff09\uff0c\u65e8\u5728\u5f25\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u91ca\u7684\u5168\u9762\u6027\uff08\u5982\u4fdd\u771f\u5ea6\u3001\u53ef\u7406\u89e3\u6027\u548c\u5145\u5206\u6027\uff09\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5668\uff08VCEs\uff09\u8fc7\u4e8e\u5173\u6ce8\u6837\u672c\u8d28\u91cf\u6216\u6700\u5c0f\u53d8\u5316\uff0c\u5ffd\u7565\u4e86\u89e3\u91ca\u7684\u5168\u9762\u6027\u9700\u6c42\u3002", "method": "\u63a2\u7d22\u65b0\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u673a\u5236\uff0c\u5e76\u6574\u5408\u4e3a\u2018\u5e73\u6ed1\u53cd\u4e8b\u5b9e\u63a2\u7d22\u5668\u2019\uff08SCE\uff09\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u7684\u7cfb\u7edf\u8bc4\u4f30\u9a8c\u8bc1\u4e86SCE\u7684\u6709\u6548\u6027\u3002", "conclusion": "SCE\u7b97\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u6ee1\u8db3\u89e3\u91ca\u7684\u5168\u9762\u6027\u9700\u6c42\uff0c\u63d0\u5347\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u900f\u660e\u5ea6\u3002"}}
{"id": "2506.14719", "pdf": "https://arxiv.org/pdf/2506.14719", "abs": "https://arxiv.org/abs/2506.14719", "authors": ["Haley Duba-Sullivan", "Aniket Pramanik", "Venkatakrishnan Singanallur", "Amirkoushyar Ziabari"], "title": "Plug-and-Play with 2.5D Artifact Reduction Prior for Fast and Accurate Industrial Computed Tomography Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "Submitted to Journal of Nondestructive Evaluation", "summary": "Cone-beam X-ray computed tomography (XCT) is an essential imaging technique for generating 3D reconstructions of internal structures, with applications ranging from medical to industrial imaging. Producing high-quality reconstructions typically requires many X-ray measurements; this process can be slow and expensive, especially for dense materials. Recent work incorporating artifact reduction priors within a plug-and-play (PnP) reconstruction framework has shown promising results in improving image quality from sparse-view XCT scans while enhancing the generalizability of deep learning-based solutions. However, this method uses a 2D convolutional neural network (CNN) for artifact reduction, which captures only slice-independent information from the 3D reconstruction, limiting performance. In this paper, we propose a PnP reconstruction method that uses a 2.5D artifact reduction CNN as the prior. This approach leverages inter-slice information from adjacent slices, capturing richer spatial context while remaining computationally efficient. We show that this 2.5D prior not only improves the quality of reconstructions but also enables the model to directly suppress commonly occurring XCT artifacts (such as beam hardening), eliminating the need for artifact correction pre-processing. Experiments on both experimental and synthetic cone-beam XCT data demonstrate that the proposed method better preserves fine structural details, such as pore size and shape, leading to more accurate defect detection compared to 2D priors. In particular, we demonstrate strong performance on experimental XCT data using a 2.5D artifact reduction prior trained entirely on simulated scans, highlighting the proposed method's ability to generalize across domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2.5D CNN\u7684PnP\u91cd\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a00\u758f\u89c6\u56feXCT\u626b\u63cf\uff0c\u901a\u8fc7\u5229\u7528\u76f8\u90bb\u5207\u7247\u4fe1\u606f\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u76f4\u63a5\u6291\u5236\u5e38\u89c1\u4f2a\u5f71\u3002", "motivation": "\u4f20\u7edf2D CNN\u5728XCT\u91cd\u5efa\u4e2d\u4ec5\u80fd\u6355\u6349\u5207\u7247\u72ec\u7acb\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6027\u80fd\uff0c\u800c2.5D CNN\u80fd\u5229\u7528\u76f8\u90bb\u5207\u7247\u4fe1\u606f\uff0c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u91c7\u75282.5D CNN\u4f5c\u4e3a\u5148\u9a8c\uff0c\u7ed3\u5408PnP\u6846\u67b6\uff0c\u76f4\u63a5\u6291\u5236XCT\u4f2a\u5f71\uff08\u5982\u5c04\u675f\u786c\u5316\uff09\uff0c\u65e0\u9700\u9884\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c2.5D\u65b9\u6cd5\u5728\u4fdd\u7559\u7ec6\u5fae\u7ed3\u6784\uff08\u5982\u5b54\u9699\u5927\u5c0f\u548c\u5f62\u72b6\uff09\u548c\u7f3a\u9677\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e2D\u65b9\u6cd5\uff0c\u4e14\u80fd\u8de8\u57df\u6cdb\u5316\u3002", "conclusion": "2.5D PnP\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56feXCT\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4f2a\u5f71\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
