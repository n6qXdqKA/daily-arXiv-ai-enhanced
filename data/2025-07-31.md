[[toc]]

## cs.CV

### [1] [Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?](https://arxiv.org/abs/2507.22099)
*Shuqing Li,Qiang Chen,Xiaoxue Ren,Michael R. Lyu*

Main category: cs.CV

TL;DR: 本文首次大规模实证研究了物理引擎（PE）中的物理失效问题，提出了失效分类、检测方法评估及开发者反馈，并发布了相关资源。

- Motivation: 物理引擎在多种应用中至关重要，但现有测试方法无法有效检测复杂的物理失效问题，影响软件可靠性和用户体验。
- Method: 研究通过实证分析，探讨物理失效的表现形式、检测技术的有效性及开发者对当前检测实践的看法。
- Result: 提出了物理失效的分类法，评估了多种检测方法（如深度学习和多模态模型），并总结了开发者的改进建议。
- Conclusion: 研究为物理失效检测提供了新见解和资源，有助于改进物理引擎的可靠性。


### [2] [Trade-offs in Image Generation: How Do Different Dimensions Interact?](https://arxiv.org/abs/2507.22100)
*Sicheng Zhang,Binzhu Xie,Zhonghao Yan,Yuli Zhang,Donghao Zhou,Xiaofei Chen,Shi Qiu,Jiaqi Liu,Guoyang Xie,Zhichao Lu*

Main category: cs.CV

TL;DR: 论文提出了TRIG-Bench和TRIGScore，用于量化文本到图像和图像到图像生成模型在多维度上的权衡，并通过实验展示了其有效性。

- Motivation: 现有研究缺乏对生成模型在多维度（如质量、对齐、多样性等）上复杂权衡的量化分析，主要因为数据集和单一指标的局限性。
- Method: 引入TRIG-Bench数据集（覆盖10个维度、40,200样本）和TRIGScore（基于VLM的自动评分指标），并开发了关系识别系统生成维度权衡图（DTM）。
- Result: 评估了14个模型，DTM能全面展示模型在维度间的权衡，且通过微调可改善模型在特定维度的弱点。
- Conclusion: TRIG-Bench和TRIGScore为生成模型的维度权衡分析提供了有效工具，DTM有助于优化模型性能。


### [3] [AI in Agriculture: A Survey of Deep Learning Techniques for Crops, Fisheries and Livestock](https://arxiv.org/abs/2507.22101)
*Umair Nawaz,Muhammad Zaigham Zaheer,Fahad Shahbaz Khan,Hisham Cholakkal,Salman Khan,Rao Muhammad Anwer*

Main category: cs.CV

TL;DR: 本文综述了人工智能在农业领域的应用，包括传统机器学习、深度学习和视觉语言基础模型，并探讨了数据变异性、性能评估等挑战及未来研究方向。

- Motivation: 全球粮食生产面临气候变异性、资源限制和可持续管理等挑战，需要高效、准确且可扩展的技术解决方案，人工智能（AI）在此领域具有重要作用。
- Method: 系统综述了200多篇研究，涵盖传统机器学习、深度学习（如视觉变换器）和视觉语言基础模型（如CLIP），应用于作物病害检测、畜牧健康管理等任务。
- Result: 总结了农业AI的主要实现挑战（如数据变异性）和实验方面（如数据集、性能评估指标），并提出了未来研究方向。
- Conclusion: 未来研究应关注多模态数据整合、边缘设备高效部署和适应多样化农业环境的AI模型。


### [4] [Color as the Impetus: Transforming Few-Shot Learner](https://arxiv.org/abs/2507.22136)
*Chaofei Qi,Zhitai Liu,Jianbin Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种受人类颜色感知启发的元学习框架ColorSense Learner，通过颜色通道交互和知识蒸馏提升小样本分类性能。

- Motivation: 人类颜色感知能力为小样本学习提供了新视角，现有方法忽视了颜色信息的重要性。
- Method: 提出ColorSense Learner框架，利用颜色通道交互提取特征，并结合知识蒸馏（ColorSense Distiller）增强学习能力。
- Result: 在11个小样本基准测试中表现出强大的泛化能力、鲁棒性和迁移性。
- Conclusion: 通过模拟人类颜色感知机制，显著提升了小样本分类效果。


### [5] [Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset](https://arxiv.org/abs/2507.22152)
*A. Piffer,J. A. Buchner,A. G. Gennari,P. Grehten,S. Sirin,E. Ross,I. Ezhov,M. Rosier,J. C. Peeken,M. Piraud,B. Menze,A. Guerreiro Stücklin,A. Jakab,F. Kofler*

Main category: cs.CV

TL;DR: 深度学习模型nnU-Net在儿童脑肿瘤分割中表现良好，尤其在整体肿瘤和T2高信号区域，但对增强肿瘤和囊性成分分割效果较差。

- Motivation: 儿童脑肿瘤（PBTs）的多样性和异质性给诊断和治疗带来挑战，深度学习分割技术有望提供解决方案。
- Method: 使用174例儿童患者的MRI数据（T1、T1-C、T2、FLAIR序列），训练3D nnU-Net模型，评估其在四种肿瘤子区域的分割性能。
- Result: 模型在整体肿瘤和T2高信号区域表现优异（DSC 0.85），接近人工标注水平（DSC 0.86），但在增强肿瘤和囊性成分上效果较差。
- Conclusion: 深度学习在儿童脑肿瘤分割中具有潜力，但需进一步优化，尤其是对增强肿瘤和囊性成分的分割。


### [6] [Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception](https://arxiv.org/abs/2507.22194)
*Christian Ellis,Maggie Wigness,Craig Lennon,Lance Fiondella*

Main category: cs.CV

TL;DR: 论文提出了一种名为Frontier-Seg的方法，用于在移动机器人视频流中实现时间一致的无人监督地形分割，解决了现有方法在无标签数据环境中的局限性。

- Motivation: 当前基于监督语义分割的地形感知导航方法依赖昂贵的数据标注，且在无标签或语义模糊的环境中表现不佳。无人监督的分割方法缺乏时间一致性，影响鲁棒性。
- Method: Frontier-Seg通过聚类从DINOv2基础模型中提取的超像素级特征，并强制帧间时间一致性，以无监督方式识别地形边界。
- Result: 在RUGD和RELLIS-3D等多样化数据集上的实验表明，Frontier-Seg能在非结构化越野环境中有效进行无人监督分割。
- Conclusion: Frontier-Seg为无标签环境下的地形分割提供了一种时间一致的解决方案，具有实际应用潜力。


### [7] [SmartCLIP: Modular Vision-language Alignment with Identification Guarantees](https://arxiv.org/abs/2507.22264)
*Shaoan Xie,Lingjing Kong,Yujia Zheng,Yu Yao,Zeyu Tang,Eric P. Xing,Guangyi Chen,Kun Zhang*

Main category: cs.CV

TL;DR: CLIP模型在多模态学习中表现优异，但存在信息错位和表征纠缠问题。本文提出一种新方法，通过灵活对齐文本和视觉表征，解决这些问题。

- Motivation: CLIP模型在处理图像-文本数据时存在信息错位和表征纠缠问题，限制了其在下游任务中的泛化能力。
- Method: 提出理论条件，确保模型能保留跨模态语义信息并解耦视觉表征，引入模块化对齐方法。
- Result: 新方法在多项任务中表现优异，验证了其解决信息错位问题的能力。
- Conclusion: 本文方法有效解决了CLIP模型的信息错位和表征纠缠问题，提升了其泛化能力。


### [8] [HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional Neural Networks for Retinal Image Classification](https://arxiv.org/abs/2507.22274)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 提出了一种基于HOG-CNN的自动化视网膜疾病诊断框架，结合手工特征和深度学习特征，在多个数据集上表现优异。

- Motivation: 传统视网膜疾病诊断依赖人工，耗时耗力，需自动化解决方案。
- Method: 融合HOG手工特征与CNN深度学习特征，构建HOG-CNN模型。
- Result: 在APTOS、ORIGA和IC-AMD数据集上表现优异，如98.5%准确率（DR分类）、92.8%准确率（AMD诊断）。
- Conclusion: HOG-CNN是轻量、可解释的自动化筛查工具，适用于资源有限环境。


### [9] [AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data](https://arxiv.org/abs/2507.22291)
*Christopher F. Brown,Michal R. Kazmierski,Valerie J. Pasquarella,William J. Rucklidge,Masha Samsikova,Chenhui Zhang,Evan Shelhamer,Estefania Lahera,Olivia Wiles,Simon Ilyushchenko,Noel Gorelick,Lihui Lydia Zhang,Sophia Alj,Emily Schechter,Sean Askay,Oliver Guinan,Rebecca Moore,Alexis Boukouvalas,Pushmeet Kohli*

Main category: cs.CV

TL;DR: AlphaEarth Foundations提出了一种通用的地理空间表示模型，通过整合多源数据生成高质量地图，无需重新训练即可超越现有方法。

- Motivation: 地球观测数据丰富但高质量标签稀缺，需要高效模型将稀疏标签转化为地图。
- Method: 采用嵌入场模型，整合空间、时间和测量上下文的多源数据。
- Result: 生成的嵌入在多样化评估中一致优于现有方法，无需重新训练。
- Conclusion: AlphaEarth Foundations为全球地图和监测系统提供了高效、准确的解决方案。


### [10] [LAMA-Net: A Convergent Network Architecture for Dual-Domain Reconstruction](https://arxiv.org/abs/2507.22316)
*Chi Ding,Qingchao Zhang,Ge Wang,Xiaojing Ye,Yunmei Chen*

Main category: cs.CV

TL;DR: 提出了一种可学习的变分模型LAMA，结合图像和测量域信息进行图像重建，并证明了其收敛性。进一步提出LAMA-Net和iLAMA-Net，在稀疏视图CT重建中表现出色。

- Motivation: 解决图像重建中非凸非光滑优化问题，结合互补信息提升性能。
- Method: 提出LAMA算法，结合残差学习和近端交替框架，证明其收敛性，并扩展为LAMA-Net和iLAMA-Net。
- Result: LAMA-Net表现出优异的稳定性和鲁棒性，iLAMA-Net进一步提升了性能。
- Conclusion: LAMA及其衍生网络在稀疏视图CT重建中优于现有方法，具有理论和实际优势。


### [11] [Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment](https://arxiv.org/abs/2507.22321)
*Yuzhen Gao,Qianqian Wang,Yongheng Sun,Cui Wang,Yongquan Liang,Mingxia Liu*

Main category: cs.CV

TL;DR: 提出了一种协作域适应（CDA）框架，结合Vision Transformer和CNN，用于基于T1加权MRI的晚年抑郁症（LLD）检测，显著提升了跨域泛化能力。

- Motivation: 解决晚年抑郁症（LLD）检测中样本量小和跨域异质性导致的模型泛化问题。
- Method: 结合ViT和CNN，分三阶段训练：源数据监督训练、目标特征自适应和协作训练。
- Result: 在多站点T1加权MRI数据上，CDA优于现有无监督域适应方法。
- Conclusion: CDA框架有效提升了LLD检测的跨域泛化能力，为疾病监测提供了可靠工具。


### [12] [UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views](https://arxiv.org/abs/2507.22342)
*Yuki Fujimura,Takahiro Kushida,Kazuya Kitano,Takuya Funatomi,Yasuhiro Mukaigawa*

Main category: cs.CV

TL;DR: 提出一种无需姿态信息的3D高斯溅射框架，用于处理不利输入视角，通过低秩适应层和几何一致性增强模块提升模型性能。

- Motivation: 现有方法依赖有利视角训练，限制了在未知相机姿态场景中的应用，需解决这一问题。
- Method: 结合低秩适应层（LoRA）和高斯适配模块，利用有利图像数据集训练，增强几何一致性。
- Result: 在合成和真实数据集上验证了方法对不利视角的有效处理。
- Conclusion: 提出的框架显著提升了模型在未知相机姿态下的表现，扩展了应用场景。


### [13] [DeltaVLM: Interactive Remote Sensing Image Change Analysis via Instruction-guided Difference Perception](https://arxiv.org/abs/2507.22346)
*Pei Deng,Wenqian Zhou,Hanlin Wu*

Main category: cs.CV

TL;DR: 论文提出了一种结合变化检测和视觉问答的新范式RSICA，并构建了大规模数据集ChangeChat-105k。提出的DeltaVLM模型在交互式遥感图像变化分析中表现优异。

- Motivation: 现有方法仅提供静态变化掩码或描述，无法支持交互式查询驱动的分析。
- Method: 提出DeltaVLM模型，包含双时相视觉编码器、视觉差异感知模块和指令引导的Q-former。
- Result: DeltaVLM在单轮描述和多轮交互分析中均达到最优性能。
- Conclusion: RSICA和DeltaVLM为遥感图像变化分析提供了高效、交互式的新方法。


### [14] [FaceGCD: Generalized Face Discovery via Dynamic Prefix Generation](https://arxiv.org/abs/2507.22353)
*Yunseok Oh,Dong-Wan Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为FaceGCD的新方法，用于解决广义人脸发现（GFD）任务，该任务结合了传统人脸识别和广义类别发现（GCD）。FaceGCD通过动态生成实例特定的特征提取器，显著优于现有方法。

- Motivation: 人脸识别系统需要区分熟悉和陌生的面孔，这是实现人工通用智能（AGI）的关键能力。GFD任务旨在统一传统人脸识别和GCD，但面临高基数和细粒度ID的挑战。
- Method: FaceGCD利用轻量级、分层前缀动态构建特征提取器，通过HyperNetwork为每个输入图像生成前缀生成器，从而捕捉细微的身份特征。
- Result: 实验表明，FaceGCD在GFD任务上显著优于现有GCD方法和ArcFace基线，达到了最先进的性能。
- Conclusion: FaceGCD为开放世界人脸识别提供了有效解决方案，推动了该领域的发展。


### [15] [GVD: Guiding Video Diffusion Model for Scalable Video Distillation](https://arxiv.org/abs/2507.22360)
*Kunyang Li,Jeffrey A Chan Santiago,Sarinda Dhanesh Samarasinghe,Gaowen Liu,Mubarak Shah*

Main category: cs.CV

TL;DR: GVD是一种基于扩散的视频数据集蒸馏方法，首次将空间和时间特征联合蒸馏，显著提升了性能。

- Motivation: 解决大视频数据集的计算和存储需求问题，通过蒸馏技术减少数据量同时保持性能。
- Method: 提出GVD（Guiding Video Diffusion），联合蒸馏空间和时间特征，确保高保真视频生成。
- Result: 在MiniUCF和HMDB51数据集上表现优异，仅用少量帧即可达到接近原始数据集的性能。
- Conclusion: GVD不仅性能领先，还能生成更高分辨率的视频，且计算成本增加不明显。


### [16] [Object Recognition Datasets and Challenges: A Review](https://arxiv.org/abs/2507.22361)
*Aria Salari,Abtin Djavadifar,Xiangrui Liu,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 该论文调查了160多个目标识别数据集，分析了其统计数据和描述，并概述了相关基准和竞赛。

- Motivation: 目标识别是计算机视觉的基础任务，数据集的质量和规模对深度学习技术至关重要，同时为研究提供了量化基准。
- Method: 通过统计和描述详细分析了160多个数据集，并总结了目标识别领域的基准和竞赛。
- Result: 提供了对常用数据集的深入分析，并介绍了评估指标和在线资源。
- Conclusion: 数据集对目标识别研究至关重要，本文的分析为数据驱动和机器学习研究者提供了重要参考。


### [17] [Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring](https://arxiv.org/abs/2507.22369)
*Sinh Trong Vu,Hieu Trung Pham,Dung Manh Nguyen,Hieu Minh Hoang,Nhu Hoang Le,Thu Ha Pham,Tai Tan Mai*

Main category: cs.CV

TL;DR: 论文探讨了视觉问答（VQA）模型在课堂行为分析中的应用，评估了LLaMA2、LLaMA3、QWEN3和NVILA等开源模型在BAV-Classroom-VQA数据集上的表现，结果显示这些模型在行为相关视觉问题回答中表现良好。

- Motivation: 课堂行为监控对教育研究和学生学习成果至关重要，而VQA模型的进步为自动分析复杂课堂互动提供了新工具。
- Method: 研究采用LLaMA2、LLaMA3、QWEN3和NVILA等VQA模型，基于BAV-Classroom-VQA数据集进行性能评估，数据集来自越南银行学院的真实课堂视频。
- Result: 实验表明，四种模型在行为相关视觉问题回答中均表现出色，展现了未来课堂分析和干预系统的潜力。
- Conclusion: VQA模型在课堂行为分析中具有实际应用价值，未来可进一步优化和扩展。


### [18] [Gems: Group Emotion Profiling Through Multimodal Situational Understanding](https://arxiv.org/abs/2507.22393)
*Anubhav Kataria,Surbhi Madan,Shreya Ghosh,Tom Gedeon,Abhinav Dhall*

Main category: cs.CV

TL;DR: GEMS框架通过多模态Swin-Transformer和S3Attention架构，预测细粒度个体情绪到粗粒度群体和事件情绪，并在VGAF-GEMS基准测试中表现出色。

- Motivation: 理解多人在社交场景中的情绪需要结合个体、群体和事件层面的情感分析，现有研究主要关注原子交互或群体层面，缺乏细粒度分析。
- Method: 采用多模态Swin-Transformer和S3Attention架构，结合场景、群体成员和上下文信息，预测离散和连续情绪（包括效价和唤醒度）。
- Result: 在VGAF-GEMS基准测试中，GEMS框架在定量和定性比较中优于现有模型。
- Conclusion: GEMS为多人在社交场景中的情绪分析提供了更细粒度和全面的方法，推动了相关研究的进一步发展。


### [19] [On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations](https://arxiv.org/abs/2507.22398)
*Jordan Vice,Naveed Akhtar,Yansong Gao,Richard Hartley,Ajmal Mian*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型（VLMs）在频率域受到细微扰动时，其真实性检测和图像字幕生成任务表现脆弱，揭示了其可靠性问题。

- Motivation: 探索VLMs在频率域扰动下的脆弱性，以评估其在真实应用中的可靠性。
- Method: 设计频率域的目标图像变换，测试五种先进VLMs在十种数据集上的表现。
- Result: VLMs对频率扰动敏感，其输出与语义内容不完全一致，且扰动在视觉上不可察觉。
- Conclusion: VLMs在频率扰动下表现脆弱，需开发更鲁棒的多模态感知系统。


### [20] [MINR: Implicit Neural Representations with Masked Image Modelling](https://arxiv.org/abs/2507.22404)
*Sua Lee,Joonhun Lee,Myungjoo Kang*

Main category: cs.CV

TL;DR: MINR框架结合隐式神经表示与掩码图像建模，提升了自监督学习的鲁棒性和泛化能力。

- Motivation: 解决MAE在掩码策略依赖性和分布外数据性能下降的问题。
- Method: 引入掩码隐式神经表示（MINR）框架，学习图像的连续函数表示。
- Result: MINR在域内和域外场景均优于MAE，同时降低模型复杂度。
- Conclusion: MINR是一种鲁棒且高效的自监督学习替代方案。


### [21] [Moiré Zero: An Efficient and High-Performance Neural Architecture for Moiré Removal](https://arxiv.org/abs/2507.22407)
*Seungryong Lee,Woojeong Baek,Younghyun Kim,Eunwoo Kim,Haru Moon,Donggon Yoo,Eunbyung Park*

Main category: cs.CV

TL;DR: MZNet是一种U形网络，通过多尺度双注意力块、多形状大核卷积块和特征融合跳跃连接，有效去除图像中的摩尔纹，达到摩尔纹零状态。

- Motivation: 摩尔纹在摄影和工业检测中是一个常见问题，现有CNN方法因感受野受限难以有效去除。
- Method: 提出MZNet，包含多尺度双注意力块（MSDAB）、多形状大核卷积块（MSLKB）和特征融合跳跃连接。
- Result: 在高分辨率数据集上表现最优，低分辨率数据集上也有竞争力，且计算成本低。
- Conclusion: MZNet是一种高效实用的摩尔纹去除解决方案。


### [22] [UAVScenes: A Multi-Modal Dataset for UAVs](https://arxiv.org/abs/2507.22412)
*Sijie Wang,Siqi Li,Yawei Zhang,Shangshu Yu,Shenghai Yuan,Rui She,Quanjiang Guo,JinXuan Zheng,Ong Kang Howe,Leonrich Chandra,Shrivarshann Srijeyan,Aditya Sivadas,Toshan Aggarwal,Heyuan Liu,Hongming Zhang,Chujie Chen,Junyu Jiang,Lihua Xie,Wee Peng Tay*

Main category: cs.CV

TL;DR: UAVScenes是一个新的大规模多模态无人机数据集，旨在填补现有数据集在高层次场景理解任务中的不足，提供帧级标注和6-DoF位姿。

- Motivation: 现有无人机数据集多偏重于定位和3D重建任务，缺乏帧级标注，限制了高层次场景理解任务的应用。
- Method: 基于MARS-LVIG数据集，手动标注了图像和LiDAR点云的语义信息，并添加了6-DoF位姿。
- Result: UAVScenes支持多种任务，包括分割、深度估计、定位、地点识别和新视角合成。
- Conclusion: UAVScenes填补了多模态无人机数据集的空白，推动了无人机感知技术的发展。


### [23] [Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching](https://arxiv.org/abs/2507.22418)
*Phi Van Nguyen,Ngoc Huynh Trinh,Duy Minh Lam Nguyen,Phu Loc Nguyen,Quoc Long Tran*

Main category: cs.CV

TL;DR: 提出了一种基于条件流匹配的方法，用于医学图像分割中的不确定性量化，优于现有生成模型。

- Motivation: 医学图像分割中，专家标注的自然变异性（aleatoric uncertainty）需要准确量化，但现有生成模型表达能力有限。
- Method: 采用条件流匹配（simulation-free flow-based generative model）学习精确密度，通过输入图像引导生成多个分割样本。
- Result: 实验表明，该方法不仅分割精度高，还能生成反映数据分布的不确定性图，捕捉边界模糊区域的不确定性。
- Conclusion: 提出的方法能有效量化不确定性，提供更可靠的分割结果，代码已开源。


### [24] [Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking](https://arxiv.org/abs/2507.22421)
*Shahla John*

Main category: cs.CV

TL;DR: 提出了一种统一的实时视频分析框架，结合空间-时间建模技术和分层注意力机制，在动作识别和对象跟踪任务中实现了更高的准确性和速度。

- Motivation: 解决现有方法在资源受限环境中难以平衡准确性和速度的问题。
- Method: 利用并行序列建模技术和新型分层注意力机制，自适应关注时空序列中的相关区域。
- Result: 在UCF-101、HMDB-51和MOT17数据集上，动作识别准确率提升3.2%，跟踪精度提升2.8%，推理时间加快40%。
- Conclusion: 该方法在保持实时性的同时，实现了最先进的性能表现。


### [25] [HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models](https://arxiv.org/abs/2507.22431)
*Zhixiang Wei,Guangting Wang,Xiaoxiao Ma,Ke Mei,Huaian Chen,Yi Jin,Fengyun Rao*

Main category: cs.CV

TL;DR: 提出了一种利用大视觉语言模型（LVLM）优化图像-文本对数据的方法，生成多粒度标注的VLM-150M数据集，并基于此训练出性能更强的HQ-CLIP模型。

- Motivation: 探索是否可以通过LVLM提升图像-文本对数据的质量，形成自我增强的循环。
- Method: 利用LVLM处理图像和原始文本，生成四种互补的文本标注（正/负长描述和正/负短标签），并基于这些标注扩展对比学习范式。
- Result: HQ-CLIP在零样本分类、跨模态检索和细粒度视觉理解任务中表现优异，甚至超越使用10倍数据训练的CLIP模型。
- Conclusion: 通过LVLM驱动的数据优化和扩展的训练范式，显著提升了模型性能，为数据与模型的协同优化提供了新思路。


### [26] [From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras](https://arxiv.org/abs/2507.22438)
*Youngho Kim,Hoonhee Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机的域适应方法，用于解决运动模糊下的人体姿态估计问题，通过事件增强和师生框架提升性能。

- Motivation: 运动模糊和低光条件下的人体姿态估计性能下降，现有数据集假设条件稳定，导致模型在模糊环境中表现不佳。
- Method: 利用事件相机的高时间分辨率数据生成运动感知模糊图像，结合师生框架和不确定性掩码优化伪标签。
- Result: 方法在运动模糊环境下优于传统域适应方法，无需目标域标注即可实现鲁棒姿态估计。
- Conclusion: 事件相机为现实运动模糊环境中的域适应提供了可扩展且有效的解决方案。


### [27] [TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation](https://arxiv.org/abs/2507.22454)
*Jiuming Liu,Zheng Huang,Mengmeng Liu,Tianchen Deng,Francesco Nex,Hao Cheng,Hesheng Wang*

Main category: cs.CV

TL;DR: 论文提出TopoLiDM框架，结合图神经网络与扩散模型，通过拓扑正则化提升LiDAR场景生成的几何真实性和全局拓扑一致性。

- Motivation: 现有LiDAR生成方法难以捕捉几何真实性和全局拓扑一致性，限制了其在自动驾驶感知任务中的应用。
- Method: TopoLiDM结合拓扑保持VAE和潜在扩散模型，引入0维持续同调约束，生成高保真LiDAR场景。
- Result: 在KITTI-360数据集上，TopoLiDM显著优于现有方法，FRID降低22.6%，MMD降低9.2%，且生成速度快（1.68样本/秒）。
- Conclusion: TopoLiDM通过拓扑正则化显著提升LiDAR生成质量，适用于实际应用。


### [28] [Exploiting Diffusion Prior for Task-driven Image Restoration](https://arxiv.org/abs/2507.22459)
*Jaeha Kim,Junghun Oh,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 论文提出EDTR方法，利用扩散先验恢复任务相关细节，提升多复杂退化场景下的任务性能和视觉质量。

- Motivation: 解决现有任务驱动图像恢复方法在多复杂退化场景下难以恢复任务相关细节的问题。
- Method: 提出EDTR方法，通过从像素误差预恢复的低质量图像生成，并添加轻度噪声，结合少量去噪步骤，避免冗余细节。
- Result: EDTR显著提升了多复杂退化任务中的性能和视觉质量。
- Conclusion: EDTR有效利用扩散先验，为任务驱动图像恢复提供了一种高效解决方案。


### [29] [Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2507.22465)
*Zheng Xiangyu,He Songcheng,Li Wanyun,Li Xiaoqiang,Zhang Wei*

Main category: cs.CV

TL;DR: 论文提出了一种新的分层记忆架构（HMHI-Net），通过结合浅层和高层特征解决现有无监督视频对象分割方法中过度依赖高层语义特征的问题，显著提升了性能。

- Motivation: 现有无监督视频对象分割方法因过度依赖高层语义特征而缺乏细粒度信息，导致性能提升有限。
- Method: 提出分层记忆架构，结合浅层和高层特征，并通过异构交互机制（PLAM和SGIM模块）实现像素与语义信息的互补利用。
- Result: HMHI-Net在所有无监督视频对象分割和视频显著性检测基准测试中均达到最先进性能，且在不同骨干网络上表现稳健。
- Conclusion: HMHI-Net通过分层记忆和异构交互机制有效解决了现有方法的不足，显著提升了无监督视频对象分割的性能和鲁棒性。


### [30] [Visual Language Models as Zero-Shot Deepfake Detectors](https://arxiv.org/abs/2507.22469)
*Viacheslav Pirogov*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型（VLM）的零样本深度伪造检测方法，优于现有传统分类器。

- Motivation: 深度伪造技术对数字媒体和身份验证构成威胁，现有检测方法仅依赖图像域分类器，缺乏辅助任务增强鲁棒性。
- Method: 利用VLM的零样本能力进行图像分类，并在高质量深度伪造数据集上评估性能。
- Result: 在60,000张图像的深度伪造数据集上，零样本模型表现优于现有方法；InstructBLIP在DFDC-P数据集上优于传统方法。
- Conclusion: VLM在深度伪造检测中表现优越，尤其在零样本和微调场景下。


### [31] [LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks](https://arxiv.org/abs/2507.22477)
*Hui Liu,Chen Jia,Fan Shi,Xu Cheng,Mengfei Shi,Xia Xie,Shengyong Chen*

Main category: cs.CV

TL;DR: 提出了一种轻量级自适应线索感知视觉Mamba网络（LIDAR），用于高效感知和融合多模态数据中的裂纹特征，实现低计算成本的像素级分割。

- Motivation: 解决现有方法在多模态裂纹分割任务中缺乏自适应感知和高效交互融合能力的问题。
- Method: 结合轻量级自适应线索感知视觉状态空间模块（LacaVSS）和轻量级双域动态协作融合模块（LD3CF），利用动态引导扫描策略和自适应频率域感知器实现多模态特征融合。
- Result: 在三个数据集上表现优于现有方法，其中在光场深度数据集上F1分数为0.8204，mIoU为0.8465，参数仅5.35M。
- Conclusion: LIDAR网络在多模态裂纹分割任务中实现了高效且低计算成本的性能提升。


### [32] [Estimating 2D Camera Motion with Hybrid Motion Basis](https://arxiv.org/abs/2507.22480)
*Haipeng Li,Tianhao Zhou,Zhanglei Yang,Yi Wu,Yan Chen,Zijing Mao,Shen Cheng,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: CamFlow提出了一种结合物理和随机基的混合运动基框架，用于估计2D相机运动，并通过新的损失函数和基准测试验证了其优越性。

- Motivation: 现有方法（如基于单应性或网格流的技术）在复杂非线性变换或非平面场景中表现不佳，需要一种更通用的解决方案。
- Method: CamFlow结合物理基（相机几何）和随机基（复杂场景），采用基于拉普拉斯分布的混合概率损失函数进行训练。
- Result: 实验表明，CamFlow在多种场景中优于现有方法，尤其在零样本设置下表现出更强的鲁棒性和泛化能力。
- Conclusion: CamFlow通过混合运动基和新的损失函数，成功解决了复杂相机运动估计问题，并提供了公开的代码和数据集。


### [33] [Robust Adverse Weather Removal via Spectral-based Spatial Grouping](https://arxiv.org/abs/2507.22498)
*Yuhwan Jeong,Yunseo Yang,Youngjo Yoon,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出了一种基于频谱分解和分组注意力的多天气图像修复方法SSGformer，通过分解高低频特征并利用分组注意力机制，有效处理复杂天气退化。

- Motivation: 现有All-in-One模型难以捕捉多样化和局部化的天气退化模式，需改进以应对复杂天气条件。
- Method: SSGformer通过频谱分解（边缘检测和SVD）提取高低频特征，利用分组注意力和空间分组Transformer块平衡特征和空间关系。
- Result: 实验证明SSGformer在多样复杂天气退化条件下表现优越。
- Conclusion: SSGformer通过分组注意力和频谱分解，显著提升了多天气图像修复的性能。


### [34] [DACA-Net: A Degradation-Aware Conditional Diffusion Network for Underwater Image Enhancement](https://arxiv.org/abs/2507.22501)
*Chang Huang,Jiahang Cao,Jun Ma,Kieren Yu,Cong Li,Huayong Yang,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的自适应水下图像增强方法，通过预测退化分数并利用物理先验，显著提升了图像质量。

- Motivation: 水下图像因散射和吸收等光学效应导致色彩失真、低可见性和结构模糊，现有方法难以自适应处理多样化退化条件。
- Method: 使用轻量级双流卷积网络预测退化分数，结合Swin UNet的条件扩散网络进行自适应噪声调度和特征细化，引入退化引导的特征融合模块和混合损失函数。
- Result: 在基准数据集上验证，方法在色彩保真度、感知质量和结构细节方面表现优异，显著优于现有技术。
- Conclusion: 提出的方法能有效恢复水下图像，为下游视觉任务提供高质量输入。


### [35] [AlphaDent: A dataset for automated tooth pathology detection](https://arxiv.org/abs/2507.22512)
*Evgeniy I. Sosnin,Yuriy L. Vasilev,Roman A. Solovyev,Aleksandr L. Stempkovskiy,Dmitry V. Telpukhov,Artem A. Vasilev,Aleksandr A. Amerikanov,Aleksandr Y. Romanov*

Main category: cs.CV

TL;DR: AlphaDent数据集是一个用于牙齿研究的独特数据集，包含295名患者的1200多张牙齿照片，标注为实例分割问题，分为9类。实验结果显示预测质量高，数据集和代码均开源。

- Motivation: 为牙齿研究提供一个新的高质量数据集，解决实例分割问题。
- Method: 使用DSLR相机拍摄牙齿照片，标注为9类实例分割问题，并训练神经网络进行实验。
- Result: 实验结果显示预测质量高。
- Conclusion: AlphaDent数据集及其相关资源开源，为牙齿研究提供了有力工具。


### [36] [Recognizing Actions from Robotic View for Natural Human-Robot Interaction](https://arxiv.org/abs/2507.22522)
*Ziyi Wang,Peiming Li,Hong Liu,Zhichao Deng,Can Wang,Jun Liu,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 论文提出了ACTIVE数据集和ACTIVE-PC方法，专注于机器人视角下的人类动作识别，解决了传统基准的局限性。

- Motivation: 自然人机交互（N-HRI）需要机器人识别不同距离和状态的人类动作，但现有基准无法满足其复杂性。
- Method: 提出ACTIVE数据集和ACTIVE-PC方法，后者采用多级邻域采样、分层识别器等技术。
- Result: 实验证明ACTIVE-PC在长距离动作识别中有效。
- Conclusion: ACTIVE数据集和方法为N-HRI的动作识别研究提供了新基准和解决方案。


### [37] [HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors](https://arxiv.org/abs/2507.22530)
*Xincheng Yao,Yijun Yang,Kangwei Guo,Ruiqiang Xiao,Haipeng Zhou,Haisu Tao,Jian Yang,Lei Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种高分辨率视频血管分割网络（HRVVS），用于肝切除术视频中的肝血管分割，并引入了一个高质量的数据集。

- Motivation: 肝血管分割在肝切除术中具有重要临床意义，但缺乏合适的数据集和任务复杂性限制了研究进展。
- Method: 提出HRVVS网络，嵌入预训练的VAR模型以减少信息退化，并设计动态记忆解码器以减少冗余信息。
- Result: 实验表明HRVVS显著优于现有方法。
- Conclusion: HRVVS在肝血管分割任务中表现出色，数据集和代码将公开。


### [38] [RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning](https://arxiv.org/abs/2507.22553)
*Kiseong Hong,Gyeong-hyeon Kim,Eunwoo Kim*

Main category: cs.CV

TL;DR: 提出了一种基于提示演化的持续学习方法，通过自适应聚合任务特定提示来提升多样性，并在图像分类和视频动作识别任务中显著优于现有方法。

- Motivation: 现有方法依赖固定提示或任务共享空间生成的提示，限制了提示的多样性，难以满足复杂任务需求。
- Method: 提出提示演化机制，自适应聚合任务特定提示，并引入可学习的概率门决定激活哪些层。
- Result: 在图像分类和视频动作识别任务中，平均性能提升9.07%和7.40%。
- Conclusion: 提示演化机制有效提升了持续学习中提示的多样性和任务适应性。


### [39] [Subtyping Breast Lesions via Generative Augmentation based Long-tailed Recognition in Ultrasound](https://arxiv.org/abs/2507.22568)
*Shijing Chen,Xinrui Zhou,Yuhao Wang,Yuhao Huang,Ao Chang,Dong Ni,Ruobing Huang*

Main category: cs.CV

TL;DR: 提出了一种双阶段框架，通过高保真数据合成解决乳腺超声图像长尾分布问题，结合强化学习自适应采样器和类可控合成网络，提升分类性能。

- Motivation: 乳腺病变亚型的准确识别对个性化治疗至关重要，但超声图像的长尾分布给自动识别带来挑战。
- Method: 采用双阶段框架，结合强化学习自适应采样器和类可控合成网络，动态调整合成与真实数据比例，保持判别能力。
- Result: 在内部长尾和公共不平衡乳腺超声数据集上表现出色，优于现有方法。
- Conclusion: 该方法有效缓解了数据分布偏差，提升了长尾分类性能，具有临床应用潜力。


### [40] [COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP](https://arxiv.org/abs/2507.22576)
*Galadrielle Humblot-Renaux,Gianni Franchi,Sergio Escalera,Thomas B. Moeslund*

Main category: cs.CV

TL;DR: COOkeD是一种异构集成方法，结合了封闭世界分类器、零样本CLIP分类器和线性探针分类器，显著提升了OOD检测性能。

- Motivation: 解决OOD检测性能受限于单一分类器能力的问题，通过集成不同方法的优势。
- Method: 创建异构集成（COOkeD），结合封闭世界分类器、零样本CLIP分类器和线性探针分类器。
- Result: 在CIFAR100和ImageNet等基准测试中，COOkeD实现了最先进的性能和更强的鲁棒性。
- Conclusion: COOkeD通过简单而模块化的方法显著提升了OOD检测性能，适用于多种挑战性场景。


### [41] [Robust Deepfake Detection for Electronic Know Your Customer Systems Using Registered Images](https://arxiv.org/abs/2507.22601)
*Takuma Amada,Kazuya Kakizaki,Taiki Miyagawa,Akinori F. Ebihara,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 提出了一种专为电子KYC系统设计的深度伪造检测算法，通过检测时间不一致性和身份差异，提高检测准确性和鲁棒性。

- Motivation: 确保电子KYC系统在面对深度伪造攻击时的可靠性，需要开发一种能够识别面部替换和面部重演的鲁棒检测器。
- Method: 通过检测面部识别模型提取的身份向量的时间不一致性，结合注册图像计算身份差异，并使用更大数据集训练的特征提取器。
- Result: 实验结果表明，该方法能全面检测面部替换和面部重演，并对各种未见过的图像退化具有鲁棒性。
- Conclusion: 提出的算法在电子KYC系统中表现出色，代码已开源。


### [42] [ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning](https://arxiv.org/abs/2507.22604)
*Xiefan Guo,Miaomiao Cui,Liefeng Bo,Di Huang*

Main category: cs.CV

TL;DR: 提出了一种基于短链去噪的高效微调方法ShortFT，解决了传统反向传播方法在长链去噪中的计算成本和梯度爆炸问题。

- Motivation: 传统基于反向传播的方法在长链去噪中面临计算成本高和梯度爆炸的风险，导致结果不理想。
- Method: 利用轨迹保留的少步扩散模型构建短链去噪路径，显著提升微调效率和效果。
- Result: 实验证明该方法适用于多种奖励函数，显著提升对齐性能，优于现有方法。
- Conclusion: ShortFT是一种高效且有效的微调策略，解决了长链去噪的局限性。


### [43] [VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning](https://arxiv.org/abs/2507.22607)
*Ruifeng Yuan,Chenghao Xiao,Sicong Leng,Jianyu Wang,Long Li,Weiwen Xu,Hou Pong Chan,Deli Zhao,Tingyang Xu,Zhongyu Wei,Hao Zhang,Yu Rong*

Main category: cs.CV

TL;DR: VL-Cogto是一个基于渐进式课程强化学习（PCuRL）的多模态推理模型，通过动态调整训练难度和推理路径长度，显著提升了多模态任务的推理能力。

- Motivation: 多模态任务的复杂性和多样性导致现有模型在不同领域和难度下表现不稳定，需要一种更有效的训练方法。
- Method: 采用PCuRL框架，包括在线难度软加权机制和动态长度奖励机制，逐步提升模型推理能力。
- Result: VL-Cogto在数学、科学、逻辑和通用理解等多个主流多模态基准测试中表现优异，优于现有模型。
- Conclusion: PCuRL框架有效提升了多模态推理模型的性能和稳定性，为复杂任务提供了新的解决方案。


### [44] [Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model](https://arxiv.org/abs/2507.22615)
*Daehee Park,Monu Surana,Pranav Desai,Ashish Mehta,Reuben MV John,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: GALTraj通过生成式主动学习改进轨迹预测，专注于长尾场景，无需改变模型结构。

- Motivation: 解决数据驱动轨迹预测在罕见长尾场景中的性能不足问题。
- Method: 提出GALTraj方法，利用可控扩散模型主动识别并增强训练中的罕见样本。
- Result: 在多个数据集和模型上显著提升长尾样本性能，同时提高整体准确性。
- Conclusion: GALTraj首次成功将生成式主动学习应用于轨迹预测，为长尾学习提供了新思路。


### [45] [Bridging the Gap in Missing Modalities: Leveraging Knowledge Distillation and Style Matching for Brain Tumor Segmentation](https://arxiv.org/abs/2507.22626)
*Shenghao Zhu,Yifei Chen,Weihong Chen,Yuanhan Wang,Chang Liu,Shuo Jiang,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: MST-KDNet提出了一种多尺度Transformer知识蒸馏方法，用于解决脑肿瘤分割中模态缺失和边界不敏感的问题，并在实验中表现优于现有方法。

- Motivation: 解决脑肿瘤分割中模态缺失和边界不敏感的问题，提升分割的准确性和鲁棒性。
- Method: 采用多尺度Transformer知识蒸馏、双模式Logit蒸馏和全局风格匹配模块，结合对抗学习。
- Result: 在BraTS和FeTS 2024数据集上，Dice和HD95分数优于现有方法，尤其在模态缺失情况下表现突出。
- Conclusion: MST-KDNet具有优异的鲁棒性和泛化能力，适用于实际临床应用。


### [46] [LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing](https://arxiv.org/abs/2507.22627)
*Federico Girella,Davide Talon,Ziyue Liu,Zanxi Ruan,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS提出了一种基于局部草图与文本的时尚图像生成方法，通过扩散模型实现全局与局部条件的融合，并在新数据集Sketchy上验证了其优越性能。

- Motivation: 时尚设计需要结合视觉与文本表达，现有方法难以同时处理全局描述与局部细节。LOTS旨在解决这一问题。
- Method: 采用模块化对偶表示将草图与文本编码到共享潜在空间，并通过扩散对偶引导在去噪过程中整合全局与局部条件。
- Result: LOTS在全局与局部指标上均达到最优性能，定性分析与人类评估显示其设计定制化能力突出。
- Conclusion: LOTS为时尚图像生成提供了高效且灵活的解决方案，支持高度定制化设计。


### [47] [SpectraSentinel: LightWeight Dual-Stream Real-Time Drone Detection, Tracking and Payload Identification](https://arxiv.org/abs/2507.22650)
*Shahriar Kabir,Istiak Ahmmed Rifti,H. M. Shadman Tabib,Mushfiqur Rahman,Sadatul Islam Sadi,Hasnaen Adil,Ahmed Mahir Sultan Rumi,Ch Md Rakin Haider*

Main category: cs.CV

TL;DR: 提出了一种双流无人机监控框架，使用独立的YOLOv11n模型处理红外和可见光数据流，避免早期融合，优化检测性能。

- Motivation: 无人机在民用空域的普及引发安全担忧，需要实时监控系统应对2025 VIP Cup挑战任务（无人机检测、跟踪和载荷识别）。
- Method: 采用双流设计，分别优化红外和可见光数据流的YOLOv11n模型，定制数据预处理和增强策略，调整超参数以应对噪声、低光和运动模糊。
- Result: 轻量级YOLOv11n模型在区分无人机与鸟类及载荷分类上表现出高精度，同时保持实时性能。
- Conclusion: 双模态设计和专门优化的训练流程实现了高效准确的无人机监控。


### [48] [Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation](https://arxiv.org/abs/2507.22668)
*Hongbin Lin,Yifan Jiang,Juangui Xu,Jesse Jiaxi Xu,Yi Lu,Zhengyu Hu,Ying-Cong Chen,Hao Wang*

Main category: cs.CV

TL;DR: 提出了一种基于图引导的双约束数据增强框架，用于3D点云分割，通过全局结构依赖提升场景合成的真实性和分割性能。

- Motivation: 现有数据增强方法多关注局部变换或语义重组，缺乏对场景全局结构依赖的考虑。
- Method: 学习真实数据中的物体关系统计，构建引导图；局部约束确保几何合理性和语义一致性，全局约束保持场景拓扑结构。
- Result: 在室内外数据集上生成多样且高质量的增强场景，显著提升多种模型的分割性能。
- Conclusion: 通过全局结构依赖的双约束增强框架，有效提升了3D点云分割的准确性和鲁棒性。


### [49] [MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model](https://arxiv.org/abs/2507.22675)
*Meiqi Hu,Lingzhi Lu,Chengxi Han,Xiaoping Liu*

Main category: cs.CV

TL;DR: MergeSAM是一种基于Segment Anything Model (SAM)的无监督变化检测方法，通过MaskMatching和MaskSplitting策略处理复杂变化，提升高分辨率遥感影像的变化检测效果。

- Motivation: 利用大型基础模型的特征提取能力，加速无监督变化检测方法的发展，增强变化检测技术的实用性。
- Method: 提出MergeSAM方法，结合SAM的分割能力，设计MaskMatching和MaskSplitting策略，构建多时相掩码以捕捉复杂变化。
- Result: 方法能够有效处理对象分割、合并等复杂变化，并将土地覆盖的空间结构嵌入变化检测过程。
- Conclusion: MergeSAM展示了在无监督变化检测中的潜力，为高分辨率遥感影像的变化检测提供了创新解决方案。


### [50] [Hydra-Bench: A Benchmark for Multi-Modal Leaf Wetness Sensing](https://arxiv.org/abs/2507.22685)
*Yimeng Liu,Maolin Gan,Yidong Ren,Gen Li,Jingkai Lin,Younsuk Dong,Zhichao Cao*

Main category: cs.CV

TL;DR: 论文提出了一种多模态数据集，用于改进叶片湿度检测的机器学习算法，并提供了详细的基准测试。

- Motivation: 现有叶片湿度检测系统在动态真实环境中的鲁棒性、准确性和环境适应性不足，需要更可靠的数据集和方法。
- Method: 通过收集六个月内五种植物在控制和户外环境中的毫米波原始数据、SAR图像和RGB图像，构建多模态数据集，并使用Hydra模型进行基准测试。
- Result: 数据集支持多模态融合策略和不同扫描距离下的性能评估，为SAR成像算法优化提供了基准。
- Conclusion: 该数据集为叶片湿度检测和SAR算法优化提供了系统化的评估工具。


### [51] [Zero-Shot Image Anomaly Detection Using Generative Foundation Models](https://arxiv.org/abs/2507.22692)
*Lemar Abdi,Amaan Valiuddin,Francisco Caetano,Christiaan Viviers,Fons van der Sommen*

Main category: cs.CV

TL;DR: 利用扩散模型作为通用感知模板，通过分析去噪轨迹和Stein分数误差，提出一种无需重新训练即可检测异常样本的新方法。

- Motivation: 在开放世界中部署安全的视觉系统需要检测分布外（OOD）输入，扩散模型作为生成工具之外的新用途值得探索。
- Method: 利用Denoising Diffusion Models（DDMs）的去噪轨迹和Stein分数误差，结合SSIM指标，开发了一种无需重新训练的OOD检测方法。
- Result: 在CelebA数据集上训练的单一模型表现优异，甚至优于ImageNet，部分基准测试接近完美性能。
- Conclusion: 生成基础模型在异常检测中具有强大潜力，未来有望进一步提升性能。


### [52] [Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints](https://arxiv.org/abs/2507.22699)
*Thuy Tran,Ruochen Chen,Shaifali Parashar*

Main category: cs.CV

TL;DR: 本文提出了一种无监督的Shape-from-Template（SfT）方法，仅使用图像观测（颜色特征、梯度和轮廓）和网格不可延展性约束，以比现有最佳无监督SfT快400倍的速度重建3D形状，并在细节生成和严重遮挡情况下表现更优。

- Motivation: 传统SfT方法需要图像与3D模板的点对应关系，在严重遮挡时性能下降；现代SfT方法依赖大量监督数据。本文旨在开发一种无需对应关系且无需监督数据的高效方法。
- Method: 结合图像观测（颜色、梯度、轮廓）和网格不可延展性约束，采用无监督方法进行3D形状重建。
- Result: 重建速度比现有最佳无监督SfT快400倍，且在细节生成和遮挡处理上显著优于现有方法。
- Conclusion: 提出的无监督SfT方法高效且鲁棒，适用于复杂场景，代码已开源。


### [53] [A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks](https://arxiv.org/abs/2507.22733)
*Hang Su,Yunlong Feng,Daniel Gehrig,Panfeng Jiang,Ling Gao,Xavier Lagorce,Laurent Kneip*

Main category: cs.CV

TL;DR: 提出一种统一方法，从任意时间戳的2D点对应关系中估计结构和线性运动，适用于多种相机类型。

- Motivation: 传统算法（如5点或8点算法）仅适用于同步视图的点对应关系，无法处理滚动快门或事件相机等异步数据。
- Method: 利用一阶动力学和恒定速度运动模型，推导出线性点入射关系，高效恢复线性速度和3D点。
- Result: 在模拟和真实数据上验证了方法的有效性，相比现有方法有显著改进。
- Conclusion: 该方法为异步数据的高效结构和运动估计提供了新途径。


### [54] [Social-Pose: Enhancing Trajectory Prediction with Human Body Pose](https://arxiv.org/abs/2507.22742)
*Yang Gao,Saeed Saadatnejad,Alexandre Alahi*

Main category: cs.CV

TL;DR: 论文提出了一种基于人体姿态的注意力编码器`Social-pose`，用于提升人类轨迹预测的准确性，并在多种数据集上验证了其优于现有方法的性能。

- Motivation: 现有的人类轨迹预测模型未能充分利用人类在空间中导航时潜意识传达的视觉线索，如身体姿态。
- Method: 提出`Social-pose`，一种基于注意力的姿态编码器，能够捕捉场景中所有人的姿态及其社交关系，并可集成到多种轨迹预测架构中。
- Result: 在合成和真实数据集上的实验表明，该方法在LSTM、GAN、MLP和Transformer等模型上均优于现有方法。
- Conclusion: 使用人体姿态（尤其是3D姿态）能显著提升轨迹预测的准确性，且该方法在机器人导航场景中具有潜在应用价值。


### [55] [HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training](https://arxiv.org/abs/2507.22781)
*Xuecheng Wu,Danlei Huang,Heli Sun,Xinyi Yin,Yifan Wang,Hao Wang,Jia Zhang,Fei Wang,Peihao Guo,Suyu Xing,Junxiao Xue,Liang He*

Main category: cs.CV

TL;DR: HOLA是一种用于视频级深度伪造检测的两阶段框架，通过大规模自监督预训练和跨模态学习模块，显著提升了检测性能，并在2025年1M-Deepfakes检测挑战赛中排名第一。

- Motivation: 当前深度伪造检测技术面临挑战，生成式AI的进步使得视频级检测更加困难，需要更高效的解决方案。
- Method: HOLA采用两阶段框架，包括跨模态学习模块、分层上下文建模和金字塔式细化器，并结合伪监督信号注入策略。
- Result: HOLA在TestA集上以0.0476 AUC的优势排名第一，显著优于其他模型。
- Conclusion: HOLA通过创新的设计和预训练策略，为视频级深度伪造检测提供了高效解决方案。


### [56] [Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques](https://arxiv.org/abs/2507.22791)
*Weide Liu,Wei Zhou,Jun Liu,Ping Hu,Jun Cheng,Jungong Han,Weisi Lin*

Main category: cs.CV

TL;DR: 本文综述了基于模态的特征匹配方法，涵盖传统手工方法和现代深度学习方法，强调其在多模态数据中的适应性和鲁棒性。

- Motivation: 特征匹配是计算机视觉中的核心任务，但传统方法在模态差异大的情况下表现不佳，需要更先进的方法应对多样化数据。
- Method: 综述了传统手工方法（如Harris、SIFT、ORB）和深度学习方法（如SuperPoint、LoFTR），并探讨了针对不同模态（RGB、深度、3D点云等）的专用技术。
- Result: 深度学习方法显著提高了跨模态匹配的鲁棒性和适应性，特别是在复杂场景（如医学图像、LiDAR）中表现突出。
- Conclusion: 特征匹配技术正朝着多模态和跨模态方向发展，未来需进一步优化以适应更复杂的数据交互。


### [57] [Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future](https://arxiv.org/abs/2507.22792)
*Guoping Xu,Jayaram K. Udupa,Yajun Yu,Hua-Chieh Shao,Songlin Zhao,Wei Liu,You Zhang*

Main category: cs.CV

TL;DR: 本文综述了基于SAM/SAM2的视频对象分割与跟踪（VOST）方法，重点分析了其在过去、现在和未来三个时间维度的策略，并探讨了当前挑战与未来研究方向。

- Motivation: 传统VOST方法在领域泛化、时间一致性和计算效率方面存在不足，而SAM/SAM2等基础模型的出现为这一问题提供了新的解决思路。
- Method: 通过回顾SAM/SAM2在VOST中的应用，分析了历史信息保留（过去）、当前帧特征提取（现在）和未来帧动态预测（未来）的方法。
- Result: 总结了从早期内存架构到SAM2实时分割的演进，以及运动感知内存选择和轨迹引导提示等创新技术。
- Conclusion: 指出了内存冗余、错误累积和提示效率低等挑战，并提出了未来研究方向，为VOST领域的研究者和实践者提供了指导。


### [58] [Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings](https://arxiv.org/abs/2507.22802)
*Dongli He,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 利用FetalCLIP模型进行胎儿超声图像质量评估，通过LoRA微调提升性能，适用于资源有限地区的产前护理。

- Motivation: 解决低收入国家因缺乏专业超声医师导致的胎儿超声图像质量评估难题。
- Method: 基于FetalCLIP模型，采用LoRA微调技术开发FetalCLIP_CLS模型，并在ACOUSLIC-AI数据集上评估。
- Result: FetalCLIP_CLS的F1分数达0.757，改进后的分割模型进一步提升至0.771。
- Conclusion: 参数高效微调胎儿超声基础模型可实现任务适配，推动资源有限地区的产前护理发展。


### [59] [MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention](https://arxiv.org/abs/2507.22805)
*Yuqi Pang,Bowen Yang,Yun Cao,Fan Rong,Xiaoyu Li,Chen He*

Main category: cs.CV

TL;DR: MoCHA是一个新的视觉框架，通过整合多种视觉骨干网络和稀疏专家混合模块，解决了现有VLLMs的高成本和模态桥接问题。

- Motivation: 现有VLLMs在处理复杂视觉信息时面临高成本、模态桥接困难和视觉细节提取不足的问题。
- Method: MoCHA整合了CLIP、SigLIP、DINOv2和ConvNeXt四种视觉骨干网络，并采用稀疏专家混合模块（MoECs）和分层组注意力（HGA）动态选择和处理视觉特征。
- Result: MoCHA在多个基准测试中表现优异，优于现有开放权重模型，如在POPE上提升3.25%，在MME上提升153分。
- Conclusion: MoCHA通过MoECs和HGA模块显著提升了性能，验证了其有效性和鲁棒性。


### [60] [DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion](https://arxiv.org/abs/2507.22813)
*Hossein Mirzaei,Zeinab Taghavi,Sepehr Rezaee,Masoud Hadi,Moein Madadi,Mackenzie W. Mathis*

Main category: cs.CV

TL;DR: 提出了一种数据无关、零样本的触发器反演策略DISTIL，通过扩散生成器在受限搜索空间中重构触发器，显著提升了后门防御效果。

- Motivation: 深度神经网络易受后门攻击，现有触发器反演方法缺乏对触发器真实性的保证，需改进。
- Method: 采用扩散生成器，在目标分类器指导下迭代生成候选触发器，避免对触发器外观的强假设。
- Result: DISTIL在BackdoorBench数据集上准确率提升7.1%，在特洛伊目标检测模型扫描中提升9.4%。
- Conclusion: DISTIL为无需大量数据或强先验假设的后门防御提供了新方向。


### [61] [Wall Shear Stress Estimation in Abdominal Aortic Aneurysms: Towards Generalisable Neural Surrogate Models](https://arxiv.org/abs/2507.22817)
*Patryk Rygiel,Julian Suk,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 提出了一种几何深度学习方法，用于快速估计腹主动脉瘤患者的血流动力学参数，并验证其在真实世界变化因素中的泛化能力。

- Motivation: 传统计算流体动力学（CFD）模拟耗时且计算量大，几何深度学习方法可快速估计血流动力学参数，为临床实践提供潜在支持。
- Method: 采用E(3)-等变深度学习模型，结合新型稳健几何描述符和投影几何代数，基于100名患者的CT扫描数据训练模型。
- Result: 模型在分布内和外部测试集上均表现出良好的泛化能力，能准确估计几何重塑和边界条件变化下的血流动力学。
- Conclusion: 该模型在血流动力学参数估计中具有高准确性和泛化能力，有望在临床实践中发挥作用。


### [62] [Bi-Level Optimization for Self-Supervised AI-Generated Face Detection](https://arxiv.org/abs/2507.22824)
*Mian Zou,Nan Zhong,Baosheng Yu,Yibing Zhan,Kede Ma*

Main category: cs.CV

TL;DR: 提出一种基于双层优化的自监督方法，用于检测AI生成的人脸，显著优于现有方法。

- Motivation: 现有基于监督学习的AI人脸检测器依赖特定生成器的合成图像，难以泛化到新兴生成技术。
- Method: 内层循环通过线性加权的前置任务预训练视觉编码器，外层循环优化任务权重以提升检测性能。
- Result: 实验表明，该方法在一类和二分类任务中表现优异，泛化能力强。
- Conclusion: 自监督方法有效解决了AI生成人脸检测的泛化问题。


### [63] [DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion](https://arxiv.org/abs/2507.22825)
*Qingcheng Zhao,Xiang Zhang,Haiyang Xu,Zeyuan Chen,Jianwen Xie,Yuan Gao,Zhuowen Tu*

Main category: cs.CV

TL;DR: DepR是一种基于深度的单视角场景重建框架，通过实例级扩散和组合范式实现，利用深度信息在训练和推理中优化重建效果。

- Motivation: 传统方法仅将深度信息用于推理阶段的物体布局估计，未能充分利用其几何信息，DepR旨在通过深度引导的扩散模型提升重建效果。
- Method: DepR采用深度引导的条件化方法，将形状先验编码到扩散模型中，并在推理阶段通过DDIM采样和布局优化增强重建与输入图像的对齐。
- Result: 尽管训练数据有限，DepR在合成和真实数据集上均表现出色，达到最先进的性能。
- Conclusion: DepR通过深度信息的全面利用，显著提升了单视角场景重建的精度和泛化能力。


### [64] [ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents](https://arxiv.org/abs/2507.22827)
*Yilei Jiang,Yaozhi Zheng,Yuxuan Wan,Jiaming Han,Qunzhong Wang,Michael R. Lyu,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出了一种模块化多智能体框架，通过三个阶段（grounding、planning、generation）将UI设计转化为前端代码，显著提升了布局准确性和代码质量。

- Motivation: 现有基于自然语言提示的LLMs方法难以捕捉UI的空间布局和视觉设计意图，而实际UI开发是多模态的，需从视觉草图或模型开始。
- Method: 采用三阶段多智能体框架：grounding代理检测并标记UI组件，planning代理构建层次化布局，generation代理生成HTML/CSS代码。
- Result: 实验表明，该方法在布局准确性、结构连贯性和代码正确性上达到SOTA性能，并通过数据引擎增强模型能力。
- Conclusion: 该框架提高了UI到代码生成的鲁棒性和可解释性，开源代码和数据集促进了进一步研究。


### [65] [CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models](https://arxiv.org/abs/2507.22828)
*Kedong Xiu,Saiqian Zhang*

Main category: cs.CV

TL;DR: CapRecover是一种跨模态反演框架，直接从中间特征恢复高级语义内容（如标签或描述），避免图像重建，解决了视觉语言模型中的语义信息泄漏问题。

- Motivation: 随着视觉语言模型（VLMs）在分体式DNN配置中的部署，中间特征的语义信息泄漏风险增加，现有图像重建方法效果不佳，需直接解决语义泄漏问题。
- Method: 提出CapRecover框架，直接从中间特征恢复语义内容，无需重建图像；并通过添加随机噪声到中间特征来防止泄漏。
- Result: CapRecover在CIFAR-10上达到92.71%的Top-1标签准确率，在COCO2017上生成流畅描述（ROUGE-L达0.52）；噪声保护方法有效防止泄漏。
- Conclusion: CapRecover能高效恢复语义内容，噪声保护方法简单有效，为分体式DNN配置中的隐私保护提供了新思路。


### [66] [TR-PTS: Task-Relevant Parameter and Token Selection for Efficient Tuning](https://arxiv.org/abs/2507.22872)
*Siqi Luo,Haoran Yang,Yi Xin,Mingyang Yi,Guangyang Wu,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: TR-PTS是一种任务驱动的参数和令牌选择框架，通过优化计算效率和准确性，显著提升性能。

- Motivation: 解决现有参数高效微调方法任务无关性导致的效率和性能不足问题。
- Method: 使用Fisher信息矩阵选择任务相关参数，动态保留信息量高的令牌并合并冗余令牌。
- Result: 在FGVC和VTAB-1k基准测试中，性能分别超过全微调3.40%和10.35%。
- Conclusion: TR-PTS通过任务驱动的参数和令牌选择，显著提升了计算效率和模型性能。


### [67] [LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content](https://arxiv.org/abs/2507.22873)
*Simon Pochinda,Momen K. Tageldeen,Mark Thompson,Tony Rinaldi,Troy Giorshev,Keith Lee,Jie Zhou,Frederick Walls*

Main category: cs.CV

TL;DR: 论文提出了一种基于AI的低复杂度缩放器（LCS），用于减轻GPU负载，通过对抗训练和量化技术优化模型，在感知质量上优于现有硬件方案。

- Motivation: 现代游戏中内容渲染的复杂性增加导致GPU负载过重，需要一种能在低功耗设备上高效运行的解决方案。
- Method: LCS基于高效超分辨率（ESR）模型，通过对抗训练、重参数化和量化技术优化模型，训练数据为低分辨率和高分辨率游戏图像对。
- Result: LCS在五项指标评估中优于AMD的EASF和FSR1，尤其在感知质量上表现更佳。
- Conclusion: LCS展示了ESR模型在资源受限设备上的潜力，为GPU负载减轻提供了有效方案。


### [68] [Viser: Imperative, Web-based 3D Visualization in Python](https://arxiv.org/abs/2507.22885)
*Brent Yi,Chung Min Kim,Justin Kerr,Gina Wu,Rebecca Feng,Anthony Zhang,Jonas Kulhanek,Hongsuk Choi,Yi Ma,Matthew Tancik,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: Viser是一个用于计算机视觉和机器人技术的3D可视化库，旨在为Python提供易于使用且可扩展的3D可视化工具。

- Motivation: 为Python开发者提供简单且功能丰富的3D可视化解决方案，支持现代编程模式和工作流程。
- Method: 提供了一套全面的3D场景和2D GUI原语，支持独立使用或组合构建专用界面，采用命令式API和基于Web的查看器。
- Result: Viser实现了易于使用且兼容现代编程模式的3D可视化工具。
- Conclusion: Viser是一个功能强大且灵活的3D可视化库，适用于计算机视觉和机器人技术领域。


### [69] [Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation](https://arxiv.org/abs/2507.22886)
*Kaining Ying,Henghui Ding,Guanquan Jie,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出了OmniAVS数据集和OISA模型，用于多模态音频-视觉分割任务，强调复杂推理和细粒度理解。

- Motivation: 现有RAVS在多模态信息整合和深度理解推理方面存在不足，需扩展边界。
- Method: 构建包含2,098视频和59,458多模态表达的OmniAVS数据集，提出OISA模型利用MLLM进行推理分割。
- Result: OISA在OmniAVS上优于现有方法，并在其他任务中表现竞争性。
- Conclusion: OmniAVS和OISA为多模态音频-视觉分割领域提供了新基准和解决方案。
## physics.optics

### [70] [Eyepiece-free pupil-optimized holographic near-eye displays](https://arxiv.org/abs/2507.22420)
*Jie Zhou,Shuyang Xie,Yang Wu,Lei Jiang,Yimou Luo,Jun Wang*

Main category: physics.optics

TL;DR: 提出了一种无目镜、优化瞳孔的全息近眼显示方法，通过球面相位调制和联合优化，显著提升图像质量。

- Motivation: 解决全息近眼显示中因有限和动态变化的瞳孔孔径导致的图像质量下降问题。
- Method: 采用定制球面相位调制策略生成多个视点，联合优化振幅和相位分布。
- Result: 显著减轻有限瞳孔采样导致的图像退化，解决球面相位引起的深度线索不明显问题。
- Conclusion: 该方法为紧凑、轻量、灵活的全息近眼显示系统提供了重要进展。
## cs.CR

### [71] [Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions](https://arxiv.org/abs/2507.22617)
*Yiting Qu,Ziqing Yang,Yihan Ma,Michael Backes,Savvas Zannettou,Yang Zhang*

Main category: cs.CR

TL;DR: 论文研究了利用文本到图像扩散模型生成仇恨性光学幻象的风险，评估了现有内容审核模型的漏洞，并提出了初步缓解措施。

- Motivation: 探讨仇恨性光学幻象的潜在滥用风险及其对内容审核模型的挑战。
- Method: 使用Stable Diffusion和ControlNet生成1,860个光学幻象，评估六种审核分类器和九种视觉语言模型的检测能力。
- Result: 现有审核模型检测仇恨性幻象的准确率低于0.245（分类器）和0.102（视觉语言模型）。
- Conclusion: 需改进视觉编码器以识别隐藏信息，并探索图像变换和训练策略以缓解风险。
## cs.LG

### [72] [Theoretical Analysis of Relative Errors in Gradient Computations for Adversarial Attacks with CE Loss](https://arxiv.org/abs/2507.22428)
*Yunrui Yu,Hang Su,Cheng-zhong Xu,Zhizhong Su,Jun Zhu*

Main category: cs.LG

TL;DR: 论文分析了基于梯度的对抗攻击中交叉熵损失的浮点计算误差，提出了T-MIFPE损失函数以优化梯度计算的准确性。

- Motivation: 解决浮点计算误差导致的梯度计算过估计问题，提升对抗攻击的准确性。
- Method: 理论分析浮点误差在四种攻击场景中的影响，提出T-MIFPE损失函数，引入最优缩放因子T。
- Result: T-MIFPE在MNIST、CIFAR-10和CIFAR-100数据集上表现优于现有损失函数。
- Conclusion: T-MIFPE能有效减少浮点误差，提升攻击效果和鲁棒性评估准确性。


### [73] [RCR-AF: Enhancing Model Generalization via Rademacher Complexity Reduction Activation Function](https://arxiv.org/abs/2507.22446)
*Yunrui Yu,Kafeng Wang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: 提出了一种新型激活函数RCR-AF，结合GELU和ReLU的优点，通过控制稀疏性和容量提升模型鲁棒性。

- Motivation: 深度神经网络对对抗攻击的脆弱性限制了其在安全敏感领域的应用，激活函数作为关键但未充分探索的组件被研究。
- Method: 设计RCR-AF，结合GELU的平滑性和ReLU的单调性，通过超参数α和γ控制模型稀疏性和容量。
- Result: 理论分析表明RCR-AF能调节Rademacher复杂度，实验证明其在干净准确率和对抗鲁棒性上优于ReLU、GELU和Swish。
- Conclusion: RCR-AF为提升模型鲁棒性提供了理论支持和实际效果，适用于标准训练和对抗训练。


### [74] [FGFP: A Fractional Gaussian Filter and Pruning for Deep Neural Networks Compression](https://arxiv.org/abs/2507.22527)
*Kuan-Ting Tu,Po-Hsien Yu,Yu-Syuan Tseng,Shao-Yi Chien*

Main category: cs.LG

TL;DR: 提出了一种结合分数阶高斯滤波器和自适应非结构化剪枝的框架（FGFP），用于高效压缩深度神经网络，显著减少模型大小且保持高精度。

- Motivation: 由于深度神经网络在边缘设备上的计算负载过重，网络压缩技术变得至关重要。现有方法在部署时仍面临挑战。
- Method: 结合分数阶微分和高斯函数构建分数阶高斯滤波器（FGFs），并引入Grünwald-Letnikov分数阶导数降低计算复杂度。同时采用自适应非结构化剪枝（AUP）提高压缩率。
- Result: 在CIFAR-10上，ResNet-20模型大小减少85.2%，精度仅下降1.52%；在ImageNet2012上，ResNet-50模型大小减少69.1%，精度仅下降1.63%。
- Conclusion: FGFP框架在模型压缩和精度保持方面优于现有方法，适用于边缘设备部署。


### [75] [Tapping into the Black Box: Uncovering Aligned Representations in Pretrained Neural Networks](https://arxiv.org/abs/2507.22832)
*Maciej Satkiewicz*

Main category: cs.LG

TL;DR: ReLU网络隐含学习了一种线性模型，通过修改反向传播可以提取其决策边界，揭示高分辨率特征，表明神经网络依赖可解释模式。

- Motivation: 探索神经网络是否依赖可解释的学习模式，并验证其可恢复性。
- Method: 通过修改反向传播过程，提取隐含线性模型的决策边界（称为excitation pullbacks）。
- Result: 在多个ImageNet预训练架构中，提取的特征具有高分辨率和感知对齐性。
- Conclusion: 神经网络确实依赖可解释模式，这一发现对知识发现和可信AI系统有深远意义。
## cs.CE

### [76] [Mesh based segmentation for automated margin line generation on incisors receiving crown treatment](https://arxiv.org/abs/2507.22859)
*Ammar Alsheghri,Ying Zhang,Farnoosh Ghadiri,Julia Keren,Farida Cheriet,Francois Guibault*

Main category: cs.CE

TL;DR: 提出了一种基于深度学习的自动确定牙冠边缘线的新框架，通过改进的网格神经网络和投票分类器技术提高分割精度，结果显示该方法在部分测试案例中表现优于人工误差阈值。

- Motivation: 传统牙冠边缘线定义依赖人工操作，存在不一致性和不可重复性问题，需要自动化解决方案。
- Method: 使用改进的网格神经网络分割牙齿区域，结合k折交叉验证和投票分类器技术优化分割结果，再通过图割法和样条拟合预测边缘线。
- Result: 集成模型在部分测试案例中表现最佳，且牙齿预备质量越高，预测与真实边缘线的差异越小。
- Conclusion: 提出的深度学习框架能有效自动预测牙冠边缘线，为牙科设计提供了更高效和一致的工具。
## q-bio.QM

### [77] [Pathology Foundation Models are Scanner Sensitive: Benchmark and Mitigation with Contrastive ScanGen Loss](https://arxiv.org/abs/2507.22092)
*Gianluca Carloni,Biagio Brattoli,Seongho Keum,Jongchan Park,Taebum Lee,Chang Ho Ahn,Sergio Pereira*

Main category: q-bio.QM

TL;DR: 论文提出ScanGen方法，通过对比损失函数减少扫描仪偏差，提升计算病理学模型的泛化能力。

- Motivation: 深度学习在计算病理学中表现优异，但扫描仪差异导致的偏差影响模型可信度和临床应用。
- Method: 提出ScanGen对比损失函数，在多扫描仪数据集上微调基础模型以减少偏差。
- Result: ScanGen显著提升模型跨扫描仪泛化能力，同时保持或改进EGFR突变预测性能。
- Conclusion: ScanGen有效解决扫描仪偏差问题，增强计算病理学工具的临床实用性。
## eess.SP

### [78] [Exploration of Low-Cost but Accurate Radar-Based Human Motion Direction Determination](https://arxiv.org/abs/2507.22567)
*Weicheng Gao*

Main category: eess.SP

TL;DR: 提出了一种低成本但准确的雷达方法，通过特征增强和混合模型结构确定人体运动方向。

- Motivation: 运动方向角影响微多普勒频谱宽度，为步态识别等下游任务提供重要先验信息，但现有方法在特征增强和运动方向确定方面仍有改进空间。
- Method: 首先生成雷达步态DTM，通过特征链接模型增强特征，再通过轻量级Vision Transformer-CNN混合模型实现运动方向确定。
- Result: 通过开源数据集验证了方法的有效性。
- Conclusion: 该方法在低成本下实现了准确的运动方向确定，并开源了代码。
## eess.IV

### [79] [A Segmentation Framework for Accurate Diagnosis of Amyloid Positivity without Structural Images](https://arxiv.org/abs/2507.22336)
*Penghan Zhu,Shurui Mei,Shushan Chen,Xiaobo Chu,Shanbo He,Ziyi Liu*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的框架，仅使用PET图像自动分割脑区并分类淀粉样蛋白阳性，无需结构MRI或CT。

- Motivation: 减少对结构成像的依赖，简化诊断流程，适用于结构成像不可用的情况。
- Method: 使用四层深度的3D U-Net架构，在200个F18-florbetapir淀粉样PET扫描数据集上进行训练和验证。
- Result: 分割性能Dice系数0.45-0.88，分类准确率0.98，AUC 0.99。
- Conclusion: 模型有望整合到仅PET的诊断流程中，未来将扩展至其他PET示踪剂。


### [80] [Whole-brain Transferable Representations from Large-Scale fMRI Data Improve Task-Evoked Brain Activity Decoding](https://arxiv.org/abs/2507.22378)
*Yueh-Po Peng,Vincent K. M. Cheung,Li Su*

Main category: eess.IV

TL;DR: STDA-SwiFT模型通过空间-时间分割注意力和自监督对比学习，利用大规模fMRI数据集提升任务诱发活动的解码性能。

- Motivation: 解决fMRI数据高维度、低信噪比和有限样本下解码心理状态的挑战。
- Method: 提出基于Transformer的STDA-SwiFT模型，利用HCP项目的995名受试者数据进行预训练。
- Result: 模型显著提升多感官和认知领域的任务诱发活动解码性能，且对数据预处理要求低。
- Conclusion: 迁移学习是克服fMRI数据解码挑战的有效方法。


### [81] [Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation Model-driven Framework](https://arxiv.org/abs/2507.22481)
*Tianyi Liu,Kejun Wu,Chen Cai,Yi Wang,Kim-Hui Yap,Lap-Pui Chau*

Main category: eess.IV

TL;DR: 提出了一种无需人工标注的盲比特流损坏视频恢复框架，结合视觉基础模型和恢复模型，显著提升了恢复效果。

- Motivation: 视频信号在传输和存储中易受损坏，现有方法依赖人工标注且恢复效果有限。
- Method: 提出DAC模型和CFC模块，利用视觉基础模型和残差专家混合结构，自适应处理损坏。
- Result: 无需人工标注即可实现高质量视频恢复，性能显著优于现有方法。
- Conclusion: 该方法提升了用户体验和应用场景，增强了多媒体系统的可靠性。


### [82] [Learned Off-aperture Encoding for Wide Field-of-view RGBD Imaging](https://arxiv.org/abs/2507.22523)
*Haoyu Wei,Xin Liu,Yuhui Liu,Qiang Fu,Wolfgang Heidrich,Edmund Y. Lam,Yifan Peng*

Main category: eess.IV

TL;DR: 该论文提出了一种通过将衍射光学元件（DOE）置于非孔径位置的方法，以提升宽视场成像质量，并通过实验验证了其有效性。

- Motivation: 现有端到端成像系统在宽视场下难以保持高图像保真度，主要受限于计算复杂性和离轴像差建模困难。
- Method: 通过将DOE置于非孔径位置，实现波前局部控制，并结合可微分的射线和波动光学模型优化成像质量。
- Result: 实验表明，非孔径DOE在45°视场下PSNR提升超过5 dB，并在28°视场下成功恢复彩色和深度信息。
- Conclusion: 该方法通过非孔径DOE设计，显著提升了成像质量，展示了系统的多功能性和有效性。


### [83] [trAIce3D: A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images](https://arxiv.org/abs/2507.22635)
*MohammadAmin Alamalhoda,Arsalan Firoozi,Alessandro Venturino,Sandra Siegert*

Main category: eess.IV

TL;DR: trAIce3D是一种深度学习架构，用于精确分割微胶质细胞，采用两阶段方法，显著提高了分割精度和泛化能力。

- Motivation: 细胞形状包含其功能的关键信息，但现有分割方法在微胶质细胞等复杂结构上表现不佳，限制了临床研究。
- Method: trAIce3D采用两阶段方法：第一阶段用3D U-Net和视觉变换器检测细胞体，第二阶段通过交叉注意力块细化分支。
- Result: 在41,230个微胶质细胞数据集上，trAIce3D显著提升了分割精度和泛化能力。
- Conclusion: trAIce3D不仅适用于微胶质细胞，还可扩展至其他复杂细胞类型，对神经生物学研究有广泛影响。


### [84] [A Dual-Feature Extractor Framework for Accurate Back Depth and Spine Morphology Estimation from Monocular RGB Images](https://arxiv.org/abs/2507.22691)
*Yuxin Wei,Yue Zhang,Moxin Zhao,Chang Shi,Jason P. Y. Cheung,Teng Zhang,Nan Meng*

Main category: eess.IV

TL;DR: 提出了一种基于深度和表面信息的新型脊柱形态估计方法，通过GAMA-Net网络精确估计背部深度信息，显著提升了脊柱曲线生成的准确性。

- Motivation: 解决X射线在脊柱侧弯评估中的辐射和可及性问题，同时克服RGB图像易受环境因素影响的局限性。
- Method: 设计GAMA-Net网络，结合双编码器和PBHA模块提取多尺度特征，通过AMFF模块动态融合信息，整合深度和表面数据估计脊柱形态。
- Result: 深度估计模型在三个评估指标上分别达到78.2%、93.6%和97.5%的准确率，脊柱曲线生成准确率高达97%。
- Conclusion: 该方法通过深度和表面信息的结合，显著提升了脊柱形态估计的准确性和稳定性，为脊柱侧弯评估提供了更安全、可靠的解决方案。
