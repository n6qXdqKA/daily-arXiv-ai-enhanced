[[toc]]

## cs.CV

### [1] [Random Direct Preference Optimization for Radiography Report Generation](https://arxiv.org/abs/2509.21351)
*Valentin Samokhin,Boris Shirokikh,Mikhail Goncharov,Dmitriy Umerenkov,Maksim Bobrin,Ivan Oseledets,Dmitry Dylov,Mikhail Belyaev*

Main category: cs.CV

TL;DR: 提出了一种模型无关的框架，使用直接偏好优化(DPO)来提升放射学报告生成(RRG)的准确性，通过随机对比采样构建训练对，无需奖励模型或人工偏好标注。

- Motivation: 现有放射学报告生成方法在真实临床环境中的质量仍不足，而大型视觉语言模型在通用领域通过采用LLM训练策略取得了显著进展。
- Method: 使用直接偏好优化(DPO)框架，通过随机对比采样构建训练对，无需额外训练数据、奖励模型或人工标注。
- Result: 在三个最先进模型上补充随机DPO后，临床性能指标提升高达5%。
- Conclusion: 该方法能有效提升放射学报告生成的准确性，且具有模型无关性和无需额外数据的优势。


### [2] [Improving Autism Detection with Multimodal Behavioral Analysis](https://arxiv.org/abs/2509.21352)
*William Saakyan,Matthias Norden,Lola Eversmann,Simon Kirsch,Muyu Lin,Simon Guendelman,Isabel Dziobek,Hanna Drimalla*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态行为分析的自闭症谱系障碍(ASC)计算机辅助诊断方法，通过分析面部表情、语音韵律、头部运动、心率变异性和凝视行为，在大型平衡数据集上实现了74%的分类准确率。

- Motivation: 由于自闭症谱系障碍诊断复杂且资源密集，现有计算机辅助诊断方法在凝视特征表现不佳和缺乏现实世界泛化能力方面存在挑战。
- Method: 使用包含168名ASC参与者和157名非自闭症参与者的标准化视频数据集，进行多模态分析，并引入新的统计描述符来量化眼动角度变异性，采用晚期融合技术整合行为标记。
- Result: 新凝视描述符将基于凝视的分类准确率从64%提升到69%，多模态融合达到74%的总体分类准确率，与临床研究中关于ASC凝视回避的发现一致。
- Conclusion: 研究结果表明基于视频的多模态行为分析具有开发可扩展自闭症筛查工具的潜力。


### [3] [KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache](https://arxiv.org/abs/2509.21354)
*Wanshun Xu,Long Zhuang*

Main category: cs.CV

TL;DR: KV-Efficient VLA是一个模型无关的内存压缩框架，通过选择性保留高效用上下文来解决VLA模型在长序列推理时的注意力计算和KV内存增长问题。

- Motivation: VLA模型在机器人感知和控制方面具有潜力，但其可扩展性受到注意力二次计算成本和长序列推理时KV内存无限增长的限制。现有方法通过扩展骨干架构改善泛化能力，但忽视了实时部署所需的关键推理效率。
- Method: 提出轻量级、训练友好的机制，将KV缓存划分为固定大小的块，使用循环门控模块根据学习的效用分数总结和过滤历史上下文。该设计保留最近的细粒度细节，同时积极修剪过时、低相关性的内存，同时保持因果性。
- Result: 理论上，KV-Efficient VLA可实现高达1.21倍的推理加速和36%的KV内存减少，对任务成功率影响最小。该方法可无缝集成到现有的自回归和混合VLA堆栈中。
- Conclusion: 该框架无需修改训练流程或下游控制逻辑，即可实现可扩展的推理，解决了VLA模型在实时部署中的效率瓶颈问题。


### [4] [Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports](https://arxiv.org/abs/2509.21356)
*Razi Mahmood,Diego Machado-Reyes,Joy Wu,Parisa Kaviani,Ken C. L. Wong,Niharika D'Souza,Mannudeep Kalra,Ge Wang,Pingkun Yan,Tanveer Syeda-Mahmood*

Main category: cs.CV

TL;DR: 提出了一种基于短语的事实核查模型，用于检测自动生成的胸部X光报告中发现结果及其位置的错误，通过合成数据集训练多标签跨模态对比回归网络。

- Motivation: 大规模视觉语言模型能生成逼真的放射学报告，但在推理过程中存在事实错误和幻觉，阻碍了其临床转化应用。
- Method: 通过扰动真实报告中的发现结果和位置来创建合成数据集，训练多标签跨模态对比回归网络进行事实核查。
- Result: 在多个X光数据集上展示了发现结果真实性预测和定位的准确性，与基于真实标注的验证达到0.997的一致性相关系数。
- Conclusion: 该方法在放射学工作流程的临床推理中具有实用价值，能有效检测最先进报告生成器的错误。


### [5] [MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification](https://arxiv.org/abs/2509.21358)
*Jason Jordan,Mohammadreza Akbari Lor,Peter Koulen,Mei-Ling Shyu,Shu-Ching Chen*

Main category: cs.CV

TL;DR: 提出了一种多模态深度学习架构MDF-MLLM，通过融合视网膜眼底图像的细粒度特征和全局文本上下文，显著提升了视网膜疾病分类的准确性。

- Motivation: 现有的多模态大语言模型在捕捉视网膜疾病诊断所需的空间细节方面存在不足，特别是在青光眼、糖尿病视网膜病变和色素性视网膜炎等疾病的诊断中。
- Method: MDF-MLLM将U-Net编码器四层的跳跃特征集成到LLaMA 3.2 11B MLLM的交叉注意力块中，通过补丁级投影、缩放交叉注意力和基于FiLM的U-Net调制进行特征融合。
- Result: 在双类型疾病分类任务中，基线MLLM准确率为60%，而MDF-MLLM达到94%的准确率，提升了56%。召回率和F1分数分别提高了67%和35%。
- Conclusion: MDF-MLLM提供了一个可泛化、可解释和模块化的眼底图像分类框架，通过多尺度特征融合优于传统MLLM基线，有望在临床决策支持系统中实际部署。


### [6] [Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models](https://arxiv.org/abs/2509.21360)
*Xingkai Peng,Jun Jiang,Meng Tong,Shuai Li,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CV

TL;DR: 提出多模态提示解耦攻击(MPDA)，通过图像模态分离不安全提示的有害语义组件，绕过文本到图像模型的安全过滤器生成NSFW内容。

- Motivation: 现有越狱方法主要操纵文本提示，对基于图像的输入漏洞探索不足，且文本方法难以绕过模型安全过滤器。
- Method: 使用大语言模型将不安全提示解耦为伪安全提示和有害提示，将有害提示重写为自然对抗提示，通过视觉语言模型生成图像描述确保语义一致性。
- Result: MPDA能够有效绕过T2I模型的安全过滤器，生成与原始不安全提示语义一致的NSFW内容。
- Conclusion: 该方法揭示了T2I模型在图像模态输入方面的安全漏洞，为模型安全防护提供了新的研究方向。


### [7] [A Mutual Learning Method for Salient Object Detection with intertwined Multi-Supervision--Revised](https://arxiv.org/abs/2509.21363)
*Runmin Wu,Mengyang Feng,Wenlong Guan,Dong Wang,Huchuan Lu,Errui Ding*

Main category: cs.CV

TL;DR: 提出一种通过联合训练显著目标检测、前景轮廓检测和边缘检测任务的方法，利用多任务监督和互学习模块来改善显著图预测的完整性和边界精度。

- Motivation: 当前深度学习方法在显著目标检测中仍存在预测不完整和边界不准确的问题，主要由于目标内部复杂性和卷积池化操作中的步幅导致。
- Method: 使用显著目标检测、前景轮廓检测和边缘检测的多任务监督框架，开发互学习模块(MLM)作为核心构建块，通过多个网络分支的互学习提升性能。
- Result: 在七个挑战性数据集上的大量实验表明，该方法在显著目标检测和边缘检测任务上都达到了最先进的性能。
- Conclusion: 通过多任务监督和互学习机制，能够有效提升显著目标检测的完整性和边界精度，在多个数据集上取得优异表现。


### [8] [MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation](https://arxiv.org/abs/2509.21365)
*Zhicheng Du,Qingyang Shi,Jiasheng Lu,Yingshan Liang,Xinyu Zhang,Yiran Wang,Peiwu Qin*

Main category: cs.CV

TL;DR: 提出MAJORScore，一种新的多模态相关性评估指标，适用于三个及以上模态的关联分析，通过多模态联合表示实现公平的相关性评分。

- Motivation: 现有评估指标仅适用于双模态关联分析，限制了多模态相似性评估，需要开发适用于三个及以上模态的相关性度量方法。
- Method: 利用多模态联合表示将多个模态整合到同一潜在空间，在同一尺度上准确表示不同模态，为公平相关性评分提供支持。
- Result: 实验表明，MAJORScore在一致性模态上提升26.03%-64.29%，在非一致性模态上降低13.28%-20.54%，相比现有方法更可靠。
- Conclusion: MAJORScore可作为大规模多模态数据集相似性评估和多模态模型性能评估的更可靠指标。


### [9] [Safety Assessment of Scaffolding on Construction Site using AI](https://arxiv.org/abs/2509.21368)
*Sameer Prabhu,Amit Patwardhan,Ramin Karim*

Main category: cs.CV

TL;DR: 开发基于AI的云平台，通过点云数据分析脚手架结构变化，实现自动化安全检查

- Motivation: 传统脚手架安全检查依赖人工目视检查，耗时且容易出错，存在安全隐患
- Method: 开发云AI平台，通过比较认证参考数据和最新点云数据来检测结构变化
- Result: 系统能够自动检测脚手架结构变化，减少人工检查时间和精力
- Conclusion: AI和数字化技术能够提高脚手架检查准确性，提升建筑工地安全性


### [10] [Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis](https://arxiv.org/abs/2509.21375)
*Aleksa Jelaca,Ying Jiao,Chang Tian,Marie-Francine Moens*

Main category: cs.CV

TL;DR: 提出自动提示工程框架，通过图像评估器、监督提示重写器和DPO训练的排序器，实现反事实尺寸控制的文本到图像生成。

- Motivation: 解决文本到图像生成中反事实可控性的挑战，特别是反事实尺寸控制（如生成微小海象与巨大按钮），这对创意和探索性应用至关重要。
- Method: 构建三个组件：图像评估器指导数据集构建，监督提示重写器生成修订提示，DPO训练的排序器选择最优修订提示。扩展Grounded SAM改进图像评估器性能。
- Result: 构建首个反事实尺寸文本-图像数据集，图像评估器性能相比骨干网络提升114%。实验表明方法优于最先进基线和ChatGPT-4o。
- Conclusion: 为反事实可控性研究奠定基础，展示了在反事实尺寸控制方面的有效性。


### [11] [In silico Deep Learning Protocols for Label-Free Super-Resolution Microscopy: A Comparative Study of Network Architectures and SNR Dependence](https://arxiv.org/abs/2509.21376)
*Shiraz S Kaderuppan,Jonathan Mar,Andrew Irvine,Anurag Sharma,Muhammad Ramadan Saifuddin,Wai Leong Eugene Wong,Wai Lok Woo*

Main category: cs.CV

TL;DR: 本研究评估了两种深度神经网络架构（O-Net和Theta-Net）在非荧光相位调制显微镜中实现超分辨率成像的性能，发现在不同信噪比条件下两种模型具有互补性。

- Motivation: 解决光学显微镜横向分辨率限制（约200nm）的问题，避免使用昂贵的外部模块或专业荧光超分辨率技术，为普通显微镜用户提供经济可行的超分辨率解决方案。
- Method: 使用两种先前开发的深度神经网络架构（O-Net和Theta-Net），在Zernike相位对比和微分干涉对比显微镜模式下，对通过原子力显微镜校准的纳米级特征测试目标进行超分辨率重建评估。
- Result: 两种模型在超分辨率重建中表现良好但具有互补性：高信噪比图像更适合O-Net模型，低信噪比图像则更适合Theta-Net模型。
- Conclusion: 模型架构与源图像信噪比的结合对深度神经网络在非荧光光学纳米显微镜中的性能至关重要，即使使用相同的训练数据集和训练周期，不同架构在不同条件下表现各异。


### [12] [Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation](https://arxiv.org/abs/2509.21377)
*Yinfeng Yu,Hailong Zhang,Meiling Zhu*

Main category: cs.CV

TL;DR: 提出DMTF-AVN方法，通过多目标架构和Transformer机制动态融合视听信息，在机器人导航中实现最先进的性能表现

- Motivation: 现有视听融合方法往往忽视更深层次的感知上下文，需要更有效地利用多模态线索来指导导航
- Method: 使用多目标架构结合改进的Transformer机制，过滤和选择性融合跨模态信息
- Result: 在Replica和Matterport3D数据集上实现最先进性能，在成功率、路径效率和场景适应性方面优于现有方法
- Conclusion: 模型展现出强大的可扩展性和泛化能力，为机器人导航中的高级多模态融合策略开辟了新途径


### [13] [SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders](https://arxiv.org/abs/2509.21379)
*Enrico Cassano,Riccardo Renzulli,Marco Nurisso,Mirko Zaffaroni,Alan Perotti,Marco Grangetto*

Main category: cs.CV

TL;DR: SAEmnesia是一种监督式稀疏自编码器训练方法，通过系统性概念标注促进概念-神经元的一对一映射，显著减少文本到图像扩散模型中概念遗忘所需的超参数搜索。

- Motivation: 现有稀疏自编码器虽然减少了神经元的多义性，但单个概念表示仍分布在多个潜在特征中，需要大量搜索过程进行概念遗忘。
- Method: 引入监督式稀疏自编码器训练，通过概念标注缓解特征分裂并促进特征集中化，学习具有更强概念关联的专用神经元。
- Result: 在推理时，可解释表示将超参数搜索减少96.67%；在UnlearnCanvas基准上比现有技术提升9.22%；在顺序遗忘任务中，9对象移除的遗忘准确率提升28.4%。
- Conclusion: SAEmnesia通过促进概念-神经元一对一映射，显著提高了文本到图像扩散模型概念遗忘的效率和准确性。


### [14] [Coreset selection based on Intra-class diversity](https://arxiv.org/abs/2509.21380)
*Imran Ashraf,Mukhtar Ullah,Muhammad Faisal Nadeem,Muhammad Nouman Noor*

Main category: cs.CV

TL;DR: 提出了一种智能轻量级核心集选择方法，通过提取类内多样性形成每类聚类，用于最终采样，以解决深度学习模型训练中的计算资源问题。

- Motivation: 深度学习模型在生物医学图像分类中需要大量计算资源和时间，特别是超参数搜索需要多轮训练。随机采样方法在类别不平衡数据集中存在偏差，无法捕捉类内多样性。
- Method: 引入智能轻量级核心集选择机制，提取类内多样性并形成每类聚类，然后基于这些聚类进行最终采样。
- Result: 在知名生物医学成像数据集上的分类实验表明，该方法在多个性能指标上优于随机采样方法。
- Conclusion: 提出的核心集选择方案能够有效减少计算需求，同时保持数据集代表性，在生物医学图像分类任务中表现出优越性能。


### [15] [The LongiMam model for improved breast cancer risk prediction using longitudinal mammograms](https://arxiv.org/abs/2509.21383)
*Manel Rakez,Thomas Louis,Julien Guillaumin,Foucauld Chamming's,Pierre Fillard,Brice Amadeo,Virginie Rondeau*

Main category: cs.CV

TL;DR: LongiMam是一个端到端的深度学习模型，整合当前和最多四次既往乳腺X光片，通过卷积和循环神经网络捕捉空间和时间模式来预测乳腺癌风险。

- Motivation: 当前大多数深度学习模型仅使用单次或有限次数的既往乳腺X光片，且缺乏对现实世界中不平衡结果分布和异质性随访的适应性。
- Method: 开发LongiMam模型，结合卷积神经网络和循环神经网络，利用当前和最多四次既往乳腺X光片数据，在大规模人群筛查数据集上进行训练和评估。
- Result: 在多种不同既往检查数量和组成的场景下，LongiMam在包含既往乳腺X光片时持续改善预测性能。结合既往和当前访问的表现优于单次访问模型，而仅使用既往数据表现较差。
- Conclusion: 纵向建模增强了乳腺癌预测能力，支持在筛查项目中使用重复乳腺X光片来优化风险分层。LongiMam已作为开源软件公开提供。


### [16] [Assessing the Alignment of Popular CNNs to the Brain for Valence Appraisal](https://arxiv.org/abs/2509.21384)
*Laurent Mertens,Elahe' Yargholi,Laura Van Hove,Hans Op de Beeck,Jan Van den Stock,Joost Vennekens*

Main category: cs.CV

TL;DR: 该论文研究了卷积神经网络(CNNs)在复杂社会认知任务（如图像情感评估）中与人类大脑处理过程的对应关系，发现CNNs难以超越简单视觉处理，未能反映高阶大脑处理。

- Motivation: 探索CNNs与人类大脑在复杂社会认知任务（如图像情感评估）中的对应关系，而不仅仅是通用视觉感知。
- Method: 通过相关性分析评估流行CNN架构与人类行为和fMRI数据的对齐程度，并提出Object2Brain框架结合GradCAM和对象检测来分析不同对象类别对CNN-人类相关性的影响。
- Result: CNNs在图像情感评估任务中难以超越简单视觉处理，不反映高阶大脑处理；不同CNN架构显示出不同的对象类别敏感性。
- Conclusion: 对于复杂的社会认知任务，CNNs与人类大脑处理的对应关系有限，需要更深入的研究来理解神经网络与人类认知的对应机制。


### [17] [Debugging Concept Bottleneck Models through Removal and Retraining](https://arxiv.org/abs/2509.21385)
*Eric Enouen,Sainyam Galhotra*

Main category: cs.CV

TL;DR: 提出了一个针对概念瓶颈模型(CBMs)的可解释调试框架，通过移除和重训练两步过程来纠正模型与专家推理之间的系统性偏差。

- Motivation: 传统CBM的概念干预无法解决模型与专家推理之间的系统性偏差问题，比如模型从有偏数据中学习捷径。
- Method: 采用移除和重训练两步框架：移除步骤中专家识别并移除不需要的概念；重训练步骤使用CBDebug方法将概念级反馈转换为样本级辅助标签，用于监督偏差缓解和针对性增强。
- Result: CBDebug在多个CBM架构(PIP-Net、Post-hoc CBM)和已知虚假相关基准测试中显著优于先前的重训练方法。
- Conclusion: 该框架有效减少了模型对不需要概念的依赖，提升了CBM与专家推理的对齐程度。


### [18] [ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data](https://arxiv.org/abs/2509.21386)
*Anja Sheppard,Tyler Smithline,Andrew Scheffer,David Smith,Advaith V. Sethuraman,Ryan Bird,Sabrina Lin,Katherine A. Skinner*

Main category: cs.CV

TL;DR: ShipwreckFinder是一个开源QGIS插件，使用深度学习从多波束声纳数据中自动检测沉船，通过合成数据增强训练集，在分割性能上优于现有工具。

- Motivation: 沉船是海洋历史的重要标记，但通过人工检查测深数据发现沉船耗时且需要专家分析，因此需要自动化工具来提高效率。
- Method: 开发开源QGIS插件，使用深度学习模型（在五大湖和爱尔兰海岸沉船数据上训练），结合合成数据生成增强数据集，自动预处理测深数据并执行推理。
- Result: 与基于深度学习的ArcGIS工具包和传统反洼地检测方法相比，该工具在分割性能上表现更优。
- Conclusion: ShipwreckFinder提供了一个有效的开源解决方案，能够自动检测沉船，显著提高沉船发现的效率。


### [19] [Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence](https://arxiv.org/abs/2509.21387)
*Sanish Suwal,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 该研究探讨了基于幅度的剪枝如何影响神经网络的可解释性，发现轻度到中度剪枝能改善显著性图的聚焦性和忠实度，而过度剪枝会降低概念连贯性。

- Motivation: 先前研究表明神经网络可以大幅剪枝而不损失性能，但剪枝对模型可解释性的影响尚不清楚，需要系统研究剪枝如何影响从低层显著性图到高层概念表示的解释性。
- Method: 使用在ImageNette上训练的ResNet-18，比较不同剪枝水平下Vanilla Gradients和Integrated Gradients的后验解释，评估稀疏性和忠实度，并应用CRAFT方法提取概念来跟踪语义连贯性变化。
- Result: 轻度到中度剪枝改善了显著性图的聚焦性和忠实度，同时保持了清晰、语义有意义的概念；而过度剪枝合并了异质特征，降低了显著性图稀疏性和概念连贯性，尽管准确率得以保持。
- Conclusion: 剪枝可以塑造内部表示使其更符合人类注意力模式，但过度剪枝会损害可解释性，需要在模型压缩和可解释性之间找到平衡。


### [20] [TUN3D: Towards Real-World Scene Understanding from Unposed Images](https://arxiv.org/abs/2509.21388)
*Anton Konushin,Nikita Drozdov,Bulat Gabdullin,Alexey Zakharov,Anna Vorontsova,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: TUN3D是首个基于多视角图像输入，无需真实相机位姿或深度监督，联合进行室内布局估计和3D目标检测的方法。

- Motivation: 现有方法通常依赖点云输入，但大多数消费级相机缺乏深度传感器，视觉数据更为普遍。需要解决仅使用多视角图像进行室内场景理解的问题。
- Method: 基于轻量级稀疏卷积骨干网络，使用两个专用头：一个用于3D目标检测，一个用于布局估计，采用新颖有效的参数化墙壁表示。
- Result: 在三个具有挑战性的场景理解基准测试中达到最先进性能：(i)使用真实点云，(ii)使用带位姿图像，(iii)使用无位姿图像。在3D目标检测方面与专门方法相当，在布局估计方面显著提升。
- Conclusion: TUN3D为整体室内场景理解设立了新基准，是首个仅使用多视角图像就能联合进行布局估计和3D目标检测的方法。


### [21] [Large AI Model-Enabled Generative Semantic Communications for Image Transmission](https://arxiv.org/abs/2509.21394)
*Qiyu Ma,Wanli Ni,Zhijin Qin*

Main category: cs.CV

TL;DR: 提出了一种创新的生成式语义通信系统，通过将图像分割为关键区域和非关键区域来优化语义粒度，关键区域使用图像导向语义编码器处理，非关键区域采用图像到文本建模方法压缩，并结合轻量级部署策略提升资源利用率。

- Motivation: 现有方法往往忽视图像不同区域的重要性差异，可能损害视觉关键内容的重建质量，需要解决这一问题以提升语义通信系统的性能。
- Method: 1. 图像分割为关键和非关键区域；2. 关键区域使用图像导向语义编码器；3. 非关键区域采用图像到文本建模压缩；4. 结合模型量化和低秩适应微调实现轻量级部署。
- Result: 仿真结果表明，所提系统在语义保真度和视觉质量方面均优于传统方法，验证了其在图像传输任务中的有效性。
- Conclusion: 该系统通过精细化语义粒度处理和轻量级部署策略，成功提升了图像传输的效率和重建质量，为语义通信系统提供了有效的解决方案。


### [22] [mmHSense: Multi-Modal and Distributed mmWave ISAC Datasets for Human Sensing](https://arxiv.org/abs/2509.21396)
*Nabeel Nisar Bhat,Maksim Karnaukh,Stein Vandenbroeke,Wouter Lemoine,Jakob Struye,Jesus Omar Lacruz,Siddhartha Kumar,Mohammad Hossein Moghaddam,Joerg Widmer,Rafael Berkvens,Jeroen Famaey*

Main category: cs.CV

TL;DR: mmHSense是一个开放的毫米波数据集，用于支持集成感知与通信系统中的人类感知研究，包含手势识别、人员识别、姿态估计和定位等应用，并展示了参数高效微调方法。

- Motivation: 为毫米波集成感知与通信系统提供开放标注数据集，支持人类感知研究，促进信号处理和深度学习在该领域的发展。
- Method: 构建包含多种人类感知任务的毫米波数据集，描述测试平台、实验设置和信号特征，并采用参数高效微调方法适配不同任务。
- Result: 成功创建了可用于多种下游任务的毫米波数据集，验证了参数高效微调在降低计算复杂度的同时保持性能的有效性。
- Conclusion: mmHSense数据集为毫米波ISAC研究提供了宝贵资源，参数高效微调方法为模型适配不同任务提供了高效解决方案。


### [23] [Skeleton Sparsification and Densification Scale-Spaces](https://arxiv.org/abs/2509.21398)
*Julia Gierke,Pascal Peter*

Main category: cs.CV

TL;DR: 提出骨架化尺度空间框架，通过稀疏化中轴实现形状的层次简化，满足尺度空间特性并支持从粗到细的逆过程。

- Motivation: 中轴对噪声敏感，传统剪枝方法存在局限性，需要满足尺度空间特性的层次简化框架。
- Method: 结合稀疏化思想，构建骨架化尺度空间，包括稀疏化和稠密化两个方向，提供连续和离散理论基础。
- Result: 框架满足层次结构、可控简化和几何变换等变性等尺度空间特性，在骨架化、形状压缩和增材制造中验证有效性。
- Conclusion: 骨架化尺度空间为形状分析提供了理论严谨的层次简化工具，具有实际应用价值。


### [24] [Downscaling climate projections to 1 km with single-image super resolution](https://arxiv.org/abs/2509.21399)
*Petr Košťál,Pavel Kordík,Ondřej Podsztavek*

Main category: cs.CV

TL;DR: 使用单图像超分辨率模型将气候预测从12.5km分辨率降尺度到1km分辨率，通过基于气候指标的评估方法验证降尺度效果，结果显示该方法不会增加气候指标误差。

- Motivation: 现有气候预测空间分辨率较低（如12.5km），限制了其在地方决策中的可用性，需要高分辨率气候预测支持地方决策。
- Method: 利用单图像超分辨率模型进行统计降尺度，在高分辨率观测网格数据集上训练模型，然后应用于低分辨率气候预测，提出基于气候指标的评估方法。
- Result: 在日平均温度上的实验表明，单图像超分辨率模型能够降尺度气候预测，且不会增加气候指标误差。
- Conclusion: 单图像超分辨率模型可以有效降尺度气候预测，提高空间分辨率而不增加气候指标误差，为地方决策提供更精细的气候信息。


### [25] [JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation](https://arxiv.org/abs/2509.21401)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.CV

TL;DR: 提出了一种名为JaiLIP的图像空间越狱攻击方法，通过最小化联合目标函数生成高效且难以察觉的对抗图像，在毒性生成方面优于现有方法。

- Motivation: 视觉语言模型(VLMs)存在被滥用的安全隐患，特别是基于图像的扰动攻击能够有效生成有害输出，现有方法存在性能不稳定和可见扰动的问题。
- Method: JaiLIP方法在图像空间中最小化联合目标函数，结合干净图像与对抗图像的均方误差损失和模型有害输出损失。
- Result: 实验结果表明该方法生成的对抗图像高效且难以察觉，在标准毒性指标上优于现有方法，并在交通领域验证了攻击的实用性。
- Conclusion: 研究强调了基于图像的越狱攻击的实际挑战，以及为VLMs开发高效防御机制的必要性。


### [26] [Overview of ExpertLifeCLEF 2018: how far automated identification systems are from the best experts?](https://arxiv.org/abs/2509.21419)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 该研究通过LifeCLEF 2018 ExpertCLEF挑战赛，比较了19个深度学习系统与9位法国植物学专家的植物识别性能，发现最先进的深度学习模型性能已接近人类专家水平。

- Motivation: 量化自动识别系统与人类专家在生物识别方面的性能差距，比较深度学习模型与植物学专家的识别准确率和不确定性。
- Method: 设计ExpertCLEF挑战赛，评估4个研究团队的19个深度学习系统，并与9位法国植物学专家进行对比分析。
- Result: 最先进的深度学习模型在植物识别任务上的表现已接近最先进的人类专家水平。
- Conclusion: 深度学习在植物自动识别领域取得了显著进展，性能已逼近人类专家，为计算机科学家和自然学家提供了有价值的参考。


### [27] [QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models](https://arxiv.org/abs/2509.21420)
*Jian Liu,Chunshi Wang,Song Guo,Haohan Weng,Zhen Zhou,Zhiqi Li,Jiaao Yu,Yiling Zhu,Jing Xu,Biwen Lei,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: QuadGPT是首个端到端的自回归四边形网格生成框架，通过序列预测范式直接生成四边形网格，避免了传统三角网格转换方法产生的拓扑质量问题。

- Motivation: 现有方法首先生成三角网格再合并为四边形，导致拓扑质量差。需要开发直接生成四边形网格的端到端方法。
- Method: 采用自回归框架，包含统一标记化方法处理混合拓扑，以及专门的强化学习微调方法tDPO提升生成质量。
- Result: 在几何精度和拓扑质量上显著超越之前的三角到四边形转换流程。
- Conclusion: 为原生四边形网格生成设定了新基准，展示了大规模自回归模型与拓扑感知RL优化相结合在创建结构化3D资源方面的强大能力。


### [28] [DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation](https://arxiv.org/abs/2509.21433)
*Jiaqi Liu,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: DyME是一个按需概念擦除框架，通过训练轻量级LoRA适配器并动态组合，实现灵活的多概念擦除，解决了现有静态擦除方法无法处理多概念冲突的问题。

- Motivation: 现有文本到图像扩散模型会无意中复制受版权保护的风格和视觉概念，引发法律和伦理问题。现有概念擦除方法无法扩展到需要擦除多个可能冲突概念的实用场景。
- Method: 训练概念特定的LoRA适配器，在推理时动态组合所需适配器。引入双层正交性约束（特征级和参数级）来解耦表示偏移并强制正交适配器子空间。
- Result: 在ErasureBench-H和标准数据集上的实验表明，DyME持续优于最先进的基线方法，在多概念擦除保真度方面表现更好，同时最小化附带退化。
- Conclusion: DyME提供了一种可扩展的按需概念擦除解决方案，通过动态适配器组合和正交性约束，有效解决了多概念擦除中的干扰问题。


### [29] [VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding](https://arxiv.org/abs/2509.21451)
*Abdul Waheed,Zhen Wu,Dareen Alharthi,Seungone Kim,Bhiksha Raj*

Main category: cs.CV

TL;DR: VideoJudge是一个专门用于评估视频理解模型输出的多模态大语言模型，在多个基准测试中优于更大的基线模型，证明了视频输入对于视频理解任务评估的重要性。

- Motivation: 传统评估指标如BLEU、ROUGE等无法捕捉人类判断的细微差别，而人工评估成本高昂。现有LLM/MLLM评估器在视频理解领域的应用相对不足。
- Method: 通过生成器-评估器的交互训练：生成器根据目标评分生成响应，不匹配评估器评分的响应被丢弃，从而训练出专门的视频评估模型。
- Result: 在四个元评估基准中的三个上，VideoJudge-7B优于更大的MLLM基线模型（如Qwen2.5-VL 32B和72B）。LLM评估器表现不如MLLM评估器，长链推理无法提升性能。
- Conclusion: 视频输入对于视频理解任务的评估至关重要，VideoJudge展示了专门化MLLM评估器在视频理解评估中的有效性。


### [30] [Residual Vector Quantization For Communication-Efficient Multi-Agent Perception](https://arxiv.org/abs/2509.21464)
*Dereje Shenkut,B. V. K Vijaya Kumar*

Main category: cs.CV

TL;DR: ReVQom是一种用于多智能体协同感知的端到端特征压缩方法，通过瓶颈网络和多阶段残差向量量化将特征从8192 bpp压缩到6-30 bpp，在保持精度的同时大幅减少通信带宽需求。

- Motivation: 多智能体协同感知通过共享信息提升场景理解能力，但通信带宽限制了系统的可扩展性，需要高效的压缩方法来减少传输负载。
- Method: 采用端到端方法，先通过瓶颈网络压缩特征维度，然后使用多阶段残差向量量化(RVQ)，仅传输每个像素的码本索引，大幅减少传输数据量。
- Result: 在DAIR-V2X数据集上实现273x到1365x的压缩比，在18 bpp时性能匹配或优于原始特征协同感知，在6-12 bpp时仍能保持可接受的性能。
- Conclusion: ReVQom实现了高效准确的多智能体协同感知，为实际V2X部署迈出了重要一步。


### [31] [Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models](https://arxiv.org/abs/2509.21466)
*Khaloud S. AlKhalifah,Malak Mashaabi,Hend Al-Khalifa*

Main category: cs.CV

TL;DR: 本研究分析了三种文本到图像AI模型在生成沙特阿拉伯职业形象时存在的性别刻板印象和文化不准确问题，发现这些模型普遍存在严重的性别失衡和文化误解。

- Motivation: 调查当代文本到图像AI模型在生成沙特职业形象时是否延续性别刻板印象和文化不准确性，评估这些模型对沙特劳动力市场性别动态和文化细微差别的反映程度。
- Method: 使用中性提示词为56种沙特职业生成1,006张图像，由两名沙特标注员从五个维度评估：感知性别、服装外观、背景环境、活动互动和年龄，并由资深研究员仲裁分歧，共获得10,100个判断。
- Result: 三种模型均显示严重性别失衡：ImageFX 85%男性、Grok 86.6%男性、DALL-E V3 96%男性，其中DALL-E V3性别刻板印象最强。领导和技术角色性别失衡最明显，所有模型都频繁出现服装、环境和活动方面的文化不准确性。
- Conclusion: 当前模型反映了训练数据中嵌入的社会偏见，对沙特劳动力市场的性别动态和文化细微差别反映有限，迫切需要更多样化的训练数据、更公平的算法和更具文化敏感性的评估框架。


### [32] [Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Moderation](https://arxiv.org/abs/2509.21486)
*Zixuan Wang,Yu Sun,Hongwei Wang,Baoyu Jing,Xiang Shen,Xin Dong,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: 提出了一种基于推理增强的多模态大语言模型预训练范式，用于统一的不当内容检测，通过三个针对性预训练任务解决短视频内容与原始预训练数据之间的分布差距问题。

- Motivation: 现有方法通常为每种不当内容类型训练单独的小型分类模型，需要大量人工标注数据且缺乏跨问题泛化能力。
- Method: 引入三个预训练任务：Caption（增强视频细节感知）、VQA（加深对问题定义和标注指南的理解）、Chain-of-Thought（增强推理能力）。
- Result: 实验结果显示该预训练方法显著提升了MLLM在零样本和监督微调设置下的性能，且对未见问题表现出强泛化能力。
- Conclusion: 该推理增强的MLLM预训练范式有效解决了短视频不当内容检测中的分布差距和复杂问题定义挑战。


### [33] [Learning GUI Grounding with Spatial Reasoning from Visual Feedback](https://arxiv.org/abs/2509.21552)
*Yu Zhao,Wei-Ning Chen,Huseyin Atahan Inan,Samuel Kessler,Lu Wang,Lukas Wutschitz,Fangkai Yang,Chaoyun Zhang,Pasquale Minervini,Saravan Rajmohan,Robert Sim*

Main category: cs.CV

TL;DR: 将GUI定位重新定义为交互式搜索任务，通过多步强化学习训练模型移动光标来定位UI元素，显著提升了定位精度。

- Motivation: 传统基于坐标预测的GUI定位方法在处理高分辨率复杂布局界面时效果不佳，因为视觉语言模型难以准确预测数字坐标。
- Method: 将GUI定位重构为交互式搜索任务，模型通过多步动作移动光标接近目标元素，使用强化学习和基于轨迹的奖励函数进行训练。
- Result: GUI-Cursor模型在ScreenSpot-v2数据集上准确率从88.8%提升到93.9%，在ScreenSpot-Pro上从26.8%提升到56.5%，95%的实例在两步内解决问题。
- Conclusion: 交互式搜索方法比直接坐标预测更有效，能够自适应处理不同难度的问题，显著提升GUI定位性能。


### [34] [X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.21559)
*Prasanna Reddy Pulakurthi,Jiamian Wang,Majid Rabbani,Sohail Dianat,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.CV

TL;DR: 提出了X-CoT框架，使用LLM的思维链推理替代传统的嵌入模型相似度排序，提高文本-视频检索的可解释性和性能。

- Motivation: 现有文本-视频检索系统存在两个问题：低质量文本-视频数据对会影响检索效果但难以识别；余弦相似度无法解释排序结果，限制了可解释性。
- Method: 设计检索思维链，包含成对比较步骤，生成详细推理和完整排序；扩展现有基准数据集，添加视频注释以支持语义理解并减少数据偏差。
- Result: X-CoT在实证中提高了检索性能并产生详细推理依据，同时促进了模型行为和数据质量分析。
- Conclusion: X-CoT框架通过LLM思维链推理实现了可解释的文本-视频检索，既能提升性能又能提供详细解释，有助于模型评估和数据检查。


### [35] [Unsupervised Defect Detection for Surgical Instruments](https://arxiv.org/abs/2509.21561)
*Joseph Huang,Yichi Zhang,Jingxi Yu,Wei Chen,Seunghyun Hwang,Qiang Qiu,Amy R. Reibman,Edward J. Delp,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出了一种针对手术器械的缺陷检测方法，通过背景掩码、基于补丁的分析和领域自适应技术，解决了现有方法在手术器械图像上误报率高、对小缺陷敏感度不足的问题。

- Motivation: 手术器械的安全性需要可靠的视觉缺陷检测，但人工检查易出错，现有基于自然/工业图像的自动检测方法无法有效迁移到手术领域，存在误报、对小缺陷敏感度不足等问题。
- Method: 整合背景掩码、基于补丁的分析策略和高效的领域自适应技术，专门针对手术器械图像优化无监督缺陷检测方法。
- Result: 该方法克服了现有方法的局限性，能够可靠地检测手术器械图像中的细粒度缺陷。
- Conclusion: 提出的方法通过领域特定的优化，实现了对手术器械缺陷的有效检测，解决了现有方法在手术领域应用中的关键问题。


### [36] [No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models](https://arxiv.org/abs/2509.21565)
*Junno Yun,Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: cs.CV

TL;DR: 提出一种基于线性可分性（LSEP）的替代正则化方法，用于训练大规模扩散模型，无需依赖外部编码器，直接在网络学习动态中整合线性探测，显著提升训练效率和生成质量。

- Motivation: 现有基于表示对齐的方法依赖计算昂贵的大型预训练编码器，需要寻找更高效的训练策略来改进扩散模型中的判别性特征表示。
- Method: 提出LSEP正则化方法，通过促进中间层表示的线性可分性来训练模型，无需辅助编码器和表示对齐，将线性探测直接整合到学习过程中。
- Result: 在基于流的Transformer架构（如SiTs）上取得了显著改进，在256×256 ImageNet数据集上达到FID 1.46的优异结果。
- Conclusion: LSEP方法提供了一种无需外部编码器的有效训练策略，在提升训练效率和生成质量方面表现出色，为大规模扩散模型训练提供了新的方向。


### [37] [Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms](https://arxiv.org/abs/2509.21573)
*Boyi Chen,Zhangyu Wang,Fabian Deuser,Johann Maximilian Zollner,Martin Werner*

Main category: cs.CV

TL;DR: 提出了一种空间正则化的对比学习策略，通过整合半变异函数来建模空间相关性，解决图像地理定位中的假阴性和难负样本问题。

- Motivation: 现有对比学习方法忽略了地理空间中的空间依赖性，导致无法有效处理假阴性（视觉和地理相似但被标记为负样本）和难负样本（视觉相似但地理距离远）的问题。
- Method: 使用半变异函数将特征空间中的图像距离与地理距离关联，拟合空间相关性模型，并以此定义期望视觉差异来识别难负样本和假阴性，将该策略集成到GeoCLIP中。
- Result: 在OSV5M数据集上的评估表明，显式建模空间先验能提高图像地理定位性能，特别是在更细粒度上表现更佳。
- Conclusion: 通过整合空间统计工具来建模空间相关性，可以有效提升对比学习在图像地理定位任务中的性能，特别是在处理空间依赖关系方面。


### [38] [X-Streamer: Unified Human World Modeling with Audiovisual Interaction](https://arxiv.org/abs/2509.21574)
*You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Guoxian Song,Xiaochen Zhao,Chao Liang,Jianwen Jiang,Hongyi Xu,Linjie Luo*

Main category: cs.CV

TL;DR: X-Streamer是一个端到端多模态人类世界建模框架，能够从单张肖像创建数字人类代理，支持文本、语音和视频的无限实时交互。

- Motivation: 构建能够在单一统一架构中实现文本、语音和视频无限交互的数字人类代理，将静态肖像转化为持久智能的视听交互。
- Method: 采用Thinker-Actor双变换器架构，Thinker模块基于预训练大语言-语音模型进行感知推理，Actor模块使用分块自回归扩散模型，通过交叉注意力机制生成时间对齐的多模态响应。
- Result: X-Streamer在两块A100 GPU上实现实时运行，能够从任意肖像维持数小时一致的视频聊天体验。
- Conclusion: 该框架为交互式数字人类的统一世界建模开辟了新途径，实现了实时、开放式的视频通话体验。


### [39] [What Happens Next? Anticipating Future Motion by Generating Point Trajectories](https://arxiv.org/abs/2509.21592)
*Gabrijel Boduljak,Laurynas Karazija,Iro Laina,Christian Rupprecht,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 该论文提出了一种从单张图像预测物体运动轨迹的方法，通过生成密集轨迹网格而非像素来直接建模运动，相比现有视频生成器在运动预测方面表现更优。

- Motivation: 现有视频生成器虽然被视为世界模型，但在从单张图像预测运动方面表现不佳，特别是在简单物理场景中。这是因为生成像素的开销过大，而非直接建模运动。
- Method: 将运动预测任务建模为条件生成密集轨迹网格，采用现代视频生成器的架构但输出运动轨迹而非像素，从而直接捕捉场景范围内的动态和不确定性。
- Result: 该方法在模拟数据上表现优异，在机器人等下游应用中有效，在真实世界直观物理数据集上显示出有前景的准确性，比现有回归器和生成器产生更准确和多样化的预测。
- Conclusion: 直接建模运动轨迹比生成像素更有效地解决从单张图像预测运动的问题，现有视频生成器由于像素生成的开销而在此任务上受限。


### [40] [Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis](https://arxiv.org/abs/2509.21595)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 比较DINOv3和V-JEPA2两种自监督学习架构在视频动作识别中的表现，发现DINOv3在姿态识别上更优，V-JEPA2在时间建模上更稳定

- Motivation: 理解不同自监督学习架构在视频动作识别中的性能差异，为选择合适的特征提取方法提供实证指导
- Method: 在UCF Sports数据集上比较DINOv3（独立帧处理）和V-JEPA2（联合时间建模），评估分类准确率、聚类性能、类内一致性和类间区分度
- Result: DINOv3聚类性能更优（轮廓系数0.31 vs 0.21），姿态识别区分度高（6.16倍分离比）；V-JEPA2性能方差更小（0.094 vs 0.288），在所有动作类型上表现稳定
- Conclusion: DINOv3的空间处理架构擅长静态姿态识别，V-JEPA2的时间建模在不同动作类别间提供更平衡的表现，选择应基于任务需求和可靠性约束


### [41] [VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment](https://arxiv.org/abs/2509.21609)
*Md. Mahfuzur Rahman,Kishor Datta Gupta,Marufa Kamal,Fahad Rahman,Sunzida Siddique,Ahmed Rafi Hasan,Mohd Ariful Haque,Roy George*

Main category: cs.CV

TL;DR: VLCE是一个多模态视觉语言系统，通过双架构方法（CNN-LSTM和Vision Transformer）结合外部语义知识，为灾害图像生成详细、情境感知的描述，显著优于现有基线模型。

- Motivation: 传统灾害评估方法缓慢危险，现有计算机视觉方法只能提供分类标签或分割掩码，缺乏对灾害场景的全面理解能力。
- Method: 采用双架构方法：基于ResNet50的CNN-LSTM模型处理卫星图像，Vision Transformer模型处理无人机图像，结合ConceptNet和WordNet的外部语义知识提升词汇覆盖和描述准确性。
- Result: VLCE在InfoMetIC指标上达到最高95.33%，显著超越LLaVA和QwenVL等基线模型，同时保持有竞争力的语义对齐性能。
- Conclusion: VLCE系统通过自动化生成信息密集的描述，在提升灾害损害评估方面展现出巨大潜力。


### [42] [A Data-driven Typology of Vision Models from Integrated Representational Metrics](https://arxiv.org/abs/2509.21628)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 该论文开发了一个生物学启发的框架，通过多种表征相似性度量来系统分析不同视觉模型家族的表征特性，发现几何结构和单元调谐携带家族特异性特征，而线性可解码信息则更广泛共享。

- Motivation: 当前大型视觉模型在架构和训练范式上差异很大，但缺乏系统方法来识别哪些表征特性是跨家族共享的，哪些反映了独特的计算策略。
- Method: 使用多种表征相似性度量（几何、单元调谐、线性可解码性），并采用相似性网络融合（SNF）方法来整合这些互补的度量维度。
- Result: 几何和调谐度量能有效区分模型家族，而线性预测性度量区分度较弱。SNF融合后实现了更强的家族分离，发现监督训练的ResNets和ViTs形成不同聚类，而所有自监督模型跨架构边界聚集在一起。
- Conclusion: 该框架为视觉模型提供了原则性的分类学，表明由架构和训练目标共同塑造的计算策略定义了表征结构，超越了表面设计类别。


### [43] [FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction](https://arxiv.org/abs/2509.21657)
*Yixiang Dai,Fan Jiang,Chiyu Wang,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: FantasyWorld是一个几何增强框架，通过向冻结的视频基础模型添加可训练的几何分支，在单次前向传播中联合建模视频潜在表示和隐式3D场，实现3D感知的视频生成。

- Motivation: 当前视频基础模型缺乏明确的3D基础能力，限制了空间一致性和下游3D推理任务的实用性。需要将视频想象力与3D感知能力相结合。
- Method: 引入跨分支监督机制，几何线索指导视频生成，视频先验正则化3D预测。使用统一的骨干网络和跨分支信息交换。
- Result: 在多个实验中，FantasyWorld在多视角一致性和风格一致性方面优于最近的几何一致性基线方法。
- Conclusion: 该框架有效桥接了视频想象力与3D感知，产生的几何分支潜在表示可作为下游3D任务的通用表示，无需逐场景优化或微调。


### [44] [MORPH: Shape-agnostic PDE Foundation Models](https://arxiv.org/abs/2509.21670)
*Mahindra Singh Rautela,Alexander Most,Siddharth Mansingh,Bradley C. Love,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.CV

TL;DR: MORPH是一个形状无关的自回归基础模型，用于处理偏微分方程。它能处理不同维度、分辨率和物理场的异构时空数据，通过组件卷积、跨场注意力等机制实现高效学习。

- Motivation: 科学观测数据通常具有异构性和多模态特性，现有模型难以统一处理不同维度、分辨率和物理场的PDE数据。需要开发一个灵活的基础模型来应对这些挑战。
- Method: 基于卷积视觉Transformer架构，结合组件卷积处理标量和矢量通道，跨场注意力建模不同物理场间信息传递，轴向注意力降低计算负担。使用预训练和参数高效微调。
- Result: 在多个下游预测任务中，MORPH在零样本和全样本泛化方面均优于从头训练的模型，匹配或超越了现有基准和最先进模型。
- Conclusion: MORPH为科学机器学习提供了一个灵活强大的基础架构，能够有效处理异构多模态科学观测数据，为可扩展和数据高效的科学机器学习开辟了新路径。


### [45] [MS-YOLO: Infrared Object Detection for Edge Deployment via MobileNetV4 and SlideLoss](https://arxiv.org/abs/2509.21696)
*Jiali Zhang,Thomas S. White,Haoliang Zhang,Wenqing Hu,Donald C. Wunsch II,Jian Liu*

Main category: cs.CV

TL;DR: 提出MS-YOLO模型，通过使用MobileNetV4骨干网络和SlideLoss损失函数，在红外图像目标检测中实现高精度和低计算成本的平衡。

- Motivation: 解决红外成像在低光照和恶劣天气条件下目标检测面临的类别不平衡、热噪声和计算约束等挑战。
- Method: 基于YOLOv8，用MobileNetV4替换CSPDarknet骨干网络降低计算量，并引入SlideLoss损失函数动态强调欠表示和被遮挡样本。
- Result: 在FLIR ADAS V2数据集上，MS-YOLO仅需6.7 GFLOPs计算量，达到了竞争性的mAP和更优的精度。
- Conclusion: MS-YOLO有效解决了保持高检测质量同时最小化计算成本的双重挑战，适合城市环境中的实时边缘部署。


### [46] [Motion-Aware Transformer for Multi-Object Tracking](https://arxiv.org/abs/2509.21715)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: 提出了Motion-Aware Transformer (MATR)，通过显式预测跨帧对象运动来提前更新跟踪查询，减少查询冲突，在多个数据集上实现了最先进的性能。

- Motivation: 现有的DETR-based多目标跟踪框架通常在单个Transformer解码器层中联合处理检测和跟踪查询，导致查询冲突和关联精度下降。
- Method: 引入MATR框架，显式预测对象跨帧运动来提前更新跟踪查询，减少查询冲突，实现更一致的训练。
- Result: 在DanceTrack上HOTA提升9+分，达到71.3（使用补充数据）；SportsMOT上72.2 HOTA；BDD100k上54.7 mTETA和41.6 mHOTA，均达到最先进水平。
- Conclusion: 在端到端Transformer中显式建模运动是推进多目标跟踪的简单而高效的方法。


### [47] [DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining](https://arxiv.org/abs/2509.21719)
*Shuning Sun,Jialang Lu,Xiang Chen,Jichao Wang,Dianjie Lu,Guijuan Zhang,Guangwei Gao,Zhuoran Zheng*

Main category: cs.CV

TL;DR: DeLiVR是一种高效的视频去雨方法，通过将时空李群微分偏置直接注入网络注意力分数中，实现几何一致的对齐和特征聚合。

- Motivation: 解决野外拍摄视频中的雨痕、模糊和噪声问题，以及相机姿态微小变化导致的跨帧不匹配和时间伪影。现有方法依赖光流或启发式对齐，计算成本高且鲁棒性差。
- Method: 提出两个互补组件：1) 旋转有界的李相对偏置，通过紧凑预测模块预测每帧的平面内角度，实现几何一致对齐；2) 微分群位移，计算相邻帧间的角度差异来估计速度，结合时间衰减和注意力掩码关注帧间关系。
- Result: 在公开基准测试上的广泛实验结果证明了该方法的有效性。
- Conclusion: 李群为视频建模中的空间和时间一致性提供了原则性的表示方法，DeLiVR通过注入李群微分偏置实现了高效且鲁棒的视频去雨。


### [48] [On the Status of Foundation Models for SAR Imagery](https://arxiv.org/abs/2509.21722)
*Nathan Inkawhich*

Main category: cs.CV

TL;DR: 本文研究基础AI/ML模型在合成孔径雷达(SAR)目标识别任务中的可行性，通过自监督学习微调公开SSL模型，开发了新的SAR基础模型AFRL-DINOv2，在性能上显著超越现有最佳SAR模型SARATR-X。

- Motivation: 受自然图像领域基础模型成功的启发，希望将这项技术应用于SAR领域。基础模型具有需要少量标注数据即可适应下游任务、对分布偏移更鲁棒、特征可迁移性强等优势。
- Method: 首先测试现有视觉基础模型(DINOv2、DINOv3、PE-Core)在SAR数据上的表现，然后使用SAR数据对公开SSL模型进行自监督微调，开发AFRL-DINOv2系列模型。
- Result: AFRL-DINOv2模型在SAR基础模型中达到新的最先进水平，显著优于当前最佳SAR模型SARATR-X。实验分析了不同骨干网络和下游任务适应策略的性能权衡。
- Conclusion: 自监督微调公开SSL模型是构建SAR基础模型的可行路径，虽然取得了积极成果，但SAR基础模型的开发仍有很长的路要走。


### [49] [UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments](https://arxiv.org/abs/2509.21733)
*Jiannan Xiang,Yun Zhu,Lei Shu,Maria Wang,Lijun Yu,Gabriel Barcik,James Lyon,Srinivas Sunkara,Jindong Chen*

Main category: cs.CV

TL;DR: UISim是一个基于图像的UI模拟器，通过两阶段方法从屏幕图像预测和合成新的UI状态，实现移动界面的动态交互模拟。

- Motivation: 现有方法依赖物理设备或静态截图分析，难以实现可扩展的UI测试和智能UI代理开发。
- Method: 采用两阶段方法：给定初始屏幕图像和用户操作，先预测下一个UI状态的抽象布局，然后基于预测布局合成视觉一致的新图像。
- Result: UISim在生成真实连贯的后续UI状态方面优于端到端UI生成基线方法。
- Conclusion: UISim为UI测试、快速原型设计和合成数据生成提供了实用价值，并为AI代理的UI导航任务规划铺平了道路。


### [50] [LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.21738)
*Mehwish Mehmood,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: 提出了LFA-Net，一种轻量级视网膜血管分割网络，结合了新的注意力模块LiteFusion-Attention，在计算资源有限的环境中实现高效的小血管分割。

- Motivation: 解决现有深度学习分割模型在小血管分割和计算成本高方面的挑战，特别是在计算资源有限的真实临床环境中。
- Method: 设计了LiteFusion-Attention注意力模块，融合残差学习连接、Vision Mamba启发的动态机制和基于调制的注意力，以轻量级方式高效捕捉局部和全局上下文信息。
- Result: 在DRIVE、STARE和CHASE_DB数据集上表现出色，Dice分数分别为83.28%、87.44%、84.50%，Jaccard指数分别为72.85%、79.31%、74.70%，模型仅需0.11M参数、0.42MB内存和4.46GFLOPs。
- Conclusion: LFA-Net在保持高性能的同时实现了轻量化，特别适合资源受限的环境，为视网膜血管分割提供了有效的解决方案。


### [51] [Incorporating Scene Context and Semantic Labels for Enhanced Group-level Emotion Recognition](https://arxiv.org/abs/2509.21747)
*Qing Zhu,Wangdong Guo,Qirong Mao,Xiaohua Huang,Xiuyan Shao,Wenming Zheng*

Main category: cs.CV

TL;DR: 提出了一种结合视觉场景上下文和标签引导语义信息的群体情绪识别新框架，通过多尺度场景编码和情感语义编码提升性能

- Motivation: 现有方法低估了视觉场景上下文信息在建模个体关系中的重要性，同时忽视了情感标签语义信息对完整理解情绪的关键作用
- Method: 包含视觉上下文编码模块（利用多尺度场景信息编码个体关系）和情感语义编码模块（使用LLM生成情感词典，通过结构化情感树构建语义表示），最后通过相似性感知交互整合视觉和语义信息
- Result: 在三个广泛使用的GER数据集上实验表明，该方法相比最先进方法取得了有竞争力的性能
- Conclusion: 提出的结合视觉场景上下文和标签引导语义信息的框架有效提升了群体情绪识别性能


### [52] [KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields](https://arxiv.org/abs/2509.21750)
*Yu Li,Da Chang,Xi Xiao*

Main category: cs.CV

TL;DR: KG-SAM是一个知识引导的医学图像分割框架，通过整合解剖学先验知识、边界优化和不确定性估计，解决了SAM模型在医学图像分割中的局限性。

- Motivation: SAM模型直接应用于医学图像分割存在边界模糊、解剖关系建模不足和缺乏不确定性量化等挑战。
- Method: KG-SAM包含三个核心组件：(1)医学知识图谱编码细粒度解剖关系；(2)基于能量的条件随机场确保解剖一致性预测；(3)不确定性感知融合模块提升临床可靠性。
- Result: 在多中心医学数据集上的实验表明，KG-SAM在前列腺分割中达到82.69%的平均Dice分数，在腹部分割中分别达到78.05%(MRI)和79.68%(CT)。
- Conclusion: KG-SAM被证明是一个稳健且可推广的医学图像分割框架，显著提升了分割性能。


### [53] [UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models](https://arxiv.org/abs/2509.21760)
*Lan Chen,Yuchao Gu,Qi Mao*

Main category: cs.CV

TL;DR: UniVid框架通过微调预训练视频生成模型来处理多种视觉任务，无需任务特定修改，展示了视频生成模型作为统一视觉基础模型的潜力。

- Motivation: 现有的大视觉模型需要跨模态和跨源的任务特定预训练，成本高且难以扩展到未见任务。受预训练视频生成模型已捕获时序依赖的启发，探索能否将其适配到多样图像和视频任务。
- Method: 提出UniVid框架，微调视频扩散变换器，将任务表示为视觉句子，其中上下文序列定义任务和期望输出模态。
- Result: 尽管仅在自然视频数据上训练，UniVid在跨模态推理（图像和视频混合上下文）和跨源任务（自然到标注数据）上都表现出良好泛化能力。
- Conclusion: 预训练视频生成模型有潜力作为可扩展的统一视觉建模基础，理解和生成任务可通过简单反转视觉句子顺序轻松切换。


### [54] [CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones](https://arxiv.org/abs/2509.21764)
*Wenyi Gong,Mieszko Lis*

Main category: cs.CV

TL;DR: 提出了一种保持空间完整性的token合并方法，解决了现代ViT架构中空间设计带来的token缩减挑战

- Motivation: 现代ViT架构采用空间设计（如窗口注意力、相对位置编码），现有token缩减方法无法保持这些架构依赖的空间结构
- Method: 使用2D缩减策略保持结构化token布局、空间感知合并算法维持相对位置、新颖的按维度最大幅值token表示保留显著特征
- Result: 在SAM-H上实现1.25倍加速且仅0.7% mIOU下降，在DeiT-B上实现1.15倍加速且无精度损失
- Conclusion: 该方法在空间和非空间架构上均取得最优性能，兼容性强且效果显著


### [55] [Training-Free Multimodal Deepfake Detection via Graph Reasoning](https://arxiv.org/abs/2509.21774)
*Yuxin Liu,Fei Wang,Kun Li,Yiqi Nie,Junjie Chen,Yanyan Wei,Zhangling Duan,Zhaohong Jia*

Main category: cs.CV

TL;DR: 提出GASP-ICL框架，无需训练即可增强大型视觉语言模型在多模态深度伪造检测中的性能，通过任务对齐检索和自适应评分器解决跨模态不一致性问题。

- Motivation: 现有大型视觉语言模型在多模态深度伪造检测中存在局限性，难以捕捉细微伪造线索、解决跨模态不一致性以及执行任务对齐检索。
- Method: 采用GASP-ICL框架，包含MDD适配的特征提取器检索对齐的图像-文本对，设计图结构泰勒自适应评分器(GSTAS)捕获跨样本关系并传播查询对齐信号，选择语义对齐的任务相关演示。
- Result: 在四种伪造类型上的实验表明，GASP-ICL超越了强基线方法，在不进行LVLM微调的情况下实现了性能提升。
- Conclusion: GASP-ICL是一个有效的训练免费框架，能够增强大型视觉语言模型在多模态深度伪造检测中的鲁棒性。


### [56] [Prompt-guided Representation Disentanglement for Action Recognition](https://arxiv.org/abs/2509.21783)
*Tianci Wu,Guangming Zhu,Jiang Lu,Siyuan Wang,Ning Wang,Nuoye Xiong,Zhang Liang*

Main category: cs.CV

TL;DR: 提出ProDA框架，通过动态提示模块和时空场景图，从多动作场景中解耦指定动作进行识别

- Motivation: 现有方法提取统一特征处理视频中所有动作，难以在多动作场景中建模不同对象间的交互
- Method: 使用时空场景图和动态提示模块引导图解析神经网络生成动作特定表示，设计视频适配的GPNN进行动态权重信息聚合
- Result: 在视频动作识别实验中证明了方法的有效性，优于现有最先进方法
- Conclusion: ProDA框架能够有效解耦复杂场景中的指定动作，提升多动作场景下的识别性能


### [57] [DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images](https://arxiv.org/abs/2509.21787)
*Dwip Dalal,Gautam Vashishtha,Anku Ranui,Aishwarya Reganti,Parth Patwa,Mohd Sarique,Chandan Gupta,Keshav Nath,Viswanatha Reddy,Vinija Jain,Aman Chadha,Amitava Das,Amit Sheth,Asif Ekbal*

Main category: cs.CV

TL;DR: 提出了一种用于识别数字内容中仇恨的多模态数据集，结合水印稳定扩散技术和数字注意力分析模块生成仇恨注意力图，并开发了DeHater视觉语言模型进行多模态去仇恨化任务。

- Motivation: 有害在线内容的增加扭曲了公共话语，对维护健康数字环境构成重大挑战，需要开发有效的仇恨内容检测和去除方法。
- Method: 使用水印稳定扩散技术结合数字注意力分析模块(DAAM)来定位图像中的仇恨元素，生成仇恨注意力图，并通过模糊这些区域来去除仇恨内容。
- Result: 创建了专门用于仇恨检测的多模态数据集，开发了DeHater视觉语言模型，为AI驱动的图像仇恨检测设立了新标准。
- Conclusion: 该方法为社交媒体中更符合伦理的AI应用开发做出了贡献，推动了数字环境中仇恨内容的有效识别和去除。


### [58] [MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning](https://arxiv.org/abs/2509.21788)
*Lihao Zheng,Jiawei Chen,Xintian Shen,Hao Ma,Tao Wei*

Main category: cs.CV

TL;DR: 提出MIRG-RL框架，通过两阶段训练（监督微调+图像感知强化学习）解决多图像推理和定位问题，在跨图像推理任务上达到64.82%的SOTA性能。

- Motivation: 当前大型视觉语言模型在多图像推理方面存在两个关键挑战：缺乏跨图像推理能力和不足的跨图像参考奖励建模。
- Method: 两阶段训练范式：结合带标注轨迹的监督微调和图像感知强化学习优化；创新性地提出轨迹数据构建方法，整合对象级和图像级标注信息；设计具有对象和图像双重奖励函数的图像感知RL策略。
- Result: 在多图像定位基准测试中达到最先进性能，跨图像推理任务上获得64.82%的准确率，比之前最佳方法提升1%。
- Conclusion: MIRG-RL框架有效解决了多图像推理和定位问题，提出的训练方法和数据集构建策略显著提升了模型的跨图像推理能力。


### [59] [LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE](https://arxiv.org/abs/2509.21790)
*Yu Shang,Lei Jin,Yiding Ma,Xin Zhang,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: 提出LongScape混合框架，结合扩散去噪和自回归生成，通过动作引导的分块机制实现稳定的长时程视频生成

- Motivation: 解决当前视频生成方法在长时程生成中的问题：扩散方法存在时间不一致性和视觉漂移，自回归方法牺牲视觉细节
- Method: 采用混合框架，结合帧内扩散去噪和帧间自回归因果生成，引入动作引导的变长分块机制和上下文感知专家混合框架
- Result: 实验结果表明该方法在长时程生成中实现了稳定和一致的表现
- Conclusion: LongScape框架有效解决了长时程视频生成的挑战，为高质量具身操作数据生成提供了可行方案


### [60] [MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation](https://arxiv.org/abs/2509.21797)
*Yu Shang,Yangcheng Yu,Xin Zhang,Xin Jin,Haisheng Su,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: 提出MoWM框架，通过融合潜在世界模型和像素空间模型的混合表示来解决具身动作规划问题，在CALVIN基准测试中达到最先进性能。

- Motivation: 现有视频生成世界模型存在视觉冗余问题，而潜在世界模型又忽略了精细细节，需要一种能同时利用运动感知表示和精细视觉特征的方法。
- Method: 使用混合世界模型框架，将潜在模型的运动感知表示作为高层先验，指导从像素空间模型提取精细视觉特征，突出对动作解码有用的视觉细节。
- Result: 在CALVIN基准测试中实现了最先进的任务成功率，并展现出优越的泛化能力。
- Conclusion: MoWM框架有效融合了不同特征空间的优势，为具身规划研究提供了有价值的见解。


### [61] [DiTraj: training-free trajectory control for video diffusion transformer](https://arxiv.org/abs/2509.21839)
*Cheng Lei,Jiayu Zhang,Yue Ma,Xinyu Wang,Long Chen,Liang Tang,Yiqiang Yan,Fei Su,Zhicheng Zhao*

Main category: cs.CV

TL;DR: DiTraj是一个无需训练的轨迹控制框架，专门为DiT视频生成模型设计，通过前景-背景分离引导和空间-时间解耦3D-RoPE实现精确的轨迹控制。

- Motivation: 现有的轨迹控制方法要么需要大量训练资源，要么专门为U-Net设计，无法充分利用DiT的优越性能。
- Method: 使用LLM将用户提示转换为前景和背景提示，分别指导视频生成；提出STD-RoPE通过修改前景token的位置嵌入来消除跨帧空间差异，增强轨迹控制。
- Result: 实验表明该方法在视频质量和轨迹可控性方面优于先前方法。
- Conclusion: DiTraj为DiT视频生成模型提供了一种简单有效的轨迹控制解决方案，无需额外训练即可实现精确控制。


### [62] [A Comprehensive Evaluation of Transformer-Based Question Answering Models and RAG-Enhanced Design](https://arxiv.org/abs/2509.21845)
*Zichen Zhang,Kunlong Zhang,Hongwei Ruan,Yiming Luo*

Main category: cs.CV

TL;DR: 本文评估了多跳问答中的检索策略，提出混合检索方法（结合稠密嵌入、词汇重叠和重排序）在HotpotQA数据集上显著优于基线方法，相对提升50%的精确匹配和47%的F1分数。

- Motivation: Transformer模型在问答领域取得进展，但多跳推理（需要结合多个段落证据）仍然困难，需要改进检索策略来支持多跳问答。
- Method: 在检索增强生成框架中比较余弦相似度、最大边际相关性和混合方法；采用EfficientRAG管道进行查询优化，引入token标记和迭代优化；在HotpotQA数据集上进行实验。
- Result: 混合方法显著优于基线，相对余弦相似度在精确匹配和F1分数上分别提升50%和47%；错误分析显示混合检索提高了实体召回率和证据互补性，但在处理干扰项和时间推理方面仍有局限。
- Conclusion: 混合检索增强生成为多跳问答提供了实用的零样本解决方案，在准确性、效率和可解释性之间取得了平衡。


### [63] [Dynamic Novel View Synthesis in High Dynamic Range](https://arxiv.org/abs/2509.21853)
*Kaixuan Zhang,Zhipeng Xiong,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出HDR动态新视角合成(HDR DNVS)问题，并开发HDR-4DGS方法，通过动态色调映射模块在动态场景中实现高动态范围渲染。

- Motivation: 现有HDR新视角合成方法主要关注静态场景，但现实世界常包含动态元素如移动物体和变化光照，需要同时建模时间辐射变化和LDR-HDR转换。
- Method: 基于高斯泼溅的HDR-4DGS架构，包含创新的动态色调映射模块，根据时间维度上辐射分布的变化动态调整色调映射函数。
- Result: HDR-4DGS在定量性能和视觉保真度上均优于现有最先进方法，实现了时间辐射一致性和空间准确的颜色转换。
- Conclusion: 该方法能够从任意视角和时间点生成逼真的HDR渲染，解决了动态场景中HDR新视角合成的挑战。


### [64] [SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes](https://arxiv.org/abs/2509.21859)
*Minje Kim,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: SRHand是一种从低分辨率图像重建详细3D手部几何和纹理的方法，结合隐式图像表示和显式手部网格，通过几何感知隐式图像函数实现多视角一致的高质量超分辨重建。

- Motivation: 现有方法依赖高分辨率多视角图像，在低分辨率图像上泛化能力差；传统超分辨方法仅适用于静态场景，不适用于可变形的手部重建。
- Method: 提出几何感知隐式图像函数(GIIF)，联合优化隐式图像函数和显式3D手部形状，通过上采样粗输入图像学习详细手部先验。
- Result: 在InterHand2.6M和Goliath数据集上，显著优于现有图像超分辨和3D手部重建方法，能够重建皱纹、指甲等精细细节。
- Conclusion: SRHand通过结合隐式和显式表示，实现了从低分辨率图像重建高质量3D手部几何和纹理，保持多视角和姿态一致性。


### [65] [Deepfakes: we need to re-think the concept of "real" images](https://arxiv.org/abs/2509.21864)
*Janis Keuper,Margret Keuper*

Main category: cs.CV

TL;DR: 论文指出当前假图像检测研究过于关注生成算法和"假"样本，忽视了"真实"图像的明确定义和数据收集，认为需要重新思考"真实"图像的概念。

- Motivation: 现代图像生成模型的普及引发了对其负面社会影响的担忧，但当前研究过度依赖少数老旧低分辨率数据集来定义"真实"图像，而现代智能手机摄影技术已发生革命性变化。
- Method: 通过分析当前假图像检测方法的局限性，指出这些方法依赖的"真实"图像数据集（如ImageNet）已无法反映现代图像获取技术的现实情况。
- Result: 发现当前假图像检测研究存在根本性缺陷，因为现代智能手机摄影同样使用神经网络算法处理多传感器输入，与"假"图像生成器技术相似。
- Conclusion: 需要重新定义"真实"图像的技术概念，建立新的基准数据集，并讨论假图像检测本身是否是一个合理的研究目标。


### [66] [Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization](https://arxiv.org/abs/2509.21871)
*Boyang Liu,Yifan Hu,Senjie Jin,Shihan Dou,Gonglei Shi,Jie Shao,Tao Gui,Xuanjing Huang*

Main category: cs.CV

TL;DR: Aes-R1是一个用于图像美学评估的多模态大语言模型框架，通过强化学习结合链式思维推理数据，提升美学评分准确性和解释能力。

- Motivation: 多模态大语言模型在图像美学评估中存在美学推理数据稀缺和美学判断主观性强的问题，难以生成准确且可解释的美学判断。
- Method: 提出Aes-R1框架：1) 使用AesCoT管道构建高质量链式思维美学推理数据；2) 采用RAPO强化学习算法联合优化绝对分数回归和相对排序；3) 生成结构化解释和评分。
- Result: 实验表明Aes-R1将骨干模型的PLCC/SRCC平均提升47.9%/34.8%，超越同类最优基线，在有限监督和分布外场景下具有鲁棒泛化能力。
- Conclusion: Aes-R1能够在统一框架中增强美学评分和推理能力，使模型生成有依据的解释和可靠评分。


### [67] [StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing](https://arxiv.org/abs/2509.21887)
*Liyang Chen,Tianze Zhou,Xu He,Boshi Tang,Zhiyong Wu,Yang Huang,Yang Wu,Zhongqian Sun,Wei Yang,Helen Meng*

Main category: cs.CV

TL;DR: StableDub是一个基于Stable-Diffusion的视觉配音框架，通过唇部习惯建模和遮挡鲁棒合成，解决了现有方法在唇部习惯相似性和遮挡处理方面的不足。

- Motivation: 现有视觉配音方法存在两个主要问题：(1)仅依赖音频驱动无法捕捉说话者特定的唇部习惯；(2)传统盲修复方法在处理遮挡时容易产生视觉伪影。
- Method: 基于Stable-Diffusion构建唇部习惯调制机制，联合建模音视频同步和说话者口面部动态；引入遮挡感知训练策略；采用混合Mamba-Transformer架构提升训练效率。
- Result: 实验表明StableDub在唇部习惯相似性和遮挡鲁棒性方面表现优异，在音频-唇部同步、视频质量和分辨率一致性方面超越其他方法。
- Conclusion: 该方法从多方面扩展了视觉配音方法的适用性，无需昂贵先验知识，在计算密集型扩散模型上展现出优越的训练效率。


### [68] [Drag4D: Align Your Motion with Text-Driven 3D Scene Generation](https://arxiv.org/abs/2509.21888)
*Minjun Kang,Inkyu Shin,Taeyeop Lee,In So Kweon,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: Drag4D是一个交互式框架，通过三阶段流程实现文本驱动的3D场景生成和物体运动控制，支持用户定义3D轨迹来动画化物体。

- Motivation: 为了解决在文本驱动的3D场景生成中集成物体运动控制的问题，让用户能够交互式地定义物体在3D空间中的运动轨迹。
- Method: 三阶段流程：1) 使用2D高斯泼溅和全景图像增强文本到3D背景生成；2) 采用3D复制粘贴方法将物体网格集成到场景中，并通过物理感知的位置学习进行空间对齐；3) 使用部分增强的运动条件视频扩散模型进行时间动画化。
- Result: 实现了高质量3D背景中用户控制物体运动的协调对齐，有效减轻了运动幻觉问题，确保了视图一致的时间对齐。
- Conclusion: Drag4D提供了一个统一的架构，成功实现了在高质量3D背景中用户控制物体运动的协调集成，展示了在3D场景生成和动画化方面的有效性。


### [69] [Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers](https://arxiv.org/abs/2509.21893)
*Jibin Song,Mingi Kwon,Jaeseok Jeong,Youngjung Uh*

Main category: cs.CV

TL;DR: Syncphony是一个音频到视频生成模型，通过运动感知损失和音频同步引导机制，实现了与音频精确同步的高质量视频生成。

- Motivation: 现有的文本到视频和图像到视频生成方法在控制运动时间方面存在局限，而音频提供了与视频运动对齐的时间线索，是时间控制视频生成的有前景条件。
- Method: 基于预训练视频骨干网络，引入两个关键组件：运动感知损失（强调高运动区域学习）和音频同步引导（使用视觉对齐的无音频层模型引导全模型）。
- Result: 在AVSync15和The Greatest Hits数据集上的实验表明，Syncphony在同步精度和视觉质量方面均优于现有方法。
- Conclusion: Syncphony通过创新的同步机制成功解决了音频到视频生成中的精细同步问题，实现了与音频输入精确对齐的高质量视频生成。


### [70] [LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation](https://arxiv.org/abs/2509.21894)
*Yixiao Liu,Yizhou Yang,Jinwen Li,Jun Tao,Ruoyu Li,Xiangkun Wang,Min Zhu,Junlong Cheng*

Main category: cs.CV

TL;DR: 提出了一种新颖的语言引导变化检测模型(LG-CD)，利用自然语言提示来指导网络关注感兴趣区域，显著提高了变化检测的准确性和鲁棒性。

- Motivation: 当前大多数基于深度学习的方法主要关注学习单模态视觉信息，而忽视了文本等多模态数据提供的丰富语义信息。
- Method: 使用视觉基础模型(SAM2)作为特征提取器获取多尺度金字塔特征，采用多层适配器微调模型，设计文本融合注意力模块(TFAM)对齐视觉和文本信息，并通过视觉-语义融合解码器(V-SFD)深度融合信息生成变化检测掩码。
- Result: 在三个数据集(LEVIR-CD、WHU-CD和SYSU-CD)上的实验表明，LG-CD始终优于最先进的变化检测方法。
- Conclusion: 该方法通过利用多模态信息为实现广义变化检测提供了新的思路。


### [71] [TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation](https://arxiv.org/abs/2509.21905)
*Qihang Wang,Yaxiong Wang,Lechao Cheng,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出了一个统一的扩散框架，结合文本和拖拽交互进行图像编辑，解决了现有方法在空间控制和纹理操作方面的互补局限性。

- Motivation: 现有的文本驱动方法擅长纹理操作但缺乏精确空间控制，而拖拽驱动方法主要修改形状结构但缺乏细粒度纹理指导。需要结合两者的优势。
- Method: 提出两个关键创新：1）点云确定性拖拽，通过3D特征映射增强潜在空间布局控制；2）拖拽-文本引导去噪，在去噪过程中动态平衡拖拽和文本条件的影响。
- Result: 方法在联合编辑中实现高保真度，并且在纯文本、纯拖拽或组合条件下都保持强大性能，甚至超越专门的单模式方法。
- Conclusion: 建立了一个多功能且可推广的可控图像操作解决方案，支持灵活的编辑模式。


### [72] [Enhancing Vehicle Detection under Adverse Weather Conditions with Contrastive Learning](https://arxiv.org/abs/2509.21916)
*Boying Li,Chang Liu,Petter Kyösti,Mattias Öhman,Devashish Singha Roy,Sofia Plazzi,Hamam Mokayed,Olle Hagner*

Main category: cs.CV

TL;DR: 提出了一种侧载对比学习适应框架，利用无标注数据通过对比学习训练CNN表示提取器，然后将其侧载到冻结的YOLO11n主干网络中，以改善北欧地区雪域环境下无人机图像的车辆检测性能。

- Motivation: 北欧地区无人机图像中的车辆检测面临强可见性挑战和由不同雪覆盖水平引起的领域偏移问题。虽然标注数据昂贵，但无标注数据通过简单飞行无人机即可廉价获取。
- Method: 提出侧载对比学习适应框架：在预训练阶段通过对比学习在无标注数据上训练CNN表示提取器，在微调阶段将其侧载到冻结的YOLO11n主干网络中。通过广泛实验比较不同融合方法和粒度来找到鲁棒的侧载对比学习适应方案。
- Result: 在NVD数据集上，提出的侧载对比学习适应模型在mAP50指标上将检测性能提高了3.8%到9.5%。
- Conclusion: 该方法有效利用廉价的无标注数据，通过对比学习和侧载适应策略显著提升了北欧雪域环境下车辆检测的性能，为解决领域偏移和可见性挑战提供了有效解决方案。


### [73] [Taming Flow-based I2V Models for Creative Video Editing](https://arxiv.org/abs/2509.21917)
*Xianghao Kong,Hansheng Chen,Yuwei Guo,Lvmin Zhang,Gordon Wetzstein,Maneesh Agrawala,Anyi Rao*

Main category: cs.CV

TL;DR: 提出IF-V2V方法，无需反演即可将现成的基于流匹配的图像到视频模型用于视频编辑，通过向量场校正和结构运动保持初始化实现高效编辑。

- Motivation: 现有视频编辑方法要么需要模型特定的反演设计，要么需要大量优化，难以利用最新的图像到视频模型将图像编辑能力迁移到视频领域。
- Method: 使用向量场校正与样本偏差将源视频信息融入去噪过程，采用结构运动保持初始化生成运动感知的时间相关噪声，并引入偏差缓存机制减少计算开销。
- Result: 相比现有方法，实现了更优的编辑质量和一致性，提供轻量级的即插即用解决方案。
- Conclusion: IF-V2V方法无需反演即可有效利用现成I2V模型进行视频编辑，在保持计算效率的同时提供高质量的编辑效果。


### [74] [Multi-View Crowd Counting With Self-Supervised Learning](https://arxiv.org/abs/2509.21918)
*Hong Mo,Xiong Zhang,Tengfei Shi,Zhongbo Wu*

Main category: cs.CV

TL;DR: SSLCounter是一个用于多视角计数(MVC)的自监督学习框架，通过神经体积渲染减少对大规模标注数据的依赖，在仅使用70%训练数据时仍能取得竞争性性能。

- Motivation: 传统多视角计数方法大多基于全监督学习，需要大量标注数据。本文旨在通过自监督学习减轻对标注数据的依赖。
- Method: 提出SSLCounter框架，利用神经体积渲染学习场景的隐式表示，通过可微分神经渲染重建连续几何形状和复杂视角相关外观。
- Result: 在多个MVC基准测试中达到最先进性能，仅使用70%训练数据时仍能保持竞争性表现，展示了优越的数据效率。
- Conclusion: SSLCounter通过自监督学习有效解决了多视角计数对标注数据的依赖问题，具有出色的数据效率和可集成性。


### [75] [Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding](https://arxiv.org/abs/2509.21922)
*Vahid Mirjalili,Ramin Giahi,Sriram Kollipara,Akshay Kekuda,Kehui Yao,Kai Zhao,Jianpeng Xu,Kaushiki Nag,Sinduja Subramaniam,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.CV

TL;DR: 提出了一个针对基础模型的对象中心空间推理基准，评估了视觉模型和视觉语言模型在空间定位、空间推理和下游检索任务中的表现，发现检测器和VLM之间存在稳定权衡。

- Motivation: 当前大多数基准测试强调定位精度，但缺乏对场景中物体排列和关系的理解评估，而有效的场景理解需要识别物体并推理它们的相对位置、分组和深度。
- Method: 使用受控合成数据集，评估了最先进的视觉模型（如GroundingDINO、Florence-2、OWLv2）和大型视觉语言模型（如InternVL、LLaVA、GPT-4o）在三个任务上的表现：空间定位、空间推理和下游检索任务。
- Result: 检测器如GroundingDINO和OWLv2提供精确的边界框但关系推理有限，而VLM如SmolVLM和GPT-4o提供粗略布局线索和流畅描述但在细粒度空间上下文方面表现不佳。
- Conclusion: 研究揭示了定位与真正空间理解之间的差距，并指出社区需要开发具有空间感知能力的基础模型。


### [76] [PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning](https://arxiv.org/abs/2509.21926)
*Jiahao Zhang,Bowen Wang,Hong Liu,Yuta Nakashima,Hajime Nagahara*

Main category: cs.CV

TL;DR: PANICL是一个无需训练的训练框架，通过利用多个上下文对来缓解视觉上下文学习中对单个上下文对的过度依赖问题，从而提高预测的稳定性和减少偏差。

- Motivation: 视觉上下文学习(VICL)通常过度依赖单个上下文对，这会导致有偏差和不稳定的预测。
- Method: 提出了基于补丁的k近邻视觉上下文学习(PANICL)，通过平滑多个上下文对之间的分配分数来减少偏差，无需额外训练。
- Result: 在多种视觉任务上的实验表明，PANICL相比强基线方法有持续改进，并对领域偏移表现出强大的鲁棒性。
- Conclusion: PANICL是一个通用且广泛适用的框架，能够提升VICL模型的性能并增强其对领域偏移的鲁棒性。


### [77] [SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference](https://arxiv.org/abs/2509.21927)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: SingRef6D是一个轻量级的6D姿态估计方法，仅需单张RGB图像作为参考，无需深度传感器或多视角图像。通过改进深度预测和深度感知匹配，在挑战性表面和光照条件下表现优异。

- Motivation: 现有6D姿态估计方法存在局限性：基于深度传感器的方法在透明或高反光表面失效，RGB方法在低光和无纹理场景中匹配性能不足。需要一种不依赖深度传感器且能处理挑战性条件的解决方案。
- Method: 1. 在Depth-Anything v2基础上提出基于token-scaler的微调机制和新优化损失，提升挑战性表面的深度预测精度；2. 引入深度感知匹配过程，在LoFTR中有效整合空间关系，处理挑战性材料和光照条件的匹配。
- Result: 1. 深度预测在REAL275上相比Depth-Anything v2（微调头）提升14.41%（δ1.05指标）；2. 在REAL275、ClearPose和Toyota-Light数据集上的姿态估计评估显示，平均召回率比最先进方法提升6.1%。
- Conclusion: SingRef6D提供了一个轻量级但强大的6D姿态估计解决方案，在资源受限环境下仍能保持鲁棒性，特别适用于挑战性表面条件和光照环境。


### [78] [DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation](https://arxiv.org/abs/2509.21930)
*Jiahui Wang,Changhao Chen*

Main category: cs.CV

TL;DR: DynaNav是一个动态视觉导航框架，通过基于场景复杂度自适应选择特征和层来降低计算开销，同时提高可解释性和导航性能。

- Motivation: 现有基础模型（特别是transformer解码器）存在计算开销高和缺乏可解释性的问题，限制了在资源受限场景中的部署。
- Method: 提出可训练的硬特征选择器进行稀疏操作，并将特征选择集成到提前退出机制中，使用贝叶斯优化确定最优退出阈值。
- Result: 相比ViNT，DynaNav实现了2.26倍FLOPs减少、42.3%推理时间降低和32.8%内存使用减少，在四个公共数据集上提高了导航性能。
- Conclusion: DynaNav框架有效解决了视觉导航中的计算效率和可解释性问题，为资源受限场景提供了可行的解决方案。


### [79] [SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet](https://arxiv.org/abs/2509.21938)
*Woosung Joung,Daewon Chae,Jinkyu Kim*

Main category: cs.CV

TL;DR: 提出SemanticControl方法，解决ControlNet在视觉条件与文本提示不对齐时的生成问题，通过自适应抑制冲突区域并增强文本引导。

- Motivation: ControlNet依赖与文本提示精确对齐的视觉条件，但在实际应用中常遇到条件不对齐的情况，导致生成质量下降。
- Method: 使用与视觉条件对齐的替代提示进行辅助去噪，提取注意力掩码，然后在目标提示去噪过程中利用这些掩码。
- Result: 实验表明该方法在各种不对齐条件下（深度图、边缘图、人体骨架）均优于现有基线方法。
- Conclusion: SemanticControl能有效利用语义相关但不对齐的视觉条件，提升生成质量。


### [80] [Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach](https://arxiv.org/abs/2509.21950)
*Daiqing Wu,Dongbao Yang,Sicheng Zhao,Can Ma,Yu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉情感评估框架MVEI，通过情感陈述判断任务和自动化流程来评估多模态大语言模型的情感感知能力，发现现有模型在情感解释和基于情境的情感判断方面表现较好，但在理解感知主观性方面存在局限。

- Motivation: 现有评估方法存在忽略合理响应、情感分类有限、忽视情境因素和标注劳动密集等问题，导致多模态大语言模型在零样本情感感知能力评估结果不一致。
- Method: 提出情感陈述判断任务，并设计自动化流程高效构建以情感为中心的陈述，最小化人工标注工作量。
- Result: 评估显示主流MLLMs在情感解释和情境情感判断方面表现较强，但在理解感知主观性方面相对受限；与人类相比，即使是GPT4o等顶级模型也存在显著性能差距。
- Conclusion: 通过开发基础评估框架和全面评估MLLMs，本研究有助于推进多模态大语言模型的情感智能发展。


### [81] [MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning](https://arxiv.org/abs/2509.21953)
*Tao Wu,Yibo Jiang,Yehao Lu,Zhizhong Wang,Zeyi Huang,Zequn Qin,Xi Li*

Main category: cs.CV

TL;DR: MultiCrafter是一个多主体图像生成框架，通过位置监督、专家混合架构和在线强化学习，解决现有方法中主体保真度不足和人类偏好对齐的问题。

- Motivation: 现有基于上下文学习的多主体图像生成方法存在严重属性泄漏问题，损害主体保真度，且无法与人类偏好对齐。
- Method: 1. 引入显式位置监督分离不同主体的注意力区域；2. 使用专家混合架构增强模型能力；3. 设计在线强化学习框架进行人类偏好对齐。
- Result: 实验验证该框架显著提高了主体保真度，并更好地与人类偏好对齐。
- Conclusion: MultiCrafter通过位置监督、MoE架构和强化学习，有效解决了多主体图像生成中的保真度和偏好对齐问题。


### [82] [PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data](https://arxiv.org/abs/2509.21965)
*Zhe Zhu,Le Wan,Rui Xu,Yiheng Zhang,Honghua Chen,Zhiyang Dou,Cheng Lin,Yuan Liu,Mingqiang Wei*

Main category: cs.CV

TL;DR: PartSAM是首个基于大规模3D数据原生训练的可提示部件分割模型，采用三平面双分支编码器架构，在多个基准测试中大幅超越现有方法。

- Motivation: 现有开放世界部件分割方法通常从2D基础模型转移监督，无法捕捉内在几何特征，导致表面理解受限、分解不可控和泛化能力有限。
- Method: 采用编码器-解码器架构，使用三平面双分支编码器生成空间结构化token；引入模型在环标注流水线，从在线资源中整理超过500万个3D形状-部件对。
- Result: PartSAM在多个基准测试中大幅超越最先进方法，实现了高度准确的部件识别，并能自动将形状分解为表面和内部结构。
- Conclusion: PartSAM标志着向3D部件理解基础模型迈出的决定性一步，其可扩展架构和多样化3D数据结合产生了新兴的开放世界能力。


### [83] [No-Reference Image Contrast Assessment with Customized EfficientNet-B0](https://arxiv.org/abs/2509.21967)
*Javad Hassannataj Joloudari,Bita Mesbahzadeh,Omid Zare,Emrah Arslan,Roohallah Alizadehsani,Hossein Moosaei*

Main category: cs.CV

TL;DR: 提出基于深度学习的无参考图像对比度质量评估框架，通过定制化预训练网络实现高效准确的对比度失真评估。

- Motivation: 现有无参考图像质量评估模型难以准确评估真实场景下的对比度失真，需要开发专门针对对比度质量评估的深度学习方法。
- Method: 定制化微调三种预训练架构（EfficientNet B0、ResNet18、MobileNetV2），添加对比度感知回归头，使用Siamese网络，在CID2013和CCID2014数据集上进行端到端训练。
- Result: 定制化EfficientNet B0模型在CCID2014上达到PLCC=0.9286、SRCC=0.9178，在CID2013上达到PLCC=0.9581、SRCC=0.9369，性能优于传统方法和深度基线模型。
- Conclusion: 对比度感知的轻量级预训练网络适配能够为无参考对比度质量评估提供高性能、可扩展的解决方案，适用于实时和资源受限应用。


### [84] [Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning](https://arxiv.org/abs/2509.21976)
*Zilun Zhang,Zian Guan,Tiancheng Zhao,Haozhan Shen,Tianyu Li,Yuxiang Cai,Zhonggen Su,Zhaojun Liu,Jianwei Yin,Xiang Li*

Main category: cs.CV

TL;DR: Geo-R1提出了一种面向少样本地理空间指代表达的推理中心强化微调范式，通过先生成显式可解释的推理链来分解指代表达，然后利用这些推理结果定位目标对象，显著提升了在数据稀缺场景下的泛化能力。

- Motivation: 解决遥感图像中指代表达理解在数据稀缺场景下泛化能力差的问题，传统监督微调方法需要大量标注数据，在少样本情况下表现不佳。
- Method: 采用推理中心强化微调(RFT)范式，强制模型先生成显式可解释的推理链来分解指代表达，然后基于这些推理结果定位目标对象，实现"先推理后行动"的过程。
- Result: 在三个精心设计的少样本地理空间指代表达基准测试中，Geo-R1模型持续且显著优于监督微调基线，并展现出强大的跨数据集泛化能力。
- Conclusion: Geo-R1通过推理中心的方法有效利用了有限的标注数据，提升了模型泛化能力和可解释性，为少样本地理空间指代表达理解提供了有效解决方案。


### [85] [Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models](https://arxiv.org/abs/2509.21979)
*Zikun Guo,Xinyue Xu,Pei Xiang,Shu Yang,Xin Han,Di Wang,Lijie Hu*

Main category: cs.CV

TL;DR: 该研究评估了视觉语言模型在医学问答中的临床顺从性问题，提出了一个医学顺从性基准数据集，并开发了VIPER缓解框架来减少模型对非证据性内容的依赖。

- Motivation: 视觉语言模型在临床工作流程中日益重要，但它们经常表现出顺从行为，优先考虑与用户措辞、社交线索或感知权威的一致性，而非基于证据的推理。
- Method: 构建医学顺从性数据集，使用心理学动机的压力模板进行对抗实验，并提出VIPER缓解策略——通过过滤非证据性内容并生成基于证据的答案。
- Result: 发现各种VLM模型普遍存在脆弱性，表现出显著的对抗响应变化，与模型准确性或大小的相关性较弱。模仿和专家提供的修正被发现是最有效的触发因素。
- Conclusion: 该基准分析和缓解框架为医学VLM在真实世界临床交互中的稳健部署奠定了基础，强调了对基于证据的防御的需求。


### [86] [Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm](https://arxiv.org/abs/2509.21980)
*Zeyu Wang,Baiyu Chen,Kun Yan,Hongjing Piao,Hao Xue,Flora D. Salim,Yuanchun Shi,Yuntao Wang*

Main category: cs.CV

TL;DR: GLARIFY是一种利用时空凝视信息增强视觉语言模型在现实应用中的方法，通过处理模糊的凝视模式和口语问题来解决智能眼镜场景中的多模态查询挑战。

- Motivation: 随着智能眼镜的普及，用户的凝视数据被整合到视觉语言模型中，但凝视数据会引入模糊性挑战：口语问题使用代词或跳过上下文变得模糊，人类凝视模式嘈杂且与口语问题存在复杂时空关系。
- Method: 首先分析数百个带凝视模态的查询样本，利用GPT-4o设计自动数据合成管道生成GLARIFY-Ambi数据集，包含专门的思维链过程处理嘈杂凝视模式，最后设计热图模块将凝视信息整合到先进VLMs中。
- Result: 在保留测试集上的实验表明，GLARIFY显著优于基线方法。
- Conclusion: 通过稳健地将VLMs与人类注意力对齐，GLARIFY为与视觉助手的可用和直观交互范式铺平了道路。


### [87] [From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs](https://arxiv.org/abs/2509.21984)
*Yingjie Zhu,Xuefeng Bai,Kehai Chen,Yang Xiang,Weili Guan,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: 该论文系统研究了大型视觉语言模型的空间偏差问题，发现模型对相同视觉信息在不同位置会产生不一致输出，并提出平衡位置分配机制来提升空间鲁棒性。

- Motivation: 大型视觉语言模型在多模态任务中取得显著成功，但其对空间变化的鲁棒性尚未得到充分理解。研究发现模型在处理相同视觉信息在不同位置时会产生不一致输出，这揭示了模型空间语义理解的根本局限性。
- Method: 通过精心设计的探测数据集分析LVLMs的空间偏差，发现问题源于语言模型中位置嵌入的不平衡设计。提出平衡位置分配机制，为所有图像标记分配相同的位置嵌入，促进视觉信息的平衡整合。
- Result: 实验表明BaPA机制无需重新训练即可增强LVLMs的空间鲁棒性，结合轻量级微调还能进一步提升在各种多模态基准测试中的性能。信息流分析显示BaPA产生平衡的注意力，实现更全面的视觉理解。
- Conclusion: LVLMs存在显著的空间偏差问题，这源于位置嵌入的不平衡设计。提出的BaPA机制简单有效，能够显著提升模型的空间鲁棒性和整体性能，为构建更稳健的多模态模型提供了重要思路。


### [88] [Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation](https://arxiv.org/abs/2509.21989)
*Abdelrahman Eldesokey,Aleksandar Cvejic,Bernard Ghanem,Peter Wonka*

Main category: cs.CV

TL;DR: 提出了一种从预训练扩散模型中解耦视觉和语义特征的新方法，支持视觉对应关系，并提出了视觉语义匹配(VSM)指标来量化主题驱动图像生成中的视觉不一致性。

- Motivation: 扩散模型主干网络已知编码了丰富的语义特征，但也必须包含视觉特征来支持图像合成能力。然而，由于缺乏标注数据集，隔离这些视觉特征具有挑战性。
- Method: 引入自动化流水线构建具有标注语义和视觉对应关系的图像对，基于现有主题驱动图像生成数据集，并设计对比架构来分离两种特征类型。
- Result: 实验结果表明，该方法在量化视觉不一致性方面优于CLIP、DINO和视觉-语言模型等全局特征指标，同时还能实现不一致区域的空间定位。
- Conclusion: 这是首个同时支持主题驱动生成中不一致性量化和定位的方法，为推进该任务提供了有价值的工具。


### [89] [WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM](https://arxiv.org/abs/2509.21990)
*Changli Tang,Qinfan Xiao,Ke Mei,Tianyi Wang,Fengyun Rao,Chao Zhang*

Main category: cs.CV

TL;DR: WAVE是首个基于LLM的统一音频-视觉嵌入模型，通过分层特征融合和联合多模态训练，实现任意模态间的跨模态检索和提示感知嵌入生成。

- Motivation: 虽然多模态大语言模型的嵌入作为通用表示表现出色，但在音频和视频等动态模态中的应用仍未被充分探索。
- Method: 采用分层特征融合策略和联合多模态多任务训练方法，构建统一的多模态表示空间。
- Result: 在MMEB-v2视频基准测试中创下新纪录，在音频和视频到音频检索中表现优异，在多模态问答中显著优于现有嵌入模型。
- Conclusion: WAVE为跨模态任意到任意应用开辟了广阔可能性，验证了联合训练策略的有效性。


### [90] [ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models](https://arxiv.org/abs/2509.21991)
*Jewon Lee,Wooksu Shin,Seungmin Yang,Ki-Ung Song,DongUk Lim,Jaeyeon Kim,Tae-Ho Kim,Bo-Kyeong Kim*

Main category: cs.CV

TL;DR: ERGO提出了一种两阶段"粗到细"推理框架，通过先分析下采样图像识别任务相关区域，然后仅对相关区域进行全分辨率处理，显著减少计算成本同时保持必要的视觉细节。

- Motivation: 现有大型视觉语言模型在处理高分辨率图像时会产生大量视觉token，导致计算开销巨大。随着"图像思维"模型的出现，推理能力扩展到视觉领域，这促使开发更高效的推理方法。
- Method: 采用强化学习框架，通过推理驱动的感知方法，利用多模态上下文确定关注区域，并能考虑感知不确定性，扩展裁剪区域以覆盖视觉模糊区域。
- Result: 在多个数据集上，ERGO比原始模型和竞争方法获得更高准确率，同时效率更高。例如在V*基准测试中超越Qwen2.5-VL-7B模型4.7分，仅使用23%的视觉token，实现3倍推理加速。
- Conclusion: ERGO通过粗到细的感知推理方法，在保持准确性的同时显著提升了大型视觉语言模型的效率，为高分辨率图像处理提供了有效的解决方案。


### [91] [DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints](https://arxiv.org/abs/2509.21992)
*Sungmin Woo,Sangyoun Lee*

Main category: cs.CV

TL;DR: DualFocus是一个新颖的深度聚焦框架，通过联合建模空间和焦距维度的焦点变化，利用变分公式和双约束来提高复杂场景中的深度估计精度。

- Motivation: 现有的基于学习的深度聚焦方法在复杂场景中表现不佳，特别是在精细纹理或深度突变区域，焦点线索可能变得模糊或误导。
- Method: 提出DualFocus框架，利用焦点堆栈中由焦点变化引起的独特梯度模式，引入具有双约束的变分公式：空间约束利用不同焦点级别的梯度模式变化来区分真实深度边缘和纹理伪影，焦距约束强制单峰、单调的焦点概率与物理焦点行为对齐。
- Result: 在四个公共数据集上的综合实验表明，DualFocus在深度精度和感知质量方面始终优于最先进的方法。
- Conclusion: DualFocus通过结合空间和焦距约束，提高了深度聚焦在挑战性区域的鲁棒性和准确性。


### [92] [Rate-Distortion Optimized Communication for Collaborative Perception](https://arxiv.org/abs/2509.21994)
*Genjia Liu,Anning Hu,Yue Hu,Wenjun Zhang,Siheng Chen*

Main category: cs.CV

TL;DR: 提出了基于信息论的协作感知理论框架RDcomm，通过任务熵离散编码和互信息驱动的消息选择，在3D目标检测和BEV分割任务中实现108倍通信量减少的同时保持最先进精度。

- Motivation: 现有协作感知研究缺乏理论基础，特别是在任务性能与通信量之间的权衡方面。需要建立理论框架来指导设计最优通信策略。
- Method: 引入实用的率失真理论，提出RDcomm框架：1）任务熵离散编码 - 为特征分配任务相关码字长度；2）互信息驱动消息选择 - 使用互信息神经估计实现无冗余传输。
- Result: 在DAIR-V2X和OPV2V数据集上，RDcomm在3D目标检测和BEV分割任务中达到最先进精度，同时通信量减少高达108倍。
- Conclusion: 基于信息论的协作感知理论框架能够有效指导通信策略设计，在保持性能的同时显著降低通信开销，为多智能体系统提供了理论基础和实践方案。


### [93] [FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration](https://arxiv.org/abs/2509.21995)
*Muxi Chen,Zhaohua Zhang,Chenchen Zhao,Mingyang Chen,Wenyu Jiang,Tianwen Jiang,Jianhuan Zhuo,Yu Tang,Qiuyong Xiao,Jihong Zhang,Qiang Xu*

Main category: cs.CV

TL;DR: FailureAtlas是一个主动探索文本到图像模型失败模式的框架，通过结构化搜索发现最小化失败诱导概念，揭示了SD1.5中超过24.7万个未知错误切片，并发现这些失败与训练数据稀缺相关。

- Motivation: 静态基准测试在比较文本到图像模型时价值有限，其被动设计诊断能力不足，难以发现系统性失败的全貌或隔离根本原因。需要一种补充性的主动探索范式。
- Method: 将错误发现构建为结构化搜索最小化失败诱导概念的问题，采用新颖的加速技术使计算爆炸性问题变得可处理，实现大规模自主探索和映射T2I模型的失败景观。
- Result: 在Stable Diffusion模型上应用该方法，发现了超过24.7万个先前未知的错误切片，并首次提供了大规模证据将这些失败与训练集中的数据稀缺联系起来。
- Conclusion: FailureAtlas通过提供原则性和可扩展的深度模型审计引擎，建立了一种新的、诊断优先的方法论，指导开发更稳健的生成式AI。


### [94] [Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors](https://arxiv.org/abs/2509.21997)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的自监督幻觉缓解方法，通过将文本投影到视觉空间来放大幻觉信号，使用双锚点编辑解码器隐藏状态，显著减少多模态大语言模型中的幻觉问题。

- Motivation: 多模态大语言模型在视觉语言任务中表现出色，但容易产生与视觉证据不一致的幻觉内容，现有方法往往需要额外微调或牺牲信息量和可扩展性。
- Method: 引入幻觉放大机制：通过文本到图像模型将描述投影到视觉空间揭示隐含幻觉信号作为负锚点，原始图像作为正锚点，利用双锚点编辑解码器隐藏状态，拉近忠实语义表示并推离幻觉方向。
- Result: 在多个基准测试中显著减少了对象、属性和关系层面的幻觉，使用LLaVA-v1.5-7B在CHAIR上实现超过5%的幻觉减少，同时在多种架构上验证了强大的跨架构泛化能力。
- Conclusion: 该方法无需人工先验或额外训练成本，在保持召回率和描述丰富性的同时有效缓解幻觉问题，对无幻觉描述几乎不产生副作用，具有鲁棒性和即插即用的实用性。


### [95] [CoFFT: Chain of Foresight-Focus Thought for Visual Language Models](https://arxiv.org/abs/2509.22010)
*Xinyu Zhang,Yuxuan Dong,Lingling Zhang,Chengyou Jia,Zhuohang Dang,Basura Fernando,Jun Liu,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出CoFFT方法，通过模拟人类视觉认知过程来增强视觉语言模型的视觉推理能力，无需训练即可提升性能

- Motivation: 当前视觉语言模型在处理复杂冗余的视觉输入时容易受到干扰，产生过多与任务无关的推理过程甚至幻觉，无法精确定位和处理推理所需的关键区域
- Method: CoFFT方法包含三个迭代阶段：多样样本生成、双重前瞻解码和视觉焦点调整，通过推理引导视觉焦点，视觉焦点又指导后续推理
- Result: 在多个基准测试中使用Qwen2.5-VL、InternVL-2.5和Llava-Next模型，性能持续提升3.1-5.8%，计算开销可控增加
- Conclusion: CoFFT方法通过模拟人类视觉认知机制，有效提升了视觉语言模型的视觉推理能力，且无需额外训练


### [96] [Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics](https://arxiv.org/abs/2509.22014)
*Saurav Jha,Stefan K. Ehrlich*

Main category: cs.CV

TL;DR: 提出轻量级多模态框架，结合Qwen2.5-VL-3B模型与SmolAgent编排层，支持视频场景理解、思维链推理和动态工具调用，在医疗机器人应用中表现优于现有VLM方法。

- Motivation: 医疗机器人需要鲁棒的多模态感知和推理能力，但当前视觉语言模型在时序推理、不确定性估计和结构化输出方面存在局限，无法满足机器人规划需求。
- Method: 结合Qwen2.5-VL-3B-Instruct模型与SmolAgent编排层，支持思维链推理、语音-视觉融合和动态工具调用，生成结构化场景图并利用混合检索模块进行可解释推理。
- Result: 在Video-MME基准测试和自定义临床数据集上的评估显示，相比最先进的VLM方法，该框架具有竞争性准确率和改进的鲁棒性。
- Conclusion: 该框架在机器人辅助手术、患者监测和决策支持等医疗应用中具有潜力，为医疗机器人提供了更可靠的多模态感知和推理能力。


### [97] [EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking](https://arxiv.org/abs/2509.22019)
*Yuki Sakai,Ryosuke Furuta,Juichun Yen,Yoichi Sato*

Main category: cs.CV

TL;DR: 提出了一个新的第一人称视角面对面教学视频数据集，用于程序步骤分割和对话状态分类任务，评估多模态大语言模型在理解教学互动中的表现。

- Motivation: 解决面对面教学场景在计算机视觉中缺乏系统性研究的问题，主要原因是缺乏合适的数据集和有限的分析技术。
- Method: 构建新的第一人称视角教学视频数据集并提供标注，基准测试多模态大语言模型与传统任务特定模型，整合处理图像、音频和文本等多种模态。
- Result: 多模态大语言模型即使没有任务特定微调也优于专门的基线模型，显示出在教学互动整体理解方面的潜力。
- Conclusion: 多模态大语言模型为全面理解教学互动提供了有前景的方法，能够有效整合语言和非语言沟通信息。


### [98] [High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling](https://arxiv.org/abs/2509.22063)
*Chao Huang,Susan Liang,Yapeng Tian,Anurag Kumar,Chenliang Xu*

Main category: cs.CV

TL;DR: DAVIS是一个基于扩散模型的音频-视觉分离框架，通过生成式学习方法解决音视频源分离任务，超越了传统基于掩码的回归方法。

- Motivation: 现有方法通常将声音分离视为基于掩码的回归问题，但在捕捉复杂数据分布以高质量分离不同类别声音方面存在局限。
- Method: 利用去噪扩散概率模型(DDPM)和流匹配(FM)等生成建模范式，在专门的分离U-Net架构中，从噪声分布直接合成分离的声音频谱图，同时以混合音频输入和相关视觉信息为条件。
- Result: 在标准AVE和MUSIC数据集上的比较评估显示，DAVIS的DDPM和Flow Matching变体在分离质量上都超越了现有方法。
- Conclusion: 生成式框架能有效解决音频-视觉源分离任务，特别擅长为不同声音类别生成高质量分离结果。


### [99] [SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection](https://arxiv.org/abs/2509.22070)
*Inzamamul Alam,Md Tanvir Islam,Simon S. Woo*

Main category: cs.CV

TL;DR: 提出SpecXNet双域网络架构，结合空间和频域特征进行深度伪造检测，在跨数据集和未知伪造场景下达到SOTA性能

- Motivation: GAN和扩散模型生成的伪造内容越来越逼真，现有方法仅关注空间或频域特征，限制了泛化能力
- Method: 使用双域特征耦合器(DDFC)分解特征到局部空间分支和全局频域分支，引入双傅里叶注意力(DFA)模块动态融合特征，基于改进的XceptionNet架构
- Result: 在多个深度伪造基准测试中达到最先进准确率，特别是在跨数据集和未知伪造场景下，同时保持实时可行性
- Conclusion: 统一的空间-频域学习对于鲁棒和可泛化的深度伪造检测非常有效


### [100] [Large Material Gaussian Model for Relightable 3D Generation](https://arxiv.org/abs/2509.22112)
*Jingrui Ye,Lingting Zhu,Runze Zhang,Zeyu Hu,Yingda Yin,Lanjiong Li,Lequan Yu,Qingmin Liao*

Main category: cs.CV

TL;DR: 提出了Large Material Gaussian Model (MGM)，一个能够生成带有PBR材质属性的高质量3D内容的新框架，解决了现有模型无法生成材质属性的问题。

- Motivation: 现有的大规模重建模型虽然能实现高质量的3D渲染，但无法生成资产的材质属性，这在多样化光照环境下的真实渲染中至关重要。
- Method: 首先微调一个新的多视角材质扩散模型，基于输入的深度和法线图生成多视角PBR图像；然后探索与2D高斯泼溅对齐的高斯材质表示，对PBR材料的每个通道进行建模；最后通过渲染重建的点云获取PBR属性。
- Result: 实验表明，该方法生成的材质不仅比基线方法更具视觉吸引力，还增强了材质建模能力，实现了实用的下游渲染应用。
- Conclusion: MGM框架成功生成了带有PBR材质属性的高质量3D内容，支持动态重光照，为实际渲染应用提供了可行方案。


### [101] [Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud](https://arxiv.org/abs/2509.22132)
*Jingjing Lu,Huilong Pi,Yunchuan Qin,Zhuo Tang,Ruihui Li*

Main category: cs.CV

TL;DR: 提出了一种基于多视图增强的自监督点云补全方法，通过将Mamba架构引入自监督点云补全任务，在合成和真实数据集上取得了最先进的结果。

- Motivation: 现有方法存在多种限制：监督方法依赖地面真值，泛化能力差；无监督方法需要完整点云；弱监督方法需要多视角观测；现有自监督方法因自监督信号能力有限而预测效果不佳。
- Method: 基于单点部分点云的多视图增强设计新型自监督信号，并将Mamba架构首次引入自监督点云补全任务以增强模型学习能力。
- Result: 在合成和真实数据集上的实验表明，该方法达到了最先进的性能。
- Conclusion: 提出的自监督点云补全方法通过多视图增强和Mamba架构的引入，有效克服了现有方法的局限性，生成了更高质量的点云。


### [102] [REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation](https://arxiv.org/abs/2509.22139)
*Yicheng Jiang,Jin Yuan,Hua Yuan,Yao Zhang,Yong Rui*

Main category: cs.CV

TL;DR: 提出Refine-Control半监督蒸馏框架，通过三级知识融合损失和利用标注/未标注数据，在降低计算成本和延迟的同时保持高质量图像生成能力

- Motivation: 解决条件图像生成模型在边缘设备部署时的高资源需求和标注数据稀缺问题，避免将用户数据发送给第三方带来的成本和隐私担忧
- Method: 采用半监督蒸馏框架，引入三级知识融合损失来传递不同层次的知识，同时利用标注和未标注数据增强泛化能力
- Result: 实验显示Refine-Control显著降低了计算成本和延迟，同时保持了高保真生成能力和可控性
- Conclusion: 该方法为在资源受限设备上部署高质量条件图像生成模型提供了有效解决方案


### [103] [Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions](https://arxiv.org/abs/2509.22150)
*Zhiqiang Tian,Weigang Li,Junwei Hu,Chunhua Deng*

Main category: cs.CV

TL;DR: 提出JGEKD方法，通过联合图熵知识蒸馏处理非独立同分布的3D点云分类问题，捕获类别间相关性并提升模型鲁棒性。

- Motivation: 传统3D点云分类假设数据独立同分布，这会破坏类别间的相关性。本文旨在处理非独立同分布的3D点云数据，保留类别间的关联信息。
- Method: 使用联合图捕获类别间隐藏关系，基于联合图熵构建损失函数进行知识蒸馏。构建孪生网络结构，开发自知识蒸馏和教师知识蒸馏两种框架来处理空间变换不变性。
- Result: 在ScanObject、ModelNet40、ScanntV2_cls和ModelNet-C数据集上的广泛实验表明，该方法能够取得有竞争力的结果。
- Conclusion: JGEKD策略通过联合图熵知识蒸馏有效处理了非独立同分布的3D点云分类问题，实现了类别相关性的知识转移，并提升了模型对数据损坏的鲁棒性。


### [104] [MultiMat: Multimodal Program Synthesis for Procedural Materials using Large Multimodal Models](https://arxiv.org/abs/2509.22151)
*Jonas Belouadi,Tamy Boubekeur,Adrien Kaiser*

Main category: cs.CV

TL;DR: MultiMat是一个多模态程序合成框架，利用大型多模态模型处理视觉和文本图表示，以改进程序材质图的生成。该方法在无条件和条件图合成中都更高效，视觉质量和保真度优于纯文本基线。

- Motivation: 现有的神经程序合成方法仅将图表示为文本程序，未能捕捉节点图固有的视觉空间特性，而这正是节点图对人类用户直观易用的关键。
- Method: 训练大型多模态模型处理视觉和文本图表示，结合约束树搜索推理算法确保语法有效性并高效导航程序空间。
- Result: 多模态程序合成方法在无条件和条件图合成中都更高效，视觉质量和保真度优于纯文本基线，达到新的最先进性能。
- Conclusion: MultiMat框架通过结合视觉和文本表示，显著提升了程序材质图的生成质量，证明了多模态方法在程序合成中的优势。


### [105] [DragGANSpace: Latent Space Exploration and Control for GANs](https://arxiv.org/abs/2509.22169)
*Kirsten Odendaal,Neela Kaushik,Spencer Halverson*

Main category: cs.CV

TL;DR: 结合StyleGAN、DragGAN和PCA来提升GAN生成图像的潜空间效率和可控性，通过PCA降维和跨模型对齐实现更高效的潜空间探索。

- Motivation: 旨在提高GAN潜空间的效率和可控性，使图像合成和编辑更加高效和直观。
- Method: 集成StyleGAN提供结构化潜空间，DragGAN实现直观图像操作，PCA进行降维和跨模型对齐。
- Result: 在AFHQ数据集上，PCA集成到DragGAN的W+潜层中能减少优化时间，保持视觉质量，提升SSIM指标，特别是在浅层潜空间（W+层=3）中效果更佳。
- Conclusion: 该方法为广泛的图像合成和编辑应用提供了高效且可解释的潜空间控制可能性。


### [106] [MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing](https://arxiv.org/abs/2509.22186)
*Junbo Niu,Zheng Liu,Zhuangcheng Gu,Bin Wang,Linke Ouyang,Zhiyuan Zhao,Tao Chu,Tianyao He,Fan Wu,Qintong Zhang,Zhenjiang Jin,Guang Liang,Rui Zhang,Wenzheng Zhang,Yuan Qu,Zhifei Ren,Yuefeng Sun,Yuanhong Zheng,Dongsheng Ma,Zirui Tang,Boyu Niu,Ziyang Miao,Hejun Dong,Siyi Qian,Junyuan Zhang,Jingzhou Chen,Fangdong Wang,Xiaomeng Zhao,Liqun Wei,Wei Li,Shasha Wang,Ruiliang Xu,Yuanyuan Cao,Lu Chen,Qianqian Wu,Huaiyu Gu,Lindong Lu,Keming Wang,Dechen Lin,Guanlin Shen,Xuanhe Zhou,Linfeng Zhang,Yuhang Zang,Xiaoyi Dong,Jiaqi Wang,Bo Zhang,Lei Bai,Pei Chu,Weijia Li,Jiang Wu,Lijun Wu,Zhenxiang Li,Guangyu Wang,Zhongying Tu,Chao Xu,Kai Chen,Yu Qiao,Bowen Zhou,Dahua Lin,Wentao Zhang,Conghui He*

Main category: cs.CV

TL;DR: MinerU2.5是一个12亿参数的文档解析视觉语言模型，采用粗到细的两阶段解析策略，在保持卓越计算效率的同时实现了最先进的识别精度。

- Motivation: 解决文档解析中高分辨率输入处理带来的计算开销问题，同时保持对密集文本、复杂公式和表格等细粒度细节的识别能力。
- Method: 采用两阶段解析策略：第一阶段在降采样图像上进行高效布局分析以识别结构元素；第二阶段基于全局布局指导，在原始图像的原生分辨率裁剪区域进行针对性内容识别。
- Result: 在多个基准测试中实现了最先进的性能，超越了通用和领域特定模型，同时保持了显著较低的计算开销。
- Conclusion: MinerU2.5展示了强大的文档解析能力，在保持计算效率的同时实现了最先进的识别精度，为文档解析任务提供了高效的解决方案。


### [107] [Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models](https://arxiv.org/abs/2509.22221)
*Jiaqi Liu,Lang Sun,Ronghao Fu,Bo Yang*

Main category: cs.CV

TL;DR: 提出了Geo-CoT框架，通过两阶段对齐策略训练RSThinker模型，在遥感分析中实现可验证的多步推理过程，显著超越现有模型性能。

- Motivation: 现有视觉语言模型在遥感复杂分析任务中表现不佳，主要原因是端到端训练绕过了关键推理步骤，导致输出不可验证。
- Method: 使用Geo-CoT380k数据集，采用两阶段对齐策略：先通过监督微调建立认知架构，再通过组奖励策略优化优化推理策略。
- Result: RSThinker模型在广泛任务中显著优于最先进模型，能够输出最终答案及其可验证的分析轨迹。
- Conclusion: Geo-CoT框架为地球观测从模糊感知转向结构化、可验证推理提供了具体路径，相关数据集和模型将公开发布。


### [108] [Polysemous Language Gaussian Splatting via Matching-based Mask Lifting](https://arxiv.org/abs/2509.22225)
*Jiayu Ding,Xinpeng Liu,Zhiyi Pan,Shiqiang Long,Ge Li*

Main category: cs.CV

TL;DR: MUSplat是一个无需训练的框架，将2D开放词汇理解提升到3D高斯泼溅场景，解决了现有方法依赖逐场景训练、单语义限制和跨视图语义不一致的问题。

- Motivation: 现有主流方法存在三个关键缺陷：(i)依赖昂贵的逐场景重新训练，无法即插即用；(ii)限制性单语义设计无法表示复杂的多概念语义；(iii)易受跨视图语义不一致影响，破坏最终语义表示。
- Method: 利用预训练的2D分割模型生成多粒度2D掩码并提升到3D，为每个高斯点估计前景概率形成初始对象组。然后使用语义熵和几何不透明度优化初始组的模糊边界。通过解释对象在最具代表性视角下的外观，视觉语言模型提取鲁棒的文本特征来协调视觉不一致，实现开放词汇查询。
- Result: 通过消除昂贵的逐场景训练过程，MUSplat将场景适应时间从数小时减少到几分钟。在开放词汇3D对象选择和语义分割的基准任务上，MUSplat优于已建立的基于训练的框架，同时解决了它们的单语义限制。
- Conclusion: MUSplat是一个训练免费的框架，成功解决了现有3D语义理解方法的三个关键缺陷，实现了高效的开放词汇3D场景理解。


### [109] [UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective](https://arxiv.org/abs/2509.22228)
*Jun He,Yi Lin,Zilong Huang,Jiacong Yin,Junyan Ye,Yuchuan Zhou,Weijia Li,Xiang Zhang*

Main category: cs.CV

TL;DR: UrbanFeel是一个评估多模态大语言模型在城市发展理解和主观环境感知方面性能的综合基准，包含14.3K个视觉问题，涵盖静态场景感知、时间变化理解和主观环境感知三个认知维度。

- Motivation: 城市发展影响全球一半以上人口，需要从人类中心视角理解其结构和感知变化。现有基准在探索MLLMs在城市环境中的表现方面有限，缺乏对时间演化和与人类感知一致的主观城市环境的系统探索。
- Method: 从全球11个代表性城市收集多时间单视图和全景街景图像，通过空间聚类、基于规则生成、模型辅助提示和人工标注的混合流程生成高质量问答对。
- Result: 评估20个最先进的MLLMs，Gemini-2.5 Pro表现最佳，准确率接近人类专家水平，平均差距仅为1.5%。大多数模型在基于场景理解的任务上表现良好，部分模型在像素级变化检测上甚至超过人类标注者。但在需要时间推理的任务上表现明显下降。
- Conclusion: UrbanFeel基准揭示了MLLMs在城市发展理解方面的能力差距，特别是在时间推理方面，同时展示了在主观感知维度上达到或超过人类一致性的潜力。


### [110] [A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.22229)
*Jiaping Yu,Muli Yang,Jiapeng Ji,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 提出了EXCL方法解决无源无监督域自适应问题，包含双专家框架和检索增强交互优化流程，在多个基准数据集上达到SOTA性能

- Motivation: 现有SFUDA方法要么只利用源模型预测，要么微调大型多模态模型，但都忽略了互补见解和目标数据的潜在结构
- Method: EXCL包含双专家框架（冻结源域模型+视觉语言模型）和RAIN三阶段流程：协作检索伪源和复杂目标样本、分别微调专家、通过共享学习结果强制学习对象一致性
- Result: 在四个基准数据集上的广泛实验表明，该方法达到了最先进的性能
- Conclusion: EXCL方法有效解决了无源无监督域自适应问题，通过双专家协作和检索增强交互优化实现了优异的性能


### [111] [FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing](https://arxiv.org/abs/2509.22244)
*Junyi Wu,Zhiteng Li,Haotong Qin,Xiaohong Liu,Linghe Kong,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FlashEdit是一个实时图像编辑框架，通过三步创新实现高质量编辑：一步式反演编辑、背景保护和稀疏空间交叉注意力机制，速度提升150倍以上。

- Motivation: 基于扩散模型的文本引导图像编辑虽然质量出色，但存在严重的延迟问题，阻碍了实际应用。
- Method: 采用三步创新方法：1）一步式反演编辑管道绕过迭代过程；2）背景保护技术选择性修改编辑区域特征；3）稀疏空间交叉注意力机制防止语义泄漏。
- Result: FlashEdit在保持优异背景一致性和结构完整性的同时，编辑时间低于0.2秒，比之前的多步方法快150倍以上。
- Conclusion: FlashEdit实现了高保真度的实时图像编辑，解决了扩散模型在图像编辑中的延迟问题，具有实际应用价值。


### [112] [Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks](https://arxiv.org/abs/2509.22258)
*Miao Jing,Mengting Jia,Junling Lin,Zhongxia Shen,Lijun Wang,Yuanyuan Peng,Huan Gao,Mingkun Xu,Shangyang Li*

Main category: cs.CV

TL;DR: 提出了Neural-MedBench基准测试，专门用于评估神经学领域的多模态临床推理能力，发现现有视觉语言模型在真实临床推理任务上存在显著性能下降。

- Motivation: 现有医学基准测试主要关注分类准确率，导致模型看似表现良好但实际缺乏高风险的诊断推理能力，需要专门设计的推理密集型评估工具。
- Method: 整合多序列MRI扫描、结构化电子健康记录和临床笔记，构建包含鉴别诊断、病灶识别和原理生成三个核心任务的基准测试，并开发结合LLM评分、临床医生验证和语义相似度度量的混合评分流程。
- Result: 对GPT-4o、Claude-4和MedGemma等最先进VLM的系统评估显示，与传统数据集相比性能急剧下降，错误分析表明推理失败而非感知错误是主要问题。
- Conclusion: 提出双轴评估框架：广度导向的大数据集用于统计泛化，深度导向的紧凑基准测试（如Neural-MedBench）用于推理保真度，为临床可信AI提供严格且成本效益高的评估方法。


### [113] [UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data](https://arxiv.org/abs/2509.22262)
*Yujian Yuan,Changjie Wu,Xinyuan Chang,Sijin Wang,Hang Zhang,Shiyi Liang,Shuang Zeng,Mu Xu*

Main category: cs.CV

TL;DR: UniMapGen是一个用于大规模地图构建的生成式框架，通过将车道线表示为离散序列并使用迭代策略生成更完整平滑的地图向量，支持多模态输入以克服卫星数据缺陷，并采用状态更新策略保证全局连续性。

- Motivation: 传统大规模地图构建方法依赖昂贵的专用数据采集车辆和人工标注，现有卫星方法存在遮挡、数据过时等固有缺陷，以及感知方法向量化效率低导致道路不连续粗糙的问题。
- Method: 将车道线表示为离散序列并建立迭代生成策略；提出支持BEV、PV和文本提示的多模态输入架构；开发状态更新策略保证地图全局连续性和一致性。
- Result: 在OpenSatMap数据集上达到最先进性能，能够推断被遮挡道路并预测数据集中缺失的道路标注。
- Conclusion: UniMapGen通过生成式方法有效解决了传统地图构建方法的局限性，为大规模地图构建提供了更高效完整的解决方案。


### [114] [GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition](https://arxiv.org/abs/2509.22276)
*Dinh Minh Nguyen,Malte Avenhaus,Thomas Lindemeier*

Main category: cs.CV

TL;DR: GS-2M是一个基于3D高斯泼溅的统一框架，能够从多视角图像同时进行网格重建和材质分解，特别擅长处理高反射表面。

- Motivation: 现有方法通常将网格重建和材质分解分开处理，难以重建高反射表面，且依赖外部模型的先验知识来提升分解效果。
- Method: 联合优化与渲染深度和法线质量相关的属性，提出基于多视角光度变化的新型粗糙度监督策略，结合精心设计的损失函数和优化过程。
- Result: 重建结果与最先进方法相当，能够生成三角形网格及其关联的材质组件，适用于下游任务。
- Conclusion: 该方法在广泛使用的数据集上验证了有效性，与最先进的表面重建方法进行了定性比较，证明了其优越性能。


### [115] [MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning](https://arxiv.org/abs/2509.22281)
*Jinkun Hao,Naifu Liang,Zhen Luo,Xudong Xu,Weipeng Zhong,Ran Yi,Yichen Jin,Zhaoyang Lyu,Feng Zheng,Lizhuang Ma,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了任务导向的桌面场景生成新任务，并开发了MesaTask框架，通过空间推理链和DPO算法生成符合任务描述的物理合理桌面场景。

- Motivation: 传统创建桌面场景的方法依赖耗时的手动布局设计或纯随机布局，在合理性和任务对齐方面存在限制，需要更智能的场景生成方法。
- Method: 提出空间推理链，将生成过程分解为物体推断、空间相互关系推理和场景图构建；开发基于LLM的MesaTask框架，并使用DPO算法进行增强。
- Result: 实验表明MesaTask在生成符合任务描述的桌面场景方面优于基线方法，能够产生具有现实布局的场景。
- Conclusion: MesaTask框架能够有效解决任务导向桌面场景生成的挑战，为机器人训练提供了高质量的合成场景数据。


### [116] [Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models](https://arxiv.org/abs/2509.22283)
*Michael Jungo,Andreas Fischer*

Main category: cs.CV

TL;DR: 研究基于规则的强化学习在文档图像分类任务中的应用，发现强化学习方法在分布外数据上具有更好的泛化能力。

- Motivation: 文档分析领域强化学习应用较少，但许多下游任务可能受益于强化学习的新兴特性，特别是增强的推理能力。
- Method: 使用基于规则的强化学习方法，在文档图像分类任务上进行实验，考察三种不同的分布外场景。
- Result: 强化学习在分布外数据上表现出更好的泛化能力，包括分布外图像、未见类别和不同模态三种场景。
- Conclusion: 基于规则的强化学习在文档分析任务中具有潜力，特别是在处理分布外数据时表现出优越的泛化性能。


### [117] [Jailbreaking on Text-to-Video Models via Scene Splitting Strategy](https://arxiv.org/abs/2509.22292)
*Wonjun Lee,Haon Park,Doehyeon Lee,Bumsub Ham,Suhyun Kim*

Main category: cs.CV

TL;DR: SceneSplit是一种针对文本到视频(T2V)模型的黑盒越狱方法，通过将有害叙述分割成多个看似良性的场景来绕过安全过滤器，显著提高有害视频的生成成功率。

- Motivation: 随着T2V模型的快速发展，其安全风险日益凸显。虽然已有研究探索了LLMs、VLMs和T2I模型的漏洞，但T2V模型的安全性问题尚未得到充分研究，存在显著的安全空白。
- Method: SceneSplit通过将有害叙述分割成多个单独良性的场景，利用场景组合作为约束来操纵生成输出空间。通过迭代场景操作绕过安全过滤器，并利用策略库重用成功攻击模式来提高攻击效果。
- Result: 在11个安全类别上对T2V模型进行评估，SceneSplit在Luma Ray2上达到77.2%的平均攻击成功率，在Hailuo上达到84.1%，在Veo2上达到78.2%，显著优于现有基线方法。
- Conclusion: 当前T2V安全机制容易受到利用叙事结构的攻击，这项工作为理解和改进T2V模型安全性提供了新的见解。


### [118] [HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models](https://arxiv.org/abs/2509.22300)
*Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: 提出了一种基于动量的采样技术HiGS，通过整合历史预测信息来提升扩散模型采样质量和效率，无需额外训练即可实现更高质量的图像生成。

- Motivation: 扩散模型在图像生成方面取得了显著进展，但在使用较少神经网络评估次数或较低引导尺度时，输出仍然可能显得不真实且缺乏细节。
- Method: HiGS利用当前预测与历史预测加权平均值之间的差异来引导采样过程，通过整合近期模型预测到每个推理步骤中，实现更真实、细节更丰富的输出。
- Result: 实验表明HiGS在不同模型和架构下都能持续提升图像质量，使用预训练的SiT模型在ImageNet 256×256无引导生成中仅用30步就达到了1.61的FID新纪录。
- Conclusion: HiGS作为一种即插即用的增强技术，能够实现更快速度、更高质量的扩散采样，无需额外计算开销或模型调优。


### [119] [Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation](https://arxiv.org/abs/2509.22307)
*Jinpeng Lu,Linghan Cai,Yinda Chen,Guo Tang,Songhan Jiang,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: VeloxSeg是一种轻量级3D医学图像分割方法，通过双流CNN-Transformer架构解决了效率与鲁棒性冲突，在保持低计算预算的同时显著提升分割性能。

- Motivation: 轻量级3D医学图像分割面临'效率/鲁棒性冲突'，特别是在处理复杂解剖结构和异质模态时，现有方法表示能力脆弱。
- Method: 采用双流CNN-Transformer架构，包含配对窗口注意力(PWA)和Johnson-Lindenstrauss引理指导的卷积(JLC)，实现'一瞥-聚焦'原则；通过模态交互和多尺度图像检索建模异质模态；使用空间解耦知识迁移(SDKT)注入纹理先验。
- Result: 在多模态基准测试中，Dice系数提升26%，GPU吞吐量提升11倍，CPU吞吐量提升48倍。
- Conclusion: VeloxSeg在保持轻量级的同时显著提升了3D医学图像分割的性能和效率，解决了效率与鲁棒性的冲突问题。


### [120] [NIFTY: a Non-Local Image Flow Matching for Texture Synthesis](https://arxiv.org/abs/2509.22318)
*Pierrick Chatillon,Julien Rabin,David Tschumperlé*

Main category: cs.CV

TL;DR: NIFTY是一个结合扩散模型和传统基于块纹理优化的混合框架，用于示例纹理合成，无需神经网络训练，解决了基于块方法的常见问题。

- Motivation: 解决示例纹理合成问题，结合扩散模型和传统块匹配方法的优势，避免神经网络训练需求，同时改进传统块方法的初始化差和视觉伪影问题。
- Method: 基于非局部块匹配的非参数流匹配模型，结合卷积神经网络训练的扩散模型洞察和经典基于块的纹理优化技术。
- Result: 实验结果表明，与文献中的代表性方法相比，所提方法具有有效性。
- Conclusion: NIFTY框架成功结合了现代扩散模型和传统纹理合成方法的优势，在无需神经网络训练的情况下实现了高质量的纹理合成。


### [121] [RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer](https://arxiv.org/abs/2509.22323)
*Wangbo Zhao,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Pengfei Zhou,Kai Wang,Bohan Zhuang,Zhangyang Wang,Fan Wang,Yang You*

Main category: cs.CV

TL;DR: RAPID3是一个零更新基础生成器的三层次强化加速框架，通过步长跳过、缓存重用和稀疏注意力三个轻量级策略头，在保持生成质量的同时实现近3倍加速采样。

- Motivation: 现有DiT加速方法要么使用统一启发式策略牺牲质量，要么需要高成本的微调，无法实现图像级自适应加速。
- Method: 使用三个独立策略头观察去噪状态并决策加速策略，通过GRPO在线训练策略参数，同时使用对抗性判别器增强奖励信号防止奖励攻击。
- Result: 在Stable Diffusion 3和FLUX等DiT骨干网络上，实现了近3倍采样加速且保持竞争力的生成质量。
- Conclusion: RAPID3框架成功实现了零更新基础生成器的图像级自适应加速，为DiT的实际应用提供了高效的解决方案。


### [122] [Pedestrian Attribute Recognition via Hierarchical Cross-Modality HyperGraph Learning](https://arxiv.org/abs/2509.22331)
*Xiao Wang,Shujuan Wu,Xiaoxia Cheng,Changwei Bi,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了一种多模态知识图谱构建方法，通过挖掘局部视觉特征与文本之间的关系以及属性与广泛视觉上下文样本之间的关系，来增强行人属性识别任务。

- Motivation: 当前的行人属性识别算法未能充分利用属性知识和上下文信息进行更准确的识别，虽然近期工作开始考虑使用属性文本作为额外输入，但这些方法仍处于起步阶段。
- Method: 提出有效的多模态知识图谱构建方法，充分考虑属性间关系以及属性与视觉标记之间的关系，并引入知识图谱引导的跨模态超图学习框架。
- Result: 在多个PAR基准数据集上的综合实验充分证明了所提出知识图谱对PAR任务的有效性。
- Conclusion: 为知识引导的行人属性识别建立了坚实基础，源代码将在GitHub上发布。


### [123] [CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process](https://arxiv.org/abs/2509.22339)
*Arman Akbari,Jian Gao,Yifei Zou,Mei Yang,Jinru Duan,Dmitrii Torbunov,Yanzhi Wang,Yihui Ren,Xuan Zhang*

Main category: cs.CV

TL;DR: CircuitSense是一个评估多模态大语言模型在电路图理解方面能力的基准测试，涵盖从组件级原理图到系统级框图的8000多个问题，重点关注从视觉输入推导符号方程的能力。

- Motivation: 工程设计需要层次化抽象和视觉理解与数学推理的结合，但目前MLLMs在从技术图表中提取数学模型的能力尚未被探索。
- Method: 提出了一个层次化合成生成管道，包括基于网格的原理图生成器和带有自动推导符号方程标签的框图生成器，评估了6个最先进的MLLMs。
- Result: 闭源模型在感知任务上准确率超过85%，但在符号推导和分析推理上低于19%，暴露了视觉解析与符号推理之间的关键差距。
- Conclusion: 具有更强符号推理能力的模型在设计任务上表现更好，确立了符号推理作为工程能力的关键指标。


### [124] [HierLight-YOLO: A Hierarchical and Lightweight Object Detection Network for UAV Photography](https://arxiv.org/abs/2509.22365)
*Defan Chen,Yaohua Hu,Luchan Zhang*

Main category: cs.CV

TL;DR: 提出HierLight-YOLO模型，通过分层特征融合和轻量化设计，在无人机摄影等复杂场景中实现小目标的实时检测，在VisDrone2019基准测试中达到最先进性能。

- Motivation: 无人机摄影等复杂场景中的小目标（<32像素）实时检测面临双重挑战：检测小目标和在资源受限平台上保持实时效率。YOLO系列检测器在大目标检测中表现优异，但在以小型目标为主的无人机检测中误报率显著更高。
- Method: 基于YOLOv8架构，提出分层扩展路径聚合网络（HEPAN）进行多尺度特征融合；设计两个轻量化模块：倒置残差深度卷积块（IRDCB）和轻量下采样模块（LDown）；专门设计小目标检测头以增强空间分辨率和特征融合。
- Result: 在VisDrone2019基准测试中表现出最先进的性能，显著降低了模型参数和计算复杂度，同时保持检测能力。
- Conclusion: HierLight-YOLO通过分层特征融合和轻量化设计，有效解决了无人机摄影中小目标实时检测的挑战，在保持实时效率的同时显著提升了小目标检测精度。


### [125] [Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results](https://arxiv.org/abs/2509.22377)
*Yasmina Kheddache,Marc Lalonde*

Main category: cs.CV

TL;DR: 本研究利用GPT-4o模型开发了一个多模态虚假信息检测框架，通过优化提示工程和结构化分析方法，在多个数据集上评估模型性能，并引入置信度和变异性评估方法。

- Motivation: 随着文本和图像结合的多模态虚假信息在数字平台上的扩散，需要开发有效的检测方法。本研究旨在探索大型多模态模型在检测和缓解虚假信息方面的潜力。
- Method: 使用GPT-4o模型，开发了优化的提示工程技术和结构化多模态分析框架，包括图像和文本的预处理方法，定义了六个评估标准和基于置信度的自评估机制。
- Result: 在Gossipcop、Politifact、Fakeddit、MMFakeBench和AMMEBA等多个异构数据集上进行了全面的性能分析，揭示了GPT-4o在虚假信息检测中的优势和局限性。
- Conclusion: 该研究提供了一个稳健且可复现的自动化多模态虚假信息分析方法论框架，为后续研究奠定了基础。


### [126] [GPT-4 for Occlusion Order Recovery](https://arxiv.org/abs/2509.22383)
*Kaziwa Saleh,Zhyar Rzgar K Rostam,Sándor Szénási,Zoltán Vámossy*

Main category: cs.CV

TL;DR: 利用预训练的GPT-4模型通过专门设计的提示来分析图像并预测遮挡顺序关系，无需标注训练数据即可进行零样本推理

- Motivation: 遮挡是当前视觉模型在解释复杂密集真实世界图像时面临的重要挑战，需要准确预测物体间的遮挡顺序关系
- Method: 通过专门设计的提示词向GPT-4提供输入图像，让其分析图像并生成顺序预测，然后将响应解析构建遮挡矩阵
- Result: 在COCOA和InstaOrder数据集上的评估显示，模型能利用语义上下文、视觉模式和常识知识产生更准确的顺序预测
- Conclusion: 该方法能以零样本方式推理遮挡关系，无需标注训练数据，可轻松集成到遮挡处理框架中


### [127] [Gradient-based multi-focus image fusion with focus-aware saliency enhancement](https://arxiv.org/abs/2509.22392)
*Haoyu Li,XiaoSong Li*

Main category: cs.CV

TL;DR: 提出了一种基于显著边界增强的多焦点图像融合方法，通过梯度域模型和Tenengrad梯度检测生成高质量融合边界，在四个公共数据集上优于12种先进方法。

- Motivation: 现有多焦点图像融合方法在保持清晰焦点-散焦边界方面存在困难，常导致模糊过渡和焦点细节丢失。
- Method: 使用梯度域模型获得具有完整边界的初始融合结果，引入Tenengrad梯度检测提取显著特征，开发基于梯度和互补信息的焦点度量进行边界细化。
- Result: 在四个公共数据集上的广泛实验表明，该方法在主观和客观评估中始终优于12种最先进的方法。
- Conclusion: 所提出的基于显著边界增强的MFIF方法能够生成高质量的融合边界，有效检测焦点信息，在图像融合任务中表现出色。


### [128] [Text Adversarial Attacks with Dynamic Outputs](https://arxiv.org/abs/2509.22393)
*Wenqiang Wang,Siyuan Liang,Xiao Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出了TDOA方法，通过聚类训练代理模型将动态输出场景转换为静态单输出场景，使用最远标签目标攻击策略，在动态和静态输出场景下都取得了优异表现。

- Motivation: 现有文本对抗攻击方法主要针对静态场景，无法有效处理动态输出场景（输出标签数量和空间不固定），需要解决这一局限性。
- Method: 采用基于聚类的代理模型训练方法将动态输出转换为静态单输出，提出最远标签目标攻击策略选择偏离模型粗粒度标签的对抗向量。
- Result: 在4个数据集和8个受害者模型上评估，单次查询最大攻击成功率50.81%；在静态场景下最大攻击成功率82.68%；在生成式设置中超越先前结果0.64 RDBLEU和0.62 RDchrF。
- Conclusion: TDOA方法在动态输出场景中有效，对大型语言模型具有潜在威胁，同时在静态场景和生成任务中也表现出色。


### [129] [Integrating Background Knowledge in Medical Semantic Segmentation with Logic Tensor Networks](https://arxiv.org/abs/2509.22399)
*Luca Bergamin,Giovanna Maria Dimitri,Fabio Aiolli*

Main category: cs.CV

TL;DR: 该论文提出使用逻辑张量网络（LTNs）将医学背景知识编码到语义分割模型的损失函数中，以提高医学图像分割性能，特别是在训练数据稀缺的情况下。

- Motivation: 当前基于深度学习的医学图像语义分割系统尚未完善，作者认为通过将常见医学知识融入分割模型的损失函数可以提升性能。
- Method: 使用逻辑张量网络（LTNs）通过一阶逻辑规则编码医学背景知识，包括分割形状约束和不同分割区域之间的关系，并与SwinUNETR结合构建端到端的分割框架。
- Result: 在脑部MRI海马体分割任务上的实验表明，LTNs能够提升基线分割性能，特别是在训练数据稀缺的情况下效果更明显。
- Conclusion: 尽管仍处于初步阶段，但神经符号方法具有足够的通用性，可以适应并应用于其他医学语义分割任务。


### [130] [Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models](https://arxiv.org/abs/2509.22400)
*Xinhao Zhong,Yimin Zhou,Zhiqi Zhang,Junhao Li,Yi Sun,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 提出VARE框架和S-VARE方法，专门针对视觉自回归模型的稳定概念擦除，通过辅助视觉token和过滤交叉熵损失实现精确概念移除，同时保持生成质量。

- Motivation: 现有概念擦除技术主要针对扩散模型，无法泛化到视觉自回归模型，因其采用下一尺度token预测范式，存在安全风险。
- Method: VARE框架利用辅助视觉token降低微调强度；S-VARE方法结合过滤交叉熵损失精确识别和最小调整不安全视觉token，以及保持损失维持语义保真度。
- Result: 大量实验表明，该方法实现了外科手术式的概念擦除，同时保持生成质量，解决了朴素微调带来的语言漂移和多样性减少问题。
- Conclusion: 该方法填补了自回归文本到图像生成中的安全空白，为VAR模型提供了有效的概念擦除解决方案。


### [131] [RAU: Reference-based Anatomical Understanding with Vision Language Models](https://arxiv.org/abs/2509.22404)
*Yiwei Li,Yikang Liu,Jiaqi Guo,Lin Zhao,Zheyuan Zhang,Xiao Chen,Boris Mailhe,Ankush Mukherjee,Terrence Chen,Shanhui Sun*

Main category: cs.CV

TL;DR: RAU是一个基于参考图像的解剖结构理解框架，结合视觉语言模型(VLMs)和SAM2，通过相对空间推理实现解剖区域的识别、定位和像素级分割。

- Motivation: 医学影像中的解剖理解对于自动报告生成、术中导航和器官定位至关重要，但专家标注数据的稀缺限制了进展。需要利用标注参考图像来指导未标注目标图像的解释。
- Method: 首先训练VLM通过参考图像和目标图像之间的相对空间推理来识别解剖区域，然后将其空间线索与SAM2的细粒度分割能力相结合，实现小解剖区域的定位和分割。
- Result: 在四个数据集（两个同分布和两个异分布）上，RAU始终优于SAM2微调基线，产生更准确的分割和更可靠的定位，并展现出强大的泛化能力。
- Conclusion: RAU是首个探索VLMs在医学图像中基于参考的解剖结构识别、定位和分割能力的方法，其优异性能突显了VLM驱动方法在自动化临床工作流程中的潜力。


### [132] [FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing](https://arxiv.org/abs/2509.22412)
*Hossein Kashiani,Niloufar Alipour Talemi,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 提出FreqDebias框架解决深度伪造检测器中的频谱偏差问题，通过Fo-Mixup增强和双重一致性正则化提升跨域泛化能力

- Motivation: 深度伪造检测器由于从有限训练数据中学习到的偏差，往往难以泛化到新的伪造类型，特别是存在频谱偏差问题
- Method: FreqDebias框架包含两个策略：1) Fo-Mixup增强动态多样化训练样本的频率特征；2) 双重一致性正则化，通过类激活图的局部一致性和超球面嵌入空间的vMF分布全局一致性
- Result: 大量实验表明FreqDebias显著提升了跨域泛化能力，在跨域和域内设置下均优于最先进方法
- Conclusion: FreqDebias通过缓解频谱偏差有效提升了深度伪造检测器的泛化性能


### [133] [LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer](https://arxiv.org/abs/2509.22414)
*Song Fei,Tian Ye,Lujia Wang,Lei Zhu*

Main category: cs.CV

TL;DR: LucidFlux是一个无需图像描述的通用图像修复框架，通过轻量级双分支调节器和自适应调制策略，在保持语义的同时修复各种未知混合退化问题。

- Motivation: 解决现有判别式修复方法和UNet-based扩散先验在未知混合退化条件下容易产生过度平滑、幻觉或语义漂移的问题。
- Method: 使用大型扩散变换器(Flux.1)，引入轻量级双分支调节器注入退化输入和轻度修复代理信号，设计时间步和层级自适应调制策略，并通过SigLIP特征实现无描述语义对齐。
- Result: 在合成和真实世界基准测试中，LucidFlux始终优于强大的开源和商业基线方法，消融研究验证了各组件的重要性。
- Conclusion: 对于大型DiTs，何时、何地以及如何调节——而不是增加参数或依赖文本提示——是实现鲁棒且无需描述的通用图像修复的关键因素。


### [134] [Explaining multimodal LLMs via intra-modal token interactions](https://arxiv.org/abs/2509.22415)
*Jiawei Liang,Ruoyu Chen,Xianghao Jiao,Siyuan Liang,Shiming Liu,Qunli Zhang,Zheng Hu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出MSEA和ARC方法增强多模态大语言模型的可解释性，通过多尺度解释聚合和激活排名相关性解决视觉和文本模态中的内部依赖问题。

- Motivation: 现有可解释性研究主要关注跨模态归因，但忽视了模态内部依赖关系。视觉模态中孤立图像块归因忽略了空间上下文，文本模态中依赖前文标记会引入虚假激活，这些干扰降低了归因保真度。
- Method: 视觉分支：提出多尺度解释聚合(MSEA)，通过多尺度输入聚合归因来动态调整感受野；文本分支：提出激活排名相关性(ARC)，通过前k个预测排名的对齐来测量上下文标记的相关性，抑制无关上下文的虚假激活。
- Result: 在最先进的多模态大语言模型和基准数据集上的广泛实验表明，该方法在可解释性方面持续优于现有方法，产生更忠实和细粒度的模型行为解释。
- Conclusion: 通过利用模态内部交互增强可解释性，提出的MSEA和ARC方法能够产生更全面、空间一致和语义连贯的解释，显著提高了多模态大语言模型决策机制的理解。


### [135] [U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image Segmentation](https://arxiv.org/abs/2509.22444)
*Bohan Huang,Qianyun Bao,Haoyuan Ma*

Main category: cs.CV

TL;DR: 提出U-MAN架构，通过渐进注意力引导特征融合和多尺度自适应KAN模块，解决医学图像分割中细节丢失和边界模糊问题，在多个数据集上优于现有方法。

- Motivation: 传统U-Net架构存在两个主要限制：(1)简单跳跃连接忽略了编码器-解码器之间的语义差距；(2)深层缺乏多尺度特征提取能力，导致医学图像分割中细节和边界信息丢失。
- Method: 提出U-MAN架构，包含两个核心模块：渐进注意力引导特征融合(PAGF)模块替代简单跳跃连接，使用注意力机制融合编码器和解码器特征；多尺度自适应KAN(MAN)模块使网络能够自适应处理多尺度特征。
- Result: 在三个公共数据集(BUSI、GLAS和CVC)上的实验表明，U-MAN优于最先进的方法，特别是在定义准确边界和保留精细细节方面表现突出。
- Conclusion: U-MAN通过结合渐进注意力引导特征融合和多尺度自适应KAN，有效解决了医学图像分割中的细节保留和边界精度问题，为复杂解剖结构和病理区域的分割提供了更优解决方案。


### [136] [$γ$-Quant: Towards Learnable Quantization for Low-bit Pattern Recognition](https://arxiv.org/abs/2509.22448)
*Mishal Fatima,Shashank Agnihotri,Marius Bock,Kanchana Vaishnavi Gandikota,Kristof Van Laerhoven,Michael Moeller,Margret Keuper*

Main category: cs.CV

TL;DR: 该论文提出了一种针对模式识别任务的可学习非线性量化方法γ-Quant，在低比特深度数据采集下实现与高比特数据相当的性能，特别适用于带宽和能耗受限的场景。

- Motivation: 传统模式识别模型使用预处理数据（如RGB图像、归一化传感器数据），但这些预处理方法可能不适用于自动化分析场景，且在带宽和能耗受限的设备上效率低下。
- Method: 提出γ-Quant方法，通过学习任务特定的非线性量化函数，在低比特深度（如4位）下对原始数据进行量化，应用于原始图像目标检测和可穿戴设备的人体活动识别。
- Result: 实验表明，使用可学习量化的4位原始数据在性能上可与12位原始数据相媲美，显著降低了数据传输需求。
- Conclusion: 任务特定的可学习量化方法能够在保持性能的同时大幅减少数据带宽需求，为资源受限设备上的模式识别应用提供了有效解决方案。


### [137] [SSVIF: Self-Supervised Segmentation-Oriented Visible and Infrared Image Fusion](https://arxiv.org/abs/2509.22450)
*Zixian Zhao,Xingchen Zhang*

Main category: cs.CV

TL;DR: 提出了一种用于分割导向可见光-红外图像融合的自监督训练框架SSVIF，通过特征级融合分割与像素级融合分割之间的一致性，无需分割标签即可学习高级语义特征。

- Motivation: 传统可见光-红外图像融合方法只关注融合图像质量，而应用导向方法需要下游任务标注数据，数据获取成本高。为解决这一问题，提出自监督训练框架。
- Method: 利用特征级融合分割与像素级融合分割的一致性，设计跨分割一致性自监督任务，采用两阶段训练策略和动态权重调整方法进行联合学习。
- Result: 在公开数据集上的实验表明，SSVIF仅使用未标注的可见光-红外图像对训练，就能超越传统融合方法，与有监督分割导向方法性能相当。
- Conclusion: 提出的自监督框架SSVIF有效解决了分割导向可见光-红外图像融合方法对标注数据的依赖问题，在无分割标签情况下仍能学习高质量语义特征。


### [138] [Bézier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation](https://arxiv.org/abs/2509.22476)
*Chen Li,Meilong Xu,Xiaoling Hu,Weimin Lyu,Chao Chen*

Main category: cs.CV

TL;DR: 提出Bézier Meets Diffusion框架，通过Bézier曲线风格迁移和条件扩散模型生成高质量跨域医学图像，提升分割性能

- Motivation: 解决医学影像跨模态训练中存在的领域差距问题，传统GAN方法在高度可变区域难以捕捉跨域映射
- Method: 1. Bézier曲线风格迁移减少领域差距 2. 使用分割模型生成伪标签 3. 条件扩散模型合成高质量目标域图像 4. 不确定性引导的分数匹配提高鲁棒性
- Result: 在公开数据集上生成逼真的标注图像，显著增强目标域数据并改善分割性能
- Conclusion: 该统一框架能有效生成高质量的跨域医学图像，为领域自适应提供有力解决方案


### [139] [PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning](https://arxiv.org/abs/2509.22481)
*Xiangmo Zhao,Nan Yang,Yang Wang,Zhanwen Liu*

Main category: cs.CV

TL;DR: 提出PSTTS模块，通过空间令牌净化和时间令牌选择，有效去除事件数据中的时空冗余令牌，在保持精度的同时显著提升计算效率。

- Motivation: 现有基于事件帧的方法忽略了事件帧序列的高空间稀疏性和帧间运动冗余，导致计算开销大。现有的RGB视频令牌稀疏化方法不适用于事件数据。
- Method: PSTTS包含两个阶段：空间令牌净化（基于时空一致性去除噪声和非事件区域）和时间令牌选择（评估相邻事件帧运动模式相似性去除冗余时间信息）。
- Result: 在HARDVS、DailyDVS-200和SeACT数据集上测试，PSTTS在DailyDVS-200数据集上减少FLOPs 29-43.6%，提升FPS 21.6-41.3%，同时保持任务精度。
- Conclusion: PSTTS能够有效利用事件数据的时空分布特性，在保持精度的同时显著提升计算效率，是一个无需额外参数的即插即用模块。


### [140] [Group Critical-token Policy Optimization for Autoregressive Image Generation](https://arxiv.org/abs/2509.22485)
*Guohui Zhang,Hu Yu,Xiaoxiao Ma,JingHao Zhang,Yaning Pan,Mingde Yao,Jie Xiao,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 提出了GCPO方法，通过识别AR视觉生成中的关键图像token，仅使用30%的token就能实现比全token优化更好的性能。

- Motivation: 现有RLVR方法对所有图像token进行统一优化，但不同token对训练贡献不同，需要识别关键token并实现有效的token级优化。
- Method: 从三个角度识别关键token：因果依赖、熵诱导的空间结构、RLVR聚焦的token多样性。为关键token引入动态token级优势权重来鼓励探索。
- Result: 在多个文本到图像基准测试中，GCPO仅使用30%的token就实现了比全token优化更好的性能。
- Conclusion: GCPO通过有效识别和优化关键token，为AR视觉生成提供了高效的策略优化方法。


### [141] [Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation](https://arxiv.org/abs/2509.22496)
*Ruoyu Chen,Xiaoqing Guo,Kangwei Liu,Siyuan Liang,Shiming Liu,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: EAGLE是一个轻量级黑盒框架，用于解释多模态大语言模型中的自回归token生成，通过将生成的token归因于紧凑的感知区域，并量化语言先验和感知证据的相对影响。

- Motivation: 多模态大语言模型在视觉输入与自然语言输出对齐方面表现出色，但生成的token对视觉模态的依赖程度尚不清楚，这限制了模型的可解释性和可靠性。
- Method: EAGLE引入了一个统一充分性（洞察分数）和必要性（必要性分数）的目标函数，通过稀疏化图像区域的贪婪搜索进行优化，实现忠实且高效的归因。
- Result: 在开源MLLMs上的广泛实验表明，EAGLE在忠实性、定位性和幻觉诊断方面始终优于现有方法，同时需要更少的GPU内存。
- Conclusion: EAGLE通过提供细粒度的模型决策可解释性，有效推进了多模态大语言模型的可解释性研究，具有实际应用价值。


### [142] [Color Names in Vision-Language Models](https://arxiv.org/abs/2509.22524)
*Alexandra Gomez-Villa,Pablo Hernández-Cámara,Muhammad Atif Butt,Valero Laparra,Jesus Malo,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: 系统评估视觉语言模型在颜色命名方面的能力，发现模型在典型颜色上表现良好，但在非典型颜色上性能显著下降，且存在跨语言训练不平衡问题。

- Motivation: 理解视觉语言模型是否像人类一样命名颜色对于有效的人机交互至关重要，因为颜色是人类视觉感知的基本维度。
- Method: 使用957个颜色样本，在五个代表性模型上复制经典颜色命名方法，进行跨语言分析和消融研究。
- Result: 模型在典型颜色上准确率高，但在扩展的非典型颜色集上性能显著下降；识别出21个在所有模型中一致出现的颜色术语；发现语言模型架构独立于视觉处理能力影响颜色命名。
- Conclusion: 视觉语言模型在颜色命名方面存在局限性，特别是在非典型颜色和跨语言场景中，需要改进以更好地匹配人类颜色感知。


### [143] [EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model](https://arxiv.org/abs/2509.22527)
*Andrii Litvynchuk,Ivan Livinsky,Anand Ravi,Nima Kalantari,Andrii Tsarov*

Main category: cs.CV

TL;DR: 提出EfficientDepth系统，结合transformer架构和轻量卷积解码器，通过双峰密度头和LPIPS损失函数实现高效的单目深度估计，在保持几何一致性和细节的同时显著降低计算资源需求。

- Motivation: 现有单目深度估计方法在3D重建和视图合成中难以同时满足几何一致性、细节保留、对反射表面等现实挑战的鲁棒性以及边缘设备效率等关键要求。
- Method: 使用transformer架构与轻量卷积解码器结合，引入双峰密度头进行深度图估计，采用合成和真实图像混合训练，结合伪标签和多阶段优化策略，并引入基于LPIPS的损失函数。
- Result: 实验结果表明EfficientDepth在性能上达到或超过现有最先进模型，同时显著减少计算资源需求。
- Conclusion: EfficientDepth系统有效解决了单目深度估计中的关键挑战，在保持高性能的同时实现了计算效率的显著提升。


### [144] [Category Discovery: An Open-World Perspective](https://arxiv.org/abs/2509.22542)
*Zhenqi He,Yuanpei Liu,Kai Han*

Main category: cs.CV

TL;DR: 这是一篇关于类别发现任务的综述论文，系统回顾了该领域的文献，分析了不同方法，并提出了分类体系、关键见解和未来研究方向。

- Motivation: 类别发现是新兴的开放世界学习任务，旨在自动分类包含未见类别实例的未标记数据。该任务近年来受到广泛关注，需要系统梳理现有方法。
- Method: 提出分类体系，区分新颖类别发现和广义类别发现等设置；详细分析表示学习、标签分配和类别数量估计三个核心组件；对所有方法进行基准测试。
- Result: 发现大规模预训练骨干网络、层次和辅助线索、课程式训练都有利于类别发现；标签分配设计、类别数量估计和多目标复杂场景仍面临挑战。
- Conclusion: 总结了文献中的关键见解，指出了有前景的未来研究方向，并创建了类别发现文献的动态综述资源。


### [145] [HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection](https://arxiv.org/abs/2509.22544)
*Mohammad Mahdi Hemmatyar,Mahdi Jafari,Mohammad Amin Yousefi,Mohammad Reza Nemati,Mobin Azadani,Hamid Reza Rastad,Amirmohammad Akbari*

Main category: cs.CV

TL;DR: HyCoVAD是一个混合SSL-LLM模型，结合自监督学习和大型语言模型来检测复杂视频异常，在ComplexVAD数据集上达到72.5%的帧级AUC，比现有基线提升12.5%

- Motivation: 现有方法难以检测复杂异常，SSL方法缺乏语义理解能力，而LLM方法计算成本高且缺乏细粒度空间定位
- Method: 使用nnFormer骨干网络构建多任务SSL时序分析器识别可疑帧，然后通过LLM进行基于规则的结构化推理验证异常
- Result: 在ComplexVAD数据集上实现72.5%帧级AUC，比基线提升12.5%，同时减少了LLM计算量
- Conclusion: HyCoVAD成功结合SSL和LLM的优势，为复杂视频异常检测提供了有效解决方案


### [146] [JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation](https://arxiv.org/abs/2509.22548)
*Shuang Zeng,Dekang Qi,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Shiyi Liang,Mu Xu,Xing Wei*

Main category: cs.CV

TL;DR: JanusVLN提出了一种新颖的视觉语言导航框架，采用双隐式神经记忆系统来分别建模空间几何和视觉语义记忆，解决了传统方法中的空间信息损失、计算冗余和内存膨胀问题。

- Motivation: 传统VLN方法依赖显式语义记忆（如文本认知地图或历史视觉帧），导致空间信息损失、计算冗余和内存膨胀，阻碍了高效导航。受人类导航中隐式场景表示的启发，提出了双隐式神经记忆框架。
- Method: 扩展MLLM以整合来自空间几何编码器的3D先验知识，增强仅基于RGB输入的空间推理能力。构建空间几何和视觉语义编码器的历史键值缓存作为双隐式记忆，仅保留初始和滑动窗口中的token KVs以避免冗余计算。
- Result: 在广泛实验中，JanusVLN超越了20多种最新方法，达到SOTA性能。与使用多种数据类型输入的方法相比，成功率提高了10.5-35.5；与使用更多RGB训练数据的方法相比，提高了3.6-10.8。
- Conclusion: 提出的双隐式神经记忆作为一种新范式，为未来VLN研究探索了有前景的新方向。


### [147] [SpikeMatch: Semi-Supervised Learning with Temporal Dynamics of Spiking Neural Networks](https://arxiv.org/abs/2509.22581)
*Jini Yang,Beomseok Oh,Seungryong Kim,Sunok Kim*

Main category: cs.CV

TL;DR: SpikeMatch是首个针对脉冲神经网络(SNNs)的半监督学习框架，利用SNNs的时间动态特性通过协同训练生成多样化伪标签，在有限标签下有效缓解确认偏差。

- Motivation: 脉冲神经网络因其生物合理性和能效优势受到关注，但与人工神经网络相比，SNNs的半监督学习方法研究不足，需要开发专门针对SNN特性的SSL框架。
- Method: 利用SNNs泄漏因子的时间动态特性，在协同训练框架中通过单个SNN的多个预测一致性生成可靠伪标签，从弱增强的无标签样本生成伪标签来训练强增强样本。
- Result: 实验表明SpikeMatch在各种标准基准测试中优于现有适应SNN骨干网络的SSL方法。
- Conclusion: SpikeMatch成功展示了利用SNNs时间动态特性进行半监督学习的有效性，为SNNs的SSL研究开辟了新方向。


### [148] [Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting](https://arxiv.org/abs/2509.22615)
*Yasmine Omri,Connor Ding,Tsachy Weissman,Thierry Tambe*

Main category: cs.CV

TL;DR: 该论文探索使用2D高斯泼溅(2DGS)作为视觉语言模型的替代视觉表示，以解决RGB图像传输和注意力机制的低效问题。

- Motivation: 解决传统RGB视觉编码器在边缘设备到云端传输时的能耗成本高，以及基于patch的tokenization导致序列长度爆炸的问题。
- Method: 开发可扩展的2DGS流水线，包括结构化初始化、亮度感知剪枝和批量CUDA内核；通过轻量级splat感知输入主干和感知器重采样器，将对比语言图像预训练(CLIP)适配到2DGS表示。
- Result: 相比先前实现，实现了90倍以上的拟合加速和约97%的GPU利用率；在DataComp子集上，GS编码器在压缩输入3-20倍的同时，获得了有意义的零样本ImageNet-1K性能。
- Conclusion: 2DGS被确立为可行的多模态基底，虽然当前精度落后于RGB编码器，但为开发既语义强大又传输高效的边缘云学习表示开辟了道路。


### [149] [LongLive: Real-time Interactive Long Video Generation](https://arxiv.org/abs/2509.22622)
*Shuai Yang,Wei Huang,Ruihang Chu,Yicheng Xiao,Yuyang Zhao,Xianbang Wang,Muyang Li,Enze Xie,Yingcong Chen,Yao Lu,Song Han,Yukang Chen*

Main category: cs.CV

TL;DR: LongLive是一个帧级自回归框架，用于实时交互式长视频生成，通过KV重缓存机制、流式长调优和帧级注意力汇等技术，在保持高质量的同时实现高效生成。

- Motivation: 解决长视频生成在效率和交互性方面的挑战：扩散模型效率低，因果注意力模型在长视频上质量下降，同时需要支持实时交互式提示输入。
- Method: 采用因果帧级自回归设计，集成KV重缓存机制实现平滑提示切换，流式长调优对齐训练和推理，短窗口注意力配合帧级注意力汇保持长程一致性。
- Result: 仅用32 GPU天将1.3B参数的短片段模型微调至分钟级生成，在单H100 GPU上达到20.7 FPS，支持240秒视频生成，INT8量化推理质量损失极小。
- Conclusion: LongLive成功解决了长视频生成的效率和质量平衡问题，支持实时交互式生成，在VBench基准测试中表现优异。


### [150] [SPARK: Synergistic Policy And Reward Co-Evolving Framework](https://arxiv.org/abs/2509.22624)
*Ziyu Liu,Yuhang Zang,Shengyuan Ding,Yuhang Cao,Xiaoyi Dong,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: SPARK是一个高效的协同进化框架，通过回收RLVR中的rollouts和正确性数据来同时训练模型作为生成式奖励模型，无需单独奖励模型或人类偏好数据。

- Motivation: 解决RLHF成本高和奖励-策略不匹配问题，以及RLVR丢弃有价值监督信号的问题。
- Method: 在RLVR基础上，回收rollouts和正确性数据，使用点对点奖励评分、成对比较和反思条件评估等目标来训练模型自我评估和改进。
- Result: SPARK在多个LLM和LVLM模型上取得显著性能提升，如SPARK-VL-7B在7个推理基准上平均提升9.7%，在2个奖励基准上提升12.1%，在8个通用基准上提升1.5%。
- Conclusion: SPARK创建了正反馈循环，支持无需外部奖励模型的自反思测试时扩展，展示了鲁棒性和广泛泛化能力。


### [151] [CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach](https://arxiv.org/abs/2509.22627)
*Alexandre Lopes,Roberto Souza,Helio Pedrini*

Main category: cs.CV

TL;DR: 提出CCNeXt架构，一种自监督的深度估计方法，在保持计算效率的同时超越现有CNN和ViT模型

- Motivation: 在机器人、自动驾驶和增强现实等计算资源受限的场景中，需要高效的深度估计方法。立体图像对提供了解方案，但获取可靠的深度真值数据困难，因此自监督技术成为重要研究方向
- Method: CCNeXt架构采用现代CNN特征提取器，在编码器中引入新颖的窗口化极线交叉注意力模块，并全面重新设计了深度估计解码器
- Result: 在KITTI Eigen Split测试数据上达到竞争性指标，比当前最佳模型快10.18倍，在KITTI Eigen Split改进真值和Driving Stereo数据集的所有指标上达到最先进水平
- Conclusion: CCNeXt在深度估计任务中实现了性能与计算效率的良好平衡，为计算资源受限的应用场景提供了有效的解决方案


### [152] [UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning](https://arxiv.org/abs/2509.22628)
*Hongyu Chen,Guangrun Wang*

Main category: cs.CV

TL;DR: UML-CoT是一个结构化推理框架，使用统一建模语言(UML)生成符号思维链和可执行动作计划，在杂乱房间清洁任务中表现优于非结构化CoT。

- Motivation: 传统思维链(CoT)提示依赖非结构化文本，在具身任务中可解释性和可执行性有限；现有结构化CoT方法只能建模低阶关系，缺乏继承、行为抽象等构造，也没有顺序或条件规划的标准语义。
- Method: 使用UML类图捕捉组合对象语义，活动图建模过程控制流；采用三阶段训练管道，结合监督微调和组相对策略优化(GRPO)，包括从仅答案数据中学习奖励。
- Result: 在MRoom-30k基准测试中，UML-CoT在可解释性、规划连贯性和执行成功率方面优于非结构化CoT。
- Conclusion: UML作为一种更具表达力和可操作性的结构化推理形式，在具身推理任务中表现出色。


### [153] [LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision](https://arxiv.org/abs/2509.22631)
*Debargha Ganguly,Sumit Kumar,Ishwar Balappanawar,Weicong Chen,Shashank Kambhatla,Srinivasan Iyengar,Shivkumar Kalyanaraman,Ponnurangam Kumaraguru,Vipin Chaudhary*

Main category: cs.CV

TL;DR: Labeling Copilot是一个用于计算机视觉的数据策展深度研究代理，通过多模态大语言模型驱动的中央协调器，提供校准发现、可控合成和共识标注三大核心功能，能够高效地从大规模数据湖中策展高质量领域特定数据集。

- Motivation: 构建高质量领域特定数据集是部署稳健视觉系统的主要瓶颈，需要在数据质量、多样性和成本之间进行复杂权衡。现有方法在处理大规模未标注数据湖时面临效率挑战。
- Method: 采用基于多模态大语言模型的中央协调器进行多步推理，执行三大核心工具：校准发现从大型存储库中获取相关数据；可控合成生成罕见场景的新数据；共识标注通过结合非极大值抑制和投票的新颖共识机制协调多个基础模型产生准确标签。
- Result: 在密集COCO数据集上，共识标注模块平均每张图像产生14.2个候选建议（接近真实对象7.4个的两倍），最终标注mAP达到37.1%。在Open Images数据集上发现了903个新的边界框类别，总能力扩展到1500多个类别。校准发现在1000万样本规模测试中，主动学习策略比替代方法计算效率高40倍。
- Conclusion: 具有优化、可扩展工具的代理工作流为策展工业规模数据集提供了稳健基础，证明了Labeling Copilot组件在数据策展任务中的有效性。


### [154] [Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance](https://arxiv.org/abs/2509.22635)
*Luc Boudier,Loris Manganelli,Eleftherios Tsonis,Nicolas Dufour,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: DIPSY是一种无需训练的小样本图像分类方法，利用IP-Adapter进行图像到图像转换，通过正负图像条件控制和类相似性采样策略生成判别性合成图像。

- Motivation: 解决小样本图像分类中标记数据有限的问题，避免现有方法需要模型微调或外部信息源的依赖。
- Method: 1) 扩展的无分类器引导方案实现正负图像条件独立控制；2) 基于类相似性的采样策略识别有效对比样本；3) 无需模型微调或外部标注过滤的简单流程。
- Result: 在十个基准数据集上达到最先进或可比性能，无需生成模型适应或依赖外部工具进行标题生成和图像过滤。
- Conclusion: 通过正负引导的双重图像提示生成类判别特征的方法特别适用于细粒度分类任务，证明了其有效性。


### [155] [Scale-Wise VAR is Secretly Discrete Diffusion](https://arxiv.org/abs/2509.22636)
*Amandeep Kumar,Nithin Gopalakrishnan Nair,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文揭示了自回归视觉变换器(VAR)与马尔可夫注意力掩码结合时，在数学上等价于离散扩散模型，建立了AR变换器与扩散模型的理论桥梁。

- Motivation: 重新审视VAR模型，发现其与扩散模型的理论联系，旨在将扩散模型的优势(如迭代精炼)引入VAR，提高效率和生成质量。
- Method: 提出SRDD框架，通过马尔可夫注意力掩码将VAR重新解释为离散扩散过程，从而可以直接引入扩散模型的优势。
- Result: 在多个数据集上验证，基于扩散视角的VAR在效率和生成质量方面都取得了持续提升，实现了更快的收敛、更低的推理成本和更好的零样本重建。
- Conclusion: 建立了AR变换器与扩散模型之间的理论桥梁，为视觉生成提供了新的视角，显著提升了VAR模型的性能和效率。


### [156] [Hierarchical Representation Matching for CLIP-based Class-Incremental Learning](https://arxiv.org/abs/2509.22645)
*Zhen-Hao Wen,Yan Wang,Ji Feng,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.CV

TL;DR: HERMAN方法通过利用LLM生成层次化文本描述符，增强CLIP在类增量学习中的语义空间，通过分层表示匹配缓解灾难性遗忘问题。

- Motivation: 现有CLIP方法使用简单模板（如"a photo of a [CLASS]"）忽略了视觉概念的层次性，且仅使用最后一层特征，忽视了早期层的层次信息。
- Method: 利用LLM递归生成区分性文本描述符，将这些描述符匹配到语义层次的不同层级，并根据任务需求自适应路由。
- Result: 在多个基准测试上的广泛实验表明，该方法持续达到最先进的性能。
- Conclusion: HERMAN通过分层表示匹配有效提升了CLIP在类增量学习中的性能，同时缓解了灾难性遗忘问题。


### [157] [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](https://arxiv.org/abs/2509.22646)
*Xingyu Fu,Siyi Liu,Yinuo Xu,Pan Lu,Guangqiuse Hu,Tianbo Yang,Taran Anantasagar,Christopher Shen,Yikai Mao,Yuanzhe Liu,Keyush Shah,Chung Un Lee,Yejin Choi,James Zou,Dan Roth,Chris Callison-Burch*

Main category: cs.CV

TL;DR: DeeptraceReward是首个细粒度、时空感知的基准数据集，用于标注人类感知的AI生成视频伪影痕迹，包含4.3K详细标注，训练的多模态语言模型在识别、定位和解释伪影方面优于GPT-5 34.7%。

- Motivation: 虽然视频生成模型快速发展，但人类是否能检测深度伪造痕迹这一关键维度被忽视。研究旨在填补这一空白，关注人类可感知的时空视觉伪影。
- Method: 构建包含4.3K详细标注的数据集，每个标注包含自然语言解释、边界框区域和精确时间戳。将标注整合为9类深度伪造痕迹，训练多模态语言模型作为奖励模型模仿人类判断和定位。
- Result: 7B奖励模型在伪影识别、定位和解释方面平均优于GPT-5 34.7%。发现难度梯度：二分类最容易，细粒度检测较难，其中自然语言解释最简单，空间定位次之，时间标注最难。
- Conclusion: 通过突出人类感知的深度伪造痕迹，DeeptraceReward为社会意识和可信赖的视频生成提供了严格的测试平台和训练信号。


### [158] [CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning](https://arxiv.org/abs/2509.22647)
*Long Xing,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jianze Liang,Qidong Huang,Jiaqi Wang,Feng Wu,Dahua Lin*

Main category: cs.CV

TL;DR: 提出CapRL框架，使用强化学习与可验证奖励来训练图像描述模型，通过非视觉语言模型基于描述回答问题的准确性来评估描述质量，显著提升了图像描述性能。

- Motivation: 当前最先进的图像描述模型依赖监督微调，需要昂贵的人工标注数据，导致模型记忆特定答案而缺乏泛化能力和多样性。需要克服SFT的局限性。
- Method: 提出Captioning Reinforcement Learning (CapRL)框架，采用解耦的两阶段流程：LVLM生成描述，然后基于描述让无视觉的LLM回答多项选择题，根据回答准确率获得客观奖励。
- Result: 在CapRL-5M数据集上预训练后，在12个基准测试中取得显著提升。在Prism框架下，性能与Qwen2.5-VL-72B相当，比基线平均提高8.4%。
- Conclusion: CapRL是首个将RLVR应用于主观图像描述任务的研究，通过实用性重新定义描述质量，有效提升了模型的泛化能力和描述质量。


### [159] [RefAM: Attention Magnets for Zero-Shot Referral Segmentation](https://arxiv.org/abs/2509.22650)
*Anna Kukleva,Enis Simsar,Alessio Tonioni,Muhammad Ferjad Naeem,Federico Tombari,Jan Eric Lenssen,Bernt Schiele*

Main category: cs.CV

TL;DR: 提出RefAM框架，无需训练即可利用扩散变换器的注意力特征进行指代分割，通过处理停用词和全局注意力汇来提升性能

- Motivation: 现有指代分割方法通常需要微调或多模型组合，而大规模生成扩散模型包含丰富语义信息，可作为通用特征提取器
- Method: 利用扩散变换器的注意力分数，识别停用词作为注意力磁铁，处理全局注意力汇，提出注意力重分配策略，开发无需训练的RefAM框架
- Result: 在零样本指代图像和视频分割基准测试中持续优于先前方法，无需微调或额外组件即达到新的最先进水平
- Conclusion: 扩散变换器的注意力特征可直接用于下游任务，通过系统处理停用词和全局注意力汇可显著提升指代分割性能
## cs.CR

### [160] [Guidance Watermarking for Diffusion Models](https://arxiv.org/abs/2509.22126)
*Enoal Gesny,Eva Giboulot,Teddy Furon,Vivien Chappelier*

Main category: cs.CR

TL;DR: 提出了一种基于梯度引导的扩散模型水印方法，可将后处理水印方案转换为生成过程中的嵌入，无需重新训练或微调。

- Motivation: 将现有的后处理水印方案转换为在扩散生成过程中嵌入水印，提高水印的鲁棒性。
- Method: 使用现成水印解码器计算的梯度来引导扩散过程，包含多种图像增强操作以提高抗攻击能力。
- Result: 该方法在不同扩散模型和检测器上验证有效，水印引导不会显著改变生成图像的质量和多样性。
- Conclusion: 该方法与变分自编码器修改技术互补，为扩散模型提供了一种有效的水印嵌入方案。
## cs.GR

### [161] [SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment](https://arxiv.org/abs/2509.20401)
*Binod Singh,Sayan Deb Sarkar,Iro Armeni*

Main category: cs.GR

TL;DR: SGAligner++是一个跨模态、语言辅助的3D场景图对齐框架，通过统一联合嵌入空间解决异构模态下部分重叠场景观测的对齐问题，在低重叠和传感器噪声条件下仍能实现准确对齐。

- Motivation: 当前3D场景图对齐方法主要依赖单模态点云数据，难以处理不完整或噪声输入。需要开发能够处理异构模态、部分重叠场景观测的对齐方法。
- Method: 采用轻量级单模态编码器和基于注意力的融合机制，学习统一的联合嵌入空间，实现跨模态场景图对齐。
- Result: 在真实世界数据集上的广泛评估显示，SGAligner++在噪声真实世界重建任务上比现有最优方法性能提升高达40%，同时实现了跨模态泛化能力。
- Conclusion: SGAligner++通过跨模态语言辅助框架有效解决了3D场景图对齐中的挑战，为视觉定位、3D重建和导航等任务提供了增强的场景理解能力，同时保证了可扩展性和低计算开销。


### [162] [ControlHair: Physically-based Video Diffusion for Controllable Dynamic Hair Rendering](https://arxiv.org/abs/2509.21541)
*Weikai Lin,Haoxiang Li,Yuhao Zhu*

Main category: cs.GR

TL;DR: ControlHair是一个混合框架，结合物理模拟器和条件视频扩散模型，实现可控的动态头发渲染。

- Motivation: 现有视频扩散模型缺乏对头发动态的精细控制，而头发模拟和渲染面临复杂的动力学、材料属性和光照交互挑战。
- Method: 采用三阶段流程：1) 将物理参数编码为每帧几何；2) 提取每帧控制信号；3) 将控制信号输入视频扩散模型生成视频。
- Result: 在10K视频数据集上训练，ControlHair优于基于文本和姿态的基线方法，能精确控制头发动态。
- Conclusion: ControlHair是首个基于物理信息的视频扩散框架，支持动态发型试戴、子弹时间效果和电影图形等应用。


### [163] [Rigidity-Aware 3D Gaussian Deformation from a Single Image](https://arxiv.org/abs/2509.22222)
*Jinhyeok Kim,Jaehun Bang,Seunghyun Seo,Kyungdon Joo*

Main category: cs.GR

TL;DR: DeformSplat：从单张图像重建物体变形的创新框架，通过高斯到像素匹配和刚性部分分割技术，解决了现有方法依赖多视角视频的限制。

- Motivation: 现有方法通常依赖多视角视频来恢复变形，这在受限场景下适用性有限。本文旨在从单张图像重建物体变形，突破这一限制。
- Method: 提出两个关键技术：1）高斯到像素匹配，弥合3D高斯表示与2D像素观测之间的领域差距；2）刚性部分分割（初始化和细化），明确识别刚性区域以保持几何一致性。
- Result: 实验表明，该方法显著优于现有方法，并能自然扩展到帧插值和交互式物体操作等多种应用。
- Conclusion: DeformSplat框架能够仅从单张图像重建一致的变形，为计算机视觉和图形学中的变形重建问题提供了有效的解决方案。


### [164] [Aerial Path Planning for Urban Geometry and Texture Co-Capture](https://arxiv.org/abs/2509.22227)
*Weidan Xiong,Bochuan Zeng,Ziyu Hu,Jianwei Guo,Ke Xie,Hui Huang*

Main category: cs.GR

TL;DR: 提出一种创新的无人机路径规划框架，用于同时采集城市几何结构和高质量纹理图像，解决现有技术中纹理质量不足的问题。

- Motivation: 当前城市场景重建技术往往忽视纹理质量，导致纹理模型出现明显视觉伪影。需要在有限的先验知识下，同时获取高质量几何结构和纹理。
- Method: 使用2D建筑轮廓图和安全飞行高度作为输入，生成高质量垂直俯视和水平平面视图。采用多目标优化策略最大化纹理保真度、几何精度，并最小化航拍成本。提出顺序路径规划算法考虑纹理一致性。
- Result: 在大规模合成和真实城市数据集上的实验表明，该方法能有效生成适合同时进行几何和纹理重建的图像集。
- Conclusion: 该方法能够以较低操作成本创建逼真的纹理场景代理，解决了城市几何和纹理协同采集的问题。
## cs.MM

### [165] [Perception-Consistency Multimodal Large Language Models Reasoning via Caption-Regularized Policy Optimization](https://arxiv.org/abs/2509.21854)
*Songjun Tu,Qichao Zhang,Jingbo Sun,Yuqian Fu,Linjing Li,Xiangyuan Lan,Dongmei Jiang,Yaowei Wang,Dongbin Zhao*

Main category: cs.MM

TL;DR: 提出Caption-Regularized Policy Optimization (CapPO)框架，通过字幕一致性正则化和KL加权优势估计来解决多模态大语言模型中视觉感知错误传播的问题，显著提升数学和通用推理任务的性能。

- Motivation: 多模态大语言模型在整合视觉感知和符号推理时存在关键脆弱性：感知诱导的错误会在推理链中传播。现有的强化学习微调方法未能解决视觉基础与后续推理过程之间的错位问题。
- Method: CapPO框架包含两个关键机制：(1) 基于字幕的一致性正则化，最小化原始图像和字幕条件下的响应差异；(2) KL加权优势估计方案，自适应缩放强化信号以增强感知一致性轨迹。
- Result: 在5个数学推理和5个通用推理基准测试中，CapPO在基础Qwen2.5-VL-7B模型上实现了+6.0%的数学任务准确率提升和+2.4%的通用推理任务提升。消融研究证实了各组件有效性，错误分析显示CapPO显著减少了感知相关错误。
- Conclusion: CapPO为改进多模态推理提供了一个简单而有效的框架，通过强制感知一致性来提升模型性能。
## cs.LG

### [166] [Are Hallucinations Bad Estimations?](https://arxiv.org/abs/2509.21473)
*Hude Liu,Jerry Yao-Chieh Hu,Jennifer Yuntong Zhang,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: 该论文将生成模型中的幻觉定义为估计结果无法与任何合理原因关联的失败，并证明即使是最优损失最小化估计器仍会产生幻觉。

- Motivation: 重新定义幻觉现象，揭示损失最小化与人类可接受输出之间的结构性错位问题。
- Method: 通过理论分析建立高概率下幻觉率的下界，并在硬币聚合、开放式问答和文生图任务中进行实验验证。
- Result: 证明即使是损失最小化的最优估计器也会产生幻觉，幻觉率存在理论下界。
- Conclusion: 幻觉本质上是损失最小化与人类期望输出之间的校准错误，而非简单的模型缺陷。


### [167] [VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations](https://arxiv.org/abs/2509.21477)
*Yuan Gao,Hao Wu,Qingsong Wen,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出了VISION模型，通过动态提示机制从部分海面观测数据重建海洋次表层动力学，并在新发布的KD48基准上取得优异性能。

- Motivation: 解决从不完备海面观测重建海洋次表层动力学（如垂直速度场）的挑战，该领域长期缺乏标准化基准。
- Method: 基于动态提示的新重建范式，通过从可用观测数据生成视觉提示，并使用状态条件提示模块将其注入具有几何和尺度感知算子的通用骨干网络。
- Result: 在KD48基准上的大量实验表明，VISION显著优于最先进模型，在极端数据缺失场景下表现出强泛化能力。
- Conclusion: 通过提供高质量基准和鲁棒模型，为数据不确定性下的海洋科学研究建立了坚实基础。


### [168] [SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models](https://arxiv.org/abs/2509.21498)
*Arani Roy,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: SlimDiff是一个无需梯度的扩散模型压缩框架，通过激活信息指导的结构化压缩，在保持生成质量的同时显著减少参数和加速推理。

- Motivation: 扩散模型虽然生成性能优秀，但存在计算开销大、参数规模庞大的问题。现有的效率优化技术需要微调或重新训练来恢复性能，存在瓶颈。
- Method: 将扩散模型压缩重构为谱近似任务，利用激活协方差定义低秩子空间，在固定压缩预算下进行动态剪枝。采用模块化分解方法处理功能权重组，自适应分配稀疏度。
- Result: 相比基线方法实现35%加速和约1亿参数减少，生成质量与未压缩模型相当，仅需约500个校准样本（比先前方法少70倍以上）。
- Conclusion: 这是第一个无需训练、基于闭式解的激活引导结构化压缩方法，为扩散模型压缩提供了理论清晰性和实际效率。


### [169] [DistillKac: Few-Step Image Generation via Damped Wave Equations](https://arxiv.org/abs/2509.21513)
*Weiqiao Han,Chenlin Meng,Christopher D. Manning,Stefano Ermon*

Main category: cs.LG

TL;DR: DistillKac是一种快速图像生成器，使用阻尼波动方程及其随机Kac表示，以有限速度移动概率质量。相比扩散模型，它强制执行有限速度传输，并引入速度空间的分类器自由引导和端点蒸馏训练。

- Motivation: 解决扩散模型中反向时间速度可能变得僵硬且隐含允许无限传播速度的问题，通过Kac动力学强制执行有限速度传输并获得全局有界动能。
- Method: 使用阻尼波动方程和随机Kac表示，引入速度空间的分类器自由引导，并提出端点蒸馏训练，让学生模型在长区间内匹配冻结的教师模型。
- Result: 实验表明DistillKac能够以极少的函数评估次数生成高质量样本，同时保持有限速度概率流的数值稳定性优势。
- Conclusion: DistillKac通过有限速度概率流实现了高效稳定的图像生成，在保持数值稳定性的同时大幅减少了计算需求。


### [170] [TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning](https://arxiv.org/abs/2509.21526)
*Hongyang He,Xinyuan Song,Yangfan He,Zeyu Zhang,Yanshu Li,Haochen You,Lifan Sun,Wenqiao Zhang*

Main category: cs.LG

TL;DR: TRiCo是一个新颖的三元博弈论协同训练框架，通过将教师、两个学生和一个对抗生成器整合到统一训练范式中，重新思考半监督学习的结构。

- Motivation: 解决现有半监督学习框架的关键限制，如静态视图交互、不可靠的伪标签和缺乏困难样本建模，提供原则性和可推广的解决方案。
- Method: 将SSL形式化为三个角色的结构化交互：两个在冻结互补表示上训练的学生分类器、通过验证反馈自适应调节伪标签选择和损失平衡的元学习教师，以及扰动嵌入以揭示决策边界弱点的非参数生成器。使用互信息而非置信度选择伪标签。
- Result: 在CIFAR-10、SVHN、STL-10和ImageNet上的广泛实验表明，TRiCo在低标签机制下始终达到最先进的性能，同时保持架构无关性并与冻结视觉骨干兼容。
- Conclusion: TRiCo通过三元博弈论协同训练框架为半监督学习提供了原则性和可推广的解决方案，在各种基准数据集上实现了最先进的性能。


### [171] [Closing the Oracle Gap: Increment Vector Transformation for Class Incremental Learning](https://arxiv.org/abs/2509.21898)
*Zihuan Qiu,Yi Xu,Fanman Meng,Runtong Zhang,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TL;DR: 提出IVT框架，通过保持与先前任务最优解的线性连接来缓解类增量学习中的灾难性遗忘问题

- Motivation: 发现oracle解通常与先前任务最优解保持低损失的线性连接，这为解决灾难性遗忘提供了新思路
- Method: IVT框架周期性地将模型参数传送到保持与先前任务最优解线性连接的变换解，使用对角Fisher信息矩阵高效近似变换
- Result: 在多个数据集上显著提升CIL基线性能，CIFAR-100上PASS基线最终准确率提升+5.12%，FGVCAircraft上SLCA基线平均准确率提升+14.93%
- Conclusion: IVT是一种有效的即插即用框架，能持续增强强CIL基线的性能，适用于无示例和基于示例的场景


### [172] [Enriching Knowledge Distillation with Intra-Class Contrastive Learning](https://arxiv.org/abs/2509.22053)
*Hua Yuan,Ning Xu,Xin Geng,Yong Rui*

Main category: cs.LG

TL;DR: 提出在教师模型训练中引入类内对比损失来丰富软标签中的类内信息，并通过整合边界损失来解决训练不稳定和收敛慢的问题。

- Motivation: 现有知识蒸馏方法中，教师模型主要遵循真实标签作为目标，没有考虑同一类别内的多样化表示，导致软标签中的类内信息不足。
- Method: 在教师训练中加入类内对比损失来增强软标签的类内多样性，同时整合边界损失以提高训练稳定性和收敛速度。
- Result: 理论分析证明类内对比损失能丰富类内多样性，实验结果表明所提方法有效。
- Conclusion: 通过类内对比损失和边界损失的结合，成功提升了知识蒸馏中软标签的类内信息质量，改善了学生模型的泛化能力。


### [173] [Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data](https://arxiv.org/abs/2509.22507)
*Zahid Iqbal*

Main category: cs.LG

TL;DR: 提出了三种联邦学习方法：DL-SH处理统计异构性，DL-MH处理模型异构性，I-DL-MH在DL-MH基础上加入激励机制。这些方法在非IID数据下显著提升准确率并降低通信成本。

- Motivation: 解决联邦学习中的三个关键挑战：客户端模型训练能力差异、统计异构性导致性能下降、缺乏有效的激励机制促进客户端参与。
- Method: DL-SH：处理统计异构性的高效隐私保护通信方法；DL-MH：管理完全异构模型并解决统计差异；I-DL-MH：在DL-MH基础上加入激励机制。
- Result: 实验显示DL-SH将全局模型准确率提升153%，I-DL-MH在非IID条件下提升225%，同时显著降低通信成本。
- Conclusion: 提出的方法有效解决了联邦学习中的统计异构性和模型异构性问题，通过激励机制促进了客户端参与，在准确率和通信效率方面优于现有方法。


### [174] [JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation](https://arxiv.org/abs/2509.22522)
*Guillem Capellera,Luis Ferraz,Antonio Rubio,Alexandre Alahi,Antonio Agudo*

Main category: cs.LG

TL;DR: JointDiff是一个新颖的扩散框架，统一生成连续时空数据和同步离散事件，在体育领域验证了其有效性，实现了最先进的性能。

- Motivation: 生成模型通常将连续数据和离散事件视为独立过程，这限制了在复杂系统中建模它们同步交互的能力。
- Method: 提出JointDiff扩散框架，同时生成连续时空数据和同步离散事件；引入CrossGuid条件操作支持多智能体领域的条件生成；创建增强文本描述的统一体育基准数据集。
- Result: 在不可控生成和两种可控生成场景（弱持有者引导和文本引导）中验证了联合建模的有效性，实现了最先进的性能。
- Conclusion: 联合建模对于构建真实且可控的交互系统生成模型至关重要。


### [175] [Activation Function Design Sustains Plasticity in Continual Learning](https://arxiv.org/abs/2509.22562)
*Lute Lillo,Nick Cheney*

Main category: cs.LG

TL;DR: 激活函数选择是持续学习中缓解塑性损失的关键因素，作者提出了两种新的非线性函数（Smooth-Leaky和Randomized Smooth-Leaky）来维持模型在非平稳环境中的适应能力。

- Motivation: 在持续学习场景中，模型不仅面临灾难性遗忘问题，还会逐渐失去适应能力（塑性损失），而激活函数在这种失效模式中的作用尚未充分探索。
- Method: 基于负分支形状和饱和行为的属性分析，引入两种即插即用的非线性函数，并在监督类增量学习和强化学习的非平稳环境中进行评估。
- Result: 研究表明，精心设计的激活函数能够在不增加额外容量或任务特定调优的情况下，持续维持模型的塑性。
- Conclusion: 激活函数设计提供了一种轻量级、领域通用的方法来维持持续学习中的塑性，是缓解塑性损失的主要架构无关手段。


### [176] [Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.22601)
*Yulei Qin,Xiaoyu Tan,Zhengbao He,Gang Li,Haojia Lin,Zongyi Li,Zihan Xu,Yuchen Shi,Siqi Cai,Renting Rui,Shaofei Cai,Yuzheng Cai,Xuan Zhang,Sheng Ye,Ke Li,Xing Sun*

Main category: cs.LG

TL;DR: 提出SPEAR方法，通过课程式自模仿学习平衡强化学习中的探索与利用，避免熵崩溃或发散问题

- Motivation: 现有RL方法通过策略熵来刺激探索，但在多轮分布偏移下容易导致训练不稳定，需要更平衡的探索-利用策略
- Method: SPEAR方法：1) 课程管理探索过程；2) 内在奖励促进技能级探索；3) 自模仿学习促进动作级探索；4) 重校准经验优势；5) 正则化控制轨迹级熵
- Result: 该方法能够在训练过程中保持熵的平衡范围，既实现广泛的工具使用技能积累，又加速解决方案迭代
- Conclusion: SPEAR通过课程式自模仿学习有效平衡了探索与利用，解决了RL训练中的稳定性问题
## cs.CL

### [177] [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
*Ke Wang,Houxing Ren,Zimu Lu,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出了VoiceAssistant-Eval基准测试，用于全面评估AI语音助手在听、说、看三方面的能力，包含10,497个测试用例，涵盖13个任务类别。

- Motivation: 现有基准测试无法充分评估大型语言模型和多模态系统的完整能力，需要更全面的评估框架来指导下一代AI助手的发展。
- Method: 构建包含10,497个精心策划示例的基准测试，涵盖自然声音、音乐、对话等听力任务，多轮对话、角色扮演等说话任务，以及多样化图像的视觉任务。评估了21个开源模型和GPT-4o-Audio。
- Result: 发现三个关键结果：1) 专有模型并非全面优于开源模型；2) 大多数模型擅长说话任务但在音频理解方面落后；3) 设计良好的小型模型可以媲美大型模型。Step-Audio-2-mini(7B)的听力准确率是LLaMA-Omni2-32B-Bilingual的两倍多。
- Conclusion: 当前模型在多模态输入和角色扮演声音模仿任务上仍有困难，在鲁棒性和安全对齐方面存在显著差距。VoiceAssistant-Eval为评估和指导下一代AI助手发展建立了严格框架。
## cs.IR

### [178] [Cross-Modal Retrieval with Cauchy-Schwarz Divergence](https://arxiv.org/abs/2509.21339)
*Jiahao Zhang,Wenzhe Yin,Shujian Yu*

Main category: cs.IR

TL;DR: 提出基于柯西-施瓦茨散度的跨模态检索方法，通过广义CS散度实现多模态直接对齐，无需成对比较。

- Motivation: 现有跨模态检索方法存在数值不稳定、超参数敏感、无法捕捉完整分布结构等问题，需要更稳健的分布对齐技术。
- Method: 引入柯西-施瓦茨散度作为超参数无关的度量，并提出基于Hölder不等式的广义CS散度，通过双向循环比较实现多模态直接对齐。
- Result: 在六个基准数据集上的实验表明，该方法在双模态和三模态检索任务中均表现有效。
- Conclusion: CS/GCS散度提供了一种稳健且高效的跨模态对齐方法，显著提升了训练稳定性和检索性能。
## cs.RO

### [179] [Language-in-the-Loop Culvert Inspection on the Erie Canal](https://arxiv.org/abs/2509.21370)
*Yashom Dighe,Yash Turkar,Karthik Dantu*

Main category: cs.RO

TL;DR: VISION是一个端到端的自主检查系统，结合视觉语言模型和受限视点规划，用于运河涵洞的自主检查，无需领域特定微调即可生成高质量检查报告。

- Motivation: 传统人工检查运河涵洞面临年龄、几何形状、照明差、天气和难以接近等挑战，需要开发自动化解决方案。
- Method: 使用网络级视觉语言模型生成感兴趣区域建议，融合立体深度恢复尺度，结合涵洞约束的规划器进行重新定位拍摄特写。
- Result: 在伊利运河涵洞的四足机器人部署中，初始感兴趣区域建议与专家达成61.4%一致，重新成像后评估达到80%一致。
- Conclusion: VISION系统能够将初步假设转化为基于事实的专家对齐发现，证明了语言在环自主检查的有效性。


### [180] [RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation](https://arxiv.org/abs/2509.22356)
*Enguang Liu,Siyuan Liang,Liming Lu,Xiyu Zeng,Xiaochun Cao,Aishan Liu,Shuchao Pang*

Main category: cs.RO

TL;DR: 提出了RoboView-Bias基准，首次系统量化机器人操作中的视觉偏见，发现所有智能体都存在显著视觉偏见，其中相机视角是最关键因素，并提出基于语义接地层的缓解策略可将偏见减少约54.5%。

- Motivation: 现有基准主要关注泛化性和鲁棒性，但缺乏对视觉偏见的系统量化，限制了理解感知如何影响决策稳定性。
- Method: 采用因子隔离原则，通过结构化变体生成框架和感知公平验证协议，创建2,127个任务实例来测量单个视觉因素及其交互引起的偏见。
- Result: 评估三个代表性具身智能体发现：(i)所有智能体都有显著视觉偏见，相机视角最关键；(ii)智能体在高饱和度颜色上成功率最高，表明继承了底层VLM的视觉偏好；(iii)视觉偏见存在强不对称耦合，视角会放大颜色相关偏见。
- Conclusion: 视觉偏见的系统分析是开发安全可靠通用具身智能体的前提条件，提出的缓解策略能有效减少视觉偏见。


### [181] [MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data](https://arxiv.org/abs/2509.22573)
*Farida Mohsen,Ali Safa*

Main category: cs.RO

TL;DR: 提出了一种仅使用RGB输入的人机交互意图预测方法，通过合成序列生成和新的损失函数，在帧级精度上实现了最先进的性能。

- Motivation: 现有方法大多依赖多模态输入（如RGB-D），而本文旨在开发仅使用RGB输入的高效人机交互意图检测方法，以实现更快的机器人响应和更好的服务质量。
- Method: 提出了MINT-RVAE合成序列生成方法，结合新的损失函数和训练策略，解决了真实世界数据中的类别不平衡问题，支持帧级精确预测。
- Result: 在AUROC指标上达到0.95，优于先前工作的0.90-0.912，同时仅需RGB输入并支持精确的帧级起始预测。
- Conclusion: 该方法在人机交互意图预测方面实现了最先进的性能，并公开发布了带有帧级标注的新数据集以支持未来研究。


### [182] [WoW: Towards a World omniscient World model Through Embodied Interaction](https://arxiv.org/abs/2509.22642)
*Xiaowei Chi,Peidong Jia,Chun-Kai Fan,Xiaozhu Ju,Weishi Mi,Kevin Zhang,Zhiyuan Qin,Wanxin Tian,Kuangzhi Ge,Hao Li,Zezhong Qian,Anthony Chen,Qiang Zhou,Yueru Jia,Jiaming Liu,Yong Dai,Qingpo Wuwu,Chengyu Bai,Yu-Kai Wang,Ying Li,Lizhang Chen,Yong Bao,Zhiyuan Jiang,Jiacheng Zhu,Kai Tang,Ruichuan An,Yulin Luo,Qiuxuan Feng,Siyuan Zhou,Chi-min Chan,Chengkai Hou,Wei Xue,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: WoW是一个140亿参数的世界模型，通过200万机器人交互轨迹训练，证明大规模真实世界交互对AI物理直觉发展至关重要。

- Motivation: 当前视频模型（如Sora）依赖被动观察，难以理解物理因果关系。假设真实的物理直觉必须基于与真实世界的大量因果丰富的交互。
- Method: 训练WoW模型，使用SOPHIA（视觉语言模型代理）评估DiT生成输出并通过迭代演化语言指令指导精炼，配合逆动力学模型将精炼计划转化为可执行机器人动作。
- Result: WoW在WoWBench基准测试中取得最先进性能，在物理因果关系、碰撞动力学和物体持久性方面表现出色。
- Conclusion: 大规模真实世界交互是发展AI物理直觉的基石，模型、数据和基准将开源。


### [183] [Pixel Motion Diffusion is What We Need for Robot Control](https://arxiv.org/abs/2509.22652)
*E-Ro Nguyen,Yichi Zhang,Kanchana Ranasinghe,Xiang Li,Michael S. Ryoo*

Main category: cs.RO

TL;DR: DAWN是一个基于扩散模型的机器人控制框架，通过结构化像素运动表示连接高层运动意图和底层机器人动作，在CALVIN和MetaWorld基准测试中取得最先进结果，并展示了从仿真到真实环境的可靠迁移能力。

- Motivation: 为了解决机器人控制中高层运动意图与底层动作之间的鸿沟，需要一个统一的、可解释的框架来连接这两个层次，同时实现从仿真到真实环境的有效迁移。
- Method: DAWN将高层和底层控制器都建模为扩散过程，使用结构化像素运动表示作为中间抽象，构建了一个完全可训练、端到端的系统。
- Result: 在CALVIN基准测试中取得最先进结果，表现出强大的多任务性能，在MetaWorld上进一步验证了有效性。尽管仿真与真实环境存在显著领域差距且真实数据有限，仅需最小微调即可实现可靠的真实世界迁移。
- Conclusion: 将扩散建模与运动中心表示相结合，为可扩展和鲁棒的机器人学习提供了一个强大的基准框架，证明了基于扩散的运动抽象在机器人控制中的实际可行性。


### [184] [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
*Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu*

Main category: cs.RO

TL;DR: SPF是一个无需训练的空中视觉语言导航框架，将动作预测视为2D空间定位任务，通过分解语言指令为2D航点，实现无人机在动态环境中的闭环导航。

- Motivation: 现有基于视觉语言模型的方法将动作预测视为文本生成任务，但作者认为空中视觉语言导航的动作预测更适合作为2D空间定位任务来处理。
- Method: 利用视觉语言模型将模糊语言指令分解为输入图像上的迭代2D航点标注，结合预测的飞行距离，将2D航点转换为3D位移向量作为无人机动作命令，并自适应调整飞行距离以提高导航效率。
- Result: 在DRL仿真基准测试中达到新的最先进水平，比之前最佳方法绝对提升63%。在真实世界评估中大幅优于强基线方法。
- Conclusion: SPF展示了出色的泛化能力，能够适应不同的视觉语言模型，为无人机在动态环境中跟随动态目标提供了有效的闭环控制解决方案。
## eess.IV

### [185] [Patch-Based Diffusion for Data-Efficient, Radiologist-Preferred MRI Reconstruction](https://arxiv.org/abs/2509.21531)
*Rohan Sanda,Asad Aali,Andrew Johnston,Eduardo Reis,Jonathan Singh,Gordon Wetzstein,Sara Fridovich-Keil*

Main category: eess.IV

TL;DR: 提出PaDIS-MRI方法，将基于补丁的扩散模型扩展到复杂值多线圈MRI重建，在数据稀缺情况下显著优于现有方法，获得放射科医生91.7%的诊断偏好。

- Motivation: MRI采集时间长导致成本高、可及性差且易受运动伪影影响。现有扩散模型需要大量训练数据，在临床数据稀缺场景中应用受限。
- Method: 扩展基于补丁的扩散逆求解器(PaDIS)到复杂值多线圈MRI重建，在FastMRI脑数据集上进行7倍欠采样重建，并与FastMRI-EDM等基线方法比较。
- Result: 仅用25个k空间图像训练的小数据集上，PaDIS-MRI在图像质量指标(PSNR、SSIM、NRMSE)、像素级不确定性、跨对比度泛化和严重欠采样鲁棒性方面均优于FastMRI-EDM。放射科医生盲评中91.7%选择PaDIS-MRI为诊断更优。
- Conclusion: 基于补丁的扩散先验在数据稀缺的临床环境中具有高保真MRI重建潜力，对诊断信心至关重要。


### [186] [Comparative Analysis of GAN and Diffusion for MRI-to-CT translation](https://arxiv.org/abs/2509.22049)
*Emily Honey,Anders Helbo,Jens Petersen*

Main category: eess.IV

TL;DR: 比较两种MRI到CT图像转换方法：条件生成对抗网络(cGAN)和条件去噪扩散概率模型(cDDPM)，发现cDDPM在多通道条件输入下表现更优。

- Motivation: 当CT图像缺失或难以获取时，需要从MRI图像生成合成CT(sCT)图像，因此需要确定最有效的MRI到CT转换策略。
- Method: 使用Pix2Pix代表cGAN，Palette代表cDDPM，将3D转换问题分解为2D横断面序列转换，研究单切片和多切片条件输入的影响。
- Result: cDDPM在多通道条件输入下表现更好，评估包括新颖的切片相似度指标SIMOS来衡量3D重建的连续性。
- Conclusion: MRI到CT生成模型受益于多通道条件输入和cDDPM架构。


### [187] [COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics](https://arxiv.org/abs/2509.22240)
*Matt Y. Cheung,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: eess.IV

TL;DR: COMPASS是一个用于医学图像分割的度量不确定性量化框架，通过在模型表示空间中进行校准，生成比传统方法更紧凑的置信区间。

- Motivation: 在临床应用中，分割模型的实用性通常基于下游度量（如器官大小）的准确性，而不是分割掩码的像素级精度。因此，对这些度量进行不确定性量化对决策至关重要。
- Method: COMPASS框架通过在深度神经网络表示空间中扰动中间特征，沿着对目标度量最敏感的低维子空间进行校准，而不是将分割到度量的复杂管道视为黑盒。
- Result: 在四个医学图像分割任务上的实验表明，COMPASS比传统共形预测基线产生显著更紧凑的区间，并且通过利用学习到的内部特征估计重要性权重，还能在协变量偏移下恢复目标覆盖。
- Conclusion: COMPASS为医学图像分割提供了实用的、基于度量的不确定性量化方法，通过利用深度神经网络的归纳偏置实现了更高效的置信区间生成。


### [188] [Deep Learning-Based Cross-Anatomy CT Synthesis Using Adapted nnResU-Net with Anatomical Feature Prioritized Loss](https://arxiv.org/abs/2509.22394)
*Javier Sequeiro González,Arthur Longuefosse,Miguel Díaz Benito,Álvaro García Martín,Fabien Baldacci*

Main category: eess.IV

TL;DR: 提出基于nnUNet的3D patch方法用于MR到CT和CBCT到CT图像转换，引入解剖特征优先损失函数，结合残差UNet网络，在SynthRAD2025数据集上实现跨模态医学图像合成。

- Motivation: 解决医学图像跨模态转换问题，特别是MR到CT和CBCT到CT的转换，以生成高质量的合成CT图像用于临床诊断和治疗规划。
- Method: 采用nnUNet框架的两种网络配置：标准UNet和残差UNet，引入解剖特征优先损失函数，使用3D patch训练，针对头颈、胸部和腹部区域进行优化。
- Result: 残差网络结合AFP损失获得了更清晰的图像重建和更好的解剖保真度，特别是在MR到CT的骨结构和CBCT到CT的病变区域表现优异，而仅使用L1损失的网络在强度指标上略优。
- Conclusion: 该方法为跨模态医学图像合成提供了稳定解决方案，证明了nnUNet自动管道与残差学习及解剖引导特征损失结合的有效性。
## cs.MA

### [189] [Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow](https://arxiv.org/abs/2509.21789)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Yongbo He,Zhangquan Chen,Zhucun Xue,Jiangning Zhang,Yue Liao,Xiaobin Hu,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.MA

TL;DR: 该论文提出ViF方法，通过视觉流和注意力重分配来缓解多智能体系统中的视觉幻觉雪球效应问题。

- Motivation: 多智能体系统在视觉语言模型驱动下存在视觉幻觉雪球效应问题，即单个智能体的幻觉会通过文本流在后续智能体中被放大，导致视觉注意力分配减少。
- Method: 通过注意力分析识别关键视觉标记，提出ViF方法：使用视觉流传递选定的视觉中继标记，并应用注意力重分配来增强视觉证据保留。
- Result: 实验结果表明，该方法显著减少了幻觉雪球效应，在基于四种常见MAS结构和十个基础模型的八个基准测试中持续提升性能。
- Conclusion: ViF是一种轻量级、即插即用的缓解范式，能有效解决多智能体系统中的视觉幻觉雪球问题。
## cs.AI

### [190] [Clinical Uncertainty Impacts Machine Learning Evaluations](https://arxiv.org/abs/2509.22242)
*Simone Lionetti,Fabian Gröger,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Alexander A. Navarini,Marc Pouly*

Main category: cs.AI

TL;DR: 论文主张在机器学习评估中应明确考虑标注不确定性，使用直接对分布进行操作的度量方法，以更好地反映临床数据的真实情况。

- Motivation: 临床数据集标签通常存在不确定性，因为标注者之间存在分歧且置信度不一致。传统的聚合方法（如多数投票）掩盖了这种变异性，影响模型评估的准确性。
- Method: 提出使用概率度量方法，这些方法直接对分布进行操作，适用于各种标注生成过程（简单计数、主观置信度评分或概率响应模型）。这些度量计算轻量，具有线性时间实现。
- Result: 在医学影像基准测试的简单实验中，考虑二元标签的置信度显著影响了模型排名，表明传统评估方法可能产生误导性结果。
- Conclusion: 呼吁社区发布数据集的原始标注，并采用不确定性感知的评估方法，使性能估计能更好地反映临床数据的真实特性。
