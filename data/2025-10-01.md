[[toc]]

## cs.CV

### [1] [LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model](https://arxiv.org/abs/2509.25304)
*Haozhe Jia,Wenshuo Chen,Yuqi Lin,Yang Yang,Lei Wang,Mang Ning,Bowen Tian,Songning Lai,Nanqian Jia,Yifan Chen,Yutao Yue*

Main category: cs.CV

TL;DR: LUMA是一个基于扩散模型的文本到动作生成方法，通过双路径锚定机制解决语义对齐和运动伪影问题，在HumanML3D和KIT-ML数据集上达到SOTA性能。

- Motivation: 现有基于U-Net架构的扩散模型在文本到动作生成任务中存在语义不对齐和运动伪影问题，深层网络梯度衰减导致高级特征学习不足。
- Method: 提出LUMA模型，包含两个路径：1）轻量级MoCLIP模型提供时间域语义监督；2）从低频DCT分量提取频域对齐信号。通过时间调制机制自适应融合两个锚点。
- Result: 在HumanML3D和KIT-ML数据集上分别达到FID分数0.035和0.123的SOTA性能，收敛速度比基线快1.4倍。
- Conclusion: LUMA通过双路径锚定机制有效解决了文本到动作生成中的语义对齐问题，是一个高效可扩展的高保真解决方案。


### [2] [VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes](https://arxiv.org/abs/2509.25339)
*Paul Gavrikov,Wei Lin,M. Jehanzeb Mirza,Soumya Jahagirdar,Muhammad Huzaifa,Sivan Doveh,Serena Yeung-Levy,James Glass,Hilde Kuehne*

Main category: cs.CV

TL;DR: VisualOverload是一个新的视觉问答基准，包含2,720个问题-答案对，专门测试视觉语言模型在密集场景中的基础视觉理解能力。测试显示即使最佳模型在最具挑战性的测试集上准确率仅为19.6%，暴露了当前模型的重大缺陷。

- Motivation: 当前视觉语言模型在标准基准上表现良好，但可能高估了其真实能力。作者认为现有基准主要关注全局图像理解，而忽略了在密集复杂场景中进行简单、无需知识的视觉任务的能力。
- Method: 创建了VisualOverload数据集，包含高分辨率公共领域绘画扫描图像，这些图像具有密集的人物、动作和复杂背景。手动标注了六类任务的问题，专门测试模型对场景细节的理解能力。
- Result: 测试了37个模型，最佳模型(o3)在最难测试集上仅达到19.6%准确率，所有问题总体准确率为69.5%。错误分析揭示了多种失败模式，包括计数能力不足、OCR失败和复杂任务下的逻辑不一致。
- Conclusion: VisualOverload揭示了当前视觉模型在密集场景理解方面的关键差距，为社区开发更好的模型提供了重要资源。


### [3] [Editing Physiological Signals in Videos Using Latent Representations](https://arxiv.org/abs/2509.25348)
*Tianwen Zhou,Akshay Paruchuri,Josef Spjut,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出一种学习框架，通过3D变分自编码器和文本提示融合来编辑视频中的生理信号，在保持视觉质量的同时保护隐私。

- Motivation: 基于摄像头的生理信号估计存在隐私风险，因为面部视频中的生命体征可能泄露个人健康和情绪状态等敏感信息。
- Method: 使用预训练的3D VAE编码输入视频，通过冻结文本编码器嵌入目标HR提示，利用可训练的时空层和AdaLN融合特征，在解码器中应用FiLM和微调输出层以避免生理信号退化。
- Result: 在选定数据集上平均PSNR为38.96 dB，SSIM为0.98，HR调制误差为10.00 bpm MAE和10.09% MAPE。
- Conclusion: 该方法能够可控地编辑HR，适用于匿名化真实视频中的生物特征信号或合成具有期望生命体征的逼真视频。


### [4] [SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs](https://arxiv.org/abs/2509.25390)
*Yuyou Zhang,Radu Corcodel,Chiori Hori,Anoop Cherian,Ding Zhao*

Main category: cs.CV

TL;DR: SpinBench是一个用于评估视觉语言模型空间推理能力的诊断基准，重点关注视角转换能力，包含平移、旋转、物体相对姿态和视角变化等细粒度诊断类别。

- Motivation: 现有的视觉语言模型在空间推理方面存在系统性弱点，需要专门的诊断工具来评估其视角转换能力，这是空间推理的核心挑战。
- Method: 设计了围绕视角转换的细粒度诊断类别，从单物体简单任务逐步过渡到多物体视角转换场景，评估了37个最先进的视觉语言模型。
- Result: 评估结果显示模型存在强烈的自我中心偏见、旋转理解能力差、在对称和语法重构下表现不一致等问题。人类准确率达到91.2%，且人类反应时间与模型准确率呈强相关。
- Conclusion: SpinBench为理解视觉语言模型的空间推理能力提供了关键见解，揭示了它们在物理空间推理方面的主要差距，并捕捉了人类和模型共享的空间推理挑战。


### [5] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.25393)
*Wendong Yao,Binhua Huang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了多模态时空Transformer（MM-STT）框架，通过融合动态位移数据和静态物理先验，显著提升了高分辨率地面沉降预测性能。

- Motivation: 传统方法如ConvLSTM难以建模长程依赖关系，且先前工作局限于单模态数据范式，无法充分利用多源信息。
- Method: 设计了联合时空注意力机制，统一处理多模态特征，实现深度多模态融合。
- Result: 在EGMS数据集上达到新的SOTA，相比基线方法（包括STGCN和STAEformer）将长程预测RMSE降低了一个数量级。
- Conclusion: 对于此类问题，架构固有的深度多模态融合能力是实现突破性性能的关键。


### [6] [DepthLM: Metric Depth From Vision Language Models](https://arxiv.org/abs/2509.25413)
*Zhipeng Cai,Ching-Feng Yeh,Hu Xu,Zhuang Liu,Gregory Meyer,Xinjie Lei,Changsheng Zhao,Shang-Wen Li,Vikas Chandra,Yangyang Shi*

Main category: cs.CV

TL;DR: VLMs通过文本监督微调和稀疏标签即可解锁强大的3D理解能力，无需复杂架构或损失函数。DepthLM方法通过视觉提示和相机参数条件增强，在深度估计任务上超越了现有VLM 2倍以上，首次使VLM达到纯视觉模型的水平。

- Motivation: 现有最先进的VLMs在从2D输入理解3D方面仍有困难，而专门的纯视觉模型在度量深度估计等3D任务中能达到超人类精度。这促使研究者探索VLMs是否能在不改变架构或损失函数的情况下达到专家级精度。
- Method: 采用基于文本的监督微调与稀疏标签，结合视觉提示解决像素参考问题，使用相机参数条件增强解决跨数据集相机模糊性。无需密集预测头或复杂的回归/正则化损失。
- Result: DepthLM在深度估计任务上超越了最先进VLMs 2倍以上的精度，首次使VLM与纯视觉模型相媲美。有趣的是，无需显式约束，训练后的VLM自然避免了过度平滑，在边界区域的飞点比纯视觉模型少得多。
- Conclusion: VLMs通过简单的文本监督微调即可获得强大的3D理解能力，DepthLM的简洁性使单个VLM能够覆盖度量深度之外的多种3D任务。


### [7] [Bayesian Transformer for Pan-Arctic Sea Ice Concentration Mapping and Uncertainty Estimation using Sentinel-1, RCM, and AMSR2 Data](https://arxiv.org/abs/2509.25437)
*Mabel Heffring,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出了一种用于泛北极海冰浓度制图和不确定性量化的贝叶斯Transformer方法，融合多源遥感数据，实现了高分辨率海冰制图和可靠的不确定性评估。

- Motivation: 泛北极高分辨率海冰制图及其不确定性量化对海冰业务化制图至关重要，但面临海冰特征细微、模型不确定性和数据异质性等挑战。
- Method: 设计了具有全局和局部模块的高分辨率Transformer模型以改进特征提取；提出贝叶斯扩展处理模型参数为随机变量以改进不确定性量化；在决策级融合Sentinel-1、RCM和AMSR2三种数据以解决数据异质性。
- Result: 在2021年9月的泛北极数据集上测试，相比其他不确定性量化方法，所提模型能够同时获得高分辨率海冰浓度图和稳健的不确定性图。
- Conclusion: 该方法成功解决了海冰制图中的关键挑战，为泛北极海冰监测提供了有效的技术方案。


### [8] [Infrastructure Sensor-enabled Vehicle Data Generation using Multi-Sensor Fusion for Proactive Safety Applications at Work Zone](https://arxiv.org/abs/2509.25452)
*Suhala Rabab Saba,Sakib Khan,Minhaj Uddin Ahmad,Jiahe Cao,Mizanur Rahman,Li Zhao,Nathan Huynh,Eren Erman Ozguven*

Main category: cs.CV

TL;DR: 本研究开发了一个基于路边摄像头和LiDAR传感器的多传感器融合框架，通过卡尔曼滤波融合策略显著提高了车辆轨迹跟踪的准确性和鲁棒性，在复杂交通环境中实现主动安全措施。

- Motivation: 解决基础设施感知在复杂道路环境（如施工区）中面临的实际部署障碍，包括透视畸变、复杂几何、遮挡和成本问题。
- Method: 集成路边摄像头和LiDAR传感器到协同仿真环境，开发可扩展的车辆检测和定位框架，采用基于卡尔曼滤波的后期融合策略。
- Result: 仿真中融合算法将纵向误差降低高达70%，横向精度保持在1-3米内；现场验证显示融合轨迹能紧密匹配真实车辆路径，即使单个传感器数据间歇或退化。
- Conclusion: 卡尔曼滤波传感器融合能可靠补偿单个传感器限制，提供精确鲁棒的车辆跟踪能力，为复杂交通环境中部署基础设施支持的多传感器系统提供了实用途径。


### [9] [Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection](https://arxiv.org/abs/2509.25502)
*Kaiqing Lin,Zhiyuan Yan,Ruoxin Chen,Junyan Ye,Ke-Yue Zhang,Yue Zhou,Peng Jin,Bin Li,Taiping Yao,Shouhong Ding*

Main category: cs.CV

TL;DR: 提出Forensic-Chat方法，通过"先观察后推理"的新范式，让多模态大语言模型在检测AI生成图像时先感知伪造痕迹再进行推理，解决了现有方法因视觉感知不足导致的性能不佳问题。

- Motivation: 现有MLLMs在检测AI生成图像时性能不佳，根源在于视觉编码器主要针对语义识别优化，对低级别伪造痕迹不敏感，且微调数据格式与预训练分布差异大，导致模型依赖语言捷径而忘记预训练知识。
- Method: 提出Seeing before Reasoning范式，先训练MLLMs感知伪造痕迹（增强伪影感知能力），然后进行推理。开发了Forensic-Chat通用可解释对话助手，并构建ExplainFake-Bench基准从五个方面评估可解释性。
- Result: 广泛实验表明该方法在泛化性和可靠可解释性方面具有优越性，能够实现真正可靠的AI生成图像检测和解释。
- Conclusion: 通过先强化视觉感知再进行推理的范式，成功解决了MLLMs在AI图像检测中的根本问题，实现了更好的泛化能力和可靠的可解释性。


### [10] [DeepFake Detection in Dyadic Video Calls using Point of Gaze Tracking](https://arxiv.org/abs/2509.25503)
*Odin Kohler,Rahul Vijaykumar,Masudul H. Imtiaz*

Main category: cs.CV

TL;DR: 提出一种利用视线追踪技术的实时深度伪造检测方法，通过分析视频会议中视线模式来识别深度伪造攻击。

- Motivation: 随着深度伪造技术的发展，恶意攻击者开始在视频会议中进行实时钓鱼攻击。由于深度伪造无法模仿真实人类对话中的微妙视线交流模式，这为检测提供了机会。
- Method: 基于可解释特征构建模型，这些特征经过对双向对话中视线模式的仔细研究后选择。利用攻击者视频流中的估计视线来检测深度伪造。
- Result: 在自建的新数据集上测试，达到82%的准确率。
- Conclusion: 这是首个利用视线点追踪进行深度伪造检测的方法，为实时检测视频会议中的深度伪造攻击提供了新途径。


### [11] [Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity](https://arxiv.org/abs/2509.25520)
*Tu-Hoa Pham,Philip Bailey,Daniel Posada,Georgios Georgakis,Jorge Enriquez,Surya Suresh,Marco Dolci,Philip Twu*

Main category: cs.CV

TL;DR: 提出了一种基于视觉的6自由度物体姿态估计算法，专门用于火星样本返回任务中的受限硬件环境，通过自定义渲染器和边缘域模板匹配实现鲁棒的姿态估计。

- Motivation: 解决火星样本返回任务中机械臂在严重硬件约束下对多个目标物体进行低间隙拾取和插入时的定位问题。
- Method: 使用自定义渲染器和专门为边缘域设计的新模板匹配度量，仅需低保真、无纹理的3D模型作为输入。
- Result: 在合成数据集、地球物理测试台和火星实地图像上的广泛评估表明，该方法在计算和内存受限的定位任务中，在鲁棒性和准确性方面均优于现有技术。
- Conclusion: 该方法为通用硬件上的廉价可靠定位开辟了新可能性。


### [12] [LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models](https://arxiv.org/abs/2509.25528)
*Pranav Saxena,Avigyan Bhattacharya,Ji Zhang,Wenshan Wang*

Main category: cs.CV

TL;DR: LLM-RG是一种结合视觉语言模型和大型语言模型的混合方法，用于解决户外驾驶场景中的指称接地问题，在Talk2Car基准测试中表现优于现有方法。

- Motivation: 户外驾驶场景中的指称接地具有挑战性，因为场景变化大、视觉相似物体多、动态元素复杂，使得解析自然语言指称（如"右边的黑色汽车"）变得困难。
- Method: 提出LLM-RG混合管道，结合现成的视觉语言模型进行细粒度属性提取和大型语言模型进行符号推理。通过LLM提取相关对象类型和属性，检测候选区域，使用VLM生成丰富的视觉描述符，然后将这些描述符与空间元数据结合成自然语言提示，输入LLM进行思维链推理以识别指称对象的边界框。
- Result: 在Talk2Car基准测试中，LLM-RG相比基于LLM和VLM的基线方法取得了显著提升。消融实验表明添加3D空间线索能进一步提高接地性能。
- Conclusion: 研究结果表明，以零样本方式应用的VLM和LLM在鲁棒的户外指称接地任务中具有互补优势。


### [13] [VISOR++: Universal Visual Inputs based Steering for Large Vision Language Models](https://arxiv.org/abs/2509.25533)
*Ravikumar Balakrishnan,Mansi Phute*

Main category: cs.CV

TL;DR: VISOR++ 是一种通过优化视觉输入来实现视觉语言模型行为控制的方法，无需运行时访问模型内部，仅通过插入特定图像即可引导模型行为。

- Motivation: 现有行为控制方法存在局限性：系统提示易被用户指令覆盖，基于激活的转向向量需要侵入式运行时模型访问，无法用于API服务和闭源模型。需要开发跨多个VLM的通用转向方法。
- Method: 通过生成优化的视觉输入（VISOR++图像）来诱导目标激活模式，这些图像可以针对VLM集合进行联合优化，无需运行时模型访问即可实现行为控制。
- Result: VISOR++图像在开放模型上实现了与转向向量相当的性能，在拒绝、奉承和生存本能三个对齐方向上表现良好，并能推广到未见过的模型，同时在无关任务上保持99.9%的性能。
- Conclusion: VISOR++提供了一种部署无关的行为控制方法，仅通过视觉输入即可有效引导VLM行为，为安全关键应用中的模型控制提供了实用解决方案。


### [14] [Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play](https://arxiv.org/abs/2509.25541)
*Qinsi Wang,Bo Liu,Tianyi Zhou,Jing Shi,Yueqian Lin,Yiran Chen,Hai Helen Li,Kun Wan,Wentian Zhao*

Main category: cs.CV

TL;DR: Vision-Zero是一个领域无关的框架，通过从任意图像对生成的竞争性视觉游戏实现视觉语言模型的自我提升，无需人工标注数据。

- Motivation: 当前强化学习方法依赖劳动密集型数据集，训练成本高，限制了视觉语言模型的实际部署。
- Method: 采用战略自玩框架，让模型在'谁是卧底'式游戏中自主生成训练数据；引入迭代自玩策略优化算法，结合自玩和带可验证奖励的强化学习。
- Result: 在推理、图表问答和视觉中心理解任务上达到最先进性能，超越了其他基于标注的方法。
- Conclusion: Vision-Zero证明了无需人工标注即可实现视觉语言模型持续自我改进的可行性，具有强大的泛化能力。


### [15] [Hybrid Approach for Enhancing Lesion Segmentation in Fundus Images](https://arxiv.org/abs/2509.25549)
*Mohammadmahdi Eshragh,Emad A. Mohammed,Behrouz Far,Ezekiel Weis,Carol L Shields,Sandor R Ferenczy,Trafford Crump*

Main category: cs.CV

TL;DR: 提出了一种结合数学/聚类分割模型与U-Net的混合方法，用于脉络膜痣的精确分割，在1024*1024眼底图像上达到89.7%的Dice系数和80.01%的IoU，显著优于Attention U-Net模型。

- Motivation: 脉络膜痣有转化为黑色素瘤的风险，早期检测至关重要。现有AI方法在眼底图像分割中面临挑战，特别是数据集分辨率低、标注不一致，限制了分割模型的有效性。
- Method: 提出混合模型，结合数学/聚类分割模型与U-Net的见解，利用两种方法的优势，减少对大规模训练数据的依赖。
- Result: 在1024*1024眼底图像上，Dice系数达89.7%，IoU达80.01%，显著优于Attention U-Net的51.3%和34.2%。在外部数据集上表现出更好的泛化能力。
- Conclusion: 该工作为开发脉络膜痣诊断决策支持系统奠定了基础，有望通过自动病变标注提高诊断和监测的速度与准确性。


### [16] [FishNet++: Analyzing the capabilities of Multimodal Large Language Models in marine biology](https://arxiv.org/abs/2509.25564)
*Faizan Farooq Khan,Yousef Radwan,Eslam Abdelrahman,Abdulwahab Felemban,Aymen Mir,Nico K. Michiels,Andrew J. Temple,Michael L. Berumen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 本文介绍了FishNet++，一个用于评估多模态大语言模型在鱼类物种识别方面能力的大规模多模态基准，揭示了现有模型在该专业领域的显著局限性。

- Motivation: 多模态大语言模型在跨领域任务中表现出色，但在海洋生物学等专业科学领域的能力尚未充分探索。鱼类物种的细粒度识别对于监测受人为压力影响的海洋生态系统至关重要。
- Method: 引入FishNet++基准，显著扩展现有资源，包含35,133个多模态学习文本描述、706,426个形态学研究关键点标注和119,399个检测边界框，提供全面的标注套件。
- Result: 系统评估显示最先进的多模态大语言模型在鱼类物种细粒度识别方面存在显著局限性，最佳开源模型的准确率低于10%。
- Conclusion: FishNet++基准有助于开发和评估能够推进水生科学的专业视觉语言模型，填补了现有模型在专业科学领域知识不足的空白。


### [17] [AttentionViG: Cross-Attention-Based Dynamic Neighbor Aggregation in Vision GNNs](https://arxiv.org/abs/2509.25570)
*Hakan Emre Gedik,Andrew Martin,Mustafa Munir,Oguzhan Baser,Radu Marculescu,Sandeep P. Chinchali,Alan C. Bovik*

Main category: cs.CV

TL;DR: 提出了一种基于交叉注意力的图神经网络节点聚合方法，并构建了AttentionViG架构，在图像识别任务中实现了SOTA性能，同时在目标检测、实例分割和语义分割等下游任务中表现出良好的迁移性。

- Motivation: 现有的Vision Graph Neural Networks (ViGs)在节点-邻居特征聚合方法上存在局限，需要一种能够有效捕捉复杂节点-邻居关系且无需架构特定优化的通用聚合方法。
- Method: 提出交叉注意力聚合方法，其中查询投影来自节点，键投影来自其邻居，并构建了使用该聚合方案进行非局部消息传递的AttentionViG架构。
- Result: 在ImageNet-1K基准测试中达到SOTA性能，在MS COCO 2017的目标检测和实例分割任务以及ADE20K的语义分割任务中表现出良好的迁移性，同时保持计算效率。
- Conclusion: 所提出的交叉注意力聚合方法不仅实现了强大的性能，而且保持了效率，在与先前视觉GNN架构相当的FLOPs下提供了有竞争力的准确性。


### [18] [MetaChest: Generalized few-shot learning of patologies from chest X-rays](https://arxiv.org/abs/2509.25590)
*Berenice Montalvo-Lezama,Gibran Fuentes-Pineda*

Main category: cs.CV

TL;DR: 提出了MetaChest数据集和评估框架，研究医学图像中的广义少样本多标签分类问题，发现迁移学习方法优于专门设计的少样本学习方法。

- Motivation: 医学图像分析中标注数据稀缺，传统少样本学习假设所有类别都是新的，但实际医学应用需要同时学习新类别并利用已知类别知识。
- Method: 构建包含479,215张胸部X光片的MetaChest数据集，设计标准少样本分类元集划分和多标签episode生成算法，评估迁移学习和ProtoNet扩展方法。
- Result: 增加每episode类别数和每类训练样本数可提升性能；迁移学习方法持续优于ProtoNet；高分辨率图像提高精度但增加计算成本；高效架构可达到类似性能且资源需求显著降低。
- Conclusion: 迁移学习在医学图像少样本多标签分类中表现优异，高效模型架构可在保持性能的同时大幅减少资源消耗，为实际医学应用提供实用解决方案。


### [19] [K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model](https://arxiv.org/abs/2509.25594)
*Bangwei Guo,Yunhe Gao,Meng Ye,Difei Gu,Yang Zhou,Leon Axel,Dimitris Metaxas*

Main category: cs.CV

TL;DR: K-Prism是一个统一的医学图像分割框架，通过整合语义先验、上下文知识和交互反馈三种知识范式，实现了灵活的分割能力。

- Motivation: 现有医学图像分割模型通常针对单一知识源和特定任务设计，与临床实践中专家整合多种知识的实际情况不符。
- Method: 将异构知识源编码为双提示表示：1-D稀疏提示定义分割内容，2-D密集提示指示关注区域，通过混合专家解码器动态路由。
- Result: 在18个公共数据集上的实验表明，K-Prism在语义、上下文和交互分割设置中均达到最先进性能。
- Conclusion: K-Prism成功模拟了临床实践的灵活性，能够在不修改架构的情况下在多种范式间灵活切换和联合训练。


### [20] [GaussianLens: Localized High-Resolution Reconstruction via On-Demand Gaussian Densification](https://arxiv.org/abs/2509.25603)
*Yijia Weng,Zhicheng Wang,Songyou Peng,Saining Xie,Howard Zhou,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: 提出GaussianLens方法，通过按需高斯致密化实现局部高分辨率重建，避免统一高分辨率重建的高成本，专注于用户指定的感兴趣区域。

- Motivation: 人类感知具有选择性注意力特性，需要能够按需重建关键区域细节的方法。现有3DGS方法要么输出均匀分辨率导致高计算成本，要么需要密集观测和长时间离线优化。
- Method: 基于低分辨率3DGS重建，使用前馈致密化框架融合初始3DGS和多视角图像的多模态信息，设计像素引导的致密化机制捕获大分辨率增加下的细节。
- Result: 在局部精细细节重建方面表现优异，能够扩展到1024×1024分辨率图像，具有强可扩展性。
- Conclusion: GaussianLens成功解决了局部高分辨率重建问题，避免了统一高分辨率重建的冗余成本，有效利用了关键区域的高分辨率捕捉。


### [21] [LMOD+: A Comprehensive Multimodal Dataset and Benchmark for Developing and Evaluating Multimodal Large Language Models in Ophthalmology](https://arxiv.org/abs/2509.25620)
*Zhenyue Qin,Yang Liu,Yu Yin,Jinyu Ding,Haoran Zhang,Anran Li,Dylan Campbell,Xuansheng Wu,Ke Zou,Tiarnan D. L. Keenan,Emily Y. Chew,Zhiyong Lu,Yih-Chung Tham,Ninghao Liu,Xiuzhen Zhang,Qingyu Chen*

Main category: cs.CV

TL;DR: 提出了一个大规模多模态眼科基准数据集，包含32,633个实例，涵盖12种常见眼科疾病和5种成像模式，用于评估生成模型在眼科AI应用中的表现。

- Motivation: 全球范围内威胁视力的眼病负担严重，但及时诊断受到医疗资源短缺和专科护理可及性限制。多模态大语言模型在医学图像解释方面有潜力，但缺乏适合评估生成模型的综合性眼科基准数据集。
- Method: 构建包含多粒度标注的大规模多模态眼科基准数据集，整合图像、解剖结构、人口统计学和自由文本注释，支持解剖结构识别、疾病筛查、疾病分期和人口统计学预测等任务。
- Result: 评估了24个最先进的多模态大语言模型，在零样本设置下，表现最佳的模型在疾病筛查中达到约58%的准确率，但在疾病分期等挑战性任务上表现仍不理想。
- Conclusion: 该研究揭示了多模态大语言模型在眼科应用中的潜力和局限性，将公开数据集、处理流程和排行榜，有望推动眼科AI应用发展，减轻全球视力威胁疾病的负担。


### [22] [Anchor-free Cross-view Object Geo-localization with Gaussian Position Encoding and Cross-view Association](https://arxiv.org/abs/2509.25623)
*Xingtao Ling,Chenlin Fu,Yingying Zhu*

Main category: cs.CV

TL;DR: 提出AFGeo方法，采用无锚框范式进行跨视角物体地理定位，通过预测四个方向偏移量直接定位物体，无需预定义锚框。

- Motivation: 现有跨视角物体地理定位方法大多基于锚框范式，这种方法受限于预定义的锚框。为了消除这种依赖，需要开发无锚框的定位方法。
- Method: AFGeo直接为每个像素预测到真实框的四个方向偏移量（左、右、上、下）。引入高斯位置编码（GPE）建模查询图像中的点击点，缓解跨视角场景中物体位置的不确定性。还包含跨视角物体关联模块（CVOAM），在不同视角间关联同一物体及其周围上下文。
- Result: 模型轻量且计算高效，在基准数据集上达到最先进的性能。
- Conclusion: AFGeo通过无锚框定位范式，结合GPE和CVOAM，以最小的参数开销实现了高效的跨视角物体地理定位。


### [23] [Generalized Contrastive Learning for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.25638)
*Jungsoo Lee,Janghoon Cho,Hyojin Park,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: 提出广义对比学习(GCL)方法，无需额外数据集就能提升多模态检索性能，在现有模型上实现一致改进

- Motivation: 解决跨模态检索模型在处理融合图像-文本模态检索时的性能下降问题，避免传统方法需要精心构建新数据集的局限性
- Method: GCL通过在小批量内对所有模态执行对比学习，利用现有图像-字幕配对数据集学习统一表示空间
- Result: 在M-BEIR、MMEB和CoVR基准测试中，对VISTA、CLIP和TinyCLIP等现有多模态检索模型均实现了性能提升
- Conclusion: GCL是一种有效的多模态检索方法，能够在不增加数据集构建负担的情况下提升模型性能


### [24] [Using Images from a Video Game to Improve the Detection of Truck Axles](https://arxiv.org/abs/2509.25644)
*Leandro Arab Marcomini,Andre Luiz Cunha*

Main category: cs.CV

TL;DR: 使用视频游戏生成的合成图像可以有效训练CNN检测真实卡车车轴，最高mAP达到99%，证明合成图像是可靠的低成本训练数据源

- Motivation: 传统CNN需要大量数据训练，但数据收集成本高昂。视频游戏能生成逼真的3D模型，可作为替代数据源
- Method: 创建三个包含真实和合成卡车的数据库，使用三种YOLO架构进行训练和测试，评估召回率、精确率、F1分数和mAP，并应用Mann-Whitney U检验统计显著性
- Result: 合成图像被证明是可靠的训练数据源，所有网络性能都有提升，最高mAP达到99%
- Conclusion: 合成图像可用于训练神经网络，为知识提取提供可靠、低成本的数据源


### [25] [DescribeEarth: Describe Anything for Remote Sensing Images](https://arxiv.org/abs/2509.25654)
*Kaiyu Li,Zixuan Jiang,Xiangyong Cao,Jiayu Wang,Yuchen Xiao,Deyu Meng,Zhi Wang*

Main category: cs.CV

TL;DR: 提出了Geo-DLC任务，即遥感图像的对象级细粒度图像描述，构建了DE-Dataset数据集和DE-Benchmark评估套件，并开发了DescribeEarth多模态大语言模型，在遥感图像描述任务中表现优于现有方法。

- Motivation: 现有遥感图像描述研究主要关注图像级别，缺乏对象级细粒度解释，无法充分利用遥感图像中丰富的语义和结构信息。
- Method: 提出DescribeEarth多模态大语言模型架构，集成尺度自适应聚焦策略和领域引导融合模块，利用遥感视觉语言模型特征编码高分辨率细节和遥感类别先验，同时保持全局上下文。
- Result: DescribeEarth模型在DE-Benchmark上持续优于最先进的通用MLLMs，在事实准确性、描述丰富性和语法正确性方面表现优异，特别是在捕捉内在对象特征和周围环境属性方面。
- Conclusion: Geo-DLC任务和DescribeEarth模型为遥感图像提供了更细粒度的对象级描述能力，推动了遥感图像理解的发展。


### [26] [YOLO-Based Defect Detection for Metal Sheets](https://arxiv.org/abs/2509.25659)
*Po-Heng Chou,Chun-Chi Wang,Wei-Lung Mao*

Main category: cs.CV

TL;DR: 提出基于YOLO的深度学习模型用于工业制造中的自动缺陷检测，结合ConSinGAN进行数据增强，YOLOv9模型表现最佳，准确率达91.3%，检测时间146ms，并集成到SCADA系统中建立自动化光学检测系统。

- Motivation: 解决工业制造中金属板材缺陷检测耗时耗力的问题，特别是缺乏足够的金属板材图像数据导致检测精度下降的问题。
- Method: 使用ConSinGAN生成大量数据增强，结合YOLOv3、v4、v7、v9四个版本模型进行缺陷检测，检测金属板材表面和孔洞的缺陷。
- Result: YOLOv9模型结合ConSinGAN表现最佳，准确率达到91.3%，检测时间为146毫秒，优于其他YOLO版本。
- Conclusion: 提出的YOLOv9模型成功集成到制造硬件和SCADA系统中，建立了实用的自动化光学检测系统，且该方法可轻松应用于工业制造中的其他组件。


### [27] [OmniDFA: A Unified Framework for Open Set Synthesis Image Detection and Few-Shot Attribution](https://arxiv.org/abs/2509.25682)
*Shiyu Wu,Shuyan Li,Jing Li,Jing Liu,Yequan Wang*

Main category: cs.CV

TL;DR: 本文提出OmniDFA框架，用于AI生成图像的开集、少样本源模型识别，同时构建了包含117万张图像的大型数据集OmniFake，在AIGI检测和溯源方面取得优异性能。

- Motivation: 现有AI生成图像检测方法容易过拟合特定伪造特征，而源模型溯源方法受限于缺乏大规模分类合成数据集，限制了其实际应用。
- Method: 提出开集少样本源识别新范式，开发OmniDFA框架，通过构建OmniFake数据集（包含45个生成模型的117万张图像）来实现图像真实性评估和少样本合成源识别。
- Result: 实验表明OmniDFA在开集溯源方面表现出色，在AIGI检测上达到最先进的泛化性能。
- Conclusion: OmniDFA框架和OmniFake数据集为AIGI检测和溯源研究提供了有效解决方案，具有实际应用价值。


### [28] [AIMCoT: Active Information-driven Multimodal Chain-of-Thought for Vision-Language Reasoning](https://arxiv.org/abs/2509.25699)
*Xiping Li,Jianghong Ma*

Main category: cs.CV

TL;DR: AIMCoT是一个主动信息驱动的多模态思维链框架，通过上下文增强注意力图生成、主动视觉探测和动态注意力转移触发三个协同组件，解决了现有方法依赖不可靠注意力图和被动选择策略的问题，显著提升了多模态推理性能。

- Motivation: 现有多模态思维链方法依赖简单的启发式方法构建交错思维链，通常基于注意力图，但实证分析表明这些注意力图不可靠。其被动、无目的的选择策略和任意触发机制在捕捉模型对信息的认知需求方面存在严重不足。
- Method: 提出AIMCoT框架，包含三个组件：(1)上下文增强注意力图生成(CAG)，缓解文本-视觉粒度不平衡问题；(2)主动视觉探测(AVP)，基于信息理论主动选择最能帮助回答问题的图像区域；(3)动态注意力转移触发(DAT)，通过监控模型文本到视觉的注意力转移来智能确定插入视觉信息的最佳时机。
- Result: 在三个具有挑战性的基准测试上的广泛实验表明，AIMCoT在不同设置下显著优于最先进的方法。
- Conclusion: 通过主动寻找信息并动态构建推理过程，AIMCoT代表了向更稳健、有效和类人化多模态推理的关键一步。


### [29] [How Diffusion Models Memorize](https://arxiv.org/abs/2509.25705)
*Juyeop Kim,Songkuk Kim,Jong-Seok Lee*

Main category: cs.CV

TL;DR: 扩散模型会记忆训练数据，这主要是由早期去噪过程中的训练样本高估驱动的，该过程减少了多样性、压缩了去噪轨迹并加速向记忆图像的收敛。

- Motivation: 尽管扩散模型在图像生成方面取得了成功，但它们会记忆训练数据，引发了严重的隐私和版权问题。先前的工作试图描述、检测和缓解记忆现象，但记忆发生的基本原因和机制仍未解决。
- Method: 通过重新审视扩散和去噪过程，分析潜在空间动态，研究记忆机制。具体包括分析训练损失、记忆提示的影响以及中间潜在变量的分解。
- Result: 发现记忆不能仅用过拟合解释，因为训练损失在记忆情况下更大；记忆提示将训练图像注入噪声预测，强制潜在轨迹收敛；中间潜在变量分解显示初始随机性被快速抑制并被记忆内容取代。
- Conclusion: 早期高估是扩散模型中记忆现象的核心机制，偏离理论去噪计划与记忆严重程度高度相关。


### [30] [ProbMed: A Probabilistic Framework for Medical Multimodal Binding](https://arxiv.org/abs/2509.25711)
*Yuan Gao,Sangwook Kim,Jianzhong You,Chris McIntosh*

Main category: cs.CV

TL;DR: 提出ProbMED模型，通过概率对比学习处理医学多模态数据中的多对多映射关系，将四种医学模态对齐到统一的概率嵌入空间，在跨模态检索和分类任务中表现优异。

- Motivation: 当前医学视觉语言预训练模型无法直接处理医学多模态数据中的多对多映射关系，需要新的方法来建模这种复杂的模态交互。
- Method: 使用概率对比学习建模嵌入分布而非确定性估计，采用InfoNCE损失和Hellinger距离整合模态间分布，引入概率合成采样损失捕获模态特定均值和方差。
- Result: 在13个医学数据集上的实验表明，模型在跨模态检索、零样本和少样本分类任务中优于现有Med-VLPMs，并展示了多模态整合的鲁棒性。
- Conclusion: ProbMED通过概率建模方法有效解决了医学多模态数据中的多对多映射问题，显著提升了模态间的绑定效果和模型性能。


### [31] [Importance Sampling for Multi-Negative Multimodal Direct Preference Optimization](https://arxiv.org/abs/2509.25717)
*Xintong Li,Chuhan Wang,Junda Wu,Rohan Surana,Tong Yu,Julian McAuley,Jingbo Shang*

Main category: cs.CV

TL;DR: MISP-DPO是一种多模态偏好优化框架，通过引入多个语义多样的负样本和使用Plackett-Luce模型来解决现有方法依赖简单成对比较的问题。

- Motivation: 现有方法依赖过度简化的成对比较，通过基本扰动或基于相似性检索生成单个负图像，无法捕捉多模态偏好的复杂性，导致优化偏差和幻觉问题。
- Method: 在CLIP空间中嵌入提示和候选图像，应用稀疏自编码器揭示可解释的语义偏差因子。基于重构难度、与正样本的语义偏差和相互多样性选择负样本，采用Plackett-Luce目标和重要性采样策略。
- Result: 在五个不同基准测试中，MISP-DPO持续改善了多模态对齐效果，优于先前方法。
- Conclusion: 语义感知的多负样本采样在基于偏好的学习中具有有效性，能够提供更广泛和更有信息量的监督。


### [32] [SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition](https://arxiv.org/abs/2509.25723)
*Shunpeng Chen,Changwei Wang,Rongtao Xu,Xingtian Pei,Yukun Song,Jinzhou Lin,Wenhao Xu,Jingyi Zhang,Li Guo,Shibiao Xu*

Main category: cs.CV

TL;DR: SAGE是一个统一的视觉地点识别训练框架，通过联合改进局部特征聚合、训练样本组织和困难样本挖掘，增强空间-视觉判别能力。

- Motivation: 现有方法主要关注描述符微调或固定采样策略，忽视了训练过程中空间上下文和视觉相似性之间的动态交互作用。
- Method: 提出轻量级Soft Probing模块学习补丁描述符的残差权重，构建在线地理-视觉图融合地理邻近性和当前视觉相似性，使用贪婪加权团扩展采样器扩展高亲和力锚点的聚类。
- Result: 在八个基准测试中达到SOTA，SPED、Pitts30k-test、MSLS-val和Nordland上的Recall@1分别为98.9%、95.8%、94.5%和96.0%，在SPED上仅使用4096D全局描述符就获得100% Recall@10。
- Conclusion: SAGE通过动态空间-视觉交互建模和参数高效微调，显著提升了视觉地点识别的性能。


### [33] [LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human Face Editing](https://arxiv.org/abs/2509.25731)
*Zhenghao Zhang,Ziying Zhang,Junchao Liao,Xiangyu Meng,Qiang Hu,Siyu Zhu,Xiaoyun Zhang,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: LaTo是一个基于地标标记化的扩散变换器，用于细粒度、身份保持的人脸编辑，通过量化地标坐标、统一位置编码和基于视觉语言模型的地标预测来解决现有方法在精确属性控制和身份保持方面的不足。

- Motivation: 现有的多模态人脸编辑方法在语义操作方面表现良好，但在精确属性控制和身份保持方面仍有困难。传统方法将结构性地标作为刚性几何约束，当条件地标与源图像差异较大时（如大表情或姿态变化、不准确的地标估计）会降低身份保持效果。
- Method: 提出LaTo方法，包含三个关键创新：(1) 地标标记器直接量化原始地标坐标为离散面部标记；(2) 位置映射位置编码整合面部和图像标记进行统一处理；(3) 利用视觉语言模型从指令和源图像推断目标地标的预测器。
- Result: 在超过15万真实人脸对的大规模基准测试中，LaTo在身份保持方面比最先进方法提升7.8%，在语义一致性方面提升4.6%。
- Conclusion: LaTo通过创新的地标标记化和统一处理框架，显著提升了人脸编辑的精确属性控制和身份保持能力，为解决现有方法的局限性提供了有效方案。


### [34] [The 1st Solution for MOSEv1 Challenge on LSVOS 2025: CGFSeg](https://arxiv.org/abs/2509.25738)
*Tingmin Li,Yixuan Li,Yang Yang*

Main category: cs.CV

TL;DR: 提出CGFSeg方法用于视频对象分割，在MOSEv1挑战赛中取得第一名，J&F得分86.37%

- Motivation: 解决复杂现实场景中视频对象分割的挑战，包括长期对象消失和重现、小对象和不易察觉对象的存在
- Method: 冻结SAM2的特征提取器，微调其余组件；推理阶段引入像素检查策略，利用多个模型的互补优势逐步优化预测
- Result: 在MOSEv1挑战赛测试集上获得86.37%的J&F得分，排名第一
- Conclusion: 该方法在复杂场景下有效解决了视频对象分割的挑战


### [35] [LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion](https://arxiv.org/abs/2509.25739)
*Donghwan Kim,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出了一种基于SO(3)扩散模型的人体网格恢复方法，通过建模3D旋转参数的分布来解决单张RGB图像恢复3D人体姿态的模糊性问题。

- Motivation: 解决现有概率方法在准确性和样本多样性之间的权衡问题，以及单预测结果不如最先进确定性模型的问题。
- Method: 使用SO(3)扩散模型生成3D旋转参数的分布，通过条件丢弃实现无条件和图像条件生成，利用transformer学习人体关节的层次结构，MLP去噪模型学习基于潜在向量的每关节分布。
- Result: 实验证明该方法能有效预测准确的姿态概率分布。
- Conclusion: 提出的SO(3)扩散模型方法能够有效建模与2D观测对齐的分布，解决了人体网格恢复中的模糊性问题。


### [36] [Dragging with Geometry: From Pixels to Geometry-Guided Image Editing](https://arxiv.org/abs/2509.25740)
*Xinyu Pu,Hongsong Wang,Jie Gui,Pan Zhou*

Main category: cs.CV

TL;DR: GeoDrag是一种基于几何引导的拖拽式图像编辑方法，通过整合3D几何线索和2D空间先验，解决了传统2D像素平面编辑在几何密集型场景中的不精确和不一致问题。

- Motivation: 现有基于拖拽的图像编辑方法主要在2D像素平面上操作，缺乏3D几何线索，导致在旋转和透视变换等几何密集型场景中产生不精确和不一致的编辑结果。
- Method: 提出统一的位移场联合编码3D几何和2D空间先验，采用无冲突分区策略隔离编辑区域，在单次前向传播中实现连贯编辑。
- Result: 在各种编辑场景下的广泛实验验证了方法的有效性，显示出优越的精度、结构一致性和可靠的多点编辑能力。
- Conclusion: GeoDrag通过几何引导实现了精确、一致且结构保持的图像编辑，解决了传统方法在复杂几何变换中的局限性。


### [37] [IPDRecon: Image-Plane Geometric Decoding for View-Invariant Indoor Scene Reconstruction](https://arxiv.org/abs/2509.25744)
*Mingyang Li,Yimeng Fan,Changsong Liu,Tianyu Zhou,Xin Wang,Yanyan Liu,Wei Zhang*

Main category: cs.CV

TL;DR: IPDRecon是一种基于图像平面解码的室内场景重建方法，通过减少对多视角几何约束的依赖，利用单视图内的丰富空间信息，显著提升了在视角受限情况下的重建稳定性和质量。

- Motivation: 现有的基于体素的室内场景重建方法严重依赖多视角像素反投影射线交点作为几何约束，导致重建质量高度依赖输入视角密度，在重叠区域和未观测区域表现不佳。
- Method: 提出IPDRecon框架，包含三个核心组件：像素级置信度编码器(PCE)、仿射补偿模块(ACM)和图像平面空间解码器(IPSD)，通过物理成像过程解码2D图像中的3D结构信息。
- Result: 在ScanNetV2上的实验表明，IPDRecon在视角数量减少40%时仍能保持几乎相同的重建质量，变异系数仅为0.24%，性能保持率达99.7%，最大性能下降仅0.42%。
- Conclusion: 利用单视图内的空间信息为实际应用中的视角受限场景提供了鲁棒的解决方案，显著提升了重建稳定性。


### [38] [FinCap: Topic-Aligned Captions for Short-Form Financial YouTube Videos](https://arxiv.org/abs/2509.25745)
*Siddhant Sukhani,Yash Bhardwaj,Riya Bhadani,Veer Kejriwal,Michael Galarnyk,Sudheer Chava*

Main category: cs.CV

TL;DR: 评估多模态大语言模型在金融短视频中的主题对齐字幕生成能力，测试了文本、音频、视频及其组合在5个金融主题上的表现，发现视频模态在多数任务中表现突出，而过多模态组合可能引入噪声。

- Motivation: 建立金融短视频字幕生成的基准，探索多模态模型在金融领域视频理解中的潜力与挑战，特别是视觉线索的复杂语义基础。
- Method: 使用624个标注的YouTube短视频，评估7种模态组合（T、A、V、TA、TV、AV、TAV）在5个金融主题任务上的表现：主要推荐、情感分析、视频目的、视觉分析和金融实体识别。
- Result: 视频模态在5个主题中的4个表现强劲，强调了视觉上下文和有效线索（如情绪、手势、肢体语言）的价值。选择性模态组合如TV或AV往往优于TAV，表明过多模态可能引入噪声。
- Conclusion: 建立了金融短视频字幕生成的第一个基准，展示了在该领域基础复杂视觉线索的潜力和挑战，代码和数据在CC-BY-NC-SA 4.0许可下公开。


### [39] [Dolphin v1.0 Technical Report](https://arxiv.org/abs/2509.25748)
*Taohan Weng,Chi zhang,Chaoran Yan,Siya Liu,Xiaoyang Liu,Yalun Wu,Boyang Wang,Boyan Wang,Jiren Ren,Kaiwen Yan,Jinze Yu,Kaibing Hu,Henan Liu,Haoyun zheng,Anjie Le,Hongcheng Guo*

Main category: cs.CV

TL;DR: 提出了Dolphin系列模型，这是首个统一多种临床任务的大规模多模态超声基础模型，包括Dolphin v1.0和推理增强版Dolphin R1，在超声AI领域实现了突破性进展。

- Motivation: 超声在医学中至关重要，但面临操作者依赖性、图像噪声和实时扫描等挑战，阻碍了AI集成。现有大模型在超声领域表现不佳，需要专门解决方案。
- Method: 构建了200万规模的多模态数据集，采用三阶段训练策略：领域专业化预训练、指令驱动对齐和基于强化的精炼。Dolphin R1通过强化学习结合超声特定奖励来增强推理能力。
- Result: 在U2-Bench基准测试的8个超声任务中，Dolphin R1获得0.5835的U2分数，是第二名模型（0.2968）的两倍多，创下新纪录。Dolphin v1.0也表现优异。
- Conclusion: 推理增强训练显著提高了诊断准确性、一致性和可解释性，证明了统一框架在高风险医疗AI中的重要性，为超声AI集成提供了有效解决方案。


### [40] [ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual Try-On](https://arxiv.org/abs/2509.25749)
*Junseo Park,Hyeryung Jang*

Main category: cs.CV

TL;DR: ART-VITON是一个基于测量引导的扩散框架，用于虚拟试穿任务，通过渐进式测量一致性确保非试穿区域的保留，同时消除边界伪影。

- Motivation: 现有虚拟试穿方法在保留非试穿区域时存在边界伪影问题，直接替换策略会导致突兀的过渡。
- Method: 将虚拟试穿重新表述为线性逆问题，采用轨迹对齐求解器，结合残差先验初始化和无伪影测量引导采样。
- Result: 在VITON-HD、DressCode和SHHQ-1.0数据集上实验表明，能有效保留身份和背景，消除边界伪影，提升视觉保真度和鲁棒性。
- Conclusion: ART-VITON通过测量一致性约束成功解决了虚拟试穿中的边界伪影问题，在多个数据集上优于现有方法。


### [41] [Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs](https://arxiv.org/abs/2509.25771)
*Jia Jun Cheng Xian,Muchen Li,Haotian Yang,Xin Tao,Pengfei Wan,Leonid Sigal,Renjie Liao*

Main category: cs.CV

TL;DR: 本文提出了文本偏好优化(TPO)框架，通过训练模型偏好匹配提示而非不匹配提示来实现文本到图像模型的免费对齐，无需配对图像偏好数据。

- Motivation: 现有基于RLHF的方法依赖昂贵的配对图像偏好数据或学习奖励函数，面临可扩展性限制。
- Method: 使用大语言模型扰动原始标题构建不匹配提示，训练模型偏好匹配提示，扩展DPO和KTO算法为TDPO和TKTO。
- Result: 在多个基准测试中，TPO方法在人类偏好得分和文本-图像对齐方面均优于原始方法。
- Conclusion: TPO框架实现了无需配对图像偏好数据的免费午餐对齐，具有良好的通用性和兼容性。


### [42] [V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs](https://arxiv.org/abs/2509.25773)
*Zhengpeng Shi,Hengli Li,Yanpeng Zhao,Jianqun Zhou,Yuxuan Wang,Qinrong Cui,Wei Bi,Songchun Zhu,Bo Zhao,Zilong Zheng*

Main category: cs.CV

TL;DR: v-HUB是一个新的视觉中心视频幽默理解基准，包含来自经典无声电影和在线资源的短视频，用于评估多模态大语言模型仅通过视觉线索理解幽默的能力。

- Motivation: 评估和诊断多模态大语言模型理解幽默的能力，这对于增强人机交互中的参与度具有实际意义。
- Method: 构建v-HUB基准，包含精选的少语言短视频，配有丰富注释（字幕、描述、解释），支持字幕匹配和幽默解释等评估任务，并构建开放式视频问答任务。
- Result: 实验结果显示MLLMs仅通过视觉线索理解幽默存在困难，所有模型在从文本评估转向视频评估（无音频）时性能显著下降。加入音频有助于视频幽默理解。
- Conclusion: 声音信息对视频幽默理解很重要，整合更丰富的模态对于复杂视频理解任务具有前景。


### [43] [PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models](https://arxiv.org/abs/2509.25774)
*Jeongjae Lee,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出PCPO框架解决文本到图像模型强化学习中的训练不稳定问题，通过比例信用分配加速收敛并提升图像质量

- Motivation: 现有策略梯度方法在文本到图像模型对齐中存在训练不稳定和高方差问题，阻碍收敛速度并损害图像质量
- Method: 引入比例信用策略优化(PCPO)，通过稳定目标重构和时间步重新加权来强制比例信用分配
- Result: PCPO显著优于现有策略梯度基线，包括最先进的DanceGRPO，在加速收敛和提升图像质量方面表现优异
- Conclusion: PCPO通过解决比例信用分配问题有效稳定了训练过程，避免了模型崩溃，实现了更好的图像生成质量


### [44] [Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation](https://arxiv.org/abs/2509.25776)
*Mingyu Kang,Yong Suk Choi*

Main category: cs.CV

TL;DR: 提出了一种名为ENM Inversion的新反演技术，用于文本引导的图像编辑，通过搜索最优噪声图来同时保证内容保持和可编辑性。

- Motivation: 现有反演方法在忠实重建源图像的同时，限制了编辑的灵活性，难以紧密遵循目标文本提示。
- Method: 分析噪声图特性以增强可编辑性，引入可编辑噪声精炼，通过最小化重建和编辑噪声图之间的差异来对齐期望编辑。
- Result: 在广泛的图像编辑任务中，ENM Inversion在内容保持和编辑保真度方面优于现有方法，并能轻松应用于视频编辑。
- Conclusion: ENM Inversion是一种有效的反演技术，能够平衡内容重建和编辑灵活性，适用于图像和视频编辑任务。


### [45] [Self-Evolving Vision-Language Models for Image Quality Assessment via Voting and Ranking](https://arxiv.org/abs/2509.25787)
*Wen Wen,Tianwu Zhi,Kanglong Fan,Yang Li,Xinge Peng,Yabin Zhang,Yiting Liao,Junlin Li,Li Zhang*

Main category: cs.CV

TL;DR: EvoQuality是一个无需人工标注的自监督框架，通过自一致性原则和群体相对策略优化，让视觉语言模型自主提升图像质量评估能力。

- Motivation: 现有的视觉语言模型后训练方法依赖昂贵的人工标注数据，而在感知领域如图像质量评估中，自监督技术的应用仍未被充分探索。
- Method: 采用自一致性原则生成伪标签，通过成对多数投票建立相对质量共识，然后使用群体相对策略优化进行迭代演化。
- Result: 在零样本设置下，基础视觉语言模型的PLCC性能提升了31.8%，在7个IQA基准测试中有5个超越了最先进的监督模型。
- Conclusion: EvoQuality证明了视觉语言模型可以通过完全自监督的方式有效提升感知能力，在图像质量评估任务上达到甚至超越监督方法的性能。


### [46] [EchoingECG: An Electrocardiogram Cross-Modal Model for Echocardiogram Tasks](https://arxiv.org/abs/2509.25791)
*Yuan Gao,Sangwook Kim,Chris McIntosh*

Main category: cs.CV

TL;DR: EchoingECG是一个概率性师生模型，利用不确定性感知的ECG嵌入和ECHO监督来改进基于ECG的心脏功能预测。

- Motivation: 心电图(ECG)成本低且易于获取，而超声心动图(ECHO)需要大量医院资源但在临床心脏评估中起关键作用。研究旨在利用ECG预测传统上需要更复杂模态(如ECHO)获得的关键结果。
- Method: 集成概率跨模态嵌入(PCME++)与ECHO-CLIP(在ECHO-文本对上预训练的视觉语言模型)，通过概率对比框架将ECHO知识蒸馏到ECG表示中。
- Result: 在零样本、少样本和微调设置下，EchoingECG在基于ECG的ECHO预测方面优于最先进的基础ECG模型。方差估计增强了模型性能理解，识别了ECG中的不确定性区域。
- Conclusion: 该方法成功地将ECHO知识蒸馏到ECG表示中，提高了ECG在心脏功能预测中的效用，同时通过不确定性估计提供了更好的模型可解释性。


### [47] [Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding](https://arxiv.org/abs/2509.25794)
*Haotian Xue,Yunhao Ge,Yu Zeng,Zhaoshuo Li,Ming-Yu Liu,Yongxin Chen,Jiaojiao Fan*

Main category: cs.CV

TL;DR: 提出了Point-It-Out (PIO)基准，通过精确的视觉定位系统评估视觉语言模型的具身推理能力，包含三个层次：目标定位、任务驱动指向和视觉轨迹预测。

- Motivation: 现有基准主要通过基于图像标注的多选题评估VLMs的具身推理能力，缺乏对精确视觉定位能力的系统评估。
- Method: 设计了三阶段分层评估协议（S1-S3），在室内、厨房、驾驶和机器人操作等关键领域收集数据，对10多个最先进VLM进行广泛实验。
- Result: 发现GPT-4o等通用模型在精确视觉定位方面表现不如某些开源模型；MoLMO在S1和S2表现良好但在S3（需要视觉轨迹规划）上表现不佳。
- Conclusion: PIO基准揭示了不同VLM在具身推理能力上的差异，强调了精确视觉定位和轨迹规划能力的重要性。


### [48] [Adapting SAM with Dynamic Similarity Graphs for Few-Shot Parameter-Efficient Small Dense Object Detection: A Case Study of Chickpea Pods in Field Conditions](https://arxiv.org/abs/2509.25805)
*Xintong Jiang,Yixue Liu,Mohamed Debbagh,Yu Tian,Valerio Hoyos-Villegas,Viacheslav Adamchuk,Shangpeng Sun*

Main category: cs.CV

TL;DR: 该研究提出了一种动态相似性图适应(DSGA)模块，结合LoRA技术，在极少量训练数据下对Segment Anything Model进行高效微调，用于农业场景中的小密集物体分割。

- Motivation: 解决农业计算机视觉任务中基础模型参数高效微调的挑战，特别是在训练数据有限和复杂田间条件下的精确分割问题。
- Method: 使用动态相似性图构建，包含可学习多项式衰减初始化权重排序机制和自适应局部特征聚合，结合LoRA技术建立互补优化框架。
- Result: 在鹰嘴豆荚数据集上，DSGA+LoRA在2、4、8和10样本设置下均表现优异，结构度量提升17.31%，自适应F度量提升62.36%，参数仅需4.00M（原SAM的4.26%）。
- Conclusion: 该方法在复杂农业环境中实现了准确的前景和实例分割，为自动化农业监测应用提供了实用解决方案，在10-120个豆荚的图像中实现了0.8987的调整R平方值。


### [49] [Logo-VGR: Visual Grounded Reasoning for Open-world Logo Recognition](https://arxiv.org/abs/2509.25811)
*Zichen Liang,Jingjing Fei,Jie Wang,Zheming Yang,Changqing Li,Pei Wu,Minghui Qiu,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: 本文提出Logo-VGR方法，将logo识别重新定义为基于比较的任务，通过领域特定的多模态推理实现大规模品牌识别的泛化，在OOD设置中比基线方法提升近10个点。

- Motivation: 现有的多模态大语言模型主要在通用基准上评估，在智能产品审核等特定领域应用不足。传统logo识别方法需要记忆数万个品牌的表示，在实际场景中不实用。
- Method: Logo-VGR将logo识别重新定义为基于比较的任务，匹配产品图像与候选logo而非直接生成品牌标签。引入领域特定多模态推理：Logo感知注入领域知识，Logo引导的视觉基础推理增强模型推理能力。
- Result: 实验结果显示Logo-VGR在OOD设置中比强基线方法提升近10个点，表现出优越的泛化能力。
- Conclusion: Logo-VGR通过新的领域特定多模态推理范式，解决了现有模型过度拟合品牌分布的问题，实现了大规模品牌识别的有效泛化。


### [50] [Overview of GeoLifeCLEF 2023: Species Composition Prediction with High Spatial Resolution at Continental Scale Using Remote Sensing](https://arxiv.org/abs/2509.25816)
*Christophe Botella,Benjamin Deneu,Diego Marcos,Maximilien Servajean,Theo Larcher,Cesar Leblanc,Joaquim Estopinan,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: GeoLifeCLEF 2023竞赛使用深度学习模型和遥感数据预测欧洲植物物种分布，通过500万观测数据和多种环境变量进行多标签分类，发现单正标签训练方法在多标签评估中存在偏差，并提出结合单标签和多标签数据的有效学习策略。

- Motivation: 理解物种时空分布是生态学和保护学的核心，通过将物种观测与地理环境预测因子配对，可以建模环境与物种关系，推动深度学习模型和遥感数据在该领域的应用。
- Method: 组织GeoLifeCLEF 2023开放机器学习竞赛，使用500万植物物种观测数据，结合高分辨率遥感影像、土地覆盖、高程以及粗分辨率气候、土壤和人类足迹变量，进行多标签分类任务。
- Result: 评估了模型在2.2万个小样地中预测物种组成的能力，发现单正标签训练方法在多标签评估中面临偏差问题。
- Conclusion: 提出了结合单标签和多标签数据的新学习策略，该方法被证明是有效的，为物种分布建模提供了改进方向。


### [51] [VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions](https://arxiv.org/abs/2509.25818)
*Kazuki Matsuda,Yuiga Wada,Shinnosuke Hirano,Seitaro Otsuki,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出了VELA评估指标和LongCap-Arena基准，用于自动评估多模态大语言模型生成的长图像描述，在三个维度上超越现有指标并达到超人类表现。

- Motivation: 现有图像描述评估指标主要针对短描述设计，不适用于长描述评估；现有LLM-as-a-Judge方法推理速度慢且视觉信息融合效率低。
- Method: 提出VELA评估指标，基于创新的LLM-Hybrid-as-a-Judge框架开发；同时构建LongCap-Arena基准，包含7,805张图像、对应长参考描述和候选描述，以及32,246个人类判断。
- Result: VELA在LongCap-Arena基准上超越了现有评估指标，并在描述性、相关性和流畅性三个维度上达到了超人类表现。
- Conclusion: VELA为长图像描述评估提供了有效的自动评估解决方案，解决了现有方法的局限性，在多个评估维度上表现出色。


### [52] [Training-Free Reward-Guided Image Editing via Trajectory Optimal Control](https://arxiv.org/abs/2509.25845)
*Jinho Chang,Jaemin Kim,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出了一种无需训练、基于奖励引导的图像编辑框架，将编辑过程建模为轨迹最优控制问题，在扩散模型的反向过程中通过伴随状态迭代更新来指导编辑过程。

- Motivation: 现有基于奖励引导的方法主要用于图像生成，而在图像编辑任务中如何保持源图像语义内容同时增强目标奖励的研究还很缺乏。
- Method: 将图像编辑建模为轨迹最优控制问题，把扩散模型的反向过程视为从源图像出发的可控轨迹，通过迭代更新伴随状态来引导编辑过程。
- Result: 在多种编辑任务上的实验表明，该方法显著优于现有的基于反转的无训练引导基线，在奖励最大化和源图像保真度之间实现了更好的平衡，且避免了奖励黑客问题。
- Conclusion: 该框架为无需训练的奖励引导图像编辑提供了一种有效解决方案，在保持源图像内容的同时成功增强了目标奖励。


### [53] [More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.25848)
*Xinyu Tian,Shu Zou,Zhaoyuan Yang,Mengqi He,Fabian Waschkowski,Lukas Wesemann,Peter Tu,Jing Zhang*

Main category: cs.CV

TL;DR: 该论文发现多模态推理在增强逻辑推理能力的同时会损害视觉感知基础，提出Vision-Anchored Policy Optimization (VAPO)方法来解决视觉遗忘问题，并在多个基准测试中取得新的最优结果。

- Motivation: 虽然多模态推理在视觉任务中表现出色，但研究发现它会逐渐削弱模型的视觉感知基础，导致在基本视觉问题上出现识别失败，这种现象被称为视觉遗忘。
- Method: 提出Vision-Anchored Policy Optimization (VAPO)方法，通过明确引导推理过程朝向视觉基础轨迹，增强模型对视觉信息的依赖。
- Result: VAPO-Thinker-7B模型显著增强了模型对视觉信息的依赖，在多个基准测试中取得了新的最优结果。
- Conclusion: VAPO方法有效解决了多模态推理中的视觉遗忘问题，平衡了逻辑推理能力和视觉感知基础，为视觉语言模型的发展提供了重要贡献。


### [54] [MuSLR: Multimodal Symbolic Logical Reasoning](https://arxiv.org/abs/2509.25851)
*Jundong Xu,Hao Fei,Yuhui Zhang,Liangming Pan,Qijun Huang,Qian Liu,Preslav Nakov,Min-Yen Kan,William Yang Wang,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出了首个多模态符号逻辑推理基准MuSLR，评估了7个最先进的视觉语言模型，发现它们在多模态符号推理方面表现不佳，最佳模型GPT-4.1仅达到46.8%。提出了LogiCAM框架提升性能。

- Motivation: 多模态符号逻辑推理在自动驾驶、医疗诊断等高风险应用中至关重要，其严谨的确定性推理有助于防止严重后果。需要评估当前视觉语言模型在这方面的能力。
- Method: 创建了MuSLR基准，包含1,093个实例，涵盖7个领域，包括35个原子符号逻辑和976个逻辑组合，推理深度从2到9。提出了LogiCAM框架，将形式逻辑规则应用于多模态输入。
- Result: 评估的7个最先进视觉语言模型在多模态符号推理方面都表现不佳，GPT-4.1最佳但仅46.8%。LogiCAM框架将GPT-4.1的思维链性能提升了14.13%，在复杂逻辑如一阶逻辑上提升更大。
- Conclusion: 当前视觉语言模型在多模态符号逻辑推理方面存在显著不足，约70%的失败源于模态间的逻辑不对齐。LogiCAM框架展示了改进潜力，为未来研究提供了关键见解。


### [55] [PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection](https://arxiv.org/abs/2509.25856)
*Po-Han Huang,Jeng-Lin Li,Po-Hsuan Huang,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.CV

TL;DR: 提出了一个统一的基于补丁的异常检测框架PatchEAD，无需训练即可与多种基础模型兼容，在少样本和零样本场景下表现优异。

- Motivation: 现有工业异常检测方法主要关注文本提示调优，而视觉对应部分则分散在各个基础模型的处理步骤中，缺乏统一框架。
- Method: 构建视觉提示技术，包括对齐模块和前景掩码，形成补丁专属异常检测框架，无需训练即可适配多种基础模型。
- Result: 实验显示在少样本和批量零样本场景下性能优于先前工作，即使没有文本特征也能取得优异结果。
- Conclusion: 统一补丁框架能够实现快速、轻量校准的部署，无需精心设计的文本提示，为实际视觉检测提供了可行的基础模型选择和配置指导。


### [56] [LiDAR Point Cloud Colourisation Using Multi-Camera Fusion and Low-Light Image Enhancement](https://arxiv.org/abs/2509.25859)
*Pasindu Ranasinghe,Dibyayan Patra,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 提出了一种硬件无关的方法，使用多摄像头输入为机械LiDAR生成彩色点云，实现360度覆盖，特别在低光照条件下具有鲁棒性。

- Motivation: 融合相机和LiDAR数据以增强空间理解，特别是在低光照条件下提高感知能力。
- Method: 通过初始校准确定相机内参，自动计算LiDAR与相机间的几何变换，集成低光照图像增强模块，使用色彩校正确保相机数据一致性。
- Result: 使用Velodyne Puck Hi-Res LiDAR和四摄像头配置测试，优化软件实现实时性能，在极低光照下可靠着色，成功恢复原本不可检测的场景细节。
- Conclusion: 该方法无需专用校准目标，简化设置流程，在低光照条件下有效增强LiDAR点云的色彩信息，提升环境感知能力。


### [57] [MAPLE: Multi-scale Attribute-enhanced Prompt Learning for Few-shot Whole Slide Image Classification](https://arxiv.org/abs/2509.25863)
*Junjie Zhou,Wei Shao,Yagao Yue,Wei Mu,Peng Wan,Qi Zhu,Daoqiang Zhang*

Main category: cs.CV

TL;DR: MAPLE是一个用于少样本全切片图像分类的分层提示学习框架，通过整合多尺度视觉语义并在实体和切片级别进行预测，解决了现有方法无法捕捉组织学实体亚型特异性表型变异的问题。

- Motivation: 现有方法通常依赖切片级提示，无法捕捉对癌症诊断至关重要的组织学实体（如细胞核、腺体）的亚型特异性表型变异，限制了少样本病理诊断的性能。
- Method: 利用LLM生成实体级和切片级提示；提出实体引导的交叉注意力模块生成实体级特征；开发跨尺度实体图学习模块丰富实体表示；结合实体级和切片级输出进行最终预测。
- Result: 在三个癌症队列上的实验证实了该方法在解决少样本病理诊断任务中的有效性。
- Conclusion: MAPLE框架通过多尺度属性增强的提示学习，能够有效捕捉组织学实体的细粒度表型特征，提升少样本全切片图像分类性能。


### [58] [DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning](https://arxiv.org/abs/2509.25866)
*Chi Zhang,Haibo Qiu,Qiming Zhang,Zhixiong Zeng,Lin Ma,Jing Zhang*

Main category: cs.CV

TL;DR: DeepSketcher是一个包含图像-文本交错数据集和自包含模型的综合套件，通过直接在视觉嵌入空间生成“视觉思维”来实现无需外部工具的图像交互推理。

- Motivation: 当前'图像思维'范式在数据构建准确性、结构设计和应用场景方面仍有很大探索空间，需要推进多模态推理的发展。
- Method: 构建包含31k条思维链推理轨迹的数据集，设计直接在视觉嵌入空间生成视觉思维而不调用外部工具的模型。
- Result: 在多模态推理基准测试中表现出强大性能，验证了数据集的有效性和模型设计的优越性。
- Conclusion: DeepSketcher通过工具无关的图像交互推理方法，为推进多模态推理提供了有效解决方案。


### [59] [A Multimodal LLM Approach for Visual Question Answering on Multiparametric 3D Brain MRI](https://arxiv.org/abs/2509.25889)
*Arvind Murari Vepa,Yannan Yu,Jingru Gan,Anthony Cuturrufo,Weikai Li,Wei Wang,Fabien Scalzo,Yizhou Sun*

Main category: cs.CV

TL;DR: mpLLM是一种用于3D脑部多参数MRI视觉问答的提示条件分层混合专家架构，通过模态级和令牌级专家融合多个相关3D模态，无需图像-报告预训练即可高效训练。

- Motivation: 解决3D脑部多参数MRI视觉问答中图像-文本配对监督数据有限的问题，开发能够处理多个相关3D模态的医学视觉问答系统。
- Method: 采用提示条件分层混合专家架构，包含模态级和令牌级投影专家；集成合成视觉问答协议从分割标注生成医学相关VQA；与医学专家合作进行临床验证。
- Result: 在多个mpMRI数据集上平均优于强医学VLM基线5.3%；创建了首个临床验证的3D脑部mpMRI VQA数据集；展示了方法的医学实用性。
- Conclusion: 模态级和令牌级专家以及提示条件路由的重要性通过消融实验得到验证；该方法在医学3D多模态视觉问答任务中表现出色。


### [60] [LLaVAShield: Safeguarding Multimodal Multi-Turn Dialogues in Vision-Language Models](https://arxiv.org/abs/2509.25896)
*Guolei Huang,Qingzhi Peng,Gan Xu,Yuxuan Lu,Yongjun Shen*

Main category: cs.CV

TL;DR: 该论文提出了多模态多轮对话安全性的首个系统性定义和研究，创建了MMDS数据集和基于MCTS的自动红队框架，并开发了LLaVAShield工具用于联合检测和评估用户输入与助手回复的风险。

- Motivation: 随着视觉语言模型进入交互式多轮使用，单轮或单模态审核会遗漏新的安全风险。在多模态多轮对话中，恶意意图可能分布在多个轮次和图像中，而上下文相关的回复仍可能推进有害内容。
- Method: 基于蒙特卡洛树搜索构建自动多模态多轮红队框架来生成不安全的多模态多轮对话；开发MMDS数据集包含4,484个标注样本；提出LLaVAShield工具联合检测用户输入和助手回复的风险。
- Result: LLaVAShield在多模态多轮内容审核任务和动态策略配置下持续优于强基线，建立了新的最先进结果。
- Conclusion: 该研究为多模态多轮对话安全提供了系统性框架、数据集和有效工具，将公开释放数据集和模型以支持未来研究。


### [61] [VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs](https://arxiv.org/abs/2509.25916)
*Peng Liu,Haozhan Shen,Chunxin Fang,Zhicheng Sun,Jiajia Liao,Tiancheng Zhao*

Main category: cs.CV

TL;DR: VLM-FO1将目标感知从坐标生成问题转化为特征检索任务，通过混合细粒度区域编码器和基于token的引用系统，在不损害基础模型能力的前提下实现卓越的细粒度视觉定位性能。

- Motivation: 视觉语言模型在高层场景理解表现出色，但在需要精确定位的细粒度感知任务上表现不佳，因为语言中心架构难以生成精确的数值坐标。
- Method: 提出VLM-FO1框架，包含混合细粒度区域编码器（HFRE）和基于token的引用系统，采用两阶段训练策略，可作为即插即用模块与任何预训练VLM集成。
- Result: 在多个基准测试中达到最先进性能，在目标定位、区域生成理解和视觉区域推理方面表现卓越。
- Conclusion: VLM-FO1建立了感知感知VLMs的有效灵活范式，弥合了高层推理与细粒度视觉定位之间的差距。


### [62] [The Impact of Scaling Training Data on Adversarial Robustness](https://arxiv.org/abs/2509.25927)
*Marco Zimmerli,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 该研究分析了36个先进视觉模型的对抗鲁棒性，发现数据量和模型规模遵循对数缩放定律，但数据质量、架构和训练目标比原始规模对鲁棒性影响更大。

- Motivation: 尽管深度神经网络架构和训练方法不断进步，但仍然容易受到对抗样本攻击。研究旨在探索训练数据特性如何影响对抗鲁棒性。
- Method: 评估了36个最先进的视觉模型，包括监督学习、自监督学习和对比学习方法，训练数据规模从120万到220亿张图像。在六种黑盒攻击类别下评估模型鲁棒性。
- Result: 鲁棒性随数据量和模型规模呈对数缩放：数据量增加10倍，攻击成功率平均降低约3.2%；模型规模增加10倍，攻击成功率平均降低约13.4%。自监督模型在精选数据集上表现优于更大但质量较差的数据集。
- Conclusion: 虽然扩展规模能提高鲁棒性，但数据质量、架构和训练目标在实现广谱对抗鲁棒性方面比原始规模更具决定性作用。


### [63] [UniMMAD: Unified Multi-Modal and Multi-Class Anomaly Detection via MoE-Driven Feature Decompression](https://arxiv.org/abs/2509.25934)
*Yuan Zhao,Youwei Pang,Lihe Zhang,Hanqi Liu,Jiaming Zuo,Huchuan Lu,Xiaoqi Zhao*

Main category: cs.CV

TL;DR: UniMMAD是一个统一的多模态多类异常检测框架，通过MoE驱动的特征解压缩机制实现自适应和分离的重建，在9个数据集上达到最先进性能。

- Motivation: 现有异常检测方法将模态和类别视为独立因素，导致解决方案碎片化、内存开销过大，且重建方法在处理大域变化时存在边界扭曲、域干扰和高误报率问题。
- Method: 采用混合专家驱动的特征解压缩机制，遵循"通用到特定"范式：编码阶段压缩多模态输入为通用特征，解码阶段通过稀疏门控交叉MoE将通用特征解压缩为模态和类别特定形式。
- Result: 在3个领域、12种模态、66个类别的9个异常检测数据集上达到最先进性能，同时参数使用减少75%。
- Conclusion: UniMMAD通过统一框架有效解决了多模态多类异常检测中的碎片化问题和域间干扰，实现了高效且高性能的检测。


### [64] [CO3: Contrasting Concepts Compose Better](https://arxiv.org/abs/2509.25940)
*Debottam Dutta,Jianchong Chen,Rajalaxmi Rajagopalan,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出CO3方法，通过校正采样策略改善文本到图像扩散模型中多概念提示的保真度，避免概念丢失或碰撞的问题

- Motivation: 解决多概念提示（如"猫和狗"）在扩散模型中常出现的概念缺失、模糊或相互碰撞的问题，这是由于模型在训练过程中对某些概念学习过强导致的模式混合
- Method: 引入校正采样策略，引导模型远离联合提示行为与单个概念重叠过强的区域，转向所有概念能够平衡共存的"纯"联合模式；分析现有多概念引导方案的不稳定权重机制，并调整采样以保持在有利区域内
- Result: 在多样化多概念提示上的实验表明，相比标准基线和现有组合方法，CO3在概念覆盖率、平衡性和鲁棒性方面都有提升，概念丢失或扭曲的情况更少
- Conclusion: 轻量级的校正引导可以显著缓解现代扩散系统中脆弱的语义对齐行为


### [65] [Self-Supervised Anatomical Consistency Learning for Vision-Grounded Medical Report Generation](https://arxiv.org/abs/2509.25963)
*Longzhen Yang,Zhangkai Ni,Ying Wen,Yihang Liu,Lianghua He,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出SS-ACL框架，无需专家标注即可生成基于视觉证据的医学报告，通过解剖一致性学习实现报告与解剖区域的自动对齐。

- Motivation: 现有方法依赖单独训练的检测模块，需要大量专家标注，成本高且受数据集病理分布偏差限制。
- Method: 构建分层解剖图，通过递归重建细粒度解剖区域实现空间对齐，引入基于解剖一致性的区域级对比学习增强语义对齐。
- Result: 在词汇准确率上优于SOTA方法10%，临床有效性提升25%，在零样本视觉定位任务上超越领先视觉基础模型8%。
- Conclusion: SS-ACL无需专家标注即可生成准确且可解释的医学报告，在多个下游视觉任务中表现优异。


### [66] [A Multi-purpose Tracking Framework for Salmon Welfare Monitoring in Challenging Environments](https://arxiv.org/abs/2509.25969)
*Espen Uri Høgstedt,Christian Schellewald,Annette Stahl,Rudolf Mester*

Main category: cs.CV

TL;DR: 提出了一种基于姿态估计的鲑鱼追踪框架，通过提取鲑鱼身体部位信息来解决水下场景中的跟踪挑战，并用于自动化福利监测。

- Motivation: 现有计算机视觉方法只能单独计算福利指标，且在水下鲑鱼场景中易受遮挡、外观相似和运动相似等问题影响，需要更高效的自动化监测方案。
- Method: 使用姿态估计网络提取鲑鱼及其身体部位的边界框，通过专门模块利用身体部位信息解决水下鲑鱼场景特有的挑战，然后利用高细节身体部位轨迹计算福利指标。
- Result: 该方法在拥挤场景中的鲑鱼ID转移和转弯时的ID切换两个跟踪挑战中，均优于当前最先进的行人跟踪器BoostTrack，并成功应用于基于尾摆分析的福利监测。
- Conclusion: 所提出的追踪框架能够有效解决水下鲑鱼场景的跟踪难题，为自动化鲑鱼福利监测提供了可行方案。


### [67] [PinPoint3D: Fine-Grained 3D Part Segmentation from a Few Clicks](https://arxiv.org/abs/2509.25970)
*Bojun Zhang,Hangjian Ye,Hao Zheng,Jianzheng Huang,Zhengyu Lin,Zhenhong Guo,Feng Zheng*

Main category: cs.CV

TL;DR: PinPoint3D是一个用于细粒度、多粒度3D分割的交互式框架，通过少量用户点击即可生成精确的部件级掩码，显著优于现有方法。

- Motivation: 现有交互式分割方法主要局限于粗粒度的实例级目标，而非交互式方法在处理稀疏真实世界扫描时表现不佳，且缺乏标注数据。
- Method: 开发了新的3D数据合成流水线来创建大规模场景级数据集，包含密集部件标注；提出了基于用户点点击的交互式分割框架。
- Result: 在首次点击设置下，每个对象部件的平均IoU达到约55.8%，仅需少量额外点击即可超过71.3% IoU；相比现有最佳基线，IoU和精度提升高达16%。
- Conclusion: 这项工作代表了在复杂3D环境中实现更细致和精确的机器感知与交互的重要进展。


### [68] [Towards Reliable and Holistic Visual In-Context Learning Prompt Selection](https://arxiv.org/abs/2509.25989)
*Wenxiao Wu,Jing-Hao Xue,Chengming Xu,Chen Liu,Xinwei Sun,Changxin Gao,Nong Sang,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出了RH-Partial2Global方法，通过结合jackknife conformal prediction和covering design-based sampling，改进了Partial2Global方法，实现了更可靠和全面的视觉上下文学习示例选择。

- Motivation: 当前VICL方法基于相似性优先假设，但这一假设缺乏充分的理论依据。Partial2Global方法依赖随机采样构建全局排序，导致覆盖不完整和冗余采样问题。
- Method: 使用jackknife conformal prediction构建可靠替代集，采用covering design-based sampling确保成对偏好的全面均匀覆盖。
- Result: 在多种视觉任务上的实验表明，RH-Partial2Global表现出色，性能优于Partial2Global。
- Conclusion: RH-Partial2Global通过更可靠的替代集构建和更全面的采样策略，有效提升了视觉上下文学习中示例选择的性能。


### [69] [VRWKV-Editor: Reducing quadratic complexity in transformer-based video editing](https://arxiv.org/abs/2509.25998)
*Abdelilah Aitrouga,Youssef Hmamouche,Amal El Fallah Seghrouchni*

Main category: cs.CV

TL;DR: 提出了VRWKV-Editor视频编辑模型，通过线性时空聚合模块降低计算复杂度，在保持质量的同时实现3.7倍加速和60%内存节省

- Motivation: 传统注意力机制在视频编辑中存在二次计算复杂度问题，难以适应长时长和高分辨率视频，限制了实时视频处理的实际应用
- Method: 将线性时空聚合模块集成到基于扩散的视频模型中，利用RWKV变换器的双向加权键值循环机制捕获全局依赖并保持时间一致性
- Result: 相比最先进的基于扩散的视频编辑方法，实现了3.7倍加速和60%内存使用降低，同时在帧一致性和文本对齐方面保持竞争力
- Conclusion: 该方法在长视频编辑中与自注意力架构的速度差距更加显著，为高效视频编辑提供了可行解决方案


### [70] [Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations](https://arxiv.org/abs/2509.26004)
*Nicola Messina,Rosario Leonardi,Luca Ciampi,Fabio Carrara,Giovanni Maria Farinella,Fabrizio Falchi,Antonino Furnari*

Main category: cs.CV

TL;DR: 提出NS-iHOS任务，利用自然语言叙述作为弱监督来学习手部物体分割，无需像素级标注。WISH模型通过从叙述中提取知识实现无叙述推理时的分割，在EPIC-Kitchens和Ego4D数据集上达到全监督方法50%以上的性能。

- Motivation: 解决第一人称视角图像中用户操作物体识别任务因标注数据稀缺而进展缓慢的问题，利用廉价易得的自然语言叙述作为弱监督信号。
- Method: 提出NS-iHOS任务框架和WISH端到端模型，从叙述中学习合理的手-物体关联，实现无叙述推理时的手部物体分割。
- Result: 在EPIC-Kitchens和Ego4D数据集上，WISH模型超越所有基线方法，恢复全监督方法50%以上的性能，且不使用细粒度像素级标注。
- Conclusion: 自然语言叙述可作为有效的弱监督信号，显著降低手部物体分割任务的标注成本，为相关应用提供可行解决方案。


### [71] [AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment](https://arxiv.org/abs/2509.26006)
*Hanwei Zhu,Yu Tian,Keyan Ding,Baoliang Chen,Bolin Chen,Shiqi Wang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了AgenticIQA框架，将视觉语言模型与传统图像质量评估工具动态结合，通过分解为四个子任务来提升评估准确性和可解释性。

- Motivation: 传统图像质量评估方法使用固定模型输出标量分数，难以适应不同失真类型、用户特定查询和可解释性需求，且评分与解释通常独立处理。
- Method: AgenticIQA框架包含规划器、执行器和总结器，将IQA分解为失真检测、失真分析、工具选择和工具执行四个子任务，动态协调VLM与传统IQA工具。
- Result: 在多个IQA数据集上的实验表明，AgenticIQA在评分准确性和解释对齐方面均优于强基线方法。
- Conclusion: AgenticIQA通过模块化代理框架有效解决了传统IQA方法的局限性，提供了更准确和可解释的图像质量评估。


### [72] [PFDepth: Heterogeneous Pinhole-Fisheye Joint Depth Estimation via Distortion-aware Gaussian-Splatted Volumetric Fusion](https://arxiv.org/abs/2509.26008)
*Zhiwei Zhang,Ruikai Xu,Weijian Zhang,Zhizhong Zhang,Xin Tan,Jingyu Gong,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: PFDepth是首个针孔-鱼眼异构多视角深度估计框架，通过利用针孔和鱼眼图像的互补特性进行联合优化，在KITTI-360和RealHet数据集上达到最先进性能。

- Motivation: 利用针孔和鱼眼图像的互补特性（无失真vs失真、小FOVvs大FOV、远场vs近场）进行异构多视角深度估计的联合优化。
- Method: 采用统一架构处理任意组合的针孔和鱼眼相机；将2D特征提升到规范3D体积空间；设计异构空间融合模块处理失真感知体积特征；将传统体素融合重新表述为3D高斯表示，使用可学习潜在高斯球动态适应局部图像纹理。
- Result: 在KITTI-360和RealHet数据集上超越了当前主流深度网络，达到最先进的性能水平。
- Conclusion: 这是首个系统研究异构针孔-鱼眼深度估计的工作，提供了技术新颖性和有价值的实证见解。


### [73] [New Fourth-Order Grayscale Indicator-Based Telegraph Diffusion Model for Image Despeckling](https://arxiv.org/abs/2509.26010)
*Rajendra K. Ray,Manish Kumar*

Main category: cs.CV

TL;DR: 提出了一种结合扩散和波动特性的四阶非线性PDE模型，用于抑制乘性噪声，相比传统二阶PDE模型能更好地减少块状伪影并保留细节。

- Motivation: 传统二阶PDE模型在抑制乘性噪声时会在去噪早期引入块状伪影，需要开发更有效的模型来解决这一问题。
- Method: 使用四阶非线性PDE模型，结合拉普拉斯算子和强度值指导的扩散过程以及保持细节的波动部分，并扩展到彩色图像处理。
- Result: 在PSNR、MSSIM和SI等指标上均优于现有的二阶各向异性扩散方法，在灰度和彩色图像上都取得了更好的去噪效果。
- Conclusion: 所提出的四阶PDE模型在抑制乘性噪声方面优于现有方法，能有效减少伪影并保持图像细节和纹理。


### [74] [SETR: A Two-Stage Semantic-Enhanced Framework for Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2509.26012)
*Yuqi Xiao,Yingying Zhu*

Main category: cs.CV

TL;DR: SETR提出了一种用于零样本组合图像检索的两阶段方法，通过交集驱动的粗检索和基于多模态大语言模型的细粒度重排序，解决了CLIP方法中特征融合不精确和语义关系解析能力不足的问题。

- Motivation: 现有基于CLIP的零样本组合图像检索方法面临两个核心挑战：基于并集的特征融合会携带不相关的背景细节，以及全局余弦相似度缺乏解析细粒度语义关系的能力。
- Method: SETR采用两阶段检索：粗检索阶段使用交集驱动策略保留参考图像和相对文本的重叠语义；重排序阶段使用LoRA适配的多模态LLM进行二元语义相关性判断。
- Result: 在CIRR、Fashion-IQ和CIRCO数据集上的实验表明，SETR达到了新的最先进性能，在CIRR上的Recall@1提升了高达15.15个百分点。
- Conclusion: 两阶段推理为鲁棒且可移植的零样本组合图像检索提供了一个通用范式。


### [75] [GeoLink: Empowering Remote Sensing Foundation Model with OpenStreetMap Data](https://arxiv.org/abs/2509.26016)
*Lubian Bai,Xiuyuan Zhang,Siqi Zhang,Zepeng Zhang,Haoyu Wang,Wei Qin,Shihong Du*

Main category: cs.CV

TL;DR: GeoLink是一个多模态框架，利用OpenStreetMap数据增强遥感基础模型，通过跨模态空间相关性实现多粒度学习信号，支持从土地覆盖分类到城市功能区映射等多种地理任务。

- Motivation: 解决遥感数据与OSM数据之间的模态差异（包括数据结构、内容和空间粒度的不同），实现有效的多模态协同，推动地理空间智能发展。
- Method: 在预训练阶段使用OSM数据提供的多粒度学习信号，通过跨模态空间相关性指导信息交互与协作，并引入图像掩码重建实现稀疏输入；在下游任务中生成单模态和多模态细粒度编码。
- Result: 实验表明，在预训练中融入OSM数据提升了遥感图像编码器的性能，在下游任务中融合RS和OSM数据增强了模型对复杂地理场景的适应性。
- Conclusion: 多模态协同在推进高级地理空间人工智能方面具有巨大潜力，空间相关性在多模态地理数据整合中起着关键作用。


### [76] [PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution](https://arxiv.org/abs/2509.26025)
*Shian Du,Menghan Xia,Chang Liu,Xintao Wang,Jing Wang,Pengfei Wan,Di Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: 提出PatchVSR方法，利用视频扩散先验进行分块视频超分辨率，通过双流适配器和位置信息注入实现高效4K超分辨率

- Motivation: 现有视频超分辨率方法采用全尺寸处理，存在计算密集和输出分辨率固定的限制，需要更高效的解决方案
- Method: 提出PatchVSR方法，包含双流适配器（分块分支和全局分支）、位置信息注入和多分块联合调制机制
- Result: 能够基于512x512基础模型实现高质量4K视频超分辨率，具有极高效率
- Conclusion: 分块范式为视频超分辨率提供了灵活高效的解决方案，克服了传统方法的计算限制


### [77] [Causally Guided Gaussian Perturbations for Out-Of-Distribution Generalization in Medical Imaging](https://arxiv.org/abs/2509.26027)
*Haoran Pei,Yuguang Yang,Kexin Liu,Baochang Zhang*

Main category: cs.CV

TL;DR: 提出CGP框架，通过基于视觉变换器的软因果掩码引导空间变化噪声注入，增强OOD泛化能力

- Motivation: 解决深度学习中OOD泛化挑战，特别是在生物医学图像领域，现有方法可能忽略泛化的因果机制
- Method: 使用因果引导的高斯扰动，在背景区域施加强扰动，前景区域施加弱扰动，鼓励模型依赖因果相关特征
- Result: 在WILDS基准Camelyon17上表现优于现有OOD基线方法
- Conclusion: 因果扰动是可靠且可解释泛化的有效工具


### [78] [SeMoBridge: Semantic Modality Bridge for Efficient Few-Shot Adaptation of CLIP](https://arxiv.org/abs/2509.26036)
*Christoph Timmermann,Hyunse Lee,Woojin Lee*

Main category: cs.CV

TL;DR: SeMoBridge通过语义模态桥将图像映射到文本模态，解决了CLIP在少样本分类中的模态内不对齐问题，在低数据场景下表现优异。

- Motivation: CLIP在零样本任务中表现良好，但在少样本分类中存在模态内不对齐问题，这是由于模态间隙和仅跨模态训练目标导致的，使得图像间直接比较不可靠。
- Method: 提出轻量级SeMoBridge方法，通过语义模态桥将图像映射到文本模态，保持语义内容不变。该方法可以是闭式解，也可通过多模态监督训练，结合图像和对齐损失优化投影。
- Result: 训练版本SeMoBridge-T仅需少量训练时间，总体优于其他方法，特别是在低数据场景（1、2、4样本）中表现突出。
- Conclusion: SeMoBridge有效解决了CLIP的模态内不对齐问题，为少样本分类提供了高效解决方案。


### [79] [SGS: Segmentation-Guided Scoring for Global Scene Inconsistencies](https://arxiv.org/abs/2509.26039)
*Gagandeep Singh,Samudi Amarsinghe,Urawee Thani,Ki Fung Wong,Priyanka Singh,Xue Li*

Main category: cs.CV

TL;DR: 扩展HAMMER模型以处理全局场景不一致性，通过分割引导评分(SGS)管道增强对前景-背景不匹配的检测能力，无需重新训练。

- Motivation: HAMMER在多模态操纵检测中表现优异，但在处理主要主体被错误放置到不合理背景的全局场景不一致时表现不佳，需要改进其区域感知推理能力。
- Method: 提出轻量级分割引导评分(SGS)管道：使用人物/面部分割掩码分离前景和背景区域，通过联合视觉语言模型提取嵌入，计算区域感知一致性分数，并与HAMMER原始预测融合。
- Result: SGS显著增强了HAMMER对全局操纵的鲁棒性，改进了二元检测、定位和token级解释，且仅需推理阶段，计算开销可忽略。
- Conclusion: 这项工作证明了区域感知推理在多模态虚假信息检测中的重要性，为处理前景-背景不匹配提供了有效解决方案。


### [80] [DGM4+: Dataset Extension for Global Scene Inconsistency](https://arxiv.org/abs/2509.26047)
*Gagandeep Singh,Samudi Amarsinghe,Priyanka Singh,Xue Li*

Main category: cs.CV

TL;DR: 该论文扩展了DGM4数据集，新增了5000个包含前景-背景不匹配及其与文本操作混合的高质量样本，创建了DGM4+基准数据集，用于测试检测器在局部和全局推理上的能力。

- Motivation: 现有DGM4数据集仅限于局部操作（如人脸交换、属性编辑和字幕更改），缺乏对全局不一致性（如前景背景不匹配）的覆盖，而这类伪造在现实世界中日益普遍。
- Method: 使用OpenAI的gpt-image-1和精心设计的提示词生成以人为中心的新闻风格图像，将真实人物置于荒谬或不可能的背景下。字幕在三种条件下生成：字面、文本属性和文本分割，产生三种新的操作类别。
- Result: 创建了包含FG-BG、FG-BG+TA和FG-BG+TS三类操作的DGM4+数据集，通过质量控制管道确保数据质量，包括人脸可见性、感知哈希去重、基于OCR的文本清理和真实标题长度。
- Conclusion: DGM4+数据集通过引入全局操作补充了现有数据集，为多模态模型（如HAMMER）提供了更全面的评估基准，这些模型目前在处理前景-背景不一致性方面存在困难。


### [81] [Geometric Learning of Canonical Parameterizations of $2D$-curves](https://arxiv.org/abs/2509.26070)
*Ioana Ciuclea,Giorgio Longari,Alice Barbara Tumpach*

Main category: cs.CV

TL;DR: 提出了一种基于主纤维丛截面理论的几何学习方法，用于在分类任务中消除对称性影响，避免数据增强，构建更可持续的算法。

- Motivation: 计算机视觉和医学应用中的数据集通常包含对称性（如旋转、缩放），传统方法使用数据增强来学习这些对称性，但希望找到更可持续的替代方案。
- Method: 使用主纤维丛截面概念来模除对称性，通过优化截面来最大化类别分离度，并在轮廓数据集上应用了平移、旋转、缩放和重参数化等对称群。
- Result: 开发了一个包含恒定速度参数化作为特例的2参数曲线规范参数化族，该方法能够有效测量对称群作用下对象轨道之间的差异。
- Conclusion: 该方法提供了一个简单有效的几何学习框架，具有广泛的应用潜力，代码和教程已开源供使用。


### [82] [EasyOcc: 3D Pseudo-Label Supervision for Fully Self-Supervised Semantic Occupancy Prediction Models](https://arxiv.org/abs/2509.26087)
*Seamie Hayes,Ganesh Sistu,Ciarán Eising*

Main category: cs.CV

TL;DR: 提出使用基础模型生成3D伪标签，结合时序信息进行标签稠密化，显著提升语义占据预测性能，同时设计了轻量级模型EasyOcc。

- Motivation: 现有自监督方法如新视角合成等计算成本高、内存消耗大，且改进难以迁移到其他架构。
- Method: 使用Grounded-SAM和Metric3Dv2生成3D伪标签，利用时序信息稠密化标签，并设计轻量级模型EasyOcc。
- Result: 在OccNeRF上mIoU从9.73提升至14.09（+45%），EasyOcc达到13.86 mIoU，在完整场景评估中达到7.71 mIoU，超越之前最佳模型31%。
- Conclusion: 基础模型、时序上下文和损失计算空间的选择对自监督学习的全面场景理解至关重要。


### [83] [Predicting Penalty Kick Direction Using Multi-Modal Deep Learning with Pose-Guided Attention](https://arxiv.org/abs/2509.26088)
*Pasindu Ranasinghe,Pamudu Ranasinghe*

Main category: cs.CV

TL;DR: 提出一个实时多模态深度学习框架，在足球接触前预测点球方向（左、中、右），使用双分支架构结合视觉特征和姿态关键点，在测试集上达到89%准确率。

- Motivation: 点球经常决定冠军归属，但守门员必须在极短时间内从细微的生物力学线索中预测踢球者的意图。
- Method: 双分支架构：MobileNetV2 CNN提取RGB帧空间特征，LSTM网络处理2D关键点并带注意力机制，姿态关键点引导视觉关注任务相关区域，基于距离的阈值方法在球接触前分割输入序列。
- Result: 在755个真实比赛视频点球事件的数据集上，模型在测试集上达到89%准确率，比纯视觉和纯姿态基线高出14-22%，推理时间为22毫秒。
- Conclusion: 轻量级且可解释的设计适用于守门员训练、战术分析和实时比赛分析。


### [84] [Text-to-Scene with Large Reasoning Models](https://arxiv.org/abs/2509.26091)
*Frédéric Berdoz,Luca A. Lanzendörfer,Nick Tuninga,Roger Wattenhofer*

Main category: cs.CV

TL;DR: Reason-3D是一个基于大型推理模型的文本到场景生成系统，通过对象检索和空间推理生成复杂的3D环境，在视觉保真度和约束遵循方面显著优于现有方法。

- Motivation: 解决当前文本到场景方法在处理复杂几何结构、对象变换和复杂指令遵循方面的局限性。
- Method: 集成基于物理、功能和上下文属性的对象检索，基于隐式和显式布局约束的对象放置，以及碰撞感知的空间推理精炼。
- Result: 在从简单到复杂的室内配置指令评估中，Reason-3D在人类评级的视觉保真度、约束遵循和资源检索质量方面显著优于先前方法。
- Conclusion: 该工作不仅推动了文本到场景生成领域的发展，还展示了现代大型推理模型的高级空间推理能力，并发布了代码库以促进相关研究。


### [85] [EVODiff: Entropy-aware Variance Optimized Diffusion Inference](https://arxiv.org/abs/2509.26096)
*Shigui Li,Wei Chen,Delu Zeng*

Main category: cs.CV

TL;DR: 提出了EVODiff方法，从信息论角度优化扩散模型的推理过程，通过优化条件熵来减少不确定性，在图像生成质量和效率上显著优于现有方法。

- Motivation: 扩散模型在图像生成方面表现出色，但存在推理速度慢和训练-推理差异的问题。现有梯度求解器缺乏信息传输效率的理论基础。
- Method: 从信息论角度分析扩散模型推理过程，提出基于条件熵优化的方法EVODiff，通过优化条件方差来最小化转换和重建误差。
- Result: 在CIFAR-10上，EVODiff将重建误差降低45.5%（FID从5.10提升到2.78）；在ImageNet-256上减少25%的NFE成本；同时改善文本到图像生成质量并减少伪影。
- Conclusion: 信息论视角为扩散模型推理提供了理论基础，EVODiff方法在生成质量和效率方面显著优于现有最先进的梯度求解器。


### [86] [EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward Subject-Driven Auto-Regressive Model](https://arxiv.org/abs/2509.26127)
*Ruixiao Dong,Zhendong Wang,Keli Liu,Li Li,Ying Chen,Kai Li,Daowen Li,Houqiang Li*

Main category: cs.CV

TL;DR: EchoGen是一个基于视觉自回归模型(VAR)的主题驱动生成框架，通过双路径注入策略实现高效、高质量的主题驱动图像生成，解决了现有方法在效率和零样本能力之间的权衡问题。

- Motivation: 当前主题驱动生成方法存在明显权衡：要么依赖计算昂贵的逐主题微调，牺牲效率和零样本能力；要么使用基于扩散模型的前馈架构，但存在推理速度慢的问题。VAR模型以其快速采样速度和强大生成质量成为理想但未被充分探索的基础。
- Method: EchoGen采用双路径注入策略：1）语义编码器提取主题的抽象身份，通过解耦交叉注意力注入以指导整体构图；2）内容编码器捕获精细视觉细节，通过多模态注意力机制集成以确保高保真纹理和结构保留。
- Result: 定量和定性结果证实了设计有效性，EchoGen在主题保真度和图像质量方面与最先进的基于扩散的方法相当，但采样延迟显著降低。
- Conclusion: EchoGen是首个基于VAR模型的前馈主题驱动框架，成功解决了主题驱动生成中效率与质量之间的权衡问题，为创意AI提供了高效且高质量的解决方案。


### [87] [EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting](https://arxiv.org/abs/2509.26157)
*Sachith Abeywickrama,Emadeldeen Eldele,Min Wu,Xiaoli Li,Chau Yuen*

Main category: cs.CV

TL;DR: 提出了EntroPE，一种基于信息熵的动态分块编码框架，通过条件熵检测时间序列中的自然过渡点来动态确定分块边界，保持时间连贯性。

- Motivation: 现有基于分块的Transformer方法采用时间无关的分块策略，任意起始位置和固定长度会破坏时间连贯性，分割自然过渡点，削弱短期依赖和表示学习。
- Method: 包含两个关键模块：基于熵的动态分块器(EDP)使用信息论标准定位自然时间变化点确定分块边界；自适应分块编码器(APE)使用池化和交叉注意力捕获块内依赖并生成固定大小的潜在表示。
- Result: 在长期预测基准测试中，EntroPE在准确性和效率方面均有提升。
- Conclusion: 基于熵的动态分块为时间序列建模提供了一个有前景的新范式。


### [88] [Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis](https://arxiv.org/abs/2509.26158)
*Kyeongryeol Go*

Main category: cs.CV

TL;DR: 提出了一种基于文本引导的边缘案例自动合成方法，通过微调的大语言模型将图像描述改写为多样化文本提示，指导文本到图像模型生成困难的视觉场景，在FishEye8K目标检测基准上表现出优越的鲁棒性。

- Motivation: 深度神经网络性能受训练数据质量影响很大，但手动筛选具有挑战性的边缘案例存在效率瓶颈，需要自动化解决方案来提升数据质量。
- Method: 使用经过偏好学习微调的大语言模型，将图像描述改写为多样化文本提示，然后利用文本到图像模型生成困难的视觉场景，实现自动化的边缘案例合成。
- Result: 在FishEye8K目标检测基准测试中，该方法超越了朴素数据增强和手动设计的提示方法，展现出更优越的鲁棒性表现。
- Conclusion: 建立了一个可扩展的框架，将数据筛选从手动工作转向自动化、有针对性的合成，为开发更可靠和持续改进的AI系统提供了有前景的方向。


### [89] [Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models](https://arxiv.org/abs/2509.26165)
*Yuansen Liu,Haiming Tang,Jinlong Peng,Jiangning Zhang,Xiaozhong Ji,Qingdong He,Donghao Luo,Zhenye Gan,Junwei Zhu,Yunhang Shen,Chaoyou Fu,Chengjie Wang,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: Human-MME是一个针对多模态大语言模型在人类中心场景理解方面的综合评估基准，包含多样化的场景、渐进式评估维度和高质量标注，旨在更全面地评估模型对人类场景的理解能力。

- Motivation: 当前多模态大语言模型在视觉理解任务上取得显著进展，但在人类中心场景理解方面的能力很少被探索，主要缺乏同时考虑人类导向的细粒度感知和高维因果推理能力的综合评估基准。
- Method: 构建Human-MME基准，包含4个主要视觉领域、15个二级领域和43个子领域，提供19,945个真实世界图像问答对，评估维度从人类导向的细粒度感知到高维推理，支持选择、简答、定位、排序和判断等多种问题类型。
- Result: 在17个最先进的多模态大语言模型上进行的广泛实验有效暴露了现有模型的局限性，为未来研究提供了指导。
- Conclusion: Human-MME基准为多模态大语言模型在人类中心图像理解方面的研究提供了更全面的评估框架，有助于推动该领域的发展。


### [90] [Beyond Overall Accuracy: Pose- and Occlusion-driven Fairness Analysis in Pedestrian Detection for Autonomous Driving](https://arxiv.org/abs/2509.26166)
*Mohammad Khoshkdahan,Arman Akbari,Arash Akbari,Xuan Zhang*

Main category: cs.CV

TL;DR: 本文首次对自动驾驶中的行人检测进行全面的姿态和遮挡感知公平性评估，发现检测模型对平行腿、直肘和侧向视角的行人存在偏见，且下半身关节遮挡比上半身和头部遮挡对检测率影响更大。

- Motivation: 虽然许多检测模型旨在降低漏检率并处理遮挡和远距离识别等挑战，但公平性仍然是一个未被充分探索但同样重要的问题。本文系统研究行人姿态变化（包括腿部状态、肘部状态和身体方向）以及单个关节遮挡如何影响检测性能。
- Method: 在EuroCity Persons Dense Pose数据集上评估五个行人专用检测器（F2DNet、MGAN、ALFNet、CSP和Cascade R-CNN）和三个通用模型（YOLOv12变体）。使用均等机会差异（EOD）指标在不同置信度阈值下量化公平性，并应用Z检验评估统计显著性和鲁棒性。
- Result: 研究发现检测模型对平行腿、直肘和侧向视角的行人存在偏见。下半身关节遮挡比上半身和头部遮挡对检测率有更负面的影响。Cascade R-CNN实现了最低的总体漏检率，并在所有属性中表现出最小的偏见。
- Conclusion: 这是自动驾驶中行人检测领域首次全面的姿态和遮挡感知公平性评估，揭示了现有检测模型在公平性方面的重要局限性，为开发更公平的行人检测系统提供了重要见解。


### [91] [AttriGen: Automated Multi-Attribute Annotation for Blood Cell Datasets](https://arxiv.org/abs/2509.26185)
*Walid Houmaidi,Youssef Sabiri,Fatima Zahra Iguenfer,Amine Abouaomar*

Main category: cs.CV

TL;DR: AttriGen是一个用于计算机视觉中自动化、细粒度多属性标注的新框架，特别针对细胞显微镜图像，结合CNN和Vision Transformer实现细胞类型分类和多属性分类，准确率达94.62%。

- Motivation: 在细胞显微镜领域，多属性分类相比传统的细胞类型分类仍未被充分研究，需要开发自动化方法来提高标注效率和模型可解释性。
- Method: 使用双模型架构：CNN用于细胞类型分类，Vision Transformer用于多属性分类，基于PBC数据集（8种细胞类型）和WBCAtt数据集（11种形态属性）。
- Result: 实现了94.62%的分类准确率，显著提高了模型可解释性，相比传统人工标注节省了大量时间和成本。
- Conclusion: AttriGen建立了一个可扩展到其他计算机视觉分类任务的新范式，有效自动化了多属性标签的扩展。


### [92] [TSalV360: A Method and Dataset for Text-driven Saliency Detection in 360-Degrees Videos](https://arxiv.org/abs/2509.26208)
*Ioannis Kontostathis,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出了TSV360数据集和TSalV360方法，用于360度视频中的文本驱动显著性检测。

- Motivation: 解决360度视频中基于文本描述的显著性检测问题，实现用户自定义的显著性目标检测。
- Method: 扩展并适配了SOTA视觉方法，开发了TSalV360方法，利用视觉语言模型进行数据表示，集成相似度估计模块和视口时空交叉注意力机制。
- Result: 在TSV360数据集上的定量和定性评估显示，TSalV360与SOTA视觉方法相比具有竞争力，能够有效执行定制化的文本驱动显著性检测。
- Conclusion: TSalV360方法在360度视频中成功实现了基于文本描述的显著性检测，为个性化视频分析提供了有效解决方案。


### [93] [Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation](https://arxiv.org/abs/2509.26219)
*Chenyang Jiang,Zhengcen Li,Hang Zhao,Qiben Shan,Shaocong Wu,Jingyong Su*

Main category: cs.CV

TL;DR: 提出GSDD方法，使用2D高斯函数的稀疏表示进行数据集蒸馏，相比传统密集像素方法更高效且可扩展。

- Motivation: 传统数据集蒸馏方法使用密集像素级表示，存在冗余且难以扩展，需要更高效的表示方法。
- Method: 使用少量高斯基元编码关键判别信息，采用CUDA-based splatting算子进行并行推理和训练，实现高效渲染。
- Result: 在CIFAR-10、CIFAR-100和ImageNet子集上达到最先进性能，同时保持高效的编码和解码成本。
- Conclusion: GSDD是一种简单有效、广泛适用且高度可扩展的数据集蒸馏方法，能显著提升性能同时降低计算和存储负担。


### [94] [An Experimental Study on Generating Plausible Textual Explanations for Video Summarization](https://arxiv.org/abs/2509.26225)
*Thomas Eleftheriadis,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 该研究通过集成大型多模态模型LLaVA-OneVision，为视频摘要结果生成文本解释，并提出基于语义重叠的评估方法来衡量解释的合理性。

- Motivation: 研究旨在为视频摘要结果生成可信的文本解释，并评估这些解释与人类推理和期望的一致性，这是可解释AI的重要特性。
- Method: 扩展现有视频摘要解释框架，集成LLaVA-OneVision模型生成文本描述，使用SBERT和SimCSE两种句子嵌入方法量化视觉解释与视频摘要文本描述之间的语义重叠。
- Result: 使用CA-SUM方法和SumMe、TVSum数据集进行实验，检验更忠实的解释是否也更合理，并确定生成合理文本解释的最佳方法。
- Conclusion: 该研究提出了评估视频摘要解释合理性的方法，并验证了不同文本解释生成方法的有效性。


### [95] [Generalized Fine-Grained Category Discovery with Multi-Granularity Conceptual Experts](https://arxiv.org/abs/2509.26227)
*Haiyang Zheng,Nan Pu,Wenjing Li,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出了MGCE框架，通过多粒度概念专家解决广义类别发现问题，无需预先知道未标记数据中的类别数量，在九个细粒度视觉识别基准测试中达到最先进性能。

- Motivation: 现有方法存在两个主要局限：未能充分利用视觉数据中的多粒度概念信息，以及假设未标记类别数量已知（这在现实场景中不切实际）。
- Method: MGCE包含两个模块：动态概念对比学习（DCCL）交替进行概念挖掘和双级表示学习；多粒度专家协作学习（MECL）引入不同粒度的专家并使用概念对齐矩阵进行跨专家协作。
- Result: 在九个细粒度视觉识别基准测试中达到最先进结果，特别是在新类准确率方面表现突出。即使不知道类别数量，MGCE也优于需要知道确切类别数量的参数化方法，平均提升3.6%。
- Conclusion: MGCE框架能够自适应挖掘视觉概念并整合多粒度知识，自动估计未标记数据中的类别数量，适用于实际的开放世界场景。


### [96] [IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance](https://arxiv.org/abs/2509.26231)
*Jiayi Guo,Chuanhao Yan,Xingqian Xu,Yulin Wang,Kai Wang,Gao Huang,Humphrey Shi*

Main category: cs.CV

TL;DR: 提出IMG框架，通过多模态大语言模型识别图像与提示词的不对齐问题，使用隐式对齐器操作扩散条件特征进行重新生成，无需额外数据或编辑操作即可提升多模态对齐精度。

- Motivation: 解决扩散模型生成的图像与输入提示词之间的多模态对齐问题，现有方法要么依赖有限的高质量偏好数据，要么通过编辑局部区域可能损害整体图像质量。
- Method: 使用多模态大语言模型识别不对齐区域，引入隐式对齐器操作扩散条件特征，制定可训练的迭代更新偏好目标，实现重新生成对齐。
- Result: 在SDXL、SDXL-DPO和FLUX上的定性和定量评估显示，IMG优于现有对齐方法，并能作为即插即用适配器增强基于微调的对齐方法。
- Conclusion: IMG提供了一种无需额外数据或编辑操作的有效多模态对齐框架，显著提升了扩散模型生成图像与提示词的对齐精度。


### [97] [Interpret, prune and distill Donut : towards lightweight VLMs for VQA on document](https://arxiv.org/abs/2509.26235)
*Adnan Ben Mansour,Ayoub Karine,David Naccache*

Main category: cs.CV

TL;DR: 提出了Donut-MINT，一种基于机制可解释性的网络剪枝方法，通过知识蒸馏训练紧凑的学生模型，显著减少推理时间和内存使用，同时在DocVQA基准上保持强性能。

- Motivation: 大型视觉语言模型如Donut在文档视觉问答中有效但计算成本高，不适合实时或资源受限应用，需要模型压缩方法。
- Method: 利用机制可解释性驱动学生架构设计，分析内部计算以识别关键子组件，决定哪些子组件应保留、近似、跳过或重新参数化，通过知识蒸馏训练紧凑模型。
- Result: 开发的Donut-MINT剪枝变体在DocVQA标准基准上保持强性能的同时，显著减少了推理时间和内存使用。
- Conclusion: 该方法将压缩重新定义为电路发现，弥合了可解释性研究与实用视觉语言模型部署之间的差距。


### [98] [Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA](https://arxiv.org/abs/2509.26251)
*Zhejia Cai,Yandan Yang,Xinyuan Chang,Shiyi Liang,Ronghan Chen,Feng Xiong,Mu Xu,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出了Farsighted-LAM框架，通过几何感知空间编码和多尺度时间建模改进潜在动作模型，解决了空间理解不足和时间感知有限的问题，并构建了SSM-VLA端到端视觉语言动作框架。

- Motivation: 现有潜在动作模型存在两个瓶颈：1）端到端训练的视觉编码器空间理解能力差；2）当输入帧间隔较大时模型脆弱，时间感知有限。这些因素阻碍了稳定清晰的动作建模。
- Method: 提出Farsighted-LAM框架，包含几何感知空间编码和多尺度时间建模，从连续帧中捕获结构先验和动态运动模式。进一步构建SSM-VLA端到端框架，集成结构化感知和视觉思维链模块来显式推理环境动态。
- Result: 在多个VLA任务的仿真和真实世界设置中验证SSM-VLA，实现了最先进的性能。
- Conclusion: 结合几何感知建模、时间一致性和显式推理的策略能有效增强具身智能的鲁棒性和泛化能力。


### [99] [PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection](https://arxiv.org/abs/2509.26272)
*Tuan Nguyen,Naseem Khan,Khang Tran,NhatHai Phan,Issa Khalil*

Main category: cs.CV

TL;DR: 提出了PRPO算法，通过强化学习在段落级别对齐LLM推理与图像内容，显著提升深度伪造检测准确性和推理质量。

- Motivation: 合成媒体的快速发展使深度伪造检测成为在线安全和信任的关键挑战，但现有方法受限于数据集稀缺，且多模态大语言模型在深度伪造检测中表现不佳，经常产生与视觉证据不符或幻觉的解释。
- Method: 引入推理标注的深度伪造检测数据集，并提出段落级相对策略优化(PRPO)强化学习算法，在段落级别对齐LLM推理与图像内容。
- Result: PRPO显著提高了检测准确率，达到4.55/5.0的最高推理分数，消融研究显示PRPO在测试条件下明显优于GRPO。
- Conclusion: 研究强调了将多模态推理基于视觉证据的重要性，以实现更可靠和可解释的深度伪造检测。


### [100] [Cat: Post-training quantization error reduction via cluster-based affine transformation](https://arxiv.org/abs/2509.26277)
*Ali Zoljodi,Radu Timofte,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: 提出了一种基于聚类的仿射变换(CAT)方法，用于改善低比特后训练量化(PTQ)的性能，通过使用聚类特定的参数来对齐量化输出与全精度输出，显著提升了低比特PTQ的准确性。

- Motivation: 传统的仿射变换在低比特PTQ中会恶化结果，因为对所有输出使用统一的仿射参数集无法有效减少量化模型与全精度模型之间的信息差异。
- Method: 提出Cluster-based Affine Transformation (CAT)框架，使用聚类特定的参数来精化低比特量化输出，仅需少量额外参数，无需微调模型或量化参数。
- Result: 在ImageNet-1K上的实验表明，该方法在多种架构和低比特设置下始终优于先前的PTQ方法，在W2A2 ResNet-18上达到53.18%的Top-1准确率，且作为插件可将现有PTQ基线提升超过3%。
- Conclusion: CAT是一种有效的误差减少框架，能够显著改善低比特后训练量化的性能，且易于集成到现有PTQ方法中。


### [101] [ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation](https://arxiv.org/abs/2509.26278)
*Edoardo Bianchi,Jacopo Staiano,Antonio Liotta*

Main category: cs.CV

TL;DR: ProfVLM是一个紧凑的视觉语言模型，通过生成式推理从多视角视频中联合预测技能水平并生成专家级反馈，在减少参数和训练时间的同时超越了现有方法。

- Motivation: 现有技能熟练度估计方法依赖黑盒视频分类器，忽略了多视角上下文且缺乏可解释性。
- Method: 使用AttentiveGatedProjector动态融合来自冻结TimeSformer骨干网络的多视角特征，并将其投影到为反馈生成调优的语言模型中。
- Result: 在EgoExo4D数据集上训练，ProfVLM超越最先进方法，同时使用参数减少20倍，训练时间减少60%，在多样活动中实现更高准确率。
- Conclusion: 生成式视觉语言建模为技能评估提供了强大的新方向，不仅能实现优越性能，还能输出与表现对齐的自然语言批评，提供透明推理。


### [102] [Point2RBox-v3: Self-Bootstrapping from Point Annotations via Integrated Pseudo-Label Refinement and Utilization](https://arxiv.org/abs/2509.26281)
*Teng Zhang,Ziqian Fan,Mingxin Liu,Xin Zhang,Xudong Lu,Wentong Li,Yue Zhou,Yi Yu,Xiang Li,Junchi Yan,Xue Yang*

Main category: cs.CV

TL;DR: Point2RBox-v3是一个基于点标注的弱监督旋转目标检测方法，通过渐进式标签分配和先验引导动态掩码损失，解决了现有方法中伪标签利用效率低和质量差的问题。

- Motivation: 现有的点监督方法存在伪标签利用效率低和质量差两个缺陷，需要开发更有效的弱监督旋转目标检测方法。
- Method: 1) 渐进式标签分配(PLA)：在不同训练阶段动态估计实例大小，实现标签分配方法的使用；2) 先验引导动态掩码损失(PGDM-Loss)：结合SAM模型和分水岭算法的优势，在稀疏和密集场景中都能表现良好。
- Result: 在多个数据集上取得优异性能：DOTA-v1.0(66.09%)、DOTA-v1.5(56.86%)、DOTA-v2.0(41.28%)、DIOR(46.40%)、STAR(19.60%)、RSAR(45.96%)，特别在目标尺寸变化大或稀疏场景中表现突出。
- Conclusion: Point2RBox-v3是首个使用动态伪标签进行标签分配的模型，创新性地结合了SAM模型和分水岭算法的优势，在稀疏和密集场景中都能达到优秀性能。


### [103] [FLOWER: A Flow-Matching Solver for Inverse Problems](https://arxiv.org/abs/2509.26287)
*Mehrsa Pourya,Bassam El Rawas,Michael Unser*

Main category: cs.CV

TL;DR: Flower是一种基于预训练流模型的逆问题求解器，通过三步迭代过程生成与观测测量一致的图像重建，在保持超参数一致的情况下实现最先进的重建质量。

- Motivation: 统一基于插拔式方法和生成式逆问题求解器的视角，开发一个能够近似贝叶斯后验采样的通用逆问题求解框架。
- Method: 三步迭代过程：(i)流一致目标估计，速度网络预测去噪目标；(ii)精炼步骤，将估计目标投影到前向算子定义的可行集；(iii)时间推进步骤，沿流轨迹重新投影精炼目标。
- Result: Flower在多种逆问题上实现了最先进的重建质量，同时使用几乎相同的超参数设置。
- Conclusion: Flower通过流模型统一了插拔式方法和生成式逆问题求解器的视角，提供了一种近似贝叶斯后验采样的有效方法，并在实践中展示了优异的性能和通用性。


### [104] [Continuous Space-Time Video Super-Resolution with 3D Fourier Fields](https://arxiv.org/abs/2509.26325)
*Alexander Becker,Julius Erbach,Dominik Narnhofer,Konrad Schindler*

Main category: cs.CV

TL;DR: 提出了一种基于3D视频傅里叶场(VFF)的连续时空视频超分辨率方法，通过神经编码器预测傅里叶系数，实现任意时空位置的采样和抗锯齿重建。

- Motivation: 传统方法将视频分解为空间和时间组件，依赖脆弱的显式帧扭曲进行运动补偿，存在局限性。需要一种更统一的表示方法。
- Method: 将视频编码为连续的3D视频傅里叶场，使用神经编码器预测傅里叶系数，支持任意时空位置采样和包含高斯点扩散函数的抗锯齿重建。
- Result: 在多个基准测试中达到新的最先进水平，在广泛的上采样因子范围内提供更清晰、时间更一致的重建，同时计算效率更高。
- Conclusion: 联合建模方法显著改善了空间和时间超分辨率性能，证明了连续傅里叶场表示在视频超分辨率任务中的有效性。


### [105] [SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2509.26330)
*Ren-Di Wu,Yu-Yen Lin,Huei-Fang Yang*

Main category: cs.CV

TL;DR: SQUARE是一个无需训练的双阶段零样本组合图像检索框架，利用多模态大语言模型增强检索性能，通过语义查询增强融合和高效批量重排来更好地捕捉用户意图。

- Motivation: 解决零样本组合图像检索中准确捕捉用户意图的挑战，避免任务特定训练和标注数据的需求。
- Method: 两阶段方法：1) 语义查询增强融合阶段，用MLLM生成目标图像描述来丰富CLIP查询嵌入；2) 高效批量重排阶段，将候选图像以网格形式呈现给MLLM进行联合视觉语义推理。
- Result: 在四个标准CIR基准测试中表现出色，即使使用轻量级预训练模型也能保持高性能。
- Conclusion: SQUARE框架简单有效，通过结合MLLM的语义理解能力显著提升了零样本组合图像检索的性能，具有良好的应用潜力。


### [106] [EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing](https://arxiv.org/abs/2509.26346)
*Keming Wu,Sicong Jiang,Max Ku,Ping Nie,Minghao Liu,Wenhu Chen*

Main category: cs.CV

TL;DR: 该论文提出了一个名为\mname的奖励模型，用于解决开源图像编辑模型缺乏可靠奖励模型的问题。该模型通过大规模人类偏好数据集训练，在多个基准测试中表现优异，并能用于筛选高质量训练数据。

- Motivation: 当前开源图像编辑模型落后于闭源模型，主要瓶颈是缺乏可靠的奖励模型来扩展高质量合成训练数据。
- Method: 构建了\mname奖励模型，使用包含20万对偏好数据的大规模人类偏好数据集进行训练，数据集由训练有素的专家按照严格协议标注。
- Result: \mname在GenAI-Bench、AURORA-Bench、ImagenHub和\benchname等基准测试中实现了最先进的人类相关性，优于多种VLM-as-judge模型。使用\mname筛选ShareGPT-4o-Image数据集后训练的Step1X-Edit模型性能显著提升。
- Conclusion: \mname作为奖励模型能够有效扩展高质量图像编辑训练数据，其强对齐性表明在基于强化学习的后训练和图像编辑模型的测试时扩展等高级应用中具有潜力。


### [107] [TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos](https://arxiv.org/abs/2509.26360)
*Xiangrui Liu,Minghao Qin,Yan Shu,Zhengyang Liang,Yang Tian,Chen Jason Zhang,Bo Zhao,Zheng Liu*

Main category: cs.CV

TL;DR: 提出了任务导向时序定位(ToTG)新问题，并开发了TimeScope框架，通过渐进式推理在长视频中定位关键时刻，在ToTG Bench基准测试中表现优于现有方法。

- Motivation: 长视频中识别关键时刻对下游理解和推理任务至关重要，但传统方法泛化能力有限且难以处理长视频。
- Method: TimeScope框架采用渐进式推理：首先在长视频中识别粗粒度时间范围，然后通过细粒度时刻划分进行精炼，并使用ToTG Pile数据集增强能力。
- Result: 大量实验表明TimeScope在各种设置下始终优于现有时序定位方法和流行的MLLMs。
- Conclusion: TimeScope能有效解决这一新的挑战性问题，在任务导向时序定位方面表现出色。


### [108] [Go with Your Gut: Scaling Confidence for Autoregressive Image Generation](https://arxiv.org/abs/2509.26376)
*Harold Haodong Chen,Xianfeng Wu,Wen-Jie Shu,Rongjin Guo,Disen Lan,Harry Yang,Ying-Cong Chen*

Main category: cs.CV

TL;DR: ScalingAR是首个专为基于下一个token预测的自回归图像生成设计的测试时缩放框架，无需早期解码或辅助奖励，通过token熵和两级缩放机制显著提升生成质量和效率。

- Motivation: 现有的测试时缩放方法依赖频繁部分解码和外部奖励模型，不适用于基于下一个token预测的自回归图像生成，因为中间解码结果不完整。
- Method: 提出ScalingAR框架，利用token熵作为视觉token生成的新信号，在配置级别融合内在和条件信号校准置信状态，在策略级别自适应终止低置信轨迹并动态调度指导强度。
- Result: 在通用和组合基准测试中，ScalingAR将基础模型在GenEval上提升12.5%，在TIIF-Bench上提升15.2%，视觉token消耗减少62.0%，在挑战性场景中性能下降缓解26.0%。
- Conclusion: ScalingAR成功填补了测试时缩放在自回归图像生成中的空白，通过新颖的熵信号和两级缩放机制实现了高效、鲁棒的图像生成。


### [109] [PANDA: Towards Generalist Video Anomaly Detection via Agentic AI Engineer](https://arxiv.org/abs/2509.26386)
*Zhiwei Yang,Chen Gao,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出PANDA，一种基于MLLM的智能AI工程师，用于通用视频异常检测，无需训练数据或人工干预即可处理任何场景和异常类型。

- Motivation: 传统视频异常检测方法依赖特定领域训练数据和人工调整，在新场景和未见异常类型上泛化能力有限且成本高昂。
- Method: 开发了四种关键能力：自适应场景感知策略规划、目标驱动启发式推理、工具增强自我反思、自我改进记忆链。包括自适应场景感知RAG机制、潜在异常引导启发式提示策略、渐进式反思机制和记忆链机制。
- Result: 在多种场景、开放集和复杂场景设置中实现了最先进的性能，无需训练和人工参与，验证了其可泛化和鲁棒的异常检测能力。
- Conclusion: PANDA展示了在无需训练和人工干预的情况下实现通用视频异常检测的可行性，为智能视频分析提供了新范式。


### [110] [MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation](https://arxiv.org/abs/2509.26391)
*Chenhui Zhu,Yilu Wu,Shuai Wang,Gangshan Wu,Limin Wang*

Main category: cs.CV

TL;DR: 提出了MotionRAG框架，通过检索增强方法从参考视频中提取运动先验，提升图像到视频生成的动态真实性。

- Motivation: 当前基于扩散模型的图像到视频生成在运动真实性方面仍面临挑战，难以准确建模复杂的物理约束、物体交互和领域特定动态。
- Method: 采用检索增强框架，包括：1）基于检索的流程提取高层次运动特征；2）上下文学习实现运动适应；3）注意力机制的运动注入适配器。
- Result: 实验表明该方法在多个领域和基础模型上均取得显著改进，推理时计算开销可忽略，且能通过更新检索数据库实现零样本泛化。
- Conclusion: 该研究通过有效检索和转移运动先验，增强了视频生成系统的核心能力，促进了真实运动动态的合成。


### [111] [Image-Difficulty-Aware Evaluation of Super-Resolution Models](https://arxiv.org/abs/2509.26398)
*Atakan Topaloglu,Ahmet Bilican,Cansu Korkmaz,A. Murat Tekalp*

Main category: cs.CV

TL;DR: 提出了一种感知图像难度的超分辨率模型评估方法，通过高频指数和旋转不变边缘指数来预测模型在困难图像上的表现差异，弥补传统平均评分方法的不足。

- Motivation: 传统基于平均评分的超分辨率模型评估方法无法反映模型在不同难度图像上的表现差异，特别是某些模型在困难图像上会产生伪影的问题无法被平均分数捕捉。
- Method: 提出了两种图像难度度量指标：高频指数和旋转不变边缘指数，用于预测测试图像中模型之间的视觉差异，并设计了一种能够反映这些视觉差异的客观评估方法。
- Result: 实验结果表明，所提出的图像难度度量和评估方法能够有效区分在平均性能接近但视觉结果不同的超分辨率模型。
- Conclusion: 提出的难度感知性能评估程序能够更好地区分超分辨率模型，特别是在那些传统平均评分方法无法反映视觉差异的困难图像上。


### [112] [PRISM: Progressive Rain removal with Integrated State-space Modeling](https://arxiv.org/abs/2509.26413)
*Pengze Xue,Shanwen Wang,Fei Zhou,Yan Cui,Xin Sun*

Main category: cs.CV

TL;DR: 提出PRISM框架，通过三阶段渐进式网络（CENet、SFNet、RNet）结合混合注意力UNet和混合域Mamba，解决图像去雨任务中细粒度恢复和全局一致性的挑战。

- Motivation: 当前单尺度模型在图像去雨任务中难以同时实现细粒度恢复和全局一致性，影响自动驾驶等关键视觉任务的清晰度。
- Method: 三阶段渐进框架：CENet进行粗提取，SFNet使用混合注意力UNet和混合域Mamba联合建模空间语义和小波域特征，RNet通过原始分辨率子网络恢复细粒度结构。
- Result: 在多个数据集上取得了与近期去雨方法竞争性的结果，能够学习高频雨特征同时保持结构细节和全局上下文。
- Conclusion: PRISM框架通过多尺度特征聚合和混合域建模，有效提升了图像去雨的质量，在保持结构细节和全局一致性方面表现优异。


### [113] [Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models](https://arxiv.org/abs/2509.26436)
*Donghoon Kim,Dongyoung Lee,Ik Joon Chang,Sung-Ho Bae*

Main category: cs.CV

TL;DR: 提出了QuaRTZ方法，通过残差截断和零抑制实现扩散模型的4位量化，在保持纹理细节的同时显著降低计算需求。

- Motivation: 扩散模型虽然能生成高质量图像，但计算需求高难以部署。现有的8位量化方法效果良好，但扩展到4位量化时，由于步长增大导致低幅值激活的舍入误差放大，丢失细粒度纹理细节。
- Method: QuaRTZ方法结合8位最小-最大量化处理异常值，通过前导零抑制压缩到4位以保留最低有效位(LSB)，从而保持纹理细节。该方法平衡了异常值保留和LSB精度。
- Result: 在FLUX.1-schnell上，4位QuaRTZ实现了6.98的FID分数，优于需要辅助FP16分支的SVDQuant方法。
- Conclusion: QuaRTZ通过理论推导和实证评估证明了其在不同激活分布上的通用性，为扩散模型的4位量化提供了有效解决方案。


### [114] [Multi-View Camera System for Variant-Aware Autonomous Vehicle Inspection and Defect Detection](https://arxiv.org/abs/2509.26454)
*Yash Kulkarni,Raman Jha,Renu Kachhoria*

Main category: cs.CV

TL;DR: 提出一个端到端的多视角感知系统AVI，通过11个同步摄像头进行360度车辆检测，结合深度学习检测器和语义规则引擎实现实时变体感知质量控制。

- Motivation: 现代生产线确保每辆车符合正确的变体规格且无可见缺陷是一个日益复杂的挑战，需要自动化解决方案来替代人工检查。
- Method: 使用11个同步摄像头捕获车辆360度视图，通过专用模块处理：YOLOv8进行零件检测、EfficientNet进行ICE/EV分类、Gemini-1.5 Flash进行吉祥物OCR、YOLOv8-Seg进行划痕和凹陷分割。视图感知融合层标准化证据，VIN条件规则引擎将检测特征与预期清单进行比较。
- Result: 在混合数据集上，AVI实现93%的验证准确率、86%的缺陷检测召回率，并维持每分钟3.3辆车的处理速度，大幅超越单视图或无分割基线方法。
- Conclusion: 这是首个在工业可部署汽车环境中统一多摄像头特征验证与缺陷检测的公开报告系统，展示了在复杂生产环境中的实际应用价值。


### [115] [Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting](https://arxiv.org/abs/2509.26455)
*Hanzhou Liu,Jia Huang,Mi Lu,Srikanth Saripalli,Peng Jiang*

Main category: cs.CV

TL;DR: Stylos是一个单次前向传播的3D高斯框架，用于3D风格迁移，无需姿态预计算或逐场景优化，能够从单张图像或多视图集合生成风格化的3D高斯场景。

- Motivation: 现有的3D风格迁移方法通常需要预计算相机姿态或进行逐场景优化，限制了其可扩展性和实用性。Stylos旨在实现无需姿态信息的零样本3D风格迁移，提升方法的通用性和效率。
- Method: 采用Transformer架构，包含两个路径：几何预测路径使用自注意力保持几何保真度，风格注入路径通过全局交叉注意力确保视图间视觉一致性。此外，引入基于体素的3D风格损失来对齐场景特征与风格统计量。
- Result: 在多个数据集上的实验表明，Stylos能够实现高质量的零样本风格化，具有良好的泛化能力，能够处理未见过的类别、场景和风格。
- Conclusion: Stylos通过全局风格-内容耦合、提出的3D风格损失以及从单视图到大规模多视图设置的可扩展性，展示了其在3D风格迁移任务中的有效性。


### [116] [Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification](https://arxiv.org/abs/2509.26457)
*Artur Barros,Carlos Caetano,João Macedo,Jefersson A. dos Santos,Sandra Avila*

Main category: cs.CV

TL;DR: 提出ASGRA框架，通过场景图和图注意力网络进行室内场景分类和敏感内容分析，无需直接访问敏感图像，同时提供可解释性和隐私保护。

- Motivation: 室内场景分类在计算机视觉中至关重要，特别是在敏感内容分析如儿童性虐待图像分类方面。传统方法面临对象间复杂关系和空间布局的挑战。
- Method: 将图像转换为场景图，然后使用图注意力网络进行推理，直接建模场景组件间的交互关系。
- Result: 在Places8数据集上达到81.27%的平衡准确率，优于基于图像的方法；在实际CSAI评估中获得74.27%的平衡准确率。
- Conclusion: 结构化场景表示是室内场景分类和CSAI分类的稳健范式，具有可解释性和隐私保护优势。


### [117] [CBAM Integrated Attention Driven Model For Betel Leaf Diseases Classification With Explainable AI](https://arxiv.org/abs/2509.26484)
*Sumaiya Tabassum,Md. Faysal Ahamed*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的CBAM-CNN模型，用于检测槟榔叶疾病，该模型仅含213万个参数，通过集成卷积块注意力模块来增强特征提取能力，在包含10,185张图像的数据集上取得了95.58%的准确率。

- Motivation: 槟榔叶作为重要经济作物易受疾病威胁，传统方法难以及时识别疾病，人工智能技术可帮助提高产量和疾病预测能力。
- Method: 采用轻量级CBAM-CNN模型，集成卷积块注意力模块自适应关注重要空间和通道信息，使用包含健康叶片、叶腐病和叶斑病三个类别的10,185张图像数据集进行训练和验证。
- Result: 模型在测试集上达到97%精确率、94%召回率、95% F1分数和95.58%准确率，优于传统预训练CNN模型，并通过Grad-CAM技术可视化模型关注区域。
- Conclusion: 提出的CBAM-CNN模型在槟榔叶疾病分类中表现出色，为植物疾病检测提供了有效的轻量级解决方案，具有实际应用价值。


### [118] [Contrastive Diffusion Guidance for Spatial Inverse Problems](https://arxiv.org/abs/2509.26489)
*Sattwik Basu,Chaitanya Amballa,Zhongweiyang Xu,Jorge Vančo Sampedro,Srihari Nelakuditi,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出了一种从用户移动轨迹重建空间布局的方法，使用基于扩散的后验采样器生成与测量一致的布局，通过对比学习在嵌入空间中重新定义似然分数来解决前向算子不可微的问题。

- Motivation: 从用户移动轨迹重建空间布局是一个不适定问题，因为多个布局可以解释相同的轨迹。现有方法面临前向算子（路径规划过程）不可逆、不可微的挑战，导致优化不稳定。
- Method: 采用扩散模型生成布局，通过对比学习训练嵌入空间，使兼容的布局和轨迹靠近，不匹配的远离。在嵌入空间中定义替代的似然分数来指导去噪过程。
- Result: CoGuide模型在实验中比可微分规划器基准和引导扩散方法产生更一致的布局，并且更稳健。
- Conclusion: 通过在嵌入空间中重新定义似然分数，可以有效解决路径规划过程不可微的问题，实现从移动轨迹重建空间布局的稳定优化。


### [119] [Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation](https://arxiv.org/abs/2509.26497)
*Miao Rang,Zhenni Bi,Hang Zhou,Hanting Chen,An Xiao,Tianyu Guo,Kai Han,Xinghao Chen,Yunhe Wang*

Main category: cs.CV

TL;DR: 提出了一种系统化的后训练流程，通过课程式监督微调和离线策略知识蒸馏，有效提升小模型在边缘设备上的性能表现。

- Motivation: 大语言模型在资源受限的边缘环境中部署困难，需要开发高性能的小模型，但仅预训练的小模型难以满足复杂任务需求。
- Method: 采用课程式监督微调(SFT)和离线策略知识蒸馏的后训练流程。
- Result: 生成的指令调优模型在十亿参数模型中达到最先进性能，在严格硬件约束下表现出强泛化能力，并在多种任务中保持竞争力。
- Conclusion: 为在Ascend边缘设备上开发高性能语言模型提供了实用高效的解决方案。


### [120] [DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance](https://arxiv.org/abs/2509.26498)
*Jijun Xiang,Longliang Liu,Xuan Zhu,Xianqi Wang,Min Lin,Xin Yang*

Main category: cs.CV

TL;DR: DEPTHOR++是一个针对轻量级dToF传感器的深度补全框架，通过模拟训练数据、异常检测机制和深度补全网络，显著提升了在噪声dToF输入下的深度增强性能。

- Motivation: 现有深度增强方法假设理想的dToF输入和完美的dToF-RGB对齐，忽略了校准误差和异常，限制了实际应用。需要解决真实世界轻量级dToF传感器的噪声特性问题。
- Method: 1) 基于合成数据集的模拟方法生成真实训练样本；2) 无学习参数异常检测机制识别错误dToF测量；3) 针对噪声dToF输入的深度补全网络，集成RGB图像和单目深度估计先验。
- Result: 在多个数据集上实现SOTA性能：ZJU-L5数据集RMSE和Rel分别提升22%和11%；Mirror3D-NYU数据集镜面区域提升37%；Hammer数据集超越L515测量22%。
- Conclusion: 该方法显著提升了深度补全在真实噪声环境下的鲁棒性，使低成本传感器能够超越高端设备性能，具有广泛的实际应用价值。


### [121] [Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents](https://arxiv.org/abs/2509.26539)
*Zhen Yang,Zi-Yi Dou,Di Feng,Forrest Huang,Anh Nguyen,Keen You,Omar Attia,Yuhao Yang,Michael Feng,Haotian Zhang,Ram Ramrakhya,Chao Jia,Jeffrey Nichols,Alexander Toshev,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: Ferret-UI Lite是一个3B参数的紧凑型端到端GUI代理，能够在移动、网页和桌面平台上运行，在GUI定位和导航任务中表现出色。

- Motivation: 开发能够有效与图形用户界面交互的自主代理仍然是一个具有挑战性的开放问题，特别是对于小型设备端模型。
- Method: 通过策划来自真实和合成来源的多样化GUI数据混合，通过思维链推理和视觉工具使用增强推理时性能，以及使用设计奖励的强化学习来构建模型。
- Result: 在GUI定位任务中，在ScreenSpot-V2、ScreenSpot-Pro和OSWorld-G基准测试中分别达到91.6%、53.3%和61.2%的分数；在GUI导航任务中，在AndroidWorld和OSWorld上分别达到28.0%和19.8%的成功率。
- Conclusion: Ferret-UI Lite在小型GUI代理中表现出竞争力，并分享了开发紧凑设备端GUI代理的方法和经验教训。


### [122] [Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation](https://arxiv.org/abs/2509.26555)
*Agneet Chatterjee,Rahim Entezari,Maksym Zhuravinskyi,Maksim Lapin,Reshinth Adithyan,Amit Raj,Chitta Baral,Yezhou Yang,Varun Jampani*

Main category: cs.CV

TL;DR: Stable Cinemetrics是一个结构化评估框架，将电影制作控制形式化为四个解耦的分层分类法：场景设置、事件、灯光和摄像机，包含76个细粒度控制节点，用于专业视频生成的评估。

- Motivation: 现有视频生成模型和基准测试未能捕捉专业视频生成的复杂性和需求，需要引入基于行业实践的结构化评估框架。
- Method: 构建包含76个细粒度控制节点的分层分类法，开发自动化的提示分类和问题生成流程，进行大规模人类研究（10+模型、20K视频、80+电影专业人士标注），并训练自动评估器。
- Result: 分析显示当前最强模型在事件和摄像机相关控制方面存在显著差距，训练的自动评估器在专家标注对齐方面优于现有零样本基线。
- Conclusion: SCINE首次将专业视频生成置于视频生成模型领域，通过以电影控制为中心的分类法和结构化评估流程指导未来研究。


### [123] [Autoproof: Automated Segmentation Proofreading for Connectomics](https://arxiv.org/abs/2509.26585)
*Gary B Huang,William M Katz,Stuart Berg,Louis Scheffer*

Main category: cs.CV

TL;DR: 利用机器学习模型自动化电子显微镜连接组学中的校对工作，显著降低人工成本并提高效率

- Motivation: 电子显微镜连接组学中的人工校对成本是当前瓶颈，阻碍了大规模连接组重建和比较连接组学的发展
- Method: 利用已有的人工标注数据训练机器学习模型，自动化或优化校对工作流程，包括引导校对和自动合并分割片段
- Result: 在果蝇中枢神经系统重建中，实现引导校对工作80%成本削减，自动合并20万个片段（相当于4年人工工作量），连接完成率提高1.3%
- Conclusion: 机器学习方法能显著降低连接组学的人工校对成本，为大规模连接组重建和比较连接组学提供了可行方案


### [124] [DiffCamera: Arbitrary Refocusing on Images](https://arxiv.org/abs/2509.26599)
*Yiyang Wang,Xi Chen,Xiaogang Xu,Yu Liu,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DiffCamera是一个基于扩散变换器的模型，能够根据任意新的焦点和模糊级别对已创建图像进行灵活重新对焦，解决了传统景深效果固定且难以修改的问题。

- Motivation: 传统摄影中的景深效果一旦创建就难以修改，当主体失焦时会产生问题。需要一种能够灵活调整图像对焦位置和模糊程度的方法。
- Method: 设计了基于扩散变换器的重新对焦学习框架，使用模拟数据生成大规模图像对，并提出了堆叠约束训练方法，通过物理原理增强模型训练。
- Result: 实验表明DiffCamera能够在各种场景下稳定进行重新对焦，为摄影和生成AI应用提供了前所未有的景深调整控制能力。
- Conclusion: DiffCamera通过创新的扩散变换器框架和堆叠约束训练，成功实现了对已创建图像的灵活重新对焦，解决了传统景深效果固定的限制。


### [125] [Video Object Segmentation-Aware Audio Generation](https://arxiv.org/abs/2509.26604)
*Ilpo Viertola,Vladimir Iashin,Esa Rahtu*

Main category: cs.CV

TL;DR: 提出SAGANet模型，通过视频对象分割图实现可控音频生成，解决了现有模型缺乏精确用户控制的问题

- Motivation: 现有多模态音频生成模型缺乏精确的用户控制，无法优先处理特定对象或避免不必要的背景声音，限制了在专业拟音工作流程中的应用
- Method: 引入视频对象分割感知音频生成新任务，提出SAGANet多模态生成模型，利用视觉分割掩码、视频和文本线索进行可控音频合成
- Result: 相比现有最先进方法有显著改进，为可控、高保真拟音合成设立了新标准
- Conclusion: SAGANet通过对象级分割实现了细粒度的视觉局部音频控制，推动了分割感知拟音研究的发展


### [126] [Hy-Facial: Hybrid Feature Extraction by Dimensionality Reduction Methods for Enhanced Facial Expression Classification](https://arxiv.org/abs/2509.26614)
*Xinjin Li,Yu Ma,Kaisen Ye,Jinghan Cao,Minghao Zhou,Yeyang Zhou*

Main category: cs.CV

TL;DR: Hy-Facial是一个混合特征提取框架，结合深度学习和传统图像处理技术，通过VGG19、SIFT和ORB提取特征，使用K-means聚类和UMAP降维，在面部表情识别数据集上达到83.3%的分类准确率。

- Motivation: 面部表情分类因面部图像数据的高维度和复杂性而具有挑战性，需要开发有效的特征提取和降维方法来提高分类性能。
- Method: 融合VGG19深度特征与SIFT、ORB手工特征，使用K-means聚类和UMAP降维技术，构建混合特征提取框架。
- Result: 在面部表情识别数据集上获得83.3%的分类准确率，UMAP被证明是最有效的降维方法。
- Conclusion: 降维不仅是预处理步骤，更是提升特征质量和整体分类性能的关键组成部分。


### [127] [DA$^2$: Depth Anything in Any Direction](https://arxiv.org/abs/2509.26618)
*Haodong Li,Wangguangdong Zheng,Jing He,Yuhao Liu,Xin Lin,Xin Yang,Ying-Cong Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: 提出了DA²模型，这是一个准确、零样本泛化能力强且完全端到端的全景深度估计器，通过数据生成引擎和SphereViT架构解决了全景数据稀缺和球面失真问题。

- Motivation: 解决全景深度估计中因数据稀缺导致的零样本泛化能力差，以及球面失真带来的效率低下问题。
- Method: 1. 开发数据生成引擎从透视图像生成高质量全景深度数据，创建了约607K全景RGB-深度对；2. 提出SphereViT架构，利用球面坐标强制全景图像特征的球面几何一致性。
- Result: 在多个数据集上实现最先进性能，AbsRel指标比最强的零样本基线平均提升38%，甚至优于先前的域内方法，且作为端到端解决方案具有更高效率。
- Conclusion: DA²通过大规模数据生成和球面几何一致性设计，显著提升了全景深度估计的零样本泛化能力和效率，超越了现有方法。


### [128] [HART: Human Aligned Reconstruction Transformer](https://arxiv.org/abs/2509.26621)
*Xiyi Chen,Shaofei Wang,Marko Mihajlovic,Taewon Kang,Sergey Prokudin,Ming Lin*

Main category: cs.CV

TL;DR: HART是一个用于稀疏视角人体重建的统一框架，能够从少量未标定RGB图像中重建出完整的人体网格、对齐的SMPL-X身体网格和高斯溅射表示。

- Motivation: 现有方法要么优化参数化模板（忽略宽松衣物和人物交互），要么在简化相机假设下训练隐函数（限制真实场景应用），因此需要一种更鲁棒的方法。
- Method: 预测逐像素3D点图、法线和身体对应关系，采用遮挡感知的泊松重建恢复完整几何，并与SMPL-X身体模型对齐，最后用人体对齐网格初始化高斯溅射。
- Result: 在多个数据集上取得SOTA：衣着网格重建的Chamfer距离提升18-23%，SMPL-X估计的PA-V2V降低6-27%，新视角合成的LPIPS降低15-27%。
- Conclusion: 前馈transformer可以作为真实场景中鲁棒人体重建的可扩展模型。


### [129] [Learning Generalizable Shape Completion with SIM(3) Equivariance](https://arxiv.org/abs/2509.26631)
*Yuqing Wang,Zhaiyu Chen,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了第一个SIM(3)-等变形状补全网络，通过架构设计使模型对姿态和尺度变化保持等变性，从而提升在真实未对齐数据上的泛化能力。

- Motivation: 现有3D形状补全方法通常假设扫描数据已预对齐到规范坐标系，这导致模型可能利用姿态和尺度线索而非内在几何进行记忆。当真实数据缺乏这种对齐时，性能会急剧下降。
- Method: 设计SIM(3)-等变形状补全网络，包含模块化层：特征规范化、相似性不变几何推理、原始坐标系恢复。采用去偏评估协议消除隐藏线索。
- Result: 在PCN基准测试中优于等变和增强基线方法；在KITTI和OmniObject3D真实数据集上分别降低最小匹配距离17%和Chamfer距离14%；在更严格的评估协议下仍优于竞争对手在偏置设置下的表现。
- Conclusion: 完整的SIM(3)等变性是实现真正可泛化形状补全的有效途径。


### [130] [Benchmarking Egocentric Visual-Inertial SLAM at City Scale](https://arxiv.org/abs/2509.26639)
*Anusha Krishnan,Shaohui Liu,Paul-Edouard Sarlin,Oscar Gentilhomme,David Caruso,Maurizio Monge,Richard Newcombe,Jakob Engel,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出了一个新的视觉-惯性SLAM数据集和基准测试，专门针对可穿戴设备的自我中心数据，提供厘米级精度的地面真实位姿，用于评估在挑战性场景下的SLAM系统性能。

- Motivation: 现有SLAM基准测试无法充分反映可穿戴设备面临的挑战，如多样化的运动模式、动态视觉内容、长时间会话和传感器校准变化等问题，且缺乏足够准确的地面真实位姿。
- Method: 使用眼镜式设备记录城市中心的多模态数据，利用测量工具获取厘米级精度的控制点作为间接位姿标注，支持极端场景（如夜间行走、车辆行驶）的评估。
- Result: 现有最先进的学术SLAM系统无法应对这些挑战，识别了导致性能下降的关键组件，并设计了不同难度级别的测试轨道以便深入分析。
- Conclusion: 该数据集填补了可穿戴设备SLAM评估的空白，为开发更鲁棒的SLAM系统提供了重要基准，并揭示了当前方法的局限性。


### [131] [Query-Kontext: An Unified Multimodal Model for Image Generation and Editing](https://arxiv.org/abs/2509.26641)
*Yuxin Song,Wenkai Dong,Shizun Wang,Qi Zhang,Song Xue,Tao Yuan,Hu Yang,Haocheng Feng,Hang Zhou,Xinyan Xiao,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出Query-Kontext方法，通过多模态"kontext"桥接视觉语言模型和扩散模型，将多模态生成推理能力委托给VLM，扩散模型专注于高质量视觉合成。

- Motivation: 当前统一多模态模型中，多模态生成推理能力与高保真合成内在纠缠，需要解耦这两个关键能力。
- Method: 三阶段渐进训练策略：1) 通过多模态kontext token连接VLM和轻量扩散头；2) 扩展到大型预训练扩散模型；3) 引入低级图像编码器进行指令调优。
- Result: 实验表明该方法与强统一基线相当，在某些情况下甚至优于任务特定的最先进方法。
- Conclusion: Query-Kontext成功解耦了多模态生成推理和视觉合成，为统一多模态模型提供了有效的设计范式。


### [132] [Stitch: Training-Free Position Control in Multimodal Diffusion Transformers](https://arxiv.org/abs/2509.26644)
*Jessica Bader,Mateusz Pach,Maria A. Bravo,Serge Belongie,Zeynep Akata*

Main category: cs.CV

TL;DR: Stitch是一种无需训练的方法，通过自动生成的边界框将外部位置控制集成到多模态扩散变换器中，解决了文本到图像生成中空间关系准确性的挑战。

- Motivation: 现有的文本到图像生成模型在准确捕捉空间关系（如"上方"或"右侧"）方面存在持续挑战，早期方法使用外部位置控制但与现代模型不兼容。
- Method: 通过自动生成边界框，在指定边界框内生成单个对象，然后无缝拼接在一起，利用目标注意力头在生成过程中隔离和裁剪单个对象。
- Result: 在PosEval基准测试中，Stitch显著提升了基础模型的性能，将FLUX在GenEval位置任务上的表现提高了218%，在PosEval上提高了206%，并在Qwen-Image上实现了最先进的结果。
- Conclusion: Stitch成功地将位置控制集成到领先模型中，无需训练即可显著提高空间关系生成的准确性，同时保持视觉吸引力。


### [133] [TTT3R: 3D Reconstruction as Test-Time Training](https://arxiv.org/abs/2509.26645)
*Xingyu Chen,Yue Chen,Yuliang Xiu,Andreas Geiger,Anpei Chen*

Main category: cs.CV

TL;DR: TTT3R是一种无需训练的3D重建方法，通过测试时训练视角改进循环神经网络，利用内存状态与观测的对齐置信度来动态调整学习率，显著提升了长度泛化能力。

- Motivation: 现代循环神经网络在3D重建中表现出色，但在超出训练上下文长度时性能显著下降，存在长度泛化有限的问题。
- Method: 从测试时训练视角重新审视3D重建基础模型，将其设计视为在线学习问题，利用内存状态与观测的对齐置信度推导闭式学习率来平衡历史信息保留与新观测适应。
- Result: TTT3R显著改善了长度泛化，在全局姿态估计上比基线提升2倍，以20 FPS的速度仅需6GB GPU内存处理数千张图像。
- Conclusion: TTT3R提供了一种无需训练的有效干预方法，通过测试时训练视角显著提升了3D重建模型的长度泛化能力。
## cs.LG

### [134] [Hyperbolic Optimization](https://arxiv.org/abs/2509.25206)
*Yanke Wang,Kyriakos Flouris*

Main category: cs.LG

TL;DR: 本文提出了在双曲流形上的优化方法，扩展了双曲随机梯度下降为双曲Adam优化器，并在扩散模型中应用，实现了更快的收敛速度而不损失生成质量。

- Motivation: 基于黎曼优化原理，探索双曲流形上的优化方法，特别是在庞加莱球上学习时，这些方法能促进庞加莱嵌入的学习，在训练早期阶段当参数远离最优值时加速收敛。
- Method: 扩展双曲随机梯度下降为双曲Adam优化器，结合双曲时间离散化的朗之万动力学，在扩散模型中应用这些双曲优化方法。
- Result: 在特定数据集上，使用双曲优化方法的扩散模型实现了更快的收敛速度，同时保持了生成质量。
- Conclusion: 双曲优化方法在双曲流形上有效，能加速训练收敛，特别是在参数远离最优值的早期阶段，且可推广到欧几里得和其他非欧几里得设置。


### [135] [Six Sigma For Neural Networks: Taguchi-based optimization](https://arxiv.org/abs/2509.25213)
*Sai Varun Kodathala*

Main category: cs.LG

TL;DR: 本文应用田口实验设计方法优化CNN超参数，用于职业拳击动作识别。通过L12(211)正交阵列评估8个超参数，开发了5种多目标优化方法，其中方法3（结合加权精度和对数变换损失函数）表现最佳，训练精度达98.84%，验证精度86.25%。

- Motivation: CNN超参数优化过程具有挑战性且计算成本高，通常需要大量试错或穷举网格搜索。本研究旨在将传统用于质量工程的统计优化技术——田口实验设计方法应用于CNN超参数优化。
- Method: 使用L12(211)正交阵列系统评估8个超参数（图像尺寸、颜色模式、激活函数、学习率、重缩放、洗牌、垂直翻转、水平翻转）。开发了5种多目标优化方法，采用信噪比分析同时优化训练精度、验证精度、训练损失和验证损失。引入了新颖的对数缩放技术来统一冲突指标。
- Result: 方法3（结合加权精度和对数变换损失函数）实现了最佳性能：训练精度98.84%，验证精度86.25%，同时保持最小损失值。田口分析显示学习率是最具影响力的参数，其次是图像尺寸和激活函数。
- Conclusion: 田口实验设计方法能有效优化CNN超参数，为超参数优先级提供了明确指导。学习率是影响性能的最关键参数，该方法为CNN优化提供了系统化的解决方案。


### [136] [InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions](https://arxiv.org/abs/2509.25270)
*Liangjian Wen,Qun Dai,Jianzhuang Liu,Jiangtao Zheng,Yong Dai,Dongkai Wang,Zhao Kang,Jun Wang,Zenglin Xu,Jiang Duan*

Main category: cs.LG

TL;DR: 提出InfMasking方法，通过无限掩码策略增强多模态表示中的协同信息，在七个基准测试中达到最先进性能

- Motivation: 现有方法难以有效捕捉多模态间的协同信息，而协同信息是多模态表示的核心价值。需要开发新方法来增强模态间的协同交互
- Method: InfMasking使用无限掩码策略，在融合过程中随机遮蔽大部分模态特征，仅保留部分信息来创建具有不同协同模式的表示。通过互信息最大化对齐未掩码和掩码的融合表示
- Result: 在控制实验中证明InfMasking有效增强模态间协同信息。在大规模真实数据集评估中，在七个基准测试中达到最先进性能
- Conclusion: InfMasking通过无限掩码策略成功增强了多模态表示中的协同信息，为多模态学习提供了有效的协同信息提取方法


### [137] [Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction](https://arxiv.org/abs/2509.25692)
*Tingyu Shi,Fan Lyu,Shaoliang Peng*

Main category: cs.LG

TL;DR: CPATTA提出了一种基于保形预测的主动测试时适应方法，通过保形评分和在线权重更新算法，在领域偏移下提高模型鲁棒性，相比现有方法准确率提升约5%。

- Motivation: 现有主动测试时适应方法使用启发式不确定性度量，数据选择效率低，浪费人工标注预算。需要引入有理论保证的不确定性度量来提高效率。
- Method: 使用平滑保形评分和top-K确定性度量，基于伪覆盖率的在线权重更新算法，领域偏移检测器调整人工监督，以及平衡人工标注和模型标注数据的分阶段更新方案。
- Result: 在广泛实验中，CPATTA始终优于最先进的ATTA方法，准确率提升约5%。
- Conclusion: CPATTA成功将保形预测引入主动测试时适应，提供了有覆盖保证的不确定性度量，显著提高了数据选择效率和模型性能。


### [138] [Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation](https://arxiv.org/abs/2509.25713)
*Hyunsoo Song,Minjung Gim,Jaewoong Choi*

Main category: cs.LG

TL;DR: 提出了UOT-RFM方法，通过无平衡最优运输和重加权策略解决长尾分布下的生成建模问题，无需类别标签信息即可改善少数类生成质量。

- Motivation: 标准流匹配在长尾分布中存在多数类偏差，导致少数类生成质量低且无法匹配真实类别比例。
- Method: 使用小批量无平衡最优运输构建条件向量场，通过基于密度比的无标签多数分数进行逆重加权。
- Result: 在长尾基准测试中优于现有流匹配基线，在平衡数据集上保持竞争力。
- Conclusion: UOT-RFM通过几何结构量化多数程度，无需类别标签即可有效改善长尾分布下的生成质量。


### [139] [From MNIST to ImageNet: Understanding the Scalability Boundaries of Differentiable Logic Gate Networks](https://arxiv.org/abs/2509.25933)
*Sven Brändle,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文研究了可微分逻辑门网络（DLGNs）在大规模多类数据集上的表现，探讨了其表达能力、可扩展性和输出策略，重点关注温度调谐对性能的影响。

- Motivation: DLGNs作为一种快速节能的神经网络替代方案，目前主要在小规模数据集（最多10个类别）上测试，需要研究其在大规模多类数据集上的表现和可扩展性。
- Method: 使用合成和真实世界数据集，研究DLGNs的表达能力、可扩展性和不同输出策略，特别关注温度调谐和Group-Sum层在大规模分类（最多2000个类别）中的应用。
- Result: 提供了关于温度调谐重要性的关键见解，评估了Group-Sum层在不同条件下的表现，并展示了如何将其应用于大规模分类任务。
- Conclusion: DLGNs在大规模多类数据集上具有潜力，温度调谐对输出层性能至关重要，Group-Sum层在特定条件下表现良好，可扩展到2000个类别的大规模分类。


### [140] [Scaling Up Temporal Domain Generalization via Temporal Experts Averaging](https://arxiv.org/abs/2509.26045)
*Aoming Liu,Kevin Miller,Venkatesh Saligrama,Kate Saenko,Boqing Gong,Ser-Nam Lim,Bryan A. Plummer*

Main category: cs.LG

TL;DR: 提出了Temporal Experts Averaging (TEA)框架，通过权重平均更新整个模型来解决时序域泛化问题，相比仅预测分类器层的方法，在保持计算效率的同时显著提升泛化能力。

- Motivation: 现有时序域泛化方法通常只预测分类器层权重，无法调整其他模型组件，限制了泛化能力。而预测完整模型权重又计算成本过高。
- Method: TEA框架包含两个步骤：1）在约束权重变化下对基础模型进行时序域微调，创建功能多样但参数相似的专家模型；2）在主成分子空间中建模时序权重轨迹，通过自适应平均系数优化偏差-方差权衡。
- Result: 在7个TDG基准测试、5个模型和2种TDG设置下的广泛实验表明，TEA比现有TDG方法性能提升高达69%，同时计算效率提升高达60倍。
- Conclusion: TEA通过权重平均有效解决了时序域泛化问题，在保持计算效率的同时显著提升了模型对未来域的泛化能力。


### [141] [Neighbor-aware informal settlement mapping with graph convolutional networks](https://arxiv.org/abs/2509.26171)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Christovam Barcellos,Nadine Dessay*

Main category: cs.LG

TL;DR: 提出基于图卷积网络的框架，通过整合邻域地理上下文信息来改进非正式住区分类，在里约热内卢案例中显著优于传统方法。

- Motivation: 现有方法将空间单元独立处理，忽略了城市结构的关联性，无法充分利用地理上下文信息进行非正式住区识别。
- Method: 构建图结构将每个空间单元与其相邻单元连接，使用轻量级图卷积网络对中心单元是否属于非正式住区进行分类。
- Result: 在里约热内卢五个不同区域的实验中，该方法显著优于基线方法，Kappa系数提高了17个百分点，且优于简单的邻域特征拼接。
- Conclusion: 图结构建模能有效编码空间结构，提升城市场景理解能力，为非正式住区制图提供了更鲁棒和可推广的解决方案。


### [142] [Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning](https://arxiv.org/abs/2509.26187)
*Youssef Sabiri,Walid Houmaidi,Aaya Bougrine,Salmane El Mansour Billah*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的室内环境质量主动管理方法，通过比较LSTM、GRU和CNN-LSTM三种架构在不同时间范围内的预测性能，为智能建筑管理系统提供预测性HVAC控制方案。

- Motivation: 传统HVAC系统在确保室内环境质量时能耗较高，需要在保证室内环境质量的同时平衡建筑能效。
- Method: 利用ROBOD数据集，对LSTM、GRU和CNN-LSTM三种深度学习架构进行基准测试，预测CO2浓度、温度和湿度等IEQ参数。
- Result: GRU在短期预测中表现最佳且计算开销较低，CNN-LSTM在长期预测中特征提取能力更强，LSTM在长程时间建模方面表现稳健。预测可靠性受数据分辨率、传感器位置和人员流动影响。
- Conclusion: 研究结果为智能建筑管理系统实施预测性HVAC控制提供了可行见解，有助于降低能耗并提升实际建筑运营中的居住者舒适度。


### [143] [Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces](https://arxiv.org/abs/2509.26594)
*John Gkountouras,Ivan Titov*

Main category: cs.LG

TL;DR: AC-RL通过强化学习训练视觉模型生成更全面的图像描述，使数学推理系统能够一次性解决问题，无需额外澄清，在7个视觉数学推理基准上平均准确率提升4.4个百分点。

- Motivation: 当前视觉语言模型生成的图像描述通常省略推理系统所需的关键细节，导致推理失败不是由于推理能力限制，而是因为缺乏关键视觉信息。
- Method: 提出自适应澄清强化学习(AC-RL)，通过交互训练视觉模型理解推理系统需要的信息，利用澄清请求揭示信息差距，惩罚需要澄清的成功案例，促使生成全面的初始描述。
- Result: 在7个视觉数学推理基准上平均准确率比预训练基线提高4.4个百分点，分析显示如果允许澄清请求，可减少高达39%的澄清需求。
- Conclusion: AC-RL证明仅通过交互学习即可有效训练视觉语言接口，无需显式标注，澄清请求可作为隐式监督信号。


### [144] [Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training](https://arxiv.org/abs/2509.26625)
*Junlin Han,Shengbang Tong,David Fan,Yufan Ren,Koustuv Sinha,Philip Torr,Filippos Kokkinos*

Main category: cs.LG

TL;DR: LLMs在纯文本预训练中意外地发展出丰富的视觉先验，这些先验由可分离的感知和推理先验组成，具有不同的扩展趋势和来源。推理先验主要来自代码、数学等推理中心数据，而感知先验则更广泛地来自语料库。

- Motivation: 研究LLMs在纯文本预训练中如何发展视觉能力，揭示视觉先验的组成和来源，为构建下一代多模态LLMs提供理论基础。
- Method: 通过100多个控制实验（消耗50万GPU小时），涵盖完整的MLLM构建流程，分析不同数据类别、模型规模和适应设置对视觉先验的影响。
- Result: 发现推理先验主要来自推理中心数据且可转移，感知先验更依赖视觉编码器和视觉指令调优数据。文本描述视觉世界很关键但性能影响快速饱和。
- Conclusion: 提出了一种从语言预训练中刻意培养视觉先验的数据中心方法，为下一代多模态LLMs的发展铺平道路。
## cs.AI

### [145] [RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration](https://arxiv.org/abs/2509.25271)
*Xiuyuan Chen,Jian Zhao,Yuchen Yuan,Tianle Zhang,Huilin Zhou,Zheng Zhu,Ping Hu,Linghe Kong,Chi Zhang,Weiran Huang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出了RADAR框架，通过多智能体协作和多轮辩论机制重构风险概念空间，显著提升大语言模型安全评估的准确性和鲁棒性。

- Motivation: 现有大语言模型安全评估方法存在评估者偏见和模型同质性导致的检测失败等固有局限性，影响了风险评估过程的鲁棒性。
- Method: 将潜在风险概念空间分解为显性风险、隐性风险和非风险三个互斥子空间，提出RADAR多智能体协作评估框架，通过四种专业互补角色和多轮辩论机制实现风险概念分布的自进化。
- Result: 在包含800个挑战性案例的测试集上，RADAR在准确性、稳定性和自评估风险敏感性等多个维度显著优于基线方法，风险识别准确率比最强基线方法提高28.87%。
- Conclusion: RADAR框架能够全面覆盖显性和隐性风险，有效减轻评估者偏见，为大语言模型安全评估提供了更可靠的方法。


### [146] [Saliency Guided Longitudinal Medical Visual Question Answering](https://arxiv.org/abs/2509.25374)
*Jialin Wu,Xiaofeng Liu*

Main category: cs.AI

TL;DR: 提出了一种用于胸部X光纵向医学视觉问答的显著性引导编码器-解码器模型，通过将后验显著性转化为可操作的监督，实现跨时间点的临床变化分析。

- Motivation: 在纵向医学VQA中，差异信号和视觉焦点的一致性比单张图像的绝对发现更具信息量，需要开发能够捕捉时间变化的方法。
- Method: 采用轻量级仿射预对齐减少访问间的干扰运动，然后执行两步循环：1)从答案提取关键词并生成关键词条件Grad-CAM获取疾病聚焦显著性；2)应用共享显著性掩码生成最终答案。
- Result: 在Medical-Diff-VQA数据集上，在BLEU、ROUGE-L、CIDEr和METEOR等指标上获得有竞争力的性能，同时提供内在可解释性。
- Conclusion: 结果表明，具有轻度预对齐的显著性条件生成是医学VQA中纵向推理的原则性框架，具有实用性和可迁移性。


### [147] [IRIS: Intrinsic Reward Image Synthesis](https://arxiv.org/abs/2509.25562)
*Yihang Chen,Yuanhao Ban,Yunqi Hong,Cho-Jui Hsieh*

Main category: cs.AI

TL;DR: 提出了IRIS框架，通过最大化自不确定性而非自确定性来改进自回归文本到图像生成模型，无需外部奖励或标注数据。

- Motivation: 由于人类偏好数据有限，传统RLHF在自回归T2I生成中受限，需要探索仅使用内部信号的学习方法。
- Method: 提出IRIS框架，使用内在奖励进行强化学习，通过最大化模型的自不确定性来改进图像生成质量。
- Result: 实验表明IRIS在自回归T2I模型上的性能可与外部奖励方法竞争甚至更优。
- Conclusion: 最大化自不确定性比最大化自确定性更有利于自回归T2I生成，IRIS框架为无外部奖励的模型改进提供了有效途径。


### [148] [Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models](https://arxiv.org/abs/2509.25584)
*Max Hartman,Vidhata Jayaraman,Moulik Choraria,Akhil Bhimaraju,Lav R. Varshney*

Main category: cs.AI

TL;DR: 提出了一个基于信息论和学习的理论框架，用于分析视觉语言模型中哪些层可以被跳过以提高推理效率，同时保持性能。

- Motivation: 视觉语言模型虽然性能优异但推理成本高，现有层跳过技术因缺乏对何时跳层有益的理论理解而未被充分利用。
- Method: 使用信息论和学习理论分析隐藏表征的演化，识别具有大冗余度的层，并与实际层跳过方法进行对比验证。
- Result: 实验表明跳过理论框架识别出的冗余层可以实现更快的推理且保持性能，而在不满足条件时跳层会导致模型性能下降。
- Conclusion: 该理论框架为多种高效推理技术提供了统一的理论支撑，能够指导在保持性能的前提下选择性跳过VLM层。


### [149] [NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language](https://arxiv.org/abs/2509.25757)
*Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: NePTune是一个神经符号框架，通过将基础视觉模型的感知能力与符号推理的组合表达能力相结合，解决了视觉语言模型在组合推理方面的局限性。

- Motivation: 现代视觉语言模型在各种任务中表现出色，但在组合推理方面存在困难。神经符号方法虽然很有前景，但通常受到严格逻辑执行或预定义谓词的限制，缺乏灵活性。
- Method: NePTune将自然语言查询动态翻译成可执行的Python程序，结合了命令式控制流和能够处理VLM生成不确定性的软逻辑运算符。采用模块化设计，将感知与推理解耦，支持无训练操作和可微调操作。
- Result: 在多个视觉推理基准测试和各种领域中使用对抗性测试进行评估，NePTune相比强大的基础模型有显著改进，并在新环境中展现出有效的组合泛化和适应能力。
- Conclusion: NePTune框架成功克服了传统神经符号方法的限制，通过混合执行模型实现了更好的组合推理能力，为视觉语言模型提供了更灵活和强大的推理解决方案。


### [150] [PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks](https://arxiv.org/abs/2509.25792)
*Alexander Branch,Omead Pooladzandi,Radin Khosraviani,Sunay Gajanan Bhat,Jeffrey Jiang,Gregory Pottie*

Main category: cs.AI

TL;DR: PureVQ-GAN是一种防御数据投毒攻击的方法，通过向量量化VAE和GAN鉴别器将投毒图像通过离散瓶颈处理，破坏细粒度触发模式同时保持语义内容。

- Motivation: 现有的扩散模型防御方法需要数百次迭代优化步骤，计算成本高，不适用于实际训练流程。需要一种更高效的防御机制来对抗数据投毒攻击。
- Method: 使用向量量化VAE（VQ-VAE）和GAN鉴别器构建防御系统。通过学习的码本对投毒图像进行量化，破坏精细的触发模式，同时GAN鉴别器确保输出符合自然图像分布。
- Result: 在CIFAR-10数据集上，对Gradient Matching和Bullseye Polytope攻击达到0%投毒成功率，对Narcissus攻击为1.64%，同时保持91-95%的干净准确率。相比扩散模型防御方法快50倍以上。
- Conclusion: PureVQ-GAN提供了一种高效实用的数据投毒防御方案，通过离散量化有效破坏后门触发器，同时保持模型性能，适合实际训练流程部署。


### [151] [CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search](https://arxiv.org/abs/2509.26037)
*Zhe Li,Zhiwei Lin,Yongtao Wang*

Main category: cs.AI

TL;DR: CoLLM-NAS是一个两阶段神经架构搜索框架，使用两个互补的大语言模型进行知识引导搜索，在ImageNet和NAS-Bench-201上取得了最先进的结果。

- Motivation: 现有的大语言模型与神经架构搜索结合方法存在架构无效性、计算效率低和性能不如传统NAS等关键限制。
- Method: 提出两阶段NAS框架，使用导航器LLM指导搜索方向，生成器LLM合成高质量候选架构，通过协调器模块管理它们的交互，结合LLM的结构化知识和迭代反馈。
- Result: 在ImageNet和NAS-Bench-201上超越了现有NAS方法和传统搜索算法，实现了新的最先进结果，并能持续提升各种两阶段NAS方法的性能和效率。
- Conclusion: CoLLM-NAS展示了出色的泛化能力，能够有效结合大语言模型的知识来改进神经架构搜索过程。


### [152] [ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning](https://arxiv.org/abs/2509.26255)
*Yichao Liang,Dat Nguyen,Cambridge Yang,Tianyang Li,Joshua B. Tenenbaum,Carl Edward Rasmussen,Adrian Weller,Zenna Tavares,Tom Silver,Kevin Ellis*

Main category: cs.AI

TL;DR: 提出一个联合学习符号状态表示和因果过程（包括内生行动和外生机制）的抽象世界模型框架，通过变分贝叶斯推理和LLM提案从有限数据中学习，在模拟桌面机器人环境中实现快速规划并泛化到更复杂任务。

- Motivation: 解决长视野具身规划中世界不仅通过智能体行动变化，还同时存在外生过程（如加热水、多米诺骨牌连锁反应）的挑战。
- Method: 提出抽象世界模型框架，联合学习符号状态表示和因果过程（内生行动和外生机制），每个因果过程建模随机因果效应关系的时间过程，通过变分贝叶斯推理结合LLM提案从有限数据中学习。
- Result: 在五个模拟桌面机器人环境中，学习到的模型能够实现快速规划，并泛化到具有更多对象和更复杂目标的保留任务，优于一系列基线方法。
- Conclusion: 该框架能够有效处理长视野具身规划中的外生过程，学习到的模型具有良好的泛化能力和规划性能。


### [153] [Zero-Shot Decentralized Federated Learning](https://arxiv.org/abs/2509.26462)
*Alessio Masano,Matteo Pennisi,Federica Proietto Salanitri,Concetto Spampinato,Giovanni Bellitto*

Main category: cs.AI

TL;DR: 提出ZeroDFL，一个完全去中心化的联邦学习框架，通过迭代提示共享机制实现零样本适应，显著降低通信开销并提升泛化能力。

- Motivation: 现有联邦提示学习方法存在泛化问题、高通信成本和依赖中央服务器等限制，影响了可扩展性和隐私保护。
- Method: 采用完全去中心化框架，通过客户端间迭代优化和交换文本提示来增强泛化能力，同时大幅减少通信开销。
- Result: 在9个多样化图像分类数据集上验证，性能优于或与最先进方法相当，通信开销比FedTPG减少118倍。
- Conclusion: ZeroDFL不仅提升了联邦零样本学习的泛化能力，还改善了可扩展性、效率和隐私保护，为大规模视觉语言模型的去中心化适应铺平了道路。
## eess.IV

### [154] [Position-Blind Ptychography: Viability of image reconstruction via data-driven variational inference](https://arxiv.org/abs/2509.25269)
*Simon Welker,Lorenz Kuger,Tim Roith,Berthy Feng,Martin Burger,Timo Gerkmann,Henry Chapman*

Main category: eess.IV

TL;DR: 提出了位置盲ptychography这一新的盲逆问题，即在不知道扫描位置的情况下进行ptychographic相位恢复，需要同时恢复图像和扫描位置。

- Motivation: 该问题源于单粒子衍射X射线成像，其中随机方向的粒子被照射并收集衍射图案。如果使用高度聚焦的X射线束，测量结果也会对每个粒子的光束位置敏感，但这些位置也是未知的。
- Method: 使用变分推理结合基于分数的扩散模型作为数据驱动的图像先验，在模拟的简化2D变体中进行图像重建研究。
- Result: 研究发现，在正确的照明结构和强先验条件下，即使在测量噪声下，也能实现可靠和成功的图像重建，除了最困难的成像场景外。
- Conclusion: 位置盲ptychography问题在适当的条件下是可行的，现代数据驱动的图像先验方法可以有效解决这一具有挑战性的逆问题。


### [155] [Anatomy-DT: A Cross-Diffusion Digital Twin for Anatomical Evolution](https://arxiv.org/abs/2509.25280)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: eess.IV

TL;DR: 提出了一种结合机理偏微分方程与可微分深度学习的数学框架，用于模拟肿瘤形态的时空演变及其与周围解剖结构的相互作用，实现解剖结构到解剖结构的生成。

- Motivation: 现有方法主要关注肿瘤生长而忽略了相邻解剖结构的伴随变化，但肿瘤演变具有高度非线性和异质性，受空间背景和邻近组织相互作用影响，因此需要联合建模肿瘤进展与周围解剖结构以获得全面的疾病动态理解。
- Method: 使用多类概率场表示解剖结构，通过交叉扩散反应-扩散系统演化，强制类间竞争和排他性；采用可微分隐式-显式方案处理刚性扩散和非线性反应项；引入拓扑正则化器保持中心线并惩罚区域重叠。
- Result: 在合成数据集上达到最先进精度并保持拓扑结构，在临床数据集上也表现出优越性能。
- Conclusion: 通过整合PDE动力学、拓扑感知正则化和可微分求解器，为数字孪生建立了一条原则性路径，实现视觉真实、解剖排他和拓扑一致的解剖结构生成。


### [156] [Multi-modal Liver Segmentation and Fibrosis Staging Using Real-world MRI Images](https://arxiv.org/abs/2509.26061)
*Yang Zhou,Kunhao Yuan,Ye Wei,Jishizhan Chen*

Main category: eess.IV

TL;DR: 开发了一个自动化管道，用于肝脏分割和纤维化分期，使用多模态MRI数据和基于形状、纹理、外观和方向特征的深度学习模型，在CARE 2025挑战中取得了顶级性能。

- Motivation: 肝纤维化的精确分期通常需要侵入性方法，存在风险和并发症。为了解决这个问题，需要开发非侵入性的自动化方法来量化分析肝纤维化。
- Method: 集成基于多模态配准的伪标记、使用深度神经网络的肝脏分割，以及基于分割掩模和MRI图像提取的形状、纹理、外观和方向特征的肝纤维化分期。
- Result: 在有限的标注数据下，提出的管道对所有MRI模态表现出优秀的泛化能力，在所有竞赛子任务中取得了顶级性能。
- Conclusion: 该方法为基于MRI的肝纤维化定量评估提供了一个快速可复现的框架，支持早期诊断和临床决策。


### [157] [Ordinal Label-Distribution Learning with Constrained Asymmetric Priors for Imbalanced Retinal Grading](https://arxiv.org/abs/2509.26146)
*Nagur Shareef Shaik,Teja Krishna Cherukuri,Adnan Masood,Ehsan Adeli,Dong Hye Ye*

Main category: eess.IV

TL;DR: CAP-WAE是一种新颖的糖尿病视网膜病变分级框架，通过非对称先验、结构化潜在空间和方向感知有序损失来解决分级任务中的有序性和长尾分布问题。

- Motivation: 糖尿病视网膜病变分级具有内在的有序性和长尾分布特性，少数阶段稀缺、异质且临床检测至关重要。传统方法依赖各向同性高斯先验和对称损失函数，与任务的不对称性质不匹配。
- Method: 采用Wasserstein自编码器，将其聚合后验与非对称先验对齐；通过边距感知正交性和紧致性损失结构化潜在空间；引入方向感知有序损失，使用轻量级头预测非对称离散度生成软标签；采用自适应多任务加权方案稳定训练。
- Result: 在公共DR基准测试中，CAP-WAE在二次加权Kappa、准确率和宏F1分数方面持续达到最先进水平，超越了有序分类和潜在生成基线方法。t-SNE可视化显示该方法将潜在流形重塑为紧凑、有序分级的簇，减少了重叠。
- Conclusion: CAP-WAE通过非对称先验、结构化潜在空间和临床优先级感知监督，有效解决了糖尿病视网膜病变分级中的有序性和长尾分布挑战，实现了优异的性能表现。


### [158] [GastroViT: A Vision Transformer Based Ensemble Learning Approach for Gastrointestinal Disease Classification with Grad CAM & SHAP Visualization](https://arxiv.org/abs/2509.26502)
*Sumaiya Tabassum,Md. Faysal Ahamed,Hafsa Binte Kibria,Md. Nahiduzzaman,Julfikar Haider,Muhammad E. H. Chowdhury,Mohammad Tariqul Islam*

Main category: eess.IV

TL;DR: 提出了一种基于预训练视觉变换器（ViT）的集成模型GastroViT，用于准确分类胃肠道内窥镜图像，在23类疾病分类中达到91.98%的准确率，模型仅含2000万参数，并整合了可解释AI方法增强模型透明度。

- Motivation: 胃肠道疾病种类繁多，从轻微炎症到致命疾病，早期准确识别对阻止疾病进展和改善治疗效果至关重要。传统方法在准确分类多种胃肠道疾病方面存在挑战。
- Method: 使用集成学习方法，结合两个预训练模型MobileViT_XS和MobileViT_V2_200的预测结果，并在HyperKvasir数据集（10,662张图像，23种疾病）上进行评估，同时整合Grad-CAM和SHAP等可解释AI方法。
- Result: 集成模型GastroViT在23类分类测试中达到91.98%准确率，平均精确率69%、召回率63%、F1分数64%；在16类分类测试中表现更好，准确率92.70%，平均精确率87%、召回率86%、F1分数87%。
- Conclusion: 提出的GastroViT集成模型在胃肠道疾病分类任务中表现出色，仅用2000万参数就实现了高精度分类，结合可解释AI方法为临床诊断提供了可靠且透明的辅助工具。
## cs.GR

### [159] [Vector sketch animation generation with differentialable motion trajectories](https://arxiv.org/abs/2509.25857)
*Xinding Zhu,Xinye Yang,Shuyang Zheng,Zhexin Zhang,Fei Gao,Jing Huang,Jiazhou Chen*

Main category: cs.GR

TL;DR: 提出了一种端到端的矢量草图动画自动生成方法，通过可微分运动轨迹(DMT)表示解决闪烁问题，使用伯恩斯坦基函数平衡多项式参数敏感性，引入稀疏轨迹点进行显式空间建模。

- Motivation: 基于图像的草图绘制已得到充分研究，但视频草图动画生成由于时间一致性要求仍然具有挑战性，特别是闪烁问题。
- Method: 使用可微分运动轨迹(DMT)表示，通过多项式轨迹描述笔画控制点的逐帧运动，采用伯恩斯坦基函数平衡参数敏感性，引入稀疏轨迹点进行显式空间建模。
- Result: 在DAVIS和LVOS数据集上的评估显示优于现有方法，在3D模型和文本到视频数据的跨域验证中表现出鲁棒性和兼容性。
- Conclusion: DMT方法通过全局语义梯度传播显著改善了语义一致性和时间连贯性，能够生成高帧率输出，并支持长时视频处理。


### [160] [GaussEdit: Adaptive 3D Scene Editing with Text and Image Prompts](https://arxiv.org/abs/2509.26055)
*Zhenyu Shu,Junlong Yu,Kai Chao,Shiqing Xin,Ligang Liu*

Main category: cs.GR

TL;DR: GaussEdit是一个基于3D高斯泼溅的3D场景编辑框架，通过文本和图像提示实现自适应编辑，采用三阶段优化流程确保编辑质量和效率。

- Motivation: 现有3D场景编辑方法在编辑精度、视觉真实性和处理速度方面存在不足，需要一种能够平衡全局场景一致性和局部细节编辑的解决方案。
- Method: 使用3D高斯泼溅作为场景表示，采用三阶段流程：初始化3D高斯、自适应全局-局部优化策略（包含类别引导正则化）、基于图像到图像合成的纹理增强。
- Result: 实验结果表明GaussEdit在编辑精度、视觉保真度和处理速度方面优于现有方法，能够成功将用户指定概念嵌入3D场景。
- Conclusion: GaussEdit是一个强大的用户驱动3D场景编辑工具，相比传统方法有显著改进，为详细3D场景编辑提供了有效解决方案。


### [161] [3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation](https://arxiv.org/abs/2509.26233)
*Balamurugan Thambiraja,Malte Prinzler,Sadegh Aliakbarian,Darren Cosker,Justus Thies*

Main category: cs.GR

TL;DR: 3DiFACE是一个用于整体语音驱动3D面部动画的新方法，能够为单一音频输入生成多样化的唇部和头部运动，并支持通过关键帧和插值进行编辑。

- Motivation: 当前语音驱动的3D面部动画方法在创建个性化动画、精确控制和编辑方面存在挑战，现有方法无法编辑/重新生成部分输入动画，且忽视了同一音频输入可能对应多种合理的唇部和头部运动。
- Method: 提出完全卷积的扩散模型，利用训练语料库中的音素级多样性；采用说话风格个性化方法和新颖的稀疏引导运动扩散来实现精确控制和编辑。
- Result: 通过定量和定性评估表明，该方法能够基于单一音频输入生成和编辑多样化的整体3D面部动画，并在高保真度和多样性之间实现控制。
- Conclusion: 3DiFACE方法成功解决了语音驱动3D面部动画中的多样性和编辑控制问题，提供了有效的解决方案。
## cs.CL

### [162] [Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer](https://arxiv.org/abs/2509.25817)
*Jaeyoung Kim,Jongho Lee,Hongjun Choi,Sion Jang*

Main category: cs.CL

TL;DR: 使用科学论文作者档案数据进行个性化图表标题生成的研究，发现结合丰富作者档案和元数据能显著提升多模态大语言模型的个性化性能，但存在作者风格匹配与标题质量维护之间的权衡。

- Motivation: 研究如何利用科学论文中的作者档案数据来生成个性化的图表标题，探索多模态大语言模型在个性化任务中的潜力。
- Method: 使用作者档案数据和相关元数据，结合多模态大语言模型进行个性化图表标题生成实验。
- Result: 丰富的作者档案数据结合元数据能显著提升个性化性能，但揭示了作者风格匹配与标题质量之间的基本权衡关系。
- Conclusion: 为开发平衡个性化风格与标题质量的实用标题自动化系统提供了有价值的见解和未来方向。


### [163] [OceanGym: A Benchmark Environment for Underwater Embodied Agents](https://arxiv.org/abs/2509.26536)
*Yida Xue,Mingjun Mao,Xiangyuan Ru,Yuqi Zhu,Baochang Ren,Shuofei Qiao,Mengru Wang,Shumin Deng,Xinyu An,Ningyu Zhang,Ying Chen,Huajun Chen*

Main category: cs.CL

TL;DR: OceanGym是首个面向海洋水下具身智能体的综合基准平台，旨在解决水下环境中的极端感知和决策挑战，包含8个现实任务领域和基于多模态大语言模型的统一智能体框架。

- Motivation: 水下环境具有低能见度、动态洋流等极端挑战，使得智能体部署异常困难，需要开发能够在这种恶劣条件下工作的鲁棒具身AI系统。
- Method: 采用多模态大语言模型驱动的统一智能体框架，整合感知、记忆和序列决策能力，要求智能体理解光学和声纳数据，在复杂环境中自主探索并完成长期目标。
- Result: 广泛实验显示，当前最先进的MLLM驱动智能体与人类专家之间存在显著差距，突显了在海洋水下环境中感知、规划和适应性方面的持续困难。
- Conclusion: OceanGym为开发鲁棒具身AI提供了高保真平台，是向能够在海洋这一地球最后未开发前沿领域运行的智能体迈出的决定性一步。
## cs.IR

### [164] [MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval](https://arxiv.org/abs/2509.26378)
*Junjie Zhou,Ze Liu,Lei Xiong,Jin-Ge Yao,Yueze Wang,Shitao Xiao,Fenfen Lin,Miguel Hu Chen,Zhicheng Dou,Siqi Bao,Defu Lian,Yongping Xiong,Zheng Liu*

Main category: cs.IR

TL;DR: MR^2-Bench是一个推理密集型多模态检索基准，超越了传统的浅层语义匹配，专注于评估模型在逻辑、空间和因果推理方面的能力。

- Motivation: 现有基准主要测试表面语义对应（如对象-文本匹配），无法评估视觉和文本信息之间复杂关系所需的深层推理能力。
- Method: 构建包含1,309个精心策划查询的基准，涵盖自然图像、图表和视觉谜题等多样化多模态数据，支持包含多个图像的复杂查询和文档。
- Result: 当前最先进模型在MR^2-Bench上表现不佳，例如Seed1.6-Embedding模型的Recall@1仅为9.91，远低于在MMEB上的77.78。
- Conclusion: 该基准揭示了多模态检索中推理能力的显著差距，强调了在推理密集型多模态检索领域进一步发展的迫切需求。
## cs.RO

### [165] [Online Mapping for Autonomous Driving: Addressing Sensor Generalization and Dynamic Map Updates in Campus Environments](https://arxiv.org/abs/2509.25542)
*Zihan Zhang,Abhijit Ravichandran,Pragnya Korti,Luobin Wang,Henrik I. Christensen*

Main category: cs.RO

TL;DR: 在校园高尔夫球车平台上部署在线地图系统，通过双前摄像头和LiDAR传感器实时生成和更新高精地图，解决传统HD地图制作成本高、维护难的问题。

- Motivation: 传统高精地图制作劳动密集、成本高昂且在动态环境中难以维护，需要开发实时在线地图生成系统以适应自动驾驶需求。
- Method: 在校园高尔夫球车平台上部署双前摄像头和LiDAR传感器，集成SemVecMap模型，通过校园特定数据微调，实现增量式地图生成和更新。
- Result: 系统能够准确预测地图并支持持续更新，在真实自动驾驶场景中展示出实用价值。
- Conclusion: 该在线地图系统成功解决了HD地图的实时生成和维护问题，为自动驾驶提供了可行的地图解决方案。


### [166] [dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.25681)
*Junjie Wen,Minjie Zhu,Jiaming Liu,Zhiyuan Liu,Yicun Yang,Linfeng Zhang,Shanghang Zhang,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: dVLA是一个基于扩散模型的视觉-语言-动作统一框架，通过多模态思维链整合视觉感知、语言推理和机器人控制，在仿真和真实环境中都取得了最先进的性能表现。

- Motivation: 现有的视觉-语言-动作模型需要将感知、语言理解和动作控制分开优化，缺乏统一的跨模态推理能力，难以泛化到新的指令和物体。
- Method: 采用扩散模型统一优化感知、语言理解和动作控制，使用多模态思维链增强跨模态推理，并通过前缀注意力掩码和KV缓存加速推理过程。
- Result: 在LIBERO基准测试中达到96.4%的平均成功率，超越离散和连续动作策略；在真实Franka机器人上成功完成包括需要多步规划的拣选任务在内的多样化任务。
- Conclusion: 统一的扩散框架为实用高性能的视觉-语言-动作机器人系统提供了有前景的解决方案。


### [167] [SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning](https://arxiv.org/abs/2509.26375)
*Zichao Shen,Chen Gao,Jiaqi Yuan,Tianchen Zhu,Xingcheng Fu,Qingyun Sun*

Main category: cs.RO

TL;DR: SDA-PLANNER是一个用于具身任务规划的新型LLM架构，通过状态依赖图和错误自适应重规划机制，解决了现有方法的固定规划范式、缺乏动作序列约束和错误不可知等限制。

- Motivation: 现有基于LLM的具身任务规划方法存在三个主要限制：固定的规划范式、缺乏动作序列约束以及对执行错误的不可知性，这限制了它们在复杂环境中的实际应用效果。
- Method: 提出SDA-PLANNER，引入状态依赖图来显式建模动作前提条件和效果，指导动态修订；采用错误自适应重规划策略，包括错误回溯与诊断以及自适应动作子树生成，基于当前环境状态局部重建受影响计划部分。
- Result: 实验表明SDA-PLANNER在成功率和目标完成度方面持续优于基线方法，特别是在各种错误条件下表现更加突出。
- Conclusion: SDA-PLANNER通过自适应规划范式、状态依赖感知和错误感知机制，为具身任务规划提供了更全面的解决方案，显著提升了规划性能和鲁棒性。
## cs.SD

### [168] [LTA-L2S: Lexical Tone-Aware Lip-to-Speech Synthesis for Mandarin with Cross-Lingual Transfer Learning](https://arxiv.org/abs/2509.25670)
*Kang Yang,Yifan Liang,Fangkun Liu,Zhenping Xie,Chengshi Zheng*

Main category: cs.SD

TL;DR: 提出Lexical Tone-Aware Lip-to-Speech (LTA-L2S)模型，通过跨语言迁移学习和流匹配技术解决普通话唇语合成中的音位映射和声调建模难题。

- Motivation: 普通话唇语合成面临复杂视位-音位映射和声调在可理解性中的关键作用等挑战，需要专门的方法来处理这些语言特性。
- Method: 采用跨语言迁移学习策略，将英语预训练的视听自监督学习模型适配到普通话；使用流匹配模型生成F0轮廓，并通过ASR微调的语音单元指导；采用两阶段训练范式提升语音质量。
- Result: 实验表明LTA-L2S在语音可理解性和声调准确性方面显著优于现有方法。
- Conclusion: 该方法成功解决了普通话唇语合成的核心挑战，为处理复杂语言特性的唇语合成提供了有效解决方案。
## cs.IT

### [169] [Challenges and Solutions in Selecting Optimal Lossless Data Compression Algorithms](https://arxiv.org/abs/2509.25219)
*Md. Atiqur Rahman,MM Fazle Rabbi*

Main category: cs.IT

TL;DR: 提出一个数学框架，将压缩率、编码时间和解码时间整合为统一性能评分，用于客观比较不同压缩算法，帮助选择最适合特定应用需求的压缩方法。

- Motivation: 现有压缩算法在压缩率、编码速度和解码速度之间存在权衡，没有算法在所有维度都表现最佳，这给需要同时关注多个性能指标的应用（如医学影像）带来了算法选择困难。
- Method: 开发了一个数学框架，通过标准化和加权方案平衡压缩率、编码时间和解码时间三个指标，形成统一的性能评分模型。
- Result: 在图像和文本数据集上的实验验证了该方法的有效性，能够可靠地为不同优先级设置识别最合适的压缩器。结果显示，现代学习型编解码器通常提供更好的压缩率，而经典算法在速度优先时仍具优势。
- Conclusion: 该框架提供了一个稳健且适应性强的决策支持工具，用于选择最优的无损数据压缩技术，弥合了理论度量与实际应用需求之间的差距。
## cond-mat.mtrl-sci

### [170] [Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework](https://arxiv.org/abs/2509.26548)
*Jian Guo Pan,Lin Wang,Xia Cai*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了基于深度学习的自动化SEM图像分割框架PerovSegNet，用于精确识别钙钛矿太阳能电池薄膜中的碘化铅、钙钛矿和缺陷区域，显著提升了分析效率和准确性。

- Motivation: 当前SEM图像分析主要依赖人工操作，限制了通量性和一致性。准确识别和量化碘化铅和钙钛矿相对于理解结晶过程和缺陷形成至关重要。
- Method: 基于改进的YOLOv8x架构，引入两个新模块：自适应洗牌扩张卷积块（增强多尺度和细粒度特征提取）和可分离自适应下采样模块（保留纹理和结构信息）。在10,994张增强SEM图像上训练。
- Result: 平均精度达到87.25%，比基线YOLOv8x-seg提升4.08%，同时模型大小和计算负载分别减少24.43%和25.22%。
- Conclusion: PerovSegNet为钙钛矿薄膜制备提供了可扩展的实时过程监控和数据驱动优化工具，能够提供晶粒级定量指标用于评估结晶效率和微观结构质量。
