[[toc]]

## cs.CV

### [1] [PuzzlePoles: Cylindrical Fiducial Markers Based on the PuzzleBoard Pattern](https://arxiv.org/abs/2511.19448)
*Juri Zach,Peer Stelldinger*

Main category: cs.CV

TL;DR: 提出了一种新型圆柱形视觉标记PuzzlePole，基于PuzzleBoard校准模式，支持360度视角的可靠识别和姿态估计。

- Motivation: 自主系统需要可靠的环境感知，校准和定位任务通常依赖于鲁棒的视觉标记。
- Method: 利用PuzzleBoard模式的独特组合结构，设计圆柱形标记PuzzlePole，实现高精度的定位和方向估计，并对遮挡具有鲁棒性。
- Result: PuzzlePole能够从360度视角进行可靠识别和姿态估计，提供高精度的定位和方向信息。
- Conclusion: PuzzlePole设计灵活，适用于机器人导航、SLAM和实体界面等多种自主系统场景。


### [2] [Personalized Reward Modeling for Text-to-Image Generation](https://arxiv.org/abs/2511.19458)
*Jeongeun Lee,Ryang Heo,Dongha Lee*

Main category: cs.CV

TL;DR: PIGReward是一个个性化奖励模型，通过动态生成用户条件评估维度和CoT推理来评估文本到图像生成结果，无需用户特定训练即可实现个性化评估和提示优化。

- Motivation: 现有的文本到图像模型评估方法（通用奖励函数或基于相似度的指标）无法捕捉个人视觉偏好的多样性和复杂性，需要更有效的个性化评估方案。
- Method: 采用自引导策略，在有限参考数据上进行推理构建丰富的用户上下文，通过动态生成用户条件评估维度和CoT推理来评估图像，并提供个性化反馈驱动提示优化。
- Result: PIGReward在准确性和可解释性方面均优于现有方法，建立了基于推理的个性化T2I评估和优化的可扩展基础。
- Conclusion: PIGReward是实现个体对齐T2I生成的稳健步骤，为个性化评估和优化提供了有效解决方案。


### [3] [SG-OIF: A Stability-Guided Online Influence Framework for Reliable Vision Data](https://arxiv.org/abs/2511.19466)
*Penghao Rao,Runmin Jiang,Min Xu*

Main category: cs.CV

TL;DR: 提出SG-OIF框架，通过算法稳定性实时控制，实现深度学习视觉模型中训练点对测试预测影响的在线估计，在噪声标签和分布外检测任务上达到SOTA性能。

- Motivation: 现有影响函数方法在深度学习视觉模型中面临挑战：逆曲率计算昂贵，训练非平稳性使静态近似失效，离线计算滞后于训练动态，缺乏置信度校准导致脆弱排名。
- Method: SG-OIF框架将算法稳定性作为实时控制器，维护轻量级锚点IHVPs，提出模块化曲率后端，使用稳定性引导的残差阈值、异常门控和置信度来调节每个样本的影响分数。
- Result: 在CIFAR-10（20%不对称噪声）上，前1%预测样本准确率达到91.1%；在MNIST上AUPR得分达到99.8%，在多个数据集的各种损坏情况下均表现优异。
- Conclusion: SG-OIF是第一个实用的在线影响估计控制器框架，能够有效识别关键样本，在噪声标签和分布外检测任务上实现最先进性能。


### [4] [Pistachio: Towards Synthetic, Balanced, and Long-Form Video Anomaly Benchmarks](https://arxiv.org/abs/2511.19474)
*Jie Li,Hongyi Cai,Mingkang Dong,Muxin Pu,Shan You,Fei Wang,Tao Huang*

Main category: cs.CV

TL;DR: Pistachio是一个新的视频异常检测/理解基准，通过生成式流水线构建，提供场景多样性、平衡异常覆盖和时间复杂性，解决了现有基准的局限性。

- Motivation: 现有视频异常检测基准缺乏场景多样性、平衡异常覆盖和时间复杂性，而视频异常理解需要更深层的语义和因果推理但难以标注。
- Method: 利用视频生成模型，通过场景条件异常分配、多步骤故事情节生成和时间一致的长格式合成策略，构建41秒的连贯视频。
- Result: Pistachio展示了规模、多样性和复杂性，为现有方法带来新挑战，并推动动态和多事件异常理解的未来研究。
- Conclusion: Pistachio基准通过生成式方法有效消除了互联网收集数据集的偏见和限制，为视频异常检测和理解提供了更可靠的评估平台。


### [5] [Tracking and Segmenting Anything in Any Modality](https://arxiv.org/abs/2511.19475)
*Tianlu Zhang,Qiang Zhang,Guiguang Ding,Jungong Han*

Main category: cs.CV

TL;DR: SATA是一个统一的跟踪和分割框架，通过解耦混合专家机制和任务感知多目标跟踪管道，解决了跨模态分布差距和跨任务特征表示差距的问题，在18个基准测试中表现出色。

- Motivation: 现有方法在处理跟踪和分割任务时通常使用专门架构或模态特定参数，限制了泛化能力和可扩展性。现有统一方法忽视了跨模态分布差距和跨任务特征表示差距这两个关键挑战。
- Method: 提出解耦混合专家机制将统一表示学习分解为跨模态共享知识和特定信息建模，以及任务感知多目标跟踪管道将所有任务输出统一为具有校准ID信息的实例集合。
- Result: SATA在18个具有挑战性的跟踪和分割基准测试中表现出优越性能。
- Conclusion: SATA为更通用的视频理解提供了新颖视角，通过解决跨模态和跨任务的知识共享问题，实现了真正的通用模型开发。


### [6] [The Determinant Ratio Matrix Approach to Solving 3D Matching and 2D Orthographic Projection Alignment Tasks](https://arxiv.org/abs/2511.19511)
*Andrew J. Hanson,Sonya M. Hanson*

Main category: cs.CV

TL;DR: 本文提出了一种基于行列式比值矩阵(DRaM)的新方法，用于解决3D-3D姿态估计(EnP)和3D-2D正交投影姿态估计(OnP)问题，并将现有方法统一到DRaM框架下。

- Motivation: 姿态估计是计算机视觉中的重要问题，现有方法如SVD和四元数特征系统能精确解决带噪声的3D-3D对齐问题，但对于带噪声的3D-2D正交投影任务缺乏可比较的闭式解。
- Method: 使用行列式比值矩阵(DRaM)方法求解无误差EnP和OnP问题的最小二乘系统，对于带噪声数据采用直接的旋转校正方案。
- Result: 提出了EnP和OnP旋转估计问题的DRaM家族解决方案，并比较了不同解决方案家族的行为。
- Conclusion: 这项工作不仅为3D和2D正交姿态估计问题提供了新解决方案，还深入揭示了这类问题的本质，DRaM方法可推广到所有类似的N维欧几里得姿态估计问题。


### [7] [Single Image to High-Quality 3D Object via Latent Features](https://arxiv.org/abs/2511.19512)
*Huanning Dong,Yinuo Huang,Fan Li,Ping Kuang*

Main category: cs.CV

TL;DR: LatentDreamer是一个从单张图像生成3D物体的新框架，使用预训练的变分自编码器将3D几何映射到潜在特征，通过序列化流程生成粗糙几何、精细几何和逼真纹理，可在70秒内完成高保真3D生成。

- Motivation: 现有的自动3D生成方法（如图像到3D）难以同时实现快速、详细和高保真的生成，需要一种能平衡这些需求的新方法。
- Method: 使用预训练的变分自编码器将3D几何映射到潜在特征，通过序列化流程依次生成粗糙几何、精细几何和逼真纹理。
- Result: 生成的3D物体对输入图像具有高保真度，整个生成过程可在70秒内完成，少量训练即可达到与当代方法竞争的性能。
- Conclusion: LatentDreamer通过潜在特征映射和序列化生成流程，实现了快速、高保真的3D物体生成，在速度和保真度之间取得了良好平衡。


### [8] [Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning](https://arxiv.org/abs/2511.19515)
*Shawn Young,Xingyu Zeng,Lijian Xu*

Main category: cs.CV

TL;DR: 本文研究了模型容量与保留图像语义所需最少视觉token数量之间的关系，提出了正交过滤方法来压缩冗余token，并发现了token与模型规模的缩放规律。

- Motivation: 受最小描述长度原理启发，研究模型容量与图像语义保存所需最少视觉token之间的基本关系，重新解释图像token为视觉语义空间中的向量。
- Method: 提出正交过滤方法，这是一个轻量级模块，能够自适应地将冗余token聚类为紧凑的正交基向量集。
- Result: 通过广泛的ViT模型实验，揭示了token与模型规模的缩放规律：更大的模型需要显著更少的token来跨越视觉语义空间。此外还贡献了一个视觉长上下文数据集。
- Conclusion: 模型容量与所需视觉token数量存在负相关关系，更大的模型能够用更少的token有效表示图像语义，正交过滤方法能有效压缩冗余信息。


### [9] [Connecting the Dots: Training-Free Visual Grounding via Agentic Reasoning](https://arxiv.org/abs/2511.19516)
*Liqin Luo,Guangyao Chen,Xiawu Zheng,Yongxing Dai,Yixiong Zou,Yonghong Tian*

Main category: cs.CV

TL;DR: 提出了GroundingAgent，一种无需任务特定微调的智能视觉定位框架，通过结合预训练的开词汇目标检测器、多模态大语言模型和大语言模型，实现零样本视觉定位。

- Motivation: 现有视觉定位方法依赖大量任务特定标注和微调，限制了在新颖或分布外场景中的泛化能力。
- Method: 采用结构化迭代推理机制，整合预训练的开词汇目标检测器、多模态大语言模型和大语言模型，通过联合语义和空间分析逐步优化候选区域。
- Result: 在RefCOCO、RefCOCO+、RefCOCOg基准测试中达到65.1%的平均零样本定位准确率；仅在选择阶段使用原始查询文本时准确率可达约90%。
- Conclusion: GroundingAgent在无需微调的情况下实现了强大的视觉定位性能，展示了LLM推理能力的关键作用，并提供良好的可解释性。


### [10] [Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning](https://arxiv.org/abs/2511.19518)
*Zhaoqi Xu,Yingying Zhang,Jian Li,Jianwei Guo,Qiannan Zhu,Hua Huang*

Main category: cs.CV

TL;DR: 提出InfoPrune框架，基于信息瓶颈原理对视觉语言模型进行自适应结构压缩，通过信息理论方法量化注意力头重要性，实现训练和训练自由两种压缩方案，在保持性能的同时显著减少计算量。

- Motivation: 现有视觉语言模型规模不断增长导致部署效率问题，传统压缩方法缺乏理论保证，需要基于信息理论的结构压缩框架。
- Method: 基于信息瓶颈原理，引入熵基有效秩和KS距离量化注意力头贡献，提出训练基注意力头剪枝和训练自由FFN低秩近似两种压缩方案。
- Result: 在VQAv2、TextVQA和GQA数据集上实现3.2倍FLOP减少和1.8倍加速，性能损失可忽略。
- Conclusion: InfoPrune为高效多模态大模型提供了理论基础和实用有效的压缩方法。


### [11] [Blinking Beyond EAR: A Stable Eyelid Angle Metric for Driver Drowsiness Detection and Data Augmentation](https://arxiv.org/abs/2511.19519)
*Mathis Wolter,Julie Stephany Berrio Perez,Mao Shan*

Main category: cs.CV

TL;DR: 提出了一种基于3D面部标志的新颖眼睑角度(ELA)指标，用于检测驾驶员困倦，相比传统方法更稳定且对视角变化更鲁棒，并利用ELA生成合成数据集来增强训练数据多样性。

- Motivation: 可靠检测驾驶员困倦对提高道路安全和辅助驾驶系统至关重要，但现有方法如EAR在视角变化时不稳定，且自然困倦数据收集困难且危险。
- Method: 从3D面部标志推导出眼睑角度(ELA)作为眼睑运动的稳定几何描述，设计眨眼检测框架提取时间特征，并利用ELA在Blender 3D中生成可控的合成数据集。
- Result: 在公共驾驶员监控数据集上的实验表明，ELA在视角变化下比EAR方差更低，能实现准确的眨眼检测，同时合成数据增强扩展了困倦识别的训练数据多样性。
- Conclusion: ELA既是可靠的生物特征测量指标，也是驾驶员状态监测中生成可扩展数据集的强大工具。


### [12] [VideoChat-M1: Collaborative Policy Planning for Video Understanding via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.19524)
*Boyu Chen,Zikang Wang,Zhengrong Yue,Kainan Yan,Chenyun Yu,Yi Huang,Zijun Liu,Yafei Wen,Xiaoxin Chen,Yang Liu,Peng Li,Yali Wang*

Main category: cs.CV

TL;DR: VideoChat-M1是一个用于视频理解的多智能体系统，采用协作策略规划范式，通过多智能体强化学习优化性能，在多个基准测试中达到最先进水平。

- Motivation: 现有视频理解系统大多采用静态、不可学习的工具调用机制，限制了在时间或空间复杂视频中发现多样化线索的能力。
- Method: 提出协作策略规划范式，包含策略生成、策略执行和策略通信三个关键过程，并采用多智能体强化学习方法联合优化智能体团队。
- Result: 在8个基准测试的4个任务中达到SOTA性能，在LongVideoBench上分别超过Gemini 2.5 pro 3.6%和GPT-4o 15.6%。
- Conclusion: VideoChat-M1通过动态协作策略规划和多智能体强化学习，显著提升了视频理解的性能，特别是在复杂视频场景下。


### [13] [Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.19526)
*Jonathan Lee,Xingrui Wang,Jiawei Peng,Luoxin Ye,Zehan Zheng,Tiezheng Zhang,Tao Wang,Wufei Ma,Siyi Chen,Yu-Cheng Chou,Prakhar Kaushik,Alan Yuille*

Main category: cs.CV

TL;DR: 提出了感知分类法(Perceptual Taxonomy)作为场景理解的结构化过程，并创建了一个包含5802张图像、28033个问题的基准测试，用于评估物理基础视觉推理能力。实验显示当前视觉语言模型在属性驱动问题上表现较差，特别是在需要多步推理的场景中。

- Motivation: 当前视觉语言基准测试主要关注表面识别或图像文本对齐，缺乏对结构化场景理解和物理基础推理能力的全面评估。人类认知中的目标导向推理需要识别对象、空间配置，并推断任务相关属性。
- Method: 1) 提出感知分类法作为结构化场景理解过程；2) 标注3173个对象的84个细粒度属性；3) 构建包含5802张图像的多选题基准测试；4) 设计四种问题类型和专家编写问题；5) 使用上下文推理示例进行提示优化。
- Result: 领先的视觉语言模型在识别任务上表现良好，但在属性驱动问题上性能下降10-20%，特别是在需要多步推理的结构化属性问题上。提供模拟场景的上下文推理示例能提高真实世界和专家编写问题的性能。
- Conclusion: 当前模型在结构化视觉理解方面存在持续差距，过度依赖模式匹配。感知分类法引导的提示方法能有效提升物理基础推理能力，为开发更强大的视觉推理模型提供了方向。


### [14] [MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training](https://arxiv.org/abs/2511.19527)
*Hongyu Lyu,Thomas Monninger,Julie Stephany Berrio Perez,Mao Shan,Zhenxing Ming,Stewart Worrall*

Main category: cs.CV

TL;DR: MapRF是一个弱监督框架，仅使用2D图像标签学习构建3D地图，通过NeRF模块生成伪标签并迭代优化，在自动驾驶在线高清地图构建中达到接近全监督方法的性能。

- Motivation: 现有方法依赖昂贵的3D地图标注进行训练，限制了在不同驾驶环境中的泛化能力和可扩展性。需要一种更经济高效的在线高清地图构建方法。
- Method: 提出弱监督框架MapRF，使用条件神经辐射场(NeRF)模块生成视图一致的3D几何和语义伪标签，通过自训练方式迭代优化地图网络，并引入Map-to-Ray匹配策略缓解误差累积。
- Result: 在Argoverse 2和nuScenes数据集上的实验表明，MapRF达到全监督方法约75%的性能，超越了多个仅使用2D标签的方法。
- Conclusion: MapRF展示了在自动驾驶中实现可扩展且经济高效的在线高清地图构建的潜力，仅需2D图像标签即可获得接近全监督的性能。


### [15] [Vidi2: Large Multimodal Models for Video Understanding and Creation](https://arxiv.org/abs/2511.19529)
*Vidi Team,Celong Liu,Chia-Wen Kuo,Chuang Huang,Dawei Du,Fan Chen,Guang Chen,Haoji Zhang,Haojun Zhao,Lingxi Zhang,Lu Guo,Lusha Li,Longyin Wen,Qihang Fan,Qingyu Chen,Rachel Deng,Sijie Zhu,Stuart Siew,Tong Jin,Weiyan Tao,Wen Zhong,Xiaohui Shen,Xin Gu,Zhenfang Chen,Zuhua Lin*

Main category: cs.CV

TL;DR: Vidi2模型在视频理解方面取得重大进展，具备细粒度时空定位能力，可同时识别时间戳和边界框，在VUE-STG和VUE-TR-V2基准测试中超越Gemini 3 Pro和GPT-5等专有系统。

- Motivation: 视频已成为互联网主要通信和创作媒介，需要可扩展、高质量的视频生产解决方案，推动下一代视频创建技术的发展。
- Method: Vidi2模型通过端到端时空定位能力，能够根据文本查询识别对应时间戳和边界框，并引入新的VUE-STG基准测试，改进视频时长、查询格式、标注质量和评估指标。
- Result: Vidi2在VUE-TR-V2和VUE-STG基准测试中显著优于领先的专有系统，同时在视频问答基准上与类似规模的开源模型竞争性结果。
- Conclusion: Vidi2模型在视频理解和时空定位方面达到先进水平，为复杂编辑场景提供强大支持，同时新基准为实际应用提供了更全面的评估标准。


### [16] [Cross-Domain Generalization of Multimodal LLMs for Global Photovoltaic Assessment](https://arxiv.org/abs/2511.19537)
*Muhao Guo,Yang Weng*

Main category: cs.CV

TL;DR: 本研究探讨了多模态大语言模型在分布式光伏系统跨域泛化方面的应用，相比传统计算机视觉模型，该模型在未见区域表现出更强的鲁棒性。

- Motivation: 分布式光伏系统的快速扩张给电网管理带来挑战，许多安装未被记录。传统计算机视觉模型需要大量标注数据且难以跨区域泛化。
- Method: 利用结构化提示和微调，将检测、定位和量化集成到统一框架中，采用多模态大语言模型进行全球光伏评估。
- Result: 跨区域评估显示，所提模型在未见区域性能下降最小，优于传统计算机视觉和transformer基线模型。
- Conclusion: 多模态大语言模型在领域偏移下具有鲁棒性，为可扩展、可迁移和可解释的全球光伏制图提供了潜力。


### [17] [Studying Maps at Scale: A Digital Investigation of Cartography and the Evolution of Figuration](https://arxiv.org/abs/2511.19538)
*Remi Petitpierre*

Main category: cs.CV

TL;DR: 该研究开发了大规模分析制图遗产的方法和数据集，通过77万+地图记录和9.9万+数字化图像，分析了六个世纪的制图历史，揭示了地图作为文化对象反映政治和认知期望的特征。

- Motivation: 虽然全球遗产机构已数字化超过100万张地图，自动化技术也能大规模识别地图内容，但这些方法很少与制图史结合，忽视了地图作为语义符号系统和文化对象反映政治和认知期望的特性。
- Method: 收集来自38个数字目录的771,561条地图记录和99,715张数字化图像，进行数据标准化；开发语义分割技术和目标检测模型，用于通用土地类别和制图符号识别；分析地理结构、出版时间线、空间焦点与政治动态的关系。
- Result: 数据集包含236,925名贡献者，时间跨度1492-1948年；揭示了制图空间焦点与政治动态的关联（如大西洋海图与三角贸易、殖民扩张的联系）；发现地图是经过设计的图像，通过居中处理和语义对称强调特征；编码了6300万个符号和2500万个片段，揭示了从阴影线到等高线的图形演变。
- Conclusion: 制图遗产分析揭示了地图作为文化对象的重要特征，包括构图设计原则、符号系统的本地一致性，以及合法性、大型参与者和主要城市在图形规范和符号文化传播中的关键作用。


### [18] [Proxy-Free Gaussian Splats Deformation with Splat-Based Surface Estimation](https://arxiv.org/abs/2511.19542)
*Jaeyeong Kim,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.CV

TL;DR: SpLap是一种无需代理的高斯溅射变形方法，基于表面感知溅射图的拉普拉斯算子，避免了传统代理方法的依赖性和计算开销

- Motivation: 现有高斯溅射变形方法依赖笼格或网格等代理，但受限于代理质量和额外计算开销；而直接应用拉普拉斯变形技术又因缺乏显式结构而无法准确捕捉表面信息
- Method: 构建表面感知溅射图，基于溅射间的相交关系而非中心距离定义邻域；引入高斯核自适应技术以在变形中保持表面结构
- Result: 在ShapeNet、Objaverse、Sketchfab和NeRF-Synthetic数据集的50个挑战性对象上，相比基于代理和无代理基线方法表现出优越性能
- Conclusion: SpLap方法通过表面感知溅射图和拉普拉斯算子实现了更真实的变形，能够保持细节和拓扑结构，提高变形后的渲染质量


### [19] [Think First, Assign Next (ThiFAN-VQA): A Two-stage Chain-of-Thought Framework for Post-Disaster Damage Assessment](https://arxiv.org/abs/2511.19557)
*Ehsan Karimi,Nhut Le,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: ThiFAN-VQA是一个用于灾害场景视觉问答的两阶段推理框架，通过链式思维提示和上下文学习生成结构化推理轨迹，再通过答案选择模块评估响应，在无人机图像数据集上实现了优越的准确性和可解释性。

- Motivation: 现有AI灾害评估方法存在数据标注成本高、数据集有限、传统分类框架灵活性不足等问题，而预训练生成模型又容易产生幻觉输出或通用回答，缺乏领域相关性。
- Method: 提出两阶段推理框架：第一阶段使用链式思维提示和上下文学习生成结构化推理轨迹；第二阶段通过答案选择模块评估生成的响应，选择最连贯和上下文准确的答案。
- Result: 在FloodNet和RescueNet-VQA两个无人机数据集上的实验表明，ThiFAN-VQA在准确性、可解释性和适应性方面表现优越。
- Conclusion: ThiFAN-VQA通过集成定制信息检索系统、领域特定提示和推理引导的答案选择，在零样本和监督方法之间架起桥梁，实现了灵活性与一致性的结合。


### [20] [HunyuanOCR Technical Report](https://arxiv.org/abs/2511.19575)
*Hunyuan Vision Team,Pengyuan Lyu,Xingyu Wan,Gengluo Li,Shangpin Peng,Weinong Wang,Liang Wu,Huawen Shen,Yu Zhou,Canhui Tang,Qi Yang,Qiming Peng,Bin Luo,Hower Yang,Houwen Peng,Hongming Yang,Senhao Xie,Binghong Wu,Mana Yang,Sergey Wang,Raccoon Liu,Dick Zhu,Jie Jiang,Linus,Han Hu,Chengquan Zhang*

Main category: cs.CV

TL;DR: HunyuanOCR是一个商业级、开源、轻量级（10亿参数）的视觉语言模型，专门用于OCR任务。它在感知任务（文本定位、解析）和语义任务（信息提取、文本图像翻译）上表现出色，在ICDAR 2025 DIMT挑战赛（小模型赛道）中获得第一名，并在OCRBench上达到SOTA水平。

- Motivation: 解决传统OCR专家模型功能单一和通用VLM效率低下的问题，实现多功能与高效率的统一；通过端到端架构消除对预处理模块的依赖，避免错误传播；探索高质量数据和强化学习策略在OCR任务中的关键作用。
- Method: 采用原生ViT和轻量级LLM通过MLP适配器连接的架构；纯端到端范式，无需布局分析等预处理模块；结合高质量数据和强化学习策略进行训练优化。
- Result: 超越商业API、传统流水线和更大模型（如Qwen3-VL-4B）；在ICDAR 2025 DIMT挑战赛小模型赛道获得第一；在OCRBench上达到参数量小于30亿的VLM中的SOTA水平；提供基于vLLM的高性能部署方案。
- Conclusion: HunyuanOCR在多功能性、效率和性能方面取得突破，为前沿研究和工业应用提供了坚实基础，已开源在HuggingFace上。


### [21] [Leveraging Unlabeled Scans for NCCT Image Segmentation in Early Stroke Diagnosis: A Semi-Supervised GAN Approach](https://arxiv.org/abs/2511.19576)
*Maria Thoma,Michalis A. Savelonas,Dimitris K. Iakovidis*

Main category: cs.CV

TL;DR: 提出一种基于GAN的半监督分割方法，用于在非对比增强CT上准确描绘早期缺血性卒中区域，解决早期缺血变化难以检测的问题。

- Motivation: 缺血性卒中是时间紧迫的医疗紧急情况，但非对比增强CT在早期超急性期往往无法显示细微的缺血变化，这会延迟关键干预措施。
- Method: 使用生成对抗网络(GANs)的半监督分割方法，通过对抗框架从有限标注数据中学习，同时利用大量未标注数据，结合Dice损失、交叉熵损失、特征匹配损失和自训练损失。
- Result: 在公开的急性缺血性卒中数据集(AISD)上的实验表明，该方法能增强诊断能力，减少手动标注负担，支持更高效的卒中临床决策。
- Conclusion: 该方法能够准确识别和描绘早期梗死区域，即使在病灶模糊或体积较小的情况下也能有效工作，有望改善卒中护理的临床决策效率。


### [22] [Multiscale Vector-Quantized Variational Autoencoder for Endoscopic Image Synthesis](https://arxiv.org/abs/2511.19578)
*Dimitrios E. Diamantis,Dimitris K. Iakovidis*

Main category: cs.CV

TL;DR: 本文提出了一种基于VAE的多尺度向量量化变分自编码器(MSVQ-VAE)，用于生成无线胶囊内窥镜(WCE)医学图像，特别是能够条件生成包含各种异常类型的合成图像，以解决医学数据稀缺问题。

- Motivation: 无线胶囊内窥镜产生大量需要人工筛查的图像，但深度学习临床决策支持系统的开发受到医疗数据稀缺的限制，主要源于隐私约束和标注成本。生成式机器学习为解决这一限制提供了可行方案。
- Method: 提出多尺度向量量化变分自编码器(MSVQ-VAE)，能够无缝地将异常引入正常WCE图像中，支持条件生成包含不同类型异常（如息肉、血管和炎症状况）的合成图像。
- Result: 实验表明，使用该方法生成的异常图像训练的临床决策支持分类器，与仅使用真实数据训练的分类器相比，获得了可比较的结果。
- Conclusion: 该方法为医学多媒体领域的各种应用提供了通用性，能够有效解决医学数据稀缺问题，支持临床决策支持系统的开发。


### [23] [SkillSight: Efficient First-Person Skill Assessment with Gaze](https://arxiv.org/abs/2511.19629)
*Chi Hsuan Wu,Kumar Ashutosh,Kristen Grauman*

Main category: cs.CV

TL;DR: SkillSight是一个用于第一人称视角技能评估的系统，通过结合注视数据和视频数据来预测技能水平，最终实现仅使用注视数据的高效技能评估。

- Motivation: 解决智能眼镜上自动技能评估的技术挑战，利用注视数据降低功耗，实现实时技能学习支持。
- Method: 采用两阶段框架：首先联合建模注视和视频数据预测技能水平，然后蒸馏出仅使用注视数据的学生模型。
- Result: 在烹饪、音乐和体育三个数据集上验证了注视在技能理解中的价值，学生模型在保持高精度的同时功耗降低73倍。
- Conclusion: SkillSight为野外AI支持的技能学习铺平了道路，实现了高效且准确的技能评估。


### [24] [On the Utility of Foundation Models for Fast MRI: Vision-Language-Guided Image Reconstruction](https://arxiv.org/abs/2511.19641)
*Ruimin Feng,Xingxin He,Ronald Mercer,Zachary Stewart,Fang Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种基于视觉语言基础模型的语义分布引导MRI重建框架，通过对比目标将重建表示与目标语义分布对齐，在保持数据保真度的同时提升重建质量。

- Motivation: 探索视觉语言基础模型是否能够通过提供超越传统先验的高层上下文信息来增强欠采样MRI重建效果。
- Method: 提出语义分布引导重建框架，使用预训练的视觉语言基础模型将重建图像和辅助信息编码为高层语义特征，通过对比目标确保重建表示与目标语义分布的一致性。
- Result: 在膝关节和脑部数据集上的实验表明，基于图像的语义先验能保留精细解剖结构并实现更优的感知质量（更低的LPIPS值、更高的Tenengrad分数和读者研究评分），图像-语言信息进一步扩展语义分布并实现对重建属性的高层控制。
- Conclusion: 视觉语言基础模型通过语义空间优化能够有效改善欠采样MRI重建效果。


### [25] [Navigating Gigapixel Pathology Images with Large Multimodal Models](https://arxiv.org/abs/2511.19652)
*Thomas A. Buckley,Kian R. Weihrauch,Katherine Latham,Andrew Z. Zhou,Padmini A. Manrai,Arjun K. Manrai*

Main category: cs.CV

TL;DR: GIANT框架让通用大模型能够像病理学家一样迭代导航全切片图像，在病理学图像解读任务中显著优于传统方法，接近或超越专门训练的模型性能。

- Motivation: 现有通用大模型在医学图像解读特别是病理学领域表现不佳，主要因为使用低分辨率缩略图或随机图像块，低估了模型潜力。
- Method: 提出GIANT框架，让LMMs能够迭代导航WSIs；创建MultiPathQA基准，包含934个WSI级别问题和128个病理学家编写的问题。
- Result: GPT-5结合GIANT在病理学家编写问题上达到62.5%准确率，优于TITAN(43.8%)和SlideChat(37.5%)等专业模型。
- Conclusion: 研究揭示了当前基础模型的优势和局限，为LMMs在病理学专家推理领域的未来发展奠定了基础。


### [26] [CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization](https://arxiv.org/abs/2511.19661)
*Xinhai Hou,Shaoyuan Xu,Manan Biyani,Mayan Li,Jia Liu,Todd C. Hollon,Bryan Wang*

Main category: cs.CV

TL;DR: 论文提出CodeV视觉代理和TAPO训练框架，通过代码化视觉工具和过程级强化学习，显著提升了视觉代理使用工具的忠实性，同时保持高准确率。

- Motivation: 当前视觉语言模型在调用图像操作时存在不忠实推理问题——可能在无关区域调用工具或忽略工具输出但仍能猜对答案，这影响了系统的可信度。
- Method: 提出CodeV代码化视觉代理和TAPO训练框架：1) 将视觉工具表示为可执行Python代码；2) 基于GRPO开发过程级RL，直接在视觉工具输入输出上定义密集奖励；3) 采用SFT+RL两阶段训练流程。
- Result: CodeV在视觉搜索基准上实现了竞争性或更优的准确率，同时显著提高了忠实工具使用率。在多种多模态推理和数学基准上也表现强劲。
- Conclusion: 明确监督中间工具行为对于构建可信赖的智能视觉推理系统至关重要，TAPO框架为训练忠实视觉代理提供了有效方法。


### [27] [OncoVision: Integrating Mammography and Clinical Data through Attention-Driven Multimodal AI for Enhanced Breast Cancer Diagnosis](https://arxiv.org/abs/2511.19667)
*Istiak Ahmed,Galib Ahmed,K. Shahriar Sanjid,Md. Tanzim Hossain,Md. Nishan Khan,Md. Misbah Khan,Md. Arifur Rahman,Sheikh Anisul Haque,Sharmin Akhtar Rupa,Mohammed Mejbahuddin Mia,Mahmud Hasan Mostofa Kamal,Md. Mostafa Kamal Sarker,M. Monir Uddin*

Main category: cs.CV

TL;DR: OncoVision是一个结合乳腺X光图像和临床数据的多模态AI系统，通过注意力机制编码器-解码器架构实现四个关键区域的精准分割，并预测十项临床特征，采用晚期融合策略提升诊断精度。

- Motivation: 解决乳腺癌诊断中影像与临床数据融合不足的问题，减少观察者间差异，为医疗资源匮乏地区提供可扩展的筛查方案。
- Method: 使用基于注意力的编码器-解码器架构进行四个ROI（肿块、钙化、腋窝发现、乳腺组织）分割，预测十项临床特征，开发两种晚期融合策略整合多模态数据。
- Result: 实现了最先进的分割精度，通过多模态数据融合提高了诊断精确度，开发了安全易用的Web应用，提供结构化报告和可视化支持。
- Conclusion: OncoVision通过结合精准分割和临床直觉，为基于AI的乳腺X光检查设立了新标准，提供了可扩展、公平的早期乳腺癌检测解决方案。


### [28] [INTERLACE: Interleaved Layer Pruning and Efficient Adaptation in Large Vision-Language Models](https://arxiv.org/abs/2511.19676)
*Parsa Madinei,Ryan Solgi,Ziqi Wen,Jonathan Skaza,Miguel Eckstein,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: INTERLACE是一个新颖的视觉语言模型层剪枝框架，通过样本高效微调在剪枝冗余层的同时保持性能。

- Motivation: 现有的层剪枝方法应用于视觉语言模型时会导致显著的性能下降，需要一种能有效保持性能的剪枝方法。
- Method: 分析连续三层中的局部冗余性，移除前两层中最冗余的层，微调剩余层以补偿损失的能力，并冻结第三层作为微调期间的稳定锚点。
- Result: 仅使用FineVision数据集的1%进行一个epoch的微调，在剪除25%网络层后实现了88.9%的平均性能保持率，达到最先进性能。
- Conclusion: 交错的微调-冻结设计能够在剪枝后使用最少数据实现快速收敛，有效解决了视觉语言模型剪枝的性能保持问题。


### [29] [IndEgo: A Dataset of Industrial Scenarios and Collaborative Work for Egocentric Assistants](https://arxiv.org/abs/2511.19684)
*Vivek Chavan,Yasmina Imgrund,Tung Dao,Sanwantri Bai,Bosong Wang,Ze Lu,Oliver Heimann,Jörg Krüger*

Main category: cs.CV

TL;DR: IndEgo是一个多模态工业任务数据集，包含3,460个第一人称视角和1,092个第三人称视角的录制，专注于协作工业任务，提供丰富的注释和基准测试。

- Motivation: 解决工业环境中协作任务的复杂多模态数据需求，为程序性和非程序性任务理解提供基准。
- Method: 收集了197小时第一人称视角和97小时第三人称视角的工业任务录制，包含眼动追踪、语音、动作等多模态数据，并提供详细注释和预处理输出。
- Result: 数据集包含丰富的多模态数据和注释，基准测试显示对当前最先进的多模态模型具有挑战性。
- Conclusion: IndEgo数据集为工业任务理解提供了有价值的资源，特别是在协作任务和错误检测方面，现有模型在该数据集上表现仍有提升空间。


### [30] [CountXplain: Interpretable Cell Counting with Prototype-Based Density Map Estimation](https://arxiv.org/abs/2511.19686)
*Abdurahman Ali Mohammed,Wallapak Tavanapong,Catherine Fonder,Donald S. Sakaguchi*

Main category: cs.CV

TL;DR: 提出了一种基于原型的方法用于可解释的细胞计数，通过密度图估计实现，在保持计数效果的同时提供模型可解释性。

- Motivation: 生物医学成像中的细胞计数对临床应用至关重要，但深度学习模型在该领域的可解释性仍然是一个重大挑战。
- Method: 在密度估计网络中集成原型层，学习细胞和背景伪影的代表性视觉模式，通过突出显示与每个原型最相似的输入图像区域来生成解释。
- Result: 在两个公共数据集上的广泛实验表明，该方法在不影响计数效果的情况下实现了可解释性，生物学家调查确认了识别出的视觉模式的相关性。
- Conclusion: 为研究人员和临床医生提供了一个透明可靠的细胞计数工具，可能增加信任并加速深度学习在关键生物医学应用中的采用。


### [31] [RADSeg: Unleashing Parameter and Compute Efficient Zero-Shot Open-Vocabulary Segmentation Using Agglomerative Models](https://arxiv.org/abs/2511.19704)
*Omar Alama,Darshil Jariwala,Avigyan Bhattacharya,Seungchan Kim,Wenshan Wang,Sebastian Scherer*

Main category: cs.CV

TL;DR: RADSeg利用RADIO视觉基础模型改进零样本开放词汇语义分割，在mIoU、延迟和参数效率三个关键维度同时提升，以更少的计算和内存成本达到SOTA精度。

- Motivation: 现有开放词汇语义分割方法要么依赖有限的训练数据影响泛化能力，要么使用零样本启发式方法但性能有限，或者组合多个模型导致计算和内存需求过高。
- Method: 通过自相关递归注意力、自相关全局聚合和计算高效的掩码细化来增强RADIO模型性能，提出RADSeg方法。
- Result: RADSeg在基础ViT类别上实现6-30% mIoU提升，速度快3.95倍，参数少2.5倍。RADSeg-base仅105M参数却超越之前850-1350M参数的大模型组合。
- Conclusion: RADSeg在零样本开放词汇语义分割中实现了精度、速度和参数效率的显著提升，以更低的计算成本达到最先进性能。


### [32] [Rethinking Vision Transformer Depth via Structural Reparameterization](https://arxiv.org/abs/2511.19718)
*Chengwei Zhou,Vipin Chaudhary,Gourav Datta*

Main category: cs.CV

TL;DR: 提出基于分支的结构重参数化技术，在训练阶段通过并行分支增强表示能力，然后合并为精简的单路径模型，可将ViT-Tiny从12层压缩到3-6层，保持ImageNet-1K准确率的同时实现移动CPU上37%的推理加速。

- Motivation: 现有ViT加速方法主要关注算法级优化如token剪枝和注意力加速，但忽略了深度架构的根本问题。本文探索能否在减少堆叠层数的同时保持相当的表示能力。
- Method: 在训练阶段使用并行分支的结构重参数化技术，通过在非线性组件入口点逐步合并分支，使FFN和MHSA模块能够进行精确数学重参数化，避免测试时引入近似误差。
- Result: 在ViT-Tiny上成功将12层架构压缩到6、4甚至3层，在ImageNet-1K上保持分类准确率，在移动CPU平台上实现高达37%的推理加速。
- Conclusion: 传统偏好极深transformer堆叠的智慧可能过于保守，为构建高效视觉transformer开辟了新机会。


### [33] [Maritime Small Object Detection from UAVs using Deep Learning with Altitude-Aware Dynamic Tiling](https://arxiv.org/abs/2511.19728)
*Sakib Ahmed,Oscar Pizarro*

Main category: cs.CV

TL;DR: 提出了一种基于高度感知的动态分块方法，通过根据无人机高度动态调整图像分块大小，提高海上小物体检测性能，在保持检测精度的同时显著提升推理速度。

- Motivation: 无人机在海上搜救任务中面临小物体检测困难的问题，由于高空拍摄导致物体与背景像素比低，传统方法检测效果不佳。
- Method: 结合高度依赖的缩放和自适应分块因子，动态调整图像分块大小，减少不必要的计算量。使用YOLOv5和SAHI框架在SeaDronesSee数据集上进行测试。
- Result: 相比基线方法，小物体的平均精度提高了38%，推理速度比静态分块方法提升超过两倍。
- Conclusion: 该方法能够在多样化条件下实现更高效、更准确的无人机搜救操作。


### [34] [Efficient Transferable Optimal Transport via Min-Sliced Transport Plans](https://arxiv.org/abs/2511.19741)
*Xinran Liu,Elaheh Akbari,Rocio Diaz Martin,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.CV

TL;DR: 本文研究了切片最优传输(min-STP)框架中优化切片的可转移性，证明在数据分布轻微扰动下，优化切片能有效迁移到新分布对，并提出了小批量min-STP以提高可扩展性。

- Motivation: 切片传输方法虽能降低计算成本，但未解决优化切片在分布偏移下的可转移性问题，这在数据演化或相关分布重复计算中至关重要。
- Method: 提出min-Sliced Transport Plan框架，研究优化切片的可转移性，引入小批量min-STP并给出统计精度保证。
- Result: 理论证明优化切片在数据分布轻微扰动下保持稳定，实证显示可转移min-STP在点云对齐和流生成建模中实现强一次性匹配性能。
- Conclusion: 优化切片具有可转移性，min-STP框架能在相关任务间高效迁移，为处理演化数据提供了有效工具。


### [35] [Leveraging Foundation Models for Histological Grading in Cutaneous Squamous Cell Carcinoma using PathFMTools](https://arxiv.org/abs/2511.19751)
*Abdul Rahman Diab,Emily E. Karn,Renchin Wu,Emily S. Ruiz,William Lotter*

Main category: cs.CV

TL;DR: PathFMTools是一个轻量级Python包，用于高效执行、分析和可视化病理学基础模型，在皮肤鳞状细胞癌组织学分级任务中评估CONCH和MUSK模型，展示了基础模型嵌入在临床应用中的潜力。

- Motivation: 计算病理学基础模型在适应特定临床任务时面临挑战，包括全切片图像处理的复杂性、学习特征的不透明性以及广泛的适应策略选择。
- Method: 开发PathFMTools工具包，用于与CONCH和MUSK两个最先进的视觉语言基础模型交互，并在440个cSCC H&E全切片图像上评估多种适应策略。
- Result: 基准测试展示了不同预测方法之间的权衡，验证了使用基础模型嵌入训练小型专家模型的潜力。
- Conclusion: 病理学基础模型在现实世界临床应用中具有前景，PathFMTools能够实现高效的分析和验证。


### [36] [What You See is (Usually) What You Get: Multimodal Prototype Networks that Abstain from Expensive Modalities](https://arxiv.org/abs/2511.19752)
*Muchang Bahng,Charlie Berens,Jon Donnelly,Eric Chen,Chaofan Chen,Cynthia Rudin*

Main category: cs.CV

TL;DR: 提出了一种多模态原型网络，通过集成图像和基因数据原型，智能分配昂贵基因数据，在保持可解释性的同时实现成本感知的物种检测。

- Motivation: 解决多模态神经网络在物种检测中的两个主要问题：黑盒决策过程缺乏可解释性，以及基因数据收集成本高昂且需要侵入性操作。
- Method: 扩展原型网络到多模态成本感知设置，集成各模态原型并使用权重确定预测对各模态的依赖程度，引入方法识别无需昂贵基因信息即可做出自信预测的情况。
- Result: 能够智能分配昂贵基因数据用于细粒度区分，同时使用丰富的图像数据进行清晰视觉分类，达到与始终使用双模态模型相当的准确率。
- Conclusion: 该方法在保持可解释性的同时，有效解决了基因数据收集成本问题，为物种检测提供了实用的多模态解决方案。


### [37] [Vision--Language Enhanced Foundation Model for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2511.19759)
*Jiaqi Guo,Mingzhen Li,Hanyu Su,Santiago López,Lexiaozi Fan,Daniel Kim,Aggelos Katsaggelos*

Main category: cs.CV

TL;DR: VESSA是一个结合视觉语言模型和半监督学习的医学图像分割框架，通过两阶段方法在有限标注条件下显著提升分割精度

- Motivation: 结合视觉语言模型的泛化能力和半监督学习减少对专家标注的依赖，解决医学图像分割中标注数据稀缺的问题
- Method: 两阶段方法：第一阶段训练VESSA作为参考引导的分割助手，使用模板库进行视觉特征匹配；第二阶段将VESSA集成到半监督学习框架中，实现与学生模型的动态交互
- Result: 在多个分割数据集和领域的广泛实验中，VESSA增强的半监督学习显著提高了分割精度，在极有限标注条件下优于最先进的基线方法
- Conclusion: VESSA成功将基础级视觉语义理解整合到半监督学习框架中，为医学图像分割提供了有效的解决方案


### [38] [A Storage-Efficient Feature for 3D Concrete Defect Segmentation to Replace Normal Vector](https://arxiv.org/abs/2511.19760)
*Linxin Hua,Jianghua Deng,Ye Lu*

Main category: cs.CV

TL;DR: 提出了一种新的特征"相对角度"，用于点云损伤重建，能在保持与法向量相似性能的同时，显著减少存储需求和输入通道数。

- Motivation: 点云损伤重建能有效解决基于图像的方法易受背景噪声影响的问题，但受限于3D数据的高体积。需要找到既能提供方向性信息又更紧凑的特征表示。
- Method: 提出相对角度特征，计算为点的法向量与其父点云平均法向量之间的夹角。通过基于熵的特征评估来过滤无损区域的冗余信息，保留损伤区域的有效信息。使用PointNet++进行训练和测试。
- Result: 基于相对角度的模型实现了与基于法向量的模型相似的性能，同时存储需求减少27.6%，输入通道压缩83%。
- Conclusion: 相对角度这一新颖特征有潜力在资源受限的硬件上实现更大批次的执行，且无需对模型架构进行修改。


### [39] [Lightweight Transformer Framework for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2511.19765)
*Ali Torabi,Sanjog Gaihre,Yaqoob Majeed*

Main category: cs.CV

TL;DR: CrispFormer通过三个关键改进增强SegFormer解码器：边界分支监督物体轮廓、不确定性引导的细化器处理噪声标签、动态多尺度融合层优化特征融合，在弱监督语义分割中显著提升边界精度和小目标召回率。

- Motivation: 弱监督语义分割需要从噪声和不完整的监督信号中学习密集掩码，现有方法在边界保持、噪声鲁棒性和多尺度特征融合方面存在不足。
- Method: 在SegFormer解码器中添加三个组件：(1)边界分支使用轻量级边缘头和边界感知损失；(2)不确定性引导细化器预测像素级不确定性并用于加权损失和残差校正；(3)动态多尺度融合层用空间softmax门控替代静态拼接。
- Result: 在标准WSSS流程中，CrispFormer相比SegFormer基线持续提升边界F分数、小目标召回率和mIoU，同时仅增加少量计算开销。
- Conclusion: CrispFormer提供了一种简单可实现的解码器中心化方案，与现有SegFormer变体兼容，为从图像级监督获得更高保真度掩码提供了可复现路径。


### [40] [Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering](https://arxiv.org/abs/2511.19768)
*Noah Frahm,Prakrut Patel,Yue Zhang,Shoubin Yu,Mohit Bansal,Roni Sengupta*

Main category: cs.CV

TL;DR: Prune-Then-Plan框架通过步骤级校准解决大型视觉语言模型在具身问答中的边界振荡问题，使用修剪-规划方法稳定探索过程。

- Motivation: 大型视觉语言模型在具身问答中直接用于步骤级探索时会出现边界振荡问题，导致导航效率低下和答案质量下降。
- Method: 提出Prune-Then-Plan框架：首先使用Holm-Bonferroni启发式修剪程序剔除不可信的边界选择，然后将最终决策委托给基于覆盖率的规划器。
- Result: 在3D-Mem EQA框架中集成该方法，视觉基础SPL和LLM-Match指标分别相对基线提升49%和33%，在相同探索预算下获得更好的场景覆盖。
- Conclusion: 该方法通过将过度自信的预测转换为保守、可解释的动作，有效校准了视觉语言模型的步骤级行为，提高了具身问答的性能。


### [41] [One Attention, One Scale: Phase-Aligned Rotary Positional Embeddings for Mixed-Resolution Diffusion Transformer](https://arxiv.org/abs/2511.19778)
*Haoyu Wu,Jingyi Xu,Qiaomu Miao,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 本文发现扩散变换器中旋转位置编码(RoPE)的线性插值在混合分辨率去噪时会导致注意力机制崩溃，并提出Cross-Resolution Phase-Aligned Attention (CRPA)作为训练即用的解决方案。

- Motivation: 当不同空间网格的token混合时，线性坐标重映射导致注意力头在不相容的采样率下比较RoPE相位，产生相位混叠，破坏分数景观。预训练的DiT对此特别脆弱。
- Method: 提出CRPA方法，修改每个注意力调用的RoPE索引映射：所有Q/K位置都基于查询的步长表达，确保相同物理距离始终产生相同的相位增量。
- Result: CRPA恢复了DiT依赖的精确相位模式，稳定所有头和层，在图像和视频生成中优于现有最先进方法。
- Conclusion: CRPA解决了RoPE在混合分辨率生成中的结构性问题，无需重新训练即可实现高保真和高效的混合分辨率生成。


### [42] [Reading Between the Lines: Abstaining from VLM-Generated OCR Errors via Latent Representation Probes](https://arxiv.org/abs/2511.19806)
*Jihan Yao,Achin Kulshrestha,Nathalie Rauschmayr,Reed Roberts,Banghua Zhu,Yulia Tsvetkov,Federico Tombari*

Main category: cs.CV

TL;DR: 提出LRP方法，通过分析VLM内部表征来检测不确定性，在STVQA任务中实现比基线方法高7.6%的弃权准确率。

- Motivation: VLM在安全关键应用中需要能够在不确信时弃权回答，特别是在OCR错误可能导致严重后果的STVQA任务中。现有弃权方法要么依赖错误校准的输出概率，要么使用不适合OCR任务的语义一致性方法。
- Method: 提出潜在表征探测(LRP)：在隐藏状态或注意力模式上训练轻量级探测器。探索三种探测器设计：跨层连接表征、聚合视觉标记的注意力、通过多数投票集成单层探测器。
- Result: 在四个图像和视频基准测试中，LRP比最佳基线方法提高了7.6%的弃权准确率。探测器能够泛化到各种不确定性来源和数据集，最优信号来自中间层而非最终层。
- Conclusion: 建立了一个基于内部状态而非不可靠输出来检测置信度信号的原则性框架，为构建可部署的AI系统提供了基础。


### [43] [Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization](https://arxiv.org/abs/2511.19811)
*Debin Meng,Chen Jin,Zheng Gao,Yanran Li,Ioannis Patras,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 提出了TPSO方法，通过优化token嵌入空间来提升文本到图像扩散模型的生成多样性，避免模型重复生成强模式样本，同时保持图像质量。

- Motivation: 文本到图像扩散模型存在生成多样性不足的问题，容易重复生成相似输出，限制了创意探索和下游应用。现有方法往往仍会陷入主导模式或导致图像质量下降。
- Method: 提出TPSO方法，在token嵌入空间中引入可学习参数来探索欠表示区域，减少模型重复生成强模式样本的趋势，同时通过提示级空间提供全局语义约束来调节分布偏移。
- Result: 在MS-COCO数据集和三个扩散骨干网络上的实验表明，TPSO显著提升了生成多样性，将基线性能从1.10提高到4.18分，且不牺牲图像质量。
- Conclusion: TPSO是一种无需训练、模型无关的模块，能有效解决文本到图像扩散模型的多样性问题，在保持高保真度的同时显著提升生成多样性。


### [44] [CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception](https://arxiv.org/abs/2511.19820)
*Miguel Carvalho,Helder Dias,Bruno Martins*

Main category: cs.CV

TL;DR: Error

- Motivation: Error
- Method: Error
- Result: Error
- Conclusion: Error


### [45] [ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding](https://arxiv.org/abs/2511.19827)
*Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: ReDirector是一种新颖的相机控制视频重拍生成方法，通过改进RoPE在时空位置对齐和引入Rotary Camera Encoding，实现了对动态捕获变长视频的精确控制。

- Motivation: 解决先前工作中RoPE的误用问题，改进输入视频和目标重拍之间的时空位置对齐，并处理多视角关系，以提升相机控制性和几何一致性。
- Method: 提出Rotary Camera Encoding (RoCE)，一种相机条件的RoPE相位偏移，捕获并整合输入和目标视频内部及跨视频的多视角关系，将相机条件集成到RoPE中。
- Result: 方法能够泛化到分布外的相机轨迹和视频长度，显著改善了动态对象定位和静态背景保持，在相机可控性、几何一致性和视频质量方面都有显著提升。
- Conclusion: ReDirector通过改进的RoPE应用和相机条件编码，在视频重拍生成任务中实现了更好的相机控制、几何一致性和视频质量，适用于各种轨迹和长度。


### [46] [Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19834)
*Haoqing Li,Jun Shi,Xianmeng Chen,Qiwei Jia,Rui Wang,Wei Wei,Hong An,Xiaowen Hu*

Main category: cs.CV

TL;DR: 提出BHD-RAG框架，通过整合多模态检索增强生成技术，结合领域专业知识解决Birt-Hogg-Dube综合征诊断中样本少、类别差异小的挑战。

- Motivation: 解决深度学习在罕见病BHD诊断中面临的两个主要挑战：临床样本有限和弥漫性囊性肺疾病类别间差异小，同时减少多模态大语言模型的幻觉风险。
- Method: 采用三阶段方法：1) 专门代理生成CT图像描述构建多模态语料库；2) 基于余弦相似度的检索器匹配相关图像-描述对；3) MLLM结合检索证据和图像数据进行诊断。
- Result: 在包含四种弥漫性囊性肺疾病的数据集上验证，取得了优越的准确率，生成的基于证据的描述与专家见解高度一致。
- Conclusion: BHD-RAG框架通过整合领域专业知识和临床先例，有效提高了BHD诊断的准确性，为罕见病诊断提供了可行的解决方案。


### [47] [Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation](https://arxiv.org/abs/2511.19835)
*Xuewen Liu,Zhikai Li,Jing Zhang,Mengjuan Chen,Qingyi Gu*

Main category: cs.CV

TL;DR: 提出了Rectified SpaAttn方法，通过隐式全注意力参考修正注意力分配，解决现有稀疏注意力方法中的系统偏差问题，在保持生成质量的同时实现显著加速。

- Motivation: 扩散变换器在视频生成中占主导地位，但注意力计算的二次复杂度带来显著延迟。现有稀疏注意力方法虽然能降低计算成本，但存在严重的性能下降问题。
- Method: 提出Rectified SpaAttn方法：1）对关键token使用孤立池化注意力重分配，通过重分配多模态池化权重计算准确修正因子；2）对非关键token使用增益感知池化修正，确保修正增益始终超过引入的误差。
- Result: 在HunyuanVideo和Wan 2.1上分别实现了3.33倍和2.08倍的加速，同时保持了高质量的生成效果。
- Conclusion: Rectified SpaAttn有效解决了稀疏注意力中的系统偏差问题，在显著加速的同时保持了生成质量，为视频生成提供了高效的解决方案。


### [48] [4DWorldBench: A Comprehensive Evaluation Framework for 3D/4D World Generation Models](https://arxiv.org/abs/2511.19836)
*Yiting Lu,Wei Luo,Peiyan Tu,Haoran Li,Hanxin Zhu,Zihao Yu,Xingrui Wang,Xinyi Chen,Xinge Peng,Xin Li,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出了4DWorldBench基准测试，用于系统评估世界生成模型在感知质量、条件-4D对齐、物理真实性和4D一致性四个关键维度的表现，覆盖图像/视频/文本到3D/4D的生成任务。

- Motivation: 传统基准测试缺乏对世界生成模型世界真实感能力的统一评估，而世界模型需要构建具有空间、时间、物理一致性的动态3D/4D世界。
- Method: 引入自适应多模态条件评估框架，将所有模态条件映射到统一的文本空间，并整合LLM-as-judge、MLLM-as-judge和传统网络方法进行综合评估。
- Result: 初步人类研究表明，自适应工具选择与主观人类判断更接近，基准测试为客观比较和改进提供了基础。
- Conclusion: 4DWorldBench将加速从"视觉生成"到"世界生成"的转变，为下一代多模态智能系统提供统一评估标准。


### [49] [Face, Whole-Person, and Object Classification in a Unified Space Via The Interleaved Multi-Domain Identity Curriculum](https://arxiv.org/abs/2511.19846)
*Thomas M Metz,Matthew Q Hill,Alice J O'Toole*

Main category: cs.CV

TL;DR: 提出IMIC方法，在单一嵌入空间中同时处理物体识别、高低质量人脸识别和全身人物识别四个任务，避免灾难性遗忘，保持基础模型的泛化能力。

- Motivation: 解决基础模型在微调时出现的灾难性遗忘问题，同时实现多任务处理而不牺牲性能。
- Method: 引入IMIC训练策略，在DINOv3、CLIP和EVA-02基础上同时微调四个任务，使用梯度耦合的交错训练计划。
- Result: EVA-02和CLIP模型在所有四个任务上表现与领域专家相当，在跨人脸、身体和物体数据集的多任务处理中比人类更准确。
- Conclusion: IMIC方法有效避免了灾难性遗忘，四个任务在统一嵌入空间中线性可分但特征共享，少量主成分即可完成所有任务。


### [50] [DOGE: Differentiable Bezier Graph Optimization for Road Network Extraction](https://arxiv.org/abs/2511.19850)
*Jiahui Sun,Junran Lu,Jinhui Yin,Yishuo Xu,Yuanqi Li,Yanwen Guo*

Main category: cs.CV

TL;DR: 提出DOGE框架，使用可微分贝塞尔图直接从分割掩码学习道路网络，无需曲线标注，在SpaceNet和CityScale基准上达到新SOTA

- Motivation: 现有道路提取方法依赖折线难以建模曲线几何，道路几何本质上是基于曲线的，但向量标注难以构建
- Method: 将任务重构为贝塞尔图上的全局优化问题，通过DiffAlign模块进行可微分渲染优化几何，TopoAdapt模块使用离散算子优化拓扑结构
- Result: 在SpaceNet和CityScale基准测试中达到新的最先进水平
- Conclusion: 提出了生成高保真道路网络矢量地图的新范式


### [51] [STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction](https://arxiv.org/abs/2511.19854)
*Jiankuo Zhao,Xiangyu Zhu,Zidu Wang,Zhen Lei*

Main category: cs.CV

TL;DR: STAvatar是一个从单目视频重建高保真可动画3D头部头像的方法，通过UV自适应软绑定和时序自适应密度控制解决现有方法在运动刚性和遮挡区域处理上的局限性。

- Motivation: 现有基于3D高斯溅射的方法通常将高斯绑定到网格三角形上，仅通过线性混合蒙皮建模变形，导致刚性运动和表达能力有限，且缺乏处理频繁遮挡区域（如口腔内部、眼睑）的专门策略。
- Method: 提出两个关键组件：(1) UV自适应软绑定框架，利用图像和几何先验学习UV空间中每个高斯特征偏移；(2) 时序ADC策略，首先聚类结构相似帧以更针对性地计算致密化标准，并引入融合感知误差作为克隆标准联合捕捉几何和纹理差异。
- Result: 在四个基准数据集上的广泛实验表明，STAvatar实现了最先进的重建性能，特别是在捕捉细粒度细节和重建频繁遮挡区域方面表现优异。
- Conclusion: STAvatar通过创新的UV自适应绑定和时序密度控制策略，显著提升了3D头部头像的重建质量和动画表达能力。


### [52] [Temporal-Visual Semantic Alignment: A Unified Architecture for Transferring Spatial Priors from Vision Models to Zero-Shot Temporal Tasks](https://arxiv.org/abs/2511.19856)
*Xiangkai Ma,Han Zhang,Wenzhong Li,Sanglu Lu*

Main category: cs.CV

TL;DR: TimeArtist是一个时间-视觉转换框架，通过语义级对齐实现从时间序列到高质量图像的生成，同时捕捉时间波动模式进行风格迁移。

- Motivation: 现有方法将时间序列转换为"伪图像"进行时间预测，但未能建立语义级对齐；探索非视觉连续序列作为高保真图像生成条件信号的潜力。
- Method: 采用"预热-对齐"范式：首先通过双自编码器和共享量化器在大规模数据集上进行自监督训练学习模态共享表示，然后冻结编码器和量化器，引入投影在表示层面对齐时间和视觉样本。
- Result: 在图像生成指标上取得满意性能，在零样本时间任务中获得优异结果，能够直接从时间序列生成高质量、多样化的图像。
- Conclusion: TimeArtist建立了跨模态生成的新范式，弥合了时间动态和视觉语义之间的差距。


### [53] [GigaWorld-0: World Models as Data Engine to Empower Embodied AI](https://arxiv.org/abs/2511.19861)
*GigaWorld Team,Angen Ye,Boyuan Wang,Chaojun Ni,Guan Huang,Guosheng Zhao,Haoyun Li,Jiagang Zhu,Kerui Li,Mengyuan Xu,Qiuping Deng,Siting Wang,Wenkang Qin,Xinze Chen,Xiaofeng Wang,Yankai Wang,Yu Cao,Yifan Chang,Yuan Xu,Yun Ye,Yang Wang,Yukun Zhou,Zhengyuan Zhang,Zhehao Dong,Zheng Zhu*

Main category: cs.CV

TL;DR: GigaWorld-0是一个统一的世界模型框架，通过视频生成和3D建模的协同优化，为视觉-语言-动作学习生成高质量、多样化的具身交互数据，使VLA模型在零真实世界交互训练下实现强大的物理机器人性能。

- Motivation: 世界模型正在成为可扩展、数据高效的具身AI基础范式，需要解决具身交互数据的生成问题，以支持VLA模型的训练。
- Method: 包含两个组件：GigaWorld-0-Video通过大规模视频生成产生多样化的具身序列；GigaWorld-0-3D结合3D生成建模、高斯溅射重建、物理系统识别和运动规划确保几何一致性和物理真实性。通过GigaTrain框架进行高效训练。
- Result: GigaWorld-0生成了高质量、多样化且可控的数据，基于该数据训练的VLA模型（如GigaBrain-0）在物理机器人上实现了强大的真实世界性能，显著提高了泛化能力和任务成功率。
- Conclusion: GigaWorld-0为具身AI提供了一个有效的数据引擎，证明了合成数据可以支持VLA模型在零真实世界交互训练下实现优秀的机器人性能。


### [54] [MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization](https://arxiv.org/abs/2511.19878)
*Chengyue Huang,Mellon M. Zhang,Robert Azarcon,Glen Chou,Zsolt Kira*

Main category: cs.CV

TL;DR: MAPS是一个用于视觉-语言-动作模型的微调框架，通过模块级邻近调度来平衡稳定性和灵活性，在保持预训练先验的同时提升泛化性能。

- Motivation: 现有VLA模型微调方法要么过度约束适应，要么忽略不同组件的角色差异，导致破坏预训练表示并损害泛化能力。
- Method: MAPS通过系统分析发现模块约束松弛的实证顺序，并线性调度这种松弛，使视觉编码器保持接近预训练先验，而面向动作的语言层更自由地适应。
- Result: 在多个基准测试和真实世界评估中，MAPS一致提升分布内和分布外性能（最高+30%），无需额外参数或数据。
- Conclusion: 经验指导的与预训练VLM的邻近性是一个简单而强大的原则，可在VLM到VLA迁移中保持广泛泛化能力。


### [55] [ChessMamba: Structure-Aware Interleaving of State Spaces for Change Detection in Remote Sensing Images](https://arxiv.org/abs/2511.19882)
*Lei Ding,Tong Liu,Xuanguang Liu,Xiangyun Liu,Haitao Guo,Jun Lu*

Main category: cs.CV

TL;DR: ChessMamba是一个用于多时相遥感图像变化检测的结构感知框架，通过棋盘交错和状态空间建模解决异构性和时空不对齐问题。

- Motivation: 现有的基于视觉变换器或状态空间模型的方法在时间序列化过程中破坏了局部结构一致性，在不对齐情况下掩盖了判别性线索，阻碍了可靠的变化定位。
- Method: ChessMamba集成了SpatialMamba编码器和轻量级跨源交互模块，采用棋盘交错与蛇形扫描顺序将多时相特征序列化，并通过多扩张卷积进行结构感知融合。
- Result: 在三个变化检测任务上的综合评估表明，ChessMamba能有效融合异构特征，相比最先进方法实现了显著的精度提升。
- Conclusion: ChessMamba通过结构感知的状态空间建模，在多时相遥感图像变化检测中实现了鲁棒的变化定位和异构特征融合。


### [56] [Distilling Cross-Modal Knowledge via Feature Disentanglement](https://arxiv.org/abs/2511.19887)
*Junhong Liu,Yuan Zhang,Tao Huang,Wenchao Xu,Renyu Yang*

Main category: cs.CV

TL;DR: 提出频率解耦的跨模态知识蒸馏方法，通过分离低频和高频特征来改进跨模态知识迁移效果

- Motivation: 传统知识蒸馏在跨模态场景（如视觉到语言）中效果不佳，因为不同模态间的表示不一致导致知识迁移困难
- Method: 利用频域特征解耦知识迁移：低频特征强制强对齐，高频特征采用松弛对齐，并引入尺度一致性损失和共享分类器
- Result: 在多个基准数据集上的实验表明，该方法显著优于传统知识蒸馏和现有最先进的跨模态知识蒸馏方法
- Conclusion: 频率解耦的跨模态知识蒸馏方法有效解决了跨模态知识迁移中的表示不一致问题，显著提升了蒸馏效果


### [57] [LiMT: A Multi-task Liver Image Benchmark Dataset](https://arxiv.org/abs/2511.19889)
*Zhe Liu,Kai Han,Siqi Ma,Yan Zhu,Jun Chen,Chongwen Lyu,Xinyi Qiu,Chengxuan Qian,Yuqing Song,Yi Liu,Liyuan Tian,Yang Ji,Yuefeng Li*

Main category: cs.CV

TL;DR: 构建了一个多任务肝脏数据集LiMT，用于肝脏和肿瘤分割、多标签病变分类和病变检测，基于动脉期增强CT，包含150个病例的CT体积数据，涵盖四种肝病和正常病例。

- Motivation: 现有CAD数据集通常只支持单一任务，限制了CAD技术的发展，需要多任务数据集来探索任务间相关性并避免任务特定数据集间的异质性。
- Method: 基于动脉期增强CT构建多任务肝脏数据集，包含150个不同病例的CT体积，由经验丰富的临床医生仔细标注和校准，涵盖四种肝病类型和正常病例。
- Result: 提供了公开的多任务数据集LiMT，包含基线实验结果，并回顾了现有肝脏相关任务的数据集和方法。
- Conclusion: LiMT数据集可能成为医学影像研究社区的宝贵资源，为探索任务间相关性提供了解决方案。


### [58] [VeriSciQA: An Auto-Verified Dataset for Scientific Visual Question Answering](https://arxiv.org/abs/2511.19899)
*Yuyi Li,Daoyuan Chen,Zhen Wang,Yutong Lu,Yaliang Li*

Main category: cs.CV

TL;DR: 提出了一个生成-验证框架来创建高质量的Scientific Visual Question Answering数据集VeriSciQA，包含20,351个QA对，覆盖20个科学领域和12种图表类型。

- Motivation: 开源大视觉语言模型在科学视觉问答任务上表现不佳，主要瓶颈在于缺乏大规模、高质量的公开数据集。现有方法使用LVLMs合成数据存在系统性错误。
- Method: 采用验证中心的生成-验证框架：首先生成带有关联文本上下文的QA对，然后通过跨模态一致性检查和辅助过滤器消除错误对。
- Result: 创建的VeriSciQA数据集对开源模型具有挑战性，领先开源模型准确率64%，而专有模型达82%。在VeriSciQA上微调的模型在SVQA基准测试中表现一致提升，性能随数据规模扩大而增强。
- Conclusion: 该框架可扩展的数据扩展方法能进一步推动开源社区在科学视觉问答能力上的进步。


### [59] [Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning](https://arxiv.org/abs/2511.19900)
*Jiaqi Liu,Kaiwen Xiong,Peng Xia,Yiyang Zhou,Haonian Ji,Lu Feng,Siwei Han,Mingyu Ding,Huaxiu Yao*

Main category: cs.CV

TL;DR: Agent0-VL是一个自演化的视觉语言智能体，通过工具集成推理实现持续自我改进，无需外部奖励或人工标注。

- Motivation: 克服视觉语言智能体依赖人工标注监督的限制，解决纯文本自评估在复杂视觉推理中的幻觉问题。
- Method: 在单一LVLM中统一两个协同角色：执行多轮工具集成推理的Solver，以及通过工具批判生成结构化反馈和细粒度自奖励的Verifier。通过自演化推理循环实现稳定自我改进。
- Result: 在几何问题求解和视觉科学分析任务上，相比基础模型实现了12.5%的性能提升。
- Conclusion: Agent0-VL通过工具集成的自评估和自修复机制，实现了无需外部奖励的持续自我改进，有效对齐了推理和验证行为。


### [60] [MHB: Multimodal Handshape-aware Boundary Detection for Continuous Sign Language Recognition](https://arxiv.org/abs/2511.19907)
*Mingyu Zhao,Zhanfu Yang,Yang Zhou,Zhaoyang Xia,Can Jin,Xiaoxiao He,Carol Neidle,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 提出了一种用于连续手语识别的多模态方法，结合骨骼特征和手形分类来检测手语边界，并在ASLLRP语料库上取得了显著改进

- Motivation: 提高连续手语识别的鲁棒性，通过检测手语边界和利用手形信息来改善识别效果
- Method: 使用3D骨骼特征捕捉手语动态特性，预训练87个手形分类器，通过多模态融合模块结合视频分割和手形分类，利用包含孤立手语和连续手语的大型数据库训练识别模型
- Result: 在ASLLRP语料库上评估，相比先前工作有显著改进
- Conclusion: 多模态方法结合骨骼特征和手形信息能有效提高连续手语识别的性能


### [61] [Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance](https://arxiv.org/abs/2511.19909)
*Haoxuan Wang,Jiachen Tao,Junyi Wu,Gaowen Liu,Ramana Rao Kompella,Yan Yan*

Main category: cs.CV

TL;DR: Motion Marionette是一个零样本框架，用于从单目源视频到单视图目标图像的刚性运动迁移。它通过内部先验而非外部先验来指导迁移过程，避免了通用性和时间一致性之间的权衡。

- Motivation: 现有方法通常使用几何、生成或模拟先验来指导运动迁移，但这些外部先验引入了辅助约束，导致通用性和时间一致性之间的权衡。
- Method: 首先将源视频和目标图像提升到统一的3D表示空间，从源视频提取运动轨迹构建空间-时间先验，该先验独立于对象几何和语义。然后将其与目标对象集成生成可控速度场，并使用基于位置的动力学进行细化。
- Result: 实验结果表明，Motion Marionette能够泛化到不同对象，产生与源运动对齐的时间一致视频，并支持可控视频生成。
- Conclusion: 该方法通过内部空间-时间先验实现了有效的刚性运动迁移，在保持时间一致性的同时具有良好的通用性。


### [62] [Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving](https://arxiv.org/abs/2511.19912)
*Dapeng Zhang,Zhenlong Yuan,Zhangquan Chen,Chih-Ting Liao,Yinda Chen,Fei Shen,Qingguo Zhou,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了Reasoning-VLA框架，通过可学习的动作查询和推理增强的视觉语言特征来并行生成连续动作轨迹，在自动驾驶决策中实现高效推理和强泛化能力。

- Motivation: 现有的VLA模型在自动驾驶中推理效率低，难以泛化到新的车辆配置和驾驶场景，需要更通用和快速的动作生成框架。
- Method: 使用高斯采样从训练数据中初始化可学习动作查询，与推理增强的视觉语言特征交互，并行生成连续动作轨迹；整合8个公开数据集为标准化格式，结合监督学习和强化学习微调。
- Result: 在多个基准测试中达到最先进性能，具有优越的泛化能力和迄今为止报道的最佳推理速度。
- Conclusion: Reasoning-VLA框架在自动驾驶决策中实现了高效推理、强泛化能力和最先进的性能表现。


### [63] [Coupled Physics-Gated Adaptation: Spatially Decoding Volumetric Photochemical Conversion in Complex 3D-Printed Objects](https://arxiv.org/abs/2511.19913)
*Maryam Eftekharifar,Churun Zhang,Jialiang Wei,Xudong Cao,Hossein Heidari*

Main category: cs.CV

TL;DR: 提出了一个预测3D打印物体光化学转化的框架，通过C-PGA架构从3D视觉数据预测非视觉体积物理性质，实现虚拟化学表征。

- Motivation: 传统视觉模型缺乏对光学物理和材料物理耦合非线性相互作用的归纳偏差，无法从3D视觉数据准确预测光化学转化过程。
- Method: 提出C-PGA多模态融合架构，使用稀疏几何和工艺参数作为Query，通过FiLM动态门控和适应密集视觉特征，并行处理原始投影堆栈及其扩散-衍射校正对应物。
- Result: 该方法在虚拟化学表征方面取得突破，无需传统后打印测量即可精确控制化学转化状态。
- Conclusion: C-PGA架构成功建模了物理耦合，为从3D视觉数据预测复杂物理过程提供了新解决方案。


### [64] [Scale Where It Matters: Training-Free Localized Scaling for Diffusion Models](https://arxiv.org/abs/2511.19917)
*Qin Ren,Yufei Wang,Lanqing Guo,Wen Zhang,Zhiwen Fan,Chenyu You*

Main category: cs.CV

TL;DR: LoTTS是一种无需训练、基于局部区域优化的扩散模型推理时缩放方法，通过识别和修复图像中的缺陷区域，在保持高质量区域的同时显著降低计算成本。

- Motivation: 现有的推理时缩放方法在全图像层面操作，忽略了图像质量的空间异质性，导致对已满意区域进行不必要计算，同时对局部缺陷修复不足。
- Method: LoTTS通过对比交叉注意力和自注意力信号来定位缺陷区域，然后仅对这些区域进行局部扰动和去噪，保持全局一致性。
- Result: 在SD2.1、SDXL和FLUX上的实验表明，LoTTS在提升局部质量和全局保真度的同时，将GPU成本相比Best-of-N采样降低了2-4倍。
- Conclusion: 局部推理时缩放是扩散模型推理时缩放的一个有前景的新方向，能够在保证质量的同时显著降低计算开销。


### [65] [HybriDLA: Hybrid Generation for Document Layout Analysis](https://arxiv.org/abs/2511.19919)
*Yufan Chen,Omar Moured,Ruiping Liu,Junwei Zheng,Kunyu Peng,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: HybriDLA是一个新颖的文档布局分析框架，结合了扩散和自回归解码，在单层中统一处理，通过多尺度特征融合编码器提升检测质量，在DocLayNet和M$^6$Doc基准测试中达到83.5% mAP的SOTA性能。

- Motivation: 传统文档布局分析方法依赖经验先验或固定数量的可学习查询，在处理现代文档时面临挑战，因为现代文档具有多样化的元素数量和复杂的布局结构。
- Method: 提出HybriDLA框架，在单层中统一扩散和自回归解码：扩散组件迭代优化边界框假设，自回归组件注入语义和上下文感知；设计多尺度特征融合编码器捕获细粒度和高层次视觉线索。
- Result: 在DocLayNet和M$^6$Doc基准测试中达到83.5%的平均精度(mAP)，超越了先前的方法，建立了最先进的性能。
- Conclusion: HybriDLA通过结合扩散和自回归解码的统一框架，有效解决了现代文档布局分析的挑战，为复杂文档布局分析提供了有效的解决方案。


### [66] [Intelligent Image Search Algorithms Fusing Visual Large Models](https://arxiv.org/abs/2511.19920)
*Kehan Wang,Tingqiong Cui,Yang Zhang,Yu Chen,Shifeng Wu,Zhenzhang Li*

Main category: cs.CV

TL;DR: 提出了DetVLM框架，将目标检测与视觉大模型融合，实现细粒度图像检索，支持状态搜索和零样本搜索，在车辆组件数据集上达到94.82%的检索准确率。

- Motivation: 解决细粒度图像检索中的关键问题：传统手动特征缺乏鲁棒性；深度学习检测器无法进行状态特定检索或零样本搜索；视觉大模型虽有语义和零样本能力但空间定位差、计算成本高。
- Method: 采用两阶段流水线：YOLO检测器进行高效组件级筛选，视觉大模型作为召回增强单元进行二次验证，支持状态搜索和零样本搜索。
- Result: 在车辆组件数据集上达到94.82%的整体检索准确率，零样本搜索中驾驶员戴口罩任务达到94.95%准确率，状态搜索任务平均准确率超过90%。
- Conclusion: DetVLM框架成功融合目标检测和视觉大模型的优势，在细粒度图像检索中实现了状态搜索和零样本搜索能力，显著优于仅使用检测器的方法。


### [67] [CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding](https://arxiv.org/abs/2511.19923)
*Yuefei Chen,Jiang Liu,Xiaodong Lin,Ruixiang Tang*

Main category: cs.CV

TL;DR: 本文提出了CounterVQA视频基准测试来评估视觉语言模型的因果推理能力，发现现有模型在复杂因果链推理上表现不佳，并开发了CFGPT方法来提升模型的视觉因果推理能力。

- Motivation: 视觉语言模型在视频理解方面取得显著进展，但其因果推理能力（推断假设条件下的替代结果）尚未得到充分探索。这种能力对于稳健的视频理解至关重要，因为它需要识别潜在的因果结构并推理未观察到的可能性，而不仅仅是识别观察到的模式。
- Method: 引入CounterVQA视频基准测试，包含三个渐进难度级别来评估因果推理的不同方面。开发了CFGPT后训练方法，通过从语言模态中提取因果推理能力来增强模型的视觉因果推理能力。
- Result: 对最先进的开源和闭源模型进行全面评估，发现显著性能差距：模型在简单因果问题上达到合理准确率，但在复杂多跳因果链上性能显著下降。CFGPT方法在所有CounterVQA难度级别上均带来一致改进。
- Conclusion: 视觉语言模型在复杂因果推理方面存在显著局限性，CFGPT方法有效提升了模型的视觉因果推理能力，为未来研究提供了重要基准和方法指导。


### [68] [Context-Aware Token Pruning and Discriminative Selective Attention for Transformer Tracking](https://arxiv.org/abs/2511.19928)
*Janani Kugarajeevan,Thanikasalam Kokul,Amirthalingam Ramanan,Subha Fernando*

Main category: cs.CV

TL;DR: CPDATrack是一种基于Transformer的跟踪框架，通过概率驱动的token剪枝和选择性注意力机制来抑制背景和干扰物干扰，同时保持目标上下文信息，在多个基准测试中达到最先进性能。

- Motivation: 传统单流Transformer跟踪器让过多背景搜索token关注目标模板token会削弱跟踪器的判别能力，现有token剪枝方法容易丢失目标附近的重要上下文信息，且干扰物会进一步降低目标识别准确性。
- Method: 1) 在指定编码器层间集成可学习模块估计每个搜索token与目标关联的概率；2) 基于概率剪枝不重要的背景token但保留目标上下文；3) 早期层使用判别性选择性注意力机制完全阻断搜索到模板的注意力；4) 后续层从局部区域选择性提取高概率目标token关注模板token。
- Result: 在多个基准测试中实现最先进性能，特别是在GOT-10k上达到75.1%的平均重叠率。
- Conclusion: CPDATrack通过概率驱动的token剪枝和选择性注意力机制有效抑制背景和干扰物干扰，同时保持计算效率，显著提升了跟踪性能。


### [69] [Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos](https://arxiv.org/abs/2511.19936)
*Youngseo Kim,Dohyun Kim,Geonhee Han,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: DRIFT框架利用预训练图像扩散模型的自注意力机制实现零样本视频对象跟踪，通过重新解释注意力图为语义标签传播核，结合SAM引导的掩码优化，在标准视频对象分割基准上达到最先进性能。

- Motivation: 探索扩散模型在图像生成之外的能力，特别是其自注意力图蕴含的丰富语义结构，可用于识别和定位任务，实现零样本对象跟踪。
- Method: 将扩散模型的自注意力图重新解释为语义标签传播核，构建时间传播核进行视频跟踪；采用DDIM反转、文本反转和自适应头加权等测试时优化策略；结合SAM进行掩码优化。
- Result: 在标准视频对象分割基准上实现了最先进的零样本性能，证明了扩散特征在鲁棒和一致的标签传播中的有效性。
- Conclusion: 扩散模型的自注意力机制提供了强大的像素级对应关系，能够有效支持零样本视频对象跟踪任务，为扩散模型在生成之外的应用开辟了新途径。


### [70] [Low-Resolution Editing is All You Need for High-Resolution Editing](https://arxiv.org/abs/2511.19945)
*Junsung Lee,Hyunsoo Lee,Yong Jae Lee,Bohyung Han*

Main category: cs.CV

TL;DR: 提出了一个用于高分辨率图像编辑的测试时优化框架，通过分块优化、细节传递和同步策略来处理超过1K分辨率的高质量图像编辑。

- Motivation: 现有图像编辑方法仅限于低分辨率设置（通常只支持1K分辨率），无法满足高分辨率内容创作的需求，需要开发有效可控的高分辨率图像操作机制。
- Method: 采用分块优化方法处理高分辨率源图像，结合细粒度细节传递模块和新型同步策略来保持各分块间的一致性。
- Result: 大量实验表明该方法能够产生高质量的编辑效果，为高分辨率内容创作开辟了新途径。
- Conclusion: 该工作成功解决了高分辨率图像编辑的挑战，提出的优化框架为高质量视觉内容创作提供了有效解决方案。


### [71] [Supervise Less, See More: Training-free Nuclear Instance Segmentation with Prototype-Guided Prompting](https://arxiv.org/abs/2511.19953)
*Wen Zhang,Qin Ren,Wenjing Liu,Haibin Ling,Chenyu You*

Main category: cs.CV

TL;DR: SPROUT是一个完全无需训练和标注的核实例分割提示框架，利用组织学先验构建切片特定参考原型，通过部分最优传输进行特征对齐，将特征转换为正负点提示，使SAM模型无需参数更新即可实现精确核分割。

- Motivation: 现有方法大多依赖密集监督和计算昂贵的微调，而无需训练的方法在计算病理学中仍未被充分探索，需要开发可扩展的训练免费核实例分割方案。
- Method: 利用组织学先验构建切片特定参考原型，通过部分最优传输方案逐步指导特征对齐，将前景和背景特征转换为正负点提示，使Segment Anything Model无需参数更新即可分割。
- Result: 在多个组织病理学基准测试上的广泛实验表明，SPROUT在无需监督或重新训练的情况下实现了有竞争力的性能。
- Conclusion: SPROUT为病理学中可扩展的训练免费核实例分割建立了新的范式。


### [72] [GFT-GCN: Privacy-Preserving 3D Face Mesh Recognition with Spectral Diffusion](https://arxiv.org/abs/2511.19958)
*Hichem Felouat,Hanrui Wang,Isao Echizen*

Main category: cs.CV

TL;DR: GFT-GCN是一个隐私保护的3D人脸识别框架，结合了谱图学习和基于扩散的模板保护技术，在保持高识别精度的同时确保生物特征模板的安全性。

- Motivation: 3D人脸识别虽然对光照、姿态变化和演示攻击具有鲁棒性，但存储的生物特征模板保护仍然至关重要。需要开发既能保护隐私又保持高性能的3D人脸认证解决方案。
- Method: 结合图傅里叶变换(GFT)和图卷积网络(GCN)从3D人脸网格中提取紧凑的谱特征，并引入谱扩散机制生成不可逆、可更新和不可链接的模板。采用轻量级客户端-服务器架构，原始生物数据不会离开客户端设备。
- Result: 在BU-3DFE和FaceScape数据集上的实验表明，该方法具有高识别精度和强大的重建攻击抵抗能力。
- Conclusion: GFT-GCN有效平衡了隐私和性能，为安全的3D人脸认证提供了实用解决方案。


### [73] [MambaEye: A Size-Agnostic Visual Encoder with Causal Sequential Processing](https://arxiv.org/abs/2511.19963)
*Changho Choi,Minho Kim,Jinkyu Kim*

Main category: cs.CV

TL;DR: MambaEye是一个基于Mamba2的因果序列视觉编码器，通过单向处理和相对移动嵌入实现输入尺寸无关的视觉编码，在ImageNet-1K分类任务中表现出色，特别是在高分辨率图像上。

- Motivation: 解决现有视觉编码器缺乏真正输入尺寸无关性的问题，实现像人类视觉一样能够处理任意尺寸输入的能力。
- Method: 使用纯Mamba2骨干网络，采用严格单向处理保持状态空间模型的因果性，引入相对移动嵌入编码空间位移，并使用扩散启发的损失函数进行密集监督训练。
- Result: 在ImageNet-1K分类任务中，特别是在1536^2等高分辨率图像上表现出稳健性能，同时保持相对于补丁数量的线性时间和内存复杂度。
- Conclusion: MambaEye成功实现了输入尺寸无关的视觉编码，为构建更接近人类视觉能力的模型提供了新途径。


### [74] [HiCoGen: Hierarchical Compositional Text-to-Image Generation in Diffusion Models via Reinforcement Learning](https://arxiv.org/abs/2511.19965)
*Hongji Yang,Yucheng Zhou,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出了HiCoGen框架，通过链式合成和强化学习解决复杂提示下的图像生成问题，显著提升了概念覆盖率和组合准确性。

- Motivation: 现有扩散模型在处理涉及多个对象和层次结构的复杂提示时，难以准确遵循指令，导致概念遗漏、混淆和组合性差的问题。
- Method: 基于链式合成范式，首先用LLM分解复杂提示为最小语义单元，然后迭代合成这些单元；引入强化学习框架，提出衰减随机性调度来增强探索，并使用分层奖励机制评估图像。
- Result: 实验表明该方法在概念覆盖率和组合准确性方面显著优于现有方法。
- Conclusion: HiCoGen框架通过层次化组合生成和增强的探索策略，有效解决了复杂提示下的图像生成挑战。


### [75] [VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction](https://arxiv.org/abs/2511.19971)
*Yu Hu,Chong Cheng,Sicheng Yu,Xiaoyang Guo,Hao Wang*

Main category: cs.CV

TL;DR: VGGT4D是一个无需训练的框架，通过挖掘VGGT全局注意力层中的动态线索，实现动态4D场景重建，在动态目标分割、相机姿态估计和密集重建方面表现优异。

- Motivation: 现有4D方法依赖外部先验、繁重后优化或需要4D数据集微调，而3D基础模型VGGT在动态物体占主导时性能下降明显，需要解决动态与静态元素的鲁棒分离问题。
- Method: 通过gram相似性挖掘和放大全局动态线索，跨时间窗口聚合，引入投影梯度驱动的细化策略来锐化掩码边界，并将精确掩码集成到VGGT早期推理中。
- Result: 在六个数据集上，在动态目标分割、相机姿态估计和密集重建方面实现优越性能，支持单次推理超过500帧的长序列。
- Conclusion: VGGT4D成功扩展了3D基础模型VGGT用于鲁棒4D场景重建，无需训练即可有效缓解运动干扰。


### [76] [Boosting Reasoning in Large Multimodal Models via Activation Replay](https://arxiv.org/abs/2511.19972)
*Yun Xing,Xiaobin Hu,Qingdong He,Jiangning Zhang,Shuicheng Yan,Shijian Lu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Activation Replay的训练免费方法，通过重放基础LMM的低熵激活来提升经过RLVR后训练的大型多模态模型的推理能力，在数学推理、视觉代理和视频推理等任务中取得显著效果。

- Motivation: RLVR（可验证奖励的强化学习）已被证明能有效提升大型多模态模型的推理能力，但其底层机制尚不明确。本文旨在探索RLVR如何影响输入激活，并基于此理解开发提升推理性能的方法。
- Method: 1. 通过logit lens视角系统研究RLVR对输入激活的影响；2. 发现RLVR主要影响低熵激活，而高熵激活受影响较小；3. 提出Activation Replay方法，在测试时重放基础LMM的低熵激活来调节RLVR模型的激活状态。
- Result: Activation Replay在多种场景下显著提升推理性能，包括数学推理、o3类视觉代理和视频推理。该方法提高了Pass@K指标，缓解了RLVR推理覆盖范围较窄的问题，且优于重放高熵激活或直接跨模型干预等替代方案。
- Conclusion: RLVR主要通过调节低熵激活来影响推理能力，而Activation Replay作为一种简单有效的训练免费方法，能够显著提升后训练LMM的多模态推理性能，为理解RLVR机制提供了新视角。


### [77] [EmoFeedback2: Reinforcement of Continuous Emotional Image Generation via LVLM-based Reward and Textual Feedback](https://arxiv.org/abs/2511.19982)
*Jingyang Jia,Kai Shu,Gang Yang,Long Xing,Xun Chen,Aiping Liu*

Main category: cs.CV

TL;DR: 提出EmoFeedback2框架，通过生成-理解-反馈的强化学习范式，利用细调的大视觉语言模型为连续情感图像生成提供奖励和文本反馈，提升情感连续性和保真度。

- Motivation: 现有连续情感图像生成方法缺乏生成图像的情感反馈，难以控制情感连续性，且情感与文本的简单对齐无法根据图像内容自适应调整情感提示，导致情感保真度不足。
- Method: 采用生成-理解-反馈强化范式，包含情感感知奖励反馈策略（LVLM评估图像情感值并计算奖励）和自我提升文本反馈框架（LVLM迭代分析图像情感内容并自适应生成提示优化建议）。
- Result: 实验结果表明该方法能有效生成具有期望情感的高质量图像，在自定义数据集上优于现有最先进方法。
- Conclusion: EmoFeedback2框架通过LVLM的推理能力解决了连续情感图像生成中的情感连续性和保真度问题，为高质量情感图像生成提供了有效解决方案。


### [78] [SONIC: Spectral Optimization of Noise for Inpainting with Consistency](https://arxiv.org/abs/2511.19985)
*Seungyeon Baek,Erqun Dong,Shadan Namazifard,Mark J. Matthews,Kwang Moo Yi*

Main category: cs.CV

TL;DR: 提出了一种无需训练的图像修复方法，通过优化初始种子噪声来匹配未遮罩区域，结合线性近似和频谱域优化实现高效稳定的修复效果。

- Motivation: 现有的基于引导的方法理论上可用于图像修复等逆问题，但实际效果有限，导致需要专门的修复模型。本文旨在开发无需训练即可使用现成文生图模型进行修复的方法。
- Method: 优化初始种子噪声以近似匹配未遮罩区域数据，采用线性近似避免昂贵的展开计算，并在频谱域进行优化以提高稳定性。
- Result: 在各种修复任务中表现出色，超越了现有技术水平。
- Conclusion: 通过优化初始种子噪声并采用线性近似和频谱域优化，实现了无需训练的高效图像修复方法，显著提升了修复质量。


### [79] [GazeProphetV2: Head-Movement-Based Gaze Prediction Enabling Efficient Foveated Rendering on Mobile VR](https://arxiv.org/abs/2511.19988)
*Farhaan Ebadulla,Chiraag Mudlpaur,Shreya Chaurasia,Gaurav BV*

Main category: cs.CV

TL;DR: 提出了一种结合时间注视模式、头部运动数据和视觉场景信息的多模态VR注视预测方法，通过门控融合机制和跨模态注意力自适应加权不同信息源，在22个VR场景的530万注视样本数据集上验证了有效性。

- Motivation: VR环境中的注视行为预测对于渲染优化和界面设计具有重要意义，但现有方法难以准确预测用户注意力模式。
- Method: 采用多模态方法，结合时间注视模式、头部运动数据和视觉场景信息，使用门控融合机制和跨模态注意力来自适应加权不同模态信息。
- Result: 多模态组合相比单一数据流显著提高了预测准确率，在1-3个未来帧的预测中表现优异，跨场景泛化测试显示93.1%的验证准确率和时间一致性。
- Conclusion: 该方法有助于理解虚拟环境中的注意力机制，在渲染优化、交互设计和用户体验评估方面具有应用潜力，为无需昂贵眼动追踪硬件的VR系统发展提供了方向。


### [80] [OmniRefiner: Reinforcement-Guided Local Diffusion Refinement](https://arxiv.org/abs/2511.19990)
*Yaoli Liu,Ziheng Ouyang,Shengtao Lou,Yiren Song*

Main category: cs.CV

TL;DR: 提出了一种细节感知的参考引导图像生成框架，通过两阶段参考驱动修正来增强像素级一致性，解决现有方法在保留细粒度视觉细节方面的不足。

- Motivation: 当前基于VAE的潜在压缩方法会丢失细微纹理信息，导致身份和属性特定线索消失，而现有的后编辑方法在放大局部细节时往往产生与原始图像在光照、纹理或形状上不一致的结果。
- Method: 采用两阶段参考驱动修正：首先通过微调单图像扩散编辑器，使其能够同时处理草稿图像和参考图像，实现全局一致的细化；然后应用强化学习进一步增强局部编辑能力，明确优化细节准确性和语义一致性。
- Result: 在具有挑战性的参考引导恢复基准测试中，该方法显著提高了参考对齐和细粒度细节保留能力，产生的编辑结果在忠实度和视觉连贯性方面超越了开源和商业模型。
- Conclusion: 该方法通过两阶段参考驱动修正有效解决了参考引导图像生成中的细节保留问题，实现了更好的像素级一致性和视觉质量。


### [81] [CREward: A Type-Specific Creativity Reward Model](https://arxiv.org/abs/2511.19995)
*Jiyeon Han,Ali Mahdavi-Amiri,Hao Zhang,Haedong Jeong*

Main category: cs.CV

TL;DR: 该论文提出了第一个类型特定的创意奖励模型CREward，涵盖几何、材质和纹理三个创意维度，用于评估和生成创意图像。

- Motivation: 传统方法将创意视为单一未分化的数量过于简单，需要从图像形成流程的角度更细致地理解和评估创意。
- Method: 首先进行人类基准评估以捕捉人类对不同类型创意的感知，然后分析LVLM预测与人类判断的相关性，最后使用LVLM生成的标签训练CREward模型。
- Result: LVLM与人类感知表现出强对齐性，CREward模型可用于创意评估、可解释创意分析以及创意样本获取。
- Conclusion: CREward模型成功实现了类型特定的创意评估和生成，为创意图像的理解和创作提供了新的工具和方法。


### [82] [On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation](https://arxiv.org/abs/2511.20002)
*Changyue Li,Jiaying Li,Youliang Yuan,Jiaming He,Zhicong Huang,Pinjia He*

Main category: cs.CV

TL;DR: 本文揭示了对抗性攻击的新威胁：单个扰动可以劫持整个决策链，同时操纵模型产生多个预定义错误输出，而不仅仅是单一错误分类。

- Motivation: 现实世界中的模型通常进行一系列决策，孤立错误容易被纠正，但级联错误会导致严重风险。传统对抗攻击只关注操纵单一决策，无法揭示这种系统性威胁。
- Method: 提出语义感知通用扰动(SAUPs)，通过归一化空间搜索和语义分离策略的有效算法，生成能根据输入语义诱导不同错误结果的扰动。
- Result: 在三个多模态大语言模型上的实验表明，仅使用一个对抗帧就能控制五个不同目标，达到70%的攻击成功率。
- Conclusion: SAUPs暴露了多模态大模型在序列决策中的系统性脆弱性，单个扰动即可引发级联错误，对现实应用构成严重安全威胁。


### [83] [Pedestrian Crossing Intention Prediction Using Multimodal Fusion Network](https://arxiv.org/abs/2511.20008)
*Yuanzhe Li,Steffen Müller*

Main category: cs.CV

TL;DR: 提出一种多模态融合网络，利用视觉和运动分支的七种模态特征来预测行人过街意图，通过深度引导注意力、模态注意力和时间注意力机制提升预测性能。

- Motivation: 行人过街意图预测对自动驾驶车辆在城市场景中的部署至关重要，但预测任务因行人行为的多样性和对多种上下文因素的依赖性而具有挑战性。
- Method: 使用基于Transformer的特征提取模块从原始输入中提取运动和视觉特征，通过深度引导注意力模块利用深度信息引导注意力，并设计模态注意力和时间注意力机制来选择性强调信息丰富的模态和捕捉时间依赖性。
- Result: 在JAAD数据集上的广泛实验验证了所提网络的有效性，相比基线方法取得了更优越的性能。
- Conclusion: 所提出的多模态融合网络能够有效提取和整合不同模态的互补线索，在行人过街意图预测任务中表现出色。


### [84] [Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments](https://arxiv.org/abs/2511.20011)
*Yuanzhe Li,Hang Zhong,Steffen Müller*

Main category: cs.CV

TL;DR: 提出了一种多上下文融合Transformer（MFT）方法，通过融合行人行为、环境、行人定位和车辆运动四个维度的上下文信息，实现准确的行人过街意图预测。

- Motivation: 准确预测行人过街意图对自动驾驶车辆提高行人安全和减少交通事故至关重要，但由于影响行人行为的因素众多，在城市场景中准确预测行人意图仍然具有挑战性。
- Method: 采用渐进融合策略：首先通过相互内部上下文注意力实现上下文内特征融合，生成上下文特定表示；然后通过相互跨上下文注意力整合不同上下文特征；最后通过引导式内部和跨上下文注意力进一步精炼表示。
- Result: 在JAADbeh、JAADall和PIE数据集上分别达到73%、93%和90%的准确率，优于现有最先进方法。
- Conclusion: MFT通过有效的多上下文融合机制显著提升了行人过街意图预测的准确性，消融研究验证了网络架构和不同输入上下文的有效性。


### [85] [ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction](https://arxiv.org/abs/2511.20020)
*Yuanzhe Li,Steffen Müller*

Main category: cs.CV

TL;DR: 提出了一种注意力引导的跨模态交互Transformer（ACIT）用于预测行人过街意图，通过六种视觉和运动模态的分组交互，结合双路径注意力机制和Transformer时序建模，在JAAD数据集上达到最优性能。

- Motivation: 有效提取和整合不同类型数据的互补信息是预测行人过街意图的关键挑战，现有方法在跨模态交互方面仍有不足。
- Method: 将六种模态分为三组交互对：全局语义地图与全局光流、局部RGB图像与局部光流、自车速度与行人边界框。采用双路径注意力机制增强主模态显著区域，并通过光流引导注意力与辅助模态深度交互。使用跨模态注意力建模运动特征，结合多模态特征融合和Transformer时序聚合模块。
- Result: 在JAADbeh和JAADall数据集上分别达到70%和89%的准确率，优于现有最优方法。消融实验验证了各模块的有效贡献。
- Conclusion: ACIT通过精心设计的跨模态交互机制和时序建模，能够有效预测行人过街意图，为自动驾驶系统提供重要安全保障。


### [86] [WaymoQA: A Multi-View Visual Question Answering Dataset for Safety-Critical Reasoning in Autonomous Driving](https://arxiv.org/abs/2511.20022)
*Seungjun Yu,Seonho Lee,Namho Kim,Jaeyo Shin,Junsung Park,Wonjeong Ryu,Raehyuk Jung,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出了安全关键推理任务，通过多视角输入解决自动驾驶中避免一个风险可能引发另一个风险的复杂场景，并发布了WaymoQA数据集来提升多模态大语言模型的安全推理能力。

- Motivation: 现有MLLMs在安全关键驾驶场景中的高级推理能力不足，特别是当避免一个交通风险可能引发另一个风险时，需要更全面的环境感知。
- Method: 定义安全关键推理任务，将其分为两个阶段：先解决即时风险，再缓解决策引发的下游风险；构建包含35,000个人工标注问答对的WaymoQA数据集，涵盖图像和视频模态。
- Result: 实验显示现有MLLMs在安全关键场景下表现不佳，但使用WaymoQA微调后推理能力显著提升。
- Conclusion: WaymoQA数据集能有效提升MLLMs在安全关键驾驶场景中的推理能力，为开发更安全的自动驾驶代理提供了重要支持。


### [87] [SAM-MI: A Mask-Injected Framework for Enhancing Open-Vocabulary Semantic Segmentation with SAM](https://arxiv.org/abs/2511.20027)
*Lin Chen,Yingjian Zhu,Qi Yang,Xin Niu,Kun Ding,Shiming Xiang*

Main category: cs.CV

TL;DR: SAM-MI是一个新颖的掩码注入框架，通过文本引导稀疏点提示、浅层掩码聚合和解耦掩码注入，有效解决SAM在开放词汇语义分割中的过分割和硬组合问题。

- Motivation: 开放词汇语义分割需要通用分割能力，SAM虽然具备强大的通用分割能力，但在实际应用中存在过分割和掩码与标签硬组合的问题。
- Method: 使用文本引导稀疏点提示器替代密集网格提示加速掩码生成；通过浅层掩码聚合缓解过分割；采用解耦掩码注入在低频和高频分别指导模型。
- Result: 在多个基准测试中验证了SAM-MI的优越性，在MESS基准上相比Grounded-SAM实现了16.7%的相对mIoU提升和1.6倍加速。
- Conclusion: SAM-MI可作为有效将SAM集成到开放词汇语义分割模型的替代方法学。


### [88] [Tell Model Where to Look: Mitigating Hallucinations in MLLMs by Vision-Guided Attention](https://arxiv.org/abs/2511.20032)
*Jianfei Zhao,Feng Zhang,Xin Sun,Chong Feng,Zhixing Tan*

Main category: cs.CV

TL;DR: 提出Vision-Guided Attention (VGA)方法，通过利用视觉token的语义内容构建精确的视觉基础，引导模型关注相关视觉区域，有效减少MLLMs的幻觉问题。

- Motivation: MLLMs的视觉注意力机制定位能力有限，导致产生幻觉。虽然MLLMs能从视觉token中准确提取语义，但在后续推理中未能充分利用这一优势。
- Method: 提出训练免费的VGA方法：首先利用视觉token的语义内容构建精确的视觉基础，然后用此基础引导模型关注相关视觉区域。在图像描述任务中，动态抑制已描述区域。
- Result: VGA仅引入4.36%的延迟开销，与FlashAttention等高效注意力实现完全兼容。在多个MLLMs和幻觉基准测试中实现最先进的去幻觉性能。
- Conclusion: 明确的视觉引导在增强MLLMs视觉理解能力中起着关键作用。


### [89] [Clair Obscur: an Illumination-Aware Method for Real-World Image Vectorization](https://arxiv.org/abs/2511.20034)
*Xingyue Lin,Shuai Peng,Xiangyu Xie,Jianhua Zhu,Yuxuan Zhou,Liangcai Gao*

Main category: cs.CV

TL;DR: COVec是一个基于明暗对比原理的照明感知矢量框架，首次在矢量域引入内在图像分解，将图像分离为反照率、阴影和光照层，通过语义引导初始化和两阶段优化实现高质量矢量转换。

- Motivation: 现有矢量方法难以表示复杂的真实世界图像，通常会产生碎片化的形状，牺牲了语义简洁性。
- Method: 引入内在图像分解到矢量域，将图像分离为反照率、阴影和光照层；采用语义引导初始化和两阶段优化，结合可微分渲染。
- Result: 在多个数据集上的实验表明，COVec相比现有方法实现了更高的视觉保真度和显著改进的可编辑性。
- Conclusion: COVec通过照明感知的矢量分解方法，成功解决了复杂真实世界图像的矢量表示问题，在保持视觉质量的同时提升了可编辑性。


### [90] [MFM-point: Multi-scale Flow Matching for Point Cloud Generation](https://arxiv.org/abs/2511.20041)
*Petr Molodyk,Jaemoo Choi,David W. Romero,Ming-Yu Liu,Yongxin Chen*

Main category: cs.CV

TL;DR: MFM-Point是一个用于点云生成的多尺度流匹配框架，通过粗到细的生成范式显著提升了基于点的方法的可扩展性和性能，同时保持了简单性和效率。

- Motivation: 现有的基于点的方法直接生成点云，具有训练成本低和算法简单的优点，但性能往往不如基于表示的方法。本文旨在提升基于点的方法的性能和可扩展性。
- Method: 提出了多尺度流匹配框架，采用粗到细的生成范式，引入了结构化下采样和上采样策略来保持几何结构并确保跨分辨率的一致性分布过渡。
- Result: MFM-Point在基于点的方法中实现了最佳性能，并挑战了最佳的基于表示的方法，在多类别和高分辨率生成任务中表现出色。
- Conclusion: MFM-Point框架显著提升了基于点的点云生成方法的性能，同时保持了其简单性和效率优势，为点云生成提供了新的解决方案。


### [91] [History-Augmented Contrastive Meta-Learning for Unsupervised Blind Super-Resolution of Planetary Remote Sensing Images](https://arxiv.org/abs/2511.20045)
*Huijia Zhao,Jie Lu,Yunqing Jiang,Xiao-Ping Lu,Kaichang Di*

Main category: cs.CV

TL;DR: 提出HACBSR，一种无需地面真值和外部核先验的无监督盲超分辨率框架，用于处理行星遥感图像中的未知退化问题。

- Motivation: 行星遥感图像受到成像环境和硬件限制导致的多样未知退化影响，缺乏地面真值图像限制了监督盲超分辨率方法的应用。
- Method: HACBSR包含两个组件：对比核采样机制（带核相似性控制）和历史增强对比学习（使用历史模型生成负样本）。
- Result: 在Ceres-50数据集上的实验表明，HACBSR在多个放大因子下与最先进的无监督方法相比具有竞争力。
- Conclusion: HACBSR为行星遥感图像的无监督盲超分辨率提供了一种有效的解决方案，无需地面真值图像和外部核先验。


### [92] [DeLightMono: Enhancing Self-Supervised Monocular Depth Estimation in Endoscopy by Decoupling Uneven Illumination](https://arxiv.org/abs/2511.20058)
*Mingyang Ou,Haojin Li,Yifeng Zhang,Ke Niu,Zhongxi Qiu,Heng Li,Jiang Liu*

Main category: cs.CV

TL;DR: 提出DeLight-Mono框架，通过光照解耦解决内窥镜图像中不均匀光照对自监督单目深度估计的影响

- Motivation: 内窥镜图像中不均匀光照导致深度估计性能下降，现有低光增强技术无法有效指导深度网络，其他领域解决方案需要良好光照条件且增加数据收集负担
- Method: 设计光照-反射率-深度模型表示内窥镜图像，使用辅助网络进行分解，提出基于解耦组件的自监督联合优化框架和新损失函数
- Result: 在两个公共数据集上通过广泛比较和消融研究验证了所提方法的有效性
- Conclusion: DeLight-Mono框架通过光照解耦有效减轻了不均匀光照对深度估计的影响


### [93] [FLaTEC: Frequency-Disentangled Latent Triplanes for Efficient Compression of LiDAR Point Clouds](https://arxiv.org/abs/2511.20065)
*Xiaoge Zhang,Zijie Wu,Mingtao Feng,Zichen Geng,Mehwish Nasim,Saeed Anwar,Ajmal Mian*

Main category: cs.CV

TL;DR: FLaTEC是一种频率感知的点云压缩模型，通过解耦低频结构和高频纹理，使用混合潜在三平面作为紧凑代理，实现高压缩比下的高质量重建。

- Motivation: 点云压缩方法需要同时优化比特率和重建失真，但低频和高频分量在同一分辨率下贡献不同，难以平衡压缩比和重建质量。
- Method: 将体素化嵌入转换为三平面表示以减少稀疏性；设计频率解耦技术提取紧凑低频内容并收集多尺度高频细节；使用频率注意力机制补偿3D相关性损失。
- Result: 在SemanticKITTI和Ford数据集上，BD-rate分别比标准编解码器提高了78%和94%，实现了最先进的率失真性能。
- Conclusion: FLaTEC通过频率感知机制有效解决了点云压缩中低频和高频分量平衡问题，显著提升了压缩性能。


### [94] [PRADA: Probability-Ratio-Based Attribution and Detection of Autoregressive-Generated Images](https://arxiv.org/abs/2511.20068)
*Simon Damm,Jonas Ricker,Henning Petzka,Asja Fischer*

Main category: cs.CV

TL;DR: 提出了PRADA方法，一种基于概率比率的可解释方法，用于检测和归因自回归生成的图像。

- Motivation: 自回归图像生成器能够生成逼真的图像，但目前缺乏专门检测这类生成图像的方法。
- Method: 通过检查模型对图像自回归token序列的条件概率和无条件概率之比，利用该比率的独特特征进行阈值检测和归因。
- Result: PRADA对8种类别到图像和4种文本到图像模型高度有效。
- Conclusion: 该方法简单、可解释且可靠，能够有效检测自回归生成的图像并归因到相应源模型。


### [95] [Learning Procedural-aware Video Representations through State-Grounded Hierarchy Unfolding](https://arxiv.org/abs/2511.20073)
*Jinghan Zhao,Yifei Huang,Feng Lu*

Main category: cs.CV

TL;DR: 提出了Task-Step-State (TSS)框架，通过引入'状态'作为视觉基础语义层，将抽象程序与可观察细节联系起来，在多个下游任务上优于基线模型。

- Motivation: 现有方法在任务和步骤级别对齐视觉内容和文本描述，但由于高度抽象性，难以与视觉数据中的具体可观察细节形成稳健对齐。
- Method: 引入'状态'作为文本快照，提出TSS框架，采用渐进式预训练策略展开TSS层次结构，强制模型在状态中基础表示。
- Result: 在COIN和CrossTask数据集上的实验表明，该方法在任务识别、步骤识别和下一步预测等多个下游任务上优于基线模型。
- Conclusion: 状态监督是性能提升的关键驱动因素，渐进式预训练策略比标准联合训练更有效，能更好地强制执行预期的层次结构。


### [96] [Blind Adaptive Local Denoising for CEST Imaging](https://arxiv.org/abs/2511.20081)
*Chu Chen,Aitor Artola,Yang Liu,Se Weon Park,Raymond H. Chan,Jean-Michel Morel,Kannie W. Y. Chan*

Main category: cs.CV

TL;DR: 提出了一种新的盲自适应局部去噪方法BALD，用于解决CEST MRI中的异方差噪声问题，无需先验噪声知识即可稳定噪声分布并分离分子信号与噪声。

- Motivation: CEST MRI在临床转化中面临空间变化噪声和复杂成像协议导致的异方差性问题，传统去噪方法无法处理这种复杂噪声且会改变关键生物医学信息。
- Method: BALD利用CEST数据的自相似性推导自适应方差稳定变换，通过局部SVD分解进行两阶段去噪，在数据线性变换上分离分子信号与噪声。
- Result: 在多个体模和体内CEST扫描实验中，BALD在去噪指标和下游任务（如分子浓度图估计和癌症检测）上均优于现有最先进的CEST去噪方法。
- Conclusion: BALD方法有效解决了CEST MRI中的异方差噪声问题，提高了定量对比度映射的准确性，在生物医学分析中表现出优越性能。


### [97] [Explainable Visual Anomaly Detection via Concept Bottleneck Models](https://arxiv.org/abs/2511.20088)
*Arianna Stropeni,Valentina Zaccaria,Francesco Borsatti,Davide Dalle Pezze,Manuel Barusco,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 将概念瓶颈模型(CBM)扩展到视觉异常检测(VAD)领域，提出CONVAD方法，通过概念学习提供人类可解释的异常描述，同时保持与传统VAD方法相当的性能。

- Motivation: 现有VAD模型虽然能提供视觉异常区域定位，但缺乏直接且语义上有意义的解释，难以让用户理解异常的具体含义。
- Method: 开发概念数据集支持CBM在VAD中的研究；改进CBM架构同时生成概念和视觉解释；引入人工异常合成管道，减少对罕见异常样本的依赖。
- Result: CONVAD在性能上与经典VAD方法相当，同时提供了更丰富、基于概念的解释，增强了VAD系统的可解释性和可信度。
- Conclusion: 概念瓶颈模型能够有效提升视觉异常检测的可解释性，通过概念驱动的解释为用户提供更深入的异常理解。


### [98] [WPT: World-to-Policy Transfer via Online World Model Distillation](https://arxiv.org/abs/2511.20095)
*Guangfeng Jiang,Yueru Luo,Jun Liu,Yi Huang,Yiyao Zhu,Zhan Qu,Dave Zhenyu Chen,Bingbing Liu,Xu Yan*

Main category: cs.CV

TL;DR: WPT提出了一种世界模型到策略的迁移训练范式，通过在线蒸馏在端到端世界模型指导下训练轻量级策略，在保持实时部署性的同时提升规划性能。

- Motivation: 现有世界模型方法存在运行时耦合紧密或依赖离线奖励信号的问题，导致推理开销大或阻碍端到端优化。
- Method: 开发可训练奖励模型将世界知识注入教师策略，通过策略蒸馏和世界奖励蒸馏将教师推理能力转移到轻量级学生策略。
- Result: 在开环和闭环基准测试中达到SOTA性能：0.11碰撞率（开环）和79.23驾驶分数（闭环），推理速度提升4.9倍。
- Conclusion: WPT在准确性和安全性上超越了基于世界模型和模仿学习的方法，同时保持了实时部署能力。


### [99] [Exploring State-of-the-art models for Early Detection of Forest Fires](https://arxiv.org/abs/2511.20096)
*Sharjeel Ahmed,Daim Armaghan,Fatima Naweed,Umair Yousaf,Ahmad Zubair,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出用于森林火灾早期检测的数据集，通过游戏模拟器生成包含烟雾和初期火灾的图像，结合现有数据集，并比较了YOLOv7和检测变换器模型的性能。

- Motivation: 现有森林火灾检测方法由于缺乏大规模数据集和专门优化的模型，存在漏检问题，需要开发早期预警系统。
- Method: 使用Red Dead Redemption 2等游戏模拟器合成包含烟雾和初期火灾的图像数据集，结合已发布图像，采用YOLOv7和检测变换器模型进行图像分类和定位。
- Result: 构建了包含烟雾和初期火灾实例的综合性数据集，并比较了不同检测模型的性能。
- Conclusion: 提出的合成数据集和模型比较为森林火灾早期检测提供了有效解决方案，解决了现有方法的数据不足问题。


### [100] [Multi Head Attention Enhanced Inception v3 for Cardiomegaly Detection](https://arxiv.org/abs/2511.20101)
*Abishek Karthik,Pandiyaraju V*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度学习和注意力机制的集成方法，用于从X射线图像中自动检测心脏肥大。采用CNN架构和Inception V3模型，结合多层注意力机制，实现了95.6%的准确率。

- Motivation: 医疗影像技术革命性地改变了心血管疾病的诊断方式，特别是心脏肥大等结构异常的检测。需要开发自动化的深度学习方法来提高诊断效率和准确性。
- Method: 基于数据收集和预处理，采用CNN架构结合Inception V3模型，并引入多层注意力机制，特别是多头注意力机制来自动学习特征并选择性关注输入图像的关键区域。
- Result: 模型在心脏肥大检测中表现出色：准确率95.6%，精确率95.2%，召回率96.2%，灵敏度95.7%，特异性96.1%，AUC值96.0%。
- Conclusion: 该集成方法成功实现了心脏肥大的高精度自动检测，证明了深度学习结合注意力机制在医疗影像诊断中的临床价值和有效性。


### [101] [LungEvaty: A Scalable, Open-Source Transformer-based Deep Learning Model for Lung Cancer Risk Prediction in LDCT Screening](https://arxiv.org/abs/2511.20116)
*Johannes Brandt,Maulik Chevli,Rickmer Braren,Georgios Kaissis,Philip Müller,Daniel Rueckert*

Main category: cs.CV

TL;DR: LungEvaty是一个基于transformer的框架，从单次低剂量CT扫描预测1-6年肺癌风险，无需像素级标注，在9万多CT扫描上训练，性能达到最先进水平。

- Motivation: 随着更多国家采用低剂量CT进行人群筛查，需要可扩展的方法来处理整个肺部体积，现有方法要么过度依赖像素级标注，要么将肺部分析为片段，限制了性能和可扩展性。
- Method: 开发了完全基于transformer的框架LungEvaty，处理整个肺部输入，从大规模筛查数据中学习，捕捉与恶性肿瘤风险相关的全面解剖和病理线索，可选使用解剖学知情注意力引导损失。
- Result: 仅使用影像数据且无区域监督，LungEvaty达到了最先进的性能，在超过9万CT扫描（包括2.8万微调和6千评估）上训练，提供简单、数据高效且完全开源的解决方案。
- Conclusion: LungEvaty为纵向和多模态肺癌风险预测的未来研究提供了可扩展的基础，是一个完全开源的数据高效解决方案。


### [102] [UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers](https://arxiv.org/abs/2511.20123)
*Min Zhao,Hongzhou Zhu,Yingze Wang,Bokai Yan,Jintao Zhang,Guande He,Ling Yang,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出UltraViCo方法，通过抑制训练窗口外token的注意力来解决视频长度外推问题，将外推极限从2倍提升到4倍。

- Motivation: 视频扩散变换器在训练长度外推时存在周期性内容重复和通用质量下降两个失败模式，现有方法仅通过位置编码解决重复问题，忽略了质量下降且外推能力有限。
- Method: 从注意力图角度分析，发现两个失败模式源于统一原因：注意力分散。提出UltraViCo方法，使用恒定衰减因子抑制训练窗口外token的注意力。
- Result: 在多个模型和外推比例上超越现有基线方法，将外推极限从2倍提升到4倍，在4倍外推时动态程度和成像质量分别提升233%和40.5%。
- Conclusion: UltraViCo能有效解决视频长度外推问题，并可无缝推广到可控视频合成和编辑等下游任务。


### [103] [Vision-Language Models for Automated 3D PET/CT Report Generation](https://arxiv.org/abs/2511.20145)
*Wenpei Jiao,Kun Shang,Hui Li,Ke Yan,Jiajin Zhang,Guangjie Yang,Lijuan Guo,Yan Wan,Xing Yang,Dakai Jin,Zhaoheng Xie*

Main category: cs.CV

TL;DR: 提出了PETRG-3D框架，用于自动生成PET/CT报告，通过3D双分支架构和风格自适应提示来解决PET/CT报告生成的独特挑战，并在淋巴瘤数据集上验证了其优越性能。

- Motivation: PET/CT在肿瘤学中至关重要，但扫描仪的快速扩张超过了专业人员的可用性，需要自动化报告生成来减轻临床工作负担。与传统结构成像相比，功能性PET具有代谢模式随示踪剂生理变化和需要全身3D上下文信息等独特挑战。
- Method: 提出PETRG-3D端到端3D双分支框架，分别编码PET和CT体积，并加入风格自适应提示来减轻医院间报告实践的变异性。构建了多中心淋巴瘤数据集PETRG-Lym和公开基准AutoPET-RG-Lym。
- Result: 实验表明PETRG-3D在自然语言指标（如ROUGE-L提升31.49%）和临床效能指标（如PET-All提升8.18%）上显著优于现有方法，验证了体积双模态建模和风格感知提示的益处。
- Conclusion: 这项工作为未来强调疾病感知推理和临床可靠评估的PET/CT特定模型奠定了基础。代码、模型和AutoPET-RG-Lym将公开发布。


### [104] [Hybrid Convolution and Frequency State Space Network for Image Compression](https://arxiv.org/abs/2511.20151)
*Haodong Pan,Hao Wei,Yusong Wang,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: HCFSSNet是一个用于学习图像压缩的混合卷积和频率状态空间网络，结合CNN的局部高频特征提取和状态空间模型的长程低频建模能力，通过频率感知模块实现更高效的比特分配。

- Motivation: 现有的Transformer和状态空间模型虽然具有强大的长程建模能力，但在图像压缩中可能导致结构信息丢失或忽略关键的频率特性，而CNN能有效捕捉局部高频细节但缺乏长程建模。
- Method: 提出HCFSSNet：1）使用CNN提取局部高频结构；2）引入视觉频率状态空间块，结合全向邻域状态空间模块和自适应频率调制模块；3）集成AFMM与Swin Transformer形成频率Swin Transformer注意力模块，用于频率感知的边信息建模。
- Result: 在Kodak、Tecnick和CLIC数据集上，HCFSSNet相比VTM锚点分别降低了18.06%、24.56%和22.44%的BD率，与最近的MambaIC等SSM编解码器相比具有竞争力的率失真性能，且参数量显著减少。
- Conclusion: HCFSSNet为未来学习图像压缩系统提供了一种高效且可解释的混合架构，成功结合了CNN的局部特征提取和状态空间模型的长程建模优势。


### [105] [Restora-Flow: Mask-Guided Image Restoration with Flow Matching](https://arxiv.org/abs/2511.20152)
*Arnela Hadzic,Franz Thaler,Lea Bogensperger,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: Restora-Flow是一种无需训练的图像修复方法，通过引导流匹配采样和轨迹校正机制，在保持高质量的同时显著减少处理时间。

- Motivation: 当前基于流模型的图像修复方法存在处理时间长或产生过度平滑结果的问题，需要一种既能保持高质量又能快速处理的方法。
- Method: 使用退化掩码引导流匹配采样，并引入轨迹校正机制来确保与退化输入的一致性，无需额外训练。
- Result: 在自然和医学数据集上的多种图像修复任务（修复、超分辨率、去噪）中，相比扩散模型和流匹配方法，显示出更优的感知质量和处理时间。
- Conclusion: Restora-Flow提供了一种高效且高质量的图像修复解决方案，在保持生成质量的同时显著提升了处理效率。


### [106] [Alzheimers Disease Progression Prediction Based on Manifold Mapping of Irregularly Sampled Longitudinal Data](https://arxiv.org/abs/2511.20154)
*Xin Hong,Ying Shi,Yinhao Li,Yen-Wei Chen*

Main category: cs.CV

TL;DR: 提出R-TNAG框架，通过黎曼流形映射、时间感知神经常微分方程和注意力机制，解决不规则采样纵向MRI数据中阿尔茨海默病进展建模的挑战。

- Motivation: 临床检查的不确定性导致纵向成像数据观察间隔不规则，现有欧几里得空间模型无法充分捕捉不规则采样纵向图像的内在连续性和非线性几何结构。
- Method: 将高维sMRI特征投影到流形空间以保持疾病进展的内在几何结构；使用时间感知神经常微分方程建模观测间潜在状态的连续演化；采用注意力机制黎曼门控循环单元自适应整合历史和当前信息处理不规则间隔。
- Result: 在疾病状态预测和认知评分回归任务中一致优于最先进模型；消融研究验证各模块的互补作用；在不同序列长度和缺失数据率下表现稳定，具有强时间泛化能力；跨数据集验证确认其鲁棒性和临床适用性。
- Conclusion: R-TNAG框架通过联合设计提高了时间一致性，在不规则采样条件下实现了稳健的AD轨迹预测，在多样化临床环境中具有良好应用前景。


### [107] [Map-World: Masked Action planning and Path-Integral World Model for Autonomous Driving](https://arxiv.org/abs/2511.20156)
*Bin Hu,Zijian Lu,Haicheng Liao,Chengran Yuan,Bin Rao,Yongkang Li,Guofa Li,Zhiyong Cui,Cheng-zhong Xu,Zhenning Li*

Main category: cs.CV

TL;DR: MAP-World是一个无先验的多模态规划框架，通过掩码动作规划和路径加权世界模型处理自动驾驶中的多模态未来预测，无需手工锚点或强化学习。

- Motivation: 现有自动驾驶规划系统通常依赖手工锚点或强化学习来选择单一最佳模式进行训练和控制，这会丢弃替代未来信息并复杂化优化过程。
- Method: 结合掩码动作规划（将未来自车运动视为掩码序列补全）和路径加权世界模型（基于候选轨迹展开未来BEV语义），使用轨迹概率作为离散路径权重计算语义损失。
- Result: 在NAVSIM数据集上，该方法与基于锚点的方法性能相当，并在基于世界模型的方法中达到最先进性能，同时避免强化学习并保持实时推理延迟。
- Conclusion: MAP-World框架能够从完整可信未来分布中学习，无需锚点库或教师策略，实现了高效的多模态规划。


### [108] [SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery](https://arxiv.org/abs/2511.20157)
*Da Li,Ji-Ping Jin,Xuanlong Yu,Wei Liu,Xiaodong Cun,Kai Chen,Rui Fan,Jiangang Kong,Shen Xi*

Main category: cs.CV

TL;DR: SKEL-CF是一个从粗到细的框架，用于估计SKEL参数，通过转换器编码器-解码器架构逐步优化相机和SKEL参数，在MOYO数据集上显著优于之前的SOTA方法。

- Motivation: 现有的参数化3D人体模型如SMPL在人体姿态和形状估计方面取得了显著进展，但其简化的运动学限制了生物力学的真实性。SKEL模型通过重新绑定SMPL与解剖学准确的骨骼解决了这一限制，但直接估计SKEL参数仍然具有挑战性。
- Method: 提出SKEL-CF框架，采用基于转换器的编码器-解码器架构。编码器预测粗略的相机和SKEL参数，解码器在连续层中逐步优化这些参数。将现有的SMPL数据集4DHuman转换为SKEL对齐版本4DHuman-SKEL，为SKEL估计提供高质量训练数据。
- Result: 在具有挑战性的MOYO数据集上，SKEL-CF实现了85.0 MPJPE / 51.4 PA-MPJPE，显著优于之前的SKEL-based SOTA方法HSMR（104.5 / 79.6）。
- Conclusion: SKEL-CF建立了一个可扩展且解剖学忠实的人体运动分析框架，弥合了计算机视觉和生物力学之间的差距。


### [109] [Harmonious Parameter Adaptation in Continual Visual Instruction Tuning for Safety-Aligned MLLMs](https://arxiv.org/abs/2511.20158)
*Ziqi Wang,Chang Che,Qi Wang,Hui Ma,Zenglin Shi,Cees G. M. Snoek,Meng Wang*

Main category: cs.CV

TL;DR: 本文提出Harmonious Parameter Adaptation (HPA)框架，用于解决安全对齐多模态大语言模型在持续视觉指令调优中的安全性和任务性能平衡问题。

- Motivation: 现有持续视觉指令调优研究主要关注未安全对齐的模型，忽略了真实世界多模态大语言模型需要安全机制来降低潜在风险。在持续适应过程中，模型不仅遭受任务遗忘，还表现出安全性退化。
- Method: 提出HPA后训练框架，包括基于焦点的参数分区、和谐平衡的参数选择和正交参数调整。将参数按关注安全或任务性能分区，从平衡角度选择要保留的焦点参数，并对参数更新施加正交约束以减轻灾难性遗忘。
- Result: 在CVIT基准和安全评估数据集上的广泛实验表明，HPA比现有基线更好地保持高安全性并减轻遗忘。
- Conclusion: HPA框架有效解决了安全对齐多模态大语言模型在持续视觉指令调优中的安全性和任务性能平衡挑战。


### [110] [While recognizing actions, LMMs struggle to detect core interaction events](https://arxiv.org/abs/2511.20162)
*Daniel Harari,Michael Sidorov,Liel David,Chen Shterental,Abrham Kahsay Gebreselasie,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 该研究评估了大型多模态模型在视频中定位物理交互事件的能力，发现虽然模型能描述物体和动作，但无法准确识别交互开始/结束的时刻和位置。

- Motivation: 探索大型多模态模型是否真正将语义理解建立在视觉输入基础上，特别是在手与物体交互的动态场景中。
- Method: 创建了首个大规模数据集（20K+标注交互），使用250名人工标注者标记接触和分离事件，然后测试两个LMM模型在短视频中定位这些事件的能力。
- Result: 模型能可靠地命名目标物体、识别动作并提供连贯推理，但一致性地无法识别交互开始/结束的帧，也无法在场景中定位事件。
- Conclusion: 模型缺乏感知基础，无法精确定义交互的物理接触时刻和位置，这表明它们对动态场景的深层理解存在局限。


### [111] [ADNet: A Large-Scale and Extensible Multi-Domain Benchmark for Anomaly Detection Across 380 Real-World Categories](https://arxiv.org/abs/2511.20169)
*Hai Ling,Jia Guo,Zhulin Tao,Yunkang Cao,Donglin Di,Hongyan Xu,Xiu Su,Yang Song,Lei Fan*

Main category: cs.CV

TL;DR: 提出了ADNet，一个包含380个类别的大规模多领域异常检测基准，揭示了现有方法在扩展到多类别时的可扩展性挑战，并提出了Dinomaly-m方法来解决这个问题。

- Motivation: 现有的异常检测基准（如MVTec-AD仅15个类别）覆盖范围有限，限制了跨上下文泛化和可扩展性的评估。需要构建一个更全面的大规模基准来推动异常检测领域的发展。
- Method: 1）构建ADNet基准：从49个公开数据集中聚合380个类别，包含196,294张RGB图像，提供标准化像素级标注和结构化文本描述；2）提出Dinomaly-m方法：基于上下文指导的专家混合扩展，在不增加推理成本的情况下扩展解码器容量。
- Result: 实验显示可扩展性挑战：现有SOTA方法在单类别设置下达到90.6% I-AUROC，但在扩展到380个类别时降至78.5%。Dinomaly-m方法达到83.2% I-AUROC和93.1% P-AUROC，优于现有方法。
- Conclusion: ADNet作为标准化可扩展基准，支持社区在不同领域扩展异常检测数据集，为未来异常检测基础模型提供可扩展基础。


### [112] [Realizing Fully-Integrated, Low-Power, Event-Based Pupil Tracking with Neuromorphic Hardware](https://arxiv.org/abs/2511.20175)
*Federico Paredes-Valles,Yoshitaka Miyatani,Kirk Y. W. Scheper*

Main category: cs.CV

TL;DR: 首个电池供电的可穿戴瞳孔中心追踪系统，结合事件视觉传感和神经形态处理，实现100Hz双目追踪，单眼功耗低于5mW

- Motivation: 眼动追踪应用广泛，但实现鲁棒、高频且超低功耗的可穿戴追踪系统仍具挑战。事件视觉传感器虽具微秒级分辨率和稀疏数据流，但缺乏完全集成的低功耗实时处理方案
- Method: 在商用Speck2f系统芯片上结合事件传感和神经形态处理，采用新型量化不确定性的脉冲神经网络和门控时序解码，配合系统部署机制弥合现实差距
- Result: 验证了多用户数据集上的性能，展示了双神经形态设备原型，实现100Hz鲁棒双目瞳孔追踪，单眼平均功耗低于5mW
- Conclusion: 端到端神经形态计算能够实现实用的、始终在线的眼动追踪，为下一代高能效可穿戴系统奠定基础


### [113] [Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis](https://arxiv.org/abs/2511.20186)
*Mohammad Mahdi,Yuqian Fu,Nedko Savov,Jiancheng Pan,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: Exo2EgoSyn是基于WAN 2.2的跨视角视频生成框架，能够从第三人称视角生成第一人称视角视频，无需从头训练。

- Motivation: 现有的基础视频生成模型如WAN 2.2虽然具有强大的文本和图像条件合成能力，但仅限于同视角生成。需要开发能够实现外中心到自我中心(Exo2Ego)跨视角视频合成的能力。
- Method: 框架包含三个关键模块：EgoExo-Align实现外中心与自我中心第一帧表示的潜在空间对齐；MultiExoCon聚合多视角外中心视频作为统一条件信号；PoseInj注入相对相机姿态信息指导跨视角的几何感知合成。
- Result: 在ExoEgo4D数据集上的实验验证了Exo2EgoSyn显著改善了Ego2Exo合成效果，实现了从第三人称观察生成高保真的自我中心视角视频。
- Conclusion: 该工作为基础模型的可扩展跨视角视频生成开辟了新途径，源代码和模型将公开发布。


### [114] [SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA](https://arxiv.org/abs/2511.20190)
*Haibin He,Qihuang Zhong,Juhua Liu,Bo Du,Peng Wang,Jing Zhang*

Main category: cs.CV

TL;DR: 提出了SFA框架，首个基于Video-LLM的训练无关方法，用于视频文本视觉问答任务，通过自适应扫描视频帧、选择性关注关键区域并直接放大，引导模型注意力到关键线索，在多个数据集上达到最先进水平。

- Motivation: 视频文本视觉问答任务面临重大挑战，需要模型准确感知和理解视频中不同尺度、方向和清晰度的场景文本，同时有效整合时间和语义上下文来生成精确答案，并需要识别问题相关文本线索并过滤冗余信息。
- Method: SFA框架基于人类回答问题过程，通过自适应扫描视频帧、选择性关注关键区域并直接放大这些区域，有效引导Video-LLM的注意力到关键线索。
- Result: SFA在多个公共Video TextVQA数据集上取得了新的最先进结果，大幅超越先前方法，证明了其有效性和泛化能力。
- Conclusion: SFA作为首个Video-LLM基础的训练无关框架，通过模拟人类认知过程，在视频文本视觉问答任务中表现出色，为相关研究提供了新的方向。


### [115] [GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering](https://arxiv.org/abs/2511.20201)
*Dionysia Danai Brilli,Dimitrios Mallis,Vassilis Pitsikalis,Petros Maragos*

Main category: cs.CV

TL;DR: GHR-VQA是一个基于场景图的人为中心的视频问答框架，通过构建视频级图结构来捕捉人类-物体交互，使用图神经网络处理并集成问题特征进行分层推理。

- Motivation: 传统基于像素的方法难以捕捉视频中复杂的人类-物体交互关系，需要更结构化、可解释的方法来理解视频内容。
- Method: 将每帧表示为场景图，跨帧连接人类节点到全局根节点形成视频级图，使用GNN处理图结构生成上下文感知嵌入，与问题特征在分层网络中集成。
- Result: 在AGQA数据集上验证，性能显著提升，物体关系推理比现有最佳方法提高了7.3%。
- Conclusion: 基于人类根节点的显式图结构增强了可解释性，能够更深入地理解时空动态，在视频问答任务中表现出色。


### [116] [Robust 3D Brain MRI Inpainting with Random Masking Augmentation](https://arxiv.org/abs/2511.20202)
*Juexin Zhang,Ying Weng,Ke Chen*

Main category: cs.CV

TL;DR: 本文介绍了一个在BraTS-Inpainting 2025挑战赛中获胜的深度学习框架，用于在3D脑部MRI扫描中合成健康组织，以减轻数据集偏见。

- Motivation: 解决脑肿瘤MRI定量分析中深度学习模型受数据集偏见限制的问题，通过合成健康组织来改善模型性能。
- Method: 采用U-Net架构训练合成受损区域的修复，并结合随机掩码增强策略来提高泛化能力。
- Result: 在验证集上获得SSIM 0.873±0.004、PSNR 24.996±4.694、MSE 0.005±0.087；在最终测试集上获得SSIM 0.919±0.088、PSNR 26.932±5.057、RMSE 0.052±0.026，在BraTS-Inpainting 2025挑战赛中排名第一。
- Conclusion: 该方法在脑部MRI修复任务中表现出色，超越了2023和2024年的获胜方案，证明了其在减轻数据集偏见方面的有效性。


### [117] [OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation](https://arxiv.org/abs/2511.20211)
*Hao Yu,Jiabo Zhan,Zile Wang,Jinglin Wang,Huaisong Zhang,Hongyu Li,Xinrui Chen,Yongxian Wei,Chun Yuan*

Main category: cs.CV

TL;DR: OmniAlpha是首个统一的、多任务的序列到序列RGBA图像生成和编辑框架，通过创新的MSRoPE-BiL架构和AlphaLayers数据集，在21个多样化任务上超越专业模型。

- Motivation: 生成模型在RGB合成方面表现出色，但实际应用需要RGBA操作。现有模型要么是专业但缺乏通用性的单任务模型，要么是局限于RGB领域的统一框架。
- Method: 提出OmniAlpha框架，采用带有双向可扩展层轴的MSRoPE-BiL RoPE方法的DiT骨干网络，能够并发处理多个输入和目标RGBA层。构建AlphaLayers数据集（1000个高质量多层三元组）进行联合训练。
- Result: 在21个多样化任务上持续超越专业基线模型，在AIM-500上实现无掩码抠图的SAD相对降低84.8%，在层条件完成任务中赢得超过90%的人类偏好。
- Conclusion: 统一的多任务模型可以学习到更优的RGBA共享表示，为更强大的层感知生成系统铺平道路。


### [118] [Text-guided Controllable Diffusion for Realistic Camouflage Images Generation](https://arxiv.org/abs/2511.20218)
*Yuhang Qian,Haiyan Chen,Wentong Li,Ningzhong Liu,Jie Qin*

Main category: cs.CV

TL;DR: 提出CT-CIG方法，通过可控文本引导生成逻辑合理的伪装图像，利用大视觉语言模型标注高质量文本提示，并结合频率交互细化模块提升伪装效果。

- Motivation: 现有方法在生成伪装图像时往往忽视伪装物体与背景环境之间的逻辑关系，导致结果不自然。
- Method: 设计伪装揭示对话机制标注数据集，微调Stable Diffusion并加入轻量控制器指导物体位置和形状，使用频率交互细化模块捕获高频纹理特征。
- Result: 通过CLIPScore评估和伪装效果测试，证明生成的文本提示语义对齐，并能产生逼真的伪装图像。
- Conclusion: CT-CIG方法能够生成逻辑合理且视觉效果自然的伪装图像，在语义对齐和伪装效果方面表现优异。


### [119] [Patch-Level Glioblastoma Subregion Classification with a Contrastive Learning-Based Encoder](https://arxiv.org/abs/2511.20221)
*Juexin Zhang,Qifeng Zhong,Ying Weng,Ke Chen*

Main category: cs.CV

TL;DR: 该研究针对胶质母细胞瘤的异质性挑战，开发了基于Vision Transformer的深度学习模型，在BraTS-Path 2025挑战赛中获得了第二名。

- Motivation: 胶质母细胞瘤具有显著的分子和病理异质性，这给诊断和患者分层带来了困难。传统组织病理学评估仍是标准方法，但深度学习为全切片图像的客观自动化分析提供了有前景的途径。
- Method: 在BraTS-Path 2025挑战赛中，我们开发了一种方法，在官方训练数据集上微调预训练的Vision Transformer编码器，并配备专用的分类头。
- Result: 在线验证集上的表现：马修斯相关系数0.7064，F1分数0.7676。最终测试集上的表现：马修斯相关系数0.6509，F1分数0.5330，在BraTS-Pathology 2025挑战赛中获得了第二名。
- Conclusion: 我们的结果为基于ViT的组织病理学分析建立了坚实的基线，未来工作将重点关注弥合在未见验证数据上观察到的性能差距。


### [120] [V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs](https://arxiv.org/abs/2511.20223)
*Sen Nie,Jie Zhang,Jianxin Yan,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: V-Attack是一种针对大型视觉语言模型的对抗攻击方法，通过操作transformer注意力块中的值特征(V)来实现精确的局部语义操控，相比现有方法平均提升36%的攻击成功率。

- Motivation: 现有对抗攻击方法在操控LVLMs图像语义时缺乏可控性，无法精确操纵特定概念的语义，这源于patch-token表示中的语义纠缠问题。
- Method: 提出V-Attack方法，包含两个核心组件：自值增强模块来优化V的语义丰富度，以及文本引导的值操控模块利用文本提示定位源概念并优化至目标概念。
- Result: 在LLaVA、InternVL、DeepseekVL和GPT-4o等多种LVLMs上的广泛实验表明，V-Attack相比最先进方法平均提升36%的攻击成功率。
- Conclusion: V-Attack通过绕过纠缠的patch特征，实现了高效的语义控制，揭示了现代视觉语言理解中的关键漏洞。


### [121] [HistoSpeckle-Net: Mutual Information-Guided Deep Learning for high-fidelity reconstruction of complex OrganAMNIST images via perturbed Multimode Fibers](https://arxiv.org/abs/2511.20245)
*Jawaria Maqbool,M. Imran Cheema*

Main category: cs.CV

TL;DR: 提出HistoSpeckle-Net深度学习架构，用于从多模光纤散斑重建结构丰富的医学图像，通过分布感知学习策略和基于直方图的互信息损失，在有限数据下实现高保真重建。

- Motivation: 现有多模光纤成像方法主要针对简单数据集，在复杂真实世界成像任务中适用性有限，且通常需要大量数据，这在处理多样复杂图像时尤为挑战。
- Method: 开发光学设置构建临床相关数据集，提出HistoSpeckle-Net架构，包含直方图计算单元和三尺度特征细化模块，使用基于直方图的互信息损失和多尺度SSIM损失。
- Result: 在复杂OrganAMNIST数据集上，HistoSpeckle-Net比U-Net和Pix2Pix等基线模型实现更高保真度，在有限训练样本和不同光纤弯曲条件下均表现优异。
- Conclusion: 通过有效重建复杂解剖特征并减少数据需求，HistoSpeckle-Net使多模光纤成像更接近实际临床环境部署。


### [122] [Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation](https://arxiv.org/abs/2511.20250)
*Daniel Kienzle,Katja Ludwig,Julian Lorenz,Shin'ichi Satoh,Rainer Lienhart*

Main category: cs.CV

TL;DR: 提出了一种新颖的两阶段流水线方法，用于从单目视频中精确重建乒乓球3D轨迹和旋转，通过分离前端感知和后端2D到3D提升任务来解决合成数据泛化问题。

- Motivation: 现有基于合成数据训练的方法难以泛化到真实世界视频中的噪声和不完美检测，主要因为缺乏真实世界视频的3D地面真值轨迹和旋转标注。
- Method: 采用两阶段流水线：前端感知任务使用新创建的TTHQ数据集进行2D监督训练，后端提升网络仅在物理正确的合成数据上训练，并重新设计以抵抗真实世界常见伪影。
- Result: 通过集成球检测器和桌面关键点检测器，将概念验证的提升方法转化为实用、鲁棒且高性能的端到端3D乒乓球轨迹和旋转分析应用。
- Conclusion: 该方法成功解决了从单目视频精确重建乒乓球3D运动的关键挑战，为实际应用提供了可行的解决方案。


### [123] [PromptMoG: Enhancing Diversity in Long-Prompt Image Generation via Prompt Embedding Mixture-of-Gaussian Sampling](https://arxiv.org/abs/2511.20251)
*Bo-Kai Ruan,Teng-Fang Hsiao,Ling Lo,Yi-Lun Wu,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 本文研究了长提示词在文本到图像生成中的保真度-多样性困境，提出了PromptMoG方法通过混合高斯采样增强多样性，并建立了LPD-Bench评估基准。

- Motivation: 当前文本到图像生成模型在处理长提示词时，虽然能提高保真度，但会显著降低输出多样性，导致重复和缺乏创造性的结果。
- Method: 提出PromptMoG训练免费方法，在嵌入空间中对提示词嵌入进行混合高斯采样，增加采样熵以增强多样性；同时建立LPD-Bench评估基准。
- Result: 在SD3.5-Large、Flux.1-Krea-Dev、CogView4和Qwen-Image四个先进模型上的实验表明，PromptMoG能持续提升长提示词生成的多样性而不损失语义准确性。
- Conclusion: PromptMoG方法有效解决了长提示词生成中的保真度-多样性权衡问题，为文本到图像生成提供了实用的多样性增强方案。


### [124] [Zoo3D: Zero-Shot 3D Object Detection at Scene Level](https://arxiv.org/abs/2511.20253)
*Andrey Lemeshko,Bulat Gabdullin,Nikita Drozdov,Anton Konushin,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: Zoo3D是首个无需训练的3D物体检测框架，通过2D实例掩码的图聚类构建3D边界框，并使用开放词汇模块分配语义标签。

- Motivation: 现实环境需要能够识别多样化、未见过的物体的模型，而封闭集方法在此方面存在主要限制。现有开放词汇3D检测器虽然放宽了标注要求，但仍依赖训练场景。
- Method: 通过2D实例掩码的图聚类构建3D边界框，使用具有最佳视图选择和视图一致性掩码生成的新开放词汇模块分配语义标签。提供两种模式：零样本Zoo3D₀（完全无需训练）和自监督Zoo3D₁（在Zoo3D₀生成的伪标签上训练类无关检测器）。
- Result: 在ScanNet200和ARKitScenes基准测试中，Zoo3D₀和Zoo3D₁在开放词汇3D物体检测方面均达到最先进结果。零样本Zoo3D₀甚至优于所有现有的自监督方法。
- Conclusion: 展示了无需训练、即插即用方法在真实世界3D理解中的强大能力和适应性。


### [125] [XiCAD: Camera Activation Detection in the Da Vinci Xi User Interface](https://arxiv.org/abs/2511.20254)
*Alexander C. Jenke,Gregor Just,Claas de Boer,Martin Wagner,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: 开发了一个基于ResNet18的轻量级管道，用于自动检测达芬奇Xi手术系统中摄像头图块的位置和激活状态，为下游手术数据分析任务提供支持。

- Motivation: 机器人辅助微创手术仅依赖内窥镜视频作为术中视觉反馈，检测摄像头激活状态可提供有价值的元数据，支持工具追踪、技能评估等下游任务。
- Method: 使用ResNet18卷积神经网络构建轻量级管道，在SurgToolLoc数据集上进行微调，并在三个公共数据集上评估，包含超过70,000帧。
- Result: 模型在二分类检测活跃摄像头方面F1分数达到0.993-1.000，在所有情况下正确定位摄像头图块，无错误多摄像头检测。
- Conclusion: 该管道能够可靠地实时提取手术视频中的摄像头激活元数据，促进自动预处理和分析，所有代码、模型和标注已公开。


### [126] [The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation](https://arxiv.org/abs/2511.20256)
*Weijia Mao,Hao Chen,Zhenheng Yang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: Adv-GRPO是一个对抗性强化学习框架，使用视觉基础模型提供密集视觉奖励，解决了传统标量奖励无法捕捉人类感知和易受奖励攻击的问题。

- Motivation: 当前强化学习方法依赖预训练的偏好模型输出标量奖励来近似人类偏好，但这些奖励往往无法准确捕捉人类感知，且容易受到奖励攻击（高分不代表更好的图像）。
- Method: 提出Adv-GRPO框架，通过对抗性奖励迭代更新奖励模型和生成器。使用参考图像作为正样本监督奖励模型，采用视觉基础模型（如DINO）提供丰富的视觉奖励而非单一标量。
- Result: 在人类评估中，Adv-GRPO在图像质量和美学方面分别以70.0%和72.4%的胜率优于Flow-GRPO和SD3。该方法在图像质量、美学和任务特定指标上均取得一致提升。
- Conclusion: 结合参考样本和基础模型奖励能够实现分布迁移和灵活的风格定制，生成的图像质量更高，且能有效避免奖励攻击问题。


### [127] [Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization](https://arxiv.org/abs/2511.20258)
*Xiaohan Wang,Zhangtao Cheng,Ting Zhong,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: MBCD是一个多模态领域泛化框架，通过自适应模态丢弃、梯度一致性约束和权重平均教师蒸馏，解决传统权重平均在多模态场景中因优化速度差异导致的模态不平衡问题。

- Motivation: 传统权重平均方法在多模态领域泛化中面临挑战：不同模态优化速度差异导致早期偏向快速收敛模态，抑制了较慢但互补模态的贡献，阻碍有效模态融合并导向更尖锐、泛化性差的损失最小值。
- Method: 1. 学生模型中自适应模态丢弃以抑制早期对主导模态的偏向；2. 梯度一致性约束对齐单模态分支与融合表示的学习信号；3. 基于权重平均的教师进行跨模态蒸馏，将融合知识转移到各单模态分支。
- Result: 在多模态领域泛化基准测试中，MBCD持续优于现有方法，在多样未见域上实现了卓越的准确性和鲁棒性。
- Conclusion: MBCD保留了权重平均诱导平坦损失表面的优势，同时克服了其在多模态上下文中的缺点，通过协调优化和跨模态蒸馏实现了更好的泛化性能。


### [128] [Advancing Image Classification with Discrete Diffusion Classification Modeling](https://arxiv.org/abs/2511.20263)
*Omer Belhasin,Shelly Golan,Ran El-Yaniv,Michael Elad*

Main category: cs.CV

TL;DR: 提出DiDiCM框架，利用扩散过程建模类别标签的后验分布，在不确定性条件下提升图像分类性能

- Motivation: 传统分类方法在输入图像被破坏或训练数据有限的高不确定性条件下表现不佳，需要更鲁棒的分类建模方法
- Method: 使用离散扩散过程建模类别标签的条件后验分布，支持在类别概率或离散标签上进行扩散预测
- Result: 在ImageNet数据集上，少量扩散迭代即可获得比基线更高的分类准确率，任务越具挑战性时准确率提升越大
- Conclusion: DiDiCM框架在不确定性条件下显著优于标准分类器，为图像分类提供了更灵活和鲁棒的解决方案


### [129] [DRL-Guided Neural Batch Sampling for Semi-Supervised Pixel-Level Anomaly Detection](https://arxiv.org/abs/2511.20270)
*Amirhossein Khadivi Noghredeh,Abdollah Safari,Fatemeh Ziaeetabar,Firoozeh Haghighi*

Main category: cs.CV

TL;DR: 提出了一种半监督深度强化学习框架，通过自适应采样、自编码器和预测器的协同工作，在工业视觉检测中实现高效异常检测，仅需少量标注数据即可准确识别细微缺陷。

- Motivation: 工业视觉检测中缺陷样本稀缺，现有无监督重建方法容易过拟合且难以检测细微缺陷，需要一种能够有效利用有限标注数据的解决方案。
- Method: 结合深度强化学习的半监督框架，包含神经批处理采样器（平衡探索与利用）、自编码器（生成损失分布图）和预测器（在损失分布空间进行分割）。
- Result: 在MVTec AD数据集上，相比现有最优方法，F1_max平均提升0.15，AUC提升0.06，最佳情况下F1_max最大提升0.37，同时保持低复杂度。
- Conclusion: 该方法能够有效学习正常和缺陷模式，在有限标注数据下实现更高的检测精度和更好的细微异常定位能力。


### [130] [VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs](https://arxiv.org/abs/2511.20272)
*Tianxiang Jiang,Sheng Xia,Yicheng Xu,Linquan Wu,Xiangyu Zeng,Limin Wang,Yu Qiao,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了VKnowU基准测试来评估多模态大语言模型的视觉知识能力，发现现有模型在理解物理和社会原理方面仍落后于人类，并提出了VideoKnow+模型来改进这一能力。

- Motivation: 当前多模态大语言模型虽然擅长物体识别，但缺乏对人类世界物理和社会原理的直观理解能力，这种视觉知识能力是连接感知和推理的关键桥梁，但在现有研究中尚未得到充分探索。
- Method: 构建了包含1,680个问题和1,249个视频的VKnowU基准测试，涵盖8种核心视觉知识类型；提出了VideoKnow+模型，采用See-Think-Answer范式，并使用带有视觉知识奖励的强化学习来显式整合视觉知识。
- Result: 评估23个最先进的多模态大语言模型显示，领先模型仍落后于人类表现，特别是在世界中心型知识方面存在显著差距；VideoKnow+在VKnowU上实现了+3.7%的提升，并在多个基准测试上取得一致改进。
- Conclusion: 视觉知识是开发更通用多模态大语言模型的关键基石，这些模型不仅能够看到，还能真正理解我们的物理和社会世界。


### [131] [ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis](https://arxiv.org/abs/2511.20274)
*Advik Sinha,Saurabh Atreya,Aashutosh A,Sk Aziz Ali,Abhijit Das*

Main category: cs.CV

TL;DR: ScenarioCLIP是一个新的CLIP模型，专门用于处理包含多个对象和关系的复杂场景图像，通过显式建模对象间关系来改进场景分析。

- Motivation: 现有CLIP模型主要处理单对象分类或简单检索任务，无法有效建模真实场景中丰富的组合结构（多对象和关系）。虽然最新方法通过挖掘负样本和改进文本提示来提升性能，但仍局限于预定义类别列表，缺乏对关系结构的显式建模。
- Method: 提出ScenarioCLIP模型，接受输入文本、基础关系、输入图像以及突出关系的聚焦区域。模型在精心策划的场景数据上进行预训练，然后在特定下游任务上进行微调。为解决领域特定数据集缺乏的问题，构建了一个新数据集，通过扩展现有室内外场景数据集的图像-文本对，使用语言模型流水线来基础化动作、对象和关系。
- Result: ScenarioCLIP在各种领域特定任务上表现出强大的零样本和微调性能，在多个场景基准任务中优于许多基线方法。
- Conclusion: ScenarioCLIP通过显式建模对象间关系，有效提升了复杂场景的理解能力，为场景分析任务提供了新的解决方案。


### [132] [DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion](https://arxiv.org/abs/2511.20278)
*Yinghui Li,Qianyu Zhou,Di Shao,Hao Yang,Ye Zhu,Richard Dazeley,Xuequan Lu*

Main category: cs.CV

TL;DR: 提出了DAPointMamba框架，首次将状态空间模型应用于领域自适应点云补全任务，解决了现有方法感受野有限和计算复杂度高的问题

- Motivation: 现有领域自适应点云补全方法使用CNN或视觉Transformer存在感受野有限或二次复杂度的问题，直接应用状态空间模型会破坏空间拓扑和局部几何特征，且缺乏领域无关表示设计
- Method: 提出DAPointMamba框架，包含三个模块：跨域补丁级扫描引入补丁级几何对应关系；跨域空间SSM对齐通过相似性调制增强空间一致性；跨域通道SSM对齐通过特征通道交错对齐解决全局语义差距
- Result: 在合成和真实世界基准测试上的广泛实验表明，DAPointMamba在计算复杂度和推理延迟更低的情况下优于最先进方法
- Conclusion: DAPointMamba框架在领域自适应点云补全中展现出强大的跨域适应能力，具有全局感受野和高效线性复杂度的优势


### [133] [SelfMOTR: Revisiting MOTR with Self-Generating Detection Priors](https://arxiv.org/abs/2511.20279)
*Fabian Gülhan,Emil Mededovic,Yuli Wu,Johannes Stegmaier*

Main category: cs.CV

TL;DR: SelfMOTR是一种新颖的跟踪变换器，利用自生成的检测先验来解决端到端跟踪中检测性能差和检测与关联冲突的问题。

- Motivation: 尽管基于变换器的端到端跟踪方法有所进展，但检测性能差以及检测与关联在联合架构中的冲突仍然是关键问题。现有方法通过使用高级去噪策略或引入外部检测器的先验知识来缓解这些问题。
- Method: 通过深入分析和消融研究，揭示了MOTR类模型隐藏的检测能力，并提出了利用这些能力的实用工具集。SelfMOTR依赖自生成的检测先验，无需外部检测器。
- Result: 在DanceTrack数据集上，SelfMOTR实现了强大的性能，与最近最先进的端到端跟踪方法相竞争。
- Conclusion: SelfMOTR证明了MOTR类模型具有隐藏的检测能力，通过自生成的检测先验可以有效提升端到端跟踪性能。


### [134] [Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement](https://arxiv.org/abs/2511.20280)
*Yang Liu,Xilin Zhao,Peisong Wen,Siran Dai,Qingming Huang*

Main category: cs.CV

TL;DR: 提出了一个迭代自优化框架，利用大语言模型和视觉语言模型为视频生成提供物理感知指导，通过多模态思维链过程基于物理不一致性反馈优化提示，提升生成质量。

- Motivation: 当前视频生成模型在视觉质量方面取得显著进展，但在遵循真实世界物理原理方面仍存在困难，需要提升物理一致性。
- Method: 采用训练免费、即插即用的迭代自优化框架，通过多模态思维链过程分析物理不一致性并优化生成提示。
- Result: 在PhyIQ基准测试中，物理IQ分数从56.31提升至62.38，证明了方法的有效性。
- Conclusion: 这项工作为物理一致性视频生成提供了初步探索，为未来研究提供了有价值的见解。


### [135] [Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations](https://arxiv.org/abs/2511.20295)
*Chao Wang,Chengan Che,Xinyue Chen,Sophia Tsoka,Luis C. Garcia-Peraza-Herrera*

Main category: cs.CV

TL;DR: 提出了BTTF优化框架，用于为视频分类器生成反事实解释，解决了现有图像反事实方法在视频中缺乏时间一致性和物理合理性的问题。

- Motivation: 现有反事实解释方法主要针对图像分类器设计，无法生成时间一致、运动平滑且物理合理的视频反事实解释，视频分类器的反事实解释研究尚不充分。
- Method: BTTF框架包含两个关键特征：1) 基于输入视频第一帧的条件化初始潜在噪声优化方案；2) 在输入视频附近搜索反事实视频的两阶段优化策略，采用渐进优化策略加速收敛。
- Result: 在Shape-Moving、MEAD和NTU RGB+D等视频数据集上的实验表明，BTTF能有效生成有效、视觉相似且逼真的反事实视频，为分类器决策机制提供具体洞察。
- Conclusion: BTTF框架成功解决了视频反事实解释的挑战，生成的解释具有时间一致性、物理合理性和视觉逼真度，为理解视频分类器决策提供了可靠工具。


### [136] [Prompting Lipschitz-constrained network for multiple-in-one sparse-view CT reconstruction](https://arxiv.org/abs/2511.20296)
*Baoshun Shi,Ke Jiang,Qiusheng Lian,Xinran Yu,Huazhu Fu*

Main category: cs.CV

TL;DR: 提出PromptCT框架，通过Lipschitz约束网络LipNet和显式提示模块，实现多视图稀疏CT重建的单一模型解决方案，降低存储成本并确保算法收敛。

- Motivation: 解决深度展开算法中先验网络难以满足Lipschitz约束的挑战，以及多视图设置下需要训练多个模型导致存储成本高的问题。
- Method: 开发LipNet作为可证明满足Lipschitz约束的先验网络，集成显式提示模块提供不同稀疏采样设置的判别知识，构建PromptCT深度展开框架。
- Result: 在模拟和真实数据实验中，PromptCT在多项稀疏CT重建任务中优于基准算法，实现更高质量重建且存储成本更低。
- Conclusion: 提出的PromptCT框架通过理论保证的Lipschitz约束网络和提示机制，成功解决了多视图稀疏CT重建的存储和收敛问题，具有实际临床应用价值。


### [137] [CrossEarth-Gate: Fisher-Guided Adaptive Tuning Engine for Efficient Adaptation of Cross-Domain Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2511.20302)
*Shilei Cao,Ziyang Gong,Hehai Lin,Yang Liu,Jiashun Cheng,Xiaoxing Hu,Haoyuan Liang,Guowen Li,Chengwei Qin,Hong Cheng,Xue Yang,Juepeng Zheng,Haohuan Fu*

Main category: cs.CV

TL;DR: CrossEarth-Gate是一种针对遥感数据的参数高效微调方法，通过建立空间、语义和频率模块工具箱，并结合Fisher信息指导的自适应选择机制，动态激活关键模块来处理多方面的领域差距。

- Motivation: 现有的专用PEFT方法在处理大规模地球观测任务时经常失败，因为它们无法完全处理遥感数据中固有的多方面和不可预测的领域差距（如空间、语义和频率偏移）。
- Method: 1. 建立包含空间、语义和频率模块的遥感模块工具箱；2. 开发基于Fisher信息的自适应选择机制，量化每个模块的重要性并动态激活最关键的模块。
- Result: 在16个跨域遥感语义分割基准测试中，CrossEarth-Gate实现了最先进的性能。
- Conclusion: 该方法在适应效果和效率方面都表现出色，验证了其有效性和泛化能力。


### [138] [TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection](https://arxiv.org/abs/2511.20306)
*Han Guo,Chenyang Liu,Haotian Zhang,Bowen Chen,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出TaCo网络，通过时空语义联合约束增强遥感变化检测，将变化建模为双时态特征之间的语义转换，结合文本引导的转换生成器和重构约束，在多个数据集上达到SOTA性能。

- Motivation: 传统方法仅依赖掩码监督，能有效定位空间变化但缺乏对时间语义转换的约束，导致语义不一致问题。需要增强时空语义一致性。
- Method: 提出TaCo网络，包含文本引导的转换生成器（集成文本语义和视觉特征构建跨时态转换特征）和时空语义联合约束（双时态重构约束和转换约束）。
- Result: 在6个公共数据集（包括二值和语义变化检测任务）上的广泛实验表明，TaCo始终达到最先进的性能，且推理时不增加计算开销。
- Conclusion: TaCo通过时空语义联合约束有效解决了遥感变化检测中的语义不一致问题，在多个任务上实现了显著的性能提升。


### [139] [TReFT: Taming Rectified Flow Models For One-Step Image Translation](https://arxiv.org/abs/2511.20307)
*Shengqian Li,Ming Gao,Yi Liu,Zuzeng Lin,Feng Wang,Feng Dai*

Main category: cs.CV

TL;DR: TReFT是一种新方法，通过直接使用预训练RF模型预测的速度作为输出来实现一步图像翻译，解决了对抗训练中的收敛问题，实现了实时推理。

- Motivation: 现有Rectified Flow模型在图像翻译中仍依赖昂贵的多步去噪，阻碍实时应用。虽然CycleGAN-Turbo在预训练扩散模型中实现了一步翻译，但直接应用于RF模型会导致严重收敛问题。
- Method: 提出TReFT方法，直接使用预训练DiT或UNet预测的速度作为输出，引入内存高效的潜在循环一致性和身份损失，以及轻量级架构简化。
- Result: 在SD3.5和FLUX等大型预训练RF模型上应用TReFT，在多个图像翻译数据集上达到与最先进方法相当的性能，同时实现实时推理。
- Conclusion: TReFT成功驯化了Rectified Flow模型用于一步图像翻译，解决了收敛问题，在保持性能的同时实现了实时处理能力。


### [140] [IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection](https://arxiv.org/abs/2511.20319)
*Xuelin Qian,Jiaming Lu,Zixuan Wang,Wenxuan Wang,Zhongling Huang,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: IrisNet是一个基于元学习的红外小目标检测框架，通过图像到解码器的变换器动态生成解码器参数，解决了传统方法在多样化场景下的模式漂移问题。

- Motivation: 传统基于深度学习的红外小目标检测方法存在模式漂移问题，无法适应不同场景（如昼夜变化、天空/海洋/地面域）的变化，限制了检测的鲁棒性。
- Method: 提出IrisNet框架，使用图像到解码器变换器建立红外图像特征与解码器参数之间的动态映射，将参数化解码器表示为结构化2D张量，通过自注意力建模层间依赖关系，交叉注意力生成自适应解码模式，并集成高频组件增强感知能力。
- Result: 在NUDT-SIRST、NUAA-SIRST和IRSTD-1K数据集上的实验表明，IrisNet实现了最先进的性能。
- Conclusion: IrisNet通过动态适应检测策略，有效解决了红外小目标检测中的模式漂移问题，在多样化场景下表现出优越的鲁棒性和检测性能。


### [141] [AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models](https://arxiv.org/abs/2511.20325)
*Tianyi Yan,Tao Tang,Xingtai Gui,Yongkang Li,Jiasen Zhesng,Weiyao Huang,Lingdong Kong,Wencheng Han,Xia Zhou,Xueyang Zhang,Yifei Zhan,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 该论文提出了一个用于自动驾驶后训练策略优化的框架，通过构建公正世界模型来解决强化学习中的乐观偏见问题，使模型能够准确预测危险情况。

- Motivation: 端到端自动驾驶模型面临安全和处理长尾事件的挑战，而强化学习由于世界模型中的乐观偏见而难以成功应用。
- Method: 引入公正世界模型框架，使用反事实合成数据管道生成丰富的碰撞和偏离道路事件课程，将模型从被动场景补全器转变为真实预测器，并集成到闭环强化学习框架中作为内部批评器。
- Result: 在包括新的风险预见基准在内的广泛实验中，该模型在预测失败方面显著优于基线方法，在具有挑战性的模拟中大幅减少了安全违规。
- Conclusion: 教会模型预见危险是构建真正安全和智能自主智能体的关键步骤。


### [142] [3D Motion Perception of Binocular Vision Target with PID-CNN](https://arxiv.org/abs/2511.20332)
*Shi Jiazhao,Pan Pan,Shi Haotian*

Main category: cs.CV

TL;DR: 该论文训练了一个用于感知双目视觉目标三维运动信息的网络，能够实时提供三维坐标、速度和加速度，具备基本的时空感知能力。从PID角度理解神经网络拟合非线性问题的能力，设计了17层、41.3万参数的小型PID卷积神经网络，在模拟随机运动球数据集上取得了接近输入图像分辨率上限的预测精度。

- Motivation: 从PID控制理论的角度理解神经网络处理非线性问题的能力，将单层神经网络视为使用二阶差分方程和非线性来描述局部问题，多层网络通过多个这样的组合逐步将原始表示转换为期望表示。
- Method: 设计了相对较小的PID卷积神经网络（17层，41.3万参数），通过拼接和池化实现了简单实用的特征重用方法，使用模拟随机运动球数据集进行训练和测试。
- Result: 实验结果显示预测精度接近输入图像分辨率所能表示的上限，分析了实验结果和误差，以及现有不足和改进方向。
- Conclusion: 讨论了高维卷积在提高计算效率和特征空间利用率方面的优势，以及使用PID信息实现记忆和注意力机制的潜在优势。


### [143] [ShelfRectNet: Single View Shelf Image Rectification with Homography Estimation](https://arxiv.org/abs/2511.20335)
*Onur Berk Tore,Ibrahim Samil Yalciner,Server Calap*

Main category: cs.CV

TL;DR: 提出基于深度学习的单图像单应性估计框架，用于矫正货架图像，在测试集上达到1.298像素的平均角点误差，在精度和推理速度上均表现优异。

- Motivation: 解决零售领域单视角货架监控和产品对齐的挑战，传统方法在只有单一视角时难以准确估计单应性变换。
- Method: 使用ConvNeXt骨干网络增强特征表示，采用归一化坐标回归提高稳定性，并引入建模和采样合成单应性的新数据增强策略来解决数据稀缺问题。
- Result: 在测试集上达到1.298像素的平均角点误差，相比传统计算机视觉和深度学习方法，在精度和推理速度方面都表现出竞争力。
- Conclusion: 该方法为现实世界单视角矫正提供了稳健高效的解决方案，并将公开数据集ShelfRectSet和代码以促进该领域进一步研究。


### [144] [AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend](https://arxiv.org/abs/2511.20343)
*Hengyi Wang,Lourdes Agapito*

Main category: cs.CV

TL;DR: AMB3R是一个多视图前馈模型，用于度量尺度的密集3D重建，可扩展到视觉里程计和大规模运动结构任务而无需特定任务微调。

- Motivation: 利用稀疏但紧凑的体素场景表示作为后端，实现具有空间紧凑性的几何推理，解决多样化的3D视觉任务。
- Method: 采用多视图前馈模型和稀疏紧凑的体素场景表示，支持从多视图重建扩展到未标定视觉里程计和大规模运动结构。
- Result: 在相机姿态、深度和度量尺度估计、3D重建方面达到最先进性能，甚至超过基于优化的SLAM和SfM方法。
- Conclusion: AMB3R展示了单一模型在多种3D视觉任务中的通用性和优越性能，无需任务特定调整即可实现高质量重建。


### [145] [Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin](https://arxiv.org/abs/2511.20348)
*João Malheiro Silva,Andy Huynh,Tong Duy Son,Holger Caesar*

Main category: cs.CV

TL;DR: 提出仅使用相机的3D重建流程，通过多视角图像进行3D高斯泼溅重建，提取语义材质掩码，转换为带材质标签的网格表面，并分配基于物理的材质属性，用于传感器模拟。

- Motivation: 传统LiDAR-相机融合方法需要复杂校准，且对玻璃等材质处理不佳。相机方法能自然捕捉语义和纹理，但缺乏物理属性。
- Method: 使用多视角图像进行3D高斯泼溅重建，通过视觉模型提取语义材质掩码，将高斯表示转换为带材质标签的网格表面，分配物理材质属性。
- Result: 实现了与LiDAR-相机融合相当的传感器模拟保真度，同时消除了硬件复杂性和校准需求。使用LiDAR作为反射率验证的基准。
- Conclusion: 相机专用流程结合了照片级真实重建与基于物理的材质分配，为数字孪生提供了有效的传感器模拟解决方案。


### [146] [Thinking in 360°: Humanoid Visual Search in the Wild](https://arxiv.org/abs/2511.20351)
*Heyang Yu,Yinan Han,Xiangyu Zhang,Baiqiao Yin,Bowen Chang,Xiangyu Han,Xinhao Liu,Jing Zhang,Marco Pavone,Chen Feng,Saining Xie,Yiming Li*

Main category: cs.CV

TL;DR: 提出了人形视觉搜索任务，让智能体在360°全景图像中主动旋转头部来搜索物体或路径，并构建了H* Bench基准测试，在真实世界场景中评估模型性能。

- Motivation: 现有视觉搜索方法局限于静态图像，忽略了物理体现与3D世界的交互。需要开发像人类一样高效的具身视觉搜索智能体，同时避免真实世界硬件的限制。
- Method: 使用后训练技术增强开源模型Qwen2.5-VL，在H* Bench基准上进行评估。H* Bench包含交通枢纽、大型零售空间等具有挑战性的真实世界场景。
- Result: 顶级专有模型成功率仅约30%。增强后的Qwen2.5-VL在物体搜索上从14.83%提升到47.38%，路径搜索从6.44%提升到24.94%。路径搜索的较低成功率反映了其内在难度。
- Conclusion: 研究展示了一条有前景的发展路径，但量化了构建能够无缝融入人类日常生活的MLLM智能体所面临的巨大挑战，特别是需要复杂的空间常识推理能力。


### [147] [GS-Checker: Tampering Localization for 3D Gaussian Splatting](https://arxiv.org/abs/2511.20354)
*Haoliang Han,Ziyuan Luo,Jun Qi,Anderson Rocha,Renjie Wan*

Main category: cs.CV

TL;DR: 提出GS-Checker方法，用于定位3D高斯泼溅模型中的篡改区域，无需昂贵的3D标签监督，通过3D篡改属性和对比机制实现准确检测。

- Motivation: 随着3DGS编辑技术的发展，恶意篡改3D内容的风险增加，需要有效的方法来定位篡改区域以防止恶意应用。
- Method: 集成3D篡改属性到高斯参数中，设计3D对比机制比较高斯间的关键属性相似性，采用循环优化策略精炼篡改属性。
- Result: 大量实验结果表明该方法能有效定位篡改的3DGS区域。
- Conclusion: GS-Checker为3DGS模型提供了可靠的篡改定位能力，无需监督标签，具有实际应用价值。


### [148] [From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations](https://arxiv.org/abs/2511.20359)
*Zhiqing Guo,Dongdong Xi,Songlin Li,Gaobo Yang*

Main category: cs.CV

TL;DR: BoxPromptIML是一个弱监督图像篡改定位框架，通过粗粒度区域标注和知识蒸馏平衡标注成本与定位精度，使用双引导特征融合策略提升定位性能。

- Motivation: 解决图像篡改定位中标注成本与定位精度之间的权衡问题，现有全监督方法依赖密集像素级标注，而弱监督方法缺乏精确空间定位能力。
- Method: 提出粗区域标注策略降低标注成本；设计轻量级学生模型通过SAM教师模型进行知识蒸馏；采用受人类潜意识记忆机制启发的双引导特征融合模块，动态结合原型模式与实时观测线索。
- Result: 在分布内和分布外数据集上的大量实验表明，BoxPromptIML性能优于或媲美全监督模型，同时保持强泛化性、低标注成本和高效部署特性。
- Conclusion: BoxPromptIML有效平衡了标注成本与定位性能，为图像篡改定位提供了一种实用且高效的解决方案。


### [149] [VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild](https://arxiv.org/abs/2511.20366)
*Xin Ming,Yuxuan Han,Tianyu Huang,Feng Xu*

Main category: cs.CV

TL;DR: VGGTFace是一个自动化的面部几何重建方法，利用3D基础模型VGGT从多视角图像重建拓扑一致的面部几何，通过Pixel3DMM注入拓扑信息，并提出拓扑感知的捆绑调整策略。

- Motivation: 现有方法需要大量人工操作、对野外数据泛化能力不足，或受限于3D形变模型的表达能力有限。
- Method: 结合VGGT和Pixel3DMM，将像素对齐的点图转换为带拓扑的点云，提出拓扑感知的捆绑调整策略，构建拉普拉斯能量项。
- Result: 在单张RTX 4090上16视图仅需10秒完成高质量重建，在基准测试中达到最先进结果，对野外数据具有出色泛化能力。
- Conclusion: VGGTFace实现了高效、自动化的拓扑一致面部几何重建，解决了现有方法的局限性。


### [150] [FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers](https://arxiv.org/abs/2511.20390)
*Xinwan Wen,Bowen Li,Jiajun Luo,Ye Li,Zhi Wang*

Main category: cs.CV

TL;DR: FREE是一个针对扩散变换器(DiTs)的加速框架，通过特征级自回归和并行验证实现无损加速，并引入不确定性引导的松弛策略进一步提升速度。

- Motivation: DiTs虽然生成质量优秀，但需要长序列去噪过程导致推理延迟高。现有的推测推理方法在DiTs上加速效果有限，因为验证阶段的草稿准确性不足。
- Method: 分析DiTs特征动态，发现顶层变换器特征具有强时间一致性和丰富语义抽象；提出轻量级草稿器进行特征级自回归与并行验证；引入不确定性引导的松弛策略动态调整接受概率。
- Result: 在ImageNet-512²上，FREE实现最高1.86倍加速，FREE(relax)进一步达到2.25倍加速，同时保持高质量生成保真度。
- Conclusion: FREE框架通过特征级自回归和不确定性感知的松弛策略，有效解决了DiTs推理延迟问题，实现了显著加速而不损失生成质量。


### [151] [A Training-Free Approach for Multi-ID Customization via Attention Adjustment and Spatial Control](https://arxiv.org/abs/2511.20401)
*Jiawei Lin,Guanlong Jiao,Jianjin Xu*

Main category: cs.CV

TL;DR: MultiID是一个无需训练的多ID定制方法，通过ID解耦交叉注意力机制将多个身份嵌入到不同图像区域，解决了多ID定制中的复制粘贴问题和文本控制性差的问题。

- Motivation: 多ID定制比单ID定制更困难，面临两个主要挑战：训练模型从裁剪人物区域重建图像时容易产生复制粘贴问题导致质量下降，以及模型文本控制性差，无法保证生成结果与输入文本对齐。
- Method: 提出ID解耦交叉注意力机制，将不同ID嵌入注入到相应图像区域；引入局部提示、深度引导空间控制和扩展自注意力三种策略来增强生成可控性；构建IDBench基准进行评估。
- Result: 广泛的定性和定量结果表明MultiID有效解决了上述两个挑战，其性能与基于训练的多ID定制方法相当甚至更好。
- Conclusion: MultiID在无需训练的情况下成功实现了高质量的多ID定制，解决了复制粘贴和文本控制性问题，性能优于现有方法。


### [152] [Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs](https://arxiv.org/abs/2511.20410)
*Bao Tang,Shuai Zhang,Yueting Zhu,Jijun Xiang,Xin Yang,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出Trajectory-Backward Consistency Model (TBCM)，通过直接从教师模型的生成轨迹中提取潜在表示，消除了对外部训练数据的依赖，显著提高了扩散模型时间步蒸馏的效率和简化性。

- Motivation: 当前连续时间一致性蒸馏方法严重依赖训练数据和计算资源，限制了在资源受限场景中的部署和向多领域的扩展。
- Method: 提出自包含的蒸馏范式，无需VAE编码和大规模数据集，直接从教师模型生成轨迹中提取潜在表示，弥合训练和推理之间的分布差距。
- Result: 在MJHQ-30k数据集上，一步生成达到6.52 FID和28.08 CLIP分数，训练时间比Sana-Sprint减少约40%，节省大量GPU内存。
- Conclusion: TBCM展示了在不牺牲质量的情况下实现卓越效率，并揭示了连续时间一致性蒸馏中的扩散-生成空间差异，为未来蒸馏研究提供了见解。


### [153] [MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts](https://arxiv.org/abs/2511.20415)
*Zilong Huang,Jun He,Xiaobin Huang,Ziyi Xiong,Yang Luo,Junyan Ye,Weijia Li,Yiping Chen,Ting Han*

Main category: cs.CV

TL;DR: MajutsuCity是一个基于自然语言驱动的3D城市生成框架，通过四阶段流水线实现结构一致且风格多样的城市场景合成，并集成了交互式编辑代理MajutsuAgent支持对象级操作。

- Motivation: 现有方法难以平衡基于文本生成的创意灵活性与显式结构表示提供的对象级可编辑性，需要一种能同时满足风格多样性、细粒度控制和结构一致性的3D城市生成方法。
- Method: 将城市表示为可控布局、资产和材料的组合，采用四阶段流水线生成；集成MajutsuAgent支持五种对象级编辑操作；构建包含2D语义布局、高度图、3D建筑资产、PBR材料和天空盒的多模态数据集。
- Result: 相比CityDreamer和CityCraft，布局FID分别降低83.7%和20.1%；在所有AQS和RDR评分中排名第一，在几何保真度、风格适应性和语义可控性方面达到最先进水平。
- Conclusion: MajutsuCity在3D城市生成领域实现了新的技术突破，为几何保真度、风格适应性和语义可控性设定了新的标准，预期将推动3D城市生成研究的新方向。


### [154] [StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections](https://arxiv.org/abs/2511.20418)
*Matvei Shelukhan,Timur Mamedov,Karina Kvanchiani*

Main category: cs.CV

TL;DR: 提出StableTrack方法，通过两阶段匹配策略和新的Bbox-Based距离度量，在低频检测下显著提升多目标跟踪性能，同时保持全频检测下的竞争力。

- Motivation: 解决多目标跟踪在计算资源受限条件下（低频检测）的性能下降问题，当前方法主要针对全频检测场景设计。
- Method: 1. 两阶段匹配策略改进跨帧关联；2. 提出Bbox-Based距离替代传统马氏距离；3. 将视觉跟踪集成到卡尔曼滤波和整体跟踪流程中。
- Result: 在MOT17-val数据集上，1Hz低频检测时HOTA指标提升11.6%，同时在标准MOT17、MOT20和DanceTrack基准测试中与最佳方法保持相当性能。
- Conclusion: StableTrack方法有效解决了低频检测场景下的多目标跟踪问题，在保持全频检测性能的同时显著提升低频检测性能。


### [155] [Block Cascading: Training Free Acceleration of Block-Causal Video Models](https://arxiv.org/abs/2511.20426)
*Hmrishav Bandyopadhyay,Nikhil Pinnaparaju,Rahim Entezari,Jim Scott,Yi-Zhe Song,Varun Jampani*

Main category: cs.CV

TL;DR: Block Cascading通过训练无关的并行化方法显著缓解了块因果视频生成中的速度-质量权衡问题，在5个GPU上实现约2倍加速，且不损失生成质量。

- Motivation: 块因果视频生成面临严重的速度-质量权衡：小模型(1.3B)只能达到16 FPS，大模型(14B)仅4.5 FPS，用户必须在响应性和质量之间做出选择。
- Method: 利用关键洞察：未来视频块不需要完全去噪的当前块即可开始生成。通过使用来自前驱块的部分去噪上下文启动块生成，将顺序流水线转换为并行级联，多个块同时去噪。
- Result: 在5个GPU上利用时间并行性，所有模型规模实现约2倍加速：1.3B模型从16加速到30 FPS，14B模型从4.5加速到12.5 FPS，同时消除了KV重缓存的开销(~200ms)。
- Conclusion: Block Cascading在块因果视频生成中实现了显著的推理加速，同时保持生成质量，为交互式生成提供了更高效的解决方案。


### [156] [BRIC: Bridging Kinematic Plans and Physical Control at Test Time](https://arxiv.org/abs/2511.20431)
*Dohun Lim,Minji Kim,Jaewoon Lim,Sungchan Kim*

Main category: cs.CV

TL;DR: BRIC是一个新颖的测试时适应框架，通过解决基于扩散的运动规划器和基于强化学习的物理控制器之间的执行差异，实现长期人体运动生成。

- Motivation: 扩散模型虽然能生成多样化的运动，但经常产生物理上不可行的输出，导致模拟中的执行漂移问题。
- Method: BRIC在测试时动态适应物理控制器以适应噪声运动计划，同时通过损失函数保留预训练技能；并引入轻量级测试时引导机制，在信号空间中引导扩散模型而不更新其参数。
- Result: BRIC在多种长期任务（包括运动组合、避障和人-场景交互）上实现了最先进的性能。
- Conclusion: 通过结合两种适应策略，BRIC能够以有效且高效的方式确保在不同环境中一致且物理上合理的长期执行。


### [157] [Object-Centric Vision Token Pruning for Vision Language Models](https://arxiv.org/abs/2511.20439)
*Guangyuan Li,Rongzhen Zhao,Jinhong Deng,Yanbo Wang,Joni Pajarinen*

Main category: cs.CV

TL;DR: OC-VTP是一种直接且保证性的视觉token剪枝方法，通过轻量级预训练的对象中心视觉token剪枝器，在不微调任何模型的情况下提升VLM推理效率，同时保持最高推理精度。

- Motivation: 视觉语言模型中的视觉token数量多但信息分散，消耗过多不必要的计算资源，现有剪枝方法都是间接且无法保证的。
- Method: 训练轻量级对象中心视觉token剪枝器，通过最小化从选定token重建原始未剪枝token的误差来保证保留最具代表性的视觉token。
- Result: 在任何视觉剪枝比例下，OC-VTP都能帮助主流VLM保持最高的推理准确率，并展现出有趣的解释性。
- Conclusion: OC-VTP提供了一种直接且保证性的视觉token剪枝解决方案，显著提升VLM推理效率同时保持精度。


### [158] [Learning to Generate Human-Human-Object Interactions from Textual Descriptions](https://arxiv.org/abs/2511.20446)
*Jeonghyeon Na,Sangwon Baik,Inhee Lee,Junyoung Lee,Hanbyul Joo*

Main category: cs.CV

TL;DR: 提出Human-Human-Object Interactions (HHOIs)新问题，开发数据集和生成方法，通过扩散模型统一生成多人与物体的交互。

- Motivation: 人类交互行为具有情境依赖性，需要建模多人与场景上下文的关系，特别是两人共享物体交互的情况。
- Method: 使用图像生成模型合成HHOI数据，分别训练text-to-HOI和text-to-HHI扩散模型，然后整合成统一生成框架。
- Result: 实验显示该方法能根据文本描述生成真实的HHOIs，优于仅关注单人与物体交互的方法，并扩展到多人交互。
- Conclusion: 提出的HHOIs框架能有效建模多人与物体的复杂交互，为多人运动生成提供了新应用。


### [159] [Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search](https://arxiv.org/abs/2511.20460)
*Yunqi Zhou,Chengjie Jiang,Chun Yuan,Jing Li*

Main category: cs.CV

TL;DR: ZoomSearch是一个无需训练、即插即用的超高分遥感视觉问答流水线，通过自适应多分支缩放搜索和布局感知补丁重组，解决了现有模型在处理超高分遥感图像时的token和内存限制问题。

- Motivation: 随着卫星星座、传感器技术和成像流水线的发展，超高分遥感图像日益普及，但现有遥感基础模型无法有效处理此类输入：全图编码会耗尽token和内存预算，而基于缩放的预处理会丢失细粒度和关键细节。
- Method: ZoomSearch结合自适应多分支缩放搜索（在图像块上进行分层搜索以定位查询相关区域）和布局感知补丁重组（将选定块重新组织为紧凑、布局忠实的画布），将'看哪里'与'如何回答'解耦。
- Result: 在超高分RS-VQA基准测试MME-RealWorld-RS和LRS-VQA上，与LLaVA-ov集成时，ZoomSearch实现了最先进的准确率，相比LLaVA-ov基线在LRS-VQA上提升26.3%，在MME-RealWorld-RS上提升114.8%。同时推理效率更高，比之前基于搜索的方法快20%~44%。
- Conclusion: ZoomSearch为超高分遥感视觉问答提供了一种高效解决方案，通过分层搜索和智能重组机制，在保持高准确率的同时显著提升了推理效率。


### [160] [STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow](https://arxiv.org/abs/2511.20462)
*Jiatao Gu,Ying Shen,Tianrong Chen,Laurent Dinh,Yuyang Wang,Miguel Angel Bautista,David Berthelot,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: STARFlow-V是基于归一化流的视频生成模型，通过全局-局部架构在时空隐空间中操作，限制因果依赖到全局隐空间，同时保留丰富的帧内局部交互，解决了标准自回归扩散模型生成中的错误累积问题。

- Motivation: 在视频生成领域，虽然归一化流在图像生成中取得进展，但现有最先进系统几乎完全依赖基于扩散的模型。本研究重新探讨这一设计空间，旨在开发基于归一化流的视频生成器，具有端到端学习、鲁棒因果预测和原生似然估计等优势。
- Method: STARFlow-V在时空隐空间中操作，采用全局-局部架构，限制因果依赖到全局隐空间，同时保留帧内局部交互。提出流-得分匹配方法，为模型配备轻量级因果去噪器，以自回归方式提高视频生成一致性。采用视频感知Jacobi迭代方案提高采样效率。
- Result: STARFlow-V在视觉保真度和时间一致性方面取得了强劲表现，相对于基于扩散的基线具有实用的采样吞吐量。这是首个证明归一化流能够实现高质量自回归视频生成的证据。
- Conclusion: STARFlow-V展示了归一化流在高质量自回归视频生成方面的能力，为构建世界模型建立了一个有前景的研究方向。


### [161] [Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features](https://arxiv.org/abs/2511.20469)
*Ben Hamscher,Arnold Brosch,Nicolas Binninger,Maksymilian Jan Dejna,Kira Maag*

Main category: cs.CV

TL;DR: 提出了一种基于姿态估计的轻量级舞蹈风格分类框架，使用Laban运动分析启发的时空描述符和快速傅里叶变换特征来捕捉舞蹈动作的局部关节动态和节奏模式。

- Motivation: 舞蹈是人类文化的重要组成部分，但基于运动数据识别和区分舞蹈类型是一个复杂问题，因为许多风格具有相似的姿势、手势和时间运动模式。
- Method: 使用从视频中提取的姿态估计，提出受Laban运动分析启发的时空描述符，捕捉局部关节动态（速度、加速度、上肢角度运动），并集成快速傅里叶变换特征编码运动的节奏和周期性方面。
- Result: 该方法以低计算成本实现了不同舞蹈风格的稳健分类，无需复杂模型架构，表明可解释的运动表示能有效捕捉风格细微差别。
- Conclusion: 提出的轻量级框架证明，通过结构化的时空特征表示，可以有效地对舞蹈风格进行分类，同时保持计算效率。


### [162] [Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification](https://arxiv.org/abs/2511.20474)
*Akshit Pramod Anchan,Jewelith Thomas,Sritama Roy*

Main category: cs.CV

TL;DR: 该研究评估了模块化架构在辅助技术中的可行性，开发了三个独立感知模块：CNN用于眼动检测、深度CNN用于表情识别、LSTM用于语音识别，分别达到93.0%、97.8%和96.89%的准确率。

- Motivation: 开发全面的辅助技术需要无缝整合视觉和听觉感知，受感知系统核心功能启发，研究模块化架构的可行性。
- Method: 提出三个独立感知模块：CNN用于眼动状态检测（困倦/注意力）、深度CNN用于面部表情识别、LSTM用于基于语音的说话人识别，使用Eyes Image、FER2013和定制音频数据集进行训练。
- Result: 三个模型分别达到93.0%、97.8%和96.89%的准确率，证明了轻量级、领域特定模型在离散任务上可以达到高保真度。
- Conclusion: 该研究为未来资源受限辅助设备中的实时多模态集成建立了经过验证的基础，展示了模块化方法的有效性。


### [163] [A Physics-Informed Loss Function for Boundary-Consistent and Robust Artery Segmentation in DSA Sequences](https://arxiv.org/abs/2511.20501)
*Muhammad Irfan,Nasir Rahim,Khalid Mahmood Malik*

Main category: cs.CV

TL;DR: 提出了一种基于物理学的损失函数PIL，用于改善脑血管DSA序列分割的几何和物理一致性，在多个架构和数据集上优于传统损失函数。

- Motivation: 传统分割损失函数仅关注像素级重叠，忽略了血管边界的几何和物理一致性，导致血管预测结果碎片化或不稳定。
- Method: 提出物理信息损失PIL，将预测边界与真实边界的交互建模为材料物理中位错理论启发的弹性过程，引入物理正则化项强制平滑轮廓演化和结构一致性。
- Result: 在DIAS和DSCA两个公开基准测试中，PIL在U-Net、U-Net++、SegFormer和MedFormer等架构上均优于交叉熵、Dice、主动轮廓和表面损失等传统损失函数，实现了更高的敏感性、F1分数和边界一致性。
- Conclusion: 将基于物理的边界交互融入深度神经网络能够提高动态血管成像中血管分割的精度和鲁棒性。


### [164] [DesignPref: Capturing Personal Preferences in Visual Design Generation](https://arxiv.org/abs/2511.20513)
*Yi-Hao Peng,Jeffrey P. Bigham,Jason Wu*

Main category: cs.CV

TL;DR: DesignPref数据集包含12k个UI设计对比，由20位专业设计师标注多级偏好评分，研究发现设计师之间存在显著分歧，传统多数投票方法无法准确反映个体偏好，个性化模型在预测个体设计师偏好方面表现更优。

- Motivation: 由于视觉设计具有主观性和高度个性化特征，不同个体的偏好差异很大，而现有的生成模型微调和基准测试主要依赖人类标注的设计偏好数据集，但缺乏对个性化偏好的研究。
- Method: 引入DesignPref数据集，包含12k对UI设计生成对比，由20位专业设计师标注多级偏好评分；研究设计师间的分歧程度；比较传统多数投票方法与个性化策略（微调或将设计师特定标注整合到RAG管道中）的效果。
- Result: 设计师之间存在显著分歧（Krippendorff's alpha = 0.25）；自然语言理由显示分歧源于对设计方面重要性的不同认知和个人偏好；个性化模型在使用少20倍示例的情况下，在预测个体设计师偏好方面始终优于聚合基线模型。
- Conclusion: 这是首个研究个性化视觉设计评估的数据集，支持未来对个体设计品味建模的研究，表明个性化方法在视觉设计生成评估中具有重要价值。


### [165] [AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs](https://arxiv.org/abs/2511.20515)
*Kuniaki Saito,Risa Shinoda,Shohei Tanaka,Tosho Hirasawa,Fumio Okura,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: AlignBench是一个评估图像-文本对齐的新基准，通过评估由多样化图像到文本和文本到图像模型生成的详细图像-描述对，提供细粒度对齐指标。

- Motivation: 现有基准依赖基于规则的扰动或简短描述，限制了测量细粒度对齐的能力。需要新的评估方法来更准确地衡量视觉语言模型的对齐性能。
- Method: 引入AlignBench基准，使用多样化图像到文本和文本到图像模型生成详细的图像-描述对，每个句子都进行正确性标注，直接评估VLMs作为对齐评估器的能力。
- Result: 对广泛基于解码器的VLMs进行基准测试发现：(i) CLIP模型几乎盲视；(ii) 检测器系统性地高估早期句子；(iii) 模型表现出强烈的自偏好，偏向自身输出并损害检测性能。
- Conclusion: AlignBench为图像-文本对齐提供了更细粒度的评估基准，揭示了现有模型的局限性，特别是CLIP模型的盲视问题和检测器的系统性偏差。


### [166] [HBridge: H-Shape Bridging of Heterogeneous Experts for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2511.20520)
*Xiang Wang,Zhifei Zhang,He Zhang,Zhe Lin,Yuqian Zhou,Qing Liu,Shiwei Zhang,Yijun Li,Shaoteng Liu,Haitian Zheng,Jason Kuen,Yuehuan Wang,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: HBridge提出了一种非对称H形架构，通过选择性桥接中间层来优化多模态生成，减少40%以上的注意力共享，提高效率并增强生成质量。

- Motivation: 现有的统一模型采用对称设计，将一种专家镜像到另一种专家以实现方便的初始化和融合，但由于固有的模态差异，这种方法仍然不够理想。
- Method: HBridge采用非对称H形架构，选择性桥接中间层，解耦浅层和深层，同时引入语义重建令牌来增强跨模态一致性。
- Result: 在多个基准测试上的广泛实验证明了HBridge的有效性和优越性能。
- Conclusion: HBridge为统一多模态生成建立了一个新的范式。


### [167] [Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos](https://arxiv.org/abs/2511.20525)
*Yayuan Li,Aadit Jain,Filippos Bellos,Jason J. Corso*

Main category: cs.CV

TL;DR: 提出了Mistake Attribution (MATT)任务，用于在自我中心视频中细粒度理解人类错误，通过MisEngine数据引擎自动构建错误样本，并开发了MisFormer模型进行跨语义、时间和空间维度的错误归因。

- Motivation: 现有的错误理解工作缺乏细粒度输出，无法具体将错误归因到指令文本或尝试视频中。
- Method: 开发了MisEngine数据引擎自动从现有数据集构建错误样本，提出了MisFormer模型，这是一个基于注意力的统一模型，用于跨语义（什么）、时间（何时）和空间（何处）维度进行错误归因。
- Result: 在大型自我中心语料库上应用MisEngine得到了EPIC-KITCHENS-M和Ego4D-M数据集，比现有错误数据集大两个数量级。MisFormer在新数据集和现有基准测试中优于强基线模型。
- Conclusion: MATT任务和MisEngine/MisFormer框架为细粒度错误理解提供了有效解决方案，在错误归因方面表现出色。


### [168] [Automated Monitoring of Cultural Heritage Artifacts Using Semantic Segmentation](https://arxiv.org/abs/2511.20541)
*Andrea Ranieri,Giorgio Palmieri,Silvia Biasotti*

Main category: cs.CV

TL;DR: 本文比较了不同CNN编码器的U-Net架构在文化遗产裂缝检测中的语义分割性能，展示了模型在未见过的文化遗产场景中的良好泛化能力。

- Motivation: 解决文化遗产保护中自动化裂缝检测的关键需求，通过语义分割技术识别雕像和纪念碑上的裂缝。
- Method: 使用不同CNN编码器的U-Net架构进行像素级裂缝识别，在OmniCrack30k数据集上进行定量评估，并在真实世界未标记的裂缝雕像和纪念碑数据集上进行定性评估。
- Result: 模型在文化遗产环境中表现出有前景的泛化能力，即使从未在雕像或纪念碑图像上进行过明确训练。
- Conclusion: 研究为不同CNN编码器在细粒度裂缝分割中的能力提供了有价值的见解，证明了语义分割方法在文化遗产保护中的实用性。


### [169] [New York Smells: A Large Multimodal Dataset for Olfaction](https://arxiv.org/abs/2511.20544)
*Ege Ozguroglu,Junbang Liang,Ruoshi Liu,Mia Chiquier,Michael DeTienne,Wesley Wei Qian,Alexandra Horowitz,Andrew Owens,Carl Vondrick*

Main category: cs.CV

TL;DR: 纽约气味数据集：包含7000个气味-图像对，覆盖3500个不同物体，是现有嗅觉数据集的70倍。支持跨模态检索、气味识别和细粒度分类任务。

- Motivation: 解决机器学习在嗅觉感知领域的瓶颈——缺乏自然环境中收集的多模态嗅觉训练数据。
- Method: 构建大规模配对图像和嗅觉信号数据集，包含室内外环境的气味-图像对，支持跨模态表示学习。
- Result: 视觉数据能够促进跨模态嗅觉表示学习，学习到的嗅觉表示优于传统手工特征。
- Conclusion: 该数据集为机器嗅觉研究提供了重要资源，证明了视觉辅助的跨模态学习在嗅觉感知中的有效性。


### [170] [Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning](https://arxiv.org/abs/2511.20549)
*Guanjie Chen,Shirui Huang,Kai Liu,Jianchen Zhu,Xiaoye Qu,Peng Chen,Yu Cheng,Yifu Sun*

Main category: cs.CV

TL;DR: Flash-DMD是一个新颖的框架，通过高效的timestep感知蒸馏策略和联合RL微调方案，显著加速扩散模型生成，同时保持高质量输出。

- Motivation: 扩散模型的迭代采样过程计算成本高，现有的timestep蒸馏技术需要大量训练且会导致图像质量下降，而RL微调过程不稳定且容易陷入奖励破解。
- Method: 1. 提出高效的timestep感知蒸馏策略，大幅降低训练成本；2. 引入联合训练方案，在继续timestep蒸馏训练的同时进行RL目标微调，利用蒸馏损失作为正则化器稳定RL训练。
- Result: Flash-DMD仅需DMD2训练成本的2.1%，在少步采样机制下实现了最先进的生成质量，在视觉质量、人类偏好和文本-图像对齐指标上优于现有方法。
- Conclusion: Flash-DMD为训练高效、高保真且稳定的生成模型提供了一个有效的范式。


### [171] [Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward](https://arxiv.org/abs/2511.20561)
*Yuwei Niu,Weiyang Jin,Jiaqi Liao,Chaoran Feng,Peng Jin,Bin Lin,Zongjian Li,Bin Zhu,Weihao Yu,Li Yuan*

Main category: cs.CV

TL;DR: UniSandbox是一个解耦评估框架，通过合成数据集揭示理解与生成之间的显著差距，发现思维链(CoT)能有效弥合推理生成和知识转移方面的差距。

- Motivation: 研究统一多模态模型中理解是否真正影响生成，避免数据泄露并实现详细分析。
- Method: 引入UniSandbox解耦评估框架，使用受控合成数据集，分析思维链在推理生成和知识转移中的作用。
- Result: 发现理解-生成差距主要体现在推理生成和知识转移两个维度，思维链能有效弥合这一差距，且自训练方法可内化推理能力。
- Conclusion: UniSandbox为设计真正弥合理解与生成差距的统一架构和训练策略提供了初步见解。


### [172] [PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding](https://arxiv.org/abs/2511.20562)
*Haoze Zhang,Tianyu Huang,Zichen Wan,Xiaowei Jin,Hongzhi Zhang,Hui Li,Wangmeng Zuo*

Main category: cs.CV

TL;DR: PhysChoreo是一个从单张图像生成具有物理真实感视频的框架，通过两阶段方法实现物理可控性和真实性。

- Motivation: 现有视频生成模型缺乏明确的物理可控性和合理性，基于物理渲染的方法在建模复杂物理属性和控制长时间序列行为方面存在挑战。
- Method: 两阶段方法：1) 通过部件感知物理属性重建估计图像中所有物体的静态初始物理属性；2) 通过时间指令和物理可编辑模拟合成高质量视频。
- Result: 实验结果显示PhysChoreo能生成具有丰富行为和物理真实感的视频，在多个评估指标上优于最先进方法。
- Conclusion: PhysChoreo框架成功解决了视频生成中的物理可控性和真实性问题，为物理引导的视频生成提供了有效解决方案。


### [173] [A Reason-then-Describe Instruction Interpreter for Controllable Video Generation](https://arxiv.org/abs/2511.20563)
*Shengqiong Wu,Weicai Ye,Yuanxing Zhang,Jiahao Wang,Quande Liu,Xintao Wang,Pengfei Wan,Kun Gai,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: ReaDe是一个通用的、模型无关的指令解释器，能够将模糊的用户输入转换为精确的视频生成规范，通过推理-描述范式提升视频生成的可控性和意图匹配度。

- Motivation: 当前扩散变换器虽然改善了视频保真度和时间一致性，但实际可控性有限。用户输入的简洁性、模糊性和组合复杂性与训练时使用的详细提示形成对比，导致意图与输出不匹配。
- Method: 采用推理-描述范式：先分析用户请求识别核心需求并解决模糊性，然后生成详细指导。通过两阶段优化训练：推理增强监督提供逐步解析和密集描述，多维奖励分配器实现稳定、反馈驱动的自然风格描述优化。
- Result: 在单条件和多条件场景下的实验显示，在指令保真度、描述准确性和下游视频质量方面均获得一致提升，对推理密集和未见输入具有强泛化能力。
- Conclusion: ReaDe为实现可控视频生成与准确解释用户意图的对齐提供了实用途径。


### [174] [DINO-Tok: Adapting DINO for Visual Tokenizers](https://arxiv.org/abs/2511.20565)
*Mingkai Jia,Mingxiao Li,Liaoyuan Fan,Tianxing Shi,Jiaxin Guo,Zeming Li,Xiaoyang Guo,Xiao-Xiao Long,Qian Zhang,Ping Tan,Wei Yin*

Main category: cs.CV

TL;DR: DINO-Tok是一种基于DINO的视觉分词器，通过整合浅层和深层特征，在信息完整的潜在空间中统一分层表示，显著提升了重建性能。

- Motivation: 现有分词器通常从头训练，难以平衡语义表示和重建保真度，特别是在高维潜在空间中。
- Method: 整合DINO的浅层特征（保留细粒度细节）和深层特征（编码全局语义），并提出全局PCA重加权机制来稳定向量量化。
- Result: 在ImageNet 256×256上达到最先进的重建性能：自编码28.54 PSNR，VQ建模23.98 PSNR，显著优于先前分词器。
- Conclusion: 将强大的预训练视觉模型（如DINO）适配用于分词，能够实现语义对齐和高保真度的潜在表示，推动下一代视觉生成模型发展。


### [175] [VQ-VA World: Towards High-Quality Visual Question-Visual Answering](https://arxiv.org/abs/2511.20573)
*Chenhui Gou,Zilong Chen,Zeyu Wang,Feng Li,Deyao Zhu,Zicheng Duan,Kunchang Li,Chaorui Deng,Hongyi Yuan,Haoqi Fan,Cihang Xie,Jianfei Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 该论文提出了VQ-VA World框架，通过大规模数据构建和训练，使开源模型能够实现视觉问答-视觉回答(VQ-VA)能力，即在回答视觉问题时生成图像而非文本。

- Motivation: 随着专有系统如NanoBanana和GPT-Image展现出VQ-VA能力，作者希望将这种能力引入开源模型，促进该领域的研究发展。
- Method: 构建了VQ-VA World数据框架，采用代理式流水线进行大规模目标数据构建，爬取了约180万高质量图像-文本交错样本用于模型训练，并发布了IntelligentBench评估基准。
- Result: 使用VQ-VA World数据训练后，LightFusion在IntelligentBench上达到53.06分，显著超越之前最佳开源基线(7.78分)，大幅缩小与领先专有系统(如NanoBanana的81.67分)的差距。
- Conclusion: 通过发布完整的模型权重、数据集和流水线，作者希望推动VQ-VA领域的未来研究。


### [176] [The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment](https://arxiv.org/abs/2511.20614)
*Ziheng Ouyang,Yiren Song,Yaoli Liu,Shihao Zhu,Qibin Hou,Ming-Ming Cheng,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出了ImageCritic方法，通过参考图像引导的后编辑解决生成图像中的细节不一致问题，使用注意力对齐损失和细节编码器来精确修正不一致性。

- Motivation: 现有方法在给定参考图像生成定制内容时，难以保持一致的细粒度细节，存在不一致性问题。
- Method: 构建参考-退化-目标三元组数据集，通过VLM选择和显式退化模拟常见不准确性；设计注意力对齐损失和细节编码器来修正不一致性；集成到智能体框架中实现多轮局部编辑。
- Result: 在多种定制生成场景中有效解决细节相关问题，相比现有方法有显著改进。
- Conclusion: ImageCritic能够有效解决生成图像中的细节不一致问题，在多种定制生成场景中表现优异。


### [177] [Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities](https://arxiv.org/abs/2511.20615)
*Seyede Niloofar Hosseini,Ali Mojibi,Mahdi Mohseni,Navid Arjmand,Alireza Taheri*

Main category: cs.CV

TL;DR: 使用双向LSTM和Transformer架构预测动态负载搬运活动中的人体全身姿态，提出通过优化成本函数保持身体段长度恒定的新方法，显著提高了预测精度。

- Motivation: 探索深度神经网络在动态负载搬运活动中对人体全身姿态预测的应用，旨在更好地理解和预测手动物料搬运活动中的运动动力学。
- Method: 使用双向LSTM和Transformer两种时间序列模型，输入包括手负载位置3D坐标、提升技术、搬运技术、体重身高以及任务前25%的身体姿态数据，预测剩余75%任务期间的身体坐标。提出通过优化新成本函数来保持身体段长度恒定的方法。
- Result: 新成本函数使手臂和腿部模型的预测误差分别降低约8%和21%。Transformer架构的均方根误差为47.0毫米，比基于BLSTM的模型长期性能准确约58%。
- Conclusion: 研究表明利用捕获时间序列依赖性的神经网络在3D运动帧中预测人体姿态是有效的，为理解和预测手动物料搬运活动中的运动动力学提供了独特方法。


### [178] [Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI](https://arxiv.org/abs/2511.20620)
*Xinhao Liu,Jiaqi Li,Youming Deng,Ruxin Chen,Yingjia Zhang,Yifei Ma,Li Guo,Yiming Li,Jing Zhang,Chen Feng*

Main category: cs.CV

TL;DR: Wanderland是一个真实到仿真的框架，用于解决具身AI中可重现闭环评估的瓶颈问题，通过多传感器采集、可靠重建和准确几何建模，为开放世界城市环境中的导航策略学习提供可信测试平台。

- Motivation: 解决具身AI（如视觉导航）中可重现闭环评估的主要瓶颈，现有视频-3DGS方法存在较大的视觉和几何仿真到真实差距，不适合基准测试。
- Method: 开发Wanderland真实到仿真框架，包含多传感器采集、可靠重建、准确几何建模和鲁棒视图合成，构建室内外城市场景的多样化数据集。
- Result: 系统展示了仅图像方法扩展性差、几何质量影响新视图合成，以及这些因素对导航策略学习和评估可靠性的不利影响。
- Conclusion: Wanderland为开放世界具身AI的可重现研究建立了新基础，不仅作为导航的可信测试平台，其丰富的原始传感器数据还可用于3D重建和新视图合成模型的基准测试。


### [179] [ShapeGen: Towards High-Quality 3D Shape Synthesis](https://arxiv.org/abs/2511.20624)
*Yangguang Li,Xianglong He,Zi-Xin Zou,Zexiang Liu,Wanli Ouyang,Ding Liang,Yan-Pei Cao*

Main category: cs.CV

TL;DR: ShapeGen通过改进3D表示和监督、提升分辨率以及利用线性变换器的优势，实现了高质量的图像到3D形状生成，解决了现有方法缺乏细节、表面过于平滑和薄壳结构碎片化的问题。

- Motivation: 当前3D形状生成方法存在缺乏精细细节、表面过于平滑和薄壳结构碎片化的问题，导致生成的3D资产无法达到艺术家偏好的标准。
- Method: 通过3D表示和监督改进、分辨率提升以及利用线性变换器的优势来实现高质量的图像到3D形状生成。
- Result: ShapeGen在图像到3D生成方面实现了显著飞跃，建立了新的最先进性能，生成的资产可以无缝集成到3D流程中。
- Conclusion: ShapeGen通过多种改进的协同效应，在图像到3D生成方面取得了重大突破，为各种应用提供了高质量的3D资产。


### [180] [MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models](https://arxiv.org/abs/2511.20629)
*Chieh-Yun Chen,Zhonghao Wang,Qi Chen,Zhifan Ye,Min Shi,Yue Zhao,Yinan Zhao,Hui Qu,Wei-An Lin,Yiru Shen,Ajinkya Kale,Irfan Essa,Humphrey Shi*

Main category: cs.CV

TL;DR: 本文提出了MapReduce LoRA和Reward-aware Token Embedding (RaTE)两种方法来解决多奖励优化中的对齐税问题，在文本到图像、文本到视频和语言任务中均取得了显著改进。

- Motivation: 基于人类反馈的强化学习在生成模型对齐方面取得了进展，但联合优化多个奖励时会出现对齐税问题，即改善一个维度会损害其他维度。
- Method: MapReduce LoRA并行训练偏好特定的LoRA专家并迭代合并以优化共享基础模型；RaTE学习奖励特定的token嵌入，在推理时组合以实现灵活的偏好控制。
- Result: 在文本到图像生成任务中，GenEval、PickScore和OCR分别提升36.1%、4.6%和55.7%；文本到视频生成中视觉和运动质量分别提升48.1%和90.0%；语言任务中帮助性和无害性分别提升43.4%和136.7%。
- Conclusion: 该框架为跨模态的多偏好对齐设定了新的最先进方案。


### [181] [iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation](https://arxiv.org/abs/2511.20635)
*Zhoujie Fu,Xianfang Zeng,Jinghong Lan,Xinyao Liao,Cheng Chen,Junyi Chen,Jiacheng Wei,Wei Cheng,Shiyu Liu,Yunuo Chen,Gang Yu,Guosheng Lin*

Main category: cs.CV

TL;DR: iMontage是一个将预训练视频模型重新用于图像生成的统一框架，通过注入图像数据的多样性到时间连贯框架中，生成具有自然过渡和更广泛动态范围的图像集。

- Motivation: 预训练视频模型具有强大的时间连贯性先验，但其动态受限于连续训练数据。通过将图像数据的丰富多样性注入这个连贯的时间框架，可以生成既保持自然过渡又具有更广泛动态范围的图像集。
- Method: 提出优雅且最小侵入的适配策略，配合定制化的数据整理流程和训练范式，使模型获得广泛的图像处理能力而不破坏原有的运动先验。
- Result: iMontage在多个主流多输入多输出任务中表现出色，不仅保持强大的跨图像上下文一致性，还生成了超越传统范围的具有非凡动态的场景。
- Conclusion: iMontage成功将强大的视频模型重新用于图像生成，实现了时间连贯性和图像多样性的统一，为图像生成和编辑任务提供了有效的解决方案。


### [182] [MotionV2V: Editing Motion in a Video](https://arxiv.org/abs/2511.20640)
*Ryan Burgert,Charles Herrmann,Forrester Cole,Michael S Ryoo,Neal Wadhwa,Andrey Voynov,Nataniel Ruiz*

Main category: cs.CV

TL;DR: 本文提出了一种通过直接编辑稀疏轨迹来修改视频运动的方法，称为"运动编辑"，结合生成式主干网络实现强大的视频编辑能力。

- Motivation: 虽然生成式视频模型在保真度和一致性方面取得了显著进展，但将这些能力应用于视频编辑仍然是一个复杂挑战。精确的运动控制作为编辑现有视频的有前景但尚未充分探索的范式。
- Method: 提出了一个生成"运动反事实"视频对的流程，这些视频对具有相同内容但不同运动。在运动条件视频扩散架构上对此数据集进行微调，允许从任意时间戳开始编辑并自然传播。
- Result: 在四向头对头用户研究中，该模型相对于先前工作获得了超过65%的偏好率。
- Conclusion: 通过直接编辑输入视频中提取的稀疏轨迹，结合生成式主干网络，可以实现强大的视频编辑能力，特别是在运动控制方面表现出色。


### [183] [Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition](https://arxiv.org/abs/2511.20641)
*Wei Tang,Zuo-Zheng Wang,Kun Zhang,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: CAPNET是一个用于长尾多标签视觉识别的端到端框架，通过显式建模CLIP文本编码器中的标签相关性，结合图卷积网络和可学习软提示，在多个基准数据集上显著优于现有方法。

- Motivation: 解决长尾多标签视觉识别中存在的两个主要问题：1）现有方法直接从不平衡数据集中推导语义类别关系，导致尾部类别相关性不可靠；2）CLIP的零样本范式针对单标签图像-文本匹配优化，在多标签任务中表现欠佳。
- Method: 提出相关性适应提示网络(CAPNET)，包含：1）从CLIP文本编码器显式建模标签相关性；2）图卷积网络进行标签感知传播；3）可学习软提示优化嵌入；4）分布平衡Focal损失与类别感知重加权；5）测试时集成和参数高效微调以对齐视觉-文本模态。
- Result: 在VOC-LT、COCO-LT和NUS-WIDE等基准数据集上的广泛实验表明，CAPNET相比最先进方法实现了显著改进。
- Conclusion: CAPNET通过有效建模标签相关性和平衡长尾分布，为现实世界长尾多标签视觉识别提供了有效的解决方案。


### [184] [Concept-Aware Batch Sampling Improves Language-Image Pretraining](https://arxiv.org/abs/2511.20643)
*Adhiraj Ghosh,Vishaal Udandarao,Thao Nguyen,Matteo Farina,Mehdi Cherti,Jenia Jitsev,Sewoong Oh,Elisa Ricci,Ludwig Schmidt,Matthias Bethge*

Main category: cs.CV

TL;DR: 提出了DataConcept数据集和概念感知批量采样框架(CABS)，用于在线、概念驱动的数据筛选，相比传统离线方法能更灵活地优化视觉语言模型训练数据。

- Motivation: 现有数据筛选方法多为离线和概念无关的，存在数据偏见问题。需要更灵活、任务自适应的在线概念驱动筛选方法。
- Method: 构建DataConcept数据集(1.28亿图像文本对)，提出CABS框架，包括多样性最大化(CABS-DM)和频率最大化(CABS-FM)两种变体，根据目标概念分布动态构建训练批次。
- Result: 在28个基准测试中，CABS方法显著提升了CLIP/SigLIP模型的性能，成为专有在线数据筛选算法的强力开源替代方案。
- Conclusion: CABS框架实现了灵活的概念驱动数据筛选，使研究人员能够定义自定义概念分布以优化特定下游任务。


### [185] [Vision-Language Memory for Spatial Reasoning](https://arxiv.org/abs/2511.20644)
*Zuntao Liu,Yi Du,Taimeng Fu,Shaoshu Su,Cherie Ho,Chen Wang*

Main category: cs.CV

TL;DR: VLM²是一个具有持久记忆的视觉语言模型，通过双记忆模块解决视频空间推理中的语义-几何错位和缺乏持久记忆的问题，在多个基准测试中达到最先进性能。

- Motivation: 当前视觉语言模型在基于视频的空间推理中表现不及人类，主要存在两个问题：语义-几何错位导致3D理解不一致，以及缺乏持久记忆来维持3D表示和理解。
- Method: 提出VLM²模型，采用双记忆模块：工作记忆作为滑动窗口关注即时上下文，情景记忆整合存储关键长期信息，实现固定计算成本下的高效长时程空间推理。
- Result: 在多个基准测试上的广泛实验表明，VLM²在仅使用视频的模型中达到了最先进的性能。
- Conclusion: VLM²显著推进了视觉空间智能的前沿，通过持久记忆和视图一致的3D感知表示，有效解决了视频空间推理的关键挑战。


### [186] [PixelDiT: Pixel Diffusion Transformers for Image Generation](https://arxiv.org/abs/2511.20645)
*Yongsheng Yu,Wei Xiong,Weili Nie,Yichen Sheng,Shiqiu Liu,Jiebo Luo*

Main category: cs.CV

TL;DR: PixelDiT是一个单阶段的端到端像素空间扩散变换器，通过双层级设计（补丁级DiT捕捉全局语义，像素级DiT细化纹理细节）直接在像素空间学习扩散过程，无需预训练自编码器，在图像生成质量上超越了现有像素生成模型。

- Motivation: 解决传统潜在空间扩散变换器依赖两阶段流程的问题，其中预训练自编码器引入有损重建，导致误差累积并阻碍联合优化。
- Method: 提出PixelDiT单阶段端到端模型，采用完全基于变换器的架构，包含补丁级DiT和像素级DiT的双层级设计，直接在像素空间进行扩散建模。
- Result: 在ImageNet 256x256上达到1.61 FID，大幅超越现有像素生成模型；扩展到文本到图像生成，在1024x1024分辨率下达到0.74 GenEval和83.5 DPG-bench，接近最佳潜在扩散模型性能。
- Conclusion: 有效的像素级令牌建模对像素扩散成功至关重要，PixelDiT证明了直接在像素空间进行端到端扩散建模的可行性，无需依赖潜在空间编码器。


### [187] [3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding](https://arxiv.org/abs/2511.20646)
*Xiaoye Wang,Chen Tang,Xiangyu Yue,Wei-Hong Li*

Main category: cs.CV

TL;DR: 提出一种通过跨视图相关性（成本体积）将几何一致性集成到多任务学习网络中的方法，使用轻量级跨视图模块来提升分割和深度估计等密集预测任务的性能。

- Motivation: 当前多任务学习方法主要在2D图像空间捕捉跨任务关系，缺乏3D感知能力，而3D感知对于建模跨任务相关性以实现全面场景理解至关重要。
- Method: 引入轻量级跨视图模块（CvM），该模块在任务间共享，通过跨视图交换信息来捕捉跨视图相关性，并与MTL编码器特征集成进行多任务预测。该模块与架构无关，可应用于单视图和多视图数据。
- Result: 在NYUv2和PASCAL-Context数据集上的广泛实验表明，该方法能有效将几何一致性注入现有MTL方法中以提高性能。
- Conclusion: 通过集成跨视图相关性作为几何一致性，可以显著提升多任务学习网络在密集预测任务中的表现，证明了3D感知对于全面场景理解的重要性。


### [188] [Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization](https://arxiv.org/abs/2511.20647)
*Tahira Kazimi,Connor Dunlop,Pinar Yanardag*

Main category: cs.CV

TL;DR: 提出了DPP-GRPO框架，通过结合行列式点过程和组相对策略优化理论，将多样性作为显式信号来优化文本到视频生成模型，在保持提示保真度和感知质量的同时提高生成视频的多样性。

- Motivation: 解决现有文本到视频扩散模型在从单一文本提示生成多个视频时输出多样性不足的问题，旨在训练能够覆盖给定提示下各种合理结果的策略。
- Method: 将问题表述为集合级策略优化问题，使用DPPs对冗余样本施加递减回报，通过GRPO在候选集上提供分组反馈，构建了即插即用且模型无关的框架。
- Result: 在WAN和CogVideoX上实现该方法，在VBench、VideoScore和人类偏好研究中一致提高了视频多样性，同时发布了代码和包含30,000个多样化提示的新基准数据集。
- Conclusion: DPP-GRPO框架成功地将多样性作为显式优化信号，在不牺牲提示保真度或感知质量的情况下，显著提升了文本到视频生成模型在视觉外观、摄像机运动和场景结构方面的多样性表现。


### [189] [LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight](https://arxiv.org/abs/2511.20648)
*Yunze Man,Shihao Wang,Guowen Zhang,Johan Bjorck,Zhiqi Li,Liang-Yan Gui,Jim Fan,Jan Kautz,Yu-Xiong Wang,Zhiding Yu*

Main category: cs.CV

TL;DR: LocateAnything3D将3D检测转化为next-token预测问题，通过Chain-of-Sight序列实现从2D检测到3D推理，在Omni3D基准上达到49.89 AP_3D，比之前最佳方法提升15.51。

- Motivation: 当前视觉语言模型擅长2D描述和定位，但缺少多物体3D检测能力，需要让模型能够感知3D世界。
- Method: 使用Chain-of-Sight序列，先进行2D检测作为视觉思维链，然后按照从近到远的顺序预测3D边界框，采用中心-尺寸-旋转的分解策略。
- Result: 在Omni3D基准上达到49.89 AP_3D，比之前最佳方法提升15.51，即使基线使用真实2D框也能超越，在零样本泛化上表现强劲。
- Conclusion: 通过将3D检测转化为规范的next-token问题，LocateAnything3D为模型提供了实用的3D感知基础。


### [190] [Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout](https://arxiv.org/abs/2511.20649)
*Hidir Yesiltepe,Tuna Han Salih Meral,Adil Kaan Akan,Kaan Oktay,Pinar Yanardag*

Main category: cs.CV

TL;DR: ∞-RoPE是一个无需训练的推理时框架，解决了自回归视频扩散模型的三个核心瓶颈：有限时间视野、缓慢的提示响应能力和无法实现不连续场景转换。

- Motivation: 当前自回归视频扩散模型存在三个主要限制：受限于3D-RoPE的有限时间视野、长视频生成中提示响应缓慢、以及无法在单次生成中实现不连续的电影式场景转换。
- Method: 提出三个相互关联的组件：Block-Relativistic RoPE（将时间编码重新表述为移动局部参考系）、KV Flush（通过保留全局sink和最后生成帧来更新KV缓存）、RoPE Cut（在时间RoPE坐标中引入受控不连续性）。
- Result: 实验表明∞-RoPE在整体VBench评分上持续超越先前的自回归模型，实现了无限视野、可控和电影式的视频扩散。
- Conclusion: ∞-RoPE为无限视野、可控和电影式视频扩散提供了一个无需训练的基础框架，有效解决了当前自回归视频扩散模型的核心限制。


### [191] [MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities](https://arxiv.org/abs/2511.20650)
*Tooba Tehreem Sheikh,Jean Lahoud,Rao Muhammad Anwer,Fahad Shahbaz Khan,Salman Khan,Hisham Cholakkal*

Main category: cs.CV

TL;DR: MedROV是首个用于医学影像的实时开放词汇检测模型，通过大规模数据集Omnis和伪标注策略解决医学影像中开放词汇检测的挑战，在检测性能上显著超越现有方法。

- Motivation: 传统医学影像目标检测模型采用闭集范式，无法检测新标签的目标。开放词汇目标检测在医学影像中研究不足，主要受限于数据集稀缺和文本-图像对齐弱的问题。
- Method: 构建包含60万检测样本的大规模数据集Omnis；采用伪标注策略处理多源数据集的缺失标注；利用预训练基础模型的知识增强泛化能力；通过对比学习和跨模态表示实现开放词汇检测。
- Result: MedROV在检测性能上比现有最佳医学影像基础模型平均提升40 mAP50，比闭集检测器提升超过3 mAP50，同时运行速度达到70 FPS。
- Conclusion: MedROV为医学影像开放词汇检测设立了新基准，能够有效检测已知和新型结构，具有实时性和高精度优势。


### [192] [RubricRL: Simple Generalizable Rewards for Text-to-Image Generation](https://arxiv.org/abs/2511.20651)
*Xuelu Feng,Yunsheng Li,Ziyu Wan,Zixuan Gao,Junsong Yuan,Dongdong Chen,Chunming Qiao*

Main category: cs.CV

TL;DR: RubricRL是一个基于评分标准的强化学习框架，通过结构化、可分解的视觉标准清单来改进文本到图像生成模型的对齐，提供更好的可解释性和用户控制。

- Motivation: 现有方法依赖固定权重的复合指标或单一标量奖励，限制了可解释性和灵活性，需要设计更有效的奖励机制。
- Method: 为每个提示动态构建结构化评分标准，包含细粒度视觉标准（如物体正确性、属性准确性、OCR保真度、真实感），由多模态评估器独立评估，并使用提示自适应加权机制。
- Result: 实验表明RubricRL提高了提示忠实度、视觉细节和泛化能力，为文本到图像架构提供了灵活可扩展的RL对齐基础。
- Conclusion: RubricRL框架通过结构化评分标准设计，为文本到图像生成模型的RL对齐提供了更可解释、模块化和用户可控的解决方案。
## q-bio.QM

### [193] [Dual-Path Knowledge-Augmented Contrastive Alignment Network for Spatially Resolved Transcriptomics](https://arxiv.org/abs/2511.17685)
*Wei Zhang,Jiajun Chu,Xinci Liu,Chen Tong,Xinyue Li*

Main category: q-bio.QM

TL;DR: DKAN是一个新颖的双路径知识增强对比对齐网络，通过整合组织病理学图像和基因表达谱来预测空间基因表达，解决了现有方法在生物上下文利用、样本检索依赖和异构模态对齐方面的局限性。

- Motivation: 空间转录组学技术成本高昂，需要从全切片图像预测空间基因表达。现有方法存在生物上下文利用不足、过度依赖样本检索和异构模态对齐不充分等局限性。
- Method: 提出DKAN框架：1）基因语义表示模块利用外部基因数据库提供生物洞察；2）统一的一阶段对比学习范式结合对比学习和监督学习；3）双路径对比对齐模块使用基因语义特征作为跨模态协调器。
- Result: 在三个公共ST数据集上的广泛实验表明，DKAN在空间基因表达预测方面优于现有最先进模型，建立了新的基准。
- Conclusion: DKAN为空间基因表达预测提供了强大的工具，能够推进生物和临床研究。
## cs.LG

### [194] [Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection](https://arxiv.org/abs/2511.19499)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 论文提出TriDetect检测器，通过分析GAN和扩散模型在流形覆盖上的差异，在半监督框架下学习生成器的架构特征，提升跨生成器的伪造检测泛化能力。

- Motivation: 当前伪造检测器在跨生成器架构（如GAN到扩散模型）时泛化能力不足，主要原因是不同架构产生的伪影存在本质差异。
- Method: 提出TriDetect半监督方法，通过Sinkhorn-Knopp算法进行平衡聚类分配和跨视图一致性机制，学习生成器的潜在架构模式。
- Result: 在两个标准基准和三个真实数据集上评估，与13个基线方法相比，展示了TriDetect对未见生成器的强大泛化能力。
- Conclusion: 通过理论分析生成器架构差异并设计相应检测机制，TriDetect有效解决了跨生成器伪造检测的泛化问题。


### [195] [Shortcut Invariance: Targeted Jacobian Regularization in Disentangled Latent Space](https://arxiv.org/abs/2511.19525)
*Shivam Pal,Sakshi Varshney,Piyush Rai*

Main category: cs.LG

TL;DR: 提出一种简单有效的训练方法，通过功能不变性而非表示不变性来解决深度神经网络中的捷径学习问题，在解耦的潜在空间中识别并削弱捷径特征的影响。

- Motivation: 深度神经网络容易学习训练数据中的捷径相关性，导致分布外泛化失败。现有方法通常学习鲁棒表示，但这种方法复杂、脆弱且难以扩展。
- Method: 在解耦的潜在空间中识别与标签强相关的捷径特征，通过在训练过程中注入有针对性的各向异性潜在噪声，使分类器对这些特征不敏感。
- Result: 在已建立的捷径学习基准测试中实现了最先进的分布外性能。
- Conclusion: 该方法通过功能不变性而非表示不变性，提供了一种简单有效的解决方案，使分类器能够忽略捷径特征并依赖更复杂的核心语义信号。


### [196] [Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport](https://arxiv.org/abs/2511.19561)
*Zecheng Pan,Zhikang Chen,Ding Li,Min Zhang,Sen Cui,Hongshuo Jin,Luqi Tao,Yi Yang,Deheng Ye,Yu Zhang,Tingting Zhu,Tianling Ren*

Main category: cs.LG

TL;DR: OTMF是一种基于最优传输理论的模型融合框架，通过语义几何对齐解决参数插值带来的分布偏移问题，支持持续融合多任务模型。

- Motivation: 现有的模型融合方法主要依赖权重空间的参数插值，这会导致特征空间的显著分布偏移，削弱任务特定知识。需要一种新方法来保持任务特定结构的同时实现有效融合。
- Method: OTMF通过最优传输计划发现应用于任务向量的通用掩码，选择性提取可迁移和任务无关的组件，同时保留每个任务的独特结构特征。支持持续融合范式，无需重新访问先前任务。
- Result: 在多个视觉和语言基准测试上的综合实验表明，OTMF在准确性和效率方面都达到了最先进的性能。
- Conclusion: 该方法在模型融合方面具有重要的实践和理论价值，能够有效解决分布偏移问题，实现高效的多任务模型集成。


### [197] [Learning Massively Multitask World Models for Continuous Control](https://arxiv.org/abs/2511.19584)
*Nicklas Hansen,Hao Su,Xiaolong Wang*

Main category: cs.LG

TL;DR: 提出了一个包含200个多样化任务的基准测试，并开发了Newt模型，通过大规模预训练和在线强化学习实现通用控制能力

- Motivation: 解决连续控制强化学习研究中单任务或离线学习主导的问题，探索单个智能体能否通过在线交互在数百个任务上进行训练
- Method: 先在大规模演示数据上预训练语言条件化的多任务世界模型，获取任务感知表示和动作先验，然后通过所有任务的在线交互进行联合优化
- Result: Newt在多任务性能和数据效率上优于强基线，展现出强大的开环控制能力，并能快速适应未见任务
- Conclusion: 证明了通过大规模预训练和在线强化学习相结合的方法，可以训练出在多样化任务上表现优异的通用控制智能体


### [198] [Terminal Velocity Matching](https://arxiv.org/abs/2511.19797)
*Linqi Zhou,Mathias Parger,Ayaan Haque,Jiaming Song*

Main category: cs.LG

TL;DR: TVM是一种流匹配的泛化方法，支持高保真的一步和少步生成建模，通过建模任意两个扩散时间步之间的转换，并在终端时间而非初始时间进行正则化。

- Motivation: 现有的扩散模型需要多步推理才能达到高质量生成，而TVM旨在实现一步或几步的高质量生成，同时提供理论保证。
- Method: TVM建模任意两个扩散时间步之间的转换，在终端时间进行正则化；引入最小架构修改实现稳定单阶段训练；开发融合注意力内核支持Jacobian-向量积的反向传播。
- Result: 在ImageNet-256x256上，TVM单步生成达到3.29 FID，4步生成达到1.99 FID；在ImageNet-512x512上，单步4.32 FID，4步2.94 FID，代表了从头训练的一步/少步模型的最先进性能。
- Conclusion: TVM在一步和少步生成建模中实现了最先进的性能，为高效高质量生成提供了新的解决方案。


### [199] [On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices](https://arxiv.org/abs/2511.19986)
*Lianming Huang,Haibo Hu,Qiao Li,Nan Guan,Chun Jason Xue*

Main category: cs.LG

TL;DR: 提出了一种按需多任务稀疏框架，通过最大化参数重用和块粒度分解来减少任务切换时的I/O开销，在自动驾驶平台上实现了6.6倍的任务切换加速。

- Motivation: 传统稀疏优化方法为单个任务独立优化稀疏模式，忽略了频繁任务切换时显著的I/O开销问题。
- Method: 将权重分解为可重用的块粒度单元，跨任务对齐稀疏结构以最大化重叠，动态加载下一任务所需的少量差异块集合。
- Result: 在真实自动驾驶平台上的实验表明，该框架实现了优异的切换效率，相比现有稀疏方法平均加速任务切换超过6.6倍。
- Conclusion: 所提出的按需多任务稀疏框架通过参数重用和动态块加载，有效缓解了传统整体方法固有的冷启动延迟问题。


### [200] [Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting](https://arxiv.org/abs/2511.20004)
*Peining Zhang,Hongchen Qin,Haochen Zhang,Ziqi Guo,Guiling Wang,Jinbo Bi*

Main category: cs.LG

TL;DR: 时间序列基础模型在零样本设置下，当输入上下文窗口足够长（覆盖超过1-2个完整季节周期）时，能够超越完全训练的LSTM模型进行叶面积指数预测。

- Motivation: 研究时间序列基础模型在农业监测中零样本预测叶面积指数的能力，探索通用基础模型是否能在无需任务特定调优的情况下超越专门的监督模型。
- Method: 使用HiQ数据集（美国，2000-2022），系统比较统计基线、全监督LSTM和Sundial基础模型在多种评估协议下的表现。
- Result: Sundial基础模型在零样本设置下，当输入上下文窗口足够长时，能够超越完全训练的LSTM模型。
- Conclusion: 预训练的时间序列基础模型在农业和环境应用中具有作为即插即用预测器的强大潜力。


### [201] [Latent Diffusion Inversion Requires Understanding the Latent Space](https://arxiv.org/abs/2511.20592)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 本文研究发现潜在扩散模型在潜在空间中存在非均匀记忆现象，提出基于解码器回拉度量的维度重要性排序方法，显著提升了成员推理攻击性能。

- Motivation: 现有模型反演技术主要关注数据域扩散模型，而忽略了潜在空间生成模型中的编码器/解码器对和潜在代码对记忆化的影响。
- Method: 提出基于解码器回拉度量的潜在维度重要性排序方法，识别对记忆化贡献最大的维度，并在计算攻击统计量时移除较少记忆的维度。
- Result: 在多个数据集上，该方法使成员推理攻击的AUROC平均提升2.7%，TPR@1%FPR显著提高6.42%，在极低误报率下能更可靠地识别成员数据。
- Conclusion: 研究揭示了自编码器几何结构对LDM记忆化的被忽视影响，为分析基于扩散的生成模型的隐私风险提供了新视角。
## cs.MM

### [202] [It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models](https://arxiv.org/abs/2511.19877)
*Xiangyu Zhao,Yaling Shen,Yiwen Jiang,Zimu Wang,Jiahe Liu,Maxmartwell H Cheng,Guilherme C Oliveira,Robert Desimone,Dominic Dwyer,Zongyuan Ge*

Main category: cs.MM

TL;DR: 提出了一种新颖的多模态大语言模型框架用于抑郁症检测，通过时间戳级别的视听特征对齐来改进跨模态时序动态建模，在DAIC-WoZ数据集上表现优于单模态方法和先前多模态方法。

- Motivation: 抑郁症是全球最普遍的心理健康障碍之一。传统大语言模型主要处理文本，无法处理音频和视觉模态中的丰富非语言线索，而这些线索在心理健康评估中至关重要。虽然多模态LLM提供了有前景的方向，但很少有专门为心理应用定制的模型。
- Method: 提出多模态LLM框架，通过增强音频语言模型的视觉理解能力，并在时间戳级别对齐视听特征。这种细粒度对齐改进了跨模态的时序动态建模，同时减少了对大量训练数据和计算资源的需求。
- Result: 在DAIC-WoZ数据集上的实验表明，该模型在抑郁症检测方面优于单模态方法和先前多模态方法。
- Conclusion: 所提出的框架可以扩展到整合额外的生理信号，为超越心理健康的更广泛临床应用铺平了道路。
## math.OC

### [203] [Optimization of Sums of Bivariate Functions: An Introduction to Relaxation-Based Methods for the Case of Finite Domains](https://arxiv.org/abs/2511.20607)
*Nils Müller*

Main category: math.OC

TL;DR: 该论文研究了在有限域上优化由多个双变量函数之和构成的函数，证明了此类优化问题的NP等价性，并提出了基于松弛、ℓ²近似和熵正则化的可处理求解方法。

- Motivation: 研究具有双变量函数和结构的函数优化问题，这类问题在多种应用中出现但计算复杂度高，需要开发有效的求解方法。
- Method: 使用测度值扩展（松弛）、ℓ²近似和熵正则化技术，提出了可通过线性规划、坐标上升法和闭式解求解的多种可处理问题表述。
- Result: 证明了双变量函数和优化问题的NP等价性，发现了此类优化中存在"免费午餐"现象，并通过实验验证了所提方法在随机函数、顶点着色和信号重建问题中的有效性。
- Conclusion: 双变量函数和是一类重要的函数类，通过适当的松弛和正则化技术可以设计出有效的优化算法，但应用这些技术存在理论限制。
## eess.SP

### [204] [Redefining Radar Segmentation: Simultaneous Static-Moving Segmentation and Ego-Motion Estimation using Radar Point Clouds](https://arxiv.org/abs/2511.20003)
*Simin Zhu,Satish Ravindran,Alexander Yarovoy,Francesco Fioranelli*

Main category: eess.SP

TL;DR: 提出了一种基于神经网络的雷达点云分割方法，能够同时分割静态和动态物体，并估计车辆自身运动速度，无需复杂的预处理步骤。

- Motivation: 传统雷达分割研究主要关注物体类别标签，但雷达与光学传感器的差异使得类别预测不可靠。在汽车雷达感知任务中，判断物体是静态还是动态是大多数任务的前提条件。
- Method: 使用多层感知机（MLPs）和循环神经网络（RNNs）构建特征提取模块，直接从原始雷达点云中提取信息，无需点云聚合、多普勒补偿、运动补偿等中间处理步骤。
- Result: 在RadarScenes真实世界雷达数据集上测试，该方法在双重任务上表现良好，并展示了在其他雷达感知任务中的广泛应用潜力。
- Conclusion: 该方法首次实现了直接从原始雷达点云同时分割静态/动态物体和估计自身运动，证明了简单而有效的神经网络模块在复杂雷达感知任务中的可行性。
## cs.RO

### [205] [Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation](https://arxiv.org/abs/2511.18525)
*Samarth Chopra,Jing Liang,Gershom Seneviratne,Yonghan Lee,Jaehoon Choi,Jianyu An,Stephen Cheng,Dinesh Manocha*

Main category: cs.RO

TL;DR: Splatblox是一个实时自主导航系统，用于在具有密集植被、不规则障碍物和复杂地形的户外环境中导航。它融合RGB图像和LiDAR点云，使用高斯泼溅构建可通行性感知的ESDF，支持语义推理和长距离任务。

- Motivation: 解决户外环境中密集植被、不规则障碍物和复杂地形带来的自主导航挑战，需要能够区分可穿越植被和刚性障碍物的系统。
- Method: 融合分割RGB图像和LiDAR点云，使用高斯泼溅构建可通行性感知的欧几里得符号距离场(ESDF)，在线更新以支持语义推理和360度几何覆盖。
- Result: 在植被丰富的场景中，相比最先进方法，成功率提高50%以上，冻结事件减少40%，路径缩短5%，目标到达时间加快13%，支持长达100米的长距离任务。
- Conclusion: Splatblox系统在户外复杂环境中表现出优越的导航性能，能够有效区分可穿越植被和障碍物，支持多平台部署。


### [206] [ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation](https://arxiv.org/abs/2511.20330)
*Yuhan Wu,Tiantian Wei,Shuo Wang,ZhiChao Wang,Yanyong Zhang,Daniel Cremers,Yan Xia*

Main category: cs.RO

TL;DR: 提出了ArtiBench基准测试和ArtiBrain框架，用于解决交互式关节物体操作中的泛化挑战，显著优于现有方法。

- Motivation: 现有视觉语言和基于扩散的策略在跨部件、实例和类别的关节操作中泛化能力不足，需要新的基准和方法来应对这些挑战。
- Method: ArtiBrain框架结合了基于VLM的任务推理器进行子目标分解和验证，以及混合控制器（几何感知关键帧执行+功能引导扩散），并通过功能记忆库积累和传播成功经验。
- Result: 在ArtiBench上的广泛实验表明，ArtiBrain在鲁棒性和泛化性方面显著优于最先进的多模态和基于扩散的方法。
- Conclusion: ArtiBrain通过统一高层推理和自适应低层控制，有效解决了关节物体操作中的泛化问题，为交互式操作提供了新的解决方案。
## cs.AI

### [207] [Fara-7B: An Efficient Agentic Model for Computer Use](https://arxiv.org/abs/2511.19663)
*Ahmed Awadallah,Yash Lara,Raghav Magazine,Hussein Mozannar,Akshay Nambi,Yash Pandya,Aravind Rajeswaran,Corby Rosset,Alexey Taymanov,Vibhav Vineet,Spencer Whitehead,Andrew Zhao*

Main category: cs.AI

TL;DR: FaraGen是一个用于多步骤网页任务的合成数据生成系统，能够生成多样化的任务和解决方案，并过滤出成功的轨迹。基于这些数据训练的Fara-7B模型在多个基准测试中表现优异，甚至能与更大的前沿模型竞争。

- Motivation: 计算机使用代理（CUAs）的发展受到缺乏大规模高质量人类与计算机交互数据集的限制，而现有文本数据不足以支持CUA的发展。
- Method: 开发FaraGen系统生成合成数据：从常用网站提出多样化任务，生成多个解决方案尝试，使用多个验证器过滤成功轨迹。基于这些数据训练Fara-7B模型，该模型仅通过截图感知计算机，通过预测坐标执行动作。
- Result: FaraGen以约1美元的成本生成每个验证轨迹，具有高吞吐量、产量和多样性。Fara-7B在WebVoyager、Online-Mind2Web和WebTailBench等基准测试中优于同类规模的CUA模型，并与更大的前沿模型竞争。
- Conclusion: 可扩展的数据生成系统在推进小型高效代理模型方面具有关键优势，Fara-7B模型已开源发布。


### [208] [Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs](https://arxiv.org/abs/2511.19773)
*Meng Lu,Ran Xu,Yi Fang,Wenxuan Zhang,Yue Yu,Gaurav Srivastava,Yuchen Zhuang,Mohamed Elhoseiny,Charles Fleming,Carl Yang,Zhengzhong Tu,Yang Xie,Guanghua Xiao,Hanrui Wang,Di Jin,Wenqi Shi,Xuan Wang*

Main category: cs.AI

TL;DR: VISTA-Gym是一个可扩展的训练环境，用于提升视觉语言模型的多步骤视觉推理能力，通过工具集成和强化学习训练VISTA-R1模型，在11个VQA基准测试中显著优于同类模型。

- Motivation: 现有的视觉语言模型在图像理解方面表现出色，但在多步骤视觉交互推理方面能力有限，需要开发能够有效集成视觉工具进行推理的系统。
- Method: 构建VISTA-Gym统一训练环境，整合7个任务的13个数据集，提供标准化视觉工具接口、可执行交互循环和反馈信号，通过多轮轨迹采样和端到端强化学习训练VISTA-R1模型。
- Result: VISTA-R1-8B在11个推理密集型VQA基准测试中，比同类最优基线模型性能提升9.51%-18.72%。
- Conclusion: VISTA-Gym是解锁视觉语言模型工具集成推理能力的有效训练平台，显著提升了模型的视觉推理性能。


### [209] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是首个将导航性能与经济可行性结合的微导航经济测试平台，通过成本-收益分析揭示导航研究指标与商业部署之间的差距。

- Motivation: 现有导航基准只关注任务成功率，忽略了经济可行性这一对自主配送机器人商业部署至关重要的因素。
- Method: 建立完整的成本-收益分析模型，包括硬件、训练、能源、维护成本和配送收入，使用行业参数进行投影分析。
- Result: 基准模型达到43.0%的服务水平协议合规率，但商业上不可行：每次运行亏损30.009美元，碰撞导致的维护成本占总成本的99.7%。
- Conclusion: 优化任务成功率与优化经济部署存在根本差异，CostNav填补了导航研究与商业部署之间的鸿沟，为不同导航范式的经济权衡提供数据驱动决策基础。


### [210] [VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422)
*Bo Pang,Chenxi Xu,Jierui Ren,Guoping Wang,Sheng Li*

Main category: cs.AI

TL;DR: VibraVerse是一个大规模几何-声学对齐数据集，通过明确的物理因果链连接3D几何、物理属性、模态参数和声学信号，用于物理一致的多模态学习。

- Motivation: 现有多模态学习框架缺乏物理一致性，忽视了物体几何、材料、振动模式和产生声音之间的内在因果关系。
- Method: 引入CLASP对比学习框架进行跨模态对齐，保持物体物理结构与其声学响应之间的因果对应关系；通过计算模态特征频率和特征向量进行冲击声音合成。
- Result: 在几何到声音预测、声音引导形状重建和跨模态表示学习等基准任务上，基于VibraVerse训练的模型展现出更高的准确性、可解释性和跨模态泛化能力。
- Conclusion: VibraVerse为物理一致和因果可解释的多模态学习建立了基准，为声音引导的具身感知和对物理世界的深入理解提供了基础。


### [211] [Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models](https://arxiv.org/abs/2511.20531)
*Shamima Hossain*

Main category: cs.AI

TL;DR: 该论文提出了一个基于知识图谱的视觉语言模型推理框架，通过多跳验证提升事实准确性，在初步实验中实现了约31%的事实准确性提升。

- Motivation: 视觉语言模型虽然功能强大，但常常产生事实不准确的输出，缺乏稳健的推理能力。在需要跨模态无缝桥接的VLM中，整合外部知识进行推理的研究仍然不足。
- Method: 引入知识引导的VLM推理框架，利用结构化知识图谱进行多跳验证，包括视觉实体识别、知识图谱遍历和基于事实的标题精炼等系统推理步骤。
- Result: 在由Google Landmarks v2、Conceptual captions和Coco captions混合构建的数据集上进行初步实验，结果显示事实准确性提高了约31%。
- Conclusion: 这项工作展示了整合外部知识在推进VLM推理方面的潜力，为构建更可靠和知识丰富的多模态系统铺平了道路。
## cs.CR

### [212] [SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.19558)
*Mohammed Talha Alam,Nada Saadi,Fahad Shamshad,Nils Lukas,Karthik Nandakumar,Fahkri Karray,Samuele Poppi*

Main category: cs.CR

TL;DR: 本文研究了文本到图像扩散模型在良性微调后安全对齐失效的问题，提出了SPQR基准来评估安全对齐方法在微调后的稳定性。

- Motivation: 现有安全对齐方法很少测试在部署后常规良性微调（如LoRA个性化、风格/领域适配器）下的安全性保持情况，而真实的安全对齐必须能够承受这些后部署调整。
- Method: 引入SPQR基准（安全-提示遵循-质量-鲁棒性），这是一个单一评分指标，通过报告单一排行榜分数来标准化评估安全对齐扩散模型在良性微调下保持安全、效用和鲁棒性的能力。
- Result: 研究发现当前安全方法在良性微调下经常失效，通过多语言、领域特定和分布外分析识别了安全对齐在良性微调后失效的具体情况。
- Conclusion: SPQR作为一个简洁而全面的基准，为T2I模型的安全对齐技术提供了标准化评估框架，展示了其在评估安全对齐稳定性方面的价值。


### [213] [Frequency Bias Matters: Diving into Robust and Generalized Deep Image Forgery Detection](https://arxiv.org/abs/2511.19886)
*Chi Liu,Tianqing Zhu,Wanlei Zhou,Wei Zhao*

Main category: cs.CR

TL;DR: 本文从频率视角分析了AI生成图像伪造检测器的泛化性和鲁棒性问题，提出了一种两步频率对齐方法，既能作为反取证攻击，也能作为通用防御手段。

- Motivation: 随着AI生成图像伪造技术的发展，检测器的泛化性和鲁棒性成为关键问题。现有研究未能深入探讨这些问题的根本原因，且缺乏同时适用于取证和反取证场景的通用方法。
- Method: 提出两步频率对齐方法：首先分析DNN伪造检测器的频率偏差问题，然后通过消除真实与伪造图像之间的频率差异来解决问题。该方法可双向应用——作为反取证的黑盒攻击或作为取证的通用防御。
- Result: 在涉及12个检测器、8个伪造模型和5个评估指标的多种实验设置中，验证了频率对齐方法的有效性，证明了其在攻击和防御两方面的优越性能。
- Conclusion: 频率偏差是导致伪造检测器泛化性和鲁棒性问题的根本原因，频率对齐方法为解决这些问题提供了有效的双向解决方案，在取证和反取证领域都具有重要应用价值。
## eess.IV

### [214] [Not Quite Anything: Overcoming SAMs Limitations for 3D Medical Imaging](https://arxiv.org/abs/2511.19471)
*Keith Moore*

Main category: eess.IV

TL;DR: 提出了一种组合式方法，将基础分割模型(SAM-2)的输出作为额外输入通道与MRI图像一起处理，用于脑部MRI中边界模糊结构的精确分割，避免了基础模型的微调。

- Motivation: 基础分割模型如SAM和SAM-2在自然图像上表现良好，但在脑部MRI中对于边界模糊、对比度低的结构(如尾状核和丘脑)分割效果不佳，需要一种无需重新训练基础模型的适应方法。
- Method: 使用轻量级3D U-Net生成SAM-2的提示，将基础模型输出作为额外输入通道与MRI图像组合；平滑处理基础模型猜测的边缘以改善对齐；同时测试了使用DINO注意力图的无提示分割。
- Result: 在基底神经节分割上达到约96%的体积精度，足以满足纵向体积变化研究需求；方法快速、标签高效且对分布外扫描具有鲁棒性。
- Conclusion: 这种组合架构避免了修改基础模型权重，能够适应领域偏移而无需重新训练基础模型，成功应用于研究突发性儿童强迫症中与炎症相关的体积变化。


### [215] [A Multi-Stage Deep Learning Framework with PKCP-MixUp Augmentation for Pediatric Liver Tumor Diagnosis Using Multi-Phase Contrast-Enhanced CT](https://arxiv.org/abs/2511.19478)
*Wanqi Wang,Chun Yang,Jianbo Shao,Yaokai Zhang,Xuehua Peng,Jin Sun,Chao Xiong,Long Lu,Lianting Hu*

Main category: eess.IV

TL;DR: 开发了一个多阶段深度学习框架，使用多期相增强CT自动诊断儿童肝肿瘤，解决了数据稀缺和类别不平衡问题，实现了高精度的良恶性分类和亚型诊断。

- Motivation: 儿童肝肿瘤是儿科常见实体肿瘤，传统病理活检具有侵入性风险且儿童配合度差，需要建立无创诊断方法。现有AI研究多忽视儿童肝肿瘤领域。
- Method: 采用多阶段深度学习框架：1) PKCP-MixUp数据增强方法解决数据稀缺；2) 肿瘤检测模型提取ROI；3) 两阶段诊断流程（良恶性分类→亚型分类）使用三个主干网络。
- Result: 肿瘤检测模型mAP=0.871，良恶性分类AUC=0.989，良性亚型分类AUC=0.915，恶性亚型分类AUC=0.979。进行了消融研究和可解释性分析。
- Conclusion: 该框架填补了儿童特异性DL诊断空白，为CT期相选择和模型设计提供可行见解，为精确、可及的儿童肝肿瘤诊断铺平道路。


### [216] [The Selective Disk Bispectrum and Its Inversion, with Application to Multi-Reference Alignment](https://arxiv.org/abs/2511.19706)
*Adele Myers,Nina Miolane*

Main category: eess.IV

TL;DR: 本文提出了一种选择性圆盘双谱方法，用于图像的旋转不变表示，解决了传统方法计算复杂度高和不可逆的问题。

- Motivation: 在计算机视觉和形状分析任务中，需要学习物体的形状信息而忽略其方向。现有方法在实现2D旋转不变性时面临计算复杂度高和不可逆的挑战。
- Method: 推导了圆盘双谱的显式逆变换，定义了选择性圆盘双谱，仅使用最小数量的系数实现准确的形状恢复。
- Result: 该方法能够实现旋转图像的多参考对齐，这是传统圆盘双谱方法之前无法完成的任务。
- Conclusion: 圆盘双谱成为旋转不变形状数据学习的实用且理论基础扎实的工具。


### [217] [DLADiff: A Dual-Layer Defense Framework against Fine-Tuning and Zero-Shot Customization of Diffusion Models](https://arxiv.org/abs/2511.19910)
*Jun Jia,Hongyi Miao,Yingjie Zhou,Linhan Cao,Yanwei Jiang,Wangqiu Zhou,Dandan Zhu,Hua Yang,Wei Sun,Xiongkuo Min,Guangtao Zhai*

Main category: eess.IV

TL;DR: 提出DLADiff方法防御扩散模型的微调和零样本生成攻击，保护人脸隐私。

- Motivation: 随着扩散模型技术的发展，恶意攻击者只需少量甚至单张图像就能创建高度逼真的合成身份，现有防御方法主要针对微调攻击，忽视了零样本生成防御。
- Method: DLADiff包含双层保护机制：第一层使用双代理模型(DSUR)和交替动态微调(ADFT)防御微调攻击；第二层简单但有效地防御零样本生成方法。
- Result: 实验结果表明，该方法在防御扩散模型微调方面显著优于现有方法，在防御零样本生成方面实现了前所未有的性能。
- Conclusion: DLADiff能有效防御扩散模型的微调和零样本生成攻击，为人脸隐私保护提供了全面解决方案。


### [218] [Development of a fully deep learning model to improve the reproducibility of sector classification systems for predicting unerupted maxillary canine likelihood of impaction](https://arxiv.org/abs/2511.20493)
*Marzio Galdi,Davide Cannatà,Flavia Celentano,Luigia Rizzo,Domenico Rossi,Tecla Bocchino,Stefano Martina*

Main category: eess.IV

TL;DR: 开发深度学习模型用于预测未萌出上颌尖牙阻生可能性，比较不同扇形分类系统的可重复性，并评估AI模型在分类任务中的表现。

- Motivation: 当前扇形分类系统在预测未萌出上颌尖牙阻生可能性时存在操作者间和操作者内可重复性差异的问题，需要开发更可靠的自动化分类方法。
- Method: 3名正畸医生和3名全科牙医使用三种扇形分类系统（5区、4区、3区）对306张X光片进行分类评估，四周后重复评估。同时使用预训练在1,222张X光片上的不同AI模型进行测试。
- Result: 3区分类系统具有最高可重复性，Cohen's K值0.80-0.92，总体一致性0.85。教育背景不影响一致性。DenseNet121模型在三个类别中表现最佳，总体准确率76.8%。
- Conclusion: AI模型能够自动分类未萌出上颌尖牙的位置，为临床诊断提供可靠工具。
## physics.ao-ph

### [219] [PhysDNet: Physics-Guided Decomposition Network of Side-Scan Sonar Imagery](https://arxiv.org/abs/2511.19539)
*Can Lei,Hayat Rajani,Nuno Gracias,Rafael Garcia,Huigang Wang*

Main category: physics.ao-ph

TL;DR: PhysDNet是一个物理引导的多分支网络，能够将侧扫声纳图像解耦为三个可解释的物理场：海底反射率、地形高程和传播损耗，从而解决声纳图像因视角依赖性强而影响下游分析的问题。

- Motivation: 侧扫声纳图像强度受到海底反射率、地形高程和声传播损耗的复杂耦合影响，导致图像具有强烈的视角依赖性，降低了后续分析的鲁棒性。需要一种方法来解耦这些物理因素。
- Method: 提出PhysDNet网络，通过嵌入Lambertian反射模型，将侧扫声纳图像分解为三个物理场，并利用重建声纳强度实现无监督训练，无需真实标注数据。
- Result: 实验表明，分解后的表示能够保持稳定的地质结构，捕捉物理一致的照明和衰减效应，并生成可靠的阴影图。
- Conclusion: 物理引导的分解为侧扫声纳分析提供了稳定且可解释的领域，既提高了物理一致性，也改善了配准和阴影解释等下游任务的性能。
