[[toc]]

## cs.CV

### [1] [CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge](https://arxiv.org/abs/2506.02010)
*Zehua Liu,Xiaolou Li,Chen Chen,Lantian Li,Dong Wang*

Main category: cs.CV

TL;DR: CNVSRC 2024是中文连续视觉语音识别挑战赛的延续，改进了基线系统和数据集，推动了该领域的技术进步。

- Motivation: 推动中文大词汇量连续视觉语音识别（LVC-VSR）的研究，通过挑战赛形式促进技术创新。
- Method: 使用CNVSRC 2023相同的数据集，但引入更强的基线系统和新增数据集CN-CVS2-P1，改进数据预处理、特征提取、模型设计和训练策略。
- Result: 挑战赛展示了在数据预处理、特征提取和模型设计等方面的重要创新，进一步提升了中文LVC-VSR的技术水平。
- Conclusion: CNVSRC 2024通过改进和新增数据集，显著推动了中文LVC-VSR领域的发展。


### [2] [OASIS: Online Sample Selection for Continual Visual Instruction Tuning](https://arxiv.org/abs/2506.02011)
*Minjae Lee,Minhyuk Seo,Tingyu Qu,Tinne Tuytelaars,Jonghyun Choi*

Main category: cs.CV

TL;DR: OASIS是一种自适应在线样本选择方法，用于持续视觉指令调优（CVIT），动态调整每批样本选择，减少冗余，性能优于现有方法。

- Motivation: 在CVIT场景中，多模态数据以流式方式持续到达，传统数据选择方法依赖预训练参考模型，无法适应未知未来数据，且固定样本选择方法易受分布偏移影响。
- Method: OASIS通过动态调整每批样本选择数量（基于批次间信息量）和迭代更新选择分数，减少样本冗余。
- Result: 实验表明，OASIS仅用25%数据即可达到全数据训练性能，并优于现有方法。
- Conclusion: OASIS解决了CVIT中的数据选择和分布偏移问题，显著提升了效率和性能。


### [3] [Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing](https://arxiv.org/abs/2506.02012)
*Zehua Liu,Xiaolou Li,Li Guo,Lantian Li,Dong Wang*

Main category: cs.CV

TL;DR: 本文探讨了如何更好地利用大型语言模型（LLMs）提升视觉语音识别（VSR）性能，提出了三项关键贡献：规模测试、上下文感知解码和迭代优化。

- Motivation: 尽管LLMs在VSR系统中已显示出性能提升，但其潜力尚未充分研究，如何有效利用LLMs仍待探索。
- Method: 通过规模测试研究LLM大小对VSR的影响，引入上下文感知解码和迭代优化方法。
- Result: 实验证明这些方法能显著提升VSR性能。
- Conclusion: 本文展示了LLMs在VSR任务中的巨大潜力，并提供了有效的利用方法。


### [4] [Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization](https://arxiv.org/abs/2506.02014)
*Wang Mengjie,Zhu Huiping,Li Jian,Shi Wenxiu,Zhang Song*

Main category: cs.CV

TL;DR: 本文提出了一种优化多模态模型在驾驶场景中的方法，包括动态提示优化、数据集构建、模型训练和部署，显著提升了模型精度和资源利用率。

- Motivation: 随着自动驾驶技术的发展，理解复杂驾驶场景的需求增加，但多模态模型在垂直领域的应用面临数据收集、训练和部署等挑战。
- Method: 方法包括动态提示优化、结合真实与合成数据的数据集构建、知识蒸馏和量化等训练技术。
- Result: 实验表明，该方法显著提升了模型在关键任务中的精度，并实现了高效的资源利用。
- Conclusion: 该方法为驾驶场景感知技术的实际应用提供了有力支持。


### [5] [Object-centric Self-improving Preference Optimization for Text-to-Image Generation](https://arxiv.org/abs/2506.02015)
*Yoonjin Oh,Yongjin Kim,Hyomin Kim,Donghwan Chi,Sungwoong Kim*

Main category: cs.CV

TL;DR: 提出了一种名为OSPO的框架，用于改进多模态大语言模型（MLLMs）在文本到图像生成任务中的细粒度视觉理解能力。

- Motivation: 尽管MLLMs在图像理解和生成方面取得了进展，但在细粒度视觉理解（尤其是文本到图像生成任务）中仍存在不足。偏好优化方法在图像理解任务中已有应用，但在图像生成领域尚未充分探索。
- Method: OSPO框架利用MLLMs的内在推理能力，无需外部数据集或模型。通过对象级对比偏好对的自改进机制（包括对象中心提示扰动、密集化和VQA评分）生成高质量偏好对数据。
- Result: 在三个代表性的文本到图像生成基准测试中，OSPO显著优于基线模型。
- Conclusion: OSPO通过自改进机制和高质量偏好对数据，有效提升了MLLMs在文本到图像生成任务中的性能。


### [6] [Are classical deep neural networks weakly adversarially robust?](https://arxiv.org/abs/2506.02016)
*Nuolin Sun,Linyuan Wang,Dongyang Li,Bin Yan,Lei Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于层间特征路径相关性的对抗样本检测与图像识别方法，避免了计算开销大的对抗训练，展示了DNN固有的对抗鲁棒性。

- Motivation: 对抗训练虽能提升DNN的对抗鲁棒性，但计算成本高，因此需要一种更高效的方法。
- Method: 利用DNN各层输出特征的聚类特性，构建特征路径并计算其与类中心特征路径的相关性。
- Result: 在ResNet-20上，干净准确率为82.77%，对抗准确率为44.17%；在ResNet-18上分别为80.01%和46.1%。
- Conclusion: 该方法揭示了DNN固有的对抗鲁棒性，挑战了传统认知，且无需依赖高计算成本的防御策略。


### [7] [Fairness through Feedback: Addressing Algorithmic Misgendering in Automatic Gender Recognition](https://arxiv.org/abs/2506.02017)
*Camilla Quaresmini,Giacomo Zanotti*

Main category: cs.CV

TL;DR: 论文探讨了自动性别识别（AGR）系统的问题，提出理论及实践上的改进建议，强调尊重个体权利和自我表达。

- Motivation: AGR系统基于性别二元假设，分类结果与性别表达不符，尤其对非二元性别者不准确，需重新思考其设计。
- Method: 区分性别、性别表达与生理性别，提出用户可修正系统输出的反馈机制。
- Result: 反馈机制虽降低系统自主性，但能显著提升AGR的公平性。
- Conclusion: AGR系统应作为尊重个体权利和自我表达的工具，需改变现有范式。


### [8] [Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying](https://arxiv.org/abs/2506.02020)
*Youze Xue,Dian Li,Gang Liu*

Main category: cs.CV

TL;DR: 本文分析了多模态大语言模型（MLLMs）中硬负样本对对比学习的贡献，提出了一种显式梯度放大器方法，显著提升了嵌入性能。

- Motivation: 尽管硬负样本挖掘在多模态嵌入中至关重要，但其具体贡献未被深入研究。本文旨在通过梯度分析揭示其作用，并提出改进方法。
- Method: 通过分析info-NCE损失的梯度，提出显式梯度放大器，增强硬负样本的梯度，从而提升嵌入的判别性。
- Result: 基于LLaVA-OneVision-7B架构的模型在MMEB基准测试中达到SOTA性能，结合自研MLLM QQMM后更在MMEB排行榜中排名第一。
- Conclusion: 显式梯度放大器有效提升了多模态嵌入的性能，为硬负样本挖掘提供了新思路。


### [9] [Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics](https://arxiv.org/abs/2506.02021)
*Yinjie Zhao,Heng Zhao,Bihan Wen,Yew-Soon Ong,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种动态感知视频蒸馏（DAViD）方法，通过强化学习预测最优时间分辨率，解决了视频数据集蒸馏中的冗余问题。

- Motivation: 视频数据集因时间信息和类别冗余差异而面临独特挑战，现有方法假设时间冗余均匀，限制了效果。
- Method: 采用强化学习（RL）预测合成视频的最优时间分辨率，并提出教师循环奖励函数更新RL策略。
- Result: DAViD显著优于现有方法，性能大幅提升。
- Conclusion: 该研究为语义自适应视频数据集蒸馏的未来研究奠定了基础。


### [10] [Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs](https://arxiv.org/abs/2506.02022)
*Aditya Kanade,Tanuja Ganu*

Main category: cs.CV

TL;DR: MLLMs在视觉感知上存在缺陷，即使答案正确也可能误解关键视觉元素。研究提出了一个名为'Do You See Me'的基准测试，发现MLLMs在复杂任务中表现远低于人类。

- Motivation: 揭示多模态大语言模型（MLLMs）在视觉感知上的不足，并提供一个系统性评估工具。
- Method: 构建包含1,758张图像和2,612个问题的基准测试，涵盖7个子任务，评估3个闭源和5个开源MLLMs。
- Result: 人类准确率96.49%，而顶级MLLMs平均低于50%，任务复杂度增加时差距更大。
- Conclusion: MLLMs需要更强大的视觉感知能力，特别是在细粒度细节处理上。


### [11] [Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences](https://arxiv.org/abs/2506.02095)
*Hyojin Bahng,Caroline Chan,Fredo Durand,Phillip Isola*

Main category: cs.CV

TL;DR: 提出了一种利用循环一致性作为监督信号的方法，用于语言与视觉对齐，避免了传统依赖人工或AI偏好的高成本问题。

- Motivation: 多模态数据日益复杂，现有方法依赖人工或AI偏好，成本高且耗时。
- Method: 通过文本到图像模型的循环一致性计算相似性，构建偏好数据集并训练奖励模型。
- Result: 奖励模型在详细描述任务中优于现有对齐指标，且提升了多种视觉语言任务和文本到图像生成的性能。
- Conclusion: 循环一致性方法高效且可扩展，为多模态对齐提供了新思路。


### [12] [SAB3R: Semantic-Augmented Backbone in 3D Reconstruction](https://arxiv.org/abs/2506.02112)
*Xuweiyi Chen,Tian Xia,Sihan Xu,Jianing Yang,Joyce Chai,Zezhou Cheng*

Main category: cs.CV

TL;DR: 论文提出新任务“Map and Locate”，结合开放词汇分割与3D重建，并介绍基线方法SAB3R，性能优于单独使用MASt3R和CLIP。

- Motivation: 统一开放词汇分割和3D重建，为现实世界AI应用提供关键步骤。
- Method: 基于MASt3R，引入轻量级蒸馏策略，将2D视觉骨干的语义特征迁移到3D模型中。
- Result: SAB3R在Map and Locate任务中表现优异，并在2D语义分割和3D任务中验证有效性。
- Conclusion: SAB3R成功结合了分割与重建任务，为未来研究提供了实用基线。


### [13] [Implicit Deformable Medical Image Registration with Learnable Kernels](https://arxiv.org/abs/2506.02150)
*Stefano Fogarollo,Gregor Laimer,Reto Bale,Matthias Harders*

Main category: cs.CV

TL;DR: 提出了一种新型隐式医学图像配准框架，通过稀疏关键点对应重建密集位移场，提高了配准的准确性和可靠性。

- Motivation: 解决AI方法在医学图像配准中产生的不可靠变形问题，以支持肿瘤治疗等临床需求。
- Method: 将图像配准重新定义为信号重建问题，学习核函数从稀疏关键点重建位移场，并采用分层架构进行粗到细的估计。
- Result: 在胸部和腹部零样本配准任务中表现优异，优于现有方法，且变形更符合解剖关系。
- Conclusion: 该方法在准确性和泛化性上表现突出，具有临床应用的潜力。


### [14] [TIIF-Bench: How Does Your T2I Model Follow Your Instructions?](https://arxiv.org/abs/2506.02161)
*Xinyu Wei,Jinrui Zhang,Zeqing Wang,Hongyang Wei,Zhen Guo,Lei Zhang*

Main category: cs.CV

TL;DR: TIIF-Bench是一个新的文本到图像（T2I）模型评估基准，旨在通过多样化和复杂的提示以及精细的评估指标，系统评估模型对复杂文本指令的遵循能力。

- Motivation: 现有T2I模型评估基准在提示多样性和复杂性以及评估指标上存在不足，难以评估文本指令与生成图像之间的细粒度对齐性能。
- Method: TIIF-Bench包含5000个按多维度组织的提示，分为三个难度级别，并为每个提示提供短版和长版。引入文本渲染和风格控制两个关键属性，并利用大型视觉语言模型的世界知识提出可计算框架。
- Result: 通过主流T2I模型的基准测试，分析了当前模型的优缺点，并揭示了现有T2I基准的局限性。
- Conclusion: TIIF-Bench为T2I模型的细粒度评估提供了系统工具，揭示了当前模型的不足和未来改进方向。


### [15] [Quantifying task-relevant representational similarity using decision variable correlation](https://arxiv.org/abs/2506.02164)
*Yu,Qian,Wilson S. Geisler,Xue-Xin Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（DVC）来比较模型和猴子大脑在图像分类任务中的决策策略相似性，发现模型与猴子之间的相似性低于模型之间或猴子之间的相似性，且随着模型性能提升反而降低。

- Motivation: 探讨深度学习模型与猴子大脑在图像分类任务中的决策策略是否相似，以揭示两者在任务相关表征上的差异。
- Method: 使用决策变量相关（DVC）方法量化模型和猴子大脑在分类任务中的决策策略相似性，并评估其在V4/IT记录和模型上的表现。
- Result: 模型间相似性与猴子间相似性相当，但模型与猴子间相似性较低且随模型性能提升而下降；对抗训练和更大数据集预训练未提升模型与猴子相似性。
- Conclusion: 猴子V4/IT与图像分类模型在任务相关表征上存在根本性差异，模型优化方向可能与生物视觉系统不一致。


### [16] [Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos](https://arxiv.org/abs/2506.02167)
*Aditi Tiwari,Farzaneh Masoud,Dac Trong Nguyen,Jill Kraft,Heng Ji,Klara Nahrstedt*

Main category: cs.CV

TL;DR: Fire360是一个用于评估消防场景中感知和推理能力的基准数据集，包含228个360度视频，支持五项任务，旨在提升AI在恶劣环境下的表现。

- Motivation: 现代AI系统在可靠性要求高的环境中表现不佳，消防员因感知问题受伤的情况频发，因此需要开发更强大的感知和推理模型。
- Method: Fire360数据集包含多样化的消防训练视频，标注了动作、物体位置和退化信息，支持五项任务，包括视觉问答、动作描述等。
- Result: 人类专家在TOR任务中达到83.5%的准确率，而GPT-4o等模型表现较差，显示出在退化条件下的推理缺陷。
- Conclusion: 通过发布Fire360数据集，目标是推动AI在不确定环境下的感知、记忆、推理和行动能力。


### [17] [Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment](https://arxiv.org/abs/2506.02221)
*Johannes Schusterbauer,Ming Gui,Frank Fundel,Björn Ommer*

Main category: cs.CV

TL;DR: Diff2Flow框架通过时间步重缩放、插值对齐和速度场转换，将预训练扩散模型知识高效迁移至流匹配模型，提升性能并减少计算开销。

- Motivation: 解决预训练扩散模型知识向流匹配模型高效迁移的挑战，利用扩散模型的架构和生态优势。
- Method: 提出Diff2Flow框架，通过时间步重缩放、插值对齐和速度场转换，实现扩散模型与流匹配模型的兼容性。
- Result: Diff2Flow在参数高效约束下优于原生流匹配和扩散微调，并在多任务中达到或超越最新方法。
- Conclusion: Diff2Flow为扩散模型与流匹配模型的结合提供了高效解决方案，具有广泛的应用潜力。


### [18] [VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis](https://arxiv.org/abs/2506.02229)
*Manas Mehta,Yimu Pan,Kelly Gallagher,Alison D. Gernand,Jeffery A. Goldstein,Delia Mwinyelle,Leena Mithal,James Z. Wang*

Main category: cs.CV

TL;DR: 论文提出两种改进视觉-语言对比学习框架的方法，以提高胎盘病理检测的准确性和效率，包括文本锚定的知识蒸馏和无监督预蒸馏。

- Motivation: 现有自动化方法计算量大，限制了其实际部署，因此需要更高效且准确的解决方案。
- Method: 1. 文本锚定的视觉-语言对比知识蒸馏（VLCD）；2. 使用大型自然图像数据集进行无监督预蒸馏以优化初始化。
- Result: 方法在模型压缩和加速的同时，性能达到或超过教师模型，且对低质量图像表现更鲁棒。
- Conclusion: VLCD提高了医疗视觉-语言对比学习的效率和可部署性，尤其适用于资源有限的环境。


### [19] [Motion aware video generative model](https://arxiv.org/abs/2506.02244)
*Bowen Xue,Giuseppe Claudio Guarnera,Shuang Zhao,Zahra Montazeri*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理的频率域方法，用于提升扩散视频生成的物理合理性，通过分析运动频率特征并设计损失函数和增强模块，显著提高了生成视频的运动质量和物理真实性。

- Motivation: 当前扩散视频生成方法主要依赖统计学习，未显式建模运动物理特性，导致生成视频存在非物理伪影。本文旨在通过频率域方法增强物理合理性。
- Method: 提出物理运动损失函数和频率域增强模块，通过分析运动频率特征并优化生成视频的物理一致性。
- Result: 实验表明，该方法显著提升了运动质量和物理合理性，同时保持了视觉质量和语义一致性。
- Conclusion: 该方法为深度学习视频合成引入物理约束提供了原则性框架，连接了数据驱动模型与物理运动模型。


### [20] [PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained Audio-Visual Fusion and Alignment Loss](https://arxiv.org/abs/2506.02247)
*Yu Wang,Juhyung Ha,David J. Crandall*

Main category: cs.CV

TL;DR: PAIR-Net结合预训练的Whisper音频编码器和微调的AV-HuBERT视觉骨干，通过跨模态对齐损失提升主动说话者检测性能，在Ego4D ASD基准上达到76.6% mAP。

- Motivation: 解决传统视觉方法在非理想视角、运动模糊和屏幕外语音等复杂场景下的性能下降问题。
- Method: 整合部分冻结的Whisper音频编码器和微调AV-HuBERT视觉骨干，引入跨模态对齐损失以平衡模态。
- Result: 在Ego4D ASD基准上达到76.6% mAP，超越LoCoNet和STHG 8.2%和12.9%。
- Conclusion: 预训练音频先验和对齐融合方法在真实场景下的主动说话者检测中具有显著价值。


### [21] [Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction](https://arxiv.org/abs/2506.02265)
*Samuel Li,Pujith Kachana,Prajwal Chidananda,Saurabh Nair,Yasutaka Furukawa,Matthew Brown*

Main category: cs.CV

TL;DR: Rig3R是一种多视角重建模型，通过结合或推断相机设备的结构信息，显著提升了3D重建、相机姿态估计和设备结构发现的性能。

- Motivation: 现有方法将图像视为无结构集合，限制了在同步设备场景中的效果。Rig3R旨在利用或推断设备结构信息以提高性能。
- Method: Rig3R结合设备元数据（如相机ID、时间和姿态），构建设备感知的潜在空间，并联合预测点图和两种射线图（全局和设备中心）。
- Result: Rig3R在3D重建、相机姿态估计和设备发现任务中表现最优，性能提升17-45% mAA。
- Conclusion: Rig3R通过单次前向传播即可实现高性能，无需后处理或迭代优化。


### [22] [Entity Image and Mixed-Modal Image Retrieval Datasets](https://arxiv.org/abs/2506.02291)
*Cristian-Ioan Blaga,Paul Suganthan,Sahil Dua,Krishna Srinivasan,Enrique Alfonseca,Peter Dornbach,Tom Duerig,Imed Zitouni,Zhe Dong*

Main category: cs.CV

TL;DR: 论文提出了一个新颖的混合模态图像检索基准，包含两个新数据集（EI和MMIR），用于评估跨模态上下文理解能力。

- Motivation: 现有混合模态图像检索的基准不足，缺乏对视觉和文本信息深度结合的要求。
- Method: 引入两个数据集：EI（实体图像数据集）和MMIR（混合模态图像检索数据集），并设计了两种查询类型（单实体和多实体）。
- Result: 通过实验验证了基准的实用性，数据集质量通过众包标注确认。
- Conclusion: 提出的基准和数据集为混合模态检索提供了有效的训练和评估工具。


### [23] [Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation](https://arxiv.org/abs/2506.02294)
*Niclas Popp,Kevin Alexander Laube,Matthias Hein,Lukas Schott*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散的数据增强策略，用于解决知识蒸馏中协变量偏移问题，显著提升了模型在协变量偏移下的性能。

- Motivation: 在数据受限的情况下，知识蒸馏的效果受限于训练数据中的虚假特征。本文旨在解决这一问题，利用扩散模型生成具有挑战性的样本。
- Method: 提出了一种新颖的扩散数据增强策略，通过最大化教师和学生模型之间的分歧生成困难样本。
- Result: 在CelebA、SpuCo Birds和spurious ImageNet上，方法显著提升了最差组和平均组准确率，以及虚假mAUC。
- Conclusion: 该方法在协变量偏移下表现出色，优于现有扩散数据增强基线。


### [24] [QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation](https://arxiv.org/abs/2506.02295)
*Ahmed Wasfy,Omer Nacar,Abdelakreem Elkhateb,Mahmoud Reda,Omar Elshehy,Adel Ammar,Wadii Boulila*

Main category: cs.CV

TL;DR: Qari-OCR系列模型通过迭代优化，显著提升了阿拉伯语OCR的准确性，尤其在处理复杂脚本和低分辨率图像方面表现优异。

- Motivation: 阿拉伯语脚本的复杂性（如连笔、变音符号和多样字体）对OCR技术提出了持续挑战。
- Method: 基于Qwen2-VL-2B-Instruct模型，通过迭代微调专用合成数据集，优化出Qari-OCR系列模型。
- Result: QARI v0.2在开放源码中达到新SOTA，WER为0.160，CER为0.061，BLEU得分为0.737。
- Conclusion: Qari-OCR显著提升了阿拉伯语OCR的准确性和效率，并开源模型和数据集以促进研究。


### [25] [Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning](https://arxiv.org/abs/2506.02327)
*Yijun Yang,Zhao-Yang Wang,Qiuping Liu,Shuwen Sun,Kang Wang,Rama Chellappa,Zongwei Zhou,Alan Yuille,Lei Zhu,Yu-Dong Zhang,Jieneng Chen*

Main category: cs.CV

TL;DR: MeWM是一种医学世界模型，通过视觉预测疾病未来状态，结合生成模型和生存分析优化临床决策。

- Motivation: 现代医学需要有效的治疗和临床决策支持，MeWM旨在通过生成模型模拟疾病动态以辅助决策。
- Method: MeWM包括视觉语言模型（策略模型）和肿瘤生成模型（动态模型），结合逆动态模型评估治疗效果。
- Result: MeWM在模拟肿瘤状态和优化治疗方案方面表现优异，显著提升临床决策的准确性。
- Conclusion: MeWM为医学世界模型的未来应用奠定了基础，可作为临床决策的辅助工具。


### [26] [Generalized Category Discovery via Reciprocal Learning and Class-Wise Distribution Regularization](https://arxiv.org/abs/2506.02334)
*Duo Liu,Zhiquan Tan,Linglan Zhao,Zhongqiang Zhang,Xiangzhong Fang,Weiran Huang*

Main category: cs.CV

TL;DR: 论文提出了一种名为RLF的互学习框架，通过引入辅助分支和类分布正则化（CDR），解决了现有参数化方法在基础类别识别上的不足，显著提升了性能。

- Motivation: 现有参数化方法在广义类别发现（GCD）中因不可靠的自监督导致基础类别识别能力不足，需要改进。
- Method: 提出互学习框架（RLF），包含主分支和辅助分支，通过伪基础样本过滤和软标签反馈形成良性循环；引入类分布正则化（CDR）减少对基础类别的学习偏差。
- Result: 在七个GCD数据集上的实验表明，RLCD方法在所有类别上表现优异，且计算开销极小。
- Conclusion: RLF和CDR的结合有效提升了GCD任务的性能，为参数化方法提供了新思路。


### [27] [RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models](https://arxiv.org/abs/2506.02354)
*Junjie Li,Nan Zhang,Xiaoyang Qu,Kai Lu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: RATE-Nav提出了一种基于区域感知的终止增强方法，通过几何预测区域分割和探索率计算，显著提升了物体导航任务的性能。

- Motivation: 当前研究中，物体导航任务存在冗余探索和探索失败的问题，探索终止的时机是关键但未被充分研究的领域。
- Method: RATE-Nav结合了几何预测区域分割算法和基于区域的探索率估计算法，并利用视觉语言模型的视觉问答能力实现高效终止。
- Result: 在HM3D数据集上，RATE-Nav的成功率为67.8%，SPL为31.3%；在MP3D数据集上，比之前的零样本方法提升了约10%。
- Conclusion: RATE-Nav通过优化探索终止策略，显著提升了物体导航任务的性能，为未来研究提供了新思路。


### [28] [InterRVOS: Interaction-aware Referring Video Object Segmentation](https://arxiv.org/abs/2506.02356)
*Woojeong Jin,Seongchan Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: InterRVOS提出了一种新的视频对象分割任务，关注对象间的交互关系，并构建了大规模数据集InterRVOS-8K和基线模型ReVIOSa。

- Motivation: 现有方法多关注单一对象的定位，忽略了对象间的交互关系，而交互在视频理解中至关重要。
- Method: 提出了InterRVOS任务，利用互补的自然语言表达对描述交互关系，并构建数据集InterRVOS-8K和基线模型ReVIOSa。
- Result: 实验表明，ReVIOSa在复杂对象交互建模中优于现有方法。
- Conclusion: InterRVOS为交互中心视频理解研究奠定了基础，未来可进一步探索。


### [29] [RoadFormer : Local-Global Feature Fusion for Road Surface Classification in Autonomous Driving](https://arxiv.org/abs/2506.02358)
*Tianze Wang,Zhang Zhang,Chao Sun*

Main category: cs.CV

TL;DR: 提出了一种基于视觉的细粒度路面分类方法RoadFormer，通过卷积和Transformer模块融合局部与全局特征，显著提升了分类精度。

- Motivation: 路面分类对自动驾驶安全至关重要，但现有视觉方法忽视了细粒度分类（如相似纹理）。
- Method: 结合卷积和Transformer模块，提出前景-背景模块（FBM）提取细粒度特征，优化特征提取策略。
- Result: 在百万样本数据集上Top-1准确率达92.52%和96.50%，比现有方法提升5.69%-12.84%。
- Conclusion: RoadFormer在路面分类任务中表现优异，提升了自动驾驶系统的路面感知可靠性。


### [30] [Auto-Labeling Data for Object Detection](https://arxiv.org/abs/2506.02359)
*Brent A. Griffin,Manushree Gangwar,Jacob Sela,Jason J. Corso*

Main category: cs.CV

TL;DR: 本文提出了一种无需真实标注的训练目标检测模型的方法，利用预训练的视觉-语言基础模型生成伪标签，显著降低了标注成本和时间。

- Motivation: 传统目标检测标注成本高，而现有替代方案功能受限或计算成本过高。本文旨在解决这一问题。
- Method: 配置预训练的视觉-语言基础模型生成伪标签，并训练轻量级检测模型。
- Result: 实验表明该方法在多个数据集上表现接近传统标注，同时大幅降低标注成本和时间。
- Conclusion: 该方法为传统标注提供了可行的替代方案，兼具性能和效率。


### [31] [A TRPCA-Inspired Deep Unfolding Network for Hyperspectral Image Denoising via Thresholded t-SVD and Top-K Sparse Transformer](https://arxiv.org/abs/2506.02364)
*Liang Li,Jianli Zhao,Sheng Fang,Siyu Chen,Hui Sun*

Main category: cs.CV

TL;DR: 提出了一种基于张量鲁棒主成分分析（TRPCA）的深度展开网络（DU-TRPCA），通过紧密集成的低秩和稀疏模块，有效去除高光谱图像中的混合噪声。

- Motivation: 高光谱图像（HSIs）在采集和传输过程中常受复杂混合噪声影响，现有混合方法未能充分利用不同先验或模块的互补优势。
- Method: 结合低秩模块（阈值张量奇异值分解）和稀疏模块（Top-K稀疏变换器），通过深度展开网络实现紧密耦合。
- Result: 在合成和真实HSIs上实验表明，DU-TRPCA在严重混合噪声下优于现有方法，并提供可解释性和稳定去噪动态。
- Conclusion: DU-TRPCA通过紧密耦合低秩和稀疏模块，显著提升了高光谱图像去噪性能。


### [32] [Approximate Borderline Sampling using Granular-Ball for Classification Tasks](https://arxiv.org/abs/2506.02366)
*Qin Xie,Qinghua Zhang,Shuyin Xia*

Main category: cs.CV

TL;DR: 提出了一种基于粒度球（GB）的近似边界采样方法（GBABS），通过受限扩散生成GB（RD-GBG）防止重叠，并结合异构最近邻概念进行边界采样，显著提升分类任务性能。

- Motivation: 现有GB采样方法缺乏边界采样策略，且存在类边界模糊或收缩问题，影响分类效果。
- Method: 提出RD-GBG方法防止GB重叠，并基于异构最近邻概念设计GBABS方法，实现边界采样和噪声数据集质量提升。
- Result: 实验表明，GBABS在噪声数据集上表现优异，无需最优纯度阈值，优于现有GB采样和其他代表性方法。
- Conclusion: GBABS是一种通用的边界采样方法，显著提升分类任务性能，尤其在噪声数据集上表现突出。


### [33] [ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery](https://arxiv.org/abs/2506.02367)
*Jiayi Su,Dequan Jin*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经场的新型架构ViTNF，替换了ViT中的MLP头，简化了训练流程，显著提升了广义类别发现（GCD）任务的性能。

- Motivation: 现有ViT的MLP头训练成本高且未充分利用特征提取器的能力，限制了GCD任务的性能。
- Method: 提出静态神经场函数构建NF分类器，替换MLP头，并简化训练流程为预训练和元测试两阶段。
- Result: 在多个数据集上超越现有方法，新类别和所有类别的准确率分别提升19%和16%。
- Conclusion: ViTNF在GCD任务中表现出显著优势，降低了训练难度并提升了性能。


### [34] [Multi-level and Multi-modal Action Anticipation](https://arxiv.org/abs/2506.02382)
*Seulgi Kim,Ghazal Kaviani,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 论文提出了一种多模态和多层次的动作预测方法（m&m-Ant），结合视觉和文本线索，通过细粒度标签生成器和时间一致性损失函数优化性能，在多个数据集上实现了3.08%的准确率提升。

- Motivation: 动作预测任务需处理不完全信息，传统方法仅依赖视觉模态，忽视了多模态信息的潜力。受人类行为启发，作者提出结合视觉和文本线索的多模态方法。
- Method: 提出m&m-Ant方法，整合视觉和文本模态，引入细粒度标签生成器和时间一致性损失函数，优化预测性能。
- Result: 在Breakfast、50 Salads和DARai数据集上，平均预测准确率提升3.08%，达到最优效果。
- Conclusion: 多模态和层次化建模在动作预测中具有潜力，为未来研究设定了新基准。


### [35] [RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection](https://arxiv.org/abs/2506.02393)
*Yongxian Liu,Boyang Li,Ting Liu,Zaiping Lin,Wei An*

Main category: cs.CV

TL;DR: 提出了一种基于循环可重用卷积注意力网络（RRCA-Net）的红外小目标检测方法，通过高效的特征提取和融合模块，实现了高性能检测。

- Motivation: 红外小目标检测因目标小、暗、形状多变而具有挑战性，现有CNN方法虽有效但计算量大。
- Method: RRCA-Net结合了可重用卷积块（RuCB）和双交互注意力聚合模块（DIAAM），通过循环迭代优化特征，并设计了目标特性启发的损失函数（DpT-k loss）。
- Result: 在多个基准数据集上表现优异，参数少且可作为插件提升其他方法的性能。
- Conclusion: RRCA-Net在红外小目标检测中实现了高效和性能的平衡，具有广泛应用潜力。


### [36] [The Devil is in the Darkness: Diffusion-Based Nighttime Dehazing Anchored in Brightness Perception](https://arxiv.org/abs/2506.02395)
*Xiaofeng Cong,Yu-Xin Zhang,Haoran Wei,Yeying Jin,Junming Hou,Jie Gui,Jing Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: DiffND框架通过数据合成和亮度感知优化，解决了夜间图像去雾中亮度映射不一致的问题。

- Motivation: 现有方法在夜间图像去雾中忽略了日间亮度知识，导致亮度映射不真实。
- Method: 提出DiffND框架，包括亮度一致的数据合成流程和基于扩散模型的亮度感知优化方法。
- Result: 实验验证了数据集的有效性和模型在去雾与亮度映射上的优越性能。
- Conclusion: DiffND在夜间去雾和亮度重建方面表现出色。


### [37] [Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather](https://arxiv.org/abs/2506.02396)
*Longyu Yang,Ping Hu,Shangbo Yuan,Lu Zhang,Jun Liu,Hengtao Shen,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: 提出了一种几何-反射协作（GRC）框架，通过分离几何和反射特征提取，提升LiDAR语义分割在恶劣天气下的鲁棒性。

- Motivation: 现有LiDAR语义分割模型在恶劣天气下准确性下降，且少有研究关注点云几何结构和反射强度的异质域偏移问题。
- Method: 采用双分支架构独立处理几何和反射特征，并通过多级特征协作模块抑制冗余信息。
- Result: 在挑战性基准测试中表现优异，超越现有方法并达到新SOTA。
- Conclusion: GRC框架无需复杂模拟或增强，即可有效提取场景本质信息并抑制干扰，提升模型鲁棒性和泛化能力。


### [38] [Modelship Attribution: Tracing Multi-Stage Manipulations Across Generative Models](https://arxiv.org/abs/2506.02405)
*Zhiya Tan,Xin Zhang,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种新方法来解决复杂迭代图像篡改的溯源问题，定义了“模型归属”任务，并开发了MAT框架。

- Motivation: 随着生成技术的普及，图像篡改日益复杂，现有方法难以应对多阶段篡改，需系统性解决方案。
- Method: 利用三种生成模型模拟多阶段篡改，构建数据集，提出MAT框架识别模型编辑模式。
- Result: MAT在多阶段篡改溯源中表现优异，实验验证其有效性。
- Conclusion: MAT为复杂图像篡改溯源提供了高效解决方案。


### [39] [Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology](https://arxiv.org/abs/2506.02408)
*Wenhao Tang,Rong Qin,Heng Fang,Fengtao Zhou,Hao Chen,Xiang Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ABMILX的新型多实例学习方法，通过全局相关性注意力优化和多头机制解决了稀疏注意力MIL的优化问题，并在计算病理学中实现了端到端学习的性能提升。

- Motivation: 传统计算病理学中，预训练编码器与MIL聚合器的分离优化导致性能受限，端到端学习虽直观但面临计算和性能挑战。本文旨在解决这些问题。
- Method: 提出ABMILX方法，结合全局相关性注意力优化和多头机制，并采用高效的多尺度随机补丁采样策略，实现端到端训练。
- Result: ABMILX在多个基准测试中超越现有两阶段方法，且计算效率高（<10 RTX3090小时）。
- Conclusion: 研究展示了端到端学习在计算病理学中的潜力，呼吁更多关注该领域。


### [40] [Guiding Registration with Emergent Similarity from Pre-Trained Diffusion Models](https://arxiv.org/abs/2506.02419)
*Nurislam Tursynbek,Hastings Greer,Basar Demir,Marc Niethammer*

Main category: cs.CV

TL;DR: 扩散模型用于医学图像配准，通过语义特征匹配提升性能。

- Motivation: 传统基于强度的相似性度量在医学图像配准中表现不佳，尤其是在解剖结构不一致时。扩散模型的特征提取能力可以解决这一问题。
- Method: 利用预训练的扩散模型提取特征，作为相似性度量指导可变形图像配准网络。
- Result: 在2D多模态（DXA到X射线）和3D单模态（脑提取与非脑提取MRI）配准任务中表现优越。
- Conclusion: 扩散模型的特征提取能力能够有效提升医学图像配准的准确性和鲁棒性。


### [41] [Empowering Functional Neuroimaging: A Pre-trained Generative Framework for Unified Representation of Neural Signals](https://arxiv.org/abs/2506.02433)
*Weiheng Yao,Xuhang Chen,Shuqiang Wang*

Main category: cs.CV

TL;DR: 提出一种基于生成AI的多模态功能神经影像统一表示框架，解决数据获取成本高和公平性问题。

- Motivation: 多模态功能神经影像获取成本高且可行性受限，同时特定群体数据不足影响BCI解码模型的公平性。
- Method: 通过生成AI将多模态功能神经影像映射到统一表示空间，生成受限模态和不足群体的数据。
- Result: 实验表明框架能生成与真实脑活动一致的数据，提升下游任务性能，并增强模型公平性。
- Conclusion: 该框架为降低多模态功能神经影像获取成本和提升BCI模型公平性提供了新范式。


### [42] [Video-Level Language-Driven Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2506.02439)
*Shuang Li,Jiaxu Leng,Changjiang Kuang,Mingpi Tan,Xinbo Gao*

Main category: cs.CV

TL;DR: 论文提出了一种基于视频的可见光-红外行人重识别（VVI-ReID）框架VLD，通过生成模态共享的语言提示和时空信息建模，解决了跨模态特征对齐的挑战。

- Motivation: 利用语言描述作为跨模态一致性表征，解决可见光和红外模态间的差异问题。
- Method: 提出VLD框架，包含两个核心模块：IMLP（生成模态共享文本提示）和STP（时空信息建模）。IMLP通过联合微调视觉编码器和提示学习器对齐模态特征；STP通过STH和STA子模块增强时空信息。
- Result: 在VVI-ReID基准测试中达到最优性能。
- Conclusion: VLD框架通过语言提示和时空信息建模有效解决了跨模态行人重识别问题。


### [43] [SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios](https://arxiv.org/abs/2506.02444)
*Lingwei Dang,Ruizhi Shao,Hongwen Zhang,Wei Min,Yebin Liu,Qingyao Wu*

Main category: cs.CV

TL;DR: 提出了一种结合视觉先验和动态约束的同步扩散框架，用于同时生成手-物体交互（HOI）视频和运动，解决了现有方法依赖预定义模型和牺牲物理合理性的问题。

- Motivation: 现有3D HOI生成方法依赖预定义3D对象模型和实验室捕获的运动数据，且视频生成方法常牺牲物理合理性。本文旨在结合视觉和动态约束，提升生成效果。
- Method: 提出同步扩散过程，结合视觉先验和动态约束；采用三模态自适应调制和3D全注意力机制；引入视觉感知的3D交互扩散模型，形成闭环反馈。
- Result: 实验表明，该方法在生成高保真、动态合理的HOI序列上优于现有方法，并在未见过的真实场景中表现出良好的泛化能力。
- Conclusion: 该方法无需依赖预定义对象模型或显式姿态指导，显著提升了视频与运动的一致性，具有广泛的应用潜力。


### [44] [VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos](https://arxiv.org/abs/2506.02448)
*Baoyu Liang,Qile Su,Shoutai Zhu,Yuchen Liang,Chao Tong*

Main category: cs.CV

TL;DR: 论文提出视频事件理解任务，并发布VidEvent数据集，包含23,000多个标注事件，支持事件脚本提取与预测。

- Motivation: 视频事件的复杂结构和动态演变对AI理解构成挑战，需新方法支持。
- Method: 创建VidEvent数据集，标注详细事件结构与逻辑关系，并提供基线模型。
- Result: 数据集高质量，基线模型为未来研究提供基准，促进算法创新。
- Conclusion: VidEvent推动视频事件理解研究，数据集公开可用。


### [45] [ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model](https://arxiv.org/abs/2506.02452)
*Wenshuo Chen,Kuimou Yu,Haozhe Jia,Kaishen Yuan,Bowen Tian,Songning Lai,Hongru Xiao,Erhang Zhang,Lei Wang,Yutao Yue*

Main category: cs.CV

TL;DR: ANT提出了一种自适应神经时间感知架构，通过动态调整语义粒度和条件-无条件比率，显著提升了文本到动作生成的性能。

- Motivation: 现有扩散模型在文本到动作生成中忽略了时间频率需求，导致早期去噪需要结构语义而后期需要局部细节，与生物形态发生过程类似。
- Method: ANT包括三个模块：语义时间自适应（STA）模块、动态无分类器引导调度（DCFG）和时间语义重加权，分别处理频率分区、条件调整和文本对齐。
- Result: 实验表明ANT能显著提升模型性能，并在StableMoFusion上实现最先进的语义对齐。
- Conclusion: ANT通过模拟生物形态发生的调控机制，有效解决了文本到动作生成中的时间频率需求问题。


### [46] [PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation](https://arxiv.org/abs/2506.02453)
*Kunyu Wang,Xueyang Fu,Yunfei Bao,Chengjie Ge,Chengzhi Cao,Wei Zhai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: PAID方法通过保留预训练权重的成对角度结构，提出了一种简单有效的持续测试时间适应方法，显著提升了性能。

- Motivation: 现有方法忽视了预训练权重中未充分利用的域不变先验信息，尤其是几何属性中的成对角度结构。
- Method: PAID方法将权重分解为幅度和方向，并通过Householder反射引入可学习的正交矩阵，仅更新幅度和正交矩阵。
- Result: 在四个广泛使用的CTTA基准测试中，PAID一致优于现有SOTA方法。
- Conclusion: 保留成对角度结构是CTTA中简单而有效的原则。


### [47] [ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment](https://arxiv.org/abs/2506.02459)
*Martin JJ. Bucher,Iro Armeni*

Main category: cs.CV

TL;DR: ReSpace是一个基于自回归语言模型的生成框架，用于文本驱动的3D室内场景合成与编辑，解决了现有方法在语义简化、编辑限制和空间推理方面的不足。

- Motivation: 当前3D室内场景合成方法存在语义简化、编辑限制和空间推理不足的问题，而基于LLM的方法虽然语义丰富但不支持编辑或布局有限。
- Method: ReSpace采用双阶段训练方法，结合监督微调和偏好对齐，利用结构化场景表示和显式房间边界，将场景编辑任务转化为下一个标记预测。
- Result: 实验结果显示，ReSpace在对象添加任务上超越现有技术，同时在完整场景合成上保持竞争力。
- Conclusion: ReSpace通过结合语言模型和结构化表示，实现了更灵活、语义丰富的3D场景合成与编辑。


### [48] [Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning](https://arxiv.org/abs/2506.02462)
*Kunyu Wang,Xueyang Fu,Xin Lu,Chengjie Ge,Chengzhi Cao,Wei Zhai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了一种基于剪枝的高效持续测试时自适应目标检测方法，通过敏感度引导的通道剪枝策略减少计算开销，同时保持性能。

- Motivation: 现有方法注重有效性但忽略计算效率，而某些源特征对目标域性能有害，需选择性剪枝。
- Method: 采用敏感度引导的通道剪枝策略，结合加权稀疏正则化和随机通道重新激活机制。
- Result: 在三个基准测试中表现优异，计算开销减少12%。
- Conclusion: 该方法在保持性能的同时显著提升计算效率。


### [49] [HRTR: A Single-stage Transformer for Fine-grained Sub-second Action Segmentation in Stroke Rehabilitation](https://arxiv.org/abs/2506.02472)
*Halil Ismail Helvaci,Justin Philip Huber,Jihye Bae,Sen-ching Samson Cheung*

Main category: cs.CV

TL;DR: 提出了一种名为HRTR的单阶段Transformer模型，用于高分辨率、亚秒级动作的检测与分类，在卒中康复和通用数据集上表现优异。

- Motivation: 卒中康复需要精确跟踪患者动作以监测进展，但现有方法难以实现细粒度和亚秒级动作检测。
- Method: 采用High Resolution Temporal Transformer (HRTR)，单阶段完成动作的时间定位与分类，无需多阶段方法或后处理。
- Result: HRTR在StrokeRehab Video、StrokeRehab IMU和50Salads数据集上分别取得70.1、69.4和88.4的Edit Score，优于现有技术。
- Conclusion: HRTR是一种高效的单阶段方法，能够精准检测细粒度和亚秒级动作，适用于卒中康复及其他领域。


### [50] [Generative Perception of Shape and Material from Differential Motion](https://arxiv.org/abs/2506.02473)
*Xinran Nicole Han,Ko Nishino,Todd Zickler*

Main category: cs.CV

TL;DR: 提出一种基于条件去噪扩散模型的方法，通过短视频中的物体运动生成形状和材质图，解决单视角感知的模糊性问题。

- Motivation: 人类通过微调视角或旋转物体解决形状和材质感知的模糊性，启发研究者设计一种类似的多视角生成模型。
- Method: 使用参数高效的架构直接在像素空间训练，通过合成物体运动视频监督形状和材质生成。
- Result: 模型在静态观察时生成多样化的多模态预测，动态观察时快速收敛到更准确的解释，并在真实物体上表现良好。
- Conclusion: 通过连续运动观察，该方法为物理嵌入系统的视觉推理提供了生成感知的新思路。


### [51] [Towards Better De-raining Generalization via Rainy Characteristics Memorization and Replay](https://arxiv.org/abs/2506.02477)
*Kunyu Wang,Xueyang Fu,Chengzhi Cao,Chengjie Ge,Wei Zhai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过逐步扩展数据集提升图像去雨网络的适应能力，模仿人脑学习机制，结合GAN和知识蒸馏，显著提升性能。

- Motivation: 现有方法因数据集有限，在真实多变雨天场景中表现不足。
- Method: 结合GAN捕获新数据特征，通过知识蒸馏和重放数据训练网络，模仿海马体和大脑皮层的协同学习机制。
- Result: 在三个基准网络上验证，框架能持续积累知识，并在新场景中超越现有方法。
- Conclusion: 该框架有效提升去雨网络的适应性和性能，适用于多变真实场景。


### [52] [Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models](https://arxiv.org/abs/2506.02488)
*Hongtao Huang,Xiaojun Chang,Lina Yao*

Main category: cs.CV

TL;DR: Flexiffusion是一种无需训练的NAS框架，通过动态结合三种计算步骤类型（完整、部分、跳过）来优化扩散模型的生成速度和架构，显著减少计算成本，同时保持生成质量。

- Motivation: 扩散模型（DMs）因多步迭代推理导致计算成本高，现有NAS方法受限于重训练需求、搜索复杂性和慢速评估。Flexiffusion旨在解决这些问题。
- Method: Flexiffusion将生成过程分解为等长段，动态结合完整、部分和跳过计算步骤，并引入轻量级评估指标rFID。
- Result: Flexiffusion在多个模型（如Stable Diffusion）上实现至少2倍加速，FID退化低于5%，最高达5.1倍速度提升。
- Conclusion: Flexiffusion为高效搜索高质量扩散模型提供了新范式，显著提升了速度和资源效率。


### [53] [Co-Evidential Fusion with Information Volume for Medical Image Segmentation](https://arxiv.org/abs/2506.02492)
*Yuanpeng He,Lijian Li,Tianxiang Zhan,Chi-Man Pun,Wenpin Jiao,Zhi Jin*

Main category: cs.CV

TL;DR: 论文提出了一种改进的半监督图像分割方法，通过引入新的不确定性度量策略和信息量评估，提升了模型性能。

- Motivation: 现有半监督图像分割方法未能充分利用体素级不确定性信息，限制了学习效果。
- Method: 1. 提出基于广义证据深度学习的pignistic共证据融合策略；2. 引入信息量评估（IVUM），设计两种证据学习方案。
- Result: 在四个数据集上验证了方法的竞争力。
- Conclusion: 新方法通过更精确的不确定性度量和信息量评估，显著提升了半监督图像分割的性能。


### [54] [Towards In-the-wild 3D Plane Reconstruction from a Single Image](https://arxiv.org/abs/2506.02493)
*Jiachen Liu,Rui Yu,Sili Chen,Sharon X. Huang,Hengkai Guo*

Main category: cs.CV

TL;DR: ZeroPlane是一个基于Transformer的框架，用于从单张图像中实现零样本3D平面检测和重建，支持跨域泛化。

- Motivation: 解决现有方法在单一数据集上训练导致的泛化能力不足问题，提出跨域3D平面重建的挑战。
- Method: 通过解耦平面法线和偏移表示，采用分类-回归范式，结合先进骨干网络和像素几何增强模块。
- Result: 在多个零样本评估数据集上显著优于现有方法，尤其在野外数据上表现突出。
- Conclusion: ZeroPlane在3D平面重建的准确性和泛化能力上取得了显著进展。


### [55] [LumosFlow: Motion-Guided Long Video Generation](https://arxiv.org/abs/2506.02497)
*Jiahao Chen,Hangjie Yuan,Yichen Qian,Jingyun Liang,Jiazheng Xing,Pengwei Liu,Weihua Chen,Fan Wang,Bing Su*

Main category: cs.CV

TL;DR: 论文提出LumosFlow框架，通过显式运动指导改进长视频生成，解决了传统方法中的时间重复和不自然过渡问题。

- Motivation: 长视频生成在娱乐和模拟等领域应用广泛，但现有方法在时间连贯性和视觉吸引力方面仍面临挑战。
- Method: 结合LMTV-DM生成关键帧，LOF-DM合成光流，MotionControlNet细化中间帧，实现15倍插值。
- Result: 实验表明，该方法能生成运动一致、外观连贯的长视频。
- Conclusion: LumosFlow通过显式运动指导显著提升了长视频生成的质量和连贯性。


### [56] [RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers](https://arxiv.org/abs/2506.02528)
*Yan Gong,Yiren Song,Yicheng Li,Chenglin Li,Yin Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉提示的图像编辑新范式RelationAdapter，通过源-目标图像对提取和转移内容感知编辑意图，解决了现有方法在非刚性变换上的局限性。

- Motivation: 受大语言模型上下文学习机制的启发，旨在解决现有单参考方法在非刚性变换上的不足。
- Method: 提出RelationAdapter轻量模块，结合Diffusion Transformer模型，从少量示例中捕捉和应用视觉变换。
- Result: 在Relation252K数据集上实验表明，RelationAdapter显著提升了模型理解和转移编辑意图的能力，生成质量和编辑性能均有显著提升。
- Conclusion: RelationAdapter为视觉提示驱动的图像编辑提供了高效且通用的解决方案。


### [57] [Enhancing Monocular Height Estimation via Weak Supervision from Imperfect Labels](https://arxiv.org/abs/2506.02534)
*Sining Chen,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种利用不完美标签数据训练单目高度估计网络的方法，通过弱监督和精心设计的损失函数，显著提升了模型在不同数据集上的泛化性能。

- Motivation: 由于高质量标签数据稀缺且主要集中在发达地区，现有模型泛化能力不足，限制了单目高度估计的大规模应用。论文首次尝试利用不完美标签数据解决这一问题。
- Method: 提出了一种基于集成学习的流程，适用于任何单目高度估计网络。通过平衡软损失和序数约束，设计了架构和损失函数以利用不完美标签中的信息。
- Result: 在DFC23和GBH数据集上的实验表明，该方法显著优于基线，平均均方根误差分别降低了22.94%和18.62%。
- Conclusion: 该方法通过弱监督有效利用了不完美标签数据，提升了模型的泛化能力和性能，为大规模应用提供了可能。


### [58] [MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection](https://arxiv.org/abs/2506.02535)
*Juntong Li,Lingwei Dang,Yukun Su,Yun Hao,Qingxin Xiao,Yongwei Nie,Qingyao Wu*

Main category: cs.CV

TL;DR: 提出了一种新的视频异常检测框架，通过稀疏特征过滤模块（SFFM）和混合专家（MoE）架构抑制过度泛化，并结合视觉语言模型（VLM）提升语义理解能力。

- Motivation: 现有基于重建或预测的视频异常检测方法存在过度泛化和忽视高级语义的问题。
- Method: 引入SFFM动态过滤异常信息，设计MoE架构提取多样化特征，集成VLM生成文本描述以联合建模语义、外观和运动线索。
- Result: 在多个公开数据集上验证了多模态联合建模框架和稀疏特征过滤范式的有效性。
- Conclusion: 提出的框架显著提升了视频异常检测的性能，解决了现有方法的局限性。


### [59] [VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning](https://arxiv.org/abs/2506.02537)
*Hao Yan,Handong Zheng,Hao Wang,Liang Yin,Xingchen Liu,Zhenbiao Cao,Xinxing Su,Zihao Chen,Jihao Wu,Minghui Liao,Chao Weng,Wei Chen,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出VisuRiddles基准和PRS框架，以解决多模态大语言模型在抽象视觉推理中的瓶颈问题，并通过实验验证了其有效性。

- Motivation: 当前多模态大语言模型在抽象视觉推理（AVR）中存在感知抽象图形的局限性，需要改进。
- Method: 1. 提出VisuRiddles基准，评估模型在五个核心维度和两类高级推理任务中的能力；2. 开发PRS框架，自动生成带有细粒度感知描述的谜题，用于训练和监督中间推理阶段。
- Result: 实验表明细粒度视觉感知是主要瓶颈，PRS框架显著提升了模型在AVR任务中的性能。
- Conclusion: PRS框架和VisuRiddles基准有效解决了AVR中的瓶颈问题，并提升了模型的训练效率和可解释性。


### [60] [Probabilistic Online Event Downsampling](https://arxiv.org/abs/2506.02547)
*Andreu Girbau-Xalabarder,Jun Nagata,Shinichi Sumiyoshi*

Main category: cs.CV

TL;DR: 论文提出了一种名为POLED的概率框架，通过事件重要性概率密度函数（ePDF）动态评估事件重要性，支持在线自适应事件下采样，并引入零样本下采样技术，确保下采样后的事件仍适用于原始任务。

- Motivation: 事件相机的高时间分辨率带来高带宽和计算需求，现有固定启发式或阈值策略的下采样方法适应性不足。
- Method: 提出POLED框架，利用可自定义的ePDF在线估计事件重要性，实现场景自适应下采样，并设计保留轮廓的ePDF。
- Result: 在四个数据集和任务（分类、插值、法线估计、检测）中验证，智能采样在事件预算限制下保持性能。
- Conclusion: POLED框架通过自适应事件下采样，显著提升了事件相机在资源受限场景下的实用性。


### [61] [Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025](https://arxiv.org/abs/2506.02550)
*Qiaohui Chu,Haoyu Zhang,Yisen Feng,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出了一种新颖的三阶段框架，用于Ego4D长期动作预测任务，结合视觉编码器、Transformer和大型语言模型，在CVPR 2025挑战赛中取得第一名。

- Motivation: 受基础模型最新进展的启发，开发一个能够高效完成长期动作预测任务的框架。
- Method: 三阶段框架：1) 视觉特征提取；2) 使用Transformer预测动词和名词，并结合共现矩阵提高准确性；3) 将预测结果输入微调的大型语言模型生成未来动作序列。
- Result: 在CVPR 2025挑战赛中取得第一名，建立了长期动作预测的新基准。
- Conclusion: 该框架通过结合多阶段模型和共现矩阵，显著提升了长期动作预测的准确性，具有广泛的应用潜力。


### [62] [SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence](https://arxiv.org/abs/2506.02555)
*Zhitao Zeng,Zhu Zhuo,Xiaojun Jia,Erli Zhang,Junde Wu,Jiaan Zhang,Yuxuan Wang,Chang Han Low,Jian Jiang,Zilong Zheng,Xiaochun Cao,Yutong Ban,Qi Dou,Yang Liu,Yueming Jin*

Main category: cs.CV

TL;DR: SurgVLM是一种针对手术智能的大型视觉语言基础模型，通过构建大规模多模态手术数据库SurgVLM-DB和基准测试SurgVLM-Bench，解决了现有通用模型在手术领域的不足。

- Motivation: 手术智能面临独特的挑战，如手术视觉感知、时间分析和推理，现有通用视觉语言模型因缺乏领域特定监督和大规模高质量数据而无法满足需求。
- Method: 构建SurgVLM-DB数据库，统一23个公共数据集，标准化标签并进行分层视觉语言对齐；基于Qwen2.5-VL开发SurgVLM，并进行指令调优以适应10+手术任务。
- Result: SurgVLM在SurgVLM-Bench上表现优异，与14种主流商业VLM（如GPT-4o、Gemini 2.0 Flash）相比具有竞争力。
- Conclusion: SurgVLM填补了手术智能领域的空白，为多任务手术应用提供了通用解决方案。


### [63] [Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models](https://arxiv.org/abs/2506.02557)
*Shizhan Gong,Yankai Jiang,Qi Dou,Farzan Farnia*

Main category: cs.CV

TL;DR: 提出一种基于核的方法，将CLIP的视觉表示与DINOv2对齐，提升感知能力并保持与文本嵌入的兼容性。

- Motivation: CLIP在细粒度感知上表现不足，影响下游多模态大语言模型的性能，而DINOv2在细节捕捉上表现优异。
- Method: 使用核方法对齐CLIP和DINOv2的视觉表示，设计高效的随机优化目标。
- Result: 对齐后的视觉编码器在零样本识别、细粒度空间推理和定位任务中表现显著提升，下游MLLM性能也增强。
- Conclusion: 该方法有效结合了CLIP和DINOv2的优势，提升了多模态模型的感知能力。


### [64] [DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image Editing](https://arxiv.org/abs/2506.02560)
*Zixiang Li,Haoyu Wang,Wei Wang,Chuangchuang Tan,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: Dual-Conditional Inversion (DCI) 是一种新的框架，通过联合条件优化解决扩散模型中重建精度与编辑灵活性之间的权衡问题。

- Motivation: 现有扩散模型的反演方法在重建精度和编辑灵活性之间存在固有权衡，难以同时保持语义对齐和结构一致性。
- Method: DCI 通过双条件固定点优化问题，联合源提示和参考图像指导反演过程，最小化潜在噪声差距和重建误差。
- Result: DCI 在多个编辑任务中达到最先进性能，显著提升重建质量和编辑精度，并在重建任务中表现出鲁棒性和泛化性。
- Conclusion: DCI 为反演过程提供了新的理解，并在实际应用中表现出优越性能。


### [65] [Contrast & Compress: Learning Lightweight Embeddings for Short Trajectories](https://arxiv.org/abs/2506.02571)
*Abhishek Vivekanandan,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer编码器和对比三元组损失的短轨迹嵌入学习框架，优于FFT基线，并在Argoverse 2数据集上验证了其性能。

- Motivation: 现有方法依赖计算密集型启发式或缺乏可解释性的潜在锚表示，需要更高效且可控的轨迹相似性检索方法。
- Method: 使用Transformer编码器和对比三元组损失学习固定维度嵌入，分析Cosine和FFT相似性度量。
- Result: Cosine相似性目标在语义和方向属性上表现更优，低维嵌入（如16维）在性能和计算开销间取得平衡。
- Conclusion: 该框架为轨迹数据提供了紧凑、语义明确且高效的表示，适用于实时系统，推动了透明可控的运动预测。


### [66] [BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations](https://arxiv.org/abs/2506.02587)
*Weiduo Yuan,Jerry Li,Justin Yue,Divyank Shah,Konstantinos Karydis,Hang Qiu*

Main category: cs.CV

TL;DR: BEVCALIB利用鸟瞰图（BEV）特征从原始数据中实现LiDAR与相机的校准，显著提升了校准精度和效率。

- Motivation: 传统校准方法需在受控环境中大量采集数据且无法补偿运动中的变换变化，限制了实际应用。
- Method: 分别提取相机和LiDAR的BEV特征，融合至共享BEV空间，并引入特征选择器优化变换解码器。
- Result: 在KITTI和NuScenes数据集上，BEVCALIB在平移和旋转误差上分别平均提升47.08%、82.32%和78.17%、68.29%。
- Conclusion: BEVCALIB开创了基于BEV特征的校准方法，显著优于现有基线，代码已开源。


### [67] [Hyperspectral Image Generation with Unmixing Guided Diffusion Model](https://arxiv.org/abs/2506.02601)
*Shiyu Shen,Bin Pan,Ziye Zhang,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出了一种基于高光谱解混的新型扩散模型，通过解混自编码器和丰度扩散模块，降低了计算复杂度并保持了高保真度，同时生成了物理一致的高光谱图像。

- Motivation: 现有生成模型依赖条件生成方案，限制了生成图像的多样性，且高光谱数据的高维度和物理约束对扩散模型提出了挑战。
- Method: 模型包含解混自编码器模块和丰度扩散模块，前者将生成任务转移到低维丰度空间，后者确保生成样本满足非负性和一致性约束。
- Result: 实验结果表明，模型能生成高质量且多样化的高光谱图像，并通过传统指标和提出的新指标验证了其有效性。
- Conclusion: 该模型在高光谱数据生成方面取得了进展，解决了高维度和物理约束的挑战。


### [68] [Application of convolutional neural networks in image super-resolution](https://arxiv.org/abs/2506.02604)
*Tian Chunwei,Song Mingjian,Zuo Wangmeng,Du Bo,Zhang Yanning,Zhang Shichao*

Main category: cs.CV

TL;DR: 本文总结了基于卷积神经网络（CNN）的图像超分辨率方法，分析了不同插值和模块的差异与关系，并通过实验比较性能，最后指出了潜在研究点和不足。

- Motivation: 由于不同深度学习方法在图像超分辨率中存在显著差异，且缺乏相关文献总结，因此本文旨在填补这一空白，为设备负载和执行速度提供参考。
- Method: 介绍了CNN在图像超分辨率中的原理，并分析了基于双三次插值、最近邻插值、双线性插值、转置卷积、子像素层和元上采样等方法的差异与关系，通过实验比较性能。
- Result: 实验比较了不同CNN插值和模块的性能，总结了各自的优缺点。
- Conclusion: 本文总结了现有方法的不足和潜在研究方向，有助于推动CNN在图像超分辨率中的发展。


### [69] [One-Step Diffusion-based Real-World Image Super-Resolution with Visual Perception Distillation](https://arxiv.org/abs/2506.02605)
*Xue Wu,Jingwei Xin,Zhijun Tu,Jie Hu,Jie Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: VPD-SR是一种针对超分辨率任务设计的视觉感知扩散蒸馏框架，通过显式语义感知监督和高频感知损失，实现高效的一步超分辨率重建。

- Motivation: 现有的多步扩散超分辨率方法在生成图像的语义对齐和感知质量上表现不佳，VPD-SR旨在解决这些问题。
- Method: VPD-SR包含显式语义感知监督（ESS）和高频感知损失（HFP），并结合对抗训练提升生成内容的真实性。
- Result: 实验表明，VPD-SR在合成和真实数据集上优于现有方法，仅需一步采样即可达到优异性能。
- Conclusion: VPD-SR通过语义和高频感知优化，显著提升了超分辨率任务的感知质量和效率。


### [70] [High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset](https://arxiv.org/abs/2506.02614)
*Guohang Zhuang,Weixi Song,Jinyang Huang,Chenwei Yang,Yan Lu*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的空间碎片追踪网络（SDT-Net），用于高精度追踪空间碎片，并创建了一个大规模数据集SDTD。实验验证了模型的有效性和数据集的挑战性。

- Motivation: 随着太空探索的快速发展，空间碎片的潜在威胁日益突出，传统信号处理方法难以应对复杂背景和高密度碎片，需要更高效的追踪方法。
- Method: 提出SDT-Net，通过深度学习有效表征碎片特征，提升端到端模型的学习效率和稳定性。同时，采用观测模拟方案生成大规模数据集SDTD。
- Result: 在SDTD数据集上进行了广泛实验，验证了模型的有效性。在真实南极站数据上测试，MOTA得分为70.6%，展示了模型的强迁移能力。
- Conclusion: SDT-Net和SDTD为空间碎片追踪提供了高效解决方案，模型在真实场景中表现优异，代码和数据集将公开。


### [71] [Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models](https://arxiv.org/abs/2506.02615)
*Safaa Abdullahi Moallim Mohamud,Minjin Baek,Dong Seog Han*

Main category: cs.CV

TL;DR: 提出了一种分层问答方法用于自动驾驶场景理解，平衡成本效率与详细视觉解释，通过定制数据集微调视觉语言模型，动态跳过问题以减少计算开销，性能接近GPT-4o但推理时间更低。

- Motivation: 解决自动驾驶场景理解中成本效率与详细视觉解释的平衡问题，提升实时性能。
- Method: 使用定制数据集微调紧凑视觉语言模型，采用分层问答策略动态分解任务并跳过无关问题。
- Result: 在定制数据集上表现接近GPT-4o，推理时间显著降低，实时部署效果良好。
- Conclusion: 该方法在保持高效推理的同时，能准确捕捉关键驾驶元素，适用于实时自动驾驶场景理解。


### [72] [Synthetic Iris Image Databases and Identity Leakage: Risks and Mitigation Strategies](https://arxiv.org/abs/2506.02626)
*Ada Sawilska,Mateusz Trokielewicz*

Main category: cs.CV

TL;DR: 本文综述了虹膜图像合成方法，旨在解决从活体个体获取大规模多样化生物特征数据集的难题，并探讨了不同方法的潜力与风险。

- Motivation: 解决生物特征数据收集的隐私和多样性问题，推动生物特征识别技术的发展。
- Method: 综述了传统图像处理技术、GAN、VAE和扩散模型等虹膜图像合成方法。
- Result: 讨论了各方法在虹膜图像生成中的潜力与保真度，并提供了预测示例。
- Conclusion: 需关注训练集中个体生物特征泄露的风险，并提出防范策略，以确保合成数据能替代真实数据集。


### [73] [ControlMambaIR: Conditional Controls with State-Space Model for Image Restoration](https://arxiv.org/abs/2506.02633)
*Cheng Yang,Lijing Liang,Zhixun Su*

Main category: cs.CV

TL;DR: ControlMambaIR是一种新型图像恢复方法，结合Mamba网络架构和扩散模型，提升图像生成过程的控制和优化。

- Motivation: 解决图像去雨、去模糊和去噪任务中的感知挑战。
- Method: 集成Mamba网络与扩散模型，通过条件网络实现精细化控制。
- Result: 在多个基准数据集上表现优异，超越现有方法，尤其在感知质量指标（LPIPS、FID）上表现突出。
- Conclusion: ControlMambaIR在图像恢复任务中具有灵活性和高效性，Mamba架构特别适合作为扩散模型的条件控制网络。


### [74] [Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language Models with AdaptNet](https://arxiv.org/abs/2506.02671)
*Xiao Chen,Jiazhen Huang,Qinting Jiang,Fanding Huang,Xianghua Fu,Jingyan Jiang,Zhi Wang*

Main category: cs.CV

TL;DR: SAIL是一种基于适配器的高效测试时自适应框架，通过轻量级AdaptNet和梯度感知重置策略，显著降低计算成本并提升性能。

- Motivation: 解决现有测试时自适应方法计算成本高、扩展性差的问题。
- Method: 使用轻量级AdaptNet与预训练VLM协作，通过置信度插值权重生成预测，并采用梯度感知重置策略避免灾难性遗忘。
- Result: 在多个基准测试中表现优异，计算成本低。
- Conclusion: SAIL高效、可扩展，适合实际部署。


### [75] [Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2506.02677)
*Jintao Tong,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 论文提出了一种解决跨域少样本分割（CD-FSS）中特征纠缠问题的方法，通过分解ViT结构并学习权重分配，显著提升了性能。

- Motivation: 当前CD-FSS方法在距离计算中存在特征纠缠问题，限制了知识迁移效果。
- Method: 分解ViT结构，分析纠缠问题，并提出通过学习权重分配来解耦特征。
- Result: 实验表明，新方法在1-shot和5-shot设置下平均准确率分别提升了1.92%和1.88%。
- Conclusion: 通过解耦ViT特征并重新组合，有效解决了CD-FSS中的纠缠问题，提升了模型性能。


### [76] [Solving Inverse Problems with FLAIR](https://arxiv.org/abs/2506.02680)
*Julius Erbach,Dominik Narnhofer,Andreas Dombos,Bernt Schiele,Jan Eric Lenssen,Konrad Schindler*

Main category: cs.CV

TL;DR: FLAIR是一种无需训练的变分框架，利用基于流的生成模型作为逆问题的先验，通过变分目标和轨迹调整解决非线性映射、数据似然难解和罕见模式恢复问题，显著优于现有方法。

- Motivation: 基于流的生成模型（如Stable Diffusion 3）在图像生成中表现出色，但作为逆问题的先验时效果不佳，主要由于非线性映射、数据似然难解和罕见模式恢复困难。
- Method: 提出FLAIR框架，结合变分目标和确定性轨迹调整，分离数据保真度和正则化优化，并引入时间依赖的校准方案。
- Result: 在标准成像基准测试中，FLAIR在重建质量和样本多样性上优于现有基于扩散和流的方法。
- Conclusion: FLAIR通过创新的变分框架和校准机制，显著提升了基于流生成模型在逆问题中的性能。


### [77] [Towards Geometry Problem Solving in the Large Model Era: A Survey](https://arxiv.org/abs/2506.02690)
*Yurui Zhao,Xiang Wang,Jiahong Liu,Irwin King,Zhitao Huang*

Main category: cs.CV

TL;DR: 本文综述了几何问题解决（GPS）领域的进展，提出了统一的框架，并指出了未来研究方向。

- Motivation: 几何问题解决在AI领域具有重要意义，但目前仍面临空间理解和逻辑推理的双重挑战，且研究分散。
- Method: 通过三个核心维度（基准构建、文本与图表解析、推理范式）系统综述GPS进展，并提出统一分析框架。
- Result: 总结了当前GPS的局限性，并提出了未来研究方向，如自动化基准生成和可解释的神经符号集成。
- Conclusion: 未来研究应致力于实现人类水平的几何推理，推动GPS领域的进一步发展。


### [78] [Large-scale Self-supervised Video Foundation Model for Intelligent Surgery](https://arxiv.org/abs/2506.02692)
*Shu Yang,Fengtao Zhou,Leon Mayer,Fuxiang Huang,Yiliang Chen,Yihui Wang,Sunan He,Yuxiang Nie,Xi Wang,Ömer Sümer,Yueming Jin,Huihui Sun,Shuchang Xu,Alex Qinyang Liu,Zheng Li,Jing Qin,Jeremy YuenChun Teoh,Lena Maier-Hein,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种视频级手术预训练框架SurgVISTA，通过联合时空建模从大规模手术视频数据中学习表示，显著提升了手术场景理解的性能。

- Motivation: 现有AI方法缺乏显式的时间建模，导致无法完整捕捉动态手术场景，限制了手术辅助系统的效能。
- Method: 构建了大规模手术视频数据集，并提出SurgVISTA框架，通过重建任务和专家指导的知识蒸馏实现联合时空建模。
- Result: SurgVISTA在13个视频级数据集上表现优于自然和手术领域的预训练模型。
- Conclusion: SurgVISTA展示了在临床场景中推动智能手术系统的潜力。


### [79] [FaceSleuth: Learning-Driven Single-Orientation Attention Verifies Vertical Dominance in Micro-Expression Recognition](https://arxiv.org/abs/2506.02695)
*Linquan Wu,Tianxiang Jiang,Wenhao Duan,Yini Fang,Jacky Keung*

Main category: cs.CV

TL;DR: FaceSleuth是一种双流架构，通过垂直注意力模块和面部位置聚焦器增强微表情识别，结合SOA模块优化方向，显著提升性能。

- Motivation: 解决微表情识别中毫秒级、低幅度面部动作的捕捉问题，同时抑制身份特异性外观。
- Method: 提出双流架构FaceSleuth，包含垂直注意力模块（CVA）、面部位置聚焦器和轻量级动作单元嵌入；进一步引入SOA模块自适应学习最优方向。
- Result: 在三个标准MER基准测试中，FaceSleuth性能显著优于现有方法，最高准确率达95.1%。
- Conclusion: FaceSleuth确立了新的SOTA，并首次证明垂直注意力偏差是MER最具区分性的方向。


### [80] [LayoutRAG: Retrieval-Augmented Model for Content-agnostic Conditional Layout Generation](https://arxiv.org/abs/2506.02697)
*Yuxuan Wu,Le Wang,Sanping Zhou,Mengnan Liu,Gang Hua,Haoxiang Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于检索和参考引导的可控布局生成方法，通过检索与给定条件兼容的布局模板作为参考，并设计条件调制注意力机制，显著提升了生成质量。

- Motivation: 现有扩散或流匹配模型在条件生成任务中表现良好，但在特定约束下生成最优布局仍有改进空间。
- Method: 通过检索与条件兼容的布局模板作为参考，并利用参考引导去噪或流传输过程，同时设计条件调制注意力机制。
- Result: 实验表明，该方法生成的布局质量高且符合给定条件，优于现有最优模型。
- Conclusion: 该方法通过检索和参考引导，有效利用了潜在信息，显著提升了布局生成的性能。


### [81] [Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences](https://arxiv.org/abs/2506.02698)
*Yunhong Lu,Qichao Wang,Hengyuan Cao,Xiaoyin Xu,Min Zhang*

Main category: cs.CV

TL;DR: SmPO-Diffusion提出了一种改进DPO目标的方法，通过建模偏好分布和优化扩散目标，解决了现有方法中偏好粒度不足和优化目标不一致的问题。

- Motivation: 现有方法在文本到图像生成中忽略了偏好的个体差异和粒度问题，导致优化效果不佳。
- Method: 引入平滑偏好分布替代二元分布，利用奖励模型模拟人类偏好，并通过偏好似然平均改进DPO损失；采用反转技术模拟扩散模型的轨迹偏好分布。
- Result: SmPO-Diffusion在偏好评估中表现最优，训练成本更低，优于基线方法。
- Conclusion: SmPO-Diffusion通过简单修改有效解决了偏好粒度和优化目标对齐问题，取得了显著性能提升。


### [82] [ToothForge: Automatic Dental Shape Generation using Synchronized Spectral Embeddings](https://arxiv.org/abs/2506.02702)
*Tibor Kubík,François Guibault,Michal Španěl,Hervé Lombaert*

Main category: cs.CV

TL;DR: ToothForge是一种基于频谱分析的方法，用于自动生成3D牙齿模型，解决了牙科形状数据集稀疏的问题。通过同步频率嵌入建模，消除了分解不稳定性带来的偏差，并提升了生成质量。

- Motivation: 牙科形状数据集稀疏，且现有方法对网格连通性有严格要求，限制了应用范围。ToothForge旨在解决这些问题，提供更灵活高效的形状生成方法。
- Method: 在频谱域中操作，通过同步频率嵌入建模，对齐所有数据样本的频谱到共同基，消除分解不稳定性。支持高分辨率牙齿网格的快速生成。
- Result: 在真实牙冠数据集上，生成形状的重建质量优于未对齐嵌入的模型。还展示了频谱分析在形状压缩和插值中的应用。
- Conclusion: ToothForge结合频谱分析和机器学习，减少了对网格结构的限制，适用于牙科及其他医学领域的形状分析。


### [83] [Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation](https://arxiv.org/abs/2506.02708)
*Naoto Tanji,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉语言模型（VLM）训练方法，用于生成图像评分及其自然语言解释，通过自训练和直接偏好优化提升评分准确性和解释一致性。

- Motivation: 理解模型的评分依据对实际应用至关重要，但目前缺乏同时生成评分和解释的方法。
- Method: 利用图像评分数据集和指令调优的VLM进行自训练，无需外部数据或模型；通过直接偏好优化迭代训练两个数据集以提升性能。
- Result: 方法提高了评分准确性和生成解释的一致性。
- Conclusion: 该方法为图像评分任务提供了一种高效且可解释的解决方案。


### [84] [LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering](https://arxiv.org/abs/2506.02733)
*Xiaoyi Feng,Kaifeng Zou,Caichun Cen,Tao Huang,Hui Guo,Zizhou Huang,Yingli Zhao,Mingqing Zhang,Diwei Wang,Yuntao Zou,Dagang Li*

Main category: cs.CV

TL;DR: LinkTo-Anime是首个专为cel动画角色运动设计的高质量数据集，填补了现有光流数据集的空白，支持光流估计及相关任务研究。

- Motivation: 现有光流数据集主要针对真实世界模拟或合成人类运动，缺乏针对cel动画角色运动的数据集，而该领域具有独特的视觉和运动特征。
- Method: 通过3D模型渲染生成LinkTo-Anime数据集，提供丰富标注（光流、遮挡掩码、骨架等），并构建多数据集基准测试。
- Result: 数据集包含395个视频序列，共24,230训练帧、720验证帧和4,320测试帧。
- Conclusion: LinkTo-Anime填补了领域空白，为光流估计及相关任务提供了重要资源。


### [85] [GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal](https://arxiv.org/abs/2506.02736)
*Shufan Qing,Anzhen Li,Qiandi Wang,Yuefeng Niu,Mingchen Feng,Guoliang Hu,Jinqiao Wu,Fengtao Nan,Yingchun Fan*

Main category: cs.CV

TL;DR: GeneA-SLAM2通过深度方差约束处理动态场景，结合自编码器优化关键点分布，提升动态环境下的SLAM精度。

- Motivation: 现有动态环境语义SLAM方法无法完全覆盖动态区域，需更鲁棒高效的解决方案。
- Method: 利用深度方差提取动态像素，生成深度掩码；结合自编码器优化关键点分布。
- Result: 在高度动态序列中表现优于现有方法，保持高精度。
- Conclusion: GeneA-SLAM2在动态场景中具有高效性和鲁棒性，代码已开源。


### [86] [Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning](https://arxiv.org/abs/2506.02738)
*Negin Baghbanzadeh,Sajad Ashkezari,Elham Dolatabadi,Arash Afkanpour*

Main category: cs.CV

TL;DR: 论文提出了一种基于Transformer的目标检测方法，用于大规模提取生物医学文献中的子图，并发布了OPEN-PMC-18M数据集，提升了视觉-语言模型的性能。

- Motivation: 生物医学文献中的复合图（多面板组合）普遍存在，但大规模子图提取问题尚未解决，影响了视觉-语言模型的表征学习。
- Method: 采用基于Transformer的目标检测方法，在50万张合成复合图上训练，开发了可扩展的子图提取流程。
- Result: 在ImageCLEF 2016和合成基准测试中达到最优性能，并发布了包含1800万子图-标题对的OPEN-PMC-18M数据集。
- Conclusion: 该方法显著提升了视觉-语言模型的检索、零样本分类和鲁棒性性能，为生物医学视觉-语言建模提供了新基准。


### [87] [VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians](https://arxiv.org/abs/2506.02741)
*Pengchong Hu,Zhizhong Han*

Main category: cs.CV

TL;DR: 论文提出了一种基于视图绑定的3D高斯表示方法，用于RGBD SLAM系统，解决了现有方法在超大场景中无法扩展的问题，提升了渲染和跟踪性能。

- Motivation: 现有方法使用3D高斯表示场景，但在超大场景中因GPU内存限制和优化效率问题无法扩展。
- Method: 提出视图绑定的3D高斯表示，简化高斯参数，并设计新的跟踪和映射策略。
- Result: 在渲染和跟踪精度及扩展性上优于最新方法。
- Conclusion: 视图绑定的3D高斯表示和优化策略显著提升了RGBD SLAM的性能和扩展性。


### [88] [RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS](https://arxiv.org/abs/2506.02751)
*Chuanyu Fu,Yuqi Zhang,Kunbin Yao,Guanying Chen,Yuan Xiong,Chuan Huang,Shuguang Cui,Xiaochun Cao*

Main category: cs.CV

TL;DR: RobustSplat通过延迟高斯增长和尺度级联掩码引导，解决了3DGS中瞬态物体导致的渲染伪影问题。

- Motivation: 现有3DGS方法在瞬态物体场景中表现不佳，导致渲染伪影。
- Method: 提出延迟高斯增长策略和尺度级联掩码引导方法，优化静态场景结构并精确预测瞬态掩码。
- Result: 在多个数据集上表现优于现有方法，验证了其鲁棒性和有效性。
- Conclusion: RobustSplat显著提升了3DGS在瞬态场景中的渲染质量。


### [89] [Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations](https://arxiv.org/abs/2506.02764)
*Fatma Youssef Mohammed,Kostas Alexis*

Main category: cs.CV

TL;DR: 研究表明自由观看和视觉搜索可以共享一种共同表示，通过HAT架构实现知识迁移，性能仅下降3.86%，同时显著降低计算成本。

- Motivation: 探索自由观看和任务驱动的视觉搜索是否存在共同表示，以验证知识迁移的可行性。
- Method: 基于Human Attention Transformer (HAT)架构，训练模型在自由观看任务中学习注意力表示，并迁移至视觉搜索任务。
- Result: 模型在视觉搜索任务中性能仅下降3.86%（SemSS指标），计算成本降低92.29%（GFLOPs）和31.23%（参数）。
- Conclusion: 自由观看和视觉搜索可以高效共享共同表示，验证了知识迁移的可行性，并显著降低计算成本。


### [90] [A Dynamic Transformer Network for Vehicle Detection](https://arxiv.org/abs/2506.02765)
*Chunwei Tian,Kai Liu,Bob Zhang,Zhixiang Huang,Chia-Wen Lin,David Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种动态Transformer网络（DTNet），用于车辆检测，通过动态卷积和混合注意力机制提升检测性能。

- Motivation: 现有基于深度网络的车辆检测算法在光照和遮挡差异下性能受限，需提升检测器的适应性。
- Method: DTNet结合动态卷积生成权重，利用混合注意力机制（通道注意力与Transformer）增强信息提取，并通过空间位置信息优化结构信息。
- Result: 实验表明DTNet在车辆检测中具有竞争力。
- Conclusion: DTNet通过动态权重和混合注意力机制有效提升了车辆检测的适应性。


### [91] [FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts](https://arxiv.org/abs/2506.02781)
*Tongyuan Bai,Wangyuanfan Bai,Dong Chen,Tieru Wu,Manyi Li,Rui Ma*

Main category: cs.CV

TL;DR: FreeScene是一个用户友好的3D室内场景合成框架，支持自由形式的用户输入（文本或图像），通过VLM-based Graph Designer和MG-DiT模型实现高质量和可控的场景生成。

- Motivation: 解决现有方法在3D场景合成中粗粒度控制（语言）或高门槛控制（图结构）的问题。
- Method: 结合文本和图像输入，通过VLM-based Graph Designer生成图表示，再使用MG-DiT模型进行图感知去噪生成场景。
- Result: FreeScene在生成质量和可控性上优于现有方法，支持多种任务（如文本到场景、图到场景等）。
- Conclusion: FreeScene提供了一种高效且用户友好的解决方案，统一了文本和图结构的场景合成。


### [92] [SAMJ: Fast Image Annotation on ImageJ/Fiji via Segment Anything Model](https://arxiv.org/abs/2506.02783)
*Carlos Garcia-Lopez-de-Haro,Caterina Fuster-Barcelo,Curtis T. Rueden,Jonathan Heras,Vladimir Ulman,Daniel Franco-Barranco,Adrian Ines,Kevin W. Eliceiri,Jean-Christophe Olivo-Marin,Jean-Yves Tinevez,Daniel Sage,Arrate Munoz-Barrutia*

Main category: cs.CV

TL;DR: SAMJ是一个基于Segment Anything Model (SAM)的ImageJ/Fiji插件，旨在简化生物医学图像标注，提供一键安装和交互式标注功能。

- Motivation: 生物医学图像标注通常耗时且劳动密集，限制了AI驱动的图像分析效率。
- Method: 开发了SAMJ插件，利用SAM模型实现实时交互式标注，支持一键安装。
- Result: SAMJ简化了标注流程，加速了标注数据集的生成。
- Conclusion: SAMJ为生物医学图像分析提供了一种高效、易用的标注工具。


### [93] [Automated Measurement of Optic Nerve Sheath Diameter Using Ocular Ultrasound Video](https://arxiv.org/abs/2506.02789)
*Renxing Li,Weiyi Tang,Peiqi Li,Qiming Huang,Jiayuan She,Shengkai Li,Haoran Xu,Yeyun Wan,Jing Liu,Hailong Fu,Xiang Li,Jiangang Chen*

Main category: cs.CV

TL;DR: 提出了一种自动识别超声视频序列中最佳帧以测量视神经鞘直径（ONSD）的方法，结合KCF跟踪和SLIC分割算法，并通过GMM和KL散度方法测量，结果与专家测量高度一致。

- Motivation: ONSD是颅内压（ICP）的生物标志物，但手动测量依赖操作者经验，需自动化方法提高准确性和效率。
- Method: 使用KCF跟踪算法和SLIC分割算法自动识别最佳帧，结合GMM和KL散度方法测量ONSD。
- Result: 与专家测量相比，平均误差为0.04，均方差为0.054，ICC为0.782，显示高度准确性。
- Conclusion: 该方法可实现高精度自动化ONSD测量，具有临床应用潜力。


### [94] [Random Registers for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2506.02843)
*Shuai Yi,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 研究发现，在跨域小样本学习中，ViT的提示调优可能损害目标域泛化能力，而随机寄存器能提升性能。提出新方法增强注意力扰动，实验验证其有效性。

- Motivation: 探索ViT在跨域小样本学习中的迁移能力，发现提示调优的局限性，并寻求改进方法。
- Method: 通过随机寄存器扰动注意力，提出增强注意力扰动的方法，优化ViT的迁移性能。
- Result: 在四个基准测试中验证了方法的有效性，并达到最先进性能。
- Conclusion: 随机寄存器能有效提升ViT的跨域迁移能力，为小样本学习提供新思路。


### [95] [Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments](https://arxiv.org/abs/2506.02845)
*Di Wen,Lei Qi,Kunyu Peng,Kailun Yang,Fei Teng,Ao Luo,Jia Fu,Yufan Chen,Ruiping Liu,Yitian Shi,M. Saquib Sarfraz,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 论文提出了MicroG-4M数据集，用于微重力环境下人类活动的时空和语义理解，填补了现有数据集的空白。

- Motivation: 现有视频理解数据集多基于地球重力条件，而微重力环境会改变人类运动和视觉语义，这对安全关键的空间应用提出了挑战。
- Method: 通过真实太空任务和电影模拟构建数据集，包含4,759个视频片段、50种动作、1,238个上下文丰富的描述和7,000多个问答对。
- Result: 数据集支持三个核心任务：细粒度多标签动作识别、时序视频描述和视觉问答，为微重力环境下的空间定位和语义推理提供评估基准。
- Conclusion: MicroG-4M填补了微重力环境下视频理解的空白，为相关研究提供了重要资源。


### [96] [PBR-SR: Mesh PBR Texture Super Resolution from 2D Image Priors](https://arxiv.org/abs/2506.02846)
*Yujin Chen,Yinyu Nie,Benjamin Ummenhofer,Reiner Birkl,Michael Paulitsch,Matthias Nießner*

Main category: cs.CV

TL;DR: PBR-SR是一种零样本方法，通过迭代优化和先验约束，从低分辨率PBR纹理生成高质量高分辨率纹理，无需额外训练或数据。

- Motivation: 解决基于视图的超分辨率方法中常见的视角不一致和光照敏感性问题，同时保持对低分辨率输入的忠实性。
- Method: 利用预训练的自然图像超分辨率模型，通过迭代最小化先验与可微分渲染之间的偏差，并结合多视角渲染的2D先验约束和纹理域的身份约束。
- Result: 生成的高分辨率PBR纹理在艺术设计和AI生成网格上均表现出高保真度，优于直接应用超分辨率模型和先前的纹理优化方法。
- Conclusion: PBR-SR在PBR和渲染评估中均表现优异，支持高级应用如重光照。


### [97] [METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding](https://arxiv.org/abs/2506.02850)
*Mengyue Wang,Shuo Chen,Kristian Kersting,Volker Tresp,Yunpu Ma*

Main category: cs.CV

TL;DR: METok是一种无需训练的多阶段事件驱动令牌压缩框架，用于加速视频大语言模型（VLLMs）的推理，同时保持准确性。

- Motivation: 处理长视频时，计算需求高且视觉数据冗余，因此需要一种高效的方法来加速推理。
- Method: METok通过三个阶段逐步消除冗余视觉令牌：事件感知压缩、分层令牌剪枝和解码阶段的KV Cache优化。
- Result: 在多个视频基准测试中，METok显著减少了计算开销（如80.6%的FLOPs减少）和内存消耗（93.5%的KV Cache节省），同时保持或提高准确性。
- Conclusion: METok在效率和准确性之间实现了最佳平衡，为长视频处理提供了高效解决方案。


### [98] [Learning Pyramid-structured Long-range Dependencies for 3D Human Pose Estimation](https://arxiv.org/abs/2506.02853)
*Mingjie Wei,Xuemei Xie,Yutong Zhong,Guangming Shi*

Main category: cs.CV

TL;DR: 论文提出了一种金字塔图注意力（PGA）模块和金字塔图变换器（PGFormer），用于3D人体姿态估计，通过多尺度捕捉长距离依赖关系，实现了更低的误差和更小的模型体积。

- Motivation: 现有方法通过增加网络深度学习非连接部分间的依赖关系，但会引入无关噪声并增加模型体积。本文旨在通过金字塔结构更有效地捕捉长距离依赖关系。
- Method: 提出PGA模块，通过并行计算多尺度信息的相关性，并结合图卷积模块构建PGFormer，利用池化将人体子结构封装到自注意力中。
- Result: 在Human3.6M和MPI-INF-3DHP数据集上，PGFormer实现了比现有方法更低的误差和更小的模型体积。
- Conclusion: PGFormer通过金字塔结构有效捕捉长距离依赖关系，为3D人体姿态估计提供了一种轻量级多尺度变换器架构。


### [99] [Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image Segmentation Framework](https://arxiv.org/abs/2506.02854)
*Mengmeng Zhang,Xingyuan Dai,Yicheng Sun,Jing Wang,Yueyang Yao,Xiaoyan Gong,Fuze Cong,Feiyue Wang,Yisheng Lv*

Main category: cs.CV

TL;DR: 论文提出了一种名为HSP-SAM的自提示框架，用于解决SAM在医学图像分割中依赖手动提示的问题，通过引入抽象提示学习，显著提升了性能。

- Motivation: SAM在自然图像分割中表现优异，但在医学图像分割中依赖手动提示，限制了其应用。现有方法难以消除这种依赖性。
- Method: 提出Hierarchical Self-Prompting SAM (HSP-SAM)，在自提示过程中学习抽象提示，而非仅依赖位置提示。
- Result: 在息肉和皮肤病变分割等任务中表现优异，泛化能力强，某些基准上性能提升达14.04%。
- Conclusion: 抽象提示比位置提示包含更丰富的语义信息，增强了模型的鲁棒性和泛化性能。


### [100] [Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection](https://arxiv.org/abs/2506.02857)
*Luca Maiano,Fabrizio Casadei,Irene Amerini*

Main category: cs.CV

TL;DR: 论文提出两种新的OOD检测方法，用于解决深度伪造检测中的开放集泛化问题，实验证明其优于现有技术。

- Motivation: 深度伪造检测在开放集场景中的泛化能力不足，现有方法难以应对不断更新的生成模型。
- Method: 提出两种OOD检测方法：一种基于输入图像重建，另一种结合注意力机制。
- Result: 实验验证了方法的有效性，在基准测试中表现优异。
- Conclusion: 该方法在动态现实应用中具有潜力，为深度伪造检测提供了鲁棒且适应性强的解决方案。


### [101] [MVTD: A Benchmark Dataset for Maritime Visual Object Tracking](https://arxiv.org/abs/2506.02866)
*Ahsan Baidar Bakht,Muhayy Ud Din,Sajid Javed,Irfan Hussain*

Main category: cs.CV

TL;DR: 论文介绍了专门为海上视觉目标跟踪设计的Maritime Visual Tracking Dataset (MVTD)，包含182个高分辨率视频序列，涵盖四种典型目标类别。实验显示现有算法在MVTD上表现不佳，但通过微调显著提升性能。

- Motivation: 海上环境中的视觉目标跟踪面临独特挑战（如水面反射、低对比度目标等），现有通用数据集无法满足需求，因此需要专门的领域数据集。
- Method: 构建MVTD数据集，包含182个视频序列（约15万帧），涵盖四种目标类别，并评估14种SOTA跟踪算法。
- Result: 现有算法在MVTD上表现显著下降，但通过微调后性能大幅提升。
- Conclusion: MVTD填补了海上视觉跟踪领域的空白，证明了领域适应和迁移学习的重要性。


### [102] [Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings](https://arxiv.org/abs/2506.02868)
*Amal S. Perera,David Fernandez,Chandi Witharana,Elias Manos,Michael Pimenta,Anna K. Liljedahl,Ingmar Nitze,Yili Yang,Todd Nicholson,Chia-Yu Hsu,Wenwen Li,Guido Grosse*

Main category: cs.CV

TL;DR: 论文探讨了使用Vision Transformers（ViTs）结合地理空间位置嵌入技术，在北极高分辨率遥感任务中检测永久冻土地貌、融化扰动和人类基础设施的优越性。

- Motivation: 北极地区的高分辨率遥感数据庞大且标注数据有限，传统CNN方法在捕捉长距离依赖和全局上下文方面存在不足，ViTs通过自监督预训练和注意力机制提供了解决方案。
- Method: 研究整合了地理空间位置嵌入到ViTs中，以提升跨区域适应性，并在三个任务（冰楔多边形、融化塌陷和人类基础设施检测）上评估模型性能。
- Result: 实验表明，结合位置嵌入的ViTs在两项任务中优于传统CNN模型，如融化塌陷检测的F1分数从0.84提升至0.92。
- Conclusion: ViTs结合空间感知能力在北极遥感任务中表现出潜力，为处理复杂光谱特征和跨区域泛化提供了新思路。


### [103] [NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results](https://arxiv.org/abs/2506.02875)
*Xiaohong Liu,Xiongkuo Min,Qiang Hu,Xiaoyun Zhang,Jie Guo,Guangtao Zhai,Shushi Wang,Yingjie Zhou,Lu Liu,Jingxin Li,Liu Yang,Farong Wen,Li Xu,Yanwei Jiang,Xilei Zhu,Chunyi Li,Zicheng Zhang,Huiyu Duan,Xiele Wu,Yixuan Gao,Yuqin Cao,Jun Jia,Wei Sun,Jiezhang Cao,Radu Timofte,Baojun Li,Jiamian Huang,Dan Luo,Tao Liu,Weixia Zhang,Bingkun Zheng,Junlin Chen,Ruikai Zhou,Meiya Chen,Yu Wang,Hao Jiang,Xiantao Li,Yuxiang Jiang,Jun Tang,Yimeng Zhao,Bo Hu,Zelu Qi,Chaoyang Zhang,Fei Zhao,Ping Shi,Lingzhi Fu,Heng Cong,Shuai He,Rongyu Zhang,Jiarong He,Zongyao Hu,Wei Luo,Zihao Yu,Fengbin Guan,Yiting Lu,Xin Li,Zhibo Chen,Mengjing Su,Yi Wang,Tuo Chen,Chunxiao Li,Shuaiyu Zhao,Jiaxin Wen,Chuyi Lin,Sitong Liu,Ningxin Chu,Jing Wan,Yu Zhou,Baoying Chen,Jishen Zeng,Jiarui Liu,Xianjin Liu,Xin Chen,Lanzhi Zhou,Hangyu Li,You Han,Bibo Xiang,Zhenjie Liu,Jianzhang Lu,Jialin Gui,Renjie Lu,Shangfei Wang,Donghao Zhou,Jingyu Lin,Quanjian Song,Jiancheng Huang,Yufeng Yang,Changwei Wang,Shupeng Zhong,Yang Yang,Lihuo He,Jia Liu,Yuting Xing,Tida Fang,Yuchun Jin*

Main category: cs.CV

TL;DR: NTIRE 2025 XGC挑战赛分为用户生成视频、AI生成视频和说话头部三个赛道，吸引了大量参与者和提交，推动了相关领域的发展。

- Motivation: 解决视频和说话头部处理领域的主要挑战，促进技术进步。
- Method: 挑战赛分为三个赛道，分别使用不同数据集（FineVD-GC、Q-Eval-Video、THQA-NTIRE），参与者提交模型和事实表。
- Result: 各赛道参与者众多，提交了大量模型，所有团队的方法均优于基线。
- Conclusion: 挑战赛成功推动了三个赛道领域的技术发展。


### [104] [GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation](https://arxiv.org/abs/2506.02882)
*Sohyun Lee,Yeho Kwon,Lukas Hoyer,Suha Kwak*

Main category: cs.CV

TL;DR: 论文提出了一种名为GaRA的轻量级适配器方法，用于提升Segment Anything Model (SAM)在输入退化情况下的鲁棒性，显著优于现有方法。

- Motivation: 提升SAM在高风险应用（如自动驾驶和机器人）中的鲁棒性，同时保持其泛化能力。
- Method: 引入动态调整权重矩阵有效秩的轻量级适配器（GaRA），通过门控模块选择性激活矩阵分量，实现细粒度和输入感知的鲁棒化。
- Result: GaRA-SAM在所有鲁棒分割基准测试中表现优异，特别是在ACDC数据集上，IoU得分比之前最佳方法提高了21.3%。
- Conclusion: GaRA方法在不牺牲SAM泛化能力的前提下，显著提升了其鲁棒性，适用于实际应用场景。


### [105] [OpenFace 3.0: A Lightweight Multitask System for Comprehensive Facial Behavior Analysis](https://arxiv.org/abs/2506.02891)
*Jiewen Hu,Leena Mathur,Paul Pu Liang,Louis-Philippe Morency*

Main category: cs.CV

TL;DR: OpenFace 3.0是一个开源工具包，用于面部行为分析，包括面部标志检测、动作单元检测、视线估计和情绪识别。它通过多任务架构训练，提升了性能、速度和效率。

- Motivation: 近年来，计算领域对面部行为分析系统的兴趣增加，OpenFace 3.0旨在提供一个轻量级、高效且易于使用的工具包。
- Method: 采用多任务架构训练的统一模型，适用于不同人群、头部姿势、光照条件和任务。
- Result: OpenFace 3.0在预测性能、推理速度和内存效率上优于类似工具包，并达到最新技术水平。
- Conclusion: OpenFace 3.0是一个高效、易用的开源工具，支持实时操作和社区贡献。


### [106] [Dense Match Summarization for Faster Two-view Estimation](https://arxiv.org/abs/2506.02893)
*Jonathan Astermark,Anders Heyden,Viktor Larsson*

Main category: cs.CV

TL;DR: 提出了一种高效的匹配摘要方案，显著加速了密集对应下的两视图相对位姿估计，同时保持与完整密集匹配相当的精度。

- Motivation: 密集匹配器虽能提高位姿估计的准确性和鲁棒性，但其大量匹配点导致RANSAC中的鲁棒估计运行时间显著增加。
- Method: 提出了一种高效的匹配摘要方案，以减少运行时间。
- Result: 在标准基准数据集上验证，运行时间快10-100倍，精度与完整密集匹配相当。
- Conclusion: 该方法在保持精度的同时显著提升了计算效率。


### [107] [FlySearch: Exploring how vision-language models explore](https://arxiv.org/abs/2506.02896)
*Adam Pardyl,Dominik Matuszek,Mateusz Przebieracz,Marek Cygan,Bartosz Zieliński,Maciej Wołczyk*

Main category: cs.CV

TL;DR: 论文探讨了视觉语言模型（VLMs）在复杂环境中是否有效，通过FlySearch实验发现其表现不佳，并提出改进方法。

- Motivation: 现实世界复杂且非结构化，需主动探索关键信息，研究VLMs在此类条件下的有效性。
- Method: 引入FlySearch，一个3D户外逼真环境，定义三种难度场景，测试VLMs表现并分析原因。
- Result: 现有VLMs在简单探索任务中表现不佳，与人类差距随任务难度增加，部分问题可通过微调解决。
- Conclusion: VLMs在复杂环境中仍需改进，公开了基准、场景和代码库。


### [108] [Towards Auto-Annotation from Annotation Guidelines: A Benchmark through 3D LiDAR Detection](https://arxiv.org/abs/2506.02914)
*Yechi Ma,Wei Hua,Shu Kong*

Main category: cs.CV

TL;DR: 论文提出新基准AnnoGuide，旨在通过专家定义的标注指南自动标注数据，减少人工标注需求，并在nuScenes数据集上验证了方法的有效性。

- Motivation: 数据标注是机器学习应用中的关键但繁琐且昂贵的过程，研究旨在通过自动化方法直接从标注指南生成标注，减少人工干预。
- Method: 采用开源基础模型进行RGB图像目标检测与分割，将2D检测投影到3D空间，并通过聚类LiDAR点生成3D立方体标注。
- Result: 方法显著提升了3D检测性能（mAP从12.1提升至21.9），但仍面临挑战，表明需要进一步发展LiDAR基础模型。
- Conclusion: AnnoGuide是一个开放且具有挑战性的问题，未来需开发更强大的LiDAR基础模型以进一步提升性能。


### [109] [MIND: Material Interface Generation from UDFs for Non-Manifold Surface Reconstruction](https://arxiv.org/abs/2506.02938)
*Xuhui Chen,Fei Hou,Wencheng Wang,Hong Qin,Ying He*

Main category: cs.CV

TL;DR: MIND算法直接从UDF生成非流形网格，解决了现有方法在提取非流形几何时的局限性。

- Motivation: 现有方法从UDF提取网格时存在拓扑缺陷和非流形几何无法表示的问题。
- Method: 通过计算多标签全局场，结合UDF生成材料界面，支持非流形网格提取。
- Result: 实验表明MIND能稳健处理复杂非流形表面，显著优于现有方法。
- Conclusion: MIND为UDF的非流形网格提取提供了高效解决方案。


### [110] [FORLA:Federated Object-centric Representation Learning with Slot Attention](https://arxiv.org/abs/2506.02964)
*Guiqiu Liao,Matjaz Jogan,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: FORLA是一种新型联邦学习框架，通过无监督槽注意力实现跨客户端的对象中心表示学习和特征适应，优于集中式基线。

- Motivation: 解决异构无标签数据集中高效视觉表示的联邦学习挑战，需要跨客户端联合信息特征并解耦领域特定因素。
- Method: 采用共享特征适配器和槽注意力模块，设计双分支师生架构，优化适配器并跨域对齐对象级表示。
- Result: 在多个真实数据集上表现优于集中式基线，学习到紧凑且通用的跨域表示。
- Conclusion: 联邦槽注意力是分布式概念跨域数据中可扩展无监督视觉表示学习的有效工具。


### [111] [HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation](https://arxiv.org/abs/2506.02975)
*Yicheng Xiao,Lin Song,Rui Yang,Cheng Cheng,Zunnan Xu,Zhaoyang Zhang,Yixiao Ge,Xiu Li,Ying Shan*

Main category: cs.CV

TL;DR: 本文提出了一种高效训练范式，构建单一Transformer模型HaploOmni，用于统一多模态理解和生成，通过多模态预热策略和特征预缩放等技术，在有限训练成本下实现竞争性能。

- Motivation: 随着语言模型的进步，多模态理解和生成从分离组件发展为统一单模型框架，本文旨在探索高效训练方法以构建此类模型。
- Method: 提出多模态预热策略、特征预缩放和多模态AdaLN技术，构建HaploOmni单一多模态Transformer。
- Result: HaploOmni在有限训练成本下，在多个图像和视频理解与生成基准测试中表现优异。
- Conclusion: HaploOmni展示了统一多模态模型的潜力，代码将开源。


### [112] [Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge](https://arxiv.org/abs/2506.02976)
*Rachid Zeghlache,Ikram Brahim,Pierre-Henri Conze,Mathieu Lamard,Mohammed El Amine Lazouni,Zineb Aziza Elaouaber,Leila Ryma Lazouni,Christopher Nielsen,Ahmad O. Ahsan,Matthias Wilms,Nils D. Forkert,Lovre Antonio Budimir,Ivana Matovinović,Donik Vršnak,Sven Lončarić,Philippe Zhang,Weili Jiang,Yihao Li,Yiding Hao,Markus Frohmann,Patrick Binder,Marcel Huber,Taha Emre,Teresa Finisterra Araújo,Marzieh Oghbaie,Hrvoje Bogunović,Amerens A. Bekkers,Nina M. van Liebergen,Hugo J. Kuijf,Abdul Qayyum,Moona Mazher,Steven A. Niederer,Alberto J. Beltrán-Carrero,Juan J. Gómez-Valverde,Javier Torresano-Rodríquez,Álvaro Caballero-Sastre,María J. Ledesma Carbayo,Yosuke Yamagishi,Yi Ding,Robin Peretzke,Alexandra Ertl,Maximilian Fischer,Jessica Kächele,Sofiane Zehar,Karim Boukli Hacene,Thomas Monfort,Béatrice Cochener,Mostafa El Habib Daho,Anas-Alexis Benyoussef,Gwenolé Quellec*

Main category: cs.CV

TL;DR: MARIO挑战赛旨在通过OCT图像分析提升AMD的自动检测与监测，包含分类和预测任务，AI在AMD进展测量上与医生表现相当，但预测未来进展仍有不足。

- Motivation: 推动AMD的自动化检测与监测技术，评估算法在多模态数据集上的性能，并为AMD监测设定基准。
- Method: 使用来自法国和阿尔及利亚的多模态数据集，35个团队参与，任务包括分类和预测AMD进展。
- Result: AI在测量AMD进展（任务1）上与医生表现相当，但预测未来进展（任务2）表现不佳。
- Conclusion: MARIO挑战赛为AMD监测设定了基准，AI在部分任务中表现优异，但预测能力仍需改进。


### [113] [Astrophotography turbulence mitigation via generative models](https://arxiv.org/abs/2506.02981)
*Joonyeoup Kim,Yu Yuan,Xingguang Zhang,Xijun Wang,Stanley Chan*

Main category: cs.CV

TL;DR: AstroDiff是一种基于扩散模型的生成修复方法，用于减轻天文图像中的大气湍流影响，优于现有学习方法。

- Motivation: 地面望远镜拍摄的天文图像常受大气湍流影响，导致质量下降，现有方法数据采集和处理复杂。
- Method: 利用扩散模型的高质量生成先验和修复能力，提出AstroDiff方法。
- Result: 实验表明，AstroDiff在湍流条件下提供更高的感知质量和结构保真度。
- Conclusion: AstroDiff在天文图像湍流修复中表现优异，代码和结果已公开。


### [114] [DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models](https://arxiv.org/abs/2506.03007)
*Jiarui Wang,Huiyu Duan,Juntong Wang,Ziheng Jia,Woo Yi Yang,Xiaorong Zhu,Yu Zhao,Jiaying Qian,Yuke Xing,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 论文提出DFBench基准和MoA-DF方法，利用多模态大模型提升深度伪造检测性能。

- Motivation: 随着生成模型的快速发展，AI生成图像的逼真度显著提高，验证数字内容真实性面临挑战。现有检测方法依赖的数据集生成模型和内容多样性有限，难以应对AI生成内容的复杂性和逼真度提升。
- Method: 提出DFBench基准，包含54万张图像，覆盖真实、AI编辑和AI生成内容，采用12种最新生成模型；提出MoA-DF方法，利用多模态大模型的组合概率策略进行检测。
- Result: MoA-DF在深度伪造检测中达到最先进性能，验证了多模态大模型的有效性。
- Conclusion: DFBench和MoA-DF为深度伪造检测提供了新基准和方法，展示了多模态大模型的潜力。


### [115] [Smartflow: Enabling Scalable Spatiotemporal Geospatial Research](https://arxiv.org/abs/2506.03022)
*David McVicar,Brian Avant,Adrian Gould,Diego Torrejon,Charles Della Porta,Ryan Mukherjee*

Main category: cs.CV

TL;DR: BlackSky的Smartflow是一个基于云的框架，支持可扩展的时空地理空间研究，结合开源工具和技术，通过标准化数据立方体处理异构地理空间数据，并利用Kubernetes实现工作流编排。

- Motivation: 解决大规模地理空间数据分析和模型开发的挑战，提供标准化和可扩展的解决方案。
- Method: 使用STAC兼容目录作为输入，结合ClearML、Tensorboard和Apache Superset等工具管理模型实验，依托Kubernetes实现工作流编排。
- Result: Smartflow适用于大范围地理区域和时间尺度的模型开发，并展示了一种用于监测大型地理区域重型建筑的新型神经网络架构。
- Conclusion: Smartflow为地理空间研究提供了高效、可扩展的解决方案，其新型神经网络架构在重型建筑监测中表现出色。


### [116] [Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](https://arxiv.org/abs/2506.03065)
*Pengtao Chen,Xianfang Zeng,Maosen Zhao,Peng Ye,Mingzhu Shen,Wei Cheng,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: Sparse-vDiT通过优化注意力机制中的稀疏模式，显著降低了视频生成的计算复杂度和推理延迟，同时保持高质量输出。

- Motivation: Diffusion Transformers在视频生成中取得了突破，但注意力机制的二次复杂度导致推理延迟高，限制了长序列生成任务。
- Method: 通过分析vDiT中的注意力图，识别出三种稀疏模式，并提出Sparse-vDiT框架，包括模式优化的稀疏核和离线稀疏扩散搜索算法。
- Result: 在多个vDiT模型中，Sparse-vDiT实现了2.09×至2.38×的理论FLOP减少和1.58×至1.85×的实际推理加速，同时PSNR值保持在22.59至27.09。
- Conclusion: 研究表明，vDiT中的潜在结构稀疏性可以系统性地用于长视频合成，显著提升效率而不牺牲视觉质量。


### [117] [EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.03067)
*Mingzhe Li,Gehao Zhang,Zhenting Wang,Shiqing Ma,Siqi Pan,Richard Cartwright,Juan Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种名为\sys的提示反转技术，用于文本到图像扩散模型，通过预训练图像描述模型初始化嵌入，在潜在空间中逆向工程优化，并通过嵌入到文本模型转换为文本。实验表明，该方法在图像相似性、文本对齐、提示可解释性和泛化性方面优于现有方法。

- Motivation: 提示反转任务（如识别用于生成特定图像的文本提示）在数据归属、模型溯源和水印验证等方面具有重要应用潜力。现有方法在语义流畅性和效率上存在挑战，而高级图像描述模型生成的提示缺乏图像相似性。
- Method: 提出\sys技术，包括：1) 使用预训练图像描述模型初始化嵌入；2) 在潜在空间中进行逆向工程优化；3) 通过嵌入到文本模型转换为文本。
- Result: 在MS COCO、LAION和Flickr等数据集上的实验表明，该方法在图像相似性、文本对齐、提示可解释性和泛化性方面优于现有方法。
- Conclusion: \sys技术在提示反转任务中表现出色，并展示了在跨概念图像合成、概念操作、进化多概念生成和无监督分割等任务中的应用潜力。


### [118] [LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM](https://arxiv.org/abs/2506.03073)
*Roman Titkov,Egor Zubkov,Dmitry Yudin,Jaafar Mahmoud,Malik Mohrat,Gennady Sidorov*

Main category: cs.CV

TL;DR: LEG-SLAM结合高斯泼溅和视觉语言特征提取，实现实时语义3D SLAM，速度快且无需预计算。

- Motivation: 解决高斯泼溅方法中语义信息集成与实时性能的挑战，适用于SLAM应用。
- Method: 融合优化高斯泼溅、DINOv2特征提取和PCA特征压缩，支持在线密集SLAM。
- Result: 在Replica和ScanNet数据集上分别达到10 fps和18 fps，重建速度优于现有方法。
- Conclusion: LEG-SLAM在实时语义3D SLAM中表现优异，适用于机器人、AR等领域。


### [119] [ORV: 4D Occupancy-centric Robot Video Generation](https://arxiv.org/abs/2506.03079)
*Xiuyu Yang,Bohan Li,Shaocong Xu,Nan Wang,Chongjie Ye,Zhaoxi Chen,Minghan Qin,Yikang Ding,Xin Jin,Hang Zhao,Hao Zhao*

Main category: cs.CV

TL;DR: ORV框架通过4D语义占据序列生成精细化的机器人视频，解决了现有方法控制精度低和泛化能力差的问题。

- Motivation: 传统基于动作的生成模型在机器人学习和仿真中因全局粗对齐导致控制精度低和泛化能力差，需更精细的表示方法。
- Method: 提出ORV框架，利用4D语义占据序列提供精细化的语义和几何指导，支持多视角视频生成。
- Result: 实验表明ORV在多个数据集和子任务中优于基线方法，具有高时间一致性和精确可控性。
- Conclusion: ORV通过占用中心表示实现了仿真数据到逼真机器人视频的高效转换，适用于下游机器人学习任务。


### [120] [SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis](https://arxiv.org/abs/2506.03082)
*Ssharvien Kumar Sivakumar,Yannik Frisch,Ghazal Ghazaei,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: SG2VID是一种基于扩散模型的视频生成方法，利用场景图实现精确视频合成和细粒度人工控制，用于手术模拟训练。

- Motivation: 传统手术模拟工具缺乏真实感和解剖变异性，现有生成模型忽视人工控制。SG2VID旨在填补这一空白。
- Method: SG2VID结合场景图和扩散模型，实现对工具和解剖结构的精确控制，生成高质量手术视频。
- Result: SG2VID在多个数据集上表现优于现有方法，支持生成增强和下游任务改进。
- Conclusion: SG2VID为手术模拟提供了高真实感和可控性，适用于生成罕见手术场景。


### [121] [InterMamba: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba](https://arxiv.org/abs/2506.03084)
*Zizhao Wu,Yingying Sun,Yiming Chen,Xiaoling Gu,Ruyu Liu,Jiazhou Chen*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba框架的高效人-人交互生成方法，解决了现有Transformer架构的可扩展性和效率问题。

- Motivation: 人-人交互生成对理解人类社交行为至关重要，但现有方法（如基于Transformer的架构）存在可扩展性和效率问题。
- Method: 采用自适应时空Mamba框架，包含两个并行SSM分支和自适应机制，结合空间和时间特征，并开发了两个关键模块（自适应和交叉自适应时空Mamba模块）。
- Result: 在两个交互数据集上实现了最先进的结果，参数规模仅为66M（InterGen的36%），推理速度为0.57秒（InterGen的46%）。
- Conclusion: 该方法在保持高效的同时显著提升了交互生成的准确性和效率。


### [122] [Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness](https://arxiv.org/abs/2506.03089)
*Lucas Piper,Arlindo L. Oliveira,Tiago Marques*

Main category: cs.CV

TL;DR: EVNets结合VOneBlock和SubcorticalBlock，提升CNN的鲁棒性，并在多项基准测试中表现优于传统CNN。

- Motivation: 解决CNN在视觉扰动和域外图像中的脆弱性问题，通过模仿生物视觉系统提升模型鲁棒性。
- Method: 提出EVNets，结合VOneBlock（模仿V1皮层）和SubcorticalBlock（模仿皮层下结构），并优化其参数以对齐神经科学研究。
- Result: EVNets在V1对齐、形状偏好和鲁棒性测试中表现优异，比基础CNN提升8.5%。结合数据增强后，性能进一步提升7.3%。
- Conclusion: EVNets展示了架构改进与数据增强的互补性，为提升模型鲁棒性提供了新方向。


### [123] [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/abs/2506.03096)
*Christian Schlarmann,Francesco Croce,Nicolas Flammarion,Matthias Hein*

Main category: cs.CV

TL;DR: FuseLIP提出了一种新的多模态嵌入架构，通过单一Transformer模型处理文本和图像标记，实现早期融合，优于现有方法。

- Motivation: 现有对比语言-图像预训练方法无法原生处理多模态输入，需额外模块融合特征。
- Method: 使用单一Transformer模型处理扩展的文本和图像标记词汇表，实现早期融合。
- Result: FuseLIP在多模态任务（如VQA和文本引导图像检索）中表现优异，单模态任务与基线相当。
- Conclusion: FuseLIP通过早期融合实现更丰富的表示，在多模态任务中具有优势。


### [124] [EgoVLM: Policy Optimization for Egocentric Video Understanding](https://arxiv.org/abs/2506.03097)
*Ashwin Vinod,Shrey Pandit,Aditya Vavre,Linshen Liu*

Main category: cs.CV

TL;DR: EgoVLM是一个专为第一人称视频设计的视觉语言模型，通过GRPO强化学习优化，显著提升在egocentric视频问答任务上的性能。

- Motivation: 随着可穿戴设备和自主代理的普及，需要从第一人称视频流中进行鲁棒的推理。
- Method: EgoVLM通过GRPO强化学习直接优化，无需监督微调，并引入关键帧奖励机制。
- Result: EgoVLM-3B在EgoSchema基准上优于通用VLMs，性能提升显著。
- Conclusion: EgoVLM通过显式生成推理轨迹增强可解释性，适用于下游应用，关键帧奖励为未来研究提供了新方向。


### [125] [DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation](https://arxiv.org/abs/2506.03103)
*Xiaoyan Cong,Angela Xing,Chandradeep Pokhariya,Rao Fu,Srinath Sridhar*

Main category: cs.CV

TL;DR: DyTact是一种无标记的动态手-物体接触捕捉方法，通过2D高斯曲面和MANO网格结合，优化动态接触重建，解决了遮挡和复杂细节问题。

- Motivation: 动态手-物体接触重建在AI动画、XR和机器人中至关重要，但现有技术因遮挡和复杂细节难以实现。
- Method: DyTact使用动态关节化2D高斯曲面表示，结合MANO网格优化，并通过接触引导的自适应采样策略处理遮挡。
- Result: DyTact在动态接触估计和新视角合成质量上达到最优，且优化速度快、内存效率高。
- Conclusion: DyTact为非侵入式动态接触捕捉提供了高效解决方案，适用于多种应用场景。


### [126] [ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions](https://arxiv.org/abs/2506.03107)
*Di Chang,Mingdeng Cao,Yichun Shi,Bo Liu,Shengqu Cai,Shijie Zhou,Weilin Huang,Gordon Wetzstein,Mohammad Soleymani,Peng Wang*

Main category: cs.CV

TL;DR: ByteMorph是一个基于指令的图像编辑框架，专注于非刚性运动，包含数据集ByteMorph-6M和基线模型ByteMorpher。

- Motivation: 现有方法主要关注静态场景或刚性变换，无法处理动态运动的复杂编辑需求。
- Method: 采用Diffusion Transformer (DiT)构建ByteMorpher模型，并通过运动引导数据生成和分层合成技术构建ByteMorph-6M数据集。
- Result: ByteMorph-6M包含600万对高分辨率图像编辑数据，并提供了评估基准ByteMorph-Bench。
- Conclusion: ByteMorph填补了非刚性运动图像编辑的空白，并通过大规模数据集和强基线模型推动了该领域的发展。


### [127] [Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning](https://arxiv.org/abs/2506.03110)
*Shuai Yi,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: ViT在跨域少样本学习中表现不佳，研究发现图像标记的连续性影响泛化能力。通过破坏连续性，模型更依赖小模式，从而减少域差距。

- Motivation: 探索ViT在跨域少样本学习中的性能下降原因，并提出改进方法。
- Method: 破坏图像标记的连续性，使模型更依赖小模式而非大模式。
- Result: 实验证明该方法有效减少域差距，性能优于现有方法。
- Conclusion: 破坏图像标记连续性是一种简单有效的跨域少样本学习方法。


### [128] [Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery](https://arxiv.org/abs/2506.03114)
*Michelle Chen,David Russell,Amritha Pallavoor,Derek Young,Jane Wu*

Main category: cs.CV

TL;DR: 论文探讨了使用预训练的Segment Anything Model 2（SAM2）进行零样本树木检测与分割的效果，展示了其在遥感图像中的泛化能力。

- Motivation: 气候变化和环境因素快速改变森林景观，大规模树木分割对生态研究至关重要，但现有方法依赖难以扩展的训练数据。
- Method: 评估预训练的SAM2模型在零样本分割和零样本迁移任务中的表现，利用现有树木检测模型的预测作为提示。
- Result: SAM2表现出强大的泛化能力，并能与专业方法协同，为遥感问题提供了新思路。
- Conclusion: 应用大型预训练模型于遥感领域是未来发展的有前景方向。


### [129] [Targeted Forgetting of Image Subgroups in CLIP Models](https://arxiv.org/abs/2506.03117)
*Zeliang Zhang,Gaowen Liu,Charles Fleming,Ramana Rao Kompella,Chenliang Xu*

Main category: cs.CV

TL;DR: 论文提出了一种三阶段方法，用于在CLIP模型中精细地选择性遗忘特定知识，同时保持模型性能。

- Motivation: 基础模型（如CLIP）在零样本任务中表现优异，但可能从互联网数据中继承有害知识，现有遗忘方法无法精细处理特定知识遗忘。
- Method: 提出三阶段方法：遗忘阶段（微调目标知识）、提醒阶段（恢复保留样本性能）和恢复阶段（通过模型汤恢复零样本能力）。
- Result: 在CIFAR-10、ImageNet-1K等数据集上验证，该方法能有效遗忘特定子组，同时保持零样本性能。
- Conclusion: 该方法显著优于基线遗忘方法，适用于CLIP等基础模型的精细知识遗忘。


### [130] [Controllable Human-centric Keyframe Interpolation with Generative Prior](https://arxiv.org/abs/2506.03119)
*Zujin Guo,Size Wu,Zhongang Cai,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: PoseFuse3D-KI是一种新框架，通过整合3D人体引导信号改进关键帧插值，显著优于现有方法。

- Motivation: 现有方法缺乏3D几何引导，难以处理复杂人体动作且控制有限。
- Method: 提出PoseFuse3D-KI，结合3D几何与2D潜在空间，并开发新数据集CHKI-Video。
- Result: PSNR提升9%，LPIPS降低38%，插值保真度显著提高。
- Conclusion: PoseFuse3D-KI在可控人体关键帧插值中表现优异，验证了3D引导的有效性。


### [131] [DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation](https://arxiv.org/abs/2506.03123)
*Zhengyao Lv,Chenyang Si,Tianlin Pan,Zhaoxi Chen,Kwan-Yee K. Wong,Yu Qiao,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种双专家一致性模型（DCM），通过语义专家和细节专家的分工，解决了视频扩散模型蒸馏中的梯度冲突问题，显著提升了时间一致性和细节质量。

- Motivation: 扩散模型在视频合成中表现优异，但计算开销大；一致性模型虽能加速，但直接应用于视频扩散模型会导致时间一致性和细节退化。
- Method: 提出DCM模型，分为语义专家和细节专家，分别优化语义布局与运动、细节细化；引入时间相干损失和GAN/特征匹配损失。
- Result: DCM在减少采样步数的同时，实现了最先进的视觉质量。
- Conclusion: 专家分工策略有效解决了视频扩散模型蒸馏中的问题，提升了合成效果。


### [132] [AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation](https://arxiv.org/abs/2506.03126)
*Lu Qiu,Yizhuo Li,Yuying Ge,Yixiao Ge,Ying Shan,Xihui Liu*

Main category: cs.CV

TL;DR: AnimeShooter是一个参考引导的多镜头动画数据集，填补了现有数据集的不足，提供层次化注释和视觉一致性，并通过AnimeShooterGen展示了其在连贯动画生成中的有效性。

- Motivation: 现有公开数据集缺乏角色参考图像和连贯的多镜头视频生成能力，限制了AI生成动画的质量和一致性。
- Method: 提出AnimeShooter数据集，包含故事级和镜头级注释，并开发AnimeShooterGen模型，结合MLLM和视频扩散模型生成连贯动画。
- Result: 实验表明，基于AnimeShooter训练的模型在跨镜头视觉一致性和参考视觉引导方面表现优异。
- Conclusion: AnimeShooter为连贯动画视频生成提供了有价值的资源，并展示了参考引导方法的潜力。


### [133] [Native-Resolution Image Synthesis](https://arxiv.org/abs/2506.03131)
*Zidong Wang,Lei Bai,Xiangyu Yue,Wanli Ouyang,Yiyuan Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的生成模型范式，支持任意分辨率和宽高比的图像合成，克服了传统固定分辨率方法的限制。

- Motivation: 传统固定分辨率的图像生成方法无法灵活处理不同分辨率和宽高比的图像，限制了生成模型的适用范围。
- Method: 提出了Native-resolution diffusion Transformer (NiT)，通过建模可变分辨率和宽高比的视觉标记，实现灵活的图像生成。
- Result: NiT在ImageNet-256x256和512x512基准测试中达到最优性能，并展示了出色的零样本泛化能力，能生成高分辨率（如1536x1536）和多样宽高比（如16:9）的图像。
- Conclusion: 原生分辨率建模具有巨大潜力，可作为视觉生成模型与先进大语言模型方法之间的桥梁。


### [134] [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/abs/2506.03135)
*Mengdi Jia,Zekun Qi,Shaochen Zhang,Wenyao Zhang,Xinqiang Yu,Jiawei He,He Wang,Li Yi*

Main category: cs.CV

TL;DR: OmniSpatial是一个基于认知心理学的空间推理基准，涵盖四大类50个子类，包含1.5K问答对，揭示了现有视觉语言模型在空间理解上的局限性。

- Motivation: 当前视觉语言模型在基础空间关系理解上表现有限，缺乏对复杂空间推理的评估。
- Method: 通过互联网数据爬取和人工标注构建OmniSpatial基准，涵盖动态推理、复杂空间逻辑、空间交互和视角转换四大类。
- Result: 实验表明现有模型在综合空间理解上存在显著不足。
- Conclusion: OmniSpatial为未来研究提供了挑战和方向。


### [135] [SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation](https://arxiv.org/abs/2506.03139)
*Siqi Chen,Xinyu Dong,Haolei Xu,Xingyu Wu,Fei Tang,Hang Zhang,Yuchen Yan,Linjuan Wu,Wenqi Zhang,Guiyang Hou,Yongliang Shen,Weiming Lu,Yueting Zhuang*

Main category: cs.CV

TL;DR: SVGenius是一个全面的SVG处理基准测试，包含2,377个查询，覆盖理解、编辑和生成三个维度，评估了22种主流模型，发现专有模型优于开源模型，且推理增强训练比纯扩展更有效。

- Motivation: 现有SVG处理基准测试存在现实覆盖不足、复杂性分层缺失和评估范式碎片化的问题，SVGenius旨在填补这一空白。
- Method: SVGenius基于24个应用领域的真实数据，通过8个任务类别和18个指标评估模型，涵盖不同规模、架构和训练范式的22种模型。
- Result: 专有模型表现优于开源模型，所有模型随复杂性增加性能下降，推理增强训练对克服限制更有效，风格迁移仍是最具挑战性的能力。
- Conclusion: SVGenius为SVG处理建立了首个系统性评估框架，为开发更强大的矢量图形模型和自动化图形设计应用提供了关键见解。


### [136] [CamCloneMaster: Enabling Reference-based Camera Control for Video Generation](https://arxiv.org/abs/2506.03140)
*Yawen Luo,Jianhong Bai,Xiaoyu Shi,Menghan Xia,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai,Tianfan Xue*

Main category: cs.CV

TL;DR: CamCloneMaster是一个无需相机参数或微调的框架，通过参考视频复制相机运动，支持图像到视频和视频到视频任务。

- Motivation: 现有方法依赖显式相机参数序列，用户操作繁琐，特别是复杂相机运动时。
- Method: 提出CamCloneMaster框架，利用参考视频复制相机运动，无需相机参数或微调，并构建Camera Clone Dataset支持学习。
- Result: 实验和用户研究表明，CamCloneMaster在相机可控性和视觉质量上优于现有方法。
- Conclusion: CamCloneMaster提供了一种更直观的相机控制方法，适用于多种任务，表现优异。


### [137] [Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval](https://arxiv.org/abs/2506.03141)
*Jiwen Yu,Jianhong Bai,Yiran Qin,Quande Liu,Xintao Wang,Pengfei Wan,Di Zhang,Xihui Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Context-as-Memory的方法，利用历史上下文作为视频生成的记忆，通过简单有效的设计和内存检索模块，显著提升了长视频生成的场景一致性。

- Motivation: 现有方法在生成长视频时因历史上下文利用不足而缺乏场景一致性记忆能力。
- Method: 1. 以帧格式存储上下文；2. 通过沿帧维度拼接上下文和待预测帧进行条件化；3. 引入内存检索模块以减少计算开销。
- Result: 实验表明，该方法在长视频生成中优于现有技术，并能泛化到未见过的开放域场景。
- Conclusion: Context-as-Memory通过高效利用历史上下文，显著提升了视频生成的记忆能力和场景一致性。


### [138] [MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query](https://arxiv.org/abs/2506.03144)
*Wei Chow,Yuan Gao,Linfeng Li,Xian Wang,Qi Xu,Hang Song,Lingdong Kong,Ran Zhou,Yi Zeng,Yidong Cai,Botian Jiang,Shilin Xu,Jiajun Zhang,Minghui Qiu,Xiangtai Li,Tianshu Yang,Siliang Tang,Juncheng Li*

Main category: cs.CV

TL;DR: 论文介绍了MERIT数据集和Coral框架，解决了多条件语义检索的局限性，性能提升45.9%。

- Motivation: 现有数据集和模型在多条件语义检索中存在不足，无法充分利用视觉信息。
- Method: 提出MERIT数据集和Coral框架，结合嵌入重构和对比学习。
- Result: Coral在MERIT上性能提升45.9%，并在多个基准测试中验证了泛化能力。
- Conclusion: 论文为多条件语义检索的未来研究奠定了基础。


### [139] [UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147)
*Bin Lin,Zongjian Li,Xinhua Cheng,Yuwei Niu,Yang Ye,Xianyi He,Shenghai Yuan,Wangbo Yu,Shaodong Wang,Yunyang Ge,Yatian Pang,Li Yuan*

Main category: cs.CV

TL;DR: UniWorld是一个基于语义特征的统一生成框架，利用视觉语言模型和对比语义编码器，仅用1%的数据量就超越了BAGEL在图像编辑任务上的表现，同时在图像理解和生成任务中保持竞争力。

- Motivation: 现有统一模型在视觉语言理解和文本生成图像方面表现优异，但在图像感知和操作任务上存在不足。GPT-4o-Image的成功启发了作者探索基于语义特征的统一框架。
- Method: 提出UniWorld框架，利用视觉语言模型和对比语义编码器提取的语义特征，构建统一生成模型。
- Result: UniWorld在图像编辑任务上超越BAGEL，同时在图像理解和生成任务中表现优异。
- Conclusion: UniWorld展示了基于语义特征的统一框架在图像感知和操作任务中的潜力，并开源了模型和数据集。


### [140] [Self-Supervised Spatial Correspondence Across Modalities](https://arxiv.org/abs/2506.03148)
*Ayush Shrivastava,Andrew Owens*

Main category: cs.CV

TL;DR: 提出了一种跨模态时空对应方法，无需对齐数据即可学习特征表示，在几何和语义匹配任务中表现优异。

- Motivation: 解决不同视觉模态（如RGB与深度图）间像素对应的问题，避免显式光一致性假设和对齐数据需求。
- Method: 扩展对比随机游走框架，同时学习跨模态和模态内匹配的循环一致特征表示。
- Result: 在RGB-深度、RGB-热成像等几何匹配及照片-素描等语义匹配任务中表现优异。
- Conclusion: 该方法简单有效，无需标注数据，适用于多种跨模态匹配任务。


### [141] [IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation](https://arxiv.org/abs/2506.03150)
*Yuanze Lin,Yi-Wen Chen,Yi-Hsuan Tsai,Ronald Clark,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: IllumiCraft是一个端到端的扩散框架，通过整合光照、外观和几何线索，生成高质量且时间一致的视频。

- Motivation: 现有的扩散模型在控制场景光照和视觉外观时缺乏几何线索的显式集成。
- Method: IllumiCraft接受三种互补输入：HDR视频图、合成重光照帧和3D点轨迹，通过统一扩散架构整合这些线索。
- Result: 生成的视频在时间上一致且与用户定义提示对齐，保真度优于现有可控视频生成方法。
- Conclusion: IllumiCraft通过显式整合几何和光照线索，提升了视频生成的质量和可控性。
## cs.RO

### [142] [Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schrödinger Bridges](https://arxiv.org/abs/2506.02489)
*Tao Zhong,Jonah Buchanan,Christine Allen-Blanchette*

Main category: cs.RO

TL;DR: 提出一种基于视觉的灵巧抓取转换方法，通过Schrödinger Bridge框架实现不同形态机械手之间的抓取意图转移。

- Motivation: 解决不同形态机械手之间抓取意图的转移问题，无需配对演示或特定手的模拟。
- Method: 使用Schrödinger Bridge框架，通过分数和流匹配学习源和目标潜在抓取空间的映射，结合物理约束的成本函数。
- Result: 实验表明，该方法能生成稳定、物理合理的抓取，具有强泛化能力。
- Conclusion: 实现了异构机械手的语义抓取转移，将视觉抓取与概率生成建模结合。


### [143] [HiLO: High-Level Object Fusion for Autonomous Driving using Transformers](https://arxiv.org/abs/2506.02554)
*Timo Osterburg,Franz Albers,Christopher Diehl,Rajesh Pushparaj,Torsten Bertram*

Main category: cs.RO

TL;DR: 论文提出了一种基于Transformer的高层对象融合方法HiLO，改进了传统的Adapted Kalman Filter（AKF），在F1分数和平均IoU上分别提升了25.9和6.1个百分点。

- Motivation: 为了解决基于学习的融合方法在硬件要求和复杂性上的限制，同时保持高性能和鲁棒性。
- Method: 提出了一种新型的基于Transformer的高层对象融合方法HiLO，并对传统的AKF进行了改进。
- Result: 在F1分数和平均IoU上分别提升了25.9和6.1个百分点，并在大规模真实数据集上验证了有效性。
- Conclusion: HiLO方法在性能和通用性上表现出色，适用于城市和高速公路场景，代码和数据已开源。


### [144] [Rodrigues Network for Learning Robot Actions](https://arxiv.org/abs/2506.02618)
*Jialiang Zhang,Haoran Geng,Yang You,Congyue Deng,Pieter Abbeel,Jitendra Malik,Leonidas Guibas*

Main category: cs.RO

TL;DR: 论文提出了一种名为Neural Rodrigues Operator的学习方法，用于增强神经网络对关节系统运动结构的理解，并设计了Rodrigues Network（RodriNet）用于动作处理。实验表明该方法在运动预测和模仿学习等任务中表现优异。

- Motivation: 现有架构（如MLPs和Transformers）缺乏对关节系统运动结构的归纳偏置，限制了动作理解和预测的能力。
- Method: 提出了Neural Rodrigues Operator，一种可学习的广义前向运动学操作，并基于此设计了RodriNet网络架构。
- Result: 在运动预测任务中表现显著优于标准架构，并在模仿学习和3D手部重建应用中验证了其有效性。
- Conclusion: 将运动学先验整合到网络架构中，能够提升动作学习的效果。
## cs.MM

### [145] [EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR](https://arxiv.org/abs/2506.02380)
*Zihao Ding,Cheng-Tse Lee,Mufeng Zhu,Tao Guan,Yuan-Chun Sun,Cheng-Hsin Hsu,Yao Liu*

Main category: cs.MM

TL;DR: 论文介绍了EyeNavGS，首个公开的6自由度导航数据集，包含46名参与者在12个3D高斯泼溅场景中的导航数据，用于推动6-DoF视口预测、自适应流媒体等研究。

- Motivation: 当前缺乏用于高保真3D高斯泼溅（3DGS）场景的真实用户导航数据，限制了相关应用的开发和性能优化。
- Method: 使用Meta Quest Pro头显记录46名参与者在12个3DGS场景中的头部姿态和眼动数据，并进行场景初始化和数据处理。
- Result: 发布了EyeNavGS数据集及开源工具，支持6-DoF导航研究。
- Conclusion: EyeNavGS为3DGS场景的视口预测、自适应流媒体等研究提供了宝贵资源。
## eess.SP

### [146] [Simulate Any Radar: Attribute-Controllable Radar Simulation via Waveform Parameter Embedding](https://arxiv.org/abs/2506.03134)
*Weiqing Xiao,Hao Huang,Chonghao Zhong,Yujie Lin,Nan Wang,Xiaoxue Chen,Zhaoxi Chen,Saining Zhang,Shuocheng Yang,Pierre Merriaux,Lei Lei,Hao Zhao*

Main category: eess.SP

TL;DR: SA-Radar是一种雷达模拟方法，结合生成式和物理模拟，通过波形参数化属性嵌入实现可控高效的雷达数据生成。

- Motivation: 解决现有雷达模拟方法在可控性和效率上的不足，支持多样化的传感器配置和场景需求。
- Method: 设计ICFAR-Net（基于3D U-Net），通过波形参数编码雷达属性，生成范围-方位-多普勒（RAD）张量，无需详细硬件规格。
- Result: 模拟数据在多种下游任务（如2D/3D目标检测和雷达语义分割）中表现真实且有效，显著提升模型性能。
- Conclusion: SA-Radar可作为自动驾驶应用的通用雷达数据引擎，支持新视角和场景编辑。
## cs.SD

### [147] [MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation](https://arxiv.org/abs/2506.02661)
*Mingyang Huang,Peng Zhang,Bang Zhang*

Main category: cs.SD

TL;DR: MotionRAG-Diff结合检索增强生成与扩散模型，生成高质量、音乐同步的长舞蹈序列。

- Motivation: 现有方法在生成长期、连贯且音乐同步的舞蹈序列时存在局限性，如固定模板库或缺乏时间一致性。
- Method: 提出混合框架，包括跨模态对比学习、优化运动图系统和多条件扩散模型。
- Result: 实验表明，MotionRAG-Diff在运动质量、多样性和音乐同步性上达到最优。
- Conclusion: 结合检索与扩散模型，为音乐驱动舞蹈生成提供了新范式。
## cs.LG

### [148] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/abs/2505.21777)
*Bao Pham,Gabriel Raya,Matteo Negri,Mohammed J. Zaki,Luca Ambrogioni,Dmitry Krotov*

Main category: cs.LG

TL;DR: 论文通过联想记忆（AM）视角分析扩散模型，揭示了在小数据和大数据条件下扩散模型的记忆与泛化现象，并预测和验证了虚假状态的存在。

- Motivation: 研究扩散模型在联想记忆框架下的行为，探讨其记忆与泛化机制，特别是虚假状态的出现。
- Method: 将扩散模型的训练和生成阶段分别类比为联想记忆的编码和检索过程，分析不同数据量下的模型行为。
- Result: 在小数据时，模型表现出强记忆性；大数据时，生成样本的流形形成新的吸引状态，虚假状态出现在过渡边界。
- Conclusion: 研究为扩散模型的记忆-泛化现象提供了新视角，并理论预测和实验验证了虚假状态的存在。


### [149] [Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/abs/2506.01970)
*Ruizhuo Song,Beiming Yuan*

Main category: cs.LG

TL;DR: 论文研究了提升AI抽象推理能力的挑战，提出Johnny架构和Spin-Transformer网络，显著提升了Raven渐进矩阵任务的性能。

- Motivation: 传统RPM解决模型依赖选项池配置，限制了推理能力，需改进。
- Method: 提出Johnny架构（表示提取与推理模块协同）和Spin-Transformer网络（优化位置关系捕捉）。
- Result: 实验显示Johnny和Spin-Transformer在RPM任务中表现优异。
- Conclusion: 新方法为提升AI抽象推理能力提供了创新途径。


### [150] [EWGN: Elastic Weight Generation and Context Switching in Deep Learning](https://arxiv.org/abs/2506.02065)
*Shriraj P. Sawant,Krishna P. Miyapuram*

Main category: cs.LG

TL;DR: 论文提出了一种名为弹性权重生成网络（EWGN）的新方法，通过动态生成权重和上下文切换来缓解神经网络中的灾难性遗忘问题，并在标准数据集上验证了其有效性。

- Motivation: 受人类多任务学习能力的启发，研究旨在解决神经网络在任务切换和任务多样性中的灾难性遗忘问题。
- Method: 提出EWGN架构，通过附加网络动态生成主网络权重，实现输入依赖的上下文切换，并结合弹性权重巩固算法。
- Result: 在MNIST和fashion-MNIST数据集上，EWGN在任务保留方面表现优于传统全连接网络和卷积神经网络。
- Conclusion: 动态权重生成和上下文切换能力为持续学习提供了新思路，有望提升性能。


### [151] [Robust Federated Learning against Noisy Clients via Masked Optimization](https://arxiv.org/abs/2506.02079)
*Xuefeng Jiang,Tian Wen,Zhiqin Yang,Lvhua Wu,Yufeng Chen,Sheng Sun,Yuwei Wang,Min Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为MaskedOptim的两阶段优化框架，用于解决联邦学习中复杂标签噪声问题，包括噪声客户端检测和标签校正，并通过几何中值聚合提升模型鲁棒性。

- Motivation: 联邦学习中不同客户端的标签噪声问题严重影响模型性能，需要一种有效策略来减轻噪声客户端的不良影响。
- Method: 提出两阶段框架：第一阶段检测高噪声率客户端，第二阶段通过端到端标签校正机制修正噪声标签，并使用几何中值聚合模型。
- Result: 在多个数据集上的实验表明，该框架在不同场景下表现鲁棒，并能有效提升噪声客户端数据的质量。
- Conclusion: MaskedOptim框架成功解决了联邦学习中的标签噪声问题，并通过开源代码促进相关研究。


### [152] [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096)
*Zijian Wu,Jinjie Ni,Xiangyan Liu,Zichen Liu,Hang Yan,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: SynthRL是一种通过合成强化学习数据提升视觉语言模型性能的管道，包含问题选择、增强和验证三阶段，显著提升了模型的复杂推理能力。

- Motivation: 研究如何通过合成强化学习数据进一步提升基于可验证奖励的强化学习（RLVR）训练的视觉语言模型（VLMs）的性能。
- Method: 提出SynthRL管道，包括选择种子问题、增强问题难度并保留答案、以及验证阶段确保正确性和难度提升。
- Result: 在MMK12数据集上，SynthRL从8K种子样本中合成了3.3K额外可验证问题，模型在五个视觉数学推理基准测试中表现显著优于基线。
- Conclusion: SynthRL能有效提升模型在复杂推理任务中的表现，尤其在最具挑战性的样本上效果显著。


### [153] [Rethinking Post-Unlearning Behavior of Large Vision-Language Models](https://arxiv.org/abs/2506.02541)
*Minsung Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.LG

TL;DR: 论文提出了一种针对大型视觉语言模型（LVLMs）的新遗忘任务，强调在隐私保护的同时生成信息丰富且视觉相关的响应。现有方法常因忽略替代输出的选择而导致不良行为，作者提出的PUBG方法有效解决了这些问题。

- Motivation: 现有遗忘方法在保护隐私时，常导致模型生成退化、幻觉或过度拒绝的响应。作者认为，生成式LVLMs需要更关注遗忘后的响应质量。
- Method: 提出PUBG方法，明确引导遗忘后的行为朝向理想输出分布，确保响应既隐私保护又信息丰富。
- Result: 实验表明，PUBG成功避免了隐私泄露，同时生成视觉相关且信息丰富的响应，解决了现有方法的不足。
- Conclusion: PUBG为LVLMs提供了一种更有效的遗忘方法，兼顾隐私保护和响应质量。


### [154] [HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification](https://arxiv.org/abs/2506.02542)
*Niklas Kormann,Masoud Ramuz,Zeeshan Nisar,Nadine S. Schaadt,Hendrik Annuth,Benjamin Doerr,Friedrich Feuerhake,Thomas Lampert,Johannes F. Lutzeyer*

Main category: cs.LG

TL;DR: 提出了一种用于肾小球分类的新型异构图神经网络HIEGNet，结合传统和机器学习方法构建图，并整合免疫细胞信息，性能优于基线模型。

- Motivation: 探索图神经网络在肾小球健康分类中的应用，解决图构建中的节点、边和特征识别难题。
- Method: 提出基于计算机视觉技术的图构建流程，设计异构图神经网络HIEGNet，整合肾小球及周围免疫细胞信息。
- Result: HIEGNet在肾移植患者数据集上表现优于基线模型，泛化能力最佳。
- Conclusion: HIEGNet为肾小球分类提供了有效解决方案，代码已开源。


### [155] [SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search](https://arxiv.org/abs/2506.02623)
*Yuyang Zhou,Ferrante Neri,Yew-Soon Ong,Ruibin Bai*

Main category: cs.LG

TL;DR: 论文提出了一种基于Siamese网络的代理模型方法（SiamNAS），用于高效解决多目标神经架构搜索（NAS）问题，显著降低计算成本。

- Motivation: 现代NAS是多目标的，需要在准确性、参数数量和计算成本之间权衡，但传统方法计算昂贵且难以高效解决。
- Method: 使用Siamese网络块集成预测候选架构的支配关系，替代拥挤距离计算，并集成到SiamNAS框架中。
- Result: 在NAS-Bench-201上，SiamNAS以极低计算成本（0.01 GPU天）找到Pareto最优解，表现优于基准。
- Conclusion: Siamese网络代理模型在多任务优化中具有潜力，并可扩展为生成多样Pareto最优解集。


### [156] [Interaction Field Matching: Overcoming Limitations of Electrostatic Models](https://arxiv.org/abs/2506.02950)
*Stepan I. Manukhov,Alexander Kolesov,Vladimir V. Palyulin,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文提出了一种名为交互场匹配（IFM）的新方法，作为静电匹配（EFM）的扩展，解决了EFM中建模复杂静电场的难题。

- Motivation: EFM需要建模复杂的静电场，这在技术上具有挑战性。
- Method: IFM扩展了EFM，允许使用更一般的交互场，并设计了一种基于强相互作用的实现方式。
- Result: 在玩具和图像数据转移问题上展示了性能。
- Conclusion: IFM解决了EFM的问题，并展示了其有效性。
## cs.SE

### [157] [Is PMBOK Guide the Right Fit for AI? Re-evaluating Project Management in the Face of Artificial Intelligence Projects](https://arxiv.org/abs/2506.02214)
*Alexey Burdakov,Max Jaihyun Ahn*

Main category: cs.SE

TL;DR: 论文评估了PMBOK指南在AI软件项目中的适用性，指出了局限性并提出了改进建议。

- Motivation: 传统项目管理框架（如PMBOK）在AI项目中存在不足，需针对AI特点（如数据复杂性、迭代开发、伦理问题）进行适配。
- Method: 分析PMBOK的局限性，提出数据生命周期管理、迭代框架和伦理嵌入等改进措施。
- Result: 发现PMBOK在数据管理、迭代支持和伦理指导方面存在缺陷，需调整以适配AI项目。
- Conclusion: 通过改进项目管理框架，可更好地支持AI软件项目的动态性和探索性。
## cs.AI

### [158] [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)
*Renyang Liu,Wenjie Feng,Tianwei Zhang,Wei Zhou,Xueqi Cheng,See-Kiong Ng*

Main category: cs.AI

TL;DR: 该论文探讨了图像生成模型遗忘（IGMU）的挑战，提出了任务分类框架CatIGMU、评估框架EvalIGMU和高质量数据集DataIGM，以解决现有方法的不足。

- Motivation: 图像生成模型的广泛应用引发数据隐私和内容安全问题，机器遗忘（MU）是解决这些问题的有效手段，但IGMU在实践中仍存在任务分类不清、评估框架缺失等问题。
- Method: 论文设计了CatIGMU任务分类框架、EvalIGMU评估框架，并构建了DataIGM数据集，用于全面评估IGMU算法。
- Result: 研究发现现有IGMU算法在多个评估维度（如保留性和鲁棒性）上表现不佳。
- Conclusion: 论文提出的框架和数据集为IGMU的理解、分类和评估提供了标准化工具，揭示了现有算法的局限性。


### [159] [DPO Learning with LLMs-Judge Signal for Computer Use Agents](https://arxiv.org/abs/2506.03095)
*Man Luo,David Cobbley,Xin Su,Shachar Rosenman,Vasudev Lal,Shao-Yen Tseng,Phillip Howard*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级视觉语言模型，用于开发隐私保护和资源高效的计算机使用代理（CUA），通过本地运行和自动数据筛选框架提升性能。

- Motivation: 现有CUA依赖云推理，存在隐私和扩展性问题，需开发本地运行的轻量级解决方案。
- Method: 采用LLM-as-Judge框架自动评估和筛选合成交互轨迹，生成高质量数据用于强化学习。
- Result: 在OS-World基准测试中，本地微调模型优于现有基线。
- Conclusion: 该方法为隐私、高效和通用的GUI代理提供了可行路径。
## eess.IV

### [160] [Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance](https://arxiv.org/abs/2506.01980)
*Lianhao Yin,Ozanan Meireles,Guy Rosman,Daniela Rus*

Main category: eess.IV

TL;DR: 论文提出了一种名为Compress-to-Explore（C2E）的自监督框架，利用Kolmogorov复杂性从手术视频中学习紧凑且信息丰富的表示，无需标注数据即可提升编码器性能。

- Motivation: 微创手术（MIS）中实时视频理解至关重要，但监督学习方法需要大量标注数据，而这些数据在医学领域中稀缺且标注成本高。现有自监督方法难以捕捉通用任务的结构和物理信息。
- Method: C2E框架通过熵最大化解码器压缩图像，同时保留临床相关细节，利用Kolmogorov复杂性学习紧凑表示。
- Result: 在大规模未标注手术数据集上训练的C2E在多种手术ML任务（如工作流分类、工具-组织交互分类、分割和诊断）中表现出强泛化能力，性能优于现有方法。
- Conclusion: C2E展示了自监督学习在提升手术AI性能和改善MIS结果方面的潜力，其紧凑表示能更好地解耦图像特征。


### [161] [Alzheimers Disease Classification in Functional MRI With 4D Joint Temporal-Spatial Kernels in Novel 4D CNN Model](https://arxiv.org/abs/2506.02060)
*Javier Salazar Cavazos,Scott Peltier*

Main category: eess.IV

TL;DR: 提出了一种新型4D卷积网络，用于提取功能MRI数据的时空特征，相比传统3D模型表现更优。

- Motivation: 现有方法在4D功能MRI数据上使用3D空间模型，可能导致特征提取不足，影响下游任务（如分类）性能。
- Method: 开发了一种4D卷积网络，提取时空联合核，同时学习空间信息和时间动态。
- Result: 实验表明，4D CNN在功能MRI数据中表现优于3D模型，提升了阿尔茨海默病的早期诊断能力。
- Conclusion: 未来研究可探索任务型fMRI应用和回归任务，以进一步理解认知表现和疾病进展。


### [162] [Are Pixel-Wise Metrics Reliable for Sparse-View Computed Tomography Reconstruction?](https://arxiv.org/abs/2506.02093)
*Tianyu Lin,Xinran Li,Chuntung Zhuang,Qi Chen,Yuanhao Cai,Kai Ding,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: 提出了一种新的解剖感知评估指标和CARE框架，用于提升稀疏视图CT重建中关键解剖结构的完整性。

- Motivation: 现有评估指标（如SSIM和PSNR）在稀疏视图CT重建中未能充分捕捉关键解剖结构的完整性，尤其是小或薄区域。
- Method: 提出了一套解剖感知评估指标，并开发了CARE框架，通过结构惩罚在训练中增强解剖结构的保留。
- Result: CARE显著提高了CT重建的结构完整性，大器官提升32%，小器官22%，肠道40%，血管36%。
- Conclusion: CARE框架能有效提升稀疏视图CT重建中解剖结构的完整性，适用于多种重建方法。


### [163] [NTIRE 2025 Challenge on RAW Image Restoration and Super-Resolution](https://arxiv.org/abs/2506.02197)
*Marcos V. Conde,Radu Timofte,Zihao Lu,Xiangyu Kongand Xiaoxia Xingand Fan Wangand Suejin Hanand MinKyu Parkand Tianyu Zhangand Xin Luoand Yeda Chenand Dong Liuand Li Pangand Yuhang Yangand Hongzhong Wangand Xiangyong Caoand Ruixuan Jiangand Senyan Xuand Siyuan Jiangand Xueyang Fuand Zheng-Jun Zhaand Tianyu Haoand Yuhong Heand Ruoqi Liand Yueqi Yangand Xiang Yuand Guanlan Hongand Minmin Yiand Yuanjia Chenand Liwen Zhangand Zijie Jinand Cheng Liand Lian Liuand Wei Songand Heng Sunand Yubo Wangand Jinghua Wangand Jiajie Luand Watchara Ruangsangand*

Main category: eess.IV

TL;DR: 本文回顾了NTIRE 2025 RAW图像修复与超分辨率挑战赛，总结了提出的解决方案和结果。

- Motivation: 现代图像信号处理（ISP）流程中，RAW图像修复与超分辨率的新方法可能至关重要，但该领域的研究不如RGB领域深入。
- Method: 挑战赛的目标包括（i）修复带有模糊和噪声的RAW图像，（ii）将RAW Bayer图像放大2倍，同时考虑未知噪声和模糊。
- Result: 共有230名参与者注册，45名在挑战期间提交了结果。
- Conclusion: 报告展示了当前RAW图像修复的最先进技术。


### [164] [Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation](https://arxiv.org/abs/2506.02312)
*Md Tauhidul Islam,Wu Da-Wen,Tang Qing-Qing,Zhao Kai-Yang,Yin Teng,Li Yan-Fei,Shang Wen-Yi,Liu Jing-Yu,Zhang Hai-Xian*

Main category: eess.IV

TL;DR: DEFFA-Unet提出了一种改进的视网膜血管分割方法，通过增加编码器、特征过滤融合模块和注意力引导的特征重构融合模块，解决了数据不足、分布不平衡和特征提取不足的问题，显著提升了模型性能。

- Motivation: 视网膜血管分割对诊断眼部和心血管疾病至关重要，但现有方法在数据不足、分布不平衡和特征提取方面存在局限性，影响了分割性能和模型泛化能力。
- Method: DEFFA-Unet引入额外编码器处理域不变预处理输入，开发特征过滤融合模块和注意力引导的特征重构融合模块，并提出创新的数据增强和平衡方法。
- Result: 在多个基准数据集上的实验表明，DEFFA-Unet优于基线模型和现有最佳模型，尤其在跨验证模型泛化方面表现突出。
- Conclusion: DEFFA-Unet通过改进特征提取和数据增强方法，显著提升了视网膜血管分割的性能和泛化能力。


### [165] [Unrolling Nonconvex Graph Total Variation for Image Denoising](https://arxiv.org/abs/2506.02381)
*Songlin Wei,Gene Cheung,Fei Chen,Ivan Selesnick*

Main category: eess.IV

TL;DR: 提出了一种新的非凸总变分项（NC-GTV），结合图Huber函数，确保整体目标凸性，并通过ADMM算法优化，最终在图像去噪任务中表现优异。

- Motivation: 传统基于模型的图像去噪方法使用凸正则化项（如TV），但存在局限性。本文旨在提出一种非凸总变分项，以提升稀疏信号表示能力。
- Method: 定义NC-GTV为图Huber函数，确保目标凸性；通过GCT计算参数a；设计基于ADMM的线性时间算法，并将其展开为轻量级前馈网络。
- Result: 实验表明，该方法在图像去噪任务中优于其他代表性方法，且网络参数更少。
- Conclusion: NC-GTV结合图Huber函数和ADMM算法，在图像去噪中表现出色，具有高效性和优越性能。


### [166] [Multi-modal brain MRI synthesis based on SwinUNETR](https://arxiv.org/abs/2506.02467)
*Haowen Pang,Weiyan Guo,Chuyang Ye*

Main category: eess.IV

TL;DR: 提出了一种基于SwinUNETR的方法，用于合成脑部MRI中缺失的模态，结合了Swin Transformer和CNN的优势，显著提升了合成图像的质量和临床价值。

- Motivation: 多模态脑部MRI在临床诊断中至关重要，但实际应用中常出现模态缺失的问题，亟需一种高效的方法来合成缺失的模态。
- Method: 采用SwinUNETR，一种结合Swin Transformer和CNN的新型网络架构，通过分层特征提取和窗口自注意力机制，有效捕捉局部和全局信息。
- Result: 实验表明，SwinUNETR在脑部MRI数据集上表现优异，生成的合成图像在质量、解剖一致性和诊断价值上均有显著提升。
- Conclusion: SwinUNETR为解决脑部MRI模态缺失问题提供了一种高效且可靠的解决方案，具有重要的临床应用潜力。


### [167] [Dynamic mapping from static labels: remote sensing dynamic sample generation with temporal-spectral embedding](https://arxiv.org/abs/2506.02574)
*Shuai Yuan,Shuang Chen,Tianwu Lin,Jie Wang,Peng Gong*

Main category: eess.IV

TL;DR: 提出TasGen框架，通过静态样本自动生成动态样本，减少人工标注需求。

- Motivation: 遥感地理制图依赖代表性样本数据，但地表动态变化快，样本易过时，人工更新负担重。
- Method: TasGen框架分两阶段，通过时间-光谱嵌入建模时间序列遥感影像的依赖关系。
- Result: 无需额外人工标注，即可捕捉地表变化。
- Conclusion: TasGen能有效解决样本过时问题，提升遥感制图效率。


### [168] [A Tree-guided CNN for image super-resolution](https://arxiv.org/abs/2506.02585)
*Chunwei Tian,Mingjian Song,Xiaopeng Fan,Xiangtao Zheng,Bob Zhang,David Zhang*

Main category: eess.IV

TL;DR: 提出了一种基于树引导的CNN（TSRNet），用于图像超分辨率，通过树架构增强关键节点效果，结合余弦变换技术和Adan优化器提升性能。

- Motivation: 现有深度卷积神经网络在图像超分辨率中难以有效利用关键层信息，导致性能受限。
- Method: 设计树引导的CNN（TSRNet），结合余弦变换提取跨域信息，并使用Adan优化器优化参数。
- Result: 实验验证了TSRNet在恢复高质量图像方面的优越性。
- Conclusion: TSRNet通过树架构和跨域信息提取显著提升了图像超分辨率性能。
## cs.GR

### [169] [FlexPainter: Flexible and Multi-View Consistent Texture Generation](https://arxiv.org/abs/2506.02620)
*Dongyu Yan,Leyi Wu,Jiantao Lin,Luozhou Wang,Tianshuo Xu,Zhifei Chen,Zhen Yang,Lie Xu,Shunsi Zhang,Yingcong Chen*

Main category: cs.GR

TL;DR: FlexPainter是一种新颖的纹理生成流程，通过多模态条件引导和一致性增强，显著提升了纹理生成的灵活性和质量。

- Motivation: 解决现有扩散方法在纹理生成中控制灵活性不足、多视角图像不一致导致质量差的问题。
- Method: 构建共享条件嵌入空间，结合图像CFG方法分解结构与风格信息；利用3D知识生成多视角图像，并通过视图同步与自适应加权模块确保局部一致性；最后使用3D感知纹理补全和增强模型生成高质量纹理。
- Result: 实验表明，FlexPainter在灵活性和生成质量上显著优于现有方法。
- Conclusion: FlexPainter通过多模态引导和一致性优化，为纹理生成提供了高效且高质量的解决方案。


### [170] [PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis](https://arxiv.org/abs/2506.02794)
*Mijeong Kim,Gunhee Kim,Jungyoon Choi,Wonjae Roh,Bohyung Han*

Main category: cs.GR

TL;DR: PhysGaia是一个专为动态新视角合成（DyNVS）设计的物理感知数据集，支持复杂动态场景建模，包含多种物理材料和交互。

- Motivation: 现有数据集主要关注逼真重建，缺乏物理感知的动态场景建模支持。PhysGaia旨在填补这一空白。
- Method: 数据集通过精心选择的物理求解器生成严格遵循物理规律的场景，提供3D粒子轨迹和物理参数等真实信息。
- Result: PhysGaia为动态视角合成、物理场景理解和深度学习模型提供了丰富的实验数据。
- Conclusion: PhysGaia将推动动态场景重建和物理模拟的深度学习研究，数据集和代码已公开。


### [171] [VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural Surface Reconstruction](https://arxiv.org/abs/2506.02895)
*Ahmad AlMughrabi,Umair Haroon,Ricardo Marques,Petia Radeva*

Main category: cs.GR

TL;DR: VolTex框架通过文本输入实现精准食物分割，结合神经表面重建方法，提升食物体积估计的准确性。

- Motivation: 现有3D食物体积估计方法在食物选择方面存在不足，VolTex旨在解决这一问题。
- Method: 用户通过文本输入指定目标食物，使用神经表面重建方法生成高保真3D网格进行体积计算。
- Result: 在MetaFood3D数据集上验证了VolTex在食物分割和体积估计中的有效性。
- Conclusion: VolTex为食物体积估计提供了更精确的食物选择和重建方法。


### [172] [PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples](https://arxiv.org/abs/2506.03004)
*Junyu Liu,R. Kenny Jones,Daniel Ritchie*

Main category: cs.GR

TL;DR: PartComposer是一个从单图像示例中学习部分级概念的框架，通过动态数据合成和最大化互信息实现可控组合。

- Motivation: 解决现有方法在细粒度概念学习或大数据需求上的不足。
- Method: 提出动态数据合成管道和概念预测器，通过最大化互信息实现概念解耦和重组。
- Result: 在相同或不同对象类别的概念混合中表现优于基线方法。
- Conclusion: PartComposer在部分级概念学习和可控组合方面表现出色。


### [173] [HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers](https://arxiv.org/abs/2506.03118)
*Zhiyuan Yu,Zhe Li,Hujun Bao,Can Yang,Xiaowei Zhou*

Main category: cs.GR

TL;DR: HumanRAM是一种新颖的前馈方法，用于从单目或稀疏图像中进行可泛化的人体重建和动画，通过引入显式姿态条件，将重建和动画统一到一个框架中。

- Motivation: 现有方法依赖密集视图捕获或耗时的优化过程，HumanRAM旨在解决这些限制。
- Method: 结合SMPL-X神经纹理的显式姿态条件，使用基于transformer的大规模重建模型和DPT解码器，从单目或稀疏图像生成新视角和新姿态的渲染。
- Result: 实验表明，HumanRAM在重建精度、动画保真度和泛化性能上显著优于先前方法。
- Conclusion: HumanRAM通过统一框架实现了高质量的人体重建和高保真姿态控制动画。
## cs.CL

### [174] [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)
*Jeonghun Kang,Soonmok Kwon,Joonseok Lee,Byung-Hak Kim*

Main category: cs.CL

TL;DR: DIAMOND是一种基于LLM的智能体，结合体育分析和自然语言推理，用于生成上下文感知的棒球比赛亮点摘要，显著优于传统方法。

- Motivation: 传统方法（如WPA或计算机视觉）无法捕捉比赛的策略深度、势头变化和故事线进展，而人工处理成本高且不可扩展。
- Method: DIAMOND结合了结构化体育分析（如Win Expectancy、WPA和Leverage Index）与LLM的自然语言推理模块，以量化和选择重要比赛片段。
- Result: 在韩国棒球联赛的五场比赛中，DIAMOND的F1分数从42.9%（仅WPA）提升到84.8%，优于商业和统计基线。
- Conclusion: DIAMOND展示了模块化、可解释的智能体框架在体育及其他领域事件级摘要中的潜力。


### [175] [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)
*Junzhe Zhang,Huixuan Zhang,Xinyu Hu,Li Lin,Mingqi Gao,Shi Qiu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文介绍了Minos-Corpus数据集和Minos模型，用于多模态生成任务的评估，特别是在文本到图像（T2I）任务中表现优异。

- Motivation: 现有研究忽视了T2I生成任务的评估能力和大规模人类评估数据的整合，本文旨在填补这一空白。
- Method: 提出Minos-Corpus数据集，结合人类和GPT评估数据，并采用数据选择与平衡、Mix-SFT训练方法和DPO技术开发Minos模型。
- Result: Minos在类似规模的开源评估模型中表现最优，尤其在T2I任务上超越所有开源和闭源模型。
- Conclusion: 高质量人类评估数据和联合训练对多模态评估任务至关重要。


### [176] [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)
*Sifan Li,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: HC-Bench基准测试显示，视觉语言模型（VLMs）在检测隐藏内容（如光学错觉或AI生成图像中的隐藏文本和物体）时表现极差（0-5.36%准确率），而人类却能本能解决。通过低分辨率缩放（SemVink方法），准确率可提升至>99%。

- Motivation: 揭示VLMs在低层次视觉任务中的不足，并提出改进方向。
- Method: 提出HC-Bench基准测试，包含112张隐藏内容图像，并引入SemVink方法（低分辨率缩放）。
- Result: VLMs在基准测试中表现极差，而SemVink方法显著提升准确率。
- Conclusion: VLMs需结合多尺度处理以弥补低层次视觉能力不足，提升实际应用中的鲁棒性。


### [177] [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143)
*Qianhui Wu,Kanzhi Cheng,Rui Yang,Chaoyun Zhang,Jianwei Yang,Huiqiang Jiang,Jian Mu,Baolin Peng,Bo Qiao,Reuben Tan,Si Qin,Lars Liden,Qingwei Lin,Huan Zhang,Tong Zhang,Jianbing Zhang,Dongmei Zhang,Jianfeng Gao*

Main category: cs.CL

TL;DR: GUI-Actor提出了一种基于视觉语言模型的无坐标GUI定位方法，通过注意力机制和验证器提升性能。

- Motivation: 解决现有方法在视觉定位中的空间语义对齐弱、模糊监督目标处理能力差以及坐标与视觉特征粒度不匹配的问题。
- Method: 引入基于注意力的动作头和对齐<ACTOR>令牌，设计验证器筛选动作区域。
- Result: 在多个基准测试中表现优异，泛化能力更强，部分模型超越现有最佳方法。
- Conclusion: GUI-Actor能在不牺牲通用性的情况下为VLM提供有效定位能力。
